

Bubble Value
at Risk

Founded in 1807, John Wiley & Sons is the oldest independent publishing
company in the United States. With ofﬁces in North America, Europe,
Australia and Asia, Wiley is globally committed to developing and
marketing print and electronic products and services for our customers’
professional and personal knowledge and understanding.
The Wiley Finance series contains books written speciﬁcally for ﬁnance
and investment professionals as well as sophisticated individual investors and
their ﬁnancial advisors. Book topics range from portfolio management to e-
commerce, risk management, ﬁnancial engineering, valuation and ﬁnancial
instrument analysis, as well as much more.
For a list of available titles, visit our Web site at www.WileyFinance.com.

Bubble Value
at Risk
A Countercyclical Risk
Management Approach
Revised Edition
MAX C. Y. WONG
John Wiley & Sons Singapore Pte. Ltd.

Copyright ª 2013 by Max Wong Chan Yue
Published by John Wiley & Sons Singapore Pte. Ltd.
1 Fusionopolis Walk, #07-01, Solaris South Tower, Singapore 138628
All rights reserved.
First edition published by Immanuel Consulting Pte. Ltd. in 2011
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form
or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as
expressly permitted by law, without either the prior written permission of the Publisher, or
authorization through payment of the appropriate photocopy fee to the Copyright Clearance Center.
Requests for permission should be addressed to the Publisher, John Wiley & Sons Singapore Pte. Ltd.,
1 Fusionopolis Walk, #07-01, Solaris South Tower, Singapore 138628, tel: 65–6643–8000, fax:
65–6643–8008, e-mail: enquiry@wiley.com.
Limit of Liability/Disclaimer of Warranty: While the publisher, author and contributors have used
their best efforts in preparing this book, they make no representations or warranties with respect to the
accuracy or completeness of the contents of this book and speciﬁcally disclaim any implied warranties
of merchantability or ﬁtness for a particular purpose. No warranty may be created or extended by
sales representatives or written sales materials. The advice and strategies contained herein may not be
suitable for your situation. You should consult with a professional where appropriate. Neither the
publisher, authors, or contributors shall be liable for any loss of proﬁt or any other commercial
damages, including but not limited to special, incidental, consequential, or other damages.
Excel is a registered trademark of the Microsoft Corporation.
RiskMetrics is a registered trademark of RiskMetrics Group.
CreditMetrics is a registered trademark of RiskMetrics Solutions.
CreditRisk+ is a registered trademark of Credit Suisse Group.
Bloomberg is a registered trademark of Bloomberg L.P.
CrashMetrics is a registered trademark of Paul Wilmott and Philip Hua.
BUVARt is a registered trademark of Wong Chan Yue
Other Wiley Editorial Ofﬁces
John Wiley & Sons, 111 River Street, Hoboken, NJ 07030, USA
John Wiley & Sons, The Atrium, Southern Gate, Chichester, West Sussex, P019 8SQ,
United Kingdom
John Wiley & Sons (Canada) Ltd., 5353 Dundas Street West, Suite 400, Toronto, Ontario, M9B 6HB,
Canada
John Wiley & Sons Australia Ltd., 42 McDougall Street, Milton, Queensland 4064, Australia
Wiley-VCH, Boschstrasse 12, D-69469 Weinheim, Germany
Library of Congress Cataloging-in-Publication Data
ISBN 978-1-118-55034-2 (Hardcover)
ISBN 978-1-118-55035-9 (ePDF)
ISBN 978-1-118-55036-6 (Mobi)
ISBN 978-1-118-55037-3 (ePub)
Typeset in 10/12pt Sabon-Roman by MPS Limited, Chennai, India
Printed in Singapore by Ho Printing
10
9
8
7
6
5
4
3
2
1

To my heavenly Father, who gave me this assignment


Contents
About the Author
xiii
Foreword
xv
Preface
xvii
Acknowledgments
xxi
PART ONE
Background
CHAPTER 1
Introduction
3
1.1
The Evolution of Riskometer
4
1.2
Taleb’s Extremistan
6
1.3
The Turner Procyclicality
7
1.4
The Common Sense of Bubble Value-at-Risk (BuVaR)
8
Notes
13
CHAPTER 2
Essential Mathematics
15
2.1
Frequentist Statistics
15
2.2
Just Assumptions
18
2.3
Quantiles, VaR, and Tails
26
2.4
Correlation and Autocorrelation
29
2.5
Regression Models and Residual Errors
35
2.6
Signiﬁcance Tests
38
2.7
Measuring Volatility
41
2.8
Markowitz Portfolio Theory
45
2.9
Maximum Likelihood Method
48
2.10
Cointegration
50
2.11
Monte Carlo Method
52
vii

2.12
The Classical Decomposition
55
2.13
Quantile Regression Model
58
2.14
Spreadsheet Exercises
62
Notes
64
PART TWO
Value at Risk Methodology
CHAPTER 3
Preprocessing
67
3.1
System Architecture
67
3.2
Risk Factor Mapping
70
3.3
Risk Factor Proxies
75
3.4
Scenario Generation
76
3.5
Basic VaR Speciﬁcation
79
Notes
81
CHAPTER 4
Conventional VaR Methods
83
4.1
Parametric VaR
84
4.2
Monte Carlo VaR
89
4.3
Historical Simulation VaR
93
4.4
Issue: Convexity, Optionality, and Fat Tails
96
4.5
Issue: Hidden Correlation
102
4.6
Issue: Missing Basis and Beta Approach
104
4.7
Issue: The Real Risk of Premiums
106
4.8
Spreadsheet Exercises
107
Notes
108
CHAPTER 5
Advanced VaR Methods
111
5.1
Hybrid Historical Simulation VaR
111
5.2
Hull-White Volatility Updating VaR
113
5.3
Conditional Autoregressive VaR (CAViaR)
114
5.4
Extreme Value Theory VaR
116
5.5
Spreadsheet Exercises
122
Notes
124
CHAPTER 6
VaR Reporting
125
6.1
VaR Aggregation and Limits
125
6.2
Diversiﬁcation
126
viii
CONTENTS

6.3
VaR Analytical Tools
127
6.4
Scaling and Basel Rules
132
6.5
Spreadsheet Exercises
136
Notes
137
CHAPTER 7
The Physics of Risk and Pseudoscience
139
7.1
Entropy, Leverage Effect, and Skewness
140
7.2
Volatility Clustering and the Folly of i.i.d.
144
7.3
“Volatility of Volatility” and Fat Tails
145
7.4
Extremistan and the Fourth Quadrant
148
7.5
Regime Change, Lagging Riskometer,
and Procyclicality
151
7.6
Coherence and Expected Shortfall
154
7.7
Spreadsheet Exercises
156
Notes
156
CHAPTER 8
Model Testing
159
8.1
The Precision Test
159
8.2
The Frequency Back Test
160
8.3
The Bunching Test
163
8.4
The Whole Distribution Test
165
8.5
Spreadsheet Exercises
167
Notes
167
CHAPTER 9
Practical Limitations of VaR
169
9.1
Depegs and Changes to the Rules of the Game
169
9.2
Data Integrity Problems
171
9.3
Model Risk
172
9.4
Politics and Gaming
174
Notes
175
CHAPTER 10
Other Major Risk Classes
177
10.1
Credit Risk (and CreditMetrics)
177
10.2
Liquidity Risk
182
10.3
Operational Risk
187
10.4
The Problem of Aggregation
190
10.5
Spreadsheet Exercises
195
Notes
195
Contents
ix

PART THREE
The Great Regulatory Reform
CHAPTER 11
Regulatory Capital Reform
199
11.1
Basel I and Basel II
199
11.2
The Turner Review
202
11.3
Revisions to Basel II Market Risk Framework (Basel 2.5)
206
11.4
New Liquidity Framework
211
11.5
The New Basel III
212
11.6
The New Framework for the Trading Book
214
11.7
The Ideal Capital Regime
215
Notes
217
CHAPTER 12
Systemic Risk Initiatives
221
12.1
Soros’ Reﬂexivity, Endogenous Risks
221
12.2
CrashMetrics
226
12.3
New York Fed CoVaR
230
12.4
The Austrian Model and BOE RAMSI
233
12.5
The Global Systemic Risk Regulator
238
12.6
Spreadsheet Exercises
240
Notes
241
PART FOUR
Introduction to Bubble Value-at-Risk (BuVaR)
CHAPTER 13
Market BuVaR
245
13.1
Why an Alternative to VaR?
245
13.2
Classical Decomposition, New Interpretation
247
13.3
Measuring the Bubble
250
13.4
Calibration
254
13.5
Implementing the Inﬂator
257
13.6
Choosing the Best Tail-Risk Measure
259
13.7
Effect on Joint Distribution
262
13.8
The Scope of BuVaR
264
13.9
How Good Is the BuVaR Buffer?
265
13.10 The Brave New World
268
13.11 Spreadsheet Exercises
271
Notes
271
x
CONTENTS

CHAPTER 14
Credit BuVaR
273
14.1
The Credit Bubble VaR Idea
273
14.2
Model Formulation
276
14.3
Behavior of Response Function
278
14.4
Characteristics of Credit BuVaR
280
14.5
Interpretation of Credit BuVaR
282
14.6
Spreadsheet Exercises
284
Notes
284
CHAPTER 15
Acceptance Tests
285
15.1
BuVaR Visual Checks
285
15.2
BuVaR Event Timing Tests
297
15.3
BuVaR Cyclicality Tests
304
15.4
Credit BuVaR Parameter Tuning
306
Notes
313
CHAPTER 16
Other Topics
315
16.1
Diversiﬁcation and Basis Risks
315
16.2
Regulatory Reform and BuVaR
317
16.3
BuVaR and the Banking Book: Response Time as Risk
319
16.4
Can BuVaR Pick Tops and Bottoms Perfectly?
321
16.5
Postmodern Risk Management
321
16.6
Spreadsheet Exercises
323
Note
323
CHAPTER 17
Epilogue: Suggestions for Future Research
325
Note
327
About the Website
329
Bibliography
331
Index
337
Contents
xi


About the Author
M
ax Wong is a specialist in the area of risk modeling and Basel III. He
started his career as a derivatives consultant at Credit Suisse First
Boston in 1996. During the Asian crisis in 1998, he traded index futures at
the open-outcry floor of SIMEX (now SGX). From 2003 to 2011, he
worked for Standard Chartered Bank as a risk manager and senior quant.
He is currently head of VaR model testing at the Royal Bank of Scotland.
He has published papers on VaR models and Basel capital, recently
looking at innovative ways to model risk more effectively during crises and to
deal with the issues of procyclicality and Black Swan events in our financial
system. He has spoken on the subject at various conferences and seminars.
He holds a BSc in physics from the University of Malaya (1994) and an
MSc in financial engineering from the National University of Singapore (2004).
He is an adjunct at Singapore Management University, a member of the edi-
torial board of the Journal of Risk Management in Financial Institutions, and a
member of the steering committee of PRMIA Singapore chapter.
xiii


Foreword
F
inancial markets are all about risk management. Banking and capital
markets activities throw up all manner of risk exposures as a matter of
course, and these need to be managed accordingly such that stakeholders are
comfortable. “Market risk” traditionally referred to risks arising from a
change in market factors, and when we say “risk” we mean risk to the profit
and loss account or to revenues. These market factors might be interest
rates, foreign currency rates, customer default rates, and so on. Managers of
a financial institution should expect to have some idea of the extent of their
risk to these dynamic factors at any one time, so that they can undertake
management action to mitigate or minimize the risk exposure. This is
Finance 101 and is as old as commerce and money itself.
Measuring market exposure has always been a combination of certain
methods that might be called scientific and others that might be described as
application of learned judgment. I have always been a fan of “modified
duration” for interest rate risk and I still recommend it. Of course it has its
flaws, which estimation method doesn’t? But when Value-at-Risk (VaR) was
first presented to the world it appeared to promise to make the risk manager’s
job easier, because it seemed to offer a more accurate estimate of risk exposure
at any time. And the latter was all it ever was, or claimed to be: an estimation
of risk exposure. A measure of risk, no better and no worse than the com-
petence of the person who was making use of the calculated numbers.
Unfortunately, in some quarters VaR was viewed as being somehow a
substitute for “risk management” itself. It didn’t help that the assumptions
underpinning every single methodology for calculating VaR were never
widely understood, at least not at the senior executive level, which made
explaining losses that exceeded the VaR estimate even more difficult than
usual. In 2012 JPMorgan announced losses of up to $9 billion in a portfolio
of corporate credits that were managed by its London-based chief invest-
ment office. Depending on which media report one follows, the VaR number
reported for the bank as a whole the day before the announcement was
alleged to be between 1 percent and 10 percent of this value. Is there any
point in going to the trouble of calculating this estimate if at any time it can
be demonstrated to be so completely off the mark?
The short answer is yes and no. VaR is a tool, nothing more nor less,
and like all tools must be used within its limitations. One could argue that a
xv

bank the size and complexity of JPMorgan is going to struggle to ever get a
meaningful estimate of its true risk exposure under all marker conditions,
but therein lies the value and the worthlessness of any statistical measure like
VaR: it is reasonable for some, indeed most, of the time but when it does get
out of kilter with market movements the difference could be huge (an order
of magnitude of up to 100 times out, if some recent headlines are to be
believed). It reminds one of the apocryphal story of the statistician who
drowned in a lake that had an “average” depth of six inches.
The circle is complete of course. It was JPMorgan that gave the world
VaR back in 1994 (one or two other banks, including CSFB, were applying
a similar sort of methodology around the same time), and eighteen years
later the bank saw for itself just how inaccurate it could be. Does that mean
we should dispense with VaR and go back to what we had before, or look to
devise some other method?
Again, yes and no. The key accompanying text for any statistical mea-
surement, VaR most definitely included, has always been “use with care, and
only within limitations.” That means, by all means, continue with your
chosen VaR methodology for now, but perhaps be aware that an actual loss
under conditions that the model is not picking up could well be many times
beyond your VaR estimate. In other words, bring in your interest rate risk and
credit risk exposure limits because the true picture is going to be in excess of
what you think it is. That is true for whichever firm one is working at.
But that isn’t all. Knowing VaR’s limitations means also seeking to
develop an understanding of what it doesn’t cover. And this is where Max
Wong’s very worthwhile and interesting book comes in. In the Basel III era of
“macroprudential regulation,” Mr Wong applies a similar logic for VaR and
presents a new concept of Bubble VaR, which is countercyclical in approach
and would be pertinent to a bank running complex exposures across multiple
markets and asset classes. But I also rate highly the first half of the book,
which gives an accessible description of the vanilla VaR concept and its
variations before launching into its limitations and how Bubble VaR is a
means of extending the concept’s usefulness. The content herein is technical
and arcane by necessity, but remains firmly in the domain of “know your
risk,” which is something every senior banker should be obsessed with.
This book is a fantastic addition to the finance literature, written by that
rare beast in financial markets, management consulting, or academia: a person
delivering something of practical value for the practitioner and that advances
our understanding and appreciation of finance as a discipline. Finance 201, so
to speak, for everyone with an interest in financial risk management.
Professor Moorad Choudhry
Department of Mathematical Sciences
Brunel University
16th December 2012
xvi
FOREWORD

Preface
T
his is a story of the illusion of risk measurement. Financial risk man-
agement is in a state of confusion. The 2008 credit crisis has wreaked
havoc on the Basel pillars of supervision by highlighting all the cracks in the
current regulatory framework that had allowed the credit crisis to fester, and
ultimately leading to the greatest crisis since the Great Depression. Policy
responses were swift—UK’s Financial Services Authority (FSA) published
the Turner Review, which calls for a revamp of many aspects of banking
regulation, and the Bank of International Settlements (BIS) speedily passed
a Revision to its Basel II, while the Obama administration called for a
reregulation of the financial industry reversing the Greenspan legacy of
deregulation. These initiatives eventually evolved into the Basel III frame-
work and Dodd-Frank Act respectively.
The value-at-risk risk measure, VaR, a central ideology for risk man-
agement, was found to be wholly inadequate during the crisis. Critically, this
riskometer is used as the basis for regulatory capital—the safety buffer
money set aside by banks to protect against financial calamities. The
foundation of risk measurement is now questionable.
The first half of this book develops the VaR riskometer with emphasis
on its traditionally known weaknesses, and talks about current advances in
risk research. The underlying theme throughout the book is that VaR is a
faulty device during turbulent times, and by its mathematical sophistication
it misled risk controllers into an illusion of safety. The author traces the
fundamental flaw of VaR to its statistical assumptions—of normality, i.i.d.,
and stationarity—the Gang of Three.
These primitive assumptions are very pervasive in the frequentist statistics
philosophy where probability is viewed as an objective notion and can be
measured by sampling. A different school of thought, the Bayesian school,
argues for subjective probability and has developed an entire mathematical
framework to incorporate the observer’s opinion into the measurement (but
this is subject matter for another publication). We argue that the frequentist’s
strict mathematical sense often acts as a blinder that restricts the way we view
and model the real world. In particular, two “newly” uncovered market
phenomena—extremistan and procyclicality—cannot be engaged using the
xvii

frequentist mindset. There were already a few other well-known market
anomalies that tripped the VaR riskometer during the 2008 crisis. All these
will be detailed later.
In Part Four of the book, the author proposes a new risk metric called
bubble VaR (buVaR), which does not invoke any of the said assumptions.
BuVaR is not really a precise measurement of risk; in fact, it presumes that
extreme loss events are unknowable (extremistan) and moves on to the more
pressing problem—how do we build an effective buffer for regulatory cap-
ital that is countercyclical, and that safeguards against extreme events.
This book is an appeal (as is this preface) to the reader to consider a new
paradigm of viewing risk—that one need not measure risk (with precision) to
protect against it. By being obsessively focused on measuring risk, the risk
controller may be fooled by the many pitfalls of statistics and randomness. This
could lead to a false sense of security and control over events that are highly
unpredictable. It is ultimately a call for good judgment and pragmatism.
Since this book was first published in 2011, the financial industry has
experienced a sea change in Basel regulation and new risk modeling require-
ments under the Basel III capital framework. There are also exciting develop-
ments in the modeling of risk at the research frontier. This revised edition is an
update to include some of these topics, even though the primary objective
remains to encourage an alternate paradigm of looking at market risk.
AUDIENCE
This book is intended to reach out to the top management of banks (CEOs
and CROs), to regulators, to policy makers, and to risk practitioners—not
all of whom may be as quantitatively inclined as the specialized risk pro-
fessional. But they are the very influencers of the coming financial reregu-
lation drama. We are living in epic times, and ideas help shape the world for
better (or for worse). It is hoped that the ideas in this book can open up new
and constructive research into countercyclical measures of risk.
Withthistargetaudienceinmind,thisbookiswritteninplainEnglishwithas
few Greek letters as possible; the focus is on concepts (and illustrations) rather
than mathematics. Because it is narrowly focused on the topic, it can be self-
contained. No prior knowledge of risk management is required; preuniversity-
level algebra and some basic financial product knowledge are assumed.
OVERVIEW OF THE CONTENTS
In order to internalize the idea of risk, this book takes the reader through the
developmental path of VaR starting from its mathematical foundation to its
xviii
PREFACE

advanced forms. In this journey, fault lines and weaknesses of this meth-
odology are uncovered and discussed. This will set the stage for the new
approach, buVaR.
Chapter 2 goes into the foundational mathematics of VaR with
emphasis on intuition and concepts rather than mathematical rigor.
Chapter 3 introduces the basic building blocks used in VaR. The con-
ventional VaR systems are then formalized in Chapter 4. At the end of the
chapter, readers will be able to calculate VaR on a simple spreadsheet and
experiment with the various nuances of VaR.
Chapter 5 discusses some advanced VaR models developed in academia
in the last decade. They are interesting and promising, and are selected to
give the reader a flavor of current risk research.
Chapter 6 deals with the tools used by banks for VaR reporting. It also
contains a prelude to the Basel Rules used to compute minimum capital.
Chapter 7 explores the phenomenology of risks. In particular, it details
the inherent weaknesses of VaR and the dangers of extreme risks not cap-
tured by VaR.
Chapter 8 covers the statistical tests used to measure the goodness of a
VaR model.
Chapter 9 discusses the weaknesses of VaR, which are not of a theo-
retical nature. These are practical problems commonly encountered in VaR
implementation.
Since this book deals primarily with market risk, Chapter 10 is a minor
digression devoted to other (nonmarket) risk classes. A broad understanding
is necessary for the reader to appreciate the academic quest (and the
industry’s ambition) for a unified risk framework where all risks are mod-
eled under one umbrella.
Chapter 11 gives a brief history of the Basel capital framework. It then
proceeds to summarize the key regulatory reforms (Basel III) that were
introduced from 2009 to 2010.
Chapter 12 discusses developments in measuring and detecting systemic
risks. These are recent research initiatives by regulators who are concerned
about global crisis contagion. Network models are introduced with as little
math as possible. The aim is to give the reader a foretaste of this important
direction of development.
The final part of this book, Part Four—spanning five chapters in total—
introduces various topics of bubble-VaR. Chapter 13 lays the conceptual
framework for buVaR, formalized for market risk.
Chapter 14 shows that with a slight modification, the buVaR idea can
be expanded to cover credit risks, including default risk.
Chapter 15 contains the results of various empirical tests of the
effectiveness of buVaR.
Preface
xix

Chapter 16 is a concluding chapter that covers miscellaneous topics for
buVaR. In particular, it summarizes how buVaR is able to meet the ideals
proposed by the Turner Review.
Lastly, Chapter 17 lists suggestions for future research. It is a wish list
for buVaR which is beyond the scope of this volume.
ADDITIONAL MATERIALS
Throughout this book, ideas are also formulated in the syntax of Excel
functions so that the reader can easily implement examples in a spreadsheet.
Exercises with important case studies and examples are included as Excel
spreadsheets at the end of each chapter and can be downloaded from the
companion website: www.wiley.com/go/bubblevalueatrisk.
Excel is an excellent learning platform for the risk apprentice. Monte
Carlo simulations are used frequently to illustrate and experiment with key
ideas, and, where unavoidable, VBA functions are used. The codes are
written with pedagogy (not efficiency) in mind.
xx
PREFACE

Acknowledgments
T
his book has benefited from the valuable comments ofvarious practitioners
and academics. I am most grateful to Michael Dutch for his generous
proofreading of the manuscript; and to John Chin, Shen Qinghua, Jayaradha
Shanker, and Moorad Choudhry for their useful comments. The book was
further enriched byreviewsandsuggestionsfrom PaulEmbrechts from ETHZ.
The production of the book involved many excellent individuals. I
thank Lim Tai Wei for grammatical edit work, Sylvia Low for web design
and the cover design team in Beijing: Michael Wong, Kenny Chai, Liu
DeBin, and Xiao Bin. I am grateful to Nick Wallwork and the staff at Wiley
for the production of the revised edition.
I am grateful to my wife, Sylvia Chen, for her patience and for taking
care of the children—Werner and Arwen—during this project.
xxi


PART
One
Background


CHAPTER 1
Introduction
T
he 2008 global credit crisis is by far the largest boom-bust cycle since the
Great Depression (1929). Asset bubbles and manias have been around
since the first recorded tulip mania in 1637 and in recent decades have
become such a regularity that they are even expected as often as once every
10 years (1987, 1997, 2007). Asset bubbles are in reality more insidious
than most people realize for it is not the massive loss of wealth that it brings
(for which investor has not entertained the possibility of financial ruin) but
because it widens the social wealth gap; it impoverishes the poor. The 2008
crisis highlighted this poignantly—in the run-up to the U.S. housing and
credit bubble, the main beneficiaries were bankers (who sold complex
derivatives on mortgages) and their cohorts. At the same time, a related
commodity bubble temporarily caused a food and energy crisis in some
parts of the developing world, notably Indonesia, the fourth-most-populous
nation in the world and an OPEC member (until 2008). When the bubble
burst, $10 trillion dollars of U.S. public money was used to bail out failing
banks and to take over toxic derivatives created by banks. On their way out,
CEOs and traders of affected banks were given million-dollar contractual
bonuses, even as the main economy lost a few million jobs. Just as in 1929,
blue-collar workers bore the brunt of the economic downturn in the form of
unemployment in the United States.
The ensuing zero interest rate policy and quantitative easing (printing of
dollars by the Fed) induced yet other bubbles—commodity prices are rising
to alarming levels and asset bubbles are building up all over Asia, as
investors chase non-U.S. dollar assets. We see home prices skyrocketing well
beyond the reach of the average person in major cities. The wealthy are
again speculating in homes, this time in East Asia. In many countries, huge
public spending on infrastructure projects that is meant to support the
headline GDP caused a substantial transfer of public wealth to property
developers and cohorts. The lower income and underprivileged are once
again left behind in the tide of inflation and growth.
3

The danger of an even larger crisis now looms. The U.S. dollar and trea-
suries are losing credibility as reserve currencies because of rising public debt.
This means that flight-to-quality, which has in the past played the role of a
pressure outlet for hot money during a crisis, is no longer an appealing option.
If there is a lesson from the 2008 crisis, it is that asset bubbles have to be
reined in at all costs. It is not just John Keynes’ “animal spirits” at work
here—the herd tipping the supply-demand imbalance—but the spirit of
“mammon”—unfettered greed. There is something fundamentally dys-
functional about the way financial institutions are incentivized and regu-
lated. Thus, a global regulatory reform is underway, led by the United
Kingdom, the European Union (EU), and the United States, with target
deadlines of 2012 and beyond. Our narrow escape from total financial
meltdown has highlighted the criticality of systemic risks in an inter-
connected world; we can no longer think in isolated silos when solving
problems in the banking system. The coming reregulation must be holistic
and concerted.
One major aspect of the reform is in the way risk is measured and
controlled. The great irony is that our progress in risk management has led
to a new risk: the risk of risk assessment. What if we are wrong (unknow-
ingly) about our measurement? The crisis is a rude wake-up call for reg-
ulators and bankers to reexamine our basic understanding of what risk is
and how effective our regulatory safeguards are.
We start our journey with a review of how our current tools for mea-
suring financial market risks were evolved. In this chapter, we will also give
a prelude to two important concepts that grew out of crisis response—
extremistan and procyclicality. These will likely become the next buzz words
in the unfolding regulatory reform drama. The final section offers bubble
VaR, a new tool researched by the author, which regulators can explore to
strengthen the safeguards against future financial crises.
1.1 THE EVOLUTION OF RISKOMETER
Necessity is the mother of invention.
—Plato, Greek philosopher, 427–347 BC
Ask a retail investor what the risks of his investment portfolio are, and he
will say he owns USD30,000 in stocks and USD70,000 in bonds, and he is
diversified and therefore safe. A lay investor thinks in notional terms, but
this can be misleading since two bonds of different duration have very dif-
ferent risks for the same notional exposure. This is because of the convexity
4
BACKGROUND

behavior peculiar to bonds. The idea of duration, a better risk measure for
bonds, was known to bankers as early as 1938.
In the equities world, two different stocks of the same notional amount
can also give very different risk. Hence, the idea of using volatility as a risk
measure was introduced by Harry Markowitz (1952). His mean-variance
method not only canonized standard deviation as a risk measure but also
introduced correlation and diversification within a unified framework.
Modern portfolio theory was born. In 1963, William Sharpe introduced the
single factor beta model. Now investors can compare the riskiness of indi-
vidual stocks in units of beta relative to the overall market index.
The advent of options introduced yet another dimension of risk, which
notional alone fails to quantify, that of nonlinearity. The Black-Scholes
option pricing model (1973) introduced the so-called Greeks, a measurement
of sensitivity to market parameters that influence a product’s pricing, an idea
that has gone beyond just option instruments. Risk managers now measure
sensitivities to various parameters for every conceivable product and impose
Greek limits on trading desks. The use of limits to control risk taking gained
acceptance in the mid-1980s but sensitivity has one blind spot—it is a local
risk measure. Consider, for example, the delta of an option (i.e., option price
sensitivity to a 1% change in spot) that has a strike near spot price. For a
10% adverse move in spot, the real loss incurred by the option is a lot larger
than what is estimated by delta (i.e., 10 times delta). This missing risk is due
to nonlinearity, a behavior peculiar to all option products. The problem is
more severe for options with complex (or exotic) features.
The impasse was solved from the early 1990s by the use of stress tests.
Here, the risk manager makes up (literally) a set of likely bad scenarios—say
a 20% drop in stocks and a 1% rise in bond yield—and computes the actual
loss of this scenario. While this full revaluation approach accounts for loss
due to nonlinearity, stress testing falls short of being the ideal riskometer—it
is too subjective and it is a static risk measure—the result is not responsive to
day-to-day market movements.
Then in 1994, JP Morgan came out with RiskMetrics, a methodology
that promotes the use of value-at-risk (VaR) as the industry standard for
measuring market risk.1 VaR is a user-determined loss quantile of a port-
folio’s return distribution. For example, if a bank chooses to use a 99%-VaR,
this result represents the minimum loss a bank is expected to incur with a 1%
probability. By introducing a rolling window of say 250 days to collect the
distributional data, VaR becomes a dynamic risk measure that changes with
new market conditions.
In 1995, the Basel Committee of Banking Supervision enshrined VaR as
the de facto riskometer for its Internal Model approach for market risk.
Introduction
5

Under Basel II, all banks are expected to come up with their implementation
of VaR (internal) models for computing minimum capital.
1.2 TALEB’S EXTREMISTAN
The idea of extremistan was made popular by Nassim Taleb, author of
the New York Times bestseller The Black Swan.2 The book narrates the
probabilistic nature of catastrophic events and warns of the common misuse
of statistics in understanding extreme events of low probability. It is uncanny
that the book came out a few months before the subprime fiasco that marked
the onset of the credit crisis.
The central idea is the distinction between two classes of probability
structures—mediocristan and extremistan. Mediocristan deals with rare events
that are thin tailed from a statistical distribution perspective. Large deviations
can occur, but they are inconsequential. Take for example the chance occur-
rence of a 10-foot bird, which has little impact on the ecosystem as a whole.
Such distributions are well described by the (tail of) bell-shaped Gaussian
statistics or modeled by random walk processes. On the other hand, extre-
mistan events are fat tailed—low probability, high impact events. Past
occurrences offer no guidance on the magnitude of future occurrences. This is a
downer for risk management. The effect of the outcome is literally immea-
surable. Some examples are World Wars, flu pandemics, Ponzi schemes,
wealth creation of the super rich, a breakthrough invention, and so on.
A philosophical digression—mediocristan and extremistan are closely
associated with scalability. In mediocristan, the outlier is not scalable—its
influence is limited by physical, biological, or environmental constraints. For
example, our lone 10-foot bird cannot invade the whole ecosystem. Extre-
mistan, in contrast, lies in the domain of scalability. For example, capitalism
and free enterprise, if unrestrained by regulation, allow for limitless upside
for the lucky few able to leverage off other people’s money (or time). Because
of scalability, financial markets are extremistan—rare events of immeasur-
able devastation or Black Swans occur more often than predicted by thin-
tailed distributions.
Another reason why financial markets are more extremistic than nature
is because they involve thinking participants. The inability of science to
quantify its cause and effect has pushed the study of this phenomenon to the
domain of behavioral finance, with expressions such as herd mentality,
animal spirits, madness of the crowd, reflexivity, endogeneity of risk, and
positive feedback loops.
VaR is a victim of extremistan. Taleb, a strong critic of VaR, sees this
method as a potentially dangerous malpractice.3 The main problem is that
6
BACKGROUND

financial modelers are in love with Gaussian statistics in which simplistic
assumptions make models more tractable. This allows risk modelers to
quantify (or estimate) with a high degree of precision events that are by
nature immeasurable (extremistan). That can lead to a false sense of security
in risk management. Taleb’s extremistan, vindicated by the 2008 crisis, has
dealt a serious blow to the pro-VaR camp.
This book introduces, bubble VaR (buVar), an extension of the VaR
idea that denounces the common basic statistical assumptions (such as sta-
tionarity). It is fair to say that the only assumption made is that one cannot
measure the true number. It is hypothetical, and it is a moving target. In fact,
we need not measure the true expected loss in order to invent an effective
safeguard. This is what buVaR attempts to achieve.
1.3 THE TURNER PROCYCLICALITY
The idea of procyclicality is not new. In a consultative paper, Danielsson and
colleagues (2001)4 first discussed procyclicality risk in the context of using
credit ratings as input to regulatory capital computation as required under the
Internal Rating Based (IRB) approach. Ratings tend to improve during an
upturn of a business cycle and deteriorate during a downturn. If the minimum
capital requirement is linked to ratings—requiring less capital when ratings
are good—banks are encouraged to lend during an upturn and cut back loans
during a downturn. Thus, the business cycle is self-reinforced artificially by
policy. This has damaging effects during a downturn as margin and collateral
are called back from other banks to meet higher regulatory minimum capital.
This danger is also highlighted in the now-famous Turner Review,5
named after Sir Adair Turner, the new Financial Service Authority (FSA)
chief, who was tasked to reform the financial regulatory regime. The review
has gone furthest to raise public awareness of hard-wired procyclicality as a
key risk. It also correctly suggested that procyclicality is an inherent defi-
ciency in the VaR measure as well. Plot any popular measure of value at risk
(VaR) throughout a business cycle, and you will notice that VaR is low when
markets are rallying and spikes up during a crisis.
This is similar to the leverage effect observed in the markets—rallies in
stock indices are accompanied by low volatility, and sell downs are
accompanied by high volatility. From the reasoning of behavioral science,
fear is a stronger sentiment than greed.
However, this is where the analogy ends. The leverage effect deals with
the way prices behave, whereas VaR is a measurement device (which can be
corrected). The Turner Review says our VaR riskometer is faulty—it con-
tains hardwired procyclicality. Compounding the problem is that trading
Introduction
7

positions are recorded using mark-to-market accounting. Hence, in a raging
bull market, profits are realized and converted into additional capital for
even more investment just as (VaR-based) regulatory capital requirements
are reduced. It is easy to see that this is a recipe for disaster—the rules of the
game encourage banks to chase the bubble.
To mitigate the risk of procyclicality, the Turner Review calls for a longer
observation period—the so-called through-the-cycle rather than point-in-time
(what VaR is doing currently) measures of risk—as well as more stress tests.
Some critics6 argue that the correct solution is not simply to make the capital
charge larger or more penal for banks, but also more timely. It is unavoidable
that VaR based on short histories is procyclical, precisely because it gives a
timely forecast. Efforts to dampen procyclicality by using a longer history will
worsen the forecast; it is no longer market sensitive and timely.
As we shall see, buVaR addresses the procyclicality problem by being
countercyclical in design, without sacrificing timeliness.
1.4 THE COMMON SENSE OF BUBBLE VALUE-AT-RISK
(BuVaR)
The idea of buVar came from a simple observation: when markets crash,
they fall downwards, rather than upwards (?). Yes, this basic asymmetry is
overlooked by present-day measures of risks. Let’s think along.
Even in the credit crisis in 2008 when credit spreads crashed upwards,
that event came after a period of unsustainable credit-spread compression.
So, to be more precise, a market crash happens only after an unsustainable
price rally or decline—often called a bubble—and in the opposite direction
to the prevailing trend.
If this is a universal truth, and there is overwhelming evidence that it is,
then does it not make sense that market risk at point C is higher than at
points A, B, and D? (Figure 1.1). We know this intuitively and emotionally
as well; suppose you do not have any trading views, then a purchase (or sale)
of stocks at which level would make you lose sleep? Because while the
bubbles are obvious, when they will burst is not. Hence the trader’s adage
“the markets climb the wall of worry.”7
Yet the conventional measure of risk, VaR, does not account for this
obvious asymmetry. Table 1.1 compares the 97.5% VaR8 for the Dow Jones
index at various points. Notice that A, B, and C have about the same risks.
Only after the crash (at D) does VaR register any meaningful increase in
risks. It’s like a tsunami warning system that issues alerts after the waves
have reached landfall! It seems VaR is reactive rather than preventive. What
happened?
8
BACKGROUND

The same situation can also be observed for Brent crude oil prices
(Figure 1.2 and Table 1.2). Is VaR just a peacetime tool? The root cause can
be traced back to model assumptions.
VaR and most risk models used by banks assume returns are indepen-
dent and identically distributed (or i.i.d.), meaning that each return event is
not affected by past returns, yet they are identical (in distribution)! As a
result, the return time series is stationary. Here stationary means that if you
take, say, a 250-day rolling window of daily returns, its distribution looks
the same in terms of behavior whether you observe the rolling window
today, a week ago, or at any date. In other words, the distribution is time
invariant. Let’s look at one such time series, the one-day returns of the Dow
Jones index (Figure 1.3). Compared to Figure 1.1, the trend has been
–
2,000
4,000
6,000
8,000
10,000
12,000
14,000
16,000
Jan-82
Jan-84
Jan-86
Jan-88
Jan-90
Jan-92
Jan-94
Jan-96
Jan-98
Jan-00
Jan-02
Jan-04
Jan-06
Jan-08
A
B
C
D
FIGURE 1.1
Dow Jones Index
TABLE 1.1
97.5% Value-at-Risk for Dow Jones Index Using Historical Simulation
Point in
Time
97.5% VAR
(weekly)
VAR (mean
adjusted)
Maximum
Weekly
Drawdown
Max Weekly
Drawdown (St.
Dev)
A
2.3%
2.5%
3.4%
2.7
B
3.3%
3.4%
3.6%
2.1
C
3.2%
3.5%
4.3%
2.5
D
6.4%
5.8%
20.0%
6.9
Introduction
9

TABLE 1.2
97.5% Value at Risk for Crude Oil Price
Point in
Time
97.5% VAR
(weekly)
VAR (mean
adjusted)
Maximum
Weekly
Drawdown
Max Weekly
Drawdown (St.
Dev)
A
8.2%
8.5%
25.4%
6.0
B
7.1%
7.3%
9.0%
2.5
C
6.4%
7.0%
9.0%
2.5
D
13.7%
13.6%
29.7%
4.4
0
20
40
60
80
100
120
140
160
Jan-00
Jan-01
Jan-02
Jan-03
Jan-04
Jan-05
Jan-06
Jan-07
Jan-08
Jan-09
US$
A
B
C
D
FIGURE 1.2
Crude Oil Price (in U.S. dollars)
–15%
–10%
–5%
0%
5%
10%
15%
Jan-82
Jan-84
Jan-86
Jan-88
Jan-90
Jan-92
Jan-94
Jan-96
Jan-98
Jan-00
Jan-02
Jan-04
Jan-06
Jan-08
A
B
C
D
FIGURE 1.3
Daily Price Change of Dow Jones Index
10
BACKGROUND

removed completely (detrended by taking the daily change); you are left with
wiggles that look almost identical anywhere along the time scale (say at A, B,
or C) and almost symmetrical about zero. At D, risk is higher only because it
wiggles more.
VaR models are built on statistics of only these detrended wiggles.
Information on price levels even if they contain telltale signs—such as
the formation of bubbles, a price run-up, widening of spreads—are ignored
(they do not meet the requirement of i.i.d.). VaR is truly nothing more
than the science of wiggles. The i.i.d. assumption lends itself to a lot of
mathematical tractability. It gives modelers a high degree of precision in
their predictions.9 Unfortunately precision does not equate to accuracy.
To see the difference between precision and accuracy, look at the bull’s-
eye diagrams in Figure 1.4. The right-side diagram illustrates the shotgun
approach to getting the correct answer—accurate but not precise. Accu-
racy is the degree of authenticity while precision is the degree of
reproducibility.
In risk measurement, Keynes’s dictum is spot on: “It is clearly better to
be approximately right, than to be precisely wrong.” The gross underesti-
mation of risk by VaR during the credit crisis, a Black Swan event, is a
painful objective lesson for banks and regulators. The events of 2008 chal-
lenge the very foundation of VaR and are a wake-up call to consider
exploring beyond the restrictive, albeit convenient, assumption of i.i.d.
BuVaR is one such initiative.
The Turner Review calls for the creation of countercyclical capital
buffers on a global scale. It will be ideal if we have a VaR system that
PRECISE BUT NOT ACCURATE
ACCURATE BUT NOT PRECISE
FIGURE 1.4
Precision versus Accuracy
Introduction
11

automatically penalizes the bank—by inflating—when positions are long
during a bubble rally, and continues to penalize the bank during a crash.
Then when the crash is over and the market overshoots on the downside,
VaR penalizes the short side positions instead. As we shall learn, buVaR
does this—it is an asymmetrical, preventive, and countercyclical risk mea-
sure that discourages position taking in the direction of a bubble.
Figure 1.5 is a preview of buVaR versus VaR10 for the Dow Jones index
during the recent credit crisis. VaR is perpetually late during a crisis and
does not differentiate between long and short positions. BuVaR peaks
ahead of the crash (is countercyclical) and is always larger than VaR, to
buffer against the risk of a crash on one side. It recognizes that the crash
risks faced by long and short positions are unequal. Used for capital pur-
poses, it will penalize positions that are chasing an asset bubble more than
contrarian positions.
If implemented on a global scale, buVaR would have the effect of reg-
ulating and dampening the market cycle. Perhaps then, this new framework
echoes the venerable philosophy of the FED:
It’s the job of the FED to take away the punch bowl just as the
party gets going.
—William McChesney Martin Jr., FED Chairman 1951–1970
–
2,000 
4,000 
6,000 
8,000 
10,000 
12,000 
14,000 
–30%
–25%
–20%
–15%
–10%
–5%
0%
5%
10%
Sep-04 Mar-05 Sep-05 Mar-06 Sep-06 Mar-07 Sep-07 Mar-08 Sep-08
Index level
% return
VaR (long)
VaR (short)
buVaR (long)
buVaR (short)
Dow Jones index
FIGURE 1.5
BuVaR and VaR Comparison
12
BACKGROUND

NOTES
1. There are claims that some groups may have experimented with risk measures
similar to VaR as early as 1991.
2. Taleb, 2007, The Black Swan: The Impact of the Highly Improbable.
3. See the discussion “Against Value-at-Risk: Nassim Taleb Replies to Phillip
Jorion,” Taleb, 1997.
4. Danielsson et al., “An Academic Response to Basel II,” Special Paper 130,
ESRC Research Centre, 2001.
5. Financial Service Authority, 2009, The Turner Review—A Regulatory Response
to the Global Banking Crisis.
6. RiskMetrics Group, 2009, “VaR Is from Mars, Capital Is from Venus.”
7. This is supported by empirical evidence that put-call ratios tend to rise as stock
market bubbles peak. This is the ratio of premium between equally out-of-
money puts and calls, and is a well-studied indicator of fears of a crash.
8. The VaR is computed using a 250-day observation period, and expressed as a
percentage loss of the index. VaR should always be understood as a loss;
sometimes a negative sign is used to denote the loss.
9. By assuming i.i.d., the return time series becomes stationary. This allows the
Law of Large Numbers to apply. This law states that, as more data is collected,
the sample mean will converge to a stable expected value. This gives the stat-
istician the ability to predict (perform estimation) with a stated, often high, level
of precision.
10. The VaR is computed by the RiskMetrics method using exponentially decaying
weights.
Introduction
13


CHAPTER 2
Essential Mathematics
T
his chapter provides the statistical concepts essential for the under-
standing of risk management. There are many good textbooks on the
topic, see Carol Alexander (2008). Here, we have chosen to adopt a selective
approach. Our goal is to provide adequate math background to understand
the rest of the book. It is fair to say that if you do not find it here, it is not
needed later. As mentioned in the preface, this book tells a story. In fact, the
math here is part of the plot. Therefore, we will include philosophy or
principles of statistical thinking and other pertinent topics that will con-
tribute to the development of the story. And we will not sidetrack the reader
with unneeded theorems and lemmas.
2.1 FREQUENTIST STATISTICS
Two schoolsofthought haveemergedfromthe history of statistics—frequentist
and Bayesian schools of thought. Bayesians and frequentists hold very dif-
ferent philosophical views on what defines probability. From a frequentist
perspective, probability is objective and can be inferred from the frequency of
observation in a large number of trials. All parameters and unknowns that
characterize an assumed distribution or regression relationship can be backed
out from the sample data. Frequentists will base their interpretations on a
limited sample; as we shall see, there is a limit to how much data they can
collect without running into other practical difficulties. Frequentists will
assume the true value of their estimate lies within the confidence interval that
they set (typically at 95%). To qualify their estimate, they will perform
hypothesis testing that will (or will not) reject their estimate, in which case they
will assume the estimate as false (or true).
Bayesians, on the other hand, interpret the concept of probability as “a
measure of a state of knowledge or personal belief” that can be updated on
arrival of more information (i.e., incorporates learning). Bayesians embrace
15

the universality of imperfect knowledge. Hence probability is subjective;
beliefs and expert judgment are permissible inputs to the model and are also
expressed in terms of probability distributions. As mentioned earlier, a fre-
quentist hypothesis (or estimate) is either true or false, but in Bayesian sta-
tistics the hypothesis is also assigned a probability.
Value at risk (VaR) falls under the domain of frequentist statistics—
inferences are backed out from data alone. The risk manager, by legacy of
industry development, is a frequentist.1
A random variable or stochastic variable (often just called variable) is a
variable that has an uncertain value in the future. Contrast this to a deter-
ministic variable in physics; for example, the future position of a planet can
be determined (calculated) to an exact value using Newton’s laws. But in
financial markets, the price of a stock tomorrow is unknown and can only be
estimated using statistics.
Let X be a random variable. The observation of X (data point) obtained
by the act of sampling is denoted with a lower case letter xi as a convention,
where the subscript i ¼ 1,2, . . . , is a running index representing the
number of observations. In general, X can be anything—price sequences,
returns, heights of a group of people, a sample of dice tosses, income samples
of a population, and so on. In finance, variables are usually price (levels) or
returns (changes in levels). We shall discuss the various types of returns later
and their subtle differences. Unless mentioned otherwise, we shall talk about
returns as daily percentage change in prices. In VaR, the data set we will be
working with is primarily distributions of sample returns and distributions
of profit and loss (PL).
Figure 2.1 is a plot of the frequency distribution (or histogram) of S&P
500 index returns using 500 days data (Jul 2007 to Jun 2009). One can think
of this as a probability distribution of events—each day’s return being a
single event. So as we obtain more and more data (trials), we get closer to the
correct estimate of the “true” distribution.
We posit that this distribution contains all available information about
risks of a particular market and we can use this distribution for forecasting.
In so doing, we have implicitly assumed that the past is an accurate guide to
future risks, at least for the next immediate time step. This is a necessary
(though arguable) assumption; otherwise without an intelligent structure,
forecasting would be no different from fortune telling.
In risk management, we want to estimate four properties of the return
distribution—the so-called first four moments—mean, variance, skewness,
and kurtosis. To be sure, higher moments exist mathematically, but they are
not intuitive and hence of lesser interest.
The mean of a random variable X is also called the expectation or
expected value, written μ ¼ E(X). The mean or average of a sample x1, . . . ,
16
BACKGROUND

xn is just the sum of all the data divided by the number of observations n. It is
denoted by x or ^μ.
x ¼ 1
n
X
n
i¼1
xi
ð2:1Þ
The Excel function is AVERAGE(.). It measures the center location of a
sample. A word on statistical notation—generally, when we consider the
actual parameter in question μ (a theoretical idea), we want to measure this
parameter using an estimator ^μ (a formula). The outcome of this measure-
ment is called an estimate, also denoted ^μ (a value). Note the use of the ^
symbol henceforth.
The kth moment of a sample x1, . . . , xn is defined and estimated as:
1
ðn  1Þ
X
n
i¼1
ðxi  xÞk
ð2:2Þ
The variance or second moment of a sample is defined as the average of the
squared distances to the mean:
^σ2 ¼
1
n  1
X
n
i¼1
ðxi  xÞ2
ð2:3Þ
0%
2%
4%
6%
8%
10%
12%
14%
16%
–9%
–8%
–7%
–6%
–5%
–4%
–3%
–2%
–1%
0%
1%
2%
3%
4%
5%
6%
7%
8%
9%
10%
Frequency
% Return
Normal Distribution
S&P Index return
distribution
Kurtosis + 4.37
Skewness + 0.15
Sigma 2.2%
FIGURE 2.1
S&P 500 Index Frequency Distribution
Essential Mathematics
17

The Excel function is VAR(.). It represents the dispersion from the mean.
The square-root of variance is called the standard deviation or sigma σ. In
risk management, risk is usually defined as uncertainty in returns, and is
measured in terms of sigma. The Excel function is STDEV(.).
The skewness or third moment (divided by ^σ3) measures the degree of
asymmetry about the mean of the sample distribution. A positive (negative)
skew means the distribution slants to the right (left). The Excel function is
SKEW(.).
skewness ¼
1
n  1
X
n
i¼1
ðxi  xÞ3: 1
^σ3
ð2:4Þ
The kurtosis or fourth moment (divided by ^σ4) measures the “peakness”
of the sample distribution and is given by:
kurtosis ¼
1
n  1
X
n
i¼1
ðxi  xÞ4: 1
^σ4
ð2:5Þ
Since the total area under the probability distribution must sum up to a
total probability of 1, a very peakish distribution will naturally have fatter tails.
Such a behavior is called leptokurtic. Its Excel function is KURT(.). A normal
distribution has a kurtosis of 3. For convenience, Excel shifts the KURT(.)
function such that a normal distribution gives an excess kurtosis of 0. We will
follow this convention and simply call it kurtosis for brevity.
Back to Figure 2.1, the S&P distribution is overlaid with a normal
distribution (of the same variance) for comparison. Notice the sharp central
peak above the normal line, and the more frequent than normal observations
in the left and right tails. The sample period (Jul 2007 to Jun 2009) corre-
sponds to the credit crisis—as expected the distribution is fat tailed. Inter-
estingly, the distribution is not symmetric—it is positively skewed! (We shall
see why in Section 7.1.)
2.2 JUST ASSUMPTIONS
i.i.d. and Stationarity
This is a pillar assumption for most statistical modeling. A random sample
(y1, . . . , yn) of size n is independent and identically distributed (or i.i.d.) if
each observation in the sample belongs to the same probability distribution as
all others, and all are mutually independent. Imagine yourself drawing random
18
BACKGROUND

numbers from a distribution. Identical means each draw must come from
the same distribution (it need not even be bell-shaped). Independent means
you must not meddle with each draw, like making the next random draw a
function of the previous draw. For example, a sample of coin tosses is i.i.d.
A time series is a sequence X1, . . . , Xt of random variables indexed by
time. A time series is stationary if the distribution of (X1, . . . , Xt) is iden-
tical to that of (X1þk, . . . , Xtþk) for all t and all positive integer k. In other
words, the distribution is invariant under time shift k. Since it is difficult to
prove empirically that two distributions are identical (in every aspect), in
financial modeling, we content ourselves with just showing that the first two
moments—mean and variance—are invariant under time shift.2 This con-
dition is called weakly stationary (often just called stationary) and is a
common assumption.
A market price series is seldom stationary—trends and periodic com-
ponents make the time series nonstationary. However, if we take the per-
centage change or take the first difference, this price change can be shown to
be often stationary. This process is called detrending (of differencing) a time
series and is a common practice.
Figure 2.2 illustrates a dummy price series and its corresponding return
series. We divide the 200-day period into two 100-day periods, and compute
the first two moments. For the price series, the mean moved from 4,693 (first
half) to 5,109 (second half). Likewise, the standard deviation changed from
50 to 212. Clearly the price series is nonstationary. The return series, on the
other hand, is stationary—its mean and standard deviation remained
4000
–2%
–1%
0%
1%
2%
3%
4%
5%
4200
4400
4600
4800
5000
5200
5400
5600
1
9
17
25
33
41
49
57
65
73
81
89
97
105
113
121
129
137
145
153
161
169
177
185
193
201
Dummy Price
% Return
FIGURE 2.2
Dummy Price and Return Series
Essential Mathematics
19

roughly unchanged at 0% and 0.5% respectively in both periods. Visually a
stationary time series always looks like white noise.
An i.i.d. process will be stationary for finite distributions.3 The benefit
of the stationarity assumption is we can then invoke the Law of Large
Numbers to estimate properties such as mean and variance in a tractable way.
Law of Large Numbers
Expected values can be estimated by sampling. Let X be a random variable
and suppose we want to estimate the expected value of some function g(X),
where the expected value is μg  E(g(X)). We sample for n observations xi of
X where i ¼ 1, . . . , n. The Law of Large Numbers states that if the sample
is i.i.d. then:
lim
n-N
1
n
X
gðxiÞ ¼ μg
ð2:6Þ
For example, in equations (2.3) to (2.5) used to estimate the moments, as
we take larger and larger samples, precision improves, our estimate con-
verges to the (“true”) expected value. We say our estimate is consistent. On
the other hand, if the sample is not i.i.d., one cannot guarantee the estimate
will always converge (it may or may not); the forecast is said to be incon-
sistent. Needless to say, a modeler would strive to derive a consistent theory
as this will mean that its conclusion can (like any good scientific result) be
reproduced by other investigators.
Let’s look at some examples. We have a coin flip (head þ1, tail 1), a
return series of a standard normal N(0,1) process and a return series of an
autoregressive process called AR(1). The first two are known i.i.d. processes;
the AR(1) is not i.i.d. (it depends on the previous random variable) and is
generated using:
AR(1) process:
Xt ¼ k0 þ k1Xt1 þ εt
ð2:7Þ
where k0 and k1 are constants and εt, t¼1, 2, . . . is a sequence of i.i.d.
random variable with zero mean and a finite variance, also known as white
noise. Under certain conditions (i.e., |k1| > 1), the AR(1) process becomes
nonstationary.
Figure 2.3 illustrates the estimation of the expected value of the three
processes using 1,000 simulations. For AR(1), we set k0 ¼ 0, k1 ¼ 0.99. The
coin flip and normal process both converge to zero (the expected value) as n
increases, but the AR(1) does not. See Spreadsheet 2.1.
20
BACKGROUND

Figure 2.4 plots the returnseries for both the normal process and the AR(1)
process. Notice there is some nonstationary pattern in the AR(1) plot, whereas
the normal process shows characteristic white noise.
To ensure stationarity, risk modelers usually detrend the price data and
model changes instead. In contrast, technical analysis (TA) has always
modeled prices. This is a well-accepted technique for speculative trading
since the 1960s after the ticker tape machine became obsolete. Dealing with
non-i.i.d. data does not make TA any less effective. It does, however, mean
that the method is less consistent in a statistical sense. Thus, TA has always
been regarded as unscientific by academia.
In fact, it would seem that the popular and persistent use of TA (such as in
program trading) by the global trading community has made its effectiveness
self-fulfilling and the market returns more persistent (and less i.i.d.). Market
momentum is a known fact. Ignoring it by detrending and assuming returns
are i.i.d. does not make risk measurement more scientific. There is no com-
pelling reason why risk management cannot borrow some of the modelling
techniques of TA such as that pertaining to momentum and cycles. From an
epistemology perspective, such debate can be seen as a tacit choice between
intuitive knowledge (heuristics) and mathematical correctness.
The Quest for Invariance
From an academic standpoint, the first step in time series modelling is to find
a certain aspect of the data set that has a repeatable pattern or is “invariant”
across time in an i.i.d. way. This is mathematically necessary because if the
–0.4
–0.3
–0.2
–0.1
0
0.1
0.2
0.3
1
35
69
103
137
171
205
239
273
307
341
375
409
443
477
511
545
579
613
647
681
715
749
783
817
851
885
919
953
987
AR(1) process
Coin toss
Normal process
FIGURE 2.3
Behavior of Mean Estimates as Number of Trials Increase
Essential Mathematics
21

variable under study is not repeatable, one cannot really say that a statistical
result derived from a past sample is reflective of future occurrences—the
notion of forecasting a number at a future horizon breaks down.
A simple graphical way to test for invariance is to plot a variable Xt with
itself at a lagged time step (Xt1). If the variable is i.i.d. this scatter plot will
resemble a circular cloud. Figure 2.5 shows the scatter plots for the normal
and AR(1) processes seen previously.
Clearly the AR(1) process is not i.i.d.; its cloud is not circular. For
example, if the return of a particular stock, Xt, follows an AR(1) process, it is
incorrect to calculate its moments and VaR using Xt. Instead, one should
estimate the constant parameters in equation (2.7) from data and back out
εt, then compute the moments and VaR from the εt component. The εt is the
random (or stochastic) driver of risk and it being i.i.d. allows us to project
the randomness to the desired forecast horizon. This is the invariant we are
after. The astute reader should refer to Meucci (2011).
Needless to say, a price series is never i.i.d. because of the presence of
trends (even if only small ones) in the series, hence, the need to transform the
price series into returns.
PDF and CDF
In practice, moments are computed using discrete data from an observed
frequency distribution (like the histogram in Figure 2.1). However, it is
intuitive and often necessary to think in terms of an abstract continuous
–25%
–20%
–15%
–10%
–5%
0%
5%
10%
15%
1
36
71
106
141
176
211
246
281
316
351
386
421
456
491
526
561
596
631
666
701
736
771
806
841
876
911
946
981
AR(1) process
Normal process
FIGURE 2.4
Return Time Series for Normal and AR(1) Processes
22
BACKGROUND

distribution. In the continuous world, frequency distribution is replaced by
probability density function (PDF). If f(x) is a PDF of a random variable X,
then the probability that X is between some numbers a and b is the area
under the graph f(x) between a and b. In other words,
Pða < X < bÞ ¼
Zb
a
fðuÞ du
ð2:8Þ
where f(.) is understood to be a function of x here. The probability density
function f(x) has the following intuitive properties:
fðxÞ ≥0; for all x
ð2:9Þ
ZN
N
fðuÞ du ¼ 1
ð2:10Þ
The cumulative distribution function (CDF) is defined as:
FðxÞ ¼
Zx
N
fðuÞ du
ð2:11Þ
10
AR(1) is not i.i.d.
Gaussian is i.i.d.
6
8
2
4
22
0
0
28
26
24
4
2
3
1
21
23
24
24 23 22 21
0
1
2
3
4
210
25
0
5
10
22
FIGURE 2.5
Scatter Plot of Xt versus Xt1 for a Gaussian Process and an AR(1)
Process
Essential Mathematics
23

It is the probability of observing the variable having values at or below
x, written F(x) ¼ Pr[X  x]. As shown in Figure 2.6, F(x) is just the area
under the graph f(x) at a particular value x.
Normal Distribution
The most important continuous distribution is the normal distribution also
known as Gaussian distribution. Among many bell-shaped distributions,
this famous one describes amazingly well the physical characteristics of
natural phenomena such as the biological growth of plants and animals, the
so-called Brownian motion of gas, the outcome of casino games, and so on.
It seems logical to assume that a distribution that describes science so
accurately should also be applicable in the human sphere of trading.
The normal distribution can be described fully by just two parameters—
its mean μ and variance σ2. Its PDF is given by:
fðxÞ ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2πσ2
p
exp ½ 1
2σ2 ðx  μÞ2
ð2:12Þ
Probability density function (PDF)
0.0
0.5
1.0
25
24
23
22
21
0
1
2
3
4
5
Cumulative distribution function (CDF)
0
f(x)
F(x)
FIGURE 2.6
Density Function and Distribution Function
24
BACKGROUND

The normal distribution is written in shorthand as X~N(μ, σ2). The
standard normal, defined as a normal distribution with mean zero and
variance 1, is a convenient simplification for modeling purposes. We
denote a random variable ε as following a standard normal distribution
by ε~N(0,1).
Figure 2.7 plots the standard normal distribution. Note that it is sym-
metric about the mean; it has skewness 0 and kurtosis 3 (or 0 excess kurtosis
in Excel convention).
How do we interpret the idea of one standard deviation σ for N (μ, σ2)?
It means 68% of its observations (area under f(x)) lies in the interval [σ,
þσ] and 95% of the observations lies within [2σ, þ2σ]. Under normal
conditions, a stock’s daily return will fluctuate between 2σ and 2σ roughly
95% of the time, or roughly 237 days out of 250 trading days per year, as a
rule of thumb.
Central Limit Theorem
The central limit theorem (CLT) is a very fundamental result. It states that
the mean of a sample of i.i.d. distributed random variables (regardless of
distribution shape) will converge to the normal distribution as the number
of samples becomes very large. Hence, the normal distribution is the limiting
distribution for the mean. See Spreadsheet 2.2 for an illustration.
0.0
0.1
0.2
0.3
0.4
24.0
22.0
0.0
2.0
4.0
Frequency
Observations
68% probability
between21and11
95% probability
between22and12    
FIGURE 2.7
Standard Normal Probability Density Function
Essential Mathematics
25

The CLT is a very useful result—it means that if you have a very large
portfolio of assets, regardless of how each asset’s returns is distributed (some
may be fat-tailed, others may be skewed, yet another may be uniform) the
average portfolio return behaves like a normal distribution. The only catch is
that all the assets must be independent of each other (even though at times
they are not).
We summarize the advantages of the normal distribution that make it
widely used in financial models:
1. By CLT, it is the limiting distribution for large systems.
2. You can describe all its characteristics with just two parameters, μ
and σ.
3. A linear combination (addition) of normally distributed random
variables is also normally distributed. This makes standard deviation
σ subadditive which is a nice feature for risk measurement (see Sec-
tion 2.8).
4. It is widely applicable in science and hence seems natural. Financial
markets are shown to be Gaussian under most (although not all)
situations.
2.3 QUANTILES, VaR, AND TAILS
A quantile is a statistical way to divide ordered data into essentially equal-
sized data subsets. For n data points and any q between [0,1], the qth
quantile is the number x such that qn of the data points are less than x and
(1  q)n of the data is greater than x. The median is just the special case of a
quantile where q ¼ 0.5. Note that the median is not the same as the mean,
especially for a skewed distribution.
To obtain the quantile from the CDF (lower panel of Figure 2.6) follow
the horizontal line from the vertical axis. When you reach the CDF, the value
at the horizontal axis is the quantile. In other words, the quantile is just the
inverse function of the CDF, or F1(x). The Excel function is PERCENTILE
({x},q).
VaR is a statistical measure of risk based on loss quantiles. It measures
market risk on a total portfolio basis taking into account diversification and
leverage. VaR of confidence level c% is defined4 as:
PrðX ≤VaRÞ ¼ q
ð2:13Þ
where q ¼ 1  c is the quantile and X is the return random variable.
26
BACKGROUND

There are three possible interpretations of VaR. Consider a 95% VaR of
$1 million; it could mean:
1. 95% chance of losing up to $1 million in the next day (maximum loss).
2. 5% chance that the loss is at least $1 million in the next one day
(minimum loss).
3. 5% chance that the loss is greater than $1 million in the next one day
(loss beyond).
Interestingly, all three mean the same thing mathematically but reflect
the mental bias we have. See Figure 2.8. In light of the 2008 crisis and the
criticism of VaR, it seems the more conservative interpretation (3) is
appropriate. Regardless, VaR is certainly not the expected loss.
Since key decision-makers in banks are often not experts from the risk
management department, knowledge gaps may exist. It was argued that the
ignorance of what VaR truly represents encouraged a false sense of security
that things were under control during the run-up and subsequent crash in the
2008 credit crisis. See Section 13.1.
The target horizon of concern for banks is usually the next one day.
That means we need to work with daily returns. Regulators normally stip-
ulate a 10-day horizon for the purpose of capital computation, so the daily
VaR is scaled up to 10 days. For limits monitoring and regulatory capital,
VaR is often reported in dollar amounts by portfolios. For risk comparison
0%
1%
2%
3%
4%
5%
6%
7%
21.8%
21.4%
21.0%
20.6%
20.2%
0.2%
0.6%
Frequency
% Return
5% of
Observations 
Maximum loss
can lose beyond
Minimum loss
95%-VaR
FIGURE 2.8
Different Interpretations of VaR
Essential Mathematics
27

between individual markets, talking in units of sigma (σ) or percentage loss is
often more intuitive.
As a convention, the VaR confidence level c is defined on the left tail of
the distribution. Hence, 97.5%VaR or c ¼ 0.975 means the quantile of
interest is q ¼ (1  0.975) ¼ 0.025.
Banks can choose the observation period (or window length) and confi-
dence level c for their VaR. This is a balancing act. The window must be short
enough to make VaR sensitive to market changes, yet long enough to be
statistically meaningful. Likewise, if the confidence level is too high, there are
too few observations in the left tail to give statistically meaningful inferences.
Figure 2.9 is an example of the return distribution of the S&P 500 taken
over a 500-day window (Jan 05 to Dec 06). We overlay with a normal
distribution line to show that the stock market behavior was reasonably
Gaussian during noncrisis periods. From this data, we can calculate VaR in a
few ways using Excel:
1. Assume the normal distribution. Then the 0.025 quantile is roughly two
times σ. You can check this on Excel: NORMSINV(0.025) ¼ 1.96.
Thus, VaR is 1.96  STDEV(.) ¼ 1.2% loss.
2. Take the 0.025 quantile. VaR is PERCENTILE(.,0.025) ¼ 1.2% loss.
3. There are 0.025  500 ¼ 12.5 observations at the tail to the left of the
VaR cutoff. So we can approximate VaR by the 13th largest loss
computed using Excel function SMALL(.,13) ¼ 1.3% loss.
0%
1%
2%
3%
4%
5%
6%
7%
8%
9%
10%
21.8%21.4%21.0%20.6%20.2% 0.2%
0.6%
1.0%
1.4%
1.8%
Frequency
% Return
97.5%VaR
means 12.5
days out
of 500   
FIGURE 2.9
Return Distribution of S&P 500 Index (Jan 05 to Dec 06)
28
BACKGROUND

Note the 1.2% loss is the daily VaR. VaR, like volatility, is often quoted
in annualized terms. Assuming a normal distribution and 250 business days
for a typical trading year, the annual VaR is given by 1.2%  O250 ¼ 19%.
(Section 6.4 covers such time scaling.)
From experience, a window length of 250 days and a 97.5% VaR can be
a reasonable choice. Unless stated otherwise, we will use this choice
throughout this book.
Note that VaR does not really model the behavior of the left tail of the
distribution. It is a point estimate, a convenient single-number proxy to
represent our knowledge of the probability distribution for the purpose of
decision making. Because it is just a quantile cutoff, VaR is oblivious to
observations to its left. These exceedences (nq ¼12 of them in the above
example) can be distributed in any number of ways without having an
impact on the VaR number. As we shall learn in Chapter 5, there are
commendable efforts to model the left tail, but exceedences have so far
remained elusive. This is the dangerous domain of extremistan.
2.4 CORRELATION AND AUTOCORRELATION
Correlation
Correlation measures the linear dependency between two variables. In
financial applications, these are usually asset return time series. Under-
standing how one asset “co-varies” with another is key to understanding
hedging between a pair of assets and risk diversification within a portfolio.
The covariance between two random variables X and Y is defined as:
Covðx, yÞ ¼ σXY ¼ EðXYÞ  EðXÞEðYÞ
ð2:14Þ
Given two sets of observed samples {xi} and {yi} where i ¼ 1, . . . , n, we
can estimate their covariance using:
^σXY ¼
1
n  1
X
n
i¼1
ðxi  xÞðyi  yÞ
ð2:15Þ
where n is the number of observations, x and y are the sample means of X
and Y respectively. The Excel function is COVAR(.).
Then correlation (sometimes called Pearson’s or linear correlation) is just
covariance standardized so that it is unitless and falls in the interval [1, 1].
Essential Mathematics
29

ρðX, YÞ ¼ CovðX, YÞ
σXσY
ð2:16Þ
A correlation of þ1 means two assets move perfectly in tandem,
whereas a correlation of 1 means that they move perfectly inversely rel-
ative to each other. A correlation of 0 means the two return variables are
independent. Correlation in the data (i.e., association) does not imply
causation, although causation will imply correlation. Correlation measures
the sign (or direction) of asset movements but not the magnitudes. The
Excel function is CORREL(.).
Linear correlation is a fundamental concept in Markowitz portfolio
theory (see Section 2.8) and is widely used in portfolio risk management. But
correlation is a minefield for the unaware. A very good paper by Embrechts,
McNeil, and Straumann (1999) documented the pitfalls of correlation. Here
we will go over the main points by looking at the bivariate case; the technical
reader is recommended to read that paper.
For correlation to be problem-free, not only must X and Y be normally
distributed, their joint distributions must also be normally distributed.
More generally, the joint distribution must be an elliptical distribution, of
which the multivariate normal is a special case. In our bivariate case, this
means the contour line of its 2D plot traces out an ellipse.5 See Figure 2.10.
Both diagrams—one actual, one simulated—are generated using N(0,1)
with ρ ¼ 0.7.
23
22
21
0.0
1
2
3
(3.0)
(2.0)
(1.0)
0.0 
1.0 
2.0
Bivariate Normal
(4.0)
(3.0)
(2.0)
(1.0)
0.0 
1.0 
2.0 
3.0 
4.0 
(4.0)
(2.0)
0.0 
2.0 
4.0 
Simulated
3.0 
FIGURE 2.10
An Elliptical Joint Distribution
30
BACKGROUND

As long as the joint distribution is not elliptical, linear correlation will be
a bad measure of dependency. It helps to remember that correlation only
measures clustering around a straight line. Figure 2.11 shows two obviously
nonelliptical distributions even though they show the same correlation
(ρ ¼ 0.7) as Figure 2.10. Clearly correlation, as a scalar (single number)
representation of dependency, tells us very little about the shape (and hence
joint risk) of the joint distribution. Correlation is also very sensitive to
extreme values or outliers. If we remove the few outliers (bottom right) in the
right panel of Figure 2.11, the correlation increases drastically from 0.7 to
0.95. Furthermore, the estimation of correlation will be biased—there is no
way to draw a best straight line through Figure 2.11 in an unbiased way (i.e.,
with points scattered evenly to the left and right of the line).
Figure 2.12, left panel shows the actual joint distribution of Lehman 5-
year CDS spread and Morgan Stanley 5-year CDS spread. During normal
times (the shaded zone) the distribution is reasonably elliptical, but during
times of stress the outliers show otherwise.
Let’s summarize key weaknesses of linear correlation when used on non-
elliptical distributions:
1. Correlation is a scalar measure. It tells us nothing about the shape (or
structure) of the joint distribution.
2. Our interpretation of correlation on a scale from 1 to 1 becomes
inconsistent. Perfect dependency does not necessarily show ρ ¼ 1, and
perfect inverse dependency does not necessarily show ρ ¼ 1.
3. Correlation of zero no longer implies that the two return variables are
independent.
(3.0)
(2.0)
(1.0)
0.0 
1.0 
2.0 
3.0 
(4.0) (3.0) (2.0) (1.0) 0.0 1.0 2.0 3.0 4.0 
(3.0)
(2.0)
(1.0)
0.0 
1.0 
2.0 
3.0 
(3.0)
(2.0)
(1.0)
0.0 
1.0 
2.0 
3.0 
FIGURE 2.11
Nonelliptical Joint Distributions
Essential Mathematics
31

4. Correlation changes under transformation of risks. This means that if X
and Y are correlated by ρ, it does not follow that functions g(X) and g
(Y) are correlated by ρ. In practice, g(.) could be a pricing formula.
5. Correlation is unsuitable for fat tail events because the variances can
appear infinite. By equation (2.16), correlation is undefined. This is the
extremistan zone.
An alternative measure of dependency, rank correlation, can solve
problems 2, 4, and 5. Unfortunately, rank correlations cannot be applied to
the Markowitz portfolio framework. They are still useful, though, as stand-
alone correlation measures.
We introduce two rank correlations: Kendall’s tau and Spearman’s rho.
Suppose we have n pairs of observations {x1, y1}, . . . , {xn, yn} of the random
variables X and Y. The ith pair and jth pair are said to be concordant if
(xi  xj) (yi  yj) > 0, and discordant if (xi  xj) (yi  yj) < 0. The sample
estimate of Kendall’s tau is computed by comparing all possible pairs of
observations (where i 6¼ j), and there are 0.5n(n1) pairs.
Kendall’s tau:
^τ ¼
c  d
0:5nðn  1Þ
ð2:17Þ
21
20.5
0
0.5
1
1.5
MS returns
Lehman returns
22.5
20.5
1.5
3.5
20.1
20.05
0
0.05
0.1
FIGURE 2.12
(Left panel) Joint Return Distribution of Credit Spreads. (Right panel)
Perfect Dependency Shows 1.0 Rank Correlation.
32
BACKGROUND

where c is the number of concordant pairs and d the number of discordant
pairs.
The sample estimate of the Spearman’s rho is calculated by applying the
Pearson’s (linear) correlation on the ranked paired data:
Spearman’s rho:
^ρs ¼ Correlation

rankðxÞ, rankðyÞ

ð2:18Þ
where rank(x) is an n-vector containing the ranks6 of {xi} and similarly for
rank(y). This can be written in Excel function as: CORREL(RANK(x, x),
RANK(y, y)).
Figure 2.12, right panel shows a bivariate distribution that has perfect
correlation since all pairs are concordant. Kendall’s tau is 1.0, Spearman’s
rho is 1.0, but linear correlation gives 0.9. This illustrates weakness (2). One
can easily illustrate weakness (4) as well as shown in Spreadsheet 2.3.
A mathematically perfect but complicated measure of association is by
using the copula function (not covered here). A copula is a function that
links the marginal distributions (or stand-alone distribution) to form a joint
distribution. If we can fully specify the functional form of the N-dimensional
joint distribution of N assets in a portfolio, and specify the marginal dis-
tributions of each asset, then we can use the copula function to tell us useful
information about risk of this system. Clearly, this is an extremely difficult
task because in practice there is seldom enough data to specify the dis-
tributions precisely, and the copula has to be assumed. Copulas are exten-
sively used in modeling of credit risk and the pricing of tranched credit
derivatives such as credit default obligations (CDOs). These models simplify
the task of representing the relationships among large numbers of credit risk
factors to a handful of latent variables. A copula is then used to link the
marginal distributions of the latent variables to the joint distribution.
Unfortunately, the choice of the copula function has a significant impact on
the final tail risk measurement. This exposes users to considerable model
risks as discussed in the article by Frey and colleagues (2001).
In the absence of such perfect knowledge as copulas, a risk manager has
to rely on linear and rank correlations. He has to be mindful of the limita-
tions of these imperfect tools.
Autocorrelation
Autocorrelation is a useful extension of the correlation idea. For a time series
variable Yt the autocorrelation of lag k is defined as:
Essential Mathematics
33

ρðYt, YtkÞ ¼ Cov ðYt, YtkÞ
σðYtÞ σ ðYtkÞ
ð2:19Þ
In other words, it is just the correlation of a time series with itself but
with a lag of k, hence the synonymous term serial correlation.
This autocorrelation can be estimated using equation (2.15) but on
samples (y1, . . . , yt) and (y1k, . . . , ytk) instead. The plot of autocorre-
lation for various lags is called the autocorrelation function (ACF) or cor-
relogram. Figure 2.13 is an ACF plot of an N(0,1) process and an AR(1)
process described by equation (2.7) with k0 ¼ 0, k1 ¼ 0.7. We will discuss
AR(p) processes next, but for now notice that the ACF plot shows significant
autocorrelation for various k lags, which tapers off as k increases. This
compares to the Gaussian process, which has no serial correlation—its ACF
fluctuates near zero for all k. The ACF is a practical way to detect serial
correlation for a time series. See Spreadsheet 2.4.
The autoregressive model AR(p) for time series variable X is des-
cribed by:
Xt ¼ k0 þ k1Xt1 þ : : : þ kpXtp þ εt
ð2:20Þ
where p is a positive integer, k’s are constant coefficients and εt is an i.i.d.
random sequence. Clearly AR(p) models are non-i.i.d. (they are past dependent).
Lag 0
Lag 2
Lag 4
Lag 6
Lag 8
Lag 10
Lag 12
Lag 14
Lag 16
Lag 18
Lag 20
Lag 22
Lag 24
N(0,1) process
Lag 0
Lag 2
Lag 4
Lag 6
Lag 8
Lag 10
Lag 12
Lag 14
Lag 16
Lag 18
Lag 20
Lag 22
Lag 24
AR(1) process
20.4
20.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
20.4
20.2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
FIGURE 2.13
ACF Plot for N(0,1) Process and AR(1) Process, with Up to 25 Lags
34
BACKGROUND

It is instructive to use the AR(1), equation (2.7), to clarify some
important concepts learned previously:
1. The AR(1) process is non-i.i.d. although “driven” by an i.i.d. random
variable εt.
2. An i.i.d. process will be stationary. That does not imply that a non-i.i.d.
process (like the AR(1)) is necessarily nonstationary. Indeed, it is sta-
tionary given certain conditions (|k1| < 1).
3. It is always serially correlated as shown by its ACF.
In short, i.i.d., stationarity and serial correlation are related yet distinct
ideas. Only under certain conditions, does one lead to another.
2.5 REGRESSION MODELS AND RESIDUAL ERRORS
Regression is a basic tool in time series modeling to find and to quantify rela-
tionships betweenvariables.Thishasmany applications in finance, forexample,
a trader may form his trading view from the analysis of relationships between
stocks and bonds, an economist may need to find relationships between the
dollar and various macroeconomic data, an interest rate hedger may need to
find relationships between one section of the yield curve against another.
The modeler starts by specifying (guessing) some relationship between
the variables based on his knowledge of the markets or by visual study of
chart patterns. For example, our interest rate hedger may look at Figure 2.14
and conclude that there is an obvious relationship between the USD 5-year
swap rate and 10-year swap rate. A reasonable guess is to assume a linear
relationship of the form:
Yt ¼ α þ βXt þ εt
ð2:21Þ
where Yt and Xt are the two random variables representing 5-year and 10-
year swap rates respectively. εt is a residual error term that captures all other
(unknown) effects not explained by the chosen variables. α and β are con-
stant parameters that need to be estimated from data. The method of esti-
mation is called ordinary least squares (OLS). If εt is a white noise (i.i.d.)
series, then the OLS method produces consistent estimates. People often
mistakenly model the level price rather than the change in prices. If one
models the price, the residual error εt is often found to be serially correlated.7
This means the OLS method will produce inconsistent (biased) estimates. To
illustrate, we will do both regressions—one where data samples {yi}, {xi} are
levels, the other where they represent changes.
Essential Mathematics
35

Start by drawing a scatter plot as in Figure 2.15—the more the data
concentrate along a straight line, the stronger the regression relationship.
Conceptually, OLS works by estimating the parameters α and β that will
minimize the residual sum squares (RSS) given by:
RSS ¼
X
i
½yi  ðα þ βxiÞ2
ð2:22Þ
where xi, yi are the ith observation. Intuitively, OLS is a fitting method that
draws the best straight line that represents all data points by minimizing
the residual distances (squared and summed) between the estimated
line ^yi ¼ ^α þ ^βxi and all the observed points yi. Linear regression can easily
be
performed
by
Excel.
From
the
toolbar,
select:
Tools - Data
Analysis - Regression.
The estimated parameters are shown in Table 2.1. The β that represents
the slope of the line is close to 1 for both cases. The α is the intercept of the
line with the vertical axis. The R-square that ranges from 0 to 1 measures the
goodness of fit. R-square of 0.9 means 90% of the variability in Y is
explained linearly by X. The Excel functions are SLOPE(.), INTERCEPT(.),
and RSQ(.) respectively.
Figure 2.16 shows that if we model using level prices, the residual series
does not behave like white noise, unlike the second case where we model
using price changes. We can use an ACF plot on the residuals to prove that
the first has serial autocorrelation and the second is stationary.
2
1
2
3
4
5
6
Jul-07
Nov-07
Mar-08
Jul-08
Nov-08
Mar-09
Rates (%)
USD 5-year swap
USD 10-year swap
FIGURE 2.14
Price Series of USD 5-Year Swap and 10-Year Swap
Source: Bloomberg Finance L.P.
36
BACKGROUND

It is worth noting:
1. A noticeable correlation in the level price chart does not imply an
authentic relationship. Most market prices have trends. In the presence
of trends, you will likely get high correlation purely by chance (spurious
correlation). Thus, correlation should be calculated on returns.
2. Always check the residuals for serial correlation. If we model nonsta-
tionary level prices, then the OLS method may give inconsistent esti-
mates due to serial correlation in residuals. (The only exception is when
the price series are cointegrated. See Section 2.10.)
2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
2
3
4
5
6 
5-year swap rate
10-year swap rate
(0.4)
(0.3)
(0.2)
(0.1)
-
0.1
0.2
0.3
0.4
(0.4)
(0.2)
-
0.2 
0.4 
5-year rate change
10-year rate change
FIGURE 2.15
Scatter Plots of Prices and Price Changes
TABLE 2.1
Linear Regression Results
Using Price
Using Price Change
Beta
1.079
0.987
Alpha
0.858
0.001
R-square
0.966
0.866
Essential Mathematics
37

3. OLS regression can easily be extended to model multiple variables or to
model time-lagged variables.
4. Even when a good fit is obtained (high R-square), the result may not be
useful for forecasting tomorrow’s returns. Our simple regression mea-
sures contemporaneous relationships, which can be useful for hedging.
But for forecasting you will need a lead-lag relationship. Since markets
are mostly efficient, lead-lag relationships are hard to find. In our
example, the trader can use the 5-year swap to hedge the 10-year swap,
but he cannot use the 10-year swap movement as a signal to trade the 5-
year swap.
2.6 SIGNIFICANCE TESTS
We next look at how statisticians measure the precision of their models and
express this in the form of statistical significance. So when we say a
hypothesis (or whatever we defined to be measured) has a 95% confidence
level that means we have chosen an error range around the expected value
such that only 5% of observations fall outside this range. Clearly the tighter
the range, the higher the precision.
20.6
20.4
20.2
0
0.2
0.4
0.6
Residual
Regression of Prices
20.15
20.1
20.05
0
0.05
0.1
0.15
0.2
0.25
Residual
Time
Regression of Price changes
FIGURE 2.16
Residual Time Series Plots
38
BACKGROUND

How to Compute t-Ratio for Regression
The t-ratio is the basic metric of significance for OLS regression. Consider
again equation (2.21)—a simple two variable linear regression. First, we
compute the standard error (SE) for the coefficient β defined by:
SE ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X
i
ðyi  ^yiÞ2=ðn  2Þ
r
=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X
i
ðxi  xÞ2
r
ð2:23Þ
where ^yi is the estimate of y at the ith observation and x is the sample mean
of x. Then the t-ratio is defined by:
t ¼ ^β=SE
ð2:24Þ
This measures how far away the slope is from showing no linear
dependence (β ¼ 0) in units of standard error. Clearly, the larger the t-ratio
the more significant is the linear relationship. The t-ratios can be calculated
by the Excel regression tool.
Hypothesis Testing
To determine how good the t-ratio is, we need to state the null hypothesis
(H0) and an alternative hypothesis (H1).
H0: The slope of the regression line is equal to zero.
H1: The slope of the regression line is not equal to zero.
If the regression relationship is significant, the slope will not equal zero;
that is, we hope to reject H0. We assume the t-ratio is distributed like the
student-t distribution centered about H0. The student-t is fatter than
the normal distribution. But for large samples, the student-t approaches the
standard normal N(0,1) distribution, and we assume this case. We need to
see if the estimated t-ratio is larger than some critical value (CV) defined on a
chosen confidence level p of the distribution. In Excel, the CV is given by
NORMSINV(p).
For example, suppose we choose 95% confidence level8 for a two-tail
test, then at the left side tail, the CV ¼ NORMSINV(0.025) ¼ 1.96.
Suppose the t–ratio of our regression works out to be 7.0, that is, falls in
the critical region (see Figure 2.17), then we can reject the null hypothesis
H0, and our regression is significant. On the other hand, if the t-ratio is 1.3,
then we cannot reject H0, and we have to think of another model.
Essential Mathematics
39

Note that statisticians will never say accept the null hypothesis. A null
hypothesis is never proven by such methods, as the absence of evidence
against the null hypothesis does not confirm it. As an example, consider a
series of five coin flips. If the outcome turns out to be five heads, an observer
will be tempted to form the opinion (the null hypothesis) that the coin is an
unfair two-headed coin. Statistically, we can say we cannot reject the null
hypothesis. We cannot accept the null hypothesis, since a series of five heads
can occur from random chance alone. So likewise if we model financial time
series using a chosen fat tail distribution (say the Laplace distribution),
we cannot say we accept that distribution as the true distribution, even if the
statistical test is highly significant. Perhaps there may be other distributions
that fit the data better.
Stationarity Tests
In statistics literature, there are many significance tests of stationarity, also
called unit root tests. Here, we will only discuss the augmented Dickey-Fuller
test, or ADF(q) test developed by Dickey and Fuller (1981). This test is based
on the regression equation:
ΔXt ¼ α þ βXt1 þ δ1ΔXt1 : : : þ δqΔXtq þ εt
ð2:25Þ
where ΔX is the first difference of X, q is the number of lags, εt is the
residual, and α, β, δs are coefficients. The test statistic is the t-ratio for
the coefficient β. The null hypothesis is for β ¼ 0 versus a one-sided alternative
0.2
0.4
(5.0)
(4.0)
(3.0)
(2.0)
(1.0)
-
1.0 
2.0 
3.0 
4.0 
5.0 
0.2
0.4
0 4
Cannot reject
Critical
value
Critical region
(Reject H0)
Critical region
(Reject H0)
t- ratio
H0
0
FIGURE 2.17
Rejecting and Not Rejecting the Null Hypothesis
40
BACKGROUND

hypothesis β < 0. If the null hypothesis is rejected, then X is stationary. The
critical values depend on q and are shown in Table 2.2 for a sample size
between 500 and 600.
Equation (2.25) can easily be implemented in Excel for up to 16 lags if
needed. We shall see an example of ADF test in Section 2.10.
2.7 MEASURING VOLATILITY
In modern risk management the basic unit of risk is measured as volatility.
Risk measures such as notional, sensitivity, and others (see Section 1.1) fall
short because they cannot be compared consistently across all products. It is
instructive to compare four well-known models of volatility—standard
deviation, EWMA, ARCH, and GARCH.
Assuming we have a rolling window of n days with price data p1, . . . ,
pn, if volatility is constant or varying slowly, we can estimate the nth day
volatility by:
St: Dev: model : ^σ2
n ¼ 1
n
X
n
i¼1
r2
i
ð2:26Þ
where ri is the observed percentage return (pi  pi1)/pi1 or log return ln
(pi/pi1). Note that equation (2.26) is just the variance equation (2.3) but
with zero mean. Therefore, volatility ^σ (normally written without the sub-
script n) is simply the standard deviation of returns. In Excel function, ^σ is
given by STDEV(.).
But in financial markets, volatility is known to change with time, often
rapidly. The first time-varying model, the Autoregressive Conditional
Heteroscedasticity (ARCH) model was pioneered by Engle (1982).
TABLE 2.2
Critical Values of ADF Test
Number of Lags, q
Signiﬁcance Level
1%
5%
1
3.43
2.86
2
3.90
3.34
3
4.30
3.74
4
4.65
4.10
Essential Mathematics
41

ARCHðnÞ model : ^σ2
n ¼ αθ2 þ ð1  αÞ 1
n
X
n
i¼1
r2
i
ð2:27Þ
In some sense, equation (2.27) is an extension of equation (2.26) which
incorporates mean reversion about a constant long-run volatility θ where α
is the weight assigned to this long-run volatility term. Unfortunately, these
first two models have an undesirable “plateau” effect as a result of giving
equal weights 1/n to each observation. This effect kicks in whenever a large
return observation drops off the rolling window as the window moves for-
ward in time, as shown in Figure 2.19.
To overcome this problem, different weights should be assigned to each
observation; logically, more recent observations (being more representative
of the present) should be weighted more. Such models are called conditional
variance since volatility is now conditional on information at past times. The
time ordering of the returns matters in the calculation. In contrast, for the
standard deviation model, the return distribution is assumed to be static.
Hence, its variance is constant or unconditional, and equal weights can be
used for every observation. A common scheme is the so-called exponentially
weighted moving average (EWMA) method promoted by J.P. Morgan’s
RiskMetrics (1994).
EWMA model : ^σ2
n ¼ ð1  λÞ
X
N
i¼1
λi1r2
niþ1
ð2:28Þ
The decay factor λ (an input parameter) must be larger than 0 and less
than 1. Equation (2.28) can be simplified to an iterative formula which can
be easily implemented in Excel:
^σ2
n ¼ λ^σ2
n1 þ ð1  λÞr2
n
ð2:29Þ
Figure 2.18 shows the EWMA weights for past observations. The larger
the λ, the slower the weights decay. Gradually falling weights solve the
plateau problem because any (large) return that is exiting the rolling window
will have a very tiny weight assigned to it. For a one-day forecast horizon,
RiskMetrics proposed a decay factor of 0.94.
Bollerslev (1986) proposed a useful extension of ARCH called Gener-
alized Autoregressive Conditional Heteroscedasticity (GARCH). There is a
whole class of models under GARCH, and one simple example of a GARCH
model is:
42
BACKGROUND

Simple GARCH:
^σ2
n ¼ αθ2 þ ð1  αÞ

λ^σ2
n1 þ ð1  λÞr2
n

ð2:30Þ
Compared to equation (2.29) it looks like an extension of the EWMA
model to include mean reversion about a constant long-term mean volatility θ,
which needs to be estimated separately. In fact, the EWMA is the simplest
0
0.01
0.02
0.03
0.04
0.05
0.06
1
26
51
76
101
126
Decay factor
= 0.98  
Moving average
model (n = 100) 
Number of days from today
Weight
Decay factor
= 0.94
FIGURE 2.18
EWMA Weights of Past Observations
0.6%
0.8%
1.0%
1.2%
1.4%
1.6%
1.8%
Sep-10
Mar-11
Sep-11
Mar-12
Sep-12
ARCH
GARCH
EWMA
St. Deviation
FIGURE 2.19
Behavior of Four Volatility Models and the Plateau Effect
Essential Mathematics
43

example of a GARCH model with only one parameter. When the weight
α ¼ 0, GARCH becomes the EWMA model.
Figure 2.19 graphs all four models for the same simulated time series.
We set α ¼ 0.4, λ ¼ 0.99 and θ ¼ 0.02. We simulated a 10% (large) return
on a particular day, halfway in the time series. We can see this caused the
artificial plateau effect for the standard deviation and ARCH models. There
is no plateau effect for the EWMA and GARCH models. This implemen-
tation is in Spreadsheet 2.5.
The main advantage of GARCH is it can account for volatility clustering
observed in financial markets. This phenomenon refers to the observation, as
noted by Mandelbrot (1963), that “large changes tend to be followed by
large changes, of either sign, and small changes tend to be followed by small
changes.” A quantitative manifestation of this is that absolute returns show
a positive and slowly decaying ACF even though returns themselves are not
autocorrelated. Figure 2.20 shows volatility clustering observed in the S&P
500 index during the credit crisis. The upper panel shows that the price
return bunches together during late 2008. This is captured by GARCH and
EWMA, which showed a peak in volatility that tapers off thereafter (lower
panel). The standard deviation and ARCH models registered the rise in risk
but not the clustering.
Another good feature is that ARCH and GARCH can produce fatter
tails than the normal distribution. Unfortunately, they cannot explain the
0.0%
0.5%
1.0%
1.5%
2.0%
2.5%
3.0%
3.5%
Aug-06
Jan-07
Jun-07
Nov-07
Apr-08
Sep-08
Feb-09
Volatility (%)
St. Deviation
ARCH
EWMA
GARCH
215%
210%
25%
0%
5%
10%
15%
Percent change
FIGURE 2.20
Modeling Volatility Clustering for the S&P 500 Index
44
BACKGROUND

leverage effect (asymmetry) observed in the market, where volatility tends to
be higher during a sell-off compared to that during a rally. The exponential
GARCH model (EGARCH) by Nelson (1991) allows for this asymmetric
effect between positive and negative asset returns.
Note that what we have calculated so far is the volatility for today (the
nth day). In risk management, we are actually interested in forecasting the
next day’s volatility. It can be shown that for standard deviation and the
EWMA model, the expected future variance is the same as today’s. Taking
expectations of equation (2.29):
Eðσ2
nþ1Þ ¼ λEðσ2
nÞ þ ð1  λÞEðR2
nþ1Þ
where
EðR2
nþ1Þ ¼ Eðσ2
nþ1Þ-Eðσ2
nþ1Þ ¼ Eðσ2
nÞ
ð2:31Þ
This is not strictly true for the ARCH and GARCH models because of
the presence of mean-reversion toward the long-term volatility. Depending
on whether the current volatility is above or below the long-term volatility,
tomorrow’s volatility will fall or rise slightly. However, for a one-day
forecast the effect is so tiny that equation (2.31) can still be applied.
The key lesson from this section is that a different choice of method
and parameters will produce materially different risk measurements (see
Figure 2.19). This is unnerving. What then is the “true” volatility (risk)?
Does it really exist? Some believe the implied volatility backed out from
option markets can represent real volatility but this runs into its own set of
problems. Firstly, not all assets have tradable options, which naturally limit
risk management coverage. Secondly, there is the volatility smile where
one asset has different volatilities depending on the strike of its options (so
which one?). Mathematically, this reflects the fact that asset returns are not
normally distributed, which gives rise to the third problem—such implied
volatility cannot be used in the Markowitz portfolio framework (explained
in the next section).
The current best practice in banks is to use standard deviation or
EWMA volatility because of its practical simplicity. These measures are well
understood and commonly used in various trades and industries.
2.8 MARKOWITZ PORTFOLIO THEORY
Modern portfolio theory was founded by Harry Markowitz (see Markowitz
1952). The Nobel Prize for Economics was eventually awarded in 1990 for
Essential Mathematics
45

this seminal work. The theory served as the foundation for the research and
practice of portfolio optimization, diversification, and risk management.
Its basic assumption is that investors are risk-averse (assumption 1)—
given two assets, A and B with the same returns, investors will prefer the
asset with lower risk. Equally, investors will take on more risk only for
higher return. So what is the portfolio composition that will give the best
risk-return tradeoff? This is an optimization problem. To answer that
question, Markowitz further assumed that investors will only consider the
first two moments—mean (expected return) and variance—for their decision
making (assumption 2), and the distributions of returns are jointly normal
(assumption 3). Investors are indifferent to other characteristics of the dis-
tribution such as skew and kurtosis. The Markowitz mean-variance
framework makes use of linear correlation to account for dependency, valid
for normal distributions.
However, behavioral finance has found evidence that investors are not
always rational; that is, not risk-averse. Assumption 2 is challenged by the
observation that skew is often priced into the market as evident in the so-
called volatility smile of option markets. So investors do consider skewness.
There is also empirical evidence that asset prices do not follow the normal
distribution during stressful periods. Moreover, distributions are seldom
normal for credit markets or when there are options in the portfolio. Despite
the weak assumptions, the mean-variance framework is well-accepted
because of its practical simplicity.
In the mean-variance framework, the expected return of a portfolio μp is
given by the weighted sum of the expected returns of individual assets μi.
μp ¼
X
i
ωiμi
ð2:32Þ
where the weights ωi are the asset allocations such that Pωi ¼ 1. The
portfolio variance is given by:
σ2
p ¼
X
i
X
j
ωiωjσiσjρij
ð2:33Þ
where the correlation ρij ¼ 1 when i ¼ j. If there are n assets then i,
j ¼ 1, . . . , n.
The incorporation of correlation into equation (2.33) leads to the idea of
diversification. An investor can reduce portfolio risk simply by holding
combinations of assets that are not perfectly positively correlated, or better,
negatively correlated. To see the effects of diversification, consider a port-
folio with just two assets a, b. Equation (2.33) then becomes:
46
BACKGROUND

σ2
p ¼ ω2
aσ2
a þ ω2
bσ2
b þ 2ωaωbσaσbρab
ð2:34Þ
As long as the assets are not perfectly correlated (ρab < 1) then:
σp < ωaσa þ ωbσb
ð2:35Þ
The risk (volatility) of the portfolio is always less than the sum of the
risk of its components. Merging portfolios should not increase risks; con-
versely splitting portfolios should not decrease risk. This desirable property
called subadditivity is generally true for standard deviation regardless of
distribution.
For the purpose of mathematical manipulation, it is convenient to write
the list of correlation pairs in the form of a matrix, the correlation matrix.
Then basic matrix algebra can be applied to solve equations efficiently. Excel
has a simple tool that generates the correlation matrix (Tools - Data
analysis - Correlation) and some functions to do matrix algebra.
The problem of finding the optimal asset allocation is called the Mar-
kowitz problem. The optimization problem can be stated in matrix form:
min
w ðwTΣwÞ subject to
X
i
ωi ¼ 1 and wTμ ¼ rT
ð2:36Þ
where w is the column vector of weights ωi, μ is the vector of investor’s
expected returns, μi, rT the targeted portfolio return, Σ the covariance matrix
derived using equation (2.15). We need to find the weights w that minimize
the portfolio variance subject to constraints on expected returns and port-
folio target return. Spreadsheet 2.6 is an example of portfolio optimization
with four assets performed using Excel Solver.
The key weakness of the classical Markowitz problem is that the
expected returns and target are all assumed inputs (guesswork)—they are
not random variables that are amenable to statistical estimation. The only
concrete input is the covariance matrix, which can be statistically estimated
from time series. It is found that the optimization result (weights) is unstable
and very sensitive to return assumptions. This could lead to slippage losses
when the portfolio is rebalanced too frequently. The Markowitz theory has
evolved over the years to handle weaknesses in the original version, but that
is outside the scope of this book.
Imagine a universe of stocks available for investment and we have to
select a portfolio of stocks by choosing different weights for each stock. Each
combination of assets will have its own risk-return characteristics and can
be represented as a point in the risk/return space (see Figure 2.21). It can be
Essential Mathematics
47

shown that if we use equation (2.36) to determine our choice of assets, these
optimal portfolios will lie on a hyperbola. The upper half of this hyperbola is
preferable to the rest of the curve and is called the efficient frontier. Along
this frontier, risky portfolios have the lowest risk for a given level of return.
The heart of portfolio management is to rebalance the weights dynamically
so that a portfolio is always located on the efficient frontier.
It is also convenient to express the risk of an individual asset relative to
its market benchmark index. This is called the beta approach, commonly
used for equities. It is reasonable to assume a linear relationship given by the
regression:
RiðtÞ ¼ αi þ βiRMðtÞ þ εiðtÞ
ð2:37Þ
where Ri is the return variable for asset i and RM is the return of the market
index at time t, αi is a constant drift for asset i. εi(t) is an i.i.d. residual
random variable specific to the i th asset; this idiosyncratic risk is assumed to
be uncorrelated to the index. The beta βi, also called the hedge ratio, is the
sensitivity of the asset to a unit change in the market index. Thus, in the beta
model, an asset return is broken down into a systematic component and an
idiosyncratic component. The parameters can be estimated using the Excel
regression tool.
2.9 MAXIMUM LIKELIHOOD METHOD
Maximum likelihood estimation (MLE) is a popular statistical method used
for fitting a statistical model to data and estimating the model’s parameters.
0
5
10
15
20
1
6
11
16
21
Portfolio Expected Return
Standard Deviation (%)
The efficient frontier
ontier
FIGURE 2.21
The Efﬁcient Frontier
48
BACKGROUND

Suppose we have a sample of observations x1, x2, . . . , xn which are
assumed to be drawn from a known distribution with PDF given by fθ(x)
with parameter θ (where θ can also be a vector). The question is: What is the
model parameter θ such that we obtain the maximum likelihood (or prob-
ability) of drawing from the distribution, the same sample as the observed
sample?
If we assume the draws are all i.i.d. then their probabilities fθ(.) are
multiplicative. Thus, we need to find θ that maximizes the likelihood func-
tion:
LðθÞ ¼ L
n
i¼1
fθ ðxiÞ
ð2:38Þ
Since the log function is a monotonic function (i.e., perpetually
increasing), maximizing L(θ) is equivalent to maximizing ln(L(θ)). In fact, it
is easier to maximize the log-likelihood function:
L*ðθÞ ¼
X
n
i¼1
ln ½ fθ ðxiÞ
ð2:39Þ
because it is easier to deal with a summation series than a multiplicative
series.
As an example, we will use MLE to estimate the decay factor λ of the
EWMA volatility model (see Section 2.7). We assume the PDF fθ(x) is normal
as given by equation (2.12). Then, the likelihood function is given by:
LðλÞ ¼ L
n
i¼1
1ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2πvi
p
exp x2
i
2vi




ð2:40Þ
where the variance νi ¼ νi(λ) is calculated using the EWMA model. Taking
logs, we can simplify to the following log-likelihood function (ignoring the
constant term and multiplicative factor):
L*ðλÞ ¼
X
n
i¼1
lnðviÞ  x2
i
vi


ð2:41Þ
The conditional variance νi(λ) ¼ σi
2 on day i is estimated iteratively
using equation (2.29). After that, the objective L*(λ) can be maximized
using Excel Solver to solve for parameter λ. Spreadsheet 2.7 illustrates how
this can be done for Dow Jones index data. The optimal decay factor works
Essential Mathematics
49

out to be ^λ ¼ 0.94 as was estimated and proposed by RiskMetrics. The
weakness of the MLE method is that it assumes the functional form of the
PDF is known. This is seldom true in financial markets.
2.10 COINTEGRATION
Cointegration analysis tests whether there is a long-run equilibrium rela-
tionship between two time series. Hence, if two stocks are cointegrated they
have the tendency to gravitate toward a common equilibrium path in the
long-run, that is, short-term deviation will be corrected. Hence, cointegra-
tion has found popular application in the area of pair trading (statistical
arbitrage) where a trader will buy one security and short another security,
betting that the securities will mean revert towards each other. This idea is
also used by index-tracking funds where a portfolio of stocks is designed to
track the performance of a chosen index.
Cointegration and correlation are very different ideas. Correlated stocks
rise and fall in synchrony, whereas cointegrated stocks will not wander off
from each other for very long without reverting back to some long-term
spread. Figure 2.22 shows the difference between the two ideas.
96
98
100
102
104
106
108
110
Cointegrated and correlated
96
98
100
102
104
106
108
Cointegrated and inverse correlated
FIGURE 2.22
Cointegration and Correlation
50
BACKGROUND

In the 1980s many economists simply used regression on level prices,
which ran into the problem of serial correlation of residuals. This means the
OLS estimation is not consistent and the correlation relationship may be
spurious. Engle and Granger (1987) formalized the cointegration approach
and spelled out the necessary conditions for OLS regression to be applied to
level prices (nonstationary data).
A time series is said to be integrated of order p written I(p) if we can
obtain a stationary series by differencing p times (but no less). Most financial
market time series are found to be I(1). A stationary time series is a special
case of I(0). Suppose X1, X2, . . . , Xn are integrated prices (or log prices),
then these variables are cointegrated if a linear combination of them pro-
duces a stationary time series. This can be formalized using the Engle-
Granger regression:
X1ðtÞ ¼ a1 þ a2X2ðtÞ þ : : : þ anXnðtÞ þ εðtÞ
ð2:42Þ
The Engle-Granger test is for the residuals ε(t) to be stationary or I(0). If
affirmative, then X1, X2, . . . , Xn are said to be cointegrated. Only in this
situation, OLS regression can be applied to these random variables.
The coefficients a1, . . . , an can be estimated using the OLS method. The
problem with the Engle-Granger regression is that the coefficients obviously
depend on which variable you choose as the dependent variable (i.e., as X1).
Hence, the estimates are not unique—there can be n1 sets of coefficients. This
is not a problem for n ¼ 2, which is mostly the case for financial applications.
A more advanced cointegration test that does not have such weakness is
the Johansen method (1988). Once a cointegration relationship has been
established, various models can be used to determine the degree of deviation
from equilibrium and the strength of subsequent corrections. For further
reading, see Alexander (2008).
There are many examples of securities in the market that show highly
visible correlation when plotted, but no cointegration when tested. Figure
2.23 shows the chart of Dow Jones index vs. Nasdaq index—both I(1) time
series. The correlation of their log returns over the period Jan 1990 to Jun
2009 is 0.86. Using the Engle-Granger regression test, we can show that the
two indices are not cointegrated. The worked-out example in Spreadsheet
2.8 also shows how the augmented Dickey-Fuller stationarity test can be
performed in Excel.
On the other hand, empirical research has found that different points on
the same yield curve are often cointegrated. Pair trading, for example, thrives
on searching for cointegrated pairs of stocks, typically from the same
industry sector. Cointegrated systems are almost always tied together by
common underlying dynamics.
Essential Mathematics
51

2.11 MONTE CARLO METHOD
Monte Carlo (MC) simulation is a numerical algorithm that is used exten-
sively in finance for derivatives pricing and VaR calculation. The method
involves the generation of large samples of random numbers repeatedly
drawn from some known distribution. With the advent of powerful com-
puters this method of brute force became popular because of its intuitive and
flexible nature. Furthermore, the simulation method is a useful visualization
device for the modeler to gain a deeper understanding of the behavior of the
model. In actual implementation, the MC simulation is generally coded in
more efficient languages such as Cþþ and R. But for reasons of pedagogy,
examples here will be implemented in Excel.
In simulating a path for a time series Xt where t ¼ 1, . . . , n, a random
variable εt needs to be generated by the computer to represent the stochastic
change element. This change is then used to evolve the price (or return) Xt to
its value Xtþ1 in the next time step. This is repeated n times until the whole
path is generated. The stochastic element represents the price impact due to
the arrival of new information to the market. If the market is efficient, it is
reasonable to assume εt is an i.i.d. white noise.
Excel has a random generator RAND() that draws a real number
between 0 and 1 with equal probability. This is called the uniform distri-
bution. The i.i.d. random variable is typically modeled using the standard
normal N(0, 1) for convenience. We can get this number easily from the
following inverse transformation NORMSINV(RAND()). This is the inverse
function of the normal CDF. Essentially Excel goes to the CDF (see Figure
5
6
Log of prices
7
8
9
Jan-90
Jan-91
Jan-92
Jan-93
Jan-94
Jan-95
Jan-96
Jan-97
Jan-98
Jan-99
Jan-00
Jan-01
Jan-02
Jan-03
Jan-04
Jan-05
Jan-06
Jan-07
Jan-08
Jan-09
S&P500
Nasdaq
FIGURE 2.23
Dow Jones versus Nasdaq (in log scale)
52
BACKGROUND

2.6), reference the probability as chosen by RAND() on the vertical axis, and
finds the corresponding horizontal axis value. Pressing the F9 (calculate)
button in Excel will regenerate all the random numbers given by RAND().
In general, there are two classes of time series processes that are fun-
damentally different—stochastic trend processes and deterministic trend
processes. A stochastic trend process is one where the trend itself is also
random. In other words (Xt  Xt1) shows a stationary randomness. One
example is the so-called random walk with drift process given by:
Random walk : Xt ¼ μ þ Xt1 þ εt
ð2:43Þ
where the constant μ is the drift, εt is i.i.d. Clearly this is an I(1) process as
the first difference will produce μ þ εt, which is an I(0) stationary process.
We call such a process stochastic trend. Another example is the geometric
Brownian motion (GBM) process:9
GBM : ΔXt
Xt
¼ μΔt þ σ
ﬃﬃﬃﬃﬃﬃ
Δt
p
εt
ð2:44Þ
where the constant μ is the annualized drift, σ the annualized volatility, εt ~
N(0,1) and ΔXt ¼ Xt  Xt1. The time step is in units of years, so for one
day step, Δt ¼ 1/250. Hence, in GBM, we simulate the percentage changes
in price; we can then construct the price series itself iteratively.
The GBM process is commonly used for derivatives pricing and VaR
calculation because it describes the motion of stock prices quite well. In
particular, negative prices are not allowed because the resulting prices are
lognormally distributed. A random variable Xt is said to follow a lognormal
distribution if its log return, ln(Xt/Xt1) is normally distributed. Figure 2.24
shows simulated GBM paths using σ ¼ 10%, μ ¼ 5%. The larger μ is, the
larger the upward drift, the larger σ is, the larger the dispersion of paths.
The other process is the deterministic trend process, where the trend is
fixed, only the fluctuations are random. It is an “I(0)þtrend” process, and
one simple form is:
Deterministic Trend : Xt ¼ α þ μt þ εt
ð2:45Þ
where α is a constant, μ is the drift, and εt is i.i.d. (α þ εt) is I(0) and μt is the
trend.
Figure 2.25 shows a comparison of the three processes generated using
30% drift and 50% volatility. Although not obvious from the chart, the
I(1) and I(0) þ trend processes are fundamentally different, and this will
Essential Mathematics
53

determine the correct method used to detrend them (see next section). It is
instructive to experiment with the processes in Excel (see Spreadsheet 2.9).
As we shall see in Monte Carlo simulation VaR (Chapter 4), we will
need to simulate returns for many assets that are correlated. The dependence
structure of the portfolio of assets is contained in the correlation matrix. To
generate random numbers that are correlated, the correlation matrix Σ needs
to be decomposed into the Cholesky matrix L. L is a lower triangular matrix
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
Price
Time
FIGURE 2.24
Geometric Brownian Motion of 50 Simulated Paths
20.5
0
0.5
1
Price
Time
GBM
Random Walk
Deterministic Trend
FIGURE 2.25
Stochastic Trend versus Deterministic Trend Processes
54
BACKGROUND

(i.e., entries above the diagonal are all zero) with positive diagonal entries,
such that LLT ¼ Σ where LT is the transpose of L.
We coded this function as cholesky(.) in Excel, that is, L ¼ cholesky(Σ).
The code is explained in the reference book by Wilmott (2007).10 Let’s say
we have n assets; first, we compute L (n-by-n matrix). Secondly, an n-vector
of random numbers {ε1(t), . . . , εn(t)} is sampled from an i.i.d. N(0, 1)
distribution. This column vector is denoted ε(t). Finally, the n-column-vector
of correlated random numbers r(t) can be computed by simple matrix
multiplication: r(t) ¼ Lε(t). For example, to generate n correlated time series
of T days, perform the above calculations for t ¼ 1, 2, . . . , T. In Excel
function notation, this is written as r ¼ MMULT(L, ε). If the vectors are
rows instead, the Excel function is written in a transposed form: r ¼
TRANSPOSE(MMULT(L,TRANSPOSE(ε))). Correlated random numbers
are generated in Spreadsheet 2.10.
2.12 THE CLASSICAL DECOMPOSITION
The study of financial market cycles is of paramount importance to policy
makers, economists, and the trading community. Clearly being able to forecast
where we are in the business cycle can bring great fortunes to a trader or allow
the regulator to legislate preemptive policies to cool a bubbling economy. It is
the million dollar question. This area of research is far from established as
endeavors of such economic impact are, by nature, often elusive.
Actual time series observed in markets are seldom stationary—they
exhibit trends and cycles. The problem is that statistical estimation methods
are mostly designed to handle stationary data. Hence, in time series analysis,
we need to decompose the data so that we obtain something stationary to
work with. Figure 2.26 illustrates a stylized time series decomposition. The
classical decomposition breaks an observed price series xt (t ¼ 1, 2, . . . )
into three additive components:
xt ¼ lt þ st þ zt
ð2:46Þ
where lt is the long-term trend, st the cycle (or seasonality), and zt the noise
(or irregular) component. Of the three components, zt is assumed to be
stationary and i.i.d. There is an abundance of research in this area of sea-
sonal decomposition; for a reference book, read Ghysels and Osborn (2001).
A general approach to time series analysis follows these steps:
1. Plot the time series xt to visually check for trend and seasonality.
2. Detrend data into a stationary time series zt.
Essential Mathematics
55

3. Chose a model (for example, AR(p), GARCH, etc.) and try to fit zt.
4. Run diagnostic tests (such as residual analysis). If the model is unac-
ceptable, go back to step 3.
5. Generate the forecasted distribution (say for a one-day horizon).
6. Invert the transformation in step 2 to bring the forecasted distribution
back to the price level.
This is illustrated in Figure 2.27.
There are many ways to detrend the data in step 2. If the times series is a
stochastic trend process then the correct approach is to take differences.
There is considerable evidence that suggests most financial market prices are
I(1) stochastic trend processes.
On the other hand, if the data is a deterministic trend—“I(0)þtrend”
process, then the correct approach is to take the deviation from a fitted trend
line (the decomposition approach). Note that if we do this to an I(1) process,
the resulting deviation from trend line may not be stationary. Nevertheless,
this is a common practice among forecasters, and it remains an open debate.
Beveridge and Nelson (1981) found that if the trend is defined as the long-
run forecast, then decomposition of an I(1) process can lead to a stationary
deviation.
As a typical example, the Berlin procedure, developed by the Federal
Statistical Office of Germany, models the long-term trend as a polynomial
and the cycle as a Fourier series. It assumes that the stochastic trend follows a
highly auto-correlated process such that the realizations will be smooth
enough to be approximated using a low-order polynomial (of order p):
2200
2100
0
100
200
300
400
500
600
Long-term trend(l) 
Cycle (s)
Noise (z)
x 5 l1s1z
Observed
prices  
FIGURE 2.26
Stylized Classical Decomposition
56
BACKGROUND

lt ¼
X
p
i¼0
αiti
ð2:47Þ
The cycle component is assumed to be weakly stationary and changes slowly
with time, so that it can be approximated using a finite Fourier series:
st ¼
X
k
i¼0
½βiCosλit þ γiSinλit
ð2:48Þ
We are dealing with 250 daily observations per year, so λ1 ¼ 2π/250 and
λi ¼iλ1 with i ¼ 1, 2, . . . , k are the harmonic waves for λ1. We can use
least square minimization11 to estimate the coefficients αi, βi, and γi.
Spreadsheet 2.11 shows the decomposition of S&P 500 index daily closing
prices using a cubic polynomial (p ¼ 3) and Fourier series (k ¼ 12). The
result is shown in Figure 2.28.
When statisticians speak of forecast, they mean estimating the most
statistically likely distribution at a future horizon (typically the next day).
While a risk manager will be interested in the four moments of such a dis-
tribution and its quantile (the VaR), such forecast of uncertainty (or risk) is
of lesser importance to traders who are more concerned about the trend,
cycle, and direction of the market. There is a fundamental difference between
forecasting distribution and forecasting direction. Statistics is good for
2200
2100
0
100
200
300
400
500
600
Detrend
Model stationary process
Inverse transform
Forecast
FIGURE 2.27
Time Series Forecast Approach
Essential Mathematics
57

distributional forecasts but poor for directional forecasts. The problem is
that estimation methods are mostly designed to handle stationary data.
During the detrending process, valuable information on trend (or anything
other than noise) is lost.
Many analysts and researchers still perform analysis on direction,
trends, and cycles using technical analysis and component models, but things
become murky once we deal with nonstationary data. We no longer have the
law of large numbers on our side, precision is lost, and we can no longer
state our results with a high confidence level. Furthermore, the model is not
identifiable—for example there are many ways to decompose a time series
into its three components.
Table 2.3 compares the two schools of forecasting. In Chapter 13 we
will propose a new interpretation of decomposition that will incorporate
directional elements into VaR forecasts.
2.13 QUANTILE REGRESSION MODEL
Aside from OLS and MLE methods, another statistical estimation method
that is increasingly useful in risk management is the quantile regression
model (QRM) introduced by Koenker and Bassett (1978).
The beauty of QRM is that it is model free in the sense that it does not
impose any distributional assumptions. The only requirement is i.i.d. In
contrast, for OLS estimation, the variables must be jointly normal; otherwise
the estimate is biased. Furthermore, OLS only produces an estimate of mean
0
200
400
600
800
1000
1200
1400
1600
1800
Jan-96
Jan-97
Jan-98
Jan-99
Jan-00
Jan-01
Jan-02
Jan-03
Jan-04
Jan-05
Jan-06
Jan-07
Jan-08
Jan-09
S&P 500 index
Trend fit
Trend + cycle fitted line
FIGURE 2.28
Classical Decomposition of S&P 500 Index
58
BACKGROUND

or location of the regression line for the whole sample. It assumes that the
regression model being used is appropriate for all data, and thus ignores
the possibility of a different degree of skewness and kurtosis at different
levels of the independent variable (such as at extreme data points).
But what if there is an internal structure in the population data? For
example, the mean salary of the population (as a function of say, age)
contains trivial information because there may be some structure in the data
that is omitted, such as segmentation into certain salary quantiles based on
differences in industry type and income tax bracket. QRM can be used to
study these internal structures.
The intuition of QRM is easy to understand if we view quantiles as a
solution to an optimization problem. Just as we can define the mean as the
solution of the optimization problem of minimizing the sum of residual
squares (RSS), the median (or 0.5-quantile) is the solution to minimizing the
sum of absolute residuals. Referring to the linear regression equation (2.21),
taking its expectation and noting that E(εt) ¼ 0, gives equation (2.49), the
conditional mean (i.e., conditional on X):
Conditional mean:
EðY9XÞ ¼ α þ βXt
ð2:49Þ
Given a sample, the OLS solution ^β, ^α will provide an estimate ^E (Y|X)
shown as the thin (middle) line in Figure 2.29. This best fit line is drawn such
that the sum of residual squares above and below the line offset. Likewise, to
estimate median, we draw a line such that the sum of absolute residuals
above and below the line offset. We can extend this idea to estimate the q-
quantile by imposing different weights to the residuals above and below the
line. Suppose q ¼ 0.05. We will draw the line such that the 0.05 weighted
TABLE 2.3
Distributional Forecast versus Directional Forecast
Type
Distributional Forecast
Directional Forecast
Forecast
Distributions, moments,
risks, quantiles
Trend, cycles, direction
Nature of time
series
Stationary
Nonstationary
Results
High precision subject to
model error
Nonunique solutions
Users
Risk managers, statisticians
Traders, economist, analyst,
policy makers
Essential Mathematics
59

absolute residuals above the line offset the 0.95 weighted absolute residuals
below the line. These are shown in Figure 2.29 for q ¼ 0.05, 0.95.
Mathematically, the quantile regression line (for a chosen q) can be
found by minimizing the weighted equation (2.50) and estimating α and β
min
α, β
X
T
t¼1
ðq  IYt ≤α þ βXtÞðYt  ðα þ βXtÞÞ
ð2:50Þ
where the indicator function I(.) is given by:
IYt ≤αþβXt ¼
1
if Yt ≤α þ βXt,
0
otherwise
	
The newly estimated coefficients ^β, ^α depend on the choice of q (a
different q leads to a line with a different slope β and intercept α). Note that
they are not parameter estimates for the linear regression (2.21) but rather
for the (linear) quantile regression model:
Conditional quantile:
F1ðq9XÞ ¼ α þ βX þ F1
ε ðq9XÞ
ð2:51Þ
where F1(.) represents the quantile function; recall a quantile is just the
inverse CDF (see Section 2.3). So the conditional q-quantile F1(q|X) is
210%
28%
26%
24%
22%
0%
2%
4%
6%
8%
10%
210% 28% 26% 24% 22%
0%
2%
4%
6%
8%
10%
Y(t)
X(t)
Y
QRM (q = 0.95)
QRM (q = 0.05)
Linear (Y)
FIGURE 2.29
Quantile Regression Lines
60
BACKGROUND

derived by taking the inverse of F(Y|X); that is, you can get equation (2.51)
simply by taking the quantile function of equation (2.21). The last term is
nonzero because the i.i.d. error εt does not have zero quantile (even though it
has zero mean).
Now, since VaR is nothing but the quantile, then (2.51) basically gives
the VaR of Y conditional on another variable X, which could be anything
such as GDP, market indices, and so on, or even a function of variables. This
conditional VaR is a powerful result and will be exploited in Section 5.3 and
Section 12.3. However, QRM is not model free in the sense that we still need
to assume (2.51) is linear and also specify what X is.
Spreadsheet 2.12 is a worked-out example of QRM estimation using the
Excel Solver. Here, we assume Y are returns of a portfolio, X is the change in
some risk sentiment index.12 Hence, the estimated ^F
1ðq9XÞ is the c% con-
fidence level VaR conditional on X (where q ¼ 1c by convention). In other
words, we assume there is some relationship between our portfolio’s VaR and
variable X (perhaps we have used X as a market timing tool for the portfolio)
and would like to study the tail loss behavior. We then use QRM to estimate
the quantile loss (or VaR) at different q for different changes in X.
The result is shown in Figure 2.30. Negative return denotes loss. Notice
the higher the VaR confidence level, the larger the loss obviously for the
same line (i.e., same change in index). But the plot revealed an interesting
structure in the tail loss—losses tend to be even higher when the risk senti-
ment index shows large positive changes (large X). It shows that the index is
doing what it is supposed to do—predicting risky sentiment.
20.1
20.08
20.06
20.04
20.02
0
0.02
0.04
0.99
0.97
0.95
0.93
0.91
0.89
0.87
0.85
Quantile Loss (conditional on X)
VaR Confidence level
X = 20.2
X = 20.1
X = 0
X = 0.1
X = 0.2
FIGURE 2.30
Structure of Quantile Loss Conditional on X
Essential Mathematics
61

2.14 SPREADSHEET EXERCISES
2.1 The Law of Large Numbers (LLN) is useful because it states that as
more measurements N are taken, statistics will converge to “true”
value, provided the random variable is i.i.d. Aim: illustrate LLN on
three processes including a non-i.i.d. AR(1) process as N increases to
1000. Action: check the convergence of mean of AR(1) by modifying
equation (2.7) for cases: |k1| < 1, |k1| > 1 and k1 ¼ 0.999.
2.2 The Central Limit Theorem (CLT) is useful because it states that as
more samples are taken from various i.i.d. distributions, the means of
those samples will be normally distributed. Aim: illustrates CLT by
sampling from a uniform distribution 500 times. The distribution of the
means of those samples is plotted. Action: Extend the number of samples
to 5000 to show that the histogram approaches a normal distribution.
2.3 Linear correlation is known to have weaknesses when the relationship
between variables is nonlinear. Rank correlation provides an alter-
native. Aim: illustrates the computation of rank correlations. Action:
Modify the spreadsheet to show that rank correlation is invariant
under nonlinear transformation of risk. Tip: first generate sample
returns x, y using NORMSINV(RAND())  0.1, then do the trans-
formation (for example to x3, y3) but keep the signs (directions) of x, y
unchanged.
2.4 An autocorrelation function (ACF) plot is useful in visually identifying
serial correlation. Aim: illustrates the computation of ACF plots of
N(0,1) and AR(1) processes. Action: Experiment with the AR(1)
process by modifying equation (2.7) for cases: |k1| < 1, |k1| > 1, and
k1 ¼ 0.999.
2.5 Standard deviation, EWMA, ARCH, GARCH are four different ways
to model the volatility (hence risks) of a financial variable. Aim:
illustrates the implementation of the four volatility models. Note the
behavior of the four graphs as the parameter α goes to 0, GARCH
becomes the EWMA model. As α goes to 1.0, the volatility of ARCH
and GARCH converges to the long-term (constant) volatility θ.
2.6 Markowitz modern portfolio theory was a breakthrough in modern
understanding of risk/reward of investments and pioneered the use of
variance as a measure of risks. Aim: illustrates the classical Markowitz
portfolio optimization for a portfolio of four assets. Action: Set a
different target return for the portfolio and rerun the Excel Solver.
Modify the expected return inputs for each asset and check how the
optimal weights respond. Tip: If the target return is unreasonably high,
the Solver cannot find an optimum solution.
62
BACKGROUND

2.7 Maximum likelihood method (MLE) is a popular statistical method to
estimate the parameters of models given a sample of data. Aim:
illustrates the use of MLE to estimate the decay parameter λ of the
EWMA volatility model.
2.8 Cointegration measures the tendency for two time series to drift
towards a common long-term equilibrium path. It is a required
behavior for pair trading. A pair of correlated variables is not neces-
sarily cointegrated. Aim: illustrates how to check for cointegration
using the Engle-Granger regression method. Note: The two I(1) time
series are logs of Dow Jones index and S&P 500 index. The residuals
of the regression are tested for I(0) stationarity using the augmented
Dickey-Fuller test.
2.9 Deterministic trend and stochastic trend processes are fundamentally
different. The time series of the former wanders around a fixed trend,
in the latter the trend itself wanders around. Aim: illustrates the two
processes using three examples: random walk, geometric Brownian
motion, and a deterministic trend process. Note: First, the GBM
process can never go below zero because it is a lognormal process.
Second, the deterministic trend process tends to have a rather stable
trend, which is just the fixed line μ þ βt. On the other hand, the trend
for the other two processes wanders around.
2.10 In Monte Carlo simulation it is crucial to be able to generate correlated
variables because most financial assets that are modeled in the banking
industry move in correlated fashion. Aim: illustrates the generation of
correlated random variables using Cholesky decomposition. Note: The
function cholesky(.) is written in Visual Basic (VB) code. Using a given
correlation matrix, the example generated 50 sets of correlated ran-
dom numbers for four assets.
2.11 The classical decomposition breaks down a time series into three
theoretical components—the trend, the cycle, and the noise—for
analysis. Aim: Illustrate the classical decomposition using cubic poly-
nomial for the trend and Fourier series for the cycle. Note: The
polynomial fit is done by OLS regression. The resulting deviation from
trend is then fitted to a Fourier series. The Excel solver is used to
estimate parameters βi and γi in equation (2.48) by minimizing the
sum of squared deviations of observation points from the fitted line.
2.12 Quantile regression model (QRM) is a statistical method to estimate
parameters of a model given a sample of data. It is increasingly pop-
ular in VaR modeling because researchers are considering models that
are conditional on external variables. Aim: illustrates QRM estimation
using Excel Solver. Action: solve for the parameters using different
Essential Mathematics
63

choices of quantiles q. Then use ^α s and ^β s to calculate the conditional
quantile as per equation (2.51) for various X. Plot the results to pro-
duce the Figure 2.30.
NOTES
1. Plight of the Fortune Tellers: Why We Need to Manage Financial Risk Dif-
ferently by Riccardo Rebonato (2007) challenges the frequentist paradigm and
advocates the Bayesian alternative. The book introduces (without using equa-
tions) potential use of subjective probability and decision theory in risk
management.
2. To be precise, the mean does not depend on t, and the autocovariance COV(Xt,
Xt þ k) is constant for any time lag k.
3. A ﬁnite distribution is one whereby its variance does not go to inﬁnity.
4. Unless mentioned otherwise, in this book we will retain the negative sign for the
VaR number to denote loss (or negative P&L), to follow the convention used in
most banks.
5. For N-variate joint distribution, imagine an object of N-dimensions that when
projected onto any two dimensional plane casts an elliptical shadow.
6. For example, rank (0.5, 0.2, 0.34, 0.23, 0) ¼ (1, 3, 2, 5, 4).
7. Possible exceptions are if the price itself is highly stationary such as the VIX
index and implied volatilities observed in option markets.
8. At higher levels, it will be easier to accept the null hypothesis, but there is a risk
of accepting a wrong null hypothesis (Type I error). At lower levels, it will be
easier to reject the null hypothesis (i.e., our tolerance is more stringent), but
there is risk of rejecting a true null hypothesis (Type II error). A 95% conﬁdence
level is commonly accepted as a choice that balances the two types of error.
Note: Do not confuse this with the VaR’s conﬁdence level, which is just one
minus the quantile.
9. The Brownian motion is originally used in physics to describe the random
motion of gas particles.
10. The code for Cholesky decomposition is widely available from the Internet and
should not distract us from the main discussion here.
11. The actual estimation in the Berlin procedure is more sophisticated than what is
described here. For more information, the avid reader can refer to the proce-
dural manual and free application software downloadable from its website
www.destatis.de/.
12. Risk sentiment (or risk appetite) indices are created by institutions for market
timing (for trading purpose) and are typically made of some average function of
VIX, FX option implied volatilities, bond-swap spreads, Treasury yields, and so
on. The indicator is supposed to gauge fear or risk perception. Although this has
not gone into mainstream risk management, we see it slowly being applied in
risks research.
64
BACKGROUND

PART
Two
Value at Risk Methodology


CHAPTER 3
Preprocessing
B
efore we explore the different VaR methodologies in Chapter 4, we need
to introduce the building blocks used by VaR calculation—in particular,
how positions in a portfolio are mapped to a set of risk factors and the
generation of scenarios from these risk factor data. This preprocessing
step represents the data aspect of VaR and forms the crucial first half of any
VaR system.
3.1 SYSTEM ARCHITECTURE
To gain a perspective of how banks build and maintain their risk manage-
ment systems, it helps to look at a typical risk management architecture.
A good design incorporates the so-called front-to-back architecture within a
single system. Unfortunately because of the merger mania in the late 1990s,
many banks inherited legacy systems that were then interfaced together
(loosely) by their in-house risk IT. In addition, the outsourcing mania in
recent years may have moved the risk IT support to a different continent.
Such a disconnect creates many weak links in the information chain. Some of
the resulting problems could be incomplete position capture, incompatible
rates and risk factors across systems, lack of risk aggregation tools, and so
on. In the extreme case, risk information becomes opaque and questionable,
which could lead to problems in hedging activities and risk monitoring.
Figure 3.1 shows a stylized system architecture. The upper part repre-
sents the risk management system while the lower part represents the trade
booking system. Both systems call on the pricing engine, which consists of
pricing libraries (models and codes) used to revalue a derivative product.
Think of it as the brain that computes the fair price of each deal as you pass
it through the engine.
The front-office (FO) booking interface lets the trader price and book
deals. It invokes the pricing engine to get real-time pricing, risk, and hedging
67

information. To value a product, the pricing engine requires rates infor-
mation from the rates engine and deal information from the booking
interface (see the bent arrows).
Once the trade is booked, the position is stored in portfolios (hierar-
chical database). Typically, each trading desk has multiple portfolios. A
cluster of portfolios gets rolled up into a book for that trading desk.
Deal information such as profit-loss (PL) and cash flows are passed to
middle office (MO) for processing. Some of these support functions include a
product control team, which performs daily mark-to-market of trades, price
testing, and PL attribution. The verified PL is then passed on to the finance
team that does management accounting. Finally, the back-office (BO) pro-
cessing team performs deal verification, post-trade documentation, and cash
settlement with the bank’s counter parties.
The rates engine takes in real-time market data feeds from sources such as
Reuters and Bloomberg and stores the data at end of day (EOD data snap-
ping). In the preprocessing step, the universe of data is cleaned and boot-
strapped (for yield curves), and deals are mapped to a set of risk factors. From
Rates Engine
Risks
Aggregation/
Storage
VaR Engine
Preprocessing
VaR Reports
FO Booking
Interface
BO Settlement
MO Processing
Positions/
Portfolios
Pricing Engine
FIGURE 3.1
Stylized System Architecture
68
VALUE AT RISK METHODOLOGY

the risk factors, the preprocessor will generate a set of scenario vectors (for
historical simulation hsVaR) or a covariance matrix (for parametric pVaR).
Next, the risk engine (or VaR engine) calculates the PL vectors for all
positions in the case of hsVaR. Here, the pricing engine is called to perform
full revaluation of each deal (using positional and rates information). In the
case of pVaR, the covariance matrix is multiplied with the sensitivity vector
directly to obtain pVaR. Here, the sensitivity vector is computed by the
pricing engine as well.
Once the output of the VaR engine—the PL vector or covariance
matrix—is generated, these risk results need to be stored at a chosen level of
granularity. Clearly, it is ideal to have information as granular as possible
(the finest being risk per deal by risk factor) so that the risk controller can
drill down with a fine comb for analysis. In practice, this is constrained by
computational time and storage space. Hence, the results are often aggre-
gated to a coarser level of granularity—for example, risk may be reported at
the portfolio level, by currency, or by product type.
The VaR reports box is an interface (often called a GUI or graphic user
interface) which the risk managers use for daily reporting or ad hoc inves-
tigations. This function also performs risk decomposition. Finer levels of
decomposition depend critically on the system’s ability to perform ad hoc
VaR runs on targeted portfolios under investigation.
VaR can be very computationally intensive. As an extreme example,
suppose we want to compute hsVaR at the finest possible granularity, and
there are 10,000 deals, 10,000 risk factors, 250 scenarios per risk factor
(these are very conservative estimates). The system will need to perform full
revaluation up to 25 billion times and may take a few days of computation
time. If some of the deals require Monte Carlo simulation for their revalu-
ation, the number can be even more staggering. In the fast-paced world of
investment banking, any VaR system that takes more than a fraction of a
day to calculate will become useless. The risk numbers will be obsolete by the
time the VaR report reaches its audience.
In practice, most deals only depend on a very small subset of risk factors
(for example a vanilla interest rate swap will only depend on a single cur-
rency yield curve), which could be used to considerable advantage through
efficient portfolio construction and an intelligent VaR engine design. For
example, such a VaR engine will not bother shocking risk factors that the
deal/portfolio does not depend on, and thus reduces the time taken for VaR
computation.
A less creative solution, given present-day technology, is to judiciously
select the granularity level for meaningful risk control and to invest in system
hardware. Thus, many banks are resorting to parallel computing to achieve
higher computational power.
Preprocessing
69

3.2 RISK FACTOR MAPPING
Rationale for Risk Factor Mapping
To understand why risk factor mapping is necessary, we reflect that VaR is a
portfolio risk measure. In the Markowitz portfolio theory, risk can be
diversified (lowered) when aggregated. The property of subadditivity allows
the portfolio total risk to be lower than the sum of risks of individual deals.
Well, we can calculate the risk of each deal (say by looking at its price
changes) and sub-add them using the Markowitz framework, but this is
inconvenient and unintuitive. Since many products are driven by common
risk factors it makes sense to map them to a set of risk factors and sub-add
the risk factors instead.
This is natural. Risk managers typically perform risk decomposition (to
analyze where the risk is coming from) based on risk factors, not based on
deals. Dealing with risk factors is also computationally efficient because the
number of deals can grow very large, but the risk factor universe remains
relatively fixed.
For example, a trader’s portfolio may contain thousands of FX option
deals and FX spot deals for hedging. The risk elements of each deal can be
mapped to just a few risk factor classes—FX spot, interest rates, and FX
volatility—and they are naturally netted within each risk factor itself. The
risk factors can then be sub-added to obtain portfolio risk.
Market Risks and Nonmarket Risks
Before we classify the risk factor universe, we need to appreciate that only
market risks can be measured by VaR adequately in a truly portfolio
diversifiable sense. Market risks are tradable risks that have continuous price
quotations. See Table 3.1. Hence, their time series, necessary basic building
blocks of VaR, are available.
In contrast, nonmarket risks such as those listed in Table 3.2 are non-
tradable. Without observed price series, this cannot be aggregated into
portfolio VaR (in a fully diversifiable sense). The risk models in this area are
still in their infancy, and data are unreliable. There are many attempts to
integrate all risks under the VaR umbrella, as seen in advances in enterprise
risk management. It is an appealing school of thought but such aggregation
is typically done as a simple VaR summation without considering the
correlation.
Sometimes the resulting VaR may not be meaningful. For example,
market risk VaR is often expressed with a 10-day horizon for regulatory
reporting. Can we really express operational risk over a 10-day horizon?
70
VALUE AT RISK METHODOLOGY

And what does it mean? Evidently, the industry is still far from a fully unified
framework for risks. The challenges of unification are discussed in Chapter 10.
Risk Dimensions
The columns in Table 3.1 refer to the various possible dimensions of risk for
a particular asset class. For example, options can be written on any asset
class, and that introduces an extra dimension of risk (on top of the common
price risk), that of volatility. Likewise, the innovation of credit derivatives in
the late 1990s created yet another dimension of risk, that of issuer default.
Some asset classes by nature of their cost of carry, exhibit a term structure or
yield curve. This means that each point (or pillar) on the yield curve is a
single risk factor with its own bid/ask quotation. We consider the term
structure a dimension of risk in its own right, because the curve features a
TABLE 3.1
Types of Market Risks
Risk Dimension (representation)
Risk Type
Price
Yield
Curve
Implied
Volatility
Default
Risk
Implied
Correlation
Foreign exchange
(FX)
Yes
Yes
Seldom
Interest rates
Yes
Yes
Seldom
Credit derivatives
and bonds
Yes
Yes
Yes
Seldom
Equities
Yes
Yes
Seldom
Commodities
Yes
Yes
Yes
Seldom
TABLE 3.2
Types of Nonmarket Risks
Risk Type
Example
Nontradable credit risk
Counterparty risk, illiquid loans (credit cards,
mortgages), commercial loans
Operational risks
Rogue trader, deal errors, security breach,
hardware breakdown, workplace hazard
Reputation risk (often
considered operational risk)
Law suits, fraud, misselling, violation of
regulations, money laundering
Liquidity risks
Bid/offer cost, market disruption, enterprise
funding risk, bank runs
Preprocessing
71

unique duration risk. Lastly, the innovation of basket and spread products
created a rather abstract dimension of risk to trade, that of correlation.
Crucially, to qualify as bona fide risk factors, the volatility and corre-
lation dimensions have to be implied from market product prices as opposed
to that derived from historical prices of the underlying. For example, it is
well known that option-implied volatility is often very different from his-
torical volatility. In particular, it can exhibit a volatility smile—where
options with strike prices away from at-the-money show progressively
higher implied volatilities. Implied volatilities of differing strikes are distinct
risk factors.
More subtly, implied correlation too is distinct from historical correla-
tion. One cannot use historical correlation in place of implied correlation,
any more than one can use historical volatility as an option’s risk factor.
Historical correlation is backward-looking (thus lagging) and provides the
general construction for portfolio diversification of any assets; that is, it is
already embedded in VaR.
In contrast, implied correlation exists only for exotic products that use
correlation as a pricing parameter. Since this parameter is tradable and
uncertain, there is (correlation) risk. Implied correlation is forward-looking
and often responds instantly to market sentiment. Despite the obvious need,
most banks do not include implied correlation risk into their VaR because
this risk factor is difficult to handle. There is a wall of challenges in terms of
modeling, technology, and data scarcity. This issue is discussed in Section
4.5. Except in that section, implied correlation risk factors will not be
considered.
Risk Factor Universe
To appreciate the size of the factor universe, let us look at a conservative
illustration of the factors used by a typical bank. Table 3.3 shows the count
of the number of factors for a combination of risk type and dimension. The
last column is the product of each row. The sum of the last column (52,369)
gives the size of the risk factor universe. Most banks will have a factor
universe twice this size or larger because of finer gradation.
Let us briefly explain each risk type. The upper half of the table displays
country-specific risk types. For simplicity, we consider only G-20 countries;
hence, there are 19 FX spot exchange rates (against the U.S. dollar). Banks
typically divide the maturity of deals into standard tenor buckets (or pillars)
for risk-mapping, for example: {1d, 1w, 1m, 3m, 6m, 9m, 1y, 2y, 3y, 4y, 5y,
7y, 10y, 15y, 20y}.
If the bank trades in FX options, these can be in any of the 19 currencies.
FX options are typically liquid from 1w to 5y maturity, so they fall into 10
72
VALUE AT RISK METHODOLOGY

tenor buckets. We also bucket the (implied) volatility risk into 5 strike levels
(with delta of 10, 25, 50, 75, 90). Thus, the number of possible risk factors
for FX options is 19  10  5 ¼ 950 combinations. The volatility plotted
on a tenor vs. strike grid is called the volatility surface.
For interest rate risky assets, we assume the curves for each currency
are quoted from 1d to 20y (15 buckets). At its simplest, there will be three
types of rate curves—the swap curve, the government curve, and the cross-
currency basis curve (against USD). The basis curve is the rate curve implied
from prices in the FX forward and currency swap markets. The basis curve
reflects the demand-supply for the foreign currency versus the USD. Hence,
for interest rate risk, we have 20  15  3 ¼ 900 risk factors.
Two vanilla (or basic) options for rates derivatives are cap/floor and
swaption. They constitute the most liquid instruments available for volatility
price discovery. A cap/floor is a basket of options to deliver a forward rate
agreement (FRA). A FRA is a forward (or deferred) start loan; for example, a
6x9 FRA is a 3-month loan that starts 6 months from trade date. The typical
volatility surface for cap/floor is defined on a grid of 11 tenors versus 7 strikes.
A swaption is an option contract to enter into a swap. Since the option
maturity is distinct from the underlying swap maturity, the swaption
TABLE 3.3
An Example of Risk Factor Universe
Risk Type
Currency/
Asset Name
Tenor
Buckets
Underlying
Tenor
Curve
Type
Strike
Levels
Number of
Risk Factors
(Country speciﬁc)
FX spot
19
19
FX volatility
19
10
5
950
Interest rates
20
15
3
900
Cap/Floor
20
11
7
1,540
Swaption
20
11
7
7
10,780
(Asset speciﬁc)
Equity spot
600
600
Equity volatility
600
5
10
30,000
Credit trading
100
5
2
1,000
Credit swaption/
bond option
100
1
1
100
Commodity
curve
24
30
720
Commodity
option
24
12
20
5,760
Total
52,369
Preprocessing
73

volatility surface is actually three-dimensional. For example, in the Table 3.3
we have 11 swaption maturities versus 7 swap maturities versus 7 strikes.
This is sometimes called the “vol cube.” With 20 vol cubes, one for each
currency, the number of risk factors swells to 10,780.
The lower half of Table 3.3 displays risk factors belonging to a specific
named asset such as that linked to a company, a debt issuer, or a specific com-
modity. Since there are many specific tradable assets in the world, the factor
universe is even more nebulous, and banks will limit the size of the set sufficient
to cover the markets they actually trade in.
In Table 3.3, we simplistically assume the bank trades an average of 30
stocks in each of the 20 countries. Hence, there will be 600 equity names.
For stock options, most listed options are quoted from 1m to 1y (five tenors)
and have 10 strike levels defined. Thus, we have 30,000 risk factors here.
The vanilla products in credit trading are risky bonds and credit default
swaps (CDS). The universe of issuers (or obligors) is large, but there were
probably just around 100 liquid names traded before the 2008 crisis. Hence,
100 is a reasonable estimate for a small bank. For risk management pur-
poses, it is important to distinguish between two curve types (for the same
issuer)—the CDS curve and the asset swap curve—because of the basis risk
between the two.1 The credit curve typically contains five tenors (1y to 5y)
although the reliable (liquid) point is really just the five-year. Options on
CDS and risky bonds are typically illiquid—the volatility surface is not
observable, and most banks will just estimate a single point volatility (five-
year, at-the-money strike) as the risk factor.
For commodity factors, we note that the Goldman Sachs commodity
index (a popular benchmark) has 24 components. These 24 commodities
represent broadly the commodity world. For most commodity futures, there
are typically 30 contract months (hence, 30 monthly tenors) in the forward
curve. Thus, we estimated 24  30 ¼ 720 factors. As for listed commodity
options, they are less liquid than futures and are commonly quoted up to one
year (12 monthly tenors) with 20 defined strike levels. Thus, commodity
options can contribute a huge number of factors—5760 in this case.
Each element in the risk factor universe has its own time series data that
are collected daily. A bank needs to maintain this data set in order to operate
its VaR engine.
In VaR applications, each deal may be mapped to multiple risk factors
that it is exposed to. For example, assuming the base currency for reporting
VaR is USD, a five-year risky bond denominated in EUR is mapped to the
following factors: EUR interest rate curve (up to 5y), EUR/USD spot
exchange rate, the credit curve of the issuer (up to 5y)—a total of 17 factors
from Table 3.3. We will see more examples in Chapter 4.
74
VALUE AT RISK METHODOLOGY

3.3 RISK FACTOR PROXIES
A proxy risk factor is a surrogate risk factor that is used when data history
for the actual held asset is not available. Proxies are often used in emerging
markets where the markets are illiquid and data history is sparse. Some asset
classes such as corporate bonds and credit derivatives are by nature illiquid.
These are issuer-specific securities that at times fall out of popularity with
investors. Hence, some parts of their price history may be stale or not
updated. Under these circumstances, a suitable proxy is required.
The handling of proxies requires much subjectivity. Under what cir-
cumstances should a proxy be considered? What makes a good proxy, and
must the proxy be scaled?
If prices for an asset exist but are not well-behaved (bad data), you are
better off using a proxy. Remember that in practice we are interested in the
portfolio level VaR; that is, how this single additional asset affects the overall
VaR. If the bad data series has many erroneous spikes, the portfolio VaR will
be overstated. On the other hand, if the bad data series has many stale
points, the portfolio VaR will be understated.
The idea that small errors from a single asset may be diversified away in
a large portfolio is dangerous. Small, unaccountable errors can build up in a
VaR engine over time, and it will be increasingly difficult to explain the
drivers of a bank’s VaR. Since early 2008, there has been a policy push by
regulators in favor of including issuer-specific or idiosyncratic risk into VaR.
Because many of the data series in this space are bad, it is questionable
whether the risk manager is including useful information or garbage into
his VaR system. The guide to staying on the right path is to always re-
member that VaR is only as good as the input data. So how good is this
particular data?
As an example, consider the data series of Lebanon five-year CDS
spread in Figure 3.2 for the period May 2007 to Apr 2008. During half of
that time the data set was illiquid and hence stale; therefore a proxy is called
for. The logical choice, based on geopolitical similarity will be the five-year
Israel CDS spread, which has better data history. On closer inspection, one
will be surprised to find the correlation of returns of the nonstale days to be
near zero. This seems to be a general observation in the credit trading market
and is due to illiquidity (compared to other asset classes). Returns are
uncorrelated even for issuers within the same industry and even though the
spread levels are themselves correlated. This poses less of a problem for VaR
since we are actually interested in correlation in the tail or quantile—as long
as large spikes in spreads are in sync between Israel and Lebanon, the VaR
will be correlated. Taking only the largest seven and smallest seven scenarios
Preprocessing
75

and computing correlation again, we find a correlation of 0.5—a more
intuitive result (that supports our choice).
Now, we just cannot replace the Lebanon time series with the Israel time
series in the VaR system—their volatilities are vastly different—the standard
deviations are 12bp versus 2.8bp respectively. We need to scale up the Israel
time series in the system by 4.5 times and rename it Lebanon proxy.
Using proxies introduces basis risk into VaR (addressed in Section 4.6).
This is still preferable to leaving bad actual data in the VaR system whose
impact it is impossible to quantify. The basis risk between Israel and
Lebanon CDS spreads should be conservatively estimated (even if only
roughly) and provisioned for using a reserve.2
3.4 SCENARIO GENERATION
In Section 3.2 we looked at how a risk factor universe is selected to represent
all tradable assets relevant for a bank. Each risk factor comes in the form of a
daily time series of level prices (or rates). But VaR uses returns as inputs since
it is the quantile of a return distribution of an observation period. Hence, the
returns series (or scenarios) need to be generated. In this book, we chose a
250-business day rolling observation period (or window) representing one
–
100
200
300
400
500
600
700
May-07
Jun-07
Jul-07
Aug-07
Sep-07
Oct-07
Nov-07
Dec-07
Jan-08
Feb-08
Mar-08
Apr-08
Lebanon 5yr CDS
Israel 5yr CDS (Scaled)
Israel 5yr CDS
FIGURE 3.2
Five-Year CDS Spreads for Lebanon and Israel
Source: Bloomberg Finance L.P.
76
VALUE AT RISK METHODOLOGY

calendar year. So the return series is represented by a scenario vector (of
length 250). The scenarios are indexed 1 to 250; by convention, scenario 1 is
the daily return at today’s close-of-business (COB) date, scenario 250 is the
return 250 days in the past from COB.
Once we have derived the scenarios, we can use a scenario to shift the
current price level (or base level) to the shifted level. Each deal in a portfolio
is then revalued at the current level and the shifted level. The difference
between the two valuations is the profit and loss (PL) for that scenario. Do
this for all 250 scenarios, and we get a PL vector, which is actually a dis-
tribution with 250 data points. We shall see in the next chapter that VaR is
just the quantile taken on the PL vector.
Different Returns
There are three common ways to generate a return series from a price series:
Absolute:
return ðiÞ ¼ rate ðiÞ  rate ði þ 1Þ
ð3:1Þ
shiftedlevel ðiÞ ¼ baselevel þ return ðiÞ
ð3:2Þ
Relative:
returnðiÞ ¼ rateðiÞ=rateði þ 1Þ  1
ð3:3Þ
shiftedlevel ðiÞ ¼ baselevel

1 þ returnðiÞ

ð3:4Þ
Log:
returnðiÞ ¼ ln

rateðiÞ=rateði þ 1Þ

ð3:5Þ
shiftedlevelðiÞ ¼ baselevel:exp

returnðiÞ

ð3:6Þ
where i ¼ 1, . . . , 250 is the time index (in reverse chronological order).
Here the terms rate and price are used interchangeably.
Absolute return is suitable for interest rate risk factors because it can
handle low and negative rates. A daily change from þ0.02% to þ0.06%
(a mere þ4bp absolute change) would imply a 200% relative change. In the
Preprocessing
77

2008 crisis, short-term overnight rates were set ultralow by central banks
to counter the liquidity crunch. When the market recovers, rates may rise to
a much higher base level even though the scenario vector remains largely
unchanged. Thus, by equation (3.4) the PL vector (and hence VaR) will
be overstated.
Sometimes rates can go negative momentarily. A change from 0.05%
to 0.04% (þ1bp rise) is reflected as a 20% fall by equation (3.3). Hence,
relative return gives the wrong sign when rates are negative.
For all other risk factors (that never go to low/negative prices) relative or
log return is preferred because it takes into account the base level. For
example, a gain of þ$1 from $1 to $2 is a lot riskier (100% increase)
compared to that of $100 to $101 (a 1% increase).
Negative Rates
Negative rates do occur for nondeliverable forwards (NDF). For some
semiconvertible currencies, government restrictions create a dislocation that
makes arbitrage difficult between onshore and offshore forward markets.
Without a corrective mechanism, the implied forward rates of offshore NDF
can go negative momentarily.
These negative implied rates are not fictitious—they do give rise to real
mark-to-market (MTM) PL should the position be liquidated. Nevertheless,
for the purpose of VaR, there are a few good reasons why we should floor
these rates at a small positive number (say at þ0.05%). Firstly, the rate
normally goes negative for very short periods and is corrected in a matter of
days. Secondly, the negative rates typically happen at the very short end of
the curve (say less than one-week tenor) where the duration risk is tiny.
Thirdly, even though vanilla product pricing can admit negative rates, many
pricing models for complex products cannot. Since most banks run exotic
books hedged with vanilla products, the overall risk of the portfolio can be
misrepresented if negative rates are permitted.
More generally, certain quantities should not admit negative values in
order to satisfy the so-called nonarbitrageable condition. These are interest
rates, forward interest rates, option volatilities, and probability of defaults
(as implied from default swaps). Since VaR uses historical scenarios, if the
current level of a risk factor is slightly above zero and a historical scenario
happens to be a large fall, it is possible for that scenario to produce a neg-
ative state. Such a scenario is unfeasible because should it happen, arbi-
tragers would be able to make a riskless profit from the situation. Banks
usually have methodologies to correct such undesirable scenarios in their
VaR system. These technicalities are beyond the scope of this book.
78
VALUE AT RISK METHODOLOGY

3.5 BASIC VaR SPECIFICATION
Simplistically VaR is just the loss quantile of the PL distribution over the
chosen observation period. In the industry, different banks use different
specifications for their VaR system. Often a firm-wide VaR number is
reported to the public or the regulator. In order to attribute any meaning to
the understanding or comparison of VaR numbers, it is important to first
specify the VaR system. A succinct way to do this is to use the format in
Table 3.4.
As a prelude, let’s describe a reasonable VaR system specification, which
is workable in my opinion and which we shall use as a base case throughout
the book. See Table 3.5.
TABLE 3.4
VaR System Speciﬁcation Format
Item
Possible Choice
Valuation method
Linear approximation, full revaluation, delta-gamma approx.
Weighting
Equal weight/exponential weight in volatility/correlation
VaR model
Parametric, historical simulation, Monte Carlo
Observation period
250 days, 500 days, 1,000 days
Conﬁdence level
95%, 97.5%, 99%
Return deﬁnition3
Relative, absolute, log
Return period
Daily, weekly
Mean adjustment
Yes/no
Scaling (if any)
Scaled to 10 days, scaled to 99% conﬁdence level
TABLE 3.5
VaR System Speciﬁcation Example
Item
Selection
Valuation method
Full revaluation
Weighting
Equal weight
VaR model
Historical simulation
Observation period
250 days
Conﬁdence level
97.5%
Return deﬁnition
Log return
Return period
Daily
Mean adjustment
Yes
Scaling (if any)
Scaled to 10-day holding period and 99% conﬁdence level
Preprocessing
79

The advantages of a full revaluation and historical simulation model will
be covered in Chapter 4. A rolling window of 250 days (one-year) is short
enough to be sensitive to recent changes (innovation)4 in the market. Had we
chosen a 1,000-day (four-year) window, the VaR will hardly move even
when faced with a regime change. This is undesirable.
At 97.5% confidence, the number of exceedences in the left tail is six to
seven data points. At higher confidence (say 99%), the number of excee-
dences becomes too few to be statistically meaningful (two to three points).
And at lower confidence, the quantile may not be representative of tail risk;
we may be measuring peacetime returns instead.
Log returns with flooring of rates (no negative rates allowed) have
the advantages mentioned in Section 3.4. Daily data is used because it is the
most granular information5 and will be more responsive than, say, weekly
data. The specified scaling is a regulatory requirement. For the bank’s
internal risk monitoring, the scaling is often omitted.
The Case for Mean Adjustment
Mean adjustment is the process of deducting the average of the PL vector
from the VaR quantile. Then, the PL distribution, which may be skewed to
one side, becomes centered (see Figure 3.3). Some banks report VaR based
on a mean-adjusted basis; others don’t. This book adopts the former
approach.
0
0.02
0.04
0.06
0.08
0.1
0.12
–15
–10
–5
0
5
10
15
VaR = –5mio
Mean-adjusted
VaR = –10mio
Mean-adjusted distribution
Original distribution
FIGURE 3.3
Mean Adjustment Applied to PL Distribution
80
VALUE AT RISK METHODOLOGY

Mean adjustment is necessary to make the VaR measure consistent.
Let’s see why.
1. In mathematical finance we have chosen to define risk in terms of
uncertainty of returns. (We could have chosen a different metric such
as leverage, duration, and so on, but that is a different starting point.)
As long as there is a distribution, there is uncertainty of returns, hence
risk. Since we measure this uncertainty as dispersion from the mean (or
expected value), it follows that mean adjustment is necessary.
2. Suppose you have a return distribution for a super bullish stock which is
so right skewed such that all return observations are positive; that is, it
went up (if only slightly) every day for the last 250 days. Without mean
adjustment, your VaR will be zero since by its definition, VaR is the
quantile of loss. It is not meaningful to say that the holder of this stock
runs no risks, since clearly a stock is a risky asset. Mean adjustment will
remove the positive drift from this distribution.
When implementing mean adjustment, it is important to deduct the
mean at the level of the dollar PL vector, not at the return scenario vector.
In practice, this means the adjustment is done after product revaluation.
For linear products it doesn’t matter—mean adjustment done at the scenario
vector (return distribution) or done at the PL vector (PL distribution),
produces the same VaR. However, in the presence of options, they are
generally unequal.
To see why it matters for options, suppose the underlying stock has a
return distribution with a mean (say 0.1%) skewed very slightly to the left.
If the portfolio contains call options, the portfolio’s PL distribution (after
revaluation) can be skewed to the right (say with mean þ0.5%) as a result of
the loss protection provided by the options. So we mean-adjust the PL dis-
tribution to the left. But had we performed mean adjustment in the return
distribution instead, we would have mistakenly adjusted to the right
resulting in a very different VaR outcome.
Forthesakeofsimplicity,inthisbook,wewilloftenmean-adjustthereturn
distribution unless the example involves an actual dollar PL distribution.
NOTES
1. The basis exists because of structural and liquidity differences between the two
products. The asset swap curve is derived from risky bond yields, whereas CDS
spreads are like an insurance premium on that issuer. CDS are traded in a sep-
arate and often more liquid market.
Preprocessing
81

2. A provision or a reserve is money set aside to cushion against any risks that are
not well captured or understood. In this case it is the risk of “not knowing the
difference in risk between the two risk factors.”
3. Different return deﬁnitions may be more suitable for different risk factors. Hence
a real-world VaR system may contain a mixture of return deﬁnitions.
4. New changes in market prices (or incoming data) are occasionally called inno-
vation in academia.
5. Intraday data is seldom considered because price gaps that occur during opening
and session breaks often make the data nonstationary.
82
VALUE AT RISK METHODOLOGY

CHAPTER 4
Conventional VaR Methods
V
aR is simplistically the loss quantile of the PL distribution. There are
many ways to generate the distribution and to compute the quantile. By
far the three most common methods used by banks are the parametric VaR,
historical simulation VaR, and Monte Carlo VaR. Their popular use stems
from the practical balance between simplicity and validity.
A VaR system must be simple enough to be intuitively understood by
top management, regulators, and operators of the system. Abstract models
that require specialized domain knowledge to comprehend seldom make it
to conventional use. This is because risk management is a huge team effort—
there are often hundreds of staff managing different aspects of the risk
architecture and in different geographic locations. A simple model provides
a common language in the chain of command.
The validity of a VaR model must be tested against the market. Until
recently, VaR models have been generally accepted (for lack of a better
alternative) on the grounds that the market behavior generally falls within
the prediction of VaR. Statistically speaking, almost all (say 99%) of the
time, the system is predictive. But the 2008 crisis has highlighted that
this peacetime tool breaks down during times of crisis, just when it is
needed most.
The best way to learn about VaR is to implement it on an Excel
spreadsheet. We will illustrate all three VaR methods using the same test
portfolio comprising a stock index, an option, and a bond. This test set will
let us illustrate all the nuances of VaR in Section 4.4. Some basic product
knowledge is assumed; the absolute beginner is referred to the excellent
textbook by Hull (2008).
The formal definition of VaR, equation (2.13) can also be expressed as:
VaRq ¼ F1ðqÞ
ð4:1Þ
83

where F1, the quantile function is defined as the inverse of the CDF of the
return variable X. The c% confidence level VaR is then just the q ¼ (1  c)
quantile. The final step of any VaR method will involve “taking the quan-
tile” as per (4.1).
4.1 PARAMETRIC VaR
Parametric VaR (pVaR) or variance-covariance (VCV) VaR was popularized
by J.P. Morgan’s RiskMetrics. The original methodology was published in
1994 and evolved over the years as it took hold as the industry standard.
Strictly speaking, RiskMetrics proposed the modeling of VaR using the
normal distribution and the exponentially weighted moving average
(EWMA) volatility measure.1 For simplicity, we will model the volatility
using standard deviation of returns (i.e., using equal weights) and combine
the risks using a variance-covariance method. The model specification is per
Table 4.1.
The valuation method of pVaR is by the sensitivity approach. This
means that to derive the PL of each scenario, we do not reprice a deal using
its pricing formula (such as the Black-Scholes equation for an option deal);
instead, we get an approximation of the PL by multiplying the deal’s risk
factor sensitivity with the risk factor’s return scenario.
Risk sensitivities or Greeks are the bread and butter of risk manage-
ment. Risk managers look at Greek reports across portfolios and product
types every day. Sensitivities measure the change in valuation (i.e., the PL) of
a deal due to a unit change in the risk factor that the deal is exposed to. The
number of Greeks and their mathematical intricacy have expanded in recent
TABLE 4.1
pVaR Model Speciﬁcation
Parameter
Selection
Valuation method
Linear approximation
Weighting
Equal weight
VaR model
Parametric
Observation period
250 days
Conﬁdence level
97.50%
Return deﬁnition
Relative
Return period
Daily
Mean adjustment
N.A.
Scaling (if any)
No
84
VALUE AT RISK METHODOLOGY

years because of product innovation. Here we will consider only the basic
sensitivities that are relevant for pVaR as listed in Table 4.2.
The logic of pVaR can be understood in terms of a Taylor series
expansion of a product’s price P around its initial price P0. P is a function of
risk factor Y.
P  P0 ¼ f 0ðY0ÞΔY þ 0:5f 00ðY0ÞðΔYÞ2 þ : : :
ð4:2Þ
where f 0(.) ¼ dP/dY is the first derivative and f 00(.) ¼ d2P/dY2 the second
derivative of the pricing function f(.) evaluated at the starting point Y0.
Δ Y ¼ Y  Y0 represents a shift (or perturbation) away from Y0. Note the
following:
1. If the product is linear (such as a stock), the second and higher-order
terms do not exist (are zero).
2. f 0(.) gives the first-order or delta sensitivity. Depending on the risk
factors involved, it is called delta, PV01, vega, or Credit01 (see Table
4.2). For example, for a stock option, f(.) is the Black-Scholes formula.
If Y is the spot (share price) risk factor, then f 0(.) is called the delta. If Y
is the volatility risk factor, then f 0(.) is called the vega.
3. For yield-curve-sensitive products (such as a bond), if ΔY represents a
one basis point (1bp ¼ 0.01%) parallel shift of the entire yield curve,
then f 0(.) is called the PV01 or present value per basis point. We can
make finer gradations by shifting individually the t-year points on
the curve to calculate the so-called bucketed PV01s. These are needed
for pVaR.
TABLE 4.2
Basic Risk Sensitivities
Basic
Sensitivity
Deﬁnition
Application
Delta
PL due to 1 unit change in price
Price-based products
and their options
Gamma
PL due to 1 unit change in Delta, that is,
second-order sensitivity to price
Options
Vega
PL due to 1% change in volatility
Options
PV01
PL due to þ1bp change in rates
Interest rate products
Convexity
Second-order PL adjustment due to þ1bp
change in rates
Interest rate products
Credit01
PL due to þ1bp change in credit spread
Credit risky products
Conventional VaR Methods
85

4. For nonlinear products, the second-order derivative f 00 (.) is called the
gamma for an option and convexity for a yield curve product.
5. The higher-order terms are progressively smaller for a small change
ΔY and are often neglected. PVaR quite often just uses the first (linear)
term to approximate the PL (¼ P  P0); this is called the linear
approximation or delta approach. One can improve accuracy by
also including the second-order term; this is called the delta-gamma
approach.
The test portfolio is shown in Table 4.3. To calculate pVaR, we need
each deal’s sensitivities to all the relevant risk factors. The sensitivities in
Table 4.3 are calculated using discrete differences. For example, to calculate
vega for deal-2, we reprice the option twice using the Black-Scholes for-
mula—once using the current parameters, and once with the volatility
bumped up by 1%. The vega is just the dollar change in valuation due to this
bumping. The sensitivity numbers of the test deals are worked out in
Spreadsheet 4.1.
Each risk factor (more precisely its scenario vector) is assumed to be
normally distributed. For large portfolios this assumption is justified by the
TABLE 4.3
Test Portfolio
Deal 1
Equity
Deal 2
Put Option
Deal 3
Bond
Risk feature
Linear
Risk feature
Nonlinear
Risk feature Convexity
Asset
Dow Jones
index
Asset
Dow Jones
option
Asset
2-year
T-bond
Notional
$1,000,000
Notional
$1,000,000
Notional
$1,000,000
Index level
10,718
Maturity
1 year
Maturity
2 years
Strike
10,718
Coupon
(p.a.)
5%
Volatility
12.07%
1-year rate
4.836%
2-year rate
4.8295%
(Valuation)
Valuation
$1,000,000
Premium
491.7
Price
100.1
Valuation
$45,875
Valuation
$1,000,964
(Sensitivity)
Delta
$93.3
Delta
(0.45)
PV01 (1y)
$(4.8)
Delta ($)
$(42.3)
PV01 (2y)
$(190.6)
Vega ($)
$3,794
PV01 (1y)
$(4.6)
86
VALUE AT RISK METHODOLOGY

Central Limit Theorem. There are four risk factors affecting our portfolio:
1-year rate, 2-year rate, index price (spot), and index volatility.2 The vola-
tility of each risk factor, σi (where i ¼ 1, . . . , 4), is computed using the
standard deviation of 250 days return data. To account for the dependence
structure, a 4×4 correlation matrix is computed from the four scenario
vectors.3
To aggregate the risk, we apply Markowitz portfolio theory, similar to
equation (2.36). The portfolio variance is given by:
σ2 ¼ wTΣw
ð4:3Þ
where Σ is the correlation matrix, and w the vector of stand-alone VaR
given by:
wi ¼ αδiσi
ð4:4Þ
where δi is the linear sensitivity to risk factor-i and σi is the volatility of risk
factor-i. The constant α is the standard normal deviate related to the con-
fidence level c. In Excel function syntax α ¼ NORMSINV(1  c). So for
97.5% VaR, α ¼ 1.96.
Multiplying by 1.96 is just a way to “take the quantile.” This goes
back to the theoretical meaning of VaR—a 97.5% VaR is defined as a
0.025-quantile; under the normal distribution, this roughly corresponds to
a loss of 2 sigma. This is a handy rule of thumb since sigma can easily be
estimated.
The portfolio pVaR is then the square root of equation (4.3). The pVaR
implementation for the test portfolio is shown in Spreadsheet 4.1.
Weakness of pVaR
Firstly, the assumption of normality is a major model weakness of pVaR. As
such—fat-tailness and skewness—two important behaviors of price returns
during market crises are ignored (see Chapter 7).
Secondly, the sensitivity approach (without doing full repricing of deals)
gives only a first-order approximation of the PL. In other words, pVaR is a
local risk measure—the sensitivity f 0 (Y0) is computed for and true only at
the current level Y0. This is okay for small shifts, but when the shift ΔY is
large, the second and higher-order terms which we have ignored become
significant. This means that nonlinear risks such as optionality and con-
vexity are ignored by pVaR.
Conventional VaR Methods
87

Thirdly, when the correlation matrix gets too large, it runs into non-
positive semidefinite problems (or becomes corrupted). This problem is
insidious and warrants some explanation.
An n-by-n square matrix Σ is positive semidefinite if all its eigenvalues
are non-negative. Eigenvalues λi are the elements of the diagonal matrix M
given by the “Eigen decomposition”:
Σ ¼ eTMe
ð4:5Þ
where:
M ¼
λ1
0
::
0
0
λ2
::
0
::
::
::
::
0
0
::
λn
0
B
B
B
@
1
C
C
C
A
and e is a matrix whose rows are called eigenvectors and eT is the transpose of e.
As a working definition, a nonpositive semidefinite matrix is one
whereby its Cholesky matrix (see Section 2.11) is undefined. For example,
consider a correlation matrix of three assets: A, B, and C. Suppose the
correlation of (A, B) and (A,C) are both þ0.9, and correlation (B,C) is 0.9,
then the Cholesky decomposition will fail because the relationship is logi-
cally impossible.4 The requirement of positive semidefiniteness is a relational
constraint to ensure that the correlation matrix is logical.
What happens to portfolio VaR when the correlation becomes illogical?
Try replacing the correlation matrix in Spreadsheet 4.1 with the upper
matrix in Table 4.4. You can check that the Cholesky matrix is undefined.
TABLE 4.4
Nonpositive Semideﬁnite Matrix
Dow Jones
USD 1y Swap
USD 2y Swap
Index Vol
Dow Jones
1.000
0.008
(0.900)
0.900
USD 1y swap
0.008
1.000
0.749
0.020
USD 2y swap
(0.900)
0.749
1.000
(0.055)
Index vol
0.900
0.020
(0.055)
1.000
Dow Jones
USD 1y Swap
USD 2y Swap
Index Vol
Dow Jones
1.000
0.528
0.682
0.999
USD 1y swap
0.528
1.000
0.454
0.080
USD 2y swap
0.682
0.454
1.000
0.493
Index vol
0.999
0.080
0.493
1.000
88
VALUE AT RISK METHODOLOGY

The computed portfolio VaR is suspect and worse still—there is no obvious
warning that something is amiss. Next, replace the correlation with the
lower matrix in Table 4.4, and change the sign of the stand-alone VaR for
Index volatility (a vulnerable situation where risk factors are almost perfectly
correlated and positions are offsetting). The portfolio variance becomes
negative, a meaningless number.
When do correlation matrices become ill-behaved? When the dimension
of the matrix grows too large relative to the length of the time series, the
relational constraints among its elements become increasingly restrictive.
The integrity of the matrix becomes vulnerable to corruption. At some point,
the positive semidefinite condition may break down under certain situations
such as:
1. When time series data is tampered with, for example, backfilled during
a holiday, or interpolated over missing points.
2. When correlation elements are calculated using time series of different
length, or when correlation elements are rounded off.
3. When the correlation matrix is estimated by traders subjectively (as
opposed to being calculated statistically).
4. When the correlation is calibrated from prices of traded instruments.
5. When the correlation matrix is perturbed (i.e., shifted) for the purpose
of a stress test.
6. When a correlation number is close to þ0.99 and there is a long-short
offsetting position, the integrity of the matrix becomes vulnerable.
7. When decay weights are used for correlation estimation, making the
effective time series period even smaller compared to the matrix
dimension.
As a solution, researchers have proposed adjustment schemes that
slightly modify the matrix elements to ensure that the correlation matrix
remains positive semidefinite. For example, see Finger (1997) and Rebonato
(1999). This is a critical check for any VaR systems that explicitly use cor-
relation matrices.
4.2 MONTE CARLO VaR
We have seen that the pVaR method can only handle a normal distribution.
However, it is well-known that markets are fat-tailed (not normal) during
stressful times. If we know the exact functional form of the distribution, we
can use Monte Carlo simulation to generate scenarios from this known
distribution and compute Monte Carlo VaR (mcVaR).
Conventional VaR Methods
89

Table 4.5 shows a simple mcVaR model specification that we will use in
this section. Our mcVaR example uses the same test portfolio as per Table
4.3 and is also implemented in Spreadsheet 4.1.
Referring to the spreadsheet, let’s trace the following general steps in the
mcVaR calculation:
1. The correlation matrix Σ and volatilities σi are computed from the risk
factors’ historical returns. The factors are indexed i ¼ 1, 2, 3, 4.
2. Compute the Cholesky matrix of Σ and use it to generate correlated
random number draws εt from a normal distribution N(0,1) (see Section
2.11). In the example, we draw 2,000 paths (t ¼ 1, . . . , 2000).
3. Use εt to simulate the return scenarios for the risk factors, Rt ¼ (dX/X)t.
We need to specify a model for this stochastic process.
4. Calculate the shifted level Xt ¼ (1 þ Rt)X0 where X0 is the current level
for the risk factor. Do this for all i risk factors.
5. The PL for scenario-t is just the present value of the deal repriced at the
shifted levels minus the present value of the deal repriced at current
levels. Do this for all t to arrive at the PL vector (it is a 1×2,000 vector).
Calculate the PL vector for each deal.
6. Sumthe PL vectorsfor all deals byscenario toderivethe portfolioPL vector.
We take the 0.025-quantile for this PL vector to give the 97.5%-VaR.
In step (2), the larger the number of simulations, the more precise the
mcVaR will be. Typically, banks may simulate more than 10,000 paths. To
model fat tails, we will need to draw from a specified fat-tailed distribution.
This is discussed in Section 4.4.
In step (3), we have chosen geometric Brownian motion (GBM) to model
the process as per equation (2.44). Since our target horizon for VaR is one
TABLE 4.5
McVaR Model Speciﬁcation
Parameter
Selection
Valuation method
Full revaluation
Weighting
N.A.
VaR model
Monte Carlo
Observation period
2,000 paths
Conﬁdence level
97.50%
Return deﬁnition
Relative
Return period
Daily
Mean adjustment
N.A.
Scaling (if any)
N.A.
Distribution model
Normal
Process model
Geometric Brownian motion
90
VALUE AT RISK METHODOLOGY

day, Δt ¼ 1/365. If a 10-day VaR is required for regulatory reporting, we
just set Δt ¼ 10/365; that is, no time scaling is required. For short horizons
one can assume the drift μ is zero, and since σ is calculated directly from
daily data (i.e., not annualized), there is no need for the factor
ﬃﬃﬃﬃﬃﬃ
Δt
p
in the
second term. With this simplification, equation (2.44) becomes (4.6). This is
implemented in the spreadsheet.
Zero drift GBM : ΔXt
Xt
¼ σεt
ð4:6Þ
Although the GBM model is suitable for stocks, it is less appropriate for
interest rates, which exhibit mean reversion. Short rate models are required.5
One class of short rate models is called the one-factor model, in that there is
only one stochastic variable εt. There are many specific models under this
class; γ ¼ 0.5 gives the popular Cox-Ingersoll-Ross (CIR) model (1985),
γ ¼ 0 gives Vasicek model, γ ¼ 1 gives the lognormal model.
One-factor model : Δrt ¼ kðθ  rtÞΔt þ σrγ
t εt
ð4:7Þ
where rt is the short rate, σ is the constant volatility, k and θ are parameters
to be estimated. The first term controls drift and mean reversion while the
second term controls volatility. The challenge in modeling complicated
stochastic processes is that the parameters need to be calibrated using
numerical methods such as moments matching. A computer algorithm is
used to find the best values for the parameters that will match the moments
of the simulated process with its theoretical moments. Unfortunately, cali-
bration is a highly unstable and error-prone exercise; for example, a slightly
different way of coding the same theory can result in different results.
In step (5), full revaluation is applied using the pricing formulas for each
product. In the spreadsheet we used the Black-Scholes equation to price the
option and cash-flow discounting to price the bond. With exact solutions
(unlike that in pVaR), mcVaR will account for nonlinear risks of derivatives.
Often, all the risk factors are simulated together for each scenario rep-
ricing; this means that the final VaR can be broken down (for analysis) by
deals but not by individual risk factors. Hence, risk decomposition is limited
in scope, unless the bank takes the huge task of running mcVaR separately
for each risk factor.
Weakness of mcVaR
The major difficulties in mcVaR are the risk of model misspecification and
technological dependency.
Conventional VaR Methods
91

The return distribution needs to be modeled. While we are sure the
normal distribution is too idealistic to hold, we are less certain which fat-
tailed distributions, of the many in the academia, are good choices. Should
they be based on extreme value theory, mixed normal distribution, skew-
normal distributions, and so on? Evidence suggests that empirical distribu-
tions evolve over time, and different markets show different distributions.
How do we account for such variations?
In addition, the evolutional (stochastic) process of the risk factor needs
to be modeled. There are many classes and subclasses of models that attempt to
describe observed market dynamics. With the exception of GBM, most
models will require calibration to determine their parameters. Unfortunately,
the parameters themselves are seldom constant, so recalibration is periodi-
cally required. Do we really know if an adopted model is the right one?
Monte Carlo simulation is also computationally intensive because full
revaluation of all deals is performed. This is especially so for portfolios with
many deals, nonlinear products, and exotic derivatives. In the latter case, if the
repricing of products itself requires Monte Carlo simulation, we have a situ-
ation called nested Monte Carlo simulation—a simulation (inner step) within a
simulation (outer step). The outer step generates the scenarios. For every outer
step, many inner step paths are generated to price the product. Hence, it often
takes a long time and many simulated paths for the portfolio VaR to converge
to a stable result. Banks normally resort to parallel computing and numerical
shortcuts—such as antithetic variable technique, control variates technique,
and importance sampling technique—to speed up computation.
Monte Carlo simulation uses pseudo random numbers generated by the
computer. The RAND() function in Excel does this. These are not truly
random numbers but a sequence that looks as independent and identically
distributed (i.i.d.) random as possible. The sequence is deterministic and
reproducible since it is generated using a known algorithm by the computer.
Figure 4.1 illustrates a problem with such random numbers—notice some
points tend to clump together to form patches, leaving out large areas. (You
can easily check this by plotting two series of RAND() in a scatter plot in
Excel.) This clumping effect is caused by random sampling with replace-
ment—after a point is drawn it is replaced back into the distribution (to
ensure i.i.d.) so that the same point has equal chance to be picked again.
(That increases the odds of a patch forming at locations that had been
previously picked.) The clumping effect is undesirable since it will take a
much longer time to fill up the whole region evenly.
This is overcome by using a quasi-random number generator which is
designed to select random numbers away from the last drawn number. It is
also called a low-discrepancy sequence or Sobol sequence. Quasi-Monte
Carlo converges much faster but one drawback is that it is no longer i.i.d.
92
VALUE AT RISK METHODOLOGY

As seen above, mcVaR faces many technological challenges. We list the
major ones:
1. Error correction algorithm to ensure that the correlation matrix is
positive semidefinite.
2. Calibration of the model parameters is prone to numerical errors and
instability.
3. Sobol sequence to generate random numbers and their inverse trans-
formation to generate the specified distribution.
4. Numerical techniques to speed up Monte Carlo computation.
5. Exotic derivatives in the portfolio may require a nested Monte Carlo
simulation approach.
Consequently, a huge investment in overhead is required to get the
technology right. The biggest danger comes from the accumulation of errors
from the various moving parts of the mcVaR system. If not designed care-
fully, the estimation error in the final VaR could become a function of
technology and model selection, and this is often opaque.
4.3 HISTORICAL SIMULATION VaR
Historical simulation VaR (hsVaR) is a popular method used by banks and is
potentially the most accurate. Pérignon and Smith (2010) found that 73% of
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
FIGURE 4.1
Clumping Effect of Random Numbers
Conventional VaR Methods
93

banks that disclose their VaR method report using historical simulation. It
can be seen as a Monte Carlo simulation method that uses historical (rather
than random) samples for its scenarios. It overcomes many of the problems
encountered by pVaR and mcVaR. Let’s look at the salient features of hsVaR.
First, it uses an empirical distribution (historical data) that bypasses the
need to assume any distribution models. This takes care of fat tails and
skewness because we let the data dictate the shape of the distribution. Note
that there will be one distribution for every risk factor.
Second, hsVaR computes the PL of each deal by full revaluation (unlike
pVaR). This will capture the nonlinear risk posed by the presence of non-
linear derivatives. Given the complexity of today’s derivatives, most port-
folios are nonlinear. The repricing of deals avoids the error arising from
delta approximation.
Third, risk aggregation is done by summation of PL across scenarios to
form the portfolio VaR. The dependence structure is accounted for in this
way. Hence, there is no need to maintain a large correlation matrix or to deal
with positive semidefinite issues. This simple cross summation is easy to
understand intuitively and allows for easy risk decomposition for the pur-
pose of reporting.
Table 4.6 shows a simple hsVaR model specification that we shall use in
this section. Our example uses the same test portfolio as per Table 4.3 and
the same Spreadsheet 4.1.
Let’s summarize the steps in computing hsVaR as illustrated in the
spreadsheet:
1. A 250-day scenario (i.e., return) vector is derived from the risk
factor’s historical prices (x1, . . . , x251). Hence, the returns are
TABLE 4.6
hsVaR Model Speciﬁcation
Parameter
Selection
Valuation method
Full revaluation
Weighting
Equal weight
VaR model
Historical simulation
Observation period
250 days
Conﬁdence level
97.50%
Return deﬁnition
Relative
Return period
Daily
Mean adjustment
Yes
Scaling (if any)
No
94
VALUE AT RISK METHODOLOGY

rt ¼ (xt/xtþ1)  1 where t ¼ 1, . . . , 250 is the time index in reverse
chronological order.
2. For each risk factor, calculate the shifted level of scenario-t, x0t ¼
(1 þ rt)x1 where x1 is the current level for that risk factor.
3. For a deal, the PL for scenario-t is just the present value (or PV) of the
deal revalued at the shifted level x0t minus the present value of the deal
revalued at the current level x1. Do this for all t scenarios to derive the PL
vector for that deal (a vector of 1 × 250). Repeat this for all deals.
In practice, a product’s pricing formula g(.) may be a function of multiple
risk factors say xt, yt, zt. In this case, the deal needs to be revalued at
shifted levels, that is, g(x0t, y0t, z0t).
4. Sum the PL vectors for all deals by scenario to derive the PL vector for
the entire portfolio, which is actually a PL distribution with 250 points.
We take the 0.025-quantile of this distribution and perform mean-
adjustment to derive the 97.5%-VaR.
Note that in step (2) we always apply the historical shifts to current price
levels and current positions. This is like replaying the tape of history on
today’s position to see what the daily PL would have looked like. This, in
essence, is what hsVaR is about.
For additional accuracy, a bank may also account for the theta effect or
time decay in hsVaR. By definition, VaR is the risk of loss moving one day
into the future. Derivatives and cash-flow products will be affected by time
decay since their maturity will be shortened by one day. This is accounted for
by simply rolling the pricing engine’s date one day forward (to COBþ1)
during full-revaluation in step (3) such that: PL ¼ PVCOBþ1(shifted state)—
PVCOB(current state).
The final step (4) can easily be implemented using the Excel function:
PERCENTILE(., .)-AVERAGE(.) which gives the portfolio VaR after mean-
adjustment (see Section 3.5).
The hsVaR method of cross summation (by scenario) allows for very
flexible risk decomposition and diagnosis. For example, if we are interested
in knowing the risk due to FX spot alone, we can shift only the FX spot risk
factor and repeat the above steps. If we are interested in VaR breakdown by
portfolio, we can do the PL vector summation for each portfolio separately.
If we are interested in the impact of a single deal, we can exclude the PL
vector of that deal from the summation, and see the difference, which is
known as the incremental VaR. We can even answer complicated questions,
such as what will be the impact on overall portfolio VaR contributed by
volatility alone if we put on a particular option deal to hedge the portfolio,
assuming the FX spot level is 20% higher than the current level?
Conventional VaR Methods
95

Weaknesses of hsVaR
HsVaR is subject to a few subtle weaknesses below. Note that (1) and (2) are
also true for VaR in general.
1. VaR assumes returns are i.i.d. In other words, the distribution of the last
250 days can be projected one day into the future—past risk is a good
predictor of future risk (if only for a day). The problem with this
assumption is that it breaks down when there is a regime shift such as at
the onset of a crisis. By definition, a regime shift marks the arrival of
(breaking) new information absent from past data. Hence, VaR will
always lag major price shocks and is only a good tool during peacetime
when changes are gradual and representative of the past.
2. VaR is only a point estimate. It is a single number chosen to represent
risk for the entire distribution and contains no information on the
shape of the tail to the left of the loss quantile. The extreme quan-
tile itself is notoriously hard to estimate with so few data points in
the tail.
3. The aggregation of PL vectors by scenario is affected by data quality.
The price series must not be unusually quiet (or stale) and must not be
too choppy (have spikes). Otherwise, the portfolio VaR becomes
unstable. This is because the tail of an empirical distribution is not
as smooth as a theoretical one as used in pVaR and mcVaR. This
problem is more pronounced for short observation period, high
confidence level, or small portfolio size. Thus, the sampling error
(precision) of hsVaR is generally poorer. The reason why hsVaR
has inferior precision is because it is plagued by the “curse of
dimensionality.”6
4. HsVaR can never be larger than the largest loss in the sample used.
There is no real reason why a potential future loss cannot be more
severe than the worst loss seen in recent history. In the next chapter, we
shall see how a clever use of weights can overcome this problem.
4.4 ISSUE: CONVEXITY, OPTIONALITY, AND FAT TAILS
An important topic increasingly emphasized by regulators is the so-called
risks not in VaR (or RNIV). It is accepted by regulators and banks that, in
practice, not all market risk can be captured in VaR. As a compensating
control, regulators normally require banks to quantify and report these
missing risks, and hold capital (reserves) against them. The rest of this
chapter will explore some of these issues.
96
VALUE AT RISK METHODOLOGY

Convexity
Convexity is a residual second-order risk that affects products that are pre-
dominantly made of cash flows. It is interesting to trace the origin of con-
vexity. Because the pricing of these products involves cost-of-carry (or equally
cash-flow discounting) the yield (y) appears in the denominator of the pricing
formula. Consider the pricing formula for a simple fixed coupon bond:
PðyÞ ¼
100
ð1 þ yÞn þ
X
n
i¼1
c
ð1 þ yÞi
ð4:8Þ
where P(y) is the price, c is the coupon, i ¼ 1, . . . , n is the remaining time to
each cash flow in fractional years.
Because y is in the denominator, the profile of the price-yield relation-
ship is necessarily convex (like the shape of the P ¼ 1/y graph). The actual
plot of (4.8) for a 20-year bond with a coupon of 5% is shown as the dotted
line in Figure 4.2. Convexity is a measure of the dispersion of cash flows
(coupons) across time. So the 20-year bond, having more dispersed coupons,
is more convex than a 5-year bond. See Figure 4.2.
HsVaR and mcVaR use equation (4.8) for full revaluation of bond deals
so the convexity risk is already included. However, pVaR uses an approx-
imation based on the Taylor expansion. Inserting equation (4.8) into
equation (4.2) keeping only the first two terms, we obtain:
PðyÞ  Pðy0Þ
Pðy0Þ
¼ Dðy  y0Þ þ 0:5γðy  y0Þ2
ð4:9Þ
50
70
90
110
130
150
170
190
0%
1%
2%
3%
4%
5%
6%
7%
8%
9%
10%
Price
Yield-to-maturity
Actual price
(20yr)
5yr price
(comparison)
Delta + gamma
estimate
Delta
estimate
FIGURE 4.2
Convexity of the Price-Yield Relationship of a Bond
Conventional VaR Methods
97

PðyÞ  Pðy0Þ ¼ ðPV01Þðy  y0Þ10, 000 þ 0:5Pðy0Þγðy  y0Þ2
ð4:10Þ
where D the modified duration and γ the convexity (also called gamma) are
measures of first-order (linear) and second-order (quadratic) sensitivities.
Equations (4.9) and (4.10) show that duration is really just another way
to express PV01 risk. Duration is measured in units of years and can be
thought of as the average weighted time of the bond (weighted by discounted
cash flows). The reader can refer to Hull (2008) for further study. Here, it
suffices to know that modified duration D can be calculated by the Excel
function MDURATION(.) and convexity is given by:
γ ¼ 1
P
X
n
i¼1
iði þ 1Þc
ð1 þ yÞiþ2 þ nðn þ 1Þ100
ð1 þ yÞnþ2
"
#
ð4:11Þ
These formulas are used in Spreadsheet 4.2 to generate Figure 4.2.
HsVaR and mcVaR refer to shifts along the dotted line to derive its PL vector.
Such PL occurs when the yield y moves away from the current yield y0 (this
base case is marked by the vertical line). In contrast, pVaR refers to shifts
along the linear line (marked “delta estimate”) to estimate its PL vector. The
error between full revaluation and linear approximation (deviation between
the dotted and linear lines) becomes apparent for large shifts away from y0.
One way to mitigate the error is to use the delta-gamma approximation,
which also takes into account the second-order convexity term. As seen in the
figure, the deviation between the delta-gamma line and dotted line is much
smaller.
Note that convexity (curvature) benefits the bond holder (long position).
For an adverse shift in yield (to the right), actual loss is smaller than the
straight line. On the other hand, for a yield shift to the left, actual gains
are larger than the straight line. This can also be seen in equation (4.9)—the
convexity term is always positive owing to the square. Conversely, convexity
hurts the short sellers.
Spreadsheet 4.2 investigates the VaR impact. Our test portfolio contains
only a 20-year zero coupon bond. First note that pVaR and mcVaR are
roughly the same under normal distribution. We then multiply the volatility
of the 20-year rate risk factor by a factor of 10 to exaggerate the scenario
shifts and hence convexity. As a result, pVaR overstates the risk as compared
to mcVaR for a long position because pVaR (delta approach) ignores the
effect of convexity. For short bonds, pVaR understates the true risk because
it does not get hurt by short convexity. This can be tested in the spreadsheet
by changing the sign of the bond notional to negative.
98
VALUE AT RISK METHODOLOGY

In a nutshell, our investigation shows that convexity is a tiny residual
risk except for long-dated bonds and under extreme movements. This risk
will be omitted if pVaR is used.
Optionality
An option allows the buyer the right, but not the obligation, to buy (call
option) or sell (put option) an asset at a predetermined price (the strike). All
options have the benefit of flooring the downside loss for the buyer (but not
for the seller). In return for that benefit, the seller is paid a premium. The
present value or “price” of an option refers to this premium. Options are
very common in the financial markets, sometimes embedded in bonds,
deposits, swaps, and other derivatives.
The flooring of risk has two effects—the risk profile becomes nonlinear
(curved) and the PL distribution becomes skewed to the positive side (which
is good for the buyer).
Figure 4.3 shows the payoff diagram for a put option where both strike
(K) and spot (S) are at 750 or at-the-money.7 The dotted line shows the
actual PL profile of the put prior to maturity calculated using the Black-
Scholes formula, while the straight line is the linear estimate of the PL profile
estimated from delta. The tangent point where they touch is the current spot
price. As we move away (shift) from the spot, there is a significant deviation
between the two lines. Thus, a linear estimate is only good when shifts are
not large; that is, it’s a local risk measure.
–20
0
20
40
60
80
100
120
140
160
600
650
700
750
800
850
900
950
1000
1050
1100
Option premium
Spot price
Premium
Final payoff at maturity
Current spot 
FIGURE 4.3
Nonlinearity of an At-the-Money Vanilla Put Option
Conventional VaR Methods
99

McVaR and hsVaR use the actual profile, while pVaR uses the linear
approximation. By the same reasoning as before, pVaR will overstate the
risk for long options (does not benefit from curvature) and understate
the risk for short options (does not get hurt by short curvature).
Spreadsheet 4.3 is used to derive Figure 4.3. The spot is at 750. If we
change the strike to 1,000 (ITM) the deviation between the two lines
becomes negligible. At a strike of 600 (OTM), the deviation between the two
lines becomes exaggerated. This shows that the risk of a deep-ITM option is
very much like the underlying; it’s linear. Whereas, when the option goes
OTM, optionality risk becomes more important.
Spreadsheet 4.3 also contains a test portfolio of one put option with a
strike of 10,718 (ATM). If we change the strike to 30,000 (ITM), pVaR and
mcVaR are roughly the same. At 9,000 strike (OTM), pVaR overstates
mcVaR by more than 20%.
Figure 4.4 illustrates the benefit of loss protection offered by the option.
The distributions are generated using mcVaR from the spreadsheet. The left
panel shows the ATM case; the distribution is largely symmetric. The right
panel shows the OTM case where optionality becomes an important effect—
the PL distribution becomes skewed to the right and the loss side is buffered.
Optionality is a serious challenge to the VaR metric. Its effects are sig-
nificant and often skew the distribution in a way that threatens the basic
assumptions of normality and subadditivity. Once the distribution is skewed
(and hence not elliptical), we cannot estimate correlation with confidence.
0
50
100
150
200
250
300
350
400
(17,509)
(12,363)
(7,218)
(2,072)
3,073
8,218
13,364
18,509
Frequency
PL
ATM put (k = 10718)
0
50
100
150
200
250
300
350
400
450
500
(242)
(62)
117
297
476
656
835
1,015
PL
OTM put (k = 8000)
FIGURE 4.4
Skewness of PL Distribution for Put Option
100
VALUE AT RISK METHODOLOGY

Also the curvature effect (gamma) is substantial when we deal with exotic
options and consider the volatility smile. This gives rise to nonlocal risk
features (located away from the current state) in a complex way not well
captured by a point estimate such as VaR. For example, a knock-out option8
may have a barrier that is just slightly beyond the movement represented by
VaR. In this case, the risk is not captured.
Fat Tails
The 2008 crisis has shown that the danger of fat tails cannot be under-
estimated. Spreadsheet 4.4 illustrates a VaR implementation similar to
Spreadsheet 4.1, except that the peacetime data of 2005 has been replaced by
the crisis data of 2008. To focus solely on the effects of a fat tail, we consider
only a linear portfolio of stocks. In the mcVaR sheet, there is a choice of
using the normal distribution or a fat-tailed (mixed-normal) distribution.
Under the normal distribution, pVaR is roughly the same as mcVaR.
HsVaR is larger than both because in 2008 the stock market returns were
generally fat-tailed and skewed. This will hurt the long stock position and is
manifested as a higher hsVaR. As expected, pVaR and mcVaR failed to
capture these higher moments because of the normal assumption.
McVaR can account for fat tails if a suitable distribution is applied. One
simple way to generate random numbers with zero mean, unit variance but
with a high degree of kurtosis is to use a mixture of normal model. The idea
is that a fat-tail bell-shaped distribution can be created by randomly mixing
variables from two normal distributions with different variances.
To do this, draw a random outcome y of either 1 or 0, with probability p
and (1  p). Draw a sample z from the standard normal N(0,1) distribution.
Designate σ1 and σ2 as the standard deviations for the two normal dis-
tributions. To independently mix them, if y¼ 1 then let x ¼ σ1z; if y ¼ 0 then
let x ¼ σ2z. To normalize the mixed distribution of X such that its variance is
1, we set:
VarðXÞ ¼ pσ2
1 þ ð1  pÞσ2
2 ¼ 1-σ2 ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  pσ2
1
1  p
s
ð4:12Þ
Thus, we can choose p and σ1 to achieve the desired level of kurtosis
given by:
KurtðXÞ ¼ 3ðpσ4
1 þ ð1  pÞσ4
2Þ
ð4:13Þ
Conventional VaR Methods
101

This is implemented in Spreadsheet 4.4 for all four risk factors. The
two parameters are chosen such that the theoretical kurtosis matches the
observed kurtosis from the market. Figure 4.5 shows conceptually how
the mixed-normal (the bar chart) is created using two normals with different
sigmas. One distribution dominates the peak, and a second distribution
dominates the two tails. The resulting distribution has a peak and tails that
are thicker than normal. It is symmetric and fat-tailed. If the mixed-normal
model is chosen, mcVaR will be larger than pVaR. For a more methodical
application of normal-mixture in mcVaR, the astute reader can refer to
Zangari (1996).
4.5 ISSUE: HIDDEN CORRELATION
It is common knowledge that correlation or the dependence structure is
inherent in Markowitz portfolio theory. The correlation allows for sub-
additivity, which acts to reduce (diversify away) portfolio risk. But there is
another type of correlation which is less known, that is, the implied corre-
lation used as a parameter to price correlation products. Unlike the former,
this is actually a risk factor. (See Section 3.2—Risk Factor Mapping.)
For these bespoke products, price series are seldom observed, and banks
typically exclude this risk factor from VaR and simply assume implied
correlation as a constant.
An alternative is to replace the unobservable implied correlation with
the historical correlation. We can calculate the latter using a short
0
100
200
300
400
500
600
700
–4.0
–3.0
–2.0
–1.0
0.0
1.0
2.0
3.0
4.0
Frequency
Higher volatility
dominates the tails
Lower volatility
dominates the peak
FIGURE 4.5
Mixture of Normal Distributions
102
VALUE AT RISK METHODOLOGY

observation window (such as 50 days) to reduce its lag and to make it more
sensitive to regime changes.
We illustrate this in Spreadsheet 4.5 using a spread option example.
A spread option has a payoff that depends on the difference (or spread)
between two underlying assets, F1 and F2, and can be priced analytically by
modifying the Black-Scholes equation to include correlation ρ. We consider a
call spread option with payoff MAX(F1-F2-K,0) where K is the spread strike.
The hsVaR is computed for the case where implied correlation is stochastic
(proxied here using a 50-day historical correlation) versus a case where the
correlation is assumed to be constant. To focus solely on the effects of
correlation, we kept the other parameters—such as maturity, rate, and
volatility—constant. In a realistic situation, all parameters are variable.
We want to test the responsiveness of VaR when a regime switch is
encountered. To do so, we simulate random paths that are highly correlated
(ρ ¼ 0.5), then switch the direction of the correlation abruptly (to ρ ¼ 0.5)
at halfway through the time series. Figure 4.6 illustrates the generated time
series and the resulting VaRs. The VaR is extremely responsive to the regime
break if the correlation parameter is stochastic. VaR rises rapidly once the
break is encountered.
Figure 4.6 also shows VaR for two other cases where correlation is not
included as a risk factor. One of them, the implied correlation, is updated
(marked) daily, and for the other the correlation is kept constant. In practice,
the correlation is updated infrequently as and when prices are observed. The
dangers of excluding implied correlation as a risk factor are apparent—
the VaR will be significantly understated, and regime breaks when they
0
0.5
1
1.5
2
2.5
3
3.5
4
60
70
80
90
100
110
120
130
140
1
73
145 217 289 361 433 505 577 649 721 793 865 937
VaR (% of notional)
Price series
Days
Asset1
Asset2
VaR (stochastic
Corr)
VaR (updated Corr)
VaR (constant Corr)
Regime switch
FIGURE 4.6
HsVaR of Spread Option—Constant versus Stochastic Correlation
Conventional VaR Methods
103

occur will not be detected. Fortunately, the problem is confined to correla-
tion products.
Most banks do not have a systematic way to handle implied correlation
risk factors, especially if correlation trading is not their core business.
Implied correlations are quoted by traders based on consensus gut feel and
hence will often violate positive semidefiniteness. The storage of these
implieds can be a technological nightmare because the matrix size will grow
rapidly9 as new correlation products are traded. When this happens, data
that were not previously collected will need to be back-filled for VaR. This
can be an impossible task. Thus, some banks treat correlation as a param-
eter, which is updated occasionally (not as a risk factor), and they com-
pensate for this missing risk by keeping a model reserve or additional capital.
A good trade-off is to use historical correlation as a proxy risk factor
and compute the required correlation pairs on-the-fly during deal revalua-
tion. This bypasses the need for storage of correlation matrices.
4.6 ISSUE: MISSING BASIS AND BETA APPROACH
Basis Risks
The term basis generally refers to the difference between two related markets,
and basis risk is the uncertainty of this basis. Usually, the trader goes long one
market and shorts another in the so-called arbitrage, spread, or basis trading.
Some examples are the spot versus futures basis, the CDS versus asset swap
basis, the basis between two points on the yield curve, cross-currency basis
between two curves of different currency denominations, the basis between
one security versus another, the basis between a stock and its benchmark
index, and so on. Can VaR account for such diverse basis risks?
The answer depends on how we map risks. To the extent that the risk
factor mapping differentiates the two markets constituting the basis, basis
risk will be captured. Sometimes, data limitations cannot afford us this level
of granularity. This is particularly serious for emerging markets. Also, cer-
tain securities are naturally illiquid, and continuous price quotation is not
available, such as in credit trading, where securities are issue-specific. For
example, an off-the-run (old issue of a) corporate bond often falls out of
favor with investors, and its price becomes stale, but an on-the-run (newly
issued) bond is often more liquid. This gives rise to a basis between the two
bonds that is not well-captured. In fact, most banks will risk-map (or
benchmark) the bonds of the same issuer and roughly of same maturity to
the on-the-run bond, effectively ignoring such basis risk.
104
VALUE AT RISK METHODOLOGY

Basis risk is notorious for causing blowups at financial institutions.
Examples include the LTCM debacle (1998) and more recently the J.P.
Morgan CIO trade (2012). Not surprisingly, there is a push from regulators
for banks to quantify the magnitude of missing basis risks for prudential
supervision. This is a somewhat self-contradictory (but necessary) proposi-
tion. For how can one quantify a risk that has little or no data? Some banks
estimate this by assuming worst case scenarios (stress test) or by looking at
other similar securities that do have the basis data (proxying).
Beta Approach
Certain markets have a vast number of risk factors, so risk managers often
benchmark them to a manageable subset using the beta approach (see Sec-
tion 2.8). This is a common practice for name-specific instruments such as
equities and, to some extent, for credit default swaps. In fact the Revision to
Basel II Market Risk Framework (2009) prescribed that “Positions in
individual securities or in sector indices could be expressed in ‘beta-
equivalents’ relative to this market-wide index.”
Equation (2.37) relates the return for security-i to the return of the
market index by way of a beta coefficient. Assets with higher betas are more
sensitive to the general market. Sharpe (1964) proved that under equilibrium
conditions of capital markets, αi ¼ Rf (1  βi) where Rf is the risk-free rate.
In this case, (2.37) becomes:
EðRiÞ  Rf ¼ βi

EðRMÞ  Rf

þ EðεiÞ
ð4:14Þ
where E(.) stands for expectation and the time subscript has been dropped
for brevity. This relationship is the famous Capital Asset Pricing Model
(CAPM), which states that the return (over the risk-free rate) of an asset
consists of two uncorrelated components—a systematic component (first
term) and a name-specific component (second term). The risk decomposition
for asset-i is given by the variance formula:
VarðRiÞ ¼ β2
i VarðRMÞ þ VarðεiÞ
ð4:15Þ
Specific risk can be diversified away if a portfolio P is very large; that is,
Var(εP) ¼ 0. When the portfolio is small, Var(εP) > 0, hence the risk cal-
culated using the beta approach will always understate the true risk. This is
illustrated by taking the square root of (4.15) with i ¼ P:
Conventional VaR Methods
105

σðRPÞ>βPσðRMÞ
ð4:16Þ
Spreadsheet 4.6 is an illustration of pVaR and hsVaR of a portfolio of
two stocks calculated using the beta approach. It can be shown that
regardless of the portfolio composition (you can even include short sales) the
beta approach understates true VaR. The lesson is that when using the beta
approach, the risk manager needs to be mindful of missing basis risks when
the portfolio is small or not well diversified.
4.7 ISSUE: THE REAL RISK OF PREMIUMS
Consider what happens when a trader purchases an out-of-the-money
option of notional $1 million. Suppose the pricing formula tells him the
premium is $3,600 and he books this deal, which the risk system values at
þ$3,600. However, he needs to pay $3,600 cash (premium) upfront,
hence, on a net basis his inception PL is zero. (A fairly priced derivative gives
zero PL at inception.) If the market remains unchanged during the life
of the option, the option value will decay from þ$3,600 to 0 at expiry
(an option value cannot go below 0). His maximum loss will be floored at
$3,600 PL.
Now suppose the trader is willing to write off this maximum loss by
recognizing this loss upfront as $3,600 PL and providing (paid) for this
loss.10 Is there any risk left in his position?
0
100
200
300
400
500
600
700
800
900
1000
Volatility = 200%
Volatility = 12%
9,099
8,637
8,174
7,711
7,249
6,786
6,324
5,861
5,398
4,936
4,473
4,011
3,548
3,085
2,623
2,160
1,698
1,235
772
310
–153
–616
–1,078
–1,541
–2,003
–2,466
–2,929
–3,391
FIGURE 4.7
Simulated PL Distribution of OTM Option
106
VALUE AT RISK METHODOLOGY

Firstly, let’s verify that one can never lose more than the premium (on a
long position). Figure 4.7 shows two PL distributions for the option deal.
The distributions are right-skewed because of the downside protection. As
we increase the simulated volatility to an extreme (say 200%) the skew
becomes larger, but the left tail can never go below $3,600 no matter how
extreme prices behave.
Secondly, we note that VaR satisfies a mathematical property called
translation invariance (see Section 7.6):
VaRðX þ kÞ ¼ VaRðXÞ  k
ð4:17Þ
Inother words,addingcashktoportfolioX willreducetheportfolioriskby
k. This means that if we pay upfront the full potential loss k, the PL distribution
gets shifted (translated) to the right by k. As a result, the entire distribution in
Figure 4.7 will be in the positive territory regardless of market conditions.
Since VaR is defined as the loss quantile of the distribution, the VaR
becomes zero. It really boils down to how you define risks. If risk is defined
as uncertainty in PL, then yes, there is risk. But if risk is defined as a loss
quantile, as required in VaR and risk capital, then there is no risk. In this
case, one can argue that deals that are fully written down can be managed
outside the VaR regime. Often, this is used as justification to trade in
sophisticated options that a bank does not have the system capacity to risk-
manage. If a trader holds only long options and is willing to write off the
premium, a risk manager is likely to allow the trading of that position.
4.8 SPREADSHEET EXERCISES
4.1 Historical simulation, Monte Carlo, and parametric VaRs are the three
most basic VaR models. More advanced models are often just variants
of these basic models. Aim: illustrates the hsVaR, mcVaR, and pVaR
for a test portfolio of three deals—a stock, an option, and a bond.
Note: the historical data used is for 2005, representing “peacetime”
market conditions. As expected, all three methods produced roughly
the same VaR.
4.2 Convexity is a (second-order) curvature risk peculiar to bonds and cash-
flow-based products. It is not captured by simple parametric VaR (delta
approach). Aim: illustrates the convexity effect using a bond’s price-
yield relationship, and shows its impact on pVaR. Note: the test port-
folio contains only a 20-year bond and the volatility of the 20-year rate
has been multiplied by 10 to exaggerate the convexity effect. McVaR
(which includes convexity) is smaller than pVaR.
Conventional VaR Methods
107

4.3 Option nonlinearity is not captured by pVaR because of the linear
approximation used. Aim: illustrates the nonlinearity of a put option’s
payoff, and the skew imposed on the PL distribution as a result of the
option’s downside protection. A VaR calculator, containing an option,
illustrates its different impact on mcVaR and pVaR. Action: change the
strike to investigate the effects of OTM and ITM options.
4.4 The three VaR methods—pVaR, mcVaR, and hsVaR—have different
abilities (or inability) to account for fat tails; pVaR fails to account for
fat tails, mcVaR models them theoretically, while hsVaR uses empirical
distributional over a short window. Aim: explore the impact of the crisis
conditions of 2008 on the three VaR methods for a single stock (linear
position). Note: The hsVaR is larger than the other two VaRs. If the
fat-tail distribution is chosen for mcVaR, the mcVaR will also be larger
than pVaR.
4.5 Implied correlations of multiunderlying derivatives are often assumed to
be constant (or not used as risk factors) in the computation of VaR. This
is often due to data and technological limitations. Aim: illustrates the
impact on VaR when implied correlation is stochastic (not kept con-
stant) by using an example of a spread call option. Note: Monte Carlo
simulation is used to generate two price series, and a regime break is
introduced in the middle. The hsVaR is found to be extremely
responsive in the case of stochastic correlation compared to the constant
correlation case. Action: Press the first button to regenerate the paths,
the second button to recalculate the hsVaRs.
4.6 The beta approach of representing risk factors in VaR can be shown in
theory to underestimate true risks. Aim: Verify the understatement of
risks by the beta approach of VaR using actual data. PVaR and hsVaR
are computed for a portfolio of two stocks with and without the beta
method. Action: change the weights of the two stocks or even short the
stocks, and note that the beta VaR will always understate the true VaR.
NOTES
1. To get a precise deﬁnition of RiskMetrics and how it has evolved, refer to
RiskMetrics, 2001, “Return to RiskMetrics: The Evolution of a Standard.”
2. The index volatility is an example of implied volatility that is observed from the
market (it’s traded); this is different from the volatility σ, which is a statistic on
returns.
3. A more advanced pVaR system may involve exponentially weighting the vola-
tility or correlation or both.
108
VALUE AT RISK METHODOLOGY

4. An analogy: Suppose there is a room with two exits (directions). There is a ﬁre,
and three persons exit the room, but all three claimed that they exited the room
from a different direction from another. This is logically impossible.
5. Likewise, to model the evolution of volatility risk factors, stochastic volatility
models are required. A popular model is the Heston model (1993).
6. This problem occurs when the length of the sample (normally less than 500
days) is much smaller than the dimension of the system (typically more than
50,000 risk factors). Consider the following: It takes at least two points to deﬁne
a one-dimensional object (a line), it takes at least three points to deﬁne a two-
dimensional object (a plane) and it takes at least four points to deﬁne a three-
dimensional object (a three-sided pyramid). The number of sampled points
required to represent (in fullness) an N-dimensional object is indeed very large
as N grows. Since we only have one year’s data, the hsVaR will be very inac-
curate and erratic. Monte Carlo simulation was historically developed to
overcome the curse of dimensionality by randomly ﬁlling up the N-dimensional
space.
7. When S ¼ K, an option is termed at-the-money (ATM). When an option is
proﬁtable (S > K for calls, S < K for puts) it is in-the-money (ITM). When an
option is at a loss (S < K for calls, S > K for puts) it is called out-of-the-money
(OTM).
8. This is an option that loses all its value when the spot price reaches a certain
target level called the barrier, causing a sudden discontinuous large loss.
9. In fact, the number of matrix elements will grow quadratically. For example a
bank sells equity spread options and basket options to customers. Suppose the
bank can quote on 30 Hong Kong stocks at the moment and wishes to expand
by an additional 3 stocks. This increases the number of pairwise correlations by
93 (from 435 to 528); that is, a 21% increase and each one is a time series.
10. In practice, this may happen if a small bank does not have the proper risk
system to book the option and is long only. One way is to record only the
premium part ($3,600) and at expiry, the ﬁnal value of the option (which can
only be larger than zero).
Conventional VaR Methods
109


CHAPTER 5
Advanced VaR Methods
I
n this chapter, we will explore some advanced VaR models; there are many
others.1 Academics were the first to systematically catalog the weaknesses
of VaR and improve the basic VaR model in order to make it consistent with
observed market behaviors. Unfortunately, the added layer of mathematical
complexity makes implementation and common understanding difficult. As
such, these models are slow in gaining industry acceptance.
One common trait of these models is the reliance on a strong mathe-
matical foundation and its basic assumptions such as stationarity and being
independent and identically distributed (i.i.d.). We will introduce the main
ideas of these models without delving into the mathematical details as far as
possible. Our goal is to cover just enough ground to be able to illustrate a
simple example on a spreadsheet.
5.1 HYBRID HISTORICAL SIMULATION VaR
It is important for hsVaR to use an observation period long enough for the
shape of the return distribution to be well-captured. But because hsVaR uses
a rolling history of empirical data, it encounters a dilemma when estimating
the quantile—with a long history the method becomes insensitive to market
changes,2 while with a short history it encounters estimation problems—
there are too few data points that define the quantile. The RiskMetrics
version of pVaR on the other hand is very sensitive to market innovation,
because of the use of exponentially declining weights on past data to esti-
mate the volatility.
The hybrid historical simulation VaR approach proposed by Boudoukh,
Richardson, and Whitelaw (1998) combines the best of both worlds. It
combines the exponentially-weighed-moving-average (EWMA) scheme of
RiskMetrics with hsVaR, so that it can have the benefit of using a long
history and yet achieve market responsiveness.
111

The hybrid VaR is implemented in three steps:
1. Let r(t) be the realized return from time (t  1) to t. To each of the
recent K returns: r(t), r(t  1), . . . , r(t  K þ 1) assign a weight
[(1  λ)/(1  λK)]λ0, [(1  λ)/(1  λK)]λ1, . . . , [(1  λ)/(1  λK)]λK-1
respectively.
2. Rank the returns in ascending order.
3. To obtain p% VaR, start from the lowest return (largest loss) and
cumulatively add the weights until p% percentile is reached. Linear
interpolation is used when necessary to get the exact point of p% between
two adjacent points. The return at this point is the VaR’s return.
Spreadsheet 5.1 illustrates the implementation of hybrid VaR using
K ¼ 250anddecayfactorλ ¼ 0.98(asintheoriginal1998paper).Wecalculate
the97.5%confidenceVaRfortheDowJonesindexfortheperiod2005to2008;
hence p ¼ 2.5%. Figure 5.1 shows the resulting hybrid VaR versus hsVaR.
Hybrid VaR is more reactive than hsVaR to significant moves in market
prices, but this unfortunately makes hybrid VaR very volatile as well. This
key weakness of hybrid VaR can be a hindrance for the purpose of setting
regulatory risk capital, where some degree of stability is desirable. Also,
there is little room to fine-tune λ for hybrid VaR. At λ ¼ 0.98 it becomes
jagged, at lower values (say λ ¼ 0.94), it becomes sawtoothed. You can try
this out in Spreadsheet 5.1.
The hybrid VaR is unsmooth because it effectively shortens the sampling
period to capture the effects of stochastic (or changing) volatility. At λ ¼ 0.98,
the most recent observation is weighted 2%—which means that a large
incoming observation will skew the results; that is, make the VaR jump ranks.
–
2,000
4,000
6,000
8,000
10,000
12,000
14,000
(1,500)
(1,300)
(1,100)
(900)
(700)
(500)
(300)
(100)
29-Dec-05
29-Dec-06
29-Dec-07
29-Dec-08
Index (points)
VaR (points)
HsVaR
HybridVaR
Dow Jones index
FIGURE 5.1
Hybrid VaR versus Historical Simulation VaR (λ ¼ 0.98)
112
VALUE AT RISK METHODOLOGY

5.2 HULL-WHITE VOLATILITY UPDATING VaR
Hull and White (1998) proposed a more natural and smoother way to
capture stochastic volatility. Their insight was the observation that when a
probability distribution is scaled by an estimate of its volatility, it is still
approximately stationary. This suggests that historical simulation VaR can
be scaled by a function of volatility without sacrificing stationarity. The
basic idea is that VaR should be a stronger function of today’s volatility than
the volatility further in the past. This approach is similar to the hsVaR,
except that each return observation in the past is scaled by the ratio of
today’s volatility to the volatility at the time of observation. To calculate the
volatility, Hull-White uses the EWMA method with λ ¼ 0.94 as recom-
mended by RiskMetrics.
The Hull-White VaR can be easily implemented in three steps:
1. Volatilities are estimated using the EWMA approach for the last 250
days, σ1, σ2, . . . , σ250 where the time index subscript represents the
number of days into the past.
2. The historical percentage returns r1, r1, . . . , r250 are scaled as: r1(σ1/
σ1), r2(σ1/σ2), . . . , r250(σ1/σ250).
3. Take the quantile of the scaled returns to obtain the Hull-White VaR.
The 97.5% Hull-White VaR is implemented in Spreadsheet 5.2 using the
same Dow Jones index data as before. Figure 5.2 shows that Hull-White
–
2,000 
4,000 
6,000 
8,000 
10,000 
12,000 
14,000 
(1,200)
(1,000)
(800)
(600)
(400)
(200)
–
3-Jan-05
3-Jan-06
3-Jan-07
3-Jan-08
3-Jan-09
Index (points)
VaR (points)
HW VaR
hsVaR
Dow Jones index
FIGURE 5.2
Hull-White VaR versus Historical Simulation VaR (λ ¼ 0.94)
Advanced VaR Methods
113

VaR is more responsive than hsVaR, which lagged the crash during the
credit crisis (July 2008 to March 2009). Relative to hybrid VaR, it is rea-
sonably continuous for the typical range of decay factor λ(0.94 to 0.99).
Note that mean-adjustment is not done in our examples for Hull-White
VaR and hybrid VaR. For this small adjustment, the exercise is nontrivial
especially if done at the portfolio level. The Hull-White VaR approach also
performs well under statistical tests of goodness such as unbiasedness and
reduced bunching (see Section 8.3).
An appealing feature of Hull-White VaR is that the VaR can be larger
than the largest loss in the sample used. Logically, there is no reason why a
potential future loss should be constrained by the maximum loss in the
recent past sample, as in the case of an equally weighted hsVaR and hybrid
hsVaR.
5.3 CONDITIONAL AUTOREGRESSIVE VaR (CAViaR)
VaR is just a loss quantile at the left tail, a point estimate. It tells us very little
of the behavior of exceedences to the left of this cutoff point, nor is it
accurately estimated, since there are too few observations in the left tail used
to define the quantile. In the example of 97.5% VaR using 250 days’ data,
only six to seven data points in the left tail decide the VaR number.
Engle and Manganelli (2004) proposed the conditional autoregressive
value-at-risk (CAViaR) model that could overcome these problems. It used
the method of QRM estimation in Section 2.13. CAViaR proposes to model the
process governing the quantile itself as a function of other variables. In QRM,
data of the entire distribution is used for the estimation process; hence, the data
scarcity problem is avoided. Moreover, the specification of the QRM regres-
sion explicitly models the tail behavior.
The starting point of CAViaR is the empirical observation of volatility
clustering; in particular, extreme returns tend to be autocorrelated. Sharp price
movements during a crash or mania tend to persist over a period as opposed to
occurring randomly over time. As a result, large returns tend to cluster
together temporally. Hence, VaR, being the tail quantile, must also exhibit
properties of autocorrelation; an autoregressive specification is thus required.
In the most general form of CAViaR, today’s VaR is modeled as a linear
regression of previous VaRs, previous returns Y, and some unknown para-
meters β’s which need to be estimated. In the seminal CAViaR paper by
Engle and Manganelli (2004), four specifications or models were proposed.
Adaptive Model:
VaRt ¼ VaRt1 þ β  hit
ð5:1Þ
114
VALUE AT RISK METHODOLOGY

where the hit function given by
hit ¼ IðYt1 <  VaRt1Þ  θ
where I(Yt1<VaRt1) is the indicator function, θ the chosen quantile for
VaR, β’s the parameter(s) to be estimated. The other three models were:
Symmetric Absolute Value:
VaRt ¼ β1 þ β2VaRt1 þ β39Yt19
ð5:2Þ
Asymmetric Slope:
VaRt ¼ β1 þ β2VaRt1 þ β3ðYt1Þþ þ β4ðYt1Þ
ð5:3Þ
Indirect GARCH(1,1):
VaRt ¼ ðβ1 þ β2VaR2
t1 þ β3Y2
t1Þ1=2
ð5:4Þ
where the short notation means: (Y)þ ¼ max(Y, 0), (Y) ¼ min(Y, 0).
For the purpose of illustration, we shall use the simple one-parameter
adaptive model. Since equation (5.1) is an iterative equation, we need the
VaR at the starting point t ¼ 1, which we simply take as: VaR(1) ¼
NORMSINV(θ)  STDEV(.); that is, the simple pVaR estimate. For 95%-
VaR, θ ¼ 0.05 and VaR(1) ¼ 1.65σ. Then VaR(t) for subsequent days can
be calculated iteratively.
VaRðtÞ ¼ VaRðt  1Þ þ
	 βð0:95Þ if IðYt1 <  VaRt1Þ ¼ 1
βð0:05Þ if IðYt1 <  VaRt1Þ ¼ 0
ð5:5Þ
Notice the behavior of the adaptive model: whenever the loss Y exceeds
VaR we should immediately increase VaR (by 0.95β), but when Y does not
exceed VaR, we should decrease it very slightly (by 0.05β). This model
does not account for the magnitude of exceedence, only its frequency; that is,
the increment/decrement is a constant regardless of the degree of exceedence.
The parameters β’s of the CAViaR model are estimated using quantile
regression (see Section 2.13). For the adaptive model, we estimate the β that
minimizes the objective function:
min
β
1
T
X
T
t¼1
½θ  IðYt < VaRtÞ½Yt  VaRt
ð5:6Þ
Advanced VaR Methods
115

where T is the number of observations. The minimization problem for this
simple example can easily be done using Excel Solver. Spreadsheet 5.3
illustrates this for the case of Dow Jones index for 5% quantile. The
resulting CAViaR is plotted against conventional hsVaR for comparison (see
Figure 5.3).
In a sense CAViaR has transformed the VaR problem of quantile
determination (with data scarcity issues) to the problem of functional
regression in the form of QRM. In this new form, one has to deal, instead,
with the subjectivity of model specification (we have to presume we know
the process for VaR) and parameterization. For example, in the adaptive
model CAViaR shown in Figure 5.3, the shape of the downtrend is straighter
than the uptrend. That is governed by the specification of the adaptive model
and would be very different if we had chosen some other specification such
as one of the other three models mentioned. But how do you choose and
justify which models to use for different asset classes which clearly behave
differently? Because of its generality and flexibility, CAViaR lacks the kind
of standardization and simplicity that the risk industry favors.
5.4 EXTREME VALUE THEORY VaR
Extreme Value Theory is a field of applied statistics traditionally used in the
insurance industry and now increasingly applied in the area of operational
risk. It provides estimation techniques to forecast extreme events with low
probability of occurring.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Dec-82
Dec-86
Dec-90
Dec-94
Dec-98
Dec-02
Dec-06
Percentage loss
CAViaR
HistSim VaR
FIGURE 5.3
CAViaR versus Historical Simulation VaR (θ ¼ 0.05)
116
VALUE AT RISK METHODOLOGY

The purpose of this section is to provide a brief foretaste of what EVT
can do in the area of VaR. Entire books have been written on the subject; a
good one is by Embrechts, Klüppelberg, and Mikosch (1997). We will end
with a simple Excel example of EVT VaR. As we shall see, EVT does not
attempt to model the tail process, hence, one can think of it as a tool to fit the
tail distribution in a statistically correct manner. The reasoning is that if
the rare event lies outside the range of available observation, it seems
essential to rely on well-founded methodology. The reason why EVT gained
some acceptance in risk management is because return distributions in
financial markets are heavily fat-tailed during times of stress.
Classical EVT
The fundamental model of EVT describes the behavior of the maxima or
minima of a distribution. Consider a collection of n observed daily returns
{x1, . . . , xn} where we will express losses as positive numbers. So consider
the ranked loss {m1, . . . , mn} such that mn ¼ max{x1, . . . , xn} is the worst-
case loss in a sample and m1 ¼ min{x1, . . . , xn} is the best-case gain. In
VaR, we are interested in modeling the extreme loss mn.
We denote the cumulative distribution function (CDF) of variable X as
F(x). Assuming the maximas are i.i.d., the CDF of mn, that is, Fn(x) can be
easily derived:
FnðxÞ ¼ Pr½mn ≤x
¼ Prðm1 ≤x, m2 ≤x, :::, mn ≤xÞ
¼ L
n
i¼1
Prðmi ≤xÞ
¼ L
n
i¼1
FðxÞ
¼ ½FðxÞn
ð5:7Þ
However, this result is degenerate; that is, the function becomes useless
as n becomes very large. See Spreadsheet 5.4. We can avoid the degenerate
result by modeling the standardized maxima instead:
m* ¼ ðmn  bnÞ=an
ð5:8Þ
where {bn} and {an} are sequences of real numbers and an > 0. The idea is to
find {bn} and {an} such that the distribution of m* converges to a nondegen-
erate CDF as n-N. This limiting distribution is called the extreme value
distribution and is described by the Fisher-Tippett theorem (1928).
Advanced VaR Methods
117

The resulting generalized extreme value distribution (GEV) has the
following density function:
HαðxÞ ¼
exp

 ð1 þ αyÞ1=α
α 6¼ 0
expðeyÞ
α ¼ 0
(
ð5:9Þ
where y ¼ (x  b)/a and (1 þ αy) > 0. The parameters are called shape (α),
location (b), and scale (a). The GEV encompasses three limiting families of
distribution identified by Gnedenko (1943): the Fréchet family (α > 0), the
Gumbel family (α ¼ 0), and Weibull family (α < 0). The probability density
functions Hα(x) for these families are plotted in Figure 5.4. Note that this is a
distribution of the standardized maxima m*, not mn or the observed returns xi.
The Fréchet family is fat-tailed and is most useful for financial risk modeling.
Gnedenko (1943) has established the necessary and sufficient conditions
such that any probability distribution will have its tail behavior belonging to
one of the three families. For example, the normal and lognormal distribu-
tions lead to a Gumbel distribution for their tails; the Student’s T and Pareto
stable distributions’ tails follow a Fréchet distribution, the uniform dis-
tribution’s tail belongs to a Weibull distribution.
The extreme value theorem is the cousin of the better known central
limit theorem, which states that the mean of a sufficiently large number of
i.i.d. random variables with finite mean and variance (regardless of indi-
vidual distributional shapes) will be approximately normally distributed. In
the same way, EVT tells us about the limiting distribution of extreme values
(i.i.d. maximas or minimas) as the number of observations increases.
–0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
–10
–8
–6
–4
–2
0
2
4
6
8
10
x
α = 0 (Gumbel)
α = +0.9 (Frechet)
α = –0.5 (Weibull)
FIGURE 5.4
Three Families of Extreme Value Distributions (scale ¼ 1, location ¼ 1)
118
VALUE AT RISK METHODOLOGY

In this GEV form, the three parameters—shape, location, scale—are
determined by the data itself. The parameters can be estimated using block
maxima method. In this approach, the global sample set (with T data) is
divided into g nonoverlapping subsets with each subset having n observa-
tions, such that T ¼ gn. The maxima of each subset is taken out {h1, h2, . . . ,
hg} and they are used to fit the GEV distribution whose parameters need
estimation. By assuming that hi’s are i.i.d., one can use the maximum like-
lihood method to estimate the parameters.
The block maxima approach is not efficient in data usage. For example,
in a typical application, one would use yearly blocks of 250 days. Suppose
we have 50 years of history, that will only afford 50 maxima points. And
most markets will have a shorter history; hence estimated results will be
statistically weak. The next approach is better able to exploit the data set.
Peaks-over-Thresholds (POT) Method
In POT, we model the exceedence (y) or excess above a certain threshold (u).
We are interested in the conditional probability of the event y ¼ x  u
conditional on loss return x > u (losses are positively defined). It can be
shown theoretically that if the maxima has a GEV distribution, then the
exceedence y follows a Generalized Pareto Distribution (GPD) as shown by
Pickands (1975) and Balkema and de Haan (1974). The GPD is a two
parameter distribution with CDF given by:
Gα, βðyÞ ¼
	 1  ð1 þ αy=βÞ1=α
α 6¼ 0
1  expðy=βÞ
α ¼ 0
ð5:10Þ
where β > 0, and where y ≥0 when α ≥0 and 0  y  β/α when α < 0.
The first step in estimating the parameters β and α is to choose an
appropriate threshold u. Clearly the threshold cannot be chosen too low, as
this will mean that data beyond the threshold will deviate from GPD; GPD is
really for data fitting in the tail, not suitable for the body of the sample dis-
tribution. If we choose a threshold that is too high, there will be too few data
points to make meaningful statistical inferences. The appropriate choice of
threshold can be chosen by first ordering the data sample {xi}. Then for each
chosen u, pick up the sample set where xi > u; that is, the exceedences only.
Calculate and plot the mean excess function e(u) against u, as defined by:
eðuÞ ¼ 1
N
X
N
j¼1
ðxj  uÞ where u < x
ð5:11Þ
Advanced VaR Methods
119

where x1, x2, . . . , xN are N observations that exceeded u. Spreadsheet 5.4
illustrates the calculation of e(u) for the left tail (losses) for Dow Jones index
that is plotted in Figure 5.5.
Since GPD is characterized by its linearity in e(u) we identify from the
plot, the point u above which the graph is approximately linear. Use this
point (u¼ 1.5% loss) as the threshold. In the Dow Jones example, data was
taken from 1962 to 2009. Using the threshold, we have about 600 data
points for the POT approach. In contrast, had we used the block minima
approach, we would have only 47 data points to work with.
One way to estimate the parameters in equation (5.10) is to use the
maximum likelihood method. It can be shown that the log-likelihood
function for the GPD is given by:
Lðα, βÞ ¼
N lnðβÞ  ð1 þ 1=αÞ
X
N
i¼1
lnð1 þ αyi=βÞ
if
α 6¼ 0
N lnðβÞ  ð1=βÞ
X
N
i¼1
yi
if
α ¼ 0
8
>
>
>
>
<
>
>
>
>
:
ð5:12Þ
In financial applications, the case of α 6¼ 0 is of primary interest. We
estimate α and β that maximize L(.) for the sample defined by the N
observations exceeding the chosen threshold u, where yi are the exceedences.
Spreadsheet 5.5 is a worked-out example using Excel Solver.
0.005
0.007
0.009
0.011
0.013
0.015
0.017
0.019
0.021
0.023
0.025
–0.02
–0.01
0
0.01
0.02
0.03
0.04
0.05
Mean excess function, e(u)
Threshold(u)
FIGURE 5.5
Plot of Mean Excess Function e(u) versus Threshold (u) for the Left Tail
120
VALUE AT RISK METHODOLOGY

With the estimated parameters, we can then compute VaR. First, we
note that equation (5.10) can first be rewritten as a function of loss return x
instead of y. From the definition of VaR in (4.1) inverting this CDF gives
VaR:
VaRq ¼ u þ
^β
^α
n
N ð1  qÞ

^α
 1


ð5:13Þ
where N is the number of loss returns exceeding threshold (u), n is the total
number of observations, q is the confidence level for VaR at (1  q)
quantile. Equation (5.13) is plotted in Figure 5.6 along with pVaR for
comparison. As can be seen, the EVT VaR grows faster than pVaR above a
certain quantile because EVT fits the left tail data more appropriately than
the normal distribution.
Many aspects of EVT application are still being researched. Some of the
known limitations of EVT include:
1. In classical EVT, one has to make assumptions on the tail model; that is,
about the shape. One has to deal with data limitations in applying the
block minima approach.
2. In the POT approach, the choice of threshold is too subjective even
though one can get some graphical guidance from the mean excess plot.
A slightly different u (and hence N) will give rise to a different VaR.
0%
1%
2%
3%
4%
5%
6%
0.90
0.92
0.94
0.96
0.98
1.00
Percentage loss
Quantile (q)
EVT VaR
parametric VaR
FIGURE 5.6
EVT VaR versus pVaR for Different Quantiles
Advanced VaR Methods
121

3. EVT VaR equation (5.13) does not depend on any recent returns (unless
it happens to be an extreme event). It deals exclusively with exceedences
and is oblivious to data outside the tail. For proper parameter estima-
tion, we require at least 100 exceedences, hence a sizable history. Often,
there are not enough exceedences to apply a rolling window like in
conventional VaR. Thus, EVT VaR behaves more like a static measure
that is not sensitive to market fluctuations. EVT has more promising
application in areas where a rolling VaR is less required, such as in
insurance and operational risks.
4. The existing literature usually focuses on EVT for a single variable.
Practical portfolio risk management necessitates a multivariate EVT,
but this is only viable for a small number of dimensions. The major
obstacle is the correlation measurement. There are just far too few
observations of correlated extreme events for meaningful measure-
ment—all the variables must take on extreme values at the same time to
qualify—this is extremely rare. During crises, extreme correlations
(calculated using only exceedences) and average correlations can be very
different; so we cannot use the latter as a substitute.
5. EVT VaR is still a point estimate with a rather wide error range, and
this error gets uncomfortably big as we move to more extreme
quantiles.
On a positive note, EVT is a useful tool that provides smooth extrap-
olation into the extreme tail zone (say < 1% quantile) where there is no
observable data. This is based on our knowledge of the functional distri-
bution of the tail. Some argue that this is a dangerous practice; one has
to have faith in the EVT theorem to do this, the same kind of faith you would
have that the central limit theorem works in the real world. The CLT rests on
the strong assumption that returns are i.i.d., and EVT assumes maximas are
i.i.d. These basic assumptions may not hold during crises. See Diebold,
Schuermann, and Stroughair (1999) for a cautionary discussion of the
potential misuse of EVT.
5.5 SPREADSHEET EXERCISES
5.1 Hybrid historical simulation VaR is an approach that combines the
exponential weighting scheme of RiskMetrics with hsVaR, so that it can
have the benefit of using a long history and yet achieve market
responsiveness. Aim: illustrates the implementation of hybrid VaR.
Note: our VaR parameters K ¼ 250, λ ¼ 0.98, p ¼ 0.025. The hybrid
122
VALUE AT RISK METHODOLOGY

VaR is choppy compared to hsVaR. Action: If VaR is too unstable, it
will be unsuitable for minimum capital. Can you choose a suitable λ
setting to reduce the jaggedness?
5.2 Hull-White introduced stochastic volatility into hsVaR by scaling the
VaR scenarios with a ratio of changing volatilities. Aim: illustrates
the implementation of Hull-White VaR. Note: We have set the decay
factor to λ ¼ 0.94 as in RiskMetrics. Action: Experiment with various
values of λ to see if the Hull-White approach is stable.
5.3 The CAViaR (conditional autoregressive value-at-risk) model uses the
method of quantile regression to make VaR a function of other vari-
ables. It bypasses the problem of lack of tail data since the whole dis-
tribution is used in modeling, but the method requires us to presume
(specify) the process for VaR. Aim: illustrates the implementation of
the CAViaR model at 95% confidence (quantile of θ ¼ 0.05). Note:
CAViaR tracks the level of the hsVaR generally, but it has its own
peculiar pattern depending on the model specification. Action: Choose
a different quantile θ and rerun the Excel Solver. To help the Solver
along, set the beta at a low number such as 0.001 before running
Solver.
5.4 (a) The cumulative distribution function (CDF) of maximas is known to
be degenerate. With a slight transformation, this degeneracy can be
avoided, which allows the modeling of extreme value theory. Aim:
illustrates the concept of a degenerate function by plotting the CDF
of the maximas. Action: Pull the slider to make n large, and observe that
the shape of the CDF becomes degenerate (or useless). (b) The peaks-
over-thresholds (POT) method is a popular EVT approach. To use it, a
suitable exceedence threshold u needs to be determined visually using
a plot of the mean excess function. Aim: illustrates the calculation and
plot of the mean excess function of the POT approach.
5.5 A popular EVT approach is to model the tail as a generalized Pareto
distribution (GPD). To obtain the VaR, the parameters of the GPD
need to be estimated statistically. Aim: illustrates the estimation of the
parameters β and α for the GPD using maximum likelihood estima-
tion. Action: This is done using the Excel Solver. To work within the
Solver’s limitation (it is not a sophisticated optimizer) we assume
the case of α 6¼ 0 in equation (5.12). We first constrain α > 0 and
estimate the parameters to maximize the objective function L(.). Then
we constrain α < 0 and re-estimate to maximize L(.). We take the
parameters β and α that give the larger L(.). EVT VaR is then com-
puted using (5.13) for various quantiles. PVaR is also plotted with
EVT VaR as a comparison.
Advanced VaR Methods
123

NOTES
1. The interested reader can check the resource at: www.gloriamundi.org.
2. This is a well-known catch-22 for anyone who has ever used moving averages
(MA). If the window is long, the MA is stable but lags; if the window is short, the
MA is responsive but unstable.
124
VALUE AT RISK METHODOLOGY

CHAPTER 6
VaR Reporting
I
n this chapter, we shall study how value at risk (VaR) is aggregated bottom
up—that is, measured at the most elemental deal level and then progres-
sively aggregated up into portfolios and then into business lines. Generally,
if VaR is computed using scenarios or simulations, the aggregation is done
by adding PL vectors by scenarios. In the parametric method, VaR is
aggregated by variance-covariance matrix multiplication. In both cases, the
correlation structure is inherent and manifested as a lower overall VaR (risk
diversification).
VaR reporting usually includes explaining where the risk is coming
from. This VaR decomposition involves slicing the VaR number into its
component parts. Depending on the needs of the risk controller, the
breakdown can be by portfolio, by risk factor dimension, by business line,
by bank subsidiary, or even by deal at its most granular level.
6.1 VaR AGGREGATION AND LIMITS
VaR reports are typically generated prior to the beginning of each trading
day for position as of the end of the previous trading session. Normally,
reports are custom made for various users—traders, risk controllers, top
management, and regulators—and will show a relevant degree of detail and
risk decomposition.
For illustration purposes, we shall use the hsVaR model as specified in
Table 4.6. Six test deals (see Table 6.1) are booked into three portfolios.
In practice, a large bank may deal in hundreds of thousands of deals booked
in a vast array of portfolios. These portfolios are typically arranged in some
logical hierarchy based on traders, business lines, branch location, currency,
or product type (as in our test case).
The PL vector for each deal is compiled1 and aggregated to derive the
portfolio level VaR. Spreadsheet 6.1 shows how the PLs are added across
125

scenarios. A high-level VaR report is shown in Table 6.2 along with some
hypothetical limits. A limit is a ceiling set by risk managers to control the
risk-taking activities of traders. It is the first line of defense for a bank. A
trader who breaches a limit will face disciplinary actions if the breach could
not be justified. At times when VaR is very close to the limit, a big market
movement may cause VaR to go above the limit, a technical breach. In this
case, a trader will often request a temporary limit extension (increase). If not
approved by risk managers, the trader may be forced to unwind some
positions to bring the VaR back under the limit.
Thanks to diversification, the total VaR or diversified VaR is always
smaller than the algebraic sum of its individual VaR components. This
simple sum is called the undiversified VaR.
6.2 DIVERSIFICATION
Markowitz portfolio diversification is located at the heart of the VaR
framework. The ratio between the diversified and undiversified VaR is called
the diversification ratio—the smaller this number, the larger the benefit of
diversification. In our test portfolio, the ratio is 30%, which implies 70%
of the risk is diversified away. This is due to the long-short nature of the
book; the hedging between long index and put option, and the negative
correlation between stock index and bonds.
In general, diversification increases (and overall VaR reduces) when the
number of securities in the portfolio increases. This is shown in Figure 6.1. The
calculation is done in Spreadsheet 6.2 where we kept the portfolio’s undi-
versified VaR fixed at $1 million, each security has equal risk, and each
TABLE 6.1
Three Portfolios Containing Six Test Deals
Portfolio/Deal
Description
Equity Portfolio
Deal 1
Long $1 million Dow Jones index
Deal 4
Short $200,000 Dow Jones index
Option Portfolio
Deal 2
$1 million 1-year put option on Dow Jones (ATM strike 10718)
Deal 5
Short $5 million 1-year put option on Dow Jones
(OTM strike 9000)
Bond Portfolio
Deal 3
$1 million 2-year 5% bond
Deal 6
$1 million 20-year zero coupon bond
126
VALUE AT RISK METHODOLOGY

pair-wise correlation ρ is the same. As we increase the number of securities
n, the diversified VaR falls exponentially. The smaller the correlation ρ, the
more the portfolio benefits from diversification, the faster VaR falls with n. As
a rule of thumb, the advantages of diversification taper off once n is above 30.
6.3 VaR ANALYTICAL TOOLS
The report in Table 6.2 gives a broad overview of portfolio risk and provides
risk managers a framework to set up limits monitoring. For risk control, it is
–
200,000
400,000
600,000
800,000
1,000,000
1,200,000
1
11
21
31
41
51
61
71
Portfolio VaR
Number of securities
Correlation 0.0
Correlation 0.3
Correlation 0.5
Correlation 0.9
FIGURE 6.1
Diversiﬁcation and the Number of Securities
TABLE 6.2
A Simple VaR Report with Limits
Entity
VaR (97.5%)
Limits
Equity Portfolio
$9,529
$20,000
Option Portfolio
$3,884
$20,000
Bond Portfolio
$9,512
$20,000
Total Portfolio
$14,244
$35,000
Undiversiﬁed VaR
$46,790
Diversiﬁcation ratio
30%
VaR Reporting
127

further necessary to determine the drivers of the VaR numbers and to see
how VaR evolves as the observation window moves forward in time. This is
the diagnostic aspect of VaR, and some basic tools are available.
The Tail Profile
In hsVaR it is crucial to monitor the profile of the tail scenarios. To plot this,
rank the 250 P&Ls of the PL vector, then plot the ranked PL (see Figure 6.2).
This tangent-shaped profile is typical of all PL vectors; in particular, the points
on both ends (tails) space out very quickly. The tail loss (of 97.5% VaR) is
determined by the seven extreme points on the left; if one holds the opposite
position to the test portfolio, the tail loss will be determined by the seven
extreme points on the right. The tail profile evolves every day as new positions
are traded and as market data rolls in (and out) of the observation interval.2
To zoom in on the tails, a horizontal bar chart is often plotted
showing only the top ranking and bottom ranking P&L (see Figure 6.3
and Spreadsheet 6.1). The scenario number of each observation is labeled.
The VaR is given approximately by the seventh-largest loss, that is,
$15.3k (scenario 71). Notice the sixth-largest loss is $16.2k (scenario
165). Suppose a new loss scenario greater than $15.3k (say $20k) comes
into the PL vector, then scenario 71 will be pushed out of the quantile
(now becomes rank 8), scenario 165 will be the new seventh-largest loss,
(30,000)
(25,000)
(20,000)
(15,000)
(10,000)
(5,000)
–
5,000
10,000
15,000
20,000
0
50
100
150
200
250
Ranked P&L
Ranks
FIGURE 6.2
Proﬁle of the Ranked PL of the Test Portfolio
128
VALUE AT RISK METHODOLOGY

and the VaR will become $16.2k (an increase seen). This is how an
incoming extreme loss observation increases hsVaR.
Also, the fourth-largest loss ($16.8k) is scenario 220, a rather old sce-
nario. In 30 days’ time, this scenario will become scenario 250 and will exit
the observation window. All else being equal, the eighth-largest loss (now
scenario 71) will then move back up one rank to become the seventh rank;
that is, the VaR decreases to $15.3k.
Component VaR
Individual VaR is not a useful way to show decomposition of risk for a
portfolio because it ignores the power of diversification. In particular, it does
not add up to the total diversified VaR. Component VaR partitions the
portfolio VaR into parts that add up to the total diversified VaR. This
additivity is a nice feature for the purpose of risk communication so that the
report is more intuitive.
It uses the beta approach, which linearly aggregates risks and implicitly
assumes the joint distribution is normal. Recall that the beta is just the slope
of the graph of return of asset i versus portfolio return. If the relationship is
nonlinear, the slope is ill-defined. Thus, component VaR is good in the
absence of nonlinear products and when the portfolio is large. The com-
ponent VaR of asset i is given by:
CVaRi ¼ VaRiρi
ð6:1Þ
218
50
181
220
96
165
71
73
197
162
160
70
44
83
48
149
51
124
85
225
(23,907)
–30000
–20000
–10000
0
10000
20000
30000
(29,383)
FIGURE 6.3
Tail Proﬁle of the Total PL Distribution of the Test Portfolio
VaR Reporting
129

where VaRi is the individual VaR of asset i and ρi is the correlation of return
of asset i with the portfolio return. Spreadsheet 6.3 is a worked-out example;
the result is in Table 6.3. Notice the component VaRs, expressed as a per-
centage, sum up to 100% of the diversified portfolio VaR.
Component VaR is a convenient representation, but it does not reflect
diversification—the risks add up linearly, whereas in reality they should
really be subadditive. Furthermore, component VaR is not additive in either
a nonnormal analytical setting or a Monte Carlo/historical simulation
context. See Hallerbach (2002), who investigated the decomposition of VaR
in a general distribution-free setting.
Incremental VaR
Incremental VaR is the difference between diversified VaR with and without
the position under investigation. This is the most correct decomposition
method for use with hsVaR (full revaluation) since it accounts for the full
impact of the position including any nonlinearity.
Incremental VaR for asset i is given by:
IVaRi ¼ VaRPþi  VaRP
ð6:2Þ
where VARPþi is the portfolio VaR inclusive of asset i, VARP is the portfolio
VaR after removing asset i. Clearly, when IVARi > 0 (for clarity we will
express VaR as positive numbers), then this position contributes by
TABLE 6.3
Component VaR Calculated Using pVaR Method
Individual VaR
Component VaR
% of Total
Correlation
Deal(1)
(12,709)
(9,034)
64%
0.71
Deal(2)
(10,030)
7,388
52%
(0.74)
Deal(3)
(1,945)
(1,165)
8%
0.60
Deal(4)
(2,542)
1,807
13%
(0.71)
Deal(5)
(11,874)
(8,641)
61%
0.73
Deal(6)
(7,646)
(4,572)
32%
0.60
Total
(14,218)
100%
Equity portfolio
(10,167)
(7,227)
51%
0.71
Option portfolio
(2,988)
(1,253)
9%
0.42
Bond portfolio
(9,591)
(5,737)
40%
0.60
Total
(14,218)
100%
Diversiﬁed VaR
(14,218)
130
VALUE AT RISK METHODOLOGY

increasing overall diversified VaR by the amount IVARi. If IVARi < 0 it
means the position is risk reducing—it hedges some of the portfolio risk by
the amount IVARi.
Spreadsheet 6.4 is a worked-out example; the result in Table 6.4 shows
that deals 2 and 4 are risk decreasing—long index put option and short stock
in an overall long equity portfolio. Note that the incremental VaR number is
drastically different from component VaR. They are different things—one is
the actual dollar impact, the other is a beta attribution. Also, the subtotals in
Table 6.4 do not add up to anything meaningful and should never be used
because such simple summations do not account for diversification.
Incremental VaR can be used in flexible ways to study risk drivers of the
overall book. For example, one can run hsVaR for the entire portfolio as a
base case, then run hsVaR again but without applying interest rate risk
factor scenarios. The difference will give incremental VaR due purely to
interest rate risk! Likewise, one can calculate incremental VaR due to FX risk
alone, due to a portfolio belonging to one trader, due to one particular deal,
due to volatility smile effect, or even due to time (theta) decay. The general
idea is to run VaR twice—with the factor(s) you want to study and with-
out—then take the difference. The combinations are limited only by the
imagination.
The weakness of this approach is that it takes a long time to fully revalue
the entire portfolio multiple times to work out the increments. And, sec-
ondly, the incremental VaR does not add up to overall VaR; this unintuitive
fact will require some effort to explain to top management.
TABLE 6.4
Incremental VaR Calculated Using hsVaR Method
Individual VaR
Incremental VaR
Deal(1)
11,858
3,966
Deal(2)
8,477
(8,622)
Deal(3)
2,065
1,261
Deal(4)
2,553
(1,518)
Deal(5)
14,875
5,435
Deal(6)
8,062
1,786
Subtotal
2,309
Equity portfolio
9,487
3,335
Option portfolio
4,412
1,778
Bond portfolio
10,127
1,113
Subtotal
6,225
Diversiﬁed VaR
14,244
VaR Reporting
131

6.4 SCALING AND BASEL RULES
Basel Rules
Most banks calculate VaR using daily returns and at less than 99% confidence
level. This is driven by practical considerations such as data availability,
statistical accuracy, and the need to ensure responsiveness to market changes.
However, regulatory reporting as stipulated by the Basel Rules (1995)
requires VaR to be stated in terms of a 10-day horizon and 99% confidence
level. The Basel Rules provide the formula to compute the market risk charge
(MRC) at day t for a bank:
MRCt ¼ max
k
60
X
60
i¼1
VaRti, VaRt1
 
!
þ SRCt
ð6:3Þ
which involves taking the larger of the most recent VaR or the average VaR
of the last 60 days. The specific risk charge (SRC) captures the idiosyncratic
risk coming from securities of specific issuers.3 The supervisory multiplier k,
which must be 3 or larger, is determined by the regulator depending on
its assessment of the “goodness” of the bank’s risk model (see back testing
in Chapter 8).
Let’s look at the merit of equation (6.3). The averagingover the last 60 days
in the first term smooths the VaR so that the MRC is stable; 60 days also reflects
the liquidity horizon, which in Basel’s opinion is sufficient for a bank in trouble
to raise funds or to unwind trading positions. The first term mostly dominates
the MRC because of the multiplication by k. The second term (VARt1) will
overtake only when there is an extreme event (say in a crash) or when traders
increase positions substantially. This design gives a less volatile capital charge
and yet provides for higher penalty under exceptional situations.
Suppose the capital charge is volatile, since positions can seldom be
adjusted quickly, a bank will need to maintain a conservative (spare) capital
buffer to absorb the fluctuation and peaks in charges—it cannot be cost
effective. Hence, a less variable, smoother capital charge is beneficial eco-
nomically. It is much easier for a bank to cut positions than to raise capital in
order to meet regulatory capital requirements. The latter entails changing the
firm’s capital structure; that is, raising equity.
Time Scaling
The 10-day horizon (also called holding period or risk horizon) stipulated
by Basel corresponds to the estimated time required for corrective action
132
VALUE AT RISK METHODOLOGY

by regulators should a bank run into liquidity problems. This could involve
orchestrating a rescue plan or an orderly unwind of positions. To scale to 10
days, Basel has recommended the use of the “square-root of time” scaling:
VaR10day ¼ VaRdaily
ﬃﬃﬃﬃﬃﬃ
10
p
ð6:4Þ
The square-root of time scaling rests on the assumptions of independent
and identically distributed (i.i.d.), normal distribution, and unchanging
position over the entire horizon. These are arguably weak assumptions.
Consider a time series of returns Xt. The variance over two equal, non-
overlapping periods (say t ¼ 1, 2) is given by:
σðX1, X2Þ2 ¼ σ2
1 þ σ2
2 þ 2σ1σ2ρ12
ð6:5Þ
By i.i.d. the variance of X1 and X2 are identical (σ1 ¼ σ2 ¼ σ) and has
zero correlation ρ12¼ 0, hence: σ(X1, X2) ¼ σ
ﬃﬃﬃﬃ
T
p
. Extending this argument
iteratively, we have σ(X1, . . . , XT) ¼ σ
ﬃﬃﬃﬃ
T
p
. Because volatility is often
quoted in annualized terms, to transform to a volatility of horizon T years,
we use:
σ ¼ σannual
ﬃﬃﬃﬃ
T
p
ð6:6Þ
To obtain daily σ for example, let T ¼ 1/250. And since VaR is just σ
times a constant, for a normal distribution, equation (6.4) is true.
Equation (6.6) underestimates risk in the presence of serial correlation
(non i.i.d.). However, this can be corrected. Let’s consider an AR(1) model of
the form:
Xt ¼ ρXt1 þ εt
ð6:7Þ
where the random element εt is white noise and ρ the autocorrelation. Once
again, the two-period variance is:
σðX1, X2Þ2 ¼ σ2 þ σ2 þ 2σ2ρ ¼ σ2ð2 þ 2ρÞ
ð6:8Þ
For T periods this can be generalized to:
σðX1, XTÞ2 ¼ σ2
T þ 2ðT  1Þρ þ 2ðT  2Þρ2 þ : : : þ 2ð1ÞρT1
ð6:9Þ
VaR Reporting
133

of which equation (6.6) is a special case when ρ ¼ 0. When ρ > 0, equation
(6.6) underestimates the true risk because equation (6.9) grows faster than
(6.6) with increasing horizon T.
A technical point—the autocorrelation ρ needs to be computed just once
at the bank-wide PL vector and not at each risk factor’s scenario vector. This
is because we want to capture the serial correlation caused by derivatives
held by the bank as a whole. For example, range accruals are options that
provide a fixed positive return (say 0.02% daily) for every day that a ref-
erence index trades within a predefined range. If a bank holds large amounts
of range accruals and the index spends a large part of its life within the
range, we can expect significant autocorrelation in PL. With ρ estimated,
equation (6.9) can be used for more accurate scaling if we assume an AR(1)
process.
A more advanced method uses power law scaling, where volatility is
assumed to scale with T1/h, where h can be statistically determined. This
method rests on a mild assumption that the distribution is stable. A stable
distribution is one whereby if you randomly draw samples from the distri-
bution F, the sum of the samples is also distributed like F. Clearly, not all
distributions are stable; for example, a uniform distribution is not stable—if
we draw many samples, their sums converge to a normal distribution (as
seen in Spreadsheet 2.2). For a stable distribution, power law scaling is a
valid method.
Suppose QT is the q-quantile of the T-day log returns. Using the return
data series, we need to estimate h from the power scaling law (6.10) where
the scale exponent is (1/h).
QT ¼ T1=hQ1
ð6:10Þ
We take log on both sides giving:
h ¼
lnðTÞ
lnðQT=Q1Þ
ð6:11Þ
Hence, h is just the slope of the ln(T) vs. ln(QT/Q1) graph. Spreadsheet
6.6 gives an example of how this can be implemented for Dow Jones index
for q ¼ 0.05. For more details, the reader may refer to Alexander (2008).
Note that the naïve method of using a rolling 10-day return to calculate
the 10-day VaR will produce a biased result because a single observation will
be reused 10 times due to overlapping 10-day periods. This causes an arti-
ficial serial correlation (see Spreadsheet 6.5) and distortion in the tails of the
distribution. Surprisingly (perhaps for lack of a better alternative), some
134
VALUE AT RISK METHODOLOGY

regulators allow the use of overlapping 10-day VaR. The error is quantified
in the paper by Sun and colleagues (2009).
The second assumption that a bank’s position remains constant for 10
days is simply untrue for investment banks, which deal with huge trading
volume. The disconnect with this assumption is that VaR will not be the 10-
day forward looking measure Basel meant it to be. The truth is banks almost
always know about incoming deals 10 days ahead of time, especially when it
comes to significant transactions. The trading desk knows about large deals
in the pipeline and customers orders ahead of the transaction. This is par-
ticularly true for underwriting and securitization deals, which take weeks to
put together. Also, if a trader needs to prehedge a large transaction or
unwind a large portfolio, the plan is known ahead of time and usually
executed over a period. Thus, the obvious alternative to the constant posi-
tion assumption is to prebook deals (for the purpose of VaR) if the trader is
confident that the deal will happen. If the deal falls through later, it can be
cancelled from the system.
In theory, this idea could work but there are two critical weaknesses.
Firstly, it is open to gaming—if the trader is about to breach his VaR limit
due to losses, he may prebook a hedge (without actually doing the trans-
action) on the pretext of intending to unwind in the next 10 days. If the
market recovers, he could decide not to unwind. In this way, he could avoid
the discipline to cut his losses. Secondly, if a large impending deal gets
prebooked, it is more susceptible to information leakage and insider trading,
which would violate client confidentiality. For these reasons, perhaps, the
naïve assumption of constant position is an unavoidable simplification.
Quantile Scaling
Since Basel stipulates a 99% confidence VaR and most banks use an
observation window of one to two years, risk modelers often run into a data
scarcity problem. For example, with a 250-day window, the 99% VaR is
determined by just 2.5 data points. Such an estimate is hardly statistically
meaningful.
To overcome this problem, banks often compute the VaR using a lower
quantile such as 97.5% confidence and then scale up to 99% by assuming
the tail belongs to a normal distribution. With this simplification, the VaR is
scaled using the standard normal deviate α(q) of quantile q or confidence
level (1  q). In Excel function α(q) ¼ NORMSINV(q). Then:
VaR99% ¼ αð0:01Þ
αð0:025Þ VaR97:5% ¼ 2:326
1:960 VaR97:5%
ð6:12Þ
VaR Reporting
135

Basel’s choice of 99% confidence level would translate to one occur-
rence of (nonoverlapping 10-day) loss larger than VaR for every four years
on average. This frequency of bank failures is still far too high for any
regulators to tolerate. Hence, the multiplicative factor k(≥3) is meant to
provide an additional capital buffer.
6.5 SPREADSHEET EXERCISES
6.1 In historical simulation VaR, risks can easily be aggregated to arrive at
portfolio VaR. Aim: illustrates the aggregation of VaR by adding PL
vectors across scenarios for a portfolio containing 6 deals. It also
illustrates the ranked tail profile of the PL distribution often used for
analysis.
6.2 The benefit of diversification increases as the number of securities in the
portfolio increases. Aim: illustrates the benefit of diversification, hence,
the fall in portfolio VaR, as the number of securities n increases. Note:
pVaR is calculated assuming a constant correlation ρ (for each pair-wise
combination). The quantity of each security is equally weighted such
that the undiversified VaR adds up to $1 million. Action: Key in a
different correlation ρ and observe how the pVaR declines with n. More
interestingly, key in a negative ρ and observe that pVaR fails because
the matrix is no longer positive semidefinite. For a small negative cor-
relation (say ρ ¼ 0.1) the pVaR can still be calculated for small n. But
as the n grows, the correlation matrix becomes too large and restrictive
in terms of satisfying the positive semidefinite condition.
6.3 Component VaR is a method to decompose portfolio VaR by assuming
its constituents are beta portions of the portfolio VaR. Aim: illustrates
the computation of component VaR using pVaR and hsVaR methods.
Note: under pVaR, the component VaRs add up to 100% of diversified
portfolio VaR. Under hsVaR, the component VaRs do not add up to
100% of total generally. It works for pVaR because both pVaR and the
beta approach assume a normal distribution.
6.4 Incremental VaR is a method to decompose portfolio VaR by calcu-
lating the actual contribution (including effects of diversification) of a
deal to the portfolio VaR. This is done by removing that deal and
recalculating portfolio VaR and taking the difference. Aim: illustrates
the computation of incremental VaR using hsVaR method. Note: the
incremental VaRs of constituents are not expected to add up to port-
folio VaR because their contributions take into account diversification.
136
VALUE AT RISK METHODOLOGY

6.5 Is it valid to calculate a 10-day VaR directly using a 10-day log return
applied over a rolling window? That is, where the return series is ln(Xt/
Xt10) where t ¼ 10,11, . . . , T. Aim: Use an ACF plot to illustrate that
a serial correlation is artificially introduced into the 10-day return series
even though the 1-day return series is i.i.d. Note: This problem can be
resolved by using nonoverlapping returns, where t ¼ 10, 20, . . .
However, this requires 10 times as much historical data.
6.6 Power law scaling is a valid method to scale daily VaR to T-day VaR
provided the distribution is stable. Aim: Implement the power law
scaling for the Dow Jones index for a linear instrument. Estimate h for
various quantiles q.
NOTES
1. The PL vectors are actually computed using the previous calculator and data
(Spreadsheet 4.1). The astute reader can use that spreadsheet to generate PL
vectors for new test deals.
2. This diagnostic does not apply for pVaR and mcVaR methods where there is no
time ordering of scenarios. Here the PL distribution is truly i.i.d. by assumption
and is oblivious to any serial correlation in returns.
3. Debt securities (corporate bonds) will contain idiosyncratic risk unique to the
issuer of that security on top of the usual interest rate risk (which falls under
MRC). These idiosyncratic factors include the risk coming from credit spread
movement and a rating downgrade.
VaR Reporting
137


CHAPTER 7
The Physics of Risk
and Pseudoscience
G
reat fortunes are made and lost on Wall Street with the power of mathe-
matics. Quantitative or “quant” modeling is akin to an arms race among
banks. The conventional wisdom is that a bank that has superior models can
better exploit market inefficiencies and manage risk competitively.
Many of the modeling techniques and ideas were borrowed from
mathematics and physics. In hard sciences, a mathematical law always
describes a truth of nature, which can be verified precisely by repeatable
experiments. In contrast, financial models are nothing more than toy
representations of reality—it is impossible to predict the madness of the
crowd consistently, and any success in doing so is often unrepeatable. It
really is pseudoscience, not precise science. The danger for a risk manager is
in not being able to tell the difference.
Empirical studies have shown that the basic model assumptions of being
independent and identically distributed (i.i.d.), stationarity, and Gaussian
thin-tailed distribution are violated under stressful market conditions.
Market prices do not exhibit Brownian motion like gas particles. Phenom-
ena that are in fact observed are fat-tailness and skewness of returns, and
evidence of clustering and asymmetry of volatility. In truth, the 2008 crisis is
one expensive experiment to debunk our deep-rooted ideas.
This chapter discusses the causes and effects of the market “anomalies”
that disrupt the VaR measure. Due to its failings, VaR is increasingly rec-
ognized as a peacetime tool. It’s like supersonic flight—the shock wave
renders the speedometer useless in measuring speed when the sound barrier
is broken; likewise our riskometer fails during market distress—the
moments such as variance and kurtosis can no longer be measured
accurately.
139

7.1 ENTROPY, LEVERAGE EFFECT, AND SKEWNESS
In physics, entropy is a measure of disorder or chaos. The famous Second
Law of Thermodynamics states that entropy can only increase (never
decrease) in an enclosed environment. For entropy to decrease, external
energy must flow into the physical system (i.e., work needs to be done).
Thus, in nature there is a spontaneous tendency towards disorder. For
example, if we stack a house of cards (as shown in Figure 7.1), a small
random perturbation will bring down the whole structure, but no amount
of random perturbation can reverse the process unless external work is done
(restacking). Interestingly, entropy acts to suggest the arrow of time. Suppose
we watch a recorded video of the collapse of the house of cards backwards—
the disordered cards spontaneously stacking themselves—we know from
experience this is physically impossible. In other words, manifestation of
entropy can act as a clock to show the direction of time.
Figure 7.2 shows two charts of an actual price series. Can you tell which
one has the time scale reversed purely by looking at the price pattern? Could
this be evidence of entropy?
Note the asymmetry—it takes a long time to achieve order (work needed)
but a very short time to destroy order (spontaneous tendency). There is evi-
dence that entropy exists even in social interactions. For example, it takes
great marketing effort for a bank to attract and build up a deposit base, but
only one day for a bank run to happen if there is a rumor of insolvency.
In information theory, entropy (or Shannon entropy) is a measure of
uncertainty associated with a random variable used to store information.1
The links between physical, informational, and social entropies are still
being debated by scientists.
FIGURE 7.1
Collapse of the House of Cards and Entropy
140
VALUE AT RISK METHODOLOGY

Financial markets’ entropy is manifested as the leverage effect, the
phenomena whereby rallies are gradual and accompanied by diminished
volatility, while sell-downs are often sharp and characterized by high vola-
tility. Destruction of wealth has more impact on the economy than creation
of wealth—it hurts the ability of firms to raise money and the individual’s
spending power. For investors, the rush to exit is always more frenzied than
the temptation to invest (there is inertia in the latter)—fear is a stronger
emotion than greed. For example, consider Figure 7.3, which shows the S&P
500 index and the VIX index.2 There is an apparent negative correlation
between a stock index and its volatility.
–
200
400
600
800
1,000
1,200
1,400
1,600
1,800
FIGURE 7.2
Which Chart Is Time Reversed? The Intuition of Entropy
10
40
70
100
130
0
200
400
600
800
1,000
1,200
1,400
1,600
Sep-97
Sep-99
Sep-01
Sep-03
Sep-05
Sep-07
Sep-09
Volatility (%p.a)
Index level
S&P 500 index
VIX index
FIGURE 7.3
Negative Correlation between an Equity Index and Its Volatility
The Physics of Risk and Pseudoscience
141

The leverage effect is also reflected in the equity option markets, in the
form of an observed negative “volatility skew”—OTM puts tend to demand
a higher premium compared to OTM calls of the same delta. The fear of loss
is asymmetric; from investors’ collective experience, crashes are more dev-
astating than market bounces. Thus the premium cost for hedging the
downside risk is more expensive than that for the upside risk. Interestingly,
this volatility skew appeared after the 1987 crash and is with us ever since.
The volatility skew is illustrated in Figure 7.4; the horizontal axis shows the
strike price of options expressed in percentage of spot (or moneyness). Points
below 100% moneyness are contributed by OTM puts, while those above
are contributed by OTM calls.
The asymmetry of risk causes skewness in distributions. In a bull trend, the
frequency of positive observations is expected to be larger than negative ones.
In this case, the skew is to the negative. As illustrated by Figure 7.5, the fre-
quency on the positive side of the hump is larger. Because the area has to sum up
to a probability of 1.0 and skew is measured on a centered basis, it causes a tail,
which is skewed to the left. Conversely, in a bear trend, the skew is positive.
Spreadsheet 7.1 lets the reader simulate these stylized examples. The rather
unintuitive result is often observed in actual data. Figure 7.6 plots the S&P 500
index during a bull market (negative skew) and bear market (positive skew)!
Consider equation (2.4) for sample skewness; it can be rewritten in
terms of centered returns yi ¼ xi  ^x:
skewness ¼
1
n  1
X
n
i¼1
ðyi2ÞðyiÞ 1
^σ3
ð7:1Þ
10
12
14
16
18
20
22
24
26
28
30
90%
95%
98%
100%
103%
105%
110%
Volatility (%p.a)
Moneyness (%)
FIGURE 7.4
Illustration of “Volatility Skew” for an Equity Index
142
VALUE AT RISK METHODOLOGY

which shows the interesting idea that skewness can be seen (loosely) as
correlation between the movement of the random variable (Yi) and its vol-
atility (Yi
2)—by equation (2.16). Generally (but not always3) the correlation
is negative during a rally and positive during a sell-down. Note the rela-
tionship between Yi and its volatility is quadratic and hence is not well
described by linear correlation.
4.7
4.3
4.0
3.7
3.4
3.1
2.8
2.5
2.1
1.8
1.5
1.2
0.9
0.6
0.3
(0.1)
(0.4)
(0.7)
(1.0)
(1.3)
(1.6)
(1.9)
(2.3)
(2.6)
(2.9)
(3.2)
Bull market (negative Skew)
Normal
FIGURE 7.5
Stylized Example of Bull Market and Negative Skew Distribution
700
800
900
1,000
1,100
1,200
1,300
1,400
1,500
1,600
Skew +0.18 (Jan 08-Dec 08)
Skew –0.53 (Jun 06-Jun 07) 
FIGURE 7.6
Skew of S&P 500 Index during Bull and Bear Phase
The Physics of Risk and Pseudoscience
143

Unfortunately, skewness is an incomplete measure of risk asymmetry. In
particular, it fails to account for price microstructure when prices approach
important support and resistance levels. Consider a defended price barrier
such as a currency peg like the USD/CNY. Speculators who sell against the
peg will bring prices down to test the peg. This downward drift is gradual
because the currency is supported by the actions of opposing traders who bet
that the peg will hold and by the central bank managing the peg. On the
other hand, each time the peg holds, short covering will likely see quick
upward spikes. This will cause occasional positive skewness in the distri-
bution, even though the real risk (of interest to a risk manager) is to the
downside—should the peg break, the downward move may be large. In fact,
the risk shows up as a negative volatility skew in the option market.4 Thus,
statistical skewness is often an understated and misleading risk measure in
situations where it matters most.
As the third moment, skewness is often erratic because it is calculated as
a summation of many y terms raised to a power of three. Hence, any
occasional large noise in the sample y will be magnified and can distort this
number, and y can be quite noisy when the market price encounters support
and resistance levels.
In conclusion, the leverage effect which depends on price path, critical
levels, and trend, is too rich to be adequately described by simple statistics
(of returns) such as skew and correlation. Measures such as VaR, made from
moments and correlation, describe an incomplete story of risk.
7.2 VOLATILITY CLUSTERING AND THE FOLLY OF i.i.d.
Volatility clustering is a phenomenon where volatility seems to cluster
together temporally at certain periods. Hence, the saying goes that high
volatility begets high volatility and low volatility begets low volatility. Figure
1.3 shows the return series of the Dow Jones index—the clustering effect is
obvious. This is caused by occasional serial correlation of returns—in stark
violation of the i.i.d. assumption.
Under the assumption of i.i.d., return series should be stationary and
volatility constant (or at least slowly changing). This naïve assumption
implies that information (news) arrives at the market at a continuously slow
rate and in small homogenous bits such that there are no surprises or shocks.
But news does not come in continuous streams. Its arrival and effects are
often lumpy—for example, news of a technological breakthrough, central
bank policy action, H1N1 flu outbreak, 9/11 tragedy, a government debt
default, and so on. Each of these can cause a sudden surge in market
volatility.
144
VALUE AT RISK METHODOLOGY

Many volatility models attempt to account for clustering by making
volatility conditional on past volatility (i.e., autoregressive); for example,
EWMA and GARCH models (see Section 2.7). VaR based on these models
will be more responsive to a surge in volatility.
Some thinkers argued that if a phenomenon is non-i.i.d., the use of fre-
quentist statistics becomes questionable in theory. The late E.T. Jaynes, a
great proponent of the Bayesian school, wrote in his book Probability The-
ory: The Logic of Science, “The traditional frequentist methods . . . are
usable and useful in many particularly simple, idealized problems; but they
represent the most proscribed special case of probability theory, because
they presuppose conditions (independent repetition of a ‘random experiment’
but no relevant prior information) that are hardly ever met in real problems.
This approach is quite inadequate for the current needs of science . . . .”
7.3 “VOLATILITY OF VOLATILITY” AND FAT TAILS
The phenomenon of volatility clustering implies another obvious fact, that
volatility is not constant but stochastic. Volatility (like returns) changes with
time in a random fashion. What is interesting is that stochastic volatility (or
“volatility of volatility“) can cause fat tails.
Figure 7.7 shows two return series, which can be simulated using
Spreadsheet 7.2. The upper panel is generated by GBM with constant
Return (Stochastic vol)
–2
–4
–6
0
2
4
6
–2
–4
–6
0
2
4
6
Return (Constant vol)
FIGURE 7.7
Return Series with Constant Volatility (upper) and Stochastic
Volatility (lower)
The Physics of Risk and Pseudoscience
145

volatility, while the lower panel is by GBM with stochastic volatility given by
a simple process:
Δσt ¼ 0:02σt1εt
ð7:2Þ
where the random element εt ~ N(0,1). For a more realistic model, see
Heston (1993). Notice the lower panel shows obvious clustering, charac-
teristic of stochastic volatility. Figure 7.8 shows the probability distribution
of both the series—constant volatility caused a normal distribution, but
stochastic volatility caused the distribution to be fat-tailed. For a discussion
of the impact of variable volatility on option pricing, see Taleb (1997).
How does stochastic volatility create fat tails? The intuitive way to
understand this is to consider the mixture of normal distributions in Figure
4.5. The fat-tailed distribution (bar chart) is just a combination of two
distributions of varying volatilities. The high volatility observations domi-
nated the tail whereas the low volatility observations dominated the middle
or peak. So the net distribution is fatter at the tail and narrower in the
middle as compared to a normal distribution. A distribution with stochastic
volatility can be thought of as being a mixture of many normal distributions
with different volatilities, and will thus result in a fat tail.
In Figure 7.8, it is hard to see any difference at the tails compared to a
normal distribution. The histogram is an unsuitable tool to study the tail.
A scatter plot is a better choice. For example, Figure 7.9 is a scatter plot of
the quantiles (expressed in units of standard deviation) of the Dow Jones
0
200
400
600
800
1,000
1,200
1,400
–4
–3
–2
–1
0
1
2
3
4
Constant vol
Stochastic vol
FIGURE 7.8
Probability Distributions with Constant and Stochastic Volatility
146
VALUE AT RISK METHODOLOGY

index daily returns from July 1962 to June 2009 versus the standard normal.
Had the empirical data been normally distributed, the plot would have fallen
on the dotted line (with unit slope). But the empirical data is fat-tailed, so the
tail quantiles are more extreme (more risky) than the normal, while the lower
quantiles are less risky.
How far off are we if we use the tail of the normal distribution to model
the risk of extreme events? Very. Empirical evidence of past market stress
revealed that the normal distribution is an impossible proposition. Table 7.1
–5
–3
–1
1
3
5
–5
–3
–1
1
3
5
Quantile of Dow Jones index returns
Quantile of Standard Normal
FIGURE 7.9
Scatter Plot of Quantiles of the Dow Jones Index versus the Standard
Normal
TABLE 7.1
Top Ten Largest Single Day Losses for Dow Jones Index (1987–2008)
Event Date
Daily Log Return
Mean Number of
Years between Occurrences
19-Oct-87
25.6%
1.86Eþ56
26-Oct-87
8.4%
69,074
15-Oct-08
8.2%
37,326
1-Dec-08
8.0%
19,952
9-Oct-08
7.6%
5,482
27-Oct-97
7.5%
3,258
17-Sep-01
7.4%
2,791
29-Sep-08
7.23%
1,684
13-Oct-89
7.16%
1,346
Jan-8-88
7.10%
1,120
The Physics of Risk and Pseudoscience
147

lists the 10 largest one-day losses experienced by the Dow Jones index in 20
years (1987 to 2008). The third column shows the statistical frequency (in
units of years) for each event assuming the normal distribution holds. These
are calculated using the simple relationship (in Excel notation):
1
250T ¼ probabilityðxÞ ¼ NORMDISTðx, 0, σ=sqrtð250Þ, TRUEÞ ð7:3Þ
where the observed event of log return x is calculated to occur once every T
years (assuming 250 business days per year). We have also assumed an
annualized volatility σ ¼ 25%, typical of equity indices.
The results show the forecast of the normal distribution is ludicrous—
Black Monday (19 Oct 87) is computed as a once in 1.86 × 1056 year event!
In contrast, the age of the universe is only 14 billion years. We have already
witnessed 10 such extreme events in a span of just 20 years—clearly extreme
events occur more frequently than forecasted by the normal distribution.
From a modeling perspective, there are two schools of thought on fat
tails. One school believes that returns follow a distribution that is fat-tailed
(such as the log-gamma distribution or stable Pareto distribution). The
second school sees returns as normal at each instant of time, but they look
fat-tailed due to time series fluctuations in volatility. In Section 13.2, we shall
propose a third, whereby fat-tailness arises from a break or compression of
market cycles.
7.4 EXTREMISTAN AND THE FOURTH QUADRANT
Chapter 1 provided a prelude to the idea of extremistan. We shall continue
by noting that financial markets are informational in nature and scalable.
This gives rise to extremistan behavior of rare events such as crashes,
manias, rogue trading, and Ponzi schemes. From the VaR perspective,
attempts to forecast these events using Gaussian models will render us fooled
by randomness because it will never let us predict Black Swans, but lull us
into a false sense of security that comes from seemingly precise measures.
Having a thick risk report on the CEO’s desk does not make this type of risk
go away. Black Swans are not predictable, even using fat-tail distributions
(such as that of EVT), which are just theoretically appealing fitting tools.
Such tools are foiled by what Taleb called “inverse problems”—there are
plenty of distributions that can fit the same set of data, and each model will
extrapolate differently. Modeling extremistan events is futile since there is no
typical (expected) value for statistical estimation to converge to.
148
VALUE AT RISK METHODOLOGY

The idea of extremistan has serious consequences. The business of risk
measurement rests on the tacit assumption that frequency distribution equals
probability distribution. The former is a histogram of observations, the latter
is a function giving theoretical probabilities. There is a subtle philosophical
transition between these two concepts, which is seldom questioned (most sta-
tistical textbooks often use the two terms interchangeably without addressing
why). This equality is the great divide between the frequentist school and the
Bayesian school (which rejects the idea). The frequentist believes that proba-
bility is objective and can be deduced from repeatable experiments much like
coin tosses. This is simply untrue where extremistan is present—extreme events
in financial markets and irregularities that happen in the corporate world are
not repeatable experiments. Without the element of reproducibility, the fre-
quency histogram becomes a bad gauge of probability, and the quantile-based
VaR loses its probabilistic interpretation. Extremistan also broke the lore of
i.i.d. since if events are nonrepeatable (and atypical), they cannot be identical
in distribution.
Do we really need to be fixated on measuring risk? Suppose we admit
that the tail is unknowable and are mindful that Black Swans will occur
more frequently than suggested by model; we are then more likely to take
evasive actions and find ways to hedge against such catastrophes. This is the
message here. Taleb (2009b) suggested the idea of the fourth quadrant—a
zonal map to classify situations that are prone to Black Swan catastrophes so
that protective actions can be taken.
Taleb observed that the fourth moment (kurtosis) of most financial
variables is dominated by just the few largest observations or outliers. This is
the reason why conventional statistical methods (which work well on more
regular observations) are incapable of tracking the occurrence of fat-tail
events. Furthermore, there is evidence that outliers, unlike regular events, are
not serially dependent on past outliers; that is, there appears to be no
predictability at all in the tail.
In Taleb’s paper, distributions are classified into two types. Type-1
(mediocristan): thin-tailed Gaussian distribution, which occurs more often in
scientific labs (including the casino) and in physical phenomena. Type-2
(extremistan): unknown tail distributions which look fat (or thick) tailed.
The danger with type-2 is that the tail is unstable and may not even have
finite variance. This wild fat tail should not be confused with just meaning
having a kurtosis larger than the Gaussian’s, like that of the power-law tail
generated by EVT and other models. For this mild form of fat tail, theoretical
self-similarity at all scales means that the tails can always be defined
asymptotically. This allows for extrapolation to more extreme quantiles
beyond our range of observation, a potentially dangerous practice as we are
extrapolating our ignorance.
The Physics of Risk and Pseudoscience
149

Since measurements are always taken on finite samples, the moments
can always be computed, which gives the illusion of finiteness of variance.
This imperfect knowledge means that one can seldom tell the difference
between the wild (type-2) and the mild form of fat tails. In fact, evidence
suggests financial markets are mostly type-2.
Payoffs are classified into simple and complex. Simple payoffs are either
binary, true, or false games with constant payout, or linear, where the mag-
nitude of PL is a linear function like for stocks. Complex payoffs are described
by nonlinear PL functions that are influenced by higher moments. A good
analogy is to think of the different payoffs as a coin toss (binary), a dice throw
(linear), and buying insurance (asymmetrical, nonlinear, option-like).
The four quadrants are mapped in Table 7.2. The first quadrant is an
ideal state (such as in casino games) where statistics reign supreme. In the
second quadrant, statistics are predictive, even though the payoff is complex.
Most academic research in derivatives pricing in the literature assumes this
idealized setting. In the third quadrant, errors in prediction can be tolerated
since the tail does not influence the payoffs.5 The dangerous financial Black
Swans reside in the fourth quadrant. Unfortunately, market crises are found
to be extremistan, and positions held by banks are mostly complex.
Taleb argued that since one cannot change the distribution, one strategy
is to change the payoff instead; that is, exit the fourth quadrant to the third.
This can be done using macro hedges to floor the payoff so that the negative
tail will no longer impact PL. In practice this could require purchasing
options, taking on specific insurance protection, or changing the portfolio
composition.
A second strategy is to keep a capital buffer for safety. Banks are capital-
efficient machines that generally optimize the use of capital. However,
optimization in the domain of extremistan is fraught with model errors; a
simple model error can blow through a bank’s capital as witnessed in the
case of CDO mispricing during the credit crisis. Overoptimization can lead
TABLE 7.2
The Four Quadrants
Simple
Payoffs
Complex
Payoffs
DISTRIBUTION
1 (Thin tailed)
First Quadrant:
Extremely safe
Second Quadrant: Safe
DISTRIBUTION
2 (Fat or
unknown tails)
Third Quadrant: Safe
Fourth Quadrant: Exposed to
Black Swans
150
VALUE AT RISK METHODOLOGY

to maximum vulnerability because it leaves no room for error. Thus, one can
argue that leaving some capital idle will be necessary for a bank’s long-term
survival and protection against Black Swans. This of course is the motivation
behind the Basel’s Pillar I capital (explained in Chapter 11).
7.5 REGIME CHANGE, LAGGING RISKOMETER,
AND PROCYCLICALITY
The Lagging Nature of VaR
In the field of technical analysis, any trend-following indicator is known to
be lagging. Such indicators typically employ some form of moving average
(MA) to track developing trends. Because it takes time for sufficient new
prices to flow into the MA’s window to influence the average, this indicator
always lags significant market moves. Figure 13.4 shows a simple 1,000-day
MA of the S&P 500 index. To reduce lag (i.e., make the indicator more
responsive) traders normally shorten the window length, but this has the
undesirable effect of making the indicator erratic and more susceptible to
whipsaws (i.e., false signals). This is the catch-22 of using rolling-windows in
general—timeliness can only be achieved at the cost of unwanted noise.
This is instructive for VaR, which also uses a rolling window of returns.
The lagging behavior means that regime shifts will be detected late, often
seriously so for risk control. Consider a situation whereby the government
explicitly guarantees all banks. One can expect a sudden shift in the way
bank shares behave—their volatilities will be lower than before, they move
more in correlation as a group, and they may even exhibit some local
behavior (microstructure) such as mean-reversion within a certain price
range. VaR will be late in detecting these changing risks, if at all.
To test how VaR performs when faced with a regime shift, we simulate a
regime change and compare the responsiveness of pVaR, hsVaR, and EWMA
pVaR. In Figure 7.10, we simulated a doubling of volatility—the lower chart
shows the return series; the upper chart shows various VaRs. The shaded area
represents the 250-day window used to compute VaR, where the left edge
marks the regime shift. The volatility to the right of this demarcation is twice
that to the left. PVaR and hsVaR are able to detect this sudden shift but with
a time lag—they increase gradually until the new volatility has fully rolled
into the 250-day window. For EWMA VaR, its exponential weights shorten
the effective window length. And, by virtue of being conditional, it is able to
capture the regime shift fully in a fraction of the time. Unfortunately, time-
liness comes with instability—the EWMA VaR is a lot noisier.
In Figure 7.11, the shaded area marks a regime shift in autocorrelation,
suddenly changing from zero to þ0.5; the volatility is kept constant. The
The Physics of Risk and Pseudoscience
151

–0.05
0
0.05
0.1
0.15
0.2
–0.025
–0.015
–0.005
0.005
0.015
0.025
0.035
0.045
pVaR
hsVaR
EWMA VaR
Ln return
07-07-08
07-08-08
07-09-08
07-10-08
07-11-08
07-12-08
07-01-09
07-02-09
07-03-09
07-04-09
07-05-09
07-06-09
07-07-09
07-08-09
07-09-09
07-10-09
07-11-09
07-12-09
FIGURE 7.10
Simulated Regime Change—Doubling of Volatility
–0.05
0
0.05
0.1
0.15
0.2
0
15-10-08
15-11-08
15-12-08
15-01-09
15-02-09
15-03-09
15-04-09
15-05-09
15-06-09
15-07-09
15-08-09
15-09-09
15-10-09
15-11-09
15-12-09
0.005
0.01
0.015
0.02
0.025
0.03
pVaR
hsVaR
EWMA VaR
Ln return
FIGURE 7.11
Simulated Regime Change—Outbreak of Seasonality
and Autocorrelation
152
VALUE AT RISK METHODOLOGY

new return series (lower panel) shows an obvious seasonal behavior, which
we have introduced artificially. This test revealed that none of the three VaR
methods is able to detect the regime shift. This case study is worked out in
Spreadsheet 7.3.
We conclude that VaR is able to capture changes in volatility regime (it
is designed for this) but with an often fatal delay. Worse still, subtler regime
changes such as that in correlation and market microstructure may not be
detected at all.
Hardwired Procyclicality
The Turner Review (2009) identified that procyclicality is hardwired into the
VaR method. A dire combination of three factors makes this a potent threat
to the financial system. Firstly, procyclicality is rooted in the lagging nature
of VaR. Consider Figure 7.10—VaR took almost a third of the 250-day
interval to increase noticeably after the regime shift. This means VaR may
lag the onset of a crisis by months, and VaR-based regulatory capital will be
late in pulling the brakes on rising risks. Secondly, the leverage effect comes
into play by manifesting subdued volatility during a rally. As a result, the
regulatory capital will be very business-conducive (low) during the boom
phase. Thirdly, mark-to-market accounting, a global standard for regulated
fair accounting practice, allows banks to recognize their profits immediately
in a rally. The gain is often used as additional capital for further investment
and leverage. Otherwise, given so much liquidity (capital) and the general
perception of low risks, banks will likely be faulted by shareholders for
underinvestment. The net effect is that banks are encouraged to chase an
economic bubble.
When a financial bubble bursts, the incentives are completely reversed.
History tells us VaR often spikes up the most during the initial fall of a bust
cycle. This increases the VaR-based capital requirement and forces
deleveraging among banks in order to stay within the regulatory minimum.6
At the same time, volatility also grows by virtue of the leverage effect. The
self-imposed discipline to mark-to-market the losses means that a bank’s
capital base will shrink rapidly as the market collapses, forcing further
deleveraging. And since most of the money in the system is leveraged
(paperless accounting entries of borrowed money), they can vanish as
quickly as they were created—liquidity dries up. This could lead to a vicious
spiral for the banking system.7
As an overhaul, the Turner Review broadly recommended a “through
the cycle” capital regime instead of the current “point in time”8 (VaR)
regime. More importantly the FSA mentioned “introducing overt counter-
cyclicality into the capital regime”—reserving more capital during a boom,
The Physics of Risk and Pseudoscience
153

which can be used to cushion losses during the bust phase. This eventually
led to the Basel III countercyclical capital buffer for the banking book
(2010); see Section 11.5. As we shall see in Part Four of this book, bubble
VaR attempts to address the same problem in the trading book.
7.6 COHERENCE AND EXPECTED SHORTFALL
The purpose of VaR is to summarize the risk of the entire loss distribution
using a single number. Even though features of the tail of the joint distri-
bution are underrepresented, there is merit in using such a point estimate; it
is convenient and intuitive for risk control and reporting.
Artzner and colleagues (1999) introduced the concept of coherence—a
list of desirable properties for any point estimate risk measure. A risk
measure (denoted here as VaR) is said to be coherent if it satisfies conditions
(7.4) to (7.7).
Monotonicity:
if X1 ≤X2 then VaRðX1Þ ≥VaRðX2Þ
ð7:4Þ
If a portfolio has values lower than another (for all scenarios), its risk
must be larger. X corresponds to the P&L random variable of a risky
position.
Homogeneity: VaRðaXÞ ¼ aVaRðXÞ
ð7:5Þ
Increasing the size of the portfolio by a factor a will linearly scale its risk
measure by the same factor.
Translation invariance: VaRðX þ kÞ ¼ VaRðXÞ  k
ð7:6Þ
Adding riskless cash k to the portfolio will lower the risk by k.
Subadditivity: VaRðX1 þ X2Þ ≤VaRðX1Þ þ VaRðX2Þ
ð7:7Þ
The risk of the portfolio is always less than (or equal to) the sum of its
component risks. This is the benefit of diversification.
A few key points regarding coherence are worth knowing. Firstly, the
quantile-based VaR is not coherent—it violates the property of sub-
additivity, except in the special case of the normal distribution. This could
lead to an illogical situation where splitting a portfolio decreases the risk.
154
VALUE AT RISK METHODOLOGY

Secondly, and more generally, coherence holds for the class of elliptical
distributions, for which the contour of its joint distribution traces an ellip-
soid. The (joint) normal distribution is a special case of an elliptical distri-
bution. An elliptical distribution has very strict criteria—it is necessarily
unimodal (single peaked) and symmetric. Hence, most realistic distributions
are just not elliptical.
Why doesn’t incoherence wreak havoc on the lives of risk managers?9 It
is because incoherence violates the integrity of the VaR measure in a stealthy
and nonovert way. Consider the truism: an elliptical distribution is surely
coherent. But the reverse is not true; that is, a nonelliptical distribution need
not be incoherent. We can only say that coherence is not guaranteed; that is,
we cannot say for sure that subadditivity is violated in realistic portfolios.
Furthermore, even if subadditivity is violated, for a large portfolio, its
effect may not be obvious (or material) enough for a risk manager to take
notice, and may be localized within a specific subportfolio. The problem
will be felt when the risk manager tries to drill down into the risk for
small subportfolios containing complex products—he may get a nonsensical
decomposition.
Fortunately, there is an easy backstop for this problem called expected
shortfall proposed by Artzner and colleagues (1999). This brings us to the
third point—expected shortfall, sometimes called conditional VaR (cVaR) or
expected tail loss (ETL), satisfies all the conditions for coherence. It is
defined as the expectation of the loss once VaR is exceeded:
ETL ¼ EðX9X ≤ VaRÞ
ð7:8Þ
For all practical purposes, it is just the simple average of all the points in
the tail left of the VaR quantile, and thus can be easily incorporated using the
existing VaR infrastructure. For example, the ETL at 97.5% confidence
using a 250-day observation period is given by the average of the 6 (rounded
from 6.25) largest losses in the left tail. For most major markets, assuming
linear positions, the 97.5% ETL works out to be in the order of magnitude
of three sigmas. Hence, under regular circumstances expected shortfall is
roughly 50% higher than pVaR (which is 2 sigma) if positions are linear.
The ETL is sensitive to changes in the shape of the tail (what we are
really after). VaR, on the other hand, is oblivious to the tail beyond the
quantile level. In Section 13.6, we will show using simulation that ETL is
superior to quantile VaR as a risk measure in terms of coherence, respon-
siveness, and stability. Indeed, the latest BIS consultative paper Fundamental
Review of the Trading Book (2012) calls for the adoption of expected
shortfall as a replacement for VaR.
The Physics of Risk and Pseudoscience
155

7.7 SPREADSHEET EXERCISES
7.1 Empirically, bull markets tend to show negative skew in return dis-
tributions, and bear markets tend to show positive skew. Aim: Illustrate
a stylized bull (bear) market distribution and its corresponding negative
(positive) skew using random simulation.
7.2 Fat-tail distributions can be caused by stochastic (or variable) volatility.
Aim: illustrates using Monte Carlo simulation that stochastic volatility
gives rise to a fat-tailed distribution.
7.3 As a risk measurement tool, VaR is often accused of being late in
detecting crises and regime changes in general. Aim: Explore the time-
liness of VaR by comparing the performances of pVaR, hsVaR, and
EWMA VaR when encountering a simulated regime change. Two
regime shifts are tested: a doubling of volatility and an introduction of
serial correlation of þ0.5.
NOTES
1. The concept was introduced by Shannon (1948), the founder of information
theory. More recently, Dionisio, Menezes, and Mendes (2007) explored the
use of Shannon entropy as a ﬁnancial risk measure, as an alternative to variance.
Gulko (1999) argues that an efﬁcient market corresponds to a state where
the informational entropy of the system is maximized.
2. The VIX index is an average of the implied volatilities of S&P index options of
different strikes. It is a general measure of precariousness of the stock market and
is often used as a fear gauge.
3. This is because the volatility is also affected by local features of the market—for
example, as prices approach support and resistance levels, volatility may some-
times decline because traders place trading bets that these levels will hold.
4. This volatility skew (which looks like Figure 7.4) can be mathematically translated
into a PDF that slants to the left side—this implied distribution will disagree with the
observed distribution. Technically speaking, the risk-neutral probability inferred
from the current option market differs from the physical probability observed from
historical spot prices.
5. For example, option pricing usually assumes geometric Brownian motion (a
second quadrant setting). But if the tail is actually fat, the real pricing formula
will be different from the Black-Scholes equation. In contrast, simple linear
payoffs will be the same even if the tail is fat.
6. A well-researched counterargument is provided by Jorion (2002). The author
found that the averaging of VaR over the past 60 days as required by Basel
(equation (6.3)) means that market risk capital moves too slowly to trigger
systemic deleveraging.
156
VALUE AT RISK METHODOLOGY

7. Without liquidity, a bank risks not being able to pay its obligations on time,
which could lead to (technical) default. This has dire consequences, and hence
banks hoarded liquidity during the 2008 credit crisis.
8. A point-in-time metric measures the current state in a timely manner, hence, it
tends to move in synch with the market cycle. A through-the-cycle metric
attempts to capture the long-term characteristic of the entire recent market cycle.
9. The issue of coherence (and subadditivity in particular) currently gets little
attention from practitioners and regulators. It is debated more often in academic
risk conferences.
The Physics of Risk and Pseudoscience
157


CHAPTER 8
Model Testing
R
egulators require banks to test their value at risk (VaR) systems regularly
to ensure they are in working order. We review a few common tests: the
precision test, which is actually a measurement of the precision of the VaR
number given a chosen method and a data set; the frequency back test,
which is a requirement to ensure model “goodness” and is used by reg-
ulators to determine a bank’s multiplier for minimum capital; and the
bunching test (or independence test), which checks that VaR exceedences are
independent and identically distributed (i.i.d.); otherwise the VaR quantile
understates what it is meant to measure. There are many more sophisticated
statistical tests available for VaR; the interested reader can refer to Campbell
(2005) for a good review.
8.1 THE PRECISION TEST
Precision is an important element in any scientific measurement. For
example, when one reports the weight of an item in a lab experiment as
2.5 kg ± 0.2 kg, one really means the object’s weight lies between 2.3 and
2.7 kg. So it may come as a surprise that such error bands are seldom
included in VaR reports. To appreciate why, let us first see how such an
error band can be computed.
Since this book favors hsVaR as a basic method, we shall illustrate a
method called statistical bootstrapping, which uses empirical observations.
This method does not make any prior assumptions about the shape of the
distribution. It involves the following steps:
1. Compute the VaR of the original sample data (T data points).
2. Perform resampling from the same sample set with replacement and
then recompute the VaR—that is, draw T points from the sample one at
159

a time, putting each back into the sample before the next draw, so that
the sample remains undepleted.
3. Repeat step (2) K times such that a distribution of VaRs is obtained.
4. The error band (with a c% confidence interval) of the original VaR
number can be determined from this distribution, and is given by the
lower 0.5(1  c) quantile and the upper 1  0.5 (1  c) quantile.
Spreadsheet 8.1 implements this method for the S&P 500 index for
the period July 2008 to June 2009 with T ¼ 250, K ¼ 1000, c ¼ 0.95. The
results (which can be resimulated) indicate a 97.5% VaR of 6.2% move in
the index with a confidence interval between 5% and 9%, a huge error band
of 4%! This statistic means there is a 95% chance or confidence that the
“correct” value of VaR is in that range.1
To express these findings in dollar terms, consider a portfolio of S&P index
stocks of $100 million. The VaR is reported as $6.2 million, but could range
anywhere from $5 million to $9 million. Small wonder VaR is seldom reported
with error bands—it would have been astounding (and possibly incapacitating)
to the top management. More importantly, it also highlights the hazard of the
false sense of security provided by seemingly precise numbers.
VaR numbers are often reported to a few decimal places. In the above
case, if VaR fell to $5.65 million the next day, a less quantitative senior
executive would have concluded, with some relief, that risk has fallen by
$0.55 million. Yet, with such a large error band, this conclusion has very
little statistical meaning. Worse still, if the senior executive believes the real
“risk” (however it may be interpreted) is at $6.2 million—she would have
been precisely wrong (or fooled by randomness). The true expected loss
could be multiple times bigger than the reported VaR!
To see graphically what bootstrapping does, see Figure 8.1 (which can
be generated using Spreadsheet 8.1). The central distribution is the actual
return distribution, which is resampled many times. For each sample (250
points resampled 1,000 times), the quantile is computed. The quantile’s
distribution is shown as the dark histogram to the left (we have rescaled it to
look nicer, so ignore the vertical axis values). This histogram is centered on
the VaR. The 95% confidence interval for VaR is just the bounds that ring
fence 95% of the area of the dark histogram. We can see this error range is
wide and is always left-skewed.
8.2 THE FREQUENCY BACK TEST
The frequency back test (also called the unconditional coverage test) mea-
sures the biasness of the VaR model to see how precisely its results follow the
160
VALUE AT RISK METHODOLOGY

quantile definition. Regulators use this to decide the “goodness” of a bank’s
internal model and to adjust the multiplier for capital charges accordingly.
When a bank’s VaR model passes back testing, its internal model is deemed
acceptable by regulators. Otherwise, a more penal minimum capital is levied
using the traffic light approach outlined by Basel Rules:
k ¼
3:0
if
N ≤4 green
3 þ 0:2ðN  4Þ
if
5 ≤N ≤9 yellow
4:0
if
N≥10 red
8
>
<
>
:
ð8:1Þ
where k is the multiplier in equation (6.3), N is the number of days when
realized PL exceeds daily 99% VaR in the last 250 days. Daily PL as pub-
lished by product controllers is used.
If VaR is truly an unbiased estimate of the quantile, by definition we
expect 2.5 violations per year statistically. Basel uses this knowledge to scale
its parameter k: as long as the number of violations is 4 or less, the multiplier
remains at its lowest level of 3. If the VaR is violated more frequently than
expected theoretically, a bank is penalized proportionally by the capital
charge. In the red zone, the VaR model is deemed inaccurate (biased
or systematically understating risk), and that calls for immediate correc-
tive action to improve the risk management system. In contrast, too few
violations would imply that the bank’s model is overly conservative—it
systematically overstates risk.
Gross violation of back testing could be symptomatic of a few potential
problems:
0%
1%
2%
3%
4%
5%
6%
7%
8%
FIGURE 8.1
Graphical Illustration of Statistical Bootstrap
Model Testing
161

1. Deals are not marked-to-market properly, and published PL is wrong.
2. The universe of risk factors is poorly designed and is not representative
of the risks affecting the portfolio.
3. The VaR model is biased (bad assumptions or bad implementation).
4. The market data exhibits clustering (non-i.i.d.) behavior characteristic
of a stressful situation.
Problem (4) is beyond the bank’s control and will cause the VaR model
to understate the true quantile risk.
Figure 8.2 illustrates a typical back testing diagram as reported to reg-
ulators. This is implemented in Spreadsheet 8.2 for the Dow Jones index
for the period January 2006 to March 2009. Every day that a return exceeded
the 99% VaR (on either tails), the hit counter will register one count. For the
final 250-day rolling period, the hits add up to N ¼ 18 (red zone). Since this
is an error-free example, the only possible cause is problem (4)—the market
was exhibiting a strong degree of clustering during the 2008 crisis period.
In fact, a widespread breakdown of back tests can be an important
signal that the market is entering a stressful period in which VaR models are
less accurate (assuming model and data errors have already been ruled out).
Some markets, such as pegged currencies, are prone to occasional volatility
clustering, which undermines back tests. In this case, rather than attempting
to justify the model (a common response), resources are better spent on
measuring the misstated risks using other tools such as stress tests specifically
targeted at positions that are causing the back test breaks, or on hedging
such exposures.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
–0.15
–0.10
–0.05
0.00
0.05
0.10
Jan-06
Jun-06
Nov-06
Apr-07
Sep-07
Feb-08
Jul-08
Dec-08
daily return
99% VaR(+)
99% VaR(–)
Hit
FIGURE 8.2
Regulatory Back Testing Diagram for Dow Jones Index
162
VALUE AT RISK METHODOLOGY

8.3 THE BUNCHING TEST
This test, also called the independence test, is targeted at measuring clus-
tering. In Figure 8.2 we plotted the occurrence of a VaR violation by
marking its time with a hit function. The hit is actually given by the indicator
function:
ItðpÞ ¼
	 1
if
xt ≤ VaRt1ðpÞ
0
if
xt>  VaRt1ðpÞ
ð8:2Þ
where {x1, . . . , xN} is the sequence of daily PL for the portfolio, p is the
quantile (i.e., for 97.5% VaR, p ¼ 2.5%).
For a VaR model to be accurate in its predictions, violations must be
independent across time—the hits must be evenly spread out in time; they
must not be bunched up or clustered together. To see why, suppose hypo-
thetically that whenever a VaR violation occurs (for p ¼ 2.5%), it always
occurs in pairs, so that when one violation occurs it is immediately followed
by another of the same loss. Let’s say VaR is at $10 million. When one
violation has occurred with $10.5 million loss, the probability of the second
violation is 100% and no longer 2.5%. Hence, the reported VaR, now at
$10.5 million, no longer represents a 2.5% probability of (left-side)
exceedence—it is understating the true risk. To be correct, the VaR result
should be farther out in the tail (say $30 million) so as to reduce its
exceedence probability back to 2.5%. This example shows that in the
presence of bunching, VaR is inaccurate because its probability interpreta-
tion is distorted.
These issues encouraged research in academia for better VaR models—
“better” in the narrow2 sense of low bias and low bunching. Some condi-
tional models such as Generalized Autoregressive Conditional Hetero-
scedasticity (GARCH), Exponentially Weighted Moving Average (EWMA),
and Hull-White VaR can achieve this by being more responsive to market
innovation (new prices). These fast-moving systems are more adept at
moving the VaR number farther out into the tail when multiple hits occur,
such that the VaR number remains an unbiased measure of the real quantile.
In testing if the hits are evenly dispersed in time, we need to determine
if they are serially independent. There are many sophisticated tests of
independence, for example, the Christoffersen test (1998) and subsequent
development of this test in this area. Here we will content ourselves with
a simpler test, the Ljung-Box statistic (1978), which tests that the auto-
correlations of different lags are jointly zero. We use the h ¼ 15 lagged
autocorrelation of the hit function I(.). The statistics Q(h) follows a
Model Testing
163

chi-squared distribution with h degrees of freedom. A value of Q (15) less
than a critical value of 25 indicates that zero correlation cannot be rejected at
the 95% confidence level. This critical value can be determined from Excel
function: CHIINV(0.05,15) ¼ 25.
Qð15Þ ¼ n
X
15
k¼1
wkρ2
k
ð8:3Þ
where n is the number of observations, ρk is the autocorrelation with lag k
days, and wk ¼ (n þ 2)/(n  k).
Spreadsheet 8.3 implements the Ljung-Box statistic for the 99% VaR
back test violation (i.e., the hits I(0.01)) for the Dow Jones index for the
period from January 1982 to March 2009. Comparison is made for var-
ious VaR models all calculated based on ex post 250 days of data (i.e., It
violation is determined by looking at VaRt1). The Ljung-Box statistics are
Q(15) ¼ 478, 18, 165 for pVaR, EWMA VaR, and hsVaR respectively
which indicates that only EWMA VaR shows statistically insignificant
bunching at the 95% confidence level.
Figure 8.3 plots the hit function I(0.01) for the three VaR models. It is
apparent that the EWMA VaR shows the most spread-out hits. The other
two unconditional models show clustered exceedences of VaR. In case Fig-
ure 8.3 misleads you into thinking that EWMA VaR removes clustering, let
me clarify that it doesn’t. Clustering is a market behavior, which obviously
0
1
1
501
1001 1501 2001 2501 3001 3501 4001 4501 5001 5501 6001 6501
Historical Sim VaR
0
1
EWMA pVaR
0
1
Parametric VaR
FIGURE 8.3
The Plot of the Hit Function I(0.01) for pVaR, EWMA VaR,
and hsVaR
164
VALUE AT RISK METHODOLOGY

cannot be influenced by its measurement. The advantage of EWMA VaR is
just that its quantile interpretation is less distorted by the clustering effect.
8.4 THE WHOLE DISTRIBUTION TEST
Is there a way to test the frequency and bunching for all quantiles at the same
time? It turns out this can be done in a single test by using the so-called
Rosenblatt transformation:3
^FðXtÞBUð0, 1Þ
ð8:4Þ
where ^Fð:Þ is the empirical cumulative distribution function (CDF) estimated
ex-ante (i.e., at time t  1) and Xt is the ex-post realization of profit and loss
(P&L) at time t. In back testing, we want to test if the one-day forward (out
of sample) point belongs to the same distribution as the past sample used to
compute VaR. If the transformed probability ^FðXtÞ is distributed uniformly
as in equation (8.4), then it will mean that the VaR model has the correct
frequency and bunching for all quantiles.
Suppose the VaR on day t ¼ 250 is estimated from P&L scenario {x1,
x2, . . . ,x250}, we use this sample to create an empirical CDF. We apply the
CDF function to x251 to get the transformed probability ^FðX251Þ. We roll
the sample window forward by one day and redo the whole computation to
obtain the next transformed probability ^FðX252Þ and so on. Once we
obtained a set of transformed probabilities we can test to see whether it is
uniformly distributed. An example is given in Spreadsheet 8.4.
There is a barrage of statistical tests for uniformity; a popular choice is
the Kolmogorov-Smirnoff test. These tests are outside the scope of this book.
An intuitive approach is to plot the histogram of ^FðXtÞ and visually inspect
to see whether it looks uniform.
As a stylized example, we test if condition (8.4) holds true for an i.i.d.
Gaussian process with constant volatility. We use the historical simulation
VaR model with equal weights. Figure 8.4 is the distribution of the trans-
formed probabilities and it shows obvious uniformity—thus, the VaR model
is a good predictor of future P&L.
Next, we create a Gaussian process with stochastic volatility. It can be
seen in Figure 8.5 that the distribution is no longer uniform—thus, the VaR
model is biased. This is a worrying conclusion—as long as the VaR model
is equally weighted, VaR will always be biased when stochastic volatility is
present; that is, under most realistic situations. The solution is to apply some
form of weighting scheme (like the EWMA) so that the VaR moves up (or
Model Testing
165

down) quickly to catch up with rising (or falling) volatility such that the
model continues to give unbiased estimates.
In practice, the inclusion of premiums and interest rate accruals can
distort back-test results slightly. These P&Ls arise from the passage of time
and are not due to movement of risk factors. They are typically included in
the ex-post realized P&L computed by product controllers, but are absent
from the data sample used for VaR. This unwanted effect is however less
serious than that of stochastic volatility.
–
2
0.02
0.08
0.14
0.20
0.26
0.32
0.38
0.44
0.50
0.56
0.62
0.68
0.74
0.80
0.86
0.92
0.98
4
6
8
10
12
14
16
18
20
FIGURE 8.4
The Distribution of the Transformed Probabilities of a Gaussian
Process
–
2
4
6
8
10
12
14
16
18
20
0.02
0.08
0.14
0.20
0.26
0.32
0.38
0.44
0.50
0.56
0.62
0.68
0.74
0.80
0.86
0.92
0.98
FIGURE 8.5
The Distribution of the Transformed Probabilities of a Stochastic
Volatility Process
166
VALUE AT RISK METHODOLOGY

8.5 SPREADSHEET EXERCISES
8.1 A method to determine the confidence interval or error band of a VaR
result is statistical bootstrapping. Aim: illustrates use of the statistical
bootstrapping method to calculate the error band (or 95% confidence
interval) for VaR. Note: A VB code resamples 1,000 times 250 random
draws (with replacement) from an empirical distribution of 250 data
points.
8.2 Back testing is a regulatory requirement to monitor the “goodness” of a
VaR model. It checks that the number of days the P&L exceeded
the VaR level during the observation period is in agreement with the
definition of the quantile. Aim: illustrates the back testing method
stipulated by Basel Rules (the traffic light approach). Note: There is an
increased number of back test breaks during the 2008 crisis period.
8.3 Bunching tests check how evenly the VaR back test breaks are distrib-
uted over time. If the breaks are bunched up in time, it suggests the
presence of strong serial correlation and VaR becomes a biased estimate
of the real quantile. Aim: Illustrate how a simple bunching test can be
performed using the Ljung-Box statistic on breaks of the 99% VaR.
Three VaR models are tested—pVaR, hsVaR, and EWMA pVaR. The
hit function is also plotted to illustrate the effect of bunching. Action:
Modify the spreadsheet to test at 97.5% VaR. What are the limitations
of using the Ljung-Box statistic for a test of bunching?
8.4 A sophisticated back-test method is to test the whole distribution (i.e.,
all quantiles jointly) using the so-called Rosenblatt transformation. This
requires implementing the empirical CDF and testing if the transformed
probabilities are uniformly distributed. Aim: Implement the Rosenblatt
transformation for back-testing. Investigate the results for a Gaussian
process with constant volatility and stochastic volatility. Replace the
random variable X with a nonlinear function g(X) ¼ X þ 0.15X2 and
check if the results hold for nonlinear products.
NOTES
1. The conﬁdence interval idea rests on the (often unquestioned) assumption that
samples have ﬁnite variance and are distributed nicely in a bell shape (localized).
In the presence of extremistan, the distribution could be asymptotically explosive
(has inﬁnite variance).
2. I used the word narrow here because a VaR model that passes the back test
perfectly can still be useless for the purpose of capital buffer. A “perfect” score
means that the loss frequency and magnitude are correctly predicted, but the
Model Testing
167

model will still be late in registering a crisis, and hence suffers the consequences of
procyclicality.
3. See the paper by Rosenblatt (1952) for the mathematical proof. Note that in this
chapter we perform back testing on the risk factor X, but in real situations back-
testing is done on g(X) where g is (in loose notation) the pricing function, often a
nonlinear function such as the Black-Scholes equation. All the conclusions in this
section can be shown to hold for the case of a nonlinear function g(X) also.
168
VALUE AT RISK METHODOLOGY

CHAPTER 9
Practical Limitations of VaR
C
hapter 7 discussed the theoretical flaws of value at risk (VaR). Here we
consider the practical limitations when implementing and using a VaR
system. It is a novice’s mistake to assume that all identifiable missing risks
can be put back into the VaR model. There are practical blind spots that are
beyond the ability of VaR to cover.
9.1 DEPEGS AND CHANGES TO THE RULES OF THE GAME
Like any measurement device, VaR is a function of its information input. To
the extent that a market event does not produce timely data for input to the
VaR system, it follows that the risk for that event can never be captured. This
is the case for some types of events that are not registered in prices, even
though they may be anticipated by a knowledgeable risk manager.
The classic examples are currency controls and depegs. During the Asian
currency crisis from 1997 to 1998, the Malaysian ringgit was attacked by
currency speculators due to contagion from a currency crisis that started in
Thailand. The Thai central bank devalued the baht on July 2, 1997, after
which the ringgit depreciated from 2.52 to a high of 4.77 against the dollar,
a whopping 47% fall. To fend off currency speculators, the Malaysian
central bank imposed capital control in September 1998—pegging the
ringgit to the dollar at 3.8. After seven years, the central bank officially lifted
the peg on July 21, 2005. The ringgit is still semicontrolled to this day.
Figure 9.1 shows this chronology of events and the effects on VaR. At
point A, VaR was late to register the onset of the devaluation even though
traders and banks already knew the ringgit was under attack—the central
bank was openly defending the ringgit, and the baht had already devalued.
Currency control was imposed at point B, and for some time the price was
untraded until point C when the peg was removed. The lesson is that VaR
169

could not predict the imminent events at A, B, and C because these events are
not reflected in prior prices even though market participants knew about the
currency attacks and expected some extreme price breakouts.
Had a risk manager relied solely on VaR, he would have detected the
risk very late and rather abruptly, and might have lost his job. This high-
lights the importance of the subjective experience of risk professionals. If a
depeg risk is anticipated subjectively, quantifying it can easily be done using
stress testing. The market participants normally have some consensus
(sometimes even an opinion poll) of the size of the revaluation if it happens.
Hence, if a bank knows its position in a pegged currency (such as the HK
dollar and the Chinese yuan) and the estimated magnitude of revaluation, it
can conservatively estimate the potential loss, and take a suitable capital
buffer or hedge out the risk.
Currency control is an example of sudden changes to the rules of the
game; that is, to the way the market is traded. It causes an artificial segre-
gation between onshore and offshore forward markets, which then behave
differently because arbitragers cannot bring equilibrium to bear. Other
examples are a government nationalizing a bank, causing the bank shares’
volatility to diminish, and a corporate takeover causing the target company’s
shares to trade differently after the acquisition. In all these cases, a regime
shift took place, after which the old data history is no longer reflective of
future risk. The VaR forecast will be biased temporarily until the old data
rolls out of the 250-day window.
–2%
0%
2%
4%
6%
8%
10%
2.0
2.5
3.0
3.5
4.0
4.5
5.0
Jan-95
Sep-96
May-98
Jan-00
Sep-01
May-03
Jan-05
Ringgit
97.5%-hsVaR
A
B
C
FIGURE 9.1
Currency Control of the Ringgit and the Impact on VaR (97.5%
hsVaR)
170
VALUE AT RISK METHODOLOGY

9.2 DATA INTEGRITY PROBLEMS
VaR is only as good as its data. Since VaR is a point estimate, a single
number result for the entire portfolio and the joint distribution, it is difficult
to tell if it is tainted by erroneous data. Back testing is too crude for the
diagnosis. The solution is prevention. As such, most banks invest substan-
tially in data maintenance work, which consist of proper sourcing of data,
capturing of end-of-day data (EOD data snapping), rates cleaning (removing
bad points and filling up holes in the data set), and data interpolation.
We list below certain data problems that can undermine the integrity of
the VaR system:
1. Misspecified risk factor universe. The resulting VaR will not represent
accurately the true risk of the portfolio.
2. Operational error in data handling—data is filled wrongly during hol-
idays causing a misalignment of scenario sequence across different risk
factors. The affected profit and loss (P&L) vector is out of step with the
other vectors. This will give a meaningless aggregated VaR because
the correlation structure is broken.
3. Asynchronous data. Some markets close at different times of the day
even though all data are snapped at the same EOD moment. For
example, consider an Asian corporate dollar bond hedged by USD
swaps. The former ceases trading after Asian hours, while the latter
trades 24 hours. If data is snapped at the New York session close, the
collected last price will be a few hours apart; the VaR of the hedged
position will be misstated. One solution is to use interpolation to
forecast the Asian bond price at the New York closing time.
4. Using in-house trader input is inferior to third-party vendor data, and
may run into conflict of interests. Data vendors are independent, and
their data is often poll-based, whereas a trader has incentive to mas-
sage the data because the same prices are often used to determine P&L,
and eventually his bonus. This is akin to what analysts call accounting
window dressing where companies smooth out earnings in order to
show above-target profits consistently every year. At times, using in-
house data is unavoidable for emerging markets that are not well
covered by data vendors.
5. Stale data and bad proxies. Data problems are especially prevalent in
credit trading where, because of the issue-specific and bespoke nature of
the derivatives, prices are not actively quoted, resulting in missing data
and discontinuity in the time series. Such data will contain a high degree of
random error. As one may recall from the study of high school physics, if
Practical Limitations of VaR
171

we measure A with error of five units, and B with error of two units, the
result for (AþB) will show at least an error of five units. This can be a
problem when we aggregate the VaR of a credit book (with large error) to
the rest of the portfolio. The aggregated VaR will have an error dictated
by the worst bad data—to be exact, the factor that has the largest dollar
error in quantile estimation. The lesson here is that when performing risk
factor mapping, it is often better to choose a less perfect proxy with good
data (the basis risk can at least be estimated) than the correct risk factor
with bad data.
The above problems are by no means exhaustive. Since VaR is a regu-
latory reported number and is used for minimum capital charges for banks,
it is ultimately one of the safeguards for the financial system. There is a
moral obligation to be accurate.
9.3 MODEL RISK
We next consider model risks that are outside the scope of the VaR engine
but that nevertheless affect the VaR result.
Pricing Model Risk
The VaR method is just a system to shock risk factors and to calculate the
quantile from the resulting PL distribution. The translation from shifted
prices to actual PL involves full revaluation (for simulated VaR) or deriva-
tion of Greek sensitivities (for parametric VaR). In both cases, we need to use
a pricing model specific to the product. In a bank’s risk architecture (see
Section 3.1) this component is stored in a pricing library and can be called
from the VaR engine. An erroneous pricing model will result in a wrong (or
at least inaccurate) VaR number. The error could be due to the wrong choice
of pricing model, bad calibration,1 or a plain coding mistake.
More profoundly, the idea of correctness of pricing is sometimes not
clear-cut except for vanilla products that use standard pricing models that
are well accepted by the industry. This ambiguity gives rise to model risk. We
use a definition of model risk adapted from Rebonato (2003): “Model risk is
the risk of occurrence of a significant difference between the mark-to-model
value of a complex and/or illiquid instrument and the price at which the
same instrument is revealed to have traded in the market.” This difference
will not exist if the efficient market hypothesis (EMH) holds true. This
school of thought argues for the “law of one price” since any violation will
be arbitraged away by pseudo-arbitragers. But when pseudo-arbitragers are
172
VALUE AT RISK METHODOLOGY

hindered by cost concerns, illiquidity, or regulatory constraints, market
prices can stray from model prices significantly.
A popular parable illustrates this: An illiquid option market has been
observed at 20% volatility irregularly. After due analysis, traders decided
that the theoretical price should be centered around 20% and used this as the
mark-to-model price in the bank’s risk system. One day, there is a sudden
market quote of 10% (perhaps from a customer), and the option trader
quickly buys the option cheap (at 10%) and delta-hedges the position (with a
hedge ratio calibrated at 20%). Which volatility (10% or 20%) should be
used for risk pricing? The trader would naturally argue for 20% since this
would let him realize an immediate profit for the “arbitrage.” On the other
hand, a risk manager who strictly follows the mark-to-market accounting
cookbook would mark the option at 10% and provision the profit. The two
approaches have very different risk management implications. If the risk
system is marked at 20%, there will be no difference in models used by
traders and risk controllers, and the VaR will be very small, reflective of a
delta-neutral position. On the other hand, if the risk system is marked at
10%, the position will appear to be not fully delta neutral—VaR will be
material. The right approach depends on the subjective information and
intelligence gathered by the prudent risk manager; that is, the nature of the
counterparty, the consensus view on choice of models and price levels among
professional players, and so on. The paper by Rebonato (2003) provides a
topical discussion.
Strategies and Futuristic Information
The VaR framework is based purely on past data and rests on the premise
that past risk measurement is an unbiased predictor of future risk (i.e., the
next day’s). The VaR method does not accommodate futuristic information
even if available. Here I am referring to trading strategies—which are tra-
ders’ intentions, formalized and committed but not yet traded upon. VaR by
definition does not include such things.
Suppose a trader holds $100 million in stocks and runs a 5% daily VaR.
He has his stop-loss at 2% below current levels. Is his risk more than $5
million or just $2 million? His real risk is just $2 million assuming no loss
from slippage when exiting positions (a reasonable assumption for very
liquid markets). As a second example, consider the popular quant trading
strategy called the constant proportion portfolio insurance (CPPI) scheme
where the position is leveraged up when the portfolio gains, and deleveraged
when the portfolio loses. There are well-defined models to execute such a
trading strategy, often automatically buying and selling at predetermined
levels. In a deep market, such a CPPI program can never lose money beyond
Practical Limitations of VaR
173

a certain fixed amount. In other words, it has a floored payoff similar to a
call option. Unfortunately, because VaR does not account for strategies, it
will overlook the downside protection and overstate the risk.
This is less of a concern for regulatory capital since the overstatement of
risk leads to a more conservative capital number. However, it does blunt the
VaR risk measure as a monitoring tool. In particular, for program trading
desks and hedge funds that use predetermined trigger levels for trading, the
VaR number is completely misleading.
The solution is to use other tools. For CPPI-type strategies, one can use
Monte Carlo methods to simulate the path dependence of the strategy and
look at the PL distribution at a future horizon (say 10 days). For trades with
predetermined stop losses, the PL distributions of such deals should be
floored at the stop-loss levels. Both methods are compatible with the VaR
approach and can be integrated into the VaR system.
9.4 POLITICS AND GAMING
Since a VaR system has many moving parts, it is open to manipulation and
negligence. For example, a trader can game his VaR by altering the distri-
bution of the desk’s PL to reflect a fixed VaR but with a much larger tail risk
by taking on long-short market-neutral positions or by selling default swaps.
The trader effectively gains from small frequent premiums or positive carry
by taking on the long-term risk of an infrequent tail event (large loss).
This human or behavioral aspect is possibly the weakest link in the VaR
system. It arises from the imbalance of power and skewed incentives in the
way banks are organized. There is a principal-agent problem at play—
decision making in banks is an arm-wrestle between risk and return, between
risk managers and risk-takers. While the front office (trading) is a profit-
generating center, risk management is often seen as a support function in
most banks. Regulation calls for an independent reporting line for risk
management to reduce the conflict of interest, but it is often the case that
profit-generating functions are usually the sponsors of support functions.
A trader earns his profit-sharing bonus (often very high by earthly
standards) by making profits and taking risks. The risk manager who con-
trols risk on the other hand is almost never rewarded for stopping a bad deal
or criticizing a suspicious position or profit. Risk management is about loss
avoidance, and it is difficult to measure what has been avoided and hence
has not occurred. This lopsided compensation encourages a culture that is
not prudent. Traders have the incentive to paint a picture of low risk—stable
profitability to top management. If a lower risk number can be achieved by
174
VALUE AT RISK METHODOLOGY

gaming the technicalities of the risk engine, then this will allow more room
for trading, since risk-taking activity is ultimately controlled by VaR limits.
On the other hand, the reward system in risk policing creates potential
incentive for politics and lobbying. There is also a propensity to suppress bad
news, the don’t rock the boat culture. When things do go wrong, it is con-
venient to blame the models, which few could understand anyway. We have
seen this during the 2008 credit crisis where the popular press sensational-
ized the blame on quants for creating credit derivatives (similar to blaming
all physicists for the atomic bomb).
Regulators are beginning to realize the importance of the risk culture in
a bank. If critical risk information is not escalated to decision makers, even a
state-of-the-art risk system is ineffective. In this respect, moral courage is an
important trait of a risk controller. Justifiably most people will not put their
jobs on the line unless they are financially and ethically very secure. And
leaving banks to correct the power imbalance is impractical. We have seen
how self-regulation and bad corporate governance led to the downfall of
prestigious institutions in 2008.2 Perhaps the solution is to legislate and
provide a whistle-blowing avenue for insiders. Another possible solution is
to outsource the VaR reporting to an independent policing entity thereby
creating a layer of protection against potential gaming and negligence.
In this chapter, we reviewed a few “risks” that cannot be captured by
VaR. This is clearly not an exhaustive list. When faced with a modeling
choice, the risk modeler should ask the question: Is this risk statistical in
nature? That is, are the observations repeatable? If not, the risk should not
be modeled using VaR. In fact, it is less confusing to call this uncertainty (see
Knight’s uncertainty in Section 11.2). One can still model the uncertainty by
applying (imaginative) assumptions, but the results may often be discon-
nected from reality. A better choice is to manage them outside VaR models.
NOTES
1. Many advanced models used by banks today (such as the Heston model and the
local volatility model, which attempt to account for the volatility smile effect) do
not price options directly using observed volatilities. Rather, the implied volatility
surface needs to be calibrated ﬁrst. This is computationally complex.
2. For an insightful read of the corporate culture of Wall Street, please refer to “A
colossal failure of commonsense: the inside story of the collapse of Lehman
Brothers” by L.G. McDonald and P. Robinson (2009).
Practical Limitations of VaR
175


CHAPTER 10
Other Major Risk Classes
T
he recognition of major risk classes by the Basel Committee for inclusion
into the regulatory capital regime has been the driving force for the
development of their risk models. The original Basel I Accord (1988)
focused on credit risk, which is by far the largest risk class faced by banks.
An Amendment (1996) included market risk into the capital regime and
introduced the “internal models” approach (by de facto value at risk, or
VaR). The Basel II reform (2004) established operational risk as a major
risk, following the fall of Enron and WorldCom, two of the largest corpo-
rate bankruptcies caused by unauthorized trading and accounting scandal.
The 2008 credit crisis highlighted two other risk classes that were over-
looked—firm’s liquidity risk and counterparty credit risk. These are
addressed in Basel 2.5 rules (in 2009), which were subsequently subsumed
into Basel III (in 2010). This chapter gives a brief overview of risk classes
(other than market risk) and the conventional models used to quantify them
before Basel III. In the next chapter, we look at the new models of Basel III.
From a regulatory capital perspective, one challenge is in aggregating
these diverse classes of risk—credit, market, operational, counterparty, and
liquidity. This so-called problem of aggregation to merge the various risks
under a unified theory has attracted much interest from academics. The last
section addresses the challenges of aggregating these very different items.
10.1 CREDIT RISK (AND CREDITMETRICS)
Credit risk refers to the risk of default and risk of ratings downgrade of
companies. Such events are company (or name or issuer) specific and often
happen when the creditworthiness (credibility to raise funds) of a company is
fundamentally impaired or is perceived by the market to have deteriorated.
177

Since banks and investors often hold exposures to companies, they are
exposed to credit risk. Such exposures come in various forms. If the exposure
is in the form of a (nontradable) loan to a company, then this is technically a
banking book credit risk. If the exposure is in bonds or securities issued by
that company, or credit derivatives written on that company, then this credit
risk is a component of trading book1 market risk. If the exposure exists
because the bank traded with that company and funds are owed, then it is
considered a counterparty risk. Such distinctions are inconvenient but nec-
essary because the nature of data, modeling method, accounting rules, and
regulatory capital treatment are all very different.
In this section, we will look at the modeling of credit risk of loans and
traded instruments using CreditMetrics developed by J.P. Morgan in 1997.
There are a few other popular models available; see Crouhy and colleagues
(2000) for a comparative analysis.
CreditMetrics asks: “Over a one-year horizon, how much value will a
loan portfolio lose within a given confidence level (say 5%) due to credit
risk?” The answer, expressed as credit VaR, will incorporate both risk of
default and rating downgrades. Our example uses simple Monte Carlo
simulation and will illustrate all the key ingredients of CreditMetrics. It helps
to divide the calculation process into three steps.
Step 1: Defining Various States of the World
A bond issuer’s creditworthiness is often rated by credit rating agencies such
as Standard & Poor’s and Moody’s. For loans, banks typically use internal
credit scores. To measure the risk of deterioration in credit quality, we need
the probabilities of migrating from the initial state to another rating level.
This information is contained in the transition matrix published regularly by
rating agencies. Table 10.1 is an example. The numbers reflect the average
annual transition probability from initial state to final state one year later.
These statistics are compiled based on actual observations of firm defaults
bucketed by specific industry sectors and ratings over a long history (i.e.,
there will be one matrix per industry sector).
While the rating transition matrix has been generally accepted by the
industry for risk modeling, it is often criticized for being backward-looking.
Observations of actual bankruptcies occur very slowly and will not capture
the shift in market expectations of a credit deterioration of a company
(expectation is forward-looking). Hence, some banks attempt to model the
transition probabilities as a function of credit spreads and macroeconomic
variables that are more contemporaneous.
178
VALUE AT RISK METHODOLOGY

Step 2: Revaluation of Loan Portfolio
The loan portfolio will need to be revalued using the 1-year forward credit
spread curve. For simplicity, we consider the case of a simple bond with
fixed annual coupon, c. The value of the bond in one year including the next
paid coupon (first term) is:
Forward value :P ¼ c þ
X
n
i¼1
c
ð1 þ fi þ siÞi þ
100
ð1 þ fn þ snÞn
ð10:1Þ
where there are n coupons left after one year, si is the forward spread,2 and fi
is the forward risk-free discount rate, which can be derived from today’s
discount curve3 ri from the simple compounding relation:
expðriþ1tiþ1Þ ¼ expðritiÞ exp

fi ðtiþ1  tiÞ

ð10:2Þ
where i ¼ 1, . . . , n, time ti is expressed in fractional years and (tiþ1  ti) is
the coupon period. Since we can observe from the market a unique spread
curve si for every rating in the matrix, we can compute the value for each
bond in the portfolio for all the seven ratings using (10.1). The defaulted
state’s value is simply given by the recovery rate4 multiplied by the forward
bond value.
Spreadsheet 10.1 is a toy implementation of CreditMetrics for a port-
folio of three bonds. In order not to digress from the main presentation, we
will not show the valuation of the bonds as per equation (10.1), but simply
TABLE 10.1
One-Year Transition Matrix (%) (numbers below are hypothetical)
Rating at Year-End (%)
Initial Rating
AAA
AA
A
BBB
BB
B
CCC
Default
AAA
90.87
8.25
0.70
0.06
0.12
0.00
0.00
0.00
AA
0.69
90.45
7.99
0.65
0.06
0.14
0.02
0.00
A
0.09
2.25
91.02
5.58
0.74
0.26
0.01
0.06
BBB
0.02
0.33
6.03
86.76
5.36
1.19
0.12
0.18
BB
0.03
0.15
0.66
7.79
80.51
8.76
1.02
1.09
B
0.00
0.11
0.24
0.43
6.55
83.42
4.17
5.08
CCC
0.22
0.00
0.22
1.31
2.36
10.85
65.52
19.52
Other Major Risk Classes
179

state the results in Table 10.2. This table will be used subsequently to look
up the bond values at each simulated credit rating end state.
Step 3: Building Correlation
Credit migrations of different companies are expected to show positive
correlation because companies tend to be influenced by common macro-
economic drivers and the business cycle. But how can we measure a default
correlation, which cannot be observed?
The solution is to consider Merton’s model (1974), which sees the
equity position of the borrowing firm similar to holding a call option on
the assets of that firm. The idea is that because of limited liability, equity
holders can never lose more than their original stake. Company default is
then modeled as the fall in asset prices below the firm’s debt (i.e., the option
“strike”). Thus, default correlation can be modeled by the correlation of
asset returns, which in turn can be measured as the correlation of readily
observed equity returns.
Once the correlation matrix is derived using equity returns, it needs to
undergo Cholesky decomposition before it can be used to simulate correlated
random asset returns. The asset returns are assumed to follow a normal
distribution. A common misconception is that by assuming normal distribu-
tion, we are somehow not capturing the well-known fat-tail effects in credit
spread changes. This is not the case, however, because we are assuming
normality for the (stochastic) drivers of credit changes, not the credit changes
themselves. In fact, as will be seen later, the final portfolio distribution (Figure
10.2) is heavily fat-tailed.
TABLE 10.2
Bond Values of Different End States at the One-Year
Horizon ($/million)
Bond 1
Bond 2
Bond 3
Initial Rating
BBB
A
CCC
Rating at one year
AAA
5.375
3.132
1.171
AA
5.368
3.13
1.165
A
5.346
3.126
1.161
BBB
5.302
3.113
1.157
BB
5.081
3.063
1.142
B
4.924
2.828
1.137
CCC
3.846
2.174
1.056
Default
1.8557
1.2504
0.3168
180
VALUE AT RISK METHODOLOGY

To translate a random draw from the multivariate normal distribution
to an asset value in the end state, we need a mapping table. This is created by
partitioning the scale of end ratings according to the normal distribution
scale by matching the transition probability. See Figure 10.1.
To obtain the portfolio distribution, Monte Carlo simulation is
employed to generate three correlated random variables (one for each bond)
for a large number of scenarios. Based on the drawn asset returns, we can
read from the partition the corresponding end states of the bond’s rating.
And, based on the end states, we can read off Table 10.2 the final dollar
Asset return <–2.91
–2.91--> –2.74-->–2.17 –2.17-->–1.49–1.49-->1.52 1.52-->2.69 2.69-->3.54 >3.54
New rating Default
CCC
B
BB
BBB
A
AA
AAA
FIGURE 10.1
Partition of Asset Change Distribution for BBB Issuer (Bond 1)
0%
10%
20%
30%
40%
50%
60%
70%
8.05 8.16 8.28 8.39 8.50 8.62 8.73 8.85 8.96 9.07 9.19 9.30 9.41 9.53 9.64
Mean value
95% worst
case loss
al
FIGURE 10.2
Portfolio Distribution and Credit VaR
Other Major Risk Classes
181

value of the bond at the horizon. The three bond values are added per
scenario to give the final portfolio distribution.
Credit VaR is then defined as the loss quantile of this distribution at a
given confidence level. Figure 10.2 shows that the 95% worst case loss from
the mean is $0.6 million. See Spreadsheet 10.1 for a worked-out example.
Note that traditional credit portfolio models such as Creditmetrics are
now superseded by more sophisticated Basel III models such as the IRC (see
Chapter 11).
10.2 LIQUIDITY RISK
What Exactly Is Liquidity Risk?
Liquidity risk has long been identified as a key risk in financial assets. To
quantify it, it helps to first define what liquidity risk is. The common term
liquidity risk is loosely defined and could mean a few things:
1. The risk of not being able to meet funding obligations as a firm leading
to a bank’s failure or bankruptcy. This is funding liquidity risk or firm
liquidity risk.
2. The risk in paying the bid-ask cost when forced to liquidate a position
during normal market conditions. Forced liquidation can happen for a
variety of reasons—closure of business unit, bankruptcy of the bank, a
VaR limit being exceeded, stop-loss being triggered, and so on. In all
cases, it is a reluctant and almost immediate unwind of positions. Since
there is uncertainty in bid-ask costs, there is risk.
3. The slippage loss when liquidating a large position. When the forced-
sale position is substantial and the market is illiquid, interbank players
can be easily tipped off by large irregular transactions. Shrewd traders
will exploit the situation. The bid-ask quote will widen against the bank
that is trying to exit the position.
4. In a disrupted market environment, such as in a crisis, risk-averse
participants will either not quote any bid-ask prices or quote them with
unrealistically wide spreads. This increased cost reflects the risk of
mark-to-market (MTM) uncertainty.
Consider the first in the list. In banks, the management of day-to-day
liquidity is run by the asset & liability management (ALM) traders and risk
controllers. The subprime crisis in 2008 has witnessed how the sudden
appearance of funding liquidity risk almost caused the complete collapse of
182
VALUE AT RISK METHODOLOGY

the world financial system. The fear of counterparty default had made banks
reluctant to lend money to each other, which resulted in the hoarding of
liquidity. The global payment system choked as liquidity dried up. After the
credit crisis, the Bank for International Settlements (BIS) tightened regula-
tions by requiring a liquidity coverage ratio, which stipulated the minimum
liquid assets banks must hold to cover a stressful period of 30 consecutive
days. This is discussed in Section 11.4.
The last three risks in the list are collectively known as market liquidity
risk. MTM uncertainty is a risk that gained notoriety during the credit crisis
and is caused by the “insidious growth of complexity”—unchecked product
innovation driven by the speculative needs of investors. As such products are
over-the-counter (OTC) derivatives—bespoke and illiquid—their risk is
difficult to regulate and is stealthy in nature. No single regulator monitors
the total outstanding notional globally or the identity of the counterparties
holding such derivatives.
For example, the models used to price credit default obligations
(CDOs) broke down during the 2008 crisis and led to a loss of confidence
in using such models (price quotes disappeared), even though trillions of
dollars of such derivatives had already been issued. With complex pro-
ducts, MTM uncertainty is intricately linked to model risk. By the defini-
tion in Section 9.3, model risk is the difference in theoretical and observed
traded prices. During the CDO debacle, both were indeterminate and
indeed this eventually led to the setup of the Troubled Asset Repurchase
Program (TARP) by the United States government. The TARP was tasked
with disposing of these toxic products over time in an orderly fashion in
order to reduce further fallout to the financial markets.
Liquidity-Adjusted VaR or L-VaR
In recent years, there has been development in incorporating market
liquidity risk into the VaR framework. For a review of such models please
refer to Sebastian and Christoph (2009). Generally, the models attempt to
include the bid-ask costs into the VaR distribution (or return scenarios).
In the more advanced models, the bid-ask cost is weighted by transac-
tion volumes using data from exchanges. Such models not only consider the
bid-ask cost but also the depth of the limit order book. As an example,
suppose the bank has q ¼ 1,000 lots of shares of a company. The liquidity
cost may be given by:
LðqÞt ¼ 0:5 aðqÞ  bðqÞ
xmid


t
ð10:3Þ
Other Major Risk Classes
183

where a(q) is the average ask price weighted by transaction volumes as a
trader sweeps the limit orders on the offer side in the limit order book, to fill
1,000 lots. Likewise, b(q) is similarly defined on the bid side. Since most
banks mark their position at mid price xmid, the multiplier of 0.5 reflects that
only half of the round trip transaction cost is incurred. This percentage
liquidity cost is converted into continuous terms by: l(q) ¼ ln(1  L(q)).
Then, the asset’s return net of liquidity cost is given by:
rnetðqÞt ¼ rt þ
lðqÞt
if long
þlðqÞt
if short
	
ð10:4Þ
where rt is the asset return on day t. To calculate the liquidity-adjusted VaR
(L-VaR), we use rnet(q)t in place of rt for scenario deal repricing and taking
of the quantile. Equation (10.4) essentially widens both sides of the return
distribution by an amount given by the liquidity cost as observed on indi-
vidual past dates.
L-VaR rests on two unrealistic assumptions. Firstly, it assumes past
liquidity cost (of the last 250-day observation period) is a good predictor of
future liquidity cost (in the next 10-day horizon). This is difficult to justify if
we reflect that the credit default swaps (CDS) bid-ask cost prior to 2007
would have understated liquidity costs during the credit crisis in 2008,
and the bid-ask cost in 2008 would have overestimated liquidity costs since
the crisis.
Secondly, L-VaR assumes the entire bank’s positions were unwound
everyday in the past observation period. Clearly a bank will only liquidate
entire holdings when it is in credit difficulties. The probability of such a mass
liquidation event is small and is not accounted for in L-VaR, leading to
possible overstatement of risks.
A Possible Add-On Formula
In this section we propose a simple add-on that can be used for reserve
purposes. First, we attempt to include slippage in the bid-ask spreads by
defining the adjusted spread as:
Aspreadn ¼ qndn
Nn
 dn
2
ð10:5Þ
where qn is the bank’s net notional holdings of asset n. For an asset n, dn is its
bid-ask spread, Nn is the quoted maximum size that can be traded in a single
transaction without moving the bid-ask quote. Equation (10.5) mimics the
184
VALUE AT RISK METHODOLOGY

“sweeping” of a limit order book. For example, if Nn is 200 lots, and a bank
needs to liquidate immediately q ¼ 1,000 lots, the transaction will con-
summate five levels of bid-ask spreads in the order book, assuming the limit
orders are homogenously queued. The second term reflects that a bank only
crosses half the bid-ask spread at the first level.
The latest Nn and dn can be subjectively observed from over-the-counter
(OTC) markets, or automatically collected for exchange-traded products.
For illiquid and bespoke products, a bank can use the spreads it quotes to a
typical client. A bank would have a good idea how much transactional cost it
wants to charge a client based on traders’ assessment of the complexity,
liquidity, and hedging costs of the product.
When “crossing the bid-ask” we can assume linearity in pricing even for
option-related products because the difference in price levels between bid
and ask is relatively small. A sensitivity approach is thus valid:
BidAskLoss ¼
X
j
n¼1
pnAbsðsensitivityn × AspreadnÞprimary
ð10:6Þ
where j is the number of assets, and pn a probability factor (explained in the
next section). We include only the primary risk factor’s bid-ask spread for
each asset n. This recognizes the way products are traded between profes-
sional counterparties—the bid-ask of only one primary risk factor is crossed.
For example, the risk factors for an FX option are spot, interest rates, and
volatility, but actual trading is quoted in volatility terms; hence the volatility
bid-ask is the only cost. The relevant sensitivity is vega in this case.
Equation (10.6) is calculated and summed across all j assets without any
offsets. Since liquidity cost cannot be diversified away, netting between long
and short is not allowed, except for instruments that are strictly identical in
risks. Another exception should be made for non-credit-risky cash-flow
products because cash flows of two such deals are considered fungible.5 One
way is to bucket such cash flows into time pillars (with offsets) and then treat
each pillar as a separate asset for the purpose of equation (10.6).
Taking Probability into Account
The pn in equation (10.6) accounts for the probability of a mass liquida-
tion event. A rough estimate is the probability of default for the bank or its
interbank counterparty (for asset n), as implied by their CDS spreads.
After all, it is reasonable to assume that when a bank fails, it has to liq-
uidate its holdings, or when its counterparty fails, the bank has to hedge
outstanding positions.
Other Major Risk Classes
185

As regulators use a 10-day horizon for safety capital, it makes sense to
use a 10-day default probability derived from a 10-day CDS spread. (If this is
not readily observed, the benchmark 5-year CDS spread can be used.) The
marginal default rate is related to CDS spread by:
pn ¼ 1  exp½sΔt=ð1  RÞ
ð10:7Þ
where s is the larger of the bank’s own CDS spread and the counterparty’s
CDS spread. Δt ¼ 10/250 and R ¼ 0.45, an assumed average default
recovery rate for the banking sector.
To get a sense of magnitude of the add-on using this simple model,
consider a credit derivative position of just one issuer with a notional of
$500 million and maturity of 5 years. The add-on calculation is shown in
Table 10.3.
The add-on may seem small for a $500 million position, but since net-
ting or diversification is not permitted (in general), these costs will add up
very quickly on a firmwide basis as the portfolio size grows.
TABLE 10.3
Calculation of Market Liquidity Risk Add-On for $500 Million
Position
Exposure to a Single Credit Derivative Issuer
Holdings of 5-year CDS
$ 500,000,000
Bid-offer spread (BP)
5.00
Maximum liquid size
$ 20,000,000
Adjusted bid-ask spread
122.50
Credit sensitivity per BP
$ 250,000
Bid-offer loss
$ 15,312,500
Calculation of Default Probability
Own CDS spread (BP)
200
Counterparty CDS spread (BP)
300
Recovery rate
0.45
Horizon (year)
0.04
10-day default probability
0.2179%
Expected liquidity loss
$ 33,373
186
VALUE AT RISK METHODOLOGY

10.3 OPERATIONAL RISK
Operational risk modeling is still in its infancy. There is no consensus on the
definition of operational risk yet. Some banks broadly define this as risks
other than credit and market risks—clearly not a very useful definition.
Others define operational risk by identifying what it includes—as risk arising
from human error, fraud, process failure, technological breakdown, and
external factors (such as lawsuits, fire hazards, natural disaster, customer
dissatisfaction, loss of reputation, etc.). With such a broad definition, risk
identification and classification (also called taxonomy) is an enormous
challenge. Compounding the problem is the general lack of statistical data
for all these individual items. Banks normally make do with a combination
of in-house data and external data provided by vendor firms.
Fortunately, the Basel Committee provided some guidelines. For the
purpose of compiling statistics—frequency and severity of losses—identified
items should be bucketed into a grid similar to Figure 10.3. The grid
represents various combinations of business line (BL) and event type (ET).
Ideally the grid must be comprehensive, and each cell should not contain
overlapping risk types.
Of course, not all items need to be modeled. It really depends on
materiality. Consider Figure 10.4, which shows a zonal map of operational
System Failures and Business Disruption
Accident and Damages to Physical Asset
Employment Malpractices and Workplace Hazards
External Fraud
Internal Fraud
Agency Services
Trading and Sales
Retail Brokerage
Retail Banking
Payment and Settlement
Commercial Banking
Corporate Finance
Asset Management
Process Efficiency, Execution, and Delivery
Business Malpractices–Client and Product
FIGURE 10.3
Operational Risk Factor Grid
Other Major Risk Classes
187

risk. Clearly, resources should not be spent on items that are of low likeli-
hood and low severity (for example, staff being late for work). On the other
extreme, an event that occurs very frequently and of high severity represents
an anomaly that should be investigated immediately, not modeled (for
example, a bank experiencing 10 robberies in a single year). Hence, the focus
for quantitative modeling is the zones marked “expected loss” and “unex-
pected loss/exceptional loss.” Expected loss (EL) is the loss due to process
failures, unexpected loss (UL) is typically due to internal control weakness,
and exceptional loss (XL) is often a Black Swan event. This division is shown
in the loss distribution (Figure 10.5 lower panel).
A popular way to model operational risk (if data is available) is by
using an actuarial approach. The result can be expressed in terms of loss
quantile consistent with VaR used for market risk and credit risk. This
loss number is called operational risk VaR or OpVaR. Each cell of the grid
in Figure 10.3 is assumed to be a standalone risk factor independent of
others. Thus, the OpVaRs for each risk factor are simply summed without
considering correlation.
For a particular risk factor, historical data on loss are collected for
observed events. These are used to model the frequency distribution f(n) and
severity distribution g(x|n ¼ 1) where x is the loss for the event, n is the
number of events per annum. Hence, g(x|n ¼ 1) is the loss density function
conditional on a single event (i.e., loss per event). These are shown in the
Medium Risk
(EL)
The Mapping of Operating Risk
Likelihood of Operational Failure
High Risk
Medium Risk
(UL, XL)
Low Risk
Severity of Loss
FIGURE 10.4
The Zonal Map of Operational Risks
188
VALUE AT RISK METHODOLOGY

upper two charts in Figure 10.5. Since OpVaR is normally measured at a one
year horizon, the density f(n) is the probability of n events per annum.
We then use a process called convolution to combine f(n) and g(x|n ¼ 1)
to derive a loss distribution. Simplistically, this can be done by tabulation;
that is, systematically recording all possible permutations of severity for
every n. Table 10.4 shows an example of frequency and severity distribu-
tions. Table 10.5 illustrates the tabulation process used to derive the loss
distribution. For example, for n ¼ 2, there are nine ways to obtain two
events given three possible severity values. For each permutation, the total
loss is just the sum of losses for those two events. The probabilities can
simply be read off Table 10.4. For example, for the sixth row, the joint
probability of n ¼ 2, loss of 1,000 and loss of 10,000 is 0.15  0.5 
0.3 ¼ 0.0225. Spreadsheet 10.2 provides a worked-out example.
The expected loss (EL) is given by the product of the expectation of the
frequency distribution and the expectation of the severity distribution; 0.75 
23500 ¼ 17625. It can also be calculated from Table 10.5 by taking the
product of total loss and probability, and then summing the products across
all permutations. The plot of the total loss vs. probability is the loss distri-
bution shown in Figure 10.5 (note that the horizontal axis is not scaled
uniformly). It is clearly fat-tailed. The 99.9% OpVaR is then defined by the
0%
10%
20%
30%
40%
50%
–
2,000 
10,000 12,000 21,000 100,000 102,000 111,000 200,000 210,000 
Loss distribution
EL = 17,625
99.9% VaR
EL
UL
XL
0%
10%
20%
30%
40%
50%
60%
0
1
2
3
Frequency distribution
0%
10%
20%
30%
40%
50%
60%
$1,000
$10,000
$100,000
Severity distribution
FIGURE 10.5
Operational Risk Loss Distribution
Other Major Risk Classes
189

loss quantile at the tail. We consider the region beyond OpVaR as repre-
senting exceptional loss (XL). The region between EL and XL gives the
unexpected loss (UL). From the diagram, OpVaR ¼ ELþUL.
Under Basel II’s loss distribution approach (an “internal model”
approach), the operational risk capital for banks is a function of the sum of
OpVaR for all combinations of business line versus event type (BL-ET). The
OpVaR is defined on a 99.9% quantile over a one-year horizon. OpVaR is
used for regulatory capital. For a discussion about the challenges of OpVaR,
see Coleman (2010).
Many experts believe that OpVaR cannot be modeled meaningfully
because of scarcity of data and is just a tick in the box exercise to satisfy
regulatory requirements.
In the absence of an effective model, the subjective judgment of a vigilant
risk manager is paramount. For example, operational risk events tend to
happen in a sequence (correlated)—the Enron and Worldcom scandals hap-
pened around the same time; likewise more recently in 2012, the Libor fixing
scandal, a United Kingdom bank’s money laundering involving Mexican drug
lords, and another United Kingdom bank’s alleged dealings with Iran (a
country under U.S. sanction). This is no coincidence—a scandal will lead to
increased scrutiny from regulators, which leads to uncovering of more scan-
dals. The size of losses from legal settlements and fines is huge. A vigilant risk
manager who monitored events closely, would have notched up the opera-
tional risk capital by a loss amount guessed from similar recent events.
10.4 THE PROBLEM OF AGGREGATION
The Danger of Adding Apples to Oranges
As directed by Basel, capital charges for various risk classes are calculated
separately—some based on internal models (as for market risk) and others
TABLE 10.4
Frequency and Severity Distributions
Frequency Distribution
Severity Distribution
Frequency (p.a.)
Probability
Severity
Probability
0
50%
$1,000
50%
1
30%
$10,000
30%
2
15%
$100,000
20%
3
5%
Expectation
0.75
Expectation
$23,500
190
VALUE AT RISK METHODOLOGY

TABLE 10.5
Tabulation of Loss Distribution
Number of Loss
Loss #1
Loss #2
Loss #3
Total Loss
Probability
0
0.5
1
1000
1,000
0.15
1
10000
10,000
0.09
1
100000
100,000
0.06
2
1000
1000
2,000
0.0375
2
1000
10000
11,000
0.0225
2
1000
100000
101,000
0.015
2
10000
1000
11,000
0.0225
2
10000
10000
20,000
0.0135
2
10000
100000
110,000
0.009
2
100000
1000
101,000
0.015
2
100000
10000
110,000
0.009
2
100000
100000
200,000
0.006
3
1000
1000
1000
3,000
0.00625
3
1000
1000
10000
12,000
0.00375
3
1000
1000
100000
102,000
0.0025
3
1000
10000
1000
12,000
0.00375
3
1000
10000
10000
21,000
0.00225
3
1000
10000
100000
111,000
0.0015
3
1000
100000
1000
102,000
0.0025
3
1000
100000
10000
111,000
0.0015
3
1000
100000
100000
201,000
0.001
3
10000
1000
1000
12,000
0.00375
3
10000
1000
10000
21,000
0.00225
3
10000
1000
100000
111,000
0.0015
3
10000
10000
1000
21,000
0.00225
3
10000
10000
10000
30,000
0.00135
3
10000
10000
100000
120,000
0.0009
3
10000
100000
1000
111,000
0.0015
3
10000
100000
10000
120,000
0.0009
3
10000
100000
100000
210,000
0.0006
3
100000
1000
1000
102,000
0.0025
3
100000
1000
10000
111,000
0.0015
3
100000
1000
100000
201,000
0.001
3
100000
10000
1000
111,000
0.0015
3
100000
10000
10000
120,000
0.0009
3
100000
10000
100000
210,000
0.0006
3
100000
100000
1000
201,000
0.001
3
100000
100000
10000
210,000
0.0006
3
100000
100000
100000
300,000
0.0004
Other Major Risk Classes
191

on a set of rules given by regulators (as for credit risk). To compute the
total charge, the regulator’s current treatment is to just sum them up. This
is nothing more than a recipe to obtain the aggregated final charge—there is
little conceptual justification. This method suffers from at least three
drawbacks, which we will examine closely: (1) It ignores the possibility of
risk diversification or offsets; (2) under certain situations the aggregated risk
can actually be less conservative; and (3) the final number is difficult to
interpret.
Firstly, one can argue that diversification is the hallmark of a good risk
measure, without which a risk model tells us an incomplete story. For
example, while diversification is not captured in operational risk VaR, we
intuitively know its necessity. Why do banks normally fly their top execu-
tives to overseas functions on separate flights? The bosses know intuitively
to diversify this operational risk (accident hazard). Likewise, a bank that
relies on multiple sources of funding has diversified its firm liquidity risk as
compared to a bank that relies solely on wholesale borrowing. Yet presently
there is no standard approach for modeling firm liquidity risk, let alone its
diversification effect.
It is important to realize that the simple summation method implicitly
assumes a correlation of one, which is more conservative than the assump-
tion of zero correlation. In other words, “no correlation” does not equate
zero correlation. Clearly, when ρ ¼ 0,
VaRA þ VaRB>
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
VaR2
A þ VaR2
B þ 2ρVaRAVaRB
q
ð10:8Þ
Empirical analysis, say by comparing market risk and credit risk time
series, would suggest that zero correlation is often a more realistic (and hence
preferred) assumption than perfect correlation.
Secondly, the popular wisdom that the simple summation rule is con-
servative (thanks to equation (10.8)) does not hold true in some situations,
especially if we are not clear what exactly we are adding. Breuer and col-
leagues (2008) showed that simply adding market risk and credit risk may
result in a less conservative aggregate in some cases.
A good example6 is foreign currency loans. Consider a foreign bank
lending U.S. dollars to a local borrower. The loan contains market risk (FX
risk) and credit risk (default risk of the borrower). The two risks are mea-
sured separately. In a scenario where the domestic economy slows and the
local currency depreciates, from the bank’s perspective, the loan appreciates
in value and the credit risk of the borrower increases—the two effects should
somewhat offset at first sight. But this diversification argument neglects the
192
VALUE AT RISK METHODOLOGY

fact that the ability of local borrowers to repay depends in a nonlinear way
on currency fluctuations. Unless the borrower has other revenues in U.S.
dollars, it may be pressed to default on the loan if the currency loss is
unbearable. This is what happened to many local borrowers during the
Asian financial crisis in 1997. Such trades where credit risk and market risk
amplify each other are known as wrong-way trades.
More generally, under Basel, the capital charge for credit and market
risk is required to be modeled separately and then simply summed up.
However, most products contain both risks, which cannot be decoupled
easily because of the complex interaction between credit and market risk. It
is unclear whether the diversification benefits are positive or negative. As in
the example of a foreign currency loan, its risk may be higher than the simple
summation of credit and market risks that exist separately.
Finally, if we add the risk up without a unifying conceptual framework,
the aggregated VaR will have little meaning. What does it really mean to add
a 10-day 99% market VaR to a one-year 99.9% OpVaR? Also a 99.9%
confidence level is incomprehensible and not empirically verifiable. It is
impossible to make intelligent decisions if one cannot interpret what the risk
numbers really mean.
The problem of aggregating these diverse risk classes is a difficult one;
they are very different in almost all aspects. Table 10.6 summarizes their
characteristic differences.
In the list, only market risk has enough quality data to be analyzed
precisely using a statistical distribution. Even here, problems in the tail can
undermine accuracy. Credit and operational risks are known to be fat-tailed
and very likely extremistan in nature. On the other hand, liquidity risk will
be handled using coverage ratios (see Section 11.4), an appropriate choice of
solution and not really a model. It is a good example, which shows that we
need not model risk precisely to protect against it. Can this be aggregated?
Adding across Different Forecast Horizons
The unavoidable part about forecasting probabilities of future events is that
one needs to specify a horizon. For example, Basel’s market VaR is a forecast
for 10 days ahead. If different risks are estimated at different horizons,
simply adding them up can lead to undesirable side effects.
As an illustration, suppose there is a casino that offers a game that has
a 60% chance of winning for the customer. The casino has a bad fire safety
record—it was burned down twice in the last four years. So the hazard rate
measured at the 1-year forecast horizon is 0.5 fires per year. Would you
play the game (assuming you can bet on-line so there is no personal safety
Other Major Risk Classes
193

concern)? Are the odds of winning simply 30% (¼ 0.6  0.5) or 60% (¼ 0.6
(10.5/365))? The situation is not unlike that of a credit-risky bond where
there is a stochastic price risk and a default risk element. There are two
lessons here: (1) In order to have a meaningful interpretation, the risks need
to be scaled to the same “unit” before adding, and (2) the risk changes with
forecast horizon (i.e., frequency of visits to the casino). If you visit just
once, the odds are 60%, but if you gamble the whole year round, the odds
are 30%.
Scaling the risk from one horizon to another is not straightforward.
Firstly, volatility scales with the square-root of time, whereas, default
probability scales linearly with time. This implies that the choice of mea-
surement horizon will have an impact on the relative importance of these
two risks. Secondly, the popular scaling method is laden with naïve
assumptions that are often taken for granted. For example, the square-root
of time scaling that is commonly used to bring a one-day VaR to 10-day
assumes that returns are independent and identically distributed (i.i.d.),
normally distributed (see Section 6.4), and that positions remain unchanged
TABLE 10.6
Summary of Characteristics of Major Risk Classes
Risk class
Methodology
Risk Factor
Mapping
Data General
Availability
Result Reliability
Market risk
(incl. speciﬁc
risk)
Quantile based
99%/10 day
Well deﬁned
Good (vendor/
price based
data)
Good (within
deﬁnition of
quantile)
Credit risk
Quantile based
99.9%/1
year
Chosen
grades/
grids
Subjective
(internal/
scoring
based)
Poor (wide error)
Operational
risk
Quantile based
99.9%/1
year
Loosely
deﬁned
taxonomy
Poor (ill-
deﬁned)
Poor (wide error)
Liquidity risk
Liquidity
coverage
ratio
N.A.
Excellent
(internal/
positional
based)
Deterministic
but risk
misspeciﬁcation
of scenarios
Counterparty
credit risk
CVA VaR,
quantile
based 99%/
10 day
Well deﬁned
Poor
Moderate. Highly
model
dependent
194
VALUE AT RISK METHODOLOGY

in the interim 10 days. These assumptions do not reflect the real trading
environment.
The general problem of aggregation can be addressed using the fol-
lowing logic: first scale all risk numbers to the same “units” (i.e., same
quantile and same horizon) before adding them up. Are there any diversi-
fication effects and complex relationships that need to be accounted for in
the summation? If so, is it material? Scaling often requires unrealistic
assumptions. Is the error due to these assumptions material at a portfolio
level? If the error is unacceptable, to avoid making such assumptions one
really has to model the various risk classes within a coherent framework. For
a realistic example of a model used by a bank, see the paper by Brockmann
and Kalkbrener (2010).
10.5 SPREADSHEET EXERCISES
10.1 CreditMetrics is a popular credit model used by banks to capture
credit default and rating migration risks on a portfolio basis. Aim:
illustrates a simple implementation of CreditMetrics for a portfolio of
three bonds. Note: All figures are hypothetical, the valuation of the
forward values of each bond at each rating is not shown. The illus-
tration focuses on the Monte Carlo simulation of rating migration and
the resulting loss distribution.
10.2 Operational risks VaR (OpVaR) is a commonly accepted method to
measure operational risks at banks if a bank chooses an actuarial
approach. While theoretically appealing, this method is fraught with
data scarcity and classification challenges. Aim: derives the operational
risk loss distribution using tabulation method given the data on fre-
quency distribution and severity distribution. Action: Run the VB
macro to generate all permutations of number of events n ¼ 0, 1, 2, 3.
The loss distribution of the permutations is computed; OpVaR is just
its loss quantile.
NOTES
1. See Section 11.1 for the difference between banking book and trading book.
2. The spread curve for a particular issuer is a market observable. For simplicity, we
assume a static curve; that is, Si observed at the one-year horizon gives the same
values as Si observed today.
Other Major Risk Classes
195

3. In practice, the observed curve is the swap curve, which is then bootstrapped to
obtain the zero coupon curve or discount curve.
4. The recovery rates depend on bond rating and seniority. Normally they are
internally estimated by the bank. Typical values range from 20% to 40% of par.
5. For example, in swaps, when credit riskiness is deemed absent (due to a netting
agreement between professional counterparties), one cash ﬂow is no different
from another if they fall on the same date.
6. The example is taken from BIS Working Paper No. 16 “Findings on the Inter-
action of Market and Credit Risk” (May 2009), which discusses the latest
progress in unifying market and credit risks.
196
VALUE AT RISK METHODOLOGY

PART
Three
The Great Regulatory
Reform


CHAPTER 11
Regulatory Capital Reform
T
he Basel Committee of Banking Supervision (dubbed “Basel”) is instru-
mental in the design and development of the current framework for
banking regulation and capital requirements. Though not legally binding,
these rules have become the de facto standard adopted globally.
This chapter begins with a short background of the Basel framework and
guidance.1 We then explore the recent policy development in response to the
credit crisis of 2008. Most of these policy papers came in a scramble after
2009; the new rules were eventually grouped under the Basel III framework
and were scheduled for implementation in 2012 and beyond. The policy
response is very comprehensive, covering many aspects of banking regula-
tions, even though it is fair to say that the industry is still looking for all the
right tools to do the job. We focus our discussion on the capital adequacy
aspect of risk regulation and the calculation of minimum capital, in line with
the purpose of this book.
11.1 BASEL I AND BASEL II
The Basel committee is comprised of the central bank governors of the group
of 10-most-industrialized nations (or G-10). The role of Basel is to promote
the safety of the global financial system and to create a level playing field
among international banks. Basel is a high-level policy body. Local regulators
such as central banks and banking supervisors use the Basel framework as a
guide to write rulebooks for national banks under their supervision.
A core element of supervision is the idea of capital adequacy—a bank
has to hold sufficient capital (in proportion to the risk taken) for the business
it is engaged in. For example, for every $100 engagement in risky activities,
the bank is required by regulation to maintain $8 (say 8%) of capital. This
minimum (or regulatory) capital acts as a safety buffer for a bank during
times of crisis to prevent bankruptcy, and to give it time to source emergency
199

funding. A larger minimum capital is more conservative but will also take a
toll on a bank’s profitability (return on capital) since the spare capital is not
put to productive use. In the EU, this element of Basel is written into the
Capital Adequacy Directive (CAD) which stipulates various rules and
methods to derive the minimum capital.
The Basel Accord (1988) or Basel I was a pioneer capital framework. It
focused only on credit risk (the largest risk class faced by banks) and was a first
attempt at designing a risk-based capital charge; that is, penalizing banks more
for holding riskier assets. However, it is a simple rules-based approach2—
risky positions (or claims) held by a bank are simply categorized according to
Table 11.1, which also gives their risk weights. Multiplying a position by its
risk weight will give its risk-weighted asset (RWA). The minimum capital is
then just 8% of RWA summed across all positions.
In 1996, the committee amended Basel I to include market risks. The
total capital charge is now given by:
MinimumCapital ¼ 8%½CRC þ ð1=8%ÞMRC
ð11:1Þ
where credit risk charge (CRC) is the RWA for loans. The market risk charge
(MRC) is computed using either the Standard Approach (another rules-
based method) or Internal Models (VaR-based). The total RWA, by its
definition, is given by the terms in the square-bracket in equation (11.1).
The introduction of internal models is a milestone development in Basel
and is a tacit recognition by regulators that banks have developed
TABLE 11.1
Risk Weights for Claims under Basel I Regime
Weights
Asset Type
0%
Cash
Claims on OECD governments
Claims on governments in local currency
20%
Cash to be received (in collection)
Claims on OECD banks and regulated securities ﬁrms
Claims on non-OECD banks below one year
Claims on multilateral development banks
Claims on foreign OECD public-sector entities
50%
Residential mortgage loans
100%
Claims on the private sector (loans, equity, etc.)
Claims on non-OECD banks above one year
Real estate
Plant and equipment
200
THE GREAT REGULATORY REFORM

sophisticated models to measure risks and can be counted on to advance on
this frontier. To qualify for internal models, a bank has to show that it has a
robust risk management infrastructure, a culture of risk awareness, and
reliable models. Since the use of internal models can lead to significant
capital savings (thanks to diversification and offsets), needless to say, banks
are well-motivated in this direction of research.
The rules-based regime of Basel I had some serious loopholes. It gave
preferential treatment to OECD claims, and there was no differentiation
between corporate loans of different ratings (see Table 11.1). This led to
widespread regulatory arbitrage where banks attempt to obtain a better
return-on-capital by divesting high-quality (hence low-yielding) assets in
favor of low-quality (high yielding) assets if they attracted the same capital
charge. It motivated U.S. banks for example to securitize high-quality loans
on their books and on-sell them to investors, and also to lend heavily to
Asian corporations in the run-up to the Asian crisis (1997). The dollar loans
were used for unproductive speculation in local assets, which were giving
high yields at that time because of the Asian currency pegs to the U.S. dollar.
Quite unintentionally Basel I encouraged undesirable behavior by banks and
led to a deterioration of banks’ credit quality.
The Basel II was published in 2004 to overcome some of these weak-
nesses. Basel II is based on three “pillars” of supervision: (I) minimum capital
requirements; (II) supervisory review process, which gives greater powers to
the regulator; and (III) market discipline, which outlines the disclosure
requirements for banks. Operational risk charge was introduced following the
high-profile scandals that bankrupted Enron in 2001 and Worldcom in 2002.
Risk models used in banks can serve two separate objectives—models
used for regulatory capital (i.e., the minimum capital required under Pillar I)
and models used for economic capital (i.e., the capital set aside internally by
banks to cover the risk of doing a certain business; it is a Pillar II concept).
Pillar I models are prescribed by regulators and tend to be more conserva-
tive, while banks are free to model economic capital. If perfect models exist,
the two capitals would converge.
As shown in Table 11.2, a menu of methods is available under Basel II
to compute various capital charges. For credit risk (loans, claims) a bank
TABLE 11.2
Methods Approved for Calculation of Capital Charges under Basel II
Credit Risk
Market Risk
Operation Risk
Standardized
Standardized
Basic indicator
Foundational IRB
Internal Models (VaR)
Standardized
Advanced IRB
Advanced measurement
Regulatory Capital Reform
201

can choose the rules-based standardized method (legacy of Basel I) or the
internal-ratings-based (IRB) approach. Generally speaking, under IRB,
the credit risk charge for each category of asset is given by:
CRC ¼ LGD × ½KV  PD × M × EAD
ð11:2Þ
wherePDistheprobabilityofdefault(oftheloan)at99.9%confidencelevelata
one-year horizon, LGD is the loss given default, EAD is the exposure at default,
and M is the loan’s maturity adjustment factor. KV is the Vasicek formula. PD
and LGD are in percentages whereas EAD is in currency units. In the foun-
dational IRB, only PD is estimated by banks, and in the advanced IRB, all
inputs are estimated using banks’ own models. The IRB formula is a simplified
portfolio credit risk model—a simple function of PD, LGD, and EAD—derived
based on the assumptions of a single risk factor and normal distribution.3
For market risk, the standardized approach is rules-based and expensive
on capital charges. Given the advances in value-at-risk (VaR) modeling
nowadays, most banks have upgraded to use internal models where the
MRC is described by equation (6.3).
The MRC is divided into two components: the general market risk
charge (GMRC) and specific risk charge (SRC), where the latter captures the
idiosyncratic risk of tradable instruments, such as the issuer default risk of
corporate bonds. Note that there is no overlap between the CRC of a loan
and a SRC of a bond of the same corporate name because CRC is a banking
book treatment whereas MRC (i.e., GMRC plus SRC) is solely for the
trading book.4 Basel has decided to exclude the banking book from (general)
interest rate charge since it contains mainly loans held to maturity. Market
fluctuations are not important since the loans will not be traded. The
treatment for equities and options is beyond the scope of this book.
For operational risks, the basic indicator and standardized approaches
are rules-based. The advanced measurement approach is in fact a nascent
internal model quite similar to VaR. To calculate the operational risk charge
(ORC) the model takes the quantile based on 99.9% confidence level and
over a one-year horizon (see Chapter 10). The total capital charge for a bank
is then given by:
MinimumCapital ¼ 8%½CRC þ ð1=8%ÞMRC þ ð1=8%ÞORC
ð11:3Þ
11.2 THE TURNER REVIEW
The Turner Review (2009) published by the Financial Service Authority
(FSA) is a comprehensive (126-page) diagnosis of the 2008 credit crisis,
202
THE GREAT REGULATORY REFORM

which contained a list of prescriptive policy actions points. This is con-
ceivably the blueprint that eventually evolved into the Basel III. Here we will
digest two sections of the review pertinent to this book: on fundamental
theoretical issues and on avoiding procyclicality in Basel II.
Fundamental Theoretical Issues
The review questioned the intellectual assumption of efficient market theory
on which previous regulatory approaches have largely been built. This
indirectly shaped the supervisory and risk management mind-set and prac-
tices of today. Five points were mentioned:
1. We can use market prices as good indicators of rationally-evaluated
economic value, for example, as unquestionable valid inputs to a risk
model. Evidence from the crisis: The markets can be irrational, illiquid,
and inefficient under stress.
2. Development of credit securitization will improve financial stability by
reallocating risks and by allowing for hedging. Evidence from crisis:
Credit securitization concentrated risk in a systemically harmful way—
banks were all on the same side of the trades.
3. Market discipline can be used as an effective tool in constraining
harmful risk taking. The market self-regulates. Evidence from crisis: As
a result of procyclicality and positive feedback, it seems prices and
market pressures have played vicious roles in exacerbating the crisis.
4. Financial innovation is a good thing—it adds value to the real economy.
Any nonbeneficial product would be snuffed out by competition. Evi-
dence from crisis: The credit product innovation that contributed to the
crisis had very little economic value, except for the illusionary effect
of wealth creation due to mark-to-market in a rising market. Also
investors were overcharged in terms of high margins—made possible
by pricing opacity and asymmetric information between banks and
customers—what the review called the harmful effect of rent extraction.
5. The risk characteristics of financial markets can be inferred from
mathematical analysis, delivering robust quantitative risk measures.
Evidence from crisis: The VaR method broke down. Four inherent
limitations were highlighted by the review (listed below).
First, a short observation period (i.e., 12 months) introduced dangerous
procyclicality. Second, the effective assumption of a normal distribution
meant that VaR models systematically underestimated the chance of fat-tail
events and Black Swan losses. Third, risk models implicitly assume that
individual participants’ reactions to price movements are sufficiently small
Regulatory Capital Reform
203

and are mutually independent such that they do not affect market equilib-
rium. In other words, risk models do not account for network externalities
or contagion effects (see Chapter 12). This could explain why VaR was at its
long-term low in spring 2007 just before the credit crisis when in fact sys-
temic risk was brewing—it wasn’t designed to detect network risk.
Finally, the assumption that information contained in past distributions
can be used to infer future distributions rests on insecure grounds, but has
gained widespread acceptance in financial modeling. Variants of this idea are
used in many areas including derivatives pricing (the Markov property) and
risk modeling (i.i.d.). But economic regime shifts can and do happen that will
invalidate the usefulness of past data. Credit risk is a good example—past
observations are of no use at all to predict future defaults since a company
can only default once; that is, past observations that a company did not
default does not tell us when or if the company will default in future. This
point is thought provoking.
This is true only because present-day risk models are not accustomed to
interpreting past data in unconventional ways. Default of Company A could
well contain useful information on the near-term future defaultability of
Company B from the same sector. Compression of credit spreads in the
spring of 2007 could signal (in a contrarian sense) that a blow-up in spreads
was imminent. Some irregular escalation in credit spreads of Company A
could well be the harbinger of a default of Company A. In other words,
today’s risk models are rigidly formulated to predict based on the logic that:
A-A, or today’s distribution is independent and identically distributed
(i.i.d.) or “more of the same occurrence in future.” But from the examples I have
given, other applications seem promising: A-B, A-non(A) (i.e., contrarian
logic), spread(A)-default(A). We really have to get past the i.i.d. paradigm!
More profoundly, this mental impasse has to do with the mathematical
quest for invariance. Mathematicians need to find a variable that remains
i.i.d. and stationary as a starting point for time series forecasting, and so
these became underlying requirements of most VaR models. The idea is that
once an unchanging element is found, we can then project this into the future
(after all, the element does not change). It is a primitive logic. For more
details, see Section 2.2.
Interestingly, the Turner Review also mentioned we should distinguish
between risk and uncertainty, and touched on the idea of Knightian
uncertainty:
The term “risk” as loosely used in everyday speech and in economic
discussions really covers two things which, functionally at least, . . .
are categorically different. The essential fact is that “risk” means in
some cases a quantity susceptible of measurement, while at other
204
THE GREAT REGULATORY REFORM

times it is distinctly not of this character; and there are far reaching
and crucial differences in the bearings of the phenomenon depending
on which of the two is really present and operating.
—Frank Knight, Risk, Uncertainty and Profit (1921)
Knight insisted that we really should name the unquantifiable risk
uncertainty. Compare the following problem statements: A dice throw is
a repeatable experiment. What is the probability of getting three sixes in a
row? This is quantifiable risk. But, what is the probability the U.S. president
will win the next reelection? The problem cannot be cast as a repeatable
experiment. It is at best a function of expert judgment (opinion poll perhaps)
and is subjected to occasional update as new information is gathered (sub-
sequent opinion polls). This is uncertainty.
In a Bayesian sense, uncertainty measures our state of ignorance or
incomplete knowledge. The less informed we are, the more uncertainty we face.
At times, we face similar situations in finance where no mathematical
exactness is possible, even though we may choose to use percentage prob-
abilities as a language device. And herein lies the danger—we may
unknowingly outsource our good judgment and common sense to mathe-
matical models if we confuse uncertainty with risk. To make things worse,
the line between risk and uncertainty can be changing. During the 2008
crisis, the normal market mechanism was disrupted, and price discovery
became unreliable,
especially for toxic derivatives—risk turned into
(Knightian) uncertainty.
Avoiding Procyclicality in Basel II
The herd actions of rating agencies to upgrade (or downgrade) firms during
a business cycle upswing (or downswing) can lead to harmful procyclicality
in credit risk scores. Likewise, the short observation period used in VaR also
leads to a safety capital, which is procyclical. These factors ultimately
encourage reckless investment behavior by borrowers and market players.
The review prescribed three ways to dampen such hard-wired procyclicality:
1. The use of variable scalars targeting specifically the IRB (banking book)
framework: Banks will be tasked to model the credit cycle on a long-
term basis or through-the-cycle instead of on a short-term basis or
point-in-time. The assessment is done for each credit grade and ulti-
mately expressed as a variable scalar. This factor is applied to the PD (in
equation (11.2)) as a correction so that the effect of procyclicality is
dampened.
Regulatory Capital Reform
205

2. Introducing an overt countercyclical capital buffer such that the
required capital would increase in good years when loan losses are
below long-run averages, creating capital buffers, which can be drawn
down during a financial crisis: The review favors a formula-driven
approach, which can be calibrated at the discretion of the supervisor.
In 2010, the Bank for International Settlements (BIS) released a working
paper on countercyclical capital buffer, paper no. 317, that was even-
tually adopted as part of Basel III.
3. Changing the current accounting system, which is known to cause
procyclicality: Today’s accounting philosophy is focused on communi-
cating facts to shareholders at the publication date (i.e., PL marked at
current prices) and should not anticipate probable future events. But this
makes the reported balance sheet hostage to irrational and cyclical price
swings (which were not seen before in that accounting year). The review
proposed the set-up of a nondistributable Economic Cycle Reserve in
published accounts, which would set aside profits in good years to
anticipate losses in the future.
While the general principles are set forth, the FSA is still working with
the U.K. banking industry to establish the detailed framework and necessary
tools. The proposals must be implemented in a consistent way and across all
banks globally to avoid the possibility of regulatory arbitrage.
11.3 REVISIONS TO BASEL II MARKET RISK FRAMEWORK
(BASEL 2.5)
July 2009 saw the release of the final version of the revision to the Basel II
market risk framework and the guidelines for computing the incremental
risk charge. We shall discuss three important new requirements stated in
these papers, now collectively known as Basel 2.5:
Incremental Risk Charge
We recall from equation (6.3) that MRC consists of GMRC (VaR-based)
and SRC (rules-based, does not differentiate issuers). Major banks are
already using internal models for GMRC and are in a transition phase to
extend the VaR model to capture (issuer) specific risk as well. However, the
explosive growth in credit derivatives and credit securitization in the mid-
2000s has prompted Basel to introduce an incremental charge. The incre-
mental risk charge (IRC) was incorporated into the trading book capital
regime in recognition that default risk of illiquid credit products is not well
206
THE GREAT REGULATORY REFORM

reflected in VaR. Note that there is no overlap between the specific risk
charge modeled as part of VaR (at 99%, 10-day) and the IRC. The former
captures risks from spread movement, thus often called credit spread VaR,
whereas the latter captures default risks, rating migration risks and default
correlation risks at a much longer horizon of one year.
Basel did not favor any particular model for IRC but provided high-level
guidelines summarized below:
1. Scope: A bank that has approval to internally model specific risks as
part of its VaR will also be subjected to an IRC. IRC covers all positions
that are internally modeled for specific risks with the exception of
securitization products.
2. IRC must be based on losses at 99.9% confidence level, one-year
horizon, and should have a soundness standard comparable to IRB.
This is to avoid possible regulatory arbitrage between the trading and
banking books.
3. Assumes a constant level of risk (as measured by VaR or other metric)
over a one-year horizon. This reflects the fact that banks can seldom
respond efficiently in the short term when in distress, such as by selling
off all positions or by raising emergency capital. Also, consistent with a
going concern view of a bank, a one-year horizon is appropriate because
a bank must continue to take risks to support its income-producing
activities despite losses.
4. Must have a liquidity horizon of at least three months. Banks must
explicitly model the liquidity horizon of different credit products. In
principle, this horizon represents the fastest time taken to sell-off/hedge
a position in a stressed market without tipping over prices.
5. Cross-correlation effects must be modeled such as the correlations
between default, migration, and other risk factors.
6. Concentration risk of issuers and sectors must be properly reflected.
7. Risk-netting and diversification rules: These rules are very strict and
would disallow netting for most long-short positions in a typical trading
book. Instead, the longs and shorts will need to be summed on a gross
basis. Diversification between gross long and gross short positions will
need to be modeled separately.
8. The IRC must reflect optionality.
The IRC is a turning point that may open up an exciting but arduous
path of development. The current VaR assumes a constant level of position
in the forecast period (10 days). So banks do not care what happens to
positions in the interim and can use a simple square-root-of-time scaling. But
now, rules (3) and (4) together require banks to model the dynamic
Regulatory Capital Reform
207

rebalancing of positions (simplistically speaking, rolling over positions) to
maintain a constant level of risk throughout a one-year period. In theory, the
liquidity horizon determines the rebalancing interval (and frequency) for
each credit product. In effect, the new legislation forces banks to think about
what happens to positions in the interim period between today and the
measurement horizon. But in keeping a constant level of risk (VaR), Basel is
hinting that it wants to model effects other than market risks that will also
influence the risk charge. This could be sunken costs that arise from exiting
illiquid products, a term structure of liquidity/default risks, PL accruals from
premiums of credit derivatives, and so on. It is a leap in sophistication and
possibly the first serious attempt to blend asset liquidity risk into the credit
VaR framework.
Since the IRB is based on a one-factor Gaussian copula model, guideline
(2) inadvertently encouraged banks to design the IRC along the lines of a
multistep one-factor Gaussian copula model. Indeed, we see evidence of this
in the industry today—see, for example, Wilkens, Brunac, and Chorniy
(2012). Another good paper is by Martin and colleagues (2011).
The current consensus is to use a multistep Monte Carlo simulation on
each product to model the dynamic rebalancing. Suppose positions A, B, and
C have liquidity horizons 3, 6, and 12 months, then the number of steps for
the simulations of default risk are 4, 2, and 1 steps, respectively. For a simple
model, the systematic factor (in the one-factor model) can be assumed to
evolve over time using an AR(1) process.
Banking Book Treatment of Securitized Credit Products
Note that the IRC is meant for unsecuritized credit products. The Basel is
unconvinced that current state-of-the-art models adequately capture the
risks of securitized credit products such as tranched securities and correla-
tion products (nth-to-default swaps) given that many of them turned into
toxic assets during the credit default obligation (CDO) debacle in 2008.
Hence, for securitized products, the more punitive capital charges of the
banking book (IRB approach) will apply with a limited exception for cor-
relation trading activity, where banks may be approved by supervisors to
calculate a comprehensive risk capital charge. This exception is recognition
by Basel that such businesses will not be viable under the IRB regime where
there are effectively no offsets between longs and shorts. Closing down such
business activities is also not possible due to the trillions of dollars of out-
standing notional that is already issued.
The comprehensive risk charge will be modeled internally by banks and
must include all risks pertinent to correlation trading. This will be a chal-
lenging task as it includes a host of complex risks.5 The comprehensive
208
THE GREAT REGULATORY REFORM

charge will be floored by a percentage of the comparative charge as calcu-
lated using the banking book approach.
Stressed VaR
The Basel II revision also requires banks to calculate a stressed VaR, which is
just the conventional 99%/10-day VaR calculated using a one-year (fixed)
observation period of high stress. The 12-month period ending December
2008 qualifies for this purpose. Unlike normal VaR, the stressed VaR is a
static risk measure that does not depend on the arrival of market movements
although it varies with positional changes, and as such is really a stress test
result. The idea is to use this stressed VaR (SVaR) as a first buffer against
procyclicality.
In summary, if a bank’s specific risk (SRC) is not approved for internal
models, the market risk charge for the trading book, equation (6.3) now
becomes:
MRCt ¼ max
k
60
X
60
i¼1
VaRti, VaRt1
 
!
þ max
m
60
X
60
i¼1
SVaRti, SVaRt1
 
!
þ SRCt
ð11:4Þ
where the multipliers k and m are set by regulators subject to k ≥3, m ≥3,
and k is determined by back-testing of normal (nonstressed) VaR. On the
other hand, if a bank’s internal model is able to include specific risk, then the
market risk charge is given by:
MRCt ¼ max
k
60
X
60
i¼1
VaR*
ti, VaR*
t1
 
!
þ max
m
60
X
60
i¼1
SVaR*
ti, SVaR*
t1
 
!
þ IRCt
ð11:5Þ
where the superscript * denotes that specific risk is included. The BIS esti-
mated6 that the new rules will increase the market risk charge by an average
of 224%. This figure excludes capital charges for securitization exposures
and the comprehensive risk capital charges; thus the actual increase will
likely be higher.
Regulatory Capital Reform
209

Counterparty Credit Risk: CVA VaR
Credit valuation adjustment (CVA) is the fair price of counterparty risk for a
deal, expressed as a chargeable fee adjustment when pricing derivative deals
for trades done without posting collateral. One can think of it (loosely) as the
premium paid on a CDS that protects against the counterparty default,
whereby the CDS notional is the derivative exposure to that counterparty.
Basel uses a simplified CVA formula:
CVA ¼ LGD 
X
T
i¼1
Max 0; exp  si1:ti1
LGD


 exp  si:ti
LGD


n
o
: EEi1:Di1 þ EEi:Di
2
	

ð11:6Þ
It is calculated on a stand-alone basis for each counterparty with no
offsets allowed among different counterparties. Time is bucketed from today
(t0) to the longest contractual maturity (tT) along the exposure netting sets
with the counterparty. si is the CDS spread at tenor ti in which the coun-
terparty is the obligor. The LGD is the loss given default (or one minus
the recovery rate) estimated for the counterparty. The term exp(st/LGD)
is the survival probability at time t, and it usually decays with time; thus, the
marginal survival rate (difference between adjacent survival probabilities
in the equation) is usually positive. However, should the CDS spread curve
become inverted, the marginal survival rate will become negative and
will reduce the CVA. To be conservative, this is floored at zero by the Max(.)
function.
EEi refers to the expected exposure at time ti (think of it as future mark-
to-market). A Monte Carlo engine reprices deals done with a counterparty
with netting allowed among deals. This generates many future paths of net
market values, but only positive values (gains) will give rise to counterparty
risk (exposures). Hence, the negative values are floored at zero. To calculate
EEi we take the average of the exposures (across many paths) at a given time
bucket. This is discounted to today’s money using discount factor Di as per
the second parenthesis. The product of the two parentheses is summed
across all time buckets up to T.
It was found that two-thirds of counterparty risk losses during the credit
crisis came from CVA repricing and only one-third came from actual
defaults. Hence, Basel III has called for a counterparty risk charge to be
computed using CVA.
This is a two-stage calculation. First, the EEi’s are simulated across time
buckets. These are then used to compute a CVA sensitivity to unit change in
210
THE GREAT REGULATORY REFORM

spread s. Second, different scenarios for spread s are simulated. Combining
these two elements produces a distribution of CVA. The 1% quantile of this
distribution at the 10-day horizon is the CVA VaR.
The CVA VaR method is a simplification of a complex and developing
subject. Certain well-known effects are absent from the model, for example,
wrong-way risk, bilateral CVA, and default risk of the counterparty. Wrong-
way risk occurs in situations where the EEi itself is positively correlated to
the spread s. The two are assumed to be independent in the simple model.
The bilateral CVA is a net fee that also includes the CVA that the coun-
terparty charges the bank in a deal. The CVA VaR is driven by changes in
spread of the counterparty (i.e., the volatility of s) and excludes the default
risk of the counterparty. CVA capital rules will likely evolve in the coming
years to account for such risks as the subject matures.
11.4 NEW LIQUIDITY FRAMEWORK
Liquidity stress testing has long been a traditional risk management practice
at banks. Even so, the credit crisis revealed how unprepared banks were for a
sudden dry-up in funding liquidity—clearly, liquidity stress testing has been
an underdeveloped discipline. In 2008, the BIS published a set of (rather
high-level) principles to guide banks in funding liquidity risk management.
To fortify this critical area, the BIS introduced two new minimum require-
ments in December 2009—these two liquidity ratios are now included in
Basel III:
Liquidity coverage ratio ¼ ðstock of high quality liquid assetsÞ=
ðnet cash outflows over a 30-day periodÞ, with a minimum of 100%
and,
Net stable funding ðNSFÞ ratio ¼ ðavailable amount of stable fundingÞ=
ðrequired amount of stable fundingÞ, with a minimum of 100%
where the terms are briefly described in Table 11.3. More details can be
found in the BIS document “Basel III: International Framework for Liquidity
Risk Measurement, Standards and Monitoring” (December 2010).
The liquidity coverage ratio is designed to ensure that a bank has enough
liquidity to cover a 30-day mini bank run scenario as defined by Basel.7 The
scenario emphasizes the danger of over-reliance on wholesale funding (as
Regulatory Capital Reform
211

opposed to deposit funding), which can disappear quickly in a crisis. The
goal is to set up a liquidity buffer to defend the balance sheet if it suddenly
comes under stress.
On the other hand, the NSF ratio aims to promote more medium-to-
long-term funding of assets and activities of banks—it discourages over-
reliance on short-term wholesale funding. It dictates an overall funding
structure where long-term commitments must (largely) be financed using
long-term liabilities. This explicitly constrains the “maturity transforma-
tion” activities of a bank.
In Table 11.3 under the “Required Amount of Stable Funding”, the per-
centages represent assumed portions of categories of assets that could not be
sold off to raise money, or pledged for collateralized borrowing, during a
period of emergency or liquidity stress lasting one year. Hence, this portion of
the asset will need to be supported by readily available stable funding.
11.5 THE NEW BASEL III
Basel III was finally released in December 2010 in the BIS paper “Basel III: A
global regulatory framework for more resilient banks and banking systems.”
TABLE 11.3
Brief Description of Terms That Deﬁne the Liquidity Ratios
Term
Brief Description
Stock of high quality liquid
assets
Central bank–eligible collateral such as cash,
government bonds, and covered bonds that are
unencumbered (not pledged for other purposes).
Net cash outﬂows over a 30-
day time period
Represents a run-off of liquidity under a Basel
deﬁned stress scenario (mini bank run) where the
bank will need to survive for 30 days. The run-off
portions are given per category. Example: Loss of
7.5% of stable retail and small business deposits,
100% of interbank funding with maturity less
than 30 days, and so on.
Available amount of stable
funding
Funding (liability) with weighting scheme given.
Example: 100% of Tier 1, Tier 2 capital, preferred
stock, 90% of “stable” deposits from retail
customers and small businesses, 50% of large
deposits by nonbanks less than one year, and so on.
Required amount of stable
funding
Weighted sum of the asset side of the balance sheet.
Example: 5% of sovereign/supranational debt
more than one year, 50% of gold, 85% loans to
retail customers less than one year, and so on. All
items must be unencumbered.
212
THE GREAT REGULATORY REFORM

It details new regulatory requirements (primarily additional capital) over
and above what is already stipulated by Basel II and includes all the new
requirements of Basel 2.5. Here is a short summary:
First, Basel III calls for the redefinition of capital—more stringent cri-
teria for what qualifies as the capital base for a bank,8 and standardization
across jurisdictions. The minimum capital (as a percentage of RWA) has
been raised to 4.5% for common equity, and to 6% for Tier 1 capital, but
total capital remains at 8% of RWA. The increase will be phased in by
January 1, 2015.
Second, as described in Section 11.3, it proposes several initiatives to
enhance risk coverage—to cover the gaps seen during the 2008 crisis. Credit
securitization risks and procyclicality are addressed by the revisions to Basel
II. Counterparty risks arising from derivatives and SPVs are dealt with by a
new set of rules—counterparty credit risk to be computed using stressed
inputs, additional capital for credit valuation adjustment or CVA, incentives
to move OTC trades to centralized clearing to minimize systemic risk,
treatment of wrong-way trades, and so on.
Third, the new Basel will require minimum capital to cover funding
liquidity risk. These are the two liquidity ratios mentioned in Section 11.4.
Fourth, it introduces a leverage ratio9 (ratio of capital-to-total exposure,
initially set at 3%) to constrain the build-up of leverage in the banking
system, which effectively dampens the positive feedback loop so that harmful
systemic deleveraging is less likely during a downturn. The leverage ratio is a
good supplement to the risk-based (VaR) approach because it does not
depend on any models (no modeling risk) and measures very different
aspects of a bank’s strength. It is also more difficult for banks to arbitrage
around two capital methods at the same time.
Fifth, Basel III includes a capital conservation buffer over and above the
minimum capital to cushion against stressful events (fat-tail risk). Banks will
need to set aside another 2.5% of RWA in terms of common equity by
January 1, 2019. A bank can draw on this buffer, but as the buffer shrinks,
the bank will be restricted by law from paying dividends and bonuses.
Finally, there is an additional countercyclical capital buffer meant to
protect the banking system from the procyclicality of credit growth. The
buffer will fluctuate within the range of 0% to 2.5% depending on
the credit-to-GDP ratio of the country of the credit exposure. This targets the
credit growth cycle of the country where the creditor or counterparty is
located, regardless of the location of the entity granting the credit. The level
of the buffer and timing of release are decided by the host regulator of the
creditor, using credit-to-GDP as a common reference. More details can be
found in the BIS working paper No. 317, “Countercyclical Capital Buffers:
Exploring Options” (2010), and Basel III document “Guidance for National
Regulatory Capital Reform
213

Authorities Operating the Countercyclical Capital Buffer” (2010). There is a
genuine concern from the industry that such a method may be prone to
regulatory arbitrage and double counting with other capital buffers if not
implemented carefully.
11.6 THE NEW FRAMEWORK FOR THE TRADING BOOK
In May 2012, the BIS published the consultative paper, “Fundamental
review of the trading book,” which revealed the supervisor’s preferred
direction for the development of market risk models and the regulatory
framework. Some key proposals pertaining to models are:
1. The replacement of VaR with expected shortfall (ES): The advantages
are well-known—ES is a coherent measure and allows for the capture of
tail risk and gap risk.10 More importantly, ES is likely to be understood
correctly by management.11 See Section 7.6 to learn more about
expected shortfall.
2. The BIS emphasized the importance of using stressed calibration for
risk models for the purpose of capital. This recognizes that models that
use recent data, even if they truthfully represent the current market
state, are inadequate when a crisis occurs; after all, a crisis is by defi-
nition a sudden, unpredictable event. Hence, the IRC model, for
example, has parameters such as correlation calibrated to reflect
stressful periods. This enforces conservativeness into risk modeling—in
spirit, Basel III modeling is more like an engineering project than a
scientific enquiry.
3. A comprehensive incorporation of market liquidity risk: Firstly, the BIS
reaffirmed its preference for dealing with liquidity risk via the concept of
differing liquidity horizons just like in the IRC model. Secondly, it
suggests that this concept be extended to market risk VaR models as a
next development. Thirdly, banks should have a capital add-on for
(bespoke) instruments that may experience a jump in liquidity premia
during crises that is not easily modeled. In short, the consultative paper
appears to be encouraging banks to develop a comprehensive risk model
using a multistep liquidity horizon structure. This would eliminate the
much-criticized double counting and model all types of trading risk
under a coherent framework.12 However, the BIS stop short of
including CVA VaR into this scheme as the topic of CVA is still not well
established. The CVA risk charge will be implemented on a stand-alone
basis under Basel III.
214
THE GREAT REGULATORY REFORM

11.7 THE IDEAL CAPITAL REGIME
At the time of this writing, many aspects of Basel III are still evolving and
implementation of some of the requirements (notably CVA VaR) may be
postponed beyond January 2013. In closing this chapter, it helps to get an
ideological view of the previous capital regimes from the perspective of
responses to the economic cycle, in order to appreciate where the future
could lead. Figure 11.1 is a stylized representation of Basel I, Basel II, and the
ideal capital regime which, for lack of a better name, we shall call Basel III.
This is plotted against a hypothetical business cycle (the dotted line).
Basel I is an ultraconservative regime that imposes a capital surcharge, a
thick buffer against losses. In fact, it is not responsive to gyrations in the
economic cycle and can be seen as a crude stress test that only differentiates
positions by simple categories.
Basel II, geared for internal models, is a market-sensitive capital regime.
It responds to market volatility in a contemporaneous way, a behavior now
referred to as point-in-time. VaR is prone to procyclicality—it is hardwired
to encourage leveraging and deleveraging at just the wrong times because, in
a boom cycle, VaR understates risk and in a bust cycle, VaR overstates risks.
Regulatory Capital
Time line
BASEL I
BASEL II (VAR)
BASEL III
THICK BUFFER
THICK BUFFER
THICK BUFFER
UNDERSTATE
UNDERSTATE
UNDERSTATE
OVERSTATE
OVERSTATE
OVERSTATE
FIGURE 11.1
Business Cycle versus Capital Held
Regulatory Capital Reform
215

Basel III? While we do not yet have an ideal capital regime, we do know
how it should behave in theory, as Figure 11.1 illustrates. The risk charge is
sensitive to the economic cycle in a preemptive or countercyclical way. The
capital increases during a boom to restrain an overheated market and to
build a safety buffer that can be dipped into during a future crisis. In the bust
phase, for a fixed-dollar position, VaR will be higher because volatility has
gone up. This means that to pare back VaR (deleverage) a bank will have to
sell even more positions. This pressure can be alleviated if regulatory capital
is less demanding (understated) during the bust. The additional buffer that
was built up earlier can now be used to buffer potential fat-tail losses.
In short, the ideal capital regime will have three properties at least: (1) it
is risk-based;13 (2) it is countercyclical (thus preemptive); and (3) it is large
enough to buffer against most (even if not all) extreme events.
After the 2008 credit crisis, there is a natural push among regulators to
move towards a more penal regime. The Turner Review even called for a
trading book capital of about three times conventional levels, understand-
ably to protect against fat-tail events. But we have to be vigilant not to
sacrifice market sensitivity, lest we regress back to Basel I. Basel I is like a
socialist program—everybody gets the same charge (one-size-fits-all) during
good or bad economic times, whereas Basel II, notwithstanding procycli-
cality (which can and should be corrected) allows a bank with better risk
management sophistication to conduct business more efficiently using cap-
ital charges that are commensurate with the risk. It gives the right incentives
for progress.
Generally speaking, basing capital rules on traditional stress tests, or by
choosing a very long observation period (multiyear) for risk measurement,
has the effect of dulling the response to the market cycle. Stress testing is an
unsuitable metric for capital requirements (for trading book risk),14 since the
risk controller is constantly fighting yesterday’s battles. The next crisis will
surely come, but the scenario will likely be completely different from any one
selected from past history, or constructed by controllers. Stress tests are easy
to perform but difficult to act on. What do you do with this very big number,
especially when you know it could have easily been some other big number if
your colleague had designed the stress test? What if the real probability of
occurrence of the chosen scenario is one trillionth of a percent? Is the sce-
nario worth considering? Without considering probability, stress testing may
be an incomplete exercise—it will mean that financial firms are permanently
braced for crisis, an approach that would be neither practical nor econom-
ical. As a step to overcome this problem, Rebonato (2007, 2010)15 devel-
oped
a
Bayesian
approach
for
stress-testing
extreme
events,
which
incorporates the risk manager’s prior beliefs or expert judgment. This is a
relatively new topic.
216
THE GREAT REGULATORY REFORM

At the time of writing, the general sense in the industry is that the new
Basel III proposal is too fragmented in design and lacks a unified strategy.
This may lead to significant double counting of capital buffers in some cases.
In the trading book capital, for example, a credit derivative will have a
capital charge coming from VaR, SRC, IRC, and Stressed VaR. It is possible
that the total charge could be higher than that of a similar position held in
the banking book (which uses the IRB model), which would encourage
regulatory arbitrage by parking positions in the banking book. In certain
situations, it may even be possible that the total charge is higher than the full
notional loss of a product, which does not make sense.
The dilemma is mainly in the area of credit securitization where trades
are illiquid and may be arbitrarily classified as trading book or banking
book based upon trading intent. These artificial boundaries in the regulation
lead to similar assets having different capital requirements and modeling
approaches. This is discussed in the “Fundamental Review of the Trading
Book” (BIS 2012).
NOTES
1. The interested reader is referred to the Basel’s website: www.bis.org, which
publicly
discloses
all
frameworks,
revisions,
consultative
papers,
and
documents.
2. From here on, the term rules-based will generically mean that the risk weights
are read mechanically from tables provided in regulatory rulebooks and used in
capital charge formulas stated in these rulebooks as opposed to model-based
(internal model) approaches such as VaR, which attempt to model the under-
lying risk phenomena.
3. The model is attributed to Vasicek (1991). Thomas and Wang (2005) provides a
clear interpretation and good critique of this formula.
4. A trading book contains positions held with trading intent and for short-term
resale. This may come from proprietary trading, arbitrage, or market-making
activities. Mark-to-market accounting applies. The banking book consists of
other instruments, mainly loans that are held to maturity. Accrual accounting
applies (except for derivatives hedges). A banking book (unlike a trading book)
is usually net long.
5. Explicitly mentioned inclusions are default, migration, multiple-default, spread
risk, cross-gamma effects, volatility of implied correlation, correlation between
spread and implied correlation, basis risk, recovery rates, beneﬁt of diversiﬁ-
cation, and hedge slippage.
6. See BIS, “Analysis of the Trading Book Quantitative Impact Study” (October
2009).
7. Broadly speaking, this stress scenario is whereby the bank is simultaneously hit
by a 3-notch credit rating downgrade, partial loss of deposits, loss of wholesale
Regulatory Capital Reform
217

funding, increase in secured funding haircuts, and margin calls. This features the
shocks experienced during the recent credit crisis.
8. A short note on capital: A bank raises capital (equity and debt) to fund the asset
side of the balance sheet. Regulators require that at least 8% of RWA must be
funded by Tiers 1 and 2 capital, with the goal of inﬂuencing the capital (liability)
structure of a bank. This minimum funding from common equity ensures that
the balance sheet has more loss-absorbing capacity, in the sense that equity
holders will absorb the ﬁrst loss, whereas any losses to debt holders (including
depositors) will trigger a default. Tier 1 capital is meant to protect a bank’s
solvency, Tier 2 capital to protect depositors—together they form total (regu-
latory) capital. The lowest-quality Tier 3 capital instruments will be phased out.
Common equity and retained earnings, often called core Tier 1, are the highest
quality subset of Tier 1 capital. Common equity is money paid up originally to
purchase common shares of the bank, and not the value of those shares now
traded at the exchange. Basel II allowed for hybrid securities to qualify for Tier
1 capital. These are bonds issued with equity-like characteristics—perpetual (no
maturity), discretionary coupons (dividend-like). However, innovative hybrids
have callable and coupon step-up features that encourage the bank to call back
the bonds for cheaper reﬁnancing, for example, after 10 years. During the 2007
credit crisis, bond yields were above the coupon, which made it uneconomical
for reﬁnancing, but many banks still called back in order to manage investors’
expectations, thus depleting the banks’ capital. Inconsistent deﬁnition by dif-
ferent regulators of what qualiﬁes for Tier 1 compounded the matter. In Basel
III, innovative hybrids will be phased out of Tier 1 capital.
9. Leverage ratios are already used by U.S. banks, which have not fully adopted
the Basel II framework. Basel III looks to standardize its deﬁnition. The
numerator is Tier 1 capital. The denominator follows the deﬁnition of exposures
in ﬁnancial accounts. Netting between deposits and loans, and credit risk
hedging of on-balance sheet exposures are not recognized in computing
exposures.
10. This refers to sudden jump losses, which are beyond VaR, (externally) caused by
speciﬁc news or endogenously caused by the triggering of stop losses.
11. VaR was blamed during the global ﬁnancial crisis for giving management a false
sense of security. This has to do with the common misinterpretation that it is an
expected loss (at a stated probability), whereas it is in fact a minimum loss (at a
stated probability). Also, its statistical imprecision is often overlooked and not
reported.
12. Naturally banks will pour resources into such projects because the capital relief
can be large enough to inﬂuence corporate performance such as return on
capital. Hopefully this economic incentive will drive the development of
coherent and sensible models.
13. An ideal risk model will reﬂect the reality of risk diversiﬁcation (and risk
ampliﬁcation in the case of wrong-way trades) and will not suffer from double
counting of different risk classes.
218
THE GREAT REGULATORY REFORM

14. Notable exceptions are the use of stress tests in analyzing interest rate risk in the
banking book (IRRBB) and liquidity stress tests for asset-liability management.
Here the variables are accounting items and yield curves. Thus, the set of sen-
sible scenarios to consider is limited and tractable; it is thus possible to think of
plausible stress scenarios for all banks. In contrast, for the trading book, the
variables are market prices/rates across many asset classes—each can move up
or down by a large range without a clear bound. The problem is compounded
by differing trading positions of banks, which can be leveraged, nonlinear, long,
or short.
15. A book by R. Rebonato (2007), Plight of the Fortune Tellers: Why We Need to
Manage Financial Risk Differently: Some of these ideas are formalized in the
paper by Frankland and colleagues (2008).
Regulatory Capital Reform
219


CHAPTER 12
Systemic Risk Initiatives
T
he idea of reflexivity1 and its connection to market crashes and systemic
risks has been studied and even published since 1987. Ironically, two
decades and three crises have passed, and we still lack the tools to measure
and safeguard our financial system from such a danger. Until very recently,
most research was focused on phenomenology—explaining but not solving
the problem. This chapter describes some key milestones in our under-
standing of the causes of systemic risk.
12.1 SOROS’ REFLEXIVITY, ENDOGENOUS RISKS
The salient feature of the current ﬁnancial crisis is that it was not
caused by some external shock like OPEC raising the price of oil or a
particular country or ﬁnancial institution defaulting. The crisis was
generated by the ﬁnancial system itself. This fact that the defect
was inherent in the system contradicts the prevailing theory, which
holds that ﬁnancial markets tend toward equilibrium and that
deviations from the equilibrium either occur in a random manner
or are caused by some sudden external event to which markets
have difﬁculty adjusting. . . . I propose an alternative paradigm that
differs from the current one in two respects. First, ﬁnancial markets
do not reﬂect prevailing conditions accurately; they provide a picture
that is always biased or distorted in one way or another. Second, the
distorted views held by market participants and expressed in market
prices can, under certain
circumstances, affect
the so-called
fundamentals that market prices are supposed to reﬂect. This two-
way circular connection between market prices and the underlying
reality I call reﬂexivity.
—George Soros, Testimony during Senate Oversight Committee
hearing, on the role played by hedge funds in the 2008 crisis
221

Soros first described the idea of market reflexivity in his book The Alchemy
of Finance in 1987. His ideas ran against conventional economic theory that
markets tend toward equilibrium as suggested by efficient market theory.
Soros argued that the market is always biased in either direction (equilibrium
is rare), and markets can influence the event they are supposed to anticipate
in a circular, self-reinforcing way. This he termed reflexivity. However, this
early reflexivity idea was couched in philosophy and behavioral science, not
risk management.
Trading is decision making under uncertainty. Conventional risk
modeling sees trading as a “single-person game against nature” where price
uncertainty is driven by exogenous factors, and not impacted by the decision
maker. This is frequentist thinking—an impartial observer scientifically
taking measurements, inferring conclusions, and ultimately making deci-
sions. This paradigm originates from the tradition of general equilibrium
theory developed by Samuelson (1947), where macroeconomics is the
aggregate behavior of a model where market participants or agents are
rational optimizing individuals. Under this paradigm, statistical techniques
can be used and indeed have been applied with great success, especially
during normal market conditions.
However, during crises, conventional risk models break down. Market
dynamics behave more like a game against other gamers—the drivers can be
endogenous. Irrational herding behavior can occur without any external
shocks. Prices can spiral out of control during a crash purely due to sentiment
within the market. This could be caused by feedback loop effects between the
actions of participants and outcomes, and spillover effect among participants
within the system. Hence, after the 2008 crisis, there is a paradigm shift
towards viewing the market as a complex adaptive system.
In the late 1990s, researchers began to formulate these effects in terms of
game theory, Bayesian thinking, and network science. The network exter-
nalities (spillover) arise because each trader’s decision does not take into
account the collective interest but ultimately affects it. For example, in a
crash each participant hopes to beat the others to the exit. This risk of
contagion increases with positional uniformity and leverage. It argues for the
role of the regulator to intervene in free markets and to monitor for systemic
risks on behalf of the collective interest.
Morris and Shin (1999) pointed out a blind spot in conventional risk
management practice—value at risk (VaR) does not take into account the
feedback loop between actions and outcomes. When many dominant players
in the market follow a uniform strategy, the consequences of the feedback
loop can be disastrous. For example, the 1987 crash was caused by portfolio
insurance, a popular program trading strategy whereby institutions increase
leverage in a rising market and deleverage in a falling market.2 The selling
222
THE GREAT REGULATORY REFORM

frenzy fed on itself on Black Monday. From 1997 to 1998, the Asian crisis
was caused by concerted currency attacks by speculators, while local gov-
ernments defended their currency pegs. Here, key players were amassing
highly uniform positions.
The authors proved that when a collective strategy is dominant, the
usual bell-shaped return distribution can become distorted. Using the yen
carry unwind in October 1998 as an example of a switching strategy,3 the
return distribution was shown to be bimodal (has two peaks!). Hence, VaR
will completely underestimate the true risk. If you plot the empirical distri-
bution during that turbulent period, you will not see any evidence of
bimodality. This is because the bimodal behavior happens only during the
brief period of panic selling, too short to be captured by statistics. We can,
however, simulate this behavior (see Spreadsheet 12.1). Figure 12.1 illus-
trates the bimodal distribution of a switching strategy where speculators
have a stop-loss exit strategy slightly below the current level.
Adrian and Shin (2008) analyzed the role of feedback loops in exacer-
bating the 2008 crisis. The key idea is that leverage targeting by banks gives
rise to procyclicality in the system, which accelerates the boom-bust cycle.
Consider a stylized bank’s balance sheet presented in Figure 12.2. Assets and
liabilities are always balanced; the balancing variable is the (shareholder’s)
equity. Hence:
equity ¼ asset  debt
ð12:1Þ
0
50
100
150
200
250
3.5 
2.8 
2.1 
1.4 
0.7 
(0.0)
(0.7)
(1.4)
(2.1)
(2.8)
(3.5)
(4.2)
(4.9)
(5.6)
(6.3)
Switching strategy
Normal
FIGURE 12.1
Bimodal Distribution of a Switching Strategy
Systemic Risk Initiatives
223

and we define:
leverage ¼ asset
equity
ð12:2Þ
In the left panel, leverage is 5(¼ 100/20) times. Suppose the price of
(risky) securities goes up by $10, assets will increase to $110 by mark-to-
market accounting, and leverage will fall to 3.67(¼ 110/30). In other words,
asset growth and leverage growth should be inversely correlated if there is no
change in positions. Surprisingly, empirical evidence by the authors showed
that asset growth and leverage growth are highly positively correlated across
U.S. financial institutions as a cohort. This suggests that banks actively
manage their positions to maintain some target leverage.
Hence, leverage is procyclical, and that in turn can cause perverse asset
price behavior. Consider the right panel; in order to maintain its initial
leverage of 5 (suppose this is the target), the bank will borrow $40 more in
debt to buy more securities that will bring assets to $150. Contrary to
textbook norm,4 the demand curve for the asset becomes upward-sloping.
The greater demand for the asset will put upward pressure on prices, and the
price rally will lead to a stronger balance sheet, which necessitates even more
purchases of assets to maintain the leverage target. Demand generates
more demand in a self-reinforcing loop, and this can cause an asset price
boom as shown in Figure 12.3 (left panel).
Asset ($100)
Liability ($100)
Asset ($150)
Liability ($150)
Initial: Leverage = 5.0
Final: Leverage = 5.0
Securities ($90)
Cash ($10)
Equity ($20)
Debt ($80)
Securities
($90 + $10 + $40)
Equity ($30)
Debt ($80 + $40)
Cash ($10)
FIGURE 12.2
A Stylized Bank’s Balance Sheet Illustration
224
THE GREAT REGULATORY REFORM

In an economic downturn, the mechanism works in reverse (right panel).
Falling asset prices cause losses to the balance sheet, and leverage as defined
in equation (12.2) increases. This will prompt banks to deleverage to
maintain the target by selling off assets to pay off debt. The selling into a
falling market can cause a downward price spiral and a sudden increase in
supply (more people wanting to sell)—the supply curve becomes downward-
sloping. This is called the loss spiral.
Another negative feedback is the margin spiral. Regulators require a
minimum amount of capital to be set aside as a safety buffer (like a trading
margin) usually determined by VaR. This can be represented simplistically as
the cash portion on the asset side in Figure 12.2. The problem is that VaR is
low during a boom and high during a downturn (see Chapter 7). Therefore,
the same amount of cash margin can support a lot more risky securities
during a boom than during a downturn—encouraging leverage during a
boom and loss-cutting during a bust.
The Geneva Report (2009)5 identified key systemic risk drivers and
negative feedback loops that contributed to the 2008 crisis; policy recom-
mendations to address systemic risks were also discussed.
Contagion from participants during stressful market conditions is the
root cause of the phenomena studied in Chapter 7. Positive feedback inad-
vertently causes serial correlation of returns that breaks the independent and
identically distributed (i.i.d.) assumption and leads to volatility clustering.
Stochastic volatility fattens the tails of the distribution. At the portfolio level,
the distribution will become fatter than normal (leptokurtic) because the
Central Limit Theorem (CLT) (which hinges on i.i.d.) no longer works. VaR
becomes understated as the regime shifts to a more volatile state. There will
be no pre-warning, after all, the sample distribution still contains mostly old
data from a regular state. By this mechanism, the effectiveness of VaR
becomes severely compromised during a crisis.
Increase leverage
Decrease leverage
Stronger
Increase
Weaker
Increase
balance sheet
purchase
balance sheet
sales
Asset price boom
Asset price collapse
FIGURE 12.3
Asset Price Boom and Bust Caused by Procyclicality of Leverage
Systemic Risk Initiatives
225

12.2 CRASHMETRICS
CrashMetrics was developed by Hua and Wilmott in 1998. Its purpose is to
locate and estimate the worst-case scenario loss for a bank as a firm. The
idea is that, since crashes cannot be delta-hedged in a continuous manner,
especially with the presence of nonlinear derivatives, it makes sense to ask
what is the worst loss for a bank given its positions. If a bank knows that this
loss can threaten its survival, then it can static hedge6 this vulnerability. The
static hedging with derivatives can be optimized to make the worst-case loss
as small as possible. This is called Platinum Hedging (not covered here) and
is proprietary to CrashMetrics.
CrashMetrics for One Stock
Unlike VaR, CrashMetrics does not make any assumptions on return dis-
tributions. There are also no prior assumptions on the probability, size, and
timing of crashes. The only assumption made is that crashes will fall in
certain ranges (say from þ100% to 50% price change).
Let’s consider a portfolio of derivatives on a single underlying asset.
If the asset price changes by δS, the portfolio value Π will change by:
δΠ ¼ FðδSÞ
ð12:3Þ
where F(.) represents the sum of all pricing functions of the derivatives. So if
it is an option, F(.) is the Black-Scholes equation; if it is a bond, F(.) is the
bond pricing formula, and so on.
We need to find the worst-loss for the portfolio; that is, the minima for F
(δS). In practice, to perform this minimization we need to constrain the
domain of δS to within a finite range [δS, δSþ]. The risk controller just
needs to assume a large but realistic range. The minimization problem
then becomes:
min
δS<δS<δSþ FðδSÞ
ð12:4Þ
For complicated portfolios, it is possible to have local and global
minimas (to be explained later). We are interested in finding δS, which gives
the global minima.
If the portfolio contains a vanilla option, F(.) will be a continuous
function, and we can approximate δΠ using a Taylor expansion:
Delta-gamma approach : δΠ ¼ ΔδS þ 0:5ΓδS2 þ : : :
ð12:5Þ
226
THE GREAT REGULATORY REFORM

where Δ and Γ are the delta and gamma as defined in Section 4.1. The worst-
case loss can be found by making the first derivative δΠ/δS ¼0, which leads
to the result:
δΠworst ¼  Δ2
2Γ
ð12:6Þ
Although this example is unrealistic (most banks would trade in non-
vanilla options), it illustrates the procedure. Geometrically speaking, the
minimization procedure involves searching the whole solution profile of δΠ
to find the lowest trough7 with slope δΠ/δS ¼ 0. For two assets, the profile is
a surface; for N assets, the profile is an N-dimensional hypercube.
Extension to Multiasset/Multibenchmark Model
To extend the model to multiasset, we need to relate the extreme moves of
each asset to the extreme moves of one (or more) benchmark index. For
example, we can map all U.S. stocks to the S&P 500 index. The benchmark
mapping is necessary to reduce the dimension to a manageable size. If the
benchmark moves by X%, then the ith asset moves by kiX%, where ki is
the crash coefficient for asset i. The crash coefficient is estimated using the
beta approach (see equation (2.37)) except that we only use the 40 largest
(extreme tail) daily moves in the history for asset i.
Figure 12.4 shows a scatter plot for HK Electric’s stock price returns
versus its benchmark, the Hang Seng index, using the last 25 years of data.
The circle and the oval show two distinct return regimes—normal and
extreme. The slope of the (dotted) regression line for the full data set is the
beta, whereas the slope of the extreme data set (line with higher slope) gives
the crash coefficient, k ¼ 0.84. Because of its shape, the diagram is called the
Saturn ring effect. The crash coefficient is higher compared to the beta
because assets are expected to be highly correlated during a crash. Fur-
thermore, the crash coefficient is found to be more stable than the beta,
which makes this method more robust.
With the presence of exotic options, the delta-gamma approach will not
work,andabankwill needtodorepricingofeachdeal infindingtheworst-loss.
For a multiasset/single index model, the minimization problem then becomes:
min
δX<δX<δXþ
X
i
giðkiδXÞ
ð12:7Þ
where δX is the change in the index, constrained to a finite range [δX, δXþ].
The sum is across i assets, all benchmarked to the same index; gi(.) is the
pricing function for asset i.
Systemic Risk Initiatives
227

The idea can be extended to multirisk factor where each factor is mapped
to a benchmark. For example, we can have benchmarks for equities, rates,
volatility, foreign exchange, and commodities. A deal may be mapped to one
or more benchmarks. For example, an equity option can be mapped to an
equity index benchmark and a volatility benchmark (such as the VIX index).
Suppose there are i risk factors and j benchmarks (j < i), then the minimi-
zation problem becomes:
min
δX
j < δXj < δXþ
j
X
n
gnðkijδXjÞ
ð12:8Þ
where the crash coefficient kij is the sensitivity of risk factor-i to an extreme
move in benchmark-j. We need to solve for the set {δX1, δX2, . . . , δXj},
where each element (a benchmark) is constrained by a user-defined range,
such that the overall portfolio value is the lowest. Note, the summation is
across n number of deals, where the pricing function of a deal gn(.) can be a
function of multiple risk factors generally.
Simple Illustration
It is possible to program an optimization algorithm to locate the global
minimum in an N-dimensional hypercube efficiently and to compute the
–0.4
–0.3
–0.2
–0.1
0
0.1
0.2
0.3
–0.4
–0.2
0
0.2
HK Electric
Hang Seng index
HK Electric
Extreme
Linear (HK
Electric)
Linear (Extreme)
Crash coefficient
= 0.84 (slope)
FIGURE 12.4
Saturn Ring Effect Showing Different Return Regimes
228
THE GREAT REGULATORY REFORM

relevant hedge ratios (the so-called Platinum hedges). This is a sophisticated
exercise and is proprietary to CrashMetrics. Here we will illustrate using a
simple brute force implementation, which can be used as a bank’s stress/
scenario testing to supplement VaR. The advantage is that while conven-
tional scenario testing points to the risk of some hypothetical made-up
situations, which could be unrealistic, CrashMetrics points objectively to a
bank’s individual vulnerabilities. This should be appealing to regulators who
can then compare worst-case losses among banks.
We first map the universe of all risk factors to five benchmark indices as
shown in Table 12.1. The crash coefficient (ki) for risk factor i is estimated
with respect to its benchmark index. When the benchmark index is stressed
by δX, that risk factor is stressed by kiδX. In effect, the crash coefficients link
risk factors to benchmarks. Hence, by stressing all five benchmarks, we
effectively stress all risk factors.
Secondly, we grid the hypercube. Suppose we have a small bank whose
core business is in rates, FX, and options trading; thus, we only need three
benchmarks. Suppose today’s state of the world is given by: (USD 10yr
swap, U.S. dollar index, VIX index) ¼ (3%, 78, 27%). We assume a range
where the extreme moves can happen for each benchmark, for example:
(USD 10yr swap, U.S. dollar index, VIX index) ¼ (0  10%, 50  100,
2  70%), and grid each range into 20 equal segments. Then the hypercube
will contain 20×20×20 ¼ 8,000 states (or scenarios).
Thirdly, we perform reverse stress tests. The bank determines the
maximum loss ψ that will cause it to fail. This critical level of loss is a
function of the bank’s own balance sheet strength and its ability to raise
emergency funds. The bank’s positions are repriced for all scenarios in the
hypercube. The scenarios that create a loss larger than ψ are highlighted as
red zones in the hypercube.
Lastly, expert judgment is required of the risk controller. What is the
probability of occurrence of the red zones? This is a function of how far they
TABLE 12.1
Mapping of Risk Factors to CrashMetrics Benchmarks
Risk Factor Class (across
all countries)
CrashMetrics Benchmark
Equities
S&P 500 index
Interest rates
USD 10-year swap
Foreign exchange rate
U.S. dollar index (measures dollar strength relative to
a basket of currencies)
Volatilities (all asset types)
VIX index (measures implied volatility)
Commodities
S&P Goldman Sachs commodity index
Systemic Risk Initiatives
229

are from the current state and known structural relationships of the markets.
It requires expert knowledge of probability and correlation. For example, if
a red zone happens when rates are at 10%, the odds are really immaterial for
say a one-year risk horizon. Is it likely for rates to move from 3% to 10% in
a recessionary, low policy-rate environment? Also if the red zone suggests a
scenario of extreme sell-down in equities and extremely high rates, this is
also highly unlikely (perhaps even unrealistic) because equities and rates are
known to be structurally correlated due to fund flows, flight-to-quality, and
central bank policy responses.
This brute force method illustrates a rough way to locate a bank’s
vulnerabilities. The method is crude because errors can arise from the esti-
mation of crash coefficients and the choice of benchmarking. It should
always be followed by stress tests targeted at a bank’s specific vulnerabilities
(or risk concentration).
As an example of the application of CrashMetrics, please refer to
Gomez, Mendoza, and Zamudio (2012). A spreadsheet demo of Crash-
Metrics is downloadable from: http://thehedger.wikispaces.com/file/view/23
.CrashMetrics.XLS.
12.3 NEW YORK FED CoVaR
Contagion VaR or CoVaR was introduced by Adrian and Brunnermeier
(2009) to measure the spillover risk across financial institutions. Specifically,
it is defined as the VaR of institution i conditional on another institution j
being in distress; that is, j’s return being at its VaR level.
CoVaR measures volatility spillover in a noncausal sense. The idea is
that institutions often hold similar positions or are exposed to common
systemic risk factors. These crowded trades give rise to dynamic competition
games, which can cause liquidity spirals. For example, in an asset fire sale or
banks’ hoarding of liquidity, individual actions by banks may appear pru-
dent for the bank, but the impact on the system is often not macroprudential.
CoVaR measures this spillover or network effect.
We recall the definition of VaR as the q-quantile:
PrðXi ≤VaRi
qÞ ¼ q
ð12:9Þ
where Xi is a variable of institution i, and VaR is a negative number. The
time series Xi may be the daily profit and loss (PL) vector of the bank’s
positions, or changes in the bank’s total financial assets in its balance sheet,
depending on what type of spillover risk we want to measure.
230
THE GREAT REGULATORY REFORM

The CoVaR of institution j conditional on institution i being in distress is
then defined as:
PrðXj ≤CoVaR
j9i
q 9Xi ≤VaRi
qÞ ¼ q
ð12:10Þ
We are most interested in the case where j is the entire financial system
(j ¼ system). In this case, the institution i’s contribution of spillover to the
financial system (j) is defined by:
ΔCoVaR
j9i
q ¼ CoVaR
j9i
q  VaRi
q
ð12:11Þ
Studying data on U.S. financial institutions, the authors found evidence of
spillover—thatCoVaR tendstobelargerthanVaR ingeneral, making equation
(12.11) positive. Furthermore, a linear regression of ΔCoVaR and VaR shows
an absence of a linear relationship, which suggests that conventional VaR does
not contain information on systemic risks at all: hence, the need for CoVaR.
Figure 12.5 shows a schematic of CoVaR between various institutions.
CoVaR of institution i depends on the risk-taking activities of other institutions
Bank A
Bank E
262
211
444
371
832
951
202
102
326
91
249
111
265
832
253
144
Bank B
181
253
Bank C
Bank D
FIGURE 12.5
Schematic of CoVaR Network (Numbers Are Hypothetical)
Systemic Risk Initiatives
231

in the network. CoVaR is also directional—CoVaR of institution i conditional
on j does not equal CoVaR of institution j conditional on i in general. This is
illustrated as two different numbers on each connecting line in Figure 12.5.
The systemic risk regulator would in principle be able to collect all the
required information Xi from each institution and calculate an institution’s
contribution to systemic risk using equation (12.11). Regulatory risk capital
should then be a function of not just VaR but also ΔCoVaR—large con-
tributors to systemic risk should be surcharged with higher capital.
Estimation of CoVaR
A convenient way to estimate CoVaR is by quantile regression (see Section
2.13). Here we assume j is the financial system and is related to bank i’s
return variable by a linear model:8
Xj ¼ α þ βXi þ εj
ð12:12Þ
where Xs are a function of time t (which we have suppressed for brevity),
and the error term εj is i.i.d. We denote the inverse CDF by F1(q) for a given
q-quantile. Then the inverse CDF for (12.12) is written:
F1
j
ðq9XiÞ ¼ αq þ βqXi
ð12:13Þ
where the new parameters αq and βq contain also the inverse CDF of εj.
From the definition of VaR as a quantile function or inverse CDF, equation
(12.13) actually represents the VaR with confidence level (1  q) of system j
conditional on the returns of institution i.
We estimate the conditional quantile function (12.13) by solving for the
parameters αq and βq that minimize:
min
αq, βq
X
t
q9Xj  ðαq þ βqXiÞ9
if Xj  ðαq þ βqXiÞ ≥0
ð1  qÞ9Xj  ðαq þ βqXiÞ9
if Xj  ðαq þ βqXiÞ < 0
(
ð12:14Þ
Since CoVaR is defined as the VaR of j conditional on bank i’s return
being at its VaR level (i.e., set Xi ¼ VaRi
q), the CoVaR is thus given by:
CoVaR
j9i
q  ^F
1
j ðq9VaRi
qÞ ¼ ^αq þ ^βqVaRi
q
ð12:15Þ
The parameters with the ^ are estimates from (12.14). Spreadsheet 12.2
illustrates the calculation of CoVaR using the Excel Solver.
232
THE GREAT REGULATORY REFORM

In the original paper, the idea of CoVaR was extended to make Xi, Xj,
and equation (12.12) conditional on other lagged variables Mt1. These
could be systemic macro variables—such as the volatility index (VIX), repo
spreads, T-bill yields—or a firm’s institutional characteristics—such as
maturity mismatch, leverage, relative size, and so on. In particular, the
authors found that, on a one-year lagged basis, a more negative ΔCoVaR is
associated with higher values of the institutional characteristics just men-
tioned. The implication is that if regulators can predict, based on today’s
firm characteristics, what the ΔCoVaR will be for next year, it can levy a
preemptive capital surcharge. This means ΔCoVaR capital could potentially
offset the procyclicality weakness of conventional VaR. For further details,
the reader can refer to the paper by Adrian and Brunnermeier (2009).
12.4 THE AUSTRIAN MODEL AND BOE RAMSI
In 2006, researchers at the Austrian Nationalbank (OeNB) introduced a
model to measure the transmission of contagion risk in the banking system.
Called Systemic Risk Monitor (SRM), it is based on the work of Elsinger,
Lehar, and Summer (2006a, 2006b), which combines standard risk man-
agement techniques with a network model9 to capture the interbank feed-
back and potential domino effect of bank defaults. Only by looking at risk
from the system perspective can regulators spot two key problem areas that
could lead to financial system instability—correlated exposures and financial
interlinkages.
This section gives a simplified illustration of a highly complex series of
models. The interested reader is referred to the original papers.10 Figure 12.6
is a schematic of the SRM that was applied to the Austrian banking system.
Each bank in the system is represented by a portfolio.
The model structure splits a bank’s risk into three parts—a collection of
tradable assets: that is, stocks, bonds, forex (“market risk losses” box), loans
to nonbanks (“noninterbank credit risk losses” box); and interbank posi-
tions (“interbank network model” box); and the third part captures the
mutual obligations or counterparty risks, which are highly contagious.
The model uses a group of risk factors including market and macro-
economic factors. The (marginal) distribution of each factor is estimated
separately every quarter for a one-quarter horizon. The correlation structure
is modeled separately using a copula. The SRM model then draws random
scenarios from this multivariate distribution (the top box in Figure 12.6).
The scenarios are then translated to portfolio PL. For market risk losses, this
is a straightforward revaluation of deals based on new scenarios. But for
loans to nonbanks, we need to use a credit risk model to calculate portfolio
Systemic Risk Initiatives
233

losses. For this purpose, SRM uses CreditRiskþ developed11 by Credit Suisse
in 1997.
The innovation of the Austrian model is its network model of the
interbank, which checks whether a bank is able to fulfill its financial obli-
gations as a result of simulated movements in asset prices and loans, given its
balance sheet situation and mutual obligations with other banks. It does this
by using a clearing procedure, which we shall illustrate using a toy example
from the original paper by Elsinger and colleagues (2006a).
Suppose there are N banks in the system; bank i is characterized by net
value ei and liability lij against bank j in the system. The entire banking
system is thus described by an N × N matrix L and a vector of values
e ¼ {e1, e2, . . . , eN}. e represents asset positions such as bonds, stocks, and
Market Risk
Losses
Analysis: Problem statistics of the banking system
divided into fundamental and contagion defaults
Scenarios
Distribution of risk factor changes
0.3
0.2
0.1
0
Interbank Network Model
Noninterbank
Credit Risk
Losses
FIGURE 12.6
Basic Structure of the SRM
234
THE GREAT REGULATORY REFORM

nonbank loans, minus liability positions such as deposits and bonds issued
by the bank itself.
Consider a three-bank system with e ¼ (1, 1, 1) and with liability
structure given by:
L ¼
0
0
2
3
0
1
3
1
0
0
B
@
1
C
A
ð12:16Þ
where the numbers are in billions of dollars. For instance, bank 2 has a
liability of $3 billion with bank 1 and a liability of $1 billion with bank 3.
The diagonals are all $0 since a bank will not have a liability with itself.
The total liabilities of each bank due to the system is given by the vector
d ¼ (2, 4, 4), the row sum of L. Note that the total income due from the
system is given by the column sum of L.
The clearing mechanism is a redistribution scheme—if the net value of a
bank becomes negative, the bank becomes insolvent. This net value is the
income from noninterbank activities ei plus income received from other
banks (part of L) minus its liabilities to other banks (part of L). Once a bank
becomes insolvent, the claims of the creditor banks are satisfied propor-
tionally; that is, distributed according to the percentage of total liability
owed. To implement this proportion sharing, we need to divide L by d to get
a normalized matrix in percentage:
πij ¼
lij
di
if di > 0
0 otherwise
8
>
<
>
:
ð12:17Þ
In our example, this becomes:
Π ¼
0
0
1
0:75
0
0:25
0:75
0:25
0
0
B
@
1
C
A
ð12:18Þ
The network model is represented by a clearing payment vector, p. The
model has to respect the limited liability of banks (i.e., it cannot pay more
than what it has in capital) and proportion sharing; pi represents the total
payment made by bank i under a clearing mechanism.
Systemic Risk Initiatives
235

pi ¼
di
if
X
N
j¼1
πjipj þ ei
≥di
X
N
j¼1
πjipj þ ei
if di>
X
N
j¼1
πjipj þ ei
≥0
0
if
X
N
j¼1
πjipj þ ei
< 0
8
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
:
ð12:19Þ
The first condition says that the bank has enough income to pay off its
total liability, and so it pays di due to the system. In the second condition, it
pays proportionally as determined by weights πji, and in the third its pay-
ment is floored at zero due to limited liability. In the last two cases, because
pi < di, bank i has defaulted with recovery rate pi/di.
To solve for the vector p, Eisenberg and Noe (2001) proposed a pro-
cedure called fictitious default algorithm and proved that the procedure
converges to a unique solution after at most N iterations. The procedure
starts by assuming all banks fully honor their obligations; that is, p ¼ d.
After this payment, if all banks have positive value the procedure stops.
Second iteration: if a bank now has a negative value, it is declared insolvent,
and its payment is given by the clearing formula, equation (12.19). In the
example, this is the case for Bank 2. We keep the other bank’s payments
fixed as per the previous iteration. It can happen that another bank that was
positively valued in the first iteration becomes negatively valued (defaulted)
in the second iteration because it only received partial payments from
Bank 2. This is indeed what happened to Bank 3. Then Bank 3 has to be
declared insolvent, and we move on to the third iteration and so forth.
A bank that becomes insolvent in the first iteration is considered funda-
mentally defaulted; that is, due to macroeconomic shocks. A bank that defaults
in subsequent iterations is dragged into insolvency and represents a case of
contagion default. Using the algorithm, we can show that the clearing solution
is p ¼ (2, 1.87, 3.47), Bank 2 is fundamentally defaulted, and Bank 3 expe-
rienced a contagion default. Spreadsheet 12.3 provides a worked-out solution.
In practice, L comes from balance sheet information of banks, and e is a
function of risky positions held by banks—both of which are at the disposal
of an ardent regulator. Market/economic scenarios sampled from the joint
distribution are applied to e to simulate the risks caused by external shocks
over a three-month horizon. Spreadsheet 12.3 also provides an example
where e is randomly simulated to calculate the probabilities of fundamental
and contagion defaults of the three banks.
236
THE GREAT REGULATORY REFORM

The SRM system is a powerful tool that can simulate systemic risk
conditions and identify vulnerabilities in a banking system. Some analyses
that could be carried out include:
1. Stress testing—the user can simulate specific scenarios, for example, a
5% fall in GDP.
2. Estimation of probabilities of fundamental and contagion defaults of
each bank in the system, and produce a ranking of banks.
3. By judiciously selecting relevant risk factors for simulation, one can
study the loss distribution due purely to credit risk, interest rate risk,
market risk, contagion risk, or combinations of various risks.
4. The loss distributions from (3) can be used for VaR-type analysis for the
whole banking system or for segments of the banking system.
5. The central bank can perform policy experiments, for example, to test
the impact of a one-time rate hike of 200bp.
In 2009, the Bank of England (BOE) embarked on a project to model the
network funding liquidity risk of the U.K. banking system. The model is a
more sophisticated extension of the Austrian model and is known as RAMSI
(Risk Assessment Model for Systemic Institutions). RAMSI is a sequence of
models designed in a modular architecture (see Figure 12.7).
Effects on bank
lending
System assets/loss
distribution
Network model of U.K. banks
and financial institutions
Feedback
Asset-side (“Market
Liquidity Risk”)
Liability-side (“Funding
Liquidity Risk”)
Macroeconomic/
Financial
Credit losses
Available for sale assets
Trading income
Net interest income
Non-interest income and expenses
Shocks/Scenarios
FIGURE 12.7
RAMSI Framework
Systemic Risk Initiatives
237

The innovation of RAMSI is in its modeling of feedback effects: the
feedback associated with asset fire sales following a bank default and
the liability-side funding liquidity feedback. Unlike the single-period Austrian
model, RAMSI is a multistep model with a three-year forecast horizon (see
Figure 12.8). A preliminary model description is available from the BOE.12
12.5 THE GLOBAL SYSTEMIC RISK REGULATOR
The 2008 crisis has highlighted the complete absence of regulation of sys-
temic risk. The Turner Review aptly described this situation as “global
finance without global government: faultlines in regulatory approach.”
National regulators do not have cross-jurisdiction powers, and coordina-
tion among regulators is not effective and timely. Even within the same
country, supervision is often fragmented with multiple regulators looking at
different but overlapping aspects of the financial system such as securities
houses, futures trading, banking, insurance, and financial accounting. Quite
the opposite, global banks operate across international borders, often
exploiting differences in regulations so as to be as capital-efficient as pos-
sible, for example, by migrating operations to regulatory or tax-friendly
jurisdictions.
There is asymmetry of information here: A bank knows exactly what its
offshore branches are doing, but not what other banks are doing. An indi-
vidual regulator knows only what the banks within its borders are doing and
not about offshore banks. The informational disconnect creates a faultline
where systemic risks can grow undetected by any one party.
Compounding the problem is that a new class of institutions (what is
now known as shadow banking) has emerged that functions like a bank but
Start of Period
Feedback Loop
Macro/
Financial
Shocks
PDs Yields
Balance Sheet
Adjustment
Available for
Sales (AFS)
Loan Book
(credit losses)
Profit &
Loss Trading
Income
Net Interest
Income
Other Income
Operating
Costs Tax &
Dividends
Rating
Model
Funding
Liquidity/
Bank
Failure
Check
Liquidity
Feedbacks
Asset Fire
Sale Funding
(Confidence)
Network
Losses
Reinvestment
FIGURE 12.8
RAMSI Model Dynamics
238
THE GREAT REGULATORY REFORM

is not regulated as a bank. These are entities such as SIVs (special investment
vehicles), pure investment banks, and mutual funds, which perform the
essential banking function of maturity transformation—effectively borrow-
ing on short-term (liability), and lending or investing on long-term (asset). It
is a crucial function that provides social and economic value, but it creates
risks—there is price risk on the asset side and liquidity risk on the funding
side. This large shadow banking system contributes to systemic risk (con-
tagion) because their positions are often on the same side as regulated banks.
In a crisis, they may face investor redemptions in the same way that a bank
may face bank runs, even though they are off the radar screens of regulators,
and are often too big to fail.
These faultlines argue for the creation of systemic risk regulators, of
which three were set up in 2009: the Financial Stability Board (FSB), which
represents the G-20 nations; the European Systemic Risk Board (ESRB) from
the European Union; and the Financial Services Oversight Council (FSOC)
from the United States. It seems logical that a single global regulator would
solve some of the coordination problems previously mentioned and reduce
political lobbying. But in the near term, this will unlikely materialize for the
same reasons that one cannot find a global government today.
Let’s for the moment envision how such a regulatory body would work.
It should have executive powers to investigate a bank13 (in a more intrusive
way), technical expertise, political independence, and a modus operandi not
unlike that of a tsunami warning center. Some of the technical roles it will
perform may include:
1. Determining the optimal parameters for VaR systems and standardizing
these for banks.
2. Providing a common set of risk factors and market data history for all
banks so that their VaRs are comparable.
3. Collating daily all the PL vectors from banks in the most granular
format, say by deal versus risk factor breakdown (a bank still computes
its PL vector using its own system). Counterparty information per deal
is also collected. Let us call this multidimensional dataset a hypercube.
The hypercube contains all information on positions and risks of each
bank.
4. Using “smart” computer programs to analyze the data hypercube. This
could ferret out information such as pockets of risk concentrations,
hidden correlation risk clusters14 in a bank or within the banking sys-
tem, counterparty risk concentrations between systemically important
firms, etc.
5. Collating from banks the aggregated PL vector for the computation of a
bank’s VaR.
Systemic Risk Initiatives
239

6. Collating balance sheet information and counterparty names for inter-
bank funding activities.
7. Calculating CoVaR of each bank to analyze the spillover risk in the
banking system. This measures potential market risk contagion and
could highlight banks which are most contagious.
8. Using network models to simulate potential default risk contagion in the
banking system. The PL vectors from each bank can be used to generate
an (empirical) joint distribution for the simulation, thereby accounting
for nonlinear positions held.
9. Issuing warnings if overall systemic risk is above a certain threshold,
or if there is undue concentration of systemic risks in certain markets or
institutions.
Most of these ideas are within reach of today’s risk technology and
knowledge. Basel III rules encourage over-the-counter derivatives to be
processed and cleared with an independent third party, the central coun-
terparty (CCP). This would set the stage for greater transparency and
availability of data for a systemic regulator. The CCP will eliminate coun-
terparty risk (in theory) since the CCP is effectively the counterparty to each
trade and will maintain the usual margining safeguard. For a participating
bank, a deal posted at the CCP will benefit from low capital requirement.
In the testimony to the U.S. House of Representative, Lo (2009) recom-
mended the data collection and centralized monitoring of seven aspects of
systemic risks: leverage, liquidity, correlation, concentration, sensitivities (to
market movement), implicit guarantees, and connectedness. For a recent
development in systemic risk measurement, read Billio and colleagues (2010).
12.6 SPREADSHEET EXERCISES
12.1 Researchers have found that trading strategies that involve switching
can cause systemic contagion when the majority of participants are
collectively applying the same strategy. The usual bell-shaped return
distribution can become distorted (bi-modal) even though this is not
reflected in empirical distribution. When this happens, VaR is an
inaccurate measure of risk. Aim: Simulate a stop-loss strategy where
the stop-loss is 0.2 standard deviations (s.d.) below the current price,
and plot the resulting return distribution. Note: The main process is
simulated using a standard normal distribution. When the stop-loss
is triggered, players will sell into a falling market and cause a three s.d.
fall in the next time step. This switching strategy generates a bi-modal
distribution.
240
THE GREAT REGULATORY REFORM

12.2 CoVaR (contagion VaR) is a recent development that measures the
spillover risks among financial institutions. It is the VaR of an insti-
tution conditional on another institution being in distress (i.e., having
a loss at/beyond its VaR level). CoVaR is estimated using quantile
regression. Aim: illustrates CoVaR using two hypothetical return time
series from two institutions (i and j). Action: CoVaR of j conditional
on i is solved by using the Excel Solver. Try to solve for CoVaR of i
conditional on j instead. Are they the same?
12.3 The Austrian model is one of the earliest models for network risks of
the banking system. Using a clearing procedure, it checks whether a
bank is able to fulfill its payment obligations to the system as a result
of simulated movements in assets prices and loans, given its balance
sheet situation and mutual obligations with other banks. Aim: Illus-
trate a simple network model for a banking system with three banks,
and illustrate the fictitious default algorithm used to solve the model.
Note: The first sheet shows a static example, and the second shows a
dynamic example where the vector e is simulated using geometric
Brownian motion (using VB macros). You may change the blue cells to
modify the banks’ balance sheet initial conditions.
NOTES
1. We use this term generically to mean what many researchers also call feedback
loop, vicious cycle, endogenous risk, and network externalities.
2. The program effectively tries to mimic the payoff of a call option so that the
downside of the portfolio is protected. It does this by calculating all the correct
sizes and levels to buy or sell. Execution is automated.
3. Hedge funds attacked the yen in 1998 while the central bank, BoJ, was hope-
lessly defending the yen. The tide turned after the Russian debt default (August
1998) and the LTCM debacle (September 1998) caused nervous speculators to
abandon the attack. In this case, certain events or price levels caused the market
(which held a uniform position) to switch strategy en masse. The rush-to-exit
(unwind) caused the yen to rise by 15% in the week of October 5–9.
4. In classic economic theory, the typical demand curve (plot of Price versus
Quantity) is negatively sloping, and the typical supply curve is positively slop-
ing. The crossover of the two curves (on the Price versus Quantity plot) gives the
equilibrium price where demand is satisﬁed by supply.
5. Brunnermeier, M., A. Crocket, A. Persaud, and H. S. Shin, “The Fundamental
Principles of Financial Regulation,” Geneva Reports on the World Economy 11
(2009).
6. Dynamic hedging (or delta hedging) of a derivative involves continuously
buying and selling the underlying asset of that derivative in such a way that the
Systemic Risk Initiatives
241

combination has a delta (and perhaps also other risk sensitivities) of zero at any
point in time. A static hedge, in contrast, is not adjusted over time and is tar-
geted to protect very speciﬁc vulnerabilities.
7. It can happen that, for a complicated portfolio, there may be multiple troughs in
the solution space. The lowest trough is called the global minimum; all other
troughs are local minima. A good optimization program is able to ﬁnd the
global minimum while avoiding all the local minima.
8. For ease of explanation, we illustrate a simpliﬁed version of the authors’ paper.
In the original paper, Xj is conditional also on a vector of external state variables
Mt-1, and the stochastic third term (hence volatility) is also conditional on Xi.
9. The network model is due to an earlier work by Eisenberg and Noe (2001).
10. For a nontechnical overview, please see “Systemic Risk Monitor: A model for
systemic risk analysis and stress testing of banking systems,” OeNB Financial
Stability Report 11.
11. This model (unlike Creditmetrics) models only default risk; migrational risk is
excluded. The default risk is not related to the capital structure of the ﬁrm but
is modeled simply as a Poisson distribution.
12. See BOE Working Paper No. 372, “Funding Liquidity Risk in a Quantitative
Model of Systemic Stability” (June 2009).
13. We use the word bank generically to also include all other systemically
important institutions (shadow banks).
14. For example, Oest and Rollbuhler, “Detection and Analysis of Correlation
Clusters and Market Risk Concentration” by Wilmott Magazine (July 2010).
242
THE GREAT REGULATORY REFORM

PART
Four
Introduction to Bubble
Value-at-Risk (BuVaR)


CHAPTER 13
Market BuVaR
I can calculate the motion of heavenly bodies, but not the madness
of people.
—Sir Isaac Newton,
mathematician and physicist, (1642–1727)
13.1 WHY AN ALTERNATIVE TO VaR?
The 2008 credit crisis has challenged the foundation and the tradition of
value at risk (VaR) as the standard risk measure and the de facto metric for
minimum capital. As we have seen in Part Two, the weaknesses in VaR had
been well known and debated in academia for some time; for example, see
the warnings of Danielsson and colleagues (2001). Unfortunately they have
been largely ignored by the industry. It was only during the global financial
crisis (2008) that these weaknesses were put to the test. VaR was subse-
quently criticized by a populist movement and popular press; see Taleb
(2007).
Three major weaknesses highlight the need for a better alternative:
1. VaR as a number is not very useful. It may have a meaning, but it is not
very useful. Recall that a 97.5% VaR is not the quantity you could lose
with 2.5% chance—it is the minimum you could lose with a 2.5%
chance. The expected loss could be many times larger. You will be
precisely inaccurate if you misunderstand this point! As an analogy:
suppose you have a volcano warning system that tells you with preci-
sion that the minimum potential loss of life is 50,000 in the next
eruption. The next day, the model says, due to a new tectonic reading,
the minimum potential loss has gone down to 10,000. You would
245

naturally feel relieved by this latest reading, but this information is not
useful at all—in fact, it is outright dangerous and misleading. Due to
extremistan, the real loss could be 11,000 or 10 million! There is no way
to tell. Still, most bankers talk about VaR with an air of certainty and
tangibility in boardroom meetings. Behavioral science reveals plenty of
evidence that the human mind tends to associate and project when given
some information, even when the information is completely irrelevant.
In a sense, we are all wired to be fooled by randomness. There is no way
to get around this instinct.
2. VaR is symmetrical (nondirectional). It does not take into account the
often richer information set contained in price levels. Valuable infor-
mation is lost in the process of detrending or differencing to arrive at a
stationary time series. Consequently, as long as the return sample
wiggles with the same degree of volatility, VaR will be the same,
regardless of where we are in the business cycle—whether at the height
of euphoria, in range-bound, or in the slump of a recession. To be fair,
simulated VaR allows for skewed distributions, but we have seen in
Section 7.1 that skewness is often a weak and incomplete measure of the
asymmetry of price behavior.
3. Modeling the tail of a distribution is futile. Such modeling is futile since
there is practically not enough data at the tail to make inferences in a
statistically meaningful manner. Some methods such as Conditional
Autoregressive VaR or, CAViaR overcome this lack of observations by
modeling the tail as being a function of some independent variables.
These quantile regression models have to struggle with specification
problems instead. In all likelihood, the real tail risk is a moving target
and unstable due to the extremistan nature of extreme events.
Although not a very useful number, proponents may argue that VaR is
at least a consistent estimate (in a statistical sense)—we can measure VaR
with a given precision or confidence level. Precision has to do with repro-
ducibility, but is reproducibility all-important when the system we are
measuring is dynamically changing with time and levels? History evolves—
technological advances in trading, economic breakthroughs, emergence of
program trading, regulatory changes to the rules of the marketplace, product
innovation—are often irreversible regime shifts (or game changers). Recall
the target analogy in Section 1.4: If we are dealing with a moving target, any
attempt to be precise will surely render the outcome inaccurate. An accurate
(though less precise) approximation is perhaps better in dynamical systems.
It is probably common sense to assume that the true expected loss E lies
somewhere between VaR (a minimum loss) and the maximum loss. Due to
extremistan, the maximum loss is elusive. In the absence of perfect foresight,
246
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

a good workable number is the all-time historical largest loss.1 It is also
likely that E changes with time. We propose to make E dependent on the
price level,2 specifically, on a function of the market cycle. We call this
estimate Bubble VaR (buVaR).3
BuVaR is a countercyclical method that gives a rough estimate of the
expected loss between the maximum and minimum bounds, conditional on
current price levels. BuVaR is designed with regulatory capital in mind and
addresses the three weaknesses mentioned earlier—we expect buVaR to be
less precise than VaR, more objective than stress testing, and likely more
accurate than both. BuVaR is directional; in particular, long positions
are penalized most at the peak of a market bubble (where crash risk is
highest), and shorts are penalized most at the market trough (where risk of a
bounce is highest). This countercyclical buffer protects against fat-tail events
(crashes and bounces) in the corrective direction.
13.2 CLASSICAL DECOMPOSITION, NEW INTERPRETATION
In Chapter 2, we introduced the concept of classical decomposition where a
price series can be seen as being composed of a long-term trend, a cycle, and
a noise component (see Figure 2.26). Conventional VaR exclusively models
the distributional properties (i.e., moments and quantiles) of the noise
component. Unfortunately, the phenomena of fat tails, clustering, and the
leverage effect have so far eluded modeling. Quantile regression and EVT are
some attempts at explaining these phenomena using tail data (i.e., excee-
dences) of the noise component. While some models are successful in
explaining certain aspects of these phenomena, we still lack the tools to
control their risks.
It is well known in the classical decomposition that valuable information
is lost in the process of differencing to derive the stationary data (i.e., in the
simple act of taking returns). So, the scenarios used for VaR computation are
informationally incomplete. This could suggest that modeling fat tails and
other phenomena using the noise component may be an exercise in precision
but not accuracy. We conjecture that the answer lies in the cycle, not the
noise component.
We suggest a new interpretation where the long-term trend is driven by
real economic growth, the cycle is caused by speculative excess (bubbles),
and the noise component is the realization of trading under normal efficient
market conditions. The latter can be modeled using a normal distribution in
the same way as VaR.
First, we posit that the fat-tail phenomenon is caused by a break in the
cycle, which we identify as a market crash or a bubble burst. Crashes are just
Market BuVaR
247

corrections in the cycle—asymmetric and sharp. Figure 13.1 illustrates the
new interpretation idea and highlights a break in the cycle. Note that
the three components add up to create the price series; differencing prices
gives the return series in the lower panel. This model also explains the
leverage effect by saying that a downward cycle break is more common than
an upward cycle break.
Second, we posit that a cycle compression causes increased serial cor-
relation in the return series, which could explain the volatility clustering
effect. The notion is that cycles do not have a constant shape—in times of
market stress both the frequency and the amplitude increase. This is illus-
trated in Figure 13.2, which shows compression in the last two cycles (the
shaded zone). Notice the resulting increased serial correlation of returns is
clearly manifested as a pattern in the noise (the lower panel). So during
market stress, returns are less independent and identically distributed (i.i.d.)
and stationary.
Figure 13.3 illustrates the situation where both cycle compression and
cycle breaks are present. The return time series exhibits both serial corre-
lation and fat-tailness. These rather stylized examples show that realistic
–200
–100
0
100
200
300
400
500
600
–150
–100
–50
0
50
100
Observed
as fat tails
Manifested return
series
Cycle with
a "break" 
Long-term trend
Combined time
series
Noise component
FIGURE 13.1
Fat Tails Caused by Breaks in the Cycle
248
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

market behaviors can be modeled using the new interpretation. Spreadsheet
13.1 lets the reader experiment with this concept.
The key benefit of the new interpretation is that the distortions in the
cycle can then be used to improve the distributional forecast. By contrast,
in the classical decomposition, only the stationary noise component deter-
mines the distributional forecast.
Our final objective is a countercyclical regulatory capital. What does this
insight mean for regulatory minimum capital? Obviously we should not
penalize the long-term trend; in fact, regulators should encourage partici-
pation in the economy. We should penalize the cycle but only to prevent a
crash. In other words, the minimum capital should be asymmetrical
depending on whether positions are long or short the bubble asset. In par-
ticular, minimum capital should discourage bubble chasing.
We start with a premise that the tail is unknowable (not modelable)
because of extremistan. We need to design a safeguard that is countercy-
clical. The noise component—representing trading during relatively stable
market conditions—is already well captured by conventional VaR. The idea
–300
–100
100
300
500
700
–100
–50
0
50
100
Manifested return
series
Observed serial
correlation
Compressed
cycle
Noise component
Long-term trend
Combined
time series
FIGURE 13.2
Cycle Compression Increases Serial Correlation and Volatility
Clustering
Market BuVaR
249

is to use VaR (or a similar tail measure) and inflate it asymmetrically, using
the cycle so as to penalize positions taking in the direction of the bubble.
13.3 MEASURING THE BUBBLE
The Notion of Equilibrium
To analyze the cycle, we need to measure price deviation from some equi-
librium level, which can be thought of as the aggregate level of relative calm
during a long-term period, or baseline growth trend of the market economy.
The further the price diverges away from this equilibrium, the larger the
bubble has formed and the higher the crash risk. It follows that VaR should
be inflated to reflect this heightened risk.
A natural choice is to represent the equilibrium as some kind of moving
average4 because we need a rolling window to make buVaR responsive to
market changes. The last three major crises (2007, 1997, 1987) occurred
10 years apart, so the window length must be shorter than 10 years in order
to exclude distortion from previous crises. Yet it must be long enough to
contain the baseline growth trend of the most recent cycle. Empirically, a
–300
–200
–100
0
100
200
300
400
500
600
700
–100
–50
0
50
100
Manifested return
series
Observed serial
correlation + fat tail
Long-term trend
Combined
time series
Compressed cycle
with a "break"
Noise component
FIGURE 13.3
Cycle Break and Cycle Compression Combined Effect
250
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

1,000-day window length, equivalent to four trading years, is workable and
will be used here.
Let’s test these ideas using a naïve 1,000-day simple moving average
(MA). Figure 13.4 shows an MA for the S&P 500 index; the area plot shows
the percentage price deviation from the MA.
From here on, we shall call such deviation from equilibrium the bubble.
The bubble is used to inflate VaR, for example if the bubble is positive
(negative), we will penalize long (short) positions proportionally. To achieve
this, we multiply the negative (positive) side of the return distribution used
for VaR calculation by a factor called the inflator. The inflator is a function
of the bubble after a calibration step.
First, in Figure 13.4 we would have penalized investments for a decade
in the 1990s (the shaded area). As mentioned earlier, the minimum capital
should not penalize the long-term trend. Otherwise, banks that use buVaR
for minimum capital will underinvest during a period of sustainable growth;
this is economically undesirable.
Second, in the July 2008 crash, the bubble turned negative too early
because during a typical crash, the price falls sharply through the “slower”
MA line. This means buVaR will penalize the shorts, when it really should
penalize the longs during a crash. Put another way, it encourages buying
during a crash. This is not prudent. Ideally, buVaR should detect that a crash
is underway and continue to penalize the long side.
–
200
400
600
800
1,000
1,200
1,400
1,600
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
FIGURE 13.4
1,000-Day MA as the Equilibrium Line (S&P 500 index)
Market BuVaR
251

Third, notice that the area plot (in Figure 13.4) is choppy because we are
taking the difference between a jagged price line and a smooth MA line. This
means the bubble measure (hence, buVaR) will be too choppy and unsuit-
able for minimum capital.
Using a simple moving average clearly does not work, but it highlights
the criteria required for a good buVaR design. In particular:
1. The measure must follow the market cycle.
2. The measure must not penalize investments during periods of sustain-
able long-term growth.
3. The measure must persistently penalize positions that are against a
market crash throughout the crash episode.
4. The measure must not be too choppy for minimum capital purposes.
The reason is mentioned in Section 6.4.
Adaptive Moving Average
To satisfy criterion (2), we introduce an adaptive moving average (AMA)
where the average is taken over a variable window length m adapted to
recent history.
m ¼ Int Min
Stdevðxn, xn1, : : : , xn500Þ
Stdevðxn, xn1, : : : , xn1000Þ  1000, 1000
	



ð13:1Þ
where xn is the price on day-n and m ranges between [500, 1000] days. Here,
we will often write equations in the form of Excel functions similar to how
they would appear in the spreadsheet.
When market prices trend up (or down) for an extended period of time
at constant speed, m is designed to adapt and shrink to about 500 days. With
a shorter rolling window, the MA line will move closer to the price, thereby
reducing the deviation (bubble). Put simply, the AMA hugs the price during
periods of sustainable growth. As a comparison, Figure 13.5 shows the
AMA and the exponentially weighted moving average (EWMA) of the S&P
500 index. It is well known that the EWMA is more timely than a simple
MA, but AMA performs even better.
Using Rank Filter
We propose the following methodology to derive a well-behaved equilibrium
measure. To measure the equilibrium on day-n using prices x up to day-n:
252
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

1. Derive a vector of daily log returns rn ¼ ln(xn/xn1) for all previous
days.
2. Apply an 8% rank filter5 to rn within the past 1,000-day rolling win-
dow. That means we set rn ¼ 0 whenever it is below the 8% quantile or
above the 92% quantile of a rolling sample {rn, . . . , rn1,000}. We have
effectively filtered off the exceedences of the return distribution.
3. Create a vector of discount factors given by dn ¼ exp(rn) for each n.
4. At day-n, reconstruct a 1,000-day new price vector {pn, . . . , pn1,000}
backward iteratively using the starting price xn. In other words, pn ¼ xn
(the starting price); then calculate pn1 ¼ pn/dn iteratively until pn1,000.
This vector is called the alternate history.
5. We defined the equilibrium μn as the adaptive moving average of the
alternate history.
This rank filtering process effectively removes all extreme price move-
ments due to crashes, euphoria, unsustainable rallies, and breaks. We have
recreated an alternate history in which bubbles and manias did not exist
hypothetically and growth was gradual and sustainable. It is the adapted
average of this new history that locates the equilibrium level. We define the
bubble as the deviation from this equilibrium.
Bn ¼ xn=μn  1
ð13:2Þ
–
200
400
600
800
1,000
1,200
1,400
1,600
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
m
S&P 500
AMA
EWMA
m = 1000 (axis not shown)
FIGURE 13.5
Price Hugging by AMA and EWMA (for S&P 500 index)
Market BuVaR
253

Figure 13.6 shows the equilibrium and the bubble for the S&P 500. The
simple MA and its deviation are also shown for comparison. In the 1990s,
the bubble shrank (because of the adaptive hugging) as the growth trend
became established, while the MA deviation continued to penalize the longs.
Another attractive feature is that the bubble is smoother than the MA
deviation, because it is the difference between two jagged lines moving in
tandem (the S&P and its equilibrium), whereas the MA deviation is the
difference between a jagged line (S&P) and its smoother MA.
When there is a sharp price fall, as occurs in a crash, the equilibrium
point falls by about the same magnitude also, because the starting price xn
used for its computation falls. As a result, the bubble is maintained and does
not fall to the negative during a crash (unlike the MA deviation). Hence, the
inflator (defined next) not only penalizes longs during bubble formation but
also throughout the crash. Only after the crash, if a bearish bubble forms,
will the inflator turn negative. The same reasoning applies to the bearish side
as well, with “crashes” taken to mean upward spikes (bounces) in prices.
13.4 CALIBRATION
To create a countercyclical VaR, we introduce the concept of an inflator that
is designed to follow the market cycle. It is desirable to have a capital buffer
that depends on what type of assets are being held by a bank; hence, the
-1
0
0
1
1
2
2
3
0
200
400
600
800
1,000
1,200
1,400
1,600
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
S&P 500 Index
Equilibrium
Simple MA
MA deviation
Bubble
FIGURE 13.6
The Equilibrium and the Bubble (for S&P 500 index)
254
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

model has to be calibrated using previous episodes of crashes or crises,
specific to that asset.
The inflator Δ is a multiplicative adjustment to every scenario on one
side of the return distribution, penalizing the bubble side. On day t, if Bt
> 0, multiply every scenario of the negative side to penalize longs, and if
Bt < 0, multiply the scenarios on the positive side to penalize shorts.
First, we define the maximum stress Ψ as the average of the five largest
positive and negative (absolute) daily returns in all history, capped by the
circuit breaker size imposed by the exchange, if any.6 For the computation of
this average, we exclude large returns caused by regulatory changes (such as
foreign exchange depegs and sovereign debt defaults—see Section 9.1)
because such events are almost never repeatable and are not caused by the
formation of financial bubbles.
Next, to derive the inflator, we assume a response function of the form:
Δ ¼ exp½ω1AbsðBtÞω2
ð13:3Þ
where ω1, ω2 are positive parameters. Aside from simplicity, equation (13.3)
has some desirable features:
1. It is floored at 1.0. This is useful as this multiplier is supposed to inflate
a scenario, not decrease it.
2. It grows exponentially with the bubble, Bt. In other words, the penalty
should expand rapidly to restrain market euphoria.
However, we need to cap the exponential equation (13.3), so that Δ
does not grow without bound. In other words:
Δ ¼ Min

cap, exp½ω1AbsðBtÞω2

ð13:4Þ
Indeed, we want VaR to be inflated but not rise beyond Ψ. We define the
cap as the shift that will bring the current VaR (which we approximate as
2σt) to Ψ. This cap should correspond to the largest absolute observed
bubble value Bmax in history.7 Using this boundary condition, equation
(13.3) gives:
Ψ=ð2σtÞ ¼ exp½ω1Bω2
max
ð13:5Þ
where σt is the standard deviation of returns of the VaR observation
period of 250 days, ending day t. Substituting this inside (13.4), we can
eliminate ω1:
Market BuVaR
255

Δ ¼ Min
Ψ
2σt
, exp
AbsðBtÞ
Bmax

ω2
ln
Ψ
2σt


	



ð13:6Þ
The response function (13.6) converts the measured bubble Bt to the
inflator Δt. The reader can use Spreadsheet 13.2 to explore the behavior of
the response function. The parameter ω2 tunes the curvature of the response
function (see Figure 13.7). We propose to set ω2 ¼ 0.5. This is found by
choosing the ω2 that gives the smoothest day-to-day variation in buVaR,
because a stable metric is useful for the purpose of risk capital.
The bubble measure often peaks one step ahead of a market crash.
Hence, the inflated VaR (buVaR) will rise well before the conventional VaR
rises, often months in advance. The latter is known to spike up at the same
moment as a crash, hence, too late for risk control. Ideally, Ψ should be
determined by the regulator—a higher upper bound will give a more con-
servative (penal) buVaR. Here we set Ψ such that the resulting buVaR is
slightly higher than the expected shortfall as witnessed during the 2008
crisis, but where this buVaR will be delivered (or charged) ahead of crises
during the euphoria (run-up) phase.
The inflator is recomputed daily because Bt and σt vary with time.
However, at any given day t, the constant Δ is multiplied to every return
1.0
1.2
1.4
1.6
1.8
2.0
2.2
0
2
4
6
8
10
Inflator
Abs(B)
ω2 = 0.5
ω2 = 3
ω2 = 1
(Ψ/(2σ),Bmax)
FIGURE 13.7
Inﬂator Response Function of Different ω2 for σ ¼ 2.5%, Bmax ¼ 6,
Ψ ¼ 10%
256
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

scenario on one side of the distribution (the side determined by the sign of
Bt). See schematic in Figure 13.8.
13.5 IMPLEMENTING THE INFLATOR
The inflator must be applied to the return distribution correctly. By design,
buVaR does not allow negative prices (or rates), for reasons explained in
Section 3.4. So negative rates, if any, will need to be floored at a small
positive number.
Since buVaR inflates one side of the distribution, if the inflator is large
enough, the negative returns (on the left tail) can cause shifted levels to become
negative, an undesirable outcome. This problem is solved by using log return
instead of the usual percentage return. Hence, instead of using equation
(13.7), we use (13.8).
shifted levelðiÞ ¼ base level 

1 þ %returnðiÞ  Δ

ð13:7Þ
shifted levelðiÞ ¼ base level  exp

log returnðiÞ  Δ

ð13:8Þ
Bubble (B) < 0, Penalizing the Shorts
Original
distribution
+VaR  
+Ψ
+Δ
Bubble (B) > 0, Penalizing the Longs
Original
distribution
–Ψ
–VaR
–Δ
FIGURE 13.8
Inﬂating the Distribution on One Side
Market BuVaR
257

where i ¼ 1, . . . , 250 is the scenario number in the VaR observation
period, and Δ ≥1 is the inflator computed earlier. We only penalize the
bubble side; that is, if Bt > 0 then Δ ¼ 1 for all positive returns, if Bt < 0
then Δ ¼ 1 for all negative returns.
Figure 13.9 shows the plots of equations (13.7) and (13.8). Left panel:
When there is no inflation (Δ ¼ 1), just as in VaR, it does not matter
whether we used log return or percentage return—they are the same. Also,
shifted levels can never become negative.
Right panel: When we inflate the returns three times (Δ ¼ 3), the shifted
level can become negative if percentage return is used. But if log return is
used, the left tail shows a gradual fall to zero that avoids the negative ter-
ritory. If Δ is large, the decline to zero will be steep. On the right tail, the
shifted level rises in a convex way and may seem unnerving at first. But in
practice, there is no concern that the resulting buVaR will blow up indefi-
nitely because the shifted level is limited at some upper boundary—the
inflator is capped at Ψ/(2σt). Using the example in Figure 13.9 (base ¼ 100),
and assuming Ψ ¼ 25% (a quantum similar to the fall on Black Monday,
1987), the shifted level can barely go above 130 (¼ 100exp(0.25)) even for a
2σt return scenario. As seen in the figure, the convex behavior is still not
noticeable at price level of 130.
An advantage of buVaR is its ease of implementation. If a bank already
uses historical simulation VaR (hsVar), the infrastructure can be easily
modified to compute buVaR. The historical simulated scenarios need to be
(200)
(100)
–
100
200
300
400
500
600
700
800
(1.0)
(0.9)
(0.7)
(0.6)
(0.4)
(0.2)
(0.1)
0.1
0.2
0.4
0.5
0.7
0.8
1.0
–
50
100
150
200
250
(1.0)
(0.9)
(0.7)
(0.6)
(0.4)
(0.2)
(0.1)
0.1
0.2
0.4
0.5
0.7
0.8
1.0
Price
Change
Perc Return method
Log Return method
FIGURE 13.9
Plot of Shifted Level versus Returns (base level ¼ 100. Left: Δ ¼ 1,
right: Δ ¼ 3)
258
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

multiplied on one side by today’s inflator, a scalar, which is computed
independently. The bank also needs to ensure log returns are used as in
equation (13.8). Once a vector of shifted levels is generated, the rest of the
calculation steps (full revaluation and computing the tail measure) are
almost identical to that of hsVaR.
In an ideal global regulatory regime, the inflators and return scenarios
for all risk factors should be calculated by an independent and approved
agency. The data could be downloaded daily by banks for their buVaR
computation. Standardization would make risks comparable across the
banking industry for supervision.
13.6 CHOOSING THE BEST TAIL-RISK MEASURE
After the operation in equation (13.8), the shifted levels will be used by a bank’s
risk engine to revalue deals in the portfolio. Just as in hsVaR, the dollar dif-
ference between values of the base case and the shifted scenario cases will give
the profit and loss (PL) vector for each deal. These PL vectors are aggregated to
give the portfolio PL vector, which represents the PL distribution.
BuVaR is defined as the expected tail loss (ETL) of this PL distribution at
a given confidence level. Let us explore the benefits of using ETL rather than
quantile as our tail measure.
We simulate the GBM paths for two assets—A and B—which are
inversely correlated: 600-day price paths are generated and a regime change
is introduced on day 300, where the correlation changes from 0.9 to þ0.9
abruptly, and volatility increases threefold (see Figure 13.10). We wish to
study the responses to the regime break for ETL, hsVaR, and pVaR. All three
risk measures use a 250-day window and a 97.5% confidence level.
The implied correlation ρ is backed-out from equation (13.9) given the
other simulated variables, and is plotted in Figure 13.11.
σ2
portfolio ¼ σ2
A þ σ2
B þ 2ρσAσB
ð13:9Þ
where σA, σB, σportfolio are replaced with the risk measures for assets A, B,
and the combined portfolio respectively. All three risk measures are tested in
this way. Spreadsheet 13.3 provides a worked-out solution.
The resultinFigure 13.11 confirmsthatthe ETL is superior in three aspects:
Coherence
The implied correlation of hsVaR goes above þ1 occasionally. From
equation (13.9), this means the hsVaR of the portfolio can sometimes be
Market BuVaR
259

–1.0
–0.5
0.0
0.5
1.0
1.5
1
41
81 121 161 201 241 281 321 361 401 441 481 521
561
ETL
pVAR
HsVaR
Regime break 
FIGURE 13.11
Implied Correlations Computed from ETL, hsVaR, and pVaR
98.0
98.5
99.0
99.5
100.0
100.5
101.0
1
41
81
121 161 201 241 281 321 361 401 441 481 521 561
FIGURE 13.10
Two GBM Paths Simulating a Regime Break Halfway through
the Series
260
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

larger than the sum of its components—subadditivity is violated. As
expected, the implied correlation of pVaR stays below þ1 because, by
assuming a normal distribution, pVaR is subadditive. Interestingly, ETL is
also subadditive—its implied correlation is always bounded below þ1.
Responsiveness
Of the three risk measures, ETL is the most responsive to a regime switch. It
takes fewer days for its implied correlation to move from 0.9 to þ0.9 after
the regime switch occurred on day 300.
To understand this, recall that for hsVaR, the 97.5% quantile corre-
sponds to the seventh-largest loss (after rounding up). Assuming the sixth-
largest loss is very close to the seventh-largest loss, then, if a new extremely
large loss (a Black Swan) enters the left tail, the sixth loss will now become
the seventh loss (the new hsVaR). Hence, the dollar change in hsVaR is still
small. However, by taking ETL, the average tail loss will increase sharply
because of the inclusion of the new extreme observation.
The pVaR is the least responsive because standard deviation as a statistic
is quite robust to outliers.
Stability
The fluctuation in Figure 13.11 gives some idea of the stability of the
portfolio risk measure. After all, if ρ fluctuates a lot, by equation (13.9),
σportfolio will be less stable. Of the three risk measures, pVaR is the most
stable because standard deviation is a smooth statistic.
The ETL is marginally more stable than hsVaR. One can understand this
by considering the case where the eighth-largest loss is very far from the
seventh-largest loss (the hsVaR scenario). Then, if one of the left tail
observations falls out of the 250-day rolling window, the eighth loss scenario
will now become the new seventh loss (the new hsVaR). Hence, a large
change in hsVaR will occur simply from data rolling off. However, for the
ETL, the averaging of the seven tail observations will soften the data roll-off
effect, making ETL more stable.
For these reasons, we choose ETL as the tail measure for buVaR. The
q% buVaR is formally defined as the ETL of the inflated distribution over a
one-day horizon at (1  q) quantile.
Spreadsheet 13.4 provides an example of how buVaR is calculated (for
the S&P 500 index data). For comparison, both buVaR and ETL are plotted
in Figure 13.12 for long versus short positions.
Market BuVaR
261

As seen, buVaR penalizes the long side at the market peak in November
2007 and penalizes the short side at the market trough in November 2002.
In 1996, it penalizes the longs, but when the rally proves to be sustainable,
the inflator declines thanks to the adaptive moving average. Note that
buVaR did not peak in 2000 for the S&P 500. This is because the dot-com
crash happened to the Nasdaq index (see Figure 15.3) and not to the S&P
500, which entered a bear market in a more gradual way. It is clear that
buVaR has desirable countercyclical characteristics—it is usually one step
ahead of market crashes. A full range of tests using different assets and
various crisis episodes are performed in Chapter 15.
For the purpose of risk reporting, we recommend two important sug-
gestions. Firstly, the mean-adjustment should be applied at the aggregated
portfolio PL so that buVaR is expressed as a loss from the mean of a joint
distribution. Secondly, buVaR at higher confidence (say 99%) should be
derived using scaling by equation (6.12) because of data scarcity.8
13.7 EFFECT ON JOINT DISTRIBUTION
It is worthwhile to consider the big picture at this point and review what
buVaR does to the distribution. Consider the bivariate normal case.
–
200
400
600
800
1,000
1,200
1,400
1,600
–10%
–5%
0%
5%
10%
15%
20%
25%
30%
35%
40%
1993
1995
1997
1999
2001
2003
2005
2007
ETL (Long)
ETL (Short)
buVaR (Long)
buVaR (Short)
S&P 500
Index Level
VaR (% loss)
FIGURE 13.12
BuVaR versus ETL for Long and Short Positions (S&P 500 index)
262
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

Figure 13.13 shows a scatter plot of return samples (x, y), where the
positive returns y (along the upper vertical axis) have been inflated by a
factor of three on one side, to simulate the effect of the inflator. Mathe-
matically, Y undergoes a variable transformation: Y - 3Y when Y > 0.
The bivariate example is simulated assuming zero correlation. Figure 13.14
is the case for correlation of þ0.8.
Distribution
Inflated distribution
FIGURE 13.13
Inﬂator Impact on Joint Distribution of Two Assets with Zero
Correlation
Distribution
Inflated distribution
FIGURE 13.14
Inﬂator Impact on Joint Distribution of Two Assets with Correlation
of þ0.8
Market BuVaR
263

As seen, the inflated distributions become elongated and are obviously
no longer elliptical. Elliptical distributions are strictly symmetrical. How-
ever, this does not automatically rule out coherence. At the time of writing, it
is not known whether the quantile of an inflated distribution is coherent or
not, in general. The proof is nontrivial. In practice, we bypass the problem
by using ETL (instead of quantile) for buVaR; it is then always coherent.
The idea of manipulating the sample distribution is not new. Acerbi
(2002) introduced the spectral risk measure (SRM), a generalization of the
expected shortfall idea. The return sample is first ordered; then each
observation (each a quantile) is multiplied by a weighting function:
SRM ¼
Z 1
0
wðuÞLðuÞdu
ð13:10Þ
where L(u) is the u-quantile loss (i.e., L(1) is the worst loss) and w(u) is a
weighting function that strictly increases over [0,1] and integrates (sums up) to
one. The expected shortfall is a special case of an SRM where the weights are
equal above a certain quantile α (say α=0.99, below α, the weights are zero).
BuVaR is not an SRM in a strict sense, since the weights are equal and larger
than one; they are also dynamically dependent on the cycle of the market.
Clearly, the prospect of manipulating the weights to take into account external
factors that drive risks is a promising development in risk measurement.
13.8 THE SCOPE OF BuVaR
It is an appealing notion to apply buVaR to all asset classes, but empirical
studies show that buVaR is not suitable for risk factors that are relatively
stationary. This is not surprising because buVaR is designed to pick up cycles
and trends. If the price series is range bound in a stationary way, the rank
filtering process will still produce a cycle measure (the bubble) where no
cycle exists. In other words, buVaR will inflate the risk measure even though
there is no market bubble.
Different risk factor families show distinct market characteristics—not
all fall under the scope of the buVaR methodology.
Price-Based Risk Factors
Price-based risk factors originate from markets that trade using prices, for
example, equity, index futures, commodities, and foreign exchange markets.
Characteristically, prices do not trade below zero; they show long-term
264
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

trends and growth cycles. The leverage effect is often apparent, and retra-
cements from a growth trend tend to be sharp and sudden. Higher prices
tend to be more volatile; this level effect makes it necessary to take log
returns for modeling. BuVaR can be used here.
Rates-Based Risk Factors
These originate from markets that trade using yield quotation, such as
interest rates and fixed-income markets. Such risk factors have growth cycles
that follow the interest rate policy cycles of central banks. Their time series
rarely go below zero and often show long-term mean reversion. BuVaR can
be used here as well.
Credit Spread Risk Factors
These are spread quotations (in basis points) from credit derivative and
corporate bond markets. They tend to be range bound and (in theory) mean
reverting with the business (credit) cycle. But, because of the risk of issuer
default, credit spreads can become mean fleeing once an issuer is under credit
stress. This credit escalation phenomenon is often quick and asymmetrical
(spread will spike upward only). If the issuer defaults, the spread becomes
discontinued; otherwise, the spread drifts back to a range-bound regime
after some time. Such a complex behavior cannot be adequately captured by
buVaR. In the next chapter, we will introduce credit buVaR, an extension of
the basic buVaR method, which can incorporate both continuous spread
risk and default risk.
Volatility-Based Risk Factors
These are implied volatilities quoted (traded) in option markets including
volatility indices (such as the VIX index). They tend to be highly stationary
with occasional upward spikes and volatility clustering whenever there is
unexpected news. Without trends, nor cycles, there is no need for buVaR—
using conventional VaR (or setting inflator as 1) is sufficient.
13.9 HOW GOOD IS THE BuVaR BUFFER?
Is a capital buffer based on buVaR adequate in covering extreme fat-tail
losses? To answer this question, we need to look at the type of market losses
that could threaten the solvency of a bank. Consider a simple back-test chart
for the Hang Seng index as per Figure 13.15. The vertical lines show the
Market BuVaR
265

number and extent of VaR exceedences of long and short positions using a
1-day return as PL.
Using 1-day PL produces results that are symmetrical—breaches happen
often in both directions—during a crash, even single-day bounces after a sell-
off often breach the VaR. However, risk managers should be more con-
cerned about persistent losses (over many days) or drawdowns, which could
wipe out a bank. Figure 13.16 shows that using a 10-day rolling return is
more appropriate in modeling drawdowns (the VAR is also scaled to 10-
days for consistency). Notice the VaR breaches are more intuitive—during a
crash, drawdowns exceeding VaR occur more often and with larger mag-
nitudes for long positions.
We can use this idea to check if a buVaR buffer adequately covers
drawdowns during major crises; 10-day returns are used to back test against
ETL (scaled to 10-days), the breaches are plotted alongside the buVaR in
Figure 13.17. Note that during the Asian crisis (1997) and credit crisis
(2007), the two sharp drawdowns are about twice the ETL! BuVaR is able to
cover these exceedences and impose a buffer months ahead of the crisis. The
buVaR buffer is also in the same direction as the drawdown, recognizing
the asymmetry of risk.
The above analysis suggests another way to calibrate the parameters of
buVaR. A supervisor may choose an upper bound Ψ and parameter ω2 such
–10,000
–5,000
0
5,000
10,000
15,000
20,000
25,000
30,000
–15%
–5%
5%
15%
25%
35%
45%
Jan-96
Jan-98
Jan-00
Jan-02
Jan-04
Jan-06
Jan-08
Jan-10
VaR (Long)
VaR (Short)
Breaks
Hang Seng index
VaR (% loss)
Price Level
FIGURE 13.15
Back-Test Exceedences Based on a One-Day Return versus One Day
VaR
266
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

–10,000
–5,000
0
5,000
10,000
15,000
20,000
25,000
30,000
–45%
–25%
5%
25%
45%
65%
85%
105%
Jan-96
Jan-98
Jan-00
Jan-02
Jan-04
Jan-06
Jan-08
Jan-10
VaR (Long)
VaR (Short)
Breaks
Hang Seng index
VaR (% loss)
Price Level
FIGURE 13.16
Back-Test Exceedences Based on a 10-Day (Rolling) Return versus
10-Day (Scaled) VaR
–20,000
–15,000
–10,000
–5,000
0
5,000
10,000
15,000
20,000
25,000
30,000
–40%
–20%
0%
20%
40%
60%
80%
100%
120%
140%
Jan-96
Jan-98
Jan-00
Jan-02
Jan-04
Jan-06
Jan-08
Jan-10
ETL (Long)
ETL (Short)
buVar(Long)
buVar(Short)
Breaks
Hang Seng index
Price Level
VaR (% loss)
FIGURE 13.17
buVaR as a Buffer for Extreme Tail Events
Market BuVaR
267

that past extreme events are sufficiently covered by buVaR (as in Figure
13.17). This does not guarantee, however, that an extreme event many times
larger than buVaR cannot occur in the future. Modeling Black Swans is an
oxymoron; there will always be a surprise element in risk modeling. The goal
is not to have a capital buffer so large that it will encompass all Black Swans
(that would be too uneconomical), but to have a reasonable cushion that
works most of the time. The method in this section provides a tool for a
supervisor to adjust this cushion based on his tolerance.
13.10 THE BRAVE NEW WORLD
The difﬁculty lies, not in the new ideas, but in escaping from the old
ones . . . .
—John Maynard Keynes, in the preface to The General Theory of
Employment, Interest and Money
Hidden Conditions
Models are created by human beings in an attempt to understand the world
we live in. Especially in social sciences, models are often mathematical
abstractions laden with simplifying assumptions. Such assumptions are
necessary pillars upon which knowledge can be built. Many “facts” that we
take for granted are nothing more than implicit assumptions.
For example, if someone asks what is the probability of getting heads in
a coin toss experiment. The natural answer (unless you are overly suspicious)
is 50%. Yet this implicitly assumes that you are under the influence of
gravity. The answer will be indeterminate if the experiment is conducted in
outer space. So perhaps the real question should be—what is the probability
conditional on the experiment being done on earth.
Lest you consider that example pedantic, let’s look at another—
assuming the average life span of an average human male is 75 years. If a
60-year-old man asks what his life expectancy is, one will be tempted to say
15 years. But what if a 74-year-old asks the same question? One year to live?
One cannot arrive at the right answer by asking the wrong question. The
correct question should be—what is his life expectancy conditional on living
up to 74 years. This statistic is found in the conditional distribution of the
lifespan of men who had survived for 74 years. In other words, those who
passed away before age 74 are excluded from this subsample.
There really are two distributions here—a distribution for everyone, and
a distribution for 74-year-olds. To answer the question correctly we have to
268
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

pick the latter. Likewise, one can think of VaR as having two distributions
too—one for normal times (which banks use), and one for stressful times
(which lacks observations). Suppose we have 1,000 data points, and suppose
we define the worst 5% of returns (50 points) as belonging to a stressed
regime. If the 10th-worst loss is USD20 million, we will be tempted to think
that this is the minimum loss with 1% probability (99% VaR). But if we
knew that we were in the stressed regime and use the latter distribution, the
odds are really 20%! If we think in terms of dual regimes, we can begin to
see why crises happen more often than suggested by Table 7.1—because we
have been using the unconditional distribution.9
There are many more hidden assumptions in the financial world that
have been brought to light by crises. The time-honored assumption of nor-
mality widely accepted in financial modeling was debunked by the 2008
crisis. Other examples include the assumption of cointegration in statistical
arbitrage (LTCM debacle), the assumption that major markets are liquid
enough to weather a bank failure (credit crunch), the assumption that the
government will defend its currency (pound devaluation) or guarantee a
bank (Lehman bankruptcy), the assumption that the market is arbitrage free
and will correct any misalignments, the assumption that many companies
cannot default at the same time (CDO debacle), the assumption that a
government will not default on local currency debt (Russian debt crisis); the
list goes on. It is a truism in risk management that what you do not see
(hidden assumptions) will hurt you the most.
VaR is loaded with assumptions too. Often its probability is uncondi-
tional. What is called conditional probability in the Generalized Auto-
regressive Conditional Heteroscedasticity (GARCH) literature really means
conditional on past returns (in an autoregressive sense) and that it assumes
past returns contain all information as suggested by the efficient market
hypothesis. Quantile regression is an important foray into introducing real
conditional probabilities. As modeled in CAViAR, the quantile itself is made
a function of (or conditional on) other variables. BuVaR is another initiative
in conditional models—the return distribution is made a function of the
market cycle directly.
Beyond i.i.d.
In Parts One and Two of this book, we have seen how pervasive the
assumption of i.i.d. is in time series modeling. Alongside the requirement of
stationarity and the assumption of normality, this Gang of Three lay the
foundation of risk modeling. These simplistic assumptions are well estab-
lished because they allow for consistent and precise estimation using known
techniques such as ordinary least squares (OLS) and maximum likelihood
Market BuVaR
269

estimation (MLE). While mathematical tractability is desirable, in VaR we
are really interested in quantile extremes, the tail observations. All evidence
suggests that the assumptions break down during times of crisis when VaR is
most needed. In effect, we are sacrificing accuracy for precision.
Perhaps it is time to go beyond the i.i.d. paradigm. This idea is not new.
Traders and investment analysts have long used methods that are seen as not
very consistent and precise by mathematicians’ standards. Yet, tools such as
technical analysis and quant trading (more recently algo trading) have
attracted a huge following. In the most capitalistic sense, such profit-driven
research is less hampered by strict statistical requirements.
In this chapter, we proposed the new interpretation of decomposition,
which isolates the cyclical (non i.i.d.) component of a time series and uses
this information to correct known weaknesses in conventional VaR. In
hindsight, it seems questionable that a stationary time series, detrended and
stripped of any directional information, can be of any value in risk moni-
toring of extreme market situations. We hope that buVaR will open up a
new path for future research, one that is admittedly less precise, but hope-
fully more accurate.
Living with Extremistan
The realization of hidden conditions and the loosening of simplistic model
assumptions will challenge the way we model risks. In particular, we have to
admit how much we don’t yet know about extreme events. A third element
that adds to the difficulty is the extremistan behavior of financial markets.
The extremistan philosophy says that the tail is inherently unknowable.
It’s impossible to model the tail of market distributions because of data
scarcity and extremistan; there is just no typicality in the tail that is amenable
to statistical quantification. How can we quantify risks that cannot be per-
ceived? The extremistan message is this: Stop being fooled by randomness, it
cannot be modelled, learn to live with extremistan by protecting yourself.
Perhaps the right question then is this: Is it possible to control risks
without modeling the tail with precision? Taleb’s fourth quadrant is an
example of how one can broadly identify positions that are vulnerable to
extremistan and take evasive actions, say by strategically hedging the tail.
In the same spirit, buVaR is not a solution to the fat-tail problem. It
broadly identifies the phase of the business cycle we are in, and penalizes the
VaR in a countercyclical way in recognition that a crash can only happen
downwards. It acts as a compensating control so that regulators can have a
sensible and workable metric for minimum capital. The added capital buffer
compensates for what we do not know about fat-tail phenomena. Until the
270
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

day we solve the extremistan conundrum, a practical paradigm is perhaps
the most prudent approach.
13.11 SPREADSHEET EXERCISES
13.1 The new interpretation of classical decomposition breaks down a time
series into the trend, the cycle, and the noise for analysis, and posits
that cycle breaks and compressions could explain well known market
behaviors such as fat-tailness, leverage effects, and clustering. Aim:
illustrates the new interpretation using a stylized time series. Action:
Use the scroll bars to manipulate the period and the amplitude of the
cycle compression. Press the button to randomly simulate cycle breaks.
Note: It is possible to generate price series of any patterns using the
decomposition concept. According to Fourier theory, any signal can be
decomposed into a series of sine and cosine waves of different wave-
lengths and amplitudes. This has widespread application in science
and engineering.
13.2 Central to the buVaR idea is the inflator, which functions to adjust the
distribution. The response function, equation (13.6), converts the
measured bubble Bt to the inflator Δ. Aim: A toy calculator that plots
the response function. Action: Use the scroll bar to adjust parameter
ω2 and observe the behavior of the plot. Which ω2 setting gives a
reasonable response function in your opinion and why?
13.3 It is interesting to compare three tail-risk measures—ETL, hsVaR, and
pVaR—in terms of coherence, responsiveness, and stability. These are
desirable properties of a risk metric. Aim: A Monte Carlo method is
used to simulate a regime break (a drastic change in correlation) in the
times series of two assets. The implied correlation is plotted for
analysis. Action: Press F9 many times to simulate many scenarios. Are
you convinced ETL is a superior measure?
13.4 BuVaR is a new risk metric introduced in this book. Aim: The
spreadsheet is a prototype calculator for buVaR, which illustrates
various aspects of its computation. For comparison and analysis,
expected tail loss (ETL) is also plotted on the same chart.
NOTES
1. This may not be as unreasonable as it seems. Consider the Dow Jones index, our
maximum will then be a 25% one-day drop (the Black Monday crash in 1987).
This is a very large number and given the circuit breakers put in place nowadays,
Market BuVaR
271

single-day losses in excess of 10% are highly unlikely (unless the shock is of a
nonmarket nature such as world wars, but that will be technically considered an
operational risk). For this reason, we shall deﬁne a reasonably estimated upper
bound for buVaR later.
2. Conditional variance is not a new idea; it is just that it has never been made
conditional on past price levels.
3. For a complete technical description, please read, M. Wong. “Market BuVaR: A
Countercyclical Risk Metric,” Journal of Risk Management in Financial Insti-
tutions 4, no. 4 (2011): 1–14.
4. Arguably, we can also apply curve-ﬁtting to determine this equilibrium, but this is
less tractable—you will need to reestimate daily, and the ﬁtted function is non-
unique. Furthermore, the risk industry is already familiar with using rolling
windows.
5. Rank ﬁlters are commonly used in digital image processing to enhance digital
information. The method is a special case of a rank convolution process that
has found wide application in technology and computer science. The choice of
8% cutoff is based on empirical studies. It is likely that the best cutoff is
slightly different for various asset classes, but would not be signiﬁcantly dif-
ferent from 8%.
6. Circuit breakers are safeguards introduced by exchanges after the 1987 crash.
Once a circuit breaker is triggered, trading is halted for the day. Using the Dow
Jones index as an example, the average is 9.92%, and the initial circuit breaker
instituted by the NYSE is 10%. So Ψ ¼ MIN(9.92%, 10%) ¼ 9.92%.
7. There will a (Bmax, Ψ)i for every risk factor i. For securities that are newly traded
and for which there is not enough history to meaningfully determine (Bmax, Ψ),
we can benchmark to an index or proxy.
8. One can debate whether the normal distribution assumption behind this scaling is
justiﬁed. In the absence of adequate knowledge of the tail for extrapolation, we
choose simplicity. It is after all nearly impossible to estimate the expected loss at
the extremes with pinpoint accuracy.
9. The example is admittedly contrived—there is obvious uncertainty in deciding
which regime we are in, and neither is there adequate data on conditional
probability. But, the example does highlight the reality of conditional probability
and the folly of making careless probability interpretations. Interestingly,
Satchkov (2010) introduced a VaR system that uses dual regimes, and weights
that depend on external risk indicators.
272
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

CHAPTER 14
Credit BuVaR
T
he basic buVaR idea cannot be used in the credit world because trends
and cycles are not well behaved. Credit spreads often trade in a range
during times of normalcy, determined by an issuer’s cost of funds and rat-
ings. Then, upon credit stress, the spreads break out and become mean-
fleeing. This rare breakout behavior throws off the buVaR’s bubble mea-
sure. Fortunately, with a modification to the buVaR framework, its usage
can be extended to the credit world with interesting outcomes.
14.1 THE CREDIT BUBBLE VaR IDEA
Conventional credit spread value at risk (VaR) that uses historical simula-
tion already captures the risk of (continuous) spread movements. On the
other hand, the default risk (a discontinuous jump risk) is separately mod-
eled using default migration models such as CreditMetrics, which use rating
migration transition matrix and actual default statistics of companies col-
lected by rating agencies. But, the historically recorded default rates can be a
misleading indicator of future default risk because they are procyclical and
backward-looking, and they critically lag actual defaults.
For example, during the precrisis golden decade (1998–2006) the
default statistics were very low and would underestimate the true default
probability realized during the 2007–2008 credit crunch when credit quality
rapidly deteriorated. And surely the default rates recorded during the credit
crunch period would grossly overstate the future default probability after the
economy has recovered. Credit buVaR overcomes this procyclicality prob-
lem by not requiring default statistics at all.
The credit buVaR model1 is an attempt to merge default risk into credit
spread VaR. The weakness of the current fragmented approach is that there
is no natural way to combine migrational credit VaR, spread VaR, and
273

market VaR in a diversifiable framework.2 But with the credit buVaR
approach, they can be aggregated in the Markowitz sense.3
The intuition of credit buVaR originates from two empirical observa-
tions. First, actual credit defaults/stresses witnessed during the 2008 credit
crisis revealed that defaults are not simply jumps, as modeled in academia.
Figure 14.1 shows a few impaired issuers during the 2008 credit crisis. They
do not look anything like Figure 14.2 which shows a theoretically modeled
jump-to-default process (a stylized time series for spreads of a five-year junk
bond with a 30% default recovery). The phenomenology is patently differ-
ent. Actual defaults are in fact always preceded by rapid spread escalation or
widening (credit escalation), which act as leading signals, often weeks to
months earlier. This observation suggests that a credit model conditional on
spread escalation could be forward-looking.
A second observation is the asymmetry of credit default/deterioration—
spreads can only escalate upwards and a default only hurts long credit risky
positions. Such positions include (long) bonds, short protection default
swaps (CDS), and, generally, other credit-risky products which show neg-
ative credit sensitivity (that is, that incur a loss upon an increase in credit
spreads). This fundamental asymmetry is not reflected by the spread VaR
model, which is just a function of spread return volatility without other
market dynamics. Hence, spread VaR grossly underestimates skew/fat-tail
0
100
200
300
400
500
600
700
800
–
1,000
2,000
3,000
4,000
5,000
6,000
7,000
Jan-06
Apr-06
Jul-06
Oct-06
Jan-07
Apr-07
Jul-07
Oct-07
Jan-08
Apr-08
Jul-08
Oct-08
Jan-09
Apr-09
GMAC (left axis)
AIG (left axis)
Lehman (right axis)
Lehman
bankruptcy
FIGURE 14.1
Spread Escalation of a Few Issuers during Credit Crisis
Source: Bloomberg Finance L.P.
274
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

risks. And since it is calculated using a rolling window, it lags significant
market moves. Unfortunately, the 2008 crisis showed that the majority of
the losses of banks came from trading spread risk (which is underestimated)
rather than actual issuer defaults.
This asymmetry is not only a behavior of default risk, but also of spread
risk. In theory at least, spreads are expected to trade in a range determined
by ratings and with some cyclicality driven by business credit conditions.
This pattern did not show up in recent history because of the overwhelming
influence of product innovation. Figure 14.3 shows the five-year CDS
spreads for the Philippines. Until July 2007, the spreads showed a trending
contraction as the euphoria for high-yielding credit products drove down
spreads generally. This bubble burst when the U.S. subprime crisis happened
and spreads reversed direction sharply. For such issuers that were affected by
the overall market distress, even if they did not eventually default, spreads
became mean-fleeing to the upside.
The credit buVaR model will incorporate both the phenomena of rapid
spread escalation and risk asymmetry. The plan is to detect the occurrence
of spread escalation and to penalize spread VaR asymmetrically using an
adjustment Δþ (the inflator). The adjustment is applied to the positive side of
the return distribution (of spreads) since default risks can only hurt long
credit positions.
0
500
1,000
1,500
2,000
2,500
3,000
3,500
Spread (Bp)
Bond defaults
(30% recovery)
FIGURE 14.2
A Modeled Jump to Default Process
Credit BuVaR
275

14.2 MODEL FORMULATION
To derive the credit buVar inflator, Δþ, we assume a response function of
the form given by equation (14.1). It has some desirable properties:
1. It is floored at 1.0. This is useful as the inflator is supposed to increase a
scenario return, not decrease it.
2. It grows exponentially with spread S. In other words, the penalty should
expand rapidly during a credit escalation to recognize that a default
event is becoming more imminent.
3. The inflator will be capped in a logical way such that as we move into
default, a bond cannot lose more than its full notional (as we shall see).
Δþ ¼ expðω1Sω2Þ
ð14:1Þ
where the þ sign means that only positive return scenarios are adjusted, S is
the current CDS spread, and ω1, ω2 are positive parameters.
Obviously, spreads (unlike stock prices) cannot grow indefinitely—at
some upper bound Scap, the reference issuer will default. One simple way to
determine Scap is to use a bond-pricing formula and assume a recovery rate
of 10% upon default.4 To derive the formula for Scap, first we work out the
0
100
200
300
400
500
600
700
800
900
1,000
Apr-02
Aug-02
Dec-02
Apr-03
Aug-03
Dec-03
Apr-04
Aug-04
Dec-04
Apr-05
Aug-05
Dec-05
Apr-06
Aug-06
Dec-06
Apr-07
Aug-07
Dec-07
Apr-08
Aug-08
Dec-08
Apr-09
Spread (Bp)
FIGURE 14.3
Credit Spread Behavior—Philippines Five-Year CDS
Source: Bloomberg Finance L.P.
276
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

yield of a defaulted bond using the Excel formula YIELD(.). The syntax is:
Ydefaulted ¼ YIELDðCOB, Maturity, Coupon, Price, Redemption,
FrequencyÞ
ð14:2Þ
where the bracketed items are the inputs for the function. Equation (14.2) is
for a specific issuer at a particular maturity (T)—it represents one credit
spread risk factor. In credit trading, most banks use up to five benchmark
risk factors on each credit curve for liquid issuers; that is, T ¼ 1, 2, 3, 5, 7.
Often, the five-year point is the most liquid point observable and other tenor
points are approximated.
Our primary interest is in designing an inflator that is representative of
the state of deterioration of a particular credit name. Thus, it is superfluous
to model the credit curve, and it suffices to choose a single benchmark,
naturally at T ¼ 5, since this is the most reliable point. The computed
inflator is then used to penalize all five risk factors of that issuer’s curve.
With the chosen benchmark the inputs for equation (14.2) are:
1. COB ¼ close-of-business (today’s) date
2. Maturity ¼ COB þ 5 years
3. Coupon ¼ 5-year swap rate. This is a convenient assumption since the
inflator result is not very sensitive to the coupon rate.
4. Price ¼ 10, the assumed recovery price of 10% par.
5. Redemption ¼ 100 or par.
6. Frequency ¼ 4, or quarterly coupon, consistent with the convention
used in the CDS market.
Using these inputs, when a default occurs, the price will be floored at
10% of par, and its yield will be capped by equation (14.2). A credit spread
is often defined as the yield premium over the risk-free rate, thus, it will be
capped at:
Scap ¼ Ydefaulted  5-year swap rate
ð14:3Þ
Scap will put a natural cap on the inflator in equation (14.1). We define
the maximum inflator Δmax to be the adjustment that will inflate the VaR (or
two sigma)5 to a loss at Scap. Mathematically,
Scap ¼ Sð1 þ 2σΔmaxÞ
ð14:4Þ
Credit BuVaR
277

where the standard deviation of returns σ is taken over a 250-day window.
This idea is illustrated in Figure 14.4. The original spread distribution is
inflated to the right by Δ (Δmax) in general, and the largest possible
inflation is Δmax shown in the schematic.
However, the maximum shift will also satisfy (14.1), hence:
Δmax ¼ expðω1Sω2
capÞ
ð14:5Þ
Substituting ω1 from equation (14.5) into equation (14.1) and then
eliminating Δmax using (14.4), we obtain:
Δþ ¼ exp
S
Scap

ω2
ln ðScap
S  1Þ=ð2σÞ


	

ð14:6Þ
We now have an equation with one parameter ω2 that lets the risk con-
troller tune the sensitivity of the response function. Setting a lower ω2 makes
the credit buVaR more conservative or punitive in terms of regulatory capital.
14.3 BEHAVIOR OF RESPONSE FUNCTION
As an example, consider a five-year corporate bond position and use the
standard parameters mentioned earlier. Scap works out to be around 5,800bp
Credit spread
distribution
(2σ)Δmax
2σ
FIGURE 14.4
Inﬂator Penalizing the Credit Spread Distribution on the Positive Side
278
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

by equation (14.3). Figure 14.5 plots the response function (14.6) using
ω2 ¼ 1.2 and σ ¼ 8% for two different volatilities.
The response profile reaches its peak when spreads are at about half of
Scap after which the inflator would fall. The hump can be explained this way:
At higher spreads close to Scap, the inflator has to reduce in order to prevent
the inflated spread (representing VaR) from overshooting Scap. In theory, the
bond defaults when Scap is reached and there is no longer any price uncer-
tainty, hence, VaR should become zero.
Note that the inflator, equation (14.6), is independent of the issuer; it is
universal for all issuers with the same spread and volatility. This is appeal-
ing, as it implies that all information on default expectations is already
contained in the spread, consistent with the efficient market hypothesis.
While we have set ω2 ¼ 1.2 for illustration, we will argue for a more con-
servative choice of ω2 in Chapter 15.
The Δþ inflator will multiply only the positive returns in the scenario
vector (or distribution). The incremental return is interpreted as coming
from default risk (see Section 14.5). We use this inflated scenario vector to
perform full revaluation of the credit portfolio to arrive at the PL vector.
Credit buVaR is formally defined as the expected tail loss taken on this
inflated PL vector over a one-day horizon.
Figure 14.6 illustrates the effect of the inflator on the VaR scenario. The
horizontal axis is the current spread level (S), and the vertical axis shows
–0.5
0.0
0.5
1.0
1.5
2.0
2.5
0
1,000
2,000
3,000
4,000
5,000
6,000
7,000
Inflator
Credit spread (bp)
1 sigma
3 sigma
Hump
Scap
VaR becomes
zero
FIGURE 14.5
Response Function Where Swap Rate ¼ 2.6%, ω2 ¼ 1.2, Volatilities
1σ and 3σ
Credit BuVaR
279

the shifted spread level S(1þ2σΔþ) after multiplying the VaR scenario (2σ)
and the inflator Δþ. Here σ is constant. The shifted scenario (2σΔþ) is the
scenario that produces the credit buVaR. The distance between the curve
and the reference straight line is the degree of inflation applied to VaR; it
shrinks to zero just as the bond defaults at 5,800bp.
14.4 CHARACTERISTICS OF CREDIT BuVaR
Let’s compare credit buVaR with conventional spread VaR. For illustration
we will take credit buVaR as equal to 2σΔþ, in other words, as a simple
inflation of parametric VaR. (In actual implementation, credit buVaR must
be computed by taking the expected tail loss of the inflated PL vector.)
Figure 14.7 shows the credit buVaR calculated for a short-protection
CDS on the Philippines. Because it is a long credit position, the buVaR is an
inflated copy of the spread VaR (the shaded graph). If it is for a short credit
position (or long-protection) no adjustment is needed; buVaR and spread
VaR are identical. See Spreadsheet 14.1 for a worked-out example.
–
1,000
2,000
3,000
4,000
5,000
6,000
0
1,000
2,000
3,000
4,000
5,000
6,000
Shifted spread (bp)
Current spread (bp)
FIGURE 14.6
Spread Level after Being Shifted: S(1 þ 2σΔþ) versus S
280
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

It is important that credit buVaR is robust to the assumptions made in
its derivation. Notably, Scap is a function of only the five-year swap rate
in equation (14.3); all other parameters are assumed. It turns out the
assumption on the coupon rate is not a sensitive one as long as the coupon is
not significantly larger (say 5% more) than the swap rate. A bond is typically
issued by a company with a coupon level set near its yield (the company’s
cost of funds), and bond yields seldom trade far above swap rates during
issuance.6 Hence, assuming the bond’s coupon is equal to the swap rate is a
reasonable standardization. It will not be a problem even for old bonds
where coupons were fixed long ago (possibly at levels different from current
yields). This is because banks use new bonds (on-the-run issues) for risk
factor mapping, never old ones.
In contrast, Scap is sensitive to the recovery rate assumption, especially if
under 10%. From Figure 14.8, a very low recovery (< 5%) will cause the hump
in the response function profile to shift to the right significantly. Fortunately,
this is less of a problem in practice—if the price of a bond is impaired to below
40%—a typical recovery rate assumed for CDS valuation—banks will most
likely take the deal off the trading book and take a full reserve on the loss. The
position will then be managed outside the VaR framework as a bad loan or
asset under receivership.
But suppose that a bank chooses to trade the impaired asset on the
trading book (such activity is known as distress debt trading) and the market
quotes extremely high spreads in anticipation of a default with very low
–10%
0%
10%
20%
30%
40%
50%
60%
70%
(200)
(100)
–
100
200
300
400
500
600
700
Apr-02
Apr-03
Apr-04
Apr-05
Apr-06
Apr-07
Apr-08
Apr-09
VaR (%Loss)
Spread (bp)
Spread pVaR
Philippines 5Y CDS
Credit buVaR(Longs)
FIGURE 14.7
Credit buVaR versus Parametric VaR for Five-Year Philippines CDS
Source: Bloomberg Finance L.P.
Credit BuVaR
281

recovery (less than our assumed 10%), then buVaR will give zero (a dis-
continuity) since the bond has in theory defaulted with no further price
uncertainty. This is shown as the thick line in the GM example (see Figure
14.9). To continue using buVaR, the bank can simply reset the recovery
rate lower to say 1% and recompute buVaR as per the thin line. This
bypasses the discontinuity problem.
In summary, credit buVaR becomes larger than spread VaR whenever
there is a credit escalation, but its increment is controlled so that a position
cannot lose more than its principal (less recovery value).
14.5 INTERPRETATION OF CREDIT BuVaR
We posit that the additional portion of buVaR over and above conventional
spread VaR represents default risk. To see this, we simulate a credit spread
time series and let this spread increase gradually until the corresponding five-
year bond defaults. See Figure 14.10.
Note that the spread VaR is small, around 0.5% (this is daily, not
annualized) compared to the inflated buVaR. At its peak (point A), buVaR
of 10% is 20 times larger than spread VaR. With the prevailing spread
at 4,700, the buVaR will bring the shifted spread to 4,700  (1 þ 0.10)
¼ 5,170, quite close to the default spread Scap ¼ 5,790 for our five-year
–
5,000
10,000
15,000
20,000
25,000
0
10
20
30
40
50
Cap on (shifted) spread
Recovery rate assumption
FIGURE 14.8
Sensitivity of Scap to Recovery Rate Assumption
282
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

bond example.7 As the spread escalates to 5,500 (point B), the buVar goes
down to 4.4% such that the shifted spread is 5,500  (1þ 0.044) ¼ 5,742,
almost reaching default. In short, as credit spread escalates, buVaR (while
falling) inflates the shifted spread toward the theoretical default spread Scap.
–
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1.0 
(6,000)
(4,000)
(2,000)
–
2,000
4,000
6,000
8,000
10,000
12,000
14,000
16,000
18,000
20,000
Apr-02
Apr-03
Apr-04
Apr-05
Apr-06
Apr-07
Apr-08
Apr-09
Spread pVaR
GM
Credit buVaR
Credit buVaR (impaired asset)
Impaired asset.
Switch to recovery
rate 1%
FIGURE 14.9
General Motors Five-Year CDS (long credit) Position: Handling of
buVaR Discontinuity
Source: Bloomberg Finance L.P.
0%
2%
4%
6%
8%
10%
12%
14%
16%
18%
20%
–
1,000
2,000
3,000
4,000
5,000
6,000
7,000
Nov-02
Nov-03
Nov-04
Nov-05
Nov-06
Nov-07
Nov-08
VaR(% Loss)
Spread (bp)
Spread pVaR
Simulated Series
Credit buVaR
A
B
Bond defaults
FIGURE 14.10
Stylized Credit buVaR: Approaching Default
Credit BuVaR
283

To get a sense of magnitude, consider a $10 million bond of five-year
duration. At point A, the VaR loss is (5,170  4,700) ¼ 470bp. In dollar
terms,
the
buVaR
works
out
to
be
10,000,000/10,000  5  470 ¼
$2,350,000, compared to conventional spread VaR of 0.5%  4,700 
10,000,000/10,000  5 ¼ $117,000 that is due purely to volatility of
spreads. The additional $2,232,500 loss in buVaR is attributed to default
risk. In Section 15.4, we find further justification for this interpretation.
14.6 SPREADSHEET EXERCISES
14.1 Credit buVaR is a new risk metric introduced in this book. Aim: The
spreadsheet is a prototype calculator for credit buVaR, which illus-
trates various aspects of its computation. For comparison and analy-
sis, the pVaR is also plotted on the same chart. Note: for simple
illustration, we simply inflate the pVaR for this single risk factor
example. At a portfolio level, expected tail loss (ETL) should be used
instead. This means that each issuer’s inflator is different and is mul-
tiplied to respective deals for all scenarios. Then, sum the PL by sce-
narios to get a diversified PL vector. Finally, ETL can be taken on this
PL distribution.
NOTES
1. M. Wong. “Credit BuVaR: Asymmetric Spread VaR with Default,” Journal of
Risk Management in Financial Institutions 5, no. 1(2011): 86–95.
2. In the context of the Basel III market risk capital, these are the IRC, speciﬁc risk
charge, and general market risk charge respectively.
3. In Basel III, we have the dilemma of adding a 99.9%/1-year IRC to a 99%/10-
day VaR. By contrast, credit buVaR captures both default and spread risks
together using the same scale. It is essentially a short-term horizon model that is
sensitized to also detect default risk in a forward-looking way.
4. In practice, the regulator can stipulate a suitable recovery rate. Here we took
guidance from Lehman’s recovery value of 8.6% during its bankruptcy and
rounded it to 10%.
5. We take a 2 standard deviation of returns as representing 97.5% VaR because it
is more stable and easier to compute than other quantile measures.
6. The exception is when the issuer is in distress, but under this circumstance it will
not be able to issue a bond anyway.
7. Our stylized bond is a ﬁve-year bond with semiannual coupon of 2.57%, default
recovery rate of 10%, and swap rate at 2.57%.
284
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

CHAPTER 15
Acceptance Tests
T
he VaR models used in banks usually undergo the strict statistical tests
found in Chapter 8. These goodness of model tests are a regulatory
requirement. Passing all these tests proves that a bank’s value at risk (VaR)
distribution is independent and identically distributed (i.i.d.) and hence the
quantile estimate is not biased or understated. Under these peacetime con-
ditions, the VaR model is a consistent metric. The great irony is that a
risk manager should be more concerned about extreme conditions, which
threaten the bank’s survival. But these are the exact conditions under
which such statistical tests would fail.
Unsurprisingly, buVaR will fail these tests even under benign conditions
since, by design, this metric is non-i.i.d. (its day-to-day distribution is con-
ditional on the cycle). For example, one can apply standard statistical tests
on the (inflated) buVaR return time series and show that it is not i.i.d.
Without the convenience of i.i.d., we will need to rely on other less con-
sistent (and less precise) tests. We will test the effectiveness of buVaR by
assessing its historical performance covering major financial assets and
indices, over a long history and over specific stressful episodes.
15.1 BuVaR VISUAL CHECKS
Our test is similar to the way a trader would test a trading system—it is
subjective, performance-based, and not based on strict hypothesis testing.
First, and perhaps most importantly, we check visually to ensure that buVaR
is generally doing what it is designed to perform. In particular, we look for
evidence of:
1. Countercyclicality: The bubble measure must inflate to penalize posi-
tions of the correct side. Ideally, it should penalize the long side most at
price peaks and penalize the short side most at price troughs.
285

2. If a rally becomes extended, the bubble should ease off. The adaptive
moving average is designed to produce this behavior, which softens the
penalty on long-term growth.
3. During a crash, the bubble should continue to penalize the long side the
whole time instead of changing direction.
The test coverage as shown in Table 15.1 is chosen to broadly represent
global financial markets. The buVaR is then computed with parameters as
described in Chapter 13. Refer to Spreadsheet 13.4 for a worked-out exam-
ple. The bubble measure and the resulting buVaR are plotted for analysis. We
also chart the expected tail loss (ETL) as a base case for comparison.
Figure 15.1 shows the bubble measure for the S&P 500 Index. The
bubble peaked at the end of 1995, but it turned out to be a false signal—
the rally developed into a long-term uptrend that lasted until mid-2000.
Due to the adaptive nature of the bubble, it eased off and maintained a
moderately positive level up to 1999 (see arrow). Generally, the bubble
peaked and troughed in tandem with the index (see circles in Figure 15.1).
Figure 15.2 shows the buVaR and the ETL of S&P 500 index. Using our
definition of upper bound ψ, the buVaR’s peak during the run-up to the
2008 credit crisis is slightly higher than the ETL’s peak during that crisis. As
seen, buVaR leads ETL (or any conventional VaR measure) significantly and
peaks almost one year ahead of the crash.
Figure 15.3 shows the bubble measure for the Nasdaq 100 index. The
peak/trough of buVaR corresponded to the peak/trough of the index.
The Nasdaq crash in 2000 was extremely sharp; had we used a simple
TABLE 15.1
Assets Chosen for Visual Checks
Asset
Asset Class
Benchmark Representation
S&P 500 Index
Equity (U.S.)
Global equity market driver
Nasdaq 100 Index
Equity (U.S.)
Technology sector
Nikkei 225 Index
Equity (Japan)
Asia equity
Hang Seng Index
Equity (HK)
Greater China
USD/JPY
Forex
Major currency
AUD/SGD
Forex
A popular cross currency
USD 10-year swap
Interest rates
Long-term global rates
USD 5-year swap
Interest rates
Medium-term global rates
USD 3-month Libor
Interest rates
Short-term global rates
Gold spot
Commodity
Inﬂation hedge, metals
Brent crude oil futures
Commodity
Energy
286
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

–30%
–10%
10%
30%
50%
70%
90%
110%
–
200
400
600
800
1,000
1,200
1,400
1,600
1,800
Oct-94
Oct-96
Oct-98
Oct-00
Oct-02
Oct-04
Oct-06
Oct-08
Bubble
S&P 500 index
Price Level
Distance(%)
FIGURE 15.1
Bubble Measure for S&P 500 Index
–
200
400
600
800
1,000
1,200
1,400
1,600
–10%
–5%
0%
5%
10%
15%
20%
25%
30%
35%
40%
14-Oct-94
14-Oct-97
14-Oct-00
14-Oct-03
14-Oct-06
14-Oct-09
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
S&P 500 index
Price Level
VaR (% loss)
FIGURE 15.2
BuVaR and ETL for S&P 500 Index
Acceptance Tests
287

moving average to construct the inflator, the bubble meter would have
flipped to a negative reading when the price collapse reached 50% of the full
crash amount, and buVaR would have wrongly penalized short positions
instead. Figure 15.4 shows the resulting buVaR for Nasdaq.
Figure 15.5 shows the bubble measure for the Nikkei 225 index. The
bubble significantly penalized positions in a countercyclical way at major
peaks and troughs (see circles in Figure 15.5). This is less obvious in the
buVaR chart in Figure 15.6 because the level of the ETL was ultralow in
1989, which lessened the impact of the inflation.
Figure 15.7 and Figure 15.8 show the bubble measure and buVaR for
the Hang Seng index. The bubble penalized positions in a countercyclical
manner ahead of peaks and troughs (circled). Notice that the bubble did not
penalize every peak and trough, only the major unsustainable ones normally
associated with crashes and manias. The circles correspond to the Asian
crisis and the credit crunch.
Figures 15.9 and 15.10 show the bubble measure and buVaR for the
USD/JPY exchange rate. Major peaks and troughs were strikingly detected
by the bubble measure. The buVaR was highest during the period in 1997–
1998 when the Bank of Japan (BoJ) actively defended the yen against
speculators as a policy.
–50%
0%
50%
100%
150%
200%
250%
300%
(500)
500
1,500
2,500
3,500
4,500
Oct-94
Oct-96
Oct-98
Oct-00
Oct-02
Oct-04
Oct-06
Oct-08
Bubble
Nasdaq index
Price Level
Distance(%)
FIGURE 15.3
Bubble Measure for Nasdaq 100 Index
288
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

–40%
10%
60%
110%
160%
(5,000)
–
5,000
10,000
15,000
20,000
25,000
30,000
35,000
40,000
Oct-84 Oct-87 Oct-90 Oct-93 Oct-96 Oct-99 Oct-02 Oct-05 Oct-08
Bubble
Nikkei index
Price Level
Distance(%)
FIGURE 15.5
Bubble Measure for Nikkei 225 Index
(1,000)
–
1,000
2,000
3,000
4,000
5,000
–10%
0%
10%
20%
30%
40%
50%
Oct-94
Oct-96
Oct-98
Oct-00
Oct-02
Oct-04
Oct-06
Oct-08
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
Nasdaq index
Price Level
VaR (% loss)
FIGURE 15.4
BuVaR and ETL for Nasdaq 100 Index
Acceptance Tests
289

–50%
0%
50%
100%
150%
200%
250%
300%
(5,000)
–
5,000
10,000
15,000
20,000
25,000
30,000
35,000
Oct-94
Oct-96
Oct-98
Oct-00
Oct-02
Oct-04
Oct-06
Oct-08
Bubble
Hang Seng index
Price Level
Distance (%)
FIGURE 15.7
Bubble Measure for Hang Seng Index
(20,000)
(10,000)
–
10,000
20,000
30,000
40,000
–15%
–5%
5%
15%
25%
35%
45%
15-Oct-84
15-Oct-89
15-Oct-94
15-Oct-99
15-Oct-04
15-Oct-09
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
Nikkei index
Price Level
VaR (% loss)
FIGURE 15.6
BuVaR and ETL for Nikkei 225 Index
290
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

(10,000)
(5,000)
–
5,000
10,000
15,000
20,000
25,000
30,000
–10%
0%
10%
20%
30%
40%
50%
Oct-94
Oct-96
Oct-98
Oct-00
Oct-02
Oct-04
Oct-06
Oct-08
Price Level
VaR (% loss)
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
Hang Seng index
FIGURE 15.8
BuVaR and ETL for Hang Seng Index
–20%
0%
20%
40%
60%
80%
100%
40
60
80
100
120
140
Apr-91
Apr-94
Apr-97
Apr-00
Apr-03
Apr-06
Apr-09
Price Level
Distance (%)
Bubble
USD/JPY
FIGURE 15.9
Bubble Measure for the USD/JPY Exchange Rate
Acceptance Tests
291

Due to the exponential function exp(.) in the response function, minor
peaks/troughs in the bubble will translate into immaterial buVaR enhance-
ment, and should not be our focus. Instead, buVaR is designed to catch
major crashes and manias. This is apparent in Figures 15.11 and 15.12,
which show the bubble measure and buVaR for the AUD/SGD exchange
rate. The left circle shows a major bubble, but when it failed to burst,
the measure rapidly tapered off by design (the arrow). The right circle
shows the carry trade bubble finally burst, triggered by the 2008 crisis, and
was forewarned by buVaR.
Swaps and treasury bonds of major currencies tend to be highly corre-
lated because central banks of major countries often coordinate their mon-
etary policies to maintain economic and market stability. Hence, for the
purpose of testing, we can use USD swap rates as a benchmark for global
interest rates. Figures 15.13 and 15.14 show the bubble measure and buVaR
for the 10-year USD swap rate. The major peaks and troughs are circled in
Figure 15.13.
Figures 15.15 and 15.16 show the results for the 5-year USD swap,
which looks similar to that for the 10-year USD swap. This is because the
midsection and long end of the same curve are expected to be highly cor-
related as they are influenced by common risk drivers.
40 
60 
80 
100 
120 
140 
–5%
0%
5%
10%
15%
20%
Oct-94
Oct-96
Oct-98
Oct-00
Oct-02
Oct-04
Oct-06
Oct-08
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
USD/JPY
Price Level
VaR (% loss)
FIGURE 15.10
BuVaR and ETL for the USD/JPY Exchange Rate
Source: Bloomberg Finance L.P.
292
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

–20%
0%
20%
40%
60%
80%
100%
0.50
0.60
0.70
0.80
0.90
1.00
1.10
1.20
1.30
1.40
1.50
Dec-90
Dec-93
Dec-96
Dec-99
Dec-02
Dec-05
Dec-08
Price Level
Distance (%)
Bubble
AUD/SGD
FIGURE 15.11
Bubble Measure for AUD/SGD Exchange Rate
Source: Bloomberg Finance L.P.
0.4
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3
1.4
–10%
–5%
0%
5%
10%
15%
20%
25%
30%
Dec-90
Dec-93
Dec-96
Dec-99
Dec-02
Dec-05
Dec-08
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
AUD/SGD
Price Level
% Tail loss
FIGURE 15.12
BuVaR and ETL for AUD/SGD Exchange Rate
Source: Bloomberg Finance L.P.
Acceptance Tests
293

–40%
–20%
0%
20%
40%
60%
80%
100%
120%
140%
0
1
2
3
4
5
6
7
8
9
10
Dec-90
Dec-93
Dec-96
Dec-99
Dec-02
Dec-05
Dec-08
Yield Level
Distance (%)
Bubble
USD 10yr swap
FIGURE 15.13
Bubble Measure for USD 10-Year Swap
Source: Bloomberg Finance L.P.
22
0
2
4
6
8
215%
25%
5%
15%
25%
35%
45%
Dec-90
Dec-93
Dec-96
Dec-99
Dec-02
Dec-05
Dec-08
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
Yield Level
VaR(% loss)
FIGURE 15.14
BuVaR and ETL for USD 10-Year Swap
Source: Bloomberg Finance L.P.
294
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

22
21
0
1
2
3
4
5
6
7
8
220%
210%
0%
10%
20%
30%
40%
50%
60%
Oct-94
Oct-96
Oct-98
Oct-00
Oct-02
Oct-04
Oct-06
Oct-08
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
USD 5yr swap
Yield Level
VaR(% loss)
FIGURE 15.16
BuVaR and ETL for USD 5-Year Swap
Source: Bloomberg Finance L.P.
240%
220%
0%
20%
40%
60%
80%
100%
120%
140%
21
0
1
2
3
4
5
6
7
8
9
Dec-90
Dec-93
Dec-96
Dec-99
Dec-02
Dec-05
Dec-08
Yield Level
Distance (%)
Bubble
US 5yr
swap
FIGURE 15.15
Bubble Measure for USD 5-Year Swap
Source: Bloomberg Finance L.P.
Acceptance Tests
295

Figures 15.17 and 15.18 show the results for the 3-month Libor rate
which, in contrast, does not look at all like the 5-year and 10-year swaps. In
fact, the Libor rate chart looks toothed and irregular because the short end
of the curve (overnight rate to 3-month rate), which acts as an anchor for the
entire yield curve, seldom moves. Central banks control the overnight rates.
The only time short rates really move is when there is an actual (or per-
ceived) central bank rate cut or rate hike. And since such actions usually
come in dosages of 25bp or multiples of it, the short rate time series becomes
toothed. The problem with unsmooth time series is that it throws off
most continuous-time modeling. As such, the bubble measure becomes
ineffective—for example, the peak in Libor rates in Jun 2007 was utterly
missed out by the model.
The solution to this problem is a simple one—since it is inconceivable
that different parts of the same yield curve would be at different business
cycles (after all, they relate to the same economy), we really only need one
single liquid benchmark from the curve for the purpose of deriving the
bubble (which will be used for all tenors). The five-year point is preferred for
two reasons: Most companies raise medium term funds by issuing corporate
bonds in this part of the curve, and default swaps are also the most liquid for
five years.
250%
0%
50%
100%
150%
200%
250%
300%
(2)
(1)
2
1
2
3
4
5
6
7
Oct-94
Oct-96
Oct-98
Oct-00
Oct-02
Oct-04
Oct-06
Oct-08
Bubble
USD 3mo Libor
Yield Level
Distance (%)
FIGURE 15.17
Bubble Measure for USD 3-Month Libor
Source: Bloomberg Finance L.P.
296
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

Figure 15.19 shows the bubble measure for the gold spot price (in USD).
BuVaR did well by penalizing the longs markedly on three occasions (see
Figure 15.20) during the bull run, but when the rally proved unstoppable,
the buVaR subsided temporarily.
Figures 15.21 and 15.22 show the bubble and buVaR for Brent crude oil
futures (of the near month contract). Remarkably, the bubble measure
preempted every major oil price spike in recent history (circled in Figure
15.21).
15.2 BuVaR EVENT TIMING TESTS
BuVaR is designed to anticipate market bubbles and make provision against
an impending crash. In this section, we look at major crises and extreme
events in the history of financial markets, to test if buVaR was able to penalize
players ahead of the crash. The test is deemed successful if the bubble measure
inflates and buVaR peaks before the crash occurs. The infamous events in
Table 15.2 are tested.
Figure 15.23 shows the buVaR of the S&P 500 index during the 2008
credit crunch. It is not easy to pinpoint exactly when the crash formally
(5)
(3)
(1)
1 
3 
5 
7 
210%
25%
0%
5%
10%
15%
20%
25%
30%
35%
40%
Oct-94
Oct-96
Oct-98
Oct-00
Oct-02
Oct-04
Oct-06
Oct-08
-
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
USD 3mo Libor
Yield Level
VaR(% loss)
FIGURE 15.18
BuVaR and ETL for USD 3-Month Libor
Source: Bloomberg Finance L.P.
Acceptance Tests
297

222%
22%
18%
38%
58%
78%
98%
118%
138%
0
200
400
600
800
1,000
1,200
Oct-94
Oct-96
Oct-98
Oct-00
Oct-02
Oct-04
Oct-06
Oct-08
Bubble
Gold
Distance (%)
Price Level
FIGURE 15.19
Bubble Measure for Gold Spot Price (USD)
Source: Bloomberg Finance L.P.
2200
0
200
400
600
800
1,000
1,200
26%
21%
4%
9%
14%
19%
24%
29%
Oct-94
Oct-96
Oct-98
Oct-00
Oct-02
Oct-04
Oct-06
Oct-08
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
Gold
Price Level
VaR (% loss)
2
1
3
FIGURE 15.20
BuVaR and ETL for Gold Spot Price (USD)
Source: Bloomberg Finance L.P.
298
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

225%
25%
75%
125%
175%
(30)
(10)
10
30
50
70
90
110
130
150
Oct-94
Oct-96
Oct-98
Oct-00
Oct-02
Oct-04
Oct-06
Oct-08
Bubble
Oil future
Price Level
Distance (%)
FIGURE 15.21
Bubble Measure for Brent Crude Oil Futures Price
Source: Bloomberg Finance L.P.
(30)
(10)
10 
30 
50 
70 
90 
110 
130 
150 
210%
0%
10%
20%
30%
40%
50%
Oct-94
Oct-96
Oct-98
Oct-00
Oct-02
Oct-04
Oct-06
Oct-08
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
Oil future
Price Level
VaR (% loss)
FIGURE 15.22
BuVaR and ETL for Brent Crude Oil Futures Price
Source: Bloomberg Finance L.P.
Acceptance Tests
299

occurred since it cascaded down. The arrow indicates the lead time from
when the bubble peaked to when the price collapse was steepest. The bubble
peaked in July 2007 just as the S&P index (and the buVaR)1 reached its
high, one year ahead of the crash, and maintained a significant penalty on
the long side.
Figure 15.24 shows the buVaR of the Dow Jones index around the
Black Monday (1987) crash, which registered a 25% single day freefall.
TABLE 15.2
Key Negative Events Found in Recent Financial History
Asset
Asset Class
Crash Events
S&P 500 Index
Equity
2008 Credit crunch
Dow Jones Index
Equity
1987 Black Monday crash
Nasdaq 100 Index
Equity
2000 Internet bubble burst
Nikkei 225 Index
Equity
1989 Japan’s asset bubble burst
Hang Seng Index
Equity
1997 Asian crisis
USD/JPY
Forex
1998 Bank of Japan intervention
AUD/SGD
Forex
2008 Yen carry unwind
Brent crude oil futures
Commodity
2008 Oil price bubble burst
2
200 
400 
600 
800 
1,000 
1,200 
1,400 
1,600 
210%
25%
0%
5%
10%
15%
20%
25%
30%
35%
40%
Nov-05
Jun-06
Jan-07
Aug-07
Mar-08
Oct-08
May-09
Dec-09
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
S&P 500 index
Price Level
VaR(% loss)
FIGURE 15.23
The 2008 Credit Crunch and the BuVaR of S&P 500 Index
300
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

Again, the arrow indicated a one-year lead time. The bubble measure was
preemptive, and the penalty was maintained until after the crash.
Figure 15.25 shows the buVaR of the Nasdaq index when the Internet
bubble burst in 2000. It may appear that buVaR peaked after the initial
collapse, but the bubble measure actually peaked just as the Nasdaq made its
high (Figure 15.3).
Figure 15.26 shows the buVaR of the Nikkei 225 index during the 1987
stock market crash and Japan’s asset bubble burst in 1989. It would seem in
1989 (circled in Figure 15.26) buVaR did not significantly penalize the long
side. This is because the ETL itself is ultralow, even though the bubble
measure peaked (see circle in Figure 15.5) its multiplicative effect is diluted.
Figure 15.27 shows the buVaR of the Hang Seng index during the 1997
Asian crisis. The buVaR peaked about six months ahead of the crash. In fact,
the dotted vertical line indicates where the bubble measure peaked just
before the crash (see also Figure 15.7).
Figure 15.28 shows the buVaR of USD/JPY exchange rate during the
BoJ intervention period in 1997–1998. There were multiple rounds of BoJ
purchases of yen; three of the largest collapses were picked up by buVaR,
including the famous yen carry unwind on October 7 and 8, 1998, that was
triggered by the Russian debt default (August 98) and LTCM collapse
(September 98). The inflation of VaR was significant and preemptive, and
would have discouraged (bank) speculators from shorting the yen during
that period.
0
500
1,000
1,500
2,000
2,500
3,000
210%
25%
0%
5%
10%
15%
20%
25%
30%
35%
40%
Jan-85
Jan-86
Jan-87
Jan-88
Jan-89
Jan-90
Jan-91
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
Dow Jones index
Price Level
VaR(% loss)
FIGURE 15.24
The 1987 Black Monday and the BuVaR of Dow Jones Index
Acceptance Tests
301

(1,000)
2
1,000
2,000
3,000
4,000
5,000
210%
0%
10%
20%
30%
40%
50%
Jul-97
Jul-98
Jul-99
Jul-00
Jul-01
Jul-02
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
Nasdaq index
Price Level
VaR(% loss)
FIGURE 15.25
The 2000 Internet Crash and the BuVaR of Nasdaq Index
2
5,000
10,000
15,000
20,000
25,000
30,000
35,000
40,000
210%
25%
0%
5%
10%
15%
20%
25%
30%
35%
40%
Aug-86
Aug-87
Aug-88
Aug-89
Aug-90
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
Nikkei index
Price Level
VaR(% loss)
FIGURE 15.26
Japan’s Asset Bubble Collapse (1989) and the BuVaR of Nikkei
Index
302
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

75
85
95
105
115 
125
135
145
23%
3%
8%
13%
18%
Jul-96
Jul-97
Jul-98
Jul-99
Jan-97
Jan-98
Jan-99
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
USD/JPY
Price Level
VaR (% loss)
1
2 
3 
FIGURE 15.28
BoJ USD/JPY Exchange Rate Intervention 1997–1998 and BuVaR of
Yen
Source: Bloomberg Finance L.P.
2
2,000
4,000
6,000
8,000
10,000
12,000
14,000
16,000
18,000
210%
0%
10%
20%
30%
40%
50%
Jan-96
Jul-96
Jan-97
Jul-97
Jan-98
Jul-98
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
Hang Seng index
Price Level
VaR(% loss)
FIGURE 15.27
The Asian Crisis (1997) and the BuVaR of Hang Seng Index
Acceptance Tests
303

Figure 15.29 shows the buVaR of the AUD/SGD exchange rate during
the yen carry unwind in 2008. The credit crunch sparked a massive flight-to-
safety and exiting of yen carry trades,2 a favorite yield pick-up trading
strategy among institutional investors. Because of the large differential
between AUD and SGD interest rates, the AUD/SGD was a favorite choice
for such trades. BuVaR peaked almost six months ahead of the AUD/SGD
collapse.
Figure 15.30 shows the buVaR of crude oil futures during the bursting
of the oil bubble in 2008. Both the bubble measure (Figure 15.21) and
buVaR peaked ahead of the crash by about three months.
We can draw some general conclusions from the tests in Sections 15.1 and
15.2. BuVaR peaks and troughs ahead of the market, often with a comfortable
lead time for banks to trim positions. In rare situations of ultralow and
declining volatilities, buVaR’s effectiveness may be hampered. In contrast,
conventional VaR is always one step behind the market in a crisis.
15.3 BuVaR CYCLICALITY TESTS
BuVaR is designed to act as a countercyclical measure of risk. To qualify, we
need to test whether the inflator synchronizes with the business cycle. There
0.5
0.6
0.7
0.8
0.9
1.0
1.1
1.2
1.3
1.4
25%
0%
5%
10%
15%
20%
25%
30%
Jun-06 Nov-06 Apr-07 Sep-07 Feb-08 Jul-08 Dec-08 May-09 Oct-09 Mar-10
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
AUD/SGD
Price Level
% Tail Loss
FIGURE 15.29
Yen Carry Unwind in 2008 and BuVaR of AUD/SGD Exchange Rate
Source: Bloomberg Finance L.P.
304
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

are many ways in the statistical literature to measure the business cycle. One
popular method is the HP filter, which assumes that a time series xt can
be divided into two components: the trend (lt) and the cycle (st), such that
xt ¼ lt þ st. Hodrick and Prescott (1981) proposed that the trend can be
backed out by optimizing the following equation:
min
lt
X
T
t¼1
ðxt  ltÞ2 þ λ
X
T
t¼1
ðltþ1  2lt þ lt1Þ2
ð15:1Þ
where λ is the smoothing parameter. The business cycle is derived by
simply taking the ratio between the price series and the cycle, hence xt/st.
Hodrick and Prescott suggest setting λ ¼ 1,600 for quarterly data. Ravn
and Uhlig (2002) proposed a way to adjust λ for data of different
frequencies—it is optimal to multiply λ with the fourth power of the
observation frequency ratio, for example, since we use daily data,3 λ should
be (90)4 1,600 ¼ 104,976,000,000.
Specifically, we want to test if the bubble indicator moves in correlation
with the business cycle. Since there is no fundamental reason to expect the
relationship to be a linear one, we use all three measures from Section 2.4:
linear correlation, Kendall tau, and Spearman rho.
(30)
(10)
10
30
50
70
90
110 
130
150
170
210%
0%
10%
20%
30%
40%
50%
-06
Jan
Jul-06
Jan-07
Jul-07
Jan-08
Jul-08
Jan-09
Jul-09
Jan-10
ETL (Longs)
ETL (Shorts)
buVaR(Longs)
buVaR(Shorts)
Oil future
Price Level
VaR(% loss)
FIGURE 15.30
Oil Bubble Burst in 2008 and BuVaR of Brent Crude Oil Futures
Source: Bloomberg Finance L.P.
Acceptance Tests
305

Correlation should be calculated on changes rather than on price levels;
otherwise effects from long-term trends can result in extremely misleading
correlation values. For example, if one price series oscillates around a long-
term bullish trend and the other price series oscillates around a bearish trend,
the correlation of prices would imply that the variables are inversely cor-
related, even if the timing of the upswings and downswings are perfectly in
step. The computation is done using monthly changes (data from December
1993 to June 2009) because daily changes can be noisy.
As per Table 15.3, the correlation results are generally positive for all
three measures and significantly different from zero (or no relationship).
This suggests that the buVaR methodology can produce a risk metric that is
correlated to the business cycle. Therefore, buVaR can be used for the
computation of countercyclical capital buffers.
The correlation result in Table 15.3 is not perfect because the bubble is
designed to not just track the cycle faithfully, but to also adapt to changing
conditions automatically. In particular, it must widen when a market bubble
is forming, it must not follow when the price crashes, and it must shrink
during extended periods of gradual growth. These adaptive benefits mean
that the correlation, while reasonably positive, will not be very high.
15.4 CREDIT BuVaR PARAMETER TUNING
As seen in the few examples in Chapter 14, credit buVaR is non-cyclical in
the sense that there is no dependency on the business cycle. It is designed to
TABLE 15.3
Correlation Tests of Bubble Measure versus Business Cycle (December
1993 to June 2009)
Test Asset
Kendall Tau
Linear Correl
Spearman Rho
AUD/SGD exchange rate
0.38
0.53
0.55
Gold (spot)
0.19
0.24
0.26
Hang Seng index
0.38
0.52
0.53
USD/JPY exchange rate
0.41
0.58
0.57
Nasdaq index
0.40
0.58
0.55
Nikkei 225 index
0.39
0.51
0.55
Crude oil futures
0.38
0.50
0.55
S&P 500 index
0.31
0.42
0.43
USD 5y swap
0.24
0.32
0.32
USD 10y swap
0.32
0.38
0.45
USD 3m Libor
0.35
0.25
0.49
Yen 1y swap
0.37
0.42
0.53
306
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

detect the occurrence of credit escalation and then penalize long credit
positions in a sensible way.
There is no purer measure of default expectation than credit spread
itself. Since expectations are forward looking, a spread escalation is pre-
emptive of a default. Because the spread is used to inflate credit buVaR, there
is no issue with timing, the effects of inflation will always be immediate—
thus no timing test is required. The crucial issue is then the setting of ω2.
How conservative (or penal) should one make credit buVaR?
To answer this question, let’s look at credit buVaR for five selected
issuers that have experienced the credit crunch in 2008. We convert five-year
spreads to bond prices using the same model assumptions as in Chapter 14,
in particular, the five-year swap rate and the bond’s coupon are taken to be
2.57%. Two sets of results are calculated, for ω2 ¼ 1.2 and, more conser-
vatively, for ω2 ¼ 0.5.
Table 15.4 revisits the conditions during the 2008 crisis; particularly the
spreads are recorded on those dates when (credit) buVaR reached its peak
for each issuer. The buVaR and VaR are expressed as shifted price level and
dollar loss. For example, in the case of Toyota, on December 16, 2008, its
VaR is a loss of $1.27 from 90.09 to 88.82 for $100 standard notional.
TABLE 15.4
Summary Calculation When Credit BuVaR Is at Its Peak
Toyota
Philippines
Lehman
AIG
General Motors
Date
16-Dec-08
02-Mar-09
15-Sep-08
22-Apr-09
04-May-09
Current Spread
224
488
701
2,670
15,445
Current Price
90.09
79.81
72.42
31.01
1.77
Price (at VaR)
88.82
77.36
68.10
24.30
1.44
Price(at
buVaR,
w ¼ 1.2)
88.68
76.76
66.73
19.67
1.23
Price(at
buVaR,
w ¼ 0.5)
86.61
71.50
58.41
15.31
1.11
VaR loss
1.27
2.45
4.32
6.71
0.32
buVaR loss
(w ¼ 1.2)
1.41
3.05
5.69
11.35
0.54
buVaR loss
(w ¼ 0.5)
3.48
8.31
14.01
15.70
0.66
Acceptance Tests
307

For each individual issuer, its spread is tabulated. Assuming a five-year
bond for the issuer, its prevailing price and shifted price are computed using
Excel function PRICE(.). For example, to calculate the AIG bond price after
being shifted by buVaR of 69.2%4 (with ω2 ¼ 0.5) use:
PRICE

TODAYðÞ; TODAYðÞ þ 5  365, 0:0257, 0:0257 þ 2670 

1 þ 0:692Þ  0:0001, 100, 4

¼ 15:31:
As expected, ω2 ¼ 0.5 produces more conservative buVaR results than
ω2 ¼ 1.2; that is, the dollar loss is larger. As we shall see, the rapidity of
credit deterioration (when it occurs) calls for a penal minimum capital
regime. Let’s look at a few examples.
Figure 15.31 shows the buVaR for Lehman five-year CDS. Its spread
broke the 200 level on February 15, 2008, and then again on May 22, 2008,
and deteriorated rapidly. Lehman defaulted 16 weeks later when spreads
-10%
10%
30%
50%
70%
90%
110%
130%
150%
(200)
(100)
-
100
200
300
400
500
600
700
May-06
Oct-06
Mar-07
Aug-07
Jan-08
Jun-08
Nov-08
Spread pVaR
Lehman 5y CDS
Credit buVaR(w = 0.5)
Credit buVaR(w = 1.2)
LEHMAN
bankruptcy 
Spread (bp)
VaR loss
FIGURE 15.31
BuVaR for Lehman Five-Year CDS Spread
Source: Bloomberg Finance L.P.
308
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

were at 701. Two important lessons for modeling are (1) default is not a
jump process, there is always a warning signal in the form of spread esca-
lation; (2) it can happen very quickly and sometimes discontinuously (in this
case at 701). If a five-year bond were to deteriorate continuously until
default, Lehman would have escalated to 5,800bp (assuming 10% recovery).
However, the market expected a Fed bailout so spread widening was not
drastic. When the rescue plan failed, the bankruptcy surprised the market
and trading stopped abruptly.
The same pattern is shown in the case of General Motors (see Figure
15.32). Its spread broke 1,000 on May 23, 2008 (the same time as Lehman’s
breakout!). It took the market 11 months to reach the high of 21,000bp. The
same observations are applicable here: (1) There is ample warning time, and
(2) the spread escalation was rapid. In practice, once spreads escalate to
above 4,000bp to 5,000bp, there is very little a bank can do to trade out of
the position since liquidity will dry up and most counterparties will treat it
like an impaired asset and look for off-market settlement. While the time
series was not disrupted, the bond has effectively defaulted on an MTM
basis (see the last column of Table 15.4). Notice the buVaR started coming
off as the spread went above 15,445. This is to constrain the shifted price
from falling below zero.
0%
20%
40%
60%
80%
100%
120%
140%
(6,000)
(4,000)
(2,000)
2
2,000
4,000
6,000
8,000
10,000
12,000
14,000
16,000
18,000
20,000
Apr-05
Nov-05
Jun-06
Jan-07
Aug-07
Mar-08
Oct-08
Spread pVaR
GM 5yr CDS
Credit buVaR(w = 0.5)
Credit buVaR(w = 1.2)
Spread (bp)
VaR loss
FIGURE 15.32
BuVaR for General Motors Five-Year CDS Spread
Source: Bloomberg Finance L.P.
Acceptance Tests
309

A notable observation is that conventional spread VaR is a poor mea-
sure of default risk. The VaR for GM moved up very little throughout the
crisis—it is capturing changes in volatility while ignoring the rapid rise in
spread levels. It is possible for spreads to trade at high levels but with low
volatility. Spread VaR completely missed the warning signs.
Figure 15.33 shows the spread of AIG, which did get a government bail-
out package. The spread broke 500 on September 11, 2008 and reached
2,500bp just seven weeks later. From these examples, it would seem that
banks may have as little as two months to react to a credit deterioration, say,
by cutting positions.
Figure 15.34 and Figure 15.35 are two examples of issuers that were
negatively affected by the sentiment during the credit crunch, but were never
in danger of default. (It is inconceivable that the Philippine government
would declare a debt moratorium because of contagion from the subprime
crisis; the economics are too far apart.) The spread escalation was equally
rapid, but the magnitude was many times smaller.
These five examples have highlighted a common theme: Credit deteri-
oration is very rapid when it occurs. Figure 15.36 shows the response
function for ω2 ¼ 1.2, ω2 ¼ 0.5. The shaded region is the “normal” trading
zone when the market is not in distress. When credit escalation occurs, the
0%
20%
40%
60%
80%
100%
120%
140%
160%
180%
200%
(1,500)
(1,000)
(500)
2
500
1,000
1,500
2,000
2,500
3,000
3,500
Apr-06
Sep-06
Feb-07
Jul-07
Dec-07
May-08
Oct-08
Mar-09
Spread pVaR
AIG 5yr CDS
Credit buVaR(w = 0.5)
Credit buVaR(w = 1.2)
AIG was AAA rated
before crisis
Spread (bp)
VaR loss
FIGURE 15.33
BuVaR for AIG Five-Year CDS Spread
Source: Bloomberg Finance L.P.
310
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
(100)
(50)
2
50
100
150
200
250
300
Apr-02
Apr-03
Apr-04
Apr-05
Apr-06
Apr-07
Apr-08
Apr-09
Spread pVaR
TOYOTA 5yr CDS
Credit buVaR(w = 0.5)
Credit buVaR(w = 1.2)
Spread (bp)
VaR loss
FIGURE 15.34
BuVaR for Toyota Five-Year CDS Spread
Source: Bloomberg Finance L.P.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
(200)
(100)
2
100
200
300
400
500
600
700
800
Apr-02
Apr-03
Apr-04
Apr-05
Apr-06
Apr-07
Apr-08
Apr-09
Spread pVaR
Philippines 5yr CDS
Credit buVaR(w = 0.5)
Credit buVaR(w = 1.2)
VaR loss
Spread (bp)
FIGURE 15.35
BuVaR for the Philippines Government Five-Year CDS Spread
Source: Bloomberg Finance L.P.
Acceptance Tests
311

credit spread accelerates rapidly above 500bp (for instance) to outside the
normal zone.
This phenomenon argues for a more penal setting (ω2 ¼ 0.5) since it will
be too late (and counterproductive) to raise minimum capital only when the
issuer company is rapidly falling apart. A capital buffer for default risk needs
to be built up when spreads are still trading in the normal zone, but showing
signs of widening.
Using ω2 ¼ 0.5 (dotted line) as an example, let’s examine how penal
buVaR is. When the spread is at 20 (AAA-rated levels) the inflator is at 1.5,
which means that for the best corporate issuers a long credit position is taken
to be 50% more risky than a short credit position. When the spread is at
350, the average five-year spread of Philippine CDS during the period
shown in Figure 15.35, the inflator is at 3.0. This means for high-yield
issuers, a long credit position is three times riskier than a short credit posi-
tion. For ω2 ¼ 0.5 the inflator is capped at 4.4 times.
It is true that a short credit position has no risk of default unlike a long
position. But is it reasonable to penalize the long side multiple times more
than the short side from the default risk standpoint? We offer a heuristic
argument.
The credit spread puzzle was well researched as discussed by Amato and
Remolona (2003)—it refers to the fact that corporate credit spreads are
much wider than what expected default would imply. The latter is derived
20.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
0
500 1,000 1,500 2,000 2,500 3,000 3,500 4,000 4,500 5,000 5,500 6,000 6,500 7,000
Inflator
Current spread (Bp)
w=1.2
w=0.5
FIGURE 15.36
Response Function for Two Settings of ω2 ¼ 1.2, ω2 ¼ 0.5
312
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

from historical default statistics available from rating agencies. Studies by
Elton et al. (2001) confirmed that expected default risks contributed no more
than 25% of observed spreads. Georges et al. (2005) showed that results can
differ widely depending on the methodology, the default cycle, and recovery
rate assumptions. In particular, for the period of high default cycle, the
proportion for Baa bonds can go as high as 71% of estimated spread,
assuming a recovery rate of 49%. For this case, when the recovery
assumption is reduced to 40%, the proportion increases to 83%.
Hence, depending on the severity of the default cycle, the proportion of
default risks in credit spreads can be as high as 83%. The rest is explained by
risk premium (arising from spread volatility), liquidity premium, and diffi-
culty in diversification of tail-risks. We argue that a short CDS position is
free from default risk, hence, should have risks unrelated to default that can
be as low as 17% of total risk priced into credit spreads. We can then infer
the ratio of risks between long and short CDS.
As a rough guide, if up to 83% of the spread is contributed by default
risks (i.e., at least 17% by nondefault risks), then a long CDS is up to 100/
17 ¼ 6 times riskier than a short CDS. Hence, the inflator should range from
1 to 6 depending on the extent of credit deterioration, in general agreement
with our cap of 4.4 times. Put another way, the incremental loss of buVaR
over and above spread VaR (which just measures volatility) can be attributed
to default risks.
In summary, credit buVaR provides an add-on buffer to conventional
spread VaR on the long side to account for default risks. The response
function is theoretically appealing, and we justified its interpretation by
referring to research on the credit spread puzzle.
NOTES
1. The buVaR does not always peak at the same time as the bubble since, if con-
ventional VaR is very small, its inﬂation may not produce a sizable buVaR. In
such cases, the bubble peak often leads the buVaR peak.
2. The trader invests in a high-yielding currency using funds borrowed in a low-
yielding currency. If the exchange rate remains unchanged, the trader stands to
earn the interest difference between the two currencies as an accrual carry proﬁt.
The strategy is dubbed yen carry because yen, with near zero rates, is tradi-
tionally used for this strategy.
3. We assume one quarter is 90 days. Free shareware implementation of the HP
ﬁlter is available from the Internet. The reader can easily show that the result of
the HP ﬁlter is similar to that of a low order (such as a cubic) polynomial ﬁt,
which is also commonly used by economists.
4. The calculation of 69.2% buVaR (which will shift the spread) is not shown here.
Acceptance Tests
313


CHAPTER 16
Other Topics
T
his chapter deals with miscellaneous topics relevant to bubble value at
risk (buVaR). First, we discuss the effects of implementing buVaR at the
portfolio level. Are diversification, aggregation, and decomposition similar
to that of conventional value at risk (VaR)? Is basis risk well captured?
Second, we look at how buVaR as a new framework for measuring risk is
able to meet the ideals proposed by the Turner Review. Lastly, we reflect
that buVaR can be seen as a potential new direction in the quest to create a
good riskometer.
16.1 DIVERSIFICATION AND BASIS RISKS
A risk measure is of limited use if it cannot be aggregated in a diversifiable
way. BuVaR preserves the correlation structure even after incorporating
countercyclicality, which means that the benefits of diversification are
maintained. Profit and loss (PL) vectors of different positions, portfolios,
and even of different banks can be aggregated. The reverse process
of decomposition is best achieved using the method of incremental VaR of
Section 6.3.
Recently there has been a lot of pressure from regulators for banks to
include and improve on the measurement of basis risk, which is seen as
under-represented in current VaR systems. Basis risk arises from long-short
and spread positions. Since such positions are intended to be market-neutral
(not sensitive to general market movement), one can expect the basis to be
not volatile almost all the time except for exceptional days when there is
idiosyncratic news that specifically moves the basis. This makes basis risk
fat-tailed (highly leptokurtic) by its very nature. Consider Figure 16.1, which
compares the kurtosis of five-year and four-year USD swaps and their spread
position over a four-year period. The kurtosis is measured using a 250-
day rolling window. The basis return shows high kurtosis in general.
315

A further complication is that in a typical VaR observation period, there
are just too few idiosyncratic observations for adequate quantification of tail
risk or true kurtosis. BuVaR compensates for this weakness by making basis
buVaR more sensitive.
We illustrate this in Exercise 16.1 using Dow Jones and S&P 500 index
data during the credit crisis period, from November 2007 to October 2008.
Their returns are highly correlated during stressed markets with coefficient
þ0.99. Since the bubble measure was positive during this period, we mul-
tiply the left side of both distributions by a factor of three (as a simple
illustration). We look at a hedged basis position or long-short strategy, with
$1 million notional on each side of the deal.
The result of the exercise is summarized in Table 16.1 which shows the
buVaR vs. the expected shortfall (ES). The long position risk measure is
0
1
2
3
4
5
6
7
–
5
10
15
20
25
Nov-05
Feb-06
May-06
Aug-06
Nov-06
Feb-07
May-07
Aug-07
Nov-07
Feb-08
May-08
Aug-08
Nov-08
Feb-09
May-09
Aug-09
Nov-09
Yield(%)
Kurtosis (250day)
4yr and 5yr swaps
Kurtosis of
Basis
Kurtosis of 4yr
and 5yr swaps
FIGURE 16.1
Kurtosis Behavior: 4-Yr Swap, 5-Yr Swap, and Their Basis
Source: Bloomberg Finance L.P.
TABLE 16.1
Comparison of BuVaR versus Expected Shortfall for $1 Million
Outright and Basis Positions
Expected Shortfall (ES)
buVaR
buVaR/ES
Long position (Dow)
(58,009)
(174,026)
3.0
Short position (S&P)
(51,073)
(51,073)
1.0
Hedged position (basis)
(6,824)
(17,702)
2.6
316
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

inflated by three times as expected while the short position is not penalized, since
it is not chasing the bubble. More interestingly, the basis buVaR risk is small
($17k) and is reflective of a hedged position, but is larger than the basis risk
as measured using ES. This indirect inflation of basis risk (by 2.6 times) com-
pensates for the underestimation of fat-tailness in conventional tail measures.
Furthermore, buVaR increases the basis risk measure whenever bubbles
are building rapidly or when there is a crash. This is a desirable feature
because a spread relationship has an increased chance of breaking during
such times. In comparison, under conventional VaR, basis risk VaR will only
creep up slowly after the start of a financial crisis as idiosyncratic observa-
tions roll into the observation window.
16.2 REGULATORY REFORM AND BuVaR
The Turner Review highlighted two dangers of using VaR as a risk measure
and metric for minimum safety capital—its inability to capture fat-tail
risk and its susceptibility to procyclicality. The Review goes on to lay down
the requirement of a future metric for risk capital—first, it must be overtly
countercyclical, and, second, it must provide a much higher capital level to
buffer against extreme tail events. We have developed the idea of buVaR in
this book. Let’s see how buVaR is able to satisfy these criteria.
Cushioning Fat-Tail Losses
From the author’s own research, it is found that buVaR is asymmetrical.
This innovation recognizes that fat-tail losses can only happen in one
direction during any given period—crashes can only happen downwards in a
bull market, and bounces can only happen upwards in a bear market.
BuVaR inflates the distribution in a unidirectional way to create a more
realistic buffer against fat-tail events. Hence, for the bank that is short,
the bubble asset will require less capital than for a bank that is chasing
this hot asset.
In the same way, in credit buVaR, a bank that is short credit risk (i.e.,
the protection buyer) can hold less capital than a bank that is long credit risk
(i.e., the protection seller). This recognizes that credit default risk is asym-
metrical and hurts only the long side.
With buVaR in place, one can expect risk capital to be multiples of the
current VaR and in the direction that penalizes imprudent investment. And
capital will be largest when the bubble is largest just when the risk of a fat-
tail crash event is highest.
Other Topics
317

Overtly Countercyclical
BuVar evidently has a countercyclical quality—it can rise and peak up to
several months ahead of the actual peak in an asset bubble. In other words,
unlike VaR, it is a leading (not a lagging) risk measure and can provide an
early warning of market distress.
BuVaR creates buffers for individual assets depending on the extent of
bubble formation in that asset. In other words, it is not a one-size-fits-all
method for minimum capital and is thus superior to a naïve method of
multiplying a countercyclical factor to a bank’s overall VaR without dif-
ferentiating between assets held. Suppose the general market index is rallying
toward the peak of a boom cycle (bubble), and a bank is short the index or
is exposed to some other market (nonbubble) for the same amount of VaR,
the bank will be unduly penalized under the naïve approach. This may create
a perverse incentive where banks will favor hot assets over benign ones
for the same amount of risk (or VaR) exposure, since the cyclical charges for
both are the same.
We now realize that VaR does not just measure but also influences
market risks. With hardwired procyclicality at work to push mandatory risk
capital and, consequently, the bank’s ability to leverage, the banking system
can collectively increase the likelihood of extreme events. It increases the
frequency and magnitude of the boom-bust cycle.
The countercyclical alternative, buVaR, if used on a global scale, will
not only build buffers against large losses but may actually decrease the
likelihood of systemic crashes. It makes it painful (or expensive) for banks to
chase hot assets even before the bubble can burst, and effectively acts as a
cycle dampener. If all regulated banks, too big to fail institutions, and
shadow banks are required to use buVaR for regulatory capital, a much
safer financial system can be created.
Other Advantages
Aside from the two criteria proposed by the Turner Review, buVaR has
other merits:
Firstly, buVaR is more responsive to regime breaks in correlation and
volatility compared to conventional VaR, thanks to the choice of using
expected shortfall for the tail estimate.
Secondly, credit buVaR is able to account for default risk in a forward-
looking way by making itself a function of credit spread level. This is because
the spread level reacts quickly to trading and is forward looking when it comes
to credit deterioration as compared to conventional spread VaR. The latter
is nothing more than a rolling standard deviation of spread changes and is thus
318
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

backward-looking. Back-testing using 2008 crisis data shows that credit
buVaR often reacted weeks ahead of credit deteriorations. More interest-
ingly, buVaR is able to integrate spread and default risks into a single measure
because it is a function of both spread volatility and spread level. Thus there is
no longer a need to rely on credit ratings to model default risks separately.
Rating signals were found to be grossly late during the credit crisis.
Finally, under the current capital regime, supervisory multipliers are
applied on the outside after the VaR has been computed (see equation
(11.5)). This means the VaRs for market risk, credit risk, and other risk
classes may only be added as lump sums and thus will not account for
diversification. In contrast, for buVaRs, the supervisory parameters are built
inside the risk model itself. The advantage is that the market buVaR and
credit buVaR can be subadded while retaining the feature of diversification
in the process. A regulator can penalize harder a weakly managed bank by
tuning the buVaR parameters. The supervisor can even subadd buVaR
across banks for example to perform Contagion VaR (CoVaR) and other
systemic risk analysis. This gives the systemic risk regulator a unified,
powerful, and centralized tool to detect and monitor pockets of risk con-
centration and network effects in the financial system.
16.3 BuVaR AND THE BANKING BOOK: RESPONSE TIME
AS RISK
BuVaR is designed for trading book capital and is less suitable for the
banking book. The calculation of buVaR requires daily price data. Hence
this metric is only applicable for markets that show good price discovery,
including tradeable credit markets but excluding actual loans.
In contrast, banking book loans are modeled by loss given default
(LGD) and probability of default (PD) using historical statistics of loan
delinquencies—daily prices are not readily observed. The new Basel III
contains a countercyclical capital buffer designed to protect against pro-
cyclicality of credit growth which directly affects the banking book (see
Section 11.5).
With the exception of pure investment firms, most lending institutions
would have a much larger banking book in terms of notional compared
to the trading book; thus, the credit risk charge is generally higher compared
to the market risk charge for a typical bank. However, the author argues that
the risk of the trading book is actually much higher (for the same notional)
compared to the risk of the banking book. This has to do with the element of
response time to calamity seldom considered in modern risk management
and measurement.
Other Topics
319

As an analogy, consider the proposition that a bush fire is less dangerous
than a tsunami, even though the former may cause relatively more devas-
tation to a wider geographical area. This is because a bush fire provides more
warning time for people to take evasive action. In short, a risk that affords
plenty of warning time is actually less risky!
We can rank the various risk categories faced by a bank in terms of
response time that they allow:
1. Operational risk, little to no warning time. For example, in a rogue
trading incident, or during the 9/11 tragedy, top management has
almost no time to react rationally.
2. Funding liquidity risk, in a matter of days (when there is a liquidity
squeeze). This is a function of information efficiency. The interbank
money market dealers operate in a relatively small community, where
any news or rumors are quickly translated into market prices and bid/
ask spreads. Fortunately, during exceptional times, the central bank
often acts as lender of last resort to ensure that there is no disruption in
the payment system.
3. Market risk, in weeks. While liquid markets react to news at Internet
speeds, the massive number of buyers/sellers with opposing views,
mean that price falls are often “supported,” and price movements,
albeit large, are orderly. For example, Figure 15.31 shows that even in
the Lehman bankruptcy, the credit spread took 16 weeks to escalate
after it broke the 200 level for the second time. An investor with
the correct foresight would have had enough time to liquidate or
hedge his position.
4. Credit risk of loans, in months. Real deterioration of credit conditions
takes time to affect borrowers; hence, NPL (nonperforming loans) take
time to rise in the banking book. This long lead time allows a prudent
institution to take evasive actions to avert a fall-out, such as by raising
fresh capital or by hedging, and so forth. It also provides time for
borrowers to look for alternate sources of funding, or to reduce debt.
The unequal response time afforded by different risk types, means that
trading book losses can be higher than banking book losses. This is sup-
ported by evidence from the credit crisis. The IMF report1 estimated a mark-
to-market loss of USD720bln from derivatives of loans (such as CDO, ABS,
MBS) as of March 2008, compared to an actual loss of USD225bln from
whole loans in the United States. Hence, for the purpose of capital buffers,
some form of scaling to account for unequal response time to risk is prob-
ably prudent.
320
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

16.4 CAN BuVaR PICK TOPS AND BOTTOMS PERFECTLY?
Many empirical studies have shown that markets are efficient; hence, fore-
casting models cannot profit from the market consistently. Markets can be
inefficient in the short run, which could explain why some traders do make
money using strategies/insider information. However, the usefulness of
strategy/insider news is often short-lived, and the trader either goes bust
subsequently, or finds himself modifying his strategy to survive. Thus, a
model that performs like a money-making machine—the holy grail of
trading—just doesn’t exist.
If you believe in efficient markets, the bubble indicator should not
consistently pick the exact occurrence of market tops and bottoms, other-
wise trading profits can be generated by exploiting this indicator.
While buVaR cannot be used for profit making, it can be used for
risk management or loss avoidance. We have determined in Chapter 15
that buVaR will provide an additional capital buffer ahead of crashes
most of the time. One drawback is, at times, that the bubble may rise in a
rally even though the frenzied rally subsequently proves to be a long-
term trend. This will make the buVaR too conservative; the adaptive
moving average is a device to mitigate this weakness. If used for trading,
such early signals will lead to losses, but used as a risk management tool,
a conservative buffer during periods of heightened risk is a prudent
approach.
Another drawback is, on very rare occasions, at the height of a bubble,
prices may advance at ultralow volatility (what I call the calm before the
storm effect). This was observed for the Nikkei 225 index in the period
leading up to the Japanese asset bubble burst at the close of 1989. The
unusual calmness made buVaR small even though the bubble indicator was
reaching a peak. Despite this weakness, buVaR is still much larger (and more
conservative) than normal VaR or expected shortfall in this case when the
market is brewing with danger.
16.5 POSTMODERN RISK MANAGEMENT
The Markowitz framework is also called modern portfolio theory. Its
greatest contribution is in providing a formalized framework for investment
decision making that incorporates a proper risk measure (variance), corre-
lation relationships, and risk diversification in an elegant way. Its major
weakness lies in its assumptions of variance as a risk metric and normal
distribution of asset returns—both of which are symmetric notions. As
Other Topics
321

modern risk management adopted the Markowitz framework, our risk
measurement is inherently symmetric.
In 1959, Markowitz suggested that a model based on semivariance
would be preferable but in light of the formidable computational challenge
at that time, his model was based on mean-variance. Simplistically, semi-
variance is the same as variance except that only returns that are below the
mean are selected for the computation. Hence, it reflects dispersion or risk
on the downside. This asymmetric measure of dispersion opened up an
area of modeling, which subsequent researchers termed postmodern port-
folio theory.
The paradigm of viewing risk as variance is deeply ingrained in risk
management practice and literature. Is this as logical as it appears to be? Ask
a person who has never studied Markowitz theory, what is his risk when he
bought an investment of stocks. His answer will inevitably be that he is
afraid prices may fall by Y% (and normally not that the variance will
increase by X%). Put simply, the most natural way (cognitively) to perceive
risk is in terms of directional risk, not dispersion (around a mean). Intui-
tively, if the market crashes, it can only crash down, never up! As a further
proof, this fear of direction is even reflected in the option prices of different
strikes in the so-called volatility smile. In the equity option market, the smile
is skewed to the negative side reflecting downside aversion.
Let’s look at another example: a trader has bought a stock and short
another in the so-called basis trading strategy. What is his risk? If he uses his
intuition, his answer will (rightly) be an adverse move of the basis in the
wrong direction (and not an X% increase in basis volatility). Arguably,
the only instance where it is natural to think in terms of volatility is when
dealing with a delta-hedged option. So why do risk managers think in terms
of volatility risk and never directional risk? Thanks to Markowitz, the
variance paradigm has dominated risk curriculum, and risk management is
framed in the language of frequentist statistics. Under this school of thought,
variables (risk factors) have to be filtered or transformed to be independent
and identically distributed (i.i.d.), so that they are fit for the purpose of
statistical inferences.
Put simply if they are not i.i.d., models become intractable, meaning we
cannot make precise statements about modeled conclusions. Unfortunately,
the strict requirement of i.i.d. means that risk management deals only with
returns and never with levels of risk factors. By ignoring price levels, we are
oblivious to information on directional risk such as overbought/oversold
conditions, the potential explosiveness of prices surrounding key levels, and
the prevailing cycle of the market.
The buVaR model is an invitation to consider a paradigm in which
directional risk is given its rightful importance. The loss of precision in
322
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

applying this framework is compensated by the benefit of having a poten-
tially more realistic measure of risk, one that is cognitively natural.
More importantly, buVaR is also a breakaway from conventional fre-
quentist thinking. It adopts a perspective that extreme tails are inherently
unknowable. Questions of “true risk” are empty, and a risk metric’s value
lies in its usefulness. The crucial question to ask is this: Used in a given
application, will buVAR promote prudence and desirable behavior at
financial institutions?
16.6 SPREADSHEET EXERCISES
16.1 The basis risk of a long-short portfolio often contains large kurtosis,
which is understated in VaR because of the lack of tail data. BuVaR
compensates for this weakness. Aim: The spreadsheet illustrates that
buVaR gives a more aggressive result for basis risk as compared to
expected shortfall.
NOTE
1. IMF, “Global Financial Stability Report,” April 2008.
Other Topics
323


CHAPTER 17
Epilogue: Suggestions
for Future Research
T
his book hopes to pave the way for a new approach in measuring risks,
one that is unrestrained by ingrained notions of being independent and
identically distributed (i.i.d.), stationarity, and normality of financial vari-
ables. Without such assumptions, mathematical risk models will lose a
degree of tractability but will gain a practical ability to handle the risk of fat
tails and procyclicality. Essentially, we have lost precision but have gained
accuracy, and hence become less “wrong” in our assessment of risks.
The removal of these strict mathematical conditions leads to an open
arena for further development of the basic bubble value at risk (buVaR)
framework beyond what is described in this book. Some potential directions
for further research include:
1. Calibration: Determining a more elegant theoretical upper limit for
buVaR calibration. In this book, we used an average of the largest daily
changes encountered in all history capped by a possible circuit-breaker
(cap on day loss) imposed by the exchange. The weakness is that some
markets do not have circuit-breakers (such as commodities and
FX markets) and nascent markets will not have enough history of
downturns for proper calibration. Ideally, the upper limit should be
based on some natural limit imposed by the financial system environ-
ment. Such a limit should be intuitively justified and should not come
from mathematical abstractions.
2. Benchmarking: The computation of the bubble measure is highly
dependent on the availability of continuous data. For some emerging
markets, illiquid prices make the measure inaccurate. In such cases,
using benchmarks may be a better alternative, for example, using an
equity index as a benchmark to derive the bubble measure for an illiquid
stock. A potentially better idea is to use a group of suitable benchmarks
325

to derive the bubble measures for all assets in the financial markets.
Using benchmarks can be appropriate because we are actually interested
in the cycle of the general market. And benchmarking will certainly lead
to more stable results.
3. Performance testing: Because buVaR is “non-i.i.d.,” back tests and most
statistical tests will not be meaningful, which leaves us to rely on per-
formance testing not unlike that done for trading systems. In such tests,
the practitioner is more concerned with accuracy (shown up as positive
performance) than with statistical precision. Hence, one area of research
is in designing a better gauge of “goodness” or effectiveness for the
buVaR model. A well-defined test metric will be useful for the purpose
of optimizing the parameters in buVaR.
4. Systemic risk: One advantage of buVaR is that it outputs profit and loss
(PL) vectors, which makes it compatible for use with other new risk tools
such as CoVaR and network models. These new models attempt to
measure the spillover and transmission of systemic risks (see Chapter 12).
Given that buVaR itself preempts many major sell-offs in history, it will
be interesting to explore its effectiveness when used in combination with
network models and Contagion VaR (CoVaR). Will it provide an early
warning of systemic events?
5. Risk indicators: It is worth exploring whether risk indicators can be
used to create an alternative bubble measure for application in buVaR.
A risk indicator is a proprietary indicator, which banks use to gauge
market sentiment; hence, it is usually very cyclical.1 Applying rank
filter (see Section 13.3) to it may produce the necessary counter-
cyclical quality.
6. Ultralow volatility problem: We observed in the tests in Chapter 15 that
on a few occasions (see Figure 15.26) the buVaR peaked and then fell
back just before the crash, even though the bubble measure correctly
timed the crash. This was due to the extremely low volatility that pre-
ceded the crash—the ETL (and VaR) was so low and depressed that,
despite the multiplier effect, the buVaR declined. This is not ideal.
Future research may attempt to correct for this “calm before the storm”
effect. Under such a rare circumstance, the buVaR will dip from its peak
before the crash, but VaR in comparison will always be too late—it will
register a big reading when the crash occurs.
7. By looking at the result charts in Chapter 15, it is apparent buVaR
inherited the plateau effect problem (see Section 2.7) from using the
equally weighted historical simulation approach. It is proper to use
decaying weights so that data rolling off the observation window will
have a smaller impact. We abstain from including a decay factor in
326
INTRODUCTION TO BUBBLE VALUE-AT-RISK (BuVaR)

Chapter 13 because we wish to simplify the illustration of the buVaR
idea and because the author does not have a strong view on the choice
of decay schemes. We leave this to future research.
NOTE
1. A risk indicator is typically a normalized formula with constituents such as the
volatility index (VIX), credit spreads, T-bill yields, Treasury-eurodollar (TED)
spread, FX option’s skew (called risk reversal) and implied volatility, and so on.
Epilogue: Suggestions for Future Research
327


ABOUT THE WEBSITE
T
he website that accompanies Bubble Value at Risk: A Countercyclical
Risk Management Approach (revised edition) is available at www.wiley
.com/go/bubblevalueatrisk and includes documents to supplement the
information in the book where examples are illustrated in the syntax of
Excel functions, so that the reader can easily implement these ideas in a
spreadsheet.
Included are downloadable resources (spreadsheet exercises):
n Description of the Exercises (1 pdf file)
n Chapter 2 (12 Excel files)
n Chapter 4 (6 Excel files)
n Chapter 5 (5 Excel files)
n Chapter 6 (6 Excel files)
n Chapter 7 (3 Excel files)
n Chapter 8 (4 Excel files)
n Chapter 10 (2 Excel files)
n Chapter 12 (3 Excel files)
n Chapter 13 (4 Excel files)
n Chapter 14 (1 Excel file)
n Chapter 16 (1 Excel file)
For more information on the author’s research and presentations, please
visit the author’s website for this book at www.bubble-value-at-risk.com.
329


Bibliography
Acerbi, C. 2002. “Spectral Measures of Risk: A Coherent Representation of Sub-
jective Risk Aversion.” Journal of Banking and Finance 26 (7): 15051518.
Adrian, T., and M. K. Brunnermeier. 2009. “CoVaR.” Princeton University and FRB
of New York Working Paper.
Adrian, T., and H. S. Shin. 2008. “Liquidity, Monetary Policy, and Financial
Cycles.” Current Issues in Economics & Finance 14 (1): 17.
Alexander, C. 2008. Market Risk Analysis: Practical Financial Econometrics.
Chichester, UK: John Wiley & Sons.
Amato, J., and E. Remolona. 2003. “The Credit Spread Puzzle.” BIS Quarterly
Review (December): 5163.
Artzner, P., F. Delbaen, J. Eber, and D. Heath. 1999. “Coherent Measures of Risk.”
Mathematical Finance 9 (3): 203228.
Balkema, A., and L. de Haan. 1974. “Residual Life Time at Great Age.” Annals of
Probability 2 (5): 792804.
Bank for International Settlements. 2008. “Principles for Sound Liquidity Risk
Management and Supervision.”
Bank for International Settlements. 2009. “Findings on the Interaction of Market
and Credit Risk.” BIS Working Paper No. 16.
Bank for International Settlements. 2009. “International Convergence of Capital
Measurement and Capital Standards: A Revised Framework-Comprehensive
Version.”
Bank for International Settlements. 2009. “Revision to Basel II Market Risk
Framework—Final Version.”
Bank for International Settlements. 2010. “Basel III: A Global Regulatory Frame-
work for More Resilient Banks and Banking Systems.”
Bank for International Settlements. 2010. “Basel III: International Framework for
Liquidity Risk Measurement, Standards and Monitoring.”
Bank
for
International
Settlements.
2010.
“Countercyclical
Capital
Buffers:
Exploring Options.” BIS Working Paper No. 317.
Bank for International Settlements. 2010. “Guidance for National Authorities
Operating the Countercyclical Capital Buffer.”
Bank for International Settlements. 2012. “Fundamental Review of the Trading
Book.” BIS Working Paper No. 219.
Beveridge, S., and C. R. Nelson. 1981. “A New Approach to Decomposition of
Economic Time Series into Permanent and Transitory Components with Par-
ticular Attention to Measurement of the Business Cycle.” Journal of Monetary
Economics 7 (2): 151174.
331

Billio, M., M. Getmansky, A. Lo, and L. Pelizzon. 2010. “Measuring Systemic Risk
in the Finance and Insurance Sectors.” MIT Sloan School of Management
Working Paper No. 4774-10.
Black, F., and M. Scholes. 1973. “The Pricing of Options and Corporate Liabilities.”
Journal of Political Economy 81 (3): 637. Retrieved from Business Source
Premier Database.
Bollerslev, T. 1986. “Generalized Autoregressive Conditional Heteroskedasticity.”
Journal of Econometrics 31 (3): 307327.
Boss, M., G. Krenn, C. Puhr, and M. Summer. 2006. “Systemic Risk Monitor: A
Model for Systemic Risk Analysis and Stress Testing of Banking Systems.”
OeNB Financial Stability Report 11.
Boudoukh, J., M. Richardson, and R. Whitelaw. 1998. “The Best of Both Worlds.”
Risk 11: 6467.
Breuer, T., M. Jandacka, K. Rheinberger, and M. Summer. 2008. “Regulatory
Capital for Market and Credit Risk Interaction: Is Current Regulation Always
Conservative?” A Discussion Paper Series 2: Banking and Financial Studies
2008 (14), Deutsche Bundesbank, Research Centre.
Brockmann, M., and M. Kalkbrener. 2010. “On the Aggregation of Risk.” Journal
of Risk 12 (3).
Brunnermeier, M., A. Crockett, C. Goodhart, A. D. Persaud, and H. S. Shin. 2009.
“The Fundamental Principles of Financial Regulation: Preliminary Conference
Draft.” Geneva Reports on the World Economy 11. Geneva, Switzerland:
International Center for Monetary and Banking Studies.
Campbell, S. D. 2005. “A Review of Backtesting and Backtesting Procedures.”
Finance and Economics Discussion Series 200521, Board of Governors of the
Federal Reserve System (U.S.).
Christoffersen, P. 1998. “Evaluating Interval Forecasts.” International Economic
Review 39: 841862.
Coleman, R. 2010. “A VaR Too Far? The Pricing of Operational Risk.” Journal of
Financial Transformation 28: 123129.
Cox, J. C., J. E. Ingersoll, Jr., and S. A. Ross. 1985. “A Theory of the Term Structure
of Interest Rates.” Econometrica 53 (2): 385407.
Crouhy, M., D. Galai, et al. 2000. “A Comparative Analysis of Current Credit Risk
Models.” Journal of Banking & Finance 24: 59117.
Danielsson, J., P. Embrechts, C. Goodhart, C. Keating, F. Muennich, O. Renault,
and H. S. Shin. 2001. “An Academic Response to Basel II.” London: Special
Paper No. 130(May), Financial Markets Group and Economic & Social
Research Council.
Dickey, D. A., and W. A. Fuller. 1981. “Likelihood Ratio Statistics for Auto-
regressive Time Series with a Unit Root.” Econometrica 49 (4): 10571079.
Diebold, F., T. Schuermann, and J. Stroughair. 1999. “Pitfalls and Opportunities in
the Use of Extreme Value Theory in Risk Management.” Journal of Risk
Finance 1 (2): 3035.
Dionisio, A., R. Menezes, and D. A. Mendes. 2007. “Entropy and Uncertainty
Analysis in Financial Markets.” Quantitative Finance Papers, Arxiv.Org.
332
BIBLIOGRAPHY

Eisenberg, L., and T. H. Noe. 2001. “Systematic Risk in Financial Systems.” Man-
agement Science 47: 236249.
Elsinger, H., A. Lehar, and M. Summer. 2006a. “Risk Assessment for Banking
Systems.” Management Science 52 (9): 13011314.
Elsinger, H., A. Lehar, and M. Summer. 2006b. “Using Market Information for
Banking Systems Risk Assessment.” International Journal of Central Banking
2 (1):13765.
Elton, E. J., M. J. Gruber, D. Agrawal, and C. Mann. 2001. “Explaining the Rate
Spread on Corporate Bonds.” Journal of Finance 56 (1): 247277.
Embrechts, P., C. Klüppelberg, and T. Mikosch. 1997. Modeling Extremal Events
for Insurance and Finance. Springer, Berlin.
Embrechts, P., A. McNeil, and D. Straumann. 1999. “Correlation and Dependency
in Risk Management: Properties and Pitfalls.” ETH Preprint.
Engle, R. F. 1982. “Autoregressive Conditional Heteroscedasticity with Estimates of
the Variance of UK Inﬂation.” Econometrica 50: 9871007.
Engle, R. F., and C. W. J. Granger. 1987. “Co-Integration and Error Correction:
Representation, Estimation and Testing.” Econometrica 55 (2): 251276.
Engle, R. F., and S. Manganelli. 2004. “Caviar: Conditional Autoregressive Value at
Risk by Regression Quantiles.”Journal of Business & Economics 4 (23):
187212.
Frey, R., A. McNeil, and M. Nyfeler. 2001. “Copulas and Credit Models.” Risk 14,
October: 11114.
Financial Service Authority. 2009. The Turner Review: A Regulatory Response to
the Global Banking Crisis. Financial Service Authority Corporate Document.
Financial Service Authority. 2010. The Prudential Regime for Trading Activities:
A Fundamental Review. FSA Discussion Paper 10/4.
Finger, C. 1997. “A Methodology to Stress Correlations.” RiskMetrics Monitor,
4th Quarter: 311.
Finger, C. 2008. “The Once Holy Grail.” RiskMetrics Research Monthly (January).
Finger, C. 2009a. “Comment on the Basel Committee on Banking Supervision’s
Consultative Document Entitled Guidelines for Computing Capital for Incre-
mental Risk in the Trading Book.” Riskmetrics Group.
Finger, C. 2009b. “IRC Comments.” RiskMetrics Research Monthly (February).
Fisher, R., and L. Tippett. 1928. “Limiting Forms of the Frequency Distribution of
the Largest or Smallest Member of a Sample.” Proceedings of the Cambridge
Philosophical Society 24: 180190.
Frankland, R., A. D. Smith, T. Wilkins, E. Varnell, A. Holtham, E. Bifﬁs, S. Eshun,
and D. Dullaway. 2008. “Modelling Extreme Market Events.” Working paper
presented to the Institute of Actuaries, U.K.
Georges, D., G. Geneviève, et al. 2005. “Default Risk in Corporate Yield Spreads.”
Financial Management 39 (2): 707731.
Ghysels, E., and D. Osborn. 2001. The Econometric Analysis of Seasonal Time
Series. Cambridge, U.K. Cambridge University Press.
Gnedenko, B. 1943. “Sur la distribution limite du terme maximum d’une série
aléatoire.” Ann. of Math 44: 423453.
Bibliography
333

Gomez, E., J. C. Mendoza, and N. Zamudio. 2012. “CrashMetrics: An Application
for Colombia.” Temas de Estabilidad Financiera, No. 69.
Gulko, L. 1999. “The Entropy Theory of Stock Option Pricing.” International
Journal of Theoretical and Applied Finance 2: 331355.
Hallerbach, W. G. 2002. “Decomposing Portfolio Value-At-Risk: A General Anal-
ysis.” Journal of Risk 5(2).
Heston, S. 1993. “A Closed Form Solution for Options with Stochastic Volatility
with Applications to Bonds and Currency Options.” Review of Financial Studies
6 (2): 327343.
Hodrick, R., and E. Prescott. 1981. “Postwar U.S. Business Cycles: An Empirical
Investigation.” Reprinted in Journal of Money, Credit, and Banking 29: 116.
Hua, P., and P. Wilmott. 1998. “CrashMetrics Technical Report: Basic Methodol-
ogy: Worst-Case Scenarios and Platinum Hedging.” Website http://paul
.wilmott.com/crashmetrics.cfm.
Hull, J. 2008. Options, Futures and Other Derivatives, 7th ed. Upper Saddle River,
NJ: Prentice Hall.
Hull, J., and A. White. 1998. “Incorporating Volatility Updating into the Historical
Simulation Method for VaR.” Journal of Risk 1: 519.
International Monetary Fund. 2008. “Containing Systemic Risks and Restoring
Financial Soundness.” Global Financial Stability Report (April 2008).
J. P. Morgan/Reuters. 1996. RiskMetrics Technical Document. 4th ed.
Johansen, S. 1988. “Statistical Analysis of Cointegration Vectors.” Journal of Eco-
nomic Dynamics and Control 12: 231254.
Jorion, P. 2002. “Fallacies About the Effects of Market Risk Management Systems.”
Financial Stability Review, 115127.
Jorion, P. 2003. Financial Risk Manager Handbook. 2nd ed. Hoboken, NJ: John
Wiley & Sons.
Jorion, P. 2007. Value at Risk. 3rd ed. New York: McGraw-Hill.
Knight, F. H. 1921. Risk, Uncertainty, and Proﬁt. 1st ed. Boston, MA: Hart,
Schaffner & Marx; Houghton Mifﬂin Co.
Koenker, R., and G. W. Bassett. 1978. “Regression Quantiles.” Econometrica 46: 3350.
Lavine, M. 2008. Introduction to Statistical Thought. Published at www.stat.duke.
edu/~michael/book.
Lo, A. 2009. “The Feasibility of Systemic Risk Measurement.” Written Testimony
for the House Financial Services Committee Hearing on Systemic Risk Regu-
lation. October.
Mandelbrot, B. 1963. “The Variation of Certain Speculative Prices.” Journal of
Business 36: 394419.
Markowitz, H. M. 1952. “Portfolio Selection.” Journal of Finance 7 (1): 7791.
Martin, M., H. Lutz, and C. Wehn. 2011. “A Practical Anatomy of Incremental Risk
Charge Modeling.” The Journal of Risk Model Validation, Vol. 5, Summer,
4560.
McDonald, L. G., and P. Robinson. 2009. A Colossal Failure of Commonsense:
The Inside Story of the Collapse of Lehman Brothers. New York Random
House.
334
BIBLIOGRAPHY

Merton, R. C. 1974. “On the Pricing of Corporate Debt: The Risk Structure of
Interest Rates.” Journal of Finance 29 (2): 449470.
Meucci, A. 2011. “The Prayer: Ten-Step Checklist for Advanced Risk and Portfolio
Management.” http://symmys.com/node/63.
Mini, J., and J. Y. Xiao. 2001. “Return to RiskMetrics: The Evolution of a Stan-
dard.” RiskMetrics Group.
Morris, S., and H. S. Shin. 1999. “Risk Management with Interdependent Choice.”
Oxford Review of Economic Policy 15: 5262.
Nelson, D. B. 1991. “Conditional Heteroskedasticity in Asset Returns: A New
Approach.” Econometrica 59: 347370.
Oest, T., and J. Rollbuhler. 2010. “Detection and Analysis of Correlation Clusters
and Market Risk Concentration.” Wilmott (July).
Pérignon, C., and D. R. Smith. 2010. “The Level and Quality of Value-at-Risk
Disclosure by Commercial Banks.” Journal of Banking & Finance 34:
362377.
Pickands, J. 1975. “Statistical Inference Using Extreme Order Statistics.” Ann. Statist
3: 119131.
Ravn, M., and H. Uhlig. 2002. “On Adjusting the HP-Filter for the Frequency of
Observations.” Review of Economics and Statistics 84 (2): 371376.
Rebonato, R. 1999. “The Most General Methodology to Create a Valid Correlation
Matrix for Risk Management and Option Pricing Purposes.” Quantitative
Research Centre of the NatWest Group.
Rebonato, R. 2003. “Theory and Practice Of Model Risk Management.” In Modern
Risk Management: A History (Ed. Sarah Jenkins). London: RiskWaters Group.
Rebonato, R. 2007. Plight of the Fortune Tellers: Why We Need to Manage
Financial Risk Differently. Woodstock, U.K.: Princeton University Press.
Rebonato, R. 2010. Coherent Stress Testing: A Bayesian Approach to the Analysis of
Financial Risk. Chichester, U.K.: John Wiley & Sons.
Rosenblatt, M. 1952. “Remarks on Multivariate Transformation.” Annals of
Mathematical Statistics 23: 470472.
Samuelson, P. 1947. Foundations of Economic Analysis. Cambridge, MA: Harvard
University Press.
Satchkov, D. 2010. “When Swans Are Grey: VaR as an Early Warning Signal.”
Journal of Risk Management in Financial Institutions 3 (4): 366379.
Sebastian, S., and K. Christoph. 2009. “Market Liquidity Risk—An Overview.”
CEFS Working Paper Series, No. 4.
Shannon, C. E. 1948. “A Mathematical Theory of Communication.” Bell System
Technical Journal 27: 379423, 623656.
Sharpe, W. 1964. “Capital Asset Prices: A Theory of Market Equilibrium under
Conditions of Risk.” Journal of Finance 19: 425442.
Speth, H.-Th. 2004. “The BV4.1 Procedure for Decomposing and Seasonally
Adjusting Economic Time Series.” Methodenberichte 3 (ed. Federal Statistical
Ofﬁce).
Sun, H., I. Nelkon, G. W. Han, and J. P. Guo. 2009. “Errors of VaR by Overlapping
Intervals.” AsiaRisk (April): 5055.
Bibliography
335

Taleb, N. 1997. Dynamic Hedging: Managing Vanilla and Exotic Options. New
York: John Wiley & Sons.
Taleb, N. 2007. The Black Swan: The Impact of the Highly Improbable. New York:
Random House.
Taleb, N. 2009a. “Common Errors in Interpreting the Ideas of the Black Swan and
Associated Papers.” Working paper.
Taleb, N. 2009b. “Errors, Robustness, and the Fourth Quadrant.” International
Journal of Forecasting 25(4): 744759.
Thomas, H., and Z. Q. Wang. 2005. “Interpreting the Internal Ratings-Based
Capital Requirements in Basel II.” Journal of Banking Regulation 6: 274289.
Vasicek, O. 1991. “Limiting Loan Loss Probability Distribution.” KMV Corpora-
tion, San Francisco, CA.
Wilde, T. 1997. “Credit Risk+, A Credit Risk Management Framework.” Credit
Suisse First Boston International.
Wilkens, S., J. B. Brunac, and V. Chorniy. 2012. “IRC and CRM: Modelling
Framework for the ‘Basel 2.5’ Risk Measures.” Working paper SSRN.
Wilmott, P. 2007. Paul Wilmott Introduces Quantitative Finance. 2nd ed. Chiche-
ster, U.K.: John Wiley & Sons.
Wong, M. 2011. “Market BuVaR: A Countercyclical Risk Metric.” Journal of Risk
Management in Financial Institutions 4 (4): 114.
Wong, M. 2011. “Credit BuVaR: Asymmetric Spread VaR with Default.” Journal of
Risk Management in Financial Institutions 5 (1): 8695.
Zangari, P. 1996. “An Improved Methodology for Measuring VaR.” RiskMetrics
Monitor. Reuters/J.P. Morgan, 725.
Zumbach, G. O. 2006. “The RiskMetrics 2006 Methodology.” RiskMetrics Group.
336
BIBLIOGRAPHY

Index
Absolute returns, 77, 78
Acceptance tests
cyclicality, 304–306
event timing, 297–304
visual checks, 285–297
Accuracy, 11, 193, 247, 270, 325, 326
Adaptive moving average (AMA), 252,
262, 321
Advanced VaR models
about, 111
conditional autoregressive value-at-risk
(CAViaR), 114–116
Extreme Value Theory (EVT), 116–122
Hull-White VaR, 113, 114
hybrid historical simulation VaR, 111, 112
Aggregation of VaR, 125, 126, 177, 190,
192–195, 259
The Alchemy of Finance (Soros), 222
Algo trading, 270
Alternative hypothesis, 39
Analytical tools for VaR, 127–131
Animal spirits, 4, 6
Arrow of time, 140
Asian currency crisis (1997-1998), 169, 193,
201, 223, 266, 288, 300, 301, 303
Asset & liability management (ALM), 182
Asset allocation, 46, 47
Asset bubbles, 3, 4, 12, 300–302, 318, 321
Asset liquidity risk, 208
Asset-speciﬁc risk, 73
Asynchronous data, 171
At-the-money (ATM), 72, 74, 99, 100
Augmented Dickey-Fuller (ADF(q))
stationarity test, 40, 41, 51, 63
Austrian Nationalbank (OeNB), 233
Autocorrelation
about, 33–36
autocorrelation function (ACF),
34–36, 44, 62
in time scaling computation, 134
Autoregressive Conditional
Heteroscedasticity (ARCH) model,
41–45, 62
Autoregressive processes (AR(p)), 20–22, 34,
35, 56, 133, 134
Back testing
and buVaR, 265–267, 326
and credit buVaR, 319
and data integrity, 171
distortion of results, 166
frequency back test, 159–162
and whole distribution test, 165
Bad loans, 281
Bank for International Settlements (BIS), 155,
183, 206, 211–214
Bank of England (BOE), 237, 238
Bank of Japan (BoJ), 288, 301, 303
Banking book, 154, 178, 202, 205, 207–209,
217, 319, 320
Basel Committee of Banking Supervision
(Basel)
about, 199
Basel 2.5, 177, 206–211, 213
Basel Accord (Basel I), 177, 200, 201, 215,
216
Basel II, 105, 177, 201–203, 206, 215, 216
Basel III, 154, 177, 182, 203, 210–216,
240, 319
Basel Rules, 132–136
role of, 199
trafﬁc light approach, 161
VaR approach, adoption of, 5, 6
Basic assumptions of VaR, theoretical
problems with
about, 139
coherence and expected shortfall, 154, 155
constant versus stochastic volatility and
production of fat tails, 145–148
entropy, 140, 141
337

Basic assumptions of VaR, theoretical
problems with (continued)
extremistan, 148–151, 246
i.i.d. assumption, 144, 145. See also
Independent and identically distributed
(i.i.d.) returns
lagging indicators, 151, 318
leverage effect, 141, 142
negative volatility skew, 142, 143
procyclicality, 153, 154
regime change, 151, 152, 246
skewness, 142–144, 246
volatility clustering, 144, 145
Basis risk, 104–106, 315–317
Bayesian statistics, 15, 16, 149, 205
Behavioral ﬁnance, 6, 46
Benchmarks and benchmarking, 48, 74, 104,
105, 227–230, 277, 325, 326
Berlin procedure, 56
Bespoke products, 102, 171, 183, 185, 214
Best ﬁt, 59
Beta approach, 48, 105, 106, 129
Beta model, 5, 48
Biasness, 160, 161
Bid-ask cost, 182
Bid-ask spread, 184, 185
Bimodality, 223
Black Monday, 148, 223, 258, 300, 301
The Black Swan (Taleb), 6
Black Swans, 6, 11, 148–151, 188,
203, 261, 268
Black-Scholes option pricing model,
5, 85, 86, 91
Block maxima method, 119
Bonds, 4, 5, 74, 75, 98, 99, 104,
126, 281, 292
Brute force method, 52, 229, 230
Bubble value-at-risk (buVaR)
about, 247
and banking book, 319, 320
and basis risk, 315–317
calibration, 251, 254–257
capital buffer, adequacy of, 265–268
common sense approach of, 8–12
countercyclical, 318
credit buVaR. See Credit buVaR
cyclicality tests, 304–306
and diversiﬁcation, 315
elements of versus conventional VaR,
247–250
event timing tests, 297–304
and extremistan, 270, 271
and hidden assumptions, 268, 269
and i.i.d. assumption, 269, 270
inﬂator, 251, 254–259, 262–264, 304–306
joint distribution, effect on, 262–264
measuring the bubble, 250–254
need for, 245–247
and postmodern risk management,
321–323
and regulatory reform, 317–319
and risk factors, 264, 265
scope of, 264, 265
tail-risk measure, selecting, 259–262
tops and bottoms, predicting, 321
visual checks, 285–297
Bunching test (independence test), 163–165
Business cycles, 7, 55, 180, 205, 215, 246,
270, 296, 304–306
BuVaR. See Bubble value-at-risk (buVaR)
Calibration (in buVaR), 251, 254–257, 325
Call options, 99
Cap-ﬂoor options, 73
Capital Adequacy Directive (CAD), 200
Capital Asset Pricing Model (CAPM), 105
Capital buffer
adequacy of, 265–268
and Basel Rules, 132, 136, 150, 151, 206,
217, 319
and buVaR, 254, 265, 268, 270, 306,
312, 321
capital conservation buffer, 213
countercyclical capital buffer, 11, 12, 154,
206, 213, 214, 306, 319
double counting, 217
scaling for unequal response time to
risk, 320
Central banks, 78, 144, 169, 199, 237, 265,
292, 296, 320. See also Asian currency
crisis (1997-1998)
Central counterparty (CCP), 240
Central limit theorem (CLT), 25, 26, 118,
122, 225
Cholesky decomposition, 88
Cholesky matrix L, 54, 55, 88, 90
Christoffersen test, 163
Classical decomposition, 55–58, 247
Clearing procedure, 234–236
Clumping effect of random sampling, 92, 93
338
INDEX

Coherence, 154, 155, 259, 261
Cointegration analysis, 50–52
Compensating controls, 270
Component VaR, 129, 130
Comprehensive risk capital charge, 208
Conditional autoregressive value-at-risk
(CAViaR), 114–116, 246, 269
Conditional mean, 59
Conditional probability, 119, 269
Conditional VaR (cVaR), 61, 155
Conditional variance, 42
Consistent estimates, 20
Constant proportion portfolio insurance
(CPPI) strategy, 173, 174
Contagion default, 236, 237
Contagion risk, 169, 204, 222, 225, 230–240
Contagion VaR (CoVaR), 230–233, 319
Conventional methods of computing VaR
about, 83, 84
and basis risks, 104–106
and beta approach, 105, 106
and convexity risk, 97–99
and fat tails, 101, 102
and hidden correlation, 102–104
historical simulation VaR (hsVaR). See
Historical simulation VaR (hsVaR)
Monte Carlo VaR (mcVaR), 83, 89–94,
96–98, 100–102
and optionality risk, 99–101
parametric VaR (pVaR), 83–89, 99
and premium risk, 106, 107
variance-covariance (VCV) VaR, 84–89
Convexity, 85, 86
Convexity risk, 97–99
Convolution, 189
Copula function, 33
Correlation. See also Autocorrelation
and cointegration, 50
default, 180
described, 29, 30
hidden, 102–104
historical, 72, 102–104
implied, 72, 102–104, 259–261
linear (Pearson’s), 29–31, 33, 46,
143, 305, 306
matrix, 47, 88, 89
problems with, 30–32
products, 102–104
rank, 32, 33
risk, 72, 103, 104
serial, 34, 35, 37, 51, 133, 134, 144, 225,
248–250
spurious, 37
stochastic, 103
Correlogram (autocorrelation function),
34, 62
Countercyclical capital buffer, 11, 12, 154,
206, 213, 214, 306, 319
Countercyclical regulatory capital, 249
Counterparty credit risk, 177, 210, 211
Counterparty risk, 240
Country-speciﬁc risk, 72, 73
Covariance, 29, 47
Cox-Ingersoll-Ross (CIR) model, 91
Crash coefﬁcient, 227, 228
CrashMetrics, 226–230
Credit buVaR
about, 273–275
advantages of over VaR, 318, 319
characteristics of, 280–282
deﬁned, 279
inﬂator, 275–279
interpretation of, 282–284
model formulation, 276–278
parameter tuning, 306–313
response function behavior, 278–280
Credit crisis. See Global credit crisis of 2008
Credit default obligations (CDOs), 183, 208
Credit default swaps (CDS), 74, 78, 105,
185, 186, 210
Credit escalation, 274
Credit risk, 177–182, 193, 204, 320
Credit risk charge (CRC), 200
Credit spread puzzle, 312, 313
Credit spread risk factors, 265
Credit spread VaR, 207, 273
Credit valuation adjustment (CVA VaR),
210, 211, 213–215
Credit VaR, 178–182
Credit01, 85
CreditMetrics, 178–182, 273
CreditRisk+, 234
Critical value (CV), 39
Crowded trades, 230
Cumulative distribution function (CDF), 23,
24, 52, 53, 119, 121
Currency controls and depegs, 169, 170
Curse of dimensionality, 96
Cycle breaks, 247, 248, 250
Cycle compression, 248–250
Index
339

Data integrity problems, 171, 172
Data interpolation, 171
Data sets, 16, 21, 68, 74, 171, 227–229, 239
Data snapping, 68, 171
Deal information, obtaining, 67, 68
Decay factor, 42–44, 49, 50, 95, 112, 114,
131, 326, 327
Decomposition, 55–58, 125, 247
Default correlation, 180
Default risk, 240, 273, 275, 284, 318
Delta, 85, 86
Delta-gamma approach, 86, 98, 226–227
Derivatives pricing, 52–55, 150, 204
Deterministic trend time series process, 53, 54
Deterministic variable, 16
De-trending a time series, 19, 21, 56, 58
Distribution
cumulative distribution function (CDF),
23, 24, 52, 53, 119, 121
elliptical, 30–32, 155
families, 118
fat tail, 18, 26, 40, 44, 89, 90, 94, 101,
102, 117, 118, 148–150, 203, 247, 248
frequency distribution (operational risk),
188, 189
generalized extreme value distribution
(GEV), 118, 119
Generalized Pareto Distribution (GPD),
119, 120
joint, 30–32, 154
leptokurtic, 18, 225
lognormal, 53
loss distribution (operational risk), 189
normal (Gaussian distribution), 24, 25
Pareto distributions, 118–120
PL (proﬁt and loss) distribution, 16, 68, 79
probability distribution, 16
severity (operational risk), 188, 189
stable, 134
standard normal, 25, 26, 39
student-t distribution, 39, 118
uniform, 52
Diversiﬁcation, 45–47, 126, 154, 192,
315–317
Drawdowns, 266
Duration risk, 72
Economic cycle, 215, 216
Economic cycle reserve, 206
Efﬁcient frontier, 48
Efﬁcient market hypotheses (EMH), 172, 203,
222, 247, 269, 279, 321
Eigen decomposition, 88
Eigenvalues, 88
Eigenvectors, 88
Elliptical distribution, 30–32, 155
End of day (EOD) data snapping, 68, 171
Engle-Granger regression, 51
Enron, 177, 190, 201
Enterprise risk management, 70
Entropy, 140, 141
Equilibrium, 250–254
Error bands, 159, 160
European Systemic Risk Board (ESRB), 239
Exceedances, 29, 119
Excel functions
AVERAGE (mean), 17
CORREL (correlation), 30
correlated random numbers, 55
CORREL(RANK) (rank correlation), 33
COVAR (covariance), 29
INTERCEPT, 36
KURT (kurtosis), 18
MDURATION (modiﬁed duration), 98
NORMSINV, 39
PERCENTILE, 26
PRICE, 308
RAND (random generator), 52, 53, 92
RSQ, 36
SKEW (skewness), 18
SLOPE, 36
STDEV (standard deviation), 18, 41
VAR (variance), 18
VaR calculation, 28, 29
YIELD, 277
Excel tools
correlation matrix, 47
regression, 36, 39, 48
Exceptional loss (XL), 188, 190. See also
Black Swans
Exotic options, 5, 72, 78, 92, 93,
101, 227, 228
Expected exposure (EE), 210, 211
Expected loss (EL), 188–190
Expected shortfall (ES), 155, 214
Expected tail loss (ETL), 155, 259, 261, 262,
264, 286–297. See also Conditional VaR
(cVaR)
Expected value (expectation), 16, 17
Exponential GARCH model (EGARCH), 45
340
INDEX

Exponentially weighted moving average
(EWMA)
and adaptive moving average (AMA),
252, 253
bunching test, 163, 164, 166
in Hull-White VaR, 113
in hybrid historical simulation VaR, 111
use of in RiskMetrics modeling
of VaR, 84
volatility measurement, 41–45, 49, 84,
145, 151, 152
Extreme Value Theory (EVT), 116–122
Extreme values, 118
Extremistan, 6, 7, 29, 148–151, 246, 249,
270–271
Fat tails
basis risk, 315–317
danger of, 101, 102
distribution, 18, 26, 40, 44, 89, 90, 94,
101, 102, 117, 118, 148–150, 203,
247, 248
events, 6, 32, 89, 149, 203, 213, 216,
247, 317
losses, 216, 265, 317, 318
modeling, 101, 102, 148, 203, 204, 247
phenomena, 139, 247, 248, 270, 271
and risk classes, 193
and stochastic volatility, 145–148
Feedback loops, 6, 203, 213, 222–225
Financial bubbles, 153
Financial Service Authority (FSA), 7, 153,
202, 206. See also Turner Review
Financial Services Oversight Council
(FSOC), 239
Financial Stability Board (FSB), 239
Firm liquidity risk, 177, 182
Fisher-Tippett theorem, 117
Forecasting, 57–59, 321
Forward rate agreement (FRA), 73
Fourier series, 57
Fourth quadrant, 149, 150
Fréhet family distribution, 118
Frequency back test (unconditional coverage
test), 160–162
Frequency distribution, 188, 189
Frequentist statistics, 15–18, 145, 149
Full revaluation, 91
“Fundamental Review of the Trading Book”
(BIS), 155, 214, 217
Funding liquidity risk, 182, 320
Futuristic information, 173, 174
Gaming the VaR system, 135,
174, 175
Gamma, 85, 86, 98, 101
Gaussian distribution (normal distribution),
24, 25
General equilibrium theory, 222
General market risk charge (GMRC), 202
General Motors, 283, 307, 309
Generalized Autoregressive Conditional
Heteroscedasticity (GARCH), 41–45, 56,
62, 145, 163, 269
Generalized extreme value distribution
(GEV), 118, 119
Generalized Pareto Distribution (GPD),
119, 120
Geometric Brownian motion (GBM), 53, 54,
90–92, 259, 260
Global credit crisis of 2008
about, 3, 4
and buVaR, 8, 297, 300
and credit buVaR, 273–275, 319
and extremistan, 7. See also Extremistan
and fat tails, 101
and regulatory reform, 199, 202–205,
208, 213, 216
and response time, 320
and risk classes, 177, 182–184
and systemic risk, 221–225, 238
Turner Review, 7, 8, 11, 153, 202–206,
216, 238, 317, 318
and VaR, 27, 75, 78, 83, 101, 139,
175, 245
Global regulation of systemic risk,
need for and proposed system,
239, 240
Going concern, 207
Goldman Sachs commodity index, 74
Goodness of model, 114, 132, 159, 161,
285, 326. See also Model testing
Greeks, 5, 84–86
Gumbel family distribution, 118
Hedge ratio, 48
Herd mentality and herding behavior, 6,
205, 222
Hidden correlation, 102–104
Histograms, 16
Index
341

Historical simulation VaR (hsVaR)
about, 93–95
accuracy of, 93, 95
and buVaR, 258–261
CAViaR compared, 116
computing, 69, 94, 95
and credit spread VaR, 273
and fat tails, 101
and Hull-White VaR, 113, 114
hybrid, 111, 112
and incremental VaR, 130, 131
and PL vectors, 68, 69, 98
and regime shifts, 151–153
scaling, 113
use of in system architecture, 69
weaknesses of, 96
Hits, 114, 115, 162–164
Holding period (liquidity horizon), 132, 133
Homogeneity, 154
HP ﬁlter, 305
Hull-White VaR, 113, 114, 163
Hybrid historical simulation VaR, 111, 112
Hypercube (dataset), 227–229, 239
Hypothesis testing, 39, 40
Idiosyncratic risk, 202
i.i.d. See Independent and identically
distributed (i.i.d.) returns
Implied correlation, 72, 102–104, 259–261
Implied volatility, 45
Incremental risk charge (IRC), 206–208, 214
Incremental VaR, 130, 131
Independence test (bunching test), 163–165
Independent and identically distributed
(i.i.d.) returns
and buVaR, 269, 270, 322
and central limit theorem, 25, 26,
118, 119, 122, 225
and extremistan, 149
maximas and minimas (extreme values),
118
and random number generation, 92
and square-root of time scaling, 133
VaR assumption, 9, 11, 18–22, 96, 139,
144, 145, 159, 269, 270
and volatility clustering, 144, 145, 225
Inﬂator (in buVaR), 251, 254–259,
262–264
Innovation, 80
Insider information, 321
Internal models approach, 5, 6, 161, 177,
190, 200–202, 206, 209, 215
Internal Rating Based (IRB) approach, 7, 201,
202, 205, 207, 208, 217
Internet bubble (2000), 300–302
In-the-money (ITM), 100
Inverse problems, 148
Inverse transformation, 52
Jaynes, E. T., 145
Johansen method, 51
Joint distribution, 30–32, 154, 262–264
JP Morgan, 5, 178–182
Kendall’s tau, 32, 33, 305, 306
Keynes, John, 4, 11
Knight, Frank, 205
Knightian uncertainty, 204, 205
Kolmogorov-Smirnoff test, 165
Kurtosis, 16, 18, 149, 315, 316
Lagging indicators, 72, 151, 153,
273–275, 318
Laplace distribution, 40
Law of Large Numbers, 20, 21
Leading indicators, 318
Lead-lag relationships, 38
Lehman default and bankruptcy, 269, 274,
307–309, 320
Leptokurtic distribution, 18, 225
Level prices, 51
Leverage effect, 7, 45, 141, 142, 248
Leverage ratio, 213
Leverage targeting, 223, 224
Linear approximation (delta approach), 86
Linear correlation (Pearson’s), 29–31, 33, 46,
143, 305, 306
Liquidity coverage ratio, 183, 211, 212
Liquidity horizon (holding period), 132,
133, 207, 208
Liquidity ratios, 211–213
Liquidity risk, 177, 182–186, 193, 214
Liquidity spiral, 230
Liquidity stress testing, 211
Liquidity-adjusted VaR (L-VaR), 183, 184
Ljung-Box statistic, 163, 164
Log returns, 77, 78
Log-likelihood function, 49, 120
Lognormal distribution, 53
Lognormal model (short rate model), 91
342
INDEX

Loss distribution, 189–191
Loss given default (LGD), 319
Loss spiral, 225
Low-discrepancy sequence, 92
Macro hedges, 150
Margin spiral, 225
Market liquidity risk, 183, 214
Market risk, 70, 71, 177, 193, 214, 240, 320
Market risk charge (MRC), 132, 200,
202, 209
Markowitz, Harry, 5, 45
Markowitz portfolio theory, 5, 30, 32, 45–48,
70, 87, 102, 126, 321, 322
Mark-to-market (MTM), 8, 78, 153,
182, 183
Maturity transformation, 239
Maximum likelihood method (MLE), 48–50,
269, 270
Mean, 16, 17, 19
Mean adjustment, 80, 81, 262
Mean excess function, 119, 120
Median, 26
Mediocristan, 6, 149
Merton’s model, 180
Minimum capital requirements, 199–202,
213. See also Basel Committee of
Banking Supervision (Basel)
Mixture of normal model, 101, 102
Model risk, 172–174, 183
Model testing
about, 159
acceptance tests for buVaR. See
Acceptance tests
bunching test (independence test), 163–165
frequency back test (unconditional
coverage test), 160–162
precision test, 159, 160
whole distribution test, 165, 166
Modern portfolio theory. See Markowitz
portfolio theory
Moments matching, 91
Monotonicity, 154
Monte Carlo (MC) simulation, 52–55, 92
Monte Carlo VaR (mcVaR), 83, 89–94,
96–98, 100–102
Negative rates, 78
Nested Monte Carlo simulation, 92
Net stable funding (NSF) ratio, 211, 212
Network externalities, 204, 222
Nondeliverable forwards (NDF), 78
Nonlinearity, 5, 99, 130
Nonmarket risk, 70, 71
Nonpositive semideﬁnite matrix, 88
Normal distribution (Gaussian distribution),
24, 25
Normality assumption, 269
Null hypothesis, 39–41
Observation period (window length), 28, 76,
77, 151, 250, 251
Off-the-run bonds, 104
Oil price bubble (2008), 297, 299,
300, 304, 305
One-factor model (short rate model), 91
On-the-run bond issues, 104, 281
Operational risk, 177, 187–190, 193,
202, 320
Operational risk charge (ORC), 201, 202
Operational risk VaR (OpVaR), 188–190
Optimization algorithm, 228–230
Optionality risk, 99–101
Options, 5, 45, 46, 71–74, 99–101, 103, 106,
107, 142, 144, 228, 322. See also
Exotic options
Ordinary least squares (OLS), 35–38, 51, 269
Out-of-the-money (OTM), 100, 106, 142
Overnight rates, 78, 296
Over-the-counter (OTC) markets, 183, 185,
213, 240
Pair trading, 50, 51
Parametric VaR (pVaR), 83–89, 99, 259–261
Pareto distributions, 118, 119
Peaks-over-thresholds (POT) method,
119–122
Pearson’s (linear) correlation, 29–31, 33, 46,
143, 305, 306
Performance testing, 326
Pillars (tenor buckets), 72, 73
Pillars of supervision (Basel II), 201. See also
Basel Committee of Banking Supervision
(Basel)
PL (proﬁt and loss)
aggregation of VaR, 125, 126, 177, 190,
192–195, 259
buVaR expected tail loss (ETL), 259
distributions, 16, 68, 79
summation of across scenarios, 94, 136
Index
343

PL (profit and loss) (continued)
VaR as loss quantile of PL distribution, 79
vectors, 69, 77, 80, 81
Plateau effect, 42–44, 326
Platinum Hedging, 226, 229
Point estimates, 29, 154, 171
Point-in-time, 8, 205, 215
Portfolio insurance, 222, 223
Portfolio theory. See Markowitz
portfolio theory
Positive semideﬁnite matrix, 88, 89
Postmodern portfolio theory, 322
Power law scaling, 134
Precision test, 159, 160
Premium risk, 106, 107
Price series, 22, 77, 78
Price-based risk factors, 264, 265
Pricing model risk, 172, 173
Probability density function (PDF), 23–25
Probability distribution, 16
Probability of default (PD), 319
Probability Theory: The Logic of Science
(Jaynes), 145
Procyclicality, 7, 8, 153, 154, 203, 205, 206,
209, 215, 223–225, 317
Product control team, 68
Program trading, 21, 222, 223
Pseudo-arbitragers, 172, 173
Put options, 99
PV01, 85, 98
Quant trading, 270
Quantile function, 84
Quantile regression model (QRM), 58–61,
114, 269
Quantile scaling, 135, 136
Quantiles, 26–28
Quantitative (quant) modeling, 139
Quasi-random number generator, 92
Random walk with drift process, 53
Rank correlation, 32, 33
Rank ﬁlter, 252–254
Rates cleaning, 171
Rates-based risk factors, 265
Rating agencies, 178, 205, 273, 313
Reﬂexivity, 6, 221–225
Regime shifts (game changers), 151, 152,
246, 261, 318
Regression models, 35–38
Regulatory arbitrage, 206, 207, 217
Regulatory reform
about, 4, 199
Basel Committee. See Basel Committee of
Banking Supervision (Basel)
and buVaR, 317–319
ideal capital regime, 215–217
new framework for trading book, 214
Turner Review. See Turner Review
Regulatory reporting, 132. See also Reporting
Relative returns, 77, 78
Rent extraction, 203
Reporting
about, 125
aggregation of VaR, 125, 126
analytical tools, 127–131
Basel Rules, 132. See also Basel Committee
of Banking Supervision (Basel)
diversiﬁcation, 126, 127
granularity, 69
quantile scaling, 135, 136
time scaling, 132–135
VaR limits, 126, 127
Research topics, suggested, 325–327
Residual errors, 35
Residual sum squares (RSS), 36, 59
Response time to calamity, 319, 320
Return series, generating from price series,
77, 78
Reverse stress tests, 229
Risk, deﬁned, 81
Risk Assessment Model for Systemic
Institutions (RAMSI), 237, 238
Risk classes
about, 177
aggregation problem, 177, 190, 192–195
characteristics of, 194
counterparty credit risk, 177
credit risk, 177–182, 193
liquidity risk, 177, 182–186, 193
market risk, 177, 193
operational risk, 187–190, 193
Risk culture, 174, 175
Risk decomposition, 69, 70, 91, 94, 95, 105,
125, 129
Risk dimensions, 71, 72
Risk factor families, 264, 265
Risk factor mapping, 70–74
Risk factor proxies, 75, 76, 104
Risk factor universe, 72–74
344
INDEX

Risk identiﬁcation and classiﬁcation
(taxonomy), 187
Risk indicators, 326
Risk management system architecture,
67–69
Risk of risk assessment, 4
Risk sensitivities. See Greeks
RiskMetrics, 5, 42, 84
Riskometer, evolution of, 4–6
Risks not in VaR (RNIV), 96
Risk-weighted asset (RWA), 200, 213
Rosenblatt transformation, 165
Saturn ring effect, 227, 228
Scaling
power law scaling, 134
quantile scaling, 135, 136
square-root of time scaling, 133, 207
time scaling, 132–135
variable scalars, 205
Scenario generation, 76–78
Second Law of Thermodynamics, 140
Securitized credit products, 208, 209
Sensitivity approach, 84
Serial correlation, 34, 35, 37, 51, 133, 134,
144, 225, 248–250
Severity distribution, 188, 189
Shadow banking, 238, 239, 318
Sharpe, William, 5
Short rate models, 91
Signiﬁcance tests, 38–41
Simple summation, 131, 192, 193
Skewness, 16, 18, 46, 142–144, 246
Slippage loss, 182, 184
Sobol sequence, 92
Soros, George, 221, 222
Spearman’s rho, 32, 33, 305, 306
Special investment vehicles (SIVs), 239
Spectral risk measure (SRM), 264
Speciﬁc risk charge (SRC), 132, 202
Spillover risk, 222, 230, 231, 240
Spread escalation, 274
Spread risk, 275
Spurious correlation, 37
Square-root of time scaling, 133, 207
Stable distribution, 134
Standard approach, 200
Standard deviation, 18, 19, 28, 41, 47, 87,
102, 155, 277
Standard error (SE), 39
Standard normal deviate, 87
Standard normal distribution, 25, 26, 39
Standardized maxima, 117
Static hedging, 226
Stationarity (stationary time series), 19–21,
35, 269
Stationarity tests, 40, 41, 51, 63
Statistical arbitrage, 269
Statistical bootstrapping, 159, 160
Stochastic correlation, 103
Stochastic trend time series process,
53, 54, 56
Stochastic variable, 16
Stocks, 5, 16, 22, 27–29, 50, 51, 53
Stress tests, 5, 8, 209, 211, 216, 229
Stressed calibration, 214
Stressed VaR, 209
Strike price, 99
Student-t distribution, 39, 118
Subadditivity, 47, 102, 154, 155, 261
Swaptions, 73, 74
Switching strategy, 223
System architecture, 67, 69, 78
Systemic risk
contagion VaR (CoVaR), 230–233, 319
and CrashMetrics, 226–230
global regulation, need for and proposed
system, 239, 240
lack of regulation, 238, 239
and reﬂexivity, 221–225
research opportunities, 326
Risk Assessment Model for Systemic
Institutions (RAMSI), 237, 238
and shadow banking, 238, 239
Systemic Risk Monitor (SRM), 233–237
Systemic Risk Monitor (SRM), 233–237
Tail behavior, 118
Tail proﬁle, 128, 129
Taleb, Nassim, 6, 148, 149
Taylor series expansion, 85, 226, 227
Technical analysis (TA), 21, 270
Technical breach, 126
Tenor buckets (pillars), 72, 73
Theta effect, 95
Thin-tailed events, 6
Through-the-cycle observation period,
8, 205
Time decay, 95
Time scaling, 132–135
Index
345

Time series
classical decomposition, 55–58, 247
deterministic trend, 53, 54, 56
de-trending, 19, 21, 56, 58
integrated, 51
invariance, 21, 22
modeling, 21, 22
stationary, 19–21, 35, 269
stochastic trend, 53, 54, 56
Trading book, 154, 155, 178, 202, 206, 207,
209, 214, 216, 217, 281, 319, 320
Trading intent, 217
Trading strategies, 173
Transformed probability, 165
Transition matrix, 178
Translation invariance, 107, 154
T-ratio, 39, 40
Troubled Asset Repurchase Program
(TARP), 183
Turner, Adair, 7
Turner Review, 7, 8, 11, 153, 202–206, 216,
238, 317, 318
Uncertainty, 175, 204, 205
Uncertainty and Proﬁt (Knight), 204, 205
Unconditional coverage test (frequency back
test), 160–162
Undiversiﬁed VaR, 126
Unexpected loss (UL), 188, 190
Uniform distribution, 52
Uniformity tests, 165
Unit root tests (stationarity tests), 40,
41, 51, 63
Value-at-risk (VaR)
advanced methods, 111–122. See also
speciﬁc methods
aggregation of, 125, 126, 177, 190,
192–195, 259
analytical tools for, 127–131
assumptions, problems with, 139–156,
245–247
conditional, 61
conventional methods, 83–107. See also
speciﬁc methods
development of, 5
diversiﬁed, 126, 127
Excel calculation, 28, 29
and frequentist statistics, 16
gaming the system, 135, 174, 175
i.i.d. assumption. See Independent and
identically distributed (i.i.d.) returns
limits, 126, 127
as portfolio risk measure, 70
practical limitations of, 169–175
preprocessing step, 67–81
reporting, 69, 125–136
risk drivers, analytical tools for, 127–131
statistical basis for, 15, 26–28
stressed, 209
system speciﬁcation, 79–81
weaknesses of, 6–8, 139–156, 245–247,
317, 318
VaR. See Value-at-risk (VaR)
Variable scalars, 205
Variance, 16, 17, 19
Variance-covariance (VCV) VaR, 84–89
Vasicek model, 91, 202
Vega, 85, 86
Volatility
clustering, 44, 114, 144, 145, 164,
165, 248
implied, 45
low volatility, 7, 144, 146, 310, 326
measuring, 41–45
negative volatility skew, 142, 143
smile, 46, 72, 101, 322
spillover, 230
stochastic, 113
surface, 73
ultralow, 326
Volatility-based risk factors, 265
Weakly stationary, 19
Weibull family distribution, 118
Whipsaws, 151
White noise, 20, 21, 35, 36, 52, 133, 247–249
Whole distribution test, 165, 166
Window length (observation period), 28, 76,
77, 151, 250, 251
Worldcom, 177, 190, 201
Wrong-way trades, 193, 211
Yen carry unwind (1998), 223, 300, 301, 304
346
INDEX

Descriptions of the
Spreadsheets for the Book
N
ote that a longer (full) description of these spreadsheets is available in the
book.
If you see the following icon in the spreadsheet, it means that the
example uses Monte Carlo simulation:
Pressing F9 will trigger a recomputation of results.
Chapter 2
2.1
The Law of Large Numbers (LLN) is useful because it states that as
more measurements N are taken, statistics will converge to “true”
value, provided the random variable is i.i.d. Aim: illustrate LLN on
three processes including a non-i.i.d. AR(1) process as N increases
to 1000.
2.2
The Central Limit Theorem (CLT) is useful because it states that as
more samples are taken from various i.i.d. distributions, the means
of those samples will be normally distributed. Aim: illustrates CLT
1

by sampling from a uniform distribution 500 times. The distribution
of the means of those samples is plotted.
2.3
Linear correlation is known to have weaknesses when the rela-
tionship between variables is nonlinear. Rank correlation provides
an alternative. Aim: illustrates the computation of rank correlations.
2.4
An autocorrelation function (ACF) plot is useful in visually identi-
fying serial correlation. Aim: illustrates the computation of ACF
plots of N(0,1) and AR(1) processes.
2.5
Standard deviation, ARCH, EWMA, and GARCH are four differ-
ent ways to model the volatility (hence risks) of a financial variable.
Aim: illustrates the implementation of the four volatility models.
2.6
Markowitz modern portfolio theory was a breakthrough in modern
understanding of risk/reward of investments and pioneered the use
of variance as a measure of risks. Aim: illustrates the classical
Markowitz portfolio optimization for a portfolio of four assets.
2.7
Maximum likelihood method (MLE) is a popular statistical method
to estimate the parameters of models given a sample of data. Aim:
illustrates the use of MLE to estimate the decay parameter λ of the
EWMA volatility model.
2.8
Cointegration measures the tendency for two time series to drift
towards a common long-term equilibrium path. It is a required
behavior for pair trading. A pair of correlated variables is not
necessarily cointegrated. Aim: illustrates how to check for coin-
tegration using the Engle-Granger regression method.
2.9
Deterministic trend and stochastic trend processes are fundamen-
tally different. The former’s time series wanders around a fixed
trend; in the latter the trend itself wanders around. Aim: illustrates
the two processes using three examples: geometric Brownian
motion, random walk, and a deterministic trend process.
2.10
In Monte Carlo simulation it is crucial to be able to generate correlated
variables because most financial assets that are modeled in the banking
industry move in correlated fashion. Aim: illustrates the generation of
correlated random variables using Cholesky decomposition.
2.11
The classical decomposition breaks down a time series into three
theoretical components—the trend, the cycle, and the noise—for
analysis. Aim: Illustrate the classical decomposition using cubic
polynomial for the trend and Fourier series for the cycle.
2.12
The quantile regression model (QRM) is a statistical method to
estimate parameters of a model given a sample of data. It is
increasingly popular in VaR modeling because researchers are
considering models that are conditional on external variables. Aim:
illustrates QRM estimation using Excel Solver.
2
DESCRIPTIONS OF THE SPREADSHEETS FOR THE BOOK

Chapter 4
4.1
Historical simulation, Monte Carlo, and parametric VaRs are the
three most basic VaR models. More advanced models are often just
variants of these basic models. Aim: illustrates the hsVaR, mcVaR and
pVaR for a test portfolio of three deals: a stock, an option, and a bond.
4.2
Convexity is a (second-order) curvature risk peculiar to bonds and
cash flowbased products. It is not captured by simple parametric
VaR (delta approach). Aim: illustrates the convexity effect using a
bond’s price-yield relationship, and shows its impact on pVaR.
4.3
Option nonlinearity is not captured by pVaR because of the linear
approximation used. Aim: illustrates the nonlinearity of a put option’s
payoff, and the skew imposed on the PL distribution as a result of the
option’s downside protection. A VaR calculator, containing an option,
illustrates its different impact on mcVaR and pVaR.
4.4
The three VaR methods—pVaR, mcVaR, and hsVaR—have different
abilities (or inability) to account for fat-tails. pVaR fails to account
for fat-tails, mcVaR models them theoretically, while hsVaR uses
empirical distributional over a short window. Aim: explore the
impact of the crisis conditions of 2008 on the three VaR methods for
a single stock (linear position).
4.5
Implied correlations of multi-underlying derivatives are often assumed
to be constant (or not used as risk factors) in the computation of VaR.
This is often due to data and technological limitations. Aim: illustrates
the impact on VaR when implied correlation is stochastic (not kept
constant) by using an example of a spread call option.
4.6
The beta approach of representing risk factors in VaR can be shown
in theory to underestimate true risks. Aim: verify the understatement
of risks by the beta approach of VaR using actual data. PVaR and
hsVaR are computed for a portfolio of two stocks with and without
the beta method.
Chapter 5
5.1
Hybrid historical simulation VaR is an approach that combines the
exponential weighting scheme of RiskMetrics with hsVaR, so that it
can have the benefit of using a long history and yet achieve market
responsiveness. Aim: illustrates the implementation of hybrid VaR.
5.2
Hull-White introduced stochastic volatility into hsVaR by scaling the
VaR scenarios with a ratio of changing volatilities. Aim: illustrates
the implementation of Hull-White VaR.
5.3
The CAViaR (conditional autoregressive value-at-risk) model uses the
method of quantile regression to make VaR a function of other
Descriptions of the Spreadsheets for the Book
3

variables. It bypasses the problem of lack of tail data since the whole
distribution is used in modeling, but the method requires us to presume
(specify) the process for VaR. Aim: illustrates the implementation of the
CAViaR model at 95% confidence (quantile of θ 5 0.05).
5.4
(a) The cumulative distribution function (CDF) of maximas is known
to be degenerate. With a slight transformation this degeneracy can be
avoided, which allows the modeling of extreme value theory. Aim:
illustrates the concept of a degenerate function by plotting the CDF of
the maximas. (b) The peaks-over-thresholds (POT) method is a
popular EVT approach. To use it, a suitable exceedence threshold u
needs to be determined visually using a plot of the mean excess
function. Aim: illustrates the calculation and plot of the mean excess
function of the POT approach.
5.5
A popular EVT approach is to model the tail as a generalized Pareto
distribution (GPD). To obtain the VaR, the parameters of the
GPD need to be estimated statistically. Aim: illustrates the estimation
of the parameters β and α for the GPD using maximum likelihood
estimation.
Chapter 6
6.1
In historical simulation VaR, risks can easily be aggregated to arrive
at portfolio VaR. Aim: illustrates the aggregation of VaR by adding
PL vectors across scenarios for a portfolio containing six deals. It also
illustrates the ranked tail profile of the PL distribution often used for
analysis.
6.2
The benefit of diversification increases as the number of securities in
the portfolio increases. Aim: illustrates the benefit of diversification,
hence, the fall in portfolio VaR, as the number of securities n
increases. Note: pVaR is calculated assuming a constant correlation ρ
(for each pair-wise combination). The quantity of each security is
equally weighted such that the undiversified VaR adds up to $1
million. Action: key in a different correlation ρ and observe how the
pVaR declines with n. More interestingly, key in a negative ρ and
observe that pVaR fails because the matrix is no longer positive semi-
definite. For a small negative correlation (ρ 5 20.1) the pVaR can
still be calculated for small n. But as the n grows, the correlation
matrix becomes too large and restrictive in terms of satisfying the
positive semi-definite condition.
6.3
Component VaR is a method to decompose portfolio VaR by
assuming its constituents are beta portions of the portfolio VaR. Aim:
illustrates the computation of component VaR using pVaR and
4
DESCRIPTIONS OF THE SPREADSHEETS FOR THE BOOK

hsVaR methods. Note: under pVaR, the component VaRs add up to
100% of diversified portfolio VaR. Under hsVaR, the component
VaRs do not add up to 100% of total generally. It works for pVaR
because both pVaR and the beta approach assume normality.
6.4
Incremental VaR is a method to decompose portfolio VaR by calcu-
lating the actual contribution (including effects of diversification) of a
deal to the portfolio VaR. This is done by removing that deal and
recalculating portfolio VaR and taking the difference. Aim: illustrates
the computation of incremental VaR using hsVaR method. Note: the
incremental VaRs of constituents are not expected to add up to portfolio
VaR because their contributions take into account diversification.
6.5
Is it valid to calculate a 10-day VaR directly using a 10-day log return
applied over a “rolling” window (i.e., where the return series is ln(Xt/
Xt210) where t 5 10, 11, . . . , T. Aim: Use an ACF plot to illustrate
that a serial correlation is artificially introduced into the 10-day return
series even though the 1-day return series is i.i.d. Note: this problem
can be resolved by using nonoverlapping returns, where t 5 10,
20, . . . However, this requires 10 times as much historical data.
6.6
Power law scaling is a valid method to scale daily VaR to T-day VaR,
provided the distribution is stable. Aim: Implement the power law
scaling for Dow Jones index for a linear instrument. Estimate h for
various quantiles q.
Chapter 7
7.1
Empirically, bull markets tend to show negative skew in return dis-
tributions, and bear markets tend to show positive skew. Aim:
illustrate a stylized bull (bear) market distribution and its corre-
sponding negative (positive) skew using random simulation.
7.2
Fat-tail distributions can be caused by stochastic (or variable) vola-
tility. Aim: illustrates using Monte Carlo simulation that stochastic
volatility gives rise to a fat-tailed distribution.
7.3
As a risk measurement tool, VaR is often accused of being late in
detecting crises and regime changes in general. Aim: explore the
timeliness of VaR by comparing the performances of pVaR, hsVaR,
and EWMA VaR when encountering a simulated regime change.
Two regime shifts are tested: a doubling of volatility and an intro-
duction of serial correlation of 10.5.
Chapter 8
8.1
A method to determine the confidence interval or error band of a
VaR result is statistical bootstrap. Aim: illustrates use of the statistical
Descriptions of the Spreadsheets for the Book
5

bootstrapping method to calculate the error band (or 95% confi-
dence interval) for VaR.
8.2
Back testing is a regulatory requirement to monitor the “goodness”
of a VaR model. It checks that the number of days the PL exceeded
the VaR level during the observation period is in agreement with the
definition of the quantile. Aim: illustrates the back-testing method
stipulated by Basel Rules (the traffic light approach).
8.3
Bunching tests check how evenly the VaR back-test “breaks” are dis-
tributed over time. If the “breaks” are bunched up in time, it suggests the
presence of strong serial correlation and VaR becomes a biased estimate
of the real quantile. Aim: illustrate how a simple bunching test can be
performed using the Ljung-Box statistic on “breaks” of the 99% VaR.
Three VaR models are tested: pVaR, EWMA pVaR, and hsVaR. The hit
function is also plotted to illustrate the effect of bunching.
8.4
A sophisticated back-test method is to test the whole distribution (i.e.
all quantiles jointly) using the so-called Rosenblatt transformation.
This requires implementing the empirical CDF and testing if the
transformed probabilities are uniformly distributed. Aim: implement
the Rosenblatt transformation for backtest. Investigate the results for
a Gaussian process with constant volatility and stochastic volatility.
Replace the random variable X with a nonlinear function g(X)5X
1 0.15 * X * X and check if the results hold for non-linear products.
Chapter 10
10.1
CreditMetrics is a popular credit model used by banks to capture
credit default and rating migration risks on a portfolio basis. Aim:
illustrates a simple implementation of CreditMetrics for a portfolio
of three bonds.
10.2
Operational risks VaR (OpVaR) is a commonly accepted method to
measure operational risks at banks if a bank chooses an actuarial
approach. While theoretically appealing, this method is fraught with
data scarcity and classification challenges. Aim: derives the opera-
tional risk loss distribution using tabulation method given the data
on frequency distribution and severity distribution.
Chapter 12
12.1
Researchers
have
found
that
trading
strategies
that
involve
“switching” can cause systemic contagion when the majority of
participants are collectively applying the same strategy. The usual
“bell-shaped” return distribution can become distorted (bimodal),
6
DESCRIPTIONS OF THE SPREADSHEETS FOR THE BOOK

even though this is not reflected in empirical distribution. When this
happens, VaR is an inaccurate measure of risk. Aim: simulate a
stop-loss strategy where the stop-loss is 0.2 standard deviations
(s.d.) below the current price, and plot the resulting return
distribution.
12.2
CoVaR (contagion VaR) is a recent development that measures the
spillover risks among financial institutions. It is the VaR of an
institution conditional on another institution being in distress (i.e.,
having a loss at or beyond its VaR level). CoVaR is estimated using
quantile regression. Aim: illustrates CoVaR using two hypothetical
return time series from two institutions (i and j).
12.3
The Austrian model is one of the earliest models for network risks of
the banking system. Using a clearing procedure, it checks whether
a bank is able to fulfill its payment obligations to the system as a
result of simulated movements in assets prices and loans, given its
balance sheet situation and mutual obligations with other banks.
Aim: illustrate a simple network model for a banking system with
three banks, and illustrate the fictitious default algorithm used to
solve the model.
Chapter 13
13.1
The new interpretation of classical decomposition breaks down a
time series into the trend, the cycle, and the noise for analysis, and
posits that cycle breaks and compressions could explain well-known
market behaviors such as fat-tailness, leverage effects, and cluster-
ing. Aim: illustrates the new interpretation using a stylized time
series.
13.2
Central to the buVaR idea is the inflator, which functions to adjust
the distribution. The response function (13.6) converts the measured
bubble Bt to the inflator Δ. Aim: a toy calculator that plots the
response function.
13.3
It is interesting to compare three tail-risk measures—ETL, hsVaR,
and pVaR—in terms of coherence, responsiveness, and stability.
These are desirable properties of a risk metric. Aim: a Monte Carlo
method is used to simulate a regime break (a drastic change in
correlation) in the times series of two assets. The implied correlation
is plotted for analysis.
13.4
BuVaR is a new risk metric introduced by this book. Aim: the
spreadsheet is a prototype calculator for buVaR that illustrates
various aspects of its computation. For comparison and analysis,
expected tail loss (ETL) is also plotted on the same chart.
Descriptions of the Spreadsheets for the Book
7

Chapter 14
14.1
Credit buVaR is a new risk metric introduced by this book. Aim: the
spreadsheet is a prototype calculator for credit buVaR that illus-
trates various aspects of its computation. For comparison and
analysis, the pVaR is also plotted on the same chart.
Chapter 16
16.1
The basis risk of a long-short portfolio often contains large kurtosis,
which is understated in VaR because of the lack of tail data. BuVaR
compensates this weakness. Aim: the spreadsheet illustrates that
buVaR gives a more aggressive result for basis risk as compared to
expected shortfall.
FOR MORE INFORMATION, CONTACT THE AUTHOR
Email: max.wong@bubble-value-at-risk.com
Website: www.bubble-value-at-risk.com and www.wiley.com/go/bubble
valueatrisk
8
DESCRIPTIONS OF THE SPREADSHEETS FOR THE BOOK

Website
W
elcome to the website that accompanies the book, Bubble Value at
Risk: A Countercyclical Risk Management Approach (revised edition)
by Max Wong.
The following documents supplement the information in the book
where examples are illustrated in the syntax of Excel functions, so that the
reader can easily implement these ideas in a spreadsheet.
Downloadable resources (spreadsheet exercises):
n Description of the Exercises (1 pdf file)
n Chapter 2 (12 Excel files)
n Chapter 4 (6 Excel files)
n Chapter 5 (5 Excel files)
n Chapter 6 (6 Excel files)
n Chapter 7 (3 Excel files)
n Chapter 8 (4 Excel files)
n Chapter 10 (2 Excel files)
n Chapter 12 (3 Excel files)
n Chapter 13 (4 Excel files)
n Chapter 14 (1 Excel file)
n Chapter 16 (1 Excel file)
For more information on the author’s research and presentations, please
visit the author’s website for this book at www.bubble-value-at-risk.com.
1

