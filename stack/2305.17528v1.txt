Two Heads are Better than One:
Towards Better Adversarial Robustness by Combining
Transduction and Rejection
Nils Palumbo ∗1, Yang Guo ∗1, Xi Wu 2, Jiefeng Chen 1, Yingyu Liang 1, Somesh Jha 1
1 University of Wisconsin-Madison, 2 Google
npalumbo@wisc.edu, yguo@cs.wisc.edu, wu.andrew.xi@gmail.com,
jchen662@wisc.edu, yliang@cs.wisc.edu, jha@cs.wisc.edu
Abstract
Both transduction and rejection have emerged as important techniques for defending
against adversarial perturbations. A recent work by Tramèr [Tra22] showed that, in
the rejection-only case (no transduction), a strong rejection-solution can be turned
into a strong (but computationally inefficient) non-rejection solution. This detector-
to-classifier reduction has been mostly applied to give evidence that certain claims
of strong selective-model solutions are susceptible, leaving the benefits of rejection
unclear. On the other hand, a recent work by Goldwasser et al. [GKKM20] showed
that rejection combined with transduction can give provable guarantees (for certain
problems) that cannot be achieved otherwise. Nevertheless, under recent strong
adversarial attacks (GMSA [CWG+22], which has been shown to be much more
effective than AutoAttack against transduction), Goldwasser et al.’s work was
shown to have low performance in a practical deep-learning setting. In this paper,
we take a step towards realizing the promise of transduction+rejection in more
realistic scenarios. Theoretically, we show that a novel application of Tramèr’s
classifier-to-detector technique in the transductive setting can give significantly
improved sample-complexity for robust generalization. While our theoretical
construction is computationally inefficient, it guides us to identify an efficient
transductive algorithm to learn a selective model. Extensive experiments using
state of the art attacks (AutoAttack, GMSA) show that our solutions provide
significantly better robust accuracy.
1
Introduction
A recent line of research [GKKM20, MHS21, Goo19, WJS+21, WYW20a] has investigated aug-
menting models with transduction or rejection to defend against adversarial perturbations. However,
the results of leveraging these new options have been mixed. For example, a recent work by
Tramèr [Tra22] gives an equivalence between classification-only and classification-with-rejection; the
major application of the author’s results has been to provide bounds on the performance of defenses
with rejection, which can be used to show that the robustness of defenses with rejection may be lower
than the authors originally claimed, casting doubt on the benefits of rejection.
On the other hand, some recent work in theory has demonstrated that transduction, that is leveraging
the unlabeled test-time input for learning the model, may have significant impact on defending
against adversarial robustness. Specifically, Montasser et al. [MHS21] studied the setting of trans-
duction (without rejection), and show that robust learning with transduction allows for significant
∗Equal contribution.
Preprint. Under review.
arXiv:2305.17528v1  [cs.LG]  27 May 2023

improvemnents in sample complexity, reducing dependency on VC dimension from exponential to
linear; however, this comes at the cost of significantly greater assumptions on the data (OPTU2 for the
realizable case rather than the OPTU of the inductive setting 2). Goldwasser et al. [GKKM20] studied
transduction and rejection, and show even more surprising results, not achievable with transduction
or rejection alone. However, one prominent limitation of these works seems to be that none has yet
resulted in practical robust learning mechanisms in the deep learning setting typically considered.
Realizable
Agnostic Generalization Bound
Condition
Generalization Bound
Induction [MHS19]
OPTU = 0
O

2VC(H) log(n)+log(1/δ)
n

OPTU + O
 q
2VC(H)+log(1/δ)
n
!
Transduction [MHS21]
OPTU2 = 0
O
 VC(H) log(n)+log(1/δ)
n

2OPTU2 + O
 q
VC(H)+log(1/δ)
n
!
Rejection (Theorem 4.2)
OPTrej
U = 0
O

2VC(T(H)) log(n)+log(1/δ)
n

OPTrej
U + O
 q
2VC(T(H))+log(1/δ)
n
!
Transduction + Rejection [GKKM20]
OPTU = 0
O
 q
VC(H) log(n)
n
+ log(1/δ)
n
!
2 OPTU +2 √2 OPTI + O
 q
VC(H) log n+log(1/δ)
n
!
Transduction + Rejection (ours) (Theorem 4.1)
OPTU2/3 = 0
O
 VC(H) log(n)+log(1/δ)
n

2OPTU2/3 + O
 q
VC(H)+log(1/δ)
n
!
Table 1: Summary of generalization bounds for the four settings. Compared to transduction alone
and [GKKM20], our defense weakens the necessary conditions in the realizable case and improves the asymp-
totic error in the agnostic case. Compared to induction and rejection alone, sample complexity has a linear rather
than exponential dependence on the VC dimension. Compared to [GKKM20], the dependence on the error
bound ϵ improves from inverse quadratic to inverse linear in the realizable case.
Specifically, compared to Goldwasser et al., which considered arbitrary perturbations, we focus on the
classic and practical scenairo of bounded perturbations for deep learning. Somewhat surprisingly, we
show that a novel application of Tramèr’s classifier-to-detector technique in the transductive setting
can give significantly improved sample-complexity for robust generalization, noting that bounded
perturbations are critical for the construction to work. To obtain these improvements, we do not
require stronger assumptions on the data, as with [MHS21]; in the realizable case, we only need
to assume OPTU2/3 = 0, which is even better than the OPTU = 0 assumption in the inductive case.
Table 1 gives more details; the notation is described in Section 3. Our results give a first constructive
application of Tramèr’s classifier-to-detector reduction which leads to improved sample complexity.
While our theoretical construction is computationally inefficient due to the use of Tramèr’s reduction,
it guides us to identify a practical transductive algorithm for learning a robust selective model. In
addition, we present an objective for general adaptive attacks targeting selective classifiers based
on our algorithm. Our transductive defense algorithm gives strong empirical performance on image
classification tasks, both against our adaptive attack and against existing state-of-the-art attacks such
as AutoAttack and standard GMSA. On CIFAR-10, we obtain 73.9% transductive robust accuracy
with rejection, a significant improvement on the current state-of-the-art result of 66.6% [CAS+20] for
robust accuracy up to the perturbation considered (l∞with budget ϵ = 8/255).
The rest of the paper is organized as follows. Section 2 reviews main related work, and Section 3
presents some necessary background. We develop our theory results in Section 4. Guided by our
theory, Section 5 develops a practical robust learning algorithm, leveraging both transduction and
rejection. We provide systematic experiments in Section 6, and conclude in Section 7.
2
Related Work
In recent years, there have been extensive studies on adversarial robustness in the traditional in-
ductive learning setting, where the model is fixed during the evaluation phase [CW17, GSS14,
MDFF16]. Most popular and effective methods are adversarial training, such as PGD [MMS+17],
TRADES [ZYJ+19]. These methods are effective against adversaries on small dataset like MNIST, but
still ineffective on complex dataset like CIFAR-10 or ImageNet [CAS+20]. Defenses beyond adver-
sarial training have been proposed but most are broken by strong adaptive attacks [CH20, TCBM20].
To break this robust bottleneck, recent work has proposed alternative settings with relaxed yet realistic
assumptions, particularly by allowing rejection and transduction. In robust learning with rejection
2The optimal robust risk is OPTU = infh∈H Pr(x,y)∼D
∃z ∈U(x) : h(z) , y.
2

(a.k.a., abstain), we allow rejection of adversarial examples instead of correctly classifying all of
them [Tra22]. Variants of adversarial training with rejection option have been considered [LF19,
PZH+22, CRC+21, KCF20, SDM+20, HYC+22], also different generalizations such as [SHS20]
(unseen attacks), [SLK20, BSRK22, SLM+] (certified robustness). [Tra22] proves an equivalence
between robust learning with rejection and standard robust learning in the indcutive setting and shows
that the evaluation of past defenses with rejection was unreliable.
The other approach is to define an alternative notion of adversarial robustness via transductive
learning, i.e. "dynamically" ensuring robustness on the particular given test samples rather than on
the whole distribution. Similar settings have been studied but under the view of "test-time defense"
or "dynamic defense" [Goo19, WJS+21, WYW20a]. [GKKM20] is the first paper to formalize
transductive learning for robust learning, and the first to consider transduction+rejection. It considers
general adversaries on test data and presents novel theoretical guarantees. [CWG+22] formally
defines the notion of transductive robustness as a maximin problem and presents a principled adaptive
attack, GMSA. [MHS21] discusses robust transductive learning against bounded perturbation from a
learning theory perspective and obtains corresponding sample complexity.
3
Preliminaries
Robust Error
Robust Error (with Rejection)
Inductive
errU(h; x, y) := supz∈U(x) 1{h(z) , y}
errrej
U (h; x, y) := supz∈U(x) 1{h(z) < {y, ⊥} ∨h(x) , y}
Transductive
err(h; x, y, ˜x, ˜z, ˜y) := 1
m
Pm
i=1 1 {h (˜zi) , ˜yi}
errrej(h; x, y, ˜x, ˜z, ˜y) := 1
m
Pm
i=1 1
(
(h (˜zi) < { ˜yi} ∧˜zi = ˜xi)
∨(h (˜zi) < { ˜yi, ⊥} ∧˜zi , ˜xi)
)
Table 2: Summary of the robust error in all settings. Note that transductive error of the learner A is
the corresponding notion of error where h = A(x, y, ˜z).
Let X denote the input space, Y the label space, D the clean data distribution over X × Y. We
will assume binary classification for our theoretical analysis: Y = {±1}. Let U(x) denote the set
of possible perturbations of an input x, e.g., for ℓp norm perturbation of budget ϵ, U is the ℓp ball
of radius ϵ: U(x) = {z : ∥z −x∥p ≤ϵ}. We assume U satisfies ∀x ∈X, x ∈U(x); essentially
all interesting perturbations satisfy this. Let U2(x) := {z : ∃t ∈U(x), such that z ∈U(t)}, and
U−1(x) := {z : x ∈U(z)}. If a perturbation set Λ satisfies Λ2 = U, then we say Λ = U1/2. When U
is the ℓp ball of radius ϵ, U2 is that of radius 2ϵ, U−1 = U, and U1/2 is that of radius ϵ/2; we define
U3 and U1/3 similarly.
All learners are provided with n i.i.d. training samples 3 (x, y) = (xi, yi)n
i=1 ∼Dn. There are m i.i.d.
test samples (˜x, ˜y) ∼Dm, and the adversary can perturb ˜x to ˜z ∈U(˜x). We describe the main settings
below; the corresponding notions of error are in Table 2. For each setting, we define risk as the
expected worst-case error up to the perturbation U, and empirical risk similarly.
Induction.
In the traditional robust classification setting (e.g., [MMS+19]; also called the inductive
setting or simply induction), the learning algorithm (the defender) is given training set (x, y), learns a
classifier h : X 7→Y from some hypothesis class H.
Rejection.
In the setting of robust classification with rejection, the classifier has the extra power of
abstaining (i.e., outputting a rejection option denoted by ⊥), and furthermore, rejecting a perturbed
input does not incur an error. The learning algorithm is given training set (x, y) and learns a selective
classifier h : X 7→Y ∪{⊥} from some hypothesis class H. An error occurs only when h rejects a
clean input, or accepts and misclassifies. We define additionally OPTrej
U := infh∈H Rrej
U (h; D).
Transduction.
In the setting of robust classification with transduction (e.g., [MHS21]), the learning
algorithm (the transductive learner) has access to the unlabeled test input data; the goal is to predict
labels only for these given test inputs (a transductive learner need not generalize). The learner A
is given the training data (x, y) and the (potentially perturbed) test inputs ˜z, and outputs m labels
3Here x = (xi)n
i=1 and similarly with y, ˜x, ˜y, etc. We will also overload the notation U, e.g., U(x) := {u ∈
Xn : ui ∈U(xi)}.
3

h(˜z) = (h(˜zi))m
i=1 as predictions for ˜z. That is, the learner is a mapping A : (X × Y)n × Xm 7→Ym. A
special case is when A learns a classifier h and use it to label ˜z; the labels are also denoted as h(˜z).
Our setting: Transduction+Rejection.
A transductive learner for selective classifiers A is given
(x, y, ˜z), and outputs rejection or a label for each input in ˜z. That is, the learner is a mapping
A : (X × Y)n × Xm 7→(Y ∪{⊥})m. An error occurs when it rejects a clean test input or accepts and
misclassifies.
4
Theoretical Analysis
In this section, we present the theorem statements and proof sketches for the realizable case in two
settings: transduction+rejection, and rejection only. The proof details and the agnostic case results
are in Appendix A.
4.1
Transduction + Rejection: Realizable Case
We first present the result for our main focus, the setting with both transduction and rejection.
For comparison with existing results in the inductive-only and transduction-only settings [MHS19,
MHS21], we follow their setup: assume there exists a classifier (without rejection) with 0 robust
error from a hypothesis class H of VC-dimension VC(H), and the learner constructs a selective
classifier for labeling the test inputs (or constructs a set of selective classifiers and uses any of them
for labeling). The goal is to design a learner with a small robust error.
Theorem 4.1. For any n ∈N, δ > 0, hypothesis class H of classifiers without rejection, perturbation
set U such that U = U−1 and U1/3 exists, and distribution D over X × Y satisfying OPTU2/3 = 0,
there exists a transductive learner A that constructs a set of selective classifiers ∆s.t. the following is
true: with probability ≥1 −δ over (x, y) ∼Dn and (˜x, ˜y) ∼Dn, if ∆, ∅, then for any h ∈∆,
errrej
U (h; x, y, ˜x, ˜y) ≤VC(H) log(2n) + log(1/δ)
n
.
For U satisfying our conditions (including lp balls), we obtain a stronger guarantee than those
using only transduction or only rejection. First, compared to the guarantee for transduction without
rejection [MHS21] (see Table 1), our result requires weaker assumptions on the data: we need
OPTU2/3 = 0 rather than OPTU2 = 0. For example, consider the ℓp norm perturbation: U(x) = {z :
∥z −x∥p ≤ϵ}. Then using transduction alone requires that there exists a classifier with 0 robust error
for perturbations U2(x) which are ℓp norm perturbations of adversarial budget 2ϵ. In contrast, our
result shows that using both transduction and rejection only requires there exists a classifier with
0 robust error for perturbations U(x) which are ℓp norm perturbations of adversarial budget 2ϵ/3.
Equivalently, for a data distribution with a margin 2ϵ, transduction without rejection can only handle
adversarial perturbations with budget ϵ, while combining transduction and rejection can handle
adversarial perturbations with budget 3ϵ, tolerating three times the adversarial magnitude. Second,
compared to rejection only (see Table 1), this bound has a linear sample complexity rather than
exponential. Therefore, combining transduction and rejection has the benefits of both techniques.
This result, while potentially very strong, comes with the caveat that the defense is not guaranteed
to find a nonempty ∆(i.e., the defense is sound but may not be complete). Consider an adversarial
budget ϵ, and suppose ˜z is the given potentially perturbed test input and ˜x is the corresponding clean
test input. To obtain the guarantee, we need to find a model which is ϵ/3-robust at q = ˜x + (˜z −˜x)/3.
Such a model always exists when OPTU2/3 = 0. However, given only ˜z without knowing q or ˜x, our
algorithm finds a model ϵ/3-robust at every perturbation within 2ϵ/3 of ˜z and thus ∆may be empty.
While weaker conditions don’t guarantee that we find a model satisfying the conditions, the result
still provides intuition for the success of our derived empirical defense. For typical data distributions
and hypothesis classes, it might be expected that, if we fail to find a ϵ-robust hypothesis at the
fully-perturbed data, we will nevertheless be more likely to find a model which is robust nearer the
clean data distribution (i.e. where the condition is required by the theory) rather than further away.
Determining conditions for this is an interesting direction for future research.
Proof Sketch.
For intuition, think of U as the ℓp norm perturbation with adversarial budget ϵ. We
omit technical details; see Appendix A.3 for the complete proof.
4

(a)
(b)
Figure 1: (a) h is ϵ/3-robust at ˜z; ˆh correctly classifies ˜z.(b) h is not ϵ/3-robust at ˜z; ˆh rejects ˜z.
Consider some clean training set x, y, clean test set ˜x, ˜y, with perturbed test data ˜z with ˜zi within ϵ of
˜xi. Let ˜z′ = ˜x + (˜z −˜x)/3 be the intermediate perturbation a third of the way between ˜x and ˜z.
First, following Montasser et al. [MHS21], define the set of robust hypotheses ∆U1/3
H
(x, y, ˜z′) as
∆U1/3
H
(x, y, ˜z′) = {RU1/3(h; x, y) = 0 ∧RU1/3(h; ˜z′) = 0}. That is, we find those classifiers that satisfy:
(1) they are ϵ/3-robustly correct (i.e., correct and robust to perturbations of budget ϵ/3) on the
training data (x, y); (2) they have ϵ/3 margin on the intermediate perturbations ˜z′ (i.e., have the same
prediction for all perturbations of budget ϵ/3).
This then guarantees, as shown in [MHS21], that with high probability, for any h ∈∆U1/3
H
(x, y, ˜z′) the
robust error facing perturbation of budget ϵ/3 is bounded by VC(H) log(2n)+log(1/δ)
n
if OPTU2/3 = 0.
Following Tramèr [Tra22], we can define a transformation FU1/3 that maps a classifier without
rejection, h, to the selective classifier ˆh = FU1/3(h): ˆh(x) =
(h(x)
if ∀x′ ∈U−1/3(x) , h(x′) = h(x)
⊥
otherwise
.
That is, ˆh rejects x if it is within ϵ/3 from h’s decision boundary, otherwise accepts and predicts h(x).
Now, continuing with the proof, consider a clean test sample (˜x, ˜y) with adversarial perturbation ˜z.
The corresponding intermediate perturbation is ˜z′ = ˜x + (˜z −˜x)/3. We will show that if h is correct at
˜z′, then ˆh makes no error at ˜z.
If ˜z = ˜x, then ˜z′ = ˜x = ˜z. Since h is ϵ/3-robust at ˜z′, h(˜z) = h(˜z′) = ˜y and so ˆh(˜z) = ˜y which is correct.
Otherwise, we need to consider two cases: (a) h is ϵ/3-robust at ˜z; (b) h is not. See visualization
in Figure 1. In both cases, the ϵ/3-balls about ˜z and ˜z′ intersect. Let ˜z′′ be 5some point in the
intersection. Since h is ϵ/3-robust at ˜z′, h(˜z′′) = h(˜z′) = ˜y. Now, in case (a) where h is ϵ/3-robust at ˜z,
h(˜z) = h(˜z′′) = ˜y, which is correct. In case (b) where h is not ϵ/3-robust at ˜z, ˆh rejects ˜z and makes no
error.
Hence the error of ˆh on ˜z is less than the error of h on ˜z′. So the error bound for h implies the desired
error bound for any ˆh in the set ∆′ =
nˆh = FU1/3(h) : h ∈∆U1/3
H
(x, y, ˜z′)
o
.
As we have access only to the adversarial test data ˜z, we need to ensure ϵ/3-robustness at any possible
˜z′ (i.e. ˜z′ within 2ϵ/3 of ˜z). This is equivalent to ensuring ϵ-robustness at ˜z in the realizable case. We
then output ∆:=
nˆh = FU1/3(h) : h ∈T
˜z′∈U−2/3(˜z) ∆U1/3
H
(x, y, ˜z′)
o
. By the above, as ∆⊆∆′, any ˆh in ∆
achieves the desired bound, leading to the theorem statement.
Remark:
More direct approaches may seem possible, but have surprising pitfalls. At first glance,
this approach may seem less natural than simply applying the analysis of [MHS21] to a potential
˜z′ ∈U1/2(˜x) with the condition of OPTU, obtaining a U1/2-robust classifer h′, and deriving an
ϵ-robust selective classifier by the transformation FU1/2. While this seems possible at first, as
Tramèr [Tra22] shows that applying this transformation results in doubled robustness, this isn’t
possible in this situation, as h′ is only guaranteed to be U1/2-robust at ˜z′, not at every ϵ/2 perturbation
of ˜x as needed by the analysis. Similarly, it might seem possible to obtain an ϵ/2-robust classifier at
˜z using [MHS21], and derive the desired ϵ-robust classifier from FU1/2; this, however, requires the
condition OPTU2, as the analysis of [MHS21] only applies on perturbations up to half the margin;
hence, this approach gains no advantage from rejection.
5

4.2
Rejection Only: Realizable Case
Theorem 4.2. For any n ∈N, δ ∈(0, 1/2), hypothesis class H of selective classifiers, perturbation
set U, and distribution D over X × Y satisfying OPTrej
U = 0, there exists an algorithm that outputs
h ∈H such that with probability ≥1 −δ over (x, y) ∼Dn,
Rrej
U (h; D) ≤2VC(T(H)) log(n) + log(1/δ)
n
where T(H) := {T(h) : h ∈H} denotes the transformed hypothesis class with transformation
T(h)(x, x′, y) := 1{h(x) , y ∨h(x′) < {y, ⊥}}.
Compared to the traditional setting [MHS19] (see Table 1), The guarantee still requires a sample
complexity exponentially large, though the requirement on data is weaker. In contrast, combining
transduction and rejection can reduce the sample complexity to linearly large.
Proof Sketch.
Our proof adapts the classical sample compression argument [LW86] with im-
provements based on [MHS19, HKS19, MY16]. The key argument is to construct the algorithm
that compresses the finite training samples into another finite compressed dataset ˆS U, where the
data inflation and discretization subroutine uses the dual space of T(H). Then, we perform the
classical PAC learning and followed by the α-boosting procedure to construct the final classifier (and
corresponding rejector) with a small robust loss under rejection. Since this is not our main focus, the
proof details are provided in Appendix A.1.
5
Defense by Transduction and Rejection
The analysis of Theorem 4.1 suggests the following defense algorithm: (1) first obtain a classifier h
that are robust and correct on the training data and also robust on the test inputs, (2) then transform h
to a selective classifier ˆh by rejecting inputs too close to the decision boundary of h. We describe the
resulting defense below, which we refer to as TLDR (Transductive Learning Defense with Rejection).
Step (1) To get h, we perform adversarial training on both the training set and the test set, using a ro-
bust cross-entropy objective. As in TADV [CWG+22] we train with private randomness. Specifically,
we train a model with softmax output as the class prediction probabilities hs and the class prediction
is h(x) = arg maxy∈Y hs
y(x). Given the labeled training data (x, y) and the test inputs ˜z, we optimize
the following objective:
min
h
1
n
X
(x,y)∈(x,y)
"
LCE(hs(x), y) + max
x′∈U(x) LCE
 hs(x′), y#
+ λ
m
X
˜z∈˜z
"
max
˜z′∈U(˜z) LCE
 hs(˜z′), h(˜z)#
(1)
where LCE is the cross-entropy loss and λ > 0 is a hyper-parameter.
Step (2) Having learned h, we now turn h into a selective classifier ˆh. Recall that ˆh rejects the input x
if there exists x′ ∈U1/3(x) with h(x) , h(x′); otherwise accepts and predicts the label h(x). So we
only need to determine the existence of x′ ∈U1/3(x) with h(x) , h(x′). We use a standard inductive
attack, PGD, for this by solving:
arg max
x′∈U1/3(x)
LCE(hs(x′), h(x)).
(2)
When U is ℓp norm ball of radius ϵ, the constraint is then ∥x′ −x∥≤ϵ/3. In practice, we can
generalize to a constraint ∥x′ −x∥≤ϵdefense where where ϵdefense is a hyper-parameter we call the
rejection radius.
5.1
Adaptive Attacks
Since no strong adaptive attacks exist for the new transduction+rejection setting to our knowledge, we
design one here. Our attack is based on GMSA in [CWG+22], which has been shown to be a strong
attack for transductive defense (without rejection). The goal of the attack is to find perturbations ˜z of
the clean test inputs ˜x such that the transductive learner has a large error when given (x, y, ˜z). GMSA
runs in stages; in each stage t, it simulates the transductive learner on the current data set (x, y, ˜zt)
6

to get a classifier ht, and then maximizes the minimum or average loss of {hi}t
i=1 to get the updated
perturbations of the test inputs ˜zt+1 (called GMSAMIN and GMSAAVG, respectively). See [CWG+22]
for the details.
GMSA does not directly apply to our setting since we have selective classifiers ˆh with a rejection
option which is not considered in GMSA. Our contribution is to design a method to get the updated
perturbations ˜z of the test inputs in each stage such that the selective classifier incurs a large error.
Recall that ˆh constructed from h incurs error in two cases: (1) it accepts ˜z and misclassifies with
h(˜z) , y; (2) ˜z = ˜x and it rejects ˜z. We consider the two cases below.
Case (1) We will propose a novel loss measuring the loss of a selective classifier ˆh on a perturbation
(˜z, y) from a clean test point (˜x, y) for such kind of error; maximizing this loss gives the desired
˜z. Recall that we need ˜z to be accepted and also the prediction h(˜z) , y. For the latter, we can
maximize LCE(hs(˜x), y) where hs is the class probabilities of h (i.e., its softmax output). The former
is equivalent to minh(˜z′),h(˜z) ∥˜z −˜z′∥≥ϵdefense.
Now, suppose LDB,h(˜z′) is a surrogate loss function on the closeness to the decision boundary; it
increases when ˜z′ gets closer to the decision boundary of h. Then the condition is equivalent to
∥˜z −p(˜z)∥= ϵdefense where p(˜z) = arg max∥˜z′−˜z∥≤ϵdefense LDB,h(˜z′). Now, as the maximum value of
∥˜z −p(˜z)∥is exactly ϵdefense. So to satisfy the condition, we would like to maximize ∥˜z −p(˜z)∥.
Summing up, for this case, we would like to maximize:
LREJ(˜z, y) := LCE(hs(˜z), y) + λ′ ∥˜z −p(˜z)∥, where p(˜z)
= arg max
∥˜z′−˜z∥≤ϵdefense
LDB,h(˜z′)
(3)
and λ′ > 0 is a hyper-parameter. Finally, for LDB,h, the following definition works well in our
experiments: LDB,h(˜z′) := rank2 hs(˜z′) −max hs(˜z′), which is minimized at the decision boundary as
the top-two class probabilities are equal.
Case (2) A critical step in an effective application of LREJ to a transductive attack is the selection
of which points to perturb. To do this, we apply a post-processing step after finding ˜z in (1). We
must predict whether ˆh is more likely to incur error on ˜z or on the clean input ˜x (i.e., ˆh(˜x) , y). If we
expect that the clean point is likely to be incorrectly classified or rejected, then we update ˜z to ˜x. In
GMSA, we have access to a series of models trained on previous attack iterations; we estimate the
likelihood of success at ˜z and ˜x by the fraction of previous models which fail at each point.
Summing up the two cases and combining with GMSA gives our final attack (details in Algorithm 1
in Appendix B.5).
6
Experiments
This section performs experiments to evaluate the proposed method TLDR and compare it with
baseline methods (e.g., those using only rejection or transduction). Our main findings are: 1)
TLDR outperforms the baselines significantly in robustness, confirming the advantage of combining
transduction and rejection. 2) Our adaptive attack is significantly stronger than existing attacks which
were not designed for the new setting, providing a strong evaluation. 3) Rejection rates rise steadily
with the rejection radius, but few clean samples are rejected and the robust accuracy remains stable.
6.1
Robustness of TLDR
Baselines.
(1) AT: adversarial training [MMS+18]; (2) AT (with rejection): adversarial training
(AT) with rejection; (3) RMC [WYW20b]; (4) DANN [AGL+15]; (5) TADV [CWG+22]; (6) Re-
jectron [GKKM20]. Among them, (1) is in the traditional induction setting, (2) is rejection only,
(3)(4)(5) are transduction only, and (6) incorporates both transduction and rejection.
Evaluation.
We attack the defenses and report the robust accuracy (1 - the robust error defined
in Section 3). To attack inductive classifiers, we use AutoAttack [CH20]. For inductive selective
classifiers, we use PGD on the rejection-aware loss LREJ from Eqn (3). For transductive classifiers,
we use GMSA which has been shown to be a strong adaptive attack on transduction [CWG+22].
Finally, for our transductive selective classifiers, we use our adaptive attack in Section 5.1 (roughly
GMSA with LREJ). For Rejectron [GKKM20] we use GMSA with a loss function LDISC targeting
their defense; see Appendix B.6 for the details.
7

Setting
Defense
Attacker
MNIST
CIFAR-10
pREJ
Robust accuracy
pREJ
Robust accuracy
Induction
AT [MMS+18]
AutoAttack
–
0.897
–
0.448
Rejection only
AT (with rejection)
PGD (LREJ)
0.852
0.968
0.384
0.634
Transduction only
RMC [WYW20b]
GMSA (LCE)
–
0.588
–
0.396
DANN [AGL+15]
GMSA (LCE)
–
0.062
–
0.055
TADV [CWG+22]
GMSA (LCE)
–
0.943
–
0.541
Transduction+Rejection
URejectron [GKKM20]
GMSA (LDISC)
0.274
0.721
0.000
0.145
Transduction+Rejection
TLDR (ours)
GMSA (LREJ)
0.126
0.972
0.208
0.739
Table 3: Results on MNIST and CIFAR-10. Robust accuracy is 1 - robust error; see Section 3. pREJ
is the percentage of inputs rejected. The baseline results are from [CWG+22]. The strongest attack
against each defense is shown. The best result is boldfaced.
Attack
MNIST
CIFAR-10
PGD (LCE)
0.991
0.794
PGD (LREJ)
0.988
0.781
AutoAttack
0.989
0.756
GMSA (LCE)
0.988
0.853
GMSA (LREJ)
0.972
0.739
Table 4: Robust accuracy by different attacks on
TLDR. The strongest attack is boldfaced.
Loss
MNIST
CIFAR-10
AutoAttack [CH20]
0.980
0.592
LCE
0.977
0.524
LREJ(LCE)
0.974
0.470
LREJ
0.973
0.458
Table 5: Robust accuracy under different attack
losses on a fixed adversarially trained model
with rejection, AutoAttack for comparison. The
strongest attack is boldfaced.
TLDR Components
Attacker
MNIST
CIFAR-10
Rejection
Ltest
pREJ
Robust accuracy
pREJ
Robust accuracy
✓
✓
GMSA (LREJ)
0.588
0.967
0.208
0.739
✓
×
GMSA (LREJ)
0.646
0.975
0.179
0.725
×
✓
GMSA (LCE)
–
0.900
–
0.516
×
×
GMSA (LCE)
–
0.935
–
0.516
Table 6: Ablation study of TLDR. The best result is boldfaced.
For transductive models, we report the stronger of GMSAMIN and GMSAAVG. Inductive models are
trained with standard adversarial training [GSS15], and transductive models with the TLDR loss in
Eqn (1). As Rejectron depends heavily on a key hyperparameter determining confidence needed to
reject, we report the results for the parameter value strongest against our attack. The best-performing
value on CIFAR-10 effectively eliminated the possibility of rejection (hence the rejection rate of 0);
other choices resulted in near-0 robust accuracy.
Datasets and Defense/Attack Setup. We evaluate on MNIST [LeC98] and CIFAR-10 [KH+09]. We
consider an adversarial budget of ϵ = 0.3 in l∞on MNIST and ϵ = 8/255 in l∞on CIFAR-10. For
defense, on MNIST, we use a LeNet architecture; on CIFAR-10 we use a ResNet-20 architecture.
In both cases, we train for 40 epochs with a learning rate of 0.001 using ADAM for optimization.
On MNIST, we use 40 iterations of PGD during training with a step size of 0.01. On CIFAR-10, we
use 10 iterations of PGD in training with a step size of 2/255. In training TLDR, we put 85% of the
weight on Ltrain, equivalent to λ = 0.176 after a warm start period epochs in which λ = 0. We use a
rejection radius of ϵ/4 for selective classifiers. For attack, we use 10 iterations of GMSA on both
datasets. On MNIST, we use 200 steps of PGD with a stepsize of 0.01 while generating adversarial
examples. On CIFAR-10, the PGD attacks use 100 steps with a stepsize of 1/255. Defense settings
used while training models in GMSA (including internal PGD settings) are the standard defense
settings. Internal optimizations in the calculation of LREJ use 10 steps of PGD with a stepsize of 15%
of the rejection radius. We use λ′ = 1 in LREJ; we observe little sensitivity to the parameter.
8

Results. Table 3 shows the robust accuracy and rejection rate of different methods. We observe
that either transduction or rejection can improve the performance, while combining both techniques
leads to the best results. In particular, our defense outperforms existing transductive defenses such as
RMC and DANN. It also outperforms the strongest existing baseline of 66.56% robust accuracy on
CIFAR-10 [CAS+20] (note that 66.56% is for the classic setting without rejection and transduction).
Finally, note that our defense passes the sanity check of [Tra22] (i.e. we do not exceed the theoretical
upper bound on robust accuracy of 79%), providing evidence that our evaluation is reliable. These
results provide positive support for the benefit of combining transduction and rejection for robustness.
6.2
Ablation Studies
Different Attacks on TLDR. Table 4 shows the results of different attack methods on TLDR.
Previous work [CWG+22] shows that transduction-aware attacks are necessary against transductive
defenses; we observe that attacks (PGD on LCE or LREJ and AutoAttack) from the traditional setting
perform poorly against our defense. We can also see that GMSA significantly outperforms even a
rejection-aware transfer attack (referred to as PGD targeting LREJ; note that PGD and AutoAttack do
not target the final model in this case, given the transductive setting, but instead target a proxy trained
by the adversary); see Algorithm 2 in Appendix B.5 for the full details. This shows that GMSA
is critical for attacking a transductive defender; while PGD and AutoAttack are strong against an
inductive model, they performs poorly facing transduction. Finally, we observe that GMSA with LCE
is much weaker than GMSA with LREJ. This shows another key component in our adaptive attack,
the loss LREJ, is also critical to get a strong attack against our defense.
Ablation on LREJ. To further investigate the importance of LREJ, we attack an adversarially trained
model with rejection (i.e., the AT+Rejection model), with PGD on different losses: LREJ, cross-
entropy LCE, and LREJ with LDB,h replaced by LCE, with AutoAttack given for comparison. We
also consider a multitargeted attack LMULTI which attempts to find, for each incorrect label, the
perturbation with the highest robust confidence up to the rejection radius. More precisely, given a
point (x, y) and a base classifier h with adversarial budget ϵ and rejection radius ϵdefense, for each
target label y′ , y, we find a perturbation z(y′) such that z(y′) and its ϵdefense-ball around it all get the
label y′ (and thus z(y′) will be accepted and misclassified): we let
z(y′) = arg min
p:∥p−x∥≤ϵ
LMULTI(p, y′), where LMULTI(p, y′) := LCE(p, y′) +
max
∥z−p∥≤ϵdefense LCE(z, y′).
(4)
Finally, the attack outputs the strongest perturbation,
z = arg min
z(y′):y′,y
LMULTI(z(y′), y′).
(5)
Table 5 shows the robust accuracy under these different attacks; note that, as with transudction,
AutoAttack is unable to find perturbations which evade defenses with rejection. LREJ leads to the
strongest attack. In particular, it can be significantly better than LCE and LMULTI, demonstrating its
importance for attacking models with rejection.
Key Components of TLDR. Compared to traditional defenses, TLDR has two novel components:
using the given test inputs in training the classifier (the second term in Equation (1), referred to as
Ltest), and transforming the trained classifier into one with rejection. Table 6 shows the results of the
ablation study on these two components. In all cases, rejection significantly improves results. The
use of transduction is helpful on CIFAR-10, but reduces performance on MNIST probably since it’s
easier to get robust predictions on MNIST and thus knowing test inputs does not help.
7
Conclusion
Existing works on leveraging transduction and rejection gave mixed results on their benefits for
adversarial robustness. In this work we take a step in realizing their promise in practical deep
learning settings. Theoretically, we show that a novel application of Tram`’er’s results give improved
sample complexity for robust learning in the bounded perturbations setting. Guided by our theory, we
identified a practical robust learning algorithm leveraging both transduction and rejection. Systematic
experiments confirm the benefits of our constructions. There are many future avenues to explore,
such as improving the theoretical bounds, and improving the efficiency of our algorithms.
9

References
[AGL+15]
Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, and Mario Marc-
hand. Domain-adversarial neural networks, 2015.
[Ass83]
Patrick Assouad. Densité et dimension. In Annales de l’Institut Fourier, volume 33,
pages 233–282, 1983.
[BEHW89] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth.
Learnability and the vapnik-chervonenkis dimension. Journal of the ACM (JACM),
36(4):929–965, 1989.
[BSRK22]
Sina Baharlouei, Fatemeh Sheikholeslami, Meisam Razaviyayn, and Zico Kolter. Im-
proving adversarial robustness via joint classification and multiple explicit detection
classes. arXiv preprint arXiv:2210.14410, 2022.
[CAS+20]
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti,
Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench:
a standardized adversarial robustness benchmark. arXiv preprint arXiv:2010.09670,
2020.
[CH20]
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with
an ensemble of diverse parameter-free attacks. In International conference on machine
learning, pages 2206–2216. PMLR, 2020.
[CRC+21]
Jiefeng Chen, Jayaram Raghuram, Jihye Choi, Xi Wu, Yingyu Liang, and Somesh Jha.
Revisiting adversarial robustness of classifiers with a reject option. In The AAAI-22
Workshop on Adversarial Machine Learning and Beyond, 2021.
[CW17]
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural
networks. In 2017 ieee symposium on security and privacy (sp), pages 39–57. Ieee,
2017.
[CWG+22] Jiefeng Chen, Xi Wu, Yang Guo, Yingyu Liang, and Somesh Jha. Towards evaluating
the robustness of neural networks learned by transduction. In International Conference
on Learning Representations, 2022.
[GKKM20] Shafi Goldwasser, Adam Tauman Kalai, Yael Kalai, and Omar Montasser. Beyond
perturbations: Learning guarantees with arbitrary adversarial test examples. Advances
in Neural Information Processing Systems, 33:15859–15870, 2020.
[Goo19]
Ian Goodfellow. A research agenda: Dynamic models to defend against correlated
attacks. arXiv preprint arXiv:1903.06293, 2019.
[GSS14]
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing
adversarial examples. arXiv preprint arXiv:1412.6572, 2014.
[GSS15]
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing
adversarial examples, 2015.
[HKS19]
Steve Hanneke, Aryeh Kontorovich, and Menachem Sadigurschi. Sample compression
for real-valued learners. In Algorithmic Learning Theory, pages 466–488. PMLR, 2019.
[HYC+22]
Zhiyuan He, Yijun Yang, Pin-Yu Chen, Qiang Xu, and Tsung-Yi Ho. Be your own
neighborhood: Detecting adversarial example by the neighborhood relations built on
self-supervised learning. arXiv preprint arXiv:2209.00005, 2022.
[KCF20]
Masahiro Kato, Zhenghang Cui, and Yoshihiro Fukuhara. Atro: Adversarial training
with a rejection option. arXiv preprint arXiv:2010.12905, 2020.
[KH+09]
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny
images. 2009.
[LeC98]
Yann LeCun. The MNIST database of handwritten digits. 1998.
[LF19]
Cassidy Laidlaw and Soheil Feizi. Playing it safe: Adversarial robustness with an
abstain option. arXiv preprint arXiv:1911.11253, 2019.
[LW86]
Nick Littlestone and Manfred Warmuth. Relating data compression and learnability.
1986.
10

[MDFF16]
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a
simple and accurate method to fool deep neural networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 2574–2582, 2016.
[MHS19]
Omar Montasser, Steve Hanneke, and Nathan Srebro. Vc classes are adversarially
robustly learnable, but only improperly. In Conference on Learning Theory, pages
2512–2530. PMLR, 2019.
[MHS21]
Omar Montasser, Steve Hanneke, and Nathan Srebro. Transductive robust learning
guarantees. arXiv preprint arXiv:2110.10602, 2021.
[MMS+17] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint
arXiv:1706.06083, 2017.
[MMS+18] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks. In 6th Inter-
national Conference on Learning Representations, Conference Track Proceedings.
OpenReview.net, 2018.
[MMS+19] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian
Vladu. Towards deep learning models resistant to adversarial attacks, 2019.
[MY16]
Shay Moran and Amir Yehudayoff. Sample compression schemes for vc classes. Journal
of the ACM (JACM), 63(3):1–10, 2016.
[PZH+22]
Tianyu Pang, Huishuai Zhang, Di He, Yinpeng Dong, Hang Su, Wei Chen, Jun Zhu,
and Tie-Yan Liu. Two coupled rejection metrics can tell adversarial examples apart. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 15223–15233, 2022.
[SDM+20]
Angelo Sotgiu, Ambra Demontis, Marco Melis, Battista Biggio, Giorgio Fumera, Xiaoyi
Feng, and Fabio Roli. Deep neural rejection against adversarial examples. EURASIP
Journal on Information Security, 2020(1):1–10, 2020.
[SF12]
Robert E Schapire and Yoav Freund. Boosting. adaptive computation and machine
learning. MIT Press, Cambridge, MA, 1(1.2):9, 2012.
[SHS20]
David Stutz, Matthias Hein, and Bernt Schiele. Confidence-calibrated adversarial
training: Generalizing to unseen attacks. In International Conference on Machine
Learning, pages 9155–9166. PMLR, 2020.
[SLK20]
Fatemeh Sheikholeslami, Ali Lotfi, and J Zico Kolter. Provably robust classification
of adversarial examples with detection. In International Conference on Learning
Representations, 2020.
[SLM+]
Fatemeh Sheikholeslami, Wan-Yi Lin, Jan Hendrik Metzen, Huan Zhang, and J Zico
Kolter. Denoised smoothing with sample rejection for robustifying pretrained classifiers.
In Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS
2022.
[SSBD14]
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From
theory to algorithms. Cambridge university press, 2014.
[TCBM20] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive
attacks to adversarial example defenses. Advances in Neural Information Processing
Systems, 33:1633–1645, 2020.
[Tra22]
Florian Tramer. Detecting adversarial examples is (nearly) as hard as classifying them.
In International Conference on Machine Learning, pages 21692–21702. PMLR, 2022.
[WJS+21]
Dequan Wang, An Ju, Evan Shelhamer, David Wagner, and Trevor Darrell. Fighting
gradients with gradients: Dynamic defenses against adversarial attacks. arXiv preprint
arXiv:2105.08714, 2021.
[WYW20a] Yi-Hsuan Wu, Chia-Hung Yuan, and Shan-Hung Wu. Adversarial robustness via
runtime masking and cleansing. In International Conference on Machine Learning,
pages 10399–10409. PMLR, 2020.
11

[WYW20b] Yi-Hsuan Wu, Chia-Hung Yuan, and Shan-Hung Wu. Adversarial robustness via runtime
masking and cleansing. In Hal DaumÃ© III and Aarti Singh, editors, Proceedings of
the 37th International Conference on Machine Learning, volume 119 of Proceedings of
Machine Learning Research, pages 10399–10409. PMLR, 13–18 Jul 2020.
[ZYJ+19]
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael
Jordan. Theoretically principled trade-off between robustness and accuracy. In Interna-
tional conference on machine learning, pages 7472–7482. PMLR, 2019.
12

Supplementary Material
A
Proof Details
Before introducing the proof for the generalization results, we first need to make some additional
definitions. We define the empirical robust risk as
ˆRU(h; S ) =
X
(x,y)∈S
sup
z∈U(x)
1{h(z) , y}

And we can define the empirical robust risk under rejection accordingly:
ˆRrej
U (h; S ) =
X
(x,y)∈S
sup
z∈U(x)
1{h(x) , y ∨h(z) < {y, ⊥}}

And we can define the corresponding robust empirical risk minimization procedure (under rejection)
as follows:
RERMH(S ) := argmin
h∈H
ˆRU(h; S )
RERMrej
H (S ) := argmin
h∈H
ˆRrej
U (h; S )
A.1
Rejection Only: Realizable Case
Definition A.1 (Realizable Robust PAC Learnability under Rejection). For Y = {0, 1}, ∀ϵ, δ ∈
(0, 1), H = Hc×Hr, the sample complexity of realizable robust (ϵ, δ) - PAC learning of H with respect
adversary U under rejection, denoted as MRE(ϵ, δ; H, U), is defined as the smallest m ∈N ∪{0} for
which there exists a learning rule A : (X × Y)m 7−→(Y ∪{⊥})X s.t. for every data distribution D over
(X × Y)m where there exists a predictor with rejection option h∗∈H with 0 risk, RU,rej(h∗; D) = 0
with probability at least 1 −δ over S ∼Dm,
Rrej
U (A(S ); D) ≤ϵ
If no such m exists, MRE(ϵ, δ; H, U) = ∞. We say that H is robustly PAC learnable under rejection
in the realizable setting with respect to adversary U if ∀ϵ, δ ∈(0, 1), MRE(ϵ, δ; H, U) is finite.
Theorem A.2 (Sample Complexity for Realizable Robust PAC Learning under Rejection). In the
realizable setting, for any H = Hc × Hr and U, and any ϵ, δ ∈(0, 1/2),
MRE(ϵ, δ; H, U) = 2O((dr+dc) log(dr+dc)) 1
ε log
 1
ε
!
+ O
 1
ε log
 1
δ
!!
(6)
where dr = VC(Hr), dc = VC(Hc).
The idea of the proof is to adapt the classical sample compression argument [LW86] with im-
provements based on [MHS19, HKS19, MY16]. The generalization result in the inductive case
(Theorem 4.2) directly comes from Equation (31).
Proof. First, we define the concept of sample compression scheme and sample compression algorithm.
Definition A.3 (Sample Compression Scheme). Given ∀m ∈N samples, S ∼Dm, a sample
compression scheme of size k is defined by the following pair of functions:
1. Compression function κ : (X × Y)m 7→(X × Y)≤k.
2. Reconstruction function: ρ : (X × Y)≤k 7→H.
An algorithm A is a sample compression algorithm if ∃κ, ρ s.t. A(S ) = (κ ◦ρ)(S ).
Fix ϵ, δ
∈
(0, 1), m
>
2(dr + dc) log(dr + dc).
Let the compression parameter, n
=
O  (dr + dc) log (dr + dc).
Let D be any distribution, then by realizability of the learner,
infh∈H Rrej
U (h; D) = 0. Thus, ∀S sampled from D, we have ˆR
rej
U (RERMrej
H (S ); S ) = 0.
13

Compression
First, we define a compression function κ as through the following inflation and
discretization procedure. Given the training data S := {(xi, yi)}i∈[m], we define the following index
mapping:
I(x) = min{i ∈[m] : x ∈U(xi)},
∀x ∈
[
i∈[m]
U(xi).
(7)
In another word, this index function outputs the first indexed training sample to include x in its
neighborhood.
Then, we consider the set of RERM mapping learned by a size n subset of the training data:
ˆH = {RERMrej
H (L) : L ⊆S, |L| = n}.
(8)
Note that
| ˆH| ≤|{L : L ⊆S, |L| = n}| =
 
m
n
!
≤
em
n
n
.
(9)
Then, we inflate the data in the following way:
S U =
[
i∈[m]
 xI(x), x, yI(x)
 : x ∈U (xi)	 .
(10)
Note that xI(x) can be different from xi.
Let’s define the following transformation T:
T(h)(x, x′, y) := 1{h(x) , y ∨h(x′) < {y, ⊥}}, h ∈H.
(11)
And we can obtain the transformed hypothesis class T(H) := {T(h)|h ∈H}.
Now, we proceed to define the dual space G of T(H) as the following set of functions.
G := {g(x,x′,y)|g(x,x′,y)(t) = t(x, x′, y), t ∈T(H)}.
(12)
We denote the VC dimension of the dual space as VC∗(T(H)) := VC(G).
By Lemma Appendix A.1,
VC(T(H)) = O  (dr + dc) log (dr + dc) .
(13)
By the classic result in [Ass83], the VC dimension of the dual space satisfies the following inequality:
VC∗(T(H)) < 2VC(T(H))+1.
(14)
Now, we can construct the compressed dataset ˆS U as the following. For each (x, x′, y) ∈S U,
{g(x,x′,y)(t)}t∈T( ˆH) gives a labeling. When ranging over (x, x′, y) ∈S U, the labeling may not be unique.
So for each unique labeling, we choose a representative (x, x′, y) ∈S U, and let ˆS U be the set of the
representatives. That is:
ˆS U =

(x, x′, y) ∈S U
 {g(x,x′,y)(t)}t∈T( ˆH) provides a unique labeling

.
(15)
Intuitively, ˆS U split the infinite size dataset S U into finite size according to the labeling of T( ˆU) on
the dual space. Thus, ˆS U is not necessarily unique but always exists. And | ˆS U| equals the number of
possible labeling for T( ˆH).
Let d∗:= VC(G) = VC∗(T(H)) denote the VC-dimension of G, the dual hypothesis class of
T( ˆH) [Ass83]. By applying Sauer’s Lemma, we obtain that for |T( ˆH)| > d∗,
| ˆS U| ≤

e|T( ˆH)|
d∗

d∗
.
(16)
14

Let n = Θ (VC (T (H))). For m ≥n, we have
| ˆS U| ≤

e|T( ˆH)|
d∗
(17)
≤

e| ˆH|
d∗
(18)
≤

e
em
n
nd∗
(19)
≤
 e2m
n
!nd∗
(20)
=
 
e2m
VC(T(H))
!Θ(VC(T(H))·VC(T(H∗)))
.
(21)
Now we have obtain the compression map: κ(S ) = ˆS U.
Reconstruction
Now, we want to reconstruct a hypothesis from ˆS U. First, suppose we have a
data distribution over ˆS U, denoted as P. This distribution P over samples will be later used in the
α−boosting procedure.
Then, we sample the set of n i.i.d. samples from P and obtain S ′ ∈ˆS U. By classic PAC learn-
ing guarantee [BEHW89], for n = Θ(VC(T(H))) = Θ(dr + dc) log(dr + dc), we have with non-
zero probability ∀t ∈T(H) with P
(x,x′,y)∈S ′ t(x, x′, y) = 0 implies E(x,x′,y)∼Pt(x, x′, y) < 1/9. Let
L = {(x, y) : (x, x′, y) ∈S ′} ⊆S , and tP = T(RERMrej
H (L)). Since ˆR
rej
U (RERMrej
H (L); L) = 0,
∀(x, x′, y) ∈S ′, tP(x, x′, y) = 0. Thus, ∀P over ˆS U, there exists a weak learner tP ∈T( ˆH), s.t.
E(x,x′,y)∼P tP(x, x′, y) < 1/9.
Now, we use tP as a weak hypothesis in a boosting algorithm, specifically α−boost algorithm
from [SF12] with ˆS U as the dataset and Pk generated at each round of the algorithm. Then with
appropriate choice of α, running α−boosting for K = O(log(| ˆS U|)) rounds gives a sequence of
hypothesis h1, . . . , hK ∈ˆH and the corresponding ti = T(hi) such that ∀(x, x′, y) ∈ˆS U,
1
K
K
X
k=1
1{hk(x) , y ∨hk(x′) < {y, ⊥}}
(22)
= 1
K
K
X
k=1
tk(x, x′, y)
(23)
< 2
9 < 1
3.
(24)
Since ˆS U includes all the unique labellings, 1
K
PK
k=1 tk(x, x′, y) < 1
3, ∀(x, x′, y) ∈ˆS U implies
1
K
K
X
k=1
tk(x, x′, y) < 1
3, ∀(x, x′, y) ∈S U.
(25)
Let ¯h := Majority(h1, . . . , hK), i.e., ¯h outputs the prediction in Y ∪{⊥} that receives the most votes
from {h1, . . . , hK}. Then ∀(x, x′, y) ∈ˆS U,
1{¯h(x) , y ∨¯h(x′) < {y, ⊥}} = 0.
(26)
This is because: (1) on x, less than 1/3 of hi’s do not output y, so ¯h(x) = y; (2) on x′, less than 1/3 of
hi’s do not output y or ⊥, so the majority vote must be in y or ⊥, i.e., ¯h(x) ∈{y, ⊥}.
In summary, given the same m training samples, we can simply find a ¯h with 0 robust error on S :
ˆR
rej
U (¯h; D) =
m
X
i=1
sup
z∈U(x)
1{¯h(x) , y ∨¯h(z) < {y, ⊥}}
= 0.
(27)
15

Now we have the compression set with size:
nK = O(VC(T(H)) log(| ˆS U|)) = O(VC(T(H))2 VC∗(T(H)) log(m/ VC(T(H))))
Then, we apply Lemma 11 of [MHS19] (Replacing RU with Rrej
U still holds), we obtain for sufficiently
large m, with probability at least 1 −δ,
Rrej
U (¯h; D) ≤O
 
VC(T(H))2 VC∗(T(H)) 1
m log(m/ VC(T(H))) log(m) + 1
m log(1/δ)
!
.
(28)
We then can extend the sparsification procedure from [MY16, MHS19] to the rejection scenario.
Since t1, . . . , tK ∈T( ˆH), the classic uniform convergence results [SSBD14] implies that we can
sample N = O(VC∗(T(H))) i.i.d. indices i1, . . . , iN ∼Uniform([K]) and obtain:
sup
(x,x′,y)∈S U

1
N
N
X
j=1
ti j(x, x′, y) −1
K
T
X
i=1
ti(x, x′, y)

< 1
18
(29)
And thus, we can combine Equation (22) with Equation (29) and obtain:
∀(x, x′, y) ∈S U, 1
N
N
X
j=1
ti j(x, x′, y) ≤−1
18 + 1
K
K
X
i=1
tk(x, x′, y) < −1
18 + 4
9 = 1
2
we can further obtain an improved hypothesis ¯t′ := Majority(ti1, . . . tiN) with
¯t′(x, x′, y) = 0, ∀(x, x′, y) ∈S U
Thus, the compression set has a reduced size:
nN = O(VC(T(H)) · VC∗(T(H)))
Now, we apply Lemma 11 of [MHS19] and can obtain the following improved bound. Applying
similar strategy from Equation (26), we can obtain
¯h
′ := Majority(hi1, . . . hiN) = ρ( ˆS U) = A(S )
(30)
which is our full reconstruction map.
Then, for large sample size m ≥c VC(T(H)) VC∗(T(H)) (c is a sufficiently large constant), with
probability at least 1 −δ,
RU,rej(¯h′; D) ≤O
 
VC(T(H)) VC∗(H) 1
m log(m) + 1
m log(1/δ)
!
(31)
Plugging in Lemma Appendix A.1 and solving for m gives
MRE(ϵ, δ; H, U) = 2O(VC(T(H))) 1
ε log
 1
ε
!
+ O
 1
ε log
 1
δ
!!
(32)
= 2O((dr+dc) log(dr+dc)) 1
ε log
 1
ε
!
+ O
 1
ε log
 1
δ
!!
(33)
□
Lemma
[VC dimension of robust loss with rejection] Let VC(Hc) = dc, and VC(Hr) = dr. Then,
VC(T(H)) = O  (dr + dc) log (dr + dc).
Proof. Suppose d > dr + dc.
By definition of VC dimension, the max number of labeling of d points is 2d on h ∈T(H). And since
the label of h is a deterministic function of hc and hr, by Sauer’s Lemma, the number of labeling of h
is at most O(ddr) × O(ddc) = O(ddr+dc).
Thus, 2d = O(ddr+dc). And d = O((dr + dc) log(dr + dc)).
If d < dr + dc, d = O(dr + dc) log(dr + dc) by definition.
□
16

A.2
Rejection Only: Agnostic Case
Now, we define notion of PAC learnability in the agnostic case under rejection setting as the follows:
Definition A.4 (Robust PAC Learnability under Rejection). For Y = {0, 1}, ∀ϵ, δ ∈(0, 1), H =
Hc × Hr, the sample complexity of robust (ϵ, δ) - PAC learning of H with respect to perturbation U
under rejection, denoted as MAG(ϵ, δ; H, U), is defined as the smallest m ∈N ∪{0} for which there
exists a learning rule A : (X × Y)m 7−→(Y ∪{⊥})X s.t. for every data distribution D over (X × Y)m,
Rrej
U (A(S ); D) ≤OPTrej
U + ϵ
with probability at least 1 −δ over S ∼Dm. If no such m exists, MAG(ϵ, δ; H, U) = ∞. We say that
H is robustly PAC learnable under rejection if MAG(ϵ, δ; H, U) is finite for all ϵ, δ ∈(0, 1).
Lemma A.5. Let MRE = MRE(1/3, 1/3; H, U). Then,
MAG(ϵ, δ; H, U) = O
 MRE
ϵ2
log2
 MRE
ϵ
!
+ 1
ϵ2 log
 1
δ
!!
(34)
Proof. The proof detail follows exactly the same from the Proof of Theorem 8 from [MHS19] with
the loss replaced.
□
Theorem A.6 (Sample Complexity for Agnostic Robust PAC Learning under Rejection). In the
agnostic setting, for any H = Hc × Hr and U, and any ϵ, δ ∈(0, 1/2),
MAG(ϵ, δ; H, U) = O

VC(T(H)) VC∗(T(H)) log (VC(T(H)) VC∗(T(H)))
(35)
1
ε2 log2
 VC(T(H)) VC∗(T(H))
ε
!
+ 1
ε2 log
 1
δ
!
(36)
= 2O(VC(H)) 1
ε2 log2
 1
ε
!
+ O
 1
ε2 log
 1
δ
!!
(37)
= 2O((dr+dc) log(dr+dc)) 1
ε2 log2
 1
ε
!
+ O
 1
ε2 log
 1
δ
!!
(38)
where dr = VC(Hr), dc = VC(Hc).
Proof. Combining results from Lemma Lemma A.5 and Theorem A.2 gives the complexity result.
Solving Equation (37) gives the following generalization result given in Table 1
Pr
(x,y)∼Dn
h
Rrej
U (A(x, y); D) ≤ϵ
i
≥1 −δ
where ϵ = O
 q
2VC(T(H))+log(1/δ)
n
!
.
□
A.3
Transduction+Rejection: Realizable Case
We will prove a more general result which then implies Theorem 4.1. First, the training data can also
be perturbed, i.e., the adversary perturbs z ∈U(x) and ˜z ∈U(˜x), and the learner A are given (z, y, ˜z)
instead of (x, y, ˜z). The criterion in the transductive rejection error (see Table 2) is then the worst case
over both z ∈U(x) and ˜z ∈U(˜x). Second, we will consider OPTU3 = 0 and prove the guarantee
tolerating U2. This then implies the guarantee tolerating U when OPTU3/2 = 0.
In general the set of optimally learned classifiers ∆is defined as follows [MHS21]:
∆U
H(z, y, ˜z) =

{h ∈H : RU−1(h; z, y) = 0 ∧RU−1(h; ˜z) = 0}
(Realizable Case)
arg min
h∈H
max {RU−1(h; z, y), RU−1(h; ˜z)}
(Agnostic Case)
where
RU(h; z, y) = sup
˜x∈U(z)
1
n
n
X
i=1
1{h(˜xi) , yi}
17

and
RU(h; z) = RU(h; z, h(z)).
Recall the transformation F which we define following Tramèr [Tra22] in Section 4.
Then, we define the relaxed robust shattering dimension following [MHS21]:
Definition A.7 (Relaxed Robust Shattering Dimension). A sequence z1, . . . , zk ∈X is relaxed U-
robustly shattered by H, if ∀y1, . . . , yk ∈{±1}: ∃xy1
1 , . . . , xyk
k ∈X and ∃h ∈H such that zi ∈U(xyi
i )
and h(U(xyi
i )) = yi, ∀1 ≤i ≤k. The relaxed U-robust shattering dimension rdimU(H) is defined as
the largest k for which there exist k points that are relaxed U-robustly shattered by H.
Define the set of intermediate perturbations as follows:
Definition A.8 (Intermediate Perturbations). Given x and z and perturbations U1 and U2, the set of
possible intermediate perturbations between x and z is
ipU1,U2(x, z) =
({x}
if x = z
U1(x) ∩U−1
2 (z)
otherwise
Theorem A.9. For any n ∈N, δ > 0, class H, perturbation set U, and distribution D over X × Y
satisfying OPTU−1U = 0:
Pr
(x,y)∼Dn
(˜x,˜y)∼Dn
" ∀z ∈U3(x), ∀z0 ∈ipU,U2(x, z), ∀˜z ∈U3(˜x), ∀˜z0 ∈ipU,U2(˜x, ˜z),
∀ˆh ∈FU

∆U
H(z0, y, ˜z0)

: errrej(ˆh; x, y, ˜x, ˜z, ˜y) ≤ϵ
#
≥1 −δ
where ϵ =
rdimU−1(H) log(2n)+log(1/δ)
n
≤VC(H) log(2n)+log(1/δ)
n
.
Proof. We adapt the strategy of Theorem 5 of [Tra22] for the rejection scenario.
By setting z = z0, ˜z = ˜z0 and applying Theorem 1 of [MHS21], we obtain the following
Pr
(x,y)∼Dn
(˜x,˜y)∼Dn
h
∀z0 ∈U(x), ∀˜z0 ∈U(˜x), ∀h ∈∆U
H(z0, y, ˜z0) : err˜z0,˜y(h) ≤ϵ
i
≥1 −δ
(39)
as OPTU−1(U) = 0.
Suppose (x, y), (˜x, ˜y) ∼Dn. Now, let z ∈U3(x), ˜z ∈U3(˜x) and take some z0 ∈ipU,U2(x, z), ˜z0 ∈
ipU,U2(˜x, ˜z), both of which are necessarily nonempty as U3 = U2U, and ˆh ∈FU

∆U
H(z0, y, ˜z0)

.
Write ˆh = FU(h) for some h ∈∆U
H(z0, y, ˜z0).
From Equation (39) (replacing z with z0 and ˜z with ˜z0), it is enough to show that
errrej(ˆh; x, y, ˜x, ˜z, ˜y) ≤err˜z0,˜y(h).
Suppose that ˆh incurs an error under rejection at point ˜zi; it is enough to show that h incurs an
error at ˜z0i. Furthermore, note that because h ∈∆U
H(z0, y, ˜z0), we have that h(U−1(˜z0i)) = {h(˜z0i)} as
˜z0i ∈U−1(˜z0i). Write h(˜z0i) = ˆyi.
We have one of the following:
1. ˆh(˜zi) , ˜yi and ˜zi = ˜xi
2. ˆh(˜zi) < {˜yi, ⊥} and ˜zi , ˜xi
In the first case, we must have ˜z0i = ˜xi as well as ˜z0i is an intermediate perturbation between ˜xi and ˜zi,
so, as h(U−1(˜zi)) = h(U−1(˜z0i)) = ˆyi, ˆh does not reject ˜z0i and ˆh(˜z0i) = ˆyi. Hence, h(˜z0i) = ˆyi as well
so, as ˆh makes an error at ˜zi, ˆyi , y and so h makes an error at ˜z0i.
In the second case, if h(U−1(˜zi)) , {h(˜zi)}, then ˆh would reject ˜zi and hence not incur an error.
So h(U−1(˜zi)) = {h(˜zi)} and so ˆh(˜zi) = h(˜zi). Since ˜z0i ∈U(˜xi) ∩U−2(˜zi), there exists some
˜z′
0i∈U(˜z0i) ∩U−1(˜zi) and so, h(˜z0i) = h(˜z′
0i) = h(˜zi) = ˆh(˜zi) = ˆyi, so h incurs an error at ˜z0i.
In either case, we have that h makes an error at ˜z0i, showing the result.
□
18

Sample Complexity
Given ϵ and δ, we need
rdimU−1(H) log(2n) + log(1/δ)
n
≤ϵ
for the result to hold.
Now, noting that log(2n) = 1 + log n ≤1 + √n for n ≥16; hence we need to solve for the n such that
rdimU−1(H)(1 + √n) + log(1/δ)
n
= ϵ
or, equivalently
rdimU−1(H) + log( 1
δ) + √n
n
= ϵ
or
√n = nϵ −rdimU−1(H) −log(1
δ)
or
n = n2ϵ2 −2ϵ
 
rdimU−1(H) + log(1
δ)
!
n +
 
rdimU−1(H) + log(1
δ)
!2
or
n2ϵ2 −
 
2ϵ
 
rdimU−1(H) + log(1
δ)
!
+ 1
!
n +
 
rdimU−1(H) + log(1
δ)
!2
= 0.
Solving, the result holds if
n ≥
2ϵ

rdimU−1(H) + log( 1
δ)

+ 1 +
q
(2ϵ

rdimU−1(H) + log( 1
δ)

+ 1)2 −4

rdimU−1(H) + log( 1
δ)
2 ϵ2
2ϵ2
= O

rdimU−1(H) + log( 1
δ)
ϵ
+
q
rdimU−1(H) + log( 1
δ)
ϵ
3
2

and, similarly, using
rdimU−1(H) log(2n) + log(1/δ)
n
≤VC(H) log(2n) + log(1/δ)
n
we have the result if
n = O

VC(H) + log( 1
δ)
ϵ
+
q
VC(H) + log( 1
δ)
ϵ
3
2

Remark:
If OPTU−1U = 0, we can guarantee the existence of an ˆh which satisfies our conditions,
but we can’t guarantee that we will find it, as we cannot find ∆U
H(z0, y, ˜z0) without z0 and ˜z0. We can,
however, construct that an algorithm which, if it returns a model, always returns on which meets the
conditions.
Simplified Result
To obtain a bound which does not involve an intermediate perturbation step, we
may let
∆U
rej,H(z, y, ˜z) :=
\
z′∈U−2(z), ˜z′∈U−2(˜z)
∆U
H(z′, y, ˜z′)
Note that for common classes of perturbations, we can simplify the definition of ∆rej. Note that the
conditions of the theorem hold for perturbations defined via ϵ-balls in a metric.
Lemma A.10. In the realizable case, if U = U−1,
∆U
rej,H(z, y, ˜z) = ∆U3
H (z, y, ˜z)
19

Proof. Suppose h ∈∆U
rej,H(z, y, ˜z). Then by the definitions of ∆rej and ∆, for any z′ ∈U−2(z), ˜z′ ∈
U−2(˜z), we have that, for any x ∈U−1(z′) and ˜x ∈U−1(˜z′), h(xi) = h(z′
i) and h(˜xi) = h(˜z′
i). Now, as
there exists some z′′ ∈U(z′) ∩U−1(bz) and h(x) = h(z′) = h(z′′) = h(z) by an argument similar
to that in Theorem A.9 and similarly for ˜x and ˜z, we have that for any x ∈U−3(z) and ˜x ∈U−3(˜z),
h(xi) = h(zi) and h(˜xi) = h(˜zi), and so
∆U
rej,H(z, y, ˜z) ⊆∆U3
H (z, y, ˜z)
Now, if h ∈∆U3
H (z, y, ˜z), we have that, for any x ∈U−3(z) and ˜x ∈U−3(˜z), h(xi) = h(zi) and h(˜xi) =
h(˜zi). Now, suppose z′ ∈U−2(z), ˜z′ ∈U−2(˜z). Since x ∈U(x) for all x, z′ ∈U−3(z), ˜z′ ∈U−3(˜z)
as well. Hence, h(z′
i) = h(zi) and h(˜z′
i) = h(˜zi). Now, if x ∈U−1(z′) and ˜x ∈U−1(˜z′), we have
x ∈U−3(z) and ˜x ∈U−3(˜z) and so h(xi) = h(zi) and h(˜xi) = h(˜zi). But then h(xi) = h(z′
i) and
h(˜xi) = h(˜z′
i). Hence, we have that
∆U3
H (z, y, ˜z) ⊆∆U
rej,H(z, y, ˜z)
and the result follows.
□
Now, by the above and from Theorem A.9 we may immediately derive Theorem 4.1 by noting
that if U = U−1, U−1U = U2, and if ˆh ∈FU(∆U
H(z, y, ˜z)) = FU1/3(∆U1/3
rej,H(z, y, ˜z)) then we have
ˆh ∈FU1/3

∆U1/3
H
(z0, y, ˜z0)

for some z0 ∈ipU1/3,U2/3(x, z) and ˜z0 ∈ipU1/3,U2/3(˜x, ˜z).
A.4
Transduction+Rejection: Agnostic Case
Note that, if U can be decomposed into a form U = (U1/3)3 where U1/3 = U−1/3 (as with standard
perturbations in lp), we obtain a bound which depends on OPTU2/3 rather than OPTU2, enabling, for ˆh
satisfying the conditions, much stronger guarantees if OPTU2/3 << OPTU2. Note that as ∀x x ∈U(x),
∀x U2/3(x) ⊆U2(x), and so OPTU2/3 ≤OPTU2.
Theorem A.11. For any n ∈N, δ > 0, class H, perturbation set U, and distribution D over X × Y:
Pr
(x,y)∼Dn
(˜x,˜y)∼Dn
" ∀z ∈U3(x), ∀z0 ∈ipU,U2(x, z), ∀˜z ∈U3(˜x), ∀˜z0 ∈ipU,U2(˜x, ˜z),
∀ˆh ∈FU

∆U
H(z0, y, ˜z0)

: errrej(ˆh; x, y, ˜x, ˜z, ˜y) ≤ϵ
#
≥1 −δ
where
ϵ = min
2 OPTU−1U +O

r
VC(H) + log(1/δ)
n
, 3OPTU−1U + O

r
rdim U(H) ln(2n) + ln(1/δ)
n

.
Proof. Suppose (x, y), (˜x, ˜y) ∼Dn.
Now, let z ∈U3(x), ˜z ∈U3(˜x) and take some z0 ∈
ipU,U2(x, z), ˜z0 ∈ipU,U2(˜x, ˜z), both of which are necessarily nonempty, and ˆh ∈FU

∆U
H(z0, y, ˜z0)

.
Write ˆh = FU(h) for some h ∈∆U
H(z0, y, ˜z0).
We will begin as in Theorem A.9. As before, there are two cases in which ˆh can incur an error at ˜zi:
1. ˆh(˜zi) , ˜yi and ˜zi = ˜xi
2. ˆh(˜zi) < {˜yi, ⊥} and ˜zi , ˜xi
Now, if ˜zi = ˜xi, an error occurs if ˆh rejects ˜zi or if h robustly predicts some ˆyi , ˜yi; hence an error
occurs if h is not U−1-robust at ˜z0i or if h(˜z0i) , ˜yi.
Otherwise, h must be U−1-robust at ˜zi, as, otherwise, ˆh would reject ˜zi. Hence, as there exists some
˜z′
0i ∈U(˜z0i) ∩U−1(˜zi), if h is U-robust at ˜z0i, we must have h(˜zi) = h(˜z0i), and so, if ˆh makes an error,
h is not U−1-robust at ˜z0i or h(˜z0i) , ˜yi.
20

Now, in both cases, errors only occur if h is not U−1-robust at ˜z0i or h(˜z0i) , ˜yi. As ˜xi ∈U−1(˜z0i), we
have, equivalently, that an error occurs if h is not U−1-robust at ˜z0i or h(˜xi) , ˜yi.
Hence,
errrej(ˆh; x, y, ˜x, ˜z, ˜y) ≤errrej(h; ˜x, ˜y) + RU−1(h; ˜z0)
Now, the right hand is exactly what is bounded in Theorem 2 of [MHS21]; as we have h ∈
∆U
H(z0, y, ˜z0), we have
errrej(ˆh; x, y, ˜x, ˜z, ˜y) ≤errrej(h; ˜x, ˜y) + RU−1(h; ˜z0) ≤ϵ
where
ϵ = min
2 OPTU−1U +O

r
VC(H) + log(1/δ)
n
, 3OPTU−1U + O

r
rdim U(H) ln(2n) + ln(1/δ)
n


with probability ≥1 −δ by its proof.
□
As in the realizable case, we can immediately derive the following corollary. However, we cannot
simplify the definition of ∆rej as before; see Lemma A.13.
Corollary A.12. For any n ∈N, δ > 0, class H, perturbation set U where U = U−1, and distribution
D over X × Y:
Pr
(x,y)∼Dn
(˜x,˜y)∼Dn
∀z ∈U3(x), ∀˜z ∈U3(˜x), ∀ˆh ∈FU

∆U
rej,H(z, y, ˜z)

:
errrej(ˆh; x, y, ˜x, ˜z, ˜y) ≤ϵ
≥1 −δ
where
ϵ = min
2 OPTU−1U +O

r
VC(H) + log(1/δ)
n
, 3OPTU−1U + O

r
rdim U(H) ln(2n) + ln(1/δ)
n

.
Lemma A.13. In the agnostic case, we have that if U = U−1,
∆U
rej,H(z, y, ˜z) ⊆∆U3
H (z, y, ˜z)
Proof. By the definition of R, we have
RU−3(h; ˜z) = 1
n
n
X
i=1
1
n
∃˜xi ∈U−3 (˜zi) : h (˜xi) , h (˜zi)
o
= 1
n
n
X
i=1
1
n
∃˜z′
i ∈U−2 (˜zi) ∃˜xi ∈U−1 
˜z′
i

: h (˜xi) , h (˜zi)
o
=
max
˜z′
i∈U−2(˜zi)
1
n
n
X
i=1
1
n
∃˜xi ∈U−1  ˜z′
i
 : h (˜xi) , h (˜zi)
o
=
max
˜z′
i∈U−2(˜zi)RU−1(h; ˜z′)
where the last equality holds as x ∈U(x) for all x and as U = U−1, which together show that if for
some ˜zi and ˜z′
i ∈U−2(˜zi) we have that h(˜z′
i) , h(˜zi), that either there exists some ˜z′′
i ∈U = U−1(˜z′
i)
such that h(˜z′′
i ) , h(˜z′
i) or there exists some ˜z′′
i ∈U = U−1(˜zi) such that h(˜z′′
i ) , h(˜zi) (as before, note
that ˜zi = U(˜z′′
i ) for some ˜z′′
i ∈U(˜z′
i) by the definition of U3); the reverse is similar.
We can derive a result for RU−3(h; z, y) similarly.
Suppose h ∈∆U
rej,H(z, y, ˜z).
Then, h minimizes max {RU−1(h; z′, y), RU−1(h; ˜z′)} for all z′ ∈
U−2(z), ˜z′ ∈U−2(˜z), so by the above, h must also minimize
max
z′∈U−2(z),˜z′∈U−2(˜z) max RU−1(h; z′, y), RU−1(h; ˜z′)	
= max
(
max
z′∈U−2(z)RU−1(h; z′, y),
max
˜z′∈U−2(˜z)RU−1(h; ˜z′)
)
= max {RU−3(h; ˜z), RU−3(h; z, y)}
21

and so h ∈∆U3
H (z, y, ˜z).
However, minimizing
max
z′∈U−2(z),˜z′∈U−2(˜z) max RU−1(h; z′, y), RU−1(h; ˜z′)	
does not necessarily imply that h minimizes max {RU−1(h; z′, y), RU−1(h; ˜z′)} for all z′ ∈U−2(z), ˜z′ ∈
U−2(˜z), so the reverse may not hold.
□
A.5
Extension to Unbalanced Training and Test Data
We provide a sketch of a proof that allows extending Theorem 1 of [MHS21] to unbalanced training
and test sets; however, for simplicity, we will work with the original form. The assumptions are the
same, except that we have n training points and m test points.
The proof is exactly as before up to the "Finite robust labelings" portion (which points are and are
not labelled don’t matter up to then and the symmetry arguments still apply). The basic idea of
determining the probability of zero loss on the training and test sets and error > ϵ on the test examples
with permutation still applies. Let Eσ,x be the event that there exists a labelling ˆh(xσ(1:n+m)) in the
allowable set where this occurs.
We have
Pr
σ
Eσ,x
 ≤Pr
σ
h
∃ˆh ∈ΠU
H(x1, . . . , xn+m) : errxσ(1:n),yσ(1:n)(ˆh) = 0 ∧errxσ(n:n+m),yσ(n:n+m)(ˆh) > ϵ
i
and, as in [MHS21], note the probability of choosing such a perturbation σ for a fixed ˆh is at most

m
n + m
s
≤

m
n + m
⌈ϵm⌉
=
n + m
m
−⌈ϵm⌉
≤
n + m
m
⌈−ϵm⌉
if we assume the number of total errors s ≥⌈ϵm⌉without loss of generality (otherwise, err > ϵ would
be impossible).
Hence, by a union bound,
Pr
σ
Eσ,x
 ≤
ΠU
H(x1, . . . , xn+m)

n + m
m
⌈−ϵm⌉
and so
Pr
σ
Eσ,x
 ≤(n + m)rdimU−1(H)n + m
m
⌈−ϵm⌉
by Sauer’s Lemma (in the form of Lemma 3 of [MHS21]).
Now, we bound the probability by δ, we need
(n + m)rdimU−1(H)n + m
m
⌈−ϵm⌉
≤δ
which, solving, gives us
ϵ ≥
rdimU−1(H) log n+m
m (n + m) + log n+m
m
1
δ
m
=
rdimU−1(H) log(n + m) + log 1
δ
m log

1 + m
n

Which reduces to the original result if n = m (note that the logarithms are base-2).
Corollary
If we fix n + m, H, and δ, the guarantee is strongest (i.e. we minimize ϵ) when n = m.
To see this, consider the denominator. Write α = m
n . Then, we wish to maximize nα log(1 + α) (or
equivalently f(α) = α log(1 + α) subject to α ≥0. Now, note that f ′(α) = log(1 + α) −1 = 0 when
α = 1, i.e. when m = n.
Also, we can see from the result above, that if we fix m and δ, then the minimum value of ϵ tends
towards ∞as n →∞, so there does not necessarily exist a labelled training set sampled from D
which provides a guarantee with high probability of arbitrarily low error on a fixed test set.
22

B
Experimental Details
B.1
Computing Infrastructure
We used a SLURM cluster with A100 GPUs to run our experiments.
B.2
Baseline Details
The baselines are trained with standard adversarial training [GSS14] [MMS+18]. Attacks against AT
without rejection use standard PGD with a cross-entropy objective, while attacks against AT with
rejection use PGD targeting LREJ as described in algorithm 3. In all cases, the parameters for PGD in
training are the same as those used in TLDR’s training process for the same dataset.
B.3
Defense
In our implementation, we begin to incorporate the transductive term in our objective (see Equa-
tion (1)) after initially training the model with the inductive loss term only; this allows learning a
better baseline before we begin to enforce robustness about the test points. In our experiments, we
use the transductive loss in the final half of the training epochs, and put 85% of the weight on the
inductive term afterwards.
B.4
Adaptive Attack
Solving for the perturbation ˜x by iteratively optimizing LREJ poses several difficulties.
First, the rejection-avoidance term
˜x −arg max||x′−˜x||≤ϵ LDB,h(x′)
 is not differentiable with respect to
˜x. While it is possible to approximate the derivative with the derivative of a proxy (e.g. differentiating
though some fixed number of PGD steps, necessitating second-order optimization), this is extremely
expensive and does not improve results in our experiments (see below).
Intuitively, we might see that this would be the case: if the decision boundary is smooth, we might
expect the maximizers in U(x + ∆) and U(x) to be the same for small ∆unless x′ is near the border
of U(x) given that U(x + ∆) ≈U(x). In this case, approximating x′ as constant with respect to x is
reasonable.
In addition, note that if h(x) = y, the adversary must find a ˜x where h(˜x) , y which is not rejected: if
maximizing LREJ with PGD, the rejection-avoidance term penalizes moving ˜x towards the decision
boundary. As this is necessary to find a valid attack (when h(˜x) = y at initialization), we adjust λ
adaptively during optimization by setting it to zero when h(˜x) = y.
B.5
Transductive Attack Details
We present two rejection-aware transductive attacks: a stronger but more computationally intensive
rejection-aware GMSA (Algorithm 1) and a weaker but faster rejection-aware transfer attack which
takes the transductive robust rejection risk into account (Algorithm 2).
Finally, note the attack with LREJ, without GMSA, is effective against selective classifiers based on
the transformation F (and via Tramèr’s equivalency, selective classifiers in general). So we summarize
this attack on a fixed model in Algorithm 3.
B.6
Rejectron Experiments
Goldwasser et al.’s implementation of Rejectron [GKKM20] trains a classifier (call it hc) on the
training set and a discriminator (hd) to distinguish between the (clean) training and (potentially-
perturbed) test data. Samples are rejected if the discriminator classifies them as test data; otherwise,
the classifier’s prediction is returned. Our adaptive attack is then very simple: we follow the approach
of Algorithm 1 but with a loss function LDISC which targets the defense.
Given a sample (x, y), the attacker’s goal is to flip the label, and, simultaneously, to avoid rejection;
hence, we maximize the following loss:
LDISC(x, y) = LCE(hs
c(x), y) + λLCE(hs
d(x), 1)
23

Algorithm 1 Rejection-Aware GMSA
Require: A clean training set T, a clean test set E, a transductive learning algorithm for classifiers A, an
adversarial budget of ϵ, mode either MIN or AVG, a radius used for rejection ϵdefense, and a maximum
number of iterations N ≥1. E|X refers to the projection on the feature space for E.
1: Search for a perturbation of the test set which fools the model space induced by (T, U(E|X)).
2: E′ = E
3: ˆE = E
4: errmax = −inf
5: for i=0,...,N-1 do
6:
Train a transductive model on the perturbed data.
7:
h(i) = A(T, E′|X)
8:
err =
1
|E′|
|E′|
X
i=1
1
n
F(h(i)) (˜xi) < { ˜yi} ∧˜xi = xi

∨

F(h(i)) (˜xi) < { ˜yi, ⊥} ∧˜xi , xi
o
{The ˜xi and the xi are the ith datapoints of E′ and E, repectively; yi is the true label.}
9:
if errmax < err then
10:
ˆE = E′
11:
end if
12:
for j = 1, . . . , |E| do
13:
if mode = MIN then
14:
˜x j = arg max
∥˜x−x j∥≤ϵ min
1≤k≤i LREJh(k)(˜x, y j)
15:
else
16:
˜xj = arg max
∥˜x−xj∥≤ϵ
1
i
iX
k=1
LREJh(k)(˜x, y j)
17:
end if
{Select whether to perturb by comparing success rates against past models for the clean and perturbed
samples.}
18:
errclean = 1
i
X
0≤k≤i
1
h
F

h(k)
(xj) , y j
i
19:
errperturbed = 1
i
X
0≤k≤i
1
h
F

h(k)
(˜xj) < {y j, ⊥}
i
{Do not perturb if the perturbation reduces robust rejection accuracy less on average than leaving the
points unchanged.}
20:
if errperturbed < errclean then
21:
˜xj = xj
22:
end if
23:
E′
j = ˜xj, yi
24:
end for
25: end for
26: Return: ˆE
where class 1 for hd corresponds to test data, signalling rejection, and where hs returns the softmax
activations of h. Maximizing LDISC then minimizes the confidence in the true label and the probability
of rejection.
Figures 2 and 3 show our adaptive attack’s performance on MNIST and CIFAR-10. τ is a key
hyperparameter of Rejectron, which determines the confidence needed by hd to reject a sample; to
evaluate Rejectron fairly, we report the results on best-performing value of τ, based on (transductive)
robust rejection accuracy; see Table 3. On CIFAR-10, performance is near-zero and rejection rate
is near 100% for small values of τ. The best-performing value of τ is 1 (effectively eliminating the
possibility of rejection), leading to a rejection rate of 0; this behavior on CIFAR-10 illustrates the
algorithm’s struggles with the practical high-complexity deep learning setting.
24

Algorithm 2 Transductive Rejection-Aware Transfer
Require: A model h, a clean labelled test point (x, y), an adversarial budget of ϵ, and a radius used for rejection
ϵdefense.
{Search for a perturbation ˜x of x for which h predicts ˆy , y robustly.}
1:
˜x = arg max
∥˜x−x∥≤ϵ
"
LCE(hs(˜x), y) + λ
˜x −arg
max
∥x′−˜x∥≤ϵdefense LDB,h(x′)

#
where LCE is the cross-entropy loss, hs returns the softmax activations of h and where
LDB,h(x) = rank2hs(x) −max hs(x).
{If the attack did not succeed against h (in other words, if h does not robustly predict ˆy , y), check whether
to leave x unperturbed.}
2:
x′ = arg
max
∥x′−˜x∥≤ϵdefense LCE(hs(x′), h(˜x))
3: if h(x′) , h(˜x) ∨h(˜x) = y then
4:
Leave x unperturbed if F(h) rejects it, or if h(x) , y.
5:
x′′ = arg
max
∥x′′−x∥≤ϵdefense LCE(hs(x′′), h(x))
6:
if h(x) , y ∨h(x′′) , h(x) then
7:
˜x = x
8:
end if
9: end if
10: Return: ˜x
Algorithm 3 Inductive Rejection-Aware Attack
Require: A model h, and a clean labelled test point (x, y), an adversarial budget of ϵ, and a radius used for
rejection ϵdefense.
1: Search for a perturbation ˜x of x for which h predicts ˆy , y robustly.
˜x = arg max
∥˜x−x∥≤ϵ
"
LCE(hs(˜x), y) + λ
˜x −arg
max
∥x′−˜x∥≤ϵdefense LDB,h(x′)

#
where LCE is the cross-entropy loss, hs returns the softmax activations of h and where
LDB,h(x′) = rank2hs(x′) −max hs(x′)
2: Return: ˜x
C
Ablation Studies
C.1
Warm Start in TLDR
Warm start (epochs)
Rejection Rate
Robust Rejection Accuracy
0
0.813
0.153
500
0.531
0.177
1000
0.830
0.171
Here we perform experiments showing that in training TLDR, it is best to first trains a baseline model
without transductive regularization Ltest in the early stage (warm start) and then add transductive
regularization for later training.
We generate the data with 100 Gaussians (one per class) equally spaced in l∞with a separation of
3 units between means. The adversarial budget is 2 units, and we ensure that the data is sparse by
generating 10 samples per class. The models are 10 layer feedforward networks with skip connections.
The synthetic models are trained for 1000 epochs total; we see the best performance when the model
has transductive regularization but is allowed to learn an initial baseline model before transductive
regularization is used in training. Doing so reduces the risk of the regularization term harming
performance.
25

Figure 2: Effects of τ on performance of Rejec-
tron on MNIST with attacker GMSA (LDISC).
Figure 3: Effects of τ on performance of Rejec-
tron on CIFAR-10 with attacker GMSA (LDISC).
TLDR Components
Attacker
MNIST
CIFAR-10
Rejection
Transductive Regularization
pREJ
Robust accuracy
pREJ
Robust accuracy
✓
✓
GMSAAVG (LREJ)
0.796
0.968
0.195
0.744
✓
✓
GMSAMIN (LREJ)
0.588
0.967
0.208
0.739
✓
×
GMSAAVG (LREJ)
0.646
0.975
0.179
0.725
✓
×
GMSAMIN (LREJ)
0.202
0.980
0.182
0.733
×
✓
GMSAAVG (LCE)
–
0.900
–
0.516
×
✓
GMSAMIN (LCE)
–
0.914
–
0.601
×
×
GMSAAVG (LCE)
–
0.935
–
0.516
×
×
GMSAMIN (LCE)
–
0.942
–
0.556
Table 7: Full ablation results of TLDR.
C.2
GMSA Method
We present extended results of our defense ablation and compare the results of GMSAAVG, which
optimizes the average loss of past iterations, and GMSAMIN, which optimizes the worst-case loss.
See [CWG+22]. We can see that while the two perform about the same on the full TLDR defense
(GMSAMIN performs slightly better), GMSAAVG is much stronger for models not incorporating both
components.
C.3
Rejection Radius
0.2
0.4
0.6
0.8
1.0
ϵdefense/ϵ
0.0
0.2
0.4
0.6
0.8
1.0
Robust Accuracy
0.2
0.4
0.6
0.8
1.0
ϵdefense/ϵ
0.0
0.2
0.4
0.6
0.8
1.0
Adversarial Rejection Rate
0.2
0.4
0.6
0.8
1.0
ϵdefense/ϵ
0.0
0.2
0.4
0.6
0.8
1.0
Clean Rejection Rate
Figure 4: Effects of rejection radius ϵdefense on MNIST (inductive) with attacker PGD (LREJ).
The rejection radius ϵdefense is an important hyper-parameter for TLDR; however, the model’s perfor-
mance is not very sensitive to it. Figure 4 shows the trend of robust accuracy, the rejection rate on
adversarial test data, and the rejection rate on clean test data, for the inductive classifier on MNIST;
Figure 5 shows those for TLDR. The robust accuracy remains stable. The theoretical analysis suggests
setting the radius to ϵ/3 where ϵ is the adversarial budget. Given TLDR’s low sensitivity to the
parameter, we use ϵ/4 for consistency as the inductive case performs best with that setting. The
rejection rate on the adversarial test data rises rapidly with the rejection radius (reaching 0.949 for
TLDR for ϵdefense = ϵ), but the rejection rate on clean data increases much more slowly (0.108 when
ϵdefense = ϵ). So among all rejected inputs only a few are clean inputs, leading to low errors as desired.
26

0.2
0.4
0.6
0.8
1.0
ϵdefense/ϵ
0.0
0.2
0.4
0.6
0.8
1.0
Robust Accuracy
0.2
0.4
0.6
0.8
1.0
ϵdefense/ϵ
0.0
0.2
0.4
0.6
0.8
1.0
Adversarial Rejection Rate
0.2
0.4
0.6
0.8
1.0
ϵdefense/ϵ
0.0
0.2
0.4
0.6
0.8
1.0
Clean Rejection Rate
Figure 5: Effects of rejection radius ϵdefense on MNIST (TLDR) with attacker GMSA (LREJ).
Figure 6: Robustness scaling with adversarial budget ϵ on MNIST
The rejection rate on clean inputs is presented for the transductive case in order to illustrate the
difference in effects on clean and perturbed data, but, as the adversary may select to perturb, some
clean points were not in the training set, and, hence, the clean rejection rates should not be considered
reliable. The rejection rates rise with the rejection radius: adversarial rejection rates increase rapidly
as the rejection radius increases, while clean rejection rates increase only slowly. In all cases, far
more perturbed samples are rejected than clean samples.
C.4
Ablation on Attacks: Attack Radius
The theory suggests that incorporating rejection can allow a transductive learner to tolerate perturba-
tions twice as large; we investigate how transduction and rejection affects the robustness as ϵ grows
(models are adversarially trained with the corresponding ϵ and the selective classifiers use a rejection
radius of ϵ/2). The results are shown for the natural choice of adversary, as in the experiment section
(e.g. GMSA with LREJ for the transduction+rejection). For selective classifiers, the rejection rate
scaling is shown.
We see that the combination of rejection and transduction does indeed maintain high accuracy for
larger ϵ; at ϵ = 0.6, it has 96.2% of the robust accuracy that transduction alone had for ϵ = 0.3. This
aligns with the theory, given the increased constant factors of OPTU2 in Corollary A.12 compared to
the results for classifiers in [MHS21].
Note also the behavior of the inductive classifier: accuracy improves past ϵ = 0.6. To see why,
note that a model adversarially trained for ϵ ≥1 will return near-uniform predictions for all classes
(resulting in a robust accuracy of approximately 10%, as seen), making finding adversarial examples
slightly more difficult than for smaller ϵ where this does not occur. The decline in rejection rate for
very large ϵ is a similar phenomenon.
C.5
Weighting of LREJ
We examine the effect of the hyperparameter λ′ between the cross-entropy and rejection-avoidance
terms in LREJ on MNIST; see Equation 3. In the inductive case, as shown in Figure 8, there is little
sensitivity to λ′ in either attack success rate or rejection rate. When targeting TLDR, there is little
27

Figure 7: Rejection rate scaling with adversarial budget ϵ on MNIST.
0.0
0.5
1.0
1.5
2.0
′
0.0
0.2
0.4
0.6
0.8
1.0
Robust Accuracy
0.0
0.5
1.0
1.5
2.0
′
0.0
0.2
0.4
0.6
0.8
1.0
Adversarial Rejection Rate
Figure 8: Effects of λ′ on results of PGD optimizing LREJ targeting adversarial training with rejection
on MNIST.
sensitivity in terms of attack success rate as seen in Figure 9; rejection rate is highest for intermediate
values of λ′ but, as expected, rejection rate declines with λ′ beyond that.
D
Limitations
While our framework is theoretical-sound with lower sampled complexity than the rejection-only
case and with more relaxed optimality condition than the transductive-only case, our sample com-
plexity proof under the transductive rejection case requires the non-emptiness of ∆in Theorem 4.1.
While weaker conditions don’t guarantee that we find a model satisfying the conditions, the result
demonstrate that empirical defense incorporating both transduction and rejection have the potential to
outperform others. Our proposed defense algorithm TLDR, though effective at improving the robust
accuracy under rejection, incurs a high computational cost relative to standard adversarial training
due to the joint training with the unlabeled data. If it is possible to delay evaluation until a sufficiently
large batch of samples arrives, the cost can be made insignificant via amortization. The need to
perform a full training process prior to evaluation means, however, that the defense is not suitable
for latency-sensitive applications. Our adaptive attack is even more costly, as effectively attacking
this defense using GMSA requires multiple iterations of the full transductive training process; hence,
adversaries attacking TLDR require substantial resources.
28

0.0
0.5
1.0
1.5
2.0
′
0.0
0.2
0.4
0.6
0.8
1.0
Robust Accuracy
0.0
0.5
1.0
1.5
2.0
′
0.0
0.2
0.4
0.6
0.8
1.0
Adversarial Rejection Rate
Figure 9: Effects of λ′ on results of GMSA optimizing LREJ targeting TLDR on MNIST.
29

