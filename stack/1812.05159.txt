Published as a conference paper at ICLR 2019
AN EMPIRICAL STUDY OF EXAMPLE FORGETTING
DURING DEEP NEURAL NETWORK LEARNING
Mariya Toneva∗†
Carnegie Mellon University
Alessandro Sordoni∗
Microsoft Research Montreal
Remi Tachet des Combes∗
Microsoft Research Montreal
Adam Trischler
Microsoft Research Montreal
Yoshua Bengio
MILA, Universit´e de Montr´eal
CIFAR Senior Fellow
Geoffrey J. Gordon
Microsoft Research Montreal
Carnegie Mellon University
ABSTRACT
Inspired by the phenomenon of catastrophic forgetting, we investigate the learning
dynamics of neural networks as they train on single classiﬁcation tasks. Our goal
is to understand whether a related phenomenon occurs when data does not un-
dergo a clear distributional shift. We deﬁne a “forgetting event” to have occurred
when an individual training example transitions from being classiﬁed correctly to
incorrectly over the course of learning. Across several benchmark data sets, we
ﬁnd that: (i) certain examples are forgotten with high frequency, and some not
at all; (ii) a data set’s (un)forgettable examples generalize across neural architec-
tures; and (iii) based on forgetting dynamics, a signiﬁcant fraction of examples
can be omitted from the training data set while still maintaining state-of-the-art
generalization performance.
1
INTRODUCTION
Many machine learning models, in particular neural networks, cannot perform continual learning.
They have a tendency to forget previously learnt information when trained on new tasks, a phe-
nomenon usually called catastrophic forgetting (Kirkpatrick et al., 2017; Ritter et al., 2018). One of
the hypothesized causes of catastrophic forgetting in neural networks is the shift in the input distri-
bution across different tasks—e.g., a lack of common factors or structure in the inputs of different
tasks might lead standard optimization techniques to converge to radically different solutions each
time a new task is presented. In this paper, we draw inspiration from this phenomenon and investi-
gate the extent to which a related forgetting process occurs as a model learns examples traditionally
considered to belong to the same task.
Similarly to the continual learning setting, in stochastic gradient descent (SGD) optimization, each
mini-batch can be considered as a mini-“task” presented to the network sequentially. In this con-
text, we are interested in characterizing the learning dynamics of neural networks by analyzing
(catastrophic) example forgetting events. These occur when examples that have been “learnt” (i.e.,
correctly classiﬁed) at some time t in the optimization process are subsequently misclassiﬁed —
or in other terms forgotten — at a time t′ > t. We thus switch the focus from studying interac-
tions between sequentially presented tasks to studying interactions between sequentially presented
dataset examples during SGD optimization. Our starting point is to understand whether there exist
examples that are consistently forgotten across subsequent training presentations and, conversely,
examples that are never forgotten. We will call the latter unforgettable examples. We hypothesize
that speciﬁc examples consistently forgotten between subsequent presentations, if they exist, must
∗Equal contribution. Correspondence: MT: mariya@cmu.edu, AS: alsordon@microsoft.com
†Work done while interning at Microsoft Research Montreal
Code available at https://github.com/mtoneva/example forgetting
1
arXiv:1812.05159v3  [cs.LG]  15 Nov 2019

Published as a conference paper at ICLR 2019
not share commonalities with other examples from the same task. We therefore analyze the propor-
tion of forgettable/unforgettable examples for a given task and what effects these examples have on
a model’s decision boundary and generalization error.
The goal of our investigation is two-fold. First, we attempt to gain insight into the optimization
process by analyzing interactions among examples during learning and their inﬂuence on the ﬁnal
decision boundary. We are particularly interested in whether we can glean insight on the com-
pressibility of a dataset, and thereby increase data efﬁciency without compromising generalization
accuracy. It is a timely problem that has been the recent focus of few-shot learning approaches via
meta-learning (Finn et al., 2017; Ravi & Larochelle, 2017). Second, we aim to characterize whether
forgetting statistics can be used to identify “important” samples and detect outliers and examples
with noisy labels (John, 1995; Brodley & Friedl, 1999; Sukhbaatar et al., 2014; Jiang et al., 2018).
Identifying important, or most informative examples is an important line of work and was exten-
sively studied in the literature. Techniques of note — among others — are predeﬁned curricula
of examples (Bengio & LeCun, 2007), self-paced learning (Kumar et al., 2010), and more recently
meta-learning (Fan et al., 2017). These research directions usually deﬁne “hardness” or “commonal-
ity” of an example as a function of the loss on that particular example at some point during training
(or possibly at convergence). They do not consider whether some examples are consistently for-
gotten throughout learning. Very recently, Chang et al. (2017) consider re-weighting examples by
accounting for the variance of their predictive distribution. This is related to our deﬁnition of for-
getting events, but the authors provide little analysis of the extent to which the phenomenon occurs
in their proposed tasks. Our purpose is to study this phenomenon from an empirical standpoint and
characterize its prevalence in different datasets and across different model architectures.
Our experimental ﬁndings suggest that: a) there exist a large number of unforgettable examples, i.e.,
examples that are never forgotten once learnt, those examples are stable across seeds and strongly
correlated from one neural architecture to another; b) examples with noisy labels are among the most
forgotten examples, along with images with “uncommon” features, visually complicated to classify;
c) training a neural network on a dataset where a very large fraction of the least forgotten examples
have been removed still results in extremely competitive performance on the test set.
2
RELATED WORK
Curriculum Learning and Sample Weighting
Curriculum learning is a paradigm that favors
learning along a curriculum of examples of increasing difﬁculty (Bengio et al., 2009). This general
idea has found success in a variety of areas since its introduction (Kumar et al., 2010; Lee & Grau-
man, 2011; Schaul et al., 2015). Kumar et al. (2010) implemented their curriculum by considering
easy the examples with a small loss. Arpit et al. (2017) also pose that easy examples exist, and
deﬁne them as those that are correctly classiﬁed after only 1 epoch of training, though they do not
examine whether these examples are later forgotten. In our experiments, we empirically validate
that unforgettable examples can be safely removed without compromising generalization.
Zhao
& Zhang (2015); Katharopoulos & Fleuret (2018) relate sample importance to the norm of its loss
gradient with respect to the parameters of the network. Fan et al. (2017); Kim & Choi (2018); Jiang
et al. (2018) learn a curriculum directly from data in order to minimize the task loss. Jiang et al.
(2018) also study the robustness of their method in the context of noisy examples. This relates to a
rich literature on outlier detection and removal of examples with noisy labels (John, 1995; Brodley
& Friedl, 1999; Sukhbaatar et al., 2014; Jiang et al., 2018). We will provide evidence that noisy
examples rank higher in terms of number of forgetting events. Koh & Liang (2017) borrow inﬂu-
ence functions from robust statistics to evaluate the impact of the training examples on a model’s
predictions.
Deep Generalization
The study of the generalization properties of deep neural networks when
trained by stochastic gradient descent has been the focus of several recent publications (Zhang et al.,
2016; Keskar et al., 2016; Chaudhari et al., 2016; Advani & Saxe, 2017). These studies suggest
that the generalization error does not depend solely on the complexity of the hypothesis space. For
instance, it has been demonstrated that over-parameterized models with many more parameters than
training points can still achieve low test error (Huang et al., 2017; Wang et al., 2018) while being
complex enough to ﬁt a dataset with completely random labels (Zhang et al., 2016). A possible
2

Published as a conference paper at ICLR 2019
explanation for this phenomenon is a form of implicit regularization performed by stochastic gradi-
ent descent: deep neural networks trained with SGD have been recently shown to converge to the
maximum margin solution in the linearly separable case (Soudry et al., 2017; Xu et al., 2018). In
our work, we provide empirical evidence that generalization can be maintained when removing a
substantial portion of the training examples and without restricting the complexity of the hypothesis
class. This goes along the support vector interpretation provided by Soudry et al. (2017).
3
DEFINING AND COMPUTING EXAMPLE FORGETTING
Our general case study for example forgetting is a standard classiﬁcation setting. Given a dataset
D = (xi, yi)i of observation/label pairs, we wish to learn the conditional probability distribution
p(y|x; θ) using a deep neural network with parameters θ. The network is trained to minimize the
empirical risk R =
1
|D|
P
i L(p(yi|xi; θ), yi), where L denotes the cross-entropy loss and yi ∈
1, . . . k. The minimization is performed using variations of stochastic gradient descent, starting
from initial random parameters θ0, and by sampling examples at random from the dataset D.
Forgetting and learning events We denote by ˆyt
i = arg maxk p(yik|xi; θt) the predicted label for
example xi obtained after t steps of SGD. We also let acct
i = 1ˆyt
i=yi be a binary variable indicating
whether the example is correctly classiﬁed at time step t. Example i undergoes a forgetting event
when acct
i decreases between two consecutive updates: acct
i > acct+1
i
. In other words, example i
is misclassiﬁed at step t + 1 after having been correctly classiﬁed at step t. Conversely, a learning
event has occurred if acct
i < acct+1
i
. Statistics that will be of interest in the next sections include the
distribution of forgetting events across examples and the ﬁrst time a learning event occurs.
Classiﬁcation margin We will also be interested in analyzing the classiﬁcation margin. Our pre-
dictors have the form p(yi|xi; θ) = σ(β(xi)), where σ is a sigmoid (softmax) activation function
in the case of binary (categorical) classiﬁcation. The classiﬁcation margin m is deﬁned as the dif-
ference between the logit of the correct class and the largest logit among the other classes, i.e.
m = βk −arg maxk′̸=k βk′, where k is the index corresponding to the correct class.
Unforgettable examples We qualify examples as unforgettable if they are learnt at some point and
experience no forgetting events during the whole course of training: example i is unforgettable if
the ﬁrst time it is learnt t∗veriﬁes t∗< ∞and for all k ≥t∗, acck
i = 1. Note that, according to this
deﬁnition, examples that are never learnt during training do not qualify as unforgettable. We refer
to examples that have been forgotten at least once as forgettable.
3.1
PROCEDURAL DESCRIPTION AND EXPERIMENTAL SETTING
Following the previous deﬁnitions, monitoring forgetting events entails computing the prediction for
all examples in the dataset at each model update, which would be prohibitively expensive. In prac-
tice, for each example, we subsample the full sequence of forgetting events by computing forgetting
statistics only when the example is included in the current mini-batch; that is, we compute forgetting
across presentations of the same example in subsequent mini-batches. This gives a lower bound on
the number of forgetting events an example undergoes during training.
We train a classiﬁer on a given dataset and record the forgetting events for each example when
they are sampled in the current mini-batch. For the purposes of further analysis, we then sort the
dataset’s examples based on the number of forgetting events they undergo. Ties are broken at ran-
dom when sampling from the ordered data. Samples that are never learnt are considered forgotten
an inﬁnite number of times for sorting purposes. Note that this estimate of example forgetting is
computationally expensive; see Sec. 6 for a discussion of a cheaper method.
We perform our experimental evaluation on three datasets of increasing complexity: MNIST (LeCun
et al., 1999), permuted MNIST – a version of MNIST that has the same ﬁxed permutation applied
to the pixels of all examples, and CIFAR-10 (Krizhevsky, 2009). We use various model architec-
tures and training schemes that yield test errors comparable with the current state-of-the-art on the
respective datasets. In particular, the MNIST-based experiments use a network comprised of two
convolutional layers followed by a fully connected one, trained using SGD with momentum and
dropout. This network achieves 0.8% test error. For CIFAR-10, we use a ResNet with cutout (De-
Vries & Taylor, 2017) trained using SGD and momentum with a particular learning rate schedule.
3

Published as a conference paper at ICLR 2019
0
5
10
15
20
25
30
number of forgetting events
0.0
0.2
0.4
0.6
0.8
1.0
fraction of examples
0
20
0.00
0.01
0.02
0.03
0.04
0
5
10
15
20
25
30
number of forgetting events
0.0
0.2
0.4
0.6
0.8
1.0
fraction of examples
0
20
0.00
0.01
0.02
0.03
0.04
0
5
10
15
20
25
30
number of forgetting events
0.0
0.2
0.4
0.6
0.8
1.0
fraction of examples
0
20
0.00
0.01
0.02
0.03
0.04
Figure 1: Histograms of forgetting events on (from left to right) MNIST, permutedMNIST and
CIFAR-10. Insets show the zoomed-in y-axis.
This network achieves a competitive 3.99% test error. For full experimentation details, see the Sup-
plementary.
4
CHARACTERIZING EXAMPLE FORGETTING
Algorithm 1 Computing forgetting statistics.
initialize prev acci = 0, i ∈D
initialize forgetting T[i] = 0, i ∈D
while not training done do
B ∼D # sample a minibatch
for example i ∈B do
compute acci
if prev acci > acci then
T[i] = T[i] + 1
prev acci = acci
gradient update classiﬁer on B
return T
Number of forgetting events
We estimate the
number of forgetting events of all the training ex-
amples for the three different datasets (MNIST, per-
mutedMNIST and CIFAR-10) across 5 random seeds.
The histograms of forgetting events computed from
one seed are shown in Figure 1. There are 55,012,
45,181 and 15,628 unforgettable examples common
across 5 seeds, they represent respectively 91.7%,
75.3%, and 31.3% of the corresponding training
sets. Note that datasets with less complexity and di-
versity of examples, such as MNIST, seem to contain
signiﬁcantly more unforgettable examples. permut-
edMNIST exhibits a complexity balanced between
MNIST (easiest) and CIFAR-10 (hardest). This ﬁnd-
ing seems to suggest a correlation between forget-
ting statistics and the intrinsic dimension of the
learning problem, as recently formalized by Li et al. (2018).
Stability across seeds To test the stability of our metric with respect to the variance generated by
stochastic gradient descent, we compute the number of forgetting events per example for 10 different
random seeds and measure their correlation. From one seed to another, the average Pearson corre-
lation is 89.2%. When randomly splitting the 10 different seeds into two sets of 5, the cumulated
number of forgetting events within those two sets shows a high correlation of 97.6%. We also ran
the original experiment on 100 seeds to devise 95% conﬁdence bounds on the average (over 5 seeds)
number of forgetting events per example (see Appendix 13). The conﬁdence interval of the least
forgotten examples is tight, conﬁrming that examples with a small number of forgetting events can
be ranked conﬁdently.
Forgetting by chance In order to quantify the possibility of forgetting occurring by chance, we ad-
ditionally analyze the distribution of forgetting events obtained under the regime of random update
steps instead of the true SGD steps. In order to maintain the statistics of the random updates similar
to those encountered during SGD, random updates are obtained by shufﬂing the gradients produced
by standard SGD on a main network (more details are provided in Appendix 12). We report the
histogram of chance forgetting events in Supplementary Figure 13: examples are being forgotten by
chance a small number of time, at most twice and most of the time less than once. The observed sta-
bility across seeds, low number of chance forgetting events and the tight conﬁdence bounds suggest
that it is unlikely for the ordering produced by the metric to be the by-product of another unrelated
random cause.
4

Published as a conference paper at ICLR 2019
unforgettable
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
forgettable
Figure 2: Pictures of unforgettable (Top) and forgettable examples (Bottom) of every CIFAR-10
class. Forgettable examples seem to exhibit peculiar or uncommon features. Additional examples
are available in Supplemental Figure 15.
First learning events
We investigate whether unforgettable and forgettable examples need to be
presented different numbers of times in order to be learnt for the ﬁrst time (i.e. for the ﬁrst learning
event to occur, as deﬁned in Section 3). The distributions of the presentation numbers at which
ﬁrst learning events occur across all datasets can be seen in Supplemental Figure 8. We observe
that, while both unforgettable and forgettable sets contain many examples that are learnt during the
ﬁrst 3-4 presentations, the forgettable examples contain a larger number of examples that are ﬁrst
learnt later in training. The Spearman rank correlation between the ﬁrst learning event presentations
and the number of forgetting events across all training examples is 0.56, indicating a moderate
relationship.
Misclassiﬁcation margin
The deﬁnition of forgetting events is binary and as such fairly crude
compared to more sophisticated estimators of example relevance (Zhao & Zhang, 2015; Chang et al.,
2017). In order to qualify its validity, we compute the misclassiﬁcation margin of forgetting events.
The misclassiﬁcation margin of an example is deﬁned as the mean classiﬁcation margin (deﬁned
in Section 3) over all its forgetting events, a negative quantity by deﬁnition. The Spearman rank
correlation between an example’s number of forgetting events and its mean misclassiﬁcation margin
is -0.74 (computed over 5 seeds, see corresponding 2D-histogram in Supplemental Figure 9). These
results suggest that examples which are frequently forgotten have a large misclassiﬁcation margin.
Visual inspection
We visualize some of the unforgettable examples in Figure 2 along with some
examples that have been most forgotten in the CIFAR-10 dataset. Unforgettable samples are easily
recognizable and contain the most obvious class attributes or centered objects, e.g., a plane on a
clear sky. On the other hand, the most forgotten examples exhibit more ambiguous characteristics
(as in the center image, a truck on a brown background) that may not align with the learning signal
common to other examples from the same class.
Detection of noisy examples
We further investigate the observation that the most forgettable ex-
amples seem to exhibit atypical characteristics. We would expect that if highly forgettable exam-
ples have atypical class characteristics, then noisily-labeled examples will undergo more forgetting
events. We randomly change the labels of 20% of CIFAR-10 and record the number of forgetting
events of both the noisy and regular examples through training. The distributions of forgetting events
across noisy and regular examples are shown in Figure 3. We observe that the most forgotten exam-
ples are those with noisy labels and that no noisy examples are unforgettable. We also compare the
forgetting events of the noisy examples to that of the same set of examples with original labels and
observe a much higher degree of forgetting in the noisy case. The results of these synthetic experi-
ments support the hypothesis that highly forgettable examples exhibit atypical class characteristics.
4.1
CONTINUAL LEARNING SETUP
We observed that in harder tasks such as CIFAR-10, a signiﬁcant portion of examples are forgotten at
least once during learning. This leads us to believe that catastrophic forgetting may be observed, to
some extent, even when considering examples coming from the same task distribution. To test this
hypothesis, we perform an experiment inspired by the standard continual learning setup (McCloskey
& Cohen, 1989; Kirkpatrick et al., 2017). We create two tasks by randomly sampling 10k examples
5

Published as a conference paper at ICLR 2019
0
5
10
15
20
25
number of forgetting events
0.000
0.025
0.050
0.075
0.100
0.125
0.150
0.175
fraction of corresponding examples
regular examples
noisy examples
0
5
10
15
20
number of forgetting events
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
fraction of corresponding examples
examples before noise
examples after noise
Figure 3: Distributions of forgetting events across training examples in CIFAR-10 when 20% of
labels are randomly changed. Left. Comparison of forgetting events between examples with noisy
and original labels. The most forgotten examples are those with noisy labels. No noisy examples
are unforgettable. Right. Comparison of forgetting events between examples with noisy labels and
the same examples with original labels. Examples exhibit more forgetting when their labels are
changed.
0
10
20
30
40
50
60
70
80
epoch
30
40
50
60
70
80
90
100
accuracy
(a.1)      (a.2)       (a.3)      (a.4)
random partition 1
random partition 2
(a) random partitions
0
10
20
30
40
50
60
70
80
epoch
30
40
50
60
70
80
90
100
accuracy
(b.1)      (b.2)       (b.3)      (b.4)
never forgotten
forgotten at least once
0
10
20
30
40
50
60
70
80
epoch
30
40
50
60
70
80
90
100
accuracy
(c.1)      (c.2)       (c.3)       (c.4)
never forgotten
forgotten at least once
(b) partitioning by forgetting events
Figure 4: Synthetic continual learning setup for CIFAR-10. Background color in each column indi-
cates the training partition, curves track performance on both partitions during interleaved training.
Solids lines represent the average of 5 runs and dashed lines represent the standard error. The ﬁgure
highlights that examples that have been forgotten at least once can “support” those that have never
been forgotten, as shown in (c.2) and (b.3).
from the CIFAR-10 training set and dividing them in two equally-sized partitions (5k examples each).
We treat each partition as a separate ”task” even though they should follow the same distribution.
We then train a classiﬁer for 20 epochs on each partition in an alternating fashion, while tracking
performance on both partitions. The results are reported in Figure 4 (a). The background color
represents which of the two partitions is currently used for training. We observe some forgetting of
the second task when we only train on the ﬁrst task (panel (a.2)). This is somewhat surprising as the
two tasks contain examples from the same underlying distribution.
We contrast the results from training on random partitions of examples with ones obtained by par-
titioning the examples based on forgetting statistics (Figure 4 (b)). That is, we ﬁrst compute the
forgetting events for all examples based on Algorithm 1 and we create our tasks by sampling 5k
examples that have zero forgetting events (named f0) and 5k examples that have non-zero forgetting
events (named fN). We observe that examples that have been forgotten at least once suffer a more
drastic form of forgetting than those included in a random split (compare (a.2) with (b.2)). In panel
(b.3) and (c.2) we can observe that examples from task f0 suffer very mild forgetting when training
on task fN. This suggests that examples that have been forgotten at least once may be able to “sup-
port” those that have never been forgotten. We observe the same pattern when we investigate the
opposite alternating sequence of tasks in Figure 4 (b, right).
6

Published as a conference paper at ICLR 2019
0
10
20
30
40
50
60
percentage of training set removed
92.5
93.0
93.5
94.0
94.5
95.0
95.5
96.0
96.5
test accuracy
none removed
selected removed
random removed
0
5
10
15
20
average number of forgetting events in removed subset
95.2
95.4
95.6
95.8
96.0
96.2
test classification accuracy
selected removed
random removed
Figure 5: Left Generalization performance on CIFAR-10 of ResNet18 where increasingly larger sub-
sets of the training set are removed (mean +/- std error of 5 seeds). When the removed examples are
selected at random, performance drops very fast. Selecting the examples according to our ordering
can reduce the training set signiﬁcantly without affecting generalization. The vertical line indicates
the point at which all unforgettable examples are removed from the training set. Right Difference
in generalization performance when contiguous chunks of 5000 increasingly forgotten examples are
removed from the training set. Most important examples tend to be those that are forgotten the most.
5
REMOVING UNFORGETTABLE EXAMPLES
As shown in the previous section, learning on examples that have been forgotten at least once min-
imally impacts performance on those that are unforgettable. This appears to indicate that unforget-
table examples are less informative than others, and, more generally, that the more an example is
forgotten during training, the more useful it may be to the classiﬁcation task. This seems to align
with the observations in Chang et al. (2017), where the authors re-weight training examples by ac-
counting for the variance of their predictive distribution. Here, we test whether it is possible to
completely remove a given subset of examples during training.
In Fig. 5 (Left), we show the evolution of the generalization performance in CIFAR-10 when we
artiﬁcially remove examples from the training dataset. We choose the examples to remove by in-
creasing number of forgetting events. Each point in the ﬁgure corresponds to retraining the model
from scratch on an increasingly smaller subset of the training data (with the same hyper-parameters
as the base model). We observe that when removing a random subset of the dataset, performance
rapidly decreases. Comparatively, by removing examples ordered by number of forgetting events,
30% of the dataset can be removed while maintaining comparable generalization performance as the
base model trained on the full dataset, and up to 35% can be removed with marginal degradation
(less than 0.2%). The results on the other datasets are similar: a large fraction of training examples
can be ignored without hurting the ﬁnal generalization performance of the classiﬁers (Figure 6).
In Figure 5 (Right), we show the evolution of the generalization error when we remove from the
dataset 5,000 examples with increasing forgetting statistics. Each point in the ﬁgure corresponds to
the generalization error of a model trained on the full dataset minus 5,000 examples as a function
of the average number of forgetting events in those 5,000 examples. As can be seen, removing the
same number of examples with increasingly more forgetting events results in worse generalization
for most of the curve. It is interesting to notice the rightmost part of the curve moving up, suggesting
that some of the most forgotten examples actually hurt performance. Those could correspond to
outliers or mislabeled examples (see Sec. 4). Finding a way to separate those points from very
informative ones is an ancient but still active area of research (John, 1995; Jiang et al., 2018).
Support vectors Various explanations of the implicit generalization of deep neural networks (Zhang
et al., 2016) have been offered: ﬂat minima generalize better and stochastic gradient descent con-
verges towards them (Hochreiter & Schmidhuber, 1997; Kleinberg et al., 2018), gradient descent
protects against overﬁtting (Advani & Saxe, 2017; Tachet et al., 2018), deep networks’ structure
biases learning towards simple functions (Neyshabur et al., 2014; Perez et al., 2018). But it remains
a poorly understood phenomenon. An interesting direction of research is to study the convergence
properties of gradient descent in terms of maximum margin classiﬁers. It has been shown recently
7

Published as a conference paper at ICLR 2019
0
20
40
60
80
100
percent of training set removed
0
10
20
30
40
50
percent increase in test error
CIFAR-10
permuted MNIST
MNIST
2% test error increase
0
20
40
60
80
100
percent of training set removed
0
1
2
3
4
5
percent increase in test error
CIFAR-10
permuted MNIST
MNIST
2% test error increase
Figure 6: Decrease in generalization performance when fractions of the training sets are removed.
When the subsets are selected appropriately, performance is maintained after removing up to 30%
of CIFAR-10, 50% of permutedMNIST, and 80% of MNIST. Vertical black line indicates the point
at which all unforgettable examples are removed from CIFAR-10. Right is a zoomed in version of
Left.
(Soudry et al., 2017) that on separable data, a linear network will learn such a maximum margin
classiﬁer. This supports the idea that stochastic gradient descent implicitly converges to solutions
that maximally separate the dataset, and additionally, that some data points are more relevant than
others to the decision boundary learnt by the classiﬁer. Those points play a part equivalent to sup-
port vectors in the support vector machine paradigm. Our results conﬁrm that a signiﬁcant portion of
training data points have little to no inﬂuence on the generalization performance when the decision
function is learnt with SGD. Forgettable training points may be considered as analogs to support vec-
tors, important for the generalization performance of the model. The number of forgetting events
of an example is a relevant metric to detect such support vectors. It also correlates well with the
misclassiﬁcation margin (see Sec.4) which is a proxy for the distance to the decision boundary.
Intrinsic dataset dimension As mentioned above, the datasets we study have various fractions of
unforgettable events (91.7% for MNIST, 75.3% for permutedMNIST and 31.3% for CIFAR-10). We
also see in Figure 6 that performance on those datasets starts to degrade at different fractions of
removed examples: the number of support vectors varies from one dataset to the other, based on the
complexity of the underlying data distribution. If we assume that we are in fact detecting analogs of
support vectors, we can put these results in perspective with the intrinsic dataset dimension deﬁned
by Li et al. (2018) as the codimension in the parameter space of the solution set: for a given archi-
tecture, the higher the intrinsic dataset dimension, the larger the number of support vectors, and the
fewer the number of unforgettable examples.
6
TRANSFERABLE FORGETTING EVENTS
Forgetting events rely on training a given architecture, with a given optimizer, for a given number of
epochs. We investigate to what extent the forgetting statistics of examples depend on those factors.
Throughout training We compute the Spearman rank correlation between the ordering obtained at
the end of training (200 epochs) and the ordering after various number of epochs. As seen in Fig. 7
(Left), the ordering is very stable after 75 epochs, and we found a reasonable number of epochs to
get a good correlation to be 25 (see the Supplementary Materials for precision-recall plots).
Between architectures A limitation of our method is that it requires computing the ordering from
a previous run. An interesting question is whether that ordering could be obtained from a simpler
architecture than residual networks. We train a network with two convolutional layers followed by
two fully connected ones (see the Supplementary for the full architecture) and compare the resulting
ordering with the one obtained with ResNet18. Figure 7 (Middle) shows a precision-recall plot of
the unforgettable examples computed with the residual network. We see a reasonably strong agree-
ment between the unforgettable examples of the convolutional neural network and the ones of the
ResNet18. Finally, we train a WideResNet (Zagoruyko & Komodakis, 2016) on truncated data sets
8

Published as a conference paper at ICLR 2019
Figure 7: Left. Ranking of examples by forgotten events stabilizes after 75 epochs in CIFAR-10.
Middle. Precision and recall of retrieving the unforgettable examples of ResNet18, using the ex-
ample ordering of a simpler convolutional neural network. Right. Generalization performance on
CIFAR-10 of a WideResNet using the example ordering of ResNet18.
using the example ordering from ResNet18. Using the same computing power (one Titan X GPU),
Resnet18 requires 2 hours to train whereas WideResNet requires 8 – estimating the forgetting statis-
tics of WideResNet via ResNet18 can save up to 6 hours of training time if the estimate is accurate.
We plot WideResNet’s generalization performance using the ordering obtained by ResNet18 in Fig-
ure 7 (Right): the network still performs near optimally with 30% of the dataset removed. This
opens up promising avenues of computing forgetting statistics with smaller architectures.
7
CONCLUSION AND FUTURE WORK
In this paper, inspired by the phenomenon of catastrophic forgetting, we investigate the learning
dynamics of neural networks when training on single classiﬁcation tasks. We show that catastrophic
forgetting can occur in the context of what is usually considered to be a single task. Inspired by this
result, we ﬁnd that some examples within a task are more prone to being forgotten, while others are
consistently unforgettable. We also ﬁnd that forgetting statistics seem to be fairly stable with respect
to the various characteristics of training, suggesting that they actually uncover intrinsic properties of
the data rather than idiosyncrasies of the training schemes. Furthermore, the unforgettable examples
seem to play little part in the ﬁnal performance of the classiﬁer as they can be removed from the
training set without hurting generalization. This supports recent research interpreting deep neural
networks as max margin classiﬁers in the linear case. Future work involves understanding forgetting
events better from a theoretical perspective, exploring potential applications to other areas of super-
vised learning, such as speech or text and to reinforcement learning where forgetting is prevalent
due to the continual shift of the underlying distribution.
8
ACKNOWLEDGMENTS
We acknowledge the anonymous reviewers for their insightful suggestions.
REFERENCES
Madhu S. Advani and Andrew M. Saxe. High-dimensional dynamics of generalization error in
neural networks. CoRR, abs/1710.03667, 2017.
Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer
look at memorization in deep networks. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 233–242. JMLR.org, 2017.
Yoshua Bengio and Yann LeCun. Scaling learning algorithms towards AI. In Large Scale Kernel
Machines. MIT Press, 2007.
Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the 26th annual international conference on machine learning, pp. 41–48. ACM,
2009.
9

Published as a conference paper at ICLR 2019
Carla E Brodley and Mark A Friedl. Identifying mislabeled training data. Journal of artiﬁcial
intelligence research, 11:131–167, 1999.
Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active Bias: Training More
Accurate Neural Networks by Emphasizing High Variance Samples. In Advances in Neural In-
formation Processing Systems, pp. 1002–1012, 2017.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing Gradi-
ent Descent Into Wide Valleys. ICLR ’17, 2016.
Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552, 2017.
Yang Fan, Fei Tian, Tao Qin, and Jiang Bian.
Learning What Data to Learn.
arXiv preprint
arXiv:1702.08635, 2017.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proc. of ICML, 2017.
S. Hochreiter and J. Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely Connected
Convolutional Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4700–4708, 2017.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. MentorNet: Learning data-
driven curriculum for very deep neural networks on corrupted labels. In Proceedings of the 35th
International Conference on Machine Learning. PMLR, 2018.
George H John. Robust decision trees: removing outliers from databases. In Proceedings of the
First International Conference on Knowledge Discovery and Data Mining, pp. 174–179. AAAI
Press, 1995.
Angelos Katharopoulos and Franois Fleuret.
Not all samples are created equal: Deep learning
with importance sampling. In Jennifer G. Dy and Andreas Krause (eds.), ICML, volume 80 of
JMLR Workshop and Conference Proceedings, pp. 2530–2539. JMLR.org, 2018. URL http:
//dblp.uni-trier.de/db/conf/icml/icml2018.html#KatharopoulosF18.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Tae-Hoon Kim and Jonghyun Choi.
Screenernet:
Learning curriculum for neural networks.
CoRR, abs/1801.00904, 2018.
URL http://dblp.uni-trier.de/db/journals/
corr/corr1801.html#abs-1801-00904.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. URL http:
//arxiv.org/abs/1412.6980. cite arxiv:1412.6980Comment: Published as a conference
paper at the 3rd International Conference for Learning Representations, San Diego, 2015.
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, and Others.
Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of
sciences, pp. 201611835, 2017.
Robert Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does sgd escape lo-
cal minima?
CoRR, abs/1802.06175, 2018.
URL http://dblp.uni-trier.de/db/
journals/corr/corr1802.html#abs-1802-06175.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via inﬂuence functions. In
Doina Precup and Yee Whye Teh (eds.), ICML, volume 70 of JMLR Workshop and Conference
Proceedings, pp. 1885–1894. JMLR.org, 2017. URL http://dblp.uni-trier.de/db/
conf/icml/icml2017.html#KohL17.
10

Published as a conference paper at ICLR 2019
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
URL
https://www.cs.toronto.edu/˜kriz/learning-features-2009-TR.
pdf.
M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-Paced Learning for Latent Variable
Models. In Proc. of NIPS, pp. 1–9, 2010.
Y. LeCun,
C. Cortes C.,
and C. Burges.
The mnist database of handwritten digits.
http://yann.lecun.com/exdb/mnist/, 1999.
Yong Jae Lee and Kristen Grauman. Learning the easy things ﬁrst: Self-paced visual category
discovery. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pp.
1721–1728. IEEE, 2011.
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension
of objective landscapes. CoRR, abs/1804.08838, 2018. URL http://dblp.uni-trier.
de/db/journals/corr/corr1804.html#abs-1804-08838.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109–165.
Elsevier, 1989.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On
the role of implicit regularization in deep learning. CoRR, abs/1412.6614, 2014. URL http:
//dblp.uni-trier.de/db/journals/corr/corr1412.html#NeyshaburTS14.
Guillermo Valle Perez, Chico Q. Camargo, and Ard A. Louis.
Deep learning generalizes be-
cause the parameter-function map is biased towards simple functions. CoRR, abs/1805.08522,
2018. URL http://dblp.uni-trier.de/db/journals/corr/corr1805.html#
abs-1805-08522.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In Proc. of ICLR,
2017.
Hippolyt Ritter, Aleksandar Botev, and David Barber. Online Structured Laplace Approximations
For Overcoming Catastrophic Forgetting. arxiv preprint arxiv: 1805.07810, 2018. URL http:
//arxiv.org/abs/1805.07810.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv
preprint arXiv:1511.05952, 2015.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The Im-
plicit Bias of Gradient Descent on Separable Data. arXiv preprint arxiv:1710.10345, 2017. URL
http://arxiv.org/abs/1710.10345.
Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training
convolutional networks with noisy labels. arXiv preprint arXiv:1406.2080, 2014.
R. Tachet, M. Pezeshki, S. Shabanian, A. Courville, and Y. Bengio. On the learning dynamics of
deep neural networks. arxiv preprint arxiv:1809.06848, 2018. doi: arXiv:1809.06848v1. URL
https://arxiv.org/abs/1809.06848.
Huan Wang, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. Identifying Generalization
Properties in Neural Networks. arXiv preprint arXiv:1809.07402, pp. 1–23, 2018. doi: arXiv:
1809.07402v1. URL http://arxiv.org/abs/1809.07402.
Tengyu Xu, Yi Zhou, Kaiyi Ji, and Yingbin Liang. Convergence of sgd in learning relu models with
separable data. CoRR, abs/1806.04339, 2018. URL http://dblp.uni-trier.de/db/
journals/corr/corr1806.html#abs-1806-04339.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks, 2016. URL http://arxiv.
org/abs/1605.07146. cite arxiv:1605.07146.
11

Published as a conference paper at ICLR 2019
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Peilin Zhao and Tong Zhang. Stochastic Optimization with Importance Sampling for Regularized
Loss Minimization. In Proc. of ICML, 2015.
12

Published as a conference paper at ICLR 2019
9
EXPERIMENTATION DETAILS
Detailed distributions
0
5
10
15
20
25
30
presentation number when first learned
0
200
400
600
800
1000
1200
1400
number of examples
unforgettable
forgettable
0
5
10
15
20
25
30
presentation number when first learned
0
200
400
600
800
1000
1200
1400
number of examples
unforgettable
forgettable
0
5
10
15
20
25
30
presentation number when first learned
0
200
400
600
800
1000
1200
1400
number of examples
unforgettable
forgettable
Figure 8: From left to right, distributions of the ﬁrst presentation at which each unforgettable and
forgettable example was learned in MNIST, permutedMNIST and CIFAR-10 respectively. Rescaled
view where the number of examples have been capped between 0 and 1500 for visualization pur-
poses. Unforgettable examples are generally learnt early during training, thus may be considered as
“easy” in the sense of Kumar et al. (2010), i.e. may have a low loss during most of the training.
Misclassiﬁcation margin
Figure 9: Left 2D-histogram of the number of forgetting events and mean misclassiﬁcation margin
across all examples of CIFAR-10. There is signiﬁcant negative correlation (-0.74, Spearman rank
correlation) between mean misclassiﬁcation margin and the number of forgetting events.
permutedMNIST The permutedMNIST data set is obtained by applying a ﬁxed random permutation
of the pixels to all the images of the standard MNIST data set. This typically makes the data set
harder to learn for convolutional neural networks as local patterns, e.g. the horizontal bar of the 7,
get shufﬂed. This statement is supported by the two following facts:
• The number of unforgettable examples for permutedMNIST is 45181 versus 55012 for
MNIST.
• The intrinsic data set dimension (Li et al., 2018) of permutedMNIST is 1400 compared to
290 for the untouched data set.
Network Architectures We use a variety of different architectures in the main text. Below are their
speciﬁcations.
The architecture for the MNIST and permutedMNIST experiments is the following:
1. a ﬁrst convolutional layer with 5 by 5 ﬁlters and 10 feature maps,
2. a second convolutional layer with 5 by 5 ﬁlters and 20 feature maps,
3. a fully connected layer with 50 hidden units
4. the output layer, with 10 logits, one for each class.
We apply ReLU nonlinearities to the feature maps and to the hidden layer. The last layer is passed
through a softmax to output probabilities for each class of the data set.
13

Published as a conference paper at ICLR 2019
The ResNet18 architecture used for CIFAR-10 is described thoroughly in DeVries & Taylor (2017),
its implementation can be found at https://github.com/uoguelph-mlrg/Cutout.
The second one is a WideResNet (Zagoruyko & Komodakis, 2016), with a depth of 28 and a widen
factor of 10.
We used the implementation found at https://github.com/meliketoy/
wide-resnet.pytorch.
The convolutional architecture used in Section 6 is the following:
1. a ﬁrst convolutional layer with 5 by 5 ﬁlters and 6 feature maps,
2. a 2 by 2 max pooling layer
3. a second convolutional layer with 5 by 5 ﬁlters and 16 feature maps,
4. a ﬁrst fully connected layer with 120 hidden units
5. a second fully connected layer with 84 hidden units
6. the output layer, with 10 logits, one for each class.
Optimization
The MNIST networks are trained to minimize the cross-entropy loss using stochastic gradient de-
scent with a learning rate of 0.01 and a momentum of 0.5.
The ResNet18 is trained using cutout, data augmentation and stochastic gradient descent with a 0.9
Nesterov momentum and a learning rate starting at 0.1 and divided by 5 at epochs 60, 120 and 160.
The WideResNet is trained using Adam (Kingma & Ba, 2014) and a learning rate of 0.001.
10
STABILITY OF THE FORGETTING EVENTS
In Fig 10, we plot precision-recall diagrams for the unforgettable and most forgotten examples of
CIFAR-10 obtained on ResNet18 after 200 epochs and various prior time steps. We see in particular
that at 75 epochs, the examples on both side of the spectrum can be retrieved with very high precision
and recall.
0.0
0.2
0.4
0.6
0.8
1.0
recall
0.0
0.2
0.4
0.6
0.8
1.0
precision
retrieving 17k hardest for resnet
0.0
0.2
0.4
0.6
0.8
1.0
recall
0.0
0.2
0.4
0.6
0.8
1.0
precision
retrieving 17k easiest for resnet
from resnet 25 epochs
from resnet 50 epochs
from resnet 75 epochs
recall at 17k
Figure 10: Right: precision and recall of retrieving the unforgettable examples from a full run of
ResNet18 (200 epochs), using the example ordering after 25, 50, and 75 epochs. The unforgettable
examples are retrieved with high precision and recall after 50 epochs. Left: same plot for the 17k
examples with the most forgetting events.
11
Noising THE DATA SETS
In Section 4, we analyzed the effect of adding label noise on the distribution of forgetting events.
Here, we examine the effect of adding pixel noise, i.e. noising the input distribution. We choose
to corrupt the inputs with additive Gaussian noise with zero mean and we choose for its stan-
dard deviation to be a multiple of channel-wise data standard deviation (i.e., σnoise = λσdata, λ ∈
{0.5, 1, 2, 10}). Note that we add the noise after applying a channel-wise standard normalization
14

Published as a conference paper at ICLR 2019
step of the training images, therefore σdata = 1 (each channel has zero mean, unit variance, this is a
standard pre-processing step and has been applied throughout all the experiments in this paper).
The forgetting distributions obtained by noising all the dataset examples with increasing noise stan-
dard deviation are presented in Figure 11. We observe that adding increasing amount of noise
decreases the amount of unforgettable examples and increases the amount of examples in the second
mode of the forgetting distribution.
number of forgetting events
0
5
10 15
20
25
30
35
40
number of examples
0
2000
4000
6000
8000
10000
12000
14000
16000
std 10
std 2
std 1
std 0.5
no noise
Figure 11: Distribution of forgetting events across all training examples in CIFAR-10 when all
training images are augmented with increasing additive Gaussian noise. The presence of increas-
ing amount of noise decreases the amount of unforgettable examples and increases the amount of
examples in the second mode of the forgetting distribution.
We follow the noisy-labels experiments of Section 4 and we apply the aforementioned pixel noise to
20% of the training data (σnoise = 10). We present the results of comparing the forgetting distribution
of the 20% of examples before and after noise was added to the pixels in Figure 12 (Left). For ease
of comparison, we report the same results in the case of label noise in Figure 12 (Right). We observe
that the forgetting distribution under pixel noise resembles the one under label noise.
0
5
10
15
20
25
number of forgetting examples
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
fraction of corresponding examples
examples before noise
examples after pixel noise
0
5
10
15
20
25
number of forgetting events
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
fraction of corresponding examples
examples before noise
examples after label noise
Figure 12: Distribution of forgetting events across all training examples in CIFAR-10 when random
20% of training examples undergo pixel noise (σnoise = 10) (Left) or label noise (Right) (same as
Figure 3). We observe that the forgetting distribution under pixel noise resembles the one under
label noise.
15

Published as a conference paper at ICLR 2019
12
”CHANCE” FORGETTING EVENTS ON CIFAR-10
0
5
10
15
20
25
number of forgetting events
0
2000
4000
6000
8000
10000
number of examples
true gradients
random gradients
0.0
0.5
1.0
1.5
2.0
2.5
3.0
number of forgetting events
0
2000
4000
6000
8000
10000
number of examples
true gradients
random gradients
Figure 13: Histogram of forgetting events under true and random gradient steps. (Right) Zoomed-in
version where the number of forgetting events is capped at 3 for visualization.
Forgetting events may happen by “chance”, i.e. some learning/forgetting events may occur even
with random gradients. In order to estimate how large the effect of “chance” is, we compute the
forgetting events of a classiﬁer obtained by randomizing the update steps. To keep the statistics of
the gradients similar to those encountered during SGD, we proceed as follows:
1. Before the beginning of training, clone the “base” classiﬁer into a new “clone” classiﬁer
with the same random weights.
2. At each training step, shufﬂe the gradients computed on the base classiﬁer and apply those
to the clone (the base classiﬁer is still optimized the same way): this ensures that the
statistics of the random updates match the statistics of the true gradients during learning.
3. Compute the forgetting events of the clone classiﬁer on the training set exactly as is done
with the base classiﬁer.
The results can be found in Fig 13, showing the histogram of forgetting events produced by the clone
network, averaged over 5 seeds. This gives an idea of the chance forgetting rate across examples. In
this setting, examples are being forgotten by chance at most twice.
13
CONFIDENCE ON FORGETTING EVENTS FOR CIFAR-10
In order to establish conﬁdence intervals on the
number of forgetting events, we computed them
on 100 seeds and formed 20 averages over 5 seeds.
In Fig 14, we show the average (in green), the bot-
tom 2.5 percentile (in blue) and top 2.5 percentile
(in orange) of those 20 curves.
Figure 14: 95% conﬁdence interval on for-
getting events averaged over 5 seeds.
14
VISUALIZATION OF FORGETTABLE AND UNFORGETTABLE IMAGES
See Fig 15 for additional pictures of the most unforgettable and forgettable examples of every
CIFAR-10 class, when examples are sorted by number of forgetting events (ties are broken ran-
domly).
16

Published as a conference paper at ICLR 2019
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
plane
car
bird
cat
deer
dog
frog
horse
ship
truck
Figure 15: Additional pictures of the most unforgettable (Left) and forgettable examples (Right) of
every CIFAR-10 class, when examples are sorted by number of forgetting events (ties are broken
randomly). Forgettable examples seem to exhibit peculiar or uncommon features.
15
FORGETTING IN CIFAR-100
0
5
10
15
20
25
30
35
number of forgetting events
0
1000
2000
3000
4000
5000
number of examples
0
5
10
15
20
25
30
35
number of forgetting events
0
1000
2000
3000
4000
5000
number of examples
Figure 16: Left: distribution of forgetting events in CIFAR-100. Right: distribution of forgetting
events in CIFAR-10 when 20% of the labels are changed at random. The distribution of forgetting in
CIFAR-100 is much closer to that of forgetting in the noisy CIFAR-10 than it is to forgetting in the
original datasets presented in Figure 1.
The distribution of forgetting events in CIFAR-100 is shown in Figure 16. There are 3809 unfor-
gettable examples (7.62% of the training set). CIFAR-100 is the hardest to classify out all of the
presented datasets and exhibits the highest percentage of forgetting events. This ﬁnding further
supports the idea that there may be a correlation between the forgetting statistics and the intrinsic
17

Published as a conference paper at ICLR 2019
dimension of the learning problem. Additionally, each CIFAR-100 class contains 10 times fewer
examples than in CIFAR-10 or the MNIST datasets, making each image all the more useful for the
learning problem.
We also observe that the distribution of forgetting in CIFAR-100 is much closer to that of forgetting in
the noisy CIFAR-10 than it is to forgetting in the original datasets presented in Figure 1. Visualizing
the most forgotten examples in CIFAR-100 revealed that CIFAR-100 contains several images that
appear multiple times in the training set under different labels. In Figure 17, we present the 36 most
forgotten examples in CIFAR-100. Note that they are all images that appear under multiple labels
(not shown: the ”girl” image also appears under the label ”baby”, the ”mouse” image also appears
under ”shrew”, one of the 2 images of ‘oak tree’ appears under ‘willow tree’ and the other under
’maple tree’).
otter
mouse
otter
seal
snake
lion
crab
shark
snake
shark
beaver
worm
girl
otter
oak_tree
porcupine
raccoon
oak_tree
whale
apple
snake
porcupine
pear
whale
snake
worm
tiger
spider
beaver
worm
snake
seal
worm
worm
seal
shrew
Figure 17: The 36 most forgotten examples in CIFAR-100. Note that they are all images that ap-
pear under multiple labels (not pictured: the ”girl” image also appears under the label ”baby”, the
”mouse” image also appears under ”shrew”, one of the 2 images of ‘oak tree’ appears under ‘wil-
low tree’ and the other under ’maple tree’.
We perform the same removal experiments we presented in Figure 5 for CIFAR-100. The results are
shown in Figure 18. Just like with CIFAR-10, we are able to remove all unforgettable examples ( 8%
of the training set) while maintaining test performance.
18

Published as a conference paper at ICLR 2019
0
5
10
15
20
25
30
35
percentage of training set removed
72
73
74
75
76
77
78
test accuracy
none removed
selected removed
random removed
Figure 18: Generalization performance on CIFAR-100 of ResNet18 where increasingly larger sub-
sets of the training set are removed (mean +/- std error of 5 seeds). When the removed examples
are selected at random, performance drops faster. Selecting the examples according to our ordering
reduces the training set without affecting generalization.
19

