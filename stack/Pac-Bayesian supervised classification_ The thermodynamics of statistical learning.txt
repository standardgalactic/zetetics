Institute of Mathematical Statistics
LECTURE NOTESâ€“MONOGRAPH SERIES
Volume 56
Pac-Bayesian Supervised
Classiï¬cation: The Thermodynamics
of Statistical Learning
Olivier Catoni
Institute of Mathematical Statistics
Beachwood, Ohio, USA

Institute of Mathematical Statistics
Lecture Notesâ€“Monograph Series
Series Editor:
Anthony C. Davison
The production of the Institute of Mathematical Statistics
Lecture Notesâ€“Monograph Series is managed by the
IMS Oï¬ƒce: Rong Chen, Treasurer and
Elyse Gustafson, Executive Director.
Library of Congress Control Number: 2007939120
International Standard Book Number (10) 0-940600-72-2
International Standard Book Number (13) 978-0-940600-72-0
International Standard Serial Number 0749-2170
DOI: 10.1214/074921707000000391
Copyright câƒ2007 Institute of Mathematical Statistics
All rights reserved
Printed in the Lithuania

Contents
Preface .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
v
Introduction.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
vii
1. Inductive PAC-Bayesian learning .
.
.
.
.
.
.
.
.
.
.
.
1
1.1. Basic inequality .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2
1.2. Non local bounds
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5
1.2.1. Unbiased empirical bounds .
.
.
.
.
.
.
.
.
.
.
.
5
1.2.2. Optimizing explicitly the exponential parameter Î»
.
.
.
8
1.2.3. Non random bounds
.
.
.
.
.
.
.
.
.
.
.
.
.
.
9
1.2.4. Deviation bounds
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
11
1.3. Local bounds .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
14
1.3.1. Choice of the prior
.
.
.
.
.
.
.
.
.
.
.
.
.
.
14
1.3.2. Unbiased local empirical bounds .
.
.
.
.
.
.
.
.
.
15
1.3.3. Non random local bounds
.
.
.
.
.
.
.
.
.
.
.
.
17
1.3.4. Local deviation bounds
.
.
.
.
.
.
.
.
.
.
.
.
.
18
1.3.5. Partially local bounds
.
.
.
.
.
.
.
.
.
.
.
.
.
22
1.3.6. Two step localization .
.
.
.
.
.
.
.
.
.
.
.
.
.
27
1.4. Relative bounds .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
33
1.4.1. Basic inequalities .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
34
1.4.2. Non random bounds
.
.
.
.
.
.
.
.
.
.
.
.
.
.
37
1.4.3. Unbiased empirical bounds .
.
.
.
.
.
.
.
.
.
.
.
40
1.4.4. Relative empirical deviation bounds
.
.
.
.
.
.
.
.
44
2. Comparing posterior distributions to Gibbs priors
.
.
.
.
51
2.1. Bounds relative to a Gibbs distribution .
.
.
.
.
.
.
.
.
51
2.1.1. Comparing a posterior distribution with a Gibbs prior .
.
52
2.1.2. The eï¬€ective temperature of a posterior distribution
.
.
55
2.1.3. Analysis of an empirical bound for the eï¬€ective temperature
56
2.1.4. Adaptation to parametric and margin assumptions
.
.
.
61
2.1.5. Estimating the divergence of a posterior with respect
to a Gibbs prior
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
67
2.2. Playing with two posterior and two local prior distributions
.
.
68
2.2.1. Comparing two posterior distributions
.
.
.
.
.
.
.
68
2.2.2. Elaborate uses of relative bounds between posteriors
.
.
70
2.2.3. Analysis of relative bounds
.
.
.
.
.
.
.
.
.
.
.
75
iii

iv
Contents
2.3. Two step localization.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
89
2.3.1. Two step localization of bounds relative to a Gibbs prior .
89
2.3.2. Analysis of two step bounds relative to a Gibbs prior
.
.
96
2.3.3. Two step localization between posterior distributions
.
.
101
3. Transductive PAC-Bayesian learning .
.
.
.
.
.
.
.
.
.
111
3.1. Basic inequalities
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
111
3.1.1. The transductive setting
.
.
.
.
.
.
.
.
.
.
.
.
111
3.1.2. Absolute bound .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
113
3.1.3. Relative bounds .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
114
3.2. Vapnik bounds for transductive classification
.
.
.
.
.
.
115
3.2.1. With a shadow sample of arbitrary size
.
.
.
.
.
.
.
115
3.2.2. When the shadow sample has the same size as the
training sample .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
118
3.2.3. When moreover the distribution of the augmented sam-
ple is exchangeable
.
.
.
.
.
.
.
.
.
.
.
.
.
.
119
3.3. Vapnik bounds for inductive classification .
.
.
.
.
.
.
.
121
3.3.1. Arbitrary shadow sample size .
.
.
.
.
.
.
.
.
.
.
121
3.3.2. A better minimization with respect to the exponential
parameter .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
123
3.3.3. Equal shadow and training sample sizes .
.
.
.
.
.
.
125
3.3.4. Improvement on the equal sample size bound in the i.i.d. case
125
3.4. Gaussian approximation in Vapnik bounds
.
.
.
.
.
.
.
.
127
3.4.1. Gaussian upper bounds of variance terms
.
.
.
.
.
.
127
3.4.2. Arbitrary shadow sample size .
.
.
.
.
.
.
.
.
.
.
128
3.4.3. Equal sample sizes in the i.i.d. case
.
.
.
.
.
.
.
.
128
4. Support Vector Machines .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
131
4.1. How to build them .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
131
4.1.1. The canonical hyperplane
.
.
.
.
.
.
.
.
.
.
.
.
131
4.1.2. Computation of the canonical hyperplane
.
.
.
.
.
.
132
4.1.3. Support vectors .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
134
4.1.4. The non-separable case .
.
.
.
.
.
.
.
.
.
.
.
.
134
4.1.5. Support Vector Machines
.
.
.
.
.
.
.
.
.
.
.
.
138
4.1.6. Building kernels
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
140
4.2. Bounds for Support Vector Machines
.
.
.
.
.
.
.
.
.
142
4.2.1. Compression scheme bounds
.
.
.
.
.
.
.
.
.
.
.
142
4.2.2. The Vapnikâ€“Cervonenkis dimension of a family of subsets
143
4.2.3. Vapnikâ€“Cervonenkis dimension of linear rules with margin
145
4.2.4. Application to Support Vector Machines .
.
.
.
.
.
.
148
4.2.5. Inductive margin bounds
.
.
.
.
.
.
.
.
.
.
.
.
149
Appendix: Classification by thresholding
.
.
.
.
.
.
.
.
.
155
5.1. Description of the model
.
.
.
.
.
.
.
.
.
.
.
.
.
.
155
5.2. Computation of inductive bounds
.
.
.
.
.
.
.
.
.
.
.
156
5.3. Transductive bounds .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
158
Bibliography.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
161

Preface
This monograph deals with adaptive supervised classiï¬cation, using tools bor-
rowed from statistical mechanics and information theory, stemming from the PAC-
Bayesian approach pioneered by David McAllester and applied to a conception of
statistical learning theory forged by Vladimir Vapnik. Using convex analysis on the
set of posterior probability measures, we show how to get local measures of the
complexity of the classiï¬cation model involving the relative entropy of posterior
distributions with respect to Gibbs posterior measures. We then discuss relative
bounds, comparing the generalization error of two classiï¬cation rules, showing how
the margin assumption of Mammen and Tsybakov can be replaced with some em-
pirical measure of the covariance structure of the classiï¬cation model. We show how
to associate to any posterior distribution an eï¬€ective temperature relating it to the
Gibbs prior distribution with the same level of expected error rate, and how to esti-
mate this eï¬€ective temperature from data, resulting in an estimator whose expected
error rate converges according to the best possible power of the sample size adap-
tively under any margin and parametric complexity assumptions. We describe and
study an alternative selection scheme based on relative bounds between estimators,
and present a two step localization technique which can handle the selection of a
parametric model from a family of those. We show how to extend systematically
all the results obtained in the inductive setting to transductive learning, and use
this to improve Vapnikâ€™s generalization bounds, extending them to the case when
the sample is made of independent non-identically distributed pairs of patterns and
labels. Finally we review brieï¬‚y the construction of Support Vector Machines and
show how to derive generalization bounds for them, measuring the complexity ei-
ther through the number of support vectors or through the value of the transductive
or inductive margin.
Olivier Catoni
CNRS â€“ Laboratoire de ProbabilitÂ´es et Mod`eles AlÂ´eatoires, UniversitÂ´e Paris 6
(site Chevaleret), 4 place Jussieu â€“ Case 188, 75 252 Paris Cedex 05.
v

to my son Nicolas

Introduction
Among the possible approaches to pattern recognition, statistical learning theory
has received a lot of attention in the last few years. Although a realistic pattern
recognition scheme involves data pre-processing and post-processing that need a
theory of their own, a central role is often played by some kind of supervised learning
algorithm. This central building block is the subject we are going to analyse in these
notes.
Accordingly, we assume that we have prepared in some way or another a sample
of N labelled patterns (Xi, Yi)N
i=1, where Xi ranges in some pattern space X and Yi
ranges in some ï¬nite label set Y. We also assume that we have devised our experi-
ment in such a way that the couples of random variables (Xi, Yi) are independent
(but not necessarily equidistributed). Here, randomness should be understood to
come from the way the statistician has planned his experiment. He may for in-
stance have drawn the Xis at random from some larger population of patterns the
algorithm is meant to be applied to in a second stage. The labels Yi may have
been set with the help of some external expertise (which may itself be faulty or
contain some amount of randomness, so we do not assume that Yi is a function of
Xi, and allow the couple of random variables (Xi, Yi) to follow any kind of joint
distribution). In practice, patterns will be extracted from some high dimensional
and highly structured data, such as digital images, speech signals, DNA sequences,
etc. We will not discuss this pre-processing stage here, although it poses crucial
problems dealing with segmentation and the choice of a representation. The aim
of supervised classiï¬cation is to choose some classiï¬cation rule f : X â†’Y which
predicts Y from X making as few mistakes as possible on average.
The choice of f will be driven by a suitable use of the information provided by the
sample (Xi, Yi)N
i=1 on the joint distribution of X and Y . Moreover, considering all
the possible measurable functions f from X to Y would not be feasible in practice
and maybe more importantly not well founded from a statistical point of view,
at least as soon as the pattern space X is large and little is known in advance
about the joint distribution of patterns X and labels Y . Therefore, we will consider
parametrized subsets of classiï¬cation rules {fÎ¸ : X â†’Y ; Î¸ âˆˆÎ˜m}, m âˆˆM, which
may be grouped to form a big parameter set Î˜ = 
mâˆˆM Î˜m.
The subject of this monograph is to introduce to statistical learning theory, and
more precisely to the theory of supervised classiï¬cation, a number of technical tools
akin to statistical mechanics and information theory, dealing with the concepts of
entropy and temperature. A central task will in particular be to control the mutual
information between an estimated parameter and the observed sample. The focus
will not be directly on the description of the data to be classiï¬ed, but on the de-
scription of the classiï¬cation rules. As we want to deal with high dimensional data,
we will be bound to consider high dimensional sets of candidate classiï¬cation rules,
and will analyse them with tools very similar to those used in statistical mechanics
vii

viii
Introduction
to describe particle systems with many degrees of freedom. More speciï¬cally, the
sets of classiï¬cation rules will be described by Gibbs measures deï¬ned on parameter
sets and depending on the observed sample value. A Gibbs measure is the special
kind of probability measure used in statistical mechanics to describe the state of a
particle system driven by a given energy function at some given temperature. Here,
Gibbs measures will emerge as minimizers of the average loss value under entropy
(or mutual information) constraints. Entropy itself, more precisely the Kullback
divergence function between probability measures, will emerge in conjunction with
the use of exponential deviation inequalities: indeed, the log-Laplace transform may
be seen as the Legendre transform of the Kullback divergence function, as will be
stated in Lemma 1.1.3 (page 4).
To ï¬x notation, let (Xi, Yi)N
i=1 be the canonical process on Î© = (X Ã— Y)N (which
means the coordinate process). Let the pattern space be provided with a sigma-
algebra B turning it into a measurable space (X, B). On the ï¬nite label space Y, we
will consider the trivial algebra Bâ€² made of all its subsets. Let M1
+

(K Ã— Y)N, (B âŠ—
Bâ€²)âŠ—N
be our notation for the set of probability measures (i.e. of positive measures
of total mass equal to 1) on the measurable space

(XÃ—Y)N, (BÃ—Bâ€²)âŠ—N
. Once some
probability distribution P âˆˆM1
+

(XÃ—Y)N, (BâŠ—Bâ€²)âŠ—N
is chosen, it turns (Xi, Yi)N
i=1
into the canonical realization of a stochastic process modelling the observed sample
(also called the training set). We will assume that P = N
i=1 Pi, where for each
i = 1, . . . , N, Pi âˆˆM1
+(X Ã— Y, B âŠ—Bâ€²), to reï¬‚ect the assumption that we observe
independent pairs of patterns and labels. We will also assume that we are provided
with some indexed set of possible classiï¬cation rules
RÎ˜ =

fÎ¸ : X â†’Y; Î¸ âˆˆÎ˜

,
where (Î˜, T) is some measurable index set. Assuming some indexation of the classi-
ï¬cation rules is just a matter of presentation. Although it leads to heavier notation,
it allows us to integrate over the space of classiï¬cation rules as well as over Î©, us-
ing the usual formalism of multiple integrals. For this matter, we will assume that
(Î¸, x) â†’fÎ¸(x) : (Î˜ Ã— X, B âŠ—T) â†’(Y, Bâ€²) is a measurable function.
In many cases, as already mentioned, Î˜ = 
mâˆˆM Î˜m will be a ï¬nite (or more
generally countable) union of subspaces, dividing the classiï¬cation model RÎ˜ =

mâˆˆM RÎ˜m into a union of sub-models. The importance of introducing such a
structure has been put forward by V. Vapnik, as a way to avoid making strong
hypotheses on the distribution P of the sample. If neither the distribution of the
sample nor the set of classiï¬cation rules were constrained, it is well known that no
kind of statistical inference would be possible. Considering a family of sub-models is
a way to provide for adaptive classiï¬cation where the choice of the model depends on
the observed sample. Restricting the set of classiï¬cation rules is more realistic than
restricting the distribution of patterns, since the classiï¬cation rules are a processing
tool left to the choice of the statistician, whereas the distribution of the patterns
is not fully under his control, except for some planning of the learning experiment
which may enforce some weak properties like independence, but not the precise
shapes of the marginal distributions Pi which are as a rule unknown distributions
on some high dimensional space.
In these notes, we will concentrate on general issues concerned with a natural
measure of risk, namely the expected error rate of each classiï¬cation rule fÎ¸, ex-
pressed as
(0.1)
R(Î¸) = 1
N
N

i=1
P

fÎ¸(Xi) Ì¸= Yi

.

Introduction
ix
As this quantity is unobserved, we will be led to work with the corresponding
empirical error rate
(0.2)
r(Î¸, Ï‰) = 1
N
N

i=1
1

fÎ¸(Xi) Ì¸= Yi

.
This does not mean that practical learning algorithms will always try to minimize
this criterion. They often on the contrary try to minimize some other criterion which
is linked with the structure of the problem and has some nice additional properties
(like smoothness and convexity, for example). Nevertheless, and independently of
the precise form of the estimator 	Î¸ : Î© â†’Î˜ under study, the analysis of R(	Î¸) is a
natural question, and often corresponds to what is required in practice.
Answering this question is not straightforward because, although R(Î¸) is the
expectation of r(Î¸), a sum of independent Bernoulli random variables, R(	Î¸) is not
the expectation of r(	Î¸), because of the dependence of 	Î¸ on the sample, and neither
is r(	Î¸) a sum of independent random variables. To circumvent this unfortunate
situation, some uniform control over the deviations of r from R is needed.
We will follow the PAC-Bayesian approach to this problem, originated in the
machine learning community and pioneered by McAllester (1998, 1999). It can be
seen as some variant of the more classical approach of M-estimators relying on
empirical process theory â€” as described for instance in Van de Geer (2000).
It is built on some general principles:
â€¢ One idea is to embed the set of estimators of the type 	Î¸ : Î© â†’Î˜ into the
larger set of regular conditional probability measures Ï :

Î©, (B âŠ—Bâ€²)âŠ—N
â†’
M1
+(Î˜, T). We will call these conditional probability measures posterior dis-
tributions, to follow standard terminology.
â€¢ A second idea is to measure the ï¬‚uctuations of Ï with respect to the sample,
using some prior distribution Ï€ âˆˆM1
+(Î˜, T), and the Kullback divergence
function K(Ï, Ï€). The expectation P

K(Ï, Ï€)

measures the randomness of
Ï. The optimal choice of Ï€ would be P(Ï), resulting in a measure of the
randomness of Ï equal to the mutual information between the sample and the
estimated parameter drawn from Ï. Anyhow, since P(Ï) is usually not better
known than P, we will have to be content with some less concentrated prior
distribution Ï€, resulting in some looser measure of randomness, as shown by
the identity P

K(Ï, Ï€)

= P

K

Ï, P(Ï)

+ K

P(Ï), Ï€

.
â€¢ A third idea is to analyse the ï¬‚uctuations of the random process Î¸ â†’r(Î¸)
from its mean process Î¸ â†’R(Î¸) through the log-Laplace transform
âˆ’1
Î» log

exp

âˆ’Î»r(Î¸, Ï‰)

Ï€(dÎ¸)P(dÏ‰)

,
as would be done in statistical mechanics, where this is called the free energy.
This transform is well suited to relate minÎ¸âˆˆÎ˜ r(Î¸) to infÎ¸âˆˆÎ˜ R(Î¸), since for
large enough values of the parameter Î», corresponding to low enough values
of the temperature, the system has small ï¬‚uctuations around its ground state.
â€¢ A fourth idea deals with localization. It consists of considering a prior dis-
tribution Ï€ depending on the unknown expected error rate function R. Thus
some central result of the theory will consist in an empirical upper bound for
K

Ï, Ï€exp(âˆ’Î²R)

, where Ï€exp(âˆ’Î²R), deï¬ned by its density
d
dÏ€

Ï€exp(âˆ’Î²R)

=
exp(âˆ’Î²R)
Ï€

exp(âˆ’Î²R)
,

x
Introduction
is a Gibbs distribution built from a known prior distribution Ï€ âˆˆM1
+(Î˜, T),
some inverse temperature parameter Î² âˆˆR+ and the expected error rate R.
This bound will in particular be used when Ï is a posterior Gibbs distribution,
of the form Ï€exp(âˆ’Î²r). The general idea will be to show that in the case when
Ï is not too random, in the sense that it is possible to ï¬nd a prior (that
is non-random) distribution Ï€ such that K(Ï, Ï€) is small, then Ï(r) can be
reliably taken for a good approximation of Ï(R).
This monograph is divided into four chapters. The ï¬rst deals with the inductive
setting presented in these lines. The second is devoted to relative bounds. It shows
that it is possible to obtain a tighter estimate of the mutual information between
the sample and the estimated parameter by comparing prior and posterior Gibbs
distributions. It shows how to use this idea to obtain adaptive model selection
schemes under very weak hypotheses.
The third chapter introduces the transductive setting of V. Vapnik (Vapnik,
1998), which consists in comparing the performance of classiï¬cation rules on the
learning sample with their performance on a test sample instead of their average
performance. The fourth one is a fast introduction to Support Vector Machines.
It is the occasion to show the implications of the general results discussed in the
three ï¬rst chapters when some particular choice is made about the structure of the
classiï¬cation rules.
In the ï¬rst chapter, two types of bounds are shown. Empirical bounds are useful
to build, compare and select estimators. Non random bounds are useful to assess the
speed of convergence of estimators, relating this speed to the behaviour of the Gibbs
prior expected error rate Î² â†’Ï€exp(âˆ’Î²R)(R) and to covariance factors related to the
margin assumption of Mammen and Tsybakov when a ï¬ner analysis is performed.
We will proceed from the most straightforward bounds towards more elaborate
ones, built to achieve a better asymptotic behaviour. In this course towards more
sophisticated inequalities, we will introduce local bounds and relative bounds.
The study of relative bounds is expanded in the third chapter, where tighter
comparisons between prior and posterior Gibbs distributions are proved. Theorems
2.1.3 (page 54) and 2.2.4 (page 72) present two ways of selecting some nearly opti-
mal classiï¬cation rule. They are both proved to be adaptive in all the parameters
under Mammen and Tsybakov margin assumptions and parametric complexity as-
sumptions. This is done in Corollary 2.1.17 (page 66) of Theorem 2.1.15 (page
65) and in Theorem 2.2.11 (page 88). In the ï¬rst approach, the performance of a
randomized estimator modelled by a posterior distribution is compared with the
performance of a prior Gibbs distribution. In the second approach posterior distri-
butions are directly compared between themselves (and leads to slightly stronger
results, to the price of using a more complex algorithm). When there are more than
one parametric model, it is appropriate to use also some doubly localized scheme:
two step localization is presented for both approaches, in Theorems 2.3.2 (page 93)
and 2.3.9 (page 107) and provides bounds with a decreased inï¬‚uence of the number
of empirically ineï¬ƒcient models included in the selection scheme.
We would not like to induce the reader into thinking that the most sophisticated
results presented in these ï¬rst two chapters are necessarily the most useful ones,
they are as a rule only more eï¬ƒcient asymptotically, whereas, being more involved,
they use looser constants leading to less precision for small sample sizes. In practice
whether a sample is to be considered small is a question of the ratio between the
number of examples and the complexity (roughly speaking the number of parame-
ters) of the model used for classiï¬cation. Since our aim here is to describe methods

Introduction
xi
appropriate for complex data (images, speech, DNA, . . . ), we suspect that practi-
tioners wanting to make use of our proposals will often be confronted with small
sample sizes; thus we would advise them to try the simplest bounds ï¬rst and only
afterwards see whether the asymptotically better ones can bring some improvement.
We would also like to point out that the results of the ï¬rst two chapters are not
of a purely theoretical nature: posterior parameter distributions can indeed be com-
puted eï¬€ectively, using Monte Carlo techniques, and there is well-established know-
how about these computations in Bayesian statistics. Moreover, non-randomized
estimators of the classical form 	Î¸ : Î© â†’Î˜ can be eï¬ƒciently approximated by pos-
terior distributions Ï : Î© â†’M1
+(Î˜) supported by a fairly narrow neighbourhood
of 	Î¸, more precisely a neighbourhood of the size of the typical ï¬‚uctuations of 	Î¸, so
that this randomized approximation of 	Î¸ will most of the time provide the same
classiï¬cation as 	Î¸ itself, except for a small amount of dubious examples for which
the classiï¬cation provided by 	Î¸ would anyway be unreliable. This is explained on
page 7.
As already mentioned, the third chapter is about the transductive setting, that
is about comparing the performance of estimators on a training set and on a test
set. We show ï¬rst that this comparison can be based on a set of exponential devi-
ation inequalities which parallels the one used in the inductive case. This gives the
opportunity to transport all the results obtained in the inductive case in a system-
atic way. In the transductive setting, the use of prior distributions can be extended
to the use of partially exchangeable posterior distributions depending on the union
of training and test patterns, bringing increased possibilities to adapt to the data
and giving rise to such crucial notions of complexity as the Vapnikâ€“Cervonenkis
dimension.
Having done so, we more speciï¬cally focus on the small sample case, where local
and relative bounds are not expected to be of great help. Introducing a ï¬ctitious
(that is unobserved) shadow sample, we study Vapnik-type generalization bounds,
showing how to tighten and extend them with some original ideas, like making no
Gaussian approximation to the log-Laplace transform of Bernoulli random vari-
ables, using a shadow sample of arbitrary size. shrinking from the use of any sym-
metrization trick, and using a suitable subset of the group of permutations to cover
the case of independent non-identically distributed data. The culminating result of
the third chapter is Theorem 3.3.3 (page 125), subsequent bounds showing the sep-
arate inï¬‚uence of the above ideas and providing an easier comparison with Vapnikâ€™s
original results. Vapnik-type generalization bounds have a broad applicability, not
only through the concept of Vapnikâ€“Cervonenkis dimension, but also through the
use of compression schemes (Little et al., 1986), which are brieï¬‚y described on page
117.
The beginning of the fourth chapter introduces Support Vector Machines, both
in the separable and in the non-separable case (using the box constraint). We then
describe diï¬€erent types of bounds. We start with compression scheme bounds, to
proceed with margin bounds. We begin with transductive margin bounds, recalling
on this occasion in Theorem 4.2.2 (page 144) the growth bound for a family of
classiï¬cation rules with given Vapnikâ€“Cervonenkis dimension. In Theorem 4.2.4
(page 145) we give the usual estimate of the Vapnikâ€“Cervonenkis dimension of a
family of separating hyperplanes with a given transductive margin (we mean by
this that the margin is computed on the union of the training and test sets). We
present an original probabilistic proof inspired by a similar one from Cristianini et
al. (2000), whereas other proofs available usually rely on the informal claim that

xii
Introduction
the simplex is the worst case. We end this short review of Support Vector Machines
with a discussion of inductive margin bounds. Here the margin is computed on the
training set only, and a more involved combinatorial lemma, due to Alon et al.
(1997) and recalled in Lemma 4.2.6 (page 149) is used. We use this lemma and the
results of the third chapter to establish a bound depending on the margin of the
training set alone.
In appendix, we ï¬nally discuss the textbook example of classiï¬cation by thresh-
olding: in this setting, each classiï¬cation rule is built by thresholding a series of
measurements and taking a decision based on these thresholded values. This rel-
atively simple example (which can be considered as an introduction to the more
technical case of classiï¬cation trees) can be used to give more ï¬‚esh to the results
of the ï¬rst three chapters.
It is a pleasure to end this introduction with my greatest thanks to Anthony
Davison, for his careful reading of the manuscript and his numerous suggestions.

Chapter 1
Inductive PAC-Bayesian
learning
The setting of inductive inference (as opposed to transductive inference to be dis-
cussed later) is the one described in the introduction.
When we will have to take the expectation of a random variable Z : Î© â†’R as
well as of a function of the parameter h : Î˜ â†’R with respect to some probability
measure, we will as a rule use short functional notation instead of resorting to the
integral sign: thus we will write P(Z) for

Î© Z(Ï‰)P(dÏ‰) and Ï€(h) for

Î˜ h(Î¸)Ï€(dÎ¸).
A more traditional statistical approach would focus on estimators 	Î¸ : Î© â†’Î˜
of the parameter Î¸ and be interested on the relationship between the empirical
error rate r(	Î¸), deï¬ned by equation (0.1, page viii), which is the number of errors
made on the sample, and the expected error rate R(	Î¸), deï¬ned by equation (0.2,
page ix), which is the expected probability of error on new instances of patterns.
The PAC-Bayesian approach instead chooses a broader perspective and allows the
estimator 	Î¸ to be drawn at random using some auxiliary source of randomness to
smooth the dependence of 	Î¸ on the sample. One way of representing the supple-
mentary randomness allowed in the choice of 	Î¸, is to consider what it is usual to
call posterior distributions on the parameter space, that is probability measures
Ï : Î© â†’M1
+(Î˜, T), depending on the sample, or from a technical perspective,
regular conditional (or transition) probability measures. Let us recall that we use
the model described in the introduction: the training sample is modelled by the
canonical process (Xi, Yi)N
i=1 on Î© =

X Ã— Y
N, and a product probability measure
P = N
i=1 Pi on Î© is considered to reï¬‚ect the assumption that the training sam-
ple is made of independent pairs of patterns and labels. The transition probability
measure Ï, along with P âˆˆM1
+(Î©), deï¬nes a probability distribution on Î© Ã— Î˜ and
describes the conditional distribution of the estimated parameter 	Î¸ knowing the
sample (Xi, Yi)N
i=1.
The main subject of this broadened theory becomes to investigate the relation-
ship between Ï(r), the average error rate of 	Î¸ on the training sample, and Ï(R), the
expected error rate of 	Î¸ on new samples. The ï¬rst step towards using some kind
of thermodynamics to tackle this question, is to consider the Laplace transform
of Ï(R) âˆ’Ï(r), a well known provider of non-asymptotic deviation bounds. This
transform takes the form
P

exp

Î»

Ï(R) âˆ’Ï(r)

,
1

2
Chapter 1.
Inductive PAC-Bayesian learning
where some inverse temperature parameter Î» âˆˆR+, as a physicist would call it, is
introduced. This Laplace transform would be easy to bound if Ï did not depend on
Ï‰ âˆˆÎ© (namely on the sample), because Ï(R) would then be non-random, and
Ï(r) = 1
N
N

i=1
Ï

Yi Ì¸= fÎ¸(Xi)

,
would be a sum of independent random variables. It turns out, and this will be
the subject of the next section, that this annoying dependence of Ï on Ï‰ can be
quantiï¬ed, using the inequality
Ï(R) âˆ’Ï(r) â‰¤Î»âˆ’1 log

Ï€

exp

Î»(R âˆ’r)

+ Î»âˆ’1K(Ï, Ï€),
which holds for any probability measure Ï€ âˆˆM1
+(Î˜) on the parameter space;
for our purpose it will be appropriate to consider a prior distribution Ï€ that is
non-random, as opposed to Ï, which depends on the sample. Here, K(Ï, Ï€) is the
Kullback divergence of Ï from Ï€, whose deï¬nition will be recalled when we will
come to technicalities; it can be seen as an upper bound for the mutual information
between the (Xi, Yi)N
i=1 and the estimated parameter 	Î¸ . This inequality will allow
us to relate the penalized diï¬€erence Ï(R) âˆ’Ï(r) âˆ’Î»âˆ’1K(Ï, Ï€) with the Laplace
transform of sums of independent random variables.
1.1. Basic inequality
Let us now come to the details of the investigation sketched above. The ï¬rst thing
we will do is to study the Laplace transform of R(Î¸) âˆ’r(Î¸), as a starting point for
the more general study of Ï(R) âˆ’Ï(r): it corresponds to the simple case where 	Î¸
is not random at all, and therefore where Ï is a Dirac mass at some deterministic
parameter value Î¸.
In the setting described in the introduction, let us consider the Bernoulli random
variables Ïƒi(Î¸) = 1

Yi Ì¸= fÎ¸(Xi)

, which indicates whether the classiï¬cation rule fÎ¸
made an error on the ith component of the training sample. Using independence
and the concavity of the logarithm function, it is readily seen that for any real
constant Î»
log

P

exp

âˆ’Î»r(Î¸)

=
N

i=1
log

P

exp

âˆ’Î»
N Ïƒi

â‰¤N log
 1
N
N

i=1
P

exp

âˆ’Î»
N Ïƒi

.
The right-hand side of this inequality is the log-Laplace transform of a Bernoulli
distribution with parameter
1
N
N
i=1 P(Ïƒi) = R(Î¸). As any Bernoulli distribution is
fully deï¬ned by its parameter, this log-Laplace transform is necessarily a function
of R(Î¸). It can be expressed with the help of the family of functions
(1.1)
Î¦a(p) = âˆ’aâˆ’1 log

1 âˆ’

1 âˆ’exp(âˆ’a)

p

,
a âˆˆR, p âˆˆ(0, 1).
It is immediately seen that Î¦a is an increasing one-to-one mapping of the unit
interval onto itself, and that it is convex when a > 0, concave when a < 0 and can

1.1.
Basic inequality
3
be deï¬ned by continuity to be the identity when a = 0. Moreover the inverse of Î¦a
is given by the formula
Î¦âˆ’1
a (q) = 1 âˆ’exp(âˆ’aq)
1 âˆ’exp(âˆ’a) ,
a âˆˆR, q âˆˆ(0, 1).
This formula may be used to extend Î¦âˆ’1
a
to q âˆˆR, and we will use this extension
without further notice when required.
Using this notation, the previous inequality becomes
log

P

exp

âˆ’Î»r(Î¸)

â‰¤âˆ’Î»Î¦ Î»
N

R(Î¸)

,
proving
Lemma 1.1.1. For any real constant Î» and any parameter Î¸ âˆˆÎ˜,
P

exp

Î»

Î¦ Î»
N

R(Î¸)

âˆ’r(Î¸)

â‰¤1.
In previous versions of this study, we had used some Bernstein bound, instead
of this lemma. Anyhow, as it will turn out, keeping the log-Laplace transform of a
Bernoulli instead of approximating it provides simpler and tighter results.
Lemma 1.1.1 implies that for any constants Î» âˆˆR+ and Ïµ âˆˆ)0, 1),
P

Î¦ Î»
N

R(Î¸)

+ log(Ïµ)
Î»
â‰¤r(Î¸)

â‰¥1 âˆ’Ïµ.
Choosing Î» âˆˆarg max
R+ Î¦ Î»
N

R(Î¸)

+ log(Ïµ)
Î»
, we deduce
Lemma 1.1.2. For any Ïµ âˆˆ)0, 1), any Î¸ âˆˆÎ˜,
P

R(Î¸) â‰¤inf
Î»âˆˆR+ Î¦âˆ’1
Î»
N

r(Î¸) âˆ’log(Ïµ)
Î»

â‰¥1 âˆ’Ïµ.
We will illustrate throughout these notes the bounds we prove with a small
numerical example: in the case where N = 1000, Ïµ = 0.01 and r(Î¸) = 0.2, we get
with a conï¬dence level of 0.99 that R(Î¸) â‰¤.2402, this being obtained for Î» = 234.
Now, to proceed towards the analysis of posterior distributions, let us put UÎ»(Î¸,
Ï‰) = Î»

Î¦ Î»
N

R(Î¸)

âˆ’r(Î¸, Ï‰)

for short, and let us consider some prior probability
distribution Ï€ âˆˆM1
+(Î˜, T). A proper choice of Ï€ will be an important question,
underlying much of the material presented in this monograph, so for the time be-
ing, let us only say that we will let this choice be as open as possible by writing
inequalities which hold for any choice of Ï€ . Let us insist on the fact that when we
say that Ï€ is a prior distribution, we mean that it does not depend on the training
sample (Xi, Yi)N
i=1. The quantity of interest to obtain the bound we are looking for
is log

P

Ï€

exp(UÎ»)

. Using Fubiniâ€™s theorem for non-negative functions, we see
that
log

P

Ï€

exp(UÎ»)

= log

Ï€

P

exp(UÎ»)

â‰¤0.
To relate this quantity to the expectation Ï(UÎ») with respect to any posterior
distribution Ï : Î© â†’M1
+(Î˜), we will use the properties of the Kullback divergence

4
Chapter 1.
Inductive PAC-Bayesian learning
K(Ï, Ï€) of Ï with respect to Ï€, which is deï¬ned as
K(Ï, Ï€) =
âŽ§
âŽª
âŽ¨
âŽª
âŽ©

log( dÏ
dÏ€)dÏ,
when Ï is absolutely continuous
with respect to Ï€,
+âˆž,
otherwise.
The following lemma shows in which sense the Kullback divergence function can be
thought of as the dual of the log-Laplace transform.
Lemma 1.1.3.
For any bounded measurable function h : Î˜ â†’R, and any proba-
bility distribution Ï âˆˆM1
+(Î˜) such that K(Ï, Ï€) < âˆž,
log

Ï€

exp(h)

= Ï(h) âˆ’K(Ï, Ï€) + K(Ï, Ï€exp(h)),
where by deï¬nition dÏ€exp(h)
dÏ€
= exp[h(Î¸)]
Ï€[exp(h)]. Consequently
log

Ï€

exp(h)]

=
sup
ÏâˆˆM1
+(Î˜)
Ï(h) âˆ’K(Ï, Ï€).
The proof is just a matter of writing down the deï¬nition of the quantities involved
and using the fact that the Kullback divergence function is non-negative, and can
be found in Catoni (2004, page 160). In the duality between measurable functions
and probability measures, we thus see that the log-Laplace transform with respect
to Ï€ is the Legendre transform of the Kullback divergence function with respect to
Ï€. Using this, we get
P

exp

sup
ÏâˆˆM1
+(Î˜)
Ï[UÎ»(Î¸)] âˆ’K(Ï, Ï€)

â‰¤1,
which, combined with the convexity of Î»Î¦ Î»
N , proves the basic inequality we were
looking for.
Theorem 1.1.4. For any real constant Î»,
P

exp

sup
ÏâˆˆM1
+(Î˜)
Î»

Î¦ Î»
N

Ï(R)

âˆ’Ï(r)

âˆ’K(Ï, Ï€)

â‰¤P

exp

sup
ÏâˆˆM1
+(Î˜)
Î»

Ï

Î¦ Î»
N â—¦R

âˆ’Ï(r)

âˆ’K(Ï, Ï€)

â‰¤1.
We insist on the fact that in this theorem, we take a supremum in Ï âˆˆM1
+(Î˜)
inside the expectation with respect to P, the sample distribution. This means that
the proved inequality holds for any Ï depending on the training sample, that is for
any posterior distribution: indeed, measurability questions set aside,
P

exp

sup
ÏâˆˆM1
+(Î˜)
Î»

Ï

UÎ»(Î¸)

âˆ’K(Ï, Ï€)

=
sup
Ï:Î©â†’M1
+(Î˜)
P

exp

Î»

Ï

UÎ»(Î¸)

âˆ’K(Ï, Ï€)

,

1.2.
Non local bounds
5
and more formally,
sup
Ï:Î©â†’M1
+(Î˜)
P

exp

Î»

Ï

UÎ»(Î¸)

âˆ’K(Ï, Ï€)

â‰¤P

exp

sup
ÏâˆˆM1
+(Î˜)
Î»

Ï

UÎ»(Î¸)

âˆ’K(Ï, Ï€)

,
where the supremum in Ï taken in the left-hand side is restricted to regular condi-
tional probability distributions.
The following sections will show how to use this theorem.
1.2. Non local bounds
At least three sorts of bounds can be deduced from Theorem 1.1.4.
The most interesting ones with which to build estimators and tune parameters,
as well as the ï¬rst that have been considered in the development of the PAC-
Bayesian approach, are deviation bounds. They provide an empirical upper bound
for Ï(R) â€” that is a bound which can be computed from observed data â€” with
some probability 1âˆ’Ïµ, where Ïµ is a presumably small and tunable parameter setting
the desired conï¬dence level.
Anyhow, most of the results about the convergence speed of estimators to be
found in the statistical literature are concerned with the expectation P

Ï(R)

, there-
fore it is also enlightening to bound this quantity. In order to know at which rate
it may be approaching infÎ˜ R, a non-random upper bound is required, which will
relate the average of the expected risk P

Ï(R)

with the properties of the contrast
function Î¸ â†’R(Î¸).
Since the values of constants do matter a lot when a bound is to be used to se-
lect between various estimators using classiï¬cation models of various complexities,
a third kind of bound, related to the ï¬rst, may be considered for the sake of its
hopefully better constants: we will call them unbiased empirical bounds, to stress
the fact that they provide some empirical quantity whose expectation under P can
be proved to be an upper bound for P

Ï(R)

, the average expected risk. The price
to pay for these better constants is of course the lack of formal guarantee given by
the bound: two random variables whose expectations are ordered in a certain way
may very well be ordered in the reverse way with a large probability, so that basing
the estimation of parameters or the selection of an estimator on some unbiased
empirical bound is a hazardous business. Anyhow, since it is common practice to
use the inequalities provided by mathematical statistical theory while replacing the
proven constants with smaller values showing a better practical eï¬ƒciency, consid-
ering unbiased empirical bounds as well as deviation bounds provides an indication
about how much the constants may be decreased while not violating the theory too
much.
1.2.1. Unbiased empirical bounds.
Let Ï : Î© â†’M1
+(Î˜) be some ï¬xed (and
arbitrary) posterior distribution, describing some randomized estimator 	Î¸ : Î© â†’Î˜.
As we already mentioned, in these notes a posterior distribution will always be a
regular conditional probability measure. By this we mean that
â€¢ for any A âˆˆT, the map Ï‰ â†’Ï(Ï‰, A) :

Î©, (B âŠ—Bâ€²)âŠ—N
â†’R+ is assumed to
be measurable;

6
Chapter 1.
Inductive PAC-Bayesian learning
â€¢ for any Ï‰ âˆˆÎ©, the map A â†’Ï(Ï‰, A) : T â†’R+ is assumed to be a probability
measure.
We will also assume without further notice that the Ïƒ-algebras we deal with are
always countably generated. The technical implications of these assumptions are
standard and discussed for instance in Catoni (2004, pages 50-54), where, among
other things, a detailed proof of the decomposition of the Kullback Liebler diver-
gence is given.
Let us restrict to the case when the constant Î» is positive. We get from Theorem
1.1.4 that
(1.2)
exp

Î»

Î¦ Î»
N

P

Ï(R)

âˆ’P

Ï(r)

âˆ’P

K(Ï, Ï€)

â‰¤1,
where we have used the convexity of the exp function and of Î¦ Î»
N . Since we have
restricted our attention to positive values of the constant Î», equation (1.2) can also
be written
P

Ï(R)

â‰¤Î¦âˆ’1
Î»
N

P

Ï(r) + Î»âˆ’1K(Ï, Ï€)

,
leading to
Theorem 1.2.1. For any posterior distribution Ï : Î© â†’M1
+(Î˜), for any positive
parameter Î»,
P

Ï(R)

â‰¤
1 âˆ’exp

âˆ’N âˆ’1P

Î»Ï(r) + K(Ï, Ï€)

1 âˆ’exp(âˆ’Î»
N )
â‰¤P

Î»
N

1 âˆ’exp(âˆ’Î»
N )


Ï(r) + K(Ï, Ï€)
Î»

.
The last inequality provides the unbiased empirical upper bound for Ï(R) we were
looking
for,
meaning
that
the
expectation
of
Î»
N

1âˆ’exp(âˆ’Î»
N )


Ï(r) + K(Ï,Ï€)
Î»

is larger than the expectation of Ï(R). Let us no-
tice that 1 â‰¤
Î»
N

1âˆ’exp(âˆ’Î»
N )
 â‰¤

1âˆ’
Î»
2N
âˆ’1 and therefore that this coeï¬ƒcient is close
to 1 when Î» is signiï¬cantly smaller than N.
If we are ready to believe in this bound (although this belief is not mathematically
well founded, as we already mentioned), we can use it to optimize Î» and to choose
Ï. While the optimal choice of Ï when Î» is ï¬xed is, according to Lemma 1.1.3 (page
4), to take it equal to Ï€exp(âˆ’Î»r), a Gibbs posterior distribution, as it is sometimes
called, we may for computational reasons be more interested in choosing Ï in some
other class of posterior distributions.
For instance, our real interest may be to select some non-randomized estimator
from a family 	Î¸m : Î© â†’Î˜m, m âˆˆM, of possible ones, where Î˜m are measurable
subsets of Î˜ and where M is an arbitrary (non necessarily countable) index set.
We may for instance think of the case when 	Î¸m âˆˆarg minÎ˜m r. We may slightly
randomize the estimators to start with, considering for any Î¸ âˆˆÎ˜m and any m âˆˆM,
Î”m(Î¸) =

Î¸â€² âˆˆÎ˜m :

fÎ¸â€²(Xi)
N
i=1 =

fÎ¸(Xi)
N
i=1

,
and deï¬ning Ïm by the formula
dÏm
dÏ€ (Î¸) = 1

Î¸ âˆˆÎ”m(	Î¸m)

Ï€

Î”m(	Î¸m)

.

1.2.
Non local bounds
7
Our posterior minimizes K(Ï, Ï€) among those distributions whose support is re-
stricted to the values of Î¸ in Î˜m for which the classiï¬cation rule fÎ¸ is identical
to the estimated one f	Î¸m on the observed sample. Presumably, in many practi-
cal situations, fÎ¸(x) will be Ïm almost surely identical to f	Î¸m(x) when Î¸ is drawn
from Ïm, for the vast majority of the values of x âˆˆX and all the sub-models Î˜m
not plagued with too much overï¬tting (since this is by construction the case when
x âˆˆ{Xi : i = 1, . . . , N}). Therefore replacing 	Î¸m with Ïm can be expected to be a
minor change in many situations. This change by the way can be estimated in the
(admittedly not so common) case when the distribution of the patterns (Xi)N
i=1 is
known. Indeed, introducing the pseudo distance
(1.3)
D(Î¸, Î¸â€²) = 1
N
N

i=1
P

fÎ¸(Xi) Ì¸= fÎ¸â€²(Xi)

,
Î¸, Î¸â€² âˆˆÎ˜,
one immediately sees that R(Î¸â€²) â‰¤R(Î¸) + D(Î¸, Î¸â€²), for any Î¸, Î¸â€² âˆˆÎ˜, and therefore
that
R(	Î¸m) â‰¤Ïm(R) + Ïm

D(Â·, 	Î¸m)

.
Let us notice also that in the case where Î˜m âŠ‚Rdm, and R happens to be convex on
Î”m(	Î¸m), then Ïm(R) â‰¥R

Î¸Ïm(dÎ¸)

, and we can replace 	Î¸m with Î¸m =

Î¸Ïm(dÎ¸),
and obtain bounds for R(Î¸m). This is not a very heavy assumption about R, in the
case where we consider 	Î¸m âˆˆarg minÎ˜m r. Indeed, 	Î¸m, and therefore Î”m(	Î¸m), will
presumably be close to arg minÎ˜m R, and requiring a function to be convex in the
neighbourhood of its minima is not a very strong assumption.
Since r(	Î¸m) = Ïm(r), and K(Ïm, Ï€) = âˆ’log

Ï€

Î”m(	Î¸m)

, our unbiased empiri-
cal upper bound in this context reads as
Î»
N

1 âˆ’exp(âˆ’Î»
N )


r(	Î¸m) âˆ’log

Ï€

Î”m(	Î¸m)

Î»

.
Let us notice that we obtain a complexity factor âˆ’log

Ï€

Î”m(	Î¸m)

which may be
compared with the Vapnikâ€“Cervonenkis dimension. Indeed, in the case of binary
classiï¬cation, when using a classiï¬cation model with Vapnikâ€“Cervonenkis dimen-
sion not greater than hm, that is when any subset of X which can be split in any
arbitrary way by some classiï¬cation rule fÎ¸ of the model Î˜m has at most hm points,
then

Î”m(Î¸) : Î¸ âˆˆÎ˜m

is a partition of Î˜m with at most

eN
hm
hm
components: these facts, if not already
familiar to the reader, will be proved in Theorems 4.2.2 and 4.2.3 (page 144).
Therefore
inf
Î¸âˆˆÎ˜m âˆ’log

Ï€

Î”m(Î¸)

â‰¤hm log
 eN
hm
!
âˆ’log

Ï€(Î˜m)

.
Thus, if the model and prior distribution are well suited to the classiï¬cation task, in
the sense that there is more â€œroomâ€ (where room is measured with Ï€) between the
two clusters deï¬ned by 	Î¸m than between other partitions of the sample of patterns
(Xi)N
i=1, then we will have
âˆ’log

Ï€

Î”m(	Î¸)

â‰¤hm log
 eN
hm
!
âˆ’log

Ï€(Î˜m)

.

8
Chapter 1.
Inductive PAC-Bayesian learning
An optimal value 	m may be selected so that
	m âˆˆarg min
mâˆˆM

inf
Î»âˆˆR+
Î»
N

1 âˆ’exp(âˆ’Î»
N )

"
r(	Î¸m) âˆ’log

Ï€

Î”m(	Î¸m)

Î»
#
.
Since Ï	
m is still another posterior distribution, we can be sure that
P

R(	Î¸	
m) âˆ’Ï	
m

D(Â·, 	Î¸	
m)

â‰¤P

Ï	
m(R)

â‰¤inf
Î»âˆˆR+ P

Î»
N

1 âˆ’exp(âˆ’Î»
N )

"
r(	Î¸	
m) âˆ’log

Ï€

Î”	
m(	Î¸	
m)

Î»
#
.
Taking the inï¬mum in Î» inside the expectation with respect to P would be possible
at the price of some supplementary technicalities and a slight increase of the bound
that we prefer to postpone to the discussion of deviation bounds, since they are the
only ones to provide a rigorous mathematical foundation to the adaptive selection
of estimators.
1.2.2. Optimizing explicitly the exponential parameter Î».
In this section
we address some technical issues we think helpful to the understanding of Theorem
1.2.1 (page 6): namely to investigate how the upper bound it provides could be
optimized, or at least approximately optimized, in Î». It turns out that this can be
done quite explicitly.
So we will consider in this discussion the posterior distribution Ï : Î© â†’M1
+(Î˜)
to be ï¬xed, and our aim will be to eliminate the constant Î» from the bound by
choosing its value in some nearly optimal way as a function of P

Ï(r)

, the average
of the empirical risk, and of P

K(Ï, Ï€)

, which controls overï¬tting.
Let the bound be written as
Ï•(Î») =

1 âˆ’exp(âˆ’Î»
N )
âˆ’1 
1 âˆ’exp

âˆ’Î»
N P

Ï(r)

âˆ’N âˆ’1P

K(Ï, Ï€)

.
We see that
N âˆ‚
âˆ‚Î» log

Ï•(Î»)

=
P

Ï(r)

exp

Î»
N P

Ï(r)

+ N âˆ’1P

K(Ï, Ï€)

âˆ’1
âˆ’
1
exp( Î»
N ) âˆ’1.
Thus, the optimal value for Î» is such that

exp( Î»
N ) âˆ’1

P

Ï(r)

= exp

Î»
N P

Ï(r)

+ N âˆ’1P

K(Ï, Ï€)

âˆ’1.
Assuming that 1 â‰«Î»
N P

Ï(r)

â‰«P[K(Ï,Ï€)]
N
, and keeping only higher order terms, we
are led to choose
Î» =
$
2NP

K(Ï, Ï€)

P

Ï(r)

1 âˆ’P

Ï(r)
,
obtaining
Theorem 1.2.2. For any posterior distribution Ï : Î© â†’M1
+(Î˜),
P

Ï(R)

â‰¤
1 âˆ’exp

âˆ’
%
2P[K(Ï,Ï€)]P[Ï(r)]
N{1âˆ’P[Ï(r)]}
âˆ’P[K(Ï,Ï€)]
N

1 âˆ’exp

âˆ’
%
2P[K(Ï,Ï€)]
NP[Ï(r)]{1âˆ’P[Ï(r)]}

.

1.2.
Non local bounds
9
This result of course is not very useful in itself, since neither of the two quantities
P

Ï(r)

and P

K(Ï, Ï€)

are easy to evaluate. Anyhow it gives a hint that replacing
them boldly with Ï(r) and K(Ï, Ï€) could produce something close to a legitimate
empirical upper bound for Ï(R). We will see in the subsection about deviation
bounds that this is indeed essentially true.
Let us remark that in the third chapter of this monograph, we will see another
way of bounding
inf
Î»âˆˆR+ Î¦âˆ’1
Î»
N
 
q + d
Î»
!
, leading to
Theorem 1.2.3.
For any prior distribution Ï€ âˆˆM1
+(Î˜), for any posterior distri-
bution Ï : Î© â†’M1
+(Î˜),
P

Ï(R)

â‰¤
"
1 + 2P

K(Ï, Ï€)

N
#âˆ’1 
P

Ï(r)

+ P

K(Ï, Ï€)

N
+
$
2P

K(Ï, Ï€)

P

Ï(r)

1 âˆ’P

Ï(r)

N
+ P

K(Ï, Ï€)
2
N 2

,
as soon as P

Ï(r)

+
$
P

K(Ï, Ï€)

2N
â‰¤1
2,
and P

Ï(R)

â‰¤P

Ï(r)

+
$
P

K(Ï, Ï€)

2N
otherwise.
This theorem enlightens the inï¬‚uence of three terms on the average expected
risk:
â€¢ the average empirical risk, P

Ï(r)

, which as a rule will decrease as the size of
the classiï¬cation model increases, acts as a bias term, grasping the ability of the
model to account for the observed sample itself;
â€¢ a variance term
1
N P

Ï(r)

1 âˆ’P

Ï(r)

is due to the random ï¬‚uctuations of
Ï(r);
â€¢ a complexity term P

K(Ï, Ï€)

, which as a rule will increase with the size of
the classiï¬cation model, eventually acts as a multiplier of the variance term.
We observed numerically that the bound provided by Theorem 1.2.2 is better
than the more classical Vapnik-like bound of Theorem 1.2.3. For instance, when
N = 1000, P

Ï(r)

= 0.2 and P

K(Ï, Ï€)

= 10, Theorem 1.2.2 gives a bound lower
than 0.2604, whereas the more classical Vapnik-like approximation of Theorem 1.2.3
gives a bound larger than 0.2622. Numerical simulations tend to suggest the two
bounds are always ordered in the same way, although this could be a little tedious
to prove mathematically.
1.2.3. Non random bounds.
It is time now to come to less tentative results and
see how far is the average expected error rate P

Ï(R)

from its best possible value
infÎ˜ R.
Let us notice ï¬rst that
Î»Ï(r) + K(Ï, Ï€) = K(Ï, Ï€exp(âˆ’Î»r)) âˆ’log

Ï€

exp(âˆ’Î»r)

.

10
Chapter 1.
Inductive PAC-Bayesian learning
Let us remark moreover that r â†’log

Ï€

exp(âˆ’Î»r)

is a convex functional, a prop-
erty which from a technical point of view can be dealt with in the following way:
(1.4)
P

log

Ï€

exp(âˆ’Î»r)

= P

sup
ÏâˆˆM1
+(Î˜)
âˆ’Î»Ï(r) âˆ’K(Ï, Ï€)

â‰¥
sup
ÏâˆˆM1
+(Î˜)
P

âˆ’Î»Ï(r) âˆ’K(Ï, Ï€)

=
sup
ÏâˆˆM1
+(Î˜)
âˆ’Î»Ï(R) âˆ’K(Ï, Ï€)
= log

Ï€

exp(âˆ’Î»R)

= âˆ’
 Î»
0 Ï€exp(âˆ’Î²R)(R)dÎ².
These remarks applied to Theorem 1.2.1 lead to
Theorem 1.2.4. For any posterior distribution Ï : Î© â†’M1
+(Î˜), for any positive
parameter Î»,
P

Ï(R)

â‰¤
1 âˆ’exp

âˆ’1
N
 Î»
0 Ï€exp(âˆ’Î²R)(R)dÎ² âˆ’1
N P

K(Ï, Ï€exp(âˆ’Î»r))

1 âˆ’exp(âˆ’Î»
N )
â‰¤
1
N

1 âˆ’exp(âˆ’Î»
N )

 Î»
0 Ï€exp(âˆ’Î²R)(R)dÎ² + P

K(Ï, Ï€exp(âˆ’Î»r))

.
This theorem is particularly well suited to the case of the Gibbs posterior distri-
bution Ï = Ï€exp(âˆ’Î»r), where the entropy factor cancels and where P

Ï€exp(âˆ’Î»r)(R)

is shown to get close to infÎ˜ R when N goes to +âˆž, as soon as Î»/N goes to 0 while
Î» goes to +âˆž.
We can elaborate on Theorem 1.2.4 and deï¬ne a notion of dimension of (Î˜, R),
with margin Î· â‰¥0 putting
(1.5)
dÎ·(Î˜, R) = sup
Î²âˆˆR+
Î²

Ï€exp(âˆ’Î²R)(R) âˆ’ess inf
Ï€ R âˆ’Î·

â‰¤âˆ’log

Ï€

R â‰¤ess inf
Ï€ R + Î·

.
This last inequality can be established by the chain of inequalities:
Î²Ï€exp(âˆ’Î²R)(R) â‰¤
 Î²
0 Ï€exp(âˆ’Î³R)(R)dÎ³ = âˆ’log

Ï€

exp(âˆ’Î²R)

â‰¤Î²

ess inf
Ï€ R + Î·

âˆ’log

Ï€

R â‰¤ess inf
Ï€ R + Î·

,
where we have used successively the fact that Î» â†’Ï€exp(âˆ’Î»R)(R) is decreasing
(because it is the derivative of the concave function Î» â†’âˆ’log

Ï€

exp(âˆ’Î»R)

)
and the fact that the exponential function takes positive values.
In typical â€œparametricâ€ situations d0(Î˜, R) will be ï¬nite, and in all circumstances
dÎ·(Î˜, R) will be ï¬nite for any Î· > 0 (this is a direct consequence of the deï¬nition
of the essential inï¬mum). Using this notion of dimension, we see that
 Î»
0
Ï€exp(âˆ’Î²R)(R)dÎ² â‰¤Î»

ess inf
Ï€ R + Î·

+
 Î»
0
dÎ·
Î² âˆ§(1 âˆ’ess inf
Ï€ R âˆ’Î·)

dÎ²

1.2.
Non local bounds
11
= Î»

ess inf
Ï€ R + Î·

+ dÎ·(Î˜, R) log

eÎ»
dÎ·(Î˜, R)

1 âˆ’ess inf
Ï€ R âˆ’Î·

.
This leads to
Corollary 1.2.5 With the above notation, for any margin Î· âˆˆR+, for any pos-
terior distribution Ï : Î© â†’M1
+(Î˜),
P

Ï(R)

â‰¤inf
Î»âˆˆR+ Î¦âˆ’1
Î»
N
&
ess inf
Ï€ R + Î· + dÎ·
Î» log
 eÎ»
dÎ·
!
+ P

K

Ï, Ï€exp(âˆ’Î»r)

Î»
'
.
If one wants a posterior distribution with a small support, the theorem can also
be applied to the case when Ï is obtained by truncating Ï€exp(âˆ’Î»r) to some level
set to reduce its support: let Î˜p = {Î¸ âˆˆÎ˜ : r(Î¸) â‰¤p}, and let us deï¬ne for any
q âˆˆ)0, 1) the level pq = inf{p : Ï€exp(âˆ’Î»r)(Î˜p) â‰¥q}, let us then deï¬ne Ïq by its
density
dÏq
dÏ€exp(âˆ’Î»r)
(Î¸) =
1(Î¸ âˆˆÎ˜pq)
Ï€exp(âˆ’Î»r)(Î˜pq),
then Ï0 = Ï€exp(âˆ’Î»r) and for any q âˆˆ(0, 1(,
P

Ïq(R)

â‰¤
1 âˆ’exp

âˆ’1
N
 Î»
0 Ï€exp(âˆ’Î²R)(R)dÎ² âˆ’log(q)
N

1 âˆ’exp(âˆ’Î»
N )
â‰¤
1
N

1 âˆ’exp(âˆ’Î»
N )

 Î»
0 Ï€exp(âˆ’Î²R)(R)dÎ² âˆ’log(q)

.
1.2.4. Deviation bounds.
They provide results holding under the distribution
P of the sample with probability at least 1âˆ’Ïµ, for any given conï¬dence level, set by
the choice of Ïµ âˆˆ)0, 1(. Using them is the only way to be quite (i.e. with probability
1 âˆ’Ïµ) sure to do the right thing, although this right thing may be over-pessimistic,
since deviation upper bounds are larger than corresponding non-biased bounds.
Starting again from Theorem 1.1.4 (page 4), and using Markovâ€™s inequality
P

exp(h) â‰¥1

â‰¤P

exp(h)

, we obtain
Theorem 1.2.6. For any positive parameter Î», with P probability at least 1 âˆ’Ïµ,
for any posterior distribution Ï : Î© â†’M1
+(Î˜),
Ï(R) â‰¤Î¦âˆ’1
Î»
N

Ï(r) + K(Ï, Ï€) âˆ’log(Ïµ)
Î»

=
1 âˆ’exp

âˆ’Î»Ï(r)
N
âˆ’K(Ï, Ï€) âˆ’log(Ïµ)
N

1 âˆ’exp

âˆ’Î»
N

â‰¤
Î»
N

1 âˆ’exp

âˆ’Î»
N


Ï(r) + K(Ï, Ï€) âˆ’log(Ïµ)
Î»

.
We see that for a ï¬xed value of the parameter Î», the upper bound is optimized
when the posterior is chosen to be the Gibbs distribution Ï = Ï€exp(âˆ’Î»r).
In this theorem, we have bounded Ï(R), the average expected risk of an estimator
	Î¸ drawn from the posterior Ï. This is what we will do most of the time in this study.
This is the error rate we will get if we classify a large number of test patterns,

12
Chapter 1.
Inductive PAC-Bayesian learning
drawing a new 	Î¸ for each one. However, we can also be interested in the error rate
we get if we draw only one 	Î¸ from Ï and use this single draw of 	Î¸ to classify a
large number of test patterns. This error rate is R(	Î¸). To state a result about its
deviations, we can start back from Lemma 1.1.1 (page 3) and integrate it with
respect to the prior distribution Ï€ to get for any real constant Î»
P

Ï€

exp

Î»

Î¦ Î»
N

R

âˆ’r

â‰¤1.
For any posterior distribution Ï : Î© â†’M1
+(Î˜), this can be rewritten as
P

Ï

exp

Î»

Î¦ Î»
N

R

âˆ’r

âˆ’log

 dÏ
dÏ€

+ log(Ïµ)

â‰¤Ïµ,
proving
Theorem 1.2.7 For any positive real parameter Î», for any posterior distribution
Ï : Î© â†’M1
+(Î˜), with PÏ probability at least 1 âˆ’Ïµ,
R(	Î¸ ) â‰¤Î¦âˆ’1
Î»
N

r(	Î¸ ) + Î»âˆ’1 log
 
Ïµâˆ’1 dÏ
dÏ€
!
â‰¤
Î»
N

1 âˆ’exp(âˆ’Î»
N )


r(	Î¸ ) + Î»âˆ’1 log
 
Ïµâˆ’1 dÏ
dÏ€
!
.
Let us remark that the bound provided here is the exact counterpart of the bound
of Theorem 1.2.6, since log

 dÏ
dÏ€

appears as a disintegrated version of the divergence
K(Ï, Ï€). The parallel between the two theorems is particularly striking in the special
case when Ï = Ï€exp(âˆ’Î»r). Indeed Theorem 1.2.6 proves that with P probability at
least 1 âˆ’Ïµ,
Ï€exp(âˆ’Î»r)(R) â‰¤Î¦âˆ’1
Î»
N

âˆ’log

Ï€

exp

âˆ’Î»r

+ log(Ïµ)
Î»

,
whereas Theorem 1.2.7 proves that with PÏ€exp(âˆ’Î»r) probability at least 1 âˆ’Ïµ
R(	Î¸ ) â‰¤Î¦âˆ’1
Î»
N

âˆ’log

Ï€

exp

âˆ’Î»r

+ log(Ïµ)
Î»

,
showing that we get the same deviation bound for Ï€exp(âˆ’Î»r)(R) under P and for 	Î¸
under PÏ€exp(âˆ’Î»r).
We would like to show now how to optimize with respect to Î» the bound given
by Theorem 1.2.6 (the same discussion would apply to Theorem 1.2.7). Let us
notice ï¬rst that values of Î» less than 1 are not interesting (because they provide a
bound larger than one, at least as soon as Ïµ â‰¤exp(âˆ’1)). Let us consider some real
parameter Î± > 1, and the set Î› = {Î±k; k âˆˆN}, on which we put the probability
measure Î½(Î±k) = [(k+1)(k+2)]âˆ’1. Applying Theorem 1.2.6 to Î» = Î±k at conï¬dence
level 1 âˆ’
Ïµ
(k+1)(k+2), and using a union bound, we see that with probability at least
1 âˆ’Ïµ, for any posterior distribution Ï,
Ï(R) â‰¤inf
Î»â€²âˆˆÎ› Î¦âˆ’1
Î»â€²
N
âŽ§
âŽ¨
âŽ©Ï(r) +
K(Ï, Ï€) âˆ’log(Ïµ) + 2 log

log(Î±2Î»â€²)
log(Î±)

Î»â€²
âŽ«
âŽ¬
âŽ­.

1.2.
Non local bounds
13
Now we can remark that for any Î» âˆˆ(1, +âˆž(, there is Î»â€² âˆˆÎ› such that Î±âˆ’1Î» â‰¤
Î»â€² â‰¤Î». Moreover, for any q âˆˆ(0, 1), Î² â†’Î¦âˆ’1
Î² (q) is increasing on R+. Thus with
probability at least 1 âˆ’Ïµ, for any posterior distribution Ï,
Ï(R) â‰¤
inf
Î»âˆˆ(1,âˆž( Î¦âˆ’1
Î»
N

Ï(r) + Î±
Î»

K(Ï, Ï€) âˆ’log(Ïµ) + 2 log

log(Î±2Î»)
log(Î±)

=
inf
Î»âˆˆ(1,âˆž(
1 âˆ’exp

âˆ’Î»
N Ï(r) âˆ’Î±
N

K(Ï, Ï€) âˆ’log(Ïµ) + 2 log

log(Î±2Î»)
log(Î±)

1 âˆ’exp(âˆ’Î»
N )
.
Taking the approximately optimal value
Î» =
$
2NÎ± [K(Ï, Ï€) âˆ’log(Ïµ)]
Ï(r)[1 âˆ’Ï(r)]
,
we obtain
Theorem 1.2.8. With probability 1 âˆ’Ïµ, for any posterior distribution Ï : Î© â†’
M1
+(Î˜), putting d(Ï, Ïµ) = K(Ï, Ï€) âˆ’log(Ïµ),
Ï(R) â‰¤inf
kâˆˆN
1 âˆ’exp

âˆ’Î±k
N Ï(r) âˆ’1
N

d(Ï, Ïµ) + log

(k + 1)(k + 2)

1 âˆ’exp
 
âˆ’Î±k
N
!
â‰¤
1 âˆ’exp
âŽ§
âŽ¨
âŽ©âˆ’
$
2Î±Ï(r)d(Ï, Ïµ)
N[1 âˆ’Ï(r)] âˆ’Î±
N
&
d(Ï, Ïµ) + 2 log
 log

Î±2%
2NÎ±d(Ï,Ïµ)
Ï(r)[1âˆ’Ï(r)]

log(Î±)
!'âŽ«
âŽ¬
âŽ­
1 âˆ’exp
&
âˆ’
$
2Î±d(Ï, Ïµ)
NÏ(r)[1 âˆ’Ï(r)]
'
.
Moreover with probability at least 1 âˆ’Ïµ, for any posterior distribution Ï such that
Ï(r) = 0,
Ï(R) â‰¤1 âˆ’exp

âˆ’K(Ï, Ï€) âˆ’log(Ïµ)
N

.
We can also elaborate on the results in an other direction by introducing the
empirical dimension
(1.6)
de = sup
Î²âˆˆR+
Î²

Ï€exp(âˆ’Î²r)(r) âˆ’ess inf
Ï€ r

â‰¤âˆ’log

Ï€

r = ess inf
Ï€ r

.
There is no need to introduce a margin in this deï¬nition, since r takes at most N
values, and therefore Ï€

r = ess infÏ€ r

is strictly positive. This leads to
Corollary 1.2.9. For any positive real constant Î», with P probability at least
1 âˆ’Ïµ, for any posterior distribution Ï : Î© â†’M1
+(Î˜),
Ï(R) â‰¤Î¦âˆ’1
Î»
N
&
ess inf
Ï€ r + de
Î» log
 eÎ»
de
!
+ K

Ï, Ï€exp(âˆ’Î»r)

âˆ’log(Ïµ)
Î»
'
.
We could then make the bound uniform in Î» and optimize this parameter in a
way similar to what was done to obtain Theorem 1.2.8.

14
Chapter 1.
Inductive PAC-Bayesian learning
1.3. Local bounds
In this section, better bounds will be achieved through a better choice of the prior
distribution. This better prior distribution turns out to depend on the unknown
sample distribution P, and some work is required to circumvent this and obtain
empirical bounds.
1.3.1. Choice of the prior.
As mentioned in the introduction, if one is willing
to minimize the bound in expectation provided by Theorem 1.2.1 (page 6), one is led
to consider the optimal choice Ï€ = P(Ï). However, this is only an ideal choice, since
P is in all conceivable situations unknown. Nevertheless it shows that it is possible
through Theorem 1.2.1 to measure the complexity of the classiï¬cation model with
P

K

Ï, P(Ï)

, which is nothing but the mutual information between the random
sample (Xi, Yi)N
i=1 and the estimated parameter Ë†Î¸, under the joint distribution PÏ.
In practice, since we cannot choose Ï€ = P(Ï), we have to be content with a
ï¬‚at prior Ï€, resulting in a bound measuring complexity according to P

K(Ï, Ï€)

=
P

K

Ï, P(Ï)

+K

P(Ï), Ï€

larger by the entropy factor K

P(Ï), Ï€

than the optimal
one (we are still commenting on Theorem 1.2.1).
If we want to base the choice of Ï€ on Theorem 1.2.4 (page 10), and if we choose
Ï = Ï€exp(âˆ’Î»r) to optimize this bound, we will be inclined to choose some Ï€ such
that
1
Î»
 Î»
0 Ï€exp(âˆ’Î²R)(R)dÎ² = âˆ’1
Î» log

Ï€

exp(âˆ’Î»R)

is as far as possible close to infÎ¸âˆˆÎ˜ R(Î¸) in all circumstances. To give a more speciï¬c
example, in the case when the distribution of the design (Xi)N
i=1 is known, one can
introduce on the parameter space Î˜ the metric D already deï¬ned by equation
(1.3, page 7) (or some available upper bound for this distance). In view of the fact
that R(Î¸) âˆ’R(Î¸â€²) â‰¤D(Î¸, Î¸â€²), for any Î¸, Î¸â€² âˆˆÎ˜, it can be meaningful, at least
theoretically, to choose Ï€ as
Ï€ =
âˆž

k=1
1
k(k + 1)Ï€k,
where Ï€k is the uniform measure on some minimal (or close to minimal) 2âˆ’k-net
N(Î˜, D, 2âˆ’k) of the metric space (Î˜, D). With this choice
âˆ’1
Î» log

Ï€

exp(âˆ’Î»R)

â‰¤inf
Î¸âˆˆÎ˜ R(Î¸)
+ inf
k

2âˆ’k + log(|N(Î˜, D, 2âˆ’k)|) + log[k(k + 1)]
Î»

.
Another possibility, when we have to deal with real valued parameters, meaning
that Î˜ âŠ‚Rd, is to code each real component Î¸i âˆˆR of Î¸ = (Î¸i)d
i=1 to some
precision and to use a prior Î¼ which is atomic on dyadic numbers. More precisely
let us parametrize the set of dyadic real numbers as
D =

r

s, m, p, (bj)p
j=1

= s2m
 
1 +
p

j=1
bj2âˆ’j
!
: s âˆˆ{âˆ’1, +1}, m âˆˆZ, p âˆˆN, bj âˆˆ{0, 1}

,

1.3.
Local bounds
15
where, as can be seen, s codes the sign, m the order of magnitude, p the precision
and (bj)p
j=1 the binary representation of the dyadic number r

s, m, p, (bj)p
j=1

. We
can for instance consider on D the probability distribution
(1.7)
Î¼

r

s, m, p, (bj)p
j=1

=

3(|m| + 1)(|m| + 2)(p + 1)(p + 2)2pâˆ’1
,
and deï¬ne Ï€ âˆˆM1
+(Rd) as Ï€ = Î¼âŠ—d. This kind of â€œcodingâ€ prior distribution can
be used also to deï¬ne a prior on the integers (by renormalizing the restriction of
Î¼ to integers to get a probability distribution). Using Î¼ is somehow equivalent to
picking up a representative of each dyadic interval, and makes it possible to restrict
to the case when the posterior Ï is a Dirac mass without losing too much (when
Î˜ = (0, 1), this approach is somewhat equivalent to considering as prior distribution
the Lebesgue measure and using as posterior distributions the uniform probabil-
ity measures on dyadic intervals, with the advantage of obtaining non-randomized
estimators). When one uses in this way an atomic prior and Dirac masses as pos-
terior distributions, the bounds proven so far can be obtained through a simpler
union bound argument. This is so true that some of the detractors of the PAC-
Bayesian approach (which, as a newcomer, has sometimes received a suspicious
greeting among statisticians) have argued that it cannot bring anything that ele-
mentary union bound arguments could not essentially provide. We do not share of
course this derogatory opinion, and while we think that allowing for non atomic
priors and posteriors is worthwhile, we also would like to stress that the upcoming
local and relative bounds could hardly be obtained with the only help of union
bounds.
Although the choice of a ï¬‚at prior seems at ï¬rst glance to be the only alternative
when nothing is known about the sample distribution P, the previous discussion
shows that this type of choice is lacking proper localisation, and namely that we
loose a factor K

P

Ï€exp(âˆ’Î»r)

, Ï€

, the divergence between the bound-optimal prior
P

Ï€exp(âˆ’Î»r)

, which is concentrated near the minima of R in favourable situations,
and the ï¬‚at prior Ï€. Fortunately, there are technical ways to get around this diï¬ƒ-
culty and to obtain more local empirical bounds.
1.3.2. Unbiased local empirical bounds.
The idea is to start with some ï¬‚at
prior Ï€ âˆˆM1
+(Î˜), and the posterior distribution Ï = Ï€exp(âˆ’Î»r) minimizing the
bound of Theorem 1.2.1 (page 6), when Ï€ is used as a prior. To improve the bound,
we would like to use P

Ï€exp(âˆ’Î»r)

instead of Ï€, and we are going to make the guess
that we could approximate it with Ï€exp(âˆ’Î²R) (we have replaced the parameter Î»
with some distinct parameter Î² to give some more freedom to our investigation,
and also because, intuitively, P

Ï€exp(âˆ’Î»r)

may be expected to be less concentrated
than each of the Ï€exp(âˆ’Î»r) it is mixing, which suggests that the best approximation
of P

Ï€exp(âˆ’Î»r)

by some Ï€exp(âˆ’Î²R) may be obtained for some parameter Î² < Î»).
We are then led to look for some empirical upper bound of K

Ï, Ï€exp(âˆ’Î²R)

. This
is happily provided by the following computation
P

K

Ï, Ï€exp(âˆ’Î²R)

= P

K(Ï, Ï€)

+ Î²P

Ï(R)

+ log

Ï€

exp(âˆ’Î²R)

= P

K

Ï, Ï€exp(âˆ’Î²r)

+ Î²P

Ï(R âˆ’r)

+ log

Ï€

exp(âˆ’Î²R)

âˆ’P

log Ï€

exp(âˆ’Î²r)

.

16
Chapter 1.
Inductive PAC-Bayesian learning
Using the convexity of r â†’log

Ï€

exp(âˆ’Î²r)

as in equation (1.4) on page 10, we
conclude that
0 â‰¤P

K

Ï, Ï€exp(âˆ’Î²R)

â‰¤Î²P

Ï(R âˆ’r)

+ P

K

Ï, Ï€exp(âˆ’Î²r)

.
This inequality has an interest of its own, since it provides a lower bound for
P

Ï(R)

. Moreover we can plug it into Theorem 1.2.1 (page 6) applied to the prior
distribution Ï€exp(âˆ’Î²R) and obtain for any posterior distribution Ï and any positive
parameter Î» that
Î¦ Î»
N

P

Ï(R)

â‰¤P

Ï(r) + Î²
Î»Ï(R âˆ’r) + 1
Î»P

K

Ï, Ï€exp(âˆ’Î²r)

.
In view of this, it it convenient to introduce the function
Î¦a,b(p) = (1 âˆ’b)âˆ’1
Î¦a(p) âˆ’bp

= âˆ’(1 âˆ’b)âˆ’1
aâˆ’1 log

1 âˆ’p

1 âˆ’exp(âˆ’a)

+ bp

,
p âˆˆ(0, 1), a âˆˆ)0, âˆž(, b âˆˆ(0, 1(.
This is a convex function of p, moreover
Î¦â€²
a,b(0) =

aâˆ’1
1 âˆ’exp(âˆ’a)

âˆ’b

(1 âˆ’b)âˆ’1,
showing that it is an increasing one to one convex map of the unit interval unto
itself as soon as b â‰¤aâˆ’1
1 âˆ’exp(âˆ’a)

. Its convexity, combined with the value of
its derivative at the origin, shows that
Î¦a,b(p) â‰¥aâˆ’1
1 âˆ’exp(âˆ’a)

âˆ’b
1 âˆ’b
p.
Using this notation and remarks, we can state
Theorem 1.3.1. For any positive real constants Î² and Î» such that 0 â‰¤Î² < N[1âˆ’
exp(âˆ’Î»
N )], for any posterior distribution Ï : Î© â†’M1
+(Î˜),
P

Ï(r) âˆ’K

Ï, Ï€exp(âˆ’Î²r)

Î²

â‰¤P

Ï(R)

â‰¤Î¦âˆ’1
Î»
N , Î²
Î»

P

Ï(r) + K

Ï, Ï€exp(âˆ’Î²r)

Î» âˆ’Î²

â‰¤
Î» âˆ’Î²
N[1 âˆ’exp(âˆ’Î»
N )] âˆ’Î² P

Ï(r) + K

Ï, Ï€exp(âˆ’Î²r)

Î» âˆ’Î²

.
Thus (taking Î» = 2Î²), for any Î² such that 0 â‰¤Î² < N
2 ,
P

Ï(R)

â‰¤
1
1 âˆ’2Î²
N
P

Ï(r) + K

Ï, Ï€exp(âˆ’Î²r)

Î²

.
Note that the last inequality is obtained using the fact that 1âˆ’exp(âˆ’x) â‰¥xâˆ’x2
2 ,
x âˆˆR+.

1.3.
Local bounds
17
Corollary 1.3.2. For any Î² âˆˆ(0, N(,
P

Ï€exp(âˆ’Î²r)(r)

â‰¤P

Ï€exp(âˆ’Î²r)(R)

â‰¤
inf
Î»âˆˆ(âˆ’N log(1âˆ’Î²
N ),âˆž(
Î» âˆ’Î²
N[1 âˆ’exp(âˆ’Î»
N )] âˆ’Î² P

Ï€exp(âˆ’Î²r)(r)

â‰¤
1
1 âˆ’2Î²
N
P

Ï€exp(âˆ’Î²r)(r)

,
the last inequality holding only when Î² < N
2 .
It is interesting to compare the upper bound provided by this corollary with
Theorem 1.2.1 (page 6) when the posterior is a Gibbs measure Ï = Ï€exp(âˆ’Î²r). We
see that we have got rid of the entropy term K

Ï€exp(âˆ’Î²r), Ï€

, but at the price of
an increase of the multiplicative factor, which for small values of
Î²
N grows from
(1 âˆ’
Î²
2N )âˆ’1 (when we take Î» = Î² in Theorem 1.2.1), to (1 âˆ’2Î²
N )âˆ’1. Therefore
non-localized bounds have an interest of their own, and are superseded by localized
bounds only in favourable circumstances (presumably when the sample is large
enough when compared with the complexity of the classiï¬cation model).
Corollary 1.3.2 shows that when 2Î²
N is small, Ï€exp(âˆ’Î²r)(r) is a tight approximation
of Ï€exp(âˆ’Î²r)(R) in the mean (since we have an upper bound and a lower bound which
are close together).
Another corollary is obtained by optimizing the bound given by Theorem 1.3.1
in Ï, which is done by taking Ï = Ï€exp(âˆ’Î»r).
Corollary 1.3.3. For any positive real constants Î² and Î» such that 0 â‰¤Î² <
N[1 âˆ’exp(âˆ’Î»
N )],
P

Ï€exp(âˆ’Î»r)(R)

â‰¤Î¦âˆ’1
Î»
N , Î²
Î»

P

1
Î» âˆ’Î²
 Î»
Î²
Ï€exp(âˆ’Î³r)(r)dÎ³

â‰¤
1
N[1 âˆ’exp(âˆ’Î»
N )] âˆ’Î² P
 Î»
Î² Ï€exp(âˆ’Î³r)(r)dÎ³

.
Although this inequality gives by construction a better upper bound for
infÎ»âˆˆR+ P

Ï€exp(âˆ’Î»r)(R)

than Corollary 1.3.2, it is not easy to tell which one of
the two inequalities is the best to bound P

Ï€exp(âˆ’Î»r)(R)

for a ï¬xed (and possibly
suboptimal) value of Î», because in this case, one factor is improved while the other
is worsened.
Using the empirical dimension de deï¬ned by equation (1.6) on page 13, we see
that
1
Î» âˆ’Î²
 Î»
Î²
Ï€exp(âˆ’Î³r)(r)dÎ³ â‰¤ess inf
Ï€ r + de log
 Î»
Î²
!
.
Therefore, in the case when we keep the ratio Î»
Î² bounded, we get a better depen-
dence on the empirical dimension de than in Corollary 1.2.9 (page 13).
1.3.3. Non random local bounds.
Let us come now to the localization of the
non-random upper bound given by Theorem 1.2.4 (page 10). According to Theorem
1.2.1 (page 6) applied to the localized prior Ï€exp(âˆ’Î²R),

18
Chapter 1.
Inductive PAC-Bayesian learning
Î»Î¦ Î»
N

P

Ï(R)

â‰¤P

Î»Ï(r) + K(Ï, Ï€) + Î²Ï(R)

+ log

Ï€

exp(âˆ’Î²R)

= P

K

Ï, Ï€exp(âˆ’Î»r)

âˆ’log

Ï€

exp(âˆ’Î»r)

+ Î²Ï(R)

+ log

Ï€

exp(âˆ’Î²R)

â‰¤P

K

Ï, Ï€exp(âˆ’Î»r)

+ Î²Ï(R)

âˆ’log

Ï€

exp(âˆ’Î»R)

+ log

Ï€

exp(âˆ’Î²R)

,
where we have used as previously inequality (1.4) (page 10). This proves
Theorem 1.3.4. For any posterior distribution Ï : Î© â†’M1
+(Î˜), for any real
parameters Î² and Î» such that 0 â‰¤Î² < N

1 âˆ’exp(âˆ’Î»
N )

,
P

Ï(R)

â‰¤Î¦âˆ’1
Î»
N , Î²
Î»

1
Î» âˆ’Î²
 Î»
Î²
Ï€exp(âˆ’Î³R)(R)dÎ³ + P
K

Ï, Ï€exp(âˆ’Î»r)

Î» âˆ’Î²

â‰¤
1
N

1 âˆ’exp(âˆ’Î»
N )

âˆ’Î²
 Î»
Î²
Ï€exp(âˆ’Î³R)(R)dÎ³ + P

K

Ï, Ï€exp(âˆ’Î»r)

.
Let us notice in particular that this theorem contains Theorem 1.2.4 (page 10)
which corresponds to the case Î² = 0. As a corollary, we see also, taking Ï = Ï€exp(âˆ’Î»r)
and Î» = 2Î², and noticing that Î³ â†’Ï€exp(âˆ’Î³R)(R) is decreasing, that
P

Ï€exp(âˆ’Î»r)(R)

â‰¤
inf
Î²,Î²<N[1âˆ’exp(âˆ’Î»
N )]
Î²
N

1 âˆ’exp(âˆ’Î»
N )

âˆ’Î² Ï€exp(âˆ’Î²R)(R)
â‰¤
1
1 âˆ’Î»
N
Ï€exp(âˆ’Î»
2 R)(R).
We can use this inequality in conjunction with the notion of dimension with margin
Î· introduced by equation (1.5) on page 10, to see that the Gibbs posterior achieves
for a proper choice of Î» and any margin parameter Î· â‰¥0 (which can be chosen to
be equal to zero in parametric situations)
(1.8)
inf
Î» P

Ï€exp(âˆ’Î»r)(R)

â‰¤ess inf
Ï€ R + Î· + 4dÎ·
N
+ 2
$
2dÎ·

ess infÏ€ R + Î·

N
+ 4d2Î·
N 2 .
Deviation bounds to come next will show that the optimal Î» can be estimated from
empirical data.
Let us propose a little numerical example as an illustration: assuming that
d0 = 10, N = 1000 and ess infÏ€ R = 0.2, we obtain from equation (1.8) that
infÎ» P

Ï€exp(âˆ’Î»r)(R)

â‰¤0.373.
1.3.4. Local deviation bounds.
When it comes to deviation bounds, for tech-
nical reasons we will choose a slightly more involved change of prior distribution
and apply Theorem 1.2.6 (page 11) to the prior Ï€exp[âˆ’Î²Î¦âˆ’Î²
N
â—¦R]. The advantage of
tweaking R with the nonlinear function Î¦âˆ’Î²
N will appear in the search for an em-
pirical upper bound of the local entropy term. Theorem 1.1.4 (page 4), used with
the above-mentioned local prior, shows that
(1.9)
P

sup
ÏâˆˆM1
+(Î˜)
Î»

Ï

Î¦ Î»
N â—¦R

âˆ’Ï(r)

âˆ’K

Ï, Ï€exp(âˆ’Î²Î¦âˆ’Î²
N
â—¦R)


â‰¤1.

1.3.
Local bounds
19
Moreover
(1.10)
K

Ï, Ï€exp[âˆ’Î²Î¦âˆ’Î²
N
â—¦R]

= K

Ï, Ï€exp(âˆ’Î²r)

+ Î²Ï

Î¦âˆ’Î²
N â—¦R âˆ’r

+ log

Ï€

exp

âˆ’Î²Î¦âˆ’Î²
N â—¦R

âˆ’log

Ï€

exp(âˆ’Î²r)

,
which is an invitation to ï¬nd an upper bound for log

Ï€

exp

âˆ’Î²Î¦âˆ’Î»
N â—¦R

âˆ’
log

Ï€

exp(âˆ’Î²r)

. For conciseness, let us call our localized prior distribution Ï€,
thus deï¬ned by its density
dÏ€
dÏ€ (Î¸) =
exp

âˆ’Î²Î¦âˆ’Î²
N

R(Î¸)

Ï€

exp

âˆ’Î²Î¦âˆ’Î²
N â—¦R
.
Applying once again Theorem 1.1.4 (page 4), but this time to âˆ’Î², we see that
(1.11)
P

exp

log

Ï€

exp

âˆ’Î²Î¦âˆ’Î²
N â—¦R

âˆ’log

Ï€

exp(âˆ’Î²r)

= P

exp

log

Ï€

exp

âˆ’Î²Î¦âˆ’Î²
N â—¦R)

+
inf
ÏâˆˆM1
+(Î˜) Î²Ï(r) + K(Ï, Ï€)

â‰¤P

exp

log

Ï€

exp

âˆ’Î²Î¦âˆ’Î²
N â—¦R)

+ Î²Ï€(r) + K(Ï€, Ï€)

= P

exp

Î²

Ï€(r) âˆ’Ï€

Î¦âˆ’Î²
N â—¦R

+ K(Ï€, Ï€)

â‰¤1.
Combining equations (1.10) and (1.11) and using the concavity of Î¦âˆ’Î²
N , we see that
with P probability at least 1 âˆ’Ïµ, for any posterior distribution Ï : Î© â†’M1
+(Î˜),
0 â‰¤K(Ï, Ï€) â‰¤K

Ï, Ï€exp(âˆ’Î²r)

+ Î²

Î¦âˆ’Î²
N

Ï(R)

âˆ’Ï(r)

âˆ’log(Ïµ).
We have proved a lower deviation bound:
Theorem 1.3.5 For any positive real constant Î², with P probability at least 1 âˆ’Ïµ,
for any posterior distribution Ï : Î© â†’M1
+(Î˜),
exp
 Î²
N

Ï(r) âˆ’K[Ï, Ï€exp(âˆ’Î²r)] âˆ’log(Ïµ)
Î²

âˆ’1
exp

 Î²
N

âˆ’1
â‰¤Ï(R).
We can also obtain a lower deviation bound for 	Î¸. Indeed equation (1.11) can
also be written as
P

Ï€exp(âˆ’Î²r)

exp

Î²

r âˆ’Î¦âˆ’Î²
N â—¦R

â‰¤1.
This means that for any posterior distribution Ï : Î© â†’M1
+(Î˜),
P

Ï

exp

Î²

r âˆ’Î¦âˆ’Î²
N â—¦R

âˆ’log

dÏ
dÏ€exp(âˆ’Î²r)

â‰¤1.
We have proved

20
Chapter 1.
Inductive PAC-Bayesian learning
Theorem 1.3.6 For any positive real constant Î², for any posterior distribution
Ï : Î© â†’M1
+(Î˜), with PÏ probability at least 1 âˆ’Ïµ,
R(	Î¸ ) â‰¥Î¦âˆ’1
âˆ’Î²
N

r(	Î¸ ) âˆ’
log

dÏ
dÏ€exp(âˆ’Î²r)

âˆ’log(Ïµ)
Î²

=
exp
 Î²
N

r(	Î¸ ) âˆ’
log

dÏ
dÏ€exp(âˆ’Î²r)

âˆ’log(Ïµ)
Î²

âˆ’1
exp
 Î²
N
!
âˆ’1
.
Let us now resume our investigation of the upper deviations of Ï(R). Using the
Cauchy-Schwarz inequality to combine equations (1.9, page 18) and (1.11, page 19),
we obtain
(1.12)
P

exp
1
2
sup
ÏâˆˆM1
+(Î˜)
Î»Ï

Î¦ Î»
N â—¦R

âˆ’Î²Ï

Î¦âˆ’Î²
N â—¦R

âˆ’(Î» âˆ’Î²)Ï(r) âˆ’K

Ï, Ï€exp(âˆ’Î²r)

= P

exp

1
2
sup
ÏâˆˆM1
+(Î˜)
 
Î»

Ï

Î¦ Î»
N â—¦R

âˆ’Ï(r)

âˆ’K(Ï, Ï€)
!
Ã— exp

1
2
 
log

Ï€

exp

âˆ’Î²Î¦âˆ’Î²
N â—¦R

âˆ’log

Ï€

exp(âˆ’Î²r)
!
â‰¤P

exp

sup
ÏâˆˆM1
+(Î˜)
 
Î»

Ï

Î¦ Î»
N â—¦R

âˆ’Ï(r)

âˆ’K(Ï, Ï€)
!1/2
Ã— P

exp
 
log

Ï€

exp

âˆ’Î²Î¦âˆ’Î²
N â—¦R

âˆ’log

Ï€

exp(âˆ’Î²r)
!1/2
â‰¤1.
Thus with P probability at least 1 âˆ’Ïµ, for any posterior distribution Ï,
Î»Î¦ Î»
N

Ï(R)

âˆ’Î²Î¦âˆ’Î²
N

Ï(R)

â‰¤Î»Ï

Î¦ Î»
N â—¦R

âˆ’Î²Ï

Î¦âˆ’Î²
N â—¦R

â‰¤(Î» âˆ’Î²)Ï(r) + K(Ï, Ï€exp(âˆ’Î²r)) âˆ’2 log(Ïµ).
(It would have been more straightforward to use a union bound on deviation in-
equalities instead of the Cauchy-Schwarz inequality on exponential moments, any-
how, this would have led to replace âˆ’2 log(Ïµ) with the worse factor 2 log( 2
Ïµ ).) Let
us now recall that
Î»Î¦ Î»
N (p) âˆ’Î²Î¦âˆ’Î²
N (p) = âˆ’N log

1 âˆ’

1 âˆ’exp

âˆ’Î»
N

p

âˆ’N log

1 +

exp

 Î²
N

âˆ’1

p

,
and let us put
B = (Î» âˆ’Î²)Ï(r) + K

Ï, Ï€exp(âˆ’Î²r)

âˆ’2 log(Ïµ)
= K

Ï, Ï€exp(âˆ’Î»r)

+
 Î»
Î² Ï€exp(âˆ’Î¾r)(r)dÎ¾ âˆ’2 log(Ïµ).
Let us consider moreover the change of variables Î± = 1âˆ’exp(âˆ’Î»
N ) and Î³ = exp( Î²
N )âˆ’
1. We obtain

1 âˆ’Î±Ï(R)

1 + Î³Ï(R)

â‰¥exp(âˆ’B
N ), leading to

1.3.
Local bounds
21
Theorem 1.3.7. For any positive constants Î±, Î³, such that 0 â‰¤Î³ < Î± < 1, with P
probability at least 1 âˆ’Ïµ, for any posterior distribution Ï : Î© â†’M1
+(Î˜), the bound
M(Ï) = âˆ’log

(1 âˆ’Î±)(1 + Î³)

Î± âˆ’Î³
Ï(r) + K(Ï, Ï€exp[âˆ’N log(1+Î³)r]) âˆ’2 log(Ïµ)
N(Î± âˆ’Î³)
=
K

Ï, Ï€exp[N log(1âˆ’Î±)r]

+
 âˆ’N log(1âˆ’Î±)
N log(1+Î³)
Ï€exp(âˆ’Î¾r)(r)dÎ¾ âˆ’2 log(Ïµ)
N(Î± âˆ’Î³)
,
is such that
Ï(R) â‰¤Î± âˆ’Î³
2Î±Î³
"$
1 +
4Î±Î³
(Î± âˆ’Î³)2

1 âˆ’exp

âˆ’(Î± âˆ’Î³)M(Ï)

âˆ’1
#
â‰¤M(Ï),
Let us now give an upper bound for R(	Î¸ ). Equation (1.12 page 20) can also be
written as
P

Ï€exp(âˆ’Î²r)

exp

Î»Î¦ Î»
N â—¦R âˆ’Î²Î¦âˆ’Î²
N â—¦R âˆ’(Î» âˆ’Î²)r
 1
2 
â‰¤1.
This means that for any posterior distribution Ï : Î© â†’M1
+(Î˜),
P

Ï

exp

Î»Î¦ Î»
N â—¦R âˆ’Î²Î¦âˆ’Î²
N â—¦R âˆ’(Î» âˆ’Î²)r âˆ’log

dÏ
dÏ€exp(âˆ’Î²r)
 1
2 
â‰¤1.
Using the concavity of the square root function, this inequality can be weakened
to
P

Ï

exp

1
2

Î»Î¦ Î»
N â—¦R âˆ’Î²Î¦âˆ’Î²
N â—¦R âˆ’(Î» âˆ’Î²)r âˆ’log

dÏ
dÏ€exp(âˆ’Î²r)

â‰¤1.
We have proved
Theorem 1.3.8.
For any positive real constants Î» and Î² and for any posterior
distribution Ï : Î© â†’M1
+(Î˜), with PÏ probability at least 1 âˆ’Ïµ,
Î»Î¦ Î»
N

R(	Î¸ )

âˆ’Î²Î¦âˆ’Î²
N

R(	Î¸ )

â‰¤(Î» âˆ’Î²) r(	Î¸ ) + log

dÏ
dÏ€exp(âˆ’Î²r) (	Î¸ )

âˆ’2 log(Ïµ).
Putting Î± = 1 âˆ’exp

âˆ’Î»
N

, Î³ = exp

 Î²
N

âˆ’1 and
M(Î¸) = âˆ’log

(1 âˆ’Î±)(1 + Î³)

Î± âˆ’Î³
r(Î¸) +
log

dÏ
dÏ€exp[âˆ’N log(1+Î³)r] (Î¸)

âˆ’2 log(Ïµ)
N(Î± âˆ’Î³)
=
log

dÏ
dÏ€exp[N log(1âˆ’Î±)r] (Î¸)

+
 âˆ’N log(1âˆ’Î±)
N log(1+Î³)
Ï€exp(âˆ’Î¾r)(r) dÎ¾ âˆ’2 log(Ïµ)
N(Î± âˆ’Î³)
,
we can also, in the case when Î³ < Î±, write this inequality as
R(	Î¸ ) â‰¤Î± âˆ’Î³
2Î±Î³
"$
1 +
4Î±Î³
(Î± âˆ’Î³)2

1 âˆ’exp

âˆ’(Î± âˆ’Î³)M(	Î¸ )

âˆ’1
#
â‰¤M(	Î¸).

22
Chapter 1.
Inductive PAC-Bayesian learning
It may be enlightening to introduce the empirical dimension de deï¬ned by equa-
tion (1.6) on page 13. It provides the upper bound
 Î»
Î²
Ï€exp(âˆ’Î¾r)(r)dÎ¾ â‰¤(Î» âˆ’Î²) ess inf
Ï€ r + de log
 Î»
Î²
!
,
which shows that in Theorem 1.3.7 (page 21),
M(Ï) â‰¤log

(1 + Î³)(1 âˆ’Î±)

Î³ âˆ’Î±
ess inf
Ï€ r
+
de log

âˆ’log(1âˆ’Î±)
log(1+Î³)

+ K

Ï, Ï€exp[N log(1âˆ’Î±)r]

âˆ’2 log(Ïµ)
N(Î± âˆ’Î³)
.
Similarly, in Theorem 1.3.8 above,
M(Î¸) â‰¤log

(1 + Î³)(1 âˆ’Î±)

Î³ âˆ’Î±
ess inf
Ï€ r
+
de log

âˆ’log(1âˆ’Î±)
log(1+Î³)

+ log

dÏ
dÏ€exp[N log(1âˆ’Î±)r] (Î¸)

âˆ’2 log(Ïµ)
N(Î± âˆ’Î³)
Let us give a little numerical illustration: assuming that de = 10, N = 1000, and
ess infÏ€ r = 0.2, taking Ïµ = 0.01, Î± = 0.5 and Î³ = 0.1, we obtain from Theorem
1.3.7 Ï€exp[N log(1âˆ’Î±)r](R) â‰ƒÏ€exp(âˆ’693r)(R) â‰¤0.332 â‰¤0.372, where we have given
respectively the non-linear and the linear bound. This shows the practical interest
of keeping the non-linearity. Optimizing the values of the parameters Î± and Î³ would
not have yielded a signiï¬cantly lower bound.
The following corollary is obtained by taking Î» = 2Î² and keeping only the linear
bound; we give it for the sake of its simplicity:
Corollary 1.3.9. For any positive real constant Î² such that exp( Î²
N )+exp(âˆ’2Î²
N ) <
2, which is the case when Î² < 0.48N, with P probability at least 1 âˆ’Ïµ, for any pos-
terior distribution Ï : Î© â†’M1
+(Î˜),
Ï(R) â‰¤Î²Ï(r) + K

Ï, Ï€exp(âˆ’Î²r)

âˆ’2 log(Ïµ)
N

2 âˆ’exp

 Î²
N

âˆ’exp

âˆ’2Î²
N

=
 2Î²
Î²
Ï€exp(âˆ’Î¾r)(r)dÎ¾ + K

Ï, Ï€exp(âˆ’2Î²r)

âˆ’2 log(Ïµ)
N

2 âˆ’exp( Î²
N ) âˆ’exp(âˆ’2Î²
N )

.
Let us mention that this corollary applied to the above numerical example gives
Ï€exp(âˆ’200r)(R) â‰¤0.475 (when we take Î² = 100, consistently with the choice Î³ =
0.1).
1.3.5. Partially local bounds.
Local bounds are suitable when the lowest
values of the empirical error rate r are reached only on a small part of the parameter
set Î˜. When Î˜ is the disjoint union of sub-models of diï¬€erent complexities, the
minimum of r will as a rule not be â€œlocalizedâ€ in a way that calls for the use of
local bounds. Just think for instance of the case when Î˜ = +M
m=1 Î˜m, where the
sets Î˜1 âŠ‚Î˜2 âŠ‚Â· Â· Â· âŠ‚Î˜M are nested. In this case we will have infÎ˜1 r â‰¥infÎ˜2 r â‰¥

1.3.
Local bounds
23
Â· Â· Â· â‰¥infÎ˜M r, although Î˜M may be too large to be the right model to use. In this
situation, we do not want to localize the bound completely. Let us make a more
speciï¬c fanciful but typical pseudo computation. Just imagine we have a countable
collection (Î˜m)mâˆˆM of sub-models. Let us assume we are interested in choosing
between the estimators 	Î¸m âˆˆarg minÎ˜m r, maybe randomizing them (e.g. replacing
them with Ï€m
exp(âˆ’Î»r)). Let us imagine moreover that we are in a typically parametric
situation, where, for some priors Ï€m âˆˆM1
+(Î˜m), m âˆˆM, there is a â€œdimensionâ€
dm such that Î»

Ï€m
exp(âˆ’Î»r)(r) âˆ’r(	Î¸m)

â‰ƒdm. Let Î¼ âˆˆM1
+(M) be some distribution
on the index set M. It is easy to see that (Î¼Ï€)exp(âˆ’Î»r) will typically not be properly
local, in the sense that typically
(Î¼Ï€)exp(âˆ’Î»r)(r) =
Î¼

Ï€exp(âˆ’Î»r)(r)Ï€

exp(âˆ’Î»r)

Î¼

Ï€

exp(âˆ’Î»r)

â‰ƒ

mâˆˆM

(inf
Î˜m r) + dm
Î»

exp

âˆ’Î»(inf
Î˜m r) âˆ’dm log

 eÎ»
dm

Î¼(m)

mâˆˆM
exp

âˆ’Î»(inf
Î˜m r) âˆ’dm log

 eÎ»
dm

Î¼(m)
â‰ƒ

inf
mâˆˆM(inf
Î˜m r) + dm
Î» log

 eÎ»
dm

âˆ’1
Î» log[Î¼(m)]

+ log
 
mâˆˆM
exp

âˆ’dm log( Î»
dm )

Î¼(m)

.
where we have used the approximations
âˆ’log

Ï€

exp(âˆ’Î»r)

=
 Î»
0
Ï€exp(âˆ’Î²r)(r)dÎ²
â‰ƒ
 Î»
0
(inf
Î˜m r) +
 dm
Î² âˆ§1

dÎ² â‰ƒÎ»(inf
Î˜m r) + dm

log

 Î»
dm

+ 1

,
and

m h(m) exp[âˆ’h(m)]Î½(m)

m exp[âˆ’h(m)]Î½(m)
â‰ƒinf
m h(m)âˆ’log[Î½(m)], Î½ âˆˆM1
+(M), taking Î½(m) =
Î¼(m) exp

âˆ’dm log

 Î»
dm


mâ€² Î¼(mâ€²) exp

âˆ’dmâ€² log

 Î»
dmâ€²
.
These approximations have no pretension to be rigorous or very accurate, but
they nevertheless give the best order of magnitude we can expect in typical situa-
tions, and show that this order of magnitude is not what we are looking for: mixing
diï¬€erent models with the help of Î¼ spoils the localization, introducing a multiplier
log

 Î»
dm

to the dimension dm which is precisely what we would have got if we had
not localized the bound at all. What we would really like to do in such situations is
to use a partially localized posterior distribution, such as Ï€	
m
exp(âˆ’Î»r), where 	m is an
estimator of the best sub-model to be used. While the most straightforward way to
do this is to use a union bound on results obtained for each sub-model Î˜m, here
we are going to show how to allow arbitrary posterior distributions on the index
set (corresponding to a randomization of the choice of 	m).
Let us consider the framework we just mentioned: let the measurable parameter
set (Î˜, T) be a union of measurable sub-models, Î˜ = 
mâˆˆM Î˜m. Let the index set

24
Chapter 1.
Inductive PAC-Bayesian learning
(M, M) be some measurable space (most of the time it will be a countable set). Let
Î¼ âˆˆM1
+(M) be a prior probability distribution on (M, M). Let Ï€ : M â†’M1
+(Î˜) be
a regular conditional probability measure such that Ï€(m, Î˜m) = 1, for any m âˆˆM.
Let Î¼Ï€ âˆˆM1
+(M Ã—Î˜) be the product probability measure deï¬ned for any bounded
measurable function h : M Ã— Î˜ â†’R by
Î¼Ï€(h) =

mâˆˆM
 
Î¸âˆˆÎ˜
h(m, Î¸)Ï€(m, dÎ¸)
!
Î¼(dm).
For any bounded measurable function h : Î© Ã— M Ã— Î˜ â†’R, let Ï€exp(h) : Î© Ã— M â†’
M1
+(Î˜) be the regular conditional posterior probability measure deï¬ned by
dÏ€exp(h)
dÏ€
(m, Î¸) = exp

h(m, Î¸)

Ï€

m, exp(h)
,
where consistently with previous notation Ï€(m, h) =

Î˜ h(m, Î¸)Ï€(m, dÎ¸) (we will
also often use the less explicit notation Ï€(h)). For short, let
U(Î¸, Ï‰) = Î»Î¦ Î»
N

R(Î¸)

âˆ’Î²Î¦âˆ’Î²
N

R(Î¸)

âˆ’(Î» âˆ’Î²)r(Î¸, Ï‰).
Integrating with respect to Î¼ equation (1.12, page 20), written in each sub-model
Î˜m using the prior distribution Ï€(m, Â·), we see that
P

exp

sup
Î½âˆˆM1
+(M)
sup
Ï:Mâ†’M1
+(Î˜)
1
2

(Î½Ï)(U) âˆ’Î½

K(

Ï, Ï€exp(âˆ’Î²r)

âˆ’K(Î½, Î¼)

â‰¤P

exp

sup
Î½âˆˆM1
+(M)
1
2Î½
 
sup
Ï:Mâ†’M1
+(Î˜)
Ï(U) âˆ’K(Ï, Ï€exp(âˆ’Î²r))
!
âˆ’K(Î½, Î¼)

= P

Î¼

exp

1
2
sup
Ï:Mâ†’M1
+(Î˜)

Ï(U) âˆ’K

Ï, Ï€exp(âˆ’Î²r)

= Î¼

P

exp

1
2
sup
Ï:Mâ†’M1
+(Î˜)

Ï(U) âˆ’K

Ï, Ï€exp(âˆ’Î²r)

â‰¤1.
This proves that
(1.13)
P

exp
&
1
2
sup
Î½âˆˆM1
+(M)
sup
Ï:Mâ†’M1
+(Î˜)
Î½Ï

Î»Î¦ Î»
N (R) âˆ’Î²Î¦âˆ’Î²
N (R)

âˆ’(Î» âˆ’Î²)Î½Ï(r) âˆ’2K(Î½, Î¼) âˆ’Î½

K

Ï, Ï€exp(âˆ’Î²r)

'
â‰¤1.
Introducing the optimal value of r on each sub-model râ‹†(m) = ess infÏ€(m,Â·) r and
the empirical dimensions
de(m) = sup
Î¾âˆˆR+
Î¾

Ï€exp(âˆ’Î¾r)(m, r) âˆ’râ‹†(m)

,
we can thus state
Theorem 1.3.10. For any positive real constants Î² < Î», with P probability at least
1 âˆ’Ïµ, for any posterior distribution Î½ : Î© â†’M1
+(M), for any conditional posterior
distribution Ï : Î© Ã— M â†’M1
+(Î˜),
Î½Ï

Î»Î¦ Î»
N (R) âˆ’Î²Î¦âˆ’Î²
N (R)

â‰¤Î»Î¦ Î»
N

Î½Ï(R)

âˆ’Î²Î¦âˆ’Î²
N

Î½Ï(R)

â‰¤B1(Î½, Ï),

1.3.
Local bounds
25
where B1(Î½, Ï) = (Î» âˆ’Î²)Î½Ï(r) + 2K(Î½, Î¼) + Î½

K

Ï, Ï€exp(âˆ’Î²r)

âˆ’2 log(Ïµ)
= Î½
 Î»
Î²
Ï€exp(âˆ’Î±r)(r)dÎ±

+ 2K(Î½, Î¼) + Î½

K

Ï, Ï€exp(âˆ’Î»r)

âˆ’2 log(Ïµ)
= âˆ’2 log

Î¼

exp
 
âˆ’1
2
 Î»
Î²
Ï€exp(âˆ’Î±r)(r)dÎ±
!
+ 2K

Î½, Î¼
 Ï€[exp(âˆ’Î»r)]
Ï€[exp(âˆ’Î²r)]
1/2

+ Î½

K

Ï, Ï€exp(âˆ’Î»r)

âˆ’2 log(Ïµ),
and therefore B1(Î½, Ï) â‰¤Î½

(Î» âˆ’Î²)râ‹†+ log

Î»
Î²

de

+ 2K(Î½, Î¼)
+ Î½

K

Ï, Ï€exp(âˆ’Î»r)

âˆ’2 log(Ïµ),
as well as B1(Î½, Ï) â‰¤âˆ’2 log

Î¼

exp
 
âˆ’(Î»âˆ’Î²)
2
râ‹†âˆ’1
2 log

 Î»
Î²

de
!
+ 2K

Î½, Î¼
Ï€[exp(âˆ’Î»r)]
Ï€[exp(âˆ’Î²r)]
1/2

+ Î½

K

Ï, Ï€exp(âˆ’Î»r)

âˆ’2 log(Ïµ).
Thus, for any real constants Î± and Î³ such that 0 â‰¤Î³ < Î± < 1, with P probability
at least 1 âˆ’Ïµ, for any posterior distribution Î½ : Î© â†’M1
+(M) and any conditional
posterior distribution Ï : Î© Ã— M â†’M1
+(Î˜), the bound
B2(Î½, Ï) = âˆ’
log

(1âˆ’Î±)(1+Î³)

Î±âˆ’Î³
Î½Ï(r) +
2K(Î½,Î¼)+Î½

K

Ï,Ï€(1+Î³)âˆ’Nr

âˆ’2 log(Ïµ)
N(Î±âˆ’Î³)
=
1
N(Î± âˆ’Î³)

2K

Î½, Î¼
Ï€[(1âˆ’Î±)Nr]
Ï€[(1+Î³)âˆ’Nr]
1/2

+ Î½

K

Ï, Ï€(1âˆ’Î±)Nr

âˆ’
2
N(Î± âˆ’Î³) log

Î¼
&
exp

âˆ’1
2
 âˆ’N log(1âˆ’Î±)
N log(1+Î³)
Ï€exp(âˆ’Î¾r)(Â·, r)dÎ¾
'
âˆ’
2 log(Ïµ)
N(Î± âˆ’Î³)
satisï¬es
Î½Ï(R) â‰¤Î± âˆ’Î³
2Î±Î³
"$
1 +
4Î±Î³
(Î± âˆ’Î³)2

1 âˆ’exp

âˆ’(Î± âˆ’Î³)B2(Î½, Ï)

âˆ’1
#
â‰¤B2(Î½, Ï).
If one is willing to bound the deviations with respect to PÎ½Ï, it is enough to
remark that the equation preceding equation (1.13, page 24) can also be written as
P

Î¼
&
Ï€exp(âˆ’Î²r)

exp

Î»Î¦ Î»
N â—¦R âˆ’Î²Î¦âˆ’Î²
N â—¦R âˆ’(Î» âˆ’Î²)r
1/2'
â‰¤1.
Thus for any posterior distributions Î½ : Î© â†’M1
+(M) and Ï : Î© Ã— M â†’M1
+(Î˜),
P

Î½

Ï

exp

Î»Î¦ Î»
N â—¦R âˆ’Î²Î¦âˆ’Î²
N â—¦R
âˆ’(Î» âˆ’Î²)r âˆ’2 log

 dÎ½
dÎ¼

âˆ’log

dÏ
dÏ€exp(âˆ’Î²r)
1/2
â‰¤1.

26
Chapter 1.
Inductive PAC-Bayesian learning
Using the concavity of the square root function to pull the integration with respect
to Ï out of the square root, we get
PÎ½Ï

exp
1
2

Î»Î¦ Î»
N â—¦R âˆ’Î²Î¦âˆ’Î²
N â—¦R
âˆ’(Î» âˆ’Î²)r âˆ’2 log

 dÎ½
dÏ€

âˆ’log

dÏ
dÏ€exp(âˆ’Î²r)

â‰¤1.
This leads to
Theorem 1.3.11. For any positive real constants Î² < Î», for any posterior distri-
butions Î½ : Î© â†’M1
+(M) and Ï : Î© Ã— M â†’M1
+(Î˜), with PÎ½Ï probability at least
1 âˆ’Ïµ,
Î»Î¦ Î»
N

R( 	m, 	Î¸ )

âˆ’Î²Î¦âˆ’Î²
N

R( 	m, 	Î¸ )

â‰¤(Î» âˆ’Î²)r( 	m, 	Î¸)
+ 2 log
 dÎ½
dÎ¼( 	m )

+ log

dÏ
dÏ€exp(âˆ’Î²r) ( 	m, 	Î¸ )

âˆ’2 log(Ïµ)
=
 Î»
Î²
Ï€exp(âˆ’Î±r)(r)dÎ±
+ 2 log
 dÎ½
dÎ¼( 	m)

+ log

dÏ
dÏ€exp(âˆ’Î»r) ( 	m, 	Î¸ )

âˆ’2 log(Ïµ)
= 2 log

Î¼

exp
 
âˆ’1
2
 Î»
Î²
Ï€exp(âˆ’Î±r)(r)dÎ±
!
+ 2 log

dÎ½
dÎ¼
Ï€[exp(âˆ’Î»r)]
Ï€[exp(âˆ’Î²r)]
1/2 ( 	m)

+ log

dÏ
dÏ€exp(âˆ’Î»r) ( 	m, 	Î¸ )

âˆ’2 log(Ïµ).
Another way to state the same inequality is to say that for any real constants Î± and
Î³ such that 0 â‰¤Î³ < Î± < 1, with PÎ½Ï probability at least 1 âˆ’Ïµ,
R( 	m, 	Î¸)
â‰¤Î± âˆ’Î³
2Î±Î³
 $
1 +
4Î±Î³
(Î± âˆ’Î³)2

1 âˆ’exp

âˆ’(Î± âˆ’Î³)B( 	m, 	Î¸)

âˆ’1
!
â‰¤B( 	m, 	Î¸),
where
B( 	m, 	Î¸) = âˆ’log

(1 âˆ’Î±)(1 + Î³)

Î± âˆ’Î³
r( 	m, 	Î¸)
+
2 log

dÎ½
dÎ¼( 	m)

+ log

dÏ
dÏ€(1+Î³)âˆ’Nr ( 	m, 	Î¸)

âˆ’2 log(Ïµ)
N(Î± âˆ’Î³)
=
2
N(Î± âˆ’Î³) log

dÎ½
dÎ¼
Ï€[(1âˆ’Î±)Nr]
Ï€[(1+Î³)âˆ’Nr]
1/2 ( 	m)

+
log

dÏ
dÏ€(1âˆ’Î±)Nr ( 	m, 	Î¸)

âˆ’2 log(Ïµ)
N(Î± âˆ’Î³)

1.3.
Local bounds
27
+
2
N(Î± âˆ’Î³) log

Î¼

exp
 
âˆ’1
2
 Î»
Î²
Ï€exp(âˆ’Î±r)(r)dÎ±
!
.
Let us remark that in the case when Î½ = Î¼
Ï€[(1âˆ’Î±)Nr]
Ï€[(1+Î³)âˆ’Nr]
1/2 and Ï = Ï€(1âˆ’Î±)Nr, we
get as desired a bound that is adaptively local in all the Î˜m (at least when M is
countable and Î¼ is atomic):
B(Î½, Ï) â‰¤âˆ’
2
N(Î±âˆ’Î³) log

Î¼

exp

N
2 log

(1 + Î³)(1 âˆ’Î±)

râ‹†
âˆ’log

âˆ’log(1âˆ’Î±)
log(1+Î³)

de
2

âˆ’
2 log(Ïµ)
N(Î± âˆ’Î³)
â‰¤inf
mâˆˆM

âˆ’
log

(1âˆ’Î±)(1+Î³)

Î±âˆ’Î³
râ‹†(m)
+ log

âˆ’log(1âˆ’Î±)
log(1+Î³)

de(m)
N(Î±âˆ’Î³) âˆ’2
log

ÏµÎ¼(m)

N(Î±âˆ’Î³)

.
The penalization by the empirical dimension de(m) in each sub-model is as desired
linear in de(m). Non random partially local bounds could be obtained in a way that
is easy to imagine. We leave this investigation to the reader.
1.3.6. Two step localization.
We have seen that the bound optimal choice of
the posterior distribution Î½ on the index set in Theorem 1.3.10 (page 24) is such
that
dÎ½
dÎ¼(m) âˆ¼
"
Ï€

exp

âˆ’Î»r(m, Â·)

Ï€

exp

âˆ’Î²r(m, Â·)

# 1
2
= exp

âˆ’1
2
 Î»
Î²
Ï€exp(âˆ’Î±r)(m, r)dÎ±

.
This suggests replacing the prior distribution Î¼ with Î¼ deï¬ned by its density
(1.14)
dÎ¼
dÎ¼(m) = exp

âˆ’h(m)

Î¼

exp(âˆ’h)
 ,
where h(m) = âˆ’Î¾
 Î³
Î²
Ï€exp(âˆ’Î±Î¦âˆ’Î·
N â—¦R)

Î¦âˆ’Î·
N â—¦R(m, Â·)

dÎ±.
The use of Î¦âˆ’Î·
N â—¦R instead of R is motivated by technical reasons which will appear
in subsequent computations. Indeed, we will need to bound
Î½
 Î»
Î²
Ï€exp(âˆ’Î±Î¦âˆ’Î·
N â—¦R)

Î¦âˆ’Î·
N â—¦R

dÎ±

in order to handle K(Î½, Î¼). In the spirit of equation (1.9, page 18), starting back
from Theorem 1.1.4 (page 4), applied in each sub-model Î˜m to the prior distribution
Ï€exp(âˆ’Î³Î¦âˆ’Î·
N â—¦R) and integrated with respect to Î¼, we see that for any positive real
constants Î», Î³ and Î·, with P probability at least 1âˆ’Ïµ, for any posterior distribution
Î½ : Î© â†’M1
+(M) on the index set and any conditional posterior distribution Ï :
Î© Ã— M â†’M1
+(Î˜),

28
Chapter 1.
Inductive PAC-Bayesian learning
(1.15)
Î½Ï

Î»Î¦ Î»
N â—¦R âˆ’Î³Î¦âˆ’Î·
N â—¦R

â‰¤Î»Î½Ï(r)
+ Î½K(Ï, Ï€) + K(Î½, Î¼) + Î½

log

Ï€

exp

âˆ’Î³Î¦âˆ’Î·
N â—¦R

âˆ’log(Ïµ).
Since x â†’f(x)
def
= Î»Î¦ Î»
N âˆ’Î³Î¦âˆ’Î·
N (x) is a convex function, it is such that
f(x) â‰¥xf â€²(0) = xN

1 âˆ’exp(âˆ’Î»
N )

+ Î³
Î·

exp( Î·
N ) âˆ’1

.
Thus if we put
(1.16)
Î³ = Î·

1 âˆ’exp(âˆ’Î»
N )

exp( Î·
N ) âˆ’1
,
we obtain that f(x) â‰¥0, x âˆˆR, and therefore that the left-hand side of equation
(1.15) is non-negative. We can moreover introduce the prior conditional distribution
Ï€ deï¬ned by
dÏ€
dÏ€ (m, Î¸) =
exp

âˆ’Î²Î¦âˆ’Î·
N â—¦R(Î¸)

Ï€

m, exp

âˆ’Î²Î¦âˆ’Î·
N â—¦R
.
With P probability at least 1 âˆ’Ïµ, for any posterior distributions Î½ : Î© â†’M1
+(M)
and Ï : Î© Ã— M â†’M1
+(Î˜),
Î²Î½Ï(r) + Î½

K(Ï, Ï€)

= Î½

K

Ï, Ï€exp(âˆ’Î²r)

âˆ’Î½

log

Ï€

exp(âˆ’Î²r)

â‰¤Î½

K

Ï, Ï€exp(âˆ’Î²r)

+ Î²Î½Ï€(r) + Î½

K(Ï€, Ï€)

â‰¤Î½

K

Ï, Ï€exp(âˆ’Î²r)

+ Î²Î½Ï€

Î¦âˆ’Î·
N â—¦R

+ Î²
Î·

K(Î½, Î¼) âˆ’log(Ïµ)

+ Î½

K(Ï€, Ï€)

= Î½

K

Ï, Ï€exp(âˆ’Î²r)

âˆ’Î½

log

Ï€

exp

âˆ’Î²Î¦âˆ’Î·
N â—¦R

+ Î²
Î·

K(Î½, Î¼) âˆ’log(Ïµ)

.
Thus, coming back to equation (1.15), we see that under condition (1.16), with P
probability at least 1 âˆ’Ïµ,
0 â‰¤(Î» âˆ’Î²)Î½Ï(r) + Î½

K

Ï, Ï€exp(âˆ’Î²r)

âˆ’Î½
 Î³
Î²
Ï€exp(âˆ’Î±Î¦âˆ’Î·
N â—¦R)

Î¦âˆ’Î·
N â—¦R

dÎ±

+ (1 + Î²
Î· )

K(Î½, Î¼) + log( 2
Ïµ )

.
Noticing moreover that
(Î» âˆ’Î²)Î½Ï(r) + Î½

K

Ï, Ï€exp(âˆ’Î²r)

= Î½

K

Ï, Ï€exp(âˆ’Î»r)

+ Î½
 Î»
Î²
Ï€exp(âˆ’Î±r)(r)dÎ±

,
and choosing Ï = Ï€exp(âˆ’Î»r), we have proved
Theorem 1.3.12. For any positive real constants Î², Î³
and Î·, such that
Î³
<
Î·

exp( Î·
N ) âˆ’1
âˆ’1,
deï¬ning
Î»
by
condition
(1.16),
so
that

1.3.
Local bounds
29
Î» = âˆ’N log

1âˆ’Î³
Î·

exp( Î·
N )âˆ’1

, with P probability at least 1âˆ’Ïµ, for any posterior
distribution Î½ : Î© â†’M1
+(M), any conditional posterior distribution Ï : Î© Ã— M â†’
M1
+(Î˜),
Î½
 Î³
Î²
Ï€exp(âˆ’Î±Î¦âˆ’Î·
N â—¦R)

Î¦âˆ’Î·
N â—¦R

dÎ±

â‰¤Î½
 Î»
Î²
Ï€exp(âˆ’Î±r)(r)dÎ±

+

1 + Î²
Î·

K(Î½, Î¼) + log

 2
Ïµ

.
Let us remark that this theorem does not require that Î² < Î³, and thus provides
both an upper and a lower bound for the quantity of interest:
Corollary 1.3.13. For any positive real constants Î², Î³ and Î· such that max{Î²,
Î³} < Î·

exp( Î·
N )âˆ’1
âˆ’1, with P probability at least 1âˆ’Ïµ, for any posterior distributions
Î½ : Î© â†’M1
+(M) and Ï : Î© Ã— M â†’M1
+(Î˜),
Î½
 Î³
âˆ’N log{1âˆ’Î²
N [exp( Î·
N )âˆ’1]}
Ï€exp(âˆ’Î±r)(r)dÎ±

âˆ’

1 + Î³
Î·

K(Î½, Î¼) + log

 3
Ïµ

â‰¤Î½
 Î³
Î²
Ï€exp(âˆ’Î±Î¦âˆ’Î·
N â—¦R)

Î¦âˆ’Î·
N â—¦R

dÎ±

â‰¤Î½
 âˆ’N log{1âˆ’Î³
Î· [exp( Î·
N )âˆ’1]}
Î²
Ï€exp(âˆ’Î±r)(r)dÎ±

+

1 + Î²
Î·

K(Î½, Î¼) + log

 3
Ïµ

.
We can then remember that
K(Î½, Î¼) = Î¾

Î½ âˆ’Î¼
 Î³
Î²
Ï€exp(âˆ’Î±Î¦âˆ’Î·
N â—¦R)

Î¦âˆ’Î·
N â—¦R

dÎ±

+ K(Î½, Î¼) âˆ’K(Î¼, Î¼),
to conclude that, putting
(1.17)
GÎ·(Î±) = âˆ’N log

1 âˆ’Î±
Î·

exp

 Î·
N ) âˆ’1

â‰¥Î±,
Î± âˆˆR+,
and
(1.18)
d	Î½
dÎ¼(m)
def
= exp

âˆ’h(m)

Î¼

exp(âˆ’h)
 where h(m) = Î¾
 Î³
GÎ·(Î²)
Ï€exp(âˆ’Î±r)(m, r)dÎ±,
the divergence of Î½ with respect to the local prior Î¼ is bounded by

1 âˆ’Î¾

1 + Î²
Î·

K(Î½, Î¼)
â‰¤Î¾Î½
 GÎ·(Î³)
Î²
Ï€exp(âˆ’Î±r)(r)dÎ±

âˆ’Î¾Î¼
 Î³
GÎ·(Î²)
Ï€exp(âˆ’Î±r)(r)dÎ±

+ K(Î½, Î¼) âˆ’K(Î¼, Î¼) + Î¾

2 + Î²+Î³
Î·

log

 3
Ïµ

â‰¤Î¾Î½
 GÎ·(Î³)
Î²
Ï€exp(âˆ’Î±r)(r)dÎ±

+ K(Î½, Î¼)
+ log

Î¼

exp
 
âˆ’Î¾
 Î³
GÎ·(Î²)
Ï€exp(âˆ’Î±r)(r)dÎ±
!

30
Chapter 1.
Inductive PAC-Bayesian learning
+ Î¾

2 + Î²+Î³
Î·

log

 3
Ïµ

= K(Î½, 	Î½) + Î¾Î½
  GÎ·(Î²)
Î²
+
 GÎ·(Î³)
Î³
!
Ï€exp(âˆ’Î±r)(r)dÎ±

+ Î¾

2 + Î²+Î³
Î·

log

 3
Ïµ

.
We have proved
Theorem 1.3.14.
For
any
positive
constants
Î²,
Î³
and
Î·
such
that
max{Î², Î³} < Î·

exp( Î·
N ) âˆ’1
âˆ’1, with P probability at least 1 âˆ’Ïµ, for any pos-
terior distribution Î½ : Î© â†’M1
+(M) and any conditional posterior distribution
Ï : Î© Ã— M â†’M1
+(Î˜),
K(Î½, Î¼) â‰¤

1 âˆ’Î¾

1 + Î²
Î·
âˆ’1
K(Î½, 	Î½)
+ Î¾Î½
  GÎ·(Î²)
Î²
+
 GÎ·(Î³)
Î³
!
Ï€exp(âˆ’Î±r)(r)dÎ±

+ Î¾

2 + Î²+Î³
Î·

log

 3
Ïµ

â‰¤

1 âˆ’Î¾

1 + Î²
Î·
âˆ’1
K(Î½, 	Î½)
+ Î¾Î½

GÎ·(Î³) âˆ’Î³ + GÎ·(Î²) âˆ’Î²

râ‹†+ log
 GÎ·(Î²)GÎ·(Î³)
Î²Î³
!
de

+ Î¾

2 + Î²+Î³
Î·

log

 3
Ïµ

,
where the local prior Î¼ is deï¬ned by equation (1.14, page 27) and the local posterior
	Î½ and the function GÎ· are deï¬ned by equation (1.18, page 29).
We can then use this theorem to give a local version of Theorem 1.3.10 (page 24).
To get something pleasing to read, we can apply Theorem 1.3.14 with constants Î²â€²,
Î³â€² and Î· chosen so that
2Î¾
1âˆ’Î¾(1+ Î²â€²
Î· ) = 1, GÎ·(Î²â€²) = Î² and Î³â€² = Î», where Î² and Î» are
the constants appearing in Theorem 1.3.10. This gives
Theorem 1.3.15.
For any positive real constants Î² < Î» and Î· such that Î» <
Î·

exp( Î·
N ) âˆ’1
âˆ’1, with P probability at least 1 âˆ’Ïµ, for any posterior distribution
Î½ : Î© â†’M1
+(M), for any conditional posterior distribution Ï : Î© Ã— M â†’M1
+(Î˜),
Î½Ï

Î»Î¦ Î»
N (R) âˆ’Î²Î¦âˆ’Î²
N (R)

â‰¤Î»Î¦ Î»
N

Î½Ï(R)

âˆ’Î²Î¦âˆ’Î²
N

Î½Ï(R)

â‰¤B3(Î½, Ï),
where B3(Î½, Ï) = Î½
 GÎ·(Î»)
Gâˆ’1
Î· (Î²)
Ï€exp(âˆ’Î±r)(r)dÎ±

+

3 +
Gâˆ’1
Î·
(Î²)
Î·

K

Î½, Î¼
exp

âˆ’

3+
Gâˆ’1
Î·
(Î²)
Î·
âˆ’1  Î»
Î² Ï€exp(âˆ’Î±r)(r)dÎ±

+ Î½

K(Ï, Ï€exp(âˆ’Î»r)

+

4 +
Gâˆ’1
Î· (Î²)+Î»
Î·

log

 4
Ïµ

â‰¤Î½

GÎ·(Î») âˆ’Gâˆ’1
Î· (Î²)

râ‹†+ log

GÎ·(Î»)
Gâˆ’1
Î· (Î²)

de


1.3.
Local bounds
31
+

3 +
Gâˆ’1
Î·
(Î²)
Î·

K

Î½, Î¼
exp

âˆ’

3+
Gâˆ’1
Î·
(Î²)
Î·
âˆ’1  Î»
Î² Ï€exp(âˆ’Î±r)(r)dÎ±

+ Î½

K(Ï, Ï€exp(âˆ’Î»r)

+

4 +
Gâˆ’1
Î· (Î²)+Î»
Î·

log

 4
Ïµ

,
and where the function GÎ· is deï¬ned by equation (1.17, page 29).
A ï¬rst remark: if we had the stamina to use Cauchy Schwarz inequalities (or more
generally HÂ¨older inequalities) on exponential moments instead of using weighted
union bounds on deviation inequalities, we could have replaced log( 4
Ïµ ) with âˆ’log(Ïµ)
in the above inequalities.
We see that we have achieved the desired kind of localization of Theorem 1.3.10
(page 24), since the new empirical entropy term
K[Î½, Î¼exp[âˆ’Î¾  Î»
Î² Ï€exp(âˆ’Î±r)(r)dÎ±]]
cancels for a value of the posterior distribution on the index set Î½ which is of the
same form as the one minimizing the bound B1(Î½, Ï) of Theorem 1.3.10 (with a
decreased constant, as could be expected). In a typical parametric setting, we will
have
 Î»
Î²
Ï€exp(âˆ’Î±r)(r)dÎ± â‰ƒ(Î» âˆ’Î²)râ‹†(m) + log

Î»
Î²

de(m),
and therefore, if we choose for Î½ the Dirac mass at
	m âˆˆarg minmâˆˆM râ‹†(m) +
log( Î»
Î² )
Î»âˆ’Î² de(m),
and Ï(m, Â·) = Ï€exp(âˆ’Î»r)(m, Â·), we will get, in the case when the index set M is
countable,
B3(Î½, Ï) â‰²max
âŽ§
âŽ¨
âŽ©

GÎ·(Î») âˆ’Gâˆ’1
Î· (Î²)

, (Î» âˆ’Î²)
log
 GÎ·(Î»)
Gâˆ’1
Î·
(Î²)

log( Î»
Î² )
âŽ«
âŽ¬
âŽ­
Ã—

râ‹†( 	m) +
log( Î»
Î² )
Î»âˆ’Î² de( 	m)

+

3 +
Gâˆ’1
Î· (Î²)
Î·

log
 
mâˆˆM
Î¼(m)
Î¼( 	m) exp

âˆ’

3 +
Gâˆ’1
Î·
(Î²)
Î·
âˆ’1
Ã—

(Î» âˆ’Î²)

râ‹†(m) âˆ’râ‹†( 	m)

+ log

 Î»
Î²

de(m) âˆ’de( 	m)

+

4 +
Gâˆ’1
Î· (Î²)+Î»
Î·

log

 4
Ïµ

.
This shows that the impact on the bound of the addition of supplementary models
depends on their penalized minimum empirical risk râ‹†(m) +
log( Î»
Î² )
Î»âˆ’Î² de(m). More
precisely the adaptive and local complexity factor
log
 
mâˆˆM
Î¼(m)
Î¼( 	m) exp

âˆ’

3 +
Gâˆ’1
Î· (Î²)
Î·
âˆ’1
Ã—

(Î» âˆ’Î²)

râ‹†(m) âˆ’râ‹†( 	m)

+ log

 Î»
Î²

de(m) âˆ’de( 	m)


32
Chapter 1.
Inductive PAC-Bayesian learning
replaces in this bound the non local factor
K(Î½, Î¼) = âˆ’log

Î¼( 	m)

= log
& 
mâˆˆM
Î¼(m)
Î¼( 	m)
'
which appears when applying Theorem 1.3.10 (page 24) to the Dirac mass Î½ = Î´	
m.
Thus in the local bound, the inï¬‚uence of models decreases exponentially fast when
their penalized empirical risk increases.
One can deduce a result about the deviations with respect to the posterior Î½Ï
from Theorem 1.3.15 (page 30) without much supplementary work: it is enough for
that purpose to remark that with P probability at least 1 âˆ’Ïµ, for any posterior
distribution Î½ : Î© â†’M1
+(M),
Î½

log

Ï€exp(âˆ’Î»r)

exp

Î»Î¦ Î»
N (R) âˆ’Î²Î¦âˆ’Î²
N (R)

âˆ’Î½
" GÎ·(Î»)
Gâˆ’1
Î· (Î²)
Ï€exp(âˆ’Î±r)(r)dÎ±
#
âˆ’

3 +
Gâˆ’1
Î·
(Î²)
Î·

K

Î½, Î¼
exp

âˆ’

3+
Gâˆ’1
Î·
(Î²)
Î·
âˆ’1  Î»
Î² Ï€exp(âˆ’Î±r)(r)dÎ±

âˆ’

4 +
Gâˆ’1
Î· (Î²)+Î»
Î·

log

4
Ïµ

â‰¤0,
this inequality being obtained by taking a supremum in Ï in Theorem 1.3.15 (page
30). One can then take a supremum in Î½, to get, still with P probability at least
1 âˆ’Ïµ,
log

Î¼
exp

âˆ’

3+
Gâˆ’1
Î·
(Î²)
Î·
âˆ’1  Î»
Î² Ï€exp(âˆ’Î±r)(r)dÎ±

&

Ï€exp(âˆ’Î»r)

exp

Î»Î¦ Î»
N (R) âˆ’Î²Î¦âˆ’Î²
N (R)

3+
Gâˆ’1
Î·
(Î²)
Î·
âˆ’1
Ã— exp
"
âˆ’

3 +
Gâˆ’1
Î· (Î²)
Î·
âˆ’1  GÎ·(Î»)
Gâˆ’1
Î· (Î²)
Ï€exp(âˆ’Î±r)(r)dÎ±
#'
â‰¤
4 +
Gâˆ’1
Î·
(Î²)+Î»
Î·
3 + Gâˆ’1
Î· (Î²)
Î·
log

 4
Ïµ

.
Using the fact that x â†’xÎ± is concave when Î± =

3 +
Gâˆ’1
Î· (Î²)
Î·
âˆ’1 < 1, we get for
any posterior conditional distribution Ï : Î© Ã— M â†’M1
+(Î˜),
Î¼
exp

âˆ’

3+
Gâˆ’1
Î·
(Î²)
Î·
âˆ’1  Î»
Î² Ï€exp(âˆ’Î±r)(r)dÎ±
Ï

exp
&
3 +
Gâˆ’1
Î·
(Î²)
Î·
âˆ’1
"
Î»Î¦ Î»
N (R) âˆ’Î²Î¦âˆ’Î²
N (R) âˆ’
 GÎ·(Î»)
Gâˆ’1
Î· (Î²)
Ï€exp(âˆ’Î±r)(r)dÎ±
+ log

dÏ
dÏ€exp(âˆ’Î»r)
( 	m, 	Î¸ )
#'

1.4.
Relative bounds
33
â‰¤exp
"
4 +
Gâˆ’1
Î· (Î²)+Î»
Î·
3 + Gâˆ’1
Î· (Î²)
Î·
log

 4
Ïµ

#
.
We can thus state
Theorem 1.3.16. For any Ïµ âˆˆ)0, 1(, with P probability at least 1 âˆ’Ïµ, for any
posterior distribution Î½ : Î© â†’M1
+(M) and conditional posterior distribution Ï :
Î© Ã— M â†’M1
+(Î˜), for any Î¾ âˆˆ)0, 1(, with Î½Ï probability at least 1 âˆ’Î¾,
Î»Î¦ Î»
N (R) âˆ’Î²Î¦âˆ’Î²
N (R) â‰¤
 GÎ·(Î»)
Gâˆ’1
Î· (Î²)
Ï€exp(âˆ’Î±r)(r)dÎ±
+

3 +
Gâˆ’1
Î· (Î²)
Î·

log
âŽ¡
âŽ¢âŽ¢âŽ£
dÎ½
dÎ¼
exp

âˆ’

3+
Gâˆ’1
Î·
(Î²)
Î·
âˆ’1  Î»
Î² Ï€exp(âˆ’Î±r)(r)dÎ±
 ( 	m)
âŽ¤
âŽ¥âŽ¥âŽ¦
+ log

dÏ
dÏ€exp(âˆ’Î»r)
( 	m, 	Î¸ )

+

4 +
Gâˆ’1
Î·
(Î²)+Î»
Î·

log

 4
Ïµ

âˆ’

3 +
Gâˆ’1
Î· (Î²)
Î·

log(Î¾).
Note that the given bound consequently holds with PÎ½Ï probability at least
(1 âˆ’Ïµ)(1 âˆ’Î¾) â‰¥1 âˆ’Ïµ âˆ’Î¾.
1.4. Relative bounds
The behaviour of the minimum of the empirical process Î¸ â†’r(Î¸) is known to
depend on the covariances between pairs

r(Î¸), r(Î¸â€²)

, Î¸, Î¸â€² âˆˆÎ˜. In this respect,
our previous study, based on the analysis of the variance of r(Î¸) (or technically
on some exponential moment playing quite the same role), loses some accuracy in
some circumstances (namely when infÎ˜ R is not close enough to zero).
In this section, instead of bounding the expected risk Ï(R) of any posterior
distribution, we are going to upper bound the diï¬€erence Ï(R) âˆ’infÎ˜ R, and more
generally Ï(R) âˆ’R(Î¸), where Î¸ âˆˆÎ˜ is some ï¬xed parameter value.
In the next section we will analyse Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R), allowing us to compare
the expected error rate of a posterior distribution Ï with the error rate of a Gibbs
prior distribution. We will also analyse Ï1(R) âˆ’Ï2(R), where Ï1 and Ï2 are two
arbitrary posterior distributions, using comparison with a Gibbs prior distribution
as a tool, and in particular as a tool to establish the required Kullback divergence
bounds.
Relative bounds do not provide the same kind of results as direct bounds on
the error rate: it is not possible to estimate Ï(R) with an order of precision higher
than (Ï(R)/N)1/2, so that relative bounds cannot of course achieve that, but they
provide a way to reach a faster rate for Ï(R) âˆ’infÎ˜ R, that is for the relative
performance of the estimator within a restricted model.
The study of PAC-Bayesian relative bounds was initiated in the second and third
parts of J.-Y. Audibertâ€™s dissertation (Audibert, 2004b).
In this section and the next, we will suggest a series of possible uses of relative
bounds. As usual, we will start with the simplest inequalities and proceed towards
more sophisticated techniques with better theoretical properties, but at the same
time less precise constants, so that which one is the more ï¬tted will depend on the
size of the training sample.

34
Chapter 1.
Inductive PAC-Bayesian learning
The ï¬rst thing we will do is to compute for any posterior distribution Ï : Î© â†’
M1
+(Î˜) a relative performance bound bearing on Ï(R) âˆ’infÎ˜ R. We will also com-
pare the classiï¬cation model indexed by Î˜ with a sub-model indexed by one of
its measurable subsets Î˜1 âŠ‚Î˜. For this purpose we will form the diï¬€erence
Ï(R) âˆ’R(Î¸), where Î¸ âˆˆÎ˜1 is some possibly unobservable value of the parame-
ter in the sub-model deï¬ned by Î˜1, typically chosen in arg minÎ˜1 R. If this is so
and Ï(R) âˆ’R(Î¸) = Ï(R) âˆ’infÎ˜1 R, a negative upper bound indicates that it is
deï¬nitely worth using a randomized estimator Ï supported by the larger parameter
set Î˜ instead of using only the classiï¬cation model deï¬ned by the smaller set Î˜1.
1.4.1. Basic inequalities.
Relative bounds in this section are based on the
control of r(Î¸) âˆ’r(Î¸), where Î¸, Î¸ âˆˆÎ˜. These diï¬€erences are related to the random
variables
Ïˆi(Î¸, Î¸) = Ïƒi(Î¸) âˆ’Ïƒi(Î¸) = 1

fÎ¸(Xi) Ì¸= Yi

âˆ’1

fÎ¸(Xi) Ì¸= Yi

.
Some supplementary technical diï¬ƒculties, as compared to the previous sections,
come from the fact that Ïˆi(Î¸, Î¸) takes three values, whereas Ïƒi(Î¸) takes only two.
Let
(1.19)
râ€²(Î¸, Î¸) = r(Î¸) âˆ’r(Î¸) = 1
N
N

i=1
Ïˆi(Î¸, Î¸),
Î¸, Î¸ âˆˆÎ˜,
and Râ€²(Î¸, Î¸) = R(Î¸)âˆ’R(Î¸) = P

râ€²(Î¸, Î¸)

. We have as usual from independence that
log

P

exp

âˆ’Î»râ€²(Î¸, Î¸)

=
N

i=1
log

P

exp

âˆ’Î»
N Ïˆi(Î¸, Î¸)

â‰¤N log
 1
N
N

i=1
P

exp

âˆ’Î»
N Ïˆi(Î¸, Î¸)

.
Let Ci be the distribution of Ïˆi(Î¸, Î¸) under P and let Â¯C = 1
N
N
i=1 Ci âˆˆM1
+

{âˆ’1, 0,
1}

. With this notation
(1.20)
log

P

exp

âˆ’Î»râ€²(Î¸, Î¸)

â‰¤N log

Ïˆâˆˆ{âˆ’1,0,1}
exp

âˆ’Î»
N Ïˆ

Â¯C(dÏˆ)

.
The right-hand side of this inequality is a function of Â¯C. On the other hand, Â¯C
being a probability measure on a three point set, is deï¬ned by two parameters, that
we may take equal to

Ïˆ Â¯C(dÏˆ) and

Ïˆ2 Â¯C(dÏˆ). To this purpose, let us introduce
M â€²(Î¸, Î¸) =

Ïˆ2 Â¯C(dÏˆ) = Â¯C(+1) + Â¯C(âˆ’1) = 1
N
N

i=1
P

Ïˆ2
i (Î¸, Î¸)

,
Î¸, Î¸ âˆˆÎ˜.
It is a pseudo distance (meaning that it is symmetric and satisï¬es the triangle
inequality), since it can also be written as
M â€²(Î¸, Î¸) = 1
N
N

i=1
P
2221

fÎ¸(Xi) Ì¸= Yi

âˆ’1

fÎ¸(Xi) Ì¸= Yi
222

,
Î¸, Î¸ âˆˆÎ˜.

1.4.
Relative bounds
35
It is readily seen that
N log

exp
 
âˆ’Î»
N Ïˆ
!
Â¯C(dÏˆ)

= âˆ’Î»Î¨ Î»
N

Râ€²(Î¸, Î¸), M â€²(Î¸, Î¸)

,
where
Î¨a(p, m) = âˆ’aâˆ’1 log

(1 âˆ’m) + m + p
2
exp(âˆ’a) + m âˆ’p
2
exp(a)

= âˆ’aâˆ’1 log

1 âˆ’sinh(a)

p âˆ’m tanh( a
2)

.
(1.21)
Thus plugging this equality into inequality (1.20, page 34) we get
Theorem 1.4.1. For any real parameter Î»,
log

P

exp

âˆ’Î»râ€²(Î¸, Î¸)

â‰¤âˆ’Î»Î¨ Î»
N

Râ€²(Î¸, Î¸), M â€²(Î¸, Î¸)

,
Î¸, Î¸ âˆˆÎ˜,
where râ€² is deï¬ned by equation (1.19, page 34) and Î¨ and M â€² are deï¬ned just above.
To make a link with previous work of Mammen and Tsybakov â€” see e.g. Mam-
men et al. (1999) and Tsybakov (2004) â€” we may consider the pseudo-distance
D on Î˜ deï¬ned by equation (1.3, page 7). This distance only depends on the dis-
tribution of the patterns. It is often used to formulate margin assumptions, in the
sense of Mammen and Tsybakov. Here we are going to work rather with M â€²: as it
is dominated by D in the sense that M â€²(Î¸, Î¸) â‰¤D(Î¸, Î¸), Î¸, Î¸ âˆˆÎ˜, with equality in
the important case of binary classiï¬cation, hypotheses formulated on D induce hy-
potheses on M â€², and working with M â€² may only sharpen the results when compared
to working with D.
Using the same reasoning as in the previous section, we deduce
Theorem 1.4.2. For any real parameter Î», any Î¸ âˆˆÎ˜, any prior distribution
Ï€ âˆˆM1
+(Î˜),
P

exp

sup
ÏâˆˆM1
+(Î˜)
Î»

Ï

Î¨ Î»
N

Râ€²(Â·, Î¸ ), M â€²(Â·, Î¸ )

âˆ’Ï

râ€²(Â·, Î¸)

âˆ’K(Ï, Ï€)

â‰¤1.
We are now going to derive some other type of relative exponential inequal-
ity. In Theorem 1.4.2 we obtained an inequality comparing one observed quantity
Ï

râ€²(Â·, Î¸ )

with two unobserved ones, Ï

Râ€²(Â·, Î¸ )

and Ï

M â€²(Â·, Î¸ )

, â€” indeed, because
of the convexity of the function Î»Î¨ Î»
N ,
Î»Ï

Î¨ Î»
N

Râ€²(Â·, Î¸ ), M â€²(Â·, Î¸ )

â‰¥Î»Î¨ Î»
N

Ï

Râ€²(Â·, Î¸ )

, Ï

M â€²(Â·, Î¸ )

.
This may be inconvenient when looking for an empirical bound for Ï

Râ€²(Â·, Î¸)

,
and we are going now to seek an inequality comparing Ï

Râ€²(Â·, Î¸ )

with empirical
quantities only.
This is possible by considering the log-Laplace transform of some modiï¬ed ran-
dom variable Ï‡i(Î¸, Î¸). We may consider more precisely the change of variable deï¬ned
by the equation
exp
 
âˆ’Î»
N Ï‡i
!
= 1 âˆ’Î»
N Ïˆi,

36
Chapter 1.
Inductive PAC-Bayesian learning
which is possible when
Î»
N âˆˆ)âˆ’1, 1( and leads to deï¬ne
Ï‡i = âˆ’N
Î» log
 
1 âˆ’Î»
N Ïˆi
!
.
We may then work on the log-Laplace transform
log

P
&
exp

âˆ’Î»
N
N

i=1
Ï‡i(Î¸, Î¸)
'
= log

P
& N
3
i=1
 
1 âˆ’Î»
N Ïˆi(Î¸, Î¸)
!'
= log

P
&
exp
 N

i=1
log

1 âˆ’Î»
N Ïˆi(Î¸, Î¸)
'
.
We may now follow the same route as previously, writing
log

P
&
exp
 N

i=1
log

1 âˆ’Î»
N Ïˆi(Î¸, Î¸)
'
=
N

i=1
log

1 âˆ’Î»
N P

Ïˆi(Î¸, Î¸)

â‰¤N log

1 âˆ’Î»
N Râ€²(Î¸, Î¸ )

.
Let us also introduce the random pseudo distance
(1.22)
mâ€²(Î¸, Î¸) = 1
N
N

i=1
Ïˆi(Î¸, Î¸)2
= 1
N
N

i=1
2221

fÎ¸(Xi) Ì¸= Yi

âˆ’1

fÎ¸(Xi) Ì¸= Yi
222,
Î¸, Î¸ âˆˆÎ˜.
This is the empirical counterpart of M â€², implying that P(mâ€²) = M â€². Let us notice
that
1
N
N

i=1
log

1 âˆ’Î»
N Ïˆi(Î¸, Î¸)

= log(1 âˆ’Î»
N ) âˆ’log(1 + Î»
N )
2
râ€²(Î¸, Î¸)
+ log(1 âˆ’Î»
N ) + log(1 + Î»
N )
2
mâ€²(Î¸, Î¸)
= 1
2 log
"
1 âˆ’Î»
N
1 + Î»
N
#
râ€²
Î¸, Î¸

+ 1
2 log

1 âˆ’Î»2
N 2

mâ€²
Î¸, Î¸

.
Let us put Î³ = N
2 log
 
1+ Î»
N
1âˆ’Î»
N
!
, so that
Î» = N tanh

 Î³
N

and N
2 log

1 âˆ’Î»2
N 2

= âˆ’N log

cosh( Î³
N )

.
With this notation, we can conveniently write the previous inequality as
P

exp

âˆ’N log

1 âˆ’tanh

 Î³
N

Râ€²(Î¸, Î¸)

âˆ’Î³râ€²
Î¸, Î¸

âˆ’N log

cosh( Î³
N )

mâ€²
Î¸, Î¸

â‰¤1.
Integrating with respect to a prior probability measure Ï€ âˆˆM1
+(Î˜), we obtain

1.4.
Relative bounds
37
Theorem 1.4.3. For any real parameter Î³, for any Î¸ âˆˆÎ˜, for any prior probability
distribution Ï€ âˆˆM1
+(Î˜),
P

exp
&
sup
ÏâˆˆM1
+(Î˜)

âˆ’NÏ

log

1 âˆ’tanh

 Î³
N

Râ€²(Â·, Î¸ )

âˆ’Î³Ï

râ€²(Â·, Î¸ )

âˆ’N log

cosh( Î³
N )

Ï

mâ€²(Â·, Î¸ )

âˆ’K(Ï, Ï€)
'
â‰¤1.
1.4.2. Non random bounds.
Let us ï¬rst deduce a non-random bound from
Theorem 1.4.2 (page 35). This theorem can be conveniently taken advantage of by
throwing the non-linearity into a localized prior, considering the prior probability
measure Î¼ deï¬ned by its density
dÎ¼
dÏ€ (Î¸) =
exp

âˆ’Î»Î¨ Î»
N

Râ€²(Î¸, Î¸ ), M â€²(Î¸, Î¸ )

+ Î²Râ€²(Î¸, Î¸ )

Ï€

exp

âˆ’Î»Î¨ Î»
N

Râ€²(Â·, Î¸ ), M â€²(Â·, Î¸ )

+ Î²Râ€²(Â·, Î¸ )
.
Indeed, for any posterior distribution Ï : Î© â†’M1
+(Î˜),
K(Ï, Î¼) = K(Ï, Ï€) + Î»Ï

Î¨ Î»
N

Râ€²(Â·, Î¸ ), M â€²(Â·, Î¸ )

âˆ’Î²Ï

Râ€²(Â·, Î¸ )

+ log

Ï€

exp

âˆ’Î»Î¨ Î»
N

Râ€²(Â·, Î¸ ), M â€²(Â·, Î¸ )

+ Î²Râ€²(Â·, Î¸ )

.
Plugging this into Theorem 1.4.2 (page 35) and using the convexity of the exponen-
tial function, we see that for any posterior probability distribution Ï : Î© â†’M1
+(Î˜),
Î²P

Ï

Râ€²(Â·, Î¸ )

â‰¤Î»P

Ï

râ€²(Â·, Î¸ )

+ P

K(Ï, Ï€)

+ log

Ï€

exp

âˆ’Î»Î¨ Î»
N

Râ€²(Â·, Î¸ ), M â€²(Â·, Î¸ )

+ Î²Râ€²(Â·, Î¸ )

.
We can then recall that
Î»Ï

râ€²(Â·, Î¸ )

+ K(Ï, Ï€) = K

Ï, Ï€exp(âˆ’Î»r)

âˆ’log

Ï€

exp

âˆ’Î»râ€²(Â·, Î¸ )

,
and notice moreover that
âˆ’P

log

Ï€

exp

âˆ’Î»râ€²(Â·, Î¸ )

â‰¤âˆ’log

Ï€

exp

âˆ’Î»Râ€²(Â·, Î¸ )

,
since Râ€² = P(râ€²) and h â†’log

Ï€

exp(h)

is a convex functional. Putting these two
remarks together, we obtain
Theorem 1.4.4.
For any real positive parameter Î», for any prior distribution
Ï€ âˆˆM1
+(Î˜), for any posterior distribution Ï : Î© â†’M1
+(Î˜),
P

Ï

Râ€²(Â·, Î¸ )

â‰¤1
Î² P

K(Ï, Ï€exp(âˆ’Î»r))

+ 1
Î² log

Ï€

exp

âˆ’Î»Î¨ Î»
N

Râ€²(Â·, Î¸ ), M â€²(Â·, Î¸ )

+ Î²Râ€²(Â·, Î¸ )


38
Chapter 1.
Inductive PAC-Bayesian learning
âˆ’1
Î² log

Ï€

exp

âˆ’Î»Râ€²(Â·, Î¸ )

â‰¤1
Î² P

K(Ï, Ï€exp(âˆ’Î»r))

+ 1
Î² log

Ï€

exp

âˆ’

N sinh( Î»
N ) âˆ’Î²

Râ€²(Â·, Î¸ )
+ 2N sinh( Î»
2N )2M â€²(Â·, Î¸ )

âˆ’1
Î² log

Ï€

exp

âˆ’Î»Râ€²(Â·, Î¸ )

.
It may be interesting to derive some more suggestive (but slightly weaker) bound
in the important case when Î˜1 = Î˜ and R(Î¸) = infÎ˜ R. In this case, it is convenient
to introduce the expected margin function
(1.23)
Ï•(x) = sup
Î¸âˆˆÎ˜
M â€²(Î¸, Î¸) âˆ’xRâ€²(Î¸, Î¸),
x âˆˆR+.
We see that Ï• is convex and non-negative on R+. Using the bound M â€²(Î¸, Î¸ ) â‰¤
xRâ€²(Î¸, Î¸ ) + Ï•(x), we obtain
P

Ï

Râ€²(Â·, Î¸ )

â‰¤1
Î² P

K(Ï, Ï€exp(âˆ’Î»r))

+ 1
Î² log

Ï€

exp

âˆ’

N sinh( Î»
N )

1 âˆ’x tanh( Î»
2N )

âˆ’Î²

Râ€²(Â·, Î¸ )

+ N sinh( Î»
N ) tanh( Î»
2N )
Î²
Ï•(x) âˆ’1
Î² log

Ï€

exp

âˆ’Î»Râ€²(Â·, Î¸ )

.
Let us make the change of variable Î³ = N sinh( Î»
N )

1 âˆ’x tanh( Î»
2N )

âˆ’Î² to obtain
Corollary 1.4.5. For any real positive parameters x, Î³ and Î» such that x â‰¤
tanh( Î»
2N )âˆ’1 and 0 â‰¤Î³ < N sinh( Î»
N )

1 âˆ’x tanh( Î»
2N )

,
P

Ï(R)

âˆ’inf
Î˜ R â‰¤

N sinh( Î»
N )

1 âˆ’x tanh( Î»
2N )

âˆ’Î³
âˆ’1
Ã—
 Î»
Î³

Ï€exp(âˆ’Î±R)(R) âˆ’inf
Î˜ R

dÎ±
+ N sinh

 Î»
N

tanh

 Î»
2N

Ï•(x) + P

K(Ï, Ï€exp(âˆ’Î»r))

.
Let us remark that these results, although well suited to study Mammen and
Tsybakovâ€™s margin assumptions, hold in the general case: introducing the convex
expected margin function Ï• is a substitute for making hypotheses about the relations
between R and D.
Using the fact that Râ€²(Î¸, Î¸ ) â‰¥0, Î¸ âˆˆÎ˜ and that Ï•(x) â‰¥0, x âˆˆR+, we can
weaken and simplify the preceding corollary even more to get
Corollary 1.4.6. For any real parameters Î², Î» and x such that x â‰¥0 and 0 â‰¤
Î² < Î» âˆ’x Î»2
2N , for any posterior distribution Ï : Î© â†’M1
+(Î˜),

1.4.
Relative bounds
39
P

Ï(R)

â‰¤inf
Î˜ R
+

Î» âˆ’x Î»2
2N âˆ’Î²
âˆ’1 Î»
Î²

Ï€exp(âˆ’Î±R)(R) âˆ’inf
Î˜ R

dÎ±
+ P

K

Ï, Ï€exp(âˆ’Î»r)

+ Ï•(x) Î»2
2N

.
Let us apply this bound under the margin assumption ï¬rst considered by Mam-
men and Tsybakov (Mammen et al., 1999; Tsybakov, 2004), which says that for
some real positive constant c and some real exponent Îº â‰¥1,
(1.24)
Râ€²(Î¸, Î¸) â‰¥cD(Î¸, Î¸)Îº,
Î¸ âˆˆÎ˜.
In the case when Îº = 1, then Ï•(câˆ’1) = 0, proving that
P

Ï€exp(âˆ’Î»r)

Râ€²(Â·, Î¸ )

â‰¤
 Î»
Î² Ï€exp(âˆ’Î³R)

Râ€²(Â·, Î¸ )

dÎ³
N sinh( Î»
N )

1 âˆ’câˆ’1 tanh( Î»
2N )

âˆ’Î²
â‰¤
 Î»
Î² Ï€exp(âˆ’Î³R)

Râ€²(Â·, Î¸ )

dÎ³
Î» âˆ’
Î»2
2cN âˆ’Î²
.
Taking for example Î» = cN
2 , Î² = Î»
2 = cN
4 , we obtain
P

Ï€exp(âˆ’2âˆ’1cNr)(R)

â‰¤inf R + 8
cN

cN
2
cN
4
Ï€exp(âˆ’Î³R)

Râ€²(Â·, Î¸)

dÎ³
â‰¤inf R + 2Ï€exp(âˆ’cN
4 R)

Râ€²(Â·, Î¸ )

.
If moreover the behaviour of the prior distribution Ï€ is parametric, meaning that
Ï€exp(âˆ’Î²R)

Râ€²(Â·, Î¸ )

â‰¤d
Î² , for some positive real constant d linked with the dimension
of the classiï¬cation model, then
P

Ï€exp(âˆ’cN
2 r)(R)

â‰¤inf R + 8 log(2)d
cN
â‰¤inf R + 5.55 d
cN .
In the case when Îº > 1,
Ï•(x) â‰¤(Îº âˆ’1)Îºâˆ’
Îº
Îºâˆ’1 (cx)âˆ’
1
Îºâˆ’1 = (1 âˆ’Îºâˆ’1)(Îºcx)âˆ’
1
Îºâˆ’1 ,
thus P

Ï€exp(âˆ’Î»r)

Râ€²(Â·, Î¸ )

â‰¤
 Î»
Î² Ï€exp(âˆ’Î³R)

Râ€²(Â·, Î¸ )

dÎ³ + (1 âˆ’Îºâˆ’1)(Îºcx)âˆ’
1
Îºâˆ’1 Î»2
2N
Î» âˆ’xÎ»2
2N âˆ’Î²
.
Taking for instance Î² = Î»
2 , x = N
2Î», and putting b = (1 âˆ’Îºâˆ’1)(cÎº)âˆ’
1
Îºâˆ’1 , we obtain
P

Ï€exp(âˆ’Î»r)(R)

âˆ’inf R â‰¤4
Î»
 Î»
Î»/2
Ï€exp(âˆ’Î³R)

Râ€²(Â·, Î¸ )

dÎ³ + b
 2Î»
N
!
Îº
Îºâˆ’1
.
In the parametric case when Ï€exp(âˆ’Î³R)

Râ€²(Â·, Î¸ )

â‰¤d
Î³ , we get
P

Ï€exp(âˆ’Î»r)(R)

âˆ’inf R â‰¤4 log(2)d
Î»
+ b
 2Î»
N
!
Îº
Îºâˆ’1
.

40
Chapter 1.
Inductive PAC-Bayesian learning
Taking
Î» = 2âˆ’1
8 log(2)d
 Îºâˆ’1
2Îºâˆ’1 (Îºc)
1
2Îºâˆ’1 N
Îº
2Îºâˆ’1 ,
we obtain
P

Ï€exp(âˆ’Î»r)(R)

âˆ’inf R â‰¤(2 âˆ’Îºâˆ’1)(Îºc)âˆ’
1
2Îºâˆ’1
 8 log(2)d
N
!
Îº
2Îºâˆ’1
.
We see that this formula coincides with the result for Îº = 1. We can thus reduce
the two cases to a single one and state
Corollary 1.4.7.
Let us assume that for some Î¸ âˆˆÎ˜, some positive real constant
c, some real exponent Îº â‰¥1 and for any Î¸ âˆˆÎ˜, R(Î¸) â‰¥R(Î¸) + cD(Î¸, Î¸)Îº. Let us
also assume that for some positive real constant d and any positive real parameter
Î³, Ï€exp(âˆ’Î³R)(R) âˆ’inf R â‰¤d
Î³ . Then
P

Ï€
exp

âˆ’2âˆ’1[8 log(2)d]
Îºâˆ’1
2Îºâˆ’1 (Îºc)
1
2Îºâˆ’1 N
Îº
2Îºâˆ’1 r
(R)

â‰¤inf R + (2 âˆ’Îºâˆ’1)(Îºc)âˆ’
1
2Îºâˆ’1
 8 log(2)d
N
!
Îº
2Îºâˆ’1
.
Let us remark that the exponent of N in this corollary is known to be the mini-
max exponent under these assumptions: it is unimprovable, whatever estimator is
used in place of the Gibbs posterior shown here (at least in the worst case com-
patible with the hypotheses). The interest of the corollary is to show not only the
minimax exponent in N, but also an explicit non-asymptotic bound with reason-
able and simple constants. It is also clear that we could have got slightly better
constants if we had kept the full strength of Theorem 1.4.4 (page 37) instead of
using the weaker Corollary 1.4.6 (page 38).
We will prove in the following empirical bounds showing how the constant Î» can
be estimated from the data instead of being chosen according to some margin and
complexity assumptions.
1.4.3. Unbiased empirical bounds.
We are going to deï¬ne an empirical coun-
terpart for the expected margin function Ï•. It will appear in empirical bounds having
otherwise the same structure as the non-random bound we just proved. Anyhow,
we will not launch into trying to compare the behaviour of our proposed empiri-
cal margin function with the expected margin function, since the margin function
involves taking a supremum which is not straightforward to handle. When we will
touch the issue of building provably adaptive estimators, we will instead formulate
another type of bounds based on integrated quantities, rather than try to analyse
the properties of the empirical margin function.
Let us start as in the previous subsection with the inequality
Î²P

Ï

Râ€²(Â·, Î¸ )

â‰¤P

Î»Ï

râ€²(Â·, Î¸ )

+ K(Ï, Ï€)

+ log

Ï€

exp

âˆ’Î»Î¨ Î»
N

Râ€²(Â·, Î¸ ), M â€²(Â·, Î¸ )

+ Î²Râ€²(Â·, Î¸ )

.
We have already deï¬ned by equation (1.22, page 36) the empirical pseudo-distance
mâ€²(Î¸, Î¸ ) = 1
N
N

i=1
Ïˆi(Î¸, Î¸ )2.

1.4.
Relative bounds
41
Recalling that P

mâ€²(Î¸, Î¸ )

=
M â€²(Î¸, Î¸ ), and using the convexity of h
â†’
log

Ï€

exp(h)

, leads to the following inequalities:
log

Ï€

exp

âˆ’Î»Î¨ Î»
N

Râ€²(Â·, Î¸ ), M â€²(Â·, Î¸ )

+ Î²Râ€²(Â·, Î¸ )

â‰¤log

Ï€

exp

âˆ’N sinh( Î»
N )Râ€²(Â·, Î¸ )
+ N sinh( Î»
N ) tanh( Î»
2N )M â€²(Â·, Î¸ ) + Î²Râ€²(Â·, Î¸ )

â‰¤P

log

Ï€

exp

âˆ’

N sinh( Î»
N ) âˆ’Î²

râ€²(Â·, Î¸ )
+ N sinh( Î»
N ) tanh( Î»
2N )mâ€²(Â·, Î¸ )

.
We may moreover remark that
Î»Ï

râ€²(Â·, Î¸ )

+ K(Ï, Ï€) =

Î² âˆ’N sinh( Î»
N ) + Î»

Ï

râ€²(Â·, Î¸ )

+ K

Ï, Ï€exp{âˆ’[N sinh( Î»
N )âˆ’Î²]r}

âˆ’log

Ï€

exp

âˆ’

N sinh( Î»
N ) âˆ’Î²

râ€²(Â·, Î¸ )

.
This establishes
Theorem 1.4.8. For any positive real parameters Î² and Î», for any posterior dis-
tribution Ï : Î© â†’M1
+(Î˜),
P

Ï

Râ€²(Â·, Î¸ )

â‰¤P

1 âˆ’N sinh( Î»
N ) âˆ’Î»
Î²

Ï

râ€²(Â·, Î¸ )

+
K

Ï, Ï€exp{âˆ’[N sinh( Î»
N )âˆ’Î²]r}

Î²
+ Î²âˆ’1 log

Ï€exp{âˆ’[N sinh( Î»
N )âˆ’Î²]r}

exp

N sinh( Î»
N ) tanh( Î»
2N )mâ€²(Â·, Î¸ )

.
Taking Î² = N
2 sinh( Î»
N ), using the fact that sinh(a) â‰¥a, a â‰¥0 and expressing
tanh( a
2) = aâˆ’14
1 + sinh(a)2âˆ’1

and a = log
4
1 + sinh(a)2+sinh(a)

, we deduce
Corollary 1.4.9. For any positive real constant Î² and any posterior distribution
Ï : Î© â†’M1
+(Î˜),
P

Ï

Râ€²(Â·, Î¸ )

â‰¤P

N
Î² log
%
1 + 4Î²2
N 2 + 2Î²
N

âˆ’1

5
67
8
â‰¤1
Ï

râ€²(Â·, Î¸ )

+ 1
Î²

K

Ï, Ï€exp(âˆ’Î²r)

+ log

Ï€exp(âˆ’Î²r)

exp

N
%
1 + 4Î²2
N 2 âˆ’1

mâ€²(Â·, Î¸ )

.

42
Chapter 1.
Inductive PAC-Bayesian learning
This theorem and its corollary are really analogous to Theorem 1.4.4 (page 37),
and it could easily be proved that under Mammen and Tsybakov margin assump-
tions we obtain an upper bound of the same order as Corollary 1.4.7 (page 40).
Anyhow, in order to obtain an empirical bound, we are now going to take a supre-
mum over all possible values of Î¸, that is over Î˜1. Although we believe that taking
this supremum will not spoil the bound in cases when over-ï¬tting remains un-
der control, we will not try to investigate precisely if and when this is actually
true, and provide our empirical bound as such. Let us say only that on qualitative
grounds, the values of the margin function quantify the steepness of the contrast
function R or its empirical counterpart r, and that the deï¬nition of the empirical
margin function is obtained by substituting P, the true sample distribution, with
P =

 1
N
N
i=1 Î´(Xi,Yi)
âŠ—N, the empirical sample distribution, in the deï¬nition of
the expected margin function. Therefore, on qualitative grounds, it seems hopeless
to presume that R is steep when r is not, or in other words that a classiï¬cation
model that would be ineï¬ƒcient at estimating a bootstrapped sample according to
our non-random bound would be by some miracle eï¬ƒcient at estimating the true
sample distribution according to the same bound. To this extent, we feel that our
empirical bounds bring a satisfactory counterpart of our non-random bounds. Any-
how, we will also produce estimators which can be proved to be adaptive using
PAC-Bayesian tools in the next section, at the price of a more sophisticated con-
struction involving comparisons between a posterior distribution and a Gibbs prior
distribution or between two posterior distributions.
Let us now restrict discussion to the important case when Î¸ âˆˆarg minÎ˜1 R.
To obtain an observable bound, let 	Î¸ âˆˆarg minÎ¸âˆˆÎ˜ r(Î¸) and let us introduce the
empirical margin functions
Ï•(x) = sup
Î¸âˆˆÎ˜
mâ€²(Î¸, 	Î¸) âˆ’x

r(Î¸) âˆ’r(	Î¸)

,
x âˆˆR+,
Ï•(x) = sup
Î¸âˆˆÎ˜1
mâ€²(Î¸, 	Î¸) âˆ’x

r(Î¸) âˆ’r(	Î¸)

,
x âˆˆR+.
Using the fact that mâ€²(Î¸, Î¸) â‰¤mâ€²(Î¸, 	Î¸) + mâ€²(	Î¸, Î¸), we get
Corollary 1.4.10. For any positive real parameters Î² and Î», for any posterior
distribution Ï : Î© â†’M1
+(Î˜),
P

Ï(R)

âˆ’inf
Î˜1 R â‰¤P

1 âˆ’N sinh( Î»
N )âˆ’Î»
Î²

Ï(r) âˆ’r(	Î¸)

+
K

Ï, Ï€exp{âˆ’[N sinh( Î»
N )âˆ’Î²]r}

Î²
+ Î²âˆ’1 log

Ï€exp{âˆ’[N sinh( Î»
N )âˆ’Î²]r}

exp

N sinh

 Î»
N

tanh

 Î»
2N

mâ€²(Â·, 	Î¸)

+ Î²âˆ’1N sinh( Î»
N ) tanh( Î»
2N )Ï•

Î²
N sinh( Î»
N ) tanh( Î»
2N )
"
1 âˆ’N sinh( Î»
N ) âˆ’Î»
Î²
#
.
Taking Î² = N
2 sinh( Î»
N ), we also obtain
P

Ï(R)

âˆ’inf
Î˜1 R â‰¤P

N
Î² log
%
1 + 4Î²2
N 2 + 2Î²
N

âˆ’1

5
67
8
â‰¤1

Ï(r) âˆ’r(	Î¸)


1.4.
Relative bounds
43
+ 1
Î²

K

Ï, Ï€exp(âˆ’Î²r)

+ log

Ï€exp(âˆ’Î²r)

exp

N
%
1 + 4Î²2
N 2 âˆ’1

mâ€²(Â·, 	Î¸)

+ N
Î²
%
1 + 4Î²2
N 2 âˆ’1

Ï•
&log
%
1 + 4Î²2
N 2 + 2Î²
N

âˆ’Î²
N
%
1 + 4Î²2
N 2 âˆ’1

'
.
Note that we could also use the upper bound mâ€²(Î¸, 	Î¸) â‰¤x

r(Î¸) âˆ’r(	Î¸)

+ Ï•(x)
and put Î± = N sinh( Î»
N )

1 âˆ’x tanh( Î»
2N )

âˆ’Î², to obtain
Corollary 1.4.11. For any non-negative real parameters x, Î± and Î», such that
Î± < N sinh( Î»
N )

1 âˆ’x tanh( Î»
2N )

, for any posterior distribution Ï : Î© â†’M1
+(Î˜),
P

Ï(R)

âˆ’inf
Î˜1 R
â‰¤P

1 âˆ’N sinh( Î»
N )

1 âˆ’x tanh( Î»
2N )

âˆ’Î»
N sinh( Î»
N )

1 âˆ’x tanh( Î»
2N )

âˆ’Î±

Ï(r) âˆ’r(	Î¸)

+
K

Ï, Ï€exp(âˆ’Î±r)

N sinh( Î»
N )

1 âˆ’x tanh( Î»
2N )

âˆ’Î±
+
N sinh( Î»
N ) tanh( Î»
2N )
N sinh( Î»
N )

1 âˆ’x tanh( Î»
2N )

âˆ’Î±
Ã—

Ï•(x) + Ï•
 
Î» âˆ’Î±
N sinh( Î»
N ) tanh( Î»
2N )
!
.
Let us notice that in the case when Î˜1 = Î˜, the upper bound provided by this
corollary has the same general form as the upper bound provided by Corollary 1.4.5
(page 38), with the sample distribution P replaced with the empirical distribution
of the sample P =

 1
N
N
i=1 Î´(Xi,Yi)
âŠ—N. Therefore, our empirical bound can be of
a larger order of magnitude than our non-random bound only in the case when our
non-random bound applied to the bootstrapped sample distribution P would be of
a larger order of magnitude than when applied to the true sample distribution P. In
other words, we can say that our empirical bound is close to our non-random bound
in every situation where the bootstrapped sample distribution P is not harder to
bound than the true sample distribution P. Although this does not prove that our
empirical bound is always of the same order as our non-random bound, this is a good
qualitative hint that this will be the case in most practical situations of interest,
since in situations of â€œunder-ï¬ttingâ€, if they exist, it is likely that the choice of the
classiï¬cation model is inappropriate to the data and should be modiï¬ed.
Another reassuring remark is that the empirical margin functions Ï• and Ï• behave
well in the case when infÎ˜ r = 0. Indeed in this case mâ€²(Î¸, 	Î¸) = râ€²(Î¸, 	Î¸) = r(Î¸),
Î¸ âˆˆÎ˜, and thus Ï•(1) = Ï•(1) = 0, and
Ï•(x) â‰¤âˆ’(x âˆ’1) infÎ˜1 r, x â‰¥1.
This shows that in this case we recover the same accuracy as with non-relative local
empirical bounds. Thus the bound of Corollary 1.4.11 does not collapse in presence
of massive over-ï¬tting in the larger model, causing r(	Î¸) = 0, which is another hint
that this may be an accurate bound in many situations.

44
Chapter 1.
Inductive PAC-Bayesian learning
1.4.4. Relative empirical deviation bounds.
It is natural to make use of
Theorem 1.4.3 (page 37) to obtain empirical deviation bounds, since this theorem
provides an empirical variance term.
Theorem 1.4.3 is written in a way which exploits the fact that Ïˆi takes only the
three values âˆ’1, 0 and +1. However, it will be more convenient for the following
computations to use it in its more general form, which only makes use of the fact
that Ïˆi âˆˆ(âˆ’1, 1). With notation to be explained hereafter, it can indeed also be
written as
(1.25)
P

exp
&
sup
ÏâˆˆM1
+(Î˜)

âˆ’NÏ

log

1 âˆ’Î»P(Ïˆ)

+ NÏ

P

log(1 âˆ’Î»Ïˆ)

âˆ’K(Ï, Ï€)
'
â‰¤1.
We have used the following notation in this inequality. We have put
P = 1
N
N

i=1
Î´(Xi,Yi),
so
that
P
is
our
notation
for
the
empirical
distribution
of
the
process
(Xi, Yi)N
i=1. Moreover we have also used
P = P(P) = 1
N
N

i=1
Pi,
where it should be remembered that the joint distribution of the process (Xi, Yi)N
i=1
is P = N
i=1 Pi. We have considered Ïˆ(Î¸, Î¸) as a function deï¬ned on X Ã— Y as
Ïˆ(Î¸, Î¸)(x, y) = 1

y Ì¸= fÎ¸(x)

âˆ’1

y Ì¸= fÎ¸(x)

, (x, y) âˆˆX Ã— Y so that it should be
understood that
P(Ïˆ) = 1
N
N

i=1
P

Ïˆi(Î¸, Î¸)

= 1
N
N

i=1
P

1

Yi Ì¸= fÎ¸(Xi)

âˆ’1

Yi Ì¸= fÎ¸(Xi)

= Râ€²(Î¸, Î¸).
In the same way
P

log(1 âˆ’Î»Ïˆ)

= 1
N
N

i=1
log

1 âˆ’Î»Ïˆi(Î¸, Î¸)

.
Moreover integration with respect to Ï bears on the index Î¸, so that
Ï

log

1 âˆ’Î»P(Ïˆ)

=

Î¸âˆˆÎ˜
log

1 âˆ’Î»
N
N

i=1
P

Ïˆi(Î¸, Î¸)

Ï(dÎ¸),
Ï

P

log(1 âˆ’Î»Ïˆ)

=

Î¸âˆˆÎ˜
 1
N
N

i=1
log

1 âˆ’Î»Ïˆi(Î¸, Î¸)

Ï(dÎ¸).

1.4.
Relative bounds
45
We have chosen concise notation, as we did throughout these notes, in order to
make the computations easier to follow.
To get an alternate version of empirical relative deviation bounds, we need to ï¬nd
some convenient way to localize the choice of the prior distribution Ï€ in equation
(1.25, page 44). Here we propose replacing Ï€ with Î¼ = Ï€exp{âˆ’N log[1+Î²P (Ïˆ)]}, which
can also be written Ï€exp{âˆ’N log[1+Î²Râ€²(Â·,Î¸)]}. Indeed we see that
K(Ï, Î¼) = NÏ

log

1 + Î²P(Ïˆ)

+ K(Ï, Ï€)
+ log

Ï€

exp

âˆ’N log

1 + Î²P(Ïˆ)

.
Moreover, we deduce from our deviation inequality applied to âˆ’Ïˆ, that (as long as
Î² > âˆ’1),
P

exp

NÎ¼

P

log(1 + Î²Ïˆ)

âˆ’NÎ¼

log

1 + Î²P(Ïˆ)

â‰¤1.
Thus
P

exp

log

Ï€

exp

âˆ’N log

1 + Î²P(Ïˆ)

âˆ’log

Ï€

exp

âˆ’NP

log(1 + Î²Ïˆ)

â‰¤P

exp

âˆ’NÎ¼

log

1 + Î²P(Ïˆ)

âˆ’K(Î¼, Ï€)
+ NÎ¼

P

log(1 + Î²Ïˆ)

+ K(Î¼, Ï€)

â‰¤1.
This can be used to handle K(Ï, Î¼), making use of the Cauchyâ€“Schwarz inequality
as follows
P

exp
&
1
2

âˆ’N log

1 âˆ’Î»Ï

P(Ïˆ)

1 + Î²Ï

P(Ïˆ)

+NÏ

P

log(1 âˆ’Î»Ïˆ)

âˆ’K(Ï, Ï€) âˆ’log

Ï€

exp

âˆ’NP

log(1 + Î²Ïˆ)
'
â‰¤P

exp
&
âˆ’N log

1 âˆ’Î»Ï

P(Ïˆ)

+ NÏ

P

log(1 âˆ’Î»Ïˆ)

âˆ’K(Ï, Î¼)
'1/2
Ã— P

exp
&
log

Ï€

exp

âˆ’N log

1 + Î²P(Ïˆ)

âˆ’log

Ï€

exp

âˆ’NP

log(1 + Î²Ïˆ)
'1/2
â‰¤1.

46
Chapter 1.
Inductive PAC-Bayesian learning
This implies that with P probability at least 1 âˆ’Ïµ,
âˆ’N log

1 âˆ’Î»Ï

P(Ïˆ)

1 + Î²Ï

P(Ïˆ)

â‰¤âˆ’NÏ

P

log(1 âˆ’Î»Ïˆ)

+ K(Ï, Ï€) + log

Ï€

exp

âˆ’NP

log(1 + Î²Ïˆ)

âˆ’2 log(Ïµ).
It is now convenient to remember that
P

log(1 âˆ’Î»Ïˆ)

= 1
2 log
 1 âˆ’Î»
1 + Î»
!
râ€²(Î¸, Î¸) + 1
2 log(1 âˆ’Î»2)mâ€²(Î¸, Î¸).
We thus can write the previous inequality as
âˆ’N log

1 âˆ’Î»Ï

Râ€²(Â·, Î¸)

1 + Î²Ï

Râ€²(Â·, Î¸)

â‰¤N
2 log
 1 + Î»
1 âˆ’Î»
!
Ï

râ€²(Â·, Î¸)

âˆ’N
2 log(1 âˆ’Î»2)Ï

mâ€²(Â·, Î¸)

+ K(Ï, Ï€)
+ log

Ï€

exp

âˆ’N
2 log
1 + Î²
1 âˆ’Î²

râ€²(Â·, Î¸)
âˆ’N
2 log(1 âˆ’Î²2)mâ€²(Â·, Î¸)

âˆ’2 log(Ïµ).
Let us assume now that Î¸ âˆˆarg minÎ˜1 R. Let us introduce 	Î¸ âˆˆarg minÎ˜ r. Decom-
posing râ€²(Î¸, Î¸) = râ€²(Î¸, 	Î¸) + râ€²(	Î¸, Î¸) and considering that
mâ€²(Î¸, Î¸) â‰¤mâ€²(Î¸, 	Î¸) + mâ€²(	Î¸, Î¸),
we see that with P probability at least 1 âˆ’Ïµ, for any posterior distribution Ï : Î© â†’
M1
+(Î˜),
âˆ’N log

1 âˆ’Î»Ï

Râ€²(Â·, Î¸)

1 + Î²Ï

Râ€²(Â·, Î¸)

â‰¤N
2 log
 1 + Î»
1 âˆ’Î»
!
Ï

râ€²(Â·, 	Î¸)

âˆ’N
2 log(1 âˆ’Î»2)Ï

mâ€²(Â·, 	Î¸)

+ K(Ï, Ï€)
+ log

Ï€

exp

âˆ’N
2 log

1+Î²
1âˆ’Î²

râ€²(Â·, 	Î¸ )

âˆ’N
2 log(1 âˆ’Î²2)mâ€²(Â·, 	Î¸ )

+ N
2 log

(1+Î»)(1âˆ’Î²)
(1âˆ’Î»)(1+Î²)

r(	Î¸ ) âˆ’r(Î¸)

âˆ’N
2 log

(1 âˆ’Î»2)(1 âˆ’Î²2)

mâ€²(	Î¸ , Î¸) âˆ’2 log(Ïµ).
Let us now deï¬ne for simplicity the posterior Î½ : Î© â†’M1
+(Î˜) by the identity
dÎ½
dÏ€ (Î¸) =
exp

âˆ’N
2 log

1+Î»
1âˆ’Î»

râ€²(Î¸, 	Î¸) + N
2 log(1 âˆ’Î»2)mâ€²(Î¸, 	Î¸)

Ï€

exp

âˆ’N
2 log

1+Î»
1âˆ’Î»

râ€²(Â·, 	Î¸) + N
2 log(1 âˆ’Î»2)mâ€²(Â·, 	Î¸)
.
Let us also introduce the random bound
B = 1
N log

Î½

exp

N
2 log

(1+Î»)(1âˆ’Î²)
(1âˆ’Î»)(1+Î²)

râ€²(Â·, 	Î¸)

1.4.
Relative bounds
47
âˆ’N
2 log

(1 âˆ’Î»2)(1 âˆ’Î²2)

mâ€²(Â·, 	Î¸ )

+ sup
Î¸âˆˆÎ˜1
1
2 log

(1âˆ’Î»)(1+Î²)
(1+Î»)(1âˆ’Î²)

râ€²(Î¸, 	Î¸ )
âˆ’1
2 log

(1 âˆ’Î»2)(1 âˆ’Î²2)

mâ€²(Î¸, 	Î¸ ) âˆ’2
N log(Ïµ).
Theorem 1.4.12. Using the above notation, for any real constants 0 â‰¤Î² < Î» < 1,
for any prior distribution Ï€ âˆˆM1
+(Î˜), for any subset Î˜1 âŠ‚Î˜, with P probability at
least 1 âˆ’Ïµ, for any posterior distribution Ï : Î© â†’M1
+(Î˜),
âˆ’log

1 âˆ’Î»

Ï(R) âˆ’inf
Î˜1 R

1 + Î²

Ï(R) âˆ’inf
Î˜1 R

â‰¤K(Ï, Î½)
N
+ B.
Therefore,
Ï(R) âˆ’inf
Î˜1 R
â‰¤Î» âˆ’Î²
2Î»Î²
"$
1 + 4
Î»Î²
(Î» âˆ’Î²)2

1 âˆ’exp
 
âˆ’B âˆ’K(Ï, Î½)
N
!
âˆ’1
#
â‰¤
1
Î» âˆ’Î²
 
B + K(Ï, Î½)
N
!
.
Let us deï¬ne the posterior 	Î½ by the identity
d	Î½
dÏ€ (Î¸) =
exp

âˆ’N
2 log

1+Î²
1âˆ’Î²

râ€²(Î¸, 	Î¸) âˆ’N
2 log(1 âˆ’Î²2)mâ€²(Î¸, 	Î¸)

Ï€

exp

âˆ’N
2 log

1+Î²
1âˆ’Î²

râ€²(Â·, 	Î¸) âˆ’N
2 log(1 âˆ’Î²2)mâ€²(Â·, 	Î¸)
.
It is useful to remark that
1
N log

Î½

exp
N
2 log
(1 + Î»)(1 âˆ’Î²)
(1 âˆ’Î»)(1 + Î²)

râ€²(Â·, 	Î¸)
âˆ’N
2 log

(1 âˆ’Î»2)(1 âˆ’Î²2)

mâ€²(Â·, 	Î¸)

â‰¤	Î½
1
2 log
(1 + Î»)(1 âˆ’Î²)
(1 âˆ’Î»)(1 + Î²)

râ€²(Â·, 	Î¸)
âˆ’1
2 log

(1 âˆ’Î»2)(1 âˆ’Î²2)

mâ€²(Â·, 	Î¸)

.
This inequality is a special case of
log

Ï€

exp(g)

âˆ’log

Ï€

exp(h)

=
 1
Î±=0
Ï€exp[h+Î±(gâˆ’h)](g âˆ’h)dÎ± â‰¤Ï€exp(g)(g âˆ’h),
which is a consequence of the convexity of Î± â†’log

Ï€

exp

h + Î±(g âˆ’h)

.

48
Chapter 1.
Inductive PAC-Bayesian learning
Let us introduce as previously Ï•(x) = supÎ¸âˆˆÎ˜ mâ€²(Î¸, 	Î¸) âˆ’x râ€²(Î¸, 	Î¸), x âˆˆR+. Let
us moreover consider Ï•(x) = supÎ¸âˆˆÎ˜1 mâ€²(Î¸, 	Î¸) âˆ’x râ€²(Î¸, 	Î¸), x âˆˆR+. These functions
can be used to produce a result which is slightly weaker, but maybe easier to read
and understand. Indeed, we see that, for any x âˆˆR+, with P probability at least
1 âˆ’Ïµ, for any posterior distribution Ï,
âˆ’N log

1 âˆ’Î»Ï

Râ€²(Â·, Î¸)

1 + Î²Ï

Râ€²(Â·, Î¸)

â‰¤N
2 log

(1 + Î»)
(1 âˆ’Î»)(1 âˆ’Î»2)x

Ï

râ€²(Â·, 	Î¸)

âˆ’N
2 log

(1 âˆ’Î»2)(1 âˆ’Î²2)

Ï•(x) + K(Ï, Ï€)
+ log

Ï€

exp

âˆ’N
2 log

(1+Î²)
(1âˆ’Î²)(1âˆ’Î²2)x

râ€²(Â·, 	Î¸)

âˆ’N
2 log

(1 âˆ’Î»2)(1 âˆ’Î²2)

Ï•
âŽ›
âŽ
log

(1+Î»)(1âˆ’Î²)
(1âˆ’Î»)(1+Î²)

âˆ’log [(1 âˆ’Î»2)(1 âˆ’Î²2)]
âŽž
âŽ 
âˆ’2 log(Ïµ)
=

N
2 log
(1+Î»)
(1âˆ’Î»)(1âˆ’Î»2)x

N
2 log
(1+Î²)
(1âˆ’Î²)(1âˆ’Î²2)x
 Ï€exp(âˆ’Î±r)

râ€²(Â·, 	Î¸)

dÎ±
+ K(Ï, Ï€exp{âˆ’N
2 log[
(1+Î»)
(1âˆ’Î»)(1âˆ’Î»2)x ]r}) âˆ’2 log(Ïµ)
âˆ’N
2 log

(1 âˆ’Î»2)(1 âˆ’Î²2)

âŽ¡
âŽ£Ï•(x) + Ï•
âŽ›
âŽ
log

(1+Î»)(1âˆ’Î²)
(1âˆ’Î»)(1+Î²)

âˆ’log[(1 âˆ’Î»2)(1 âˆ’Î²2)]
âŽž
âŽ 
âŽ¤
âŽ¦.
Theorem 1.4.13. With the previous notation, for any real constants 0 â‰¤Î² < Î» <
1, for any positive real constant x, for any prior probability distribution Ï€ âˆˆM1
+(Î˜),
for any subset Î˜1 âŠ‚Î˜, with P probability at least 1âˆ’Ïµ, for any posterior distribution
Ï : Î© â†’M1
+(Î˜), putting
B(Ï) =
1
N(Î» âˆ’Î²)

N
2 log
(1+Î»)
(1âˆ’Î»)(1âˆ’Î»2)x

N
2 log
(1+Î²)
(1âˆ’Î²)(1âˆ’Î²2)x
 Ï€exp(âˆ’Î±r)

râ€²(Â·, 	Î¸)

dÎ±
+
K(Ï, Ï€exp{âˆ’N
2 log[
(1+Î»)
(1âˆ’Î»)(1âˆ’Î»2)x ]r}) âˆ’2 log(Ïµ)
N(Î» âˆ’Î²)
âˆ’
1
2(Î» âˆ’Î²) log

(1 âˆ’Î»2)(1 âˆ’Î²2)

âŽ¡
âŽ£Ï•(x) + Ï•
âŽ›
âŽ
log

(1+Î»)(1âˆ’Î²)
(1âˆ’Î»)(1+Î²)

âˆ’log[(1 âˆ’Î»2)(1 âˆ’Î²2)]
âŽž
âŽ 
âŽ¤
âŽ¦
â‰¤
1
N(Î» âˆ’Î²)de log
âŽ›
âŽ
log

(1+Î»)
(1âˆ’Î»)(1âˆ’Î»2)x

log

(1+Î²)
(1âˆ’Î²)(1âˆ’Î²2)x

âŽž
âŽ 
+
K(Ï, Ï€exp{âˆ’N
2 log[
(1+Î»)
(1âˆ’Î»)(1âˆ’Î»2)x ]r}) âˆ’2 log(Ïµ)
N(Î» âˆ’Î²)

1.4.
Relative bounds
49
âˆ’
1
2(Î» âˆ’Î²) log

(1 âˆ’Î»2)(1 âˆ’Î²2)

âŽ¡
âŽ£Ï•(x) + Ï•
âŽ›
âŽ
log

(1+Î»)(1âˆ’Î²)
(1âˆ’Î»)(1+Î²)

âˆ’log[(1 âˆ’Î»2)(1 âˆ’Î²2)]
âŽž
âŽ 
âŽ¤
âŽ¦,
the following bounds hold true:
Ï(R) âˆ’inf
Î˜1 R
â‰¤Î» âˆ’Î²
2Î»Î²
"$
1 +
4Î»Î²
(Î» âˆ’Î²)2

1 âˆ’exp

âˆ’(Î» âˆ’Î²)B(Ï)

âˆ’1
#
â‰¤B(Ï).
Let us remark that this alternative way of handling relative deviation bounds
made it possible to carry on with non-linear bounds up to the ï¬nal result. For
instance, if Î» = 0.5, Î² = 0.2 and B(Ï) = 0.1, the non-linear bound gives Ï(R) âˆ’
infÎ˜1 R â‰¤0.096.

50
Chapter 1.
Inductive PAC-Bayesian learning

Chapter 2
Comparing posterior
distributions to Gibbs priors
2.1. Bounds relative to a Gibbs distribution
We now come to an approach to relative bounds whose performance can be
analysed with PAC-Bayesian tools.
The empirical bounds at the end of the previous chapter involve taking suprema
in Î¸ âˆˆÎ˜, and replacing the expected margin function Ï• with some empirical coun-
terparts Ï• or Ï•, which may prove unsafe when using very complex classiï¬cation
models.
We are now going to focus on the control of the divergence K

Ï, Ï€exp(âˆ’Î²R)

. It
is already obvious, we hope, that controlling this divergence is the crux of the
matter, and that it is a way to upper bound the mutual information between
the training sample and the parameter, which can be expressed as K

Ï, P(Ï)

=
K

Ï, Ï€exp(âˆ’Î²R)

âˆ’K

P(Ï), Ï€exp(âˆ’Î²R)

, as explained on page 14.
Through the identity
(2.1)
K

Ï, Ï€exp(âˆ’Î²R)

= Î²

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

+ K(Ï, Ï€) âˆ’K

Ï€exp(âˆ’Î²R), Ï€

,
we see that the control of this divergence is related to the control of the diï¬€erence
Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R). This is the route we will follow ï¬rst.
Thus comparing any posterior distribution with a Gibbs prior distribution will
provide a ï¬rst way to build an estimator which can be proved to reach adaptively
the best possible asymptotic error rate under Mammen and Tsybakov margin as-
sumptions and parametric complexity assumptions (at least as long as orders of
magnitude are concerned, we will not discuss the question of asymptotically opti-
mal constants).
Then we will provide an empirical bound for the Kullback divergence K

Ï,
Ï€exp(âˆ’Î²R)

itself. This will serve to address the question of model selection, which
will be achieved by comparing the performance of two posterior distributions possi-
bly supported by two diï¬€erent models. This will also provide a second way to build
estimators which can be proved to be adaptive under Mammen and Tsybakov mar-
gin assumptions and parametric complexity assumptions (somewhat weaker than
with the ï¬rst method).
51

52
Chapter 2.
Comparing posterior distributions to Gibbs priors
Finally, we will present two-step localization strategies, in which the performance
of the posterior distribution to be analysed is compared with a two-step Gibbs prior.
2.1.1. Comparing a posterior distribution with a Gibbs prior.
Similarly
to Theorem 1.4.3 (page 37) we can prove that for any prior distribution Ï€ âˆˆM1
+(Î˜),
(2.2)
P

Ï€ âŠ—Ï€

exp

âˆ’N log(1 âˆ’N tanh

 Î³
N

Râ€²)
âˆ’Î³râ€² âˆ’N log

cosh( Î³
N )

mâ€²

â‰¤1.
Replacing Ï€ with Ï€exp(âˆ’Î²R) and considering the posterior distribution ÏâŠ—Ï€exp(âˆ’Î²R),
provides a starting point in the comparison of Ï with Ï€exp(âˆ’Î²R); we can indeed state
with P probability at least 1 âˆ’Ïµ that
(2.3)
âˆ’N log

1 âˆ’tanh

 Î³
N

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

â‰¤Î³

Ï(r) âˆ’Ï€exp(âˆ’Î²R)(r)

+ N log

cosh( Î³
N )

Ï âŠ—Ï€exp(âˆ’Î²R)

(mâ€²)
+ K

Ï, Ï€exp(âˆ’Î²R)

âˆ’log(Ïµ).
Using equation (2.1, page 51) to handle the entropy term, we get
(2.4)
âˆ’N log

1 âˆ’tanh( Î³
N )

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

âˆ’Î²

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

â‰¤Î³

Ï(r) âˆ’Ï€exp(âˆ’Î²R)(r)

+ N log

cosh

 Î³
N

Ï âŠ—Ï€exp(âˆ’Î²R)(mâ€²)
+ K(Ï, Ï€) âˆ’K

Ï€exp(âˆ’Î²R), Ï€

âˆ’log(Ïµ).
We can then decompose in the right-hand side Î³

Ï(r) âˆ’Ï€exp(âˆ’Î²R)(r)

into (Î³ âˆ’
Î»)

Ï(r) âˆ’Ï€exp(âˆ’Î²R)(r)

+ Î»

Ï(r) âˆ’Ï€exp(âˆ’Î²R)(r)

for some parameter Î» to be set
later on and use the fact that
Î»

Ï(r) âˆ’Ï€exp(âˆ’Î²R)(r)

+ N log

cosh( Î³
N )

Ï âŠ—Ï€exp(âˆ’Î²R)(mâ€²)
+ K(Ï, Ï€) âˆ’K

Ï€exp(âˆ’Î²R), Ï€

â‰¤Î»Ï(r) + K(Ï, Ï€) + log

Ï€

exp

âˆ’Î»r + N log

cosh( Î³
N )

Ï(mâ€²)

= K

Ï, Ï€exp(âˆ’Î»r)

+ log

Ï€exp(âˆ’Î»r)

exp

N log

cosh( Î³
N )

Ï(mâ€²)

,
to get rid of the appearance of the unobserved Gibbs prior Ï€exp(âˆ’Î²R) in most places
of the right-hand side of our inequality, leading to
Theorem 2.1.1.
For any real constants Î² and Î³, with P probability at least 1 âˆ’Ïµ,
for any posterior distribution Ï : Î© â†’M1
+(Î˜), for any real constant Î»,

N tanh( Î³
N ) âˆ’Î²

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

â‰¤âˆ’N log

1 âˆ’tanh( Î³
N )

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

âˆ’Î²

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

â‰¤(Î³ âˆ’Î»)

Ï(r) âˆ’Ï€exp(âˆ’Î²R)(r)

+ K

Ï, Ï€exp(âˆ’Î»r)


2.1.
Bounds relative to a Gibbs distribution
53
+ log

Ï€exp(âˆ’Î»r)

exp

N log

cosh( Î³
N )

Ï(mâ€²)

âˆ’log(Ïµ)
= K

Ï, Ï€exp(âˆ’Î³r)

+ log

Ï€exp(âˆ’Î³r)

exp

(Î³ âˆ’Î»)r + N log

cosh( Î³
N )

Ï(mâ€²)

âˆ’(Î³ âˆ’Î»)Ï€exp(âˆ’Î²R)(r) âˆ’log(Ïµ).
We would like to have a fully empirical upper bound even in the case when Î» Ì¸= Î³.
This can be done by using the theorem twice. We will need a lemma.
Lemma 2.1.2 For any probability distribution Ï€ âˆˆM1
+(Î˜), for any bounded mea-
surable functions g, h : Î˜ â†’R,
Ï€exp(âˆ’g)(g) âˆ’Ï€exp(âˆ’h)(g) â‰¤Ï€exp(âˆ’g)(h) âˆ’Ï€exp(âˆ’h)(h).
Proof. Let us notice that
0 â‰¤K(Ï€exp(âˆ’g), Ï€exp(âˆ’h)) = Ï€exp(âˆ’g)(h) + log

Ï€

exp(âˆ’h)

+ K(Ï€exp(âˆ’g), Ï€)
= Ï€exp(âˆ’g)(h) âˆ’Ï€exp(âˆ’h)(h) âˆ’K(Ï€exp(âˆ’h), Ï€) + K(Ï€exp(âˆ’g), Ï€)
= Ï€exp(âˆ’g)(h) âˆ’Ï€exp(âˆ’h)(h) âˆ’K(Ï€exp(âˆ’h), Ï€) âˆ’Ï€exp(âˆ’g)(g) âˆ’log

Ï€

exp(âˆ’g)

.
Moreover
âˆ’log

Ï€

exp(âˆ’g)

â‰¤Ï€exp(âˆ’h)(g) + K(Ï€exp(âˆ’h), Ï€),
which ends the proof. â–¡
For any positive real constants Î² and Î», we can then apply Theorem 2.1.1 to
Ï = Ï€exp(âˆ’Î»r), and use the inequality
(2.5)
Î»
Î²

Ï€exp(âˆ’Î»r)(r) âˆ’Ï€exp(âˆ’Î²R)(r)

â‰¤Ï€exp(âˆ’Î»r)(R) âˆ’Ï€exp(âˆ’Î²R)(R)
provided by the previous lemma. We thus obtain with P probability at least 1 âˆ’Ïµ
âˆ’N log

1 âˆ’tanh( Î³
N ) Î»
Î²

Ï€exp(âˆ’Î»r)(r) âˆ’Ï€exp(âˆ’Î²R)(r)

âˆ’Î³

Ï€exp(âˆ’Î»r)(r) âˆ’Ï€exp(âˆ’Î²R)(r)

â‰¤log

Ï€exp(âˆ’Î»r)

exp

N log

cosh( Î³
N )

Ï€exp(âˆ’Î»r)(mâ€²)

âˆ’log(Ïµ).
Let us introduce the convex function
FÎ³,Î±(x) = âˆ’N log

1 âˆ’tanh( Î³
N )x

âˆ’Î±x â‰¥

N tanh( Î³
N ) âˆ’Î±

x.
With P probability at least 1 âˆ’Ïµ,
âˆ’Ï€exp(âˆ’Î²R)(r) â‰¤inf
Î»âˆˆRâˆ—
+

âˆ’Ï€exp(âˆ’Î»r)(r)
+ Î²
Î»F âˆ’1
Î³, Î²Î³
Î»

log

Ï€exp(âˆ’Î»r)

exp

N log

cosh( Î³
N )

Ï€exp(âˆ’Î»r)(mâ€²)

âˆ’log(Ïµ)

.
Since Theorem 2.1.1 holds uniformly for any posterior distribution Ï, we can apply
it again to some arbitrary posterior distribution Ï. We can moreover make the result
uniform in Î² and Î³ by considering some atomic measure Î½ âˆˆM1
+(R) on the real
line and using a union bound. This leads to

54
Chapter 2.
Comparing posterior distributions to Gibbs priors
Theorem 2.1.3.
For any atomic probability distribution on the positive real line
Î½ âˆˆM1
+(R+), with P probability at least 1 âˆ’Ïµ, for any posterior distribution Ï :
Î© â†’M1
+(Î˜), for any positive real constants Î² and Î³,

N tanh( Î³
N ) âˆ’Î²

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

â‰¤FÎ³,Î²

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

â‰¤B(Ï, Î², Î³), where
B(Ï, Î², Î³) =
inf
Î»1âˆˆR+,Î»1â‰¤Î³
Î»2âˆˆR,Î»2> Î²Î³
N tanh( Î³
N )âˆ’1

K

Ï, Ï€exp(âˆ’Î»1r)

+ (Î³ âˆ’Î»1)

Ï(r) âˆ’Ï€exp(âˆ’Î»2r)(r)

+ log

Ï€exp(âˆ’Î»1r)

exp

N log

cosh( Î³
N )

Ï(mâ€²)

âˆ’log

ÏµÎ½(Î²)Î½(Î³)

+ (Î³ âˆ’Î»1) Î²
Î»2
F âˆ’1
Î³, Î²Î³
Î»2

log

Ï€exp(âˆ’Î»2r)

exp

N log

cosh( Î³
N )

Ï€exp(âˆ’Î»2r)(mâ€²)

âˆ’log

ÏµÎ½(Î²)Î½(Î³)

â‰¤
inf
Î»1âˆˆR+,Î»1â‰¤Î³
Î»2âˆˆR,Î»2> Î²Î³
N tanh( Î³
N )âˆ’1

K

Ï, Ï€exp(âˆ’Î»1r)

+ (Î³ âˆ’Î»1)

Ï(r) âˆ’Ï€exp(âˆ’Î»2r)(r)

+ log

Ï€exp(âˆ’Î»1r)

exp

N log

cosh( Î³
N )

Ï(mâ€²)

+ Î²
Î»2
(1 âˆ’Î»1
Î³ )
 N
Î³ tanh( Î³
N ) âˆ’Î²
Î»2
 log

Ï€exp(âˆ’Î»2r)

exp

N log

cosh( Î³
N )

Ï€exp(âˆ’Î»2r)(mâ€²)

âˆ’

1 + Î²
Î»2
(1âˆ’Î»1
Î³ )
[ N
Î³ tanh( Î³
N )âˆ’Î²
Î»2 ]

log

ÏµÎ½(Î²)Î½(Î³)


,
where we have written for short Î½(Î²) and Î½(Î³) instead of Î½({Î²}) and Î½({Î³}).
Let us notice that B(Ï, Î², Î³) = +âˆžwhen Î½(Î²) = 0 or Î½(Î³) = 0, the uniformity
in Î² and Î³ of the theorem therefore necessarily bears on a countable number of
values of these parameters. We can typically choose distributions for Î½ such as the
one used in Theorem 1.2.8 (page 13): namely we can put for some positive real ratio
Î± > 1
Î½(Î±k) =
1
(k + 1)(k + 2),
k âˆˆN,
or alternatively, since we are interested in values of the parameters less than N, we
can prefer
Î½(Î±k) =
log(Î±)
log(Î±N),
0 â‰¤k < log(N)
log(Î±) .
We can also use such a coding distribution on dyadic numbers as the one deï¬ned
by equation (1.7, page 15).

2.1.
Bounds relative to a Gibbs distribution
55
Following the same route as for Theorem 1.3.15 (page 30), we can also prove the
following result about the deviations under any posterior distribution Ï:
Theorem 2.1.4 For any Ïµ âˆˆ)0, 1(, with P probability at least 1âˆ’Ïµ, for any posterior
distribution Ï : Î© â†’M1
+(Î˜), with Ï probability at least 1 âˆ’Î¾,
FÎ³,Î²

R(	Î¸) âˆ’Ï€exp(âˆ’Î²R)(R)

â‰¤
inf
Î»1âˆˆR+,Î»1â‰¤Î³,
Î»2âˆˆR,Î»2> Î²Î³
N tanh( Î³
N )âˆ’1

log
&
dÏ
dÏ€exp(âˆ’Î»1r)
(	Î¸ )
'
+ (Î³ âˆ’Î»1)

r(	Î¸ ) âˆ’Ï€exp(âˆ’Î»2r)(r)

+ log

Ï€exp(âˆ’Î»1r)

exp

N log

cosh( Î³
N )

mâ€²(Â·, 	Î¸ )

âˆ’log

ÏµÎ¾Î½(Î²)Î½(Î³)

+ (Î³ âˆ’Î»1) Î²
Î»2
F âˆ’1
Î³, Î²Î³
Î»2

log

Ï€exp(âˆ’Î»2r)

exp

N log

cosh( Î³
N )

Ï€exp(âˆ’Î»2r)(mâ€²)

âˆ’log

ÏµÎ½(Î²)Î½(Î³)

.
The only tricky point is to justify that we can still take an inï¬mum in Î»1 without
using a union bound. To justify this, we have to notice that the following variant of
Theorem 2.1.1 (page 52) holds: with P probability at least 1 âˆ’Ïµ, for any posterior
distribution Ï : Î© â†’M1
+(Î˜), for any real constant Î»,
Ï

FÎ³,Î²

R âˆ’Ï€exp(âˆ’Î²R)(R)

â‰¤K

Ï, Ï€exp(âˆ’Î³r)

+ Ï

inf
Î»âˆˆR log

Ï€exp(âˆ’Î³r)

exp

(Î³ âˆ’Î»)r + N log

cosh( Î³
N

mâ€²(Â·, 	Î¸ )

âˆ’(Î³ âˆ’Î»)Ï€exp(âˆ’Î²R)(r)

âˆ’log(Ïµ).
We leave the details as an exercise.
2.1.2. The effective temperature of a posterior distribution.
Using
the parametric approximation Ï€exp(âˆ’Î±r)(r) âˆ’infÎ˜ r â‰ƒ
de
Î± , we get as an order of
magnitude
B(Ï€exp(âˆ’Î»1r), Î², Î³) â‰²âˆ’(Î³ âˆ’Î»1)de

Î»âˆ’1
2
âˆ’Î»âˆ’1
1

+ 2de log
Î»1
Î»1 âˆ’N log

cosh( Î³
N )

x
+ 2 Î²
Î»2
(1 âˆ’Î»1
Î³ )
 N
Î³ tanh( Î³
N ) âˆ’Î²
Î»2
de log
"
Î»2
Î»2 âˆ’N log

cosh( Î³
N )

x
#
+ 2N log

cosh( Î³
N )

1 + Î²
Î»2
(1 âˆ’Î»1
Î³ )
 N
Î³ tanh( Î³
N ) âˆ’Î²
Î»2


Ï•(x)
âˆ’

1 + Î²
Î»2
(1 âˆ’Î»1
Î³ )
[ N
Î³ tanh( Î³
N ) âˆ’Î²
Î»2 ]

log

Î½(Î²)Î½(Î³)Ïµ

.

56
Chapter 2.
Comparing posterior distributions to Gibbs priors
Therefore, if the empirical dimension de stays bounded when N increases, we are
going to obtain a negative upper bound for any values of the constants Î»1 > Î»2 > Î²,
as soon as Î³ and N
Î³ are chosen to be large enough. This ability to obtain negative
values for the bound B(Ï€exp(âˆ’Î»1r), Î³, Î²), and more generally B(Ï, Î³, Î²), leads the
way to introducing the new concept of the eï¬€ective temperature of an estimator.
Definition 2.1.1 For any posterior distribution Ï : Î© â†’M1
+(Î˜) we deï¬ne the
eï¬€ective temperature T(Ï) âˆˆR âˆª{âˆ’âˆž, +âˆž} of Ï by the equation
Ï(R) = Ï€exp(âˆ’
R
T (Ï) )(R).
Note that Î² â†’Ï€exp(âˆ’Î²R)(R) : R âˆª{âˆ’âˆž, +âˆž} â†’(0, 1) is continuous and strictly
decreasing from ess supÏ€ R to ess infÏ€ R (as soon as these two bounds do not co-
incide). This shows that the eï¬€ective temperature T(Ï) is a well-deï¬ned random
variable.
Theorem 2.1.3 provides a bound for T(Ï), indeed:
Proposition 2.1.5.
Let
	Î²(Ï) = sup

Î² âˆˆR;
inf
Î³,N tanh( Î³
N )>Î² B(Ï, Î², Î³) â‰¤0

,
where B(Ï, Î², Î³) is as in Theorem 2.1.3 (page 54). Then with P probability at least
1âˆ’Ïµ, for any posterior distribution Ï : Î© â†’M1
+(Î˜), T(Ï) â‰¤	Î²(Ï)âˆ’1, or equivalently
Ï(R) â‰¤Ï€exp[âˆ’	
Î²(Ï)R](R).
This notion of eï¬€ective temperature of a (randomized) estimator Ï is interesting
for two reasons:
â€¢ the diï¬€erence Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R) can be estimated with better accuracy
than Ï(R) itself, due to the use of relative deviation inequalities, leading to
convergence rates up to 1/N in favourable situations, even when infÎ˜ R is not
close to zero;
â€¢ and of course Ï€exp(âˆ’Î²R)(R) is a decreasing function of Î², thus being able to
estimate Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R) with some given accuracy, means being able
to discriminate between values of Ï(R) with the same accuracy, although
doing so through the parametrization Î² â†’Ï€exp(âˆ’Î²R)(R), which can neither
be observed nor estimated with the same precision!
2.1.3. Analysis of an empirical bound for the effective temperature.
We are now going to launch into a mathematically rigorous analysis of the bound
B(Ï€exp(âˆ’Î»1r),Î²,Î³) provided by Theorem 2.1.3 (page 54), to show that infÏâˆˆM1
+(Î˜)
Ï€exp[âˆ’	
Î²(Ï)R](R) converges indeed to infÎ˜ R at some optimal rate in favourable sit-
uations.
It is more convenient for this purpose to use deviation inequalities involving M â€²
rather than mâ€². It is straightforward to extend Theorem 1.4.2 (page 35) to
Theorem 2.1.6. For any real constants Î² and Î³, for any prior distributions Ï€, Î¼ âˆˆ
M1
+(Î˜), with P probability at least 1 âˆ’Î·, for any posterior distribution Ï : Î© â†’
M1
+(Î˜),
Î³Ï âŠ—Ï€exp(âˆ’Î²R)

Î¨ Î³
N (Râ€², M â€²)

â‰¤Î³Ï âŠ—Ï€exp(âˆ’Î²R)(râ€²) + K(Ï, Î¼) âˆ’log(Î·).

2.1.
Bounds relative to a Gibbs distribution
57
In order to transform the left-hand side into a linear expression and in the same
time localize this theorem, let us choose Î¼ deï¬ned by its density
dÎ¼
dÏ€ (Î¸1) = Câˆ’1 exp

âˆ’Î²R(Î¸1)
âˆ’Î³

Î˜

Î¨ Î³
N

Râ€²(Î¸1, Î¸2), M â€²(Î¸1, Î¸2)

âˆ’N
Î³ sinh( Î³
N )Râ€²(Î¸1, Î¸2)

Ï€exp(âˆ’Î²R)(dÎ¸2)

,
where C is such that Î¼(Î˜) = 1. We get
K(Ï, Î¼) = Î²Ï(R) + Î³Ï âŠ—Ï€exp(âˆ’Î²R)

Î¨ Î³
N (Râ€², M â€²) âˆ’N
Î³ sinh( Î³
N )Râ€²
+ K(Ï, Ï€)
+ log

Î˜
exp

âˆ’Î²R(Î¸1)
âˆ’Î³

Î˜

Î¨ Î³
N

Râ€²(Î¸1, Î¸2), M â€²(Î¸1, Î¸2)

âˆ’N
Î³ sinh( Î³
N )Râ€²(Î¸1, Î¸2)

Ï€exp(âˆ’Î²R)(dÎ¸2)

Ï€(dÎ¸1)

= Î²

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

+ Î³Ï âŠ—Ï€exp(âˆ’Î²R)

Î¨ Î³
N (Râ€², M â€²) âˆ’N
Î³ sinh( Î³
N )Râ€²
+ K(Ï, Ï€) âˆ’K(Ï€exp(âˆ’Î²R), Ï€)
+ log

Î˜
exp

âˆ’Î³

Î˜

Î¨ Î³
N

Râ€²(Î¸1, Î¸2), M â€²(Î¸1, Î¸2)

âˆ’N
Î³ sinh( Î³
N )Râ€²(Î¸1, Î¸2)

Ï€exp(âˆ’Î²R)(dÎ¸2)

Ï€exp(âˆ’Î²R)(dÎ¸1)

.
Thus with P probability at least 1 âˆ’Î·,
(2.6)

N sinh( Î³
N ) âˆ’Î²

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

â‰¤Î³

Ï(r)âˆ’Ï€exp(âˆ’Î²R)(r)

+K(Ï, Ï€)âˆ’K(Ï€exp(âˆ’Î²R), Ï€)âˆ’log(Î·)+C(Î², Î³)
where C(Î², Î³) = log

Î˜
exp

âˆ’Î³

Î˜

Î¨ Î³
N

Râ€²(Î¸1, Î¸2), M â€²(Î¸1, Î¸2)

âˆ’N
Î³ sinh( Î³
N )Râ€²(Î¸1, Î¸2)

Ï€exp(âˆ’Î²R)(dÎ¸2)

Ï€exp(âˆ’Î²R)(dÎ¸1)

.
Remarking that
K

Ï, Ï€exp(âˆ’Î²R)

= Î²

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

+ K(Ï, Ï€) âˆ’K(Ï€exp(âˆ’Î²R), Ï€),
we deduce from the previous inequality
Theorem 2.1.7.
For any real constants Î² and Î³, with P probability at least 1âˆ’Î·,
for any posterior distribution Ï : Î© â†’M1
+(Î˜),
N sinh( Î³
N )

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

â‰¤Î³

Ï(r) âˆ’Ï€exp(âˆ’Î²R)(r)

+ K

Ï, Ï€exp(âˆ’Î²R)

âˆ’log(Î·) + C(Î², Î³).

58
Chapter 2.
Comparing posterior distributions to Gibbs priors
We can also go into a slightly diï¬€erent direction, starting back again from equa-
tion (2.6, page 57) and remarking that for any real constant Î»,
Î»

Ï(r) âˆ’Ï€exp(âˆ’Î²R)(r)

+ K(Ï, Ï€) âˆ’K(Ï€exp(âˆ’Î²R), Ï€)
â‰¤Î»Ï(r) + K(Ï, Ï€) + log

Ï€

exp(âˆ’Î»r)

= K

Ï, Ï€exp(âˆ’Î»r)

.
This leads to
Theorem 2.1.8.
For any real constants Î² and Î³, with P probability at least 1âˆ’Î·,
for any real constant Î»,

N sinh( Î³
N ) âˆ’Î²

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

â‰¤(Î³ âˆ’Î»)

Ï(r) âˆ’Ï€exp(âˆ’Î²R)(r)

+ K

Ï, Ï€exp(âˆ’Î»r)

âˆ’log(Î·) + C(Î², Î³),
where the deï¬nition of C(Î², Î³) is given by equation (2.6, page 57).
We can now use this inequality in the case when Ï = Ï€exp(âˆ’Î»r) and combine it
with Inequality (2.5, page 53) to obtain
Theorem 2.1.9 For any real constants Î² and Î³, with P probability at least 1 âˆ’Î·,
for any real constant Î»,
 NÎ»
Î² sinh( Î³
N ) âˆ’Î³

Ï€exp(âˆ’Î»r)(r) âˆ’Ï€exp(âˆ’Î²R)(r)

â‰¤C(Î², Î³) âˆ’log(Î·).
We deduce from this theorem
Proposition 2.1.10 For any real positive constants Î²1, Î²2 and Î³, with P probabil-
ity at least 1 âˆ’Î·, for any real constants Î»1 and Î»2, such that Î»2 < Î²2
Î³
N sinh( Î³
N )âˆ’1
and Î»1 > Î²1
Î³
N sinh( Î³
N )âˆ’1,
Ï€exp(âˆ’Î»1r)(r) âˆ’Ï€exp(âˆ’Î»2r)(r) â‰¤Ï€exp(âˆ’Î²1R)(r) âˆ’Ï€exp(âˆ’Î²2R)(r)
+ C(Î²1, Î³) + log(2/Î·)
NÎ»1
Î²1 sinh( Î³
N ) âˆ’Î³
+ C(Î²2, Î³) + log(2/Î·)
Î³ âˆ’NÎ»2
Î²2 sinh( Î³
N ) .
Moreover, Ï€exp(âˆ’Î²1R) and Ï€exp(âˆ’Î²2R) being prior distributions, with P probability
at least 1 âˆ’Î·,
Î³

Ï€exp(âˆ’Î²1R)(r) âˆ’Ï€exp(âˆ’Î²2R)(r)

â‰¤Î³Ï€exp(âˆ’Î²1R) âŠ—Ï€exp(âˆ’Î²2R)

Î¨âˆ’Î³
N (Râ€², M â€²)

âˆ’log(Î·).
Hence
Proposition 2.1.11 For any positive real constants Î²1, Î²2 and Î³, with P prob-
ability at least 1 âˆ’Î·, for any positive real constants Î»1 and Î»2 such that Î»2 <
Î²2
Î³
N sinh( Î³
N )âˆ’1 and Î»1 > Î²1
Î³
N sinh( Î³
N )âˆ’1,
Ï€exp(âˆ’Î»1r)(r) âˆ’Ï€exp(âˆ’Î»2r)(r)
â‰¤Ï€exp(âˆ’Î²1R) âŠ—Ï€exp(âˆ’Î²2R)

Î¨âˆ’Î³
N (Râ€², M â€²)

+
log( 3
Î·)
Î³
+
C(Î²1, Î³) + log( 3
Î·)
NÎ»1
Î²1 sinh( Î³
N ) âˆ’Î³ +
C(Î²2, Î³) + log( 3
Î·)
Î³ âˆ’NÎ»2
Î²2 sinh( Î³
N ) .

2.1.
Bounds relative to a Gibbs distribution
59
In order to achieve the analysis of the bound B(Ï€exp(âˆ’Î»1r), Î², Î³) given by Theo-
rem 2.1.3 (page 54), it now remains to bound quantities of the general form
log

Ï€exp(âˆ’Î»r)

exp

N log

cosh( Î³
N )

Ï€exp(âˆ’Î»r)(mâ€²)

=
sup
ÏâˆˆM1
+(Î˜)
N log

cosh( Î³
N )

Ï âŠ—Ï€exp(âˆ’Î»)(mâ€²) âˆ’K

Ï, Ï€exp(âˆ’Î»r)

.
Let us consider the prior distribution Î¼ âˆˆM1
+(Î˜ Ã— Î˜) on couples of parameters
deï¬ned by the density
dÎ¼
d(Ï€ âŠ—Ï€)(Î¸1, Î¸2) = Câˆ’1 exp

âˆ’Î²R(Î¸1) âˆ’Î²R(Î¸2) + Î±Î¦âˆ’Î±
N

M â€²(Î¸1, Î¸2)

,
where the normalizing constant C is such that Î¼(Î˜Ã—Î˜) = 1. Since for ï¬xed values of
the parameters Î¸ and Î¸â€² âˆˆÎ˜, mâ€²(Î¸, Î¸â€²), like r(Î¸), is a sum of independent Bernoulli
random variables, we can easily adapt the proof of Theorem 1.1.4 on page 4, to
establish that with P probability at least 1 âˆ’Î·, for any posterior distribution Ï and
any real constant Î»,
Î±Ï âŠ—Ï€exp(âˆ’Î»r)(mâ€²) â‰¤Î±Ï âŠ—Ï€exp(âˆ’Î»r)

Î¦âˆ’Î±
N (M â€²)

+ K(Ï âŠ—Ï€exp(âˆ’Î»r), Î¼) âˆ’log(Î·)
= K

Ï, Ï€exp(âˆ’Î²R)

+ K

Ï€exp(âˆ’Î»r), Ï€exp(âˆ’Î²R)

+ log

Ï€exp(âˆ’Î²R) âŠ—Ï€exp(âˆ’Î²R)

exp

Î±Î¦âˆ’Î±
N â—¦M â€²
âˆ’log(Î·).
Thus for any real constant Î² and any positive real constants Î± and Î³, with P
probability at least 1 âˆ’Î·, for any real constant Î»,
(2.7)
log

Ï€exp(âˆ’Î»r)

exp

N log

cosh( Î³
N )

Ï€exp(âˆ’Î»r)(mâ€²)

â‰¤
sup
ÏâˆˆM1
+(Î˜)
 
N
Î± log

cosh( Î³
N )

K

Ï, Ï€exp(âˆ’Î²R)

+ K

Ï€exp(âˆ’Î»r), Ï€exp(âˆ’Î²R)

+ log

Ï€exp(âˆ’Î²R) âŠ—Ï€exp(âˆ’Î²R)

exp(Î±Î¦âˆ’Î±
N â—¦M â€²)

âˆ’log(Î·)

âˆ’K

Ï, Ï€exp(âˆ’Î»r)
!
.
To
ï¬nish,
we
need
some
appropriate
upper
bound
for
the
entropy
K

Ï, Ï€exp(âˆ’Î²R)

. This question can be handled in the following way: using The-
orem 2.1.7 (page 57), we see that for any positive real constants Î³ and Î², with P
probability at least 1 âˆ’Î·, for any posterior distribution Ï,
K

Ï, Ï€exp(âˆ’Î²R)

= Î²

Ï(R) âˆ’Ï€exp(âˆ’Î²R)(R)

+ K(Ï, Ï€) âˆ’K(Ï€exp(âˆ’Î²R), Ï€)
â‰¤
Î²
N sinh( Î³
N )

Î³

Ï(r) âˆ’Ï€exp(âˆ’Î²R)(r)

+ K

Ï, Ï€exp(âˆ’Î²R)

âˆ’log(Î·) + C(Î², Î³)

+ K(Ï, Ï€) âˆ’K(Ï€exp(âˆ’Î²R), Ï€)
â‰¤K

Ï, Ï€exp(âˆ’
Î²Î³
N sinh( Î³
N ) r)


60
Chapter 2.
Comparing posterior distributions to Gibbs priors
+
Î²
N sinh( Î³
N )

K

Ï, Ï€exp(âˆ’Î²R)

+ C(Î², Î³) âˆ’log(Î·)

.
In other words,
Theorem 2.1.12. For any positive real constants Î² and Î³ such that Î² < N Ã—
sinh( Î³
N ), with P probability at least 1 âˆ’Î·, for any posterior distribution Ï : Î© â†’
M1
+(Î˜),
K

Ï, Ï€exp(âˆ’Î²R)

â‰¤
K

Ï, Ï€exp[âˆ’Î² Î³
N sinh( Î³
N )âˆ’1r]

1 âˆ’
Î²
N sinh( Î³
N )
+ C(Î², Î³) âˆ’log(Î·)
N sinh( Î³
N )
Î²
âˆ’1
,
where the quantity C(Î², Î³) is deï¬ned by equation (2.6, page 57). Equivalently, it will
be in some cases more convenient to use this result in the form: for any positive real
constants Î» and Î³, with P probability at least 1 âˆ’Î·, for any posterior distribution
Ï : Î© â†’M1
+(Î˜),
K

Ï, Ï€exp[âˆ’Î» N
Î³ sinh( Î³
N )R]

â‰¤K

Ï, Ï€exp(âˆ’Î»r)

1 âˆ’Î»
Î³
+
C(Î» N
Î³ sinh( Î³
N ), Î³) âˆ’log(Î·)
Î»
Î² âˆ’1
.
Choosing in equation (2.7, page 59) Î± = N log

cosh( Î³
N )

1 âˆ’
Î²
N sinh( Î³
N )
and Î² = Î» N
Î³ sinh( Î³
N ),
so that Î± = N log

cosh( Î³
N )

1 âˆ’Î»
Î³
, we obtain with P probability at least 1 âˆ’Î·,
log

Ï€exp(âˆ’Î»r)

exp

N log

cosh( Î³
N )

Ï€exp(âˆ’Î»r)(mâ€²)

â‰¤2Î»
Î³

C(Î², Î³) + log( 2
Î·)

+

1 âˆ’Î»
Î³

log

Ï€exp(âˆ’Î²R) âŠ—Ï€exp(âˆ’Î²R)

exp(Î±Î¦âˆ’Î±
N â—¦M â€²)

+ log( 2
Î·)

.
This proves
Proposition 2.1.13.
For any positive real constants Î» < Î³, with P probability at
least 1 âˆ’Î·,
log

Ï€exp(âˆ’Î»r)

exp

N log

cosh( Î³
N )

Ï€exp(âˆ’Î»r)(mâ€²)

â‰¤2Î»
Î³

C( NÎ»
Î³ sinh( Î³
N ), Î³) + log( 2
Î·)

+

1 âˆ’Î»
Î³

log

Ï€âŠ—2
exp[âˆ’NÎ»
Î³
sinh( Î³
N )R]

exp
 N log[cosh( Î³
N )]
1 âˆ’Î»
Î³
Î¦
âˆ’
log[cosh( Î³
N )]
1âˆ’Î»
Î³
â—¦M â€²
!
+

1 âˆ’Î»
Î³

log( 2
Î·).

2.1.
Bounds relative to a Gibbs distribution
61
We are now ready to analyse the bound B(Ï€exp(âˆ’Î»1r), Î², Î³) of Theorem 2.1.3
(page 54).
Theorem 2.1.14.
For any positive real constants Î»1, Î»2, Î²1, Î²2, Î² and Î³, such
that
Î»1 < Î³,
Î²1 < NÎ»1
Î³
sinh( Î³
N ),
Î»2 < Î³,
Î²2 > NÎ»2
Î³
sinh( Î³
N ),
Î² < NÎ»2
Î³
tanh( Î³
N ),
with P probability 1 âˆ’Î·, the bound B(Ï€exp(âˆ’Î»1r), Î², Î³) of Theorem 2.1.3 (page 54)
satisï¬es
B(Ï€exp(âˆ’Î»1r), Î², Î³)
â‰¤(Î³ âˆ’Î»1)

Ï€exp(âˆ’Î²1R) âŠ—Ï€exp(âˆ’Î²2R)

Î¨âˆ’Î³
N (Râ€², M â€²)

+
log( 7
Î·)
Î³
+
C(Î²1, Î³) + log( 7
Î·)
NÎ»1
Î²1 sinh( Î³
N ) âˆ’Î³ +
C(Î²2, Î³) + log( 7
Î·)
Î³ âˆ’NÎ»2
Î²2 sinh( Î³
N )

+ 2Î»1
Î³

C

 NÎ»1
Î³
sinh( Î³
N ), Î³

+ log( 7
Î·)

+

1 âˆ’Î»1
Î³

log

Ï€âŠ—2
exp[âˆ’NÎ»1
Î³
sinh( Î³
N )R]

exp
 
N log[cosh( Î³
N )]
1âˆ’Î»1
Î³
Î¦
âˆ’
log[cosh( Î³
N )]
1âˆ’Î»1
Î³
â—¦M â€²
!
+

1 âˆ’Î»1
Î³

log( 7
Î·) âˆ’log

Î½({Î²})Î½({Î³})Ïµ

+ (Î³ âˆ’Î»1) Î²
Î»2 F âˆ’1
Î³, Î²Î³
Î»2

2Î»2
Î³

C

 NÎ»2
Î³
sinh( Î³
N ), Î³

+ log

 7
Î·

+

1 âˆ’Î»2
Î³

log

Ï€âŠ—2
exp[âˆ’NÎ»2
Î³
sinh( Î³
N )R]

exp
 N log[cosh( Î³
N )]
1 âˆ’Î»2
Î³
Î¦
âˆ’
log[cosh( Î³
N )]
1âˆ’Î»2
Î³
â—¦M â€²
!
+

1 âˆ’Î»2
Î³

log

 7
Î·

âˆ’log

Î½({Î²})Î½({Î³})Ïµ


,
where the function C(Î², Î³) is deï¬ned by equation (2.6, page 57).
2.1.4. Adaptation to parametric and margin assumptions.
To help un-
derstand the previous theorem, it may be useful to give linear upper-bounds to the
factors appearing in the right-hand side of the previous inequality. Introducing Î¸
such that R(Î¸) = infÎ˜ R (assuming that such a parameter exists) and remembering
that
Î¨âˆ’a(p, m) â‰¤aâˆ’1 sinh(a)p + 2aâˆ’1 sinh( a
2)2m,
a âˆˆR+,
Î¦âˆ’a(p) â‰¤aâˆ’1
exp(a) âˆ’1

p,
a âˆˆR+,

62
Chapter 2.
Comparing posterior distributions to Gibbs priors
Î¨a(p, m) â‰¥aâˆ’1 sinh(a)p âˆ’2aâˆ’1 sinh( a
2)2m,
a âˆˆR+,
M â€²(Î¸1, Î¸2) â‰¤M â€²(Î¸1, Î¸) + M â€²(Î¸2, Î¸),
Î¸1, Î¸2 âˆˆÎ˜,
M â€²(Î¸1, Î¸) â‰¤xRâ€²(Î¸1, Î¸) + Ï•(x),
x âˆˆR+, Î¸1 âˆˆÎ˜,
the last inequality being rather a consequence of the deï¬nition of Ï• than a property
of M â€², we easily see that
Ï€exp(âˆ’Î²1R) âŠ—Ï€exp(âˆ’Î²2R)

Î¨âˆ’Î³
N (Râ€², M â€²)

â‰¤N
Î³ sinh( Î³
N )

Ï€exp(âˆ’Î²1R)(R) âˆ’Ï€exp(âˆ’Î²2R)(R)

+ 2N
Î³ sinh( Î³
2N )2Ï€exp(âˆ’Î²1R) âŠ—Ï€exp(âˆ’Î²2R)(M â€²)
â‰¤N
Î³ sinh( Î³
N )

Ï€exp(âˆ’Î²1R)(R) âˆ’Ï€exp(âˆ’Î²2R)(R)

+ 2xN
Î³
sinh( Î³
2N )2
Ï€exp(âˆ’Î²1R)

Râ€²(Â·, Î¸)

+ Ï€exp(âˆ’Î²2R)

Râ€²(Â·, Î¸)

+ 4N
Î³ sinh( Î³
2N )2Ï•(x),
that
C(Î², Î³) â‰¤log

Ï€exp(âˆ’Î²R)

exp

2N sinh

 Î³
2N
2Ï€exp(âˆ’Î²R)(M â€²)

â‰¤log

Ï€exp(âˆ’Î²R)

exp

2N sinh

 Î³
2N
2M â€²(Â·, Î¸)

+ 2N sinh( Î³
2N )2Ï€exp(âˆ’Î²R)

M â€²(Â·, Î¸)

â‰¤log

Ï€exp(âˆ’Î²R)

exp

2xN sinh( Î³
2N )2Râ€²(Â·, Î¸)

+ 2xN sinh( Î³
2N )2Ï€exp(âˆ’Î²R)

Râ€²(Â·, Î¸)

+ 4N sinh( Î³
2N )2Ï•(x)
=
 Î²
Î²âˆ’2xN sinh( Î³
2N )2 Ï€exp(âˆ’Î±R)

Râ€²(Â·, Î¸)

dÎ±
+ 2xN sinh( Î³
2N )2Ï€exp(âˆ’Î²R)

Râ€²(Â·, Î¸)

+ 4N sinh( Î³
2N )2Ï•(x)
â‰¤4xN sinh( Î³
2N )2Ï€exp[âˆ’(Î²âˆ’2xN sinh( Î³
2N )2)R]

Râ€²(Â·, Î¸)

+ 4N sinh( Î³
2N )2Ï•(x),
and that
log

Ï€âŠ—2
exp(âˆ’Î²R)

exp

NÎ±Î¦âˆ’Î±â—¦M â€²
â‰¤2 log

Ï€exp(âˆ’Î²R)

exp

N

exp(Î±) âˆ’1

M â€²(Â·, Î¸)

â‰¤2xN

exp(Î±) âˆ’1

Ï€exp[âˆ’(Î²âˆ’xN[exp(Î±)âˆ’1])R]

Râ€²(Â·, Î¸)

+ 2xN

exp(Î±) âˆ’1

Ï•(x).
Let us push further the investigation under the parametric assumption that for
some positive real constant d
(2.8)
lim
Î²â†’+âˆžÎ²Ï€exp(âˆ’Î²R)

Râ€²(Â·, Î¸)

= d,

2.1.
Bounds relative to a Gibbs distribution
63
This assumption will for instance hold true with d = n
2 when R : Î˜ â†’(0, 1) is a
smooth function deï¬ned on a compact subset Î˜ of Rn that reaches its minimum
value on a ï¬nite number of non-degenerate (i.e. with a positive deï¬nite Hessian)
interior points of Î˜, and Ï€ is absolutely continuous with respect to the Lebesgue
measure on Î˜ and has a smooth density.
In case of assumption (2.8), if we restrict ourselves to suï¬ƒciently large values of
the constants Î², Î²1, Î²2, Î»1, Î»2 and Î³ (the smaller of which is as a rule Î², as we
will see), we can use the fact that for some (small) positive constant Î´, and some
(large) positive constant A,
(2.9)
d
Î±(1 âˆ’Î´) â‰¤Ï€exp(âˆ’Î±R)

Râ€²(Â·, Î¸)

â‰¤d
Î±(1 + Î´),
Î± â‰¥A.
Under this assumption,
Ï€exp(âˆ’Î²1R) âŠ—Ï€exp(âˆ’Î²2R)

Î¨âˆ’Î³
N (Râ€², M â€²)

â‰¤N
Î³ sinh( Î³
N )
 d
Î²1 (1 + Î´) âˆ’d
Î²2 (1 âˆ’Î´)

+ 2xN
Î³
sinh( Î³
2N )2(1 + Î´)
 d
Î²1 + d
Î²2

+ 4N
Î³ sinh( Î³
2N )2Ï•(x).
C(Î², Î³) â‰¤d(1 + Î´) log

Î²
Î²âˆ’2xN sinh( Î³
2N )2

+ 2xN sinh( Î³
2N )2 (1+Î´)d
Î²
+ 4N sinh( Î³
2N )2Ï•(x).
log

Ï€âŠ—2
exp(âˆ’Î²R)

exp

NÎ±Î¦âˆ’Î±â—¦M â€²
â‰¤2xN

exp(Î±) âˆ’1

d(1 + Î´)
Î² âˆ’xN[exp(Î±) âˆ’1] + 2N

exp(Î±) âˆ’1

Ï•(x).
Thus with P probability at least 1 âˆ’Î·,
B(Ï€exp(âˆ’Î»1r), Î², Î³) â‰¤âˆ’(Î³ âˆ’Î»1) N
Î³ sinh( Î³
N ) d
Î²2 (1 âˆ’Î´)
+ (Î³ âˆ’Î»1)

N
Î³ sinh( Î³
N ) (1+Î´)d
Î²1
+ 2xN
Î³
sinh( Î³
2N )2(1 + Î´)
 d
Î²1 + d
Î²2

+ 4N
Î³ sinh( Î³
2N )2Ï•(x) +
log( 7
Î·)
Î³
+
4xN sinh( Î³
2N )2
(1+Î´)d
Î²1âˆ’2xN sinh( Î³
2N )2 + 4N sinh( Î³
2N )2Ï•(x) + log( 7
Î·)
NÎ»1
Î²1 sinh( Î³
N ) âˆ’Î³
+
4xN sinh( Î³
2N )2
(1+Î´)d
Î²2âˆ’2xN sinh( Î³
2N )2 + 4N sinh( Î³
2N )2Ï•(x) + log( 7
Î·)
Î³ âˆ’NÎ»2
Î²2 sinh( Î³
N )

+ 2Î»1
Î³

4xN sinh( Î³
2N )2
(1+Î´)d
NÎ»1
Î³
sinh( Î³
N )âˆ’2xN sinh( Î³
2N )2
+ 4N sinh( Î³
2N )2Ï•(x) + log( 7
Î·)

+

1 âˆ’Î»1
Î³

2d(1 + Î´)
"
Î»1 sinh

 Î³
N

xÎ³

exp

log[cosh( Î³
N )]
1âˆ’Î»1
Î³

âˆ’1
 âˆ’1
#âˆ’1
+ 2N

exp

log[cosh( Î³
N )]
1âˆ’Î»1
Î³

âˆ’1

Ï•(x)


64
Chapter 2.
Comparing posterior distributions to Gibbs priors
+

1 âˆ’Î»1
Î³

log( 7
Î·) âˆ’log

Î½({Î²})Î½({Î³})Ïµ

+
1 âˆ’Î»1
Î³
NÎ»2
Î²Î³ tanh( Î³
N ) âˆ’1

2Î»2
Î³

4xN sinh( Î³
2N )2
(1+Î´)d
NÎ»2
Î³
sinh( Î³
N )âˆ’2xN sinh( Î³
2N )2
+ 4N sinh( Î³
2N )2Ï•(x) + log( 7
Î·)

+

1 âˆ’Î»2
Î³
&
2d(1 + Î´)
"
Î»2 sinh

 Î³
N

xÎ³

exp

log[cosh( Î³
N )]
1âˆ’Î»2
Î³

âˆ’1
 âˆ’1
#âˆ’1
+ 2N

exp
 log[cosh( Î³
N )]
1âˆ’Î»2
Î³

âˆ’1

Ï•(x)
'
+

1 âˆ’Î»2
Î³

log( 7
Î·) âˆ’log

Î½(Î²)Î½(Î³)Ïµ


.
Now let us choose for simplicity Î²2 = 2Î»2 = 4Î², Î²1 = Î»1/2 = Î³/4, and let us
introduce the notation
C1 = N
Î³ sinh( Î³
N ),
C2 = N
Î³ tanh( Î³
N ),
C3 = N 2
Î³2

exp( Î³2
N 2 ) âˆ’1

and
C4 =
2N 2(1 âˆ’2Î²
Î³ )
Î³2

exp

Î³2
2N 2(1 âˆ’2Î²
Î³ )

âˆ’1

,
to obtain
B(Ï€exp(âˆ’Î»1r), Î², Î³) â‰¤âˆ’C1Î³
8Î² (1 âˆ’Î´)d
+ C1Î³
2

4(1+Î´)d
Î³
+ x Î³
2N (1 + Î´)
 4d
Î³ +
d
4Î²

+ Î³
N Ï•(x)

+ 1
2 log

 7
Î·

+
1
2C1 âˆ’1

(1 + Î´)d

N
2xC1Î³ âˆ’1
âˆ’1
+ C1
Î³2
2N Ï•(x) + 1
2 log( 7
Î·)

+
1
2 âˆ’C1

2(1 + Î´)d

8NÎ²
xC1Î³2 âˆ’1
âˆ’1
+ C1
Î³2
N Ï•(x) + log( 7
Î·)

+ 2xÎ³(1 + Î´)d
N âˆ’xÎ³
+ C1
Î³2
N Ï•(x) + log( 7
Î·)
+ d(1 + Î´)xÎ³
N
 C1
2C3
âˆ’xÎ³
N
!âˆ’1
+ Î³2
N C3Ï•(x) +
log( 7
Î·)
2
âˆ’log

Î½(Î²)Î½(Î³)Ïµ

+

4C2 âˆ’2
âˆ’1

4Î²
Î³

xÎ³2
N C1(1 + Î´)d

2Î²C1 âˆ’xC1
Î³2
2N
âˆ’1
+ Î³2
N Ï•(x) + log( 7
Î·)

+

1 âˆ’2Î²
Î³

2d(1 + Î´)xÎ³
N
4Î²C1
Î³C4
 
1 âˆ’2Î²
Î³
!
âˆ’xÎ³
N
âˆ’1

2.1.
Bounds relative to a Gibbs distribution
65
+
Î³2
N(1 âˆ’2Î²
Î³ )
C4Ï•(x)

+

1 âˆ’2Î²
Î³

log( 7
Î·) âˆ’log

Î½(Î²)Î½(Î³)Ïµ


.
This simpliï¬es to
B(Ï€exp(âˆ’Î»1r), Î², Î³) â‰¤âˆ’C1
8 (1 âˆ’Î´)dÎ³
Î²
+ 2C1(1 + Î´)d + log( 7
Î·)

2 +
3C1
(4C1âˆ’2)(2âˆ’C1) +
1 + 2Î²
Î³
4C2 âˆ’2

âˆ’

1 +
1
4C2âˆ’2

log

Î½(Î²)Î½(Î³)Ïµ

+ (1 + Î´)dxÎ³
N

C1 +
1
2C1âˆ’1

1
2C1 âˆ’Î³x
N
âˆ’1
+ 2

1 âˆ’Î³x
N
âˆ’1
+

C1
2C3 âˆ’Î³x
N
âˆ’1
+
4C1Î²
Î³(4C2âˆ’2)

+ (1 + Î´)dxÎ³2
NÎ²

C1
16 +
2
2âˆ’C1

8
C1 âˆ’xÎ³2
NÎ²
âˆ’1
+

1 âˆ’2Î²
Î³

1
2C2âˆ’1

4C1
C4

1 âˆ’2Î²
Î³

âˆ’Î³2x
Î²N
âˆ’1
+ Î³2
N Ï•(x)

3C1
2
+
C1
4C1âˆ’2 +
C1
2âˆ’C1 + C3 +
4Î²
Î³(4C2âˆ’2) +
C4
4C2âˆ’2

.
This shows that there exist universal positive real constants A1, A2, B1, B2, B3,
and B4 such that as soon as Î³ max{x,1}
N
â‰¤A1
Î²
Î³ â‰¤A2,
B(Ï€exp(âˆ’Î»1r), Î², Î³) â‰¤âˆ’B1(1 âˆ’Î´)dÎ³
Î² + B2(1 + Î´)d
âˆ’B3 log

Î½(Î²)Î½(Î³)Ïµ Î·

+ B4
Î³2
N Ï•(x).
Thus Ï€exp(âˆ’Î»1r)(R) â‰¤Ï€exp(âˆ’Î²R)(R) â‰¤infÎ˜ R + (1+Î´)d
Î²
as soon as
Î²
Î³ â‰¤
B1
B2
(1+Î´)
(1âˆ’Î´) + B4
Î³2
N Ï•(x)âˆ’B3 log[Î½(Î²)Î½(Î³)ÏµÎ·]
(1âˆ’Î´)d
.
Choosing some real ratio Î± > 1, we can now make the above result uniform for
any
(2.10)
Î², Î³ âˆˆÎ›Î±
def
=

Î±k; k âˆˆN, 0 â‰¤k < log(N)
log(Î±)

,
by substituting Î½(Î²) and Î½(Î³) with
log(Î±)
log(Î±N) and âˆ’log(Î·) with âˆ’log(Î·) + 2 Ã—
log

log(Î±N)
log(Î±)

.
Taking Î· = Ïµ for simplicity, we can summarize our result in
Theorem 2.1.15.
There exist positive real universal constants A, B1, B2, B3
and B4 such that for any positive real constants Î± > 1, d and Î´, for any prior

66
Chapter 2.
Comparing posterior distributions to Gibbs priors
distribution Ï€ âˆˆM1
+(Î˜), with P probability at least 1 âˆ’Ïµ, for any Î², Î³ âˆˆÎ›Î± (where
Î›Î± is deï¬ned by equation (2.10) above) such that
sup
Î²â€²âˆˆR,Î²â€²â‰¥Î²
2222
Î²â€²
d

Ï€exp(âˆ’Î²â€²R)(R) âˆ’inf
Î˜ R

âˆ’1
2222 â‰¤Î´
and such that also for some positive real parameter x
Î³ max{x, 1}
N
â‰¤AÎ²
Î³
and Î²
Î³ â‰¤
B1
B2
(1+Î´)
(1âˆ’Î´) +
B4
Î³2
N Ï•(x)âˆ’2B3 log(Ïµ)+4B3 log

log(N)
log(Î±)

(1âˆ’Î´)d
,
the bound B(Ï€exp(âˆ’Î³
2 r), Î², Î³) given by Theorem 2.1.3 on page 54 in the case where we
have chosen Î½ to be the uniform probability measure on Î›Î±, satisï¬es B(Ï€exp(âˆ’Î³
2 r), Î²,
Î³) â‰¤0, proving that 	Î²(Ï€exp(âˆ’Î³
2 r)) â‰¥Î² and therefore that
Ï€exp(âˆ’Î³ r
2 )(R) â‰¤Ï€exp(âˆ’Î²R)(R) â‰¤inf
Î˜ R + (1 + Î´)d
Î²
.
What is important in this result is that we do not only bound Ï€exp(âˆ’Î³
2 r)(R),
but also B(Ï€exp(âˆ’Î³
2 r), Î², Î³), and that we do it uniformly on a grid of values of Î²
and Î³, showing that we can indeed set the constants Î² and Î³ adaptively using the
empirical bound B(Ï€exp(âˆ’Î³
2 r), Î², Î³).
Let us see what we get under the margin assumption (1.24, page 39). When
Îº = 1, we have Ï•(câˆ’1) â‰¤0, leading to
Corollary 2.1.16. Assuming that the margin assumption (1.24, page 39) is sat-
isï¬ed for Îº = 1, that R : Î˜ â†’(0, 1) is independent of N (which is the case for
instance when P = P âŠ—N), and is such that
lim
Î²â€²â†’+âˆžÎ²â€²
Ï€exp(âˆ’Î²â€²R)(R) âˆ’inf
Î˜ R

= d,
there are universal positive real constants B5 and B6 and N1 âˆˆN such that for any
N â‰¥N1, with P probability at least 1 âˆ’Ïµ
Ï€exp(âˆ’	Î³ r
2 )(R) â‰¤inf
Î˜ R + B5d
cN

1 + B6
d log
 log(N)
Ïµ
!2
,
where 	Î³ âˆˆarg maxÎ³âˆˆÎ›2 max

Î² âˆˆÎ›2; B(Ï€exp(âˆ’Î³ r
2 ), Î², Î³) â‰¤0

, where Î›2 is deï¬ned
by equation (2.10, page 65), and B is the bound of Theorem 2.1.3 (page 54).
When Îº > 1, Ï•(x) â‰¤(1 âˆ’Îºâˆ’1)

Îºcx
âˆ’
1
Îºâˆ’1 , and we can choose Î³ and x such that
Î³2
N Ï•(x) â‰ƒd to prove
Corollary 2.1.17.
Assuming that the margin assumption (1.24, page 39) is sat-
isï¬ed for some exponent Îº > 1, that R : Î˜ â†’(0, 1) is independent of N (which is
for instance the case when P = P âŠ—N), and is such that
lim
Î²â€²â†’+âˆžÎ²â€²
Ï€exp(âˆ’Î²â€²R)(R) âˆ’inf
Î˜ R

= d,
there are universal positive constants B7 and B8 and N1 âˆˆN such that for any
N â‰¥N1, with P probability at least 1 âˆ’Ïµ,
Ï€exp(âˆ’	Î³ r
2 )(R) â‰¤inf
Î˜ R + B7câˆ’
1
2Îºâˆ’1

1 + B8
d log
 log(N)
Ïµ
!
2Îº
2Îºâˆ’1  d
N
!
Îº
2Îºâˆ’1
,

2.1.
Bounds relative to a Gibbs distribution
67
where 	Î³ âˆˆarg maxÎ³âˆˆÎ›2 max

Î² âˆˆÎ›2; B(Ï€exp(âˆ’Î³ r
2 ), Î², Î³) â‰¤0

, Î›2 being deï¬ned by
equation (2.10, page 65) and B by Theorem 2.1.3 (page 54).
We ï¬nd the same rate of convergence as in Corollary 1.4.7 (page 40), but this
time, we were able to provide an empirical posterior distribution Ï€exp(âˆ’	Î³ r
2 ) which
achieves this rate adaptively in all the parameters (meaning in particular that we do
not need to know d, c or Îº). Moreover, as already mentioned, the power of N in this
rate of convergence is known to be optimal in the worst case (see Mammen et al.
(1999); Tsybakov (2004); Tsybakov et al. (2005), and more speciï¬cally in Audibert
(2004b) â€” downloadable from its authorâ€™s web page â€” Theorem 3.3, page 132).
2.1.5. Estimating the divergence of a posterior with respect to a
Gibbs prior.
Another interesting question is to estimate K

Ï, Ï€exp(âˆ’Î²R)

using
relative deviation inequalities. We follow here an idea to be found ï¬rst in (Audib-
ert, 2004b, page 93). Indeed, combining equation (2.3, page 52) with equation (2.1,
page 51), we see that for any positive real parameters Î² and Î», with P probability
at least 1 âˆ’Ïµ, for any posterior distribution Ï : Î© â†’M1
+(Î˜),
K

Ï, Ï€exp(âˆ’Î²R)

â‰¤
Î²
N tanh( Î³
N )

Î³

Ï(r) âˆ’Ï€exp(âˆ’Î²R)(r)

+ N log

cosh( Î³
N )

Ï âŠ—Ï€exp(âˆ’Î²R)(mâ€²)
+ K

Ï, Ï€exp(âˆ’Î²R)

âˆ’log(Ïµ)

+ K(Ï, Ï€) âˆ’K

Ï€exp(âˆ’Î²R), Ï€

â‰¤K

Ï, Ï€exp[âˆ’
Î²Î³
N tanh( Î³
N ) r]

+
Î²
N tanh( Î³
N )

K

Ï, Ï€exp(âˆ’Î²R)

âˆ’log(Ïµ)

+ log

Ï€exp[âˆ’
Î²Î³
N tanh( Î³
N ) r]

exp

Î²
tanh( Î³
N ) log

cosh( Î³
N )

Ï(mâ€²)

.
We thus obtain
Theorem 2.1.18.
For any positive real constants Î² and Î³ such that Î² < N Ã—
tanh( Î³
N ), with P probability at least 1 âˆ’Ïµ, for any posterior distribution Ï : Î© â†’
M1
+(Î˜),
K

Ï, Ï€exp(âˆ’Î²R)

â‰¤
 
1 âˆ’Î²
N tanh
 Î³
N
âˆ’1!âˆ’1
Ã—

K

Ï, Ï€exp[âˆ’Î²Î³
N tanh( Î³
N )âˆ’1r]

âˆ’
Î²
N tanh( Î³
N ) log(Ïµ)
+ log

Ï€exp[âˆ’Î²Î³
N tanh( Î³
N )âˆ’1r]

exp

Î² tanh( Î³
N )âˆ’1 log[cosh( Î³
N )]Ï(mâ€²)

.
This theorem provides another way of measuring over-ï¬tting, since it gives an
upper bound for K

Ï€exp[âˆ’Î²Î³
N tanh( Î³
N )âˆ’1r], Ï€exp(âˆ’Î²R)

. It may be used in combination
with Theorem 1.2.6 (page 11) as an alternative to Theorem 1.3.7 (page 21). It will
also be used in the next section.
An alternative parametrization of the same result providing a simpler right-hand
side is also useful:

68
Chapter 2.
Comparing posterior distributions to Gibbs priors
Corollary 2.1.19.
For any positive real constants Î² and Î³ such that Î² < Î³,
with P probability at least 1 âˆ’Ïµ, for any posterior distribution Ï : Î© â†’M1
+(Î˜),
K

Ï, Ï€exp[âˆ’N Î²
Î³ tanh( Î³
N )R]

â‰¤
 
1 âˆ’Î²
Î³
!âˆ’1
K

Ï, Ï€exp(âˆ’Î²r)

âˆ’Î²
Î³ log(Ïµ)
+ log

Ï€exp(âˆ’Î²r)

exp

N Î²
Î³ log

cosh( Î³
N )

Ï(mâ€²)

.
2.2. Playing with two posterior and two local prior distributions
2.2.1. Comparing two posterior distributions.
Estimating the eï¬€ective
temperature of an estimator provides an eï¬ƒcient way to tune parameters in a
model with parametric behaviour. On the other hand, it will not be ï¬tted to choose
between diï¬€erent models, especially when they are nested, because as we already
saw in the case when Î˜ is a union of nested models, the prior distribution Ï€exp(âˆ’Î²R)
does not provide an eï¬ƒcient localization of the parameter in this case, in the sense
that Ï€exp(âˆ’Î²R)(R) does not go down to infÎ˜ R at the desired rate when Î² goes to
+âˆž, requiring a resort to partial localization.
Once some estimator (in the form of a posterior distribution) has been chosen
in each sub-model, these estimators can be compared between themselves with the
help of the relative bounds that we will establish in this section. It is also possible
to choose several estimators in each sub-model, to tune parameters in the same
time (like the inverse temperature parameter if we decide to use Gibbs posterior
distributions in each sub-model).
From equation (2.2 page 52) (slightly modiï¬ed by replacing Ï€ âŠ—Ï€ with Ï€1 âŠ—Ï€2),
we easily obtain
Theorem 2.2.1.
For any positive real constant Î», for any prior distributions
Ï€1, Ï€2 âˆˆM1
+(Î˜), with P probability at least 1 âˆ’Ïµ, for any posterior distributions Ï1
and Ï2 : Î© â†’M1
+(Î˜),
âˆ’N log

1 âˆ’tanh

 Î»
N

Ï2(R) âˆ’Ï1(R)

â‰¤Î»

Ï2(r) âˆ’Ï1(r)

+ N log

cosh

 Î»
N

Ï1 âŠ—Ï2(mâ€²)
+ K

Ï1, Ï€1
+ K

Ï2, Ï€2
âˆ’log(Ïµ).
This is where the entropy bound of the previous section enters into the game,
providing a localized version of Theorem 2.2.1 (page 68). We will use the notation
(2.11)
Îža(q) = tanh(a)âˆ’1
1 âˆ’exp(âˆ’aq)

â‰¤
a
tanh(a)q,
a, q âˆˆR.
Theorem 2.2.2.
For any Ïµ âˆˆ)0, 1(, any sequence of prior distributions (Ï€i)iâˆˆN âˆˆ
M1
+(Î˜)N, any probability distribution Î¼ on N, any atomic probability distribution Î½
on R+, with P probability at least 1 âˆ’Ïµ, for any posterior distributions Ï1, Ï2 : Î© â†’
M1
+(Î˜),
Ï2(R) âˆ’Ï1(R) â‰¤B(Ï1, Ï2), where

2.2.
Playing with two posterior and two local prior distributions
69
B(Ï1, Ï2) =
inf
Î»,Î²1<Î³1,Î²2<Î³2âˆˆR+,i,jâˆˆN Îž Î»
N


Ï2(r) âˆ’Ï1(r)

+ N
Î» log

cosh( Î»
N )

Ï1 âŠ—Ï2(mâ€²)
+
1
Î»

1 âˆ’Î²1
Î³1


K

Ï1, Ï€i
exp(âˆ’Î²1r)

+ log

Ï€i
exp(âˆ’Î²1r)

exp

Î²1 N
Î³1 log

cosh( Î³1
N )

Ï1(mâ€²)

âˆ’Î²1
Î³1
log

Î½(Î³1)

+
1
Î»

1 âˆ’Î²2
Î³2


K

Ï2, Ï€j
exp(âˆ’Î²2r)

+ log

Ï€j
exp(âˆ’Î²2r)

exp

Î²2 N
Î³2 log

cosh( Î³2
N )

Ï2(mâ€²)

âˆ’Î²2
Î³2
log

Î½(Î³2)

âˆ’

 Î³1
Î²1 âˆ’1
âˆ’1 +

 Î³2
Î²2 âˆ’1
âˆ’1 + 1
log

3âˆ’1Î½(Î²1)Î½(Î²2)Î½(Î»)Î¼(i)Î¼(j)Ïµ

Î»

.
The sequence of prior distributions (Ï€i)iâˆˆN should be understood to be typically
supported by subsets of Î˜ corresponding to parametric sub-models, that is sub-
models for which it is reasonable to expect that
lim
Î²â†’+âˆžÎ²

Ï€i
exp(âˆ’Î²R)(R) âˆ’ess inf
Ï€i R

exists and is positive and ï¬nite. As there is no reason why the bound B(Ï1, Ï2) pro-
vided by the previous theorem should be sub-additive (in the sense that B(Ï1, Ï3) â‰¤
B(Ï1, Ï2)+B(Ï2, Ï3)), it is adequate to consider some workable subset P of posterior
distributions (for instance the distributions of the form Ï€i
exp(âˆ’Î²r), i âˆˆN, Î² âˆˆR+),
and to deï¬ne the sub-additive chained bound
(2.12)
B(Ï, Ïâ€²) = inf
nâˆ’1

k=0
B(Ïk, Ïk+1); n âˆˆNâˆ—, (Ïk)n
k=0 âˆˆPn+1,
Ï0 = Ï, Ïn = Ïâ€²

,
Ï, Ïâ€² âˆˆP.
Proposition 2.2.3.
With P probability at least 1 âˆ’Ïµ, for any posterior distribu-
tions Ï1, Ï2 âˆˆP, Ï2(R)âˆ’Ï1(R) â‰¤B(Ï1, Ï2). Moreover for any posterior distribution
Ï1 âˆˆP, any posterior distribution Ï2 âˆˆP such that B(Ï1, Ï2) = infÏ3âˆˆP B(Ï1, Ï3) is
unimprovable with the help of B in P in the sense that infÏ3âˆˆP B(Ï2, Ï3) â‰¥0.
Proof. The ï¬rst assertion is a direct consequence of the previous theorem, so only
the second assertion requires a proof: for any Ï3 âˆˆP, we deduce from the optimality
of Ï2 and the sub-additivity of B that
B(Ï1, Ï2) â‰¤B(Ï1, Ï3) â‰¤B(Ï1, Ï2) + B(Ï2, Ï3).
â–¡

70
Chapter 2.
Comparing posterior distributions to Gibbs priors
This proposition provides a way to improve a posterior distribution Ï1 âˆˆP by
choosing Ï2 âˆˆarg minÏâˆˆP B(Ï1, Ï) whenever B(Ï1, Ï2) < 0. This improvement is
proved by Proposition 2.2.3 to be one-step: the obtained improved posterior Ï2
cannot be improved again using the same technique.
Let us give some examples of possible starting distributions Ï1 for this improve-
ment scheme: Ï1 may be chosen as the best posterior Gibbs distribution according
to Proposition 2.1.5 (page 56). More precisely, we may build from the prior distrib-
utions Ï€i, i âˆˆN, a global prior Ï€ = 
iâˆˆN Î¼(i)Ï€i. We can then deï¬ne the estimator
of the inverse eï¬€ective temperature as in Proposition 2.1.5 (page 56) and choose
Ï1 âˆˆarg minÏâˆˆP 	Î²(Ï), where P is as suggested above the set of posterior distribu-
tions
P =

Ï€i
exp(âˆ’Î²r); i âˆˆN, Î² âˆˆR+

.
This starting point Ï1 should already be pretty good, at least in an asymptotic
perspective, the only gain in the rate of convergence to be expected bearing on
spurious log(N) factors.
2.2.2. Elaborate uses of relative bounds between posteriors.
More
elaborate uses of relative bounds are described in the third section of the second
chapter of Audibert (2004b), where an algorithm is proposed and analysed, which
allows one to use relative bounds between two posterior distributions as a stand-
alone estimation tool.
Let us give here some alternative way to address this issue. We will assume for
simplicity and without great loss of generality that the working set of posterior
distributions P is ï¬nite (so that among other things any ordering of it has a ï¬rst
element).
It is natural to deï¬ne the estimated complexity of any given posterior distribution
Ï âˆˆP in our working set as the bound for infiâˆˆN K(Ï, Ï€i) used in Theorem 2.2.1
(page 68). This leads to set (given some conï¬dence level 1 âˆ’Ïµ)
C(Ï) =
inf
Î²<Î³âˆˆR+,iâˆˆN
 
1 âˆ’Î²
Î³
!âˆ’1
K

Ï, Ï€i
exp(âˆ’Î²r)

+ log

Ï€i
exp(âˆ’Î²r)

exp

Î² N
Î³ log

cosh( Î³
N )

Ï(mâ€²)

âˆ’Î²
Î³ log

3âˆ’1Î½(Î³)Î½(Î²)Î¼(i)Ïµ

.
Let us moreover call Î³(Ï), Î²(Ï) and i(Ï) the values achieving this inï¬mum, or
nearly achieving it, which requires a slight change of the deï¬nition of C(Ï) to take
this modiï¬cation into account. For the sake of simplicity, we can assume without
substantial loss of generality that the supports of Î½ and Î¼ are large but ï¬nite, and
thus that the minimum is reached.
To understand how this notion of complexity comes into play, it may be inter-
esting to keep in mind that for any posterior distributions Ï and Ïâ€² we can write
the bound in Theorem 2.2.2 (page 68) as
(2.13)
B(Ï, Ïâ€²) = inf
Î»âˆˆR+ Îž Î»
N

Ïâ€²(r) âˆ’Ï(r) + SÎ»(Ï, Ïâ€²)

,
where
SÎ»(Ï, Ïâ€²) = SÎ»(Ïâ€², Ï) â‰¤N
Î» log

cosh( Î»
N )

Ï âŠ—Ïâ€²(mâ€²) + C(Ï) + C(Ïâ€²)
Î»
âˆ’log(3âˆ’1Ïµ)
Î»

2.2.
Playing with two posterior and two local prior distributions
71
âˆ’log

Î½

Î²(Ï)

Î¼

i(Ï)

Î»

1 âˆ’Î²(Ïâ€²)
Î³(Ïâ€²)

âˆ’log

Î½

Î²(Ïâ€²)

Î¼

i(Ïâ€²)

Î»

1 âˆ’Î²(Ï)
Î³(Ï)

âˆ’

 Î³(Ï)
Î²(Ï) âˆ’1
âˆ’1 +

 Î³(Ïâ€²)
Î²(Ïâ€²) âˆ’1
âˆ’1 + 1
log

Î½(Î»)

Î»
.
(Let us recall that the function Îž is deï¬ned by equation (2.11, page 68).) Thus for
any Ï, Ïâ€² such that B(Ïâ€², Ï) > 0, we can deduce from the monotonicity of Îž Î»
N that
Ïâ€²(r) âˆ’Ï(r) â‰¤inf
Î»âˆˆR+ SÎ»(Ï, Ïâ€²),
proving that the left-hand side is small, and consequently that B(Ï, Ïâ€²) and its
chained counterpart deï¬ned by equation (2.12, page 69) are small:
B(Ï, Ïâ€²) â‰¤B(Ï, Ïâ€²) â‰¤inf
Î»âˆˆR+ Îž Î»
N

2SÎ»(Ï, Ïâ€²)

.
It is also worth noticing that B(Ï, Ïâ€²) and B(Ï, Ïâ€²) are upper bounded in terms of
variance and complexity only.
The presence of the ratios Î³(Ï)
Î²(Ï) should not be obnoxious, since their values should
be automatically tamed by the fact that Î²(Ï) and Î³(Ï) should make the estimate
of the complexity of Ï optimal.
As an alternative, it is possible to restrict to set of parameter values Î² and Î³
such that, for some ï¬xed constant Î¶ > 1, the ratio Î³
Î² is bounded away from 1 by
the inequality Î³
Î² â‰¥Î¶. This leads to an alternative deï¬nition of C(Ï):
C(Ï) =
inf
Î³â‰¥Î¶Î²âˆˆR+,iâˆˆN
 
1 âˆ’Î²
Î³
!âˆ’1
K

Ï, Ï€i
exp(âˆ’Î²r)

+ log

Ï€i
exp(âˆ’Î²r)

exp

Î² N
Î³ log

cosh( Î³
N )

Ï(mâ€²)

âˆ’Î²
Î³ log

3âˆ’1Î½(Î³)Î½(Î²)Î¼(i)Ïµ

âˆ’log

Î½(Î²)Î¼(i)

(1 âˆ’Î¶âˆ’1)
âˆ’log(3âˆ’1Ïµ)
2
.
We can even push simpliï¬cation a step further, postponing the optimization of the
ratio Î³
Î² , and setting it to the ï¬xed value Î¶. This leads us to adopt the deï¬nition
(2.14)
C(Ï) =
inf
Î²âˆˆR+,iâˆˆN

1 âˆ’Î¶âˆ’1âˆ’1

K

Ï, Ï€i
exp(âˆ’Î²r)

+ log

Ï€i
exp(âˆ’Î²r)

exp
 N
Î¶ log

cosh( Î¶Î²
N )

Ï(mâ€²)

âˆ’Î¶ + 1
Î¶ âˆ’1

log

Î½(Î²)Î¼(i)

+ 2âˆ’1 log(3âˆ’1Ïµ)

.
With either of these modiï¬ed deï¬nitions of the complexity C(Ï), we get the upper
bound
(2.15)
SÎ»(Ï, Ïâ€²) â‰¤SÎ»(Ï, Ïâ€²)
def
= N
Î» log

cosh( Î»
N )

Ï âŠ—Ïâ€²(mâ€²)
+ 1
Î»

C(Ï) + C(Ïâ€²) âˆ’Î¶ + 1
Î¶ âˆ’1 log

Î½(Î»)

.

72
Chapter 2.
Comparing posterior distributions to Gibbs priors
With these deï¬nitions, we have for any posterior distributions Ï and Ïâ€²
B(Ï, Ïâ€²) â‰¤inf
Î»âˆˆR+ Îž Î»
N

Ïâ€²(r) âˆ’Ï(r) + SÎ»(Ï, Ïâ€²)

.
Consequently in the case when B(Ïâ€², Ï) > 0, we get
B(Ï, Ïâ€²) â‰¤B(Ï, Ïâ€²) â‰¤inf
Î»âˆˆR+ Îž Î»
N

2SÎ»(Ï, Ïâ€²)

.
To select some nearly optimal posterior distribution in P, it is appropriate to or-
der the posterior distributions of P according to increasing values of their complex-
ity C(Ï) and consider some indexation P = {Ï1, . . . , ÏM}, where C(Ïk) â‰¤C(Ïk+1),
1 â‰¤k < M.
Let us now consider for each Ïk âˆˆP the ï¬rst posterior distribution in P which
cannot be proved to be worse than Ïk according to the bound B:
(2.16)
t(k) = min

j âˆˆ{1, . . . M} : B(Ïj, Ïk) > 0

.
In this deï¬nition, which uses the chained bound deï¬ned by equation (2.12, page
69), it is appropriate to assume by convention that B(Ï, Ï) = 0, for any posterior
distribution Ï. Let us now deï¬ne our estimated best Ï âˆˆP as Ï	k, where
(2.17)
	k = min(arg max t).
Thus we take the posterior with smallest complexity which can be proved to be bet-
ter than the largest starting interval of P in terms of estimated relative classiï¬cation
error.
The following theorem is a simple consequence of the chosen optimisation scheme.
It is valid for any arbitrary choice of the complexity function Ï â†’C(Ï).
Theorem 2.2.4.
Let us put 	t = t(	k), where t is deï¬ned by equation (2.16) and 	k
is deï¬ned by equation (2.17). With P probability at least 1 âˆ’Ïµ,
Ï	k(R) â‰¤Ïj(R) +
âŽ§
âŽª
âŽª
âŽª
âŽ¨
âŽª
âŽª
âŽª
âŽ©
0,
1 â‰¤j < 	t,
B(Ïj, Ït(j)),
	t â‰¤j < 	k,
B(Ïj, Ï	t) + B(Ï	t, Ï	k),
j âˆˆ(arg max t),
B(Ïj, Ï	k),
j âˆˆ
	k + 1, . . . , M

\ (arg max t),
where the chained bound B is deï¬ned from the bound of Theorem 2.2.2 (page 68)
by equation (2.12, page 69). In the mean time, for any j such that 	t â‰¤j < 	k,
t(j) < 	t = max t, because j Ì¸âˆˆ(arg max t). Thus
Ï	k(R) â‰¤Ït(j)(R) â‰¤Ïj(R) + inf
Î»âˆˆR+ Îž Î»
N

2SÎ»(Ïj, Ït(j))

while Ït(j)(r) â‰¤Ïj(r) + inf
Î»âˆˆR+ SÎ»(Ïj, Ït(j)),
where the function Îž is deï¬ned by equation (2.11, page 68) and SÎ» is deï¬ned by
equation (2.13, page 70). For any j âˆˆ(arg max t), (including notably 	k),
B(Ï	t, Ïj) â‰¥B(Ï	t, Ïj) > 0,

2.2.
Playing with two posterior and two local prior distributions
73
B(Ïj, Ï	t) â‰¥B(Ïj, Ï	t) > 0,
so in this case
Ï	k(R) â‰¤Ïj(R) + inf
Î»âˆˆR+ Îž Î»
N

SÎ»(Ïj, Ï	t) + SÎ»(Ï	t, Ï	k) + SÎ»(Ïj, Ï	k)

,
while Ï	t(r) â‰¤Ïj(r) + inf
Î»âˆˆR+ SÎ»(Ïj, Ï	t),
Ï	k(r) â‰¤Ï	t(r) + inf
Î»âˆˆR+ SÎ»(Ï	t, Ï	k),
and Ï	t(R) â‰¤Ïj(R) + inf
Î»âˆˆR+ Îž Î»
N

2SÎ»(Ïj, Ï	t)

.
Finally in the case when j âˆˆ
	k + 1, . . . , M

\ (arg max t), due to the fact that in
particular j Ì¸âˆˆ(arg max t),
B(Ï	k, Ïj) â‰¥B(Ï	k, Ïj) > 0.
Thus in this last case
Ï	k(R) â‰¤Ïj(R) + inf
Î»âˆˆR+ Îž Î»
N

2SÎ»(Ïj, Ï	k)

,
while Ï	k(r) â‰¤Ïj(r) + inf
Î»âˆˆR+ SÎ»(Ïj, Ï	k).
Thus for any j = 1, . . . , M, Ï	k(R)âˆ’Ïj(R) is bounded from above by an empirical
quantity involving only variance and entropy terms of posterior distributions Ïâ„“such
that â„“â‰¤j, and therefore such that C(Ïâ„“) â‰¤C(Ïj). Moreover, these distributions Ïâ„“
are such that Ïâ„“(r) âˆ’Ïj(r) and Ïâ„“(R) âˆ’Ïj(R) have an empirical upper bound of
the same order as the bound stated for Ï	k(R) âˆ’Ïj(R) â€” namely the bound for
Ïâ„“(r) âˆ’Ïj(r) is in all circumstances not greater than Îžâˆ’1
Î»
N
applied to the bound
stated for Ï	k(R) âˆ’Ïj(R), whereas the bound for Ïâ„“(R) âˆ’Ïj(R) is always smaller
than two times the bound stated for Ï	k(R) âˆ’Ïj(R). This shows that variance terms
are between posterior distributions whose empirical as well as expected error rates
cannot be much larger than those of Ïj.
Let us remark that the estimation scheme described in this theorem is very
general, the same method can be used as soon as some conï¬dence interval for the
relative expected risks
âˆ’B(Ï2, Ï1) â‰¤Ï2(R) âˆ’Ï1(R) â‰¤B(Ï1, Ï2) with P probability at least 1 âˆ’Ïµ,
is available. The deï¬nition of the complexity is arbitrary, and could in an abstract
context be chosen as
C(Ï1) = inf
Ï2Ì¸=Ï1 B(Ï1, Ï2) + B(Ï2, Ï1).
Proof. The case when 1 â‰¤j < 	t is straightforward from the deï¬nitions: when
j < 	t, B(Ïj, Ï	k) â‰¤0 and therefore Ï	k(R) â‰¤Ïj(R).
In the second case, that is when 	t â‰¤j < 	k, j cannot be in arg max t, because of
the special choice of 	k in arg max t. Thus t(j) < 	t and we deduce from the ï¬rst case
that
Ï	k(R) â‰¤Ït(j)(R) â‰¤Ïj(R) + B(Ïj, Ït(j)).

74
Chapter 2.
Comparing posterior distributions to Gibbs priors
Moreover, we see from the deï¬ntion of t that B(Ït(j), Ïj) > 0, implying
Ït(j)(r) â‰¤Ïj(r) + inf
Î»âˆˆR+ SÎ»(Ïj, Ït(j)),
and therefore that
Ï	k(R) â‰¤Ïj(R) + inf
Î» Îž Î»
N

2SÎ»(Ïj, Ït(j))

.
In the third case j belongs to arg max t. In this case, we are not sure that
B(Ï	k, Ïj) > 0, and it is appropriate to involve 	t, which is the index of the ï¬rst
posterior distribution which cannot be improved by Ï	k, implying notably that
B(Ï	t, Ïk) > 0 for any k âˆˆarg max t. On the other hand, Ï	t cannot either improve
any posterior distribution Ïk with k âˆˆ(arg max t), because this would imply for any
â„“< 	t that B(Ïâ„“, Ï	t) â‰¤B(Ïâ„“, Ïk) + B(Ïk, Ï	t) â‰¤0, and therefore that t(	t) â‰¥	t + 1, in
contradiction of the fact that 	t = max t. Thus B(Ïk, Ï	t) > 0, and these two remarks
imply that
Ï	t(r) â‰¤Ïj(r) + inf
Î»âˆˆR+ SÎ»(Ïj, Ï	t),
Ï	k(r) â‰¤Ï	t(r) + inf
Î»âˆˆR+ SÎ»(Ï	t, Ï	k)
â‰¤Ïj(r) + inf
Î»âˆˆR+ SÎ»(Ïj, Ï	t) + inf
Î»âˆˆR+ SÎ»(Ï	t, Ï	k),
and consequently also that
Ï	k(R) â‰¤Ïj(R) + B(Ïj, Ï	k)
â‰¤Ïj(R) + inf
Î»âˆˆR+ Îž Î»
N

SÎ»(Ïj, Ï	t) + SÎ»(Ï	t, Ï	k) + SÎ»(Ïj, Ï	k)

and that
Ï	t(R) â‰¤Ïj(R) + inf
Î»âˆˆR+ Îž Î»
N

2SÎ»(Ïj, Ï	t)

â‰¤Ïj(R) + 2 inf
Î»âˆˆR+ 2Îž Î»
N

SÎ»(Ïj, Ï	t)

,
the last inequality being due to the fact that Îž Î»
N is a concave function. Let us
notice that it may be the case that 	k < 	t, but that only the case when j â‰¥	t is to
be considered, since otherwise we already know that Ï	k(R) â‰¤Ïj(R).
In the fourth case, j is greater than 	k, and the complexity of Ïj is larger than the
complexity of Ï	k. Moreover, j is not in arg max t, and thus B(Ï	k, Ïj) > 0, because
otherwise, the sub-additivity of B would imply that B(Ïâ„“, Ïj) â‰¤0 for any â„“â‰¤	t and
therefore that t(j) â‰¥	t = max t. Therefore
Ï	k(r) â‰¤Ïj(r) + inf
Î»âˆˆR+ SÎ»(Ïj, Ï	k),
and
Ï	k(R) â‰¤Ïj(R) + B(Ïj, Ï	k) â‰¤Ïj(R) + inf
Î»âˆˆR+ Îž Î»
N

2SÎ»(Ïj, Ï	k)

.
â–¡

2.2.
Playing with two posterior and two local prior distributions
75
2.2.3. Analysis of relative bounds.
Let us start our investigation of the the-
oretical properties of the algorithm described in Theorem 2.2.4 (page 72) by com-
puting some non-random upper bounds for B(Ï, Ïâ€²), the bound of Theorem 2.2.2
(page 68), and C(Ï), the complexity factor deï¬ned by equation (2.14, page 71), for
any Ï, Ïâ€² âˆˆP.
This analysis will be done in the case when
P =

Ï€i
exp(âˆ’Î²r) : Î½(Î²) > 0, Î¼(i) > 0

,
in which it will be possible to get some control on the randomness of any Ï âˆˆP,
in addition to controlling the other random expressions appearing in the deï¬nition
of B(Ï, Ïâ€²), Ï, Ïâ€² âˆˆP. We will also use a simpler choice of complexity function,
removing from equation (2.14 page 71) the optimization in i and Î² and using
instead the deï¬nition
(2.18)
C(Ï€i
exp(âˆ’Î²r))
def
=

1 âˆ’Î¶âˆ’1âˆ’1 log

Ï€i
exp(âˆ’Î²r)

exp

N
Î¶ log

cosh

 Î¶Î²
N

Ï€i
exp(âˆ’Î²r)(mâ€²)

+ Î¶ + 1
Î¶ âˆ’1 log

Î½(Î²)Î¼(i)

.
With this deï¬nition,
SÎ»(Ï€i
exp(âˆ’Î²r), Ï€j
exp(âˆ’Î²â€²r)) â‰¤N
Î» log

cosh( Î»
N )

Ï€i
exp(âˆ’Î²r) âŠ—Ï€j
exp(âˆ’Î²â€²r)(mâ€²)
+
C

Ï€i
exp(âˆ’Î²r)

+ C

Ï€j
exp(âˆ’Î²â€²r)

Î»
+ (Î¶ + 1)
(Î¶ âˆ’1)Î» log

3âˆ’1Î½(Î»)Ïµ

,
where SÎ» is deï¬ned by equation (2.13, page 70), so that
B

Ï€i
exp(âˆ’Î²r), Ï€j
exp(âˆ’Î²â€²r)

= inf
Î»âˆˆR+ Îž Î»
N

Ï€j
exp(âˆ’Î²â€²r)(r) âˆ’Ï€i
exp(âˆ’Î²r)(r)
+ SÎ»

Ï€i
exp(âˆ’Î²r), Ï€j
exp(âˆ’Î²r)

.
Let us successively bound the various random factors entering into the deï¬ni-
tion of B

Ï€i
exp(âˆ’Î²r), Ï€j
exp(âˆ’Î²â€²r)

. The quantity Ï€j
exp(âˆ’Î²â€²r)(r) âˆ’Ï€i
exp(âˆ’Î²r)(r) can be
bounded using a slight adaptation of Proposition 2.1.11 (page 58).
Proposition 2.2.5.
For any positive real constants Î», Î»â€² and Î³, with P probability
at least 1 âˆ’Î·, for any positive real constants Î², Î²â€² such that Î² < Î» Î³
N sinh( Î³
N )âˆ’1
and Î²â€² > Î»â€² Î³
N sinh( Î³
N )âˆ’1,
Ï€j
exp(âˆ’Î²â€²r)(r) âˆ’Ï€i
exp(âˆ’Î²r)(r)
â‰¤Ï€j
exp(âˆ’Î»â€²R) âŠ—Ï€i
exp(âˆ’Î»R)

Î¨âˆ’Î³
N (Râ€², M â€²)

+
log

 3
Î·

Î³
+
Cj(Î»â€², Î³) + log( 3
Î·)
NÎ²â€²
Î»â€² sinh( Î³
N ) âˆ’Î³
+
Ci(Î», Î³) + log( 3
Î·)
Î³ âˆ’NÎ²
Î» sinh( Î³
N )
,

76
Chapter 2.
Comparing posterior distributions to Gibbs priors
where
Ci(Î», Î³)
def
= log

Î˜
exp

âˆ’Î³

Î˜

Î¨ Î³
N

Râ€²(Î¸1, Î¸2), M â€²(Î¸1, Î¸2)

âˆ’N
Î³ sinh( Î³
N )Râ€²(Î¸1, Î¸2)

Ï€i
exp(âˆ’Î»R)(dÎ¸2)

Ï€i
exp(âˆ’Î»R)(dÎ¸1)

â‰¤log

Ï€i
exp(âˆ’Î»R)

exp

2N sinh

 Î³
2N
2Ï€i
exp(âˆ’Î»R)

M â€²
.
As for Ï€i
exp(âˆ’Î²r) âŠ—Ï€j
exp(âˆ’Î²â€²r)(mâ€²), we can write with P probability at least 1 âˆ’Î·,
for any posterior distributions Ï and Ïâ€² : Î© â†’M1
+(Î˜),
Î³Ï âŠ—Ïâ€²(mâ€²) â‰¤log

Ï€i
exp(âˆ’Î»R) âŠ—Ï€j
exp(âˆ’Î»â€²R)

exp

Î³Î¦âˆ’Î³
N (M â€²)

+ K

Ï, Ï€i
exp(âˆ’Î»R)

+ K

Ïâ€², Ï€j
exp(âˆ’Î»â€²R)

âˆ’log(Î·).
We can then replace Î» with Î² N
Î» sinh( Î»
N ) and use Theorem 2.1.12 (page 60) to get
Proposition 2.2.6.
For any positive real constants Î³, Î», Î»â€², Î² and Î²â€², with P
probability 1 âˆ’Î·,
Î³Ï âŠ—Ïâ€²(mâ€²)
â‰¤log

Ï€i
exp[âˆ’Î² N
Î» sinh( Î»
N )R] âŠ—Ï€j
exp[âˆ’Î²â€² N
Î»â€² sinh( Î»â€²
N )R]

exp

Î³Î¦âˆ’Î³
N (M â€²)

+
K

Ï, Ï€i
exp(âˆ’Î²r)

1 âˆ’Î²
Î»
+ Ci
Î² N
Î» sinh( Î»
N ), Î»

âˆ’log( Î·
3)
Î»
Î² âˆ’1
+
K

Ïâ€², Ï€j
exp(âˆ’Î²â€²r)

1 âˆ’Î²â€²
Î»â€²
+ Cj
Î²â€² N
Î»â€² sinh( Î»â€²
N ), Î»â€²
âˆ’log( Î·
3)
Î»
Î²â€² âˆ’1
âˆ’log( Î·
3).
The last random factor in B(Ï, Ïâ€²) that we need to upper bound is
log

Ï€i
exp(âˆ’Î²r)

exp

Î² N
Î³ log

cosh( Î³
N )

Ï€i
exp(âˆ’Î²r)(mâ€²)

.
A slight adaptation of Proposition 2.1.13 (page 60) shows that with P probability
at least 1 âˆ’Î·,
log

Ï€i
exp(âˆ’Î²r)

exp

Î² N
Î³ log

cosh( Î³
N )

Ï€i
exp(âˆ’Î²r)(mâ€²)

â‰¤2Î²
Î³ Ci NÎ²
Î³ sinh( Î³
N ), Î³

+

1 âˆ’Î²
Î³

log

Ï€i
exp[âˆ’NÎ²
Î³
sinh( Î³
N )R]
âŠ—2
exp
 N log

cosh( Î³
N )

Î³
Î² âˆ’1
Î¦
âˆ’
log[cosh( Î³
N )]
Î³
Î² âˆ’1
â—¦M â€²
!
+

1 + Î²
Î³

log( 2
Î·),
where as usual Î¦ is the function deï¬ned by equation (1.1, page 2). This leads us to

2.2.
Playing with two posterior and two local prior distributions
77
deï¬ne for any i, j âˆˆN, any Î², Î²â€² âˆˆR+,
(2.19)
C(i, Î²)
def
=
2
Î¶ âˆ’1Ci
N
Î¶ sinh( Î¶Î²
N ), Î¶Î²

+ log

Ï€i
exp[âˆ’N
Î¶ sinh( Î¶Î²
N )R]
âŠ—2

exp
 N log

cosh( Î¶Î²
N )

Î¶ âˆ’1
Î¦
âˆ’
log[cosh( Î¶Î²
N )]
Î¶âˆ’1
â—¦M â€²
!
âˆ’Î¶ + 1
Î¶ âˆ’1

2 log

Î½(Î²)Î¼(i)

+ log

 Î·
2

.
Recall that the deï¬nition of Ci(Î», Î³) is to be found in Proposition 2.2.5, page 75.
Let us remark that, since
exp

NaÎ¦âˆ’a(p)

= exp

N log

1 +

exp(a) âˆ’1

p

â‰¤exp

N

exp(a) âˆ’1

p

,
p âˆˆ(0, 1), a âˆˆR,
we have
C(i, Î²) â‰¤
2
Î¶ âˆ’1 log

Ï€i
exp[âˆ’N
Î¶ sinh( Î¶Î²
N )R]

exp

2N sinh

 Î¶Î²
2N
2Ï€i
exp[âˆ’N
Î¶ sinh( Î¶Î²
N )R]

M â€²
+ log

Ï€i
exp[âˆ’N
Î¶ sinh( Î¶Î²
N )R]
âŠ—2
exp

N

exp

(Î¶ âˆ’1)âˆ’1 log

cosh

 Î¶Î²
N

âˆ’1

M â€²
âˆ’Î¶ + 1
Î¶ âˆ’1

2 log

Î½(Î²)Î¼(i)

+ log

 Î·
2

.
Let us put
SÎ»

(i, Î²), (j, Î²â€²)
 def
= N
Î» log

cosh( Î»
N )

inf
Î³âˆˆR+ Î³âˆ’1

log

Ï€i
exp[âˆ’N
Î¶ sinh( Î¶Î²
N )R] âŠ—Ï€j
exp[âˆ’N
Î¶ sinh( Î¶Î²â€²
N )R]

exp

Î³Î¦âˆ’Î³
N (M â€²)

+
Ci N
Î¶ sinh( Î¶Î²
N ), Î¶Î²

âˆ’log( Î·
3)
Î¶ âˆ’1
+
Cj N
Î¶ sinh( Î¶Î²â€²
N ), Î¶Î²â€²
âˆ’log( Î·
3)
Î¶ âˆ’1
âˆ’log( Î·
3)

+ 1
Î»

C(i, Î²) + C(j, Î²â€²) âˆ’Î¶ + 1
Î¶ âˆ’1 log

3âˆ’1Î½(Î»)Ïµ

,
where
Î· = Î½(Î³)Î½(Î²)Î½(Î²â€²)Î¼(i)Î¼(j)Î·.
Let us remark that

78
Chapter 2.
Comparing posterior distributions to Gibbs priors
SÎ»

(i, Î²), (j, Î²â€²)

â‰¤inf
Î³âˆˆR+
Î»
2NÎ³ log

Ï€i
exp[âˆ’N
Î¶ sinh( Î¶Î²
N )R] âŠ—Ï€j
exp[âˆ’N
Î¶ sinh( Î¶Î²â€²
N )R]

exp

N

exp

 Î³
N

âˆ’1

M â€²
+
 
Î»
2NÎ³(Î¶ âˆ’1) +
2
Î»(Î¶ âˆ’1)
!
log

Ï€i
exp[âˆ’N
Î¶ sinh( Î¶Î²
N )R]

exp

2N sinh

 Î¶Î²
2N
2Ï€i
exp[âˆ’N
Î¶ sinh( Î¶Î²
N )R]

M â€²
+ Î»âˆ’1 log

Ï€i
exp[âˆ’N
Î¶ sinh( Î¶Î²
N )R]
âŠ—2
exp

N

exp

(Î¶ âˆ’1)âˆ’1 log

cosh

 Î¶Î²
N

âˆ’1

M â€²
+
 
Î»
2NÎ³(Î¶ âˆ’1) +
2
Î»(Î¶ âˆ’1)
!
log

Ï€j
exp[âˆ’N
Î¶ sinh( Î¶Î²â€²
N )R]

exp

2N sinh

 Î¶Î²â€²
2N
2Ï€j
exp[âˆ’N
Î¶ sinh( Î¶Î²â€²
N )R]

M â€²
+ Î»âˆ’1 log

Ï€j
exp[âˆ’N
Î¶ sinh( Î¶Î²â€²
N )R]
âŠ—2
exp

N

exp

(Î¶ âˆ’1)âˆ’1 log

cosh

 Î¶Î²â€²
N

âˆ’1

M â€²
âˆ’
(Î¶ + 1)Î»
2N(Î¶ âˆ’1)Î³ log

3âˆ’1Î½(Î³)Î½(Î²)Î½(Î²â€²)Î¼(i)Î¼(j)Î·

âˆ’(Î¶ + 1)
(Î¶ âˆ’1)Î»
 
2 log

2âˆ’1Î½(Î²)Î½(Î²â€²)Î¼(i)Î¼(j)Î·

+ log

3âˆ’1Î½(Î»)Ïµ
!
.
Let us deï¬ne accordingly
B

(i, Î²), (j, Î²â€²)
 def
=
inf
Î» Îž Î»
N

inf
Î±,Î³,Î±â€²,Î³â€²

Ï€j
exp(âˆ’Î±â€²R) âŠ—Ï€i
exp(âˆ’Î±R)

Î¨âˆ’Î»
N (Râ€², M â€²)

âˆ’log

Î·
3

Î»
+ Cj(Î±â€², Î³â€²) âˆ’log

Î·
3

NÎ²â€²
Î±â€² sinh( Î³â€²
N ) âˆ’Î³â€²
+ Ci(Î±, Î³) âˆ’log

Î·
3

Î³ âˆ’NÎ²
Î± sinh( Î³
N )

+ SÎ»

(i, Î²), (j, Î²â€²)


,
where
Î· = Î½(Î»)Î½(Î±)Î½(Î³)Î½(Î²)Î½(Î±â€²)Î½(Î³â€²)Î½(Î²â€²)Î¼(i)Î¼(j)Î·.
Proposition 2.2.7.
â€¢ With P probability at least 1 âˆ’Î·, for any Î² âˆˆR+ and i âˆˆN,
C(Ï€i
exp(âˆ’Î²r)) â‰¤C(i, Î²);
â€¢ With P probability at least 1 âˆ’3Î·, for any Î», Î², Î²â€² âˆˆR+, any i, j âˆˆN,
SÎ»

(i, Î²), (j, Î²â€²)

â‰¤SÎ»

(i, Î²), (j, Î²â€²)

;

2.2.
Playing with two posterior and two local prior distributions
79
â€¢ With P probability at least 1 âˆ’4Î·, for any i, j âˆˆN, any Î², Î²â€² âˆˆR+,
B(Ï€i
exp(âˆ’Î²r), Ï€j
exp(âˆ’Î²â€²r)) â‰¤B

(i, Î²), (j, Î²â€²)

.
It is also interesting to ï¬nd a non-random lower bound for C(Ï€i
exp(âˆ’Î²r)). Let us
start from the fact that with P probability at least 1 âˆ’Î·,
Ï€i
exp(âˆ’Î±R) âŠ—Ï€i
exp(âˆ’Î±R)

Î¦ Î³â€²
N (M â€²)

â‰¤Ï€i
exp(âˆ’Î±R) âŠ—Ï€i
exp(âˆ’Î±R)(mâ€²) âˆ’log(Î·)
Î³â€²
.
On the other hand, we already proved that with P probability at least 1 âˆ’Î·,
0 â‰¤
 
1 âˆ’
Î±
N tanh( Î»
N )
!
K

Ï, Ï€i
exp(âˆ’Î±R)

â‰¤
Î±
N tanh( Î»
N )

Î»

Ï(r) âˆ’Ï€i
exp(Î±R)(r)

+ N log

cosh( Î»
N )

Ï âŠ—Ï€i
exp(âˆ’Î±R)(mâ€²) âˆ’log(Î·)

+ K

Ï, Ï€i
âˆ’K

Ï€i
exp(âˆ’Î±R), Ï€i
.
Thus for any Î¾ > 0, putting Î² =
Î±Î»
N tanh( Î»
N ), with P probability at least 1 âˆ’Î·,
Î¾Ï€i
exp(âˆ’Î±R) âŠ—Ï€i
exp(âˆ’Î±R)

Î¦ Î³â€²
N (M â€²)

â‰¤Ï€i
exp(âˆ’Î±R)

log

Ï€i
exp(âˆ’Î²r)

exp

Î² N
Î» log

cosh( Î»
N )

Ï€i
exp(âˆ’Î²r)(mâ€²) + Î¾mâ€²
âˆ’
 Î²
Î» + Î¾
Î³â€²
!
log
 Î·
2
!
â‰¤log

Ï€i
exp(âˆ’Î²r)

exp

Î² N
Î» log

cosh( Î»
N )

Ï€i
exp(âˆ’Î²r)(mâ€²)

Ã— Ï€i
exp(âˆ’Î²r)

exp

Î² N
Î» log

cosh( Î»
N )

Ï€i
exp(âˆ’Î²r)(mâ€²) + Î¾mâ€²
âˆ’
 
2Î²
Î» + Î¾
Î³â€²
!
log
 Î·
2
!
â‰¤2 log

Ï€i
exp(âˆ’Î²r)

exp

Î¾ + Î² N
Î» log

cosh( Î»
N )

Ï€i
exp(âˆ’Î²r)(mâ€²)

âˆ’
 
2Î²
Î» + Î¾
Î³â€²
!
log
 Î·
2
!
â‰¤2 log

Ï€i
exp(âˆ’Î²r)

exp

Î¾ + Î²Î»
2N

Ï€i
exp(âˆ’Î²r)(mâ€²)

âˆ’
 2Î²
Î» + Î¾
Î³â€²
!
log
 Î·
2
!
.
Taking Î¾ = Î²Î»
2N , we get with P probability at least 1 âˆ’Î·

80
Chapter 2.
Comparing posterior distributions to Gibbs priors
Î²Î»
4N

Ï€i
exp[âˆ’Î² N
Î» tanh( Î»
N )R]
âŠ—2
Î¦ Î³â€²
N

M â€²
â‰¤log

Ï€i
exp(âˆ’Î²r)

exp
Î²Î»
N Ï€i
exp(âˆ’Î²r)(mâ€²)

âˆ’
 2Î²
Î» +
Î²Î»
2NÎ³â€²
!
log
 Î·
2
!
.
Putting
Î» = N 2
Î³ log

cosh( Î³
N )

and Î¥(Î³)
def
=
Î³ tanh
 N
Î³ log

cosh( Î³
N )

N log

cosh( Î³
N )

âˆ¼
Î³â†’0 1,
this can be rewritten as
Î²N
4Î³ log

cosh( Î³
N )

Ï€i
exp(âˆ’Î²Î¥(Î³)R)
âŠ—2
Î¦ Î³â€²
N

M â€²
â‰¤log

Ï€i
exp(âˆ’Î²r)

exp

Î² N
Î³ log

cosh( Î³
N )

Ï€i
exp(âˆ’Î²r)(mâ€²)

âˆ’
 
2Î²Î³
N 2 log

cosh( Î³
N )
 + Î²N log

cosh( Î³
N )

2Î³Î³â€²
!
log
 Î·
2
!
.
It is now tempting to simplify the picture a little bit by setting Î³â€² = Î³, leading to
Proposition 2.2.8. With P probability at least 1 âˆ’Î·, for any i âˆˆN, any Î² âˆˆR+,
C

Ï€i
exp(âˆ’Î²r)

â‰¥C(i, Î²)
def
=
1
Î¶ âˆ’1

N
4 log

cosh( Î¶Î²
N )

Ï€i
exp(âˆ’Î²Î¥(Î¶Î²)R)
âŠ—2
Î¦ Î¶Î²
N

M â€²
+
"
2Î¶2Î²2
N 2 log

cosh( Î¶Î²
N )
 + N log

cosh( Î¶Î²
N )

2Î¶Î²
#
log

2âˆ’1Î½(Î²)Î¼(i)Î·

âˆ’(Î¶ + 1)

log

Î½(Î²)Î¼(i)

+ 2âˆ’1 log

3âˆ’1Ïµ

,
where C

Ï€i
exp(âˆ’Î²r)

is deï¬ned by equation (2.18, page 75).
We are now going to analyse Theorem 2.2.4 (page 72). For this, we will also need
an upper bound for SÎ»(Ï, Ïâ€²), deï¬ned by equation (2.13, page 70), using M â€² and
empirical complexities, because of the special relations between empirical complex-
ities induced by the selection algorithm. To this purpose, a useful alternative to
Proposition 2.2.6 (page 76) is to write, with P probability at least 1 âˆ’Î·,
Î³Ï âŠ—Ïâ€²(mâ€²) â‰¤Î³Ï âŠ—Ïâ€²
Î¦âˆ’Î³
N

M â€²
+ K

Ï, Ï€i
exp(âˆ’Î»R)

+ K

Ïâ€², Ï€j
exp(âˆ’Î»â€²R)

âˆ’log(Î·),
and thus at least with P probability 1 âˆ’3Î·,

2.2.
Playing with two posterior and two local prior distributions
81
Î³Ï âŠ—Ïâ€²(mâ€²) â‰¤Î³Ï âŠ—Ïâ€²
Î¦âˆ’Î³
N

M â€²
+ (1 âˆ’Î¶âˆ’1)âˆ’1

K

Ï, Ï€i
exp(âˆ’Î²r)

+ log

Ï€i
exp(âˆ’Î²r)

exp
 N
Î¶ log

cosh

 Î¶Î²
N

Ï(mâ€²)

âˆ’Î¶âˆ’1 log(Î·)

+ (1 âˆ’Î¶âˆ’1)âˆ’1

K

Ï, Ï€j
exp(âˆ’Î²â€²r)

+ log

Ï€j
exp(âˆ’Î²â€²r)

exp
 N
Î¶ log

cosh

 Î¶Î²â€²
N

Ï(mâ€²)

âˆ’Î¶âˆ’1 log(Î·)

âˆ’log(Î·).
When Ï = Ï€i
exp(âˆ’Î²r) and Ïâ€² = Ï€j
exp(âˆ’Î²â€²r), we get with P probability at least 1 âˆ’Î·,
for any Î², Î²â€², Î³ âˆˆR+, any i, j âˆˆN,
Î³Ï âŠ—Ïâ€²(mâ€²) â‰¤Î³Ï âŠ—Ïâ€²
Î¦âˆ’Î³
N

M â€²
+ C(Ï) + C(Ïâ€²) âˆ’Î¶ + 1
Î¶ âˆ’1

log

3âˆ’1Î½(Î³)Î·

.
Proposition 2.2.9. With P probability at least 1 âˆ’Î·, for any Ï = Ï€i
exp(âˆ’Î²r), any
Ïâ€² = Ï€j
exp(âˆ’Î²â€²r) âˆˆP,
SÎ»(Ï, Ïâ€²) â‰¤N
Î» log

cosh( Î»
N )

Ï âŠ—Ïâ€²
Î¦âˆ’Î³
N

M â€²
+
1 + N
Î³ log

cosh( Î»
N )

Î»

C(Ï) + C(Ïâ€²)

âˆ’(Î¶ + 1)
(Î¶ âˆ’1)Î»

log

3âˆ’1Î½(Î»)Ïµ

+ N
Î³ log

cosh

 Î»
N

log

3âˆ’1Î½(Î³)Î·

.
In order to analyse Theorem 2.2.4 (page 72), we need to index P =

Ï1, . . . , ÏM

in order of increasing empirical complexity C(Ï). To deal in a convenient way with
this indexation, we will write C(i, Î²) as C

Ï€i
exp(âˆ’Î²r)

, C(i, Î²) as C

Ï€i
exp(âˆ’Î²r)

, and
S

(i, Î²), (j, Î²â€²)

as S

Ï€i
exp(âˆ’Î²r), Ï€j
exp(âˆ’Î²â€²r)

.
With P probability at least 1 âˆ’Ïµ, when 	t â‰¤j < 	k, as we already saw,
Ï	k(R) â‰¤Ïi(R) â‰¤Ïj(R) + inf
Î»âˆˆR+ Îž Î»
N

2SÎ»(Ïj, Ïi)

,
where i = t(j) < 	t. Therefore, with P probability at least 1 âˆ’Ïµ âˆ’Î·,
Ïi(R) â‰¤Ïj(R) + inf
Î»âˆˆR+ Îž Î»
N

2N
Î» log

cosh

 Î»
N

Ïj âŠ—Ïi

Î¦âˆ’Î³
N

M â€²
+ 4
1 + N
Î³ log

cosh

 Î»
N

Î»
C(Ïj)
âˆ’(Î¶ + 1)
(Î¶ âˆ’1)Î»

log

3âˆ’1Î½(Î»)Ïµ

+ N
Î³ log

cosh

 Î»
N

log

3âˆ’1Î½(Î³)Î·

.

82
Chapter 2.
Comparing posterior distributions to Gibbs priors
We can now remark that
Îža(p + q) â‰¤Îža(p) + qÎžâ€²
a(p)q â‰¤Îža(p) + Îžâ€²
a(0)q = Îža(p) +
a
tanh(a)q
and that
Î¦âˆ’a(p + q) â‰¤Î¦âˆ’a(p) + Î¦â€²
âˆ’a(0)q = Î¦âˆ’a(p) + exp(a) âˆ’1
a
q.
Moreover, assuming as usual without substantial loss of generality that there exists
Î¸ âˆˆarg minÎ˜ R, we can split M â€²(Î¸, Î¸â€²) â‰¤M â€²(Î¸, Î¸) + M â€²(Î¸, Î¸â€²). Let us then consider
the expected margin function deï¬ned by
Ï•(y) = sup
Î¸âˆˆÎ˜
M â€²(Î¸, Î¸) âˆ’yRâ€²(Î¸, Î¸),
y âˆˆR+,
and let us write for any y âˆˆR+,
Ïj âŠ—Ïi

Î¦âˆ’Î»
N

M â€²
â‰¤Ïj âŠ—Ïi

Î¦âˆ’Î³
N

M â€²(., Î¸) + yRâ€²(., Î¸) + Ï•(y)

â‰¤Ïj

Î¦âˆ’Î»
N

M â€²(., Î¸) + Ï•(y)

+ Ny

exp( Î³
N ) âˆ’1

Î³

Ïi(R) âˆ’R(Î¸)

and
"
1 âˆ’2yN

exp( Î³
N ) âˆ’1

log

cosh

 Î»
N

Î³ tanh

 Î»
N

#

Ïi(R) âˆ’R(Î¸)

â‰¤

Ïj(R) âˆ’R(Î¸)

+ Îž Î»
N

2N
Î» log

cosh

 Î»
N

Ïj

Î¦âˆ’Î³
N

M â€²(., Î¸) + Ï•(y)

+ 4
1 + N
Î³ log

cosh

 Î»
N

Î»
C(Ïj)
âˆ’2(Î¶ + 1)
(Î¶ âˆ’1)Î»

log

3âˆ’1Î½(Î»)Ïµ

+ N
Î³ log

cosh

 Î»
N

log

3âˆ’1Î½(Î³)Î·

.
With P probability at least 1âˆ’Ïµâˆ’Î·, for any Î», Î³, x, y âˆˆR+, any j âˆˆ
	t, . . . , 	kâˆ’1

,
Ï	k(R) âˆ’R(Î¸) â‰¤Ïi(R) âˆ’R(Î¸)
â‰¤
"
1 âˆ’2yN

exp( Î³
N ) âˆ’1

log

cosh

 Î»
N

Î³ tanh

 Î»
N

#âˆ’1
"
1 + 2xN

exp( Î³
N ) âˆ’1

log

cosh

 Î»
N

Î³ tanh

 Î»
N

#

Ïj(R) âˆ’R(Î¸)

+ Îž Î»
N
2N
Î» log

cosh

 Î»
N

Î¦âˆ’Î³
N

Ï•(x) + Ï•(y)

+ 4
1 + N
Î³ log

cosh

 Î»
N

Î»
C(Ïj)
âˆ’2(Î¶ + 1)
(Î¶ âˆ’1)Î»

log

3âˆ’1Î½(Î»)Ïµ

+ N
Î³ log

cosh

 Î»
N

log

3âˆ’1Î½(Î³)Î·

.

2.2.
Playing with two posterior and two local prior distributions
83
Now we have to get an upper bound for Ïj(R). We can write Ïj = Ï€â„“
exp(âˆ’Î²â€²r), as we
assumed that all the posterior distributions in P are of this special form. Moreover,
we already know from Theorem 2.1.8 (page 58) that with P probability at least
1 âˆ’Î·,

N sinh

 Î²â€²
N

âˆ’Î²â€²Î¶âˆ’1
Ï€â„“
exp(âˆ’Î²â€²r)(R) âˆ’Ï€â„“
exp(âˆ’Î²â€²Î¶âˆ’1R)(R)

â‰¤Câ„“(Î²â€²Î¶âˆ’1, Î²â€²) âˆ’log

Î½(Î²â€²)Î¼(â„“)Î·

.
This proves that with P probability at least 1 âˆ’Ïµ âˆ’2Î·,
Ï	k(R) â‰¤R(Î¸)
+
 
1 âˆ’2yN

exp

 Î³
N

âˆ’1

log

cosh

 Î»
N

Î³ tanh

 Î»
N

!âˆ’1
 
1 + 2xN

exp

 Î³
N

âˆ’1

log

cosh

 Î»
N

Î³ tanh

 Î»
N

!
Ã—
"
Ï€â„“
exp(âˆ’Î¶âˆ’1Î²â€²R)(R) âˆ’R(Î¸) + Câ„“(Î¶âˆ’1Î²â€², Î²â€²) âˆ’log

Î½(Î²â€²)Î¼(â„“)Î·

N sinh( Î²â€²
N ) âˆ’Î¶âˆ’1Î²â€²
#
+ Îž Î»
N
2N
Î» log

cosh

 Î»
N

Î¦âˆ’Î³
N

Ï•(x) + Ï•(y)

+ 4
1 + N
Î³ log

cosh

 Î»
N

Î»
C(â„“, Î²â€²)
âˆ’2(Î¶ + 1)
(Î¶ âˆ’1)Î»

log

3âˆ’1Î½(Î»)Ïµ

+ N
Î³ log

cosh

 Î»
N

log

3âˆ’1Î½(Î³)Î·

.
The case when j âˆˆ
	k +1, . . . , M

\(arg max t) is dealt with exactly in the same
way, with i = t(j) replaced directly with 	k itself, leading to the same inequality.
The case when j âˆˆ(arg max t) is dealt with bounding ï¬rst Ï	k(R)âˆ’R(Î¸) in terms
of Ï	t(R) âˆ’R(Î¸), and this latter in terms of Ïj(R) âˆ’R(Î¸). Let us put
A(Î», Î³) =
"
1 âˆ’2xN

exp

 Î³
N

âˆ’1

log

cosh

 Î»
N

Î³ tanh

 Î»
N

#
,
B(Î», Î³) = 1 + 2yN

exp

 Î³
N

âˆ’1

log

cosh

 Î»
N

Î³ tanh

 Î»
N

,
D(Î», Î³, Ïj) = Îž Î»
N
2N
Î» log

cosh

 Î»
N

Î¦âˆ’Î³
N

Ï•(x) + Ï•(y)

+4
1 + N
Î³ log

cosh

 Î»
N

Î»
C(Ïj)
âˆ’2(Î¶ + 1)
(Î¶ âˆ’1)Î»

log

3âˆ’1Î½(Î»)Ïµ

+N
Î³ log

cosh

 Î»
N

log

3âˆ’1Î½(Î³)Î·

,
(2.20)
where C(Ïj) = C(â„“, Î²â€²) is deï¬ned, when Ïj = Ï€â„“
exp(âˆ’Î²â€²r), by equation (2.19, page

84
Chapter 2.
Comparing posterior distributions to Gibbs priors
77). We obtain, still with P probability 1 âˆ’Ïµ âˆ’2Î·,
Ï	k(R) âˆ’R(Î¸) â‰¤B(Î», Î³)
A(Î», Î³)

Ï	t(R) âˆ’R(Î¸)

+ D(Î», Î³, Ïj)
A(Î», Î³)
,
Ï	t(R) âˆ’R(Î¸) â‰¤B(Î», Î³)
A(Î», Î³)

Ïj(R) âˆ’R(Î¸)

+ D(Î», Î³, Ïj)
A(Î», Î³)
.
The use of the factor D(Î», Î³, Ïj) in the ï¬rst of these two inequalities, instead of
D(Î», Î³, Ï	t), is justiï¬ed by the fact that C(Ï	t) â‰¤C(Ïj). Combining the two we get
Ï	k(R) â‰¤R(Î¸) + B(Î», Î³)2
A(Î», Î³)2

Ïj(R) âˆ’R(Î¸)

+
B(Î», Î³)
A(Î», Î³) + 1
D(Î», Î³, Ïj)
A(Î», Î³)
.
Since it is the worst bound of all cases, it holds for any value of j, proving
Theorem 2.2.10.
With P probability at least 1 âˆ’Ïµ âˆ’2Î·,
Ï	k(R) â‰¤R(Î¸ ) +
inf
i,Î²,Î»,Î³,x,y

B(Î», Î³)2
A(Î», Î³)2

Ï€i
exp(âˆ’Î²r)(R) âˆ’R(Î¸)

+
B(Î», Î³)
A(Î», Î³) + 1
D(Î», Î³, Ï€i
exp(âˆ’Î²r))
A(Î», Î³)

â‰¤R(Î¸) +
inf
i,Î²,Î»,Î³,x,y

B(Î», Î³)2
A(Î», Î³)2
"
Ï€i
exp(âˆ’Î¶âˆ’1Î²R)(R) âˆ’R(Î¸) + Ci(Î¶âˆ’1Î², Î²) âˆ’log

Î½(Î²)Î¼(i)Î·

N sinh

 Î²
N

âˆ’Î¶âˆ’1Î²
#
+
B(Î», Î³)
A(Î», Î³) + 1
D(Î», Î³, Ï€i
exp(âˆ’Î²r))
A(Î», Î³)

,
where the notation A(Î», Î³), B(Î», Î³) and D(Î», Î³, Ï) is deï¬ned by equation (2.20 page
83) and where the notation Ci(Î², Î³) is deï¬ned in Proposition 2.2.5 (page 75).
The bound is a little involved, but as we will prove next, it gives the same rate
as Theorem 2.1.15 (page 65) and its corollaries, when we work with a single model
(meaning that the support of Î¼ is reduced to one point) and the goal is to choose
adaptively the temperature of the Gibbs posterior, except for the appearance of the
union bound factor âˆ’log

Î½(Î²)

which can be made of order log

log(N)

without
spoiling the order of magnitude of the bound.
We will encompass the case when one must choose between possibly several
parametric models. Let us assume that each Ï€i is supported by some measurable
parameter subset Î˜i ( meaning that Ï€i(Î˜i) = 1), let us also assume that the
behaviour of Ï€i is parametric in the sense that there exists a dimension di âˆˆR+
such that
(2.21)
sup
Î²âˆˆR+
Î²

Ï€i
exp(âˆ’Î²R)(R) âˆ’inf
Î˜i R

â‰¤di.
Then
Ci(Î», Î³) â‰¤log

Ï€i
exp(âˆ’Î»R)

exp

2N sinh

 Î³
2N
2M â€²(., Î¸)


2.2.
Playing with two posterior and two local prior distributions
85
+ 2N sinh

 Î³
2N
2Ï€i
exp(âˆ’Î»R)

M â€²(., Î¸)

â‰¤log

Ï€i
exp(âˆ’Î»R)

exp 2xN sinh

 Î³
2N
2
R âˆ’R(Î¸)

+ 2xN sinh

 Î³
2N
2Ï€i
exp(âˆ’Î»R)

R âˆ’R(Î¸)

+ 4N sinh

 Î³
2N
2Ï•(x)
â‰¤2xN sinh

 Î³
2N
2Ï€i
exp{âˆ’[Î»âˆ’2xN sinh( Î³
2N )2]R}

R âˆ’R(Î¸)

+ 2xN sinh

 Î³
2N
2Ï€i
exp(âˆ’Î»R)

R âˆ’R(Î¸)

+ 4N sinh

 Î³
2N
2Ï•(x).
Thus
Ci(Î», Î³) â‰¤4N sinh

 Î³
2N
2
"
x

inf
Î˜i R âˆ’R(Î¸)

+ Ï•(x)
+ xdi
2Î» +
xdi
2Î» âˆ’4xN sinh

 Î³
2N
2
#
.
In the same way,
C(i, Î²) â‰¤
8N
Î¶âˆ’1 sinh

 Î¶Î²
2N
2
&
x

inf
Î˜i R âˆ’R(Î¸)

+ Ï•(x)
+
Î¶xdi
2N sinh

 Î¶Î²
N

 
1 +
1
1 âˆ’xÎ¶ tanh

 Î¶Î²
2N

!'
+ 2N

exp

Î¶2Î²2
2N 2(Î¶âˆ’1)

âˆ’1
"
Ï•(x) + x

inf
Î˜i R âˆ’R(Î¸)

+
xÎ¶di
N sinh

 Î¶Î²
N

âˆ’xÎ¶N

exp

Î¶2Î²2
2N 2(Î¶âˆ’1)

âˆ’1

#
âˆ’(Î¶ + 1)
(Î¶ âˆ’1)

2 log

Î½(Î²)Î¼(i)

+ log

 Î·
2

.
In order to keep the right order of magnitude while simplifying the bound, let us
consider
(2.22)
C1 = max

Î¶ âˆ’1,

2N
Î¶Î²max
2
sinh

Î¶Î²max
2N
2
,
2N 2(Î¶âˆ’1)
Î¶2Î²2max

exp

Î¶2Î²2
max
2N 2(Î¶âˆ’1)

âˆ’1

.
Then, for any Î² âˆˆ(0, Î²max),
C(i, Î²) â‰¤inf
yâˆˆR+
3C1Î¶2Î²2
(Î¶ âˆ’1)N
&
y

inf
Î˜i R âˆ’R(Î¸)

+ Ï•(y) +
ydi
Î²

1 âˆ’
yC1Î¶2Î²
2(Î¶âˆ’1)N

'
âˆ’(Î¶ + 1)
(Î¶ âˆ’1)

2 log

Î½(Î²)Î¼(i)

+ log

 Î·
2

.

86
Chapter 2.
Comparing posterior distributions to Gibbs priors
Thus
D

Î», Î³, Ï€i
exp(âˆ’Î²r)

â‰¤
Î»
N tanh

 Î»
N


Î»

exp

 Î³
N

âˆ’1

Î³

Ï•(x) + Ï•(y)

+ 4
1 +
Î»2
2NÎ³
Î»
&
3C1Î¶2Î²2
(Î¶ âˆ’1)N
"
z

inf
Î˜i R âˆ’R(Î¸)

+ Ï•(z) +
zdi
Î²

1 âˆ’
zC1Î¶2Î²
2(Î¶âˆ’1)N

#
âˆ’(Î¶ + 1)
(Î¶ âˆ’1)

2 log

Î½(Î²)Î¼(i)

+ log

 Î·
2
'
âˆ’2(Î¶ + 1)
(Î¶ âˆ’1)Î»

log

3âˆ’1Î½(Î»)Ïµ

+
Î»2
2NÎ³ log

3âˆ’1Î½(Î³)Î·

If we are not seeking tight constants, we can take for the sake of simplicity
Î» = Î³ = Î², x = y and Î¶ = 2.
Let us put
(2.23)
C2 = max

C1, N

exp

 Î²max
N

âˆ’1

Î²max
,
2N log

cosh

 Î²max
N

Î²max tanh

 Î²max
N
 ,
Î²max
N tanh

 Î²max
N


,
so that
A(Î², Î²)âˆ’1 â‰¤
 
1 âˆ’C2xÎ²
N
!âˆ’1
,
B(Î², Î²) â‰¤1 + C2xÎ²
N
,
D

Î², Î², Ï€i
exp(âˆ’Î²r)

â‰¤C2
2
2Î²
N Ï•(x)
+

4 + 2Î²
N
C2
Î²
&
12C1Î²2
N
"
z

inf
Î˜i R âˆ’R(Î¸)

+ Ï•(z) +
zdi
Î²

1 âˆ’2zC1Î²
N

#
âˆ’6 log

Î½(Î²)Î¼(i)

âˆ’3 log

 Î·
2

'
âˆ’6C2
Î²

log

3âˆ’1Î½(Î²)Ïµ

+ Î²
2N log

3âˆ’1Î½(Î²)Î·

and
Ci(Î¶âˆ’1Î², Î²) â‰¤C1Î²2
N
 
x

inf
Î˜i R âˆ’R(Î¸)

+ Ï•(x) +
2xdi
Î²

1 âˆ’xÎ²
N

!
.
This leads to
Ï	k(R) â‰¤R(Î¸) + inf
i,Î²
"
1 + C2xÎ²
N
1 âˆ’C2xÎ²
N
#2
2di
Î² + inf
Î˜i R âˆ’R(Î¸)
+ 2
Î²
&
C1Î²2
N
 
x

inf
Î˜i R âˆ’R(Î¸

+ Ï•(x) +
2xdi
Î²

1 âˆ’xÎ²
N

!

2.2.
Playing with two posterior and two local prior distributions
87
âˆ’log

Î½(Î²)Î¼(i)Î·

'
+
2

1 âˆ’C2xÎ²
N
2

C2
2
2Î²
N Ï•(x)
+

4 + 2Î²
N
C2
Î²
12C1Î²2
N
 
x

inf
Î˜i R âˆ’R(Î¸)

+ Ï•(x) +
xdi
Î²

1 âˆ’2xC1Î²
N

!
âˆ’6 log

Î½(Î²)Î¼(i)

âˆ’3 log

 Î·
2

âˆ’6C2
Î²

log

3âˆ’1Î½(Î²)Ïµ

+ Î²
2N log

3âˆ’1Î½(Î²)Î·

.
We see in this expression that, in order to balance the various factors depending
on x it is advisable to choose x such that
inf
Î˜i R âˆ’R(Î¸) = Ï•(x)
x
,
as long as x â‰¤
N
4C2Î² .
Following Mammen and Tsybakov, let us assume that the usual margin assump-
tion holds: for some real constants c > 0 and Îº â‰¥1,
R(Î¸) âˆ’R(Î¸) â‰¥c

D(Î¸, Î¸)
Îº.
As D(Î¸, Î¸ ) â‰¥M â€²(Î¸, Î¸ ), this also implies the weaker assumption
R(Î¸) âˆ’R(Î¸ ) â‰¥c

M â€²(Î¸, Î¸)
Îº,
Î¸ âˆˆÎ˜,
which we will really need and use. Let us take Î²max = N and
Î½ =
1
âŒˆlog2(N)âŒ‰
âŒˆlog2(N)âŒ‰

k=1
Î´2k.
Then, as we have already seen, Ï•(x) â‰¤(1âˆ’Îºâˆ’1)

Îºcx
âˆ’
1
Îºâˆ’1 . Thus Ï•(x)/x â‰¤bxâˆ’
Îº
Îºâˆ’1 ,
where b = (1 âˆ’Îºâˆ’1)

Îºc
âˆ’
1
Îºâˆ’1 . Let us choose accordingly
x = min

x1
def
=
 infÎ˜i R âˆ’R(Î¸)
b
!âˆ’Îºâˆ’1
Îº
, x2
def
=
N
4C2Î²

.
Using the fact that when r âˆˆ(0, 1
2),

 1+r
1âˆ’r
2 â‰¤1+16r â‰¤9, we get with P probability
at least 1 âˆ’Ïµ, for any Î² âˆˆsupp Î½, in the case when x = x1 â‰¤x2,
Ï	k(R) â‰¤inf
Î˜i R + 538 C2
2
Î²
N b
Îºâˆ’1
Îº 
inf
Î˜i R âˆ’R(Î¸)
 1
Îº
+ C2
Î²

138 di + 166 log

1 + log2(N)

âˆ’134 log

Î¼(i)

âˆ’102 log(Ïµ) + 724

,
and in the case when x = x2 â‰¤x1,

88
Chapter 2.
Comparing posterior distributions to Gibbs priors
Ï	k(R) â‰¤inf
Î˜i R + 68C1

inf
Î˜i R âˆ’R(Î¸)

+ 269 C2
2
Î²
N Ï•(x)
+ C2
Î²

138 di + 166 log

1 + log2(N)

âˆ’134 log

Î¼(i)

âˆ’102 log(Ïµ) + 724

â‰¤inf
Î˜i R + 541C2
2
Î²
N Ï•(x)
+ C2
Î²

138 di + 166 log

1 + log2(N)

âˆ’134 log

Î¼(i)

âˆ’102 log(Ïµ) + 724

.
Thus with P probability at least 1 âˆ’Ïµ,
Ï	k(R) â‰¤inf
Î˜i R +
inf
Î²âˆˆ(1,N) 1082 C2
2
Î²
N max

b
Îºâˆ’1
Îº 
inf
Î˜i R âˆ’R(Î¸)
 1
Îº ,
b
 4C2Î²
N
!
1
Îºâˆ’1 
+ C2
Î²

138 di + 166 log

1 + log2(N)

âˆ’134 log

Î¼(i)

âˆ’102 log(Ïµ) + 724

.
Theorem 2.2.11.
With probability at least 1 âˆ’Ïµ, for any i âˆˆN,
Ï	k(R) â‰¤inf
Î˜i R
+ max
âŽ§
âŽª
âŽª
âŽª
âŽ¨
âŽª
âŽª
âŽª
âŽ©
847C
3
2
2
=
>
>
?b
Îºâˆ’1
Îº 
infÎ˜i R âˆ’R(Î¸)
 1
Îº 
di + log

1+log2(N)
ÏµÎ¼(i)

+ 5

N
,
2C2

1082 b
 Îºâˆ’1
2Îºâˆ’1 4
1
2Îºâˆ’1
âŽ§
âŽ¨
âŽ©
166C2

di + log

1+log2(N)
ÏµÎ¼(i)

+ 5

N
âŽ«
âŽ¬
âŽ­
Îº
2Îºâˆ’1
âŽ«
âŽª
âŽª
âŽª
âŽ¬
âŽª
âŽª
âŽª
âŽ­
,
where C2, given by equation (2.23 page 86), will in most cases be close to 1, and in
any case less than 3.2.
This result gives a bound of the same form as that given in Theorem 2.1.15 (page
65) in the special case when there is only one model â€” that is when Î¼ is a Dirac
mass, for instance Î¼(1) = 1, implying that R(Î¸1)âˆ’R(Î¸) = 0. Morover the parametric
complexity assumption we made for this theorem, given by equation (2.21 page 84),
is weaker than the one used in Theorem 2.1.15 and described by equation (2.8, page
62). When there is more than one model, the bound shows that the estimator makes
a trade-oï¬€between model accuracy, represented by infÎ˜i R âˆ’R(Î¸), and dimension,
represented by di, and that for optimal parametric sub-models, meaning those for
which infÎ˜i R = infÎ˜ R, the estimator does at least as well as the minimax optimal
convergence speed in the best of these.
Another point is that we obtain more explicit constants than in Theorem 2.1.15.
It is also clear that a more careful choice of parameters could have brought some
improvement in the value of these constants.

2.3.
Two step localization
89
These results show that the selection scheme described in this section is a good
candidate to perform temperature selection of a Gibbs posterior distribution built
within a single parametric model in a rate optimal way, as well as a proposal with
proven performance bound for model selection.
2.3. Two step localization
2.3.1. Two step localization of bounds relative to a Gibbs prior.
Let
us reconsider the case where we want to choose adaptively among a family of
parametric models. Let us thus assume that the parameter set is a disjoint union
of measurable sub-models, so that we can write Î˜ = âŠ”mâˆˆMÎ˜m, where M is some
measurable index set. Let us choose some prior probability distribution on the
index set Î¼ âˆˆM1
+(M), and some regular conditional prior distribution Ï€ : M â†’
M1
+(Î˜), such that Ï€(i, Î˜i) = 1, i âˆˆM. Let us then study some arbitrary posterior
distributions Î½ : Î© â†’M1
+(M) and Ï : Î© Ã— M :â†’M1
+(Î˜), such that Ï(Ï‰, i, Î˜i) = 1,
Ï‰ âˆˆÎ©, i âˆˆM. We would like to compare Î½Ï(R) with some doubly localized prior
distribution Î¼exp[âˆ’
Î²
1+Î¶2 Ï€exp(âˆ’Î²R)(R)]

Ï€exp(âˆ’Î²R)

(R) (where Î¶2 is a positive parameter
to be set as needed later on). To ease notation we will deï¬ne two prior distributions
(one being more precisely a conditional distribution) depending on the positive real
parameters Î² and Î¶2, putting
(2.24)
Ï€ = Ï€exp(âˆ’Î²R) and Î¼ = Î¼exp[âˆ’
Î²
1+Î¶2 Ï€(R)].
Similarly to Theorem 1.4.3 on page 37 we can write for any positive real constants
Î² and Î³
P

(Î¼ Ï€) âŠ—(Î¼ Ï€)

exp

âˆ’N log

1 âˆ’tanh( Î³
N )Râ€²
âˆ’Î³râ€² âˆ’N log

cosh( Î³
N )

mâ€²
â‰¤1,
and deduce, using Lemma 1.1.3 on page 4, that
(2.25)
P

exp

sup
Î½âˆˆM1
+(M)
sup
Ï:Mâ†’M1
+(Î˜)

âˆ’N log

1 âˆ’tanh( Î³
N )(Î½Ï âˆ’Î¼ Ï€)(R)

âˆ’Î³(Î½Ï âˆ’Î¼ Ï€)(r) âˆ’N log

cosh( Î³
N )

(Î½Ï) âŠ—(Î¼ Ï€)(mâ€²)
âˆ’K(Î½, Î¼) âˆ’Î½

K(Ï, Ï€)

â‰¤1.
This will be our starting point in comparing Î½Ï(R) with Î¼ Ï€(R). However, obtaining
an empirical bound will require some supplementary eï¬€orts. For each index of the
model index set M, we can write in the same way
P

Ï€ âŠ—Ï€

exp

âˆ’N log

1 âˆ’tanh( Î³
N )Râ€²
âˆ’Î³râ€² âˆ’N log

cosh( Î³
N )

mâ€²
â‰¤1.
Integrating this inequality with respect to Î¼ and using Fubiniâ€™s lemma for positive
functions, we get
P

Î¼(Ï€ âŠ—Ï€)

exp

âˆ’N log

1 âˆ’tanh( Î³
N )Râ€²
âˆ’Î³râ€² âˆ’N log

cosh( Î³
N )

mâ€²
â‰¤1.

90
Chapter 2.
Comparing posterior distributions to Gibbs priors
Note that Î¼(Ï€ âŠ—Ï€) is a probability measure on M Ã— Î˜ Ã— Î˜, whereas (Î¼ Ï€) âŠ—(Î¼ Ï€)
considered previously is a probability measure on (M Ã— Î˜) Ã— (M Ã— Î˜). We get as
previously
(2.26)
P

exp

sup
Î½âˆˆM1
+(M)
sup
Ï:Mâ†’M1
+(Î˜)

âˆ’N log

1 âˆ’tanh( Î³
N )Î½(Ï âˆ’Ï€)(R)

âˆ’Î³Î½(Ï âˆ’Ï€)(r) âˆ’N log

cosh( Î³
N )

Î½(Ï âŠ—Ï€)(mâ€²)
âˆ’K(Î½, Î¼) âˆ’Î½

K(Ï, Ï€)

â‰¤1.
Let us ï¬nally recall that
K(Î½, Î¼) =
Î²
1+Î¶2 (Î½ âˆ’Î¼)Ï€(R) + K(Î½, Î¼) âˆ’K(Î¼, Î¼),
(2.27)
K(Ï, Ï€) = Î²(Ï âˆ’Ï€)(R) + K(Ï, Ï€) âˆ’K(Ï€, Ï€).
(2.28)
From equations (2.25), (2.26) and (2.28) we deduce
Proposition 2.3.1.
For any positive real constants Î², Î³ and Î¶2, with P probability
at least 1 âˆ’Ïµ, for any posterior distribution Î½ : Î© â†’M1
+(M) and any conditional
posterior distribution Ï : Î© Ã— M â†’M1
+(Î˜),
âˆ’N log

1 âˆ’tanh( Î³
N )(Î½Ï âˆ’Î¼ Ï€)(R)

âˆ’Î²Î½(Ï âˆ’Ï€)(R)
â‰¤Î³(Î½Ï âˆ’Î¼ Ï€)(r) + N log

cosh( Î³
N )

(Î½Ï) âŠ—(Î¼ Ï€)(mâ€²)
+ K(Î½, Î¼) + Î½

K(Ï, Ï€)

âˆ’Î½

K(Ï€, Ï€)

+ log

 2
Ïµ

.
and
âˆ’N log

1 âˆ’tanh( Î³
N )Î½(Ï âˆ’Ï€)(R)

â‰¤Î³Î½(Ï âˆ’Ï€)(r) + N log

cosh( Î³
N )

Î½(Ï âŠ—Ï€)(mâ€²)
+ K(Î½, Î¼) + Î½

K(Ï, Ï€)

+ log

 2
Ïµ

,
where the prior distribution Î¼ Ï€ is deï¬ned by equation (2.24) on page 89 and depends
on Î² and Î¶2.
Let us put for short
T = tanh( Î³
N ) and C = N log

cosh( Î³
N )

.
We will use an entropy compensation strategy for which we need a couple of
entropy bounds. We have according to Proposition 2.3.1, with P probability at
least 1 âˆ’Ïµ,
Î½

K(Ï, Ï€)

= Î²Î½(Ï âˆ’Ï€)(R) + Î½

K(Ï, Ï€) âˆ’K(Ï€, Ï€)

â‰¤
Î²
NT

Î³Î½(Ï âˆ’Ï€)(r) + CÎ½(Ï âŠ—Ï€)(mâ€²)
+ K(Î½, Î¼) + Î½

K(Ï, Ï€)

+ log( 2
Ïµ )

+ Î½

K(Ï, Ï€) âˆ’K(Ï€, Ï€)

.

2.3.
Two step localization
91
Similarly
K(Î½, Î¼) =
Î²
1 + Î¶2
(Î½ âˆ’Î¼)Ï€(R) + K(Î½, Î¼) âˆ’K(Î¼, Î¼)
â‰¤
Î²
(1 + Î¶2)NT

Î³(Î½ âˆ’Î¼)Ï€(r) + C(Î½Ï€) âŠ—(Î¼ Ï€)(mâ€²)
+ K(Î½, Î¼) + log( 2
Ïµ )

+ K(Î½, Î¼) âˆ’K(Î¼, Î¼).
Thus, for any positive real constants Î², Î³ and Î¶i, i = 1, . . . , 5, with P probability
at least 1 âˆ’Ïµ, for any posterior distributions Î½, Î½3 : Î© â†’M1
+(Î˜), any posterior
conditional distributions Ï, Ï1, Ï2, Ï4, Ï5 : Î© Ã— M â†’M1
+(Î˜),
âˆ’N log

1 âˆ’T(Î½Ï âˆ’Î¼ Ï€)(R)

âˆ’Î²Î½(Ï âˆ’Ï€)(R)
â‰¤Î³(Î½Ï âˆ’Î¼ Ï€)(r) + C(Î½Ï) âŠ—(Î¼ Ï€)(mâ€²)
+ K(Î½, Î¼) + Î½

K(Ï, Ï€) âˆ’K(Ï€, Ï€)

+ log( 2
Ïµ ),
Î¶1
NT
Î² Î¼

K(Ï1, Ï€)

â‰¤Î¶1Î³Î¼(Ï1 âˆ’Ï€)(r) + Î¶1CÎ¼(Ï1 âŠ—Ï€)(mâ€²)
+ Î¶1Î¼

K(Ï1, Ï€)

+ Î¶1 log( 2
Ïµ ) + Î¶1
NT
Î² Î¼

K(Ï1, Ï€) âˆ’K(Ï€, Ï€)

,
Î¶2
NT
Î² Î½

K(Ï2, Ï€)

â‰¤Î¶2Î³Î½(Ï2 âˆ’Ï€)(r) + Î¶2CÎ½(Ï2 âŠ—Ï€)(mâ€²)
+ Î¶2K(Î½, Î¼) + Î¶2Î½

K(Ï2, Ï€)

+ Î¶2 log( 2
Ïµ )
+ Î¶2
NT
Î² Î½

K(Ï2, Ï€) âˆ’K(Ï€, Ï€)

,
Î¶3(1 + Î¶2)NT
Î² K(Î½3, Î¼) â‰¤Î¶3Î³(Î½3 âˆ’Î¼)Ï€(r)
+ Î¶3C

(Î½3Ï€) âŠ—(Î½3Ï1) + (Î½3Ï1) âŠ—(Î¼ Ï€)

(mâ€²) + Î¶3K(Î½3, Î¼) + Î¶3 log( 2
Ïµ )
+ Î¶3(1 + Î¶2)NT
Î²

K(Î½3, Î¼) âˆ’K(Î¼, Î¼)

,
Î¶4
NT
Î² Î½3

K(Ï4, Ï€)

â‰¤Î¶4Î³Î½3(Ï4 âˆ’Ï€)(r)
+ Î¶4CÎ½3(Ï4 âŠ—Ï€)(mâ€²) + Î¶4K(Î½3, Î¼) + Î¶4Î½3

K(Ï4, Ï€)

+ Î¶4 log( 2
Ïµ )
+ Î¶4
NT
Î² Î½3

K(Ï4, Ï€) âˆ’K(Ï€, Ï€)

,
Î¶5
NT
Î² Î¼

K(Ï5, Ï€)

â‰¤Î¶5Î³Î¼(Ï5 âˆ’Ï€)(r) + Î¶5CÎ¼(Ï5 âŠ—Ï€)(mâ€²)
+ Î¶5Î¼

K(Ï5, Ï€)

+ Î¶5 log( 2
Ïµ ) + Î¶5
NT
Î² Î¼

K(Ï5, Ï€) âˆ’K(Ï€, Ï€)

.
Adding these six inequalities and assuming that
(2.29)
Î¶4 â‰¤Î¶3

(1 + Î¶2) NT
Î² âˆ’1

,
we ï¬nd
âˆ’N log

1 âˆ’T(Î½Ï âˆ’Î¼ Ï€)(R)

âˆ’Î²(Î½Ï âˆ’Î¼ Ï€)(R)
â‰¤âˆ’N log

1 âˆ’T(Î½Ï âˆ’Î¼ Ï€)(R)

âˆ’Î²(Î½Ï âˆ’Î¼ Ï€)(R)

92
Chapter 2.
Comparing posterior distributions to Gibbs priors
+ Î¶1

 NT
Î² âˆ’1

Î¼

K(Ï1, Ï€)

+ Î¶2

 NT
Î² âˆ’1

Î½

K(Ï2, Ï€)

+

Î¶3(1 + Î¶2) NT
Î² âˆ’Î¶3 âˆ’Î¶4

K(Î½3, Î¼)
+ Î¶4

 NT
Î² âˆ’1

Î½3

K(Ï4, Ï€)

+ Î¶5

 NT
Î² âˆ’1

Î¼

K(Ï5, Ï€)

â‰¤Î³(Î½Ï âˆ’Î¼ Ï€)(r) + Î¶1Î³Î¼(Ï1 âˆ’Ï€)(r) + Î¶2Î³Î½(Ï2 âˆ’Ï€)(r)
+ Î¶3Î³(Î½3 âˆ’Î¼)Ï€(r) + Î¶4Î³Î½3(Ï4 âˆ’Ï€)(r) + Î¶5Î³Î¼(Ï5 âˆ’Ï€)(r)
+ C

(Î½Ï) âŠ—(Î¼ Ï€) + Î¶1Î¼(Ï1 âŠ—Ï€) + Î¶2Î½(Ï2 âŠ—Ï€)
+ Î¶3(Î½3Ï€) âŠ—(Î½3Ï1) + Î¶3(Î½3Ï1) âŠ—(Î¼ Ï€)
+ Î¶4Î½3(Ï4 âŠ—Ï€) + Î¶5Î¼(Ï5 âŠ—Ï€)

(mâ€²)
+ (1 + Î¶2)

K(Î½, Î¼) âˆ’K(Î¼, Î¼)

+ Î½

K(Ï, Ï€) âˆ’K(Ï€, Ï€)

+ Î¶1 NT
Î² Î¼

K(Ï1, Ï€) âˆ’K(Ï€, Ï€)

+ Î¶2 NT
Î² Î½

K(Ï2, Ï€) âˆ’K(Ï€, Ï€)

+ Î¶3(1 + Î¶2) NT
Î²

K(Î½3, Î¼) âˆ’K(Î¼, Î¼)

+ Î¶4 NT
Î² Î½3

K(Ï4, Ï€) âˆ’K(Ï€, Ï€)

+ Î¶5 NT
Î² Î¼

K(Ï5, Ï€) âˆ’K(Ï€, Ï€)

+ (1 + Î¶1 + Î¶2 + Î¶3 + Î¶4 + Î¶5) log( 2
Ïµ ),
where we have also used the fact (concerning the 11th line of the preceding inequal-
ities) that
âˆ’Î²(Î½Ï âˆ’Î¼ Ï€)(R) + K(Î½, Î¼) + Î½

K(Ï, Ï€)

â‰¤âˆ’Î²(Î½Ï âˆ’Î¼ Ï€)(R) + (1 + Î¶2)K(Î½, Î¼) + Î½

K(Ï, Ï€)

= (1 + Î¶2)

K(Î½, Î¼) âˆ’K(Î¼, Î¼)

+ Î½

K(Ï, Ï€) âˆ’K(Ï€, Ï€)

.
Let us now apply to Ï€ (we shall later do the same with Î¼) the following inequalities,
holding for any random functions of the sample and the parameters h : Î© Ã— Î˜ â†’R
and g : Î© Ã— Î˜ â†’R,
Ï€(g âˆ’h) âˆ’K(Ï€, Ï€) â‰¤
sup
Ï:Î©Ã—Mâ†’M1
+(Î˜)
Ï(g âˆ’h) âˆ’K(Ï, Ï€)
= log

Ï€

exp(g âˆ’h)

= log

Ï€

exp(âˆ’h)

+ log

Ï€exp(âˆ’h)

exp(g)

= âˆ’Ï€exp(âˆ’h)(h) âˆ’K(Ï€exp(âˆ’h), Ï€) + log

Ï€exp(âˆ’h)

exp(g)

.
When h and g are observable, and h is not too far from Î²r â‰ƒÎ²R, this gives a
way to replace Ï€ with a satisfactory empirical approximation. We will apply this
method, choosing Ï1 and Ï5 such that Î¼ Ï€ is replaced either with Î¼Ï1, when it comes
from the ï¬rst two inequalities or with Î¼Ï5 otherwise, choosing Ï2 such that Î½Ï€ is
replaced with Î½Ï2 and Ï4 such that Î½3Ï€ is replaced with Î½3Ï4. We will do so because
it leads to a lot of helpful cancellations. For those to happen, we need to choose
Ïi = Ï€exp(âˆ’Î»ir), i = 1, 2, 4, where Î»1, Î»2 and Î»4 are such that
(1 + Î¶1)Î³ = Î¶1 NT
Î² Î»1,
(2.30)
Î¶2Î³ =

1 + Î¶2 NT
Î²

Î»2,
(2.31)
(Î¶4 âˆ’Î¶3)Î³ = Î¶4
NT
Î² Î»4,
(2.32)
Î¶3Î³ = Î¶5 NT
Î² Î»5,
(2.33)
and to assume that
(2.34)
Î¶4 > Î¶3.

2.3.
Two step localization
93
We obtain that with P probability at least 1 âˆ’Ïµ,
âˆ’N log

1 âˆ’T(Î¼Ï âˆ’Î¼ Ï€)(R)

âˆ’Î²(Î½Ï âˆ’Î¼ Ï€)(R)
â‰¤Î³(Î½Ï âˆ’Î¼ Ï1)(r) + Î¶3Î³(Î½3Ï4 âˆ’Î¼Ï5)(r)
+ Î¶1 NT
Î² Î¼

log
&
Ï1

exp

C
Î²
NT Î¶1

Î½Ï + Î¶1Ï1

(mâ€²)
'
+

1 + Î¶2 NT
Î²

Î½

log

Ï2

exp

C
1+Î¶2 NT
Î² Î¶2Ï2(mâ€²)
'
+ Î¶4 NT
Î² Î½3

log
&
Ï4

exp

C
Î²
NT Î¶4

Î¶3Î½3Ï1 + Î¶4Ï4

(mâ€²)
'
+ Î¶5 NT
Î² Î¼

log
&
Ï5

exp

C
Î²
NT Î¶5

Î¶3Î½3Ï1 + Î¶5Ï5

(mâ€²)
'
+ (1 + Î¶2)

K(Î½, Î¼) âˆ’K(Î¼, Î¼)

+ Î½

K(Ï, Ï€) âˆ’K(Ï2, Ï€)

+ Î¶3(1 + Î¶2) NT
Î²

K(Î½3, Î¼) âˆ’K(Î¼, Î¼)

+
 
1 +
5

i=1
Î¶i
!
log

 2
Ïµ

.
In order to obtain more cancellations while replacing Î¼ by some posterior distri-
bution, we will choose the constants such that Î»5 = Î»4, which can be done by
choosing
(2.35)
Î¶5 =
Î¶3Î¶4
Î¶4 âˆ’Î¶3
.
We can now replace Î¼ with Î¼exp âˆ’Î¾1Ï1(r)âˆ’Î¾4Ï4(r), where
Î¾1 =
Î³
(1 + Î¶2)

1 + NT
Î² Î¶3
,
(2.36)
Î¾4 =
Î³Î¶3
(1 + Î¶2)

1 + NT
Î² Î¶3
.
(2.37)
Choosing moreover Î½3 = Î¼exp âˆ’Î¾1Ï1(r)âˆ’Î¾4Ï4(r), to induce some more cancellations,
we get
Theorem 2.3.2.
Let us use the notation introduced above. For any positive real
constants satisfying equations (2.29, page 91), (2.30, page 92), (2.31, page 92),
(2.32, page 92), (2.33, page 92), (2.34, page 92), (2.35, page 93), (2.36, page 93),
(2.37, page 93), with P probability at least 1 âˆ’Ïµ, for any posterior distribution
Î½ : Î© â†’M1
+(M) and any conditional posterior distribution Ï : Î© Ã— M â†’M1
+(Î˜),
âˆ’N log

1 âˆ’T(Î½Ï âˆ’Î¼ Ï€)(R)

âˆ’Î²(Î½Ï âˆ’Î¼ Ï€)(R) â‰¤B(Î½, Ï, Î²),
where B(Î½, Ï, Î²)
def
= Î³(Î½Ï âˆ’Î½3Ï1)(r)
+ (1 + Î¶2)

1 + NT
Î² Î¶3

Ã— log

Î½3
&
Ï1

exp

C
Î²
NT Î¶1

Î½Ï + Î¶1Ï1

(mâ€²)

Î¶1NT
Î²(1+Î¶2)(1+ NT
Î²
Î¶3)

94
Chapter 2.
Comparing posterior distributions to Gibbs priors
Ã— Ï4

exp

C
Î²
NT Î¶5

Î¶3Î½3Ï1 + Î¶5Ï4

(mâ€²)

Î¶5NT
Î²(1+Î¶2)(1+ NT
Î²
Î¶3)
'
+

1 + Î¶2 NT
Î²

Î½

log

Ï2

exp

C
1+Î¶2 NT
Î² Î¶2Ï2(mâ€²)
'
+ Î¶4 NT
Î² Î½3

log
&
Ï4

exp

C
Î²
NT Î¶4

Î¶3Î½3Ï1 + Î¶4Ï4

(mâ€²)
'
+ (1 + Î¶2)

K(Î½, Î¼) âˆ’K(Î½3, Î¼)

+ Î½

K(Ï, Ï€) âˆ’K(Ï2, Ï€)

+
 
1 +
5

i=1
Î¶i
!
log

 2
Ïµ

.
This theorem can be used to ï¬nd the largest value 	Î²(Î½Ï) of Î² such that B(Î½, Ï,
Î²) â‰¤0, thus providing an estimator for Î²(Î½Ï) deï¬ned as Î½Ï(R) = Î¼Î²(Î½Ï)Ï€Î²(Î½Ï)(R),
where we have mentioned explicitly the dependence of Î¼ and Ï€ in Î², the constant
Î¶2 staying ï¬xed. The posterior distribution Î½Ï may then be chosen to maximize
	Î²(Î½Ï) within some manageable subset of posterior distributions P, thus gaining
the assurance that Î½Ï(R) â‰¤Î¼	
Î²(Î½Ï)Ï€	
Î²(Î½Ï)(R), with the largest parameter 	Î²(Î½Ï)
that this approach can provide. Maximizing 	Î²(Î½Ï) is supported by the fact that
limÎ²â†’+âˆžÎ¼Î²Ï€Î²(R) = ess infÎ¼Ï€ R. Anyhow, there is no assurance (to our knowledge)
that Î² â†’Î¼Î²Ï€Î²(R) will be a decreasing function of Î² all the way, although this may
be expected to be the case in many practical situations.
We can make the bound more explicit in several ways. One point of view is to
put forward the optimal values of Ï and Î½. We can thus remark that
Î½

Î³Ï(r) + K(Ï, Ï€) âˆ’K(Ï2, Ï€)

+ (1 + Î¶2)K(Î½, Î¼)
= Î½

K

Ï, Ï€exp(âˆ’Î³r)

+ Î»2Ï2(r) +
 Î³
Î»2 Ï€exp(âˆ’Î±r)(r)dÎ±

+ (1 + Î¶2)K(Î½, Î¼)
= Î½

K

Ï, Ï€exp(âˆ’Î³r)

+ (1 + Î¶2)K

Î½, Î¼exp

âˆ’Î»2Ï2(r)
1+Î¶2
âˆ’
1
1+Î¶2
 Î³
Î»2
Ï€exp(âˆ’Î±r)(r)dÎ±

âˆ’(1 + Î¶2) log

Î¼
&
exp

âˆ’
Î»2
1 + Î¶2
Ï2(r) âˆ’
1
1 + Î¶2
 Î³
Î»2
Ï€exp(âˆ’Î±r)(r)dÎ±
'
.
Thus
B(Î½, Ï, Î²) = (1 + Î¶2)

Î¾1Î½3Ï1(r) + Î¾4Î½3Ï4(r)
+ log

Î¼

exp

âˆ’Î¾1Ï1(r) âˆ’Î¾4Ï4(r)

âˆ’(1 + Î¶2) log

Î¼
&
exp

âˆ’
Î»2
1 + Î¶2
Ï2(r) âˆ’
1
1 + Î¶2
 Î³
Î»2
Ï€exp(âˆ’Î±r)(r)dÎ±
'
âˆ’Î³Î½3Ï1(r) + (1 + Î¶2)

1 + NT
Î² Î¶3

Ã— log

Î½3
&
Ï1

exp

C
Î²
NT Î¶1

Î½Ï + Î¶1Ï1

(mâ€²)

Î¶1NT
Î²(1+Î¶2)(1+ NT
Î²
Î¶3)
Ã— Ï4

exp

C
Î²
NT Î¶5

Î¶3Î½3Ï1 + Î¶5Ï4

(mâ€²)

Î¶5NT
Î²(1+Î¶2)(1+ NT
Î²
Î¶3)
'

2.3.
Two step localization
95
+

1 + Î¶2 NT
Î²

Î½

log

Ï2

exp

C
1+Î¶2 NT
Î² Î¶2Ï2(mâ€²)
'
+ Î¶4 NT
Î² Î½3

log
&
Ï4

exp

C
Î²
NT Î¶4

Î¶3Î½3Ï1 + Î¶4Ï4

(mâ€²)
'
+ Î½

K

Ï, Ï€exp(âˆ’Î³r)

+ (1 + Î¶2)K

Î½, Î¼exp

âˆ’Î»2Ï2(r)
1+Î¶2
âˆ’
1
1+Î¶2
 Î³
Î»2
Ï€exp(âˆ’Î±r)(r)dÎ±

+
 
1 +
5

i=1
Î¶i
!
log

 2
Ïµ

.
This formula is better understood when thinking about the following upper bound
for the two ï¬rst lines in the expression of B(Î½, Ï, Î²):
(1 + Î¶2)

Î¾1Î½3Ï1(r) + Î¾4Î½3Ï4(r) + log

Î¼

exp

âˆ’Î¾1Ï1(r) âˆ’Î¾4Ï4(r)

âˆ’(1 + Î¶2) log

Î¼
&
exp

âˆ’
Î»2
1 + Î¶2
Ï2(r)
âˆ’
1
1 + Î¶2
 Î³
Î»2
Ï€exp(âˆ’Î±r)(r)dÎ±
'
âˆ’Î³Î½3Ï1(r)
â‰¤Î½3

Î»2Ï2(r) +
 Î³
Î»2
Ï€exp(âˆ’Î±r)(r)dÎ± âˆ’Î³Ï1(r)

.
Another approach to understanding Theorem 2.3.2 is to put forward Ï0
=
Ï€exp(âˆ’Î»0r), for some positive real constant Î»0 < Î³, noticing that
Î½

K(Ï0, Ï€) âˆ’K(Ï2, Ï€)

= Î»0Î½(Ï2 âˆ’Ï0)(r) âˆ’Î½

K(Ï2, Ï0)

.
Thus
B(Î½, Ï0, Î²) â‰¤Î½3

(Î³ âˆ’Î»0)(Ï0 âˆ’Ï1)(r) + Î»0(Ï2 âˆ’Ï1)(r)

+ (1 + Î¶2)

1 + NT
Î² Î¶3

Ã— log

Î½3
&
Ï1

exp

C
Î²
NT Î¶1

Î½Ï0 + Î¶1Ï1

(mâ€²)

Î¶1NT
Î²(1+Î¶2)(1+ NT
Î²
Î¶3)
Ã— Ï4

exp

C
Î²
NT Î¶5

Î¶3Î½3Ï1 + Î¶5Ï4

(mâ€²)

Î¶5NT
Î²(1+Î¶2)(1+ NT
Î²
Î¶3)
'
+

1 + Î¶2 NT
Î²

Î½

log

Ï2

exp

C
1+Î¶2 NT
Î² Î¶2Ï2(mâ€²)
'
+ Î¶4 NT
Î² Î½3

log
&
Ï4

exp

C
Î²
NT Î¶4

Î¶3Î½3Ï1 + Î¶4Ï4

(mâ€²)
'
+ (1 + Î¶2)K

Î½, Î¼exp

âˆ’(Î³âˆ’Î»0)Ï0(r)+Î»0Ï2(r)
1+Î¶2


âˆ’Î½

K(Ï2, Ï0)

+
 
1 +
5

i=1
Î¶i
!
log

 2
Ïµ

.

96
Chapter 2.
Comparing posterior distributions to Gibbs priors
In the case when we want to select a single model 	m(Ï‰), and therefore to set
Î½ = Î´	
m, the previous inequality engages us to take
	m âˆˆarg min
mâˆˆM(Î³ âˆ’Î»0)Ï0(m, r) + Î»0Ï2(m, r).
In parametric situations where
Ï€exp(âˆ’Î»r)(r) â‰ƒrâ‹†(m) + de(m)
Î»
,
we get
(Î³ âˆ’Î»0)Ï0(m, r) âˆ’Î»0Ï2(m, r) â‰ƒÎ³

râ‹†(m) + de(m)

 1
Î»0 + Î»0âˆ’Î»2
Î³Î»2

,
resulting in a linear penalization of the empirical dimension of the models.
2.3.2. Analysis of two step bounds relative to a Gibbs prior.
We will
not state a formal result, but will nevertheless give some hints about how to establish
one. This is a rather technical section, which can be skipped at a ï¬rst reading ,
since it will not be used below. We should start from Theorem 1.4.2 (page 35),
which gives a deterministic variance term. From Theorem 1.4.2, after a change
of prior distribution, we obtain for any positive constants Î±1 and Î±2, any prior
distributions Î¼1 and Î¼2 âˆˆM1
+(M), for any prior conditional distributions Ï€1 and
Ï€2 : M â†’M1
+(Î˜), with P probability at least 1 âˆ’Î·, for any posterior distributions
Î½1Ï1 and Î½2Ï2,
Î±1(Î½1Ï1 âˆ’Î½2Ï2)(R) â‰¤Î±2(Î½1Ï1 âˆ’Î½2Ï2)(r)
+ K

(Î½1Ï1) âŠ—(Î½2Ï2), (Î¼1 Ï€1) âŠ—(Î¼2 Ï€2)

+ log

(Î¼1 Ï€1) âŠ—(Î¼2 Ï€2)

exp

âˆ’Î±2Î¨ Î±2
N (Râ€², M â€²) + Î±1Râ€²
âˆ’log(Î·).
Applying this to Î±1 = 0, we get that
(Î½Ï âˆ’Î½3Ï1)(r) â‰¤1
Î±2

K

(Î½Ï) âŠ—(Î½3Ï1), (Î¼ Ï€) âŠ—(Î¼3 Ï€1)

+ log

(Î¼ Î½) âŠ—(Î¼3 Ï€1)

exp

Î±2Î¨âˆ’Î±2
N (Râ€², M â€²)

âˆ’log(Î·)

.
In the same way, to bound quantities of the form
log

Î½3
&
Ï1

exp

C1(Î½Ï + Î¶1Ï1)(mâ€²)
p1
Ã— Ï4

exp

C2

Î¶3Î½3Ï1 + Î¶5Ï4

(mâ€²)
p2'
= sup
Î½5

p1 sup
Ï5

C1

(Î½Ï) âŠ—(Î½5Ï5) + Î¶1Î½5(Ï1 âŠ—Ï5)

(mâ€²) âˆ’K(Ï5, Ï1)

+ p2 sup
Ï6

C2

Î¶3(Î½3Ï1) âŠ—(Î½5Ï6)
+ Î¶5Î½5(Ï4 âŠ—Ï6)

(mâ€²) âˆ’K(Ï6, Ï4)

âˆ’K(Î½5, Î½3)

,

2.3.
Two step localization
97
where C1, C2, p1 and p2 are positive constants, and similar terms, we need to use
inequalities of the type: for any prior distributions Î¼i Ï€i, i = 1, 2, with P probability
at least 1 âˆ’Î·, for any posterior distributions Î½iÏi, i = 1, 2,
Î±3(Î½1Ï1) âŠ—(Î½2Ï2)(mâ€²) â‰¤log

(Î¼1 Ï€1) âŠ—(Î¼2 Ï€2) exp

Î±3Î¦ âˆ’Î±3
N (M â€²)

+ K

(Î½1Ï1) âŠ—(Î½2Ï2), (Î¼1 Ï€1) âŠ—(Î¼2 Ï€2)

âˆ’log(Î·).
We need also the variant: with P probability at least 1 âˆ’Î·, for any posterior dis-
tribution Î½1 : Î© â†’M1
+(M) and any conditional posterior distributions Ï1, Ï2 :
Î© Ã— M â†’M1
+(Î˜),
Î±3Î½1(Ï1 âŠ—Ï2)(mâ€²) â‰¤log

Î¼1

Ï€1 âŠ—Ï€2

exp

Î±3Î¦âˆ’Î±3
N (M â€²)

+ K(Î½1, Î¼1) + Î½1

K

Ï1 âŠ—Ï2, Ï€1 âŠ—Ï€2

âˆ’log(Î·).
We deduce that
log

Î½3
&
Ï1

exp

C1(Î½Ï + Î¶1Ï1)(mâ€²)
p1
Ã— Ï4

exp

C2

Î¶3Î½3Ï1 + Î¶5Ï4

(mâ€²)
p2'
â‰¤sup
Î½5

p1 sup
Ï5
&
C1
Î±3

log

(Î¼ Ï€) âŠ—(Î¼5 Ï€5) exp

Î±3Î¦âˆ’Î±3
N (M â€²)

+ K

(Î½Ï) âŠ—(Î½5Ï5), (Î¼ Ï€ âŠ—(Î¼5 Ï€5)

+ log( 2
Î·)
+ Î¶1

log

Î¼5

Ï€1 âŠ—Ï€5

exp

Î±3Î¦âˆ’Î±3
N (M â€²)

+ K(Î½5, Î¼5) + Î½5

K

Ï1 âŠ—Ï5, Ï€1 âŠ—Ï€5

+ log

 2
Î·

âˆ’K(Ï5, Ï1)
'
+ p2 sup
Ï6
&
C1
Î±3

log

(Î¼3 Ï€1) âŠ—(Î¼5 Ï€6) exp

Î±3Î¦âˆ’Î±3
N (M â€²)

+ K

(Î½3Ï1) âŠ—(Î½5Ï6), (Î¼3 Ï€1 âŠ—(Î¼5 Ï€6)

+ log( 2
Î·)
+ Î¶1

log

Î¼5

Ï€4 âŠ—Ï€6

exp

Î±3Î¦âˆ’Î±3
N (M â€²)

+ K(Î½5, Î¼5) + Î½5

K

Ï4 âŠ—Ï6, Ï€4 âŠ—Ï€6

+ log

 2
Î·

âˆ’K(Ï6, Ï4)
'
âˆ’K(Î½5, Î½3)

.
We are then left with the need to bound entropy terms like K(Î½3Ï1, Î¼3Ï€1), where
we have the choice of Î¼3 and Ï€1, to obtain a useful bound. As could be expected,
we decompose it into
K(Î½3Ï1, Î¼3Ï€1) = K(Î½3, Î¼3) + Î½3

K(Ï1, Ï€1)

.
Let us look after the second term ï¬rst, choosing Ï€1 = Ï€exp(âˆ’Î²1R):

98
Chapter 2.
Comparing posterior distributions to Gibbs priors
Î½3

K(Ï1, Ï€1)

= Î½3

Î²1(Ï1 âˆ’Ï€1)(R) + K(Ï1, Ï€) âˆ’K(Ï€1, Ï€)

â‰¤Î²1
Î±1

Î±2Î½3(Ï1 âˆ’Ï€1)(r) + K(Î½3, Î¼3) + Î½3

K(Ï1, Ï€1)

+ log

Î¼3

Ï€âŠ—2
1

exp

âˆ’Î±2Î¨ Î±2
N (Râ€², M â€²) + Î±1Râ€²
âˆ’log(Î·)

+ Î½3

K(Ï1, Ï€) âˆ’K(Ï€1, Ï€)

â‰¤Î²1
Î±1

K(Î½3, Î¼3) + Î½3

K(Ï1, Ï€1)

+ log

Î¼3

Ï€âŠ—2
1

exp

âˆ’Î±2Î¨ Î±2
N (Râ€², M â€²) + Î±1Râ€²
âˆ’log(Î·)

+ Î½3

K

Ï1, Ï€exp(âˆ’Î²1Î±2
Î±1
r)

.
Thus, when the constraint Î»1 = Î²1Î±2
Î±1
is satisï¬ed,
Î½3

K(Ï1, Ï€1)

â‰¤

1 âˆ’Î²1
Î±1
âˆ’1 Î²1
Î±1

K(Î½3, Î¼3)
+ log

Î¼3

Ï€âŠ—2
1

exp

âˆ’Î±2Î¨ Î±2
N (Râ€², M â€²) + Î±1Râ€²
âˆ’log(Î·)

.
We can further specialize the constants, choosing Î±1 = N sinh( Î±2
N ), so that
âˆ’Î±2Î¨ Î±2
N (Râ€², M â€²) + Î±1Râ€² â‰¤2N sinh
 Î±2
2N
2
M â€².
We can for instance choose Î±2 = Î³, Î±1 = N sinh( Î³
N ) and Î²1 = Î»1 N
Î³ sinh( Î³
N ), leading
to
Proposition 2.3.3. With the notation of Theorem 2.3.2, the constants being set
as explained above, putting Ï€1 = Ï€exp(âˆ’Î»1 N
Î³ sinh( Î³
N )R), with P probability at least
1 âˆ’Î·,
Î½3

K(Ï1, Ï€1)

â‰¤

1 âˆ’Î»1
Î³
âˆ’1 Î»1
Î³

K(Î½3, Î¼3)
+ log

Î¼3

Ï€âŠ—2
1

exp

2N sinh( Î³
2N )2M â€²
âˆ’log(Î·)

.
More generally
Î½3

K(Ï, Ï€1)

â‰¤

1 âˆ’Î»1
Î³
âˆ’1 Î»1
Î³

K(Î½3, Î¼3)
+ log

Î¼3

Ï€âŠ—2
1

exp

2N sinh( Î³
2N )2M â€²
âˆ’log(Î·)

+

1 âˆ’Î»1
Î³
âˆ’1
Î½3

K(Ï, Ï1)

.
In a similar way, let us now choose Î¼3 = Î¼exp[âˆ’Î±3Ï€(R)]. We can write
K(Î½, Î¼3) = Î±3(Î½ âˆ’Î¼3)Ï€(R) + K(Î½, Î¼) âˆ’K(Î¼3, Î¼)

2.3.
Two step localization
99
â‰¤Î±3
Î±1

Î±2(Î½ âˆ’Î¼3)Ï€(r) + K(Î½, Î¼3)
+ log

(Î¼3Ï€) âŠ—(Î¼3Ï€)

exp

âˆ’Î±2Î¨ Î±2
N (Râ€², M â€²) + Î±1Râ€²
âˆ’log(Î·)

+ K(Î½, Î¼) âˆ’K(Î¼3, Î¼).
Let us choose Î±2 = Î³, Î±1 = N sinh( Î³
N ), and let us add some other entropy inequal-
ities to get rid of Ï€ in a suitable way, the approach of entropy compensation being
the same as that used to obtain the empirical bound of Theorem 2.3.2 (page 93).
This results with P probability at least 1 âˆ’Î· in

1 âˆ’Î±3
Î±1

K(Î½, Î¼3) â‰¤Î±3
Î±1

Î³(Î½ âˆ’Î¼3)Ï€(r)
+ log

(Î¼3Ï€) âŠ—(Î¼3Ï€)

exp

âˆ’Î³Î¨ Î³
N (Râ€², M â€²) + Î±1Râ€²
+ log( 2
Î·)

+ K(Î½, Î¼) âˆ’K(Î¼3, Î¼),
Î¶6

1 âˆ’Î²
Î±1

Î¼3

K(Ï6, Ï€)

â‰¤Î¶6
Î²
Î±1

Î³Î¼3(Ï6 âˆ’Ï€)(r)
+ log

Î¼3

Ï€âŠ—2
exp

âˆ’Î³Î¨ Î³
N (Râ€², M â€²) + Î±1Râ€²
+ log( 2
Î·)

+ Î¶6Î¼3

K(Ï6, Ï€) âˆ’K(Ï€, Ï€)

,
Î¶7

1 âˆ’Î²
Î±1

Î¼3

K(Ï7, Ï€)

â‰¤Î¶7
Î²
Î±1

Î³Î¼3(Ï7 âˆ’Ï€)(r)
+ log

Î¼3

Ï€âŠ—2
exp

âˆ’Î³Î¨ Î³
N (Râ€², M â€²) + Î±1Râ€²
+ log( 2
Î·)

+ Î¶7Î¼3

K(Ï7, Ï€) âˆ’K(Ï€, Ï€)

,
Î¶8

1 âˆ’Î²
Î±1

Î½

K(Ï8, Ï€)

â‰¤Î¶8
Î²
Î±1

Î³Î½(Ï8 âˆ’Ï€)(r) + K(Î½, Î¼3)
+ log

Î¼3

Ï€âŠ—2
exp

âˆ’Î³Î¨ Î³
N (Râ€², M â€²) + Î±1Râ€²
+ log( 2
Î·)

+ Î¶8Î½

K(Ï8, Ï€) âˆ’K(Ï€, Ï€)

,
Î¶9

1 âˆ’Î²
Î±1

Î½

K(Ï9, Ï€)

â‰¤Î¶9
Î²
Î±1

Î³Î½(Ï9 âˆ’Ï€)(r) + K(Î½, Î¼3)
+ log

Î¼3

Ï€âŠ—2
exp

âˆ’Î³Î¨ Î³
N (Râ€², M â€²) + Î±1Râ€²
+ log( 2
Î·)

+ Î¶9Î½

K(Ï9, Ï€) âˆ’K(Ï€, Ï€)

,
where we have introduced a bunch of constants, assumed to be positive, that we
will more precisely set to
x8 + x9 = 1,
(Î¶6Î² + x8Î±3) Î³
Î±1
= Î»6,
(Î¶7Î² + x9Î±3) Î³
Î±1
= Î»7,
(Î¶8Î² âˆ’x8Î±3) Î³
Î±1
= Î»8,

100
Chapter 2.
Comparing posterior distributions to Gibbs priors
(Î¶9Î² âˆ’x9Î±3) Î³
Î±1
= Î»9.
We get with P probability at least 1 âˆ’Î·,

1 âˆ’Î±3
Î±1
âˆ’(Î¶8 + Î¶9) Î²
Î±1

K(Î½, Î¼3) â‰¤
Î±3
Î±1

Î³

Î½(x8Ï8 + x9Ï9)(r) âˆ’Î¼3(x8Ï6 + x9Ï7)(r)

+ Î±3
Î±1
log

(Î¼3Ï€) âŠ—(Î¼3Ï€)

exp

âˆ’Î³Î¨ Î³
N (Râ€², M â€²) + Î±1Râ€²
+ (Î¶6 + Î¶7 + Î¶8 + Î¶9) Î²
Î±1
log

Î¼3

Ï€âŠ—2
exp

âˆ’Î³Î¨ Î³
N (Râ€², M â€²) + Î±1Râ€²
+ K(Î½, Î¼) âˆ’K(Î¼3, Î¼) +
Î±3
Î±1
+ (Î¶6 + Î¶7 + Î¶8 + Î¶9) Î²
Î±1

log

 2
Î·

.
Let us choose the constants so that Î»1 = Î»7 = Î»9, Î»4 = Î»6 = Î»8, Î±3x9
Î³
Î±1 = Î¾1 and
Î±3x8
Î³
Î±1 = Î¾4. This is done by setting
x8 =
Î¾4
Î¾1 + Î¾4
,
x9 =
Î¾1
Î¾1 + Î¾4
,
Î±3 = N
Î³ sinh( Î³
N )(Î¾1 + Î¾4),
Î¶6 = N
Î³ sinh( Î³
N )(Î»4 âˆ’Î¾4)
Î²
,
Î¶7 = N
Î³ sinh( Î³
N )(Î»1 âˆ’Î¾1)
Î²
,
Î¶8 = N
Î³ sinh( Î³
N )(Î»4 + Î¾4)
Î²
,
Î¶9 = N
Î³ sinh( Î³
N )(Î»1 + Î¾1)
Î²
.
The inequality Î»1 > Î¾1 is always satisï¬ed. The inequality Î»4 > Î¾4 is required for
the above choice of constants, and will be satisï¬ed for a suitable choice of Î¶3 and
Î¶4.
Under these assumptions, we obtain with P probability at least 1 âˆ’Î·

1 âˆ’Î±3
Î±1
âˆ’(Î¶8 + Î¶9) Î²
Î±1

K(Î½, Î¼3) â‰¤(Î½ âˆ’Î¼3)(Î¾1Ï1 + Î¾4Ï4)(r)
+ Î±3
Î±1
log

(Î¼3Ï€) âŠ—(Î¼3Ï€)

exp

âˆ’Î³Î¨ Î³
N (Râ€², M â€²) + Î±1Râ€²
+ (Î¶6 + Î¶7 + Î¶8 + Î¶9) Î²
Î±1
log

Î¼3

Ï€âŠ—2
exp

âˆ’Î³Î¨ Î³
N (Râ€², M â€²) + Î±1Râ€²
+ K(Î½, Î¼) âˆ’K(Î¼3, Î¼) +
Î±3
Î±1
+ (Î¶6 + Î¶7 + Î¶8 + Î¶9) Î²
Î±1

log

 2
Î·

.
This proves
Proposition 2.3.4. The constants being set as explained above, with P probability
at least 1 âˆ’Î·, for any posterior distribution Î½ : Î© â†’M1
+(M),

2.3.
Two step localization
101
K(Î½, Î¼3) â‰¤

1 âˆ’Î±3
Î±1
âˆ’(Î¶8 + Î¶9) Î²
Î±1
âˆ’1
K(Î½, Î½3)
+ Î±3
Î±1
log

(Î¼3Ï€) âŠ—(Î¼3Ï€)

exp

âˆ’Î³Î¨ Î³
N (Râ€², M â€²) + Î±1Râ€²
+ (Î¶6 + Î¶7 + Î¶8 + Î¶9) Î²
Î±1
log

Î¼3

Ï€âŠ—2
exp

âˆ’Î³Î¨ Î³
N (Râ€², M â€²) + Î±1Râ€²
+
Î±3
Î±1
+ (Î¶6 + Î¶7 + Î¶8 + Î¶9) Î²
Î±1

log

 2
Î·

.
Thus
K(Î½3Ï1, Î¼3 Ï€1) â‰¤
1 +

1 âˆ’Î»1
Î³
âˆ’1 Î»1
Î³
1 âˆ’Î±3
Î±1 âˆ’(Î¶8 + Î¶9) Î²
Î±1
Ã—
Î±3
Î±1
log

(Î¼3Ï€ âŠ—(Î¼3Ï€)

exp

âˆ’Î³Î¨ Î³
N (Râ€², M â€²) + Î±1Râ€²
+ (Î¶6 + Î¶7 + Î¶8 + Î¶9) Î²
Î±1
log

Î¼3

Ï€âŠ—2
exp

âˆ’Î³Î¨ Î³
N (Râ€², M â€²) + Î±1Râ€²
+
Î±3
Î±1
+ (Î¶6 + Î¶7 + Î¶8 + Î¶9) Î²
Î±1

log

 2
Î·

+

1 âˆ’Î»1
Î³
âˆ’1 Î»1
Î³

log

Î¼3

Ï€âŠ—2
1

exp

2N sinh

 Î³
2N
2M â€²
âˆ’log( 2
Î·)

.
We will not go further, lest it may become tedious, but we hope we have given
suï¬ƒcient hints to state informally that the bound B(Î½, Ï, Î²) of Theorem 2.3.2 (page
93) is upper bounded with P probability close to one by a bound of the same ï¬‚avour
where the empirical quantities r and mâ€² have been replaced with their expectations
R and M â€².
2.3.3. Two step localization between posterior distributions.
Here we
work with a family of prior distributions described by a regular conditional prior
distribution Ï€ = M â†’M1
+(Î˜), where M is some measurable index set. This family
may typically describe a countable family of parametric models. In this case M = N,
and each of the prior distributions Ï€(i, .), i âˆˆN satisï¬es some parametric complexity
assumption of the type
lim sup
Î²â†’+âˆž
Î²

Ï€exp(âˆ’Î²R)(i, .)(R) âˆ’ess inf
Ï€(i,.) R

= di < +âˆž,
i âˆˆM.
Let us consider also a prior distribution Î¼ âˆˆM1
+(M) deï¬ned on the index set M.
Our aim here will be to compare the performance of two given posterior distri-
butions Î½1Ï1 and Î½2Ï2, where Î½1, Î½2 : Î© â†’M1
+(M), and where Ï1, Ï2 : Î© Ã— M â†’
M1
+(Î˜). More precisely, we would like to establish a bound for (Î½1Ï1 âˆ’Î½2Ï2)(R)
which could be a starting point to implement a selection method similar to the one
described in Theorem 2.2.4 (page 72). To this purpose, we can start with Theorem
2.2.1 (page 68), which says that with P probability at least 1 âˆ’Ïµ,
âˆ’N log

1 âˆ’tanh( Î»
N )

Î½1Ï1 âˆ’Î½2Ï2

(R)

â‰¤Î»(Î½1Ï1 âˆ’Î½2Ï2)(r)
+ N log

cosh( Î»
N )

(Î½1Ï1) âŠ—(Î½2Ï2)(mâ€²) + K(Î½1, Î¼) + K(Î½2, Î¼)
+ Î½1

K(Ï1, Ï€)

+ Î½2

K(Ï2, Ï€)

âˆ’log(Ïµ),

102
Chapter 2.
Comparing posterior distributions to Gibbs priors
where Î¼ âˆˆM1
+(M) and Ï€ : M â†’M1
+(Î˜) are suitably localized prior distributions
to be chosen later on. To use these localized prior distributions, we need empirical
bounds for the entropy terms K(Î½i, Î¼) and Î½i

K(Ïi, Ï€)

, i = 1, 2.
Bounding Î½

K(Ï, Ï€)

can be done using the following generalization of Corollary
2.1.19 page 68:
Corollary 2.3.5.
For any positive real constants Î³ and Î» such that Î³ < Î», for
any prior distribution Î¼ âˆˆM1
+(M) and any conditional prior distribution Ï€ : M â†’
M1
+(Î˜), with P probability at least 1 âˆ’Ïµ, for any posterior distribution Î½ : Î© â†’
M1
+(M), and any conditional posterior distribution Ï : Î© Ã— M â†’M1
+(Î˜),
Î½

K

Ï, Ï€exp[âˆ’N Î³
Î» tanh( Î»
N )R]

â‰¤Kâ€²(Î½, Ï, Î³, Î», Ïµ) +
1
Î»
Î³ âˆ’1K(Î½, Î¼),
where
Kâ€²(Î½, Ï, Î³, Î», Ïµ)
def
=

1 âˆ’Î³
Î»
âˆ’1
Î½

K(Ï, Ï€exp(âˆ’Î³r)

âˆ’Î³
Î» log(Ïµ) + Î½

log

Ï€exp(âˆ’Î³r)

exp

N Î³
Î» log

cosh( Î»
N )

Ï(mâ€²)

.
To apply this corollary to our case, we have to set
Ï€ = Ï€exp[âˆ’N Î³
Î» tanh( Î»
N )R].
Let us also consider for some positive real constant Î² the conditional prior distrib-
ution
Ï€ = Ï€exp(âˆ’Î²R)
and the prior distribution
Î¼ = Î¼exp[âˆ’Î±Ï€(R)].
Let us see how we can bound, given any posterior distribution Î½ : Î© â†’M1
+(M),
the divergence K(Î½, Î¼). We can see that
K(Î½, Î¼) = Î±(Î½ âˆ’Î¼)Ï€(R) + K(Î½, Î¼) âˆ’K(Î¼, Î¼).
Now, let us introduce the conditional posterior distribution
	Ï€ = Ï€exp(âˆ’Î³r)
and let us decompose
(Î½ âˆ’Î¼)

Ï€(R)

= Î½

Ï€(R) âˆ’	Ï€(R)

+ (Î½ âˆ’Î¼)

	Ï€(R)

+ Î¼

	Ï€(R) âˆ’Ï€(R)

.
Starting from the exponential inequality
P

Î¼

Ï€ âŠ—Ï€

exp

âˆ’N log

1 âˆ’tanh( Î³
N )Râ€²
âˆ’Î³râ€² âˆ’N log

cosh( Î³
N )

mâ€²
â‰¤1,
and reasoning in the same way that led to Theorem 2.1.1 (page 52) in the simple
case when we take in this theorem Î» = Î³, we get with P probability at least 1 âˆ’Ïµ,
that

2.3.
Two step localization
103
âˆ’N log

1 âˆ’tanh( Î³
N )Î½(Ï€ âˆ’	Ï€)(R)

+ Î²Î½(Ï€ âˆ’	Ï€)(R)
â‰¤Î½

log

	Ï€

exp

N log

cosh( Î³
N )	Ï€(mâ€²)

+ K(Î½, Î¼) âˆ’log(Ïµ).
âˆ’N log

1 âˆ’tanh( Î³
N )Î¼(	Ï€ âˆ’Ï€)(R)

âˆ’Î²Î¼(	Ï€ âˆ’Ï€)(R)
â‰¤Î¼

log

	Ï€

exp

N log

cosh( Î³
N )	Ï€(mâ€²)

âˆ’log(Ïµ).
In the meantime, using Theorem 2.2.1 (page 68) and Corollary 2.3.5 above, we
see that with P probability at least 1âˆ’2Ïµ, for any conditional posterior distribution
Ï : Î© Ã— M â†’M1
+(Î˜),
âˆ’N log

1 âˆ’tanh( Î»
N )(Î½ âˆ’Î¼)Ï(R)

â‰¤Î»(Î½ âˆ’Î¼)Ï(r)
+ N log

cosh( Î»
N )

(Î½Ï) âŠ—(Î¼Ï)(mâ€²) + (Î½ + Î¼)K(Ï, Ï€) + K(Î½, Î¼) âˆ’log(Ïµ)
â‰¤Î»(Î½ âˆ’Î¼)Ï(r) + N log

cosh( Î»
N )

(Î½Ï) âŠ—(Î¼Ï)(mâ€²) + K(Î½, Î¼) âˆ’log(Ïµ)
+

1 âˆ’Î³
Î»
âˆ’1
(Î½ + Î¼)

K

Ï, 	Ï€

+ log

	Ï€

exp

N Î³
Î» log

cosh( Î»
N )

Ï(mâ€²)

+

Î»
Î³ âˆ’1
âˆ’1
K(Î½, Î¼) âˆ’2 log(Ïµ)

.
Putting all this together, we see that with P probability at least 1 âˆ’3Ïµ, for any
posterior distribution Î½ âˆˆM1
+(M),

1 âˆ’
Î±
N tanh( Î³
N ) + Î² âˆ’
Î±
N tanh( Î»
N )

1 âˆ’Î³
Î»


K(Î½, Î¼) â‰¤
Î±

N tanh( Î³
N ) + Î²
âˆ’1
Î½

log

	Ï€

exp

N log

cosh( Î³
N )

	Ï€(mâ€²)

âˆ’log(Ïµ)

+ Î±

N tanh( Î³
N ) âˆ’Î²
âˆ’1
Î¼

log

	Ï€

exp

N log

cosh( Î³
N )

	Ï€(mâ€²)

âˆ’log(Ïµ)

+ Î±

N tanh( Î»
N )
âˆ’1

Î»(Î½ âˆ’Î¼)	Ï€(r) + N log

cosh( Î»
N )

(Î½	Ï€) âŠ—(Î¼	Ï€)(mâ€²)
+

1 âˆ’Î³
Î»
âˆ’1
(Î½ + Î¼)

log

	Ï€

exp

N Î³
Î» log

cosh( Î»
N )

	Ï€(mâ€²)

âˆ’1 + Î³
Î»
1 âˆ’Î³
Î»
log(Ïµ)

+ K(Î½, Î¼) âˆ’K(Î¼, Î¼).
Replacing in the right-hand side of this inequality the unobserved prior distribution
Î¼ with the worst possible posterior distribution, we obtain
Theorem 2.3.6. For any positive real constants Î±, Î², Î³ and Î», using the notation,
Ï€ = Ï€exp(âˆ’Î²R),
Î¼ = Î¼exp[âˆ’Î±Ï€(R)],
	Ï€ = Ï€exp(âˆ’Î³r),

104
Chapter 2.
Comparing posterior distributions to Gibbs priors
	Î¼ = Î¼exp[âˆ’Î± Î»
N tanh( Î»
N )âˆ’1	Ï€(r)],
with P probability at least 1 âˆ’Ïµ, for any posterior distribution Î½ : Î© â†’M1
+(M),

1 âˆ’
Î±
N tanh( Î³
N ) + Î² âˆ’
Î±
N tanh( Î»
N )

1 âˆ’Î³
Î»


K(Î½, Î¼) â‰¤K(Î½, 	Î¼)
+
Î±
N tanh( Î³
N ) + Î²

Î½

log

	Ï€

exp

N log

cosh( Î³
N )

	Ï€(mâ€²)

+
Î±
N tanh( Î»
N )(1 âˆ’Î³
Î»)

Î½

log

	Ï€

exp

N Î³
Î» log

cosh( Î»
N )

	Ï€(mâ€²)

+ log

	Î¼
&
	Ï€

exp

N log

cosh( Î³
N )

	Ï€(mâ€²)

Î±
N tanh( Î³
N )âˆ’Î²
Ã—

	Ï€

exp

N Î³
Î» log

cosh( Î»
N )

	Ï€(mâ€²)

Î±
N tanh( Î»
N )(1âˆ’Î³
Î» )
Ã— exp
Î± log[cosh( Î»
N )]
tanh( Î»
N )
(Î½	Ï€) âŠ—	Ï€(mâ€²)
'
+

1
N tanh( Î³
N ) + Î² +
1
N tanh( Î³
N ) âˆ’Î² +
1 + Î³
Î»
N tanh( Î»
N )

1 âˆ’Î³
Î»


log

 3
Ïµ

.
This result is satisfactory, but in the same time hints at some possible improve-
ment in the choice of the localized prior Î¼, which is here somewhat lacking a variance
term. We will consider in the remainder of this section the use of
(2.38)
Î¼ = Î¼exp[âˆ’Î±Ï€(R)âˆ’Î¾Ï€âŠ—Ï€(M â€²),
where Î¾ is some positive real constant and Ï€ = Ï€exp(âˆ’
Î²R) is some appropriate
conditional prior distribution with positive real parameter Î². With this new choice
K(Î½, Î¼) = Î±(Î½ âˆ’Î¼)Ï€(R) + Î¾(Î½ âˆ’Î¼)(Ï€ âŠ—Ï€)(M â€²) + K(Î½, Î¼) âˆ’K(Î¼, Î¼).
We already know how to deal with the ï¬rst factor Î±(Î½ âˆ’Î¼)Ï€(R), since the com-
putations we made to give it an empirical upper bound were valid for any choice
of the localized prior distribution Î¼. Let us now deal with Î¾(Î½ âˆ’Î¼)(Ï€ âŠ—Ï€)(M â€²).
Since mâ€²(Î¸, Î¸â€²) is a sum of independent Bernoulli random variables, we can easily
generalize the result of Theorem 1.1.4 (page 4) to prove that with P probability at
least 1 âˆ’Ïµ
N

1 âˆ’exp(âˆ’Î¶
N )

Î½(Ï€ âŠ—Ï€)(M â€²)
â‰¤Î¶Î¦ Î¶
N

Î½(Ï€ âŠ—Ï€)(M â€²)

â‰¤Î¶Î½(Ï€ âŠ—Ï€)(mâ€²) + K(Î½, Î¼) âˆ’log(Ïµ).
In the same way, with P probability at least 1 âˆ’Ïµ,
âˆ’N

exp( Î¶
N ) âˆ’1

Î¼(Ï€ âŠ—Ï€)(M â€²)
â‰¤âˆ’Î¶Î¦âˆ’Î¶
N

Î¼(Ï€ âŠ—Ï€)(M â€²)

â‰¤âˆ’Î¶Î¼(Ï€ âŠ—Ï€)(mâ€²) âˆ’log(Ïµ).
We would like now to replace (Ï€âŠ—Ï€)(mâ€²) with an empirical quantity. In order to do
this, we will use an entropy bound. Indeed for any conditional posterior distribution
Ï : Î© Ã— M â†’M1
+(Î˜),

2.3.
Two step localization
105
Î½

K(Ï, Ï€)

= Î²Î½(Ï âˆ’Ï€)(R) + Î½

K(Ï, Ï€) âˆ’K(Ï€, Ï€)

â‰¤
Î²
N tanh( Î³
N )

Î³Î½(Ï âˆ’Ï€)(r) + N log

cosh( Î³
N )

Î½(Ï âŠ—Ï€)(mâ€²)
+ K(Î½, Î¼) + Î½

K(Ï, Ï€)

âˆ’log(Ïµ)

+ Î½

K(Ï, Ï€) âˆ’K(Ï€, Ï€)

.
Thus choosing Î² = N tanh( Î³
N ),
Î³Î½(Ï€ âˆ’Ï)(r) + Î½

K(Ï€, Ï€) âˆ’K(Ï, Ï€)

â‰¤N log

cosh( Î³
N )

Î½(Ï âŠ—Ï€)(mâ€²) + K(Î½, Î¼) âˆ’log(Ïµ).
Choosing Ï = 	Ï€, we get
Î½

K(Ï€, 	Ï€)

â‰¤N log

cosh( Î³
N )

Î½(	Ï€ âŠ—Ï€)(mâ€²) + K(Î½, Î¼) âˆ’log(Ïµ).
This implies that
Î¾Î½(	Ï€ âŠ—Ï€)(mâ€²) = Î½

Ï€

Î¾	Ï€(mâ€²)

âˆ’K(Ï€, 	Ï€)

+ Î½

K(Ï€, 	Ï€)

â‰¤Î½

log

	Ï€

exp

Î¾	Ï€(mâ€²)

+ N log

cosh( Î³
N )

Î½(	Ï€ âŠ—Ï€)(mâ€²) + K(Î½, Î¼) âˆ’log(Ïµ).
Thus

Î¾ âˆ’N log

cosh( Î³
N )

Î½(	Ï€ âŠ—Ï€)(mâ€²)
â‰¤Î½

log

	Ï€

exp

Î¾	Ï€(mâ€²)

+ K(Î½, Î¼) âˆ’log(Ïµ)
and
Î½

K(Ï€, 	Ï€)

â‰¤
 
Î¾
N log[cosh( Î³
N )] âˆ’1
!âˆ’1
Î½

log

	Ï€

exp

Î¾	Ï€(mâ€²)

+ K(Î½, Î¼) âˆ’log(Ïµ)

+ K(Î½, Î¼) âˆ’log(Ïµ).
Taking for simplicity Î¾ = 2N log

cosh( Î³
N )

and noticing that
2N log

cosh( Î³
N )

= âˆ’N log

1 âˆ’
Î²2
N 2

,
we get
Theorem 2.3.7. Let us put Ï€ = Ï€exp(âˆ’
Î²R) and 	Ï€ = Ï€exp(âˆ’Î³r), where Î³ is some
arbitrary positive real constant and Î² = N tanh( Î³
N ), so that Î³ =
N
2 log

1+ 
Î²
N
1âˆ’
Î²
N

.
With P probability at least 1 âˆ’Ïµ,
Î½

K(Ï€, 	Ï€)

â‰¤Î½

log

	Ï€

exp

2N log

cosh( Î³
N )

	Ï€(mâ€²)

+ 2

K(Î½, Î¼) âˆ’log(Ïµ)

.

106
Chapter 2.
Comparing posterior distributions to Gibbs priors
As a consequence
Î¶Î½(Ï€ âŠ—Ï€)(mâ€²) = Î¶Î½(Ï€ âŠ—Ï€)(mâ€²) âˆ’Î½

K(Ï€ âŠ—Ï€, 	Ï€ âŠ—	Ï€)

+ 2Î½

K(Ï€, 	Ï€)

â‰¤Î½

log

	Ï€ âŠ—	Ï€

exp(Î¶mâ€²)

+ 2Î½

log

	Ï€

exp

2N log

cosh( Î³
N )

	Ï€(mâ€²)

+ 4

K(Î½, Î¼) âˆ’log(Ïµ)

.
Let us take for the sake of simplicity Î¶ = 2N log

cosh( Î³
N )

, to get
Î¶Î½(Ï€ âŠ—Ï€)(mâ€²) â‰¤3Î½

log

	Ï€ âŠ—	Ï€

exp(Î¶mâ€²)

+ 4

K(Î½, Î¼) âˆ’log(Ïµ)

.
This proves
Proposition 2.3.8. Let us consider some arbitrary prior distribution Î¼ âˆˆM1
+(M)
and some arbitrary conditional prior distribution Ï€ : M â†’M1
+(Î˜). Let Î² < N be
some positive real constant. Let us put Ï€ = Ï€exp(âˆ’
Î²R) and 	Ï€ = Ï€exp(âˆ’Î³r), with
Î² = N tanh( Î³
N ). Moreover let us put Î¶ = 2N log

cosh( Î³
N )

. With P probability at
least 1 âˆ’2Ïµ, for any posterior distribution Î½ âˆˆM1
+(M),
Î½(Ï€ âŠ—Ï€)(M â€²) â‰¤
3Î½

log

	Ï€ âŠ—	Ï€

exp(Î¶mâ€²)

+ 5

K(Î½, Î¼) âˆ’log(Ïµ)

N

1 âˆ’exp(âˆ’Î¶
N )

=
1
N tanh( Î³
N )2

3Î½

log

	Ï€ âŠ—	Ï€

exp

2N log

cosh( Î³
N )

mâ€²
+ 5

K(Î½, Î¼) âˆ’log(Ïµ)

.
In the same way,
âˆ’Î¶Î¼(Ï€ âŠ—Ï€)(mâ€²) â‰¤Î¼

log

	Ï€ âŠ—	Ï€

exp(âˆ’Î¶mâ€²)

+ 2Î¼

log

	Ï€

exp

2N log

cosh( Î³
N )

	Ï€(mâ€²)

âˆ’4 log(Ïµ)
and thus
âˆ’Î¼(Ï€ âŠ—Ï€)(M â€²) â‰¤
1
N

exp( Î¶
N ) âˆ’1


Î¼

log

	Ï€ âŠ—	Ï€

exp(âˆ’Î¶mâ€²)

+ 2Î¼

log

	Ï€

exp

2N log

cosh( Î³
N )

	Ï€(mâ€²)

âˆ’5 log(Ïµ)

.
Here we have purposely kept Î¶ as an arbitrary positive real constant, to be tuned
later (in order to be able to strengthen more or less the compensation of variance
terms).
We are now properly equipped to estimate the divergence with respect to Î¼, the
choice of prior distribution made in equation (2.38, page 104). Indeed we can now
write

1 âˆ’
Î±
N tanh( Î³
N ) + Î² âˆ’
Î±
N tanh( Î»
N )

1 âˆ’Î³
Î»
 âˆ’
5Î¾
N tanh( Î³
N )2

K(Î½, Î¼)

2.3.
Two step localization
107
â‰¤
Î±
N tanh( Î³
N ) + Î²

Î½

log

	Ï€

exp

N log

cosh( Î³
N )

	Ï€(mâ€²)

âˆ’log(Ïµ)

+
Î±
N tanh( Î³
N ) âˆ’Î²

Î¼

log

	Ï€

exp

N log

cosh( Î³
N )

	Ï€(mâ€²)

âˆ’log(Ïµ)

+
Î±
N tanh( Î»
N )

Î»(Î½ âˆ’Î¼)	Ï€(r) + N log

cosh( Î»
N )

(Î½	Ï€) âŠ—(Î¼	Ï€)(mâ€²)
+

1 âˆ’Î³
Î»
âˆ’1
(Î½ + Î¼)

log

	Ï€

exp

N Î³
Î» log

cosh( Î»
N )

	Ï€(mâ€²)

âˆ’1 + Î³
N
1 âˆ’Î³
N
log(Ïµ)

+
Î¾
N tanh( Î³
N )2

3Î½

log

	Ï€ âŠ—	Ï€

exp

2N log

cosh( Î³
N )

mâ€²
âˆ’5 log(Ïµ)

+
Î¾
N

exp( Î¶
N ) âˆ’1


Î¼

log

	Ï€ âŠ—	Ï€

exp(âˆ’Î¶mâ€²)

+ 2Î¼

log

	Ï€

exp

2N log

cosh( Î³
N )

	Ï€(mâ€²)

âˆ’5 log(Ïµ)

.
+ K(Î½, Î¼) âˆ’K(Î¼, Î¼).
It remains now only to replace in the right-hand side of this inequality Î¼ with
the worst possible posterior distribution to obtain
Theorem 2.3.9.
Let Î» > Î³ > Î², Î¶, Î± and Î¾ be arbitrary positive real constants.
Let us use the notation Ï€ = Ï€exp(âˆ’Î²R), Ï€ = Ï€exp(âˆ’N tanh( Î³
N )R), 	Ï€ = Ï€exp(âˆ’Î³r), Î¼ =
Î¼exp[âˆ’Î±Ï€(R)âˆ’Î¾Ï€âŠ—Ï€(M â€²)] and let us deï¬ne the posterior distribution 	Î¼ : Î© â†’M1
+(M)
by
d	Î¼
dÎ¼ âˆ¼exp

âˆ’
Î±Î»
N tanh( Î»
N )	Ï€(r)
+
Î¾
N

exp( Î¶
N ) âˆ’1
 log

	Ï€ âŠ—	Ï€

exp(âˆ’Î¶mâ€²)

.
Let us assume moreover that
Î±
N tanh( Î³
N ) + Î² +
Î±
N tanh( Î»
N )(1 âˆ’Î³
Î») +
5Î¾
N tanh( Î³
N )2 < 1.
With P probability at least 1 âˆ’Ïµ, for any posterior distribution Î½ : Î© â†’M1
+(M),
K(Î½, Î¼) â‰¤

1 âˆ’
Î±
N tanh( Î³
N ) + Î²
âˆ’
Î±
N tanh( Î»
N )

1 âˆ’Î³
Î»
 âˆ’
5Î¾
N tanh( Î³
N )2
âˆ’1
K(Î½, 	Î¼)
+
Î±
N tanh( Î³
N ) + Î²

Î½

log

	Ï€

exp

N log

cosh( Î³
N )

	Ï€(mâ€²)


108
Chapter 2.
Comparing posterior distributions to Gibbs priors
+
Î±
N tanh( Î»
N )

1 âˆ’Î³
Î»


Î½

log

	Ï€

exp

N Î³
Î» log

cosh( Î»
N )

	Ï€(mâ€²)

+
Î¾
N tanh( Î³
N )2

3Î½

log

	Ï€ âŠ—	Ï€

exp

2N log

cosh( Î³
N )

mâ€²
+
Î¾
N

exp( Î¶
N ) âˆ’1


Î½

log

	Ï€ âŠ—	Ï€

exp(âˆ’Î¶mâ€²)

+ log

	Î¼

	Ï€

exp

N log

cosh( Î³
N )

	Ï€(mâ€²)

Î±
N tanh( Î³
N )âˆ’Î²
Ã—

	Ï€

exp

N Î³
Î» log

cosh( Î»
N )

	Ï€(mâ€²)

Î±
N tanh( Î»
N )

1âˆ’Î³
Î»

Ã—

	Ï€

exp

2N log

cosh( Î³
N )

	Ï€(mâ€²)

2Î¾
N

exp( Î¶
N )âˆ’1

Ã— exp

N log

cosh( Î»
N )

(Î½	Ï€) âŠ—	Ï€

(mâ€²)

+
&
Î±
N tanh( Î³
N ) + Î² +
Î±
N tanh( Î³
N ) âˆ’Î² +
2Î±

1 + Î³
N

N tanh( Î»
N )

1 âˆ’Î³
Î»

+
5Î¾
N tanh( Î³
N )2 +
5Î¾
N

exp( Î¶
N ) âˆ’1

'
log

 5
Ïµ


.
The interest of this theorem lies in the presence of a variance term in the localized
posterior distribution 	Î¼, which with a suitable choice of parameters seems to be an
interesting option in the case when there are nested models: in this situation there
may be a need to prevent integration with respect to 	Î¼ in the right-hand side to
put weight on wild oversized models with large variance terms. Moreover, the right-
hand side being empirical, parameters can be, as usual, optimized from data using
a union bound on a grid of candidate values.
If one is only interested in the general shape of the result, a simpliï¬ed inequality
as the one below may suï¬ƒce:
Corollary 2.3.10.
For any positive real constants Î» > Î³ > Î², Î¶, Î± and Î¾, let us
use the same notation as in Theorem 2.3.9 (page 107). Let us put moreover
A1 =
Î±
N tanh( Î³
N ) + Î² +
Î±
N tanh( Î»
N )

1 âˆ’Î³
Î»
 +
5Î¾
N tanh( Î³
N )2 ,
A2 =
Î±
N tanh( Î³
N ) + Î² +
Î±
N tanh( Î»
N )

1 âˆ’Î³
Î»
 +
3Î¾
N tanh( Î³
N )2
A3 =
Î¾
N

exp

 Î¶
N

âˆ’1

A4 =
Î±
N tanh( Î³
N ) âˆ’Î² +
Î±
N tanh( Î»
N )(1 âˆ’Î³
Î») +
2Î¾
N[exp( Î¶
N ) âˆ’1]
,
A5 =
Î±
N tanh( Î³
N ) + Î² +
Î±
N tanh( Î³
N ) âˆ’Î² +
2Î±

1 + Î³
N

N tanh( Î»
N )

1 âˆ’Î³
Î»

+
5Î¾
N tanh( Î³
N )2 +
5Î¾
N

exp( Î¶
N ) âˆ’1
,
C1 = 2N log

cosh

 Î»
N

,

2.3.
Two step localization
109
C2 = N log

cosh

 Î»
N

.
Let us assume that A1 < 1. With P probability at least 1 âˆ’Ïµ, for any posterior
distribution Î½ : Î© â†’M1
+(M),
K(Î½, Î¼) â‰¤K(Î½, Î±, Î², Î³, Î», Î¾, Î¶, Ïµ)
def
=

1 âˆ’A1
âˆ’1

K(Î½, 	Î¼)
+ A2Î½

log

	Ï€ âŠ—	Ï€

exp

C1mâ€²
+ A3Î½

log

	Ï€ âŠ—	Ï€

exp

âˆ’Î¶mâ€² 
+ log

	Î¼

	Ï€

exp

C1	Ï€(mâ€²)
A4
exp

C2

(Î½	Ï€) âŠ—	Ï€

(mâ€²)

+ A5 log

 5
Ïµ


.
Putting this corollary together with Corollary 2.3.5 (page 102), we obtain
Theorem 2.3.11. Let us consider the notation introduced in Corollary 2.3.5 (page
102) and in Theorem 2.3.9 (page 107) and its Corollary 2.3.10 (page 108). Let us
consider real positive parameters Î», Î³â€²
1 < Î»â€²
1 and Î³â€²
2 < Î»â€²
2. Let us consider also two
sets of parameters Î±i, Î²i, Î³i, Î»i, Î¾i, Î¶i,where i = 1, 2, both satisfying the conditions
stated in Corollary 2.3.10 (page 108). With P probability at least 1 âˆ’Ïµ, for any
posterior distributions Î½1, Î½2 : Î© â†’M1
+(M), any conditional posterior distributions
Ï1, Ï2 : Î© Ã— M â†’M1
+

Î˜

,
âˆ’N log

1 âˆ’tanh

 Î»
N

Î½1Ï1 âˆ’Î½2Ï2

(R)

â‰¤Î»

Î½1Ï1 âˆ’Î½2Ï2

(r)
+ N log

cosh

 Î»
N

Î½1Ï1

âŠ—

Î½2Ï2

mâ€² 
+ Kâ€²
Î½1, Ï1, Î³â€²
1, Î»â€²
1, Ïµ
5

+ Kâ€²
Î½2, Ï2, Î³â€²
2, Î»â€²
2, Ïµ
5

+
1
1 âˆ’Î³â€²
1
Î»â€²
1
K

Î½1, Î±1, Î²1, Î³1, Î»1, Î¾1, Î¶1, Ïµ
5

+
1
1 âˆ’Î³â€²
2
Î»â€²
2
K

Î½2, Î±2, Î²2, Î³2, Î»2, Î¾2, Î¶2, Ïµ
5

âˆ’log

 Ïµ
5

.
This theorem provides, using a union bound argument to further optimize the
parameters, an empirical bound for Î½1Ï1(R) âˆ’Î½2Ï2(R), which can serve to build
a selection algorithm exactly in the same way as what was done in Theorem 2.2.4
(page 72). This represents the highest degree of sophistication that we will achieve
in this monograph, as far as model selection is concerned: this theorem shows that
it is indeed possible to derive a selection scheme in which localization is performed
in two steps and in which the localization of the model selection itself, as opposed
to the localization of the estimation in each model, includes a variance term as well
as a bias term, so that it should be possible to localize the choice of nested mod-
els, something that would not have been feasible with the localization techniques
exposed in the previous sections of this study. We should point out however that
more sophisticated does not necessarily mean more eï¬ƒcient: as the reader may
have noticed, sophistication comes at a price, in terms of the complexity of the
estimation schemes, with some possible loss of accuracy in the constants that can
mar the beneï¬ts of using an asymptotically more eï¬ƒcient method for small sample
sizes.

110
Chapter 2.
Comparing posterior distributions to Gibbs priors
We will do the hurried reader a favour: we will not launch into a study of the
theoretical properties of this selection algorithm, although it is clear that all the
tools needed are at hand!
We would like as a conclusion to this chapter, to put forward a simple idea:
this approach of model selection revolves around entropy estimates concerned with
the divergence of posterior distributions with respect to localized prior distribu-
tions. Moreover, this localization of the prior distribution is more eï¬€ectively done
in several steps in some situations, and it is worth mentioning that these situations
include the typical case of selection from a family of parametric models. Finally,
the whole story relies upon estimating the relative generalization error rate of one
posterior distribution with respect to some local prior distribution as well as with
respect to another posterior distribution, because these relative rates can be esti-
mated more accurately than absolute generalization error rates, at least as soon
as no classiï¬cation model of reasonable size provides a good match to the training
sample, meaning that the classiï¬cation problem is either diï¬ƒcult or noisy.

Chapter 3
Transductive PAC-Bayesian
learning
3.1. Basic inequalities
3.1.1. The transductive setting.
In this chapter the observed sample (Xi,
Yi)N
i=1 will be supplemented with a test or shadow sample (Xi, Yi)(k+1)N
i=N+1 . This point
of view, called transductive classiï¬cation, has been introduced by V. Vapnik. It may
be justiï¬ed in diï¬€erent ways.
On the practical side, one interest of the transductive setting is that it is often a
lot easier to collect examples than it is to label them, so that it is not unrealistic to
assume that we indeed have two training samples, one labelled and one unlabelled.
It also covers the case when a batch of patterns is to be classiï¬ed and we are allowed
to observe the whole batch before issuing the classiï¬cation.
On the mathematical side, considering a shadow sample proves technically fruit-
ful. Indeed, when introducing the Vapnikâ€“Cervonenkis entropy and Vapnikâ€“Cervo-
nenkis dimension concepts, as well as when dealing with compression schemes, albeit
the inductive setting is our ï¬nal concern, the transductive setting is a useful detour.
In this second scenario, intermediate technical results involving the shadow sample
are integrated with respect to unobserved random variables in a second stage of the
proofs.
Let us describe now the changes to be made to previous notation to adapt them
to the transductive setting. The distribution P will be a probability measure on the
canonical space Î© = (XÃ—Y)(k+1)N, and (Xi, Yi)(k+1)N
i=1
will be the canonical process
on this space (that is the coordinate process). Unless explicitly mentioned, the
parameter k indicating the size of the shadow sample will remain ï¬xed. Assuming
the shadow sample size is a multiple of the training sample size is convenient without
signiï¬cantly restricting generality. For a while, we will use a weaker assumption than
independence, assuming that P is partially exchangeable, since this is all we need in
the proofs.
Definition 3.1.1. For i = 1, . . . , N, let Ï„i
: Î© â†’Î© be deï¬ned for any
111

112
Chapter 3.
Transductive PAC-Bayesian learning
Ï‰ = (Ï‰j)(k+1)N
j=1
âˆˆÎ© by
âŽ§
âŽª
âŽ¨
âŽª
âŽ©
Ï„i(Ï‰)i+jN = Ï‰i+(jâˆ’1)N,
j = 1, . . . , k,
Ï„i(Ï‰)i = Ï‰i+kN,
and Ï„i(Ï‰)m+jN = Ï‰m+jN,
m Ì¸= i, m = 1, . . . , N, j = 0, . . . k.
Clearly, if we arrange the (k + 1)N samples in a N Ã— (k + 1) array, Ï„i performs
a circular permutation of k + 1 entries on the ith row, leaving the other rows
unchanged. Moreover, all the circular permutations of the ith row have the form
Ï„ j
i , j ranging from 0 to k.
The probability distribution P is said to be partially exchangeable if for any
i = 1, . . . , N, P â—¦Ï„ âˆ’1
i
= P.
This means equivalently that for any bounded measurable function h : Î© â†’R,
P(h â—¦Ï„i) = P(h).
In the same way a function h deï¬ned on Î© will be said to be partially exchange-
able if h â—¦Ï„i = h for any i = 1, . . . , N. Accordingly a posterior distribution Ï : Î© â†’
M1
+(Î˜, T) will be said to be partially exchangeable when Ï(Ï‰, A) = Ï

Ï„i(Ï‰), A

, for
any Ï‰ âˆˆÎ©, any i = 1, . . . , N and any A âˆˆT.
For any bounded measurable function h, let us deï¬ne Ti(h) =
1
k+1
k
j=0 h â—¦Ï„ j
i .
Let T(h) = TN â—¦Â· Â· Â·â—¦T1(h). For any partially exchangeable probability distribution
P, and for any bounded measurable function h, P

T(h)

= P(h). Let us put
Ïƒi(Î¸) = 1

fÎ¸(Xi) Ì¸= Yi

,
indicating the success or failure of fÎ¸
to predict Yi from Xi,
r1(Î¸) = 1
N
N

i=1
Ïƒi(Î¸),
the empirical error rate of fÎ¸
on the observed sample,
r2(Î¸) =
1
kN
(k+1)N

i=N+1
Ïƒi(Î¸),
the error rate of fÎ¸ on the shadow sample,
r(Î¸) = r1(Î¸) + kr2(Î¸)
k + 1
=
1
(k + 1)N
(k+1)N

i=1
Ïƒi(Î¸),
the global error
rate of fÎ¸,
Ri(Î¸) = P

fÎ¸(Xi) Ì¸= Yi

,
the expected error
rate of fÎ¸ on the ith input,
R(Î¸) = 1
N
N

i=1
Ri(Î¸) = P

r1(Î¸)

= P

r2(Î¸)

,
the average expected
error rate of fÎ¸ on all inputs.
We will allow for posterior distributions Ï : Î© â†’M1
+(Î˜) depending on the shadow
sample. The most interesting ones will anyhow be independent of the shadow labels
YN+1, . . . , Y(k+1)N. We will be interested in the conditional expected error rate of
the randomized classiï¬cation rule described by Ï on the shadow sample, given the
observed sample, that is, P

Ï(r2)|(Xi, Yi)N
i=1

. This is a natural extension of the
notion of generalization error rate: this is indeed the error rate to be expected
when the randomized classiï¬cation rule described by the posterior distribution Ï
is applied to the shadow sample (which should in this case more purposefully be
called the test sample).

3.1.
Basic inequalities
113
To see the connection with the previously deï¬ned generalization error rate, let us
comment on the case when P is invariant by any permutation of any row, meaning
that
P

h(Ï‰ â—¦s)

= P

h(Ï‰)

for all s âˆˆS({i + jN; j = 0, . . . , k})
and all i = 1, . . . , N, where S(A) is the set of permutations of A, extended to
{1, . . . , (k + 1)N} so as to be the identity outside of A. In other words, P is as-
sumed to be invariant under any permutation which keeps the rows unchanged.
In this case, if Ï is invariant by any permutation of any row of the shadow sam-
ple, meaning that Ï(Ï‰ â—¦s) = Ï(Ï‰) âˆˆM1
+(Î˜), s âˆˆS({i + jN; j = 1, . . . , k}),
i = 1, . . . , N, then P

Ï(r2)|(Xi, Yi)N
i=1

=
1
N
N
i=1 P

Ï(Ïƒi+N)|(Xi, Yi)N
i=1

, meaning
that the expectation can be taken on a restricted shadow sample of the same size as
the observed sample. If moreover the rows are equidistributed, meaning that their
marginal distributions are equal, then
P

Ï(r2)|(Xi, Yi)N
i=1

= P

Ï(ÏƒN+1)|(Xi, Yi)N
i=1

.
This means that under these quite commonly fulï¬lled assumptions, the expectation
can be taken on a single new object to be classiï¬ed, our study thus covers the case
when only one of the patterns from the shadow sample is to be labelled and one is
interested in the expected error rate of this single labelling. Of course, in the case
when P is i.i.d. and Ï depends only on the training sample (Xi, Yi)N
i=1, we fall back
on the usual criterion of performance P

Ï(r2)|(Zi)N
i=1

= Ï(R) = Ï(R1).
3.1.2. Absolute bound.
Using an obvious factorization, and considering for the
moment a ï¬xed value of Î¸ and any partially exchangeable positive real measurable
function Î» : Î© â†’R+, we can compute the log-Laplace transform of r1 under T,
which acts like a conditional probability distribution:
log

T

exp(âˆ’Î»r1)

=
N

i=1
log

Ti

exp(âˆ’Î»
N Ïƒi)

â‰¤N log
 1
N
N

i=1
Ti

exp

âˆ’Î»
N Ïƒi

= âˆ’Î»Î¦ Î»
N (r),
where the function Î¦ Î»
N was deï¬ned by equation (1.1, page 2). Remarking that
T

exp

Î»

Î¦ Î»
N (r) âˆ’r1

= exp

Î»Î¦ Î»
N (r)

T

exp(âˆ’Î»r1)

we obtain
Lemma 3.1.1. For any Î¸ âˆˆÎ˜ and any partially exchangeable positive real measur-
able function Î» : Î© â†’R+,
T

exp

Î»

Î¦ Î»
N

r(Î¸)

âˆ’r1(Î¸)

â‰¤1.
We deduce from this lemma a result analogous to the inductive case:
Theorem 3.1.2. For any partially exchangeable positive real measurable function
Î» : Î© Ã— Î˜ â†’R+, for any partially exchangeable posterior distribution Ï€ : Î© â†’
M1
+(Î˜),
P

exp

sup
ÏâˆˆM1
+(Î˜)
Ï

Î»

Î¦ Î»
N (r) âˆ’r1

âˆ’K(Ï, Ï€)

â‰¤1.

114
Chapter 3.
Transductive PAC-Bayesian learning
The proof is deduced from the previous lemma, using the fact that Ï€ is partially
exchangeable:
P

exp

sup
ÏâˆˆM1
+(Î˜)
Ï

Î»

Î¦ Î»
N (r) âˆ’r1

âˆ’K(Ï, Ï€)

= P

Ï€

exp

Î»

Î¦ Î»
N (r) âˆ’r1

= P

TÏ€

exp

Î»

Î¦ Î»
N (r) âˆ’r1

= P

Ï€

T exp

Î»

Î¦ Î»
N (r) âˆ’r1

â‰¤1.
3.1.3. Relative bounds.
Introducing in the same way
mâ€²(Î¸, Î¸â€²) = 1
N
N

i=1
2221

fÎ¸(Xi) Ì¸= Yi

âˆ’1

fÎ¸â€²(Xi) Ì¸= Yi
222
and
m(Î¸, Î¸â€²) =
1
(k + 1)N
(k+1)N

i=1
2221

fÎ¸(Xi) Ì¸= Yi

âˆ’1

fÎ¸â€²(Xi) Ì¸= Yi
222,
we could prove along the same line of reasoning
Theorem 3.1.3. For any real parameter Î», any Î¸ âˆˆÎ˜, any partially exchangeable
posterior distribution Ï€ : Î© â†’M1
+(Î˜),
P

exp

sup
ÏâˆˆM1
+(Î˜)
Î»

Ï

Î¨ Î»
N

r(Â·) âˆ’r(Î¸), m(Â·, Î¸)

âˆ’

Ï(r1) âˆ’r1(Î¸)

âˆ’K(Ï, Ï€)

â‰¤1,
where the function Î¨ Î»
N was deï¬ned by equation (1.21, page 35).
Theorem 3.1.4. For any real constant Î³, for any Î¸ âˆˆÎ˜, for any partially ex-
changeable posterior distribution Ï€ : Î© â†’M1
+(Î˜),
P

exp
&
sup
ÏâˆˆM1
+(Î˜)

âˆ’NÏ

log

1 âˆ’tanh

 Î³
N

r(Â·) âˆ’r(Î¸)

âˆ’Î³

Ï(r1) âˆ’r1(Î¸)

âˆ’N log

cosh

 Î³
N

Ï

mâ€²(Â·, Î¸)

âˆ’K(Ï, Ï€)
'
â‰¤1.
This last theorem can be generalized to give
Theorem 3.1.5. For any real constant Î³, for any partially exchangeable posterior
distributions Ï€1, Ï€2 : Î© â†’M1
+(Î˜),
P

exp
&
sup
Ï1,Ï2âˆˆM1
+(Î˜)

âˆ’N log

1 âˆ’tanh

 Î³
N

Ï1(r) âˆ’Ï2(r)

âˆ’Î³

Ï1(r1) âˆ’Ï2(r1)

âˆ’N log

cosh

 Î³
N

Ï1 âŠ—Ï2(mâ€²)
âˆ’K(Ï1, Ï€1) âˆ’K(Ï2, Ï€2)
'
â‰¤1.

3.2.
Vapnik bounds for transductive classiï¬cation
115
To conclude this section, we see that the basic theorems of transductive PAC-
Bayesian classiï¬cation have exactly the same form as the basic inequalities of in-
ductive classiï¬cation, Theorems 1.1.4 (page 4), 1.4.2 (page 35) and 1.4.3 (page 37)
with R(Î¸) replaced with r(Î¸), r(Î¸) replaced with r1(Î¸) and M â€²(Î¸, Î¸) replaced with
m(Î¸, Î¸).
Thus all the results of the ï¬rst two chapters remain true under the hypotheses
of transductive classiï¬cation, with R(Î¸) replaced with r(Î¸), r(Î¸) replaced with r1(Î¸)
and M â€²(Î¸, Î¸ ) replaced with m(Î¸, Î¸).
Consequently, in the case when the unlabelled shadow sample is observed, it is
possible to improve on the Vapnik bounds to be discussed hereafter by using an ex-
plicit partially exchangeable posterior distribution Ï€ and resorting to localized or
to relative bounds (in the case at least of unlimited computing resources, which of
course may still be unrealistic in many real world situations, and with the caveat,
to be recalled in the conclusion of this study, that for small sample sizes and com-
paratively complex classiï¬cation models, the improvement may not be so decisive).
Let us notice also that the transductive setting when experimentally available,
has the advantage that
d(Î¸, Î¸â€²) =
1
(k + 1)N
(k+1)N

i=1
1

fÎ¸â€²(Xi) Ì¸= fÎ¸(Xi)

â‰¥m(Î¸, Î¸â€²) â‰¥r(Î¸) âˆ’r(Î¸â€²),
Î¸, Î¸â€² âˆˆÎ˜,
is observable in this context, providing an empirical upper bound for the diï¬€erence
r(	Î¸) âˆ’Ï(r) for any non-randomized estimator 	Î¸ and any posterior distribution Ï,
namely
r(	Î¸) â‰¤Ï(r) + Ï

d(Â·, 	Î¸)

.
Thus in the setting of transductive statistical experiments, the PAC-Bayesian frame-
work provides fully empirical bounds for the error rate of non-randomized estima-
tors 	Î¸ : Î© â†’Î˜, even when using a non-atomic prior Ï€ (or more generally a non-
atomic partially exchangeable posterior distribution Ï€), even when Î˜ is not a vector
space and even when Î¸ â†’R(Î¸) cannot be proved to be convex on the support of
some useful posterior distribution Ï.
3.2. Vapnik bounds for transductive classification
In this section, we will stick to plain unlocalized non-relative bounds. As we have
already mentioned, (and as it was put forward by Vapnik himself in his seminal
works), these bounds are not always superseded by the asymptotically better ones
when the sample is of small size: they deserve all our attention for this reason. We
will start with the general case of a shadow sample of arbitrary size. We will then
discuss the case of a shadow sample of equal size to the training set and the case of
a fully exchangeable sample distribution, showing how they can be taken advantage
of to sharpen inequalities.
3.2.1. With a shadow sample of arbitrary size.
The great thing with the
transductive setting is that we are manipulating only r1 and r which can take only
a ï¬nite number of values and therefore are piecewise constant on Î˜. This makes it
possible to derive inequalities that will hold uniformly for any value of the parameter

116
Chapter 3.
Transductive PAC-Bayesian learning
Î¸ âˆˆÎ˜. To this purpose, let us consider for any value Î¸ âˆˆÎ˜ of the parameter the
subset Î”(Î¸) âŠ‚Î˜ of parameters Î¸â€² such that the classiï¬cation rule fÎ¸â€² answers the
same on the extended sample (Xi)(k+1)N
i=1
as fÎ¸. Namely, let us put for any Î¸ âˆˆÎ˜
Î”(Î¸) =

Î¸â€² âˆˆÎ˜; fÎ¸â€²(Xi) = fÎ¸(Xi), i = 1, . . . , (k + 1)N

.
We see immediately that Î”(Î¸) is an exchangeable parameter subset on which r1 and
r2 and therefore also r take constant values. Thus for any Î¸ âˆˆÎ˜ we may consider
the posterior ÏÎ¸ deï¬ned by
dÏÎ¸
dÏ€ (Î¸â€²) = 1

Î¸â€² âˆˆÎ”(Î¸)

Ï€

Î”(Î¸)
âˆ’1,
and use the fact that ÏÎ¸(r1) = r1(Î¸) and ÏÎ¸(r) = r(Î¸), to prove that
Lemma 3.2.1. For any partially exchangeable positive real measurable function Î» :
Î© Ã— Î˜ â†’R such that
(3.1)
Î»(Ï‰, Î¸â€²) = Î»(Ï‰, Î¸),
Î¸ âˆˆÎ˜, Î¸â€² âˆˆÎ”(Î¸), Ï‰ âˆˆÎ©,
and any partially exchangeable posterior distribution Ï€ : Î© â†’M1
+(Î˜), with P prob-
ability at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜,
Î¦ Î»
N

r(Î¸)

+ log

ÏµÏ€

Î”(Î¸)

Î»(Î¸)
â‰¤r1(Î¸).
We can then remark that for any value of Î» independent of Ï‰, the left-hand side
of the previous inequality is a partially exchangeable function of Ï‰ âˆˆÎ©. Thus this
left-hand side is maximized by some partially exchangeable function Î», namely
arg max
Î»

Î¦ Î»
N

r(Î¸)

+ log

ÏµÏ€

Î”(Î¸)

Î»

is partially exchangeable as depending only on partially exchangeable quantities.
Moreover this choice of Î»(Ï‰, Î¸) satisï¬es also condition (3.1) stated in the previous
lemma of being constant on Î”(Î¸), proving
Lemma 3.2.2. For any partially exchangeable posterior distribution Ï€ : Î© â†’
M1
+(Î˜), with P probability at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜ and any Î» âˆˆR+,
Î¦ Î»
N

r(Î¸)

+ log

ÏµÏ€

Î”(Î¸)

Î»
â‰¤r1(Î¸).
Writing r = r1+kr2
k+1
and rearranging terms we obtain
Theorem 3.2.3. For any partially exchangeable posterior distribution Ï€ : Î© â†’
M1
+(Î˜), with P probability at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜,
r2(Î¸) â‰¤k + 1
k
inf
Î»âˆˆR+
1 âˆ’exp
"
âˆ’Î»
N r1(Î¸) + log

ÏµÏ€

Î”(Î¸)

N
#
1 âˆ’exp

âˆ’Î»
N

âˆ’r1(Î¸)
k
.

3.2.
Vapnik bounds for transductive classiï¬cation
117
If we have a set of binary classiï¬cation rules {fÎ¸; Î¸ âˆˆÎ˜} whose Vapnikâ€“Cervo-
nenkis dimension is not greater than h, we can choose Ï€ such that Ï€

Î”(Î¸)

is
independent of Î¸ and not less than
 
h
e(k + 1)N
!h
, as will be proved further on in
Theorem 4.2.2 (page 144).
Another important setting where the complexity term âˆ’log

Ï€

Î”(Î¸)

can eas-
ily be controlled is the case of compression schemes, introduced by Little et al.
(1986). It goes as follows: we are given for each labelled sub-sample (Xi, Yi)iâˆˆJ,
J âŠ‚{1, . . . , N}, an estimator of the parameter
	Î¸

(Xi, Yi)iâˆˆJ

= 	Î¸J,
J âŠ‚{1, . . . , N}, |J| â‰¤h,
where
	Î¸ :
N
@
k=1

X Ã— Y
k â†’Î˜
is an exchangeable function providing estimators for sub-samples of arbitrary size.
Let us assume that 	Î¸ is exchangeable, meaning that for any k = 1, . . . , N and any
permutation Ïƒ of {1, . . . , k}
	Î¸

(xi, yi)k
i=1

= 	Î¸

(xÏƒ(i), yÏƒ(i))k
i=1

,
(xi, yi)k
i=1 âˆˆ

X Ã— Y
k.
In this situation, we can introduce the exchangeable subset

	Î¸J; J âŠ‚{1, . . . , (k + 1)N}, |J| â‰¤h

âŠ‚Î˜,
which is seen to contain at most
h

j=0
 (k + 1)N
j
!
â‰¤
 e(k + 1)N
h
!h
classiï¬cation rules â€” as will be proved later on in Theorem 4.2.3 (page 144). Note
that we had to extend the range of J to all the subsets of the extended sample,
although we will use for estimation only those of the training sample, on which
the labels are observed. Thus in this case also we can ï¬nd a partially exchangeable
posterior distribution Ï€ such that
Ï€

Î”(	Î¸J)

â‰¥
 
h
e(k + 1)N
!h
.
We see that the size of the compression scheme plays the same role in this complexity
bound as the Vapnikâ€“Cervonenkis dimension for Vapnikâ€“Cervonenkis classes.
In these two cases of binary classiï¬cation with Vapnikâ€“Cervonenkis dimension
not greater than h and compression schemes depending on a compression set with
at most h points, we get a bound of
r2(Î¸) â‰¤k + 1
k
inf
Î»âˆˆR+
1 âˆ’exp
âŽ›
âŽâˆ’Î»
N r1(Î¸) âˆ’
h log

e(k+1)N
h

âˆ’log(Ïµ)
N
âŽž
âŽ 
1 âˆ’exp

âˆ’Î»
N

âˆ’r1(Î¸)
k
.

118
Chapter 3.
Transductive PAC-Bayesian learning
Let us make some numerical application: when N = 1000, h = 10, Ïµ = 0.01, and
infÎ˜ r1 = r1(	Î¸) = 0.2, we ï¬nd that r2(	Î¸) â‰¤0.4093, for k between 15 and 17,
and values of Î» equal respectively to 965, 968 and 971. For k = 1, we ï¬nd only
r2(	Î¸) â‰¤0.539, showing the interest of allowing k to be larger than 1.
3.2.2. When the shadow sample has the same size as the training sam-
ple.
In the case when k = 1, we can improve Theorem 3.1.2 by taking advantage
of the fact that Ti(Ïƒi) can take only 3 values, namely 0, 0.5 and 1. We see thus that
Ti(Ïƒi)âˆ’Î¦ Î»
N

Ti(Ïƒi)

can take only two values, 0 and 1
2 âˆ’Î¦ Î»
N ( 1
2), because Î¦ Î»
N (0) = 0
and Î¦ Î»
N (1) = 1. Thus
Ti(Ïƒi) âˆ’Î¦ Î»
N

Ti(Ïƒi)

=

1 âˆ’|1 âˆ’2Ti(Ïƒi)|
 1
2 âˆ’Î¦ Î»
N ( 1
2)

.
This shows that in the case when k = 1,
log

T

exp(âˆ’Î»r1)

= âˆ’Î»r + Î»
N
N

i=1
Ti(Ïƒi) âˆ’Î¦ Î»
N

Ti(Ïƒi)

= âˆ’Î»r + Î»
N
N

i=1

1 âˆ’|1 âˆ’2Ti(Ïƒi)|
 1
2 âˆ’Î¦ Î»
N ( 1
2)

â‰¤âˆ’Î»r + Î»
 1
2 âˆ’Î¦ Î»
N ( 1
2)

1 âˆ’|1 âˆ’2r|

.
Noticing that 1
2 âˆ’Î¦ Î»
N ( 1
2) = N
Î» log

cosh( Î»
2N )

, we obtain
Theorem 3.2.4. For any partially exchangeable function Î» : Î©Ã—Î˜ â†’R+, for any
partially exchangeable posterior distribution Ï€ : Î© â†’M1
+(Î˜),
P

exp

sup
ÏâˆˆM1
+(Î˜)
Ï

Î»(r âˆ’r1)
âˆ’N log

cosh( Î»
2N )

1 âˆ’|1 âˆ’2r|

âˆ’K(Ï, Ï€)

â‰¤1.
As a consequence, reasoning as previously, we deduce
Theorem 3.2.5. In the case when k = 1, for any partially exchangeable posterior
distribution Ï€ : Î© â†’M1
+(Î˜), with P probability at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜ and
any Î» âˆˆR+,
r(Î¸) âˆ’N
Î» log

cosh( Î»
2N )

1 âˆ’|1 âˆ’2r(Î¸)|

+ log

ÏµÏ€

Î”(Î¸)

Î»
â‰¤r1(Î¸);
and consequently for any Î¸ âˆˆÎ˜,
r2(Î¸) â‰¤2 inf
Î»âˆˆR+
r1(Î¸) âˆ’log

ÏµÏ€

Î”(Î¸)

Î»
1 âˆ’2N
Î» log

cosh( Î»
2N )
 âˆ’r1(Î¸).
In the case of binary classiï¬cation using a Vapnikâ€“Cervonenkis class of
Vapnikâ€“Cervonenkis dimension not greater than h, we can choose Ï€ such that
âˆ’log

Ï€

Î”(Î¸)

â‰¤h log( 2eN
h ) and obtain the following numerical illustration of

3.2.
Vapnik bounds for transductive classiï¬cation
119
this theorem: for N = 1000, h = 10, Ïµ = 0.01 and infÎ˜ r1 = r1(	Î¸) = 0.2, we ï¬nd an
upper bound r2(	Î¸) â‰¤0.5033, which improves on Theorem 3.2.3 but still is not un-
der the signiï¬cance level 1
2 (achieved by blind random classiï¬cation). This indicates
that considering shadow samples of arbitrary sizes some noisy situations yields a
signiï¬cant improvement on bounds obtained with a shadow sample of the same size
as the training sample.
3.2.3. When moreover the distribution of the augmented sample is ex-
changeable.
When k = 1 and P is exchangeable meaning that for any bounded
measurable function h : Î© â†’R and any permutation s âˆˆS

{1, . . . , 2N}

P

h(Ï‰ â—¦
s)

= P

h(Ï‰)

, then we can still improve the bound as follows. Let
T â€²(h) = 1
N!

sâˆˆS

{N+1,...,2N}
 h(Ï‰ â—¦s).
Then we can write
1 âˆ’|1 âˆ’2Ti(Ïƒi)| = (Ïƒi âˆ’Ïƒi+N)2 = Ïƒi + Ïƒi+N âˆ’2ÏƒiÏƒi+N.
Using this identity, we get for any exchangeable function Î» : Î© Ã— Î˜ â†’R+,
T

exp

Î»(r âˆ’r1) âˆ’log

cosh( Î»
2N )
 N

i=1

Ïƒi + Ïƒi+N âˆ’2ÏƒiÏƒi+N

â‰¤1.
Let us put
A(Î») = 2N
Î» log

cosh( Î»
2N )

,
(3.2)
v(Î¸) =
1
2N
N

i=1
(Ïƒi + Ïƒi+N âˆ’2ÏƒiÏƒi+N).
(3.3)
With this notation
T

exp

Î»

r âˆ’r1 âˆ’A(Î»)v

â‰¤1.
Let us notice now that
T â€²
v(Î¸)

= r(Î¸) âˆ’r1(Î¸)r2(Î¸).
Let Ï€ : Î© â†’M1
+(Î˜) be any given exchangeable posterior distribution. Using the
exchangeability of P and Ï€ and the exchangeability of the exponential function, we
get
P

Ï€

exp

Î»

r âˆ’r1 âˆ’A(r âˆ’r1r2)

= P

Ï€

exp

Î»

r âˆ’r1 âˆ’AT â€²(v)

â‰¤P

Ï€

T â€² exp

Î»

r âˆ’r1 âˆ’Av

= P

T â€²Ï€

exp

Î»

r âˆ’r1 âˆ’Av

= P

Ï€

exp

Î»

r âˆ’r1 âˆ’Av

= P

TÏ€

exp

Î»

r âˆ’r1 âˆ’Av

= P

Ï€

T exp

Î»

r âˆ’r1 âˆ’Av

â‰¤1.
We are thus ready to state

120
Chapter 3.
Transductive PAC-Bayesian learning
Theorem 3.2.6. In the case when k = 1, for any exchangeable probability dis-
tribution P, for any exchangeable posterior distribution Ï€ : Î© â†’M1
+(Î˜), for any
exchangeable function Î» : Î© Ã— Î˜ â†’R+,
P

exp

sup
ÏâˆˆM1
+(Î˜)
Ï

Î»

r âˆ’r1 âˆ’A(Î»)(r âˆ’r1r2)

âˆ’K(Ï, Ï€)

â‰¤1,
where A(Î») is deï¬ned by equation (3.2, page 119).
We then deduce as previously
Corollary 3.2.7. For any exchangeable posterior distribution Ï€ : Î© â†’M1
+(Î˜),
for any exchangeable probability measure P âˆˆM1
+(Î©), for any measurable exchange-
able function Î» : Î© Ã— Î˜ â†’R+, with P probability at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜,
r(Î¸) â‰¤r1(Î¸) + A(Î»)

r(Î¸) âˆ’r1(Î¸)r2(Î¸)

âˆ’log

ÏµÏ€

Î”(Î¸)

Î»
,
where A(Î») is deï¬ned by equation (3.2, page 119).
In order to deduce an empirical bound from this theorem, we have to make
some choice for Î»(Ï‰, Î¸). Fortunately, it is easy to show that the bound holds uni-
formly in Î», because the inequality can be rewritten as a function of only one
non-exchangeable quantity, namely r1(Î¸). Indeed, since r2 = 2r âˆ’r1, we see that
the inequality can be written as
r(Î¸) â‰¤r1(Î¸) + A(Î»)

r(Î¸) âˆ’2r(Î¸)r1(Î¸) + r1(Î¸)2
âˆ’log

ÏµÏ€

Î”(Î¸)

Î»
.
It can be solved in r1(Î¸), to get
r1(Î¸) â‰¥f

Î», r(Î¸), âˆ’log

ÏµÏ€

Î”(Î¸)

,
where
f(Î», r, d) =

2A(Î»)
âˆ’1

2rA(Î») âˆ’1
+
A
1 âˆ’2rA(Î»)
2 + 4A(Î»)

r

1 âˆ’A(Î»)

âˆ’d
Î»

.
Thus we can ï¬nd some exchangeable function Î»(Ï‰, Î¸), such that
f

Î»(Ï‰, Î¸), r(Î¸), âˆ’log

ÏµÏ€

Î”(Î¸)

= sup
Î²âˆˆR+
f

Î², r(Î¸), âˆ’log

ÏµÏ€

Î”(Î¸)

.
Applying Corollary 3.2.7 (page 120) to that choice of Î», we see that
Theorem 3.2.8. For any exchangeable probability measure P âˆˆM1
+(Î©), for any
exchangeable posterior probability distribution Ï€ : Î© â†’M1
+(Î˜), with P probability
at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜, for any Î» âˆˆR+,
r(Î¸) â‰¤r1(Î¸) + A(Î»)

r(Î¸) âˆ’r1(Î¸)r2(Î¸)

âˆ’log

ÏµÏ€

Î”(Î¸)

Î»
,
where A(Î») is deï¬ned by equation (3.2, page 119).

3.3.
Vapnik bounds for inductive classiï¬cation
121
Solving the previous inequality in r2(Î¸), we get
Corollary 3.2.9. Under the same assumptions as in the previous theorem, with
P probability at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜,
r2(Î¸) â‰¤inf
Î»âˆˆR+
r1(Î¸)

1 + 2N
Î» log

cosh( Î»
2N )

âˆ’2 log

ÏµÏ€

Î”(Î¸)

Î»
1 âˆ’2N
Î» log

cosh( Î»
2N )

1 âˆ’2r1(Î¸)

.
Applying this to our usual numerical example of a binary classiï¬cation model
with Vapnikâ€“Cervonenkis dimension not greater than h = 10, when N = 1000,
infÎ˜ r1 = r1(	Î¸) = 10 and Ïµ = 0.01, we obtain that r2(	Î¸) â‰¤0.4450.
3.3. Vapnik bounds for inductive classification
3.3.1. Arbitrary shadow sample size.
We assume in this section that
P =
 N
B
i=1
Pi
!âŠ—âˆž
âˆˆM1
+

X Ã— Y
NN
,
where Pi âˆˆM1
+

X Ã— Y

: we consider an inï¬nite i.i.d. sequence of independent
non-identically distributed samples of size N, the ï¬rst one only being observed.
More precisely, under P each sample (Xi+jN, Yi+jN)N
i=1 is distributed according
to N
i=1 Pi, and they are all independent from each other. Only the ï¬rst sample
(Xi, Yi)N
i=1 is assumed to be observed. The shadow samples will only appear in the
proofs. The aim of this section is to prove better Vapnik bounds, generalizing them
in the same time to the independent non-i.i.d. setting, which to our knowledge has
not been done before.
Let us introduce the notation Pâ€²
h(Ï‰)

= P

h(Ï‰) | (Xi, Yi)N
i=1

, where h may be
any suitable (e.g. bounded) random variable, let us also put Î© =

(X Ã— Y)NN.
Definition 3.3.1. For any subset A âŠ‚N of integers, let C(A) be the set of circular
permutations of the totally ordered set A, extended to a permutation of N by taking
it to be the identity on the complement N \ A of A. We will say that a random
function h : Î© â†’R is k-partially exchangeable if
h(Ï‰ â—¦s) = h(Ï‰),
s âˆˆC

{i + jN ; j = 0, . . . , k}

, i = 1, . . . , N.
In the same way, we will say that a posterior distribution Ï€ : Î© â†’M1
+(Î˜) is
k-partially exchangeable if
Ï€(Ï‰ â—¦s) = Ï€(Ï‰) âˆˆM1
+(Î˜),
s âˆˆC

{i + jN ; j = 0, . . . , k}

, i = 1, . . . , N.
Note that P itself is k-partially exchangeable for any k in the sense that for any
bounded measurable function h : Î© â†’R
P

h(Ï‰ â—¦s)

= P

h(Ï‰)

,
s âˆˆC

{i + jN ; j = 0, . . . , k}

, i = 1, . . . , N.
Let Î”k(Î¸) =

Î¸â€² âˆˆÎ˜ ;

fÎ¸â€²(Xi)
(k+1)N
i=1
=

fÎ¸(Xi)
(k+1)N
i=1

, Î¸ âˆˆÎ˜, k âˆˆNâˆ—, and let
also rk(Î¸) =
1
(k + 1)N
(k+1)N

i=1
1

fÎ¸(Xi) Ì¸= Yi

. Theorem 3.1.2 shows that for any

122
Chapter 3.
Transductive PAC-Bayesian learning
positive real parameter Î» and any k-partially exchangeable posterior distribution
Ï€k : Î© â†’M1
+(Î˜),
P

exp

sup
Î¸âˆˆÎ˜
Î»

Î¦ Î»
N (rk) âˆ’r1

+ log

ÏµÏ€k

Î”k(Î¸)

â‰¤Ïµ.
Using the general fact that
P

exp(h)

= P

Pâ€²
exp(h)

â‰¥P

exp

Pâ€²(h)

,
and the fact that the expectation of a supremum is larger than the supremum of
an expectation, we see that with P probability at most 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜,
Pâ€²
Î¦ Î»
N

rk(Î¸)

â‰¤r1(Î¸) âˆ’
Pâ€²
log

ÏµÏ€k

Î”k(Î¸)

Î»
.
For short let us put
Â¯dk(Î¸) = âˆ’log

ÏµÏ€k

Î”k(Î¸)

,
dâ€²
k(Î¸) = âˆ’Pâ€²
log

ÏµÏ€k

Î”k(Î¸)

,
dk(Î¸) = âˆ’P

log

ÏµÏ€k

Î”k(Î¸)

.
We can use the convexity of Î¦ Î»
N and the fact that Pâ€²(rk) = r1+kR
k+1 , to establish
that
Pâ€²
Î¦ Î»
N

rk(Î¸)

â‰¥Î¦ Î»
N
r1(Î¸) + kR(Î¸)
k + 1

.
We have proved
Theorem 3.3.1. Using the above hypotheses and notation, for any sequence Ï€k :
Î© â†’M1
+(Î˜), where Ï€k is a k-partially exchangeable posterior distribution, for any
positive real constant Î», any positive integer k, with P probability at least 1 âˆ’Ïµ, for
any Î¸ âˆˆÎ˜,
Î¦ Î»
N
r1(Î¸) + kR(Î¸)
k + 1

â‰¤r1(Î¸) + dâ€²
k(Î¸)
Î»
.
We can make as we did with Theorem 1.2.6 (page 11) the result of this theorem
uniform in Î» âˆˆ{Î±j ; j âˆˆNâˆ—} and k âˆˆNâˆ—(considering on k the prior
1
k(k+1) and on
j the prior
1
j(j+1)), and obtain
Theorem 3.3.2. For any real parameter Î± > 1, with P probability at least 1 âˆ’Ïµ,
for any Î¸ âˆˆÎ˜,
R(Î¸) â‰¤
inf
kâˆˆNâˆ—,jâˆˆNâˆ—
1 âˆ’exp

âˆ’Î±j
N r1(Î¸) âˆ’1
N

dâ€²
k(Î¸) + log

k(k + 1)j(j + 1)

k
k+1

1 âˆ’exp

âˆ’Î±j
N

âˆ’r1(Î¸)
k
.

3.3.
Vapnik bounds for inductive classiï¬cation
123
As a special case we can choose Ï€k such that log

Ï€k

Î”k(Î¸)

is independent of
Î¸ and equal to log(Nk), where
Nk =
22
fÎ¸(Xi)
(k+1)N
i=1
; Î¸ âˆˆÎ˜
22
is the size of the trace of the classiï¬cation model on the extended sample of size
(k+1)N. With this choice, we obtain a bound involving a new ï¬‚avour of conditional
Vapnik entropy, namely
dâ€²
k(Î¸) = P

log(Nk) |(Zi)N
i=1

âˆ’log(Ïµ).
In the case of binary classiï¬cation using a Vapnikâ€“Cervonenkis class of Vapnikâ€“
Cervonenkis dimension not greater than h = 10, when N = 1000, infÎ˜ r1 = r1(	Î¸) =
0.2 and Ïµ = 0.01, choosing Î± = 1.1, we obtain R(	Î¸) â‰¤0.4271 (for an optimal value
of Î» = 1071.8, and an optimal value of k = 16).
3.3.2. A better minimization with respect to the exponential parame-
ter.
If we are not pleased with optimizing Î» on a discrete subset of the real line,
we can use a slightly diï¬€erent approach. From Theorem 3.1.2 (page 113), we see
that for any positive integer k, for any k-partially exchangeable positive real mea-
surable function Î» : Î© Ã— Î˜ â†’R+ satisfying equation (3.1, page 116) â€” with Î”(Î¸)
replaced with Î”k(Î¸) â€” for any Ïµ âˆˆ)0, 1) and Î· âˆˆ)0, 1),
P

Pâ€²

exp

sup
Î¸
Î»

Î¦ Î»
N (rk) âˆ’r1

+ log

ÏµÎ·Ï€k

Î”k(Î¸)

â‰¤ÏµÎ·,
therefore with P probability at least 1 âˆ’Ïµ,
Pâ€²

exp

sup
Î¸
Î»

Î¦ Î»
N (rk) âˆ’r1

+ log

ÏµÎ·Ï€k

Î”k(Î¸)

â‰¤Î·,
and consequently, with P probability at least 1âˆ’Ïµ, with Pâ€² probability at least 1âˆ’Î·,
for any Î¸ âˆˆÎ˜,
Î¦ Î»
N (rk) + log

ÏµÎ·Ï€k

Î”k(Î¸)

Î»
â‰¤r1.
Now we are entitled to choose
Î»(Ï‰, Î¸) âˆˆarg max
Î»â€²âˆˆR+ Î¦ Î»â€²
N (rk) + log

ÏµÎ·Ï€k

Î”k(Î¸)

Î»â€²
.
This shows that with P probability at least 1 âˆ’Ïµ, with Pâ€² probability at least 1 âˆ’Î·,
for any Î¸ âˆˆÎ˜,
sup
Î»âˆˆR+
Î¦ Î»
N (rk) âˆ’
Â¯dk(Î¸) âˆ’log(Î·)
Î»
â‰¤r1,
which can also be written
Î¦ Î»
N (rk) âˆ’r1 âˆ’
Â¯dk(Î¸)
Î»
â‰¤âˆ’log(Î·)
Î»
,
Î» âˆˆR+.
Thus with P probability at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜, any Î» âˆˆR+,
Pâ€²

Î¦ Î»
N (rk) âˆ’r1 âˆ’
Â¯dk(Î¸)
Î»

â‰¤âˆ’log(Î·)
Î»
+

1 âˆ’r1 + log(Î·)
Î»

Î·.

124
Chapter 3.
Transductive PAC-Bayesian learning
On the other hand, Î¦ Î»
N being a convex function,
Pâ€²

Î¦ Î»
N (rk) âˆ’r1 âˆ’
Â¯dk(Î¸)
Î»

â‰¥Î¦ Î»
N

Pâ€²(rk)

âˆ’r1 âˆ’dâ€²
k
Î»
= Î¦ Î»
N
 kR + r1
k + 1
!
âˆ’r1 âˆ’dâ€²
k
Î» .
Thus with P probability at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜,
kR + r1
k + 1
â‰¤inf
Î»âˆˆR+ Î¦âˆ’1
Î»
N

r1(1 âˆ’Î·) + Î· + dâ€²
k âˆ’log(Î·)(1 âˆ’Î·)
Î»

.
We can generalize this approach by considering a ï¬nite decreasing sequence Î·0 =
1 > Î·1 > Î·2 > Â· Â· Â· > Î·J > Î·J+1 = 0, and the corresponding sequence of levels
Lj = âˆ’log(Î·j)
Î»
, 0 â‰¤j â‰¤J,
LJ+1 = 1 âˆ’r1 âˆ’log(J) âˆ’log(Ïµ)
Î»
.
Taking a union bound in j, we see that with P probability at least 1 âˆ’Ïµ, for any
Î¸ âˆˆÎ˜, for any Î» âˆˆR+,
Pâ€²

Î¦ Î»
N (rk) âˆ’r1 âˆ’
Â¯dk + log(J)
Î»
â‰¥Lj

â‰¤Î·j,
j = 0, . . . , J + 1,
and consequently
Pâ€²

Î¦ Î»
N (rk) âˆ’r1 âˆ’
Â¯dk + log(J)
Î»

â‰¤
 LJ+1
0
Pâ€²

Î¦ Î»
N (rk) âˆ’r1 âˆ’
Â¯dk + log(J)
Î»
â‰¥Î±

dÎ±
â‰¤
J+1

j=1
Î·jâˆ’1(Lj âˆ’Ljâˆ’1)
= Î·J

1 âˆ’r1 âˆ’log(J) âˆ’log(Ïµ) âˆ’log(Î·J)
Î»

âˆ’log(Î·1)
Î»
+
Jâˆ’1

j=1
Î·j
Î» log
 Î·j
Î·j+1
!
.
Let us put
dâ€²â€²
k

Î¸, (Î·j)J
j=1

= dâ€²
k(Î¸) + log(J) âˆ’log(Î·1)
+
Jâˆ’1

j=1
Î·j log
 Î·j
Î·j+1
!
+ log
ÏµÎ·J
J

Î·J.
We have proved that for any decreasing sequence (Î·j)J
j=1, with P probability at
least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜,
kR + r1
k + 1
â‰¤inf
Î»âˆˆR+ Î¦âˆ’1
Î»
N

r1(1 âˆ’Î·J) + Î·J + dâ€²â€²
k

Î¸, (Î·j)J
j=1

Î»

.
Remark 3.3.1. We can for instance choose J = 2, Î·2 =
1
10N , Î·1 =
1
log(10N),
resulting in
dâ€²â€²
k = dâ€²
k + log(2) + log log(10N) + 1 âˆ’log log(10N)
log(10N)
âˆ’log

 20N
Ïµ

10N
.

3.3.
Vapnik bounds for inductive classiï¬cation
125
In the case where N = 1000 and for any Ïµ âˆˆ)0, 1), we get dâ€²â€²
k â‰¤dâ€²
k + 3.7, in the case
where N = 106, we get dâ€²â€²
k â‰¤dâ€²
k +4.4, and in the case N = 109, we get dâ€²â€²
k â‰¤dâ€²
k +4.7.
Therefore, for any practical purpose we could take dâ€²â€²
k = dâ€²
k + 4.7 and Î·J =
1
10N
in the above inequality.
Taking moreover a weighted union bound in k, we get
Theorem 3.3.3. For any Ïµ âˆˆ)0, 1), any sequence 1 > Î·1 > Â· Â· Â· > Î·J > 0, any
sequence Ï€k : Î© â†’M1
+(Î˜), where Ï€k is a k-partially exchangeable posterior distri-
bution, with P probability at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜,
R(Î¸) â‰¤inf
kâˆˆNâˆ—
k + 1
k
inf
Î»âˆˆR+ Î¦âˆ’1
Î»
N

r1(Î¸) + Î·J

1 âˆ’r1(Î¸)

+ dâ€²â€²
k

Î¸, (Î·j)J
j=1

+ log

k(k + 1)

Î»

âˆ’r1(Î¸)
k
.
Corollary 3.3.4. For any Ïµ âˆˆ)0, 1), for any N â‰¤109, with P probability at least
1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜,
R(Î¸) â‰¤inf
kâˆˆNâˆ—inf
Î»âˆˆR+
k + 1
k

1 âˆ’exp(âˆ’Î»
N )
âˆ’1

1 âˆ’exp

âˆ’Î»
N

r1(Î¸) +
1
10N

âˆ’Pâ€²
log(Nk) | (Zi)N
i=1

âˆ’log(Ïµ) + log

k(k + 1)

+ 4.7
N

âˆ’r1(Î¸)
k
.
Let us end this section with a numerical example: in the case of binary classi-
ï¬cation with a Vapnikâ€“Cervonenkis class of dimension not greater than 10, when
N = 1000, infÎ˜ r1 = r1(	Î¸) = 0.2 and Ïµ = 0.01, we get a bound R(	Î¸) â‰¤0.4211 (for
optimal values of k = 15 and of Î» = 1010).
3.3.3. Equal shadow and training sample sizes.
In the case when k =
1, we can use Theorem 3.2.5 (page 118) and replace Î¦âˆ’1
Î»
N (q) with

1 âˆ’2N
Î» Ã—
log

cosh( Î»
2N )
âˆ’1q, resulting in
Theorem 3.3.5. For any Ïµ âˆˆ)0, 1), any N â‰¤109, any one-partially exchangeable
posterior distribution Ï€1 : Î© â†’M1
+(Î˜), with P probability at least 1 âˆ’Ïµ, for any
Î¸ âˆˆÎ˜,
R(Î¸) â‰¤inf
Î»âˆˆR+

1 + 2N
Î» log

cosh( Î»
2N )

r1(Î¸) + 1
5N + 2dâ€²
1(Î¸) + 4.7
Î»
1 âˆ’2N
Î» log

cosh( Î»
2N )

.
3.3.4. Improvement on the equal sample size bound in the i.i.d. case.
Finally, in the case when P is i.i.d., meaning that all the Pi are equal, we can improve
the previous bound. For any partially exchangeable function Î» : Î© Ã— Î˜ â†’R+, we
saw in the discussion preceding Theorem 3.2.6 (page 120) that
T

exp

Î»(rk âˆ’r1) âˆ’A(Î»)v

â‰¤1,

126
Chapter 3.
Transductive PAC-Bayesian learning
with the notation introduced therein. Thus for any partially exchangeable positive
real measurable function Î» : Î© Ã— Î˜ â†’R+ satisfying equation (3.1, page 116), any
one-partially exchangeable posterior distribution Ï€1 : Î© â†’M1
+(Î˜),
P

exp

sup
Î¸âˆˆÎ˜
Î»

rk(Î¸) âˆ’r1(Î¸) âˆ’A(Î»)v(Î¸)

+ log

ÏµÏ€1

Î”(Î¸)

â‰¤1.
Therefore with P probability at least 1 âˆ’Ïµ, with Pâ€² probability 1 âˆ’Î·,
rk(Î¸) â‰¤r1(Î¸) + A(Î»)v(Î¸) + 1
Î»
 Â¯d1(Î¸) âˆ’log(Î·)

.
We can then choose Î»(Ï‰, Î¸) âˆˆarg min
Î»â€²âˆˆR+ A(Î»â€²)v(Î¸)+
Â¯d1(Î¸) âˆ’log(Î·)

Î»â€²
, which satis-
ï¬es the required conditions, to show that with P probability at least 1 âˆ’Ïµ, for any
Î¸ âˆˆÎ˜, with Pâ€² probability at least 1 âˆ’Î·, for any Î» âˆˆR+,
rk(Î¸) â‰¤r1(Î¸) + A(Î»)v(Î¸) +
Â¯d1(Î¸) âˆ’log(Î·)
Î»
.
We can then take a union bound on a decreasing sequence of J values Î·1 â‰¥Â· Â· Â· â‰¥
Î·J of Î·. Weakening the order of quantiï¬ers a little, we then obtain the following
statement: with P probability at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜, for any Î» âˆˆR+, for any
j = 1, . . . , J
Pâ€²

rk(Î¸) âˆ’r1(Î¸) âˆ’A(Î»)v(Î¸) âˆ’
Â¯d1(Î¸) + log(J)
Î»
â‰¥âˆ’log(Î·j)
Î»

â‰¤Î·j.
Consequently for any Î» âˆˆR+,
Pâ€²

rk(Î¸) âˆ’r1(Î¸) âˆ’A(Î»)v(Î¸) âˆ’
Â¯d1(Î¸) + log(J)
Î»

â‰¤âˆ’log(Î·1)
Î»
+ Î·J

1 âˆ’r1(Î¸) âˆ’log(J) âˆ’log(Ïµ) âˆ’log(Î·J)
Î»

+
Jâˆ’1

j=1
Î·j
Î» log
 Î·j
Î·j+1
!
.
Moreover Pâ€²
v(Î¸)

=
r1+R
2
âˆ’r1R, (this is where we need equidistribution) thus
proving that
R âˆ’r1
2
â‰¤A(Î»)
2

R + r1 âˆ’2r1R

+ dâ€²â€²
1

Î¸, (Î·j)J
j=1

Î»
+ Î·J

1 âˆ’r1(Î¸)

.
Keeping track of quantiï¬ers, we obtain
Theorem 3.3.6. For any decreasing sequence (Î·j)J
j=1, any Ïµ âˆˆ)0, 1), any one-
partially exchangeable posterior distribution Ï€ : Î© â†’M1
+(Î˜), with P probability
at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜,
R(Î¸) â‰¤inf
Î»âˆˆR+

1 + 2N
Î» log

cosh( Î»
2N )

r1(Î¸) + 2dâ€²â€²
1

Î¸, (Î·j)J
j=1

Î»
+ 2Î·J

1 âˆ’r1(Î¸)

1 âˆ’2N
Î» log

cosh( Î»
2N )

1 âˆ’2r1(Î¸)

.

3.4.
Gaussian approximation in Vapnik bounds
127
3.4. Gaussian approximation in Vapnik bounds
3.4.1. Gaussian upper bounds of variance terms.
To obtain formulas which
could be easily compared with original Vapnik bounds, we may replace p âˆ’Î¦a(p)
with a Gaussian upper bound:
Lemma 3.4.1. For any p âˆˆ(0, 1
2), any a âˆˆR+,
p âˆ’Î¦a(p) â‰¤a
2p(1 âˆ’p).
For any p âˆˆ( 1
2, 1),
p âˆ’Î¦a(p) â‰¤a
8.
Proof. Let us notice that for any p âˆˆ(0, 1),
âˆ‚
âˆ‚a

âˆ’aÎ¦a(p)

= âˆ’
p exp(âˆ’a)
1 âˆ’p + p exp(âˆ’a),
âˆ‚2
âˆ‚2a

âˆ’aÎ¦a(p)

=
p exp(âˆ’a)
1 âˆ’p + p exp(âˆ’a)
 
1 âˆ’
p exp(âˆ’a)
1 âˆ’p + p exp(âˆ’a)
!
â‰¤

p(1 âˆ’p)
p âˆˆ(0, 1
2),
1
4
p âˆˆ( 1
2, 1).
Thus taking a Taylor expansion of order one with integral remainder:
âˆ’aÎ¦(a) â‰¤
âŽ§
âŽª
âŽª
âŽª
âŽª
âŽª
âŽª
âŽ¨
âŽª
âŽª
âŽª
âŽª
âŽª
âŽª
âŽ©
âˆ’ap +
 a
0
p(1 âˆ’p)(a âˆ’b)db
= âˆ’ap + a2
2 p(1 âˆ’p),
p âˆˆ(0, 1
2),
âˆ’ap +
 a
0
1
4(a âˆ’b)db = âˆ’ap + a2
8 ,
p âˆˆ( 1
2, 1).
This ends the proof of our lemma. â–¡
Lemma 3.4.2.
Let us consider the bound
B(q, d) =
 
1 + 2d
N
!âˆ’1 
q + d
N +
A
2dq(1 âˆ’q)
N
+ d2
N 2

,
q âˆˆR+, d âˆˆR+.
Let us also put
Â¯B(q, d) =

B(q, d)
B(q, d) â‰¤1
2,
q +
%
d
2N
otherwise.
For any positive real parameters q and d
inf
Î»âˆˆR+ Î¦âˆ’1
Î»
N
 
q + d
Î»
!
â‰¤Â¯B(q, d).
Proof. Let p = inf
Î» Î¦âˆ’1
Î»
N
 
q + d
Î»
!
. For any Î» âˆˆR+,
p âˆ’Î»
2N (p âˆ§1
2)

1 âˆ’(p âˆ§1
2)

â‰¤Î¦ Î»
N (p) â‰¤q + d
Î».

128
Chapter 3.
Transductive PAC-Bayesian learning
Thus
p â‰¤q + inf
Î»âˆˆR+
Î»
2N (p âˆ§1
2)

1 âˆ’(p âˆ§1
2)

+ d
Î»
= q +
$
2d(p âˆ§1
2)

1 âˆ’(p âˆ§1
2)

N
â‰¤q +
A
d
2N .
Then let us remark that B(q, d) = sup

pâ€² âˆˆR+ ; pâ€² â‰¤q +
A
2dpâ€²(1 âˆ’pâ€²)
N

. If
moreover 1
2 â‰¥B(q, d), then according to this remark 1
2 â‰¥q +
%
d
2N â‰¥p. Therefore
p â‰¤1
2, and consequently p â‰¤q +
%
2dp(1âˆ’p)
N
, implying that p â‰¤B(q, d). â–¡
3.4.2. Arbitrary shadow sample size.
The previous lemma combined with
Corollary 3.3.4 (page 125) implies
Corollary 3.4.3. Let us use the notation introduced in Lemma 3.4.2 (page 127).
For any Ïµ âˆˆ)0, 1), any integer N â‰¤109, with P probability at least 1 âˆ’Ïµ, for any
Î¸ âˆˆÎ˜,
R(Î¸) â‰¤inf
kâˆˆNâˆ—
k + 1
k

Â¯B

r1(Î¸) +
1
10N , dâ€²
k(Î¸) + log

k(k + 1)

+ 4.7

âˆ’r1(Î¸)
k
.
3.4.3. Equal sample sizes in the i.i.d. case.
To make a link with Vapnikâ€™s
result, it is useful to state the Gaussian approximation to Theorem 3.3.6 (page 126).
Indeed, using the upper bound A(Î») â‰¤
Î»
4N , where A(Î») is deï¬ned by equation (3.2)
on page 119, we get with P probability at least 1 âˆ’Ïµ
R âˆ’r1 âˆ’2Î·J â‰¤inf
Î»âˆˆR+
Î»
4N

R + r1 âˆ’2r1R

+ 2dâ€²â€²
1
Î»
=
A
2dâ€²â€²
1(R + r1 âˆ’2r1R)
N
,
which can be solved in R to obtain
Corollary 3.4.4. With P probability at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜,
R(Î¸) â‰¤r1(Î¸) + dâ€²â€²
1(Î¸)
N

1 âˆ’2r1(Î¸)

+ 2Î·J
+
$
4dâ€²â€²
1(Î¸)

1 âˆ’r1(Î¸)

r1(Î¸)
N
+ dâ€²â€²
1(Î¸)2
N 2

1 âˆ’2r1(Î¸)
2 + 4dâ€²â€²
1(Î¸)
N

1 âˆ’2r1(Î¸)

Î·J.
This is to be compared with Vapnikâ€™s result, as proved in Vapnik (1998, page
138):
Theorem 3.4.5 (Vapnik). For any i.i.d. probability distribution P, with P prob-
ability at least 1 âˆ’Ïµ, for any Î¸ âˆˆÎ˜, putting
dV = log

P(N1)

+ log(4/Ïµ),
R(Î¸) â‰¤r1(Î¸) + 2dV
N
+
A
4dV r1(Î¸)
N
+ 4d2
V
N 2 .

3.4.
Gaussian approximation in Vapnik bounds
129
Recalling that we can choose (Î·j)2
j=1 such that Î·J = Î·2 =
1
10N (which brings a
negligible contribution to the bound) and such that for any N â‰¤109,
dâ€²â€²
1(Î¸) â‰¤P

log(N1) | (Zi)N
i=1

âˆ’log(Ïµ) + 4.7,
we see that our complexity term is somehow more satisfactory than Vapnikâ€™s, since
it is integrated outside the logarithm, with a slightly larger additional constant
(remember that log 4 â‰ƒ1.4, which is better than our 4.7, which could presumably
be improved by working out a better sequence Î·j, but not down to log(4)). Our
variance term is better, since we get r1(1 âˆ’r1), instead of r1. We also have dâ€²â€²
1
N
instead of 2dV
N , because we use no symmetrization trick.
Let us illustrate these bounds on a numerical example, corresponding to a situ-
ation where the sample is noisy or the classiï¬cation model is weak. Let us assume
that N = 1000, infÎ˜ r1 = r1(	Î¸) = 0.2, that we are performing binary classiï¬cation
with a model with Vapnikâ€“Cervonenkis dimension not greater than h = 10, and
that we work at conï¬dence level Ïµ = 0.01. Vapnikâ€™s theorem provides an upper
bound for R(	Î¸) not smaller than 0.610, whereas Corollary 3.4.4 gives R(	Î¸) â‰¤0.461
(using the bound dâ€²â€²
1 â‰¤dâ€²
1 + 3.7 when N = 1000). Now if we go for Theorem 3.3.6
and do not make a Gaussian approximation, we get R(	Î¸) â‰¤0.453. It is interesting
to remark that this bound is achieved for Î» = 1195 > N = 1000. This explains why
the Gaussian approximation in Vapnikâ€™s bound can be improved: for such a large
value of Î», Î»r1(Î¸) does not behave like a Gaussian random variable.
Let us recall in conclusion that the best bound is provided by Theorem 3.3.3
(page 125), giving R(	Î¸) â‰¤0.4211, (that is approximately 2/3 of Vapnikâ€™s bound),
for optimal values of k = 15, and of Î» = 1010. This bound can be seen to take ad-
vantage of the fact that Bernoulli random variables are not Gaussian (its Gaussian
approximation, Corollary 3.4.3, gives a bound R(Î¸) â‰ƒ0.4325, still with an optimal
k = 15), and of the fact that the optimal size of the shadow sample is signiï¬cantly
larger than the size of the observed sample. Moreover, Theorem 3.3.3 does not as-
sume that the sample is i.i.d., but only that it is independent, thus generalizing
Vapnikâ€™s bounds to inhomogeneous data (this will presumably be the case when
data are collected from diï¬€erent places where the experimental conditions may not
be the same, although they may reasonably be assumed to be independent).
Our little numerical example was chosen to illustrate the case when it is non-
trivial to decide whether the chosen classiï¬er does better than the 0.5 error rate
of blind random classiï¬cation. This case is of interest to choose â€œweak learnersâ€
to be aggregated or combined in some appropriate way in a second stage to reach
a better classiï¬cation rate. This stage of feature selection is unavoidable in many
real world classiï¬cation tasks. Our little computations are meant to exemplify the
fact that Vapnikâ€™s bounds, although asymptotically suboptimal, as is obvious by
comparison with the ï¬rst two chapters, can do the job when dealing with moderate
sample sizes.

130
Chapter 3.
Transductive PAC-Bayesian learning

Chapter 4
Support Vector Machines
4.1. How to build them
4.1.1. The canonical hyperplane.
Support Vector Machines, of wide use and
renown, were conceived by V. Vapkik (Vapnik, 1998). Before introducing them,
we will study as a prerequisite the separation of points by hyperplanes in a ï¬nite
dimensional Euclidean space. Support Vector Machines perform the same kind of
linear separation after an implicit change of pattern space. The preceding PAC-
Bayesian results provide a ï¬t framework to analyse their generalization properties.
In this section we deal with the classiï¬cation of points in Rd in two classes. Let
Z = (xi, yi)N
i=1 âˆˆ

Rd Ã— {âˆ’1, +1}
N be some set of labelled examples (called the
training set hereafter). Let us split the set of indices I = {1, . . . , N} according to
the labels into two subsets
I+ = {i âˆˆI : yi = +1},
Iâˆ’= {i âˆˆI : yi = âˆ’1}.
Let us then consider the set of admissible separating directions
AZ =

w âˆˆRd : sup
bâˆˆR
inf
iâˆˆI(âŸ¨w, xiâŸ©âˆ’b)yi â‰¥1

,
which can also be written as
AZ =

w âˆˆRd : max
iâˆˆIâˆ’âŸ¨w, xiâŸ©+ 2 â‰¤min
iâˆˆI+âŸ¨w, xiâŸ©

.
As it is easily seen, the optimal value of b for a ï¬xed value of w, in other words the
value of b which maximizes infiâˆˆI(âŸ¨w, xiâŸ©âˆ’b)yi, is equal to
bw = 1
2

max
iâˆˆIâˆ’âŸ¨w, xiâŸ©+ min
iâˆˆI+âŸ¨w, xiâŸ©

.
Lemma 4.1.1. When AZ Ì¸= âˆ…, inf{âˆ¥wâˆ¥2 : w âˆˆAZ} is reached for only one value
wZ of w.
Proof. Let w0 âˆˆAZ. The set AZ âˆ©{w âˆˆRd : âˆ¥wâˆ¥â‰¤âˆ¥w0âˆ¥} is a compact convex
set and w â†’âˆ¥wâˆ¥2 is strictly convex and therefore has a unique minimum on this
set, which is also obviously its minimum on AZ. â–¡
131

132
Chapter 4.
Support Vector Machines
Definition 4.1.1. When AZ Ì¸= âˆ…, the training set Z is said to be linearly sepa-
rable. The hyperplane
H = {x âˆˆRd : âŸ¨wZ, xâŸ©âˆ’bZ = 0},
where
wZ = arg min{âˆ¥wâˆ¥: w âˆˆAZ},
bZ = bwZ,
is called the canonical separating hyperplane of the training set Z. The quantity
âˆ¥wZâˆ¥âˆ’1 is called the margin of the canonical hyperplane.
As miniâˆˆI+âŸ¨wZ, xiâŸ©âˆ’maxiâˆˆIâˆ’âŸ¨wZ, xiâŸ©= 2, the margin is also equal to half the
distance between the projections on the direction wZ of the positive and negative
patterns.
4.1.2. Computation of the canonical hyperplane.
Let us consider the con-
vex hulls X+ and Xâˆ’of the positive and negative patterns:
X+ =

iâˆˆI+
Î»ixi :

Î»i

iâˆˆI+ âˆˆRI+
+ ,

iâˆˆI+
Î»i = 1

,
Xâˆ’=
 
iâˆˆIâˆ’
Î»ixi :

Î»i

iâˆˆIâˆ’âˆˆRIâˆ’
+ ,

iâˆˆIâˆ’
Î»i = 1

.
Let us introduce the closed convex set
V = X+ âˆ’Xâˆ’=

x+ âˆ’xâˆ’: x+ âˆˆX+, xâˆ’âˆˆXâˆ’

.
As v â†’âˆ¥vâˆ¥2 is strictly convex, with compact lower level sets, there is a unique
vector vâˆ—such that
âˆ¥vâˆ—âˆ¥2 = inf
vâˆˆV

âˆ¥vâˆ¥2 : v âˆˆV

.
Lemma 4.1.2. The set AZ is non-empty (i.e. the training set Z is linearly separa-
ble) if and only if vâˆ—Ì¸= 0. In this case
wZ =
2
âˆ¥vâˆ—âˆ¥2 vâˆ—,
and the margin of the canonical hyperplane is equal to 1
2âˆ¥vâˆ—âˆ¥.
This lemma proves that the distance between the convex hulls of the positive
and negative patterns is equal to twice the margin of the canonical hyperplane.
Proof. Let us assume ï¬rst that vâˆ—= 0, or equivalently that X+ âˆ©Xâˆ’Ì¸= âˆ…. For
any vector w âˆˆRd,
min
iâˆˆI+âŸ¨w, xiâŸ©= min
xâˆˆX+âŸ¨w, xâŸ©,
max
iâˆˆIâˆ’âŸ¨w, xiâŸ©= max
xâˆˆXâˆ’âŸ¨w, xâŸ©,
so miniâˆˆI+âŸ¨w, xiâŸ©âˆ’maxiâˆˆIâˆ’âŸ¨w, xiâŸ©â‰¤0, which shows that w cannot be in AZ and
therefore that AZ is empty.

4.1.
How to build them
133
Let us assume now that vâˆ—Ì¸= 0, or equivalently that X+ âˆ©Xâˆ’= âˆ…. Let us put
wâˆ—= 2vâˆ—/âˆ¥vâˆ—âˆ¥2. Let us remark ï¬rst that
min
iâˆˆI+âŸ¨wâˆ—, xiâŸ©âˆ’max
iâˆˆIâˆ’âŸ¨wâˆ—, xiâŸ©= inf
xâˆˆX+âŸ¨wâˆ—, xâŸ©âˆ’sup
xâˆˆXâˆ’
âŸ¨wâˆ—, xâŸ©
=
inf
x+âˆˆX+,xâˆ’âˆˆXâˆ’âŸ¨wâˆ—, x+ âˆ’xâˆ’âŸ©
=
2
âˆ¥vâˆ—âˆ¥2 inf
vâˆˆVâŸ¨vâˆ—, vâŸ©.
Let us now prove that infvâˆˆVâŸ¨vâˆ—, vâŸ©= âˆ¥vâˆ—âˆ¥2. Some arbitrary v âˆˆV being ï¬xed,
consider the function
Î² â†’âˆ¥Î²v + (1 âˆ’Î²)vâˆ—âˆ¥2 : [0, 1] â†’R.
By deï¬nition of vâˆ—, it reaches its minimum value for Î² = 0, and therefore has
a non-negative derivative at this point. Computing this derivative, we ï¬nd that
âŸ¨v âˆ’vâˆ—, vâˆ—âŸ©â‰¥0, as claimed. We have proved that
min
iâˆˆI+âŸ¨wâˆ—, xiâŸ©âˆ’max
iâˆˆIâˆ’âŸ¨wâˆ—, xiâŸ©= 2,
and therefore that wâˆ—âˆˆAZ. On the other hand, any w âˆˆAZ is such that
2 â‰¤min
iâˆˆI+âŸ¨w, xiâŸ©âˆ’max
iâˆˆIâˆ’âŸ¨w, xiâŸ©= inf
vâˆˆVâŸ¨w, vâŸ©â‰¤âˆ¥wâˆ¥inf
vâˆˆVâˆ¥vâˆ¥= âˆ¥wâˆ¥âˆ¥vâˆ—âˆ¥.
This proves that âˆ¥wâˆ—âˆ¥= inf

âˆ¥wâˆ¥: w âˆˆAZ

, and therefore that wâˆ—= wZ as
claimed. â–¡
One way to compute wZ would therefore be to compute vâˆ—by minimizing
CCCCC

iâˆˆI
Î»iyixi
CCCCC
2
: (Î»i)iâˆˆI âˆˆRI
+,

iâˆˆI
Î»i = 2,

iâˆˆI
yiÎ»i = 0

.
Although this is a tractable quadratic programming problem, a direct computation
of wZ through the following proposition is usually preferred.
Proposition 4.1.3.
The canonical direction wZ can be expressed as
wZ =
N

i=1
Î±âˆ—
i yixi,
where (Î±âˆ—
i )N
i=1 is obtained by minimizing
inf

F(Î±) : Î± âˆˆA

where
A =

(Î±i)iâˆˆI âˆˆRI
+,

iâˆˆI
Î±iyi = 0

,
and
F(Î±) =
CCC

iâˆˆI
Î±iyixi
CCC
2
âˆ’2

iâˆˆI
Î±i.

134
Chapter 4.
Support Vector Machines
Proof. Let w(Î±) = 
iâˆˆI Î±iyixi and let S(Î±) =
1
2

iâˆˆI Î±i. We can express
the function F(Î±) as F(Î±) = âˆ¥w(Î±)âˆ¥2 âˆ’4S(Î±). Moreover it is important to no-
tice that for any s âˆˆR+, {w(Î±) : Î± âˆˆA, S(Î±) = s} = sV. This shows that
for any s âˆˆR+, inf{F(Î±) : Î± âˆˆA, S(Î±) = s} is reached and that for any
Î±s
âˆˆ
{Î±
âˆˆ
A
:
S(Î±)
=
s} reaching this inï¬mum, w(Î±s)
=
svâˆ—. As
s â†’s2âˆ¥vâˆ—âˆ¥2 âˆ’4s : R+ â†’R reaches its inï¬mum for only one value sâˆ—of s, namely
at sâˆ—=
2
âˆ¥vâˆ—âˆ¥2 , this shows that F(Î±) reaches its inï¬mum on A, and that for any
Î±âˆ—âˆˆA such that F(Î±âˆ—) = inf{F(Î±) : Î± âˆˆA}, w(Î±âˆ—) =
2
âˆ¥vâˆ—âˆ¥2 vâˆ—= wZ. â–¡
4.1.3. Support vectors.
Definition 4.1.2. The set of support vectors S is deï¬ned by
S = {xi : âŸ¨wZ, xiâŸ©âˆ’bZ = yi}.
Proposition 4.1.4.
Any Î±âˆ—minimizing F(Î±) on A is such that
{xi : Î±âˆ—
i > 0} âŠ‚S.
This implies that the representation wZ = w(Î±âˆ—) involves in general only a limited
number of non-zero coeï¬ƒcients and that wZ = wZâ€², where Zâ€² = {(xi, yi) : xi âˆˆS}.
Proof. Let us consider any given i âˆˆI+ and j âˆˆIâˆ’, such that Î±âˆ—
i > 0 and Î±âˆ—
j > 0.
There exists at least one such index in each set Iâˆ’and I+, since the sum of the
components of Î±âˆ—on each of these sets are equal and since 
kâˆˆI Î±âˆ—
k > 0. For any
t âˆˆR, consider
Î±k(t) = Î±âˆ—
k + t1(k âˆˆ{i, j}),
k âˆˆI.
The vector Î±(t) is in A for any value of t in some neighbourhood of 0, therefore
âˆ‚
âˆ‚t |t=0F

Î±(t)

= 0. Computing this derivative, we ï¬nd that
yiâŸ¨w(Î±âˆ—), xiâŸ©+ yjâŸ¨w(Î±âˆ—), xjâŸ©= 2.
As yi = âˆ’yj, this can also be written as
yi

âŸ¨w(Î±âˆ—), xiâŸ©âˆ’bZ

+ yj

âŸ¨w(Î±âˆ—), xjâŸ©âˆ’bZ

= 2.
As w(Î±âˆ—) âˆˆAZ,
yk

âŸ¨w(Î±âˆ—), xkâŸ©âˆ’bZ

â‰¥1,
k âˆˆI,
which implies necessarily as claimed that
yi

âŸ¨w(Î±âˆ—), xiâŸ©âˆ’bZ

= yj

âŸ¨w(Î±âˆ—), xjâŸ©âˆ’bZ

= 1.
â–¡
4.1.4. The non-separable case.
In the case when the training set Z = (xi,
yi)N
i=1 is not linearly separable, we can deï¬ne a noisy canonical hyperplane as fol-
lows: we can choose w âˆˆRd and b âˆˆR to minimize
(4.1)
C(w, b) =
N

i=1

1 âˆ’

âŸ¨w, xiâŸ©âˆ’b

yi

+ + 1
2âˆ¥wâˆ¥2,
where for any real number r, r+ = max{r, 0} is the positive part of r.

4.1.
How to build them
135
Theorem 4.1.5. Let us introduce the dual criterion
F(Î±) =
N

i=1
Î±i âˆ’1
2
CCCC
N

i=1
yiÎ±ixi
CCCC
2
and the domain Aâ€² =

Î± âˆˆRN
+ : Î±i â‰¤1, i = 1, . . . , N,
N

i=1
yiÎ±i = 0

. Let Î±âˆ—âˆˆAâ€²
be such that F(Î±âˆ—) = supÎ±âˆˆAâ€² F(Î±). Let wâˆ—= N
i=1 yiÎ±âˆ—
i xi. There is a threshold
bâˆ—(whose construction will be detailed in the proof), such that
C(wâˆ—, bâˆ—) =
inf
wâˆˆRd,bâˆˆR C(w, b).
Corollary 4.1.6. (scaled criterion) For any positive real parameter Î» let us
consider the criterion
CÎ»(w, b) = Î»2
N

i=1

1 âˆ’(âŸ¨w, xiâŸ©âˆ’b)yi

+ + 1
2âˆ¥wâˆ¥2
and the domain
Aâ€²
Î» =

Î± âˆˆRN
+ : Î±i â‰¤Î»2, i = 1, . . . , N,
N

i=1
yiÎ±i = 0

.
For any solution Î±âˆ—of the minimization problem F(Î±âˆ—) = supÎ±âˆˆAâ€²
Î» F(Î±), the vector
wâˆ—= N
i=1 yiÎ±âˆ—
i xi is such that
inf
bâˆˆR CÎ»(wâˆ—, b) =
inf
wâˆˆRd,bâˆˆR CÎ»(w, b).
In the separable case, the scaled criterion is minimized by the canonical hyper-
plane for Î» large enough. This extension of the canonical hyperplane computation
in dual space is often called the box constraint, for obvious reasons.
Proof. The corollary is a straightforward consequence of the scale property
CÎ»(w, b, x) = Î»2C(Î»âˆ’1w, b, Î»x), where we have made the dependence of the crite-
rion in x âˆˆRdN explicit. Let us come now to the proof of the theorem.
The minimization of C(w, b) can be performed in dual space extending the couple
of parameters (w, b) to w = (w, b, Î³) âˆˆRd Ã— R Ã— RN
+ and introducing the dual
multipliers Î± âˆˆRN
+ and the criterion
G(Î±, w) =
N

i=1
Î³i +
N

i=1
Î±i

1 âˆ’(âŸ¨w, xiâŸ©âˆ’b)yi

âˆ’Î³i

+ 1
2âˆ¥wâˆ¥2.
We see that
C(w, b) = inf
Î³âˆˆRN
+
sup
Î±âˆˆRN
+
G

Î±, (w, b, Î³)

,
and therefore, putting W = {(w, b, Î³) : w âˆˆRd, b âˆˆR, Î³ âˆˆRN
+

, we are led to solve
the minimization problem
G(Î±âˆ—, wâˆ—) = inf
wâˆˆW
sup
Î±âˆˆRN
+
G(Î±, w),

136
Chapter 4.
Support Vector Machines
whose solution wâˆ—= (wâˆ—, bâˆ—, Î³âˆ—) is such that C(wâˆ—, bâˆ—) = inf(w,b)âˆˆRd+1 C(w, b),
according to the preceding identity. As for any value of Î±â€² âˆˆRN
+,
inf
wâˆˆW
sup
Î±âˆˆRN
+
G(Î±, w) â‰¥inf
wâˆˆW
G(Î±â€², w),
it is immediately seen that
inf
wâˆˆW
sup
Î±âˆˆRN
+
G(Î±, w) â‰¥sup
Î±âˆˆRN
+
inf
wâˆˆW
G(Î±, w).
We are going to show that there is no duality gap, meaning that this inequality is
indeed an equality. More importantly, we will do so by exhibiting a saddle point,
which, solving the dual minimization problem will also solve the original one.
Let us ï¬rst make explicit the solution of the dual problem (the interest of this
dual problem precisely lies in the fact that it can more easily be solved explicitly).
Introducing the admissible set of values of Î±,
Aâ€² =

Î± âˆˆRN : 0 â‰¤Î±i â‰¤1, i = 1, . . . , N,
N

i=1
yiÎ±i = 0

,
it is elementary to check that
inf
wâˆˆW
G(Î±, w) =

inf
wâˆˆRd G

Î±, (w, 0, 0)

,
Î± âˆˆAâ€²,
âˆ’âˆž,
otherwise.
As
G

Î±, (w, 0, 0)

= 1
2âˆ¥wâˆ¥2 +
N

i=1
Î±i

1 âˆ’âŸ¨w, xiâŸ©yi

,
we see that infwâˆˆRd G

Î±, (w, 0, 0)

is reached at
wÎ± =
N

i=1
yiÎ±ixi.
This proves that
inf
wâˆˆW
G(Î±, w) = F(Î±).
The continuous map Î± â†’infwâˆˆW G(Î±, w) reaches a maximum Î±âˆ—, not necessarily
unique, on the compact convex set Aâ€². We are now going to exhibit a choice of
wâˆ—âˆˆW such that (Î±âˆ—, wâˆ—) is a saddle point. This means that we are going to show
that
G(Î±âˆ—, wâˆ—) = inf
wâˆˆW
G(Î±âˆ—, w) = sup
Î±âˆˆRN
+
G(Î±, wâˆ—).
It will imply that
inf
wâˆˆW
sup
Î±âˆˆRd
+
G(Î±, w) â‰¤sup
Î±âˆˆRN
+
G(Î±, wâˆ—) = G(Î±âˆ—, wâˆ—)
on the one hand and that
inf
wâˆˆW
sup
Î±âˆˆRd
+
G(Î±, w) â‰¥inf
wâˆˆW
G(Î±âˆ—, w) = G(Î±âˆ—, wâˆ—)

4.1.
How to build them
137
on the other hand, proving that
G(Î±âˆ—, wâˆ—) = inf
wâˆˆW
sup
Î±âˆˆRN
+
G(Î±, w)
as required.
Construction of wâˆ—.
â€¢ Let us put wâˆ—= wÎ±âˆ—.
â€¢ If there is j âˆˆ{1, . . . , N} such that 0 < Î±âˆ—
j < 1, let us put
bâˆ—= âŸ¨xj, wâˆ—âŸ©âˆ’yj.
Otherwise, let us put
bâˆ—= sup{âŸ¨xi, wâˆ—âŸ©âˆ’1 : Î±âˆ—
i > 0, yi = +1, i = 1, . . . , N}.
â€¢ Let us then put
Î³âˆ—
i =

0,
Î±âˆ—
i < 1,
1 âˆ’(âŸ¨wâˆ—, xiâŸ©âˆ’bâˆ—)yi,
Î±âˆ—
i = 1.
If we can prove that
(4.2)
1 âˆ’(âŸ¨wâˆ—, xiâŸ©âˆ’bâˆ—)yi
âŽ§
âŽª
âŽ¨
âŽª
âŽ©
â‰¤0,
Î±âˆ—
i = 0,
= 0,
0 < Î±âˆ—
i < 1,
â‰¥0,
Î±âˆ—
i = 1,
it will show that Î³âˆ—âˆˆRN
+ and therefore that wâˆ—= (wâˆ—, bâˆ—, Î³âˆ—) âˆˆW. It will also
show that
G(Î±, wâˆ—) =
N

i=1
Î³âˆ—
i +

i,Î±âˆ—
i =0
Î±i

1 âˆ’(âŸ¨wâˆ—, xiâŸ©âˆ’bâˆ—)yi

+ 1
2âˆ¥wâˆ—âˆ¥2,
proving that G(Î±âˆ—, wâˆ—) = supÎ±âˆˆRN
+ G(Î±, wâˆ—). As obviously G(Î±âˆ—, wâˆ—) = G

Î±âˆ—, (wâˆ—,
0, 0)

, we already know that G(Î±âˆ—, wâˆ—) = infwâˆˆW G(Î±âˆ—, w). This will show that
(Î±âˆ—, wâˆ—) is the saddle point we were looking for, thus ending the proof of the theo-
rem. â–¡
Proof of equation (4.2). Let us deal ï¬rst with the case when there is j âˆˆ
{1, . . . , N} such that 0 < Î±âˆ—
j < 1.
For any i âˆˆ{1, . . . , N} such that 0 < Î±âˆ—
i < 1, there is Ïµ > 0 such that for any
t âˆˆ(âˆ’Ïµ, Ïµ), Î±âˆ—+tyiei âˆ’tyjej âˆˆAâ€², where (ek)N
k=1 is the canonical base of RN. Thus
âˆ‚
âˆ‚t |t=0F(Î±âˆ—+ tyiei âˆ’tyjej) = 0. Computing this derivative, we obtain
âˆ‚
âˆ‚t |t=0F(Î±âˆ—+ tyiei âˆ’tyjej) = yi âˆ’âŸ¨wâˆ—, xiâŸ©+ âŸ¨wâˆ—, xjâŸ©âˆ’yj
= yi

1 âˆ’

âŸ¨w, xiâŸ©âˆ’bâˆ—
yi

.
Thus 1 âˆ’

âŸ¨w, xiâŸ©âˆ’bâˆ—
yi = 0, as required. This shows also that the deï¬nition of bâˆ—
does not depend on the choice of j such that 0 < Î±âˆ—
j < 1.

138
Chapter 4.
Support Vector Machines
For any i âˆˆ{1, . . . , N} such that Î±âˆ—
i = 0, there is Ïµ > 0 such that for any
t âˆˆ(0, Ïµ), Î±âˆ—+ tei âˆ’tyiyjej âˆˆAâ€². Thus
âˆ‚
âˆ‚t |t=0F(Î±âˆ—+ tei âˆ’tyiyjej) â‰¤0, showing
that 1 âˆ’

âŸ¨wâˆ—, xiâŸ©âˆ’bâˆ—
yi â‰¤0 as required.
For any i âˆˆ{1, . . . , N} such that Î±âˆ—
i = 1, there is Ïµ > 0 such that Î±âˆ—âˆ’tei +
tyiyjej âˆˆAâ€². Thus
âˆ‚
âˆ‚t |t=0F(Î±âˆ—âˆ’tei + tyiyjej) â‰¤0, showing that 1 âˆ’

âŸ¨wâˆ—, xiâŸ©âˆ’
bâˆ—
yi â‰¥0 as required. This shows that (Î±âˆ—, wâˆ—) is a saddle point in this case.
Let us deal now with the case where Î±âˆ—âˆˆ{0, 1}N. If we are not in the trivial case
where the vector (yi)N
i=1 is constant, the case Î±âˆ—= 0 is ruled out. Indeed, in this
case, considering Î±âˆ—+ tei + tej, where yiyj = âˆ’1, we would get the contradiction
2 = âˆ‚
âˆ‚t |t=0F(Î±âˆ—+ tei + tej) â‰¤0.
Thus there are values of j such that Î±âˆ—
j = 1, and since N
i=1 Î±iyi = 0, both
classes are present in the set {j : Î±âˆ—
j = 1}.
Now for any i, j âˆˆ{1, . . . , N} such that Î±âˆ—
i = Î±âˆ—
j = 1 and such that yi = +1 and
yj = âˆ’1,
âˆ‚
âˆ‚t |t=0F(Î±âˆ—âˆ’tei âˆ’tej) = âˆ’2 + âŸ¨wâˆ—, xiâŸ©âˆ’âŸ¨wâˆ—, xjâŸ©â‰¤0. Thus
sup{âŸ¨wâˆ—, xiâŸ©âˆ’1 : Î±âˆ—
i = 1, yi = +1} â‰¤inf{âŸ¨wâˆ—, xjâŸ©+ 1 : Î±âˆ—
j = 1, yj = âˆ’1},
showing that
1 âˆ’

âŸ¨wâˆ—, xkâŸ©âˆ’bâˆ—
yk â‰¥0, Î±âˆ—
k = 1.
Finally, for any i such that Î±âˆ—
i = 0, for any j such that Î±âˆ—
j = 1 and yj = yi, we have
âˆ‚
âˆ‚t |t=0F(Î±âˆ—+ tei âˆ’tej) = yiâŸ¨wâˆ—, xi âˆ’xjâŸ©â‰¤0,
showing that 1 âˆ’

âŸ¨wâˆ—, xiâŸ©âˆ’bâˆ—
yi â‰¤0. This shows that (Î±âˆ—, wâˆ—) is always a saddle
point.
4.1.5. Support Vector Machines.
Definition 4.1.3. The symmetric measurable kernel K : X Ã— X â†’R is said to be
positive (or more precisely positive semi-deï¬nite) if for any n âˆˆN, any (xi)n
i=1 âˆˆXn,
inf
Î±âˆˆRn
n

i=1
n

j=1
Î±iK(xi, xj)Î±j â‰¥0.
Let Z = (xi, yi)N
i=1 be some training set. Let us consider as previously
A =

Î± âˆˆRN
+ :
N

i=1
Î±iyi = 0

.
Let
F(Î±) =
N

i=1
N

j=1
Î±iyiK(xi, xj)yjÎ±j âˆ’2
N

i=1
Î±i.
Definition 4.1.4. Let K be a positive symmetric kernel. The training set Z is
said to be K-separable if
inf

F(Î±) : Î± âˆˆA

> âˆ’âˆž.

4.1.
How to build them
139
Lemma 4.1.7. When Z is K-separable, inf{F(Î±) : Î± âˆˆA} is reached.
Proof. Consider the training set Zâ€² = (xâ€²
i, yi)N
i=1, where
xâ€²
i =

K(xk, xâ„“)
N
N
k=1,â„“=1
1/2
(i, j)
N
j=1
âˆˆRN.
We see that F(Î±) = âˆ¥N
i=1 Î±iyixâ€²
iâˆ¥2 âˆ’2 N
i=1 Î±i. We proved in the previous section
that Zâ€² is linearly separable if and only if inf{F(Î±) : Î± âˆˆA} > âˆ’âˆž, and that the
inï¬mum is reached in this case. â–¡
Proposition 4.1.8.
Let K be a symmetric positive kernel and let Z = (xi, yi)N
i=1
be some K-separable training set. Let Î±âˆ—âˆˆA be such that F(Î±âˆ—) = inf{F(Î±) : Î± âˆˆ
A}. Let
Iâˆ—
âˆ’= {i âˆˆN : 1 â‰¤i â‰¤N, yi = âˆ’1, Î±âˆ—
i > 0}
Iâˆ—
+ = {i âˆˆN : 1 â‰¤i â‰¤N, yi = +1, Î±âˆ—
i > 0}
bâˆ—= 1
2
 N

j=1
Î±âˆ—
jyjK(xj, xiâˆ’) +
N

j=1
Î±âˆ—
jyjK(xj, xi+)

,
iâˆ’âˆˆIâˆ—
âˆ’, i+ âˆˆIâˆ—
+,
where the value of bâˆ—does not depend on the choice of iâˆ’and i+. The classiï¬cation
rule f : X â†’Y deï¬ned by the formula
f(x) = sign
" N

i=1
Î±âˆ—
i yiK(xi, x) âˆ’bâˆ—
#
is independent of the choice of Î±âˆ—and is called the support vector machine deï¬ned
by K and Z. The set S = {xj : N
i=1 Î±âˆ—
i yiK(xi, xj) âˆ’bâˆ—= yj} is called the set of
support vectors. For any choice of Î±âˆ—, {xi : Î±âˆ—
i > 0} âŠ‚S.
An important consequence of this proposition is that the support vector machine
deï¬ned by K and Z is also the support vector machine deï¬ned by K and Zâ€² =
{(xi, yi) : Î±âˆ—
i > 0, 1 â‰¤i â‰¤N}, since this restriction of the index set contains the
value Î±âˆ—where the minimum of F is reached.
Proof. The independence of the choice of Î±âˆ—, which is not necessarily unique,
is seen as follows. Let (xi)N
i=1 and x âˆˆX be ï¬xed. Let us put for ease of notation
xN+1 = x. Let M be the (N + 1) Ã— (N + 1) symmetric semi-deï¬nite matrix deï¬ned
by M(i, j) = K(xi, xj), i = 1, . . . , N + 1, j = 1, . . . , N + 1. Let us consider the
mapping Î¨ : {xi : i = 1, . . . , N + 1} â†’RN+1 deï¬ned by
(4.3)
Î¨(xi) =

M 1/2(i, j)
N+1
j=1 âˆˆRN+1.
Let us consider the training set Zâ€² =

Î¨(xi), yi
N
i=1. Then Zâ€² is linearly separable,
F(Î±) =
CCC
N

i=1
Î±iyiÎ¨(xi)
CCC
2
âˆ’2
N

i=1
Î±i,
and we have proved that for any choice of Î±âˆ—
âˆˆ
A minimizing F(Î±),
wZâ€² = N
i=1 Î±âˆ—
i yiÎ¨(xi). Thus the support vector machine deï¬ned by K and Z
can also be expressed by the formula
f(x) = sign

âŸ¨wZâ€², Î¨(x)âŸ©âˆ’bZâ€²

140
Chapter 4.
Support Vector Machines
which does not depend on Î±âˆ—. The deï¬nition of S is such that Î¨(S) is the set of
support vectors deï¬ned in the linear case, where its stated property has already
been proved. â–¡
We can in the same way use the box constraint and show that any solution
Î±âˆ—âˆˆarg min{F(Î±) : Î± âˆˆA, Î±i â‰¤Î»2, i = 1, . . . , N} minimizes
(4.4)
inf
bâˆˆR Î»2
N

i=1

1 âˆ’
 N

j=1
yjÎ±jK(xj, xi) âˆ’b
!
yi

+
+ 1
2
N

i=1
N

j=1
Î±iÎ±jyiyjK(xi, xj).
4.1.6. Building kernels.
Except the last, the results of this section are drawn
from Cristianini et al. (2000). We have no reference for the last proposition of this
section, although we believe it is well known. We include them for the convenience
of the reader.
Proposition 4.1.9. Let K1 and K2 be positive symmetric kernels on X. Then for
any a âˆˆR+
(aK1 + K2)(x, xâ€²)
def
= aK1(x, xâ€²) + K2(x, xâ€²)
and (K1 Â· K2)(x, xâ€²)
def
= K1(x, xâ€²)K2(x, xâ€²)
are also positive symmetric kernels. Moreover, for any measurable function
g : X â†’R, Kg(x, xâ€²)
def
= g(x)g(xâ€²) is also a positive symmetric kernel.
Proof. It is enough to prove the proposition in the case when X is ï¬nite and
kernels are just ordinary symmetric matrices. Thus we can assume without loss of
generality that X = {1, . . . , n}. Then for any Î± âˆˆRN, using usual matrix notation,
âŸ¨Î±, (aK1 + K2)Î±âŸ©= aâŸ¨Î±, K1Î±âŸ©+ âŸ¨Î±, K2Î±âŸ©â‰¥0,
âŸ¨Î±, (K1 Â· K2)Î±âŸ©=

i,j
Î±iK1(i, j)K2(i, j)Î±j
=

i,j,k
Î±iK1/2
1
(i, k)K1/2
1
(k, j)K2(i, j)Î±j
=

k

i,j

K1/2
1
(k, i)Î±i

K2(i, j)

K1/2
1
(k, j)Î±j

5
67
8
â‰¥0
â‰¥0,
âŸ¨Î±, KgÎ±âŸ©=

i,j
Î±ig(i)g(j)Î±j =
"
i
Î±ig(i)
#2
â‰¥0.
â–¡
Proposition 4.1.10. Let K be some positive symmetric kernel on X. Let p : R â†’
R be a polynomial with positive coeï¬ƒcients. Let g : X â†’Rd be a measurable func-
tion. Then
p(K)(x, xâ€²)
def
= p

K(x, xâ€²)

,

4.1.
How to build them
141
exp(K)(x, xâ€²)
def
= exp

K(x, xâ€²)

and Gg(x, xâ€²)
def
= exp

âˆ’âˆ¥g(x) âˆ’g(xâ€²)âˆ¥2
are all positive symmetric kernels.
Proof. The ï¬rst assertion is a direct consequence of the previous proposition. The
second comes from the fact that the exponential function is the pointwise limit of a
sequence of polynomial functions with positive coeï¬ƒcients. The third is seen from
the second and the decomposition
Gg(x, xâ€²) =

exp

âˆ’âˆ¥g(x)âˆ¥2
exp

âˆ’âˆ¥g(xâ€²)âˆ¥2
exp

2âŸ¨g(x), g(xâ€²)âŸ©

â–¡
Proposition 4.1.11. With the notation of the previous proposition, any training
set Z = (xi, yi)N
i=1 âˆˆ

XÃ—{âˆ’1, +1}
N is Gg-separable as soon as g(xi), i = 1, . . . , N
are distinct points of Rd.
Proof. It is clearly enough to prove the case when X = Rd and g is the identity.
Let us consider some other generic point xN+1 âˆˆRd and deï¬ne Î¨ as in (4.3). It is
enough to prove that Î¨(x1), . . . , Î¨(xN) are aï¬ƒne independent, since the simplex,
and therefore any aï¬ƒne independent set of points, can be split in any arbitrary way
by aï¬ƒne half-spaces. Let us assume that (x1, . . . , xN) are aï¬ƒne dependent; then
for some (Î»1, . . . , Î»N) Ì¸= 0 such that N
i=1 Î»i = 0,
N

i=1
N

j=1
Î»iG(xi, xj)Î»j = 0.
Thus, (Î»i)N+1
i=1 , where we have put Î»N+1 = 0 is in the kernel of the symmetric
positive semi-deï¬nite matrix G(xi, xj)i,jâˆˆ{1,...,N+1}. Therefore
N

i=1
Î»iG(xi, xN+1) = 0,
for any xN+1 âˆˆRd. This would mean that the functions x â†’exp(âˆ’âˆ¥x âˆ’xiâˆ¥2)
are linearly dependent, which can be easily proved to be false. Indeed, let n âˆˆRd
be such that âˆ¥nâˆ¥= 1 and âŸ¨n, xiâŸ©, i = 1, . . . , N are distinct (such a vector exists,
because it has to be outside the union of a ï¬nite number of hyperplanes, which is
of zero Lebesgue measure on the sphere). Let us assume for a while that for some
(Î»i)N
i=1 âˆˆRN, for any x âˆˆRd,
N

i=1
Î»i exp(âˆ’âˆ¥x âˆ’xiâˆ¥2) = 0.
Considering x = tn, for t âˆˆR, we would get
N

i=1
Î»i exp(2tâŸ¨n, xiâŸ©âˆ’âˆ¥xiâˆ¥2) = 0,
t âˆˆR.
Letting t go to inï¬nity, we see that this is only possible if Î»i = 0 for all values of i.
â–¡

142
Chapter 4.
Support Vector Machines
4.2. Bounds for Support Vector Machines
4.2.1. Compression scheme bounds.
We can use Support Vector Machines
in the framework of compression schemes and apply Theorem 3.3.3 (page 125).
More precisely, given some positive symmetric kernel K on X, we may consider
for any training set Zâ€² = (xâ€²
i, yâ€²
i)h
i=1 the classiï¬er Ë†fZâ€² : X â†’Y which is equal to
the Support Vector Machine deï¬ned by K and Zâ€² whenever Zâ€² is K-separable,
and which is equal to some constant classiï¬cation rule otherwise; we take this
convention to stick to the framework described on page 117, we will only use Ë†fZâ€²
in the K-separable case, so this extension of the deï¬nition is just a matter of
presentation. In the application of Theorem 3.3.3 in the case when the observed
sample (Xi, Yi)N
i=1 is K-separable, a natural if perhaps sub-optimal choice of Zâ€²
is to choose for (xâ€²
i) the set of support vectors deï¬ned by Z = (Xi, Yi)N
i=1 and to
choose for (yâ€²
i) the corresponding values of Y . This is justiï¬ed by the fact that
Ë†fZ = Ë†fZâ€², as shown in Proposition 4.1.8 (page 139). If Z is not K-separable, we
can train a Support Vector Machine with the box constraint, then remove all the
errors to obtain a K-separable sub-sample Zâ€² = {(Xi, Yi) : Î±âˆ—
i < Î»2, 1 â‰¤i â‰¤N},
using the same notation as in equation (4.4) on page 140, and then consider its
support vectors as the compression set. Still using the notation of page 140, this
means we have to compute successively Î±âˆ—âˆˆarg min{F(Î±) : Î± âˆˆA, Î±i â‰¤Î»2}, and
Î±âˆ—âˆ—âˆˆarg min{F(Î±) : Î± âˆˆA, Î±i = 0 when Î±âˆ—
i = Î»2}, to keep the compression set
indexed by J = {i : 1 â‰¤i â‰¤N, Î±âˆ—âˆ—
i
> 0}, and the corresponding Support Vector
Machine 	fJ. Diï¬€erent values of Î» can be used at this stage, producing diï¬€erent
candidate compression sets: when Î» increases, the number of errors should decrease,
on the other hand when Î» decreases, the margin âˆ¥wâˆ¥âˆ’1 of the separable subset Zâ€²
increases, supporting the hope for a smaller set of support vectors, thus we can use Î»
to monitor the number of errors on the training set we accept from the compression
scheme. As we can use whatever heuristic we want while selecting the compression
set, we can also try to threshold in the previous construction Î±âˆ—âˆ—
i
at diï¬€erent levels
Î· â‰¥0, to produce candidate compression sets JÎ· = {i : 1 â‰¤i â‰¤N, Î±âˆ—âˆ—
i
> Î·} of
various sizes.
As the size |J| of the compression set is random in this construction, we must
use a version of Theorem 3.3.3 (page 125) which handles compression sets of arbi-
trary sizes. This is done by choosing for each k a k-partially exchangeable posterior
distribution Ï€k which weights the compression sets of all dimensions. We immedi-
ately see that we can choose Ï€k such that âˆ’log

Ï€k(Î”k(J))

â‰¤log

|J|(|J| + 1)

+
|J| log

(k+1)eN
|J|

.
If we observe the shadow sample patterns, and if computer resources permit, we
can of course use more elaborate bounds than Theorem 3.3.3, such as the transduc-
tive equivalent for Theorem 1.3.15 (page 30) (where we may consider the submod-
els made of all the compression sets of the same size). Theorems based on relative
bounds, such as Theorem 2.2.4 (page 72) or Theorem 2.3.9 (page 107) can also be
used. Gibbs distributions can be approximated by Monte Carlo techniques, where
a Markov chain with the proper invariant measure consists in appropriate local
perturbations of the compression set.
Let us mention also that the use of compression schemes based on Support Vector
Machines can be tailored to perform some kind of feature aggregation. Imagine that
the kernel K is deï¬ned as the scalar product in L2(Ï€), where Ï€ âˆˆM1
+(Î˜). More
precisely let us consider for some set of soft classiï¬cation rules

fÎ¸ : X â†’R ; Î¸ âˆˆÎ˜


4.2.
Bounds for Support Vector Machines
143
the kernel
K(x, xâ€²) =

Î¸âˆˆÎ˜
fÎ¸(x)fÎ¸(xâ€²)Ï€(dÎ¸).
In this setting, the Support Vector Machine applied to the training set Z = (xi,
yi)N
i=1 has the form
fZ(x) = sign
"
Î¸âˆˆÎ˜
fÎ¸(x)
N

i=1
yiÎ±ifÎ¸(xi)Ï€(dÎ¸) âˆ’b
#
and, if this is too burdensome to compute, we can replace it with some ï¬nite
approximation
fZ(x) = sign
"
1
m
m

k=1
fÎ¸k(x)wk âˆ’b
#
,
where the set {Î¸k, k = 1, . . . , m} and the weights {wk, k = 1, . . . , m} are computed
in some suitable way from the set Zâ€² = (xi, yi)i,Î±i>0 of support vectors of fZ. For
instance, we can draw {Î¸k, k = 1, . . . , m} at random according to the probability
distribution proportional to
22222
N

i=1
yiÎ±ifÎ¸(xi)
22222 Ï€(dÎ¸),
deï¬ne the weights wk by
wk = sign
" N

i=1
yiÎ±ifÎ¸k(xi)
# 
Î¸âˆˆÎ˜
22222
N

i=1
yiÎ±ifÎ¸(xi)
22222 Ï€(dÎ¸),
and choose the smallest value of m for which this approximation still classiï¬es Zâ€²
without errors. Let us remark that we have built fZ in such a way that
lim
mâ†’+âˆž
fZ(xi) = fZ(xi) = yi,
a.s.
for any support index i such that Î±i > 0.
Alternatively, given Zâ€², we can select a ï¬nite set of features Î˜â€² âŠ‚Î˜ such that Zâ€²
is KÎ˜â€² separable, where KÎ˜â€²(x, xâ€²) = 
Î¸âˆˆÎ˜â€² fÎ¸(x)fÎ¸(xâ€²) and consider the Support
Vector Machines fZâ€² built with the kernel KÎ˜â€². As soon as Î˜â€² is chosen as a function
of Zâ€² only, Theorem 3.3.3 (page 125) applies and provides some level of conï¬dence
for the risk of fZâ€².
4.2.2. The Vapnikâ€“Cervonenkis dimension of a family of subsets.
Let
us consider some set X and some set S âŠ‚{0, 1}X of subsets of X. Let h(S) be the
Vapnikâ€“Cervonenkis dimension of S, deï¬ned as
h(S) = max

|A| : A âŠ‚X, |A| < âˆžand A âˆ©S = {0, 1}A
,
where by deï¬nition A âˆ©S = {A âˆ©B : B âˆˆS} and |A| is the number of points in A.
Let us notice that this deï¬nition does not depend on the choice of the reference set
X. Indeed X can be chosen to be  S, the union of all the sets in S or any bigger
set. Let us notice also that for any set B, h(B âˆ©S) â‰¤h(S), the reason being that
A âˆ©(B âˆ©S) = B âˆ©(A âˆ©S).

144
Chapter 4.
Support Vector Machines
This notion of Vapnikâ€“Cervonenkis dimension is useful because, as we will see
for Support Vector Machines, it can be computed in some important special cases.
Let us prove here as an illustration that h(S) = d + 1 when X = Rd and S is made
of all the half spaces:
S = {Aw,b : w âˆˆRd, b âˆˆR}, where Aw,b = {x âˆˆX : âŸ¨w, xâŸ©â‰¥b}.
Proposition 4.2.1. With the previous notation, h(S) = d + 1.
Proof. Let (ei)d+1
i=1 be the canonical base of Rd+1, and let X be the aï¬ƒne subspace
it generates, which can be identiï¬ed with Rd. For any (Ïµi)d+1
i=1 âˆˆ{âˆ’1, +1}d+1,
let w = d+1
i=1 Ïµiei and b = 0. The half space Aw,b âˆ©X is such that {ei ; i =
1, . . . , d + 1} âˆ©(Aw,b âˆ©X) = {ei ; Ïµi = +1}. This proves that h(S) â‰¥d + 1.
To prove that h(S) â‰¤d + 1, we have to show that for any set A âŠ‚Rd of size
|A| = d+2, there is B âŠ‚A such that B Ì¸âˆˆ(Aâˆ©S). Obviously this will be the case if
the convex hulls of B and A\B have a non-empty intersection: indeed if a hyperplane
separates two sets of points, it also separates their convex hulls. As |A| > d + 1,
A is aï¬ƒne dependent: there is (Î»x)xâˆˆA âˆˆRd+2 \ {0} such that 
xâˆˆA Î»xx = 0 and

xâˆˆA Î»x = 0. The set B = {x âˆˆA : Î»x > 0} and its complement A \ B are non-
empty, because 
xâˆˆA Î»x = 0 and Î» Ì¸= 0. Moreover 
xâˆˆB Î»x = 
xâˆˆA\B âˆ’Î»x > 0.
The relation
1

xâˆˆB Î»x

xâˆˆB
Î»xx =
1

xâˆˆB Î»x

xâˆˆA\B
âˆ’Î»xx
shows that the convex hulls of B and A \ B have a non-void intersection. â–¡
Let us introduce the function of two integers
Î¦h
n =
h

k=0
 n
k
!
,
which can alternatively be deï¬ned by the relations
Î¦h
n =

2n
when n â‰¤h,
Î¦hâˆ’1
nâˆ’1 + Î¦h
nâˆ’1
when n > h.
Theorem 4.2.2.
Whenever  S is ï¬nite,
|S| â‰¤Î¦
222
D
S
222 , h(S)

.
Theorem 4.2.3.
For any h â‰¤n,
Î¦h
n â‰¤exp

nH

 h
n

â‰¤exp

h

log( n
h) + 1

,
where H(p) = âˆ’p log(p)âˆ’(1âˆ’p) log(1âˆ’p) is the Shannon entropy of the Bernoulli
distribution with parameter p.
Proof of theorem 4.2.2. Let us prove this theorem by induction on | S|. It
is easy to check that it holds true when | S| = 1. Let X =  S, let x âˆˆX and
Xâ€² = X \ {x}. Deï¬ne (â–³denoting the symmetric diï¬€erence of two sets)
Sâ€² = {A âˆˆS : A â–³{x} âˆˆS},
Sâ€²â€² = {A âˆˆS : A â–³{x} Ì¸âˆˆS}.

4.2.
Bounds for Support Vector Machines
145
Clearly, âŠ”denoting the disjoint union, S = Sâ€²âŠ”Sâ€²â€² and Sâˆ©Xâ€² = (Sâ€²âˆ©Xâ€²)âŠ”(Sâ€²â€²âˆ©Xâ€²).
Moreover |Sâ€²| = 2|Sâ€² âˆ©Xâ€²| and |Sâ€²â€²| = |Sâ€²â€² âˆ©Xâ€²|. Thus
|S| = |Sâ€²| + |Sâ€²â€²| = 2|Sâ€² âˆ©Xâ€²| + |Sâ€²â€²| = |S âˆ©Xâ€²| + |Sâ€² âˆ©Xâ€²|.
Obviously h(S âˆ©Xâ€²) â‰¤h(S). Moreover h(Sâ€² âˆ©Xâ€²) = h(Sâ€²)âˆ’1, because if A âŠ‚Xâ€² is
shattered by Sâ€² (or equivalently by Sâ€² âˆ©Xâ€²), then Aâˆª{x} is shattered by Sâ€² (we say
that A is shattered by S when Aâˆ©S = {0, 1}A). Using the induction hypothesis, we
then see that |S âˆ©Xâ€²| â‰¤Î¦h(S)
|Xâ€²| +Î¦h(S)âˆ’1
|Xâ€²|
. But as |Xâ€²| = |X|âˆ’1, the right-hand side
of this inequality is equal to Î¦h(S)
|X| , according to the recurrence equation satisï¬ed
by Î¦.
Proof of theorem 4.2.3: This is the well-known Chernoï¬€bound for the
deviation of sums of Bernoulli random variables: let (Ïƒ1, . . . , Ïƒn) be i.i.d. Bernoulli
random variables with parameter 1/2. Let us notice that
Î¦h
n = 2nP
" n

i=1
Ïƒi â‰¤h
#
.
For any positive real number Î» ,
P
 n

i=1
Ïƒi â‰¤h
!
â‰¤exp(Î»h)E

exp
 
âˆ’Î»
n

i=1
Ïƒi
!
= exp

Î»h + n log

E

exp

âˆ’Î»Ïƒ1

.
Diï¬€erentiating the right-hand side in Î» shows that its minimal value is
exp

âˆ’nK( h
n, 1
2)

, where K(p, q) = p log( p
q ) + (1 âˆ’p) log( 1âˆ’p
1âˆ’q ) is the Kullback diver-
gence function between two Bernoulli distributions Bp and Bq of parameters p and
q. Indeed the optimal value Î»âˆ—of Î» is such that
h = nE

Ïƒ1 exp(âˆ’Î»âˆ—Ïƒ1)

E

exp(âˆ’Î»âˆ—Ïƒ1)

= nBh/n(Ïƒ1).
Therefore, using the fact that two Bernoulli distributions with the same expecta-
tions are equal,
log

E

exp(âˆ’Î»âˆ—Ïƒ1)

= âˆ’Î»âˆ—Bh/n(Ïƒ1) âˆ’K(Bh/n, B1/2) = âˆ’Î»âˆ—h
n âˆ’K( h
n, 1
2).
The announced result then follows from the identity
H(p) = log(2) âˆ’K(p, 1
2)
= p log(pâˆ’1) + (1 âˆ’p) log(1 +
p
1 âˆ’p) â‰¤p

log(pâˆ’1) + 1

.
4.2.3. Vapnikâ€“Cervonenkis dimension of linear rules with margin.
The
proof of the following theorem was suggested to us by a similar proof presented in
Cristianini et al. (2000).
Theorem 4.2.4.
Consider a family of points (x1, . . . , xn) in some Euclidean vec-
tor space E and a family of aï¬ƒne functions
H =

gw,b : E â†’R ; w âˆˆE, âˆ¥wâˆ¥= 1, b âˆˆR

,

146
Chapter 4.
Support Vector Machines
where
gw,b(x) = âŸ¨w, xâŸ©âˆ’b,
x âˆˆE.
Assume that there is a set of thresholds (bi)n
i=1 âˆˆRn such that for any
(yi)n
i=1 âˆˆ{âˆ’1, +1}n, there is gw,b âˆˆH such that
n
inf
i=1

gw,b(xi) âˆ’bi

yi â‰¥Î³.
Let us also introduce the empirical variance of (xi)n
i=1,
Var(x1, . . . , xn) = 1
n
n

i=1
CCCCxi âˆ’1
n
n

j=1
xj
CCCC
2
.
In this case and with this notation,
(4.5)
Var(x1, . . . , xn)
Î³2
â‰¥

n âˆ’1
when n is even,
(n âˆ’1) n2âˆ’1
n2
when n is odd.
Moreover, equality is reached when Î³ is optimal, bi = 0, i = 1, . . . , n and (x1, . . . ,
xn) is a regular simplex (i.e. when 2Î³ is the minimum distance between the convex
hulls of any two subsets of {x1, . . . , xn} and âˆ¥xi âˆ’xjâˆ¥does not depend on i Ì¸= j).
Proof. Let (si)n
i=1 âˆˆRn be such that n
i=1 si = 0. Let Ïƒ be a uniformly distributed
random variable with values in Sn, the set of permutations of the ï¬rst n integers
{1, . . . , n}. By assumption, for any value of Ïƒ, there is an aï¬ƒne function gw,b âˆˆH
such that
min
i=1,...,n

gw,b(xi) âˆ’bi

21(sÏƒ(i) > 0) âˆ’1

â‰¥Î³.
As a consequence
E n

i=1
sÏƒ(i)xi, w
F
=
n

i=1
sÏƒ(i)

âŸ¨xi, wâŸ©âˆ’b âˆ’bi

+
n

i=1
sÏƒ(i)bi
â‰¥
n

i=1
Î³|sÏƒ(i)| + sÏƒ(i)bi.
Therefore, using the fact that the map x â†’

max

0, x
2
is convex,
E
"CCCC
n

i=1
sÏƒ(i)xi
CCCC
2#
â‰¥E
âŽ¡
âŽ£
"
max

0,
n

i=1
Î³|sÏƒ(i)| + sÏƒ(i)bi
#2âŽ¤
âŽ¦
â‰¥
"
max

0,
n

i=1
Î³E

|sÏƒ(i)|

+ E

sÏƒ(i)

bi
#2
= Î³2
" n

i=1
|si|
#2
,
where E is the expectation with respect to the random permutation Ïƒ. On the other
hand
E
"CCCC
n

i=1
sÏƒ(i)xi
CCCC
2#
=
n

i=1
E(s2
Ïƒ(i))âˆ¥xiâˆ¥2 +

iÌ¸=j
E(sÏƒ(i)sÏƒ(j))âŸ¨xi, xjâŸ©.

4.2.
Bounds for Support Vector Machines
147
Moreover
E(s2
Ïƒ(i)) = 1
nE
" n

i=1
s2
Ïƒ(i)
#
= 1
n
n

i=1
s2
i .
In the same way, for any i Ì¸= j,
E

sÏƒ(i)sÏƒ(j)

=
1
n(n âˆ’1)E
âŽ›
âŽ
iÌ¸=j
sÏƒ(i)sÏƒ(j)
âŽž
âŽ 
=
1
n(n âˆ’1)

iÌ¸=j
sisj
=
1
n(n âˆ’1)
&" n

i=1
si
5 67 8
=0
#2
âˆ’
n

i=1
s2
i
'
= âˆ’
1
n(n âˆ’1)
n

i=1
s2
i .
Thus
E
"CCCC
n

i=1
sÏƒ(i)xi
CCCC
2#
=
" n

i=1
s2
i
# âŽ¡
âŽ£1
n
n

i=1
âˆ¥xiâˆ¥2 âˆ’
1
n(n âˆ’1)

iÌ¸=j
âŸ¨xi, xjâŸ©
âŽ¤
âŽ¦
=
" n

i=1
s2
i
# & 1
n +
1
n(n âˆ’1)
!
n

i=1
âˆ¥xiâˆ¥2
âˆ’
1
n(n âˆ’1)
CCCC
n

i=1
xi
CCCC
2'
=
n
n âˆ’1
" n

i=1
s2
i
#
Var(x1, . . . , xn).
We have proved that
Var(x1, . . . , xn)
Î³2
â‰¥
(n âˆ’1)
 n

i=1
|si|
!2
n
n

i=1
s2
i
.
This can be used with si = 1(i â‰¤n
2 ) âˆ’1(i > n
2 ) in the case when n is even and
si =
2
(nâˆ’1)1(i â‰¤nâˆ’1
2 ) âˆ’
2
n+11(i > nâˆ’1
2 ) in the case when n is odd, to establish the
ï¬rst inequality (4.5) of the theorem.
Checking that equality is reached for the simplex is an easy computation when
the simplex (xi)n
i=1 âˆˆ(Rn)n is parametrized in such a way that
xi(j) =

1
if i = j,
0
otherwise.
Indeed the distance between the convex hulls of any two subsets of the simplex is
the distance between their mean values (i.e. centers of mass). â–¡

148
Chapter 4.
Support Vector Machines
4.2.4. Application to Support Vector Machines.
We are going to apply
Theorem 4.2.4 (page 145) to Support Vector Machines in the transductive case.
Let (Xi, Yi)(k+1)N
i=1
be distributed according to some partially exchangeable dis-
tribution P and assume that (Xi)(k+1)N
i=1
and (Yi)N
i=1 are observed. Let us con-
sider some positive kernel K on X. For any K-separable training set of the form
Zâ€² = (Xi, yâ€²
i)(k+1)N
i=1
, where (yâ€²
i)(k+1)N
i=1
âˆˆY(k+1)N, let Ë†fZâ€² be the Support Vector
Machine deï¬ned by K and Zâ€² and let Î³(Zâ€²) be its margin. Let
R2 =
max
i=1,...,(k+1)N K(Xi, Xi) +
1
(k + 1)2N 2
(k+1)N

j=1
(k+1)N

k=1
K(Xj, Xk)
âˆ’
2
(k + 1)N
(k+1)N

j=1
K(Xi, Xj).
This is an easily computable upper-bound for the radius of some ball containing
the image of (X1, . . . , X(k+1)N) in feature space.
Let us deï¬ne for any integer h the margins
(4.6)
Î³2h = (2h âˆ’1)âˆ’1/2
and
Î³2h+1 =

2h
 
1 âˆ’
1
(2h + 1)2
!âˆ’1/2
.
Let us consider for any h = 1, . . . , N the exchangeable model
Rh =
 Ë†fZâ€² : Zâ€² = (Xi, yâ€²
i)(k+1)N
i=1
is K-separable and Î³(Zâ€²) â‰¥RÎ³h

.
The family of models Rh, h = 1, . . . , N is nested, and we know from Theorem 4.2.4
(page 145) and Theorems 4.2.2 (page 144) and 4.2.3 (page 144) that
log

|Rh|

â‰¤h log

 (k+1)eN
h

.
We can then consider on the large model R = +N
h=1 Rh (the disjoint union of the
sub-models) an exchangeable prior Ï€ which is uniform on each Rh and is such that
Ï€(Rh) â‰¥
1
h(h+1). Applying Theorem 3.2.3 (page 116) we get
Proposition 4.2.5. With P probability at least 1 âˆ’Ïµ, for any h = 1, . . . , N, any
Support Vector Machine f âˆˆRh,
r2(f) â‰¤
k + 1
k
inf
Î»âˆˆR+
1 âˆ’exp

âˆ’Î»
N r1(f) âˆ’h
N log

e(k+1)N
h

âˆ’log[h(h+1)]âˆ’log(Ïµ)
N

1 âˆ’exp(âˆ’Î»
N )
âˆ’r1(f)
k
.
Searching the whole model Rh to optimize the bound may require more computer
resources than are available, but any heuristic can be applied to choose f, since the
bound is uniform. For instance, a Support Vector Machine f â€² using a box constraint
can be trained from the training set (Xi, Yi)N
i=1 and then (yâ€²
i)(k+1)N
i=1
can be set to
yâ€²
i = sign(f â€²(Xi)), i = 1, . . . , (k + 1)N.

4.2.
Bounds for Support Vector Machines
149
4.2.5. Inductive margin bounds for Support Vector Machines.
In or-
der to establish inductive margin bounds, we will need a diï¬€erent combinatorial
lemma. It is due to Alon et al. (1997). We will reproduce their proof with some tiny
improvements on the values of constants.
Let us consider the ï¬nite case when X = {1, . . . , n}, Y = {1, . . . , b} and
b â‰¥3. The question we will study would be meaningless when b â‰¤2. Assume
as usual that we are dealing with a prescribed set of classiï¬cation rules R =

f :
X â†’Y

. Let us say that a pair (A, s), where A âŠ‚X is a non-empty set of shapes
and s : A â†’{2, . . . , b âˆ’1} a threshold function, is shattered by the set of func-
tions F âŠ‚R if for any (Ïƒx)xâˆˆA âˆˆ{âˆ’1, +1}A, there exists some f âˆˆF such that
minxâˆˆA Ïƒx

f(x) âˆ’s(x)

â‰¥1.
Definition 4.2.1.
Let the fat shattering dimension of (X, R) be the maximal size
|A| of the ï¬rst component of the pairs which are shattered by R.
Let us say that a subset of classiï¬cation rules F âŠ‚YX is separated whenever for
any pair (f, g) âˆˆF 2 such that f Ì¸= g, âˆ¥f âˆ’gâˆ¥âˆž= maxxâˆˆX|f(x) âˆ’g(x)| â‰¥2. Let
M(R) be the maximum size |F| of separated subsets F of R. Note that if F is a
separated subset of R such that |F| = M(R), then it is a 1-net for the Lâˆždistance:
for any function f âˆˆR there exists g âˆˆF such that âˆ¥f âˆ’gâˆ¥âˆžâ‰¤1 (otherwise f
could be added to F to create a larger separated set).
Lemma 4.2.6.
With the above notation, whenever the fat shattering dimension of
(X, R) is not greater than h,
log

M(R)

< log

(b âˆ’1)(b âˆ’2)n


log
h
i=1

n
i

(b âˆ’2)i
log(2)
+ 1

+ log(2)
â‰¤log

(b âˆ’1)(b âˆ’2)n


log

(bâˆ’2)n
h

+ 1

h
log(2) + 1

+ log(2).
Proof. For any set of functions F âŠ‚YX, let t(F) be the number of pairs (A, s)
shattered by F. Let t(m, n) be the minimum of t(F) over all separated sets of
functions F âŠ‚YX of size |F| = m (n is here to recall that the shape space X is
made of n shapes). For any m such that t(m, n) > h
i=1

n
i

(b âˆ’2)i, it is clear that
any separated set of functions of size |F| â‰¥m shatters at least one pair (A, s) such
that |A| > h. Indeed, from its deï¬nition t(m, n) is clearly a non-decreasing function
of m, so that t(|F|, n) > h
i=1

n
i

(bâˆ’2)i. Moreover there are only h
i=1

n
i

(bâˆ’2)i
pairs (A, s) such that |A| â‰¤h. As a consequence, whenever the fat shattering
dimension of (X, R) is not greater than h we have M(R) < m.
It is clear that for any n â‰¥1, t(2, n) = 1.
Lemma 4.2.7. For any m â‰¥1, t

mn(bâˆ’1)(bâˆ’2), n

â‰¥2t

m, nâˆ’1

, and therefore
t

2n(n âˆ’1) Â· Â· Â· (n âˆ’r + 1)(b âˆ’1)r(b âˆ’2)r, n

â‰¥2r.
Proof. Let F = {f1, . . . , fmn(bâˆ’1)(bâˆ’2)} be some separated set of functions of size
mn(b âˆ’1)(b âˆ’2). For any pair (f2iâˆ’1, f2i), i = 1, . . . , mn(b âˆ’1)(b âˆ’2)/2, there is
xi âˆˆX such that |f2iâˆ’1(xi) âˆ’f2i(xi)| â‰¥2. Since |X| = n, there is x âˆˆX such that
mn(bâˆ’1)(bâˆ’2)/2
i=1
1(xi = x) â‰¥m(b âˆ’1)(b âˆ’2)/2. Let I = {i : xi = x}. Since there
are (b âˆ’1)(b âˆ’2)/2 pairs (y1, y2) âˆˆY2 such that 1 â‰¤y1 < y2 âˆ’1 â‰¤b âˆ’1, there

150
Chapter 4.
Support Vector Machines
is some pair (y1, y2), such that 1 â‰¤y1 < y2 â‰¤b and such that 
iâˆˆI 1({y1, y2} =
{f2iâˆ’1(x), f2i(x)}) â‰¥m. Let J =

i âˆˆI : {f2iâˆ’1(x), f2i(x)} = {y1, y2}

. Let
F1 = {f2iâˆ’1 : i âˆˆJ, f2iâˆ’1(x) = y1} âˆª{f2i : i âˆˆJ, f2i(x) = y1},
F2 = {f2iâˆ’1 : i âˆˆJ, f2iâˆ’1(x) = y2} âˆª{f2i : i âˆˆJ, f2i(x) = y2}.
Obviously |F1| = |F2| = |J| = m. Moreover the restrictions of the functions of F1 to
X\{x} are separated, and it is the same with F2. Thus F1 strongly shatters at least
t(m, n âˆ’1) pairs (A, s) such that A âŠ‚X \ {x} and it is the same with F2. Finally,
if the pair (A, s) where A âŠ‚X \ {x} is both shattered by F1 and F2, then F1 âˆªF2
shatters also (A âˆª{x}, sâ€²) where sâ€²(xâ€²) = s(xâ€²) for any xâ€² âˆˆA and sâ€²(x) = âŒŠy1+y2
2
âŒ‹.
Thus F1 âˆªF2, and therefore F, shatters at least 2t(m, n âˆ’1) pairs (A, s). â–¡
Resuming the proof of lemma 4.2.6, let us choose for r the smallest integer such
that 2r > h
i=1

n
i

(b âˆ’2)i, which is no greater than

log
h
i=1 (
n
i)(bâˆ’2)i
log(2)
+ 1

.
In the case when 1 â‰¤n â‰¤r,
log(M(R)) < |X| log(|Y|) = n log(b) â‰¤r log(b) â‰¤r log

(b âˆ’1)(b âˆ’2)n

+ log(2),
which proves the lemma. In the remaining case n > r,
t

2nr(b âˆ’1)r(b âˆ’2)r, n

â‰¥t

2n(n âˆ’1) . . . (n âˆ’r + 1)(b âˆ’1)r(b âˆ’2)r, n

>
h

i=1
 n
i
!
(b âˆ’2)i.
Thus |M(R)| < 2

(b âˆ’2)(b âˆ’1)n
r
as claimed. â–¡
In order to apply this combinatorial lemma to Support Vector Machines, let us
consider now the case of separating hyperplanes in Rd (the generalization to Support
Vector Machines being straightforward). Assume that X = Rd and Y = {âˆ’1, +1}.
For any sample (X)(k+1)N
i=1
, let
R(X(k+1)N
1
) = max{âˆ¥Xiâˆ¥: 1 â‰¤i â‰¤(k + 1)N}.
Let us consider the set of parameters
Î˜ =

(w, b) âˆˆRd Ã— R : âˆ¥wâˆ¥= 1

.
For any (w, b) âˆˆÎ˜, let gw,b(x) = âŸ¨w, xâŸ©âˆ’b. Let h be some ï¬xed integer and let
Î³ = R(X(k+1)N
1
)Î³h, where Î³h is deï¬ned by equation (4.6, page 148).
Let us deï¬ne Î¶ : R â†’Z by
Î¶(r) =
âŽ§
âŽª
âŽª
âŽª
âŽª
âŽª
âŽª
âŽª
âŽª
âŽ¨
âŽª
âŽª
âŽª
âŽª
âŽª
âŽª
âŽª
âŽª
âŽ©
âˆ’5
when
r â‰¤âˆ’4Î³,
âˆ’3
when
âˆ’4Î³ <r â‰¤âˆ’2Î³,
âˆ’1
when
âˆ’2Î³ <r â‰¤0,
+1
when
0 <r â‰¤2Î³,
+3
when
2Î³ <r â‰¤4Î³,
+5
when
4Î³ <r.

4.2.
Bounds for Support Vector Machines
151
Let Gw,b(x) = Î¶

gw,b(x)

. The fat shattering dimension (as deï¬ned in 4.2.1) of

X(k+1)N
1
,

(Gw,b + 7)/2 : (w, b) âˆˆÎ˜

is not greater than h (according to Theorem 4.2.4, page 145), therefore there is
some set F of functions from X(k+1)N
1
to {âˆ’5, âˆ’3, âˆ’1, +1, +3, +5} such that
log

|F|

â‰¤log

20(k + 1)N


h
log(2)

log
 4(k + 1)N
h
!
+ 1

+ 1

+ log(2).
and for any (w, b) âˆˆÎ˜, there is fw,b âˆˆF such that sup

|fw,b(Xi) âˆ’Gw,b(Xi)| : i =
1, . . . , (k + 1)N

â‰¤2. Moreover, the choice of fw,b may be required to depend on
(Xi)(k+1)N
i=1
in an exchangeable way. Similarly to Theorem 3.2.3 (page 116), it can
be proved that for any partially exchangeable probability distribution P âˆˆM1
+(Î©),
with P probability at least 1 âˆ’Ïµ, for any fw,b âˆˆF,
1
kN
(k+1)N

i=N+1
1

fw,b(Xi)Yi â‰¤1

â‰¤k + 1
k
inf
Î»âˆˆR+

1 âˆ’exp(âˆ’Î»
N )
âˆ’1

1âˆ’
exp

âˆ’Î»
N 2
N

i=1
1

fw,b(Xi)Yi â‰¤1

âˆ’log

|F|

âˆ’log(Ïµ)
N

âˆ’
1
kN
N

i=1
1

fw,b(Xi)Yi â‰¤1

.
Let us remark that
1

21

gw,b(Xi) â‰¥0

âˆ’1 Ì¸= Yi

= 1

Gw,b(Xi)Yi < 0

â‰¤1

fw,b(Xi)Yi â‰¤1

and
1

fw,b(Xi)Yi â‰¤1

â‰¤1

Gw,b(Xi)Yi â‰¤3

â‰¤1

gw,b(Xi)Yi â‰¤4Î³

.
This proves the following theorem.
Theorem 4.2.8.
Let us consider the sequence (Î³h)hâˆˆNâˆ—deï¬ned by equation (4.6,
page 148). With P probability at least 1 âˆ’Ïµ, for any (w, b) âˆˆÎ˜,
1
kN
(k+1)N

i=N+1
1

21

gw,b(Xi) â‰¥0

âˆ’1 Ì¸= Yi

â‰¤k + 1
k
inf
Î»âˆˆR+,hâˆˆNâˆ—

1 âˆ’exp(âˆ’Î»
N )
âˆ’1

1âˆ’
exp
&
âˆ’Î»
N 2
N

i=1
1

gw,b(Xi)Yi â‰¤4RÎ³h

âˆ’
log

20(k + 1)N

h
log(2) log

4e(k+1)N
h

+ 1

+ log

2h(h+1)
Ïµ

N
'

152
Chapter 4.
Support Vector Machines
âˆ’
1
kN
N

i=1
1

gw,b(Xi)Yi â‰¤4RÎ³h

.
Properly speaking this theorem is not a margin bound, but more precisely a margin
quantile bound, since it covers the case where some fraction of the training sample
falls within the region deï¬ned by the margin parameter Î³h which optimizes the
bound.
As a consequence though, we get a true (weaker) margin bound: with P proba-
bility at least 1 âˆ’Ïµ, for any (w, b) âˆˆÎ˜ such that
Î³ =
min
i=1,...,N gw,b(Xi)Yi > 0,
1
kN
(k+1)N

i=N+1
1

gw,b(Xi)Yi < 0

â‰¤k+1
k

1 âˆ’exp

âˆ’
log

20(k+1)N

N

16R2+2Î³2
log(2)Î³2 log

e(k+1)NÎ³2
4R2

+ 1

+ 1
N log( Ïµ
2)

.
This inequality compares favourably with similar inequalities in Cristianini et al.
(2000), which moreover do not extend to the margin quantile case as this one.
Let us also mention that it is easy to circumvent the fact that R is not observed
when the test set X(k+1)N
N+1
is not observed.
Indeed, we can consider the sample obtained by projecting X(k+1)N
1
on some
ball of ï¬xed radius Rmax, putting
tRmax(Xi) = min

1, Rmax
âˆ¥Xiâˆ¥

Xi.
We can further consider an atomic prior distribution Î½ âˆˆM1
+(R+) bearing on Rmax,
to obtain a uniform result through a union bound. As a consequence of the previous
theorem, we have
Corollary 4.2.9. For any atomic prior Î½ âˆˆM1
+(R+), for any partially exchange-
able probability measure P âˆˆM1
+(Î©), with P probability at least 1 âˆ’Ïµ, for any
(w, b) âˆˆÎ˜, any Rmax âˆˆR+,
1
kN
(k+1)N

i=N+1
1

21

gw,b â—¦tRmax(Xi) â‰¥0

âˆ’1 Ì¸= Yi

â‰¤k + 1
k
inf
Î»âˆˆR+,hâˆˆNâˆ—

1 âˆ’exp(âˆ’Î»
N )
âˆ’1

1âˆ’
exp
&
âˆ’Î»
N 2
N

i=1
1

gw,b â—¦tRmax(Xi)Yi â‰¤4RmaxÎ³h

âˆ’
log

20(k + 1)N

h
log(2) log

4e(k+1)N
h

+ 1

+ log

2h(h+1)
ÏµÎ½(Rmax)

N
'

4.2.
Bounds for Support Vector Machines
153
âˆ’
1
kN
N

i=1
1

gw,b â—¦tRmax(Xi)Yi â‰¤4RmaxÎ³h

.
Let us remark that tRmax(Xi) = Xi, i = N + 1, . . . , (k + 1)N, as soon as we
consider only the values of Rmax not smaller than maxi=N+1,...,(k+1)Nâˆ¥Xiâˆ¥in this
corollary. Thus we obtain a bound on the transductive generalization error of the
unthresholded classiï¬cation rule 21

gw,b(Xi) â‰¥0

âˆ’1, as well as some incitation to
replace it with a thresholded rule when the value of Rmax minimizing the bound
falls below maxi=N+1,...,(k+1)Nâˆ¥Xiâˆ¥.

154
Chapter 4.
Support Vector Machines

Appendix: Classiï¬cation by
thresholding
In this appendix, we show how the bounds given in the ï¬rst section of this mono-
graph can be computed in practice on a simple example: the case when the clas-
siï¬cation is performed by comparing a series of measurements to threshold values.
Let us mention that our description covers the case when the same measurement is
compared to several thresholds, since it is enough to repeat a measurement in the
list of measurements describing a pattern to cover this case.
5.1. Description of the model
Let us assume that the patterns we want to classify are described through h
real valued measurements normalized in the range (0, 1). In this setting the pattern
space can thus be deï¬ned as X = (0, 1)h.
Consider the threshold set T = (0, 1)h and the response set R = Y{0,1}h. For any
t âˆˆ(0, 1)h and any a : {0, 1}h â†’Y, let
f(t,a)(x) = a

1(xj â‰¥tj)
h
j=1

,
x âˆˆX,
where xj is the jth coordinate of x âˆˆX. Thus our parameter set here is Î˜ =
T Ã— R. Let us consider the Lebesgue measure L on T and the uniform probability
distribution U on R. Let our prior distribution be Ï€ = L âŠ—U. Let us deï¬ne for any
threshold sequence t âˆˆT
Î”t =

tâ€² âˆˆT : (tâ€²
j, tj) âˆ©{Xj
i ; i = 1, . . . , N} = âˆ…, j = 1, . . . , h

,
where Xj
i is the jth coordinate of the sample pattern Xi, and where the interval
(tâ€²
j, tj) of the real line is deï¬ned as the convex hull of the two point set {tâ€²
j, tj},
whether tâ€²
j â‰¤tj or not. We see that Î”t is the set of thresholds giving the same
response as t on the training patterns. Let us consider for any t âˆˆT the middle
m(Î”t) =

Î”t tâ€²L(dtâ€²)
L(Î”t)
of Î”t. The set Î”t being a product of intervals, its middle is the point whose coordi-
nates are the middle of these intervals. Let us introduce the ï¬nite set T composed
of the middles of the cells Î”t, which can be deï¬ned as
T = {t âˆˆT : t = m(Î”t)}.
It is easy to see that |T| â‰¤(N + 1)h and that |R| = |Y|2h.
155

156
Appendix
5.2. Computation of inductive bounds
For any parameter (t, a) âˆˆT Ã— R = Î˜, let us consider the posterior distribution
deï¬ned by its density
dÏ(t,a)
dÏ€
(tâ€², aâ€²) = 1

tâ€² âˆˆÎ”t

1

aâ€² = a

Ï€

Î”t Ã— {a}

.
In fact we are considering a ï¬nite number of posterior distributions, since Ï(t,a) =
Ï(m(Î”t),a), where m(Î”t) âˆˆT. Moreover, for any exchangeable sample distribution
P âˆˆM1
+

(X Ã— Y)N+1
and any thresholds t âˆˆT,
P

(Xj
N+1, tj) âˆ©{Xj
i , i = 1, . . . , N} = âˆ…

â‰¤
2
N + 1.
Thus, for any (t, a) âˆˆÎ˜,
P

Ï(t,a)

f.(XN+1)

Ì¸= f(t,a)(XN+1)

â‰¤
2h
N + 1,
showing that the classiï¬cation produced by Ï(t,a) on new examples is typically non-
random; this result is only indicative, since it is concerned with a non-random choice
of (t, a).
Let us compute the various quantities needed to apply the results of the ï¬rst
section, focussing our attention on Theorem 2.1.3 (page 54).
First note that Ï(t,a)(r) = r[(t, a)]. The entropy term is such that
K(Ït,a, Ï€) = âˆ’log

Ï€

Î”t Ã— {r}

= âˆ’log

L(Î”t)

+ 2h log

|Y|

.
Let us notice accordingly that
min
(t,a)âˆˆÎ˜ K(Ï(t,a), Ï€) â‰¤h log(N + 1) + 2h log

|Y|

.
Let us introduce the counters
bt
y(c) = 1
N
N

i=1
1

Yi = y and

1(Xj
i â‰¥tj)
h
j=1 = c

,
t âˆˆT, c âˆˆ{0, 1}h, y âˆˆY,
bt(c) =

yâˆˆY
bt
y(c) = 1
N
N

i=1
1

1(Xj
i â‰¥tj)
h
j=1 = c

,
t âˆˆT, c âˆˆ{0, 1}h.
Since
r[(t, a)] =

câˆˆ{0,1}h

bt(c) âˆ’bt
a(c)(c)

,
the partition function of the Gibbs estimator can be computed as
Ï€

exp(âˆ’Î»r)

=

tâˆˆT
L(Î”t)

aâˆˆR
1
|Y|2h exp

âˆ’Î»
N

i=1
1

Yi Ì¸= f(t,a)(Xi)

=

tâˆˆT
L(Î”t)

aâˆˆR
1
|Y|2h exp

âˆ’Î»

câˆˆ{0,1}h

bt(c) âˆ’bt
a(c)(c)


5.2.
Computation of inductive bounds
157
=

tâˆˆT
L(Î”t)
3
câˆˆ{0,1}h
&
1
|Y|

yâˆˆY
exp
 
âˆ’Î»

bt(c) âˆ’bt
y(c)
!'
.
We see that the number of operations needed to compute Ï€

exp(âˆ’Î»r)

is propor-
tional to |T| Ã— 2h Ã— |Y| â‰¤(N + 1)h2h|Y|. An exact computation will therefore be
feasible only for small values of N and h. For higher values, a Monte Carlo approx-
imation of this sum will have to be performed instead.
If we want to compute the bound provided by Theorem 2.1.3 (page 54) or by
Theorem 2.2.2 (page 68), we need also to compute, for any ï¬xed parameter Î¸ âˆˆÎ˜,
quantities of the type
Ï€exp(âˆ’Î»r)

exp

Î¾mâ€²(Â·, Î¸)

= Ï€exp(âˆ’Î»r)

exp

Î¾ÏÎ¸(mâ€²)

,
Î», Î¾ âˆˆR+.
We need to introduce
b
t
y(Î¸, c) = 1
N
N

i=1
2221

fÎ¸(Xi) Ì¸= Yi

âˆ’1(y Ì¸= Yi)
2221

1(Xj
i â‰¥tj)
h
j=1 = c

.
Similarly to what has been done previously, we obtain
Ï€

exp

âˆ’Î»r + Î¾mâ€²(Â·, Î¸)

=

tâˆˆT
L(Î”t)
3
câˆˆ{0,1}h
 1
|Y|

yâˆˆY
exp
 
âˆ’Î»

bt(c) âˆ’bt
y(c)

+ Î¾b
t
y(Î¸, c)
!
.
We can then compute
Ï€exp(âˆ’Î»r)(r) = âˆ’âˆ‚
âˆ‚Î» log

Ï€

exp(âˆ’Î»r)

,
Ï€exp(âˆ’Î»r)

exp

Î¾ÏÎ¸(mâ€²)

= Ï€

exp

âˆ’Î»r + Î¾mâ€²(Â·, Î¸)

Ï€

exp(âˆ’Î»r)

,
Ï€exp(âˆ’Î»r)

mâ€²(Â·, Î¸)

= âˆ‚
âˆ‚Î¾ |Î¾=0
log

Ï€

exp

âˆ’Î»r + Î¾mâ€²(Â·, Î¸)

.
This is all we need to compute B(ÏÎ¸, Î², Î³) (and also B(Ï€exp(âˆ’Î»r), Î², Î³)) in Theorem
2.1.3 (page 54), using the approximation
log

Ï€exp(âˆ’Î»1r)

exp

Î¾Ï€exp(âˆ’Î»2r)(mâ€²)

â‰¤log

Ï€exp(âˆ’Î»1r)

exp

Î¾mâ€²(Â·, Î¸)

+ Î¾Ï€exp(âˆ’Î»2r)

mâ€²(Â·, Î¸)

,
Î¾ â‰¥0.
Let us also explain how to apply the posterior distribution Ï(t,a), in other words
our randomized estimated classiï¬cation rule, to a new pattern XN+1:
Ï(t,a)

fÂ·(XN+1) = y

= L(Î”t)âˆ’1

Î”t
1

a

1(Xj
N+1 â‰¥tâ€²
j)
h
j=1

= y

L(dtâ€²)
= L(Î”t)âˆ’1

câˆˆ{0,1}h
L

tâ€² âˆˆÎ”t :

1(Xj
N+1 â‰¥tâ€²
j)
h
j=1 = c

1

a(c) = y

.
Let us deï¬ne for short
Î”t(c) =

tâ€² âˆˆÎ”t :

1(Xj
N+1 â‰¥tâ€²
j)
h
j=1 = c

,
c âˆˆ{0, 1}h.

158
Appendix
With this notation
Ï(t,a)

f.(XN+1) = y

= L

Î”t
âˆ’1

câˆˆ{0,1}h
L

Î”t(c)

1

a(c) = y

.
We can compute in the same way the probabilities for the label of the new pattern
under the Gibbs posterior distribution:
Ï€exp(âˆ’Î»r)

fÂ·(XN+1) = yâ€²
=

tâˆˆT
3
câˆˆ{0,1}h
 1
|Y|

yâˆˆY
exp
 
âˆ’Î»

bt(c) âˆ’bt
y(c)
!
Ã—

câˆˆ{0,1}h
L

Î”t(c)

yâˆˆY 1(y = yâ€²) exp

âˆ’Î»

bt(c) âˆ’bt
y(c)


yâˆˆY exp

âˆ’Î»

bt(x) âˆ’bty(c)


Ã—

tâˆˆT
L(Î”t)
3
câˆˆ{0,1}h
 1
|Y|

yâˆˆY
exp

âˆ’Î»

bt(c) âˆ’bt
y(c)
âˆ’1
.
5.3. Transductive bounds
In the case when we observe the patterns of a shadow sample (Xi)(k+1)N
i=N+1 on top
of the training sample (Xi, Yi)N
i=1, we can introduce the set of thresholds responding
as t on the extended sample (Xi)(k+1)N
i=1
Î”t =

tâ€² âˆˆT : (tâ€²
j, tj) âˆ©

Xj
i ; i = 1, . . . , (k + 1)N} = âˆ…, j = 1, . . . , h

,
consider the set
T =

t âˆˆT : t = m(Î”t)

,
of the middle points of the cells Î”t, t âˆˆT, and replace the Lebesgue measure
L âˆˆM1
+

(0, 1)h
of the previous section with the uniform probability measure L on
T. We can then consider Ï€ = LâŠ—U, where U is as previously the uniform probability
measure on R. This gives obviously an exchangeable posterior distribution and
therefore qualiï¬es Ï€ for transductive bounds. Let us notice that |T| â‰¤

(k + 1)N +
1
h, and therefore that Ï€(t, a) â‰¥

(k + 1)N + 1
âˆ’h|Y|âˆ’2h, for any (t, a) âˆˆT Ã— R.
For any (t, a) âˆˆT Ã— R we may similarly to the inductive case consider the
posterior distribution Ï(t,a) deï¬ned by
dÏ(t,a)
dÏ€
(tâ€², aâ€²) = 1(tâ€² âˆˆÎ”t)1(aâ€² = a)
Ï€

Î”t Ã— {a})
,
but we may also consider Î´(m(Î”t),a), which is such that ri{[m(Î”t), a]} = ri[(t, a)],
i = 1, 2, whereas only Ï(t,a)(r1) = r1[(t, a)], while
Ï(t,a)(r2) =
1
|T âˆ©Î”t|

tâ€²âˆˆT âˆ©Î”t
r2[(tâ€², a)].
We get

5.3.
Transductive bounds
159
K(Ï(t,a), Ï€) = âˆ’log

L(Î”t)

+ 2h log

|Y|

â‰¤log

|T|

+ 2h log(|Y|) = K(Î´[m(Î”t),a], Ï€)
â‰¤h log

(k + 1)N + 1

+ 2h log(|Y|),
whereas we had no such uniform bound in the inductive case. Similarly to the
inductive case
Ï€

exp(âˆ’Î»r1)

=

tâˆˆT
L(Î”t)
3
câˆˆ{0,1}h
 1
|Y|

yâˆˆY
exp
 
âˆ’Î»

bt(c) âˆ’bt
y(c)
!
.
Moreover, for any Î¸ âˆˆÎ˜,
Ï€

exp

âˆ’Î»r1 + Î¾ÏÎ¸(mâ€²)

= Ï€

exp

âˆ’Î»r1 + Î¾mâ€²(Â·, Î¸)

=

tâˆˆT
L(Î”t)
3
câˆˆ{0,1}h
 1
|Y|

yâˆˆY
exp
 
âˆ’Î»

bt(c) âˆ’bt
y(c)

+ Î¾b(Î¸, c)
!
.
The bound for the transductive counterpart to Theorems 2.1.3 (page 54) or 2.2.2
(page 68), obtained as explained page 115, can be computed as in the inductive
case, from these two partition functions and the above entropy computation.
Let us mention ï¬nally that, using the same notation as in the inductive case,
Ï€exp(âˆ’Î»r1)

fÂ·(XN+1) = yâ€²
=

tâˆˆT
3
câˆˆ{0,1}h
 1
|Y|

yâˆˆY
exp
 
âˆ’Î»

bt(c) âˆ’bt
y(c)
!
Ã—

câˆˆ{0,1}h
L

Î”t(c)

yâˆˆY 1(y = yâ€²) exp

âˆ’Î»

bt(c) âˆ’bt
y(c)


yâˆˆY exp

âˆ’Î»

bt(x) âˆ’bty(c)


Ã—

tâˆˆT
L(Î”t)
3
câˆˆ{0,1}h
 1
|Y|

yâˆˆY
exp

âˆ’Î»

bt(c) âˆ’bt
y(c)
âˆ’1
.
To conclude this appendix on classiï¬cation by thresholding, note that similar fac-
torized computations are feasible in the important case of classiï¬cation trees. This
can be achieved using some variant of the context tree weighting method discovered
by Willems et al. (1995) and successfully used in lossless compression theory. The
interested reader can ï¬nd a description of this algorithm applied to classiï¬cation
trees in Catoni (2004, page 62).

160
Appendix

Bibliography
Alon, N., Ben-David, S., Cesa-Bianchi, N. and Haussler, D. (1997). Scale
sensitive dimensions, uniform convergence and learnability. J. ACM 44 615â€“631.
Audibert, J.-Y. (2004a). Aggregated estimators and empirical complexity for least
square regression. Ann. Inst. H. PoincarÂ´e Probab. Statist. 40 685â€“736.
Audibert, J.-Y. (2004b). PAC-Bayesian statistical learning theory. Ph.D. thesis,
Univ. Paris 6. Available at http://cermics.enpc.fr/ audibert/.
Barron, A. (1987). Are Bayes rules consistent in information? In Open Problems
in Communication and Computation (T. M. Cover and B. Gopinath, eds.) 85â€“91.
Springer, New York.
Barron, A. and Yang, Y. (1999). Information-theoretic determination of mini-
max rates of convergence. Ann. Statist. 27 1564â€“1599.
Barron, A., BirgÂ´e, L. and Massart, P. (1999). Risk bounds for model selection
by penalization. Probab. Theory Related Fields 113 301â€“413.
Blanchard, G. (1999). The â€œprogressive mixtureâ€ estimator for regression trees.
Ann. Inst. H. PoincarÂ´e Probab. Statist. 35 793â€“820.
Blanchard, G. (2001). Mixture and aggregation of estimators for pattern recog-
nition. Application to decision trees. Ph.D. thesis, Univ. Paris 13. Available at
http://ida.ï¬rst.fraunhofer.de/ blanchard/.
Blanchard, G. (2004). Un algorithme accÂ´elÂ´erÂ´e dâ€™Â´echantillonnage BayÂ´esien pour
le mod`ele CART. Rev. Intell. Artiï¬cielle 18 383â€“410.
BirgÂ´e, L. and Massart, P. (1997). From model selection to adaptive estimation.
In Festschrift for Lucien Le Cam (D. Pollard, ed.) 55â€“87. Springer, New York.
BirgÂ´e, L. and Massart, P. (1998). Minimum contrast estimators on sieves.
Bernoulli 4 329â€“375.
BirgÂ´e, L. and Massart, P. (2001a). A generalized Cp criterion for Gaussian
model selection. Preprint. Available at http://www.math.u-psud.fr/ massart/.
BirgÂ´e, L. and Massart, P. (2001b). Gaussian model selection. J. Eur. Math.
Soc. 3 203â€“268.
Blum, A. and Langford, J. (2003). PAC-MDL bounds. Computational Learn-
ing Theory and Kernel Machines. 16th Annual Conference on Computational
Learning Theory and 7th Kernel Workshop. COLT/Kernel 2003, Washington,
DC, USA, August 24-27, 2003, Proceedings. Lecture Notes in Comput. Sci. 2777
344â€“357. Springer, New York.
Catoni, O. (2002). Data compression and adaptive histograms. In Foundations of
Computational Mathematics. Proceedings of the Smalefest 2000 (F. Cucker and
J. M. Rojas eds.) 35â€“60. World Scientiï¬c.
Catoni, O. (2003). Laplace transform estimates and deviation inequalities. Ann.
Inst. H. PoincarÂ´e Probab. Statist. 39 1â€“26.
Catoni, O. (2004). Statistical learning theory and stochastic optimization. Ecole
161

162
Bibliography
dâ€™Â´EtÂ´e de ProbabilitÂ´es de Saint-Flour XXXIâ€”2001. Lecture Notes in Math. 1851
1â€“270. Springer, New York.
Cristianini, N. and Shawe Taylor, J. (2000). An Introduction to Support Vec-
tor Machines and Other Kernel Based Learning Methods. Cambridge Univ. Press.
Feder, M. and Merhav, N. (1996). Hierarchical universal coding. IEEE Trans.
Inform. Theory 42 1354â€“1364.
Hastie, T., Tibshirani, R. and Friedman, J. (2001). The Elements of Statis-
tical Learning. Springer, New York.
Langford, J. and McAllester, D. (2004). Computable shell decomposition
bounds. J. Machine Learning Research 5 529â€“547.
Langford,
J.
and
Seeger,
M.
(2001a).
Bounds
for
averaging
classi-
ï¬ers. Technical report CMU-CS-01-102, Carnegie Mellon Univ. Available at
http://www.cs.cmu.edu/ jcl.
Langford, J., Seeger, M. and Megiddo, N. (2001b). An improved predictive
accuracy bound for averaging classiï¬ers. International Conference on Machine
Learning 18 290â€“297.
Littlestone,
N.
and
Warmuth,
M. (1986). Relating data compression
and learnability. Technical report, Univ. California, Santa Cruz. Available at
http://www.soe.ucsc.edu/ manfred/pubs.html.
McAllester, D. A. (1998). Some PAC-Bayesian theorems. In Proceedings of the
Eleventh Annual Conference on Computational Learning Theory (Madison, WI,
1998) 230â€“234. ACM, New York.
McAllester, D. A. (1999). PAC-Bayesian model averaging. In Proceedings of
the Twelfth Annual Conference on Computational Learning Theory (Santa Cruz,
CA, 1999) 164â€“170. ACM, New York.
McDiarmid, C. (1998) Concentration. In Probabilistic Methods for Algorithmic
Discrete Mathematics (M. Habib, C. McDiarmid and B. Reed, eds.) 195â€“248.
Springer, New York.
Mammen, E. and Tsybakov, A. (1999). Smooth discrimination analysis. Ann.
Statist. 27 1808â€“1829.
Ryabko, B. Y. (1984). Twice-universal coding. Problems Inform. Transmission
20 24â€“28.
Seeger, M. (2002). PAC-Bayesian generalization error bounds for Gaussian
process classiï¬cation. J. Machine Learning Research 3 233â€“269.
Shawe-Taylor, J., Bartlett, P. L., Williamson, R. C. and Anthony,
M. (1998). Structural risk minimization over data-dependent hierarchies. IEEE
Trans. Inform. Theory 44 1926â€“1940.
Shawe-Taylor, J. and Cristianini, N. (2002). On the generalization of soft
margin algorithms. IEEE Trans. Information Theory 48 2721â€“2735.
Tsybakov, A. (2004). Optimal aggregation of classiï¬ers in statistical learning.
Ann. Statist. 32 135â€“166.
Tsybakov, A. and Van de Geer, S. (2005). Square root penalty: Adaptation to
the margin in classiï¬cation and in edge estimation. Ann. Statist. 33 1203â€“1224.
Van de Geer, S. (2000). Applications of Empirical Process Theory. Cambridge
Univ. Press.
Vapnik, V. N. (1998). Statistical Learning Theory. Wiley, New York.
Vert, J.-P. (2000). Double mixture and universal inference. Preprint. Available
at http://cbio.ensmp.fr/ vert/publi/.
Vert, J.-P. (2001a). Adaptive context trees and text clustering. IEEE Trans.
Inform. Theory 47 1884â€“1901.
Vert, J.-P. (2001b). Text categorization using adaptive context trees. Proceedings

Bibliography
163
of the CICLing-2001 Conference (A. Gelbukh, ed.) 423â€“436. Lecture Notes in
Comput. Sci. 2004. Springer, New York.
Willems, F. M. J., Shtarkov, Y. M. and Tjalkens, T. J. (1995). The
context-tree weighting method: Basic properties. IEEE Trans. Inform. Theory
41 653â€“664.
Willems, F. M. J., Shtarkov, Y. M. and Tjalkens, T. J. (1996). Con-
text weighting for general ï¬nite-context sources. IEEE Trans. Inform. Theory
42 1514â€“1520.
Zhang, T. (2006a). From Ïµ-entropy to KL-entropy: Analysis of minimum informa-
tion complexity density estimation. Ann. Statist. 34 2180â€“2210.
Zhang, T. (2006b). Information-theoretic upper and lower bounds for statistical
estimation. IEEE Trans. Inform. Theory 52 1307â€“1321.

