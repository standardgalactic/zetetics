Automated Negotiations∗
Can Automated Agents Proﬁciently Negotiate with Humans?
Raz Lin1 and Sarit Kraus1,2
1 Department of Computer Science
Bar-Ilan University
Ramat-Gan, Israel 52900
{linraz,sarit}@cs.biu.ac.il
2 Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742 USA
1.
INTRODUCTION
Negotiation is a process in which interested parties confer
with the aim of reaching an agreement. The ability to nego-
tiate successfully is critical for any social interaction. In par-
ticular, automated negotiators should be able to proﬁciently
interact and collaborate with people. Negotiation surrounds
our every day life even without noticing or paying careful at-
tention to it. We often ﬁnd ourselves in situations, whether
simple or complex, which require negotiations.
They can
either be bilateral or multilateral negotiations.
They can
also be simple and ordinary, like haggling over a price in the
market or deciding on a meeting time. Nonetheless, they
can also have colossal eﬀects on the lives of millions, such
as negotiations involving inter-country disputes and nuclear
disarmament [14]. No matter what the domain, negotiation
is not an easy task. Even what might be perceived as a“sim-
ple” case of a single-issue bilateral bargaining over a price in
the market can demonstrate the diﬃculties that arise dur-
ing the negotiation process. In fact, it may demonstrate the
complexity of the negotiation process and the modeling of
the environment. In this case there are two sides, each with
her own preferences, that might, or might not, be known to
the other party. In addition, some of these preferences might
conﬂict and thus reaching an agreement would require a cer-
tain degree of cooperation or concession.
Keeping all this in mind, the negotiation domain is an
attractive environment for automated agents, resulting in
many beneﬁts. Automated agents can be used with humans
in the loop or without them. On the one hand, they can
alleviate some of the eﬀorts required of people during ne-
gotiations and also assist people that are less qualiﬁed in
the negotiation process. On the other hand there may be
situations in which automated negotiators can even replace
human negotiators.
Another possibility is for people em-
∗This research is based upon work supported in part by
the U.S. Army Research Laboratory and the U.S. Army Re-
search Oﬃce under grant number W911NF-08-1-0144 and
under NSF grant 0705587.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
Copyright 2008 ACM 0001-0782/08/0X00 ...$5.00.
barking on important negotiation tasks to use these agents
as a training tool, prior to actually performing the task.
Thus, success in developing an automated agent with nego-
tiation capabilities has great advantages and implications.
The design of automated agents that proﬁciently negotiate
is a challenging task, as there are diﬀerent environments and
constraints that should be considered.
The negotiation environment deﬁnes the speciﬁc settings
of the negotiation. Based on these settings, diﬀerent con-
siderations should then be taken into account. We describe
the possible settings in the following subsection.
In this paper we focus on the question of whether an auto-
mated agent can proﬁciently negotiate with human negotia-
tors. To this end we deﬁne a proﬁcient automated negotiator
as one that can achieve the best possible agreement for it-
self. This, of course, also depends on the preferences of the
other party and thus adds complexity to the design of such
an agent.
1.1
The Negotiation Environment
When designing an automated agent, the designer needs
to take into account the environment in which the agent will
operate.
The environment determines several parameters
which dictate the number of negotiators taking part in the
negotiation, the time frame of the negotiation and the issues
on which the negotiation is being conducted. The number of
parties participating in the negotiation process can be two
(bilateral negotiations) or more (multilateral negotiations).
For example, in a market there can be one seller but many
buyers, all involved in negotiating over a certain item. On
the other hand, if the item is common, there may also be
many sellers taking part in the negotiation process.
The negotiation environment also consists of a set of ob-
jectives and issues to be resolved. Various types of issues
can be involved, including discrete enumerated value sets,
integer-value sets, and real-value sets. A negotiation con-
sists of multi-attribute issues if the parties have to negotiate
an agreement which involves several attributes for each is-
sue. Negotiations that involves multi-attribute issues allow
making complex decisions while taking into account multiple
factors [18]. The negotiation environment can consist of non-
cooperative negotiators or cooperative negotiators.
Gen-
erally speaking, cooperative agents try to maximize their
combined joint utilities (e.g., see [40]) while non-cooperative
agents try to maximize their own utilities regardless of the

other sides’ utilities.
Finally, the negotiation protocol deﬁnes the formal inter-
action between the negotiators: whether the negotiation is
done only once (one-shot) or repeatedly, and how the ex-
change of oﬀers between the agents is conducted. A com-
mon exchange of oﬀers model is the alternating oﬀers model
[32].
In addition, the protocol states whether agreements
are enforceable or not, and whether the negotiation has a
ﬁnite or inﬁnite horizon. The negotiation is said to have a
ﬁnite horizon if the length of every possible history of the
negotiation is ﬁnite. In this respect, time costs may also be
assigned and they may increase or decrease the utility of the
negotiator.
Figure 1 depicts the diﬀerent variations in the settings,
along with the location of each system that is described in
Section 3. For example, point D in the cube represents bi-
lateral negotiations with multi-attribute issues and repeated
interactions, while point B represents multi-lateral negotia-
tions with a single attribute for negotiation and a one-shot
encounter.
Figure 1: Variations of the negotiation settings.
The negotiation domain encompasses the negotiation ob-
jectives and issues and assigns diﬀerent values to each. Thus,
an agent may be tailored to a given domain (e.g., the Diplo-
mat agent [22] described below is tailored to a speciﬁc do-
main of the Diplomacy game) or domain independent (e.g.,
the QOAgent [24] described below).
1.2
The Information Model
The information model dictates what is known to each
agent. It can be a model of complete information, in which
each agent has complete knowledge of both the state of the
world and the preferences of other agents; or it can be a
model of incomplete information, in which agents may have
only partial knowledge of either the states of the world or
the preferences of other agents (e.g., bargaining games with
asymmetric information), or they may be ignorant of the
preferences of the opponents and the states of the world
[33].
The incomplete information can be modeled in dif-
ferent ways with respect to the uncertainty regarding the
preferences of the other party. One approach to modeling
the information is to assume that there is a set of diﬀerent
agent types and the other party can be any one of these
types.
1.3
Human-Agent Negotiations
The issue of automated negotiation is too broad to cover
in a short review paper. To this end, we have decided to
concentrate on adversarial bilateral bargaining in which the
automated agent is matched with people.
The challenges
in this area could motivate readers to pursue this ﬁeld (note
that this sets the focus and leaves most auction settings out-
side the scope of this article, even though automated agents
that bid in auctions competing with humans have been pro-
posed and evaluated in the literature (e.g., Grossklags and
Schmidt [11])).
1.3.1
Automated Negotiator Agents
The problem of developing an automated agent for negoti-
ations is not new for researchers in the ﬁelds of Multi-Agent
Systems and Game Theory (e.g., Kraus [20] and Muthoo
[26]). However, designing an automated agent that can suc-
cessfully negotiate with a human counterpart is quite dif-
ferent from negotiating with another automated agent. Al-
though an automated agent that played in the Diplomacy
game with other human players was introduced by Kraus
and Lehmann [22] some twenty years ago, the diﬃculties of
designing proﬁcient automated negotiators have not been re-
solved. In essence, in most research, assumptions are made
that do not necessarily apply in genuine negotiations with
humans, such as assuming complete information or the ra-
tionality of the opponent negotiator. In this sense, both par-
ties are assumed to be rational in their behavior (e.g., the
decisions made by the agents are described as rational and
the agents are considered to be expected utility maximizing
agents that cannot deviate from their prescribed behavior).
Yet, when dealing with human counterparts, one must take
into consideration the fact that humans do not necessarily
maximize expected utility or behave rationally. In particu-
lar, results from social sciences suggest that people do not
follow equilibrium strategies [6, 25]. Moreover, when play-
ing with humans, the theoretical equilibrium strategy is not
necessarily the optimal strategy [38]. In this respect, equi-
librium based automated agents that play with people must
incorporate heuristics to allow for “unknown” deviations in
the behavior of the other party. Moreover, when people are
the ones that design agents, they do not always design them
to follow equilibrium strategies [12]. Nonetheless, some as-
sumptions are made, i.e. mainly that the other party will not
necessarily maximize its expected utility, however, if given
two oﬀers it will prefer the one with the highest utility value.
Lastly, it has been shown that whether the opponent is
oblivious or has full knowledge that its counterpart is a
computer agent can change the overall result.
For exam-
ple, Grossklags and Schmidt [11] showed that eﬃcient mar-
ket prices were achieved when human subjects knew that
computer agents existed in a double auction market envi-
ronment.
Sanfey et al.
[34] matched humans with other
humans and with computer agents in the Ultimatum Game
and showed that people rejected unfair oﬀers made by hu-
mans at signiﬁcantly higher rates than those made when
matched with a computer agent.
1.3.2
Automated Agents Negotiating with People
Researchers have tried to take some of these issues into
consideration when designing agents that are capable of pro-

ﬁciently negotiating with people. For example, dealing only
with the bounded rationality of the opponent, several re-
searchers have suggested new notions of equilibria (e.g., the
trembling hand equilibrium described in Rasmusen ([30], p.
139)). Approximately ten years ago, Kasbah, a seminal ne-
gotiation model between agents designed by humans, was
presented in the virtual marketplace by Chavez and Maes
[5]. Here, the agent’s behavior was fully controlled by hu-
man players. The main idea was to help users in the negoti-
ation process between buyers and sellers by using automated
negotiators. Chavez and Maes’s main innovation was, not so
much the sophisticated design of the automated negotiators,
but rather the creation of a multi-agent negotiation environ-
ment. Kraus et al. [21] describe an automated agent that
negotiates proﬁciently with humans.
Although they also
deal with negotiation with humans, there is complete infor-
mation in their settings. Other researchers have suggested
to shift from quantitative decision theory to qualitative deci-
sion theory [36]. In using such a model it is not necessary to
assume that the opponent will follow the equilibrium strat-
egy or try to be a utility maximizer.
Another approach
was to develop heuristics for negotiations motivated by the
behavior of people in negotiations [22]. However, the funda-
mental question of whether it is possible to build automated
agents for negotiations with humans in open environments
has not been fully addressed by these researchers.
Another direction currently under pursuit is the devel-
opment of virtual humans to train people in interpersonal
skills (e.g., Kenny et al. [19]). Achieving this requires imple-
menting cognitive and emotional modeling, natural language
processing, speech recognition and knowledge representa-
tion. This in addition to constructing and implementing the
appropriate logic for the task at hand (e.g., negotiation), is
in order to make the virtual human into a good trainer. An
example of their research prototype, in which trainees con-
duct real-time negotiations with a virtual human doctor and
a village elder to move a clinic to another part of the town
out of harms way is given in Figure 2.
Figure 2: Example of virtual humans’ negotiations.
Commercial companies and schools have also displayed in-
terest in automated negotiation technologies. Many courses
and seminars are oﬀered for the public and for institutions.
These courses often guarantee that upon completion you will
“know many strategies on which to base the negotiation”,
“Discover the negotiation secrets and techniques”, “Learn
common rival’s tactics and how to neutralize them” and ”Be
able to apply an eﬃcient negotiation strategy” (e.g., [1, 27]).
Yet, in many of these courses, the agents are restricted to one
domain and cannot be generalized. Some of the automated
agents cannot be adapted to the user and are restricted to
a single attribute negotiation with no time constraints.
Nonetheless, human factors and results of laboratory and
ﬁeld experiments reviewed in esteemed publications (e.g., [9,
29]) provide guidelines for the design of automated negotia-
tors. Yet, it is still a great challenge to incorporate these
guidelines in the inherent design of an agent to allow it to
proﬁciently negotiate with people.
2.
THE MAIN CHALLENGES
The main diﬃculty, though, in the development of auto-
mated negotiators is that in order to negotiate proﬁciently
with a human counterpart, they have to be able to work in
settings with both opponents with bounded rationality and
incomplete information. The diﬃculty can also stem from
the fact the humans are also inﬂuenced by behavioral as-
pects and by social preferences that hold between players
(such as inequity-aversion [2] and reciprocity [4]). Thus, it
is diﬃcult to predict individual choices.
Tackling the issues of bounded rationality and incomplete
information is a complex task.
To achieve this, an auto-
mated agent is required to have two inter-dependent mech-
anisms.
The ﬁrst is a decision making component which
works via modeling human factors. This mechanism is in
charge of generating oﬀers and deciding whether to accept
or reject oﬀers made by the opponent. The challenge behind
this mechanism does not lie in the computational complex-
ity of making good decisions, but rather in reasoning about
the psychological and social factors that characterize human
behavior. The second component is learning, which allows
the agent to infer the opponent’s preferences and strategies,
based on his actions.
Another inherent problem in the design of the automated
agent is the ability to generalize its behavior.
While hu-
mans can negotiate in diﬀerent settings and domains, when
designing an automated agent a decision should be made
whether the agent should be a general purpose negotiator,
that is, will be able to successfully negotiate in many set-
tings and be domain-independent (e.g., Lin et al. [24]), or
the agent will only be suitable for one speciﬁc domain (e.g.,
Ficici and Pfeﬀer [8], Kraus and Lehmann [22]). Perhaps
the advantage of the agent’s speciﬁcity is the ability to con-
struct better strategies that could allow it to achieve better
agreements, as compared to a more general purpose nego-
tiator. This is due to the fact that the speciﬁcity allows the
designer to debug the agent’s strategy more carefully and
against more test cases. By doing so, the designer can ﬁne-
tune the agent’s strategy and allow for a more proﬁcient au-
tomated negotiator. Agents that are domain independent,
on the other hand, are harder to test against all possible
cases and states.
The issue of trust also plays an important role in nego-
tiations, especially when the other side’s behavior is unpre-

dictable. Successful negotiations depend on the trust estab-
lished between all parties, which can depend on cheap-talk
during negotiations (that is, unveriﬁable information with
regard to the other party’s private information [7]) and the
introduction of unenforceable agreements. Based on the ac-
tions and information each party can update its reputation
(for better or for worse) with regard to the other party and
thus build trust between the sides.
Some of the systems
that we review below do allow cheap-talk and unenforceable
agreements. Building trust can also depend on past and fu-
ture interactions with the other party (e.g., one-shot inter-
action or repeated interactions). In this article, though, due
to limited space we do not cover the issue of trust in detail.
Readers can for example refer to [31] for a comprehensive
review on the topic of trust.
Another important issue is how automated agents can be
evaluated and compared. Such an evaluation is important
in order to select the most appropriate agent for the task at
hand. Yet, no single criteria is deﬁned. The answer to the
questions of “what constitutes a good negotiator agent?” is
multifaceted. For example, is a good agent an agent that:
• achieves a maximal payoﬀwhen matched with human
negotiators?
But will it also generate these payoﬀs
when matched with other automated agents, which
might be more accessible than human negotiators, and
which also exist in open environments?
• generates a maximal combined payoﬀfor both nego-
tiators, that is, the agent is more concerned with max-
imizing the combined utilities than its own reward?
• allows most negotiations to end with an agreement,
rather than one of the sides opting-out or terminating
the negotiations with a status-quo outcome?
• is domain dependent and its technique is suitable only
for that domain or one that is domain independent and
can be adapted to several domains? This might be an
important factor if an agent is required to adapt to
dynamic settings, for example.
• behave in such a manner that would leave its counter-
part speculating whether it is an automated negotiator
or a human one?
In this article we do not deﬁne what or whether there is a
best answer. We also do not claim that a best answer indeed
exists. Yet, researchers should take these and other mea-
sures into consideration when designing their agents. Per-
haps, certain criteria and benchmarks are in order to allow
an adequate comparison between automated agents.
In the rest of this article we will review automated agents
that incorporate the two mechanisms of decision making via
modeling human factors and learning the opponent’s model.
By doing so they try to tackle the aforementioned challenges
in bilateral negotiations. While many automated negotia-
tors’ designs have been suggested in the literature, we only
review those that have actually been evaluated and tested
with human counterparts. This is mainly due to the fact
that in order to test the proﬁciency of an automated nego-
tiator whose purpose is to negotiate with human negotiators,
one must match it with humans. It is not suﬃce to test it
with other automated agents, even if they were supposed to
have been designed by humans as bounded rational agents,
due to many of the reasons previously mentioned.
3.
TACKLING THE CHALLENGES
In this section we describe several automated agents that
try to tackle the challenges and proﬁciently negotiate in
open environments. All of the described agents were eval-
uated with human counterparts.
It is worth noting that
most of the agents described below use structured (or semi-
structured) language and do not implement any natural lan-
guage processing methods (with one exception of the Virtual
Human agent). In addition, the agents vary with respect
to their characteristics.
For example, some are domain-
dependent, while others are domain-independent and are
more general in nature; some use the history of past in-
teractions to model the opponent, while others only have
access to current interaction data. Figure 3 depicts a gen-
eral architecture for an automated agent design. We begin
by describing the oldest agent of all of them, i.e. the Diplo-
mat agent.
Figure 3: Architecture of a general agent’s design.
3.1
The Diplomat Agent
Over twenty years ago Kraus and Lehmann developed an
agent called Diplomat [22], that played the Diplomacy game
(see Figure 4) with the goal to win. The game involves nego-
tiations in multi-issue settings with incomplete information
concerning the other agents’ goals, and misleading informa-
tion can be exchanged between the diﬀerent agents.
The
negotiation protocol extends the model of alternating oﬀers
and allows simultaneous negotiations between the parties,
as well as multiple interactions with the opponent agents
during each time period. The issue of trust also plays an
important role, as commitments might be breached. In ad-
dition, as each game consists of several sessions, it can be
viewed as repeated negotiation settings.
The main innovation of the Diplomat agent is, most prob-

Figure 4: The Diplomacy game.
ably, the fact that it consists of ﬁve diﬀerent modules that
work together to achieve a common goal. Diﬀerent person-
ality traits are implemented in the diﬀerent modules. These
personality traits aﬀect the behavior of the agent and can be
changed during each run, which allows Diplomat to change
its ‘personality’ from one game to another and to act non-
deterministically. In addition, the agent has a limited learn-
ing capability which allows it to try to estimate the per-
sonality traits of its rivals (e.g., their risk attitude). Based
on this, Diplomat assesses whether or not the other play-
ers will keep their promises. In addition, Diplomat incorpo-
rates randomization in its decision making component. This
randomization, inﬂuenced by Diplomat’s personality traits,
determines whether some agreements will be breached or
fulﬁlled.
The results reported by Kraus and Lehmann show that
Diplomat played well in the games in which it participated,
and most human players were not able to guess which of the
players was played by the automated agent. Nonetheless,
the main disadvantage of Diplomat is that it is a domain-
dependent agent - i.e. suitable only for the Diplomacy game.
Since the game is quite complex and time consuming not
many experiments were carried out with human players to
validate the results and reach a level of signiﬁcance. Yet,
at the time Diplomat did open a new and exciting line of
research, some of which we review below.
We continue with a more recent agent which is also con-
strained to a speciﬁc domain and involves single-issue nego-
tiations. However, it also takes into account the history of
past interactions to model the opponents.
3.2
The AutONA Agent
Byde et al. [3] developed AutONA, which is an automated
negotiation agent. Their problem domain involves multiple
negotiations between buyers and sellers over the price and
quantity of a given product. The negotiation protocol fol-
lows the alternating oﬀers model. Each oﬀer is directed at
only one player on the other side of the market, and is pri-
vate information between each pair of buyers and sellers. In
each round, a player can make a new oﬀer, accept an oﬀer,
or terminate negotiations. In addition, a time cost is used to
provide incentives for timely negotiations. While the model
can be viewed as one-shot negotiations, for each experiment,
AutONA was provided with data from previous experiments.
In order to model the opponent, AutONA attaches a belief
function to each player, which tries to estimate the proba-
bility of a price for a given seller and a given quantity. This
belief function is updated based on observed prices in prior
negotiations. Several tactics and heuristics are implemented
to form the strategy of the negotiator during the negotiation
process (e.g., for selecting the opponents with which it will
negotiate and for determining the ﬁrst oﬀer it will suggest
to the opponent). Byde et al. also allowed cheap-talk dur-
ing negotiations, that is, the proposition of oﬀers with no
commitments. The results obtained from the experiments
with human negotiators revealed that the negotiators did
not detect which negotiator was the software agent. In ad-
dition, Byde et al.
found that AutONA is not suﬃciently
aggressive during negotiations and thus many remained in-
complete. Their experiments showed that at ﬁrst AutONA
performed worse than the human players. Thus, a modiﬁed
version, which ﬁne-tuned several conﬁguration parameters of
the AutONA agent, improved the results which were more in
line with those of human negotiators, yet not better. They
conclude that diﬀerent environments would most likely re-
quire changing the conﬁgurations of the AutONA agent.
After reviewing the AutONA agent, we proceed with agents
that are applicable to a larger family of domains. The next
agent is applicable to the Cliﬀ-Edge family of domains. A
Cliﬀ-Edge environment is characterized by a conﬂict be-
tween the desire to maximize proﬁts while preventing the
entire deal from falling through. An example of such a do-
main is the Ultimatum Game. The ultimatum game is an
experimental economics game in which two players have to
decide how to divide a sum of money between them. The
ﬁrst player can propose a division while the second player
can either accept the proposal or reject it. Only if the sec-
ond player accepts the oﬀer the money is split between the
two. The game is played only once.
3.3
The Cliﬀ-Edge Agent
Katz and Kraus [16] proposed an innovative model for
human learning and decision making. Their agent competes
repeatedly in one-shot interactions, each time against a dif-
ferent human opponent (e.g., sealed-bid ﬁrst-price auctions,
ultimatum game). Katz and Kraus utilized a reinforcement
learning algorithm, which integrates virtual learning with re-
inforcement learning. That is, oﬀers higher than an accepted
oﬀer are treated as successful (virtual) oﬀers, notwithstand-
ing that they were not actually proposed. Similarly, oﬀers
lower than a rejected oﬀer are treated as having been (virtu-
ally) unsuccessfully proposed. A threshold is also employed
to allow for some deviations from this strict categorization.
The results of previous interactions are stored in a database,
which is used for later interactions. The decision making
mechanism of Katz and Kraus’s Ultimatum Game agent fol-
lows a heuristic based on the qualitative theory of Learning
Direction [35]. Simply speaking, if an oﬀer is rejected at a
given interaction, then at the next interaction the proposer
will oﬀer the opponent a higher oﬀer. In contrast, if an oﬀer
is accepted, then during the following interaction the oﬀer
will be decreased.
Katz and Kraus show that their algo-
rithm performs better than other automated agents. When
compared to human behavior, there is an advantage to their
automated agent over the human’s average payoﬀ.
Later, Katz and Kraus [17] improved the learning of their
agent by allowing gender sensitive learning. In this case, the
information obtained from previous negotiations is stored in
three databases, one is general and the other two are each
associated with a speciﬁc gender. During the interaction,

the agent’s algorithm tries to determine when to use each
database. Katz and Kraus show that their gender-sensitive
agent yields higher payoﬀs than the generic approach, which
lacks gender sensitivity.
However, Katz and Kraus’s agent was tested in a single-
issue domain with repeated interactions that are used to
improve the learning and decision making mechanism.
It
is not clear whether their approach would be applicable to
negotiation domains in which several rounds are made with
the same opponent and multi-issue oﬀers are made. In addi-
tion, the success of their gender sensitive approach depends
on the existence of diﬀerent behavioral patterns of diﬀerent
gender groups.
The agents described next are tailored to a rich environ-
ment of multi-issue negotiations. Similar to the agent pro-
posed by Katz et al. the history of past interactions is used
to ﬁne-tune agents’ behavior and modeling.
3.4
The Colored-Trails Agents
Ficici and Pfeﬀer [8] were concerned with understand-
ing human reasoning, and using this understanding to build
their automated agents. They did so by means of collecting
negotiation data and then constructing a proﬁcient auto-
mated agent.
Both Byde et al. ’s AutONA agent [3] and
the Colored-Trail agent collect historical data and use it to
model the opponent. Byde et al. used the data to update
the belief regarding the price for each player, while Ficici and
Pfeﬀer used it to construct diﬀerent models of how humans
reason in the game.
Figure 5: The Colored-Trail game screenshot.
The negotiation was conducted in the Colored Trails game
environment [12], which is a game played on a nxm board
of colored squares. Players are issued colored chips and are
required to move from their initial square to a designated
goal square. To move to an adjacent square, a player must
turn in a chip of the same color as the square.
Players
must negotiate with each other to obtain chips needed to
reach the goal square (see Figure 5). Their learning mecha-
nism involved constructing diﬀerent possible models for the
players and using gradient descent to learn the appropriate
model. Ficici and Pfeﬀer trained their agents with results
obtained from human-human simulations and then incorpo-
rated their models in their automated agents, which were
later matched against human players. They show that this
method allows them to generate more successful agents in
terms of the expected number of accepted oﬀers and the ex-
pected total beneﬁt for the agent. They also show that their
agent contributes to the social good by providing high util-
ity scores for the other players. Ficici and Pfeﬀer were also
able to show that their agent performs similarly to human
players.
In order for the Colored-Trails Agent to model the oppo-
nent, prior knowledge regarding the behavior of humans is
needed. The learning mechanism requires suﬃcient human
data for training and is currently limited to one domain only.
Gal et al.
[10] also discuss automated agent design in
the domain of the Colored Trails. They present a machine-
learning approach for modeling human behavior in a two-
player negotiation, where one player proposes a trade to the
other, who can accept or reject it.
Their model tries to
predict the reaction of the opponent to the diﬀerent oﬀers,
and using this prediction it determines the best strategy for
the agent.
The domain on which Gal et al.
tested their
agent can also be viewed as a Cliﬀ-Edge environment, more
complex than the Ultimatum Game, upon which Katz and
Kraus evaluated their agent [16].
Gal et al.
show that the proposed model successfully
learns the social preferences of the opponent and achieves
better results than the Nash equilibrium, Nash bargaining
computer agents, and human players.
We continue now with agents that are domain-independent
and propose an agent with more generality than the afore-
mentioned agents.
3.5
The Guessing Heuristic Agent
Jonker et al. [15] deal with bilateral multi-issue and multi-
attribute negotiations which involve incomplete information.
The negotiation follows the alternating oﬀer protocol and is
conducted once with each opponent. Jonker et al. designed
a generic agent that uses a “guessing heuristic” in the buyer-
seller domain1. This heuristic tries to predict the opponent’s
preferences based on its oﬀers’ history. This is under the as-
sumption that the opponent’s utility has a linear function
structure. Jonker et al. assert that this heuristic allows their
agent to improve the outcome of the negotiations. Regard-
ing the oﬀer generation mechanism, they use a concession
mechanism to obtain the next oﬀer. In their experiments,
the automated agent acts as a proxy for the human user.
The user is involved only in the beginning when he inputs
the preference parameters.
Then the agent generates the
oﬀers and the counter-oﬀers. When comparing negotiations
involving only automated agents with negotiations involving
only humans, the agents usually outperformed the humans
(in the buyer’s role). Yet, in an additional experiment they
matched humans versus agent negotiators.
In this exper-
iment, humans only played the role of the buyer.
When
comparing the human vs. agent negotiations to that of only
1Although Jonker et al. discuss and present results on one
domain only, they state that their model is generic and has
also been applied in other domains.

automated agents, the humans attained somewhat better re-
sults than the agents (in the buyer’s role), based on the av-
erage utilities. The authors believe this should be accounted
to the fact that humans forced the automated negotiators
to make more concessions then they themselves did.
The next agent we discuss also deals with bilateral multi-
issue negotiations that involve incomplete information. Nonethe-
less the negotiation protocol is richer than that of the Guess-
ing Heuristic Agent.
3.6
The QOAgent
The QOAgent [24] is a domain independent agent that
can negotiate with people in environments of ﬁnite horizon
bilateral negotiations with incomplete information. The ne-
gotiations consider a ﬁnite set of multi-attribute issues and
time-constraints. Costs are assigned to each negotiator, such
that during the negotiation process, the negotiator might
gain or lose utility over time. If no agreement is reached
by a given deadline a status quo outcome is enforced.
A
negotiator can also opt-out of the negotiation if it decides
that the negotiation is not proceeding in a favorable manner.
Similar to the negotiation protocol in the Diplomat agent’s
domain, the negotiation protocol in the QOAgent’s domain
extends the model of alternating oﬀers such that each agent
can perform up to M > 0 interactions with the opponent
agent during each time period.
In addition, queries and
promises are allowed which adds unenforceable agreements
to the environment. With respect to incomplete informa-
tion, each negotiator keeps his preferences private, though
the preferences might be inferred from the actions of each
side (e.g., oﬀers made or responses to oﬀers proposed). In-
complete information is expressed as uncertainty regarding
the utility preferences of the opponent, and it is assumed
that there is a ﬁnite set of diﬀerent negotiator types. These
types are associated with diﬀerent additive utility functions
(e.g., one type might have a long term orientation regard-
ing the ﬁnal agreement, while the other type might have
a more constrained orientation). Lastly, the negotiation is
conducted once with each opponent.
As for incomplete information, the QOAgent tackles the
problem by applying a simple Bayesian update mechanism,
which, after each action tries to infer which utility best suits
the opponent (whether when receiving an oﬀer or when re-
ceiving a response to an oﬀer).
For the decision making
process, the approach used by the QOAgent is more of a
qualitative approach [36]. While the QOAgent’s model ap-
plies utility functions, it is based on a non-classical decision
making method, rather than focusing on maximizing the ex-
pected utility. The QOAgent uses the maximin function and
the qualitative valuation of oﬀers. Using these methods the
QOAgent generates oﬀers and decides whether to accept or
reject proposals it has received.
Lin et al. [24] tested the QOAgent in several distinct do-
mains and their results show that the QOAgent reaches more
agreements and plays more eﬀectively than its human coun-
terparts, when the eﬀectiveness is measured by the score of
the individual utility. They also show that the sum of utili-
ties is higher in negotiations when the QOAgent is involved,
as compared to human-human negotiations. Thus, they as-
sert, that it is indeed possible to build an automated agent
that can negotiate successfully with humans. However, it is
also important to state that their agent has certain limita-
tions. They assume that there is a ﬁnite set of diﬀerent agent
types and thus their agent cannot generate a dynamic model
(and perhaps a more accurate one) of the opponent. In ad-
dition, they have not shown whether their agent can also
maintain high scores when matched with other automated
agents, which is an important characteristic of open environ-
ment negotiations. Moreover, the QOAgent does not scale
well when numerous oﬀers are proposed, which can cause its
performance to deteriorate.
Finally, we conclude with a description of a more complex
type of agent that incorporates many features, far beyond
the negotiation strategy itself.
3.7
The Virtual Human Agent
Kenny et al.
[19] describe work on virtual humans that
are used for interpersonal training for skills, such as: nego-
tiation, leadership, interviewing and cultural training. To
achieve this they require a large amount of research in many
ﬁelds (such as: knowledge representation, cognitive and emo-
tional modeling, natural language processing, etc.). Their
intelligent agent is based on the Soar Cognitive Architec-
ture, which is a symbolic reasoning system used to make
decisions.
Traum et al. discuss the negotiation strategies of the vir-
tual human agent in more detail [37]. In their paper they
describe a set of strategies implemented by the agent (e.g.,
when to act aggressively if it seems that the current outcome
will incur a negative utility, or when to ﬁnd the appropriate
issue on which to currently negotiate). The strategy chosen
each time is inﬂuenced by several factors: the control the
agent has over the negotiations, the estimated utility of an
outcome and the estimated best utility of an outcome, the
trust the agent bestows the opponent and the commitment
of all agents to the given issues. The virtual agent also tries
to model the opponent by reasoning about its mental state.
Traum et al.
tested their agents in several negotiation
scenarios. One of these scenarios is a simulation for soldiers
that practice and conduct bilateral engagements with vir-
tual humans, and in situations in which culture plays an
important role.
In this case, the diﬀerent actions can be
selected from a menu which includes appropriate questions
based on the history of the simulation thus far. The second
domain requires trainees to communicate with an embod-
ied virtual human doctor to negotiate and convince him to
move a clinic, located in a middle of a war zone, out of harm’s
way (see Figure 2). Their prototypes are continuously tested
with cadets and civilians. Traum et al. are more concerned
with the system as a whole and thus they do not provide
insights with respect to the proﬁciency of their automated
negotiator. Regarding the environment, they state that the
subjects enjoy using the system for negotiations and that it
also allows them to learn from their mistakes.
Traum et al. also report some of the existing limitations
of their system.
Currently, the virtual agent cannot con-
sider arbitrary oﬀers made by a human negotiator. In addi-
tion, more strategies are required to better cover the envi-
ronment’s rich settings. They also state that the negotiation
problem can be addressed more in depth (following other re-
searchers who have focused mainly on the negotiation ﬁeld),
rather than in breadth (as presently conducted in their sys-
tem).
After having reviewed all the agents we conclude with
a brief discussion on the characteristics and the design of
future agents.

Agent
Main contribution
Diplomat
Changing the agent’s personality heuristics
Non deterministic behavior / randomization
AutONA
Tactics and heuristics
Incorporating data from past interactions
Concession mechanism
Cliﬀ-Edge
Virtual learning
Incorporating data from past interactions
Gender-sensitive approach
Non deterministic behavior / randomization
(implicitly)
Colored-Trails
Incorporating data from past interactions
Machine learning
Guessing
Generic agent / domain independent
Heuristic
Concession mechanism
QOAgent
Generic agent / domain independent
Qualitative decision making
Non deterministic behavior / randomization
Virtual Human
Tactics and heuristics
Cognitive architecture
Table 1: Main contributions of each agent.
3.8
The Rule of Thumb for Designing Auto-
mated Agents
We should probably begin with the conclusion. Despite
the title of this section, there may not be a good rule of
thumb for designing automated negotiators with human ne-
gotiators. Table 1 summarizes the main contributions made
by each of the reviewed agents. If we look into the design
of all the aforementioned agents, we cannot ﬁnd one speciﬁc
feature that connects them or can account for their good
negotiation skills. Nonetheless, we can note several features
that have been used in several agents. Agent designers might
take these features into consideration when designing their
automated agent, while also taking into account the settings
and the environment in which their agent will operate.
The ﬁrst feature is randomization, which was used in Diplo-
mat, QOAgent and also (though not explicitly) in the Cliﬀ-
Edge agents. The randomization factor allows these agents
to be more resilient (or robust) to adversaries that try to
manipulate them to gain better results on their part. In ad-
dition, it allows them to be more ﬂexible, rather than strict,
in accepting agreements and ending negotiations.
The second feature can be viewed as a concession strat-
egy. Both the AutONA agent and the Guessing Heuristic
agent implemented this strategy, which inﬂuenced the oﬀer
generation mechanism of their agent. A concession strategy
might also have a psychological eﬀect on the opponent which
would make it more comfortable for the opponent to accept
agreements or to make concessions on his own as well.
The last feature which was common in several agents is
the use of a database. The database can be built based on
previous interactions with the same human opponent or it
can be built for all opponents. The agent consults the data-
base to better model the opponent, to learn about possible
behaviors and actions and to adjust its behavior to the spe-
ciﬁc opponent. A database of the history can also be used to
obtain information about the behavior of the opponents, if
such information is not known, or cannot be characterized,
in advance.
Lastly, though not exactly a feature, but worth mention-
ing, is that none of the agents we reviewed implemented
equilibrium strategies.
This is an interesting observation
and most likely is due to the fact that these strategies have
been shown to behave poorly when implemented in auto-
mated negotiators matched with human negotiators, mainly
due to the complex environment and the bounded rational-
ity of people. In some cases (e.g., [21]) experiments have
shown that when the automated agent follows its equilib-
rium strategy the human negotiators who negotiate with it
become frustrated, mainly since the automated agent re-
peatedly proposes the same oﬀer, and the negotiation often
ends with no agreement. This has been shown in cases in
which the complexity of ﬁnding the equilibrium is low and
the players have full information.
4.
CONCLUSIONS
In this article we presented the challenges and current
state-of-the-art automated solutions for proﬁcient negotia-
tions with humans. Nonetheless we do not claim that all
existing solutions have been summarized in this article. We
brieﬂy state the importance of automated negotiators and
propose suggestions for future work in this ﬁeld.
4.1
The Importance of Automated Negotiators
The importance of designing an automated negotiator that
can negotiate eﬃciently with humans cannot be understated
and we have shown that indeed it is possible to design such
negotiators. By pursuing non-classical methods of decision
making and a learning mechanism for modeling the oppo-
nent it could be possible to achieve greater ﬂexibility and
eﬀective outcomes. As we have shown, this can also be ac-
complished without constraining the model to the domain.
Many of the automated negotiation agents are not in-
tended to replace humans in negotiations, but rather as an
eﬃcient decision support tool or as a training tool for nego-
tiations with people. Thus, such agents can be used to sup-
port training in real life negotiations, such as: e-commerce
and electronic negotiations (e-negotiations), and they can
also be used as the main tool in conventional lectures or
online courses, aimed at turning the trainee into a better
negotiator.
To this date, it seems that research in AI has neglected the
issue of proﬁciently negotiating with people, at the expense
of designing automated agents aimed to negotiate with ra-
tional agents or other automated agents (e.g., [39]). Others
have focused on improving diﬀerent heuristics and strate-
gies and the analysis of game theory aspects (e.g., [20, 26]).
Nonetheless it is noteworthy that these are important as-
pects in which the AI community has certainly made an
impact. Unfortunately, not much progress has been made
with regard to automated negotiators with people, leaving
many unfaced challenges.
4.2
Suggestions for Future Research
The work is far from being complete and the challenges
are still exciting. To entice the reader, we list a few of these
challenges below.
The ﬁrst challenge is to enrich the negotiation language.
Many researchers restrict themselves to the basic model of

alternating oﬀers whereby the language consists of oﬀers and
counter-oﬀers alone. Rich and realistic negotiations, how-
ever, consist of other types of actions (e.g., threats, com-
ments, promises and queries), as well as simultaneous ac-
tions (that is, each agent can perform up to M > 0 interac-
tions with the other party each time period). It is essential
that these actions and behaviors are modeled in the auto-
mated negotiators to allow better negotiations with human
negotiators.
Another challenge, also discussed in the introduction, is
the need for a general all-purpose automated negotiator.
With the vast amount of applications and domains, auto-
mated agents cannot be restricted to one single domain and
must be adaptable to diﬀerent settings.
The tradeoﬀbe-
tween the performance of a general purpose automated ne-
gotiator and a domain-dependent negotiator should be con-
sidered and methods for improving the eﬃcacy of a general
purpose negotiator should be sought.
Achieving this will
also contribute to the feasibility of comparing between dif-
ferent automated agents when matched with people. Pre-
liminary work on this facet is already under pursuit by Hin-
driks et al. [13] and Oshrat et al. [28], however, we believe
the aspect of generality should be addressed more by re-
searchers.
In this respect, metrics should be designed to
allow a comparison between agents. To achieve this some of
the questions of “what constitutes a good negotiator agent?”
as described in Section 2 should be answered as well.
In addition, argumentation, though, dealt with in the
past, still poses a challenge for researchers in this ﬁeld.
For example, about ten years ago Kraus et al.
[23] pre-
sented argumentation as an iterative process emerging from
exchanges among agents to persuade each other and bring
about a change in intentions. In their work they developed a
formal logic that forms a basis for the development of a for-
mal axiomatization system for argumentation. In particular,
Kraus et al. identiﬁed argumentation categories in human
negotiations and demonstrated how the logic can be used
to specify argument formulations and evaluations. Finally,
they developed an agent which was implemented, based on
the logical model.
However, this agent was not matched
with human negotiators. Moreover, there are several open
research questions associated with how to integrate the ar-
gumentation model into automated negotiators. Since the
argumentation module is based on logic and thus is time
consuming, a more eﬃcient approach should be used. In ad-
dition, the current model is built on a very complex model
of the opponent and therefore should be incorporated in the
automated negotiator’s model of the opponent. In order to
facilitate the design, a mapping between the logical model
and the utility based model is required.
To conclude, in recent years the ﬁeld of automated nego-
tiators that can proﬁciently negotiate with human players
has received much needed focus and the results are encour-
aging. We present several of these automated negotiators
and show that it is indeed possible to design such proﬁcient
agents.
Nonetheless, there are still challenges which pose
interesting research questions that need to be pursued and
exciting work is still very much in progress.
5.
ACKNOWLEDGMENTS
We thank Dr. David Sarne, Dr. Ya’akov (Kobi) Gal and
the anonymous referees for their helpful remarks and sug-
gestions.
6.
REFERENCES
[1] Bargaining negotiations course.
https://www.irwaonline.org/eweb/dynamicpage.aspx?
webcode=205, 2008.
[2] G. Bolton. A comparative model of bargaining:
Theory and evidence. American Economic Review,
81(5):1096–1136, 1989.
[3] A. Byde, M. Yearworth, K.-Y. Chen, and C. Bartolini.
AutONA: A system for automated multiple 1-1
negotiation. In Proceedings of the 2003 IEEE
International Conference on Electronic Commerce
(CEC), pages 59–67, 2003.
[4] G. Charness and M. Rabin. Understanding social
preferences with simple tests. The Quarterly Journal
of Economics, 117(3):817–869, 2002.
[5] A. Chavez and P. Maes. Kasbah: An agent
marketplace for buying and selling goods. In
Proceedings of the ﬁrst international Conference on
the Practical Application of Intelligent Agents and
Multi-Agent Technology, pages 75–90, 1996.
[6] I. Erev and A. Roth. Predicting how people play
games: Reinforcement learning in experimental games
with unique, mixed strategy equilibrium. American
Economic Review, 88(4):848–881, 1998.
[7] J. Farrell and M. Rabin. Cheap talk. Journal of
Economic Perspectives, 10(3):103˝U118, 1996.
[8] S. Ficici and A. Pfeﬀer. Modeling how humans reason
about others with partial information. In Proceedings
of the 7th International Conference on Autonomous
Agents and Multiagent Systems (AAMAS), pages
315–322, 2008.
[9] R. Fisher and W. Ury. Getting to Yes: Negotiating
Agreement Without Giving In. Penguin Books, 1991.
[10] Y. Gal, A. Pfeﬀer, F. Marzo, and B. J. Grosz.
Learning social preferences in games. In Proceedings of
the National Conference on Artiﬁcial Intelligence
(AAAI), pages 226–231, 2004.
[11] J. Grossklags and C. Schmidt. Software agents and
market (in) eﬃciency: a human trader experiment.
IEEE Transactions on Systems, Man, and
Cybernetics, Part C: Applications and Reviews,
36(1):56–67, 2006.
[12] B. Grosz, S. Kraus, S. Talman, and B. Stossel. The
inﬂuence of social dependencies on decision-making:
Initial investigations with a new game. In Proceedings
of 3rd International Joint Conference on Multiagent
Systems (AAMAS), pages 782–789, 2004.
[13] K. Hindriks, C. Jonker, and D. Tykhonov. Towards an
open negotiation architecture for heterogeneous
agents. In 12th International Workshop on
Cooperative Information Agents (CIA), volume 5180
of LNAI, pages 264–279. Springer, 2008.
[14] P. T. Hoppman. The Negotiation Process and the
Resolution of International Conﬂicts. University of
South Carolina Press, Columbia, SC, May 1996.
[15] C. M. Jonker, V. Robu, and J. Treur. An agent
architecture for multi-attribute negotiation using
incomplete preference information. Autonomous
Agents and Multi-Agent Systems, 15(2):221–252, 2007.
[16] R. Katz and S. Kraus. Eﬃcient agents for cliﬀedge
environments with a large set of decision options. In
Proceedings of the 5th International Conference on

Autonomous Agents and Multi-Agent Systems
(AAMAS), pages 697–704, 2006.
[17] R. Katz and S. Kraus. Gender-sensitive automated
negotiators. In Proceedings of the 22nd National
Conference on Artiﬁcial Intelligence (AAAI), pages
821–826, 2007.
[18] R. Keeney and H. Raiﬀa. Decisions with Multiple
Objective: Preferences and Value Tradeoﬀs. John
Wiley & Sons, NY, USA, 1976.
[19] P. Kenny, A. Hartholt, J. Gratch, W. Swartout,
D. Traum, S. Marsella, and D. Piepol. Building
interactive virtual humans for training environments.
In Proceedings of Interservice/Industry Training,
Simulation and Education Conference (I/ITSEC),
2007.
[20] S. Kraus. Strategic Negotiation in Multiagent
Environments. MIT Press, Cambridge MA, USA,
2001.
[21] S. Kraus, P. Hoz-Weiss, J. Wilkenfeld, D. R.
Andersen, and A. Pate. Resolving crises through
automated bilateral negotiations. Artiﬁcial
Intelligence, 172(1):1–18, 2008.
[22] S. Kraus and D. Lehmann. Designing and building a
negotiating automated agent. Computational
Intelligence, 11(1):132–171, 1995.
[23] S. Kraus, K. Sycara, and A. Evenchik. Reaching
agreements through argumentation: a logical model
and implementation. Artiﬁcial Intelligence,
104(1-2):1–69, 1998.
[24] R. Lin, S. Kraus, J. Wilkenfeld, and J. Barry.
Negotiating with bounded rational agents in
environments with incomplete information using an
automated agent. Artiﬁcial Intelligence,
172(6-7):823–851, 2008.
[25] R. D. McKelvey and T. R. Palfrey. An experimental
study of the centipede game. Econometrica,
60(4):803–836, 1992.
[26] A. Muthoo. Bargaining Theory with Applications.
Cambridge University Press, 1999.
[27] Online negotiation course. http://www.negotiate.tv/,
2008.
[28] Y. Oshrat, R. Lin, and S. Kraus. Facing the challenge
of human-agent negotiations via eﬀective general
opponent modeling. In Proceedings of the 8th
International Conference on Autonomous Agents and
Multiagent Systems (AAMAS), 2009.
[29] H. Raiﬀa. The Art and Science of Negotiation.
Harvard University Press, 1982.
[30] E. Rasmusen. Games and Information: An
Introduction to Game Theory. Blackwell Publishers,
2001.
[31] W. Ross and J. LaCroix. Multiple meanings of trust in
negotiation theory and research: A literature review
and integrative model. International Journal of
Conﬂict Management, 7(4):314–360, 1996.
[32] A. Rubinstein. Perfect equilibrium in a bargaining
model. Econometrica, 50(1):97–109, 1982.
[33] A. Rubinstein. A bargaining model with incomplete
information about preferences. Econometrica,
53(5):1151–1172, 1985.
[34] A. Sanfey, J. Rilling, J. Aronson, L. Nystrom, and
J. Cohen. The neural basis of economic
decision-making in the ultimatum game. Science,
300:1755–1758, 2003.
[35] R. Selten and R. Stoecker. End behavior in sequences
of ﬁnite prisoner’s dilemma supergames: A learning
theory approach. Economic Behavior and
Organization, 7(1):47–70, 1986.
[36] M. Tennenholtz. On stable social laws and qualitative
equilibrium for risk-averse agents. In Proceedings of
the 5th International Conference on Principles of
Knowledge Representation and Reasoning (KR-96),
pages 553–561, 1996.
[37] D. Traum, S. Marsella, J. Gratch, J. Lee, and
A. Hartholt. Multi-party, multi-issue, multi-strategy
negotiation for multi-modal virtual agents. In
Proceedings of the 8th International Conference on
Intelligent Virtual Agents, 2008.
[38] A. Tversky and D. Kahneman. The framing of
decisions and the psychology of choice. Science,
211:453–458, 1981.
[39] M. P. Wellman, A. Greenwald, and P. Stone.
Autonomous Bidding Agents: Strategies and Lessons
from the Trading Agent Competition. MIT Press,
Cambridge MA, USA, 2007.
[40] X. Zhang, V. Lesser, and R. Podorozhny.
Multi-dimensional, multistep negotiation for task
allocation in a cooperative system. Autonomous
Agents and MultiAgent Systems, 10(1):5–40, 2005.

