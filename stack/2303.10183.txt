

Citation: Salmaso, F.; Trisolini, M.;
Colombo, C. A machine learning and
feature engineering approach for the
prediction of the uncontrolled
re-entry of space objects. Preprints
2022, 1, 0. https://doi.org/
Publisher’s Note: MDPI stays neutral
with regard to jurisdictional claims in
published maps and institutional afﬁl-
iations.
Copyright:
© 2022 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open access article
distributed
under
the
terms
and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
Article
A machine learning and feature engineering approach for the
prediction of the uncontrolled re-entry of space objects
Francesco Salmaso 1, Mirko Trisolini
1* and Camilla Colombo
1
1
Department of Space Science and Technology, Politecnico di Milano, via La Masa 34, 10156, Milan, Italy
*
Correspondence: mirko.trisolini@polimi.it
Abstract:
The continuously growing number of objects orbiting around the Earth is expected to be
accompanied by an increasing frequency of objects re-entering the Earth’s atmosphere. Many of these
re-entries will be uncontrolled, making their prediction challenging and subject to several uncertainties.
Traditionally, re-entry predictions are based on the propagation of the object’s dynamics using state-of-
the-art modelling techniques for the forces acting on the object. However, modelling errors, particularly
related to the prediction of atmospheric drag may result in poor prediction accuracies. In this context,
we explore the possibility to perform a paradigm shift, from a physics-based approach to a data-driven
approach. To this aim, we present the development of a deep learning model for the re-entry prediction
of uncontrolled objects in Low Earth Orbit (LEO). The model is based on a modiﬁed version of the
Sequence-to-Sequence architecture and is trained on the average altitude proﬁle as derived from a set
of Two-Line Element (TLE) data of over 400 bodies. The novelty of the work consists in introducing in
the deep learning model, alongside the average altitude, three new input features: a drag-like coefﬁcient
(B∗), the average solar index, and the area-to-mass ratio of the object. The developed model is tested on
a set of objects studied in the Inter-Agency Space Debris Coordination Committee (IADC) campaigns.
The results show that the best performances are obtained on bodies characterised by the same drag-like
coefﬁcient and eccentricity distribution as the training set.
Keywords: re-entry predictions; machine learning; deep learning; features engineering; uncontrolled
re-entry
1. Introduction
From the launch of the ﬁrst artiﬁcial satellite on the 4th of October 1957, the population of
bodies orbiting around the Earth has been continuously growing with an exponential trend
[1]. Furthermore, in the last years, this growth has been exacerbated by the creation of large
constellations for telecommunication, navigation and global internet coverage. Post-mission
disposal guidelines [2,3] have been introduced in order to preserve the Low Earth Orbit (LEO)
region and incentivise the early disposal of satellites so that they would not affect the already
polluted orbital environment. Focusing on the re-entry problem, as of March 2022, a total of
26135 bodies have re-entered the atmosphere1. According to Pardini et al. [5], since April
2013, only one out of ﬁve re-entries of intact objects was controlled, corresponding to a total
mass of roughly 11000 metric tonnes, mainly related to rocket bodies and spacecraft. Therefore,
the continuously growing number of orbiting objects should be correlated with an expected
increase in the re-entries of both controlled and uncontrolled objects. However, predicting the
re-entry epoch and the location of an uncontrolled body remains critical because it is affected
by several uncertainties, which could translate into risks of casualties on the ground [5].
In the typical dynamics-based approach to the re-entry prediction problem, the trajectory
of the spacecraft is propagated until it reaches the altitude when the break-up occurs. This
1
www.space-track.org
arXiv:2303.10183v1  [cs.LG]  17 Mar 2023

2 of 26
approach consists of determining the initial position of the object and accurately modelling
the forces acting on it in order to predict the evolution of its trajectory. However, in general,
it is difﬁcult to accurately model the forces acting on the spacecraft and to precisely know its
physical characteristics and state. In fact, a typically recommended relative uncertainty value of
±20% is recommended for re-entry predictions [6,7]. [5] give an in-depth review of the different
sources of uncertainties in re-entry predictions, the most relevant being the modelling of the
drag acceleration. Other forces affect the re-entry dynamics such as solar radiation pressure
and the lunisolar attraction. However, in this work, we focus on the latest stage of the re-entry
prediction of low eccentricity orbits; therefore, the dominant force is atmospheric drag and the
main uncertainties are related to the atmospheric density. It is modelled through empirical
models which describe the density variations on spatial and temporal scales [8–12]. However,
they are affected by two types of uncertainties [11]: the simpliﬁed physical modelling on which
they are based; and the uncertainties and complexities in predicting the space weather, in
particular the solar index. Another important variable for the predictions of low eccentricity
orbits is the ballistic coefﬁcient of the object of interest. The ballistic coefﬁcient depends on
the object’s shape, orientation, and attitude, which can be difﬁcult to accurately know for
uncontrolled objects.
In this circumstance, it becomes interesting to adopt a paradigm shift to model the problem,
from a physical to a data-driven point of view. In this context, machine learning represents
a new approach, where, instead of accurately modelling the space environment, its forces
and how the spacecraft interacts with them, it autonomously builds its knowledge based
on data, autonomously dealing with the complexities and uncertainties of the phenomenon.
Jung et al. [13] proposed a machine learning approach to predict the re-entry epoch of an
uncontrolled satellite based on Recurrent Neural Networks (RNNs). In their work, the RNN
is trained on a set of Two-Line Elements (TLEs) sets using as the only feature the evolution
of the average altitude of the object. TLE sets provide, among other information, the time
evolution of the position of catalogued objects in orbit. However, as also addressed by Lidtke
et al. [14], TLEs suffer from some inaccuracies that must be taken into account. Most notably,
TLEs of an object are not homogeneous; they can be provided at irregular time steps and
contain incorrect observations or observations belonging to other objects (outliers). In addition,
TLEs do not directly provide information on the physical characteristics of the object (e.g., the
ballistic coefﬁcient), rather, they combine all the uncertainties and modelling errors in a single
drag-like coefﬁcient, referred to as B∗[9,10]. Finally, TLEs do not contain accuracy information
on the measure they provide that is they do not provide covariance information. However,
TLE sets are currently the only public source of data that can be used for re-entry predictions.
Therefore, despite their limitations and inaccuracies, they are used in this work to assess the
capabilities of a machine-learning approach using multiple input features. As shown in [15],
initial covariance information can be estimated using pseudo-observations, and the accuracy of
TLE data can be improved as shown [16,17]. While in this work we focus on the application of
a machine learning technique to raw TLE data, future work can beneﬁt form the inclusion of
such information to increase the accuracy of the re-entry predictions.
Starting from the work of Jung et al. [13], we expand on the machine learning model
introducing additional features, namely the drag-like coefﬁcient, B∗, and the solar index. The
ﬁrst feature will allow the machine learning model to account for the physical characteristics of
the objects, while the second feature will give a temporal dimension to the model, accounting
for the different solar activity levels and how they affect the lifetime of an object. The developed
model is trained on hundreds of TLE sets belonging to uncontrolled objects such as spent
payloads and rocket bodies, and debris. Consequently, the same model can be tested on
different objects, in order to obtain the output sequence and assess its accuracy.

3 of 26
The paper is structured as follows. Section 2 describes the deep learning model architecture
adopted, Section 3 contains the procedure used for the pre-processing and ﬁltering of the data
required by the deep learning model, Section 4 contains an analysis of the characteristics of the
input features of the model, and Section 5 shows the results obtained by the model on selected
test cases, together with the optimisation procedure of the model’s hyperparameters.
2. Deep learning model
From a machine learning perspective, the task of predicting the re-entry of an object is
a time-series problem, where the aim of the model consists in predicting the evolution of the
trajectory based on an initial set of conditions so that the re-entry epoch can be determined.
In other words, it is a regression problem. Time series are discrete-time data and can be
considered as a particular case of sequential data, where each value is associated with a time
stamp. Therefore, the order of the series is imposed by the time dimension and it must be
preserved. Let us denote an input sequence of length Tx as [18]:
x = {x<1>, x<2>, ..., x<Tx>}
(1)
where the vector x<t> ∈Rp contains the input features at time step t. Furthermore, let us
denote a target sequence of length Ty as [18]:
y = {y<1>, y<2>, ..., y<Ty>}
(2)
where y<t> ∈R is the target feature. We deﬁne the predicted sequence as [18]:
ˆy = { ˆy<1>, ˆy<2>, ..., ˆy<Ty>}
(3)
The aim of the deep learning problem consists in predicting the target sequence, min-
imising a loss function, that in this work has been selected as the Mean Squared Error (MSE)
[18]:
LMSE(y, ˆy) = 1
Ty
Ty
∑
t=1
 y<t> −ˆy<t>2
(4)
The choice of the MSE is natural in this context since the similarity with the Euclidean
distance. Indeed, the MSE can be intuitively considered as the squared distance between the
true position and the predicted one, averaged by the number of observations in the output
sequence.
2.1. Sequence-to-Sequence architecture
In this work, we selected a Sequence-to-Sequence (Seq2Seq) [19] architecture to model
the re-entry problem, due to its ability to deal with input and output sequences of different
lengths. As shown in Fig. 1 , a Seq2Seq architecture is composed of two neural networks, the
encoder and the decoder, and the context vector. The encoder receives as input a sequence of
arbitrary length, Tx, and generates a ﬁxed-length vector, called the context vector. This vector
summarises the information contained in the input sequence. The outputs of the decoder are
simply discarded. At each time step, the cell produces the hidden state considering the input at
the same time step and the hidden state at the previous time step [20]:
h<t>
Enc = f (x<t>, h<t−1>
Enc
, θ)
(5)
where h<t>
Enc is the hidden state of the encoder at time t, h<t−1>
Enc
is the hidden state at the
previous time step, x<t> is the input vector at the current time, and θ is the weights vector.
The function f represents a neural network, which can be a simple Recurrent Neural Network

4 of 26
(RNN) [21,22] or a more complex Gate Recurrent Unit (GRU) [20,21,23] or a Long-Short Term
Memory (LSTM) [20,21] network. At the ﬁnal time step, Tx, the hidden state of the encoder is
deﬁned as the context vector as follows:
ccontext = h<Tx>
Enc
(6)
The decoder is initialised with the ﬁnal state of the encoder, h<t′=0>
Dec
= ccontext. The aim of
the decoder is to generate the output sequence based on this encoded vector. At each time step
t′ ∈[1, Ty], given the hidden state and the output at the previous time step t′ −1, the decoder
hidden state can be deﬁned as follows [20]:
h<t′>
Dec
= g( ˆy<t′−1>, h<t′−1>
Dec
, θ′)
(7)
where again g(·) can be a simple RNN, a GRU or an LSTM. Subsequently, it is possible to
obtain the output ˆy<t′> by passing the corresponding output of the decoder to an output layer.
Note that the hidden state at time t′ = Ty is simply discarded.
During training, the decoder receives its own previous hidden state and the true input, or
the predicted output at the previous time step, to generate a new state and output sequentially.
When only the "true input" is provided to the decoder, the training method is referred to as
Teacher Forcing [21]. Therefore, in this method, the predicted output of the model is substituted
with our reference data, which is considered the ground truth. However, this method introduces
a discrepancy between the training phase and the inference phase in which the network is used
in open-loop, which can introduce errors in the prediction as the network is not able to learn
from its own mistakes. To mitigate this problem, Bengio et al. [24] proposed an approach called
Curriculum Learning. It consists of randomly selecting either the previous true output or the
model estimate, with a probability that is proportional to the training epoch. Intuitively, at the
initial epochs of training, the model should be trained with teacher forcing since it is not still
well trained and errors can be made. Instead at the ﬁnal epochs, the model is expected to be
well trained so that it is possible to use the model output, as during inference. Speciﬁcally, for
every output to predict y<t′> of every batch at the j-th epoch, a coin is ﬂipped to decide to feed
either the previous true output, associated with a probability ϵj, or the model estimate, with a
probability (1 −ϵj). Given the fact that in this context the size of the dataset is quite small and
the training epoch number is large, the probability is set as a function of the training epoch,
instead of the iteration [24], as:
ϵj = kj
(8)
where k < 1 is a constant that depends on the speed of convergence, which is the decay
rate of the scheduled sampling.
The representation of the Seq2Seq model during training is provided in Fig. 1, where x<t>
denotes each time component of the input sequence; ˆy<t> the predicted output; and y<t> the
ground truth.
2.1.1. Gate Recurrent Unit
In this work, both the encoder and the decoder are based on Gate Recurrent Units (GRUs)
[23], given their capability of dealing with the vanishing gradient problem that characterises
simple RNN when trained with long sequences. It can be seen as a simpliﬁed version of
the LSTM and it has the advantage to reduce the computational cost, providing comparable
performances [25]. At each time step, the GRU computes:

5 of 26
GRU
GRU
GRU
GRU
Context Vector
GRU
GRU
GRU
GRU
Dense
Dense
Dense
Dense
Select
Select
Select
x<1>
x<2>
x<Tx−1>
x<Tx>
ˆy<1>
ˆy<2>
ˆy<Ty−1>
ˆy<Ty>
y<1>
y<Ty−2>
y<Ty−1>
Figure 1. Representation of Seq2Seq architecture during training.
z<t> = σ

Wzx<t> + Vzc<t−1> + bz

(9)
r<t> = σ

Wrx<t> + Vrc<t−1> + br

(10)
˜c<t> = tanh

Wcxt + Vc

r<t> ⊙c<t−1>
+ bc

(11)
c<t> =
 1 −z<t> ⊙˜c<t> + z<t> ⊙c<t−1>
(12)
where Wz, Wr, Wc ∈Rk×p denote the input weight matrices; Vz, Vr, Vc ∈Rk×k are the
hidden state weights; bz, br, bc ∈Rk are the bias vectors; ⊙the element-wise product; σ(·)
and tanh(·) are the sigmoid function and the hyperbolic tangent function, respectively. The
variables z<t> ∈Rk and r<t> ∈Rk represent the update and reset gate respectively, which
helps to tackle the vanishing gradient problem. Their aim consists in controlling the quantity of
information of the past time step that needs to be carried forward. In particular, the update gate
is responsible for updating the old state, by controlling the similarity between the new state
and the past one. The reset gate is responsible for setting the amount of information that needs
to be forgotten. For these reasons, the values of the gates are designed to be bounded between
0 and 1, by using the sigmoid function. The output hidden state is computed in two steps.
First, the candidate memory cell ˜c ∈Rk is calculated according to Eq. (11). It takes as input the
previous hidden state, which is multiplied by the reset gate, and the current input. Depending
on the entries of the reset gate, two limit cases can be identiﬁed. When its values are close to
unity, the information of the previous time step is carried forward in time, like a simple RNN.
Instead, when the values are close to 0, the information from the previous time step is ignored.
After having derived the candidate memory cell, the variable c ∈Rk, called memory cell, can
be computed according to Eq. (12). The memory cell receives as input the current candidate
and the previous memory cell and it is governed by the reset gate only. In this case, when
the entries of the reset gate are close to 0, the new memory cell is updated with the candidate,
meaning that the information of the previous time step is dropped. Instead, when the values of
the gate approach to unity, the candidate is simply discarded and the memory cell maintains
its state. Finally, the hidden state and the output of the GRU can be deﬁned as, respectively:

6 of 26
h<t> = c<t>
context
(13)
ˆo<t> = c<t>
context
(14)
The computational ﬂow diagram of a GRU is summarised in Fig. 2.
σ
⊙
+
⊙
⊙
σ
tanh
c<t−1> = h<t−1>
c<t> = h<t>
x<t>
Update gate
Reset gate
1−
Figure 2. Computational ﬂow of GRU architecture.
The Seq2Seq model is built on TensorFlow [26], a widely used open-source library for
machine and deep learning, developed by Google. TensorFlow automatically derives the
gradients with respect to the weights of the model using auto-differentiation, making the
training of the model a straightforward task. The high-level interface to TensorFlow is provided
by Keras [27], a Python library that allows building, training, and testing neural networks.
3. Data pre-processing
The raw data used to train and validate the deep learning model has been retrieved
from Space Track1. Speciﬁcally, all the decayed objects from the 1st of January 2000 to the 7th
of October 2021, for which the Tracking and Impact Prediction (TIP) is available, have been
considered. The TLE data is retrieved in the form of Orbital Mean-Elements Messages (OMM);
once the raw TLE data is available, it is necessary to control the quality of the data and prune
possible outliers. Following [14], the outliers can be found by focusing on the mean motion
and B∗coefﬁcient. The main steps of the pruning procedure are as follows:
1.
Identify the presence of TLE corrections by deﬁning a time threshold such that the
previous observation is considered incorrect. A common time threshold is half an orbit.
2.
Identify large time intervals between consecutive TLEs, and, consequently, divide the
entire set into different windows in order to ensure TLEs consistency.
3.
Find and ﬁlter out single TLEs with discordant values of the mean motion, based on a
robust linear regression technique and on a sliding window of ﬁxed length. In particular,
the results obtained through regression are compared with the OMM following the sliding
window and an outlier is identiﬁed if a relative and an absolute tolerance are exceeded
(optimal tuning parameters for the ﬁlter can be found in Table 3 of [14]).

7 of 26
4.
Identify and ﬁlter out possible outliers in eccentricity and inclination, using a statisti-
cal approach. Particularly, the mean is computed within a sliding window, and it is
subtracted from the central orbital element of the interval, obtaining a time series of
differences. Afterwards, using another sliding window that scans the aforementioned
series of differences, the mean absolute deviation is computed. An outlier is removed
if the orbital element presents a difference from the mean that is higher than a given
threshold ((optimal tuning parameters for the ﬁlter can be found in Table 4 and Table 5 of
[14]).
5.
Filter out TLEs that present negative values of B∗because they can be associated with
unmodelled dynamics or manoeuvres.
For a complete description of the ﬁlters and their performance, as well as the optimal
tuning of the ﬁlter parameters, the reader is referred to the work of Lidtke et al. [14]. It is
worth noting that some outliers may not be identiﬁed by such ﬁlters; however, the percentage
of outliers is considered to be sufﬁciently small to ensure the validity of the data. After the
pruning process, the mean elements contained in the TLEs are converted to the corresponding
osculating variables using the SGP4 propagator [10]. It is also important to consider other
characteristics of the dataset. Speciﬁcally, we want to focus on re-entries that have a sufﬁcient
number of TLE points, whose re-entry prediction is sufﬁciently accurate. The minimum number
of TLE points has been selected via a trade-off between the number of pruned TLEs and the
accuracy of the ﬁtting procedure (see Section 4.1) [29]. In addition, the orbit should not be
too eccentric as the re-entry mechanism for these types of orbits changes signiﬁcantly. The
following criteria have been used to ﬁlter the dataset:
•
Maximum re-entry uncertainty: 20 min.
•
Maximum average altitude of the initial TLE: 200 km.
•
Minimum average altitude of the ﬁnal TLE: 180 km.
•
Maximum eccentricity: 0.1.
•
Minimum number of points: 4 TLE.
The described criteria adopt of the average altitude, h, that is computed according to:
h = a −R⊕,
(15)
where a is the osculating semi major axis; and R⊕the Earth radius. The result of this
pruning and ﬁltering procedure is a dataset with a total of 417 bodies, which is then split in 80%
training and 20% validation (see Appendix A. Note that the objects in the validation set must
resemble the characteristics of the training set. For example, the average, µ, and the relative
standard deviation, σ, of the B∗coefﬁcient and the eccentricity for the training and validation
sets should be comparable (Table 1).
Variable
Training Set
Validation Set
Ecentricity [-]
µ
7.2842 · 10−3
5.8856 · 10−3
σ
9.3963 · 10−3
7.1684 · 10−3
B∗[1/ER]
µ
6.3362 · 10−4
5.9503 · 10−4
σ
5.4924 · 10−4
3.8341 · 10−4
Table 1. B∗and eccentricity characteristics for the training and validation sets.
Moreover, the re-entry epochs are distributed quite uniformly from roughly January 2005
to April 2021 for both the sets, therefore the different objects experience different solar ﬂux
conditions and, therefore, similar values of drag acceleration (Fig. 3). Finally, it should be
highlighted that the majority of the bodies in the training and validation sets is characterised

8 of 26
by an uncertainty in the re-entry epoch of less than 2 minutes.
100
150
200
250
F10.7
F10.7
Re-entry Epoch in Training Set
2005-01-01
2010-01-01
2015-01-01
2020-01-01
Time (utc)
100
150
200
250
Re-entry Epoch in Validation Set
F10.7, F10.7 [sfu]
Figure 3. Re-entry epochs in training (Top) and validation (Bottom) set and Solar Flux indexes.
The objects for testing the prediction capabilities of the model are selected among the
bodies analysed during IADC campaigns, as listed in Table 2. Note that the re-entry epochs of
23853 and 19045 are manually changed to, respectively, 16:25 UTC and 15:08 UTC [7].
NORAD
Name
Re-entry Epoch [UTC]
23853
COSMOS 2332
2005-01-28T18:05*
26873
CORONAS F
2005-12-06T17:24
10973
COSMOS 1025
2007-03-10T12:56
31599
DELTA 2 R/B
2007-08-16T09:23
31928
EAS
2008-11-03T04:51
20813
MOLNIYA 3-39
2009-07-08T22:42
11601
SL-3 R/B
2010-04-30T16:44
21701
UARS
2011-09-24T04:00
20638
ROSAT
2011-10-23T01:50
37872
PHOBOS-GRUNT
2012-01-15T17:46
19045
COSMOS 1939
2014-10-29T15:32*
40138
CZ-2D R/B
2015-06-13T23:58
39000
CZ-2C R/B
2016-06-27T19:04
38086
AVUM
2016-11-02T04:43
37820
TIANGONG 1
2018-04-02T00:16
Note: the epochs denoted with * are manually changed according
to the indications of the IADC campaigns.
Table 2. List of test objects derived from TIP.
4. Features engineering
In this section, we outline the processes to select and pre-process the most important
input features required to predict the re-entry epoch by using a machine learning model.
However, this is not a straightforward task because it is necessary to ﬁnd input variables that

9 of 26
are correlated with the output of the deep learning model. Therefore, it is essential to analyse
each feature from a physical point of view, so that this knowledge can be then applied to
constructing the deep learning model, which is then going to beneﬁt from each input variable.
This process is typically known as feature engineering. In this optic, the deep learning model is
considered a physical model, where, given the input variables, the output is obtained by means
of physical relations. Instead, in machine learning, the model is treated as a black box, which is
autonomously capable of learning the input-output relations, based on the true values and the
input features. Therefore, each feature can be selected and generated based on the knowledge
of the physical principles that govern the re-entry phenomenon, leaving the deep learning
model to cope with the learning, and the uncertainties of the physical problem. By analysing
each feature, the relations between the considered variables can be highlighted so that we can
make a more informed decision on whether the considered feature can be beneﬁcial to the deep
learning model [28,29].
In addition, the raw data, provided by the TLEs requires a pre-processing and regular-
isation step in order for it to be used with an RNN. In fact, TLE data is characterised by a
non-uniform generation frequency, which differs from object to object; it also lacks high accu-
racy and may present outliers. All these aspects must be taken into account before feeding the
data to the RNN. The following sections analyse the contribution of the features considered in
this work, which are the average spacecraft altitude, the ballistic coefﬁcient, the solar index,
and the B∗coefﬁcient.
4.1. Average altitude
The average altitude feature is of fundamental importance for deep-learning-driven re-
entry prediction. In fact, it is related to the deﬁnition of the loss function used to train the
RNN Eq. (4). This means that it is used both as input and as output for the deep learning
model, such that given a part of the average altitude proﬁle as input, the remaining part is
predicted. The pre-processing of the average altitude proﬁle follows the work of Jung et al.
[13]. In particular, each trajectory is generated by ﬁtting the available TLEs, below 240 km,
according to the following equation:
f (t) = a1 + a2

tre f −t
 1
2 + a3

tre f −t
 1
3
+ a4

tre f −t
 1
4
(16)
where tre f is the re-entry time, and a1, a2, a3 and a4, are the parameter obtained through
curve ﬁtting using the Levenberg-Marquardt algorithm, which is available through the python
library SciPy [30]. The ﬁrst coefﬁcient, a1, is equal to 80 km, which is the reference altitude at
which TIP messages are provided [7]. This assumption avoids the ﬁtting algorithm changing
the re-entry epoch from the TIP message and, therefore, ensures consistency with the TLE data.
Subsequently, the epoch corresponding to an altitude of 200 km is set as the initial epoch so
that the time span from the starting altitude of 200 km to the re-entry at 80 km, identiﬁes the
residual time.
As each object will have different residual times, it is convenient to interpolate the residual
time as a function of the average altitude. To do so, we subdivide the considered altitude range
(between 200 km and 80 km) into a uniform grid of 25 points. Consequently, each trajectory is
identiﬁed by a series of observations {xi, ti}, with i = 1, 2, ... 25, where xi represents the average
altitude and ti the relative epoch. In this way, it is possible to use only ti as the output of the deep
learning model. Fig. 4 shows a result of such a procedure for the object identiﬁed by NORAD
27392. The resulting trajectories are represented for the training, validation, and test sets are

10 of 26
represented in Fig. 5. It can be seen that there is a concentration of trajectories characterised by
residual lifetimes between 0.5 days and 3 days. Meanwhile, only 3 trajectories with residual
lifetimes higher than 6 days are present, and only two of them are used for training. This is
important because this distribution incorporates the ballistic coefﬁcient information and it can
be directly related to the prediction capability of the model. We also note that in the test set,
the leftmost trajectory has a residual time of roughly 14 minutes and it is associated with the
Molniya 3-39 spacecraft. In this case, the trajectory is highly elliptical and does not satisfy our
selection criterion deﬁned in Section 3; however, it has been included in the test set to assess
the capabilities of the model with such trajectories.
2
4
6
8
10
12
Julian Day [-]
+2.4581000000×106
75
100
125
150
175
200
225
250
Average Altitude [km]
TLE
240 km Limit
Fitted Trajectory
(a)
0.0
0.5
1.0
1.5
2.0
Day from 200 km Epoch [-]
80
100
120
140
160
180
200
Average Altitude [km]
Fitted Trajectory
(b)
Figure 4. Example of ﬁtted (a) and sampled (b) re-entry trajectory for the object with NORAD number
27392.
In a real case scenario where the entire trajectory is not known a priori, Eq. (16) can be used
with tre f as the epoch of the last available TLE. However, the resulting curve will be different
from the one obtained with all the available TLEs. We thus expect to observe some differences
in performance between the training and testing sets. Further analysis should be carried out
on this aspect because there may be variations in the predicted trajectory that depends on the
adopted procedure and on the number of available TLEs. However, this difference should be
minimised when a high number of TLE points is available, which is the case, for example, of
the objects analysed during IADC campaigns.
4.2. Ballistic coefﬁcient and B*
The ballistic coefﬁcient, B, has a key importance on the re-entry of low eccentricity orbits
because the dynamic is dominated by the atmospheric drag perturbation. The ballistic coefﬁ-
cient, as deﬁned in Eq. (17) [31], describes the aerodynamic interaction of the spacecraft with
the atmosphere via its shape, mass, and drag coefﬁcient.
B = CD
A
m
(17)
However, as we are using TLE data for the re-entry prediction, we use the drag-like
coefﬁcient, B∗, as a proxy of the ballistic coefﬁcient. In fact, B∗is the coefﬁcient introduced in
the SGP4 propagator, which in turn is used for the generation of TLE data. The expression for
the B∗is as follows [10]:
B∗= 1
2
CDA
m
ρoR⊕
(18)

11 of 26
Figure 5. Trajectories generated with the ﬁtting and sampling procedure for the training (top), validation
(middle), and test (bottom) sets. Darker shadings are associated to higher concentrations of trajectories
for a given re-entry time.
where ρ0 is the atmospheric density at the perigee of the orbit, assumed as 2.461 ×
10−5 kg/m2/ER, with ER = 6375.135km; and R⊕the Earth radius of 6378.135 km. In general,
the B∗coefﬁcient of the SGP4 formulation soaks up different modelling errors. Despite the
ballistic coefﬁcient can be retrieved from B∗, they are not identical. However, in this context,
we assume B∗can be used as a proxy of B as we are considering objects with similar orbital
parameters that will be subject to similar modelling uncertainties. Similarly to the average
altitude, it is essential to pre-process the B∗data in order to feed them to the RNN. The ﬁrst
step of the processing consists of deﬁning a sliding window with a ﬁxed size to create a series
of average values, in which each term is given by the average of all the previous points. The
second step consists in applying a linear piece-wise interpolation scheme, in which, between
two consecutive values, the preceding one is kept constant. Subsequently, the interpolated
curve is sampled at the epochs deﬁned through the average altitude processing. An example
of the aforementioned procedure is represented in Fig. 6 for NORAD 27392.
Given the features generated so far, it is possible to apply the PCA to observe if the
expected relations between the B∗, the residual time in days and average altitude exist. The
results of the analysis are summarised in Fig. 7. Focusing on the loading plot and the PC1,
the feature related to the time has a strong positive loading; meanwhile, the others have
negative loadings. Indeed, the time feature can be seen as a synonym of the residual lifetime,
therefore as the B∗increases, the lifetime of an object decreases due to the drag acceleration
growth. Furthermore, the altitude-time relation is associated with the altitude decrease, as time
proceeds.
4.3. Solar Index
The Solar Index, F10.7, is a parameter that describes the intensity of solar radiation. It is
important for predicting the variations of the atmospheric density [10]. The data from TLEs
do not include the atmospheric density; however, the B∗accounts also for the solar index
variations. To highlight this dependence, Fig. 8 represents the drag-like coefﬁcient as a function

12 of 26
4
6
8
10
12
Julian Day [-]
+2.4581000000×106
0.0002
0.0003
0.0004
0.0005
0.0006
B∗[1/ER]
2458110
0.0005
B∗from TLEs
Averaged B∗
Interpolated Curve
Sampled
Figure 6. Representation of the pre-processing of the B∗feature generation: step-wise linear interpolation
with moving window average.
1
2
3
Principal Component Number [-]
0.0
0.2
0.4
0.6
0.8
1.0
Explained Variance [-]
Cumulative
Individual
−1
0
1
PC1
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
PC2
B∗
Average Altitude
Days
Figure 7. Variance plot (Left) and loading plot (Right) with B∗, residual time and altitude.
of the last 81-day average of the solar index F10.7 for different bodies. It can be observed that,
as F10.7 increases, the B∗growths with a non-linear behaviour, showing the existence of a
relation between the variables. In addition, this is also highlighted by the same colours of B∗
groups for the same values of F10.7, and by the terms of the curve ﬁtting associated with F2
10.7.
Therefore, the F10.7 index is introduced as a feature of the RNN, along with the area-to-mass
ratio. This decision stems from the fact that the model must deal with objects with similar
B∗but completely different solar indexes. From an implementation point of view, it has been
found that including only the F10.7 index corresponding to the starting epoch of the prediction
led to the highest performance for the RNN. Therefore, it is included and repeated in the input
sequence as a constant value. The same procedure is applied to the area-to-mass ratio, which is
kept ﬁxed.
Similarly to Sections 4.1 and 4.2, it is possible to explore the features generated so far
in order to ﬁnd if the expected relations can be found. To this aim, the PCA can be applied
considering the F10.7 index, the B∗and the area-to-mass ratio. The results are summarised in

13 of 26
0.0000
0.0005
0.0010
B∗[1/ER]
TIANGONG 1
B∗
Fitted 7.29e-05 + 1.56e-08 F2
10.7
0.0000
0.0005
0.0010
B∗[1/ER]
ISS (ZARYA)
B∗
Fitted -4.93e-06 + 1.44e-08 F2
10.7
80
100
120
140
160
F10.7 [sfu]
0.0000
0.0005
0.0010
B∗[1/ER]
HUBBLE SPACE TELESCOPE
B∗
Fitted -4.33e-05 + 1.12e-08 F2
10.7
Sep 2011
Apr 2013
Dec 2014
Jul 2016
Apr 2018
Date [-]
Figure 8. B∗as a function of the mean solar index for Tiangong 1, ISS and HST.
Fig. 9. From the loading plot, it can be seen that, along PC1, all the variables are characterised
by positive loadings. In particular, the relation between the A/m and the B∗is related to the
drag-like coefﬁcient deﬁnition. The relation between the B∗and the solar index is the same as
discussed in Fig. 8, where a positive correlation is present. Moreover, the relation between the
area-to-mass ratio and the F10.7 can be explained through the B∗.
1
2
3
Principal Component Number [-]
0.0
0.2
0.4
0.6
0.8
1.0
Explained Variance [-]
Cumulative
Individual
−1
0
1
PC1
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
PC2
B∗
F10.7
A/m
Figure 9. PCA characteristics of Solar index, B∗and area-to-mass ratio.
4.4. Data regularisation
The features generated so far include the average altitude proﬁle, B∗, the F10.7 and the
area-to-mass ratio. Due to their nature, the solar index and time-related feature are normalised
according to [18]:

14 of 26
¯x =
x −min(x)
max(x) −min(x)
(19)
where ¯x represents the normalised feature; and x the related original input feature.
5. Results
The Seq2Seq architecture described in Section 2 is independently trained on four different
cases, characterised by different starting altitudes, as shown in Table 3. In this way, the
sensitivity of the prediction accuracy with respect to the starting altitude can be assessed.
Case
Tx [-]
Starting
alti-
tude [km]
Training
epochs [-]
A
5
180
2900
B
9
160
3000
C
13
140
1200
D
17
120
1200
Table 3. Characteristics of training cases.
In each one of the cases in Table 3, the entire set of TLEs is split into two independent
parts, the training and validation sets. Furthermore, both the inputs and outputs of the two
sets have the same lengths. Therefore, the model is trained to predict the output sequence
of length Ty, given an input sequence of length Tx, and the same strategy is also applied for
validation. This means that the losses, LVal
MSE and LTrain
MSE, are computed on the values associated
with the same average altitudes. The only difference is represented by the objects analysed,
which are different between the sets. The input dataset is mathematically represented by a
tensor of rank 3, which, in TensorFlow’s notation, has dimension [Objects × Time × Features].
Therefore, according to the problem deﬁnition, the model is faced only with a part of the input
trajectory of the objects, together with all the related input features. The length of the time
dimension can be chosen according to the desired needs, by varying the value of Tx, which is
associated with the starting altitude of the prediction. Hence, the ﬁve different cases in Table 3
are identiﬁed by different lengths of the input sequence Tx. Regarding the output, the model
has to predict the ﬁnal part of the average trajectory altitude until the re-entry epoch. The sum
of the input and output sequence is Tx + Ty = 25.
5.1. Hyperparameters optimisation
The deep learning model described in Section 2 requires the deﬁnition of speciﬁc hyper-
parameters that are variables on which the learning algorithm relies for its optimisation and
prediction processes. In general, hyperparameters may be different depending on the problem
in exam; therefore, it is necessary to perform an informed selection of such parameters. This is
usually achieved by optimising the hyperparameters in order to maximise the performances of
the deep learning model. However, the hyperparameters optimisation process can be computa-
tionally intensive; therefore, for this work, it has been decided to perform the optimisation only
of Case A of Table 3. In fact, this case is characterised by the smallest input sequence and is
therefore considered the most challenging of the four cases. The optimisation is designed using
HyperOpt [32] module, that is Bayesian optimisation library, together with the Asynchronous
Successive Halving (ASHA) algorithm [33] to combine random search with the early stopping
of hyperparameters combinations that exhibit poor performances. These algorithms can be
conveniently combined and parallelised using the Python package Ray Tune [34]. In this work,
part of the hyperparameter space has been kept constant and another part has been optimised.
Table 4 shows the ﬁxed hyperparameters and their associated value. As shown in Eq. (4),

15 of 26
the loss function is the Mean Squared Error; the selected optimiser is Adam [35], which is a
commonly used algorithm for deep learning model weights optimisation. It is a ﬁrst-order
gradient-based optimisation, based on adaptive estimates of ﬁrst and second-order moments
(i.e., mean and variance). As speciﬁed in Table 4, the decay rate of the ﬁrst order moment, β1,
and of the second order moment, β2, have been set, together with the Clipnorm parameters,
which forces a re-scaling of the gradient whenever its norm exceeds the speciﬁed value.
Parameter
Value [-]
Fixed
Loss
Mean Squared Error
Optimiser
Adam
β1
0.999
β2
0.999
Clipnorm
0.1
Table 4. Fixed hyperparameters.
The remaining hyperparameters that are optimised are summarised in Table 5, together
with the relevant search spaces and sampling mode. The parameter related to the layer is
deﬁned as the number of stacked GRU units of the encoder and decoder, respectively. This
means that if the number of layers is 2, the model is going to be composed of 4 units, where
2 are associated with the encoder and the remaining with the decoder. Speciﬁcally, the ﬁnal
hidden state of each layer of the encoder is passed to their respective layer of the decoder. For
example, the ﬁrst GRU unit of the decoder is initialised with the hidden state of the ﬁrst unit of
the encoder.
Hyperparameter
Interval [min, max]
Sampling Mode2
Learning Rate
[1 · 10−6, 1 · 10−1]
Uniform Logarithmic
Number of layers
[1, 3]
QUniform
Hidden size GRU
[16, 256]
QUniform
Batch Size
[8, 64]
QUniform
Decay Scheduled Sampling
[1 · 10−1, 9.9 · 10−1]
Uniform Logarithmic
Table 5. Optimised hyperparameters and their relevant search space in HyperOpt.
Finally, it is also necessary to specify the parameters for the ASHA algorithm. Table 6
shows the aforementioned parameters and the selected values. Speciﬁcally, the total number
of trials refers to the amount of hyperparameters combinations the algorithm attempts before
stopping, the reduction factor represents the fraction of trials removed after each run and the
grace period that corresponds to the epochs for which a trial must be trained before halving.
These values have to be selected considering the computational resources of the hardware and
the expected results. The most suitable parameters, which have been found with a trade-off
between results and computational time, are represented in Table 6. It has to be noted that a
small grace period may bias ASHA towards those solutions that converge quite rapidly but do
not lead to the best performances. Therefore, a good compromise is experimentally found to be
at 400 epochs. Furthermore, the maximum value of the training epoch is set at 2100 because it
has been observed that the optimal value is typically above 2000.
2
It refers to the particular distribution from which the hyperparameter is selected. See http://hyperopt.github.io/
hyperopt/getting-started/search_spaces/ for further details.

16 of 26
Input
Value
Number of Trials
100 [-]
Reduction Factor
4 [-]
Grace Period
400 [Epochs]
Maximum Training Epoch
2100 [Epochs]
Table 6. Input parameters of ASHA.
Fig. 10 shows the validation losses as a function of the value of each hyperparameter. We
can observe that the curves associated with lower losses are approximately distributed at low
values of the batch size, between 20 and 40, and hidden state size, in particular between 50 and
150. Furthermore, these distributions provide the qualitative importance of each parameter.
Indeed, an important parameter can be deﬁned by observing the magnitude changes of the
resulting loss with respect to a variation of its value. This means that a change, even small, in
such an input parameter, determines an important variation of the resulting loss. For example,
by looking at the learning rate in the parallel coordinates plot, it can be seen the distinction
between red and blue curves, where the lower rates lead to high validation losses. Instead,
values between 0.002 and 0.0075 seem to lead to better performances. Contrarily, the decay rate
of the Scheduled Sampling mechanism appears to have a negligible effect on the validation
loss. Indeed, it is not possible to identify a range of values that leads to higher performances.
This is related to the fact that the decay rate in Eq. (8) does not have a signiﬁcant importance
on the convergence because the threshold, for selecting the ground truth or the model output,
decreases quickly for all the values that the optimiser selected, forcing the model to learn from
its predicted outputs. Therefore, it may suggest that the model performs better when it has to
deal with its own errors, in a way that is more similar to inference than training.
Batch
Size
Hidden
Size
Learning
Rate
Number
Layers
Decay
Validation
Loss
10
20
30
40
50
60
50
100
150
200
250
0.0000
0.0025
0.0050
0.0075
0.0100
0.0125
0.0150
0.0175
1.00
1.25
1.50
1.75
2.00
2.25
2.50
2.75
3.00
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.00000
0.00002
0.00004
0.00006
0.00008
0.00010
2.5×10−6
6.3×10−6
1.6×10−5
4×10−5
9.87×10−5
Validation Loss [-]
Figure 10. Representation of selected hyperparameter combinations as parallel lines that pass to the
vertices corresponding to the hyperparameters associated to each trial, where each colour is related to the
validation loss.
Finally, the optimised values of the hyperparameters are given in Table 7.
5.2. Dataset characterisation
To analyse and understand the performances of the model, the test objects have to be
divided depending on their physical properties with respect to the training set. We deﬁne two

17 of 26
Parameter
Value [-]
Optimised
Learning Rate
0.001795
Number of Layers
3
Hidden Size GRU
59
Batch Size
27
Decay Scheduled Sampling
0.15665
Table 7. Optimised hyperparameters.
categories of objects as in Table 3 (Appendix A.3.1) in which the objects of the test set share
similar characteristics to the training set. Speciﬁcally, as shown in Fig. 11, the two categories are
subdivided with respect to the B∗coefﬁcient: Category 1 objects (red label) have a median B∗
coefﬁcient inside the 0.25 and 0.75 quantiles of the training set distribution, while the median
B∗of objects in Category 2 (black label) lies outside this range. Therefore, given the signiﬁcant
importance of B∗, it is expected that the deep learning model is going to perform better with
those objects for which their distributions are similar to the training.
10973
11601
19045
20638
20813
21701
23853
26873
31599
31928
37820
37872
38086
39000
40138
Training
NORAD ID [-]
0.0000
0.0002
0.0004
0.0006
0.0008
0.0010
0.0012
0.0014
B∗[1/ER]
Test
Training
Figure 11. B∗distribution of objects in training and test set, where red NORAD IDs identify test bodies
with similar B∗distributions as the training set (Category 1).
To analyse and understand the performances of the model, the test objects have to be
divided depending on their physical properties with respect to the training set. Hence, Fig. 11
represents the B∗distributions of the objects in the test and training sets, where two categories
of bodies can be identiﬁed, as highlighted by the different colours of the NORAD IDs. Another
parameter to analyse is the eccentricity, which distributions for the training and test set are
represented in Fig. 12. It can be seen that 20813 represents an anomaly with respect to all the
other objects. Indeed, it is characterised by a median of roughly 0.7, which identiﬁes it as
a highly elliptical orbit, for which the nature of the re-entry is different. However, we have

18 of 26
decided to maintain this object to further test the capabilities and limitations of the developed
deep learning model.
10973
11601
19045
20638
20813
21701
23853
26873
31599
31928
37820
37872
38086
39000
40138
Training
NORAD ID [-]
10−4
10−3
10−2
10−1
100
e [-]
Test
Training
Figure 12. Eccentricity distribution of objects in training and test set.
To characterise the performances and compare the results, let the absolute error, ϵabs, and
the relative error, ϵrel, be respectively:
ϵabs = |tPredicted −tActual|
(20)
ϵrel = |tPredicted −tActual|
tActual −tInitial
× 100
(21)
where tPredicted is the predicted re-entry epoch; and tActual is the assessed re-entry epoch.
5.3. Testing
Starting from the objects of the ﬁrst category, Table 8 represents the predictions’ errors for
all the different starting altitudes deﬁned in Table 3. Generally, it can be seen that the model is
successfully capable of predicting the output trajectory and therefore the re-entry epoch. For
example, 39000 shows an absolute error that is less than 2 minutes, roughly 24 hours before the
re-entry. Furthermore, this error stays conﬁned below roughly 3 minutes, for all the predictions.
This behaviour is related to the similarities of the B∗distributions of each object with respect to
the one of the training set. Indeed, it should be highlighted that the highest absolute error is 14
minutes and it is associated with 40138. Moreover, observing the MSEs, all the relative errors
lay between 10−5 and 10−7, showing that the similarities of the B∗are associated with excellent
performances. It is interesting to note that comparing the different cases, the MSE decrease
does not necessarily correspond to a lower absolute error, as for example in 11601 and 39000.
This indicates that better results could also be obtained by selecting a different loss function.
Regarding the objects in the second category, the errors for all the bodies are listed in
Table 9. It can be generally seen that the performances of the model are inferior with respect

19 of 26
Case
Metric
NORAD
10973
11601
20638
21701
26873
31599
38086
39000
40138
Case A
εabs [hrs]
0.0129
0.0012
0.0593
0.0287
0.0381
0.0835
0.0856
0.0263
0.2225
εrel [%]
0.0582
0.0051
0.2930
0.1262
0.1800
0.5004
0.3469
0.1093
1.2464
MSE [day2]
4.4482e-6
1.1707e-6
1.0218e-5
1.0236e-5
1.7067e-6
5.2804e-6
2.1809e-5
1.0643e-6
1.9246e-5
Case B
εabs [hrs]
0.0915
0.0411
0.0417
0.0021
0.0376
0.0640
0.0187
0.0191
0.0191
εrel [%]
0.9903
0.4878
0.6032
0.0265
0.4828
1.0072
0.2198
0.2134
0.2952
MSE [day2]
7.0168e-6
1.8629e-6
2.6473e-6
4.4563e-6
2.6635e-7
1.2836e-6
1.1987e-6
1.7427e-6
1.8259e-6
Case C
εabs [hrs]
0.0934
0.0043
0.0125
0.0268
0.0610
0.1025
0.0368
0.0065
0.0198
εrel [%]
2.8694
0.1839
0.7061
1.3862
2.8083
5.6717
1.7670
0.2461
1.1732
MSE [day2]
1.8350e-5
2.1482e-6
9.5146e-7
4.1151e-6
1.5597e-6
5.9912e-6
4.7233e-6
1.5328e-6
6.7500e-7
Case D
εabs [hrs]
0.0670
0.0560
0.0244
0.0340
0.0581
0.0271
0.0580
0.0420
0.0093
εrel [%]
8.1350
14.1151
9.1041
13.5330
15.8149
8.8874
20.3210
8.3022
3.7092
MSE [day2]
6.4009e-6
3.1455e-6
1.3601e-6
1.3073e-6
2.2189e-6
1.0388e-6
2.2950e-6
2.1382e-6
1.5021e-6
Table 8. Results of objects in category 1.
to the previous category of objects, due to the low number of bodies characterised by B∗
distributions similar to the one of the training set. This can be clearly observed in 37872, which
can not be associated with any B∗distributions in the sets. Hence, the resulting outputs are
characterised by the highest MSEs, due to the fact that the model tends to overestimate all the
relative epochs. However, it can be demonstrated that the output follows the shape of trajectory
altitude quite accurately. Furthermore, it can be noted that 20813 is characterised by the highest
relative errors and equally signiﬁcant MSEs. Hence, this indicates that the model is not capable
of predicting the output trajectory and it completely misses all the predicted epochs. Indeed, by
inspecting Fig. 11, 20813 is characterised by a large distribution of B∗values, which maximum
is similar to 19045. However, the model has shown to follow the output proﬁle reasonably
well for 19045 in all the cases, with a maximum relative error of roughly 18 minutes and MSE
between 10−5 and 10−7 [days2]. However, object 20813 also has a large value of the eccentricity
(see Fig. 12, which justiﬁes the difﬁculties of the deep learning model in predicting its re-entry
and indicates that, as expected, the contribution of the eccentricity cannot be ignored for orbits
that are not quasi-circular.
5.3.1. Orbit prediction - Case A
In this section, we show in detail the results for Case A. Speciﬁcally, we present the
evolution of the training and validation losses and the results of the trajectory predictions of
the developed deep learning model for selected objects in the test set. For the test case, both
categories of objects (Appendix A.3.1) have been included. Fig. 13 shows the training and
validation losses as a function of the training epoch, for a total of 2900 epochs. We observe how
the rate of convergence of the validation loss is quite lower than the training loss; nonetheless,
the ﬁnal loss is below 1 × 10−5, which has been observed to produce accurate results.
Fig. 14 shows the trajectory prediction for selected objects of Category 1 in the test set. The
black curve represents the input average altitude (the initial 5 elements of the input sequence,
Tx), the red line represents the predicted sequence, and the blue trajectory deﬁnes the true
output (as obtained from TLE data).
In general, it can be seen that the model is capable of predicting the re-entry epoch with
excellent results. The model can successfully estimate the output trajectory of each object
because the outputs follow the true paths quite accurately. Indeed, as also shown in Table 8,
the highest MSE is associated with 38086; however, the relative errors on the re-entry epoch are
small. Therefore, even if the MSE appears to be relatively high, the absolute and relative errors

20 of 26
Case
Metric
NORAD
19045
20813
23853
31928
37820
37872
Case A
εabs [hrs]
0.2994
1.2758
0.3083
0.6206
0.5829
1.3427
εrel [%]
2.5476
964.0562
0.8570
1.7715
1.6664
1.6942
MSE [day2]
6.1088e-5
1.1331e-3
9.9711e-5
3.6330e-4
3.2982e-4
1.0313e-2
Case B
εabs [hrs]
0.0355
0.7159
0.0273
0.0854
0.0894
2.5015
εrel [%]
0.8068
1309.4202
0.2152
0.7246
0.7297
9.3210
MSE [day2]
1.7582e-6
2.6453e-4
3.2465e-6
7.4105e-6
5.7928e-6
1.2797e-2
Case C
εabs [hrs]
0.0029
0.2194
0.0033
0.0080
0.0222
1.9536
εrel [%]
0.2301
2254.3293
0.1022
0.2579
0.6580
31.1668
MSE [day2]
1.1096e-7
6.2342e-5
6.7083e-6
1.6303e-5
1.5640e-5
1.1795e-2
Case D
εabs [hrs]
0.0805
0.0098
0.1186
0.0847
0.0788
2.9645
εrel [%]
35.9520
2532.4688
26.520
16.8331
13.4101
373.4046
MSE [day2]
9.3967e-6
2.5700e-5
2.4415e-5
2.3780e-5
1.9272e-5
1.5264e-2
Table 9. Results of objects in category 2.
0
500
1000
1500
2000
2500
3000
Epoch [-]
10−6
10−5
10−4
10−3
10−2
Loss [-]
Training
Validation
Figure 13. Representation of the training and validation loss curves in case A, where each curve is
represented through its exponentially weighted average (coloured) and its original values (shaded).
on the re-entry epoch are small. Moreover, the absolute errors are all lower than approximately
13 minutes, except for 40138, which is characterised by an error of 13.35 minutes. This can be
explained by the higher B∗distribution of 40138 with respect to the training set. Furthermore,
considering objects 40138 and 10973, it appears that the model has correctly assimilated the
knowledge related to the relation between the solar index and the drag-like coefﬁcient.
Concerning the objects with B∗distributions different from the one of the training set
(Category 2), the results are summarised in Fig. 15 and Fig. 16.
Generally, it can be observed that all errors are higher with respect to the previous case.
This is related to the low number of objects characterised by a B∗distribution similar to the
training, which makes the model perform less accurately. However, in 4 out of 7 cases, the
absolute errors are lower than approximately 40 minutes. Therefore, it can be concluded, in this
case, that the model is capable of approximating the true output reasonably well. Nevertheless,

21 of 26
0.0
0.5
1.0
1.5
2.0
Time [Day]
80
100
120
140
160
180
200
Average Altitude [km]
εabs = 0.01[hrs]
εrel = 0.06%
NORAD 10973
Input
Real
Predicted
(a) NORAD 10973
0.0
0.5
1.0
1.5
2.0
Time [Day]
80
100
120
140
160
180
200
Average Altitude [km]
εabs = 0.03[hrs]
εrel = 0.13%
NORAD 21701
Input
Real
Predicted
(b) NORAD 21701
0.00
0.25
0.50
0.75
1.00
1.25
1.50
Time [Day]
80
100
120
140
160
180
200
Average Altitude [km]
εabs = 0.08[hrs]
εrel = 0.50%
NORAD 31599
Input
Real
Predicted
(c) NORAD 31599
0.0
0.5
1.0
1.5
2.0
2.5
Time [Day]
80
100
120
140
160
180
200
Average Altitude [km]
εabs = 0.09[hrs]
εrel = 0.35%
NORAD 38086
Input
Real
Predicted
(d) NORAD 38086
0.0
0.5
1.0
1.5
2.0
Time [Day]
80
100
120
140
160
180
200
Average Altitude [km]
εabs = 0.03[hrs]
εrel = 0.11%
NORAD 39000
Input
Real
Predicted
(e) NORAD 39000
0.00
0.25
0.50
0.75
1.00
1.25
1.50
Time [Day]
80
100
120
140
160
180
200
Average Altitude [km]
εabs = 0.22[hrs]
εrel = 1.25%
NORAD 40138
Input
Real
Predicted
(f) NORAD 40138
Figure 14. Predictions of selected objects in category 1 from 180 km altitude.
object 20813 represents an exception in terms of errors, as can be seen in Fig. 17, where the
output trajectory is completely distorted.

22 of 26
0.0
0.2
0.4
0.6
0.8
1.0
Time [Day]
80
100
120
140
160
180
200
Average Altitude [km]
εabs = 0.30[hrs]
εrel = 2.55%
NORAD 19045
Input
Real
Predicted
(a) NORAD 19045.
0
2
4
6
8
Time [Day]
80
100
120
140
160
180
200
Average Altitude [km]
εabs = 1.34[hrs]
εrel = 1.69%
NORAD 37872
Input
Real
Predicted
(b) NORAD 37872.
Figure 15. Predictions of 19045 and 37872 from 180 km altitude.
0
1
2
3
Time [Day]
80
100
120
140
160
180
200
Average Altitude [km]
εabs = 0.58[hrs]
εrel = 1.67%
NORAD 37820
Input
Real
Predicted
(a) NORAD 37820.
0
1
2
3
Time [Day]
80
100
120
140
160
180
200
Average Altitude [km]
εabs = 0.62[hrs]
εrel = 1.77%
NORAD 31928
Input
Real
Predicted
(b) NORAD 31928.
Figure 16. Predictions of 37820 and 31928 from 180 km altitude.
0.00
0.01
0.02
0.03
0.04
0.05
0.06
Time [Day]
80
100
120
140
160
180
200
Average Altitude [km]
εabs = 1.28[hrs]
εrel = 964.06%
NORAD 20813
Input
Real
Predicted
Figure 17. Prediction of NORAD 20813 from 180 km altitude.

23 of 26
By inspecting Fig. 11, it can be seen that the B∗distribution of 20813 is characterised by a
large range of values. However, its interquartile is lower than 19045, and the model has proved
itself to provide acceptable performances on the latter, with an output that accurately follows
the true one. Therefore, it appears that the drag-like coefﬁcient can not be directly linked to
bad performance. Indeed, 20813 is characterised by high values of eccentricity, as highlighted
in Fig. 12. This makes the nature of the re-entry different, and the model is not capable of
acquiring this knowledge because it is not trained for it. Therefore, the prediction capabilities
have to be related not only to the B∗but also to the eccentricity.
6. Conclusions and Discussion
This work presents the development of a deep learning model that is based on a variant
of the Seq2Seq architecture using curriculum learning for the re-entry prediction of uncon-
trolled objects in Low Earth Orbit based on TLE data. We therefore propose a paradigm shift
from a physical to a data-driven approach. Based on the physical knowledge of the re-entry
phenomenon, we introduced additional features alongside the average altitude proﬁle, in
an effort to better characterise and predict the re-entry epoch. Two additional key features
have been identiﬁed: the drag-like coefﬁcient, B∗, and the solar index, F10.7. The drag-like
coefﬁcient is of key importance as it relates the re-entry time to the interaction of the object
with the atmosphere. In our work, we present a methodology to introduce such a parameter
as a feature into a deep learning model, via a stepwise moving window interpolation scheme.
The solar index is equivalently relevant in the re-entry prediction as it affects the density of the
atmosphere and, with it, the re-entry time. Analysing this parameter, the need to introduce
an additional feature has arisen, in order to relate the solar index with the physical properties
of the object. Therefore, the area-to-mass ratio was also added as a feature. Studying the
behaviour of the deep learning model with respect to these features, we identiﬁed possible
ways to introduce them in the model. The developed deep learning model has been tested
on a set of objects studied during the IADC campaigns. It has been observed, in accordance
with Jung et al. [13], that the B∗distribution has signiﬁcant importance on the performances
of the model. However, the poor performance related to object 20813 (Fig. 17) suggests that
also eccentricity has a relevant impact. Hence, future works may include the processing of
highly eccentricity orbits. In addition, it has been found that a decrease in the selected loss
function (MSE) did not necessarily correspond to higher accuracy of the re-entry prediction.
Consequently, future studies could also investigate the effect of different loss functions on
the output of the deep learning model. Overall, the analyses showed promising results for
the re-entry prediction using machine learning; however, as can be expected, the prediction
capabilities of the model strongly depend on the quality and quantity of the training data. It is
therefore crucial, in order to expand the capabilities of this data-driven approach, to expand
the size of the training data and, potentially, improve the quality of such data by considering
additional sources other than TLEs.
Author Contributions: Conceptualization, Mirko Trisolini, Francesco Salmaso, Camilla Colombo; method-
ology, Francesco Salmaso, Mirko Trisolini; software, Francesco Salmaso; validation, Francesco Salmaso;
formal analysis, Francesco Salmaso; investigation, Francesco Salmaso; writing—original draft preparation,
Mirko Trisolini, Francesco Salmaso; writing—review and editing, Mirko Trisolini, Camilla Colombo;
visualization, Francesco Salmaso; supervision, Mirko Trisolini, Camilla Colombo; funding acquisition,
Camilla Colombo. All authors have read and agreed to the published version of the manuscript.
Funding: The research presented in this thesis received funding from the European Research Council
(ERC) under the European Union’s Horizon 2020 research and innovation programme as part of the
COMPASS project (Grant agreement No 679086).
Conﬂicts of Interest: The authors declare no conﬂict of interest.

24 of 26
Abbreviations
The following abbreviations are used in this manuscript:
TLE
Two Line Element
TIP
Tracking and Impact Prediction
PCA
Principal Component Analysis
Seq2Seq
Sequence to Sequence
TPE
Tree-structured Parzen Estimator
ASHA
Asynchronous Successive Halving Algorithm
ANN
Artiﬁcial Neural Network
RNN
Recurrent Neural Network
GRU
Gated Recurrent Unit
MSE
Mean Squared Error
OMM
Orbital Mean-Elements Message
Appendix A
Appendix A.1 NORAD IDs of training set
22277, 33594, 38223, 43538, 49130, 38976, 43033, 37727, 12389, 28883, 45942, 45729, 33440, 40459,
28824, 45601, 39573, 41813, 25431, 29504, 25577, 26481, 12072, 40714, 40700, 33448, 41640, 43703,
40120, 43586, 1377, 37346, 28701, 44500, 36604, 39262, 28642, 38349, 28842, 33387, 40085, 28827,
39561, 24968, 28400, 49045, 28822, 34670, 24840, 38048, 25288, 33592, 48853, 34906, 28486, 28424,
29480, 41478, 39259, 39683, 25274, 27451, 39147, 28641, 25291, 3835, 46930, 39507, 27375, 24837,
31602, 10861, 46390, 25285, 28655, 40949, 40452, 41313, 38075, 8063, 41628, 28504, 12071, 43705,
43475, 25173, 22781, 24872, 40589, 42718, 39145, 40897, 43007, 29058, 33507, 40096, 28896, 40728,
44210, 28625, 32267, 37674, 37397, 39033, 37874, 39566, 28777, 25064, 11745, 34872, 40957, 43243,
25342, 44228, 32007, 39409, 32377, 42701, 44070, 36835, 41865, 46397, 36096, 27372, 39178, 26897,
42734, 29247, 39116, 33456, 43166, 32261, 40745, 25647, 27474, 23769, 40421, 39560, 40430, 40740,
25275, 35642, 41821, 27450, 31596, 26874, 49065, 44551, 41819, 44385, 27374, 48804, 32485, 40723,
40453, 25290, 41100, 32269, 39193, 24945, 37883, 37821, 43532, 29053, 40314, 39580, 41559, 28933,
40668, 40945, 32270, 46614, 43494, 27642, 28823, 28362, 41480, 43757, 14207, 29157, 39571, 33054,
41776, 28425, 40363, 28878, 37197, 29109, 41909, 28519, 38848, 39503, 32757, 37172, 12054, 34603,
21694, 42723, 43863, 33435, 45112, 25468, 39527, 40127, 45596, 28099, 11474, 29667, 40702, 38462,
10582, 38074, 11849, 14694, 27392, 43691, 44372, 36130, 21931, 38249, 25276, 39623, 25040, 25106,
32477, 30778, 29080, 42757, 41486, 39519, 32262, 37860, 27841, 43090, 31394, 28867, 44844, 43239,
32002, 39126, 44707, 35694, 41596, 37680, 41769, 25171, 23020, 39514, 29229, 29263, 42731, 38862,
32751, 29253, 47619, 44438, 32059, 25287, 39393, 20299, 27700, 43212, 35818, 39031, 38037, 35867,
29403, 41574, 28363, 33457, 44206, 24794, 37255, 44834, 41777, 40727, 27550, 28907, 37217, 35947,
37942, 29653, 43667, 39569, 39513, 25777, 7338, 39062, 35011, 36362, 41125, 37634, 42703, 43244,
29394, 40898, 40126, 43637, 44826, 39220, 42972, 25263, 41027, 39171, 24839, 45604, 36506, 27706,
24905, 41487, 37758, 43027, 46669, 28403, 25530, 41572, 44799, 29386, 39180, 39149, 41597, 47255,
28130, 29246, 39386, 37878, 47348, 42730, 41387, 41178, 35941, 38672, 24966
Appendix A.2 NORAD IDs of validation Set
41868, 45177, 29488, 39682, 60, 32386, 38336, 33332, 38550, 24792, 48160, 36522, 32284, 39649,
35949, 43064, 41636, 28191, 40247, 6212, 25346, 37362, 11671, 28643, 44317, 42938, 24904, 43129,
11332, 28445, 27475, 41107, 40393, 44505, 40313, 36512, 48866, 36087, 37873, 37729, 42716, 39363,
48870, 44111, 40359, 13402, 39187, 33273, 47782, 26405, 37856, 28506, 41455, 39408, 39493, 41475,
39137, 24869, 34840, 42733, 41629, 48275, 23757, 46463, 40361, 40619, 28997, 42702, 42800, 28942,
44075, 36749, 32713, 33449, 38076, 37876, 41449, 28774, 31798, 41483, 45938, 39532, 37383, 40886

25 of 26
Appendix A.3 NORAD IDs of test set
20813, 19045, 11601, 10973, 40138, 38086, 23853, 21701, 37872, 39000, 20638, 31928, 37820, 26873,
31599
Appendix A.3.1 Test set subdivision in Category 1 and Category 2
Table A1 shows the NORAD IDs of the test set objects subdivided into Category 1 and
Category 2. Category 1 contains objects with a median B∗within the interquartile distance of
the B∗distribution of the training set, while objects in Category 2 have a median B∗outside
this interval.
Category
NORAD IDs
1
10973
11601
20638
21701
26873
31599
38086
39000
40138
2
19045
20813
23853
31928
37820
37872
Table A1. Subdivision of the bodies in the test set according to the B∗distributions.
References
1.
ESA Space Debris Ofﬁce. ESA’S Annual Space Environent Report, 2021.
2.
Alby, F.; Alwes, D.; Anselmo, L.; Baccini, H.; Bonnal, C.; Crowther, R.; Flury, W.; Jehn, R.; Klinkrad, H.; Portelli, C.; et al. The European
Space Debris Safety and Mitigation Standard. Advances in Space Research 2004, 34, 1260–1263. https://doi.org/10.1016/j.asr.2003.08.
043.
3.
NASA. Process for Limiting Orbital Debris, NASA-STD-8719.14B, 2019.
4.
Space-Track. www.space-track.org.
5.
Pardini, C.; Anselmo, L. Re-entry predictions for uncontrolled satellites: results and challenges. In Proceedings of the 6th IAASS
Conference "Safety is Not an Option"; , 2013.
6.
Pardini, C.; Anselmo, L. Performance evaluation of atmospheric density models for satellite reentry predictions with high solar
activity levels. Transactions of The Japan Society for Aeronautical and Space Sciences 2003, 46, 42–46. https://doi.org/10.2322/tjsass.46.42.
7.
Pardini, C.; Anselmo, L. Assessing the risk and the uncertainty affecting the uncontrolled re-entry of manmade space objects. Journal
of Space Safety Engineering 2018, 5, 46–62. https://doi.org/10.1016/j.jsse.2018.01.003.
8.
Braun, V.; Flegel, S.; Gelhaus, J.; Kebschull, C.; Moeckel, M.; Wiedemann, C.; Sánchez-Ortiz, N.; Krag, H.; Vörsmann, P. Impact of
Solar Flux Modeling on Satellite Lifetime Predictions. In Proceedings of the 63rd International Astronautical Congress; , 2012.
9.
Vallado, D.A.; Finkleman, D. A critical assessment of satellite drag and atmospheric density modeling. Acta Astronautica 2014,
95, 141–165. https://doi.org/10.1016/j.actaastro.2013.10.005.
10.
Vallado, D. Fundamentals of Astrodynamics and Applications, second ed.; Springer Dordrecht, 2001.
11.
Anselmo, L.; Pardini, C. Computational methods for reentry trajectories and risk assessment. Advances in Space Research 2005,
35, 1343–1352. https://doi.org/10.1016/j.asr.2005.04.089.
12.
Frey, S.; Colombo, C.; Lemmens, S. Extension of the King-Hele orbit contraction method for accurate, semi-analytical propagation of
non-circular orbits. Advances in Space Research 2019, 64, 1–17. https://doi.org/https://doi.org/10.1016/j.asr.2019.03.016.
13.
Jung, O.; Seong, J.; Jung, Y.; Bang, H. Recurrent neural network model to predict re-entry trajectories of uncontrolled space objects.
Advances in Space Research 2021, 68, 2515–2529. https://doi.org/10.1016/j.asr.2021.04.041.
14.
Lidtke, A.A.; Gondelach, D.J.; Armellin, R. Optimising ﬁltering of two-line element sets to increase re-entry prediction accuracy for
GTO objects. Advances in Space Research 2019, 63, 1289–1317. https://doi.org/10.1016/j.asr.2018.10.018.
15.
Flohrer, T.; Krag, H.; Klinkrad, H. Assessment and categorization of TLE orbit errors for the US SSN catalogue. In Proceedings of the
Advanced Maui Optical and Space Surveillance Technologies (AMOS) Conference, 2008.
16.
Levit, C.; Marshall, W. Improved orbit predictions using two-line elements.
Advances in Space Research 2011, 47, 1107–1115.
https://doi.org/10.1016/j.asr.2010.10.017.
17.
Aida, S.; Kirschner, M. Accuracy assessment of SGP4 orbit information conversion into osculating elements. In Proceedings of the 6th
European Conference on Space Debris; , 2013.
18.
Raschka, S. Python machine learning, third ed.; Packt Publishing Ltd, 2019.
19.
Sutskever, I.; Vinyals, O.; Le, Q.V. Sequence to Sequence Learning with Neural Networks. In Proceedings of the 27th International
Conference on Neural Information Processing Systems - Volume 2; MIT Press: Cambridge, MA, USA, 2014; p. 3104–3112.

26 of 26
20.
Zhang, A.; Lipton, Z.C.; Li, M.; Smola, A.J. Dive into Deep Learning, 2021. arXiv:2106.11342.
21.
Goodfellow, I.; Bengio, Y.; Courville, A. Deep learning; MIT press, 2016.
22.
Elman, J.L. Finding structure in time. Cognitive science 1990, 14, 179–211. https://doi.org/10.1207/s15516709cog1402_1.
23.
Cho, K.; van Merriënboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; Bengio, Y. Learning Phrase Representations
using RNN Encoder–Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP); Association for Computational Linguistics: Doha, Qatar, 2014; pp. 1724–1734. https:
//doi.org/10.3115/v1/D14-1179.
24.
Bengio, S.; Vinyals, O.; Jaitly, N.; Shazeer, N. Scheduled sampling for sequence prediction with recurrent neural networks. Advances
in neural information processing systems 2015, 28.
25.
Chung, J.; Gulcehre, C.; Cho, K.; Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling, 2014.
arXiv:1412.3555.
26.
Abadi, M.; Agarwal, A.; Barham, P.; Brevdo, E.; Chen, Z.; Citro, C.; Corrado, G.S.; Davis, A.; Dean, J.; Devin, M.; et al. TensorFlow:
Large-Scale Machine Learning on Heterogeneous Systems, 2015. Software available from tensorﬂow.org.
27.
Chollet, F. Deep learning with Python; Simon and Schuster, 2021.
28.
Dong, G.; Liu, H. Feature engineering for machine learning and data analytics; CRC Press, 2018.
29.
Salmaso, F. Machine learning model for uncontrolled re-entry predictions of space objects and feature engineering. PhD thesis,
Politecnico di Milano, 2022.
30.
Virtanen, P.; Gommers, R.; Oliphant, T.E.; Haberland, M.; Reddy, T.; Cournapeau, D.; Burovski, E.; Peterson, P.; Weckesser, W.;
Bright, J.; et al.
SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing in Python.
Nature Methods 2020, 17, 261–272.
https://doi.org/10.1038/s41592-019-0686-2.
31.
Curtis, H.D. Orbital Mechanics for Engineering Students, third ed.; Butterworth-Heinemann, 2014.
32.
Bergstra, J.; Yamins, D.; Cox, D. Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for
Vision Architectures. In Proceedings of the Proceedings of the 30th International Conference on Machine Learning; , 2013; Vol. 28, pp.
115–123.
33.
Li, L.; Jamieson, K.; Rostamizadeh, A.; Gonina, E.; Hardt, M.; Recht, B.; Talwalkar, A. A System for Massively Parallel Hyperparameter
Tuning, 2020. arXiv:1810.05934.
34.
Liaw, R.; Liang, E.; Nishihara, R.; Moritz, P.; Gonzalez, J.E.; Stoica, I. Tune: A research platform for distributed model selection and
training. arXiv preprint arXiv:1807.05118 2018.
35.
Kingma, D.P.; Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 2014.

