Video-LLaMA
An Instruction-tuned Audio-Visual Language Model
for Video Understanding
Hang Zhang
Xin Li
Lidong Bing
DAMO Academy, Alibaba Group
{zh401075, xinting.lx, l.bing}@alibaba-inc.com
https://github.com/DAMO-NLP-SG/Video-LLaMA
Abstract
We present Video-LLaMA, a multi-modal framework that empowers Large Lan-
guage Models (LLMs) with the capability of understanding both visual and audi-
tory content in the video. Video-LLaMA bootstraps cross-modal training from the
frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous
vision-LLMs that focus on static image comprehensions such as MiniGPT-4 (Zhu
et al., 2023) and LLaVA (Liu et al., 2023), Video-LLaMA mainly tackles two
challenges in video understanding: (1) capturing the temporal changes in visual
scenes, (2) integrating audio-visual signals. To counter the first challenge, we
propose a Video Q-former to assemble the pre-trained image encoder into our
video encoder and introduce a video-to-text generation task to learn video-language
correspondence. For the second challenge, we leverage ImageBind (Girdhar et al.,
2023), a universal embedding model aligning multiple modalities as the pre-trained
audio encoder, and introduce an Audio Q-former on top of ImageBind to learn
reasonable auditory query embeddings for the LLM module. To align the output
of both visual & audio encoders with LLM’s embedding space, we train Video-
LLaMA on massive video/image-caption pairs as well as visual-instruction-tuning
datasets of moderate amount but higher quality. We found Video-LLaMA show-
cases the ability to perceive and comprehend video content, generating meaningful
responses that are grounded in the visual and auditory information presented in the
videos. This highlights the potential of Video-LLaMA as a promising prototype
for audio-visual AI assistants.
1
Introduction
Large Language Models (LLMs) (Chowdhery et al., 2022; Bai et al., 2022; OpenAI, 2023) trained
on massive amounts of textual data are the most impressive breakthroughs of AI since 2022. LLMs
enabled general-purpose AI assistants123 have demonstrated a remarkable capability of understanding
and following user intentions and instructions. Despite their success, most of the users are only
allowed to interact with LLMs via text-based conversations4. Obviously, text-only human-computer
interaction is less optimal for a powerful AI assistant. In order to further explore the potential of
1https://chat.openai.com/chat
2https://www.anthropic.com/product
3https://bard.google.com/
4As of June 2, 2023, GPT-4’s public API does not yet support image input.
Preprint. Work in progress
arXiv:2306.02858v2  [cs.CL]  6 Jun 2023

LLMs, many researchers attempt to endow LLMs with visual understanding capability (Tsimpoukelli
et al., 2021a; Alayrac et al., 2022b; Wang et al., 2022a; ?; Li et al., 2022; Wang et al., 2022b; Li et al.,
2023a; Xu et al., 2023b; Huang et al., 2023b; ?).
Among these efforts, BLIP-2 (Li et al., 2023a) bootstraps vision-language pre-training from frozen
pre-trained image encoders and frozen language decoders. It receives increasing attention for its
compute-efficiency and the flexibility of leveraging readily-available instruction-following LLMs
(e.g., FLAN-T5 (Chung et al., 2022) and Vicuna (Chiang et al., 2023)). Based on BLIP-2, Zhu et al.
(2023); Liu et al. (2023); Ye et al. (2023) conduct initial attempts to introduce vision foundation
models as the plugin of LLMs to accommodate image input. In these frameworks, the BLIP-style
cross-model pre-training well connects the LLMs with vision foundation models. Moreover, the
intrinsic property of each unimodal pre-trained model, especially the instruction-following capability
of LLMs, is preserved during vision-language pre-training. Therefore, these works empower LLMs
to support both text-based conversations and image-grounded conversations. On the other hand, as
another popular form of content on the social media platform, video has not been considered integrated
into chatting yet. The reasons probably come from the difficulty of accurately understanding non-
static visual scenes. Besides, mitigating the modality gap between video and text, which typically
requires the processing of both visual signals and audio signals, is more challenging than that between
image and text.
In this work, to fill in this blank, we investigate the possibility of building multi-modal LLMs that
support the input of video and allow users to chat with computers around the user-uploaded video,
which is usually composed of multiple video frames and audio. Instead of employing external
perception models to convert visual/auditory signals to textual signals (Shen et al., 2023; Li et al.,
2023b), we choose to build an end-to-end model that can handle the data from multiple modalities
within one single framework. Specifically, we adopt the idea of BLIP-2 (Li et al., 2023a) to guarantee
the efficiency of cross-modal pre-training. To explicitly capture the change of visual scenes in the
video, we use a pre-trained visual encoder to separately compute frame representations. Then, we
introduce a frame embedding layer to inject temporal information and a video Q-Former to generate
visual query tokens. As for the audio signals from the video, we additionally leverage a pre-trained
audio encoder as well as an audio Q-former to learn reasonable auditory query embeddings (see the
right part of Figure 1).
To align textual output with video, we devise multi-branch cross-model pre-training to learn the
vision-language correspondence and the audio-language correspondence. For vision-language corre-
spondence, we first pre-train the vision-related components on a large-scale video caption dataset
with a video-clips-to-text generation task. To enhance the understanding of static visual concepts, we
also add image-caption data into this pre-training stage. Then, we further fine-tune these components
on a video-based conversation dataset to execute visual instruction tuning. For the alignment between
the audio encoder and language decoder, we further pre-train the audio-related components on an
audio caption dataset with an audio-to-text generation task. For the audio-language correspondence,
we leverage Imagebind (Girdhar et al., 2023) as an encoder, which performs exceptionally well
in aligning different modalities to a common embedding space. Given the limited availability of
audio-text data, we utilize vision-text data to train the audio-related components. These components
learn to align the common embedding space provided by Imagebind with the embedding space of
LLMs. Despite not being explicitly trained with audio-text data, Video-LLaMA exhibits a remarkable
zero-shot audio understanding capability during inference.
In summary, our contributions are as follows:
• We propose Video-LLaMA, a multi-model large language model that achieves video-grounded
conversations between humans and computers by connecting language decoder with off-the-shelf
unimodal pre-trained models.
• To empower LLMs with video understanding capability, we propose a multi-branch cross-model
pre-training framework to achieve both vision-language alignment and audio-language alignment.
• We open-source the entire codebase for pre-training and fine-tuning as well as the model weights of
all the variants of Video-LLaMA5. We also prepare the demos for video-grounded conversation67.
5https://github.com/DAMO-NLP-SG/Video-LLaMA
6https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA
7https://modelscope.cn/studios/damo/video-llama/summary
2

Visual Encoder 
(ViT & Q-Former)
Video Q-Former
Audio Encoder 
Audio Q-Former
Linear
Linear
LLM (Vicuna/LLaMA)
Describe this video:
# Human:
This video is an animation of a rocket
launching from a launch pad at night...
Video frames
Audio signals
Vision-Language Branch
Audio-Language Branch
Figure 1: Overall architecture of Video-LLaMA.
2
Method
Video-LLaMA aims to empower frozen LLMs with the capability of understanding both visual and
auditory content in videos. As shown in Figure 1, we design two branches, namely Vision-Language
Branch and Audio-Language Branch, to respectively transform the video frames and audio signals
into query representations that are compatible with the textual inputs of LLMs. In this section, we
first introduce the overall architecture and the building blocks of each branch. Then, we delineate
the procedures of the proposed multi-branch cross-modal pre-training and audio-visual instruction
tuning.
2.1
Architecture
2.1.1
Vision-Language Branch
The Vision-Language Branch is designed for enabling the LLMs to understand visual inputs. As
shown in the left part of Figure 1, it is composed of a frozen pre-trained image encoder to extract
features from video frames, a position embedding layer to inject temporal information into video
frames, a video Q-former to aggregate frame-level representations and a linear layer to project the
3

output video representations into the same dimension as the text embeddings of LLMs. Given one
video consists of N frames, the image encoder will first map each frame/image into Kf image
embedding vectors, yielding video frame representations V = [v1, v2, ..., vN] where vi ∈RKf ×df
is the set of df-dimensional image embeddings corresponding to the i-th frame.
Since the frame representations vi from the frozen image encoder are computed without consider-
ing any temporal information, we further apply position embeddings as the indicator of temporal
information to the representations from different frames. Then, we feed the position-encoded frame
representations to Video Q-former, which shares the same architecture with Query Transformer
(Q-Former) in BLIP-2 (Li et al., 2023a), to obtain kV video embedding vectors of dimension dv as
the representation ˆv ∈RkV ×dv of the video.
To adapt the video representations to the input of LLMs, we add a linear layer to transform the
video embedding vectors into the video query vectors. The video query vectors are of the same
dimension as the text embeddings of LLMs. In the forward pass, they will be concatenated to input
text embeddings as a video soft prompt and guide the frozen LLMs to generate text conditioned on
the video content.
As for the implementation of the Vision-Language Branch, we utilize the pre-trained vision component
of BLIP-2 (Li et al., 2023a) as the frozen visual encoder, which includes a ViT-G/14 from EVA-
CLIP (Fang et al., 2022) and a pre-trained Q-former. The remaining components, including the
position embedding layer, Video Q-former, and Linear layer are randomly initialized and optimized
to well connect the output of the frozen visual encoder to frozen LLMs.
2.1.2
Audio-Language Branch
To deal with the auditory content of the given video, we introduce the Audio-Language Branch.
Concretely, it consists of a pre-trained audio encoder to compute features given a short segment
of origin audio, a position embedding layer to inject temporal information to audio segments, an
audio Q-former to fuse the features of different audio segments, and a linear layer to map the audio
representation into the embedding space of LLMs.
In practice, we utilize the pre-trained Imagebind (Girdhar et al., 2023) as the audio encoder. We
first uniformly sample M segments of 2-second short audio clips from the video, then convert
each 2-second audio clip into spectrograms using 128 mel-spectrogram bins. After obtaining the
spectrogram list of input audio, the audio encoder will map each spectrogram into a dense vector. So
the generated audio representation of the given video can be denoted as A = [a1, a2, ..., aM].
Similar to Video Q-Former, the Audio Q-former injects temporal information by adding learnable
positional embeddings to audio segments. It then generates fixed-length audio features by computing
the interaction across the position-encoded audio segments. Audio Q-Former adopts the same
architecture as Q-Former. It projects the variable-length audio representation list A into a fixed-length
sequence ˆA ∈RKa×da, where the Ka is the number of audio embedding vectors and da is the
dimension of each vector. Finally, we employ a linear layer to map audio features to the embedding
space of the LLM.
2.2
Multi-branch Cross-Modal Training
We train the vision-language and audio-language branches separately. In the first stage, large-scale
vision-caption datasets are used for training, and in the second stage, high-quality instruction-
following datasets were used for fine-tuning. The image is treated as a one-frame video.
2.2.1
Training of Vision-Language Branch
For pre-training vision-language branch, we utilized Webvid-2M (Bain et al., 2021), a large-scale
dataset of short videos with textual descriptions sourced from stock footage sites. Moreover, we
employed the image caption dataset CC595k, which is sourced from CC3M (Sharma et al., 2018) and
filtered by Liu et al. (2023). We adopt a video-to-text generation task during the pre-training stage,
i.e., given the representation of a video, prompting the frozen LLM to generate the corresponding
text description. We find that a significant portion of textual descriptions are insufficient to reflect
the entire content of the videos. Therefore, the visual semantics in the videos are not fully aligned
with the textual semantics in the video descriptions. Nevertheless, this stage aimed to utilize a vast
4

amount of data and enable video features to contain as much visual knowledge as possible. We left
the abilities of vision-text alignment and instruction-following for the next stage.
After the pre-training stage, the model can generate content about information in the video, but
its ability to follow instructions has decreased. Therefore, in the second stage, we fine-tuned the
model using high-quality instruction data. We integrated the image-detail-description dataset from
MiniGPT-4 (Zhu et al., 2023), the image-instruction dataset from LLaVA (Liu et al., 2023), and
the video-instruction dataset from Video-Chat (Li et al., 2023b). After fine-tuning, Video-LLaMA
exhibited remarkable abilities in following instructions and comprehending images and videos.
2.2.2
Training of Audio-Language Branch
Training the audio-language branch directly using audio-text data is highly challenging due to the
rarity of such data. The objective of the learnable parameters in the audio-language branch is to align
the output embedding of the frozen audio encoder with the embedding space of LLM. Given the
scarcity of audio-text data, we employ a workaround strategy to achieve this objective. ImageBind,
which is used as our audio encoder, has a remarkable ability to align different modalities’ embeddings
to one common space, demonstrating impressive performance on cross-modal retrieval and generation
tasks. In light of the scarcity of audio-text data and the abundance of visual-text data, we train the
audio-language branch using visual-text data, following the same data and process as the vision
branch. Thanks to the shared embedding space provided by ImageBind, Video-LLaMA exhibits the
ability to comprehend audio during inference, even though the audio interface has never been trained
on audio data.
3
Related Works
Large Language Models: Large language models (LLMs) (Black et al., 2022; Scao et al., 2022;
OpenAI, 2023; Tsimpoukelli et al., 2021b) have demonstrated remarkable language understanding
and reasoning abilities, enabling the generation of high-quality natural language text across various
domains, including articles, conversations, stories, and poetry. LLMs have already sparked a tech-
nological revolution and have been widely applied in different applications. Moreover, a series of
open source large models, such as LLaMA (Touvron et al., 2023), BLOOM (Scao et al., 2022) and
OPT (Zhang et al., 2022), have greatly promoted technological advancement and made outstanding
contributions to the NLP community. Building upon the foundation of these impressive LLMs,
researchers have further extended their capabilities and developed excellent models for various NLP
tasks. Examples include Vicuna (Chiang et al., 2023) and Baize (Xu et al., 2023a). Our work is based
on these awesome LLMs and provides plug-and-play plugins that empower them with the capability
of comprehending both visual and auditory content in videos.
Multi-modal Large Language Models: Researchers have been actively exploring the use of LLMs
for processing multi-modal problems (Gao et al., 2023; Li et al., 2023b). Existing approaches can
be categorized into two main groups. The first category involves employing LLMs as controllers
and utilizing existing multi-modal models as tools. In this approach, when receiving the user’s text
instruction, the LLM recognizes the user’s intention and makes decisions about which tools to call. It
then generates comprehensive responses by incorporating the results obtained from these off-the-shelf
multi-modal models. Examples include ChatGPT (Wu et al., 2023), HuggingGPT (Shen et al.,
2023), and AudioGPT (Huang et al., 2023a). The second category focuses on training fundamental
large-scale multimodal models. The key idea of this line of work is to align other modal pre-trained
models to textual LLMs. For instance, Flamingo (Alayrac et al., 2022a) ultilizes a perceiver resampler
and a gated cross-attention layer to connect a frozen image encoder and LLM. BLIP2 (Li et al.,
2023a) introduces a Q-Former to map learned image queries to the textual embedding space of LLMs.
LLaVA (Liu et al., 2023), mPLUG-owl (Ye et al., 2023) and MiniGPT4 (Zhu et al., 2023) develop
instruction-following image-LLMs using image-instruction-following dataset. Video-Chat (Li et al.,
2023b) extends image encoders, enabling large models to understand visual content in videos.
PandaGPT (Su et al., 2023) utilizes multimodal encoders from ImageBind, trained exclusively on
image-instruction pairs, to enable large models to understand six modalities. Our work falls into the
second category, where we train fundamental models to comprehend both the visual and auditory
content in videos.
5

4
Limitations
Although Video-LLaMA has demonstrated impressive abilities in understanding both visual and
auditory content in videos, it is still an early-stage prototype and has some limitations, including:
(1) Limited perception capacities: Video-LLaMA’s performance is hindered by the quality and
scale of the current training dataset. We are actively constructing a high-quality audio-video-text
alignment dataset to enhance the model’s perception capabilities. (2) Limited ability to handle long
videos. Long videos(such as movies, and TV shows) contain a large volume of information and
impose higher demands on computational resources. This challenge remains a crucial issue that the
research community is actively working to address. (3) Hallucination. Video-LLaMA inherits the
hallucination problem from the frozen LLMs. Future advancements in more powerful LLMs are
expected to alleviate this issue. We will continue to address these challenges and look forward to
developing a more powerful language model for video understanding.
5
Examples
In this section, we show some cases to demonstrate Video-LLaMA’s multi-modal instruction-
following capability in video/audio/image-grounded conversations.
(1) Audio-visual integration perception ability.
Figure 2 and Figure 3 show Video-LLaMA’s
unique ability to comprehend auditory and visual information simultaneously. The videos in both
cases contain audio. In each conversation, we pose two questions related to visual and auditory
content respectively. If the model could only receive one modal, it would be unable to answer both of
these questions. However, we can observe that Video-LLaMA accurately responds to both visual and
auditory questions in both cases.
(2) The ability to perceive and understand static images.
Figure 4 and Figure 5 show Video-
LLaMA’s ability to perceive and understand pictures. In Figure 4, not only does Video-LLaMA
accurately describe the main content of the image, but it also associates it with the friendly interaction
between a dog and a human. Figure 5 demonstrates Video-LLaMA’s ability to understand the concept
of "unusual" and specifically describe the unusual scene.
(3) The ability of common-knowledge concept recognition.
Figure 7 and Figure 6 demonstrate
Video-LLaMA’s remarkable capacity for recognizing common-knowledge concepts in visual signals.
Video-LLaMA successfully recognizes famous landmarks and characters and can engage in common-
sense question-answering.
(4) The ability to capture temporal dynamics in videos.
Figure 8 and Figure 9 illustrate the
capability of Video-LLaMA to identify actions over time. It successfully describes the actions being
performed by the girl and the direction of the moving boat.
6

Describe what you hear?
I hear sounds of footsteps on the floor in the background, 
and there is also a dog barking in the house.
…
…
Does the man wear glasses?
Yes, he wears a pair of glasses in the video.
Figure 2: A case that Video-LLaMA answers the questions based on the background sound and visual
content of the video.
How was the audience's response?
The audience was really enthusiastic and clapped loudly for 
the performer.
…
…
What is this man doing?
He is playing a saxophone on stage at the concert.
Figure 3: A case showing Video-LLaMA’s ability to identify the sound of applause in a video
and infer the positive response from the audience. Additionally, it infers that a man is playing the
saxophone on stage based on the visual content.
7

Describe the image in details.
The image features a beautiful and playful golden retriever 
running on a bright green lawn. The dog is running with a 
tennis ball in its mouth, as it is an active and lively animal 
that enjoys playing and having fun. The green grass in the 
background creates a vibrant contrast with the golden color 
of the dog's coat, adding a touch of color and texture to the 
scene. The dog's fur is well-groomed, shiny, and nicely 
groomed, highlighting its health and appearance. The 
brightness of the green lawn and the playful behavior of the 
dog suggest a joyful atmosphere, inviting viewers to engage 
in this lovely scenery and the dog's energetic play.
It's worth mentioning that the dog's focus and dedication to 
playing with the ball also showcase its natural instincts and 
its hunting abilities. Additionally, this image captures a fun 
and heartwarming moment of bonding between the dog and 
its human companion, as the dog's positive attitude towards 
the ball reflects its enjoyment of interacting with its owner 
and its environment. Overall, the image conveys an inviting 
and joyful atmosphere.
Figure 4: A case where Video-LLaMA provides a detailed description of the static image content.
What is unusual about this image?
Unusual about this image is that a man in a yellow shirt is 
standing on top of a parked car while holding an ironing board 
and pressing clothes
Figure 5: A case demonstrating Video-LLaMA’s ability to comprehend static images.
8

Do you know the building in this picture?
The building in the image appears to be the United States 
Capitol. It is a neoclassical style building located in Washington, 
D.C., the capital of the United States. It serves as the home of 
the U.S. Congress and is one of the most iconic landmarks in 
the country
Figure 6: A case demonstrating Video-LLaMA’s ability to recognize famous landmarks.
Do you know who the man on the right is?
The man on the right is Jon Snow, played by Kit Harington in the 
television series Game of Thrones. 
Who is the lady on the left?
The lady on the left is likely Daenerys Targaryen, played by Emilia 
Clarke in the television series Game of Thrones. 
What is the relationship between them? 
Jon Snow and Daenerys Targaryen are both characters in the 
television series Game of Thrones. In the show, they have a 
romantic relationship. They first meet in the seventh season, and 
their attraction to each other grows over the course of the eighth 
season. Despite their initial attraction, they have a complicated 
relationship due to their respective backgrounds and the 
challenges they face in the series
Figure 7: A case showing Video-LLaMA’s ability to recognize renowned characters and participate
in video-grounded question answering.
9

Describe this video.
The video shows a young red-haired woman, who is 
wearing a striped shirt, posing for the camera in front of a 
gray background. She is holding her fingers up to her mouth, 
as if she is trying to be quiet, and the caption mentions her 
long hair and red lips. The woman is young, with fair skin, 
and looks calm and confident. She is holding her fingers to 
her mouth, which suggests she might be trying to signal 
something to the camera or trying to convey a message.
…
…
Figure 8: A case where Video-LLaMA provides a detailed description of the visual content in a
dynamic video.
Summarize this video in one sentence.
The video shows a beautiful scenery of a cherry blossom-
lined river flowing by a boat on the water, and a cityscape 
with tall buildings in the background.
…
…
What direction is the ship going
The ship is going towards the right side of the video.
Figure 9: A case showing Video-LLaMA’s ability to identify actions over time.
10

References
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language
model for few-shot learning. Advances in Neural Information Processing Systems, 35:23716–
23736, 2022a.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan
Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian
Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo
Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language
model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022b.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness
from ai feedback. arXiv preprint arXiv:2212.08073, 2022.
Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and
image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision,
2021.
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,
Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive
language model. arXiv preprint arXiv:2204.06745, 2022.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot
impressing gpt-4 with 90%* chatgpt quality, 2023.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac
Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra,
Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai
hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.
Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.
Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong
Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale.
arXiv preprint arXiv:2211.07636, 2022.
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, W. Zhang, Pan Lu,
Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Jiao Qiao. Llama-adapter v2: Parameter-efficient
visual instruction model. arXiv preprint arXiv:2304.15010, 2023.
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand
Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15180–15190, 2023.
Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,
Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. Audiogpt: Understanding and generating speech,
music, sound, and talking head. arXiv preprint arXiv:2304.12995, 2023a.
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv,
Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav
Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning
perception with language models. arXiv preprint arXiv:2302.14045, 2023b.
11

Chenliang Li, Haiyang Xu, Junfeng Tian, Wei Wang, Ming Yan, Bin Bi, Jiabo Ye, He Chen, Guohai
Xu, Zheng Cao, Ji Zhang, Songfang Huang, Fei Huang, Jingren Zhou, and Luo Si. mPLUG:
Effective and efficient vision-language learning by cross-modal skip-connections. In Proceedings
of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 7241–7259,
2022. URL https://aclanthology.org/2022.emnlp-main.488.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597,
2023a.
Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen Wang, Ping Luo, Yali Wang, Limin Wang, and
Yu Qiao. Videochat: Chat-centric video understanding. 2023b.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv
preprint arXiv:2304.08485, 2023.
OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: A 176b-
parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.
2556–2565. Association for Computational Linguistics, 2018. URL https://aclanthology.
org/P18-1238.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:
Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580,
2023.
Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to
instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and Felix Hill.
Multimodal few-shot learning with frozen language models. In Neural Information Processing
Systems, 2021a.
Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.
Multimodal few-shot learning with frozen language models. Advances in Neural Information
Processing Systems, 34:200–212, 2021b.
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou,
Jingren Zhou, and Hongxia Yang. Unifying architectures, tasks, and modalities through a simple
sequence-to-sequence learning framework. In International Conference on Machine Learning,
2022a.
Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,
Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign lan-
guage: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442,
2022b.
Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Vi-
sual chatgpt: Talking, drawing and editing with visual foundation models.
arXiv preprint
arXiv:2303.04671, 2023.
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with
parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196, 2023a.
12

Haiyang Xu, Qinghao Ye, Mingshi Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi,
Qiuchen Qian, Wei Wang, Guohai Xu, Ji Zhang, Songfang Huang, Feiran Huang, and Jingren
Zhou. mplug-2: A modularized multi-modal foundation model across text, image and video. arXiv
preprint arXiv:2302.00402, 2023b.
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu,
Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi,
Ji Chao Zhang, and Feiyan Huang. mplug-owl: Modularization empowers large language models
with multimodality. arXiv preprint arXiv:2304.14178, 2023.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language
models. arXiv preprint arXiv:2205.01068, 2022.
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
Minigpt-4: En-
hancing vision-language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592, 2023.
13

