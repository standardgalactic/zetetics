arXiv:1903.05671v6  [math.OC]  1 Apr 2021
ACCELERATED FIRST-ORDER METHODS: DIFFERENTIAL
EQUATIONS AND LYAPUNOV FUNCTIONS
Jonathan W. Siegel
Department of Mathematics
Pennsylvania State University
University Park, PA 16802
jus1949@psu.edu
April 2, 2021
ABSTRACT
We develop a theory of accelerated ﬁrst-order optimization from the viewpoint of differential equa-
tions and Lyapunov functions. Building upon the previous work of many researchers, we consider
differential equations which model the behavior of accelerated gradient descent. Our main contri-
butions are to provide a general framework for discretizating the differential equations to produce
accelerated methods, and to provide physical intuition which helps explain the optimal damping
rate. An important novelty is the generality of our approach, which leads to a uniﬁed derivation of a
wide variety of methods, including versions of Nesterov’s accelerated gradient descent, FISTA, and
accelerated coordinate descent.
1
Introduction
Minimizing convex and strongly convex functions is a fundamental problem which arises in many areas of science.
We concern ourselves here with the problem
arg min
x∈Rd
f(x),
(1)
where f : Rd →R is a strongly convex function. Due to the importance of very large scale problems of the form
(1), which arise in machine learning and data science, ﬁrst-order methods have gained popularity in recent years. In
practice, methods which only utilize gradient information are often the only ones which can be applied to large scale
problems of the form (1).
Motivated by this, there has been a lot of research into developing optimal ﬁrst-order methods for convex optimization.
Beginning with Polyak’s discovery of the heavy ball method [1], which attains an accelerated convergence rate locally,
and the seminal discovery of Nesterov’s globally accelerated gradient descent [2], many different accelerated methods
have been developed by many authors. For instance, accelerated methods for solving composite optimization problems
are developed in [3] and [4] and an accelerated version of coordinate descent was developed in [5], to name only a
few.
In spite of this progress, these methods have remained somewhat mysterious and difﬁcult to understand. Consequently,
there has been a lot of work in explaining these methods. For instance, in [6], a geometric explanation of acceleration
is given, in [7] accelerated methods are derived as a coupling of gradient and mirror descent, and in [8–11] these
methods are studied via the differential equations which they discretize. In [9, 12], a detailed Lyapunov analysis of
both the continuous and discrete dynamics of accelerated methods are presented and connected via a discretization
analysis. In addition, the Lyapunov analysis is shown to be equivalent to the technique of estimate sequences. In the
non-smooth case, accelerated forward-backward methods such as FISTA [3] are analyzed in terms of the differential
inclusions which they discretize in [13,14]. These lines of analysis are further extended in the recent work [11].
In this paper, we provide more analysis of accelerated methods and their connection with continuous dynamics. We
build upon the work in [8, 9, 12–14] and study accelerated ﬁrst-order methods from the viewpoint of the underlying

A PREPRINT - APRIL 2, 2021
differential equations in both the smooth and non-smooth cases. We are mainly interested in the strongly convex case,
but we also analyze accelerated methods for non-smooth objectives in the convex case in Section 7.
Our main contribution is the derivation of a general framework for discretizing the differential equations to produce
accelerated methods. A framework for doing this has already been developed in [12], and we attempt to build upon
their work. Our contribution is to connect accelerated forward-backward methods for non-smooth problems to differ-
ential equations and to provide a Lyapunov analysis for such methods. In addition, we also connect our framework
to accelerated coordinate methods [5, 15], for example accelerated Gauss-Seidel [16]. The question of whether such
methods, for example accelerated coordinate descent, can systematically be connected to differential equations was
posed in [9]. Our treatment leads to a uniﬁed derivation of a wide variety of methods in the literature, including de-
terministic methods such as Nesterov’s accelerated gradient descent [2] and FISTA [3], and stochastic methods like
accelerated coordinate descent.
The key to our theory is that many of these methods can be derived as special discretizations of damped Hamiltonian
dynamics with an appropriately chosen damping rate. The discretizations we consider all look very similar; they
consist of an explicit forward step in position, a semi-implicit step in velocity, and ﬁnally a small perturbation (second
order in the step size) which ensures a sufﬁcient decrease in the objective. This general framework, which we introduce
in sections three through six, provides a uniﬁed derivation of a wide variety of accelerated ﬁrst-order methods. We
ﬁnd it fascinating that so many methods can be obtained as discretizations of the same equations in such a simple way.
The paper is organized as follows. In Section 2, we brieﬂy analyze the differential equations underlying accelerated
methods, following the treatment in [9,12]. In Section 3, we analyze the linear case and give some physical intuition
which explains the optimal damping rate and helps to explain why acceleration is possible. Then, in Section 4,
we discuss how to discretize the differential equations to obtain accelerated methods when the objective is smooth,
obtaining a variant of Nesterov’s accelerated gradient descent method. This analysis is then modiﬁed in Section 5 to
incorporate non-smooth composite objectives. In Section 6, we further modify the theory to incorporate stochastic
methods such as randomized coordinate descent. In Sections 5 and 6, we also show how accelerated methods for
composite optimization and a version of accelerated coordinate descent follow as special cases of our theory. Then,
in Section 7 we treat the convex case as well. Finally, we provide some concluding remarks and further research
directions.
2
The Differential Equations
In this section, we present the differential equations underlying accelerated ﬁrst-order optimization methods. The
theory in the convex, i.e. sublinear, case was considered in [8,9]. We are mainly concerned with the strongly convex
case, which was studied in [9,12], and we brieﬂy summarize some of their analysis in this section.
We ﬁrst recall the notion of strong convexity.
Deﬁnition 1. Let α > 0. A convex function f is α-strongly convex if for all x, y and g ∈∂f(y), it holds that
f(x) ≥f(y) + ⟨g, x −y⟩+ α
2 ∥x −y∥2.
(2)
2.1
Strongly Convex Dynamics
If f is α-strongly convex and differentiable, we consider the following damped Hamiltonian dynamics with potential
energy f (see [12], equation 7)
˙x = v, ˙v = −2√αv −∇f(x).
(3)
For non-differentiable f, we replace the gradient by an element of the sub-differential to obtain the dynamics
˙x = v, −2√αv −˙v ∈∂f(x).
(4)
Following the argument in [12], we use a Lyapunov function to prove that the objective error decreases at a linear rate
of −√α under this dynamics.
Theorem 1. Let f be α-strongly convex and differentiable. Assume that x(t) and v(t) obey the dynamics 4 (or
equivalently 3) and v(0) = 0. Then we have
f(x(t)) −f(x∗) ≤2e−√αt(f(x(0)) −f(x∗)),
(5)
where x∗minimizes f.
2

A PREPRINT - APRIL 2, 2021
Before we give the proof of Theorem 1, we remark upon the special damping rate of 2√α which is taken in (4).
In many practical instances, the strong convexity parameter may not be known and it may be a difﬁcult problem to
determine the correct damping rate. In this case, methods with an adaptive damping rate have been developed [17,18].
In addition, when the objective is only convex, but not strongly convex, the damping rate should be taken decreasing
to 0 at a rate of O( 1
t ) [8].
Proof of Theorem 1. Consider the Lyapunov function
L(t) = f(xt) −f(x∗) + 1
2∥√α(xt −x∗) + vt∥2.
Here we have written xt for x(t) and vt for v(t) to simplify notation. We will show that L′(t) ≤−√αL(t). This
completes the proof since
f(xt) −f(x∗) ≤L(t) ≤e−√αtL(0) = e−√αt(f(x0) −f(x∗) + α
2 ∥x0 −x∗∥2),
(6)
and α
2 ∥x0 −x∗∥2 ≤f(x0) −f(x∗) by the strong convexity of f, so that
f(xt) −f(x∗) ≤2e−√αt(f(x(0)) −f(x∗)).
(7)
Using the properies of the subdifferential, we bound L′(t) as follows:
L′(t) = ⟨gt, ˙xt⟩+ ⟨√α ˙xt + ˙vt, √α(xt −x∗) + vt⟩,
(8)
where gt := −2√αvt −˙vt ∈∂f(xt). We now use the dynamics 3 to evaluate ˙xt and ˙vt, we obtain
L′(t) = ⟨gt, vt⟩+ ⟨−√αvt −gt, √α(xt −x∗) + v⟩.
(9)
Simplifying this, we see that
L′(t) = −√α⟨gt, (xt −x∗)⟩−α⟨vt, (xt −x∗)⟩−√α⟨vt, vt⟩.
(10)
It is here that we use strong convexity, namely
⟨gt, (xt −x∗)⟩≥f(xt) −f(x∗) + α
2 ∥xt −x∗∥2,
for any gt ∈∂f(xt). Plugging this into 10 and simplifying the inner products we get
L′(t) ≤−√α

f(xt) −f(x∗) + 1
2∥√α(xt −x∗) + vt∥2

−
√α
2 ∥vt∥2,
(11)
which implies
L′(t) ≤−√αL(t)
(12)
as desired.
3
Intuition Behind the Dynamics
The intuition behind the dynamics in (3) comes from imagining a particle in a potential deﬁned by the objective f.
Without any damping (friction), the particle will oscillate freely in this potential and the total energy f(x) + 1
2∥v∥2
is conserved. The damping term −γv in (3) causes the particle to lose energy so that it will eventually settle at the
minimum of f.
We want to choose the damping rate γ so that the particle will settle in the minimum energy conﬁguration as fast as
possible. The optimal damping rate can be determined in the case when the objective f is quadratic, which provides
intuition behind the damping rate in (3). Finally, we consider discretizing (3) with a ﬁxed step-size in the quadratic
case to help provide intuition about why acceleration is possible.
So let f(x) =
1
2xT Ax be a positive deﬁnite quadratic objective. We diagonalize A and write x(t) in the basis of
eigenvectors of A as
x(t) =
X
k
xk(t)wk, v(t) =
X
k
vk(t)wk,
(13)
where w1, ..., wd are the eigenvectors of A with corresponding eigenvalues 0 < λ1 ≤· · · ≤λd. The dynamics
decouples across each of the eigendirections and we get
˙xk(t) = vk(t), ˙vk(t) = −γvk(t) −λkxk(t),
(14)
3

A PREPRINT - APRIL 2, 2021
a damped harmonic oscillator for each eigendirection.
Analyzing a damped harmonic oscillator is an undergraduate physics exercise. The characteristic polynomial of (14)
is
p(z) = z2 + γz + λk.
(15)
The roots of this polynomial, z =
1
2(−γ ±
p
γ2 −4λk), determine the behavior of the harmonic oscillator. The
qualitative behavior of the system depends upon whether the characteristic polynomial has two real roots, a repeated
real root, or two imaginary roots.
If γ2 −4λk > 0, the characteristic polynomial has two real roots. This is the over-damped regime, and the oscillator
stays on the same side of equilibrium throughout the dynamics. The decay rate of the harmonic oscillator is dominated
by the largest root, z = 1
2(−γ ±
p
γ2 −4λk).
If γ2 −4λk < 0, the characteristic polynomial has two imaginary roots. This is the under-damped regime, because
the oscillator swings back and forth, losing energy in each oscillation. The damping is not strong enough to keep
the oscillator on the same side of equilibrium. The decay rate in this regime is equal to the real part of the roots,
Re(z) = −1
2γ.
If γ2 −4λk = 0, the characteristic polynomial has repeated real roots. This is the critical damping rate for harmonic
oscillator. The oscillator stays on the same side of equilibrium but decays toward equilibrium as fast as possible. The
decay rate is equal to the root z = −1
2γ.
Fixing λk, we see that the fastest decay rate possible is −√λk, which occurs for a critically damped harmonic oscil-
lator. This follows since in the under damped regime the decay rate is −1
2γ, with γ < 2√λk, and in the over damped
regime we have
γ2 −4λk = (γ + 2
p
λk)(γ −2
p
λk) > (γ −2
p
λk)2
(16)
The last inequality occurs because γ > 2√λk. This means that
p
γ2 −4λk > γ −2
p
λk
and so for the larger real root
z = 1
2(−γ +
p
γ2 −4λk) > −
p
λk
Now, in the dynamics (3), we must choose a single damping rate γ for all eigenvalues λk. In order to obtain the fastest
possible decay rate, we want to maximize the slowest decay rate among the λk.
Notice that the fastest this decay rate can be is −√λ1, since the harmonic oscillator corresponding to the smallest
eigenvalue cannot decay any faster. Moreover, this decay rate is achieved when γ is chosen so that the mode corre-
sponding to λ1 is critically damped, because then all other modes will be under damped and will also decay at the rate
−1
2γ.
What about the step size required for a stable discretization of the dynamics? Just as we chose a single damping rate
for all eigenmodes, we must choose a single step size for all of the eigenvalues λk. If we use an integrator whose
region of stability contains the negative unit semicircle (such as Runge-Kutta 4 [19,20]), then the discretization of the
harmonic oscillator corresponding to λk will be stable if
1
∆t ≥max{|z1|, |z2|}
where z1 and z2 are the (possibly complex) roots of the characteristic equation. Since the optimal damping parameter
is 2√α, all of the modes are either critically damped or under damped. So the roots zi are complex and we calculate
|z1| = |z2| = 1
2
p
γ2 + 4λk −γ2 =
p
λk ≤
p
λd
So we see that the dynamics decays with exponential rate −√λ1 and the step size required for a stable discretiziation
is ∆t < 1/√λd. This explains why discretizing (3) produces a method which requires O(√κ) iterations to converge,
where κ = λd
λ1 is the condition number.
Indeed, for the quadratic problem we can use any integrator whose region of stability contains the negative unit
semicircle to obtain an accelerated method. Compare the resulting methods with the Chebyshev semi-iterative methods
used for solving linear systems, see [21], Section 10.1.5 or [22], Chapter 5. The Chebyshev semi-iterative methods use
(shifted, depending upon the eigenvalue bounds of A) Chebyshev polynomials, while the accelerated methods derived
above use a different sequence of polynomials with the same asymptotics.
4

A PREPRINT - APRIL 2, 2021
4
Discrete Dynamics for Smooth Objectives
Theorem 1 concerns the convergence of the dynamics (3) in the continuous case. The existence of discrete schemes
which are able to achieve accelerated convergence has been known since the work of Polyak and Nesterov [1, 2]. In
this section, we connect these two viewpoints and derive a version of Nesterov’s accelerated gradient descent [2] as
a particular discretization of (3). The method is made up of a forward step in position (x), a semi-implicit step in
velocity (v), and ﬁnally a small perturbation (second order in the step size) which ensures a sufﬁcient decrease in
the Lyapunov function. This perturbation moves x to decrease the objective and moves v to compensate, so that the
second part of the Lyapunov function (the squared norm) doesn’t increase. These steps are all explained in detail in
this section.
We begin by brieﬂy recalling the deﬁnition of smoothness.
Deﬁnition 2. Let L > 0. A differentiable function f is L-smooth if
∥∇f(x) −∇f(y)∥≤L∥x −y∥.
(17)
It is an easy consequence of the above deﬁnition that
f(y) ≤f(x) + ⟨∇f(x), y −x⟩+ L
2 ∥x −y∥2.
(18)
Our goal will now be to discretize (3) so that a discrete version of the Lyapunov function in the proof of Theorem 1,
for instance
Ln = f(xn) −f(x∗) + 1
2∥√α(xn −x∗) + vn∥2,
will be decreased by a constant factor in each timestep. In order to make the Lyapunov argument work, however, a
slight modiﬁcation is necessary. We will actually consider the discrete Lyapunov function
Ln = f(xn) −f(x∗) + 1
2∥√α(xn −x∗) + (1 + s√α)vn∥2,
(19)
where s is the step size of the discretization.
We ﬁrst note that the L-smoothness of f implies that
f

xn −1
L∇f(xn)

−f(x∗) ≤f(xn) −f(x∗) −1
2L∥∇f(xn)∥2,
i.e. that taking a small gradient step ensures a decrease of the objective. Observe also that the second part of the
Lyapunov function,
1
2∥√α(xn −x∗) + (1 + s√α)vn∥2,
can be kept constant by adjusting vn appropriately, which will guarantee a sufﬁcient decrease of the entire Lyapunov
function. Speciﬁcally, we adjust vn so that √α(xn −x∗) + (1 + s√α)vn is kept constant. Thus, if we subtract
1
L∇f(xn) from xn, then we must add (1 + s√α)−1
√α
L to vn. Putting this together, we obtain the following update:
xn ←xn −1
L∇f(xn), vn ←vn + (1 + s√α)−1
√α
L ∇f(xn),
(20)
which decreases the Lyapunov function by
1
2L∥∇f(xn)∥2, i.e. so that
Ln ←Ln −1
2L∥∇f(xn)∥2.
We call this a sufﬁcient decrease update and it plays an important role in ensuring the global convergence of our
discretization, which we now introduce.
Consider the following discretization of (3), which consists of a forward Euler step in x and a semi-implicit step in v,
followed by a sufﬁcient decrease update:
x′
n = xn + svn
v′
n = vn −(1 + s√α)−1  s√αvn + s∇f(x′
n)

−s√αv′
n
xn+1 = x′
n −1
L∇f(x′
n)
vn+1 = v′
n + (1 + s√α)−1
√α
L ∇f(x′
n).
(21)
5

A PREPRINT - APRIL 2, 2021
The most complicated part of this discretization is the semi-implicit update for v. Unfortunately, we haven’t found a
simpler update for which the discretization remains stable, i.e. for which global convergence can be proved.
We now show that as long as the step size s ≤
1
√
L, the discretization (21) converges linearly. Note that it is critical
that we are able to take the step size as large as
1
√
L. This results in an accelerated convergence rate of (1 + κ−1
2 )−n.
Theorem 2. If s ≤
1
√
L and f is α-strongly convex and L-smooth, then the iteration (21) satisﬁes
Ln+1 ≤(1 + s√α)−1Ln.
In particular, if v0 = 0, then
f(xn) −f(x∗) ≤2
 1 + s√α
−n (f(x0) −f(x∗)).
Before proving Theorem 2, we compare (21) with the traditional Nesterov accelerated gradient scheme [2], which is
given by:
xn = yn −1
L∇f(yn)
yn+1 = xn +
 √
L −√α
√
L + √α
!
(xn −xn−1),
(22)
and the heavy ball method [1], given by:
xn+1 = xn −
4
(
√
L + √α)2 ∇f(xn) +
 √
L −√α
√
L + √α
!2
(xn −xn−1).
(23)
By setting vn = xn −xn−1, both of these methods can be seen to be discretizations of the dynamics (3), with
Nesterov’s scheme corresponding to a step size of
1
√
L and Polyak’s method corresponding to a larger step size of
2
√
L+√α. However, Polyak’s larger step size comes at the cost of only local convergence and convergence for quadratic
objectives, while Nesterov’s scheme enjoys global convergence for general strongly convex objectives.
In Theorem 2, we are able to take a step size which matches, but does not exceed, Nesterov’s scheme. In addition,
our convergence rate of (1 + κ−1
2 )−n is slightly worse than Nesterov’s rate of (1 −κ−1
2 )n. For this reason, we don’t
expect (21) to perform better in practice than Nesterov’s scheme. We merely present it as a connection between the
continuous dynamics (3) and discrete accelerated methods.
Proof of Theorem 2. Throughout the proof, we will use the following elementary fact. Let tn be some quantity which
changes throughout our iteration. Then
1
2∥tn+1∥2 −1
2∥tn∥2 = ⟨tn+1 −tn, tn⟩+ 1
2∥tn+1 −tn∥2
and
1
2∥tn+1∥2 −1
2∥tn∥2 = ⟨tn+1 −tn, tn+1⟩−1
2∥tn+1 −tn∥2.
Applying each of these identities once, we see that if tn+1 −tn = a + b, then
1
2∥tn+1∥2 −1
2∥tn∥2 = ⟨a, tn⟩+ ⟨b, tn+1⟩+ 1
2∥a∥2 −1
2∥b∥2.
We will use these identities without explicit mention in what follows. We now prove that
Ln+1 ≤Ln −s√αLn+1,
where Ln is the Lyapunov function given in (19). To do so, we calculate the change in L due to the forward step in x,
L(x′
n, vn) −Ln = f(x′
n) −f(xn) + s⟨√αvn, √α(x′
n −x∗) + (1 + s√α)vn⟩
−s2α
2 ∥vn∥2,
(24)
and the change due to the semi-implicit step for v, noting that the change in v can be broken up as
(1 + s√α)(v′
n −vn) = −(s√αvn + s∇f(x′
n)) −s√α(1 + s√α)v′
n,
6

A PREPRINT - APRIL 2, 2021
to get
L(x′
n, v′
n) −L(x′
n, vn) = −s⟨√αvn + ∇f(x′
n), √α(x′
n −x∗) + (1 + s√α)vn⟩
−s⟨√α(1 + s√α)v′
n, √α(x′
n −x∗) + (1 + s√α)v′
n⟩
+ s2
2 ∥√αvn + ∇f(x′
n)∥2
−s2α
2 ∥v′
n∥2.
(25)
Adding equations (24) and (25), collecting terms, and recalling that x′
n −xn = svn, we obtain
L(x′
n, v′
n) −Ln =f(x′
n) −f(xn) −⟨∇f(x′
n), x′
n −xn⟩
−s√α⟨∇f(x′
n), x′
n −x∗⟩
−s√α⟨(1 + s√α)v′
n, √α(x′
n −x∗) + (1 + s√α)v′
n⟩
+ s2
2 ∥∇f(x′
n)∥2 −s2α
2 ∥v′
n∥2.
(26)
The terms on the ﬁrst line are ≤0, by the convexity of f. The inner product on the second line can be bounded using
the strong convexity of f, as
⟨∇f(x′
n), x′
n −x∗⟩≥f(x′
n) −f(x∗) + α
2 ∥x′
n −x∗∥2.
Plugging this bound into equation (26) and completing the square with line three, yields
L(x′
n, v′
n) −Ln ≤−s√αL(x′
n, v′
n) + s2
2 ∥∇f(x′
n)∥2
−s2α
2 ∥v′
n∥2 −s√α
2
(1 + s√α)2∥v′
n∥2.
(27)
The sufﬁcient decrease update now decreases the Lyapunov function by at least s2
2 ∥∇f(x′
n)∥2 since s ≤
1
√
L, and we
obtain
Ln+1 −Ln ≤−s√αL(x′
n, v′
n).
(28)
To complete the proof, we merely note that Ln+1 ≤L(x′
n, v′
n) (the sufﬁcient decrease update decreased the Lyapunov
function). This implies
Ln+1 −Ln ≤−s√αLn+1
(29)
as desired.
By induction, we thus have
f(xn) −f(x∗) ≤Ln ≤
 1 + s√α
−n L0.
Finally, if v0 = 0, we get, since f is α-strongly convex,
L0 = f(x0) −f(x∗) + α
2 ∥x0 −x∗∥2 ≤2(f(x0) −f(x∗)).
5
Discrete Dynamics for Non-smooth Objectives
In this section we consider the situation where the objective f is not smooth, but we still assume that f is α-strongly
convex. We ﬁrst consider simply replacing ∇f(x′
n) by some element in the sub-differential of f, say gn ∈∂f(x′
n).
We observe that the argument for smooth functions continues to apply as long as we can ﬁnd an analog of the sufﬁcient
decrease update (20), i.e. if we have an update
x′
n ←x′
n −δn, vn ←vn + (1 + s√α)−1√αδn
(30)
such that
f(x′
n −δn) ≤f(x′
n) −s2
2 ∥gn∥2.
(31)
7

A PREPRINT - APRIL 2, 2021
Unfortunately, in many cases of interest this is not possible. To get around this, we allow gn /∈∂f(x′
n), and generalize
the condition (31). In particular, we will show that our Lyapunov argument still works if we replace the decrease
condition (31) by the more general condition
∀z, f(x′
n −δn) −f(z) ≤⟨gn, x′
n −z⟩−α
2 ∥x′
n −z∥2 −s2
2 ∥gn∥2.
(32)
Let us examine this condition for a moment. Note that if gn ∈∂f(x′
n), then by the strong convexity, we have
∀z, f(x′
n) −f(z) ≤⟨gn, x′
n −z⟩−α
2 ∥x′
n −z∥2.
So, in this case, the above condition is equivalent to a decrease in the objective.
f(x′
n −δn) = f(x′
n −δn) ≤f(x′
n) −s2
2 ∥gn∥2.
(33)
So what the new condition (32) does is simply to allow gn /∈∂f(x′
n), but still to enforce a combined decrease and
strong convexity condition. We will see that for many problems of interest, in particular composite optimization, gn
and δn can be chosen to satisfy this condition. This will lead to an accelerated version of forward-backward iteration,
which is similar to FISTA [3] and the methods in [14].
Inserting this into the scheme (21), we arrive at the following discretization.
x′
n = xn + svn
v′
n = vn −(1 + s√α)−1  s√αvn + sgn

−s√αv′
n
xn+1 = x′
n −δn
vn+1 = v′
n + (1 + s√α)−1√αδn,
(34)
where gn and δn are chosen so that (32) holds.
This is the same as in the smooth case, except that the gradients have been replaced by gn and the sufﬁcient decrease
update has been changed. We now prove that this scheme leads to an accelerated method. The proof is very similar to
the smooth case. In this case, we do not even need to assume the strong convexity of f. This assumption is subsumed
by (32).
Theorem 3. Assume that gn and δn are chosen so that the condition (32) holds at every iteration of the scheme 34.
Then we have
Ln+1 ≤(1 + s√α)−1Ln,
where Ln is the same Lyapunov function as in the smooth case (equation (19)).
In particular, if v0 = 0, we get
f(xn) −f(x∗) ≤
 1 + s√α
−n 
f(x0) −f(x∗) + α
2 ∥x0 −x∗∥2
.
Proof. We proceed exactly as in the proof of theorem 2, replacing ∇f(x′
n) by gn to obtain, in place of equation (26),
L(x′
n, v′
n) −Ln =f(x′
n) −f(xn) −⟨gn, x′
n −xn⟩
−s√α⟨gn, x′
n −x∗⟩
−s√α⟨(1 + s√α)v′
n, √α(x′
n −x∗) + (1 + s√α)v′
n⟩
+ s2
2 ∥gn∥2 −s2α
2 ∥v′
n∥2.
(35)
Noting that by the construction of the sufﬁcient decrease update (the last two lines of (34)),
Ln+1 −L(x′
n, v′
n) = f(xn+1) −f(x′
n),
we get
Ln+1 −Ln =f(xn+1) −f(xn) −⟨gn, x′
n −xn⟩
−s√α⟨gn, x′
n −x∗⟩
−s√α⟨(1 + s√α)v′
n, √α(x′
n −x∗) + (1 + s√α)v′
n⟩
+ s2
2 ∥gn∥2 −s2α
2 ∥v′
n∥2.
(36)
8

A PREPRINT - APRIL 2, 2021
We now apply the decrease condition (32) with z = xn and z = x∗to the ﬁrst two lines of this equation. This gives
Ln+1 −Ln ≤−α
2 ∥x′
n −xn∥2 −s2
2 ∥gn∥2
−s√α

f(xn+1) −f(x∗) + α
2 ∥x′
n −x∗∥2 + s2
2 ∥gn∥2

−s√α⟨(1 + s√α)v′
n, √α(x′
n −x∗) + (1 + s√α)v′
n⟩
+ s2
2 ∥gn∥2 −s2α
2 ∥v′
n∥2.
(37)
Completing the square and noting that the sufﬁcient decrease update (the last two lines of (34)) is designed so that
1
2∥√α(x′
n −x∗) + (1 + s√α)v′
n∥2 = 1
2∥√α(xn+1 −x∗) + (1 + s√α)vn+1∥2,
we see that
Ln+1 −Ln ≤−s√αLn+1
(38)
as desired.
As in the proof of Theorem 2, by induction we have
f(xn) −f(x∗) ≤Ln ≤
 1 + s√α
−n L0.
Finally, if v0 = 0, we get
L0 = f(x0) −f(x∗) + α
2 ∥x0 −x∗∥2,
which proves the second statement.
5.1
Accelerated Forward-Backward Splitting
In this subsection, we apply theorem 3 to strongly convex composite objectives, i.e. objectives of the form
f(x) = g(x) + h(x),
(39)
where g is α-strongly convex and L-smooth, and h is an arbitrary convex function. We also assume that we are able
to compute a proximal update for h, i.e. solve
y∗= proxs,h(x) = arg min
y
h(y) + 1
2s∥y −x∥2
(40)
The proximal update is essentially a step of backward Euler with step size s, hence the name accelerated forward-
backward method. For many convex functions of interest, the proximal update can be efﬁciently computed.
For example, if h is the characteristic function of a convex set S, then (40) is just a projection onto S. In this case,
Theorem 3 recovers a version of accelerated projected gradient descent for strongly convex objectives.
Another example of interest is h(x) = ∥x∥1, in which case (40) is just soft-thresholding with parameter s. In this
instance Theorem 3 recovers a version of FISTA [3], and more generally for other h we obtain a version of accelerated
forward-backward descent [14], which are designed for strongly convex objectives.
Our goal is to show how gn and δn can be chosen to satisfy the decrease condition (32). The next lemma answers this
question for us.
Lemma 1. Assume that f(x) = g(x)+h(x), with g an L-smooth, α-strongly convex function and h a convex function.
Then setting
gn = 1
s2
 x′
n −proxs2,h
 x′
n −s2∇g(x′
n)

and δn = s2gn will satisfy the condition (32) as long as s ≤
1
√
L.
Again it is important that we are able to choose s as large as
1
√
L, since this results in the accelerated rate of (1 +
κ−1
2 )−n.
9

A PREPRINT - APRIL 2, 2021
Proof. Note that the choice of gn and δn implies that
xn+1 = arg min
x
h(x) +
1
2s2 ∥x −
 x′
n −s2∇g(x′
n)

∥2.
(41)
We denote by yn the intermediate point x′
n −s2∇g(x′
n) so that the above becomes
xn+1 = arg min
x
h(x) +
1
2s2 ∥x −yn∥2.
(42)
Now let z be arbitrary and note that our goal is to bound
f(xn+1) −f(z) = (g(xn+1) −g(z)) + (h(xn+1) −h(z)).
We consider the second of these terms ﬁrst. Note that equation (42) implies that L(yn −xn+1) ∈∂h(xn+1). This
means that (since h is convex)
h(xn+1) −h(z) ≤1
s2 ⟨(yn −xn+1), xn+1 −z⟩.
(43)
We proceed to bound
g(xn+1) −g(z) = (g(xn+1) −g(x′
n)) + (g(x′
n) −g(z)).
(44)
The ﬁrst term above is bounded due to the L-smoothness of g and the assumption that s2 ≤1
L:
g(xn+1) −g(x′
n) ≤⟨∇g(x′
n), xn+1 −x′
n⟩+
1
2s2 ∥xn+1 −x′
n∥2.
The second is bounded due to the strong convexity of g:
g(x′
n) −g(z) ≤⟨∇g(x′
n), x′
n −z⟩−α
2 ∥x′
n −z∥2.
Combining these two bounds with equation (43) and noting that
gn = 1
s2 (x′
n −xn+1) = 1
s2 (yn −xn+1) + ∇g(x′
n),
we obtain
f(xn+1) −f(z) ≤⟨gn, xn+1 −z⟩−α
2 ∥x′
n −z∥2 + s2
2 ∥gn∥2.
(45)
Now we write
xn+1 −z = xn+1 −x′
n + x′
n −z = −s2gn + x′
n −z,
to get
f(xn+1) −f(z) ≤⟨gn, x′
n −z⟩−α
2 ∥x′
n −z∥2 −s2
2 ∥gn∥2,
(46)
which is exactly (32).
6
Discrete Stochastic Dynamics
In this section, we extend our theory to what we call stochastic discretizations. By this we simply mean schemes which
introduce randomness in each iteration. These are not really discretizations of the dynamics (3) in a strict sense of the
term, but lead to a class of accelerated methods nonetheless. The important new step here is to modify the sufﬁcient
decrease update appropriately when the gradient is sampled randomly in a certain sense. To illustrate the ideas, we
derive a variant of accelerated coordinate descent.
We begin by considering smooth objectives f. Recall that for smooth objectives the sufﬁcient decrease update was
critical for obtaining a stable accelerated method. What we needed was a way of decreasing the objective sufﬁciently
by perturbing x. Then we could perturb v appropriately to keep the second term in our Lyapunov function constant.
In this way, we obtained an update of the form (20), which reduced the Lyapunov function by at least s2
2 ∥∇f(x′
n)∥2.
When deriving stochastic accelerated methods, we want to replace ∇f(x′
n) by some sample gn, where En(gn) =
∇f(x′
n) (here En denotes the expectation taken with respect to randomness introduced in iteration n, essentially a
conditional expectation). It turns out that this will work as long as we can guarantee an objective decrease of at least
10

A PREPRINT - APRIL 2, 2021
s2
2 ∥gn∥2. Note here that there is no expectation inside of the norm, i.e. this is the norm of the actual gradient sample
encountered at iteration n.
We consider the following discretization:
x′
n = xn + svn
v′
n = vn −(1 + s√α)−1  s√αvn + sgn

−s√αv′
n
xn+1 = x′
n −δn
vn+1 = v′
n + (1 + s√α)−1√αδn,
(47)
where En(gn) = ∇f(x′
n) and δn is chosen (dependent on gn) so that
f(xn+1) ≤f(x′
n) −s2
2 ∥gn∥2.
(48)
We show that this method will achieve an accelerated convergence rate under this conditions.
Theorem 4. Assume that f is α-strongly convex and differentiable. Then as long as condition (48) holds, the iterates
of (47) will satisfy
En(Ln+1) ≤(1 + s√α)−1Ln,
where Ln is the same Lyapunov function introduced in (19).
In particular, if v0 = 0 we have
E(f(xn) −f(x∗)) ≤2(1 + s√α)−n(f(x0) −f(x∗)).
This theorem will follow as a special case of the theorem for non-smooth schemes, so we omit the proof for the
moment.
We now turn to stochastic schemes for non-smooth functions. The idea is very similar to the deterministic scheme for
non-smooth functions. We want to choose gn as a random sample of an element in the subgradient ∂f(x′
n), however,
we don’t restrict En(gn) ∈∂f(x′
n). Instead, we enforce a stochastic version of the constraint (32). The scheme we
consider is same as (34)
x′
n = xn + svn
v′
n = vn −(1 + s√α)−1  s√αvn + sgn

−s√αv′
n
xn+1 = x′
n −δn
vn+1 = v′
n + (1 + s√α)−1√αδn,
(49)
except that we allow gn and δn to be (potentially dependent) random variables and enforce the following condition
∀z, f(xn+1) −f(z) ≤⟨En(gn), x′
n −z⟩−α
2 ∥x′
n −z∥2 −s2
2 ∥gn∥2.
(50)
which only differs from (32) in the expectation taken in the ﬁrst inner product.
Note ﬁrst that the schemes (47) and (49) are exactly the same, except that the condition (50) is weaker than the
conditions enforced in the smooth case. This follows since if f is differentiable and α-strongly convex, then
∀z, f(x′
n) −f(z) ≤⟨∇f(x′
n), x′
n −z⟩−α
2 ∥x′
n −z∥2.
This, combined with the requirement in (47) that En(gn) = ∇f(x′
n) and that δn is chosen so that (48) holds, implies
the condition (50). This means that the proof below will imply Theorem 4. In fact, the scheme (49) is most often
applied in this way to smooth, strongly convex functions. In this case it leads to different versions of accelerated
coordinate descent, as we will show later.
We now prove that the scheme (49) achieves the desired accelerated convergence rate. As in the non-smooth determin-
istic case, we don’t need to assume that our objective is strongly convex. This assumption is superseded by condition
(50).
Theorem 5. Assume that gn and δn are chosen so that the condition (50) holds at every iteration of the scheme (49).
Then we will have
En(Ln+1) ≤(1 + s√α)−1Ln,
where Ln is the same Lyapunov function as in the smooth case (equation (19)).
In particular, if v0 = 0, we will have
E(f(xn) −f(x∗)) ≤
 1 + s√α
−n 
f(x0) −f(x∗) + α
2 ∥x0 −x∗∥2
.
11

A PREPRINT - APRIL 2, 2021
Proof of Theorem 5. As in the proof of Theorem 3, we obtain (equation (35))
L(x′
n, v′
n) −Ln =f(x′
n) −f(xn) −⟨gn, x′
n −xn⟩
−s√α⟨gn, x′
n −x∗⟩
−s√α⟨(1 + s√α)v′
n, √α(x′
n −x∗) + (1 + s√α)v′
n⟩
+ s2
2 ∥gn∥2 −s2α
2 ∥v′
n∥2.
(51)
We note that the sufﬁcient decrease update (the last two lines of (5)) implies that Ln+1−L(x′
n, v′
n) = f(xn+1)−f(x′
n),
so that
Ln+1 −Ln =f(xn+1) −f(xn) −⟨gn, x′
n −xn⟩
−s√α⟨gn, x′
n −x∗⟩
−s√α⟨(1 + s√α)v′
n, √α(x′
n −x∗) + (1 + s√α)v′
n⟩
+ s2
2 ∥gn∥2 −s2α
2 ∥v′
n∥2.
(52)
We now apply the decrease condition (50) with z = xn and z = x∗to the ﬁrst two lines of this equation. Because
we have an expectation En(gn) in the condition (50), this results in extra terms which are an inner produce with the
difference gn −En(gn). We get
Ln+1 −Ln ≤−α
2 ∥x′
n −xn∥2 −s2
2 ∥gn∥2 −⟨gn −En(gn), x′
n −xn⟩
−s√α

f(xn+1) −f(x∗) + α
2 ∥x′
n −x∗∥2 + s2
2 ∥gn∥2

−s√α⟨gn −En(gn), x′
n −x∗⟩
−s√α⟨(1 + s√α)v′
n, √α(x′
n −x∗) + (1 + s√α)v′
n⟩
+ s2
2 ∥gn∥2 −s2α
2 ∥v′
n∥2.
(53)
Collecting terms and completing the square as in the previous proofs, we get (throwing out unnecessary negative
terms)
Ln+1 −Ln ≤−s√α

f(xn+1 −f(x∗) + 1
2∥√α(x′
n −x∗) + (1 + s√α)v′
n∥2

−⟨gn −En(gn), x′
n −xn⟩
−s√α⟨gn −En(gn), x′
n −x∗⟩.
(54)
Recalling that the sufﬁcient decrease update (the last two lines of (5)) was designed so that
1
2∥√α(x′
n −x∗) + (1 + s√α)v′
n∥2 = 1
2∥√α(xn+1 −x∗) + (1 + s√α)vn+1∥2,
we see that
Ln+1 −Ln ≤−s√αLn+1
−⟨gn −En(gn), x′
n −xn⟩
−s√α⟨gn −En(gn), x′
n −x∗⟩.
(55)
Finally, we take the expectation En on both sides to obtain
En(Ln+1) −Ln ≤−s√αEn(Ln+1).
which implies
En(Ln+1) ≤(1 + s√α)−1Ln,
(56)
as desired.
Taking the expectation with respect to the randomness introduced in previous iterations, we obtain
E(Ln+1) ≤(1 + s√α)−1E(Ln),
12

A PREPRINT - APRIL 2, 2021
so that
E(f(xn) −f(x∗)) ≤E(Ln+1) ≤(1 + s√α)−nL0.
Finally, if v0 = 0 we get
L0 ≤f(x0) −f(x∗) + α
2 ∥x0 −x∗∥2,
which completes the proof of the second statement. Note that if f is α-strongly convex, then
L0 ≤2(f(x0) −f(x∗)),
which applies in the case of Theorem 4.
6.1
Accelerated Coordinate Descent
We now show how two versions of accelerated coordinate descent follow as special cases of Theorem 4.
The premise we consider is that the objective f is α-strongly convex and the gradient is coordinate-wise Li-smooth,
i.e. if we denote by ∇f(x)i the i-th coordinate of the gradient of f, then
|∇f(x)i −∇f(x + cei)i| ≤Li|c|.
Intuitively, this means that the i-th diagonal entry of the Hessian of f is bounded by Li at each point.
Note also that the Li-smoothness implies that we obtain a sufﬁcient decrease when moving in the direction i, i.e.
f

x −1
Li
∇f(x)i

≤f(x) −
1
2Li
∥∇f(x)i∥2.
This will be exactly what we need when choosing gn and δn in the scheme (47).
To obtain accelerated coordinate descent, we choose a coordinate i at random, with the probability of choosing coor-
dinate i proportional to √Li. We then set
gn =
1
s√Li
∇f(x′
n)i.
and choose s so that En(gn) = ∇f(x′
n). This gives a step size of
s =
 n
X
i=1
p
Li
!−1
.
We now note that the required decrease in (48) is
f(xn+1) ≤f(x′
n) −s2
2 ∥gn∥2 = f(x′
n) −∥sgn∥2 = f(x′
n) −
1
2Li
∥∇f(x)i∥2.
So we must simply choose δn so that
f(x′
n −δn) ≤f(x′
n) −
1
2Li
∥∇f(x)i∥2.
(57)
By the Li-smoothness, the choice δn =
1
Li ∇f(x)i will work. This recovers a version of the original accelerated
coordinate descent in [5]. The relationship between the new method and Nesterov’s original method is similar to the
relationship in the deterministic case described in section 4.
Another possible choice is to choose δn =
1
Lj ∇f(x)j where j is the coordinate which maximizes the decrease
1
2Lj ∥∇f(x)j∥2. This recovers the accelerated semi-greedy scheme presented in [23].
Finally, we would like to conclude by explaining why accelerated coordinate descent is very efﬁcient if ∇f(x)i can
be calculated using only a small number (say k ≪n) of entries xi1, ..., xik.
The issue is that each step of accelerated coordinate descent requires updating the full vectors x and v. Namely in
the ﬁrst and second steps in (47) we update the whole vectors, while in the last two steps we only update the selected
coordinate i (gn and δn are only non-zero in one coordinate).
13

A PREPRINT - APRIL 2, 2021
The key observation is that we actually don’t need to update all of the x and v coordinates in each step. To see why,
imagine for a moment that gn = δn = 0 in each step. Then the iteration becomes
xn+1 = xn + svn
vn+1 = (1 + s√α)−2vn,
(58)
which can be solved in closed form any number of iterations in the future. The key is to treat all of the coordinates
in this manner between their selection times. When coordinate i is selected, this simple iteration must be modiﬁed to
include ∇f(x)i. However, until it is selected again, we can evaluate xi and vi at any iteration in closed form, as long
as we know how many iterations have passed and what the values were the last time i was selected. This observation
permits an efﬁcient implementation of (47) as long as ∇f(x)i doesn’t depend on too many indices i1, ..., ik. This
idea can be used to construct fast solvers for sparse, symmetric, diagonally dominant linear systems; compare with the
work in [24], for instance.
7
Accelerated Methods in the Convex Case
In the rest of the article, we were mainly interested in the case of strongly convex objectives f. In this section, we treat
the convex case as well. So throughout this section, we assume that f is convex and L-smooth, but not necessarily
strongly convex. In this case, the differential equations modelling accelerated gradient descent were ﬁrst determined
and analyzed in [8]. The equations introduced there are
˙x = v, ˙v = −r
t v −∇f(x),
(59)
with r ≥3. So for objectives which are not strongly convex, the damping rate isn’t ﬁxed, but should decay at a rate of
O( 1
n). With the help of the Lyapunov function
L(t) = t2(f(x(t)) −f(x∗)) + 1
2∥(r −1)(x(t) −x∗) + tv(t)∥2,
(60)
it is shown in [8] that the dynamics (59) with v(0) = 0 satisﬁes
f(x(t)) −f(x∗) ≤(r −1)2
2t2
∥x(0) −x∗∥2.
(61)
The discrete scheme analyzed in [8] in this case is given by
x0 = y0, xn+1 = yn −1
L∇f(yn), yn+1 = xn+1 +
n
n + r (xn+1 −xn).
(62)
Inspired by this, in [25], the following somewhat more general scheme is analyzed:
x0 = y0, xn+1 = yn −γn∇f(yn), yn+1 = xn+1 + αn(xn+1 −xn),
(63)
where the following convergence theorem is proved.
Theorem 6 (Theorem 3.3.2 in [25]). Let qn be a sequence of non-negative real number satisfying q0 = 0 and
(qn+1 + 1)2 ≤(qn + 2)2 + 1.
(64)
Then, if we set αn =
qn
2+qn+1 in (63) and choose γn to be decreasing and satisfying the descent condition
f(xn+1) ≤f(yn) −γn
2 ∥∇f(yn)∥2,
(65)
then we have
f(xn) −f(x∗) ≤
2
γnqn(qn + 2)∥x0 −x∗∥2.
(66)
Note that in order to apply Theorem 6 we do not even need to know the smoothness parameter L. We simply need
to ensure that step γn is chosen small enough to satisfy (65) (and is decreasing). If f is L-smooth, γn =
1
L, and
qn =
2n
r−1, we obtain the scheme (62) from [8]. We can obtain a slightly faster convergence rate by enforcing equality
in (64).
Our purpose in this section is to extend this result to the non-smooth setting, by modifying the descent condition (65).
14

A PREPRINT - APRIL 2, 2021
Similar to the approach in Section 5, the key to the analysis in the non-smooth case is to replace the descent condition
(65) by the condition
∀z, f(xn+1) −f(z) ≤⟨gn, yn −z⟩−γn
2 ∥gn∥2,
(67)
where xn+1 = yn −γngn for an appropriately chosen gn and γn. Note that if f is L-smooth and we set gn = ∇f(xn)
and γn ≤1
L, then this condition follows from (65) since f(yn) −f(z) ≤⟨∇f(yn), yn −z⟩by the convexity of y.
The most important instance where the condition (67) can be guaranteed for a non-smooth function f is when the
objective f is of the form
f(x) = g(x) + h(x),
(68)
where g is L-smooth and convex, and h is a potentially non-smooth function for which we can calculate the proximal
step in equation (40). In this case, updating xn+1 via a forward-backward step
xn+1 = proxγn,h(yn −γn∇g(yn)),
(69)
enables us to ensure that condition (67) is satisﬁed for γn ≤
1
L (here we set gn =
1
γn (yn −xn+1) so that xn+1 =
yn −γngn). In particular, we have the following analog of Lemma 1.
Lemma 2. Suppose that f(x) = g(x) + h(x) with g(x) an L-smooth convex function and h(x) a convex function.
Then, setting
xn+1 = proxγn,h(yn −γn∇g(yn)),
(70)
or, written another way, xn+1 = yn −γngn with
gn = 1
γn
(yn −proxγn,h(yn −γn∇g(yn))),
(71)
we guarantee that condition (67) holds as long as γn ≤1
L.
Proof. Note that from equation (70), we see that
gn = 1
γn
(yn −xn+1) = (∇g(yn) + dn),
(72)
where dn ∈∂h(xn+1). Further, the convexity of g and h implies that for any z we have
g(yn) −g(z) ≤⟨∇g(yn), yn −z⟩,
(73)
and
h(xn+1) −h(z) ≤⟨dn, xn+1 −z⟩.
(74)
In addition, the L-smoothness of g implies that
g(xn+1) −g(yn) ≤⟨∇g(yn), xn+1 −yn⟩+ L
2 ∥xn+1 −yn∥2.
(75)
Adding together equations (73), (74), and (75), we get (using that yn −xn+1 = γngn)
f(xn+1) −f(yn) ≤⟨gn, yn −z⟩−γn⟨dn + ∇g(yn), gn⟩+ Lγ2
n
2 ∥gn∥2
= ⟨gn, yn −z⟩+ γn
Lγn
2
−1

∥gn∥2.
(76)
Finally, setting γn ≤1
L guarantees that Lγn
2
−1 ≤−1
2, which completes the proof.
We proceed to analyze the following accelerated scheme:
x0 = y0, xn+1 = yn −γngn, yn+1 = xn+1 + αn(xn+1 −xn),
(77)
where gn and γn are chosen so that the descent condition (67) is satisﬁed. We have the following convergence result,
analogous to Theorem 6.
15

A PREPRINT - APRIL 2, 2021
Theorem 7. Let qn be a sequence of non-negative real number satisfying q0 = 0 and
(qn+1 + 1)2 ≤(qn + 2)2 + 1.
(78)
Then, if we set αn =
qn
2+qn+1 in (77), and choose γn and gn so that γn is decreasing and condition (67) is satisﬁed, we
have
f(xn) −f(x∗) ≤
2
γnqn(qn + 2)∥x0 −x∗∥2.
(79)
Note that Theorem 7 holds as long as we can ensure the decrease condition (67) and that γn is decreasing. In particular,
we do not necessarily need to know any smoothness parameters explicitly.
Proof. We largely follow the argument in [25] with minor modiﬁcations. Consider the Lyapunov function
Ln = γnqn(qn + 2)(f(xn) −f(x∗)) + 1
2∥2(yn −x∗) + qn(yn −xn)∥2.
(80)
We will show that Ln is decreasing, i.e. that Ln+1 ≤Ln.
This proves the theorem since γnqn(qn + 2)(f(xn) −f(x∗)) ≤Ln and L0 = 2∥y0 −x∗∥2 = 2∥x0 −x∗∥2.
We begin by breaking Ln into two pieces, namely
L1
n = γnqn(qn + 2)(f(xn) −f(x∗))
(81)
and
L2
n = 1
2∥2(yn −x∗) + qn(yn −xn)∥2.
(82)
Then we see that
L1
n+1 −L1
n = γnqn(qn + 2)(f(xn+1) −f(xn))
+ (γn+1qn+1(qn+1 + 2) −γnqn(qn + 2))(f(xn+1) −f(x∗)).
(83)
Since by assumption γn+1 ≤γn and qn+1(qn+1+2) = (qn+1+1)2−1 ≤(qn+2)2 we see that γn+1qn+1(qn+1+2) ≤
γn(qn + 2)2 and the bottom line in equation (83) is bounded by
(γn(qn + 2)2 −γnqn(qn + 2))(f(xn+1) −f(x∗)) = 2γn(qn + 2)(f(xn+1) −f(x∗)).
(84)
Thus we get the bound
J1
n+1 −J1
n ≤γn(qn + 2)[2(f(xn+1) −f(x∗)) + qn(f(xn+1) −f(xn))]
(85)
We now utilize the decrease condition (67) with z = x∗and with z = xn in equation (85) to get
J1
n+1 −J1
n ≤⟨γn(qn + 2)gn, 2(yn −x∗) + qn(yn −xn)⟩
−(γn(qn + 2))2
2
∥gn∥2
(86)
Next, we consider J2
n+1 −J2
n. Note that J2
n = (1/2)∥tn∥2 with
tn = 2(yn −x∗) + qn(yn −xn).
Thus
J2
n+1 −J2
n = ⟨tn+1 −tn, tn⟩+ 1
2∥(tn+1 −tn)∥2.
(87)
So we compute
tn+1 −tn = (2 + qn)(yn+1 −yn) −qn(xn+1 −xn) + (qn+1 −qn)(yn+1 −xn+1).
(88)
Using the iteration (77), we see that
yn+1 −xn+1 = αn(xn+1 −xn)
and
yn+1 −yn = −γngn + αn(xn+1 −xn).
This simpliﬁes to
tn+1 −tn = −(qn + 2)γngn + (2αn + qn+1αn −qn)(xn+1 −xn) = −(qn + 2)γngn,
(89)
16

A PREPRINT - APRIL 2, 2021
where the second equality is die to the choice αn =
qn
2+qn+1 . Using equation (87), we then get
J2
n+1 −J2
n = −⟨(qn + 2)γngn, 2(yn −x∗) + qn(yn −xn)⟩+ (γn(qn + 2))2
2
∥gn∥2.
(90)
Adding this to equation (86), we ﬁnally get
Jn+1 −Jn = J1
n+1 −J1
n + J2
n+1 −J2
n ≤0,
(91)
as desired.
Combining Theorem 7 and Lemma 2, we obtain the convergence rate of accelerated forward-backward gradient de-
scent:
x0 = y0, xn+1 = proxγn,h(yn −γn∇g(yn)), yn+1 = xn+1 + αn(xn+1 −xn),
(92)
in the convex case.
In particular, if we set γn = 1
L and qn = n, so that αn =
n
n+3 in (92), then we have
f(xn) −f(x∗) ≤
2L
n(n + 2)∥x0 −x∗∥2,
(93)
where f(x) = g(x) + h(x) with g convex and L-smooth.
Note that we do not need to know the smoothness parameter L to attain this convergence rate. In particular, by the
remark after Theorem 7, it sufﬁces to choose γn adaptively to satisfy (67) and such that γn ≤γn−1. This can typically
be done using a simple line search.
Further, if we wish, a slightly better convergence rate can be obtained by enforcing equality in (78), which results in a
slightly different choice of αn.
8
Conclusion
We found it remarkable that so many accelerated methods in the literature were discretizing the same underlying
differential equations. Furthermore, these differential equations have also been considered by the physics community
in the context of, for example, electronic structure calculations [26].
We hope that the ideas we have developed will help lead to the discovery of novel accelerated methods. In the future,
we hope to use our general framework to derive and numerically test specialized accelerated ﬁrst-order algorithms.
Of particular interest are accelerated methods on manifolds. We believe the differential equation approach will prove
important in understanding whether acceleration is possible in the presence of curvature.
Finally, we believe that our approach simpliﬁes and clariﬁes the connections between the vast number of accelerated
optimization methods in the literature and hope that it will help other researchers gain intuition about how they work
and how to derive new ones.
9
Acknowledgements
We would like to thank Professors Russel Caﬂisch, Stanley Osher, and Jinchao Xu for their helpful suggestions and
comments. This work was partially supported by AFOSR grant FA9550-15-1-0073.
References
[1] Polyak, B.T.: Some methods of speeding up the convergence of iteration methods. USSR Computational Math-
ematics and Mathematical Physics 4(5), 1–17 (1964)
[2] Nesterov, Y.: A method of solving a convex programming problem with convergence rate o (1/k2). In: Soviet
Mathematics Doklady, vol. 27, pp. 372–376 (1983)
[3] Beck, A., Teboulle, M.: A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM
journal on imaging sciences 2(1), 183–202 (2009)
17

A PREPRINT - APRIL 2, 2021
[4] Nesterov, Y., et al.: Gradient methods for minimizing composite objective function (2007)
[5] Nesterov, Y.: Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on
Optimization 22(2), 341–362 (2012)
[6] Bubeck, S., Lee, Y.T., Singh, M.: A geometric alternative to nesterov’s accelerated gradient descent. arXiv
preprint arXiv:1506.08187 (2015)
[7] Allen-Zhu, Z., Orecchia, L.: Linear coupling: An ultimate uniﬁcation of gradient and mirror descent. arXiv
preprint arXiv:1407.1537 (2014)
[8] Su, W., Boyd, S., Candes, E.: A differential equation for modeling nesterov’s accelerated gradient method:
Theory and insights. In: Advances in Neural Information Processing Systems, pp. 2510–2518 (2014)
[9] Wibisono, A., Wilson, A.C., Jordan, M.I.: A variational perspective on accelerated methods in optimization.
Proceedings of the National Academy of Sciences 113(47), E7351–E7358 (2016)
[10] Attouch, H., Cabot, A.: Asymptotic stabilization of inertial gradient dynamics with time-dependent viscosity.
Journal of Differential Equations 263(9), 5412–5458 (2017)
[11] Attouch, H., Chbani, Z., Fadili, J., Riahi, H.: First-order optimization algorithms via inertial systems with hessian
driven damping. Mathematical Programming pp. 1–43 (2020)
[12] Wilson, A.C., Recht, B., Jordan, M.I.: A lyapunov analysis of momentum methods in optimization. arXiv
preprint arXiv:1611.02635 (2016)
[13] Attouch, H., Chbani, Z., Peypouquet, J., Redont, P.: Fast convergence of inertial dynamics and algorithms with
asymptotic vanishing viscosity. Mathematical Programming 168(1-2), 123–175 (2018)
[14] Attouch, H., Cabot, A.: Convergence rates of inertial forward-backward algorithms. SIAM Journal on Optimiza-
tion 28(1), 849–874 (2018)
[15] Nesterov, Y., Stich, S.U.: Efﬁciency of the accelerated coordinate descent method on structured optimization
problems. SIAM Journal on Optimization 27(1), 110–123 (2017)
[16] Tu, S., Venkataraman, S., Wilson, A.C., Gittens, A., Jordan, M.I., Recht, B.: Breaking locality accelerates block
gauss-seidel. In: Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3482–
3491. JMLR. org (2017)
[17] O’donoghue, B., Candes, E.: Adaptive restart for accelerated gradient schemes. Foundations of computational
mathematics 15(3), 715–732 (2015)
[18] Nesterov, Y.: Gradient methods for minimizing composite functions. Mathematical Programming 140(1), 125–
161 (2013)
[19] Runge, C.: ¨Uber die numerische auﬂ¨osung von differentialgleichungen. Mathematische Annalen 46(2), 167–178
(1895)
[20] Wanner, G., Hairer, E.: Solving ordinary differential equations II. Springer Berlin Heidelberg (1996)
[21] Golub, G.H., Van Loan, C.F.: Matrix computations, vol. 3. JHU press (2013)
[22] Varga, R.S.: Iterative analysis. Springer (1962)
[23] Lu, H., Freund, R.M., Mirrokni, V.:
Accelerating greedy coordinate descent methods.
arXiv preprint
arXiv:1806.02476 (2018)
[24] Lee, Y.T., Sidford, A.: Efﬁcient accelerated coordinate descent methods and faster algorithms for solving linear
systems. arXiv preprint arXiv:1305.1922 (2013)
[25] Siegel, J.: Accelerated ﬁrst-order optimization with orthogonality constraints. Ph.D. thesis, UCLA (2018)
[26] Tassone, F., Mauri, F., Car, R.: Acceleration schemes for ab initio molecular-dynamics simulations and
electronic-structure calculations. Physical Review B 50(15), 10,561 (1994)
18

