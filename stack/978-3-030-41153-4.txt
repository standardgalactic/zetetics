Compact Textbooks in Mathematics
Simeon Ball
AÂ Course in 
Algebraic 
Error-Correcting 
Codes

Compact Textbooks in Mathematics

Compact Textbooks in Mathematics
This textbook series presents concise introductions to current topics in math-
ematics and mainly addresses advanced undergraduates and master students.
The concept is to offer small books covering subject matter equivalent to 2- or 3-
hour lectures or seminars which are also suitable for self-study. The books provide
students and teachers with new perspectives and novel approaches. They may
feature examples and exercises to illustrate key concepts and applications of the
theoretical contents. The series also includes textbooks specifically speaking to
the needs of students from other disciplines such as physics, computer science,
engineering, life sciences, finance.
â€¢
compact: small books presenting the relevant knowledge
â€¢
learning made easy: examples and exercises illustrate the application of the
contents
â€¢
useful for lecturers: each title can serve as basis and guideline for a semester
course/lecture/seminar of 2â€“3 hours per week.
More information about this series at http://www.springer.com/series/11225

Simeon Ball
A Course in Algebraic
Error-Correcting
Codes

Simeon Ball
Department of Mathematics
Polytechnic University of Catalonia
Barcelona, Spain
ISSN 2296-4568
ISSN 2296-455X
(electronic)
Compact Textbooks in Mathematics
ISBN 978-3-030-41152-7
ISBN 978-3-030-41153-4
(eBook)
https://doi.org/10.1007/978-3-030-41153-4
Mathematics Subject Classification (2010): 94BXX, 51EXX, 94AXX
Â© Springer Nature Switzerland AG 2020
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole
or part of the material is concerned, specifically the rights of translation, reprinting, reuse
of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical
way, and transmission or information storage and retrieval, electronic adaptation, computer
software, or by similar or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a specific statement, that such names are
exempt from the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information
in this book are believed to be true and accurate at the date of publication. Neither the
publisher nor the authors or the editors give a warranty, expressed or implied, with respect
to the material contained herein or for any errors or omissions that may have been made.
The publisher remains neutral with regard to jurisdictional claims in published maps and
institutional affiliations.
This book is published under the imprint BirkhÃ¤user, www.birkhauser-science.com by the
registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

v
Preface
This book is based on lecture notes for a course on coding theory given
as part of the Applied Mathematics and Mathematical Engineering masterâ€™s
degree at the Universitat PolitÃ©cnica de Catalunya. The aim of the course is
to give an up-to-date account of error-correcting codes from a mathematical
point of view, an analysis of the construction of these codes, and of the
various algorithms which are implemented to correct corrupted messages.
The lectures were prepared for an audience at masterâ€™s level, although a
large proportion of the book should be accessible to students at undergradu-
ate level and to engineering and physics students too. There is some formal
algebra that may not be familiar, mainly in the introduction of ï¬nite ï¬elds in
Chapter 2, but this is not essential to be able to follow the main part of the
content. It is enough to know how to perform ï¬nite ï¬eld arithmetic and how
to factorise polynomials over ï¬nite ï¬elds, all of which are explained in detail
in Chapter 2.
A large part of the material included in the text dates back to the latter
part of the last century. However, there have been recent advances in the
algebraic theory of error-correcting codes, many of which are included here.
There are still questions and problems which remain unresolved despite
considerable effort having been directed towards their resolution. Many of
these are highlighted in the text. The book takes a combinatorial, algebraic,
and geometric view of coding theory but combines this with a practical
consideration of how the codes constructed are implemented.
Shannonâ€™s theorem, the highlight of Chapter 1 and which dates back
to 1947, tells us that given a noisy, not totally unreliable, communication
channel, there are codes which provide a means of reliable communication
at a transmission rate arbitrarily close to the capacity. However, Shannonâ€™s
theorem only tells us that reliable communication is possible, it does not
provide us with a feasible way in which to encode or decode the transmission.
One of the aims of the book is to ï¬nd codes with a high transmission rate,
which allow fast encoding and decoding. On the way towards this objective,
we construct various types of codes and consider a number of decoding
algorithms.
Chapter 2 is a brief introduction to ï¬nite ï¬elds and the geometries
associated with these ï¬elds. An emphasis is given to the factorisation of
cyclotomic polynomials over ï¬nite ï¬elds, which is put to use in Chapter 5.
The concept of a minimum distance between any two codewords of a
block code is introduced in Chapter 3. The larger the minimum distance, the
more errors one can correct and, together with the length and size of the code,
are the fundamental parameters of a block code. There are various bounds
on these parameters proven in Chapter 3, both the Gilbertâ€“Varshamov
lower bound, given by the greedy algorithm, and upper bounds, a discrete

vi
Preface
sphere-packing bound and the better Plotkin and Elias-Bassalygo bounds.
The codes given by the Gilbertâ€“Varshamov bound are asymptotically good
codes in the sense that both the transmission rate, the proportion of the bits of
the message which contains information, and the relative minimum distance
are bounded away from zero as the length of the code increases. However,
in practice, as in the case of the randomly chosen code, there is no efï¬cient
way in which to encode or decode the message using these codes.
The advantage of linear codes, the focus of Chapter 4 and fundamental
to most of the rest of the book, is that they are efï¬cient to encode. One can
encode by simply multiplying a vector by a matrix. We consider a decoding
algorithm for linear codes based on syndromes. The question of existence
of a vector of small weight with a speciï¬ed syndrome is shown to be NP-
complete, which implies that the decoding algorithm is not feasible for long
codes, although it is used in practice for short length codes.
The classical 2-error correcting and 3-error correcting perfect codes
are constructed in Chapter 5, as well as the general class of BCH codes.
Although BCH codes exist for all lengths, it is known that there are no
sequences of BCH codes for which the transmission rate and the relative
minimum distance are both bounded away from zero.
Reedâ€“Solomon codes are codes whose codewords are the evaluation of
polynomials of low degree. In Chapter 6, we will exploit this algebraic
structure to explicitly develop a polynomial time decoding algorithm for
Reedâ€“Solomon codes which will correct any error bits, providing the number
of errors is less than half the minimum distance. We will also show that
there is a polynomial time list decoding algorithm which produces a short
list of possible candidates for the sent codeword when far more errors
occur. By employing two Reedâ€“Solomon codes, this allows one to decode
correctly well beyond the half the minimum distance bound with very high
probability. Another important application of MDS codes, and in particular
Reedâ€“Solomon codes, is the storage of data in distributed storage systems.
The fact that the codewords of an MDS code are uniquely determined by
relatively few bits means that data, stored across a number of different
servers, can be recovered from just a few.
In Chapter 7 we prove that there are subï¬eld subcodes of generalised
Reedâ€“Solomon codes meeting the Gilbertâ€“Varshamov bound. Since these
codes are linear, they are fast to encode and the fact that they have an
algebraic structure allows us to decode using the list decoding algorithm
from Chapter 6. We then go on to consider codes constructed from algebraic
curves. These algebraic-geometric codes include codes which surpass the
Gilbertâ€“Varshamov bound for codes over large alphabets.
There are linear codes constructed from low-density parity check matri-
ces for which both the transmission rate and the relative minimum distance
are bounded away from zero. We will prove in Chapter 8 that we can encode
and decode certain low-density parity check codes with polynomial time

Preface
vii
algorithms. These codes are widely implemented in wireless communication,
with a transmission rate close to the capacity set out by Shannonâ€™s theorem.
Although not asymptotically good, Reedâ€“Muller codes and their sub-
codes, the theme of Chapter 9, are widely implemented since there are
fast decoding algorithms for these codes, such as a majority logic decoding
algorithm which is detailed here. Kerdock codes are certain subcodes of the
second-order Reedâ€“Muller codes. They are of particular interest since there
are Kerdock codes which are non-linear codes with parameters for which it
is known that no linear code exists.
Bringing together p-adic numbers and cyclic codes in Chapter 10, we
construct non-linear codes which are linear over the ring of integers modulo
a prime power. Within this class of codes we again construct a non-linear
binary code with parameters for which it is known no binary linear code
exists. This suggests that, more generally, these codes could be a source of
codes which perform better than linear codes.
The three main conjectures concerning error-correcting codes are
included. The Information Theory and Applications Center of the University
of California San Diego offers prizes for the resolution of any of these
three conjectures. The three conjectures can be roughly stated as, there is no
inï¬nite sequence of binary codes better than the Gilbertâ€“Varshamov bound,
there are no non-trivial constant weight perfect codes, and there are no
linear MDS codes longer than the Reedâ€“Solomon codes, apart from some
three-dimensional even characteristic codes and their duals.
I would like to thank Tim Alderson, Anurag Bishnoi, Aart Blokhuis,
Massimo Giulietti, Victor Hernandez, Michel Lavrauw, Sonia Mansilla,
Valentina Pepe, and Oriol Serra for their comments and suggestions. I would
also like to thank Francesc Comellas for drawing some of the ï¬gures.
Barcelona, Spain
Simeon Ball
October 2019

viii
Preface
The dependencies between the chapters are as follows.

ix
Table of Parameters for Codes in the Text
The following table lists the parameters of the speciï¬c codes constructed in
the text. An [n, k, d]q code refers to a k-dimensional linear code over Fq
of length n and minimum distance d. A (n, K, d)r code refers to a code of
length n, size K, and minimum distance d over an alphabet of size r.
Parameters
Name
Example 3.4
(6, 8, 3)2
Example 3.6
(7, 8, 4)2
Example 3.11 (6, 4, 4)2
Exercise 3.2
(7, 16, 3)2
Exercise 3.4
(10, 6, 6)2
Exercise 3.5
(6, 4, 5)3
Example 4.2
[7, 4, 3]2
Binary Hamming
Example 4.3
[9, 3, 6]3
Example 4.5
[(qm âˆ’1)/(q âˆ’1), (qm âˆ’1)/(q âˆ’1) âˆ’m, 3]q Hamming
Example 4.8
[8, 4, 4]3
Example 4.16 [qâˆšq + 1, 3, qâˆšq âˆ’âˆšq]q
Hermitian curve
Example 4.19 [10, 6, 4]3
Example 4.23 [10, 4, 6]3
Exercise 4.7
[6, 3, 4]5
Exercise 4.8
[7, 3, 5]7
Exercise 4.9
[9, 3, 6]3
Exercise 4.14 [10, 6, 4]3
Example 5.5
[11, 6, 5]3
Ternary Golay
Example 5.5
[12, 6, 6]3
Extended ternary Golay
Example 5.9
[23, 12, 7]2
Binary Golay
Example 5.9
[24, 12, 8]2
Extended binary Golay
Example 5.11 [31, 16, 7]2
Example 5.12 [n, n âˆ’d + 1, d]q
Cyclic Reedâ€“Solomon
Exercise 5.6
[15, 7, 5]2
Exercise 5.6
[31, 11, 11]2
Exercise 5.6
[13, 4, 7]3
Exercise 5.7
[17, 9, 5]2
Zetterberg
Exercise 5.7
[18, 9, 6]2
Extended Zetterberg
Exercise 5.8
[11, 6, 5]4
Exercise 5.9
[17, 9, 7]4
(continued)

x
Table of Parameters for Codes in the Text
Example 6.4
[q + 1, q âˆ’d + 2, d]q
Reedâ€“Solomon
Exercise 6.9
[2h + 2, 3, 2h]2h
Translation hyperoval
Exercise 6.10 [10, 5, 6]9
Glynn
Exercise 6.11 [2h + 1, 4, 2h âˆ’2]2h
Segre
Example 7.2
[10, 7, 3]3
Example 7.14 [8, 3, 5]4
Example 7.14 [8, 5, 3]4
Exercise 7.10 [24, 4, 18]9
Example 8.4
[12, 3, 6]2
Afï¬ne plane of order 3
Theorem 9.3
[2m, 1 +
m
1

+ Â· Â· Â· +
m
r

, 2mâˆ’r]2 Reedâ€“Muller
Theorem 9.11 (2m, 22m, 2mâˆ’1 âˆ’2m/2âˆ’1)2
Kerdock
Example 9.12 (16, 256, 6)2
Nordstromâ€“Robinson
Exercise 10.6 (12, 8, 6)2
Exercise 10.7 (16, 256, 6)2
Nordstromâ€“Robinson

xi
Contents
Table of Parameters for Codes in the Text...........................
ix
1
Shannonâ€™s Theorem ..............................................
1
1.1
Entropy.............................................................
1
1.2
Information Channels ..............................................
4
1.3
System Entropies and Mutual Information..........................
5
1.4
Decoding and Transmission Rate ...................................
10
1.5
Shannonâ€™s Theorem ................................................
11
1.6
Comments..........................................................
14
1.7
Exercises ...........................................................
14
2
Finite Fields .......................................................
17
2.1
Definitions and Construction .......................................
17
2.2
Properties of Finite Fields...........................................
20
2.3
Factorisation of Cyclotomic Polynomials............................
21
2.4
Affine and Projective Spaces over Finite Fields......................
24
2.5
Comments..........................................................
26
2.6
Exercises ...........................................................
26
3
Block Codes .......................................................
29
3.1
Minimum Distance .................................................
29
3.2
Bounds on Block Codes.............................................
32
3.3
Asymptotically Good Codes ........................................
36
3.4
Comments..........................................................
43
3.5
Exercises ...........................................................
43
4
Linear Codes ......................................................
47
4.1
Preliminaries .......................................................
47
4.2
Syndrome Decoding................................................
51
4.3
Dual Code and the MacWilliams Identities ..........................
54
4.4
Linear Codes and Sets of Points in Projective Spaces ...............
58
4.5
Griesmer Bound ....................................................
59
4.6
Constructing Designs from Linear Codes ...........................
63
4.7
Comments..........................................................
66
4.8
Exercises ...........................................................
67

xii
Contents
5
Cyclic Codes .......................................................
71
5.1
Basic Properties.....................................................
71
5.2
Quadratic Residue Codes ...........................................
75
5.3
BCH Codes ..........................................................
78
5.4
Comments ..........................................................
80
5.5
Exercises ............................................................
81
6
Maximum Distance Separable Codes ...........................
83
6.1
Singleton Bound ....................................................
84
6.2
Reedâ€“Solomon Code................................................
84
6.3
Linear MDS Codes ..................................................
91
6.4
MDS Conjecture ....................................................
94
6.5
Comments ..........................................................
100
6.6
Exercises ............................................................
101
7
Alternant and Algebraic Geometric Codes .....................
105
7.1
Subfield Subcodes ..................................................
105
7.2
Generalised Reedâ€“Solomon Codes..................................
107
7.3
Alternant Codes Meeting the Gilbertâ€“Varshamov Bound ...........
109
7.4
Algebraic Geometric Codes .........................................
112
7.5
Algebraic Geometric Codes Surpassing the
Gilbertâ€“Varshamov Bound ..........................................
117
7.6
Comments ..........................................................
119
7.7
Exercises ............................................................
119
8
Low Density Parity Check Codes ................................
123
8.1
Bipartite Graphs with the Expander Property .......................
123
8.2
Low Density Parity Check Codes ....................................
126
8.3
Decoding LDPC Codes ..............................................
128
8.4
Comments ..........................................................
131
8.5
Exercises ............................................................
131
9
Reedâ€“Muller and Kerdock Codes................................
133
9.1
Binary Reedâ€“Muller Codes ..........................................
133
9.2
Decoding Reedâ€“Muller Codes.......................................
135
9.3
Kerdock Codes ......................................................
142
9.4
Non-binary Reedâ€“Muller Codes .....................................
145
9.5
Comments ..........................................................
148
9.6
Exercises ............................................................
149

Contents
xiii
10
p-Adic Codes .....................................................
151
10.1
p-Adic Numbers ....................................................
151
10.2
Polynomials over the p-Adic Numbers..............................
153
10.3
p-Adic Codes .......................................................
155
10.4
Codes over Z/phZ ..................................................
157
10.5
Codes over Z/4Z ...................................................
160
10.6
Comments..........................................................
162
10.7
Exercises ...........................................................
162
Hints and Answers to Selected Exercises ...............................
165
Bibliography ..............................................................
170
Index.......................................................................
175

1
1
Shannonâ€™s Theorem
The content of this chapter is rather different in nature to what appears in the rest
of the book, since Shannonâ€™s theorem is really a theorem from information theory
and not coding theory. However, we include it here because it tells us that reliable
communication can be achieved using a noisy channel and sets a limit for what is
feasible in terms of the proportion of data we can send whilst being almost sure to be able
to recover the original message from the distorted signal. Essentially, this chapter is a
very brief introduction to information theory, the mathematics this entails is probabilistic
in nature, whereas later it will be more algebraic and to some extent geometric. It is not
essential to the rest of the text and can be treated as optional.
1.1
Entropy
Let S = {s1, . . . , sm} be a ï¬nite set of values and suppose that we have a random
variable X which takes the value si âˆˆS with a probability pi. In other words, we
have a probability function, where the probability that X is si is
P(X = si) = pi.
Therefore, 0 â©½pi â©½1, and
m

i=1
pi = 1.
This random variable may be the result of some kind of experiment, where S is the
set of possible results, or a communication, where S is the set of possible symbols which
can be sent (or received). Usually the value of a random variable is a real number (which
allows us to calculate its expectation and other quantities dependent on the random
variable) but we will not be so strict here. We will not calculate these quantities and will
satisfy ourselves that a string of symbols is a legitimate value of the random variable.
Â© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_1

1
2
Chapter 1 â€¢ Shannonâ€™s Theorem
Example 1.1
Let S = {2, 3, . . . , 12} and let the probability that X = si be given by
si
2
3
4
5
6
7
8
9
10
11
12
pi
1
36
2
36
3
36
4
36
5
36
6
36
5
36
4
36
3
36
2
36
1
36
the probability that the sum of two dice throws is equal to si.
â– 
Let f be a function
f : (0, 1] â†’[0, âˆ),
with the property that
(I1)
f (x) is a continuous decreasing function and f (x) = 0 if x = 1,
(I2)
f (xy) = f (x) + f (y).
The function f can be interpreted as a measure of information we get if we consider
f applied to a probability p, where p is the probability that a certain value is selected by
a random variable X. The axiom (I1) indicates that more information is gained when a
lower the probability event occurs. If we imagine that we are repeating the experiment,
then (I2) is saying that the information we get from two elements is equal to the sum of
the information we get from each element.
Lemma 1.2 If f is a function satisfying (I1) and (I2), then
f (x) = âˆ’logr(x),
for some r > 1.
Proof
Deï¬ne g(x) = f (eâˆ’x).
Then (I2) implies
g(x + y) = f (eâˆ’(x+y)) = f (eâˆ’xeâˆ’y) = f (eâˆ’x) + f (eâˆ’y) = g(x) + g(y).
Therefore, g is a continuous additive function which implies that g(x) = cx for some c âˆˆR.
Putting y = eâˆ’x gives
f (y) = g(âˆ’ln y) = âˆ’c ln y.
Since ln y is an increasing function of y and, according to (I1) f is a decreasing function, we
have c > 0.
The lemma follows by letting c = (ln r)âˆ’1.
âŠ“âŠ”

1.1 Â· Entropy
3
1
To deï¬ne a measure of the information we get from a random variable X, we weigh
the sum of the information according to the probabilities.
The (r-ary) entropy Hr(X) of X is
Hr(X) =
m

i=1
pif (pi) = âˆ’
m

i=1
pi logr(pi).
We assume throughout that the function x log x is zero when evaluated at 0.
Example 1.3
Suppose that S = {0, 1} and that P (X = 0) = p and P(X = 1) = 1 âˆ’p. Then, the binary
entropy H2(X) of X is
h(p) = âˆ’p log2 p âˆ’(1 âˆ’p) log2(1 âˆ’p).
This function of p, deï¬ned on the real line interval [0, 1], is called the binary entropy
function . Its graph is drawn in âŠ¡Figure 1.1.
â– 
Observe that for the entropy to be zero every term in the sum must be zero, since
every term in the sum is non-negative. But pi logr(pi) = 0 implies pi is either 0 or 1.
Therefore, P(X = si) = 1, for some i and P(X = sj) = 0 for all j Ì¸= i. In this case
X conveys no information, since there is no uncertainty. At the other end of the scale,
intuitively, the most uncertainty, and so the most information, is conveyed when X is
equally likely to be any of the elements of S. This is proven in the following theorem.
âŠ¡Fig. 1.1 The binary entropy
function on the interval [0, 1].
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0

1
4
Chapter 1 â€¢ Shannonâ€™s Theorem
Theorem 1.4
Let X be a random variable taking values from a ï¬nite set S. Then
Hr(X) â©½logr |S|
with equality if and only if P (X = s) = 1/|S|, for all s âˆˆS.
Proof
Observe that ln x â©½x âˆ’1 with equality if and only if x = 1.
By deï¬nition,
Hr(X) âˆ’logr |S| = âˆ’

i
pi logr(pi) âˆ’

i
pi logr |S|
=

i
pi logr(
1
pi|S|) =
1
ln r

i
pi ln(
1
pi|S|) â©½
1
ln r

i
pi(
1
pi|S| âˆ’1) = 0.
Equality occurs if and only if pi|S| = 1 for all i = 1, . . . , |S|.
âŠ“âŠ”
1.2
Information Channels
Let X and Y be random variables which take values from ï¬nite sets S and T ,
respectively, according to the probability distributions P(X = si) = pi and P(Y =
tj) = qj.
We consider the elements of S as the symbols which are sent and the elements of T
as the symbols which are received. The channel through which the symbols are sent is
denoted by , which is deï¬ned by the matrix (pij), where
pij = P(Y = tj|X = si)
is the probability that tj is received given that si was sent.
We deï¬ne
qij = P(X = si|Y = tj)
and
rij = P(X = si, Y = tj).
The probabilities pij, qij and rij are called the forwards probabilities, the
backwards probabilities and the joint probabilities, respectively.

1.3 Â· System Entropies and Mutual Information
5
1
Example 1.5
In a binary symmetric channel both S = {0, 1} and T = {0, 1}. The channel is deï¬ned by
the matrix
(pij) =

Ï†
1 âˆ’Ï†
1 âˆ’Ï†
Ï†

for some Ï† âˆˆ[0, 1]. The rows and columns of the matrix are indexed by 0 and 1 in that order.
Suppose that X is the random variable deï¬ned by the probability
P(X = 0) = p.
From the channel matrix we can calculate the probabilities for Y from
P(Y = b) =

aâˆˆ{0,1}
P (Y = b|X = a)P (X = a).
For example,
P(Y = 0) = Ï†p + (1 âˆ’p)(1 âˆ’Ï†).
We can calculate the joint probabilities from
P(X = a, Y = b) = P (Y = b | X = a)P (X = a).
For example,
P(X = 1, Y = 0) = P (Y = 0 | X = 1)P (X = 1) = (1 âˆ’Ï†)(1 âˆ’p).
We can calculate the backwards probabilities using
P(X = a | Y = b)P(Y = b) = P (X = a, Y = b).
For example,
P(X = 1 | Y = 0) = P (X = 1, Y = 0)
P (Y = 0)
=
(1 âˆ’Ï†)(1 âˆ’p)
Ï†p + (1 âˆ’p)(1 âˆ’Ï†).
â– 
1.3
System Entropies and Mutual Information
We deï¬ne the input entropy and the output entropy as
H(X) = âˆ’

i
pi log pi

1
6
Chapter 1 â€¢ Shannonâ€™s Theorem
and
H(Y) = âˆ’

j
qj log qj,
respectively.
We suppress the r in the logarithm, but assume that it is the same for both deï¬nitions.
Given that we have received tj âˆˆT , we can calculate the entropy of X, conditional
on the fact that we know Y = tj, by using the backwards probabilities. This gives
H(X|Y = tj) = âˆ’

i
qij log qij.
This tells us the average information of X, knowing that Y = tj.
If this is zero, then it would say that we know everything about X. This would mean
that the backwards probabilities qij would be 1 for some i and zero for the others. In
other words, that if we receive tj, then we know which symbol was sent.
If this is H(X), then this says that we learn nothing about X when we receive tj.
This would happen if
qij = P(X = si | Y = tj) = P(X = si) = pi.
Averaging over tj âˆˆY, we obtain the conditional entropy, the average information
of X knowing Y,
H(X|Y) = âˆ’

i,j
qjqij log qij.
Similarly, the average information of Y knowing X is
H(Y|X) = âˆ’

i,j
pjpij log pij.
The joint entropy H(X, Y) is given by the joint probabilities and is the average
information gained from both the input and output,
H(X, Y) = âˆ’

i,j
rij log rij.
Example 1.6
Suppose  is the channel with input random variable X, deï¬ned by
P(X = 0) = P (X = 1) = 1
2,

1.3 Â· System Entropies and Mutual Information
7
1
and output random variable Y taking a value from the set {0, 1, âˆ—} and that the channel matrix
(pij) =

3
4 0 1
4
0 1
2
1
2

has rows indexed by 0 and 1 in that order and columns indexed by 0, 1, âˆ—in that order.
We can calculate directly
H(Y|X) = âˆ’1
2( 3
4 log 3
4 + 2 1
2 log 1
2 + 1
4 log 1
4) = âˆ’3
8 log 3 + 3
2 log 2
and
H(X) = âˆ’1
2(log 1
2 + log 1
2) = log 2.
We can calculate the output probabilities using qj = 
i pipij. This gives
(q0, q1, qâˆ—) = ( 3
8, 1
4, 3
8).
We can calculate the backwards probabilities qij using
qij = pijpi
qj
,
and obtain
(qij) =

1 0 1
3
0 1 2
3

.
Therefore,
H(X|Y) = âˆ’3
8( 1
3 log 1
3 + 2
3 log 2
3) = 3
8 log 3 âˆ’1
4 log 2
and
H(Y) = âˆ’2 3
8 log 3
8 âˆ’1
4 log 1
4 = 11
4 log 2 âˆ’3
4 log 3.
Observe that
H(Y) âˆ’H(Y|X) = âˆ’3
8 log 3 + 5
4 log 2 = H(X) âˆ’H(X|Y).
Finally, we can calculate the joint probabilities from rij = pipij and get
(rij) =

3
8 0 1
8
0 1
4
1
4

.

1
8
Chapter 1 â€¢ Shannonâ€™s Theorem
The joint entropy is
H(X, Y) = âˆ’2 1
4 log 1
4 âˆ’3
8 log 3
8 âˆ’1
8 log 1
8 = 5
2 log 2 âˆ’3
8 log 3.
â– 
Observe that in Example 1.6
H(X, Y) = H(X|Y) + H(Y) = H(Y|X) + H(X).
This is no coincidence and holds in general.
Lemma 1.7 For random variables X and Y deï¬ned on ï¬nite sets,
H(X, Y) = H(X|Y) + H(Y) = H(Y|X) + H(X).
Proof
Since
P(X = si, Y = tj) = P (X = si | Y = tj)P (Y = tj),
we have that
rij = qjqij.
By direct calculation,
H(X, Y) = âˆ’

i,j
rij log rij = âˆ’

i,j
qjqij log qjqij
= âˆ’

i,j
qjqij log qj âˆ’

i,j
qjqij log qij.
Since 
i qij = 1,
H(X, Y) = âˆ’

j
qj log qj + H(X|Y) = H(Y) + H(X|Y).
Reversing the roles of X and Y we obtain the second equality.
âŠ“âŠ”
The mutual information of X and Y is
I(X, Y) = H(X) âˆ’H(X|Y) = H(Y) âˆ’H(Y|X),
the amount of information about X conveyed by Y and vice versa.

1.3 Â· System Entropies and Mutual Information
9
1
If H(X) = H(X|Y), then Y tells us nothing about X, so the mutual information is
zero. This is an unreliable channel and useless as a means of communication.
If H(X|Y) = 0, then knowing Y we know everything about X, so I(X, Y) = H(X).
This is the ideal situation, where when we receive something we know exactly what was
sent.
Example 1.8
Suppose  is the channel with input random variable X deï¬ned on {0, 1, 2} in which the
probability P(X = x) = 1
3, for all x âˆˆ{0, 1, 2}. Suppose Y is the output random variable
deï¬ned on {0, 1} and that the channel matrix is
(pij) =
â›
âœâ
1 0
0 1
1 0
â
âŸâ ,
where the rows are indexed by 0, 1, 2 in that order and the columns are indexed by 0, 1 in
that order.
The conditional entropy
H(Y|X) = 0.
This indicates that knowing X we know Y. Explicitly, we know that if 0 or 2 is sent, then 0
will be received, and if 1 is sent, then 1 will be received.
The entropy of X is
H(X) = âˆ’3 1
3 log 1
3 = log 3.
The output probabilities, applying
qj =

i
pipij,
are
(q0, q1) = ( 2
3, 1
3).
The backwards probabilities qij, applying
qij = pijpi
qj
,
are
(qij) =
â›
âœâ
1
2 0
0 1
1
2 0
â
âŸâ .

1
10
Chapter 1 â€¢ Shannonâ€™s Theorem
Therefore,
H(X|Y) = âˆ’2
32( 1
2 log 1
2) = 2
3 log 2
and the entropy of Y is
H(Y) = âˆ’2
3 log 2
3 âˆ’1
3 log 1
3 = log 3 âˆ’2
3 log 2.
The mutual information is
I(X, Y) = H(X) âˆ’H(X|Y) = log 3 âˆ’2
3 log 2 = H(Y).
Observe that knowing Y, we do not know X, even though the reverse is true.
â– 
1.4
Decoding and Transmission Rate
Let  be a channel with input random variable X and output random variable Y deï¬ned
on ï¬nite sets S = {s1, . . . , sm} and T = {t1, . . . , tn}, respectively.
A decoding  is a map from {1, . . . , n} to {1, . . . , m}, which induces a map from T
to S by mapping tj to s(j). This map we interpret as the receiver decoding tj as s(j).
For convenience sake, we will sometimes write (v) = u, where v âˆˆT and u âˆˆS. The
probability that a decoding is correct is
q(j)j = P(X = s(j)|Y = tj),
the probability that s(j) was sent given that tj was received.
The average probability PCOR of a correct decoding is
PCOR =

j
qjq(j)j.
In most applications we know (pij), how the channel behaves, but not the probability
distributions which deï¬ne the random variables X and Y. We choose  to be the
decoding map where (j) is chosen so that p(j)j â©¾pij for all i. This decoding map
is called maximum likelihood decoding.
Suppose that X is deï¬ned on the elements of a set A with r symbols.
A block code C is a subset of An. We will often refer to a block code as simply a
code.
The (transmission) rate of C is deï¬ned as
R = logr |C|
n
.
Thus, R = 1 if and only if |C| = rn if and only if C = An.

1.5 Â· Shannonâ€™s Theorem
11
1
The capacity of a channel  is
 = max I(X, Y),
where we maximise over all input random variables X and output random variables Y.
In other words, maximising over all probability distributions pi and qj.
Shannonâ€™s theorem tells us that given the channel, for sufï¬ciently large n, there
are block codes of An whose rate R is arbitrarily close to  for which, when we use
maximum likelihood decoding, the probability PCOR is arbitrarily close to 1. To be able
to prove Shannonâ€™s theorem we will require a few lemmas.
For any u, v âˆˆAn, the Hamming distance d(u, v) between u and v is the number
of coordinates in which they differ.
Lemma 1.9 For the binary symmetric channel deï¬ned as in Example 1.5 and block code
C âŠ†{0, 1}n, maximum likelihood decoding is (v) = u, where u is the closest element of C
to v with respect to the Hamming distance.
Proof
Let Ï† denote the probability that a symbol does not change when sent through the binary
symmetric channel.
Suppose that d(u, v) = i. Then
P(u was sent | v was received) = Ï†nâˆ’i(1 âˆ’Ï†)i = Ï†n(1 âˆ’Ï†
Ï†
)i,
which is a decreasing function of i (assuming Ï† > 1 âˆ’Ï†).
Then maximum likelihood decoding will give (v) = u, where u is the closest (with
respect to the Hamming distance) n-tuple to v.
âŠ“âŠ”
In general, the decoding map deï¬ned by (v) = u, where u is the closest (with
respect to the Hamming distance) n-tuple to v is called nearest neighbour decoding.
1.5
Shannonâ€™s Theorem
We are almost in a position to prove Shannonâ€™s theorem for the binary symmetric
channel. To be able to do so, we ï¬rst calculate the channel capacity.
Lemma 1.10 For the binary symmetric channel the capacity is
 = 1 + Ï† log2 Ï† + (1 âˆ’Ï†) log2(1 âˆ’Ï†),
where Ï† denotes the probability that a symbol does not change.

1
12
Chapter 1 â€¢ Shannonâ€™s Theorem
Proof
Let p denote the probability that the input random variable X is 0. Let q denote the
probability that the output random variable Y is 0. Then
H(Y) = âˆ’q log2 q âˆ’(1 âˆ’q) log2(1 âˆ’q).
The conditional entropy is
H(Y|X) = âˆ’

i,j
pipij log2 pij
= âˆ’p(Ï† log2 Ï† + (1 âˆ’Ï†) log2(1 âˆ’Ï†)) âˆ’(1 âˆ’p)(Ï† log2 Ï† + (1 âˆ’Ï†) log2(1 âˆ’Ï†)),
and the mutual information is
I(X, Y) = âˆ’q log2 q âˆ’(1 âˆ’q) log2(1 âˆ’q) + Ï† log2 Ï† + (1 âˆ’Ï†) log2(1 âˆ’Ï†),
since
I(X, Y) = H(Y) âˆ’H(Y|X).
To obtain the channel capacity we maximise over all random variables X and Y, which in
this case involves maximising over q. The function
h(q) = âˆ’q log2 q âˆ’(1 âˆ’q) log2(1 âˆ’q)
is maximised when q = 1
2 (see âŠ¡Figure 1.1), where it has the value 1.
âŠ“âŠ”
Lemma 1.11 For 0 < Î» â©½1
2,
âŒŠÎ»nâŒ‹

i=0
n
i

â©½2nh(Î»).
Proof
Observe that Î»/(1 âˆ’Î») â©½1 implies that, for i = 1, . . . , âŒŠÎ»nâŒ‹,

Î»
1 âˆ’Î»
i
â©¾

Î»
1 âˆ’Î»
Î»n
.
By the binomial theorem and the above inequality,
1 = (Î» + 1 âˆ’Î»)n =
n

i=0
n
i

Î»i(1 âˆ’Î»)nâˆ’i =
n

i=0
n
i

Î»
1 âˆ’Î»
i
(1 âˆ’Î»)n
â©¾
âŒŠÎ»nâŒ‹

i=0
n
i

Î»
1 âˆ’Î»
i
(1 âˆ’Î»)n â©¾
âŒŠÎ»nâŒ‹

i=0
n
i

Î»
1 âˆ’Î»
Î»n
(1 âˆ’Î»)n.

1.5 Â· Shannonâ€™s Theorem
13
1
Hence,
âŒŠÎ»nâŒ‹

i=0
n
i

â©½Î»âˆ’Î»n(1 âˆ’Î»)âˆ’(1âˆ’Î»)n.
Since Î»âˆ’Î»n = 2âˆ’nÎ» log2 Î», the lemma follows.
âŠ“âŠ”
The following theorem, Theorem 1.12, is Shannonâ€™s noisy channel coding theorem.
Theorem 1.12
Let Î´ be an arbitrarily small positive real number and let R be a positive real number
such that R < . For all sufï¬ciently large n, there is a code of length n and rate R,
such that when we use maximum likelihood decoding the probability PCOR of a correct
decoding is larger than 1 âˆ’Î´.
Proof (for the binary symmetric channel)
Choose C to be a random subset of âŒŠ2nRâŒ‹vectors of {0, 1}n.
Let u âˆˆC and consider the transmission of u through the binary symmetric channel.
On average, the number of coordinates which will change in the transmission of the n bits
of u is n(1 âˆ’Ï†). As n gets large, the law of large numbers tells us that the probability that
the number of symbols that change in the transmission varies from the average by a ï¬xed
constant tends to zero, so we can assume that the number of symbols which will change in
the transmission of the n bits of u is n(1 âˆ’Ï†).
Suppose that we receive the n-tuple v. By Lemma 1.9, we decode v to the n-tuple in C
that is nearest to v with respect to the Hamming distance.
The probability that we mistakenly decode v to a different element of C, i.e., that there
are other n-tuples of C with Hamming distance at most n(1 âˆ’Ï†) to v, is at most

wâˆˆC\{u}
P(d(w, v) â©½n(1 âˆ’Ï†)).
Counting the number of n-tuples at Hamming distance at most (1 âˆ’Ï†)n to v, and observing
that there are 2n n-tuples in total,
P(d(w, v) â©½n(1 âˆ’Ï†)) < 1
2n
âŒŠ(1âˆ’Ï†)nâŒ‹

j=0
n
j

.
Since there are fewer than 2nR n-tuples in C \ {u} and these were chosen randomly,
1 âˆ’PCOR < 2nR 1
2n
âŒŠ(1âˆ’Ï†)nâŒ‹

j=0
n
i

.

1
14
Chapter 1 â€¢ Shannonâ€™s Theorem
Lemma 1.11 implies that
1 âˆ’PCOR < 2n(Râˆ’(1+Ï† log2 Ï†+(1âˆ’Ï†) log2(1âˆ’Ï†))).
By Lemma 1.10, the capacity of the binary symmetric channel is
 = 1 + Ï† log2 Ï† + (1 âˆ’Ï†) log2(1 âˆ’Ï†),
so
1 âˆ’PCOR < 2n(Râˆ’).
Since R < , for n sufï¬ciently large,
2n(Râˆ’) < Î´,
which proves the theorem.
âŠ“âŠ”
We have established that, given a channel with non-zero capacity, there are codes
which allow us to communicate using the channel and decode with a probability of a
correct decoding being close to 1. Our aim will be to ï¬nd such codes which can be
encoded and decoded in an efï¬cient manner.
1.6
Comments
Although â–·Chapter 1 is primarily about Shannonâ€™s theorem [65], it is also a very brief
introduction to information theory. For a more complete introduction, see the excellent
book by Jones and Jones [40] or the classical introduction by Ash [3]. Jones and Jones
[40] also contains an introduction to coding theory, albeit at a lower level to the treatment
here. The idea to measure information dates back to Hartleyâ€™s 1928 paper [35], although
Shannonâ€™s work from the 1940s is widely acknowledged as the beginning of information
theory. In 1961, Fano [23] proved that the capacity of a channel bounds the rate at
which reliable transmissions can be achieved. For a proof of this, under some additional
hypothesis, see Jones and Jones [40].
1.7
Exercises
1.1 Calculate the r-ary entropy Hr(X) in Example 1.1 and verify that Hr(X) â©½logr 11, as
claimed by Theorem 1.4.
1.2 Consider the random variable X deï¬ned on the elements of a set with n symbols where
the probability that X is the i-th symbol is Î³/2i. Calculate the entropy H2(X) and verify that
H2(X) â†’2 as n â†’âˆ. Compare this with the bound H2(X) â©½log2 n given by Theorem 1.4.

1.7 Â· Exercises
15
1
1.3 Let X be the random variable deï¬ned on a set S with three symbols with probabilities
1
2, 1
4 and 1
4. Let Xn be the random variable deï¬ned on a set Sn in which the probability that
Xn = (s1, . . . , sn) is
n

i=1
pi,
where P(X = si) = pi.
i.
Calculate, for how many symbols, Xn has probability 2jâˆ’2n, where j = 0, . . . , n.
ii.
Calculate H2(X) and H2(Xn) and verify that H2(Xn) = nH2(X).
1.4 Calculate the mutual information for the binary erasure channel deï¬ned by the channel
matrix
(pij) =

Ï† 0 1 âˆ’Ï†
0 Ï† 1 âˆ’Ï†

,
in terms of p and Ï†, where p is the probability that the input random variable is one of the
symbols.
1.5 Let X and Y be random variables deï¬ned on the elements of a set S of size r, where
P(X = s) = 1/r, for all s âˆˆS, and deï¬ne a channel by the matrix (pij), where
pii = Ï†, and pij = 1 âˆ’Ï†
r âˆ’1 , i Ì¸= j.
Calculate the entropy Hr(Y) and the mutual information I(X, Y).
1.6 Calculate PCOR for the binary symmetric channel, in which a bit changes with
probability Ï†, using maximum likelihood decoding.
1.7 Prove that for the repetition code C
= {00 . . . 0, 11 . . . 1} âŠ‚{0, 1}n, applying
nearest neighbour decoding whilst transmitting through a reliable binary symmetric channel,
PCOR â†’1 and R â†’0 as n â†’âˆ.
1.8 Let  be the channel with input random variable X taking values from {0, 1}, output
random variable Y taking values from {0, 1, âˆ—} and channel matrix
(pij) =

1 0 0
0 3
4
1
4

.
Let p = P(X = 0).
i.
Calculate H(Y|X), H(Y) and H(X) in terms of p.
ii.
Calculate the capacity of the channel .
iii.
Give an interpretation of the fact that H(X) equals the mutual information I(X, Y).

1
16
Chapter 1 â€¢ Shannonâ€™s Theorem
1.9 Let  be the channel with channel matrix
(pij) =

3
4
1
8
1
8
0 1
2
1
2

,
where the input random variable X takes values from {0, 1} and output random variable Y
takes values from {0, 1, âˆ—}.
Let p = P(X = 0).
i.
Calculate the entropy H(Y) as a function of p.
ii.
Prove that the mutual information is
I(X, Y) = âˆ’3
4p log p âˆ’1
4(4 âˆ’3p) log(4 âˆ’3p) + (2 âˆ’2p) log 2.
iii.
Calculate the channel capacity of .

17
2
Finite Fields
This chapter is a brief introduction to ï¬nite ï¬elds. The most important facts that will
be established are that ï¬nite ï¬elds necessarily contain ph elements, for some prime
number p and positive integer h, and that the ï¬eld with ph elements is unique, up to
isomorphism. We will study how to factorise cyclotomic polynomials over ï¬nite ï¬elds,
which is used in â–·Chapter 5 to construct cyclic codes. The projective and afï¬ne space
over a ï¬nite ï¬eld are also introduced and will appear later in various chapters.
2.1
Definitions and Construction
A commutative ring R is a set with two binary operations, addition and multiplication,
such that it is a commutative group with respect to addition with identity element 0,
and multiplication is commutative (ab = ba), associative ((ab)c = a(bc)), distributive
(a(b + c) = ab + ac) and has an identity element 1.
A ï¬eld is a commutative ring in which every non-zero element has a multiplicative
inverse. In other words, for all a Ì¸= 0, there is a b such that ab = 1. In particular, this
implies that if ab = 0, then either a = 0 or b = 0.
Example 2.1
The rational numbers Q, the real numbers R and the complex numbers C are all ï¬elds.
â– 
In the above example, the sum 1 + Â· Â· Â· + 1 is never zero.
Let F be a ï¬eld with multiplicative identity 1. Suppose there is a n for which
summing n ones gives zero and let n be minimal with this property. If p is a divisor
of n, then
1 + Â· Â· Â· + 1



n
= (1 + Â· Â· Â· + 1)



p
(1 + Â· Â· Â· + 1)



n/p
,
which contradicts the minimality of n, since the left-hand is zero implies one of the terms
in the product on the right-hand side is zero. It follows that n is a prime p. The number
Â© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_2

2
18
Chapter 2 â€¢ Finite Fields
p is called the characteristic of the ï¬eld. If no such n exists, then the characteristic is
zero.
Throughout the remainder of this chapter p will be a prime number.
Example 2.2
The ring Z/pZ, the integers modulo p, is a ï¬eld of characteristic p and is denoted Fp.
â– 
The ring Z/nZ is not a ï¬eld when n is not a prime, since it has non-zero elements
which have no multiplicative inverse.
In the following theorem, (f ) denotes the set of elements of the ring of polynomials
Fp[X] which are multiples of the polynomial f . The elements of the quotient ring
Fp[X]/(f ) are cosets of the form g + (f ), where addition is deï¬ned as
g + (f ) + h + (f ) = g + h + (f )
and multiplication is deï¬ned as
(g + (f ))(h + (f )) = gh + (f ).
One can think of the quotient ring as the polynomials modulo f .
Theorem 2.3
If f is an irreducible polynomial in the ring Fp[X], then Fp[X]/(f ) is a ï¬eld of
characteristic p.
Proof
We have to show that g + (f ) has a multiplicative inverse for all g âˆˆFp[X], such that
g + (f ) Ì¸= 0 + (f ).
Let
B = {gh + (f ) | h + (f ) âˆˆFp[X]/(f )}.
If
gh1 + (f ) = gh2 + (f )
then
g(h1 âˆ’h2) + (f ) = 0 + (f ).
Since f is irreducible and g is not a multiple of f , this implies that h1 âˆ’h2 is a multiple of
f , which implies
h1 + (f ) = h2 + (f ).

2.1 Â· Definitions and Construction
19
2
Therefore, if
h1 + (f ) Ì¸= h2 + (f )
then
gh1 + (f ) Ì¸= gh2 + (f )
and so B contains as many elements as the ï¬nite set Fp[X]/(f ).
In particular, there is an element h + (f ) for which
(g + (f ))(h + (f )) = 1 + (f ),
so g + (f ) has a multiplicative inverse.
âŠ“âŠ”
If f is an irreducible polynomial of degree h, then Fp[X]/(f ) is a ï¬eld with ph
elements.
For example, Table 2.1 is the addition and multiplication table of
F2[X]/(X2 + X + 1),
a ï¬nite ï¬eld with four elements and Table 2.2 is the multiplication table of
F3[X]/(X2 + 1),
a ï¬nite ï¬eld with nine elements.
Table 2.1 The addition and multiplication table for the field F2[X]/(X2 + X + 1).
+
0
1
X
1+X
0
0
1
X
1+X
1
1
0
1+X
X
X
X
1+X
0
1
1+X
1+X
X
1
0
.
0
1
X
1+X
0
0
0
0
0
1
0
1
X
1+X
X
0
X
1+X
1
1+X
0
1+X
1
X
Table 2.2 The multiplication table for the field F3[X]/(X2 + 1).
.
0
1
2
X
1+X
2+X
2X
1+2X
2+2X
0
0
0
0
0
0
0
0
0
0
1
0
1
2
X
1+X
2+X
2X
1+2X
2+2X
2
0
2
1
2X
2+2X
1+2X
X
2+X
1+X
X
0
X
2X
2
2+X
2+2X
1
1+X
1+2X
1+X
0
1+X
2+2X
2+X
2X
1
1+2X
2
X
2+X
0
2+X
1+2X
2+2X
1
X
1+X
2X
2
2X
0
2X
X
1
1+2X
1+X
2
2+2X
2+X
1+2X
0
1+2X
2+X
1+X
2
2X
2+2X
X
1
2+2X
0
2+2X
1+X
1+2X
X
2
2+X
1
2X

2
20
Chapter 2 â€¢ Finite Fields
2.2
Properties of Finite Fields
Throughout the remainder of this chapter q will be a power of the prime number p.
Theorem 2.4
For all x in a ï¬nite ï¬eld F with q elements xq = x.
Proof
Suppose x Ì¸= 0. Let A = {ax | a âˆˆF}. Since a1x Ì¸= a2x, for a1 Ì¸= a2, the set A consists of
all the elements of F. If we multiply together the non-zero elements of A, then we obtain the
product of all the non-zero elements of F,

aâˆˆF\{0}
ax =

aâˆˆF\{0}
a,
which implies xqâˆ’1 = 1.
âŠ“âŠ”
Let Fq denote the splitting ï¬eld of the polynomial Xq âˆ’X over the ï¬eld Fp, that is
the smallest ï¬eld extension of Fp in which Xq âˆ’X factorises into linear factors.
Theorem 2.5
A ï¬nite ï¬eld with q elements is isomorphic to Fq.
Proof
By Exercise 2.1, q = ph for some prime p. The theorem follows from the uniqueness of
splitting ï¬elds and Theorem 2.4.
âŠ“âŠ”
In practice, when we work over a ï¬eld with q elements, we ï¬x an irreducible
polynomial f of degree h and compute in the ring Fp[X]/(f ) which, by Theorem 2.3,
is a ï¬eld with ph elements.
Lemma 2.6 The map Ïƒ which maps x â†’xp is an automorphism of Fq.

2.3 Â· Factorisation of Cyclotomic Polynomials
21
2
Proof
The characteristic of Fq is p, so
(x + y)p =
p

j=0
p
j

xjypâˆ’j = xp + yp,
for all x, y âˆˆFq.
Note that the binomial coefï¬cient is an element of the ï¬eld Fp and since
p
j

is divisible
by p for j = 1, . . . , p âˆ’1, it is zero.
Clearly (xy)p = xpyp, so Ïƒ preserves the additive and multiplicative structure of the
ï¬eld.
âŠ“âŠ”
The automorphism Ïƒ in Lemma 2.6 is called the Frobenius automorphism.
Observe that the group of automorphisms generated by Ïƒ is a cyclic group of order
h, where q = ph.
2.3
Factorisation of Cyclotomic Polynomials
In this section we will see how cyclotomic polynomials factorise over ï¬nite ï¬elds. These
factorisations will be used principally in â–·Chapter 5 and implicitly in â–·Chapter 10.
As in the previous section q = ph, for some prime p.
By Lemma 2.4, the polynomial Xqâˆ’1 âˆ’1 factorises into distinct linear factors in
Fq[X].
Lemma 2.7 The polynomial Xqâˆ’1 âˆ’1 factorises in Fp[X] into distinct irreducible factors
whose degrees are divisors of h.
Proof
Let Ïµ âˆˆFq and let Fp(Ïµ) denote the smallest extension ï¬eld of Fp containing Ïµ.
Since Ïµ âˆˆFq, Fp(Ïµ) is a subï¬eld of Fq. This subï¬eld is generated as a vector space
over Fp by 1, Ïµ, . . . , Ïµrâˆ’1, where r is the dimension of this vector space. Hence, there are
a0, . . . , ar âˆˆFp, such that
a0 + a1Ïµ + Â· Â· Â· + arÏµr = 0.
This implies Ïµ is a zero of the polynomial
a0 + a1X + Â· Â· Â· + arXr.
For the elements in Fp(Ïµ), Lemma 2.4 implies xpr = x. Since xph = x, this implies Xpr âˆ’X
divides Xph âˆ’X which, by Exercise 2.3, implies r divides h.
âŠ“âŠ”
Observe that in the following example X8 âˆ’X = X8 + X, since p = 2.

2
22
Chapter 2 â€¢ Finite Fields
Example 2.8
The polynomial X8 + X factorises in F2[X] as
(X3 + X + 1)(X3 + X2 + 1)(X + 1)X.
Suppose that e is a root of X3 + X + 1. Then e3 = e + 1, e4 = e2 + e, e5 = e2 + e + 1,
e6 = e2 + 1 and e7 = 1. Therefore, every non-zero element of F2(e) âˆ¼= F8 is a power of e. â– 
An element e with the property that every non-zero element x âˆˆFq can be written
as x = ei, for some i, is called primitive.
Example 2.9
The polynomial X9 âˆ’X factorises in F3[X] as
(X2 + X âˆ’1)(X2 âˆ’X âˆ’1)(X2 + 1)(X âˆ’1)(X + 1)X.
â– 
Observe that the elements of F9 which are roots of X2 + 1 are not primitive.
Example 2.10
The polynomial Xqâˆ’1 âˆ’1 factorises as
(X(qâˆ’1)/2 âˆ’1)(X(qâˆ’1)/2 + 1)
when q is odd. The roots of the ï¬rst polynomial are the non-zero squares in Fq and the roots
of the second factor are the non-squares in Fq. Note that the squares are never primitive
elements. If âˆ’1 is a square, then âˆ’1 = a2, for some a âˆˆFq, which implies (âˆ’1)(qâˆ’1)/2 =
aqâˆ’1 = 1 and so q â‰¡1 modulo 4.
â– 
Lemma 2.11 The polynomial
(X âˆ’Î±1) Â· Â· Â· (X âˆ’Î±m) âˆˆFq[X]
if and only if
{Î±1, . . . , Î±m} = {Î±q
1, . . . , Î±q
m}
as multi-sets.
Proof
Let
f (X) = (X âˆ’Î±1) Â· Â· Â· (X âˆ’Î±m) = a0 + a1X + Â· Â· Â· + amXm.

2.3 Â· Factorisation of Cyclotomic Polynomials
23
2
Since
(X âˆ’Î±q
1) Â· Â· Â· (X âˆ’Î±q
m) = aq
0 + aq
1 X + Â· Â· Â· + aq
mXm,
f âˆˆFq[X] if and only if
f (X) = (X âˆ’Î±q
1) Â· Â· Â· (X âˆ’Î±q
m).
âŠ“âŠ”
Suppose we want to factorise Xn âˆ’1 in Fq[X].
Our ï¬rst observation is that if (n, q) = qâ€² > 1, then
Xn âˆ’1 = (Xn/qâ€² âˆ’1)qâ€²,
since
qâ€²
j

= 0,
for j = 1, . . . , qâ€² âˆ’1.
Hence, it sufï¬ces to know how to factorise Xm âˆ’1, where (m, q) = 1, in order to
be able to factorise Xn âˆ’1.
We look for an extension ï¬eld of Fq which contains n-th roots of unity by ï¬nding
the minimum h such that n divides qh âˆ’1. This is equivalent to calculating the
(multiplicative) order of q in Z/nZ.
An element Ïµ âˆˆFqh is a primitive n-th root of unity if {1, Ïµ, . . . , Ïµnâˆ’1} is the set of
all n-th roots of unity.
Lemma 2.12 Suppose (n, q) = 1 and let Ïµ âˆˆFqh be a primitive n-th root of unity. The
irreducible factors of Xn âˆ’1 in Fq[X] are given by polynomials
(X âˆ’Ïµr)(X âˆ’Ïµrq) . . . (X âˆ’Ïµrqdâˆ’1),
(2.1)
for r = 0, . . . , nâˆ’1, where d, which depends on r, is the minimum positive integer such that
rqd â‰¡r modulo n.
Proof
By Lemma 2.11,
g(X) = (X âˆ’Ïµr)(X âˆ’Ïµrq) . . . (X âˆ’Ïµrqdâˆ’1) âˆˆFq[X].
Suppose that
f =
m

i=0
aiXi âˆˆFq[X]

2
24
Chapter 2 â€¢ Finite Fields
and that f (Î±) = 0. Then
0 =
m

i=0
aiÎ±i =
m

i=0
aiÎ±iq,
which implies f (Î±q) = 0.
Therefore, g(X) is the minimal degree polynomial in Fq[X] which is zero at Ïµr.
Hence, g is irreducible in Fq[X].
âŠ“âŠ”
For each r âˆˆ{0, . . . n âˆ’1}, the set
{r, rq, rq2, . . . , rqdâˆ’1},
where the elements of the set are computed modulo n, is called a cyclotomic coset.
The set {0, . . . n âˆ’1} splits into the disjoint union of cyclotomic cosets. Considering
the polynomial in (2.1), we see that a cyclotomic coset of size d corresponds to an
irreducible factor of degree d of Xn âˆ’1 in its factorisation over Fq.
Example 2.13
Suppose we wish to factorise X12 âˆ’1 in F17[X].
Since 17 â‰¡5 mod 12, the cyclotomic cosets are
{0}, {1, 5}, {2, 10}, {3}, {4, 8}, {6}, {7, 11}, {9}.
By Lemma 2.12, there are four factors in F17[X] of degree 2 and four factors of degree one.
Alternatively, and somewhat intuitively, X12 âˆ’1 factorises as (X6 âˆ’1)(X6 + 1) and
X6 âˆ’1 = (X3 âˆ’1)(X3 + 1) = (X âˆ’1)(X2 + X + 1)(X + 1)(X2 âˆ’X + 1).
Moreover, âˆ’((X/4)6 âˆ’1) â‰¡X6 + 1 mod 17, so
X6 + 1 â‰¡âˆ’((X/4 âˆ’1)((X/4)2 + (X/4) + 1)((X/4) + 1)((X/4)2 âˆ’(X/4) + 1))
â‰¡(X âˆ’4)(X2 + 4X âˆ’1)(X + 4)(X2 âˆ’4X âˆ’1) (mod 17),
which gives an explicit factorisation of X12 âˆ’1.
â– 
2.4
Affine and Projective Spaces over Finite Fields
The projective space PG(k âˆ’1, q) is the geometry whose i-dimensional subspaces are
the (i +1)-dimensional subspaces of the vector space Fk
q, for i = 0, . . . , k âˆ’2. The 0, 1,
2-dimensional subspaces of PG(k âˆ’1, q) are called points, lines, planes, respectively.
The dimension shift is necessary so that familiar geometric properties hold, such as two
points being joined by a line or three non-collinear points span a plane. A hyperplane

2.4 Â· Affine and Projective Spaces over Finite Fields
25
2
âŠ¡Fig. 2.1 The projective plane over the field of two elements.
is a subspace of co-dimension 1, that is a subspace of one dimension less than the whole
space. A hyperplane of a projective space can be deï¬ned as the kernel of a linear form
(i.e. the set of zeros of a linear form). We use the notation
(x1 : x2 : . . . : xk)
to denote the point of the projective space PG(k âˆ’1, q) which corresponds to the one-
dimensional subspace spanned by the vector (x1, x2, . . . , xk) of Fk
q.
The geometry of points and lines drawn in âŠ¡Figure 2.1 is PG(2, 2). In the left-
hand copy the points have been labelled and in the right-hand copy the lines have been
labelled.
The relevance of projective geometries in the study of error-correcting codes is partly
due to the following. Suppose that the rows of a k Ã— n matrix G form a basis of a k-
dimensional subspace of Fn
q. A vector of the subspace is
(u1, . . . , un) = (a1, . . . , ak)G
for some a = (a1, . . . , ak) âˆˆFk
q.
We will be particularly interested in how many zero coordinates the vector u has.
Let si be the i-th column of the matrix G. We suppose that si Ì¸= 0, for all i = 1, . . . , n.
Let Â· denote the standard scalar product deï¬ned on Fk
q.
Observe that ui = 0 if and only if
a Â· si = 0
if and only if
Î¼a Â· Î»si = 0
for any non-zero Î», Î¼ âˆˆFq. Consider the columns of G as a set (or possibly multi-set)
of points in the projective space PG(k âˆ’1, q).

2
26
Chapter 2 â€¢ Finite Fields
âŠ¡Fig. 2.2 The affine plane over the field of three elements.
Let Ï€a be the hyperplane which is the kernel of the linear form
a1X1 + Â· Â· Â· + akXk.
In geometric terms, the above says that ui = 0 if and only if the point si is incident with
the hyperplane Ï€a. We will return to this in â–·Section 4.4.
The afï¬ne space AG(k, q) is the geometry whose i-dimensional subspaces are the
cosets of the i-dimensional subspaces of the vector space Fk
q, i = 0, . . . , k âˆ’1. The
geometry of points and lines drawn in âŠ¡Figure 2.2 is AG(2, 3). As in the projective
space, the 0, 1, 2 and (k âˆ’1)-dimensional subspaces of AG(k, q) are called points,
lines, planes and hyperplanes, respectively.
2.5
Comments
The standard reference for ï¬nite ï¬elds is Lidl and Neiderreiter [46], a comprehensive
text dedicated to ï¬nite ï¬elds. The more recent
[52] contains a wealth of results
concerning ï¬nite ï¬elds and their applications. There is a chapter on ï¬nite geometries
by Cameron in [16] and Ball [6] is a textbook dedicated to the subject.
2.6
Exercises
2.1 Prove that a ï¬nite ï¬eld has ph elements, for some prime p and positive integer h.
2.2 Construct the addition and multiplication table for a ï¬eld F3[X]/(X2 + X + 2).
2.3 Prove that Xpr âˆ’X divides Xph âˆ’X if and only if r divides h. Conclude that a ï¬nite
ï¬eld with ph elements has a subï¬eld with pr elements if and only if r divides h.
2.4 Use one of the irreducible factors of degree 3 in the factorisation of X7 âˆ’1 in F2[X] to
construct the multiplication table for a ï¬eld with eight elements.

2.6 Â· Exercises
27
2
2.5 Determine the degrees of the irreducible factors of
i.
X15 âˆ’1 in F17[X],
ii.
X23 âˆ’1 in F2[X],
ii.
X12 âˆ’1 in F5[X].
2.6
i.
Factorise X9 âˆ’1 in F7[X].
ii.
Factorise X11 âˆ’1 in F19[X].
iii.
Factorise X8 âˆ’1 in F19[X].
2.7 Suppose n is prime and q is primitive in Fn. Prove that Xnâˆ’1 factorises into irreducible
factors in Fq[X] as
(X âˆ’1)(Xnâˆ’1 + Â· Â· Â· + X + 1).
2.8 Prove that

xâˆˆFq
xj =

0 if q âˆ’1 does not divide j,
âˆ’1 if q âˆ’1 does divide j Ì¸= 0.
2.9 Label the points of âŠ¡Figure 2.2 with the vectors from F2
3 (which are the cosets of the
zero-vector) in such a way that three points are collinear if and only if the three vectors are
contained in a coset of a one-dimensional subspace of the vector space. Note that a coset
of a one-dimensional subspace of the vector space contains the vectors (x, y) which are the
zeros of an equation y = mx + c for some m, c âˆˆF3 or an equation x = c, for some c âˆˆF3.
2.10 By adding four points and one line to âŠ¡Figure 2.2 and extending the lines of the
afï¬ne plane with one point each, complete the afï¬ne plane AG(2, 3) to the projective plane
PG(2, 3).
2.11
i.
Prove that the number of ordered r-tuples of r linearly independent vectors in Fk
q is
(qk âˆ’1)(qk âˆ’q) Â· Â· Â· (qk âˆ’qrâˆ’1).
ii.
Prove that the number of (r âˆ’1)-dimensional subspaces in PG(k âˆ’1, q) is
(qk âˆ’1)(qkâˆ’1 âˆ’1) Â· Â· Â· (qkâˆ’r+1 âˆ’1)
(qr âˆ’1)(qrâˆ’1 âˆ’1) Â· Â· Â· (q âˆ’1)
.
iii.
Prove that the number of (r âˆ’1)-dimensional subspaces in PG(k âˆ’1, q) containing a
ï¬xed (s âˆ’1)-dimensional subspace is equal to the number of (r âˆ’s âˆ’1)-dimensional
subspaces in PG(k âˆ’s âˆ’1, q)
2.12 Prove that the geometry one obtains by deleting a hyperplane from the projective space
PG(k, q) is isomorphic to the afï¬ne space AG(k, q).

29
3
Block Codes
The main parameters of an error correcting block code, which we will often refer to
simply as a code, are its length and minimum distance. In this chapter, we shall primarily
be concerned with the relationship between the size of the code and these parameters.
If we ï¬x the length of the code, then we wish to maximise the minimum distance and
the size of the code, which are contrary aims. If we ï¬x the minimum distance too, then
we simply consider the problem of maximising the size of the code. We shall prove
the Gilbertâ€“Varshamov lower bound, which is obtained by constructing block codes
of a given length and minimum distance by applying the greedy algorithm. We will
prove various upper bounds which will put limits on just how good a block code one
can hope to ï¬nd of a ï¬xed length and minimum distance. Since Shannonâ€™s theorem is
an asymptotic result telling us what rates we can achieve with a code of arbitrarily long
length, we shall for a large part of this chapter focus on sequences of codes whose length
tends to inï¬nity. If we use nearest neighbour decoding then, so that the probability we
decode correctly does not tend to zero, we will be interested in ï¬nding sequences of
codes for which both the transmission rate and the ratio of the minimum distance to the
length are bounded away from zero. We set aside trying to answer the question of how
these codes are implemented until later chapters in which we work with codes which
have more structure.
3.1
Minimum Distance
Let A be a ï¬nite set of r elements called the alphabet. Recall that a block code (or
simply a code) C is a subset of An. We say that C is an r-ary code of length n. A 2-ary
code is called a binary code and a 3-ary code is called a ternary code.
Our aim will be to choose C so that the codewords, the elements of C, are far
apart with respect to the Hamming distance. In this way, the code will have good error-
correcting properties when we use nearest neighbour decoding.
Â© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_3

3
30
Chapter 3 â€¢ Block Codes
Lemma 3.1 Let u, v, w âˆˆAn. The Hamming distance satisï¬es the triangle inequality
d(u, v) â©½d(u, w) + d(w, v).
Proof
If u and v differ in the i-th coordinate, then w differs from one of u or v in the i-th coordinate.
âŠ“âŠ”
Since we will use no other metric on An, we will refer to the Hamming distance
between two elements u and v of An as the distance between u and v.
The minimum distance d of a code C is the minimum distance between any two
codewords of C.
Lemma 3.2 Using nearest neighbour decoding, a block code of minimum distance d can
correct up to 1
2(d âˆ’1) errors.
Proof
For any w âˆˆAn and codewords u and v, Lemma 3.1 implies
d â©½d(u, v) â©½d(u, w) + d(w, v).
Hence, there is at most one codeword at distance at most 1
2(d âˆ’1) from w.
âŠ“âŠ”
Example 3.3
Let A = {a1, . . . , ar}. The r-ary repetition code C of length n is a block code with r
codewords where, for each a âˆˆA, there is a codeword in which a is repeated n times. The
minimum distance of C is n, so C can correct up to 1
2(n âˆ’1) errors using nearest neighbour
decoding. The transmission rate of C is logr |C|/n = 1/n.
bit
a1
a2
...
ar
codeword
a1 Â· Â· Â· a1
a2 Â· Â· Â· a2
...
ar Â· Â· Â· ar
â– 
Example 3.4
The code
C = {000000, 001011, 010101, 100110, 011110, 101101, 110011, 111000}
is a block code of length 6 with 8 codewords which can correct up to 1 error, since it has
minimum distance 3. We can assign to each codeword a distinct triple, which corresponds to
the ï¬rst three bits of the codeword. In this way, for each 3-tuple of bits, we send 6 bits. This
is coherent with the deï¬nition of the transmission rate of C, which is (log2 8)/6 = 1/2.

3.1 Â· Minimum Distance
31
3
triple
000
001
010
100
011
101
110
111
codeword
000000
001011
010101
100110
011110
101101
110011
111000
â– 
Let C be a block code of length n and minimum distance d. An extension of C is
code C of length n + 1 obtained by adding a coordinate to each codeword of C in such
a way that C has minimum distance d + 1. The code C is called an extended code of
the code C.
Theorem 3.5
If C is a binary code of length n and minimum distance d and d is odd, then C has an
extension.
Proof
We can suppose that C âŠ‚{0, 1}n. Let
C = {(u1, . . . , un, un+1) | (u1, . . . , un) âˆˆC},
where
un+1 = u1 + u2 + Â· Â· Â· + un (mod 2).
Suppose that u and v are two codewords of C.
If d(u, v) â©¾d + 1, then their corresponding codewords are at distance at least d + 1 in
C too.
Suppose d(u, v) = d. Consider the sum of the coordinates of u and v modulo 2. In n âˆ’d
coordinates u and v are the same, so these n âˆ’d coordinates contribute zero to the sum. In d
coordinates they are different, so we sum d ones and d zeros, which gives d. Since d is odd,
the sum is non-zero modulo 2. Therefore, un+1 Ì¸= vn+1 and the distance between u and v in
C is d + 1.
âŠ“âŠ”
Example 3.6
Let C be the binary block code of length 6 and minimum distance 3 from Example 3.4. The
extended code C is a binary block code of length 7 and minimum distance 4.
codeword in C
000000
001011
010101
100110
codeword in C
0000000
0010111
0101011
1001101
codeword in C
011110
101101
110011
111000
codeword in C
0111100
1011010
1100110
1110001

3
32
Chapter 3 â€¢ Block Codes
3.2
Bounds on Block Codes
Let Ar(n, d) denote the maximum |C|, for which there exists a r-ary block code C of length
n and minimum distance d.
Theorem 3.7 (Gilbertâ€“Varshamov bound)
We have the lower bound
Ar(n, d)(1 +
n
1

(r âˆ’1) + Â· Â· Â· +

n
d âˆ’1

(r âˆ’1)dâˆ’1) â©¾rn.
â– 
Proof
Let A be the alphabet, a set of r elements such that C âŠ†An, and suppose that C is a block
code of size Ar(n, d).
Let u âˆˆC.
The set Bdâˆ’1(u), of n-tuples in An at distance at most d âˆ’1 to u, has size
1 +
n
1

(r âˆ’1) + Â· Â· Â· +
n
r

(r âˆ’1)dâˆ’1,
since there are precisely
n
j

(r âˆ’1)j of the n-tuples of An at distance j to u.
If
|C|(1 +
n
1

(r âˆ’1) + Â· Â· Â· +

n
d âˆ’1

(r âˆ’1)dâˆ’1) < rn,
then there is an n-tuple which is at distance at least d from all the codewords of C. Therefore,
C is not a code of length n and minimum distance d of maximum size, a contradiction.
â– 
Recall that the binary entropy function was deï¬ned as
h(p) = âˆ’p log2(p) âˆ’(1 âˆ’p) log2(1 âˆ’p).
For a code of length n and minimum distance d, we deï¬ne the relative minimum
distance to be Î´ = d/n.
Corollary 3.8 The following inequality holds
1
n log A2(n, d) â©¾1 âˆ’h(Î´).

3.2 Â· Bounds on Block Codes
33
3
Proof
By Lemma 1.11 and Theorem 3.7,
A2(n, d)2nh(Î´) â©¾2n.
Take logarithms of both sides and divide by n.
â– 
The sphere-packing problem in Rn asks how many spheres can we pack into a box of
given dimensions or, equivalently, what is the maximum ratio of spheres to the volume of the
box one can achieve. In three dimensions, one can think of packing oranges into a box and
trying to maximise the percentage of space inside the box which is taken up with oranges. In
the discrete world, the analogous problem of packing spheres gives us the following theorem.
Let t = âŒŠ(d âˆ’1)/2âŒ‹. In deference to Lemma 3.2, we will sometimes call a code with
minimum distance d, a t-error correcting code.
Theorem 3.9 (Sphere packing bound)
We have the upper bound
Ar(n, d)(1 +
n
1

(r âˆ’1) + . . . +
n
t

(r âˆ’1)t) â©½rn.
Proof
Let A be the alphabet of size r.
Let C âŠ†An be a code of size Ar(n, d).
Suppose that u âˆˆC.
The set Bt(u), of n-tuples in An at distance at most t to u, has size
|Bt(u)| = 1 +
n
1

(r âˆ’1) + . . . +
n
t

(r âˆ’1)t.
For any pair u, v âˆˆC, suppose that w âˆˆBt(u) âˆ©Bt(v). By Lemma 3.1,
d(u, v) â©½d(u, w) + d(w, v) â©½2t â©½d âˆ’1.
This is a contradiction, since the distance between u and v is at least d.
Therefore,
Bt(u) âˆ©Bt(v) = âˆ….
The total number of n-tuples is rn, so

uâˆˆC
|Bt(u)| â©½rn,
from which the required bound follows.
â– 

3
34
Chapter 3 â€¢ Block Codes
In contrast to packing spheres into a box in real space, in spaces over a ï¬nite alphabet it
is possible that the whole space is ï¬lled. A block code which meets the sphere packing bound
in Theorem 3.9 is called a perfect code. For there to exist a perfect code we need parameters
n, r and t so that
1 +
n
1

(r âˆ’1) + . . . +
n
t

(r âˆ’1)t
divides rn.
For example, it is possible that there is a perfect 2-error correcting binary code of length
90, a perfect 3-error correcting binary code of length 23, a perfect 2-error correcting ternary
code of length 11. The former possibility is ruled out in Exercise 3.3. However, as we shall
see in â–·Chapter 5, the latter two perfect codes do occur, see Example 5.9 and Example 5.5.
The proof of the following lemma, Lemma 3.10, counts in two ways the sum of the
distances between any pair of codewords of an r-ary code. In the exercises related to
Lemma 3.10, Exercise 3.4 treats a special case of equality in the bound and Exercises 3.5
and 3.6 exploit the fact that if r does not divide |C|, then improvements can be made to the
coordinate sum estimates.
Lemma 3.10 (Plotkin lemma) An r-ary code C of length n and minimum distance d
satisï¬es
|C|(d + n
r âˆ’n) â©½d.
Proof
Let A be the alphabet, a set of r elements such that C âŠ†An. Fix i âˆˆ{1, . . . , n} and let Î»a
denote the number of times a âˆˆA occurs in the i-th coordinate of a codeword of C.
Clearly,

aâˆˆA
Î»a = |C|.
Since

aâˆˆA
(Î»a âˆ’|C|/r)2 â©¾0,
we have

aâˆˆA
Î»2
a âˆ’2|C|2/r + |C|2/r â©¾0.
Let
S =

u,vâˆˆC
d(u, v),
the sum of all distances between a pair (u, v) of codewords of C.

3.2 Â· Bounds on Block Codes
35
3
Let Si be the contribution that the i-th coordinate makes to this sum.
Then,
Si =

aâˆˆA
Î»a(|C| âˆ’Î»a) â©½|C|2 âˆ’|C|2/r,
using the equality and the inequality from above.
Finally,
d|C|(|C| âˆ’1) â©½S =
n

i=1
Si â©½n(|C|2 âˆ’|C|2/r).
â– 
Example 3.11
Suppose that we wish to ï¬nd a binary code C of length 2d âˆ’2 and minimum distance d.
According to the bound in Lemma 3.10, |C| â©½d. If we suppose that |C| = d, then we have
equality in all the inequalities in the proof of Lemma 3.10. In particular Î»a = d/2, which
implies that d must be even. Moreover, the distance between any two codewords must be
exactly d.
For d = 4, it is not difï¬cult to ï¬nd such a code, for example,
C = {(0, 0, 0, 0, 0, 0), (1, 1, 1, 1, 0, 0), (1, 1, 0, 0, 1, 1), (0, 0, 1, 1, 1, 1)}.
For d = 6 it is not so straightforward, see Exercise 3.4.
â– 
Lemma 3.10 only gives a bound on |C| when n < d + n/r. If n â©¾d + n/r then,
shortening the code by deleting coordinates so that for the shortened code nâ€² < dâ€² + nâ€²/r,
we can use Lemma 3.10 repeatedly to obtain bounds for block codes with relatively smaller
minimum distance. In Theorem 3.12, we restrict to binary codes. For a general Plotkin bound
on r-ary codes, see Exercise 3.8. The bound in Theorem 3.12 will be used to bound the rate
of sequences of binary codes in â–·Section 3.3.
Theorem 3.12 (Plotkin bound)
If C is a binary code of length n and minimum distance d â©½1
2n, then
|C| â©½d2nâˆ’2d+2.
Proof
Let m = n âˆ’2d + 1. For each x âˆˆ{0, 1}m, let Cx be the subset of C whose ï¬rst m bits are
the string x, where these m bits are then deleted. Then Cx is a block code of length 2d âˆ’1
and minimum distance d + e, for some e â©¾0. By Lemma 3.10,

3
36
Chapter 3 â€¢ Block Codes
|Cx| â©½2(d + e)
2e + 1
â©½2d.
Hence,
|C| =

xâˆˆ{0,1}m
|Cx| â©½2m2d = d2nâˆ’2d+2.
â– 
3.3
Asymptotically Good Codes
If we want to send a large amount of data with short length codes, then we have to cut up a
long string of n bits into strings of a ï¬xed length n0. If the probability of decoding the string
of length n0 correctly is p, then the probability of decoding the string of length n is pn/n0,
which tends to zero as n tends to inï¬nity. Shannonâ€™s theorem, Theorem 1.12, tells us that we
should be able to send the string of n bits, through a channel with capacity , which encodes
almost n bits of information, and then decode correctly with a probability approaching 1. In
the proof of Shannonâ€™s theorem, we used the fact the average number of errors which occur
in the transmission of n bits through a binary symmetric channel is (1 âˆ’Ï†)n. Therefore, our
code of length n should be able to correct a number of errors which is linear in n. For this
reason, in a sequence of codes of length n, we want that the minimum distance of the codes
in the sequence also grows linearly with n.
A sequence of asymptotically good codes is a sequence of codes Cn of length n, where
n â†’âˆ, in which d/n and log |Cn|/n are bounded away from zero. In other words, both the
relative minimum distance and the rate are bounded away from zero.
For a sequence of asymptotically good binary codes Cn, deï¬ne
R = lim inf(log2 |Cn|)/n.
For the remainder of this chapter, we will consider only binary codes and prove bounds
on R. We start by applying the sphere packing bound to obtain an upper bound on R.
Theorem 3.13 (Sphere packing bound)
For a sequence of asymptotically good binary codes of relative minimum distance Î´,
R â©½1 âˆ’h( 1
2Î´).
Proof
Using Stirlingâ€™s approximation n! âˆ¼(n/e)nâˆš
2Ï€n,
log
n
t

âˆ¼n log n âˆ’t log t âˆ’(n âˆ’t) log(n âˆ’t),

3.3 Â· Asymptotically Good Codes
37
3
so
1
n log
n
t

âˆ¼log n âˆ’Ï„ log(Ï„n) âˆ’(1 âˆ’Ï„) log((1 âˆ’Ï„)n) = h(Ï„),
where Ï„ = t/n.
Taking logarithms of both sides in the bound in Theorem 3.9,
nR + log
n
t

â©½n.
Therefore, for n large enough,
R + h(Ï„) â©½1,
which proves the bound, since Ï„ = t/n = âŒŠ(Î´n âˆ’1)/2âŒ‹/n.
â– 
Applying Plotkinâ€™s bound we can improve on this for larger values of Î´, see âŠ¡Figure 3.1.
Theorem 3.14 (Plotkin bound)
If Î´ â©½1
2, then
R â©½1 âˆ’2Î´.
Proof
By Theorem 3.12,
2Rn â©½Î´n2nâˆ’2Î´n+2.
Taking logarithms, dividing by n and letting n tend to inï¬nity, the bound follows.
â– 
Our aim now is to improve on Plotkinâ€™s bound. The main idea behind the proof of
Theorem 3.16 is to bound the number of codewords at a ï¬xed distance w from a ï¬xed
n-tuple v. By changing 0 to 1 and 1 to 0 in the coordinates where v has a 1 (applying
the change to all codewords) we obtain a code with the same parameters (length n and
minimum distance d) in which v is now the all-zero n-tuple. Then the number of codewords
at distance w to v is the number of codewords of weight w, where the weight of u âˆˆ{0, 1}n,
denoted wt(u), is the number of non-zero coordinates that u has. Therefore, Lemma 3.15
is really bounding the number of codewords at a ï¬xed distance w to a ï¬xed n-tuple v. In
Exercise 3.12, we will bound the number of codewords at distance at most w to a ï¬xed
n-tuple v. This is an important bound because it tells us that although we cannot in general
correct more than 1
2d errors, even if more errors occur, there are relatively few codewords

3
38
Chapter 3 â€¢ Block Codes
âŠ¡Fig. 3.1 Plotting of the
bounds relative minimum
distance (x) against rate (y).
Gilbertâ€“Varshamov
Plotkin
Sphere packing
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
which could have been sent, providing that not more than 1
2n(1 âˆ’âˆš1 âˆ’2(d/n)) errors
occur. This opens up the possibility of a list decoding algorithm which creates a short list
of possible codewords which could have been sent, even in the case that the received n-tuple
contains a large amount of errors. It is also possible that the list contains just one codeword.
Although e â©¾1
2d errors occur in the transmission, it may be the case that there is only one
codeword at distance e from the received n-tuple. It is even more useful if we use two codes
simultaneously. Take two codes with high rates R1 and R2, which code the same sequence
of bits and suppose both codes are systematic, which means that they conserve the original
message and add some check bits. Example 3.4 is an example of a systematic code and, as
we shall see, so is a linear code with a generator matrix in standard form. Consider the code
we obtain by sending the original message and the two sets of check bits. This code has rate
R1R2
R1 + R2 âˆ’R1R2
.
If we have a list decoding algorithm for both codes, then the sent codeword will be in the
intersection of the two lists and with a high probability will be the only candidate in the
intersection.
Let A(n, d, w) denote the maximum size of a binary code of length n and minimum
distance d in which each codeword has exactly w ones.
Lemma 3.15 The following inequality holds
A(n, d, w) â©½
nd
2w2 âˆ’2wn + dn.

3.3 Â· Asymptotically Good Codes
39
3
Proof
Let C be a binary code of length n and minimum distance d of size A(n, d, w) in which all
codewords have weight w. For any two codewords u, v âˆˆC, d(u, v) â©¾d, so there are at
least d coordinates in which they differ. One of the two codewords has ones in at least 1
2d of
these coordinates. Hence, the scalar product
u Â· v â©½w âˆ’1
2d.
Summing over all pairs of codewords we get

u,vâˆˆC
u Â· v â©½(w âˆ’1
2d)|C|(|C| âˆ’1).
Let Î»i denote the number of times one appears in the i-th coordinate of a codeword of C.
Counting in two ways the number of triples (ui, vi, i) where u and v are codewords such
that ui = vi = 1 and i âˆˆ{1, . . . , n},
n

i=1
Î»i(Î»i âˆ’1) =

u,vâˆˆC
u Â· v.
Since every codeword of C has exactly w ones,
n

i=1
Î»i = w|C|.
The sum
n

i=1
(Î»i âˆ’w
n |C|)2 â©¾0,
which implies that
n

i=1
Î»2
i â©¾w2
n |C|2.
Hence,
w2
n |C|2 âˆ’w|C| â©½(w âˆ’1
2d)|C|(|C| âˆ’1),
from which the bound follows.
â– 
The bound in the following theorem improves on the Plotkin bound and the sphere
packing bound, see âŠ¡Figure 3.2. Observe that if we had a better bound on A(n, d, w),
then we could improve the bound on R. Recall that Î´ is the relative minimum distance.

3
40
Chapter 3 â€¢ Block Codes
âŠ¡Fig. 3.2 Plotting the bounds
of relative minimum distance (x)
against rate (y).
Gilbertâ€“Varshamov
Plotkin
Sphere packing
Eâ€“B
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Theorem 3.16 (Eliasâ€“Bassalygo bound)
If Î´ < 1
2, then
R â©½1 âˆ’h( 1
2(1 âˆ’
âˆš
1 âˆ’2Î´)).
Proof
Let C be a binary code of length n and minimum distance d = âŒŠÎ´nâŒ‹.
For a ï¬xed w âˆˆ{1, . . . , n}, codeword u âˆˆC and v âˆˆ{0, 1}n, let
b(u, v) =

1 d(u, v) = w,
0 otherwise.
Then,

uâˆˆC

vâˆˆ{0,1}n
b(u, v) =

uâˆˆC
n
w

=
n
w

|C|.
For a ï¬xed v, let Câ€² be the set of codewords of C at distance w to v. If we interchange 0 and
1 in the coordinates where v has a 1, we obtain a code from Câ€² of constant weight w. The
number of codewords u for which b(u, v) = 1 is |Câ€²|, which is at most A(d, n, w).

3.3 Â· Asymptotically Good Codes
41
3
Switching the order of the summations in the above equality implies

vâˆˆ{0,1}n

uâˆˆC
b(u, v) â©½2nA(n, d, w).
Hence,
n
w

|C| â©½2nA(n, d, w).
By Lemma 3.15,
(2w2 âˆ’2wn + dn)
n
w

|C| â©½nd2n.
Now, choose w âˆˆN so that
2w
n â‰ˆ1 âˆ’

1 âˆ’2d
n + 2
n.
Then,
2w2 âˆ’2wn + dn â‰ˆ2(w âˆ’1
2n)2 âˆ’1
2n2 + dn â‰ˆn.
As in the proof of Theorem 3.13, using Stirlingâ€™s approximation,
1
n log2
n
w

> h(w/n),
for n large enough. Taking logarithms of
n
w

|C| â©½d2n,
gives
log2 |C| + nh(w/n) â©½n + log2 d.
Dividing by n, and letting n tend to inï¬nity, we get the bound.
â– 
The following bound is an improvement to Eliasâ€“Bassalygo bound for Î´ > 0.14. We
include it here without a proof. There are other improvements to Eliasâ€“Bassalygo bound
known but they are quite complicated, even to state.

3
42
Chapter 3 â€¢ Block Codes
Theorem 3.17 (McElieceâ€“Rodemichâ€“Rumseyâ€“Welch bound)
R â©½h( 1
2 âˆ’

Î´(1 âˆ’Î´)).
In âŠ¡Figure 3.3, the Gilbertâ€“Varshamov lower bound is compared to the best upper
bounds we have seen, the Eliasâ€“Bassalygo bound and the McElieceâ€“Rodemichâ€“Rumseyâ€“
Welch bound. There has been little progress in closing the gap between the lower and upper
bounds. The following conjecture is one of the oldest and yet still unresolved conjectures in
coding theory.
Conjecture 3.18 Given Î´, the Gilbertâ€“Varshamov bound gives the maximum rate for binary
codes with relative minimum distance Î´ as n â†’âˆ.
The following conjecture has been open since 1973.
Conjecture 3.19 Apart from trivial examples, there are no perfect constant weight binary
codes, i.e. there are no constant weight codes achieving the bound in Exercise 3.9.
Although Theorem 3.7 tells us that asymptotically good codes exist, there is a funda-
mental issue which needs to be addressed. We have no feasible algorithm which allows us to
decode such codes. The best we can do, for such a randomly chosen code, is to simply go
Gilbertâ€“Varshamov
Mâ€“Râ€“Râ€“W
Eâ€“B
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
âŠ¡Fig. 3.3 Plotting of the bounds relative minimum distance (x) against rate (y).

3.5 Â· Exercises
43
3
through the codewords, one-by-one, until we ï¬nd a codeword which is at a distance at most t
to the received vector. This is laboriously slow and almost never efï¬cient. In fact, we do not
even have a fast algorithm to assign the codewords. In the next few chapters we shall start to
address these issues.
3.4
Comments
There are many texts which cover block codes, the books by Bierbrauer [10], Hill [37],
Roman [61], Berlekamp [8], MacWilliams and Sloane [50], Ling and Xing [48] and Van
Lint [74] being some classical examples. They are all interesting reads and give more insight
into coding theory. In this book we cover much of the material common to these books and
will progress to topics which have been developed since their publication.
The Gilbertâ€“Varshamov bound is from Gilbert [25], Varshamov proving a similar bound
for linear codes in
[75], see Exercise 4.3. There are no known explicit constructions
of asymptotically good codes meeting the Gilbertâ€“Varshamov bound, unless the relative
minimum distance is close to 1
2, see Ta-Shma [69]. There are many improvements to the
Gilbertâ€“Varshamov bound known but these do not outdo the bound asymptotically.
The Eliasâ€“Bassalygo bound is from
[7]. The Plotkin bound is from
[58] and the
McEliece, Rodemich, Rumsey and Welch bound highlighted in the text is from [51]. There
are bounds of Delsarte which better these bounds for some values of Î´, see [21].
Conjecture 3.19 is from Delsarteâ€™s thesis, see [21]. The main result of Roos [62] restricts
the range of parameters for which equality can occur.
3.5
Exercises
3.1 Prove that if we use the repetition code and nearest neighbour decoding for the binary
symmetric channel in which the probability that a bit changes is 1 âˆ’Ï† < 1
2, then PCOR â†’1
as n â†’âˆ.
3.2 Prove that
C = {(x1, x2, x3, x4, x1 + x2 + x3, x1 + x2 + x4, x1 + x3 + x4) | x1, x2, x3, x4 âˆˆF2}
is a perfect 1-error correcting binary code of length 7.
3.3 Prove that there is no perfect 2-error correcting binary code of length 90.
3.4 Construct a binary code of length 10 and minimum distance 6 of size 6 and so prove
that A2(10, 6) = 6. Use the solution to prove that the bound in Lemma 3.15 is attainable for
A2(10, 6, 6).

3
44
Chapter 3 â€¢ Block Codes
3.5
i.
Prove that there is no ternary code of length 6, minimum distance 5 of size 5.
ii.
Suppose that C is a ternary code of length 6, minimum distance 5 of size 4. Prove that
for every symbol x âˆˆ{0, 1, 2} and every coordinate i, there is a codeword in C with an
x in the i-th coordinate.
iii.
Construct a ternary code of length 6, minimum distance 5 of size 4 and conclude that
A3(6, 5) = 4.
3.6 Prove that A2(8, 5) = 4.
3.7 Let C be the code obtained from two systematic codes of rates R1 and R2 where the two
sets of check bits are appended to the message bits. Prove that the rate of C is
R1R2
R1 + R2 âˆ’R1R2
.
3.8 Prove that if n â©¾dr/(r âˆ’1), then an r-ary code C satisï¬es
|C| â©½
drm+1
dr âˆ’(n âˆ’m)(r âˆ’1),
where m is such that m > n âˆ’rd/(r âˆ’1).
3.9 Prove the sphere packing bound for binary constant weight codes,
 e

i=0
n
i
n âˆ’w
i

A(n, 4e + 2, w) â©½
n
w

.
3.10 Prove that if d is odd, then
A(n, d, w) â©½
dn + n
2w2 âˆ’2wn + dn + n.
3.11 Prove that A(8, 5, 5) = 2.
3.12
i.
Prove that if v1, . . . , vr âˆˆRn have the property that vi Â· vj â©½0, then r â©½2n.
ii.
For u = (u1, . . . , un) âˆˆ{0, 1}n, let Ïƒ(u) be the vector whose j-th coordinate is (âˆ’1)uj .
Prove that
Ïƒ(u) Â· (1, 1, . . . , 1) = n âˆ’2wt(u).

3.5 Â· Exercises
45
3
iii.
For u, v âˆˆ{0, 1}n, prove that
Ïƒ(u) Â· Ïƒ(v) = n âˆ’2d(u, v).
iv.
Let C be a binary code of length n and minimum distance d. Prove that for u, v âˆˆC
(Ïƒ(u) âˆ’Î»(1, 1, . . . , 1)) Â· (Ïƒ(v) âˆ’Î»(1, 1, . . . , 1)) â©½Î»2n + 2Î»(2w âˆ’n) + n âˆ’2d.
v.
By choosing an appropriate Î» in iv., prove that if w â©½1
2n(1 âˆ’âˆš1 âˆ’2(d/n)), then
w

j=0
A(n, d, j) â©½2n.

47
4
Linear Codes
There is a lack of structure in the block codes we have considered in the ï¬rst few
chapters. Either we chose the code entirely at random, as in the proof of Theorem 1.12,
or we built the code using the greedy algorithm, as in the proof of the Gilbertâ€“Varshamov
bound, Theorem 3.7. In this chapter, we introduce some algebraic structure to the block
codes by restricting our attention to linear codes, codes whose codewords are the vectors
of a subspace of a vector space over a ï¬nite ï¬eld. Linear codes have the immediate
advantage of being fast to encode. We shall also consider a decoding algorithm for
this broad class of block codes. We shall prove the Griesmer bound, a bound which
applies only to linear codes and show how certain linear codes can be used to construct
combinatorial designs.
4.1
Preliminaries
If A = Fq and C is a subspace of Fn
q, then we say that C is a linear code over Fq
or simply a linear code. If the subspace has dimension k, then we say that C is a k-
dimensional linear code over Fq. Observe that |C| = qk.
As in the case of an n-tuple, we deï¬ne the weight wt(v) of a vector v âˆˆFn
q as the
number of non-zero coordinates that v has. Recall that the elements of a code are called
codewords.
Lemma 4.1 The minimum distance d of a linear code C is equal to the minimum weight w
of a non-zero codeword of C.
Proof
Suppose u âˆˆC is a codeword of minimum non-zero weight w. Since C is a subspace, the
zero vector 0 is in C. Clearly d(u, 0) = w, so w â©¾d.
Suppose u and v are two codewords at minimum distance from each other, so d(u, v) =
d. Since C is linear, u âˆ’v âˆˆC, and d(u âˆ’v, 0) = d. Hence, there is a codeword in C with
weight d, which implies that d â©¾w.
âŠ“âŠ”
Â© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_4

4
48
Chapter 4 â€¢ Linear Codes
We can describe a linear code C by means of a basis. A matrix G whose rows are a
basis for C is called a generator matrix for C. Thus,
C = {vG | v âˆˆFk
q}.
We will often use the short-hand notation [n, k, d]q code to mean that the code is
a k-dimensional linear code of length n and minimum distance d over Fq. For a not
necessarily linear code, we use the notation (n, K, d)r code to mean a code of length n,
minimum distance d of size K over an alphabet of size r.
Example 4.2
The minimum weight of the non-zero codewords of the 4-dimensional linear code of length
7 over F2 generated by the matrix
G =
â›
âœâœâœâ
1 0 0 0 1 1 1
0 1 0 0 1 1 0
0 0 1 0 1 0 1
0 0 0 1 0 1 1
â
âŸâŸâŸâ 
is 3, so it follows from Lemma 4.1 that the code is a [7, 4, 3]2 code.
â– 
Example 4.3
Consider the [9, 3, d]3 code generated by the matrix
G =
â›
âœâ
1 0 0 1 1 0 1 1 2
0 1 0 1 0 1 1 2 1
0 0 1 0 1 1 2 1 1
â
âŸâ .
Each row of G has weight 6 and it is immediate to verify that a linear combination of two
of the rows also has weight 6. Any linear combination of the ï¬rst two rows has at most 3
coordinates in common with the third row, so we can conclude that the minimum weight of
a non-zero codeword is 6. By Lemma 4.1, G is the generator matrix of a [9, 3, 6]3 code.
â– 
We can also deï¬ne a linear code as the solution of a system of linear equations. A
check matrix for a linear code C is an m Ã— n matrix H with entries from Fq, with the
property that
C = {u âˆˆFn
q | uHt = 0},
where Ht denotes the transpose of the matrix H.
Lemma 4.4 Let C be a linear code with check matrix H. If every set of d âˆ’1 columns of
H are linearly independent, and some set of d columns are linearly dependent, then C has
minimum distance d.

4.1 Â· Preliminaries
49
4
Proof
Let u be a codeword of C and let D be the set of non-zero coordinates of u, so |D| = wt(u).
Let hi be the i-th column of H. Since H is a check matrix for C,

iâˆˆD
uihi = 0.
Thus, there is a linear combination of |D| columns of H which are linearly dependent.
Applying Lemma 4.1 concludes the proof.
âŠ“âŠ”
Example 4.5
Let C be the linear code over Fq deï¬ned by the m Ã— n check matrix H, whose columns are
vectors which span distinct one-dimensional subspaces of Fm
q . In other words, the columns
of H are vector representatives of distinct points of PG(mâˆ’1, q). Since any two columns are
H are linearly independent, Lemma 4.4 implies that C has minimum distance at least 3. By
Exercise 2.11, the number of points of PG(m âˆ’1, q) is (qm âˆ’1)/(q âˆ’1), so
n â©½(qm âˆ’1)/(q âˆ’1).
If we take
n = (qm âˆ’1)/(q âˆ’1),
then C is a code of size qk with parameters, d = 3 and
k = (qm âˆ’1)/(q âˆ’1) âˆ’m.
This code C attains the bound in Theorem 3.9, since
|C|(1 + n(q âˆ’1)) = qk(1 + qm âˆ’1) = qn.
Thus, C is a perfect code.
â– 
Example 4.5 is called the Hamming code. Example 4.2 is the Hamming code with
q = 2 and m = 3. A check matrix for this code is
H =
â›
âœâ
1 1 1 0 1 0 0
1 1 0 1 0 1 0
1 0 1 1 0 0 1
â
âŸâ .
One can readily check that GHt = 0, where G is as in Example 4.2, by verifying that
the scalar product of any row of G with any row of H is zero.

4
50
Chapter 4 â€¢ Linear Codes
Lemma 4.6 Let G be a generator matrix for a k-dimensional linear code C. An mÃ—n matrix
H is a check matrix for C if and only if GHt = 0 and the rank of H is n âˆ’k.
Proof
Suppose that H is an m Ã— n check matrix for C. All the rows of G are codewords of C, so
if u is a row of G, then uHt = 0, which implies GHt = 0. The dimension of the code C is
n âˆ’rank(H), which implies that the rank of H is n âˆ’k.
Suppose that GHt = 0 and that the rank of H is n âˆ’k. A codeword u is a linear
combination of the rows of G, so uHt = 0. Hence, the left kernel of Ht contains C. Since the
rank of H is n âˆ’k, the left kernel of Ht has dimension k, so the left kernel of Ht is C.
âŠ“âŠ”
Let Ir denote the r Ã— r identity matrix.
A generator matrix which has the form
(Ik | A) ,
for some k Ã— (n âˆ’k) matrix A, is said to be in standard form. The uncoded string v
is encoded by vG, whose ï¬rst k coordinates are precisely the coordinates of v. There
are obvious advantages in using a generator matrix in this standard form. Once errors
have been corrected, the uncoded string can be recovered from the codeword by simply
deleting the last n âˆ’k coordinates. Moreover, the following lemma implies that there is
a check matrix with a similar simple form.
Lemma 4.7 Let C be the linear code generated by
G = (Ik | A) ,
for some k Ã— (n âˆ’k) matrix A. Then the matrix
H =

âˆ’At | Inâˆ’k

is a check matrix for C.
Proof
We have to check that the inner product of the i-th row of G = (gij) with the â„“-th row of
H = (hâ„“j) is zero. The entries gij = 0 for j â©½k unless i = j, in which case gii = 1. The
entries hâ„“j = 0 for j â©¾k + 1 unless â„“= j âˆ’k, in which case hâ„“,â„“+k = 1. Hence,
n

j=1
gijhâ„“j =
k

j=1
gijhâ„“j +
n

j=k+1
gijhâ„“j = hâ„“i + gi,â„“+k = âˆ’aiâ„“+ aiâ„“= 0.
âŠ“âŠ”

4.2 Â· Syndrome Decoding
51
4
4.2
Syndrome Decoding
Given a generator matrix G for a linear code C, encoding is fairly simple since we
assign the codeword vG to each vector v of Fk
q. Moreover, if the generator matrix is in
standard form, as described in the previous section, then we can encode by appending the
n âˆ’k coordinates of vA to v. Decoding is a far trickier affair. To use nearest neighbour
decoding we have to ï¬nd the codeword of length n which is nearest to the received n-
tuple. For a code with no obvious structure, this can only be done by calculating the
distance between the received n-tuple and each codeword, something which is laborious
and infeasible for large codes. In this section, we consider a decoding algorithm for
linear codes which exploits the linearity property.
Let C be a linear code with check matrix H. The syndrome of a vector v âˆˆFn
q is
s(v) = vHt.
Note that s(v) = 0 if and only if v âˆˆC, since
C = {v âˆˆFn
q | vH t = 0}.
To use syndrome decoding we compute a look-up table with entries s(e) for all
vectors e of weight at most t = âŒŠ(d âˆ’1)/2âŒ‹. To decode a vector v we compute s(v),
use the look-up table to ï¬nd e such that s(v) = s(e), and decode v as v âˆ’e. Note that
v âˆ’e âˆˆC and the distance between v and v âˆ’e is at most t.
Example 4.8
The matrix
G =
â›
âœâœâœâ
1 0 0 0 0 1 1 1
0 1 0 0 1 0 1 1
0 0 1 0 1 1 0 1
0 0 0 1 1 1 1 0
â
âŸâŸâŸâ 
is the generator matrix of a [8, 4, 4]3 code.
Suppose that a codeword u has been sent and we have received the vector
v = (1, 0, 1, 0, 0, 1, 0, 2).
By Lemma 4.7, the matrix
H =
â›
âœâœâœâ
0 2 2 2 1 0 0 0
2 0 2 2 0 1 0 0
2 2 0 2 0 0 1 0
2 2 2 0 0 0 0 1
â
âŸâŸâŸâ 
is a check matrix for C.

4
52
Chapter 4 â€¢ Linear Codes
Note that âˆ’1 = 2, since we are doing arithmetic with elements of F3.
To decode using syndrome decoding, we calculate the syndrome of v,
s(v) = vHt = (2, 2, 2, 0).
Then we look for the low weight vector e, in this example a vector of weight one, such that
s(v) = s(e). If only one error has occurred in the transmission, the syndrome s(v) must be
equal to s(e), for some vector e of F8
q of weight one. Indeed,
s(v) = s((0, 0, 0, 1, 0, 0, 0, 0)).
Therefore, we correct v to the codeword
v âˆ’e = (1, 0, 1, 2, 0, 1, 0, 2),
which is (1, 0, 1, 2)G.
â– 
In general, using a look-up table would involve searching through
t
j=1
n
j

(q âˆ’1)j
entries, an entry for each non-zero vector of Fn
q of weight at most t. For n large, this
implies that we would have to search through a table with an exponential number of
entries, since
 n
1
2Î´n

âˆ¼2h( 1
2 Î´)n.
This does not imply that there might not be a better method to ï¬nd the vector e with the
property that s(e) = s(v), especially if the linear code has some additional properties
we can exploit. However, we will now prove that decoding a linear code using syndrome
decoding is an NP problem. Under the assumption that P Ì¸= NP, this implies that there
is no polynomial time algorithm that will allow us to decode using syndrome decoding.
Problems in NP are, by deï¬nition, decision problems. So what we mean by saying
that decoding a linear code using syndrome decoding is an NP problem, is that deciding
if we can decode a linear code using syndrome decoding is an NP problem. A decision
problem is in P if there exists a polynomial time algorithm which gives a yes/no answer
to the problem. A decision problem is in NP, if there exists a polynomial time algorithm
which veriï¬es that a â€œyesâ€ solution to the problem, really is a solution. For example,
the Hamiltonian path problem asks if there is a path in a graph which visits all the
vertices without repeating any vertex. This is an NP problem since a â€œyesâ€ solution to
the problem is a Hamiltonian path. This solution can be checked in polynomial time by
checking that each edge in the path is an edge of the graph.

4.2 Â· Syndrome Decoding
53
4
It is not known if NP is a larger class of problems than P or not. A decision problem
D is said to be NP-complete if there is a polynomial time algorithm which reduces every
problem in NP to D. This implies that if we had a polynomial time algorithm to solve
D, then we would have a polynomial time algorithm to solve all problems in NP.
Let T be a subset of {1, . . . , n}3.
A perfect matching M is a subset of T of size n,
M = {(aj1, aj2, aj3) | j = 1, . . . , n} âŠ†T,
where for all i âˆˆ{1, 2, 3},
{aji | j = 1, . . . , n} = {1, . . . , n}.
Deciding whether T has a perfect matching or not is the three-dimensional
matching problem. This decision problem is NP-complete.
For example, let T be the set of triples
{(1, 1, 1), (1, 2, 3), (1, 4, 2), (2, 1, 4), (2, 3, 3), (3, 2, 1), (3, 3, 4),
(4, 3, 2), (4, 3, 3), (4, 4, 4)} .
The three-dimensional matching problem asks if it is possible to ï¬nd a subset M of T
such that each element of {1, 2, 3, 4} appears in each coordinate of an element of M
exactly once. In this example the answer is afï¬rmative,
M = {(1, 4, 2), (2, 1, 4), (3, 2, 1), (4, 3, 3)}.
Theorem 4.9
Decoding a linear code using syndrome decoding is NP-complete.
Proof
To decode a linear code using syndrome decoding, we have to ï¬nd a vector e of weight at
most t, such that eHt = s, where s = s(v) and v is the received vector.
We make this a decision problem by asking if there is a vector e of weight at most t such
that eHt = s. We will show that this decision problem is NP-complete by proving that if
we had a polynomial time algorithm to solve this decision problem, then we would have a
polynomial time algorithm to solve the three-dimensional matching problem.
Let Ri = {1, . . . , n} for i = 1, 2, 3. Let T be a subset of R1 Ã— R2 Ã— R3. Consider
the matrix A whose rows are indexed by the triples in T , whose columns are indexed by
R1 âˆªR2 âˆªR3, where the ((a1, a2, a3), ri) entry is 1 if ai = ri and zero otherwise. Thus, each
row has three ones and 3n âˆ’3 zeros. A perfect matching is given by a vector v of {0, 1}|T |,
necessarily of weight n, such that vA is equal to the all-one vector j. Therefore, if we have a

4
54
Chapter 4 â€¢ Linear Codes
polynomial time algorithm which can decide if there is a vector e of weight at most t, such
that eH t = s, then we can use this to solve the three-dimensional perfect matching decision
problem by asking if there is a vector v of weight n such that vA = j.
âŠ“âŠ”
4.3
Dual Code and the MacWilliams Identities
Let C be a k-dimensional linear code over Fq.
The dual code of a linear code C is
CâŠ¥= {v âˆˆFn
q | u Â· v = u1v1 + Â· Â· Â· + unvn = 0, for all u âˆˆC}.
In other words CâŠ¥is the orthogonal subspace to C, with respect to the standard inner
product. The subspace CâŠ¥is the set of solutions of a homogeneous system of linear
equations of rank k in n unknowns. Hence, the dual code CâŠ¥is a (n âˆ’k)-dimensional
linear code and length n over Fq.
The following lemma is immediate.
Lemma 4.10 If H is a (n âˆ’k) Ã— n check matrix for a k-dimensional linear code C, then H
is a generator matrix for CâŠ¥. Likewise, if G is a generator matrix for C, then G is a check
matrix for CâŠ¥.
If C = CâŠ¥, then we say that C is self-dual.
Example 4.11
The extended code of the binary four-dimensional code in Example 4.2 is a self-dual code. It
has a generator (and check) matrix
â›
âœâœâœâ
1 0 0 0 1 1 1 0
0 1 0 0 1 1 0 1
0 0 1 0 1 0 1 1
0 0 0 1 0 1 1 1
â
âŸâŸâŸâ .
â– 
Let Ai denote the number of codewords of weight i of a linear code C of length n.
The weight enumerator of C is a polynomial deï¬ned as
A(X) =
n

i=0
AiXi.
Let AâŠ¥(X) denote the weight enumerator of the dual code CâŠ¥.

4.3 Â· Dual Code and the MacWilliams Identities
55
4
There is an important relationship between A(X) and AâŠ¥(X), which implies that
one is determined by the other. To be able to prove this relationship, which we shall do
in Theorem 4.13, we introduce the trace map and characters.
Let p be the prime such that q = ph. Then the trace map from Fq to Fp is deï¬ned
as
Tr(x) = x + xp + Â· Â· Â· + xq/p.
By Lemma 2.6, it is additive, i.e.
Tr(x + y) = Tr(x) + Tr(y),
and by Lemma 2.4 and Lemma 2.6,
Tr(x)p = Tr(x),
so, again by Lemma 2.4, Tr(x) âˆˆFp.
Observe that if Tr(Î»x) = 0 for all Î» âˆˆFq, then x = 0, since as a polynomial (in Î») it
has degree q/p. For the same reason, every element of Fp has exactly q/p pre-images
of the trace map from Fq to Fp.
For u âˆˆFn
q, we deï¬ne a character as a map from Fn
q to C by
Ï‡u(x) = e
2Ï€i
p Tr(xÂ·u).
Note that this deï¬nition makes sense since Fp is Z/(pZ).
Lemma 4.12 Let C be a linear code over Fq. Then

uâˆˆC
Ï‡u(x) =

0 if x Ì¸âˆˆCâŠ¥
|C| if x âˆˆCâŠ¥.
Proof
If x âˆˆCâŠ¥, then x Â· u = 0 for all u âˆˆC which implies Ï‡u(x) = 1 for all u âˆˆC and we are
done.
Suppose x Ì¸âˆˆCâŠ¥. If Ï‡v(x) = 1 for all v âˆˆC, then Tr(v Â· x) = 0 for all v âˆˆC, so
Tr(Î»(v Â· x)) = 0 for all Î» âˆˆFq and v âˆˆC. This, we observed before, implies v Â· x = 0 for
all v âˆˆC, so x âˆˆCâŠ¥, a contradiction. Thus, there is a v âˆˆC such that Ï‡v(x) Ì¸= 1. Then,
Ï‡v(x)

uâˆˆC
Ï‡u(x) =

uâˆˆC
Ï‡u+v(x) =

uâˆˆC
Ï‡u(x).
which implies

uâˆˆC
Ï‡u(x) = 0.
âŠ“âŠ”

4
56
Chapter 4 â€¢ Linear Codes
The following theorem relates the weight enumerator of a linear code to the weight
enumerator of its dual code. It is known as the MacWilliams identities.
Theorem 4.13 (MacWilliams)
For a k-dimensional linear code C over Fq of length n we have
qkAâŠ¥(X) = (1 + (q âˆ’1)X)nA

1 âˆ’X
1 + (q âˆ’1)X

.
Proof
Let u = (u1, . . . , un) âˆˆFn
q.
If ui Ì¸= 0, then

wiâˆˆFq
Ï‡wiei(u) = 0,
since we sum each p-th root of unity q/p times, and the sum of the p-th roots of unity is
zero.
Therefore,

wiâˆˆFq\{0}
Ï‡wiei(u) =

q âˆ’1
if ui = 0
âˆ’1
if ui Ì¸= 0
and so
n

i=1

1 +

wiâˆˆFq\{0}
Ï‡wiei(u)X

= (1 + (q âˆ’1)X)nâˆ’wt(u)(1 âˆ’X)wt(u).
Multiplying out the brackets,
n

i=1

1 +

wiâˆˆFq\{0}
Ï‡wiei(u)X

=

wâˆˆFnq
Xwt(w)
n

i=1
Ï‡wiei(u) =

wâˆˆFnq
Xwt(w)Ï‡u(w).
Combining the above two equations,

wâˆˆFnq
Xwt(w)Ï‡u(w) = (1 + (q âˆ’1)X)nâˆ’wt(u)(1 âˆ’X)wt(u).
Summing over u âˆˆC, we have

uâˆˆC

wâˆˆFnq
Xwt(w)Ï‡u(w) = (1 + (q âˆ’1)X)nA

1 âˆ’X
1 + (q âˆ’1)X

,

4.3 Â· Dual Code and the MacWilliams Identities
57
4
since
A(X) =

uâˆˆC
Xwt(u).
Switching the order of the summations, and applying Lemma 4.12,

wâˆˆFnq
Xwt(w) 
uâˆˆC
Ï‡u(w) =

wâˆˆCâŠ¥
Xwt(w)|C| = |C|AâŠ¥(X).
âŠ“âŠ”
Observe that Theorem 4.13 implies that if we know the weights of the codewords
of C, then we know the weights of the codewords of CâŠ¥and in particular the minimum
weight of a non-zero codeword and therefore, by Lemma 4.1, the minimum distance of
CâŠ¥.
If C is a self-dual code, we can get information about the weights of the codewords
of C from Theorem 4.13.
Example 4.14
Let C be a self-dual 4-dimensional binary linear code of length 8, for instance, as in
Example 4.11. Then, equating the coefï¬cient of Xj, for j = 0, . . . , 8, in
A(X) = 2âˆ’4(1 + X)8A((1 âˆ’X)/(1 + X)),
where
A(X) = 1 +
8

i=1
aiXi,
will give a system of nine linear equations and eight unknowns.
This system has the solution
A(X) = 1 + 14X4 + X8 + Î»(X2 âˆ’2X4 + X6),
for some Î» âˆˆ{0, . . . , 7}. Thus, C must contain the all-one vector and if the minimum distance
of C is 4, then
A(X) = 1 + 14X4 + X8.
â– 
We will see an important application of the MacWilliams identities in â–·Section 4.6
where we will exploit these equations to prove that, under certain hypotheses, we can
construct combinatorial designs from a linear code.

4
58
Chapter 4 â€¢ Linear Codes
4.4
Linear Codes and Sets of Points in Projective Spaces
A linear code C is the row space of a generator matrix G. The multi-set S of columns of
G also contains information about the code and its parameters. The length of C is |S|, the
dimension of C is the length of the vectors in S and, as we shall prove in Lemma 4.15,
the weights of the codewords in C can be deduced from the intersection of S with the
hyperplanes of Fk
q. Observe that S is a multi-set since columns can be repeated.
Lemma 4.15 The multi-set S of columns of a generator matrix G of a [n, k, d]q code C is a
multi-set of n vectors of Fk
q in which every hyperplane of Fk
q contains at most n âˆ’d vectors
of S, and some hyperplane of Fk
q contains exactly n âˆ’d vectors of S.
Proof
There is a bijection between the vectors of Fk
q and the codewords, given by
v â†’vG.
For each non-zero vector v of Fk
q, the subspace consisting of the vectors (x1, . . . , xk) âˆˆ
Fk
q, such that
v1x1 + Â· Â· Â· + vkxk = 0,
is a hyperplane of Fk
q, which we denote by Ï€v. The non-zero multiplies of v deï¬ne the same
hyperplane, so Ï€v = Ï€Î»v, for all non-zero Î» âˆˆFq.
We can label the coordinates of vG by the elements of S. The s-coordinate of the
codeword vG is the value of the scalar product v Â· s. The scalar product v Â· s = 0 if and
only if s âˆˆÏ€v. Therefore, the codeword vG has weight w if and only if the hyperplane Ï€v
contains n âˆ’w vectors of S. The lemma follows since, by Lemma 4.1, the minimum weight
of a non-zero vector of C is equal to the minimum distance.
âŠ“âŠ”
Lemma 4.15 is still valid if we replace a vector s of S by a non-zero scalar multiple
of s. Thus, we could equivalently state the lemma for a multi-set of points in PG(k âˆ’
1, q), assuming that the vectors in S are non-zero vectors. In the projective space, the
hyperplane Ï€v is a hyperplane of PG(k âˆ’1, q). The s-coordinate of the codeword vG
is zero if and only if the point s is incident with the hyperplane Ï€v, as we saw in â–·
Section 2.4.
We could also try and construct a multi-set S of points of PG(k âˆ’1, q) in which we
can calculate (or at least bound) the size of the intersections of S with the hyperplanes
of PG(k âˆ’1, q). Then Lemma 4.15 implies that we can bound from below the minimum
distance of the linear code we obtain from a generator matrix whose columns are vector
representatives of the points of S.
Example 4.16
Let Ï†(X) = Ï†(X1, X2, X3) be an irreducible homogeneous polynomial over Fq in three
variables of degree m. Let S be the set of points of PG(2, q) which are zeros of this

4.5 Â· Griesmer Bound
59
4
polynomial. Since Ï† is irreducible, each line of PG(2, q) contains at most m points of S.
By Lemma 4.15, the matrix whose columns are a vector representative of the points of S is
a 3 Ã— |S| matrix which generates a code with minimum distance at least n âˆ’deg Ï†. This can
give an easy way to make codes with surprisingly good parameters. For example, suppose q
is a square and we take the Hermitian curve, deï¬ned as the zeros of the polynomial
Ï†(X) = X
âˆšq+1
1
+ X
âˆšq+1
2
+ X
âˆšq+1
3
.
This curve has qâˆšq+1 points and is irreducible. Thus we obtain a [qâˆšq+1, 3, qâˆšqâˆ’âˆšq]q
code.
â– 
We say that two codes are equivalent if one can be obtained from the other by
a permutation of the coordinates and permutations of the symbols in each coordinate.
Note that non-linear codes can be equivalent to linear codes. Indeed, one can obtain a
non-linear code (of the same size, length and minimum distance) from a linear code by
simply permuting the symbols of Fq in a ï¬xed coordinate.
We can use S to obtain a model for all codes that are equivalent to a linear code
C, this is called the Aldersonâ€“Bruenâ€“Silverman model. Let S be the multi-set of n
points of  = PG(k âˆ’1, q), obtained from the columns of a generator matrix G of the
k-dimensional linear code C of length n. For each point (s1 : . . . : sk) of S, we deï¬ne a
hyperplane Ï€s of  = PG(k âˆ’1, q) as the kernel of the linear form
Î±s(X) = s1X1 + Â· Â· Â· + skXk.
We embed  in a PG(k, q) and consider PG(k, q) \  which, by Exercise 2.12, is
isomorphic to AG(k, q). Within PG(k, q), we label each hyperplane (Ì¸= ) containing
Ï€s with an element of Fq. For each point v of the afï¬ne space PG(k, q) \  we
obtain a codeword u of Câ€², a code equivalent to the code C. The coordinates of u are
indexed by the elements of S, and the s-coordinate of u is the label given to the unique
hyperplane of PG(k, q) spanned by Ï€s and v. Observe that two codewords u and uâ€² of
Câ€² (obtained from the points v and vâ€², respectively) agree in an s-coordinate if and only
if Î±s(v) = Î±s(vâ€²). The vectors vG and vâ€²G are codewords of C, so agree in at most
n âˆ’d coordinates, which implies that there are at most n âˆ’d elements s âˆˆS such that
Î±s(v) = Î±s(vâ€²). Thus, u and uâ€² agree in at most n âˆ’d coordinates. Furthermore, there
are two codewords which agree in exactly n âˆ’d coordinates. Therefore, the code Câ€² is
of length n and minimum distance d. It is Exercise 4.10, to prove that the code Câ€² is
equivalent to the linear code C. This model is used in Exercise 4.11 to prove that if a
linear code has a non-linear extension, then it has a linear extension.
4.5
Griesmer Bound
In â–·Chapter 3 we proved various bounds involving the length, the minimum distance
and the size of a block code. In this section, we shall prove another bound involving these

4
60
Chapter 4 â€¢ Linear Codes
parameters, the Griesmer bound, which is speciï¬cally for linear codes. The Griesmer
bound follows almost directly from the following lemma.
Lemma 4.17 If there is a [n, k, d]q code, then there is a [n âˆ’d, k âˆ’1, â©¾

d
q

]q code.
Proof
Let S be the multi-set of columns of a generator matrix G of a k-dimensional linear code C
of length n and minimum distance d over Fq.
By Lemma 4.15, there is a non-zero vector v âˆˆFk
q such that the hyperplane Ï€v of Fk
q
contains n âˆ’d vectors of S. Let Sâ€² be this multi-set of n âˆ’d vectors. Let Gâ€² be the k Ã—
(n âˆ’d) matrix whose columns are the vectors of Sâ€². The matrix Gâ€² generates a linear code
Câ€², obtained from Gâ€² by left multiplication by a vector of Fk
q. The matrix Gâ€² is not, strictly
speaking, a generator matrix of Câ€², since its rows are not linearly independent. The vector v
is in the left nucleus of Gâ€². The code Câ€² is the subspace spanned by the rows of the matrix
Gâ€².
We want to prove that Câ€² is a (k âˆ’1)-dimensional linear code. The rank of Gâ€² is at most
k âˆ’1, since vGâ€² = 0. If the rank is less than k âˆ’1, then there is another vector vâ€² âˆˆFk
q,
not in the subspace spanned by v, for which vâ€²Gâ€² = 0. But then we can ï¬nd a Î» âˆˆFq
such that (v + Î»vâ€²)G has zeros in more than n âˆ’d coordinates, which implies that C has
non-zero codewords of weight less than d, which contradicts Lemma 4.1. Hence, Câ€² is a
(k âˆ’1)-dimensional linear code.
Let dâ€² be the minimum distance of the code Câ€² . By Lemma 4.15, there is a hyperplane
Ï€â€² of Ï€v which contains n âˆ’d âˆ’dâ€² vectors of Sâ€². By Exercise 2.12, there are precisely
q + 1 hyperplanes of Fk
q containing the co-dimensional two subspace Ï€â€². Each one of these
hyperplanes contains at most n âˆ’d vectors of S and so at most dâ€² vectors of S \ Ï€â€². Hence,
n â©½(q + 1)dâ€² + n âˆ’d âˆ’dâ€²,
which gives
dâ€² â©¾
d
q

.
âŠ“âŠ”
Theorem 4.18 (Griesmer bound)
If there is a [n, k, d]q code, then
n â©¾
kâˆ’1

i=0
 d
qi

.

4.5 Â· Griesmer Bound
61
4
Proof
By induction on k.
For k = 1 the bound gives n â©¾d, which is clear.
By Lemma 4.17, there is a [n âˆ’d, k âˆ’1, dâ€²]q code, where
dâ€² â©¾
d
q

.
By induction,
n âˆ’d â©¾
kâˆ’2

i=0
 dâ€²
qi

â©¾
kâˆ’2

i=0
 d
qi+1

=
kâˆ’1

i=1
 d
qi

.
âŠ“âŠ”
Example 4.19
Consider the problem of determining the largest ternary code C of length 10 and minimum
distance 4. The Plotkin bound from Lemma 3.10 does not apply, since d+n/râˆ’n is negative.
The sphere packing bound, Theorem 3.9, implies
|C| â©½310/21.
The Griesmer bound tells us that if there is a linear code with these parameters, then
10 â©¾4 + 2 + k âˆ’2.
and so
|C| â©½36.
To construct such a code, according to Lemma 4.15, we need to ï¬nd a set S of 10 points in
PG(5, 3) with the property that any hyperplane is incident with at most 6 points of S. Let G
be the 6 Ã— 10 matrix whose columns are vector representatives of the 10 points of S. The
matrix G is the generator matrix of a [10, 6, 4]3 code. Such a matrix G can be found directly,
see Exercise 4.14. However, we can construct such a code geometrically in the following
way.
Let CâŠ¥be the linear code over Fq generated by the 4Ã—(q2+1) matrix H, whose columns
are the points of an elliptic quadric. For example, we could take the elliptic quadric deï¬ned
as the zeros of the homogeneous quadratic form
X1X2 âˆ’f (X3, X4),
where f (X3, X4) is an irreducible homogeneous polynomial of degree two. Explicitly the
points of the quadric are

4
62
Chapter 4 â€¢ Linear Codes
{(1, f (x, y), x, y) | x, y âˆˆFq} âˆª{(0, 1, 0, 0)}.
As in the real projective space, the elliptic quadric has no more than two points incident with
any line. To verify this algebraically, consider the line which is the intersection of the planes
deï¬ned by X1 = a3X3 + a4X4 and X2 = b3X3 + b4X4. The x3 and x4 coordinates in the
intersection with the quadric satisfy
(a3x3 + a4x4)(b3x3 + b4x4) âˆ’f (x3, x4) = 0,
which is a homogeneous polynomial equation of degree two in two variables. It is not
identically zero, since f is irreducible, so there are at most two (projectively distinct or
homogeneous) solutions for (x3, x4); the x1 and x2 coordinates are then determined by
x1 = a3x3 + a4x4 and x2 = b3x3 + b4x4. This checks the intersection with q4 lines, the
intersection with the remaining lines can be checked similarly.
Therefore, any three columns of the matrix H are linearly independent, since three
linearly dependent columns would imply three collinear points on the elliptic quadric. The
elliptic quadric has four co-planar points, so H has four linearly dependent columns. By
Lemma 4.4, C has a minimum distance 4 and is therefore a [q2 + 1, q2 âˆ’3, 4]q code.
Substituting q = 3, we obtain a ternary linear code C meeting the Griesmer bound.
The geometry also allows us to calculate the weight enumerator of CâŠ¥and hence the
weight enumerator of C. Since any three points span a plane which intersects the elliptic
quadric in a conic, and a conic contains q + 1 points, there are
(q2 + 1)q2(q2 âˆ’1)
(q + 1)q(q âˆ’1)
= (q2 + 1)q
planes incident with q + 1 points of the elliptic quadric and the remaining q2 + 1 planes are
incident with exactly one point. This implies that CâŠ¥has (q2 + 1)q(q âˆ’1) codewords of
weight q2 âˆ’q, (q2 + 1)(q âˆ’1) codewords of weight q2 and one codeword of weight zero.
For q = 3, the weight enumerator of CâŠ¥is
AâŠ¥(X) = 1 + 60X6 + 20X9.
The MacWilliams identities, Theorem 4.13, imply that C has weight enumerator,
A(X) = 1 + 60X4 + 144X5 + 60X6 + 240X7 + 180X8 + 20X9 + 24X10.
Even if we do not restrict ourselves to linear codes, there is no larger code known with these
parameters. The best known upper bound is |C| â©½891.
â– 
Example 4.20
Consider the problem of determining if there is a (16, 256, 6)2 code C, that is a binary code
of length 16 with minimum distance 6 and size 256. The sphere packing bound, Theorem 3.9,
implies

4.6 Â· Constructing Designs from Linear Codes
63
4
|C|(1 + 16 +
16
2

) â©½216,
which is satisï¬ed. The Plotkin bound, Theorem 3.12, does not give a contradiction since
|C| â©½d2nâˆ’2d+2 = 384.
Now, suppose that the code is linear, so C is a [16, 8, 6]2 code. The Griesmer bound is
also satisï¬ed since,
n â©¾6 +
6
2

+
6
4

+
7

i=3
 6
2i

= 16.
However, Lemma 4.17 implies the existence of a [10, 7, â©¾3]2 code. This code is a 1-error
correcting binary code of length 10, so the sphere packing bound, Theorem 3.9, implies that
(1 + 10)27 â©½210,
which is a contradiction. Therefore, there is no [10, 7, â©¾3]2 code. Hence, there is no
[16, 8, 6]2 code. However, there is a non-linear (16, 256, 6)2 code and we shall construct
one both in â–·Chapter 9 and in â–·Chapter 10.
â– 
4.6
Constructing Designs from Linear Codes
A Ï„-design is a collection D of Îº-subsets of {1, . . . , n} with the property that every Ï„-
subset of {1, . . . , n} is contained in precisely Î» subsets of D, for some ï¬xed positive
integer Î». If we want to specify the parameters, then we say that D is a Ï„-(n, Îº, Î»)
design.
Let u âˆˆFn
q. The support of u = (u1, . . . , un) is a subset of {1, . . . , n} deï¬ned as
{i âˆˆ{1, . . . , n} | ui Ì¸= 0}.
In this section we shall prove that if the codewords of the dual of a linear code C have
few distinct weights, then one can construct Ï„-designs from the supports of codewords
of C of a ï¬xed weight. Before proving the main theorem, we will prove by counting that
we can construct a 3-design from the extended Hamming code, Example 4.11.
Example 4.21
In Example 4.14, we calculated the weight distribution for the extended Hamming code in
Example 4.11 and deduced that there are 14 codewords of weight 4. Two codewords u and v
of weight 4 have at most two 1â€™s in common, since otherwise u + v would be a codeword of
weight 2. Therefore, every 3-subset of {1, . . . , 8} is contained in the support of at most one
codeword of weight 4. There are 14
4
3

= 56 subsets of size 3 of the 14 supports of the 14
codewords of weight 4 and
8
3

= 56 subsets of size 3 of {1, . . . , 8}. Hence, each 3-subset is

4
64
Chapter 4 â€¢ Linear Codes
contained in a unique support of a codeword of weight 4 and we have deduced that the set of
these supports is a 3-(8, 4, 1) design.
â– 
In the following theorem, Îº can be any number in the set {d, . . . , n} in the case that
q = 2, since the condition is vacuous. If q Ì¸= 2, then, by Exercise 4.15, the condition is
surely satisï¬ed if
Îº âˆˆ{d, . . . , d âˆ’1 +
d âˆ’1
q âˆ’2

}.
In order to simplify the statement of the following theorem, we say that C has a weight
w if there is a codeword of C of weight w.
Theorem 4.22
Let C be an [n, k, d]q code such that CâŠ¥has at most d âˆ’Ï„ non-zero weights of weight
at most n âˆ’Ï„, for some Ï„ â©½d âˆ’1. If Îº has the property that two codewords of C of
weight Îº have the same support if and only if they are multiples of each other, then the
set of supports of the codewords of C of weight Îº is a Ï„-(n, Îº, Î») design, for some Î».
Proof
Let T be a Ï„-subset of {1, . . . , n}. Let C \ T be the code obtained from C by deleting the
coordinates indicated by the elements of T . If after deleting Ï„ coordinates the codewords u
and v are the same, then u and v differ in at most Ï„ coordinates. Since Ï„ â©½d âˆ’1, this cannot
occur, so deleting the coordinates does not reduce the number of codewords. Hence, C \ T is
a k-dimensional linear code of length n âˆ’Ï„.
Let CâŠ¥
T be the subset of codewords of CâŠ¥which have zeros in all the coordinates
indicated by the elements of T . Then CâŠ¥
T \ T is a linear code and
CâŠ¥
T \ T âŠ†(C \ T )âŠ¥,
since a vector in CâŠ¥
T is orthogonal to all the vectors of C and has zeros in the coordinates
indicated by the elements of T . Furthermore,
dim(CâŠ¥
T \ T ) = dim CâŠ¥
T
since the codewords of CâŠ¥
T have zeros in the coordinates indexed by T , so deleting these
coordinates does not reduce the number of codewords.
Let H be a generator matrix for CâŠ¥. Let L be the set of Ï„ vectors of Fnâˆ’k
q
which are the
columns of H indicated by the elements of T . Then
CâŠ¥
T = {vH | v âˆˆFnâˆ’k
q
, v Â· s = 0, for all s âˆˆL},

4.6 Â· Constructing Designs from Linear Codes
65
4
since vH is a codeword of CâŠ¥and has zeros in the coordinates indexed by T precisely when
v Â· s = 0, for all s âˆˆL.
Hence,
dim CâŠ¥
T â©¾n âˆ’k âˆ’Ï„.
Now,
dim(C \ T ) = k
implies
dim(C \ T )âŠ¥= n âˆ’Ï„ âˆ’k
and we just proved that
dim(CâŠ¥
T \ T ) â©¾n âˆ’Ï„ âˆ’k,
so we have that
CâŠ¥
T \ T = (C \ T )âŠ¥.
The weight of a codeword of CâŠ¥
T \ T is the weight of the corresponding codeword of CâŠ¥. By
hypothesis, CâŠ¥has at most d âˆ’Ï„ non-zero weights of weight at most n âˆ’Ï„. Since at least Ï„
of the coordinates of a codeword of CâŠ¥
T are zero, CâŠ¥
T has weights at most n âˆ’Ï„. Therefore,
(C \ T )âŠ¥has at most d âˆ’Ï„ non-zero weights.
Since C \ T has minimum distance at least d âˆ’Ï„, Exercise 4.16 implies that the weight
enumerator of C \ T is determined.
If u is a non-zero codeword, then Î¼u is another codeword with the same support as u,
for all non-zero Î¼ âˆˆFq. The number Î»(q âˆ’1), of codewords of C \ T of weight Îº âˆ’Ï„,
is determined by the weight enumerator of C \ T . The number Î» does not depend on which
subset T we choose, only the size of the subset T . By induction on Îº, for all Ï„-subsets T of
{1, . . . , n}, there are a ï¬xed number of supports of the codewords of weight Îº containing T .
Therefore, the set of the supports of the codewords of C of weight Îº is a Ï„-(n, Îº, Î»)
design.
âŠ“âŠ”
Example 4.23
Consider the [10, 6, 4]3 code from Example 4.19. The dual code CâŠ¥has codewords of weight
0,6 and 9 so, according to Theorem 4.22, the set of supports of the codewords of weight Îº
is a 3-design, provided that no two codewords of C of weight Îº have the same support. By
Exercise 4.15, we can be assured of this for Îº âˆˆ{4, 5, 6, 7}.
To calculate Î», we count in two ways the number of 3-subsets. Each 3-subset of
{1, . . . , 10} is contained in Î» 3-subsets of the design, so
10
3

Î» =
Îº
3

Î±,

4
66
Chapter 4 â€¢ Linear Codes
where Î± is the number of supports of codewords of C of weight Îº. The number of supports
of codewords of weight Îº is the number of codewords of weight Îº divided by q âˆ’1.
Therefore, from the code C we can construct a 3-(10, 4, 1)-design, a 3-(10, 5, 6)-design
and a 3-(10, 6, 5)-design.
â– 
In Example 4.23, we could have constructed the designs directly from the elliptic
quadric. For example, the 3-(10, 4, 1) design is obtained by taking subsets of 4 co-
planar points and the 3-(10, 5, 6) design is obtained by taking subsets of 5 points,
no 4 co-planar. In â–·Chapter 5 we shall construct codes from polynomial divisors
of Xn âˆ’1 which will often satisfy the hypothesis of Theorem 4.22 and allow us to
construct designs. In many cases, these designs cannot be constructed directly from any
geometrical object.
4.7
Comments
The MacWilliams identities from â–·Chapter 4 appear in MacWilliamsâ€™ thesis â€œCom-
binatorial Problems of Elementary Group Theoryâ€, although the standard reference is
[50]. The MacWilliams identities lead to a set of constraints on the existence of an
[n, k, d]q code. We have that A0 = 1 and A1 = Â· Â· Â· = Adâˆ’1 = 0 and that
1 + Ad + Â· Â· Â· + An = qk.
Since
AâŠ¥
i â©¾0,
Theorem 4.13 implies, for a ï¬xed n and q, the linear constraint
n

j=0
AjKi(j) â©¾0.
The coefï¬cients
Ki(j) =
j

r=0
j
r
n âˆ’j
i âˆ’r

(âˆ’1)r(q âˆ’1)iâˆ’r
are called the Krawtchouk polynomials. Delsarte [21] proved that from the distance
distribution between the codewords of an arbitrary code (not necessarily a linear code)
one can deduce similar inequalities, called the linear programming bound. This can be
a powerful tool, not only in ruling out certain parameter sets, but also for the construction
of codes, since it can give signiï¬cant information about the distance distribution.

4.8 Â· Exercises
67
4
The Griesmer bound is from [31] and the Hamming code was ï¬rst considered by
Hamming in [34]. The upper bound on the size of the code in Example 4.19 is from
[55].
The Aldersonâ€“Bruenâ€“Silverman model for codes equivalent to linear codes in â–·
Section 4.4 is from [2]. The fact that a linear code with a non-linear extension has a
linear extension, Exercise 4.11, is due to Alderson and GÃ¡cs, see [1].
Theorem 4.22 is the Assmusâ€“Mattson theorem from [4].
The bound in Exercise 4.3 is due to Varshamov [75] and is known as the linear
Gilbertâ€“Varshamov bound.
4.8
Exercises
4.1 Prove that if C is linear, then the extended code C is linear.
4.2 Prove that the code in Example 4.2 is a perfect code.
4.3 Prove that if
dâˆ’2

j=0
n âˆ’1
j

(q âˆ’1)j < qnâˆ’k,
then there exists an [n, k, d]q code.
4.4 Prove that the system of equations in Example 4.14 has the solution
A(X) = 1 + 14X4 + X8 + Î»(X2 âˆ’2X4 + X6).
4.5 Prove that the code in Example 4.8 has minimum distance 4 and decode the received
vector (0, 1, 1, 0, 2, 2, 2, 0) using syndrome decoding.
4.6 Prove that the code C in Example 3.4 is linear but not self-dual although for the weight
enumerator A(X) of C, we have A(X) = AâŠ¥(X). Prove that C is equivalent to CâŠ¥.
4.7 Let C be the linear code over F5 generated by the matrix
G =
â›
âœâ
1 0 0 1 1 2
0 1 0 1 2 1
0 0 1 2 1 1
â
âŸâ .
Calculate the minimum distance of C and decode the received vector (0, 2, 3, 4, 3, 2) using
syndrome decoding.

4
68
Chapter 4 â€¢ Linear Codes
4.8 Let C be the linear code over F7 deï¬ned by the check matrix
H =
â›
âœâœâœâ
1 1 1 1 1 1 1
0 1 2 3 4 5 6
0 1 4 2 2 4 1
0 1 1 6 1 6 6
â
âŸâŸâŸâ .
i.
Prove that C is a [7, 3, 5]7 code.
ii.
Decode the received vector (2, 2, 3, 6, 1, 2, 2) using syndrome decoding.
4.9 Let C be the 3-dimensional linear code over F3 generated by the matrix
â›
âœâ
1 0 0 1 1 2 0 1 1
0 1 0 1 2 1 1 0 1
0 0 1 2 1 1 1 1 0
â
âŸâ .
Prove that C has minimum distance 6 and use syndrome decoding to decode the received
vector
(1, 2, 0, 2, 0, 2, 0, 0, 0).
4.10 Prove that the code Câ€² obtained from the Aldersonâ€“Bruenâ€“Silverman model is
equivalent to the linear code C from which the model is set up.
4.11 Let S be the set of n vectors obtained from the set of columns of a generator matrix of
a linear code C and suppose that C has an extension to a code of length n + 1 and minimum
distance d + 1.
i.
Prove that there is a function
f : Fk
q â†’Fq
with the property that if f (u) = f (v), then u âˆ’v is orthogonal (with respect to the
standard inner product) to less than n âˆ’d points of S.
ii.
Let T be the set of vectors of Fk
q which are orthogonal to n âˆ’d vectors of S. Let v âˆˆT
and let u1, . . . , ukâˆ’2 be a set of k âˆ’2 vectors extending v to a set of k âˆ’1 linearly
independent vectors. Prove that for all Î»1, . . . , Î»kâˆ’2, Î», Î¼ âˆˆFq, Î» Ì¸= Î¼,
f (Î»1u1 + Â· Â· Â· + Î»kâˆ’2ukâˆ’2 + Î»v) Ì¸= f (Î»1u1 + Â· Â· Â· + Î»kâˆ’2ukâˆ’2 + Î¼v).
iii.
Prove that if every hyperplane of Fk
q contains a vector of T , then every hyperplane of
Fk
q contains qkâˆ’2 vectors u such that f (u) = 0.
iv.
Prove that there is a hyperplane of Fk
q not containing a vector of T .
v.
Prove that C has a linear extension. In other words, it can be extended to a [n+1, k, d+
1]q code.

4.8 Â· Exercises
69
4
4.12 Prove that for ï¬xed r = n âˆ’d, the Griesmer bound implies n â©½(r âˆ’k + 2)q + r.
4.13 Let r = n âˆ’d and let S be the set of columns of a generator matrix of a 3-dimensional
linear code C of length (r âˆ’1)q + r, so we have equality in the bound of Exercise 4.12.
Prove that S is a set of vectors of Fk
q in which every hyperplane contains 0 or r vectors of S.
Equivalently show that the non-zero codewords of C have weight n or d.
4.14
i.
Verify that equality in the Griesmer bound occurs for the parameters of the code C in
Example 4.19 if and only if q = 3.
ii.
Let G be a 6 Ã— 10 matrix
G =

I6 A

.
Let S be the set of rows of the 6 Ã— 4 matrix A, considered as 6 points of PG(3, 3).
Prove that G is a generator matrix of a [10, 6, 4]3 code if and only if S has the property
all points of S have weight at least three (i.e. the points of S have at most one zero
coordinate), no two points of S are collinear with a point of weight one and that no
three points of S are collinear.
iii.
Find a matrix A so that G is a generator matrix for a [10, 6, 4]3 code.
4.15 Let C be a linear code over Fq, where q Ì¸= 2.
i.
Prove that if w âˆ’âŒˆw/(q âˆ’1)âŒ‰< d, where d is the minimum distance of a linear code
C, then two codewords of C of weight w have the same support if and only if they are
multiples of each other.
ii.
Prove that if w â©½(d âˆ’1)(q âˆ’1)/(q âˆ’2), then w âˆ’âŒˆw/(q âˆ’1)âŒ‰< d.
4.16 Let C be a linear code of length n and minimum distance d with the property that CâŠ¥
has at most d distinct weights, w1, . . . , wd.
i.
Let Aj denote the number of codewords of C of weight j and let AâŠ¥
j denote the number
of codewords of CâŠ¥of weight j. Prove that
qk
n

j=0
AâŠ¥
j (1âˆ’X)j = (1+(q âˆ’1)(1âˆ’X))n+
n

j=d
AjXj(1+(q âˆ’1)(1âˆ’X)nâˆ’j).
ii.
Prove that the n + 1 polynomials Xnâˆ’r(1 + (q âˆ’1)(1 âˆ’X)r) (r = 0, . . . , n âˆ’d),
(1 âˆ’X)wj (j = 1, . . . , d) are linearly independent.
iii.
Prove that the weight enumerator of CâŠ¥is determined.
iv.
Prove that the weight enumerator of C is determined.

71
5
Cyclic Codes
Although it will turn out that cyclic codes are not asymptotically good codes, they are
an important class of codes which include many useful and widely implemented short
length codes, most notably the Golay codes and the general class of BCH codes. BCH
codes have a prescribed minimum distance which means that, by construction, we can
bound from below the minimum distance and therefore guarantee some error-correction
properties. Cyclic codes also provide examples of linear codes with few weights, which
allows us to construct designs via Theorem 4.22. The cyclic structure of these codes will
appear again in â–·Chapter 10, when we consider p-adic codes.
5.1
Basic Properties
A linear code C is called cyclic if, for all (c1, . . . , cn)
âˆˆ
C, the vector
(cn, c1, . . . , cnâˆ’1) âˆˆC.
The map
(c1, . . . , cn) â†’c1 + c2X + Â· Â· Â· + cnXnâˆ’1
is a bijection between the vectors of Fn
q and the polynomials in
Fq[X]/(Xn âˆ’1).
We deï¬ne the weight wt(u) of a polynomial u(X) âˆˆFq[X]/(Xn âˆ’1) of degree less
than n, as the weight of the corresponding vector of Fn
q. In other words, the number of
non-zero coefï¬cients that it has.
An ideal I of a polynomial ring is a subspace with the property that if f âˆˆI, then
Xf âˆˆI.
Â© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_5

5
72
Chapter 5 â€¢ Cyclic Codes
Lemma 5.1 A cyclic code C is mapped by the bijection to an ideal I in Fq[X]/(Xn âˆ’1).
Proof
This is precisely the condition that a linear code satisï¬es to be cyclic.
âŠ“âŠ”
We assume that (n, q) = 1 so that the polynomial Xn âˆ’1 has no repeated factors in
its factorisation, see â–·Section 2.3.
The ring Fq[X]/(Xn âˆ’1) is a principal ideal ring, so I in Lemma 5.1 is a principal
ideal. Hence,
I = âŸ¨gâŸ©= {fg | f âˆˆFq[X]/(Xn âˆ’1)}
for some polynomial g, which is monic and of lowest degree in the ideal.
Therefore, a cyclic code C is mapped by the bijection to âŸ¨gâŸ©. We will from now on
write C = âŸ¨gâŸ©, for some polynomial g.
Lemma 5.2 If C = âŸ¨gâŸ©is a cyclic code of length n, then g divides Xn âˆ’1 and C has
dimension at least n âˆ’deg g.
Proof
If g(X) does not divide Xnâˆ’1, then, using the Euclidean algorithm, we can ï¬nd polynomials
a(X) and b(X) such that
a(X)g(X) + b(X)(Xn âˆ’1)
is equal to the greatest common divisor of g(X) and Xn âˆ’1, which has degree less than g.
This contradicts the property that g has minimal degree in the ideal I. Therefore, g divides
Xn âˆ’1.
The polynomials Xjg, for j
= 0, . . . , n âˆ’deg(g) âˆ’1 are linearly independent
polynomials in âŸ¨gâŸ©, so the dimension of C is at least n âˆ’deg g.
âŠ“âŠ”
In fact, we shall see that the dimension k of C is precisely n âˆ’deg g. This follows
from the following theorem.
Theorem 5.3
Let C = âŸ¨gâŸ©be a cyclic code of length n. The dual code CâŠ¥is the cyclic code âŸ¨â†âˆ’
h âŸ©,
where g(X)h(X) = Xn âˆ’1 and â†âˆ’
h (X) = Xkh(Xâˆ’1).
Proof
Suppose that
g(X) =
nâˆ’k

j=0
gjXj

5.1 Â· Basic Properties
73
5
and
h(X) =
k

i=0
hiXi.
The code âŸ¨gâŸ©contains the row span of the k Ã— n matrix
G =
â›
âœâœâœâœâœâœâœâœâ
g0 . . . gnâˆ’k
0
. . . . . .
0
0 g0
. . . gnâˆ’k 0 . . .
0
0
0
...
. . . ... ...
...
...
...
...
...
0
0 . . . . . .
0
g0 . . . gnâˆ’k
â
âŸâŸâŸâŸâŸâŸâŸâŸâ 
and the code âŸ¨â†âˆ’
h âŸ©contains the row span of the (n âˆ’k) Ã— n matrix
H =
â›
âœâœâœâœâœâœâœâœâ
hk . . . h0
0 . . . . . . 0
0 hk . . . h0 0 . . . 0
0
0 ... . . . ... ... ...
...
... ...
... 0
0 . . . . . . 0 hk . . . h0
â
âŸâŸâŸâŸâŸâŸâŸâŸâ 
.
The scalar product between the s-th row of G and the r-th row of H, where s âˆˆ{1, . . . , k}
and r âˆˆ{1, . . . , n âˆ’k} is
k+r

i=s
giâˆ’shk+râˆ’i,
which is the coefï¬cient of Xk+râˆ’s in gh. Since 1 â©½k + r âˆ’s â©½n âˆ’1, this coefï¬cient is
zero and so GHt = 0.
Since
n = dim C + dim CâŠ¥â©¾rank(G) + rank(H) = n,
(5.1)
the theorem follows.
âŠ“âŠ”
Corollary 5.4 The code C = âŸ¨gâŸ©of length n has dimension n âˆ’deg g.
Proof
Let G and H be as in the previous proof. Equation (5.1) implies that the dimension of C is
the rank of G, which is k.
âŠ“âŠ”

5
74
Chapter 5 â€¢ Cyclic Codes
Example 5.5 (perfect ternary Golay code)
Consider the factorisation of X11 âˆ’1 over F3. As in â–·Section 2.3, we calculate the
cyclotomic subsets of the multiples of 3 modulo 11,
{0}, {1, 3, 9, 5, 4}, {2, 6, 7, 10, 8}.
According to Lemma 2.12, there are two factors of degree 5 which are
(X âˆ’Î±)(X âˆ’Î±3)(X âˆ’Î±9)(X âˆ’Î±5)(X âˆ’Î±4)
and
(X âˆ’Î±2)(X âˆ’Î±6)(X âˆ’Î±7)(X âˆ’Î±10)(X âˆ’Î±8),
where Î± is a primitive 11-th root of unity in F35.
Suppose that
X5 + a4X4 + a3X3 + a2X2 + a1X + a0
is the ï¬rst of these factors. Then a0 = âˆ’Î±22 = âˆ’1. Since the roots of the ï¬rst factor are the
reciprocals of the roots of the second factor, the second factor is
X5 âˆ’a1X4 âˆ’a2X3 âˆ’a3X2 âˆ’a4X âˆ’1.
It is fairly easy to deduce from this that the factorisation is
X11 âˆ’1 = (X âˆ’1)(X5 âˆ’X3 + X2 âˆ’X âˆ’1)(X5 + X4 âˆ’X3 + X2 âˆ’1).
The cyclic code C = âŸ¨X5 âˆ’X3 + X2 âˆ’X âˆ’1âŸ©over F3 is the perfect ternary Golay code of
length 11. To prove that this is a perfect code we need to show that the minimum weight of a
non-zero codeword is 5 (and hence the minimum distance is 5 according to Lemma 4.1) and
observe that

1 + 2
11
1

+ 4
11
2

36 = 311,
so the sphere-packing bound of Theorem 3.9 is attained.
Adding a column of 1â€™s to the generator matrix
â›
âœâœâœâœâœâœâœâœâ
âˆ’1 âˆ’1 1 âˆ’1 0
1
0
0
0 0 0
0 âˆ’1 âˆ’1 1 âˆ’1 0
1
0
0 0 0
0
0 âˆ’1 âˆ’1 1 âˆ’1 0
1
0 0 0
0
0
0 âˆ’1 âˆ’1 1 âˆ’1 0
1 0 0
0
0
0
0 âˆ’1 âˆ’1 1 âˆ’1 0 1 0
0
0
0
0
0 âˆ’1 âˆ’1 1 âˆ’1 0 1
â
âŸâŸâŸâŸâŸâŸâŸâŸâ 

5.2 Â· Quadratic Residue Codes
75
5
we get a generator matrix of a self-dual code C of length 12. This we can check by computing
the scalar product of any two rows and verifying that it is zero (modulo 3). Since this code
is self-dual, the codewords have weights which are multiples of 3. If we can rule out the
possibility that a codeword has weight 3, which we will in â–·Section 5.3, then the minimum
weight of a non-zero codeword of C is 6, which implies that the minimum weight of a non-
zero codeword of the cyclic code âŸ¨X5 âˆ’X3 + X2 âˆ’X âˆ’1âŸ©is 5. Therefore, C is a [11, 6, 5]3
code and C is a [12, 6, 6]3 code.
â– 
5.2
Quadratic Residue Codes
Let n and q be primes for which q is a square in Fn, where we consider the ï¬eld Fn âˆ¼=
Z/nZ to be addition and multiplication modulo n, deï¬ned on the set {0, 1, . . . , n âˆ’1}.
Let Î± be a primitive n-th root of unity in some extension ï¬eld of Fq.
Deï¬ne
g(X) =

(X âˆ’Î±r),
where the product runs over the non-zero squares r in Fn.
Lemma 5.6 The polynomial g(X) divides Xn âˆ’1 in Fq[X].
Proof
Since q is a square in Fn, the map
r â†’qr
is a bijection from the squares of Fn to the squares of Fn, for all non-zero squares r âˆˆFn.
Hence,
g(X) =

(X âˆ’Î±r) =

(X âˆ’Î±rq),
where the product runs over the non-zero squares r in Fn.
Lemma 2.11 implies that g(X) âˆˆFq[X] and note that the roots of g(X) are distinct n-th
roots of 1.
âŠ“âŠ”
Since g(X) is a factor of Xn âˆ’1, we can deï¬ne the cyclic code âŸ¨gâŸ©of length n over
Fq. This code is called the quadratic residue code.
We can obtain evidence that the minimum distance of a quadratic residue code is
quite good from the following theorems.
Theorem 5.7
If u âˆˆâŸ¨gâŸ©and u(1) Ì¸= 0, then wt(u)2 â©¾n.

5
76
Chapter 5 â€¢ Cyclic Codes
Proof
Since u âˆˆâŸ¨gâŸ©, the n-th roots of unity Î±r of Fq, where r is a non-zero square in Fn, are zeros
of u(X).
Let t be a non-square of Fn. The n-th roots of unity Î±s of Fq, where s is a non-square in
Fn, are zeros of u(Xt), since the product of two non-squares is a square. Therefore, all the
n-th roots of unity of Fq, except 1, are zeros of u(X)u(Xt). Hence,
u(X)u(Xt) = (1 + X + Â· Â· Â· + Xnâˆ’1)v(X),
for some polynomial v(X). Since u(1) Ì¸= 0, we have that v(1) Ì¸= 0.
Therefore, in the ring Fq[X]/(Xn âˆ’1),
u(X)u(Xt) = (1 + X + Â· Â· Â· + Xnâˆ’1)v(1),
since v(X) = v(1) + (X âˆ’1)v1(X), for some polynomial v1(X).
Since u(X) has wt(u) terms, this implies that wt(u)2 â©¾n.
âŠ“âŠ”
Theorem 5.8
If n â‰¡âˆ’1 mod 4, u âˆˆâŸ¨gâŸ©and u(1) Ì¸= 0, then wt(u)2 âˆ’wt(u) + 1 â©¾n.
Proof
If n â‰¡âˆ’1 mod 4, then âˆ’1 is a non-square in Fn, since (âˆ’1)(nâˆ’1)/2 = âˆ’1. Therefore, in the
proof of Theorem 5.7, we can take t = âˆ’1. Then,
u(X)u(Xâˆ’1) = (1 + X + Â· Â· Â· + Xnâˆ’1)v(1).
In the product there are at least wt(u) terms of u(X) which multiply with a term of u(Xâˆ’1)
to give a constant term, since XjXâˆ’j = 1. Hence,
wt(u)2 âˆ’wt(u) â©¾n âˆ’1.
âŠ“âŠ”
Example 5.9 (perfect binary Golay code)
Consider the quadratic residue code with n = 23 and q = 2. Let Ïµ be a primitive element of
F211 âˆ¼= F2[X]/(X11 + X2 + 1) and let Î± = Ïµ89. Then Î± is a primitive 23-rd root of unity. By
Lemma 5.6, the factorisation of X23 âˆ’1 in F2[X] has a factor
g(X) =

râˆˆS
(X âˆ’Î±r),
where S = {1, 2, 4, 8, 16, 9, 18, 13, 3, 6, 12} is the set of non-zero squares of F23.

5.2 Â· Quadratic Residue Codes
77
5
If Î±j is a root of g(X), then Î±âˆ’j is not, which implies that
X23 âˆ’1 = (X âˆ’1)g(X)â†âˆ’
g (X).
Solving this polynomial identity we deduce that one of g(X) or â†âˆ’
g (X) is
X11 + X9 + X7 + X6 + X5 + X + 1.
By checking that the sum of the roots of g(X) is zero, we deduce that this polynomial is
g(X).
The quadratic residue code âŸ¨gâŸ©is the perfect binary Golay code of length 23. By
Corollary 5.4, it has dimension 12.
Observe that

1 +
23
1

+
23
2

+
23
3

212 = 223,
so the bound in Theorem 3.9 is attained.
The following matrix is a generator matrix for the code âŸ¨gâŸ©:
â›
âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0
0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0
0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0
0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0
0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0
0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0
0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0
0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0
0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0 0
0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 0
0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0
0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 0 1
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
Adding a column of 1â€™s to this matrix we get a generator matrix for a 12-dimensional linear
code C of length 24. One can verify that all codewords of C have weights which are multiples
of four, see Exercise 5.3. We shall prove in â–·Section 5.3 that the cyclic code âŸ¨gâŸ©has
minimum weight at least 5. Therefore, the minimum weight of a non-zero codeword of C is
8, which implies that the minimum weight of a non-zero codeword of âŸ¨gâŸ©is 7. By Lemma 4.1,
the minimum distance of âŸ¨gâŸ©is 7. Hence, âŸ¨gâŸ©is a [23, 12, 7]2 code and C is a [24, 12, 8]2
code.
â– 

5
78
Chapter 5 â€¢ Cyclic Codes
5.3
BCH Codes
Let Î± be a primitive n-th root of unity in Fqm. BCH codes are a class of cyclic codes in
which we choose Î± so that Î±, Î±2, . . . , Î±d0âˆ’1 are roots of a low degree polynomial g of
Fq[X], for some d0 < n. This allows us to bound the minimum distance of the code âŸ¨gâŸ©.
The lower the degree of g, the larger the dimension (and hence the size) of the code.
Suppose that g(X) âˆˆFq[X] is the polynomial of minimal degree such that
g(Î±j) = 0,
for j = 1, . . . , d0 âˆ’1.
The code âŸ¨gâŸ©is called a BCH code, after Bose, Ray-Chaudhuri and Hocquenghem
who introduced this family of cyclic codes. The parameter d0 is called the prescribed
minimum distance because of the following theorem.
Theorem 5.10
The dimension of the BCH code âŸ¨gâŸ©is at least nâˆ’m(d0 âˆ’1) and its minimum distance
is at least d0.
Proof
Let j âˆˆ{1, . . . , d0 âˆ’1}. By Lemma 2.11, the polynomial
(X âˆ’Î±j)(X âˆ’Î±jq) Â· Â· Â· (X âˆ’Î±jqmâˆ’1)
is in Fq[X]. Clearly, it is zero at Î±j. Since this polynomial has degree m this implies that there
is a polynomial of degree m(d0 âˆ’1) in Fq[X] which is zero at Î±j, for all j = 1, . . . , d0 âˆ’1.
Thus, the degree of g is at most m(d0 âˆ’1) so, by Corollary 5.4, the dimension of âŸ¨gâŸ©is
at least n âˆ’m(d0 âˆ’1).
Suppose that there is an f âˆˆâŸ¨gâŸ©for which wt(f ) is at most d0 âˆ’1. Then
f (X) = b1Xk1 + Â· Â· Â· + bd0âˆ’1Xkd0âˆ’1,
for some k1, . . . , kd0âˆ’1.
Since f âˆˆâŸ¨gâŸ©,
f (Î±j) = 0
for all j = 1, . . . , d0 âˆ’1. Writing this in matrix form these equations are
â›
âœâœâœâ
Î±k1
Î±k2
. . .
Î±kd0âˆ’1
Î±2k1
Î±2k2
. . .
Î±2kd0âˆ’1
.
.
.
.
Î±(d0âˆ’1)k1 Î±(d0âˆ’1)k2 . . . Î±(d0âˆ’1)kd0âˆ’1
â
âŸâŸâŸâ 
â›
âœâœâœâœâœâœâ
b1
b2
.
.
bd0âˆ’1
â
âŸâŸâŸâŸâŸâŸâ 
= 0.

5.3 Â· BCH Codes
79
5
The determinant of the matrix is

iÌ¸=j
(Î±ki âˆ’Î±kj ),
which is non-zero. This implies that the only solution to the above system is f (X) = 0.
Hence, the minimum weight of a non-zero codeword of the cyclic code âŸ¨gâŸ©is at least d0.
The lemma follows since, by Lemma 4.1, the minimum weight of a non-zero codeword of a
linear code is equal to its minimum distance.
âŠ“âŠ”
Example 5.11
Let Î± be a primitive 31-st root of unity in F32. By Lemma 2.12, we obtain the factorisation
of X31 âˆ’1 over F2 by considering the cyclotomy classes
{1, 2, 4, 8, 16}, {3, 6, 12, 24, 17}, {5, 10, 20, 9, 18}, {7, 14, 28, 25, 19},
{11, 22, 13, 26, 21}.
The i-th cyclotomy class gives a polynomial fi(X) in F2[X] which is zero at Î±j for j in the
cyclotomy class. For example,
f1(X) = (X âˆ’Î±)(X âˆ’Î±2)(X âˆ’Î±4)(X âˆ’Î±8)(X âˆ’Î±16)
is in F2[X] and is zero at Î±j for j âˆˆ{1, 2, 4, 8, 16}.
Let
g(X) = f1(X)f2(X)f3(X).
According to Corollary 5.4, the cyclic code âŸ¨gâŸ©is a 16-dimensional linear code.
Since 1, 2, 3, 4, 5 and 6 appear in the ï¬rst three cyclotomic subsets,
g(Î±j) = 0,
for j = 1, . . . , 6. Theorem 5.10 implies that âŸ¨gâŸ©is a [31, 16, â©¾7]2 code. It is in fact a
[31, 16, 7]2 code. Since there exists a [31, 16, 8]2 code, âŸ¨gâŸ©is not an optimal linear code for
this length and dimension.
â– 
Example 5.12 (shortened Reedâ€“Solomon code)
Let Î± be a primitive (q âˆ’1)-st root of unity in Fq. By Theorem 2.4, the polynomial Xqâˆ’1 âˆ’1
factorises into linear factors over Fq. Each cyclotomy class has size 1 and the factors are
fi(X) = X âˆ’Î±i,
for i = 0, . . . , q âˆ’2.

5
80
Chapter 5 â€¢ Cyclic Codes
Let
g(X) = f1(X)f2(X) Â· Â· Â· fdâˆ’1(X).
According to Corollary 5.4, âŸ¨gâŸ©is a (n âˆ’d + 1)-dimensional linear code of length n.
According to Theorem 5.10, âŸ¨gâŸ©has minimum distance at least d. This is an example of
an MDS code, which we will study in more depth in â–·Chapter 6.
â– 
Example 5.13
In Example 5.9, the numbers 1, 2, 3 and 4 appear in the same cyclotomy class, so
Theorem 5.10 implies that the binary Golay code has weight at least 5. As observed in
Example 5.9, this implies that the extended binary Golay code C has no codewords of weight
4, which implies that the minimum distance of C is 8. This, in turn, implies that the minimum
distance of the binary Golay code is 7.
â– 
Example 5.14
Theorem 5.10 generalises in a straightforward way to Exercise 5.5. We can now establish that
the minimum distance of the ternary Golay code is 5. By Exercise 5.5, since 3, 4 and 5 appear
in the same cyclotomy class (and 6, 7 and 8 appear in the same cyclotomy class), the ternary
Golay code in Example 5.5 has minimum distance at least 4. Therefore, the extended code
C has no codewords of weight three, so the weight of a non-zero codeword of the extended
code is either 6, 9 or 12. As observed in Example 5.5, this implies that the minimum distance
of the ternary Golay code is 5.
â– 
The following theorem, which we quote without proof, states that there is no
sequence of asymptotically good BCH codes.
Theorem 5.15
There is no inï¬nite sequence of [n.k, d]q BCH codes for which both Î´ = d/n and
R = k/n are bounded away from zero.
5.4
Comments
The introduction of cyclic codes and quadratic residue codes is widely accredited
to Eugene Prange and Andrew Gleason who proved the automorphism group of an
extended quadratic residue code has a subgroup which is isomorphic to either PSL(2, p)
or SL(2, p), see [12]. The Golay codes were discovered by Golay [27]. The BCH
codes were introduced by Bose and Ray-Chaudhuri in
[13] and independently by
Hocquenghem in [38]. The fact that long BCH codes are asymptotically bad is proven
by Lin and Welden in [47]. The code in Exercise 5.7 is a Zetterberg code, one of a
family of [4m + 1, 4m + 1 âˆ’4m, 5]2 codes.

5.5 Â· Exercises
81
5
5.5
Exercises
5.1 Let C be the extended ternary Golay code from Example 5.5.
i.
Verify that the factorisation of X11 âˆ’1 in F3[X] is as in Example 5.5.
ii.
Prove that the weight enumerator of C is
A(X) = 1 + 264X6 + 440X9 + 24X12.
iii.
Let S be the set of 12 points of PG(5, 3) obtained from the set of columns of a generator
matrix of the code C. Label the points of S by the elements of {1, . . . , 12} and deï¬ne
a set D of 6-subsets to be the points of S which are dependent (i.e. are contained in a
hyperplane of PG(5, 3)). Prove that D is a 5-(12, 6, 1) design.
iv.
Verify that Theorem 4.22 implies that the set of supports of the codewords of weight 6
of C is a 5-(12, 6, 1) design.
5.2 Prove that in Example 5.9 the code âŸ¨â†âˆ’
g âŸ©is equivalent to the code âŸ¨gâŸ©.
5.3
i.
Prove that the extended Golay code over F2, the code C in Example 5.9, is self-dual and
that the weights of the codewords of C are multiples of 4.
ii.
Prove that the weight enumerator of the code C is
A(X) = 1 + 759X8 + 2576X12 + 759X16 + X24.
iii.
Apply Theorem 4.22 to construct a 5-(24, 8, 1) design.
5.4 Investigate the observation that if n â‰¡âˆ’1 modulo 4 and âŸ¨gâŸ©is a quadratic residue code,
then the reverse of the polynomial (Xn âˆ’1)/(X âˆ’1)g(X) is g(X). Does this imply that the
extension of the code âŸ¨gâŸ©is self-dual?
5.5 Suppose that g(X) âˆˆFq[X] is the polynomial of minimal degree such that
g(Î±j) = 0,
for j = â„“+ 1, . . . , â„“+ d0 âˆ’1.
Prove that the dimension of âŸ¨gâŸ©is at least n âˆ’m(d0 âˆ’1) and the minimum distance of
âŸ¨gâŸ©is at least d0.
5.6 Construct the largest possible BCH code with the following parameters.
i.
A binary code of length 15 with minimum distance at least 5.
ii.
A binary code of length 31 with minimum distance at least 11.
iii.
A ternary code of length 13 with minimum distance at least 7.

5
82
Chapter 5 â€¢ Cyclic Codes
Compare the dimension of the codes with the Griesmer bound, the sphere-packing bound and
the Gilbertâ€“Varshamov bound.
5.7
i.
Prove that X17 + 1 factorises in F2[X] as (X + 1)f (X)g(X), where
f (X) = â†âˆ’
f (X) = X8 + X7 + X6 + Â· Â· Â·
and g(X) = â†âˆ’
g (X).
ii.
Construct a [17, 9, 5]2 code.
ii.
Construct a [18, 9, 6]2 code.
5.8
i.
Prove that the polynomial X11 + 1 factorises in F4[X] into two irreducible factors of
degree 5 and one of degree 1.
ii.
Using one of the factors of degree 5, construct a [11, 6, d]4 code C.
iii.
Prove that C is a [11, 6, â©¾4]4 code.
iv.
With the aid of a computer, or not, verify that C is a [11, 6, 5]4 code.
5.9
i.
Prove that the polynomial X17 + 1 factorises in F4[X] into four irreducible factors of
degree 4 and one of degree 1.
ii.
Construct a [17, 9, â©¾7]4 code.
iii.
Let g(X) = X8 + eX7 + X6 + X5 + (1 + e)X4 + X3 + X2 + eX + 1, where e is an
element of F4 such that e2 = e + 1. Prove that g divides X17 + 1.
iv.
Assuming that the code in ii. is âŸ¨gâŸ©, prove that the minimum distance of the code
constructed in ii. is 7.

83
6
Maximum Distance Separable Codes
Two codewords of a block code of length n and minimum distance d must differ on any
set of n âˆ’d + 1 coordinates, since they are at distance at least d from each other. This
observation leads to the Singleton bound, Theorem 6.1. A code whose parameters give
an equality in the Singleton bound is called a maximum distance separable code or
simply an MDS code. Therefore, an MDS code is a block code in which every possible
(n âˆ’d + 1)-tuple of elements of the alphabet occurs in a unique codeword for any set
of n âˆ’d + 1 coordinates. The focus in this chapter will be on linear MDS codes, since
not so much is known about non-linear MDS codes, and there are no known non-linear
MDS codes which outperform linear MDS codes.
The most widely implemented linear MDS codes are the Reedâ€“Solomon codes,
whose codewords are the evaluation of polynomials of low degree. Exploiting this
algebraic structure of Reedâ€“Solomon codes will allow us to develop a fast decoding
algorithm which corrects up to 1
2(1 âˆ’R)n errors, where R is the transmission rate of
the code. For an arbitrary received vector, we can only correct a number of errors less
than half the minimum distance. However, it may be that, although the number of errors
e â©¾1
2d, there is only one codeword that is at distance at most e from the received vector.
We will prove that there is an algorithm, which was discovered only recently, which
creates a relatively short list of possible sent codewords, when up to (1 âˆ’
âˆš
2R)n errors
have occurred. If we are in the afore-mentioned case that there is only one codeword
close to the received vector then the list will contain only one codeword. Moreover, this
list decoding algorithm can be used simultaneously for two codes which will effectively
allow one to decode beyond the bound of half the minimum distance, albeit at a slightly
reduced rate, as explained in â–·Section 3.3.
The fundamental question concerning linear MDS codes asks if there are MDS codes
which better the Reedâ€“Solomon code. The MDS conjecture postulates that no such
codes exist, apart from some known exceptions. The conjecture was recently proved
for codes over ï¬elds of prime order but remains open over ï¬elds of non-prime order.
We will prove that the longest three-dimensional linear MDS codes over a ï¬eld of odd
characteristic are the Reedâ€“Solomon codes (this is not the case for even characteristic
ï¬elds) and delve a little deeper into the proof to see how the tools implemented therein
can be used to prove the MDS conjecture over prime ï¬elds.
Â© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_6

6
84
Chapter 6 â€¢ Maximum Distance Separable Codes
6.1
Singleton Bound
Theorem 6.1 (Singleton bound)
An r-ary code C of length n and minimum distance d satisï¬es |C| â©½rnâˆ’d+1.
Proof
Consider any set of n âˆ’(d âˆ’1) coordinates of a codeword. If two codewords agree on these
coordinates, then their distance is at most d âˆ’1. Hence, they must be different on these
n âˆ’d + 1 coordinates. There are rnâˆ’d+1 distinct (n âˆ’d + 1)-tuples, which gives the bound.
âŠ“âŠ”
The following example is a rather trivial example of a code which meets the
Singleton bound.
Example 6.2
Let A be an abelian group with r elements. Deï¬ne
C = {(a1, . . . , anâˆ’1, a1 + Â· Â· Â· + anâˆ’1) | ai âˆˆA}.
If ai = bi for all but one i, then a1 + Â· Â· Â· + anâˆ’1 Ì¸= b1 + Â· Â· Â· + bnâˆ’1. Hence, the minimum
distance of C is 2. Since |C| = rnâˆ’1, it is an MDS code.
â– 
Theorem 6.3
If there is a [n, k, d]q code, then k â©½n âˆ’d + 1 and a [n, k, d]q code is an MDS code
if and only if k = n âˆ’d + 1.
Proof
For a [n, k, d]q code, Theorem 6.1 implies that qk â©½qnâˆ’d+1.
âŠ“âŠ”
6.2
Reedâ€“Solomon Code
The Reedâ€“Solomon code is the classical example of a linear MDS code. It is an example
of an evaluation code. An evaluation code is a code whose codewords are the evaluation
of certain functions. In the case of a Reedâ€“Solomon code the functions are given by low
degree uni-variate polynomials. In â–·Chapter 7 and â–·Chapter 9, we will see other
examples of evaluation codes.

6.2 Â· Reedâ€“Solomon Code
85
6
Example 6.4
Let {a1, . . . , aq} be the set of elements of Fq. The Reedâ€“Solomon code is
C = {(f (a1), . . . , f (aq), cf ) | f âˆˆFq[X], deg(f ) â©½k âˆ’1},
where cf is the coefï¬cient of Xkâˆ’1 in f .
â– 
Note that the coordinate given by cf can be interpreted as the evaluation of f at âˆ.
By homogenising the polynomial f (X), we get a homogeneous polynomial
h(X, Y) = Y kf (X/Y)
of degree k. The evaluation of f at x is h(x, 1) and h(1, 0) = cf .
Lemma 6.5 The Reedâ€“Solomon code C in Example 6.4 is a [q + 1, k, q + 2 âˆ’k]q linear
MDS code.
Proof
We will ï¬rst prove that C is linear.
Let f and g be two polynomials of degree at most k âˆ’1. Then f + g is a polynomial of
degree at most k âˆ’1 and the coefï¬cient of Xkâˆ’1 in f + g is the sum of the coefï¬cients of
Xkâˆ’1 in f and g. Hence, if u, v âˆˆC, then u + v âˆˆC.
Let Î» âˆˆFq. Then Î»f is a polynomial of degree at most k âˆ’1 and the coefï¬cient of Xkâˆ’1
in Î»f is Î» times the coefï¬cient of Xkâˆ’1 in f . Hence, if u âˆˆC, then Î»u âˆˆC.
Therefore, C is a k-dimensional linear code of length n = q + 1.
It follows from the fact that a non-zero polynomial of degree at most k âˆ’1 has at most
k âˆ’1 zeros that the weight of a codeword for which cf Ì¸= 0 is at least n âˆ’(k âˆ’1). If cf = 0
and f Ì¸= 0, then the polynomial f has degree at most k âˆ’2 and so the codeword has at most
k âˆ’1 zeros. Therefore, the non-zero codewords of C have weight at least n âˆ’k + 1. Thus,
by Lemma 4.1, the code C has minimum distance at least n âˆ’k + 1. Then, by the Singleton
bound from Theorem 6.1, C has minimum distance n âˆ’k + 1. Hence, C is an MDS code.
âŠ“âŠ”
To construct a generator matrix (gij) for the Reedâ€“Solomon code, we choose
k linearly independent polynomials f1(X), . . . , fk(X) of degree at most k âˆ’1 and
index the rows with these polynomials. Then we index the columns with the elements
a1, . . . , aq of Fq. The entry gij = fi(aj) for j â©½q and the gi,q+1 entry is the coefï¬cient
of Xkâˆ’1 in the polynomial fi(X).
For example, with fi(X) = Xiâˆ’1 the matrix
(gij) =
â›
âœâœâœâœâœâ
1
1
. . .
1
0
a1
a2
. . . aq
0
.
.
. . .
.
.
akâˆ’2
1
akâˆ’2
2
. . . akâˆ’2
q
0
akâˆ’1
1
akâˆ’1
2
. . . akâˆ’1
q
1
â
âŸâŸâŸâŸâŸâ 

6
86
Chapter 6 â€¢ Maximum Distance Separable Codes
is a generator matrix for the Reedâ€“Solomon code.
What makes Reedâ€“Solomon codes so attractive for implementation is the availability
of fast decoding algorithms. In the following theorem, we prove that there is a decoding
algorithm that will correct up to t = âŒŠ(d âˆ’1)/2âŒ‹errors, where d is the minimum
distance.
Although it is not really necessary, to make the proof of the following theorem
easier, we shall only use the shortened Reedâ€“Solomon code in which we delete the last
coordinate. In this way every coordinate of a codeword is the evaluation of a polynomial
at an element of Fq.
Theorem 6.6
There is a decoding algorithm for a k-dimensional shortened Reedâ€“Solomon code of
length n, which corrects up to 1
2(nâˆ’k) errors and completes in a number of operations
which is polynomial in n.
Proof
Suppose that we have received the vector (y1, . . . , yn). We want to ï¬nd the polynomial f âˆˆ
Fq[X] of degree at most k âˆ’1 such that
(y1, . . . , yn) = (f (a1), . . . , f (an)) + e,
where e is the error vector of weight at most 1
2(n âˆ’k).
Observe that since the Reedâ€“Solomon code is an MDS code,
1
2(n âˆ’k) = 1
2(d âˆ’1).
Let h(X) be an arbitrary polynomial of degree âŒŠ1
2(n âˆ’k)âŒ‹and let g(X) be an arbitrary
polynomial of degree k + âŒˆ1
2(n âˆ’k)âŒ‰âˆ’1.
We determine the coefï¬cients of g and h by solving the system of n equations,
g(aj) âˆ’h(aj)yj = 0,
for j = 1, . . . , n. This homogeneous linear system has
âŒŠ1
2(n âˆ’k)âŒ‹+ 1 + k + âŒˆ1
2(n âˆ’k)âŒ‰= n + 1
unknowns (the coefï¬cients of g and h) and n equations. Hence, we can ï¬nd a non-trivial
solution for h(X) and g(X) in a number of operations that is polynomial in n using Gaussian
elimination.
By assumption, there is a polynomial f of degree at most k âˆ’1, such that yj = f (aj)
for at least n âˆ’âŒŠ1
2(n âˆ’k)âŒ‹values of j. For these values of j, aj is a zero of

6.2 Â· Reedâ€“Solomon Code
87
6
g(X) âˆ’h(X)f (X).
The degree of this polynomial is at most k + âŒˆ1
2(n âˆ’k)âŒ‰âˆ’1. Since
n âˆ’âŒŠ1
2(n âˆ’k)âŒ‹> k + âŒˆ1
2(n âˆ’k)âŒ‰âˆ’1
the polynomial g(X) âˆ’h(X)f (X) has more zeros than its degree, so it is identically zero.
Therefore, h(X) divides g(X) and the quotient is f (X).
âŠ“âŠ”
In the following example we apply the algorithm in Theorem 6.6 to a concrete case.
Example 6.7
Suppose that we have sent a codeword u of the 2-dimensional shortened Reedâ€“Solomon code
over F7 = {0, 1, 2, 3, 4, 5, 6} (ordering the elements of F7 in that order) and that we have
received
y = (1, 0, 0, 0, 6, 2, 5).
According to the algorithm in the proof of Theorem 6.6, we should ï¬nd a polynomial g(X)
of degree 4 and a polynomial h(X) of degree 2, such that
g(aj) = h(aj)yj,
for j = 1, . . . , 7 and where aj is the j-th element of F7.
The equations are
g(0) = h(0), g(1) = 0, g(2) = 0, g(3) = 0, g(4) = 6h(4),
g(5) = 2h(5), and g(6) = 5h(6).
From this we deduce that
g(X) = (X âˆ’1)(X âˆ’2)(X âˆ’3)(g1X + g0)
and
h(X) = h2X2 + h1X + h0,
for some h2, h1, h0, g1, g0 âˆˆF7, which are solutions of the system
g0 = h0, 6(4g1 + g0) = 6(2h2 + 4h1 + h0)
3(5g1 + g0) = 2(4h2 + 2h1 + h0), 4(6g1 + g0) = 5(h2 + 6h1 + h0).
This system of equations has a solution g1 = 0, g0 = 3, h2 = 1, h1 = 3 and h0 = 3.

6
88
Chapter 6 â€¢ Maximum Distance Separable Codes
If less than âŒŠ1
2(nâˆ’k)âŒ‹= 2 errors have occurred, then the codeword u is the evaluation of
f (X) = g(X)
h(X) = 3(X âˆ’1)(X âˆ’2)(X âˆ’3)
X2 + 3X + 3
= 3X + 1.
Evaluating the polynomial f , we deduce that
u = (1, 4, 0, 3, 6, 2, 5).
â– 
Theorem 6.6 allows us to deduce the sent vector providing at most 1
2n(1 âˆ’R) errors
occur in transmission. Recall, that the transmission rate of a k-dimensional linear code
of length n is R = k/n. We interpreted Exercise 3.12 v. as saying that (at least for a
binary code) the number of codewords at a distance at most 1
2n(1âˆ’âˆš1 âˆ’2(d/n)) from
a ï¬xed vector is less than 2n. If we consider the ï¬xed vector as the received vector, then
this implies that it should be feasible to construct a short list of possible sent codewords,
even if the number of errors which have occurred exceeds half the minimum distance. A
decoding algorithm which produces such a short list is called a list decoding algorithm.
It may be that although the received codeword is more than half the minimum distance
away from a codeword, it is near to only one codeword. In such a case the list decoding
algorithm may allow us to correct the errors. In other words, we may be able to decode
uniquely even when more than 1
2(d âˆ’1) errors have occurred. And if we encode the
message with two distinct codes, as we saw in â–·Section 3.3 this can be done so that
the rate is not reduced by much, then with a high probability the intersection of the two
lists will be the sent codeword.
We can list decode in a simple way by using standard array decoding. We make
a table whose rows are indexed by the error vectors and whose columns are indexed by
the codewords and whose entry is found by summing the error vector and the codeword.
To decode a received vector v, one searches through the table entries for v making a list
of the codewords u for which v appears in the column indexed by u. If there is a unique
entry in the list, then v can be uniquely decoded. One downside to this algorithm is that
for any code of reasonable size, the table requires a large amount of storage space.
The following example illustrates the main idea behind the algorithm presented in
Theorem 6.9, which is a list decoding algorithm for Reedâ€“Solomon codes.
Example 6.8
Suppose that we have sent a codeword of the shortened 4-dimensional Reedâ€“Solomon code
over F13, where the elements of F13 are ordered as
{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}.
The unique decoding algorithm in Theorem 6.6 allows us to correct up to âŒŠ1
2(d âˆ’1)âŒ‹= 4
errors. Suppose that 5 errors have occurred and that we have received
y = (4, 11, 0, 3, 0, 1, 0, 0, 0, 0, 0, 3, 12).

6.2 Â· Reedâ€“Solomon Code
89
6
Let
Q(X, Y) = cY 2 + g(X)Y + h(X),
where c âˆˆF13,
g(X) = g4X4 + g3X3 + g2X2 + g1X + g0
is an arbitrary polynomial of degree at most 4 and h(X) is an arbitrary polynomial of degree
at most 7.
We ï¬nd g(X) and h(X), and hence Q(X, Y), by solving the set of equations,
Q(xj, yj) = 0,
for j = 1, . . . , 13, where xj is the j-th element of F13 and yj is the j-th coordinate of y.
There are 14 unknowns in this homogeneous linear system of equations, the coefï¬cients of
g and h and the constant c, and 13 equations. Hence, there is a non-trivial solution to this
system of equations.
For xj âˆˆ{2, 4, 6, 7, 8, 9, 10}, the equation Q(xj, yj) = 0 implies h(xj) = 0, since
yj = 0 for j âˆˆ{3, 5, 7, 8, 9, 10, 11}.
Thus,
h(X) = a(X âˆ’2)(X âˆ’4)(X âˆ’6)(X âˆ’7)(X âˆ’8)(X âˆ’9)(X âˆ’10)
for some a âˆˆF13.
The remaining equations imply,
4c+11(g0+g1+g2+g3+g4)+10a = 0, 9c+3(g0+3g1+9g2+g3+3g4)+11a = 0,
c+g0+5g1+12g2+8g3+g4+4a = 0, 9c+3(g0+11g1+4g2+5g3+3g4)+7a = 0,
and
c + 12(g0 + 12g1 + g2 + 12g3 + g4) + 10a = 0.
Up to scalar factor, the solution of this system implies
Q(X, Y) = 3Y 2 + (12X4 + 5X3 + 8X2 + 7X + 5)Y
+(X âˆ’2)(X âˆ’4)(X âˆ’6)(X âˆ’7)(X âˆ’8)(X âˆ’9)(X âˆ’10).
Suppose that f (X) is the polynomial of degree at most 3, whose evaluation is the sent
codeword u. Then Q(X, f (X)) is a polynomial of degree at most 7, which is zero whenever
yj = f (xj). This occurs whenever yj = uj and no error has occurred in the j-th coordinate.
Assuming that at most 5 errors have occurred, y and u agree in at least 8 coordinates. Hence,

6
90
Chapter 6 â€¢ Maximum Distance Separable Codes
Q(X, f (X)) has at least 8 zeros. Therefore, Q(X, f (X)) â‰¡0 which implies Y âˆ’f (X)
divides Q(X, Y).
Indeed, Q(X, Y) factorises as
Q(X, Y) = (Y âˆ’X3 âˆ’X2 âˆ’5X âˆ’4)(3Y âˆ’X4 + 8X3 + 11X2 + 9X + 4).
Since f (X) is a polynomial of degree at most 3, it must be that
f (X) = X3 + X2 + 5X + 4.
The evaluation of f (X) is
u = (4, 11, 0, 3, 0, 10, 2, 9, 1, 10, 3, 12),
which is at distance 5 from the received vector y.
â– 
The following theorem follows the same idea as Example 6.8 and provides an
algorithm which outputs a short list of possibilities for the sent codeword having
received a vector in which up to approximately n(1 âˆ’
âˆš
2R) errors have occurred. As
with the algorithm described in Theorem 6.6, this algorithm also completes in a number
of operations which is polynomial in n.
Theorem 6.9
There is a decoding algorithm for a k-dimensional shortened Reedâ€“Solomon code of
length n, that completes in a number of operations which is polynomial in n and which
outputs a list of less than âˆš2n/(k âˆ’1) codewords, one of which is the sent vector,
provided less than n âˆ’
âˆš
2nk errors have occurred in transmission.
Proof
Let m = âŒˆâˆš2n/(k âˆ’1)âŒ‰âˆ’1 and deï¬ne a bi-variate polynomial
Q(X, Y) =
m

i=0
âŒˆ1
2 kâŒ‰+i(kâˆ’1)

j=0
qijXjY mâˆ’i,
where the coefï¬cients are to be determined. Since
m

i=0
(âŒˆ1
2kâŒ‰+ i(k âˆ’1)) = âŒˆ1
2kâŒ‰(m + 1) + 1
2m(m + 1)(k âˆ’1)
> 1
2(m + 1)2(k âˆ’1) â©¾n,
the polynomial Q(X, Y) has more than n coefï¬cients.
Let (y1, . . . , yn) be the received vector.

6.3 Â· Linear MDS Codes
91
6
The coordinates of a codeword of the shortened Reedâ€“Solomon code are indexed by
{x1, . . . , xn}, the set of elements of Fq.
We make a homogeneous system of â„“equations, where for each â„“âˆˆ{1, . . . , n}, we have
the equation
Q(xâ„“, yâ„“) = 0.
Since we have more than n unknowns, this homogeneous system has a non-trivial solution.
And we can ï¬nd a solution, using Gaussian elimination, in a number of operations which is
polynomial in n.
Let g(X) be a polynomial of degree at most k âˆ’1. Then the uni-variate polynomial
Q(X, g(X)) has degree at most
âŒˆ1
2kâŒ‰+ m(k âˆ’1) <

2n(k âˆ’1) <
âˆš
2nk.
By hypothesis, less than n âˆ’
âˆš
2nk errors have occurred in transmission, so there is a
polynomial f (X), of degree at most k âˆ’1, for which yâ„“= f (xâ„“) for more than
âˆš
2nk values
of â„“. Therefore, Q(X, f (X)) â‰¡0, since a non-zero polynomial cannot have more zeros than
its degree.
We can write
Q(X, Y) = (Y âˆ’f (X))C(X, Y) + R(X)
for some polynomials C(X, Y) and R(X) and conclude, substituting Y
= f (X), that
R(X) â‰¡0.
Hence, Y âˆ’f (X) divides Q(X, Y). The bi-variate polynomial Q(X, Y) can be factorised
in a number of operation that is polynomial in its degree.
Thus, if f (X) is the polynomial which the sent codeword is the evaluation of, then Y âˆ’
f (X) is a factor of Q(X, Y). Since the degree in Y of Q(X, Y) is at most m, there are at most
m possibilities for the sent codeword.
âŠ“âŠ”
6.3
Linear MDS Codes
In this section we consider the general class of linear MDS codes. For the Reedâ€“
Solomon code, the minimum distance d = q +2âˆ’k. However, for a hypothetical linear
MDS code, the trivial upper bound, given by Exercise 6.6, is d â©½q. Thus, the trivial
bound does not rule out the possibility that there are linear MDS codes which are much
better than Reedâ€“Solomon codes. We will prove that linear MDS codes are equivalent to
a certain geometric object and prove that, in the case that q is odd, a three-dimensional
linear MDS code of length q + 1 is a Reedâ€“Solomon code. This proof contains all the
ingredients needed to prove that there are no linear MDS codes over ï¬elds of prime
order better than the Reedâ€“Solomon codes. The non-prime case remains open and, in
part, is a more difï¬cult problem since there are examples of linear MDS codes of length

6
92
Chapter 6 â€¢ Maximum Distance Separable Codes
q + 1 which are not equivalent to Reedâ€“Solomon codes. These appear in Exercise 6.9,
Exercise 6.10 and Exercise 6.11.
Theorem 6.10
G is a generator matrix of a linear MDS code if and only if every subset of k columns
of G is a basis of Fk
q.
Proof
Let S be the multi-set of columns of the matrix G.
Suppose that G is the generator matrix of a linear MDS code. By Lemma 4.15, a
hyperplane of Fk
q contains at most n âˆ’d = k âˆ’1 vectors of S. This implies that S is a
set and not a multi-set and that any k-subset of S is a basis of Fk
q.
Suppose that every k-subset of S is a basis of Fk
q. Then uG has at most k âˆ’1 zeros for
any non-zero u âˆˆFk
q. Therefore, the non-zero codewords of the code C generated by G have
weight at least n âˆ’k + 1. By Lemma 4.1, the minimum weight of a non-zero codeword is
equal to the minimum distance, so C has minimum distance at least n âˆ’k + 1. Theorem 6.1
implies that the minimum distance of C is n âˆ’k + 1 and so C is MDS.
âŠ“âŠ”
Theorem 6.11
The dual of a linear MDS code is a linear MDS code.
Proof
Let C be a k-dimensional linear MDS code of length n and let S = {s1, . . . , sn} be the set of
columns of a generator matrix for C. The dual code CâŠ¥is a (nâˆ’k)-dimensional linear code.
Suppose (u1, . . . , un) âˆˆCâŠ¥is a non-zero codeword of weight at most k. Since
u1s1 + Â· Â· Â· + unsn = 0,
this implies that there is a linear combination of at most k of the vectors of S which are
linearly dependent, contradicting Theorem 6.10. Therefore, the minimum non-zero weight
of CâŠ¥is k + 1. Lemma 4.1 implies that CâŠ¥has minimum distance at least k + 1, which
implies that CâŠ¥is MDS since
n âˆ’(n âˆ’k) + 1 = k + 1.
âŠ“âŠ”
An arc is a set S of vectors of Fk
q with the property that every subset of S of size k
is a set of linearly independent vectors, i.e. is a basis of Fk
q. Putting the vectors of an arc

6.3 Â· Linear MDS Codes
93
6
of size n as the columns of a k Ã— n matrix, one obtains a generator matrix of a linear
MDS code of length n. Vice versa, the set of columns of a generator matrix of a linear
MDS code is an arc of Fk
q of size n. One can also consider the arc as a set of points in
the projective space PG(k âˆ’1, q), since the linear independence property is unchanged
if we take non-zero scalar multiples of the vectors of S. Thus, an arc in PG(k âˆ’1, q) is
a set S of points of PG(k âˆ’1, q) with the property that every subset of size k spans the
whole space.
Theorem 6.12
If k â©¾q, then a k-dimensional linear MDS code over Fq has minimum distance at
most 2.
Proof
Suppose that the minimum distance of a linear MDS code C of length n is at least 3. By
Theorem 6.3, n = d + k âˆ’1 â©¾k + 2.
By Theorem 6.10, the set S of columns of a generator matrix of C is an arc of Fk
q.
Order the vectors in S arbitrarily and let
Bâ€² = {e1, . . . , ek}
be the ï¬rst k vectors of S.
Consider the coordinates of the (k + 1)-st and the (k + 2)-nd vector of S with respect
to the basis Bâ€². Suppose there is a zero in the i-th coordinate of one of them. Then the
hyperplane, deï¬ned by Xi = 0, contains k vectors of S (since it contains k âˆ’1 vectors of
Bâ€²), contradicting the arc property.
Therefore, we can ï¬nd Î»j, non-zero elements of Fq, such that the (k + 1)-st vector in S
is the all-one vector with respect to the basis
B = {Î»1e1, . . . , Î»kek}.
Let (s1, . . . , sk) be coordinates of the (k + 2)-nd vector of S with respect to the basis B. As
observed above si Ì¸= 0, for all i = 1, . . . , k. Since k > q âˆ’1, the pigeon-hole principle
implies there exists an i and j such that si = sj. Therefore, the hyperplane deï¬ned by the
equation Xi = Xj contains k vectors of S, again contradicting the arc property.
âŠ“âŠ”
We can obtain a binary code from a linear code over Fq by identifying each element
of Fq with a binary string of length âŒˆlog2 qâŒ‰. In this way, if we take a k-dimensional
linear code of length N over Fq, then we get a binary code of length n = NâŒˆlog2 qâŒ‰
with qk elements. The rate of the binary code is log2 |C|/n â‰ˆk/N and the relative
minimum distance is approximately d/(N log2 q).
From a linear MDS code we obtain a binary code with a transmission rate R of
approximately k/N, whose relative minimum distance is approximately

6
94
Chapter 6 â€¢ Maximum Distance Separable Codes
âŠ¡Fig. 6.1 A burst of three errors distorting only one Fq bit.
1 âˆ’R
log2 q + 1
n.
If k â©¾q, then according to Theorem 6.12 the minimum distance is at most 2, so we
assume k < q. But then Exercise 6.6 implies N < 2q âˆ’1 which implies that n <
2q log2 q. If we want n to tend to inï¬nity, we must have q going to inï¬nity, which implies
that if the rate is bounded away from zero, then the relative minimum distance will tend
to zero. Hence, we cannot hope to obtain asymptotically good codes in this way. This
does not mean that MDS codes and in particular Reedâ€“Solomon codes are not used
in practice. Although as a binary code, a Reedâ€“Solomon code can only guarantee the
correction of a relatively small amount of errors, in certain circumstances it can correct
many more errors.
Consider âŠ¡Figure 6.1. Each element of Fq is mapped to a string of âŒˆlog2 qâŒ‰binary
bits. If all the errors in the transmission of the binary string occur in the same run of
bits, then when we look to decode the Fq-linear code, only one bit has been distorted.
This is particularly useful for channels in which errors tend to come in bursts. For this
reason Reedâ€“Solomon codes, and more generally linear codes over large ï¬elds, are very
useful for burst-error correction. Furthermore, a common method in the application of
Reedâ€“Solomon codes is to simultaneously use two codes of high rate. For example, in
a CD the data is encoded using two Reedâ€“Solomon codes over F64 which are cross-
interleaved. Add to that the fast decoding algorithms which allow us to decode past the
half the minimum distance bound with high probability and one has a fast and efï¬cient
means of error-correction.
6.4
MDS Conjecture
Theorem 6.12 implies that we should only look for linear MDS codes of dimension
k â©½q âˆ’1. Theorem 6.11 implies that a k-dimensional MDS code of length n exists
if and only if a (n âˆ’k)-dimensional MDS code of length n exists. By Exercise 6.6, a
2-dimensional MDS code has length at most q + 1 and a 3-dimensional MDS code has
length at most q + 2. By Theorem 6.11, the dual of a (q âˆ’1)-dimensional MDS code
of length q + 2 is a 3-dimensional MDS code of length q + 2, which by Exercise 6.7
does not exist for q odd and, by Exercise 6.9, does exist for q even. From this we can
deduce the length of the longest linear MDS codes for all dimensions not in the range

6.4 Â· MDS Conjecture
95
6
4 â©½k â©½q âˆ’2. The MDS conjecture asserts that, within this range, one cannot do
better than the Reedâ€“Solomon code.
Conjecture 6.13 (MDS conjecture) If 4 â©½k â©½q âˆ’2, then a k-dimensional linear MDS
code of length n satisï¬es n â©½q + 1.
The MDS conjecture has been veriï¬ed for q prime. It follows from the following
theorem, whose proof we will sketch at the end of this section.
Theorem 6.14
Let q = ph, where p is prime. If k â©½p, then a k-dimensional linear MDS code of
length n satisï¬es n â©½q + 1.
We will prove the following theorem only in the case k = 3. In this case the
hypothesis k Ì¸=
1
2(q + 1) is not necessary. The hypothesis k â©½p is necessary. The
following example is not equivalent to a Reedâ€“Solomon code.
Example 6.15
Let F32 = {a1, . . . , a32} and let C be the three-dimensional linear code over F32 generated by
the matrix G whose i-the column is (1, ai, a4
i )t, for i = 1, . . . , 32, and whose 33-rd column
is (0, 0, 1)t. A generic 3 Ã— 3 submatrix of G is of the form
â›
âœâ
1
1 1
x
y
z
x4 y4 z4
â
âŸâ .
The determinant of this matrix is
(z âˆ’x)4(y âˆ’x) âˆ’(y âˆ’x)4(z âˆ’x)
If this determinant is zero, then ((z âˆ’x)/(y âˆ’x))3 = 1. Since the ï¬eld F32 contains no
element e Ì¸= 1 such that e3 = 1, this implies that z = y. In this way we see that all 3 Ã— 3
submatrices of G are non-singular and, by Theorem 6.10, C is an MDS code.
â– 

6
96
Chapter 6 â€¢ Maximum Distance Separable Codes
Theorem 6.16
Let q = ph, where p is prime. If k â©½p and k Ì¸= 1
2(q +1), then a k-dimensional linear
MDS code of length q + 1 is a Reedâ€“Solomon code.
Proof (for k = 3)
Let S be the set of columns of a generator matrix G of a 3-dimensional linear MDS code C
of length q + 1. By Theorem 6.10, a hyperplane of F3
q contains at most two vectors of S.
Let s âˆˆS. There are q + 1 hyperplanes containing s, q of which contain a vector of
S \ {s}. Hence, there is exactly one hyperplane of F3
q which contains s and no other vector
of S.
Let fs(X) be a linear form whose kernel is this hyperplane.
Let x, y, z âˆˆS. We claim that
fx(y)fy(z)fz(x) = fy(x)fz(y)fx(z).
With respect to the basis B = {x, y, z}, the hyperplane that contains s = (s1, s2, s3) and
x = (1, 0, 0) is the kernel of the linear form
X2 âˆ’(s2/s3)X3.
Note that s3 Ì¸= 0 since the hyperplane ker X3 is incident with x and y.
The linear form
fx(X) = a2X2 + a3X3,
for some a2, a3 âˆˆFq, since the kernel of fx(X) contains x.
For distinct s âˆˆS \ {x, y, z}, the value of s2/s3 is distinct, since the linear form
X2 âˆ’(s2/s3)X3
is different, for different s âˆˆS \ {x, y, z}.
Since,
X2 âˆ’(s2/s3)X3 Ì¸= X2 + (a3/a2)X3,
for any s âˆˆS \ {x, y, z}, we have that
{s2
s3
| s âˆˆS \ {x, y, z}} âˆª{âˆ’a3
a2
}
is the set of all non-zero elements of Fq.
In âŠ¡Figure 6.2, the vectors x, y and z are drawn as points in PG(2, q) and the
hyperplanes deï¬ned as the kernels of fx, fy and fz are lines in the plane.
Note that a2 = fx(y) and a3 = fx(z). The product of all the non-zero elements of Fq is
âˆ’1, so we have that

6.4 Â· MDS Conjecture
97
6
âŠ¡Fig. 6.2 The lines joining the basis points to s and the tangent lines.
âˆ’fx(z)
fx(y)

sâˆˆS\B
s2
s3
= âˆ’1.
Similarly,
fy(x)
fy(z)

sâˆˆS\B
s3
s1
= 1 and
fz(y)
fz(x)

sâˆˆS\B
s1
s2
= 1.
Multiplying the three equations together establishes the claim.
For any s âˆˆS, the linear form fs(X) is determined by its value at the basis elements, so
fs(X) = fs(x)X1 + fs(y)X2 + fs(z)X3.
Evaluating at X = s, and using the fact that fs(s) = 0,
fs(x)s1 + fs(y)s2 + fs(z)s3 = 0.
By the claim,
s1 + fy(s)fx(y)
fx(s)fy(x)s2 + fz(s)fx(z)
fx(s)fz(x)s3 = 0.
Now, substituting
fx(s) = fx(y)s2 + fx(z)s3,
fy(s) = fy(x)s1 + fy(z)s3,
fz(s) = fz(x)s1 + fz(y)s2,
we get

6
98
Chapter 6 â€¢ Maximum Distance Separable Codes
2(c3s1s2 + c2s1s3 + c1s2s3) = 0,
for some c1, c2, c3 âˆˆFq \ {0}.
Explicitly,
c1 = fy(z)fx(y)
fy(x), c2 = fx(z), and c3 = fx(y).
Since we are assuming that q is odd, the vectors of S are zeros of the quadratic form
c3X1X2 + c2X1X3 + c1X2X3.
The zeros of this quadratic form excluding (0, 0, 1) are parameterisable by
s = (t, c2
c2
1
(c3 âˆ’c1t), t
c1
(c1t âˆ’c3)).
Therefore, the i-th coordinate of a column of G is the evaluation of the polynomial fi(X),
where
f1(X) = X, f2(X) = âˆ’c2câˆ’1
1 X + câˆ’2
1 c2c3
and
f3(X) = X2 âˆ’câˆ’1
1 c3X,
with the exception of the column (0, 0, 1), whose i-th coordinate is the coefï¬cient of X2 of
fi(X).
Hence, the codeword (u1, u2, u3)G is the evaluation of the polynomial
u1f1(X) + u2f2(X) + u3f3(X),
so C is a Reedâ€“Solomon code.
âŠ“âŠ”
The proof of Theorem 6.14 and the general proof of Theorem 6.16 follow the same
strategy as the proof of Theorem 6.16 given here for k = 3.
Let S be the set of columns of a generator matrix of a k-dimensional linear MDS
code over Fq. Let Ï„ = q + k âˆ’1 âˆ’|S|.
Let A be a (k âˆ’2)-subset of S. There are q + 1 hyperplanes which contain the
(k âˆ’2)-dimensional subspace of Fk
q spanned by the points of A, Ï„ of which contain no
other vectors of S. With linear forms Î±1, . . . , Î±Ï„, whose kernels are these Ï„ hyperplanes,
we deï¬ne a polynomial
fA(X) =
Ï„
i=1
Î±i(X).

6.4 Â· MDS Conjecture
99
6
This deï¬nes fA(X) uniquely up to scalar factor. The claim is then that
fDâˆª{x}(y)fDâˆª{y}(z)fDâˆª{z}(x) = (âˆ’1)Ï„+1fDâˆª{y}(x)fDâˆª{z}(y)fDâˆª{x}(z),
for all k-subsets D âˆª{x, y, z} of S.
To simplify matters slightly, let us assume that Ï„ is odd. Since the polynomials
fA(X) are deï¬ned up to scalar factor, we can scale them in such a way that the above
equality gives
fDâˆª{x}(y) = fDâˆª{y}(x).
This implies that for C, a (k âˆ’1)-subset of S, there is a non-zero aC âˆˆFq, such that for
all e âˆˆC,
fC\{e}(e) = aC.
The interpolation of the linear form fs(X) in the proof for k = 3 is generalised to
interpolating the polynomial fA(X) of degree Ï„. Even though fA(X) is a homogeneous
polynomial in k variables, since it is the product of linear forms whose kernels contain
the subspace spanned by A = {a1, . . . , akâˆ’2}, with respect to a basis of Fk
q containing
A, fA(X) is a homogeneous polynomial in two variables. Hence, it can be interpolated.
To be able to interpolate we ï¬x a subset E of S of size k + Ï„ and an element x âˆˆE.
Writing det(X, u, A) as a shorthand for
            
X1
X2
. . .
Xk
u1
u2
. . .
uk
a11
a12
. . .
a1k
...
...
. . .
...
akâˆ’2,1 akâˆ’2,2 . . . akâˆ’2,k
            
the interpolation implies
fA(X) =

eâˆˆE\(Aâˆª{x})
fA(e)

uâˆˆE\(Aâˆª{x,e})
det(X, u, A)
det(e, u, A) .
One can check that substituting X = e gives fA(e) on the right-hand side for all e âˆˆ
E \ {x}.
Substituting X = x and rearranging terms leads to the equation

eâˆˆE\A
fA(e)

uâˆˆE\(Aâˆª{e})
det(u, e, A)âˆ’1 = 0.

6
100
Chapter 6 â€¢ Maximum Distance Separable Codes
This implies that for a (k âˆ’2)-subset A of E,

CâŠƒA
aC

uâˆˆE\C
det(u, C)âˆ’1 = 0,
where the sum runs over the (k âˆ’1)-subsets C of E containing A.
Thus, we get an equation for each (k âˆ’2)-subset A of E. Setting
Î»C = aC

uâˆˆE\C
det(u, C)âˆ’1,
the equation is simply

CâŠƒA
Î»C = 0,
where the sum runs over the (k âˆ’1)-subsets C of E containing A.
This set of equations is enough to prove both Theorem 6.14 and, with a little more
work, Theorem 6.16. To prove Theorem 6.14, we assume that the length of the MDS
code is q + 2 and so |S| = q + 2. We can assume by Theorem 6.11 that k â©½(q + 2)/2,
taking the dual code if necessary. Since |S| = q + 2 we have that Ï„ = k âˆ’3 and
|E| = 2k âˆ’3. There are N =
2kâˆ’3
kâˆ’2

linear equations. For each (k âˆ’1)-subset C of
E, we have an unknown Î»C, so in all we have
2kâˆ’3
kâˆ’1

unknowns. Thus, we have a linear
system of N equations in N unknowns. This system of equations implies the equation
(k âˆ’1)!Î»C = 0,
which is a contradiction if k â©½p, since Î»C Ì¸= 0.
6.5
Comments
The Singleton bound appears in Singletonâ€™s paper [66] on MDS codes from 1964, the
Reedâ€“Solomon codes having already been published some years before in [60]. In
the same paper the authors detail the decoding algorithm presented in Theorem 6.6.
An algorithm based on interpolation was proposed by Berlekamp and Welch [9]. As
mentioned in the text, the list decoding of Theorem 6.9, from Sudan [68], decodes up
to (1 âˆ’
âˆš
2R)n errors and produces a list with a constant number of codewords. This
bound improves on the unique decoding bound of 1
2(1 âˆ’R)n when R < 3 âˆ’2
âˆš
2. The
bound can be improved to (1 âˆ’
âˆš
R)n by interpolating the zeros with multiplicity, see
Guruswami and Sudan [33]. The bound (1 âˆ’
âˆš
R)n is larger than the unique decoding
bound for all R. There is a limit of (1 âˆ’R)n errors, beyond which one cannot hope to
produce a list with a constant number of codewords, see Guruswami and Rudra [32].
Indeed, one can ï¬nd received vectors for which the number of codewords of the Reedâ€“
Solomon code, at a distance of (1 âˆ’R)n or less, is not bounded by a polynomial in n.

6.6 Â· Exercises
101
6
One can use a Reedâ€“Solomon code, or more generally an MDS code, in concate-
nation codes to produce binary codes which come arbitrarily close to the Gilbertâ€“
Varshamov bound asymptotically. This construction is due to Thommesen
[70].
Suppose we have an MDS code of length N and rate R over a ï¬eld with 2k elements.
A codeword is a vector v = (v1, . . . , vN) âˆˆFN
2k. For each coordinate, we randomly
choose Gi, a k Ã—n matrix with entries from F2. We consider each coordinate vi of v as a
vector in Fk
2. Recall that, in â–·Chapter 2, we constructed the elements of F2k as elements
of F2[X]/(f ), where f (X) is an irreducible polynomial of degree k in F2[X]. If
vi = a0 + a1X + Â· Â· Â· + akâˆ’1Xkâˆ’1
in this quotient ring, then we consider vi as the vector (a0, a1, . . . , akâˆ’1). We then make
a binary code of length nN by taking as its elements the strings
u = (v1G1, v2G2, . . . , vNGN).
With a high probability this binary code will be arbitrarily close to the Gilbertâ€“
Varshamov bound.
For more on applications of Reedâ€“Solomon codes, in particular the simultaneous
use of two Reedâ€“Solomon codes in compact discs, see [76].
MDS codes are widely used in distributed storage systems to protect data from server
failures. A codeword vG of a k-dimensional linear MDS code can be recovered from
just k coordinates, since k of the coordinates uniquely determine v and therefore the
codeword. This recoverability property is the important feature of local reconstruction
codes, of which MDS codes are a special sub-class. A local reconstruction code is a
block code in which for each coordinate i, there is a subset of the coordinates Ri, such
that knowing the coordinates in Ri of a codeword, one can recover the i-th coordinate of
the codeword. It is this more general class of codes which are implemented in distributed
storage systems.
The MDS conjecture appears in [50] although it origins can be traced back to
the fundamental questions asked by Segre [64] in 1967. Proofs of Theorem 6.14 and
Theorem 6.16 can be found in Ball [5], although the original proof of Theorem 6.16 for
k = 3 is due to Segre [63].
The MDS code in Exercise 6.10 was discovered by Glynn [26], and the MDS code
in Exercise 6.11 is due to Segre [64].
6.6
Exercises
6.1 Prove that the dual of a Reedâ€“Solomon code is a Reedâ€“Solomon code.
6.2 Shortening the k-dimensional Reedâ€“Solomon code over Fq, by removing the last column
and the column which is the evaluation of the polynomial at zero in Example 6.4, we get a
k-dimensional linear code C of length q âˆ’1. Prove that C is the cyclic code in Example 5.12.

6
102
Chapter 6 â€¢ Maximum Distance Separable Codes
6.3 Let C be the 4-dimensional shortened Reedâ€“Solomon code of length 7 over F7 which is
the evaluation of polynomials of degree at most three, where the elements of F7 are ordered as
{0, 1, 2, 3, 4, 5, 6}. Decode the received vector (1, 1, 0, 4, 2, 2, 1) using the algorithm from
Theorem 6.6.
6.4 Suppose that E is the set of coordinates where an error has occurred in transmission.
Prove that in the set of equations in Theorem 6.6, h(ai) = 0 for all i âˆˆE.
6.5 Let C be the linear code from Exercise 4.8. Prove that CâŠ¥is an MDS code and that
therefore C is an MDS code.
6.6 Prove that if there exists an [n, k, n âˆ’k + 1]q code, then n â©½q + k âˆ’1.
6.7 Let S be the set of columns of a generator matrix of a 3-dimensional linear MDS code
of length n, considered as a set of points of PG(2, q).
i.
Prove that S is a set of n points, no three of which are collinear.
ii.
Prove that if q is odd then a 3-dimensional linear MDS code has length at most q + 1.
iii.
Prove that if q is even, then a 3-dimensional linear MDS code of length q + 1 is
extendable to a 3-dimensional linear MDS code of length q + 2.
6.8 Let S be the set of columns of a generator matrix of a 4-dimensional linear MDS code
of length n, considered as a set of points of PG(3, q).
i.
Prove that S is a set of n points, no 4 of which are contained in a hyperplane.
ii.
Prove that if |S| = q + 2 and q is even, then to each point x of S there is a line â„“x,
incident with x, with the property that each plane containing â„“x contains exactly one
point of S \ {x}.
iii.
Prove that there are no 4-dimensional linear MDS codes of length q + 3.
6.9 Show that if e and h are co-prime, then the matrix whose columns are {(1, t, t2e) | t âˆˆ
Fq} âˆª{(0, 1, 0), (0, 0, 1)} generates a 3-dimensional linear MDS of length q + 2 over Fq,
q = 2h.
6.10 Show that the matrix whose columns are {(1, t, t2 + Î·t6, t3, t4) | t
âˆˆ
F9} âˆª
{(0, 0, 0, 0, 1)}, where Î·4 = âˆ’1, generates a 5-dimensional MDS of length 10 over F9.
6.11 Suppose q is even and let Ïƒ be an automorphism of Fq. Let
M =
â›
âœâœâœâ
eÏƒ+1 eÏƒ b ebÏƒ bÏƒ+1
eÏƒ c
eÏƒ d bÏƒ c bÏƒ d
cÏƒ e
cÏƒ b dÏƒ e dÏƒ b
cÏƒ+1 cÏƒ d cdÏƒ dÏƒ+1
â
âŸâŸâŸâ .
Let A be the 4 Ã— 4 matrix whose i-th column is the transpose of (1, ti, tÏƒ
i , tÏƒ+1
i
).

6.6 Â· Exercises
103
6
i.
Verify that if e, b, c, d are chosen so that c+dt1 = 0, e+bt2 = 0 and e+bt3 = c+dt3,
then
det MA = ((e + bt1)(c + dt2)(e + bt3))Ïƒ+1 det
â›
âœâœâœâ
1 0 1 (e + bt4)Ïƒ+1
0 0 1 (e + bt4)Ïƒ (c + dt4)
0 0 1 (c + dt4)Ïƒ (e + bt4)
0 1 1 (c + dt4)Ïƒ+1
â
âŸâŸâŸâ .
ii.
Prove that if there is no non-zero element a âˆˆFq for which aÏƒ = a, then the code
generated by the matrix G, whose columns are the transpose of (1, t, tÏƒ , tÏƒ+1), for each
t âˆˆFq, is a 4-dimensional linear MDS code of length q.
iii.
Prove that by adding the column (0, 0, 0, 1)t to G, the code generated by the matrix
extends the 4-dimensional linear MDS code of length q to a 4-dimensional linear MDS
code of length q + 1.

105
7
Alternant and Algebraic Geometric
Codes
Alternant codes are subï¬eld subcodes of a generalised Reedâ€“Solomon code over an
extension ï¬eld of Fq. This is a large class of linear codes which includes BCH codes,
one of the families of cyclic codes which appeared in â–·Chapter 5. Although BCH codes
are not asymptotically good, we will prove that there are asymptotically good alternant
codes. Not only are alternant codes linear, and so easy to encode, they also have an
algebraic structure which can be exploited in decoding algorithms. However, as with
the codes constructed in Theorem 3.7, the construction of these asymptotically good
alternant codes is probabilistic. We prove that such a code must exist without giving an
explicit construction.
Algebraic geometric codes are codes constructed from algebraic curves. We shall
cover the basic properties of these codes and prove that algebraic geometric codes can
provide examples of asymptotically good codes. In the case of r-ary codes, where r is
the square of an odd prime larger than or equal to 7, there are algebraic geometric codes
whose rate and relative minimum distance exceed the Gilbertâ€“Varshamov bound.
7.1
Subfield Subcodes
Let C be a linear code over Fqh of length n.
The subï¬eld subcode A(C) of C is a code over Fq, deï¬ned as the set of codewords
of C all of whose coordinates are elements of Fq.
Lemma 7.1 If C is a [n, kâ€², d]qh code, then A(C) is a [n, k, â©¾d]q code, where kâ€² â©¾k â©¾
n âˆ’(n âˆ’kâ€²)h.
Â© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_7

7
106
Chapter 7 â€¢ Alternant and Algebraic Geometric Codes
Proof
Suppose u, v âˆˆA(C). Then u, v âˆˆC and since C is linear u + v âˆˆC. Since u + v has
coordinates in Fq, u + v âˆˆA(C). Similarly Î»u âˆˆA(C), for all Î» âˆˆFq, so A(C) is linear
over Fq.
Let H be a (n âˆ’kâ€²) Ã— n check matrix for C. Then (a1, . . . , an) âˆˆA(C) if and only if
a1x1 + Â· Â· Â· anxn = 0,
for all rows (x1, . . . , xn) of H.
As we saw in â–·Chapter 2, the elements xi of Fqh are polynomials in the ring Fq[X]/(f ),
where f is an irreducible polynomial of degree h in Fq[X]. Writing the equation above over
Fq[X] gives at most h constraints (j = 1, . . . , h) on A(C) of the form
a1x1j + Â· Â· Â· + anxnj = 0,
where xij âˆˆFq is deï¬ned by
xi = xi1 + xi2X + Â· Â· Â· + xihXhâˆ’1.
Therefore, a check matrix for A(C) has rank at most (n âˆ’kâ€²)h, which implies that the
dimension of A(C) is at least n âˆ’(n âˆ’kâ€²)h.
âŠ“âŠ”
Example 7.2
Let e be a primitive element of F9 which satisï¬es e2 = e + 1.
Consider the check matrix
H =

1 1 . . . 1 1 0
e e2 . . . e8 0 1

of a [10, 8, 3]9 code C.
Writing out the elements of H as a + be, where a, b âˆˆF3, we get a check matrix
â›
âœâœâœâ
1 1 1 1 1 1 1 1 1 0
0 0 0 0 0 0 0 0 0 0
0 1 1 2 0 2 2 1 0 1
1 1 2 0 2 2 1 0 0 0
â
âŸâŸâŸâ 
for A(C) over F3. The rank of this matrix is 3, so A(C) has dimension 7. A non-zero
codeword of C has weight at least 3, so it follows that a non-zero codeword of A(C) has
weight at least 3. Thus, A(C) is a [10, 7, â©¾3]3 code and since (0, 0, 0, 0, 0, 0, 1, 2, 2) âˆˆ
A(C), A(C) is a [10, 7, 3]3 code.
â– 

7.2 Â· Generalised Reedâ€“Solomon Codes
107
7
7.2
Generalised Reedâ€“Solomon Codes
Let x1, . . . , xn be distinct elements of Fqh and let v1, . . . , vn be non-zero elements
of Fqh.
The linear code over Fqh deï¬ned by
C = {(v1f (x1), . . . , vnf (xn)) | f âˆˆFqh[X], deg f â©½kâ€² âˆ’1},
where kâ€² < qh, is a generalised Reedâ€“Solomon (GRS) code.
The subï¬eld subcode A(C) is an alternant code if C is a GRS code. Alternant codes
are useful because they allow us to construct linear codes over small ï¬elds (including
binary codes). Not only can they have good parameters, the algebraic structure, inherent
in their construction, can be exploited to develop reasonably fast decoding algorithms.
Example 7.3
Let e be a primitive element of F8, such that e3 = e + 1 and let C be the GRS code
{(f (0), f (1), f (e), e6f (e2), f (e3), e4f (e4), e5f (e5), e3f (e6)) |f âˆˆF8[X],
deg f â©½5}.
The matrix

1 1 1 e 1 e3 e2 e4
0 1 e e3 e3 1 1 e3

is a check matrix for C. This can readily checked. The scalar product of the ï¬rst and second
rows of the matrix with a codeword of C is

xâˆˆF8
f (x) and

xâˆˆF8
xf (x)
respectively. For a polynomial f of degree at most 5 these sums are zero, see Exercise 2.8.
As in Example 7.2, we write out the elements of H as a + be + ce2, where a, b, c âˆˆF2.
In this way, we get a check matrix
H =
â›
âœâœâœâœâœâœâœâœâ
1 1 1 0 1 1 0 0
0 0 0 1 0 1 0 1
0 0 0 0 0 0 1 1
0 1 0 1 1 1 1 1
0 0 1 1 1 0 0 1
0 0 0 0 0 0 0 0
â
âŸâŸâŸâŸâŸâŸâŸâŸâ 
for A(C). The matrix H has rank 5, so the dimension of A(C) is 3.
By solving the system of equations Hut = 0, where u âˆˆF8
2, we can ï¬nd a basis for
the code A(C) and therefore a generator matrix G for A(C). One can readily check that
GHt = 0, where

7
108
Chapter 7 â€¢ Alternant and Algebraic Geometric Codes
G =
â›
âœâ
0 0 1 1 0 1 0 0
1 1 1 0 1 0 0 0
1 1 0 1 0 0 1 1
â
âŸâ .
The seven non-zero codewords of A(C) have weight at least 3. By Lemma 4.1, the minimum
distance of A(C) is 3, since this is minimum weight of a non-zero codeword of A(C).
Therefore, A(C) is a [8, 3, 3]2 code.
â– 
Lemma 7.4 If C is a kâ€²-dimensional GRS code of length n over Fqh, then the minimum
distance of A(C) is at least n âˆ’kâ€² + 1.
Proof
As for Reedâ€“Solomon codes, since a non-zero polynomial of degree at most kâ€²âˆ’1 has at most
kâ€² âˆ’1 zeros, a non-zero codeword of the code C has weight at least n âˆ’(kâ€² âˆ’1). Therefore,
the weight of any non-zero codeword of A(C) is at least n âˆ’kâ€² + 1. By Lemma 4.1, the
minimum weight of a non-zero codeword is equal to the minimum distance.
âŠ“âŠ”
The following lemma is Lagrange interpolation, which will be used to prove
Lemma 7.6.
Lemma 7.5 Suppose that a1, . . . , ak are distinct elements of Fq and let b1, . . . , bk be
elements of Fq. There is a unique polynomial f âˆˆFq[X] of degree at most k âˆ’1 such
that f (ai) = bi, for all i = 1, . . . , k.
Proof
For each i = 1, . . . , k, the equality f (ai) = bi is a constraint on the polynomial f (X) =
kâˆ’1
i=1 ciXi,
c0 + c1ai + Â· Â· Â· + ckâˆ’1akâˆ’1
i
= bi.
In matrix form this system of equations is
â›
âœâœâœâ
1 a1 . . . akâˆ’1
1
. . . . .
.
. . . . .
.
1 ak . . . akâˆ’1
k
â
âŸâŸâŸâ 
â›
âœâœâœâ
c0
.
.
ckâˆ’1
â
âŸâŸâŸâ =
â›
âœâœâœâ
b1
.
.
bk
â
âŸâŸâŸâ .
Since the matrix is a Vandermonde matrix and the aiâ€™s are distinct, the matrix has non-zero
determinant and so the system of equations has a unique solution.
âŠ“âŠ”
In Theorem 7.7, we are going to prove that there are alternant codes which do not
have any non-zero codewords of small weight. We do this by counting, bounding by
above the number of alternant codes containing vectors of small weight d. Then, by
proving there are more alternant codes than alternant codes containing vectors of weight
less than d, we conclude that there are alternant codes with minimum distance at least

7.3 Â· Alternant Codes Meeting the Gilbertâ€“Varshamov Bound
109
7
d. Firstly, we bound the number of alternant codes containing a ï¬xed non-zero vector
of Fn
q.
Lemma 7.6 Let a be a non-zero vector of Fn
q. The number of kâ€²-dimensional GRS codes of
length n over Fqh containing a, for a ï¬xed n-tuple (x1, . . . , xn) of distinct elements of Fqh,
is at most (qh âˆ’1)kâ€².
Proof
If there is a kâ€²-dimensional GRS codes of length n over Fqh containing a, then a has at most
kâ€² âˆ’1 zero coordinates. After a suitable permutation of the coordinates, assume that all the
zero coordinates of a are contained in the ï¬rst kâ€² âˆ’1 coordinates. Choose v1, . . . , vkâ€² âˆˆ
Fqh \ {0}.
So that a is contained in the GRS code with this choice of v1, . . . , vkâ€², there must be a
polynomial f of degree at most kâ€² âˆ’1 such that
f (xi) = ai
vi
,
for i = 1, . . . , kâ€². By Lemma 7.5, there is a unique polynomial f with this property.
For j = kâ€²+1, . . . , n, if f (xj) = 0, then a is not contained in a GRS code for this choice
of v1, . . . , vkâ€², since the zeros of a all occur in the ï¬rst kâ€² âˆ’1 coordinates. Hence, f (xj) Ì¸= 0
and the elements vj are ï¬xed by
vj =
aj
f (xj).
Moreover, vj Ì¸= 0 since aj Ì¸= 0 for these values of j.
âŠ“âŠ”
7.3
Alternant Codes Meeting the Gilbertâ€“Varshamov Bound
We now prove that there are alternant codes of rate R and minimum distance d whose
parameters approximate to the parameters of a code on the Gilbertâ€“Varshamov curve of
âŠ¡Figure 3.1.
Theorem 7.7
There are asymptotically good alternant codes of rate R âˆˆQ meeting the Gilbertâ€“
Varshamov bound.
Proof
Choose h and n so that Rn is an integer, h divides n âˆ’Rn and
kâ€² = n âˆ’n(1 âˆ’R)
h
< qh.

7
110
Chapter 7 â€¢ Alternant and Algebraic Geometric Codes
By Lemma 7.6, for a ï¬xed x1, . . . , xn, the number of kâ€²-dimensional GRS codes over Fqh of
length n containing the vectors of Fn
q of weight at most d âˆ’1 is at most
(qh âˆ’1)kâ€² dâˆ’1

j=0
(q âˆ’1)j
n
j

.
Since we can choose v1, . . . , vn âˆˆFqh \ {0}, the total number of kâ€²-dimensional GRS codes
for a ï¬xed x1, . . . , xn is (qh âˆ’1)n.
Therefore, if
(qh âˆ’1)kâ€² dâˆ’1

j=0
(q âˆ’1)j
n
j

< (qh âˆ’1)n,
then there is a kâ€²-dimensional GRS code C with no non-zero codewords of weight less than d.
By Lemma 7.1, the alternant code A(C) has dimension k â©¾n âˆ’(n âˆ’kâ€²)h = Rn, so the
rate of A(C) is at least R. Substituting kâ€² = n âˆ’n(1 âˆ’R)/h, the condition is that
(qh âˆ’1)Rn/h
dâˆ’1

j=0
(q âˆ’1)j
n
j

< (qh âˆ’1)n/h,
which is (asymptotically) the Gilbertâ€“Varshamov bound, since (qh âˆ’1)r/h â‰ˆqr.
âŠ“âŠ”
The following theorem says that we can decode received vectors with few errors
quickly. Observe that the minimum distance d, for the alternant code which we get from
Theorem 7.7, is at least n âˆ’kâ€² + 1 by Lemma 7.4.
Theorem 7.8
Let C be a kâ€²-dimensional GRS code of length n. If the number of errors that occur in
the transmission of a codeword of the alternant code A(C) is at most 1
2(n âˆ’kâ€²), then
there exists a polynomial time decoding algorithm which corrects the errors.
Proof
This is similar to the proof of Theorem 6.6.
We suppose g is an arbitrary polynomial of degree âŒˆ1
2(n + kâ€²)âŒ‰âˆ’1 and that h is an
arbitrary polynomial of degree âŒŠ1
2(n âˆ’kâ€²)âŒ‹.
We determine the coefï¬cients of g and h by solving the system of n equations,
g(aj) âˆ’h(aj)vâˆ’1
j yj = 0,
for j = 1, . . . , n. The homogeneous linear system has n + 1 unknowns (the coefï¬cients of g
and h) and n equations. Hence, we can ï¬nd a non-trivial solution (in polynomial time) which
determines h(X) and g(X).

7.3 Â· Alternant Codes Meeting the Gilbertâ€“Varshamov Bound
111
7
By assumption, there is a polynomial f of degree at most kâ€² âˆ’1, such that yj = vjf (aj)
for at least n âˆ’âŒŠ1
2(n âˆ’kâ€²)âŒ‹values of j. For these values of j, the evaluation at aj of
g(X) âˆ’h(X)f (X)
is zero. The degree of this polynomial is at most âŒˆ1
2(n + kâ€²)âŒ‰âˆ’1. Since
n âˆ’âŒŠ1
2(n âˆ’kâ€²)âŒ‹> âŒˆ1
2(n + kâ€²)âŒ‰âˆ’1,
it has more zeros than its degree, so it is identically zero. Therefore, h(X) divides g(X) and
the quotient is f (X) = g(X)/h(X).
âŠ“âŠ”
Example 7.9
Suppose that we are using the code A(C), where C is deï¬ned as in Example 7.3, and that we
have received the vector
y = (0, 0, 1, 0, 1, 0, 1, 1).
According to Theorem 7.8, we should solve the equations
vjg(aj) âˆ’h(aj)yj = 0,
where j = 1, . . . , 8, and where g(X) is a polynomial of degree at most 6 and
h(X) = h1X + h0.
The equations are
g(0) = 0, g(1) = 0, g(e) = h(e), g(e2) = 0, g(e3) = h(e3), g(e4) = 0,
g(e5)e5 = h(e3), g(e6)e3 = h(e6).
From these equations we have that
g(X) = X(X + 1)(X + e2)(X + e4)(g2X2 + g1X + g0)
for some g0, g1 and g2 and that
â›
âœâœâœâ
e5 e4 e3 e 1
1 e4 e e3 1
e6 e e3 e5 1
e5 e6 1 e6 1
â
âŸâŸâŸâ 
â›
âœâœâœâœâœâœâ
g2
g1
g0
h1
h0
â
âŸâŸâŸâŸâŸâŸâ 
= 0.

7
112
Chapter 7 â€¢ Alternant and Algebraic Geometric Codes
This homogeneous system of equations has a solution with g2 = 1 which gives
g(X) = X(X + 1)(X + e2)(X + e4)(X2 + eX + e2)
and
h(X) = eX + e3.
Therefore, by Theorem 7.8, the sent codeword is the evaluation of
g(X)
h(X) = e6X(X + 1)(X + e4)(X2 + eX + e2).
One can check that the evaluation of this polynomial is the codeword
(0, 0, 1, 1, 1, 0, 1, 1).
â– 
We can use the list decoding algorithm of Theorem 6.9 to correct more distorted
codewords. To do this we make a slight modiï¬cation of the interpolation and solve the
system of equations
Q(xâ„“, vâˆ’1
â„“yâ„“) = 0.
This will produce a short list of possibilities for the sent codeword. The list of
possibilities is further reduced since we can discard any vectors in the list which are
not codewords of A(C).
7.4
Algebraic Geometric Codes
Let Ï† be an absolutely irreducible homogeneous polynomial in Fq[X, Y, Z] of degree
m. Recall that absolutely irreducible means that Ï† is irreducible over Fq, an algebraic
closure of Fq.
Let Ï‡ be the plane curve, deï¬ned as the points where Ï† is zero, where the points are
points of the projective plane deï¬ned over Fq. It is not necessary to take Ï‡ to be a plane
curve, but it will make things simpler and more apparent if we assume for the moment
that it is. In the next section, we will consider an example of a higher dimensional curve.
We direct the reader to the comments section for references to a more general treatment
of algebraic geometric codes. Although we will deï¬ne our codes over Fq, we have to
consider the curve over Fq. It is essential that when we apply Bezoutâ€™s theorem, the
number of points in the intersection of a curve deï¬ned by a homogeneous polynomial g
and a curve deï¬ned by a homogeneous polynomial h is (deg g)(deg h), where we have
to count the intersections with multiplicity.

7.4 Â· Algebraic Geometric Codes
113
7
The coordinate ring is deï¬ned as the quotient ring
Fq[Ï‡] = Fq[X, Y, Z]/(Ï†).
Therefore, the elements of Fq[Ï‡] are residue classes which can be represented by
polynomials. We will only be interested in residue classes represented by homogeneous
polynomials. We will be particularly interested in the elements of
Fq(Ï‡) = {f | f = g/h, for some homogeneous g, h âˆˆFq[Ï‡] of the same degree}.
An element f of Fq(Ï‡) can have very different representations.
Example 7.10
Let Ï‡ be the curve deï¬ned by X3 = Y 2Z. The element f of Fq(Ï‡) represented by X2/Y 2 is
the same as the element of Fq(Ï‡) represented by Z/X.
â– 
The elements of Fq(Ï‡) do not in general deï¬ne functions on the curve Ï‡, since there
may be a point P of Ï‡ where h is zero. However, there may be another representation
of the same element of Fq(Ï‡), where h is not zero, so the evaluation of f is deï¬ned.
As in â–·Section 2.4, we denote by (x : y : z) the point of PG(2, q) with vector
representative (x, y, z).
Example 7.11
In Example 7.10, f is deï¬ned at the point P = (0 : 1 : 0), even though in the representation
Z/X we have a zero in the denominator. Indeed, using the representation X2/Y 2, we deduce
that f has a zero at P . However, f is not deï¬ned at the point (0 : 0 : 1), where it has a pole.
â– 
The elements of Fq are representatives of elements of Fq(Ï‡), since they are
polynomials of degree 0 divided by a polynomial of degree 0. These elements deï¬ne
a constant function on the curve Ï‡.
We deï¬ne a divisor as a ï¬nite sum of the points of Ï† with integer coefï¬cients,
D =

PâˆˆÏ‡
nP P.
At ï¬rst glance, this seems like an odd thing to deï¬ne. But it helps us keep track of the
zeros and poles of an element of Fq(Ï‡) and this will be of utmost importance to us.
Assume that Ï‡ is a non-singular curve.
We deï¬ne the divisor of f âˆˆFq(Ï‡) to be the divisor, denoted (f ), where we sum
the zeros of g intersect Ï†, counted with multiplicity, and subtract the zeros of h intersect
Ï†, counted with multiplicity. This deï¬nition of (f ) is well-deï¬ned in that it does not
depend on the representatives g and h for f = g/h.

7
114
Chapter 7 â€¢ Alternant and Algebraic Geometric Codes
The degree of a divisor D is
deg(D) =

PâˆˆÏ‡
nP .
Bezoutâ€™s theorem, as mentioned before, ensures that the degree of the divisor (f ), for
any f âˆˆFq(Ï‡), is zero.
Example 7.12
Let us calculate the divisor of f , from Example 7.11, using the representative Z/X. The
curve deï¬ned by the equation Z = 0 and the curve deï¬ned by X3 = Y 2Z intersect in the
point P2 = (0 : 1 : 0) with multiplicity three. The curve deï¬ned by X = 0 and the curve
deï¬ned by X3 = Y 2Z intersect in the point P2 = (0 : 1 : 0) and the point P3 = (0 : 0 : 1)
with multiplicity two. Hence,
(f ) = 3P2 âˆ’2P3 âˆ’P2 = 2P2 âˆ’2P3.
If we had used the representative X2/Y 2, we would have arrived at the same conclusion. â– 
Let
D =

PâˆˆÏ‡
nP P.
We write D â©¾0 if and only if the coefï¬cients nP â©¾0 for all P âˆˆÏ‡.
Observe that the coefï¬cients of (f ) are positive and negative unless f is represented
by a polynomial of degree zero. Hence, (f ) â©¾0 if and only if f = c, for some c âˆˆFq.
If P is a point of Ï‡ whose coordinates are all in Fq, then we say that P is an Fq-
rational point of Ï‡.
Suppose that D is a divisor in which the sum is restricted to Fq-rational points. It is
straightforward to verify that the subset
L(D) = {f âˆˆFq(Ï‡) | (f ) + D â©¾0}
is a vector space over Fq. Moreover, its dimension can be calculated from the Riemannâ€“
Roch theorem. Precisely, if deg(D) â©¾2g âˆ’1, then
dim L(D) = deg(D) âˆ’g + 1,
where g is the genus of the curve Ï‡. Recall that for a non-singular plane curve the genus
of Ï‡ is (m âˆ’1)(m âˆ’2)/2, where m is the degree of Ï†.
Observe that if deg D < 0, then L(D) = {0}.

7.4 Â· Algebraic Geometric Codes
115
7
We are now in a position to prove that the code whose codewords are the evaluation
of the functions in L(D) at certain points of the curve Ï‡, will be a linear code over Fq,
whose dimension we know and whose minimum distance we can bound from below. In
other words, we have a prescribed minimum distance as we did for BCH codes.
Theorem 7.13
Suppose
D =

P âˆˆÏ‡
nP P,
where the sum is over the Fq-rational points of Ï‡.
Let {P1, . . . , Pn} be a set of n Fq-rational points of Ï‡ for which nPj = 0, for all
j âˆˆ{1, . . . , n}.
If n > deg(D) â©¾2g âˆ’1, then
C(Ï‡, D) = {(f (P1), . . . , f (Pn)) | f âˆˆL(D)}
is a
[n, deg(D) âˆ’g + 1, â©¾n âˆ’deg(D)]q
code.
Proof
Let Î± : L(D) â†’Fn
q be deï¬ned by
Î±(f ) = (f (P1), . . . , f (Pn)).
Since L(D) is a vector space, this deï¬nes a linear map.
Let E = P1 +Â· Â· Â·+Pn. If f âˆˆker Î±, then f âˆˆL(D âˆ’E), since f âˆˆL(D) and f is zero
at P1, . . . , Pn. Since deg(D âˆ’E) < 0, this implies f = 0. Therefore, the image of Î± has
dimension dim L(D), which is deg(D) âˆ’g + 1, by the Riemannâ€“Roch theorem mentioned
above.
Suppose Î±(f ) has weight w > 0. Then, after a suitable reordering of the points, we can
assume
f (P1) = Â· Â· Â· = f (Pnâˆ’w) = 0.
Since f Ì¸= 0 and f âˆˆL(D âˆ’P1 âˆ’Â· Â· Â· âˆ’Pnâˆ’w), this implies that
deg(D âˆ’P1 âˆ’Â· Â· Â· âˆ’Pnâˆ’w) â©¾0,

7
116
Chapter 7 â€¢ Alternant and Algebraic Geometric Codes
which gives deg(D) â©¾n âˆ’w. Therefore, the minimum weight of a non-zero codeword of C
is at least n âˆ’deg(D) which, by Lemma 4.1, implies that the minimum distance of the linear
code C(Ï‡, D) is at least n âˆ’deg(D).
âŠ“âŠ”
Example 7.14
Let Ï‡ be the curve of genus 1 deï¬ned as the zeros of the polynomial
X3 + Y 2Z + Z2Y âˆˆF4[X, Y, Z].
The line X = 0 intersects Ï‡ in the points Q = (0 : 1 : 0), P = (0 : 0 : 1) and R = (0 : 1 : 1).
The line Y = 0 intersects Ï‡ in the point P with multiplicity 3 and the line Z = 0
intersects Ï‡ in the point Q with multiplicity 3.
Suppose D = 3Q. Then, since
(X
Z ) = P + R âˆ’2Q
and
( Y
Z ) = 3P âˆ’3Q
and
dim L(D) = deg(D) âˆ’g + 1 = 3,
we have that
{X
Z , Y
Z , 1}
is a basis for L(D). A generator matrix for C(Ï‡, 3Q) is given by
G =
â›
âœâ
0 0 1 1 e e e2 e2
0 1 e e2 e e2 e e2
1 1 1 1 1 1 1 1
â
âŸâ ,
where e âˆˆF4 and e2 = e + 1. In this case we get a generator matrix whose columns are the
eight F4-rational points of Ï‡ not equal to Q.
By Theorem 7.13, the minimum distance of C(Ï‡, 3Q) is at least 5. The codeword
(1, 1, 1)G has weight 5, so C(Ï‡, 3Q) is an [8, 3, 5]4 code.
Now, suppose D = 5Q. By Theorem 7.13 the dimension of C(Ï‡, 5Q) is 5. To ï¬nd a
generator matrix for C(Ï‡, 5Q), we can extend the basis for L(3Q) to L(5Q) by observing
that

7.5 Â· Algebraic Geometric Codes Surpassing the Gilbertâ€“Varshamov Bound
117
7
(X2
Z2 ) = 2P + 2R âˆ’4Q
and
(XY
Z2 ) = 4P + R âˆ’5Q.
Therefore,
{X
Z , Y
Z , X2
Z2 , XY
Z2 , 1}
is a basis for L(5Q).
Thus, C(Ï‡, 5Q) has a generator matrix
Gâ€² =
â›
âœâœâœâœâœâœâ
0 0 1 1 e e e2 e2
0 1 e e2 e e2 e e2
0 0 1 1 e2 e2 e e
0 0 e e2 e2 1 1 e
1 1 1 1 1 1 1 1
â
âŸâŸâŸâŸâŸâŸâ 
.
According to Theorem 7.13, the minimum distance of C(Ï‡, 5Q) is at least nâˆ’deg(5Q) = 3.
The codeword (1, 1, 1, 1, 0)Gâ€² has weight 3, so C(Ï‡, 5Q) has minimum distance equal to 3.
Therefore, C(Ï‡, 5Q) is an [8, 5, 3]4 code.
â– 
7.5
Algebraic Geometric Codes Surpassing the
Gilbertâ€“Varshamov Bound
Algebraic geometric codes are of particular interest because they can provide examples
of asymptotically good codes which better the Gilbertâ€“Varshamov bound for certain
large enough alphabets. We ï¬rst calculate the asymptotic Gilbertâ€“Varshamov bound for
a general alphabet of size r, as we did in Corollary 3.8 for binary codes.
Supposing that the codes have rate R and relative minimum distance Î´, the bound in
Theorem 3.7 gives
 n
Î´n

(r âˆ’1)Î´nrnR > rn,
which by Lemma 1.11 gives
2h(Î´)n(r âˆ’1)Î´nrnR > rn.

7
118
Chapter 7 â€¢ Alternant and Algebraic Geometric Codes
Taking logarithms and dividing by n, we have that the r-ary asymptotic Gilbertâ€“
Varshamov bound is
R > 1 âˆ’h(Î´) logr 2 âˆ’Î´ logr(r âˆ’1).
Theorem 7.13 implies that a k-dimensional algebraic geometric code of a curve of
genus g satisï¬es
k + d â©¾n âˆ’g + 1,
where d is the minimum distance and n is the length, which is maximised when we take
as many Fq-rational points of Ï‡ as possible. Dividing by n, this gives the bound
R + Î´ â©¾1 âˆ’g/n + 1/n.
Therefore, to ï¬nd asymptotically good codes, we need a sequence of curves Ci (i âˆˆN)
of genus gi, for which gi â†’âˆ, and where gi/ni tends to a number smaller than 1.
Here, ni is the number of Fq-rational points of Ci. One way to construct such curves
is with recursive towers. These curves are constructed from an absolutely irreducible
polynomial f âˆˆFq[X, Y]. The afï¬ne points of Ci are deï¬ned in the following way. The
curve C1 is deï¬ned by f (X1, X2) = 0. The second curve C2 is deï¬ned by
f (X1, X2) = f (X2, X3) = 0,
the third curve C3 is deï¬ned by
f (X1, X2) = f (X2, X3) = f (X3, X4) = 0,
and so on. Observe that the curve Ci is a set of points in PG(i + 1, q), although we have
only described the afï¬ne part of the curve. The points on the hyperplane at inï¬nity are
obtained by homogenising the polynomials and setting the new variable to zero.
If q is an even power of an odd prime, then the sequence of curves constructed in
this way from
f (X, Y) = (Y
âˆšq + Y)(1 + X
âˆšqâˆ’1) âˆ’X
âˆšq
is a sequence of asymptotically good codes of rate R and relative minimum distance Î´
for which
R + Î´ â©¾1 âˆ’(âˆšq âˆ’1)âˆ’1.
For q â©¾49, this line surpasses the Gilbertâ€“Varshamov bound, for some range of values
of Î´, see âŠ¡Figure 7.1. This example is due to Tsfasman, VlË˜adut and Zink.

7.7 Â· Exercises
119
7
âŠ¡Fig. 7.1 The asymptotic
Gilbertâ€“Varshamov curve and
the Tsfasmanâ€“VlË˜adutâ€“Zink
bound for q = 49.
Gilbertâˆ’Varshamov
Tsfasmanâˆ’VlÄƒdutâˆ’Zink
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
7.6
Comments
Alternant codes were introduced by Helgert [36] in 1974, Goppa having previously
considered a subclass of alternant codes [28]. It was Goppaâ€™s later work from [29]
which led to algebraic geometric codes. The book of Tsfasman and VlË˜adut on algebraic
geometric codes [72] was published in 1991 and contains a wealth of results on such
codes. The particular bound in âŠ¡Figure 7.1 is from [73] and a survey of the asymptotic
bounds can be found in Tsfasman [71].
7.7
Exercises
7.1 Suppose that we have received the vector (âˆ’1, 1, 1, 1, 0, 0, 1, 1, 1), having sent a
codeword of the subï¬eld subcode in Example 7.2. Use syndrome decoding to ï¬nd and correct
the error bit.
7.2 Let M be an invertible matrix and let H be as in Exercise 7.6. Let C be the linear code
over Fqh with check matrix H and let Câ€² be the linear code over Fqh with check matrix MH.
Prove that A(C) = A(Câ€²).
7.3 Let Ï†(X) be a polynomial in Fqh[X] of degree r and let {Î±1, . . . , Î±n} be the set of
elements of Fqh which are not zeros of Ï†(X). For each u = (u1, . . . , un) âˆˆFqh, let

7
120
Chapter 7 â€¢ Alternant and Algebraic Geometric Codes
fu(X) =
n

i=1
ui
Ï†(Î±i) âˆ’Ï†(X)
X âˆ’Î±i

Ï†(Î±i)âˆ’1.
Prove that the subï¬eld subcode A(C), where
C = {(u1, . . . , un) âˆˆFqh | fu(X) = 0},
is an alternant code.
7.4 Let C be the linear code over F8 with check matrix
H =

1 1 1 1 1 1 1 1
e e2 e3 e4 e5 e6 e7 0

,
where e is a primitive element of F8.
Prove that the binary alternant code A(C) is a [8, 4, 4]2 code.
7.5 Let e be a primitive element of F8, such that e3 = e + 1 and let C be the GRS code
{(e4f (0), f (1), e6f (e), e4f (e2), f (e3), e6f (e4), e4f (e5), f (e6)) | f âˆˆF8[X],
deg f â©½4}.
Prove that
A(C) = {(0, 0, 0, 0, 0, 0, 0, 0), (1, 1, 1, 1, 0, 1, 0, 1)}.
7.6
i.
Prove that the GRS code in Lemma 7.4 has a check matrix
H =
â›
âœâœâœâœâœâœâ
1
1
. . .
1
Î±1
Î±2
. . .
Î±n
.
.
. . .
.
.
.
. . .
.
Î±nâˆ’kâ€²âˆ’1
1
Î±nâˆ’kâ€²âˆ’1
2
. . . Î±nâˆ’kâ€²âˆ’1
n
â
âŸâŸâŸâŸâŸâŸâ 
â›
âœâœâœâœâœâœâœâœâœâ
vâˆ’1
1
0
. . . . . .
0
0
vâˆ’1
2
0 . . .
...
0
0
... 0
0
...
. . .
0 ...
0
0
. . . . . . 0 vâˆ’1
n
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
.
ii.
Prove that the dual of a GRS code is a GRS code.
7.7 Let C be the GRS code deï¬ned over F4 = {0, 1, e, e2} by
C = {(f (0), f (1), eâˆ’1f (e), eâˆ’2f (e2)) | f âˆˆF4[X], deg f â©½1}.

7.7 Â· Exercises
121
7
Suppose that we have received the vector y = (1, 1, 1, 1). Use the decoding algorithm in
Theorem 7.8 to ï¬nd g and h, polynomials of degree 2 and 1, respectively. Verify that h divides
g, deduce f and correct the error in y.
7.8 Prove that, in the decoding algorithm of Theorem 7.8, if n + kâ€² is odd, then
deg g â©½âŒˆ1
2(n + kâ€²)âŒ‰âˆ’2.
7.9 Let C be the GRS code deï¬ned over F9 by
C = {(f (0), f (1), ef (e), f (e2), e2f (e3)) | f âˆˆF9[X], deg f â©½1},
where e2 = e + 1.
Suppose that we have received the vector y = (1, 2, 1 + 2e, 2 + e, 0).
Use the decoding algorithm in Theorem 7.8 to ï¬nd g and h, polynomials of degree at
most 3 and 1, respectively. Verify, as claimed in Exercise 7.8, that the degree of g is 2, that h
divides g, deduce f and correct the error in y.
7.10 Let Ï‡ be the curve deï¬ned as the zeros of the polynomial
X4 + Y 3Z + Z3Y.
Let P1 = (1 : 0 : 0), P2 = (0 : 1 : 0) and P3 = (0 : 0 : 1).
i.
Calculate the divisor (Y/Z).
ii.
Find a basis for L(D), where D = 3P1 + 3P2.
iii.
The curve Ï‡ has 28 rational points over F9. Construct a [24, 4, 18]9 code.
iv.
Verify that the Griesmer bound for a [24, 4, d]9 code gives d â©½19.

123
8
Low Density Parity Check Codes
A linear code with a check matrix in which each column has few non-zero entries is
called a low density parity check code or, for brevity, an LDPC code. These codes were
introduced in the 1960s by Gallager who proved that probabilistic constructions of such
matrices produce asymptotically good linear codes. Moreover, he observed that LDPC
codes perform well when applying the following decoding algorithm. On receiving a
vector v, one calculates the weight of the syndrome of v +e, for each vector e of weight
one. If the weight of this syndrome is less than the weight of the syndrome of v, for
some e, then we replace v by v + e and repeat the process. If at each iteration there
is such a vector e, then, since after replacing v by v + e, the weight of the syndrome
of v decreases, we will eventually ï¬nd a vector whose syndrome is zero, which must
be the syndrome of some codeword u. We then decode v as u. If at some iteration no
such e exists then the decoding breaks down. If at some iteration more than one such
vector e exists, then one could choose e so that the weight of the syndrome of v + e is
minimised. In this chapter we will prove that there are LDPC codes, constructed from
graphs with the expander property, for which the decoding algorithm will not break
down. Provided that the number of error bits is less than half the minimum distance,
the decoding algorithm will return the nearest codeword to the received vector. We will
use probabilistic arguments to construct the graphs, and from these a sequence of codes
which are asymptotically good.
8.1
Bipartite Graphs with the Expander Property
A graph is a pair (V, E), where V is a set and E is a set of 2-subsets of V . We consider
the elements of V to be vertices and the set E to be a set of edges, where an edge {v1, v2}
joins the two vertices v1 and v2. A bipartite graph is a graph in which V is the disjoint
union V1 âˆªV2 and for all e = {v1, v2} âˆˆE, we have v1 âˆˆV1 and v2 âˆˆV2. In other
words, there are no edges joining two vertices in V1 and no edges joining two vertices in
V2. The subsets V1 and V2 are called the stable sets of the bipartite graph. The degree
of a vertex v is the number of edges which contain v and we say that u is a neighbour
of v if {u, v} is an edge. A bipartite graph with stable sets V1 and V2 is left Î³ -regular if
Â© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_8

8
124
Chapter 8 â€¢ Low Density Parity Check Codes
every vertex in V1 has degree Î³ , i.e. has Î³ neighbours in V2. For a subset S of V1, denote
by N(S) the set of vertices of V2 which are neighbour to some vertex in S.
Let Î´ be a real number in the interval (0, 1). A left Î³ -regular bipartite graph with
stable sets V1 and V2 of size n and m, respectively, has the expander property with
respect to Î´ if for all subsets S of V1 of size less than Î´n, |N(S)| > 3
4Î³ |S|.
Example 8.1
âŠ¡Figure 8.1 is a left 3-regular bipartite graph with the expander property with respect to
Î´ = 1
4. The 12 vertices of V1 are the vertices on the outer circle and the 9 vertices of V2 are
the vertices on the inner circle. One can verify that every subset S of V1 of size 1 has three
neighbours, so the graph is left 3-regular. If S is a 2-subset of V1, then N(S) has either 5 or
6 vertices. Therefore, for every subset S of V1 of size less than Î´n = 3, |N(S)| > 9
4|S|.
â– 
Lemma 8.2 Given Î³ > 4 and R âˆˆ(0, 1), there is a constant Î´ âˆˆ(0, 1), dependent on Î³ and
R, for which a left Î³ -regular bipartite graph with the expander property with respect to Î´
exists, for all n large enough, where the left and right stable sets have size n and âŒŠ(1 âˆ’R)nâŒ‹,
respectively.
Proof
Consider the set  of bipartite left Î³ -regular graphs with stable sets V1 and V2 of size n and
m = âŒŠ(1 âˆ’R)nâŒ‹.
âŠ¡Fig. 8.1 A left 3-regular bipartite graph with the expander property.

8.1 Â· Bipartite Graphs with the Expander Property
125
8
For a graph  âˆˆ, a subset S of V1 of size s < Î´n and a subset T of V2 of size âŒŠ3
4Î³ sâŒ‹,
deï¬ne a random variable XS,T which takes the value 1 if all edges of  with an end-vertex
in S have an end-vertex in T and 0 otherwise. If we can prove that the probability
P(

S,T
XS,T = 0) Ì¸= 0,
then we can deduce that there is a graph  âˆˆ for which XS,T = 0 for all subsets S and T .
This implies that the graph  has the property that the union of the neighbours of the vertices
in S has more than 3
4Î³ |S| vertices, for all subsets S of V1 of size less than Î´n. Hence,  has
the expander property with respect to Î´.
Since the probability that a randomly chosen edge has an end-vertex in T is âŒŠ3
4Î³ sâŒ‹/m,

S,T
P(XS,T = 1) <
âŒŠÎ´nâŒ‹

s=1
n
s

m
âŒŠ3
4sÎ³ âŒ‹
 
âŒŠ3
4Î³ sâŒ‹
m
Î³ s
,
where the sum is over all subsets S of V1 of size s < Î´n and all subsets T of V2 of size
âŒŠ3
4Î³ sâŒ‹.
Since
ek =
âˆ

j=0
kj
j! > kk
k! ,
we have
n
k

â©½nk
k! < (ne
k )k,
which in the above gives

S,T
P(XS,T = 1) <
Î´n

s=1
en
s
s

eâŒŠ(1 âˆ’R)nâŒ‹
âŒŠ3
4Î³ sâŒ‹
 3
4 sÎ³ 
âŒŠ3
4Î³ sâŒ‹
âŒŠ(1 âˆ’R)nâŒ‹
Î³ s
.
Since
s( 1
4 Î³ âˆ’1)s < (Î´n)( 1
4 Î³ âˆ’1)s,
this gives

S,T
P(XS,T = 1) <
Î´n

s=1
(NÎ´
1
4 Î³ âˆ’1)s,
for some constant N, dependent on Î³ and R, but not dependent on n.
Now, if we choose Î´ so that NÎ´
1
4 Î³ âˆ’1 < 1
2, then this sum is less than 1.

8
126
Chapter 8 â€¢ Low Density Parity Check Codes
Since,
P(

S,T
XS,T Ì¸= 0) â©½

S,T
P (XS,T = 1) < 1,
we have that
P(

S,T
XS,T = 0) Ì¸= 0.
âŠ“âŠ”
8.2
Low Density Parity Check Codes
A low density parity check code is a linear code which has a check matrix H with
the property that H has few non-zero elements in each column. We will only consider
low density parity check binary codes, so H will have the property that it has few 1â€™s
in each column. We make this vague deï¬nition precise for sequences of codes of length
n, where n tends to inï¬nity, by insisting that the few non-zero elements is a constant
number.
Lemma 8.2 proves that sequences of bipartite expander graphs exist for n large
enough. We now prove that we can construct a matrix from a bipartite graph in this
sequence, which is a check matrix of a code in a sequence of asymptotically good linear
codes.
Fix R to be the rate of transmission we would like to achieve and Î´ to be the relative
minimum distance.
Lemma 8.3 Given a left Î³ -regular bipartite graph  with stable sets of size n and m =
âŒŠ(1 âˆ’R)nâŒ‹and the expander property with respect to Î´, there exists a binary linear code
C() with rate at least R and relative minimum distance at least Î´.
Proof
Let H be the m Ã— n matrix whose rows are indexed by the vertices of V2 and whose columns
are indexed by the vertices of V1. A row-column entry is 1 if there is an edge joining the
vertex of V1 to the vertex of V2 and zero otherwise. Then H has Î³ 1â€™s in each column.
Let C() be the binary linear code deï¬ned by the check matrix H, i.e.
C() = {u âˆˆFn
2 | uHt = 0}.
Since the rank of H is at most the number of rows, the dimension of C() is at least
n âˆ’m = n âˆ’âŒŠ(1 âˆ’R)nâŒ‹â©¾Rn, so C() will have rate at least R.
Suppose that C() has minimum distance less than Î´n. By Lemma 4.1, there is a non-
zero vector u of C() of weight less than Î´n. Let S be the support of u, the set of coordinates
where u has a 1. These coordinates correspond to vertices of V1, since the rows of Ht are

8.2 Â· Low Density Parity Check Codes
127
8
indexed by the vertices of V1, so we can think of S as a subset of V1. Since |S| < Î´n and
 has the expander property with respect to Î´, the size of the set N(S), the set of vertices
neighbour to some vertex of S, is at least 3
4Î³ |S|.
If every vertex of N(S) has at least two edges joining it to vertices of S, then, counting
edges with an end-vertex in S,
|S|Î³ â©¾2|N(S)|,
(8.1)
which contradicts |N(S)| â©¾3
4|S|Î³ .
Therefore, there is some vertex v in N(S) which is joined to just one vertex of S. A vertex
of V2 indexes a row r of the check matrix H. Since r is a row of the check matrix, it has the
property that r Â· w = 0, for all codewords w âˆˆC().
However, v is joined to just one vertex of the support S of u, so r Â· u = 1, which is
a contradiction, since the scalar product of a row of the check matrix H and a codeword is
zero. Hence, each non-zero vector in C() has weight at least Î´n. By Lemma 4.1, C() has
minimum distance at least Î´n.
âŠ“âŠ”
Observe that we could relax the expander property to |N(S)| >
1
2Î³ |S| and
Lemma 8.3 would still hold. However, we insist upon |N(S)| >
3
4Î³ |S|, so that the
decoding algorithm, which we will see in the following section, works.
Example 8.4
The check matrix obtained from the bipartite graph  in Example 8.1 is
H =
â›
âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
1 0 0 0 1 0 1 0 0 1 0 0
0 1 0 0 0 1 1 0 0 0 0 1
0 0 1 1 0 0 1 0 0 0 1 0
1 0 0 0 0 1 0 1 0 0 1 0
0 1 0 1 0 0 0 1 0 1 0 0
0 0 1 0 1 0 0 1 0 0 0 1
1 0 0 1 0 0 0 0 1 0 0 1
0 1 0 0 1 0 0 0 1 0 1 0
0 0 1 0 0 1 0 0 1 1 0 0
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
.
Lemma 8.3 implies that the rate of C() is at least 1
4 and the minimum distance is at least 3.
It is Exercise 8.1 to verify that H has rank 9 and so the dimension of C() is 3 and the rate
of C() is precisely 1
4.
Consider the graph in Example 8.1. We can deï¬ne a geometry which has as points the
vertices of V2 and as lines the vertices of V1, where for each vertex u of V1, we have a line of
the geometry consisting of the points which are neighbours to u in the graph. This geometry
is AG(2, 3), the afï¬ne plane of order 3, see â–·Section 2.4, and in particular âŠ¡Figure 2.2.
Each block of three columns corresponds to a parallel set of lines, so C() has a generator
matrix

8
128
Chapter 8 â€¢ Low Density Parity Check Codes
G =
â›
âœâ
1 1 1 1 1 1 0 0 0 0 0 0
0 0 0 0 0 0 1 1 1 1 1 1
1 1 1 0 0 0 1 1 1 0 0 0
â
âŸâ .
Thus we deduce that all codewords of C() have weight 0, 6 or 12, so C() is a [12, 3, 6]2
code.
The Griesmer bound from Theorem 4.18 for a [12, 3, d]2 code gives the bound d â©½6.
â– 
8.3
Decoding LDPC Codes
Lemma 8.2 and Lemma 8.3 imply that we can ï¬nd asymptotically good codes using
bipartite graphs with the expander property. Moreover, these codes are linear, so fast
to encode. However, the really useful property that these codes have is that they are
also fast to decode. We will prove this in Theorem 8.7, but ï¬rst we need to prove the
following lemma.
Recall that for x âˆˆFn
2, the syndrome of x is
s(x) = xH t
and wt(x) is the number of non-zero coordinates that x has.
Let ei denote the vector of weight one and length n with a 1 in the i-th coordinate.
Lemma 8.5 Suppose that x âˆˆFn
2 and that d(x, u) < Î´n for some u âˆˆC(), where C()
is the binary linear code obtained from a bipartite graph  which has the expander property
with respect to Î´. Then there is an i âˆˆ{1, . . . , n} such that wt(s(x + ei)) < wt(s(x)).
Proof
Let S be the coordinates where x and u differ. By assumption, |S| < Î´n. As in the proof of
Lemma 8.3, the set S corresponds to a subset of the vertices V1 of the graph . As before, let
N(S) denote the set of vertices which are neighbour to some vertex of S. Then, as in the proof
of Lemma 8.3, the vertices of N(S) index rows of the matrix H which in turn correspond to
linear constraints on the code C(). Divide N(S) into T , constraints that x satisï¬es and U,
constraints that x does not satisfy. In other words, T is the subset of N(S) where s(x) has a
zero and U is the subset of N(S) where s(x) has a 1. Here, we are identifying the coordinates
of s(x) with vertices of V2.
Since |S| < Î´n, the expander property implies
|T | + |U| = |N(S)| > 3
4Î³ |S|.
Counting edges between S and N(S), we have
|U| + 2|T | â©½Î³ |S|,

8.3 Â· Decoding LDPC Codes
129
8
since there must be at least two edges joining T to vertices in S, otherwise the constraint
would not be satisï¬ed.
Combining the inequalities this implies |T | < 1
4Î³ |S| and therefore
|U| > 1
2Î³ |S|.
(8.2)
This implies that
wt(s(x)) > 1
2Î³ |S|.
Since,
s(x) = s(x + u) =

iâˆˆS
s(ei),
the pigeon-hole principle implies that there is an i for which s(ei) and s(x) both have a 1 in
more than 1
2Î³ of the coordinates. Since wt(s(ei)) = Î³ , this implies that
wt(s(x + ei)) < wt(s(x)).
âŠ“âŠ”
Example 8.6
Suppose that we have sent a codeword of C(), deï¬ned by the check matrix in Example 8.4,
and that we have received
x = (0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0).
By calculating xHt,
s(x) = (0, 0, 0, 1, 1, 1, 1, 1, 1).
The weights of s(x + ei) are given in the following table:
i
1
2
3
4
5
6
7
8
9
10
11
12
wt(s(x + ei))
5
5
5
5
5
5
9
3
3
5
5
5.
Assuming that less than two errors have occurred in transmission, Lemma 8.5 implies
that there is an i for which wt(s(x + ei)) < 6 and this is the case for all i Ì¸= 7.
We will return to this example in Example 8.8.
â– 
Lemma 8.5 allows us to apply the decoding algorithm described in Theorem 8.7.
This type of decoding algorithm is called belief propagation. Observe that in
Lemma 8.5 we are not claiming that by summing ei to x we are correcting an error,

8
130
Chapter 8 â€¢ Low Density Parity Check Codes
only that the weight of the syndrome decreases. It may be that in the algorithm in
Theorem 8.7, we introduce new errors at a particular iteration. However, since the
weight of the syndrome is decreasing, it will eventually have weight zero and all errors,
even those which we may have inadvertently introduced, will have been corrected.
Theorem 8.7
Let C() be the linear code of length n obtained from a bipartite graph  with the
expander property with respect to Î´. There is a decoding algorithm for C() which
completes in a number of steps which is polynomial in n and which corrects up to 1
2Î´n
error bits.
Proof
We will provide an algorithm for decoding the received vector x âˆˆFn
2, where d(x, u) < 1
2Î´n,
for some u âˆˆC.
Let S be the support of x âˆ’u, i.e. the coordinates where x and u differ.
Although we do not know what S is, by assumption |S| < 1
2Î´n.
By Lemma 8.5, if we test x + ei for i = 1, . . . , n, we will ï¬nd an i for which
wt(s(x + ei)) < wt(s(x)).
So, we can repeat this process with x+ei and |U|, the size of the set of unsatisï¬ed constraints,
will decrease.
The maximum value of |U| (at the ï¬rst step since |U| is decreasing) is bounded by
|U| â©½|N(S)| â©½Î³ |S| < 1
2Î³ Î´n.
At each iteration, when we apply Lemma 8.5, equation (8.2) implies |S| < Î´n. Therefore,
the hypothesis of Lemma 8.5 is satisï¬ed and can be applied at the next iteration.
The weight of s(x) in the ï¬rst iteration is less than, 1
2Î³ Î´n. In each of the following
iterations the weight of the syndrome decreases, so the number of iterations will be at most
1
2Î³ Î´n. In each iteration we have to multiply a matrix with n vectors, so the whole algorithm
completes in a number of steps which is polynomial in n.
âŠ“âŠ”
Example 8.8
Theorem 8.7 only guarantees that we can correct up to 1
2Î´n errors. In Example 8.4, this
implies that we can correct up to only one error bit even though the minimum distance is 6
and we would expect to be able to correct up to two error bits.
If we apply the algorithm described in Theorem 8.7 to the table of syndromes we
calculated in Example 8.6, then we have many choices for i in the ï¬rst iteration. Let us
suppose we decide to choose an i for which the weight of s(x + ei) is minimised at each
iteration. Then we would replace x by x + e8 at the ï¬rst iteration, then x + e8 by x + e8 + e9
at the second iteration and would have correctly found the codeword
(0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0)
at distance 2 to x.
â– 

8.5 Â· Exercises
131
8
8.4
Comments
This chapter has realised an important aim that of proving that there are asymptotically
good codes which we can encode and decode in an efï¬cient manner. There are
LDPC codes whose rate nears the Shannon capacity, see MacKay and Neal
[49].
For an excellent survey on expanders, see Hoory, Linial and Wigderson [39]. The
decoding algorithm for expander codes is due to Sipser and Spielman [67]. Regarding
Exercise 8.5, a list of the known hyperovals and their collineation groups can be found
in Penttila and Pinneriâ€™s article [56]. For more on LDPC codes constructed from ï¬nite
geometries, see Kou, Lin and Fossorie [45] and Pepe [57].
LDPC codes have replaced turbo codes in 5G mobile networks. Turbo codes are not
covered here in this text, principally because they lack any algebraic structure. LDPC
codes are widely used in mobile and satellite communication and have been employed
by NASA in recent missions as an alternative to Reedâ€“Solomon codes.
8.5
Exercises
8.1
Prove that there is no subset S of points of AG(2, 3) with the property that every line is
incident with an even number of points of S. Conclude that the check matrix from Example 8.4
has rank 9 over F2.
8.2 Let  be the bipartite graph whose vertices V1 are the lines of PG(3, 4) and whose
vertices V2 are the points of PG(3, 4) and where a point is joined to a line by an edge in  if
and only if the point and line are incident in the geometry.
i.
Apply Lemma 8.3 to prove that C() is a linear code of rate at least 272
357 with relative
minimum distance at least
4
357.
ii.
Prove that the minimum distance of C() is at least 9.
iii.
Determine up to how many error bits we can guarantee to correct when applying belief
propagation decoding.
8.3 Prove that the point-line incidence matrix of AG(2, 4) is the adjacency matrix of a left
4-regular graph  with the expander property with respect to Î´ = 1
5, with stable sets of size
20 and 16.
8.4 Let q be odd.
i.
Prove that there is no set S of points of AG(2, q) with the property that every line is
incident with an even number of points of S.
ii.
Let H be the matrix whose rows are indexed by the points of AG(2, q) and whose columns
are indexed by the lines of AG(2, q) and where an entry in the matrix is 1 if the point is
incident with the line and 0 otherwise.
Let C be the binary linear code with check matrix H. Prove that the dimension of C
is at least q and that the minimum distance of C is at least q + 2.

8
132
Chapter 8 â€¢ Low Density Parity Check Codes
A dual hyperoval in AG(2, q) or PG(2, q) is a subset L of q + 2 lines with the
property that every point is incident with either zero or two lines of L.
8.5
i.
Prove that if AG(2, q) has a dual hyperoval, then q is even.
ii.
Suppose that u is a codeword of weight 6 of the binary linear code C(), where  is the
expander graph in Exercise 8.3. Prove that u is the indicator vector of a dual hyperoval.
In other words, if S is the support of u, then, as a set of lines of AG(2, 4), S is a dual
hyperoval.
iii.
Deduce the minimum distance of C() by proving that there are no small sets of lines L
with the property that every point of AG(2, 4) is incident with an even number of lines
of L.
iv.
By calculating the rank of a check matrix for C() (with the aid of a computer),
determine the dimension of C().

133
9
Reedâ€“Muller and Kerdock Codes
In â–·Chapter 6, we studied Reedâ€“Solomon codes, codes whose codewords are the
evaluation of polynomials in one variable of degree at most k âˆ’1 at the elements of
Fq âˆª{âˆ}. Reedâ€“Solomon codes are short length codes, where the length n is bounded
by q + 1, and only useful when we take the ï¬eld to be large. The alternant codes which
we constructed from generalised Reedâ€“Solomon codes in â–·Chapter 7 allowed us to
construct codes over small ï¬elds and we put this to good use. In this chapter we will
consider another generalisation of Reedâ€“Solomon codes, codes whose codewords are
the evaluation of polynomials in many variables. This again allows us to construct
linear codes over small ï¬elds and we will restrict our attention, for the most part,
to binary linear codes. It will turn out that these codes are not asymptotically good.
Nevertheless, they are an important class of codes which are widely implemented due to
the availability of fast decoding algorithms. One example of such a decoding algorithm
is the majority-logic decoding algorithm that we will study here. We will then go on and
construct Kerdock codes which are certain subcodes of the second-order Reedâ€“Muller
codes. These codes can give examples of non-linear codes with parameters for which no
linear code exists.
9.1
Binary Reedâ€“Muller Codes
A Boolean function from Fm
2 to F2 is the evaluation map of a polynomial with
coefï¬cients from F2 in m variables generated by monomials in which the degree of
any particular indeterminate is at most 1.
Note that for both elements x of F2, x2 = x, so the function deï¬ned by the evaluation
of the polynomial x2
1x3
2x3 at the elements of F3
2 and the polynomial x1x2x3 will be the
same. Therefore, it makes sense that when considering evaluations of polynomials in
many variables over F2, we restrict our attention to Boolean functions.
The r-th order Reedâ€“Muller code is a binary code R(r, m) of length 2m deï¬ned by
R(r, m) = {(f (a1), . . . , f (a2m)) | deg f â©½r},
Â© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_9

9
134
Chapter 9 â€¢ Reedâ€“Muller and Kerdock Codes
where {a1, . . . , a2m} is the set of vectors of Fm
2 and f runs through all Boolean functions
that are deï¬ned by polynomials in m indeterminates of degree at most r.
The code R(r, m) is a linear code over F2, since
(f (a1), . . . , f (a2m)) + (g(a1), . . . , g(a2m)) = ((f + g)(a1), . . . , (f + g)(a2m)).
The vector space of Boolean functions of degree at most r in m variables has a canonical
basis, which is the set of monomials of degree at most r in m variables and degree at
most one in any particular variable. Therefore, the code R(r, m) has a generator matrix
whose rows are indexed by these monomials. For example, the set of monomials
{1, x1, . . . , xm, x1x2, . . . , xmâˆ’1xm}
is a basis for the vector space of Boolean functions in m variables of degree at most 2.
Example 9.1
The 11 Ã— 16 matrix
G =
â›
âœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1
0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1
0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1
0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1
0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1
0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1
0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1
0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1
0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
1
x1
x2
x3
x4
x1x2
x1x3
x1x4
x2x3
x2x4
x3x4
is a generator matrix of the code R(2, 4), with the rows being indexed by the monomials in
four variables of degree at most two.
â– 
We have already proved the following lemma.
Lemma 9.2 R(r, m) is a linear code of length 2m and of dimension
1 +
m
1

+
m
2

+ . . . +
m
r

.
Since R(r, m) is linear, Lemma 4.1 implies that its minimum distance is equal to the
minimum weight of a non-zero codeword. In the above example, evidently R(2, 4) has
codewords of weight 4 and this is indeed its minimum distance. In Theorem 9.3 we will

9.2 Â· Decoding Reedâ€“Muller Codes
135
9
calculate the minimum distance for binary Reedâ€“Muller codes. Later, in Theorem 9.15,
we will calculate the minimum distance for non-binary Reedâ€“Muller codes.
Theorem 9.3
The minimum distance of R(r, m) is 2mâˆ’r.
Proof
By induction on m. If m = r, then the evaluation of the polynomial X1 Â· Â· Â· Xr is a codeword
of weight one.
Suppose that the minimum distance of R(r, m) is 2mâˆ’r.
Order the vectors of Fm+1
2
so that the ï¬rst 2m vectors have xm+1 = 0.
A codeword (u, u + v) of R(r, m + 1) is the evaluation of a polynomial
f (X) + Xm+1g(X),
where f (X) is a polynomial of degree at most r in m variables and g(X) is a polynomial of
degree at most r âˆ’1 in m variables. Then u âˆˆR(r, m), since it is the evaluation of f (X) and
v âˆˆR(r âˆ’1, m), since it is the evaluation of g(X).
If u = 0, then the codeword is (0, v) and by induction has non-zero weight at least
2mâˆ’(râˆ’1).
If u + v = 0, then u = v âˆˆR(r âˆ’1, m) and so the codeword (u, 0) has non-zero weight
at least 2mâˆ’(râˆ’1).
If neither u nor u + v is zero, then (u, u + v) has weight at least 2 Â· 2mâˆ’r = 2mâˆ’r+1,
since both u and u + v are in R(r, m).
Thus, the minimum weight of a non-zero codeword of R(r, m + 1) is 2mâˆ’r+1. By
Lemma 4.1, the minimum weight of a non-zero codeword of a linear code is equal to its
minimum distance.
âŠ“âŠ”
9.2
Decoding Reedâ€“Muller Codes
The popularity of Reedâ€“Muller codes in real-world applications is due in part to the fact
that there are fast decoding algorithms, the most common of which is the focus of this
section. Before we consider this decoding algorithm, we ï¬rst prove a couple of lemmas
which prove some properties of Boolean functions.
For each non-empty subset J of {1, . . . , m}, let
fJ(X) =

jâˆˆJ
Xj
and deï¬ne
fâˆ…(X) = 1.

9
136
Chapter 9 â€¢ Reedâ€“Muller and Kerdock Codes
Then
{fJ(X) | J âŠ†{1, . . . , m}, |J| â©½r}
is a basis for the space of polynomials in m variables of degree at most r whose
evaluations deï¬ne Boolean functions.
We will exploit the following lemma repeatedly.
Lemma 9.4 Let J be a subset of {1, . . . , m}. Suppose
g(X) =

LâŠ†{1,...,m}
aLfL(X),
for some aL âˆˆF2, where the sum is over all subsets L of size at most m âˆ’|J|.
Then

xâˆˆFm
2
fJ (x)g(x) = a{1,...,m}\J .
Proof
Let K âŠ†{1, . . . , m}.
If there is an i âˆˆ{1, . . . , m} \ K, then

{xâˆˆFm
2 |xi=0}
fK(x) =

{xâˆˆFm
2 |xiÌ¸=0}
fK(x).
This implies

xâˆˆFm
2
fK(x) = 0,
(9.1)
unless K = {1, . . . , m}.
Then

xâˆˆFm
2
fJ (x)g(x) =

LâŠ†{1,...,m}

xâˆˆFm
2
aLfJ (x)fL(x),
where the ï¬rst sum on the right-hand side is over all subsets L of size at most m âˆ’|J|.
This expression is equal to

LâŠ†{1,...,m}
aL

xâˆˆFm
2
fJâˆªL(x) = a{1,...,m}\J ,
by (9.1).
âŠ“âŠ”

9.2 Â· Decoding Reedâ€“Muller Codes
137
9
Theorem 9.5
The dual of the code R(r, m) is the Reedâ€“Muller code R(m âˆ’r âˆ’1, m).
Proof
A codeword u of R(r, m) is the evaluation of a polynomial
g(X) =

KâŠ†{1,...,m}
aKfK(X),
for some aK âˆˆF2, where the sum is over all subsets of size at most r.
A codeword v of R(m âˆ’r âˆ’1, m) is the evaluation of
h(X) =

LâŠ†{1,...,m}
bLfL(X),
for some bL âˆˆF2, where the sum is over all subsets of size at most m âˆ’r âˆ’1.
The inner product of u and v is

xâˆˆFm
2
h(x)g(x) =

xâˆˆFm
2

K

L
aKbLfK(x)fL(x)
=

K

L
aKbL

xâˆˆFm
2
fKâˆªL(x) = 0,
by Lemma 9.4.
Therefore,
R(m âˆ’r âˆ’1, m) âŠ†R(r, m)âŠ¥.
By Theorem 9.3, the sum of the dimensions of R(r, m) and R(m âˆ’r âˆ’1, m) is 2m, which is
the length of the codes.
Hence,
dim R(m âˆ’r âˆ’1, m) = dim R(r, m)âŠ¥.
âŠ“âŠ”
The following lemma is fundamental to the decoding algorithm.
Lemma 9.6 Let
g(X) =

KâŠ†{1,...,m}, |K|â©½r
bKfK(X),

9
138
Chapter 9 â€¢ Reedâ€“Muller and Kerdock Codes
where bK âˆˆF2 and let J be a subset of {1, . . . , m} of size r.
For all 2mâˆ’r choices of ai âˆˆF2, i âˆˆ{1, . . . , m} \ J,

xâˆˆFn
2
g(x)

iâˆˆ{1,...,m}\J
(xi + ai) = bJ .
Proof
When we expand the product in the sum, all terms have degree less than m except those
coming from
g(x)

iâˆˆ{1,...,m}\J
xi = g(x)f{1,...,m}\J (x).
The lemma follows from Lemma 9.4.
âŠ“âŠ”
We are now in a position to describe a decoding algorithm for Reedâ€“Muller codes,
which is an example of a majority-logic decoding algorithm. Let v be the received
vector, whose coordinates vx are indexed by the vectors x âˆˆFm
2 . For each subset J of
{1, . . . , m} of size r, we perform a test. We wish to determine whether uJ is zero or one,
where the sent codeword u is the evaluation of

JâŠ†{1,...,m}, |J|â©½r
uJfJ(X).
For all 2mâˆ’r choices of ai âˆˆF2, i âˆˆ{1, . . . , m} \ J, we calculate

xâˆˆFm
2
vx

iâˆˆ{1,...,m}\J
(xi + ai).
If the result of this test is 1 in the majority of cases, then we conclude that uJ = 1 and
vice versa, if it is 0 in the majority of cases, then we conclude that uJ = 0. Once we have
completed this for all subsets J of {1, . . . , m} of size r, we subtract the evaluation of

KâŠ†{1,...,m}, |K|=r
uKfK(X),
from the received vector and continue with the subsets of size r âˆ’1 supposing that, if
we are correctly decoding, we now have a corrupted codeword of R(r âˆ’1, m).
All that remains to be shown, to prove that this decoding algorithm will correct up to
2mâˆ’râˆ’1 âˆ’1 error bits, is to show that an error bit will only affect one of the tests. Before
we prove this in Lemma 9.8, we consider an example.
Example 9.7
Suppose that we have encoded using R(2, 4) and have received
v = (1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0),

9.2 Â· Decoding Reedâ€“Muller Codes
139
9
where the vectors of F4
2 are ordered as in the matrix G in Example 9.1.
We calculate
w = vGt = (1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0).
The coordinates are indexed by subsets of {1, 2, 3, 4} of size at most 2, as in Example 9.1.
Indexing the coordinates explicitly
J
âˆ…{1} {2} {3} {4} {12} {13} {14} {23} {24} {34}
wJ 1 0
1
1
0
1
1
0
0
1
0
where
wJ =

xâˆˆFm
2
vxfJ (x)
are the coordinates of w.
We start by determining uJ for the subsets J of size r = 2.
To determine u{12}, we make 2mâˆ’r = 4 tests by calculating

xâˆˆFm
2
vxx3x4,

xâˆˆFm
2
vx(x3x4 + x3),

xâˆˆFm
2
vx(x3x4 + x4)
and

xâˆˆFm
2
vx(x3x4 + x3 + x4 + 1),
which is
w{34}, w{34} + w{3}, w{34} + w{4} and w{34} + w{3} + w{4} + wâˆ…,
respectively.
The results of these tests are 0, 1, 0, 0, respectively, so we decode u{12} as 0, since there
are a majority of zeros.
The following table lists the results of these tests for all subsets of size 2 and indicates
the majority decision.
u{12} 0, 1, 0, 0 â†’0 u{13} 1, 0, 1, 1 â†’1 u{14} 0, 1, 1, 1 â†’1
u{23} 0, 0, 0, 1 â†’0 u{24} 1, 1, 0, 1 â†’1 u{34} 1, 1, 0, 1 â†’1
Based on the results of those tests, we subtract
(0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1)G

9
140
Chapter 9 â€¢ Reedâ€“Muller and Kerdock Codes
from v and get
v1 = v + (0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1)G = (1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0).
If we are decoding correctly, v1 should be a (possibly) corrupted codeword of R(1, 4). To
determine uJ , where J is a subset of size 1, we repeat the above.
We calculate
w1 = v1Gt
1,
where G1 is the generator matrix of R(3, m). This vector will have coordinates
w1
K =

xâˆˆFm
2
fK(x)v1
x,
where K is a subset of {1, 2, 3, 4} of size at most 3.
Indexing the coordinates explicitly as before
K âˆ…{1} {2} {3} {4} {12} {13} {14}
w1
K 1 0
1
1
0
0
0
0
K {23} {24} {34} {123} {124} {134} {234}
w1
K
1
0
0
0
0
1
0
allows us to perform 2mâˆ’(râˆ’1) = 8 tests for each uJ .
To determine u{1}, we make 8 tests by calculating
w1
{234}, w1
{234} + w1
{23}, w1
{234} + w1
{24}, w1
{234} + w1
{34}, w1
{234} + w1
{23} + w1
{24} + w1
{2},
w1
{234} + w1
{23} + w1
{34} + w1
{3}, w1
{234} + w1
{24} + w1
{34} + w1
{4}
and
w1
{234} + w1
{23} + w1
{24} + w1
{34} + w1
{2} + w1
{3} + w1
{4} + w1
âˆ….
The results of these tests are
u{1} 0, 1, 0, 0, 0, 0, 0, 0 â†’0 u{2} 1, 1, 1, 1, 1, 0, 1, 1 â†’1
u{3} 0, 0, 0, 0, 0, 1, 0, 0 â†’0 u{4} 0, 0, 0, 1, 0, 0, 0, 0 â†’0
Based on the results of the tests, we subtract
(0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0)G

9.2 Â· Decoding Reedâ€“Muller Codes
141
9
from v1 and get
v2 = v + (0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1)G = (1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1).
Summing
(1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)G
to v2 we have that
v + (1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1)G = (0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0).
Therefore, we have determined that the error is in the 7-th bit, that the uncoded string
u = (1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1)
and that the sent codeword was uG.
â– 
To ï¬nish this section, we prove that an error bit affects exactly one of the tests when
testing a corrupted codeword of R(r, m). Since we perform 2mâˆ’r tests, this implies that
we can correct up to 2mâˆ’râˆ’1 âˆ’1 error bits, or in other terms 1
2d âˆ’1 error bits, since the
minimum distance d of R(r, m) is 2mâˆ’r.
Lemma 9.8 Suppose that e is a vector of F2m
2
of weight one, whose coordinates are indexed
by the vectors of Fm
2 . Let J be a subset of {1, . . . , m}. For all but one of the choices of a,
whose coordinates ai âˆˆF2 for i âˆˆ{1, . . . , m} \ J,

xâˆˆFm
2
ex

iâˆˆ{1,...,m}\J
(xi + ai) = 0,
where ex is the coordinate of e indexed by x.
Proof
Let y be the vector of Fm
2 indexing the coordinate where the vector e has a 1.
The vector e is the evaluation of
m

i=1
(Xi + yi + 1),
since it is zero unless Xi = yi for all i = 1, . . . , m.
Hence, for all x âˆˆFm
2 ,
ex

iâˆˆ{1,...,m}\J
(xi + ai) =
m

i=1
(xi + yi + 1)

iâˆˆ{1,...,m}\J
(xi + ai),

9
142
Chapter 9 â€¢ Reedâ€“Muller and Kerdock Codes
which will contain a factor x2
i + xi (and is therefore zero) unless ai = yi + 1 for all i âˆˆ
{1, . . . , m} \ J.
âŠ“âŠ”
To decode using this majority-logic decoding algorithm we perform at most 2m tests
k times, where k is the dimension of the code. This is less than n2 tests, where n is
the length of the code. Each test involves summing less than n terms, so the decoding
algorithm is completed in a number of steps which is polynomial in the length of the
code. This should be compared to syndrome decoding from â–·Chapter 4, which involved
searching through a look-up table with a number of entries which is exponential in
n. For this reason Reedâ€“Muller codes and the majority-logic decoding algorithm are
widely implemented. However, they do not give a sequence of asymptotically good
codes. Although the relative minimum distance is 2âˆ’r, which we can bound away from
zero by ï¬xing r, the transmission rate of R(r, m) is less than
r
n
log n
r

which tends to zero as n tends to inï¬nity.
9.3
Kerdock Codes
A codeword of R(2, m) \ R(1, m) is the evaluation of polynomials of the form
q(X) + â„“(X) or q(X) + â„“(X) + 1,
where â„“(X) is a linear form in m variables and
q(X) =

1â©½i<jâ©½m
aijXiXj
is a non-zero quadratic form.
If the quadratic form q(X) has maximum rank, then we will prove that, for all the
linear forms â„“(X), these codewords will have large weight. Therefore, if we can ï¬nd a
set of quadratic forms whose differences are quadratic forms of maximum rank, then the
distance between any two codewords will be large. In this section we will develop and
formalise this idea.
Let A = (aij) be the symmetric matrix deï¬ned by the symmetric bilinear form
b(X, Y) = q(X + Y) âˆ’q(X) âˆ’q(Y) =

1â©½i<jâ©½m
aij(XiYj + XjYi) = XtAY.
The rank of the bilinear form b(X, Y) is deï¬ned to be the rank of A.

9.3 Â· Kerdock Codes
143
9
Lemma 9.9 Suppose m is even. The evaluation of
m/2

i=1
X2iâˆ’1X2i
at the vectors of Fm
2 has 2mâˆ’1 + 2m/2âˆ’1 zeros.
Proof
There are 2m/2 zeros of the form (0, x2, 0, x4, . . . , 0, xm).
If x2iâˆ’1 Ì¸= 0 for some i = 1, . . . , m/2, then one of the x2i is determined by
m/2

i=1
x2iâˆ’1X2i = 0,
which gives 2m/2âˆ’1(2m/2 âˆ’1) zeros of this form, 2m/2âˆ’1 zeros for each non-zero vector
(x1, x3, . . . , xmâˆ’1).
Hence, there are precisely 2mâˆ’1 + 2m/2âˆ’1 zeros when evaluated at the vectors of Fm
2 . âŠ“âŠ”
We are going to construct codes whose codewords are the evaluation of the sum of
a quadratic form and a linear form. For this reason, we want to know the weights of the
vectors which are the evaluations of these Boolean functions.
Lemma 9.10 Suppose m is even, q(X) is a quadratic form and â„“(X) is a linear form. If
the bilinear form associated to q(X) has rank m, then the evaluation of q(X) + â„“(X) at the
vectors of Fm
2 has either 2mâˆ’1 + 2m/2âˆ’1 or 2mâˆ’1 âˆ’2m/2âˆ’1 zeros.
Proof
Dicksonâ€™s theorem, Exercise 9.2, implies that there is a basis of Fm
2 with respect to which
q(x) + â„“(X) is
m/2

i=1
(X2iâˆ’1X2i + a2iâˆ’1X2iâˆ’1 + a2iX2i).
This is equal to
m/2

i=1
(X2iâˆ’1 + a2i)(X2i + a2iâˆ’1) + b
for some b âˆˆF2. By Lemma 9.9, the evaluation of q(X) + â„“(X) has either 2mâˆ’1 + 2m/2âˆ’1
zeros or 2m âˆ’(2mâˆ’1 + 2m/2âˆ’1) zeros, depending on whether b = 0 or 1.
âŠ“âŠ”

9
144
Chapter 9 â€¢ Reedâ€“Muller and Kerdock Codes
Let K be a set of symmetric m Ã— m matrices over F2, which have zeros on the
diagonal, and which have the property that the matrix A âˆ’Aâ€² has rank m for all distinct
A, Aâ€² âˆˆK.
No two matrices in K can have the same ï¬rst row, since their difference is of rank
m. The entries on the diagonal of the matrices in K are zero, so the top-left entry of a
matrix in K is zero. Hence, we have that
|K| â©½2mâˆ’1.
For each A = (aij) âˆˆK, let
qA(X) =

1â©½i<jâ©½m
aijXiXj.
Let C(K) be the code whose codewords are the evaluation at the vectors of Fm
2 of
qA(X) + â„“(X) or qA(X) + â„“(X) + 1,
for all A âˆˆK and for all linear forms â„“(X).
Theorem 9.11
Suppose that m is even. The code C(K) is a binary block code of length 2m, size
|K||R(1, m)| and minimum distance 2mâˆ’1 âˆ’2m/2âˆ’1.
Proof
The distance between the evaluation of
qA(X) + â„“(X) + b
and
qAâ€²(X) + â„“â€²(X) + bâ€²
is the weight of the evaluation of
qAâˆ’Aâ€²(X) + â„“(X) âˆ’â„“â€²(X) + b âˆ’bâ€².
Since, A âˆ’Aâ€² has rank m, Lemma 9.10 implies that this distance is at least 2mâˆ’1 âˆ’2m/2âˆ’1.
âŠ“âŠ”
A Kerdock code is a code C(K) where |K| = 2mâˆ’1. Thus, for a Kerdock code, K
is of maximum size and the set K is called a Kerdock set. A Kerdock code is a binary

9.4 Â· Non-binary Reedâ€“Muller Codes
145
9
block code of length 2m, it has minimum distance 2mâˆ’1 âˆ’2m/2âˆ’1 and size 22m, i.e. it is
a (2m, 22m, 2mâˆ’1 âˆ’2m/2âˆ’1)2 code.
There are many non-equivalent Kerdock codes. Indeed, if m âˆ’1 is not prime, then
there are at least 2
âˆšm/2 inequivalent Kerdock codes of length 2m. However, a sequence
of Kerdock codes, whose lengths tend to inï¬nity, is asymptotically bad. Although the
relative minimum distance tends to 1
2, the transmission rate is 2m/2m, which tends to
zero.
Kerdock codes are of interest because they can be non-linear. The algebraic and
geometric nature of their construction allows for non-trivial decoding algorithms to be
implemented. The fact that Kerdock codes can be non-linear opens up the possibility of
constructing codes with parameter sets for which linear codes do not exist.
Example 9.12
Consider the set of 4 Ã— 4 matrices over F2
{
â›
âœâœâœâ
0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
â
âŸâŸâŸâ ,
â›
âœâœâœâ
0 1 0 0
1 0 0 0
0 0 0 1
0 0 1 0
â
âŸâŸâŸâ ,
â›
âœâœâœâ
0 0 1 0
0 0 0 1
1 0 0 1
0 1 1 0
â
âŸâŸâŸâ ,
â›
âœâœâœâ
0 0 0 1
0 0 1 1
0 1 0 1
1 1 1 0
â
âŸâŸâŸâ }.
This set of matrices can be extended to a set K of 8 matrices with the property that the
difference of any two matrices has rank 4, see Exercise 9.5. Thus, K is a Kerdock set and,
by Theorem 9.11, the binary Kerdock code C(K) is a (16, 256, 6)2 code. We proved in
Example 4.20 that there is no binary linear code with these parameters. This code is the
Nordstromâ€“Robinson code.
â– 
9.4
Non-binary Reedâ€“Muller Codes
Until now we have only considered Reedâ€“Muller codes over F2, but one can naturally
generalise the deï¬nition of a Reedâ€“Muller code over a general ï¬nite ï¬eld Fq. The
codewords of Rq(r, m) are the evaluations at the vectors of Fm
q of polynomials of degree
at most r in m variables, where the degree in any particular variable is at most q âˆ’1. The
number of vectors in an m-dimensional vector space over Fq is qm, so the length of the
linear code Rq(r, m) is qm. Its dimension is more difï¬cult to calculate, see Exercise 9.7.
In the following examples, we calculate the dimension for some speciï¬c cases and some
low weight codewords, which we will then go on and prove are of minimum non-zero
weight.
Example 9.13
Suppose r â©½q âˆ’1. The evaluation of any polynomial in m variables of degree at most r will
be a codeword of Rq(r, m). The set
{Xc1
1 Â· Â· Â· Xcm
m | c1 + Â· Â· Â· + cm â©½r}

9
146
Chapter 9 â€¢ Reedâ€“Muller and Kerdock Codes
is a basis for the space of polynomials in m variables of degree at most r. Hence, the
dimension of Rq(r, m) is
m + r
r

since this is the number of non-negative integer solutions to
c1 + Â· Â· Â· + cm â©½r.
Let g(X1) be a polynomial of degree r with r distinct roots in Fq. The evaluation of g is
a codeword with precisely rqmâˆ’1 zero coordinates; a zero coordinate being indexed by a
vector of Fm
q whose ï¬rst coordinate is a root of g. Therefore, Rq(r, m) has codewords of
weight (q âˆ’r)qmâˆ’1.
â– 
Example 9.14
The space of polynomials of degree at most 3 in three variables in which no variable has an
exponent larger than 2 has a basis
{1, X1, X2, X3, X2
1, X2
2, X2
3, X1X2, X1X3, X2X3,
X2
1X2, X2
1X3, X2
2X1, X2
2X3, X2
3X1, X2
3X2, X1X2X3}.
Therefore, the code R3(3, 3) is a 17-dimensional ternary linear code of length 27. We could
also have arrived at this conclusion by considering a monomial basis for all polynomials of
degree at most three in three variables and deleting X3
1, X3
2 and X3
3, see Exercise 9.7.
â– 
Suppose that r = a(q âˆ’1) + b, where 0 â©½b â©½q âˆ’2. If g(X1) is a polynomial of
degree b with b distinct roots in Fq, then the evaluation of
g(X1)(Xqâˆ’1
2
âˆ’1) Â· Â· Â· (Xqâˆ’1
a+1 âˆ’1),
a polynomial of degree r, is non-zero only when evaluated at
x = (x1, 0, . . . , 0, xa+2, . . . , xm)
for some x1 which is not a root of g. Therefore, Rq(r, m) has a codeword of weight
(q âˆ’b)qmâˆ’aâˆ’1.
We shall prove that this is the minimum weight of a non-zero codeword in the
following theorem, the proof of which is an example of a proof using the polynomial
method. This type of proof, which one sees often in combinatorics, attempts to
obtain bounds from the fact that the number of zeros of a non-zero polynomial is
bounded. The application of the method is often something like the following. Given

9.4 Â· Non-binary Reedâ€“Muller Codes
147
9
a combinatorial object, a polynomial is constructed in such a way that the properties
of the combinatorial object are translated into algebraic properties of the polynomial.
Usually we are interested in the zeros of the polynomial, often restricted to subsets of a
vector space. Here, the polynomial is directly given as the polynomial whose evaluation
is the codeword. By bounding from above the number of zeros of the polynomial, we
will bound from below the weight of the codeword.
Theorem 9.15
The minimum distance of Rq(r, m) is (q âˆ’b)qmâˆ’aâˆ’1, where r = a(q âˆ’1) + b and
0 â©½b â©½q âˆ’2.
Proof
By induction on m.
If m = 1, then the codewords are the evaluation of a polynomial of degree r â©½q âˆ’1 in
one variable. The polynomial has at most r zeros, so the codeword has weight at least q âˆ’r.
Observe that if r = q âˆ’1, then a = 1 and b = 0 and
(q âˆ’b)qmâˆ’aâˆ’1 = q/q = 1 = q âˆ’r.
Suppose that the codeword u âˆˆRq(r, m) is the evaluation of the polynomial
f (X) = f (X1, . . . , Xm).
We write f (X) as a polynomial in Xm, whose coefï¬cients are polynomials in X1, . . . , Xmâˆ’1.
Thus,
f (X) =
c

i=0
fi(X1, . . . , Xmâˆ’1)Xi
m,
where c is the degree of f (X) in the indeterminate Xm. Note that fc(X1, . . . , Xmâˆ’1) Ì¸â‰¡0
and
deg fc â©½deg f âˆ’c â©½r âˆ’c.
The codeword of Rq(r âˆ’c, m âˆ’1) which is the evaluation of fc has, by induction, at least
(q âˆ’bâ€²)qmâˆ’aâ€²âˆ’2
non-zero coordinates, where r âˆ’c = aâ€²(q âˆ’1) + bâ€² and 0 â©½bâ€² â©½q âˆ’2.
For any (x1, . . . , xmâˆ’1) such that fc(x1, . . . , xmâˆ’1) Ì¸= 0, there are at least q âˆ’c elements
of Fq for which f (x1, . . . , xmâˆ’1, Xm) is not zero. Hence, the codeword u has weight at least
(q âˆ’c)(q âˆ’bâ€²)qmâˆ’aâ€²âˆ’2.

9
148
Chapter 9 â€¢ Reedâ€“Muller and Kerdock Codes
It remains to prove that
(q âˆ’c)(q âˆ’bâ€²)qmâˆ’aâ€²âˆ’2 â©¾(q âˆ’b)qmâˆ’aâˆ’1.
The theorem then follows since, by Lemma 4.1, the minimum distance of a linear code is
equal to the minimum weight of a non-zero codeword.
If aâ€² â©½a âˆ’2, then this is clear, so we can assume aâ€² = a âˆ’1 or a.
Suppose aâ€² = a âˆ’1 and (q âˆ’c)(q âˆ’bâ€²) < q âˆ’b. This inequality implies bâ€² > b. We
have r = a(q âˆ’1) + b and r âˆ’c = aâ€²(q âˆ’1) + bâ€², so
c = (a âˆ’aâ€²)(q âˆ’1) + b âˆ’bâ€² = q âˆ’1 + b âˆ’bâ€².
Then (q âˆ’c)(q âˆ’bâ€²) < (q âˆ’b) implies (bâ€² âˆ’b + 1)(q âˆ’bâ€²) < q âˆ’b, a contradiction.
Suppose aâ€² = a and (q âˆ’c)(q âˆ’bâ€²) < q(q âˆ’b). We have r = a(q âˆ’1) + b and
r âˆ’c = aâ€²(q âˆ’1) + bâ€², so c = b âˆ’bâ€². Then (q âˆ’c)(q âˆ’bâ€²) < q(q âˆ’b) implies (q âˆ’b +
bâ€²)(q âˆ’bâ€²) < q(q âˆ’b) which implies b < bâ€² and c < 0, a contradiction.
âŠ“âŠ”
9.5
Comments
Reedâ€“Muller codes were introduced by Reed [59] and Muller [53] in the 1950s.
We have taken an algebraic rather than a geometric approach to the majority-logic
decoding algorithm. For a geometric description of the algorithm, see Van Lint [74] or
MacWilliams and Sloane [50].
Dicksonâ€™s classiï¬cation of quadratic form over ï¬elds of even characteristic is from
[22].
If m is odd, then there are examples of sets K for which Exercise 9.6 is a ( 1
2(m2 +
m) + 1 âˆ’rm)-dimensional binary linear code. The 11-dimensional codes (m = 5 and
r = 2) are the codes which caused a dispute between Apple and Samsung, referred to
in James Davisâ€™ lecture [20]. They can be found in Corollary 17 (m = 5, d = t = 2)
on page 455 of MacWilliams and Sloane [50].
Kerdock codes were ï¬rst considered by Kerdock in [44] in 1972. That there are an
exponential number of inequivalent Kerdock codes is proven by Kantor in [41]. Kantor
takes a geometric approach to Kerdock codes in the articles [42], a treatment of which
can be found in Chapter 12 of Cameron and van Lintâ€™s book [17]. The Nordstromâ€“
Robinson code is from [54]. Kerdock codes have applications to quantum mechanics,
see [15] and [18].
The non-binary Reedâ€“Muller codes were deï¬ned by various authors. Theorem 9.15
is attributed to Kasami, Lin and Peterson [43] in Bishnoi [11], where the proof given
here is adapted from.

9.6 Â· Exercises
149
9
9.6
Exercises
9.1 Suppose that we have sent a codeword of the code R(2, 4), the coordinates ordered as
in Example 9.1, and have received the vector
(0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1).
i.
Decode the received vector using syndrome decoding.
ii.
Decode the received vector using majority-logic decoding.
9.2 Suppose that q(X) is a quadratic form of rank m of the type
q(X) =

1â©½i<jâ©½m
qijXiXj.
Prove that there is a basis of Fm
2 , with respect to which, q(X) is
m/2

i=1
X2iâˆ’1X2i.
9.3
i.
Prove that we can select half the codewords of R(1, m) so that the 2m Ã— 2m matrix H,
whose rows are the selected codewords with zeros changed to minus one, has the property
that HHt = 2mI, where I is the 2m Ã— 2m identity matrix.
ii.
Prove that for each vector v âˆˆF2m
2
there is a codeword u of R(1, m) such that d(u, v) â©½
2mâˆ’1 âˆ’2m/2âˆ’1.
9.4 Prove that the Kerdock code C(K) of length 2m is linear if and only if the Kerdock set
K is a subspace of the vector space of m Ã— m matrices.
9.5 Complete the set of matrices to a Kerdock set K of eight matrices and prove that C(K)
is a non-linear (16, 256, 16) code.
{
â›
âœâœâœâ
0 0 0 0
0 0 0 0
0 0 0 0
0 0 0 0
â
âŸâŸâŸâ ,
â›
âœâœâœâ
0 1 0 0
1 0 0 0
0 0 0 1
0 0 1 0
â
âŸâŸâŸâ ,
â›
âœâœâœâ
0 0 1 0
0 0 0 1
1 0 0 1
0 1 1 0
â
âŸâŸâŸâ ,
â›
âœâœâœâ
0 0 0 1
0 0 1 1
0 1 0 1
1 1 1 0
â
âŸâŸâŸâ }
9.6
i.
Prove that the evaluation of
q(X) =
r

i=1
X2iâˆ’1X2i

9
150
Chapter 9 â€¢ Reedâ€“Muller and Kerdock Codes
at the vectors of Fm
2 , has 2mâˆ’1 + 2mâˆ’râˆ’1 zeros.
ii.
Prove that the evaluation of q(X) + â„“(X), where â„“(X) is a linear form and q(X) is a
quadratic form whose associated bilinear form is of rank 2r, at the vectors of Fm
2 , has
either 2mâˆ’1 + 2mâˆ’râˆ’1, 2mâˆ’1 or 2mâˆ’1 âˆ’2mâˆ’râˆ’1 zeros.
iii.
Suppose that K is a set of mÃ—m symmetric matrices over F2 with the property that A+
Aâ€² has rank at least 2r for all A, Aâ€² âˆˆK. Construct a (2m, |K|2m+1, 2mâˆ’1 âˆ’2mâˆ’râˆ’1)2
code.
iv.
Use iii. to construct a [32, 11, 12]2 code.
v.
Construct a linear code with the same parameters from the code of length 31 constructed
in Exercise 5.6.
9.7
i.
By ï¬nding a monomial basis for the space of polynomials in 3 variables of degree at
most 4, in which the degree of each variable is at most 2, calculate the dimension of
R3(4, 3).
ii.
Prove that if r â©½q âˆ’1, then the dimension of Rq(r, m) is
r

i=0
m + i âˆ’1
m âˆ’1

.
iii.
Prove that the dimension of Rq(r, m) is
r

i=0
m

j=0
(âˆ’1)j
m + i âˆ’1 âˆ’jq
m âˆ’1
m
j

.

151
10
p-Adic Codes
The p-adic numbers were ï¬rst considered by Hensel in the 19th century. He observed
that the primes play an analogous role in the integers as linear polynomials do in C[X].
The Laurent expansion of a rational function led him to consider the p-adic expansion
of a rational number. In this chapter, for a ï¬xed prime p, we will construct block codes
over the rings Z/phZ simultaneously, by constructing codes over the p-adic numbers
and then considering the coordinates modulo ph. These codes will be linear over the
ring but when mapped to codes over Z/pZ will result in codes which are not equivalent
to linear codes. We start with a brief introduction to p-adic numbers, which will cover
enough background for our purposes. The classical cyclic codes, that we constructed
in â–·Chapter 5, lift to cyclic codes over the p-adic numbers. In the case of the cyclic
Hamming code, this lift extends to a code over Z/4Z which, when mapped to a binary
code, gives a non-linear code with a set of parameters for which no linear code exists.
10.1 p-Adic Numbers
Let p be a prime.
The set of p-adic integers, which is denoted by Zp, is the set of sequences,
a = (a1, a2, a3, . . .),
where ai âˆˆZ/piZ for all i âˆˆN and
aj+1 â‰¡aj (mod pj).
An ordinary integer n âˆˆZ is an element of Zp deï¬ned by the sequence
aj â‰¡n (mod pj).
Â© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4_10

10
152
Chapter 10 â€¢ p-Adic Codes
The sequence deï¬ned by
aj+1 = aj + pj,
j âˆˆN, is a p-adic integer which is not an ordinary integer.
For example, with a1 = 3 and p = 5, this sequence begins
(3, 8, 33, 158, 783, . . .).
We deï¬ne addition and multiplication on the sequences component-wise, so
a + b = (a1, a2, a3, . . .) + (b1, b2, b3, . . .) = (a1 + b1, a2 + b2, a3 + b3, . . .).
To verify a + b âˆˆZp, observe that
aj+1 + bj+1 â‰¡aj + bj (mod pj).
Similarly,
ab = (a1, a2, a3, . . .)(b1, b2, b3, . . .) = (a1b1, a2b2, a3b3, . . .).
To verify ab âˆˆZp, observe that
aj+1bj+1 â‰¡ajbj (mod pj).
With these deï¬nitions multiplication is distributive with respect to addition, so Zp is a
ring and has a multiplicative identity element
1 = (1, 1, 1, . . .).
If a is a sequence for which a1 = 0, then a does not have a multiplicative inverse, so Zp
is not a ï¬eld. It is, however, an integral domain (xy = 0 implies either x = 0 or y = 0),
so it has a quotient ï¬eld. This quotient ï¬eld is called the ï¬eld of p-adic numbers and is
denoted Qp. Elements of Qp are called p-adic numbers.
All non-zero elements of Zp can be written as the product of a unit times some
non-negative power of p.
For example, the 5-adic integer
(0, 15, 40, 290, 915, . . .) = 5(3, 8, 58, 183, . . . , ),
since 15 â‰¡0 modulo 5, 40 â‰¡15 modulo 25, 290 â‰¡40 modulo 125, etc.
The ï¬eld Qp consists of the sequences where we allow negative powers of p as well.
For example,
5âˆ’2(2, 22, 97, 222, . . .),

10.2 Â· Polynomials over the p-Adic Numbers
153
10
is a 5-adic number.
The product of pÎ±(a1, a2, a3, . . .) and pÎ²(b1, b2, b3, . . .) is
pÎ±+Î²(a1b1, a2b2, a3b3, . . .).
Returning to the previous examples,
5(3, 8, 58, 183, . . . , )5âˆ’2(2, 22, 97, 222, . . .) = 5âˆ’1(1, 1, 1, 1, . . .).
10.2 Polynomials over the p-Adic Numbers
Let Qp denote an algebraic closure of Qp. Recall that, since Qp is an algebraic closure,
the polynomials of positive degree over Qp factorise into linear factors over Qp. The
following lemma is a straightforward application of the binomial theorem.
Lemma 10.1 If Î±, Î² âˆˆQp and
Î± â‰¡Î² (mod pr)
then
Î±p â‰¡Î²p (mod pr+1).
Proof
We can write Î± = Î² + prÎ³ , for some Î³ âˆˆZp. Then
Î±p = (Î² + prÎ³ )p â‰¡Î²p (mod pr+1).
âŠ“âŠ”
In â–·Chapter 2 we studied how to factorise cyclotomic polynomials over ï¬nite ï¬elds
and put this to use in â–·Chapter 5 while constructing cyclic codes. The following
theorem tells us that a factorisation over Fp â€œliftsâ€ to a factorisation over the p-adic
numbers. As in â–·Chapter 5, we will exploit this factorisation to construct cyclic codes
and their extensions with some surprising results.
Theorem 10.2
Let p be a prime and let n be a positive integer which is not a multiple of p. If h is a
monic irreducible divisor of Xnâˆ’1 in (Z/pZ)[X], then there exists a monic irreducible
polynomial hâˆin Zp[X] which divides Xn âˆ’1 and is congruent to h modulo p.

10
154
Chapter 10 â€¢ p-Adic Codes
Proof
By induction on r, we will ï¬nd a polynomial hr(X) âˆˆ(Z/prZ)[X] such that hr(X) divides
Xn âˆ’1 and hr â‰¡h modulo p. Then hâˆwill be the polynomial hr as r â†’âˆ.
An element c âˆˆZ/prZ can be extended to an element of Zp by taking the sequence
(c1, c2, . . . , crâˆ’1, c, c, c, . . .),
where ci = c mod pi, for i = 1, . . . r âˆ’1. Therefore, the coefï¬cients of hr(X) can be viewed
as elements of Zp and therefore as elements of Qp.
Since n is not a multiple of p, the roots of h1(X) in Qp are distinct. By induction, we
can assume that the roots of hr(X) are distinct.
For each root Î± of hr(X) (in Qp),
Î±n â‰¡1 (mod pr).
Let
f (X) = hr(X) + prg(X),
for some polynomial g(X) âˆˆZp[X].
For each root Î² of f , there is a root Î± of hr(X) such that
Î² â‰¡Î± (mod pr).
Then, by Lemma 10.1,
Î²p â‰¡Î±p (mod pr+1).
Lemma 10.1 also implies that
Î±np â‰¡1 (mod pr+1)
from which we deduce that
Î²np â‰¡1 (mod pr+1).
Let
hr+1(X) =

(X âˆ’Î²p),
where the product runs over the roots Î² of f .
Then hr+1 divides Xn âˆ’1 modulo pr+1. Since
Î²p â‰¡Î±p â‰¡Î± (mod p),

10.3 Â· p-Adic Codes
155
10
hr+1 and hr have the same roots modulo p.
Thus, the roots of hr+1 are distinct and
hr+1 â‰¡hr (mod p).
âŠ“âŠ”
10.3 p-Adic Codes
Let R be a commutative ring with multiplicative identity 1. An R-module M is a
commutative group with a left multiplication from R Ã— M â†’M satisfying Î»(u + v) =
Î»u + Î»v, (Î» + Î¼)u = Î»u + Î¼u, (Î»Î¼)u = Î»(Î¼u) and 1u = u, for all u, v âˆˆM and all
Î», Î¼ âˆˆR.
The set Zn
p of n-tuples over the p-adic integers is a commutative group with respect
to addition. We deï¬ne left multiplication of an element (u1, . . . , un) âˆˆZn
p by an element
Î» âˆˆZp as
Î»(u1, . . . , un) = (Î»u1, . . . , Î»un).
This scalar multiplication satisï¬es Î»(u + v) = Î»u + Î»v, (Î» + Î¼)u = Î»u + Î¼u,
(Î»Î¼)u = Î»(Î¼u) and 1u = u, for all u, v âˆˆZn
p and all Î», Î¼ âˆˆZp. Thus, with this
scalar multiplication Zn
p is a Zp-module.
A submodule C of Zn
p is a non-empty subset of Zn
p which is closed under linear
combinations. In other words,
Î»u + Î¼v âˆˆC,
for all u, v âˆˆC and all Î», Î¼ âˆˆZp.
We now re-deï¬ne the analogous objects that we saw for linear codes over a ï¬eld for
codes over Zp. A p-adic code of length n is a subset of Zn
p. A linear code over Zp is a
submodule of Zn
p.
A generator matrix for a linear code C over Zp is a kÃ—n matrix G with the property
that
C = {(u1, . . . , uk)G| (u1, . . . , uk) âˆˆZk
p}.
We deï¬ne the scalar product on Zn
p as the standard inner product
u Â· v = u1v1 + Â· Â· Â· + unvn.
The dual code of a linear code C is deï¬ned, as in the case of a linear code over a
ï¬nite ï¬eld, as
CâŠ¥= {v âˆˆZn
p | u Â· v = 0 for all u âˆˆC}.

10
156
Chapter 10 â€¢ p-Adic Codes
A linear code C is cyclic if
(c1, c2, . . . , cn) âˆˆC
implies
(cn, c1, . . . , cnâˆ’1) âˆˆC.
A codeword of the cyclic code corresponds to a polynomial in the ring Zp[X]/(Xn âˆ’1)
under the correspondence
(c1, c2, . . . , cn) â†’c1 + c2X + Â· Â· Â· + cnXnâˆ’1.
As in the case of ï¬nite ï¬elds, under this correspondence, a cyclic code is an ideal âŸ¨gâŸ©,
where g is some divisor of Xn âˆ’1.
Example 10.3
The polynomial X3+X+1 divides X7âˆ’1 in (Z/2Z)[X]. Theorem 10.2 implies the existence
of a polynomial in Z2[X] which divides X7 âˆ’1. One can verify that
g(X) = X3 + Î»X2 + (Î» âˆ’1)X âˆ’1
divides X7 âˆ’1 in Z2[X] if and only if Î»2 âˆ’Î» + 2 = 0 by observing that
X7 âˆ’1 = (X3 + Î»X2 + (Î» âˆ’1)X âˆ’1)(X3 + (1 âˆ’Î»)X2 âˆ’Î»X âˆ’1)(X âˆ’1).
To calculate Î», suppose
Î» = (a1, a2, a3, . . .).
Since a1 âˆˆZ/2Z, we have a1 = 0 or 1.
If a1 = 0, then substituting Î» â‰¡0 + 2a2 (mod 4) in
Î»2 âˆ’Î» + 2 â‰¡0 (mod 4)
implies
âˆ’2a2 + 2 â‰¡0 (mod 4),
so a2 = 1 and Î» â‰¡2 (mod 4).
Substituting Î» â‰¡2 + 4a3 (mod 8) in
Î»2 âˆ’Î» + 2 â‰¡0 (mod 8)

10.4 Â· Codes over Z/phZ
157
10
implies
4 âˆ’2 âˆ’4a3 + 2 â‰¡0 (mod 8),
so a3 = 1 and Î» â‰¡6 (mod 8).
Continuing in this way we deduce that one of the roots of Î»2 âˆ’Î» + 2 is
Î» = (0, 2, 6, 6, 6, 38, 38, 166, 422, . . .).
The cyclic code âŸ¨gâŸ©is a 2-adic linear code of length 7 with generator matrix
G =
â›
âœâœâœâ
âˆ’1 Î» âˆ’1
Î»
1
0
0 0
0
âˆ’1
Î» âˆ’1
Î»
1
0 0
0
0
âˆ’1
Î» âˆ’1
Î»
1 0
0
0
0
âˆ’1
Î» âˆ’1 Î» 1
â
âŸâŸâŸâ .
â– 
To make use of these p-adic codes, we will now consider the coordinates of the
codewords of a p-adic code modulo ph for some h. The resulting code will be a code
deï¬ned over the ï¬nite alphabet Z/phZ. We will use the matrix G from Example 10.3 in
Example 10.9.
10.4 Codes over Z/phZ
A linear code over Z/phZ is a (Z/phZ)-submodule of (Z/phZ)n. As in the case for a
linear code over Fq, we deï¬ne a generator matrix for a linear code C over (Z/phZ)n
as a r Ã— n matrix G with the property that
C = {(u1, . . . , ur)G| (u1, . . . , ur) âˆˆ(Z/phZ)r}.
If all the elements in the i-th row of G are divisible by pj, then we can restrict ui to
Z/phâˆ’jZ.
Example 10.4
Let
G =
â›
âœâ
1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0 1 1 1 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8
0 0 3 6 3 6 3 6 3 6 3 6 3 6 3 6 3 6 3 6
â
âŸâ ,
where the elements of G are from Z/9Z.

10
158
Chapter 10 â€¢ p-Adic Codes
The code generated by the matrix G is
C = {(u1, u2, u3)G | u1, u2 âˆˆZ/9Z, u3 âˆˆZ/3Z}.
Thus, the code C is a 9-ary code of length 20 of size 243.
The codeword
(3, 0, 1)G = (3, 0, 3, 6, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0)
and the all-zero codeword differ in 11 coordinates, so the minimum distance is at most 11. It
is Exercise 10.3 to verify that the minimum distance is 11.
â– 
Theorem 10.5
After a suitable permutation of the coordinates, a linear code C over (Z/phZ)n has a
generator matrix of the form
G =
â›
âœâœâœâœâœâœâœâœâœâœâ
I A01 A02
A03
Â· Â· Â·
A0,hâˆ’1
A0,h
0 pI pA12 pA13 Â· Â· Â· pA1,hâˆ’1
pA1,h
0
0
p2I p2A23 Â· Â· Â· p2A2,hâˆ’1
p2A2,h
... ...
...
...
...
Â· Â· Â·
...
... Â· Â· Â·
...
...
...
...
...
0
.
Â· Â· Â·
0
0
phâˆ’1I
phâˆ’1Ahâˆ’1,h
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
.
If the block sizes of the columns are k0, k1, . . . , kh (necessarily summing to n), then
|C| = pk,
where
k =
hâˆ’1

i=0
(h âˆ’i)ki.
Proof
Applying elementary row operations to the matrix does not change the code C generated
by the matrix. Since we are also allowed to permute the columns the only impediment to
obtaining a generator matrix of the form

I B01

,

10.4 Â· Codes over Z/phZ
159
10
is rows in which all elements are divisible by p. Thus, we obtain a generator matrix of the
form
G =

I B01
0 pB02

,
for some matrices B01 and B02. We continue applying row operations and column permuta-
tions. Again, the only impediment to obtaining a generator matrix of the form

I B01 B02
0 pI pB12

,
is rows in which all elements are divisible by p2.
Therefore, there is a generator matrix for C of the form
G =
â›
âœâ
I B01
B02
0 pI
pB12
0
0
p2B22
â
âŸâ .
The form of G follows by continuing applying row operations and column permutations.
The code generated by G is
C = {(u1, . . . , ur)G | ui âˆˆZ/phZ}.
If all the entries in the â„“-th row of G are divisible by pj, then we can restrict uâ„“to Z/phâˆ’jZ,
which implies that the size of the code is as claimed.
âŠ“âŠ”
Example 10.6
Consider the code over Z/8Z generated by the matrix

0 2 1 4 1 1
4 6 7 4 7 1

.
By shifting the coordinates one coordinate to the right, we obtain an equivalent code with
generator matrix

1 0 2 1 4 1
1 4 6 7 4 7

.
Subtracting the ï¬rst row from the second, we obtain a generator matrix for the same code

1 0 2 1 4 1
0 4 4 6 0 6

.

10
160
Chapter 10 â€¢ p-Adic Codes
Multiplying the second row by 3 we obtain another generator matrix for the code

1 0 2 1 4 1
0 4 4 2 0 2

.
Finally, interchanging the second and sixth column we obtain an equivalent code with
generator matrix

1 1 2 1 4 0
0 2 4 2 0 4

.
Comparing this to the claim of Theorem 10.5, the matrix A01 = (1), the matrix A02 =
(2 1 4 0) and the matrix A12 = (2 1 0 2).
Note that the code has size 32 and not 64, which is not immediately apparent from the
initial generator matrix.
â– 
10.5 Codes over Z/4Z
The Gray map is a map Î³ from Z/4Z to {0, 1}2 deï¬ned by
x
0
1
2
3
Î³ (x)
(0, 0)
(0, 1)
(1, 1)
(1, 0) .
We extend the Gray map to a map from (Z/4Z)n to {0, 1}2n by applying Î³ to each
coordinate.
If C is a block code of length n over Z/4Z, then Î³ (C), deï¬ned by
Î³ (C) = {Î³ (v) | v âˆˆC},
is a binary code of length 2n. It is immediate that if C has minimum distance d, then
Î³ (C) has minimum distance at least d.
However, there is a possibility that the minimum distance of Î³ (C) is larger than d.
Example 10.7
Let C be the code over Z/4Z generated by the matrix

1 0 2 1 1 1
0 2 2 2 0 0

.
The 8 codewords of C and the code Î³ (C) are

10.5 Â· Codes over Z/4Z
161
10
C
Î³ (C)
(0,0,0,0,0,0)
(0,0,0,0,0,0,0,0,0,0,0,0)
(1,0,2,1,1,1)
(0,1,0,0,1,1,0,1,0,1,0,1)
(0,2,2,2,0,0)
(0,0,1,1,1,1,1,1,0,0,0,0)
(1,2,0,3,1,1)
(0,1,1,1,0,0,1,0,0,1,0,1)
(2,0,0,2,2,2)
(1,1,0,0,0,0,1,1,1,1,1,1)
(2,2,2,0,2,2)
(1,1,1,1,1,1,0,0,1,1,1,1)
(3,0,2,3,3,3)
(1,0,0,0,1,1,1,0,1,0,1,0)
(3,2,0,1,3,3)
(1,0,1,1,0,0,0,1,1,0,1,0)
One readily checks that the minimum distance of C is 3 and the minimum distance of
Î³ (C) is 6.
â– 
The Lee distance between two elements u and v of (Z/4Z)n is deï¬ned as the
Hamming distance between Î³ (u) and Î³ (v). The Lee weight of an element u of (Z/4Z)n
is the Lee distance between u and the all zero n-tuple.
Lemma 10.8 Let C be a linear code over Z/4Z. The minimum Lee weight of a non-zero
codeword of C is equal to the minimum distance of Î³ (C).
Proof
Let u = (u1, . . . , un) and v = (v1, . . . , vn) be two codewords of C.
By checking all possibilities for ui, vi âˆˆZ/4Z, one can verify that the distance between
Î³ (ui) and Î³ (vi) is equal to the distance between (0, 0) and Î³ (ui âˆ’vi).
Thus,
d(Î³ (u), Î³ (v)) =
n

i=1
d(Î³ (ui), Î³ (vi)) =
n

i=1
d(Î³ (ui âˆ’vi), (0, 0))
which is equal to the Lee weight of u âˆ’v.
âŠ“âŠ”
In the following example, we return to Example 10.3 and consider the entries in the
matrix modulo 4. This matrix will then generate a code over Z/4Z.
Example 10.9
By Example 10.3, we have that X3 + 2X2 + X + 3 divides X7 âˆ’1 in (Z/4Z)[X]. This
polynomial generates a cyclic code of length 7 which extends to a code of length 8 with
generator matrix
G =
â›
âœâœâœâ
3 1 2 1 0 0 0 1
0 3 1 2 1 0 0 1
0 0 3 1 2 1 0 1
0 0 0 3 1 2 1 1
â
âŸâŸâŸâ .

10
162
Chapter 10 â€¢ p-Adic Codes
Let C be the Z/4Z-linear code of length 8 with 256 codewords deï¬ned by
C = {uG | u âˆˆ(Z/4Z)4}.
The code Î³ (C) is a binary code of length 16 with 256 codewords. By Exercise 10.7,
the minimum distance of Î³ (C) is 6. This code is equivalent to the code constructed in
Example 9.12. As mentioned there, an important observation is that there is no binary linear
code with these parameters, which we proved in Example 4.20.
â– 
Example 10.9 suggests that codes over rings may be a good place to look for non-
linear codes which have better parameter sets than linear codes. It may be the case that
we need to consider non-linear codes to disprove Conjecture 3.18.
10.6 Comments
This chapter leans somewhat on the enlightening article by Calderbank and Sloane on
p-adic codes [14]. Carlet [19] has generalised the Gray map to a bijection from Z/2kZ
to R(1, k âˆ’1). This can be extended to (Z/2kZ)n and can therefore be used to construct
(non-linear) binary codes from Z/2kZ-linear codes.
Theorem 10.2 is a special case of Henselâ€™s lifting lemma. For more on p-adic
numbers, including the lifting lemma, see [30].
10.7 Exercises
10.1 Let
Î» = (1, b2, b3, b4, . . .)
be the 2-adic integer which is a root of X2 âˆ’X + 2. Calculate the numbers b2, b3, b4 in the
sequence of Î».
10.2 Prove that the code generated by the 4 Ã— 8 matrix obtained by extending the generator
matrix in Example 10.3 with the all-one vector is a self-dual code.
10.3 Check, with the aid of a computer or not, that the code in Example 10.4 has minimum
distance 11.
10.4
i.
Prove that the dual code CâŠ¥, to the code C generated by the matrix in Theo-
rem 10.5, has a generator matrix of the form

10.7 Â· Exercises
163
10
G =
â›
âœâœâœâœâœâœâœâœâœâœâ
B0,h
B0,hâˆ’1
Â· Â· Â·
B03
B02 B01 I
pB1,h
pB1,hâˆ’1 Â· Â· Â· pB13 pB12 pI 0
p2B2,h
p2B2,hâˆ’1 Â· Â· Â· p2B23 p2I
0
0
.
.
.
...
...
.
.
.
.
...
...
.
.
.
phâˆ’1Bhâˆ’1,h
phâˆ’1I
0
. . .
. . .
. . . 0
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
,
for some matrices Bij, where the blocks of columns have the same size as in Theo-
rem 10.5.
ii.
Prove that |CâŠ¥| = pkâŠ¥, where
kâŠ¥=
h

i=1
iki.
10.5 Let C be a linear code over Z/phZ. Prove that (CâŠ¥)âŠ¥= C.
10.6
Let C be the linear code over Z/4Z from Example 10.7.
i.
Check that the minimum Lee weight of a non-zero codeword of C is 6 and verify that the
minimum Hamming distance between any two codewords of Î³ (C) is 6.
ii.
The code Î³ (C) is a non-linear binary code of length 12, minimum distance 6 and size
8. Construct a linear code with the same parameters.
iii.
The code
C = {Î»u + 2Î¼v | Î» âˆˆZ/4Z, Î¼ âˆˆZ/2Z}
for some u âˆˆ(Z/4Z)6 and v âˆˆ(Z/2Z)6, where the weight of v is 3. Construct a code
with the same parameters as C in which the weight of v is 4.
10.7
i.
Prove, using row operations, that the code C in Example 10.9 has a generator matrix
G = G1 + 2G2,
where
G1 =
â›
âœâœâœâ
1 0 0 0 0 1 1 1
0 1 0 0 1 0 1 1
0 0 1 0 1 1 0 1
0 0 0 1 1 1 1 0
â
âŸâŸâŸâ 

10
164
Chapter 10 â€¢ p-Adic Codes
and
G2 =
â›
âœâœâœâ
0 0 0 0 1 1 0 0
0 0 0 0 0 1 1 0
0 0 0 0 0 0 1 1
0 0 0 0 1 0 0 1
â
âŸâŸâŸâ .
ii.
Prove, using Lemma 10.8, that the code Î³ (C) in Example 10.9 has minimum distance 6.
10.8
i.
Prove that X2 + Î»X âˆ’1 divides X8 âˆ’1 in Zp[X], where Î» is a p-adic integer satisfying
Î»2 = âˆ’2.
ii.
Calculate the next few numbers in the sequences (1, 4, . . .) and (2, 5, . . .) which are both
solutions of Î»2 = âˆ’2 in Z3.
10.9
i.
Prove that X5 + Î»X4 âˆ’X3 + X2 + (Î» âˆ’1)X âˆ’1 divides X11 âˆ’1 in Zp[X], where Î» is
a p-adic integer satisfying Î»2 = Î» âˆ’3.
ii.
Calculate the ï¬rst few numbers in the sequences which are solutions of Î»2 = Î»âˆ’3 in Z3.
10.10
i.
Prove that X11 + Î»X10 + (Î» âˆ’3)X9 âˆ’4X8 âˆ’(Î» + 3)X7 âˆ’(2Î» + 1)X6 âˆ’(2Î» âˆ’3)X5 âˆ’
(Î» âˆ’4)X4 + 4X3 + (Î» + 2)X2 + (Î» âˆ’1)X âˆ’1 divides X23 âˆ’1 in Zp[X], where Î» is a
p-adic integer satisfying Î»2 = Î» âˆ’6.
ii.
Calculate the ï¬rst few numbers in the sequences which are solutions of Î»2 = Î»âˆ’6 in Z2.

165
Hints and Answers to Selected Exercises
1.1 Hr(X) = logr(223/1835/35âˆ’5/18) â‰ˆlogr 9.6759.
1.2 H2(X) = n âˆ’log2(2n âˆ’1) + (1 âˆ’2âˆ’n)(2 âˆ’21âˆ’n âˆ’n2âˆ’n).
1.3
i.
n
j

2nâˆ’j.
ii. H2(X) = 3
2.
1.4 Ï†hr(p).
1.5 Hr(Y) = m(1 âˆ’logr m), where m = (Ï†r + 1 âˆ’2Ï†)/(r âˆ’1).
I(X, Y) = m(1 âˆ’logr m) + Ï† logr Ï† + r logr((1 âˆ’Ï†)/(r âˆ’1)).
1.6 Ï†.
1.8
i. H(Y|X) = âˆ’(1 âˆ’p)( 3
4 logr 3 âˆ’logr 4).
H(Y) = hr(p) âˆ’(1 âˆ’p)( 3
4 logr 3 âˆ’logr 4).
H(X) = hr(p).
ii. logr 2.
1.9
i.
âˆ’3
4p log 3 + 3
2p log 2 âˆ’

4âˆ’3p
4

log

4âˆ’3p
8

.
iii.
I(X, Y) evaluated at p = 4/(28/3 + 3).
2.5
i.
1, 2, 4, 4, 4.
ii.
1, 11, 11.
iii.
1, 1, 1, 1, 2, 2, 2, 2.
2.6
i.
(X âˆ’1)(X âˆ’2)(X âˆ’4)(X3 âˆ’2)(X3 âˆ’4).
ii.
(X âˆ’1)(X10 + X9 + X8 + X7 + X6 + X5 + X4 + X3 + X2 + X + 1).
iii.
(X âˆ’1)(X + 1)(X2 + 1)(X2 + 5X âˆ’1)(X2 âˆ’5X âˆ’1).
2.12 Hint: Let U be a k-dimensional subspace of Fk+1
q
, so U is a hyperplane of
PG(k, q). Let V be an (r+1)-dimensional subspace of Fk+1
q
and observe that v+(V âˆ©U),
where v âˆˆV \ U, is a coset of a subspace of U âˆ¼= Fk
q.
Â© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4

166
Hints and Answers to Selected Exercises
3.4 Hint: Assuming (0, 0, 0, 0, 0, 0, 0, 0, 0, 0) âˆˆC, the non-zero vectors all have
weight 6. Look at the proof of Lemma 3.10.
The codewords are the six rows of the matrix
â›
âœâœâœâœâœâœâœâ
0 0 0 0 0 0 0 0 0 0
1 1 1 1 1 1 0 0 0 0
1 1 1 0 0 0 1 1 1 0
1 0 0 1 1 0 1 1 0 1
0 1 0 1 0 1 1 0 1 1
0 0 1 0 1 1 0 1 1 1
â
âŸâŸâŸâŸâŸâŸâŸâ 
.
3.5 Hint: By considering the proof of Lemma 3.10, observe that equality in the bound
cannot occur.
3.6 Hint: follow the same strategy as Exercise 3.5.
3.12
ii.
The vector Ïƒ(u) has n âˆ’wt(u) ones and wt(u) minus ones, where wt(u) is the
number of ones that u has.
iii.
If u and v agree on a coordinate, then that coordinate contributes 1 to the scalar
product Ïƒ(u) Â· Ïƒ(v). If u and v differ on a coordinate, then that coordinate
contributes âˆ’1 to the scalar product Ïƒ(u) Â· Ïƒ(v). They agree on n âˆ’d(u, v)
coordinates and differ on d(u, v) coordinates.
iv.
Using ii. and iii. and the fact that d(u, v) â©¾d for all u, v âˆˆC, the bound follows.
v.
The bound on w ensures that the scalar product in iv. is non-positive, with Î» =
1
2
âˆš1 âˆ’2(d/n). Assuming that (0, 0, . . . , 0) âˆˆC, we have by i. and iv. that the
number of codewords at a distance at most w from (0, 0, . . . , 0) is at most 2n.
4.3 Prove there exists a [nâˆ’k +r, r, d]q code for r = 1, . . . , k, by induction on r. Note
that there is a [n âˆ’k + 1, 1, d]q code, since the condition implies d â©½n âˆ’k + 1. The
condition implies the condition for n replaced by n âˆ’1 and k replaced by k âˆ’1 so, by
induction, there exists a [n âˆ’1, k âˆ’1, d]q code. Let H be a (n âˆ’k) Ã— (n âˆ’1) check
matrix for this code. The condition allows us to add a column to H so that any d âˆ’1
columns of the extended matrix are linearly independent, i.e. there is a non-zero vector
of Fnâˆ’k
q
which is not a linear combination of any subset of at most d âˆ’2 columns of H.
Apply Lemma 4.4.
4.4 Hint: Since the code is self-dual it has no codewords of odd weight.
The set of equations in matrix form is

Hints and Answers to Selected Exercises
167
â›
âœâœâœâœâœâœâœâœâœâœâœâœâœâœâ
âˆ’15
1
1
1
1
8
4
0
âˆ’4
âˆ’8
28 âˆ’12 âˆ’4
4
8
56
âˆ’4
0
4
âˆ’56
70 âˆ’10 âˆ’10 âˆ’10 70
56
âˆ’4
0
4
âˆ’56
28
4
âˆ’4 âˆ’12
8
8
4
0
âˆ’4
âˆ’8
1
1
1
1
âˆ’15
â
âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâ 
â›
âœâœâœâœâœâ
1
a2
a4
a6
a8
â
âŸâŸâŸâŸâŸâ 
= 0.
4.5 The sent vector is (1, 1, 1, 0, âˆ’1, âˆ’1, âˆ’1, 0).
4.6 A(X) = 1 + 4X3 + 3X4.
4.7 The sent vector is (1, 2, 3)G.
4.8
i. Prove that every 4 Ã— 4 sub-matrix of H is non-singular.
ii. The sent vector is (1, 6, 3, 6, 1, 2, 2).
4.9 The sent vector is (1, 2, 1, 2, 0, 2, 0, 2, 0).
4.14 For example,
A =
â›
âœâœâœâœâœâœâœâ
1 1
1
0
1 1
0
1
1 0
1
1
0 1
1
1
1 1 âˆ’1 âˆ’1
1 âˆ’1 1 âˆ’1
â
âŸâŸâŸâŸâŸâŸâŸâ 
.
4.15 Let u and v be two codewords of weight w with the same support. For all non-zero
Î» âˆˆFq the vector Î»v is a codeword. For each non-zero coordinate ui of u there is a Î»
such that ui = Î»vi, where vi is the i-th coordinate of v. By the pigeon-hole principle,
there is a Î» such that âŒˆw/(q âˆ’1)âŒ‰of the coordinates agree with the corresponding
coordinate in Î»v. Then u âˆ’Î»v is a codeword of weight at most w âˆ’âŒˆw/(q âˆ’1)âŒ‰.
4.16
i.
Hint: Make the substitution X â†’1 âˆ’X in the MacWilliams identities.
ii.
Hint: The subspace of polynomials spanned by
{Xj(1 + (q âˆ’1)(1 âˆ’X))nâˆ’j | j = d, . . . , n}
is equal to the subspace of polynomials spanned by
{Xj | j = d, . . . , n}.

168
Hints and Answers to Selected Exercises
iii.
Applying ii., there is a unique way to write the polynomial (1 + (q âˆ’1)(1 âˆ’X))n
as a linear combination of the n + 1 polynomials in ii.
iv.
Apply Theorem 4.13.
5.1 Hint: Since C is self-dual, all codewords have weight which are multiples of 3.
Using this observation, solve the equations given by the equality
729A(X) = (1 + 2X)12A
 1 âˆ’X
1 + 2X

,
given by Theorem 4.13.
5.2 â†âˆ’
g (X) = X11 + X10 + X6 + X5 + X4 + X2 + 1.
5.6 The largest dimensions are i. 7. ii. 11 iii. 4.
5.7
i.
f (X) = X8 + X7 + X6 + X4 + X2 + X + 1.
g(X) = X8 + X5 + X4 + X3 + 1.
ii.
Use Theorem 5.10.
iii.
Use Theorem 3.5 and prove that the extended code is also linear.
5.8
i.
The polynomial X11 + 1 factorises as
(X +1)(X5 +ÏµX4 +X3 +X2 +Ïµ2X +1)(X5 +Ïµ2X4 +X3 +X2 +ÏµX +1),
where Ïµ2 = 1 + Ïµ.
iii.
Hint: Use Exercise 5.5.
5.9
ii.
Apply Exercise 5.5.
iv.
Find a codeword of weight 7.
6.1 Hint: use Exercise 2.8.
6.2 Hint: (c0, . . . , cnâˆ’1) âˆˆC if and only if there is a polynomial h of degree at most
k âˆ’1 such that ci = h(Î±i). Consider the polynomial c(X) = nâˆ’1
i=0 ciXi. To prove the
exercise observe that it is sufï¬cient to prove that c(Î±j) = 0, for all j = 1, . . . , n âˆ’k.
6.3 The sent codeword is the evaluation of the polynomial F(X) = X3 âˆ’X + 1.
6.5 Hint: The 4 Ã— 4 sub-matrices are Vandermonde.
6.6 Hint: use the Griesmer bound.
6.7 Hint: use i. to prove ii.

Hints and Answers to Selected Exercises
169
6.8
ii.
Use Exercise 6.7 iii.
iii.
For q odd, use Exercise 6.7 ii. For q even, use ii.
6.9 Hint: x2eâˆ’1 = y2eâˆ’1 implies x = y.
6.10 Hint: Let A be a 5 Ã— 5 sub-matrix of the matrix. Prove that (det A)3 = v âˆ’Î·v3,
where v is the determinant of a Vandermonde matrix. Apply Theorem 6.10.
6.11 Hint: Prove that det A Ì¸= 0 and use Theorem 6.10.
7.1 The sent codeword is (âˆ’1, 1, 1, 1, 1, 0, 1, 1, 1).
7.3 Hint: Use Exercise 7.2.
7.7 g(X) = aX2, h(X) = aX, f (X) = X.
7.9 g(X) = (X + 1)h(X), h(X) = a(2(e + 1)X + 1), f (X) = X + 1.
7.10
i. (Y/Z) = 4P3 âˆ’4P2.
ii. {1, X/Y, X/Z, (Y 2 + Z2)/X2}.
8.2
iii.
1.
8.5
iii.
The minimum distance is 6.
iv.
The dimension is 11.
9.1 The sent vector is
(0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1),
which the evaluation of the polynomial X3 + X4 + X1X2.
9.2 Hint: Start by selecting a vector e1 such that q(e1) = 0. Then choose e2 such that
q(e2) = 0 and b(e1, e2) = 0, where b(X, Y) = q(X + Y) âˆ’q(X) âˆ’q(Y).
9.5 Hint: we know the ï¬rst row in the remaining matrices. Since the matrices are
symmetric and have zeroes on the diagonal, each of the remaining matrices has only
three entries to be determined.
The four additional matrices are
{
â›
âœâœâœâ
0 1 1 0
1 0 1 1
1 1 0 0
0 1 0 0
â
âŸâŸâŸâ ,
â›
âœâœâœâ
0 1 0 1
1 0 1 0
0 1 0 0
1 0 0 0
â
âŸâŸâŸâ ,
â›
âœâœâœâ
0 0 1 1
0 0 1 0
1 1 0 1
1 0 1 0
â
âŸâŸâŸâ ,
â›
âœâœâœâ
0 1 1 1
1 0 0 1
1 0 0 0
1 1 0 0
â
âŸâŸâŸâ }

170
Hints and Answers to Selected Exercises
9.6
i.
As in the proof of Lemma 9.9.
ii.
As in the proof of Lemma 9.10 but there is also the case that the q(X) + â„“(X) is of
the form
r

i=1
X2iâˆ’1X2i + X2r+1
after a suitable change of basis.
iv.
Hint: Find a basis for a 5-dimensional subspace of ï¬ve 5 Ã— 5 matrices all of whose
non-zero matrices have rank at least 4.
9.7 The number of monomials of degree i in m indeterminates in which the degree of
each indeterminate is at most qâˆ’1 is equal to the number of solutions of x1+Â· Â· Â·+xm =
i, where 0 â©½xj â©½q âˆ’1. Use inclusion-exclusion.
10.1 Hint: b2 = 2c2 + 1 for some c2 âˆˆ{0, 1}. We can solve for c2 from
(2c2 + 1)2 âˆ’(2c2 + 1) + 2 = 0 (mod 4).
Î» = (1, 3, 3, 11, . . .).
10.6
ii.
The linear code over F2 generated by the matrix
â›
âœâ
1 0 0 0 1 1 1 1 0 0 0 1
0 1 0 1 0 1 1 0 1 0 1 0
0 0 1 1 1 0 1 0 0 1 1 1
â
âŸâ 
has minimum distance 6.
iii.
For example, the code generated by the matrix

1 1 2 0 1 1
0 0 2 2 2 2

.
10.8
ii. (1, 4, 22, 76, . . .) and (2, 5, 5, 59, . . .).
10.9
ii. (0, 3, 12, . . .) and (1, 7, 16, . . .).
10.10
ii. (0, 2, 2, 10, . . .) and (1, 3, 7, 7, . . .).

171
Bibliography
1. T. Alderson, A. GÃ¡cs, On the maximality of linear codes. Des. Codes Cryptogr. 53, 59â€“68 (2009)
2. T. Alderson, A.A. Bruen, R. Silverman, Maximum distance separable codes and arcs in projective
spaces. J. Combin. Theory Ser. A 114, 1101â€“1117 (2007)
3. R.B. Ash, Information Theory (Dover, Mineola, 1965)
4. E.F. Assmus Jr., H.F. Mattson Jr., New 5-designs. J. Combin. Theory 6, 122â€“151 (1969)
5. S. Ball, On sets of vectors of a ï¬nite vector space in which every subset of basis size is a basis. J.
Eur. Math. Soc. 14, 733â€“748 (2012)
6. S. Ball, Finite Geometry and Combinatorial Applications. London Mathematical Society Student
Texts, vol. 82 (Cambridge University Press, Cambridge, 2015)
7. L.A. Bassalygo, New upper bounds for error-correcting codes. Probl. Inform. Transm. 1, 32â€“35
(1965)
8. E.R. Berlekamp, Algebraic Coding Theory (McGraw-Hill, New York, 1968)
9. E.R. Berlekamp, L.R. Welch, Error correction for algebraic block codes, U.S. Patent 4,633,470, 30
Dec 1986
10. J. Bierbrauer, Introduction to Coding Theory, 2nd edn. (Chapman and Hall/CRC Press, Boca Raton,
2016)
11. A. Bishnoi, Some contributions to incidence geometry and the polynomial method. PhD thesis,
Universiteit Gent, Gent, 2016
12. R.E. Blahut, The Gleason-Prange theorem. IEEE Trans. Inform. Theory 37, 1269â€“1273 (2006)
13. R.C. Bose, D.K. Ray-Chaudhuri, On a class of error correcting binary group codes. Inform. Control
3, 68â€“79 (1960)
14. R. Calderbank, N.J.A. Sloane, Modular and p-adic cyclic codes. Des. Codes Cryptogr. 6, 21â€“35
(1995)
15. R. Calderbank, E.M. Rains, P.W. Shor, N.J.A. Sloane, Quantum error correction via codes over
GF(4). IEEE Trans. Inform. Theory 44, 1369â€“1387 (1998)
16. P.J. Cameron, Combinatorics: Topics, Techniques, Algorithms (Cambridge University Press,
Cambridge, 1994; reprinted 1996)
17. P.J. Cameron, J.H. van Lint, Designs, Codes, Graphs and Their Links. London Mathematical
Society Student Texts, vol. 22 (Cambridge University Press, Cambridge, 1991)
18. T. Can, N. Rengaswamy, R. Calderbank, H.D. Pï¬ster, Kerdock codes determine unitary 2-designs
(2019), https://arxiv.org/abs/1904.07842
19. C. Carlet, Z2k-linear codes. IEEE Trans. Inform. Theory 44, 1543â€“1547 (1998)
20. J. Davis, IMS Public Lecture: Apple vs Samsung: a Mathematical Battle, https://ims.nus.edu.sg/
resourcevideo.php, 18 May 2016
21. P. Delsarte, An Algebraic Approach to the Association Schemes of Coding Theory. Philips Research
Reports Supplement, vol. 10 (N.V. Philipsâ€™ Gloeilampenfabrieken, Amsterdam, 1973)
22. L.E. Dickson, Linear Groups: With an Exposition of the Galois Field Theory (Dover Publications,
New York, 1901)
23. R. Fano, Transmission of Information; A Statistical Theory of Communications (MIT Press,
Cambridge, 1961)
24. R.G. Gallager, Low Density Parity Check Codes (MIT Press, Cambridge, 1963)
25. E.N. Gilbert, A comparison of signalling alphabets. Bell Syst. Tech. J. 31, 504â€“522 (1952)
26. D.G. Glynn, The non-classical 10-arc of PG(4, 9). Discrete Math. 59, 43â€“51 (1986)
27. M.J.E. Golay, Notes on digital coding. Proc. Inst. Radio Eng. 37, 657 (1949)
Â© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4

172
Bibliography
28. V.D. Goppa, A new class of linear error-correcting codes. Probl. Inform. Transm. 6, 207â€“212 (1970)
29. V.D. Goppa, Codes on algebraic curves. Soviet Math. Dokl. 24, 170â€“172 (1981)
30. F. Gouvea, p-Adic Numbers: An Introduction (Springer, Berlin, 1997)
31. J.H. Griesmer, A bound for error-correcting codes. IBM J. Res. Dev. 4, 532â€“542 (1960)
32. V. Guruswami, A. Rudra, Limits to list decoding Reed-Solomon codes. IEEE Trans. Inform.
Theory 52, 3642â€“3649 (2006)
33. V. Guruswami, M. Sudan, Improved decoding of Reed-Solomon and algebraic-geometry codes.
IEEE Trans. Inform. Theory 45, 1757â€“1767 (1999)
34. R.W. Hamming, Error detecting and error correcting codes. Bell Labs Tech. J. 29, 147â€“160 (1950)
35. R.V.L. Hartley, Transmission of information. Bell Syst. Tech. J. 7, 535â€“563 (1928)
36. H.J. Helgert, Alternant codes. Inform. Control 26, 369â€“380 (1974)
37. R. Hill, A First Course in Coding Theory (Oxford University Press, Oxford, 1988)
38. A. Hocquenghem, Codes correcteurs dâ€™erreurs. Chiffres 2, 147â€“156 (1959)
39. S. Hoory, N. Linial, A. Wigderson, Expander graphs and their applications. Bull. Am. Math. Soc.
43, 439â€“561 (2006)
40. G.A. Jones, J.M. Jones, Information and Coding Theory. Springer Undergraduate Mathematics
Series (Springer, Berlin, 2000)
41. W.M. Kantor, An exponential number of generalized Kerdock codes. Inform. Control 53, 74â€“80
(1982)
42. W.M. Kantor, Spreads, translation planes and Kerdock sets, I, II. SIAM J. Algebraic Discrete Math.
3, 151â€“165 and 308â€“318 (1983)
43. T. Kasami, S. Lin, W.W. Peterson, Generalized Reed-Muller codes. Electron. Commun. Jpn. 51,
96â€“104 (1968)
44. A.M. Kerdock, A class of low-rate non-linear binary codes. Ann. Univ. Turku, Ser. A 20, 182â€“187
(1972)
45. Y. Kou, S. Lin, M. Fossorier, Low-density parity-check codes based on ï¬nite geometries: a
rediscovery and new results. IEEE Trans. Inform. Theory 47, 2711â€“2736 (2001)
46. R. Lidl, H. Niederreiter, Finite Fields. Encyclopedia of Mathematics and Its Applications, vol. 20,
2nd edn. (Cambridge University Press, Cambridge, 1997)
47. S. Lin, E.J. Welden, Long BCH codes are bad. Inform. Control 11, 445â€“451 (1967)
48. S. Ling, C. Xing, Coding Theory: A First Course (Cambridge University Press, Cambridge, 2004)
49. D.J.C. MacKay, R.M. Neal, Near Shannon limit performance of low density parity check codes.
Electron. Lett. 32, 1645â€“1646 (1996)
50. F.J. MacWilliams, N.J.A. Sloane, The Theory of Error-Correcting Codes (North-Holland, New
York, 1977)
51. R.J. McEliece, E.R. Rodemich, H. Rumsey, L.R. Welch, New upper bounds on the rate of a code
via the Delsarte-MacWilliams inequalities. IEEE Trans. Inform. Theory 23, 157â€“166 (1997)
52. G.L. Mullen, D. Panario (eds.), Handbook of Finite Fields. Discrete Mathematics and Its
Applications (CRC Press, Boca Raton, 2013)
53. D.E. Muller, Application of Boolean algebra to switching circuit design and to error detection.
IEEE Trans. Comput. 3, 6â€“12 (1954)
54. A.W. Nordstrom, J.P. Robinson, An optimum nonlinear code. Inform. Control 11, 613â€“616 (1967)
55. P.R.J. Ã–stergÃ¥rd, On binary/ternary error-correcting codes with minimum distance 4, in Applied
Algebra, Algebraic Algorithms and Error-Correcting Codes, ed. by M. Fossorier, H. Imai, S. Lin,
A. Poli. Lecture Notes in Computer Science, vol. 1719 (Springer, Berlin, 1999), pp. 472â€“481
56. T. Penttila, I. Pinneri, Hyperovals. Australas. J. Combin. 19, 101â€“114 (1999)
57. V. Pepe, LDPC codes from the Hermitian curve. Des. Codes Cryptogr. 42, 303â€“315 (2007)
58. M. Plotkin, Binary codes with speciï¬ed minimum distance. IRE Trans. Inform. Theory 6, 445â€“450
(1960)
59. I.S. Reed, A class of multiple-error-correcting codes and the decoding scheme. IEEE Trans. Inform.
Theory 4, 38â€“49 (1954)

Bibliography
173
60. I.S. Reed, G. Solomon, Polynomial codes over certain ï¬nite ï¬elds. J. Soc. Ind. Appl. Math. 8,
300â€“304 (1960)
61. S. Roman, Coding and Information Theory. Graduate Texts in Mathematics, vol. 134 (Springer,
Berlin, 1992)
62. C. Roos, A note on the existence of perfect constant weight codes. Discrete Math. 47, 121â€“123
(1983)
63. B. Segre, Ovals in a ï¬nite projective plane. Canad. J. Math. 7, 414â€“416 (1955)
64. B. Segre, Introduction to Galois geometries. Atti Accad. Naz. Lincei Mem. 8, 133â€“236 (1967)
65. C.E. Shannon, A Mathematical Theory of Communication (University of Illinois Press, Champaign,
1949; reprinted 1998)
66. R.C. Singleton, Maximum distance q-nary codes. IEEE Trans. Inform. Theory 10, 116â€“118 (1964)
67. M. Sipser, D.A. Spielman, Expander codes. IEEE Trans. Inform. Theory 42, 1710â€“1722 (1996)
68. M. Sudan, Decoding of Reed Solomon codes beyond the error-correction bound. J. Complexity 13,
180â€“193 (1997)
69. A. Ta-Shma, Explicit, almost optimal, epsilon-balanced codes, in Proceedings of the 49th Annual
ACM SIGACT Symposium on Theory of Computing (2017), pp. 238â€“251
70. C. Thommesen, The existence of binary linear concatenated codes with Reed-Solomon outer codes
which asymptotically meet the Gilbert-Varshamov bound. IEEE Trans. Inform. Theory 29, 850â€“853
(1983)
71. M.A. Tsfasman, Algebraic-Geometric codes and asymptotic problems. Discrete Applied Math. 33,
241â€“256 (1991)
72. M.A. Tsfasman, S.V. VlË˜adut, Algebraic-Geometric Codes (Kluwer Academic Publishers, Norwell,
1991)
73. M.A. Tsfasman, S.V. VlË˜adut, T. Zink, Modular curves, Shimura curves, and Goppa codes, better
than the Varshamov-Gilbert bound. Math. Nachr. 109, 21â€“28 (1982)
74. J.H. van Lint, Introduction to Coding Theory. Graduate Texts in Mathematics, vol. 86, 3rd edn.
(Springer, Berlin, 1999)
75. R.R. Varshamov, Estimate of the number of signals in error correcting codes. Dokl. Acad. Nauk
SSSR 117, 739â€“741 (1957)
76. S.B. Wicker, V.K. Bhargava (eds.), Reed-Solomon Codes and Their Applications (IEEE Press,
Piscataway, 1994)

175
Index
A
A(n, d, w), 38, 44
Ar(n, d), 32, 43
Afï¬ne plane, 26, 131
Afï¬ne space, 26
AG(k, q), 26, 131
Alderson-Bruen-Silverman model, 59
Alphabet, 29
Arc, 92
B
Belief propagation, 128
Block code, 10
Boolean function, 133
Bound
â€“ Eliasâ€“Bassalygo, 39
â€“ Gilbertâ€“Varshamov, 32, 67, 109
â€“ Griesmer, 60, 69
â€“ linear programming, 66
â€“ McElieceâ€“Rodemichâ€“Rumseyâ€“Welch, 41
â€“ Plotkin, 35, 37, 63
â€“ Singleton, 84
â€“ sphere packing, 33, 36, 44, 63
Burst errors, 94
C
Channel, 4
â€“ binary erasure, 6, 15
â€“ binary symmetric, 5, 11
â€“ capacity of, 11
â€“ information, 4
Character, 55
Check matrix, 48, 51, 106, 120, 126
Code
â€“ algebraic geometric, 112
â€“ alternant, 107
â€“ asymptotically good, 36
â€“ BCH, 78, 81
â€“ block, 10, 29
â€“ constant weight, 38
â€“ cyclic, 71, 156
â€“ dual, 54, 72, 136, 155, 162
â€“ equivalent, 59
â€“ evaluation, 85
â€“ extended, 31, 67
â€“ extension, 31
â€“ generalised Reedâ€“Solomon, 107, 120
â€“ Golay, 73, 76, 80, 81
â€“ Hamming, 49
â€“ Kerdock, 142, 144
â€“ LDPC, 126
â€“ length of, 29
â€“ linear, 47, 92, 155, 157
â€“ local reconstruction, 101
â€“ MDS, 83
â€“ Nordstromâ€“Robinson, 145
â€“ over a ring, 157
â€“ over Z/4Z, 160, 163
â€“ p-adic, 155
â€“ perfect, 34, 43, 67, 73
â€“ quadratic residue, 75
â€“ Reedâ€“Muller, 133
â€“ Reedâ€“Solomon, 85, 96, 101
â€“ repetition, 30
â€“ self-dual, 54
â€“ subï¬eld sub, 105
â€“ systematic, 38
â€“ turbo, 131
Combinatorial design, 63
Conjecture
â€“ constant weight binary codes, 42
â€“ Gilbertâ€“Varshamov, 42
â€“ MDS, 95
Coordinate ring, 113
Cyclotomic polynomial, 21, 71, 81
D
Decoding, 10
â€“ belief propagation, 128
â€“ list, 88
â€“ majority-logic, 138
â€“ maximum likelihood, 10
â€“ nearest neighbour, 30
Â© Springer Nature Switzerland AG 2020
S. Ball, A Course in Algebraic Error-Correcting Codes, Compact Textbooks in Mathematics,
https://doi.org/10.1007/978-3-030-41153-4

176
Index
â€“ Reedâ€“Solomon code, 86
â€“ standard array, 88
â€“ syndrome, 51, 67
Degree
â€“ of a divisor, 114
Design, 63
Distance
â€“ Hamming, 11, 30
â€“ Lee, 161
â€“ minimum, 30
Divisor, 113
â€“ degree of, 114
Dual code, 54, 72
E
End-vertex, 123
Entropy, 3
â€“ conditional, 6
â€“ input, 5
â€“ joint, 6
â€“ output, 5
Entropy function, 3
Expander graph, 124
F
Field, 17
â€“ ï¬nite, 19
â€“ subï¬eld, 26
Frobenius automorphism, 21, 55
G
Generator matrix, 48, 69, 155, 157
Genus, 114
Graph, 123
â€“ bipartite, 123
Gray map, 160
H
Hamming distance, 11
h(p), 3, 12, 32
Hyperplane, 24
I
Information, 2
â€“ average, 6
â€“ mutual, 8
L
Lee distance, 161
Lee weight, 161
List decoding, 88
M
MacWilliams identities, 56
Matrix
â€“ check, 48, 51, 106, 120, 126
â€“ generator, 48, 69, 155, 157
Minimum distance, 47, 161
â€“ prescribed, 78, 115
Minimum weight, 47
Module, 155
N
Neighbour, 123
[n, k, d]q, 48
(n, K, d)r, 48
P
p-adic integers, 151
p-adic numbers, 152
PG(k âˆ’1, q), 24, 92
Plotkin lemma, 34, 43
Polynomial
â€“ cyclotomic, 21
â€“ interpolation, 99, 108
â€“ Krawtchouk, 66
Primitive element, 22
Probability
â€“ backward, 4
â€“ of a correct decoding, 10
â€“ forward, 4
â€“ joint, 4
Projective plane, 24
Projective space, 24, 92
R
Random variable, 1
Rate, 10, 36
Ring, 17
â€“ coordinate, 113

Index
177
S
Shannonâ€™s theorem, 13
Stable set, 123
Standard array decoding, 88
Submodule, 155
Support, 63
Syndrome, 51, 128
Syndrome decoding, 51, 67
T
Trace map, 55
Transmission rate, 10
W
Weight, 37, 47, 71
â€“ Lee, 161
Weight enumerator, 54

