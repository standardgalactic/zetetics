MNRAS 000, 1â€“11 (2021)
Preprint 11 May 2021
Compiled using MNRAS LATEX style ï¬le v3.0
zeus: A Python implementation of Ensemble Slice Sampling for eï¬ƒcient
Bayesian parameter inference
Minas Karamanis
,1â˜…Florian Beutler
,1 and John A. Peacock
1
1Institute for Astronomy, University of Edinburgh, Royal Observatory, Blackford Hill, Edinburgh EH9 3HJ, UK
Accepted XXX. Received YYY; in original form ZZZ
ABSTRACT
We introduce zeus, a well-tested Python implementation of the Ensemble Slice Sampling (ESS) method for Bayesian parameter
inference. ESS is a novel Markov chain Monte Carlo (MCMC) algorithm speciï¬cally designed to tackle the computational
challenges posed by modern astronomical and cosmological analyses. In particular, the method requires no hand-tuning of any
hyper-parameters, its performance is insensitive to linear correlations and it can scale up to 1000s of CPUs without any extra eï¬€ort.
Furthermore, its locally adaptive nature allows to sample eï¬ƒciently even when strong non-linear correlations are present. Lastly,
the method achieves a high performance even in strongly multimodal distributions in high dimensions. Compared to emcee, a
popular MCMC sampler, zeus performs 9 and 29 times better in a cosmological and an exoplanet application respectively. The
code is publicly available at https://github.com/minaskar/zeus.
Key words: Markov Chain Monte Carlo â€“ Slice Sampling â€“ Bayesian Inference
1 INTRODUCTION
Over the past few decades the volume of astronomical and cosmolog-
ical data has increased substantially. In response to that, a variety of
astrophysical models have been developed to explain the plethora of
observations. Markov chain Monte Carlo (MCMC) has been estab-
lished as the standard procedure of inferring the model parameters
subject to the available data in a Bayesian framework.
Given some data ğ·and a model M with parameters ğœƒwe deï¬ne
the posterior distribution P(ğœƒ) â‰¡ğ‘ƒ(ğœƒ|ğ·, M) using Bayesâ€™s rule:
P(ğœƒ) = L(ğœƒ)ğœ‹(ğœƒ)
Z
,
(1)
where L(ğœƒ) â‰¡ğ‘ƒ(ğ·|ğœƒ, M) is the likelihood function, ğœ‹(ğœƒ) â‰¡
ğ‘ƒ(ğœƒ|M) is the prior distribution of the model parameters ğœƒ, and
Z â‰¡ğ‘ƒ(ğ·|M) is the, so called, Bayesian model evidence or marginal
likelihood and in this context it can be treated as a simple normali-
sation constant. It is worth noting that the model evidence is of great
importance for model comparison and its calculation is generally a
non-trivial task.
MCMC does not in general require knowing the value of the
model evidence and it only depends on the ability to evaluate the
unnormalised posterior distribution for arbitrary values of ğœƒ. MCMC
methods can then be used to generate (Markov) chains of samples
from the posterior distribution. Those samples can be used to cal-
culate integrals (e.g. parameter uncertainties, marginal distributions
etc.) that are paramount for modern astronomical and cosmological
analyses.
The most commonly used MCMC methods are variants of the
Metropolis-Hastings (MH) algorithm (Metropolis et al. 1953; Hast-
ings 1970). MH consists of two steps. First, given the last sample in
â˜…E-mail: minas.karamanis@ed.ac.uk
the chain, a new sample is proposed and then the Metropolis crite-
rion determines whether or not that new sample should be accepted
and thus added to the chain. The resulting chain is Markovian in the
sense that each sample is proposed based only on the previous sam-
ple. The purpose of the Metropolis acceptance criterion is to bias the
chain so that the time spent in a region of the parameter space would
be proportional to the posterior probability in that region. In other
words, the stationary distribution of the Markov chain is the target
distribution i.e. the posterior distribution. For a detailed introduction
to MCMC methods we direct the reader to MacKay (2003) and for
an intuitive introduction to Bayesian inference to Jaynes (2003).
Arguably, the most diï¬ƒcult part of the MH algorithm is the pro-
posal step. There are many ways of choosing a new sample and the
eï¬ƒciency of the method depends on this choice. By far the simplest
one is the use of a normal (Gaussian) distribution, centred around the
previous sample to generate the new proposed sample. The resulting
method is often called Metropolis algorithm and its performance is
highly sensitive to the ğ‘›(ğ‘›+ 1)/2 elements that form its covariance
matrix. Those elements generally need to be chosen a priori or be
adaptively tuned. More eï¬ƒcient methods utilise the gradient of the
target distribution (Betancourt 2017) or an ensemble of parallel and
communicating chains (Gilks et al. 1994; Ter Braak 2006; ter Braak
& Vrugt 2008; Goodman & Weare 2010).
Out of the methods mentioned in the previous paragraph we will fo-
cus our attention on the last one, the ensemble or population MCMC
variety. The reason is simple, the Metropolis algorithm requires a
great amount of tuning (or a priori knowledge) for it to perform
eï¬ƒciently and even then there is no guarantee that the proposal co-
variance matrix is optimal for the whole parameter space. On the
other hand, gradient based methods, although very powerful, are in
general unsuitable for astronomical applications in which the models
that are used are almost always not diï¬€erentiable.
One beneï¬t of ensemble MCMC over its alternatives is that the
Â© 2021 The Authors
arXiv:2105.03468v1  [astro-ph.IM]  7 May 2021

2
M. Karamanis et al.
ensemble of parallel chains (also known as walkers) collectively
sample the posterior, thus information about their distribution can
be shared and used to make better educated proposals. Other advan-
tages include the lack of hand-tuning of hyper-parameters and their
capacity for parallel implementation. For the aforementioned rea-
sons, ensemble MCMC methods have dominated astronomical anal-
yses. The most common ones are aï¬ƒneâ€“invariant ensemble sampling
(AIES) (Goodman & Weare 2010) and diï¬€erential evolution MCMC
(DEMC) (Ter Braak 2006; ter Braak & Vrugt 2008), both imple-
mented in the popular Python package emcee (Foreman-Mackey
et al. 2013, 2019).
In this paper we introduce zeus, a stable and well-tested Python
implementation of Ensemble Slice Sampling (ESS) (Karamanis &
Beutler 2020). ESS is a method based on the ensemble MCMC
paradigm with the crucial diï¬€erence being that its proposals are
performed via Slice Sampling updates (Neal 2003) instead of
Metropolis-Hastings ones. As we will thoroughly demonstrate in
Section 3, this subtle diï¬€erence leads to substantial improvements in
terms of sampling eï¬ƒciency and robustness. zeus is a user-friendly
tool that does not require any hand-tuning or preliminary runs and
can scale up to 1000s of CPUs without any extra eï¬€ort from the user.
zeus has been used in various astronomical and cosmologi-
cal analyses, including cosmological tests of gravity (Tamosiunas
2020), relativistic eï¬€ects and primordial non-Gaussianity (Wang
et al. 2020), 21cm intensity mapping (Umeh et al. 2021), and has
been implemented as part of the CosmoSIS package (Zuntz et al.
2015).
zeus is open source software that is publicly available at https:
//github.com/minaskar/zeus under the GPL-3 Licence. De-
tailed documentation and examples on how to get started are available
at https://zeus-mcmc.readthedocs.io.
2 ENSEMBLE SLICE SAMPLING
zeus is a Python implementation of the Ensemble Slice Sampling
method presented in Karamanis & Beutler (2020). Here we will pro-
vide a high-level description of the method and will refer to the
accompanying paper for more details about the underlying algorith-
mic structure and mathematics.
ESS combines the ensemble MCMC paradigm with slice sam-
pling. Since the use of slice sampling in astronomical parameter
inference is rare we will start by explaining its function and how it
diï¬€ers from MH updates. Then we will move on to discuss how it
can be eï¬ƒciently combined with ensemble MCMC.
2.1 Slice sampling
Slice sampling is based on the idea that sampling from a distribution
with density ğ‘ƒ(ğ‘¥) is equivalent to uniform sampling from the area
under the plot of ğ‘“(ğ‘¥) âˆğ‘ƒ(ğ‘¥). To this end, we introduce an auxiliary
variable ğ‘¦, called height, such that the joint deï¬nition ğ‘ƒ(ğ‘¥, ğ‘¦) is
uniform over the region ğ‘ˆ= {(ğ‘¥, ğ‘¦) : 0 < ğ‘¦< ğ‘“(ğ‘¥)}. To sample
from the marginal distribution ğ‘ƒ(ğ‘¥), we ï¬rst sample from ğ‘ƒ(ğ‘¥, ğ‘¦)
and then we marginalise by dropping the ğ‘¦value of each sample.
In order to generate samples from ğ‘ƒ(ğ‘¥, ğ‘¦) we utilise the following
scheme (Neal 2003):
(i) Given the current state ğ‘¥0, draw ğ‘¦0 uniformly from (0, ğ‘“(ğ‘¥0)).
(ii) Find an interval ğ¼= (ğ¿, ğ‘…) that contains all, or at least part,
of the slice ğ‘ = {ğ‘¥: ğ‘¦0 < ğ‘“(ğ‘¥)}.
(iii) Draw the new sample ğ‘¥1 uniformly from ğ¼âˆ©ğ‘†.
L
R
R'
L'
x
y
0
0
Figure 1. Illustration of the univariate slice sampling update. Given the cur-
rent sample ğ‘¥0, a value ğ‘¦0 is uniformly sampled along the vertical slice
(0, ğ‘“(ğ‘¥0)) (dashed line) thus deï¬ning the initial point (blue). An interval
(ğ¿, ğ‘…) is uniformly positioned horizontally around (ğ‘¥0, ğ‘¦0) and it is ex-
panded in steps of size ğ‘…âˆ’ğ¿until both its ends are outside the slice. The
new sample is generated by repeatedly sampling (uniformly) from the interval
(ğ¿â€², ğ‘…â€²) until a sample (green star) is found inside the slice. Samples outside
of the slice (red star) are rejected and they are instead used to shrink (ğ¿â€², ğ‘…â€²).
To construct the interval ğ¼(step ii), Neal (2003) introduced the
stepping-out procedure that works by randomly positioning an in-
terval of length ğœ‡around the sample ğ‘¥0 and then expanding it in
steps of size ğœ‡until both its ends are outside the slice. To obtain ğ‘¥1
we then use the shrinking procedure in which candidates are sampled
uniformly from ğ¼until a point inside the slice ğ‘†is found. Samples out-
side of the slice are used to shrink the interval ğ¼. The two procedures
are shown in Figure 1.
The length scale ğœ‡is the only free hyperparameter of slice sampling
and although its choice can reduce or increase the computational cost
of the method it generally does not aï¬€ect its mixing properties (e.g.
convergence rate, autocorrelation time, etc.). zeus utilises a Rob-
binsâ€“Monro stochastic optimization algorithm (Robbins & Monro
1951) in order to tune ğœ‡to its optimal value (see Section 3.1 of
Karamanis & Beutler (2020) for more details).
It is important to note here that for multimodal target distributions
there is no guarantee that the approximate slice would cross any of the
other modes. In particular, if the initial estimate of the length scale
ğœ‡is low then the probability of missing the other peaks, assuming
that they are located far away, is also low. As we will show in Section
3, unlike simple slice sampling, ESS and thus zeus does not suï¬€er
from this eï¬€ect.
2.2 Walkers, moves and parallelism
The slice sampling update described in the previous paragraphs is
a univariate update scheme. For it to be used to sample from mul-
tivariate target distributions it needs to be generalised accordingly.
Perhaps the simplest such generalisation in a multivariate setting is
the use of slice sampling to sample along each coordinate axis in turn
(i.e. component-wise slice sampling) or to sample along randomly
selected directions in parameter space (MacKay 2003). Although
MNRAS 000, 1â€“11 (2021)

zeus: Lightning Fast MCMC
3
X
X
Y
k
Xl
m
Figure 2. The ï¬gure illustrates the diï¬€erential move in the context of En-
semble Slice Sampling. The walker ğ‘‹ğ‘˜to be updated is shown in red. Two
walkers, ğ‘‹ğ‘™and ğ‘‹ğ‘š, (blue) are uniformly selected from the complementary
ensemble (grey). The approximate slice (dotted line) is constructed parallel
to the two walkers ğ‘‹ğ‘™and ğ‘‹ğ‘šusing the stepping-out procedure. The new
position ğ‘Œ(green) of ğ‘‹ğ‘˜is sampled using the shrinking procedure along the
approximate slice.
valid, both of these approaches are unsuitable in cases of correlated
parameters in which the proper choice of direction can substantially
accelerate mixing.
To address this issue, Tibbits et al. (2014) proposed to orthog-
onalise the parameter space using the sample covariance thus get-
ting rid of linear correlations between parameters. We will instead
follow a diï¬€erent, perhaps more ï¬‚exible, approach to construct an
eï¬ƒcient slice sampler. Our aim is to utilise an ensemble of parallel
chains/walkers that can exchange information about the covariance
structure of the target distribution and thus by-pass the diï¬ƒculties
posed by correlations.
As hinted in the introduction, the ensemble of walkers collectively
sample the target distribution and thus their positions encode infor-
mation about the correlations between the parameters. One way to
take advantage of this information is to use it to construct direction
vectors along which slice sampling can take place. We name the
simplest such algorithm as the diï¬€erential move, shown in Figure 2.
Using the diï¬€erential move, Ensemble Slice Sampling updates the
position of each walker in the ensemble by slice sampling along a
direction deï¬ned by the diï¬€erence between two uniformly selected
walkers from the rest of the ensemble (i.e. the complementary en-
semble).
Other moves that generate direction vectors from the comple-
mentary ensemble are possible. zeus oï¬€ers a collection of them,
including some that utilise clustering algorithms and density estima-
tion methods. As we will show in Section 3, such moves can help
accelerate sampling in diï¬ƒcult cases such as strongly multimodal
distributions. Any distribution of the complementary ensemble can
be used as a valid proposal to generate such direction vectors and
zeus oï¬€ers a highly ï¬‚exible interface for the user to deï¬ne such a
move or choose one (or a mixture) from the ones that are already
implemented and tested.
To parallelise this process and capitalise on the availability of mul-
tiple CPUs we split the ensemble into two sets of walkers (i.e. active
and passive sets) and choose to update the positions of the active
walkers along direction vectors deï¬ned by passive walkers. Then the
passive become active and vice versa and the process is repeated.
The ensemble splitting technique is required in order to parallelise
the algorithm without violating detailed balance. Heuristics to deter-
mine the number of required walkers per application are discussed
in Section 4.
3 EMPIRICAL EVALUATION
For the empirical evaluation of zeus we will use three toy examples
that manifest signiï¬cant aspects of real astronomical applications1
(i.e. linear and non-linear correlations, multimodality) and two real-
world astronomical examples characteristic of modern astronomical
analyses.
3.1 Toy examples
In order to understand the behaviour of zeus in various sampling
scenarios it is important to study its performance in diï¬€erent toy ex-
amples that demonstrate diï¬€erent characteristics of common target
distributions that arise in astronomical applications. For that reason
we chose three such toy examples. The ï¬rst one is a normal (Gaus-
sian) distribution which by deï¬nition is characterised only by linear
correlation between its parameters. The second toy problem is the
ring distribution, a characteristic example of strong non-linear corre-
lations. The last example is a Gaussian mixture with two components.
While the purpose of the ï¬rst two examples is to study the behaviour
of the algorithm in the presence of linear and non-linear correlations
respectively, the goal of the third example is to demonstrate the ability
of zeus to sample eï¬ƒciently from multimodal target distributions.
We will compare zeus with two popular alternatives oï¬€ered
by emcee, namely aï¬ƒneâ€“invariant ensemble sampling with the
stretch move (emcee/AIES) and the diï¬€erential evolution move
(emcee/DEMC). The main goal of this analysis is to justify our
choice of slice sampling as the basis of zeus instead of MH updates
through the use of simple yet instructive toy examples.
For all three toy examples discussed bellow we adopt the same
analysis procedure, where we initialise the walkers by sampling from
a normal distribution N (1, I) where I is the identity covariance ma-
trix and we discarded 104 samples as burn-in. We will use the ex-
pected squared jump distance and the distribution of steps of the
walkers as the metrics to evaluate the diï¬€erent sampling techniques.
3.1.1 The correlated normal distribution
Starting with the normal target distribution it is important to note
here that all three of the methods used in the comparison are aï¬ƒneâ€“
invariant2, meaning that their performance is immune to any linear
correlations between the parameters. Since the normal distribution
incorporates, by construction, only linear correlations (i.e. the 2D
marginal distribution contours look like ellipses), it is the perfect
1 For additional demonstrations on similarly common structures (e.g. the
funnel) we direct the reader to the accompanying paper (Karamanis & Beutler
2020).
2 Diï¬€erential evolution Metropolis is only approximately aï¬ƒneâ€“invariant
due to the jitter that it is often added to its proposal. This however has a
negligible eï¬€ect.
MNRAS 000, 1â€“11 (2021)

4
M. Karamanis et al.
2
0
2
zeus 
x1
D = 2
D = 10
D = 25
P25(x1)
2
0
2
emcee/AIES 
x1
0
200
400
600
800
1000
Iteration
2
0
2
emcee/DEMC 
x1
0
200
400
600
800
1000
Iteration
0
200
400
600
800
1000
Iteration
Figure 3. The ï¬gure shows numerical results (i.e. walker trajectories/chains for the ï¬rst parameter) demonstrating the performance of the three ensemble MCMC
methods in the case of a normal (Gaussian) target distribution in 2, 10 and 25 dimensions respectively. The last column illustrates the 1-D marginal posterior
corresponding to the ï¬rst parameter ğ‘¥1 estimated directly from the samples for the 25-dimensional case.
0
2
4
6
8
10
r (Separation)
100
101
102
103
104
Number of steps
Normal distribution in 25 dimensions
zeus
emcee/AIES
emcee/DEMC
Figure 4. This ï¬gure shows the distribution of step sizes of walkers for the
three diï¬€erent samplers in the case of a normal (Gaussian) target distribution
in ğ·= 25. It is important to note here that both emcee algorithms exhibit
a peak at zero separation; zeus on the other hand does not due to its non-
rejection nature.
Table 1. The table shows a comparison of emcee/AIES, emcee/DEMC and
zeus in terms of the expected squared jump distance (ESJD; higher is better)
for the three toy examples.
emcee/AIES
emcee/DEMC
zeus
Normal
0.5288
1.1162
2.1354
Ring
0.0043
0.0006
0.1257
Mixture
0.0037
0.0056
0.1015
testing ground to assess the eï¬€ect that high dimensionality has on
the three methods independently of other complications. For our
example we used a zero-mean normal distribution with a covariance
matrix in which the diagonal elements are set to 1 and the oï¬€-
diagonal ones are equal to 0.95. We then proceed by sampling the
aforementioned distribution in 2, 10 and 25 dimensions. Based on
Figure 3 one can see that the walkers of emcee/AIES dissolve into
an ineï¬ƒcient random walk characterised by low step size and high
autocorrelation time as the number of parameters increases. zeus
and emcee/DEMC are not so severely aï¬€ected by the high number
of parameters exhibiting a substantially lower autocorrelation.
Let us now try to explain this diï¬€erence in behaviour by looking
into the distribution of the steps of the walkers in Figure 4. By
step we mean the distance spanned in parameter space by a single
walker in a single iteration. This is a fundamental measure of the
eï¬ƒciency of an MCMC method and it is directly related to the
expected squared jump distance (Pasarica & Gelman 2010) (see Table
1). One thing to notice here is that the distribution of the steps
of zeusâ€™s walkers extends signiï¬cantly further away than those of
emcee/AIES and emcee/DEMC. This should come as no surprise
since the construction of the approximate slice allows for larger steps
than MH updates as shown in Table 1. This is because when a
proposal is rejected in slice sampling the approximate slice shrinks
and another sample is proposed instead. This way zeusâ€™s walkers
always move and the chance of staying ï¬xed is zero unlike MH-
based updates in which frequent rejection of samples is a necessity.
This aforementioned procedure leads to greater steps in parameter
space. The diï¬€erence between emcee/AIES and emcee/DEMC is
attributed to the fact that DEMC uses a proposal scale3 ğ›¾= 2.38/
âˆš
ğ·
that guarantees a constant acceptance rate accounting for the number
3 The proposal scale ğ›¾is similar to ğœ‡used in ESS in the sense that its value
determines the length scale of the proposed jumps in parameter space. A high
value would lead to large steps that are often rejected and a low value would
lead to small steps that are often accepted but do not carry the walkers far.
For such methods, a balance must me found.
MNRAS 000, 1â€“11 (2021)

zeus: Lightning Fast MCMC
5
2
1
0
1
2
zeus 
x1
D = 2
D = 10
D = 25
P25(x1)
2
1
0
1
2
emcee/AIES 
x1
0
200
400
600
800
1000
Iteration
2
1
0
1
2
emcee/DEMC 
x1
0
200
400
600
800
1000
Iteration
0
200
400
600
800
1000
Iteration
Figure 5. The ï¬gure shows numerical results (i.e. walker trajectories/chains for the ï¬rst parameter) demonstrating the performance of the three ensemble MCMC
methods in the case of the ring target distribution in 2, 10 and 25 dimensions respectively. The last column illustrates the 1-D marginal posterior corresponding
to the ï¬rst parameter ğ‘¥1 estimated directly from the samples for the 25-dimensional case. One can notice here that in 10 and 25 dimensions both emcee methods
mix very slowly. In the 25-dimensional case almost all of emcee/DEMCâ€™s walkers are unable to move and the autocorrelation time is eï¬€ectively inï¬nite.
0.0
0.5
1.0
1.5
2.0
r (Separation)
100
101
102
103
104
105
Number of steps
Ring distribution in 25 dimensions
zeus
emcee/AIES
emcee/DEMC
Figure 6. This ï¬gure shows the distribution of step sizes of walkers for the
three diï¬€erent samplers in the case of a ring target distribution in ğ·= 25. It
is important to note here that both emcee algorithms exhibit a peak at zero
separation; zeus on the other hand does not. The existence of the zero-peak
in emcee is due to the high number of rejected proposals (i.e. low acceptance
rate).
of dimensions ğ·. This proposal scale is however optimal only in
the case of a normal target distribution such as the one that we are
studying here and there is no guarantee that it would return acceptable
results in non-Gaussian distributions. For the case of emcee/AIES,
the relevant proposal scale ğ›¾is allowed to vary in the range between
1/ğ›¼and ğ›¼where ğ›¼= 2 is often taken as the typical value. It is
clear that in the latter case ğ›¾does not possess the desired scaling
ğ›¾âˆ1/
âˆš
ğ·and thus, although the method generates proposals in the
right overall direction, most of the samples do not reside in the bulk
of the posterior mass (Speagle 2019). In other words, the lack of
proper scaling of the proposal scale with the number of dimensions
leads to emcee/AIES â€œovershootingâ€ the typical set where most of
the posterior mass is located.
The above discussion allows us to clearly state a crucial distinction
between the three methods, that is their response to the curse of di-
mensionality. As the number of dimensions increases, the probability
mass of a distribution is concentrated into a thin shell at the tails of the
distribution (i.e. the typical set). To account for this and maintain its
eï¬ƒciency, a sampling method has to adjust its proposal scale other-
wise the proposals will not be located in the typical set and thus they
will not be accepted. The three methods that we mentioned so far deal
with this in diï¬€erent ways. emcee/AIESâ€™s proposal scale is not ad-
justed and thus its proposals become increasingly ineï¬ƒcient in high
dimensions. emcee/DEMCâ€™s proposal scale is adjusted based on the
theoretical expectation for the case of the normal target distribution.
Although both emcee methods perform well in this example, their
sub-optimal scaling will degrade their performance in non-Gaussian
target distributions as we will demonstrate in the next toy example.
Finally, zeusâ€™s proposal scale is continuously adapted, as the slice
expands and contracts in every iteration, thus guaranteeing optimal
scaling.
3.1.2 The ring distribution
The ring distribution deï¬ned as
ln ğ‘ƒ(ğ‘¥) = âˆ’
"
(ğ‘¥2ğ‘›+ ğ‘¥2
1 âˆ’ğ‘)2
ğ‘
#2
âˆ’
ğ‘›âˆ’1
âˆ‘ï¸
ğ‘–=1
"
(ğ‘¥2
ğ‘–+ ğ‘¥2
ğ‘–+1 âˆ’ğ‘)2
ğ‘
#2
,
(2)
where ğ‘= 2, ğ‘= 1 and ğ‘›is the total number of parameters, is an
artiï¬cial target distribution that exhibits strong non-linear correla-
tions between its parameters. This aspect of the ring distribution al-
lows us to demonstrate the locally adaptive nature of zeus. Whereas
emcee/AIES and emcee/DEMC use a single global proposal scale
for all regions of the parameter space, zeus has the ability to adjust
its proposal scale locally by expanding the slice appropriately. As
MNRAS 000, 1â€“11 (2021)

6
M. Karamanis et al.
1.0
0.5
0.0
0.5
1.0
zeus 
x1
D = 2
D = 10
D = 25
P25(x1)
1.0
0.5
0.0
0.5
1.0
emcee/AIES 
x1
0
200
400
600
800
1000
Iteration
1.0
0.5
0.0
0.5
1.0
emcee/DEMC 
x1
0
200
400
600
800
1000
Iteration
0
200
400
600
800
1000
Iteration
Figure 7. The ï¬gure shows numerical results (i.e. walker trajectories/chains for the ï¬rst parameter) demonstrating the performance of the three ensemble MCMC
methods in the case of a two-component Gaussian mixture target distribution in 2, 10 and 25 dimensions respectively. The last column illustrates the 1-D
marginal posterior corresponding to the ï¬rst parameter ğ‘¥1 estimated directly from the samples for the 25-dimensional case. Whereas all three samplers make
valid within-mode proposals, it is only zeus that manages to perform between-mode jumps and thus sample correctly from the target distribution in the 10 and
25-dimensional cases. Between-mode jumps are paramount to distribute the probability mass correctly between diï¬€erent modes.
0
1
2
3
4
5
r (Separation)
100
101
102
103
104
Number of steps
Gaussian Mixture in 25 dimensions
zeus
emcee/AIES
emcee/DEMC
Figure 8. This ï¬gure shows the distribution of step sizes of walkers for the
three diï¬€erent samplers in the case of a two-component Gaussian mixture
target distribution in ğ·= 25. It is important to note here that both emcee
algorithms exhibit a peak at zero separation; zeus on the other hand does not
due to its non-rejection basis.
expected, this will allow zeus to sample eï¬ƒciently even in cases in
which strong non-linear correlations are present. Looking at Figure
5 one can see that zeus manages to generate multiple samples ef-
ï¬ciently even in high dimensions. On the other hand, emcee/AIES
and emcee/DEMC do not eï¬ƒciently produce valid proposals: for
emcee/AIES this leads to an ineï¬ƒcient random walk, characterised
by small steps; for emcee/DEMC the acceptance rate almost vanishes
beyond ğ·= 2. The expected squared jump distance of each method
for the case of ğ·= 25 is shown in Table 1. It is important to note
here that out of the three samplers only zeus manages to converge
in all three cases (i.e. in 2, 10 and 25 dimensions). emcee/AIES
and emcee/DEMC on the other hand converge successfully only in
2 dimensions.
To explain this result one only has to look at the distribution of
walker steps of the diï¬€erent methods at Figure 6. zeusâ€™s steps extend
to large distances in parameter space whereas most of emcee/AIESâ€™s
and emcee/DEMCâ€™s steps are rejected (i.e. shown as zero in the
histogram). We can see that emcee/DEMC manages to perform some
long distance steps but those are few and there is almost nothing in
between. It is clear from this and the previous toy examples that
the ğ›¾= 2.38/
âˆš
ğ·scaling of emcee/DEMCâ€™s scale factor does not
generalise well beyond the Gaussian case.
3.1.3 The two-component Gaussian mixture distribution
One other important aspect of astronomical posterior distributions is
the fact that many of them exhibit multiple peaks. Multimodality can
arise either from non-linear models or sparse and uninformative data.
In either case, multimodal target distributions present a formidable
challenge for most MCMC methods. Perhaps the simplest example
of such a distribution is the two-component Gaussian mixture. In this
example we will position the two, equal-mass, components at âˆ’0.5
and +0.5 respectively with standard deviation of 0.1. Sampling from
multimodal distributions requires two types of proposals, local pro-
posals that sample diï¬€erent modes individually and global proposals
that transfer walkers from one mode to the other. For this reason
we will make use of zeusâ€™s GlobalMove that uses a Dirichlet Pro-
cess Gaussian Mixture model of the ensemble to eï¬ƒciently propose
between-mode and within-mode steps.
As seen in Figure 7, zeusâ€™s walkers manage to move from one
mode to the other frequently enough for mixing to be eï¬ƒcient even
in the ğ·= 25 case. Out of emcee/AIES and emcee/DEMC, only
the latter proposes valid steps from one mode to the other in the
ğ·= 2 case. As for the ğ·= 25 case, one can see in Figure 8 that
zeusâ€™s walkers perform numerous jumps whereas emceeâ€™s walkers
MNRAS 000, 1â€“11 (2021)

zeus: Lightning Fast MCMC
7
1
2
B
0.95
1.05
0.9
1.1
||
5
15
10
25
||
1
5
FOG
1
2
0
20000
45
10000
0
44
0
2000
43
100
0
42
0
1
41
30000
0
25
0
20000
24
4000
0
23
50
50
22
0.0
0.8
21
15000
0
05
0
10000
04
2000
0
03
0
30
02
0
01
0
01
0
30
02
2000
0
03
0
10000
04
15000
0
05
0.0
0.8
21
50
50
22
4000
0
23
0
20000
24
30000
0
25
0
1
41
100
0
42
0
2000
43
10000
0
44
0
20000
45
1
2
1
5
FOG
10
25
||
5
15
0.9
1.1
||
0.95
1.05
zeus
emcee/AIES
emcee/DEMC
Figure 9. A corner plot showing the 1-D and 2-D marginalised posteriors for the 22-parameter Baryon Acoustic Oscillation model as produced by the three
diï¬€erent ensemble MCMC methods.
are unable to do so. The ability of the walkers to jump from mode
to mode is of paramount importance if we want to sample correctly
from the target distribution. Lack of such proposals will lead to
an improper probability mass ratio between the two modes and thus
biased inference. The expected squared jump distance of each method
for the case of ğ·= 25 is shown in Table 1.
Clustering-based proposals have also been applied to MH-type
ensemble MCMC methods but as shown in Karamanis & Beutler
(2020), they fail to generate valid proposals in problems with mod-
erate number of dimensions. The reason is, as discussed in Section
3, that MH has to propose a valid point in the other mode. In other
words, whereas Ensemble Slice Sampling only needs to get the di-
rection of the other mode relative to the chosen walker correctly,
MH requires to guess both the direction and the distance, a task that
rapidly becomes very hard as the number of dimensions rises.
3.2 Real astronomical analyses
The previous section, through the use of toy examples, shows various
scenarios that might emerge during sampling and how zeus is better
equipped to handle them. To demonstrate the eï¬ƒciency of zeus
compared to other samplers in realistic target distributions we chose
MNRAS 000, 1â€“11 (2021)

8
M. Karamanis et al.
20.88
Pb
0
4
jit
7
4
0.000
0.002
0.004
0.1
0.0
1
2
lnKc
0.6
0.0
0.6
esin
c
0.5
0.0
0.5
ecos
c
2082.60
2082.65
Tconjc
42.335
42.360
42.385
Pc
1
2
lnKb
0.6
0.0
0.6
esin
b
0.5
0.0
0.5
ecos
b
2072.78
2072.80
2072.82
Tconjb
2072.79
Tconjb
0.3 0.3
ecos
b
0.4 0.4
esin
b
1
2
lnKb
42.36
Pc
2082.62
Tconjc
0.4
0.4
ecos
c
0.5 0.5
esin
c
1
2
lnKc
0
0.002
7
4
0
4
jit
zeus
emcee/AIES
emcee/DEMC
Figure 10. A corner plot showing the 1-D and 2-D marginalised posteriors for the 14-parameter radial velocity model as produced by the three diï¬€erent ensemble
MCMC methods.
two common astronomical inference problems as the testing ground.
Those are the cases of baryon acoustic oscillation (BAO) parameter
inference and exoplanet parameter estimation.
We used the same three samplers in our comparison, namely emcee
with AIES and DEMC, and of course zeus. We performed three
distinct tests:
â€¢ The ï¬rst of which was to estimate the eï¬ƒciency, deï¬ned as the
number of independent samples produced per log-likelihood evalu-
ation, for each sampler. To this end, we ran the MCMC procedure 5
times for each sampler and computed the mean eï¬ƒciency using the
estimated autocorrelation time of the chains.
â€¢ The second test that we performed relates to the convergence
rate of the three algorithms. As convergence rate we deï¬ne the inverse
of the number of iterations required until all the convergence criteria
are met. In order to estimate the mean convergence rate we ran the
sampling procedure 40 times for each sampler initialising the walkers
close to the Maximum a Posteriori (MAP) estimate.
â€¢ Finally, we tested the sensitivity of the samplers to the initial
conditions by running 40 realisations with the walkers initialised
from randomly chosen regions in the prior volume and counting how
MNRAS 000, 1â€“11 (2021)

zeus: Lightning Fast MCMC
9
many of those attempts led to converged chains before a predeter-
mined number of likelihood evaluations.
To determine whether a chain has converged we used four diï¬€erent
metrics: the Gelman-Rubin split-R statistic (Gelman et al. 1992,
2013), the Geweke test (Geweke 1992), a minimum length of the
chain as a multiple of the integrated autocorrelation time (IAT) as
well as an upper bound on the rate of change of the IAT. The number
of walkers used in both examples was close to the minimum value
of 2 Ã— ğ·. As we will discuss in Section 4 this often leads to faster
convergence.
3.2.1 Cosmological inference
The particular inference problem that we face here is that of the
anisotropic BAO parameter inference using estimates of the galaxy
power spectrum. The data we used come from the 12th data re-
lease (DR12) of the high-redshift North Galactic Cap (NGC) sample
as observed by the Sloan Digital Sky Survey (SDSS) (Eisenstein
et al. 2011) Baryon Oscillation Spectroscopic Survey (BOSS) (Daw-
son et al. 2013). Our analysis follows closely that of Beutler et al.
(2017) with the diï¬€erence that we chose not to ï¬x any parameters
and ï¬t the hexadecapole multipole of the power spectrum as well
as the monopole and quadrupole. Those choices were made solely
to render the problem more challenging. Indeed the inclusion of the
hexadecapole does not contribute any additional constraining power
for the data that we used. However, such extended models will prove
useful when analysing data from larger galaxy surveys such as DESI
(DESI Collaboration et al. 2016). In terms of Bayesian inference,
the problem has 22 free parameters. The results of our analysis are
consistent with those of Beutler et al. (2017). We used weakly in-
formative ï¬‚at (uniform) priors for all parameters except for the two
scaling parameters, ğ›¼âˆ¥and ğ›¼âŠ¥for which we used normal (Gaussian)
priors. We used 50 walkers in total.
In terms of eï¬ƒciency, zeus generates at least 5 independent sam-
ples for each one generated by emcee/DEMC and at least 9 for each
one generated by emcee/AIES factoring in the diï¬€erent computa-
tional cost of the methods. As for the convergence rate, zeus con-
verges more than 3 times faster than either emcee variant. Finally, we
found that zeus is less sensitive to the initialisation than either of the
other two methods. In particular, out of the 40 tests conducted with
diï¬€erent initialisation, zeus converged 36 times, emcee/DEMC 14
times and emcee/AIES 7 times prior to the predetermined maximum
number of likelihood evaluations (i.e. 106 in this case). The afore-
mentioned results are presented in detail in Table 2. The 1-D and 2-D
marginal posterior distributions are shown in Figure 9 demonstrating
the agreement between the three methods4.
3.2.2 Exoplanet inference
Another common application of MCMC methods in astronomy is the
problem of exoplanet parameter inference through modelling of Kep-
lerian orbits and radial velocity time series data. In this section we will
demonstrate the performance of zeus using a two-planet model with
14 free parameters and real data from the K2-24 (EPIC-203771098)
extrasolar system (Petigura et al. 2016) that is known to host two
exoplanets. We used the popular Python package RadVel (Fulton
4 No upper limit on the number of likelihood evaluations or iterations was
used for this run and convergence was diagnosed using all the metrics that we
introduced.
Table 2. The table shows a comparison of emcee/AIES, emcee/DEMC and
zeus in terms of the eï¬ƒciency (i.e. number of independent samples per
likelihood evaluation), the convergence rate and the convergence fraction (i.e.
times that each method converged starting from a diï¬€erent initialisation). Both
the eï¬ƒciency and the convergence rate are normalised by the emcee/AIES
value.
emcee/AIES
emcee/DEMC
zeus
Cosmological inference
eï¬ƒciency
1.0
1.8
9.2
convergence rate
1.0
1.1
3.7
convergence fraction
7/40
14/40
36/40
Exoplanet inference
eï¬ƒciency
1.0
4.1
29.3
convergence rate
1.0
2.1
7.5
convergence fraction
23/40
29/40
38/40
et al. 2018) for the Keplerian modelling of the planetary orbits. The
results of our analysis are consistent with published constraints for
the aforementioned extrasolar system (Petigura et al. 2016). We used
30 walkers in total for sampling.
We performed the same suite of tests as in the cosmological infer-
ence case. In terms of eï¬ƒciency, zeus generates more than 7 inde-
pendent samples per each one generated by emcee/DEMC and more
than 29 independent samples per each one generated by emcee/AIES.
As for the convergence rate, zeus converges 7.5 times faster than
emcee/AIES and 3.5 faster than emcee/DEMC on average. Finally,
we found again that zeus is less sensitive to the speciï¬c initialisa-
tion of the walkers. In particular, out of the 40 tests conducted with
diï¬€erent initialisation, zeus converged 38 times, emcee/DEMC 29
times and emcee/AIES 23 times prior to the predetermined maxi-
mum number of likelihood evaluations (i.e. 103 in this case). Detailed
results about the values of the used metrics are shown in Table 2. The
1-D and 2-D marginal posterior distributions are shown in Figure 10
demonstrating the agreement between the three methods.
4 DISCUSSION
So far we have neglected any discussion about the initialisation of the
walkers. There are two important questions that need to be answered.
First, how many walkers are necessary and, second, how to choose
the initial positions of the walkers. Although there are many ways of
answering those questions and no consistent solution that works for
all target distributions we will try to provide some general rules and
heuristics to help make the task of choosing the number and initial
positions of the walkers an easy task for most cases.
Let us ï¬rst discuss the eï¬€ect of the number of the walkers on the
general performance of zeus. It is clear that the absolute minimum
number of walkers is equal to twice the number of dimensions5. If
a smaller number is chosen then the walkers can be trapped in a
lower-dimensional hyper-plane of the parameter space, being unable
to sample properly and leading to erroneous results. Although there
is no upper bound on the number of walkers, we recommend to use
between two to three times the number of dimensions. The reason is
that increasing the number of dimensions can increase the duration
5 The reason that the minimum is not ğ·+ 1 as one might expect is the
ensemble splitting technique that was introduced in Section 2 to render the
algorithm parallelisable.
MNRAS 000, 1â€“11 (2021)

10
M. Karamanis et al.
of the burn-in period. Ideally, one wants to use the minimum number
(or close to that) of walkers until the burn-in period is over and then
increase the number of walkers to rapidly produce a great number of
independent samples. It is also worth noting that in cases in which
either non-linear correlations or multiple modes are present it is
recommended to use more walkers (e.g. 4-6 times the number of
parameters for a bimodal target distribution).
As for the initialisation of the walkers, there are many ways to
choose their starting positions ranging from prior sampling to more
localised initial positions. Empirical tests indicate that the latter often
outperforms the former (i.e. leads to shorter burn-in periods). That
is not surprising since the total probability of a prior-sampled ini-
tialisation can be very small when the number of parameters is high.
In particular we found that initialising the walkers from a tight re-
gion in parameter space (i.e. normal distribution with small variance)
consistently leads to good performance. For low to moderate dimen-
sional problems initialising the walkers from a tight ball around the
Maximum A Posteriori (MAP) estimate can substantially reduce the
burn-in period (Foreman-Mackey et al. 2013).
Finally, while emcee/AIES and emcee/DEMC can sample accept-
ably from most target distributions with ğ·â‰²20, the eï¬ƒcient scaling
of zeus with the number of parameters allows us to extend this range
and eï¬ƒciently test more complicated models (Karamanis & Beutler
2020). Like most gradient-free methods, zeus will fail to sample
eï¬ƒciently in very high dimensional problems in which ğ·= O(102).
In such cases, more sophisticated algorithms (e.g. tempering, block
updating, etc.) need to be used (Robert et al. 2018).
5 CONCLUSIONS
The aim of this project was to develop a tool that could facilitate
Bayesian parameter inference in computationally demanding astro-
nomical analyses and tackle the challenges posed by the complexity
of the models and data that are often used by astronomers. To this end,
we introduced zeus, a parallel, black-box and gradient-free Python
implementation of Ensemble Slice Sampling.
After introducing the method in Section 2, we thoroughly demon-
strated its performance compared to two popular alternatives (i.e.
emcee with aï¬ƒne-invariant ensemble sampling and diï¬€erential evo-
lution Metropolis) using a variety of artiï¬cial and realistic target
distributions in Section 3. The artiï¬cial toy examples helped to shed
light on the general behaviour of the samplers in target distributions
characterised by linear and non-linear correlations as well as multi-
modal densities. When compared to emcee/AIES and emcee/DEMC
in the problems of Baryon Acoustic Oscillation parameter inference
and exoplanet radial velocity ï¬tting, zeus consistently converges
faster (i.e. its burn-in is shorter by a factor of at least 3 times), it is
less sensitive to the initialisation of the walkers and generates sub-
stantially more independent samples per likelihood evaluation (i.e.
approximately Ã—9 and Ã—29 speed-up compared to emcee/AIES in the
cosmological and exoplanet examples, respectively).
We have shown that zeus performs similarly or better than ex-
isting MCMC methods in a range of problems. We hope that zeus
will prove useful to the astronomical and cosmological community
by complementing existing approaches and facilitating the study
of novel models and data over the coming years. zeus is pub-
licly available at https://github.com/minaskar/zeus with de-
tailed documentation and examples that can be found at https:
//zeus-mcmc.readthedocs.io.
ACKNOWLEDGEMENTS
The authors would like to express their gratitude to anyone who con-
tributed to the early success of zeus by incorporating it into their own
research. In particular, we thank Andrius Tamosiunas, Jamie Donald-
McCann, Mike Wang and Obinna Umeh for providing feedback on
an early version of the code. This project has received funding from
the European Research Council (ERC) under the European Unionâ€™s
Horizon 2020 research and innovation programme (grant agreement
853291). FB is a University Research Fellow.
zeus and this work have beneï¬ted from a variety of Python
packages including numpy (Van Der Walt et al. 2011), scipy (Vir-
tanen et al. 2020), matplotlib (Hunter 2007), seaborn (Waskom
2021), getdist (Lewis 2019), sklearn (Pedregosa et al. 2011),
tqdm (da Costa-Luis 2019), and mpi4py (Dalcin et al. 2011).
DATA AVAILABILITY
All data used in this work are publicly available. Power spec-
trum estimates, covariance matrices and window functions used
in the cosmological inference example are available at http://
www.sdss3.org/science/boss_publications.php. Radial ve-
locity measurements of the K2-24 system used in the exoplanet
inference are available as part of the RadVel package at https:
//github.com/California-Planet-Search/radvel.
REFERENCES
Betancourt M., 2017, preprint (arXiv:1701.02434)
Beutler F., et al., 2017, MNRAS, 464, 3409
DESI Collaboration et al., 2016, preprint (arXiv:1611.00036)
Dalcin L. D., Paz R. R., Kler P. A., Cosimo A., 2011, Adv. Water Resour., 34,
1124
Dawson K. S., et al., 2013, AJ, 145, 10
Eisenstein D. J., et al., 2011, AJ, 142, 72
Foreman-Mackey D., Hogg D. W., Lang D., Goodman J., 2013, PASP, 125,
306
Foreman-Mackey D., et al., 2019, preprint (arXiv:1911.07688)
Fulton B. J., Petigura E. A., Blunt S., Sinukoï¬€E., 2018, PASP, 130, 044504
Gelman A., Rubin D. B., et al., 1992, Stat. Sci., 7, 457
Gelman A., Carlin J. B., Stern H. S., Dunson D. B., Vehtari A., Rubin D. B.,
2013, Bayesian Data Analysis. CRC Press
Geweke J., 1992, Bayesian Stat., 4, 641
Gilks W. R., Roberts G. O., George E. I., 1994, J. R. Stat. Soc. Series D (The
Statistician), 43, 179
Goodman J., Weare J., 2010, Comm. App. Math. Comp. Sci., 5, 65
Hastings W. K., 1970, Biometrika, 57, 97
Hunter J. D., 2007, IEEE Ann. Hist. Comput., 9, 90
Jaynes E. T., 2003, Probability Theory: The Logic of Science. Cambridge
University Press
Karamanis M., Beutler F., 2020, preprint (arXiv:2002.06212)
Lewis A., 2019, preprint (arXiv:1910.13970)
MacKay D. J., 2003, Information Theory, Inference and Learning Algorithms.
Cambridge University Press
Metropolis N., Rosenbluth A. W., Rosenbluth M. N., Teller A. H., Teller E.,
1953, J. Chem. Phys., 21, 1087
Neal R. M., 2003, Ann. Stat., pp 705â€“741
Pasarica C., Gelman A., 2010, Stat. Sin., pp 343â€“364
Pedregosa F., et al., 2011, J. Mach. Learn. Res., 12, 2825
Petigura E. A., et al., 2016, ApJ, 818, 36
Robbins H., Monro S., 1951, Ann. Math. Stat., pp 400â€“407
Robert C. P., Elvira V., Tawn N., Wu C., 2018, preprint (arXiv:1804.02719)
Speagle J. S., 2019, preprint (arXiv:1909.12313)
Tamosiunas A., 2020, preprint (arXiv:2011.08786)
MNRAS 000, 1â€“11 (2021)

zeus: Lightning Fast MCMC
11
Ter Braak C. J., 2006, Stat. Comput., 16, 239
Tibbits M. M., Groendyke C., Haran M., Liechty J. C., 2014, J. Comput.
Graph. Stat., 23, 543
Umeh O., Maartens R., Padmanabhan H., Camera S., 2021, preprint
(arXiv:2102.06116)
Van Der Walt S., Colbert S. C., Varoquaux G., 2011, Comput. Sci. Eng., 13,
22
Virtanen P., et al., 2020, Nat. Methods, 17, 261
Wang M. S., Beutler F., Bacon D., 2020, MNRAS, 499, 2598
Waskom M. L., 2021, J. Open Source Softw., 6, 3021
Zuntz J., et al., 2015, Astron. Comput., 12, 45
da Costa-Luis C. O., 2019, J. Open Source Softw., 4, 1277
ter Braak C. J., Vrugt J. A., 2008, Stat. Comput., 18, 435
APPENDIX A: CRONUS
Along with zeus, we introduce cronus, a Python package that
wraps zeus as well as other samplers (e.g. emcee). cronus is de-
signed to be used via the terminal through simple, human-reabable
parameters ï¬les. cronus implements an automated suite of conver-
gence diagnostics and all its methods are massively parallel using the
Message Passing Interface (MPI). Our aim is to automate parameter
inference and facilitate highly reproducible analyses for current and
future astronomical surveys such as the Dark Energy Spectroscopic
Instrument (DESI) (DESI Collaboration et al. 2016).
This paper has been typeset from a TEX/LATEX ï¬le prepared by the author.
MNRAS 000, 1â€“11 (2021)

