MNRAS 000, 1–11 (2021)
Preprint 11 May 2021
Compiled using MNRAS LATEX style ﬁle v3.0
zeus: A Python implementation of Ensemble Slice Sampling for eﬃcient
Bayesian parameter inference
Minas Karamanis
,1★Florian Beutler
,1 and John A. Peacock
1
1Institute for Astronomy, University of Edinburgh, Royal Observatory, Blackford Hill, Edinburgh EH9 3HJ, UK
Accepted XXX. Received YYY; in original form ZZZ
ABSTRACT
We introduce zeus, a well-tested Python implementation of the Ensemble Slice Sampling (ESS) method for Bayesian parameter
inference. ESS is a novel Markov chain Monte Carlo (MCMC) algorithm speciﬁcally designed to tackle the computational
challenges posed by modern astronomical and cosmological analyses. In particular, the method requires no hand-tuning of any
hyper-parameters, its performance is insensitive to linear correlations and it can scale up to 1000s of CPUs without any extra eﬀort.
Furthermore, its locally adaptive nature allows to sample eﬃciently even when strong non-linear correlations are present. Lastly,
the method achieves a high performance even in strongly multimodal distributions in high dimensions. Compared to emcee, a
popular MCMC sampler, zeus performs 9 and 29 times better in a cosmological and an exoplanet application respectively. The
code is publicly available at https://github.com/minaskar/zeus.
Key words: Markov Chain Monte Carlo – Slice Sampling – Bayesian Inference
1 INTRODUCTION
Over the past few decades the volume of astronomical and cosmolog-
ical data has increased substantially. In response to that, a variety of
astrophysical models have been developed to explain the plethora of
observations. Markov chain Monte Carlo (MCMC) has been estab-
lished as the standard procedure of inferring the model parameters
subject to the available data in a Bayesian framework.
Given some data 𝐷and a model M with parameters 𝜃we deﬁne
the posterior distribution P(𝜃) ≡𝑃(𝜃|𝐷, M) using Bayes’s rule:
P(𝜃) = L(𝜃)𝜋(𝜃)
Z
,
(1)
where L(𝜃) ≡𝑃(𝐷|𝜃, M) is the likelihood function, 𝜋(𝜃) ≡
𝑃(𝜃|M) is the prior distribution of the model parameters 𝜃, and
Z ≡𝑃(𝐷|M) is the, so called, Bayesian model evidence or marginal
likelihood and in this context it can be treated as a simple normali-
sation constant. It is worth noting that the model evidence is of great
importance for model comparison and its calculation is generally a
non-trivial task.
MCMC does not in general require knowing the value of the
model evidence and it only depends on the ability to evaluate the
unnormalised posterior distribution for arbitrary values of 𝜃. MCMC
methods can then be used to generate (Markov) chains of samples
from the posterior distribution. Those samples can be used to cal-
culate integrals (e.g. parameter uncertainties, marginal distributions
etc.) that are paramount for modern astronomical and cosmological
analyses.
The most commonly used MCMC methods are variants of the
Metropolis-Hastings (MH) algorithm (Metropolis et al. 1953; Hast-
ings 1970). MH consists of two steps. First, given the last sample in
★E-mail: minas.karamanis@ed.ac.uk
the chain, a new sample is proposed and then the Metropolis crite-
rion determines whether or not that new sample should be accepted
and thus added to the chain. The resulting chain is Markovian in the
sense that each sample is proposed based only on the previous sam-
ple. The purpose of the Metropolis acceptance criterion is to bias the
chain so that the time spent in a region of the parameter space would
be proportional to the posterior probability in that region. In other
words, the stationary distribution of the Markov chain is the target
distribution i.e. the posterior distribution. For a detailed introduction
to MCMC methods we direct the reader to MacKay (2003) and for
an intuitive introduction to Bayesian inference to Jaynes (2003).
Arguably, the most diﬃcult part of the MH algorithm is the pro-
posal step. There are many ways of choosing a new sample and the
eﬃciency of the method depends on this choice. By far the simplest
one is the use of a normal (Gaussian) distribution, centred around the
previous sample to generate the new proposed sample. The resulting
method is often called Metropolis algorithm and its performance is
highly sensitive to the 𝑛(𝑛+ 1)/2 elements that form its covariance
matrix. Those elements generally need to be chosen a priori or be
adaptively tuned. More eﬃcient methods utilise the gradient of the
target distribution (Betancourt 2017) or an ensemble of parallel and
communicating chains (Gilks et al. 1994; Ter Braak 2006; ter Braak
& Vrugt 2008; Goodman & Weare 2010).
Out of the methods mentioned in the previous paragraph we will fo-
cus our attention on the last one, the ensemble or population MCMC
variety. The reason is simple, the Metropolis algorithm requires a
great amount of tuning (or a priori knowledge) for it to perform
eﬃciently and even then there is no guarantee that the proposal co-
variance matrix is optimal for the whole parameter space. On the
other hand, gradient based methods, although very powerful, are in
general unsuitable for astronomical applications in which the models
that are used are almost always not diﬀerentiable.
One beneﬁt of ensemble MCMC over its alternatives is that the
© 2021 The Authors
arXiv:2105.03468v1  [astro-ph.IM]  7 May 2021

2
M. Karamanis et al.
ensemble of parallel chains (also known as walkers) collectively
sample the posterior, thus information about their distribution can
be shared and used to make better educated proposals. Other advan-
tages include the lack of hand-tuning of hyper-parameters and their
capacity for parallel implementation. For the aforementioned rea-
sons, ensemble MCMC methods have dominated astronomical anal-
yses. The most common ones are aﬃne–invariant ensemble sampling
(AIES) (Goodman & Weare 2010) and diﬀerential evolution MCMC
(DEMC) (Ter Braak 2006; ter Braak & Vrugt 2008), both imple-
mented in the popular Python package emcee (Foreman-Mackey
et al. 2013, 2019).
In this paper we introduce zeus, a stable and well-tested Python
implementation of Ensemble Slice Sampling (ESS) (Karamanis &
Beutler 2020). ESS is a method based on the ensemble MCMC
paradigm with the crucial diﬀerence being that its proposals are
performed via Slice Sampling updates (Neal 2003) instead of
Metropolis-Hastings ones. As we will thoroughly demonstrate in
Section 3, this subtle diﬀerence leads to substantial improvements in
terms of sampling eﬃciency and robustness. zeus is a user-friendly
tool that does not require any hand-tuning or preliminary runs and
can scale up to 1000s of CPUs without any extra eﬀort from the user.
zeus has been used in various astronomical and cosmologi-
cal analyses, including cosmological tests of gravity (Tamosiunas
2020), relativistic eﬀects and primordial non-Gaussianity (Wang
et al. 2020), 21cm intensity mapping (Umeh et al. 2021), and has
been implemented as part of the CosmoSIS package (Zuntz et al.
2015).
zeus is open source software that is publicly available at https:
//github.com/minaskar/zeus under the GPL-3 Licence. De-
tailed documentation and examples on how to get started are available
at https://zeus-mcmc.readthedocs.io.
2 ENSEMBLE SLICE SAMPLING
zeus is a Python implementation of the Ensemble Slice Sampling
method presented in Karamanis & Beutler (2020). Here we will pro-
vide a high-level description of the method and will refer to the
accompanying paper for more details about the underlying algorith-
mic structure and mathematics.
ESS combines the ensemble MCMC paradigm with slice sam-
pling. Since the use of slice sampling in astronomical parameter
inference is rare we will start by explaining its function and how it
diﬀers from MH updates. Then we will move on to discuss how it
can be eﬃciently combined with ensemble MCMC.
2.1 Slice sampling
Slice sampling is based on the idea that sampling from a distribution
with density 𝑃(𝑥) is equivalent to uniform sampling from the area
under the plot of 𝑓(𝑥) ∝𝑃(𝑥). To this end, we introduce an auxiliary
variable 𝑦, called height, such that the joint deﬁnition 𝑃(𝑥, 𝑦) is
uniform over the region 𝑈= {(𝑥, 𝑦) : 0 < 𝑦< 𝑓(𝑥)}. To sample
from the marginal distribution 𝑃(𝑥), we ﬁrst sample from 𝑃(𝑥, 𝑦)
and then we marginalise by dropping the 𝑦value of each sample.
In order to generate samples from 𝑃(𝑥, 𝑦) we utilise the following
scheme (Neal 2003):
(i) Given the current state 𝑥0, draw 𝑦0 uniformly from (0, 𝑓(𝑥0)).
(ii) Find an interval 𝐼= (𝐿, 𝑅) that contains all, or at least part,
of the slice 𝑠= {𝑥: 𝑦0 < 𝑓(𝑥)}.
(iii) Draw the new sample 𝑥1 uniformly from 𝐼∩𝑆.
L
R
R'
L'
x
y
0
0
Figure 1. Illustration of the univariate slice sampling update. Given the cur-
rent sample 𝑥0, a value 𝑦0 is uniformly sampled along the vertical slice
(0, 𝑓(𝑥0)) (dashed line) thus deﬁning the initial point (blue). An interval
(𝐿, 𝑅) is uniformly positioned horizontally around (𝑥0, 𝑦0) and it is ex-
panded in steps of size 𝑅−𝐿until both its ends are outside the slice. The
new sample is generated by repeatedly sampling (uniformly) from the interval
(𝐿′, 𝑅′) until a sample (green star) is found inside the slice. Samples outside
of the slice (red star) are rejected and they are instead used to shrink (𝐿′, 𝑅′).
To construct the interval 𝐼(step ii), Neal (2003) introduced the
stepping-out procedure that works by randomly positioning an in-
terval of length 𝜇around the sample 𝑥0 and then expanding it in
steps of size 𝜇until both its ends are outside the slice. To obtain 𝑥1
we then use the shrinking procedure in which candidates are sampled
uniformly from 𝐼until a point inside the slice 𝑆is found. Samples out-
side of the slice are used to shrink the interval 𝐼. The two procedures
are shown in Figure 1.
The length scale 𝜇is the only free hyperparameter of slice sampling
and although its choice can reduce or increase the computational cost
of the method it generally does not aﬀect its mixing properties (e.g.
convergence rate, autocorrelation time, etc.). zeus utilises a Rob-
bins–Monro stochastic optimization algorithm (Robbins & Monro
1951) in order to tune 𝜇to its optimal value (see Section 3.1 of
Karamanis & Beutler (2020) for more details).
It is important to note here that for multimodal target distributions
there is no guarantee that the approximate slice would cross any of the
other modes. In particular, if the initial estimate of the length scale
𝜇is low then the probability of missing the other peaks, assuming
that they are located far away, is also low. As we will show in Section
3, unlike simple slice sampling, ESS and thus zeus does not suﬀer
from this eﬀect.
2.2 Walkers, moves and parallelism
The slice sampling update described in the previous paragraphs is
a univariate update scheme. For it to be used to sample from mul-
tivariate target distributions it needs to be generalised accordingly.
Perhaps the simplest such generalisation in a multivariate setting is
the use of slice sampling to sample along each coordinate axis in turn
(i.e. component-wise slice sampling) or to sample along randomly
selected directions in parameter space (MacKay 2003). Although
MNRAS 000, 1–11 (2021)

zeus: Lightning Fast MCMC
3
X
X
Y
k
Xl
m
Figure 2. The ﬁgure illustrates the diﬀerential move in the context of En-
semble Slice Sampling. The walker 𝑋𝑘to be updated is shown in red. Two
walkers, 𝑋𝑙and 𝑋𝑚, (blue) are uniformly selected from the complementary
ensemble (grey). The approximate slice (dotted line) is constructed parallel
to the two walkers 𝑋𝑙and 𝑋𝑚using the stepping-out procedure. The new
position 𝑌(green) of 𝑋𝑘is sampled using the shrinking procedure along the
approximate slice.
valid, both of these approaches are unsuitable in cases of correlated
parameters in which the proper choice of direction can substantially
accelerate mixing.
To address this issue, Tibbits et al. (2014) proposed to orthog-
onalise the parameter space using the sample covariance thus get-
ting rid of linear correlations between parameters. We will instead
follow a diﬀerent, perhaps more ﬂexible, approach to construct an
eﬃcient slice sampler. Our aim is to utilise an ensemble of parallel
chains/walkers that can exchange information about the covariance
structure of the target distribution and thus by-pass the diﬃculties
posed by correlations.
As hinted in the introduction, the ensemble of walkers collectively
sample the target distribution and thus their positions encode infor-
mation about the correlations between the parameters. One way to
take advantage of this information is to use it to construct direction
vectors along which slice sampling can take place. We name the
simplest such algorithm as the diﬀerential move, shown in Figure 2.
Using the diﬀerential move, Ensemble Slice Sampling updates the
position of each walker in the ensemble by slice sampling along a
direction deﬁned by the diﬀerence between two uniformly selected
walkers from the rest of the ensemble (i.e. the complementary en-
semble).
Other moves that generate direction vectors from the comple-
mentary ensemble are possible. zeus oﬀers a collection of them,
including some that utilise clustering algorithms and density estima-
tion methods. As we will show in Section 3, such moves can help
accelerate sampling in diﬃcult cases such as strongly multimodal
distributions. Any distribution of the complementary ensemble can
be used as a valid proposal to generate such direction vectors and
zeus oﬀers a highly ﬂexible interface for the user to deﬁne such a
move or choose one (or a mixture) from the ones that are already
implemented and tested.
To parallelise this process and capitalise on the availability of mul-
tiple CPUs we split the ensemble into two sets of walkers (i.e. active
and passive sets) and choose to update the positions of the active
walkers along direction vectors deﬁned by passive walkers. Then the
passive become active and vice versa and the process is repeated.
The ensemble splitting technique is required in order to parallelise
the algorithm without violating detailed balance. Heuristics to deter-
mine the number of required walkers per application are discussed
in Section 4.
3 EMPIRICAL EVALUATION
For the empirical evaluation of zeus we will use three toy examples
that manifest signiﬁcant aspects of real astronomical applications1
(i.e. linear and non-linear correlations, multimodality) and two real-
world astronomical examples characteristic of modern astronomical
analyses.
3.1 Toy examples
In order to understand the behaviour of zeus in various sampling
scenarios it is important to study its performance in diﬀerent toy ex-
amples that demonstrate diﬀerent characteristics of common target
distributions that arise in astronomical applications. For that reason
we chose three such toy examples. The ﬁrst one is a normal (Gaus-
sian) distribution which by deﬁnition is characterised only by linear
correlation between its parameters. The second toy problem is the
ring distribution, a characteristic example of strong non-linear corre-
lations. The last example is a Gaussian mixture with two components.
While the purpose of the ﬁrst two examples is to study the behaviour
of the algorithm in the presence of linear and non-linear correlations
respectively, the goal of the third example is to demonstrate the ability
of zeus to sample eﬃciently from multimodal target distributions.
We will compare zeus with two popular alternatives oﬀered
by emcee, namely aﬃne–invariant ensemble sampling with the
stretch move (emcee/AIES) and the diﬀerential evolution move
(emcee/DEMC). The main goal of this analysis is to justify our
choice of slice sampling as the basis of zeus instead of MH updates
through the use of simple yet instructive toy examples.
For all three toy examples discussed bellow we adopt the same
analysis procedure, where we initialise the walkers by sampling from
a normal distribution N (1, I) where I is the identity covariance ma-
trix and we discarded 104 samples as burn-in. We will use the ex-
pected squared jump distance and the distribution of steps of the
walkers as the metrics to evaluate the diﬀerent sampling techniques.
3.1.1 The correlated normal distribution
Starting with the normal target distribution it is important to note
here that all three of the methods used in the comparison are aﬃne–
invariant2, meaning that their performance is immune to any linear
correlations between the parameters. Since the normal distribution
incorporates, by construction, only linear correlations (i.e. the 2D
marginal distribution contours look like ellipses), it is the perfect
1 For additional demonstrations on similarly common structures (e.g. the
funnel) we direct the reader to the accompanying paper (Karamanis & Beutler
2020).
2 Diﬀerential evolution Metropolis is only approximately aﬃne–invariant
due to the jitter that it is often added to its proposal. This however has a
negligible eﬀect.
MNRAS 000, 1–11 (2021)

4
M. Karamanis et al.
2
0
2
zeus 
x1
D = 2
D = 10
D = 25
P25(x1)
2
0
2
emcee/AIES 
x1
0
200
400
600
800
1000
Iteration
2
0
2
emcee/DEMC 
x1
0
200
400
600
800
1000
Iteration
0
200
400
600
800
1000
Iteration
Figure 3. The ﬁgure shows numerical results (i.e. walker trajectories/chains for the ﬁrst parameter) demonstrating the performance of the three ensemble MCMC
methods in the case of a normal (Gaussian) target distribution in 2, 10 and 25 dimensions respectively. The last column illustrates the 1-D marginal posterior
corresponding to the ﬁrst parameter 𝑥1 estimated directly from the samples for the 25-dimensional case.
0
2
4
6
8
10
r (Separation)
100
101
102
103
104
Number of steps
Normal distribution in 25 dimensions
zeus
emcee/AIES
emcee/DEMC
Figure 4. This ﬁgure shows the distribution of step sizes of walkers for the
three diﬀerent samplers in the case of a normal (Gaussian) target distribution
in 𝐷= 25. It is important to note here that both emcee algorithms exhibit
a peak at zero separation; zeus on the other hand does not due to its non-
rejection nature.
Table 1. The table shows a comparison of emcee/AIES, emcee/DEMC and
zeus in terms of the expected squared jump distance (ESJD; higher is better)
for the three toy examples.
emcee/AIES
emcee/DEMC
zeus
Normal
0.5288
1.1162
2.1354
Ring
0.0043
0.0006
0.1257
Mixture
0.0037
0.0056
0.1015
testing ground to assess the eﬀect that high dimensionality has on
the three methods independently of other complications. For our
example we used a zero-mean normal distribution with a covariance
matrix in which the diagonal elements are set to 1 and the oﬀ-
diagonal ones are equal to 0.95. We then proceed by sampling the
aforementioned distribution in 2, 10 and 25 dimensions. Based on
Figure 3 one can see that the walkers of emcee/AIES dissolve into
an ineﬃcient random walk characterised by low step size and high
autocorrelation time as the number of parameters increases. zeus
and emcee/DEMC are not so severely aﬀected by the high number
of parameters exhibiting a substantially lower autocorrelation.
Let us now try to explain this diﬀerence in behaviour by looking
into the distribution of the steps of the walkers in Figure 4. By
step we mean the distance spanned in parameter space by a single
walker in a single iteration. This is a fundamental measure of the
eﬃciency of an MCMC method and it is directly related to the
expected squared jump distance (Pasarica & Gelman 2010) (see Table
1). One thing to notice here is that the distribution of the steps
of zeus’s walkers extends signiﬁcantly further away than those of
emcee/AIES and emcee/DEMC. This should come as no surprise
since the construction of the approximate slice allows for larger steps
than MH updates as shown in Table 1. This is because when a
proposal is rejected in slice sampling the approximate slice shrinks
and another sample is proposed instead. This way zeus’s walkers
always move and the chance of staying ﬁxed is zero unlike MH-
based updates in which frequent rejection of samples is a necessity.
This aforementioned procedure leads to greater steps in parameter
space. The diﬀerence between emcee/AIES and emcee/DEMC is
attributed to the fact that DEMC uses a proposal scale3 𝛾= 2.38/
√
𝐷
that guarantees a constant acceptance rate accounting for the number
3 The proposal scale 𝛾is similar to 𝜇used in ESS in the sense that its value
determines the length scale of the proposed jumps in parameter space. A high
value would lead to large steps that are often rejected and a low value would
lead to small steps that are often accepted but do not carry the walkers far.
For such methods, a balance must me found.
MNRAS 000, 1–11 (2021)

zeus: Lightning Fast MCMC
5
2
1
0
1
2
zeus 
x1
D = 2
D = 10
D = 25
P25(x1)
2
1
0
1
2
emcee/AIES 
x1
0
200
400
600
800
1000
Iteration
2
1
0
1
2
emcee/DEMC 
x1
0
200
400
600
800
1000
Iteration
0
200
400
600
800
1000
Iteration
Figure 5. The ﬁgure shows numerical results (i.e. walker trajectories/chains for the ﬁrst parameter) demonstrating the performance of the three ensemble MCMC
methods in the case of the ring target distribution in 2, 10 and 25 dimensions respectively. The last column illustrates the 1-D marginal posterior corresponding
to the ﬁrst parameter 𝑥1 estimated directly from the samples for the 25-dimensional case. One can notice here that in 10 and 25 dimensions both emcee methods
mix very slowly. In the 25-dimensional case almost all of emcee/DEMC’s walkers are unable to move and the autocorrelation time is eﬀectively inﬁnite.
0.0
0.5
1.0
1.5
2.0
r (Separation)
100
101
102
103
104
105
Number of steps
Ring distribution in 25 dimensions
zeus
emcee/AIES
emcee/DEMC
Figure 6. This ﬁgure shows the distribution of step sizes of walkers for the
three diﬀerent samplers in the case of a ring target distribution in 𝐷= 25. It
is important to note here that both emcee algorithms exhibit a peak at zero
separation; zeus on the other hand does not. The existence of the zero-peak
in emcee is due to the high number of rejected proposals (i.e. low acceptance
rate).
of dimensions 𝐷. This proposal scale is however optimal only in
the case of a normal target distribution such as the one that we are
studying here and there is no guarantee that it would return acceptable
results in non-Gaussian distributions. For the case of emcee/AIES,
the relevant proposal scale 𝛾is allowed to vary in the range between
1/𝛼and 𝛼where 𝛼= 2 is often taken as the typical value. It is
clear that in the latter case 𝛾does not possess the desired scaling
𝛾∝1/
√
𝐷and thus, although the method generates proposals in the
right overall direction, most of the samples do not reside in the bulk
of the posterior mass (Speagle 2019). In other words, the lack of
proper scaling of the proposal scale with the number of dimensions
leads to emcee/AIES “overshooting” the typical set where most of
the posterior mass is located.
The above discussion allows us to clearly state a crucial distinction
between the three methods, that is their response to the curse of di-
mensionality. As the number of dimensions increases, the probability
mass of a distribution is concentrated into a thin shell at the tails of the
distribution (i.e. the typical set). To account for this and maintain its
eﬃciency, a sampling method has to adjust its proposal scale other-
wise the proposals will not be located in the typical set and thus they
will not be accepted. The three methods that we mentioned so far deal
with this in diﬀerent ways. emcee/AIES’s proposal scale is not ad-
justed and thus its proposals become increasingly ineﬃcient in high
dimensions. emcee/DEMC’s proposal scale is adjusted based on the
theoretical expectation for the case of the normal target distribution.
Although both emcee methods perform well in this example, their
sub-optimal scaling will degrade their performance in non-Gaussian
target distributions as we will demonstrate in the next toy example.
Finally, zeus’s proposal scale is continuously adapted, as the slice
expands and contracts in every iteration, thus guaranteeing optimal
scaling.
3.1.2 The ring distribution
The ring distribution deﬁned as
ln 𝑃(𝑥) = −
"
(𝑥2𝑛+ 𝑥2
1 −𝑎)2
𝑏
#2
−
𝑛−1
∑︁
𝑖=1
"
(𝑥2
𝑖+ 𝑥2
𝑖+1 −𝑎)2
𝑏
#2
,
(2)
where 𝑎= 2, 𝑏= 1 and 𝑛is the total number of parameters, is an
artiﬁcial target distribution that exhibits strong non-linear correla-
tions between its parameters. This aspect of the ring distribution al-
lows us to demonstrate the locally adaptive nature of zeus. Whereas
emcee/AIES and emcee/DEMC use a single global proposal scale
for all regions of the parameter space, zeus has the ability to adjust
its proposal scale locally by expanding the slice appropriately. As
MNRAS 000, 1–11 (2021)

6
M. Karamanis et al.
1.0
0.5
0.0
0.5
1.0
zeus 
x1
D = 2
D = 10
D = 25
P25(x1)
1.0
0.5
0.0
0.5
1.0
emcee/AIES 
x1
0
200
400
600
800
1000
Iteration
1.0
0.5
0.0
0.5
1.0
emcee/DEMC 
x1
0
200
400
600
800
1000
Iteration
0
200
400
600
800
1000
Iteration
Figure 7. The ﬁgure shows numerical results (i.e. walker trajectories/chains for the ﬁrst parameter) demonstrating the performance of the three ensemble MCMC
methods in the case of a two-component Gaussian mixture target distribution in 2, 10 and 25 dimensions respectively. The last column illustrates the 1-D
marginal posterior corresponding to the ﬁrst parameter 𝑥1 estimated directly from the samples for the 25-dimensional case. Whereas all three samplers make
valid within-mode proposals, it is only zeus that manages to perform between-mode jumps and thus sample correctly from the target distribution in the 10 and
25-dimensional cases. Between-mode jumps are paramount to distribute the probability mass correctly between diﬀerent modes.
0
1
2
3
4
5
r (Separation)
100
101
102
103
104
Number of steps
Gaussian Mixture in 25 dimensions
zeus
emcee/AIES
emcee/DEMC
Figure 8. This ﬁgure shows the distribution of step sizes of walkers for the
three diﬀerent samplers in the case of a two-component Gaussian mixture
target distribution in 𝐷= 25. It is important to note here that both emcee
algorithms exhibit a peak at zero separation; zeus on the other hand does not
due to its non-rejection basis.
expected, this will allow zeus to sample eﬃciently even in cases in
which strong non-linear correlations are present. Looking at Figure
5 one can see that zeus manages to generate multiple samples ef-
ﬁciently even in high dimensions. On the other hand, emcee/AIES
and emcee/DEMC do not eﬃciently produce valid proposals: for
emcee/AIES this leads to an ineﬃcient random walk, characterised
by small steps; for emcee/DEMC the acceptance rate almost vanishes
beyond 𝐷= 2. The expected squared jump distance of each method
for the case of 𝐷= 25 is shown in Table 1. It is important to note
here that out of the three samplers only zeus manages to converge
in all three cases (i.e. in 2, 10 and 25 dimensions). emcee/AIES
and emcee/DEMC on the other hand converge successfully only in
2 dimensions.
To explain this result one only has to look at the distribution of
walker steps of the diﬀerent methods at Figure 6. zeus’s steps extend
to large distances in parameter space whereas most of emcee/AIES’s
and emcee/DEMC’s steps are rejected (i.e. shown as zero in the
histogram). We can see that emcee/DEMC manages to perform some
long distance steps but those are few and there is almost nothing in
between. It is clear from this and the previous toy examples that
the 𝛾= 2.38/
√
𝐷scaling of emcee/DEMC’s scale factor does not
generalise well beyond the Gaussian case.
3.1.3 The two-component Gaussian mixture distribution
One other important aspect of astronomical posterior distributions is
the fact that many of them exhibit multiple peaks. Multimodality can
arise either from non-linear models or sparse and uninformative data.
In either case, multimodal target distributions present a formidable
challenge for most MCMC methods. Perhaps the simplest example
of such a distribution is the two-component Gaussian mixture. In this
example we will position the two, equal-mass, components at −0.5
and +0.5 respectively with standard deviation of 0.1. Sampling from
multimodal distributions requires two types of proposals, local pro-
posals that sample diﬀerent modes individually and global proposals
that transfer walkers from one mode to the other. For this reason
we will make use of zeus’s GlobalMove that uses a Dirichlet Pro-
cess Gaussian Mixture model of the ensemble to eﬃciently propose
between-mode and within-mode steps.
As seen in Figure 7, zeus’s walkers manage to move from one
mode to the other frequently enough for mixing to be eﬃcient even
in the 𝐷= 25 case. Out of emcee/AIES and emcee/DEMC, only
the latter proposes valid steps from one mode to the other in the
𝐷= 2 case. As for the 𝐷= 25 case, one can see in Figure 8 that
zeus’s walkers perform numerous jumps whereas emcee’s walkers
MNRAS 000, 1–11 (2021)

zeus: Lightning Fast MCMC
7
1
2
B
0.95
1.05
0.9
1.1
||
5
15
10
25
||
1
5
FOG
1
2
0
20000
45
10000
0
44
0
2000
43
100
0
42
0
1
41
30000
0
25
0
20000
24
4000
0
23
50
50
22
0.0
0.8
21
15000
0
05
0
10000
04
2000
0
03
0
30
02
0
01
0
01
0
30
02
2000
0
03
0
10000
04
15000
0
05
0.0
0.8
21
50
50
22
4000
0
23
0
20000
24
30000
0
25
0
1
41
100
0
42
0
2000
43
10000
0
44
0
20000
45
1
2
1
5
FOG
10
25
||
5
15
0.9
1.1
||
0.95
1.05
zeus
emcee/AIES
emcee/DEMC
Figure 9. A corner plot showing the 1-D and 2-D marginalised posteriors for the 22-parameter Baryon Acoustic Oscillation model as produced by the three
diﬀerent ensemble MCMC methods.
are unable to do so. The ability of the walkers to jump from mode
to mode is of paramount importance if we want to sample correctly
from the target distribution. Lack of such proposals will lead to
an improper probability mass ratio between the two modes and thus
biased inference. The expected squared jump distance of each method
for the case of 𝐷= 25 is shown in Table 1.
Clustering-based proposals have also been applied to MH-type
ensemble MCMC methods but as shown in Karamanis & Beutler
(2020), they fail to generate valid proposals in problems with mod-
erate number of dimensions. The reason is, as discussed in Section
3, that MH has to propose a valid point in the other mode. In other
words, whereas Ensemble Slice Sampling only needs to get the di-
rection of the other mode relative to the chosen walker correctly,
MH requires to guess both the direction and the distance, a task that
rapidly becomes very hard as the number of dimensions rises.
3.2 Real astronomical analyses
The previous section, through the use of toy examples, shows various
scenarios that might emerge during sampling and how zeus is better
equipped to handle them. To demonstrate the eﬃciency of zeus
compared to other samplers in realistic target distributions we chose
MNRAS 000, 1–11 (2021)

8
M. Karamanis et al.
20.88
Pb
0
4
jit
7
4
0.000
0.002
0.004
0.1
0.0
1
2
lnKc
0.6
0.0
0.6
esin
c
0.5
0.0
0.5
ecos
c
2082.60
2082.65
Tconjc
42.335
42.360
42.385
Pc
1
2
lnKb
0.6
0.0
0.6
esin
b
0.5
0.0
0.5
ecos
b
2072.78
2072.80
2072.82
Tconjb
2072.79
Tconjb
0.3 0.3
ecos
b
0.4 0.4
esin
b
1
2
lnKb
42.36
Pc
2082.62
Tconjc
0.4
0.4
ecos
c
0.5 0.5
esin
c
1
2
lnKc
0
0.002
7
4
0
4
jit
zeus
emcee/AIES
emcee/DEMC
Figure 10. A corner plot showing the 1-D and 2-D marginalised posteriors for the 14-parameter radial velocity model as produced by the three diﬀerent ensemble
MCMC methods.
two common astronomical inference problems as the testing ground.
Those are the cases of baryon acoustic oscillation (BAO) parameter
inference and exoplanet parameter estimation.
We used the same three samplers in our comparison, namely emcee
with AIES and DEMC, and of course zeus. We performed three
distinct tests:
• The ﬁrst of which was to estimate the eﬃciency, deﬁned as the
number of independent samples produced per log-likelihood evalu-
ation, for each sampler. To this end, we ran the MCMC procedure 5
times for each sampler and computed the mean eﬃciency using the
estimated autocorrelation time of the chains.
• The second test that we performed relates to the convergence
rate of the three algorithms. As convergence rate we deﬁne the inverse
of the number of iterations required until all the convergence criteria
are met. In order to estimate the mean convergence rate we ran the
sampling procedure 40 times for each sampler initialising the walkers
close to the Maximum a Posteriori (MAP) estimate.
• Finally, we tested the sensitivity of the samplers to the initial
conditions by running 40 realisations with the walkers initialised
from randomly chosen regions in the prior volume and counting how
MNRAS 000, 1–11 (2021)

zeus: Lightning Fast MCMC
9
many of those attempts led to converged chains before a predeter-
mined number of likelihood evaluations.
To determine whether a chain has converged we used four diﬀerent
metrics: the Gelman-Rubin split-R statistic (Gelman et al. 1992,
2013), the Geweke test (Geweke 1992), a minimum length of the
chain as a multiple of the integrated autocorrelation time (IAT) as
well as an upper bound on the rate of change of the IAT. The number
of walkers used in both examples was close to the minimum value
of 2 × 𝐷. As we will discuss in Section 4 this often leads to faster
convergence.
3.2.1 Cosmological inference
The particular inference problem that we face here is that of the
anisotropic BAO parameter inference using estimates of the galaxy
power spectrum. The data we used come from the 12th data re-
lease (DR12) of the high-redshift North Galactic Cap (NGC) sample
as observed by the Sloan Digital Sky Survey (SDSS) (Eisenstein
et al. 2011) Baryon Oscillation Spectroscopic Survey (BOSS) (Daw-
son et al. 2013). Our analysis follows closely that of Beutler et al.
(2017) with the diﬀerence that we chose not to ﬁx any parameters
and ﬁt the hexadecapole multipole of the power spectrum as well
as the monopole and quadrupole. Those choices were made solely
to render the problem more challenging. Indeed the inclusion of the
hexadecapole does not contribute any additional constraining power
for the data that we used. However, such extended models will prove
useful when analysing data from larger galaxy surveys such as DESI
(DESI Collaboration et al. 2016). In terms of Bayesian inference,
the problem has 22 free parameters. The results of our analysis are
consistent with those of Beutler et al. (2017). We used weakly in-
formative ﬂat (uniform) priors for all parameters except for the two
scaling parameters, 𝛼∥and 𝛼⊥for which we used normal (Gaussian)
priors. We used 50 walkers in total.
In terms of eﬃciency, zeus generates at least 5 independent sam-
ples for each one generated by emcee/DEMC and at least 9 for each
one generated by emcee/AIES factoring in the diﬀerent computa-
tional cost of the methods. As for the convergence rate, zeus con-
verges more than 3 times faster than either emcee variant. Finally, we
found that zeus is less sensitive to the initialisation than either of the
other two methods. In particular, out of the 40 tests conducted with
diﬀerent initialisation, zeus converged 36 times, emcee/DEMC 14
times and emcee/AIES 7 times prior to the predetermined maximum
number of likelihood evaluations (i.e. 106 in this case). The afore-
mentioned results are presented in detail in Table 2. The 1-D and 2-D
marginal posterior distributions are shown in Figure 9 demonstrating
the agreement between the three methods4.
3.2.2 Exoplanet inference
Another common application of MCMC methods in astronomy is the
problem of exoplanet parameter inference through modelling of Kep-
lerian orbits and radial velocity time series data. In this section we will
demonstrate the performance of zeus using a two-planet model with
14 free parameters and real data from the K2-24 (EPIC-203771098)
extrasolar system (Petigura et al. 2016) that is known to host two
exoplanets. We used the popular Python package RadVel (Fulton
4 No upper limit on the number of likelihood evaluations or iterations was
used for this run and convergence was diagnosed using all the metrics that we
introduced.
Table 2. The table shows a comparison of emcee/AIES, emcee/DEMC and
zeus in terms of the eﬃciency (i.e. number of independent samples per
likelihood evaluation), the convergence rate and the convergence fraction (i.e.
times that each method converged starting from a diﬀerent initialisation). Both
the eﬃciency and the convergence rate are normalised by the emcee/AIES
value.
emcee/AIES
emcee/DEMC
zeus
Cosmological inference
eﬃciency
1.0
1.8
9.2
convergence rate
1.0
1.1
3.7
convergence fraction
7/40
14/40
36/40
Exoplanet inference
eﬃciency
1.0
4.1
29.3
convergence rate
1.0
2.1
7.5
convergence fraction
23/40
29/40
38/40
et al. 2018) for the Keplerian modelling of the planetary orbits. The
results of our analysis are consistent with published constraints for
the aforementioned extrasolar system (Petigura et al. 2016). We used
30 walkers in total for sampling.
We performed the same suite of tests as in the cosmological infer-
ence case. In terms of eﬃciency, zeus generates more than 7 inde-
pendent samples per each one generated by emcee/DEMC and more
than 29 independent samples per each one generated by emcee/AIES.
As for the convergence rate, zeus converges 7.5 times faster than
emcee/AIES and 3.5 faster than emcee/DEMC on average. Finally,
we found again that zeus is less sensitive to the speciﬁc initialisa-
tion of the walkers. In particular, out of the 40 tests conducted with
diﬀerent initialisation, zeus converged 38 times, emcee/DEMC 29
times and emcee/AIES 23 times prior to the predetermined maxi-
mum number of likelihood evaluations (i.e. 103 in this case). Detailed
results about the values of the used metrics are shown in Table 2. The
1-D and 2-D marginal posterior distributions are shown in Figure 10
demonstrating the agreement between the three methods.
4 DISCUSSION
So far we have neglected any discussion about the initialisation of the
walkers. There are two important questions that need to be answered.
First, how many walkers are necessary and, second, how to choose
the initial positions of the walkers. Although there are many ways of
answering those questions and no consistent solution that works for
all target distributions we will try to provide some general rules and
heuristics to help make the task of choosing the number and initial
positions of the walkers an easy task for most cases.
Let us ﬁrst discuss the eﬀect of the number of the walkers on the
general performance of zeus. It is clear that the absolute minimum
number of walkers is equal to twice the number of dimensions5. If
a smaller number is chosen then the walkers can be trapped in a
lower-dimensional hyper-plane of the parameter space, being unable
to sample properly and leading to erroneous results. Although there
is no upper bound on the number of walkers, we recommend to use
between two to three times the number of dimensions. The reason is
that increasing the number of dimensions can increase the duration
5 The reason that the minimum is not 𝐷+ 1 as one might expect is the
ensemble splitting technique that was introduced in Section 2 to render the
algorithm parallelisable.
MNRAS 000, 1–11 (2021)

10
M. Karamanis et al.
of the burn-in period. Ideally, one wants to use the minimum number
(or close to that) of walkers until the burn-in period is over and then
increase the number of walkers to rapidly produce a great number of
independent samples. It is also worth noting that in cases in which
either non-linear correlations or multiple modes are present it is
recommended to use more walkers (e.g. 4-6 times the number of
parameters for a bimodal target distribution).
As for the initialisation of the walkers, there are many ways to
choose their starting positions ranging from prior sampling to more
localised initial positions. Empirical tests indicate that the latter often
outperforms the former (i.e. leads to shorter burn-in periods). That
is not surprising since the total probability of a prior-sampled ini-
tialisation can be very small when the number of parameters is high.
In particular we found that initialising the walkers from a tight re-
gion in parameter space (i.e. normal distribution with small variance)
consistently leads to good performance. For low to moderate dimen-
sional problems initialising the walkers from a tight ball around the
Maximum A Posteriori (MAP) estimate can substantially reduce the
burn-in period (Foreman-Mackey et al. 2013).
Finally, while emcee/AIES and emcee/DEMC can sample accept-
ably from most target distributions with 𝐷≲20, the eﬃcient scaling
of zeus with the number of parameters allows us to extend this range
and eﬃciently test more complicated models (Karamanis & Beutler
2020). Like most gradient-free methods, zeus will fail to sample
eﬃciently in very high dimensional problems in which 𝐷= O(102).
In such cases, more sophisticated algorithms (e.g. tempering, block
updating, etc.) need to be used (Robert et al. 2018).
5 CONCLUSIONS
The aim of this project was to develop a tool that could facilitate
Bayesian parameter inference in computationally demanding astro-
nomical analyses and tackle the challenges posed by the complexity
of the models and data that are often used by astronomers. To this end,
we introduced zeus, a parallel, black-box and gradient-free Python
implementation of Ensemble Slice Sampling.
After introducing the method in Section 2, we thoroughly demon-
strated its performance compared to two popular alternatives (i.e.
emcee with aﬃne-invariant ensemble sampling and diﬀerential evo-
lution Metropolis) using a variety of artiﬁcial and realistic target
distributions in Section 3. The artiﬁcial toy examples helped to shed
light on the general behaviour of the samplers in target distributions
characterised by linear and non-linear correlations as well as multi-
modal densities. When compared to emcee/AIES and emcee/DEMC
in the problems of Baryon Acoustic Oscillation parameter inference
and exoplanet radial velocity ﬁtting, zeus consistently converges
faster (i.e. its burn-in is shorter by a factor of at least 3 times), it is
less sensitive to the initialisation of the walkers and generates sub-
stantially more independent samples per likelihood evaluation (i.e.
approximately ×9 and ×29 speed-up compared to emcee/AIES in the
cosmological and exoplanet examples, respectively).
We have shown that zeus performs similarly or better than ex-
isting MCMC methods in a range of problems. We hope that zeus
will prove useful to the astronomical and cosmological community
by complementing existing approaches and facilitating the study
of novel models and data over the coming years. zeus is pub-
licly available at https://github.com/minaskar/zeus with de-
tailed documentation and examples that can be found at https:
//zeus-mcmc.readthedocs.io.
ACKNOWLEDGEMENTS
The authors would like to express their gratitude to anyone who con-
tributed to the early success of zeus by incorporating it into their own
research. In particular, we thank Andrius Tamosiunas, Jamie Donald-
McCann, Mike Wang and Obinna Umeh for providing feedback on
an early version of the code. This project has received funding from
the European Research Council (ERC) under the European Union’s
Horizon 2020 research and innovation programme (grant agreement
853291). FB is a University Research Fellow.
zeus and this work have beneﬁted from a variety of Python
packages including numpy (Van Der Walt et al. 2011), scipy (Vir-
tanen et al. 2020), matplotlib (Hunter 2007), seaborn (Waskom
2021), getdist (Lewis 2019), sklearn (Pedregosa et al. 2011),
tqdm (da Costa-Luis 2019), and mpi4py (Dalcin et al. 2011).
DATA AVAILABILITY
All data used in this work are publicly available. Power spec-
trum estimates, covariance matrices and window functions used
in the cosmological inference example are available at http://
www.sdss3.org/science/boss_publications.php. Radial ve-
locity measurements of the K2-24 system used in the exoplanet
inference are available as part of the RadVel package at https:
//github.com/California-Planet-Search/radvel.
REFERENCES
Betancourt M., 2017, preprint (arXiv:1701.02434)
Beutler F., et al., 2017, MNRAS, 464, 3409
DESI Collaboration et al., 2016, preprint (arXiv:1611.00036)
Dalcin L. D., Paz R. R., Kler P. A., Cosimo A., 2011, Adv. Water Resour., 34,
1124
Dawson K. S., et al., 2013, AJ, 145, 10
Eisenstein D. J., et al., 2011, AJ, 142, 72
Foreman-Mackey D., Hogg D. W., Lang D., Goodman J., 2013, PASP, 125,
306
Foreman-Mackey D., et al., 2019, preprint (arXiv:1911.07688)
Fulton B. J., Petigura E. A., Blunt S., SinukoﬀE., 2018, PASP, 130, 044504
Gelman A., Rubin D. B., et al., 1992, Stat. Sci., 7, 457
Gelman A., Carlin J. B., Stern H. S., Dunson D. B., Vehtari A., Rubin D. B.,
2013, Bayesian Data Analysis. CRC Press
Geweke J., 1992, Bayesian Stat., 4, 641
Gilks W. R., Roberts G. O., George E. I., 1994, J. R. Stat. Soc. Series D (The
Statistician), 43, 179
Goodman J., Weare J., 2010, Comm. App. Math. Comp. Sci., 5, 65
Hastings W. K., 1970, Biometrika, 57, 97
Hunter J. D., 2007, IEEE Ann. Hist. Comput., 9, 90
Jaynes E. T., 2003, Probability Theory: The Logic of Science. Cambridge
University Press
Karamanis M., Beutler F., 2020, preprint (arXiv:2002.06212)
Lewis A., 2019, preprint (arXiv:1910.13970)
MacKay D. J., 2003, Information Theory, Inference and Learning Algorithms.
Cambridge University Press
Metropolis N., Rosenbluth A. W., Rosenbluth M. N., Teller A. H., Teller E.,
1953, J. Chem. Phys., 21, 1087
Neal R. M., 2003, Ann. Stat., pp 705–741
Pasarica C., Gelman A., 2010, Stat. Sin., pp 343–364
Pedregosa F., et al., 2011, J. Mach. Learn. Res., 12, 2825
Petigura E. A., et al., 2016, ApJ, 818, 36
Robbins H., Monro S., 1951, Ann. Math. Stat., pp 400–407
Robert C. P., Elvira V., Tawn N., Wu C., 2018, preprint (arXiv:1804.02719)
Speagle J. S., 2019, preprint (arXiv:1909.12313)
Tamosiunas A., 2020, preprint (arXiv:2011.08786)
MNRAS 000, 1–11 (2021)

zeus: Lightning Fast MCMC
11
Ter Braak C. J., 2006, Stat. Comput., 16, 239
Tibbits M. M., Groendyke C., Haran M., Liechty J. C., 2014, J. Comput.
Graph. Stat., 23, 543
Umeh O., Maartens R., Padmanabhan H., Camera S., 2021, preprint
(arXiv:2102.06116)
Van Der Walt S., Colbert S. C., Varoquaux G., 2011, Comput. Sci. Eng., 13,
22
Virtanen P., et al., 2020, Nat. Methods, 17, 261
Wang M. S., Beutler F., Bacon D., 2020, MNRAS, 499, 2598
Waskom M. L., 2021, J. Open Source Softw., 6, 3021
Zuntz J., et al., 2015, Astron. Comput., 12, 45
da Costa-Luis C. O., 2019, J. Open Source Softw., 4, 1277
ter Braak C. J., Vrugt J. A., 2008, Stat. Comput., 18, 435
APPENDIX A: CRONUS
Along with zeus, we introduce cronus, a Python package that
wraps zeus as well as other samplers (e.g. emcee). cronus is de-
signed to be used via the terminal through simple, human-reabable
parameters ﬁles. cronus implements an automated suite of conver-
gence diagnostics and all its methods are massively parallel using the
Message Passing Interface (MPI). Our aim is to automate parameter
inference and facilitate highly reproducible analyses for current and
future astronomical surveys such as the Dark Energy Spectroscopic
Instrument (DESI) (DESI Collaboration et al. 2016).
This paper has been typeset from a TEX/LATEX ﬁle prepared by the author.
MNRAS 000, 1–11 (2021)

