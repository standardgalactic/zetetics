BAYESIAN FACTOR ANALYSIS
by
Gordon M. Kaufman*
and
S. James Press**
May 1973
_MM 
6 
-7:3
Not to be quoted without the authors' permission.
*
Gordom M. Kaufman, M.I.T. Sloan School of Management
S. J. Press, University of Chicago, Graduate School of Business
and the Department of Economics
This Working Paper also appeared in the University of Chicago
series as Report 7322, Center for Mathematical Studies in
Business and Economics, Department of Economics and Graduate
School of Business, April 1973.

Underscored letters denote boldface, type
March, 1973
Bayesian Factor Analysis
by
G. M. Kaufman 
S. J. Press
Massachusetts Institute 
University of Chicago
of Technology
1. Introduction
In this paper we discuss the problem of factor analysis from the
Bayesian viewpoint. First, the classical factor analysis model is
generalized in several directions. 
Then, prior distributions are
adopted for the parameters of the generalized model and posterior dis-
tributions are developed in the light of observed data. 
Finally, we
develop a large sample marginal posterior distribution for the elements
of the factor loading matrix. It will be seen that the Bayesian
approach provides a formal mechanism for using subjective prior infor-
mation to eliminate ambiguities and dogmatic constraints ever present
in earlier factor analysis models.
There has been a large literature devoted to the classical factor
analysis problem (see, for example, [7] Mulaik, 1972 and his 183 refer-
ences). However, though subjective information has been brought to bear
on many earlier analyses in an informal and often ad hoc way, a formal
Bayes approach was not suggested until recently (See 
8] Press, 1972,
p. 317). 
That approach, however, was limited by the constraints of the
classical factor analysis model. Below, we will show how generalizing
the form of the classical ieodel permits an operationally useful Bayesian
solution to be generated.
On leave at the Departments of Administrative Sciences and Statistics,
Yale University, 1972-1973.

2
Let X1, ..., X 
denote independent and identically distributed
observations of the random m-vector X, and suppose
X 
+ 
A 
f 
sE
(mXl) 
(mxl) 
(mxs) (sX) 
(mxl)
where 
(f) N(O, I) and 
() - N(O, _), 
> 
(X 
is 
a positive definite symmetric matrix); that is, 
the
probability law of 
f 
is 
an s-variate normal distribution with zero
mear. and identity covariance matrix, and the probability law of 
is
a normal distribution with mean zero and positive definite covariance
matrix 
S. 
Suppose 
f 
and 
e 
are independent. 
The s-vector 
f 
is
often called the vector of common factors, and with var(f) 
I, the
various factors are postulated to be mutually orthogonal; we will treat
only the orthogonal factor model here since the extension of these results
to an oblique factor model is 
conceptually straightforward. 
A 
is often
referred to as the factor loading matrix and it 
is 
assumed to be non-random;
its 
rows represent m sets of weights to be assigned to the 
s 
factors.
E (called the disturbance or error vector) denotes a vector which includes
both the effects of factors specific to each of the components of 
X,
and also, terms which are intended to account for non-linearities and
measurement errors in the model. The expected value of X is denoted by
A. All terms on the right hand side of (1.1) are assumed to be unobservable.
1.1 Classical Model
The classical model of factor analysis presupposes that

3
(a) 
is diagonal, and
(b) s 
m.
The explanation of (a) is that any correlation observed among the
components of 
X is accounted for by the common factors (so that
the elements of e 
are uncorrelated). The implication of (b) is that
there are no more factors than there are observed attributes (and
usually, s 
is assumed to be considerably smaller than m). The
first problem of classical factor analysis is to specify s 
and to
estimate A and the diagonal elements of 
.
Then, checks on the
goodness of fit of the model must be made, and it is sometimes also
of interest to estimate the factors themselves.
1.2 Bayesian Model
The Bayesian model of factor analysis developed below assumes the
following:
(c) 
is a general positive definite symmetric covariance matrix,
(d) s 
m,
(e) rank (A) - m.
The factor analysis model with assumptions (c) - (e) will be seen to
be more general and flexible than the classical model. 
The cost of
greater generality and flexibility is that we will be required to assess
informative prior distributions for a large set of parameters. 
A broad
comparison of the two approaches is given below.

4
1.3 
Comparison of Classical and Bayesian Models
Adoption of the Bayesian viewpoint has a major advantage: 
it
allows explicit introduction of subjective judgment about model
structure in a way that copes neatly with specification error and with
the identification problem that characterizes factor analytic models.
(A precise account ensues below.) 
The arbitrary and often dogmatic
assumptions commonly introduced in order to obtain unique estimators
of model parameters are replaced with multivariate prior distributions
of high dimensionality which can express all degrees of subjective
views ranging from dogmatic assumptions regarding the parameters to
complete vagueness. All of the usual problems that arise when encoding
subjective 
udgments about many ointly dependent uncertain quantities
are present: 
What constitutes a class of flexible yet analytically
tractable prior distributions? 
How does one elicit a particular member
of this class in practice? 
And so on. 
Each of the escape routes
commonly employed in the literature--Jeffreys -like diffuse priors,
natural conjugate priors-- suffers from one or more fatal defects, as
will be seen in the next section, and so, we will be forced to consider
more complicated functional forms for priors.
The Bayesian model has been posited with more underlying common
factors than observable attributes (in diametric opposition to the
classical approach). 
In almost any real world situation we take only a
small fraction of the total number of possible measurements. That is,

5
there are generally a great many factors underlying any real object
or construct but only a few observations are taken on each object.
If we grant that s 
m in the real world, how then is parsimony
to be achieved? That is, how will it be possible to conclude our
analysis with a representation of the observed data in a lower dimen-
sional space, a common objective of factor analysis? The answer
lies in our choice of the prior distribution. 
Partition the factor
loading matrix and the factor vector as shown below (where t 
denotes
transpose):
'.
(mXrx)
ft
(sxl)
( (A1)
(mxp)
=(fl
(pxl)
A2)
(mx(8-p))
,p<s 
.
t(s-p)
((s-p-xl)
r
Suppose we believe a priori that p factors are really adequate to
explain the bserved date, p 
s .
(In som problems it may be
desirable to take 
p < m .
This is not a general requirement,
however.) 
This subjective belief can be translated into a formal
prior distribution by assuming that a priori, 
A2
lies in a small
neighborhood of the zero matrix. 
With this partitioning,(l1) becomes
X- 
+Af 
+ 
Al 
Af + 
.
(1.2)
31

6
By adjusting the hyperparameters (the parameters of the prior distribu-
tion) appropriately, the effect of the 
(A2f 2 ) 
term in the posterior
distribution can be made as insignificant as is deemed desirable. 
How-
ever, one never really knows with certainty that exactly p factors
"explain" the X's. A more common state of affairs is 
that one has a
strong (subjective) belief that among 
s 
candidates for factors, 
p
of them account for a large portion of the variance of the X's. 
Thus,
in the Bayesian model there is considerable flexibility regarding the
choice of number of factors and how important they are relative to one
another.
In the Bayesian model, by permitting components of 
to be
correlated we account for the possibility of specification error; that
is, 
omission of one or more factors from the model. By contrast, the
classical model assumes all components of 
are uncorrelated (assumption
(a)).
Finally, it will be seen that the rotation problem of classical
factor analysis can be completely circumvented by implementing the Bayesian
analysis described below. That is, if subjective information is introduced
through the class of priors suggested, the usual rotational identification
problem will disappear so that all parameters of the factor loading
matrix will be identified, and at least in large samples, it will be
straightforward to make posterior inferences about the loadings since the
appropriate distribution will turn out to be normal (truncated).

7
The problem of parameter identification is discussed in the next
section. 
Then in Section 3, we develop the main result of the paper,
namely, the posterior distribution of the factor loading matrix and
obtain a modal point estimator. 
In Section 4, it is explained how
informative prior information may be brought to bear on the problem of
assessing a particular member of the appropriate family of prior distri-
butions.
2. 
Identification
n 
n
Let 
nl 
X 
and S 
-X) 
-
) t 
denote the
Ij 
1 
-} 
-o
sample mean and covariance matrices, respectively. 
Since 
(X) 
N(, 
,
where 
(ij) 
-
AAt + 
, 
(S,N) 
is sufficient for LA,), N = n -
1
(we are not generally interested in making inferences about 
). 
Thus,
the likelihood function is given by
(AzIS,N) = AAt + 2 N/2xp[(-1/2)tr(AAt + z)- S 
(2.1)
In the sequel we shall assume that N 
m so that S > 0 with probability
one. 
Note that the likelihood function may be written in the equivalent form
Y .(A,iS,N) 
-
JiN/2exp[(-1/2) 
tr -
S] 
(2.2)

8
where 
L denotes a function with 
m(m + 1)/2 
functionally
independent components 
wij and 
L is constant over all A and 
Z
such that AAt + Z = 
.
Since 
is composed of 
m(m + 1)/2
functionally independent elements and 
(Mt + 
) 
is composed of
[ms + m(m + 1)/2] 
functionally independent elements we are faced with
an identification problem akin to that that arises in the analysis of
simultaneous equation systems 
and Dr&ze's commentary thereupon applies
with equal force- here:
A *mmon feature of all these studies is that
the a priori restrictions are imposed in exact form,
through equalities or inequalities with known, non-
stochastic coefficients. 
Typically, however, the
a priori information provided by economic theory,
past observation (casual or systematic), etc,, is not
of such an exact nature and would be better reflected
in probabilistic statements. The weakness of the
traditional approach is thus twofold in my opinion: 
.,
(i) 
restrictions are imposed in exact form, whereas
they should rather take a stochastic form, reflecting
the imprecise nature of the prior information; (ii) cer-
tain parameters are left completely unrestricted,
about which some prior information is available....
In other words, the traditional approach lacks flex-
ibility in that it relies upon too few a priori res-
trictions that are, however, too strict. A more
realistic formulation would call for submitting more --
eventually all -- 
parameters to be estimated to stochas-
tic a priori restrictions.
The Bayesin counterpart of the identificaton problem we face is this: 
Leot
A be a measurable subset of Euclidean 
-space; then the probability that the
random matrix 
AeA , conditional on I , remains the same no matter how
2
much data generated according to (1.1) is observed. 
Namely, given a set
1 cf. Draze [1], and Fisher [2].
Dreze [l] calls attention to this feature of the problem of inference
about parameters of a simultaneous equation.

9
of observations generated according to (1.1) and leading to (S,N),
and given that we have assigned a prior density f (A,S_) Jointly to
A and 
, the conditional (prior) density p(Ali) 
of A given a
is unmodified by (S,N) .
This is almost self-evident upon factoring
f'(A,R) into p(Ajt)k(a) where k() 
is the marginal prior density
of Q_ , and then observing that the likelihood function (2.1) bears
on _? alone, as in (2.2), whereupon Bayes Theorem implies that the
joint posterior density of A and f is proportional to
p(Apj x k(I)L* (2jS,N)
However, the assertion that p(Aja) remains unaffected by obser-
vation of data leading to 
(S,N) must not be taken as implying that
the data is uninformative about A .
As N 
, 
converges in
0
probability to the true value, say 2 , of 
2 , 
and the posterior
marginal density of A converges (pointwise) to p(A|Q- ) 
In this
sense, as more and more data accumulates the Bayesian procedure picks
out with increasing probabilistic accuracy that member of the set
~{p(Aj2)|>O} yof @wdtioal prior densities of 
A. given 
( 
correspon-
0
ding to the true uderlyiag value 
of 
.
The marginal density of 
A posterior to observing 
(S,N) will in
general differ from the marginal prior density of A , 
and may do so
in such a way as to permit identification of a unique modal value of
A a posteriori, thus resolving the identification problem in a certain
sense. The a priori restrictions commonly imposed when doing maximum
likelihood estimation of A are very dogmatic and yet leave us faced
with a non-unique estimator of 
A , one invariant with respect to post-
multiplication by an orthogonal matrix. 
The Bayesian solution is more
appealing in principle, but more difficult to implement, demanding

10
careful coherent formulation of a large number of stochastic a
priori restrictions which may vary in nature from problem to problem.
3. 
Prior to Posterior Analysis
3.1 Non-informative Priors
Our approach to analysis of the model (1.1) with assumptions (c),
(d), and (e) is to consider various classes of distributions as
candidates for priors, and then to develop posteriors by means of Bayes
theorem. A natural starting point is to consider the class of non-in-
formative (vague) prior densities 
of the form
m+l
D (A,) 
1 .z -
2 
.
(3.1)
Priors of this form are invariant with respect to various groups of
transformations (depending-on the value of a) and have been used ex-
tensively in the Bayesian literature (the case of a 0 corresponds
to the isffreys [5] Jvffiiant prior for 
) .
Besinanng with the alter-
nate parameterization A and f , a non-informative (marginal) density
for 
is of the same functional form as that of 
; however, 
A given
a is restricted to be such that 
-
AAt > 0 , so that assignment of a
joint prior density
Primes will denote prior densities and 
ouble primes will denote
posterior densities. 
The symbol 
will denote proportionality.

11
a--
h'(A, _n) 
lIal 
2
for 
- AAt > 0; and equal to zero otherwise implies that
D'(a, t) 
I 
1Mt+ zl 
2
in place of (3.1). 
While compelling in many contexts such priors are
now shown to be unsatisfactory here.
Bayes theorem requires that if 
D (A,E) 
denotes the posterior
density of 
(A,)
D (i._) 
D (A,) 
(AS,N) 
.
(3.2)
Substituting from (2.1) and (3.1) gives
Is
D (A,E) 
exp{(-l/2)tr(AAt 
+ 
) -S
Iz ('+l)/2lAAt + zIN/2
I
where
D (A) 
D (A,E)d
£>0
denotes the marginal posterior density of 
A .
Noting that since
t1t 
I 
1
D (-A) 
D (A) , if 
E(A|S) 
exists at all we must have 
E(AIS) = 
;
D (A) 
is proper only for a> 
2 
or 
N - m + 1

12
that is, 
there are on the average no underlying factors which "explain"
the variance in 
X .
The same argument holds for a prior of the form
h'(A, 
) .
This is an unsatisfactory situation and results from our
having selected a class of prior distributious which was not rich enough
to accommodate the demands of our problem. 
The broader class considered
below will in fact lead to a satisfactory result.
3.2 
Informative Priors
Suppose prior information is introduced in the form of a joint
density on 
(A,_) , instead of on 
(A,E) 
.
Properties of a joint prior
density for 
(A,Q) 
of the form
h'(A,l) 
exp-(l/2)tr(A -
2)V2 (A - A)tG}
(3.3)
x 
D
1}a(vl)/2epi (l2)tr 
i
for 
-
AAt > 0 , and equal to zero otherwise, wi novw be explored.
Here 
YtA) 
denotes the quadratic form -0 
+ (A.- 
A1 )V!(A 
-
)t 
.
Factoring
h'(,) 
h(R)h (Aln)
h'(AQ) is seen to be a truncated noal 
density, and A is restrictei
to lie in 
(A: 
-
AAt > 0 .
Factoring

13
h' (j,n_) 
hj,(nQlA 
(A) ,
hi(jIA) 
is sen to be a truncated inverted Wishart density, and n 
is
restricted to lie in 
{a: 
- AAt
> O}
For very large samples it is intuitively reasonable to replace 
1 
1 
o
in h2(A{1.) 
with N S , since 1 S converges in probability to n °
the true underlying value of 
, and to regard h() as concentrated
in a very small neighborhood of 
S .
Upon doing o, h(AI) 
is
approximately proportional to
exp{-(l/2)tr[G(A - A2 )V2(A _ )t + NS 
(A)]} 
,
which upon completing the square in rows (or in columns) of A can be
seen to be a normal density concentrated on
(A: 
S - AAt > 0) ; that is, 
a truncated normal density. 
The ensuing
development renders this intuitive argument precise , showing that the last
expression is also the form of the asymptotic marginal posterior density
of 
A as N 
.
The most striking feature of this density is that it
allows the possibility of a tnique mode, and in so doing offers an a posteriori
estimator that resolves the rotational problem.

14
Now suppese th.t a prorori n I 
fnJlo 
l 
n 
nvoprted Wirhart
distribution (truncated, since Q2 is bounded below by 
AAt)
Specifically, adopt the conditional prior density
c(A) 
h l(1-A) 
- -(F 
)/2 exp{(-1/2)tr[Q l,(A)]} 
, 
(3.4)
for v > 
-
1 
and 
(A) > 
0 , 
where 
(A) 
is defined in (3.3), c(A)
denotes the proportionality constant (which depends on A) renderng
h l( iA) 
a proper density, and where 
(A,) 
is restricted to lie in
{(A,f) 
: 
> 0 ,n 
-
AAt > o} 
.
Note that 
f{A, 
, 
' 
Vlv} 
is a set
of hyperparameters which must be assessed on the basis of prior informa-
tion. 
(cf. Section 4 for methods of assessment). 
The hyperparameters sa-
tisfy the constraints: 
>
V >_, 
v 
> o .
The constant 
c(A) 
is evaluated as follows. 
Since
n-ll- 
_
1 exp(-1/2) tr[a->(l) ]}
t-{i
2-~AA '>o

15
Transforming from fS to H = 
-1 
, with Jacobian J(Q + H) = IH -(m1)
gives
[c(A)] 
HI(v-m- )/exp{(-1/2)tr[ 
(A)]dH 
.
(A^t ) -H>O
Now transforming from X to P 
w 
(l/2)[_()] 1/ 2H[(A)]l/2 
with
Jacobian 
J(H + P) 
|(A) 
, gives
[c(A) ]-i 
_(A) I-v/2 f 
'pi(v-m-Z)/2 -tr _p
K-P>O
where K(1l/2) [(A) /2 (AAt)- [(A)]/2 .
Note that up to a scale constant,
the last integral is the nultivariate analogue of the incomplete gamma
function. From Herz 
3], p. 488, we may express the last integral as
a confluent hypergeometric function of matrix argument. Thus,
[c(A)]-l 
(A) I-/2/2IK 
1/2 
F 
_ 
; 
+
m + 1 
K
-- 
=p 
(F 
-
-
11 
; 
-K 
) 
Recalling that the hypergeometric functions are symmetric functions of
their arguments, and using the definition of K , gives
[c(A)]-
1
, IAAtl-v/2 F 
1 (/2Fl(AA ) 
(3.5)
2~~~~~~.5
1 
(A)]/2 , and all other matrix square roots in the sequel, are
assumed to be positive definite symmetric matrices.

16
Combining (3.4) and (3.5) gives the joint prior density
h (A ,) 
h2(A) IthI /2expf (-1/2)tr -1 p(A) }
IQI 
1)/ 
1 
; 
+2 +1 
(-1/2) (AAt) - w(A)
(3.6)
Substituting (2.2) and (3.6) into Bayes theorem ((3.2) with Z replaced
by 
) gives the joint posterior density
h,(A)IAAt I 
exp{(-1/2)tr[l ((A) + S}l}
h (A, 2) 
-(v 
+1 
-
-m 
-
1F1 
; 
2 
(-1/2)(At) 
~ A).
(3.7)
when 
Q _- AAt 
> 
; h (A,_Q) 
is zero in the region where .
-
AAt
is not positive definite. The marginal posterior density of the factor
loading matrix is given by
h (A) 
h (A,2)d 
.
2 -A t
> O0
(3.8)
Substituting (3.7) into (3.8) gives
II
h () 
h2 (A) AA 
/2I (A)
(2 
2) 
_l 
(-/ 
(A)
1F1 
2 -1 
2 
; (-1/2)(AAt ) ---
'i 
+m+ 
t 
Jl!(A
(3.9)
I,

17
where
exp{(-1/2)tr 
[(A) + S]}
() j 
-
-
-
dR .
(3.9)
:---(mtN+1)/2
..IAAt>0 
-Ao
Using the same argument used to evaluate c(A) 
above, we find
F(v+ N 
v + N + m+ 1;-(1 /2)(AAt)-l [j(A) + S])
~IA) 
W~ 
IAAtl (WV+N)/2 
(3.10)
Substituting (3.10) into (3.9) gives
v + N 
V+N+m+l 
t-l
't+N/v+ 
1 F 1
m 
2 
!;-(1/2)(AAt)-[p(A) 
+ S]
h (A) = h2 (A)jAAt -N / 2 x 1
F
. 1 
2 
.
/2)(A
(3.11)
The next problem is to select the class of marginal prior densities
on A so that the class is not only sufficiently rich to accommodate
diverse types of prior information, but also, it yields a proper and
statistically meaningful posterior density, consistent with analytical
tractibility. We suggest the class with hyperparameters {A1, A2, V1, V2
I, 
G V}
_2
>o 
> O,
t -v/2 F 
v 
+m 
; 
1 ;-(1/2) (AAt)-1
x exp{(-l/2)tr[G(A -'A2)V2(A 
A2)] } 
(3.12)
which upon being combined with 
hl(_JQA) 
as displayed in (3.4) gives a
joint prior of the form (3.3).

18
Substituting (3.12) into (3.11) generates the class of marginal
posterior densities
+ N 
+ N + m+ 
t 
-
h (A) 
2 
22 (-2--; 
-1/2(AA ) 
[!(A) + s5)
h (A~) 
~I!AAtl 
(v+N)/2
x exp{(-1/2)tr[G(A -
A2)V 2(A 
-
A)t] } 
(3.13)
The result in (3.13) can now be evaluated numerically for given
hyperparameters and a given data covariance matrix S .
The task is
not simple, however. For example, the 
IF1 function might be expressed
in a zonal polynomial series expansion (see, e.g., James [4], p. 477).
However, as James pointed out in [4], p. 497:
... for numerical evaluation of the probability
density functions, the power series expansions.,.
of the hypergeometric functions occurring in the
distributions are of limited value. 
If even one
root is significant, it will take a very large
number of terms of the series to give values of
the likelihood function accurate enough to be of
use.
Alternatively, we shall assume we have a moderately large data set
and shal seek a large sample approximation to the result in (3.13).
3.3 Asymptotics
Define
JN*(A) 
p (v+N-m-1)/2e-tr 
dp 
(3.14)
-NJ,(.4

19
where 
- 1/2[(A) + S]/2(AAt) 
[(A) + S] 1 / 2
.
From the analysis
and evaluation of 
c(A) 
in Section 3.2 it 
may be inferred that
+ N 
+ N +m+
iFi (--2; 
2 
2 
; -
1/2(AAt) 
[*(A) + S] 
J(A)
F();+ m ltlJN(A)
Atl(V+N)/2 
IS + (AI (N)/2
(3.15)
It is shown in Appendix 1 that while JN(A) varies with both N 
and
A , for sufficiently large N , JN(A) may be treated as if it were not
dependent upon A .
Using this result by absorbing JN(A) into the
proportionality constant, and substituting (3.15) into (3.13), yields
the large sample marginal posterior density for A
,, 
exp{(-1/2)tr[G(A - _A)V2(A 
, 
-
) t]}
h_(A) -
()+N 
(3.16)
is 
(A) i(v+)/2
Next note that as N -+ 
, (S/N) converges in probability to the
true value 
_ of Q2 so that in the limit, the asymptotic posterior
density of A becomes concentrated on 0 < A t < 
.
Consequently,
in large samples, h (A) may be regarded as being concentrated on
0 < At < (S/N) .
That is, we take h (A) = 0 if [(S/N) - AAt] 
is
not positive definite.
The result in (3.16) may be simplified still further (in large samples)
as follows. Since

20
I+ 
k() I-(VN)/2 
- Aexp 
{ ( 
2)logIS + 
(A) 
+ j(A)[-(vt*N)/2 
exp 
(i+ NloglI + 
1/2 
-1/2
where 
W 
S1/2 (A)S 1/2 
and the proportionality constant depends only
upon sample data. Note that W is a positive definite symmetric
matrix whose latent roots converge in probability to zero with in-
creasing sample size. Thus, using the argument in 8], page 277 we
find that in large samples,
+ !(A) 
(N)/2 
exp { 
(v 
N)tr 
(A 
(3.17)
Combining (3.17) with (3.16) gives
hl(A) 
exp { (-l/2)tr[(A - A2 )V 2(A 
A2)tG + (S) 
(A)]}, (3.18)
where S 
S/(v + N)
Now recall the matrix identity which holds for any conformable matrices
t 
z 
bt(,,Z), 
(3.
tr(QB t)Z = b (Q Z)b , 
(3.19)
where )denotes the Kronecker product of two matrices, 
- (1' 
"' hr)

21
and 
bt (bt 
-
b) 
.
Accordingly define 
A 
(X , ., 
s ) 
,
Q) 
Q) 
( iD
a 's) 
.>, 
, j = 1, 2, and 
, a 
j = 1, 2
Then from (3.4) and (3.19), (3.18) becomes
h(A) 
A 
exp {(- 1/2)[(A 
a(2) t(v
2
O 
( 
G) 
-(
2) )
-i
+ ( - a(1))t(Vji 
~ )(A 
a(i)} 
(3.20)
Combining terms and completing the square in the exponent gives the large
sample marginal posterior density of the factor loading matrix
exp 
{(-1/2) 
( 
-1
h.(A) 
exp 
(-1/2)(X - ) 
( 
) 
(3.21)
for (S/N) -
A t > O , and h(A) = O0
_ 
__ 
_W 
-
otherwise, where
-1 
= r 
5'] +Vr2 Q)G 
'
and
'3 -I 
(1) 
Y 
iiA i 
ia 
_
_ 
L 
_
(2)~ 
+ [V, , 
Gg~C)a
U3
(3.22)

21-.
That is, the large sample marginal posterior density of A is
truncated normal, concentrated on 
(S/N) -
AAt > O 
0 
with modal value
at 
, defined in (3.21). Thus, if 
(S/N) -
AA 
> 0 , 
a modal estimator
of A is 
A, 
where 
-1' 
and 
t 
t)
If 
(S/N) -
NiAt 
is not positive definite there is no modal estimator.
In correspondence with the a priori udgment that the last s-p columns
A2 
of 
A 
are concentrated in a neighborhood of the zero matrix it 
may be
natural in some circumstances to consider the posterior density of 
A1
given
A2 -
0 in place of that of 
A .
If 
the hyperparameters 
a (1 )
and 
a ( 2 )
have componets 
a(1) 
... 
a( 1 ) 
a(2) 
a(2) 
-s p+! 
then S p+l 
.. 
0 , and the conditional posterior density corresponding
to (3.21) is
h"(A1 {2 -
O) 
exp{(- 1/2)(A 
1-)t 
( 
-
-)} 
(3.23)
-=1~X 
-
1 
-
-11)(323)
t 
t 
t
where 
At 
( 
.. , At)
and l
t
(I1 
t); here A
-1
A1 1 -
A12A 
21 
and A has been partitioned conformably with 
A .
This
Ali 
-12-22-21
conditional density is also truncated normal, but concentrated on
_ 
S- 
At > 
)
iS -
A 
> 0 
(in place of 
S -
AAt > 0)
N 
-1-1 
N- 
-
_ __I_

23
4. Assessment of the Prior Distribution
Numerical evaluation of the modal Bayes estimator, given in (3.22),
depends upon knowledge of the hyperparameters, 
{A1,'A2'- 1
V'V2 0
i, 
v} .
In this section we show that under certain conditions the hyperparamaters
can be assessed in a straightforward way.
Substituting (3.12) into (3.6) yields the joint prior density (3.3),
exp{(-1/2)tr[a-l (A) + 
(A)]}
h (A,') 
(4.1)
[_a (vial)/2
for 
-
AAt > 0 , and h (A,Q) = 0 
otherwise, where
-v 
0(A_) 
a ( - A)
2(A - A )t 
For some problems it is reasonable to take
G -
I .
In others, having G available for free choice isetes as an
additional degree of freedom for imposing oe's subjective beliefs on the
problem and will be welcomed by many assessors.
The. marginal prior density of A is given in (3.12). 
Using the
asymptotic result in (3.15) shows that if 
v 
is large and if 
O is large
in such a way that (O/v) 
remains large, but finite, 
h2() 
may be
written approximately as
,1 ) 
~exp{(-1/2)tr 
(A)}
h. (A) 
1/2
1(^)I v/ 2

24
Now applying to 
S0 the same argument used in Section 3.3 for
S , shows that for large 
v 
and 
0 
(A) 
/ 2
exp{(-v /2)tr SO 
A)V(A - A1))t
Thus, for large v 
and 
, it is approximately true that
h(A) - exp{(-1/2)tr[(A -
A2)V2 (A - A2)t +(S) 
(A - A)V(A -
A1)
]
where 
(/V) 
.
Adapting the result in (3.19) gives
h (A) 
exp{(-1/2)[(I - a(2))t(V2 i 
G) ( 
-
a ( 2 )
t 
-1
(A- a 
) fi(r 
) 
)(a 
))3 
.
Combining terms and completing the square in the exponent gives the
approximate marginal prior density
(-1 
-X*
h (A) 
exp{(-1/2)(X -
)A 
( -
)}
where
*-1
-1
+ Cv2 ®GI 
.

25
and
X"' A {LVQ) 
la' 
+ V 
E) 
G]a
2
That is, it is approximately true (for large v 
and large 
i)
that a priori,
* 
*
(X) = N(A ,A ) 
(4.2)
In order to concentrate the last s-p 
columns, A2
, of A 
in a
neighborhood of the zero matrix as suggested at the outset, set a 
)
a(j)
- 0 for j - 1, 2 and partitioning
-s
v(j) 
()
--'11 
-12
Vj -
: 
(s - P) 
(s - p) 
x (s -
p)
v(J) 
()
-21 
V22
set -22(J) 
tI 
t arbitrarily large. The corresponding variance of 
A2
will then be small. That is, it is approximately true that a ptiori, A2
is 
normally distributed with mean zero and a covariance matrix whose largest
latent root is approximately zero.

26
Next concentrate on 
QA 
.
By substituting (3.5) into (3.4) we
find that for 
-
lA t > O ,
hl( A) =
IAlAt
/2
+ep{ 
(-1/2) tr [ 
(A) 
]I}
il(v+l); 
/2 
; 
+ m + 1 
(-1/2)(A t ) -1(A)
1F 
i 
2 +1 
m+A
Now assume S 
and v 
are large. Adapting the result in (3.15) to
the 
1F1 function in the last equation gives approximately,
1 1
hl (fnl|A) -
I(A) v/2 exp{ (-1/2) tr[l-1(A) }
(4.3)
for 
-
AAt > 
, and hl(flA) - 0 otherwise. But by taking SO
and 
large so that §0 
is also large, most of the mass of the distribution

27
of _lA. 
is moved far from zero so that 
P{f -
AAt > OA 
is close
to unity. Therefore, we assume as an approximation, that (4.3) holds
for all n > , so that a priori, 
1 
follows an inverted Wishart
distribution.
Taking moments in (4.2) and (4.3) shows that a priori, we have
approximately 
(cf. [6])
E(A) 
A 
, var(A) 
= 
,
(4.4)
E(fIA) S 
A 
1)-1 (A -
l)
-
m -
1
, v>m+l
(4.5)
var(wiiIA) =
2
2ii
2
(V. m- 1) (v- m -
3)
V > m+ 3,
and for i 
,
var(wi jlA) -
GII* 
+ v- 
m + 
i2
( -
m)(v -
m -
)(.v -
m -
3)
for v > m + 3 , and
[2 
-
_i 
, + 
.
cov[( w wk)I 
(V 
-
(m 
-
i- 
)(v 
-
m- 
3)
(V - m)( 
- m -
)(. 
- m -
3)
(4.6)
(4.7)
(4.8)

28
for 
v > m + 3 , where n- 
()ij, 
and 
hij 
denotes the 
(i,j) 
element
of 
(A)
In some situations there is some underlying theory to suggest
values for the first two moments of 
A 
and 
iA 
.
In other situations
one or several factor analyses may have been carried out on earlier data
under similar circumstances. 
In such an event, this earlier data can
be used to fit the moments of 
A 
and 
.2lA 
.
In any case, the
hyperparameters may then be found by solving the moment equations (4.4)
- (4.8) simultaneously, adjusting 
v 
and 
a 
so that they are large
enough to provide any preassigned level of comfort to the analyst that
his approximations are reasonably valid.
For example, suppose we take 
[S /(v -
m -
1)] -
a2I, for some 
a2
to be determined. From (4.5) it is seen that this can be made to be the
dominant term in E(ljA) .
If we wish to ensure that
I 
T 
-AAJ 
> 0 , for given A , we must choose a 
so that for
2t 
tA t 
any z 
O , a z z > zA z .
But it is well known that
(ztAA tz)/(ztz) never exceeds 
NM 
the largest latent root of AAt .
So
we only need to choose a2 larger than OM , for given A, to satisfy the
positive definiteness condition for the mean of 
RJA 
.
Then, choosing
2
a 
even larger provides an increasingly closer approximation to the
theoretical prior distribution.

29
It 
may be noted that since n -
AAt + 
, E(OA) -
AAt + E() 
,
and var 
(A) 
var () 
.
Thus, the task of assessing hyperparameters
may be carried out by thinking about assessing moments of 
E .
This is
quite analogous to the classical factor analysis problem of estimating
communalities.
Suppose the results of an earlier factor analysis were available
and 
(A,) 
were estimated by the analysis as (A,Z) 
.
A reasonable
assessment might then be
E(nlA) -
AA
t + Z -
.
Finally, note from (4.3), and (4.5) that fixing Q and v is sufficient
to define the entire prior distribution of (Q|A) (cf. [8], page 112,
Remark). That is, when 
and v are given, not only is the left hand
side of (4.5) known, but also, the left hand sides of (4.6) -
(4.8).

30
Appendix 1
In this appendix we show that 
JN(A) 
, defined in (3.14),
varies with both 
N and 
A , but for large 
N , the dependence of
J (A) 
on 
A disappears.
Suppose P : m x m is a positive definite symmetric random
matrix with density
v 
2N-m-
2
f(P) 
k(m,N) IPI
exp{-tr P } , 
> 
and 
f(P) 
0
otherwise, where V + N 
m , 
and
[k(m,N) 1-
m
m(m-l)/4Tr V + N + 1 - i
j1
Define
JN (A) 
f (P) 
dP ,
where
_+ =(1/2)( AAt)[*(A) 
+ S]/2 
_
and 
*(A) 
denotes the positive definite symmetric quadratic form
_____ 
____1____1_________111___

31
I(A 
- !o+ ( - Al)
(A - A)t
introduced in (3.4). 
Note from (3.14)
that 
JN(A) 
k(m,N)JN(A) 
.
We now show that for large 
N , 
JN(A)
does not depend upon 
A , therefore, neither does 
JN(A) 
Lima
Theorem Al: 
JN(A) 
1
Proof: 
Since 
P 
follows a Wishart distribution with scale matrix
I/2, 
E(P) ' 
[(V + N)/2]I .
By the strong law of large numbers 
(SLLN.)
rim 
21 
urn 1.if) I 
, 
a.s.,
N-o 1 
+v 
N 
N
and 
lim (S/N) 
Q , 
a.s.
If R 
'l/2(Alt) 
g1f/2 , by the SLLN., lim 
R
, as.,
(because 
(A) 
does not depend upon 
N , 
(A)/N 
converges to zero),
Thus,
9li LNN 
-P 
'
(A.1)
Next note that
and since 
2 -
-1/2 
-1/2
> O. 
-
2 
t
> 
.
Therefore, if (1 -8)
denotes a latent root of 
I -
1 / ZZ 1/, 
(1 -
8) > 0 .
Moreover, since
Convergence almost surely is denoted by a.s.
-
_
1/2 (a- 
I1 1/2 
R- 
-12 
U2 1/21-1

32
0 
is a latent root of 
n-1/2 2-1/2
we must have 0 < 8 < 1 .
Thus,
, and the latter is positive definite,
R may be expanded in a power
series of the form
R = 
I 
(-l/
2
-l/2)
J=0
so that if
00
G -
(2-1/ 2
-1/2), 
where clearly G > 0 , R = I + G 
1
Therefore from (A.1)
Nlim 
LK 
2P
N-o 
N 
N
= G > 0 , a.s.
Equivalently,
lim P( 
_ p) 
n 
-
1 .
N--o PN () 
j ,
Since by definition,
(A) 
P 
-
P > 
= P 
-
) 
the theorem follows.
Thus, for large N , JN(A) depends only upon (m, N) and not on A .
(A.2)
II_________________1_____1^__1__
IL_I______

33
References
[ 1] 
Dr&ze, J. H. (1968) 
"Limited Information Estimation from a Bayesian
Viewpoint," Report No. 6816, Ceter for Operations Research and
Econometrics, Catholic University of Louvain, Heverlee, Belgium.
[ 2] 
Fisher, F. 
(1968) 
The Identification Problem in Econometrics,
New York: 
McGraw Hill, Inc.
[ 3] 
Herz, C. S. 
(1955) 
"Bessel Functions of Matrix Argument", Annals
of Mathematics, Vol. 61, No. 3, pp. 474-523.
[ 4] 
James, A. 
(1964) 
"Distribution of Matrix Variates and Latent Roots
Derived from Normal Samples", Annals of Math. Stat., Vol. 35, pp.
475-501.
[ _7 
Jeffreys, H. (1961) Theory of Probability, Oxford: 
Clarendon Press.
[ 6] 
Kaufman, G. M. (1967) "Some Bayesian Moment Formulae", Report No. 6710,
Center for Operations Research and Econometrics, Catholic University
of Louvain, Heverlee, Belgium.
[ 7 
Mulaik, S. A. (1972) The Foundations of Factor Analysis, New York:
McGraw-Hill Book Co.
[ 8] 
Press, S. J. (1972) Applied Multivariate Analysis, New York:
Holt, Rinehart and Winston, Inc.
--~1 11~~~~..I-- 
--- 
-_·· --- LIC·-____

