Inaccessible Neural Language Models Could
Reinvigorate Linguistic Nativism
Patrick Perrine
California Polytechnic State University
1 Grand Ave, San Luis Obispo, CA, 93410
paperrin@calpoly.edu
Abstract
Large Language Models (LLMs) have been mak-
ing big waves in the machine learning community
within the past few years. The impressive scalabil-
ity of LLMs due to the advent of deep learning can
be seen as a continuation of empiricist lingusitic
methods, as opposed to rule-based linguistic meth-
ods that are grounded in a nativist perspective. Cur-
rent LLMs are generally inaccessible to resource-
constrained researchers, due to a variety of factors
including closed source code. This work argues
that this lack of accessibility could instill a nativist
bias in researchers new to computational linguis-
tics, given that new researchers may only have rule-
based, nativist approaches to study to produce new
work. Also, given that there are numerous critics
of deep learning claiming that LLMs and related
methods may soon lose their relevancy, we spec-
ulate that such an event could trigger a new wave
of nativism in the language processing community.
To prevent such a dramatic shift and placing favor
in hybrid methods of rules and deep learning, we
call upon researchers to open source their LLM
code wherever possible to allow both empircist and
hybrid approaches to remain accessible.
1
Introduction
Large Language Models (LLMs) have been a pop-
ular topic of research among the academic com-
munity (Srivastava et al., 2022). The promise of
a near-general purpose neural model for a variety
of language processing tasks is indeed an attrac-
tive one (Xu et al., 2022). Deep learning has made
signiﬁcant developments in language tasks such as
conversational language understanding (Tur et al.,
2018), spoken/text-based dialog systems (Celikyil-
maz et al., 2018), and natural language generation
from images (He and Deng, 2018). Large Lan-
guage models can be viewed as the natural pro-
gression away from the rigid rule-based systems
that we’ve had since the 1950’s (Chiticariu et al.,
2013), continuing the empiricist mentality of sta-
tistical natural language processing without the po-
tentially costly and context-speciﬁc activity of fea-
ture engineering (Collobert et al., 2011). However,
with large corporations touting their ever-growing,
state-of-the-art models under closed-source code
and payment walls, it could be seen that these
large language models are becoming less acces-
sible. Some organizations have acknowledged the
potential harms that deep learning models could
cause by establishing ethical frameworks (Ashurst
et al., 2022) (Weidinger et al., 2022), but there are
still growing concerns regarding accessibility and
the result of false/irreproducible science (Kapoor
and Narayanan, 2022).
This criticism for empiricist methods is not new
in linguistics-based science, in that Chomsky’s
Poverty of the Stimulus Argument (Stich, 1978)
has a rich history of discussion and debate amongst
linguists, scientists, and philosophers (Laurence
and Margolis, 2001). In this work, we will brieﬂy
introduce this debate over language learning be-
tween nativists and empiricists, relate these topics
to research in natural language processing, and dis-
ucss how the current state of this research is rein-
forcing an imbalance between the two perspectives.
We intend to deliver a neutral ground of analysis,
as we agree that a hybrid approach for NLP re-
search can lead to strong results. The current bias
towards the highly-popular, but inaccessible em-
piricist methods utilizing LLMs could lead to a
new wave of nativism in natural language process-
ing work, following a large backlash against such
empirical methods.
2
Background
We now provide a holistic background on the lin-
guistic and scientiﬁc developments that encompass
this issue.
arXiv:2301.05272v1  [cs.CL]  12 Jan 2023

2.1
The Three Waves of Modern NLP
We will give a brief background on the three main
waves of modern natural language processing re-
search: the rule-based theories popularized by
Noam Chomsky (Chomsky, 1965), the statistics-
based empiricist experiments (Jelinek, 1976), and
today’s popular methodology of deep learning for
natural language processing (Collobert et al., 2011).
The ﬁrst wave is considered to be under a na-
tivist perspective (Laurence and Margolis, 2001),
whereas the latter waves are in support of an em-
piricist lens (Frank et al., 2019).
2.1.1
Rule-based NLP
The concept of viewing language as a static sys-
tem of rules to determine interpretation has been
present as early as the 1830’s (Humboldt, 1836).
Noam Chomsky popularized this perspective in the
domain of linguistics as a challenge to an exist-
ing overbearance of empiricist methods (Chomsky,
1956b; Laurence and Margolis, 2001).
This rule-based approach to linguistics domi-
nated the ﬁeld for decades, following Chomsky’s
mutliple works emphasizing and reinforcing this
doctrine (Chomsky, 1956a, 1957, 1963, 1965;
Chomsky and Halle, 1968). Being based in proposi-
tional logic and a ﬁxed content, rule-based methods
are arguably rather accessible to researchers with
limited resources. These methods continued to be
prevalent in the ﬁeld until the 1970’s, when statisti-
cal methods were proven to be very useful.
2.1.2
Statistical NLP
The roots of statistical language processing stem
from Andrey Markov’s efforts in computing bi-
gram and trigram probabilities (Jurafsky and Mar-
tin, 2022) of vowel/consonant predictions using a
novel as a corpus in 1913 (Markov, 2006). This
n-gram approach was later applied to predicting
sequences of English words (Shannon, 1948). This
popularized the notion of using Markov chains for
use in a variety of applications within and outside
of linguistics.
Chomsky speciﬁcally challenged this use of
ﬁnite-state Markov processes, the processes that
formed n-gram based approaches, to be useless
in serving as a comprehensive cognitive model
of grammatical knowledge in humans (Chomsky,
1956b, 1957; Miller and Chomsky, 1963). This
hindered the progress of probabilistic approaches
in linguistics.
Over a decade later, statistical language process-
ing was revitalized due in part to a series of success-
ful experiments using n-gram models for speech
recognition (Baker, 1975a,b; Jelinek, 1976; Bahl
et al., 1983; Jelinek et al., 1990). These empiricist-
based experiments showed that Chomsky’s nativist
theories do not extend to recognizing speech in real
time as previously proposed (Chomsky and Halle,
1968).
This marked a shift towards looking at language
processing through an empirical lens, where a hy-
pothesis test primarily guides the experimentation
process, rather than theoretical insights (Manning
and Schutze, 1999). After the successful statistical
speech recognition experiments of the mid 1970’s,
statistical NLP reigned as the dominant approach
for decades.
2.1.3
ML-based NLP
Researchers soon began to use shallow neural net-
works to reinforce statistical methodologies in NLP.
In the late 2000’s, the advent of deeper neural net-
works for NLP began to stir when scalable, hierar-
chical language models (Morin and Bengio, 2005;
Mnih and Hinton, 2008) and increased computing
power became available for use by researchers.
Alongside these developments, researchers be-
came tiresome of having to hand-engineer features
for neural networks to learn from, as this can be a
costly and rather context-speciﬁc task (Collobert
et al., 2011). In was in the 2010’s that deep learning
became known more globally (LeCun et al., 2015),
with NLP being a highly prominent application
for deep neural networks. This sparked the current
practice of training large language models in efforts
to create a general model for many language tasks
(Srivastava et al., 2022). In essence, the empiri-
cist era of NLP has persisted to today through the
evolution of deep learning practices. Some appli-
cations of deep learning outside of language have
even used empiricist terms such as tabula rasa very
openly (Silver et al., 2017). The use of deep neural
networks for language tasks has been conﬁrmed to
reinforce empircist ideology (Frank et al., 2019).
3
Deep Learning Can Be Inaccessible
Deep learning as a science has been under ﬁre for a
number of reasons. While there have been encour-
aging results across many application domains of
deep learning and positive insights about their role
in advancing empiricism (Buckner, 2018), deep
learning has garnered skepticsm from both in and

outside of its community (Marcus, 2018; Buckner,
2019).
These critcisms of deep NLP can stem from a
lack of open sourcing of model code and also data
(Klein et al., 2017; Fadel et al., 2019; Chen et al.,
2021; Guo et al., 2022; Xu et al., 2022). These
issues are not exclusive to language processing,
as other domains have reasons to leave aspects of
their experimentation private or inaccessible when
publishing (Siegle et al., 2015; Suresha et al., 2018;
Farooq and Hafeez, 2020; Zuin et al., 2020; Guo
et al., 2022).
We now focus on issues with closed-source large
language models due to their popularity and the re-
cent claims of greater intelligence (even sentience),
as opposed to other models (y Arcas, 2022).
4
Potential Harms
4.1
Potential Harms of Open-Sourcing LLMs
To offer a well-rounded argument in favor of open-
sourcing LLMs, we will brieﬂy cover some intu-
itions behind close-sourcing them in terms of po-
tential harms.
LLMs could be repurposed for malicious pur-
poses, particularly in generative tasks. LLMs have
been seen to learn negative human biases/patterns
in speech such as hate speech, discrimination, and
the promotion of misinformation (Schramowski
et al., 2022). If a powerful, pre-trained LLM is
made open source, then it could be repurposed as
an engine to cause harm across the internet at great
scale (Weidinger et al., 2022). It could also be ar-
gued that open sourcing LLM code that has been
deployed to end-users could pose security risks
(Chang et al., 2020).
We counter the argument of potential LLM mis-
use by malicious parties by arguing that such mod-
els or derivatives of such should not be published
in any form, open or closed source. We argue that
LLM experimental papers that indicate such po-
tential to cause harm at scale should be ﬁltered
out at the publication review stage, something that
has been discussed in the deep learning community
as of late (Ashurst et al., 2022). We also counter
the security concern argument by saying that this
could hold true for all open source software that is
deployable, not just LLMs.
4.2
Potential Harms from Continued
Close-Sourcing of LLMs
We argue that there are more potential harms in the
continued prevalence of close sourced LLM code
than the potential harms of open sourcing them.
4.2.1
Nativist Biases
Given that LLM experiments are becoming so large,
costly, and complex, it is difﬁcult to argue that an in-
dependent researcher can stake a claim in this sub-
ﬁeld. With top publication venues focusing heavily
on empiricist experimentation (Russell and Norvig,
2021), researchers outside the typical corporate
scope of research could be incentivized to explore
nativist, rule-based approaches to solve problems
in the NLP domain. If it is the empiricist group’s
better interest to foster growth in their methodolo-
gies and not opposing methods, steps should be
taken in order to make their approaches accessible.
Also, for hybrid methods to function, an ML-based
solution should be made accessible to combine
with the ruleset from the nativist side. This trend
could be fostering a new generation of Chomsky-
following nativist NLP researchers, which would
not bode well for empiricists if the public begins to
lose interest in deep learning methods for NLP.
4.2.2
Lack of Reproducibility
We mention reproducibility and will further clarify
its meaning due to an also recent, yet broader prob-
lem in deep learning research, the reproducibility
crisis (Kapoor and Narayanan, 2022). Not only are
large language models becoming difﬁcult to repro-
duce, results from other areas of ML are becoming
difﬁcult to produce (de Freitas Pereira et al., 2022;
Towers et al., 2022). Initiatives to measure repro-
ducibility across publication venues have been cre-
ated, such as the ML Reproducibility Challenge.
LLM experiments have been speciﬁcally reviewed
to have a questionable about of reproducibility
(Crane, 2018; Wieling et al., 2018; Cahyawijaya
et al., 2022; Silva et al., 2022). There is also im-
plied to be a signiﬁcant amount of computational
irreproducibility of LLM experimentation, given
model complexity and data, however, we leave this
exploration for future work.
There is some hope in the form of positive re-
producibility reports in deep learning (Gibson and
Cano, 2022). However, this growing amount of
“bad press” for deep learning, speciﬁcally LLMs,
could cause the public to begin distrusting LLM

research. This, again, could trigger a revisiting of
Chomsky’s rule-based theories of language.
4.2.3
Issues in NLP Education
Given the previously mentioned issues, this lack
of accessibility could affect the education of NLP
methods. If students do not have access to code
of LLMs, it could be difﬁcult for them to learn to
implement complex language model code of their
own and learn to keep up with the state of the art. A
lack of reproducibility could also be disenfranchis-
ing to a young, empircist NLP researcher, leading
them to pursue nativist approaches. These issues
could reinforce the use of statistical, pre-deep learn-
ing techniques in the classroom, but it is difﬁcult to
argue that publication venues are interested in shal-
low neural network experimentation at this time.
These issues combine to form an uneven playing
ﬁeld for students to study NLP in empiricist and
hybrid forms. After studying NLP formally, they
may be inclined to commit to nativist methods or
even reinforce the popularity of them at scale.
5
Potential Solution
We ask that publication venues merit open source
LLM experiments signiﬁcantly higher than they
do currently. We believe that this would mitigate
the issues discussed previously in this work. There
seem to be developments occuring now in the deep
learning publication space to help implement this in
a proper form of governance (Ashurst et al., 2022).
6
Conclusion
In this work, we provided a comprehensive history
of natural language processing methodologies over
roughly the past century. We then used this nar-
rative to lead into today’s deep learning practices
used in language processing, and current issues in
an excessive closed sourcing of code for LLMs.
It is our hope that this work inspires researchers
and reviewers to champion open source language
model code in order to pave the way for a more
balanced research space.
References
Carolyn Ashurst, Emmie Hine, Paul Sedille, and Alexis
Carlier. 2022. Ai ethics statements: Analysis and
lessons learnt from neurips broader impact state-
ments. In 2022 ACM Conference on Fairness, Ac-
countability, and Transparency, pages 2047–2056.
Lalit R Bahl, Frederick Jelinek, and Robert L Mer-
cer. 1983. A maximum likelihood approach to con-
tinuous speech recognition. IEEE transactions on
pattern analysis and machine intelligence, (2):179–
190.
James Baker. 1975a. The dragon system–an overview.
IEEE Transactions on Acoustics, speech, and signal
Processing, 23(1):24–29.
James K. Baker. 1975b.
Stochastic Modeling as a
Means of Automatic Speech Recognition. Ph.D. the-
sis, USA.
Cameron Buckner. 2018. Empiricism without magic:
Transformational abstraction in deep convolutional
neural networks. Synthese, 195(12):5339–5372.
Cameron
Buckner.
2019.
Deep
learning:
A
philosophical introduction.
Philosophy compass,
14(10):e12625.
Samuel Cahyawijaya, Alham Fikri Aji, Holy Lovenia,
Genta Indra Winata, Bryan Wilie, Rahmad Mahen-
dra, Fajri Koto, David Moeljadi, Karissa Vincentio,
Ade Romadhony, et al. 2022. Nusacrowd: A call
for open and reproducible nlp research in indonesian
languages. arXiv preprint arXiv:2207.10524.
Asli Celikyilmaz, Li Deng, and Dilek Hakkani-Tür.
2018. Deep learning in spoken and text-based dia-
log systems. In Deep Learning in Natural Language
Processing, pages 49–78. Springer.
Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang,
Honglei Zhang, Peng Cui, Wenwu Zhu, and Jun-
zhou Huang. 2020. A restricted black-box adversar-
ial framework towards attacking graph embedding
models. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 34, pages 3389–3396.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, et al. 2021.
Evaluating large lan-
guage models trained on code.
arXiv preprint
arXiv:2107.03374.
Laura Chiticariu, Yunyao Li, and Frederick Reiss. 2013.
Rule-based information extraction is dead! long live
rule-based information extraction systems! In Pro-
ceedings of the 2013 conference on empirical meth-
ods in natural language processing, pages 827–832.
Noam Chomsky. 1956a. The logical structure of lin-
guistic theory. Synthese, 40(2):317–352.
Noam Chomsky. 1956b. Three models for the descrip-
tion of language. IRE Transactions on information
theory, 2(3):113–124.
Noam Chomsky. 1957. Syntactic structures. In Syntac-
tic Structures. De Gruyter Mouton.
Noam Chomsky. 1963. Formal properties of grammars.
Handbook of Math. Psychology, 2:328–418.

Noam Chomsky. 1965. Aspects of the theory of syntax.
Multilingual Matters: MIT Press.
Noam Chomsky and Morris Halle. 1968. The sound
pattern of english.
Ronan Collobert, Jason Weston, Léon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.
Natural language processing (almost) from
scratch.
Journal of machine learning research,
12(ARTICLE):2493–2537.
Matt Crane. 2018. Questionable answers in question
answering research: Reproducibility and variability
of published results. Transactions of the Association
for Computational Linguistics, 6:241–252.
Tiago de Freitas Pereira, Dominic Schmidli, Yu Linghu,
Xinyi Zhang, Sébastien Marcel, and Manuel Gün-
ther. 2022.
Eight years of face recognition re-
search: Reproducibility, achievements and open is-
sues. arXiv, (2208.04040).
Ali Fadel, Ibraheem Tuffaha, Mahmoud Al-Ayyoub,
et al. 2019.
Arabic text diacritization using deep
neural networks. In 2019 2nd international confer-
ence on computer applications & information secu-
rity (ICCAIS), pages 1–7. IEEE.
Muhammad Farooq and Abdul Hafeez. 2020. Covid-
resnet:
A deep learning framework for screen-
ing of covid19 from radiographs.
arXiv preprint
arXiv:2003.14395.
Stefan L Frank,
Padraic Monaghan,
and Chara
Tsoukala. 2019. Neural network models of language
acquisition and processing.
In Human language:
From genes and brain to behavior, pages 277–293.
MIT Press.
Perry Gibson and José Cano. 2022.
Productive
reproducible workﬂows for dnns:
A case study
for industrial defect detection.
arXiv preprint
arXiv:2206.09359.
Yinpeng Guo, Liangyou Li, Xin Jiang, and Qun Liu.
2022.
Freetransfer-x: Safe and label-free cross-
lingual transfer from off-the-shelf models.
arXiv
preprint arXiv:2206.06586.
Xiaodong He and Li Deng. 2018.
Deep learning in
natural language generation from images. In Deep
Learning in Natural Language Processing, pages
289–307. Springer.
W von Humboldt. 1836.
Uber die verschiedenheit
des menschlichen sprachbaues. Berlin: Konigliche
Akademie der Wissenschaften.
Fred Jelinek, B Merialdo, S Roukos, M Strauss, et al.
1990. Self-organized language modeling for speech
recognition. In Readings in speech recognition.
Frederick Jelinek. 1976. Continuous speech recogni-
tion by statistical methods. Proceedings of the IEEE,
64(4):532–556.
Daniel Jurafsky and James H Martin. 2022.
Speech
and language processing: An introduction to natural
language processing, computational linguistics, and
speech recognition (draft of january 12, 2022).
Sayash Kapoor and Arvind Narayanan. 2022. Leakage
and the reproducibility crisis in ml-based science.
arXiv preprint arXiv:2207.07048.
Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-
lart, and Alexander M Rush. 2017. Opennmt: Open-
source toolkit for neural machine translation. arXiv
preprint arXiv:1701.02810.
Stephen Laurence and Eric Margolis. 2001.
The
poverty of the stimulus argument. British Journal
for the Philosophy of Science, 52(2).
Yann LeCun, Yoshua Bengio, Geoffrey Hinton, et al.
2015. Deep learning. Nature, 521 (7553), 436-444.
Christopher Manning and Hinrich Schutze. 1999.
Foundations of statistical natural language process-
ing. MIT press.
Gary Marcus. 2018.
Deep learning: A critical ap-
praisal. arXiv preprint arXiv:1801.00631.
Andre˘ı Andreevich Markov. 2006. An example of sta-
tistical investigation of the text eugene onegin con-
cerning the connection of samples in chains. Science
in Context, 19(4):591–600.
George A Miller and Noam Chomsky. 1963. Finitary
models of language users. handbook of mathemati-
cal psychology, vol. by r. duncan luce, robert r. bush,
and eugene galanter, 419–91.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable
hierarchical distributed language model. Advances
in neural information processing systems, 21.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
International workshop on artiﬁcial intelligence and
statistics, pages 246–252. PMLR.
Stuart Russell and Peter Norvig. 2021. Artiﬁcial Intel-
ligence: A Modern Approach. Pearson.
Patrick Schramowski, Cigdem Turan, Nico Andersen,
Constantin A Rothkopf, and Kristian Kersting. 2022.
Large pre-trained language models contain human-
like biases of what is right and wrong to do. Nature
Machine Intelligence, 4(3):258–268.
Claude Elwood Shannon. 1948. A mathematical the-
ory of communication.
The Bell system technical
journal, 27(3):379–423.
Joshua H Siegle, Gregory J Hale, Jonathan P Newman,
and Jakob Voigts. 2015. Neural ensemble communi-
ties: open-source approaches to hardware for large-
scale electrophysiology. Current opinion in neurobi-
ology, 32:53–59.

Marília Costa Rosendo Silva, Felipe Alves Siqueira,
João Pedro Mantovani Tarrega, João Vitor Pataca
Beinotti, Augusto Sousa Nunes, Miguel de Mattos
Gardini, Vinícius Adolfo Pereira da Silva, Nádia
Félix Felipe da Silva, and André Carlos Ponce de
Leon Ferreira de Carvalho. 2022.
No pattern, no
recognition: a survey about reproducibility and dis-
tortion issues of text clustering and topic modeling.
arXiv preprint arXiv:2208.01712.
David Silver, Julian Schrittwieser, Karen Simonyan,
Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian
Bolton, et al. 2017. Mastering the game of go with-
out human knowledge. nature, 550(7676):354–359.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta,
Adrià Garriga-Alonso, et al. 2022.
Beyond the
imitation game: Quantifying and extrapolating the
capabilities of language models.
arXiv preprint
arXiv:2206.04615.
Stephen P Stich. 1978. Empiricism, innateness, and
linguistic universals. Philosophical Studies: An In-
ternational Journal for Philosophy in the Analytic
Tradition, 33(3):273–286.
S Suresha, L Kidzi´nski, E Halilaj, GE Gold, and
SL Delp. 2018.
Automated staging of knee os-
teoarthritis severity using deep neural networks. Os-
teoarthritis and Cartilage, 26:S441.
David Towers,
Matthew Forshaw,
Amir Atapour-
Abarghouei, and Andrew Stephen McGough. 2022.
Long-term reproducibility for neural architecture
search. arXiv preprint arXiv:2207.04821.
Gokhan Tur, Asli Celikyilmaz, Xiaodong He, Dilek
Hakkani-Tür, and Li Deng. 2018.
Deep learning
in conversational language understanding. In Deep
Learning in Natural Language Processing, pages
23–48. Springer.
Laura Weidinger, Jonathan Uesato, Maribeth Rauh,
Conor
Grifﬁn,
Po-Sen
Huang,
John
Mellor,
Amelia Glaese, Myra Cheng, Borja Balle, Atoosa
Kasirzadeh, et al. 2022. Taxonomy of risks posed
by language models. In 2022 ACM Conference on
Fairness, Accountability, and Transparency, pages
214–229.
Martijn Wieling, Josine Rawee, and Gertjan van Noord.
2018. Reproducibility in computational linguistics:
are we willing to share? Computational Linguistics,
44(4):641–649.
Frank F Xu, Uri Alon, Graham Neubig, and Vincent Jo-
sua Hellendoorn. 2022. A systematic evaluation of
large language models of code. In Proceedings of
the 6th ACM SIGPLAN International Symposium on
Machine Programming, pages 1–10.
Blaise Agüera y Arcas. 2022. Do large language mod-
els understand us? Daedalus, 151(2):183–197.
Gianluca Zuin, Adriano Veloso, João Cândido Porti-
nari, and Nivio Ziviani. 2020. Automatic tag recom-
mendation for painting artworks using diachronic de-
scriptions. In 2020 International Joint Conference
on Neural Networks (IJCNN), pages 1–8. IEEE.

