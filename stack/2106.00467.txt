The zoo of Fairness metrics in Machine Learning
Alessandro Castelnovoa,1, Riccardo Crupia,1, Greta Grecoa,1, Daniele Regolia,1
aData Science and Artiﬁcial Intelligence, Intesa Sanpaolo S.p.A., Torino, Italy
Abstract
In the recent years, the problem of addressing fairness in Machine Learning (ML) and automatic decision-making has
attracted a lot of attention in the scientiﬁc communities dealing with Artiﬁcial Intelligence. A plethora of diﬀerent
deﬁnitions of fairness in ML have been proposed, that consider diﬀerent notions of what is a “fair decision” in situations
impacting individuals in the population. The precise diﬀerences, implications and “orthogonality” between these notions
have not yet been fully analyzed in the literature. In this work, we try to make some order out of this zoo of deﬁnitions.
Keywords:
Machine learning; Fairness
Introduction
The issue of discrimination bias in Machine Learning (ML)
has gained a lot of momentum in the scientiﬁc community
in the last few years.
It is now widely recognized that data-driven decision mak-
ing is not per se safe from producing unfair or biased de-
cisions, either for ampliﬁcations of biases already present
in the data they learn from, or for algorithmic inaccu-
racies (see, among others, Barocas and Selbst (2016);
Angwin et al. (2016); O’neil (2016)).
The scientiﬁc literature addressing the problem of fairness
in ML has focused in the recent years on two main as-
pects: i) how to measure and assess fairness (or, equiva-
lently, bias), and ii) how to mitigate bias in models when
necessary.
Regarding the ﬁrst aspect, in the last few years an incred-
ible number of deﬁnitions have been proposed, formalizing
diﬀerent perspectives from which to assess and monitor
fairness in decision making processes. A popular tutorial
presented at the Conference on Fairness, Accountability,
and Transparency in 2018 was titled “21 fairness deﬁni-
tions and their politics” (Narayanan, 2018). The number
has grown since then.
The proliferation of fairness deﬁnitions is not per se an
issue: it reﬂects the evidence that fairness is not a precise
concept, and concentrates on itself diﬀerent meanings and
nuances, in turn depending in complex ways on the speciﬁc
situation considered.
Researches, in proposing deﬁnitions, have focused on dif-
ferent intuitive notions of “unfair decisions”, often con-
sidered as the ones impacting people in diﬀerent ways on
1The views and opinions expressed are those of the authors and
do not necessarily reﬂect the views of Intesa Sanpaolo, its aﬃliates
or its employees.
the basis of some personal characteristics, such as gen-
der, ethnicity, age, sexual or political or religious orien-
tations, considered to be protected, or sensitive. Relation-
ships and interdependence of these sensitive variables with
other features useful for making decisions is one of the cru-
cial points, that entangles in complex ways the ultimate
aim of any model, i.e. making eﬃcient decisions, and the
desired goal of not allowing unfair discrimination to im-
pact people’s lives.
Fairness notions proposed in the literature are usually clas-
siﬁed in broad areas, such as: deﬁnitions based on parity
of statistical metrics across groups identiﬁed by diﬀerent
values in protected attributes (e.g. male and female in-
dividuals, or people in diﬀerent age groups); deﬁnitions
focusing on preventing diﬀerent treatment for individuals
considered similar with respect to a speciﬁc task; deﬁni-
tions advocating the necessity of ﬁnding and employing
causality among variables in order to really disentangle
unfair impacts on decisions.
These three broad classes can be further seen as the result
of two important distinctions:
1. observational vs. causality-based criteria;
2. group (or statistical) vs.
individual (or similarity-
based) criteria.
Distinction 1. discriminates criteria that are purely based
on observational distribution of the data from criteria that
try to ﬁrst unveil causal relationships among the variables
at play (mainly through a mixture of domain knowledge
and opportune inference techniques) and then assess fair-
ness in the speciﬁc situation.
Distinction 2. discriminates criteria that focus on equality
of treatment among groups of people from criteria requir-
ing equality of treatment among couples of similar indi-
viduals.
Preprint submitted to Elsevier
June 2, 2021
arXiv:2106.00467v1  [cs.LG]  1 Jun 2021

Regarding aspect ii), i.e.
that of removing bias when
present, several approaches have been introduced in or-
der to provide bias mitigation in data-driven decisions.
Roughly speaking, they fall into three families, depend-
ing on the speciﬁc point along the ML pipeline in which
they operate: pre-processing methods, that try to remove
bias directly from data; in-processing techniques, impos-
ing fairness either as a constraint or as an additional loss
term during training optimization; post-processing meth-
ods, working directly on the outcomes of the model.
The present work focuses on the ﬁrst of these aspects, and
in particular deals with the aforementioned fact that the
number of diﬀerent metrics for fairness introduced in the
literature has boomed. The researcher or practitioner ﬁrst
approaching this facet of ML may easily feel confused and
somehow lost in this zoo of deﬁnitions.
These multiple
deﬁnitions capture diﬀerent aspects of the concept of fair-
ness but, at the best of our knowledge, there is still no
clear understanding of the overall landscape where these
metrics live. This work aims to take a step in the direc-
tion of analyzing the relationship among the metrics, and
trying to put order in the fairness landscape. Table 1 pro-
vide a rough schematic list of fairness metrics discussed
throughout the paper.
As general references for the deﬁnition of fairness in ML we
refer to the book Barocas et al. (2019) and to the papers
Verma and Rubin (2018); Mitchell et al. (2018); Oneto and
Chiappa (2020) for more compact surveys. Chouldechova
and Roth (2018, 2020) provide a general view of the state
of the art of fairness in ML.
In this work, we shall deal with (binary) classiﬁcation
problems, and we shall only make a brief reference to the
problem of multiple sensitive attributes: despite this huge
simpliﬁcations, the landscape of fairness deﬁnitions is nev-
ertheless extremely rich and complex.
The rest of this paper is organized as follows: section 1 pro-
poses an account of the most important sources of bias in
ML models; section 2 introduces individual fairness; sec-
tion 3 is devoted to the most proliﬁc set of deﬁnitions,
namely the group (or statistical) metrics; causality-based
criteria are discussed in section 4.
1. The problem of bias in data-driven decisions
Most sources of bias in data-driven decision making lie in
the data itself and in the way in which they are collected.
It is out of the scope of this manuscript to list and discuss
all the possible shades of biases that can hide in the data,
we want nevertheless to say something about the main
cases that can be encountered in many real-life scenarios.
The most important qualitative distinction, that we draw
from Mitchell et al. (2018) and Barocas and Selbst (2016),
lies between statistical or representation biases, i.e. when
the model learns from biased sampling from the popula-
tion, and historical or societal biases, i.e. when the model
learns from data where decisions are already impacted by
past prejudice.
1.1. Statistical bias
Generally speaking, statistical bias occurs whenever the
data used for model training are not representative of the
true population. This can be due to a form of selection
bias, i.e. when the individuals appearing in the data come
from a non-random selection of the full population. This
happens, for example, in the context of credit lending,
where the information of the repayment is known only for
people that were granted the loan.
Another way in which statistical bias can enter the data
is via systematic measurement errors. This happens when
the record of past errors and performance is systemati-
cally distorted, especially in the case of diﬀerent amount
of distortion for diﬀerent groups of people.
Similarly, it may happen that data are systematically miss-
ing or poorly recorded for entire strata of the population.
One aspect of paramount importance, but often omitted,
is the following: in the context of fairness, even in samples
perfectly representative of the population it may be that
some protected group happens to be a minority group.
All in all, minorities exist. In this case, especially when
the minority group is highly unbalanced, a ML model can
learn less accurately or may learn to discard errors on
that group simply because it is less important in terms
of crude counting. Again, notice that this is not a form
of statistical bias tout court, since in this case the “under-
representativeness” of the protected group is a true feature
of the population.
Nevertheless, this is something very
common, and one of the most important source of unfair
discrimination against protected groups that happen to be
minorities (Hardt, 2014).
This last issue may be reasonably considered as a source
of biased decisions due to poor modeling rather than to a
problem in data. The boundary is subtle, and in any case
this is a matter of nomenclature: the important thing is
to be aware of the risk.
1.2. Historical or societal bias
Even when the data are free from statistical bias, i.e. they
truly represent the population, take into account minori-
ties and there is no systematic error in recording, still it
may be that bias exists simply because data reﬂect biased
decisions.
In most cases, this is due to a form of labelling bias, i.e.
a systematic favour/disfavour towards groups of people at
the moment of creating the target variable from which
the model is going to learn. If the recorded outcomes are
somehow due to human decisions (e.g. a model for grant-
ing loans may be trained on loan oﬃcers’ past decisions),
2

Table 1: Fairness metrics. Qualitative schema of the most important fairness metrics discussed throughout the paper.
notion
use of Y
condition
group
fairness
Demographic Parity
-
equal acceptance rate across groups
Conditional Demographic Parity
-∗
equal acceptance rate across groups
in any strata
error parity
Equal Accuracy
✓
equal accuracy across groups
Equality of Odds
✓
equal FPR and FNR across groups
Predictive Parity
✓
equal precision across groups
individual
fairness
FTU/Blindness
-
no explicit use of sensitive attributes
Fairness Through Awareness
-∗
similar people are given similar decisions
causality-based
fairness
Counterfactual Fairness
-
an individual would have been given
the same decision if she had had diﬀerent
values in sensitive attributes
path-speciﬁc
Counterfactual Fairness
-
same as above, but keeping ﬁxed
some speciﬁc attributes
∗there are exceptions to these cases where Y is actually employed, e.g. CDP conditioning on Y becomes Equality of Odds,
and there are notions of individual fairness that use a similarity metric deﬁned on the target space (Berk et al., 2017).
then we cannot in general trust their objectiveness and
“fairness”.
Other forms of historical bias may be even more radical:
gender bias has a rather long history, and is embedded
in all sorts of characteristics and features in such a way
that it is diﬃcult evaluate its impact and disentangle its
dependence on other variables. Think for example of in-
come or profession disparities, just to name a few out of
many. Thus, this is a situation in which long-lasting biases
cause systematic diﬀerences in features pertaining diﬀer-
ent groups of people.
Again, this is not a form of un-
representativeness of the sample, it is a bias present in the
full population.
Finally, this is not meant to be an exhaustive account, bias
can lurk in the data in a variety of additional ways, e.g.
a poor selection of features may result in a loss of impor-
tant information in disproportionate ways across groups
(Barocas and Selbst, 2016), and many others. We refer
the interested reader to Barocas and Selbst (2016); Baro-
cas et al. (2019); Mehrabi et al. (2019); Hardt (2014), and
references therein.
2. Individual fairness
2.1. Toy examples and notation
To be as clear as possible, in what follows we shall make
several references to 2 toy examples: credit lending deci-
sions, and job recruiting. We call A the categorical ran-
dom variable representing the protected attribute, that
for reference we shall take to be gender, we label with
X all the other (non-sensitive) random variables that the
decision-maker is going to use to provide its yes/no deci-
sions ˆY = f(X, A) ∈{0, 1}; while we label Y ∈{0, 1}
the ground truth target variable that needs to be es-
timated/predicted – typically by minimizing some loss
function L(Y, ˆY ).
e
X = (X, A) collectively represent all
the features of the problem.
We denote with lowercase
letters the speciﬁc realizations of random variables, e.g.
{(x1, y1), . . . , (xn, yn)} represent a dataset of n indepen-
dent realizations of (X, Y ). We employ calligraphic sym-
bols to refer to domain spaces, namely X denotes the space
where features X live2.
2.2. Similarity-based criteria
Individual fairness is embodied in the following principle:
similar individuals should be given similar decisions. This
principle deals with the comparison of single individuals
rather than focusing on groups of people sharing some
characteristics. On the other hand, group fairness starts
from the idea that there are groups of people potentially
suﬀering biases and unfair decisions, and thus tries to reach
equality of treatment for groups instead of individuals.
The ﬁrst attempt to deal with a form of individual fairness
is in Dwork et al. (2012), where the concept is introduced
2Technically, since we employ uppercase letters to denote random
variables, it would be more proper to say that X is the image space
of X : Ω→X, where Ωis the event space, and that x ∈X. We shall
nevertheless use this slight abuse of notation for sake of simplicity.
3

as a Lipschitz condition of the map f from the feature
space to the model space:
distY (ˆyi, ˆyj) < L × dist e
X(exi, exj),
(1)
where distY and dist e
X denote a suitable distance in target
space and feature space, respectively, and L is a constant.
Loosely speaking, a small distance in feature space (i.e.
similar individuals) must correspond to a small distance
in decision space (i.e. similar outcomes). They also pro-
pose an approach to enforce such kind of fairness by intro-
ducing a constrained optimization at training time, once
a distance on the feature space is given.
The concept of individual fairness is straightforward, and
it certainly resonates with our intuitive notion of “equal-
ity”.
Formula (1) provides also an easy way to assess
whether the decision model ˆY = f( e
X) satisﬁes such con-
cept. However, the major drawback of this type of deﬁ-
nitions lies in the subtle concept of “similar individuals”.
Indeed, deﬁning a suitable distance metric dist e
X on fea-
ture space embodying the concept of similarity on “ethi-
cal” grounds alone is almost as tricky as deﬁning fairness
in the ﬁrst place.
Indeed,
while
for
target
space
the
natural
choice
distY ( ˆYi, ˆYj) =| ˆYi −ˆYj| will do, for feature space the nat-
ural choice of the euclidean distance on the space A × X
implies that any smooth function f shall satisfy condi-
tion (1), but this is not a very interesting notion of fairness.
The point is that one should come up with a distance that
captures the essential features for determining that target,
and that do not mix with sensitive attributes. But, again,
this is not very diﬀerent from deﬁning what is fairness in
a speciﬁc situation.
Take, e.g., the job recruiting framework: what identiﬁes
the couples of individuals that should be considered sim-
ilar and thus given the same chance of being recruited?
Maybe the ones with same level of skills and experience
irrespective of anything else?
One possibility is to deﬁne similar individuals as couples
belonging to diﬀerent groups with respect to sensitive fea-
tures (e.g.
male and female) but with the same values
for all the other features. With this choice, what we are
requiring is that its outcome should be unchanged if we
take an observation and we only change its protected at-
tribute A. This concept is usually referred to as Fairness
Through Unawareness (FTU) or blindness (Verma and Ru-
bin, 2018), and it is expressed as the requirement of not
explicitly employing protected attributes in making deci-
sions.
Notice that this idea is very likely one of the ﬁrst that
one may think of when asked for, e.g., a decision-making
process that does not discriminate against gender: not to
explicitly use gender to make decisions, i.e. ˆY = f(X). In-
deed, this concept is also referred to as disparate treatment,
i.e. there is disparate treatment whenever two individuals
sharing the same values of non-sensitive features but dif-
fering on the sensitive ones are treated diﬀerently (Barocas
and Selbst, 2016; Zafar et al., 2017).
Unfortunately, despite its compelling simplicity, fairness
through unawareness comes not without ﬂaws. Firstly, if
it is true that reaching FTU is straightforward, it is not
that easy to assess. The problem of bias assessment in a
model consists in measuring whether the model decisions
are biased given a set of realizations of (A, X, ˆY ), namely
{(a1, x1, ˆy1), . . . , (an, xn, ˆyn)} – the corresponding values
of Y are needed as well for some criteria. In this setting,
it is tricky to measure whether FTU holds, the main reason
being that it is more a request on how the model works
rather than a request on properties of the output decisions.
One possible candidate metric is the following:
consistency = 1 −1
n


n
X
i=1

ˆyi −1
k
X
xj∈kNN(xi)
ˆyj


,
(2)
introduced in Zemel et al. (2013). Namely, for each obser-
vation (xi, ˆyi) it measures how much the decision ˆyi is close
to the decisions given to its k nearest neighbors kNN(xi)
in the X space3. Notice that it may happen that the k
neighbors of, say, a male individual are all males: in this
case consistency (2) would in fact be equal to 1, but this
does not prevent the model from explicitly using A in mak-
ing decisions. Another possibility, partly inspired by Berk
(2009), would be to compute
1
n1n2
X
ai=1,
aj=0
e−dist(xi,xj)|ˆyi −ˆyj|,
(3)
which measures the diﬀerence in decisions among men and
women weighted by their similarity in feature space: the
higher its value the higher the diﬀerence in treatment for
couples of similar males and females.4 On the other hand,
it is much easier to assess FTU if we also have access to
the model: we could create a synthetic dataset by ﬂipping
A5: {(a′
1, x1), . . . , (a′
n, xn)}, feeding it to the model to get
the corresponding outcomes ˆy′
1, . . . , ˆy′
n, and then compute
the average 1
n
Pn
i=1|ˆyi −ˆy′
i|.
However, the main drawback of FTU is the following: it
does not take into account the possible interdependence
between A and X. Other features may contain informa-
tion on the sensitive attribute, thus explicitly removing
the sensitive attribute is not suﬃcient to remove its in-
formation from the dataset. Namely, it may be that in
3Notice that also in the computation of kNN one has to choose
a distance function on X.
4The term e−dist(xi,xj) can be substituted with any measure of
similarity of the points xi and xj.
5This holds if A is binary. But it is easy to come up with gener-
alizations to the multiclass case.
4

the actual dataset there is a very low chance that a male
and a female have similar values in all the (other) features,
since gender is correlated with some of them. One may or
may not decide that (some of) these correlations are legit-
imate6. This is one of the reasons why the deﬁnition of a
issue-speciﬁc distance is crucial for a more reﬁned notion
of individual fairness.
One straightforward way to deal with correlations is to
develop a model that is blind not only with respect to
the sensitive attribute, but also to all the other variables
with suﬃciently high correlation with it. This is a method
known as suppression (Kamiran and Calders, 2012). Apart
from the obvious issue in deﬁning a good threshold for cor-
relation above which a predictor should be removed, this
approach has the main drawback in the potentially huge
loss of legitimate information that may reside in features
correlated with the sensitive attribute.
One line of research has tried to solve this issue by “clean-
ing” the training dataset in order to remove both the sensi-
tive attribute and the information coming from the sensi-
tive attribute “lurking” inside other features, while keeping
the features. This can be done, e.g., by using residuals of
regressions of the (correlated) features on the sensitive at-
tribute (Berk, 2009; Johndrow et al., 2019), i.e. projecting
the feature space in a subspace orthogonal to A. However,
it is tricky to account for dependencies on feature interac-
tions. Another way of keeping information while remov-
ing “unfairness” due to the sensitive attribute is to learn a
fair representation of the training dataset, see Zemel et al.
(2013); Louizos et al. (2015); McNamara et al. (2017); Cal-
mon et al. (2017), i.e. to learn a new set of variables Z,
such that they are able to reconstruct X with as few errors
as possible, while at the same time being as independent
as possible of A7. The idea is then that one can use any
ML model on this “cleaned” representation of the original
dataset and this will be fair by design.
Notice, however, that in most situations, when trying to
remove from the data the dependence of A and X, indi-
vidual fairness will be spoiled.
We shall discuss this in
section 3.6.
Counterfactual frameworks (Kusner et al., 2017; Chiappa,
2019), that will be more thoroughly discussed in section 4,
provide a clear way in which this similarity should be
thought of: a male individual is similar to himself in the
counterfactual world where he is a woman. Notice that
this is crucially diﬀerent from fairness through unaware-
ness approach: a male individual transported in the coun-
terfactual world where he is a woman will have diﬀer-
6E.g. gender may be correlated with income, but, depending on
the problem, one may decide that the use of income, even if correlated
with gender, is not a source of unfair discrimination. In section 4
we will dwell more on this issue, namely that there may be some
information correlated to the sensitive attribute but still considered
“fair”.
7In the sense that it is hard to reconstruct A from Z.
ences in other features as well, and such diﬀerences are
precisely due to the causal structure among the variables
(very roughly speaking, this is the “causal way” to account
for correlations). As an example, in the now popular 1973
Berkeley admission case (Bickel et al., 1975) where there
is diﬀerent admission rate between men and women, being
a woman “causes” the choice of higher demanding depart-
ments, thus impacting the admission rate (in causal theory
jargon, department choice is said to be a mediator from
gender to admission).
In this case, a male transported
in the counterfactual world where he is a woman would
have himself chosen higher demanding departments, thus
this feature would change as well. In this case, a simple
gender-blind model would not guarantee individual fair-
ness. Indeed attribute ﬂipping in general does not produce
valid counterfactuals. More details on this will be given in
section 4. We refer to Barocas et al. (2019) for a thorough
discussion on the Berkeley admission case from the point
of view of causal reasoning.
Turning to the general similarity-based deﬁnition (1), some
work has been done to address the problem of ﬁnding a
suitable similarity metrics in feature space. E.g. Dwork
et al. (2012) and Jung et al. (2019); Ilvento (2019) intro-
duce the possibility to learn a issue-speciﬁc distance from
data and from the contributions of domain experts. In-
deed, the simple idea of using standard similarities, e.g.
related to the euclidean distance on feature space, does
not take into account the trivial fact that some feature are
more important than other in determining the relationship
of an individual to speciﬁc target. Namely, for two appli-
cants for a loan, the diﬀerence in income is much more
important than the diﬀerence in, say, age, or even profes-
sion. Thus, judging what does it mean to be similar with
respect to a speciﬁc task is not that simple, and is in some
sense connected also to the ground truth target variable
Y .
Indeed, Berk et al. (2017) propose a notion of individual
fairness as a penalty function – to be added to the risk
minimization ﬁtting – which shifts the concept of simi-
larity from the feature space to the target space: namely
two individuals are deemed similar if they have a similar
value of the target variable. Thus, this notion of individ-
ual fairness relies directly on the target attribute to deﬁne
a task-speciﬁc distance. Of course, this deﬁnition is prone
to biases possibly present in the target.
Broadly speaking, the Lipschitz notion (1) is sometimes
referred to as Fairness Through Awareness (FTA), as op-
posed to Fairness Through Unawareness: in fact, even if
they share the same principle of treating equally similar
individuals, FTA is generally meant to use similarity met-
rics that are problem and target speciﬁc, i.e. that derive
from an “awareness” of the possible impact, while FTU is a
simple recipe that does not depend on the actual scenario.
5

3. Group fairness
Group fairness criteria are typically expressed as condi-
tional independence statements among the relevant vari-
ables in the problem.
There are three main broad notions of observational group
fairness, independence, separation, suﬃciency (Barocas
et al., 2019). Independence is strictly linked to what is
known as Demographic Parity or Statistical Parity, separa-
tion is related to Equality of Odds and its relaxed versions,
while suﬃciency is connected to the concept of calibration
and Predictive Parity.
There is a crucial aspect that discriminates independence
criteria from the others: independence criteria rely only
on the distribution of features and decisions, namely on
(A, X, ˆY ), while separation and suﬃciency criteria make
use of the target variable Y as well. This is an important
thing to have in mind when trying to ﬁnd your way in the
zoo of fairness criteria in a speciﬁc case study: when us-
ing separation-based or suﬃciency-based criteria one must
be careful to check whether the target variable Y is itself
prone to sources of bias and unfairness.
In the example of credit lending, if Y
represents the
decision of a loan oﬃcer, than separation-based and
suﬃciency-based criteria must be used with particular
care, since Y can be itself biased against some groups of
people.
Actually, even if Y stands for the repayment (or lack of it)
of the loan, a form of selection bias is very likely at work:
we have that information only on applicants that received
the loan in the ﬁrst place, and these are almost surely not
representative of the whole population of applicants.
3.1. Independence
The criterion of independence (Barocas et al., 2019) states
that the decisions should be independent of any sensitive
attribute:
ˆY ⊥⊥A.
(4)
This can be also expressed as follows:
P( ˆY = 1 | A = a) = P( ˆY = 1 | A = b),
∀a, b ∈A, (5)
i.e. the ratio of loans granted to men should be equal to
the ratio of loans granted to women.
The ratio of favorite outcomes is sometimes known as pos-
itive prediction ratio (ppr), thus independence is equiva-
lent to requiring the same positive prediction ratio across
groups identiﬁed by the sensitive features. This form of in-
dependence is usually known as Demographic Parity (DP),
statistical parity, or sometimes as group fairness.8
8Demographic parity is a very common concept in the literature
on fairness in ML. Kamiran and Calders (2009) provides one of the
ﬁrst mathematical formulation (even if without actually using the
Figure 1: example of demographic parity in gender in credit lending toy
model.
Figure 1 shows a very simple visualization of a model
reaching demographic parity among men and women.
If some group has a signiﬁcantly lower positive prediction
ratio with respect to others, we say that demographic par-
ity is not satisﬁed by the model decisions ˆY . In order to
have a single number summarizing the amount of dispar-
ity, it is common to use either maximum possible diﬀer-
ence or the minimum possible ratio of positive prediction
ratios: a diﬀerence close to 0 or a ratio close to 1 indicate
a decision system fair with respect to A in the sense of
DP. Typically, some tolerance is considered by employing
a threshold below (or above) which the decisions are still
considered acceptable.
3.1.1. Subtleties of demographic parity
The meaning of DP is intuitive only to a superﬁcial anal-
ysis. For example, one may at ﬁrst think that removing
the sensitive attribute from the decision making process is
enough to guarantee independence and thus demographic
parity.
But, in general, this is not the case. Take the credit lend-
ing example and assume that, for whatever reason, women
tend to actually pay back their loans with higher proba-
bility than men.
If this is the case, it is reasonable to
assume that a rating variable that we call R will be higher
for women than for men. In this scenario, the sensitive at-
tribute gender (A) is correlated with rating. If the model
uses only rating (but not gender) to compute its decision,
there will be a higher rate of loans granted to women than
to men, resulting in a demographic disparity among these
groups. In this case, if you want to reach DP, the model
needs to favor men over women, granting loans to men
with a lower rating threshold with respect to women.
now common nomenclature). We refer to Barocas et al. (2019) and
Chouldechova (2017) for a general account.
6

Figure 2: example of a subtlety of demographic parity: in order to reach
demographic parity between men and women and still using rating as fun-
damental feature, one must use a diﬀerent threshold between the groups,
thus manifestly treating diﬀerently men and women.
Thus, because of interdependence of X and A, not only it
is not enough to remove the sensitive attribute from the
decision making process, but if you want to have DP you
need, in general, to treat diﬀerent groups in diﬀerent ways,
precisely in order to compensate for the (unwanted) eﬀect
of this dependence. This is somehow the opposite of an
intuitive notion of fairness!
Figure 2 displays this example: to reach equal ppr, one
must use a diﬀerent rating threshold for each group.
This, in turn, reveals another subtlety: even if your dataset
and your setting does not apparently contain any sensitive
features, discrimination could lurk in via correlations to
sensitive features that you are not even collecting.
We want to stress here what we think is a crucial aspect:
another possibility to reach DP would be to use neither
gender nor rating in making decisions, i.e. trying to re-
move all gender information from the dataset. Notice that
this could be problematic for the accuracy of your deci-
sions, since it’s plausible that, by removing all variables
correlated to A, information useful to estimate the target
is lost as well.
This is called suppression, and was dis-
cussed already in section 2, together with the concept of
fair representation, by which one tries to remove all sen-
sitive information from the dataset while keeping as much
useful information as possible.
Another possible ﬂaw is the following: if it is true that
women repay their loans with higher probability, is it really
fair to have demographic parity between men and women?
Why the bank should agree to grant loans with the same
rate to groups that actually pays back with diﬀerent prob-
abilities? Should we stick to actual repayment rates or we
should ask why these probabilities are diﬀerent and if this
is possibly due to gender discrimination in the ﬁrst place?
We try to summarize in a non-exhaustive list, a set of sce-
narios in which it might be reasonable to take into account
demographic parity, among other metrics:
• when you want to actively enforce some form of equal-
ity between groups, irrespective of all other informa-
tion. Indeed there are some characteristics that are
widely recognized to be independent in principle of
sensitive attributes, e.g. intelligence and talent, and
there may be the need to enforce an independence in
problems where the decisions is fundamentally linked
to those characteristics; more in general, there may be
reasons to consider unfair any relation among A and
Y , even if the data (objective data as well) is telling
diﬀerently;
• (somehow similar to the previous) when you deem
that in your speciﬁc problem there are hidden his-
torical biases that impact in a complex way the entire
dataset;
• when you cannot trust the objectivity of the target
variable Y , then demographic parity still makes sense,
while other metrics don’t (e.g. separation).
3.1.2. Conditional demographic parity
Another version of the independence criteria is the one of
Conditional Demographic Parity (CDP), ﬁrst introduced
in Kamiran et al. (2013). In the example given above, we
may think that a fairer thing to do, with respect to full
independence, is to require independence of the decision
on gender only for men and women with the same level of
rating. In other words, if a man and a woman have both
a certain level of rating, we want them to have the same
chance of getting the loan. This goes somewhat in the di-
rection of being an individual form of fairness requirement,
since parity is assessed into smaller groups with respect to
the entire sample.
Formally, this results in requiring ˆY independent of A
given R,
ˆY ⊥⊥A | R,
(6)
or, in other terms:
P( ˆY = 1 | A = a, R = r) = P( ˆY = 1 | A = b, R = r),
∀a, b ∈A, ∀r.
(7)
This seems a very reasonable requirement in many real-
life scenarios. For example, if you think of a recruitment
setting where you don’t want to bias women against men,
but still you want to recruit the most skilled candidates,
you may require your decision to be independent of gender
but conditional on a score based on the curriculum and
past work experiences: among people with an “equivalent”
set of skills, you want to recruit men and women with the
same rate.
In other words, the only disparities that you are willing
7

to accept between male and female candidates are those
justiﬁed by curriculum and experience.
Unfortunately, also in this case one must be really careful,
because the variable that you are conditioning on might
itself be a source of unfair discrimination. For example, it
might well be that rating is higher for women not because
they actually pay back their debts more likely than men,
but because the rating system is biased against men. And
this may be due to self-fulﬁlling prophecy: if men have
lower rating they may receive loans with higher interest
rates, and thus have a higher probability of not paying
them back, in a self-reinforcing loop.
Moreover, it may be not be so straightforward to select the
variables to condition on: why condition on rating and not,
e.g., on level of income, or profession, etc...?
Finally, in line with what outlined above for demographic
parity, one may argue that women’s curricula and work ex-
periences tend to be on average diﬀerent than those of male
candidates for historical reasons and for a long-lasting (and
die-hard) man-centered society.
This may suggest that
plain demographic parity could be more appropriately en-
forced in this case to reach a “true” equality.
Incidentally, notice that pushing to the extreme the notion
of conditional demographic parity, i.e. conditioning on all
the (non-sensitive) variables, one has
ˆY ⊥⊥A | X,
(8)
i.e.
P( ˆY = 1 | A = a, X = x) = P( ˆY = 1 | A = b, X = x),
∀a, b ∈A, ∀x ∈X;
(9)
meaning that a male and a female with the same value for
all the other features must be given the same outcome.
This criterion is reachable by a gender-blind model, thus
is strictly connected to the notion of FTU. We can indeed
notice – but leave the details to section 3.6 – that condi-
tioning is equivalent to restrict the groups of people among
which we require parity, thus is a way to go in the direction
of obtaining an individual criterion.
3.2. Separation
Independence and conditional independence do not make
use of the true target Y . What if, instead of conditioning
over rating R we condition on the target Y ?
This is equivalent to requiring the independence of the
decision ˆY and gender A separately for individuals that
actually repay their debt and for individuals that don’t.
Namely, among people that repay their debt (or don’t),
we want to have the same rate of loan granting for men
and women.
This concept has been called separation (Barocas et al.,
2019):
ˆY ⊥⊥A | Y.
(10)
In other terms
P( ˆY = 1 | A = a, Y = y) = P( ˆY = 1 | A = b, Y = y),
∀a, b ∈A, y ∈{0, 1}.
(11)
Equivalently, disparities in groups with diﬀerent values of
A (male and female) should be completely justiﬁed by the
value of Y (repayment or not).
As in the conditional independence case, this seems a
very reasonable fairness requirement, provided that you can
completely trust the target variable. Namely, one should be
extremely careful to check whether the target Y is not it-
self a source of bias.
For example, if Y instead of reﬂecting true repayment was
the outcome of loan oﬃcers’ decision on whether to grant
the loans, it could incorporate bias, thus we it would be
risky to assess fairness with direct comparisons with Y .
Moreover, as we said above, even in the objective case
where Y is the actual repayment, a form of selection bias
would likely distort the rate of repayment.
We can express separation in terms of what are known
in statistics as type I and type II errors.
Indeed, it is
easy to see that the two conditions in equation (11) (one
for y = 1 and one for y = 0) are equivalent to requiring
that the model has the same false positive rate and false
negative rate across groups identiﬁed via A. False positives
and false negatives are precisely type I and type II errors,
respectively. Namely, individuals that are granted loans
but are not able to repay, and individuals that are able to
repay but are not granted loans.
This is known as Equality of Odds (Hardt et al., 2016),
and is thus the requirement of having the same type I and
type II error rates across relevant groups, as displayed in
Figure 3.
There are two relaxed version of this criterion:
• Predictive Equality:
equality of false positive rate
across groups,
P( ˆY = 1 | A = a, Y = 0) = P( ˆY = 1 | A = b, Y = 0),
∀a, b ∈A,
• Equality of Opportunity:
equality of false negative
rate across groups,
P( ˆY = 0 | A = a, Y = 1) = P( ˆY = 0 | A = b, Y = 1),
a, b ∈A.
While demographic parity, and independence in general,
focuses on equality in terms of acceptance rate (loan grat-
ing rate), Equality of Odds, and separation in general,
8

Figure 3: Example of Equality of Odds between men and women: false
negative and false positve rates must be equal across groups.
focuses on equality in terms of error rate: the model is fair
if it is as eﬃcient in one group as it is in the other.
As remarked above, in order to assess fairness in terms of
model errors, one needs to trust the ground truth Y .
The diﬀerence between Predictive Equality and Equality
of Opportunity is the perspective from which equality is
required: Predictive Equality takes the perspective of peo-
ple that won’t repay the loan, while Equality of Opportu-
nity takes the one of people that will repay. Depending on
the problem at hand, one may consider either of these two
perspectives as more important. For example, Predictive
Equality may be considered when we want to minimize the
risk of innocent people from being erroneously arrested:
in this case it may be reasonable to focus on the parity
of among innocents. Both in the credit lending and job
listing examples, on the other hand, it may be reasonable
to focus on Equality of Opportunity, i.e.
on the parity
among people that are indeed deserving.
Here follows a non exhaustive set of situations in which
separation criteria may be suitable:
• when your target variable Y is an objective ground
truth;
• when you are willing to make discrimination as long
as they are justiﬁed by actual trustable data;
• when you do not want to actively enforce an “ideal”
form of equality, and you want to be as equal as pos-
sible given the data.
We can summarize by saying that that separation, being
a concept of parity given the ground truth outcome, is
a notion that takes the point of view of people that are
subject to the model decisions, rather than that of the
decision maker. In the next subsection, instead, we shall
take into account the other side of the coin, i.e. parity
given the model decision.
3.3. Suﬃciency
Suﬃciency (Barocas et al., 2019) takes the perspective of
people that are given the same model decision, and re-
quires parity among them irrespective of sensitive features.
While separation deals with error rates in terms of frac-
tion of errors over the ground truth, e.g. the number of
individuals whose loan request is denied among those who
would have repaid, suﬃciency takes into account the num-
ber of individuals who won’t repay among those who are
given the loan.
Mathematically speaking, this is the same distinction you
have between recall (or true positive rate) and precision,
i.e. P( ˆY = 1 | Y = 1) and P(Y = 1 | ˆY = 1), respectively.
A fairness criterion that focuses on this type of error rate
is called Predictive Parity (Chouldechova, 2017), also re-
ferred to as outcome test (Verma and Rubin, 2018; Mitchell
et al., 2018):
P(Y = 1 | A = a, ˆY = 1) = P(Y = 1 | A = b, ˆY = 1),
∀a, b ∈A,
(12)
i.e. the model should have the same precision across sen-
sitive groups. If we require condition (12) to hold for the
case Y = 0 as well, then we get the following conditional
independence statement:
Y ⊥⊥A | ˆY ,
which is referred to as suﬃciency (Barocas et al., 2019).
Predictive Parity, and its more general form of suﬃciency,
focuses on error parity among people who are given the
same decision.
In this respect, Predictive Parity takes
the perspective of the decision maker, since they group
people with respect to the decisions rather than the true
outcomes. Taking the credit lending example, the deci-
sion maker is indeed more in control of suﬃciency rather
than separation, since parity given decision is something
directly accessible, while parity given truth is known only
in retrospect. Moreover, as we have discussed above, the
group of people who are given the loan (ˆY = 1) is less
prone to selection bias than the group of people who repay
the loan (Y = 1): indeed we can only have the information
of repayment for the ˆY = 1 group, but we know nothing
about all the others (ˆY = 0).
As you may notice, going along a similar reasoning, one
can deﬁne other group metrics, such as Equality of Accu-
racy across groups: P( ˆY = Y | A = a) = P( ˆY = Y | A =
b), for all a, b ∈A, i.e.
focusing on over unconditional
errors, and others (Verma and Rubin, 2018).
Zafar et al. (2017) introduce the notion of disparate mis-
treatment to refer to all group fairness criteria relying on
9

disparity of errors, thus, in general, all group metrics deal-
ing with comparisons among decisions ˆY and true out-
comes Y .
3.4. Group fairness on scores
In most cases, even in classiﬁcation setting, the actual out-
put of a model is not a binary value, but rather a score
S ∈R, estimating the probability, for each observation, to
have the target equal to the favorable outcome (usually la-
belled 1). Then, the ﬁnal decision is made by the following
t-threshold rule:
ˆY =
(
1
S ≥t,
0
S < t.
(13)
Most of the things we have outlined for group fairness met-
rics regarding ˆY can be formulated for the joint distribu-
tion (A, Y, S) as well:
independence
S ⊥⊥A,
separation
S ⊥⊥A | Y,
suﬃciency
Y ⊥⊥A | S.
(14)
These formulations provide stronger constraints on the
model with respect to the analogous with ˆY , e.g. Jiang
et al. (2020); Oneto and Chiappa (2020) call the condition
S ⊥⊥A strong demographic parity.
For instance, all three criteria in their form (5), (11), (12),
i.e.
with constraints on the joint distribution (A, Y, ˆY )
can eﬀectively be satisﬁed by deﬁning group dependent
thresholds t on black-box model outcomes S (a technique
that goes under the name of postprocessing (Barocas et al.,
2019)), while this is not the case for (14).
Instead of requiring conditions on the full distribution of
S as in (14), something analogous to the “binary versions”
of group fairness criteria have been deﬁned simply by re-
quiring parity of the average score (Kleinberg et al., 2016).
Balance of the Negative Class, deﬁned as
E(S | Y = 0, A = a) = E(S | Y = 0, A = b),
∀a, b ∈A,
(15)
corresponds to Predictive Equality, while Balance of the
Positive Class (same as (15) with Y = 1) corresponds
to Equality of Opportunity.
Notice that these two last
deﬁnitions fall into the one given in subsection 3.2 when
S = ˆY . Requiring both balances is of course equivalent to
requiring EO.
AUC parity, namely the equality of the area under the
ROC for diﬀerent groups identiﬁed by A, can be seen as
the analogous of the equality of accuracy.
Finally, notice that, the score formulation of suﬃciency is
connected to the concept of calibration. Calibration holds
when
P(Y = 1 | S = s) = s,
(16)
i.e. if the model assigns a score s to 100 people then, on
average, 100 × s of them will actually be positive. Writing
down the condition for suﬃciency in score:
P(Y = 1 | S = s, A = a) = P(Y = 1 | S = s, A = b),
∀a, b ∈A, ∀s,
(17)
it is clearly related to what can be called Calibration within
Groups (Kleinberg et al., 2016):
P(Y = 1 | A = a, S = s) = s
(18)
– actually a consequence of it. This condition is in general
not so hard to achieve, and it is often satisﬁed “for free” by
most models. See Barocas et al. (2019) for more details.
3.5. Incompatibility statements
It is interesting to analyse the relationship among diﬀerent
criteria of fairness.
In the next subsection 3.6 we shall
discuss in detail the connections between individual and
group notions, while here we focus on diﬀerences among
various group criteria. We have already seen that each of
them highlights one speciﬁc aspect of an overall idea of
fairness, and we may wonder what happens if we require
to satisfy multiple of them at once. The short answer is
that, in general, it is not possible.
Most incompatibility results have been provided by
Chouldechova (2017); Kleinberg et al. (2016); Barocas
et al. (2019).
We take the following three results from
the formulation of Barocas et al. (2019).
1. if Y is binary, Y ̸⊥⊥S and Y ̸⊥⊥A, then separation
and independence are incompatible.
In other words, to achieve both separation and inde-
pendence, the only possibility is that either the model
is completely useless (Y ⊥⊥S), or the outcome is inde-
pendent of the sensitive attribute (Y ⊥⊥A), which im-
plies an equal base rate for diﬀerent sensitive groups.
Namely, if there is an imbalance in groups identiﬁed
by A, then you cannot have both EO and DP holding.
2. Analogously: if Y ̸⊥⊥A, then suﬃciency and indepen-
dence cannot hold simultaneously.
Thus, if there is an imbalance in groups identiﬁed by
A, then you cannot impose both suﬃciency and inde-
pendence.
3. Finally: if Y ̸⊥⊥A and the distribution (A, S, Y ) is
strictly positive, then separation and suﬃciency are
incompatible.
Meaning that separation and suﬃciency can both hold
either when there is an imbalance in sensitive groups,
or when, given a certain value of A and given the
score S, the probability of having Y = 0 or Y = 1
(for binary targets) are both non-null, i.e. when the
score never exactly resolves the true target.
10

Kleinberg et al. (2016) prove that Balance of Positive
Class, Balance of Negative Class and Calibration within
Groups can hold together only if either there is no imbal-
ance in groups identiﬁed by A or if each individual is given
a perfect prediction (i.e. S = 0, 1 everywhere).
Chouldechova (2017) focuses on the COMPAS recidivism
case, now popular in the fairness literature. Indeed, the
debate on this case is a perfect example to highlight the
fact that there are diﬀerent and non-compatible notions of
fairness, and that this may have concrete consequences on
people. While we refer to Washington (2018) and Choulde-
chova (2017) for a thorough discussion of the COMPAS
case, we here just point out that in the debate there were
two parties, one stating that the model predicting recidi-
vism was fair since it satisﬁed Predictive Parity by eth-
nicity, while the other claiming it was unfair since it had
diﬀerent false positive and false negative rates for black
and white individuals. Chouldechova (2017) showed that,
if Y ̸⊥⊥A, i.e. if the true recidivism rate is diﬀerent for
black and white people, then Predictive Parity and Equal-
ity of Odds cannot both hold, thus implying that a reﬂec-
tion on which of the two (in general of the many) notions
is more important to be pursued in that speciﬁc case must
be carefully considered.
Summarizing, apart from trivial or peculiar scenarios, the
three families of group criteria are not mutually compati-
ble.
3.6. Group vs individual
The most common issue with group fairness deﬁnitions
is the following: since group fairness requires to satisfy
conditions only on average among groups, it leaves room
to bias discrimination inside the groups. As we argued in
the example of section 3 referring to ﬁgure 2, one way of
reaching DP is to use a diﬀerent rating threshold for men
and women: this means that there will be a certain range
of ratings for which men will receive the loan, and women
won’t. More formally, conditionally on rating there is no
independence among A and ˆY . In general, to reach group
fairness one may “ﬁne tune” the interdependence of A and
X to reach parity on average, but eﬀectively producing
diﬀerences in subgroups of A.
Notice that this is precisely what individual fairness is
about. In the example above, a men and a woman that
have the same rating may be treated diﬀerently, thus vio-
lating the individual notion of fairness.
As already discussed in section 3, of course this is only one
possibility: one may as well reach DP by not using neither
gender nor rating, and grant loan on the basis of other
information, provided it is independent of A. In this case,
there will be no group discrimination, but there won’t be
any subgroup discrimination as well.
However, we can say that, if we want to reach DP by
using as much information of Y as possible contained in
(X, A), i.e. minimizing the risk EL(f(X, A), Y ), then it
is unavoidable to have some form of disparate treatment
among people in diﬀerent groups with respect to A when-
ever X ̸⊥⊥A.
This has been thoroughly discussed by
Dwork et al. (2012), where they call fair aﬃrmative ac-
tion the process of requiring DP while trying to keep as
low as possible the amount of disparate treatment between
people having similar X.
To clarify the general picture, we can put the diﬀerent
notions of (observational) fairness in a plane with two
qualitative dimensions (see ﬁgure 4): 1. to what extent
a model is fair at the individual level, 2. how much in-
formation of A is retained in making decisions. The ﬁrst
dimension represents to what extent two individuals with
similar overall features X are given similar decisions: the
maximum value is reached by models blind on A (FTU).
These are the models that are also using all information
in X, irrespective of the interdependence with A, thus
FTU-compliant models will use all information contained
in e
X apart from the information that is contained in A
only. The minimum value in this dimension is reached by
models that satisfy DP. Models using suppression meth-
ods, being blind to both A and other features with high
correlation with A, are individually fair in the sense of
preventing disparate treatment. In so doing, they can ex-
ploit more or less information of A with respect to general
DP-compliant models depending on how many correlated
variables are discarded. However, the price to pay for dis-
carding variables is in terms of errors in approximating
Y , which is not highlighted in this plot. Notice that, of
course, full suppression – i.e. removing all variables de-
pendent on A – trivially satisﬁes the condition ˆY ⊥⊥A,
i.e. it is DP-compliant as well. In other terms, one can
have a DP-compliant model that is individual as well. In
ﬁgure 4, we label with DP a general model that tries to
maximize performance while satisfying a DP constraint,
without any further consideration.
Models satisfying CDP are somewhat in-between, of course
depending on the speciﬁc variables considered for condi-
tioning. They guarantee less disparate treatment than un-
conditional DP, and they use more information of A by
controlling for other variables possibly dependent on A.
Notice that approaches such as fair representation (see
section 2), where you try to remove all information of
A from X to get new variables Z which are as close as
possible to being independent of A, produce decision sys-
tems ˆY = f(Z) that are not, in general, individually fair.
This is due to the simple fact that, precisely to remove the
interdependence of A and X while keeping as much in-
formation of X as possible, two individuals with same X
and diﬀerent A will be mapped in two distinct points on
Z, thus having, in general, diﬀerent outcomes. Referring
again to the credit lending example, suppose that we have
R = g(A)+U, with g a complicated function encoding the
interdependence of rating and gender, and U some other
11

DP
group
individual
information of A used
FTU
CDP
suppression
ˆY ⊥⊥A
Figure 4: Landscape of observational fairness criteria with respect to the
group-vs-individual dimension and the amount of information of A used
(via X).
factor independent of A representing other information in
R “orthogonal” to A. In this setting, the variable Z that
we are looking for is precisely U. Notice that U is indeed
independent of A, thus any decision system ˆY = f(U) sat-
isﬁes DP, but given two individuals with R = r nothing
prevents them from having diﬀerent U. In other terms,
you need to have some amount of disparate treatment to
guarantee DP and employ as much information as possible
to estimate Y .
Figure 5 shows a qualitative representation of observa-
tional metrics with respect to the amount of information
of A (through X) that is used by the model, and the pre-
dictive performance. Notice that DP can be reached in
many ways: e.g. a constant score model, namely a model
accepting with the same chance all the individuals irre-
spective of any feature, is DP-compliant (incidentally, it
is also individual), a model in which all the variables de-
pendent on A have been removed (a full suppression), or
a model where DP is reached while trying to maximize
performance (e.g. through fair representations). All these
ways diﬀer in terms of the overall performance of the DP-
compliant model.
FTU-compliant models, on the other hand, by employing
all information in X will be, in general, more eﬃcient in
terms of model performance.9
Incidentally, notice that this discussion is to be taken at a
qualitative level, one can come up with scenarios in which,
e.g., models satisfying DP have higher performances than
models FTU-compliant (think, e.g., of a situation in which
Y ⊥⊥A and X ̸⊥⊥A).
3.7. Multiple sensitive features
Even if a detailed discussion of the problem of multiple
sensitive features is out of the scope of this manuscript,
we shall nevertheless give a brief overview.
9Of course it is understood that the model f in ˆY = f(X) is
trained to maximize performance.
DP
model performance
information of A used
FTU
CDP
suppression
constant score
ˆY ⊥⊥A
Figure 5: Landscape of observational fairness criteria with respect to the
model performance dimension and the amount of information of A used
(via X).
Generally speaking, all the deﬁnitions and results we gave
in previous sections are subject to the fact that the sensi-
tive feature A is represented by a single categorical vari-
able. If, for a given problem, we identify more than one
characteristic that we need to take into account as sensi-
tive or protected – say (A1, . . . , Al) – we can easily assess
fairness on each of them separately. This approach, that
Yang et al. (2020) call independent group fairness, unfortu-
nately is in general not enough: even if fairness is achieved
(in whatever sense) separately on each sensitive variable
Ai, it may happen that some subgroups given by the inter-
section of two or more Ai’s undergo unfair discrimination
with respect to the general population. This is sometimes
referred to as intersectional bias, or, more speciﬁcally, fair-
ness gerrymandering (Kearns et al., 2018).
To prevent bias from occurring in all the possible subgroups
identiﬁed by all Ai’s one can simply identify a new feature
A = (A1, . . . , Al), whose values are the collection of values
on all the sensitive attributes, and require fairness con-
straints on A. Yang et al. (2020) call this intersectional
group fairness.
This last “trick” indeed solves the problem of intersec-
tional bias, at least theoretically. Still, issues remain at a
computational and practical level, whose two main reasons
are:
• the exponential increase of the number of subgroups
when adding sensitive features,
• the fact that, with ﬁnite samples, many of the sub-
groups will be empty or with very few observations.
These two aspects imply that even assessing (group) fair-
ness with respect to multiple sensitive attributes may be
practically unfeasible in many cases. This is a huge prob-
lem, that most of the literature on fairness in ML does
not address. We refer to Yang et al. (2020); Kearns et al.
(2018); Buolamwini and Gebru (2018) for a more detailed
discussion.
12

4. Causality-based criteria
Another important distinction of fairness criteria is the
one between observational and causality-based criteria.
As we have seen, observational criteria rely only on ob-
served realizations of the distribution of data and predic-
tions. In fact, they focus on enforcing equal metrics (ac-
ceptance rate, error rate, etc. . . ) for diﬀerent groups of
people. In this respect, they don’t make further assump-
tions on the mechanism generating the data and suggest
to assess fairness through statistical computation on ob-
served data.
Causality-based criteria, on the other hand, try to employ
domain and expert knowledge in order to come up with a
casual structure of the problem, through which it becomes
possible to answer questions like “what would have been
the decision if that individual had a diﬀerent gender?”.
While counterfactual questions like this seem in general
closer to what one may intuitively think of as “fairness as-
sessment”, the observational framework is on the one hand
easier to assess and constrain on, and on the other more ro-
bust, since counterfactual criteria are subject to strong as-
sumptions about the data and the underlying mechanism
generating them, some of which are not even falsiﬁable.
As we argued above (section 2), answering to counterfac-
tual questions is very diﬀerent from taking the feature vec-
tor of, e.g., a male individual and just ﬂip the gender label
and see the consequences in the outcome. The diﬀerence
lies precisely in the causal chain of “events” that this ﬂip
would trigger. If there are some features related, e.g., to
the length of the hair, or the height, then it is pretty ob-
vious that the ﬂip of gender should come together with a
change in these two variables as well. And this may be the
case for other, less obvious but more important, variables.
This also suggests why counterfactual statements involve
causality relationships among the variables.
In general,
to answer counterfactual questions, one needs to know
the causal links underlying the problem. This requires a
certain number of assumptions, usually driven by domain
knowledge.
However, as major drawback, once given the casual struc-
ture there are many counterfactual models compatible
with that structure (actually inﬁnite), and the choice of
one of them is not falsiﬁable in any way. Indeed, causality-
based criteria can be formulated at least in two diﬀerent
levels: at the level of interventions and at the level of
counterfactuals.
Fairness at the level of interventions can be formally ex-
pressed as follows (Kilbertus et al., 2017):
P( ˆY = 1 | do(A = a), X = x) =
P( ˆY = 1 | do(A = b), X = x),
∀a, b ∈A, x ∈X.
(19)
In words, if you take a random individual with X = x
and force it to be, e.g., female (do(A = a)), you want to
give him the same chance of acceptance as for a random
individual with X = x forced to be male (do(A = b)). We
refer to it as Intervention Fairness.
The same requirement can be set at the counterfac-
tual level,
and is known as Counterfactual Fairness
(CFF) (Kusner et al., 2017):
P( ˆYA←a = 1 | A = a, X = x) =
P( ˆYA←b = 1 | A = a, X = x),
∀a, b ∈A,
(20)
which, in words, reads: if you take a random individual
with A = a and X = x and the same individual if she had
A = b, you want to give them the same chance of being
accepted.
The diﬀerence between the two levels is subtle but impor-
tant: roughly speaking, when talking about interventions
one is considering the average value over exogenous fac-
tors U, while counterfactuals consider only the values of U
that are compatible with the factual observation (namely,
the distribution P(U | A = a, X = x)). In other words,
counterfactuals consider only events that take into account
actual observed value of A (and X as well) (see Supple-
mentary Material of Kusner et al. (2017)).
Equations (19), (20) make use of Pearl’s do-calculus, where
do(A = a), called intervention, consists in modifying the
causal structure of the problem by exogenously setting
A = a, thus removing any causal paths impacting on A
– the theoretical equivalent of randomized experiments.
The notation ˆYA←b, on the other hand, stands for the
three steps of counterfactual calculus, abduction, action,
prediction:
• abduction is where you account for observed values
and compute the distribution of U | (A = a, X = x);
• action corresponds to implementing the intervention
do(A = b);
• prediction consists in using the new causal structure
and the exogenous conditional distribution P(U | A =
a, X = x) to compute the posterior of ˆY .
We refer to Pearl and Mackenzie (2018) for a general re-
view on causal inference and do-calculus, and again to
Barocas et al. (2019) for a thorough discussion of causality
in the context of fairness.
As in the observational setting, causality-based criteria
have a group and an individual version: equations (19),
(20) correspond to the individual form, but nothing pre-
vents to take the version unconditional on X, i.e. holding
on average on all the individuals, or even to condition on
only some subset of X, as in CDP.
13

4.1. Group vs. individual fairness in causality-based set-
ting
We call Expectation Intervention Fairness the condition:
P( ˆY = 1 | do(A = a)) =
P( ˆY = 1 | do(A = b))
∀a, b ∈A,
(21)
i.e. requiring that, on average, an individual taken at ran-
dom from the whole population and forced to be woman
(do(A = a)) should have the same chance of being accepted
as a random individual forced to be man. Analogously, we
can deﬁne Expectation Counterfactual Fairness (ECFF)
as:
P( ˆYA←a = 1 | A = a) =
P( ˆYA←b = 1 | A = a)
∀a, b ∈A,
(22)
which states that, on average, the acceptance rate for a
random woman (A = a) should be the same as the one
given to a random woman forced to be a man. Similar
deﬁnitions can be given conditioning on partial informa-
tion R.
Summarizing, we can visualize CFF as the following pro-
cess: the conditioning on some (A = a, R = r) represents
the group of people that we take into account (a single
individual for R = X), and any consideration we do on
them must hold on average over that group; given that
group, we force a ﬂipping of the sensitive attribute from
A = a to A = b (and here we are going in a new, non-
observable, distribution), and this will trigger a cascade of
causal consequences on the other features X (namely, on
the descendants of A in the causal graph). Then, we com-
pare the model outcomes averaged on the observed group
and on the counterfactual group.
Intervention Fairness is slightly diﬀerent: you take again
all the individuals compatible with the conditioning –
which this time is R = r without ﬁxing the sensitive A
– then force them ﬁrst to have A = a, and then to have
A = b (with all the causal consequences of these interven-
tions), and require them to have, on average, the same
acceptance rate.
This may resonate with the notion of FTU: when ﬂipping
A for an observation you want the outcome of the model
not to change. Indeed Fairness Through Unawareness is a
causality-based notion of fairness, at least in its formula-
tion of not explicitly using A. The very concept of ﬂipping
A is nothing but an intervention. Notice, incidentally, that
the ﬂipping of A without any impact on other variables
corresponds to assuming a causal graph where A has no
descendant, or, better, corresponds to assuming that all
the changes caused by the ﬂipping of A on other variables
are legitimate, i.e. considered fair. Moreover, the fact that
FTU is diﬃcult to be measured without having access to
the model, is due to its nature of being a non-observational
notion, but it requires ﬁctitious data to be assessed – a
dataset with A ﬂipped, i.e. a dataset not sampled from the
real distribution of (A, X, ˆY ). This is similar to the way in
which CFF can be assessed: compare the predictions over
a dataset with the ones on the same datasets but with A
ﬂipped and with all the changes caused by this ﬂipping
(of course, knowledge of the underlying SCM is needed).
This, in turn, reveals the subtle diﬀerence among CDP in
the form ˆY ⊥⊥A | X and FTU: CDP is an observational
notion, i.e. can be measured, in principle, by having ac-
cess to (realizations from) the distribution of (ˆY , A, X),
while FTU does not. We say “in principle” since it’s very
likely that in real-world datasets there will be very few
observations corresponding to X = x (typically one), thus
resulting in a poor estimation of the distribution of ˆY | X.
Causality-based notions are richer than the observational
ones, and permit a further possibility:
to select which
causal paths from A to ˆY are considered legitimate and
which, instead, we want to forbid. In the job listing ex-
ample, we may consider that the type of degree of the
applicants is fundamental for the job position, and thus
crucial to making decisions. But it could well be that the
type of degree is correlated, and even “caused” by the gen-
der of the applicants: women and men may have diﬀerent
attitudes towards choosing the preferred degree. In this
situation, if we require CFF, we would compare a man
with some degree to his “parallel self” in the counterfac-
tual world where he is a woman. In that world, however,
it’s very likely that her degree is going to be diﬀerent.
Thus, requiring CFF means to somehow prevent the deci-
sion maker to employ the degree type for the assessment.
Taking into account situations like this one is not very dif-
ﬁcult. Supplementary Material of Kusner et al. (2017) and
Chiappa (2019) introduce the following deﬁnition, known
as path-speciﬁc Counterfactual Fairness (pCFF):
P( ˆYA←a,XF ←x1 = 1 | A = a, XF = x1, Xc
F = x2) =
P( ˆYA←b,XF ←x1 = 1 | A = a, XF = x1, Xc
F = x2),
∀a, b ∈A, ∀x1, x2,
(23)
where XF correspond to the variables descendants of A
that we consider fair mediators to make decisions (e.g. the
degree type), and Xc
F is its complement with respect to X,
namely X = (XF , Xc
F ). In words, take a man (A = a) with
a speciﬁc degree x1 and other features x2, and force him
to be a woman, letting all the causal consequences of this
ﬂipping to happen with the exception of the degree, kept
ﬁxed at x1, then compare their outcomes.
Incidentally, notice that if we allow XF to contain all the
descendants of A, then we end up with a notion that we
may call direct Counterfactual Fairness (dCFF), namely
the only path that we are concerned of is the direct causal
path from gender A to the decision ˆY . In this case we
don’t allow any causal consequences of the gender ﬂipping
to happen when assessing fairness. This is, again, strictly
14

ECFF
CFF
group
individual
A →ˆY information used
dCFF
CFF
pCFF
conditional
consider only some paths
from A to ˆY to be unfair
Figure 6: Landscape of causality-based fairness criteria.
connected to FTU.
Figure 6 is a schematic and qualitative visualization of
many of the points discussed above:
the dimension of
group vs. individual is “controlled” by how much infor-
mation we condition on, while the dimension of the causal
impact of A on the decision is controlled by the fraction of
paths causally connecting A to ˆY that are considered fair.
Figure 6 is meant to be the causal analogue of ﬁgure 4. Of
course there are many possible cases that, for simplicity,
we have omitted from ﬁgure 6, namely all the Intervention
notions, and the path-speciﬁc notions valid on average in
broader groups.10
If we focus on CFF, it can be reached by training a model
on the space (Σ, UA, Y ) where Σ are the non-descendants
of A, i.e. variables not caused by A, directly or indirectly;
UA denotes the information inside descendants of A that
is not attributable to A. A decision system ˆY = f(Σ, U) is
counterfactually fair (20) by design. This closely reﬂects
what we discussed in section 2 about fair representations:
train a model on a new space “cleaned” from all A infor-
mation. Indeed, in that case we searched for statistical
independence of ˆY on A while here we look for “causal
independence” of ˆY on A. In this speciﬁc sense, counter-
factual fairness can be reached via a form of preprocessing
of the dataset, and is very similar, in spirit, to the concept
of fair representation.
We may then wonder why the fair representation approach,
reaching DP, is a notion of group rather than individual
fairness, while CFF is considered an individual notion: the
point is simply that we employed two diﬀerent concepts of
10For instance, one could think of a measure taking into account
only the A ﬂip, without causal consequences, and valid on average
over X as well (i.e. a group notion), namely
P( ˆYA←a,X←X = 1 | A = a) =
P( ˆYA←b,X←X = 1 | A = a),
∀a, b ∈A.
(24)
individual fairness in the observational and in the coun-
terfactual setting. Namely, in the observational setting we
expressed disparate treatment as the event in which two
individuals with same X but diﬀerent A are given diﬀerent
outcomes. In general, fair representation is not guaranteed
to prevent such scenario. In the counterfactual context,
instead, we consider disparate treatment when an individ-
ual and her “counterfactual self” with A ﬂipped are given
diﬀerent outcomes. CFF is designed precisely to prevent
this. Notice that this concept, translated in the observa-
tional setting, is analogous to requiring a similar treatment
for two individuals similar in Z, not in X. Indeed, if you
take an individual with some (A = a, X = x) and ﬂip her
gender while taking into account the interdependence of X
and A, she will be transformed in the same point Z.
This is an example of the fact that the concept of indi-
vidual fairness is strongly dependent on the concept of
similarity that one decides to consider.
Summarizing, CFF and fair representation are very similar
in the way in which they deal with disparities, namely
they both try to remove all (causal) information of A from
the feature space, and then use the “cleaned” space to
make decisions. In this respect, CFF can be seen as both
a “causal analogue” of a DP-compliant methods, in the
sense that all the way in which A may impact the decision
are forbidden, and an individually fair notion in the sense
that it imposes condition on individual basis.
Notice that, to ﬁnd the UA variables, one needs to assume
a Structural Causal Model (SCM), i.e. besides the causal
graph encoding the “direction” of causal relationships, one
needs a set of equations e
X = F(U) that express the pre-
cise relations among the observed variables e
X and all the
unobserved (exogenous) ones U. One typical assumption
is the so-called Additive Noise Model:
e
Xα = fα(parents( e
Xα)) + Uα,
α = 1, . . . d + 1,
where e
Xα represents the α-th feature among the variables
(X, A).
To fully specify a SCM, the functions fα must
be speciﬁed as well. The problem with assuming a speciﬁc
SCM is that it is not falsiﬁable: given a causal graph, there
is an inﬁnite number of SCMs compatible with that graph
and with the observations.
5. Conclusions
The notion of fairness in decision making has many nu-
ances, that have been reﬂected in the high number of
proposed mathematical and statistical deﬁnitions.
No-
tice, moreover, that this aspect is not limited to ML or
artiﬁcial intelligence: the problem of how to deﬁne and
assess bias discrimination in decision making processes is
present largely independently of “who” is making the de-
cision. The growing attention to this issue in the domain
of automated data-driven decisions can be imputed to the
15

fact that these processes can potentially amplify biases at
scale, and could possibly do that without human oversight.
Even if a lot of work has been done in this respect, still
there is confusion on the interplay among diﬀerent fair-
ness notions and metrics to assess them. In this paper,
we tried to highlight some important aspects about the
relationships between fairness metrics, in particular with
respect to the distinctions individual vs. group and obser-
vational vs. causality-based.
Some have expressed critics and doubts about the possibil-
ity of capturing the complexity of notions such as equity
and fairness with quantitative methods (Green and Hu,
2018).
Even if these doubts are reasonable, we believe
that quantitative research in the domain of fairness no-
tions can provide an important contribution to the more
general issue of bias discrimination, at least in terms of
understanding and comprehension – let alone a boost in
the attention of the scientiﬁc community on it. Thus, we
strongly welcome more research both on the quantitative
aspects, dealing with metrics assessment and algorithmic
mitigation, and on the social and legal aspects, and hope-
fully in the interplay between them.
References
Angwin, J., Larson, J., Mattu, S., Kirchner, L., 2016. Machine bias.
ProPublica, May 23, 139–159.
Barocas, S., Hardt, M., Narayanan, A., 2019. Fairness and Machine
Learning. fairmlbook.org. http://www.fairmlbook.org.
Barocas, S., Selbst, A.D., 2016. Big data’s disparate impact. Calif.
L. Rev. 104, 671.
Berk, R., 2009. The role of race in forecasts of violent crime. Race
and social problems 1, 231.
Berk, R., Heidari, H., Jabbari, S., Joseph, M., Kearns, M., Morgen-
stern, J., Neel, S., Roth, A., 2017. A convex framework for fair
regression. arXiv preprint arXiv:1706.02409 .
Bickel, P.J., Hammel, E.A., O’Connell, J.W., 1975. Sex bias in grad-
uate admissions: Data from berkeley. Science 187, 398–404.
Buolamwini, J., Gebru, T., 2018.
Gender shades:
Intersectional
accuracy disparities in commercial gender classiﬁcation, in: Con-
ference on fairness, accountability and transparency, PMLR. pp.
77–91.
Calmon, F., Wei, D., Vinzamuri, B., Ramamurthy, K.N., Varshney,
K.R., 2017. Optimized pre-processing for discrimination preven-
tion, in: Advances in Neural Information Processing Systems, pp.
3992–4001.
Chiappa, S., 2019.
Path-speciﬁc counterfactual fairness, in: Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence, pp.
7801–7808.
Chouldechova, A., 2017. Fair prediction with disparate impact: A
study of bias in recidivism prediction instruments. Big data 5,
153–163.
Chouldechova, A., Roth, A., 2018. The frontiers of fairness in ma-
chine learning. arXiv preprint arXiv:1810.08810 .
Chouldechova, A., Roth, A., 2020. A snapshot of the frontiers of
fairness in machine learning.
Communications of the ACM 63,
82–89.
Dwork, C., Hardt, M., Pitassi, T., Reingold, O., Zemel, R., 2012.
Fairness through awareness, in: Proceedings of the 3rd innovations
in theoretical computer science conference, pp. 214–226.
Green, B., Hu, L., 2018. The myth in the methodology: Towards a
recontextualization of fairness in machine learning, in: Proceed-
ings of the machine learning: the debates workshop.
Hardt, M., 2014. How big data is unfair. Medium, https://medium.
com/@ mrtz/how-big-data-is-unfair-9aa544d739de .
Hardt, M., Price, E., Srebro, N., 2016. Equality of opportunity in
supervised learning, in: Advances in neural information processing
systems, pp. 3315–3323.
Ilvento, C., 2019.
Metric learning for individual fairness.
arXiv
preprint arXiv:1906.00250 .
Jiang, R., Pacchiano, A., Stepleton, T., Jiang, H., Chiappa, S., 2020.
Wasserstein fair classiﬁcation, in: Uncertainty in Artiﬁcial Intel-
ligence, PMLR. pp. 862–872.
Johndrow, J.E., Lum, K., et al., 2019. An algorithm for removing
sensitive information: application to race-independent recidivism
prediction. The Annals of Applied Statistics 13, 189–220.
Jung, C., Kearns, M., Neel, S., Roth, A., Stapleton, L., Wu, Z.S.,
2019. Eliciting and enforcing subjective individual fairness. arXiv
preprint arXiv:1905.10660 .
Kamiran, F., Calders, T., 2009. Classifying without discriminating,
in: 2009 2nd International Conference on Computer, Control and
Communication, IEEE. pp. 1–6.
Kamiran, F., Calders, T., 2012. Data preprocessing techniques for
classiﬁcation without discrimination. Knowledge and Information
Systems 33, 1–33.
Kamiran, F., ˇZliobait˙e, I., Calders, T., 2013. Quantifying explainable
discrimination and removing illegal discrimination in automated
decision making. Knowledge and information systems 35, 613–644.
Kearns, M., Neel, S., Roth, A., Wu, Z.S., 2018. Preventing fairness
gerrymandering: Auditing and learning for subgroup fairness, in:
International Conference on Machine Learning, PMLR. pp. 2564–
2572.
Kilbertus, N., Carulla, M.R., Parascandolo, G., Hardt, M., Janzing,
D., Sch¨olkopf, B., 2017. Avoiding discrimination through causal
reasoning, in: Advances in Neural Information Processing Sys-
tems, pp. 656–666.
Kleinberg, J., Mullainathan, S., Raghavan, M., 2016.
Inherent
trade-oﬀs in the fair determination of risk scores. arXiv preprint
arXiv:1609.05807 .
Kusner, M.J., Loftus, J., Russell, C., Silva, R., 2017. Counterfactual
fairness, in: Advances in neural information processing systems,
pp. 4066–4076.
Louizos, C., Swersky, K., Li, Y., Welling, M., Zemel, R., 2015. The
variational fair autoencoder. arXiv preprint arXiv:1511.00830 .
McNamara, D., Ong, C.S., Williamson, R.C., 2017. Provably fair
representations. arXiv preprint arXiv:1710.04394 .
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., Galstyan, A.,
2019. A survey on bias and fairness in machine learning. arXiv
preprint arXiv:1908.09635 .
Mitchell, S., Potash, E., Barocas, S., D’Amour, A., Lum, K., 2018.
Prediction-based decisions and fairness: A catalogue of choices,
assumptions, and deﬁnitions. arXiv preprint arXiv:1811.07867 .
Narayanan, A., 2018.
Translation tutorial: 21 fairness deﬁnitions
and their politics, in: Proc. Conf. Fairness Accountability Transp.,
New York, USA, pp. 6–2.
O’neil, C., 2016. Weapons of math destruction: How big data in-
creases inequality and threatens democracy. Crown.
Oneto, L., Chiappa, S., 2020. Fairness in machine learning, in: Re-
cent Trends in Learning From Data. Springer, pp. 155–196.
Pearl, J., Mackenzie, D., 2018. The book of why: the new science of
cause and eﬀect. Basic Books.
Verma, S., Rubin, J., 2018. Fairness deﬁnitions explained, in: 2018
IEEE/ACM International Workshop on Software Fairness (Fair-
Ware), IEEE. pp. 1–7.
Washington, A.L., 2018. How to argue with an algorithm: Lessons
from the compas-propublica debate. Colo. Tech. LJ 17, 131.
Yang, F., Cisse, M., Koyejo, O.O., 2020. Fairness with overlapping
groups; a probabilistic perspective. Advances in Neural Informa-
tion Processing Systems 33.
Zafar, M.B., Valera, I., Gomez Rodriguez, M., Gummadi, K.P., 2017.
Fairness beyond disparate treatment & disparate impact: Learn-
ing classiﬁcation without disparate mistreatment, in: Proceedings
of the 26th international conference on world wide web, pp. 1171–
1180.
16

Zemel, R., Wu, Y., Swersky, K., Pitassi, T., Dwork, C., 2013. Learn-
ing fair representations, in: International Conference on Machine
Learning, pp. 325–333.
17

