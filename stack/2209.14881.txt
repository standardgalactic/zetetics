Published as a conference paper at ICLR 2023
SEQUENTIAL ATTENTION FOR FEATURE SELECTION
Taisuke Yasuda*â€ 
Computer Science Department
Carnegie Mellon University
taisukey@cs.cmu.edu
MohammadHossein Bateni, Lin Chen, Matthew Fahrbach, Gang Fu*, and Vahab Mirrokni
Google Research
{bateni,linche,fahrbach,thomasfu,mirrokni}@google.com
ABSTRACT
Feature selection is the problem of selecting a subset of features for a machine learn-
ing model that maximizes model quality subject to a budget constraint. For neural
networks, prior methods, including those based on â„“1 regularization, attention,
and other techniques, typically select the entire feature subset in one evaluation
round, ignoring the residual value of features during selection, i.e., the marginal
contribution of a feature given that other features have already been selected. We
propose a feature selection algorithm called Sequential Attention that achieves
state-of-the-art empirical results for neural networks. This algorithm is based on an
efï¬cient one-pass implementation of greedy forward selection and uses attention
weights at each step as a proxy for feature importance. We give theoretical insights
into our algorithm for linear regression by showing that an adaptation to this setting
is equivalent to the classical Orthogonal Matching Pursuit (OMP) algorithm, and
thus inherits all of its provable guarantees. Our theoretical and empirical analyses
offer new explanations towards the effectiveness of attention and its connections to
overparameterization, which may be of independent interest.
1
INTRODUCTION
Feature selection is a classic problem in machine learning and statistics where one is asked to ï¬nd
a subset of ğ‘˜features from a larger set of ğ‘‘features, such that the prediction quality of the model
trained using the subset of features is maximized. Finding a small and high-quality feature subset is
desirable for many reasons: improving model interpretability, reducing inference latency, decreasing
model size, regularization, and removing redundant or noisy features to improve generalization. We
direct the reader to Li et al. (2017b) for a comprehensive survey on the role of feature selection in
machine learning.
The widespread success of deep learning has prompted an intense study of feature selection algorithms
for neural networks, especially in the supervised setting. While many methods have been proposed,
we focus on a line of work that studies the use of attention for feature selection. The attention
mechanism in machine learning roughly refers to applying a trainable softmax mask to a given layer.
This allows the model to â€œfocusâ€ on certain important signals during training. Attention has recently
led to major breakthroughs in computer vision, natural language processing, and several other areas
of machine learning (Vaswani et al., 2017). For feature selection, the works of Wang et al. (2014); Gui
et al. (2019); Skrlj et al. (2020); Wojtas & Chen (2020); Liao et al. (2021) all present new approaches
for feature attribution, ranking, and selection that are inspired by attention.
One problem with naively using attention for feature selection is that it can ignore the residual values
of features, i.e., the marginal contribution a feature has on the loss conditioned on previously-selected
features being in the model. This can lead to several problems such as selecting redundant features or
ignoring features that are uninformative in isolation but valuable in the presence of others.
*Corresponding authors
â€ This work was done while T.Y. was an intern at Google Research.
1
arXiv:2209.14881v3  [cs.LG]  25 Apr 2023

Published as a conference paper at ICLR 2023
Figure 1: Sequential attention applied to model ğ‘“(Â·; ğœƒ). At each step, the selected features ğ‘–âˆˆğ‘†are
used as direct inputs to the model and the unselected features ğ‘–Ì¸âˆˆğ‘†are downscaled by the scalar
value softmaxğ‘–(w, ğ‘†), where w âˆˆRğ‘‘is the vector of learned attention weights and ğ‘†= [ğ‘‘] âˆ–ğ‘†.
This work introduces the Sequential Attention algorithm for supervised feature selection. Our algo-
rithm addresses the shortcomings above by using attention-based selection adaptively over multiple
rounds. Further, Sequential Attention simpliï¬es earlier attention-based approaches by directly training
one global feature mask instead of aggregating many instance-wise feature masks. This technique
reduces the overhead of our algorithm, eliminates the toil of tuning unnecessary hyperparameters,
works directly with any differentiable model architecture, and offers an efï¬cient streaming implemen-
tation. Empirically, Sequential Attention achieves state-of-the-art feature selection results for neural
networks on standard benchmarks. The code for our algorithm and experiments is publicly available.1
Sequential Attention.
Our starting point for Sequential Attention is the well-known greedy forward
selection algorithm, which repeatedly selects the feature with the largest marginal improvement in
model loss when added to the set of currently selected features (see, e.g., Das & Kempe (2011)
and Elenberg et al. (2018)). Greedy forward selection is known to select high-quality features, but
requires training ğ‘‚(ğ‘˜ğ‘‘) models and is thus impractical for many modern machine learning problems.
To reduce this cost, one natural idea is to only train ğ‘˜models, where the model trained in each step
approximates the marginal gains of all ğ‘‚(ğ‘‘) unselected features. Said another way, we can relax
the greedy algorithm to fractionally consider all ğ‘‚(ğ‘‘) feature candidates simultaneously rather than
computing their exact marginal gains one-by-one with separate models. We implement this idea by
introducing a new set of trainable variables w âˆˆRğ‘‘that represent feature importance, or attention
logits. In each step, we select the feature with maximum importance and add it to the selected set. To
ensure the score-augmented models (1) have differentiable architectures and (2) are encouraged to
hone in on the best unselected feature, we take the softmax of the importance scores and multiply
each input feature value by its corresponding softmax value as illustrated in Figure 1.
Formally, given a dataset X âˆˆRğ‘›Ã—ğ‘‘represented as a matrix with ğ‘›rows of examples and ğ‘‘feature
columns, suppose we want to select ğ‘˜features. Let ğ‘“(Â·; ğœƒ) be a differentiable model, e.g., a neural
network, that outputs the predictions ğ‘“(X; ğœƒ). Let y âˆˆRğ‘›be the labels, â„“(ğ‘“(X; ğœƒ), y) be the loss
between the modelâ€™s predictions and the labels, and âˆ˜be the Hadamard product. Sequential Attention
outputs a subset ğ‘†âŠ†[ğ‘‘] := {1, 2, . . . , ğ‘‘} of ğ‘˜feature indices, and is presented below in Algorithm 1.
Theoretical guarantees.
We give provable guarantees for Sequential Attention for least squares
linear regression by analyzing a variant of the algorithm called regularized linear Sequential Attention.
This variant (1) uses Hadamard product overparameterization directly between the attention weights
and feature values without normalizing the attention weights via softmax(w, ğ‘†), and (2) adds â„“2
regularization to the objective, hence the â€œlinearâ€ and â€œregularizedâ€ terms. Note that â„“2 regularization,
or weight decay, is common practice when using gradient-based optimizers (Tibshirani, 2021). We
give theoretical and empirical evidence that replacing the softmax by different overparameterization
schemes leads to similar results (Section 4.2) while offering more tractable analysis. In particular, our
main result shows that regularized linear Sequential Attention has the same provable guarantees as
the celebrated Orthogonal Matching Pursuit (OMP) algorithm of Pati et al. (1993) for sparse linear
regression, without making any assumptions on the design matrix or response vector.
Theorem 1.1. For linear regression, regularized linear Sequential Attention is equivalent to OMP.
1The code is available at: github.com/google-research/google-research/tree/master/sequential attention
2

Published as a conference paper at ICLR 2023
Algorithm 1 Sequential Attention for feature selection.
1: function SEQUENTIALATTENTION(dataset X âˆˆRğ‘›Ã—ğ‘‘, labels y âˆˆRğ‘›, model ğ‘“, loss â„“, size ğ‘˜)
2:
Initialize ğ‘†â†âˆ…
3:
for ğ‘¡= 1 to ğ‘˜do
4:
Let (ğœƒ*, w*) â†arg minğœƒ,w â„“(ğ‘“(X âˆ˜W; ğœƒ), y), where W = 1ğ‘›softmax(w, ğ‘†)âŠ¤for
softmaxğ‘–(w, ğ‘†) :=
â§
âª
â¨
âª
â©
1
if ğ‘–âˆˆğ‘†
exp(wğ‘–)
âˆ‘ï¸€
ğ‘—âˆˆğ‘†exp(wğ‘—)
if ğ‘–âˆˆğ‘†:= [ğ‘‘] âˆ–ğ‘†
(1)
5:
Set ğ‘–* â†arg maxğ‘–Ì¸âˆˆğ‘†w*
ğ‘–
â—unselected feature with largest attention weight
6:
Update ğ‘†â†ğ‘†âˆª{ğ‘–*}
7:
return ğ‘†
We prove this equivalence using a novel two-step argument. First, we show that regularized linear
Sequential Attention is equivalent to a greedy version of LASSO (Tibshirani, 1996), which Luo
& Chen (2014) call Sequential LASSO. Prior to our work, however, Sequential LASSO was only
analyzed in a restricted â€œsparse signal plus noiseâ€ setting, offering limited insight into its success in
practice. Second, we prove that Sequential LASSO is equivalent to OMP in the fully general setting
for linear regression by analyzing the geometry of the associated polyhedra. This ultimately allows
us to transfer the guarantees of OMP to Sequential Attention.
Theorem 1.2. For linear regression, Sequential LASSO (Luo & Chen, 2014) is equivalent to OMP.
We present the full argument for our results in Section 3. This analysis takes signiï¬cant steps towards
explaining the success of attention in feature selection and the various theoretical phenomena at play.
Towards understanding attention.
An important property of OMP is that it provably approximates
the marginal gains of featuresâ€”Das & Kempe (2011) showed that for any subset of features, the
gradient of the least squares loss at its sparse minimizer approximates the marginal gains up to a factor
that depends on the sparse condition numbers of the design matrix. This suggests that Sequential
Attention could also approximate some notion of the marginal gains for more sophisticated models
when selecting the next-best feature. We observe this phenomenon empirically in our marginal gain
experiments in Appendix B.6. These results also help reï¬ne the widely-assumed conjecture that
attention weights correlate with feature importances by specifying an exact measure of â€œimportanceâ€
at play. Since a countless number of feature importance deï¬nitions are used in practice, it is important
to understand which best explains how the attention mechanism works.
Connections to overparameterization.
In our analysis of regularized linear Sequential Attention
for linear regression, we do not use the presence of the softmax in the attention mechanismâ€”rather,
the crucial ingredient in our analysis is the Hadamard product parameterization of the learned weights.
We conjecture that the empirical success of attention-based feature selection is primarily due to the
explicit overparameterization.2 Indeed, our experiments in Section 4.2 verify this claim by showing
that if we substitute the softmax in Sequential Attention with a number of different (normalized)
overparamterized expressions, we achieve nearly identical performance. This line of reasoning is also
supported in the recent work of Ye et al. (2021), who claim that attention largely owes its success to
the â€œsmoother and stable [loss] landscapesâ€ induced by Hadamard product overparameterization.
1.1
RELATED WORK
Here we discuss recent advances in supervised feature selection for deep neural networks (DNNs)
that are the most related to our empirical results. In particular, we omit a discussion of a large body of
works on unsupervised feature selection (Zou et al., 2015; Altschuler et al., 2016; BalÄ±n et al., 2019).
2Note that overparameterization here refers to the addition of ğ‘‘trainable variables in the Hadamard product
overparameterization, not the other use of the term that refers to the use of a massive number of parameters in
neural networks, e.g., in Bubeck & Sellke (2021).
3

Published as a conference paper at ICLR 2023
The group LASSO method has been applied to DNNs to achieve structured sparsity by pruning neurons
(Alvarez & Salzmann, 2016) and even ï¬lters or channels in convolutional neural networks (Lebedev &
Lempitsky, 2016; Wen et al., 2016; Li et al., 2017a). It has also be applied for feature selection (Zhao
et al., 2015; Li et al., 2016; Scardapane et al., 2017; Lemhadri et al., 2021).
While the LASSO is the most widely-used method for relaxing the â„“0 sparsity constraint in feature
selection, several recent works have proposed new relaxations based on stochastic gates (Srinivas
et al., 2017; Louizos et al., 2018; BalÄ±n et al., 2019; Trelin & ProchÂ´azka, 2020; Yamada et al., 2020).
This approach introduces (learnable) Bernoulli random variables for each feature during training, and
minimizes the expected loss over realizations of the 0-1 variables (accepting or rejecting features).
There are several other recent approaches for DNN feature selection. Roy et al. (2015) explore using
the magnitudes of weights in the ï¬rst hidden layer to select features. Lu et al. (2018) designed the
DeepPINK architecture, extending the idea of knockoffs (Benjamini et al., 2001) to neural networks.
Here, each feature is paired with a â€œknockoffâ€ version that competes with the original feature; if the
knockoff wins, the feature is removed. Borisov et al. (2019) introduced the CancelOut layer, which
suppresses irrelevant features via independent per-feature activation functions, i.e., sigmoids, that act
as (soft) bitmasks.
In contrast to these differentiable approaches, the combinatorial optimization literature is rich with
greedy algorithms that have applications in machine learning (Zadeh et al., 2017; Fahrbach et al.,
2019b;a; Chen et al., 2021; Halabi et al., 2022; Bilmes, 2022). In fact, most inï¬‚uential feature selection
algorithms from this literature are sequential, e.g., greedy forward and backward selection (Ye & Sun,
2018; Das et al., 2022), Orthogonal Matching Pursuit (Pati et al., 1993), and several information-
theoretic methods (Fleuret, 2004; Ding & Peng, 2005; Bennasar et al., 2015). These approaches,
however, are not normally tailored to neural networks, and can suffer from quality, efï¬ciency, or both.
Lastly, this paper studies global feature selection, i.e., selecting the same subset of features across
all training examples, whereas many works consider local (or instance-wise) feature selection. This
problem is more related to model interpretability, and is better known as feature attribution or
saliency maps. These methods naturally lead to global feature selection methods by aggregating their
instance-wise scores (Cancela et al., 2020). Instance-wise feature selection has been explored using a
variety of techniques, including gradients (Smilkov et al., 2017; Sundararajan et al., 2017; Srinivas
& Fleuret, 2019), attention (Arik & Pï¬ster, 2021; Ye et al., 2021), mutual information (Chen et al.,
2018), and Shapley values from cooperative game theory (Lundberg & Lee, 2017).
2
PRELIMINARIES
Before discussing our theoretical guarantees for Sequential Attention in Section 3, we present several
known results about feature selection for linear regression, also called sparse linear regression. Recall
that in the least squares linear regression problem, we have
â„“(ğ‘“(X; ğœƒ), y) = â€–ğ‘“(X; ğœƒ) âˆ’yâ€–2
2 = â€–Xğœƒâˆ’yâ€–2
2.
(2)
We work in the most challenging setting for obtaining relative error guarantees for this objective by
making no distributional assumptions on X âˆˆRğ‘›Ã—ğ‘‘, i.e., we seek ËœğœƒâˆˆRğ‘‘such that
â€–XËœğœƒâˆ’yâ€–2
2 â‰¤ğœ…min
ğœƒâˆˆRğ‘‘â€–Xğœƒâˆ’yâ€–2
2,
(3)
for some ğœ…= ğœ…(X) > 0, where X is not assumed to follow any particular input distribution. This
is far more applicable in practice than assuming the entries of X are i.i.d. Gaussian. In large-scale
applications, the number of examples ğ‘›often greatly exceeds the number of features ğ‘‘, resulting in
an optimal loss that is nonzero. Thus, we focus on the overdetermined regime and refer to Price et al.
(2022) for an excellent discussion on the long history of this problem.
Notation.
Let X âˆˆRğ‘›Ã—ğ‘‘be the design matrix with â„“2 unit columns and let y âˆˆRğ‘›be the
response vector, also assumed to be an â„“2 unit vector.3 For ğ‘†âŠ†[ğ‘‘], let Xğ‘†denote the ğ‘›Ã— |ğ‘†| matrix
consisting of the columns of X indexed by ğ‘†. For singleton sets ğ‘†= {ğ‘—}, we write Xğ‘—for X{ğ‘—}.
Let Pğ‘†:= Xğ‘†X+
ğ‘†denote the projection matrix onto the column span colspan(Xğ‘†) of Xğ‘†, where
3These assumptions are without loss of generality by scaling.
4

Published as a conference paper at ICLR 2023
X+
ğ‘†denotes the pseudoinverse of Xğ‘†. Let PâŠ¥
ğ‘†= Iğ‘›âˆ’Pğ‘†denote the projection matrix onto the
orthogonal complement of colspan(Xğ‘†).
Feature selection algorithms for linear regression.
Perhaps the most natural algorithm for sparse
linear regression is greedy forward selection, which was shown to have guarantees of the form of (3)
in the breakthrough works of Das & Kempe (2011); Elenberg et al. (2018), where ğœ…= ğœ…(X) depends
on sparse condition numbers of X, i.e., the spectrum of X restricted to a subset of its columns. Greedy
forward selection can be expensive in practice, but these works also prove analogous guarantees for
the more efï¬cient Orthogonal Matching Pursuit algorithm, which we present formally in Algorithm 2.
Algorithm 2 Orthogonal Matching Pursuit (Pati et al., 1993).
1: function OMP(design matrix X âˆˆRğ‘›Ã—ğ‘‘, response y âˆˆRğ‘›, size constraint ğ‘˜)
2:
Initialize ğ‘†â†âˆ…
3:
for ğ‘¡= 1 to ğ‘˜do
4:
Set ğ›½*
ğ‘†â†arg minğ›½âˆˆRğ‘†â€–Xğ‘†ğ›½âˆ’yâ€–2
2
5:
Let ğ‘–* Ì¸âˆˆğ‘†maximize
â—maximum correlation with residual
âŸ¨Xğ‘–, y âˆ’Xğ‘†ğ›½*
ğ‘†âŸ©2 = âŸ¨Xğ‘–, y âˆ’Pğ‘†yâŸ©2 =
âŸ¨ï¸€
Xğ‘–, PâŠ¥
ğ‘†y
âŸ©ï¸€2
6:
Update ğ‘†â†ğ‘†âˆª{ğ‘–*}
7:
return ğ‘†
The LASSO algorithm (Tibshirani, 1996) is another popular feature selection method, which simply
adds â„“1-regularization to the objective in Equation (2). Theoretical guarantees for LASSO are known
in the underdetermined regime (Donoho & Elad, 2003; Candes & Tao, 2006), but it is an open problem
whether LASSO has the guarantees of Equation (3). Sequential LASSO is a related algorithm that
uses LASSO to select features one by one. Luo & Chen (2014) analyzed this algorithm in a speciï¬c
parameter regime, but until our work, no relative error guarantees were known in full generality (e.g.,
the overdetermined regime). We present the Sequential LASSO in Algorithm 3.
Algorithm 3 Sequential LASSO (Luo & Chen, 2014).
1: function SEQUENTIALLASSO(design matrix X âˆˆRğ‘›Ã—ğ‘‘, response y âˆˆRğ‘›, size constraint ğ‘˜)
2:
Initialize ğ‘†â†âˆ…
3:
for ğ‘¡= 1 to ğ‘˜do
4:
Let ğ›½*(ğœ†, ğ‘†) denote the optimal solution to
arg min
ğ›½âˆˆRğ‘‘
1
2â€–Xğ›½âˆ’yâ€–2
2 + ğœ†â€–ğ›½ğ‘†â€–1
(4)
5:
Set ğœ†*(ğ‘†) â†sup{ğœ†> 0 : ğ›½*(ğœ†, ğ‘†)ğ‘†Ì¸= 0}
â—largest ğœ†with nonzero LASSO on ğ‘†
6:
Let ğ´(ğ‘†) = limğœ€â†’0{ğ‘–âˆˆğ‘†: ğ›½*(ğœ†* âˆ’ğœ€, ğ‘†)ğ‘–Ì¸= 0}
7:
Select any ğ‘–* âˆˆğ´(ğ‘†)
â—non-empty by Lemma 3.5
8:
Update ğ‘†â†ğ‘†âˆª{ğ‘–*}
9:
return ğ‘†
Note that Sequential LASSO as stated requires a search for the optimal ğœ†* in each step. In practice, ğœ†
can simply be set to a large enough value to obtain similar results, since beyond a critical value of ğœ†,
the feature ranking according to LASSO coefï¬cients does not change (Efron et al., 2004).
3
EQUIVALENCE FOR LEAST SQUARES: OMP AND SEQUENTIAL ATTENTION
In this section, we show that the following algorithms are equivalent for least squares linear regression:
regularized linear Sequential Attention, Sequential LASSO, and Orthogonal Matching Pursuit.
5

Published as a conference paper at ICLR 2023
1
2
3
4
5
6
7
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
ï˜ƒÎ²1ï˜„
ï˜ƒÎ²2ï˜„
5
10
15
20
25
30
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
ï˜ƒÎ²1ï˜„
ï˜ƒÎ²2ï˜„
100
200
300
400
500
600
700
0
2
4
6
8
10
0
2
4
6
8
10
ï˜ƒÎ²1ï˜„
ï˜ƒÎ²2ï˜„
Figure 2: Contour plot of ğ‘„*(ğ›½âˆ˜ğ›½) for
ğ›½âˆˆR2 at different zoom-levels of |ğ›½ğ‘–|.
3.1
REGULARIZED LINEAR SEQUENTIAL ATTENTION AND SEQUENTIAL LASSO
We start by formalizing a modiï¬cation to Sequential Attention that admits provable guarantees.
Deï¬nition 3.1 (Regularized linear Sequential Attention). Let ğ‘†âŠ†[ğ‘‘] be the set of currently se-
lected features. We deï¬ne the regularized linear Sequential Attention objective by removing the
softmax(w, ğ‘†) normalization in Algorithm 1 and introducing â„“2 regularization on the importance
weights w âˆˆRğ‘†and model parameters ğœƒâˆˆRğ‘‘restricted to ğ‘†. That is, we consider the objective
min
wâˆˆRğ‘‘,ğœƒâˆˆRğ‘‘â€–X(s(w) âˆ˜ğœƒ) âˆ’yâ€–2
2 + ğœ†
2
(ï¸
â€–wâ€–2
2 + â€–ğœƒğ‘†â€–2
2
)ï¸
,
(5)
where s(w) âˆ˜ğœƒdenotes the Hadamard product, ğœƒğ‘†âˆˆRğ‘†is ğœƒrestricted to indices in ğ‘†, and
sğ‘–(w, ğ‘†) :=
{ï¸‚1
if ğ‘–âˆˆğ‘†,
wğ‘–
if ğ‘–Ì¸âˆˆğ‘†.
By a simple argument due to Hoff (2017), the objective function in (5) is equivalent to
min
ğœƒâˆˆRğ‘‘â€–Xğœƒâˆ’yâ€–2
2 + ğœ†â€–ğœƒğ‘†â€–1.
(6)
It follows that attention (or more generally overparameterization by trainable weights w) can be seen
as a way to implement â„“1 regularization for least squares linear regression, i.e., the LASSO (Tibshirani,
1996). This connection between overparameterization and â„“1 regularization has also been observed in
several other recent works (Vaskevicius et al., 2019; Zhao et al., 2022; Tibshirani, 2021).
By this transformation and reasoning, regularized linear Sequential Attention can be seen as iteratively
using the LASSO with â„“1 regularization applied only to the unselected featuresâ€”which is precisely
the Sequential LASSO algorithm in Luo & Chen (2014). If we instead use softmax(w, ğ‘†) as in (1),
then this only changes the choice of regularization, as shown in Lemma 3.2 (proof in Appendix A.3).
Lemma 3.2. Let ğ·: Rğ‘‘â†’Rğ‘†be the function deï¬ned by ğ·(w)ğ‘–= 1/softmax2
ğ‘–(w, ğ‘†), for ğ‘–âˆˆğ‘†.
Denote its range and preimage by ran(ğ·) âŠ†Rğ‘†and ğ·âˆ’1(Â·) âŠ†Rğ‘‘, respectively. Moreover, deï¬ne
the functions ğ‘„: ran(ğ·) â†’R and ğ‘„* : Rğ‘†â†’R by
ğ‘„(q) =
inf
wâˆˆğ·âˆ’1(q)â€–wâ€–2
2
and
ğ‘„*(x) =
inf
qâˆˆran(ğ·)
â›
ââˆ‘ï¸
ğ‘–âˆˆğ‘†
xğ‘–qğ‘–+ ğ‘„(q)
â
â .
Then, the following two optimization problems with respect to ğ›½âˆˆRğ‘‘are equivalent:
inf
ğ›½âˆˆRğ‘‘
s.t. ğ›½=softmax(w,ğ‘†)âˆ˜ğœƒ
wâˆˆRğ‘‘,ğœƒâˆˆRğ‘‘
â€–Xğ›½âˆ’yâ€–2
2 + ğœ†
2
(ï¸
â€–wâ€–2
2 + â€–ğœƒğ‘†â€–2
2
)ï¸
= inf
ğ›½âˆˆRğ‘‘â€–Xğ›½âˆ’yâ€–2
2 + ğœ†
2 ğ‘„*(ğ›½âˆ˜ğ›½). (7)
We present contour plots of ğ‘„*(ğ›½âˆ˜ğ›½) for ğ›½âˆˆR2 in Figure 2. These plots suggest that ğ‘„*(ğ›½âˆ˜ğ›½)
is a concave regularizer when |ğ›½1| + |ğ›½2| > 2, which would thus approximate the â„“0 regularizer and
induce a sparse solution of ğ›½(Zhang & Zhang, 2012), as â„“1 regularization does (Tibshirani, 1996).
3.2
SEQUENTIAL LASSO AND OMP
This connection between Sequential Attention and Sequential LASSO gives us a new perspective
about how Sequential Attention works. The only known guarantee for Sequential LASSO, to the best
6

Published as a conference paper at ICLR 2023
of our knowledge, is a statistical recovery result when the input is a sparse linear combination with
Gaussian noise in the ultra high-dimensional setting (Luo & Chen, 2014). This does not, however,
fully explain why Sequential Attention is such an effective feature selection algorithm.
To bridge our main results, we prove a novel equivalence between Sequential LASSO and OMP.
Theorem 3.3. Let X âˆˆRğ‘›Ã—ğ‘‘be a design matrix with â„“2 unit vector columns, and let y âˆˆRğ‘‘denote
the response, also an â„“2 unit vector. The Sequential LASSO algorithm maintains a set of features
ğ‘†âŠ†[ğ‘‘] such that, at each feature selection step, it selects a feature ğ‘–âˆˆğ‘†such that
âƒ’âƒ’âŸ¨ï¸€
Xğ‘–, PâŠ¥
ğ‘†y
âŸ©ï¸€âƒ’âƒ’=
âƒ¦âƒ¦XâŠ¤PâŠ¥
ğ‘†y
âƒ¦âƒ¦
âˆ,
where Xğ‘†is the ğ‘›Ã— |ğ‘†| matrix given formed by the columns of X indexed by ğ‘†, and PâŠ¥
ğ‘†is the
projection matrix onto the orthogonal complement of the span of Xğ‘†.
Note that this is extremely close to saying that Sequential LASSO and OMP select the exact same set
of features. The only difference appears when there are multiple features with norm â€–XâŠ¤PâŠ¥
ğ‘†yâ€–âˆ.
In this case, it is possible that Sequential LASSO chooses the next feature from a set of features
that is strictly smaller than the set of features from which OMP chooses, so the â€œtie-breakingâ€ can
differ between the two algorithms. In practice, however, this rarely happens. For instance, if only one
feature is selected at each step, which is the case with probability 1 if random continuous noise is
added to the data, then Sequential LASSO and OMP will select the exact same set of features.
Remark 3.4. It was shown in (Luo & Chen, 2014) that Sequential LASSO is equivalent to OMP in
the statistical recovery regime, i.e., when y = Xğ›½* + ğœ€for some true sparse weight vector ğ›½* and
i.i.d. Gaussian noise ğœ€âˆ¼ğ’©(0, ğœIğ‘›), under an ultra high-dimensional regime where the dimension ğ‘‘
is exponential in the number of examples ğ‘›. We prove this equivalence in the fully general setting.
The argument below shows that Sequential LASSO and OMP are equivalent, thus establishing
that regularized linear Sequential Attention and Sequential LASSO have the same approximation
guarantees as OMP.
Geometry of Sequential LASSO.
We ï¬rst study the geometry of optimal solutions to Equation (4).
Let ğ‘†âŠ†[ğ‘‘] be the set of currently selected features. Following work on the LASSO in Tibshirani &
Taylor (2011), we rewrite (4) as the following constrained optimization problem:
min
zâˆˆRğ‘›,ğ›½âˆˆRğ‘‘
1
2â€–z âˆ’yâ€–2
2 + ğœ†â€–ğ›½ğ‘†â€–1
subject to
z = Xğ›½.
(8)
It can then be shown that the dual problem is equivalent to ï¬nding the projection, i.e., closest point in
Euclidean distance, u âˆˆRğ‘›of PâŠ¥
ğ‘†y onto the polyhedral section ğ¶ğœ†âˆ©colspan(Xğ‘†)âŠ¥, where
ğ¶ğœ†:=
{ï¸€
uâ€² âˆˆRğ‘›:
âƒ¦âƒ¦XâŠ¤uâ€²âƒ¦âƒ¦
âˆâ‰¤ğœ†
}ï¸€
and colspan(Xğ‘†)âŠ¥denotes the orthogonal complement of colspan(Xğ‘†). See Appendix A.1 for the
full details. The primal and dual variables are related through z by
Xğ›½= z = y âˆ’u.
(9)
Selection of features in Sequential LASSO.
Next, we analyze how Sequential LASSO selects
its features. Let ğ›½*
ğ‘†= X+
ğ‘†y be the optimal solution for features restricted in ğ‘†. Then, subtracting
Xğ‘†ğ›½*
ğ‘†from both sides of (9) gives
Xğ›½âˆ’Xğ‘†ğ›½*
ğ‘†= y âˆ’Xğ‘†ğ›½*
ğ‘†âˆ’u
= PâŠ¥
ğ‘†y âˆ’u.
(10)
Note that if ğœ†â‰¥â€–XâŠ¤PâŠ¥
ğ‘†yâ€–âˆ, then the projection of PâŠ¥
ğ‘†y onto ğ¶ğœ†is just u = PâŠ¥
ğ‘†y, so by (10),
Xğ›½âˆ’Xğ‘†ğ›½*
ğ‘†= PâŠ¥
ğ‘†y âˆ’PâŠ¥
ğ‘†y = 0,
meaning that ğ›½is zero outside of ğ‘†. We now show that for ğœ†slightly smaller than â€–XâŠ¤PâŠ¥
ğ‘†yâ€–âˆ, the
residual PâŠ¥
ğ‘†y âˆ’u is in the span of features Xğ‘–that maximize the correlation with PâŠ¥
ğ‘†y.
7

Published as a conference paper at ICLR 2023
Lemma 3.5 (Projection residuals of the Sequential LASSO). Let p denote the projection of PâŠ¥
ğ‘†y
onto ğ¶ğœ†âˆ©colspan(Xğ‘†)âŠ¥. There exists ğœ†0 <
âƒ¦âƒ¦XâŠ¤PâŠ¥
ğ‘†y
âƒ¦âƒ¦
âˆsuch that for all ğœ†âˆˆ(ğœ†0, â€–XâŠ¤PâŠ¥
ğ‘†yâ€–âˆ)
the residual PâŠ¥
ğ‘†y âˆ’p lies on colspan(Xğ‘‡), for
ğ‘‡:=
{ï¸€
ğ‘–âˆˆ[ğ‘‘] :
âƒ’âƒ’âŸ¨ï¸€
Xğ‘–, PâŠ¥
ğ‘†y
âŸ©ï¸€âƒ’âƒ’=
âƒ¦âƒ¦XâŠ¤PâŠ¥
ğ‘†y
âƒ¦âƒ¦
âˆ
}ï¸€
.
We defer the proof of Lemma 3.5 to Appendix A.2.
By Lemma 3.5 and (10), the optimal ğ›½when selecting the next feature has the following properties:
1. if ğ‘–âˆˆğ‘†, then ğ›½ğ‘–is equal to the ğ‘–-th value in the previous solution ğ›½*
ğ‘†; and
2. if ğ‘–Ì¸âˆˆğ‘†, then ğ›½ğ‘–can be nonzero only if ğ‘–âˆˆğ‘‡.
It follows that Sequential LASSO selects a feature that maximizes the correlation |âŸ¨Xğ‘—, PâŠ¥
ğ‘†yâŸ©|, just
as OMP does. Thus, we have shown an equivalence between Sequential LASSO and OMP without
any additional assumptions.
4
EXPERIMENTS
4.1
FEATURE SELECTION FOR NEURAL NETWORKS
Small-scale experiments.
We investigate the performance of Sequential Attention, as presented in
Algorithm 1, through experiments on standard feature selection benchmarks for neural networks. In
these experiments, we consider six datasets used in experiments in Lemhadri et al. (2021); BalÄ±n et al.
(2019), and select ğ‘˜= 50 features using a one-layer neural network with hidden width 67 and ReLU
activation (just as in these previous works). For more points of comparison, we also implement the
attention-based feature selection algorithms of BalÄ±n et al. (2019); Liao et al. (2021) and the Group
LASSO, which has been considered in many works that aim to sparisï¬y neural networks as discussed
in Section 1.1. We also implement natural adaptations of the Sequential LASSO and OMP for neural
networks and evaluate their performance.
In Figure 3, we see that Sequential Attention is competitive with or outperforms all feature selection
algorithms on this benchmark suite. For each algorithm, we report the mean of the prediction
accuracies averaged over ï¬ve feature selection trials. We provide more details about the experimental
setup in Appendix B.2, including speciï¬cations about each dataset in Table 1 and the mean prediction
accuracies with their standard deviations in Table 2. We also visualize the selected features on MNIST
(i.e., pixels) in Figure 5.
SA
LLY
GL
SL
OMP CAE
0.97
0.98
0.99
1.00
Prediction Accuracy
Mice Protein
SA
LLY
GL
SL
OMP CAE
0.90
0.92
0.94
0.96
Prediction Accuracy
MNIST
SA
LLY
GL
SL
OMP CAE
0.82
0.84
0.86
Prediction Accuracy
MNIST-Fashion
SA
LLY
GL
SL
OMP CAE
0.850
0.875
0.900
0.925
0.950
Prediction Accuracy
ISOLET
SA
LLY
GL
SL
OMP CAE
0.94
0.96
0.98
1.00
Prediction Accuracy
COIL-20
SA
LLY
GL
SL
OMP CAE
0.86
0.88
0.90
0.92
0.94
Prediction Accuracy
Activity
Figure 3: Feature selection results for small-scale neural network experiments. Here, SA = Sequential
Attention, LLY = (Liao et al., 2021), GL = Group LASSO, SL = Sequential LASSO, OMP = OMP,
and CAE = Concrete Autoencoder (BalÄ±n et al., 2019).
We note that our algorithm is considerably more efï¬cient compared to prior feature selection algo-
rithms, especially those designed for neural networks. This is because many of these prior algorithms
introduce entire subnetworks to train (BalÄ±n et al., 2019; Gui et al., 2019; Wojtas & Chen, 2020; Liao
et al., 2021), whereas Sequential Attention only adds ğ‘‘additional trainable variables. Furthermore, in
8

Published as a conference paper at ICLR 2023
these experiments, we implement an optimized version of Algorithm 1 that only trains one model
rather than ğ‘˜models, by partitioning the training epochs into ğ‘˜parts and selecting one feature in
each of these ğ‘˜parts. Combining these two aspects makes for an extremely efï¬cient algorithm. We
provide an evaluation of the running time efï¬ciency of Sequential Attention in Appendix B.2.3.
Large-scale experiments.
To demonstrate the scalability of our algorithm, we perform large-scale
feature selection experiments on the Criteo click dataset, which consists of 39 features and over three
billion examples for predicting click-through rates (Diemert Eustache, Meynet Julien et al., 2017).
Our results in Figure 4 show that Sequential Attention outperforms other methods when at least 15
features are selected. In particular, these plots highlight the fact that Sequential Attention excels at
ï¬nding valuable features once a few features are already in the model, and that it has substantially less
variance than LASSO-based feature selection algorithms. See Appendix B.3 for further discussion.
10
15
20
25
30
35
Number of Selected Features
0.7150
0.7175
0.7200
0.7225
0.7250
0.7275
0.7300
0.7325
0.7350
AUC
10
15
20
25
30
35
Number of Selected Features
0.1355
0.1360
0.1365
0.1370
0.1375
0.1380
Loss
Sequential Attention
CMIM
Group LASSO, = 10
1
Group LASSO, = 10
4
Sequential LASSO, = 10
1
Sequential LASSO, = 10
4
[LLY2021]
Figure 4: AUC and log loss when selecting ğ‘˜âˆˆ{10, 15, 20, 25, 30, 35} features for Criteo dataset.
4.2
THE ROLE OF HADAMARD PRODUCT OVERPARAMETERIZATION IN ATTENTION
In Section 1, we argued that Sequential Attention has provable guarantees for least squares linear
regression by showing that a version that removes the softmax and introduces â„“2 regularization
results in an algorithm that is equivalent to OMP. Thus, there is a gap between the implementation of
Sequential Attention in Algorithm 1 and our theoretical analysis. We empirically bridge this gap by
showing that regularized linear Sequential Attention yields results that are almost indistinguishable
to the original version. In Figure 10 (Appendix B.5), we compare the following Hadamard product
overparameterization schemes:
â€¢ softmax: as described in Section 1
â€¢ â„“1: sğ‘–(w) = |wğ‘–| for ğ‘–âˆˆğ‘†, which captures the provable variant discussed in Section 1
â€¢ â„“2: sğ‘–(w) = |wğ‘–|2 for ğ‘–âˆˆğ‘†
â€¢ â„“1 normalized: sğ‘–(w) = |wğ‘–|/ âˆ‘ï¸€
ğ‘—âˆˆğ‘†|wğ‘—| for ğ‘–âˆˆğ‘†
â€¢ â„“2 normalized: sğ‘–(w) = |wğ‘–|2/ âˆ‘ï¸€
ğ‘—âˆˆğ‘†|wğ‘—|2 for ğ‘–âˆˆğ‘†
Further, for each of the benchmark datasets, all of these variants outperform LassoNet and the other
baselines considered in Lemhadri et al. (2021). See Appendix B.5 for more details.
5
CONCLUSION
This work introduces Sequential Attention, an adaptive attention-based feature selection algorithm
designed in part for neural networks. Empirically, Sequential Attention improves signiï¬cantly upon
previous methods on widely-used benchmarks. Theoretically, we show that a relaxed variant of
Sequential Attention is equivalent to Sequential LASSO (Luo & Chen, 2014). In turn, we prove a
novel connection between Sequential LASSO and Orthogonal Matching Pursuit, thus transferring the
provable guarantees of OMP to Sequential Attention and shedding light on our empirical results. This
analysis also provides new insights into the the role of attention for feature selection via adaptivity,
overparameterization, and connections to marginal gains.
9

Published as a conference paper at ICLR 2023
We conclude with a number of open questions that stem from this work. The ï¬rst question concerns
the generalization of our theoretical results for Sequential LASSO to other models. OMP admits
provable guarantees for a wide class of generalized linear models (Elenberg et al., 2018), so is the
same true for Sequential LASSO? Our second question concerns the role of softmax in Algorithm 1.
Our experimental results suggest that using softmax for overparametrization may not be necessary, and
that a wide variety of alternative expressions can be used. On the other hand, our provable guarantees
only hold for the overparameterization scheme in the regularized linear Sequential Attention algorithm
(see Deï¬nition 3.1). Can we obtain a deeper understanding about the pros and cons of the softmax
and other overparameterization patterns, both theoretically and empirically?
BIBLIOGRAPHY
Jason M. Altschuler, Aditya Bhaskara, Gang Fu, Vahab S. Mirrokni, Afshin Rostamizadeh, and
Morteza Zadimoghaddam. Greedy column subset selection: New bounds and distributed algorithms.
In Proceedings of the 33nd International Conference on Machine Learning, volume 48, pp. 2539â€“
2548. JMLR, 2016.
Jose M Alvarez and Mathieu Salzmann. Learning the number of neurons in deep networks. Advances
in neural information processing systems, 29, 2016.
Sercan Â¨O Arik and Tomas Pï¬ster. TabNet: Attentive interpretable tabular learning. In Proceedings of
the AAAI Conference on Artiï¬cial Intelligence, volume 35, pp. 6679â€“6687, 2021.
Muhammed Fatih BalÄ±n, Abubakar Abid, and James Zou. Concrete autoencoders: Differentiable
feature selection and reconstruction. In International conference on machine learning, pp. 444â€“453.
PMLR, 2019.
Yoav Benjamini, Dan Drai, Greg Elmer, Neri Kafkaï¬, and Ilan Golani. Controlling the false discovery
rate in behavior genetics research. Behavioural Brain Research, 125(1-2):279â€“284, 2001.
Mohamed Bennasar, Yulia Hicks, and Rossitza Setchi. Feature selection using joint mutual informa-
tion maximisation. Expert Systems with Applications, 42(22):8520â€“8532, 2015.
Jeff Bilmes.
Submodularity in machine learning and artiï¬cial intelligence.
arXiv preprint
arXiv:2202.00132, 2022.
Vadim Borisov, Johannes Haug, and Gjergji Kasneci. CancelOut: A layer for feature selection in deep
neural networks. In International conference on artiï¬cial neural networks, pp. 72â€“83. Springer,
2019.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge University Press, 2004.
SÂ´ebastien Bubeck and Mark Sellke. A universal law of robustness via isoperimetry. In Advances in
Neural Information Processing Systems, pp. 28811â€“28822, 2021.
Brais Cancela, VerÂ´onica BolÂ´on-Canedo, Amparo Alonso-Betanzos, and JoËœao Gama. A scalable
saliency-based feature selection method with instance-level information. Knowl. Based Syst., 192:
105326, 2020.
Emmanuel J Candes and Terence Tao. Near-optimal signal recovery from random projections:
Universal encoding strategies? IEEE Transactions on Information Theory, 52(12):5406â€“5425,
2006.
Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An information-
theoretic perspective on model interpretation. In International Conference on Machine Learning,
pp. 883â€“892. PMLR, 2018.
Lin Chen, Hossein Esfandiari, Gang Fu, Vahab S. Mirrokni, and Qian Yu. Feature Cross Search via
Submodular Optimization. In 29th Annual European Symposium on Algorithms (ESA 2021), pp.
31:1â€“31:16, 2021.
10

Published as a conference paper at ICLR 2023
Abhimanyu Das and David Kempe. Submodular meets spectral: Greedy algorithms for subset
selection, sparse approximation and dictionary selection. In Proceedings of the 28th International
Conference on Machine Learning, pp. 1057â€“1064, 2011.
Sandipan Das, Alireza M Javid, Prakash Borpatra Gohain, Yonina C Eldar, and Saikat Chatterjee.
Neural greedy pursuit for feature selection. arXiv preprint arXiv:2207.09390, 2022.
Diemert Eustache, Meynet Julien, Pierre Galland, and Damien Lefortier. Attribution modeling
increases efï¬ciency of bidding in display advertising. In Proceedings of the AdKDD and TargetAd
Workshop, KDD, Halifax, NS, Canada, August, 14, 2017. ACM, 2017.
Chris H. Q. Ding and Hanchuan Peng. Minimum redundancy feature selection from microarray gene
expression data. J. Bioinform. Comput. Biol., 3(2):185â€“206, 2005.
David L Donoho and Michael Elad. Optimally sparse representation in general (nonorthogonal)
dictionaries via â„“1 minimization. Proceedings of the National Academy of Sciences, 100(5):
2197â€“2202, 2003.
Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. The
Annals of Statistics, 32(2):407â€“499, 2004.
Ethan R Elenberg, Rajiv Khanna, Alexandros G Dimakis, and Sahand Negahban. Restricted strong
convexity implies weak submodularity. The Annals of Statistics, 46(6B):3539â€“3568, 2018.
Matthew Fahrbach, Vahab Mirrokni, and Morteza Zadimoghaddam. Non-monotone submodular
maximization with nearly optimal adaptivity and query complexity. In International Conference
on Machine Learning, pp. 1833â€“1842. PMLR, 2019a.
Matthew Fahrbach, Vahab Mirrokni, and Morteza Zadimoghaddam. Submodular maximization with
nearly optimal approximation, adaptivity and query complexity. In Proceedings of the Thirtieth
Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 255â€“273. SIAM, 2019b.
FrancÂ¸ois Fleuret. Fast binary feature selection with conditional mutual information. Journal of
Machine learning research, 5(9), 2004.
Ning Gui, Danni Ge, and Ziyin Hu. AFS: An attention-based mechanism for supervised feature
selection. In Proceedings of the AAAI conference on artiï¬cial intelligence, volume 33, pp. 3705â€“
3713, 2019.
Marwa El Halabi, Suraj Srinivas, and Simon Lacoste-Julien. Data-efï¬cient structured pruning via
submodular optimization. arXiv preprint arXiv:2203.04940, 2022.
Peter D Hoff. Lasso, fractional norm and structured sparse estimation using a Hadamard product
parametrization. Computational Statistics & Data Analysis, 115:186â€“198, 2017.
Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2554â€“2564, 2016.
Ismael Lemhadri, Feng Ruan, and Rob Tibshirani. Lassonet: Neural networks with feature sparsity.
In International Conference on Artiï¬cial Intelligence and Statistics, pp. 10â€“18. PMLR, 2021.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ï¬lters for
efï¬cient convnets. In 5th International Conference on Learning Representations (ICLR), 2017a.
Jundong Li, Kewei Cheng, Suhang Wang, Fred Morstatter, Robert P Trevino, Jiliang Tang, and Huan
Liu. Feature selection: A data perspective. ACM Computing Surveys (CSUR), 50(6):1â€“45, 2017b.
Yifeng Li, Chih-Yu Chen, and Wyeth W Wasserman. Deep feature selection: Theory and application
to identify enhancers and promoters. Journal of Computational Biology, 23(5):322â€“336, 2016.
Yiwen Liao, RaphaÂ¨el Latty, and Bin Yang. Feature selection using batch-wise attenuation and feature
mask normalization. In 2021 International Joint Conference on Neural Networks (IJCNN), pp.
1â€“9. IEEE, 2021.
11

Published as a conference paper at ICLR 2023
Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through
ğ¿0 regularization. In 6th International Conference on Learning Representations (ICLR), 2018.
Yang Lu, Yingying Fan, Jinchi Lv, and William Stafford Noble. DeepPINK: Reproducible feature
selection in deep neural networks. Advances in Neural Information Processing Systems, 31, 2018.
Scott M Lundberg and Su-In Lee. A uniï¬ed approach to interpreting model predictions. Advances in
Neural Information Processing Systems, 30, 2017.
Shan Luo and Zehua Chen. Sequential lasso cum EBIC for feature selection with ultra-high di-
mensional feature space. Journal of the American Statistical Association, 109(507):1229â€“1240,
2014.
Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy Krishnaprasad. Orthogonal
matching pursuit: Recursive function approximation with applications to wavelet decomposition.
In Proceedings of 27th Asilomar conference on signals, systems and computers, pp. 40â€“44. IEEE,
1993.
Eric Price, Sandeep Silwal, and Samson Zhou. Hardness and algorithms for robust and sparse
optimization. In International Conference on Machine Learning, pp. 17926â€“17944. PMLR, 2022.
Debaditya Roy, K Sri Rama Murty, and C Krishna Mohan. Feature selection using deep neural
networks. In 2015 International Joint Conference on Neural Networks (IJCNN), pp. 1â€“6. IEEE,
2015.
Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. Group sparse regular-
ization for deep neural networks. Neurocomputing, 241:81â€“89, 2017.
Blaz Skrlj, Saso Dzeroski, Nada Lavrac, and Matej Petkovic. Feature importance estimation with
self-attention networks. In 24th European Conference on Artiï¬cial Intelligence (ECAI), volume
325 of Frontiers in Artiï¬cial Intelligence and Applications, pp. 1491â€“1498. IOS Press, 2020.
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda ViÂ´egas, and Martin Wattenberg. SmoothGrad:
Removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.
Suraj Srinivas and FrancÂ¸ois Fleuret. Full-gradient representation for neural network visualization.
Advances in Neural Information Processing Systems, 32, 2019.
Suraj Srinivas, Akshayvarun Subramanya, and R Venkatesh Babu. Training sparse neural networks.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition workshops,
pp. 138â€“145, 2017.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
International Conference on Machine Learning, pp. 3319â€“3328. PMLR, 2017.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267â€“288, 1996.
Ryan J Tibshirani. Equivalences between sparse models and neural networks. Working Notes. URL
https://www. stat. cmu. edu/Ëœ ryantibs/papers/sparsitynn. pdf, 2021.
Ryan J. Tibshirani and Jonathan Taylor. The solution path of the generalized lasso. The Annals of
Statistics, 39(3):1335â€“1371, 2011.
Andrii Trelin and AleË‡s ProchÂ´azka. Binary stochastic ï¬ltering: Feature selection and beyond. arXiv
preprint arXiv:2007.03920, 2020.
Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal sparse
recovery. Advances in Neural Information Processing Systems, 32, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz
Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing
Systems, 30, 2017.
12

Published as a conference paper at ICLR 2023
Qian Wang, Jiaxing Zhang, Sen Song, and Zheng Zhang. Attentional neural network: Feature
selection using cognitive feedback. Advances in Neural Information Processing Systems, 27, 2014.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. Advances in Neural Information Processing Systems, 29, 2016.
Maksymilian Wojtas and Ke Chen. Feature importance ranking for deep learning. Advances in
Neural Information Processing Systems, 33:5105â€“5114, 2020.
Yutaro Yamada, Oï¬r Lindenbaum, Sahand Negahban, and Yuval Kluger. Feature selection using
stochastic gates. In International Conference on Machine Learning, pp. 10648â€“10659. PMLR,
2020.
Mao Ye and Yan Sun. Variable selection via penalized neural network: A drop-out-one loss approach.
In International Conference on Machine Learning, pp. 5620â€“5629. PMLR, 2018.
Xiang Ye, Zihang He, Heng Wang, and Yong Li. Towards understanding the effectiveness of attention
mechanism. arXiv preprint arXiv:2106.15067, 2021.
Sepehr Abbasi Zadeh, Mehrdad Ghadiri, Vahab S. Mirrokni, and Morteza Zadimoghaddam. Scalable
feature selection via distributed diversity maximization. In Proceedings of the Thirty-First AAAI
Conference on Artiï¬cial Intelligence, pp. 2876â€“2883. AAAI Press, 2017.
Cun-Hui Zhang and Tong Zhang. A general theory of concave regularization for high-dimensional
sparse estimation problems. Statist. Sci., 27(4):576â€“593, 2012.
Lei Zhao, Qinghua Hu, and Wenwu Wang. Heterogeneous feature selection with multi-modal deep
neural networks and sparse group lasso. IEEE Transactions on Multimedia, 17(11):1936â€“1948,
2015.
Peng Zhao, Yun Yang, and Qiao-Chu He. High-dimensional linear regression via implicit regulariza-
tion. Biometrika, 2022.
Qin Zou, Lihao Ni, Tong Zhang, and Qian Wang. Deep learning based feature selection for remote
sensing scene classiï¬cation. IEEE Geosci. Remote. Sens. Lett., 12(11):2321â€“2325, 2015.
13

Published as a conference paper at ICLR 2023
A
MISSING PROOFS FROM SECTION 3
A.1
LAGRANGIAN DUAL OF SEQUENTIAL LASSO
We ï¬rst show that the Lagrangian dual of (8) is equivalent to the following problem:
min
uâˆˆRğ‘›
1
2â€–y âˆ’uâ€–2
2
subject to
âƒ¦âƒ¦XâŠ¤u
âƒ¦âƒ¦
âˆâ‰¤ğœ†
XâŠ¤
ğ‘—u = 0,
âˆ€ğ‘—âˆˆğ‘†
(11)
We then use the Pythagorean theorem to replace y by PâŠ¥
ğ‘†y.
First consider the Lagrangian dual problem:
max
uâˆˆRğ‘›
min
zâˆˆRğ‘›,ğ›½âˆˆRğ‘‘
1
2â€–z âˆ’yâ€–2
2 + ğœ†â€–ğ›½|ğ‘†â€–1 + uâŠ¤(z âˆ’Xğ›½).
(12)
Note that the primal problem is strictly feasible and convex, so strong duality holds (see, e.g., Section
5.2.3 of Boyd & Vandenberghe (2004)). Considering just the terms involving the variable z in (12),
we have that
1
2â€–z âˆ’yâ€–2
2 + uâŠ¤z = 1
2â€–zâ€–2
2 âˆ’(y âˆ’u)âŠ¤z + 1
2â€–yâ€–2
2
= 1
2â€–z âˆ’(y âˆ’u)â€–2
2 + 1
2â€–yâ€–2
2 âˆ’1
2â€–y âˆ’uâ€–2
2,
which is minimized at z = y âˆ’u as z varies over Rğ‘›. On the other hand, consider just the terms
involving the variable ğ›½in (12), that is,
ğœ†â€–ğ›½ğ‘†â€–1 âˆ’uâŠ¤Xğ›½.
(13)
Note that if XâŠ¤u is nonzero on any coordinate in ğ‘†, then (13) can be made arbitrarily negative
by setting ğ›½ğ‘†to be zero and ğ›½ğ‘†appropriately. Similarly, if â€–XâŠ¤uâ€–âˆ> ğœ†, then (13) can also be
made to be arbitrarily negative. On the other hand, if (XâŠ¤u)ğ‘†= 0 and
âƒ¦âƒ¦XâŠ¤u
âƒ¦âƒ¦
âˆâ‰¤ğœ†, then (13) is
minimized at 0. This gives the dual in Equation (11).
We now show that by the Pythagorean theorem, we can project PâŠ¥
ğ‘†y in (11) rather than y. In (11),
recall that u is constrained to be in colspan(Xğ‘†)âŠ¥. Then, by the Pythagorean theorem, we have
1
2â€–y âˆ’uâ€–2
2 = 1
2
âƒ¦âƒ¦y âˆ’PâŠ¥
ğ‘†y + PâŠ¥
ğ‘†y âˆ’u
âƒ¦âƒ¦2
2
= 1
2
âƒ¦âƒ¦y âˆ’PâŠ¥
ğ‘†y
âƒ¦âƒ¦2
2 + 1
2
âƒ¦âƒ¦PâŠ¥
ğ‘†y âˆ’u
âƒ¦âƒ¦2
2,
since y âˆ’PâŠ¥
ğ‘†y = Pğ‘†y is orthogonal to colspan(Xğ‘†)âŠ¥and both PâŠ¥
ğ‘†y and u are in colspan(Xğ‘†)âŠ¥.
The ï¬rst term in the above does not depend on u and thus we may discard it. Our problem therefore
reduces to projecting PâŠ¥
ğ‘†y onto ğ¶ğœ†âˆ©colspan(Xğ‘†)âŠ¥, rather than y.
A.2
PROOF OF LEMMA 3.5
Proof of Lemma 3.5. Our approach is to reduce the projection of PâŠ¥
ğ‘†y onto the polytope deï¬ned by
ğ¶ğœ†âˆ©colspan(X)âŠ¥to a projection onto an afï¬ne space.
We ï¬rst argue that it sufï¬ces to project onto the faces of ğ¶ğœ†speciï¬ed by set ğ‘‡. For ğœ†> 0, feature
indices ğ‘–âˆˆ[ğ‘‘], and signs Â±, we deï¬ne the faces
ğ¹ğœ†,ğ‘–,Â± := {u âˆˆRğ‘›: Â±âŸ¨Xğ‘–, uâŸ©= ğœ†}
of ğ¶ğœ†. Let ğœ†= (1 âˆ’ğœ€)â€–XâŠ¤PâŠ¥
ğ‘†yâ€–âˆ, for ğœ€> 0 to be chosen sufï¬ciently small. Then clearly
(1 âˆ’ğœ€)PâŠ¥
ğ‘†y âˆˆğ¶ğœ†âˆ©colspan(Xğ‘†)âŠ¥,
14

Published as a conference paper at ICLR 2023
so
min
uâˆˆğ¶ğœ†âˆ©colspan(Xğ‘†)âŠ¥
âƒ¦âƒ¦PâŠ¥
ğ‘†y âˆ’u
âƒ¦âƒ¦2
2 â‰¤
âƒ¦âƒ¦PâŠ¥
ğ‘†y âˆ’(1 âˆ’ğœ€)PâŠ¥
ğ‘†y
âƒ¦âƒ¦2
2
= ğœ€2âƒ¦âƒ¦PâŠ¥
ğ‘†y
âƒ¦âƒ¦2
2.
In fact, (1 âˆ’ğœ€)PâŠ¥
ğ‘†y lies on the intersection of faces ğ¹ğœ†,ğ‘–,Â± for an appropriate choice of signs and
ğ‘–âˆˆğ‘‡. Without loss of generality, we assume that these faces are just ğ¹ğœ†,ğ‘–,+ for ğ‘–âˆˆğ‘‡. Note also that
for any ğ‘–/âˆˆğ‘‡,
min
uâˆˆğ¹ğœ†,ğ‘–,Â±
âƒ¦âƒ¦PâŠ¥
ğ‘†y âˆ’u
âƒ¦âƒ¦2
2 â‰¥
min
uâˆˆğ¹ğœ†,ğ‘–,Â±
âŸ¨ï¸€
Xğ‘–, PâŠ¥
ğ‘†y âˆ’u
âŸ©ï¸€2
(Cauchyâ€“Schwarz, â€–Xğ‘–â€–2 â‰¤1)
=
min
uâˆˆğ¹ğœ†,ğ‘–,Â±
âƒ’âƒ’XâŠ¤
ğ‘–PâŠ¥
ğ‘†y âˆ’XâŠ¤
ğ‘–u
âƒ’âƒ’2
=
(ï¸€âƒ’âƒ’XâŠ¤
ğ‘–PâŠ¥
ğ‘†y
âƒ’âƒ’âˆ’ğœ†
)ï¸€2
(u âˆˆğ¹ğœ†,ğ‘–,Â±)
â‰¥
(ï¸
(1 âˆ’ğœ€)
âƒ¦âƒ¦XâŠ¤PâŠ¥
ğ‘†y
âƒ¦âƒ¦
âˆâˆ’
âƒ¦âƒ¦XâŠ¤
ğ‘‡PâŠ¥
ğ‘†y
âƒ¦âƒ¦
âˆ
)ï¸2
.
For all ğœ€< ğœ€0, for ğœ€0 small enough, this is larger than ğœ€2â€–PâŠ¥
ğ‘†yâ€–2
2. Thus, for ğœ€small enough, PâŠ¥
ğ‘†y is
closer to the faces ğ¹ğœ†,ğ‘–,+ for ğ‘–âˆˆğ‘‡than any other face. Therefore, we set ğœ†0 = (1âˆ’ğœ€0)â€–XâŠ¤PâŠ¥
ğ‘†yâ€–âˆ.
Now, by the complementary slackness of the KKT conditions for the projection u of PâŠ¥
ğ‘†y onto ğ¶ğœ†,
for each face of ğ¶ğœ†we either have that u lies on the face or that the projection does not change if we
remove the face. For ğ‘–/âˆˆğ‘‡, note that by the above calculation, the projection u cannot lie on ğ¹ğœ†,ğ‘–,Â±,
so u is simply the projection onto
ğ¶â€² =
{ï¸€
u âˆˆRğ‘›: XâŠ¤
ğ‘‡u â‰¤ğœ†1ğ‘‡
}ï¸€
.
By reversing the dual problem reasoning from before, the residual of the projection onto ğ¶â€² must lie
on the column span of Xğ‘‡.
A.3
PARAMETERIZATION PATTERNS AND REGULARIZATION
Proof of Lemma 3.2. The optimization problem on the left-hand side of Equation (7) with respect
to ğ›½is equivalent to
inf
ğ›½âˆˆRğ‘‘
â›
ââ€–Xğ›½âˆ’yâ€–2
2 + ğœ†
2 inf
wâˆˆRğ‘‘
â›
ââ€–wâ€–2
2 +
âˆ‘ï¸
ğ‘–âˆˆğ‘†
ğ›½2
ğ‘–
sğ‘–(w)2
â
â 
â
â .
(14)
If we deï¬ne
Ëœğ‘„*(x) = inf
wâˆˆRğ‘‘
â›
ââ€–wâ€–2
2 +
âˆ‘ï¸
ğ‘–âˆˆğ‘†
xğ‘–
sğ‘–(w)2
â
â ,
then the LHS of (7) and (14) are equivalent to infğ›½âˆˆRğ‘‘(â€–Xğ›½âˆ’yâ€–2
2+ ğœ†
2 Ëœğ‘„*(ğ›½âˆ˜ğ›½)). Re-parameterizing
the minimization problem in the deï¬nition of Ëœğ‘„*(x) (by setting q = ğ·(w)), we obtain Ëœğ‘„* = ğ‘„*.
B
ADDITIONAL EXPERIMENTS
B.1
VISUALIZATION OF SELECTED MNIST FEATURES
In Figure 5, we present visualizations of the features (i.e., pixels) selected by Sequential Attention
and the baseline algorithms. This provides some intuition on the nature of the features that these
algorithms select. Similar visualizations for MNIST can be found in works such as BalÄ±n et al. (2019);
Gui et al. (2019); Wojtas & Chen (2020); Lemhadri et al. (2021); Liao et al. (2021). Note that these
visualizations serve as a basic sanity check about the kinds of pixels that these algorithms select. For
instance, the degree to which the selected pixels are â€œclusteredâ€ can be used to informally assess
the redundancy of features selected for image datasets, since neighboring pixels tend to represent
15

Published as a conference paper at ICLR 2023
redundant information. It is also useful at time to assess which regions of the image are selected. For
example, the central regions of the MNIST images are more informative than the edges.
Sequential Attention selects a highly diverse set of pixels due to its adaptivity. Sequential LASSO also
selects a very similar set of pixels, as suggested by our theoretical analysis in Section 3. Curiously,
OMP does not yield a competitive set of pixels, which demonstrates that OMP does not generalize
well from least squares regression and generalized linear models to deep neural networks.
Sequential Attention
Liao-Latty-Yang 2021
Group LASSO
Sequential LASSO
OMP
Figure 5: Visualizations of the ğ‘˜= 50 pixels selected by the feature selection algorithms on MNIST.
B.2
ADDITIONAL DETAILS ON SMALL-SCALE EXPERIMENTS
We start by presenting details about each of the datasets used for neural network feature selection
in BalÄ±n et al. (2019) and Lemhadri et al. (2021) in Table 1.
Table 1: Statistics about benchmark datasets.
Dataset
# Examples
# Features
# Classes
Type
Mice Protein
1,080
77
8
Biology
MNIST
60,000
784
10
Image
MNIST-Fashion
60,000
784
10
Image
ISOLET
7,797
617
26
Speech
COIL-20
1,440
400
20
Image
Activity
5,744
561
6
Sensor
In Figure 3, the error bars are computed using the standard deviation over ï¬ve runs of the algorithm
with different random seeds. The values used to generate these plots are provided below in Table 2.
Table 2: Feature selection results for small-scale datasets (see Figure 3 for a key). These values are
the average prediction accuracies on the test data and their standard deviations.
Dataset
SA
LLY
GL
SL
OMP
CAE
Mice Protein
0.993 Â± 0.008
0.981 Â± 0.005
0.985 Â± 0.005
0.984 Â± 0.008
0.994 Â± 0.008
0.956 Â± 0.012
MNIST
0.956 Â± 0.002
0.944 Â± 0.001
0.937 Â± 0.003
0.959 Â± 0.001
0.912 Â± 0.004
0.909 Â± 0.007
MNIST-Fashion
0.854 Â± 0.003
0.843 Â± 0.005
0.834 Â± 0.004
0.854 Â± 0.003
0.829 Â± 0.008
0.839 Â± 0.003
ISOLET
0.920 Â± 0.006
0.866 Â± 0.012
0.906 Â± 0.006
0.920 Â± 0.003
0.727 Â± 0.026
0.893 Â± 0.011
COIL-20
0.997 Â± 0.001
0.994 Â± 0.002
0.997 Â± 0.004
0.988 Â± 0.005
0.967 Â± 0.014
0.972 Â± 0.007
Activity
0.931 Â± 0.004
0.897 Â± 0.025
0.933 Â± 0.002
0.931 Â± 0.003
0.905 Â± 0.013
0.921 Â± 0.001
B.2.1
MODEL ACCURACIES WITH ALL FEATURES
To adjust for the differences between the values reported in Lemhadri et al. (2021) and ours due (e.g.,
due to factors such as the implementation framework), we list the accuracies obtained by training the
models with all of the available features in Table 3.
B.2.2
GENERALIZING OMP TO NEURAL NETWORKS
As stated in Algorithm 2, it may be difï¬cult to see exactly how OMP generalizes from a linear
regression model to neural networks. To do this, ï¬rst observe that OMP naturally generalizes to
16

Published as a conference paper at ICLR 2023
Table 3: Model accuracies when trained using all available features.
Dataset
Lemhadri et al. (2021)
This paper
Mice Protein
0.990
0.963
MNIST
0.928
0.953
MNIST-Fashion
0.833
0.869
ISOLET
0.953
0.961
COIL-20
0.996
0.986
Activity
0.853
0.954
generalized linear models (GLMs) via the gradient of the link function, as shown in Elenberg et al.
(2018). Then, to extend this to neural networks, we view the neural network as a GLM for any ï¬xing
of the hidden layer weights, and then we use the gradient of this GLM with respect to the inputs as
the feature importance scores.
B.2.3
EFFICIENCY EVALUATION
In this subsection, we evaluate the efï¬ciency of the Sequential Attention algorithm against our other
baseline algorithms. We do so by ï¬xing the number of epochs and batch size for all of the algorithms,
and then evaluating the accuracy as well as the wall clock time of each algorithm. Figures 6 and 7
provide a visualization of the accuracy and wall clock time of feature selection, while Tables 5 and 6
provide the average and standard deviations. Table 4 provides the epochs and batch size settings that
were ï¬xed for these experiments.
SA
LLY
GL
SL
OMP CAE
0.94
0.96
0.98
1.00
Prediction Accuracy
Mice Protein
SA
LLY
GL
SL
OMP CAE
0.900
0.925
0.950
0.975
1.000
Prediction Accuracy
MNIST
SA
LLY
GL
SL
OMP CAE
0.83
0.84
0.85
0.86
Prediction Accuracy
MNIST-Fashion
SA
LLY
GL
SL
OMP CAE
0.80
0.85
0.90
0.95
1.00
Prediction Accuracy
ISOLET
SA
LLY
GL
SL
OMP CAE
0.96
0.98
1.00
Prediction Accuracy
COIL-20
SA
LLY
GL
SL
OMP CAE
0.86
0.88
0.90
0.92
0.94
Prediction Accuracy
Activity
Figure 6: Feature selection accuracy for efï¬ciency evaluation.
SA
LLY
GL
SL
OMP CAE
0
100
200
300
400
Selection Time (s)
Mice Protein
SA
LLY
GL
SL
OMP CAE
0
50
100
Selection Time (s)
MNIST
SA
LLY
GL
SL
OMP CAE
0
200
400
600
Selection Time (s)
MNIST-Fashion
SA
LLY
GL
SL
OMP CAE
0
200
400
600
Selection Time (s)
ISOLET
SA
LLY
GL
SL
OMP CAE
0
100
200
300
Selection Time (s)
COIL-20
SA
LLY
GL
SL
OMP CAE
0
500
1000
Selection Time (s)
Activity
Figure 7: Feature selection wall clock time in seconds for efï¬ciency evaluation.
17

Published as a conference paper at ICLR 2023
Table 4: Epochs and batch size used to compare the efï¬ciency of feature selection algorithms.
Dataset
Epochs
Batch Size
Mice Protein
2000
256
MNIST
50
256
MNIST-Fashion
250
128
ISOLET
500
256
COIL-20
1000
256
Activity
1000
512
Table 5: Feature selection accuracy for efï¬ciency evaluation. We report the mean accuracy on the test
dataset and the the standard deviation across ï¬ve trials.
Dataset
SA
LLY
GL
SL
OMP
CAE
Mice Protein
0.984 Â± 0.012
0.907 Â± 0.042
0.968 Â± 0.028
0.961 Â± 0.025
0.556 Â± 0.032
0.956 Â± 0.012
MNIST
0.958 Â± 0.001
0.932 Â± 0.009
0.930 Â± 0.001
0.957 Â± 0.001
0.912 Â± 0.000
0.909 Â± 0.007
MNIST-Fashion
0.852 Â± 0.001
0.833 Â± 0.004
0.834 Â± 0.001
0.852 Â± 0.003
0.722 Â± 0.029
0.839 Â± 0.003
ISOLET
0.931 Â± 0.001
0.853 Â± 0.016
0.885 Â± 0.000
0.921 Â± 0.002
0.580 Â± 0.025
0.893 Â± 0.011
COIL-20
0.993 Â± 0.004
0.976 Â± 0.014
0.997 Â± 0.000
0.993 Â± 0.004
0.988 Â± 0.002
0.972 Â± 0.007
Activity
0.926 Â± 0.002
0.881 Â± 0.013
0.923 Â± 0.002
0.927 Â± 0.006
0.909 Â± 0.000
0.921 Â± 0.001
Table 6: Feature selection wall clock time in seconds for efï¬ciency evaluations. These values are the
mean wall clock time on the test dataset and their standard deviation across ï¬ve trials.
Dataset
SA
LLY
GL
SL
OMP
CAE
Mice Protein
57.5 Â± 1.5
90.5 Â± 3.5
49.0 Â± 1.0
46.5 Â± 1.5
50.5 Â± 1.5
375.0 Â± 16.0
MNIST
54.5 Â± 2.5
80.0 Â± 5.0
53.0 Â± 1.0
51.5 Â± 2.5
55.5 Â± 0.5
119.0 Â± 1.0
MNIST-Fashion
279.5 Â± 0.5
553.0 Â± 2.0
265.0 Â± 1.0
266.0 Â± 1.0
309.5 Â± 3.5
583.5 Â± 2.5
ISOLET
61.0 Â± 0.0
76.0 Â± 0.0
59.0 Â± 1.0
56.5 Â± 1.5
62.0 Â± 1.0
611.5 Â± 21.5
COIL-20
52.0 Â± 0.0
54.0 Â± 0.0
47.0 Â± 1.0
48.5 Â± 0.5
52.0 Â± 1.0
304.5 Â± 7.5
Activity
123.0 Â± 1.0
159.5 Â± 0.5
113.5 Â± 0.5
116.0 Â± 0.0
121.5 Â± 0.5
1260.5 Â± 2.5
B.2.4
NOTES ON THE ONE-PASS IMPLEMENTATION
We make several remarks about the one-pass implementation of Sequential Attention. First, as noted
in Section 4.1, our practical implementation of Sequential Attention only trains one model instead
of ğ‘˜models. We do this by partitioning the training epochs into ğ‘˜parts and selecting one part in each
phase. This clearly gives a more efï¬cient running time than training ğ‘˜separate models. Similarly,
we allow for a â€œwarm-upâ€ period prior to the feature selection phase, in which a small fraction
of the training epochs are allotted to training just the neural network weights. When we do this
one-pass implementation, we observe that it is important to reset the attention weights after each of
the sequential feature selection phases, but resetting the neural network weights is not crucial for
good performance.
Second, we note that when there is a large number of candidate features ğ‘‘, the softmax mask severely
scales down the gradient updates to the model weights, which can lead to inefï¬cient training. In these
cases, it becomes important to prevent this by either using a temperature parameter in the softmax to
counteract the small softmax values or by adjusting the learning rate to be high enough. Note that
these two approaches can be considered to be equivalent.
B.3
LARGE-SCALE EXPERIMENTS
In this section, we provide more details and discussion on our Criteo large dataset results. For this
experiment, we use a dense neural network with 768, 256, and 128 neurons in each of the three hidden
layers with ReLU activations. In Figure 4, the error bars are generated as the standard deviation over
running the algorithm three times with different random seeds, and the shadowed regions linearly
interpolate between these error bars. The values used to generate the plot are provided in Table 7 and
Table 8.
18

Published as a conference paper at ICLR 2023
We ï¬rst note that this dataset is so large that it is expensive to make multiple passes through the
dataset. Therefore, we modify the algorithms (both Sequential Attention and the other baselines) to
make only one pass through the data by using disjoint fractions of the data for different â€œstepsâ€ of the
algorithm. Hence, we select ğ‘˜features while only â€œtrainingâ€ one model.
Table 7: AUC of Criteo large experiments. SA is Sequential Attention, GL is generalized LASSO, and
SL is Sequential LASSO. The values in the header for the LASSO methods are the â„“1 regularization
strengths used for each method.
ğ‘˜
SA
CMIM
GL (ğœ†= 10âˆ’1)
GL (ğœ†= 10âˆ’4)
SL (ğœ†= 10âˆ’1)
SL (ğœ†= 10âˆ’4)
Liao et al. (2021)
5
0.67232 Â± 0.00015
0.63950 Â± 0.00076
0.68342 Â± 0.00585
0.50161 Â± 0.00227
0.60278 Â± 0.04473
0.67710 Â± 0.00873
0.58300 Â± 0.06360
10
0.70167 Â± 0.00060
0.69402 Â± 0.00052
0.71942 Â± 0.00059
0.64262 Â± 0.00187
0.62263 Â± 0.06097
0.70964 Â± 0.00385
0.68103 Â± 0.00137
15
0.72659 Â± 0.00036
0.72014 Â± 0.00067
0.72392 Â± 0.00027
0.65977 Â± 0.00125
0.66203 Â± 0.04319
0.72264 Â± 0.00213
0.69762 Â± 0.00654
20
0.72997 Â± 0.00066
0.72232 Â± 0.00103
0.72624 Â± 0.00330
0.72085 Â± 0.00106
0.70252 Â± 0.01985
0.72668 Â± 0.00307
0.71395 Â± 0.00467
25
0.73281 Â± 0.00030
0.72339 Â± 0.00042
0.73072 Â± 0.00193
0.73253 Â± 0.00091
0.71764 Â± 0.00987
0.73084 Â± 0.00070
0.72057 Â± 0.00444
30
0.73420 Â± 0.00046
0.72622 Â± 0.00049
0.73425 Â± 0.00081
0.73390 Â± 0.00026
0.72267 Â± 0.00663
0.72988 Â± 0.00434
0.72487 Â± 0.00223
35
0.73495 Â± 0.00040
0.73225 Â± 0.00024
0.73058 Â± 0.00350
0.73512 Â± 0.00058
0.73029 Â± 0.00509
0.73361 Â± 0.00037
0.73078 Â± 0.00102
Table 8: Log-loss of Criteo experiments. SA is Sequential Attention, GL is generalized LASSO, and
SL is Sequential LASSO. The values in the header for the LASSO methods are the â„“1 regularization
strengths used for each method.
ğ‘˜
SA
CMIM
GL (ğœ†= 10âˆ’1)
GL (ğœ†= 10âˆ’4)
SL (ğœ†= 10âˆ’1)
SL (ğœ†= 10âˆ’4)
Liao et al. (2021)
5
0.14123 Â± 0.00005
0.14323 Â± 0.00010
0.14036 Â± 0.00046
0.14519 Â± 0.00000
0.14375 Â± 0.00163
0.14073 Â± 0.00061
0.14415 Â± 0.00146
10
0.13883 Â± 0.00009
0.13965 Â± 0.00008
0.13747 Â± 0.00015
0.14339 Â± 0.00019
0.14263 Â± 0.00304
0.13826 Â± 0.00032
0.14082 Â± 0.00011
15
0.13671 Â± 0.00007
0.13745 Â± 0.00008
0.13693 Â± 0.00005
0.14227 Â± 0.00021
0.14166 Â± 0.00322
0.13713 Â± 0.00021
0.13947 Â± 0.00050
20
0.13633 Â± 0.00008
0.13726 Â± 0.00010
0.13693 Â± 0.00057
0.13718 Â± 0.00004
0.13891 Â± 0.00187
0.13672 Â± 0.00035
0.13806 Â± 0.00048
25
0.13613 Â± 0.00013
0.13718 Â± 0.00009
0.13648 Â± 0.00051
0.13604 Â± 0.00004
0.13760 Â± 0.00099
0.13628 Â± 0.00010
0.13756 Â± 0.00043
30
0.13596 Â± 0.00001
0.13685 Â± 0.00004
0.13593 Â± 0.00015
0.13594 Â± 0.00005
0.13751 Â± 0.00095
0.13670 Â± 0.00080
0.13697 Â± 0.00015
35
0.13585 Â± 0.00002
0.13617 Â± 0.00006
0.13666 Â± 0.00073
0.13580 Â± 0.00012
0.13661 Â± 0.00096
0.13603 Â± 0.00010
0.13635 Â± 0.00005
B.4
THE ROLE OF ADAPTIVITY
We show in this section the effect of varying adaptivity on the quality of selected features in Sequential
Attention. In the following experiments, we select 64 features on six datasets by selecting 2ğ‘–features
at a time over a ï¬xed number of epochs of training, for ğ‘–âˆˆ{0, 1, 2, 3, 4, 5, 6}. That is, we investigate
the following question: for a ï¬xed budget of training epochs, what is the best way to allocate the
training epochs over the rounds of the feature selection process? For most datasets, we ï¬nd that
feature selection quality decreases as we select more features at once. An exception is the mice
protein dataset, which exhibits the opposite trend, perhaps indicating that the features in the mice
protein dataset are less redundant than in other datasets. Our results are summarized in Table 8 and
Table 9. We also illustrate the effect of adaptivity for Sequential Attention on MNIST in Figure 9.
One observes that the selected pixels â€œclump togetherâ€ as ğ‘–increases, indicating a greater degree of
redundancy.
Our empirical results in this section suggest that adaptivity greatly enhances the quality of features
selected by Sequential Attention, and in feature selection algorithms more broadly.
0
2
4
6
Log Number of Features Selected at a Time
0.985
0.990
0.995
Prediction Accuracy
Mice Protein
0
2
4
6
Log Number of Features Selected at a Time
0.93
0.94
0.95
0.96
Prediction Accuracy
MNIST
0
2
4
6
Log Number of Features Selected at a Time
0.84
0.85
0.86
Prediction Accuracy
MNIST-Fashion
0
2
4
6
Log Number of Features Selected at a Time
0.80
0.85
0.90
Prediction Accuracy
ISOLET
0
2
4
6
Log Number of Features Selected at a Time
0.96
0.98
1.00
Prediction Accuracy
COIL-20
0
2
4
6
Log Number of Features Selected at a Time
0.90
0.92
0.94
Prediction Accuracy
Activity
Figure 8: Sequential Attention with varying levels of adaptivity. We select 64 features for each model,
and take 2ğ‘–features in each round for increasing values of ğ‘–. We plot accuracy as a function of ğ‘–.
19

Published as a conference paper at ICLR 2023
Table 9: Sequential Attention with varying levels of adaptivity. We select 64 features for each model,
and take 2ğ‘–features in each round for increasing values of ğ‘–. We give the accuracy as a function of ğ‘–.
Dataset
ğ‘–= 0
ğ‘–= 1
ğ‘–= 2
ğ‘–= 3
ğ‘–= 4
ğ‘–= 5
ğ‘–= 6
Mice Protein
0.990 Â± 0.006
0.990 Â± 0.008
0.989 Â± 0.006
0.989 Â± 0.006
0.991 Â± 0.005
0.992 Â± 0.006
0.990 Â± 0.007
MNIST
0.963 Â± 0.001
0.961 Â± 0.001
0.956 Â± 0.001
0.950 Â± 0.003
0.940 Â± 0.007
0.936 Â± 0.001
0.932 Â± 0.004
MNIST-Fashion
0.860 Â± 0.002
0.856 Â± 0.002
0.852 Â± 0.003
0.852 Â± 0.004
0.847 Â± 0.002
0.849 Â± 0.002
0.843 Â± 0.003
ISOLET
0.934 Â± 0.005
0.930 Â± 0.003
0.927 Â± 0.005
0.919 Â± 0.004
0.893 Â± 0.022
0.845 Â± 0.021
0.782 Â± 0.022
COIL-20
0.998 Â± 0.002
0.997 Â± 0.005
0.999 Â± 0.001
0.998 Â± 0.003
0.995 Â± 0.005
0.972 Â± 0.012
0.988 Â± 0.009
Activity
0.938 Â± 0.008
0.934 Â± 0.007
0.928 Â± 0.010
0.930 Â± 0.008
0.915 Â± 0.004
0.898 Â± 0.010
0.913 Â± 0.010
i = 0
i = 1
i = 2
i = 3
i = 4
i = 5
i = 6
Figure 9: Sequential Attention with varying levels of adaptivity on the MNIST dataset. We select 64
features for each model, and select 2ğ‘–features in each round for increasing values of ğ‘–.
B.5
VARIATIONS ON HADAMARD PRODUCT PARAMETERIZATION
We provide evaluations for different variations of the Hadamard product parameterization pattern as
described in Section 4.2. In Table 10, we provide the numerical values of the accuracies achieved.
Table 10: Accuracies of Sequential Attention for different Hadamard product parameterizations.
Dataset
Softmax
â„“1
â„“2
â„“1 normalized
â„“2 normalized
Mice Protein
0.990 Â± 0.006
0.993 Â± 0.010
0.993 Â± 0.010
0.994 Â± 0.006
0.988 Â± 0.008
MNIST
0.958 Â± 0.002
0.957 Â± 0.001
0.958 Â± 0.002
0.958 Â± 0.001
0.957 Â± 0.001
MNIST-Fashion
0.850 Â± 0.002
0.843 Â± 0.004
0.850 Â± 0.003
0.853 Â± 0.001
0.852 Â± 0.002
ISOLET
0.920 Â± 0.003
0.894 Â± 0.014
0.908 Â± 0.009
0.921 Â± 0.003
0.921 Â± 0.003
COIL-20
0.997 Â± 0.004
0.997 Â± 0.004
0.995 Â± 0.006
0.996 Â± 0.005
0.996 Â± 0.004
Activity
0.922 Â± 0.005
0.906 Â± 0.015
0.908 Â± 0.012
0.933 Â± 0.010
0.935 Â± 0.007
SM
L1
L2
L1N
L2N
0.970
0.975
0.980
0.985
0.990
0.995
1.000
1.005
Prediction Accuracy
Mice Protein
SM
L1
L2
L1N
L2N
0.954
0.956
0.958
0.960
Prediction Accuracy
MNIST
SM
L1
L2
L1N
L2N
0.835
0.840
0.845
0.850
0.855
Prediction Accuracy
MNIST-Fashion
SM
L1
L2
L1N
L2N
0.87
0.88
0.89
0.90
0.91
0.92
0.93
Prediction Accuracy
ISOLET
SM
L1
L2
L1N
L2N
0.990
0.992
0.994
0.996
0.998
1.000
1.002
Prediction Accuracy
COIL-20
SM
L1
L2
L1N
L2N
0.89
0.90
0.91
0.92
0.93
0.94
Prediction Accuracy
Activity
Figure 10: Accuracies of Sequential Attention for different Hadamard product parameterization
patterns. Here, SM = softmax, L1 = â„“1, L2 = â„“2, L1N = â„“1 normalized, and L2N = â„“2 normalized.
20

Published as a conference paper at ICLR 2023
B.6
APPROXIMATION OF MARGINAL GAINS
Finally, we present our experimental results that show the correlations between weights computed by
Sequential Attention and traditional feature selection marginal gains.
Deï¬nition B.1 (Marginal gains). Let â„“: 2[ğ‘‘] â†’R be a loss function deï¬ned on the ground set [ğ‘‘].
Then, for a set ğ‘†âŠ†[ğ‘›] and ğ‘–/âˆˆğ‘†, the marginal gain of ğ‘–with respect to ğ‘†is â„“(ğ‘†) âˆ’â„“(ğ‘†âˆª{ğ‘–}).
In the setting of feature selection, marginal gains are often considered for measuring the importance of
candidate features ğ‘–given a set ğ‘†of features that have already be selected by using the set function â„“,
which corresponds to the model loss when trained on a subset of features. It is known that greedily
selecting features based on their marginal gains performs well in both theory (Das & Kempe, 2011;
Elenberg et al., 2018) and practice (Das et al., 2022). These scores, however, can be extremely
expensive to compute since they require training a model for every feature considered.
In this experiment, we ï¬rst compute the top ğ‘˜features selected by Sequential Attention for ğ‘˜âˆˆ
{0, 9, 49} on the MNIST dataset. Then we compute (1) the true marginal gains and (2) the attention
weights according to Sequential Attention, conditioned on these features being in the model. The
Sequential Attention weights are computed by only applying the attention softmax mask over the
ğ‘‘âˆ’ğ‘˜unselected features, while the marginal gains are computed by explicitly training a model for
each candidate feature to be added to the preselected ğ‘˜features. Because our Sequential Attention
algorithm is motivated by an efï¬cient implementation of the greedy selection algorithm that uses
marginal gains (see Section 1), one might expect that these two sets of scores are correlated in some
sense. We show this by plotting the top scores according to the two sets of scores and by computing
the Spearman correlation coefï¬cient between the marginal gains and attention logits.
In the ï¬rst and second rows of Figure 11, we see that the top 50 pixels according to the marginal gains
and attention weights are visually similar, avoiding previously selected regions and ï¬nding new areas
which are now important. In the third row, we quantify their similarity via the Spearman correlation
between these feature rankings. While the correlations degrade as we select more features (which is
to be expected), the marginal gains become similar among the remaining features after removing the
most important features.
0
10
20
0
5
10
15
20
25
Top 50 Marginal Gain Pixels, 0 Selected
0
10
20
0
5
10
15
20
25
Top 50 Marginal Gain Pixels, 9 Selected
0
10
20
0
5
10
15
20
25
Top 50 Marginal Gain Pixels, 49 Selected
0
10
20
0
5
10
15
20
25
Top 50 Attention Logit Pixels, 0 Selected
0
10
20
0
5
10
15
20
25
Top 50 Attention Logit Pixels, 9 Selected
0
10
20
0
5
10
15
20
25
Top 50 Attention Logit Pixels, 49 Selected
0
200
400
600
800
Marginal Gains
0
200
400
600
800
Attention Weights
Spearman: 0.9078
0
200
400
600
800
Marginal Gains
0
200
400
600
800
Attention Weights
Spearman: 0.8268
0
200
400
600
Marginal Gains
0
200
400
600
Attention Weights
Spearman: 0.2877
Figure 11: Marginal gain experiments. The ï¬rst and second rows show that top 50 features chosen
using the true marginal gains (top) and Sequential Attention (middle). The bottom row shows the
Spearman correlation between these two computed sets of scores.
21

