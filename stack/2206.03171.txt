Introspective Experience Replay: Look Back When Surprised
Ramnath Kumar 1 Dheeraj Nagaraj 1
Abstract
In reinforcement learning (RL), experience replay-
based sampling techniques play a crucial role in
promoting convergence by eliminating spurious
correlations. However, widely used methods such
as uniform experience replay (UER) and priori-
tized experience replay (PER) have been shown
to have sub-optimal convergence and high seed
sensitivity respectively. To address these issues,
we propose a novel approach called Introspective
Experience Replay (IER) that selectively sam-
ples batches of data points prior to surprising
events. Our method builds upon the theoretically
sound reverse experience replay (RER) technique,
which has been shown to reduce bias in the output
of Q-learning-type algorithms with linear func-
tion approximation. However, this approach is
not always practical or reliable when using neu-
ral function approximation. Through empirical
evaluations, we demonstrate that IER with neural
function approximation yields reliable and supe-
rior performance compared to UER, PER, and
hindsight experience replay (HER) across most
tasks.
1. Introduction
Reinforcement learning (RL) involves learning with depen-
dent data derived from trajectories of Markov processes. In
this setting, the iterations of descent algorithms designed for
i.i.d. data co-evolve with the trajectory at hand, leading to
poor convergence. Experience replay (Lin, 1992) involves
storing the received data points in a large buffer and produc-
ing a random sample from this buffer whenever the learning
algorithm requires it. Therefore experience replay is usually
deployed with popular algorithms like DQN, DDPG, and
TD3 to achieve state-of-the-art performance (Mnih et al.,
2015; Lillicrap et al., 2015). It has been shown experimen-
tally (Mnih et al., 2015) and theoretically (Nagaraj et al.,
1Google AI Research Lab,Bengaluru, India 560016. Correspon-
dence to: Ramnath Kumar <ramnathk@google.com>, Dheeraj
Nagaraj <dheerajnagaraj@google.com>.
Preprint, Under Review.
2020) that these learning algorithms for Markovian data
show sub-par performance without experience replay.
The simplest and most widely used experience replay
method is the uniform experience replay (UER), where
the data points stored in the buffer are sampled uniformly
at random every time a data point is queried (Mnih et al.,
2015). However, UER might pick uninformative data points
most of the time, which may slow down the convergence.
For this reason, optimistic experience replay (OER) and
prioritized experience replay (PER) (Schaul et al., 2015)
were introduced, where samples with higher TD error (i.e.,
‘surprise’) are sampled more often from the buffer. Opti-
mistic experience replay (originally called “greedy TD-error
prioritization”) was shown to have a high bias1. This leads
to the algorithm predominantly selecting rare data points
and ignoring the rest. Prioritized experience replay was
proposed to solve this issue (Schaul et al., 2015) by using a
sophisticated sampling approach. However, as shown in our
experiments outside of the Atari environments, PER still
suffers from a similar problem, and its performance can
be highly sensitive to seed and hyper-parameter. Although
this speeds up the learning process in many cases, its per-
formance can be quite bad and erratic due to picking and
choosing only speciﬁc data points. The design of experience
replay continues to be an active ﬁeld of research. Several
other experience replay techniques like Hindsight experi-
ence replay (HER) (Andrychowicz et al., 2017), Reverse
Experience Replay (RER) (Rotinov, 2019), and Topological
Experience Replay (TER) (Hong et al., 2022) have been
proposed. An overview of these methods is discussed in
Section 2.
Even though these methods are widely deployed in practice,
theoretical analyses have been very limited. Recent results
on learning dynamical systems (Kowshik et al., 2021b;a)
showed rigorously in a theoretical setting that RER is
the conceptually-grounded algorithm when learning from
Markovian data. Furthermore, this work was extended to
the RL setting in (Agarwal et al., 2021) to achieve efﬁ-
cient Q learning with linear function approximation. The
RER technique achieves good performance since reverse
1We use the term ‘bias’ here as used in (Schaul et al., 2015),
which means biased with respect to the empirical distribution over
the replay buffer
arXiv:2206.03171v4  [cs.LG]  6 Feb 2023

Introspective Experience Replay
order sampling of the data points prevents the build-up of
spurious correlations in the learning algorithm. In this paper,
we build on this line of work and introduce Introspective
Experience Replay (IER). Roughly speaking, IER ﬁrst
picks top k ‘pivot’ points from a large buffer according to
their TD error. It then returns batches of data formed by se-
lecting the consecutive points temporally before these pivot
points. In essence, the algorithm looks back when surprised.
The intuition behind our approach is linked to the fact that
the agent should always associate outcomes to its past ac-
tions, just like in RER. The summary of our approach is
shown in Figure 1. This technique is an amalgamation of
Reverse Experience Replay (RER) and Optimistic Experi-
ence Replay (OER), which only picks the points with the
highest TD error.
Batch 1
Pivot 1
Batch 3
Pivot 3
Batch 2
Pivot 2
Pivot k
Batch k
Figure 1. An illustration of our proposed methodology when se-
lecting k batches in the CartPole environment. The red color is
used to indicate the batches being sampled from the replay buffer.
The green samples are the un-sampled states from the buffer. The
arrow explicitly points the pivots and the snapshot of the surprising
state encountered.
Our main ﬁndings are summarized below:
Better Performance Against SOTA: Our proposed
methodology (IER) outperforms previous state-of-the-art
baselines such as PER, UER and HER on most environ-
ments (see Table 1, Section 5).
Conceptual Understanding: We consider a simple toy ex-
ample where we understand the differences between UER,
RER, OER and IER (Section 3.2). This study illustrates
the better performance of our proposed method by show-
ing a) why naive importance sampling, like in OER suffers
from poor performance, often failing to learn any non-trivial
policy and b) why techniques like UER and RER are slow
to learn.
Forward vs. Reverse: We show that the temporal direction
(forward/reverse) to consider after picking the pivot is non-
trivial. We show empirically that IER performs much better
than its forward counterpart, IER (F) (see Table 3). This
gives evidence that causality plays a role in the success of
IER.
Whole vs. Component Parts: Our method (IER) is ob-
tained by combining two methods RER (which picks the
samples in the reverse order as received) and OER (which
greedily picks the samples with the largest TD error). We
show that neither of these components performs well com-
pared to their amalgamation, IER (Table 4).
Minimal hyperparameter tuning: Our proposed method-
ology uses minimal hyperparameter tuning. We use the
same policy network architecture, learning rate, batch size,
and all other parameters across all our runs for a given en-
vironment. These hyperparameters are selected based on
the setting required to achieve SOTA performance on UER.
However, we have few options available, a Hindsight ﬂag,
leading to the H-IER sampler, and the Uniform Mixing frac-
tion leading to the U-IER sampler. Furthermore, for most
of our experiments, we use a mixing fraction of 0.
Table 1. IER outperforms previous state-of-the-art baselines.
These baselines include samplers such as UER (Mnih et al., 2013),
PER (Schaul et al., 2015), and HER (Andrychowicz et al., 2017)
across many environments. Results are from 13 different envi-
ronments that cover a broad category of MDPs. These include a
few Atari environments (previously used to support the efﬁcacy of
UER, PER), some Robotics environments (previously used to sup-
port the efﬁcacy of HER), and many other classes of environments,
including Classic Control, Box 2D, and Mujoco. More details
about these experiments and their setup have been discussed in
Section 5.
Experience Replay Method
UER
PER
HER
IER
Best Performance Frequency
1
0
1
11
2. Related Works and Comparison
2.1. Experience Replay Techniques
Experience replay involves storing consecutive temporally
dependent data in a (large) buffer in a FIFO order. Whenever
a learning algorithm queries for batched data, the experi-
ence replay algorithm returns a sub-sample from this buffer
such that this data does not hinder the learning algorithms
due to spurious correlations. The most basic form of expe-
rience replay is UER (Lin, 1992) which samples the data
in the replay buffer uniformly at random. This approach
has signiﬁcantly improved the performance of off-policy
RL algorithms like DQN (Mnih et al., 2015). Several other
methods of sampling from the buffer have been proposed
since; PER (Schaul et al., 2015) samples experiences from a
probability distribution which assigns higher probability to
experiences with signiﬁcant TD error and is shown to boost
the convergence speed of the algorithm. This outperforms
UER in most Atari environments. HER (Andrychowicz
et al., 2017) works in the "what if" scenario, where even a
bad policy can lead the agent to learn what not to do and
nudge the agent towards the correct action. There have also
been other approaches such as (Liu et al., 2019; Fang et al.,

Introspective Experience Replay
2018; 2019) have adapted HER in order to improve the
overall performance with varying intuition. RER processes
the data obtained in a buffer in the reverse temporal order.
We refer to the following sub-section for a detailed review
of this and related techniques. We will also consider ‘opti-
mistic experience replay’ (OER), the naive version of PER,
where at each step, only top B elements in the buffer are
returned when batched data is queried. This approach can
become highly selective to the point of completely ignoring
certain data points, leading to poor performance. This is mit-
igated by a sophisticated sampling procedure employed in
PER. Other works such as (Fujimoto et al., 2020; Pan et al.,
2022) attempt to study PER and address some of its short-
comings, and (Lahire et al., 2021) introduces ’large batch
experience replay’ (LaBER), which reduces the stochastic
noise in gradients while keeping the updates unbiased with
respect to the empirical data distribution in the replay buffer
2.2. Reverse Sweep Techniques
Reverse sweep or backward value iteration refers to meth-
ods that process the data as received in reverse order. This
has been studied in the context of planning tabular MDPs
(Dai & Hansen, 2007; Grze´s & Hoey, 2013). We refer
to Section 4 for a brief overview of why these methods
are considered. However, this line of work assumes that
the MDP and the transition functions are known. Inspired
by the behavior of biological networks, (Rotinov, 2019)
proposed reverse experience replay where the experience
replay buffer is replayed in a LIFO order. Since RER forms
mini-batches with consecutive data points can be unreliable
with Neural approximation (i.e., it does not learn consis-
tently well across different environments). Therefore, the
iterations of namelo are ‘mixed’ with UER. However, the
experiments are limited and do not demonstrate that this
method outperforms even UER. A similar procedure named
Episodic Backward Update (EBU) is introduced in (Lee
et al., 2019). However, to ensure that the pure RER works
well, the EBU method also seeks to change the target for Q
learning instead of just changing the sampling scheme in the
replay buffer. The reverse sweep was independently redis-
covered as RER in the context of streaming linear system
identiﬁcation in (Kowshik et al., 2021b), where SGD with re-
verse experience replay was shown to achieve near-optimal
performance. In contrast, naive SGD was signiﬁcantly sub-
optimal due to the coupling between the Markovian data
and the SGD iterates. The follow-up work (Agarwal et al.,
2021) analyzed off-policy Q learning with linear function
approximation and reverse experience replay to provide
near-optimal convergence guarantees using the unique super
martingale structure endowed by reverse experience replay.
(Hong et al., 2022) considers topological experience replay,
which executes reverse replay over a directed graph of ob-
served transitions. Mixed with PER enables non-trivial
learning in some challenging environments. Another line of
work (Florensa et al., 2017; Moore & Atkeson, 1993; Goyal
et al., 2018; Schroecker et al., 2019) considers reverse sweep
with access to a simulator or using a ﬁtted generative model.
On the other hand, our work only seeks on-policy access to
the MDP.
3. Background and Proposed Methodology
We consider episodic reinforcement learning (Sutton &
Barto, 2018), where at each time step an agent takes ac-
tions at in an uncertain environment with state st, and re-
ceives a reward rt. The environment then evolves into a
new state st+1 whose law depends only on st, at. Our goal
is to (approximately) ﬁnd the policy π∗which maps the
environmental state s to an action a such that when the
agent takes the action at = π∗(st), the discounted reward
E [P∞
t=0 γtrt] is maximized. To achieve this, we consider
algorithms like DQN ((Mnih et al., 2015)), DDPG ((Lilli-
crap et al., 2015)) and TD3 ((Fujimoto et al., 2018)), which
routinely use experience replay buffers. In this paper, we
introduce a new experience replay method, IER, and inves-
tigate the performance of the aforementioned RL algorithms
with this modiﬁcation. In this work, when we say “return”,
we mean discounted episodic reward.
3.1. Methodology
We now describe our main method in a general way where
we assume that we have access to a data collection mecha-
nism T which samples new data points. This then appends
the sampled data points to a buffer H and discards some
older data points. The goal is to run an iterative learning
algorithm A, which learns from batched data of batch size
B in every iteration. We also consider an important metric
I associated with the problem. At each step, the data collec-
tion mechanism T collects a new episode and appends it to
the buffer H and discards some old data points, giving us
the new buffer as H ←T(H). We then sort the entries of
H based on the importance metric I and store the indices of
the top G data points in an array P = [P[0], . . . , P[G −1]].
Then for every index in P, we run the learning algorithm
A with the batch D = (H(P[i]), . . . , H(P[i] −B + 1)). In
some cases, we can ‘mix’2 this with the standard UER sam-
pling mechanism to reduce bias of the stochastic gradients
with respect to the empirical distribution in the buffer, as
shown below. Our experiments show that this amalgama-
tion helps convergence in certain cases. We describe this
procedure in Algorithm 1.
In the reinforcement learning setting, T runs an environ-
2Mixing here denotes sampling with a given probability from
one sampler A, and ﬁlling the remaining samples of a batch with
sampler B.

Introspective Experience Replay
Algorithm 1 Our proposed Introspective Experience Replay (IER) for Reinforcement Learning
Input: Data collection mechanism T, Data buffer H, Batch size B, grad steps per Epoch G, number of episodes N,
Importance function I, learning procedure A, Uniform Mixing fraction p
n ←0
while n < N do
n ←n + 1
H ←T(H)
;
// Add a new episode to the buffer
I ←I(H)
;
// Compute importance of each data point in the buffer
P ←Top(I; G)
;
// Obtain index of top G elements of I
g ←0
while g < G do
if g < (1 −p)G then
D ←H[P[g] −B, P[g]]
;
// Load batch of previous B examples from pivot P[g]
else
D ←H[Uniform(H, B)]
;
// Randomly chose B indices from buffer
end
g ←g + 1
A(D)
;
// Run the learning algorithm with batch data D
end
end
ment episode with the current policy and appends the tran-
sitions and corresponding rewards to the buffer H in the
FIFO order, maintaining a total of 1E6 data points, usually.
We choose A to be an RL algorithm like TD3 or DQN or
DDPG. The importance function I is the magnitude of the
TD error with respect to the current Q-value estimate pro-
vided by the algorithm A (i.e., I = |Q(s, a) −R(s, a) −
γ supa′ Qtarget(s′, a′)|). When the data collection mecha-
nism (T) is the same as in UER, we will call this method
IER. In optimistic experience replay (OER), we take T to
be the same as in UER. However, we query top BG data
points from the buffer H and return G disjoint batches each
of size B from these ‘important’ points. It is clear that
IER is a combination of OER and RER. Notice that we
can also consider the data collection mechanism like that
of HER, where examples are labeled with different goals,
i.e. T has now been made different, keeping the sampling
process exactly same as before. In this case, we will call our
algorithm H-IER. Our experiment in Enduro and Acrobat
depicts an example of this successful coalition. We also
consider the RER method, which served as a motivation for
our proposed approach. Under this sampling methodology,
the batches are drawn from H in the temporally reverse
direction. This approach is explored in the works mentioned
in Section 2.2. We discuss this methodology in more detail
in Appendix F.
3.2. Didactic Toy Example
In this section, we discuss the working of IER on a simple
environment such as GridWorld-1D, and compare this with
some of our baselines such as UER, OER, RER, and IER
(F). In this environment, the agent lives on a discrete 1-
dimensional grid of size 40 with a max-timestep of 1000
steps, and at each time step, the agent can either move left
or right by one step. The agent starts from the starting
state (S; [6]), the goal of the agent is to reach goal state
(G; [40]) getting a reward of +1, and there is also a trap
state (T; [3]), where the agents gets a reward of −2. The
reward in every other state is 0. For simplicity, we execute
an ofﬂine exploratory policy where the agent moves left
or right with a probability of half and obtain a buffer of
size 30000. The rewarding states occur very rarely in the
buffer since it is hard to reach for this exploration policy.
The episode ends upon meeting either of two conditions: (i)
the agent reaches the terminal state, which is the goal state,
or (ii) the agent has exhausted the max-timestep condition
and has not succeeded in reaching any terminal state. An
overview of our toy environment is depicted in Figure 2(a).
Other hyperparameters crucial to replicating this experiment
are described in Appendix B.
In this example, reaching the goal state as quickly as pos-
sible is vital to receive a positive reward and avoid the fail
state. Therefore, it is essential to understand the paths which
reach the goal state. Figure 2(b) depicts the number of times
each state occurs in the buffer. Furthermore, the remaining
subplots of Figure 2 depict the Absolute Frequency of our
off-policy algorithm trained in this environment. A state’s
“absolute frequency” is the number of times the replay tech-
nique samples a given state during the algorithm’s run. The
experiments on this simple didactic toy environment do
highlight a few interesting properties:
Comparison of UER and IER: Since the goal state ap-

Introspective Experience Replay
pears very rarely in buffer, UER and RER rarely sample
the goal state and hence do not manage to learn effectively.
While RER naturally propagates the information about the
reward back in time to the states that led to the reward, it
does not often sample the rewarding state.
Limitation of OER: While OER samples a lot from the
states close to the goal state, the information about the re-
ward does not propagate to the start state. We refer to the
bottleneck in Figure 2(e) where some intermediate states
are not sampled.
Advantage of IER: IER prioritizes sampling from the goal
state and propagates the reward backward so that the en-
tire path leading to the reward is now aware of how to
reach the reward. Therefore, a combination of RER and
OER reduces the sampling bias in OER by preventing the
bottlenecks seen in Figure 2(e).
Bottleneck of IER (F): IER (F) has a more signiﬁcant bot-
tleneck when compared to RER and chooses to sample the
non-rewarding middle states most often. Also, note that
whenever IER (F) chooses the goal state as the pivot, it se-
lects the rest of the batch to overﬂow into the next episode,
which begins at the starting state. This does not allow the
algorithm to effectively learn the path which led to the goal
state.
The toy example above models several salient features in
more complicated RL environments.
(i) In the initial stages of learning, the exploratory policy
is essentially random, and such a naive exploratory policy
does not often lead to non-trivial rewards.
(ii) Large positive and negative reward states (the goal and
trap states), and their neighbors provide the pivot state for
IER and OER.
We show empirically that this holds in more complicated en-
vironments as well. Figure 3 depicts the surprise vs. reward
for the Ant environment. Here we see a strong correlation
between absolute reward and TD error (“Surprise factor”).
4. Understanding Reverse Replay
There are various conceptual ways we can look at RER and
IER. This section outlines some of the motivations behind
using this technique. Theoretical works such as (Agarwal
et al., 2021; Kowshik et al., 2021b) have established rigorous
theoretical guarantees for RER by utilizing super martingale
structures. This structure is not present in forward replay
techniques (i.e., the opposite of reverse replay) as shown in
(Kowshik et al., 2021a). We refer to Appendix D, where
we show via an ablation study that going forward in time
instead of reverse does not work very well. We give the
following explanations for the success of IER.
Propogation of Sparse Rewards: In many RL problems,
non-trivial rewards are sparse and only received at the goal
states. Therefore, processing the data backward in time
from such goal states helps the algorithm learn about the
states that led to this non-trivial reward. Our study (see
Figure 3 and Appendix E for further details) shows that in
many environments IER picks pivots which are the states
with large (positive or negative) rewards, enabling effective
learning.
More spread than OER: OER, which greedily chooses
the examples with the largest TD error to learn from, per-
forms very poorly since it is overtly selective to the point of
ignoring most of the states. To illustrate this phenomenon,
we refer to the didactic example in Section 3.2. One possible
way of viewing IER is that RER is used to reduce this afﬁn-
ity to picking a minimal subset of states in OER. PER is
designed to achieve a similar outcome with a sophisticated
and computationally expensive sampling scheme over the
buffer.
Causality: MDPs have a natural causal structure: actions
and events in the past inﬂuence the events in the future.
Therefore, whenever we see a surprising or unexpected
event, we can understand why or how it happened by look-
ing into the past. Further theoretical work is needed to
realize IER better.
We further illustrate the same by referring to the straightfor-
ward didactic example (Section 3.2), where we can see the
effects of each of the experience replay methods. We also
demonstrate superior performance on more complicated en-
vironments (Section 5), showcasing the robustness of our
approach with minimal hyperparameter tuning.
5. Experimental Results
In this section, we brieﬂy discuss our experimental setup as
well as the results of our experiments.
Environments:
We evaluate our approach on a diverse
class of environments, such as (i) Environments with low-
dimensional state space (including classic control and Box-
2D environments), (ii) Multiple joint dynamic simulations
and robotics environments (including Mujoco and Robotics
environments), and (iii) Human-challenging environments
(such as Atari environments). Note that previous semi-
nal papers in the ﬁeld of experience replay, such as (Mnih
et al., 2013), (Schaul et al., 2015), and (Andrychowicz et al.,
2017), showed the efﬁcacy of their approach on a subset of
these classes. For instance, UERand PER was shown to
work well on Atari games. Furthermore, HER was effec-
tive in the Robotics environments such as FetchReach. In
this work, we perform a more extensive study to show the
robustness and effectiveness of our model not just in Atari
and Robotics environments but also in Mujoco, Box2D, and
Classic Control environments. Due to computational limi-
tations and the non-reproducibility of some baselines, we

Introspective Experience Replay
State:
Trap (T)
Start (S)
Goal (G)
1
2
3
4
5
6
7
8
9
....
37
38
39
40
(a) GridWorld-1D environment
T
S
G
0
250
500
750
1000
1250
1500
1750
2000
State Distribution in Buffer
(b) Buffer
T S
G
0
100
200
300
400
Absolute Frequency
(c) UER
T S
G
0
50
100
150
200
250
300
350
400
Absolute Frequency
(d) RER
T S
G
0
100
200
300
400
Absolute Frequency
(e) OER
T S
G
0
100
200
300
400
Absolute Frequency
(f) IER
T S
G
0
100
200
300
400
Absolute Frequency
(g) IER (F)
Figure 2. Gridworld-1D environment is depicted in Figure 2(a). Distribution of states in the buffer (Figure 2(b)) and relative frequency of
different experience replay samplers on the didactic example of GridWorld-1D environment (Figure 2(c);2(d);2(e);2(f);2(g)).
could not extend our experiments to some Atari environ-
ments. We refer to Appendix A for a brief description of the
environments used.
Hyperparameters:
Refer to Appendix B for the exact hy-
perparameters used. Across all our experiments on various
environments, we use a standard setting for all the different
experience replay buffers. This classic setting is set so we
can reproduce state-of-the-art performance using UER on
the respective environment. For most of our experiments,
we set the uniform mixing fraction (p) from Algorithm 1
to be 0. We use a non-zero p value only for a few environ-
ments to avoid becoming overtly selective while training,
as described in Appendix B. For PER, we tune the α and
β hyperparameters used in the (Schaul et al., 2015) paper
across all environments other than Atari. The default values
of α = 0.4 and β = 0.6 are robust on Atari environments
as shown by extensive hyperparameter search by (Schaul
et al., 2015). We detail the results from our grid search in
Appendix C.2.
Metric:
To compare the different models, we use the Top-
K seeds moving average return as the evaluation metric
across all our runs. Top-K seeds here mean we take the aver-
age of k = 3 seeds that gave the best performance out of a
total of n = 5 trials. This measures the average performance
of the top k/n quantile of the trajectories. This factors in the
seed sensitivity and is robust despite the highly stochastic
learning curves encountered in RL, where there is a non-
zero probability that non-trivial policies are not learned after
many steps. It is common to use top-1 and Top-K trials to
be selected from among many trials in the reinforcement
learning literature (see (Schaul et al., 2015);(Sarmad et al.,
2019);(Wu et al., 2017);(Mnih et al., 2016)). Furthermore,
we motivate our design choice of using the Top-K metric as
our metric of choice from our study in Appendix C.1. We
show that the Top-K metric is more stable and suitable for
comparing the performances of different RL agents, which
generally suffer from highly volatile learning curves, as de-
scribed above. Moving average with a given window size
is taken for learning curves (with a window size of 20 for
FetchReach and 50 for all others) to reduce the variation in
return which is inherently present in each epoch. We argue
that taking a moving average is essential since, usually, pure
noise can be leveraged to pick a time instant where a given
method performs best (Henderson et al., 2018). Considering
the Top-K seed averaging of the last step performance of
the moving average of the learning curves gives our metric -
the Top-K seeds moving average return.
Comparison with SOTA:
Table 2 depicts our results in
various environments upon using different SOTA replay
sampler mechanisms (UER, PER and HER). Our proposed
sampler outperforms all other baselines in most tasks and
compares favorably in others. Our experiments on various
environments across various classes, such as classic control,

Introspective Experience Replay
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Normalized TD Error
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Normalized Reward
(a) UER
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Normalized TD Error
0.0
0.5
1.0
1.5
2.0
2.5
Normalized Reward
(b) RER
0
1
2
3
4
TD Error
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Reward
(c) IER
Figure 3. Relationship between absolute values of TD Error (Surprise factor) and Reward for the Ant environment.
and Atari, amongst many others, show that our proposed
methodology consistently outperforms all other baselines in
most environments, as summarized in Table 1. Furthermore,
our proposed methodology is robust across various environ-
ments, as highlighted in Table 2. The learning curves for
our experiments have been depicted in the Appendix. (see
Appendix C)
Table 2. Top-K seeds Moving Average Return results across various
environments. From our experiments, we note that IER outper-
forms previous SOTA baselines in most environments. Appendix B
depicts the hyperparameters used for the experiment.
Dataset
UER
PER
HER
IER
CartPole
153.14 ± 32.82
198.06 ± 3.68
173.84 ± 26.11
199.83 ± 0.31
Acrobot
-257.93 ± 184.28
-233.45 ± 127.99
-389.42 ± 113.22
-193.90 ± 57.56
Inverted Pendulum
-161.93 ± 10.55
-155.05 ± 9.95
-629.42 ± 815.24
-150.27 ± 9.63
LunarLander
-4.42 ± 20.06
11.21 ± 15.51
6.00 ± 10.02
12.32 ± 27.55
HalfCheetah
10808.93 ± 1094.32
99.75 ± 1124.46
11072.54 ± 297.12
10544.88 ± 342.01
Ant
3932.85 ± 1024.86
-2699.84 ± 1.34
3803.17 ± 996.81
4203.21 ± 345.22
Reacher
-4.97 ± 0.31
-5.42 ± 0.61
-5.30 ± 0.50
-4.92 ± 0.27
Walker
3597.03 ± 1203.79
1709.48 ± 1635.90
889.82 ± 1427.92
4349.29 ± 680.35
Hopper
3072.65 ± 621.40
784.00 ± 1536.00
2685.72 ± 1221.81
3205.05 ± 406.35
Inverted Double Pendulum
8489.54 ± 927.69
8654.78 ± 981.67
9002.05 ± 464.20
9241.61 ± 91.68
Fetch-Reach
-1.84 ± 0.57
-49.90 ± 0.10
-2.92 ± 1.79
-1.74 ± 0.24
Pong
19.15 ± 1.32
17.02 ± 3.27
18.70 ± 1.02
19.10 ± 1.20
Enduro
227.01 ± 319.15
565.23 ± 116.36
514.32 ± 132.39
586.32 ± 111.44
Forward vs.
Reverse:
The intuitive limitation to the
"looking forward" approach is that in many RL problems,
the objective for the agent is to reach a ﬁnal goal state, where
the non-trivial reward is obtained. Since non-trivial rewards
are only offered in this goal state, it is informative to look
back from here to learn about the states that lead to this.
When the goals are sparse, the TD error is more likely to
be large upon reaching the goal state. Our algorithm selects
these as pivots, and IER (F) might select batches overﬂow-
ing into the next episode. Our studies on many environments
(see Figure 3 and Appendix E) show that the pivot points
selected based on importance indeed have large (positive
or negative) rewards. Our experiments depicted in Table 3
show that IER outperforms IER (F) in most environments.
Table 3. Top-K seeds Moving Average Return results across various
environments for Temporal Ablation study between IER (F) and
IER (R; default). Note that we use the base setting of IER in this
section to avoid spurious comparisons (i.e., with p = 0 and no
hindsight).
Dataset
Forward
Reverse
CartPole
196.51 ± 6.26
199.83 ± 0.31
Acrobot
-423.15 ± 108.08
-313.03 ± 190.27
Inverted Pendulum
-882.13 ± 521.77
-1111.51 ± 559.83
LunarLander
-22.75 ± 30.19
12.32 ± 27.55
HalfCheetah
9369.30 ± 454.40
10108.15 ± 919.27
Ant
2963.71 ± 828.50
4203.21 ± 345.22
Reacher
-5.25 ± 0.33
-4.92 ± 0.27
Walker
2213.89 ± 1684.03
2830.03 ± 881.51
Hopper
393.64 ± 181.89
505.58 ± 266.52
Inverted Double Pendulum
9260.27 ± 95.59
9241.61 ± 91.68
FetchReach
-17.73 ± 27.91
-2.28 ± 1.11
Pong
17.92 ± 2.60
19.10 ± 1.20
Enduro
600.87 ± 149.99
525.84 ± 146.39
Whole vs. Component Parts:
Our approach is an amal-
gamation of OER and RER. Here we compare these indi-
vidual parts with IER. Table 4 describes the Top-K seeds
Moving Average Return across various environments in this
domain. As demonstrated, IER outperforms its component
parts OER nor RER . Furthermore, we also motivate each
of our design choices such as the pivot point selection (see
Appendix C.8 where we compare our proposed approach
with it’s variant where the pivot points are randomly se-
lected), and temporal structure (see Appendix C.9 where we
compare our proposed approach with it’s variant where the
points are randomly sampled instead of temporally looking
backward after selecting a pivot point, and also buffer batch
size sensitivity (see Appendix C.7).

Introspective Experience Replay
Table 4. Top-K seeds Moving Average Return results across various
environments for ablation study between RER, OER, and IER.
Dataset
RER
OER
IER
CartPole
163.93 ± 40.03
162.36 ± 34.89
199.83 ± 0.31
Acrobot
-320.70 ± 144.05
-472.63 ± 31.21
-193.90 ± 57.56
Inverted Pendulum
-735.38 ± 613.5
-166.57 ± 17.23
-150.27 ± 9.63
LunarLander
9.84 ± 13.23
-16.08 ± 15.38
12.32 ± 27.55
HalfCheetah
9449.39 ± 648.60
2237.91 ± 2824.72
10544.88 ± 342.01
Ant
2168.47 ± 415.53
-47.93 ± 20.47
4203.21 ± 345.22
Reacher
-5.91 ± 0.37
-5.28 ± 0.58
-4.92 ± 0.27
Walker
1578.33 ± 1313.11
207.51 ± 193.06
4349.29 ± 680.35
Hopper
206.23 ± 318.49
660.24 ± 580.77
3205.05 ± 406.35
Inverted Double Pendulum
8953.44 ± 456.95
7724.99 ± 1726.58
9241.61 ± 91.68
Fetch-Reach
-49.94 ± 0.07
-47.72 ± 3.33
-1.74 ± 0.24
Pong
18.58 ± 1.75
3.52 ± 21.33
19.10 ± 1.20
Enduro
483.98 ± 75.45
361.21 ± 86.30
586.32 ± 111.44
6. Discussion
We summarize our results and discuss possible future steps.
Speedup:
IER shows a signiﬁcant speedup in terms of
time complexity over PER as depicted in Table 5. On
average IER achieves a speedup improvement of 26.20%
over PER across a large umbrella of environment classes.As
the network becomes more extensive or complicated, our
approach does have a higher overhead (especially computing
TD error). Future work can investigate how to further reduce
the computational complexity of our method by computing
the TD error fewer times at the cost of operating with an
older TD error. We also notice a speedup of convergence
toward a local-optimal policy of our proposed approach,
as shown in a few environments. Furthermore, the lack of
speedup in some of the other experiments (even if they offer
an overall performance improvement) could be since the
"surprised" pivot cannot be successfully utilized to teach the
agent rapidly in the initial stages. We refer to Appendix C
for the learning curves.
Table 5. Average Speedup in terms of time complexity over
PER across various environment classes.
Environment
Average Speedup
Classic Control
32.66% ↑
Box-2D
54.32% ↑
Mujoco
18.09% ↓
Robotics
55.56% ↑
Atari
6.53% ↑
Issues with stability and consistency
Picking pivot
points by looking at the TD error might cause us to sam-
ple rare events much more often and hence cause instabil-
ity compared to UER as seen in some environments like
HalfCheetah, LunarLander, and Ant, where there is a sudden
drop in performance for some episodes (see Appendix C).
We observe that our strategy IER corrects itself quickly, un-
like RER, which cannot do this (see Figure 7(e)). Increasing
the number of pivot points per episode (the parameter G)
and the uniform mixing probability p usually mitigates this.
In this work, we do not focus on studying the exact effects
of p and G since our objective was to obtain methods that
require minimal hyper-parameter tuning. However, future
work can systematically investigate the signiﬁcance of these
parameters in various environments.
Why does IER outperform the traditional RER?
The
instability3 and unreliability of pure RER with neural ap-
proximation has been noted in various works (Rotinov,
2019; Lee et al., 2019), where RER is stabilized by mixing
it with UER . (Hong et al., 2022) stabilizes reverse sweep
by mixing it with PER. This is an interesting phenomenon
since RER is near-optimal in the tabular and linear approx-
imation settings (Agarwal et al., 2021). Two explanations
of this are i) The loss function used to train the neural net-
work is highly non-convex, which hinders the working of
RER and ii) The proof given in (Agarwal et al., 2021) relies
extensively on ‘coverage’ of the entire state-action space -
that is, the entire state-action space is visited enough num-
ber of times - which might not hold, as shown in the toy
example in Section 3.2.
7. Conclusion
In conclusion, our proposed approach, Introspective Expe-
rience Replay (IER), has shown signiﬁcant promise as a
solution for improving the convergence and robustness of
reinforcement learning algorithms. We have demonstrated
through extensive experiments that IER outperforms state-
of-the-art techniques such as uniform experience replay
(UER), prioritized experience replay (PER), and hindsight
experience replay (HER) on a wide range of tasks, and also
shows a signiﬁcant average speedup improvement in terms
of time complexity over PER. One of the key strengths of
our proposed approach is its ability to selectively sample
batches of data points prior to surprising events. This allows
for the removal of bias and spurious correlations that can
impede the convergence of RL algorithms. Additionally,
our method is based on the theoretically rigorous reverse
experience replay (RER) technique, which adds a further
level of rigor to our approach.
Overall, our proposed approach, Introspective Experience
Replay (IER), is a promising solution that offers signiﬁcant
improvements over existing methods and has the potential to
advance the ﬁeld of reinforcement learning. We believe that
our proposed approach could be widely adopted in various
RL applications and have a positive impact on the ﬁeld of
AI.
3i.e., sudden drop in performance without recovery like catas-
trophic forgetting

Introspective Experience Replay
Reproducibility Statement
In this paper, we work with thirteen datasets, all of
which are open-sourced in gym (https://github.
com/openai/gym). More information about the envi-
ronments is available in Appendix A. We predominantly
use DQN, DDPG and TD3 algorithms in our research, both
of which have been adapted from their open-source code.
We also experimented with seven different replay buffer
methodologies, all of which have been adapted from their
source code4. More details about the models and hyperpa-
rameters are described in Appendix B. All runs have been
run using the A100-SXM4-40GB, TITAN RTX, and V100
GPUs. Our source code is made available for additional
reference 5.
References
Agarwal, N., Chaudhuri, S., Jain, P., Nagaraj, D., and Netra-
palli, P. Online target q-learning with reverse experience
replay: Efﬁciently ﬁnding the optimal policy for linear
mdps. arXiv preprint arXiv:2110.08440, 2021.
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,
R., Welinder, P., McGrew, B., Tobin, J., Pieter Abbeel, O.,
and Zaremba, W. Hindsight experience replay. Advances
in neural information processing systems, 30, 2017.
Barto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike
adaptive elements that can solve difﬁcult learning con-
trol problems. IEEE transactions on systems, man, and
cybernetics, (5):834–846, 1983.
Brockman, G., Cheung, V., Pettersson, L., Schneider, J.,
Schulman, J., Tang, J., and Zaremba, W. Openai gym.
arXiv preprint arXiv:1606.01540, 2016.
Dai, P. and Hansen, E. A. Prioritizing bellman backups
without a priority queue. In ICAPS, pp. 113–119, 2007.
Fang, M., Zhou, C., Shi, B., Gong, B., Xu, J., and Zhang, T.
Dher: Hindsight experience replay for dynamic goals. In
International Conference on Learning Representations,
2018.
Fang, M., Zhou, T., Du, Y., Han, L., and Zhang, Z.
Curriculum-guided hindsight experience replay.
Ad-
vances in neural information processing systems, 32,
2019.
Florensa, C., Held, D., Wulfmeier, M., Zhang, M., and
Abbeel, P. Reverse curriculum generation for reinforce-
ment learning. In Conference on robot learning, pp. 482–
495. PMLR, 2017.
4https://github.com/rlworkgroup/garage
5https://github.com/google-research/
look-back-when-surprised
Fujimoto, S., Hoof, H., and Meger, D. Addressing function
approximation error in actor-critic methods. In Interna-
tional conference on machine learning, pp. 1587–1596.
PMLR, 2018.
Fujimoto, S., Meger, D., and Precup, D. An equivalence be-
tween loss functions and non-uniform sampling in expe-
rience replay. Advances in neural information processing
systems, 33:14219–14230, 2020.
Goyal, A., Brakel, P., Fedus, W., Singhal, S., Lillicrap, T.,
Levine, S., Larochelle, H., and Bengio, Y. Recall traces:
Backtracking models for efﬁcient reinforcement learning.
arXiv preprint arXiv:1804.00379, 2018.
Grze´s, M. and Hoey, J. On the convergence of techniques
that improve value iteration. In The 2013 International
Joint Conference on Neural Networks (IJCNN), pp. 1–8.
IEEE, 2013.
Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup,
D., and Meger, D. Deep reinforcement learning that mat-
ters. In Proceedings of the AAAI conference on artiﬁcial
intelligence, volume 32, 2018.
Hong, Z.-W., Chen, T., Lin, Y.-C., Pajarinen, J., and
Agrawal, P. Topological experience replay. arXiv preprint
arXiv:2203.15845, 2022.
Kowshik, S., Nagaraj, D., Jain, P., and Netrapalli, P. Near-
optimal ofﬂine and streaming algorithms for learning
non-linear dynamical systems. Advances in Neural Infor-
mation Processing Systems, 34, 2021a.
Kowshik, S., Nagaraj, D., Jain, P., and Netrapalli, P. Stream-
ing linear system identiﬁcation with reverse experience
replay. Advances in Neural Information Processing Sys-
tems, 34, 2021b.
Lahire, T., Geist, M., and Rachelson, E. Large batch experi-
ence replay. arXiv preprint arXiv:2110.01528, 2021.
Lee, S. Y., Sungik, C., and Chung, S.-Y. Sample-efﬁcient
deep reinforcement learning via episodic backward up-
date. Advances in Neural Information Processing Sys-
tems, 32, 2019.
Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,
T., Tassa, Y., Silver, D., and Wierstra, D. Continuous
control with deep reinforcement learning. arXiv preprint
arXiv:1509.02971, 2015.
Lin, L.-J. Self-improving reactive agents based on reinforce-
ment learning, planning and teaching. Machine learning,
8(3):293–321, 1992.
Liu, H., Trott, A., Socher, R., and Xiong, C. Competi-
tive experience replay. arXiv preprint arXiv:1902.00528,
2019.

Introspective Experience Replay
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,
Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing
atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-
land, A. K., Ostrovski, G., et al. Human-level control
through deep reinforcement learning. nature, 518(7540):
529–533, 2015.
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-
chronous methods for deep reinforcement learning. In
International conference on machine learning, pp. 1928–
1937. PMLR, 2016.
Moore, A. W. and Atkeson, C. G. Prioritized sweeping:
Reinforcement learning with less data and less time. Ma-
chine learning, 13(1):103–130, 1993.
Nagaraj, D., Wu, X., Bresler, G., Jain, P., and Netrapalli, P.
Least squares regression with markovian data: Fundamen-
tal limits and algorithms. Advances in neural information
processing systems, 33:16666–16676, 2020.
Pan, Y., Mei, J., Farahmand, A.-m., White, M., Yao, H.,
Rohani, M., and Luo, J. Understanding and mitigating the
limitations of prioritized replay. In The 38th Conference
on Uncertainty in Artiﬁcial Intelligence, 2022.
Plappert, M., Andrychowicz, M., Ray, A., McGrew, B.,
Baker, B., Powell, G., Schneider, J., Tobin, J., Chociej,
M., Welinder, P., et al. Multi-goal reinforcement learn-
ing: Challenging robotics environments and request for
research. arXiv preprint arXiv:1802.09464, 2018.
Rotinov, E. Reverse experience replay. arXiv preprint
arXiv:1910.08780, 2019.
Sarmad, M., Lee, H. J., and Kim, Y. M. Rl-gan-net: A
reinforcement learning agent controlled gan network for
real-time point cloud shape completion. In Proceedings
of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 5898–5907, 2019.
Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-
tized experience replay. arXiv preprint arXiv:1511.05952,
2015.
Schroecker, Y., Vecerik, M., and Scholz, J. Generative pre-
decessor models for sample-efﬁcient imitation learning.
arXiv preprint arXiv:1904.01139, 2019.
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel,
P. High-dimensional continuous control using generalized
advantage estimation. arXiv preprint arXiv:1506.02438,
2015.
Sutton, R. S. Generalization in reinforcement learning: Suc-
cessful examples using sparse coarse coding. Advances
in neural information processing systems, 8, 1995.
Sutton, R. S. and Barto, A. G. Reinforcement learning: An
introduction. MIT press, 2018.
Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics
engine for model-based control. In 2012 IEEE/RSJ inter-
national conference on intelligent robots and systems, pp.
5026–5033. IEEE, 2012.
Wawrzy´nski, P. A cat-like robot real-time learning to run.
In International Conference on Adaptive and Natural
Computing Algorithms, pp. 380–390. Springer, 2009.
Wu, Y., Mansimov, E., Grosse, R. B., Liao, S., and Ba,
J. Scalable trust-region method for deep reinforcement
learning using kronecker-factored approximation. Ad-
vances in neural information processing systems, 30,
2017.

Introspective Experience Replay
A. Environments
For all OpenAI environments, data is summarized from https://github.com/openai/gym, and more information
is provided in the wiki https://github.com/openai/gym/wiki. Below we brieﬂy describe some of the tasks we
experimented on in this paper.
A.1. CartPole-v0
CartPole, as introduced in (Barto et al., 1983), is a task of balancing a pole on top of the cart. The cart has access to position
and velocity as its state vector. Furthermore, it can go either left or right for each action. The task is over when the agent
achieves 200 timesteps without a positive reward (balancing the pole) which is the goal state or has failed, either when (i)
the cart goes out of boundaries (± 2.4 units off the center), or (ii) the pole falls over (less than ± 12 deg). The agent is given
a continuous 4-dimensional space describing the environment and can respond by returning one of two values, pushing the
cart either right or left.
A.2. Acrobot-v1
Acrobot, as introduced in (Sutton, 1995), is a task where the agent is given rewards for swinging a double-jointed pendulum
up from a stationary position. The agent can actuate the second joint by one of three actions: left, right, or no torque. The
agent is given a six-dimensional vector comprising the environment’s angles and velocities. The episode terminates when
the end of the second pole is over the base. Each timestep that the agent does not reach this state gives a -1 reward, and the
episode length is 500 timesteps.
A.3. Pendulum-v0
The inverted pendulum swingup problem, as introduced in (Lillicrap et al., 2015) is based on the classic problem in control
theory. The system consists of a pendulum attached ﬁxed at one end, and free at the other end. The pendulum starts in a
random position and the goal is to apply torque on the free end to swing it into an upright position, with its center of gravity
right above the ﬁxed point. The episode length is 200 timesteps, and the maximum reward possible is 0, when no torque is
being applied, and the object has 0 velocity remaining at an upright conﬁguration.
A.4. LunarLander-v2
The LunarLander environment introduced in (Brockman et al., 2016) is a classic rocket trajectory optimization problem.
The environment has four discrete actions - do nothing, ﬁre the left orientation engine, ﬁre the right orientation engine, and
ﬁre the main engine. This scenario is per Pontryagin’s maximum principle, as it is optimal to ﬁre the engine at full throttle
or turn it off. The landing coordinates (goal) is always at (0, 0). The coordinates are the ﬁrst two numbers in the state vector.
There are a total of 8 features in the state vector. The episode terminates if (i) the lander crashes, (ii) the lander gets outside
the window, or (iii) the lander does not move nor collide with any other body.
A.5. HalfCheetah-v2
HalfCheetah is an environment based on the work by (Wawrzy´nski, 2009) adapted by (Todorov et al., 2012). The HalfCheetah
is a 2-dimensional robot with nine links and eight joints connecting them (including two paws). The goal is to apply torque
on the joints to make the cheetah run forward (right) as fast as possible, with a positive reward allocated based on the
distance moved forward and a negative reward is given for moving backward. The torso and head of the cheetah are ﬁxed,
and the torque can only be applied to the other six joints over the front and back thighs (connecting to the torso), shins
(connecting to the thighs), and feet (connecting to the shins). The reward obtained by the agent is calculated as follows:
rt = ˙xt −0.1 ∗∥at∥2
2
A.6. Ant-v2
Ant is an environment based on the work by (Schulman et al., 2015) and adapted by (Todorov et al., 2012). The ant is a 3D
robot with one torso, a free rotational body, and four legs. The task is to coordinate the four legs to move in the forward
direction by applying torques on the eight hinges connecting the two links of each leg and the torso. Observations consist of

Introspective Experience Replay
positional values of different body parts of the ant, followed by the velocities of those individual parts (their derivatives),
with all the positions ordered before all the velocities. The reward obtained by the agent is calculated as follows:
rt = ˙xt −0.5 ∗∥at∥2
2 −0.0005 ∗
scontact
t
2
2 + 1
A.7. Reacher-v2
The Reacher environment, as introduced in (Todorov et al., 2012), is a two-jointed robot arm. The goal is to move the
robot’s end effector (called *ﬁngertip*) close to a target that is spawned at a random positions. The action space is a
two-dimensional vector representing the torque to be applied at the two joints. The state space consists of angular positions
(in terms of cosine and sine of the angle formed by the two moving arms), coordinates, and velocity states for different body
parts followed by the distance from target for the whole object.
A.8. Hopper-v2
The Hopper environment, as introduced in (Todorov et al., 2012), sets out to increase the number of independent state
and control variables compared to classic control environments. The hopper is a two-dimensional ﬁgure with one leg that
consists of four main body parts - the torso at the top, the thigh in the middle, the leg at the bottom, and a single foot on
which the entire body rests. The goal of the environment is to make hops that move in the forward (right) direction by
applying torques on the three hinges connecting the body parts. The action space is a three-dimensional element vector. The
state space consists of positional values for different body parts followed by the velocity states of individual parts.
A.9. Walker-v2
The Walker environment, as builds on top of the Hopper environment introduced in (Todorov et al., 2012), by adding another
set of legs making it possible for the robot to walker forward instead of hop. The hopper is a two-dimensional ﬁgure with
two legs that consists of four main body parts - the torso at the top, two thighs in the middle, two legs at the bottom, and two
feet on which the entire body rests. The goal of the environment is to coordinate both feel and move in the forward (right)
direction by applying torques on the six hinges connecting the body parts. The action space is a six-dimensional element
vector. The state space consists of positional values for different body parts followed by the velocity states of individual
parts.
A.10. Inverted Double-Pendulum-v2
Inverted Double-Pendulum as introduced in (Todorov et al., 2012) is built upon the CartPole environment as introduced in
(Barto et al., 1983), with the infusion of Mujoco. This environment involves a cart that can be moved linearly, with a pole
ﬁxed and a second pole on the other end of the ﬁrst one (leaving the second pole as the only one with one free end). The cart
can be pushed either left or right. The goal is to balance the second pole on top of the ﬁrst pole, which is on top of the cart,
by applying continuous forces on the cart. The agent takes a one-dimensional continuous action space in the range [-1,1],
denoting the force applied to the cart and the sign depicting the direction of the force. The state space consists of positional
values of different body parts of the pendulum system, followed by the velocities of those individual parts (their derivatives)
with all the positions ordered before all the velocities. The goal is to balance the double-inverted pendulum on the cart while
maximizing its height off the ground and having minimum disturbance in its velocity.
A.11. FetchReach-v1
The FetchReach environment introduced in (Plappert et al., 2018) was released as part of OpenAI Gym and used the Mujoco
physics engine for fast and accurate simulation. The goal is 3-dimensional and describes the desired position of the object.
Rewards in this environment are sparse and binary. The agent obtains a reward of 0 if the target location is at the target
location (within a tolerance of 5 cm) and −1 otherwise. Actions are four-dimensional, where 3 speciﬁes desired gripper
movement, and the last dimension controls the opening and closing of the gripper. The FetchReach aims to move the gripper
to a target position.

Introspective Experience Replay
A.12. Pong-v0
Pong, also introduced in (Mnih et al., 2013), is comparatively more accessible than other Atari games such as Enduro. Pong
is a two-dimensional sports game that simulates table tennis. The player controls an in-game paddle by moving vertically
across the left and right sides of the screen. Players use this paddle to hit the ball back and forth. The goal is for each
player to reach eleven points before the opponent, where the point is earned for each time the agent returns the ball and the
opponent misses.
A.13. Enduro-v0
Enduro, introduced in (Mnih et al., 2013), is a hard environment involving maneuvering a race car in the National Enduro,
a long-distance endurance race. The goal of the race is to pass a certain number of cars each day. The agent must pass
200 cars on the ﬁrst day and 300 cars on all subsequent days. Furthermore, as time passes, the visibility changes as well.
At night in the game, the player can only see the oncoming cars’ taillights. As the days’ progress, cars will become more
challenging to avoid. Weather and time of day are factors in how to play. During the day, the player may drive through an
icy patch on the road, which would limit control of the vehicle, or a patch of fog may reduce visibility.
B. Model and Hyperparameters
In this paper, we work with two classes of algorithms: DQN, DDPG and TD3. The hyperparameters used for training
our DQN algorithms in various environments are described in Table 6. The hyperparameters used for training our DDPG
algorithms in various environments are described in Table 7. The hyperparameters used for training DDPG are described in
Table 7. Furthermore, the hyperparameters used for training TD3 are described in Table 8.
Table 6. Hyperparameters used for training DQN on various environments.
Description
CartPole
Acrobot
LunarLander
Pong
Enduro
argument_name
General Settings
Discount
0.9
0.9
0.9
0.99
0.99
discount
Batch size
512
512
512
32
32
batch_size
Number of epochs
100
100
200
150
800
n_epochs
Steps per epochs
10
10
10
20
20
steps_per_epoch
Number of train steps
500
500
500
125
125
num_train_steps
Target update frequency
30
30
10
2
2
target_update_frequency
Replay Buffer size
1e6
1e6
1e6
1e4
1e4
buffer_size
Algorithm Settings
CNN Policy Channels
-
-
-
(32, 64, 64)
(32, 64, 64)
cnn_channel
CNN Policy Kernels
-
-
-
(8, 4, 3)
(8, 4, 3)
cnn_kernel
CNN Policy Strides
-
-
-
(4, 2, 1)
(4, 2, 1)
cnn_stride
Policy hidden sizes (MLP)
(8, 5)
(8, 5)
(8, 5)
(512, )
(512, )
pol_hidden_sizes
Buffer batch size
64
128
128
32
32
batch_size
Exploration Settings
Max epsilon
1.0
1.0
1.0
1.0
1.0
max_epsilon
Min epsilon
0.01
0.1
0.1
0.01
0.01
min_epsilon
Decay ratio
0.4
0.4
0.12
0.1
0.1
decay_ratio
Optimizer Settings
Learning rate
5e−5
5e−5
5e−5
1e−4
1e−4
lr
PER Speciﬁc Settings
Prioritization Exponent
0.4
0.6
0.8
0.4
0.4
α
Bias Annealing Parameter
0.6
0.6
0.8
0.6
0.6
β
IER Speciﬁc Settings
Use Hindsight for storing states
−
✓
−
−
✓
use_hindsight
Mixing Factor (p)
0
0
0
0
0
p
Additionally, we use Tabular MDPs for learning a policy in our toy example. Since the environment is fairly simpler, and
has very few states, function approximation is unnecessary. For all the agents trained on GridWorld, we use a common
setting as described in Table 9.

Introspective Experience Replay
Table 7. Hyperparameters used for training DDPG on Pendulum environment.
Description
Pendulum
argument_name
General Settings
Discount
0.95
discount
Batch size
256
batch_size
Number of epochs
50
n_epochs
Steps per epochs
50
steps_per_epoch
Number of train steps
40
num_train_steps
Target update Tau
0.01
target_update_frequency
Replay Buffer size
1e6
buffer_size
Algorithm Settings
Policy hidden sizes (MLP)
(400, 300)
pol_hidden_sizes
QF hidden sizes (MLP)
(400, 300)
qf_hidden_sizes
Buffer batch size
256
batch_size
Exploration Settings
Exploration Policy
Ornstein Uhlenbeck Noise
exp_policy
Sigma
0.2
sigma
Optimizer Settings
Policy Learning rate
1e−4
pol_lr
QF Learning rate
1e−3
qf_lr
PER Speciﬁc Settings
Prioritization Exponent
0.4
α
Bias Annealing Parameter
0.8
β
IER Speciﬁc Settings
Use Hindsight for storing states
−
use_hindsight
Mixing Factor (p)
0
p
C. Additional Results
C.1. Analysis of Fault Tolerance in Reinforcement Learning
Selecting an appropriate metric for comparison in RL is crucial due to the high variance in results attributed to the
environment stochasticity and the stochasticity in the learning process. As such, using the average performance metric with
small number of seeds can give statistically different distributions in results. This has been highlighted by the works of
(Henderson et al., 2018). Reliable reinforcement learning still remains an open problem. As such, using the average over
all seeds might not give us a stable comparison of the algorithms considered, unless we can increase our number of seeds
to large numbers such as > 20. This is computationally very expensive and such high number of trials are seldom used in
practice. Instead, other approaches use the maximum of N runs to report performance. This however is not ideal since it
overestimates your performance by a large amount. We can see this with the PER hyper-parameter search where we deploy
the best performing hyper-parameter from the hyper-parameter grid search into an independent experiment with Top-K
out of n metric. We notice that this under-performs the grid search result by a large margin. Thus, the top-1 metric is not
desirable.
Instead, we use the Top-K performance metric which we show below to be robust to noise and is closer to providing the
right analysis in comparison to average metric approach. Similar to work such as (Mnih et al., 2016) which used a top-5/50
experiments, we use a top-3/5 experiments to limit the overestimation of results further.
To explain our decision choice of Top-K metric, we consider a fault-based model for RL algorithm output and show via a
simulation of this in a simple toy example why Top-K seeds can give us a better way of inferring the best algorithm than
the average, especially when the number of independent trials is low. RL algorithms usually perform well sometimes, and

Introspective Experience Replay
Table 8. Hyperparameters used for training TD3 on various environments.
Description
Ant
Reacher
Walker
Double-Pendulum
HalfCheetah
Hopper
FetchReach
argument_name
General Settings
Discount
0.99
0.99
0.99
0.99
0.99
0.99
0.95
discount
Batch size
250
250
250
100
100
100
256
batch_size
Number of epochs
500
500
500
750
500
500
100
n_epochs
Steps per epochs
40
40
40
40
20
40
50
steps_per_epoch
Number of train steps
50
50
50
1
50
100
100
num_train_steps
Replay Buffer size
1e6
1e6
1e6
1e6
1e6
1e6
1e6
buffer_size
Algorithm Settings
Policy hidden sizes (MLP)
(256, 256)
(256, 256)
(256, 256)
(256, 256)
(256, 256)
(256, 256)
(256, 256)
pol_hidden_sizes
Policy noise clip
0.5
0.5
0.5
0.5
0.5
0.5
0.5
pol_noise_clip
Policy noise
0.2
0.2
0.2
0.2
0.2
0.2
0.2
pol_noise
Target update tau
0.005
0.005
0.005
0.005
0.005
0.005
0.01
tau
Buffer batch size
100
100
100
100
100
100
256
batch_size
Gaussian noise Exploration Settings
Max sigma
0.1
0.1
0.1
0.1
0.1
0.1
0.1
max_sigma
Min sigma
0.1
0.1
0.1
0.1
0.1
0.1
0.1
min_sigma
Optimizer Settings
Policy Learning rate
1e−3
1e−3
1e−4
3e−4
1e−3
3e−4
1e−3
pol_lr
QF Learning rate
1e−3
1e−3
1e−3
1e−3
1e−3
1e−3
1e−3
qf_lr
PER Speciﬁc Settings
Prioritization Exponent
0.4
0.4
0.4
0.8
0.4
0.4
0.4
α
Bias Annealing Parameter
0.6
0.6
0.6
0.6
0.6
0.2
0.6
β
IER Speciﬁc Settings
Use Hindsight for storing states
−
−
−
−
−
−
−
use_hindsight
Mixing Factor (p)
0
0
0.4
0
0.3
0.8
0
p
Table 9. Hyperparameters used for training Tabular MDP on GridWorld-1D environment.
Description
GridWorld
argument_name
Discount
0.99
discount
Batch size
1
batch_size
Number of epochs
100
n_epochs
Replay Buffer size
3e4
buffer_size
Buffer batch size
64
batch_size
Exploration factor
0.3
max_epsilon
Learning rate
0.1
lr
some other times they fail badly. This has been noted in the literature (Henderson et al., 2018) and can also be seen in our
experiments. Since the deviations are so large between seeds, this can be modeled as a ‘fault’ rather than a mean + gaussian
noise. To give a better understanding, let us elaborate with a simpliﬁed scenario: Consider the following setting where we
have 20 different environments. There are two algorithms: A and B. Algorithm A gives a moving average return of 0.9
50% of the time and a moving average return of 0 the remaining times (on all 20 environments). Algorithm B, on the other
hand, gives a moving average return of 1 50% of the time and a moving average return of 0 the remaining times (on all 20
environments). In reality, Algorithm B performs better than Algorithm A in all 20 environments. We can conclude this with
the empirical average if we extend the number of experiments to very large numbers, such as 50 seeds per environment or
larger. We further extend our above analysis by adding another Algorithm C that gives a moving average return of 0.8 50%
of the time and a moving average return of 0 the remaining times (on all 20 environments).
In this simpliﬁed model, we test how well the Top-K metric and the average metric perform in recovering the ground truth
(i.e, Algorithm B) via Monte-Carlo simulation with 500 trials. In each trial, we generate the return of each algorithm over
each of the 20 environments from the model described above with 10 different random seeds. For each environment, we
check the best algorithm with the average metric and the best algorithm with the Top-K metric. We compare this with
the ground truth (i.e, algorithm B being the best in all 20 environments). Figure 4(a) depicts the comparison between the
average metric, and Top-K metric with respect to the ground truth. As illustrated, the Top-K metric is more robust to faults
and is closer to the ground truth than the other metrics. We further add gaussian noise of mean 0, and standard deviation 0.2,
keeping all other parameters constant. This noise is added to fault runs and standard ones. We note little difference in our
results and depict the results averaged over 500 runs as depicted in Figure 4(b) Therefore, our model suggests that the Top-K
metric is more robust to faulty runs and can help facilitate the comparison of various learning algorithms with high volatility.

Introspective Experience Replay
Average metric
Top-k metric
Ground Truth
0
5
10
15
20
Frequency
Algorithm A
Algorithm B
Algorithm C
(a) Without Gaussian Noise
Average metric
Top-k metric
Ground Truth
0
5
10
15
20
Frequency
Algorithm A
Algorithm B
Algorithm C
(b) With Gaussian Noise (σ = 0.2)
Figure 4. Analysis of Fault Tolerance Metrics.
C.2. Grid-Search for tuning PER hyperparameters
In this section we present the results from our grid search experiment for tuning the hyperparameters for PER. We tune the
bias annealing parameter (β), and the prioritization exponent (α). We perform a robust grid search across all environments
we have experimented with with the range of the priortization exponent and beta as [0.2, 0.4, 0.6, 0.8, 1.0]. Figure ??
illustrates the performance of PER with varying hyperparameter
We brieﬂy summarize the ﬁndings from our grid-search experiment below:
• On environments such as CartPole , Reacher, we notice only a marginal differences between default hyper parameters
(α = 0.4, β = 0.6) and the best hyper parameters tuned for the model.
• For other environments such as HalfCheetah, Ant, FetchReach, and Walker, we notice a signiﬁcant gap between the
performance of PER and IER even after a thorough tuning of PER hyperparameters. For instance, IERoutperforms
PER by more that 8000 in terms of average return on HalfCheetah environment. Furthemore, IER outperforms
PER by almost 48 in terms of average return on FetchReach. Finally, IER outperforms PER by more than 6200 in
terms of average return on Ant. On Walker, IER outperforms PER by almost 1500 in terms of average return. For
these environments, we report the results with the default setting, i.e. α = 0.4 and β = 0.6.
• Some environments such as Acrobot, Pendulum, LunarLander, Hopper, and Double-Inverted-Pendulum did show
signiﬁcant improvements when tuned for the prioritization exponent, and bias annealing factor. We take the best
hyperparameters from the grid-search experiment, and re-run the code to compute Top-K metrics (i.e, we pick top 3 out
of 5 runs). However, from our experiments, we show that even the select hyperparameter is not robust across seeds, and
overall performs worse than our proposed approach of IER across all environments: Acrobot, Pendulum, LunarLander,
Hopper, and Double-Inverted-Pendulum.
• For Atari environments such as Pong and Enduro, we use the default parameters recommended by (Schaul et al., 2015),
and do not perform a grid-search experiment.
C.3. Performance of IER with Low-Dimensional State Space
This section brieﬂy discusses our results on environments with a low-dimensional state space, such as classic control
environments (CartPole, Acrobot, and Pendulum) and Box-2D environments (LunarLander). Figure 6 depicts the learning
curves of our DQN/DDPG agents in these environments.
We note that our proposed methodology can signiﬁcantly outperform other baselines for the classic control algorithms.
Furthermore, Figure 6(a) shows excellent promise as we achieve a near-perfect score across all seeds in one-tenth of the
time it took to train PER.
C.4. Performance in Multiple Joint Dynamics Simulation and Robotics Environments
Multiple joint dynamic simulation environments (mujoco physics environments) and robotics environments such as
HalfCheetah, Ant, Inverted Double-Pendulum, and FetchReach ((Todorov et al., 2012; Plappert et al., 2018)) are more

Introspective Experience Replay
(a) CartPole-v0
(b) Acrobot-v1
(c) Pendulum-v0
(d) LunarLander-v2
(e) HalfCheetah-v2
(f) Ant-v2
(g) Reacher-v2
(h) Walker-v2
(i) Hopper-v2
(j) InvertedDoublePendulum-v2
(k) FetchReach-v1
Figure 5. Grid-Search of Prioritization exponent (α), and Bias Annealing parameter (β) respectively.
complex and enable us to study whether the agent can understand the physical phenomenon of real-world environments.
Figure 7 depicts the learning curves of our TD3 agents in these environments. Again, our proposed methodology outperforms
all other baselines signiﬁcantly in most of the environments studied in this section. Additionally, it is essential to point out
that our proposed method shows an impressive speedup of convergence in Inverted Double Pendulum and convergence to a
much better policy in Ant.
C.5. Performance of IER in Human Challenging Environments
This section brieﬂy discusses our results on human-challenging environments such as Atari environments (Pong and Enduro).
These environments are highly complex, and our algorithms take millions of steps to converge to a locally optimal policy.
Figure 8 depicts the learning curves of our DQN agents in these environments. We note that our proposed methodology
can perform favorably when compared to other baselines for the Atari environments and can reach large reward policies
signiﬁcantly faster than UER.
C.6. Whole vs. Component Parts
This section brieﬂy presents the learning curves of our models on three different sampling schemes: IER, OER and RER.

Introspective Experience Replay
(a) CartPole-v0
(b) Acrobot-v1
(c) Pendulum-v0
(d) LunarLander-v2
Figure 6. Learning curves of DQN/DDPG agents on Classic Control and Box-2D environments.
C.7. Buffer Batch size sensitivity of IER
This section brieﬂy presents the sensitivity to the buffer batch size hyperparameter for our proposed approach (IER). To
analyze this, we run our experiments on the CartPole environment with varying batch size of the range 2-256. Table 10 and
Figure 10 depict the buffer batch size sensitivity results from our proposed sampler.
Table 10. Buffer Batch size sensitivity of IER on the CartPole environment.
Buffer Batch Size
Average Reward
2
126.69 ± 41.42
4
192.33 ± 13.29
8
181.27 ± 32.13
16
199.24 ± 1.32
32
199.99 ± 0.001
64
199.83 ± 0.31
128
193.23 ± 10.08
256
179.95 ± 18.94
C.8. How important is sampling pivots?
This section brieﬂy presents the ablation study to analyze the importance of sampling “surprising” states as pivots. As
a baseline, we build a experience replay where these pivots are randomly sampled from the buffer. The "looking back"

Introspective Experience Replay
(a) HalfCheetah-v2
(b) Ant-v2
(c) Reacher-v2
(d) Walker-v2
(e) Hopper-v2
(f) InvertedDoublePendulum-v2
(g) FetchReach-v1
Figure 7. Learning curves of TD3 agents on Mujoco and Robotics environments.
approach is used to create batches of data. For nomenclature, we refer to our proposed approach (IER) to use the “TD
Metric” sampling of pivots, and the baseline that uses “Uniform” sampling of pivots. Table 11 and Figure 11 depict the
buffer batch size sensitivity results from our proposed sampler.

Introspective Experience Replay
(a) Pong-v0
(b) Enduro-v0
Figure 8. Learning curves of DQN agents on Atari environments.
Table 11. Importance of sampling pivots in our proposed approach (IER) on the CartPole environment.
Sampling Scheme
Average Reward
TD Metric (IER)
199.83 ± 0.31
Uniform (IER)
136.71 ± 19.59
C.9. How important is “looking back”?
This section brieﬂy presents the ablation study to analyze the importance of “looking back” after sampling pivots. As a
baseline, we build a experience replay where we sample uniformly instead of looking back. For nomenclature, we refer to
our proposed approach (IER) to use the “Looking Back” approach (similar to IER), and the baseline that uses “Uniform”
approach. We refer to these two approaches as possible ﬁlling schemes, i.e. ﬁll the buffer with states once the pivot state is
sampled.
Table 12 and Figure 12 depict the buffer batch size sensitivity results from our proposed sampler.
Table 12. Importance of looking back in our proposed approach (IER) on the CartPole environment.
Filling Scheme
Average Reward
Looking Back (IER)
199.83 ± 0.31
Uniform (IER)
182.5 ± 23.49
D. Ablation study of Temporal effects
This section studies the ablation effects of going temporally forward and backward once we choose a pivot/surprise point.
Furthermore, Figure 13 depicts the learning curves of the two proposed methodologies. The forward sampling scheme is
worse in most environments compared to the reverse sampling scheme.
E. Sparsity and Rewards of Surprising States
E.1. Surprising States Have Large Rewards
In this section, we study the “learning from sparse reward” intuition provided in Section 4 – i.e., we want to check if the
states corresponding to large TD error correspond to states with large (positive or negative) rewards. To test the hypothesis,
we consider a sampled buffer and plot the TD error of these points in the buffer against the respective reward. Figure 3
shows the distribution of TD error against reward for the sampled buffers in the Ant environment. We see that high reward

Introspective Experience Replay
(a) CartPole-v0
(b) Acrobot-v1
(c) Pendulum-v0
(d) LunarLander-v2
(e) HalfCheetah-v2
(f) Ant-v2
(g) Reacher-v2
(h) Walker-v2
(i) Hopper-v2
(j) InvertedDoublePendulum-v2
(k) FetchReach-v1
(l) Pong-v0
(m) Enduro-v0
Figure 9. Ablation study of OER, RER and IER.
states (positive or negative) also have higher TD errors. Therefore, our algorithm picks large reward states as endpoints to
learn in such environments.
E.2. Surprising States are Sparse and Isolated
Figure 14 and Figure 15 depict the distribution of “surprise”/TD error in a sampled batch for CartPole and Ant environments
respectively. These two ﬁgures help show that the states with a large “surprise” factor are few and that even though the pivot
of a buffer has a large TD error, the rest of the buffer typically does not.

Introspective Experience Replay
Figure 10. Buffer batch size sensitivity of IER sampler on the CartPole Environment
Figure 11. Ablation study of Importance Sampling of IER sampler on the CartPole Environment. Here “Uniform Sampling” denotes the
uniformly random sampling of pivots, and “TD Metric Sampling” denotes our proposed approach (IER).
Figure 14(d) and Figure 15(d) show a magniﬁed view of Figure 14(c) and Figure 15(d) where the pivot point selected is
dropped. This helps with a uniform comparison with the remaining timesteps within the sampled buffer. Again, we notice
little correlation between the timesteps within the sampled buffer.
F. Reverse Experience Replay (RER)
This section discusses our implementation of Reverse Experience Replay (RER), which served as a motivation for our
proposed approach. The summary of the RER approach is shown in Figure 16. Furthermore, an overview of our implemented
approach to RER is described brieﬂy in Algorithm 2.

Introspective Experience Replay
Figure 12. Ablation study of Filling Scheme of IER sampler on the CartPole Environment. Here “Uniform Filling” denotes the uniformly
random sampling of states to ﬁll after sampling the pivot state, and “Looking Back Filling” denotes our proposed approach (IER).
Algorithm 2 Reverse Experience Replay
Input: Data collection mechanism T, Data buffer H, Batch size B, grad steps per Epoch G, number of episodes N, learning
procedure A
n ←N P ←len(H) ;
// Set index to last element of Buffer H
while n < N do
n ←n + 1 H ←T(H) ;
// Add a new episode to the buffer
g ←0 while g < G do
if P −B < 0 then
P ←len(H) ;
// Set index to last element of Buffer H
else
P ←P −B
end
D ←H[P −B, P] ;
// Load batch of previous B samples from index P
g ←g + 1 A(D);
// Run the learning algorithm with batch data D
end
end

Introspective Experience Replay
(a) CartPole-v0
(b) Acrobot-v1
(c) Pendulum-v0
(d) LunarLander-v2
(e) HalfCheetah-v2
(f) Ant-v2
(g) Reacher-v2
(h) Walker-v2
(i) Hopper-v2
(j) InvertedDoublePendulum-v2
(k) FetchReach-v1
(l) Pong-v0
(m) Enduro-v0
Figure 13. Ablation study of the effects of the temporal structure on the performance of the agent.

Introspective Experience Replay
0
10
20
30
40
50
60
Normalized TD Error
0.0
0.2
0.4
0.6
0.8
1.0
Normalized Reward
Sparsity Analysis over 3 sampled buffers for UER
(a) UER
0
10
20
30
40
50
60
Normalized TD Error
0.0
0.2
0.4
0.6
0.8
1.0
Normalized Reward
Sparsity Analysis over 3 sampled buffers for RER
(b) RER
0
10
20
30
40
50
60
Normalized TD Error
0.0
0.2
0.4
0.6
0.8
1.0
Normalized Reward
Sparsity Analysis over 3 sampled buffers for IER
(c) IER
0
10
20
30
40
50
60
Normalized TD Error
0.00
0.05
0.10
0.15
0.20
0.25
Normalized Reward
Sparsity Analysis over 3 sampled buffers for IER magnified
(d) IER Magniﬁed
Figure 14. Normalized TD Error ("Surprise factor") of each timestep over three different sampled buffers on the CartPole environment.
Best viewed when zoomed.

Introspective Experience Replay
0
20
40
60
80
100
Normalized TD Error
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Normalized Reward
Sparsity Analysis over 3 sampled buffers for UER
(a) UER
0
20
40
60
80
100
Normalized TD Error
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Normalized Reward
Sparsity Analysis over 3 sampled buffers for RER
(b) RER
0
20
40
60
80
100
Normalized TD Error
0.0
0.2
0.4
0.6
0.8
1.0
Normalized Reward
Sparsity Analysis over 3 sampled buffers for IER
(c) IER
0
20
40
60
80
100
Normalized TD Error
0.0
0.1
0.2
0.3
0.4
0.5
Normalized Reward
Sparsity Analysis over 3 sampled buffers for IER magnified
(d) IER Magniﬁed
Figure 15. Normalized TD Error ("Surprise factor") of each timestep over three different sampled buffers on the Ant environment. Best
viewed when zoomed.
Batch k
Batch 2
Batch 1
Temporally Reverse Direction
Figure 16. An illustration of Reverse Experience Replay (RER) when selecting k batches from the Replay Buffer.

