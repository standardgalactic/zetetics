
Differential Geometry
of Manifolds


Differential Geometry
of Manifolds
Stephen Lovett
A K Peters, Ltd.
Natick, Massachusetts

CRC Press
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2010 by Taylor & Francis Group, LLC
CRC Press is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Version Date: 20110714
International Standard Book Number-13: 978-1-4398-6546-0 (eBook - PDF)
This book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been made 
to publish reliable data and information, but the author and publisher cannot assume responsibility for the validity of all 
materials or the consequences of their use. The authors and publishers have attempted to trace the copyright holders of 
all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not 
been obtained. If any copyright material has not been acknowledged please write and let us know so we may rectify in any 
future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or uti-
lized in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopy-
ing, microfilming, and recording, or in any information storage or retrieval system, without written permission from the 
publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com (http://www.
copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-
750-8400. CCC is a not-for-profit organization that provides licenses and registration for a variety of users. For organiza-
tions that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for identi-
fication and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com

Contents
Preface
vii
Acknowledgments
xiii
1
Analysis of Multivariable Functions
1
1.1
Functions from Rn to Rm
. . . . . . . . . . . . . . . . . .
1
1.2
Continuity, Limits, and Diﬀerentiability
. . . . . . . . . .
9
1.3
Diﬀerentiation Rules: Functions of Class C r . . . . . . . .
20
1.4
Inverse and Implicit Function Theorems . . . . . . . . . .
27
2
Coordinates, Frames, and Tensor Notation
37
2.1
Curvilinear Coordinates . . . . . . . . . . . . . . . . . . .
37
2.2
Moving Frames in Physics . . . . . . . . . . . . . . . . . .
44
2.3
Moving Frames and Matrix Functions
. . . . . . . . . . .
53
2.4
Tensor Notation
. . . . . . . . . . . . . . . . . . . . . . .
56
3
Differentiable Manifolds
79
3.1
Deﬁnitions and Examples . . . . . . . . . . . . . . . . . .
80
3.2
Diﬀerentiable Maps between Manifolds . . . . . . . . . . .
94
3.3
Tangent Spaces and Diﬀerentials . . . . . . . . . . . . . .
99
3.4
Immersions, Submersions, and Submanifolds . . . . . . . .
113
3.5
Chapter Summary . . . . . . . . . . . . . . . . . . . . . .
122
4
Analysis on Manifolds
125
4.1
Vector Bundles on Manifolds
. . . . . . . . . . . . . . . .
126
4.2
Vector Fields on Manifolds
. . . . . . . . . . . . . . . . .
135
4.3
Diﬀerential Forms
. . . . . . . . . . . . . . . . . . . . . .
145
4.4
Integration on Manifolds . . . . . . . . . . . . . . . . . . .
158
4.5
Stokes’ Theorem
. . . . . . . . . . . . . . . . . . . . . . .
177
v

vi
Contents
5
Introduction to Riemannian Geometry
185
5.1
Riemannian Metrics
. . . . . . . . . . . . . . . . . . . . .
186
5.2
Connections and Covariant Diﬀerentiation . . . . . . . . .
204
5.3
Vector Fields Along Curves: Geodesics . . . . . . . . . . .
219
5.4
The Curvature Tensor . . . . . . . . . . . . . . . . . . . .
234
6
Applications of Manifolds to Physics
249
6.1
Hamiltonian Mechanics
. . . . . . . . . . . . . . . . . . .
250
6.2
Electromagnetism . . . . . . . . . . . . . . . . . . . . . . .
262
6.3
Geometric Concepts in String Theory
. . . . . . . . . . .
269
6.4
A Brief Introduction to General Relativity . . . . . . . . .
278
A
Point Set Topology
295
A.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . .
295
A.2 Metric Spaces . . . . . . . . . . . . . . . . . . . . . . . . .
296
A.3 Topological Spaces . . . . . . . . . . . . . . . . . . . . . .
313
A.4 Proof of the Regular Jordan Curve Theorem . . . . . . . .
335
A.5 Simplicial Complexes and Triangulations . . . . . . . . . .
339
A.6 Euler Characteristic
. . . . . . . . . . . . . . . . . . . . .
343
B
Calculus of Variations
347
B.1
Formulation of Several Problems . . . . . . . . . . . . . .
347
B.2
The Euler-Lagrange Equation . . . . . . . . . . . . . . . .
348
B.3
Several Dependent Variables . . . . . . . . . . . . . . . . .
353
B.4
Isoperimetric Problems and Lagrange Multipliers . . . . .
359
C
Multilinear Algebra
365
C.1
Direct Sums . . . . . . . . . . . . . . . . . . . . . . . . . .
366
C.2
Bilinear and Quadratic Forms . . . . . . . . . . . . . . . .
368
C.3
The Hom Space and the Dual Space . . . . . . . . . . . .
376
C.4
The Tensor Product
. . . . . . . . . . . . . . . . . . . . .
381
C.5
Symmetric Product and Alternating Product . . . . . . .
390
C.6
The Wedge Product and Analytic Geometry . . . . . . . .
401
Bibliography
411
Index
415

Preface
Purpose of This Book
This book is the second in a pair of books that together are intended to
bring the reader through classical diﬀerential geometry into the modern
formulation of the diﬀerential geometry of manifolds, assuming only prior
experience in multivariable calculus and linear algebra. The ﬁrst book in
the pair, Diﬀerential Geometry of Curves and Surfaces by Banchoﬀand
Lovett [5], introduces the classical theory of curves and surfaces. This book
continues the development of diﬀerential geometry by studying manifolds—
the natural generalization of regular curves and surfaces to higher dimen-
sions. Though [5] provides many examples of one- and two-dimensional
manifolds that lend themselves well to visualization, this book does not
rely on [5] and can be read independently.
Taken on its own, this book attempts to provide an introduction to dif-
ferentiable manifolds, geared toward advanced undergraduate or beginning
graduate readers and retaining a view toward applications in physics. For
readers primarily interested in physics, this book may ﬁll a gap between
the geometry typically oﬀered in undergraduate programs and the geom-
etry expected in physics graduate programs. For example, some graduate
programs in physics ﬁrst introduce electromagnetism in the context of a
manifold.
The student who is unaccustomed to the formalism of man-
ifolds may be lost in the notation at worst or, at best, be unaware of
how to parametrize coordinate patches or how to do explicit calculations
of diﬀerentials of maps between manifolds. For readers with primarily a
mathematics leaning, this book gives a concrete introduction to the theory
of manifolds at an advanced undergraduate or beginning graduate level.
What is Differential Geometry?
Diﬀerential geometry studies properties of and analysis on curves, surfaces,
and higher-dimensional spaces using tools from calculus and linear algebra.
vii

viii
Preface
Just as the introduction of calculus expands the descriptive and predictive
abilities of nearly every ﬁeld of scientiﬁc study, the use of calculus in ge-
ometry brings about avenues of inquiry that extend far beyond classical
geometry.
Though diﬀerential geometry does not possess the same restrictions as
Euclidean geometry on what types of objects it studies, not every conceiv-
able set of points falls within the purview of diﬀerential geometry. One of
the underlying themes of this book is the development and description of
the types of geometric sets on which it is possible to “do calculus.” This
leads to the deﬁnition of diﬀerentiable manifolds. A second, and some-
what obvious, theme is how to actually do calculus (e.g., measure rates
of change of functions or interdependent variables) on manifolds. A third
general theme is how to “do geometry” (e.g., measure distances, areas, and
angles) on such geometric objects. This theme leads us to the notion of a
Riemannian manifold.
Applications of diﬀerential geometry outside of mathematics ﬁrst arise
in mechanics in the study of the dynamics of a moving particle or system
of particles. The study of inertial frames is common to both physics and
diﬀerential geometry. Most importantly, however, diﬀerential geometry is
necessary to study physical systems that involve functions on curved spaces.
For example, just to make sense of directional derivatives of the surface tem-
perature at a point on the earth (a sphere) requires analysis on manifolds.
The study of mechanics and electromagnetism on a curved surface also
requires analysis on a manifold. Finally, arguably the most revolutionary
application of diﬀerential geometry to physics came from Einstein’s theory
of general relativity. In this theory, Einstein proposed that space and time
were joined together as a spacetime unit, and he described this spacetime
as a 4-manifold that curved more tightly in the presence of mass.
Organization of Topics
A typical calculus sequence analyzes single-variable real functions (R →R),
parametric curves (R →Rn), multivariable functions (Rn →R), and vector
ﬁelds (R2 →R2 or R3 →R3). This does not quite reach the full generality
that one needs for the deﬁnition of manifolds.
Chapter 1 presents the
analysis of functions f : Rn →Rm for any positive integers n and m.
Chapter 2 discusses the calculus of moving frames.
The concept of
moving frames arises as a summary to some results found in Chapters 1, 3
and 9 of [5] but also in the context of inertial frames, an important topic in

Preface
ix
dynamics. Implicit in the treatment is a view toward Lie algebra. However,
to retain the chosen level of this book, we do not develop this theory here.
Chapter 3 deﬁnes the category of diﬀerentiable manifolds. Manifolds
serve as the appropriate and most complete generalization to higher dimen-
sions of regular curves and regular surfaces. The chapter also introduces
the deﬁnition for the tangent space on a manifold and attempts to provide
the underlying intuition behind the abstract deﬁnitions.
Having deﬁned the concept of a manifold manifolds, Chapter 4 develops
the analysis on diﬀerentiable manifolds, including the diﬀerentials of func-
tions between manifolds, vector ﬁelds, diﬀerential forms, and integration.
Chapter 5 introduces Riemannian geometry without any pretention of
being comprehensive. One can easily take an entire course on Riemannian
geometry, the proper context in which one can do both calculus and ge-
ometry on a curved space. The chapter introduces the notions of metrics,
connections, geodesics, parallel transport, and the curvature tensor.
Having developed the technical machinery of manifolds, in Chapter 6
we apply the theory to a few areas in physics. We consider the Hamiltonian
formulation of dynamics, with a view toward symplectic manifolds; the ten-
sorial formulation of electromagnetism; a few geometric concepts involved
in string theory, namely the properties of the world sheet that describes
a string moving in a Minkowski space; and some fundamental concepts in
general relativity.
In order to be comprehensive and rigorous and still only require the
standard core of most undergraduate math programs, three appendices
provide any necessary background from topology, calculus of variations,
and multilinear algebra.
Using This Book
Because of the intended purpose of the book, it can serve well either as a
textbook or for self study. The conversational style attempts to introduce
new concepts in an intuitive way, explaining why one formulates certain
deﬁnitions as one does.
As a mathematics text, proofs or references to
proofs are provided for all claims. On the other hand, this book does not
supply all the physical theory and discussion behind the topics we broach.
Each section concludes with an ample collection of exercises. The au-
thor has marked exercises that require some speciﬁc physics background
by (Phys) and exercises that require ordinary diﬀerential equations with
(ODE). Problems marked with (*) indicate diﬃculty that may be related
to technical ability, insight, or length.

x
Preface
As mentioned above, this book only assumes prior knowledge of mul-
tivariable calculus and linear algebra. A few key results presented in this
textbook rely on theorems from the theory of diﬀerential equations but
either the calculations are all spelled out or a reference to the appropriate
theorem has been provided. Therefore, experience with diﬀerential equa-
tions is helpful though not necessary.
A proper introduction to the theory of diﬀerentiable manifolds does re-
quire some topology and some linear algebra beyond what is normally cov-
ered by the stated prerequisites. The appendices provide this background.
Appendix A on topology primarily supports Chapter 3 and to some degree
Chapter 1. Appendix B on calculus of variations supports Chapter 5 in
the context of geodesics and Section 6.1 on Hamiltonian mechanics. Ap-
pendix C on advanced linear algebra primarily supports Chapter 4 and
gives a complete explanation for the dichotomy between contravariant and
covariant indices for tensors. The reader should feel free to consult these
appendices as indicated above or simply when referred to them in the main
text.
Notation
It has been said jokingly that “diﬀerential geometry is the study of things
that are invariant under a change of notation.” A quick perusal of the
literature on diﬀerential geometry shows that mathematicians and physi-
cists present topics in this ﬁeld in a variety of diﬀerent ways. One could
argue that notational diﬀerences have contributed to a communication
gap between mathematicians and physicists.
In addition, the classical
and modern formulations of many diﬀerential geometric concepts vary
signiﬁcantly. Whenever diﬀerent notations or modes of presentation ex-
ist for a topic (e.g., diﬀerentials, metric tensor, and tensor ﬁelds), this
book attempts to provide an explicit coordination between the notation
variances.
As a comment on vector and tensor notation, this book consistently uses
the follows conventions. A vector or vector function in a Euclidean vector
space is denoted by ⃗v, ⃗X(t), or ⃗X(u, v).
Curves on manifolds, tangent
vectors, vector ﬁelds, or tensor ﬁelds have no superscript vector designation
and are written, for example, as γ, X, or T . A fair number of physics texts
use a bolded font like
  or
 to indicate tensors or tensor ﬁelds. Therefore,
when discussing tensors taken from a physics problem, we sometimes also
use that notation.

Preface
xi
The authors of [5] chose the following notations for certain speciﬁc ob-
jects of interest in diﬀerential geometry. Often γ indicates a curve para-
metrized by ⃗X(t), while writing ⃗X(t) = ⃗X(u(t), v(t)) indicates a curve on
a surface. The unit tangent and the binormal vectors of a curve in space
are written in the standard notation ⃗T(t) and ⃗B(t), respectively, but the
principal normal is written ⃗P(t), reserving ⃗N(t) to refer to the unit normal
vector to a curve on a surface. For a plane curve, ⃗U(t) is the vector obtained
by rotating ⃗T(t) by a positive quarter turn. Furthermore, we consistently
denote by κg(t) the curvature of a plane curve because one identiﬁes this
curvature as the geodesic curvature in the theory of curves on surfaces.
When these concepts occur in this text, we use the same conventions.
Occasionally, there arise irreconcilable discrepancies in deﬁnitions or
notations, e.g., the deﬁnition of a critical point for a function f : Rn →Rm,
how to place the signs on a Minkowski metric, how one deﬁnes θ and φ
in spherical coordinates, what units to use in electromagnetism, etc. In
these instances, we made a choice that best suits our purposes and indicate
commonly used alternatives.


Acknowledgments
I would ﬁrst like to thank Thomas Banchoﬀ, my teacher, mentor, and
friend. After one class, he invited me to join his team of students on devel-
oping electronic books for diﬀerential geometry and multivariable calculus.
Despite ultimately specializing in algebra, the exciting projects he lead
and his inspiring course in diﬀerential geometry instilled in me a passion
for diﬀerential geometry. His ability to introduce diﬀerential geometry as
a visually stimulating and mathematically interesting topic served as one
of my personal motivations for writing this book.
I am grateful to the students and former colleagues at Eastern Nazarene
College.
In particular, I would like to acknowledge the undergraduate
students who served as a sounding board for the ﬁrst few drafts of this
manuscript: Luke Cochran, David Constantine, Joseph Cox, Stephen Mapes,
and Christopher Young.
Special thanks are due to my colleagues Karl
Giberson, Lee Hammerstrom, and John Free. In addition, I am indebted
to Ellie Waal who helped with editing and index creation.
The continued support from my colleagues at Wheaton College made
writing this book a gratifying project. In particular, I must thank Terry
Perciante, Chair of the Department of Mathematics and Computer Science,
for his enthusiasm and his interest. I am indebted to Dorothy Chapell, Dean
of the Natural and Social Sciences, and to Stanton Jones, Provost of the
College, for their encouragement and for a grant that freed up my time to
ﬁnish writing. I am also grateful to Thomas VanDrunen and Darren Craig.
Finally, I cannot adequately express in just a few words how much I am
grateful to my wife, Carla Favreau Lovett, and my daughter, Anne. While
I was absorbed in this project, they provided a loving home, they braved
the signiﬁcant time commitment, and encouraged me at every step. They
also kindly put up with my occasional geometry comments such as how to
see the Gaussian curvature in the reﬂection of “the Bean” in Chicago.
xiii


CHAPTER
1
Analysis of Multivariable Functions
Parametrized curves into Rn are continuous functions from an interval of R
to Rn. Therefore, the study of local and global properties of curves requires
single-variable calculus. As one might expect, the study of surfaces in Eu-
clidean three-space involves continuous functions from R2 to R3. Therefore,
in order to properly phrase the theory of surfaces and, more generally, the
theory of manifolds, one needs to understand the analysis of multivariable
functions f : Rn →Rm.
1.1
Functions from Rn to Rm
Let U be a subset of Rn, and let f : U →Rm be a function from U to Rm.
Let {⃗u1, . . . , ⃗un} be a basis of Rn, and let {⃗v1, . . . ,⃗vm} be a basis of Rm.
Writing the input variable as
⃗x = (x1, x2, . . . , xn)
= x1⃗u1 + x2⃗u2 + · · · + xn⃗un,
we denote the output assigned to ⃗x by f(⃗x) or f(x1, . . . , xn). Since the
codomain of f is Rm, the images of f are vectors and one writes
f(⃗x) = (f1(⃗x), . . . , fm(⃗x))
= f1(x1, . . . , xn)⃗v1 + · · · + fm(x1, . . . , xn)⃗vm.
The functions f1(x1, . . . , xn), . . . , fm(x1, . . . , xn) are called the component
functions of f with respect to the basis {⃗v1, . . . ,⃗vm}.
One sometimes uses the notation ⃗f(⃗x) to emphasize the fact that the
codomain Rm is a vector space and that any operation on m-dimensional
vectors is permitted on functions ⃗f : Rn →Rm. Therefore, some authors
call such functions vector functions of a vector variable.
1

2
1. Analysis of Multivariable Functions
In any Euclidean space Rn, the standard basis is the set of vectors
written as {⃗e1,⃗e2, . . . ,⃗en}, where
⃗ei =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
...
1
...
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
with the only nonzero entry 1 occurring in the ith coordinate. If no basis
is explicitly speciﬁed for Rn, then it is assumed that one uses the standard
basis.
At this point, a remark is in order concerning the diﬀerences in nota-
tions between calculus and linear algebra. In calculus, one usually denotes
an element of Rn as an n-tuple and writes this element on one line as
(x1, . . . , xn). On the other hand, in order to reconcile vector notation with
the rules for multiplying a matrix by a vector, in linear algebra one denotes
an element of Rn as a column vector
⎛
⎜
⎝
x1
...
xn
⎞
⎟
⎠.
This unfortunate diﬀerence in notation is a result of history and impossible
to change now. Nor is the diﬀerence a trivial one since, in linear algebra,
a column vector is an n × 1 matrix, which is distinct from a 1 × n matrix.
This variance in notation is also related to the diﬀerence between a vector
space and its dual, a concept which we develop later. In the rest of this
book, we will write the components of a vector function on one line as per
the n-tuple notation, but whenever a vector or vector function appears in
a linear algebraic equation, it must be understood as a column vector.
Example 1.1.1 (Curves in Rn). A parametrized curve into n-dimensional space
is a continuous function ⃗x : I →Rn.
Parametrized curves are vector
functions of a vector variable, but the variable is taken from a subset of a
one-dimensional vector space.
Example 1.1.2 (Nonlinear Coordinate Changes).
A general change of coordi-
nates in R2 is a function F : U →R2, where U is the subset of R2 in
which the coordinates are deﬁned. For example, the change from polar

1.1. Functions from Rn to Rm
3
coordinates to Cartesian coordinates is given by the function F : R2 →R2
deﬁned by
F(r, θ) = (r cos θ, r sin θ).
Example 1.1.3. In a usual multivariable calculus course, one studies func-
tions F : Rn →R, written as F(x1, x2, . . . , xn). All such functions are just
examples of vector functions of a vector variable with codomain R.
Example 1.1.4. As an example of a function from R2 to R3, consider the
function
F(x1, x2) =

2x2(1 −x2
1)
(1 + x2
1)(1 + x2
2),
4x1x2
(1 + x2
1)(1 + x2
2), 1 −x2
2
1 + x2
2
	
.
Notice that the component functions satisfy
F 2
1 + F 2
2 + F 2
3 = 4x2
2(1 −x2
1)2 + 16x2
1x2
2 + (1 + x2
1)2(1 −x2
2)2
(1 + x2
1)2(1 + x2
2)2
= 4x2
2(1 + x2
1)2 + (1 + x2
1)2(1 −x2
2)2
(1 + x2
1)2(1 + x2
2)2
= (1 + x2
1)2(1 + x2
2)2
(1 + x2
1)2(1 + x2
2)2
= 1.
Thus, the image of F lies on the unit sphere S2 = {(x, y, z) ∈R3 | x2 +y2 +
z2 = 1}.
It is interesting to note that F is not surjective onto S2. Assuming
x2 + y2 + z2 = 1, if F(u, v) = (x, y, z), then in particular
z = 1 −v2
1 + v2 ⇐⇒v =

1 −z
1 + z ,
which implies that −1 < z ≤1, and hence, the point (0, 0, −1) is not an
image of F. Furthermore, since
z2 +

2v
1 + v2
	2
= 1
and thus
2v
1 + v2 =

1 −z2,
for any ﬁxed z, we have
x = 1 −u2
1 + u2

1 −z2
and
y =
2u
1 + u2

1 −z2.

4
1. Analysis of Multivariable Functions
Figure 1.1. Portion of the image for Example 1.1.4.
But then, if y = 0, it is impossible to obtain x = −
√
1 −z2. Consequently,
the image of F is all points on S2 except points in the set
{(x, y, z) ∈S2 | x = −

1 −z2 with z < 1}.
Figure 1.1 shows the image of F over the rectangle (x1, x2) ∈[−2, 5] ×
[0.5, 5].
Whether one has at one’s disposal a computer algebra system with
graphing capabilities or whether one attempts to picture a function with
paper and pencil, there are a few diﬀerent ways to visualize functions.
Of course, as n and m get beyond 2 or 3, visualization becomes more
diﬃcult. Recall that the graph of a function f : Rn →Rm is the subset of
Rn × Rm = Rn+m deﬁned by
{(x1, . . . , xn, y1, . . . , ym) ∈Rn+m | (y1, . . . , ym) = f(x1, . . . , xn)}.
The usual method to depict functions f : R →R (respectively f : R2 →R)
is to depict the graph as a subset of R2 (respectively R3).
This is the
standard practice in every calculus course.
For functions F : R2 →R (respectively F : R3 →R), another way
to attempt to visualize F is by plotting together (or in succession if one
has dynamical graphing capabilities) a collection of level curves (respec-
tively surfaces) deﬁned by F(x, y) = ci (respectively F(x, y, z) = ci) for
a discrete set of values ci.
This is typically called a contour diagram
of F. Figure 1.2(a) depicts a contour diagram of 2y/(x2 + y2 + 1) with
c = 0, ±0.2, ±0.4, ±0.6, ±0.8.

1.1. Functions from Rn to Rm
5
x
y
(a) A contour diagram.
x
y
z
(b) A space curve.
Figure 1.2. Methods of visualizing functions.
In a multivariable calculus course or in a basic diﬀerential geometry
course [5], one typically uses yet another technique to visualize functions
of the form ⃗f : R →Rn, for n = 2 or 3. One usually depicts a function
⃗f : R →R2 as a plane curve and a function ⃗f : R →R3 as a space curve,
but one only plots the image of ⃗f in R2 or R3. In doing so, we lose visual
information about the magnitude ∥⃗f ′(t)∥, intuitively speaking, how fast
one travels along the curve. Figure 1.2(b) shows the image of the so-called
space cardioid, given by the function
⃗f(t) = ((1 −cos t) cos t, (1 −cos t) sin t, sin t) .
Similarly, when one studies the geometry of surfaces, one again depicts a
function ⃗F : R2 →R3 by plotting its image in R3. (The graph of a function
of the form R2 →R3 is a subset of R5, which is quite diﬃcult to visualize
no matter what computer tools one has at one’s disposal!)
One can deﬁne the usual operations on functions as one would expect.
Deﬁnition 1.1.5. Let ⃗f and ⃗g be two functions deﬁned over a subset U of
Rn with codomain Rm. Then we deﬁne the following functions:
1. (⃗f + ⃗g) : U →Rm, where (⃗f + ⃗g)(⃗x) = ⃗f(⃗x) + ⃗g(⃗x).
2. (⃗f · ⃗g) : U →R, where (⃗f · ⃗g)(⃗x) = ⃗f(⃗x) · ⃗g(⃗x).
3. If m = 3, (⃗f × ⃗g) : U →R3, where (⃗f × ⃗g)(⃗x) = ⃗f(⃗x) × ⃗g(⃗x).

6
1. Analysis of Multivariable Functions
Deﬁnition 1.1.6. Let ⃗f be a function from a subset U ⊂Rn to Rm, and let
⃗g be a function from V ⊂Rm to Rs. If the image of ⃗f is a subset of V ,
then the composition function ⃗g ◦⃗f is the function U →Rs deﬁned by
(⃗g ◦⃗f)(⃗x) = ⃗g

⃗f(⃗x)

.
Out of the vast variety of possible functions one could study, the class
of linear functions serves a fundamental role in the analysis of multivariable
functions. (This book assumes the reader is familiar with linear algebra,
but we present a reminder of a few fundamental facts at this point.)
Deﬁnition 1.1.7. A function F : Rn →Rm is called a linear function if
F(⃗x + ⃗y) = F(⃗x) + F(⃗y)
for all ⃗x, ⃗y ∈Rn,
F(k⃗x) = kF(⃗x)
for all k ∈R and all ⃗x ∈Rn.
If a function F : Rn →Rm is linear, then
F(⃗0) = F(⃗0 −⃗0) = F(⃗0) −F(⃗0) = ⃗0,
and hence F maps the origin of Rn to the origin of Rm.
If B = {⃗u1, . . . , ⃗un} is a basis of Rn, then any vector ⃗u ∈Rn can be
written uniquely as a linear combination of vectors in B as
⃗u = c1⃗u1 + · · · + cn⃗un.
One often writes the coeﬃcients in linear algebra as the column vector
[⃗u]B =
⎛
⎜
⎝
c1
...
cn
⎞
⎟
⎠.
If the basis B is not speciﬁed, one assumes that the coeﬃcients are given
in terms of the standard basis. If F is a linear function, then
F(⃗u) = c1F(⃗u1) + · · · + cnF(⃗un),
hence, to know all outputs of F one needs to know the coeﬃcients of
[⃗u]B and the output of the basis vectors of B. Suppose also that B′ =
{⃗v1, . . . ,⃗vm} is a basis of Rm. If the B′-coordinates of the outputs of the
vectors in B are
[F(⃗u1)]B′ =
⎛
⎜
⎜
⎜
⎝
a11
a21
...
am1
⎞
⎟
⎟
⎟
⎠,
[F(⃗u2)]B′ =
⎛
⎜
⎜
⎜
⎝
a12
a22
...
am2
⎞
⎟
⎟
⎟
⎠,
· · · ,
[F(⃗un)]B′ =
⎛
⎜
⎜
⎜
⎝
a1n
a2n
...
amn
⎞
⎟
⎟
⎟
⎠,

1.1. Functions from Rn to Rm
7
then the image of the vector ⃗u ∈Rn is given by
[F(⃗u)]B′ = c1
⎛
⎜
⎜
⎜
⎝
a11
a21
...
am1
⎞
⎟
⎟
⎟
⎠+ c2
⎛
⎜
⎜
⎜
⎝
a12
a22
...
am2
⎞
⎟
⎟
⎟
⎠+ · · · + cn
⎛
⎜
⎜
⎜
⎝
a1n
a2n
...
amn
⎞
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎝
a11
a12
· · ·
a1n
a21
a22
· · ·
a2n
...
...
...
...
am1
am2
· · ·
amn
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
c1
c2
...
cn
⎞
⎟
⎟
⎟
⎠.
The matrix
A =
⎛
⎜
⎜
⎜
⎝
a11
a12
· · ·
a1n
a21
a22
· · ·
a2n
...
...
...
...
am1
am2
· · ·
amn
⎞
⎟
⎟
⎟
⎠
is called the B, B′-matrix of the linear function F. Therefore, one of the
fundamental results of linear algebra is that, with respect to the bases B
of Rn and B′ of Rm, any linear function F : Rn →Rm can be written
uniquely as
[F(⃗u)]B′ = A [⃗u]B
for some m × n matrix A.
Given a linear function F : Rn →Rn, one calls the image of F the set
Im(F) = F(Rn), and the kernel of F is the zero set
ker F = {⃗u ∈Rn | F(⃗u) = ⃗0}.
The image Im F is a vector subspace of Rm, and the kernel is a subspace
of Rn. The rank of F is the dimension dim(Im F) and can be shown to
be equal to the size of the largest nonvanishing minor in the matrix of F,
which is independent of the bases. The image of F cannot have a greater
dimension than either the domain or the codomain can, so
rank F ≤min{m, n},
and one says that F has maximal rank if F = min{m, n}. It is not hard
to show that a linear function F : Rn →Rm is surjective if and only if
rank F = m and F is injective if and only if rank F = n.
The rank is also useful in determining the linear dependence between a
set of vectors. If {⃗u1, ⃗u2, . . . , ⃗un} is a set of vectors in Rm, then the matrix
A =
⃗u1
⃗u2
· · ·
⃗un

,

8
1. Analysis of Multivariable Functions
where the ⃗ui are viewed as column vectors, represents a linear function
F : Rn →Rm, with
Im F = Span{⃗u1, ⃗u2, . . . , ⃗un}.
Thus, the set of vectors {⃗u1, ⃗u2, . . . , ⃗un} is linearly independent if and only
if rank F = n.
In the case of n = m, the determinant provides an alternative charac-
terization to linear independence. If F is a linear function from Rn to itself
with associated matrix A, then | det A| is the n-volume of the image under
F of the unit n-cube. Consequently, if the columns of A are not linearly
independent, the n-volume of this parallelepiped will be 0. This leads one
to a fundamental summary theorem in linear algebra.
Theorem 1.1.8. For a linear function F : Rn →Rn with associated square
matrix A, the following statements are equivalent:
1. rankF = n.
2. det A ̸= 0.
3. Im F = Rn.
4. ker F = {⃗0}.
5. The column vectors of A are linearly independent.
6. The column vectors of A form a basis of Rn.
7. The column vectors of A span Rn.
8. F has an inverse function.
We remind the reader that matrix multiplication is deﬁned in such a
way so that if A is the matrix for a linear function F : Rn →Rm and B is
the matrix for a linear function matrix G : Rp →Rn, then the product AB
is the matrix representing the composition F ◦G : Rp →Rm. Furthermore,
if m = n and rank F = n, then the matrix A−1 is the matrix that represents
the inverse function of F.
Problems
1.1.1. Consider the function F in Example 1.1.4. Prove algebraically that if the
domain is restricted to R × (0, +∞), it is injective. What is the image of
F in this case?

1.2. Continuity, Limits, and Differentiability
9
1.1.2. Let F : R2 →R2 be the function deﬁned by F(s, t) = (s2 −t2, 2st), and
let G : R2 →R2 be the function deﬁned by G(u, v) = (2u2 −3v, uv + v3).
Calculate the component functions of F ◦G and of G ◦F.
1.1.3. Show that the function ⃗X : [0, 2π] × [0, π] →R3, with
⃗X(x1, x2) = (cos x1 sin x2, sin x1 sin x2, cos x2),
deﬁnes a mapping onto the unit sphere in R3. Which points on the unit
sphere have more than one preimage?
1.1.4. Consider the function F from R3 to itself deﬁned by
F(x1, x2, x3) = (x1 + 2x2 + 3x3, 4x1 + 5x2 + 6x3, 7x1 + 8x2 + 9x3).
Prove that this is a linear function. Find the matrix associated to F (with
respect to the standard basis). Find the rank of F, and if the rank is less
than 3, ﬁnd equations for the image of F.
1.1.5. Consider a line L in Rn traced out by the parametric equation ⃗x(t) = t⃗a+⃗b.
Prove that for any linear function F : Rn →Rm, the image F(L) is either
a line or a point.
1.1.6. Let F : Rn →Rm be a linear function, and let L1 and L2 be parallel lines
in Rn. Prove that F(L1) and F(L2) are either both points or both lines
in Rm. If F(L1) and F(L2) are both lines, prove that they are parallel.
1.1.7. Let F : Rn →Rm be a linear function with associated matrix A. Prove
that F(L1) ⊥F(L2) for any pair of perpendicular lines L1 and L2 in Rn
if and only if AT A = Im.
1.1.8. Let ⃗ω be a nonzero vector in Rn. Deﬁne the function F : Rn →R as
F(⃗x) = ⃗ω · ⃗x.
Prove that F is a linear function. Find the matrix associated to F (with
respect to the standard basis).
1.1.9. Let ⃗ω be a nonzero vector in R3. Deﬁne the function F : R3 →R3 as
F(⃗x) = ⃗ω × ⃗x.
Prove that F is a linear function. Find the matrix associated to F (with
respect to the standard basis). Prove that rank F = 2.
1.2
Continuity, Limits, and Differentiability
Intuitively, a function is called continuous if it preserves “nearness.” A
rigorous mathematical deﬁnition for continuity for functions from Rn to
Rm is hardly any diﬀerent for functions from R →R.

10
1. Analysis of Multivariable Functions
In calculus of a real variable, one does not study functions deﬁned over
a discrete set of real values because the notions behind continuity and dif-
ferentiability do not make sense over such sets. Instead, one often assumes
the function is deﬁned over some interval. Similarly, for the analysis of
functions Rn to Rm, one does not study functions deﬁned from any sub-
set of Rn into Rm. One typically considers functions deﬁned over what is
called an open set in Rn, a notion we deﬁne now.
Deﬁnition 1.2.1. The open ball around ⃗x0 of radius r is the set
Br(⃗x0) = {⃗x ∈Rn : ∥⃗x −⃗x0∥< r} .
A subset U ⊂Rn is called open if for all ⃗x ∈U there exists an r > 0 such
that Br(⃗x) ⊂U.
The reader is encouraged to consult Section A.2.2 for more background
on open and closed sets. The situation in which we need to consider an
open set U and a point ⃗x0 in U is so common that another terminology
exists for U in this case.
Deﬁnition 1.2.2. Let ⃗x0 ∈Rn. Any open set U in Rn such that ⃗x0 ∈U is
called an open neighborhood, or more simply, a neighborhood, of ⃗x0.
We are now in a position to formally deﬁne continuity for functions.
Deﬁnition 1.2.3. Let U be an open subset of Rn, and let ⃗F be a function
from U into Rm. The function ⃗F is called continuous at the point ⃗x0 ∈U
if F(⃗x0) exists and if, for all ε > 0, there exists a δ > 0 such that for all
⃗x ∈R,
∥⃗x −⃗x0∥< δ
=⇒
∥⃗F(⃗x) −⃗F(⃗x0)∥< ϵ.
The function ⃗F is called continuous on U if it is continuous at every point
of U.
With the language of open balls, one can rephrase the deﬁnition of
continuity as follows. Let U be an open subset of Rn. A function ⃗F : U →
Rm is continuous at a point ⃗x0 if for all ε > 0 there exists a δ > 0 such
that
F(Bδ(⃗x0)) ⊂Bε(F(⃗x0)).
(Sections A.2.2 and A.2.4 provide a comprehensive discussion about open
sets in a metric space, a generalization of Rn, and continuity of functions
between metric spaces.)

1.2. Continuity, Limits, and Differentiability
11
Example 1.2.4. Consider the function F : Rn →Rn deﬁned by
F(⃗x) =

⃗x/∥⃗x∥,
if ⃗x ̸= ⃗0,
⃗0,
if ⃗x = ⃗0.
This function leaves⃗0 ﬁxed and projects the rest of Rn onto the unit sphere.
If ⃗x ̸= ⃗0, then
∥F(⃗x) −F(⃗x0)∥=

⃗x
∥⃗x∥−
⃗x0
∥⃗x0∥
 ≤

⃗x
∥⃗x∥−
⃗x
∥⃗x0∥
 +

⃗x
∥⃗x0∥−
⃗x0
∥⃗x0∥
 .
However,

⃗x
∥⃗x∥−
⃗x
∥⃗x0∥
 =

1
∥⃗x∥−
1
∥⃗x0∥
 ∥⃗x∥= ∥⃗x∥| ∥⃗x0∥−∥⃗x∥|
∥⃗x∥∥⃗x0∥
≤
1
∥⃗x0∥∥⃗x −⃗x0∥,
and thus,
∥F(⃗x) −F(⃗x0)∥≤
2
∥⃗x0∥∥⃗x −⃗x0∥.
Consequently, given any ε > 0 and setting
δ = min

∥⃗x0∥, 1
2ε∥⃗x0∥
	
,
we know that ⃗x ̸= ⃗0 and also that ∥F(⃗x) −F(⃗x0)∥< ε.
Hence, F is
continuous at all ⃗x0 ̸= ⃗0.
On the other hand, if ⃗x0 = ⃗0, for all ⃗x ̸= ⃗x0,
∥F(⃗x) −F(⃗0)∥= ∥F(⃗x) −⃗0∥= ∥F(⃗x)∥= 1,
which can never be less than ε if ε ≤1.
Example 1.2.5. As a contrast to Example 1.2.4, consider the function
⃗F(⃗x) =

⃗x,
if all components of ⃗x are rational,
⃗0,
otherwise.
The function F is obviously continuous at ⃗0, with δ = ε satisfying the
requirements of Deﬁnition 1.2.3. On the other hand, if ⃗x0 ̸= ⃗0, then in
Bδ(⃗x0), for any δ > 0, one can always ﬁnd an ⃗x that has either all rational
components or has at least one irrational component. Thus, if ε < ∥⃗x0∥,
for all δ > 0, we have
F(Bδ(⃗x0)) ⊈Bε(F(⃗x0)).
Thus, F is not continuous at any point diﬀerent from ⃗0.

12
1. Analysis of Multivariable Functions
The following theorem implies many other corollaries concerning conti-
nuity of multivariable functions.
Theorem 1.2.6. Let U be an open subset of Rn, let ⃗F : U →Rm be a function,
and let Fi, with i = 1, . . . , m, be the component functions. The function ⃗F
is continuous at the point ⃗a ∈U if and only if, for all i = 1, . . . , m, the
component function Fi : U →R is continuous at ⃗a.
Proof: Without loss of generality, assume that the component functions of
⃗F are given in terms of an orthonormal basis of Rm.
Suppose that ⃗F is continuous at ⃗a. Thus, for all ε > 0, there exists a
δ > 0 such that ∥⃗x −⃗a∥< δ implies ∥⃗F(⃗x) −⃗F(⃗a)∥< ε. Since
∥⃗F(⃗x) −⃗F(⃗a)∥=

(⃗F (⃗x)1 −⃗F(⃗a)1)2 + · · · + (⃗F (⃗x)m −⃗F(⃗a)m)2
≥|⃗F(⃗x)i −⃗F(⃗a)i|,
given any ε, the above δ suﬃces since ∥⃗x −⃗a∥< δ implies that
|Fi(⃗x) −Fi(⃗a)| ≤∥⃗F(⃗x) −⃗F(⃗a)∥< ε.
Conversely, suppose that all the functions Fi are continuous at ⃗a. Thus,
for any ε and for all i, there exist δi > 0 such that ∥⃗x −⃗a∥< δi implies
|Fi(⃗x) −Fi(⃗a)| < ε/√m. Then taking δ = min(δ1, . . . , δm), if ∥⃗x −⃗a∥< δ,
then
∥⃗F(⃗x) −⃗F(⃗a)∥=

|F1(⃗x) −F1(⃗a)|2 + · · · + |Fm(⃗x) −Fm(⃗a)|2
≤

ε2
m + · · · + ε2
m = ε.
Thus, ⃗F is continuous.
□
Theorem 1.2.7. Let U be an open set in Rn, let ⃗F and ⃗G be functions from
U to Rm, let h : U →R, and suppose that ⃗F, ⃗G, and h are all continuous
at ⃗a ∈U. Then the following functions are also continuous at ⃗a:
∥⃗F∥, ⃗F + ⃗G, h⃗F, ⃗F · ⃗G.
If m = 3, then the vector function ⃗F × ⃗G is also continuous at ⃗a.
Proof: (Left as an exercise for the reader.)
□

1.2. Continuity, Limits, and Differentiability
13
If U is an open set containing a point ⃗a, then the set U −{⃗a} is called a
deleted neighborhood of ⃗a. If a function ⃗F is a function into Rm deﬁned on
a deleted neighborhood of a point ⃗a ∈Rn, it is possible to deﬁne the limit
of ⃗F at ⃗a. The limit of ⃗F at ⃗a is the value ⃗p such that if ⃗F(⃗a) were ⃗p, then
⃗F(⃗a) would be continuous at ⃗a. We make this more precise as follows.
Deﬁnition 1.2.8. Let ⃗a ∈Rn.
Let ⃗F be a function from an open subset
U −{⃗a} ⊂Rn into Rm. The limit of ⃗F at ⃗a is deﬁned as the point ⃗L if, for
all ε, there exists a δ such that
⃗F(Bδ(⃗a) −{⃗a}) ⊂Bε(⃗L).
In most multivariable calculus courses, before meeting the partial deriva-
tives, one encounters the directional derivative, which measures the rate of
change of a function in a given direction. Therefore, before introducing the
diﬀerential of a vector function of a vector variable, we introduce directional
derivatives.
Deﬁnition 1.2.9. Let ⃗F be a function from an open subset U ⊂Rn into Rm,
let ⃗x0 ∈U be a point, and let ⃗u be a unit vector. The directional derivative
of ⃗F in the direction ⃗u at the point ⃗x0 is
D⃗u ⃗F(⃗x0) = lim
h→0
⃗F(⃗x0 + h⃗u) −⃗F(⃗x0)
h
whenever the limit exists.
An alternate deﬁnition of the directional derivative is to deﬁne the curve
into Rm by ⃗F(t) = ⃗F(⃗x0 + t⃗u). Then D⃗u ⃗F(⃗x0) is equal to the derivative of
⃗F at t = 0, namely,
d
dt ⃗F(⃗x0 + t⃗u)|t=0.
Like the directional derivative introduction in multivariable calculus
for functions from Rn into R, the deﬁnition of D⃗u ⃗F(⃗x0) reduces to a single
variable before taking a derivative. However, unlike multivariable calculus,
the resulting quantity is a vector, not a real number.
Example 1.2.10. Consider the function ⃗F(s, t) = (s2 −t2, 2st) from R2 to
itself. We will calculate the directional derivative of ⃗F at ⃗x0 = (1, 2) in the
direction of ⃗u = (1/2, −
√
3/2).
One can picture this kind of function by plotting a discrete set of coor-
dinate lines mapped under ⃗F (see Figure 1.3). However, for functions like
⃗F that are not injective, even this method of picturing ⃗F can be misleading
since every point in the codomain can have multiple preimages.

14
1. Analysis of Multivariable Functions
s
t
(1, 2)
⃗u
t = ±1
t = ±2
s = ±1
s = ±2
D⃗u ⃗F(⃗x0)
⃗F
Figure 1.3. Example 1.2.10.
Now,
⃗F(⃗x0 + t⃗u) =
⎛
⎝

1 + 1
2t
	2
−

2 −
√
3
2 t
2
, 2

1 + 1
2t
	 
2 −
√
3
2 t
⎞
⎠
=

−3 + (2 + 4
√
3)t −2t2, 4 + (4 −2
√
3)t −2
√
3t2
,
so
D⃗u ⃗F(⃗x0) =

(2 + 4
√
3) −4t, (4 −2
√
3) −4
√
3t
 
t=0 = (2+4
√
3, 4−2
√
3).
Figure 1.3 shows the curve ⃗F(⃗x0 + t⃗u) and illustrates the directional
derivative as being the derivative of ⃗F(⃗x0 + t⃗u) at t = 0. The ﬁgure shows
that though ⃗u must be a unit vector, the directional derivative is usually
not.
Now suppose that an orthonormal basis {⃗u1, . . . , ⃗un} in Rn is ﬁxed. Let
⃗F be a function from an open set U ⊂Rn to Rm. For any point ⃗x0 ∈U,
the directional derivative of ⃗F in the direction ⃗uk at ⃗x0 is called the kth
partial derivative of ⃗F at ⃗x0. The kth partial derivative of ⃗F is itself a
vector function possibly deﬁned on a smaller set than U. Writing
⃗F(⃗x) = (F1(x1, . . . , xn), . . . , Fm(x1, . . . , xn)) ,
some common notations for the kth partial derivative D⃗uk ⃗F are
⃗Fxk,
∂⃗F
∂xk
,
Dk ⃗F,
⃗F,k.

1.2. Continuity, Limits, and Differentiability
15
Figure 1.4. Graph of the function in Example 1.2.11.
(The notation ⃗F,k for the kth partial derivative is not standard but is
typically understood as the derivative with respect to the kth variable. One
uses the comma to distinguish the derivative operation from an index.) It
is not hard to show that
∂⃗F
∂xk
(⃗x) =
∂F1
∂xk
(x1, . . . , xn), . . . , ∂Fm
∂xk
(x1, . . . , xn)
	
.
Example 1.2.11. Consider the real-valued function f(x1, x2) deﬁned by
f(x1, x2) =
⎧
⎨
⎩
x1x2
2
x2
1 + x4
2
,
if (x1, x2) ̸= (0, 0),
0,
otherwise.
See Figure 1.4. We study the behavior of f near ⃗x = ⃗0.
Let ⃗u = (u1, u2) be a unit vector, with u1 ̸= 0. Then
D⃗uf(⃗0) = lim
h→0
f(⃗0 + h⃗u) −f(⃗0)
h
= lim
h→0
h3u1u2
2
h(h2u2
1 + h4u4
2)
= lim
h→0
u1u2
2
(u2
1 + h2u4
2) = u2
2
u1
.
If u1 = 0, then f(⃗0+h⃗u) = 0 for all h, so D⃗uf(⃗0) = 0. Thus, the directional
derivative D⃗uf(⃗0) is deﬁned for all unit vectors ⃗u.
On the other hand, consider the curve ⃗x(t) = (t2, t). Along this curve,
if t ̸= 0,
f(⃗x(t)) =
t4
t4 + t4 = 1
2.

16
1. Analysis of Multivariable Functions
Thus,
f(⃗x(t)) =

1
2,
if t ̸= 0,
0,
if t = 0,
which is not continuous. Notice that this implies that f as a function from
R2 to R is not continuous at ⃗0 since making ε = 1
4, for all δ > 0, there exist
points ⃗x (in this case, points of the form ⃗x = (t2, t)) such that ∥⃗x∥< δ
have |f(⃗x)| > ε.
Therefore, the function f is deﬁned at ⃗0, has directional derivatives in
every direction at ⃗0, but is not continuous at ⃗0.
Example 1.2.11 shows that it is possible for a vector function to have
directional derivatives in every direction at some point ⃗a but, at the same
time, fail to be continuous at ⃗a. The reason for this is that the directional
derivative depends only upon the behavior of a function along a line through
⃗a, while approaching ⃗a along other families of curves may exhibit a diﬀerent
behavior of the function.
The behavior of the function in Example 1.2.11 makes it clear that even
if all the partial derivatives of a function ⃗F exist at a point, one should
not call it diﬀerentiable there. A better approach is to call a function dif-
ferentiable at some point if it can be approximated by a linear function. A
more precise deﬁnition follows.
Deﬁnition 1.2.12. Let ⃗F be a function from an open set U ⊂Rn to Rm and
let ⃗a ∈U. We call ⃗F diﬀerentiable at ⃗a if there exist a linear transformation
L : Rn →Rm and a function ⃗R deﬁned in a neighborhood of ⃗a such that
⃗F(⃗a + ⃗v) = ⃗F(⃗a) + L(⃗v) + ⃗R(⃗v),
with
lim
⃗v→⃗0
⃗R(⃗v)
∥⃗v∥= ⃗0.
If ⃗F is diﬀerentiable at ⃗a, the linear transformation L is denoted by d⃗F⃗a
and is called the diﬀerential of ⃗F at ⃗a.
Notations for the diﬀerential vary widely. Though we will consistently
use d⃗F⃗a for the diﬀerential of ⃗F at ⃗a, some authors write d⃗F(⃗a) instead.
The notation in this text attempts to use the most common notation in dif-
ferential geometry texts and to incorporate some notation that is standard
among modern linear algebra texts.
If ⃗F is diﬀerentiable over an open set U ⊂Rn, the diﬀerential d⃗F is a
function from U to Hom(Rn, Rm), the set of linear transformations from

1.2. Continuity, Limits, and Differentiability
17
Rn to Rm. If bases B and B′ are given for Rn and Rm, then we denote the
matrix for d⃗F⃗a by

d⃗F⃗a
B′
B .
Assuming we use the standard bases for Rn and Rm, we write the matrix
for d⃗F⃗a as [d⃗F⃗a]. The matrix [d⃗F ], which is a matrix of functions deﬁned
over U, is called the Jacobian matrix of ⃗F. If m = n, the determinant
of the Jacobian matrix is simply called the Jacobian and is written J(⃗F ).
Note that J(⃗F) is a function from U to R.
Theorem 1.2.13. Let ⃗F be a function from an open set U ⊂Rn to Rm, and
let ⃗a ∈U. If ⃗F is diﬀerentiable at ⃗a, then it has a directional derivative in
every direction at ⃗a. Furthermore, D⃗u ⃗F(⃗a) = d⃗F⃗a(⃗u).
Proof: (Left as an exercise for the reader.)
□
Since the diﬀerential d⃗F⃗a is a linear function from Rn to Rm, for a
vector ⃗v = (v1, v2, . . . , vn) with coordinates given with respect to some
ﬁxed orthonormal basis {⃗u1, . . . , ⃗un}, we have at any point ⃗a
d⃗F⃗a(v1⃗u1 + · · · + vn⃗un) = v1d⃗F⃗a(⃗u1) + · · · + vnd⃗F⃗a(⃗un)
= v1
∂⃗F
∂x1

⃗a + · · · + vn
∂⃗F
∂xn

⃗a,
where the second line follows from the last part of Theorem 1.2.13. Finally,
viewing each partial derivative ∂⃗F
∂xi (⃗a) as a column vector, we have
d⃗F⃗a(⃗v) =

∂⃗F
∂x1
(⃗a)
∂⃗F
∂x2
(⃗a)
· · ·
∂⃗F
∂xn
(⃗a)
	
⃗v.
This proves the following proposition.
Proposition 1.2.14. Let ⃗F be a function from an open set U ⊂Rn to Rm. At
every point ⃗a ∈U where ⃗F is diﬀerentiable, if we write ⃗F = (F1, F2, . . . , Fm)
with respect to some orthonormal basis, then the Jacobian matrix of ⃗F is

d⃗F

=

∂⃗F
∂x1
∂⃗F
∂x2
· · ·
∂⃗F
∂xn
	
=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
∂F1
∂x1
∂F1
∂x2
· · ·
∂F1
∂xn
∂F2
∂x1
∂F2
∂x2
· · ·
∂F2
∂xn
...
...
...
...
∂Fm
∂x1
∂Fm
∂x2
· · ·
∂Fm
∂xn
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
(1.1)

18
1. Analysis of Multivariable Functions
If m = n, because of Equation (1.1), the Jacobian is commonly denoted
by any one of the following notations:
J(⃗F ),
∂(F1, . . . , Fn)
∂(x1, . . . , xn) ,
det
∂Fi
∂xj
	
,
det(d⃗F).
Example 1.2.11 shows that the implication statement in Theorem 1.2.13
cannot be replaced with an equivalence statement. Therefore, one should
remember the caveat that the Jacobian matrix may exist at a point ⃗a
without ⃗F being diﬀerentiable at ⃗a, but in that case, d⃗F⃗a does not even
exist.
Example 1.2.15. Consider a function h from an open set U ⊂Rn to R. The
diﬀerential dh has the matrix
[dh] =
 ∂h
∂x1
∂h
∂x2
· · ·
∂h
∂xn
	
,
which is in fact the gradient of h, though viewed as a row vector.
Example 1.2.16. As a simple example of calculating the Jacobian matrix,
consider the function
F(x1, x2) =

3x1 + x2
2, x1 cos x2, ex1−2x2 + 2x2

.
It is deﬁned over all R2. The Jacobian matrix is
⎛
⎜
⎜
⎝
∂F1
∂x1
∂F1
∂x2
∂F2
∂x1
∂F2
∂x2
∂F3
∂x1
∂F3
∂x2
⎞
⎟
⎟
⎠=
⎛
⎝
3
2x2
cos x2
−x1 sin x2
ex1−2x2
−2ex1−2x2 + 2
⎞
⎠.
If, for example, ⃗a = (2, π/2), then the matrix for d⃗F⃗a is
[d⃗F⃗a] =
⎛
⎝
3
π
0
−2
e2−π
−2e2−π + 2
⎞
⎠.
If, in addition, ⃗v = (3, −4) with coordinates given in the standard basis,
then
d⃗F⃗a(⃗v) =
⎛
⎝
3
π
0
−2
e2−π
−2e2−π + 2
⎞
⎠
 3
−4
	
=
⎛
⎝
9 −4π
8
11e2−π −8
⎞
⎠.
To calculate the directional derivative in the direction of ⃗v, we must use
the unit vector ⃗u = ⃗v/∥⃗v∥= (0.6, −0.8) and
D⃗u ⃗F(⃗a) = d⃗F⃗a(⃗u) =
⎛
⎝
1.8 −0.8π
1.6
2.2e2−π −1.6
⎞
⎠.

1.2. Continuity, Limits, and Differentiability
19
Problems
1.2.1. Let ⃗F(x, y) = (3x−2y +4xy, x4 −3x3y2 +3xy+1). Determine the domain
of the function, explain why or why not the function is continuous over its
domain, and ﬁnd all its (ﬁrst) partial derivatives.
1.2.2. Repeat Problem 1.2.1 with ⃗F(x, y) = ((x −y)/(x + y), y ln x).
1.2.3. Repeat Problem 1.2.1 with
⃗F(x, y, z) =

tan
 x
y
	
, x3ey+3z,

x2 + y2 + z2
	
.
1.2.4. Let ⃗F(x, y, z) = (cos(4x+3yz), xz/(1+x2 +y2)). Calculate ⃗Fxx, ⃗Fyz, and
⃗Fxyz.
1.2.5. Let F : Rn →R be a function deﬁned by F(⃗x) = e⃗u·⃗x, where ⃗u is a unit
vector in Rn. Prove that
∂2F
∂2x1 + ∂2F
∂2x2 + · · · + ∂2F
∂2xn = F.
1.2.6. If ⃗F is a linear function, show that ⃗F is continuous.
1.2.7. Show that the following function is continuous everywhere:
⃗F(u, v) =

x1 sin

1
x2

+ x2 sin

1
x1

,
if x1x2 ̸= 0,
0,
if x1x2 = 0.
1.2.8. Find the directional derivative of ⃗F(s, t) = (s3 −3st2, 3s2t −t3) at (2, 3)
in the direction ⃗u = (1/
√
2, 1/
√
2).
1.2.9. Find the directional derivative of ⃗F(x1, x2, x3) = (x1 + x2 + x3, x1x2 +
x2x3+x1x3, x1x2x3) at (1, 2, 3) in the direction of ⃗u = (1/
√
2, 1/
√
3, 1/
√
6).
1.2.10. Let ⃗F : R2 →R2 be deﬁned by F(u, v) = (u2 −v2, 2uv). Calculate the
Jacobian matrix of ⃗F. Find all points in R2 where J(⃗F) = 0.
1.2.11. Deﬁne ⃗F over R2 by ⃗F(x, y) = (ex cos y, ex sin y). Calculate the partial
derivatives ⃗Fx and ⃗Fy. Show that the Jacobian J(⃗F) is never 0. Conclude
that ⃗Fx and ⃗Fy are never collinear.
1.2.12. Let ⃗F(u, v) = (cos u sin v, sin u sin v, cos v) be a function deﬁned over [0, 2π]
×[0, π]. Show that the image of ⃗F lies on the unit sphere in R3. Calculate
d ⃗F(u,v) for all (u, v) in the domain.
1.2.13. Deﬁne ⃗F : R3 →R3 by
⃗F(u, v, w) =

(u3 + uv) cos w, (u3 + uv) sin w, u2
.
Calculate the partial derivatives ⃗Fx, ⃗Fy, and ⃗Fz. Calculate the Jacobian
J(⃗F). Determine where ⃗F does not have maximal rank.

20
1. Analysis of Multivariable Functions
1.2.14. Deﬁne ⃗F over the open set {(x, y, z) ∈R3 | x > 0, y > 0, z > 0} by
⃗F(x, y, z) = (x · yz, y · zx, z · xy). Calculate the partial derivatives ⃗Fx, ⃗Fy,
and ⃗Fz. Calculate the Jacobian J(⃗F).
1.2.15. Let F : Rn →Rm be a linear function, with F(⃗v) = A⃗v for some m × n
matrix A. Prove that the Jacobian matrix is the constant matrix A and
that for all ⃗a, dF⃗a = F.
1.2.16. Let ⃗F(u, v) = (u cos v, u sin v, u) deﬁned over R2. Show that the image of
⃗F is a cone z =

x2 + y2. Calculate the diﬀerential, and determine where
the diﬀerential does not have maximal rank.
1.2.17. Prove Theorem 1.2.7.
1.2.18. Prove Theorem 1.2.13. [Hint: Using Deﬁnition 1.2.12, set ⃗v = h⃗u, where
⃗u is a unit vector.]
1.2.19. Prove that if a function ⃗F is diﬀerentiable at ⃗a, then ⃗F is continuous at ⃗a.
1.2.20. Mean Value Theorem.
Let F be a real-valued function deﬁned over an
open set U ∈Rn and diﬀerentiable at every point of U. If the segment
[⃗a,⃗b] ⊂U, then there exists a point ⃗c in the segment [⃗a,⃗b] such that
F(⃗b) −F(⃗a) = dF⃗c(⃗b −⃗a).
1.2.21. (*) Let n ≤m, and consider a function F : U →Rm of class C1, where U
is an open set in Rn. Let p ∈U, and suppose that dFp is injective.
(a) Prove that there exists a positive real number Ap such that ∥dFp(⃗v)∥
≥Ap∥⃗v∥for ⃗v ∈Rn.
(b) Use part (a) and the Mean Value Theorem to show that F is locally
injective near p, i.e., there exists an open neighborhood U ′ of p such
that F : U ′ →F(U ′) is injective.
1.3
Differentiation Rules: Functions of Class Cr
In a single-variable calculus course, one learns a number of diﬀerentiation
rules. With functions ⃗F from Rn to Rm, one must use some caution since
the matrix [d⃗F] of the diﬀerential d⃗F is not a vector function but a matrix
of functions. (Again, we remind the reader that our notation for evaluating
the matrix of functions [d⃗F] at a point ⃗a is [d⃗F⃗a].)
Theorem 1.3.1. Let U be an open set in Rn. Let ⃗f and ⃗g be functions from
U to Rm, and let h : U →R be a scalar function. If ⃗f, ⃗g, and h are
diﬀerentiable at ⃗x, then ⃗f + ⃗g and h⃗f are diﬀerentiable at ⃗x and
1. d(⃗f + ⃗g) = d⃗f + d⃗g;
2. the diﬀerential d(h⃗f) has the matrix [d(h⃗f)] = h[d⃗f] + [⃗f] [dh].

1.3. Differentiation Rules: Functions of Class Cr
21
Note that in Theorem 1.3.1, [⃗f] is a column vector of dimension m
while [dh] is a row vector of dimension m, so [⃗f] [dh] is an m × n matrix,
as expected.
Example 1.3.2. Let ⃗f(u, v) = (u2 −v, v3, u + 2v + 1), and let h(u, v) =
u3 + uv −2. The diﬀerentials of ⃗f and h are

d⃗f

=
⎛
⎝
2u
−1
0
3v2
1
2
⎞
⎠
and
[dh] =
3u2 + v
u
.
According to Theorem 1.3.1, the Jacobian matrix of h⃗f is
[d(h⃗f)] = h

d⃗f

+

⃗f

[dh]
= (u3 + uv −2)
⎛
⎝
2u
−1
0
3v2
1
2
⎞
⎠+
⎛
⎝
u2 −v
v3
u + 2v + 1
⎞
⎠3u2 + v
u
=
⎛
⎝
2u(u3 + uv −2)
−(u3 + uv −2)
0
3v2(u3 + uv −2)
(u3 + uv −2)
2(u3 + uv −2)
⎞
⎠+
⎛
⎝
(u2 −v)(3u2 + v)
u(u2 −v)
v3(3u2 + v)
uv3
(u + 2v + 1)(3u2 + v)
u(u + 2v + 1)
⎞
⎠
=
⎛
⎝
5u4 −v2 −4u
−2uv + 2
3u2v4 + v4
3u3v2 + 4uv3 −6v2
4u3 + 6u2v + 3u2 + 2uv + 2v2 + v −2
2u3 + u2 + 4uv + u −4
⎞
⎠.
If we had to ﬁnd [d(h⃗f)(1,2)], one could simplify the work and do
[d(h⃗f)(1,2)] = h(1, 2)

d⃗f(1,2)

+

⃗f(1, 2)
 
dh(1,2)

= 1
⎛
⎝
2
−1
0
12
1
2
⎞
⎠+
⎛
⎝
−1
8
6
⎞
⎠5
1
=
⎛
⎝
2
−1
0
12
1
2
⎞
⎠+
⎛
⎝
−5
−1
40
8
30
6
⎞
⎠=
⎛
⎝
−3
−2
40
20
31
8
⎞
⎠.
We now consider the composition of two multivariable functions. Let ⃗f
be a function from a set U ⊂Rn to Rm, and let ⃗g be a function from a set
V ⊂Rp to Rn such that ⃗g(V ) ⊂U. The composite function ⃗f◦⃗g : V →Rm,
depicted by the diagram
V (⊂Rp)
⃗g
 U(⊂Rn)
⃗f
 Rm,

22
1. Analysis of Multivariable Functions
is the function such that, for each ⃗a ∈V ,
(⃗f ◦⃗g)(⃗a) = ⃗f (⃗g(⃗a)) .
As a consequence of a general theorem in topology (see Proposition
A.2.30), we know that the composition of two continuous functions is con-
tinuous. The same is true for diﬀerentiable functions, and the chain rule
tells us how to compute the diﬀerential of the composition of two functions.
Theorem 1.3.3 (The Chain Rule). Let ⃗f be a function from an open set U ⊂Rn
to Rm, and let ⃗g be a function from an open set V ⊂Rp to Rn such that
⃗g(V ) ⊂U. Let ⃗a ∈V . If ⃗g is diﬀerentiable at ⃗a and ⃗f is diﬀerentiable at
⃗g(⃗a), then ⃗f ◦⃗g is diﬀerentiable at ⃗a and
d(⃗f ◦⃗g)⃗a = d⃗f⃗g(⃗a) ◦d⃗g⃗a.
(1.2)
The Jacobian matrices satisfy the matrix product

d(⃗f ◦⃗g)⃗a

=

d⃗f⃗g(⃗a)

[d⃗g⃗a] .
Before proving this theorem, we establish a lemma.
Lemma 1.3.4. Let A be an m × n matrix. For all ⃗v ∈Rn, with ∥⃗v∥= 1, the
length ∥A⃗v∥is less than or equal to the square root of the largest eigenvalue
λ1 of AT A. Furthermore, if ⃗u1 is a unit eigenvector of AT A corresponding
to λ1, then ∥A⃗u1∥= √λ1.
Proof: Assuming that we use standard bases in Rn and Rm, then
∥A⃗v∥2 = (A⃗v) · (A⃗v) = (A⃗v)T (A⃗v) = ⃗vT AT A⃗v.
By the Spectral Theorem from linear algebra, since AT A is a symmetric
matrix, it is diagonalizable, has an orthonormal eigenbasis {⃗u1, . . . , ⃗un},
and all the eigenvalues are real. Assume ⃗ui has eigenvalue λi, and, without
loss of generality, assume that λ1 ≥λ2 ≥· · · ≥λn. It is also not hard to
conﬁrm that the eigenvalues of AT A are in fact nonnegative. Then if ⃗v has
unit length, we can write ⃗v = x1⃗u1 + · · · + xn⃗un, with x2
1 + · · · + x2
n = 1.
Then
∥A⃗v∥2 = ⃗vT AT A⃗v = λ1x2
1 + · · · + λnx2
n.
A simple calculation using Lagrange multipliers shows that ∥A⃗v∥2, subject
to the constraint ∥⃗v∥= 1, is maximized when λ = λ1 and (x1, . . . , xn) =
(1, 0, . . . , 0). The lemma follows.
□

1.3. Differentiation Rules: Functions of Class Cr
23
We will call √λ1 in the above theorem the matrix norm of A and denote
it by |A|. One notices that for all ⃗v ∈Rn, ∥A⃗v∥≤|A| ∥⃗v∥.
Proof (of Theorem 1.3.3): Let ⃗f and ⃗g be functions as deﬁned in the hy-
potheses of the theorem. Then there exist an m×n matrix A and an n×p
matrix B such that
⃗g(⃗a + ⃗h) = ⃗g(⃗a) + B⃗h + R1(⃗h),
⃗f(⃗g(⃗a) + ⃗k) = ⃗f(⃗g(⃗a)) + A⃗k + R2(⃗k),
with
lim
⃗h→⃗0
R1(⃗h)
∥⃗h∥
= ⃗0
and
lim
⃗k→⃗0
R2(⃗k)
∥⃗k∥
= ⃗0.
(1.3)
Then for the quantity (⃗f ◦⃗g)(⃗a + ⃗h), one has
(⃗f ◦⃗g)(⃗a + ⃗h) = ⃗f(⃗g(⃗a)) + AB⃗h + AR1(⃗h) + R2(B⃗h + R1(⃗h)).
(1.4)
Note that ∥AR1(⃗h)∥≤|A| ∥R1(⃗h)∥, so
lim
⃗h→⃗0
∥AR1(⃗h)∥
∥⃗h∥
≤lim
⃗h→⃗0
|A| ∥R1(⃗h)∥
∥⃗h∥
= 0.
Thus,
lim
⃗h→⃗0
AR1(⃗h)
∥⃗h∥
= ⃗0.
Now since lim⃗h→⃗0 ∥R1(⃗h)∥/∥⃗h∥= 0, then for any ε > 0, there exists
a δ > 0 such that if ⃗h ∈Rn, with ∥⃗h∥< δ, then ∥R1(⃗h)∥< ε∥⃗h∥. In
particular, pick ε = 1 and let δ0 be the corresponding value of δ. Then if
∥⃗h∥< δ0, we have
∥B⃗h + R1(⃗h)∥≤∥B⃗h∥+ ∥R1(⃗h)∥≤(|B| + 1)∥⃗h∥.
This leads to
0 ≤∥R2(B⃗h + R1(⃗h))∥
∥⃗h∥
≤(|B| + 1)∥R2(B⃗h + R1(⃗h))∥
∥B⃗h + R1(⃗h)∥
.
(1.5)
However, by Equation (1.3), one concludes that
lim
⃗h→⃗0
∥R2(B⃗h + R1(⃗h))∥
∥B⃗h + R1(⃗h)∥
= 0

24
1. Analysis of Multivariable Functions
and consequently, by Equation (1.5),
lim
⃗h→⃗0
∥R2(B⃗h + R1(⃗h))∥
∥⃗h∥
= 0.
We have shown that setting R3(⃗h) = AR1(⃗h) + R2(B⃗h + R1(⃗h)), we have
lim⃗h→⃗0 R3(⃗h)/∥⃗h∥= ⃗0. All parts of the theorem now follow from Equa-
tion (1.4).
□
Example 1.3.5. Consider the following functions: ⃗f(r, θ) = (r cos θ, r sin θ)
and ⃗g(s, t) = (s2 −t2, 2st). Calculating the composition function directly,
we have
(⃗g ◦⃗f)(r, θ) = (r2 cos2 θ −r2 sin2 θ, 2r2 cos θ sin θ) = (r2 cos 2θ, r2 sin 2θ).
Thus,
[d(⃗g ◦⃗f)(r,θ)] =
2r cos 2θ
−2r2 sin 2θ
2r sin 2θ
2r2 cos 2θ
	
.
On the other hand, we have
[d⃗g(s,t)] =

2s
−2t
2t
2s
	
and
[d⃗f(r,θ)] =

cos θ
−r sin θ
sin θ
r cos θ
	
.
Using the right-hand side of the chain rule, we calculate
[d⃗g ⃗f(r,θ)] [d⃗f(r,θ)] = [d⃗g(r cos θ,r sin θ)] [d⃗f(r,θ)]
=
2r cos θ
−2r sin θ
2r sin θ
2r cos θ
	 cos θ
−r sin θ
sin θ
r cos θ
	
=

2r cos 2θ
−2r2 sin 2θ
2r sin 2θ
2r2 cos2θ
	
= [d(⃗g ◦⃗f)(r,θ)],
as expected.
The style of presentation of the chain rule in Theorem 1.3.3 is often
attributed to Newton’s notation.
Possible historical inaccuracies aside,
Equation (1.2) is commonly used by mathematicians. In contrast, physi-
cists tend to use Leibniz’s notation, which we present now.
Suppose that the vector variable ⃗y = (y1, . . . , yn) is given as a function
of a variable ⃗x = (x1, . . . , xp) (this function corresponds to ⃗g in Equa-
tion (1.2)), and suppose that the vector variable ⃗z = (z1, . . . , zm) is given

1.3. Differentiation Rules: Functions of Class Cr
25
as a function of the variable ⃗y (this function corresponds to ⃗f ).
With
Leibniz’s notation, one writes the chain rule as
∂zi
∂xj
=
n

k=1
∂zi
∂yk
∂yk
∂xi
for all i = 1, . . . , m and j = 1, . . . , p.
Implicit in this notation is that when evaluating ∂zi/∂xj at a point ⃗a ∈Rp,
one must calculate
∂zi
∂xj

⃗a =
n

k=1
∂zi
∂yk

⃗y(⃗a)
∂yk
∂xi

⃗a.
Suppose a function ⃗f is diﬀerentiable over an open set U ⊂Rn to
Rm. Then for any unit vector ⃗u ⊂Rn, the directional derivative D⃗u ⃗f is
itself a vector function from U to Rm, and we can consider the directional
derivative D⃗v(D⃗u ⃗f) along some unit vector ⃗v. This second-order directional
derivative is denoted by D2
⃗v⃗u ⃗f.
Higher-order directional derivatives are
deﬁned in the same way.
If Rn is given a basis, then one can take higher-order partial derivatives
with respect to this basis. Some common notations for the second partial
derivative
∂
∂xj ( ∂⃗f
∂xi ) are
∂2 ⃗f
∂xj∂xi
,
∂j∂i ⃗f ,
DjDi ⃗f ,
⃗fxixj ,
⃗f,ij
and notations for third partial derivatives are
∂3 ⃗f
∂xk∂xj∂xi
,
∂k∂j∂i ⃗f ,
DkDjDi ⃗f ,
⃗fxixjxk ,
⃗f,ijk .
Most advanced physics texts use the notation ∂i ⃗f for the partial derivative
∂⃗f
∂xi . In that case, the second and third partial derivatives are ∂j∂i ⃗f and
∂k∂j∂i ⃗f, as indicated above. Of course, if one uses the notation f instead
of ⃗f to refer to a function from U ⊂Rn to Rn, all the above notations
change correspondingly.
Note that the order of the indices or subscripts is important since it is
possible that
∂2 ⃗f
∂x1∂x2
̸=
∂2 ⃗f
∂x2∂x1
.
However, this inequality does not happen if both of the second partials are
continuous. (See [14, Theorem 8.24] for a proof of this fact from multivari-
able calculus.)

26
1. Analysis of Multivariable Functions
In general, we say that a function ⃗f is of class Cr in U if all of its rth
partial derivatives exist and are continuous. More precisely, if we wish to
specify the domain U ⊂Rn and the codomain Rm in the notation, we write
⃗f ∈Cr(U, Rm). Finally, we say that ⃗f is of class C∞if all of its higher
partial derivatives exist and are continuous.
Problems
1.3.1. Prove Theorem 1.3.1.
1.3.2. Suppose that ⃗f and ⃗g are diﬀerentiable at ⃗a ∈Rn. Prove that the function
⃗f · ⃗g is diﬀerentiable at ⃗a and that
d(⃗f · ⃗g)⃗a = ⃗f(⃗a) · d⃗g⃗a + ⃗g(⃗a) · d⃗f⃗a
are linear functions.
1.3.3. Let ⃗f(r, θ, φ) = (r cos θ sin φ, r sin θ sin φ, r cos φ). Calculate the Jacobian
matrix. Prove that the Jacobian is the function r2 sin φ.
1.3.4. Let

z1
= 2y1 + 3y2,
z2
= y1y2
2,
and

y1
= ex1 + x2 + x3,
y2
= ex2−x3 + x1.
Use the chain rule to calculate the partial derivatives
∂zi
∂xj for i = 1, 2 and
j = 1, 2, 3.
1.3.5. Let ⃗f be a diﬀerentiable function from an open set U ⊂Rn to Rn, and let
⃗g be a diﬀerentiable function from an open set V ⊂Rn to U. Prove that
J(⃗f ◦⃗g) = J(⃗f)J(⃗g).
1.3.6. Suppose that U and V are open sets in Rn and that ⃗F is bijective from
U to V . Suppose in addition that ⃗F is diﬀerentiable on U and ⃗F −1 is
diﬀerentiable on V .
Prove that for all ⃗a ∈U, the linear function d ⃗F⃗a
is invertible and that
(d ⃗F⃗a)−1 = d⃗F −1
⃗F(⃗a).
Conclude that J(⃗F −1) = 1/J(⃗F ).
1.3.7. Let ⃗f be a function from U ⊂R2 to R3 such that d⃗f⃗x has rank 2 for all
⃗x ∈U. Let ⃗α be a regular curve from an interval I to U. Show that
(a) the function ⃗β(t) = ⃗f(⃗α(t)) is a regular curve in R3;
(b) the speed of ⃗β satisﬁes

d⃗β
dt

2
=

∂⃗f
∂x1

2  dα1
dt
	2
+2

∂⃗f
∂x1 · ∂⃗f
∂x2

dα1
dt
dα2
dt +

∂⃗f
∂x2

2 dα2
dt
	2
.

1.4. Inverse and Implicit Function Theorems
27
1.3.8. Repeat part (b) of Problem 1.3.7, but prove that
∥⃗β′(t)∥2 = (⃗α′(t))T 
d⃗f
T 
d⃗f

⃗α′(t).
[Hint: Recall that we view the vectors ⃗a,⃗b ∈Rn as column vectors and
⃗a ·⃗b = ⃗aT⃗b as a matrix product.]
1.3.9. Let ⃗f(s, t) = (s2t + t3, tes + set), and let ⃗u be the unit vector in the
direction (1, 1) and ⃗v be the unit vector in the direction (2, 3). Calculate
the second directional derivative function D2
⃗v⃗u ⃗f. [Hint: This is a function
of (s, t).]
1.3.10. Let ⃗f be a function from an open set U ⊂Rn to Rm. Let ⃗v and ⃗u be two
unit vectors in Rn. Prove that
D2
⃗v⃗u ⃗f =
n

i,j=1
∂⃗f
∂xi∂xj viuj.
1.3.11. Let ⃗f(r, θ) = (r cos θ, r sin θ). Calculate all the second partial derivatives
of ⃗f. Prove that ⃗f is of class C∞over all of R2.
1.3.12. Let ⃗f(u, v) = (u2 + ve2u, v + tan−1(u + 3), sin v). Find the domain of ⃗f.
Calculate all of its second partial derivatives. Calculate the following third
partial derivatives: ⃗fvvu, ⃗fvuv, and ⃗fuuv.
1.3.13. If (w1, w2) = (e−x1+x2
2, cos(x2 + x3)), calculate
∂2w1
∂x1∂x3 ,
∂2w1
∂x3∂x2 ,
∂3w2
∂x1∂x2∂x3 .
1.3.14. Let the function f : R2 →R be deﬁned by
f(s, t) =
⎧
⎨
⎩
2st(s2 −t2)
s2 + t2
,
if (s, t) ̸= (0, 0),
0,
if (s, t) = (0, 0).
Show that f is of class C1. Show that the mixed second partial derivatives
fst and fts exist at every point of R2. Show that fst(0, 0) ̸= fts(0, 0).
1.4
Inverse and Implicit Function Theorems
In single-variable and multivariable calculus of a function F : Rn →R, one
deﬁnes a critical point as a point ⃗a = (a1, . . . , an) such that the gradient
of F at ⃗a is ⃗0, i.e.,
∇F(⃗a) =
 ∂F
∂x1
(⃗a), . . . , ∂F
∂x1
(⃗a)
	
= ⃗0.

28
1. Analysis of Multivariable Functions
At such a point, F is said to have a ﬂat tangent line or tangent plane, and,
according to standard theorems in calculus, F(⃗a) is either a local minimum,
local maximum, or a “saddle point.” This notion is a special case of the
following general deﬁnition.
Deﬁnition 1.4.1. Let U be an open subset of Rn, F : U →Rm a diﬀerentiable
function, and q a point in U. We call q a critical point of F if F is not
diﬀerentiable at q or if dFq : Rn →Rm is not of maximum rank, i.e., if
rank dFq < min(m, n). If q is a critical point of F, we call F(q) a critical
value. If p ∈Rm is not a critical value of F (even if p is not in the image
of F), then we call p a regular value of F.
We point out that this deﬁnition simultaneously generalizes the notion
of a critical point for functions F : U →R, with U an open subset of
Rn, and the deﬁnition for a critical point of a parametric curve in Rn [5,
Deﬁnition 3.2.1]. If m = n, the notion of a critical point has a few alternate
equivalent criteria.
Proposition 1.4.2. Let U be an open subset of Rn, F : U →Rn a diﬀeren-
tiable function, and q a point in U such that F is diﬀerentiable at q. The
following are equivalent:
1. q is a critical point of U.
2. J(F)(q) = 0.
3. The set of partial derivatives { ∂F
∂x1 (q), . . . , ∂F
∂xn (q)} is a linearly de-
pendent set of vectors.
4. The diﬀerential dFq is not invertible.
Proof: These all follow from Theorem 1.1.8.
□
Finally, if n ̸= m and both are greater than 1, determining for what
values of q in the domain U the diﬀerential dFq does not have maximal
rank is not easy if done simply by looking at the matrix of functions [dFq].
The following proposition provides a concise criterion.
Proposition 1.4.3. Let F : U →Rm be a function where U is an open subset
of Rn, with n ̸= m. Let q ∈U such that F is diﬀerentiable at q, and call
A the Jacobian matrix [dFq] at q. Then the following are equivalent:

1.4. Inverse and Implicit Function Theorems
29
1. q is a critical point of U.
2. The determinants of all the maximal square submatrices of A are 0.
3. The sum of the squares of the determinants of all the maximal square
submatrices of A is 0.
Furthermore, if n > m, then q is a critical point of U if and only if
det(AAT ) ̸= 0.
Proof: To prove 1 ↔2, note that by deﬁnition, q is a critical point if dFq
does not have maximal rank, which means that the set of column vectors
or the set of row vectors of [dFq] is linearly dependent. This is equivalent
to the determinants of all maximal submatrices of A (sometimes referred
to as the maximal minors of A) being 0 since, if one such determinant
were not 0, then no nontrivial linear combination of the columns of [dFq]
or of the rows of [dFq] would be 0, and hence, this set would be linearly
independent.
The equivalence 2 ↔3 is trivial.
To prove the last part of the proposition, assuming that n > m, recall
that if {⃗v1, . . . ,⃗vm} are vectors in Rn, the m-volume of the parallelepiped
formed by {⃗v1, . . . ,⃗vm} is

det(BT B),
where B is the n × m matrix, with the ⃗vi as columns (see [13, Fact 6.3.7]).
Now the m-volume of this parallelepiped is 0 if and only if {⃗v1, . . . ,⃗vm} are
linearly dependent. Thus, taking B = AT and taking the ⃗vi as the columns
of AT establishes the result.
□
By referring to some advanced linear algebra, it is possible to prove
directly that, if n > m, Condition 3 in the above proposition implies that
det(AAT ) ̸= 0. In fact, even more can be said. If A is an m×n matrix with
n > m, then det(AAT ) is equal to the sum of the squares of the maximal
minors of A. (See Proposition C.6.2 in Appendix C for a proof of this fact.)
Example 1.4.4. For example, consider the function F : R3 →R2 deﬁned
by F(x, y, z) = (x2 + 3y + z3, xy + z2 + 1). The Jacobian matrix for this
function is
[dF] =
2x
3
3z2
y
x
2z
	
.
In this case, the easiest way to ﬁnd the critical points of this function is to
use the second equivalence statement in Proposition 1.4.3. The maximal

30
1. Analysis of Multivariable Functions
2 × 2 submatrices are
2x
3
y
x
	
,
2x
3z2
y
2z
	
,
3
3z2
x
2z
	
,
so since critical points occur where all of these have determinant 0, the
critical points satisfy the system of equations
⎧
⎪
⎨
⎪
⎩
2x2 −3y = 0,
4xz −3yz2 = 0,
6z −3xz2 = 0.
This is equivalent to
⎧
⎪
⎨
⎪
⎩
y = 2
3x2,
4xz −2x2z2 = 0,
z(2 −xz) = 0,
⇐⇒
⎧
⎪
⎨
⎪
⎩
y = 2
3x2,
xz(2 −xz) = 0,
z(2 −xz) = 0.
Thus, the set of critical points of F is

x, 2
3x2, 2
x
	
∈R3  x ∈R −{0}
 
∪

x, 2
3x2, 0
	
∈R3  x ∈R
 
.
The set of critical values is then

3x2 + 8
x3 , 2
3x3 + 4
x2 + 1
	
∈R2  x ∈R −{0}
 
∪

3x2, 2
3x3 + 1
	
∈R2  x ∈R
 
.
One important aspect of critical points already arises with real func-
tions. With a real diﬀerentiable function f : [a, b] →R, if f ′(x0) = 0, one
can show that f does not have an inverse function that is diﬀerentiable
over a neighborhood of x0. Conversely, if f ′(x0) ̸= 0, the function f has a
diﬀerentiable inverse in a neighborhood of x0, with
(f −1)′(y0) =
1
f ′(f −1(y0)).
A similar fact holds for multivariable functions and is called the Inverse
Function Theorem.
The proof of the Inverse Function Theorem and the following Implicit
Function Theorem are quite long and not necessary for the purposes of
this book, so we refer the reader to a book on analysis for a proof (see,
for example, [14, Section 8.5]). Instead, we simply state the theorems and
present a few examples.

1.4. Inverse and Implicit Function Theorems
31
Theorem 1.4.5 (Inverse Function Theorem). Let F be a function from an open
set U ⊂Rn to Rn, and suppose that F is of class Cr, with r ≥1. If
q ∈U is not a critical point of F, then dFq is invertible and there exists
a neighborhood V of q such that F is one-to-one on V , F(V ) is open, and
the inverse function F −1 : F(V ) →V is of class Cr. Furthermore, for all
p ∈F(V ), with p = F(q),
d(F −1)p = (dFq)−1.
Example 1.4.6. Consider the function F(s, t) = (s2 −t2, 2st) and q = (2, 3).
Note that F is deﬁned on all U = R2. The Jacobian matrix is
2s
−2t
2t
2s
	
,
and thus, the Jacobian is the function J(F)(s, t) = 4(s2 + t2). By Propo-
sition 1.4.2, the only critical point of F is (0, 0), so F satisﬁes the condi-
tions of the Inverse Function Theorem at q. For simplicity, let’s assume
V = {(s, t) ∈R2 | s > 0, t > 0}. We set (x, y) = F(s, t), solve for (s, t), and
ﬁnd that F(V ) = {(x, y) ∈R2 | y > 0} and the inverse of F is given by
s =
!
x2 + y2 + x
2
and
t =
!
x2 + y2 −x
2
.
Calculating the partial derivative ∂s/∂x, we have
∂s
∂x =
1
2
√
2

x2 + y2 + x

x

x2 + y2 + 1

=
1
2

x2 + y2
!
x2 + y2 + x
2
,
and similarly, the Jacobian matrix of F −1 is

dF −1
=
⎛
⎜
⎜
⎝
1
2√
x2+y2
√
x2+y2+x
2
1
2√
x2+y2
√
x2+y2−x
2
−
1
2√
x2+y2
√
x2+y2−x
2
1
2√
x2+y2
√
x2+y2+x
2
⎞
⎟
⎟
⎠.
(1.6)
Now with q = (2, 3), we have F(q) = (−5, 12) and
[dFq] =
4
−6
6
4
	
and
[dFq]−1 = 1
26
 2
3
−3
2
	
.

32
1. Analysis of Multivariable Functions
Furthermore, using Equation (1.6), we calculate directly that

dF −1
F (q)

=
1
2

(−5)2 + 122
⎛
⎝
√
(−5)2+122−5
2
√
(−5)2+122+5
2
−
√
(−5)2+122+5
2
√
(−5)2+122−5
2
⎞
⎠
= 1
26
 2
3
−3
2
	
.
Hence, [dFq]−1 =[dF −1
F (q)], thus illustrating the Inverse Function Theorem.
Another important theorem about functions in the neighborhood of a
point p that is not critical, is the fact that the level set through p can
be parametrized by (is the image of) an appropriate function. This is the
Implicit Function Theorem.
Theorem 1.4.7 (Implicit Function Theorem). Let F be a function from an open
set U ⊂Rn to Rm, with n > m, and suppose that F is of class Cr, with
r ≥1. Let q ∈U, and let Σ be the level surface of F through q, deﬁned as
Σ = {⃗x ∈Rn | F(⃗x) = F(p)}.
If q ∈U is not a critical point, then the coordinates of Rn can be relabeled
so that
n−m
m
dFq =

S

T

m,
with T an m × m invertible matrix. Then there exist an open neighborhood
V of q in Rn, an open neighborhood W of a = (q1, . . . , qn−m) in Rn−m,
and a function g : W →Rm that is of class Cr such that Σ∩V is the graph
of g, i.e.,
Σ ∩V = {(⃗s, g(⃗s)) |⃗s ∈W}.
Furthermore, the Jacobian matrix of g at a is
[dga] = −T −1S.
Example 1.4.8. We use the Implicit Function Theorem to tell us something
about the set
Σ = {(x, y, z) ∈R3 | x2 + y2 + z2 = 1 and x + y + z = 1}.
This is the intersection between a sphere and a plane, which is a circle
lying in R3. (In Figure 1.5, Σ is the circle shown as the intersection of

1.4. Inverse and Implicit Function Theorems
33
Figure 1.5. Example 1.4.8.
the sphere and the plane.) Consider the point q = ( 4
13, −3
13, 12
13) ∈Σ. To
study Σ near q, consider the function F : R3 →R2 deﬁned by F(x, y, z) =
(x2 + y2 + z2, x + y + z). The Jacobian matrix of F is
[dF] =

2x
2y
2z
1
1
1
	
,
and so the critical points of F are points (x, y, z) ∈R3 such that x = y = z.
Thus, q is not a critical point and
[dFq] =
 8
13
−6
13
24
13
1
1
1
	
.
Writing
S =
 8
13
1
	
and
T =
−6
13
24
13
1
1
	
,
since T is invertible, F and q satisfy the criteria of the Implicit Function
Theorem. Thus, there exist an open neighborhood V of q in R3, an open
interval W around a =
4
13 in R, and a function g : W →R2 such that the
portion of the circle Σ ∩V is the graph of g. Also, the Jacobian matrix of
g at a (the gradient of g at a) is
dga = ⃗∇g
 4
13
	
= −T −1S = −

−13
30
24
30
13
30
6
30
  8
13
1
	
=

−8
15
−7
15

.
(1.7)

34
1. Analysis of Multivariable Functions
One can ﬁnd Σ by ﬁrst noting that the subspace x + y + z = 0 has
{(0, −1, 1), (−2, 1, 1)} as an orthogonal basis. Thus, the plane x+y+z = 1
can be parametrized by
⃗X(u, v) =
1
3, 1
3, 1
3
	
+ u(0, −1, 1) + v(−2, 1, 1),
and all vectors in this expression are orthogonal to each other. The addi-
tional condition that x2 + y2 + z2 = 1 be equivalent to ⃗X · ⃗X = 1 leads to
2u2 + 6v2 = 2
3. This shows that the set Σ can be parametrized by
⎧
⎪
⎪
⎨
⎪
⎪
⎩
x
= 1
3 −2
3 sin t,
y
= 1
3 −
1
√
3 cos t + 1
3 sin t,
z
= 1
3 +
1
√
3 cos t + 1
3 sin t.
However, this parametrization is not the one described by the Implicit
Function Theorem. But by using it, one can ﬁnd that in a neighborhood
of q = ( 4
13, −3
13, 12
13), Σ is parametrized by

x, 1 −x
2
−1
2

1 + 2x −3x2, 1 −x
2
+ 1
2

1 + 2x −3x2
	
,
and thus the implicit function g in Theorem 1.4.7 is
g(x) =
1 −x
2
−1
2

1 + 2x −3x2, 1 −x
2
+ 1
2

1 + 2x −3x2
	
.
From here it is not diﬃcult to verify Equation (1.7) directly.
Example 1.4.8 illustrates the use of the Implicit Function Theorem.
However, though the theorem establishes the existence of the implicit func-
tion g and provides a method to calculate [dga], the theorem provides no
method to calculate the function g. In fact, unlike in Example 1.4.8, in
most cases, one cannot calculate g with elementary functions.
Problems
1.4.1. Find the critical points of the following R →R functions: (a) f(x) = x3,
(b) g(x) = sin x, and (c) h(x) = x3 −3x2 + x + 1.
1.4.2. Find all the critical points of the function F(x, y) = (x3 −xy + y2, x2 −y)
deﬁned over all R2.
1.4.3. Let F : R3 →R3 be deﬁned by F(x, y, z) = (z2−xy, x3−3xyz, x2+y2+z2).

1.4. Inverse and Implicit Function Theorems
35
(a) Find an equation describing the critical points of this function. (If
you have access to a computer algebra system, plot it.)
(b) Prove that if (x0, y0, z0) is a critical point of F, then any point
(λx0, λy0, λz0), with λ ∈R, is also a critical point.
(That is, if
(x0, y0, z0) is a critical point, then any point on the line through
(0, 0, 0) and (x0, y0, z0) is also critical. We say that the equation for
the critical points is a homogeneous equation.)
1.4.4. Let F : R3 →R2 be deﬁned by F(x, y, z) = (exy, z cos x). Find all the
critical points of F.
1.4.5. Consider the function f : R3 →R3 deﬁned by
f(x1, x2, x3) = (x1 cos x2 sin x3, x1 sin x2 sin x3, x1 cos x3).
Find the critical points and the critical values of f.
1.4.6. Let F : R2 →R2 be the function deﬁned by F(s, t) = (s3 −3st2, 3s2t−t3),
and let q = (2, 3). Find the critical points of F. Prove that there exists a
neighborhood V of q such that F is one-to-one on V so that F −1 : F(V ) →
V exists. Let p = F(q) = (−46, 9). Find d(F −1)(−46,9).
1.4.7. Let ⃗F : R2 →R2 be deﬁned by ⃗F(x, y) = (y2 sin x+1, (x+2y)cos y). Show
that (0, π/2) is not a critical point of ⃗F. Show that ⃗F is a bijection from
a neighborhood U of (0, π/2) to a neighborhood V of (1, 0). If ⃗G : V →U
is the inverse function ⃗G = ⃗F −1, then ﬁnd the matrix of d ⃗G(1,0).
1.4.8. Consider the function
⃗f(x1, x2, x3) =

x2 + x3
1 + x1 + x2 + x3 ,
x1 + x3
1 + x1 + x2 + x3 ,
x1 + x2
1 + x1 + x2 + x3
	
deﬁned over the domain U = R3 −{(x1, x2, x3) | 1 + x1 + x2 + x3 = 0}.
(a) Show that no point in the domain of ⃗f is a critical point. [Hint:
Prove that J(⃗f) = 2/(1 + x1 + x2 + x3)4.]
(b) Prove that ⃗f is injective.
(c) Find [d⃗f −1] in terms of (x1, x2, x3) at every point using the Implicit
Function Theorem.
(d) Show that the inverse function is
⃗f −1(y1, y2, y3) =
 −y1 + y2 + y3
2 −y1 −y2 −y3 ,
y1 −y2 + y3
2 −y1 −y2 −y3 ,
y1 + y2 −y3
2 −y1 −y2 −y3
	
.
(e) Prove that ⃗f is a bijection between
U = R3 −{(x1, x2, x3) | 1 + x1 + x2 + x3 = 0}
and
V = R3 −{(x1, x2, x3) | 2 −x1 −x2 −x3 = 0}.
1.4.9. Verify all the calculations of Example 1.4.8.


CHAPTER
2
Coordinates, Frames,
and Tensor Notation
The strategy of choosing a particular coordinate system or frame to perform
a calculation or to present a concept is ubiquitous in both mathematics and
physics. For example, Newton’s equations of planetary motion are much
easier to solve in polar coordinates than in Cartesian coordinates. In the
diﬀerential geometry of curves, calculations of local properties are often
simpler when carried out in the Frenet frame associated to the curve at a
point. This chapter introduces general coordinate systems on Rn and the
concept of variable frames in a consistent and general manner. With a solid
foundation of coordinate systems, we conclude the chapter by introducing
tensor notation in what one might call the physics style or the classical
style.
2.1
Curvilinear Coordinates
Many problems in introductory mechanics involve ﬁnding the trajectory of
a particle under the inﬂuence of various forces and/or subject to certain
constraints. The ﬁrst approach uses the coordinate functions and describes
the trajectory as
⃗r(t) = (x(t), y(t), z(t)) = x(t)⃗i + y(t)⃗j + z(t)⃗k.
Newton’s equations of motion then lead to diﬀerential equations in the
three coordinate functions x(t), y(t), and z(t).
We point out that
d
dt⃗i = 0,
d
dt⃗j = 0, and
d
dt⃗k = 0. Following the presen-
tation we used to describe how the Frenet frame behaves under derivatives,
we write
d
dt

⃗i
⃗j
⃗k

=

⃗i
⃗j
⃗k

⎛
⎝
0
0
0
0
0
0
0
0
0
⎞
⎠,
37

38
2. Coordinates, Frames, and Tensor Notation
where, as in linear algebra, we view the vectors as columns. This remark
appears trivial but it will become important as we study the behavior of
frames associated to other natural coordinate systems. Alternate coordi-
nate frames are important because when a particle is under the inﬂuence
of a force (radial, for example) or is bound by certain constraints, cylindri-
cal or spherical coordinates oﬀer a simpler description for the equations of
motion.
With cylindrical coordinates (Figure 2.1(a)), one locates a point in R3
using the distance r between the origin and the projection of the point
onto the xy-plane, the angle θ from the positive x-axis, and the height z
above the xy-plane. We have the following relationship between Cartesian
coordinates and cylindrical coordinates:
⎧
⎪
⎨
⎪
⎩
x = r cos θ,
y = r sin θ,
z = z,
←→
⎧
⎪
⎨
⎪
⎩
r =

x2 + y2,
θ = tan−1  y
x

,
z = z.
Of course, by the expression tan−1(y/x), one must understand that we
assume that x > 0. For x ≤0, one must adjust the formula to obtain the
appropriate corresponding angle. Using cylindrical coordinates, one would
locate a point in space by
⃗r = (r cos θ, r sin θ, z).
We deﬁne the natural frame with respect to this coordinate system
as follows. To each independent variable in the coordinate system, one
associates the unit vector that corresponds to the directions of change with
respect to that variable. For example, with cylindrical coordinates, we have
the following three unit vectors:
⃗er =
∂⃗r
∂r

∂⃗r
∂r

,
⃗eθ =
∂⃗r
∂θ

∂⃗r
∂θ

,
⃗ez =
∂⃗r
∂z

∂⃗r
∂z

.
(2.1)
To get explicit descriptions of these unit vectors, we merely compute the
above formulas to get
⃗er = (cos θ, sin θ, 0) = cos θ⃗i + sin θ⃗j,
⃗eθ = (−sin θ, cos θ, 0) = −sin θ⃗i + cos θ⃗j,
(2.2)
⃗ez = (0, 0, 1) = ⃗k.

2.1. Curvilinear Coordinates
39
Of course, we are using the Cartesian frame (⃗i,⃗j,⃗k) to describe this new
basis that corresponds to cylindrical coordinates. As opposed to the ﬁxed
frame (⃗i,⃗j,⃗k), the frames associated to non-Cartesian coordinates depend
on the coordinates of the base point p of the frame.
Thus, the frame
(⃗er,⃗eθ,⃗ez) associated to cylindrical coordinates depends explicitly on the
coordinates (r, θ, z) (in this case, only on θ) of the frame’s origin point.
We apply this treatment of frames associated to non-Cartesian coor-
dinate systems to space curves. Consider a space curve parametrized by
⃗r : I →R3, where I is an interval of R. We can attach the frame (⃗er,⃗eθ,⃗ez)
to each point ⃗r(t) of the curve, but, unlike with the ﬁxed Cartesian frame,
the frame (⃗er,⃗eθ,⃗ez) is not constant. As one rephrases equations of mo-
tion in the new coordinate system, one is led to take higher derivatives
of ⃗r(t) and express them with components in the frame associated to the
particular coordinate system.
Using cylindrical coordinates, a point is located by
⃗r = r⃗er + z⃗ez,
and if one considers a curve in cylindrical coordinates, r, θ, and z are
functions of time t. Therefore, taking the derivative with respect to time
t, we get
⃗r ′ = r′⃗er + r d
dt⃗er + z′⃗ez + z d
dt⃗ez.
Thus, in order to write equations of motion in cylindrical coordinates, one
must determine
d
dt⃗er,
d
dt⃗eθ, and
d
dt⃗ez. One obtains
⃗e ′
r = d
dt(cos θ, sin θ, 0) = (−θ′ sin θ, θ′ cos θ, 0) = θ′⃗eθ,
⃗e ′
θ = d
dt(−sin θ, cos θ, 0) = (−θ′ cos θ, −θ′ sin θ, 0) = −θ′⃗er,
⃗e ′
z = d
dt(0, 0, 1) = ⃗0.
Following the same method of presenting the change in the Frenet frame,
we can write
d
dt
⃗er
⃗eθ
⃗ez

=
⃗er
⃗eθ
⃗ez

⎛
⎝
0
−θ′
0
θ′
0
0
0
0
0
⎞
⎠.

40
2. Coordinates, Frames, and Tensor Notation
x
y
z
θ
r
z
(a) Cylindrical coordinates
x
y
z
θ
ϕ ρ
(b) Spherical coordinates
Figure 2.1. Cylindrical and spherical coordinates.
One can now write, for the velocity vector and acceleration vector,
⃗r ′ = r′⃗er + rθ′⃗eθ + z′⃗ez,
⃗r ′′ = r′′⃗er + r′θ′⃗eθ + r′θ′⃗eθ + rθ′′⃗eθ + r(θ′)2(−⃗er) + z′′⃗ez
=

r′′ −r(θ′)2
⃗er + (2r′θ′ + rθ′′)⃗eθ + z′′⃗ez.
If we restrict ourselves to polar coordinates, the above formula would still
hold but with no z-component. In the study of trajectories in the plane,
the ﬁrst four terms in the last expression have particular names (see [21,
Section 5.2]). One calls
r′′⃗er the radial acceleration,
−r(θ′)2⃗er the centripetal acceleration,
2r′θ′⃗eθ the Coriolis acceleration, and
rθ′′⃗eθ the component due to angular acceleration.
For spherical coordinates (Figure 2.1(b)), we merely need to repeat the
constructions for cylindrical coordinates, only beginning with the appro-
priate relationship between Cartesian and spherical coordinate systems.
Using spherical coordinates, one locates a point P in R3 as follows. Let
P ′ be the projection of P onto the xy-plane. Use the distance from the
origin ρ = OP, longitude θ (i.e., the angle from the positive x-axis to the

2.1. Curvilinear Coordinates
41
ray [OP ′)), and the angle ϕ, which is the angle between the positive z-axis
and the ray [OP). We have the following relationship between Cartesian
coordinates and cylindrical coordinates:
⎧
⎪
⎨
⎪
⎩
x = ρ cosθ sin ϕ,
y = ρ sin θ sin ϕ,
z = ρ cos ϕ,
←→
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
ρ =

x2 + y2 + z2,
θ = tan−1  y
x

,
ϕ = cos−1

z
√
x2+y2+z2
	
,
(2.3)
with the same caveat for θ as discussed with cylindrical coordinates.
It becomes a simple exercise to determine the natural frame for the
spherical coordinate system as
⃗eρ = (cos θ sin ϕ, sin θ sin ϕ, cos ϕ),
⃗eθ = (−sin θ, cos θ, 0),
⃗eϕ = (cos θ cos ϕ, sin θ cos ϕ, −sin ϕ).
Furthermore, if ⃗r : I →R3 is a parametric curve in R3, we can determine
the rate of change of the spherical frame along that curve as
d
dt

⃗eρ
⃗eθ
⃗eϕ

=

⃗eρ
⃗eθ
⃗eϕ

⎛
⎝
0
−θ′ sin ϕ
−ϕ′
θ′ sin ϕ
0
θ′ cos ϕ
ϕ′
−θ′ cos ϕ
0
⎞
⎠.
(2.4)
All the coordinate systems we have considered thus far, though curvi-
linear, are examples of orthogonal coordinate systems, that is to say, the
basis vectors we associate to the coordinate system are mutually perpen-
dicular. In general, this is not the case. We point out that, as shown in
Equation (2.2), both mathematicians and physicists make the traditional
choice when they impose that the frames associated to the cylindrical and
spherical coordinate systems be composed of unit vectors. As useful as this
is for calculations involving distances or angles, this choice has considerable
drawbacks in general. Consequently, in what follows, we do not make this
imposition but show how we must change the formula for distances.
We now consider general coordinate systems in Rn.
Let S be an open set in Rn. A continuous surjective function f : U →S,
where U is an open set in Rn, deﬁnes a coordinate system on S by associ-
ating to every point P ∈S an n-tuple x(P) = (x1(P), x2(P), · · · , xn(P))
such that f(x(P)) = P. In this notation, the superscripts do not indicate
powers of a variable x but the ith coordinate for that point in the given
coordinate system. Though a possible source of confusion at ﬁrst, diﬀer-
ential geometry literature uses superscripts instead of the usual subscripts

42
2. Coordinates, Frames, and Tensor Notation
in order to mesh properly with subsequent tensor notation. One should be
aware that, as with polar coordinates where (r0, θ0) and (r0, θ0 + 2π) cor-
respond to the same point in the plane, the n-tuple need not be uniquely
associated to the point P.
Let (x1, x2, . . . , xn) be a coordinate system in Rn. Now Rn is a vector
space, and we can talk about position vectors of points in Rn. To say that
the n-tuple (x1, x2, . . . , xn) gives coordinates of a point p means that p
has a position vector ⃗r that is a function of (x1, x2, . . . , xn). For a general
system of coordinates, the associated basis of Rn at p is the set of vectors
 ∂⃗r
∂x1

p, ∂⃗r
∂x2

p, . . . , ∂⃗r
∂xn

p
 
.
(2.5)
If there is no cause for confusion, one often drops the |p but understands
from context that the derivatives are evaluated at a given point p. We say
that the components of a vector ⃗A at p in this system of coordinates are
⃗A = (A1, A2, . . . , An) if we can write
⃗A =
n

i=1
Ai
∂⃗r
∂xi
.
In this general setup, one must note that, in general,
∥⃗A∥2 ̸= A2
1 + A2
2 + · · · + A2
n.
The usual length formula no longer holds precisely because the basis vectors
∂⃗r
∂xi are no longer orthonormal. However, if ⃗B = (B1, . . . , Bn) is another
vector given in the basis associated to the coordinate system at p, then
⃗A · ⃗B =
n

i=1
n

j=1
AiBj
∂⃗r
∂xi
· ∂⃗r
∂xj
.
(2.6)
The coeﬃcients
∂⃗r
∂xi ·
∂⃗r
∂xj depend only on p and not on the vectors ⃗A and
⃗B, and since
∂⃗r
∂xi
· ∂⃗r
∂xj
= ∂⃗r
∂xj
· ∂⃗r
∂xi
,
Equation (2.6) is a symmetric bilinear form. We can now use this to calcu-
late lengths of vectors and angles between vectors using their components
in the basis associated to the coordinate system. Though Equation (2.6)
may seem unpleasant, we point out that this formula for calculating dis-
tances meshes perfectly with the formalism of the so-called metric tensor,
which we introduced in Section 6.1 in [5].

2.1. Curvilinear Coordinates
43
x
y
z
P
Figure 2.2. Coordinate planes for parabolic coordinates.
To try to tie together this correct presentation of bases associated to
a general coordinate system with the tradition shown for cylindrical and
spherical coordinates, some authors (e.g., [35, p. 9]) say, for example, that
spherical coordinates (r, θ, ϕ) have scale factors of (1, r sin ϕ, r). By this,
such authors mean that a vector ⃗A based at a point p has components
⃗A = (A1, A2, A3) if
⃗A = A1 ⃗er + A2r sin ϕ⃗eθ + A3r ⃗eϕ.
Problems
2.1.1. Prove Equation (2.4) for the rate of change of the spherical coordinates
frame.
2.1.2. Calculate ⃗r ′ and ⃗r ′′ in terms of functions of spherical coordinates.
2.1.3. The parabolic coordinates system of R3 consists of the triple (u, v, θ), with
u ∈[0, +∞), v ∈[0, +∞), and θ ∈[0, 2π) with equations
⎧
⎪
⎨
⎪
⎩
x = uv cos θ,
y = uv sin θ,
z = 1
2(u2 −v2).
Figure 2.2 shows the three coordinate “planes” for parabolic coordinates
in R3 passing through the point P ∈R3 with coordinates (u, v, θ) =
(1, 1/2, π/4).

44
2. Coordinates, Frames, and Tensor Notation
(a) Find the rate of change of coordinate functions that go from Carte-
sian coordinates to parabolic coordinates.
(b) Find the basis vectors for the associated frame according to Equa-
tion (2.5).
(c) Consider also the basis {⃗eu,⃗ev,⃗eθ} that involves unit vectors using
Equation (2.1). Calculate the rate of change matrix for this frame.
2.1.4. Consider the coordinate system on R2 that employs the pair (r, α) ∈
[0, +∞) × [0, 2π) to represent the point on the ellipse
x2
4 + y2 = r2
that lies on the ray that comes out of the origin and goes through (cos α,
sin α).
(a) Determine change of coordinate system equations from and to Carte-
sian coordinates.
(b) Find the set B of basis vectors for the associated frame according to
Equation (2.5).
(c) Calculate the rate of change matrix associated to this frame B.
2.2
Moving Frames in Physics
In many instances in physics, it is convenient to use a frame that is diﬀerent
from the Cartesian frame.
Changing types of frames sometimes makes
diﬃcult integrals tractable or makes certain diﬃcult diﬀerential equations
manageable.
In Chapter 1 in [5], we used the {⃗T, ⃗U} frame to study the local prop-
erties of a plane curve ⃗x(t). The vector ⃗T (t) is the unit tangent vector
⃗T(t) = ⃗x′(t)/∥⃗x′(t)∥, and the unit normal vector ⃗U(t), is the result of
rotating ⃗T(t) by π/2 in a counterclockwise direction.
This is a moving
frame that is deﬁned in terms of a given regular curve ⃗x(t) and, at t = t0,
is viewed as based at the point ⃗x(t0). To compare with applications in
physics, it is important to note that the {⃗T, ⃗U} frame is not the same as
the polar coordinate frame {⃗er,⃗eθ}. From Equation (2.2) (and ignoring the
z-component), we know that
⃗er = (cos θ, sin θ)
and
⃗eθ = (−sin θ, cos θ).
Assuming that x, y, r, and θ are functions of t and since x = r cos θ and
y = r sin θ, we have
⃗x ′(t) = (x′(t), y′(t)) = (r′ cos θ −rθ′ sin θ, r′ sin θ + rθ′ cos θ) = r′⃗er + rθ′⃗eθ.

2.2. Moving Frames in Physics
45
We then calculate the speed function to be
s′(t) = ∥⃗x′(t)∥=

(r′)2 + r2(θ′)2
and ﬁnd the unit tangent and unit normal vectors to be
⃗T =
1

(r′)2 + r2(θ′)2 (r′⃗er + rθ′⃗eθ) ,
⃗U =
1

(r′)2 + r2(θ′)2 (−rθ′⃗er + r′⃗eθ) .
Therefore, the orthogonal matrix
1

(r′)2 + r2(θ′)2
 r′
−rθ′
rθ′
r′
	
is the transition matrix between the {⃗T, ⃗U} basis and the {⃗er,⃗eθ} basis.
Parenthetically, it is now not diﬃcult to obtain a formula for the plane
curvature of ⃗x(t) in terms of the functions r(t) and θ(t). We use either of
the formulations
κg(t) =
1
s′(t)
⃗T ′ · ⃗U =
1
(s′(t))3 (⃗x′ × ⃗x′′) · ⃗k,
and we ﬁnd that
κg(t) = −rr′′θ′ + r2(θ′)3 + 2(r′)2θ′ + rr′θ′′
((r′)2 + r2(θ′)2)3/2
.
(2.7)
In general, to understand dynamics with respect to a moving frame, one
considers the trajectory ⃗x : I →R3 of a particle in reference to a moving
frame F. Suppose that the base of the frame moves along the trajectory
α(t) and that the basis vectors {⃗e1,⃗e2,⃗e3} of the frame F vary with time
but always form an orthonormal set. In addition, we will always suppose
that this basis is labeled so that it is a positively oriented basis, that is, it
satisﬁes ⃗e1 × ⃗e2 = ⃗e3. Now. for all t,
⃗ei · ⃗ej =

1,
if i = j,
0,
if i ̸= j,
so
⃗e ′
i · ⃗ej =

0,
if i = j,
−⃗ei · ⃗e ′
j,
if i ̸= j.

46
2. Coordinates, Frames, and Tensor Notation
Thus, there exists a vector ⃗ω, namely,
⃗ω = (⃗e ′
2 · ⃗e3)⃗e1 + (⃗e ′
3 · ⃗e1)⃗e2 + (⃗e ′
1 · ⃗e2)⃗e3
(2.8)
such that
⃗e ′
i = ⃗ω × ⃗ei
for all i. This vector is the angular velocity vector of the moving frame F.
We now want to determine the perceived position, velocity, and ac-
celeration vectors in the frame F in terms of the true position, velocity,
and acceleration. Label (⃗x)F, (⃗x′)F, and (⃗x′′)F as the perceived position,
velocity, and acceleration vectors. Firstly,
(⃗x)F = ⃗x −⃗α,
(2.9)
but the perceived velocity and acceleration of ⃗x are obtained by taking the
derivatives of the components of (⃗x)F in {⃗e1,⃗e2,⃗e3}. More explicitly, we
have
(⃗x)F = ((⃗x −⃗α) · ⃗e1)⃗e1 + ((⃗x −⃗α) · ⃗e2)⃗e2 + ((⃗x −⃗α) · ⃗e3)⃗e3,
(⃗x ′)F = d
dt ((⃗x −⃗α) · ⃗e1)⃗e1 + d
dt ((⃗x −⃗α) · ⃗e2)⃗e2 + d
dt ((⃗x −⃗α) · ⃗e3)⃗e3,
(⃗x ′)F = d2
dt2 ((⃗x −⃗α) · ⃗e1)⃗e1 + d2
dt2 ((⃗x −⃗α) · ⃗e2)⃗e2 + d2
dt2 ((⃗x −⃗α) · ⃗e3)⃗e3.
To understand dynamics in the moving frame F, one must relate the
perceived position, velocity, and acceleration to the position, velocity, and
acceleration with respect to the ﬁxed frame {⃗i,⃗j,⃗k}. By Equation (2.9),
we have
⃗x = (⃗x)F + ⃗α.
Then for the velocity,
⃗x′ = d
dt(⃗x)F + ⃗α′
=
 3

i=1
d
dt((⃗x −⃗α) · ⃗ei)⃗ei

+
 3

i=1
((⃗x −⃗α) · ⃗ei)⃗e′
i

+ ⃗α′
(2.10)
= (⃗x′)F +
 3

i=1
((⃗x −⃗α) · ⃗ei)⃗ω × ⃗e

+ ⃗α′
= (⃗x′)F + ⃗ω × (⃗x)F + ⃗α′.

2.2. Moving Frames in Physics
47
For the acceleration,
⃗x′′ = d
dt(⃗x′)F + d
dt (⃗ω × (⃗x)F) + ⃗α′′
= d
dt
 3

i=1
((⃗x −⃗α) · ⃗ei)⃗ei

+ d⃗ω
dt × (⃗x)F + ⃗ω × d
dt(⃗x)F + ⃗α′′
=
 3

i=1
d2
dt2 ((⃗x −⃗α) · ⃗ei)⃗ei

+
 3

i=1
d
dt((⃗x −⃗α) · ⃗ei)⃗e′
i

+ ⃗ω′ × (⃗x)F + ⃗ω × d
dt(⃗x)F + ⃗α′′
= (⃗x′′)F + ⃗ω × (⃗x′)F + ⃗ω′ × (⃗x)F + ⃗ω × ((⃗x′)F + ⃗ω × (⃗x)F) + ⃗α′′,
where the second-to-last term follows from Equation (2.10). Thus,
⃗x′′ = (⃗x′′)F + 2⃗ω × (⃗x′)F + ⃗ω′ × (⃗x)F + ⃗ω × (⃗ω × (⃗x)F) + ⃗α′′.
(2.11)
All of the above terms have names in physics (see [21, p. 118]). The term
(⃗x′′)F is called the perceived acceleration or acceleration with respect to F.
The component 2⃗ω×(⃗x′)F is the Coriolis acceleration, while ⃗ω×(⃗ω×(⃗x)F)
is the centripetal acceleration. The term ⃗ω′ × (⃗x)F is sometimes called the
transverse acceleration because it is perpendicular to the perceived position
vector (⃗x)F. Finally, the vector ⃗α′′ is the translational acceleration of the
frame.
As an example of the application of diﬀerential geometry of curves to
physics, we consider the notion of centripetal acceleration of a curve and
its relation to the Frenet frame.
Example 2.2.1 (Centripetal Acceleration of Curves).
One ﬁrst encounters cen-
tripetal acceleration in the context of a particle moving around on a circle
with constant speed v, and one deﬁnes it as the acceleration due to the
change in the velocity vector. Phrasing the scenario mathematically, con-
sider a particle moving along the trajectory with equations of motion
⃗x(t) = (R cos(ωt), R sin(ωt)),
where R is the radius of the circle and ω is the (constant) angular speed.
The velocity, speed and acceleration are, respectively,
⃗x′(t) = (−Rω sin(ωt), Rω cos(ωt)),
s′(t) = v = Rω,
⃗x′′(t) = (−Rω2 cos(ωt), −Rω2 sin(ωt)).

48
2. Coordinates, Frames, and Tensor Notation
Hence, the acceleration is
⃗x′′(t) = −ω2⃗x(t) = −ω2R⃗er = −v2
R ⃗er,
(2.12)
where ⃗er is the unit vector in the radial direction (see Equation (2.2)). This
is the centripetal acceleration for circular motion, often written ⃗ac.
The angular velocity vector ⃗ω is the vector of magnitude ω that is
perpendicular to the plane of rotation and with direction given by the right-
hand rule. Thus, taking ⃗k as the direction perpendicular to the plane, we
have in this simple setup ⃗ω = ω⃗k. Setting the radial vector ⃗R = r⃗er, it is
not hard to show that for this circular motion,
⃗ac = ⃗ω × (⃗ω × ⃗R),
as expected from Equation (2.11).
Now consider a general curve in space ⃗x : I →R3, where I is an interval
of R. We recall a few diﬀerential geometric properties of space curves. The
derivative ⃗x ′(t) is called the velocity, and s′(t) = ∥⃗x ′(t)∥is called the speed.
The curve ⃗x(t) is called regular at t if ⃗x′(t) ̸= ⃗0. At all regular points of a
curve, we deﬁne the unit tangent as ⃗T(t) = ⃗x ′(t)/∥⃗x ′(t)∥. Because ⃗T(t) is
a unit vector for all t, ⃗T ′(t) is perpendicular to ⃗T(t).
The curvature of the curve is the unique nonnegative function κ(t) such
that
⃗T ′(t) = s′(t)κ(t)⃗P (t)
(2.13)
for some unit vector ⃗P(t). The vector function ⃗P(t) is called the principal
normal vector. Finally, we deﬁne the binormal vector function ⃗B(t) by
⃗B = ⃗T × ⃗P. In so doing, we have deﬁned an orthonormal set {⃗T, ⃗P, ⃗B}
associated to each point of the curve ⃗x(t). This set {⃗T, ⃗P, ⃗B} is called the
Frenet frame.
It is not hard to show that, by construction, the derivative ⃗B ′(t) is
perpendicular to ⃗B and to ⃗T.
We deﬁne the torsion function τ(t) of a
space curve as the unique function such that
⃗B ′(t) = −s′(t)τ(t)⃗P (t).
(2.14)
Finally, from Equations (2.13) and (2.14), we get that
⃗P ′(t) = −s′(t)κ(t)⃗T (t) + s′(t)τ(t) ⃗B(t).
(The above paragraphs only give the deﬁnitions of the concepts we will
use here. A full treatment of these topics can be found in Chapter 3 of [5].)

2.2. Moving Frames in Physics
49
x
y
z
 ⃗α(t)
⃗T(t)
⃗P(t)
⃗B(t)
  ⃗x(t)
Figure 2.3. Center of curvature and osculating circle.
Since a space curve is not necessarily circular, one cannot use Equa-
tion (2.12) to determine the centripetal acceleration of ⃗x. Instead, we view
⃗x in relation to an appropriate moving frame in which centripetal acceler-
ation makes sense. The osculating circle is the unique circle of maximum
contact with the curve ⃗x(t) at any point t, and hence, the appropriate frame
F is based at the center of curvature
⃗α(t) = ⃗x(t) +
1
κ(t)
⃗P(t)
and has the vectors of the Frenet frame {⃗T, ⃗P, ⃗B} as its basis. Figure 2.3
depicts a space curve along with the center of curvature ⃗α(t) and the os-
culating circle associated to a point ⃗x(t) on the curve.
By Equation (2.8), the angular velocity vector of F is
⃗ω = (⃗P ′ · ⃗B)⃗T + ( ⃗B′ · ⃗T)⃗P + (⃗T ′ · ⃗P) ⃗B
= −s′τ ⃗T + s′κ ⃗B.
The relative position vector for the curve ⃗x with reference to its center of
curvature is ⃗R = (⃗x)F = −1
κ ⃗P. Therefore, the centripetal acceleration is

50
2. Coordinates, Frames, and Tensor Notation
⃗ac = ⃗ω × (⃗ω × ⃗R)
= ⃗ω ×

(−s′τ ⃗T + s′κ ⃗B) × (−1
κ
⃗P)
	
= (−s′τ ⃗T + s′κ ⃗B) × (s′ ⃗T + s′ τ
κ
⃗B)
= (s′)2κ⃗P + (s′)2 τ 2
κ
⃗P
= (s′)2 κ2 + τ2
κ
⃗P.
(2.15)
It is interesting to note that if a curve happens to be planar, then τ = 0,
and the centripetal acceleration becomes ⃗ac = (s′)2κ⃗P, which matches
Equation (2.12) exactly since s′ = v and κ is the reciprocal of the radius of
curvature,
1
R. However, Equation (2.15) shows that, for a curve in space,
the “corkscrewing” eﬀect, measured by τ, produces a greater centripetal
acceleration than does simply rotating about the same axis. (Hence, on a
roller coaster, a rider will experience more centrifugal force—the force that
balances out centripetal acceleration—if the roller coaster corkscrews than
when it simply rotates around with the same radius of curvature.)
Example 2.2.2 (Radial Forces). As an application of cylindrical coordinate sys-
tems, we can study Newton’s equation of motion applied to a particle un-
der the inﬂuence of a radial force. By deﬁnition, a force is called radial if
⃗F(⃗r) = f(r)⃗er, that is, if the force only depends on the distance from an
origin and is parallel to the position vector ⃗r. (The force of gravity be-
tween two point objects and the electric force between two charged point
objects are radial forces, while the magnetic force on a charged particle
is not.)
Newton’s law of motion produces the following vector diﬀerential equa-
tion:
m⃗r ′′ = f(r)⃗er.
In order to solve this diﬀerential equation explicitly, one needs the initial
position ⃗r0 and the initial velocity ⃗v0.
For convenience, choose a plane P that goes through the origin and
is parallel to both ⃗r0 and ⃗v0.
(If ⃗r0 and ⃗v0 are not parallel, then this
information deﬁnes a unique plane in R3. If ⃗r0 and ⃗v0 are parallel, then
any plane parallel to these vectors suﬃces.) Consider P to be the xy-plane,
choose any direction for the ray [Ox), and now use cylindrical coordinates
in R3.

2.2. Moving Frames in Physics
51
For radial forces, Newton’s law of motion written in the cylindrical
frame as three diﬀerential equations is
⎧
⎪
⎨
⎪
⎩
⃗er :
m(r′′ −r(θ′)2) = f(r),
⃗eθ :
m(2r′θ′ + rθ′′) = 0,
⃗ez :
0 = 0.
(2.16)
Obviously, since ⃗r0 and ⃗v0 lie in the plane through the origin and parallel
to ⃗er and ⃗eθ, the equations show that ⃗r(t) never leaves the xy-plane. Thus,
z(t) = 0.
One can now solve the second diﬀerential equation in the above system
in such a way as to obtain a relationship between the functions r and θ.
First write
2r′
r
= −θ′′
θ′ .
Integrating both sides with respect to t, we then obtain 2 ln |r| = −ln |θ′|+
C, where C is some constant of integration. Taking the exponential of both
sides, one obtains the relationship r2θ′ = h where h is a constant. In terms
of the initial conditions, one has
⃗r × ⃗r ′ = r2θ′⃗ez
and therefore, for all time t, we have
h = (⃗r0 × ⃗v0) · ⃗ez.
Thus, one concludes that the quantity ⃗L = ⃗r × (m⃗v) = m(⃗r × ⃗v), which is
called the angular momentum and in general depends on t, is a constant
vector function for radial forces.
Finally, to solve the system in Equation (2.16) completely, it is conve-
nient to substitute variables and write the ﬁrst equation in terms of u = 1
r
and θ. Since r = 1
u, we have
dr
dt = −1
u2
du
dt = −1
u2
dθ
dt
du
dθ = −hdu
dθ .
The second derivative of r gives
d2r
dt2 = −h d
dt
du
dθ
	
= −hdθ
dt
d2u
dθ2 = −h2u2 d2u
dθ2 .
The ﬁrst part of Equation (2.16) becomes
d2u
dθ2 + u = −
1
mh2u2 f(u−1).
(2.17)

52
2. Coordinates, Frames, and Tensor Notation
If the radial force in question is an inverse-square law (such as the force
of gravity and the electrostatic force caused by a point charge), then the
radial force is of the form
f(r) = −k
r2 .
In this case, Equation (2.17) becomes
d2u
dθ2 + u = −
k
mh2 .
Techniques from diﬀerential equations show that the general solution to
this equation is
u(θ) = −k
mh2 + C cos(θ −θ0),
where C and θ0 are constants of integration that depend on the original
position and velocity of the point particle under the inﬂuence of this radial
force. In polar coordinates, this gives the equation
r(θ) =
1
−
k
mh2 + C cos(θ −θ0).
(2.18)
Problems
2.2.1. Provide the details for the proof of Equation (2.7).
2.2.2. Prove that Equation (2.8) is the correct vector to satisfy ⃗e′
i = ⃗ω × ⃗ei for
all i.
2.2.3. Determine the transition matrix between the cylindrical coordinate frame
and the Frenet frame.
2.2.4. Calculate the curvature and torsion of a space curve deﬁned by the func-
tions, in cylindrical coordinates, (r, θ, z) = (r(t), θ(t), z(t)).
2.2.5. Determine the transition matrix between the spherical coordinate frame
and the Frenet frame.
2.2.6. Calculate the curvature and torsion of a space curve deﬁned by the func-
tions, in spherical coordinates, (r, θ, φ) = (r(t), θ(t), φ(t)).
2.2.7. Determine the transition matrix between the parabolic coordinate frame
and the Frenet frame (see Problem 2.1.3).
2.2.8. Consider the solution r(θ) in Equation (2.18). Determine h, C, and θ0 in
terms of some initial conditions for position and velocity ⃗r(0) and ⃗v(0).
Prove that for diﬀerent initial conditions and diﬀerent values of the con-
stants, the locus of Equation (2.18) is a conic. State under what conditions
one obtains a circle, ellipse, parabola, and hyperbola.
2.2.9. (ODE) Find the locus of the trajectory of a particle moving under the
eﬀect of a radial force that is an inverse cube, i.e., f(r) = −k/r3.

2.3. Moving Frames and Matrix Functions
53
2.3
Moving Frames and Matrix Functions
In the preceding sections, we encountered many examples of moving frames.
In the local theory of plane curves [5, Chapter 1], one uses the {⃗T, ⃗U} frame
and, in this section, we reviewed the polar frame {⃗er,⃗eθ}, both of which are
considered to be attached at a point of a regular plane curve ⃗x : I →R2.
Similarly, in the local theory of space curves (see [5, Section 3.2]), one
studies the Frenet frame while, in this section, we reviewed the frames
associated to cylindrical and spherical coordinates, all three of which are
considered to be based at a point.
All of these scenarios are examples of moving frames and, more precisely,
orthonormal moving frames (though Problem 2.1.4 presented an example
where the coordinate frame is not orthonormal). In all ﬁve of these exam-
ples of orthonormal moving frames attached to a curve, the rate of change
of the frame was related to the frame by multiplication of an antisymmetric
matrix, i.e., a matrix A such that AT = −A, where AT is the transpose
of A. This is no coincidence but depends only on the fact that the moving
frame is orthonormal, as we shall see shortly.
An orthonormal basis in Rn is any n-tuple of vectors (⃗u1, ⃗u2, . . . , ⃗un)
such that
⃗ui · ⃗uj =

1,
if i = j,
0,
if i ̸= j.
(2.19)
One easily conﬁrms that if one considers the ⃗ui as column vectors, then
the matrix
M =
⎛
⎝
|
|
|
⃗u1
⃗u2
· · ·
⃗un
|
|
|
⎞
⎠
is an orthogonal n × n matrix, that is, it satisﬁes
M T M = In
where In is the n × n identity matrix. Since M T M = In, an orthogonal
matrix M satisﬁes det(M) = ±1. An orthonormal basis (⃗u1, ⃗u2, . . . , ⃗un) is
called positively oriented if det(M) = 1 and negatively oriented otherwise.
The set of orthogonal n × n matrices is denoted by O(n), and the set of
positive orthogonal matrices is denoted by
SO(n) = {M ∈O(n) | det(M) = 1 }.
(Note that the order of the basis vectors in the n-tuple (⃗u1, ⃗u2, . . . , ⃗un)
matters because a permutation of these vectors may change the sign of the

54
2. Coordinates, Frames, and Tensor Notation
determinant of the corresponding matrix M. Consequently, we must talk
about an n-tuple of vectors as opposed to just a set of vectors. One should
also be aware that a permutation of vectors in the basis B = (⃗u1, ⃗u2, . . . , ⃗un)
would lead to another basis B′ that consists of the same set of vectors but
has a coordinate transition matrix that is a permutation matrix.)
One can therefore deﬁne a moving orthonormal frame in Rn in two
equivalent ways:
as an n-tuple of continuous vector functions (⃗u1(t),
⃗u2(t), . . . , ⃗un(t)) that satisfy the condition in Equation (2.19) or as a contin-
uous function M : I →O(n). In the latter way of viewing an orthonormal
frame, the notion of continuity for a function F : R →O(n) makes sense
when one views O(n) as a subset of the vector space of n × n matrices,
which is isomorphic as a vector space to Rn2. (The set O(n) has the sub-
set topology induced from the Euclidean topology on Rn2.) Furthermore,
we call the moving frame diﬀerentiable if all of the coordinate functions
involved are diﬀerentiable.
Proposition 2.3.1. Let I ⊂R be an interval, and let M : I →O(n) be a
diﬀerentiable function. Viewing M(t) as an orthogonal matrix for all t,
then for all t ∈I, we have
M ′(t) = M(t)A(t)
where A(t) is an antisymmetric matrix.
Proof: Since for all t ∈I, the frame (⃗u1(t), ⃗u2(t), . . . , ⃗un(t)) is a basis of
Rn, one can write ⃗u′
i(t) as a linear combination of these basis vectors (with
coeﬃcients that depend on t).
Thus, we can write M ′(t) = M(t)A(t),
where the ith column of A(t) are the coordinates of ⃗u ′
i in the moving basis.
Furthermore, taking the derivative of the condition in Equation (2.19), we
obtain

2⃗u ′
i(t) · ⃗ui(t) = 0,
⃗u ′
i(t) · ⃗uj(t) + ⃗ui(t) · ⃗u ′
j(t) = 0,
⇐⇒

⃗ui(t) · ⃗u ′
i(t) = 0,
⃗ui(t) · ⃗u ′
j(t) = −⃗uj(t) · ⃗u ′
i(t).
(2.20)
Since M(t) is an orthogonal matrix, then
A(t) = M(t)−1M ′(t) = M(t)T M ′(t),
and the conditions in Equation (2.20) imply that
A(t)T = M ′(t)T M(t) = −M(t)T M ′(t) = −A(t).
Hence, A(t) is an antisymmetric matrix for all t ∈I.
□

2.3. Moving Frames and Matrix Functions
55
Proposition 2.3.1 is only a particular case of a broader fact. Identifying
the set of m × n matrices with Rmn, one can consider continuous functions
of matrices as curves γ : I →Rmn, where I is an interval of R. As with any
parametrized curve, the derivative γ′(t) is taken component-wise, which as
matrices means entry-wise.
Proposition 2.3.2. Let γ1(t) and γ2(t) be matrix functions deﬁned over an
interval I, and let A be any constant matrix. Assuming the operations are
deﬁned, the following identities hold:
1.
d
dt(A) is the 0-matrix of the same dimensions of A.
2.
d
dt(Aγ1(t)) = Aγ′
1(t) and
d
dt(γ1(t)A) = γ′
1(t)A.
3.
d
dt(γ1(t) + γ2(t)) = γ′
1(t) + γ′
2(t).
4.
d
dt

γ1(t)T 
= (γ′
1(t))T .
5.
d
dt(γ1(t)γ2(t)) = γ′
1(t)γ2(t) + γ1(t)γ′
2(t).
6. If γ1(t) is invertible for all t, then d
dt

γ1(t)−1
=−γ1(t)−1γ′
1(t)γ1(t)−1.
Proof: (Left as exercises for the reader.)
□
Since the derivative of a constant matrix is the zero matrix, it is obvious
that the rank of a matrix function and the invertibility of a square matrix
function do not “commute” with diﬀerentiation (see Problem 2.3.2).
Problems
2.3.1. Let A be a constant invertible n × n matrix, and let γ : I →Rn2 be a
matrix function. Suppose that γ(t0) = In (the n× n identity matrix), and
suppose that for all t, the identity γ(t)T Aγ(t) = I holds. Find an equation
that γ′(t0) satisﬁes.
2.3.2. Find an example of an n×n-matrix function γ(t) such that f ′(t) is never 0,
where f(t) = det(γ(t)), but such that det(γ′(t)) = 0 for all t.
2.3.3. Let γ : I →Rm×n be a diﬀerentiable matrix function, and let f : J →I
be a diﬀerentiable function.
Prove the chain rule for matrix functions,
namely,
d
dt (γ(f(t))) = γ′(f(t)) f ′(t).
2.3.4. Let A(t) and B(t) be two n × n matrix functions deﬁned over an interval
I ⊂R.
(a) Suppose that A(t) and B(t) are similar for all t ∈I. Prove that A′(t)
and B′(t) are not necessarily similar.

56
2. Coordinates, Frames, and Tensor Notation
(b) Suppose that A(t) and B(t) are similar in that B(t) = SA(t)S−1
for some ﬁxed invertible matrix S. Prove that A′(t) and B′(t) are
similar.
(c) Suppose that A(t0) = 0. Prove that A′(t0) and B′(t0) are similar.
2.3.5. Prove Proposition 2.3.2.
2.3.6. Let A(t) be an n × n matrix function that is antisymmetric (i.e., A =
−AT ) for all t. Suppose that M(t) is an n × n matrix function such that
M ′(t) = M(t)A(t). Prove that M T (t)M(t) is constant.
2.3.7. Recall that given a square matrix A, the exponential eA is deﬁned by
eA =
∞

i=0
1
i!Ai
and that this inﬁnite series converges for all matrices A.
(a) Prove that
d
dt(eAt) = AeAt.
(b) If A(t) is a matrix function, is it always true that
d
dt(eA(t)) = A′(t)eA(t)?
2.4
Tensor Notation
In the local theory of surfaces, one encounters the ﬁrst fundamental form,
which is also called the metric tensor of the regular surface. In an intro-
ductory physics course, one may encounter the moment of inertia tensor.
At the time one ﬁrst encounters these objects, one usually does not have
the time to discuss what a tensor is in general.
Mathematicians and physicists often present tensors and the tensor
product in very diﬀerent ways, sometimes making it diﬃcult for a reader to
see that authors in diﬀerent ﬁelds are talking about the same thing. In this
section, we introduce classical tensor notation by emphasizing how com-
ponents of objects change under a coordinate transformation. Readers of
mathematics who are well acquainted with tensor algebras on vector spaces
might ﬁnd this approach unsatisfactory, but physicists should recognize it.
In later chapters, as we study analysis on manifolds, we will mesh this
current approach to tensors with the modern mathematical presentation of
tensors. Appendix C on multilinear algebra provides the algebraic back-
ground behind the tensor product and tensor algebra. In particular, Sec-
tion C.3 explains the duality between covariance and contravariance while,
Section C.4 explains the linear algebra of tensor products of vector spaces.
(For readers who have also read [5], this section is essentially the same as
Section 7.1 of that text.)

2.4. Tensor Notation
57
2.4.1
Transformations of Coordinate Systems
Let S be an open set in Rn, and consider two coordinate systems on S, rel-
ative to which the coordinates of a point P are denoted by (x1, x2, . . . , xn)
and (¯x1, ¯x2, . . . , ¯xn). Suppose that the open set U ⊂Rn parametrizes S
in the (x1, x2, . . . , xn) system and that the open set V ⊂Rn parametrizes
S in the (¯x1, ¯x2, . . . , ¯xn) system. We assume that there exists a bijective
change of coordinates function F : U →V so that we can write
(¯x1, ¯x2, . . . , ¯xn) = F(x1, x2, . . . , xn).
Again using the superscript notation, we might write explicitly
⎧
⎪
⎪
⎨
⎪
⎪
⎩
¯x1
= F 1(x1, x2, . . . , xn),
...
¯xn
= F n(x1, x2, . . . , xn),
where F i are functions from U to R. We will assume from now on that
the change-of-variables function F is always of class C2, i.e., that all the
second partial derivatives are continuous. Unless it becomes necessary for
clarity, one often abbreviates the notation and writes
¯xi = ¯xi(xj),
by which one understands that the coordinates (¯x1, ¯x2, . . . , ¯xn) functionally
depend on the coordinates (x1, x2, . . . , xn). Therefore, one writes ∂¯xi/∂xj
for ∂F i/∂xj, and the matrix of the diﬀerential dFP (see Proposition 1.2.14)
is given by
(dFP )ij =
 ∂¯xi
∂xj
	
.
Just as the functions ¯xi = ¯xi(xj) represent the change of variables F,
we write xj = xj(¯xk) to indicate the component functions of the inverse
F −1 : V →U. In this notation, the Inverse Function Theorem (Theorem
1.4.5) states that, as matrices,
∂xj
∂¯xi
	
=
 ∂¯xi
∂xj
	−1
,
(2.21)
where we assume that the functions in the ﬁrst matrix are evaluated at p in
the (¯x1, . . . , ¯xn)-coordinate system while the functions in the second matrix
are evaluated at p in the (x1, . . . , xn)-coordinate system. One can express

58
2. Coordinates, Frames, and Tensor Notation
the same relationship in an alternate way by writing xi = xi(¯xj(xk)) and
applying the chain rule when diﬀerentiating with respect to xk as follows:
∂xi
∂xk = ∂xi
∂¯x1
∂¯x1
∂xk + ∂xi
∂¯x2
∂¯x2
∂xk + · · · + ∂xi
∂¯xn
∂¯xn
∂xk =
n

j=1
∂xi
∂¯xj
∂¯xj
∂xk .
However, by deﬁnition of a coordinate system in Rn, there must be no
functional dependance of one variable on another so
∂xi
∂xk = δi
k,
where δi
k is the Kronecker delta symbol deﬁned by
δi
j =

1,
if i = j,
0,
if i ̸= j.
Therefore, since δi
j are essentially the entries of the identity matrix, we
conclude that
n

j=1
∂xi
∂¯xj
∂¯xj
∂xk = δi
k
(2.22)
and hence recover Equation (2.21).
Though we have presented the notion of curvilinear coordinates in gen-
eral, one should keep in mind linear coordinate changes, namely, where
⎛
⎜
⎜
⎜
⎝
¯x1
¯x2
...
¯xn
⎞
⎟
⎟
⎟
⎠= M
⎛
⎜
⎜
⎜
⎝
x1
x2
...
xn
⎞
⎟
⎟
⎟
⎠,
where M is an n × n matrix. If either of these coordinate systems is given
as coordinates in a basis of Rn, then the other system simply corresponds
to a change of basis in Rn. It is easy to conﬁrm that the transition matrix
is then
 ∂¯xi
∂xj
	
= M,
the usual basis transition matrix. Furthermore, (∂¯xi/∂xj) is constant over
all Rn.
Example 2.4.1 (Spherical Coordinates). Let us call (x1, x2, x3) = (x, y, z) the
Cartesian system of coordinates and (¯x1, ¯x2, ¯x3) = (ρ, θ, ϕ) the spherical

2.4. Tensor Notation
59
coordinate system (recall Figure 2.1(b)). Then from the coordinate-change
function given by Equation (2.3), one shows that the diﬀerential of the
transformation is
∂xj
∂¯xi
	
=
⎛
⎝
cos θ sin ϕ
−ρ sin θ sin ϕ
ρ cos θ cos ϕ
sin θ sin ϕ
−ρ cosθ sin ϕ
ρ sin θ cos ϕ
cos ϕ
0
−ρ sin ϕ
⎞
⎠.
(2.23)
2.4.2
Tensors: Deﬁnition and Notation
We are now in a position to discuss how components of various quantities
deﬁned locally, namely, in a neighborhood U of a point p ∈R3, change un-
der a coordinate transformation on U. As mentioned before, our deﬁnitions
do not possess the usual mathematical ﬂavor and the supporting discussion
might feel like a game of symbols, but it is important to understand the
transformational properties of tensor components even before becoming fa-
miliar with the machinery of linear algebra of tensors. We begin with the
simplest situation.
Deﬁnition 2.4.2. Let p ∈Rn, and let U be a neighborhood of p. Suppose
that (x1, x2, . . . , xn) and (¯x1, ¯x2, . . . , ¯xn) are two systems of coordinates
on U. A function f(x1, . . . , xn) given in the (x1, x2, . . . , xn)-coordinates is
said to be a scalar if its transformation ¯f(¯x1, . . . , ¯xn) in the (¯x1, . . . , ¯xn)-
coordinates has the same numerical value. In other words,
¯f(¯x1, . . . , ¯xn) = f(x1, . . . , xn).
In Deﬁnition 2.4.2, it is understood that the coordinates (x1, x2, . . . , xn)
and (¯x1, ¯x2, . . . , ¯xn) refer to the same point p in Rn.
This deﬁnition might appear at ﬁrst glance not to hold much content in
that every function deﬁned in reference to some coordinate system should
possess this property, but that is not true. Suppose that f is a scalar. The
quantity that gives the derivative of f in the ﬁrst coordinate is not a scalar,
for though ¯f(¯x1, . . . , ¯xn) = f(x1, . . . , xn), we have
∂¯f
∂¯x1 = ∂f
∂x1
∂x1
∂¯x1 + ∂f
∂x2
∂x2
∂¯x1 + · · · + ∂f
∂xn
∂xn
∂¯x1 ,
which in general is not ∂f/∂x1.
As a second example of how quantities change under coordinate trans-
formations, we consider the gradient ⃗∇f of a diﬀerentiable scalar function f.

60
2. Coordinates, Frames, and Tensor Notation
Recall that
⃗∇f =
 ∂f
∂x1 , ∂f
∂x2 , . . . , ∂f
∂xn
	
in usual Cartesian coordinates. The gradient is a vector ﬁeld, or we may
simply consider the gradient of f at P, namely, ⃗∇fP , which is a vector. We
highlight the transformational properties of the gradient. The chain rule
gives
∂¯f
∂¯xj =
n

i=1
∂xi
∂¯xj
∂f
∂xi .
(2.24)
However, it turns out that this is not the only way components of what
we usually call a “vector” can change under a coordinate transformation.
Again, consider two coordinate systems (x1, x2, . . . , xn) and (¯x1,
¯x2, . . . , ¯xn) on an open set of Rn.
Let ⃗A be a vector in Rn, which we
consider based at P. The components of the vector ⃗A in the respective
coordinate systems is (A1, . . . , An) and ( ¯A1, . . . , ¯An), where
⃗A =
n

i=1
Ai ∂⃗r
∂xi =
n

j=1
¯Aj ∂⃗r
∂¯xj .
Since ∂⃗r/∂xi = "
j(∂⃗r/∂¯xj)(∂¯xj/∂xi), we ﬁnd that the components of ⃗A
in the two systems of coordinates are related by
¯Aj =
n

i=1
∂¯xj
∂xi Ai.
(2.25)
Example 2.4.3 (Velocity in Spherical Coordinates). Consider a space curve that
is parametrized by ⃗r(t) for t ∈I. The chain rule allows us to write the
velocity vector in spherical or Cartesian coordinates as
⃗r ′(t) = ρ′(t)∂⃗r
∂ρ + θ′(t)∂⃗r
∂θ + ϕ′(t) ∂⃗r
∂ϕ
= x′(t)⃗i + y′(t)⃗j + z′(t)⃗k.
However, diﬀerentiating Equation (2.3), assuming ρ, θ, and ϕ are functions
of t, allows us to identify the Cartesian coordinates of the velocity vector as
⎛
⎝
x′
y′
z′
⎞
⎠=
⎛
⎝
cos θ sin ϕ
−ρ sin θ sin ϕ
ρ cos θ cos ϕ
sin θ sin ϕ
−ρ cosθ sin ϕ
ρ sin θ cos ϕ
cos ϕ
0
−ρ sin ϕ
⎞
⎠
⎛
⎝
ρ′
θ′
ϕ′
⎞
⎠.
If we label the spherical coordinates as (¯x1, ¯x2, ¯x3) and the Cartesian co-
ordinates as (x1, x2, x3), the transition matrix between components of the

2.4. Tensor Notation
61
velocity vector from spherical coordinates to Cartesian coordinates is pre-
cisely (∂xi/∂¯xj), as calculated in Equation (2.23). Thus, the velocity vector
of any curve ⃗γ(t) through a point P = ⃗γ(t0) does not change according to
Equation (2.24) but according to Equation (2.25).
The relations established in Equations (2.24) and (2.25) show that there
are two diﬀerent kinds of vectors, each following diﬀerent transformational
properties under a coordinate change. This distinction is not emphasized
in most linear algebra courses but essentially corresponds to the diﬀerence
between a column vector and a row vector, which in turn corresponds to
vectors in Rn and its dual (Rn)∗.
(The ∗notation denotes the dual of
a vector space. See Appendix C.3 for background.) We summarize this
dichotomy in the following two deﬁnitions.
Deﬁnition 2.4.4. Let (x1, . . . , xn) and (¯x1, . . . , ¯xn) be two coordinate sys-
tems in a neighborhood of a point p ∈Rn. An n-tuple (A1, A2, . . . , An) is
said to constitute the components of a contravariant vector at a point p if
these components transform according to the relation
¯Aj =
n

i=1
∂¯xj
∂xi Ai,
where we assume the partial derivatives are evaluated at p.
Deﬁnition 2.4.5.
Under the same conditions as above, an n-tuple (B1,
B2, . . . , Bn) is said to constitute the components of a covariant vector at a
point p if these components transform according to the relation
¯Bj =
n

i=1
∂xi
∂¯xj Bi,
where we assume the partial derivatives are evaluated at p.
A few comments are in order at this point.
Though the above two
deﬁnitions are unsatisfactory from the modern perspective in that they
do not clearly deﬁne some set of vectors, these are precisely what one is
likely to ﬁnd in a classical mathematics text or a physics text presenting
diﬀerential geometry. Nonetheless, we will content ourselves with these def-
initions and with the more general Deﬁnition 2.4.6. We defer until Chap-
ter 4 what is considered the proper modern deﬁnition of a tensor on a
manifold.

62
2. Coordinates, Frames, and Tensor Notation
Next, we point out that the quantities (A1, A2,. . . ,An) in Deﬁnition 2.4.4
or (B1, B2, . . . , Bn) in Deﬁnition 2.4.5 can either be constant or can be
functions of the coordinates in a neighborhood of p. If the quantities are
constant, one says they form the components of an aﬃne vector. If the
quantities are functions in the coordinates, then the components deﬁne a
diﬀerent vector for every point in an open set, and thus, one views these
quantities as the components of a vector ﬁeld over a neighborhood of p.
Finally, in terms of notation, we distinguish between the two types
of vectors by using subscripts for covariant vectors and superscripts for
contravariant vectors. This convention of notation is consistent throughout
the literature and forms a central part of tensor calculus. This convention
also explains the use of superscripts for the coordinates since (xi) represents
the components of a contravariant vector, namely, the position vector of a
point.
As a further example to motivate the deﬁnition of a tensor, recall the
metric tensor of a regular surface S. If a coordinate patch of a regular
surface is parametrized by ⃗X(x1, x2) (so that (x1, x2) forms a coordinate
system on that patch), then one deﬁnes the components of the metric
tensor as
gij = ∂⃗X
∂xi · ∂⃗X
∂xj .
(2.26)
Suppose that some open set U of S can be parametrized by ⃗X(x1, x2)
and by some other parametrization ⃗Y (¯x1, ¯x2). (The pair (¯x1, ¯x2) deﬁnes
another coordinate system on U.) Deﬁne ¯gkl similarly to Equation (2.26)
but with respect to the parametrization ⃗Y . It is not hard to show that
¯gkl =
2

i=1
2

j=1
∂xi
∂¯xk
∂xj
∂¯xl gij.
This formula mimics but generalizes the transformational properties in Def-
inition 2.4.4 and Deﬁnition 2.4.5.
As we will see, many objects of interest that arise in diﬀerential geom-
etry and in physics possess similar transformational properties. This leads
to the following deﬁnition of a tensor.
Deﬁnition 2.4.6. Let (x1, . . . , xn) and (¯x1, . . . , ¯xn) be two coordinate sys-
tems in a neighborhood of a point p ∈Rn. A set of nr+s quantities T i1i2···ir
j1j2···js
is said to constitute the components of a tensor of type (r, s) if under a
coordinate transformation, these quantities transform according to

2.4. Tensor Notation
63
¯T k1···kr
l1···ls
=
n

i1=1
· · ·
n

ir=1
n

j1=1
· · ·
n

js=1
∂¯xk1
∂xi1 · · · ∂¯xkr
∂xir
∂xj1
∂¯xl1 · · · ∂xjs
∂¯xls T i1···ir
j1···js ,
(2.27)
where we assume that the partial derivatives are evaluated at p. We deﬁne
the rank of the tensor as the integer r + s.
From the above deﬁnition, one could rightly surmise that basic cal-
culations with tensors involve numerous repeated summations. In order
to alleviate this notational burden, mathematicians and physicists who use
the tensor notation as presented above utilize the Einstein summation con-
vention. In this convention of notation, one assumes that one takes a sum
from 1 to n (the dimension of Rn or the number of coordinates) over any
index that appears both in a superscript and a subscript of a product. Fur-
thermore, for this convention, in a partial derivative ∂¯xi/∂xj, the index i
is considered a superscript and the index j is considered a subscript.
For example, if the quantities Aij form the components of a (0, 2)-tensor
and the quantities Bk constitute the components of a contravariant vector,
with the Einstein summation convention, the expression AijBj means
n

j=1
AijBj.
As another example, with the Einstein summation convention, the trans-
formational property of a tensor as given in Equation (2.27) is written as
¯T k1k2···kr
l1l2···ls
= ∂¯xk1
∂xi1
∂¯xk2
∂xi2 · · · ∂¯xkr
∂xir
∂xj1
∂¯xl1
∂xj2
∂¯xl2 · · · ∂xjs
∂¯xls T i1i2···ir
j1j2···js,
where the summations from 1 to n over the indices i1, i2 · · · ir, j1, j2 · · · js
are understood.
As a third example, if the quantities Cij
kl constitute a
tensor of type (2, 2), then the expression Cij
kj means
n

j=1
Cij
kj.
On the other hand, with this convention, we do not sum over the index i
in the expression Ai+Bi or even in Ai+Bi. In fact, as we shall see, though
the former expression has a linear algebraic interpretation, the latter does
not.
In the rest of this book, we will use the Einstein summation convention
when working with components of tensors.

64
2. Coordinates, Frames, and Tensor Notation
2.4.3
Operations on Tensors
It is possible to construct new tensors from old ones. (Again, the reader
is encouraged to consult Appendix C.4 to see the underlying algebraic
meaning of the following operations.)
First of all, if Si1i2···ir
j1j2···js and T i1i2···ir
j1j2···js are both components of tensors of
type (r, s), then the quantities
W i1i2···ir
j1j2···js = Si1i2···ir
j1j2···js + T i1i2···ir
j1j2···js
form the components of another (r, s)-tensor. In other words, tensors of
the same type can be added to obtain another tensor of the same type.
The proof is very easy and follows immediately from the transformational
properties and distributivity.
Secondly, if Si1i2···ir
j1j2···js and T k1k2···kt
l1l2···lu
are components of tensors of type
(r, s) and (t, u), respectively, then the quantities obtained by multiplying
these components
W i1i2···irk1k2···kt
j1j2···jsl1l2···lu = Si1i2···ir
j1j2···jsT k1k2···kt
l1l2···lu
form the components of another tensor but of type (r+t, s+u). Again, the
proof is very easy, but one must be careful with the plethora of indices. One
should note that this operation of tensor product works also for multiplying
a tensor by a scalar since a scalar is a tensor of rank 0.
Finally, another common operation on tensors is the contraction be-
tween two indices. We illustrate the contraction with an example. Let
Aijk
rs be the components of a (3, 2)-tensor and deﬁne the quantities as
Bij
r = Aijk
rk
def
=
n

k=1
Aijk
rk
(by Einstein summation convention).
It is not hard to show (left as an exercise for the reader) that Bij
r constitute
the components of a tensor of type (2, 1). More generally, starting with a
tensor of type (r, s), if one sums over an index that appears both in the
superscript and in the subscript, one obtains the components of a (r −1,
s −1)-tensor. This is the contraction of a tensor over the stated indices.
2.4.4
Examples
Example 2.4.7. Following the terminology of Deﬁnition 2.4.6, a covariant
vector is often called a (0, 1)-tensor and similarly a contravariant vector is
called a (1, 0)-tensor.

2.4. Tensor Notation
65
Example 2.4.8 (Inverse of a (0, 2)-Tensor). As a more involved example, con-
sider the components Aij of a (0, 2)-tensor in Rn. Denote by Aij the quan-
tities given as the coeﬃcients of the inverse matrix of (Aij). (Parentheses
around the components of a tensor of rank 2 indicate that we consider the
associated matrix and not just the collection of components.) We prove
that Aij form the components of a (2, 0)-tensor.
Suppose that the coeﬃcients Aij are given in a coordinate system with
variables (x1, . . . , xn), and ¯Ars are given in the (¯x1, . . . , ¯xn) coordinate
system. That they are the inverse to the matrices (Aij) and ( ¯Ars) means
that Aij and ¯Ars are the unique quantities such that
AijAjk = δi
k,
and
¯Ars ¯Ast = δr
t ,
(2.28)
where the reader must remember that we are using the Einstein summation
convention. Combining Equation (2.28) and the transformational proper-
ties of Ajk, we get
¯Ars ∂xi
∂¯xs
∂xj
∂¯xt Aij = δr
t .
Multiplying both sides by ∂¯xt/∂xα and summing over t, we obtain
¯Ars ∂xi
∂¯xs
∂xj
∂¯xt Aij
∂¯xt
∂xα = δr
t
∂¯xt
∂xα
⇐⇒
¯Ars ∂xi
∂¯xs δj
αAij = ∂¯xr
∂xα
⇐⇒
¯Ars ∂xi
∂¯xs Aiα = ∂¯xr
∂xα .
Multiplying both sides by Aαβ and then summing over α, we get
¯Ars ∂xi
∂¯xs δβ
i = ¯Ars ∂xβ
∂¯xs = ∂¯xr
∂xα Aαβ.
Finally, multiplying the rightmost equality by ∂¯xs/∂xβ and summing over
β, one concludes that
¯Ars = ∂¯xr
∂xα
∂¯xs
∂xβ Aαβ.
This shows that the quantities Aij satisfy Deﬁnition 2.4.6 and form the
components of a (2, 0)-tensor.
By a similar manipulation, one can show that if Bij are the components
of a (2, 0)-tensor, then the quantities Bij corresponding to the inverse of
the matrix (Bij) form the components of a (0, 2)-tensor.

66
2. Coordinates, Frames, and Tensor Notation
Example 2.4.9 (Gauss Map Coefﬁcients). In diﬀerential geometry, one denotes
by gij the coeﬃcients of the inverse of the matrix associated to the ﬁrst
fundamental form. Example 2.4.8 showed that gij is a (2, 0)-tensor. The
coeﬃcients gij occur frequently in the geometry of surfaces and in Rie-
mannian geometry (Chapter 5). For example, the Weingarten equations,
which we will see in Example 3.3.9 (also presented in [5, Chapter 6]), can
be written concisely as
ai
j = −Ljkgki.
By tensor product and contraction, we see that the functions ai
j must form
the components of a (1, 1)-tensor. (These are called the coeﬃcients of the
Gauss map.)
Example 2.4.10 (Metric Tensors). It is important to understand some standard
operations on vectors in the context of tensor notation. Consider two vec-
tors in a vector space V of dimension n. Using tensor notation, one refers
to these vectors as aﬃne contravariant vectors with components Ai and
Bj, with i, j = 1, 2, . . ., n. We have seen that addition of the vectors or
scalar multiplication are the usual operations from linear algebra. Another
operation between vectors in V is the dot product, which was originally
deﬁned as
n

i=1
AiBi,
but this is not the correct way to understand the dot product in the context
of tensor algebra. The very fact that one cannot use Einstein summation
convention is a hint that we must analyze this operation in more detail. The
use of the usual dot product for its intended geometric purpose makes an
assumption of the given basis of V , namely, that the basis is orthonormal.
When using tensor algebra, one makes no such assumption. Instead, one
associates to the basis of V with respect to which coordinates are deﬁned
as a (0, 2)-tensor gij, called the metric tensor. Then the ﬁrst fundamental
form (or scalar product) between Ai and Bj is
gijAiBj
(by Einstein summation).
One immediately notices that because of tensor multiplication and contrac-
tion, the result is a scalar quantity and hence will remain unchanged under
a coordinate transformation. In this formulation, the assumption that a
basis is orthonormal is equivalent to having
gij =

1,
if i = j,
0,
if i ̸= j.

2.4. Tensor Notation
67
2.4.5
Symmetries
The usual operations of tensor addition and scalar multiplication were ex-
plained above. We should point out that using distributivity and associa-
tivity, one notices that the set of aﬃne tensors of type (r, s) in Rn form a
vector space. The (r + s)-tuple of all the indices can take on nr+s values,
so this vector space has dimension nr+s.
However, it is not uncommon that there exist symmetries within the
components of a tensor. For example, as we saw for the metric tensor,
we always have gij = gji. In the context of matrices, we said that the
matrix (gij) is a symmetric matrix but in the context of tensor notation,
we say that the components gij are symmetric in the indices i and j. More
generally, if T i1i2···ir
j1j2···js are the components of a tensor of type (r, s), we say
that the components are symmetric in a set S of indices if the components
remain equal when we interchange any two indices from among the indices
in S.
For example, let Aijk
rs be the components of a (3, 2)-tensor. To say that
the components are symmetric in {i, j, k} aﬃrms the equalities
Aijk
rs = Aikj
rs = Ajik
rs = Ajki
rs = Akij
rs = Akji
rs
for all i, j, k ∈{1, 2, . . ., n}. Note that, because of the additional conditions,
the dimension of the vector space of all (3, 2)-tensors that are symmetric
in their contravariant indices is smaller than n5 but is not simply n5/6
either. We can ﬁnd the dimension of this vector space by determining the
cardinality of
I =
#
(i, j, k) ∈{1, 2, . . ., n}3 | 1 ≤i ≤j ≤k ≤n
$
.
We will see shortly that |I| =
n+2
3

, and therefore, the dimension of the
vector space of (3, 2)-tensors that are symmetric in their contravariant in-
dices is
n+2
3

n2. We provide the following proposition for completeness.
Proposition 2.4.11. Let Aj1···jr
k1···ks be the components of a tensor over Rn that
is symmetric in a set S of its indices. Assuming that all the indices are
ﬁxed except for the indices of S, the number of independent components of
the tensor is equal to the cardinality of
I = {(i1, . . . , im) ∈{1, 2, . . . , n}m | 1 ≤i1 ≤i2 ≤· · · ≤im ≤n} .
This cardinality is
n −1 + m
m
	
= (n −1 + m)!
(n −1)!m! .

68
2. Coordinates, Frames, and Tensor Notation
Proof: Since the components are symmetric in the set S of indices, one
gets a unique representative of equivalent components by imposing that the
indices in question be listed in nondecreasing order. This remark proves
the ﬁrst part of the proposition. To prove the second part, consider the
set of integers {1, 2, . . ., n + m}, and pick m distinct integers {l1, . . . , lm}
that are greater than 1 from among this set. We know from the deﬁnition
of combinations that there are
n−1+m
m

ways to do this. Assuming that
l1 < l2 < · · · < lm, deﬁne it = lt −t. It is easy to see that the resulting
m-tuple (i1, . . . , im) is in the set I. Furthermore, since one can reverse the
process by lt = it + t for 1 ≤t ≤m, there exists a bijection between I and
the m-tuples (l1, . . . , lm) described above. This establishes that
|I| =
n −1 + m
m
	
.
□
Another common situation with relationships between the components
of a tensor occurs when components are antisymmetric in a set of indices.
We say that the components are antisymmetric in a set S of indices if
the components are negated when we interchange any two indices from
among the indices in S. This condition imposes a number of immediate
consequences.
Consider, for example, the components of a (0, 3)-tensor
Aijk that are antisymmetric in all their indices.
If k is any value, but
i = j, then
Aijk = Aiik = Ajik = −Aijk,
and so Aiik = 0. Hence, any triple (i, j, k) in which at least two of the
indices are equal, the corresponding component is equal to 0. As another
consequence of the antisymmetric condition, consider the component A231.
One obtains the triple (2, 3, 1) from (1, 2, 3) by ﬁrst interchanging 1 and 2 to
get (2, 1, 3) and then interchanging the last two to get (2, 3, 1). Therefore,
we see that
A123 = −A213 = A231.
In modern algebra, a permutation (a bijection on a ﬁnite set) that inter-
changes two inputs and leaves the rest ﬁxed is called a transposition. We
say that we used two transpositions to go from (1, 2, 3) to (2, 3, 1).
The above example illustrates that the value of the component involving
a particular m-tuple (i1, . . . , im) of distinct indices determines the value of
any component involving a permutation (j1, . . . , jm) of (i1, . . . , im) accord-
ing to
Aj1...jm = ±Ai1...im,

2.4. Tensor Notation
69
where the sign ± is + (respectively −) if it takes an even (respectively
odd) number of interchanges to get from (i1, . . . , im) to (j1, . . . , jm). A
priori, if one could get from (i1, . . . , im) to (j1, . . . , jm) with both an odd
and an even number of transpositions, then the value of Ai1...im and all
components indexed by a permutation of (i1, . . . , im) would be 0. However,
a fundamental fact in modern algebra (see [24, Theorem 5.5]) states that
given a permutation σ on {1, 2, . . ., m}, if we have two ways to write σ as
a composition of transpositions, e.g.,
σ = τ1 ◦τ2 ◦· · · ◦τa = τ′
1 ◦τ′
2 ◦· · · ◦τ′
b ,
then a and b have the same parity.
Deﬁnition 2.4.12. We call a permutation even (respectively odd) if this com-
mon parity is even (respectively odd) and the sign of σ is
sign(σ) =

1,
if σ is even,
−1,
if σ is odd.
The above discussion leads to the following proposition about the com-
ponents of an antisymmetric tensor.
Proposition 2.4.13. Let Aj1···jr
k1···ks be the components of a tensor over Rn that
is antisymmetric in a set S of its indices. If any of the indices in S are
equal, then
Aj1···jr
k1···ks = 0.
If |S| = m, then ﬁxing all but the indices in S, the number of independent
components of the tensor is equal to
nr+s−m
n
m
	
=
n!
m!(n −m)!nr+s−m.
Finally, if the indices of Ai1···ir
j1···js diﬀer from Ak1···kr
l1···ls
only by a permutation
σ on the indices in S, then
Ak1···kr
l1···ls = sign(σ) Ai1···ir
j1···js.
Proof: We already saw the ﬁrst part of the proposition.
If we ﬁx values for all the indices not in S, the remaining number of
independent components of the tensor Aj1···jr
k1···ks corresponds to a collection
of distinct indices in S. There are precisely
 n
m

ways to choose the indices
in S to be distinct. Given any choice of values for the indices in S, there

70
2. Coordinates, Frames, and Tensor Notation
are n choices for all the remaining indices. Since there are r +s−m indices
not in S, the number of independent components of the tensor is equal to
nr+s−m
n
m

=
n!
m!(n −m)!nr+s−m.
For the last part of the proposition, if we can go from Ai1···ir
j1···js to Ak1···kr
l1···ls
with h transpositions, then by the previous discussion, the quantities diﬀer
by (−1)h.
The proposition follows from the deﬁnition of the sign of a
permutation.
□
2.4.6
Numerical Tensors
As a motivating example of what are called numerical tensors, note that the
quantities δi
j form the components of a (1, 1)-tensor. To see this, suppose
that δi
j is given in a system of coordinates (x1, . . . , xn) and ¯δk
l is its trans-
formed value in another system of coordinates (¯x1, . . . , ¯xn). Obviously, for
all ﬁxed i and j, the value of δi
j is constant, and therefore,
¯δk
l =

1,
if k = l,
0,
if k ̸= l.
But using the properties of δi
j and the chain rule,
∂¯xk
∂xi
∂xj
∂¯xl δi
j = ∂¯xk
∂xi
∂xi
∂¯xl = ∂¯xk
∂¯xl = ¯δk
l .
Therefore, δi
j is a (1, 1)-tensor in a tautological way.
A numerical tensor is a tensor of rank greater than 0 whose components
are constant in the variables (x1, . . . , xn) and hence also (¯x1, . . . , ¯xn). The
Kronecker delta is just one example of a numerical tensor, and we have
already seen that it plays an important role in many complicated calcu-
lations. The Kronecker delta is the simplest case of the most important
numerical tensor, the generalized Kronecker delta. The generalized Kro-
necker delta of order r is a tensor of type (r, r), with components denoted
by δi1···ir
j1···jr deﬁned as the following determinant:
δi1···ir
j1···jr =

δi1
j1
δi1
j2
· · ·
δi1
jr
δi2
j1
δi2
j2
· · ·
δi2
jr
...
...
...
...
δir
j1
δir
j2
· · ·
δir
jr

.
(2.29)

2.4. Tensor Notation
71
It is not obvious from Equation (2.29) that the quantities δi1···ir
j1···jr form
the components of a tensor. However, one can write the components of the
generalized Kronecker delta of order 2 as
δij
kl = δi
kδj
l −δi
lδj
k,
which presents δij
kl as the diﬀerence between two (2, 2)-tensors, which shows
that δij
kl is indeed a tensor. More generally, expanding out Equation (2.29)
by the Laplace expansion of a determinant gives the generalized Kronecker
delta of order r as a sum of r! components of tensors of type (r, r), proving
that δi1···ir
j1···jr are the components of an (r, r)-tensor.
Properties of the determinant imply that δi1···ir
j1···jr is antisymmetric in
the superscript indices and antisymmetric in the subscript indices. That
is to say that δi1···ir
j1···jr = 0 if any of the superscript indices are equal or if
any of the subscript indices are equal, and the value of a component is
negated if any two superscript indices are interchanged and similarly for
subscript indices. We also note that if r > n, where we assume δi1···ir
j1···jr are
the components of a tensor in Rn, then δi1···ir
j1···jr = 0 for all choices of indices
since at least two superscript (and at least two subscript) indices would be
equal.
We introduce one more symbol related to the generalized Kronecker
delta, namely, the permutation symbol. Deﬁne
εi1···in = δi1···in
1···n ,
εj1···jn = δ1···n
j1···jn.
(2.30)
Note that the maximal index n in Equation (2.30) as opposed to r is in-
tentional. Because of the properties of the determinant, it is not hard to
see that one has the values
εi1···in = εi1···in
=
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1,
if (i1, . . . , in) is an even permutation of (1, 2, . . . , n),
−1,
if (i1, . . . , in) is an odd permutation of (1, 2, . . . , n),
0,
if (i1, . . . , in) is not a permutation of (1, 2, . . . , n).
We are careful, despite the notation, not to call the permutation sym-
bols the components of a tensor for they are not. Instead, we have the
following proposition.

72
2. Coordinates, Frames, and Tensor Notation
Proposition 2.4.14. Let (x1, . . . , xn) and (¯x1, . . . , ¯xn) be two coordinate sys-
tems. The permutation symbols transform according to
¯εj1···jn = J ∂¯xj1
∂xi1 · · · ∂¯xjn
∂xin εi1···in,
¯εk1···kn = J−1 ∂xh1
∂¯xk1 · · · ∂xhn
∂¯xkn εh1···hn,
where J = det( ∂¯xi
∂xj ) is the Jacobian of the transformation of coordinates
function.
Proof: (Left as an exercise for the reader.)
□
Example 2.4.15 (Cross Product). As an example of the permutation symbol,
consider two contravariant vectors Ai and Bj in R3. If we deﬁne Ck =
εijkAiBj, we easily ﬁnd that
C1 = A2B3 −A3B2,
C2 = A3B1 −A1B3,
C3 = A1B2 −A2B1.
The values Ck are precisely the terms of the cross product of the vectors
Ai and Bj. However, a quick check shows that the quantities Ck do not
form the components of a covariant tensor.
One explanation in relation to standard linear algebra for the fact that
Ck does not give a contravariant vector is that if ⃗a and ⃗b are vectors in R3
given with coordinates in a certain basis and if M is a coordinate-change
matrix, then
(M⃗a) × (M⃗b) ̸= M(⃗a ×⃗b).
In many physics textbooks, when one assumes that we use the usual
metric, (gij) being the identity matrix, one is not always careful with the su-
perscript and subscript indices. This is because one can obtain a contravari-
ant vector Bj from a covariant vector Ai simply by deﬁning Bj = gijAi,
and the components (B1, B2, B3) are numerically equal to (A1, A2, A3).
Therefore, in this context, one can deﬁne the cross product as the vector
with components
Cl = gklεijkAiBj,
(2.31)
However, one must remember that this is not a contravariant vector since it
does not satisfy the transformational properties of a tensor. From Propo-
sition 2.4.14, it is not hard to show that the quantity Cl will transform
according to
¯Ch = J−1 ∂¯xh
∂xl Cl,
where J is the Jacobian of the transformation from (xi)- to (¯xj)-coordinates.

2.4. Tensor Notation
73
The generalized Kronecker delta has a close connection to determinants
which, we will elucidate here.
Note that if the superscript indices are
exactly equal to the subscript indices, then δi1···ir
j1···jr is the determinant of the
identity matrix. Thus, the contraction over all indices, δj1···jr
j1···jr counts the
number of permutations of r indices taken from the set {1, 2, . . ., n}. Thus,
δj1···jr
j1···jr =
n!
(n −r)!.
Another property of the generalized Kronecker delta is that εj1···jnεi1···in
= δj1···jn
i1···in , the proof of which is left as an exercise for the reader (Prob-
lem 2.4.11). Now let ai
j be the components of a (1, 1)-tensor, which we can
view as the matrix of a linear transformation from Rn to Rn. By deﬁnition
of the determinant,
det(ai
j) = εj1···jna1
j1 · · · an
jn.
Then, by properties of the determinant related to rearranging rows or
columns, we have
εi1···in det(ai
j) = εj1···jnai1
j1 · · · ain
jn.
Multiplying by εi1···in and summing over all the indices i1, . . . , in, we have
εi1···inεi1···in det(ai
j) = δj1···jn
i1···in ai1
j1 · · · ain
jn,
and since εi1···inεi1···in counts the number of permutations of {1, . . . , n},
we have
n! det(ai
j) = δj1···jn
i1···in ai1
j1 · · · ain
jn.
Problems
2.4.1. Consider the rectangular and spherical coordinate systems as described in
Example 2.4.1. Calculate (∂¯xi/∂xj) directly from formulas for ρ, θ, ϕ in
terms of x, y, z. Conﬁrm that, as matrices,
∂xk
∂¯xl
	
=
 ∂¯xi
∂xj
	−1
.
2.4.2. Prove that (a) δi
jδj
kδk
l = δi
l and (b) δi
jδj
kδk
i = n.
2.4.3. Let Bi be the components of a covariant vector. Prove that
∂Bj
∂xk −∂Bk
∂xj
form the components of a (0, 2)-tensor.

74
2. Coordinates, Frames, and Tensor Notation
2.4.4. Let T i1i2···ir
j1j2···js be a tensor of type (r, s). Prove that the quantities T ii2···ir
ij2···js ,
obtained by contracting over the ﬁrst two indices, form the components
of a tensor of type (r −1, s −1). Explain why one still obtains a tensor
when one contracts over any superscript and subscript index.
2.4.5. Let Sijk be the components of a tensor, and suppose they are antisymmet-
ric in {i, j}. Find a tensor with components Tijk that is antisymmetric in
j, k satisfying
−Tijk + Tjik = Sijk.
2.4.6. If Ajk is antisymmetric in its indices and Bjk is symmetric in its indices,
show that the scalar AjkBjk is 0.
2.4.7. Consider Ai, Bj, and Ck to be the components of three contravariant
vectors. Prove that εijkAiBjCk is the value of the triple product ( ⃗A ⃗B ⃗C) =
⃗A · ( ⃗B × ⃗C), which is the volume of the parallelepiped spanned by these
three vectors.
2.4.8. Prove that εijkεrsk = δrs
ij . Assume that we use a metric gij as the identity
matrix, and deﬁne the cross product ⃗C of two contravariant vectors ⃗A =
(Ai) and ⃗B = (Bj) as Ck = gklεijlAiBj. Use what you just proved to
show that
⃗A × ( ⃗B × ⃗C) = ( ⃗A · ⃗C) ⃗B −( ⃗A · ⃗B) ⃗C.
2.4.9. Let ⃗A, ⃗B, ⃗C, and ⃗D be vectors in R3. Use the εijk symbols to prove that
( ⃗A × ⃗B) × ( ⃗C × ⃗D) = ( ⃗A ⃗B ⃗D) ⃗C −( ⃗A ⃗B ⃗C) ⃗D.
2.4.10. Prove Proposition 2.4.14.
2.4.11. Prove that εi1···inεj1···jn = δi1···in
j1···jn.
2.4.12. Let Aij be the components of an antisymmetric tensor of type (0, 2), and
deﬁne the quantities
Brst = ∂Ast
∂xr + ∂Atr
∂xs + ∂Ars
∂xt .
(a) Prove that Brst are the components of a tensor of type (0, 3).
(b) Prove that the components Brst are antisymmetric in all their in-
dices.
(c) Determine the number of independent components of antisymmetric
tensors of type (0, 3) over Rn.
(d) Would the quantities Brst still be the components of a tensor if Aij
were symmetric?
2.4.13. Let A be an n × n matrix with coeﬃcients A = (Aj
i), and consider the
coordinate transformation
¯xj =
n

i=1
Aj
ixi.

2.4. Tensor Notation
75
Recall that this transformation is called orthogonal if AAT = I where
AT is the transpose of A and I is the identity matrix. The orthogonality
condition implies that det(A) = ±1.
An orthogonal transformation is
called special or proper if, in addition, det(A) = 1. A set of quantities
T i1···ir
j1···js is called a proper tensor of type (r, s) if it satisﬁes the tensor
transformation property from Equation (2.27) for all proper orthogonal
transformations.
(a) Prove that the orthogonality condition is equivalent to requiring that
ηij = ηhkAh
i Ak
j ,
where
ηij =

1,
if i = j,
0,
if i ̸= j.
(b) Prove that the orthogonality condition is also equivalent to saying
that orthogonal transformations are the invertible linear transforma-
tions that preserve the quantity
(x1)2 + (x2)2 + · · · + (xn)2.
(c) Prove that (i) the space of proper tensors of type (r, s) form a vector
space over R, (ii) the product of a proper tensor of type (r1, s1) and a
proper tensor of type (r2, s2) is a proper tensor of type (r1 + r2, s1 +
s2), and (iii) contraction over two indices of a proper tensor of type
(r, s) produces a proper tensor of type (r −1, s −1).
(d) Prove that the permutation symbols are proper tensors of type (n, 0)
or (0, n), as appropriate.
(e) Use this to prove that the cross product of two contravariant vec-
tors in R3, as deﬁned by Equation (2.31) is a proper tensor of type
(1, 0). [Remark: This explains that the cross product of two vectors
transforms correctly only if we restrict ourselves to proper orthogonal
transformations on R3.]
(f) Suppose that we are in R3. Prove that the rotation with matrix
A =
⎛
⎝
cos α
−sin α
0
sin α
cos α
0
0
0
1
⎞
⎠
is a proper orthogonal transformation.
(g) Again, suppose that we are in R3. Prove that the linear transforma-
tion with matrix given with respect to the standard basis
B =
⎛
⎝
cos β
sin β
0
sin β
−cos β
0
0
0
1
⎞
⎠
is an orthogonal transformation that is not proper.

76
2. Coordinates, Frames, and Tensor Notation
2.4.14. Consider the vector space Rn+1 with coordinates (x0, x1, . . . , xn) where
(x1, . . . , xn) are called the space coordinates and x0 is the time coordinate.
The usual connection between x0 and time t is x0 = ct, where c is the
speed of light. We equip this space with the metric deﬁned by ημν, where
η00 = −1, ηii = 1 for 1 ≤i ≤n, and ηij = 0 if i ̸= j. (The quantities
ημν do not give a metric in the sense we have presented in this text so
far because it is not positive deﬁnite. For this reason, the quantities ημν
are often said to constitute a pseudometric. Though we do not provide
the details here, this unusual metric gives a mathematical justiﬁcation for
why it is impossible to travel faster than the speed of light c.) This vector
space equipped with the metric ημν is called the n-dimensional Minkowski
spacetime, and ημν is called the Minkowski metric.
Let L be an n × n matrix with coeﬃcients Lα
β, and consider the linear
transformation
¯xj =
n

i=0
Lj
ixi.
A Lorentz transformation is an invertible linear transformation on Minkowski
spacetime, with matrix L such that
ηαβ = ημνLμ
αLν
β.
Finally, a collection of quantities T i1···ir
j1···js , with indices ranging in {0, 1, . . . , n},
is called a Lorentz tensor of type (r, s) if it satisﬁes the tensor transfor-
mation property Equation (2.27) for all Lorentz transformations.
(a) Prove that a transformation of Minkowski spacetime is a Lorentz
transformation if and only if it preserves the quantity
−(x0)2 + (x1)2 + (x2)2 + · · · + (xn)2.
(b) Suppose we are working in three-dimensional Minkowski spacetime.
Prove that the rotation matrix
A =
⎛
⎜
⎜
⎝
1
0
0
0
0
cos α
−sin α
0
0
sin α
cos α
0
0
0
0
1
⎞
⎟
⎟
⎠
represents a Lorentz transformation.
(c) Again, suppose we are working in three-dimensional Minkowski space-
time. Consider the matrix
L =
⎛
⎜
⎜
⎝
γ
−βγ
0
0
−βγ
γ
0
0
0
0
1
0
0
0
0
1
⎞
⎟
⎟
⎠,
where β is a positive real number satisfying −1 < β < 1 and γ =
1/

1 −β2. Prove that L represents a Lorentz transformation.

2.4. Tensor Notation
77
(d) Prove that (i) the space of Lorentz tensors of type (r, s) form a vector
space over R, (ii) the product of a Lorentz tensor of type (r1, s1)
and a Lorentz tensor of type (r2, s2) is a Lorentz tensor of type
(r1 + r2, s1 + s2), and (iii) contraction over two indices of a Lorentz
tensor of type (r, s) produces a Lorentz tensor of type (r −1, s −1).
2.4.15. Let ai
j be the components of a (1, 1)-tensor, or in other words the matrix
of a linear transformation from Rn to Rn given with respect to some basis.
Recall that the characteristic equation for the matrix is
det(ai
j −λδi
j) = 0.
(2.32)
Prove that Equation (2.32) is equivalent to
λn +
n

r=1
(−1)ra(r)λn−r = 0,
where
a(r) = 1
r!δi1···ir
j1···jrai1
j1 · · · air
jr.
[Hint: The solutions to Equation (2.32) are the eigenvalues of the matrix
(ai
j).]
2.4.16. Moment of Inertia Tensor. Suppose that R3 is given a basis that is not
necessarily orthonormal. Let gij be the metric tensor corresponding to this
basis, which means that the scalar product between two (contravariant)
vectors Ai and Bj is given by
⟨⃗A, ⃗B⟩= gijAiBj.
In the rest of the problem, call (x1, x2, x3) the coordinates of the position
vector ⃗r.
Let S be a solid in space with a density function ρ(⃗r), and suppose that
it rotates about an axis ℓthrough the origin. The angular velocity vector
⃗ω is deﬁned as the vector along the axis ℓ, pointing in the direction that
makes the rotation a right-hand corkscrew motion with magnitude ω that
is equal to the radians per second swept out by the motion of rotation.
Let (ω1, ω2, ω3) be the components of ⃗ω in the given basis. The moment
of inertia of the solid S about the direction ⃗ω is deﬁned as the quantity
Iℓ=
%%%
S
ρ(⃗r)r2
⊥dV,
where r⊥is the distance from a point ⃗r with coordinates (x1, x2, x3) to
the axis ℓ.
The moment of inertia tensor of a solid is often presented using cross
products, but we deﬁne it here using a characterization that is equivalent

78
2. Coordinates, Frames, and Tensor Notation
to the usual deﬁnition but avoids cross products. We deﬁne the moment
of inertia tensor as the unique (0, 2)-tensor Iij such that
(Iijωi)ωj
ω = Iℓω,
(2.33)
where ω = ∥⃗ω∥=

⟨⃗ω, ⃗ω⟩.
(a) Prove that
r2
⊥= gijxixj −(gklωkxl)2
grsωrωs .
(b) Prove that, using the metric gij, the moment of inertia tensor is given
by
Iij =
%%%
S
ρ(x1, x2, x3)(gijgkl −gikgjl)xkxl dV.
(c) Show that
(gijgkl −gikgjl)xkxl = gipgqlδpq
jkxkxl,
where δpq
jk is the generalized Kronecker delta of order 2.
(d) Prove that Iij is symmetric in its indices.
(e) Prove that if the basis of R3 is orthonormal (which means that (gij)
is the identity matrix), one recovers the following usual formulas one
ﬁnds in physics texts:
I11 =
%%%
S
ρ((x2)2 + (x3)2) dV,
I12 = −
%%%
S
ρx1x2 dV,
I22 =
%%%
S
ρ((x1)2 + (x3)2) dV,
I13 = −
%%%
S
ρx1x3 dV,
I33 =
%%%
S
ρ((x1)2 + (x2)2) dV,
I23 = −
%%%
S
ρx2x3 dV.
(We took the relation in Equation (2.33) as the deﬁning property of the
moment of inertia tensor because of the theorem that Iℓω is the component
of the angular moment vector along the axis of rotation that is given by
(Iijωi) ωj
ω . See [21, pp. 221–222] and, in particular, Equation (9.7) for an
explanation. The interesting point about this approach is that it avoids
the use of an orthonormal basis and provides a formula for the moment of
inertia tensor when one has an aﬃne metric tensor that is not the identity.
Furthermore, since it avoids the cross product, the above deﬁnitions for
the moment of inertia tensor of a solid about an axis are generalizable to
solids in Rn.)

CHAPTER
3
Differentiable Manifolds
Until now, our study of curves and surfaces has always described them as
subsets of some ambient Euclidean space Rn. We deﬁned parametrizations
as vector functions of one (for a curve) or two (for a surface) variables into
R2 or R3, without pointing out that many of our constructions relied on
the fact that R2 and R3 are topological vector spaces. That we have only
studied geometric objects that are subsets of R3 does not bely our intuition
since the daily reality of human experience evolves (or at least appears to
evolve) completely in three dimensions that we feel are ﬂat. However, both
in mathematics and in physics, one does not need to take such a large step
in abstraction to realize the insuﬃciency of this intuition.
In geometry, one can easily deﬁne natural point sets that cannot be
properly represented in only three dimensions. For example, the real pro-
jective plane RP2 can be deﬁned as the set of equivalence classes of lines
through the origin in R3 or also as the set of equivalence classes of points
in R3 −{(0, 0, 0)} under the equivalence relation
(x0, x1, x2) ∼(y0, y1, y2)
if and only if
(y0, y1, y2) = (λx0, λx1, λx2) for some λ ∈R −{0}.
This object plays a role of fundamental importance in both topology and
algebraic geometry. In some sense, it appears that the projective plane
should be a two-dimensional object since, from a topological viewpoint, it is
the identiﬁcation space (see Deﬁnition A.3.38) of a three-dimensional object
by a one-dimensional object. Both in classical geometry and in algebraic
geometry, there exist natural methods to study curves on the projective
plane, thereby providing a language to “do analysis” on it. Nonetheless, it
is not hard to show that no subset of R3 is homeomorphic to RP2. There
does exist a subset of R4 that is homeomorphic to RP2 but this fact is
not obvious from the deﬁnition of the projective plane. Consequently, to
provide deﬁnitions that include projective spaces and other more abstract
geometric objects, one must avoid referring to some ambient Euclidean
space.
79

80
3. Differentiable Manifolds
In physics, the need for eliminating a Euclidean ambient space boasts
a more colorful history. Inspired by evidence provided by scientists like
Toricelli, explorers of the ﬁfteenth and sixteenth centuries debunked the
ﬂat-earth theory by circumnavigating the globe. Though the normal Eu-
clidean geometry remained valid on the small scale, namely, doing geometry
on a ﬂat surface (sheet of paper or plot of land), such methods no longer
suﬃced when considering the geometry of the earth as a whole. In particu-
lar, the science of cartography suddenly became far more mathematical in
nature as navigators attempted to represent, with some degree of accuracy,
coastlines of continents on a ﬂat sheet of paper.
No less revolutionary was Einstein’s theory of general relativity in which
both space and time are connected as a single, four-dimensional space-time
entity that could itself be curved. In fact, following from the postulate that
nothing with mass travels faster than the speed of light, Einstein’s theory
purports that mass must distort space-time.
The practical need to do geometry or do physics in continuous point-set
spaces that are not Euclidean leads us to generalize our concepts of curves
and surfaces to higher-dimensional objects. We will call these objects of
study diﬀerentiable manifolds. We will then deﬁne maps between manifolds
and establish an analysis of maps between diﬀerentiable manifolds. Our
deﬁnitions, which may seem a little weighty, attempt to retain suﬃcient
restrictions to ensure that doing calculus on the sets is possible, while
preserving enough freedom to incorporate the rich variety of geometric
objects to which we wish to apply our techniques.
3.1
Deﬁnitions and Examples
As a motivating example for diﬀerentiable manifolds, we recall the deﬁni-
tion of a regular surface in R3 (see [5, Chapter 5] for more background).
Deﬁnition 3.1.1. A subset S ⊆R3 is a regular surface if for each p ∈S,
there exists an open set U ∈R2, an open neighborhood V of p in R3, and
a surjective continuous function ⃗X : U →V ∩S such that
1. ⃗X is diﬀerentiable: if we write ⃗X(u, v) = (x(u, v), y(u, v), z(u, v)),
then the functions x(u, v), y(u, v), and z(u, v) have continuous partial
derivatives of all orders;
2. ⃗X is a homeomorphism: ⃗X is continuous and has an inverse ⃗X−1 :
V ∩S →U such that ⃗X−1 is continuous;

3.1. Deﬁnitions and Examples
81
3. ⃗X satisﬁes the regularity condition: for each (u, v) ∈U, the diﬀeren-
tial d ⃗X(u,v) : R2 →R3 is a one-to-one linear transformation.
This deﬁnition already introduces many of the subtleties that are in-
herent in the concept of a manifold. In the above deﬁnition, each function
⃗X : U →V ∩S is called a parametrization of a coordinate neighborhood.
Now, as we set out to deﬁne diﬀerentiable manifolds and remove any
reference to an ambient Euclidean space, we begin from the context of
topological spaces. (Appendix A gives a brief introduction to topological
spaces.) Not every topological space can ﬁt the bill of usefulness for dif-
ferential geometry, so we require some additional properties of what types
of topological spaces we will consider. We ﬁrst impose the requirement of
having a cover of open sets, each of which is homeomorphic to an open set
in a Euclidean space.
Deﬁnition 3.1.2. A topological manifold of dimension k is a Hausdorﬀtopo-
logical space M with a countable base such that for all x ∈M, there exists
an open neighborhood of x that is homeomorphic to an open set of Rk.
The reader is encouraged to refer to Section A.3 for deﬁnitions and dis-
cussions about the base of a topology and the property of being Hausdorﬀ.
The technical aspect of this deﬁnition attempts to make the category of
objects of study as general as possible, while still remaining relevant for
geometry.
In the deﬁnition of a topological manifold, a given homeomorphism
of a neighborhood of M with a subset of Rk provides a local coordinate
system or coordinate patch. As one moves around on the manifold, one
passes from one coordinate patch to another. In the overlap of coordinate
patches, there exist change-of-coordinate functions that, by deﬁnition, are
homeomorphisms between open sets in Rn (see Figure 3.1). However, in
order to deﬁne a theory of calculus on the manifold, these functions must
be diﬀerentiable. We make this clear in the following deﬁnition.
Deﬁnition 3.1.3. A diﬀerentiable manifold M of dimension n is a topological
manifold along with a collection of maps φα : Uα →Rn, called charts, with
Uα open subsets in M, satisfying the following three conditions:
1. The collection of sets Uα, called coordinate patches, cover M.
2. Each chart is a homeomorphism φα : Uα →Vα, where Vα is an open
subset of Rn.

82
3. Differentiable Manifolds
φ1
φ2
φ2 ◦φ−1
1
V1
V2
M
U1
U2
Figure 3.1. Change-of-coordinate maps.
3. For any pair of charts φα and φβ, the change-of-coordinates function
φαβ
def
= φα ◦φ−1
β |φβ(Uα∩Uβ) : φβ(Uα ∩Uβ) −→φα(Uα ∩Uβ),
called the transition function, is a function of class C1 between open
subsets of Rn.
The set of pairs {(Uα, φα)}α∈I, where I is some indexing set, is called an
atlas.
A diﬀerentiable manifold is called a Ck manifold, a smooth manifold, or
an analytic manifold if the transition functions in the atlas are respectively
Ck, C∞, or analytic.
Two comments about notation are in order here. Mimicking the no-
tation habits for common sets (the n sphere as Sn or the real projective
space RPn), if M is an n-dimensional manifold, we sometimes shorten the
language by referring to the diﬀerentiable manifold M n.
Also, though
technically a chart is a function φ : U →Rn, where U is an open subset of
the manifold M, one sometimes refers to the chart (U, φ) to emphasize the
letter to be used for the domain of φ.
One might wonder why it is necessary to remove any reference to an
ambient space. Indeed, at ﬁrst glance, the above deﬁnition of a diﬀeren-
tiable manifold may seem unnecessarily complicated. A ﬁrst response to
such a criticism is to say that this is the safe thing to do: that a priori

3.1. Deﬁnitions and Examples
83
we do not know whether a given manifold can be described as a subset of
an ambient Euclidean space. A second response comes from physics. Gen-
eral relativity establishes that spacetime is not a Euclidean space. Popular
literature sometimes calls spacetime a “curved” space. However, it would
be misleading to think of this curved spacetime as a subset of a larger
Euclidean space since the coordinates of the ambient space would have
no physical meaning. Removing any reference to an ambient space is the
proper approach to presenting a mathematical structure that appropriately
models a non-Euclidean space in which we wish to do calculus. The above
deﬁnition and subsequent constructions have proven to be general enough
and structured enough to be useful in geometry and in physics.
Many properties of manifolds that arise in analysis are local properties,
in that we only need to know information about the manifold in some neigh-
borhood of a point p ∈M. When this is the case, we can restrict our atten-
tion to a single coordinate chart φα : Uα →Rn, where p ∈Uα. Saying that
the coordinates of a point p (with respect to this chart) are (x1, x2, . . . , xn)
means that φα(p) = (x1, x2, . . . , xn). For reasons that will only become
clear later, it is convenient to follow the tensor notation convention of us-
ing superscripts for coordinates. This makes writing polynomial functions
in the coordinates more tedious but this notation will provide a convenient
way to distinguish between covariant and contravariant properties.
Example 3.1.4 (Sphere). Consider the unit sphere S2 = {(x, y, z) ∈R3 | x2 +
y2 + z2 = 1}, which we cover with an atlas of two coordinate patches: the
stereographic projection from the north pole and from the south pole (see
Figure 3.2). It is not hard to verify the following formulas for stereographic
projection:
1. The north pole is πN : S2 −{(0, 0, 1)} →R2, with
πN(x, y, z) =

x
1 −z ,
y
1 −z
	
.
2. the south pole is πS : S2 −{(0, 0, −1)} →R2, with
πS(x, y, z) =

x
1 + z ,
y
1 + z
	
.
It is not hard to conﬁrm that πN and πS are homeomorphisms and that
every point p ∈S2 is in the domain of one of these charts. Furthermore,
the overlap of these two coordinate patches is the entire sphere except the
north and south poles. We now check the second condition in the deﬁnition

84
3. Differentiable Manifolds
x
y
z
N
   p
 π(p)
Figure 3.2. Stereographic projection.
of a manifold, namely, that the change of coordinates in this overlap is
diﬀerentiable.
In Problem 3.1.1, one will show that
π−1
N (u, v) =

2u
u2 + v2 + 1,
2v
u2 + v2 + 1, u2 + v2 −1
u2 + v2 + 1
	
.
Then the change-of-coordinate function πS ◦π−1
N
: R2 −{(0, 0)} →
R2 −{(0, 0)} is explicitly given by
πS ◦π−1
N (u, v) =

u
u2 + v2 ,
v
u2 + v2
	
.
By standard methods of multivariable calculus on R2, one can show that
πS ◦π−1
N
is C∞on R2 −{(0, 0)}. Similarly, one can show that πN ◦π−1
S
is
C∞. Since the set {πN, πS} provides an atlas on S2 and since the associated
transition functions are C∞, we conclude that S2 equipped with this atlas
is in fact a smooth manifold.
Example 3.1.5 (Sphere with Another Atlas). We can prove that the unit sphere
S2 is a smooth two-dimensional manifold using another atlas, this time
using rectangular coordinates for the parametrizations.
Consider a point p = (x, y, z) ∈S2, and let V = {(u, v) | u2 + v2 < 1}.
If z > 0, then the mapping ⃗X(1) : V →R3 deﬁned by (u, v,
√
1 −u2 −v2)
is clearly a bijection between V and S2 ∩{(x, y, z)|z > 0}. ⃗X(1) is also a
homeomorphism because it is continuous and its inverse ⃗X−1
(1) is simply the
vertical projection of the upper unit sphere onto R2, and since projection
is a linear transformation, it is continuous.

3.1. Deﬁnitions and Examples
85
⃗X(1)
⃗X(2)
⃗X(3)
⃗X(4)
⃗X(5)
⃗X(6)
Figure 3.3. Six coordinate patches on the sphere.
We cover S2 with the following parametrizations ⃗X(i) : V →R3:
if z > 0, ⃗X(1)(u, v) = (u, v,

1 −u2 −v2),
if z < 0, ⃗X(2)(u, v) = (u, v, −

1 −u2 −v2),
if y > 0, ⃗X(3)(u, v) = (u,

1 −u2 −v2, v),
if y < 0, ⃗X(4)(u, v) = (u, −

1 −u2 −v2, v),
if x > 0, ⃗X(5)(u, v) = (

1 −u2 −v2, u, v),
if x < 0, ⃗X(6)(u, v) = (−

1 −u2 −v2, u, v).
The inverses for each of these parametrizations give coordinate charts φi =
⃗X−1
(i) : Ui →Vi, which together form an atlas on the sphere (see Figure 3.3).
One notices in this case that all Vi = {(u, v) | u2 + v2 < 1}. Also, not
all Ui overlap. For example, U1 ∩U2 = ∅(i.e., the empty set) and similarly
for U3 ∩U4 and U5 ∩U6. To show that the sphere equipped with this atlas
is a diﬀerentiable manifold, one must show that all transition functions are
C1. We illustrate this with φ31 = φ3 ◦φ−1
1 .
The identiﬁcation (¯u, ¯v) = φ3 ◦φ−1
1 (u, v) is equivalent to
(¯u,

1 −¯u2 −¯v2, ¯v) = (u, v,

1 −u2 −v2).
This leads to
φ1(U1 ∩U3) = {(u, v) | u2 + v2 < 1 and v > 0},
φ3(U1 ∩U3) = {(¯u, ¯v) | ¯u2 + ¯v2 < 1 and ¯v > 0},

86
3. Differentiable Manifolds
and
φ31(u, v) = (u,

1 −u2 −v2).
It is now easy to verify that φ31 is of class C1 over φ1(U1 ∩U3). In fact,
higher derivatives of φ31 involve polynomials in u and v possibly divided
by powers of
√
1 −u2 −v2. Hence, over φ1(U1 ∩U3), the function φ31 is
of class C∞. It is not hard to see that all other transition functions are
similar. Thus, this atlas A = {(Ui, φi)}6
i=1 equips S2 with the structure of
a smooth manifold.
Example 3.1.6 (Projective Space). The n-dimensional real projective space RPn
is deﬁned as the set of lines in Rn+1 through the origin.
No two lines
through the origin intersect any place else and, for each point p in Rn,
there exists a unique line through the origin and p. Therefore, one can de-
scribe RPn as the set of equivalence classes of points in Rn+1−{(0, 0, . . ., 0)}
under the equivalence relation
(x0, x1, . . . , xn) ∼(y0, y1, . . . , yn)
if and only if
(y0, y1, . . . , yn) = (λx0, λx1, . . . , λxn) for some λ ∈R −{0}.
One often designates the equivalence class of a point (x0, x1, . . . , xn) by the
notation (x0 : x1 : . . . : xn). The set RPn is a topological space with the
identiﬁcation topology coming from Rn+1 −{0}.
One can deﬁne an atlas on RPn as follows. Note that if (x0, x1, . . . , xn) ∼
(y0, y1, . . . , yn), then for any i, we have xi = 0 if and only if yi = 0. For
i ∈{0, 1, . . ., n}, deﬁne Ui = {(x0 : x1 : . . . : xn) ∈RPn | xi ̸= 0} and deﬁne
φi : Ui →Rn by
φi(x0 : x1 : . . . : xn) =
x0
xi
, x1
xi
, . . . , &
xi
xi
, . . . , xn
xi
	
,
where the 'a indicates ignoring that entry. It is easy to see that each φi is a
homeomorphism between Ui and Rn. Furthermore, U0 ∪· · · ∪Un includes
all ratios (x0 : . . . : xn) for which not all xi = 0. Thus, U0 ∪· · ·∪Un = RPn.
Now assume, without loss of generality, that i < j and analyze the
change of coordinates in any Ui∩Uj. We have φi(Ui∩Uj) = {(a1, . . . , an) ∈
Rn | aj ̸= 0} and φj(Ui ∩Uj) = {(a1, . . . , an) ∈Rn | ai+1 ̸= 0}.
(The
diﬀerence comes from i < j.)
Then the change-of-coordinate function
φj ◦φ−1
i
is

3.1. Deﬁnitions and Examples
87
φj ◦φ−1
i (a1, a2, . . . , an) = φj(a1 : a2 : . . . :
1
()*+
ith
: . . . : an)
=
⎛
⎜
⎜
⎜
⎝
a1
aj
, a2
aj
, . . . ,
1
aj
()*+
ith
, . . . , &
aj
aj
()*+
jth
, . . . , an
aj
⎞
⎟
⎟
⎟
⎠.
It is easy to see that this is indeed a bijection between φi(Ui ∩Uj) and
φj(Ui ∩Uj). Furthermore, all higher partial derivatives of φj ◦φ−1
i
exist
over φi(Ui ∩Uj).
The same reasoning works if i > j. Therefore, this atlas satisﬁes the
condition required to equip RPn with the structure of a smooth manifold.
Before providing more examples, we must emphasize a technical aspect
to the deﬁnition of a diﬀerentiable manifold. If M is a topological manifold
not inherently deﬁned as the subset of a Euclidean space, one does not show
whether M is or is not a diﬀerentiable manifold, but rather, one discusses
whether one can equip M with a diﬀerentiable atlas.
This amounts to
providing a set M with an atlas A = {φα}α∈I that satisﬁes Deﬁnition 3.1.3.
Two diﬀerentiable (respectively, Ck, smooth, analytic) atlases {φα} and
{ψi} on a topological manifold M are said to be compatible if the union
of the two atlases is again an atlas on M in which all the transition func-
tions are diﬀerentiable (respectively, Ck, smooth, analytic). Interestingly
enough, not all atlases are compatible in a given category. It is also possible
for the union of two atlases of class Ck to form an atlas of class Cl, with
l < k. The notion of compatibility between atlases is an equivalence rela-
tion, and an equivalence class of diﬀerentiable (respectively, Ck, smooth,
analytic) atlases is called a diﬀerentiable (respectively, Ck, smooth, ana-
lytic) structure. Proving that a given topological manifold has a unique
diﬀerentiable structure or enumerating the diﬀerentiable structures on a
given topological manifold involves techniques that are beyond the scope
of this book. For example, in 1963, Kervaire and Milnor proved that S7
has exactly 28 nondiﬀeomorphic smooth structures [28].
Example 3.1.7. We point out that for any integer n ≥1, the Euclidean space
Rn is an n-dimensional manifold. (The standard atlas consists of only one
function, the identity function on Rn.)
Example 3.1.8. A manifold M of dimension 0 is a set of points with the
discrete topology, i.e., every subset of M is open. If M is a subset of Rn,

88
3. Differentiable Manifolds
then the points of M are isolated. The notion of diﬀerentiability is vacuous
over a 0-dimensional manifold.
Note that this example indicates that a manifold is not necessarily con-
nected but may be a union of connected components, each of which is a
manifold in its own right.
When working with examples of manifolds in Rk, it is often easier to
specify coordinate charts x : U ⊂M n →Rn by providing a parametrization
x−1 : x(U) →M n that is homeomorphic with its image. Since the chart is
a homeomorphism, this habit does not lead to any diﬃculties.
Example 3.1.9. Consider a trefoil knot K in R3. One can realize K as the
image of the parametric curve
γ(t) = ((2 + cos(3t)) cos(2t), (2 + cos(3t)) sin(2t), sin(3t))
for t ∈R. We can choose an atlas of K as follows. Set one coordinate
patch on K to be U1 = γ((0, 2π)) and another coordinate patch to be
U2 = γ((π, 3π)). Use as charts the functions φ1 and φ2, which are the
inverse functions of γ : (0, 2π) →K and γ : (π, 3π) →K, respectively.
Now
φ2(U1 ∩U2) = (π, 2π) ∪(2π, 3π)
and
φ1(U1 ∩U2) = (0, π) ∪(π, 2π),
and the coordinate transition functions are
φ1 ◦φ−1
2 (t) =

t,
if t ∈(π, 2π),
t −2π,
if t ∈(2π, 3π);
φ2 ◦φ−1
1 (t) =

t + 2π,
if t ∈(0, π),
t,
if t ∈(π, 2π).
Both of these transition functions are diﬀerentiable on their domains. This
shows that K, equipped with the given atlas, is a 1-manifold.
From the previous example, it is easy to see that any regular, simple,
closed curve in Rk can be given an atlas that gives it the structure of a
diﬀerentiable 1-manifold. Our intuition might tell us that, say, a square in
the plane should not be a diﬀerentiable 1-manifold because of its corners.
This idea, however, is erroneous, as we shall now explain.
Example 3.1.10. Consider the square with unit length side, and deﬁne two
chart functions as follows. The function φ1 measures the distance traveled

3.1. Deﬁnitions and Examples
89
0
1
2
3
4
1
2
3
4
5
Figure 3.4. A square as a diﬀerentiable manifold.
as one travels around the square in a counterclockwise direction, starting
with a value of 0 at (0, 0). The function φ2 measures the distance traveled
as one travels around the square in the same direction, starting with a value
of 1 at (1, 0) (see Figure 3.4).
The functions φ1 and φ2 are homeomorphisms, and the coordinate tran-
sition function is
φ1 ◦φ−1
2
: (1, 4) ∪(4, 5) →(0, 1) ∪(1, 4),
with
φ1 ◦φ−1
2 (x) =

x,
if x ∈(1, 4),
x −4,
if x ∈(4, 5).
This transition function (and its inverse, the other transition function) is
diﬀerentiable over its domain.
Therefore, the atlas {φ1, φ2} equips the
square with the structure of a diﬀerentiable manifold.
This example shows that, in and of itself, the square can be given the
structure of a diﬀerentiable 1-manifold. However, this does not violate our
intuition about diﬀerentiability and smoothness because one only perceives
the “sharp” corners of the square in reference to the diﬀerential structure
of R2.
Once we have the appropriate deﬁnitions, we will say that the
square is not a submanifold of R2 with the usual diﬀerential structure (see
Deﬁnition 3.4.1). In fact, the atlases in Examples 3.1.9 and 3.1.10 bear
considerable similarity, and, ignoring the structure of the ambient space,
both the square and the knot resemble a circle. We develop these notions
further when we consider functions between manifolds.
It is not hard to verify that a regular surface S in R3 (see Deﬁni-
tion 3.1.1) is a diﬀerentiable 2-manifold.
The only nonobvious part is
showing that the properties of coordinate patches of a regular surface imply

90
3. Differentiable Manifolds
Figure 3.5. Not a bijection.
p
p
q
q
Figure 3.6. The Klein bottle.
that the coordinate transition functions are diﬀerentiable. We leave this as
an exercise for the reader (see Problem 3.1.6).
Parametrized surfaces that are not regular surfaces provide examples
of geometric sets in R3 that are not diﬀerentiable manifolds. For exam-
ple, with the surface in Figure 3.5, for any point along the line of self-
intersection, there cannot exist an open set of R2 that is in bijective cor-
respondence with any given neighborhood of p. However, the notion of a
regular surface in R3 has more restrictions than that of a 2-manifold for
two reasons. Applying the ideas behind Example 3.1.10, a circular (single)
cone can be given the structure of a diﬀerentiable manifold even though it
is not a regular surface. Furthermore, not every diﬀerentiable 2-manifold
can be realized as a regular surface or even as a union of such surfaces in
R3. A simple example is the Klein bottle, deﬁned topologically as follows.
Consider a rectangle, and identify opposite edges according to Figure 3.6.
One pair of sides is identiﬁed directly opposite each other (the horizontal
edges in Figure 3.6), and the other pair of sides is identiﬁed in the reverse
direction.
It is not hard to see that the Klein bottle can be given an atlas that
makes it a diﬀerentiable 2-manifold. However, it turns out that the Klein
bottle cannot be realized as a regular surface in R3.
Despite the ﬂexibility of the deﬁnition of a manifold with a given dif-
ferential structure, it does not allow for a boundary. In many interesting
applications, it is useful to have the notion of a manifold with a boundary.
A separate deﬁnition is needed.
Deﬁnition 3.1.11. A diﬀerentiable n-manifold M with boundary has the same
deﬁnition as in Deﬁnition 3.1.3 except that the codomains for the charts

3.1. Deﬁnitions and Examples
91
can be open subsets of the half-space
{(x1, x2, . . . , xn) ∈Rn | xn ≥0}.
The boundary of the manifold, written ∂M, is the set consisting of all points
mapped to {(x1, x2, . . . , xn) | xn = 0} by a chart.
For simplicity of notation, we will write Rn
+ for {(x1, x2, . . . , xn) ∈
Rn | xn ≥0} and will refer to it as the upper half-space in Rn.
Proposition 3.1.12. Let M be a diﬀerentiable (respectively, Ck, smooth, an-
alytic) n-manifold with boundary. Its boundary ∂M is a diﬀerentiable (re-
spectively, Ck, smooth, analytic) (n −1)-manifold without boundary.
Proof: Let A = {φα}α∈I be an atlas for M, where the codomain for all
charts is Rn
+. Consider the projection π : Rn
+ →Rn−1 deﬁned by ignoring
the last coordinate xn. Let I′ be the subset of the indexing set I such that
the domain of φα contains points of ∂M. For α ∈I′, since π|xn=0 is a
homeomorphism with Rn−1, then the restricted function ψα = (π ◦φα)|∂M
is a homeomorphism onto its image.
Furthermore, the domains of ψα
for α ∈I′ cover ∂M.
Since the transition functions for the collection
A′ = {ψα}α∈I′ are restrictions of the transition functions of M, then ∂M
has the same diﬀerential (respectively, Ck, smooth, or analytic) structure
as M.
□
Example 3.1.13. As an example of a manifold with boundary, consider the
half-torus in R3 given as the image of ⃗X : [0, π] × [0, 2π] →R3, with
⃗X(u, v) = ((2 + cos v) cos u, (2 + cos v) sin u, sin v) .
The image of ⃗X is a half-torus M with y ≥0, which, to conform to Deﬁni-
tion 3.1.11, is easily covered by four coordinate patches. The boundary ∂M
is the disconnected manifold with the following two connected components:
R+ = {(x, y, z) | (x + 2)2 + z2 = 1, y = 0},
R−= {(x, y, z) | (x −2)2 + z2 = 1, y = 0}.
Deﬁnition 3.1.14 (Orientability).
Let M n be a diﬀerentiable n-manifold
equipped with an atlas {φα}α∈I. Suppose that for any two charts φα and
φβ of the atlas, the Jacobian of the transition function φβα = φβ ◦φ−1
α
is
positive at all points in the domain. Then M is called an oriented manifold
or is referred to as an orientable manifold.

92
3. Differentiable Manifolds
Deﬁnition 3.1.15. Let M n be a diﬀerentiable n-manifold equipped with two
separate atlases A = {φα}α∈I and B = {ψβ}β∈J. Suppose that both atlases
equip M with the structure of an oriented manifold. The atlases A and B
are said to have equivalent orientations if the atlas A ∪B also makes M
an oriented manifold. An equivalence class of oriented atlases is called an
orientation.
Example 3.1.16. Returning to Example 3.1.4 of the sphere, it turns out that
the atlas {πN, πS} does not show that the sphere is orientable. However, the
atlas {πN, ¯πS} does give the sphere an oriented structure, where ¯πS is the
composition of πS with the reﬂection (u, v) →(u, −v) so that ¯πS(x, y, z) =
(x/(1 + z), −y/(1 + z)). Writing (¯u, ¯v) = ¯πS ◦π−1
N (u, v), we get
¯u =
u
u2 + v2
and
¯v = −
v
u2 + v2 .
We easily ﬁnd that the Jacobian is
∂(¯u, ¯v)
∂(u, v) =

∂¯u
∂u
∂¯u
∂v
∂¯v
∂u
∂¯v
∂v
 =

u2−v2
(u2+v2)2
−
2uv
(u2+v2)2
2uv
(u2+v2)2
u2−v2
(u2+v2)2

=
1
(u2 + v2)2 .
(3.1)
This shows that the atlas {πN, ¯πS} gives the sphere the structure of an
oriented smooth manifold. From now on, we will typically use this atlas as
the standard atlas on S2.
We end the section by deﬁning the product structure of two manifolds.
Deﬁnition 3.1.17. Let M m and N n be two diﬀerential (respectively, Ck,
smooth, analytic) manifolds.
Call their respective atlases {φα}α∈I and
{ψi}i∈J. Consider the set M × N that is equipped with the product topol-
ogy. If φ : U →Rm is a chart for M and ψ : V →Rn is a chart for
N, then deﬁne the function φ × ψ : U × V →Rm+n.
The collection
{φα × ψi}(α,i)∈I×J deﬁnes a diﬀerential (respectively, Ck, smooth, ana-
lytic) structure on M × N, called the product structure.
Consider, for example, the circle S1 with a smooth structure.
The
product S1×S1 is topologically equal to a (two-dimensional) torus, and the
product structure deﬁnes a smooth structure on the torus. By extending
this construction, one can deﬁne the 3-torus as the manifold T 3 = S1 ×
S1 × S1 and inductively the n-torus as T n = T n−1 × S1.

3.1. Deﬁnitions and Examples
93
Problems
3.1.1. Stereographic Projection.
One way to deﬁne coordinates on the surface
of the sphere S2 given by x2 + y2 + z2 = 1 is to use the stereographic
projection of π : S2 −{N} →R2, where N = (0, 0, 1), deﬁned as follows.
Given any point p ∈S2, the line (pN) intersects the xy-plane at exactly
one point, which is the image of the function π(p). If (x, y, z) are the
coordinates for p in S2, let us write π(x, y, z) = (u, v) (see Figure 3.2).
(a) Prove that π(x, y, z) = (x/(1 −z), y/(1 −z)).
(b) Prove that
π−1(u, v) =

2u
u2 + v2 + 1,
2v
u2 + v2 + 1, u2 + v2 −1
u2 + v2 + 1
	
.
(c) Prove that, using stereographic projections, it is possible to cover
the sphere with two coordinate neighborhoods.
3.1.2. Consider the n-dimensional sphere Sn = {(x1, . . . , xn+1) ∈Rn+1 | x2
1+· · ·+
x2
n+1 = 1}. Exhibit an atlas that gives Sn the structure of a diﬀerentiable
n-manifold. Explicitly show that the atlas you give satisﬁes the axioms of
a manifold.
3.1.3. Explicitly show that solid ball Bn = {(x1, . . . , xn) ∈Rn | x2
1+· · ·+x2
n ≤1}
is a smooth n-manifold with boundary and show that its boundary is the
sphere Sn−1.
3.1.4. Let V be an open set in Rk, and let f : V →Rm be a diﬀerentiable
function. Prove that the graph of f,
f(x) = {(x, f(x)) ∈Rk+m | x ∈V },
is a diﬀerentiable k-manifold.
3.1.5. Describe an atlas for the 3-torus T 3 = S1 × S1 × S1. Find a parametric
function X : U →R4, where U is a subset of R3, such that the image of
X is a 3-torus.
3.1.6. Prove that a regular surface in R3 (see Deﬁnition 3.1.1) is a diﬀerentiable
2-manifold.
3.1.7. Show that if M is a compact manifold, then so is ∂M. [Hint: See Deﬁni-
tion A.3.45.]
3.1.8. Prove that if M is an orientable manifold with boundary, then ∂M is also
orientable.
3.1.9. Consider X the subset of R2 given as the graph of the function f(x) = |x|.
Deﬁne a set of charts on X in which the charts are deﬁned as restrictions
to X of functions R2 →R that are smooth on R2 in the usual sense. Show
that this set of charts does not equip X with a diﬀerentiable structure.

94
3. Differentiable Manifolds
3.1.10. Consider the following two parametrizations of the circle S1 as a sub-
set of R2:
X1(t) = (cos t, sin t)
for t ∈(0, 2π),
Y1(t) =
1 −t2
1 + t2 ,
2t
1 + t2
	
.
Find functions X2 and Y2 “similar” to X1 and Y1 respectively, to make
{X1, X2} and {Y1, Y2} atlases that give S1 diﬀerentiable structures. Show
that these two diﬀerentiable structures are compatible.
3.1.11. Consider the real projective plane RP2. The atlas described for RP2 has
three coordinate charts.
(a) Calculate explicitly all six of the coordinate transition functions, and
verify directly that φij = φ−1
ji .
(b) Show that RP2 is not orientable.
3.1.12. Show that RPn is orientable if and only if n is odd.
3.1.13. Consider S2 to be the unit sphere in R3. Consider the parametrizations
f : (0, 2π) × (0, π) →S2,
with
f(u, v) = (cos u sin v, sin u sin v, cos v),
g : (0, 2π) × (0, π) →S2,
with
g(¯u, ¯v) = (−cos ¯u sin ¯v, cos ¯v, sin ¯u sin ¯v).
We have seen that f is injective and so is a bijection onto its range.
(a) Find the range U of f and the range V of g.
(b) Determine f −1(x, y, z) and g−1(x, y, z), where (x, y, z) ∈S2.
(c) Show that the set of functions {(U, f −1), (V, g−1)} forms an atlas for
S2 and equips S2 with a diﬀerentiable structure.
3.2
Differentiable Maps between Manifolds
From a purely set-theoretic perspective, it is easy to deﬁne functions be-
tween manifolds. Since diﬀerentiable manifolds are topological manifolds
to begin with, one can easily discuss continuous functions between mani-
folds just as one does in the context of topology. However, a diﬀerential
structure on a manifold expressed by a speciﬁc atlas allows one to make
sense of the notion of diﬀerentiable maps between manifolds.
Deﬁnition 3.2.1. Let M m and N n be diﬀerentiable (respectively, Ck, smooth,
analytic) manifolds. A continuous function f : M m →N n is said to be dif-
ferentiable (respectively, Ck, smooth, analytic) if for any chart y : V →Rn

3.2. Differentiable Maps between Manifolds
95
R2
R2
x
U
x(U)
M
f
y
V
y(V )
N
y ◦f ◦x−1
Figure 3.7. Diﬀerentiable map between manifolds.
on N and for any chart x : U →Rm on M, the map
y ◦f ◦x−1 : x(U ∩f −1(V )) ⊂Rm −→y(V ) ⊂Rn
is a diﬀerentiable (respectively, Ck, smooth, analytic) function (see Fig-
ure 3.7).
In the above deﬁnition, the domain and codomain of y◦f◦x−1 may seem
complicated, but the domain is simply the natural one for this composition
of functions.
Note that from the deﬁnition it follows that a function between two
manifolds cannot have a stronger diﬀerentiability property than do the
manifolds themselves.
In linear algebra, where one studies the category of vector spaces, one
does not study properties of any kind of function between two vector spaces.
Instead, we restrict the attention to linear transformations. In an intuitive
sense, one does this because linear transformations “preserve the structure”
of vector spaces. Furthermore, from the perspective of linear algebra, two
vector spaces V and W are considered the same (isomorphic) if there ex-
ists a bijective linear transformation between the two. In the same way,
in the category of diﬀerentiable manifolds, one is primarily interested in
diﬀerentiable (or perhaps Ck or smooth) maps between these manifolds.

96
3. Differentiable Manifolds
The following deﬁnition is the equivalent notion of an isomorphism but for
manifolds.
Deﬁnition 3.2.2. Let M and N be two diﬀerentiable manifolds. A diﬀeomor-
phism (respectively, Ck diﬀeomorphism) between M and N is a bijective
function F : M →N such that F is diﬀerentiable (respectively, Ck) and
F −1 is diﬀerentiable (respectively, Ck). If a diﬀeomorphism exists between
M and N, we say that M and N are diﬀeomorphic.
Example 3.2.3. Consider the map f : S2 →RP2 that identiﬁes antipodal
points on the unit sphere
f(x, y, z) = (x : y : z)
for any (x, y, z) ∈S2. For S2, we use the atlas {πN, ¯πS} as presented in
Example 3.1.16, and for RP2, we use the atlas in Example 3.1.6, namely,
φi : Ui →R2 for 0 ≤i ≤2, with φi(x0 : x1 : x2) = (. . . , ˆxi, . . .).
For each pairing of coordinate charts we have
φi ◦f ◦π−1
N (u, v) = φi

2u
u2 + v2 + 1 :
2v
u2 + v2 + 1 : u2 + v2 −1
u2 + v2 + 1
	
and
φi ◦f ◦¯π−1
S (u, v) = φi

−
2u
u2 + v2 + 1 :
2v
u2 + v2 + 1 : u2 + v2 −1
u2 + v2 + 1
	
.
In all six cases, the resulting functions are diﬀerentiable functions R2 →
R2. This shows that the antipodal identiﬁcation map f : S2 →RP2 is
diﬀerentiable.
Example 3.2.4. Similar to the real projective space RPn, we can also deﬁne
the complex projective space CPn as follows.
Deﬁne the relation ∼on
nonzero (n + 1)-tuples in Cn+1 by (z0, z1, . . . , zn) ∼(w0, w1, . . . , wn) if and
only if there exists λ ∈C such that wi = λzi for 0 ≤i ≤n. This relation
is an equivalence relation, and the complex projective space CPn is the set
of equivalence classes. In fewer words, one typically writes
CPn =

Cn+1 −{(0, . . . , 0)}

/ {(z0, z1, . . . , zn) ∼(λz0, λz1, . . . , λzn), for λ ∈C} .
One writes (z0 : z1 : · · · : zn) for the equivalence class of (z0, z1, . . . , zn).
The stereographic projection πN of the sphere onto the plane sets up a
homeomorphism h between CP1 and the unit 2-sphere S2 such that
h(z0 : z1) =

π−1
N (z1/z0),
if z0 ̸= 0,
(0, 0, 1),
if z0 = 0.

3.2. Differentiable Maps between Manifolds
97
Note that if z0 ̸= 0, then there is a unique z′ such that (z0 : z1) = (1, z′),
namely, z′ = z1/z0, and that if z0 = 0, then (z0 : z1) = (0, z1) for all
z1 ̸= C −{0}. Therefore, one sometimes says that CP1 is the complex
plane C with a “point at inﬁnity,” where this point at inﬁnity corresponds
to the class of (0 : z1). The function h is a bijection that maps the point
at inﬁnity to the north pole of the sphere, but we leave it as an exercise
for the reader to verify that this function is indeed a homeomorphism.
In complex analysis, one studies holomorphic (i.e., analytic) functions.
This notion is tantamount to diﬀerentiable in the complex variable. Any
holomorphic function f : C →C deﬁnes a map pf : S2 →S2 by identifying
R2 with C and
pf(q) =

π−1
N ◦f ◦πN(q),
if q ̸= (0, 0, 1),
(0, 0, 1),
if q = (0, 0, 1).
(That pf must send (0, 0, 1) to (0, 0, 1) follows from Liouville’s Theorem in
complex analysis.)
Consider S2 as a diﬀerentiable manifold with atlas {πN, ¯πS}, as de-
scribed in Example 3.1.16. It is interesting to notice that, according to
Example 3.1.16, the change-of-coordinates map ¯πS ◦πN corresponds to
z →1/z over C −{0}.
Take for example f(z) = z2. The associated function pf leaves (0, 0, −1)
and (0, 0, 1) ﬁxed and acts in a nonobvious manner on S2. According to
Deﬁnition 3.2.1, in order to verify the diﬀerentiability of pf as a function
S2 →S2, we need to determine explicitly the four combinations
(πN or ¯πs) ◦pf ◦(πN or ¯πs)−1
and show that they are diﬀerentiable on their appropriate domains.
Setting z = u + iv, we have z2 = (u2 −v2) + (2uv)i. Since we are using
the stereographic projection from the north pole to deﬁne pf in the ﬁrst
place, we have πN ◦pf ◦π−1
N (u, v) = (u2 −v2, 2uv). Determining the other
three combinations, we ﬁnd that
πN ◦pf ◦π−1
N (u, v) = (u2 −v2, 2uv),
¯πS ◦pf ◦π−1
N (u, v) =
 u2 −v2
(u2 + v2)2 , −
2uv
(u2 + v2)2
	
,
πN ◦pf ◦¯π−1
S (u, v) =
 u2 −v2
(u2 + v2)2 , −
2uv
(u2 + v2)2
	
,
¯πS ◦pf ◦¯π−1
S (u, v) = (u2 −v2, 2uv).

98
3. Differentiable Manifolds
It is not hard to show that, with f(z) = z2, the corresponding natural
domains of these four functions are R2, R2 −{(0, 0)}, R2 −{(0, 0)}, and
R2. Then it is an easy check that all these functions are diﬀerentiable on
their domain and, hence, that pf is a diﬀerentiable function from S2 to S2.
Since R is a one-dimensional manifold, if M is a diﬀerentiable manifold,
then one can discuss whether a real-valued function f : M →R is diﬀeren-
tiable by testing it against Deﬁnition 3.2.1. Suppose also that p is a point
of M and that x : U →Rm is a coordinate chart of a neighborhood of p.
We can write x as an m-tuple of functions x = (x1, x2, . . . , xm). Then we
deﬁne the partial derivative of f at p in the xi coordinate as
∂f
∂xi

p
def
= ∂(f ◦x−1)
∂xi

x(p)
.
(3.2)
The notation on the left-hand side is deﬁned as the partial derivative of the
right-hand side, which is taken in the usual multivariable calculus sense.
The notion of a diﬀerentiable map between diﬀerentiable manifolds also
allows us to easily deﬁne what we mean by a curve on a manifold.
Deﬁnition 3.2.5. Let M be a diﬀerentiable manifold. A diﬀerentiable curve
on M is a diﬀerentiable function f : (a, b) →M, where the interval (a, b)
is understood as a one-dimensional manifold with the diﬀerential structure
inherited from R. A diﬀerentiable curve with endpoints on M is a diﬀeren-
tiable function f : [a, b] →M, where the interval [a, b] is understood as a
one-dimensional manifold with boundary. A closed diﬀerentiable curve on
M is a diﬀerentiable function f : S1 →M, where S1 is the circle manifold.
Problems
3.2.1. Consider the antipodal identiﬁcation map described in Example 3.2.3. Ex-
plicitly write out all six functions φi ◦f ◦πN and φi ◦f ◦¯πS. Prove that
each one is diﬀerentiable on its natural domain.
3.2.2. In Example 3.2.4, with f(z) = z2, consider points on the unit sphere S2
with coordinates (x, y, z) ∈R3. Express pf on S2 −{(0, 0, 1)} in terms of
(x, y, z)-coordinates by calculating π−1
N ◦f ◦πN(x, y, z).
3.2.3. Consider the torus T 2 in R3 parametrized by
X(u, v) = ((2 + cos v) cos u, (2 + cos v) sin u, sin v)
for (u, v) ∈[0, 2π]2. Consider the Gauss map of the torus n : T 2 →S2
that sends each point of the torus to its outward unit normal vector as an
element of S2. Using the stereographic projection of the sphere, explicitly
show that this Gauss map is diﬀerentiable.

3.3. Tangent Spaces and Differentials
99
3.2.4. Consider the (unit) sphere given with the atlas deﬁned by stereographic
projection A = {(S2 −{(0, 0, 1)}, πN), (S2 −{(0, 0, −1)}, ¯πS)}, where
(u, v) = πN(x, y, z) =

x
1 −z ,
y
1 −z
	
and
(¯u, ¯v) = ¯πS(x, y, z) =

x
1 + z , −
y
1 + z
	
.
Consider the function f : S2 →R given by f(x, y, z) = z in terms of
Cartesian coordinates.
(a) Show that for points in the sphere in the coordinate chart of πN, a
formula for the partial derivatives of f is
∂f
∂u =
4u
(u2 + v2 + 1)2
and
∂f
∂v =
4u
(u2 + v2 + 1)2 .
(b) Find a formula for the partial derivatives ∂f
∂¯u and ∂f
∂¯v over the coor-
dinate chart ¯πS.
(c) Explain in what sense these partial derivatives are equal over S2 −
{(0, 0, 1), (0, 0, −1)}.
[Hint: Use the chain rule and the Jacobian
matrix from Equation (3.1).]
3.2.5. Consider the 3-sphere described by S3 = {(z1, z2) ∈C2 | |z1|2 + |z2|2 = 1}.
Consider the function f : S3 →S2 deﬁned by f(z1, z2) = (z1 : z2) where
we identify S2 with CP1. Prove that this function is diﬀerentiable. (This
function is called the Hopf map.)
3.2.6. Let f : Rn+1 −{0} →Rm+1 −{0} be a diﬀerentiable map. Let d ∈Z, and
suppose that f is such that f(λx) = λdf(x) for all λ ∈R −{0} and all
x ∈Rn+1 −{0}. Such a map is said to be homogeneous of degree d. For
any x ∈Rk+1 −{0}, denote by ¯x the corresponding equivalence class in
RPk. Show that the map F : RPn →RPm deﬁned by F(¯x) = f(x) is well
deﬁned and diﬀerentiable.
3.3
Tangent Spaces and Differentials
3.3.1
Tangent Vectors
In the local theory of regular surfaces S ⊂R3, the tangent plane plays a
particularly important role. One also deﬁnes the ﬁrst fundamental form on
the tangent plane as the restriction of the dot product in R3 to the tangent.
From the coeﬃcients of the ﬁrst fundamental form, one obtains all the con-
cepts of intrinsic geometry, which include angles between curves, areas of

100
3. Differentiable Manifolds
regions, Gaussian curvature, geodesics, and even the Euler characteristic
(see references to intrinsic geometry in [5]). The deﬁnition of a real diﬀer-
entiable manifold, however, makes no reference to an ambient Euclidean
space, so one cannot imitate the theory of surfaces in R3 to deﬁne a tangent
space to a manifold as a vector subspace of some Rn.
The reader can anticipate that to circumvent this diﬃculty, we must
take a step in the direction of abstraction.
The strategy is to deﬁne a
tangent vector as a directional derivative at a point p of a real-valued
function on a manifold M. Furthermore, since we cannot use vectors in
an ambient Euclidean space to provide the notion of direction, we simply
talk about curves on M through p as providing a notion of direction. The
following construction makes this precise.
Deﬁnition 3.3.1. Let M m be a diﬀerentiable manifold, p be a point on M,
and let U be an open neighborhood of M. Let ε > 0, and let γ : (−ε, ε) →U
be a diﬀerentiable curve on M such that γ(0) = p. For any diﬀerentiable
function f : U ⊂M →R, we deﬁne the directional derivative of f along γ
to be the number
Dγ(f) = d
dt(f(γ(t)))

t=0.
(3.3)
The operator Dγ is called the tangent vector to γ at p. If γ1 and γ2 are
two curves, then we set Dγ1 = Dγ2 if these operators have the same value
at p for all diﬀerentiable functions f : U →R.
Note that f ◦γ is a function (−ε, ε) →R, so the derivative in Equa-
tion (3.3) is taken as a usual real function. It is also interesting to observe
that the above deﬁnition does not explicitly refer to any particular chart
on U.
The above deﬁnition of a tangent vector may initially come as a source
of mental discomfort since it presents tangent vectors as operators instead
of as the geometric objects with which one is used to working. However, any
tangent vector (deﬁned in the classical sense) to a regular surface S in R3
naturally deﬁnes a directional derivative of a function S →R so Deﬁnition
3.3.1 generalizes the usual notion of a tangent vector (see [5, Section 5.2]).
Furthermore, as the name “tangent vector” suggests, the set of all tangent
vectors forms a vector space, a fact that we show now.
Call C1(U, R) the set of all diﬀerentiable functions from U to R. A
priori, the set of tangent vectors Dγ at p on M is a subset of all operators
W = {C1(U, R) →R}. As a set of functions with domains in R, one can
naturally deﬁne addition and scalar multiplication on W. We need to show

3.3. Tangent Spaces and Differentials
101
that the set of tangent vectors is closed under scalar multiplication and
addition.
Let γ : (−ε, ε) →M be a diﬀerentiable curve with γ(0) = p. If we
deﬁne γ1(t) = γ(at), where a is some real number, then using the usual
chain rule for any diﬀerentiable function f ∈C1(U, R), we have
Dγ1(f) = d
dt(f(γ(at)))

t=0 = a d
dt(f(γ(t)))

t=0 = aDγ(f).
This shows that the set of tangent vectors is closed under scalar multipli-
cation.
In order to prove that the set of tangent vectors is closed under addition,
we make reference to the coordinate chart x : U →Rm. We rewrite the
composition f ◦γ = f ◦x−1 ◦x ◦γ where x ◦γ : (−ε, ε) →Rm and
f ◦x−1 : x(U) ⊂Rm →R. By the chain rule in multivariable analysis,
Theorem (1.3.3), we have
Dγ(f) = d
dt(f(γ(t)))

t=0
= d
dt(f ◦x−1(x ◦γ(t)))

t=0
= d(f ◦x−1)pd(x ◦γ)

t=0.
Now let x : U →Rm be a coordinate chart of a neighborhood of p on
M such that x(p) = (0, · · · , 0). Let α and β be two diﬀerentiable curves
on M such that α(0) = β(0) = p. Over the intersection of the domains of
α and β, deﬁne the curve γ by
γ(t) = x−1 (x ◦α(t) + x ◦β(t)) .
Then for any function f : U →R, we have
Dα(f) + Dβ(f) = d(f ◦x−1)pd(x ◦α)|t=0 + d(f ◦x−1)pd(x ◦β)|t=0
= d(f ◦x−1)p (d(x ◦α)|t=0 + d(x ◦β)|t=0)
= d(f ◦x−1)pd(x ◦α + x ◦β)|t=0
= Dγ(f).
Thus, the set of tangent vectors is closed under addition. This brings
us in a position to prove the following foundational fact.
Proposition 3.3.2. Let M be a diﬀerentiable manifold of dimension m, and
let p be a point of M. The set of all tangent vectors to M at p is a vector
space of dimension m with basis {∂/∂xi | i = 1, . . . , m}.

102
3. Differentiable Manifolds
Deﬁnition 3.3.3. The vector space of tangent vectors is called the tangent
space of M at p and is denoted by TpM.
Proof (of Proposition 3.3.2): The prior discussion has shown that the set
TpM is a vector space. It remains to be shown that it has dimension m.
Let x : U →Rm be a system of local coordinates at p. Write x(q) =
(x1(q), . . . , xm(q)), and deﬁne the coordinate line curve vi : (−ε, ε) →M by
vi(t) = x−1(0, . . . , 0, t, 0, . . ., 0) where the t occurs in the ith place. Then
Dvi(f) = d
dt

f ◦x−1(0, . . . , 0, t, 0, . . ., 0)
 
t=0 = ∂f
∂xi

p
according to the notation given in Equation (3.2). We can therefore write,
as operators, Dvi =
∂
∂xi |p.
For any diﬀerentiable curve γ on M with γ(0) = p, we can then write
in coordinates x ◦γ(t) = (γ1(t), . . . , γm(t)), where γi = xi(γ(t)). Then
Dγ(f) = d
dtf ◦x−1(γ1(t), . . . , γm(t))

t=0
=
m

i=1
∂f
∂xi

p
dγi
dt

t=0.
This presents the operator Dγ as a linear combination of the operators
∂
∂xi |p.
It is also a trivial matter to show that for 1 ≤i ≤m, the operators
∂
∂xi |p are linearly independent. Consequently, they form a basis of TpM,
which proves that dim TpM = m.
□
Because the operators
∂
∂xi occur so often in the theory of manifolds,
one often uses an abbreviated notation. Whenever the coordinate system
is understood by context, where one uses x = (x1, . . . , xn) or another letter,
we write
∂i
def
=
∂
∂xi ,
(3.4)
whose explicit meaning is given by Equation (3.2). This notation shortens
the standard partial derivative notation and makes it easier to write it in
inline formulas.
The tangent space TpM to M at p carries additional structure than
just a vector subspace of W. Any tangent vector X ∈TpM is an operator
on C1(U, R). More precisely, X is a derivation on the algebra C1(U, R) of
functions because it satisﬁes

3.3. Tangent Spaces and Differentials
103
1. linearity: X(af + bg) = aX(f) + bX(g) for all f, g ∈C1(U, R) and
a, b ∈R;
2. Leibniz’s rule: X(fg) = X(f)g(p) + f(p)X(g) for all f, g ∈C1(U, R).
(An algebra is a vector space V with the additional structure of a multi-
plication between elements of V that only has to satisfy distributivity over
the addition of the vector space.)
Example 3.3.4 (Tangent Space of Rn). We consider the tangent space for the
manifold Rn itself.
We assume that the diﬀerential structure on Rn is
given by the atlas consisting simply of the identity map.
Let p be a point in Rn, and let v = (v1, . . . , vn) be a vector. Consider the
line traced out by the curve γ(t) = p+tv. We wish to ﬁnd the coordinates of
the tangent vector Dγ with respect to the standard basis of TpM, namely,
{ ∂
∂xi } or, according to the notation of Equation (3.4), {∂i}. For any real
function f deﬁned over a neighborhood of p, we have
Dγ(f) = d
dtf(p1 + tv1, . . . , pn + tvn)

t=0 =
n

i=1
∂f
∂xi

pvi
Therefore, with respect to the basis {∂/∂xi}, the coordinates of Dγ are
(v1, . . . , vn). Thus, at each p ∈Rn, the map v →Dγ sets up an isomor-
phism between the vector spaces Rn and Tp(Rn) by identifying ∂i with the
ith standard basis vector. It is common to abuse the notation and view
these spaces as equal.
Example 3.3.5 (Regular Surfaces). Let S be a regular surface in R3. In Chap-
ter 5 of [5], the authors deﬁne the tangent plane to S at p as the subspace of
R3 consisting of all vectors γ′(0), where γ(t) is a curve on S with γ(0) = p.
The correspondence γ′(0) ↔Dγ identiﬁes the tangent space for regular
surfaces with the tangent space of manifolds as deﬁned above. This shows
that the present deﬁnition directly generalizes the previous deﬁnition as a
subspace of the ambient space Rn.
In multivariable calculus, one shows that given a parametrization ⃗X :
V ⊂R2 →R3 of a coordinate patch of a regular surface, if p = ⃗X(u0, v0),
then a basis for TpS is
,
⃗Xu(u0, v0), ⃗Xv(u0, v0)
-
.
The deﬁnition of the tangent plane given in calculus meshes with Deﬁni-
tion 3.3.1 and Proposition 3.3.2 in the following way. A tangent vector in

104
3. Differentiable Manifolds
the classical sense, ⃗w ∈TpM, is a vector such that ⃗w = ⃗γ′(t0), where ⃗γ(t) is
a curve on S with ⃗γ(t0) = p. Write ⃗γ(t) = ⃗X(α(t)), with α(t0) = (u0, v0).
Writing α(t) = (u(t), v(t)), we have
⃗w = u′(t0) ⃗Xu(u0, v0) + v′(t0) ⃗Xv(u0, v0).
Now the corresponding coordinate chart x on S in the language of
manifolds is the inverse of the parametrization x = ⃗X−1 deﬁned over
U = ⃗X(V ) ⊂S. The tangent vector (in the phrasing of Deﬁnition 3.3.1)
associated to γ at p is
Dγ(f) = d
dt (f(⃗γ(t)))

t0 = d
dt

f

⃗X(α(t))
 
t0 = d
dt

f ◦x−1(α(t))
 
t0
= u′(t0)∂f
∂u

p + v′(t0)∂f
∂v

p.
where the partial derivatives
∂f
∂u|p and
∂f
∂v |p are in the sense of Equa-
tion (3.2). We can write as operators
Dγ = u′(t0) ∂
∂u

p + v′(t0) ∂
∂v

p.
Therefore, we see that the correspondence between the deﬁnition of the
tangent space for manifolds and the deﬁnition for tangent spaces to reg-
ular surfaces in R3 identiﬁes ⃗Xu(u0, v0) with
∂f
∂u|p and similarly for the
v-coordinate.
3.3.2
The Differential of a Map
Having established the notion of a tangent space to a diﬀerentiable manifold
at a point, we are in a position to deﬁne the diﬀerential of a diﬀerentiable
map f : M →N. In some sense, this is where analysis on manifolds begins.
Recall that in multivariable real analysis, we call a function F : Rm →Rn
diﬀerentiable at a point ⃗p ∈Rm if there exists an n×m matrix A such that
F(⃗p + ⃗h) = F(⃗p) + A⃗h + R(⃗h)⃗h,
where R(⃗h) is a matrix function such that ∥R(⃗h)∥→0 as ∥⃗h∥→0. We refer
to the matrix A as the diﬀerential dFp. Surprisingly, given our deﬁnition
of the tangent space to a manifold, there exists a more natural way to
generalize the diﬀerential.

3.3. Tangent Spaces and Differentials
105
p
γ
TpM
φ(p)
φ ◦γ
Tφ(p)N
φ
Dγ
Dφ◦γ
dφp
Figure 3.8. The diﬀerential of a map between manifolds.
Deﬁnition 3.3.6. Let φ : M m →N n be a diﬀerentiable map between diﬀer-
entiable manifolds. We deﬁne the diﬀerential of φ at p ∈M as the linear
transformation between vector spaces
dφp : TpM −→Tφ(p)N,
Dγ −→Dφ◦γ.
The diﬀerential dφp is also denoted by φ∗when the point p is understood
by context. If X ∈TpM, then φ∗(X) is also called the push-forward of X
by φ. (See Figure 3.8.)
From this deﬁnition, it is not immediately obvious that dφp is linear,
but, as the following proposition shows, we can give an equivalent deﬁnition
of the diﬀerential that makes it easy to show that the diﬀerential is linear.
Proposition 3.3.7. Let φ : M →N be a diﬀerentiable map between diﬀeren-
tiable manifolds. Then at each p ∈M, the function φ∗= dφp satisﬁes
φ∗(X)(g) = X(g ◦φ)

106
3. Differentiable Manifolds
for every vector X ∈TpM and every function g from N into R deﬁned in
a neighborhood of φ(p). Furthermore, φ∗is linear.
Proof: Let X ∈TpM, with X = Dγ, for some curve γ on M with γ(0) = p.
For every real-valued function g deﬁned in a neighborhood of φ(p) on N,
we have
φ∗(X)(g) = dφp(Dγ)(g) = Dφ◦γ(g)
= d
dt(g ◦φ ◦γ)(t)

p = Dγ(g ◦φ) = X(g ◦φ).
Now, let X, Y ∈TpM and a, b ∈R. Then
φ∗(aX + bY )(g) = aX(g ◦φ) + bY (g ◦φ) = aφ∗(X)(g) + bφ∗(Y )(g),
which shows that φ∗is linear.
□
Note that this deﬁnition is independent of any coordinate system near
p or φ(p).
However, given speciﬁc coordinate charts x : U →Rm and
y : V →Rn whose domains are, respectively, neighborhoods of p in M and
φ(p) in N, with f(U) ⊂V , we can deﬁne a matrix that represents dφp. Set
vi as the coordinate line for the variable xi in the chart x. In the usual
basis of TpM, we have
φ∗(Dvi) = φ∗
 ∂
∂xi
	
= Dφ◦vi.
However, for any smooth function g : N →R,
Dφ◦γ(g) =
n

j=1
∂g
∂yj
d(yj ◦φ ◦γ)
dt

t=0
=
n

j=1
∂g
∂yj
 m

i=1
∂(yj ◦φ)
∂xi
dγi
dt

t=0

,
where γi = xi ◦γ. Therefore, in terms of these coordinate patches, the
matrix for φ∗with respect to the standard bases {∂/∂xi} on TpM and
{∂/∂yj} on Tφ(p)N is
[dφp] =
∂φj
∂xi
	
p
,
with
1 ≤i ≤m and 1 ≤j ≤n,
(3.5)
by which we explicitly mean
∂φj
∂xi

p
def
= ∂(yj ◦φ ◦x−1)
∂xi

x(p)
.
(3.6)

3.3. Tangent Spaces and Differentials
107
Example 3.3.8 (Curves on a Manifold). We used the notion of a curve on a
manifold to deﬁne tangent vectors in the ﬁrst place.
However, we can
now restate the notion of a curve on a manifold as a diﬀerentiable map
γ : I →M, where I is an open interval of R and M is a diﬀerentiable
manifold. The tangent vector Dγ ∈Tγ(t0)M to the curve γ can also be
understood as
Dγ = γ∗
 d
dt

t0
	
.
Matching with notation from calculus courses, this tangent vector is some-
times denoted as γ′(t0). Then this tangent vector acts on diﬀerentiable
functions f : M →R by
γ′(t0)(f) = γ∗
 d
dt

t0
	
(f) = d(f ◦γ)
dt

t0.
Example 3.3.9 (Gauss Map). Consider a regular oriented surface S in R3 with
orientation n : S →S2. (Recall that the orientation is a choice of a unit
normal vector to S at each point such that n : S →S2 is a continuous
function.) In the local theory of surfaces, the function n is often called the
Gauss map. The diﬀerential of the Gauss map plays a central role in this
theory. In that context, one deﬁnes the diﬀerential of the Gauss map dnp
at a point p ∈S in the following way.
A parametrization ⃗X of a coordinate patch U around p amounts to the
inverse ⃗X = x−1 of a chart x : U →R2. Similarly, on S2, the parame-
trization ⃗N = n ◦⃗X is the inverse of a chart y on S2 of a neighborhood
of n(p).
Since ⃗N : U →R3 is a unit vector, by the comments in Sec-
tion 2.2, we know that ⃗Nu and ⃗Nv are perpendicular to ⃗N and hence are
in the tangent space TpS. Hence, one often identiﬁes TpS = Tn(p)(S2). Let
⃗X(t) = ⃗X(⃗α(t)) be any curve on the surface such that ⃗X(0) = p. Then dnp
is the transformation on TpS that sends a tangent vector ⃗X′(0) ∈Tp(S) to
d
dt( ⃗N(⃗α(t)))|t=0.
Via the association of γ′(0) →Dγ between the classical and the modern
deﬁnition of the tangent space, we see that the classical deﬁnition of the
Gauss map is precisely Deﬁnition 3.3.6. (Note that Figure 3.8 speciﬁcally
illustrates the diﬀerential of the Gauss map.)
Over some neighborhood of n(p), ⃗N : x−1(U) →S2 gives a parametr-
ization of a coordinate neighborhood of n(p) on S2. Write the coordinate
functions as x(q) = (x1(q), x2(q)) and similarly for y. Then the associated

108
3. Differentiable Manifolds
bases on TpS and Tn(p)(S2) are
 ∂
∂x1 , ∂
∂x2
 
= { ⃗Xu, ⃗Xv}
and
 ∂
∂y1 , ∂
∂y2
 
= { ⃗Nu, ⃗Nv}.
Thus, with respect to the coordinate charts x and y as described here, the
matrix for dnp is
[dnp] =

ai
j

,
where
⃗Nj = a1
j ⃗X1 + a2
j ⃗X2,
where by ⃗Xi, we mean ∂⃗X/∂xi. It is not hard to show that
a1
1
a1
2
a2
1
a2
2
	
= −
g11
g12
g21
g22
	−1 L11
L12
L21
L22
	
,
(3.7)
where gij = ⃗Xi· ⃗Xj and Lij = ⃗Xij· ⃗N. In classical diﬀerential geometry, this
matrix equation for the coeﬃcients ai
j is called the Weingarten equations.
As noted in Example 2.4.9, using tensor notation, Equation (3.7) is written
simply as
ai
j = −gikLkj.
Corollary 3.3.10 (The Chain Rule). Let M, N, and S be diﬀerentiable mani-
folds, and consider φ : M →N and ψ : N →S to be diﬀerentiable maps
between them. Then
(ψ ◦φ)∗= ψ∗◦φ∗.
More speciﬁcally, at every point p ∈M,
d(ψ ◦φ)p = dψφ(p) ◦dφp .
Proof: By Proposition 3.3.7, for all functions f from a neighborhood of
ψ(φ(p)) on S to R and for all X ∈TpM, we have
(ψ ◦φ)∗(X)(g) = X(g ◦ψ ◦φ) = (φ∗(X))(g ◦ψ)
= (ψ(φ∗(X)))(g) = ψ∗◦φ∗(X)(g).
□
Deﬁnition 3.3.6 for the diﬀerential avoids referring to any coordinate
neighborhood of points on M. In contrast to the matrix for the diﬀerential
introduced in Chapter 1, the matrix for the diﬀerential dfp of maps be-
tween manifolds depends on the coordinate charts used around p and f(p),
according to Equations (3.5) and (3.6). We can, however, say the following
about how the matrix of the diﬀerential changes under coordinate changes.

3.3. Tangent Spaces and Differentials
109
Proposition 3.3.11. Let f : M →N be a diﬀerentiable map between diﬀer-
entiable manifolds.
Let x = (x1, . . . , xm) and ¯x = (¯x1, . . . , ¯xm) be two
coordinate systems in a neighborhood of p, and let y = (y1, . . . , yn) and
¯y = (¯y1, . . . , ¯yn) be two coordinate systems in a neighborhood of f(p). Let
[dfp](x,y) be the matrix for dfp associated to the x- and y- coordinate sys-
tems and similarly for [dfp](¯x,¯y). Then [dfp](x,y) and [dfp](¯x,¯y) are similar
matrices. Furthermore,
[dfp](¯x,¯y) =
 ∂¯yi
∂yj

f(p)
	
[dfp](x,y)
∂xk
∂¯xl

p
	
.
Proof: (The proof is left as an exercise for the reader.)
□
3.3.3
Orientability and Tangent Spaces
We end this section with a brief comment on the connection between the
orientation of a manifold and the standard bases in coordinate systems.
Let M n be an oriented manifold, and let (U, x) and ( ¯U, ¯x) be two over-
lapping coordinate systems on M. Let p ∈U ∩¯U. The ordered bases of
TpM with respect to (U, x) and ( ¯U, ¯x), respectively, are
 ∂
∂x1 , . . . ,
∂
∂xn
	
and
 ∂
∂¯x1 , . . . ,
∂
∂¯xn
	
.
(As a comment on notation, we use usual parentheses ( ) to describe the
basis as an n-tuple of vectors, as opposed to a set notation { }, because the
order of how we list the basis vectors is crucial for the notion of orientabil-
ity.) The transition matrix between these two bases (from the former to the
latter) is given by the matrix (∂xi/∂¯xj). According to Deﬁnition 3.1.14,
since M is oriented, then
det
 ∂xi
∂¯xj
	
> 0
at p. Hence, given a coordinate patch (U, x) on a connected component
of an orientable manifold, calling the basis ( ∂
∂x1 , . . . ,
∂
∂xn ) as positively
oriented for all p ∈U naturally deﬁnes an orientation on that connected
component. Any other basis of TpM given as the standard tangent vectors
of another coordinate patch would also need to be positively oriented in
reference to ( ∂
∂x1 , . . . ,
∂
∂xn ).
Note that if M is a manifold with c connected components, there are
2c possible orientations on M.
This includes the degenerate case of 0-
manifolds that correspond to a set of points equipped with the discrete
topology. In this case, each point can have an orientation of +1 or −1.

110
3. Differentiable Manifolds
∂
∂x1
∂
∂x2
∂
∂x1
−
∂
∂x2
p
q
M
∂M
Figure 3.9. Half-torus with boundary.
These comments allow us to induce a natural orientation onto ∂M from
an orientation on M. The following construction is essential for Stokes’
Theorem in Section 4.5.
Let M n be an oriented diﬀerential manifold with boundary. Let p ∈
∂M, and let (U, x) be a coordinate neighborhood of p. Recall that since
p ∈∂M, the coordinate function x is a homeomorphism onto an open
subset of Rn
+ = {(x1, . . . , xn) | xn ≥0}. The tangent vector −∂/∂xn is in
TpM but not in Tp(∂M). We say that −∂n = −∂/∂xn is a tangent vector
that points outward from ∂M. (The vector ∂n = ∂/∂xn points inward.)
Deﬁnition 3.3.12. Let M n be an oriented manifold with boundary ∂M. Over
a given coordinate chart U, we say that the basis (∂1, . . . , ∂n−1) is the
induced orientation on ∂M if (−∂n, ∂1, . . . , ∂n−1) is positively oriented
and that the induced orientation is the opposite from (∂1, . . . , ∂n−1) if
(−∂n, ∂1, . . . , ∂n−1) is negatively oriented on M.
One should note from this deﬁnition that if M is an n-dimensional
manifold with boundary, then (∂1, . . . , ∂n−1) is positively oriented on ∂M
if n is even and is negatively oriented if n is odd.
Example 3.3.13. Consider the half-torus M shown in Figure 3.9. The bound-
ary ∂M has two components. The point q is a generic point in a neigh-
borhood that contains the boundary component where p is. If (x1, x2) is a
coordinate system in a neighborhood of p, the boundary component that
contains p is given by x2 = 0. The ﬁgure depicts the basis (∂1, ∂2) at the
generic point q and also the basis (−∂2, ∂1) at p. Since these two bases have
the same orientation (imagine moving the standard basis at q over to p),
then ∂1 determines the induced orientation on ∂M (as opposed to −∂1).

3.3. Tangent Spaces and Differentials
111
For the other boundary component, the reasoning is the same except
that one must use at least one other coordinate chart (¯x1, ¯x2) where the
boundary is given by ¯x2 = 0 and the portion of M that is not on ∂M has
¯x2 > 0. Intuitively speaking, in order for (¯x1, ¯x2) to have an orientation
compatible with (x1, x2), one must switch the direction of the basis vector
∂/∂¯x2 (from what one would obtain from moving ∂/∂x2 over along a line
of x1 =const.). One must then also switch the sign of ∂/∂x1 to get the
equivalent ∂/∂¯x1 in order to keep a positively oriented atlas. The induced
orientation on the second boundary component is shown with an arrow.
We set a convention for use later concerning 1-manifolds. Let γ : [a, b] →
M be a 1-manifold with two boundary points p1 = γ(a) and p2 = γ(b).
Then we equip p2 with a positive orientation +1 and p1 with a negative
orientation −1.
p1
p2
−1
+1
This association of endpoints to −1 and +1 as shown above is, by conven-
tion, the induced orientation of γ onto ∂γ.
Problems
3.3.1. Let φ : Rm →Rn be a linear transformation. Show that under the identiﬁ-
cation of Tp(Rk) with Rk as described in Example 3.3.4, φ∗gets identiﬁed
with φ.
3.3.2. Consider a diﬀerentiable manifold M m and a real-valued, diﬀerentiable
function h : M m →R.
Apply Proposition 3.3.7 to show that h∗(X)
corresponds to the diﬀerential operator
h∗(X) = X(h) d
dt
on functions g : R →R, where we assume we use the variable t on R.
3.3.3. Let T 2 be the torus given as a subset of R3 with a parametrization
⃗X(u, v) = ((2 + cos v) cos u, (2 + cos v) sin u, sin v) .
Consider the sphere S2 given as a subset of R3, and use the stereographic
atlas {πN, ¯πs} as the coordinate patches of S2. Consider the map f : T 2 →
S2 deﬁned by
x →
x
∥x∥.
Explicitly calculate the matrix of the diﬀerential dfp, with p given in terms
of (u, v)-coordinates for (u, v) ∈(0, 2π)2 and using the stereographic atlas
on the sphere.

112
3. Differentiable Manifolds
3.3.4. Let S3 be the 3-sphere given in R4 by S3 = {u ∈R4 : ∥u∥= 1}, and let S2
be the unit sphere in R3, where we use coordinates (x1, x2, x3). Consider
the Hopf map h : S3 →S2 given by
h(u1, u2, u3, u4)=

2(u1u2 + u3u4), 2(u1u4 −u2u3), (u2
1 + u2
3) −(u2
2 + u2
4)

.
(a) Show that this map indeed surjects S3 onto S2.
(b) Show that the preimage h−1(q) of any point q ∈S2 is a circle on S3.
(c) For a coordinate patch of your choice on S3 and also on S2, calculate
the diﬀerential dhp for points p on S3.
(Note that the description of h is equivalent to the one given in Prob-
lem 3.2.5.)
3.3.5. Consider the map F : RP3 →RP2 deﬁned by
f(x : y : z : w) = (x3 −y3 : xyz −2xw2 + z3 : z3 + 2yz2 −6y2z −w3).
This function is homogeneous, and the result of Problem 3.2.6 ensures that
this map is diﬀerentiable. Let p = (1 : 2 : −1 : 3) ∈RP3.
(a) After choosing standard coordinate neighborhoods of RP3 and RP2
that contain, respectively, p and f(p), calculate the matrix of dfp
with respect to these coordinate neighborhoods.
(b) Choose a diﬀerent pair of coordinate neighborhoods for p and f(p)
and repeat the above calculation.
(c) Explain how these two matrices are related.
3.3.6. Consider the spheres S2 and S3, viewed as subsets of R3 and R4, respec-
tively. Assume that we use the atlas {πN, πS} on S2 given by stereographic
projection, as described in Example 3.1.4, and assume that we also use
the atlas {ΠN, ΠS} on S3 that corresponds to the stereographic projection
(into R3).
(a) Write out ΠN and Πs and their inverses as done for S2 in Example
3.1.4.
(b) Suppose that πN deﬁnes the coordinate system (u1, u2) on the open
set U = S2 −{(0, 0, 1)} on S2, that ΠN deﬁnes the coordinate system
(v1, v2, v3) on an open set V of S3, and that we use (x1, x2, x3) as
the coordinates of R3 and (y1, y2, y3, y4) as the coordinates in R4.
Let F : R3 →S3 be the function deﬁned by
f(x1, x2, x3) =

sin(x1) sin(x2) sin(x3), sin(x1) sin(x2) cos(x3), sin(x1) cos(x2), cos(x1)

,
and deﬁne f : S2 →S3 by restricting F to S2 (as a subset of R3).
Calculate the matrix of the diﬀerential [dfp] for any p ∈U given with
respect to (u1, u2)- and (v1, v2, v3)-coordinates.

3.4. Immersions, Submersions, and Submanifolds
113
3.3.7. Example 3.1.6 shows that if we give R3 the coordinates (x0, x1, x2), there
is a natural surjection f : R3 −{(0, 0, 0)} →RP2 via π(x0, x1, x2) = (x0 :
x1, x2). Consider the unit sphere S2 (centered at the origin), and consider
the map g : S2 →RP2 given as the restriction of f to S2.
Using the
oriented atlas on the sphere given in Example 3.1.16 and the coordinate
patches for RP2 as described in Example 3.1.6, give the matrix for dgp
between the north pole patch πN and U0. Do the same between the north
pole patch and U1 and explicitly verify Proposition 3.3.11.
3.3.8. Prove Proposition 3.3.11.
3.3.9. Let M1 and M2 be two diﬀerentiable manifolds, and consider their product
manifold M1 × M2. Call πi : M1 × M2 →Mi for i = 1, 2 the projection
maps. Show that for all points p1 ∈M1 and p2 ∈M2, the linear transfor-
mation
S : T(p1,p2)(M1 × M2) −→Tp1M1 ⊕Tp2M2,
X −→(π1∗(X), π2∗(X))
is an isomorphism.
3.4
Immersions, Submersions, and Submanifolds
The linear transformation φ∗(which is implicitly local to p) and the associ-
ated matrix [dφp] allow us to discuss the relation of one manifold to another.
A number of diﬀerent situations occur frequently enough to warrant their
own terminologies.
Deﬁnition 3.4.1. Let φ : M →N be a diﬀerentiable map between diﬀeren-
tiable manifolds.
1. If φ∗is an injective at all points p ∈M, then φ is called an immersion.
2. If φ∗is a surjective at all points p ∈M, then φ is called a submersion.
3. If φ is an immersion and one-to-one, then the pair (M, φ) is called a
submanifold of N.
4. If (M, φ) is a submanifold and φ : M →φ(M) is a homeomorphism for
the topology on φ(M) induced from N, then φ is called an embedding
and φ(M) is called an embedded submanifold.
It is important to pause and give examples of the above four situations.
In fact, in the theory of diﬀerentiable manifolds, it is only in the context of
Deﬁnition 3.4.1 that one can discuss how a manifold “sits” in an ambient

114
3. Differentiable Manifolds
⃗γ(t)
⃗γ′(0)
Figure 3.10. Double cone.
Figure 3.11. Enneper’s surface.
Euclidean space by considering a diﬀerentiable function f : M →Rn, where
Rn is viewed as a manifold with its usual diﬀerential structure.
Clearly, every embedded submanifold is a submanifold and every sub-
manifold is an immersion. These three categories represent diﬀerent situa-
tions that we addressed when studying regular surfaces in R3. The cylinder
S1 ×R is a diﬀerentiable manifold. We can consider the double cone in Fig-
ure 3.10 as the image of a map f : S1 × R →R3 given by
f(u, v) = (v cos u, v sin u, v).
It is not hard to check that at all points p = (u, 0) ∈S1 ×R, the diﬀerential
dφp is not injective. Thus, the cone is not an immersion in R3.
Enneper’s surface (see Figure 3.11) is deﬁned as the locus of the para-
metrization
⃗X(u, v) =

u −u3
3 + uv2, v −v3
3 + vu2, u2, v2
	
for (u, v) ∈R2.
Enneper’s surface can be considered a diﬀerentiable map of ⃗X : R2 →
R3. It is not hard to check that according to Deﬁnition 3.4.1, Enneper’s
surface is an immersion, but because ⃗X is not one-to-one, the surface is
not a submanifold. (In Figure 3.11, the locus of self-intersection of the
parametrized surface is indicated in thick black.)
To illustrate the idea of a submanifold that is not an embedded sub-
manifold, consider the ribbon surface in Figure 3.12. One can consider this
surface to be a function between manifolds in the following sense. Consider

3.4. Immersions, Submersions, and Submanifolds
115
⃗X
Figure 3.12. Not a homeomorphism.
the two-dimensional manifold without boundary M = (0, 5) × (0, 1) with
the natural product topology and diﬀerential product structure. Then the
ribbon surface can be viewed as a diﬀerentiable map f : (0, 5)×(0, 1) →R3
between manifolds. The map f is a submanifold but not an embedded sub-
manifold because, as Figure 3.12 shows, open sets on M might not be open
sets on f(M) with the topology induced from R3.
Some authors (usually out of sympathy for their readers) introduce
the theory of “manifolds in Rn.” By this one means manifolds that are
embedded submanifolds of Rn. Though not as general as Deﬁnition 3.4.1,
that approach has some merit as it more closely mirrors Deﬁnition 3.1.1
for regular surfaces in R3. However, our current approach to discussing
relations of one manifold to another is more general. Admittedly, it might
seem strange to call a diﬀerentiable map a submanifold, but, as the above
examples show, this tactic generalizes the various situations of interest for
subsets of R3. Furthermore, this approach again removes the dependence
on an ambient Euclidean space. Consequently, it is not at all strange to
discuss submanifolds of RPn or any other space of interest.
We now wish to spend a few paragraphs discussing embedded subman-
ifolds of a diﬀerentiable manifold since they occupy an important role in
subsequent sections and allow us to quickly determine certain classes of
manifolds.
Proposition 3.4.2. Let M m be a diﬀerentiable manifold. An open subset S of
M is an embedded submanifold of dimension m.

116
3. Differentiable Manifolds
y2 −x3 = 0
Figure 3.13. Not an embedded submanifold.
Proof: Let {φi : Ui →Rm}i∈I be the atlas of M. Equip S with the atlas
{φi|S}i∈I. The inclusion map ι : S →M is a one-to-one immersion. The
topology of S is induced from M, so S, with the given atlas, is an embedded
submanifold of M.
□
Example 3.4.3. Consider the set Mn×n of n × n matrices with real coeﬃ-
cients. We can equip Mn×n with a Euclidean topology by identifying Mn×n
with Rn2. In particular, Mn×n is a diﬀerentiable manifold. Consider the
subset GLn(R) of invertible matrices in Mn×n. We claim that, with the
topology induced from Mn×n, GLn(R) is an embedded submanifold. We
can see this by the fact that an n × n matrix A is invertible if and only
if det A ̸= 0. However, the function det : Mn×n →R is continuous, and
therefore,
GLn(R) = det −1(R −{0})
is an open subset of Mn. Proposition 3.4.2 proves the claim.
The proof of Proposition 3.4.2 is deceptively simple. If S ⊂M, though
the inclusion map ι : S →M is obviously one-to-one, one cannot use it to
show that any subset is an embedded submanifold. Consider the subset S
of R2 deﬁned by the equation y2 −x3 = 0 (see Figure 3.13). The issue is
that in order to view S as a manifold, we must equip it with an atlas. In
this case, the atlas of R2 consists of one coordinate chart, the identity map.
The restriction of the identity map id|S is not a homeomorphism into an
open subset of R or R2 so cannot serve as a chart. In fact, if we put any
atlas {φi} of coordinate charts on S, the inclusion ι : S →R2 is such that
ι ◦φi will be some regular reparametrization of t →(t2, t3), i.e.,
ι ◦φi(t) = (g(t)2, g(t)3),

3.4. Immersions, Submersions, and Submanifolds
117
where g′(t) ̸= 0. Hence,
[dιt] =

2g(t)g′(t)
3g(t)2g′(t)
	
,
and thus dιt fails to be an immersion at the point where g(t) = 0, which
corresponds to (0, 0) ∈S, the cusp of the curve.
Having a clear deﬁnition of the diﬀerential of a function between mani-
folds, we can now imitate Deﬁnition 1.4.1 and given a deﬁnition for regular
points and for critical points of functions between manifolds.
Deﬁnition 3.4.4. Let φ : M m →N n be a diﬀerentiable map between dif-
ferentiable manifolds. Then any point p ∈M is called a critical point if
rank(φ∗) < min(m, n), i.e., dφp is not of maximal rank. If p is a critical
point, then the image φ(p) is called a critical value. Furthermore, any el-
ement q ∈N that is not a critical value is called a regular value (even if
q /∈φ(M)).
We remind the reader that this deﬁnition for critical point directly
generalizes all the previous deﬁnitions for critical points of functions (see
the discussion following Deﬁnition 1.4.1). The only novelty here from the
discussion in Chapter 1 was to adapt the deﬁnition for functions from Rm
to Rn to functions between manifolds.
Our main point in introducing the above deﬁnition is to introduce the
Regular Value Theorem. A direct generalization to a similar theorem for
regular surfaces (see [5, Proposition 5.2.13]), the Regular Value Theorem
provides a class of examples of manifolds for which it would otherwise take a
considerable amount of work to verify that these sets are indeed manifolds.
However, we need a few supporting theorems ﬁrst.
Theorem 3.4.5. Let f : M m →N n be a diﬀerentiable function between dif-
ferentiable manifolds, and assume that dfp is injective. Then there exist
charts φ at p and ψ at f(p) that are compatible with the respective diﬀer-
ential structures on M and N and such that ¯f = ψ ◦f ◦φ−1 corresponds
to the standard inclusion
(x1, . . . , xm) −→(x1, . . . , xm, 0, . . . , 0)
of Rm into Rn.
Proof: Let φ and ψ be charts on M and N, respectively, for neighborhoods
of p and f(p). If necessary, translate φ and ψ so that φ(p) and ψ(f(p))

118
3. Differentiable Manifolds
correspond to the origin. Then, with respect to these charts, dfp has the
same matrix as d ¯f0, so d ¯f0 is injective by assumption. By a rotation in Rn,
we can assume that the image of d ¯f0 is {(x1, . . . , xm, 0, . . . , 0) ∈Rn}.
We wish to change coordinates on Rn via some diﬀeomorphism h : Rn →
Rn that would make ¯f the standard inclusion. We view Rn as Rm × Rn−m
and deﬁne the function h : Rn →Rn by
h(x, y) = ( ¯f(x), y).
(3.8)
Note that the diﬀerential of h at 0 is dh0 = d ¯f0 ⊕id or, as matrices,
[dh0] =

[d ¯f0]
0
0
In−m
	
.
Since d ¯f0 is injective, we see that dh0 is invertible.
Now, by the Inverse Function Theorem (Theorem 1.4.5), we know that
h−1 exists and is diﬀerentiable. Thus, h is a diﬀeomorphism between open
neighborhoods of the origin in Rn. We reparametrize the neighborhood of
f(p) with the chart h−1 ◦ψ. By Equation (3.8), replacing ψ with ψ′ =
h−1 ◦ψ leads to an atlas that is compatible with the given atlas on N.
Furthermore, by construction, the new ¯f satisﬁes
¯f ′(x) = ψ′ ◦f ◦φ−1(x) = h−1 ◦¯f(x) = (x, 0)
as desired.
□
The functional relationship discussed in the above proof is often de-
picted using the following diagram:
M m
f

φ

N n
ψ

Rn
¯
f
 Rn
We say that the diagram is commutative if, when one takes diﬀerent di-
rected paths from one node to another, the diﬀerent compositions of the
corresponding functions are equal.
In this simple case, to say that the
above diagram is commutative means that
ψ ◦f = ¯f ◦φ .
These kinds of diagrams are often used in algebra and in geometry as a
schematic to represent the kind of relationship illustrated by Figure 3.7.

3.4. Immersions, Submersions, and Submanifolds
119
Corollary 3.4.6. Let M be a diﬀerentiable manifold of dimension m, and let
S ⊂M. The subset S is an embedded submanifold of M of dimension k
if and only if, for all p ∈S, there is a coordinate neighborhood (U, φ) of p
compatible with the atlas on M such that
U ∩S = {(x1, . . . , xk, xk+1, . . . , xm) | xk+1 = · · · = xm = 0}.
Proof: The implication (→) follows immediately from Theorem 3.4.5.
For the converse (←), assume that for all p ∈S, there is a coordinate
neighborhood (U, φ) in M compatible with the atlas of M satisfying the
condition for S ∩U. We cover S with a collection of such open sets {Uα ∩
S}α∈I. Let π : Rn →Rk be the projection that ignores the last n −k
variables. Then on each coordinate neighborhood, π ◦φα : Uα ∩S →Rk is
a coordinate chart for S. It is easy to see that
{(Uα ∩S, π ◦φα)}α∈I
forms an atlas on S that gives S the structure of a diﬀerentiable manifold.
Furthermore, the inclusion map satisﬁes all the axioms of an embedded
submanifold.
□
The following theorem is similar to Theorem 3.4.5 but applies to local
submersions.
Theorem 3.4.7. Let f : M m →N n be a diﬀerentiable map such that f∗:
TpM →Tf(p)N is onto.
Then there are charts φ at p and ψ at f(p)
compatible with the diﬀerentiable structures on M and N, respectively, such
that
ψ ◦f = π ◦φ,
where π is the standard projection of Rm onto Rn by ignoring the last m−n
variables.
Proof: (The proof mimics the proof of Theorem 3.4.5 and is left to the
reader.)
□
Theorem 3.4.8 (Regular Value Theorem). Suppose that m ≥n, let f : M m →
N n be a diﬀerentiable map, and let q be a regular value of f. Then f −1(q)
is an embedded submanifold of M of dimension m −n.
Proof: Since m ≥n and q is a regular value, then for all p ∈f −1(q), dfp
has rank n.

120
3. Differentiable Manifolds
We ﬁrst prove that the set of points p ∈M, where rank dfp = n is an
open subset of M. Let {(Uα, φα)} be an atlas for M. For all α, deﬁne
gα : Uα →R, with gα(p) as the product of determinants of all minors
of dfp. Note that there are
m
n

minors in dfp and that, for all α, each
function gα is well deﬁned on the coordinate patch Uα. The functions gα
need not induce a well-deﬁned function g : M →R. (One would need
gα|Uα∩Uβ = gβ|Uα∩Uβ for all pairs (α, β).) However, gα(p) = 0 if and only
if rank dfp < n and the rank is independent of any coordinate system, so
g−1
α (0) ∩Uβ = g−1
β (0) ∩Uα. Deﬁne Vα = g−1(R −{0}). Since each gα is
continuous, Vα is open and the set
V =
.
α∈I
Vα
is an open subset of M. By construction, V is precisely the set of points
in which rank dfp = n. Since V is open in M, by Proposition 3.4.2, V is an
embedded submanifold of dimension m.
We consider now the diﬀerentiable map f|V
: V
→N.
Let p ∈
f −1(q), and let U be a coordinate neighborhood of p in V with coordinates
(x1, . . . , xm). By Theorem 3.4.7, we can assume that U and a coordinate
neighborhood of q in N are such that
f −1(q) ∩U = {(x1, . . . , xn, xn+1, . . . , xm) | xn+1 = · · · = xm = 0}.
By Corollary 3.4.6, f −1(q) is therefore an embedded submanifold of M. □
The Regular Value Theorem is also called the Regular Level Set Theo-
rem because any subset of the form f −1(q), where q ∈N, is called a level
set of f.
Example 3.4.9 (Spheres). With the Regular Value Theorem at our disposal,
it is now easy to show that certain objects are diﬀerentiable manifolds. We
consider the sphere Sn as the subset of Rn+1, with
(x1)2 + (x2)2 + · · · + (xn+1)2 = 1.
The Euclidean spaces Rn+1 and R are diﬀerentiable manifolds with trivial
coordinate charts.
Consider the diﬀerentiable map f : Rn+1 →R de-
ﬁned by
f(x) = ∥x∥2.

3.4. Immersions, Submersions, and Submanifolds
121
In the standard coordinates, the diﬀerential of f is
[dfx] =
⎛
⎜
⎜
⎜
⎝
2x1
2x2
...
2xn+1
⎞
⎟
⎟
⎟
⎠.
We note that the only critical point of f is (0, . . . , 0) and that the only
critical value is 0.
Thus, Sn = f −1(1) is an embedded submanifold of
Rn+1, and hence, Sn is a diﬀerentiable manifold in its own right when
equipped with the subspace topology of Rn+1.
Problems
3.4.1. Let M be a diﬀerentiable manifold, and suppose that f : M →R is a
diﬀerentiable map.
Prove that if f∗= 0 at all points of M, then f is
constant on each connected component of M.
3.4.2. Let N be an embedded submanifold of a diﬀerentiable manifold M. Prove
that at all points p ∈N, TpN is a subspace of TpM.
3.4.3. Let M m be a diﬀerentiable manifold that is embedded in Rn. (By Ex-
ercise 3.4.2, TpM is a subspace of Tp(Rn) ∼= Rn.) Let f : Rn →R be a
diﬀerentiable function deﬁned in a neighborhood of p ∈M. Show that if f
is constant on M, then f∗(v) = 0 for all v ∈TpM. Conclude that, viewed
as a vector in Tp(Rn), the diﬀerential dfp is perpendicular to TpM.
3.4.4. Let M be a diﬀerentiable manifold, and let U be an open set in M. Deﬁne
ι : U →M as the inclusion map. Prove that for any p ∈U, the diﬀerential
ι∗: TpU →TpM is an isomorphism.
3.4.5. Let M and N be k-manifolds in Rn, in the sense that they are both
embedded submanifolds. Show that the set M ∪N is not necessarily an
embedded submanifold of Rn. Give suﬃcient conditions for M ∪N to be
a manifold.
3.4.6. Suppose that the deﬁning rectangle of the Klein bottle, as illustrated in
Figure 3.6, is [0, 2π] × [0, 2π]. It is a well-known fact that it is impossible
to embed the Klein bottle in R3. Show that the parametrization
X(u, v) = ((2 + cos v) cos u, (2 + cos v) sin u, sin v cos(u/2), sin v sin(v/2))
gives an embedding of the Klein bottle in R4. (Remark: This parame-
trization is similar to the standard parametrization of the torus in R3 as
the union of circles traced in the normal planes of a planar circle of larger
radius. A planar circle in R4 admits a normal three-space. The parame-
trization X is the locus of a circle in the normal three-space that rotates
in the fourth coordinate dimension by half a twist as one travels around
the circle of larger radius.)

122
3. Differentiable Manifolds
3.4.7. Deﬁne O(n) as the set of all orthogonal n × n matrices.
(a) Prove that O(n) is a smooth manifold of dimension 1
2n(n −1).
(b) Consider the tangent space to O(n) at the identity matrix, TI(O(n)),
as a subspace of the tangent space to Mn×n (which is Mn×n itself).
Prove that A ∈Mn×n is a tangent vector in TI(O(n)) if and only if
A is skew-symmetric, i.e., AT = −A.
3.4.8. Prove Theorem 3.4.7.
3.4.9. Let M m and N n be embedded submanifolds of a diﬀerentiable manifold
Ss, and suppose that m + n > s. Let p ∈M ∩N. We say that M and N
intersect transversally at p in S if
dim(TpM ∩TpN) = m + n −s,
where the tangent spaces are understood as subspaces of TpS by virtue
of Problem 3.4.2. Show that if M and N intersect transversally at each
point of M ∩N, then M ∩N is a diﬀerentiable manifold.
3.5
Chapter Summary
This section introduces nothing new but summarizes the essential points
underlying the concepts of a diﬀerentiable manifold.
Intuitively speaking, a diﬀerentiable manifold M of dimension n is a
topological space that locally resembles Rn. This means that around point
p ∈M, there is an open set U in which we can talk about coordinates
(x1, . . . , xn) for points in U. A manifold M must be covered by such coor-
dinate patches, and it is possible a given point p ∈M lies in more than one
coordinate patch. Whether a manifold is considered topological, diﬀeren-
tiable, Ck, smooth, or analytic depends on whether the coordinate transi-
tion functions are, respectively, continuous, diﬀerentiable, Ck, smooth, or
analytic.
In order to do calculus on functions f : M →N between two dif-
ferentiable manifolds, one always refers to local coordinate charts, say
x : M →Rm on M and y : N →Rn on N, so that one may apply
the usual calculus of functions from Rm →Rn to y ◦f ◦x−1. A function
f : M →N is called diﬀerentiable if it is “locally” diﬀerentiable in the
sense that y ◦f ◦x−1 is diﬀerentiable for all choices of coordinate charts x
and y.
Since a manifold is deﬁned without reference to any ambient space,
one must take some care to even deﬁne the diﬀerential of a diﬀerentiable
function f : M →N. The diﬀerential is an operation on “directions” near

3.5. Chapter Summary
123
a given point, but without reference to an ambient space, the notion of a
direction of travel is diﬃcult as well. For analysis of functions from Rm to
R, a vector and a directional derivative have well-deﬁned meanings. For
functions from a manifold M to R, only the notion of a directional derivative
at a point makes sense, and that only in reference to curves passing through
that point. The solution to the problem of deﬁning a direction vector was
to equate it with a corresponding directional derivative. Though seemingly
abstract compared to usual analysis, this deﬁnition of a tangent vector
works in that it generalizes the idea of a tangent space to a regular surface
in R3 and has the linear algebraic properties one expects for direction
vectors.
With the notion of a tangent vector established and with the tangent
space of a manifold M at a point p, the diﬀerential at p of a diﬀerentiable
function f : M →N is a linear transformation dfp : TpM →Tf(p)N that
describes how the function f aﬀects directional derivatives. Deﬁnition 3.3.6
presents the diﬀerential independently from any reference to coordinate
systems. However, if systems of coordinates x = (x1, . . . , xm) and y =
(y1, . . . , yn) are given for neighborhoods of p and f(p), respectively, then
in the standard coordinates for TpM and Tf(p)N, the matrix of dfp is the
n × m matrix whose (i, j)th entry is the partial derivative
∂f i
∂xj ,
where this notation is explicitly given by Equation (3.6).
This matrix
depends on local coordinates of p and f(p), but it changes according to
Proposition 3.3.11 when one changes coordinate systems near p and f(p).
In subsequent chapters, we will further develop the analysis on mani-
folds and discuss applications to physics.


CHAPTER
4
Analysis on Manifolds
In Chapter 3, we introduced the concept of a diﬀerentiable manifold as
motivated by a search for topological spaces over which it is possible to
do calculus or do physics. The idea of having a topological space locally
homeomorphic to Rn drove the deﬁnition of a diﬀerentiable manifold. Sub-
sequent sections in that chapter discussed diﬀerentiable maps between man-
ifolds and the corresponding diﬀerential. We used these to introduce the
important notions of immersions, submersions, and submanifolds as quali-
ﬁers of one way manifolds can relate to one another.
The astute reader would point out that we have not so far made good
on our promise to do physics on a manifold, no matter how amorphous that
expression may be. As an illustrative example, consider Newton’s second
law of motion applied to, say, simple gravity, as follows:
m⃗x′′(t) = m⃗g,
(4.1)
where m is constant and ⃗g is a constant vector. In introductory physics,
⃗x(t) is a curve in R3 and ⃗x′′(t) is its acceleration vector, also in R3. In order
for Equation (4.1) to have meaning, it is essential that the quantities on
both sides of the equation exist in the same Euclidean space. Applying this
type of equation to the context of manifolds poses a variety of diﬃculties.
Firstly, note that a curve in a manifold M is a submanifold γ : I →M,
where I is an open interval of R, whereas the velocity vector of a curve
at a point p is an element of the tangent plane to M at p. Secondly, the
discussion of diﬀerentials in Chapter 3 does not readily extend to a concept
of second derivatives for a curve in a manifold. It is not even obvious in
what space a second derivative would exist. Consequently, it is not at all
obvious how to transcribe equations of curves in R3 that involve ⃗x, ⃗x′, and
⃗x′′ to the context of manifolds.
As a second point of concern, one also encounters numerous diﬃculties
when one tries to express in the context of diﬀerentiable manifolds the
classical local theory of surfaces in R3 (as presented in [5, Chapter 5]). It
is not diﬃcult to deﬁne the ﬁrst fundamental form as a bilinear form on
125

126
4. Analysis on Manifolds
TpM. However, since we do not view a given manifold M as a subset of
any Euclidean (vector) space, the concept of normal vectors does not exist.
Therefore, there is no equivalent of the second fundamental form, and all
concepts of curvature become problematic to deﬁne (see [5, Chapter 6]).
This chapter does not yet discuss how to do physics on a manifold,
but it does begin to show how to do calculus. We study in greater detail
the relationship between the tangent space to a manifold M at p. Also,
in order to overcome the conceptual hurdles mentioned in the previous
paragraphs, we introduce the formalism of vector bundles on a manifold,
discuss vector (and tensor) ﬁelds on the manifold, develop the calculus of
diﬀerential forms, and end by considering integration on manifolds.
Geometers and physicists both use tensors but usually with very diﬀer-
ent formalism. The ways of describing tensors are so diﬀerent that many
mathematicians and physicists do not immediately recognize them as rep-
resenting the same kind of object. As an important aside in this chapter,
we explain in Section 4.2 how the tangent bundle formalism used by math-
ematicians is the same as the component description of tensors usually
preferred by physicists. However, we must begin by introducing the vector
bundle formalism.
4.1
Vector Bundles on Manifolds
A vector bundle over a manifold is a particular case of what is called a
ﬁber bundle over a topological space. However, as we do not need the full
generality of ﬁber bundles in this book, we do not develop the notion of
ﬁber bundles. Instead, we refer the interested reader to [50] or [11].
In Chapter 3, we discussed tangent spaces to manifolds. In particular,
to each point p ∈M, we associated a tangent space. The elements of the
tangent space are diﬀerential operators of diﬀerentiable functions f : M →
R, but despite their abstraction, the diﬀerential operators properly model
the function of tangent vectors. Since M is not a subset of some Euclidean
space, the tangent spaces TpM are not subspaces of any ambient space
either. A manifold equipped with tangent spaces at each point motivates
the idea of “attaching” a vector space to each point p of a manifold M.
Furthermore, from an intuitive perspective, we would like to attach these
vector spaces, in some sense, continuously. We make this formal in the
following deﬁnition.
Deﬁnition 4.1.1.
Let M n be a diﬀerentiable manifold with atlas A =
{(Uα, φα)}α∈I, and let V be a ﬁnite-dimensional, real, vector space. A

4.1. Vector Bundles on Manifolds
127
vector bundle over M of ﬁber V is a Hausdorﬀtopological space E with a
continuous surjection π : E →M (called a bundle projection) and a collec-
tion Ψ of homeomorphisms (called trivializations) ψα : Uα ×V →π−1(Uα)
such that if Uα ∩Uβ ̸= ∅, then
ψ−1
β
◦ψα : (Uα ∩Uβ) × V →(Uα ∩Uβ) × V
is of the form
ψ−1
β
◦ψα(p, v) = (p, θβα(p)v),
where θβα(p) : Uα ∩Uβ →GL(V ) is a continuous map into the general
linear group (i.e., the set of invertible transformations from V to V ). The
vector bundle is called diﬀerentiable (respectively, Ck or smooth) if M
is diﬀerentiable (respectively, Ck or smooth) and if all the maps involved
are diﬀerentiable (respectively, Ck or smooth) as maps between manifolds.
A vector bundle is often denoted by a single Greek letter ξ or η. The
topological space E is called the total space and denoted E(ξ) while the
manifold M is called the base space and denoted B(ξ). All of the relevant
data for a vector bundle is implied.
A few examples are in order.
Example 4.1.2 (The Trivial Bundle). Let M n be a manifold with atlas A =
{φα}, and let V be a real vector space.
The topological space M × V
is a vector bundle over M. The trivialization maps ψα are all the identity
maps on Uα × V and the maps θβα are the identity linear transformation.
Example 4.1.3. Consider the circle S1 to be a manifold with the following
atlas:
φ1 : S1 −{(1, 0)} →(0, 2π),
with φ1(cos θ, sin θ) = θ,
φ2 : S1 −{(−1, 0)} →(0, 2π),
with φ2(cos(θ + π), sin(θ + π)) = θ.
Note that the transition map between these two charts is
φ2 ◦φ−1
1
: (0, π) ∪(π, 2π) →(0, π) ∪(π, 2π)
θ
→(θ + π)
mod 2π,
where a mod 2π is the unique real number r ∈[0, 2π) such that a = 2πk+r
for some k ∈Z. Now consider the vector bundle ξ of ﬁber R over S1 deﬁned
by the map θ21 : (0, π) ∪(π, 2π) →GL(R) = R, where
θ21(x) =

1,
if 0 ≤x ≤π,
−1,
if π ≤x ≤2π.

128
4. Analysis on Manifolds
This vector bundle can be realized as a parametrized surface in R4 via
⃗X(u, t) =

cos u, sin u, t cos
u
2

, t sin
u
2

, with (u, t) ∈[0, 2π] × R.
This is evidently not the cylinder S1 × R. Furthermore, one can get an
intuition for this set as a M¨obius band of inﬁnite width.
The intuitive stance behind this deﬁnition is that a vector bundle is not
just a manifold with a vector space V associated to each point but that
the vector spaces “vary continuously” or that the vector bundle is locally
homeomorphic to M × V .
Consider now a diﬀerentiable manifold, and consider also the disjoint
union of all the tangent planes to M at points p ∈M, i.e.,
/
p∈M
TpM = {(p, X) | p ∈M and X ∈TpM}.
The identity map i : M →M is certainly diﬀerentiable, and we calculate
its diﬀerential at a point p ∈Uα ∩Uβ in overlapping charts. Label the
coordinate charts x = φα and ¯x = φβ. According to Equation (3.5), the
matrix of the diﬀerential of the identity map is
[dip] =

∂¯xj
∂xi

p

,
and the reader should recall that the explicit meaning of this partial deriva-
tive is given in Equation (3.6). Given any pair of overlapping coordinate
charts, this diﬀerential is invertible so it is an element in GLn(R) and cor-
responds to the maps θβα.
One can arrive at this same result in another way. Consider the coor-
dinate systems deﬁned by ¯x and x over Uα ∩Uβ. The chain rule gives, as
operators,
∂
∂¯xj

p
=
n

i=1
∂xi
∂¯xj

p
∂
∂xi

p
.
(4.2)
(The subscript |p becomes tedious and so in the remaining paragraphs,
we understand the diﬀerential operators and the matrices as depending on
p ∈M.) Recall that (∂xi/∂¯xj) and (∂¯xj/∂xi) are invertible matrices to
each other, so, in particular,
n

i=1
∂xi
∂¯xj
∂¯xk
∂xi = δk
j ,
(4.3)

4.1. Vector Bundles on Manifolds
129
where δk
j is the Kronecker delta. Note that Equation (4.3) follows from
Equation (4.2) by applying
∂
∂¯xj to ¯xk.
Let X ∈TpM be a vector in the tangent space.
Suppose that the
vector X has coordinates aj in the basis ( ∂
∂xj ) and coordinates ¯aj in the
basis ( ∂
∂¯xj ). Then
X =
n

j=1
¯aj ∂
∂¯xj .
Then we have
X =
n

j=1
¯aj
n

i=1
∂xi
∂¯xj
∂
∂xi =
n

i=1
⎛
⎝
n

j=1
¯aj ∂xi
∂¯xj
⎞
⎠∂
∂xi ,
and so
ai =
n

j=1
¯aj ∂xi
∂¯xj .
Multiplying by ∂¯xk/∂xi and summing over i, we obtain
n

i=1
∂¯xk
∂xi ai =
n

i=1
n

j=1
¯aj ∂xi
∂¯xj
∂¯xk
∂xi
=
n

j=1
¯aj
 n

i=1
∂xi
∂¯xj
∂¯xk
∂xi

=
n

i=1
∂¯xk
∂xi ai =
n

j=1
¯ajδk
j = ¯ak.
So this leads to the change-of-coordinates formula
¯ak =
n

i=1
∂¯xk
∂xi ai.
(4.4)
The above calculations are important in their own right, but in terms
of vector bundles, they allow one to conclude the following proposition.
Proposition 4.1.4. Let M n be a diﬀerentiable manifold. The disjoint union
of all the tangent planes to M
/
p∈M
TpM,
is a vector bundle with ﬁber Rn over M. It is called the tangent bundle to
M and is denoted by T M.

130
4. Analysis on Manifolds
Figure 4.1. Intuitive picture for a tangent bundle.
Proof: An element of T M is of the form (p, Xp), where p ∈M and Xp ∈
TpM. We point out ﬁrst that the bundle projection π : T M →M is simply
the function π(p, Xp) = p.
We have already seen that for each p ∈M, the matrix ∂¯xk/∂xi|p is
invertible. It remains to be veriﬁed that this matrix varies continuously in
p ∈M over Uα ∩Uβ, where x is the coordinate system over Uα and ¯x is
the coordinate system over Uβ. However, ∂¯xk/∂xi|p is the matrix of the
diﬀerential of ¯x ◦x−1 and the fact that this is continuous is part of the
deﬁnition of a diﬀerentiable manifold (see Deﬁnition 3.1.3).
□
There is an inherent diﬃculty in visualizing the tangent bundle, and
more generally any vector bundle, to a manifold. Consider the tangent
bundle to a circle. The circle S1 is a one-dimensional manifold that one
typically visualizes as the unit circle as a subset of R2. If one views the
tangent spaces to the circle as subspaces of R2 or even as the geometric
tangent lines to S1 at p, then one would view the union 0
p TpM as a subset
of R2. This is not what is meant by the deﬁnition of T M. The spaces
TpM and TqM do not intersect if p ̸= q. At best, if M is an embedded
submanifold of Rn, then TpM may be viewed as a subspace of a diﬀerent
Rn. Thus, for example, for the circle S1, the tangent bundle T (S1) can be
realized as an embedded submanifold of R4. In fact, one can parametrize
T (S1) by
Y (u, t) = (cos u, sin u, −t sinu, t cos u)
for
(u, t) ∈[0, 2π] × R.
Thus, even in this simple example, visualizing the tangent bundle requires
more than three dimensions. Nonetheless, it is not uncommon to illustrate
the tangent bundle over a manifold by a picture akin to Figure 4.1.

4.1. Vector Bundles on Manifolds
131
Proposition 4.1.5. If M m is a diﬀerentiable manifold of dimension m, and
V is a real vector space of dimension n, then a diﬀerentiable vector bundle
of ﬁber V over M is a diﬀerentiable manifold of dimension m + n.
Proof: Let E be a vector bundle of a ﬁber V over a diﬀerentiable manifold
M with the data described in Deﬁnition 4.1.1. Since V is isomorphic to Rn,
without loss of generality, let’s take V = Rn. On each open set π−1(Uα) in
the ﬁber bundle E, consider the function τα deﬁned by the composition
τα : π−1(Uα)
ψ−1
α
−−−−→Uα × Rn
φα×id
−−−−→Rm × Rn = Rm+n,
where by φα × id we mean the function (φα × id)(p, v) = (φα(p), v). We
prove that the collection of functions {(π−1(Uα), τα)} is an atlas that equips
E with the structure of a diﬀerentiable manifold.
Since π is continuous, π−1(Uα) is open and, by construction, the col-
lection of open sets π−1(Uα) cover E. The function φα : Uα →Vα is a
homeomorphism, where Vα is an open subset of Rm. Therefore, it is easy
to check that for each α ∈I,
φα × id : Uα × Rn →Vα × Rn
is a homeomorphism. Thus, since ψα is a homeomorphism by deﬁnition,
the composition τα is also a homeomorphism.
Let (y, v) ∈φβ(Uα ∩Uβ)×Rn, and let (p, v) = (φβ ×id)−1(y, v) so that
(p, v) is in the domain of the trivialization for ψβ. Then we calculate that
(τα ◦τ−1
β )(y, v) =

(φα × id) ◦ψ−1
α
◦ψβ ◦(φ−1
β
× id)

(y, v)
=

φα ◦φ−1
β (y), θαβ(p)v

because ψ−1
α
◦ψβ(p, v) = (p, θαβ(p)v) by deﬁnition of a vector bundle.
At this stage, we must use the fact that θαβ is a diﬀerentiable map be-
tween the diﬀerentiable manifolds M and GL(Rn). Since GL(Rn) inherits
its manifold structure as an embedded submanifold of Rn2, the following
quantities exist as n × n matrices:
∂(θαβ ◦φ−1
β )
∂yi
for 1 ≤i ≤m.
To simplify notations, we set F = θαβ ◦φ−1
β . Then a simple calculation for
the function τα ◦τ −1
β
as a function from Rm+n into itself gives the following

132
4. Analysis on Manifolds
diﬀerential as a block matrix:
[d(τα ◦τ−1
β )(y,v)] =
⎛
⎜
⎜
⎝
[d(φα ◦φ−1
β )y]
0
∂F
∂y1 v · · ·
∂F
∂ym v
F(y)
⎞
⎟
⎟
⎠.
Furthermore, each of the entries in the above matrix is continuous. This
shows that all the transition functions τα ◦τ −1
β
are of class C1, establishing
that the diﬀerentiable vector bundle is indeed a diﬀerentiable manifold. □
It is not hard to see that by adapting the above proof, one can also
show that a Ck (respectively, smooth) vector bundle is a Ck (respectively,
smooth) manifold. However, we point out the following consequence for
tangent bundles to a manifold.
Corollary 4.1.6. If M is a manifold of class Ck and dimension m, then T M
is a manifold of class Ck−1 and dimension 2m.
Furthermore, if M is
a smooth manifold of dimension m, then T M is a smooth manifold of
dimension 2m.
Proof: This follows from the proof of the above proposition and the fact
that the linear transformation θαβ is (∂¯xj/∂xi), where (¯xj) are the coordi-
nates with respect to φβ and (xi) are the coordinates with respect to φα.
Therefore, in order for the functions θαβ to be of class Cl, the transition
functions φβ ◦φ−1
α
must be of class Cl+1.
The second claim of the corollary follows immediately.
□
Example 4.1.7 (Tangent Bundle of Rn.). As we saw in Example 3.3.4, the tan-
gent plane to any point p in Rn is again Rn. However, we can now make
the stronger claim that the tangent bundle of Rn is T (Rn) = Rn × Rn. We
can see this from the fact that Rn is a manifold that can be equipped with
an atlas of just one coordinate chart. Then, from Deﬁnition 4.1.1, there is
only one trivialization map. Thus, the tangent bundle is a trivial bundle.
We remind the reader of a few operations on vector spaces and en-
courage the reader who is unfamiliar with multilinear algebra to consult
Appendix C. Given a vector space V of dimension m, the dual V ∗, the
symmetric product Symk V , and the alternating product 1k V (see Sec-
tion C.5) are all other vector spaces associated to V . Also, if we are given
a vector space W of dimension n, the direct product V ⊕W and the ten-
sor product V ⊗W are new vector spaces. In each case, if V and W are
equipped with bases, there exist natural bases on the new vector spaces.

4.1. Vector Bundles on Manifolds
133
These constructions on vector spaces carry over to vector bundles over
a diﬀerentiable manifold M in the following way. Let ξ be a vector bundle
over M with ﬁber V , and let η be a vector bundle over M with ﬁber W. It
is possible to construct the following vector bundles over M in such a way
that their bundle data are compatible with the data for ξ and η and the
properties of the associated ﬁber:
• The dual bundle ξ∗. The ﬁber is the vector space V ∗.
• The direct sum ξ ⊕η. The ﬁber is the vector space V ⊕W. The
direct sum is also called the Whitney sum of two vector bundles.
• The tensor product ξ ⊗η. The ﬁber is the vector space V ⊗W.
• The symmetric product Symk ξ for some positive integer k. The ﬁber
is the vector space Symk V .
• The alternating product 1k ξ for some positive integer k. The ﬁber
is the vector space 1k V .
Each of the above situations requires careful construction and proof
that they are in fact vector bundles over M. We omit the details here but
refer the reader to Chapter 3 in [38] for a careful discussion of how to get
new vector bundles from old ones.
One of the ﬁrst useful bundles constructed from the tangent bundle is
the cotangent bundle, T M ∗, the dual to the tangent bundle. We describe
a natural basis on the ﬁbers of T M ∗. Recall that if p ∈M m, U is an
open neighborhood of p in M, and x : U →Rm is a chart for U, then the
operators
∂1, . . . , ∂m
def
=
∂
∂x1

p
, . . . ,
∂
∂xm

p
form a basis of TpM. We call this the basis associated to the chart x. The
basis of the dual bundle TpM ∗associated to x consists of the covectors of
these basis vectors for TpM, written
dx1, dx2, . . . , dxm,
(4.5)
deﬁned as the linear function on TpM →R such that
dxi(∂j) = dxi

∂
∂xj

p

= δi
j =

1
if i = j,
0
if i ̸= j.
(4.6)
The dependence on the point p ∈M is understood by context.

134
4. Analysis on Manifolds
Example 4.1.8. Consider a regular surface M in R3. M is a two-dimensional
diﬀerentiable manifold given as an embedded submanifold of R3. Consider
the bundle T M ∗⊗T M ∗over M. Via a comment after Proposition C.4.8,
we identify T M ∗⊗T M ∗as the vector bundle over M such that each ﬁber at
a point p ∈M corresponds to the vector space of all bilinear forms on TpM.
The formalism of vector bundles over manifolds may initially appear
unnecessarily pedantic. However, since in general a manifold need not be
given as a subset of an ambient Euclidean space, it is only in the context
of the tangent bundle on a manifold that one can make sense of tangent
vectors to M at various points p ∈M. We discussed how to obtain new
bundles from old ones so that it would be possible to discuss other linear
algebraic objects associated to the tangent bundle, such as bilinear forms
on T M, as in Example 4.1.8.
The value for physics is that if one must study the motion of a particle
or a system of particles that is not in Rn, then the ambient space for this
system would be a manifold.
Without the structure of a diﬀerentiable
manifold, one cannot talk about diﬀerentiability at all.
However, on a
diﬀerentiable manifold, any kind of diﬀerentiation will be given in reference
to the tangent bundle. It is not hard to imagine the need to do physics on a
sphere, say if one were studying global earth phenomenon but only looking
at the surface of the earth. In some natural problems, the variable space
(the space in which the variables of interest exist) is not a Euclidean space,
and in this context, the equations of dynamics must take into account
the fact that the ambient space is a manifold. Perhaps the most blatant
examples of the need for manifolds come from cosmology, in which it is now
well understood that our universe is not ﬂat. Therefore, doing cosmological
calculations (calculations on large portions of the universe) requires the
manifold formalism.
In popular literature, one reads phrases like “the laws of physics break
down at a black hole.” The geometric meaning of such a phrase is that
a black hole is a point where the manifold that is the universe is not dif-
ferentiable. The manifold is singular or has a singularity at that point.
In particular, the tangent bundle does not exist at a black hole, and any
equation that involves diﬀerentials has no meaning.
Problems
4.1.1. Recall the inﬁnite M¨obius strip introduced in Example 4.1.3. Explicitly
describe the bundle projection p and the trivializations ψ1 and ψ2 for the
image of the parametrization ⃗X(u, t).

4.2. Vector Fields on Manifolds
135
4.1.2. Consider the unit sphere S2 equipped with the oriented stereographic atlas
{πN, ¯πS} described in Examples 3.1.16 and 3.1.4. Explicitly describe an
atlas for the tangent bundle T (S2) as a manifold and write down the
transition functions for this atlas.
4.1.3. Normal Bundle. Consider a regular surface S in R3. At each point p ∈S,
let N(p) be the set of all normal vectors. Explicitly show that the points
in S, along with its normal vectors at corresponding points, form a vector
bundle. Determine the functions θβα between diﬀerent trivialization maps.
(This vector bundle is called the normal bundle.)
4.1.4. Normal Bundle.
Let M m be a diﬀerentiable manifold embedded in Rn
where m < n. Repeat the previous exercise given this situation. Prove
that the dimension of each ﬁber is n −m.
4.1.5. In the study of dynamics of a particle, one locates the position of a point
in R3 using its three coordinates.
Therefore, the variable space is R3.
Explain why the variable space for a general solid object (or system of
particles rigidly attached to each other) is R3 ×S2 ×S1. In particular, why
one requires six variables to completely describe the position of a solid
object in R3.
4.1.6. Provide appropriate details behind the construction of the Whitney sum
of two vector bundles.
4.1.7. Consider the real projective space M = RPn. We view RPn as the set
of one-dimensional subspaces of Rn+1. Consider the set {(V, ⃗u) ∈RPn ×
Rn+1 | ⃗u ∈V }.
(a) Show that this is a vector bundle where each ﬁber has dimension of
one.
(b) Show that this vector bundle is not the trivial bundle.
(This bundle is called the canonical line bundle on RPn.)
4.1.8. Consider the following parametrization for a torus S1×S1 as a subset of R3
⃗X(u, v) = ((2 + cos u) cos v, (2 + cos u) sin v, sin u) ,
for (u, v) ∈[0, 2π]×[0, 2π]. Using ⃗X, given the associated parametrization
of the manifold T(S1 × S1) as a subset of R6.
4.2
Vector Fields on Manifolds
Deﬁnition 4.2.1. Let ξ be a vector bundle over a manifold M with ﬁber V ,
with projection π : E(ξ) →M. A global section of ξ is a continuous map
s : M →E(ξ) such that π ◦s is the identity function on M. The set of all
global sections is denoted by Γ(ξ). Given an open set U ⊆M, we call a
local section over U a continuous map s : U →E(ξ) such that π ◦s is the
identity on U. The set of all local sections on U is denoted by Γ(U; ξ).

136
4. Analysis on Manifolds
Note that sections of a vector bundle (whether local or global) can
be added or multiplied by a scalar.
If s1, s2 ∈Γ(U; ξ), then for each
p ∈U ⊆M, s1(p) and s2(p) are vectors in the same ﬁber π−1(p), so
as1(p) + bs2(p) is well deﬁned as an element in π−1(p), where a and b are
scalars.
Deﬁnition 4.2.2. Let M be a diﬀerentiable manifold. A global section of
T M is called a vector ﬁeld on M. In other words, a vector ﬁeld associates
to each p ∈M a vector X(p) (also denoted by Xp) in TpM. The set of all
vector ﬁelds on M is denoted by X(M). A vector ﬁeld X is said to be of
class Ck if X : M →T M is a map of class Ck between manifolds.
Example 4.2.3. Let M be a regular surface in R3. In the local theory of
regular surfaces in R3, the ﬁrst fundamental form (alternatively called the
metric tensor) is the bilinear product Ip(·, ·) on TpM obtained as the re-
striction of the dot product in R3. Therefore, with the formalism of vector
bundles and using Example 4.1.8, the ﬁrst fundamental form is a section
of T M ∗⊗T M ∗. In fact, since Ip(·, ·) is symmetric and deﬁned for all p,
independent of any particular basis on T M ∗⊗T M ∗, then the metric tensor
is in fact a global section of Sym2 T M ∗.
Let p be a point of M, and let U be a coordinate neighborhood of p
with coordinates (x1, x2). This coordinate system deﬁnes the basis
dx1 ⊗dx1, dx1 ⊗dx2, dx2 ⊗dx1, dx2 ⊗dx2
on TpM ∗⊗TpM ∗.
Furthermore, each basis vector is a local section in
Γ(U, T M ∗⊗T M ∗). The coeﬃcient functions gij of the metric tensor are
functions so that, as an element of Γ(U, T M ∗⊗T M ∗), the metric tensor
can be written as
g11dx1 ⊗dx1 + g12dx1 ⊗dx2 + g21dx2 ⊗dx1 + g22dx2 ⊗dx2.
Deﬁnition 4.2.4. A tensor ﬁeld of type (r, s) is a global section of the vector
bundle T M ⊗r ⊗T M ∗⊗s.
We are now in a position to connect the usual physics formalism for
tensors (introduced in Section 2.4) over Rn with the modern mathematical
formalism.
Let A be a tensor ﬁeld of type (r, s) on a manifold M n. Over a coordi-
nate patch U of a diﬀerentiable manifold M with coordinates (x1, x2, . . . , xn),
we write the components of A as Ai1i2···ir
j1j2···js.
This means that Ai1i2···ir
j1j2···js

4.2. Vector Fields on Manifolds
137
are nr+s functions U →R.
Furthermore, with respect to the basis on
T M ⊗r
p
⊗T M ∗⊗s
p
associated to the coordinates,
A = Ai1i2···ir
j1j2···js
∂
∂xi1 ⊗
∂
∂xi2 ⊗· · · ⊗
∂
∂xir ⊗dxj1 ⊗dxj2 ⊗· · · ⊗dxjs,
where we have used the Einstein summation convention.
If U ′ is another coordinate patch on M with coordinates (¯x1, ¯x2, . . . , ¯xn),
we label the components of A in reference to this system as ¯Ak1k2···kr
l1l2···ls .
Again, these components are a collection of nr+s functions U ′ →R. On
the intersection U ∩U ′, both sets of components describe the same ten-
sor but in reference to diﬀerent bases. Following the same reasoning that
proves Equation (4.4), one shows that the components are related to each
other by
¯Ak1k2···kr
l1l2···ls
= ∂¯xk1
∂xi1
∂¯xk2
∂xi2 · · · ∂¯xkr
∂xir
∂xj1
∂¯xl1
∂xj2
∂¯xl2 · · · ∂xjs
∂¯xls Ai1i2···ir
j1j2···js.
(4.7)
In Deﬁnition 2.4.6, we called the collection of nr+s functions that trans-
form according to Equation (4.7) the components of a tensor of type (r, s).
We have now shown precisely what is meant by a tensor and why their
components transform according to Deﬁnition 2.4.6. Furthermore, in the
process, we have generalized the notion of tensors in Rn to tensor ﬁelds
over a manifold M.
As we promised in the introduction to this chapter, vector bundles on a
manifold allow for the possibility of doing physics on a manifold. We begin
to see this in the following way.
Let δ > 0, and let γ : (−δ, δ) →M be a diﬀerentiable curve on M.
Recall that we must understand γ as a diﬀerentiable function between
manifolds. Let X be a vector ﬁeld on M so that for each p ∈M, Xp is a
tangent vector in TpM. Referring to Example 3.3.8 for notation, the curve
γ is called a trajectory of X through p if
γ′(t) = Xγ(t)
for all t ∈(−δ, δ) and α(0) = p. A trajectory is also called an integral curve
of the vector ﬁeld X.
Since a diﬀerentiable manifold is locally diﬀeomorphic to Rn, the stan-
dard theorems of existence, uniqueness, and continuous dependence on ini-
tial conditions for ordinary diﬀerential equations carry over to the context
of diﬀerentiable manifolds (see [3, Sections 2.7 and 2.8]). These theorems
imply the local existence and uniqueness of trajectories of a vector ﬁeld

138
4. Analysis on Manifolds
Figure 4.2. A vector ﬁeld on a manifold.
through any points on a diﬀerentiable manifold. We restate these funda-
mental results from the theory of diﬀerential equations for the context of
manifolds.
Theorem 4.2.5. Let M be a diﬀerentiable manifold, and let X be a vector
ﬁeld on M. Let p ∈M. There exists an open neighborhood U of p in M,
a positive real δ > 0, and a diﬀerentiable map ϕ : (−δ, δ) × U →M such
that t →ϕ(t, q) for t ∈(−δ, δ) is the unique curve that satisﬁes
∂ϕ
∂t = Xϕ(t,q),
and ϕ(0, q) = q.
Note in the above theorem that ∂ϕ
∂t means the tangent vector ϕ∗( ∂
∂t|(t,q)).
It is common to write ϕt(q) for ϕ(t, q). Then one calls the function ϕt :
U →M the local ﬂow of X on M.
Figure 4.2 depicts a vector ﬁeld X on two-dimensional manifold M
(embedded in R3). The black curve is a particular trajectory since every
tangent vector to the curve at p (a point on the curve) is parallel to Xp.
Furthermore, the shown curve is only the locus of the trajectory since the
trajectory itself is a curve parametrized in such a way that the velocity
vector at each point p is exactly Xp.
4.2.1
Push-Forwards of Vector Fields
If F : M →N is a diﬀerentiable map and X is a vector ﬁeld on M, then for
each point p ∈M we deﬁne the vector F∗Xp ∈TF (p)N as the push-forward

4.2. Vector Fields on Manifolds
139
of X by F. This does not in general deﬁne a vector ﬁeld on N. If F is not
surjective, there is no natural way to deﬁne a vector ﬁeld associated to X
on N −F(M). (Even setting the putative push-forward vector ﬁeld to 0 on
N −F(M) would not ensure a continuous vector ﬁeld on N.) Furthermore,
if F is not injective and if p1 and p2 are preimages of a point q ∈F(M), then
nothing guarantees that F∗(Xp1) = F∗(Xp2). Thus, the push-forward is not
well deﬁned in this case. However, we can make the following deﬁnition.
Deﬁnition 4.2.6. Let M and N be diﬀerentiable manifolds, let F : M →N
be a diﬀerentiable map, let X be a vector ﬁeld on M, and let Y be a vector
ﬁeld on N. We say that X and Y are F-related if F∗(Xp) = YF (p) for all
p ∈M.
With this terminology, the above comment can be rephrased to say that
if X is a vector ﬁeld on M and F : M →N is a diﬀerentiable map, then
there does not necessarily exist a vector ﬁeld on N that is F-related to X.
Proposition 4.2.7. Let F : M →N be a diﬀerentiable map between diﬀer-
entiable manifolds. Let X ∈X(M) and Y ∈X(N). The vector ﬁelds X
and Y are F-related if and only if for every open subset U of N and every
function f ∈C1(U, R) we have
X(f ◦F) = (Y f) ◦F.
Proof: For any p ∈M and any f ∈C1(U, R), where U is a neighborhood
of F(p), we have
X(f ◦F)(p) = Xp(f ◦F) = dFp(Xp)(f) = F∗(Xp)(f).
On the other hand,
(Y f) ◦F(p) = (Y f)(F(p)) = YF (p)f.
Thus, X(f ◦F) = (Y f) ◦F is true for all f if and only if F∗(Xp) = YF (p)
for all p ∈M. The proposition follows.
□
Though in general vector ﬁelds cannot be pushed forward via a diﬀeren-
tiable map, we show one particular case in which push-forwards for vector
ﬁelds exist.
Proposition 4.2.8. Let X ∈X(M) be a vector ﬁeld, and let F : M →N be
a diﬀeomorphism. There exists a unique vector ﬁeld Y ∈X(N) that is F-
related to X. Furthermore, if X is of class Ck and F is a diﬀeomorphism
of class Ck, then so is Y .

140
4. Analysis on Manifolds
Proof: In order for X and Y to be F-related, we must have F∗Xp = YF (p).
Therefore, we deﬁne Yq = F∗(XF −1(q)). Since F is a diﬀeomorphism, the
association q →Yq is well deﬁned. However, we must check this association
is continuous before we can call it a vector ﬁeld.
If (xi) is a coordinate system on a neighborhood of p = F −1(q) and if
(yj) is a coordinate system on a neighborhood of q, then the coordinates
of Yq are
Yq = ∂F j
∂xi

F −1(q)
Xi
F −1(q)
∂
∂yj

q
.
Finally, if F −1 and X are of class Ck, then by composition and product
rule, the map N →T N deﬁned by q →(q, Yq) is of class Ck.
□
4.2.2
The Lie Bracket
We remind the reader that a vector ﬁeld X on a manifold M is such that, at
each point p ∈M, we have a diﬀerential operator on real-valued functions
Xp : C1(M, R) →R. It is convenient to think of a vector ﬁeld as a mapping
X : C1(M, R) →C0(M, R) via the identiﬁcation
Xf = (p →Xp(f)).
Over a coordinate chart U of M with coordinate system (x1, x2, . . . , xn),
we write
Xp =
n

i=1
ai(p)∂i,
so the real-valued function Xf on M is deﬁned by
(Xf)(p) =
n

i=1
ai(p) ∂f
∂xi

p
.
Assume now that M is a C2-manifold and that X is a diﬀerentiable
vector ﬁeld, i.e., that over any coordinate chart, the corresponding compo-
nents ai are diﬀerentiable functions M →R. With the above interpretation
of a vector ﬁeld on M, we can talk about the functions Y (Xf) or X(Y f),
where X and Y are two diﬀerentiable vector ﬁelds on M. However, neither
of the operations XY or Y X leads to another vector ﬁeld. Set
X = ai∂i
and
Y = bj∂j,

4.2. Vector Fields on Manifolds
141
where we use Einstein summation convention. Then for any function f ∈
C2(M, R), we have
X(Y f) = X(bj∂jf) =
n

i=1
ai∂i(bj∂jf) = ai(∂ibj)(∂jf) + aibj(∂i(∂jf)).
(4.8)
Thus, we see that f →X(Y f)(p) is obviously not a tangent vector to M
at p since it involves a repeated diﬀerentiation of f. Nonetheless, we do
have the following proposition.
Proposition 4.2.9. Let M be a diﬀerentiable manifold, and let X and Y be
two vector ﬁelds of class C1. Then the operation f →(XY −Y X)f is
another vector ﬁeld.
Proof: Since the second derivatives of f are continuous, then the mixed
partials with respect to the same variables, though ordered diﬀerently, are
equal. By using Equation (4.8) twice, we ﬁnd that
(XY −Y X)f =

ai∂ibj∂jf

−

bj∂jai∂if

=

ai∂ibj −bi∂iaj ∂f
∂xj .
Since for all j = 1, . . . , n the expressions in the above parentheses are
continuous real-valued functions on M, then (XY −Y X) has the structure
of a vector ﬁeld.
□
Deﬁnition 4.2.10. The vector ﬁeld deﬁned in Proposition 4.2.9 is called the
Lie bracket of X and Y and is denoted by [X, Y ] = XY −Y X. If X and Y
are of class Cn, then [X, Y ] is of class Cn−1. Also, if X and Y are smooth
vector ﬁelds, then so is [X, Y ].
The proof of Proposition 4.2.9 shows that, in a coordinate neighbor-
hood, if X = ai∂i and Y = bj∂j, the Lie bracket is
[X, Y ] =

ai∂ibj −bi∂iaj
∂j.
Example 4.2.11. Consider the manifold R3 −{(x, y, z)|z = 0}, and consider
the two vector ﬁelds
X = xy ∂
∂x + 1
z
∂
∂y −3yz3 ∂
∂z ,
Y = ∂
∂x + (x + y) ∂
∂z .

142
4. Analysis on Manifolds
The one iterated derivation is
XY f =

xy ∂
∂x + 1
z
∂
∂y −3yz3 ∂
∂z
	 ∂f
∂x + (x + y)∂f
∂z
	
= xy ∂2f
∂x2 + xy ∂f
∂z + xy(x + y) ∂2f
∂x∂z + 1
z
∂2f
∂y∂x + 1
z
∂f
∂z
+ 1
z (x + y) ∂2f
∂y∂z −3yz3 ∂2f
∂z∂x −3yz3(x + y)∂2f
∂z2 .
The expression Y Xf has exactly the same second derivative expressions
for f, and upon subtracting, we ﬁnd that
[X, Y ] = (XY −Y X) = −y ∂
∂x+ 1
z2 (x+y) ∂
∂y +

xy + 1
z + 9yz2(x + y)
	 ∂
∂z .
For speciﬁc calculations, suppose that M is an m-dimensional manifold
and that x : U →Rm is a coordinate patch on M. Then x−1 : x(U) →U
is a parametrization of U. Suppose that X ∈X(M) and that over U the
components of X are functions ai : M →R so that X = ai∂i. Then the
partial derivative ∂jai is explicitly
∂(ai ◦x−1)
∂xj
.
In local coordinates, the functions ai are explicitly ai(p) = (ai◦x−1)(x1, . . . ,
xm). Thus, one calculates these partials if the ai are given in local coordi-
nates or if ai are given as functions M →R and we have a parametrization
x−1 of the coordinate neighborhood U.
The Lie bracket has the following algebraic properties.
Proposition 4.2.12. Let X, Y , and Z be diﬀerentiable vector ﬁelds on a dif-
ferentiable manifold M. Let a, b ∈R, and let f and g be diﬀerentiable
functions M →R. Then the following hold:
1. Anticommutativity: [Y, X] = −[X, Y ].
2. Bilinearity: [aX + bY, Z] = a[X, Z] + b[Y, Z] and similarly for the
second input to the bracket.
3. Jacobi identity: [[X, Y ], Z] + [[Y, Z], X] + [[Z, X], Y ] = 0.
4. [fX, gY ] = fg[X, Y ] + fX(g)Y −gY (f)X.
Proof: (The proofs of these facts are straightforward and are left as exer-
cises for the reader.)
□

4.2. Vector Fields on Manifolds
143
p
t
t
−t
−t
X
Y
X
Y
p
c(t)
Figure 4.3. The curve paths deﬁning c(t).
Besides the algebraic properties, the Lie bracket also carries a more
geometric interpretation. The bracket [X, Y ] measures an instantaneous
path dependence between the integral curves of X and Y . To be more
precise, for suﬃciently small t ∈(−ε, ε), consider the curve c(t) that
• starts at a point c(0) = p;
• follows the integral curve of X starting at p for time t;
• starting from there, follows the integral curve of Y for time t;
• then follows the integral curve of X backwards by time −t;
• then follows the integral curve of Y backwards by time −t.
(See Figure 4.3.) If ϕt is the ﬂow for X and ψt is the ﬂow for Y , then this
curve c : (−ε, ε) →M is
c(t) = ψ−t(φ−t(ψt(φt(0)))).
Two properties are obvious. If t approaches 0, then c(t) approaches p.
Also, if x is a system of coordinates on a patch U of M and if X =
∂1 and Y = ∂2, then the above steps for the description of c(t) travel
around a “square” with side t based at p, and thus c(t) is constant. Other
properties are not so obvious, and we refer the reader to [49, Proposition
5.15, Theorem 5.16] for proofs.
Proposition 4.2.13. Deﬁning the curve c(t) as above,
1. c′(0) = 0;
2. c′′(0) is a derivation and hence an element of TpM;
3. c′′(0) = 2[X, Y ]p.

144
4. Analysis on Manifolds
Consequently, from an intuitive perspective, the Lie bracket [X, Y ] is
a vector ﬁeld that at p measures the second-order derivation of c(t) at p.
Since the ﬁrst derivative c′(0) is 0, then c′′(0) = 2[X, Y ]p gives the direction
of motion of c(t) out of p as a second-order approximation.
Problems
4.2.1. Let M = R2. Calculate the Lie bracket [X, Y ] for each of the following
pairs of vector ﬁelds:
(a) X = x ∂
∂x + y ∂
∂y and Y = −y ∂
∂x + x ∂
∂y .
(b) X = sin(x + y) ∂
∂x + cos x ∂
∂y and Y = cos x ∂
∂x + sin y ∂
∂y .
4.2.2. Let M = R3. Calculate the Lie bracket [X, Y ] for each of the following
pairs of vector ﬁelds:
(a) X = z2 ∂
∂x + xy ∂
∂z and Y = (x + y3) ∂
∂y + yz ∂
∂z .
(b) X = yz ∂
∂x + xz ∂
∂y + xy ∂
∂z and Y = x ∂
∂x + y ∂
∂y + z ∂
∂z .
(c) X = ln(x2 + 1) ∂
∂y + tan−1(xy) ∂
∂z and Y = ∂
∂x.
4.2.3. Let M = S2 be the unit sphere and let U be the coordinate patch para-
metrized by
x−1(u1, u2) = (cos u1 sin u2, sin u1 sin u2, cos u2),
with (u1, u2) ∈(0, 2π) × (0, π). Let X = cos u1 sin u2∂1 + sin u1 sin u2∂2,
Y = ∂1, and Z = sin u2∂1 be vector ﬁelds over U.
(a) Show that X and Z can be extended continuously to vector ﬁelds
over all of M.
(b) Show that Y cannot be extended continuously to a vector ﬁeld in
X(M).
(c) Calculate the components of [X, Z] over U.
4.2.4. Let S be a regular surface in R3, and let X be a vector ﬁeld on R3. For
every p ∈S, deﬁne Yp as the orthogonal projection of Xp onto TpS. Show
that Y is a vector ﬁeld on S.
4.2.5. Suppose that M is the torus that has a dense coordinate patch parame-
trized by
x−1(u, v) = ((3 + cos v) cos u, (3 + cos v) sin u, sin v) .
Consider the vector ﬁeld X = −z ∂
∂x + x ∂
∂z ∈R3. In terms of the coordi-
nates (u, v), calculate the vector ﬁeld on M induced from X by orthogonal
projection, as described in the previous exercise.

4.3. Differential Forms
145
4.2.6. Let M = S1 × S1 × S1 be the 3-torus given as an embedded submanifold
of R4 by the parametrization
(u, v, w) →( (4 + (2 + cos u) cos v) cos w,
(4 + (2 + cos u) cos v) sin w, (2 + cos u) sin v, sin u).
Consider the radial vector ﬁeld in R4 given by Z = x1∂1 + x2∂2 + x3∂3 +
x4∂4. In terms of the coordinates (u, v, w), calculate the vector ﬁeld on M
induced from X by orthogonal projection of Xp onto TpM for all p ∈M.
4.2.7. Find a vector ﬁeld on S2 that vanishes at one point. Write down a formula
expression for this vector ﬁeld in some coordinate patch of S2.
4.2.8. Prove that TS1 is diﬀeomorphic to S1 × R.
4.2.9. Prove Proposition 4.2.12.
4.2.10. Let F : M →N be a diﬀerentiable map. Let X1, X2 ∈X(M), and let
Y1, Y2 ∈X(N). Suppose that Xi is F-related to Yi. Prove that [X1, X2] is
F-related to [Y1, Y2].
4.2.11. Let M be any diﬀerentiable manifold.
Show that X(M) is an inﬁnite-
dimensional vector space.
4.3
Differential Forms
We now consider a particular class of tensor ﬁelds, which are called dif-
ferential forms. As we will see, they have many uses in geometry and in
physics, in particular for integration on manifolds. We refer the reader to
Section C.5 for the linear algebra behind the wedge operation deﬁned in this
section. We also encourage the reader to see Section C.6, which provides
some background on the usefulness of diﬀerential forms on manifolds.
For simplicity, from now on we will restrict our attention to smooth
manifolds. One should understand that in what follows, one could discuss
manifolds, vector ﬁelds, etc., that are merely diﬀerentiable or of class Ck
without changing much of the presentation.
4.3.1
Deﬁnitions
Deﬁnition 4.3.1. Let M n be a smooth manifold. A diﬀerential form ω of
rank r on M (or more succinctly, r-form) is a smooth global section (tensor
ﬁeld) of 1r(T M ∗).
In other words, for each p ∈M, one associates ωp ∈1r(TpM ∗) in such
a way that ωp varies smoothly with p. The tensor ωp is an alternating
r-multilinear function TpM ⊗r →R. We point out that diﬀerential forms of

146
4. Analysis on Manifolds
rank 0 are simply smooth real-valued functions on M and that a diﬀerential
form of rank 1 is a covector ﬁeld, i.e., a smooth vector ﬁeld in T M ∗.
Let U be a coordinate neighborhood of M with coordinates x = (x1,
x2, . . . , xn). Deﬁne I(r, n) as the set of all increasing sequences of length r
with values in {1, 2, . . ., n}. For example, (2, 3, 7) ∈I(3, 7) because there
are three elements in the sequence, they are listed in increasing order, and
their values are in {1, 2, . . ., 7}. Over the coordinate patch U, an r-form ω
can be written in a unique way as
ω =

I∈I(r,n)
aI dxI,
where each aI is a smooth function, and where we denote dxI = dxi1 ∧
· · · ∧dxir when I is the r-tuple I = (i1, . . . , ir). Recall that the symbol dxi
is deﬁned in Equations (4.5) and (4.6).
Deﬁnition 4.3.2. If U is an open subset of M, we denote by Ωr(U) the set
of all diﬀerential forms of rank r on U.
We remark that, similar to Problem 4.2.11, for each r, the set Ωr(U) is
in fact an inﬁnite-dimensional vector space. In particular, if ω, η ∈Ωr(U)
and λ ∈R, then ω + η ∈Ωr(U) and λω ∈Ωr(U), where by deﬁnition
(ω + η)p = ωp + ηp
and
(λω)p = λ ωp
in
r2
T M ∗.
Stronger yet, not only is each Ωr(U) closed under scalar multiplication,
but it is closed under multiplication by a smooth function. More precisely,
for all smooth functions f : U →R, we have fω ∈Ωr(U), where (fω)p =
f(p)ωp for all p ∈U.
Finally, similar to the alternating products of a ﬁxed vector space, for
ω ∈Ωr(U) and η ∈Ωs(U), we deﬁne the exterior product ω ∧η ∈Ωr+s(U)
as the diﬀerential form deﬁned by (ω ∧η)p = ωp ∧ηp for all p ∈M.
Example 4.3.3. Consider the sphere S2, and let U be the coordinate neigh-
borhood with a system of coordinates x deﬁned by the parametrization
x−1(u, v) = (cos u sin v, sin u sin v, cos v) deﬁned on (0, 2π) × (0, π). Let
ω = (sin2 v) du + (sin v cos v) dv,
η = cos u sin v du + (sin u cos v −sin v) dv
be two 1-forms on S2. Remarking that du∧du = dv∧dv = 0, one calculates
ω ∧η = sin2 v(sin u cosv −cos u cosv −sin v)du ∧dv.

4.3. Differential Forms
147
4.3.2
The Exterior Differential
Let f be a smooth real-valued function on a smooth manifold M, and
let X ∈X(M). Viewing f as a diﬀerential map between manifolds, the
diﬀerential df is such that, at each point p ∈M, it evaluates dfp(Xp) to a
tangent vector in Tf(p)(R). However, the tangent space Tf(p)(R) is equal
to R, so dfp(Xp) is just a real number. Hence, dfp ∈TpM ∗, and since all of
the operations vary smoothly with p, then df ∈Ω1(M). If x = (x1, . . . , xn)
is a coordinate system on an open set U ⊂M, then in coordinates we have
df =
n

i=1
∂f
∂xi dxi.
Since C∞(U) = Ω0(M), the diﬀerential d deﬁnes a linear transformation
d : Ω0(U) →Ω1(U).
We now generalize this remark by the following
deﬁnition.
Deﬁnition 4.3.4. Let ω = "
I aI dxI be a smooth diﬀerential r-form over U.
The exterior diﬀerential of ω is the (r + 1)-form written as dω and deﬁned
by
dω =

I∈I(r,n)
(daI) ∧dxI.
(4.9)
Example 4.3.5. Revisiting Example 4.3.3, we calculate dω and dη. First, for
dω we have
dω = (d(sin2 v)) ∧du + (d(sin v cos v)) ∧dv
= (2 sin v cos v dv) ∧du +

(cos2 v −sin2 v) dv

∧dv
= (−2 sin v cos v) du ∧dv.
For dη, we calculate
dη = (d(cos u sin v)) ∧du + (d(sin u cosv −sin v)) ∧dv
= ((−sin u sin v) du + (cos u cosv) dv) ∧du
+ ((cos u cosv) du + (−sin u sin v −cos v) dv) ∧dv
= (cos u cosv) dv ∧du + (cos u cosv) du ∧dv = 0.
The diﬀerential form η has the perhaps unexpected property that dη = 0.
We will say that η is a closed 1-form (see Deﬁnition 4.3.10).
Proposition 4.3.6. Let M be a smooth manifold, and let U be an open subset
of M. The exterior diﬀerential satisﬁes the following:

148
4. Analysis on Manifolds
1. For each 0 ≤r ≤n−1, the operator d : Ωr(U) →Ωr+1(U) is a linear
map.
2. If ω ∈Ωr(U) and η ∈Ωs(U), then
d(ω ∧η) = dω ∧η + (−1)rω ∧dη.
3. For all ω ∈Ωr(U), we have d(dω).
Proof: For part 1, set ω = "
I aI dxI, where the summation is over all
I ∈I(r, n) and aI are smooth real-valued functions on M. Then from
Equation (4.9), each daI is a 1-form, so obviously the summation is over
(r + 1)-forms.
Now let η = "
I bI dxI be another r-form, and let λ, μ ∈R. Then
d(λω + μη) =

I
d(λaI + μbI) ∧dxI
=

I
⎛
⎝
n

j=1
∂
∂xj (λaI + μbI) dxj
⎞
⎠∧dxI
=

I
⎛
⎝
n

j=1

λ∂aI
∂xj + μ ∂bI
∂xj
	
dxj
⎞
⎠∧dxI
= λ

I
⎛
⎝
n

j=1
∂aI
∂xj dxj
⎞
⎠∧dxI + μ

I
⎛
⎝
n

j=1
∂bI
∂xj dxj
⎞
⎠∧dxI
= λdω + μdη.
This proves linearity of d.
For part 2, again let ω be as above, and let η ∈Ωs(U) be given by
η = "
J bJ dxJ, where the summation in J runs over I(s, n).
By the
linearity of the wedge product, we can write
ω ∧η =

I

J
aIbJ dxI ∧dxJ.
Note that for various combinations of I and J, the wedge products dxI∧dxJ
will cancel if I and J share any common indices. Then

4.3. Differential Forms
149
d(ω ∧η) =

I

J
 n

k=1
∂aIbJ
∂xk
dxk

∧dxI ∧dxJ
=

I

J
 n

k=1
 ∂aI
∂xk bJ + aI
∂bJ
∂xk
	
dxk

∧dxI ∧dxJ
=

I

J
 n

k=1
∂aI
∂xk bJ dxk

∧dxI ∧dxJ
+

I

J
 n

k=1
aI
∂bJ
∂xk dxk

∧dxI ∧dxJ.
But by the properties of wedge products, dxk ∧dxI ∧dxJ = (−1)rdxI ∧
dxk ∧dxJ (see Proposition C.5.20). Thus,
d(ω ∧η) =

I
n

k=1

J
∂aI
∂xk bJ dxk ∧dxI ∧dxJ
+ (−1)r 
I

J
n

k=1
aI
∂bJ
∂xk dxI ∧dxk ∧dxJ
=

I
n

k=1
∂aI
∂xk dxk ∧dxI

∧η
+ (−1)rω ∧

J
n

k=1
∂bJ
∂xk dxk ∧dxJ

= dω ∧η + (−1)rω ∧dη.
To prove part 3, we ﬁrst show that d(df) = 0 for a smooth function f
on M. We have
d(df) = d
 n

i=1
∂f
∂xi dxi

=
n

i=1
n

j=1
∂2f
∂xj∂xi dxj ∧dxi
=

I∈I(2,n)

∂2f
∂xi1∂xi2 −
∂2f
∂xi2∂xi1
	
dxI,
where we assume I = (i1, i2). However, since the function f is smooth, by
Clairaut’s Theorem on mixed partials each component function is 0. Thus
d(df) = 0.

150
4. Analysis on Manifolds
Now for any r-form ω = "
I aI dxI we have
d(dω) = d

I
d(aI) ∧dxI

=

I
d(d(aI) ∧dxI)
(by linearity)
=

I

d(daI) ∧dxI −daI ∧d(dxI)

(by part 2)
= 0,
where the last line follows because d(daI) = 0 and d(dxI) = 0 for all I. □
It is illuminating to see what the exterior diﬀerential represents when
the manifold in question is Rn. We ﬁrst emphasize two particular cases.
First, let f be a smooth real-valued function on Rn. Then
df =
n

i=1
∂f
∂xi dxi.
Thus, df has exactly the same components as the gradient, deﬁned in
multivariable calculus as
grad f = ⃗∇f = (∂1f, ∂2f, . . . , ∂nf) .
Therefore, in our presentation, the gradient of a function f is in fact a
covector ﬁeld, i.e., a vector ﬁeld in T M ∗= (Rn)∗.
In calculus courses, one does not distinguish between vectors and cov-
ectors, i.e., vectors in Rn or in (Rn)∗, since these are isomorphic as vector
spaces. However, as we saw in Deﬁnitions 2.4.4 and 2.4.5, vector ﬁelds and
covector ﬁelds have diﬀerent transformational properties under change-of-
coordinate.
One can easily verify that the gradient of a function transforms co-
variantly, but it is also instructive to see how this plays out in common
formulas in calculus. For example, the chain rule for paths states that if
⃗c(t) is a diﬀerentiable curve in Rn and f : Rn →R is diﬀerentiable, then
d
dtf(⃗c(t)) = ⃗∇f⃗c(t) · ⃗c ′(t).
However, from the perspective of multilinear algebra, one should under-
stand the dot product as the contraction map V ∗⊗V →R deﬁned by

4.3. Differential Forms
151
λ ⊗⃗v →λ(⃗v). Since by deﬁnition ⃗c ′(t) is a tangent vector to Rn at ⃗c(t),
then we should view the gradient ⃗∇f as a covector in (Rn)∗.
As a second illustration, consider (n −1)-forms over Rn.
For each
1 ≤j ≤n, deﬁne the (n −1)-forms ηj as
ηj = (−1)j−1dx1 ∧· · · ∧dxj−1 ∧dxj+1 ∧· · · ∧dxn.
(4.10)
For each p ∈M, the set {ηj
p} is a basis for 1n−1 TpM ∗, so any (n−1)-form
ω can be written as ω = "n
i=1 aiηi. Thus, ignoring contravariance and
covariance, we can view ω as a vector ﬁeld over Rn. Note that having the
(−1)j−1 factor in the deﬁnition of ηj leads to the identity
dxj ∧ηi =

0,
if i ̸= j,
dx1 ∧dx2 ∧· · · ∧dxn,
if i = j.
(4.11)
Thus, for the diﬀerential of ω, we have
dω =
n

i=1
n

j=1
∂ai
∂xj dxj ∧ηj =
 n

i=1
∂ai
∂xi

dx1 ∧dx2 ∧· · · ∧dxn.
Hence, for the case of (n−1)-forms, the exterior diﬀerential d operates like
the divergence operator div = ⃗∇· on a vector ﬁeld (a1, . . . , an) in Rn.
In the particular case of R3, the exterior diﬀerential carries another
point of signiﬁcance. Let ω ∈Ω1(R3), and write ω = "n
i=1 ai dxi. Then
dω =
n

i=1
n

j=1
∂ai
∂xj dxj ∧dxi
=
∂a2
∂x1 −∂a1
∂x2
	
dx1 ∧dx2 +
∂a3
∂x1 −∂a1
∂x3
	
dx1 ∧dx3
+
∂a3
∂x2 −∂a2
∂x3
	
dx2 ∧dx3
=
∂a3
∂x2 −∂a2
∂x3
	
η1 +
∂a1
∂x3 −∂a3
∂x1
	
η2 +
∂a2
∂x1 −∂a1
∂x2
	
η3,
which is precisely the curl of the vector ﬁeld (a1, a2, a3).
It is particularly interesting to note that the property d(dω) = 0 in
Proposition 4.3.6 summarizes simultaneously the following two standard
theorems in multivariable calculus:
curl grad f = ⃗0
[52, Theorem 17.3],
div curl ⃗F = 0
[52, Theorem 17.11],

152
4. Analysis on Manifolds
where f : R3 →R is a function of class C2 and ⃗F : R3 →R3 is a vector
ﬁeld of class C2.
We point out that the forms ηj deﬁned in Equation (4.10) are particular
instances of the Hodge star operator ⋆that will be introduced in Section
C.6.3. One constructs the Hodge star operator in the general context of
a vector space equipped with an inner product (a bilinear form that is
symmetric and nondegenerate). In the above situation, we have V = Rn
and the inner product ⟨, ⟩is the standard Euclidean dot product. Then
according to Proposition C.6.10, we have
ηj = ⋆dxj.
4.3.3
Pull-Backs
We now deﬁne the notion of a pull-back of a diﬀerential form of rank r by a
smooth function between manifolds. Though the construction of pull-backs
is interesting in its own right, the pull-back is essential for deﬁning how to
integrate on a manifold.
Deﬁnition 4.3.7. Let f : M m →N n be a smooth map between two smooth
manifolds, and let ω ∈Ωr(N). Deﬁne the pull-back of ω by f, written f ∗ω,
by the alternating multilinear function on TpM that is deﬁned by
(f ∗ω)p(v1, v2, . . . , vr) = ωp(dfp(v1), dfp(v2), . . . , dfp(vr)),
(4.12)
where vi are tangent vectors in TpM.
If x is a local coordinate system on M and y is a coordinate system on N,
then locally, for every v ∈TpM, the coordinates of dfp(v) are "m
i=1 ∂if jvi
for j = 1, . . . , n.
Recall that f j = yj ◦f : M →R.
Then for every
I = (i1, . . . , ir) ∈I(r, m), we have
(f ∗dxI)p(v1, . . . , vr) = dxI(dfp(v1), . . . , dfp(vr))
= (df i1)p(v1, . . . , vr) ∧· · · ∧(df ir)p(v1, . . . , vr)
= (df i1 ∧· · · ∧df ir)p(v1, . . . , vr).
We conclude that in coordinates, as a vector ﬁeld over M,
f ∗
⎛
⎝

i∈I(r,m)
aI dxI
⎞
⎠=

i∈I(r,m)
(aI ◦f) df i1 ∧· · · ∧df ir.
(4.13)

4.3. Differential Forms
153
As a corollary to this, which we leave as an exercise (Problem 4.3.13),
we deduce the following fundamental formula. If M and N are smooth
manifolds of the same dimension n, is f a smooth map between them, and
ω is an n-form, then
f ∗(ω)p = (det dfp)ωf(p).
(4.14)
The pull-back of r-forms satisﬁes a few more properties, listed in these
propositions.
Proposition 4.3.8. Let f : M m →N n be a smooth map between smooth
manifolds. The following hold for all r ≤min(m, n):
1. The pull-back f ∗: Ωr(N) →Ωr(M) is a linear function.
2. For all ω, η ∈Ωr(N), f ∗(ω ∧η) = (f ∗ω) ∧(f ∗η).
3. For all ω ∈Ωr(N) with r < min(m, n), f ∗(dω) = d(f ∗ω).
Proof: Part 1 follows immediately from the functional deﬁnition in Equa-
tion (4.12). Part 2 is an easy application of Equation (4.13). Finally, for
part 3, note that d(df i1 ∧· · · ∧df ir) = 0 by a repeated use of Proposition
4.3.6(2) and the fact that d(df i) = 0.
Then, if ω = "
I aIdxI, Equa-
tion (4.13) gives
d(f ∗ω) =

I∈I(r,m)
d

(aI ◦f) df i1 ∧· · · ∧df ir
=

I∈I(r,m)
d(aI ◦f) ∧df i1 ∧· · · ∧df ir + (aI ◦f)d(df i1 ∧· · · ∧df ir)
=

I∈I(r,m)
d(aI ◦f) ∧df i1 ∧· · · ∧df ir
=

I∈I(r,m)
d(aI ◦f) ∧f ∗(dxI)
= f ∗(dω).
□
Proposition 4.3.9. Let f : M →N and g : U →M be smooth functions
between smooth manifolds. Then (f ◦g)∗= g∗◦f ∗.
Proof: (Left as an exercise for the reader.)
□

154
4. Analysis on Manifolds
4.3.4
Closed and Exact Forms
As a ﬁnal paragraph in this section, we brieﬂy mention how the apparently
simple fact that d(dω) = 0 serves as the basis for profound mathematics
related to the global geometry of a manifold.
Deﬁnition 4.3.10. Let M be a smooth manifold. A diﬀerential form ω ∈
Ωr(M) is called closed if dω = 0 and is called exact if there exist η ∈
Ωr−1(M) such that ω = dη.
With this terminology, one can say that the identity d(dω) = 0 means
that every exact form is closed. The converse is not true in general, and
it is precisely this fact that leads to profound results in topology. In the
language of homology, the sequence of vector spaces and linear maps
Ω0(M)
d
−−−−→Ω1(M)
d
−−−−→Ω2(M)
d
−−−−→· · ·
d
−−−−→Ωn(M)
satisfying the identity d◦d = 0 is called a complex. To distinguish between
ranks, one writes dr for the diﬀerential d : Ωr(M) →Ωr+1(M). The fact
that every exact form is closed can be restated once more by saying that
Im dr−1 is a vector subspace of ker dr. The quotient vector space
ker dr/ Im dr−1 = ker(d : Ωr(M) →Ωr+1(M))/ Im(d : Ωr−1(M) →Ωr(M))
is called the rth de Rham cohomology group of M, denoted Hr
dR(M). The
de Rham cohomology groups are in fact global properties of the manifold
M and are related to profound topological invariants of M. This topic
exceeds the scope of this book, but we wish to point out two ways in which
one can glimpse why the groups Hr
dR(M) are global properties of M.
As a ﬁrst example, consider the explicit covector ﬁelds ω and η on S2
described in Example 4.3.3. These vector ﬁelds were deﬁned in terms of
a coordinate patch U, which turned out to be dense on S2 (covers all but
a subset that is not two-dimensional). Furthermore, they were deﬁned on
U in such a way that they can be extended continuously over all of S2,
thereby deﬁning continuous (and here smooth) vector ﬁelds. The key is to
note that in terms of the local coordinates (u, v), one cannot use any pair
of smooth functions a1(u, v) and a2(u, v) to deﬁne a 1-form
ω = a1(u, v) du + a2(u, v) dv
that can be extended to create a smooth 1-form on S2. This restriction
shows that Ω1(S2) is aﬀected by the global geometry of S2. The principle
behind this example is true in general: the vector spaces Ωr(M), though

4.3. Differential Forms
155
inﬁnite-dimensional and usually impossible to describe explicitly, depend
on the global structure of M.
As a second example, we determine H0
dR(M) for any manifold.
Of
course, d−1 does not exist explicitly so we set, by convention, Ω−1(M) = 0,
i.e., the zero-dimensional vector space. Then Im d−1 = {0} is the trivial
subspace in Ω0(M). Furthermore, since Ω0(M) is the space of all smooth
real-valued functions on M, the 0th cohomology group is
H0
dR(M) = ker(d : C∞(M) →Ω1(M))/{0} = ker(d : C∞(M) →Ω1(M)),
namely, the subspace of all smooth functions on M whose diﬀerentials are
0. In other words, H0
dR(M) is the space of all functions that are constant
on each connected component of M. Thus, H0
dR(M) = Rℓ, where ℓis the
number of connected components of M, a global property.
Problems
4.3.1. Let M be a smooth manifold. Let ω ∈Ωr(U) be a nonzero r-form. Char-
acterize the forms η ∈Ωs(M) such that ω ∧η.
4.3.2. Let M = R3. Find the exterior diﬀerential of the following:
(a) x dy ∧dz + y dz ∧dx + z dx ∧dy.
(b) xy2z3 dx + y sin(xz) dz.
(c) dx ∧dy + x dy ∧dz
x2 + y2 + z2 + 1 .
4.3.3. Let M = Rn.
Let ω = x1 dx1 + · · · + xn dxn and η = x2 dx1 + · · · +
xn dxn−1 + x1 dxn.
(a) Calculate dω and dη.
(b) Calculate ω ∧η and d(ω ∧η).
(c) Calculate the exterior diﬀerential of x1η1 + x2η2 + · · · + xnηn, where
the forms ηi are deﬁned as in Equation (4.10).
4.3.4. Let M = S1 × S1 be the torus in R3 that has a coordinate neighborhood
(U, x) that can be parametrized by
x−1(u, v) →( (3 + cos u) cos v, (3 + cos u) sin v, sin u)
for (u, v) ∈(0, 2π)2.
Consider the two diﬀerential forms ω and η, given over U by ω = cos(u +
v) du + 2 sin2 u dv and η = 3 sin2 v du −4 dv.
(a) Show why ω and η extend to diﬀerential forms over the whole torus.
(b) Calculate ω ∧ω and ω ∧η.
(c) Calculate dω and dη.

156
4. Analysis on Manifolds
4.3.5. Consider the manifold RP3 with the standard atlas described in Example
3.1.6. Consider also the 1-form that is described in coordinates over U0 as
ω = x1 dx1 + x2(x3)3 dx2 + x1x2 dx3.
(a) Write down a coordinate expression for ω in U1, U2, and U3.
(b) Calculate dω and ω ∧ω in coordinates over U0.
(c) Calculate dω in coordinates over U1 and show explicitly that the
coordinates change as expected over U0 ∩U1.
4.3.6. Set ω = x1x2 dx2 + (x2 + 3x4x5) dx3 + ((x2)2 + (x3)2) dx5 as a 1-form over
R5. Calculate dω, ω ∧dω, ω ∧ω, and dω ∧dω ∧ω.
4.3.7. Consider the spacetime variables x, y, z, and t in R3, and label new
variables by x0 = ct, x1 = x, x2 = y, and x3 = z. Consider the two
2-forms α and β deﬁned by
α = −
3

i=1
Ei dx0 ∧dxi +
3

j=1
Bjηj
and
β =
3

i=1
Bi dx0 ∧dxi −
3

j=1
Ejηj,
where the forms ηj are the 2-forms deﬁned in Equation (4.10) over the
space variables, i.e., η1 = dx2 ∧dx3, η2 = −dx1 ∧dx3, and η3 = dx1 ∧dx2.
(a) Writing ⃗E = (E1, E2, E3) and ⃗B = (B1, B2, B3) as time-dependent
vector ﬁelds in R3, show that the source-free Maxwell’s equations
∇× ⃗E = −1
c
∂⃗B
∂t ,
∇· ⃗E = 0,
∇× ⃗B = 1
c
∂⃗E
∂t ,
∇· ⃗B = 0,
can be expressed in the form
dα = 0
and
dβ = 0.
(b) If we write the 1-form λ = −φ dx0 + A1 dx1 + A2 dx2 + A3 dx3, show
that dλ = α if and only if
⃗E = −∇φ −1
c
∂⃗A
∂t
and
⃗B = ∇× ⃗A.
4.3.8. In the theory of diﬀerential equations, if M and N are functions of x and y,
an integrating factor for an expression of the form M dy
dx + N is a function
I(x,y) such that
I(x, y)

M(x, y)dy
dx + N(x, y)
	
= d
dxF(x, y)
for some function F(x, y). If M is a smooth manifold and ω ∈Ω1(M), we
call an integrating factor of ω a smooth function f that is nowhere 0 on
M and such that fω is exact. Prove that if such a function f exists, then
ω ∧dω = 0.

4.3. Differential Forms
157
4.3.9. Let ω = (1 + xy2)exy2 dx + 2x2yexy2 dy be a 1-form on R2. Show that
dω = 0. Then ﬁnd a function f : R2 →R such that ω = df.
4.3.10. Let ω = yz dx ∧dz + (−y + xz) du ∧dz be a 2-form on R3. Show that
dω = 0. Then ﬁnd a 1-form λ such that ω = dλ.
4.3.11. Suppose that ω ∈Ω1(M) for some smooth manifold M. Suppose that over
each coordinate chart one writes ω = ωi dxi. Prove that if ∂jωi = ∂iωj
holds in every coordinate neighborhood around every point p ∈M, then
ω is a closed form.
4.3.12. Prove Proposition 4.3.9.
4.3.13. Let f : M →N be a smooth map between smooth manifolds, both of
dimension n. Let ω be a diﬀerential form of rank n. Prove that
f ∗(ω) = (det df) ω ◦f.
4.3.14. Let ω and η be forms on a smooth manifold M.
(a) Show that if ω and η are closed, then so is ω ∧η.
(b) Show that if ω and η are exact, then so is ω ∧η.
4.3.15. Let M be a manifold of dimension m ≥4. Let ω be a 2-form on M, and
let α and β be 1-forms. Show that
ω ∧α ∧β = 0
if and only if there exist 1-forms λ and η such that
ω = λ ∧α + η ∧β.
4.3.16. Consider the manifold GLn(R) of invertible matrices, and consider the
function det : GLn(R) →R as a function between manifolds.
(a) Prove that for all X ∈GLn(R), the tangent space is TXGLn(R) ∼=
Rn×n, the space of n × n matrices.
(b) Writing the entries of a matrix X ∈GLn(R) as X = (xi
j), prove that
∂det
∂xi
j
(X) = (det X)(X−1)i
j.
(c) Prove that the diﬀerential of the determinant map can be written as
d(det)X(A) = (det X) Tr(X−1A),
where Tr M = "
i mi
i is the trace of the matrix.

158
4. Analysis on Manifolds
4.4
Integration on Manifolds
The last topic in this chapter on analysis on a manifold covers the theory of
integration on manifolds. Two comments motivate the following approach.
First, the theory of integration on manifolds that we propose to develop
must generalize all types of integration introduced in the usual calculus
sequence. This includes
• integration of a one-variable, real-valued function over an interval;
• integration of a multivariable, real-valued function over a domain
in Rn;
• line integrals of functions in Rn;
• line integrals of vector ﬁelds in Rn;
• surface integrals of a real-valued function deﬁned over a closed and
bounded region of a regular surface;
• surface integrals of vector ﬁelds in R3.
It may seem hopeless to consolidate all of these operations into a single,
concise description, but diﬀerential forms allow us to do precisely that.
Second, readers who have pursued a course in analysis may be aware
of the diﬀerence between Riemannian integration, the theory introduced
in the usual calculus sequence, and Lebesgue integration, which relies on
the more diﬃcult techniques of measure theory. The theory developed here
below does not depend on either of these theories of integration but could
use either. The deﬁnitions for integration on a manifold use the fact that
a manifold is locally diﬀeomorphic to an open subset in Rn and deﬁne an
integral on a manifold in reference to integration on Rn. Therefore, one
can presuppose the use of either Riemannian integration or integration with
respect to the Lebesgue measure.
4.4.1
Partitions of Unity
The basis for deﬁning integration on a smooth manifold M n relies on re-
lating the integral on M to integration in Rn. However, since a manifold
is only locally homeomorphic to an open set in Rn, one can only deﬁne
directly integration on a manifold over a coordinate patch.

4.4. Integration on Manifolds
159
We begin this section by introducing a technical construction that
makes it possible, even from just a theoretical perspective, to piece to-
gether the integrals of a function over the diﬀerent coordinate patches of
the manifold’s atlas.
Deﬁnition 4.4.1. Let M be a manifold, and let V = {Vα}α∈I be a collection
of open sets that covers M. A partition of unity subordinate to V is a col-
lection of continuous functions {ψα : M →R}α∈I that satisfy the following
properties:
1. 0 ≤ψα(x) ≤1 for all α ∈I and all x ∈M.
2. ψα(x) vanishes outside a compact subset of Vα.
3. For all x ∈M, there exists only a ﬁnite number of α ∈I such that
ψα(x) ̸= 0.
4. "
α∈I ψα(x) = 1 for all x ∈M.
Note that the summation in the fourth condition always exists since,
by the third criterion, for all x ∈M, it is only a ﬁnite sum. Therefore,
one does not worry about issues of convergence in this deﬁnition.
The
terminology “partition of unity” comes from the fact that the collection of
functions {ψα} add up to the constant function 1 on M.
Theorem 4.4.2 (Existence of Partitions of Unity). Let M be a smooth manifold
with atlas A = {(Uα, φα)}α∈I. There exists a smooth partition of unity of
M subordinate to A.
For the sake of space, we forego a complete proof of this theorem and
refer the reader to [32, pp. 54–55], [47, Theorem 10.8], or [14, Section 14.1].
The proof relies on the existence of smooth real-valued functions that are
nonzero in an open set U ⊂Rn but identically 0 outside of U. Many of the
common examples of partitions of unity depend on the following lemma.
Lemma 4.4.3. The function f : R →R deﬁned by
f(x) =

0,
if x ≤0,
e−1/x,
if x > 0,
is a smooth function. (See Figure 4.4.)
The proof for this lemma is an exercise in calculating higher derivatives
and evaluating limits. (Interestingly enough, this function at x = 0 is an

160
4. Analysis on Manifolds
x
Figure 4.4. f(x) = e−1/x.
example of a function that is smooth, i.e., has all its higher derivatives,
but is not analytic, i.e., equal to its Taylor series over a neighborhood of
x = 0.)
The function f(x) in Lemma 4.4.3 is useful because it passes smoothly
from constant behavior to nonconstant behavior. This function f(x) also
leads immediately to functions with other desirable properties. For exam-
ple, f(x −a) + b is a smooth function that is constant and equal to b for
x ≤a and then nonconstant for x > a. In contrast, f(a −x) + b is a
smooth function that is constant and equal to b for x ≥a and then non-
constant for x < a. More useful still for our purposes, if a < b, the function
g(x) = f(x −a)f(b −x) is smooth, identically equal to 0 for x /∈(a, b), and
is nonzero for x ∈(a, b). We can call this a bump function over (a, b) (see
Figure 4.5(a)). Also, the function
h(x) =
f(b −x)
f(x −a) + f(b −x)
is smooth, is identically equal to 1 for x ≤a, identically equal to 0 for
x ≥b, and strictly decreasing over (a, b). The function h(x) is sometimes
called a cut-oﬀfunction (see Figure 4.5(b)).
We will illustrate how to construct partitions of unity over a manifold
with the following two simple examples.
x
1
3
(a) Bump function
x
1
3
(b) Cut-oﬀfunction
Figure 4.5. Useful smooth functions.

4.4. Integration on Manifolds
161
Example 4.4.4. Consider the real line R as a 1-manifold, and consider the
open cover U = {Ui}, where Ui = (i −1, i + 1). In this open cover, we note
that if n is an integer, then n is only contained in one set, Un, and if t is
not an integer, then t is contained in both U⌊t⌋and U⌊t⌋+1. Consider ﬁrst
the bump functions gi(x) deﬁned by
gi(x) = f(x −(i −0.9))f((i + 0.9) −x)
=
⎧
⎪
⎨
⎪
⎩
0,
if x ≤i −0.9,
e 1.8/(x−i+0.9)(x−i−0.9),
if i −0.9 < x < i + 0.9,
0,
if x ≥i + 0.9,
where we use the function f as deﬁned in Lemma 4.4.3. It is not hard to
show that these functions are smooth. Furthermore, by deﬁnition, gi(x) =
0 for x /∈[i −0.9, i + 0.9] = Ki, which is a compact subset of Ui. For any
i ∈Z, the only functions that are not identically 0 on Ui are gi−1, gi, and
gi+1. Now deﬁne
ψi(x) =
gi(x)
gi−1(x) + gi(x) + gi+1(x).
We claim that the collection {ψi}i∈Z forms a partition of unity subordinate
to U. Again, ψi(x) ̸= 0 for x ∈Ki and ψi(x) = 0 for x /∈Ki. Furthermore,
the only functions ψk that are not identically 0 on Ui are ψi−1, ψi, and
ψi+1. If x = n is an integer, then

i∈Z
ψi(x) = ψn(n) =
gn(n)
gn−1(n) + gn(n) + gn+1(n) = gn(n)
gn(n) = 1.
If instead x is not an integer, then when we set n = ⌊x⌋, we have

i∈Z
ψi(x) = ψn(x) + ψn+1(x)
=
gn(x)
gn−1(x) + gn(x) + gn+1(x) +
gn+1(x)
gn(x) + gn+1(x) + gn+2(x)
=
gn(x)
gn(x) + gn+1(x) +
gn+1(x)
gn(x) + gn+1(x) = 1
since gn−1(x) = gn+2(x) = 0 for x ∈Un ∩Un+1.
Example 4.4.5. Consider the unit sphere S2 given as a subset of R3. Cover
S2 with two coordinate patches (U1, x) and (U2, ¯x), where the coordinate

162
4. Analysis on Manifolds
Figure 4.6. Example 4.4.5.
functions have the following inverses:
x−1(u, v) = (cos u sin v, sin u sin v, cos v)
for (u, v) ∈(0, 2π) × (0, π),
¯x−1(¯u, ¯v) = (−cos ¯u sin ¯v, −cos ¯v, −sin ¯u sin ¯v)
for (¯u, ¯v) ∈(0, 2π) × (0, π).
Deﬁne now the bump functions
g1(u, v) = f(u −0.1)f(6 −u)f(v −0.1)f(3 −v),
g2(¯u, ¯v) = f(¯u −0.1)f(6 −¯u)f(¯v −0.1)f(3 −¯v),
where f is the function in Lemma 4.4.3. These functions are smooth and
vanish outside [0.1, 6] × [0.1, 3] = K, which is a compact subset of (0, 2π) ×
(0, π). Deﬁne also the bump functions hi : S2 →R by
h1(p) =

g1 ◦x(p),
if p ∈U1,
0,
if p /∈U1,
and
h2(p) =

g2 ◦¯x(p),
if p ∈U2,
0,
if p /∈U2.
By construction, these functions are smooth on S2 and vanish outside a
compact subset of U1 and U2, namely, x−1(K) and ¯x−1(K) respectively.
In Figure 4.6, the half-circles depict the complements of U1 and U2 on S2,
and the piecewise-smooth curves that surround the semicircles show the
boundary of x−1(K) and ¯x−1(K).
Finally, deﬁne the functions ψi : S2 →R by
ψi(p) =
hi(p)
h1(p) + h2(p).
These functions are well deﬁned since h1 and h2 are nonzero on the interior
of x−1(K) and ¯x−1(K), respectively, and these interiors cover S2. The pair
of functions {ψ1, ψ2} is a smooth partition of unity that is subordinate to
the atlas that we deﬁned on S2.

4.4. Integration on Manifolds
163
An object that recurs when dealing with partitions of unity is the set
over which the function is nonzero. We make the following deﬁnition.
Deﬁnition 4.4.6. Let f : M →R be a real-valued function from a mani-
fold M. The support of f, written supp f is deﬁned as the closure of the
nonzero set, i.e.,
supp f = {p ∈M | f(p) ̸= 0}.
A function is said to have compact support if supp f is a compact set.
4.4.2
Integrating Differential Forms
We are now in a position to deﬁne integration of n-forms on a smooth n-
dimensional manifold. We must begin by connecting integration of forms
in Rn to usual integration.
Deﬁnition 4.4.7. Let ω be a diﬀerential form of rank n over Rn. Let K be a
compact subset of Rn. If we write
ω = f(x1, . . . , xn) dx1 ∧· · · ∧dxn,
then we deﬁne the integral as

K
ω =

K
f(x1, . . . , xn) dx1 dx2 · · · dxn,
where the right-hand side represents the usual Riemann integral.
(As pointed out at the beginning of this section, one can also use the
Lebesgue integral instead of the Riemann integral.) Also, if ω is a form
that vanishes outside a compact set K, which is a subset of an open set U,
then we deﬁne

U ω =

K ω.
In order to connect the integration on a manifold M n to integration in
Rn, we must ﬁrst show that this can be done independent of the coordinate
system used.
Lemma 4.4.8.
Let M n be a smooth, oriented manifold with atlas A =
{(Ui, φi)}i∈I. Let K be a compact set with K ∈U1 ∩U2, and let ω be an
n-form that vanishes outside of K. If we set Vi = φi(Ui) for i = 1, 2, then
the following integrals are equal:

V1
(φ−1
1 )∗(ω) =

V2
(φ−1
2 )∗(ω).

164
4. Analysis on Manifolds
Proof: Using the standard notation for transition functions, write Vαβ =
φα(Uα ∩Uβ) and φ12 = φ2 ◦φ−1
1 , a homeomorphism from V12 to V21.
Suppose that in coordinates (x1, . . . , xn) of the (U1, φ1) patch, we write
(φ−1
1 )∗(ω) = f(x1, . . . , xn) dx1 ∧· · · ∧dxn as a form in Rn. Then according
to the usual substitution-of-variables formula for integration, one has
%
V12
f dx1 · · · dxn =
%
V21
f ◦φ21 | det dφ21| d¯x1 · · · d¯xn,
where (¯x1, . . . , d¯xn) are the coordinates over U2. Note that | det dφ21| is the
absolute value of the Jacobian of the change-of-coordinate function φ21.
The integration of forms on a manifold gives
%
V2
(φ−1
2 )∗(ω) =
%
V21
(φ−1
2 )∗(ω) =
%
V21
(φ−1
1 ◦φ21)∗(ω) =
%
V21
φ∗
12((φ−1
1 )∗(ω)).
However, by Equation (4.14), φ∗
21(η) = (det dφ21)η ◦φ21. Furthermore,
by virtue of the manifold being oriented, we know that det(dφ21) > 0, so
det dφ21 = | det dφ21|. Thus,
%
V2
(φ−1
2 )∗(ω) =
%
V21
f ◦φ21| det dφ21| d¯x1 · · · d¯xn =
%
V12
f dx1 · · · dxn
=
%
V12
(φ−1
1 )∗(ω) =
%
V1
(φ−1
1 )∗(ω).
□
This lemma justiﬁes the following deﬁnition in that it is independent of
the choice of coordinate system.
Deﬁnition 4.4.9. Let M be an oriented, smooth n-dimensional manifold. Let
ω be an n-form that vanishes outside of a compact subset K of M, and
suppose that K is also a subset of a coordinate neighborhood (U, φ). Then
we deﬁne the integral as
%
M
ω =
%
φ(U)
(φ−1)∗(ω),
where the right-hand side is an integral of a form over Rn, whose meaning
is given by Deﬁnition 4.4.7.
This deﬁnition explains how to integrate an n-form when it vanishes
outside a compact subset of a coordinate patch. If this latter criterion does
not hold, we use partitions of unity to piece together calculations that fall
under Deﬁnition 4.4.9.

4.4. Integration on Manifolds
165
Deﬁnition 4.4.10. Let M n be an oriented, smooth manifold, and let ω be an
n-form that vanishes outside a compact set. Let {ψi}i∈I be a partition of
unity subordinate to the atlas on M. Deﬁne
%
M
ω =

i∈I
%
M
ψiω
where we calculate each summand on the left using Deﬁnition 4.4.9.
One should notice that the summation only involves a ﬁnite number
of nonzero terms since ω vanishes outside a compact set. The reader may
wonder why we only consider forms that vanish outside of a compact sub-
set of the manifold. This is similar to restricting one’s attention to deﬁnite
integrals in standard calculus courses. Otherwise, one faces improper in-
tegrals and must discuss limits. As it is, many manifolds we consider are
themselves compact; in the context of compact manifolds, the requirement
that ω vanish outside a compact subset is superﬂuous.
As it is, one should now understand the diﬃculty of integrating n-forms
on an n-dimensional manifold. By virtue of the structure of a manifold,
simply to provide a consistent deﬁnition, one is compelled to use a formula
similar to that presented in Deﬁnition 4.4.10. On the other hand, integrals
involving terms such as e−1/x or bump functions as described in Example
4.4.5 are near impossible to compute by hand.
The next proposition outlines some properties of integration of n-forms
on n-dimensional manifolds that easily follow from properties of integration
of functions in Rn as seen in ordinary calculus. However, we ﬁrst give a
lemma that restates the change-of-variables rule in integration over Rn.
Lemma 4.4.11. Let A and B be compact subsets of Rn. Let f : A →B
be a smooth map whose restriction to the interior A◦is a diﬀeomorphism
with the interior B◦. Then on A◦, f is either orientation-preserving or
orientation-reversing on each connected component. Furthermore,
%
B
ω = ±1
%
A
f ∗ω,
where the sign is +1 (respectively, −1) if f is orientation-preserving (re-
spectively, orientation-reversing) over A.
Proof: By the Inverse Function Theorem, f −1 is diﬀerentiable at a point
f(p) if and only if dfp is invertible and if and only if det dfp ̸= 0. Since
each component function in the matrix of dfp is continuous, then det dfp is

166
4. Analysis on Manifolds
a continuous function from A to R. By the Intermediate Value Theorem,
det dfp does not change signs over any connected component of A◦. Thus,
f is orientation-preserving or orientation-reversing on each connected com-
ponent of A◦.
Let (x1, x2, · · · , xn) be a system of coordinates on A ⊂Rn and (y1,
y2, · · · , yn) a system of coordinates on B ⊂Rn. Then we can write ω =
α dy1 ∧· · · ∧dyn for a smooth function α : Rn →R. By Problem 4.3.13,
f ∗ω = α ◦f(det df) dx1 ∧· · · ∧dxn.
Furthermore, according to the change-of-variables formula for integration
in Rn (see [52, Section 16.9, Equations (9) and (13)]) in the usual calculus
notation, we have
%
B
α dy1 dy2 · · · dyn =
%
A
α ◦f | det df| dx1 dx2 · · · dxn.
Therefore, if f is orientation-preserving on A,
%
B
ω =
%
B
α dy1 dy2 · · · dyn =
%
A
α ◦f | det df| dx1 dx2 · · · dxn
=
%
A
α ◦f(det df) dx1 dx2 · · · dxn =
%
A
f ∗ω.
If f is orientation-reversing, the above reasoning simply changes by
| det df| = −det df and a −1 factors out of the integral.
□
Proposition 4.4.12 (Properties of Integration). Let M and N be oriented, smooth
manifolds with or without boundaries. Let ω and η be smooth forms that
vanish outside of a compact set on M.
1. Linearity: For all a, b ∈R,
3
M(aω + bη) = a
3
M ω + b
3
M η.
2. Orientation change: If we denote by (−M) the manifold M but with
the opposite orientation, then
%
(−M)
ω = −
%
M
ω.
3. Substitution rule: If g : N →M is an orientation-preserving diﬀeo-
morphism, then
%
M
ω =
%
N
g∗ω.

4.4. Integration on Manifolds
167
Proof: Part 1 is left as an exercise for the reader.
If M is an oriented manifold with atlas {(Uα, φα)}α∈I, then equipping
M with an opposite orientation means giving a diﬀerent atlas {(Vβ, ˜φβ)}β∈J
such that det d(˜φβ ◦φα) < 0 whenever ˜φβ ◦φα is deﬁned. Following the
proof of Lemma 4.4.8, one can show from the reversal in orientation that
%
(−K)
ω = −
%
K
ω
for any compact set K in any intersection Uα ∩Vβ. Hence, by using ap-
propriate partitions of unity and piecing together the integral according to
Deﬁnition 4.4.10, we deduce part 2 of the proposition.
To prove part 3, assume again that ω is compactly supported in just
one coordinate chart (U, φ) of M. Otherwise, using a partition of unity, we
can write ω as a ﬁnite sum of n-forms, each compactly supported in just
one coordinate neighborhood.
Without loss of generality, suppose that
g−1(U) is a subset of a coordinate chart (V, ψ) on N. Saying that g is
orientation-preserving means that det(φ ◦g ◦ψ−1) > 0. Since g−1(U) ⊂V ,
then V contains the support of g∗ω. Now, by applying Lemma 4.4.11 to
the diﬀeomorphism φ ◦g ◦ψ−1, we have
%
M
ω =
%
φ(U)
(φ−1)∗ω =
%
ψ(V )
(φ ◦g ◦ψ−1)∗(φ−1)∗ω
=
%
ψ(V )
(φ−1 ◦φ ◦g ◦ψ−1)∗ω =
%
ψ(V )
(g ◦ψ−1)∗ω
=
%
ψ(V )
(ψ−1)∗(g∗ω) =
%
N
g∗ω.
□
The following useful proposition combines some of the above properties
and gives a method to calculate integrals of forms on a manifold using
parametrizations while avoiding the use of an explicit partition of unity.
The proposition breaks the calculation into integrals over compact subsets
of Rn, but we need to ﬁrst comment on what types of compact sets we
can allow. We will consider compact sets C ⊂Rn whose boundary ∂C has
“measure 0.” By “measure 0,” we mean
3
∂C 1 dV . More intuitively, we do
not want C to be strange enough that its boundary ∂C has any n-volume.
Proposition 4.4.13. Let M m be a smooth, oriented manifold with or without
boundary. Suppose that there exists a ﬁnite collection {Ci}k
i=1 of compact
subsets of Rm, each with boundary ∂Ci of measure 0, along with a collection
of smooth functions Fi : Ci →M such that: (1) each Fi is a diﬀeomorphism

168
4. Analysis on Manifolds
from the interior C◦
i onto the interior Fi(Ci)o and (2) any pair Fi(Ci) and
Fj(Cj) intersect only along their boundary. Then for any n-form ω on M,
which has a compact support that is contained in F1(C1) ∪· · · ∪Fk(Ck),
%
M
ω =
k

i=1
%
Ci
F ∗
i ω.
Proof: We need the following remarks from set theory and topology. Recall
that for any function f : X →Y and any subsets A, B of Y , we have
f −1(A ∪B) = f −1(A) ∪f −1(B) and f −1(A ∩B) = f −1(A) ∩f −1(B). For
general functions, the same equalities do not hold when one replaces f with
f −1. However, if f is bijective, the equality does hold in both directions.
Let A = {(Uα, φα)}α∈I be the atlas given on M. Let K be the support
of ω. Note that since each Fi is continuous, then Fi(Ci) is compact.
Suppose ﬁrst that K is a subset of a single coordinate chart (U0, φ).
Since K ⊂F1(C1) ∪· · · ∪Fk(Ck),
K = K ∩(F1(C1) ∪· · · ∪Fk(Ck)) = (F1(C1) ∩K) ∪· · · ∪(Fk(Ck) ∩K),
and, again, because φ is a bijection,
φ(K) = (φ ◦F1(C1) ∩φ(K)) ∪· · · ∪(φ ◦Fk(Ck) ∩K) .
(4.15)
Since φ is a homeomorphism and since any pair Fi(Ci) and Fj(Cj) intersect
only along their boundaries, then the same holds for any pair K∩Fi(Ci) and
K ∩Fj(Cj) and also for any pair φ(K)∩(φ◦Fi)(Ci) and φ(K)∩(φ◦Fj)(Cj).
By deﬁnition of integration of n-forms over a coordinate chart, i.e.,
Deﬁnition 4.4.9,
%
M
ω =
%
φ(U)
(φ−1)∗ω =
%
φ(K)
(φ−1)∗ω.
By Equation (4.15) and the theorem on subdividing an integral by nonover-
lapping regions in Rn (see [52, Section 16.3, Equation (9)] for the statement
for integrals over R2),
%
M
ω =
k

i=1
%
φ(K)∩(φ◦Fi)(Ci)
(φ−1)∗ω.
Note that this is precisely where we need to require that the Ci have bound-
aries of measure 0.

4.4. Integration on Manifolds
169
The setup for the proposition was speciﬁcally designed to apply Lemma
4.4.11 to the function φ ◦Fi : Ci →(φ ◦Fi)(Ci) for each i ∈{1, . . ., k}. We
have
%
φ(K)∩φ◦Fi(Ci)
(φ−1)∗ω =
%
F −1
i
(K)∩Ci
(φ ◦Fi)∗(φ−1)∗ω
=
%
F −1
i
(K)∩Ci
F ∗
i ω =
%
Ci
F ∗
i ω,
and the proposition follows for when K is a subset of a single coordinate
chart.
If K is not a subset of a single coordinate chart, we use a partition of
unity subordinate to the atlas of M. In this case, the proposition again fol-
lows, using Proposition 4.3.8(2), so that for each partition-of-unity function
ψj, we have
F ∗
i (ψjω) = (ψj ◦Fi)F ∗
i ω.
□
We are ﬁnally in a position to present an example of integration of
n-forms on a smooth n-manifold.
Example 4.4.14. Consider the 2-torus T = T2 = S1 × S1 embedded in R3 as
the locus of the parametrization
F(u, v) = ((3 + cos v) cos u, (3 + cos v) sin u, sin v)
for (u, v) ∈[0, 2π]2.
We choose an atlas on T so that the function F : (0, 2π)2 →T gives
a parametrization of one coordinate system. Since T = F([0, 2π]2) and
[0, 2π]2 is compact, then we can apply Proposition 4.4.13 to this situation,
with k = 1. Thus, for any 2-form on T ,
%
T
ω =
%
[0,2π]2 F ∗ω.
For example, consider ω = −y dx ∧dz + x dy ∧dz and calculate
3
T ω.
First, we calculate
F ∗(dx ∧dz) = d((3 + cos v) cos u) ∧d(sin v)
= −sin u cosv(3 + cos v) du ∧dv,
F ∗(dy ∧dz) = d((3 + cos v) sin u) ∧d(sin v)
= cos u cos v(3 + cos v) du ∧dv.

170
4. Analysis on Manifolds
Thus,
F ∗ω = −(3 + cos v) sin u F ∗(dx ∧dz) + (3 + cos v) cos u F ∗(dy ∧dz)
= (3 + cos v)2 cos v du ∧dv.
Therefore, we calculate that
%
T
ω =
%
[0,2π]2(3 + cos v)2 cos v du ∧dv
=
% 2π
0
% 2π
0
(3 + cos v)2 cos v du dv
= 2π
% 2π
0
3 cosv + 6 cos2 v + 9 cos3 v dv = 12π2.
Example 4.4.15. Consider the unit sphere S2 in R3 covered by the six coordi-
nate patches described in Example 3.1.5. Adjusting notation to F1 = ⃗X(1)
and F2 = ⃗X(2), one observes that if we use the compact set C1 = C2 as the
closed unit disk {(u, v) | u2 + v2 ≤1}, then the sphere can be covered by
F1(C1) and F2(C2). Thus, we have k = 2 in the setup of Proposition 4.4.13.
Consider the 2-form ω = xz3 dy ∧dz on S2, with the x, y, z representing
the coordinates in R3. We have
F1(u, v) = (u, v,

1 −u2 −v2)
and
F2(u, v) = (u, v, −

1 −u2 −v2),
so we calculate that
F ∗
1 ω = u(1 −u2 −v2)3/2 dv ∧

−
u
√
1 −u2 −v2 du −
v
√
1 −u2 −v2 dv
	
= u2(1 −u2 −v2) du ∧dv,
and similarly, F ∗
2 ω = u2(1 −u2 −v2) du ∧dv. Then by Proposition 4.4.13,
%
S2 ω =
%
C1
F ∗
1 ω +
%
C2
F ∗
2 ω = 2
%
C1
u2(1 −u2 −v2) du dv.
Putting this in polar coordinates, we get
%
S2 ω = 2
% 2π
0
% 1
0
r2 cos2 θ(1 −r2)r dr dθ = 2π
% 1
0
r3 −r5 dr = π
6 .
The above two examples actually illustrate a special case of a particular
situation. Both the sphere S2 and the torus T2 are (embedded) submani-
folds of R3. The examples motivate the following deﬁnition.

4.4. Integration on Manifolds
171
Deﬁnition 4.4.16 (Integration on Submanifolds). If M is an immersed subman-
ifold of dimension m with the immersion f : M m →N n and if ω ∈Ωm(N),
then we deﬁne
%
f(M)
ω =
%
M
f ∗ω.
A particular case of this deﬁnition is a line integral.
Deﬁnition 4.4.17. Let γ : [a, b] →M be a smooth curve, and let ω be a
1-form on M. We deﬁne the line integral of ω over γ as
%
γ
ω =
%
[a,b]
γ∗ω.
In addition, if γ is a piecewise-smooth curve, we deﬁne
%
γ
ω =
k

i=1
%
[ci−1,ci]
γ∗ω,
where [ci−1, ci], with i = 1, . . . , k, are the smooth arcs of γ.
At the beginning of this section, we proposed to ﬁnd a deﬁnition of
integration that generalizes many common notions from standard calculus.
We explain now how the above two deﬁnitions generalize the concepts of
line integrals in Rn and integrals of vector ﬁelds over surfaces.
Consider ﬁrst the situation of line integrals in R3. (The case for Rn
is identical in form.) In vector calculus (see [52, Deﬁnition 17.2.13]), one
considers a continuous vector ﬁeld ⃗F : R3 →R3 deﬁned over a smooth
curve ⃗γ : [a, b] →R3. Then one deﬁnes the line integral as
%
⃗γ
⃗F · d⃗r =
% b
a
⃗F(⃗γ(t)) · ⃗γ′(t) dt.
To connect the classic line integral to the line integral in our present for-
mulation, set ω = F1 dx + F2 dy + F3 dz, where ⃗F = (F1, F2, F3). If we
write γ(t) = ⃗γ(t) = (γ1(t), γ2(t), γ3(t)), then
γ∗ω = F1(γ(t))d(γ1) + F2(γ(t))d(γ2) + F3(γ(t))d(γ3)
=

F1(γ(t))(γ1)′(t) + F2(γ(t))(γ2)′(t) + F3(γ(t))(γ3)′(t)

dt
= ⃗F(⃗γ(t)) · ⃗γ′(t) dt.
Thus, we have shown that the classic and modern line integrals are equal via
%
γ
ω =
% b
a
γ∗ω =
%
⃗γ
⃗F · d⃗r.

172
4. Analysis on Manifolds
Second, consider the situation for surface integrals. In vector calculus
(see [52, Deﬁnitions 17.7.8 and 17.7.9]), one considers a continuous vector
ﬁeld ⃗F : R3 →R3 deﬁned over an oriented surface S parametrized by
⃗r : D →R3, where D is a compact region in R2. If (u, v) are the variables
used in D, then
%%
S
⃗F · d⃗S =
%%
D
⃗F(⃗r(u, v)) · (⃗ru × ⃗rv) dA.
To demonstrate the connection with the modern formulation, if we write
⃗F = (F1, F2, F3), then set
ω = F1η1 + F2η2 + F3η3,
where ηj are the 2-forms described in Equation (4.10). Set also f(u, v) =
⃗r(u, v), and write f = (f 1, f 2, f 3) as component functions in R3. Then
f ∗ω = F1(f(u, v))f ∗η1 + F2(f(u, v))f ∗η2 + F3(f(u, v))f ∗η3.
(4.16)
But we calculate that
f ∗η1 = f ∗(dx2 ∧dx3) = df 2 ∧df 3
=
∂f 2
∂u du + ∂f 22
∂v
dv
	
∧
∂f 3
∂u du + ∂f 3
∂v dv
	
=
∂f 2
∂u
∂f 3
∂v −∂f 2
∂v
∂f 3
∂u
	
du ∧dv.
Repeating similar calculations for f ∗η2 and f ∗η3 and putting the results
in Equation (4.16), we arrive at
f ∗ω = ⃗F(⃗r(u, v)) · (⃗ru × ⃗rv) du ∧dv.
Using Deﬁnition 4.4.16 for the integration on a submanifold, we conclude
that
%
S
ω =
%
D
f ∗ω =
%%
S
⃗F · d⃗S,
thereby showing how integration of 2-forms on a submanifold gives the
classical surface integral.
It is interesting to observe how the integration of forms on manifolds
and on submanifolds of a manifold generalizes simultaneously many of the
integrals that are studied in classic calculus, which are in turn studied for
their applicability to science. However, the reader who has been check-
ing oﬀthe list at the beginning of this section of types of integration we

4.4. Integration on Manifolds
173
proposed to generalize might notice that until now we have not provided
generalizations for path integrals
3
C f ds or integrals of scalar functions
over a surface
3
S f dA. The reason for this is that these integrals involve
an arclength element ds or a surface area element dA.
However, given
a smooth manifold M without any additional structure, there is no way
to discuss distances, areas, or n-volumes on M. Riemannian manifolds,
which we introduce in the next chapter, provide a structure that allows us
to make geometric calculations of length and volume. In that context, one
can easily deﬁne generalizations of path integrals and integrals of scalar
functions over a surface.
Before moving on to applications to physics, we mention a special case
where the line integral is easy to compute.
Theorem 4.4.18 (Fundamental Theorem for Line Integrals). Let M be a smooth
manifold, let f : M →R be a smooth function, and let γ : [a, b] →M be a
piecewise-smooth curve on M. Then
%
γ
df = f(γ(a)) −f(γ(b)).
Proof: By Proposition 4.3.8(3), γ∗(df) = d(γ∗f), so we have
γ∗(df) = d(γ∗f) = d(f ◦γ) = (f ◦γ)′(t) dt,
where t is the variable on the manifold [a, b]. Thus,
%
γ
df =
%
[a,b]
γ∗(df) =
% b
a
(f ◦γ)′(t) dt = f(γ(a)) −f(γ(b)).
□
4.4.3
Conservative Vector Fields
We now wish to look at a central topic from elementary physics through
the lens of our theory of integration on a manifold.
In elementary physics, one of the ﬁrst areas one studies is the dynamics
of a particle under the action of a force or force ﬁeld. We remind the reader
of some basic facts from physics. Suppose a particle of constant mass m
is acted upon by a force ⃗F (which may depend on time and space) and
follows a trajectory parametrized by ⃗r(t). Writing ⃗v = ⃗r ′ for the velocity
and v = ∥⃗v∥for the speed of the particle, one deﬁnes the kinetic energy by
T = 1
2mv2. Furthermore, since m is constant, according to Newton’s law

174
4. Analysis on Manifolds
of motion, ⃗F = m⃗v ′. Finally, as the particle travels for t1 ≤t ≤t2, one
deﬁnes the work done by ⃗F as the line integral
W =
% t2
t1
⃗F · d⃗r.
The kinetic energy T depends on time, so we have
dT
dt = d
dt
1
2m⃗v · ⃗v
	
= md⃗v
dt · ⃗v.
Thus, as a particle moves along ⃗r(t) for t1 ≤t ≤t2, the change in kinetic
energy is
T2 −T1 = T (t2) −T (t1) =
% t2
t1
⃗F · ⃗v dt =
% t2
t1
⃗F · d⃗r = W.
(4.17)
Thus, the change in kinetic energy is equal to the work done by the external
forces. This result is often called the Energy Theorem. A force is called
conservative if it does not depend on time and, if, as a particle travels over
any closed, piecewise, smooth curve the kinetic energy does not change.
Though in physics one simply speaks of vector ﬁelds, because of the
transformational properties under a coordinate change, one should in fact
usually understand them as covector ﬁelds, i.e., as either 1-forms or 2-forms.
The possible confusion between whether a given “vector ﬁeld” should be
understood as a 1-form or a 2-form stems from the fact that over a smooth
manifold of dimension 3, for each p ∈M, 11 TpM ∗, 12 TpM ∗, and TpM
are all isomorphic as vector spaces.
Deﬁnition 4.4.19. A 1-form (covector ﬁeld) on a smooth manifold M is called
conservative if
%
γ
ω = 0
for all closed, piecewise-smooth curves γ on M.
This deﬁnition has a diﬀerent and perhaps more useful characterization.
If γ1 and γ2 are two piecewise-smooth paths from points p1 to p2, then the
path γ1 • (−γ2) deﬁned by ﬁrst traveling from p1 to p2 along γ1 and then
traveling backwards from p2 to p1 along γ2 is a closed, piecewise-smooth
curve (see Figure 4.7). It is not hard to show that for any 1-form ω,
%
γ1•(−γ2)
ω =
%
γ1
ω +
%
(−γ2)
ω =
%
γ1
ω −
%
γ2
ω.

4.4. Integration on Manifolds
175
p1
p2
γ1
γ2
Figure 4.7. Two paths between p1 and p2.
Hence, a covector ﬁeld ω is conservative if and only if the integral of ω
between any two points p1 and p2 is independent of the path between
them.
A smooth 1-form has another alternative characterization, whose proof
we leave as an exercise for the reader.
Theorem 4.4.20. Let M be any smooth manifold. A 1-form ω ∈Ω1(M) is
conservative if and only if ω is exact.
Returning to physics in Euclidean R3, according to the Energy Theo-
rem from Equation (4.17), a force is conservative if and only if the work
done over a piecewise-smooth path between any two points p1 and p2 is
independent of the path chosen. Thus, if ⃗F is conservative, one deﬁnes the
potential energy by
V (x, y, z) = −
% (x,y,z)
(x0,y0,z0)
⃗F · d⃗r.
where (x0, y0, z0) is any ﬁxed point. Obviously, the potential energy of ⃗F
is a function that is well deﬁned only up to a constant that corresponds to
the selected origin point (x0, y0, z0). It is easy to check that
⃗F = −∇V.
For a conservative force ⃗F with potential energy V , the work of ⃗F as
the particle travels along ⃗r(t) for t ∈[t1, t2] is
W =
% t2
t1
⃗F · d⃗r = −
% t2
t1
∇V · d⃗r = −(V (⃗r(t2)) −V (⃗r(t1))) = −(V2 −V1).
Hence, the Energy Theorem can be rewritten as
T1 + V1 = T2 + V2.

176
4. Analysis on Manifolds
The sum T + V of kinetic and potential energy is often referred to simply
as the energy or total energy of a particle. This justiﬁes the terminology
“conservative”: the total energy of a particle moving under the action of a
conservative force is conserved along any path.
Problems
4.4.1. Let γ be the curve in R4 parametrized by γ(t) = (1 + t2, 2t −1, t3 −4t, 1
t )
for t ∈[1, 3]. Let ω = x dy+(y2+z) dz+xw dw. Calculate the line integral
3
γ ω.
4.4.2. Calculate the line integral
3
γ ω, where γ is the triangle in R3 with vertices
(0, 1, 2), (1, 2, 4), and (−3, 4, −2) and where ω is the 1-form given in R3 by
ω = (2xy + 1) dx + 3x dy + yz dz.
4.4.3. Evaluate
3
M ω, where M is the portion of paraboloid in R3 given by z =
9−x2 −y2 above the xy-plane and where ω is the 2-form ω = y2 dx ∧dy +
z2 dx ∧dz + 2 dy ∧dz.
4.4.4. Let T2 be the torus embedded in R4 that is given by the equations x2+y2 =
z2 + w2 = 1. Note that the ﬂat torus can be parametrized by
⃗X(u, v) = (cos u, sin u, cos v, sin v)
for appropriate u and v. Compute the integral
3
T2 ω, where ω is the 2-form
in R2 given by
(a) ω = x3 dy ∧dw;
(b) ω = x3z dy ∧dw;
(c) ω = (xyz + 1) dx ∧dy + exyz dx ∧dw.
4.4.5. Consider the unit sphere M = S2 embedded in R3. Let
ω = z2 dx ∧dy + x dx ∧dz + xy dy ∧dz
x2 + y2 + z2
be a 2-form pulled back to S2. Calculate directly the integral
3
M ω using
(a) the latitude-longitude parametrization ⃗r(u, v)=(cos u sin v, sin u sin v,
cos v);
(b) the stereographic parametrizations {πn, ¯πS} deﬁned in Problem 3.2.4
and Example 3.1.16. [Hint: Use two coordinate patches.]
4.4.6. Consider the 3-torus described in Problem 4.2.6. Calculate
3
M ω, where
(a) ω = (cos2 v + (1 + sin w)/(2 + cos u)) du ∧dv ∧dw given in local
coordinates;
(b) ω = x1 dx1 ∧dx2 ∧dx3 + x2 dx2 ∧dx3 ∧dx4 in coordinates in R4.

4.5. Stokes’ Theorem
177
4.4.7. Prove part 1 of Proposition 4.4.12.
4.4.8. The force exerted by an electric charge placed at the origin on a charged
particle is given by the force ﬁeld ⃗F(⃗r) = K⃗r/∥⃗r∥3, where K is a constant
and ⃗r = (x, y, z) is the position vector of the charged particle.
(a) Calculate the work exerted by the force on a charged particle that
travels along the straight line from (3, −1, 2) to (4, 5, −1).
(b) Prove that ⃗F is a conservative force.
(c) If we write ⃗F = (F1, F2, F3) in component functions, then set ω =
F1η1 +F2η2 +F3η3. Calculate
3
M ω, where M is the sphere of radius
R in R3.
4.4.9. Let T2 be the 2-torus embedded in R3, using the parametrization given in
Example 4.4.14. Show by direct calculation that
%
T2 ω = 0
where ω is the 2-form described in Problem 4.4.8.
4.4.10. Let M be a smooth, oriented manifold. Referring to Example 4.4.20, prove
that a smooth (co)vector ﬁeld ω on M is conservative if and only if ω is
exact.
4.5
Stokes’ Theorem
In the last section of this chapter, we present Stokes’ Theorem, a central
result in the theory of integration on manifolds.
In multivariable calculus, one encounters a theorem by the same name.
What is called Stokes’ Theorem for vector ﬁelds in R3 states that if S is
an oriented, piecewise-smooth surface that is bounded by a simple closed,
piecewise-smooth curve C, then for any C1 vector ﬁeld ⃗F deﬁned over an
open region that contains S,
%
C
⃗F · d⃗r =
%
S
(∇× ⃗F) · d⃗S.
(See [52, Section 17.8].)
It is a striking result that the generalization of this theorem to the
context of manifolds simultaneously subsumes the Fundamental Theorem
of Integral Calculus, Green’s Theorem, the classic Stokes’ Theorem, and
the Divergence Theorem.
Before giving the theorem, we state a convention for what it means
to integrate a 0-form on an oriented, zero-dimensional manifold.
If N

178
4. Analysis on Manifolds
is an oriented zero-dimensional manifold, then N = {p1, p2, . . . , pc} is a
discrete set of points equipped with an association of signs si = ±1 for
each i = 1, . . . , c. Then by convention for any 0-form f (i.e., a function
on N),
%
N
f =
c

i=1
sif(pi).
(4.18)
Theorem 4.5.1 (Stokes’ Theorem). Let M n be a smooth, oriented manifold with
or without boundary, and let ω be an (n −1)-form that is compactly sup-
ported on M.
If one equips ∂M with the induced orientation, then the
following integrals are equal
%
M
dω =
%
∂M
ω,
(4.19)
where on the right side we take ω to mean the restriction of ω to ∂M. If
∂M = ∅, we understand the right side as 0.
Proof: We ﬁrst treat the case where n > 1.
Suppose ﬁrst that ω is compactly supported in a single coordinate chart
(U, φ). Then by the deﬁnition of integration and by Proposition 4.3.8,
%
M
dω =
%
Rn
+
(φ−1)∗dω =
%
Rn
+
d

(φ−1)∗ω

.
Using the (n−1)-forms ηj deﬁned in Equation (4.10) as a basis for Ωn(Rn
+),
write (φ−1)∗ω = "n
j=1 ωjηj. Then, for the exterior diﬀerential, we have
d

(φ−1)∗ω

=
n

j=1
 n

i=1
∂ωj
∂xi dxi

∧ηj
=
 n

i=1
∂ωi
∂xi

dx1 ∧· · · ∧dxn,
where the second equality follows from Equation (4.11).
Since ω is compactly supported in U, then for large enough R, the
component functions ωi(x1, . . . , xn) vanish identically outside the paral-
lelepiped
DR = [−R, R] × · · · × [−R, R]
(
)*
+
n−1
× [0, R].

4.5. Stokes’ Theorem
179
Therefore, we remark that for all i = 1, . . . , n −1, we have
% R
−R
∂ωi
∂xi dxi = [ωi(x)]xi=R
xi=−R = 0.
(4.20)
Consequently, we deduce that
%
M
dω =
% R
0
% R
−R
· · ·
% R
−R
 n

i=1
∂ωi
∂xi

dx1 · · · dxn−1 dxn
=
n

i=1
% R
0
% R
−R
· · ·
% R
−R
∂ωi
∂xi dx1 · · · dxn
=
n−1

i=1
% R
0
% R
−R
· · ·
% R
−R
% R
−R
∂ωi
∂xi dxi

dx1 · · · &
dxi · · · dxn
+
% R
−R
· · ·
% R
−R
% R
0
∂ωn
∂xn dxn

dx1 · · · dxn−1
=
% R
−R
· · ·
% R
−R
[ωn(x)]xn=R
xn=0 dx1 · · · dxn−1
by Equation (4.20)
= −
% R
−R
· · ·
% R
−R
ωn(x1, . . . , xn−1, 0) dx1 · · · dxn−1,
(4.21)
where the last equality holds because ωn(x1, . . . , xn−1, R) = 0. Note that if
the support of ω does not meet the boundary ∂M, then ωn(x1, . . . , xn−1, 0)
is also identically 0 and 3
M ω = 0.
To understand the right-hand side of Equation (4.19), let i : ∂M →M
be the embedding of the boundary into M. The restriction of ω to ∂M
is i∗(ω).
Furthermore, in coordinates in (U, φ), i∗(dxk) = dxk if k =
1, . . . , n −1 and i∗(dxn) = 0. Hence, i∗(ηj) = 0 for all j ̸= n. Thus, in
coordinates,
i∗(ω) = ωn(x1, · · · , xn−1, 0)ηn
= (−1)n−1ωn(x1, · · · , xn−1, 0) dx1 ∧· · · ∧dxn−1.
However, by Deﬁnition 3.3.12 for the orientation induced on the boundary
of a manifold, we have
%
∂M
a(x) dx1 ∧· · · ∧dxn−1 = (−1)n
%
Rn−1 a(x) dx1 · · · dxn−1

180
4. Analysis on Manifolds
for the (n −1)-form a dx1 ∧· · · ∧dxn−1. Thus,
%
∂M
ω =
%
∂M
i∗(ω)
=
%
DR∩{xn=0}
(−1)n−1ωn(x1, · · · , xn−1, 0) dx1 ∧· · · ∧dxn−1
= −
% R
−R
· · ·
% R
−R
ωn(x1, . . . , xn−1, 0) dx1 · · · dxn−1,
which by Equation (4.21) is equal to
3
M dω. This proves Equation (4.19)
for the case when ω is supported in a compact subset of a single coordinate
patch.
Suppose now that ω is supported over a compact subset K of M that is
not necessarily a subset of any particular coordinate patch in the atlas A =
{(Uα, φα)} for M. Then we use a partition of unity {ψα} that is subordinate
to A.
Since K is compact, we can cover it with a ﬁnite collection of
coordinate patches {(Ui, φi)}k
i=1. Then
%
∂M
ω =
k

i=1
%
∂M
ψiω =
k

i=1
%
M
d(ψiω)
by application of Stokes’ Theorem for each form ψiω that is supported over
a compact set in the coordinate patch Ui. But d(ψiω) = dψi ∧ω +ψidω, so
%
∂M
ω =
k

i=1
%
M
(dψi ∧ω + ψidω)
=
k

i=1
%
M
dψi ∧ω +
k

i=1
%
M
ψidω
=
%
M
d
 k

i=1
ψi

∧ω +
k

i=1
%
M
ψidω
=
%
M
d(1) ∧ω +
k

i=1
%
M
ψidω
= 0 +
k

i=1
%
M
ψidω
=
%
M
dω.
This establishes Stokes’ Theorem for n > 1.

4.5. Stokes’ Theorem
181
Now consider the case of a 1-manifold M; then ∂M is a zero-dimensional
manifold. The 0-form ω is simply a real-valued function on M. Now for a
compact set K contained in a coordinate system φ : U →R+ on M, the
intersection K∩∂M is either empty or consists of a single point {pi}. Thus,
with the assumption that ω is supported over a compact set contained in
a coordinate patch of M, we conclude that

M
df = f(pi)si
simply by the usual Fundamental Theorem of Integral Calculus. By the
convention in Equation (4.18) for integration on a zero-dimensional man-
ifold, we also have

∂M f = f(pi)si. Utilizing a partition of unity when
ω is not assumed to be supported in a single coordinate patch, one also
immediately recovers Stokes’ Theorem.
□
Two cases of Stokes’ Theorem occur frequently enough to warrant spe-
cial emphasis. The proofs are implicit in the above proof of Stokes’ Theo-
rem.
Corollary 4.5.2. If M is a smooth manifold without boundary and ω is a
smooth (n −1)-form, then

M
dω = 0.
Corollary 4.5.3. If M is a smooth manifold with or without boundary and ω
is a smooth (n −1)-form that is closed (i.e., dω = 0), then

∂M
ω = 0.
The convention for integrating 0-forms on a zero-dimensional manifold
allows Stokes’ Theorem to directly generalize the Fundamental Theorem
of Calculus in the following way.
Consider the interval [a, b] as a one-
dimensional manifold M with boundary with orientation of displacement
from a to b. Then ∂M = {a, b} with an orientation of −1 for a and +1 for b.
A 0-form on M is a smooth function f : [a, b] →R. Then Theorem 4.5.1
simply states that

[a,b]
df =
 b
a
f ′(x) dx = f(b) −f(a),
which is precisely the Fundamental Theorem of Calculus.

182
4. Analysis on Manifolds
(The careful reader might remark that, as stated, Stokes’ Theorem on
manifolds only generalizes the Fundamental Theorem of Calculus (FTC)
when f is a smooth function, whereas most calculus texts only presup-
pose that f is C1 over [a, b]. The history behind the FTC and the search
for exactly which functions satisfy the FTC formula is long. In fact, the
work in analysis related to the FTC in part motivated the deﬁnition of
the Lebesgue integral and also loosened the conditions on what properties
f had to satisfy.
See [12] for an excellent historical account.
Since we
restricted our attention to smooth manifolds and smooth functions, these
technical details are moot.)
It is possible to generalize Stokes’ Theorem even further by includ-
ing manifolds that are piecewise-smooth or manifolds with corners. (One
should be familiar with these situations from the classic Stokes’ Theorem
and the Divergence Theorem.) However, to present these generalizations
properly one must deﬁne smoothness in these contexts and discuss orien-
tations on such objects. For the sake of space, we refrain from developing
that topic.
Problems
4.5.1. Explicitly show how Stokes’ Theorem on manifolds directly generalizes
Stokes’ Theorem from standard multivariable calculus.
4.5.2. Use Stokes’ Theorem to evaluate
3
S dω, where S is the image in R4 of the
parametrization
r(u, v) = (1 −v)(cos u, sin u, sin 2u, 0) + v(2, cos u, sin u, sin 2u)
and where ω = x2 dx1 + x3 dx2 + x4 dx3 −x1 dx4.
4.5.3. Let B4 = {x ∈R4 | ∥x∥≤1} be the unit ball in R4, and note that ∂B4 = S3.
We use the coordinates (x, y, z, w) in R4 and hence in B4. Use Stokes’
Theorem to evaluate
%
S3(exy cos w dx ∧dy ∧dz + x2z dx ∧dy ∧dw).
4.5.4. Let M be a compact, oriented, n-manifold, and let ω ∈Ωj(M) and η ∈
Ωk(M), where j + k = n −1. Suppose that η vanishes on the boundary
∂M or that ∂M = ∅. Show that
%
M
ω ∧dη = (−1)j−1
%
M
dω ∧η.
4.5.5. Let M be a compact, oriented n-manifold. Let ω and η be forms of rank
j and k respectively, such that j + k = n −2. Show that
%
M
dω ∧dη =
%
∂M
ω ∧dη.

4.5. Stokes’ Theorem
183
Explain how this generalizes the well-known result in multivariable calcu-
lus that
%
C
(f∇g) · d⃗r =
%%
S
(∇f × ∇g) · d⃗S,
where S is a regular surface in R3 with boundary C and where f and
g are real-valued functions that are deﬁned and have continuous second
derivatives over an open set containing S.
4.5.6. Integration by Parts on a Curve. Let M be a compact and connected one-
dimensional smooth manifold. Let f, g : M →R be two smooth functions
on the curve M.
Show that ∂M consists of two discrete points {p, q}.
Suppose that M is oriented so that the orientation induced on ∂M is −1
for p and +1 for q. Show that
%
M
f dg = f(q)g(q) −f(p)g(p) −
%
M
g df.
4.5.7. Let M be an embedded submanifold of Rn of dimension n −1. Suppose
that M encloses a compact region R. Setting ω =
1
n("n
i=1 xiηi), where
the ηj are deﬁned by Equation (4.10), show that the n-volume of R is
3
M ω.
4.5.8. Consider M = Rn −{(0, . . . , 0)} as a submanifold of Rn, and let Sn−1 be
the unit sphere in Rn centered at the origin.
(a) Show that if ω ∈Ωn−1(M) is exact, then
3
Sn−1 ω = 0.
(b) Find an example of a closed form ω
∈
Ωn−1(M) such that
3
Sn−1 ω ̸= 0.


CHAPTER
5
Introduction to
Riemannian Geometry
To recapitulate what we have done in the past two chapters, manifolds
are topological spaces that are locally homeomorphic to a Euclidean space
Rn in which one could do calculus.
Chapter 4 introduced the analysis
on manifolds by connecting it to analysis on Rn via coordinate charts.
However, the astute reader might have noticed that our presentation of
analysis on manifolds so far has not recovered one of the foundational
aspects of Euclidean calculus: the concept of distance. And related to the
concept of distance are angles, areas, volumes, curvature, etc.
In the local theory of regular surfaces S in R3, the ﬁrst fundamental
form (see Example 4.2.3) allows one to calculate the length of curves on
a surface, the angle between two intersecting curves, and the area of a
compact set on S (see [5, Section 6.1] for details).
This should not be
surprising: we deﬁned the ﬁrst fundamental form on S as the restriction
of the usual Euclidean dot product in R3 to the tangent space Tp(S) for
any given point p ∈S, and the dot product is the basis for measures of
distances and angles in R3.
In general, manifolds are not given as topological subspaces of Rn so
one does not immediately have a ﬁrst fundamental form as we deﬁned in
Example 4.2.3. Furthermore, from the deﬁnition of a diﬀerential manifold,
it is not at all obvious that it has a metric (though we will see in Propo-
sition 5.1.8 that every smooth manifold has a metric structure). Conse-
quently, one must equip a manifold with a metric structure, which we will
call a “Riemannian structure.” Applications of manifolds to geometry and
curved space in physics will require this additional metric structure.
As in many mathematics text, our treatment of manifolds and Rieman-
nian metrics does not emphasize how long it took these ideas to develop
nor have the previous two chapters followed the historical trajectory of the
subject. After the discovery of non-Euclidean geometries (see [10] for a
good historical discussion), by using only an intuitive notion of a manifold,
185

186
5. Introduction to Riemannian Geometry
it was Riemann [45, Section II] in 1854 who ﬁrst proposed the idea of a
metric that varied at each point of a manifold. During the following 50
or more years, many mathematicians (Codazzi, Beltrami, Ricci-Curbastro,
Levi-Civita, and Klein, to name a few) developed the theories of curvature
and of geodesy for Riemann spaces. However, the concept of a diﬀerential
manifold as presented in Chapter 3 did not appear until 1913 in the work of
H. Weyl [56, I.§4]. According to Steenrod [50, p. v], general deﬁnitions for
ﬁber bundles and vector bundles, which we introduced in part in Chapter 4,
did not appear until the work of Whitney in 1935–1940.
Turning to physics, general relativity, one of the landmark achieve-
ments in science of the early twentieth century, stands as the most visible
application of Riemann manifolds to science.
Starting from the princi-
ple that the speed of light in a vacuum is constant regardless of reference
frame [20, p. 42], Einstein developed the theory of special relativity, de-
ﬁned in the absence of gravity. The “interpretation” of the law that “the
gravitational mass of a body is equal to its inertial mass” [20, p. 65] and
the intention to preserve the principle of the constancy of the speed of
light led Einstein to understand spacetime as a curved space where “the
geometrical properties of space are not independent, but . . . determined by
matter” [20, p. 113]. Riemannian metrics, curvature, and the associated
theorems for geodesics gave Einstein precisely the mathematical tools he
needed to express his conception of a curvilinear spacetime.
The reader should be aware that other applications of manifolds to
science do not (and should not) always require a metric structure. Appli-
cations of manifolds to either geometry or physics may require a diﬀerent
structure from or additional structure to a Riemann metric. For exam-
ple, in its properly generalized context, Hamiltonian mechanics require the
structure of what is called a symplectic manifold.
5.1
Riemannian Metrics
5.1.1
Deﬁnitions and Examples
Deﬁnition 5.1.1. Let M be a smooth manifold. A Riemannian metric on M
is a tensor ﬁeld g in Sym2 T M ∗that is positive deﬁnite. In more detail, at
each point p ∈M, a Riemannian metric determines a symmetric bilinear
inner product on TpM that is positive deﬁnite (i.e., g(X, X) > 0 if X ̸= 0).
A smooth manifold M together with a Riemannian metric g is called a
Riemannian manifold and is denoted by the pair (M, g).

5.1. Riemannian Metrics
187
Over a coordinate patch of M with coordinate system (x1, . . . , xn), as
a section of T M ∗⊗2, one writes the metric g as
gij dxi ⊗dxj,
where gij are smooth functions on M. (In this chapter, we regularly use
Einstein’s summation convention.) Since at each point, g is a symmetric
tensor, gij = gji identically. Furthermore, using the notation from Ap-
pendix Section C.5, since g is a section of Sym2 TpM ∗, we write
gij dxi dxj.
(5.1)
The square root of the expression in Equation (5.1) is called the line element
ds associated to this metric. Many texts, in particular, physics texts, give
the metric in reference to the line element by writing ds2 = gij dxi dxj.
For vectors X, Y ∈TpM, we sometimes use the same notation as the
ﬁrst fundamental form and write ⟨X, Y ⟩p for gp(X, Y ), and it is also com-
mon to drop the subscript p whenever the point p is implied by context. By
analogy with the dot product, the Riemannian metric allows one to deﬁne
many common notions in geometry.
Deﬁnition 5.1.2. Let (M, g) be a Riemannian manifold.
Suppose that X
and Y are vectors in TpM.
1. The length of X, denoted ∥X∥, is deﬁned by ∥X∥=

g(X, X).
2. The angle θ between X and Y is deﬁned by cos θ = g(X, Y )
∥X∥∥Y ∥.
3. X and Y are called orthogonal if g(X, Y ) = 0.
Whenever one introduces a new mathematical structure, one must dis-
cuss functions between them and when two structures are considered equiv-
alent. In the context of Riemannian manifolds, one still studies any smooth
functions between two manifolds. However, two Riemannian manifolds are
considered the same if they have the same metric. The following deﬁnition
makes this precise.
Deﬁnition 5.1.3. Let M and N be two Riemannian manifolds. A diffeomor-
phism f : M →N is called an isometry if for all p ∈M,
⟨X, Y ⟩p = ⟨dfp(X), dfp(Y )⟩f(p)
for all X, Y ∈TpM.
Two Riemannian manifolds are called isometric if there exists an isometry
between them.

188
5. Introduction to Riemannian Geometry
Figure 5.1. Bending the catenoid into the helicoid.
From an intuitive perspective, an isometry is a transformation that
bends (which also includes rigid motions) one manifold into another with-
out stretching or cutting. Problem 5.1.5 asks the reader to show that the
catenoid and the helicoid are isometric.
Figure 5.1 shows intermediate
stages of bending the catenoid into the helicoid. Though one might think
this transformation incorporates some stretching because the longitudinal
lines straighten out, the twist created in the helicoid strip “balances out”
the ﬂattening of the lines in just the right way so that one only needs to
bend the surface.
Many examples of Riemannian metrics arise naturally as submanifolds
of Riemannian manifolds.
Deﬁnition 5.1.4. Let (N, ˜g) be a Riemannian manifold and M any smooth
manifold. Let f : M →N be an immersion of M into N, i.e., f is diﬀer-
entiable and dfp is injective for all p. The metric g on M induced by f (or
“from N”) is deﬁned as the pull-back g = f ∗˜g. In other words,
⟨X, Y ⟩p = ⟨dfp(X), dfp(Y )⟩f(p)
for all p ∈M and X, Y ∈TpM.
Note that the property that dfp is injective ensures that ⟨, ⟩p is positive
deﬁnite.
Example 5.1.5 (Euclidean Spaces).
Consider the manifold M = Rn, where
Tp(Rn) = Rn is naturally equipped with a Riemannian metric: the usual
dot product. In particular,
g (∂i, ∂j) = δij =

1,
if i = j,
0,
if i ̸= j.
This metric is called the Euclidean metric.

5.1. Riemannian Metrics
189
Example 5.1.6 (First Fundamental Form). A regular surface S is a manifold em-
bedded in R3, where the embedding map is simply the injection i : S →R3.
The ﬁrst fundamental form (see Example 4.2.3) is precisely the metric on
S induced by i from the Euclidean metric on R3. This connection gives us
immediately a whole host of examples of Riemannian 2-manifolds that we
take from the local theory of regular surfaces.
Proposition 5.1.7. Let M be an m-dimensional manifold embedded in Rn. If
⃗F(u1, . . . , um) is a parametrization of a coordinate patch of M, then over
this coordinate patch, the coeﬃcients of the metric g on M induced from
Rn are
gij = ∂⃗F
∂ui · ∂⃗F
∂uj .
Proof: Let (x1, . . . , xn) be the coordinates on Rn. Suppose that a coordi-
nate patch (U, φ) of M has coordinates (u1, . . . , um) and that ⃗F(u1, . . . , um)
= φ−1(u1, . . . , um) is a parametrization of this coordinate patch. By Equa-
tion (3.6) the matrix of d⃗F in the given coordinate systems is
∂F i
∂uj
	
,
where ⃗F = (F 1, . . . , F n).
Set ⟨, ⟩as the usual dot product in Rn. Then at each point in U, the
coeﬃcients gkl of the metric g satisfy
gkl = g
 ∂
∂uk , ∂
∂ul
	
=
4
d⃗F
 ∂
∂uk
	
, d⃗F
 ∂
∂uk
	5
= ∂F i
∂uk
∂F j
∂ul
4 ∂
∂xi , ∂
∂xj
5
= ∂F i
∂uk
∂F j
∂ul δij = ∂⃗F
∂uk · ∂⃗F
∂ul .
□
We should emphasize at this point that a given manifold can be equipped
with nonisometric Riemannian metrics. Problem 5.1.1 presents two diﬀer-
ent metrics on the 3-torus, each depending on a diﬀerent embedding into
some Euclidean space. In both cases, the 3-torus can be equipped with the
same atlas, and so in both situations, the 3-torus is the same as a smooth
manifold.
As another example, already in his seminal dissertation [45], Riemann
introduced the following metric on the open unit ball in Rn:
gii =
1
1 −∥x∥2
and
gij = 0 if i ̸= j.

190
5. Introduction to Riemannian Geometry
As we will see, this is not isometric with the open unit ball equipped with
the Euclidean metric.
Example 5.1.5 could be misleading in its simplicity. The reader might
consider the possibility of deﬁning a metric on any smooth manifold M by
taking ⟨, ⟩p as the usual dot product in each TpM with respect to the co-
ordinate basis associated to a particular coordinate system. The problem
with this idea is that it does not deﬁne a smooth section in Sym2 T M ∗
over the whole manifold. Nonetheless, as the proof of the following propo-
sition shows, we can use a partition of unity and stitch these bilinear forms
together.
Proposition 5.1.8. Every smooth manifold M has a Riemannian metric.
Proof: Let M be a smooth manifold with atlas A = {(Uα, φα)}α∈I. For
each α ∈I, label ⟨, ⟩α as the usual dot product with respect to the coor-
dinate basis over Uα. Let {ψα} be a partition of unity that is subordinate
to A. For each p ∈M, deﬁne the bilinear form ⟨, ⟩p on TpM by
⟨X, Y ⟩p
def
=

α∈I
ψα(p)⟨X, Y ⟩α
p
(5.2)
for any X, Y ∈TpM.
Since for each p ∈M, only a ﬁnite number of α ∈I have ψα(p) ̸= 0,
then the sum in Equation (5.2) is ﬁnite. It is obvious by construction that
⟨X, Y ⟩p is symmetric. To prove that ⟨, ⟩p is positive deﬁnite, note that
each ⟨X, Y ⟩α
p is. Let I′ be the set of all indices αI such that ψα(p) ̸= 0.
By deﬁnition of a partition of unity, 0 < ψα(p) ≤1. Thus, for all X ∈
TpM, clearly ⟨X, X⟩p ≥0. Furthermore, if ⟨X, X⟩p = 0, then at least one
summand in

α∈I′
ψα(p)⟨X, X⟩α
p
is 0.
(In fact, all summands are 0.) Thus, there exists an α ∈I with
⟨X, X⟩α
p = 0. Since ⟨, ⟩α
p is positive deﬁnite, then X = 0. Hence, ⟨X, X⟩p
itself is positive deﬁnite.
□
One should note that though Equation (5.2) presents a Riemannian
metric on any smooth manifold M, this is not in general easy to work with
for speciﬁc calculations since it uses a partition of unity, which involves
functions that are usually complicated. Furthermore, at any given point
p ∈M, Equation (5.2) does not involve one coordinate system around p
but all of the atlas’s coordinate neighborhoods of p.

5.1. Riemannian Metrics
191
5.1.2
Lengths and Volume
Using integration, the Riemannian metric allows for formulas that measure
nonlocal properties, such as length of a curve and volume of a region on a
manifold.
For example, consider a C1 curve γ : [a, b] →M on a Riemannian
manifold (M, g).
At each point γ(t) in M, the vector γ′(t) =
dγ
dt is a
tangent vector, called the velocity vector. The Riemannian metric g = ⟨, ⟩
allows one to calculate the length ∥γ′(t)∥, which we call the speed. This
motivates the following deﬁnition.
Deﬁnition 5.1.9. Let γ : [a, b] →M be a curve on a Riemannian manifold
M of class C1. The arclength of the curve γ is
ℓ(γ) =
% b
a
!4dγ
dt , dγ
dt
5
γ(t)
dt.
Proposition 5.1.10. Let (M, g) be an oriented Riemannian manifold of di-
mension n. There exists a unique n-form, denoted dV , such that dVp(e1, . . . ,
en) = 1 for all orthonormal bases (e1, . . . , en) in TpM. Furthermore, over
any coordinate patch U with coordinates x = (x1, . . . , xn),
dV =

det(gij) dx1 ∧· · · ∧dxn,
(5.3)
where gij = g(∂i, ∂j) = ⟨∂/∂xi, ∂/∂xj⟩.
Proof: The content of this proposition is primarily linear algebraic. By
Proposition C.6.6, on each coordinate patch Uα, the form dV |Uα = ωα
exists on each TpM and is given by Equation (5.3). In order to deﬁne the
form dV on the whole manifold, one refers to a partition of unity {ψα}
subordinate to the atlas on M and deﬁnes
dV =

α
ψαωα.
□
Deﬁnition 5.1.11. The form dV described in Proposition 5.1.10 is called the
volume form of (M, g) and is sometimes denoted dVM if there is a chance
of confusion about the manifold.
If M m is a compact manifold, then we can integrate dV over M. In
that case, we deﬁne the m-volume of M as the integral
Vol(M) =
%
M
dV.

192
5. Introduction to Riemannian Geometry
If i : M m →N n is an embedded submanifold of a Riemannian manifold
(N, ˜g) then one can also calculate the m-volume of the submanifold M by
equipping M with the metric g = i∗˜g, i.e., the metric induced from N.
One should note, however, that the volume form on M is not necessarily
i∗(dVN). In particular, if m < n, then i∗(dVN) would be an n-form on M,
but there are no n-forms on M.
Example 5.1.12 (Volume Form on Sn). We consider the unit n-sphere Sn as a
Riemannian manifold when equipped with the metric induced from Rn+1.
Consider the usual latitude and longitude parametrization of sphere S2:
⃗X(u, v) = (cos u sin v sin u sin v, cos v)
for
(u, v) ∈[0, 2π] × [0, π].
Note that if we restrict the domain to (0, 2π) × (0, π), one obtains a dense
open subset of S2.
By Proposition 5.1.7, with respect to this associate
coordinate system, the coeﬃcients of the metric tensor are
gij =

sin2 v
0
0
1
	
.
Since sin v ≥0 for v ∈[0, π], the volume form on S2 with respect to this
coordinate system is dV = sin v du∧dv. By Proposition 4.4.13, the volume
of the sphere is easily calculated by
V =
%
S2 dV =
% 2π
0
% π
0
sin v du dv = 2π [−cosv]π
0 = 4π.
We now calculate the volume form on Sn using an alternate approach.
By Example C.5.23, we see that the volume form on Rn+1 is
e∗
1 ∧· · · ∧e∗
n+1,
where {e1, . . . , en+1} is the standard basis on Rn+1.
Furthermore, one
should recall that as an alternating function,
e∗
1 ∧· · · ∧e∗
n+1(v1, . . . , vn+1) = det(v1, · · · , vn+1)
for any (n + 1)-tuple of vectors (v1, . . . , vn+1).
Deﬁne a form ω ∈Ωn(Rn+1), where for each x ∈Rn+1 and for any
vectors vi
ωx = det(x, v1 , · · · , vn).
By the properties of the determinant, for each x, ωx is an alternating n-
multilinear function on Rn+1, so ω is indeed an n-form. Using the Laplace

5.1. Riemannian Metrics
193
 x
x
w1
w2
Figure 5.2. Volume form on the sphere.
expansion of the determinant, it is easy to show that
ω =
n+1

i=1
(−1)i−1xi dx1 ∧· · · ∧&
dxi ∧· · · ∧dxn+1,
where the ' notation means to exclude the bracketed term. Using the forms
ηj introduced in Equation (4.10), we can write ω = "
j xjηj.
Now if x ∈Sn, then from the geometry of the sphere, x is perpendicular
to TxSn as a subspace of Rn+1 (equipped with the Euclidean metric). Thus,
if {v1, . . . , vn} forms a basis of TxSn, then {x, v1, . . . , vn} forms a basis of
Rn+1. Furthermore, if the n-tuple (v1, . . . , vn) is an orthonormal, positively
oriented basis of TxSn, then the n+1-tuple (x, v1, . . . , vn) is an orthonormal,
positively oriented basis of Rn+1. But then the restriction of ω to Sn has
the properties described in Proposition 5.1.10. Hence, if i is the inclusion
map for the sphere into Rn+1, we obtain the volume form of Sn as
dVSn = ω|Sn = i∗(ω).
(See Figure 5.2.)
In Section 4.4, we attempted to generalize with the single technique
of integration on manifolds all the types of integration introduced in a
standard calculus sequence. However, as we mentioned at the end of the
section, there were two types of integrals in the list at the beginning of
the section that did not ﬁt in the formalism we had developed for the
integration of n-forms on n-dimensional smooth manifolds, namely

194
5. Introduction to Riemannian Geometry
• line integrals of functions in Rn,
• surface integrals of a real-valued function deﬁned over a closed and
bounded region of a regular surface in R3.
Both of these types of integrals ﬁt into the theory of integration on mani-
folds in the following ways.
For the line integral of functions in Rn over a piecewise-smooth curve
C, let γ : [a, b] →Rn be a parametrization of C. Let ⟨, ⟩be the Euclidean
form on Rn (i.e., the dot product). Then each smooth piece of γ is a one-
dimensional submanifold of Rn, equipped with the metric induced from Rn.
The volume form on γ is dVγ so that for any smooth function f deﬁned on
a neighborhood of C,
%
γ
f dVγ =
% b
a
f(t)
!4dγ
dt , dγ
dt
5
γ(t)
dt =
%
C
f ds.
For surface integrals of a function f on a compact regular surface
S ⊂R3, it is not hard to show that dS = dVS, where dVS is the volume
form on S equipped with the metric induced from the Euclidean metric.
Thus, connecting the classical notation with the notation introduced in this
section,
%
S
f dS =
%
S
f dVS.
5.1.3
Raising and Lowering Indices: Trace
Recall that for any vector space V over the reals R, the dual vector space
V ∗is the vector space of linear transformations V →R (see Section C.3).
An element of V ∗is sometimes called a functional on V .
If V is a vector space equipped with any bilinear form ⟨, ⟩, then this
form deﬁnes a linear transformation into the dual V ∗by
V −→V ∗,
v −→λv
= (w →⟨v, w⟩).
(See Section C.3 for background.) It is not hard to check that if V is ﬁnite-
dimensional and ⟨, ⟩is positive deﬁnite, then the mapping (v →λv) is an
isomorphism.
We will assume from now on that ⟨, ⟩is symmetric and positive deﬁnite.

5.1. Riemannian Metrics
195
In coordinates, let B = {u1, . . . , un} be a basis of V and let B∗=
{u∗
1, . . . , u∗
n} be the associated basis (or dual cobasis) for V ∗. The coordi-
nates of a vector v ∈V are written (vi) to mean that v = viui (Einstein
summation convention). Let A = (aij) be the matrix of ⟨, ⟩with respect
to B, so that
⟨v, w⟩= [v]T
BA[w]B = aijviwj.
(5.4)
Note that aij = ⟨ui, uj⟩. Since ⟨, ⟩is positive deﬁnite, then det A ̸= 0 and
A is invertible. It is customary to denote the indices of A−1 as
(A−1)ij = aij.
The right-most expression in Equation (5.4) gives the coordinates of λv
with respect to B∗as aijvi. Note that the indices for the components of
λv arise naturally as subscripts, consistent with our notation. Similarly, if
λ ∈V ∗is a functional on V , then λ = λiu∗
i . If λ = λv for some v ∈V ,
then
ajkλj = ajkaijvi = aijajkvi = δk
i vi = vk.
(5.5)
One often says that the process of mapping v to λv “lowers the indices,”
while mapping λv to v “raises the indices.”
Now consider a Riemannian manifold (M, g). If X ∈X(M) is a vector
ﬁeld on M, then on a given coordinate patch, X has coordinates Xi. By
the process described in the previous paragraphs, one naturally deﬁnes a
covector ﬁeld on M (or 1-form) with component functions as
gijXi.
Mimicking musical notation, one often denotes this covector ﬁeld as X♭
since the result is to lower the indices. Similarly, if ω ∈Ω1(M) is a 1-form
on M, then on a given coordinate patch, ω has coordinates ωi. Then one
naturally deﬁnes a vector ﬁeld on M with component functions
gijωi.
Keeping the musical analogy, one denotes this vector ﬁeld by ω♯since the
process raises the indices.
More generally, if T is any tensor ﬁeld of type (p, q) on M, then
giαkT i1···ip
j1···jq
and
gjβlT i1···ip
j1···jq
deﬁne tensors ﬁelds of type (p−1, q+1) and type (p+1, q−1), respectively.
It is common to still use the ♭and ♯notation, but one must indicate in
words upon which index one performs the lowering or raising operations.

196
5. Introduction to Riemannian Geometry
Recall that in linear algebra, the trace of a matrix A is deﬁned as the
sum of the eigenvalues (counted with multiplicity) or equivalently as the sum
of the diagonal elements. If A has components Ai
j, then the trace is just
Ai
i, using the Einstein summation convention. Now A corresponds to a
linear transformation T (⃗v) = A⃗v on a vector space V . Since the trace Tr A
is the sum of the eigenvalues, the trace remains unchanged under a change
in basis in V .
Now, if A is a symmetric (0, 2)-tensor, then A♯is a (1, 1)-tensor, and
the trace Tr A is deﬁned in its usual linear algebraic sense. This process is
common enough that we deﬁne the trace with respect to g of A to be
Trg A
def
= Tr A♯.
In coordinates, Trg A = gijAij.
5.1.4
Pseudo-Riemannian Metrics
We end this section with a brief comment on another useful metric that
relaxes the requirements of Riemannian metrics.
Deﬁnition 5.1.13. A pseudo-Riemannian metric on a smooth manifold M
is a symmetric tensor ﬁeld g of type (0, 2) that is nondegenerate at every
point. In more detail, at each point p ∈M, gp(X, Y ) = 0 for all Y ∈TpM
if and only if X = 0 in TpM.
Consider a vector space V equipped with a symmetric bilinear form
⟨, ⟩. If ⟨, ⟩is positive deﬁnite, then it is nondegenerate. However, the
converse is not true in general. In this sense, pseudo-Riemannian metrics
are a direct generalization of Riemannian metrics. With a nondegenerate
bilinear form, it is not necessarily true that ⟨v, v⟩> 0 for all nonzero vectors
nor is it true that ⟨v, v⟩= 0 if and only if v = 0.
The Gram-Schmidt orthonormalization on V , with ⟨, ⟩positive deﬁnite,
allows us to construct a basis {u1, . . . , un} of V in which
⟨, ⟩= (u∗
1)2 + (u∗
2)2 + · · · + (u∗
n)2
as an element of Sym2 V ∗. In contrast, if ⟨, ⟩is only nondegenerate, one
can adapt the Gram-Schmidt process to show that there exists a basis
{u1, . . . , un} of V such that
⟨, ⟩= −(u∗
1)2 −· · · −(u∗
r)2 + (u∗
r+1)2 + · · · + (u∗
n)2
(5.6)

5.1. Riemannian Metrics
197
for some integer 0 ≤r ≤n. Furthermore, n −r is the maximum dimension
of any subspace on which ⟨, ⟩is positive deﬁnite. Hence, by this charac-
terization, r is independent of any choice of basis.
Deﬁnition 5.1.14. Let ⟨, ⟩be a symmetric nondegenerate bilinear form on V .
The integer r in Equation (5.6) is called the index of ⟨, ⟩. One also says
that the space (V, ⟨, ⟩) has signature (n −r, r).
The most important applications of pseudo-Riemannian metrics occur
in relativity where one uses Lorentz metrics, pseudo-Riemannian metrics
of index 1 (or n−1). Special relativity is based on the principle (and exper-
imental observation) that the speed of light is the same for all observers,
regardless of their frame of reference. Applying this rule to frames that
move with constant velocity with respect to each other forces a complete
reworking of Newtonian mechanics and leads to many counterintuitive re-
sults. These include relinquishing the absolute nature of time, lengths, and
mass or the idea that the composition of two velocities is their sum (see [40]
for a comprehensive treatment of special relativity).
Now in order to construct laws of physics, one should have at the foun-
dation a notion of distance that is the same for all observers in any inertial
frame. One should recall that the frames of reference for the two observers
can be moving and rotating with respect to each other. In classical me-
chanics, given two points P1 and P2, a distance quantity that does not
change for any two observers is

(Δx)2 + (Δy)2 + (Δx)2.
(5.7)
Problem 2.4.13 shows the invariance of this quantity under the possible
transformations of coordinates between reference frames.
Taking into account the fundamental principle of relativity that the
speed of light is the same for every observer, this quantity is no longer
the same for diﬀerent observers. In fact, Einstein showed that the speed
diﬀerence between inertial frames aﬀects Equation (5.7). In order to ﬁnd a
distance-like quantity that is preserved between diﬀerent reference frames,
then one must incorporate time as a dependent variable on par with the
space variables.
To contrast with the mentality of classical mechanics,
in the classical formulation of dynamics, one typically views time t as an
independent parameter and the space variables x, y, and z as functions
of t. In special relativity, one uses four coordinates, i.e.,
(x0, x1, x2, x3) = (ct, x, y, z),
(5.8)

198
5. Introduction to Riemannian Geometry
where c is the speed of light. One calls this mental perspective spacetime.
A point in this spacetime is called an event. The trajectory of a particle in
spacetime is viewed as a one-dimensional submanifold of spacetime, called
the world line of the particle.
Using work by Minkowski, Einstein showed that if one observer’s ref-
erence frame is moving with a constant velocity with respect to the other,
then a distance-like quantity would need to preserve, in addition to, or-
thogonal transformations of the space variables, a Lorentz transformation.
For example, if one frame moves at a constant speed along the x-axis with
reference to another frame, then the Lorentz transformation between coor-
dinates in these diﬀerent frames is
L =
⎛
⎜
⎜
⎝
γ
−βγ
0
0
−βγ
γ
0
0
0
0
1
0
0
0
0
1
⎞
⎟
⎟
⎠,
where γ =
1

1 −β2 and β = v
c .
The quantity preserved by Lorentz transformations and orthogonal trans-
formations of the space variables is
−(Δs)2 def
= −(Δx0)2 + (Δx1)2 + (Δx2)2 + (Δx3)2.
(5.9)
(See Problem 2.4.14 for more on Lorentz transformations.)
Deﬁnition 5.1.15. We deﬁne (n + 1)-Minkowski spacetime as a real vector
space with coordinates (x0, x1, . . . , xn) that is equipped with the pseudo-
Riemannian metric
g = −(dx0)2 + (dx1)2 + · · · + (dxn)2.
(5.10)
In other words,
g00 = −1,
gii = 1 if i > 0,
gij = 0 if i ̸= j.
When there is no confusion, Minkowski spacetime is often denoted by R1,n.
The particular metric in Equation (5.10) is often denoted by the letter η.
There is some inconsistency in the literature for notation of Minkowski
space. Some authors (typically physicists) prefer our notation, while others
(primarily mathematicians) deﬁne Δs2 as the negative of what we chose in
Equation (5.9). We have chosen to use the form η as described in Equa-
tion (5.10), but we use the variable s to satisfy −ds2 = η. The beneﬁt of
this choice of s comes from relativity, as shown in the next paragraph.

5.1. Riemannian Metrics
199
The locus of a particle moving in a Minkowski space is described by
a function X(λ) = (x0(λ), x1(λ), x2(λ), x3(λ)), where λ is any parameter,
not necessarily time t. The one thing we can say about the function t(λ) is
that if the particle travels slower than c, then t(λ) is a strictly increasing
function. The image of X in R1,3 is the world line of the particle. Then
the speed (with respect to λ, using the Minkowski metric) is
−
 ds
dλ
	2
= −c2
 dt
dλ
	2
+
dx
dλ
	2
+
 dy
dλ
	2
+
 dz
dλ
	2
.
Since dt/dλ > 0, we can rewrite this in terms of t.
We ﬁnd that if a
particle has a vector function ⃗v(t) in the Euclidean part, then ds and dt
are related by
ds = c

1 −v2
c2 dt.
(5.11)
The quantity ds/c is called the proper time interval and the function
τ = 1
c
% λ
0
ds
(5.12)
is called the proper time of the particle traveling on its world line. Proper
time plays a central role in the theory of relativity since it is a timelike
quantity that is the same for all inertial observers, i.e., unchanged by any
Lorentz transformation.
In the observed Minkowski space of R1,3, two events for which Δs2 > 0
are called timelike separated. Clearly, for two timelike separated events,
time must have elapsed. Also, since a particle cannot travel faster than
the speed of light, any two events on the world line of a particle must
be timelike separated.
From another perspective, two events are called
timelike separated if a particle can travel between them (without moving
faster than the speed of light). Two events for which Δs2 = 0 are called
lightlike separated if only a particle traveling in a straight line at the speed
of light can connect the two events. Finally, two events for which Δs2 < 0
are called spacelike separated because no particle can have a world line that
connects the two events. [57, Section 2.2] The light cone based at an event
P is the set of all events that are lightlike separated from P. Figure 5.3
shows the light cone for the origin, though we can only display the variables
(x0, x1, x2).
As we will see in Section 6.4, in the general theory of relativity, gravity
aﬀects the metric and leads to a metric that varies from point to point but
is still Lorentz.

200
5. Introduction to Riemannian Geometry
lightlike
timelike
spacelike
Figure 5.3. Light cone.
Problems
5.1.1. Consider the 3-torus T3 = S1 × S1 × S1. Calculate the induced metrics for
the following two embeddings:
(a) Into R6 as the image of the parametrization
⃗F(u1, u2, u3) =

cos u1, sin u1, cos u2, sin u2, cos u3, sin u3
.
(b) Into R4 as the image of the parametrization ⃗F(u1, u2, u3) given by
((c + (b + a cos u1) cos u2) cos u3, (c + (b + a cos u1) cos u2) sin u3,
(b + a cos u1) sin u2, a sin u1), where b > a > 0 and c > a + b.
(c) Prove that these two Riemannian manifolds are not isometric.
[Hint: This gives two diﬀerent metrics on the 3-torus that can be equipped
with the same atlas in each case.]
5.1.2. We consider an embedding of S1×S2 in R4 by analogy with the embedding
of the torus S1 × S1 in R3. Place the sphere S2 with radius a at (0, 0, b, 0)
(where b > a) as a subset of the x1x2x3-subspace, and rotate this sphere
about the origin with a motion parallel to the x3x4-axis.
Call this submanifold M and equip it with the metric induced from R4.
(a) Show that the described manifold M is an embedding as claimed.
(b) Find a parametrization F : D →R4 where D ⊂R3 such that, as
sets of points F(D) = M, and F(D◦) is an open subset of M that is
homeomorphic to D◦. (D◦is the interior of the set D.)
(c) Calculate the coeﬃcients gij of the metric on M in the coordinate
patch deﬁned by the above parametrization.

5.1. Riemannian Metrics
201
5.1.3. Let (M1, g1) and (M2, g2) be two Riemannian manifolds, and consider the
product manifold M1 × M2 with a (0, 2)-tensor ﬁeld deﬁned by
g(X1 + X2, Y1 + Y2) = g1(X1, Y1) + g2(X2, Y2).
(a) Show that g deﬁnes a metric on M1 × M2.
(b) Let (x1, . . . , xn) be local coordinates on M1 and (xn+1, . . . , xn+m) be
local coordinates on M2 so that (x1, . . . , xm+n) are local coordinates
on M1 ×M2. Determine the components of the metric g on M1 ×M2
in terms of g1 and g2.
5.1.4. Repeat Problem 5.1.2 with S1 × S, where S is a regular surface in R3 that
does not intersect the plane z = x3 = 0.
5.1.5. Consider the following two regular surfaces in R3 parametrized by
• catenoid: F(¯u1, ¯u2) = (¯u2 cos ¯u1, ¯u2 sin ¯u1, cosh−1 ¯u2) for (u1, u2) ∈
[0, 2π) × R,
• helicoid: F(u1, u2)=(u2 cos u1, u2 sin u1, u1) for (u1, u2)∈[0, 2π)×R.
Prove that the helicoid and catenoid are isometric, and ﬁnd an isometry
between them.
5.1.6. Let M be a hypersurface of Rn (submanifold of dimension n−1), and equip
M with Riemannian structure with the metric induced from Rn. Suppose
that an open set U of M is a graph of an (n −1)-variable function f, i.e.,
the parametrization of U is
x1 = u1, . . . , xn−1 = un−1, xn = f(u1, . . . , un−1),
(u1, . . . , un−1) ∈D.
(a) Find the coeﬃcients of the metric tensor g on M, and conclude that
a formula for the (n −1)-volume of U is
%
D

1 + ∥grad f∥2 dV.
(b) Use this result to calculate the 3-volume of the surface in R4 given
by w = x2 + y2 + z2 for x2 + y2 + z2 ≤4.
5.1.7. Let M, N, and S be Riemannian manifolds, and let f : M →N and
h : N →S be isometries.
(a) Show that f −1 is an isometry.
(b) Show that h ◦f is an isometry.
(c) If you have seen some group theory, show that the set of isometries
on a Riemannian manifold M forms a group.
5.1.8. Let γ be a curve on a Riemannian manifold (M, g). Show precisely how
the induced metric on γ generalizes Deﬁnition 5.1.9.

202
5. Introduction to Riemannian Geometry
5.1.9. Poincar´e ball. The Poincar´e ball is the open ball Bn
R in n dimensions of
radius R equipped with the metric
4R4
(R2 −∥x∥2)2

(dx1)2 + · · · + (dxn)2
.
(a) Set n = 2 and R = 1.
(This choice of parameters is called the
unit Poincar´e disk.) Calculate the area of the region R deﬁned by
∥x∥< 1
2 and 0 ≤θ ≤π/2.
(b) Set n = 3 and R = 2.
Calculate the length of the curve γ(t) =
(cos t, sin t, t) in the Poincar´e ball.
5.1.10. Divergence Theorem.
Let (M, g) be an oriented, compact, Riemannian
manifold with boundary.
Given any vector ﬁeld X ∈X(M) and any
tensor ﬁeld T of type (p, q), with q ≥1, we deﬁne the contraction of X
with T, denoted iXT, as the tensor ﬁeld of type (p, q −1) that over any
coordinate chart has components
XlT
i1···ip
lj2···jq.
We deﬁne the divergence operator div : X(M) →C∞(M) implicitly by
d(iX dV ) = (div X) dV.
(a) Show that for a k-form ω, the contraction iXω is the (k −1)-form
such that, for any vectors v1, . . . , vk−1 ∈TpM,
iXω(v1, . . . , vk−1) = ω(X, v1, . . . , vk−1).
(b) Prove the Divergence Theorem, which states that for any X ∈X(M)
%
M
div X dV =
%
∂M
g(X, N) d ˜V ,
where N is the outward unit normal to ∂M and d ˜V is the volume
form associated to the metric on ∂M induced from M.
5.1.11. We consider the sphere S3 of radius R as a submanifold in R4 with the
induced Euclidean metric.
(a) Show that
F(u1, u2, u3) =
(R cos u1 sin u2 sin u3, R sin u1 sin u2 sin u3, R cos u2 sin u3, R cos u3),
where (u1, u2, u3) ∈[0, 2π] × [0, π]2 gives a parametrization for S3
that is homeomorphic to its image when restricted to the open set
V = (0, 2π) × (0, π)2.
(b) Calculate the components of the metric tensor on the coordinate
patch F(V ) = U.

5.1. Riemannian Metrics
203
(c) Use part (b) to calculate the volume of a 3-sphere of radius R.
(d) Leaving R unspeciﬁed, consider the function f(x1, x2, x3, x4) =
(x1)2 + (x2)2 + (x3)2 and calculate the volume integral
3
S3 f dV .
(Note that this integral would give the radius of gyration of the
spherical shell of radius R about a principal axis—if such a thing
existed in R4!)
5.1.12. Calculate the 5-volume of the 5-sphere S5 of radius R as a submanifold
of R6.
5.1.13. (ODE) A loxodrome on the unit sphere S2 is a curve that makes a con-
stant angle with all meridian lines.
We propose to study analogues of
loxodromes on S3. Consider the unit 3-sphere S3 with the parametriza-
tion from Problem 5.1.11. Set R = 1. We will call a loxodrome on S3 any
curve γ such that γ′ makes a constant angle of α2 with
∂
∂u2 and a constant
angle of α3 with
∂
∂u3 .
(a) Find equations that the components of γ must satisfy.
(b) Solve the diﬀerential equations we get in part (a). [Hint: Obtain u1
and u2 as functions of u3. You might only be able to obtain one of
these functions implicitly.]
5.1.14. Consider the function r : Rn+1 −{0} →Sn given by r(x) = x/∥x∥.
(a) Using Example 5.1.12, prove that
˜ω = r∗(dVSn) =
1
∥x∥n+1
n+1

j=1
xjηj.
(b) Show that ˜ω is closed but not exact in Rn+1 −{0}.
(c) Use dVSn to show that
Vol(Sn) = (n + 1)Vol(Bn+1),
where Bn+1 is the unit ball in Rn+1.
5.1.15. Let M be a Riemannian manifold, and let f : M →M be an isometry on
M. Prove that f ∗(dVM) = ±dVM. (The isometry f is called orientation-
preserving if f ∗(dVM) = dVM and orientation-reversing if f ∗(dVM) =
−dVM.)
5.1.16. Suppose that J and K are disjoint, compact, oriented, connected, smooth
submanifolds of Rn+1 whose dimensions are greater than 0 and such that
dim J + dim K = n. Deﬁne the function Ψ by
Ψ : J × K −→Sn
(x, y) −→
y −x
∥y −x∥.

204
5. Introduction to Riemannian Geometry
The linking number between J and K is deﬁned as
link(J, K) =
1
Vol(Sn)
%
J×K
Ψ∗(dVSn).
Prove Gauss’s Linking Formula for the linking number of two space curves:
link(C1, C2) = 1
4π
%
I
%
J
det(⃗α(u) −⃗β(v), ⃗α′(u), ⃗β′(v))
∥⃗α(u) −⃗β(v)∥3
du dv.
5.1.17. Let g be a pseudo-Riemannian metric on a smooth manifold M. Prove
that the index of g is constant on each connected component of M.
5.1.18. Hodge Star Operator.
Let (M, g) be an oriented Riemannian manifold.
Section C.6.3 deﬁnes the Hodge star operator on inner product spaces.
Given a form η ∈Ωk(M), at each p ∈M, the Hodge star operator deﬁnes
an isomorphism ⋆: 1k TpM ∗→1n−k TpM ∗.
(a) Show that the Hodge star operator ⋆is a map between vector bundles
Ωk(M) →Ωn−k(M) that leaves every base point ﬁxed and that varies
smoothly on M.
(b) Show that for all functions f : C∞(M), the Hodge star operator is
given by ⋆f = fdVg.
5.1.19. Consider Rn as a manifold with the standard Euclidean metric.
(a) Calculate ⋆dxi for any i = 1, . . . , n.
(b) Set n = 4, and calculate ⋆(dxi ∧dxj).
(c) Set n = 4, and now assume that the metric is the Minkowski metric.
In other words, consider R1,3, with g deﬁned by Equation (5.10).
Repeat part (b) in this situation.
5.1.20. Let (M, g) be an oriented Riemannian manifold. Prove the following iden-
tities for any vector ﬁeld X ∈X(M).
(a) div X = ⋆d ⋆X♭, where div is the divergence operator deﬁned in
Problem 5.1.10.
(b) iXdVg = ⋆X♭.
5.2
Connections and Covariant Differentiation
Despite all the “heavy machinery” we have developed in order to create
a theory of analysis on manifolds, we are still unable to calculate or even
deﬁne certain things that are simple in Rn.
For example, if γ : [a, b] →M is a smooth curve on a smooth manifold,
we have no way at present to talk about the acceleration of γ. Let p ∈M,

5.2. Connections and Covariant Differentiation
205
with γ(t0) = p. In Deﬁnition 3.3.1, we presented the tangent vector γ′(t0)
at p as the operator Dγ : C1(M) →R that evaluates
Dγ(f) = d
dtf(γ(t))

0.
In Section 3.3, we developed the linear algebra of expressions Dγ for curves
γ through p. The vector space of such operators is what we called the
tangent space TpM.
Mimicking what one does in standard calculus, one could try to deﬁne
the acceleration vector γ′′(t0) at p as a limiting ratio as t →t0 of γ′(t) −
γ′(t0) with t −t0. However, what we just wrote does not make sense in
the context of manifolds because γ′(t) and γ′(t0) are not even in the same
vector spaces and so their diﬀerence is not deﬁned.
A “smarter” attempt to deﬁne the acceleration might follow Deﬁnition
3.3.1 and try to deﬁne γ′′(t0) at p as the operator D(2)
γ
: C2(M) →R,
where
D(2)
γ (f) = d2
dt2 f(γ(t))

t0.
This operator is well deﬁned and linear. However, D(2)
γ
does not satisfy
Leibniz’s rule, and therefore, there does not exist another curve ˜γ, with
˜γ(t0) = p such that D(2)
γ
= D˜γ. Hence, D(2)
γ
/∈TpM. One could develop
properties for the operators of the form D(2)
γ
but, since the operators do
not exist in any T M ⊗p ⊗T M ∗⊗q, this is not the direction the theory of
manifolds has developed.
Another lack in our current theory is the ability to take partial deriva-
tives or, more generally, directional derivatives of a vector ﬁeld. Over Rn,
it is easy to deﬁne ∂⃗F/∂xj, where ⃗F is a vector ﬁeld, and, under suitable
diﬀerentiability conditions, ∂⃗F/∂xj is again another vector ﬁeld. In con-
trast, if X is a vector ﬁeld over a smooth manifold M and U is a coordinate
neighborhood of p ∈M, one encounters the same problem with deﬁning a
vector ∂iXp as one does in deﬁning the acceleration of a curve.
A more subtle attempt to deﬁne partial derivatives of a vector ﬁeld X
on M in a coordinate chart would be to imitate the exterior diﬀerential of
forms (see Deﬁnition 4.3.4) and set as a diﬀerential for X the quantity
∂Xi
∂xj dxj
	
⊗∂i = ∂Xi
∂xj ∂i ⊗dxj.
However, this does not deﬁne a tensor ﬁeld of type (1, 1) on M. In other
words, the quantities ∂Xi
∂xj do not form the components of a (1, 1)-tensor

206
5. Introduction to Riemannian Geometry
on M. It is easiest to see this by showing how the components violate
the transformational properties of a tensor ﬁeld. Let ¯x = (¯x1, . . . , ¯xn) be
another system of coordinates that overlaps with the coordinate patch for
x = (x1, . . . , xn). Call ¯X the components of the vector ﬁeld X in the ¯x
system. We know that
¯Xj(¯x) = ∂¯xj
∂xi Xi(x).
Taking a derivative with respect to ¯xk and inserting appropriate chain rules,
we have
∂¯Xj
∂¯xk =
∂
∂¯xk
∂¯xj
∂xi
	
Xi + ∂¯xj
∂xi
∂Xi
∂¯xk
=
∂2¯xj
∂xl∂xi
∂xl
∂¯xk Xi + ∂¯xj
∂xi
∂xl
∂¯xk
∂Xi
∂xl .
(5.13)
The presence of the ﬁrst term on the right-hand side shows that the col-
lection of functions ∂jXi do not satisfy Equation (4.7).
To solve the above conceptual problems, we need some coordinate-
invariant way to compare vectors in tangent spaces at nearby points. This
is the role of a connection. A connection on a smooth manifold is an addi-
tional structure that, though we introduced it in the context of Riemannian
manifolds, is entirely independent of any Riemannian structure. We can in
fact deﬁne a connection on any vector bundle over M. Since we require this
generality for our applications, we introduce connections in this manner.
Deﬁnition 5.2.1. Let M be a smooth manifold, and let ξ be a vector bundle
over M. Let E(ξ) denote the subspace of Γ(ξ) of smooth global sections
of ξ. A connection on ξ is a map
∇: X(M) × E(ξ) →E(ξ),
written ∇XY instead of ∇(X, Y ), that satisﬁes the following:
1. For all vector ﬁelds Y ∈E(ξ), ∇( , Y ) is linear over C∞(M), i.e., for
all f, g ∈C∞(M),
∇fX1+g ˜
XY = f∇XY + g∇˜
XY.
2. For all vector ﬁelds X ∈X(M), ∇(X, ) is linear over R, i.e., for all
a, b ∈R,
∇X(aY + b ˜Y ) = a∇XY + b∇X ˜Y .

5.2. Connections and Covariant Differentiation
207
3. For all vector ﬁelds X ∈X(M), ∇(X, ) satisﬁes the product rule
∇X(fY ) = (Xf)Y + f∇XY
for all f ∈C∞(M).
The vector ﬁeld ∇XY in E(ξ) is called the covariant derivative of Y in the
direction of X.
The symbol ∇is pronounced “del.”
The deﬁning properties of the
covariant derivative are modeled after the properties of directional deriva-
tives of vector ﬁelds on Rn (see Problem 5.2.1). Intuitively, the connection
explicitly deﬁnes how to take a partial derivative in E(ξ) with respect to
vector ﬁelds in T M. In fact, Problems 5.2.3 and 5.2.4 show that ∇XY
depends only on the values of Xp in TpM and the values of Y in a neigh-
borhood of p on M. Therefore ∇XY |p is truly a directional derivative of Y
at p in the direction Xp. Hence, we often write ∇XpY instead of ∇XY |p.
For the applications of interest in diﬀerential geometry, we will usu-
ally be interested in using connections on vector bundles of the form ξ =
T M ⊗r ⊗T M ∗⊗s. As it will turn out, connections on these vector bundles
are closely related to possible connections on T M. Therefore, we temporar-
ily restrict our attention to connections
∇: X(M) × X(M) →X(M).
Over a coordinate patch U of M, the deﬁning properties are such that ∇is
completely determined once one knows its values for X = ∂i and Y = ∂j.
Since ∇∂i∂j is another vector ﬁeld in M, we write
∇∂i∂j = Γk
ij∂k.
(5.14)
The components Γk
ij are smooth functions M →R.
Deﬁnition 5.2.2. The functions Γk
ij in Equation (5.14) are called the Christof-
fel symbols of the connection ∇.
As it turns out, there are no restrictions besides smoothness on the
functions Γk
ij.
Proposition 5.2.3. Let M n be a smooth manifold, and let U be a coordinate
patch on M. There is a bijective correspondence between connections on
X(U) and collections of n3 smooth functions Γi
jk deﬁned on U. The bijec-
tion is given by the formula
∇XY = (Xj∂jY i + Γi
jkXjY k)∂i.
(5.15)

208
5. Introduction to Riemannian Geometry
Proof: First, suppose that ∇is a connection on X(U) and let X, Y ∈X(U).
Then by the relations in Deﬁnition 5.2.1,
∇XY = ∇(Xj∂j)(Y k∂k) = Xj∇∂j(Y k∂k)
= Xj(∂jY k)∂k + XjY k∇∂j∂k = Xj(∂jY k)∂k + XjY kΓi
jk∂i.
and Equation (5.15) holds by changing the variable of summation from k
to i in the ﬁrst term of the last expression. Conversely, if Γi
jk are any
smooth functions on U and if we deﬁne an operator X(U) × X(U) →X(U)
by Equation (5.15), it is quick to check that the three criteria of Deﬁnition
5.2.1 hold. Thus, Equation (5.15) deﬁnes a connection on X(U).
□
Consequently, over any coordinate patch U ⊂M, there are many pos-
sible connections on T U. In addition to Proposition 5.2.3, one can use a
partition of unity subordinate to a cover of M by coordinate patches to
deﬁne a connection on all of M. In this case, one cannot deﬁne a cor-
respondence between connections and collections of functions in the same
vein as in Proposition 5.2.3.
Example 5.2.4 (The Flat Connection on Rn). In Rn, the vector ﬁelds ∂i are con-
stant, and we identify them with the standard basis vector ⃗ei. According
to Proposition 5.2.3, a connection exists for any collection of n3 functions.
However, if Y = Y j∂j is a vector ﬁeld in Rn, our usual way of taking partial
derivatives of vector ﬁelds is
∇∂iY = ∂Y j
∂xi ∂j,
which takes partial derivatives componentwise on Y . By Proposition 5.15,
we see that Γi
jk = 0 for all choices of the indices. A connection with this
property is called a ﬂat connection over the coordinate patch.
Even though the symbols Γi
jk resemble our notation for the components
of a (1, 2)-tensor, a connection is not a tensor ﬁeld. The reason derives
from the fact that ∂jXi is not a (1, 1)-tensor ﬁeld. In fact, from (5.13)
and the transformational properties of a vector ﬁeld between overlapping
coordinate systems on M, we can deduce the transformational properties
of the component functions of a connection.
Proposition 5.2.5. Let ∇be a connection on X(M). Suppose that U and ¯U
are overlapping coordinate patches, and denote by Γi
jk and ¯Γl
mn the com-
ponent functions of ∇over these patches, respectively. Then over U ∩¯U,
the component functions are related to each other by
¯Γl
mn = ∂xj
∂¯xm
∂xk
∂¯xn
∂¯xl
∂xi Γi
jk −∂xj
∂¯xm
∂xk
∂¯xn
∂2¯xl
∂xj∂xk .

5.2. Connections and Covariant Differentiation
209
Proof: (Left as an exercise for the reader.)
□
The astute reader might have noticed already from Deﬁnition 5.2.1 that
a connection is not a tensor ﬁeld of type (1, 2). If an operator F : X(M) ×
X(M) →X(M) were a tensor ﬁeld in T M ⊗T M ∗⊗2, then F(X, ) would
be linear in C∞(M) and would not satisfy the third property in Deﬁnition
5.2.1.
Example 5.2.6 (Polar Coordinates). We consider the connection ∇on R2 that
is ﬂat over the Cartesian coordinate system. We calculate the components
of ∇with respect to polar coordinates. We could calculate the Christoﬀel
symbols from Proposition 5.2.3, but instead, we use Proposition 5.2.5. Set
x1 = x, x2 = y, ¯x1 = r, and ¯x2 = θ, and denote by Γi
jk = 0 and by ¯Γl
mn
the Christoﬀel symbols for ∇in polar coordinates.
By direct calculation,
¯Γ2
12 = −
2

j,k=1
∂xj
∂¯x1
∂xk
∂¯x2
∂2¯x2
∂xj∂xk
= −

−r cos θ sin θ ∂2θ
∂x2 + r2 cos2 θ ∂2θ
∂x∂y −r sin2 θ ∂2θ
∂y∂x
+ r sin θ cos θ ∂2θ
∂y2
	
= −

−r sin θ cos θ
2xy
(x2 + y2)2 + r(cos2 θ −sin2 θ) y2 −x2
(x2 + y2)2
−r sin θ cos θ
2xy
(x2 + y2)2
	
= 1
r

2 sin2 θ cos2 θ + (cos2 θ −sin2 θ)2 + 2 sin2 θ cos2 θ

= 1
r .
It is not hard (though perhaps a little tedious) to show that
¯Γ1
11 = 0,
¯Γ1
12 = ¯Γ1
21 = 0,
¯Γ1
22 = −r,
¯Γ2
11 = 0,
¯Γ2
12 = ¯Γ2
21 = 1
r ,
¯Γ2
22 = 0.
We now wish to extend our discussion of connections on T M to con-
nections on any tensor bundle T M ⊗r ⊗T M ∗⊗s in a natural manner for
any pair (r, s). Two situations are settled: (1) if f ∈T M 0 = C∞(M),
then we want ∇Xf = X(f), the expected directional derivative; and (2)
if X ∈T M, then the connection should follow the properties described in
Deﬁnition 5.2.1 and Proposition 5.2.3.

210
5. Introduction to Riemannian Geometry
Lemma 5.2.7. Let M be a smooth manifold, and let ∇be a connection on T M.
For each pair (r, s) ∈N2, there exists a unique connection on the tensor
bundle T M ⊗r ⊗T M ∗⊗s, also denoted ∇, given by the following condi-
tions:
1. Consistency: ∇is equal to the connection given on T M.
2. Directional derivative: ∇Xf = X(f) for all f ∈C∞(M) = T M 0.
3. Contraction product rule: if we deﬁne the pairing ⟨ω, Y ⟩= ω(Y ) for
all covector ﬁelds ω and vector ﬁelds X, then
∇X (⟨ω, Y ⟩) = ⟨∇Xω, Y ⟩+ ⟨ω, ∇XY ⟩.
4. Tensor product rule: for all tensor ﬁelds A and B of any type,
∇X(A ⊗B) = (∇XA) ⊗B + A ⊗(∇XB).
We omit the proof of this lemma since it is merely constructive. Prop-
erty 3 determines uniquely how to deﬁne ∇Xω for any covector ﬁeld and
then Property 4 extends the connection to all other types of tensors.
Deﬁnition 5.2.8. Let M be a smooth manifold. We call ∇an aﬃne connec-
tion on T M ⊗r ⊗T M ∗⊗s if it satisﬁes the conditions of Lemma 5.2.7.
Let ∇be an aﬃne connection on a smooth manifold M. Let F be a
tensor ﬁeld of type (r, s). Then the mapping ∇F that maps a vector ﬁeld
X to ∇XF is a C∞(M)-linear transformation from X(M) to the space
of tensor ﬁelds of type (r, s).
Thus, for each p ∈M, ∇F|p is a linear
transformation TpM →TpM ⊗r ⊗TpM ∗⊗s, so by Proposition C.4.7,
∇F|p ∈Hom(TpM, TpM ⊗r ⊗TpM ∗⊗s) = TpM ⊗r ⊗TpM ∗⊗(s+1).
Furthermore, since ∇F|p varies smoothly with p, then ∇F is a smooth
section of the tensor bundle TpM ⊗r ⊗TpM ∗⊗s+1, and hence, it is a tensor
ﬁeld of type (r, s + 1).
Deﬁnition 5.2.9. Let M be a smooth manifold equipped with an aﬃne con-
nection ∇. If F is a tensor ﬁeld of type (r, s), then the tensor ﬁeld ∇F of
type (r, s + 1) is called the covariant derivative of F.

5.2. Connections and Covariant Differentiation
211
Proposition 5.2.10. Let F be a tensor ﬁeld of type (r, s) over a manifold M.
Suppose that F has components F i1···ir
j1···js over a coordinate chart U. Then
the components of the covariant derivative ∇F are
F i1···ir
j1···js;k
def
=
∂F i1···ir
j1···js
∂xk
+
r

α=1
Γiα
kpF i1···iα−1piα+1···ir
j1···js
−
s

β=1
Γp
kjβF i1···ir
j1···jβ−1pjβ+1···js.
(5.16)
(Some authors use the notation F i1···ir
j1···js|k for the components of the
covariant derivative.) The notation in Equation (5.16) is a little heavy, but
it should become clear with a few examples. If ω is a 1-form, then ∇ω is a
2-form with local components given by
∇ω = ωj;kdxj ⊗dxk,
where
ωj;k = ∂kωj −Γp
kjωp.
Similarly, if Aij
k are the components of a (2, 1)-tensor ﬁeld A, then ∇A is
a (2, 2)-tensor ﬁeld with local components given by
∇A = Aij
k;l∂i⊗∂j⊗dxk⊗dxl, where Aij
k;l = ∂Aij
k
∂xl +Γi
lpApj
k +Γj
lpAip
k −Γp
lkAij
p .
Proposition 5.2.3 gives one considerable freedom in choosing the compo-
nents of a connection. Returning to the context of Riemannian geometry,
one may wish to use a connection that is in some sense “nice” with re-
spect to the metric on the manifold. The following theorem is motivated
by results in classical diﬀerential geometry of surfaces discussed in [5, Sec-
tion 7.2] but is so central to Riemannian geometry that it is sometimes
called the “miracle” of Riemannian geometry [43].
Theorem 5.2.11 (Levi-Civita Theorem). Let (M, g) be a Riemannian manifold.
There exists a unique aﬃne connection ∇that satisﬁes the following two
conditions:
1. Compatibility: ∇g is identically 0.
2. Symmetry: for all X, Y ∈X(M), [X, Y ] = ∇XY −∇Y X.
A few comments are in order before we prove this theorem. The condi-
tion that ∇g = 0 intuitively says that ∇is ﬂat with respect to the metric.
We say that ∇is compatible with the metric. We leave it as an exercise

212
5. Introduction to Riemannian Geometry
for the reader (Problem 5.2.12) to show that if we write g = ⟨, ⟩, then ∇g
is identically 0 (i.e., gij;k = 0 in local coordinates) if and only if
∇X(⟨Y, Z⟩) = ⟨∇XY, Z⟩+ ⟨Y, ∇XZ⟩.
(5.17)
Hence, if ∇is compatible with the metric g, then it satisﬁes a product rule
with respect to the metric.
By Problem 5.2.14, condition 2 implies that over any coordinate patch
of the manifold, the Christoﬀel symbols Γi
jk of the connection ∇satisfy
Γi
jk = Γi
kj, which justiﬁes the terminology of a symmetric connection.
Deﬁnition 5.2.12. The connection ∇described in Theorem 5.2.11 is called
the Levi-Civita connection or the Riemannian connection with respect to
the metric g on M.
Proof (of Theorem 5.2.11): Let X, Y, Z ∈X(M), and denote g = ⟨, ⟩.
Since ⟨X, Y ⟩is a smooth function on M, then we write ∇Z(⟨X, Y ⟩) =
Z⟨X, Y ⟩.
Now suppose that such a connection ∇exists. Then
X⟨Y, Z⟩= ⟨∇XY, Z⟩+ ⟨Y, ∇XZ⟩,
(5.18)
Y ⟨Z, X⟩= ⟨∇Y Z, X⟩+ ⟨Z, ∇Y X⟩,
(5.19)
Z⟨X, Y ⟩= ⟨∇ZX, Y ⟩+ ⟨X, ∇ZY ⟩.
(5.20)
Adding Equations (5.18) and (5.19) and subtracting Equation (5.20), using
the symmetry of the metric, we get
X⟨Y, Z⟩+ Y ⟨Z, X⟩−Z⟨X, Y ⟩= ⟨∇XY −∇Y X, Z⟩+ ⟨∇XZ −∇ZX, Y ⟩
+ ⟨∇Y Z −∇ZY, X⟩+ 2⟨Z, ∇Y X⟩.
Using the fact that ∇is symmetric, we have
X⟨Y, Z⟩+ Y ⟨Z, X⟩−Z⟨X, Y ⟩= ⟨[X, Y ], Z⟩+ ⟨[X, Z], Y ⟩
+ ⟨[Y, Z], X⟩+ 2⟨Z, ∇XY ⟩,
and thus
⟨Z, ∇XY ⟩= 1
2 (X⟨Y, Z⟩+ Y ⟨Z, X⟩−Z⟨X, Y ⟩
−⟨[X, Y ], Z⟩−⟨[X, Z], Y ⟩−⟨[Y, Z], X⟩) .
(5.21)
Now a connection on any coordinate patch is uniquely determined by its
Christoﬀel symbols. However, setting X = ∂i, Y = ∂j and Z = ∂k, Equa-
tion (5.21) gives a method to obtain the Christoﬀel symbols of ∇strictly

5.2. Connections and Covariant Differentiation
213
in terms of the metric. Hence, if a connection as described in the theorem
exists, then it is unique.
To show that such a connection exists, simply start by deﬁning ∇using
the identity in Equation (5.21).
Then it is not hard to show that the
connection is both symmetric and compatible with g.
□
Proposition 5.2.13. Let (M n, g) be a smooth Riemannian manifold.
Then
over a coordinate patch of M with coordinates (x1, . . . , xn), the Christoﬀel
symbols of the Levi-Civita connection are given by
Γi
jk =
n

l=1
1
2gil
∂gkl
∂xj + ∂glj
∂xk −∂gjk
∂xl
	
,
(5.22)
where gij are the entries to the inverse matrix of (gkl).
Proof: Set g = ⟨, ⟩, and let X = ∂i, Y = ∂j, and Z = ∂k. By the Levi-
Civita connection deﬁned in Equation (5.21), we have
6
∂k,
n

l=1
Γl
ij∂l
7
= 1
2 (∂i⟨∂j, ∂k⟩+ ∂j⟨∂k, ∂i⟩−∂k⟨∂i, ∂j⟩
−⟨[∂i, ∂j], ∂k⟩−⟨[∂i, ∂k], ∂j⟩−⟨[∂j, ∂k], ∂i⟩) .
However, the smoothness condition implies that [∂i, ∂j] = 0 for any indices
i, j. Furthermore, by deﬁnition, gij = ⟨∂i, ∂j⟩, so by the linearity of the
metric on the left-hand side,
n

l=1
gklΓl
ij = 1
2 (∂igjk + ∂jgki −∂kgij) .
The proposition follows by multiplying (and contracting) by gkl, the com-
ponents of the inverse of (gkl).
□
For the reader who is familiar with the diﬀerential geometry of sur-
faces (especially one who has read [5]), he or she has already seen Propo-
sition 5.2.13 but in a more limited context. In [5, Section 7.2], the authors
talk about Gauss’s equations for a regular surface over a parametrization
⃗X. In that section, one sees that even though the normal vector to a sur-
face is not an intrinsic property, ⃗Xij · ⃗Xk is intrinsic and in fact is given by
the Christoﬀel symbols of the ﬁrst kind, which are precisely those in Equa-
tion (5.22), though with n = 2. This is not a mere coincidence. In deﬁning
the Levi-Civita connection, that we might want ∇to be compatible with

214
5. Introduction to Riemannian Geometry
g made intuitive sense. However, the stipulation that we would want ∇to
be symmetric may have seemed somewhat artiﬁcial at the time. It is very
interesting that the two conditions in Theorem 5.2.11 lead to Christoﬀel
symbols that match those deﬁned for surfaces in classical diﬀerential ge-
ometry.
It is possible to develop a theory of embedded submanifolds M m of Rn
following the theory of regular surfaces in R3. Mimicking the presentation
in [5, Section 7.2], if ⃗X is a parametrization of a coordinate patch of M,
then, by setting
∂2 ⃗X
∂xi∂xj =
m

k=1
Γk
ij
∂⃗X
∂xk + (Normal component),
the components Γk
ij are again the Christoﬀel symbols of the second kind,
given by the same formula in Equation (5.22). This shows that for subman-
ifolds of a Euclidean space, the Levi-Civita connection on a Riemannian
manifold is essentially the ﬂat connection on Rn restricted to the manifold.
We ﬁnish this section with a comment on the divergence operator on
tensors introduced in Problem 5.1.10. We will show in Problem 5.2.16 that,
using the Levi-Civita connection, the divergence operator on a vector ﬁeld
X ∈X(M) can be written as
div X = Xi
;i.
This motivates, ﬁrst, the deﬁnition of the divergence of any tensor T of
type (r, s), with r ≥1, on a Riemannian manifold. If T has components
T i1···ir
j1···js in a coordinate system, then the divergence of T , written div T or
∇· T , is the tensor ﬁeld of type (r −1, s) with component functions
T αi2···ir
j1j2···js ;α = ∇∂αT αi2···ir
j1···js .
Of course, one can just as easily take the divergence with respect to any
contravariant index but one must specify which index. If the index is not
speciﬁed, we assume the divergence is taken with respect to the ﬁrst index.
One can also deﬁne the divergence of a covariant index by raising that
index ﬁrst. Thus, for example, if ω is a 1-form, then
div ω = (gijωj);i.
(5.23)
Problem 5.2.16 shows that whether one raises the index before or after the
covariant derivative is irrelevant.

5.2. Connections and Covariant Differentiation
215
Problems
5.2.1. Consider the special case of the manifold M = R3. Let X be the constant
vector ﬁeld ⃗v, and let X(R3) be the space of vector ﬁelds R3 →R3. Show
that the usual partial derivative D⃗v applied to X(R3) satisﬁes conditions
2 and 3 of Deﬁnition 5.2.1.
5.2.2. Recall the permutation symbol deﬁned in Equation (2.30). Let M be a
three-dimensional manifold equipped with a symmetric aﬃne connection.
Let A and B be vector ﬁelds on M. Show that
εijkAj;i = εijk ∂Aj
∂xi
and that
(εijkAjBk);i = εijkAj;iBk −εijkAkBj;i.
If M = R3, explain how the latter formula is equivalent to ⃗∇· ( ⃗A × ⃗B) =
(⃗∇× A) · ⃗B −⃗A · (⃗∇× ⃗B).
5.2.3. Let ∇be a connection on a vector bundle ξ over a smooth manifold M.
Prove that if X = ˜X and Y = ˜Y over a neighborhood of p, then
∇XY

p = ∇˜
X ˜Y

p.
5.2.4. Let ∇be a connection on a vector bundle ξ over a smooth manifold M.
Use the result of Problem 5.2.3 to show that ∇XY |p depends only on Xp
and the values of Y in a neighborhood of p.
5.2.5. Prove Proposition 5.2.5.
5.2.6. Prove that the Levi-Civita connection for the Euclidean space Rn is such
that ∇XY = X(Y k)∂k.
5.2.7. Consider the open ﬁrst quadrant U = {(u, v) ∈R2 | u > 0, v > 0}, and
equip U with the metric
(gij) =
⎛
⎝
1
1
√
u2+v2
1
√
u2+v2
1
u2
⎞
⎠.
Calculate the Christoﬀel symbols for the associate Levi-Civita connection.
5.2.8. Consider the cylinder in S2 × R in R4 given by the parametrization
F(u1, u2, u3) = (cos u1 sin u2, sin u1 sin u2, cos u2, u3)
and equip it with the metric induced from R4. Over the open coordinate
patch U = (0, 2π) × (0, π) × R, calculate the metric coeﬃcients and the
Christoﬀel symbols for the Levi-Civita connection.
5.2.9. Consider the unit sphere S3 as a submanifold of R4 with the induced
metric. Consider the coordinate patch on S3 given by the parametrization
in 5.1.11(a). Calculate one nonzero Christoﬀel symbol Γi
jk. (It would be
quite tedious to ask one to calculate all of the symbols since there could
be as many as 27 of them.) [Hint: Show that the conditions of Problem
5.2.13 apply to this coordinate patch and use the result.]

216
5. Introduction to Riemannian Geometry
5.2.10. Let M be a smooth manifold equipped with an aﬃne connection ∇. Let
ω ∈Ωk(M) be a k-form. Show that ∇ω is a (k+1)-form and that ∇ω = dω
regardless of the choice of connection.
5.2.11. Finish calculating directly the Christoﬀel symbols in Example 5.2.6.
5.2.12. Let (M, g) be a Riemannian manifold. Prove that a connection ∇satisﬁes
∇g = 0 identically if and only if Equation (5.17) holds where g = ⟨, ⟩.
5.2.13. Let (M, g) be a Riemannian manifold and let U be an orthogonal coor-
dinate patch, i.e., gij = 0 if i ̸= j over U.
Let ∇be the Levi-Civita
connection on M.
(a) Prove that on U the Christoﬀel symbols Γi
jk = 0 unless i = j, j = k,
or i = k.
(b) Show that ∇can be speciﬁed on U by 2n2 −n smooth functions, i.e.,
there are at most that many distinct nonzero Christoﬀel symbols.
(c) Show that
Γi
jj = ±1
2gii ∂gjj
∂xi ,
and
Γi
ij = 1
2gii ∂gii
∂xj
where there is no summation in either of these formulas and where
the sign of ± is +1 if i = j and −1 if i ̸= j.
5.2.14. Let M be a smooth manifold, and let ∇be a connection on TM. Deﬁne
a map τ : X(M) × X(M) →X(M) by
τ(X, Y ) = ∇XY −∇Y X −[X, Y ].
(a) Show that τ is a tensor ﬁeld of type (1, 2).
(b) The connection ∇is called symmetric if its torsion vanishes identi-
cally. Prove that ∇is symmetric if and only if over every coordinate
patch U, the component functions satisfy Γi
jk = Γi
kj.
5.2.15. Let ∇be an aﬃne connection on M.
Prove that ∇+ A is an aﬃne
connection, where A is a (1, 2)-tensor ﬁeld. Conversely, prove that every
aﬃne connection is of the form ∇+ A for some (1, 2)-tensor ﬁeld A.
5.2.16. Consider the divergence operator introduced in Problem 5.1.10 and dis-
cussed at the end of this section.
(a) Show from the deﬁnition in Problem 5.1.10 that
div X = Xi
;i,
where we’ve used the Levi-Civita connection to take the covariant
derivative.
(b) Consider the deﬁnition in Equation (5.23) for the divergence on a
1-form. Show that
div ω = (gijωj);i = gijωj;i.

5.2. Connections and Covariant Differentiation
217
5.2.17. Let f ∈C∞(M) be a smooth function on a manifold M equipped with
any aﬃne connection ∇. Show that
f;i;j −f;j;i = −τ m
ij f;m,
where τ is the torsion tensor from Exercise 5.2.14. Conclude that if ∇is
symmetric, then so is f;i;j.
5.2.18. Let M be a smooth manifold, let η ∈Ω2(M) be a 2-form, and let ∇be
any symmetric connection on M. Show that in any coordinate system,
cαβγ = ηαβ;γ + ηβγ;α + ηγα;β = ∂γηαβ + ∂αηβγ + ∂βηγα.
(5.24)
Show that if we write η =
1
2ηαβdxα ∧dxβ, then the left-hand side of
Equation (5.24) is the component of dη in the basis dxα ∧dxβ ∧dxγ in
the sense that
dω = cαβγdxα ∧dxβ ∧dxγ,
where we sum over all α, βγ = 1, . . . , n.
5.2.19. Let (M, g) be a Riemannian metric with Levi-Civita connection ∇. Show
that over every coordinate patch,
∂(ln √det g)
∂xk
= Γj
jk,
where one sums over j on the right-hand side.
[Hint: Use a result in
Problem C.6.5.]
5.2.20. Let ∇be an aﬃne connection on M, and let U be a coordinate patch
on M.
(a) Show that there exists a unique matrix of 1-forms ωj
i deﬁned on U
such that
∇X∂i = ωj
i (X)∂j
for all X ∈X(M). (The matrix ωj
i is called the connection 1-forms
for this coordinate system.)
(b) Suppose that (M, g) is a Riemannian manifold.
Show that ∇is
compatible with the metric g if, over any coordinate system U,
gjkωk
i + gikωk
j = dgij.
5.2.21. The Lie Derivative. There exists an alternative way of deﬁning a deriva-
tive on tensor ﬁelds on a manifold M. This exercise introduces the Lie
derivative in terms of coordinates and then establishes a coordinate-free
interpretation.
Let M be a smooth manifold, and let T i1···ir
j1···js be the components of a tensor
ﬁeld T deﬁned over a coordinate system (U, x) of M. Let X ∈X(M) be
a vector ﬁeld on M. For a ﬁxed (small) t, the ﬂow Ft of X on M can be
expressed in coordinates by the transformation
xi −→¯xi = xi + tXi(x1, . . . , xn).
(5.25)

218
5. Introduction to Riemannian Geometry
Let T i1···ir
j1···js be the components of a tensor T in the coordinates (xi), and
for a ﬁxed t, denote by T
k1···kr
l1···ls the components of T under the coordinate
transformation in Equation (5.25). We deﬁne the Lie derivative LXT as
the tensor of type (r, s) with the components
(LXT )i1···ir
j1···js = lim
t→0
1
t

T i1···ir
j1···js (xk) −T
i1···ir
j1···js(xk)

.
Prove the following results about the Lie derivative:
(a) The components of the Lie derivative satisfy
(LXT)i1···ir
j1···js = ∂T i1···ir
j1···js
∂xh
Xh −
r

α=1
T
i1···iα−1miα+1···ir
j1···js
∂Xiα
∂xm
+
s

β=1
T i1···ir
j1···jβ−1mjβ+1···js
∂Xm
∂xjβ .
(b) If f ∈C∞(M) is a smooth function, then LXf is the function such
that
(LXf)(p) = dfp(Xp).
(c) If Y ∈X(M) is a vector ﬁeld on M, then
LXY = d
dt(dF(−t)(Y ))

t=0 = [X, Y ].
(d) If η ∈Ω1(M) is a 1-form on M, then
LXη = d
dt(F ∗
t η)

t=0.
(e) If A and B are any tensor ﬁelds on M, then
LX(A ⊗B) = (LXA) ⊗B + A ⊗(LXB).
(The properties from (b)–(e) recursively show how to deﬁne the Lie deriva-
tive of any tensor ﬁeld on M in a coordinate-free way.)
5.2.22. Properties of the Lie Derivative. Let α ∈Ωk(M) be a k-form on M. Prove
the following properties of the Lie derivative.
(a) LXα = d(iXα) + iX(dα), where iX is the contraction operation de-
ﬁned in Problem 5.1.10.
(b) LX(α ∧β) = (LXα) ∧β + α ∧(LXβ) for all β ∈Ωl(M).
(c) (LXα)(X1, . . . , Xk) = LX(α(X1, . . . , Xk)) −"k
i=1 α(X1, . . . ,
[X, Xi], . . . , Xk).
(d) LX(dα) = d(LXα).
(e) LX(iXα) = iX(LXα).
(f) L[X,Y ]α = LXLY α −LY LXα
[Hint: Recall the description of 1k V as a subspace of V ⊗k, as described
in Section C.5.2.]

5.3. Vector Fields Along Curves: Geodesics
219
γ
Figure 5.4. A nonextendable vector ﬁeld on a curve.
5.3
Vector Fields Along Curves: Geodesics
Suppose we think of the trajectory of a particle on a manifold M. One
would describe it as curve γ(t) on M. Furthermore, in order to develop a
theory of dynamics on manifolds, one would need to be able to make sense
of the acceleration of the curve or of higher derivatives of the curve. In
this section, we deﬁne vector ﬁelds on curves on manifolds. Once we deﬁne
a covariant derivative of a vector ﬁeld on a curve, we can then discuss
parallel vector ﬁelds on the curve and the acceleration ﬁeld along the curve.
We then show that deﬁning a geodesic as a curve whose acceleration is
identically 0 leads to the classical understanding of a geodesic as a path of
minimum length in some sense.
5.3.1
Vector Fields Along Curves
Deﬁnition 5.3.1. Let M be a smooth manifold, and let γ : I →M be a
smooth curve in M, where I is an interval in R. We call V a vector ﬁeld
along γ if for each t ∈I, V (t) is a tangent vector in Tγ(t)M and if V deﬁnes
a smooth map I →T M. We denote by Xγ(M) the set of all smooth vector
ﬁelds on M along γ.
A vector ﬁeld along a curve is not necessarily the restriction of a vector
ﬁeld on M to γ(I). For example, whenever a curve self-intersects, γ(t0) =
γ(t1), with t0 ̸= t1, but since V (t0) ̸= V (t1) there exists no vector ﬁeld
Y on M such that V (t) = Yγ(t) for all t ∈I (see Figure 5.4). If V is the
restriction of a vector ﬁeld Y , then we say that V is induced from Y or
that V extends to Y .

220
5. Introduction to Riemannian Geometry
Proposition 5.3.2. Let M be a smooth manifold with an aﬃne connection ∇,
and let γ : I →M be a smooth curve on M. There exists a unique operator
Dt : Xγ(M) →Xγ(M) (also denoted by D
dt) such that
1. Dt(V + W) = DtV + DtW for all V, W ∈Xγ(M);
2. Dt(fV ) = df
dtV + fDtV for all V ∈Xγ(M) and all f ∈C∞(M);
3. if V extends to a vector ﬁeld Y ∈X(M), then DtV = ∇γ′(t)Y .
Note that the last condition makes sense by the fact that ∇XY |p only
depends on the values of Y in a neighborhood of p and on the value of Xp
(see Exercise 5.2.4).
Before proving Proposition 5.3.2, we introduce the dot notation for
derivatives. The only purpose is to slightly simplify our equations’ notation.
If x(t) is a real-valued function of a real variable, we write
˙x(t)
def
= x′(t) = dx
dt
and
¨x(t)
def
= x′′(t) = d2x
dt2 .
The dot notation is common in physics in the context of taking derivatives
with respect to time. Therefore, ˙x is usually used when one uses the letter
t as the only independent variable for the function x.
Proof (of Proposition 5.3.2): Let us ﬁrst suppose that an operator Dt with
Properties 1–3 exists. Let U be a coordinate patch of M with coordinates
x = (x1, . . . , xn). For any V ∈Xγ(M), write V = vi∂i where vi ∈C∞(I)
are smooth functions over I. By Conditions 1 and 2, we have
DtV = ˙vj∂j + vjDt(∂j).
Now if we write γ(t) = (γ1(t), . . . , γn(t)) for the coordinate functions of γ
over U, then γ′(t) = "n
i=1 ˙γi∂i. Thus, by Condition 3,
Dt(∂j) = ∇γ′(t)∂j =
n

i=1
˙γi∇∂i∂j = ˙γiΓk
ij∂k.
Hence, we deduce the following formula for DtV in coordinates over U:
DtV =
dvj
dt ∂j
	
+

Γk
ij
dγi
dt vj∂k
	
=

˙vk + Γk
ij ˙γivj
∂k.
(5.26)
Equation (5.26) shows that if there does exist an operator satisfying
properties Conditions 1–3, then the operator is unique. To prove existence

5.3. Vector Fields Along Curves: Geodesics
221
over all of M, we deﬁne Dα
t by Equation (5.26) on each coordinate chart
Uα. However, since Dα
t is unique on each coordinate chart, then Dα
t = Dβ
t
over Uα ∩Uβ if Uα and Uβ are overlapping coordinate charts. Hence, as
α ranges over all coordinate charts in the atlas, the collection of operators
Dα
t extends to a single operator Dt over all of M.
□
Note that Equation (5.26) in the above proof gives the formula for
Dt over a coordinate patch of M.
In particular, the expression in the
parentheses on the right gives the component functions (in the index k) for
DtV .
Deﬁnition 5.3.3. The operator Dt : Xγ(M) →Xγ(M) deﬁned in Proposition
5.3.2 is called the covariant derivative along γ.
In the context of Riemannian manifolds, the covariant derivative along
a curve has the following interesting property.
Proposition 5.3.4. Let γ be a smooth curve on a Riemannian manifold (M, g)
equipped with the Levi-Civita connection. Write g = ⟨, ⟩. Let V and W be
vector ﬁelds along γ. Then
d
dt⟨V, W⟩= ⟨DtV, W⟩+ ⟨V, DtW⟩.
Proof: (Left as an exercise for the reader. See Problem 5.3.10.)
□
The notion of a vector ﬁeld along a curve (in a manifold M) leads us
immediately to two useful notions: parallel transport and acceleration.
Deﬁnition 5.3.5. Let M be a smooth manifold with an aﬃne connection ∇,
and let γ : I →M be a smooth curve on M. A vector ﬁeld V along γ is
called parallel if DtV = 0 identically.
The existence of parallel vector ﬁelds on a curve amounts to the solv-
ability of a system of diﬀerential equations.
Proposition 5.3.6 (Parallel Transport). Let M be a smooth manifold with an
aﬃne connection ∇, and let γ : I →M be a smooth curve on M, where
I is a compact interval of R. Let t0 ∈I, set p = γ(t0), and let V0 be any
vector in TpM. There exists a unique vector ﬁeld of M along γ that is
parallel and has V (t0) = V0.

222
5. Introduction to Riemannian Geometry
Proof: Suppose ﬁrst that M is a manifold that is covered with a single
coordinate system x = (x1, . . . , xn).
By Equation (5.26), the condition
DtV = 0 means that
˙vk + Γk
ij ˙γivj = 0
for all k = 1, . . . , n.
(5.27)
The values Γk
ij depend on the position of γ(t) as do the derivatives ˙γi(t),
but neither of these depend on the functions vi(t). Hence, Equation (5.27)
is a system of linear, homogeneous, ordinary, diﬀerential equations in the
n functions vi(t). By a standard result of ordinary diﬀerential equations
(see [18, Appendix A] or [3, Section 8]), given an initial value t = t0 and
initial conditions vi(t0) = vi
0, there exists a unique solution to the system
of equations satisfying these initial conditions. (The particular form of the
nonautonomous system from Equation (5.27) and the hypothesis that I
is compact imply that the system satisﬁes the Lipschitz condition, which
establishes the uniqueness of the solutions.) Hence V exists and is unique.
Now suppose that M cannot be covered by a single coordinate chart.
We only need to consider coordinate charts that cover γ(I).
But since
γ(I) is compact, we can cover it with only a ﬁnite number of coordinate
charts.
However, on each of these charts, we have seen that there is a
unique parallel vector ﬁeld, as described. By identifying the vector ﬁelds
over each coordinate chart, we obtain a single vector ﬁeld over all of γ that
is parallel to V0.
□
Deﬁnition 5.3.7. The vector ﬁeld V in Proposition 5.3.6 is called the parallel
transport of V0 along γ.
It is important to note that the parallel transport of V0 from a point p
to a point q along two diﬀerent paths generally results in diﬀerent vectors in
TqM. In Figure 5.5, the tangent vector V0 at p produces diﬀerent tangent
vectors at q when transported along the black curve versus along the gray
curve. One says that parallel transport is nonintegrable. However, it is
not hard to see, either geometrically or by solving Equation (5.27), that
in Rn parallel transport does not depend on the path.
Therefore, this
nonintegrability of parallel transport characterizes the notion of curvature,
as we will see in the following section.
As a second application of the covariant derivative along a curve, we
ﬁnally introduce the notion of acceleration of a curve on a manifold.
Deﬁnition 5.3.8. Let M be a smooth manifold with an aﬃne connection
and let γ : I →M be a smooth curve on M. For all t ∈I, we deﬁne the
acceleration of γ on M as the covariant derivative Dtγ′(t) of γ′(t) along γ.

5.3. Vector Fields Along Curves: Geodesics
223
  p
q
V0
Figure 5.5. Path dependence of parallel transport.
Example 5.3.9. With the deﬁnition of the acceleration, we are in a position
to be able to phrase Newton’s second law of motion on a manifold. In
R3, Newton’s law states that if a particle has constant mass m and is
inﬂuenced by the exterior forces ⃗Fi, then the particle follows a path ⃗x(t)
that satisﬁes "
i ⃗Fi = m⃗x′′. Translated into the theory of manifolds, if a
force (or collection of forces) makes a particle move along some curve γ,
then writing F as the vector ﬁeld along γ that describes the force, γ must
satisfy
mDtγ′(t) = F(t).
The acceleration is itself a vector ﬁeld along the curve γ so the notions
of all the higher derivatives are deﬁned as well.
5.3.2
Geodesics
Intuitively speaking, a geodesic on a manifold is a curve that generalizes
the notion of a straight line in Rn. The seemingly simple task of deﬁning
a geodesic is surprisingly diﬃcult.
Only now, do we possess the neces-
sary background to do so. Even in Euclidean geometry, though everyone
“knows” what a straight line is, even Euclid’s original deﬁnitions for a
straight line do not satisfy today’s standards of precision. We introduce
geodesics using two diﬀerent approaches, each taking a property of straight
lines in Rn and translating it into the context of manifolds.
Deﬁnition 5.3.10. Let M be a smooth manifold with an aﬃne connection ∇.
A curve γ : I →M is called a geodesic if its acceleration is identically 0,
i.e., Dtγ′(t) = 0.

224
5. Introduction to Riemannian Geometry
Note that this deﬁnition does not require a metric structure on M,
simply an aﬃne connection. One should also observe that this deﬁnition
relies on a speciﬁc parametrization of γ. The deﬁnition is modeled after
the fact that one can parametrize a straight line in Rn by ⃗γ(t) = ⃗p + t⃗v
for constant vectors ⃗p and ⃗v and that with this parametrization ⃗γ ′′(t) =
⃗0. However, the curve ⃗x(t) = ⃗p + t3⃗v traces out the same set of points
but ⃗x ′′(t) = 3t2⃗v, which is not identically 0. Despite this, one can leave
Deﬁnition 5.3.10 as it is and keep in mind the role of the parametrization.
Proposition 5.3.11 (Geodesic Equations). Let M be a smooth manifold equipped
with an aﬃne connection, and let x = (x1, . . . , xn) be a system of coordi-
nates on a chart U.
A curve γ is a geodesic on U if and only if the
coordinate functions γ(t) = (γ1(t), . . . , γn(t)) satisfy
d2γi
dt2 + Γi
jk(γ(t))dγj
dt
dγk
dt = 0
for all i.
(5.28)
Proof: This follows immediately from Equation (5.26).
□
Equation (5.28) for a geodesic is a second-order system of ordinary
diﬀerential equations in the functions γi(t). Setting vi(t) = ˙γi, one can
write Equation (5.28) as a ﬁrst-order system in the 2n functions γi and
vi by

˙γi = vi,
˙vi = −Γi
jk(γ(t))vjvk.
This system is now ﬁrst-order and non-linear but autonomous (does not
depend explicitly on t). By standard theorems in diﬀerential equations [3,
Theorems 7.3, 7.4], we immediately conclude the following theorem.
Theorem 5.3.12. Let M be a manifold with an aﬃne connection. For any
p ∈M, for any V ∈TpM, and for any t0 ∈R, there exists an open interval
I containing t0 and a unique geodesic γ : I →M satisfying γ(t0) = p and
γ′(t0) = V .
This theorem shows the existence of the curve γ by solving Equa-
tion (5.28) over a coordinate neighborhood. In this case, the interval I
may be limited by virtue of the fact that γ(I) ⊂U. It may be possible
to extend γ over other coordinate patches.
If γ(t1) for some t1 ∈I is
in another coordinate patch ¯U, then we can uniquely extend the geodesic
over ¯U as going through the point γ(t1) with velocity γ′(t1). We deﬁne a
maximal geodesic as a geodesic γ : I →M whose domain interval cannot

5.3. Vector Fields Along Curves: Geodesics
225
p
q
Figure 5.6. Two geodesics on a cylinder.
be extended. If γ is a maximal geodesic with γ(t0) = p and γ′(t0) = V for
some t0 ∈I, we call γ the geodesic with initial point p and initial velocity
V ∈TpM, and we denote it by γV .
Another deﬁning property of a straight line in Rn is that the shortest
path between two points is a straight line segment.
If we use the con-
cept of distance, we need a metric. Let (M, g) be a Riemannian metric
equipped with the Levi-Civita connection, and let γ be a geodesic on M.
By Proposition 5.3.4,
d
dt⟨γ′(t), γ′(t)⟩= 2⟨Dtγ′(t), γ′(t)⟩= 0,
so we can conclude the following initial result.
Proposition 5.3.13. A geodesic on a Riemannian manifold has constant speed.
Now on a Riemannian manifold, an alternate approach to deﬁning geo-
desics on a manifold is to call a geodesic a path of shortest length between
two points. However, this deﬁnition is not quite good enough, as Figure 5.6
indicates. Both curves connecting p and q are geodesics, but one is shorter
than the other. To be more precise, we call γ a geodesic connecting p1 and
p2 if there is an interval [t1, t2] such that γ(t1) = p1, γ(t2) = p2, and γ
minimizes the arclength integral
L =
% t2
t1

gij(γ(t))˙γi(t)˙γj(t) dt.
(5.29)

226
5. Introduction to Riemannian Geometry
Techniques of calculus of variations discussed in Appendix B produce
the diﬀerential equations for the curve γ that minimizes the arclength.
However, similar to optimization methods in regular calculus, the solutions
we obtain are local minima, which means in our case that there are no
small deviations of γ that produce a shorter path between p and q. It is
tedious to show, but Theorem B.3.1 implies that a curve γ that minimizes
the integral in Equation (5.29) must satisfy
d2γi
ds2 + Γi
jk
dγj
ds
dγk
ds = 0,
for i = 1, . . . , n,
(5.30)
where s is the arclength of γ. Proposition 5.3.11 along with Proposition
5.3.13 show that deﬁning a geodesic as having no acceleration is equivalent
to deﬁning it as minimizing length in the above sense.
Example 5.3.14 (Sphere). Consider the parametrization of the sphere given by
⃗X(x1, x2) = (R cos x1 sin x2, R sin x1 sin x2, R cos x2),
where x1 is the longitude θ in spherical coordinates and x2 is the angle
ϕ down from the positive z-axis. In Example 5.1.12, we determined the
coeﬃcients of the metric tensor. Then it is easy to calculate the Christoﬀel
symbols Γi
jk for the sphere. Equations (5.30) for geodesics on the sphere
become
d2x1
ds2 + 2 cot(x2)dx1
ds
dx2
ds = 0,
d2x2
ds2 −sin(x2) cos(x2)
dx1
ds
	2
= 0.
(5.31)
A geodesic on the sphere is now just a curve of the form ⃗γ(s) =
⃗X(x1(s), x2(s)) where x1(s) and x2(s) satisfy the system of diﬀerential
equations in Equation (5.31). Taking a ﬁrst derivative of ⃗γ(s) gives
⃗γ′(s) = R

−sin x1 sin x2 dx1
ds + cos x1 cos x2 dx2
ds ,
cos x1 sin x2 dx1
ds + sin x1 cos x2 dx2
ds , −sin x2 dx2
ds
	
,
and the second derivative, after simpliﬁcation using Equation (5.31), is
d2⃗γ
ds2 = −
8
sin2(x2)
dx1
ds
	2
+
dx2
ds
	29
⃗γ(s).

5.3. Vector Fields Along Curves: Geodesics
227
However, the term R2[sin2(x2)( dx1
ds )2 + ( dx2
ds )2] is the ﬁrst fundamental
form on
((x1)′(s), (x2)′(s)),
which is precisely the square of the speed of ⃗γ(s).
However, since the
geodesic is parametrized by arclength its speed is identically 1.
Thus,
Equation (5.31) leads to the diﬀerential equation
⃗γ′′(s) + 1
R2⃗γ(s) = 0.
Standard techniques with diﬀerential equations allow one to show that all
solutions to this diﬀerential equation are of the form
⃗γ(s) = ⃗a cos
 s
R

+⃗b sin
 s
R

,
where ⃗a and ⃗b are constant vectors. Note that ⃗γ(0) = ⃗a and that ⃗γ′(0) =
1
R⃗b. Furthermore, to satisfy the conditions that ⃗γ(s) lie on the sphere of
radius R and be parametrized by arclength, we deduce that ⃗a and ⃗b satisfy
∥⃗a∥= R,
∥⃗b∥= R, and
⃗a ·⃗b = 0.
Therefore, we ﬁnd that ⃗γ(s) traces out a great arc on the sphere that is the
intersection of the sphere and the plane through the center of the sphere
spanned by ⃗γ(0) and ⃗γ′(0).
There are many properties of lines that no longer hold for geodesics on
manifolds. For example, lines in Rn are (“obviously”) simple curves, i.e.,
they do not intersect themselves. In Example 5.3.14, we showed that the
geodesics on a sphere are arcs of great circles (equators). In this case, a
maximal geodesic is a whole circle that, as a closed curve, is still simple.
In contrast, Figure 5.7 of a distorted sphere shows only a portion of a
geodesic that is not closed and intersects itself many times. The problem
of ﬁnding closed geodesics on surfaces illustrates how central the study of
geodesics is in current research: in 1917, Birkhoﬀused techniques from
dynamical systems to show that every deformed sphere has at least one
closed geodesic [9]; in 1929, Lusternik and Schnirelmann improved upon
this and proved that there always exist three closed geodesics on a deformed
sphere [36]; and in 1992 and 1993, Franks and Bangert [6,22] proved that
there exist an inﬁnite number of closed geodesics on a deformed sphere.
However, a proof of the existence of a closed geodesic would not necessarily
help us construct one for any given surface.

228
5. Introduction to Riemannian Geometry
Figure 5.7. A nonclosed geodesic on a manifold.
We end this section by presenting the so-called exponential map. The-
orem 5.3.12 allows us to deﬁne a map, for each p ∈M, from the tangent
plane TpM to M by mapping V to a ﬁxed distance along the unique geo-
desic γV .
Deﬁnition 5.3.15. Let p be a point on a Riemannian manifold (M, g). Let
Dp be the set of tangent vectors V ∈TpM such that the geodesic γV , with
γV (0) = p, is deﬁned over the interval [0, 1]. The exponential map, written
expp, is the function
expp : Dp −→M,
V −→γV (1).
Lemma 5.3.16 (Scaling Lemma). Let V ∈TpM, and let c ∈R>0. Suppose that
γV (t) is deﬁned over (−δ, δ), with γV (0) = p. Then γcV (t) is deﬁned over
the interval (−δ/c, δ/c), and
γcV (t) = γV (ct).
Proof: (Left as an exercise for the reader. See Problem 5.3.11.)
□
By virtue of the scaling lemma, we can write for the geodesic through
p along V ,
γV (t) = expp(tV ).
Proposition 5.3.17. For all p ∈M, there exists a neighborhood U of p on M
and a neighborhood D of the origin in TpM such that expp : D →U is a
diﬀeomorphism.
Proof: The diﬀerential of expp at 0 is a linear transformation d(expp)0 :
T0(TpM) →TpM. However, since TpM is a vector space, then the tangent

5.3. Vector Fields Along Curves: Geodesics
229
space T0(TpM) is naturally identiﬁed with TpM. Thus, d(expp)0 is a linear
transformation on the vector space TpM. The proposition follows from the
Inverse Function Theorem (Theorem 1.4.5) once we show that d(expp)0 =
(expp)∗is invertible.
We show this indirectly using the chain rule. Let V be a tangent vector
in V ∈TpM, and let f : (−δ, δ) →TpM be the curve f(t) = tV . The
function expp ◦f is a curve on M. Then
d(expp ◦f)0 = d expp(tV )
dt

t=0
= dγV (t)
dt

t=0
= V.
However, by the chain rule, we also have
d(expp ◦f)0 = d(expp)0df0 = (expp)∗V.
Hence, for all V ∈TpM, we have (expp)∗V = V . Hence, (expp)∗is in fact
the identity transformation so it is invertible, and the proposition follows.□
Now if {eμ} is any basis of TpM, the exponential map sets up a coor-
dinate system on a neighborhood of p on M deﬁned by
expp(Xμeμ).
We call this the normal coordinate system at p with respect to {eμ}. If q is
a point in the neighborhood U, as in Proposition 5.3.17, then q is the image
of a unique tangent vector Xq under expp. The coordinates of q are Xμ
q .
Interestingly enough, the coeﬃcients of the Levi-Civita connection van-
ish at p in the normal coordinate system Xμ at p. Consider a geodesic on
M from p to q given by c(t) = expp(tXμ
q eμ), which in coordinates is just
Xμ(t) = tXμ
q . From the geodesic equation,
d2Xμ
dt2
+ Γμ
λν
dXλ
dt
dXν
dt
= Γμ
λν(tXi
q)Xλ
q Xν
q .
Setting t = 0, we ﬁnd that Γμ
λν(0)Xλ
q Xν
q = 0 for any q. Thus, by appropri-
ate choices of q, we determine that Γμ
λν(0) = 0, which are the components
of the Levi-Civita connection at p in the normal coordinate system.
The exponential map allows us to redeﬁne some common geometric
objects in Rn in the context of Riemannian manifolds. Notice ﬁrst that by
Proposition 5.3.13, the arclength from p to expp(V ) along γV (t) is ∥V ∥p.
Now, let r > 0 be a positive real number and Br(0) be the open ball of
radius r centered at the origin in TpM. If r is small enough that Br(0)

230
5. Introduction to Riemannian Geometry
is contained in the neighborhood U from Proposition 5.3.17, then we call
expp(Br(0)) the geodesic ball of radius r centered at p. If the sphere Sr(0)
of radius r centered at 0 in TpM is contained in U, then we call expp(Sr(0))
the geodesic sphere of radius r centered at p.
5.3.3
Geodesics on Pseudo-Riemannian Manifolds
We point out that all of our discussion about geodesics carries through
without considerable change for pseudo-Riemannian manifolds. None of
the formulas for the Christoﬀel symbols or for the equations of geodesics
rely on the positive-deﬁnite nature of a metric. A few relevant changes
arise in the following contexts:
• One can no longer deﬁne the length of a tangent vector if gp(V, V )<0.
• One cannot deﬁne the arclength of a curve γ if gγ(t)(γ′(t), γ′(t)) < 0
for some t ∈I.
• One might not be able to deﬁne the volume of a region R of M.
Despite these possible obstructions, the equations for geodesics still satisfy
the existence and uniqueness properties of Theorem 5.3.12. Furthermore,
like Proposition 5.3.13, geodesics on pseudo-Riemannian manifolds have a
constant ⟨γ′(t), γ′(t)⟩. Thus, geodesics come in three categories depending
on the sign of ⟨γ′(t), γ′(t)⟩= gij ˙γi(t)˙γj(t).
In the context of a Lorentz spacetime R1,3, where the metric has index 1,
we say that a geodesic is
• a timelike geodesic if ⟨γ′(t), γ′(t)⟩< 0;
• a null geodesic if ⟨γ′(t), γ′(t)⟩= 0;
• a spacelike geodesic if ⟨γ′(t), γ′(t)⟩> 0.
Problems
5.3.1. Let S be a regular surface in R3, and let ⃗X be a parametrization of a
coordinate chart U of S. Let ∇be the Levi-Civita connection on S with
respect to the ﬁrst fundamental form metric. Let ⃗γ(t) = γ(t) be a curve
on S. Prove that the acceleration Dtγ′(t) is the orthogonal projection of
⃗γ′′(t) onto the tangent plane to S at γ(t).
5.3.2. Consider the torus parametrized by
⃗X(u, v) = ((a + b cos v) cos u, (a + b cos v) sin u, b sin v),

5.3. Vector Fields Along Curves: Geodesics
231
where a > b. Show that the geodesics on a torus satisfy the diﬀerential
equation
dr
du = 1
Cbr

r2 −C2
b2 −(r −a)2,
where C is a constant and r = a + b cos v.
5.3.3. Find the diﬀerential equations that determine geodesics on a function
graph z = f(x, y).
5.3.4. If ⃗X : U →R3 is a parametrization of a coordinate patch on a regular
surface S such that g11 = E(u), g12 = 0, and g22 = G(u), show that
(a) the u-parameter curves (i.e., over which v is a constant) are geodesics;
(b) the v-parameter curve u = u0 is a geodesic if and only if Gu(u0) = 0;
(c) the curve ⃗x(u, v(u)) is a geodesic if and only if
v = ±
%
C

E(u)

G(u)

G(u) −C2 du,
where C is a constant.
5.3.5. Pseudosphere. Consider a surface with a set of coordinates (u, v) deﬁned
over the upper half of the uv-plane, i.e., on H = {(u, v) ∈R2 | v > 0},
such that the metric tensor is
(gij) =
1
0
0
e2v
	
.
Prove in this coordinate system that all the geodesics appear in the H as
vertical lines or semicircles with center on the u-axis.
5.3.6. Let (M, g) be a two-dimensional Riemannian manifold. Suppose that on
a coordinate patch U with coordinates x = (x1, x2), the metric is given by
g11 = 1, g22 = (x2)2, and g12 = g21 = 0. Show that the geodesics of M on
U satisfy the existence and uniqueness of
x1 = a sec(x2 + b).
5.3.7. Let (M, g) be a four-dimensional manifold with a Lorentzian metric that
over a particular coordinate system has the matrix
gij =
⎛
⎜
⎜
⎝
1
0
0
0
0
1
0
0
0
0
1
gt
0
0
gt
k2 −g2t2
⎞
⎟
⎟
⎠.
Show that the geodesics that have the initial condition (x, y, z, t) = (0, 0, 0, 0)
when s = 0 satisfy
x = at,
y = bt,
and
z = −1
2gt2 + ct.
Use this to give a physical interpretation of this metric.

232
5. Introduction to Riemannian Geometry
  Figure 5.8. Mercator projection.
5.3.8. The Mercator projection used in cartography maps the globe (except the
north and south poles, S2 −{(0, 0, 1), (0, 0, −1)}) onto a cylinder, which
is then unrolled into a ﬂat map of the earth.
However, one does not
necessarily use the radial projection as shown in Figure 5.8. Consider a
map f from (x, y) ∈(0, 2π) × R to the spherical coordinates (θ, φ) ∈S2 of
the form (θ, φ) = f(x, y) = (x, h(y)).
(a) Recall that the usual Euclidean metric on S2 is
g =
sin2 φ
0
0
1
	
.
The Mercator projection involves the above function f(x, y), such
that h(y) gives a pull-back f ∗(g) that is a metric with a line element
of the form ds2 = G(y)dx2 +G(y)dy2. Prove that h(y) = 2 cot−1(ey)
works, and determine the corresponding function G(y).
(b) Show that the geodesics on R2 equipped with the metric obtained
from this h(y) are of the form
sinh y = α sin(x + β)
for some constants α and β.
5.3.9. Consider the Poincar´e ball Bn
R from Problem 5.1.9. Prove that the geodes-
ics in the Poincar´e ball are either straight lines through the origin or circles
that intersect the boundary ∂Bn
R perpendicularly. (The Poincar´e ball is an
example of a hyperbolic geometry. In this geometry, given a “straight line”

5.3. Vector Fields Along Curves: Geodesics
233
Bn
R
Figure 5.9. A few geodesics in the Poincar´e disk.
(geodesic) L and a point p not on L, there exists a nonempty continuous
set of lines (geodesics) through p that do not intersect L. See Figure 5.9)
5.3.10. Prove Proposition 5.3.4.
[Hint: Use Equation (5.26) and the fact that
since the Levi-Civita connection is compatible with g, then gij;k = 0.]
5.3.11. Prove Lemma 5.3.16.
5.3.12. (ODE) Consider the usual sphere S2 of radius R in R3. In the coordinate
patch where (θ, φ) ∈(0, 2π) × (0, π), the Christoﬀel symbols are given in
Equation (5.31) of Example 5.3.14, where we use the coordinates (θ, φ) =
(x1, x2). Consider a point p on the sphere given by P = (θ0, φ0). Let V0
be a vector in TpS2 with coordinates (V 1
0 , V 2
0 ).
(a) Show that the stated Christoﬀel symbols used in Equation (5.31) are
correct.
(b) Calculate the coordinates of the parallel transport V (t) of V0 along
the curve γ(t) = (θ0, t), using the initial condition t0 = φ0. Show
that the length of the tangent vectors V (t) does not change.
(c) Calculate the coordinates of the parallel transport V (t) of V0 along
the curve γ(t) = (t, φ0), using the initial condition t0 = θ0. Show
that ∥V (t)∥2 is constant.
5.3.13. Determine the geodesics in a coordinate patch on a pseudo-Riemannian
manifold with metric with the line element
ds2 = x3 (dx0)2 −(dx1)2 −(dx2)2 −1
x3 (dx3)2.
5.3.14. Show that the locus of a geodesic on the n-sphere Sn (as a submanifold of
Rn+1) is the intersection of Sn with 2-planes that pass through the origin.

234
5. Introduction to Riemannian Geometry
5.3.15. Let M be a two-dimensional manifold, and suppose that on a coordinate
patch (x1, x2), the metric is of the form
g =
f(r)
0
0
f(r)
	
,
where r2 = (x1)2 + (x2)2.
Find the function f(r) that gives a ﬂat connection.
5.3.16. Let M be a pseudo-Riemannian manifold of dimension 3 with the line
element
ds2 = −dt2 +
1
1 −λr2 dr2 + r2dθ2,
where we assume r2 < 1/λ.
Show that the null geodesics satisfy the
relationship
 dr
dθ
	2
= r2(1 −λr2)(Cr2 −1),
where C is a constant. Use the substitution u = 1/r2 to solve this diﬀer-
ential equation, and show that the solutions are ellipses if we interpret r
and θ as the usual polar coordinates.
5.3.17. Let M be a pseudo-Riemannian manifold of dimension 4 that has a line
element of
ds2 = (1 −2gx)dt2 −
1
1 −2gxdx2 −dy2 −dz2,
where g >0 is a constant. Show that the curve deﬁned by (1−2gx) cosh2(gt)
= 1, y = z = 0 is a geodesic passing through the origin.
5.3.18. Determine the geodesics corresponding to the line element metric
ds2 = −xdt2 + 1
xdx2 + dy2 + dz2.
5.4
The Curvature Tensor
In the study of curves and surfaces in classical diﬀerential geometry, cur-
vature plays a central role. We approach the notion of curvature on Rie-
mannian manifolds in two diﬀerent but equivalent ways.
5.4.1
Coordinate-Dependent
The ﬁrst approach to curvature involves investigating mixed, partial, co-
variant derivatives. For smooth functions in Rn, mixed, second-order par-
tial derivatives are independent of the order of diﬀerentiation. Problem
5.2.17 showed that if a connection ∇on M is not symmetric, the same re-
sult is no longer true for the mixed, covariant, partial derivatives of smooth

5.4. The Curvature Tensor
235
functions on a manifold. We found that if f : M →R, then over a given
coordinate patch U, one has
f;j;i −f;i;j = τk
ijf;k,
where τ is the torsion tensor associated to ∇(see Problem 5.2.14), and the
coordinate components are
τk
ij = Γk
ij −Γk
ji.
(5.32)
If we repeat the exercise with a vector ﬁeld instead of a smooth function,
a new phenomenon appears.
Proposition 5.4.1. Let M be a smooth manifold equipped with an aﬃne con-
nection ∇. Let U be a coordinate patch on M, and let X be a vector ﬁeld
deﬁned over U.
Then, in components, the mixed, covariant derivatives
satisfy
Xi
;k;j −Xi
;j;k = Ki
jklXl + τm
jkXi
;m
where
Ki
jkl = ∂Γi
lk
∂xj −
∂Γi
lj
∂xk + Γi
hjΓh
lk −Γi
hkΓh
lj.
(5.33)
Proof: This is a simple matter of calculation. Starting from Xi
;j = ∂Xi/∂xj+
Γi
ljXl, we obtain
Xi
;j;k =
∂
∂xk
∂Xi
∂xj + Γi
ljXl
	
+ Γi
mkXm
;j −Γh
jkXi
;h
=
∂2Xi
∂xk∂xj +
∂Γi
lj
∂xk Xl + Γi
lj
∂Xl
∂xk
+ Γi
mk
∂Xm
∂xj + Γm
lj Xl
	
−Γh
jk
∂Xi
∂xh + Γi
lhXl
	
.
The result of the proposition follows after collecting and canceling like
terms in the resulting expression for Xi
;k;j −Xi
;j;k.
□
We point out that some authors deﬁne Ki
jkl as the negative of the
expression on the right-hand side of Equation (5.33) and some use the
indices of Ki
jkl in diﬀerent orders (e.g., write K i
l jk for our Ki
jkl). In this
text, we have chosen, with other authors, to use this presentation in order
to be able to mesh this approach with the coordinate-free approach of the
curvature tensor.
We can check the following crucial result.

236
5. Introduction to Riemannian Geometry
Proposition 5.4.2. The collection of functions Ki
jkl deﬁned in Equation (5.33)
form the components of a tensor of type (1, 3).
Proof: (This proposition relies on the coordinate-transformation properties
of the component functions Γi
jk given in Proposition 5.2.5. The proof is
left as an exercise for the reader.)
□
The functions Ki
jkl are the components of the so-called curvature tensor
associated to the connection ∇.
The components of the curvature tensor came into play when we con-
sidered the mixed, covariant, partial derivatives of a vector ﬁeld instead of
just a smooth function. One could then ask whether some new quantity
appears when one considers the mixed covariant partials of other tensors.
Surprisingly, the answer is no.
Theorem 5.4.3 (Ricci’s Identities). Let T i1···ir
j1···js be the components of a tensor
ﬁeld of type (r, s) over a coordinate patch of a manifold equipped with a
connection ∇. Then the mixed, covariant, partial derivatives diﬀer by
T i1···ir
j1···js;k;h −T i1···ir
j1···js;h;k =
r

α=1
Kiα
hkmT i1···iα−1miα+1···ir
j1···js
−
s

β=1
Km
hkjβT i1···ir
j1···jβ−1mjβ+1···js + τm
hk T i1···ir
j1···js;m.
Over the coordinate patch U, the components of the curvature tensor
satisfy the Bianchi identities.
Proposition 5.4.4 (Bianchi Identities). With Ki
jkl deﬁned as in (5.33) and τ i
jk
deﬁned as in Equation (5.32), then
Ki
jkl + Ki
klj + Ki
ljk = −τi
jk;l −τi
kl;j −τi
lj;k −τi
jm τ m
kl −τi
km τ m
lj −τi
lm τ m
jk
and
Ki
jkl;h + Ki
jlh;k + Ki
jhk;l = −τm
kl Ki
jmh −τm
lh Ki
jmk −τm
hk Ki
jml.
The second Bianchi identity is also called the diﬀerential Bianchi identity.
Proof: (Left as an exercise for the reader.)
□
In particular, if ∇is a symmetric connection, the Bianchi identities
reduce to
ﬁrst identity:
Ki
jkl + Ki
klj + Ki
ljk = 0,
second identity:
Ki
jkl;h + Ki
jlh;k + Ki
jhk;l = 0.

5.4. The Curvature Tensor
237
Now if (M, g) is a Riemannian manifold and ∇is the Levi-Civita con-
nection associated to g, then since ∇is symmetric, the torsion tensor τ is
identically 0 and the components of the curvature tensor are denoted by
Ri
jkl. This notation emphasizes the context of Riemannian manifold and
the use of the Levi-Civita connection.
By contracting with the metric tensor g, one obtains the components
of a tensor of type (0, 4) by writing
Rjklm = gimRi
jkl,
(5.34)
which one refers to as the Riemann (or covariant) curvature tensor. Not
all the components of Ri
jkl or of Rjklm are independent. We wish to de-
termine the number of independent component functions in Rjklm, which
by Equation (5.34) is also the number of independent component functions
of Ri
jkl.
By the deﬁnition from Equation (5.33), we see that Ri
jkl = −Ri
kjl and,
therefore, that
Rjklm = −Rkjlm.
(5.35)
Furthermore, the ﬁrst Bianchi identity gives
Rjklm + Rkljm + Rljkm = 0.
(5.36)
The compatibility condition of the Levi-Civita connection glm;k = 0 leads
to another relation. The fact that gij;k = 0 and Theorem 5.4.3 imply that
0 = −Rh
jklghm −Rh
jkmglh,
which is tantamount to
Rjklm = −Rjkml.
(5.37)
Equations (5.35) and (5.37) show that the covariant curvature tensor is
skew-symmetric in the ﬁrst two indices and also in the last two indices.
Furthermore, this skew-symmetry relation combined with the identity in
Equation (5.36) leads to
Rjklm = Rlmjk.
(5.38)
One can see this from
0 = Rjklm + Rkljm + Rljkm = −Rjkml −Rklmj + Rljkm
= Rkmjl + Rmjkl + Rlmkj + Rmklj + Rljkm
= 2Rkmjl −Rmjlk −Rlmjk + Rljkm
= 2Rkmjl + Rjlmk + Rljkm = 2Rkmjl −2Rjlkm.
Hence, Rkmjl = Rjlkm and relabeling the indices gives Equation (5.38).

238
5. Introduction to Riemannian Geometry
We now count the number of independent functions given the relations
in Equations (5.35), (5.36), and (5.37). There are ﬁve separate cases de-
pending on how many indices are distinct. By virtue of Equation (5.35),
the cases when all indices are equal or when three of the indices are equal
lead to identically 0 functions for the components of the covariant tensor. If
there are two pairs of equal indices, then we must have Riijj = 0 while the
quantities Rijij could be nonzero. In this case, the identities in Equations
(5.35), (5.36), and (5.38) explicitly determine all other possibilities with
two pairs of equation indices from Rijij. There are
n
2

ways to select the
pair {i, j} to deﬁne Rijij. If the indices have one pair of equal indices and
the other two indices are diﬀerent, then by Equations (5.35) and (5.37), the
only nonzero possibilities can be determined by Rijik (where i, j, and k
are all distinct). Hence, there are n
n−1
2

choices of independent functions
here. Lastly, suppose that all four indices are distinct. All the functions
for combinations of indices can be obtained from the relations, given the
functions for Rijkl and Riljk. Thus, there are 2
n
4

independent functions
in this case. In total, the covariant curvature tensor is determined by
n
2
	
+ n
n −1
2
	
+ 2
n
4
	
= 1
12n2(n2 −1)
independent functions.
It is interesting to note that for manifolds of dimension n = 2, there
is only one independent function in the curvature tensor, namely, R1212.
Equation (7.47) in [5] shows that the Gaussian curvature of the surface at
any point is equal to K = R1212/ det(gij). From this, we also ﬁnd that

R1
121
R1
122
R2
121
R2
122

= R1212
g21
−g11
g22
−g12
	
,
which gives the formula K = det(Ri
12j) for the Gaussian curvature of a
surface.
5.4.2
Coordinate-Free
A second and more modern approach to curvature on a Riemannian man-
ifold (M, g) deﬁnes the curvature tensor in a coordinate-free way, though
still from a perspective of analyzing repeated covariant diﬀerentiation. If
X, Y , and Z are vector ﬁelds on M, the diﬀerence in repeated covariant
derivatives is
∇X∇Y Z −∇Y ∇XZ.
(5.39)

5.4. The Curvature Tensor
239
Even with general vector ﬁelds in Rn, Equation (5.39) does not necessarily
cancel out. However, by Problem 5.2.6, for vector ﬁelds in Rn, ∇X∇Y Z =
X(Y (Zk))∂k, so
∇X∇Y Z −∇Y ∇XZ = ∇[X,Y ]Z.
(5.40)
Since the Levi-Civita connection is uniquely determined by the Riemannian
metric, Equation (5.40) holds for any manifold that is locally isometric to
Rn. Therefore, the condition in Equation (5.40) holds for ﬂat manifolds
and, since ∇XZ is not necessarily as simple for other manifolds, it might
not hold for manifolds not locally isometric to Rn. This motivates deﬁning
the quantity
R(X, Y )Z
def
= ∇X∇Y Z −∇Y ∇XZ −∇[X,Y ]Z.
(5.41)
The notation R(X, Y )Z emphasizes the understanding that for each vec-
tor ﬁeld X and Y , R(X, Y ) is an operator acting on Z. At ﬁrst glance,
R(X, Y )Z is just a smooth mapping X(M) × X(M) × X(M) →X(M),
smooth because the resulting vector ﬁeld is smooth.
However, more is
true.
Proposition 5.4.5. The function R(X, Y )Z deﬁned in Equation (5.41) is a
tensor ﬁeld of type (1, 3), which is antisymmetric in X and Y .
Proof: The antisymmetry property follows immediately from [Y, X] =
−[X, Y ] and Deﬁnition 5.2.1.
To prove the tensorial property, we need
only to show that R(X, Y )Z is multilinear over C∞(M) in each of the
three vector ﬁelds. We show linearity for the X variable, from which lin-
earity immediately follows for the Y variable. We leave it as an exercise
for the reader to prove linearity in Z.
Let f1, f2 ∈C∞(M). Then
R(f1X1 + f2X2, Y )Z = (f1∇X1 + f2∇X2)∇Y Z
−∇Y (f1∇X1 + f2∇X2)Z −∇[f1X1,Y ]+[f2X2,Y ]Z.
By Proposition 4.2.12(4), [fiXi, Y ] = fi[Xi, Y ] −Y (fi)Xi. Thus,
R(f1X1 + f2X2, Y )Z
= f1∇X1∇Y Z + f2∇X2∇Y Z −f1∇Y ∇X1Z −Y (f1)∇X1Z −f2∇Y ∇X2Z
−Y (f2)∇X2Z −∇f1[X1,Y ]−Y (f1)X1Z −∇f2[X2,Y ]−Y (f2)X2Z
= f1∇X1∇Y Z −f1∇Y ∇X1Z −f1∇[X1,Y ] + f2∇X2∇Y Z −f2∇Y ∇X2Z
−f2∇[X2,Y ] −Y (f1)∇X1Z −Y (f2)∇X2Z + Y (f1)∇X1Z + Y (f2)∇X2Z
= f1R(X1, Y )Z + f2R(X2, Y )Z.
□

240
5. Introduction to Riemannian Geometry
Deﬁnition 5.4.6. The tensor ﬁeld R is called the curvature tensor.
We connect this approach to the coordinate-dependent Deﬁnition 5.33
as follows. Let x be a coordinate system on a coordinate patch of M. By
the C∞(M)-linearity,
R(X, Y )Z = XiY jZk R(∂i, ∂j)∂k,
where X = Xi∂i and similarly for Y and Z. The components of R in local
coordinates are Rl
ijk, where
R
 ∂
∂xi , ∂
∂xj
	
∂
∂xk = Rl
ijk
∂
∂xl .
Now since [∂i, ∂j] = 0,
R(∂i, ∂j)∂k = ∇∂i∇∂j∂k −∇∂j∇∂i∂k
= ∇∂i

Γh
jk∂h

−∇∂j

Γh
ik∂h

= Γh
jk∇∂i∂h +
∂Γh
jk
∂xi ∂h −Γh
ik∇∂j∂h −∂Γh
ik
∂xj ∂h
=

Γh
jkΓl
ih −Γh
ikΓl
jh +
∂Γl
jk
∂xi −∂Γl
ik
∂xj

∂l,
from which we obtain
Rl
ijk =
∂Γl
jk
∂xi −∂Γl
ik
∂xj + Γh
jkΓl
ih −Γh
ikΓl
jh,
which recovers exactly the coordinate-dependent Deﬁnition 5.33.
We deﬁne also the Riemann (or covariant) curvature tensor Rm = R♭
in a coordinate-free way by
Rm(X, Y, Z, W) = ⟨R(X, Y )Z, W⟩
where ⟨, ⟩is the Riemannian metric on M. One can easily check that in
a coordinate system, the components of the tensor Rm are precisely the
collection of functions Rjklm deﬁned in Equation (5.34).
The properties of the Riemann curvature tensor presented earlier in a
coordinate-dependent manner have equivalent expressions in a coordinate-
free formulation.
Proposition 5.4.7. The covariant curvature tensor Rm satisﬁes the following
symmetry properties for vector ﬁelds X, Y , Z, W, and T :

5.4. The Curvature Tensor
241
1. Rm(X, Y, Z, W) = −R(Y, X, Z, W).
2. Rm(X, Y, Z, W) = −R(X, Y, W, Z).
3. Rm(X, Y, Z, W) = R(Z, W, X, Y ).
4. Bianchi’s ﬁrst identity:
Rm(X, Y, Z, W) + Rm(Y, Z, X, W) + Rm(Z, X, Y, W) = 0.
5. Bianchi’s diﬀerential identity:
∇Rm(X, Y, Z, W, T )+∇Rm(X, Y, W, T, Z)+∇Rm(X, Y, T, Z, W)=0.
5.4.3
Geometric Interpretation
Until now, we have not given an interpretation for the geometric meaning
of the curvature or torsion tensors.
Consider ﬁrst the torsion tensor. (Of course, by deﬁnition, the Levi-
Civita connection is symmetric and so the torsion is 0, but we give an
interpretation for any aﬃne connection.) We will use a ﬁrst-order approx-
imation discussion, following the presentation in [42, Section 7.3.2]. This
reasoning diﬀers slightly from a rigorous mathematical explanation, but we
include it for the sake of familiarity with physics-style reasoning.
Let p ∈M, with coordinates xμ in a coordinate system on M. Let
X = δμ∂μ and Y = εμ∂μ be two vectors in TpM. Let γX(t) be the curve
with coordinate functions δμt and let γY (t) be the curve with coordinate
functions εμt. Consider the parallel transport of the vector X along γY (t).
The coordinates of the resulting vector are δμ + Γμ
λνδλεν. The coordinates
of p′, the tip of the parallel transport of X, are
p′ :
δμ + εμ + Γμ
λνδλεν
If we take the parallel transport of Y along γX(t), the coordinates of the
resulting vector are εμ + Γμ
λνελδν. The coordinates of p′′, the tip of the
parallel transport of Y , are
p′′ :
δμ + εμ + Γμ
λνελδν.
The diﬀerence between these two parallel transports is
(Γμ
λν −Γμ
νλ)δλεν,

242
5. Introduction to Riemannian Geometry
p
γX
γY
(xμ)
(xμ + δμ)
(xμ + εμ)
X
Y
p′′
p′
Figure 5.10. Geometric interpretation of the tor-
sion tensor.
p
q (xμ + δμ)
r (xμ + εμ)
s (xμ + δμ + εμ)
Z
A′′Z
A′Z
Figure 5.11. Geometric interpretation of the cur-
vature tensor.
which is τ μ
λνδλεν. Therefore, intuitively speaking, the torsion tensor gives a
local measure of how much parallel transport of two noncollinear directions
with respect to each other fails to close a parallelogram (see Figure 5.10).
The curvature tensor, on the other hand, measures the path dependence
of parallel transport.
In the coordinate-free deﬁnition of the curvature
tensor from Equation (5.41), the expression ∇X∇Y Z is a vector ﬁeld that
measures the rate of change of parallel transport of the vector ﬁeld Z along
an integral curve of Y and then a rate of change of parallel transport of
this ∇Y Z along an integral curve of X. The expression ∇Y ∇XZ reverses
the process.
Now, as discussed in Section 4.2 in the subsection on Lie
brackets (see also Figure 4.3), the successive ﬂows of a distance h along
the integral curves of Y and then along the integral curves of X do not in
general lead one to the same point if one follows the integral curves of X
and then of Y . Proposition 4.2.13 shows that [X, Y ] is a sort of measure
for this nonclosure of integral paths in vector ﬁelds. Subtracting ∇[X,Y ]Z
from ∇X∇Y Z −∇Y ∇XZ eliminates the quantity of path dependence of
parallel transport on a manifold that is naturally caused by the nonclosure
of “square” paths of integral curves in vector ﬁelds.
Another perspective is to consider a vector Z based at p with coordi-
nates xμ and look at the path dependence of the parallel transport along
two sides of a “parallelogram” based at p and spanned by directions δμ and
εμ. Locally, i.e., when δμ and εμ are small, the parallel transport of Z from
p to q = (xμ + δμ) to s = (xμ + δμ + εμ) produces a vector A′Z. Similarly,
the parallel transport of Z from p to r = (xμ + εμ) to s = (xμ + δμ + εμ)
produces a vector A′′Z (see Figure 5.11). The diﬀerence Z →A′′Z −A′Z is

5.4. The Curvature Tensor
243
a linear transformation deﬁned locally at p that depends on the directions
δμ and εμ. In fact, it is not hard to show that, in coordinates,
(A′′Z −A′Z)i = Ri
jklδjεkZl.
5.4.4
The Ricci Curvature Tensor and the Einstein Tensor
Since tensors of order 4 are so unwieldy, there are a few common ways to
summarize some of the information contained in the curvature tensor.
One of the most common constructions is the Ricci curvature tensor,
denoted by Ric or Rc. Since it is so common in the literature, one writes
Rij instead of Ricij for the components of this tensor with respect to a
coordinate system. In coordinates, the components are deﬁned by
Rij = Rk
kij = gkmRkijm.
By the symmetries of the curvature tensor, Rij can be expressed equiva-
lently as
Rij = Rk
kij = −Rk
ikj = −gkmRikjm = −gkmRjmki.
Proposition 5.4.8. The Ricci tensor Ric is symmetric.
Proof: We prove this still within the context of a coordinate system.
Since Rij = Rk
kij, we can write
Rij = ∂Γk
ij
∂xk −
∂Γk
kj
∂xi + Γh
ijΓk
kh −Γh
kjΓk
ih.
(5.42)
In this expression, since the connection is symmetric, the ﬁrst and third
terms of the right-hand side are obviously symmetric in i and j. The fourth
term Γh
kjΓk
ih is also symmetric in i and j by a relabeling of the summation
variables h and k. Surprisingly, the second term in Equation (5.42) is also
symmetric.
By Problem 5.2.19,
∂(ln √det g)
∂xj
= Γk
jk.
Thus,
∂Γk
jk
∂xi =
∂2
∂xi∂xj (ln

det g) =
∂2
∂xj∂xi (ln

det g) = ∂Γk
ik
∂xj .
Hence, all the terms in Equation (5.42) are symmetric in i and j, so Rij =
Rji and the result follows.
□

244
5. Introduction to Riemannian Geometry
The scalar curvature function R is deﬁned as the trace of the Ricci
tensor with respect to g, i.e.,
S = Trg Ric = gijRij.
(5.43)
(Sometimes, texts use the letter R to denote the scalar curvature, but we
have opted for the other common notation of S so as not to be confused
with the curvature tensor symbol.)
The scalar curvature allows one to
deﬁne the Einstein tensor, which is of fundamental importance.
Deﬁnition 5.4.9. On any Riemannian manifold (M, g) the Einstein tensor
G is the tensor of type (0, 2) described in coordinates by
Gλμ = Rλμ −1
2gλμS,
where S is the scalar curvature.
As we will see in Section 6.4, the Einstein tensor is of central importance
in general relativity. For the moment, we prove the important result about
its divergence (see Problems 5.1.10 and 5.2.16).
Proposition 5.4.10. Let G be the Einstein tensor on a Riemannian manifold.
Then, using the deﬁnition of divergence in Equation (5.23),
div G = 0.
In coordinates, this reads Gν
λ;ν = (gνμGλμ);ν = gμνGμλ;ν = 0.
Proof: The proof of this proposition follows from the diﬀerential Bianchi
identity. For the Riemann curvature tensor, by Proposition 5.4.7(5), we
have
Rjklm;h + Rjkmh;l + Rjkhl;m = 0.
Taking the trace with respect to g over the variable pair (j, m),
Rkl;h −Rkh;l + gjmRjkhl;m = 0,
where the trace operator commutes with the covariant derivative because
of the compatibility condition of the Levi-Civita connection. Multiplying
by gkl and contracting gives
S;h −gklRhk;l −gjmRjh;m = 0.

5.4. The Curvature Tensor
245
Relabeling summation indices and using the symmetry of g (or Ric), we
deduce that
S;h −2gklRhk;l = 0.
(5.44)
But Gν
λ = gνμRλμ −1
2δν
λR, so
Gν
λ;ν = gνμRλμ;ν −1
2δν
λS;ν = gνμRλμ;ν −1
2S;λ,
and the vanishing divergence follows from Equation (5.44). The last claim
in the proposition follows from Problem 5.2.16.
□
Of particular interest in Riemannian geometry and in general relativ-
ity are manifolds in which the Ricci curvature is proportional to the metric
tensor. The corresponding metric is called an Einstein metric and the man-
ifold is called an Einstein space. More precisely, a Riemannian manifold
(M, g) has an Einstein metric if
Ric = λg
(5.45)
for some smooth function λ : M →R. Taking the trace with respect to
g of Equation (5.45) and noting that Trg g = dim M, we ﬁnd that λ must
satisfy
λ =
R
dim M .
Proposition 5.4.11. If (M, g) is an Einstein space of dimension dim M ≥3,
then the scalar curvature is constant on each connected component of M.
Proof: (Left as an exercise for the reader.)
□
In part because of Proposition 5.4.11, Einstein metrics continue to re-
main an active area of research not only because of their applications to
physics but more so because of their application to possible classiﬁcation
theorems for diﬀeomorphic manifolds. The Uniformization Theorem, a fun-
damental result in the theory of surfaces, establishes that every connected
2-manifold admits a Riemannian metric with constant Gaussian curvature.
This in turn leads to a classiﬁcation of diﬀeomorphism classes for surfaces.
One could hope that, in parallel with surfaces, all connected higher-
dimensional manifolds (dim M > 2) would possess an Einstein metric that
would in turn lead to a classiﬁcation theorem of diﬀeomorphism classes
of manifolds. This turns out not to be the case. There do exist higher-
dimensional compact manifolds that admit no Einstein metric [8]. Never-
theless, in attempts to reach a generalization to the Uniformization Theo-
rem for higher-dimensional manifolds, Einstein metrics play a vital role.

246
5. Introduction to Riemannian Geometry
Problems
5.4.1. Calculate the 16 component functions of the curvature tensor for the
sphere S2 in the standard (θ, φ) coordinate system.
5.4.2. Prove Proposition 5.4.2.
5.4.3.
(a) Prove the ﬁrst Bianchi identity in Proposition 5.4.4 using a coordinate-
dependent approach.
(b) Prove the ﬁrst Bianchi identity in Proposition 5.4.7 using a coordinate-
free approach.
5.4.4.
(a) Prove the second (diﬀerential) Bianchi identity in Proposition 5.4.4
using a coordinate-dependent approach.
(b) Prove the second (diﬀerential) Bianchi identity in Proposition 5.4.7
using a coordinate-free approach. [Hint: This can be long and tedious
if done directly. Instead, since ∇Rm is C∞(M)-multilinear, choose
X, Y, Z, W, T to be coordinate basis vector ﬁelds. Also, to make the
computations even easier, use the normal coordinate system.]
5.4.5. Prove that the quantity R(X, Y )Z deﬁned in Equation (5.41) is C∞(M)-
linear in the Z variable.
5.4.6. A smooth family of smooth curves is a function c : (−ε, ε) × [a, b] →M
such that cs(t) = c(s, t) is a smooth curve in M for each s ∈(−ε, ε).
Note that by symmetry, ct(s) is also a smooth curve for each t ∈[a, b]. A
vector ﬁeld along c is a smooth map V : (−ε, ε) × [a, b] →TM such that
V (s, t) ∈Tc(s,t)M for each (s, t). Deﬁne the vector ﬁelds S and T on c by
S = ∂sc and ∂tc, i.e., the tangent vectors to c in the indicated direction.
Show that for any vector ﬁeld V on c,
DsDtV −DtDs = R(S, T)V.
(This gives another geometric interpretation of the curvature tensor.)
5.4.7. The Jacobi Equation. This exercise considers variations along a geodesic γ.
A variation through geodesics along γ is a smooth family of smooth curves
c (deﬁned in Problem 5.4.6) such that for each s, the curve cs(t) = c(s, t) is
a geodesic and c(0, t) = γ(t). The variation ﬁeld V of a variation through
geodesics along γ is the vector ﬁeld along γ deﬁned by V (t) = (∂sc)(0, t).
Show that V satisﬁes the Jacobi equation
D2
t V + R(V, ˙γ)˙γ = 0.
5.4.8. Consider the 3-sphere S3, and consider the coordinate patch given by
the parametrization described in Problem 5.1.11. Calculate the curvature
tensor, the Ricci curvature tensor, and the scalar curvature.
5.4.9. Calculate the curvature tensor, the Ricci curvature tensor, and the scalar
curvature for the Poincar´e ball. (See Problem 5.1.9.)
5.4.10. Consider the 3-torus described in Problem 5.1.1(b) with the metric induced
from R4. Calculate all the components of the curvature tensor, the Ricci
tensor, and the scalar curvature, given in the coordinates deﬁned by the
parametrization given in Problem 5.1.1(b).

5.4. The Curvature Tensor
247
5.4.11. Consider the metric associated to spherical coordinates in R3, given by
g = dr2 + r2 sin2 φdθ2 + r2dφ2.
(Note, we have used the mathematics labeling of the longitude and latitude
angles. Physics texts usually have the θ and φ reversed.) Prove that all
the components of the curvature tensor are identically 0.
5.4.12. The Schwarzschild metric has the tensor
g = −

1 −2M
r
	
dt2 +
1
1 −2M
r
dr2 + r2 sin2 φdθ2 + r2dφ2.
where 0 < 2M < r. Calculate the curvature tensor, the Ricci tensor and
the scalar curvature. (According to general relativity, the Schwarzschild
metric describes how spacetime curves in the presence of a sphere of mass
M.)
5.4.13. Consider the Riemannian manifold of dimension 2 equipped with the met-
ric g = f(u + v)(du2 + dv2) for some function f. Solve for which f lead to
Rjklm = 0.
5.4.14. Let R be the Riemann curvature tensor (deﬁned with respect to the Levi-
Civita connection). Prove that
Rijkl =1
2
 ∂2gik
∂xj∂xl −∂2gjk
∂xi∂xl −
∂2gil
∂xj∂xk +
∂2gjl
∂xi∂xk
	
+ gλμ

Γλ
ikΓμ
jl −Γλ
ilΓμ
jk

.
Conclude that in normal coordinates centered at p, the following holds
at p:
Rijkl = 1
2(∂j∂lgik −∂i∂lgjk −∂j∂kgil + ∂i∂kgjl).
5.4.15. The Killing Equation. Let (M, g) be a Riemannian manifold and let X ∈
X(M). Consider the function fε : M →M deﬁned by
fε(p) = γ(ε)
where γ is the integral curve of X through p. Thus, the linear approx-
imation of fε for small ε maps p = (xi) to the point with coordinates
xi + εXi(p). Suppose that fε is an isometry for inﬁnitesimal ε.
(a) Use a linear approximation in ε on the change-of-coordinates formula
for the metric g to show that g and X satisfy the Killing equation:
∂gij
∂xk Xk + ∂Xl
∂xi glj + ∂Xl
∂xj gil = 0.
(b) Let ∇be the Levi-Civita connection. Show that the Killing equa-
tion is equivalent to the condition that (∇X)♭is antisymmetric. In
components related to a coordinate system, this means that
Xi;j + Xj;i = 0,
(5.46)
where Xi = gikXk.

248
5. Introduction to Riemannian Geometry
5.4.16. Consider a covector ﬁeld ω on a Riemannian manifold (M, g). Suppose
that ω satisﬁes the covariant Killing equation (see Equation (5.46)), i.e.,
ωi;j +ωj;i = 0. Show that along any geodesic γ(s) of M, ω(˙γ) is a nonzero
constant.
5.4.17. Suppose that on a Riemannian manifold (M, g), the curvature tensor sat-
isﬁes div R = 0, or in coordinates Ri
jkl;i = 0 for all j, k, l. Show that the
following also hold:
(a) Rij;h = Rih;j;
(b) S;j = 0;
(c) gmlRi
jklRmh + gmlRi
khlRmj + gmlRi
hjlRmk = 0.
5.4.18. Show that if Rijkl + Rljki = 0, then the covariant curvature tensor is
identically 0.
5.4.19. Prove Proposition 5.45. More precisely, show that R;h =0 when dim M ≥3.
5.4.20. Let Gi
j = gikGjk, where Gjk are the components of the Einstein tensor,
and deﬁne Ril
jk = glmRi
jkm. Prove that
Gi
j = −1
4δiνκ
jλμRλμνκ,
where we have used the generalized Kronecker symbol deﬁned in Equa-
tion (2.29).
5.4.21. Consider a Lorentzian metric given by ds2 = −dt2 + f(t)2dx2, where f(t)
is any smooth function of t. Show that the Einstein tensor is identically 0.

CHAPTER
6
Applications of Manifolds
to Physics
In the previous chapters, we set forth the intuitive goal of doing calculus
on curved spaces as the motivating force behind the development of the
theory of manifolds. Having developed a theory of manifolds that satisﬁes
this goal, we now present four applications to physics that utilize this theory
to varying degrees. Each section shows just the tip of the iceberg on very
broad areas of active research.
Hamiltonian mechanics motivate the notion of symplectic manifolds,
which in turn lead to complex structures and K¨ahler manifolds. Because of
their speciﬁc properties, symplectic geometry and K¨ahler manifolds have
become quite signiﬁcant. Furthermore, it was the Hamiltonian formulation
of dynamics that lent itself best to quantization and hence to Schr¨odinger’s
equation in quantum mechanics.
A few exercises in this text have dealt with the theory of electromag-
netism. In Section 6.2, we gather together some of the results we have seen
in the theory of electromagnetism and rephrase them into the formalism of
a Lorentzian spacetime.
We also discuss a few geometric concepts underlying string theory. Be-
tween 1900 and 1940, physics took two large steps in opposite directions
of the size scale, with quantum mechanics describing the dynamics of the
very small scale and general relativity describing the very large scale. These
theories involve very diﬀerent types of mathematics, which led physicists
to look for reformulations or generalizations that could subsume both theo-
ries. However, despite extensive work to ﬁnd a unifying theory, the task has
proven exceedingly diﬃcult, even on mathematical grounds. String theory
is a model for the structure of elementary particles that currently holds
promise to provide such a uniﬁcation. We wish to mention string theory in
this book because, at its core, the relativistic dynamics of a string involve
a two-dimensional submanifold of a Minkowski space, an object that arises
nicely as an application of pseudo-Riemannian metrics.
249

250
6. Applications of Manifolds to Physics
Finally, Einstein’s theory of general relativity stands as a direct appli-
cation of Riemannian manifolds. In fact, general relativity motivated some
of the development and helped proliferate the notions of Riemannian (and
pseudo-Riemannian) geometry beyond the conﬁnes of pure mathematics.
Many of the “strange” phenomena which ﬁll the pages of popular books on
cosmology occur as consequences of the mathematics of this geometry.
This chapter assumes that the reader has some experience in physics
but no more than a ﬁrst college course (calculus-based) in mechanics. All
the other material will be introduced as needed. Furthermore, we do not
discuss issues of quantization as those exceed the scope of this book.
6.1
Hamiltonian Mechanics
6.1.1
Equations of Motion
The study of dynamics relies almost exclusively on Newton’s laws of motion,
in particular, his second law. This law states that the sum of exterior forces
on a particle or object is equal to the rate of change of momentum, i.e.,
 ⃗Fext = d⃗p
dt ,
(6.1)
where ⃗p = m d⃗x/dt and ⃗x(t) is the position of the particle at time t. If m
is constant, Equation (6.1) reduces to
 ⃗Fext = md2⃗x
dt2 .
(6.2)
Furthermore, by a simple calculation, Equation (6.1) directly implies the
following law of motion for angular momentum about an origin O:
d⃗L
dt =

⃗τext,
(6.3)
where ⃗L = ⃗r × ⃗p is the angular momentum of a particle or solid, where ⃗r is
the position vector of the particle or center of mass of the solid, and where
"⃗τext is the sum of the torques about O. (Recall that the torque about
the origin of a force ⃗F is τ = ⃗r × ⃗F.)
Though Equation (6.1) undergirds all of classical dynamics, the value
of ancillary equations, such as Equation (6.3), arises from the fact that
these other equations may elucidate conserved quantities or produce more
tractable equations when using diﬀerent variables besides the Cartesian

6.1. Hamiltonian Mechanics
251
coordinates. For example, when describing the orbits of planets around
the sun, polar (cylindrical) coordinates are far better suited than Carte-
sian coordinates. In particular, as shown in Example 2.2.2, one ﬁnds that
the angular momentum is a conserved quantity for a particle under the
inﬂuence of forces that are radial about some origin.
It turns out that in many cases (in particular when the forces are con-
servative), either Equation (6.2) or Equation (6.3) follows from a speciﬁc
variational principle that has extensive consequences. Suppose that the
state of a physical system is described by a system of coordinates qk, with
k = 1, 2, . . ., n. Hamilton’s principle states that the motion of a system
evolves according to a path P parametrized by (q1(t), . . . , qn(t)) between
times t1 and t2 so as to minimize the integral
S =
%
P
L dt =
% t2
t1
L dt
where L is the Lagrangian function. The integral S is called the action of
the system. When the system is under the inﬂuence of only conservative
forces, the Lagrangian is L = T −V , where T is the kinetic energy and V
is the potential energy. Recall that for a conservative force ⃗F, its potential
energy V , which is a function of the position variables alone, satisﬁes
⃗F = −⃗∇V = −gradV.
Intuitively speaking, in the case of conservative forces, Hamilton’s princi-
ple states that a system evolves in such a way as to minimize the total
variation between kinetic and potential energy. However, even if a force is
not conservative, it may still possess an associated Lagrangian that pro-
duces the appropriate equation of motion. (See Problem 6.1.8 for such an
example.)
We consider the Lagrangian L as an explicit function of t, the coordi-
nates qk, and their time derivatives ˙qk = dqk/dt. According to Theorem
B.3.1, the Lagrangian must satisfy the Euler-Lagrange equation in each
coordinate qk, namely,
∂L
∂qk
−d
dt
 ∂L
∂˙qk
	
= 0.
(6.4)
This is called Lagrange’s equations of motion. Though this system of equa-
tions moves away from the nice vector expression of Equation (6.2), it has
the distinct advantage of expressing equations of motion in a consistent
way for any choice of coordinates.

252
6. Applications of Manifolds to Physics
α
h
x
Figure 6.1. An round object rolling downhill.
Example 6.1.1. Consider a ball (or cylinder) of radius R rolling down a plane
inclined with angle α, as depicted in Figure 6.1. Because the object rolls
instead of sliding, the rotation about its center leads to an additional kinetic
energy amount of 1
2I ˙θ2, where ˙θ is the rate of rotation about its center.
However, because there is no slipping, we deduce that ˙x = R ˙θ, where x is
the coordinate of the distance of the center of mass of the object up the
incline. Thus, the Lagrangian of this system is
L = 1
2I ˙θ2 + 1
2mv2 −mgh,
L(x, ˙x) =
I
2R2 I ˙x2 + 1
2m ˙x2 −mgx sin α.
The Euler-Lagrange Equation (6.4) gives
∂L
∂x −d
dt
∂L
∂˙x
	
= −mg sin α −(I/R2 + m)¨x = 0,
which leads to the equation of motion
d2x
dt2 = −g sin α
I
mR2 + 1,
a well-known result from classical mechanics.
Though Example 6.1.1 involves a variable x that is essentially taken
from R, physical systems in general may typically be described by other
types of variables. When studying the motion of a simple pendulum (see
Figure 6.2(a)), one uses as a variable the angle θ of deviation of the pen-
dulum from the vertical. A system that is a double pendulum (see Fig-
ure 6.2(b)) involves two angles.
If a physical system can be described by using n locally independent
variables, then we say the system has n degrees of freedom. The set of

6.1. Hamiltonian Mechanics
253
θ
(a) Simple pendulum
θ1
θ2
(b) Double pendulum
Figure 6.2. Simple and compound pendulum.
all possible states of a physical system is a real manifold Q of dimension
n, called the conﬁguration space of the system. The variables (qk) that
locate a point on (a coordinate chart of) the manifold Q are called the
position variables. (Note, we will use the subscript indices for the position
variables to conform with physics texts and literature on symplectic mani-
folds, though one should remember at this stage that they are contravariant
quantities.) For example, the conﬁguration space of the system in Exam-
ple 6.1.1 is simply Q = R, while the conﬁguration space for the simple
pendulum is Q = S1 and the conﬁguration space of the double pendulum
is the torus Q = S1 × S1.
The time development of a system corresponds to a curve γ : t →
(qk(t)) on the manifold, and the functions ˙q1, . . . , ˙qn are the coordinates of
a tangent vector along γ in the tangent space T Q.
Now the Euler-Lagrange Equation (6.4) is a system of second-order,
ordinary, diﬀerential equations. We would like to change this into a system
of ﬁrst-order diﬀerential equations for two reasons: (1) many theorems on
diﬀerential equations are stated for systems of ﬁrst-order equations and
(2) it is easier to discuss ﬁrst-order equations in the context of manifolds.
We do this in the following way.
Deﬁne the generalized momenta functionally by
pk = ∂L
∂˙qk
.
(6.5)
The quantities pk are the components of the momentum vector, which is
in fact an element of Tγ(t)Q∗. We can see this as follows. Let W be an
n-dimensional vector space, and let f : W →R be any diﬀerentiable func-
tion. Then the diﬀerential df⃗v at a point ⃗v ∈W is a linear transformation

254
6. Applications of Manifolds to Physics
df⃗v : W →R. Thus, by deﬁnition of the dual space, df⃗v ∈W ∗. Conse-
quently, the diﬀerential df gives a correspondence df : W →W ∗via
⃗v −→df⃗v =
n

i=1
∂f
∂xi

⃗v
dxi.
Taking W as the vector space Tγ(t)Q, the momentum at the point γ(t) is
the vector dL( ˙qk) ∈Tγ(t)Q∗. Hence, we can think of the momentum vector
p as a covector ﬁeld along the curve γ given at each point by dL( ˙qk).
Consider now the Hamiltonian function H deﬁned by
H =
n

k=1
pk ˙qk −L(q1, . . . , qn, ˙q1, . . . , ˙qn, t).
Since we can write the quantity ˙qk in terms of the components pk, we can
view the Hamiltonian as a time-dependent function on T Q∗. Given any
conﬁguration space Q, we deﬁne the cotangent bundle T Q∗as the phase
space of the system. If Q is an n-dimensional manifold, then T Q∗is a
manifold of dimension 2n.
The variables ˙qi are now functions of the independent variables (t, q1, . . . ,
qn, p1, . . . , pn). Taking derivatives of H, we ﬁnd that
∂H
∂pi
= ˙qi +
n

k=1
pk
∂˙qk
∂pi
−
n

k=1
∂L
∂˙qk
∂˙qk
∂pi
= ˙qi +
n

k=1

pk −∂L
∂˙qk
	 ∂˙qk
∂pi
= ˙qi,
where each term of the summation is 0 by deﬁnition of pk. Furthermore,
note that Lagrange’s equation reduces to ∂L/∂qi = ˙pi. Thus, taking deriva-
tives with respect to qi, we get
∂H
∂qi
=
n

k=1
pk
∂˙qk
∂qi
−

∂L
∂qi
+
n

k=1
∂L
∂˙qk
∂˙qk
∂qi

= −∂L
∂qi
+
n

k=1

pk −∂L
∂˙qk
	 ∂˙qk
∂qi
= −∂L
∂qi
= −˙pi.

6.1. Hamiltonian Mechanics
255
Therefore, given the deﬁnition in Equation (6.5), the Euler-Lagrange Equa-
tion (6.4) is equivalent to
˙qk = ∂H
∂pk
,
˙pk = −∂H
∂qk
.
(6.6)
This system of equations is called Hamilton’s equations of motion. They
consist of 2n ﬁrst-order, ordinary, diﬀerential equations in n unknown 2n
functions, whereas Lagrange’s equations of motion consisted of n second-
order, ordinary, diﬀerential equations in n unknown functions.
For simple dynamic systems, the kinetic energy T is a homogeneous
quadratic function in the variables ˙qk. If this is the case, then it is not
hard to show that
n

k=1
˙qkpk = 2T,
where T is the kinetic energy. If in addition, the forces acting on the system
are conservative, then
H = 2T −(T −V ) = T + V,
which is the total energy of the system.
6.1.2
Symplectic Manifolds
We now introduce the notion of a symplectic manifold and show how Hamil-
ton’s equations of motion arise naturally in this context. The theory of
symplectic geometry is a branch of geometry in and of itself so we do not
pretend to cover it extensively here. Instead, we refer the reader to [7]
or [1] for a more thorough introduction.
Deﬁnition 6.1.2. Let W be a vector space over a ﬁeld K. A symplectic form
is a bilinear form
ω : V × V →K;
that is,
1. antisymmetric: ω(v, v) = 0 for all v ∈W;
2. nondegenerate: if ω(v, w) = 0 for all w ∈W, then v = 0.
The pair (V, ω) is called a symplectic vector space.

256
6. Applications of Manifolds to Physics
Proposition 6.1.3. Let (V, ω) be a ﬁnite-dimensional, symplectic vector space.
There exists a basis B of V relative to which the matrix of ω is
[ω]B =
 0
In
−In
0
	
.
where In is the n × n identity matrix. In addition, V has even dimension.
Proof: (Left as an exercise for the reader. See Problem 6.1.3.)
□
Since the form ω is antisymmetric and bilinear, then ω ∈12 V . Suppose
that V has a basis B = {e1, . . . , e2n}, and let B∗= {e∗
1, . . . , e∗
2n} be the
associated dual basis (see Section C.3). Then in coordinates, we can write
ω as
ω =

1≤i<j≤2n
ωije∗
i ∧e∗
j.
However, from Proposition 6.1.3 follows immediately a nice corollary.
Corollary 6.1.4. Let (V, ω) be a symplectic vector space of dimension 2n.
Then there exists a basis B = {e1, . . . , e2n} such that ω can be written as
ω =
n

i=1
e∗
i ∧e∗
n+i.
The expression in Corollary 6.1.4 is called the canonical form of the
symplectic form ω.
Deﬁnition 6.1.5. A symplectic manifold (M, ω) is a smooth manifold M
equipped with a 2-form ω that is closed (dω = 0) and nondegenerate. In
other words, M is a smooth manifold such that for each P ∈M, TP M is a
symplectic vector space with symplectic form ωP and ωP varies smoothly
with P.
By Proposition 6.1.3, one sees that a symplectic manifold has even
dimension.
Deﬁnition 6.1.6. If (M, ω) and ( ˜
M, ˜ω) are two symplectic manifolds, then a
smooth map F : M →˜
M is called symplectic if
F ∗˜ω = ω.
We say that F preserves the symplectic structure. If in addition, F −1 is
also a smooth symplectic map, then F is called a symplectomorphism.

6.1. Hamiltonian Mechanics
257
Darboux’s Theorem, a fundamental result in the theory of symplectic
manifolds, establishes that given any two symplectic forms ω and ˜ω such
that ωP = ˜ωP at some point P ∈M, there exists a neighborhood U of
P and a diﬀeomorphism F : U →F(U) ⊂M such that F(P) = P and
F ∗˜ω = ω. (We refer the reader to [7, Section 2.2] for a proof.) Darboux’s
Theorem is equivalent to the following formulation.
Theorem 6.1.7. Let (M, ω) be a symplectic manifold. For each point P ∈M,
there exists an open neighborhood U of P and a symplectomorphism F of
U onto F(U) ⊂R2n such that (F −1)∗ω takes the canonical form in R2n.
As a consequence of this theorem, at every point P ∈M, there exists a
coordinate neighborhood U of P with coordinates x in which
ω =
n

i=1
dxi ∧dxn+i.
The formalism of symplectic manifolds applies to Hamiltonian mechan-
ics in the following way. Consider the conﬁguration space Q for a physical
system.
Suppose that Q is a manifold of dimension n.
The cotangent
space M = T Q∗is a manifold in itself of dimension 2n. If U is a coordi-
nate neighborhood of Q with coordinates (q1, . . . , qn), then ˜U = π−1(U) is
a coordinate neighborhood for the manifold T Q∗, where π : T Q∗→Q is
the bundle projection map. The quantities (q1, . . . , qn, p1, . . . , pn) of posi-
tion coordinates and corresponding generalized momenta form a coordinate
system on ˜U. By the identiﬁcation of TpRn ∼= Rn, it is not hard to show
that T M = T (T Q∗) ∼= T Q ⊕T Q∗.
Proposition 6.1.8.
The 2-form deﬁned over a particular coordinate patch
π−1(U) by
ω =
n

i=1
dqi ∧dpi
(6.7)
extends to a 2-form ω ∈Ω2(T Q∗) over the whole phase space T Q∗. Fur-
thermore, it is deﬁned in exactly the same way as in Equation (6.7) over
every coordinate patch on T Q∗obtained as π−1( ¯U), where ¯U is any other
coordinate patch of Q.
Consequently, the form ω endows T Q∗with the
structure of a symplectic manifold.
Proof: Let F : U ∩¯U →U ∩¯U be a coordinate transformation from (qi) to
(¯qi) coordinates, and let G : π−1(U∩¯U) →π−1(U∩¯U) be the corresponding
coordinate transformation from (qi, pi) to (¯qi, ¯pi) on T Q∗.
Since pi are

258
6. Applications of Manifolds to Physics
coordinates in the cotangent space, the diﬀerential of G has coordinate
functions
[dG] =
⎛
⎜
⎝
∂¯qi
∂qj
0
0
∂qk
∂¯ql
⎞
⎟
⎠=

[dF]
0
0
[dF]−1
	
.
In particular, we deduce that
d¯qi = ∂¯qi
∂qj
dqj
and
d¯pi = ∂qk
∂¯qi
dpk.
Thus,
n

i=1
d¯qi ∧d¯pi =
n

i=1
 ∂¯qi
∂qj
dqj
	
∧
∂qk
∂¯qi
dpk
	
=
n

j=1
n

k=1
 n

i=1
∂¯qi
∂qj
∂qk
∂¯qi

dqj ∧dpk
=
n

j=1
n

k=1
δjkdqj ∧dpk =
n

j=1
dqj ∧dpj.
The result follows.
□
The Hamiltonian function H is a smooth function T Q∗→R. We deﬁne
the Hamiltonian vector ﬁeld XH as the unique vector ﬁeld that satisﬁes
iXHω = dH,
(6.8)
where iX is the contraction operator iX deﬁned in Problem 5.1.10. Accord-
ing to Problem 5.1.10, iXHω is the 1-form deﬁned by iXHω(Y ) = ω(XH, Y )
at all P ∈M and for all Y ∈X(M). It is not too hard to show that in
coordinates of T (T Q∗), the vector ﬁeld XH is
XH =
n

i=1
∂H
∂pi
∂
∂qi
−
n

i=1
∂H
∂qi
∂
∂pi
.
(6.9)
Proposition 6.1.9. A curve γ on the phase space T Q∗is an integral curve
of the vector ﬁeld XH if and only if in each coordinate system the com-
ponents γ(t) = (qk(t), pk(t)) satisfy Hamilton’s equations of motion from
Equation (6.6).

6.1. Hamiltonian Mechanics
259
Proof: As a vector ﬁeld on the curve γ, the derivative ˙γ(t) is written in
coordinates as
˙γ(t) =
n

i=1
˙qi
∂
∂qi
+
n

i=1
˙pi
∂
∂pi
.
(6.10)
By Equation (6.9), the Hamiltonian vector ﬁeld XH at points along the
curve is expressed in coordinates as
(XH)γ(t) =
n

i=1
∂H
∂pi
(γ(t)) ∂
∂qi
−
n

i=1
∂H
∂qi
(γ(t)) ∂
∂pi
.
(6.11)
The proposition follows by identiﬁcation of Equations (6.10) and (6.11).□
In other words, Proposition 6.1.9 states that a solution to Hamilton’s
equations of motion corresponds to a curve γ(t) in the phase space T Q∗
such that
˙γ(t) = (XH)γ(t).
Because of the importance of this formulation, it has its own terminology.
If (M, ω) is a symplectic manifold and H ∈C∞(M), then with XH deﬁned
by Equation (6.8), the triple (M, ω, XH) is called a Hamiltonian system.
Problems
6.1.1. Explain why the conﬁguration space of a general solid in Euclidean three-
space is Q = R3 × S2 × S1.
6.1.2. (Phys) Write down the Lagrangian equations of motion and the Hamil-
tonian equations of motion for an elastic pendulum: a particle of mass
m attached to a (massless) elastic string of elasticity constant k and un-
stretched length L0.
6.1.3. (Phys) Consider the motion of the earth around the sun. Placing the sun
at the origin, use polar coordinates (r, θ) to locate the center of the earth
with respect to the sun. The force of gravity of the sun acting on the
earth has a potential energy function of V (r) = −GMSME/r, where G is
Newton’s universal constant of gravity, MS is the mass of the sun and ME
is the mass of the earth. Take into account the fact that the earth rotates
on its own axis. Use the additional angle ψ to orient the earth around its
axis. Write down the Hamiltonian function for this system, taking into
account earth’s rotation. Show that, despite the fact that the rotation of
the earth aﬀects the Hamiltonian, the rotation does not aﬀect the motion
of the earth around the sun.
6.1.4. Suppose that Q is the conﬁguration space for a physical system involving
a particle of mass m, and suppose that Q is a Riemannian manifold with

260
6. Applications of Manifolds to Physics
metric g = ⟨, ⟩. Then the kinetic energy of a particle traveling along a
curve γ(t) is
T = 1
2m⟨˙γ(t), ˙γ(t)⟩.
(a) Consider the sphere S2 of radius R, and use the coordinates (θ, φ).
Write down the Lagrangian, the Hamiltonian, and Hamilton’s equa-
tions of motion of a particle of mass m aﬀected by a potential
V = f(θ, φ).
(b) Let Q be any Riemannian manifold with metric g and with the as-
sociated Levi-Civita connection.
Show that if the potential V is
constant, then a solution to Hamilton’s equations of motion deﬁnes
a geodesic on Q.
6.1.5. Prove Proposition 6.1.3.
6.1.6. Let V be a vector space of dimension 2n, and let ω be any bilinear form
on V . Show that ω is nondegenerate if and only if ωn = ω ∧· · · ∧ω is
nonzero.
6.1.7. Let (V, ω) be a real symplectic vector space. Let B = {e1, · · · , e2n} be a
basis of V that gives ω a canonical form.
(a) Show that if a linear transformation T : V →V leaves the form
invariant, i.e.,
ω(T(⃗v), T(⃗w)) = ω(⃗v, ⃗w)
for all ⃗v, ⃗w ∈V,
then the matrix A of T with respect to the basis B satisﬁes
AT JA = J,
where
J =
 0
In
−In
0
	
.
(b) Suppose that T leaves ω invariant. Show that if λ is an eigenvalue
of T with multiplicity k, then 1λ, ¯λ, and 1/¯λ are also eigenvalues of
T with multiplicity k.
6.1.8. (Phys) Classical electromagnetism. Consider a charged particle of mass m
and charge e under the inﬂuence of a static electric ﬁeld ⃗E and magnetic
ﬁeld ⃗B. The non-relativistic theory of electromagnetism [44] states that
the force applied to the particle is
⃗F = e( ⃗E + 1
c⃗v × ⃗B),
where ⃗v = d⃗x/dt is the velocity vector of the particle and c is the speed
of light. (The presence of c is a mere scaling factor due to the choice of
units.) The electric ﬁeld is induced from an electric potential φ so that
⃗E = −⃗∇φ. The magnetic force, however, is not a conservative force. Show
that the Lagrangian
L = 1
2mv2 + eφ + e
c⃗v · ⃗A

6.1. Hamiltonian Mechanics
261
yields Newton’s equation of motion from Equation (6.2), where ⃗A is the
vector potential satisfying ⃗B = ⃗∇× ⃗A. Show that the Hamiltonian of this
system given in coordinates (xi, pi) is
H(⃗x, ⃗p) =
1
2m(p2
1 + p2
2 + p2
3) −eφ(⃗x) −e
mc(p1A1 + p2A2 + p3A3).
6.1.9. (Phys) Action for a Relativistic Point Particle. The action of a free (no exter-
nal forces) non-relativistic particle traveling between t = t1 and t = t2 is
simply
S =
% t2
t1
1
2mv2 dt =
% t2
t1
1
2m

d⃗x
dt

2
dt,
and thus the Lagrangian is L = T = 1
2mv2. To give a relativistic formu-
lation for the action of a free particle, let us ﬁrst assume we are in the
context of a Minkowski space with coordinates described in Equation (5.8).
One must describe the action in a way that is invariant under a Lorentz
transformation.
Therefore, we cannot directly use the particle velocity
since the velocity is not a Lorentz invariant. This exercise seeks to justify
the deﬁnition of the action of a relativistic point particle as
S = −mc
%
P
ds,
(6.12)
where we integrate over a world line P of the particle. According to Equa-
tion (5.11), the action in Equation (6.12) has an associated Lagrangian of
L = −mc2

1 −v2
c2 .
(6.13)
(a) Calculate the 6th-order Taylor expansion of
√
1 −x2, and show that
the quadratic approximation to L is
L ∼= −mc2 + 1
2mv2.
(6.14)
(b) Using Equation (6.13), show that the generalized momentum vector
⃗p and the Hamiltonian H satisfy
⃗p =
m⃗v

1 −v2
c2
and
H =
mc2

1 −v2
c2
.
(This formula for H conforms with the formula [23, (1-16)] for the
total energy of a free relativistic particle.)
(c) Let g be any Lorentz metric on a pseudo-Riemannian manifold of
index 1.
Repeat Problem 6.1.4(b) using −ds2 = gij dxi dxj, and
show that a free relativistic particle travels along a geodesic.
6.1.10. An alternative way to deﬁne the Hamiltonian vector ﬁeld XH involves us-
ing the process of raising indices as deﬁned in Equation (5.5) in Section 5.1.
Show that XH = dH♯, relative to the canonical form ω on TQ∗.

262
6. Applications of Manifolds to Physics
6.1.11. Prove Equation (6.9). [Hint: Use the embedding of 12 TQ∗in TQ∗⊗TQ∗
given by dqi ∧dpi = dqi ⊗dpi −dpi ⊗dqi.]
6.1.12. Let Q be a conﬁguration space and let M be the associated phase space
M = TQ∗. Let π : TQ∗→Q be the canonical projection. Deﬁne the
Liouville form ϑ ∈Ω1(M) by
ϑm(X)
def
= λq (dπm(X))
for any point m = (q, λq) of the phase space M and for any vector X ∈
TmM.
(a) Using the standard coordinates on π−1(U) in TQ∗, where U is a co-
ordinate patch of Q, show that the Liouville form has the expression
ϑ =
n

i=1
pi dqi.
(b) Conclude that the canonical symplectic form on TQ∗satisﬁes ω =
−dϑ.
6.1.13. Poisson Bracket.
Consider the phase space M = TQ∗for a conﬁgura-
tion space Q.
Deﬁne the Poisson bracket { , } on the function space
C∞(TQ∗) by
{f, g} =
n

i=1
 ∂f
∂qi
∂g
∂pi −∂f
∂pi
∂g
∂qi
	
.
(a) Show that { , } is a diﬀerential in each entry, i.e.,
{f1f2, g} = {f1, g}f2 + f1{f2, g}
and similarly for the second entry.
(b) Prove that { , } gives C∞(TQ∗) the structure of a Lie algebra, i.e.,
{ , } satisﬁes the ﬁrst three items of Proposition 4.2.12.
(c) Show that Hamilton’s equations of motion from Equation (6.6) are
equivalent to
˙qk = {qk, H}
and
˙pk = {pk, H}
for k = 1, . . . , n.
6.2
Electromagnetism
The goal of this section is to summarize the dynamics of a charged particle
moving under the inﬂuence of an electric ﬁeld ⃗E and a magnetic ﬁeld ⃗B,
both of which are time and space dependent. In no way does this brief sec-
tion attempt to encapsulate all of the theory of electromagnetism. Rather
we show how to pass from a classical formulation of a few of the basic laws
of electromagnetism to a modern formulation that uses Minkowski metrics
and the language of forms. (Note: all formulas in this section use CGS

6.2. Electromagnetism
263
units, i.e., centimeters-grams-seconds units. In this system, force is mea-
sured in dyne, energy in erg, electric charge in esu, electric potential in
statvolt, and the magnetic ﬁeld strength in gauss.)
The mathematical theory relies on the model (based on experiment)
that point charges exist, i.e., particles of negligible size with charge. For
example, the electron and the proton ﬁt this bill. In contrast, magnetic
monopoles—point-like particles with a magnetic charge—do not (appear
to) exist. The observation of a single magnetic monopole would change
the rest of the theory (by adding an extra magnetic charge density and
magnetic current) but even this “would not alter the fact that in matter as
we know it, the only sources of the magnetic ﬁeld are electric currents” [44,
p. 405].
Coulomb’s law of electrostatic force states that the force acting on point
charge 2 induced by point charge 1 is inversely proportional to the square
of the distance between the charges. More precisely,
⃗F = q1q2
r2 ˆr,
where q1 and q2 are the respective charges of the particles, r is the distance
between them, and ˆr is the unit vector pointing from 1 to 2. One then
considers systems of charges, modeled by a charge density ρ(x, y, z), acting
on a point particle with charge q. We call the electric ﬁeld of a charged
system the vector ﬁeld
⃗E = 1
q
⃗F
(6.15)
=

R3 ρ(x′, y′, z′)
(x −x′, y −y′, z −z′)
((x −x′)2 + (y −y′)2 + (z −z′)2)3/2 dx′ dy′ dz′,
where ⃗F is the force the system would exert on a particle of charge q at
position (x, y, z). An application of Gauss’s Theorem from vector calculus
gives Gauss’s Law for electrostatics, i.e.,
div ⃗E = 4πρ,
where, if there is time dependence, then the divergence is only taken in the
space variables.
One can show that an inverse square force is conservative, which means
that there exists a function ϕ(x, y, z) such that
⃗E = −⃗∇ϕ.
(6.16)

264
6. Applications of Manifolds to Physics
The function ϕ is called the electric potential. The potential of a distribu-
tion of charge can be written as the integral
ϕ(x, y, z, t) =
%%%
R3
ρ(x′, y′, z′, t)

(x −x′)2 + (y −y′)2 + (z −z′)2 dx′ dy′ dz′.
(6.17)
The potential energy of the electric force ﬁeld acting on a particle with
charge q is V = qϕ. If the system of electrical charges is moving, then ⃗E,
ϕ, and ρ are also functions of time, but Equations (6.15) and (6.16) still
hold with the caveat that the integration and the gradient only involve the
space variables. A system of time-dependent current density also induces
what are called electrical currents. One calls current density the vector ﬁeld
⃗J that at each point (x, y, z), intuitively speaking, measures the direction
of the current and how much current is passing per area and per time. A
direct application of Gauss’s Theorem from vector calculus gives
div ⃗J = −∂ρ
∂t .
(6.18)
At the heart of electromagnetism lies an interdependence between mag-
netic ﬁelds and electric ﬁelds. A charged particle that is moving in the pres-
ence of a current experiences a force perpendicular to its velocity. That
force acting on the particle is called the magnetic force. The magnetic ﬁeld
of a system of charges is the ﬁeld ⃗B deﬁned implicitly by
⃗F = q( ⃗E + 1
c⃗v × ⃗B).
This overall eﬀect on a particle with charge q is called the electromagnetic
force. It is no longer conservative due to the presence of ⃗v. One can deﬁne
the magnetic vector potential ⃗A by
⃗A(x, y, z) = 1
c
%%%
R3
⃗J(x′, y′, z′, t)

(x −x′)2 + (y −y′)2 + (z −z′)2 dx′ dy′ dz′.
(6.19)
Furthermore, Faraday discovered that not only does a time-dependent dis-
tribution of charge induce a magnetic ﬁeld but so does a variable magnetic
ﬁeld similarly aﬀect the electric ﬁeld. The inter-relationship between the
electric and magnetic ﬁelds can be summarized by two separate sets of
equations: Faraday’s law for potential, i.e.,
⃗E = −⃗∇ϕ −1
c
∂⃗A
∂t
⃗B = ⃗∇× ⃗A,
(6.20)

6.2. Electromagnetism
265
and the celebrated Maxwell’s equations (in a vacuum), i.e.,
⃗∇· ⃗E = 4πρ,
⃗∇× ⃗E = −1
c
∂⃗B
∂t ,
⃗∇· ⃗B = 0,
⃗∇× ⃗B = 1
c
∂⃗E
∂t + 4π
c
⃗J.
Maxwell’s equations stand as a crowning achievement in electromagnetism.
It encapsulates the interdependent phenomena of induction and the static
source of the various ﬁelds. Furthermore, solving the equations for empty
space (i.e., ρ = 0 and ⃗J = ⃗0) leads to an interpretation of light as an
electromagnetic wave.
Hidden in Maxwell’s equations lie relativistic eﬀects. If a charged par-
ticle travels fast (a non-trivial fraction of the speed of light), then due
to relativistic eﬀects, its electric ﬁeld appears distorted to a stationary ob-
server. Lorentz transformations, presented in Problem 2.4.14, describe how
the electric and magnetic ﬁelds look diﬀerent in diﬀerent moving frames of
reference.
Having developed considerable analytical machinery in the previous
chapters, we are in a position to reformulate the theory of electromag-
netism in a more concise way. We work in a four-dimensional Lorentzian
spacetime, which means the pseudometric
  = ⟨, ⟩has index 1. We label
the coordinates as x0 = ct, x1 = x, x2 = y, and x3 = z. If we use the
speciﬁc pseudometric η deﬁned by
η00 = −1,
ηii = 1 for i = 1, 2, 3,
and
ηij = 0 otherwise,
then we call our conﬁguration space the Minkowski spacetime. However,
as we wish to leave open the possibility of considering electromagnetism in
a curved spacetime, we assume
  can be non-Minkowski.
Deﬁne the 4-vector potential
 as the 1-form with components
Ai = (−φ, A1, A2, A3).
(6.21)
We call the electromagnetic tensor
 as the 2-form
F = −
3

i=1
Eidx0 ∧dxi +
3

i=1
Bi(˜⋆dxi)
= −E1 dx0 ∧dx1 −E2 dx0 ∧dx2 −E3 dx0 ∧dx3
+ B1 dx2 ∧dx3 −B2 dx1 ∧dx3 + B3 dx1 ∧dx2,

266
6. Applications of Manifolds to Physics
where by ˜⋆we mean the Hodge star operator acting only on the space
variables. If we exhibit the components of
 in an antisymmetric matrix,
we write
Fμν =
⎛
⎜
⎜
⎝
0
−E1
−E2
−E3
E1
0
B3
−B2
E2
−B3
0
B1
E3
B2
−B1
0
⎞
⎟
⎟
⎠.
(6.22)
In Problem 4.3.7, one showed that Faraday’s law for potential from Equa-
tion (6.20) can be expressed simply as
d =
.
Interestingly enough, this formula does not refer to any metric but rather
to the simple fact that the electromagnetic tensor
 is an exact form.
We also deﬁne the 4-current vector by
 = (cρ, J1, J2, J3), where ρ
is the charge density and (J1, J2, J3) = ⃗J is the classic current-density
vector. Using the Minkowski metric η, recall that by F αβ we mean the
raising-indices operation F αβ = ηαμηβνFμν and similarly for the lowering
operation Jα = ηαβJβ. Since there is no possibility of confusion for
, we
write
♭for the covector associated to
. Note that
F αβ =
⎛
⎜
⎜
⎝
0
E1
E2
E3
−E1
0
B3
−B2
−E2
−B3
0
B1
−E3
B2
−B1
0
⎞
⎟
⎟
⎠
and
Jα = (−cρ, J1, J2, J3).
With this setup, it is not hard to show that Maxwell’s equations can be
written in tensor form as
F βα
;α = 4π
c Jβ,
εαβγδFαβ;γ = 0.
(6.23)
Note that since we are using a ﬂat pseudometric, the second equation in
Equation (6.23) can be written equivalently as
εαβγδFαβ;γ = Fαβ;γ + Fβγ;α + Fγα;β = ∂γFαβ + ∂αFβγ + ∂βFγα = 0.
Using 4-vectors, one can easily describe the potential between the cur-
rent 4-vector and the potential 4-vector. First, we deﬁne the D’Alembertian
operator as
□= ∂2
∂x2 + ∂2
∂y2 + ∂2
∂z2 −1
c2
∂2
∂t2 .

6.2. Electromagnetism
267
Note that applying Equation (6.18) to Equations (6.19) and (6.17), one can
show that ⃗∇· ⃗A = −1
c∂ϕ/∂t. Thus, taking the divergence of ⃗E expressed
in Equation (6.20), we obtain
□φ = −4πρ.
(6.24)
Using similar calculations, we can also show that
□Ai = −4π
c Ji
(6.25)
for i = 1, 2, 3.
Using the language of forms, Maxwell’s equations become even simpler.
Let us suppose we are in the context of a ﬂat Minkowski space with the
pseudometric η. Using the result of Problem 5.2.18, the second equation in
Equation (6.23) simply states that d = 0. Therefore, Maxwell’s equations
can be restated as
⋆d(⋆) = 4π
c
♭,
d = 0,
(6.26)
where ⋆is the Hodge star operator on the Minkowski space. Though we
presented Equation (6.26) as a reformulation of Maxwell’s equations in
Minkowski space, it turns out this formulation works in any curved space-
time, regardless of the metric. The composition of operations ⋆d⋆has ap-
peared in this text before as the divergence operator on a vector ﬁeld. The
result from Problem 5.1.20 shows that the ﬁrst equation of Equation (6.26)
can be rephrased as div
 = 4π/c
♭. Furthermore, the formula for the
divergence operator on tensors introduced in Problem 5.2.16 immediately
recovers Equation (6.23).
(One must take a little care with the reformulation given in Equa-
tion (6.26).
Some authors deﬁne the current-density vector as our ⋆♭.
However, because of Proposition C.6.11 and Proposition 2.4.14, the com-
ponents of ⋆♭do not transform as the components of a tensor, but as
the components of a tensor density or weight -1, a concept that we do not
deﬁne here.)
Problem 2.4.14 described the Lorentz transformations, which are trans-
formations that preserve the Minkowski metric.
According to the laws
of special relativity, the type of Lorentz transformation described in Prob-
lem 2.4.14(c), with β = v
c where c the speed of light, is precisely the change
of coordinates that corresponds to a second frame traveling at velocity v
along the x-axis with respect to a reference frame.

268
6. Applications of Manifolds to Physics
Problems
6.2.1. Suppose we are in R1,3, and let F be the standard reference frame. Sup-
pose that another frame F′ keeps the x-, y-, and z-axes in the same orien-
tation but has an origin O′ that travels at velocity v along the x-axis of F.
Let ⃗E and ⃗B be joint electric and magnetic force ﬁelds with coordinates
(E1, E2, E3) and (B1, B2, B3) as observed in F. Use the electromagnetic
tensor from Equation (6.22) and the coordinate transformation described
in Problem 2.4.14 to show that in F′ the components of the same vector
ﬁelds are observed as having the components
E′
1 = E1,
E′
2 = γ(E2 −βB3),
E′
3 = γ(E3 + βB2),
B′
1 = B1,
B′
2 = γ(B2 + βE3),
B′
3 = γ(B3 −βE2).
(This result conforms to standard results of special relativistic eﬀects in
electromagnetism. [44, Equation (58), Chapter 6].)
6.2.2. Let f be a smooth function deﬁned over the Minkowski space R1,3. As
always, set x0 = ct, x1 = x, x2 = y, and x3 = z. Prove that
d(⋆(df)) = c□fdt ∧dx ∧dy ∧dz.
6.2.3. Suppose we are in Minkowski spacetime.
(a) Prove that 1
2F αβFαβ = ∥⃗E∥2 −∥⃗B∥2. Conclude that ∥⃗E∥2 −∥⃗B∥2
is preserved under any Lorentz transformation.
(b) Prove that −1
4ηij(⋆F)jkηilFlk = ⃗E · ⃗B.
Conclude that ⃗E · ⃗B is
preserved under any Lorentz transformation.
6.2.4. Let M be any pseudo-Riemannian manifold. Consider the operation that
consists of the compositions ⋆d ⋆d.
(a) Show that ⋆d ⋆d is an R-linear operator Ωk(M) →Ωk(M) for k <
dim M.
(b) Let M = Rn be a standard Euclidean space. Recalling that Ω0(M) =
C∞(M), show that for any smooth function f,
⋆d ⋆df = ∇2f,
where ∇2 is the usual Laplacian ∇2 =
∂2
∂(x1)2 + · · · +
∂2
∂(xn)2 .
(c) Suppose we are in Minkowski space.
Show that □= ⋆d ⋆d, and
conclude that Equations (6.24) and (6.25) can be summarized by
⋆d ⋆d   = 4π
c
♭.
6.2.5. In [57, Equation (5.38)], the author states that “the full action for the
electrically charged point particle is”
S = −mc
%
P
ds + q
c
%
P
Aμ dxμ,
(6.27)

6.3. Geometric Concepts in String Theory
269
where ds is given by Equation (5.11) and Aμ are the components of the
potential covector given in Equation (6.21). Suppose a charged particle
travels along a path (x1(t), x2(t), x3(t)).
(a) Write the action in Equation (6.27) as an integral of time t alone.
(b) Determine the Lagrangian for this system, and write down Lagrange’s
equations of motion.
(c) Write down Hamilton’s equations of motion.
6.3
Geometric Concepts in String Theory
What is generically understood in physics as string theory is a collection of
theories called superstring theories. The name of these models derives from
the fact that in many of the ﬁrst proposed theories, elementary particles
were viewed as a string. Since then, theories have been formulated in terms
of points or surfaces. The string can be either open on the ends or be a
closed loop. For theoretical reasons, the length of the strings should be
on the order of the Planck length, ℓP = 1.6162 × 10−35 m. This size is
so small as to render it impossible to directly observe the string structure
with present technology or, so it would seem, with technology that will be
available in the near future.
In this model, observed properties of the
particle, such as mass or electric charge, arise as speciﬁc properties of
the vibration of the string.
A string in common day occurrence is made of some material like thread
or wire. One could ask what these strings are made of, i.e., what is the
nature of the “thread.” This type of question is, however, vacuous because
the string is not made up of any constituent parts. One should rather think
of the particle-wave duality that drew considerable debate during the incep-
tion of quantum mechanics. In this duality, under diﬀerent circumstances,
a particle would exhibit billiard-ball–like behavior while in other circum-
stances it would display a wave-like behavior. While some physicists dis-
cussed the fundamental nature of particles, many simply emphasized the
fact that growing experimental evidence supported the probability wave
function model, without worrying about the ontology.
As a reﬁnement to the standard model of quantum mechanics, string
theory bears a similar duality in that one thinks of the particle as having
a string nature as well as a probability wave nature.
The space of the
“state” functions (i.e., functions that describe the state of the particle) is
the same, but there are more operators than in the point-particle theory.
In practice, instead of debating the nature of the string, the theories work

270
6. Applications of Manifolds to Physics
x
y
z
S
Figure 6.3. The world sheet of a (nonrelativistic) closed string.
out mathematical consequences of this formulation in the hope that the
resulting theory agrees with experimental observations and uniﬁes without
irreparable inconsistencies previously established theories.
Our goal in this section is to introduce a few of the geometric notions
that underlie the relativistic dynamics of a string. Issues of quantization
of these dynamics exceed the scope of this book.
We ﬁrst consider the nonrelativistic dynamics of a string of length L in
Euclidean Rn. If the string is open, we can pick an end of the string and
use the arclength parameter to locate a point on the string. If it is closed,
we pick a speciﬁc point on the string and locate other points on the string
using the same arclength parameter. The position of the string in space at
time t is described by a smooth function X : [0, L]×R →Rn, where X(s, t)
is the location of the point of position s on the string at time t. Therefore,
while the trajectory of a classic particle is described by a curve in Rn, the
“trajectory” of a string is a surface (see Figure 6.3). In keeping with the
terminology of “world line” for a relativistic point particle, the surface S
is called the world sheet of the string.
To study the dynamics of a relativistic string, we must work in the
context of a Lorentzian spacetime. (This can be curved or ﬂat and can have
any number of space dimensions but can have only one time dimension. In
other words, the pseudometric on the space has index 1.) As always, the
coordinates in the spacetime are xμ = (x0, x1, · · · , xd), with d being the
number of space dimensions and x0 = ct.

6.3. Geometric Concepts in String Theory
271
One can no longer parametrize the world sheet S with the time parame-
ter t since x0 = ct is one of the coordinates in the target space. Nonetheless,
the world sheet requires two parameters, say ξ1 and ξ2. Furthermore, we
can no longer give the same deﬁnition of the domain of X as in the nonrel-
ativistic description of moving strings. One refers to the domain of X as
the parameter space for the world sheet.
Now we encounter something new in Lorentzian spacetime that we never
encountered in the study of Riemannian manifolds.
The world sheet S
must be such that at each point there exists at least one spacelike tangent
vector and at least one timelike tangent vector (recall Section 5.1.4 for the
deﬁnitions). It is not hard to see the need for a spacelike tangent vector.
Any point in time corresponds to a slice of x0. Intersecting such a slice with
S gives the locus of the string at a given time. Any point on the string in
this slice will have a tangent vector that is a spacelike vector. On the other
hand, if there did not exist a timelike tangent vector at some point on S,
one would interpret that as that point not having any evolution through
time. This is not a physical situation. Hence, at each point P of S, the
tangent space has both a timelike direction and a spacelike direction. This
is the criterion for motion of the string.
If g is the pseudo-Riemannian metric of the spacetime target space,
then the induced metric ˜g on the tangent bundle T S is deﬁned by
˜gij = g
∂X
∂ξi , ∂X
∂ξj
	
.
The criterion of motion for the string is equivalent to ˜g having index 1 at
all points of S.
Proposition 6.3.1. Let X(ξ1, ξ2) be a parametrization for a surface S in Lo-
rentzian space with metric g such that at each point of S, X has at least
one nontrivial spacelike tangent vector and at least one nontrivial timelike
vector. Then
det(˜gij) = det

g
∂X
∂ξi , ∂X
∂ξj
		
< 0
at all points of S.
Proof: Let P be a point on S, and let V (α) be the vector in TP S deﬁned by
V (α) = cos α∂X
∂ξ1 + sin α∂X
∂ξ2

272
6. Applications of Manifolds to Physics
for α ∈[0, 2π]. Then
∥V (α)∥2 = cos2 α g
∂X
∂ξ1 , ∂X
∂ξ1
	
+ 2 sin α cosα g
∂X
∂ξ1 , ∂X
∂ξ2
	
+ sin2 α g
∂X
∂ξ2 , ∂X
∂ξ2
	
(6.28)
= cos2 α ˜g11 + 2 sin α cos α ˜g12 + sin2 α ˜g22.
The property of tangent vectors of being timelike or spacelike is indepen-
dent of the length or sign of the vector. Thus, for some α1, there exists a
vector V (α1) such that ∥V (α1)∥2 < 0, and for some α2 there exists a V (α2)
such that ∥V (α2)∥2 > 0. Furthermore, ∥V (αi + π)∥2 = ∥V (αi)∥2. Hence,
since ∥V (α)∥2 changes sign twice over α ∈[0, π], it must have at least two
distinct roots. Therefore, Equation (6.28) leads to quadratic equations in
tan α or cot α. Either way, according to the quadratic formula, the equation
for tan α or for cot α has two distinct roots if and only if
˜g2
12 −˜g11˜g22 > 0.
The proposition follows immediately.
□
It is customary to parametrize S with two variables labeled σ and τ
deﬁned in such a way that for all points in the parameter space, ∂X/∂σ
is a spacelike tangent vector and ∂X/∂τ is a timelike tangent vector. The
parameters σ and τ no longer directly represent position along the string
or time, respectively. One could say that σ and τ approximately represent
position and time along the world sheet. In fact, with the sole exception
of the endpoints, when considering the motion of open strings, one cannot
know the movement of individual points on the string. In general, σ ranges
over a ﬁnite interval [0, σ1], while τ ranges over all of R.
The derivatives ∂X/∂σ and ∂X/∂τ occur often enough that it is com-
mon to use the symbols X′ and ˙X for them, respectively. In components,
we write
X′μ = ∂Xμ
∂σ
and
˙Xμ = ∂Xμ
∂τ .
The area element of the world sheet in this Lorentzian space is deﬁned as
dA =

−det ˜g =

g( ˙X, X′)2 −g( ˙X, ˙X)g(X′, X′).
We ﬁnish this section by brieﬂy discussing the Nambu-Goto action for a
free relativistic string and the resulting equations of motion for the string.

6.3. Geometric Concepts in String Theory
273
Figure 6.4. A vibrating string.
Deﬁnition 6.3.2. Let g = ⟨, ⟩be a pseudometric of index 1. The Nambu-
Goto action of the string is deﬁned as
S
def
= −T0
c
%%
S
dA = −T0
c
% τ2
τ1
% σ1
0

−det ˜gαβ dσ dτ
(6.29)
= −T0
c
% τ2
τ1
% σ1
0

⟨˙X, X′⟩2 −∥˙X∥2 ∥X′∥2 dσ dτ,
where T0 is called the string tension and c is the speed of light.
Before proceeding, we must give some justiﬁcation for this deﬁnition.
First of all, it mimics the action for a free relativistic particle given in
Equation (6.12). The diﬀerence is that instead of deﬁning the action as
a multiple of the length of the path in the ambient Lorentzian space, we
deﬁne it as a multiple of the area of the world-sheet. Furthermore, this
action is obviously invariant under reparametrization since the area is a
geometrical quantity.
As a more convincing argument, we consider a classical vibrating string
of length ℓ. Using Figure 6.4 as a guide, we model the motion of the string
by a function y(x, t) that measures the deviation of the string from rest at
horizontal position x and at time t. If the string has constant density μ0
and tension T0, then the diﬀerential equation of motion for a string with
small deviations is
μ0
∂2y
∂t2 = T0
∂2y
∂x2 .
The fraction μ0/T0 has the units of time2/length2 and is in fact equal to
1/c2, where c is the speed of propagation of the wave. It is not hard to
reason that at any point in time t, the total kinetic energy of the string is
T =
% ℓ
0
1
2μ0
∂y
∂t
	2
dx,

274
6. Applications of Manifolds to Physics
and that the total potential energy is
V =
% ℓ
0
1
2T0
∂y
∂x
	2
dx.
Thus the Lagrangian of the system is
L =
% ℓ
0
1
2μ0
∂y
∂t
	2
−1
2T0
∂y
∂x
	2
dx.
(6.30)
The integrand of Equation (6.30) is called the Lagrangian density and is
denoted by L. It is explicitly a function of ∂y/∂t and ∂y/∂x. The action
of the system for t ∈[t1, t2] is
S =
% t2
t1
% ℓ
0
1
2μ0
∂y
∂t
	2
−1
2T0
∂y
∂x
	2
dx dt.
Now assume that we are in a Minkowski space R1,2 with the ﬂat pseu-
dometric −ds2 = −(dx0)2 + (dx1)2 + (dx2)2, where x0 = ct, x1 = x, and
x2 = y. After some manipulation, we can rewrite that string action as
S = −T0
c
% ct2
ct1
% ℓ
0
1
2

−
 ∂y
∂x0
	2
+
 ∂y
∂x1
	2
dx1 dx0.
(6.31)
The motion of the string can be parametrized in R1,2 by ⃗f(x0, x1) =
(x0, x1, y(x0, x1)).
With the inner product induced by this metric, the
area element becomes
:
;
;
;
<−
⎛
⎝
6
∂⃗f
∂x0 , ∂⃗f
∂x0
7 6
∂⃗f
∂x1 , ∂⃗f
∂x1
7
−
6
∂⃗f
∂x0 , ∂⃗f
∂x1
72⎞
⎠
=
!
1 −
 ∂y
∂x0
	2
+
 ∂y
∂x1
	2
∼= 1 + 1
2

−
 ∂y
∂x0
	2
+
 ∂y
∂x1
	2
.
Adjusting for x0 = ct, the Lagrangian associated to Equation (6.31) diﬀers
from the linear approximation to the Nambu-Goto action
−T0
% ℓ
0
1 + 1
2

−
 ∂y
∂x0
	2
+
 ∂y
∂x1
	2
dx1

6.3. Geometric Concepts in String Theory
275
by
−T0
% ℓ
0
1 dx1 = −T0ℓ= −μ0ℓc2 = −mc2.
Similar to the linear approximation to the Lagrangian for the free rela-
tivistic particle in Equation (6.14), this diﬀerence is precisely the negative
of the rest energy mc2 of the string.
Since this is a constant, it leaves
the Euler-Lagrange equations unchanged. This shows how the classic La-
grangian of a wave is a linear approximation for the Lagrangian associated
to the Nambu-Goto action.
We now wish to obtain the equations of motion associated to the Nambu-
Goto action. The Lagrangian density in Equation (6.29) is
L( ˙Xμ, X′μ) = −T0
c

⟨˙X, X′⟩2 −∥˙X∥2 ∥X′∥2.
(6.32)
This is an explicit function of the eight variables X′μ and
˙Xμ for μ =
0, 1, 2, 3. Hamilton’s principle states that the system will evolve in such a
way as to minimize the action. According to a generalization of the Euler-
Lagrange Theorem in the calculus of variations (see Problem 6.3.2), the
Nambu-Goto action is minimized if and only if the Xμ(s, t) satisfy
d
dσ
 ∂L
∂X′μ
	
+ d
dτ
 ∂L
∂˙Xμ
	
= 0
for all μ. These are the equations of motion for a relativistic string, whether
open or closed. More explicitly, the equations of motion read
∂
∂σ
⎛
⎝⟨˙X, X′⟩gμν ˙Xν −∥˙X∥2gμνX′ν

⟨˙X, X′⟩2 −∥˙X∥2 ∥X′∥2
⎞
⎠
+ ∂
∂τ
⎛
⎝⟨˙X, X′⟩gμνX′ν −∥X′∥2gμν ˙Xν

⟨˙X, X′⟩2 −∥˙X∥2 ∥X′∥2
⎞
⎠= 0
(6.33)
for μ = 0, 1, 2, 3. At ﬁrst glance, these equations are incredibly complicated.
They involve a system of four second-order partial diﬀerential equations of
four functions each in two variables. A remarkable fact among the basic
results of string theory is that it is possible to solve Equation (6.33) once
one makes a suitable choice of σ and τ.
Using the notion of generalized momenta deﬁned in Equation (6.5), we
deﬁne two momenta densities Pσ and Pτ that are cotangent vectors on the

276
6. Applications of Manifolds to Physics
world sheet with components
Pσ
μ
def
= −T0
c
⟨˙X, X′⟩gμν ˙Xν −∥˙X∥2gμνX′ν

⟨˙X, X′⟩2 −∥˙X∥2 ∥X′∥2
,
Pτ
μ
def
= −T0
c
⟨˙X, X′⟩gμνX′ν −∥X′∥2gμν ˙Xν

⟨˙X, X′⟩2 −∥˙X∥2 ∥X′∥2
.
(One should note that in this case the superscript σ and τ in Pσ
μ and Pτ
μ are
not indices but are parameter indicators.) Then the equations of motion
read
∂Pσ
μ
∂σ + ∂Pτ
μ
∂τ
= 0.
(6.34)
This is all we will say about the underlying geometry in string theory.
String theory extends well beyond the scope of this book, and we encourage
the reader to consult [57] for an artful and accessible introduction to the
subject.
Problems
6.3.1. Show that at some point on the world-sheet of a string, if the point moves
at the speed of light, there is no timelike direction.
6.3.2. Use the methods of calculus of variations provided for the proof of The-
orem B.3.1 to prove the following result.
Let x1(s, t), . . . , xn(s, t) be
n twice-diﬀerentiable functions in two variables.
Denote derivatives by
x′i = dxi/ds and ˙xi = dxi/dt. Suppose that a function f is given explic-
itly in terms of xi, x′i, ˙xi, s, and t. Show that the integral
% t2
t1
% s2
s1
f(x1, . . . , xn, x′1, . . . , x′1, ˙x1, . . . , ˙xn, s, t) ds dt
is optimized when
∂f
∂xi −d
ds
 ∂f
∂x′i
	
−d
dt
 ∂f
∂˙xi
	
= 0
for all i = 1, . . . , n.
6.3.3. Consider a free relativistic string with σ-length σ1. The Hamiltonian for
the system is
H =
% σ1
0
Pτ
μ ˙Xμ −L dσ.
(a) Recover the equations of motion as in Equation (6.34) from Hamil-
ton’s equations of motion.
(b) Show that H vanishes identically for all τ.

6.3. Geometric Concepts in String Theory
277
(c) Let L be as in Equation (6.32). Consider the matrix with entries
∂2L/(∂˙Xμ∂˙Xν). Show that this matrix has two 0 eigenvalues, with
eigenvectors ˙X and X′. Deduce the following conditions on the mo-
mentum Pτ:
iX′(Pτ) = Pτ
μX′μ = 0,
∥Pτ∥2 + T 2
0
c2 ∥X′∥2 = gμνPτ
μPτ
ν + T 2
0
c2 gμνX′μX′ν = 0.
6.3.4. Show that according to the relativistic string equations of motion, the
endpoints of an open string move with the speed of light.
6.3.5. Consider a relativistic string in Minkowski space R1,d but only consider
the history of the string in the real space. We parametrize this history
as ⃗X(σ, τ). (We use the vector superscript to indicate vectors in the Eu-
clidean R part of the spacetime.) Deﬁne s(σ) to be the length of the string
along [0, σ], so that s(0) = 0 and s(σ1) is the length of the string. Also
set t = τ.
(a) Prove that ∂⃗X/∂s is a unit vector.
(b) Deﬁne the vector ⃗v⊥as the component of the velocity vector ∂⃗X/∂t
that is perpendicular to the string. Thus,
⃗v⊥= ∂⃗X
∂t −

∂⃗X
∂t · ∂⃗X
∂s

∂⃗X
∂s ,
where we use the usual dot product. Prove that one can write the
Nambu-Goto string action as
S = −T0
% t2
t1
% σ1
0
ds
dσ

1 −v2
⊥
c2 dσ dt
6.3.6. (Phys,*) The Nambu-Goto Bubble Action. Suppose that instead of consid-
ering particles as strings, we model them as bubbles. Then a world sheet
S is given by a function X(σ, ¯σ, τ) into a pseudo-Riemannian manifold M
with index 1.
(a) Explain why it still makes sense to deﬁne the action of the free motion
of the relativistic bubble for τ1 ≤τ ≤τ2 by
S = −T0
c
%%%
S

−det ˜gαβ dσ d¯σ dτ,
where T0 is now a surface tension and ˜g is the metric induced from
M on S.
(b) Write down the equations of motion associated to this action.

278
6. Applications of Manifolds to Physics
6.4
A Brief Introduction to General Relativity
As with the previous sections, one might consider it outlandish (to say
the least!)
that we only allow one section to discuss general relativity.
General relativity is a vast subject with contributions from an uncountable
(well, mathematically countable, but very large) number of scientists, and
it stands alongside quantum mechanics as one of the most revolutionary
ideas in physics of the twentieth century.
On the other hand, most textbooks on general relativity take a consid-
erable amount of time to develop the techniques of analysis on manifolds,
in particular, pseudo-Riemannian manifolds. However, these are precisely
the mathematical methods we have developed in the previous few chapters,
so we are in a position to introduce some diﬀerential geometric concepts in
general relativity as applications.
In the rest of this section, when we refer to special relativity, we mean
that we are in the context of a Minkowski space R1,3 with metric η, de-
scribed in Deﬁnition 5.1.15. When we refer to general relativity, we mean
that we work in a pseudo-Riemannian manifold M with index 1 and with
non-Minkowski metric tensor
 . Since each tangent space to M has the
structure of a ﬂat Lorentzian space R1,3, such manifolds are also called
locally Minkowski.
6.4.1
The Stress-Energy Tensor
In the mechanics of elastic media, one encounters the concept of a stress
tensor, which is a tensor-valued function deﬁned at each point within the
body or medium. Suppose the body is in equilibrium but subject to exter-
nal forces and/or body forces (i.e., forces that act through the whole body).
Then there must exist internal forces. Let Q be a point, ⃗n a vector based
at Q, and consider the area element ΔA that is in the plane perpendicular
to ⃗n and has area equal to ∥⃗n∥(see Figure 6.5). Let Δ⃗F be the overall
internal forces distributed over the area element ΔA. The stress vector
through the area element ΔA is the vector
⃗T(⃗n) =
lim
ΔA→0
Δ⃗F
ΔA.
(6.35)
It is not hard to show that the function ⃗T(⃗n) is a linear function [53,
Section 10.6]. Thus the stress tensor with respect to an orthogonal basis B
based at the point Q is the matrix σ such that
⃗T(⃗n) = σ [⃗n]B .

6.4. A Brief Introduction to General Relativity
279
Q
⃗n
T(⃗n)
dA
Figure 6.5. Stress tensor: an area element in a continuous medium.
Consider now a small rectangular parallelepiped with sides parallel to the
coordinate planes. The stress acts on each face as depicted in Figure 6.6.
Then the columns of σ are given by
σ⃗ei = T (⃗ei) =
⎛
⎝
σi1
σi2
σi3
⎞
⎠.
Under minimal assumptions, one can reason that the stress tensor σ is
symmetric.
As a simple example, in an ideal ﬂuid, the stress on any small area
element is composed only of pressure, and there is no shearing force. Con-
sequently, the stress tensor is σ = P
, where P is the pressure and
 is the
3 × 3 identity matrix. (This restates the claim given in calculus texts on
the applications of integration to hydrostatics when one says that “at any
point in a liquid the pressure is the same in all directions” [52, p. 576].)
The stress tensor arises also in the dynamics of viscous ﬂuids where it is
no longer necessarily diagonal. The stress tensor at a point “may be a
function of the density and temperature, of the relative positions and ve-
locities of elements near [the point], and perhaps also the previous history
of the medium.” [53, p. 434] This characterization describes the stress ten-
sor as a function of many ambient quantities, but the reference to “relative
positions” indicates that the stress tensor need not be diagonal.
Einstein’s equation in general relativity involves the so-called stress-
energy tensor. This tensor is diﬀerent from the stress tensor but is based
on exactly the same concept.
In general relativity, the velocity 4-vector of a particle on a world line
P parametrized by X is the tangent vector along P given by
 = dX
dτ ,

280
6. Applications of Manifolds to Physics
x
y
z
⃗e1
T(⃗e1)
⃗e2
T(⃗e2)
⃗e3
T(⃗e3)
Figure 6.6. Action of stress on an inﬁnitesimal coordinate cube.
where, just as in Equation (5.12), we deﬁne the proper time of a particle on
P as τ = 1
c
3
P ds. In special relativity, using Equations (5.11) and (5.12),
it is not hard to show that
 = (u0, u1, u2, u3) = (γc, γvx, γvy, γvz),
(6.36)
where γ = (1 −v2/c2)−1/2 and ⃗v = (dx/dt, dy/dt, dz).
The momentum 4-vector of a particle of rest mass m0 is the tangent
covector deﬁned by
 = m0
♭
or in coordinates as
pμ = m0gμνuν.
This vector is often called the energy-momentum 4-vector because in special
relativity, where we use the metric η, we have
(p0, p1, p2, p3) =

−E
c , px, py, pz
	
,
(6.37)
where (px, py, pz) = m0γ(vx, vy, vz) is the relativistic 3-vector momentum.
To fully understand Equation (6.37), it is essential to be aware of the
fundamental result in special relativity of the correspondence between mass
and energy via E = mc2 = m0γc2. Therefore, we talk about mass and
energy of particles interchangeably. One should also note that even if a
particle of rest mass m0 is not moving, it still has a nontrivial momentum
4-vector of
 = (−m0c, 0, 0, 0).

6.4. A Brief Introduction to General Relativity
281
Underlying the assumptions that deﬁne the stress-energy tensor, we as-
sume that “spacetime contains a ﬂowing river of 4-momentum” [39, p.130].
Any mass that is moving or anything with energy contributes to the 4-
momentum. We could think of an individual particle, in which case the
4-momentum would only be deﬁned on the particle’s world line, or we could
consider a system of many particles carrying this 4-momentum. In the lat-
ter case, we should think of the 4-momentum as a covector ﬁeld on the
spacetime manifold M, that is, as a 1-form.
Let
 be any 1-form on M. Then at each point Q ∈M,
Q is per-
pendicular (using
  = ⟨, ⟩at Q) to a three-dimensional subspace of TQM.
This subspace can be spanned by vectors AQ, BQ, and CQ such that
Q(u) = −Vol
 (u, AQ, BQ, CQ),
(6.38)
where on the right-hand side we mean the 4-volume (with respect to
 ) of
the 4-parallelepiped spanned by u, AQ, BQ, and CQ. Then the 3-volume of
the 3-parallelepiped AQ, BQ, CQ is the length ∥∥. We call this 3-volume
ΔV . Then we deﬁne the (mixed) stress-energy tensor at P in the direction
of
 by
	 ♭() =
lim
ΔV →0
Δ
ΔV .
In other words, at a point Q,
	 ♭() gives the rate of change of 4-momentum
through the plane perpendicular to
. As deﬁned,
	 ♭is a tensor of type
(1, 1). For the contravariant stress-energy tensor, denoted by
	, we simply
raise the covariant index by T αβ = gανT β
ν . We can also deﬁne the stress-
energy tensor
	 by saying that for all 1-forms α on M,
	(α,
) = ⟨α,
⟩
 .
The tensor
	 is a tensor ﬁeld of type (2, 0) on M.
The stress-energy
	 is also called the energy-momentum tensor because
it contains information pertaining to the momentum ﬂowing through space
and the presence of static or moving energy in space. The name “stress-
energy tensor” is commonly used since it is modeled oﬀthe stress tensor
in mechanics of elastic media.
The following gives a summary of the information included in the stress-
energy tensor. Assuming j, k > 0,
T 00 = density of energy (including mass),
(6.39)
T j0 = jth component of the momentum density,
(6.40)

282
6. Applications of Manifolds to Physics
T 0k = kth component of the energy ﬂux,
(6.41)
T jk = (j, k)th component of the momentum stress
(6.42)
= k’th component of the ﬂux of the j-component of momentum.
The notion of ﬂux in this context refers to a similar limit as in Equa-
tion (6.35) but in the situation where one is concerned with the movement
of something (energy, ﬂuid momentum, heat, etc.) through the inﬁnites-
imal area element d ⃗A. In fact, with this particular concept of ﬂux, one
can deﬁne the stress-energy tensor in short by saying that T kj is the kth
component of the ﬂux of the jth component of the 4-momentum.
We now state two facts about the stress-energy tensor that we do not
fully justify here.
Proposition 6.4.1. The (contravariant) stress-energy tensor
	 is symmetric.
Proposition 6.4.2 (Einstein’s Conservation Law). The conservation of energy is
equivalent to the identity
div
	 = T αβ
;β = 0.
Proof (Sketch): Suppose that energy is conserved in a certain region of M.
In other words, though energy and mass may move around, no energy or
mass is created or annihilated in M.
Then given any four-dimensional
submanifold V with boundary ∂V, the total ﬂux of 4-momentum passing
through ∂V must be 0. We can restate this as
%%%
∂V
	 · d
 (3) = 0,
where d
 (3) is the 3-volume element with direction along the outward-
pointing normal vector to ∂V. (We can view this as a volume 1-form.) By
the product · we mean the contraction of
	 with the volume 1-form element
d
 (3). By Stokes’ Theorem applied to pseudo-Riemannian manifolds, one
can show that
%%%%
V
div
	dV (4) =
%%%
∂V
	 · d
 (3) = 0.
Since this is true for all V as described above, one can show with a limiting
argument that div
	 = 0 everywhere.
□
Example 6.4.3 (Perfect Fluid Stress-Energy Tensor). A perfect ﬂuid is a ﬂuid in
which the pressure p is the same in any direction. The ﬂuid must be free

6.4. A Brief Introduction to General Relativity
283
of heat conduction and viscosity and any process that can cause internal
sheers. Using the interpretation of
	 from Equations (6.39)–(6.42), we
immediately see that T jk = 0 if j ̸= k and 0 < j, k. Furthermore, since
the pressure is the same in all directions, T jj = p for j = 1, 2, 3.
For
components involving j = 0 or k = 0, we ﬁrst have T 00 = ρ, the energy
density. This quantity includes rest mass m = γm0 density but also other
types of energy such as compression energy. For the remaining oﬀdiagonal
terms T 0j = T j0, these are 0 because of the assumption that there is no
heat conduction in the perfect ﬂuid. Thus, the stress-energy tensor has
components
T αβ =
⎛
⎜
⎜
⎝
ρ
0
0
0
0
p
0
0
0
0
p
0
0
0
0
p
⎞
⎟
⎟
⎠.
(6.43)
If we suppose that an observer is in the Lorentz frame that is at rest
with respect to the movement of the ﬂuid, then the velocity has components
uα = (1, 0, 0, 0). With respect to the Minkowski metric η, we can write
Equation (6.43) as
T αβ = (ρ + p)uαuβ + pηαβ.
We can rewrite this in a coordinate-free way in any metric as
	 = p  −1 + (p + ρ) ⊗
,
where we have written
  −1 for the contravariant tensor of type (2, 0) asso-
ciated to the metric tensor
 .
Example 6.4.4 (Electromagnetic Stress-Energy Tensor). Directly using the inter-
pretation of
	 given in Equations (6.39)–(6.42) and results from electro-
magnetism, which we do not recreate here, one can determine the compo-
nents of the stress-energy tensor for the electromagnetic ﬁeld in free space.
If F μν are the components of the electromagnetic ﬁeld tensor, then
T αβ = 1
μ0

F αμgμνF βν −1
4gαβF μνFμν
	
in SI units
= 1
4π

F αμgμνF βν −1
4gαβF μνFμν
	
in CGS units,
where μ0 = 4π × 10−7 N/A−2 is a constant sometimes called the vacuum
permeability.

284
6. Applications of Manifolds to Physics
6.4.2
Einstein Field Equations
The Einstein ﬁeld equations (EFE) are the heart of general relativity. They
stem from the juxtaposition of the two following principles:
1. Every aspect of gravity is merely a description of the spacetime ge-
ometry.
2. Mass (energy) is the source of gravity.
The metric tensor
  encapsulates all the information about the geom-
etry of the spacetime. From
  one constructs the Levi-Civita connection
∇, the (1, 3)-Riemann curvature tensor
, the Ricci curvature tensor
,
and the scalar curvature function R, deﬁned in Chapter 5. (Note: In math
texts on Riemannian geometry, one often denotes by S the scalar curvature
while texts on general relativity invariably denote it by R. Using the bold
font
 to indicate the curvature tensor alleviates any confusion between
the scalar and tensor curvature.)
On the other hand, the stress-energy tensor describes the spacetime
content of mass-energy. In fact, any observer with 4-velocity
 measures
the density of mass-energy as
ρ =
 ·
	 ·
 = Tαβuαuβ.
In order to put together the two above principles, we should be able to
write the tensor
	 exclusively in terms of the components of the metric
tensor
 . The conservation of energy states that div
	 = 0. Also, if
	 is to
serve as a measure of the curvature of spacetime, it should explicitly involve
only components of
 and of
  (no derivatives of any of these terms) and
it should be linear in the components of
. It turns out that under these
restrictions, there are only a few options for a geometric description of
	.
In Problem 6.4.2, one shows that for purely mathematical reasons, these
constraints impose that
Tαβ = C

Rαβ −1
2Rgαβ + Λgαβ
	
,
where Rαβ are the components of the Ricci curvature tensor, R is the scalar
curvature, and Λ and C are real constants. This leads to Einstein’s ﬁeld
equations.
Let
 be the Einstein curvature tensor deﬁned in Deﬁnition 5.4.9. Gen-
eral relativity is summarized in this following equation. The presence of

6.4. A Brief Introduction to General Relativity
285
mass-energy deforms spacetime according to
 + Λ   = 8πG
c4
	
in SI units,
(6.44)
where G = 6.67 × 10−11 m3s−2kg−1 is the gravity constant and Λ is the
cosmological constant. If in addition, one assumes that empty (devoid of
energy) spacetime is ﬂat, then
 = 8πG
c4
	.
(6.45)
Equation (6.45) is called collectively the Einstein ﬁeld equations (EFE),
and the formulas in Equation (6.44) are the Einstein ﬁeld equations with
cosmological constant. These equations are as important in astrophysics
as Newton’s second law of motion is in classic mechanics.
In Equation (2) of Einstein’s original paper on general relativity [19],
Einstein made the assumption that
 vanishes when spacetime is empty
of mass-energy. This corresponds to the assumption that Λ = 0. However,
Equation (6.45) predicts a dynamic universe. This result did not appeal
to Einstein and, at the time, there existed no astronomical evidence to
support this. In 1917, he introduced the constant Λ because it allows for a
static universe. Physically, Λ ̸= 0 would imply the presence of an otherwise
unexplained force that counteracts gravity or a sort of negative pressure.
When Hubble discovered that the universe is expanding, the cosmolog-
ical constant no longer appeared to be necessary and many physicists did
away with it. In fact, in his autobiography, George Gamow relays that
Einstein told Gamow that he considered the introduction of the cosmolog-
ical constant as “the biggest blunder of my life” [25]. However, as of the
writing of this book, the possibility of a small nonzero Λ has resurfaced and
regularly enters into the debates around the current most vexing problems
in physics, namely, the nature of dark energy and the eﬀort to unify gravity
and quantum mechanics.
One should note the Einstein ﬁeld equations (EFE) are very compli-
cated. Finding a solution to the EFE means ﬁnding the metric tensor
 that satisﬁes Equation (6.45). Thus, in their most general form, the EFE
consist of 10 second-order, nonlinear, partial diﬀerential equations of 10
functions gij(x0, x1, x2, x3), with 0 ≤i ≤j ≤3. Determining the trajec-
tory of a particle or of radiation amounts to determining the geodesics in
this metric. Surprisingly, under some circumstances, especially scenarios
that involve a high level of symmetry, it is possible to provide an exact
solution.

286
6. Applications of Manifolds to Physics
One cannot describe in only a few words the full consequences of the
Einstein ﬁeld equations in Equations (6.45) or (6.44). Whole books have
been written about consequences of solutions to this equation that deviate
from Newtonian mechanics: space and time form a single spacetime unit
that is in general curved; light is bent by the presence of massive objects;
stars may collapse and become black holes, which is the name given for
singularities on the spacetime manifold; the universe as a whole expands;
gravity aﬀects the frequency of light . . . .
Physicists have been able to
experimentally verify many of the predictions in favor of general relativ-
ity over Newtonian mechanics. Though any theory that can unify gravity
and quantum mechanics must be able to derive Equation (6.44) and hence
generalize relativity, the Einstein ﬁeld equations have been repeatedly sup-
ported by observation, and they are still held to accurately model nature
at our current possible levels of observation.
6.4.3
The Schwarzschild Metric
We ﬁnish this section with one of the earliest proven and most important
consequences of general relativity, i.e., the Schwarzschild metric, which is
an exact solution to Einstein’s ﬁeld equations.
One of the main contexts in which one can expect to see the eﬀects of
general relativity against Newtonian mechanics is in the context of astron-
omy. The simplest dynamical problem in astronomy involves calculating
the orbit of a single planet around the sun. One can hope that the EFE
for the eﬀect of the sun on the space around it will become simple under
the following two assumptions (approximations):
1. The sun is a spherically symmetric distribution of mass-energy den-
sity.
2. Outside of the sun, the stress-energy tensor should vanish.
The spherical symmetry implies that the components of the metric tensor
should be given as functions of x0 and r alone, where r2 = (x1)2 + (x2)2 +
(x3)2.
Since we are looking only for solutions outside the sun, we are
looking for solutions in a vacuum. Thus
	 = 0, from which we deduce that
 = 0. Thus, Tr
  = 0. However,
Tr
  = Tr
 
 −1
2R
 	
= R −1
2R · 4 = −R,
where this follows from Equation (5.43) and the fact thatTr
   =dim M=4.
Thus, R = 0 and the fact that
 = 0 implies that we are looking for

6.4. A Brief Introduction to General Relativity
287
spherically symmetric solutions to the equation
Rαβ = 0.
Since we are looking for solutions in a vacuum, it seems as though we have
lost information, but, as we shall see, that is not the case.
The following derivation follows the treatment in [51]. We leave some
of the detailed work as exercises for the reader.
A judicious choice of coordinates and a few coordinate transformations
will simplify the problem. We ﬁrst start with the coordinates
(¯x0, x1, x2, x3) = (¯x0, ¯r, θ, ϕ),
where ¯x0 = cT for some timelike variable T , ¯r2 = (x1)2 + (x2)2 + (x3)2
and where θ and ϕ are given in the physics style of deﬁning spherical
coordinates, i.e., so that ϕ is the longitudinal angle and θ is the latitude
angle measured down from a “positive” vertical direction. We know that
the standard line element in spherical coordinates is
ds2 = d¯r2 + ¯r2dθ2 + ¯r2 sin2 θdϕ2.
Though we are not working with the Euclidean metric, spherical symmetry
does imply that the metric tensor in the space coordinates is orthogonal
and that no perpendicular direction to the radial direction is singled out.
Thus, the metric tensor has the form
gαβ =
⎛
⎜
⎜
⎝
g00(¯x0, ¯r)
g01(¯x0, ¯r)
g02(¯x0, ¯r)
g03(¯x0, ¯r)
g10(¯x0, ¯r)
g11(¯x0, ¯r)
0
0
g20(¯x0, ¯r)
0
f(¯x0, ¯r)2
0
g30(¯x0, ¯r)
0
0
f(¯x0, ¯r)2 sin2 θ
⎞
⎟
⎟
⎠,
where f is any smooth function. We actually have some choice on θ and
ϕ because they are usually given in reference to some preferred x-axis and
z-axis. We choose θ and ϕ (which may change over time with respect to
some ﬁxed Cartesian frame) so that g20 = g30 = 0, and then the metric
looks like
gαβ =
⎛
⎜
⎜
⎝
g00(¯x0, ¯r)
g01(¯x0, ¯r)
0
0
g10(¯x0, ¯r)
g11(¯x0, ¯r)
0
0
0
0
f(¯x0, ¯r)2
0
0
0
0
f(¯x0, ¯r)2 sin2 θ
⎞
⎟
⎟
⎠.
We make the coordinate transformation r = f(¯x0, ¯r) and all the other co-
ordinates remain the same. In this coordinate system, the metric looks like

288
6. Applications of Manifolds to Physics
gαβ =
⎛
⎜
⎜
⎝
g00(¯x0, r)
g01(¯x0, r)
0
0
g10(¯x0, r)
g11(¯x0, r)
0
0
0
0
r2
0
0
0
0
r2 sin2 θ
⎞
⎟
⎟
⎠.
(6.46)
Finally, we can orthogonalize the metric tensor by a suitable coordinate
transformation of x0 = ct = h(¯x0, r), with r staying ﬁxed (see Prob-
lem 6.4.3). Since we know that the metric has index 1, we can write the
metric in the coordinate system (x0, r, θ, ϕ) as
gαβ =
⎛
⎜
⎜
⎝
−eν(x0,r)
0
0
0
0
eλ(x0,r)
0
0
0
0
r2
0
0
0
0
r2 sin2 θ
⎞
⎟
⎟
⎠,
(6.47)
where λ and ν are smooth functions. The metric in Equation (6.47) is an
orthogonal metric that is spherically symmetric in the space variables.
Using the notation ˙u = ∂u/∂x0 and u′ = ∂u/∂r, one can show (see
Problem 6.4.4) that the independent nonzero Christoﬀel symbols for the
Levi-Civita connection are
Γ0
00 = 1
2 ˙ν, Γ0
01 = 1
2ν′,
Γ0
11 = 1
2
˙λeλ−ν, Γ1
00 = 1
2ν′eν−λ,
Γ1
01 = 1
2
˙λ, Γ1
11 = 1
2λ′,
Γ1
22 = −re−λ,
Γ1
33 = −r sin2 θe−λ, (6.48)
Γ2
12 = 1
r ,
Γ2
33 = −sin θ cos θ,
Γ3
13 = 1
r ,
Γ3
23 = cot θ.
Though it is a little long to calculate (see Problem 6.4.5), one can then
determine that the only nonzero components of the Ricci tensor are
R00 = eν−λ
ν′′
2 + (ν′)2
4
−ν′λ′
4
+ ν′
r
	
−
¨λ
2 −
˙λ2
4 +
˙λ ˙ν
4 ,
R01 =
˙λ
r ,
R11 = −ν′′
2 −(ν′)2
4
+ ν′λ′
4
+ λ′
r + eλ−ν
 ¨λ
2 +
˙λ2
4 −
˙λ ˙ν
4

,
R22 = −e−λ 
1 + r
2(ν′ −λ′)

+ 1,
R33 = sin2 θ R22.
(6.49)
Since we are trying to solve Rαβ = 0, we obtain conditions on the
functions λ and ν. Since R01 = 0, we deduce immediately that ˙λ = 0,

6.4. A Brief Introduction to General Relativity
289
which means that λ is a function of r alone. Also, since ∂R22/∂t = 0, we
ﬁnd that ∂ν′/∂t = 0. Therefore, we can write the function ν as
ν = ν(r) + f(t)
for some function f(t).
We now make one ﬁnal coordinate change. In the metric line element, t
appears only in the summand eνd(x0)2 = eν(r)ef(t)d(ct)2. So by choosing
the variable ¯t in such a way that
d¯t
dt = ef(t)/2
and then renaming ¯t to just t, we obtain a metric which is independent
of any timelike variable. (The variable ¯t, relabeled as t, is not necessarily
time anymore so one cannot necessarily call the solution static.)
We can now assume there is no t dependence. Simplifying the expression
R00 + eν−λR11 leads to
1
r (λ′ + ν′) = 0.
This implies that λ(r) = −ν(r) + C for some constant C. Without loss of
generality, we can assume that C = 0 since we have not speciﬁed λ or ν.
Thus, we set λ(r) = −ν(r). Then R22 = 0 in Equation (6.49) implies that
e−λ(1 −rλ′) = 1.
Now setting h(r) = e−λ(r), this last equation becomes
h′ + h
r = 1
r .
This is a linear, ﬁrst-order, ordinary, diﬀerential equation whose general
solution is
h(r) = e−λ(r) = 1 −2M
r ,
where M is a constant of integration. One can verify directly that R11 =
R00 = 0 in Equation (6.49) are satisﬁed by this solution and therefore give
no additional conditions.
Therefore, the spherically symmetric vacuum
solution to the EFE gives a metric with line element
−ds2 = −

1 −2M
r
	
c2dt2 +

1 −2M
r
	−1
dr2 + r2dθ2 + r2 sin2 θdϕ2.
(6.50)

290
6. Applications of Manifolds to Physics
This is called the Schwarzschild metric. This metric provided the ﬁrst exact
solution to the Einstein ﬁeld equations.
Though it is still complicated,
this metric can be compared in fundamental importance to the solution
in mechanics to the diﬀerential equations d2⃗x/dt2 = −m⃗g. Many of the
veriﬁable predictions of general relativity arise from this metric.
In order to understand Equation (6.50), one needs to have some sense of
the meaning of the constant M. Obviously, if M = 0, then the Schwarzschild
metric is simply the ﬂat Minkowski metric for spacetime.
To derive an interpretation for M ̸= 0, we study some consequences
of Equation (6.50) for small velocities. If the velocity v is much smaller
than the speed of light, i.e., v ≪c, then special relativity tells us that
proper time is approximately coordinate time τ ∼= t = x0/c. Furthermore,
from Equation (6.36) we can approximate the velocity of any particle as
 = (c, 0, 0, 0). Plugging these into the geodesic equation
d2xi
dτ 2 = −Γi
jk
dxj
dτ
dxk
dτ ,
we obtain the approximate relationship
d2xi
dt2 = −Γi
00c2 = c2
2 gil ∂g00
∂xl ,
where the second equality follows from the formula for Christoﬀel symbols
and the fact that the functions gij are not x0 dependent. However, since
  is diagonal and g00 depends only on r, we ﬁnd that the only nonzero
derivative is
d2r
dt2 = −c2
2

1 −2M
r
	 ∂
∂r

1 −2M
r
	
= −Mc2
r2

1 −2M
r
	
.
We must compare this to the formula for gravitational attraction in New-
tonian mechanics, namely,
d2r
dt2 = −GMS
r2
where MS is the mass of the attracting body (and G is the gravitational
constant).
Thus, we ﬁnd as a ﬁrst approximation that the constant of
integration M is
M ∼= GMS
c2
.

6.4. A Brief Introduction to General Relativity
291
Hence, M is a constant multiple of the mass of the attracting body. The
constant 2M has the dimensions of length, and one calls rG = 2M the
Schwarzschild radius. The formula for it is
rG = 2G
c2 · MS = (1.48 × 10−27 m/kg)MS.
The Schwarzschild radius is 2.95 km for the sun and 8.8 mm for the
Earth. Evidently, for spherically symmetric objects that one encounters
in common experience, the Schwarzschild radius is much smaller than the
object’s actual radius. In fact, if a spherically symmetric object has radius
RS and mass MS, then
rG < RS ⇐⇒2G
c2 MS < RS.
A sphere with rG > RS would need to have an enormous density. Fur-
thermore, this situation would seem to be physically impossible for the
following reason. It is understood that the Schwarzschild metric holds only
in the vacuum outside of the body (planet or star). However, if rG > RS,
then the Schwarzschild radius would correspond to a sphere outside of the
spherical body where the Schwarzschild metric has a singularity g11 = 1/0.
For this reason, some physicists initially claimed this to be a result of the
successive approximations or simply a physically impossible situation.
The history of science has occasionally shown that singularities in the
equations do not immediately imply that the scenario is impossible. The
possibility of traveling at the speed of sound was thought to be impos-
sible because of the consequences for the Doppler eﬀect equation. Now,
military jets regularly ﬂy faster than the speed of sound. Similarly, in re-
cent decades, physicists regularly study objects considered to be so dense
that 2GMS/c2 > RS. Such objects are called black holes. For a time,
the existence of black holes remained in the realm of hypothesis, but now
astronomers are convinced they have observed many such objects, and as-
trophysicists have worked out many of their dynamic properties.
For many, the concept of more than three dimensions, let alone a curved
spacetime, is literally unimaginable. One might argue that this is a con-
sequence of Euclid’s legacy and the Euclidean inﬂuence on mathematical
education. However, like quantum mechanics which has proven revolution-
ary and incredibly fruitful despite the advanced level of the mathematics
that govern it, so general relativity and the diﬀerential geometry that un-
derlies it hold a central place in modern physics.

292
6. Applications of Manifolds to Physics
Problems
6.4.1. Show that we can rephrase the explanation for Equation (6.38) by saying
that for any three vectors A, B, and C in TP M,
(⋆P )(A, B, C) = Vol
 (♯
P , A, B, C),
where Vol
  is the volume form with respect to the metric
.
6.4.2. Let
 be a symmetric tensor of type (0, 2), constructible from the full
curvature tensor
 and the metric tensor
 and linear in
.
(a) Show that
 can only have the form
Lαβ = aRαβ + bRgαβ + λgαβ,
where Rαβ are the components of the Ricci curvature tensor, R is
the scalar curvature, and a, b, and λ are real constants.
(b) Show that div
 = 0 if and only if b = −1
2a.
(c) If gαβ = ηαβ, i.e., one is in ﬂat spacetime, show that
 = 0 if and
only if λ = 0.
6.4.3. Find the “suitable” coordinate transformation h that allows one to pass
from Equation (6.46) to Equation (6.47).
6.4.4. Prove that Equation (6.48) is correct.
6.4.5. Prove Equation (6.49).
6.4.6. Light Propagation in the Schwarzschild Metric. In the Schwarzschild metric,
light travels along the null-geodesics, i.e., where ds2 = 0.
(a) Explain why setting θ = 0 does not lose any generality to ﬁnding the
null-geodesics.
(b) Prove that if one sets u = 1/r, then ds2 = 0 implies that
d2u
dϕ2 + u = 3Mu2.
(6.51)
(c) Deduce that in the vicinity of a black hole, light travels in a circle
precisely at the radius r = 3
2rG.
(d) Solve Equation (6.51) for M = 0. This corresponds to empty space
(no mass present). Call this solution u0(ϕ).
(e) (ODE) Now look for general solutions u to Equation (6.51) by setting
u = u1 + u0. Then u1(ϕ) must satisfy
d2u1
dϕ2 + u1 = 3M
R0 sin2(ϕ −ϕ0).
Solve this diﬀerential equation explicitly, and ﬁnd the complete so-
lution to Equation (6.51).

6.4. A Brief Introduction to General Relativity
293
R0
ϕ∞
r = RS/ sin ϕ
Figure 6.7. Deviation of light near a massive body.
(f) In the complete solution, show that M = 0 (empty space) corre-
sponds to traveling along a straight line u =
1
R0 sin(ϕ −ϕ0), where
R0 is the distance from the line to the origin.
(g) Show that the general solution to Equation (6.51) is asymptotically
a line.
(h) We now consider Eddington’s famous experiment to measure the de-
viation of light by the sun. Consider a geodesic G in the Schwarzschild
metric that passes right alongside the sun, i.e., passes through the
point r = RS and ϕ = 0. Deﬁne ϕ∞as the limiting angle of deviation
between the line r = RS/ sin ϕ and the geodesic G (see Figure 6.7).
The sun bends the light away from the straight line by a total of
2ϕ∞. Using (at a judicious point) the approximation that sin ϕ ∼= ϕ,
prove that the total deviation of light is
2ϕ∞= 4GMS
RSc2 ,
where MS is the mass of the sun and RS is the sun’s radius.


APPENDIX
A
Point Set Topology
A.1
Introduction
Though mathematicians, when developing a new area of mathematics, may
deﬁne and study any object as they choose, the “natural” notion of a sur-
face requires a far more intricate deﬁnition than does a curve in the theory
of diﬀerential geometry. The deﬁnition for a regular curve in R2, which im-
mediately generalizes to parametrized curves in Rn, reﬂects the fact that
the most general curve in the plane or in space can be given entirely by a
single n-tuple of continuous coordinate functions ⃗x(t) = (x1(t), . . . , xn(t))
that satisfy the regularity condition. On the other hand, even for simple
surfaces the situation becomes more complicated. Example 3.1.4 showed
that one needs at least two parametrizations ⃗X : U ⊂R2 →R3 to cover
the sphere according to the requirements of a manifold (or regular surface).
Though at ﬁrst somewhat unwieldy, the deﬁnitions for regular surfaces and
for a diﬀerential manifold are necessary for the concepts in diﬀerential ge-
ometry to appropriately generalize both the theory of calculus of functions
and the local theory of curves.
On the other hand, numerous concepts introduced in basic calculus
courses can be generalized not by formulating more constrained deﬁnitions
but by expanding the context in which one deﬁnes these concepts. As an
example of this contrast, consider a connected parametrized surface S in
R3. The formula for the distance between two points P and Q on S may
be very complicated, and yet this distance is well deﬁned as
D(P, Q) = glb{length of C | C is a curve on S connecting P and Q},
where glb stands for the greatest lower bound. That a greatest lower bound
exists for all subsets of R≥0 is a property of the real numbers. Furthermore,
one can make sense of many common notions of geometry in Euclidean
space Rn in a wider context simply if one possesses the notion of distance
at one’s disposal.
295

296
A. Point Set Topology
The ﬁrst wider context presented in this appendix is the category of
metric spaces. After introducing the axioms, some basic theorems, and a
few examples of metric spaces, we move on to consider the most general
context for geometry: topological spaces. Though topology is a very wide
ﬁeld of mathematics in its own right, this appendix presents just the basic
notions that support this book’s presentation of diﬀerential geometry. We
refer the reader to [26] for a readable and thorough introduction to point set
topology and to [41] and [2] for an introduction to topology that includes
homology, the fundamental group, algebraic topology, and the classiﬁcation
of surfaces.
A.2
Metric Spaces
A.2.1
Metric Spaces: Deﬁnition
A metric space is a set that comes with the notion of “closeness” between
two points. Since one cannot deﬁne geometric closeness arbitrarily, we im-
pose a few numerical conditions that mimic geometry in Euclidean spaces.
Deﬁnition A.2.1. Let X be any set. A metric on X is a function D : X×X →
R≥0 such that
1. equality: D(x, y) = 0 if and only if x = y;
2. symmetry: D(x, y) = D(y, x) for all x, y ∈X;
3. triangle inequality: D(x, y) + D(y, z) ≥D(x, z) for all x, y, z ∈X.
A pair (X, D) where X is a set with a metric D is called a metric space.
Example A.2.2 (Euclidean Spaces).
Euclidean spaces Rn are metric spaces
where the metric D is the usual Euclidean distance formula between two
points, namely, if P = (p1, p2, . . . , pn) and Q = (q1, q2, . . . , qn), then
D(P, Q) =
:
;
;
<
n

i=1
(qi −pi)2.
Note that many notions in usual geometry (circles, parallelism, midpoint,
etc.)
depend vitally on this particular distance formula.
Furthermore,
notice that if n = 1, this formula simpliﬁes to the usual distance formula
on the real line R, namely,
d(x, y) = |y −x|.

A.2. Metric Spaces
297
To prove that (Rn, D) is indeed a metric space, one must verify the
three axioms in Deﬁnition A.2.1. The ﬁrst holds because
D(P, Q) = 0 ⇐⇒
n

i=1
(qi −pi)2 = 0,
which is equivalent to (qi −pi)2 = 0 for all 1 ≤i ≤n, and hence qi = pi for
all 1 ≤i ≤n. The second obviously holds, and we prove the third axiom
as follows. Since D(P, Q), D(Q, R), and D(P, R) are positive we have
D(P, Q) + D(Q, R) ≥D(P, R)
⇐⇒
:
;
;
<
n

i=1
(qi −pi)2 +
:
;
;
<
n

i=1
(ri −qi)2 ≥
:
;
;
<
n

i=1
(ri −pi)2
⇐⇒
n

i=1
(qi −pi)2 +
n

i=1
(ri −qi)2
+ 2
:
;
;
<
n

i=1
(qi −pi)2
:
;
;
<
n

i=1
(ri −qi)2 ≥
n

i=1
(ri −pi)2
⇐⇒
2
:
;
;
<
n

i=1
(qi −pi)2
:
;
;
<
n

i=1
(ri −qi)2 ≥2
n

i=1
(ri −qi)(qi −pi).
Regardless of the sign on the right-hand side, one can show that this last
line is equivalent to
 n

i=1
(qi −pi)2
  n

i=1
(ri −qi)2

−
 n

i=1
(ri −qi)(qi −pi)
2
≥0
⇐⇒

1≤i<j≤n
((qi −pi)(rj −qj) −(qj −pj)(ri −qi))2 ≥0.
Since the conclusion is true and since all statements are equivalences, the
third axiom of a metric holds. Of course, the triangle inequality is used in
Deﬁnition A.2.1 precisely because it is one of the fundamental properties of
the Euclidean distance function. However, we needed to verify the triangle
inequality based on the formula given for the Euclidean metric, and the
example illustrates what is required in order to establish the three axioms.
Example A.2.3. There exists, in fact, a variety of other metrics on Euclidean
space, and we illustrate a few of these alternate metrics for R2. Let P =

298
A. Point Set Topology
(x1, y1) and Q = (x2, y2).
We leave to the reader the proofs that the
following functions are metrics on R2:
D1(P, Q) = |x2 −x1| + |y2 −y1|,
D3(P, Q) =
3
|x2 −x1|3 + |y2 −y1|3 ,
D∞(P, Q) = max {|x2 −x1|, |y2 −y1|} .
Example A.2.4 (Six Degrees of Kevin Bacon). A humorous example of a metric
space is the set of syndicated actors A equipped with the function D deﬁned
as follows. Create a simple graph using A as the set of vertices and place
an edge between two actors a1 and a2 if they acted in a movie together.
D(a1, a2) is deﬁned as the minimum number of edges it takes to create a
path connecting a1 and a2. With the additional assumption that D(a, a)=0
for all a ∈A, shows that the pair (A, D) is a metric space.
The party game called “Six Degrees of Kevin Bacon” asks players to
ﬁnd the shortest path (and hence the distance D) between any actor and
the actor Kevin Bacon.
Having a notion of distance in a set, we may want to consider the subset
of all points that are within a certain range of a ﬁxed point.
Deﬁnition A.2.5. Let (X, D) be a metric space, and let p ∈X be a point.
We deﬁne the open ball of radius r around p as the set
Br(p) = {y ∈X | D(p, y) < r}.
The reader who is new to topology should note that the terminology
“open ball” might be initially misleading since the set Br(p) only takes the
shape of an actual ball (disk, sphere, etc.) in the case of the Euclidean
metric on Rn.
Example A.2.6. Consider the metric D1 from Example A.2.3 above, and let
O = (0, 0). The ball of radius 1 around the origin O using the metric D1
is the set
B1(O) = {(x, y) ∈R2 | |x| + |y| < 1}.
Notice that the equation |x| + |y| = 1 has a locus that is symmetric about
the x-axis and about the y-axis so to determine its locus we only need to
see what happens in the ﬁrst quadrant. In the ﬁrst quadrant, the equation
|x| + |y| = 1 becomes x + y = 1, which is a line segment from (1, 0)
to (0, 1).
Thus, the open ball B1(O) is the open square with vertices
{(1, 0), (0, 1), (−1, 0), (0, −1)}.

A.2. Metric Spaces
299
a
b
f(x)
Figure A.1. Example A.2.7: a collar around f(x).
Example A.2.7. Metric spaces can encompass a much wider range than the
above examples have illustrated so far.
Let X = C0([a, b]) be the set
of continuous real functions deﬁned on the closed interval [a, b], or let
X = Fbounded([a, b]) be the set of all bounded functions on interval [a, b].
(A theorem of calculus tells us that any function f continuous over [a, b] is
bounded so C0([a, b]) ⊂Fbounded([a, b]).) Deﬁne the function D : X ×X →
R≥0 as
D(f, g) = lub{|g(x) −f(x)| : x ∈[a, b]},
where lub refers to the least upper bound of a subset of reals.
The open ball of radius r around a function f is the set of all the
functions g ∈X such that |f(x) −g(x)| < r for all x ∈[a, b], or in other
words,
f(x) −r < g(x) < f(x) + r
for all x ∈[a, b].
In this context, we call the region f(x) −r < y < f(x) + r with a ≤x ≤b
the r-collar of f(x). (See Figure A.1.)
In a metric space (X, D), having the notion of distance between two
points allows one to also deﬁne the notion of distance between two subsets
of X.
Deﬁnition A.2.8. Let (X, D) be a metric space, and let A and B be two
subsets of X. The distance between A and B is
D(A, B) = glb{D(a, b) | a ∈A, b ∈B},
where glb stands for the greatest lower bound. The distance between a
point x ∈X and a subset A ⊂X is D({x}, A).

300
A. Point Set Topology
This deﬁnition of distance between subsets does not establish a metric
on P(X), the set of subsets of X. Indeed, for any two subsets A and B in X
such that A ̸= B and A ∩B ̸= 0, the distance between them is D(A, B) =
0, and hence, even the ﬁrst axiom for metric spaces fails.
However, in
geometry, the notion of distance between sets, especially disjoint sets, is
quite useful.
Deﬁnition A.2.9. Let (X, D) be a metric space, and let A be any set. We
deﬁne the diameter of A to be
diam A = lub {D(x, y) | x, y ∈A}.
A subset A of X is called bounded if diam A < ∞.
A.2.2
Open and Closed Sets
In the study of real functions, one often must refer to subsets of R, and one
also uses the notions of open intervals and closed intervals. In this context,
one simply says that a bounded interval is open if it does not include its
endpoints and closed if it includes both of them, and a similar deﬁnition
is given for an unbounded interval. Then a subset of R is called open if
it is a disjoint union of open intervals. In contrast, in Rn or in a metric
space, given the wide range of possibilities for the shape of sets, one cannot
legitimately talk about endpoints, though one could attempt to make sense
of the concept of “including its boundary points.” Regardless, a diﬀerent
deﬁnition for openness and closedness is required. Here is what works.
Deﬁnition A.2.10. Let (X, D) be a metric space. A subset U ⊆X is called
open if for all p ∈S there exists r > 0 such that the open ball Br(p) ⊂U.
A subset F ⊆X is called closed if the complement F = X −F is open.
Intuitively, this deﬁnition states that a subset U of a metric space is
called open if around every point there is an open ball, perhaps with a
small radius, that is completely contained in U. Note that one may wish
to consider more than one metric at the same time on the same set X. In
this case, we will refer to a D-open set.
Proposition A.2.11. Let (X, D) be a metric space. Then
1. X and ∅are both open;
2. the intersection of any two open sets is open;
3. the union of any collection of open sets is open.

A.2. Metric Spaces
301
Proof: For part 1, if p ∈X, then any open ball satisﬁes Br(p) ⊂X. Also,
since ∅is empty, the criteria for openness holds trivially for ∅.
To prove part 2, let U1 and U2 be two open sets and let p ∈U1 ∩U2.
Since U1 and U2 are open, there exist r1 and r2 such that Br1(p) ⊂U1
and Br2(p) ⊂U2. Take r = min(r1, r2). Then Br(p) ⊆Br1(p) ⊂U1 and
Br(p) ⊆Br2(p) ⊂U2 so Br(p) ⊂U1 ∩U2. Thus, U1 ∩U2 is open.
Finally, consider a collection of open sets Uα where α is an index taken
from some indexing set I, which is not necessarily ﬁnite. Deﬁne
U =
.
α∈I
Uα .
For any p ∈U, there exists some α0 ∈I such that p ∈Uα0. Since Uα0
is open, there exists r such that Br(p) ⊂Uα0, and thus, Br(p) ⊂U.
Consequently, U is open.
□
Using Proposition A.2.11(2), it is easy to show that any intersection of
a ﬁnite number of open sets is again open. In contrast, part 3 states that
the union of any collection of open subsets of X is again open, regardless
of whether this collection is ﬁnite or not. This diﬀerence between unions
and intersections of open sets is not an insuﬃciency of this proposition but
rather a fundamental aspect of open sets in a metric space. In fact, as the
following simple example shows, the inﬁnite intersection of open sets need
not be open.
Example A.2.12. For each integer n ≥1, consider the open intervals In =
(0, 1 + 1
n), and deﬁne
S =
∞
=
n=1
In .
Obviously, In+1 ⊊In, and so the intervals form a decreasing, nested chain.
Since limn→∞1
n = 0, we expect S to contain (0, 1), but we must determine
whether it contains anything more. If r > 1, then if n is large enough so
that 1
n < r −1, we have r /∈In. On the other hand, for all n ∈Z≥1, 1
n > 0,
so 1 < 1 + 1
n. Hence, 1 ∈In for all n ∈Z≥1, and thus, 1 ∈S. Thus, we
conclude that S = (0, 1]. This shows that the inﬁnite intersection of open
sets need not be open.
Example A.2.13. As a more down-to-earth example, we wish to show that ac-
cording to this deﬁnition, the set S =
#
(x, y) ∈R2 | 0<x<1 and 0<y<1}
is open in R2 equipped with the Euclidean metric. Let p = (x0, y0) be a
point in S. Since p ∈S, we see that x0 > 0, 1 −x0 > 0, y0 > 0, and

302
A. Point Set Topology
p
(a) Example A.2.13
p
(b) Example A.2.14
Figure A.2. Open and not open squares.
1−y0 > 0. Since the closed distance from a point p to any line L is along a
perpendicular to L, then the closest distance between p and any of the lines
x = 0, x = 1, y = 0, and y = 1 is min{x0, 1 −x0, y0, 1 −y0}. Consequently,
if r is any positive real number such that
r ≤min{x0, 1 −x0, y0, 1 −y0},
then Br(p) ⊂S. (See Figure A.2(a).)
Example A.2.14. In contrast to the previous example, consider the set
T = {(x, y) ∈R2 | 0 ≤x < 1 and 0 < y < 1},
where again we assume R2 is equipped with the Euclidean metric. The work
in Example A.2.13 shows that for any point p = (x0, y0), with 0 < x0 < 1
and 0 < y0 < 1, there exists a positive radius r such that Br(p) ⊂S ⊂T .
Thus, consider now points p ∈T with coordinates (0, y0). For all positive r,
the open ball Br(p) contains the point (−r/2, y0), which is not in T . Hence,
no open ball centered around points (0, y0) is contained in T , and hence T ,
is not open. (See Figure A.2(b).)
It is very common in proofs and deﬁnitions that rely on topology to
refer to an open set that contains a particular point. Here is the common
terminology.
Deﬁnition A.2.15. Let p be a point in a metric space (X, D). An open neigh-
borhood (or simply neighborhood) of p is any open set of X that contains p.

A.2. Metric Spaces
303
Closed sets satisfy properties quite similar to those described in Propo-
sition A.2.11, with a slight but crucial diﬀerence.
Proposition A.2.16. Let (X, D) be a metric space. Then
1. X and ∅are both closed;
2. the union of any two closed sets is closed;
3. the intersection of any collection of closed sets is closed.
Because a set is deﬁned as closed if its complement is open and because
of DeMorgan laws for sets, this proposition is actually a simple corollary of
Proposition A.2.11.
Therefore, we leave the details of the proof to the
reader.
Note that in any metric space, the whole set X and the empty set ∅are
both open and closed. Depending on the particular metric space, these
are not necessarily the only subsets of X that are both open and closed.
Proposition A.2.17. Let (X, D) be a metric space, and let x ∈X. The sin-
gleton set {x} is a closed subset of X.
Proof: To prove that {x} is closed, we must prove that X −{x} is open.
Let y be a point in X −{x}. Since x ̸= y, by the axioms of a metric space,
D(x, y) > 0. Let r =
1
2D(x, y). The real number r is positive, and we
consider the open ball Br(y).
Since D(x, y) > r, then x /∈Br(y), and
hence, Br(y) ⊂X −{x}. Hence, we have shown that X −{x} is open
and thus that {x} is closed.
□
The notion of distance between sets provides an alternate characteri-
zation of closed sets in metric spaces. Recall that for any subset A ⊂X,
x ∈A implies that D(x, A) = 0. The following proposition shows that the
converse holds precisely for closed sets.
Proposition A.2.18. Let (X, D) be a metric space. A subset F is closed if and
only if D(x, F) = 0 implies x ∈F.
Proof: Suppose ﬁrst that F is closed. If x /∈F, then x ∈X −F, which
is open, so there exists an open ball Br(x) around x contained entirely in
X −F. Hence, the distance between any point a ∈F and x is greater than
the radius r > 0, thus, D(x, F) > 0, and in particular, D(x, F) ̸= 0. Thus,
D(x, F) = 0 implies that x ∈F.
We now prove the converse.
Suppose that F is a subset of X such
that D(x, F) = 0 implies that x ∈F. Then for all x ∈X −F, we have

304
A. Point Set Topology
D(x, F) > 0. Take the positive number r = 1
2D(x, F), and consider the
open ball Br(x). Let p be any point in F and a any point in Br(x). Form
the triangle inequality
D(p, x) ≤D(p, a) + D(a, x) ⇐⇒D(p, a) ≥D(p, x) −D(a, x).
The least possible value for D(p, a) occurs when D(p, x) is the least possible
and when D(a, x) is the greatest possible, that is when D(p, x) = D(x, F)
and D(a, x) = r.
Thus, we ﬁnd that D(p, a) ≥r > 0.
Hence, for all
a ∈Br(x), we have D(F, a) > 0 and thus Br(x) ∩F = ∅.
Therefore,
Br(x) ⊂X −F so X −F is open and F is closed.
□
Proposition A.2.18 indicates that given any subset A of a metric space
X, one can obtain a closed subset of X by adjoining all the points with 0
distance from A. This motivates the following deﬁnition.
Deﬁnition A.2.19. Let (X, D) be a metric space, and let A ⊂X be any
subset. Deﬁne the closure of A as
Cl A = {x ∈X | D(x, A) = 0}.
Proposition A.2.20. Let (X, D) be a metric space and A any subset of X.
Cl A is the smallest closed set containing A. In other words,
Cl A =
=
A⊂F, F closed
F .
Proof: (Left as an exercise for the reader. See Problem A.2.19.)
□
A.2.3
Sequences
In standard calculus courses, one is introduced to the notion of a sequence
of real numbers along with issues of convergence and limits. The deﬁnition
given in such courses for when we say a sequence converges to a certain
limit formalizes the idea of all terms in the sequence ultimately coming
arbitrarily close to the limit point. Consequently, since limits formalize a
concept about closeness and distance, the natural and most general context
for convergence and limits is in a metric space.
Deﬁnition A.2.21. Let (X, D) be a metric spaces and let {xn}n∈N be a se-
quence in X. The sequence {xn} is said to converge to the limit ℓ∈X if
for all ε ∈R>0 there exists N ∈N such that if n > N, then D(xn, ℓ) < ε
(i.e., xn ∈Bε(ℓ)). If {xn} converges to ℓ, then we write
lim
n→∞xn = ℓ.

A.2. Metric Spaces
305
Note that we can restate Deﬁnition A.2.21 to say that {xn} converges
to ℓif for all positive ε ∈R>0, only ﬁnitely many elements of the sequence
{xn} are not in the open ball Bε(ℓ).
Example A.2.22. Consider the sequence {xn}n≥1 in R3 given by xn = (3,
1/(n + 2), 2n/(n + 1)). We prove that {xn} converges to (3, 0, 2). We know
that as sequences of real numbers,
lim
n→∞
1
n + 2 = 0
and
lim
n→∞
2n
n + 1 = 2.
Pick any positive ε. Choose N1 such that n > N1 implies that 1/(n + 2) <
ε/
√
2; choose N2 such that n > N2 implies that |2n/(n + 1) −2| < ε/
√
2.
Using the Euclidean distance
D(xn, (3, 0, 2)) =
!
(3 −3)2 +

1
n + 2 −0
	2
+
 2n
n + 1 −2
	2
,
one sees that if n > N = max(N1, N2), then
D(xn, (3, 0, 2)) <

ε
2 + ε
2 = ε.
This proves that lim xn = (3, 0, 2).
Note that we could have proved directly that lim xn = (3, 0, 2) by con-
sidering the limit of D(xn, (3, 0, 2)) as a sequence of real numbers and
proving that this converges to 0.
Example A.2.23. Consider the set X of bounded, real-valued functions de-
ﬁned over the interval [0, 1] equipped with the metric deﬁned in Exam-
ple A.2.7. For n ≥1, consider the sequence of functions given by
fn(x) =

1 −nx,
for 0 ≤x ≤1
n,
0,
for 1
n ≤x ≤1.
One might suspect that the limit of this sequence fn(x) would be the
function
f(x) =

1,
if x = 0,
0,
for x > 0,
but this is not the case. Let r = 1
4, and consider the r-collar around f(x).
There is no n such that fn(x) lies within the 1
4-collar around f(x). Conse-
quently, fn(x) does not converge to f(x) in the metric space (X, D). Note,
however, that for all x ∈[0, 1], as sequences of real numbers limn→∞fn(x) =
f(x). We say that fn(x) converges pointwise.

306
A. Point Set Topology
1
1
fn(x)
n = 1
n = 2
n = 3
. . .
(a) Sequence of functions
1
1
fn(x)
(b) 1
4-collar around f(x)
Figure A.3. Example A.2.23.
Proposition A.2.24. Let (X, D) be a metric space. Any sequence {xn} can
converge to at most one limit point.
Proof: Suppose that
lim
n→∞xn = ℓ
and
lim
n→∞xn = ℓ′.
Let ε be any positive real number. There exists N1 such that n > N1
implies that D(xn, ℓ) < ε
2, and there exists N2 such that n > Ns implies
that D(xn, ℓ′) < ε
2. Thus, taking some n > max(N1, N2), we deduce from
the triangle inequality that
D(ℓ, ℓ′) ≤D(xn, ℓ) + D(xn, ℓ′) ≤ε
2 + ε
2 = ε.
Thus, since D(ℓ, ℓ′) is less than any positive real number, we deduce that
D(ℓ, ℓ′) = 0 and hence that ℓ= ℓ′.
□
In any metric space, there are plenty of sequences that do not converge
to any limit. For example, the sequence {an}n≥1 of real numbers given by
an = (−1)n + 1
n does not converge toward anything but, in the long term,
alternates between being very close to 1 and very close to −1. Referring to
the restatement of Deﬁnition A.2.21, one can loosen the deﬁnition of limit
to incorporate the behavior of such sequences as the one just mentioned.
Deﬁnition A.2.25. Let (X, D) be a metric space, and let {xn} be a sequence
in X. A point p ∈X is called an accumulation point of {xn} if for all real

A.2. Metric Spaces
307
ε > 0, an inﬁnite number of elements xn are in Bε(p). The accumulation
set of {xn} is the set of all accumulation points.
Example A.2.26. Consider again the real sequence an = (−1)n + 1
n. Let ε
be any positive real number. If n > 1
ε and n is even, then an ∈Bε(1). If
n > 1
ε and n is odd, then an ∈Bε(−1). Hence, 1 and −1 are accumulation
points. However, for any r diﬀerent than 1 or −1, suppose we choose a ε
such that ε < min(|r −1|, |r + 1|). If n is large enough, then
1
n < |min(|r −1|, |r + 1|) −ε| ,
and for such n, we have an /∈Bε(r). Thus, 1 and −1 are the only ac-
cumulation points of {an}. In the terminology of Deﬁnition, A.2.25, the
accumulation set is {−1, 1}.
A.2.4
Continuity
For the same reason as for the convergence of sequences, the notion of
continuity, ﬁrst introduced in the context of real functions over an interval,
generalizes naturally to the category of metric spaces. Here is the deﬁnition.
Deﬁnition A.2.27. Let (X, D) and (Y, D′) be two metric spaces. A function
f : X →Y is called continuous at a ∈X if for all ε ∈R>0, there exists
δ ∈R>0 such that D(x, a) < δ implies that D(f(x), f(a)) < ε.
The
function f is called continuous if it is continuous at all points a ∈X.
Example A.2.28. As a ﬁrst example of Deﬁnition A.2.27, consider the func-
tion f : R2 →R given by f(x, y) = x + y, where we assume R2 and
R are equipped with the usual Euclidean metrics. Consider some point
(a1, a2) ∈R2. Let ε > 0 be any positive real number. Choosing δ = ε
2 will
suﬃce, as we now show. First note that
D((x, y), (a1, a2)) =

(x −a1)2 + (y −a2)2 < ε
2
implies that
|x −a1| < ε
2
and
|y −a2| < ε
2.
But if this is so, then
|f(x, y) −f(a1, a2)| = |x + y −(a1 + a2)| < |x −a1| + |y −a2| < ε
2 + ε
2 = ε.
Thus, f is continuous.

308
A. Point Set Topology
Example A.2.29. Deﬁnition A.2.27 allows one to study the continuity of func-
tions in much more general contexts, as we show with this example. Let X
be a proper subset of Rn, and let ⃗p be a point in Rn −X. We view X as
a metric space by restricting the Euclidean metric to it. Let Sn−1 be the
unit sphere in Rn also with its metric coming from the Euclidean one in
Rn. Deﬁne a function f : X →Sn−1 by
f(⃗x) =
⃗x −⃗p
∥⃗x −⃗p ∥.
We will show that f is continuous.
Let ⃗a ∈X. (We use vector notation in this example and note that with
vectors, the Euclidean metric can be expressed as D(⃗x,⃗a) = ∥⃗x−⃗a∥.) Then
D(f(⃗a), f(⃗x)) =

⃗a −⃗p
∥⃗a −⃗p ∥−
⃗x −⃗p
∥⃗x −⃗p ∥

=
! ⃗a −⃗p
∥⃗a −⃗p ∥−
⃗x −⃗p
∥⃗x −⃗p ∥
	
·
 ⃗a −⃗p
∥⃗a −⃗p ∥−
⃗x −⃗p
∥⃗x −⃗p ∥
	
=
!
2 −2(⃗a −⃗p ) · (⃗x −⃗p )
∥⃗a −⃗p ∥∥⃗x −⃗p ∥
=
√
2 −2 cosα,
where α is the angle between the vectors (⃗a −⃗p ) and (⃗x −⃗p ). However,
from the trigonometry identity sin2 θ = (1 −cos 2θ)/2, we deduce that
D(f(⃗a), f(⃗x)) = 2 sin
α
2

.
However, if ⃗x and ⃗a are close enough, then (⃗a −⃗p ) and (⃗x −⃗p ) form an
acute angle, and hence, if d is the height from ⃗a to the segment between ⃗p
and ⃗x, we have
D(f(⃗x), f(⃗a)) = 2 sin
α
2

≤2 sin α = 2
d
∥⃗p −⃗a∥≤2∥⃗x −⃗a∥
∥⃗p −⃗a∥.
Therefore, choosing δ small enough so that the angle between (⃗a −⃗p ) and
(⃗x −⃗p ) is acute and δ < 1
2∥⃗p −⃗a∥ε, we conclude that
∥⃗x −⃗a∥< δ =⇒D(f(⃗x), f(⃗a)) < ε
proving that f is continuous at all points ⃗a ∈X.

A.2. Metric Spaces
309
Proposition A.2.30. Let (X, D), (Y, D′), and (Z, D′′) be metric spaces. Let
f : X →Y and g : Y →Z be functions such that f is continuous at a
point a ∈X and g is continuous at f(a) ∈Y . Then the composite function
g ◦f : X →Z is continuous at a.
Proof: We know that for all ε1 ∈R>0, there exists δ1 ∈R>0 such that
D(x, a) < δ1 implies that D(f(x), f(a)) < ε1. At the same time, we know
that for all ε2 ∈R>0, there exists δ2 ∈R>0 such that D(y, f(a)) < δ2
implies that D(g(y), g(f(a))) < ε2. Therefore, given any ε > 0, set ε2 = ε
and choose ε1 so that ε1 < δ2. Then
D(x, a) < δ1 →D(f(x), f(a)) < ε1 < δ2 →D (g(f(x)), g(f(a))) < ε,
showing that g ◦f is continuous at a.
□
As Example A.2.29 indicates, proofs with continuity can be unwieldy,
but there exists an alternate formulation for when a function between met-
ric spaces is continuous.
Proposition A.2.31. Let (X, D) and (Y, D′) be two metric spaces, and let f :
X →Y be a function. The function f is continuous if and only if for all
open subsets U ⊂Y , the set
f −1(U) = {x ∈X | f(x) ∈U}
is an open subset of X.
Proof: First suppose that f is continuous. Let U be an open subset of Y ,
and let x be some point in f −1(U). Of course f(x) ∈U. Since U is open,
there exists a real ε > 0 such that Bε(f(x)) ⊂U. Since f is continuous,
there exists a δ > 0 such that y ∈Bδ(x) implies that f(y) ∈Bε(f(x)).
Hence,
f(Bδ(x)) ⊂Bε(f(x)) ⊂U,
and thus, Bδ(x) ⊂f −1(U).
Conversely, suppose that f −1(U) is an open set in X for every open set
U in Y . Let f(x) be a point in U, and let ε be a positive real number.
Then
f −1 (Bε(f(x)))
is an open set in X. Thus, since x ∈f −1(Bε(f(x))) is open, we know that
there exists some δ such that Bδ(x) ⊂f −1(Bε(f(x))). Thus, f(Bδ(x)) ⊂
Bε(f(x)), and therefore, f is continuous.
□

310
A. Point Set Topology
The following proposition is an equivalent formulation to Proposition
A.2.31 but often more convenient for proofs.
Proposition A.2.32. Let (X, D) and (Y, D′) be two metric spaces and let f :
X →Y be a function. The function f is continuous if and only if for all
open balls Br(p) in Y , the set f −1(Br(p)) is an open subset of X.
Proof: (Left as an exercise for the reader. See Problem A.2.26.)
□
Much more could be included in an introduction to metric spaces. How-
ever, many properties of metric spaces and functions between them hold
simply because of the properties of open sets (Proposition A.2.11) and the
characterization of continuous functions in terms of open sets (Proposi-
tion A.2.31). This fact motivates the deﬁnition of topological spaces.
Problems
A.2.1. Prove that D1, D3, and D∞from Example A.2.3 are in fact metrics on R2.
A.2.2. In the following functions on R2 × R2, which axioms fail to make the
function into a metric?
(a) D1((x1, y1), (x2, y2)) = |x1| + |x2| + |y1| + |y2|.
(b) D2((x1, y1), (x2, y2)) = −

(x2 −x1)2 + (y2 −y1)2
.
(c) D3((x1, y1), (x2, y2)) = |x2 −x1| · |y2 −y1|.
(d) D4((x1, y1), (x2, y2)) = |x2
2 −x2
1| + |y2 −y1|.
A.2.3. Let (X1, D1) and (X2, D2) be metric spaces. Consider the Cartesian prod-
uct X = X1 × X2 made up of pairs (p1, p2)b with p1 ∈X1 and p2 ∈X2.
Prove that the following function is a metric on X:
D((p1, p2), (q1, q2)) = D(p1, q1) + D(p2, q2).
A.2.4. Let X = Pﬁn(Z) be the set of all ﬁnite subsets of the integers. Recall that
the symmetric diﬀerence between two sets A and B is A△B = (A −B) ∪
(B −A). Deﬁne the function D : X × X →R≥0 by
D(A, B) = n(A△B),
where n means the cardinality of, or number of elements in, a set. Prove
that D is a metric on X.
A.2.5. In Euclidean geometry, the median line between two points p1 and p2 in
R2 is deﬁned as the set of points that are of equal distance from p1 and
p2, i.e.,
M = {q ∈R2 | D(q, p1) = D(q, p2)}.
What is the shape of the median lines in R2 for D1, D2, and D∞from
Example A.2.3?

A.2. Metric Spaces
311
A.2.6. Prove that if (X, D) is any metric space, then D(x, y)n where n is any
positive integer, is also metric on X.
A.2.7. Let (X, D) be a metric space, and let S be any subset of X. Prove that
(S, D) is also a metric space. (The metric space (S, D) is referred to as
the restriction of D to S.)
A.2.8. Let S2 be the unit sphere in R3, i.e.,
S2 = {(x, y, z) ∈R3 | x2 + y2 + z2 = 1}.
Sketch the open balls on S2 obtained by the restriction of the Euclidean
metric to S2 (see Problem A.2.7). Setting the radius r < 2, for some point
p ∈S2, describe Br(p) algebraically by the equation x2 + y2 + z2 = 1 and
some linear inequality in x, y, and z.
A.2.9. Prove that Example A.2.7 is in fact a metric space.
A.2.10. Find the distance between the following pairs of sets in R2:
(a) A = {(x, y) | x2 + y2 < 1} and B = {(x, y) | (x −1)2 + y2 < 1}.
(b) A = {(x, y) | xy = 1} and B = {(x, y) | xy = 0}.
(c) A = {(x, y) | xy = 2} and B = {(x, y) | x2 + y2 < 1}.
A.2.11. Prove that a subset A of a metric space (X, D) is bounded if and only if
A ⊂Br(p) for some r ∈R≥0 and p ∈X.
A.2.12. Inﬁnite Intersections and Unions. Let An = [n, +∞) and let Bn = [ 1
n, sin n].
Find
(a)
∞
.
n=0
An,
(b)
∞
=
n=0
An,
(c)
∞
.
n=2
Bn.
A.2.13. Deﬁne the metric D on R2 as follows.
For any p = (px, py) and q =
(qx, qy), let D(p, q) =

4(px −qx)2 + (py −qy)2. Prove that this is in
fact a metric. What is the shape of a unit ball Br((x, y))? What is the
shape of the “median” between two points? [Hint: See Problem A.2.5.]
A.2.14. Are the following subsets of the plane (using the usual Euclidean metric)
open, closed, or neither:
(a) {(x, y) ∈R2 : x2 + y2 < 1}.
(b) {(x, y) ∈R2 : x2 + y2 ≥1}.
(c) {(x, y) ∈R2 : x + y = 0}.
(d) {(x, y) ∈R2 : x + y ̸= 0}.
(e) {(x, y) ∈R2 : x2 + y2 ≤1 or x = 0}.
(f) {(x, y) ∈R2 : x2 + y2 < 1 or x = 0}.
(g) the set A where A = {(x, y) ∈R2 : x = 0 and −1 ≤y ≤1}.
A.2.15. Let L be a line in the plane R2. Prove that R2−L is open in the Euclidean
metric and in the three metrics presented in Example A.2.3.

312
A. Point Set Topology
A.2.16. Let X = R2, and deﬁne D2 as the Euclidean metric and D1 as
D1((x1, y1), (x2, y2)) = |x2 −x1| + |y2 −y1|.
Prove that any D2-open ball contains a D1-open ball and is also contained
in a D1-open ball. Conclude that a subset of R2 is D2-open if and only if
it is D1-open.
A.2.17. Prove that the set { 1
n | n ∈Z>0} is not closed in R whereas the set { 1
n | n ∈
Z>0} ∪{0} is.
A.2.18. Consider a metric space (X, D), and let x and y be two distinct points of
X. Prove that there exists a neighborhood U of x and a neighborhood
V of y such that U ∩V = ∅. (In general topology, this property is called
the Hausdorﬀproperty, and this exercise shows that all metric spaces are
Hausdorﬀ.)
A.2.19. Prove Proposition A.2.20.
A.2.20. Let A be a subset of a metric space (X, D). Suppose that every sequence
{xn} in A that converges in X converges to an element of A. Prove that
A is closed.
A.2.21. Let {xn}n∈N be a sequence in a metric space (X, D). Prove that the closure
of the set of elements {xn} is {xn | n ∈N} together with the accumulation
set of the sequence {xn}.
A.2.22. Using Deﬁnition A.2.27, prove that the real function f(x) = 2x −5 is
continuous over R.
A.2.23. Using Deﬁnition A.2.27, prove that
(a) the real function f(x) = x2 is continuous over R;
(b) the real function of two variables f(x, y) = 1/(x2 + y2 + 1) is con-
tinuous over all R2 (using the usual Euclidean metric).
A.2.24. Let (X, D) and (Y, D′) be metric spaces, and let f : X →Y be a contin-
uous function. Prove that if a sequence {xn} in X converges to a limit
point ℓ, then the sequence {f(xn)} in Y converges to f(ℓ).
A.2.25. Let f : X →Y be a function from the metric space (X, D) to the metric
space (Y, D′).
Suppose that f is such that there exists a positive real
number λ, with
D′(f(x1), f(x2))
D(x1, x2)
≤λ
for all x1 ̸= x2,
i.e., that the stretching ratio for f is bounded. Show that f is continuous.
A.2.26. Prove Proposition A.2.32.
A.2.27. Let X = Rm and Y = Rn equipped with the usual Euclidean metric.
Prove that any linear transformation from X to Y is continuous.

A.3. Topological Spaces
313
A.3
Topological Spaces
A.3.1
Deﬁnitions and Examples
Deﬁnition A.3.1. A topological space is a set X along with a set τ of subsets
of X satisfying the following:
1. X and ∅are in τ.
2. For all U and V in τ, U ∩V ∈τ.
3. For any collection {Uα}α ∈I of sets in τ, the union 0
α∈I Uα is in τ.
We refer to the pair (X, τ) as a topological space and call τ a topology on
X. The elements on τ are called open subsets of X and a subset F ⊂X
is called closed if X −F is open, i.e., if X −F ∈τ. Finally, any open set
U ∈τ that contains a point x ∈X is called a neighborhood of x.
If one works with more than one topology on the same underlying set
X, one often refers to τ-open and τ-closed subsets to avoid ambiguity.
It is important to note that, as with the properties of open sets in metric
spaces, in criterion 3 of Deﬁnition A.3.1, the indexing set I need not be
countable, and hence, one should not assume that the collection {Uα}α ∈I
can be presented as a sequence of subsets.
Example A.3.2. According to Proposition A.2.11, if (X, D) is a metric space,
it is also a topological space, where we use the topology τ to be the open
sets as deﬁned by Deﬁnition A.2.10. (Of course, it was precisely Propo-
sition A.2.11 along with the discovery by mathematicians that collections
of sets with the properties described in this proposition arise naturally in
numerous other contexts that led to the given deﬁnition of a topological
space.)
Example A.3.3. Let X be any set. Setting τ = P(X) to be the set of all
subsets of X is a topology on X called the discrete topology on X.
In
the discrete topology, all subsets of X are both closed and open. On the
opposite end of the spectrum, setting τ = {X, ∅} also satisﬁes the axioms of
a topology, and this is called the trivial topology on X. These two examples
represent the largest and the smallest possible examples of topologies on a
set X.
Example A.3.4. Let X = {a, b, c} be a set with three elements. Let τ =
{∅, {a}, {a, b}, X}. A simple check shows that τ is a topology on X, namely

314
A. Point Set Topology
that τ satisﬁes all the axioms for a topology. Notice that {a} is open, {c}
is closed (since {a, b} is open) and that {b} is neither open nor closed. By
Proposition A.2.17, there is no metric D on X such that the D-open sets
of X are the open sets in the topology of τ.
Deﬁnition A.3.5. Let (X, τ) be a topological space.
A collection of open
sets B ⊂τ is called a base of the topology if every open set is a union of
elements in B ⊂τ.
The following proposition provides a characterization of a base for a
topology.
Proposition A.3.6. Let (X, τ) be a topological space, and suppose that B is a
base. Then
1. the elements of B cover X;
2. if B1, B2 ∈B, then for all x ∈B1 ∩B2 there exists B3 ∈B such that
x ∈B3 ⊂B1 ∩B2.
Conversely, if any collection B of open sets satisﬁes the above two proper-
ties, then there exists a unique topology on X for which B is a base. (This
topology is said to be generated by B.)
Proof: (Left as an exercise for the reader.)
□
This characterization allows one to easily describe topologies by pre-
senting a base of open sets.
Example A.3.7. By Deﬁnition A.2.10, in a metric space, the topology asso-
ciated to a metric has the set of open balls as a base.
Example A.3.8. Let X = R2, and consider the collection B of sets of the
form
U = {(x, y) ∈R2 | x > x0 and y > y0},
where x0 and y0 are constants.
This collection B satisﬁes both of the
criteria in Proposition A.3.6, hence there exists a unique topology τ on
R2 with B as a base. It is easy to see that τ is diﬀerent from the usual
Euclidean topology. Note that for any open set U ∈τ, if (a, b) ∈U, then
the half-inﬁnite ray
{(a + t, b + t) | t ≥0}
is a subset of U. This is not a property of the Euclidean metric topology
on R2.

A.3. Topological Spaces
315
Proposition A.3.9. Let (X, τ) be a topological space. Then the following are
true about the τ-closed sets of X:
1. X and ∅are closed.
2. The union of any two closed sets is closed.
3. The intersection of any collection of closed sets is closed.
Proof: Part 1 is obviously true since both X and ∅are open.
For part 2, let F1 and F2 be any two closed subsets of X. Then F1 and
F2 are open sets. Thus F1 ∩F2 is open. However, by the DeMorgan laws,
F1 ∩F2 = F1 ∪F2.
Thus, since F1 ∪F2 is open, F1 ∪F2 is closed.
For part 3, let {Fα}, where α is in some indexing set I, be a collection of
closed subsets. The collection {Fα} is a collection of open sets. Therefore,
0
α Fα is open. Thus,
=
α∈I
Fα =
.
α∈I
Fα
is closed.
□
A converse to this proposition turns out to be useful for deﬁning certain
classes of topologies on sets.
Proposition A.3.10. Let X be a set. Suppose that a collection C of subsets of
X satisﬁes the following properties:
1. X and ∅are in C.
2. The union of any two sets in C is again in C.
3. The intersection of any collection of sets in C is again in C.
Then the set of all complements of sets in C form a topology on X.
Proof: (Left as an exercise for the reader.)
□
Example A.3.11. Let X be any set. Consider the collection C of subsets of
X that include X, ∅, and all ﬁnite subsets of X. The collection C satisﬁes
all three criteria in Proposition A.3.10, so X, ∅, and the complements of
ﬁnite subsets of X form a topology on X. This is often called the ﬁnite
complement topology.

316
A. Point Set Topology
We now present a topology on Rn that has more open sets than the
ﬁnite complement topology but fewer than the usual Euclidean topology
(induced from the Euclidean metric).
Example A.3.12. Let X = Rn. Let C be the collection of all ﬁnite unions of
aﬃne subspaces of Rn, where by aﬃne subspace we mean any set of points
that is the solution set to a set of linear equations in x1, x2, . . . , xn (i.e.,
points, lines, planes, etc.). Taking the empty set of linear equations or an
inconsistent set of linear equations, one obtains X and ∅as elements of
C. Since the union operation of sets is associative, a ﬁnite union of ﬁnite
unions of aﬃne spaces is again just a union of aﬃne spaces.
To establish that the third criterion in Proposition A.3.10 holds for C,
we must prove that the intersection of any collection of ﬁnite unions of aﬃne
spaces is a ﬁnite union of aﬃne spaces. Note ﬁrst that if the intersection
of two aﬃne subspaces A1 and A2 is a strict subspace of both A1 and A2,
then dim A1 ∩A2 is strictly less than dim A1 and dim A2. Let {Fα}α∈I be
a collection of sets in C, and let {αi}i∈N be a sequence of indices. Given
the sequence {αi}, create a sequence of “intersection” trees according to
the following recursive deﬁnition. The tree T0 is the tree with a base node,
and an edge for each Aα0,j in
Fα0 =
.
j
Aα0,j ,
with a corresponding leaf for each Aα0,j. For each tree Ti, construct Ti+1
from Ti as follows. Writing
Fαi =
.
j
Aαi,j ,
for each leaf F of Ti and for each j such that F ∩Aαi,j ̸= F, adjoin an
edge labeled by Aαi,j to F and label the resulting new leaf by F ∩Aαi,j.
As constructed, for each k, the leaves of the tree Tk are labeled by
intersections of aﬃne spaces so that
k=
i=0
Fαi
is the union of the leaves of Tk. Since only a ﬁnite number of edges gets
added to every leaf, for all i ≥0, there can be only a ﬁnite number of
vertices at a ﬁxed distance from the base node. Furthermore, since any
nontrivial intersection A ∩B of aﬃne spaces has dimension
dim(A ∩B) ≤min(dim A, dim B) −1

A.3. Topological Spaces
317
and since the ambient space is Rn, each branch (descending path) in any
tree Ti can have at most n + 1 edges. In conclusion, for all Ti, there can
only be a ﬁnite number of leaves. Thus,
∞
=
i=0
Fαi
is a ﬁnite union of intersections of aﬃne spaces. Since this holds for all
sequences {αi} in I, this then proves that
=
α∈I
Fα
is a ﬁnite union of intersections of aﬃne spaces.
This rather lengthy proof shows that the collection C of ﬁnite unions
of aﬃne spaces in Rn satisﬁes the criteria of Proposition A.3.10. Conse-
quently, the set τ of subsets of Rn that are complements of a ﬁnite union
of aﬃne spaces forms a topology on Rn.
Example A.3.12, along with the result of Problem A.3.3, show that
given a set X, it is possible to have two topologies τ and τ ′ on X such
that τ ⊊τ ′, or in other words, that every τ-open set in X is τ′-open but
not vice versa. This leads to a useful notion, which we do not develop any
further than to give its deﬁnition
Deﬁnition A.3.13. Let X be a set and let τ and τ ′ be two topologies on X.
If τ ⊆τ ′, then we say that τ′ is ﬁner than τ and that τ is coarser than τ′.
If in addition τ ̸= τ ′, we say that τ′ is strictly ﬁner than τ and that τ is
strictly coarser than τ ′.
In Section A.2.2, Proposition A.2.20 proved that the closure of a subset
A (as deﬁned by Deﬁnition A.2.19) of a metric space (X, D) is the inter-
section of all closed subsets of X containing A. This formulation of the
closure of a set does not rely explicitly on the metric D and carries over
without changes to topological spaces. The closure of a subset A ⊂X is an
example of a topological operator. Other topological operators, which one
could encounter in the context of metric spaces, exist in any topological
space.
Deﬁnition A.3.14. Let (X, τ) be a topological space and A a subset of X.
We deﬁne
1. the closure of A, written Cl A, as the intersection of all closed sets in
X containing A;

318
A. Point Set Topology
2. the interior of A, written A◦, as the union of all open sets of X
contained in A;
3. the frontier of A, written Fr A, as the set
{x ∈X | every neighborhood U of x satisﬁes
U ∩A ̸= ∅and U ∩(X −A) ̸= ∅} .
We leave some of the basic properties of the above topological operators
to the exercises but present one common characterization of closed sets.
Proposition A.3.15. Let (X, τ) be a topological space, and let A be any subset
of X. The set A is closed if and only if
A = Cl A .
Proof: If A is closed, then A is among the closed sets F ⊂X that contain
A, and there is no smaller closed subset containing A. Hence A = Cl A.
Conversely, if A = Cl A, then since the intersection of any collection of
closed sets is closed, Cl A is closed, so A is closed.
□
Another useful characterization of closed sets relies not on a topological
operator but on properties of its limit points.
Deﬁnition A.3.16. Let A be any subset of the topological space X. A limit
point of A is any point p ∈X such that every open neighborhood of p
contains at least one point of A −{p}.
(In the vocabulary of sequences in a metric space, if the subset A is a
sequence {xn}, then in Deﬁnition A.2.25, we would call the limit points
of A the accumulation points of A. For this reason, some authors use the
alternate terminology of accumulation point for limit points of a subset A in
a topological space. The discrepancy in terminology is unfortunate, but in
topology, the majority of authors use the vocabulary of Deﬁnition A.3.16.)
Proposition A.3.17. Let A be a subset of a topological space X. The set A is
closed if and only if it contains all of its limit points.
Proof: Assume ﬁrst that A is closed. The complement X −A is open and
hence is a neighborhood of each of its points. Therefore, there is no point
in X −A that is a limit point of A. Hence, A contains all its limit points.
Assume now that A contains all of its limit points. Let p ∈X −A.
Since p is not a limit point of A, there exists an open neighborhood U of p
such that U ∩A = ∅. Thus, X −A is a neighborhood of each of its points,
and hence, it is open. Thus, A is closed.
□

A.3. Topological Spaces
319
Finally, we mention one last term related to closures.
Deﬁnition A.3.18. Let (X, τ) be a topological space. A subset A of X is
called dense in X if Cl A = X.
In other words, by Proposition A.3.17 a set A is dense in X if every
point of X is a limit point of A. We give a few common examples of dense
subsets.
Example A.3.19. Let I = [a, b] be a closed interval in R, and equip I with
the topology induced from the Euclidean metric on R. The open subsets
in this topology on I are of the form U ∩I, where U is an open subset of
R. Furthermore, the open interval (a, b) is dense in I.
Proposition A.3.20. The set Q of rational numbers is dense in R.
Proof: A precise proof of this statement must rely on a deﬁnition of real
numbers, as constructed from the rationals. One may ﬁnd this deﬁnition in
any introductory analysis book but is just beyond the scope of this book.
However, using a high school understanding of real numbers as numbers
with an inﬁnite decimal expansion that is not periodic, we can supply a
simple proof of this fact.
Let x0 ∈R be any real number, and let U be an open neighborhood
of x0. By deﬁnition, there exists a positive real ε > 0 such that (x0 −ε,
x0 + ε) ∈U. Let N = 1 −log10 ε. Consider q the fraction that represents
the decimal approximation of x0 that stops at N digits after the decimal
period. Then, q ∈Q and q ∈U. Hence, x0 is a limit point of Q.
□
A.3.2
Continuity
When working with topological spaces (X, τ) and (Y, τ′), one is not always
interested in studying the properties of just any function between f : X →Y
because a function without any special properties will not necessarily relate
the topology on X to that on Y or vice versa. In other words, with respect
to the function f, discussion about topologies on X and Y is irrelevant. In
Section A.2.4, Proposition A.2.31 provided a characterization of continuous
functions between metric spaces only in terms of the open sets in the metric
space topology. This motivates the deﬁnition of continuity for functions
between topological spaces.
Deﬁnition A.3.21. Let (X, τ) and (Y, τ′) be two topological spaces, and let
f : X →Y be a function. We call f continuous (with respect to τ and τ ′)
if for every open set U ⊂Y , the set f −1(U) is open in X.

320
A. Point Set Topology
Example A.3.22. Proposition A.2.31 shows that any function called contin-
uous between two metric spaces (X, D) and (Y, D′) is continuous with
respect to the topologies induced by the metrics on X and Y , respectively.
Proposition A.3.23. Let (X, τ) and (Y, τ′) be topological spaces, and let f :
X →Y be a function. Then f is continuous if and only if for all f(x) ∈Y
and any neighborhood V of f(x), there exists a neighborhood U of x such
that f(U) ⊂V .
Proof: First, suppose that f is continuous. Then for all f(x) ∈Y and for
all open neighborhoods V of f(x), f −1(V ) is open in X. Furthermore, x ∈
f −1(V ) so f −1(V ) is an open neighborhood of x. Also, since f(f −1(V )) =
V for any set, we see that setting U = f −1(V ) proves one direction.
Second, assume the conclusion of the proposition. Let V be any open
set in Y . If V contains no image f(x), then f −1(V ) = ∅, which is open in
X. Therefore, assume that V contains some image f(x). Let W = f −1(V ).
According to the assumption, for all x ∈W there exists open neighborhoods
Ux of x such that f(Ux) ⊂V . Since f(Ux) ⊂V , then Ux ⊂f −1(V ) = W.
But then
W ⊂
.
x∈W
Ux ⊂W ◦,
and since W ◦⊂W always, we conclude that W = W ◦, which implies (see
Problem A.3.7) that W is open. This shows that f is continuous.
□
Proposition A.3.24. Let (X, τ), (Y, τ ′), and (Z, τ ′′) be three topological spaces.
If f : X →Y and g : Y →Z are continuous functions, then g ◦f : X →Z
is also a continuous function.
Proof: (Left as an exercise for the reader.)
□
For metric spaces, a continuous function is one that preserves “near-
ness” of points.
Though topological spaces are not necessarily metric
spaces, a continuous function f : X →Y between topological spaces pre-
serves nearness in the sense that if two images f(x1) and f(x2) are in the
same open set V , then there is an open set U that contains both x1 and
x2, with f(U) ⊂V .
In set theory, one views two sets as “ultimately the same” if there exists
a bijection between them: they are identical except for how one labels the
speciﬁc elements. For topological spaces to be considered “the same,” not
only do the underlying sets need to be in bijection, but this bijection must
preserve the topology. This is the concept of a homeomorphism.

A.3. Topological Spaces
321
Deﬁnition A.3.25. Let (X, τ) and (Y, τ′) be two topological spaces, and let
f : X →Y be a function. The function f is called a homeomorphism if
1. f is a bijection;
2. f : X →Y is continuous;
3. f −1 : Y →X is continuous.
If there exists a homeomorphism between two topological spaces, we call
them homeomorphic.
Example A.3.26. Any two squares S1 and S2, as subsets of R2 with the Eu-
clidean metric, are homeomorphic. For i = 1, 2, let ti be the translation
that brings the center of Si to the origin, Ri a rotation that makes the edges
of ti(Si) parallel to the x- and y-axes, and let hi be a scaling (homothetie)
that changes Ri ◦ti(Si) into the square {(−1, −1), (−1, 1), (1, 1), (1, −1)}.
It is easy to see that translations are homeomorphisms of R2 to itself. Fur-
thermore, by Problem A.2.27, we see that Ri and hi are continuous, and
since they are invertible linear transformations, their inverses are continu-
ous as well. Thus, Ri and hi are homeomorphisms. Thus, the function
f = t−1
2
◦R−1
2
◦h−1
2
◦h1 ◦R1 ◦t1
is a homeomorphism of R2 into itself that sends S1 to S2. Thus, S1 and S2
are homeomorphic.
Example A.3.27. Any circle and any square, as subsets of R2, are homeomor-
phic. To see this, one must present a homeomorphism between a circle and
a square. By Problem A.3.26 with modiﬁcations as needed, one can see
that any two squares are homeomorphic and any two circles are homeomor-
phic. So consider a circle S and a square T that we can assume, without
loss of generality, have the same center O.
Let f : T →S be the function that associates to a point P on the
square T the only other intersection Q of the ray [O, P) with the circle S.
An open ball Br(Q) in S corresponds to the intersection of S with Br(Q)
as an open ball in R2 and, in this case, is also the intersection of S with
an appropriate open sector U centered at O (see Figure A.4).
The set
f −1(Br(Q)) is the open set U ∩T , showing that f is continuous. Since
the construction is perfectly reversible, f −1 is also continuous. Thus we’ve
proven that any circle is homeomorphic to any square.

322
A. Point Set Topology
P
Q
Figure A.4. A homeomorphism between a circle and a square (Example A.3.27).
Example A.3.28. The above two examples only begin to illustrate how dif-
ferent homeomorphic spaces may look. In this example, we prove that any
closed, simple parametrized curve γ in Rn is homeomorphic to the unit
circle S1 (in R2). Let ⃗x : [0, l] →Rn be a parametrization by arclength
for the curve γ such that ⃗x(t1) = ⃗x(t2) implies that t1 = 0 and t2 = l,
assuming t1 < t2. The function f : γ →S1 deﬁned by
f(P) =

cos
2π
l ⃗x−1(P)
	
, sin
2π
l ⃗x−1(P)
		
(A.1)
produces the appropriate homeomorphism. Note that ⃗x−1 is not well de-
ﬁned only at the point ⃗x(0) because ⃗x−1(⃗x(0)) = {0, l}. However, using
either 0 or l in Equation (A.1) is irrelevant.
Example A.3.29. In contrast to the previous example, consider the closed,
regular curve γ2 parametrized by ⃗x : [0, 2π] →R2 with
⃗x(t) = (cos t, sin 2t).
The curve γ2 traces out a ﬁgure eight of sorts and is not simple because
⃗x(π/2) = (0, 0) = ⃗x(3π/2) (and ⃗x′(π/2) ̸= ⃗x′(3π/2)). We show that it is
not homeomorphic to a circle S1 (see Figure A.5). Call P = (0, 0).
There does exist a surjective continuous map f of γ2 onto the circle S1,
namely, using the parametrization ⃗x(t),
f(⃗x(t)) = (cos 2t, sin 2t)
which amounts to folding the ﬁgure eight back onto itself so that the circle
is covered twice. However, there exists no continuous bijection g : γ2 →S1.

A.3. Topological Spaces
323
P
g
Q
Figure A.5. Figure eight not homeomorphic to a circle.
To see this, call Q = g(P), and let U be a small open neighborhood of Q. If
g is a bijection, it has exactly one preimage for every element x ∈U. Thus,
g−1(U) is the image of a nonintersecting parametrized curve. However,
every open neighborhood of P includes two segments of curves. Thus, the
circle and γ2 are not homeomorphic.
Example A.3.30. Consider the same regular, closed parametrized curve as
in the previous example. The function f : (0, 1) →γ2 deﬁned by f(t) =
⃗x(2πt −π
2 ) is a bijection. Furthermore, f is continuous since it is contin-
uous as a function (0, 1) →R2. However, the inverse function f −1 is not
continuous and in fact no continuous bijection g : (0, 1) →γ2 can have
a continuous inverse.
Let a ∈(0, 1) be the real number such that f(a) = P, the point of self-
intersection on γ2. Take an open segment U around a. The image f(U) is
a portion of γ2 through P (see the heavy lines in Figure A.6). However, this
portion f(U) is not an open subset of γ2 since every open neighborhood of
P includes f((0, ε1)) and f((1 −ε2, 1)).
We conclude this section on continuity between topological spaces by
mentioning one particular result, the details of which exceed the scope of
this appendix and for which we therefore do not supply a proof.
Theorem A.3.31. The Euclidean spaces Rn and Rm are homeomorphic if and
only if m = n.
P
f
0
1
a
Figure A.6. Figure eight not homeomorphic to a segment.

324
A. Point Set Topology
This theorem states that Euclidean spaces can only be homeomorphic
if they are of the same dimension. This might seem obvious to the casual
reader but this fact hides a number of subtleties. First of all, the notion
of dimension of a set in topology is not at all a simple one. Secondly, one
must be careful to consider space-ﬁlling curves, such as the Peano curve,
which is a continuous surjection of the closed interval [0, 1] onto the closed
unit square [0, 1] × [0, 1]. (See [27], Section 3-3, for a construction.) The
construction for space-ﬁlling curves can be generalized to ﬁnd continuous
surjections of Rn onto Rm even if n < m. However, Theorem A.3.31 implies
that no space-ﬁlling curve is bijective and has a continuous inverse. That
Euclidean spaces of diﬀerent dimensions are not homeomorphic means that
they are distinct from the perspective of topology.
A.3.3
Derived Topological Spaces
Given any topological space (X, τ), there are a number of ways to create
a new topological space. We present two common ways—subset topology
and identiﬁcation spaces—which are used throughout this book.
Deﬁnition A.3.32. Let (X, τ) be a topological space, and let S be any subset
of X. We deﬁne a topology τ′ on S by calling a subset A ⊂S open if and
only if there exists an open subset U of X such that A = S ∩U.
The subset topology is sometimes called the topology induced on S from
X. There is an alternate way to characterize it.
Proposition A.3.33. Let (X, τ) be a topological space, and let S ⊂X. Let
i : S →X be the inclusion function. The induced topology on S is the
coarsest topology such that i is a continuous function.
Proof: Given any subset A of X, we have i−1(A) = A∩S. If i is continuous,
then for all open subsets U ⊂X, the set i−1(U) = U ∩S is open. However,
according to Deﬁnition A.3.32, the subset topology on S has no other open
subsets and therefore is coarser than any other topology on S, making i
continuous.
□
Example A.3.34. Consider R equipped with the usual topology. Let S = [a, b]
be a closed interval. If a < c < b, in the subset topology on S, the interval
[a, c) is open. To see this, take any real d < a. Then
[a, c) = (d, c) ∩[a, b],
and (d, c) is open in R.

A.3. Topological Spaces
325
A second and often rather useful way to create new topological spaces
is to induce a topology on a quotient set.
Deﬁnition A.3.35. Let X be a set, and let R be an equivalence relation on
X. The set of equivalence classes of R is denoted by X/R and is called the
quotient set of X with respect to R.
The concept of a quotient set arises in many areas of mathematics
(congruence classes in number theory, quotient groups in group theory,
quotient rings in ring theory, etc.) but also serves as a convenient way to
deﬁne interesting objects in topology and geometry. We provide a few such
examples before discussing topologies on quotient sets.
Example A.3.36 (Real Projective Space). Let X be the set of all lines in Rn+1,
and let R be the equivalence relation of parallelism on X. We can therefore
discuss the set of equivalence classes X/R.
Each line in X is uniquely
parallel to a line that passes through the origin, and hence, X/R may be
equated with the set of lines passing through the origin. This set is called
the real projective space of dimension n and is usually denoted by RPn.
As an alternate characterization for RPn, consider the unit n-sphere Sn
in Rn+1 and centered at the origin. Each line through the origin intersects
Sn at two antipodal points. Therefore, RPn is the quotient set of Sn with
respect to the equivalence relation, in which two points are called equivalent
if they are antipodal (form the ends of a diameter through Sn).
Example A.3.37 (Grassmannian). Let Xr be the set of all r-dimensional aﬃne
subspaces (planes) in Rn, and let R be the equivalence relation of paral-
lelism between r-planes. The set of equivalence classes is called a Grass-
mannian and is denoted G(r, n). Again, since each r-dimensional plane
is uniquely parallel to one plane through the origin, which is then an r-
dimensional vector subspace, G(r, n) is the set of all r-dimensional vectors
subspaces of Rn.
Of particular interest to geometry is the question of how to give a
topology to X/R if X is equipped with a topology. Proposition A.3.33
illustrates how to make a reasonable deﬁnition.
Deﬁnition A.3.38. Let (X, τ) be a topological space and let R be an equiv-
alence relation on X. Deﬁne f : X →X/R as the function that sends an
element in X to its equivalence class; f is called the identiﬁcation map.
We call identiﬁcation topology on X/R the ﬁnest topology that makes f
continuous, and X/R equipped with this topology is called an identiﬁcation
space.

326
A. Point Set Topology
The above deﬁnition for the identiﬁcation topology does not make it
too clear what the open sets of X/R should be. The following proposition
provides a diﬀerent characterization.
Proposition A.3.39. Let (X, τ) be a topological space, and let R be an equiv-
alence relation on X. Let f be the identiﬁcation map. A set U is open in
the identiﬁcation topology on X/R if and only if f −1(U) is open in X.
Proof: Let τ ′ be a topology on X/R such that f : X →X/R is continuous.
Then for all open sets U ∈τ ′, we must have f −1(U) be open in X. Note
ﬁrst that f −1(∅) = ∅and f −1(X/R) = X, which are both open in X.
Now, it is not hard to show, using basic set theory, that for any function
F : A →B and any collection C of subsets of B that
F −1
 .
S∈C
S

=
.
S∈C
F −1(S)
and
F −1
 =
S∈C
S

=
=
S∈C
F −1(S).
(See Section 1.3 in [46].) Therefore, if U1 and U2 are such that f −1(U1) ∩
f −1(U2) is open, then f −1(U1 ∩U2) is open. Also, for any collection of sets
{Uα}α∈I in X/R,
f −1
 .
α∈I
Uα

=
.
α∈I
f −1(Uα)
so if the right-hand side is open, then so is the left-hand side. Consequently,
the collection of subsets B of X/R such that U ∈B if and only if f −1(U) ∈τ
is a topology on X/R. However, any ﬁner topology on X/R would include
some subset S ⊂X/R such that f −1(S) would not be open in X and,
hence, would make f not continuous.
□
Example A.3.40 (Circles). Let I be the interval [0, 1] equipped with the topol-
ogy induced from R. Consider the equivalence relation that identiﬁes 0 with
1 and sets everything else inequivalent. (See Figure A.7.) The identiﬁca-
tion space I/R is homeomorphic to a circle.
We may use the function
f : I/R →S1 deﬁned by
f(t) = (cos 2πt, sin 2πt),
which is well deﬁned since f(0) = f(1), so whether one takes 0 or 1 for the
equivalence class {0, 1}, one obtains the same output. This function f is
clearly bijective.

A.3. Topological Spaces
327
0
1
{0, 1}
0.4
0.4
Figure A.7. Open set around {0, 1}.
To prove that f is continuous, let P ∈S1, and let x ∈I/R. If P ̸= (1, 0),
then any open neighborhood U of P contains an open interval of angles
θ1 < θ < θ2, where 0 < θ1 < θ2 < 2π and the angle θ0 corresponding to P
satisﬁes θ1 < θ0 < θ2. Then
f
 θ1
2π , θ2
2π
	
contains P and is a subset of U. On the other hand, if P = (1, 0), any open
neighborhood U ′ of P in S1 contains an open arc of angles θ1 < θ < θ2,
with θ1 < 0 < θ2. Then if g : I →I/R is the identiﬁcation map,
f ◦g
>
0, θ2
2π
	
∪

1 −θ1
2π , 1
?	
contains P and is contained in U ′. Furthermore, by deﬁnition, g([0, θ2
2π) ∪
(1 −θ1
2π, 1]) is open in I/R since [0, θ2
2π) ∪(1 −θ1
2π, 1] is open in the subset
topology of [0, 1]. By Proposition A.3.23, f is continuous.
Using a similar argument, it is not hard to show that f −1 is continuous,
concluding that I/R is homeomorphic to S1.
Example A.3.41. Consider the real projective space RPn given as the quo-
tient space Sn/ ∼, where ∼is the equivalence relation on Sn and where
p1 ∼p2 if and only if they are antipodal to each other, i.e., form a diameter
of the sphere. The unit sphere Sn naturally inherits the subspace topology
from Rn+1. Using Deﬁnition A.3.38 provides the induced topology for RPn.
Example A.3.42 (M¨obius Strip). Let I = [0, 1] × [0, 1] be the unit square with
the topology induced from Rn. Deﬁne the identiﬁcation (equivalence rela-
tion) between points by

(0, y) ∼(1, 1 −y), for all y ∈[0, 1],
no other points are equivalent to any others.

328
A. Point Set Topology
p
p
Figure A.8. M¨obius strip.
The topological space obtained is called the M¨obius strip.
In R3, the
M¨obius strip can be viewed as a strip of paper twisted once and with ends
glued together. Figure A.8 shows an embedding of the M¨obius strip in
R3, as well as a diagrammatic representation of the M¨obius strip. In the
diagrammatic representation, the arrows indicate that the opposite edges
are identiﬁed but in inverse direction. The shaded area shows a disk around
a point p on the identiﬁed edge.
A.3.4
Compactness
In any calculus course, one encounters the following theorem used in the
proofs of many theorems, such as Rolle’s Theorem and the Mean Value
Theorem.
Theorem A.3.43. Let I ⊆R be a closed interval, and let f : I →R be a
continuous real-valued function. Then f attains a maximum and minimum
over I.
As topology developed, a variety of attempts were made to generalize
the idea contained in this fact.
The essential properties of the interval
[a, b] are that it is closed and bounded, two properties that make sense in
any metric space. It turns out that in topological spaces, the notion of
compactness provides a generalization for the combined property of being
closed and bounded.
Deﬁnition A.3.44. Let (X, τ) be a topological space. Let I be any set, and
let U = {Ui}i∈I be a collection of open sets in X. We call U an open cover
of X if
X =
.
i∈I
Ui.

A.3. Topological Spaces
329
If J ⊂I, the collection V = {Vj}j∈J is called an open subcover of U if V is
itself an open cover of X.
Deﬁnition A.3.45. A topological space (X, τ) is called compact if every open
cover of X has a ﬁnite subcover. If A is a subset of X, we call A compact
if it is compact when equipped with the subspace topology induced from
(X, τ).
Some examples of compact spaces are obvious: for example, any ﬁnite
subset of a topological space is compact. However, the following theorem
justiﬁes, at least in part, the given deﬁnition of compactness.
Theorem A.3.46 (Heine-Borel). A closed and bounded interval [a, b] of R is
compact.
(A variety of proofs exist for the Heine-Borel Theorem. The following
proof is called the “creeping along” proof.)
Proof: Let U = {Uα}α∈I be an open cover of [a, b]. Deﬁne the subset of
[a, b] by
E = {x ∈[a, b] | [a, x] is contained in a ﬁnite subfamily of U}.
Obviously, E is an interval, but a priori we do not know whether it is open
or closed or even nonempty. We will show that b ∈E to establish the
theorem.
Now a ∈E because there exists some Uα0 such that a ∈Uα0.
Let
c = lub E. Clearly a ≤c ≤b. Suppose that c < b. Since U covers [a, b],
there exists some index β such that c ∈Uβ, and therefore there exists ε
such that (c −ε, c + ε) ⊂Uβ. Since c is the least upper bound of E, any
x ∈(c −ε, c) is in E. Therefore, there exists a ﬁnite set {α1, α2, . . . , αn}
such that
[a, x] ⊂
n.
i=1
Uαi.
But then the ﬁnite union of open sets
(c −ε, c + ε) ∪
n.
i=1
Uαi
contains the point c + ε/2, contradicting the assumption that c < b and
c = lub E. Thus, c = b and b ∈E.
□

330
A. Point Set Topology
Compactness is an important property of topological spaces, and we
refer the reader to [2, Chapter 3], [14, Section 6.6], or [26, Chapter 7] for
complete treatments of the topic. For the purposes of this book however,
we are primarily interested in two results, namely Theorem A.3.50 and
Theorem A.3.51, and a few necessary propositions to establish them. We
will not prove Theorem A.3.50 completely but again refer the reader to the
above sources for proofs.
Proposition A.3.47. Let (X, τ) be a topological space, and let K be a compact
subset of X. Every closed subset of K is compact.
Proof: Let F ⊂K be a closed set. Then X −F is open. If U is an open
cover of F, then U ∪X −F is an open cover of K. Since K is compact,
U ∪X −F must admit a ﬁnite subcover of K. This ﬁnite subcover of K is
of the form U′ ∪X −F, where U′ is a ﬁnite subcover of U of F. Thus, F
is compact.
□
Problem A.2.18 established the fact that two distinct points p1 and p2
in a metric space possess, respectively, open neighborhoods U1 and U2 such
that U1 ∩U2 = ∅. This type of property is called a separation property of a
topological space because it gives some qualiﬁcation for how much one can
distinguish points in the topological space. There exists a variety of sepa-
ration axioms, but we only present the one that is relevant for diﬀerential
geometry.
Deﬁnition A.3.48. Let (X, τ) be a topological space. X is called Hausdorﬀ
if given any two points p1 and p2 in X, there exist in τ open neighborhoods
U1 of p1 and U2 of p2 such that U1 ∩U2 = ∅.
Proposition A.3.49. If (X, τ) is a Hausdorﬀtopological space, then every
compact subset K of X is closed.
Proof: Let K be compact. Since X is Hausdorﬀ, for every x ∈X −K and
y ∈K, there exist open sets Uxy and Vxy, with x ∈Uxy and y ∈Vxy, such
that Uxy ∩Vxy = ∅. For each x ∈X −K,
{Vxy | y ∈K}
is an open cover of K, so it must possess a ﬁnite subcover that we index
with a ﬁnite number of points y1, y2, . . . , yn. But then for each x,
Ux =
n
=
i=1
Uxyi

A.3. Topological Spaces
331
is open since it is a ﬁnite intersection of open sets. Since K ⊂0n
i=1 Vxyi,
we conclude that K ∩Ux = ∅for all x ∈X −K.
Thus, X −K is a
neighborhood of all of its points and hence it is open. Thus, K is closed.□
Theorem A.3.50. Let A be any subset of Rn (equipped with the Euclidean
topology). The set A is compact if and only if it is closed and bounded.
Proof: (We only prove (→).) Suppose that A is compact. Since by Prob-
lem A.2.18 any metric space is Hausdorﬀ, Proposition A.3.49 allows us to
conclude that A is closed. Since every open set in a metric space is the union
of open balls, any open cover of A can be viewed as an open cover of open
balls. If A is compact, it is contained in only a ﬁnite number of such open
balls {Bri(pi)}m
i=1. There exists an open ball Br(p) that contains all the
Bri(pi). The radius r will be less than (m −1) max{d(pi, pj)} + 2 max{ri}.
Then K ⊂Br(P), and hence, K is bounded.
(The proof of the converse is more diﬃcult and uses other techniques
that we do not have the time to develop here.)
□
Theorem A.3.50 establishes that we might view closed and bounded
subsets of Rn as the topological analog to [a, b] ⊂R in Theorem A.3.43.
We now complete the generalization to topological spaces.
Theorem A.3.51. Let f : X →Y be a continuous function between topological
spaces X and Y . If X is a compact space, then f(X) is compact in Y .
Proof: Let U be an open cover of f(X). Since f is continuous, each f −1(U)
is open and the collection {f −1(U) | U ∈U} is an open cover of X. Since
X is compact, there exists a ﬁnite set {U1, U2, . . . , Un} ⊂U such that
X =
n.
i=1
f −1(Ui).
Since for any functions f(f −1(A)) ⊂A always and f(0
λ Aλ) = 0
λ f(Aλ),
then
f(X) = f
 n
.
i=1
f −1(Ui)

=
n.
i=1
f(f −1(Ui)) ⊂
n.
i=1
Ui.
Thus, f(X) is compact.
□
Corollary A.3.52. Let X be a compact topological space, and let f : X →R
be a real-valued function from X. Then f attains both a maximum and a
minimum.

332
A. Point Set Topology
Proof: By Theorem A.3.51, the image f(X) is compact. By Theorem A.3.50,
f(X) is a closed and bounded subset of R. Hence, lub {f(x) | x ∈X} and
glb {f(x) | x ∈X} are both elements of f(X) and hence are the maximum
and minimum of f over X.
□
Corollary A.3.53. Let (X, τ) and (Y, τ ′) be topological spaces, and let f :
X →Y be a continuous function onto Y . If X is compact, then so is Y .
Corollary A.3.54. Let X be a compact topological space.
If a space Y is
homeomorphic to X, then Y is compact.
One does not always restrict oneself to compact topological spaces,
though they do possess many nice properties. However, we mention two
ﬁnal deﬁnitions that concern open covers or local compactness of sets and
that appear as essential components of deﬁnitions in other parts of this
book.
Deﬁnition A.3.55. A topological space (X, τ) is called second countable if
there exists a countable subset of τ (i.e., a countable collection of open
sets) that covers X.
Deﬁnition A.3.56. A topological space (X, τ) is called locally compact if
given any x ∈X and any neighborhood U of x, there is a compact set
K such that
x ∈K◦⊂K ⊂U.
As an example, note that Rn is not compact, but it is locally compact.
Problems
A.3.1. Prove Proposition A.3.10.
A.3.2. Find all the topologies on the set {a, b, c}. How many diﬀerent topologies
exist on a set of four elements?
A.3.3. Consider the topology τ constructed in Example A.3.12. Prove that every
open set in τ is also open in the topology τ ′ induced from the Euclidean
metric. Give an example of an open set in τ ′ that is not τ-open. (When
these two facts hold, one says that τ ′ is a strictly ﬁner topology than τ.)
A.3.4. Let τ be the set of all subsets of R that are unions of intervals of the form
[a, b). Prove that τ is a topology on R. Is τ the same topology as that
induced by the absolute value (Euclidean) metric?
A.3.5. Prove that in a topological space (X, τ), a set A is open if and only if it
is equal to its interior.

A.3. Topological Spaces
333
A.3.6. Prove Proposition A.3.6.
A.3.7. Let (X, τ) be a topological space, and let A and B be any subsets of X.
Prove the following:
(a) (A◦)◦= A◦.
(b) A◦⊂A.
(c) (A ∩B)◦= A◦∩B◦.
(d) A subset U ⊂X is open if and only U = U ◦.
A.3.8. Find an example that shows that (A ∪B)◦is not necessarily equal to
A◦∪B◦.
A.3.9. Let (X, τ) be a topological space, and let A and B be any subsets of X.
Prove the following:
(a) Cl(Cl A) = Cl A.
(b) A ⊂Cl A.
(c) Cl(A ∪B) = Cl A ∪Cl B.
(d) A subset F ⊂X is closed if and only Cl F = F.
A.3.10. Find an example that shows that Cl(A ∩B) is not necessarily equal to
Cl A ∩Cl B.
A.3.11. Let (X, τ) be a topological space, and let A be any subset of X. Prove
the following:
(a) Cl A = A◦∪Fr A.
(b) Cl A −Fr A = A◦.
(c) Fr A = Fr(X −A).
A.3.12. Show that every open subset of R is the union of disjoint open intervals.
A.3.13. Let (X, D) be any metric space, and let A be any subset of X. Consider
the function f : X →R≥0 deﬁned by D(x, A), where R≥0 is equipped
with the usual topology.
(Note that [0, a) is open in this topology on
R≥0.) Prove that f is continuous. (The set f −1([0, r)) is sometimes called
the open r-envelope of A.)
A.3.14. Let (X, D) be any metric space, and let A and B be closed subsets of X.
Use the previous exercise to construct a continuous function g : X →R
such that g(a) = 1 for all a ∈A and g(b) = −1 for all b ∈B.
A.3.15. Let X = Rn −{⃗0} be equipped with the induced Euclidean topology.
Let f : Rn →Sn−1 be the radial projection onto the unit (n −1)-sphere
deﬁned by
f(⃗x) =
⃗x
∥⃗x∥.
Prove that f is continuous.

334
A. Point Set Topology
A.3.16. Let S2 be the two-dimensional unit sphere in R3. Give S2 the topology of
a metric induced on S2 as a subset of R3. Suppose we locate points on
S2 using (θ, φ) in spherical coordinates. Let fα : S2 →S2 be the rotation
function such that
f(θ, φ) = (θ + α, φ).
Prove that f is continuous.
A.3.17. Let X be a topological space, and let f : X →R be a continuous function.
Prove that the set of zeroes of f, namely {x ∈X | f(x) = 0}, is closed.
A.3.18. Prove Proposition A.3.24.
A.3.19. Let (X, τ) and (Y, τ ′) be topological spaces, and let f : X →Y be a
function.
Prove that F is continuous if and only if for all closed sets
F ⊂Y , the set f −1(F) is closed in X.
A.3.20. Let R be the set of real numbers equipped with the absolute value topology.
Prove the following:
(a) Any open interval (a, b) is homeomorphic to the open interval (0, 1).
(b) Any inﬁnite open interval (a, +∞) is homeomorphic to (1, ∞).
(c) Any inﬁnite open interval (a, +∞) is homeomorphic to (−∞, a).
(d) The open interval is homeomorphic to the set of reals R. [Hint: Use
f(x) = tan x.]
(e) The interval (1, ∞) is homeomorphic to (0, 1).
Conclude that all open intervals of R are homeomorphic.
A.3.21. Prove that a circle and a line segment are not homeomorphic.
A.3.22. Finish proving that the function f in Example A.3.28 is a homeomorphism.
A.3.23. Consider Z and Q as subsets of R equipped with the absolute value metric.
Decide whether Z and Q are homeomorphic.
A.3.24. Let (X, τ) and (Y, τ ′) be topological spaces, and suppose that there exists a
continuous surjective function f : X →Y . Deﬁne the equivalence relation
on X by
x ∼y ⇐⇒f(x) = f(y).
Prove that X/ ∼is homeomorphic to (Y, τ ′).
A.3.25. What space do we obtain if we take a M¨obius strip and identify all the
points on its boundary to a point?
A.3.26. Find a quotient space of R2 homeomorphic to each of the following:
(a) A straight line.
(b) A sphere.
(c) A (ﬁlled) rectangle.
(d) A torus.

A.4. Proof of the Regular Jordan Curve Theorem
335
A.3.27. Describe each of the following spaces:
(a) A ﬁnite cylinder with each of its boundary circles identiﬁed to a
point.
(b) The sphere S2 with an equator identiﬁed to a point.
(c) R2 with points identiﬁed according to (x, y) ∼(−x, −y).
A.3.28. Find an open cover of the following sets that does not contain a ﬁnite
subcover:
(a) R;
(b) [0, 1);
(c) (0, 1).
A.3.29. Let K be a subset of a metric space (X, D). Prove that K is compact if
and only if every sequence in K has an accumulation point in K.
A.3.30. Let K be a compact subset of a metric space (X, D).
Show that the
diameter of K is equal to D(x, y) for some pair x, y ∈K. Prove that
given any x ∈X, D(x, A) is equal to D(x, y) for some y ∈K.
A.3.31. Let X be a compact topological space, and let {Kn}∞
n=1 be a sequence
of nonempty closed subsets of X, with Kn+1 ⊂Kn for all n. Prove that
@∞
n=1 Kn is nonempty.
A.3.32. Prove that the union of ﬁnitely many compact spaces is compact. Is the
intersection between two compact sets necessarily compact?
A.3.33. Prove that the set R equipped with the ﬁnite complement topology (see
Example A.3.11) is not Hausdorﬀ.
A.3.34. Prove that the topological space described in Example A.3.12 is not Haus-
dorﬀ.
A.3.35. Prove Corollary A.3.53.
A.4
Proof of the Regular Jordan Curve Theorem
As mentioned in Section 2.2 of [5], we refrained from providing the proof
of the Jordan Curve Theorem for regular curves. We provide a proof here
since it ﬁts naturally in the context of topology.
Recall that a parametrized plane curve ⃗x : I →R2 is called regular if
⃗x′(t) ̸= ⃗0 for all t in the interval I.
Theorem A.4.1 (Regular Jordan Curve Theorem). Let C be a simple, closed, reg-
ular plane curve parametrized by ⃗x : [a, b] →R2.
Then the open set
R2 −⃗x([a, b]) has exactly two connected components with common boundary
⃗x([a, b]). Only one of the connected components is bounded, and we call this
component of R2 −⃗x([a, b]) the interior.

336
A. Point Set Topology
Proof: The proof involves three steps: (1) show that the winding numbers
of two points near but on either side of the curve diﬀer by 1, (2) establish
the existence of the outside of the curve, and (3) show that the interior of
the complement of the outside of the curve (which by step 1 is not ∅) is
connected.
Step 1. Let ⃗p be a point on the curve, and suppose that we reparametrize
C by arclength so that ⃗p = ⃗x(0). We use the domain [−l/2, l/2] for ⃗x(s),
where l is the arclength of the curve. Call ⃗U(s) the unit normal vector (i.e.,
is a counterclockwise rotation of the unit tangent vector ⃗T(s) by π/2). Let
δ ∈R, and consider the parallel curve ⃗γδ(s) = ⃗x(s) + δ⃗U(s).
Using basic results from the local theory of plane curves, the velocity
of ⃗γδ(s) is
⃗γ′
δ(s) = ⃗x′(s) + δ⃗U ′(s) = ⃗T(s) + δκg(s)⃗T (s) = (1 + δκg(s))⃗T(s),
where κg(s) is the plane curvature of ⃗x(s). Since [−l/2, l/2] is a compact
interval and since κg(s) is continuous, |κg(s)| attains a maximum over
[−l/2, l/2], say κmax. Thus, for |δ| < 1/κmax, the parallel curve ⃗γδ(s) is
regular. Call δ1 one such δ, and deﬁne
⃗p1 = ⃗p + δ1⃗U(0)
and
⃗p2 = ⃗p −δ1 ⃗U(0).
Let d(s) be the distance between ⃗x(s) and the tangent line to ⃗x at ⃗p.
Since ⃗x is diﬀerentiable at s = 0, the Taylor Series Remainder Theorem
for vector functions says that there exists a constant A such that ∥⃗x(s) −
⃗x(0)∥< As for all s ∈[−l/2, l/2]. Since ⃗x(0) = ⃗p is on the tangent line to
⃗x at ⃗p, we have
d(s) ≤∥⃗x(s) −⃗x(0)∥< As.
Consider now the following diagram.
L
r p2
r p1
δ
δ








q′
qq
t
d
........θ
The law of cosines gives, after some computation,
cos θ =
t2 + δ2
1 −δ1d

t2 + (δ1 −d)2 
t2 + δ2
1
.

A.4. Proof of the Regular Jordan Curve Theorem
337
Now assume that d = d(t) is a continuous function of t satisfying d(t) < At
(for t in some neighborhood of 0). Using the double inequality 0 ≤d(t) ≤
At, one can easily show that
t2 + δ2
1 −Aδ1t
t2 + δ2
1
≤
t2 + δ2
1 −δ1d(t)

t2 + (δ1 −d(t))2 
t2 + δ2
1
≤

t2 + δ2
1

t2 + (δ1 −At)2 .
Consequently, by the Sandwich Theorem,
lim
t→0 cos(θ(t)) = 1,
and hence, limt→0 θ(t) = 0. Furthermore, if θ2(t) is the angle ∠q′pq, then
θ2(t) ≤θ(t).
We apply this construction to the curve ⃗x near p by using ⃗x(s) for the
point q, the tangent line for L, t = f(s) for the distance between ⃗p and
the projection of ⃗x(s) onto the tangent line and d(s) as above. Let ⃗ξ(s)
be the projection of ⃗x(s) onto the tangent line to ⃗x at ⃗p, and, for i = 1, 2,
let θi(s) be the angle between ⃗x(s) −⃗pi and ⃗ξ(s) −⃗pi. We have shown that
the θi(s) are continuous functions, with
lim
s→0 θi(s) = 0,
Since the limit of θ1 and θ2 is 0 as s →0, we can choose s1 small enough
so that the angles θ1(s) and θ2(s) are all less than or equal to ε/10 for all
s ∈[−s1, s1]. Deﬁne the position maps ϕ1, ϕ2 : [−l/2, l/2] →S1 by
ϕi(s) =
⃗x(s) −⃗pi
∥⃗x(s) −⃗pi∥,
and let ˜ϕi : [−l/2, l/2] →R be their liftings relative to 0.
We assume
without loss of generality that the curve is oriented as shown in Fig-
ure A.9.
We note that by construction, for all s ∈[−l/2, −s1] ∪[s1, l/2], the
distances ∥⃗x(s)−⃗p1∥and ∥⃗x(s)−⃗p2∥remain bounded below by δ, regardless
of the parameter s. Consequently, for all s ∈[−l/2, −s1]∪[s1, l/2], the angle
between ⃗x(s) −⃗p1 and ⃗x(s) −⃗p2 tends uniformly to 0 as δ1 approaches 0,
i.e., as ⃗p1 approaches ⃗p2. We choose δ1 small enough so that the angle
between ⃗x(s) −⃗p1 and ⃗x(s) −⃗p2 is less than ε/10.
We also notice that since ⃗p1 and ⃗p2 are on the normal line to C at p,
we can choose δ1 smaller still (if necessary) so that
π −∠q−p1q+ < ε
10
and
π −∠q−p2q+ < ε
10.

338
A. Point Set Topology
q−
q+
θ1(s1)
⃗x(s1)
p1
p2
⃗x(s)
Figure A.9. Setup for the Jordan Curve Theorem.
Furthermore, by construction and keeping track of orientation,
|( ˜ϕ1(−s1) −˜ϕ1(s1)) −∠q−p1q+| < ε
10 + ε
10 = ε
5,
and
|( ˜ϕ2(−s1) −˜ϕ2(s1)) + ∠q−p2q+| < ε
10 + ε
10 = ε
5.
Thus,
|( ˜ϕ1(−s1) −˜ϕ1(s1)) −π| < 3ε
10
and
|( ˜ϕ2(−s1) −˜ϕ2(s1)) + π| < 3ε
10.
Let w(p1) and w(p2) be the winding numbers of C around p1 and p2
(see Deﬁnition 2.2.5 in [5]). From the deﬁnition of the winding numbers
and Equation (2.3) in [5], we see that
2π(w(p1) −w(p2)) = ( ˜ϕ1(l/2) −˜ϕ1(−l/2)) −( ˜ϕ2(l/2) −˜ϕ2(−l/2))
= ( ˜ϕ1(l/2) −˜ϕ1(s1)) + ( ˜ϕ1(s1) −˜ϕ1(−s1))
+ ( ˜ϕ1(−s1) −˜ϕ1(−l/2)) + ( ˜ϕ2(l/2) −˜ϕ2(s1))
+ ( ˜ϕ2(s1) −˜ϕ2(−s1)) + ( ˜ϕ2(−s1) −˜ϕ2(−l/2))
= [( ˜ϕ1(l/2) −˜ϕ2(l/2)) −( ˜ϕ1(s1) −˜ϕ2(s1))]
+ [( ˜ϕ1(s1) −˜ϕ1(−s1)) −( ˜ϕ2(s1) −˜ϕ2(−s1))]
+ [( ˜ϕ1(−s1) −˜ϕ2(−s1)) −( ˜ϕ1(−l/2) −˜ϕ2(−l/2))].
By our constructions, the middle term diﬀers from 2π by at most 6ε/10,
and also, by our choice of δ1, for all s ∈[−l/2, −s1] ∪[s1, l/2], we have

A.5. Simplicial Complexes and Triangulations
339
| ˜ϕ1(s) −˜ϕ2(s)| < ε/10. Putting all the inequalities together, we deduce
that our choice of δ1 implies that
|2π(w(p1) −w(p2)) −2π| < ε.
Consequently, since we know that w(p1) −w(p2) is an integer, choosing δ1
small enough so that ε = π, we can conclude that w(p1) −w(p2) = 1.
We remark here that a diﬀerent choice of orientation for C would lead
to w(p1)−w(p2) = −1, so we conclude that in general w(p1)−w(p2) = ±1.
This ﬁnishes step 1.
Step 2. It is easy to see that since C is a regular closed curve, it is bounded
and therefore contained in some open disk DR centered at the origin and
of radius R. Let ⃗p be a ﬁxed point in R2 −DR. Let U1 be the connected
component of R2 −C containing ⃗p. Let ⃗q be the closest point on DR to ⃗p.
Then
(⃗x(t) −⃗p ) · (⃗q −⃗p ) > 0,
and hence, the angle of ⃗x(t) −⃗p with respect to the ﬁxed vector ⃗q −⃗p is
strictly between (−π
2 , π
2 ).
Consequently, the winding number of ⃗x with
respect to ⃗p is 0. We call the open set U1 the outside of C.
Step 3. To conclude the proof, we need to show that the complement of
U1 ∪C is connected.
Let ⃗p1 and ⃗p2 be two points in U1 ∪C, and let ⃗x(s1) and ⃗x(s2) be
points on the curve that minimize ∥⃗x(s) −⃗pi∥for i = 1, 2. Without loss of
generality, we can assume that s1 < s2. Let δ be small enough that ⃗γδ(s)
is a parallel curve to ⃗x(s) that is simple and does not intersect ⃗x(s), with
the sign of delta chosen so that ⃗x(0) ∈U1 ∪C. Consider the path that
consists of (1) a line segment from ⃗p1 to ⃗γδ(s1), (2) adjoined by ⃗γδ(s) for
s1 ≤s ≤s2, and (3) adjoined by a line segment from ⃗γδ(s2) to ⃗p2. This
path connects ⃗p1 and ⃗p2, and all its points are in U1 ∪C. Thus, U1 ∪C is
connected.
□
A.5
Simplicial Complexes and Triangulations
In Section A.3, we introduced basic notions concerning topological spaces
and continuous functions between them. When one applies the theory of
point set topology to speciﬁc applications, one quickly encounters a few
diﬃculties.
For example, when proving a geometric result, such as the
Jordan Curve Theorem, one needs to work in a narrower context than the

340
A. Point Set Topology
full generality of a topological space. On the other hand, when one deﬁnes
invariants of topological spaces, it is important to be able to calculate
these invariants for a large enough class of spaces for such invariants to be
of interest. Triangulating a space is a technique that can both be helpful
for proving certain geometric theorems and also for calculating invariants
of topological spaces.
In this section, we discuss triangulations of topological spaces and ex-
pand on some of the results used in Section 8.4 of [5] to prove the global
Gauss-Bonnet Theorem. There is not enough space in this book for a com-
prehensive treatment of triangulations so the propositions here are pre-
sented without proofs. We encourage the reader to consult [2], [41], or [27]
for a more complete introduction to these topics.
Deﬁnition A.5.1. The points p0, p1, . . . , pk in Rn are said to be in general
position if any strict subset of {p0, p1, . . . , pk} determines a smaller hyper-
plane. Vectors ⃗v0,⃗v1, . . . ,⃗vk in Rn are in general position if {⃗v1 −⃗v0, . . . ,
⃗vk −⃗v0} is a linearly independent set.
One can see that the notions of general position for vectors and for
points are equivalent by viewing vectors in Rn as position vectors of points.
Deﬁnition A.5.2. Given k + 1 vectors (or points) ⃗v0,⃗v1, . . . ,⃗vk in general
position in Rn, we call the set
{⃗x ∈Rn | ⃗x=c0⃗v0+c1⃗v1+· · ·+ck⃗vk such that c0+c1+· · ·+ck =1 and ci ≤0}
a simplex of dimension k or k-simplex. We denote this set by [⃗v0,⃗v1, . . . ,⃗vk].
The points ⃗v0,⃗v1, . . . ,⃗vk are called the vertices of the simplex.
In more geometric terms, the k-simplex of a set of points in general
position is the convex hull of (i.e., the smallest convex set containing) its
vertices. From an intuitive perspective, a k-simplex is the k-dimensional
generalization of a triangle, as the following list shows. For 0 ≤k ≤3, the
k-simplexes are
k
shape
0
point
1
closed line segment
2
triangle
3
tetrahedron
By analogy with the language of polyhedra, simplexes have faces. If
A is any nonempty subset of {⃗v0,⃗v1, . . . ,⃗vk}, then [A] is called a face of
[⃗v0,⃗v1, . . . ,⃗vk].

A.5. Simplicial Complexes and Triangulations
341
(a) A simplicial complex.
(b) Not a simplicial complex.
Figure A.10. Simplexes in R3.
Deﬁnition A.5.3. A simplicial complex is a ﬁnite collection K of simplexes
in Rn such that
1. if a simplex is in the collection K, then so are all of its faces;
2. the intersection of any pair of simplexes in K is a face.
Figure A.10(a) provides an example of a simplicial complex. Note that
if a simplicial complex K includes the triangle simplex [p1, p2, p3], then the
simplicial complex must also include all the faces, namely the simplexes
[p1, p2], [p2, p3], [p1, p3], [p1], [p2], and [p3]. In contrast, Figure A.10(b)
shows two simplexes in R3 whose intersection is not a face, and therefore,
this collection of simplexes is not a simplicial complex nor can they be a
part of a simplicial complex.
It is important to make a distinction between the simplicial complex
K, which consists of combinatorial data, and what is called its geometric
realization, denoted |K|.
The realization of a simplicial complex is the
topological space associated to K whose points consist of the union of all
the simplexes of K and whose topology is the subset topology induced
from Rn.
Deﬁnition A.5.4. Let X be a topological space. A triangulation of X consists
of a simplicial complex and a homeomorphism h : |K| →X. If for a space
X there exists a triangulation, then X is called triangulable.
Example A.5.5 (M¨obius Strip). Figure A.11 shows that the M¨obius strip is tri-
angulable. Figure A.11(a) shows the realization of a simplicial complex in
R3, while Figure A.11(b) shows the M¨obius strip along with the images of
all the faces of the complex in (a) via the triangulation homeomorphism.

342
A. Point Set Topology
(a) Simplicial complex
(b) Image of the triangulation
Figure A.11. Triangulation of a M¨obius strip.
It is important to note that triangulations of topological spaces are in
no way unique. Not every topological space admits a triangulation, but
most classes of spaces of interest in diﬀerential geometry do.
We now present a few propositions relating triangulations to the theory
of surfaces. The following ﬁrst result, originally due to Rado (1925), is
what makes the techniques of triangulations useful.
Theorem A.5.6. Every compact regular surface admits a triangulation.
The proof of this useful theorem is not particularly diﬃcult but is te-
dious, so we refer the reader to [17]. The next theorem discusses compati-
bility of triangulations of compact regular surfaces with coordinate neigh-
borhoods.
Proposition A.5.7. Let S be a compact regular surface, and let {Ui}, with
1 ≤i ≤n, be a ﬁnite collection of open sets in S that covers S. Then there
exists a simplicial complex K and a triangulation h : |K| →S such that
every 2-simplex (triangle) T of K satisﬁes h(T ) ⊂Ui for some i.
Since one lists the vertices of a simplex [⃗v0,⃗v1, . . . ,⃗vk] in a particular
order, one can associate an orientation to a simplex as a permutation of the
vertices considered equivalent up to the sign of the permutation. Hence,
there are only two possible orientations to any given simplex, depending on
the sign of the associated permutation. When sketching oriented triangles,
one typically draws an oriented arc of a circle in the interior of the triangle
to show the orientation (see Figure A.12).
Suppose two oriented k-simplexes meet along a (k −1)-dimensional
face F. Call σ and τ the permutations of the two simplexes that deﬁne
their respective orientations, and deﬁne σF and τF as the permutations
induced from σ and τ, respectively, by ignoring the vertex not in F. We

A.6. Euler Characteristic
343
v0
v1
v2
Figure A.12. Oriented triangle.
(a) Compatible orientation
(b) Incompatible orientation
Figure A.13. Adjacent oriented triangles.
say that these adjacent triangles have compatible orientation if σF and τF
have opposite signs as permutations.
When considering adjacent triangles, we can think of the orientation of
a triangle as a direction of travel around the edges. Two adjacent triangles
have a compatible orientation if the orientation of the ﬁrst leads one to
travel along the common edge in the opposite direction as the orientation
of the second triangle (see Figure A.13).
Proposition A.5.8. If S is a compact, orientable, regular surface that admits
a triangulation h : |K| →S, then all the triangles of K can be given an
orientation so that the orientations are compatible.
A.6
Euler Characteristic
Consider a simplicial complex K.
We denote by αi the number of i-
simplexes in K and let n be the maximum dimension of simplexes in K.
We deﬁne the Euler characteristic χ(K) of the simplicial complex by the
integer
χ(K) =
n

i=0
(−1)iαi.
The Euler characteristic has numerous nice properties that we attempt to
illustrate with a few simple examples.
If k > 0, call the interior of the k-simplex [⃗v0,⃗v1, . . . ,⃗vk] the set
{⃗x ∈Rn | ⃗x=c0⃗v0+c1⃗v1+· · ·+ck⃗vk such that c0+c1+· · ·+ck =1 and ci >0}.
In other words, the interior of a k-simplex is the geometric realization of
the simplex minus the geometric realization of all its faces.
Let K be a simplicial complex, and suppose that K′ is obtained from K
by adding a point q in the interior of a k-simplex A of K and then adding

344
A. Point Set Topology
all appropriate simplexes to complete K ∪{q} to a simplicial complex.
It is not hard to see that, in this case, one must add
k+1
i

simplexes of
dimension i for 0 < i < k and that the k-face A must be replaced with k
faces of dimension k. Then the calculation of χ(K′) gives
χ(K′) =
k−1

i=0
(−1)i

αi +
k + 1
i
		
+ (−1)k(αk + k) +
n

i=k+1
(−1)iαi
=
n

i=0
(−1)iαi +
k−1

i=0
(−1)i
k + 1
i
	
+ (−1)k(k + 1) + (−1)k+1
=
n

i=0
(−1)iαi +
k+1

i=0
(−1)i
k + 1
i
	
=
n

i=0
(−1)iαi = χ(K).
Without much more work, one arrives at the following proposition.
Proposition A.6.1. Let K and K′ be two simplicial complexes such that
|K| = |K′|.
Then K and K′ have the same Euler characteristic, i.e.,
χ(K) = χ(K′).
The following theorem provides a deep generalization of this proposi-
tion.
Theorem A.6.2. Let X be a triangulable topological space, and let K and K′
be simplicial complexes such that h : |K| →X and h′ : |K′| →X are
triangulations of X. Then χ(K) = χ(K′).
Proof: Follows from Theorem 8-11 in [27].
□
The value of this theorem is that one can now deﬁne the Euler charac-
teristic for any triangulable space.
Deﬁnition A.6.3. Let X be a triangulable space. The Euler characteristic of
X, denoted χ(X), is deﬁned as χ(K), where h : |K| →X is a triangulation
of X.
The deﬁnition of a triangulation leads immediately to the following
proposition.
Proposition A.6.4. Let X and Y be two homeomorphic topological spaces. If
one is triangulable, then so is the other, and χ(X) = χ(Y ).

A.6. Euler Characteristic
345
Figure A.14. Torus triangulation.
One can rephrase Proposition A.6.4 by saying that the Euler charac-
teristic is a topological invariant. A topological invariant is any object or
number associated to a topological space that remains unchanged under a
homeomorphism.
It is not hard to see that a two-dimensional sphere S2 is homeomorphic
to the surface of a tetrahedron K, by which we mean the tetrahedron with
its interior removed.
This polyhedron is the geometric realization of a
simplicial complex that has α0 = 4, α1 = 6, and α2 = 4. Consequently, the
Euler characteristic of a sphere is χ(S2) = 2. Figure A.14 shows a simplicial
complex that gives a triangulation of a torus. This simplicial complex has
α0 = 16, α1 = 48, and α2 = 32. Therefore, the Euler characteristic of a
torus Figure A.15(a) is χ = 0. Similarly, it is not hard to calculate that a
two-holed torus (Figure A.15(c)) has an Euler characteristic of χ = −2.
In topology, a torus is often called a sphere with one handle (see Fig-
ure A.15(b)), and by extension, one discusses spheres with n-handles. It
is easy to determine that any surface homeomorphic to a sphere with n
handles has an Euler characteristic of χ = −2(n −1). The following clas-
siﬁcation theorem is of profound importance in the study of surfaces.
(a) Torus
(b) Sphere with one handle
(c) Two-holed torus
Figure A.15. Tori.

346
A. Point Set Topology
Theorem A.6.5 (Classiﬁcation of Orientable Surfaces). Any compact orientable
surface is homeomorphic to a sphere with a ﬁnite number of handles added.
If S is homeomorphic to a sphere with n handles added, then it has an Euler
characteristic of χ(S) = −2(n −1).
Proof: (See [2, Chapter 7].)
□
The number g of handles on a closed orientable surface is called the
genus. The relation to the Euler characteristic is
g = 2 −χ(S)
2
.
We conclude with an example that is useful in the topological or the
diﬀerential geometric theory of surfaces. The proofs are simple and left to
the reader.
Lemma A.6.6. Let S be a subset of Rn that is homeomorphic to a closed disk
(in R2). Then χ(S) = 1.
Corollary A.6.7. Let S be a compact orientable surface that is homeomorphic
to a sphere with n handles. Suppose that S′ is obtained from S by removing
a region that is homeomorphic to an open disk (in R2).
Then χ(S′) =
−2(n −1) −1.

APPENDIX
B
Calculus of Variations
B.1
Formulation of Several Problems
One of the greatest uses of calculus is the principle that extrema of a con-
tinuous function occur at critical points, i.e., at real values of the function,
where the ﬁrst derivative (partial derivatives if one is dealing with a mul-
tivariable function) is (are all) 0 or not deﬁned. In practical applications,
when one wishes to optimize a certain quantity, one writes down a func-
tion describing said quantity in terms of relevant independent variables,
calculates the ﬁrst partials, and solves the equations obtained by setting
the derivatives equal to 0 or undeﬁned.
Many other problems in math and physics, however, involve quantities
that do not just depend on independent variables but on an independent
function.
Some classic examples are problems that ask one to ﬁnd the
shortest distance between two points, the shape with ﬁxed perimeter en-
closing the most area, and the curve of quickest descent between two points.
Calculus of variations refers to a general method to deal with such prob-
lems.
Let [x1, x2] be a ﬁxed interval of real numbers. For any diﬀerentiable
function y : [x1, x2] →R, the deﬁnite integral
I(y) =
% x2
x1
f(x, y, y′) dx
(B.1)
is a well-deﬁned quantity that depends only on y(x) when the integrand
f is a function of the arguments x, y, and y′.
We can view the above
integral I as a function from C1([x1, x2], R), the set of all continuously
diﬀerentiable functions from [x1, x2], to R.
The problem is to ﬁnd all
functions y(x) for which I(y) attains a minimum or maximum value for all
y ∈C1([x1, x2], R). Unlike optimization problems in usual multivariable
calculus that involve solving algebraic equations, this initial problem in
the calculus of variations involves a second-order diﬀerential equation for
347

348
B. Calculus of Variations
which the constants of integration are ﬁxed once one sets y(x1) = y1 and
y(x2) = y2.
Many generalizations to this ﬁrst problem exist. For example, similar to
optimization problems in multiple variables, one may impose certain con-
ditions so that one considers only a subset of functions in C1([x1, x2], R)
among those to optimize I(y). In another direction, one may seek to opti-
mize the double integral
I(w) =
%%
D
f

x, y, w, ∂w
∂x , ∂w
∂y
	
dA
where D is a region of R2 and w is a two-variable function. The solution
would be a function w ∈C1(D, R) that produces the maximum or minimum
value for the integral. Of course, one can consider situations where the
unknown function w is a function of any number of variables. As a third
type of generalization, one may consider the integral
I(x, y) =
% t2
t1
f

t, x, dx
dt , y, dy
dt
	
dt,
where I(x, y) involves two unknown functions of one independent variable t.
Finally, one may then consider any number of combinations to the above
generalizations. For example, the isoperimetric problem—the problem of
ﬁnding the shape with a ﬁxed perimeter and maximum area—involves ﬁnd-
ing parametric equations x(t) and y(t) that produce a simple closed curve
that maximizes area (a one-variable integration by Green’s Theorem), sub-
ject to the condition that the perimeter is some ﬁxed constant.
The following sections follow the excellent presentation given in [55].
B.2
The Euler-Lagrange Equation
B.2.1
The Main Theorem
Many problems in calculus of variations amount to solving a particular dif-
ferential equation called the Euler-Lagrange equation and variants thereof.
However, all the theorems that justify the use of the Euler-Lagrange equa-
tion hinge on one lemma and its subsequent generalizations.
Lemma B.2.1. Let G be a continuous real-valued function on an interval
[x1, x2]. If
% x2
x1
η(x)G(x) dx = 0
(B.2)

B.2. The Euler-Lagrange Equation
349
for all continuously diﬀerentiable functions η(x) that satisfy η(x1) = η(x2)
= 0, then G(x) = 0 for all x ∈[x1, x2].
Proof: We prove the contrapositive, namely, if G is not identically 0 then
there exists some function η(x) on [x1, x2] that does not satisfy Equa-
tion (B.2).
If we assume that G is not identically 0, then there exists
c ∈[x1, x2] such that G(c) ̸= 0. By continuity, there exist a, b such that
xa ≤a < c < b ≤x2 and G(x) ̸= 0 for all x ∈[a, b]. Now consider the
function
η(x) =
⎧
⎪
⎨
⎪
⎩
0,
for x1 ≤x ≤a,
G(c)(x −a)2(x −b)2,
for a ≤x ≤b,
0,
for b ≤x ≤x2.
The function η(x) is continuously diﬀerentiable, and we have
% x2
x1
η(x)G(x) dx =
% b
a
G(c)G(x)(x −a)2(x −b)2 dx.
The integrand on the right is nonnegative since G(x) has the same sign as
G(c) and, by construction, equal to 0 only at x = a and x = b. Conse-
quently, the integral on the right is positive. This proves the lemma.
□
Let us consider the ﬁrst problem in the calculus of variations, in which
we wish to optimize the integral in Equation (B.1), with the only condition
that y(x1) = y1 and y(x2) = y2. The general tactic proceeds as follows.
Assume y(x) is a function that optimizes I(y). Let η(x) be an arbitrary
continuously diﬀerentiable function on [x1, x2], with η(x1) = η(x2) = 0.
Deﬁne the one-parameter family of functions Yε by
Yε(x) = y(x) + εη(x).
Obviously, for all ε, we have Yε(x1) = y(x1) = y1 and Yε(x2) = y(x2) = y2.
For shorthand, we deﬁne
I(ε) = I(Yε) =
% x2
x1
f(x, Yε, Y ′
ε) dx.
With this notation, we see that I(0) = I(y), and since y(x) is an optimizing
function, then
I′(0) = 0
(B.3)
no matter the choice of arbitrary function η(x).

350
B. Calculus of Variations
To calculate the derivative in Equation (B.3), we obtain
I′(ε) =
% x2
x1
 ∂f
∂Y
∂Y
∂ε + ∂f
∂Y ′
∂Y ′
∂ε
	
dx =
% x2
x1
 ∂f
∂Y η + ∂f
∂Y ′ η′
	
dx,
where ∂f
∂Y means explicitly ∂f
∂y (x, Yε(x), Y ′
ε(x)) and similarly for
∂f
∂Y ′ . Set-
ting Equation (B.3) then becomes
I′(0) =
% x2
x1
∂f
∂y η + ∂f
∂y′ η′
	
dx = 0.
Integrating the second term in this integral by parts, we obtain
I′(0) =
> ∂f
∂y′ η(x)
?x2
x1
+
% x2
x1
∂f
∂y η −d
dx
 ∂f
∂y′
	
η dx
=
% x2
x1
∂f
∂y −d
dx
 ∂f
∂y′
		
η dx = 0.
Applying Lemma B.2.1 to the above equation proves the following theorem.
Theorem B.2.2. Let y : [x1, x2] →R be a function that optimizes
I(y) =
% x2
x1
f(x, y, y′) dx.
Then y satisﬁes the diﬀerential equation
∂f
∂y −d
dx
 ∂f
∂y′
	
= 0,
(B.4)
which is called the Euler-Lagrange equation.
Just as a solution x0 to f ′(x) = 0 is not necessarily a maximum or
minimum, a function that does satisfy this equation is not necessarily an
optimizing function. Consequently, we call a solution to Equation (B.4)
an extremizing function. Understanding that ∂f
∂y′ means fy′(x, y(x), y′(x)),
one notices that the Euler-Lagrange equation is a second-order diﬀerential
equation of y in terms of x.
Since Equation (B.4), and in particular the left-hand side of this equa-
tion, occurs frequently, we deﬁne it as the Lagrangian operator L on a
function f(x, y, y′), where y is a function of x, by
L(f) = ∂f
∂y −d
dx
 ∂f
∂y′
	
.
Note that L is a linear operator in f. On the other hand, whether the
diﬀerential equation L(f) = 0 is a linear operator in y(x) depends on f.

B.2. The Euler-Lagrange Equation
351
B.2.2
The Brachistochrone Problem
At the turn of the eighteenth century, Johann Bernoulli posed the problem
of ﬁnding the path in space that a particle will take when traveling un-
der the action of gravity between two ﬁxed points but taking the shortest
amount of time. To be precise, the problem assumes no friction, a simple
constant force of gravity mg (where m is the mass of the particle and g
the gravity constant), and an initial velocity v1 that is not necessarily 0.
This problem became known as the “brachistochrone” problem, the roots
of which come from the Greek words brachistos (shortest) and chronos
(time).
We suppose the two ﬁxed points A and B lie in a vertical plane that we
can label as the xy-plane, with the y-axis directed vertically upward and
the x-axis oriented so that passing from A to B means an increase in x.
Let A = (x1, y1) and B = (x2, y2) so that any curve y(x) connecting A and
B satisﬁes y(x1) = y1 and y(x2) = y2. Note that though the shape of a
curve from A to B is a function y(x), a particle moving along this curve
under the action of gravity travels with nonconstant speed.
The speed along the curve is given by v =
ds
dt , where the arclength
function s(x) satisﬁes
ds
dx =

1 + (y′(x))2.
The total time T of descent along the path y(x) is given by the integral
T =
% x=x2
x=x1
1dt =
% x2
x1
ds
v =
% x2
x1

1 + (y′)2
v
dx.
Since there is no friction and since gravity is a conservative force, the sum
of the kinetic energy and potential energy remains constant, namely,
1
2mv2
1 + mgy1 = 1
2mv + mgy.
Solving for v we obtain
v =

v2
1 + 2gy1 −2gy =

2g√y0 −y,
where y0 = y1 + (v2
1/2g) is the height from which the particle descended
from rest to reach v1 at height y1. The time of travel is
T =
1
√2g
% x2
x1

1 + (y′)2
√y0 −y
dx,
(B.5)

352
B. Calculus of Variations
and ﬁnding the path with the shortest time of travel amounts to ﬁnding a
function y(x) that minimizes this integral.
Applying the Euler-Lagrange equation, we label the integrand in Equa-
tion (B.5) as
f(x, y, y′) =

1 + (y′)2
√y0 −y .
(B.6)
One notices that this problem has one simpliﬁcation from the general Euler-
Lagrange equation: f does not depend explicitly on x. This fact allows one
to make a useful simpliﬁcation. The chain rule gives
df
dx = ∂f
∂x + y′ ∂f
∂y + y′′ ∂f
∂y′ = y′ ∂f
∂y + y′′ ∂f
∂y′
since f does not depend directly on x. However,
d
dx

y′ ∂f
∂y′
	
= y′′ ∂f
∂y′ + y′ d
dx
 ∂f
∂y′
	
,
so
df
dx = d
dx

y′ ∂f
∂y′
	
+ y′
∂f
∂y −d
dx
 ∂f
∂y′
		
= d
dx

y′ ∂f
∂y′
	
,
where the second term in the middle expression is identically 0 due to
the Euler-Lagrange equation. Integrating both sides with respect to x we
obtain
y′ ∂f
∂y′ −f = C
for some constant C. Using the speciﬁc function in Equation (B.6), we
obtain
(y′)2

(y0 −y)(1 + (y′)2)
−

1 + (y′)2
√y0 −y
= C.
Solving for y′ = dy
dx, we obtain
dy
dx =

C−2 −(y0 −y)
√y0 −y
,
which, upon taking the inverse and integrating with respect to y, becomes
x =
%
√y0 −y

C−2 −(y0 −y)
dy.
(B.7)
Using the substitution
y0 −y = 1
C2 sin2 θ
2,
(B.8)

B.3. Several Dependent Variables
353
the integral in Equation (B.7) becomes
x = −1
C2
%
sin2 θ
2 dθ = −1
2C2
%
1 −cos θ dθ
=
1
2C2 (sin θ −θ) + x0,
where x0 is some constant of integration. Rewriting Equation (B.8), setting
a = 1/(2C2), and substituting t = −θ, we obtain the equations

x = x0 + a(t −sin t),
y = y0 −a(1 −cos t).
Obviously, these equations do not give y as an explicit function of x but do
show that the path with most rapid descent is in the shape of an upside-
down cycloid.
B.3
Several Dependent Variables
B.3.1
The Main Theorem
A ﬁrst generalization to the basic problem in the calculus of variations
is to ﬁnd n twice-diﬀerentiable functions x1(t), . . . , xn(t) deﬁned over the
interval [t1, t2] that optimize the integral
I =
% t2
t1
f(x1, . . . , xn, x′
1, . . . , x′
n, t) dt.
We follow the same technique as in Section B.2. Label x1(t), . . . , xn(t) as
the actual optimizing functions and deﬁne corresponding one-parameter
families of functions by
Xi(t) = xi(t) + εξi(t),
where ξi(t) are any diﬀerentiable functions with
ξi(t1) = ξi(t2) = 0
for 1 ≤i ≤n.
With the one-parameter families Xi, we form the integral
I(ε) =
% t2
t1
f(X1, . . . , Xn, X′
1, . . . , X′
n, t) dt.
Then I(0) = I, and since by assumption the functions x1, . . . , xn are the
optimizing functions, we must also have I′(0) = 0.

354
B. Calculus of Variations
Taking the derivative of I(ε) and using the chain rule, we have
I′(ε) =
% t2
t1
∂f
∂X1
ξ1 + · · · + ∂f
∂Xn
ξn + ∂f
∂X′
1
ξ′
1 + · · · + ∂f
∂X′n
ξ′
n dt,
where by ∂f/∂Xi we mean the partial derivative to f with respect to
the variable that one evaluates to be the one parameter of functions Xi.
Regardless of the arbitrary functions ξi, setting ε = 0 replaces the family
of functions Xi with the function xi. Using the same abuse of notation for
∂f/∂xi, we have
I′(0) =
% t2
t1
∂f
∂x1
ξ1 + · · · + ∂f
∂xn
ξn + ∂f
∂x′
1
ξ′
1 + · · · + ∂f
∂x′n
ξ′
n dt = 0.
Since this equation must hold for all choices of the functions xi, we can in
particular set ξj = 0 for all indices j ̸= i. The we deduce that
% t2
t1
∂f
∂xi
ξi + ∂f
∂x′
i
ξ′
i dt = 0
for all i.
Integrating the second term in the above integral by parts and using the
fact that ξi(t1) = ξi(t2) = 0, we obtain
% t2
t1
 ∂f
∂xi
−d
dt
 ∂f
∂x′
i
		
ξi dt = 0.
Then using Lemma B.2.1, we deduce the following theorem.
Theorem B.3.1. Consider the integral
I =
% t2
t1
f(x1, . . . , xn, x′
1, . . . , x′
n, t) dt,
where f is a continuous function and where each function xi(t) is twice-
diﬀerentiable and deﬁned over [t1, t2]. Then the functions x1, x2, . . . , xn
optimize the integral I if and only if
∂f
∂xi
−d
dt
 ∂f
∂x′
i
	
= 0
for all 1 ≤i ≤n.
Here again, if f is a function as deﬁned in the above theorem, we deﬁne
the Lagrangian operator Li or Lxi as
Li(f) = ∂f
∂xi
−d
dt
 ∂f
∂x′
i
	
.

B.3. Several Dependent Variables
355
B.3.2
Lagrange Equations of Motion
As a ﬁrst application of Theorem B.3.1, we present the Lagrange equa-
tions of motion, which do not replace but rather rephrase Newton’s laws
of motion. In fact, this reformulation is particularly well suited for solving
equations of motion of a particle when the particle is naturally conﬁned to
a predeﬁned subset of R3 or when considering the motion of a solid.
As simple examples, consider the motion of a bead on a ring or a particle
on the surface of a sphere acted on by external forces. These examples
indicate that for many situations the usual Cartesian coordinates in R2 or
R3 are not ideal since they are not independent. If a particle is constrained
to a circle, one would tend to use the angle θ as a coordinate, and if a
particle is constrained to a sphere, one would tend to use the longitude
and latitude (θ, ϕ).
Therefore, we assume that the positional state of a particular physical
system can be described using coordinates q1, q2, . . . , qn that depend on
the independent time variable t. One calls the derivatives (with respect
to time) q′
1, q′
2, . . . , q′
n the generalized velocities. Both the kinetic energy T
and potential energy V can be expressed as functions of q1, q2, . . . , qn and
q′
1, q′
2, . . . , q′
n. If forces involved in the system are time dependent, then T
and V may also depend explicitly on the time variable t.
Hamilton’s principle states that the motion of a system evolves so as to
minimize the integral
S =
%
P
L dt =
% t2
t1
L dt
(B.9)
for arbitrary moments in time t1 and t2, where L is the Lagrangian function.
The integral S is called the action of the system.
When the system is
under the inﬂuence of only conservative forces, it is possible to show that
the Lagrangian is L = T −V , where T is the kinetic energy and V is the
potential energy. Problem 6.1.8 presented the action for a nonconservative
force.
Theorem B.3.1 allows us to deduce the Lagrange equations from this
principle, namely, Li(L) = 0 for all variables qi or, in other words,
∂L
∂qi
−d
dt
 ∂L
∂q′
i
	
= 0
for all 1 ≤i ≤n.
Example B.3.2 (The Spherical Pendulum). As an example, consider the spheri-
cal pendulum (see Figure B.1). This classical problem consists of a point

356
B. Calculus of Variations
x
y
z
 ϕ
θ
Figure B.1. Spherical pendulum.
mass that is hanging from a string and is free to move not just in a vertical
plane but in both its natural degrees of freedom. We label the mass of
the object at the end of the string as m and the length of the string as l.
For simplicity, we assume that the string is massless and that there is no
friction where the string attaches at a ﬁxed point. This scenario is called
the spherical pendulum problem because the same equations govern the
motion of an object moving in a spherical bowl under the action of gravity
and with no (negligible) friction.
We use a Cartesian frame of reference, in which the origin is the ﬁxed
point to which the string is attached and the z-axis lines up with the
vertical axis that the string makes when at rest and hanging straight down.
Furthermore, we orient the z-axis downward. With this setup, the degrees
of freedom are the usual angles θ and ϕ from spherical coordinates. To
obtain the Lagrange equations of motion, we need to ﬁrst identify the
kinetic energy T and potential energy V .
The velocity vector for the particle moving at the end of the string is
⃗v = l(ϕ′ cos ϕ cos θ −θ′ sin ϕ sin θ, ϕ′ cos ϕ sin θ + θ′ sin ϕ cos θ, −ϕ′ sin ϕ),
so after simpliﬁcations, the kinetic energy is
T = 1
2mgl2 
(ϕ′)2 + (θ′)2 sin2 ϕ

.
The potential energy is V = mgl(1 −cos ϕ), so the Lagrangian is
L = 1
2mgl2 
(ϕ′)2 + (θ′)2 sin2 ϕ

−mgl(1 −cos ϕ).

B.3. Several Dependent Variables
357
The Lagrange equations of motion are
d
dt
 ∂L
∂θ′
	
= ∂L
∂θ
and
d
dt
 ∂L
∂ϕ′
	
= ∂L
∂ϕ,
which lead to
d
dt

ml2θ′ sin2 ϕ

= 0
and
d
dt

ml2ϕ′
= ml2(θ′)2 sin ϕ cos ϕ −mgl sin ϕ.
After simpliﬁcations, this leads to the two equations
pθ
def
= ml2θ′ sin2 ϕ = const.,
(B.10)
ϕ′′ = sin ϕ

(θ′)2 cos ϕ −g
l

.
Since the expression we labeled as pθ is a constant, we can solve for θ′ out
of the ﬁrst equation and write the second equation only in terms of ϕ to
get the following system:
⎧
⎪
⎨
⎪
⎩
pθ = ml2θ′ sin2 ϕ,
ϕ′′ =
p2
θ
m2l4
cos ϕ
sin3 ϕ −g
l sin ϕ.
(B.11)
It might appear that the sin3 ϕ in the denominator in the second equa-
tion in Equation (B.11) could be a cause for concern at ϕ = 0 but it is not,
as we now explain. If pθ = 0, then the second equation in Equation (B.11)
does not possess a singularity at ϕ = 0. On the other hand, if pθ ̸= 0, then
by Equation (B.10), ϕ is never 0.
We have isolated the variable ϕ.
Thus, one solves the equations of
motion for a spherical pendulum by ﬁrst solving the second equation in
Equation (B.11) for ϕ(t). By integrating the ﬁrst equation, one then ob-
tains
θ′ =
pθ
ml2 sin2(ϕ(t)),
and θ(t) follows by integration once more.
B.3.3
Hamilton Equations of Motion
We now present the Hamilton equations of motion, which arise as a natu-
ral progression from Lagrange equations and also have important theoret-
ical consequences. The presentation we give here is the classical format,
where one does not worry about what the coordinates (q1, . . . , qn) actually
parametrize. The reader should compare this to the modern formulation
of Hamiltonian mechanics, which is presented in Section 6.1.

358
B. Calculus of Variations
As with the Lagrange equations above, we use a set of coordinates
(q1, q2, . . . , qn) to describe the position of the particle or system of particles
whose motion we wish to describe. We associate to each coordinate qi the
generalized momenta pi deﬁned by
pi = ∂L
∂q′
i
,
where L is the Lagrangian.
Deﬁne now the Hamiltonian of the physical system as the function in
terms of the 2n variables q1, . . . , qn, p1, . . . , pn deﬁned by
H =
n

i=1
piq′
i −L .
Writing L = " piq′
i−H, we view the Lagrangian as a function that depends
explicitly on the variables q1, . . . , qn, p1, . . . , pn, their derivatives with re-
spect to time, and the variable time t. We obtain the Hamilton equations
of motion by applying Hamilton’s principle to the Lagrangian phrased in
this manner. Consequently, we require that the integral in Equation (B.9)
be minimized for all pairs (t1, t2), and we deduce, by the Euler-Lagrange
equations, that
d
dt
∂
∂q′
i
 n

i=1
piq′
i −H

= ∂
∂qi
 n

i=1
piq′
i −H

,
and
d
dt
∂
∂p′
i
 n

i=1
piq′
i −H

=
∂
∂pi
 n

i=1
piq′
i −H

.
Since by deﬁnition of the generalized momenta, ∂L/∂q′
i = pi, the previ-
ous system of 2n equations leads immediately to the following Hamilton
equations of motion:
⎧
⎪
⎪
⎨
⎪
⎪
⎩
p′
i
= −∂H
∂qi
,
q′
i
= ∂H
∂pi
.
This formulation of the equations of motion for a physical system may,
at ﬁrst glance, seem a little obtuse, but a careful reader should note that
they apply to nonconservative systems, systems in which the potential en-
ergy may depend on some q′
i, and systems in which L depends explicitly
on time. In practice, when solving simple problems in dynamics, Newton’s
laws of motion or perhaps the Lagrange equation suﬃce. On the other

B.4. Isoperimetric Problems and Lagrange Multipliers
359
hand, the Hamiltonian formulation is central to quantum mechanics and
useful for theoretical mechanics.
If the system is conservative and the potential energy V does not depend
explicitly on q′
i, then the Hamiltonian H is equal to the total energy of the
system, as we shall see.
First note that since ∂V/∂q′
i = 0, then pi =
∂T/∂q′
i.
Suppose that we label a particle in the system as j, and call
its coordinates in a Cartesian system (xj, yj, zj). Since the whole system
can be described by the generalized coordinates q1, q2, . . . , qn, then the
coordinates (xj, yj, zj) are functions of the qis. Then we can write
x′
j =
n

i=1
∂xj
∂qi
q′
i,
y′
j =
n

i=1
∂yj
∂qi
q′
i,
z′
j =
n

i=1
∂zj
∂qi
q′
i,
where all derivatives are taken with respect to time t. Then, the kinetic
energy of the physical system (whether one must integrate or merely take
a summation), one has
T = 1
2

j
mj((x′
j)2 + (y′
j)2 + (z′
j)2),
and so T is homogeneous of degree 2 in the variables q′
i. Thus, we have in
particular
2T =
n

i=1
q′
i
∂T
∂q′
i
=
n

i=1
q′
ipi.
(B.12)
Because of Equation (B.12), we can also write
H = 2T −(T −V ) = T + V,
which shows that H is the total energy of the system, a fact that does not
hold if V is a function of the generalized velocities q′
i.
B.4
Isoperimetric Problems and Lagrange Multipliers
B.4.1
The Main Theorem
In this section, we approach a new class of problems in which one desires
not only to optimize a certain integral but to do so considering only func-
tions that satisfy an additional criterion besides the usual restriction of
continuity. In all the problems we consider, the criteria consist of imposing
a prescribed value on a certain integral related to our variable function.

360
B. Calculus of Variations
More precisely, we will wish to construct a function x(t) deﬁned over an
interval [t1, t2] that optimizes the integral
I =
% t2
t1
f(x, x′, t) dt,
(B.13)
subject to the condition that
% t2
t1
g(x, x′, t) dt = J
(B.14)
for some ﬁxed value of J. It is assumed that f and g are twice-diﬀerentiable
functions in their variables.
Such a problem is called an isoperimetric
problem.
Following the same approach as in Section B.2, we label x(t) as the
actual optimizing function to the integral in Equation (B.13), which we
assume also satisﬁes Equation (B.14), and we introduce a two-parameter
family of functions
X(t) = x(t) + ε1ξ1(t) + ε2ξ2(t),
where ξ1(t) and ξ2(t) are any diﬀerentiable functions that satisfy
ξ1(t1) = ξ2(t1) = ξ1(t2) = ξ2(t2) = 0.
(B.15)
The condition in Equation (B.15) guarantees that X(t1) = x(t1) = x1 and
X(t2) = x(t2) = x2 for all choices of the parameters ε1 and ε2. We use the
family of functions X(t) as a comparison to the optimizing function x(t),
but in contrast to Section B.2, we need a two-parameter family, as we shall
see shortly.
We replace the function x(t) with the family X(t) in Equations (B.13)
and (B.14) to obtain
I(ε1, ε2) =
% t2
t1
f(X, X′, t) dt
and
J(ε1, ε2) =
% t2
t1
g(X, X′, t) dt.
The parameters ε1 and ε2 cannot be independent if the family X(t) is to
always satisfy Equation (B.14).
Indeed, since J is constant, ε1 and ε2
satisfy the equation
J(ε1, ε2) = J
(a constant).
(B.16)

B.4. Isoperimetric Problems and Lagrange Multipliers
361
Since x(t) is assumed to be the optimizing function, then I(ε1, ε2) is op-
timized with respect to ε1 and ε2, subject to Equation (B.16) when ε1 =
ε2 = 0, no matter the particular choice of ξ1(t) and ξ2(t).
Consequently, one can apply the method of Lagrange multipliers, usu-
ally presented in a multivariable calculus course. Following that method,
I(ε1, ε2) is optimized, subject to Equation (B.16), when
⎧
⎨
⎩
∂I
∂εi
= λ ∂J
∂εi
,
for i = 1, 2, and
J(ε1, ε2) = J,
(B.17)
where λ is a free parameter called the Lagrange multiplier. In order to
apply this to Euler-Lagrange methods of optimizing integrals, deﬁne the
function
f ∗(x, x′, t) = f(x, x′, t) −λg(x, x′, t).
Then the ﬁrst two equations in Equation (B.17) are tantamount to solving
∂f ∗
∂εi
= 0.
Following a nearly identical approach as in Section B.2, the details of which
we leave to the interested reader, one can prove the following theorem.
Theorem B.4.1. Assume that f and g are twice-diﬀerentiable functions
R3 →R. Let x : [t1, t2] →R be a function that optimizes
I =
% t2
t1
f(x, x′, t) dt,
subject to the condition that
J =
% t2
t1
g(x, x′, t) dt
remains constant. Then x satisﬁes the diﬀerential equation
∂f ∗
∂x −d
dt
∂f ∗
∂x′
	
= 0,
(B.18)
where f ∗= f −λg. Furthermore, the solution to Equation (B.18) produces
an expression for x(t) that depends on two constants of integration and the
parameter λ and, if a solution to this isoperimetric problem exists, then
these quantities are ﬁxed by requiring that x(t1) = x1, x(t2) = x2, and J
be a constant.

362
B. Calculus of Variations
Many generalizations extend this theorem, but rather than presenting
in great detail the variants thereof, we present an example that shows why
one refers to the class of problems presented in this section as isoperimetric
problems.
B.4.2
Problem of Maximum Enclosed Area
Though simple to phrase and yet surprisingly diﬃcult to solve is the clas-
sic question, “What closed simple curve of ﬁxed length encloses the most
area?”
Even Greek geometers “knew” that if one ﬁxes the length of a
closed curve, the circle has the largest area, but no rigorous proof is possi-
ble without the techniques of calculus of variations.
To solve this problem, consider parametric curves ⃗x = (x(t), y(t)) with
t ∈[t1, t2].
We assume the curve is closed so that ⃗x(t1) = ⃗x(t2) and
similarly for all derivatives of ⃗x. The arclength formula for this curve is
S =
% t2
t1

(x′)2 + (y′)2 dt,
and by a corollary to Green’s Theorem, the area of the enclosed region is
A =
% t2
t1
xy′ dt.
Therefore, we wish to optimize the integral A, subject to the constraint
that the integral S is ﬁxed, say S = p.
Following Theorem B.4.1 but adapting it to the situation of more than
one dependent variable, we deﬁne the function
f ∗(x, x′, y, y′, t) = xy′ −λ

x′2 + y′2,
and conclude that the curve with the greatest area satisﬁes

Lx(f ∗) = 0,
Ly(f ∗) = 0.
(B.19)
Taking appropriate derivatives, Equation (B.19) becomes
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
y′ + λ d
dt

x′

x′2 + y′2

= 0,
d
dt

x −λ
y′

x′2 + y′2

= 0,

B.4. Isoperimetric Problems and Lagrange Multipliers
363
and integrating with respect to t, we obtain
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
y + λ
x′

x′2 + y′2 = C1,
x −λ
y′

x′2 + y′2 = C2.
From this, we deduce the relation
(x −C2)2 + (y −C1)2 = λ2,
(B.20)
which means that the curve with a given perimeter and with maximum area
lies on a circle. Since the curve is closed and simple, the parametric curve
⃗x(t) is an injective function (except for ⃗x(t1) = ⃗x(t2)), and the image is in
fact a circle, though there is no assumption that ⃗x travels around the circle
at a uniform rate. That the Lagrange multiplier appears in Equation (B.20)
is not an issue because, since we know that the perimeter is ﬁxed at p, we
know that λ = p/2π.


APPENDIX
C
Multilinear Algebra
Manifolds can be thought of locally as vector spaces in the sense that at
any point of the manifold, the tangent space is a vector space. Therefore,
in order to properly understand the objects of interest in the study of
manifolds, it is necessary to deﬁne and study multilinear functions and
related vector spaces.
This appendix introduces multilinear algebraic objects arising in this
book that are not commonly included in a ﬁrst linear algebra course. The
underlying ﬁeld for all the objects in this book is the ﬁeld of real numbers
R, but this appendix introduces the concepts for an arbitrary ﬁeld K of
characteristic 0.
The reader may wish to think of R or C since these
are the most commonly used base ﬁelds in geometry. (In fact, many of
the deﬁnitions and propositions in this appendix can be generalized to
modules over commutative rings. See [30] for a comprehensive introduction
to module theory.)
Before discussing multilinear algebra, we wish to stress the sometimes
overlooked fact that coordinates depend on a particular basis. Let V be
a vector space over K. If V is ﬁnite-dimensional, with dim V = n, and if
B = {⃗u1, . . . , ⃗un} is a basis of V , the coordinates of ⃗v with respect to B are
[⃗v]B =
⎛
⎜
⎜
⎜
⎝
v1
v2
...
vn
⎞
⎟
⎟
⎟
⎠,
where
⃗v = v1⃗u1 + · · · vn⃗un.
If the basis of V is understood from the problem or if one uses a standard
basis of V , one writes [⃗v] for the coordinates or just ⃗v, though this latter
designation is an abuse of notation since it confuses the vector with its
coordinates in a particular coordinate system.
365

366
C. Multilinear Algebra
C.1
Direct Sums
If V1 and V2 are subspaces of a vector space V , one deﬁnes the vector
subspace sum as the set
V1 + V2 = {⃗v ∈V |⃗v = ⃗v1 + ⃗v2, where ⃗v1 ∈V1 and ⃗v1 ∈V1}.
It is an easy exercise to see that this is closed under addition and scalar
multiplication, making this subset a new subspace of V . In general, V1 and
V2 share a subspace V1 ∩V2 that need not be the null space, and when this
occurs, the expression ⃗v1 + ⃗v2 for a vector in V1 + V2 need not be unique.
Two subspaces V1 and V2 of a vector space V are called complements if
V = V1 + V2 and V1 ∩V2 = {⃗0}. (See Figure C.1.) It is also an exercise in
linear algebra to prove that V1 and V2 are complements in V if and only if
every ⃗v ∈V can be written uniquely as ⃗v = ⃗v1 +⃗v2 for ⃗v1 ∈V1 and ⃗v2 ∈V2.
We leave it as an exercise (Problem C.1.1) to show that if V1 and V2 are
complements in V , then
dim V1 + dim V2 = dim V.
Deﬁnition C.1.1. Let V and W be two vector spaces over a ﬁeld K. The
direct sum V ⊕W is the vector space with underlying set V × W and
operations deﬁned as follows:
1. (⃗v1, ⃗w1) + (⃗v2, ⃗w2) = (⃗v1 + ⃗v2, ⃗w1 + ⃗w2).
2. c · (⃗v, ⃗w) = (c⃗v, c⃗w) for all c ∈K.
As phrased, this deﬁnition makes the implicit assumption that the de-
scribed set equipped with the given operations satisﬁes the deﬁnition of a
vector space, but this is an easy exercise that we omit here.
In the direct sum V ⊕W, the subspace of vectors (⃗v,⃗0) is isomorphic
to V and the subspace of vectors (⃗0, ⃗w) is isomorphic to W and so, by
an abuse of notation, we will often say that V and W are subspaces of
V ⊕W, with V and W identiﬁed according to these natural isomorphisms.
As subspaces of V ⊕W, we have V ∩W = {⃗0} and V ⊕W = V + W,
so V and W are complements in V ⊕W. Therefore, we conclude that if
{⃗v1, . . . ,⃗vm} is a basis of V and {⃗w1, . . . , ⃗wn} is a basis of W, then the set
{(⃗v1,⃗0), . . . , (⃗vm,⃗0), (⃗0, ⃗w1), . . . , (⃗0, ⃗wn)}
is a basis of V ⊕W. Consequently, one also has the following equality for
dimensions:
dim(V ⊕W) = dim V + dim W.

C.1. Direct Sums
367
V1
 V2
Figure C.1. Complementary subspaces of R3.
As a point of notation, if V is any vector space over a ﬁeld K, we use
the notation V n to denote the repeated direct sum
V n = V ⊕V ⊕· · · ⊕V,
where there are n copies of V in the direct sum.
Problems
C.1.1. Prove that if V1 and V2 are complementary subspaces of V , then dim V1 +
dim V2 = dim V .
C.1.2. Let V1, V2, W1, and W2 be vector spaces over a ﬁeld K. Suppose that
L : V1 →V2 and T : W1 →W2 are linear transformations with respect to
given bases. Deﬁne the function
f : V1 ⊕W1 →V2 ⊕W2
(⃗v, ⃗w) −→(L(⃗v), T(⃗w)).
Prove that f is a linear transformation and that the matrix of f with
respect to natural bases on V1 ⊕W1 and V2 ⊕W2 is block diagonal.
C.1.3. Let V and W be ﬁnite-dimensional vector spaces over a ﬁeld K with
dimensions m and n, respectively, and let f : V →W be a linear trans-
formation. Deﬁne the linear transformation
T : V ⊕W →V ⊕W
(⃗v, ⃗w) −→(⃗v,⃗v + f(⃗w)).
(a) Prove that the only eigenvalue of T is 1 (with multiplicity m + n).
(b) Prove that the eigenspace of 1 is
E1 = ker f ⊕W,
and conclude that the geometric multiplicity of 1 is m + n −rank f.

368
C. Multilinear Algebra
C.1.4. Let V be a vector space, and let W be a subspace. Deﬁne the relation ∼
on vectors of V by
⃗v1 ∼⃗v2 ⇐⇒⃗v1 −⃗v2 ∈W.
(a) Prove that ∼is an equivalence relation.
(b) Denote by V/W the set of equivalence classes. Prove that V/W has
the structure of a vector space under the operations: [⃗v1] + [⃗v2] =
[⃗v1 + ⃗v2] and c · [⃗v] = [c⃗v].
(c) Suppose that V is ﬁnite-dimensional. Prove that dim V/W = dim V −
dim W .
(The vector space V/W is called the quotient vector space of V with
respect to W .)
C.2
Bilinear and Quadratic Forms
Deﬁnition C.2.1. Let V and W be vector spaces over the ﬁeld K. A bilinear
form ⟨·, ·⟩on V × W is a function V × W →K such that for all ⃗v ∈V ,
⃗w ∈W, and all λ ∈K,
⟨λ⃗v, ⃗w⟩= λ⟨⃗v, ⃗w⟩,
and
⟨⃗v, λ⃗w⟩= λ⟨⃗v, ⃗w⟩.
Equivalently, given any ﬁxed ⃗v0 ∈V ,
⃗x →⟨⃗v0, ⃗x⟩
is a linear transformation W →K, and for any ﬁxed ⃗w0 ∈W,
⃗x →⟨⃗x, ⃗w0⟩
is linear from V to K. Consequently, a bilinear form is also called a bilinear
transformation.
The notation used for a bilinear form varies widely in the literature
because of the many areas in which it is used. In terms of function notation,
one might encounter the functional notation f : V × W →K or perhaps
ω : V × W →K for a bilinear form and ⟨·, ·⟩or (·, ·) for the “product”
notation. If V = W, one sometimes writes the pair (V, f) to denote the
vector space V equipped with the bilinear form f.
In basic linear algebra, the most commonly known example of a bilinear
form on Rn is the dot product between two vectors deﬁned in terms of
standard coordinates by
⃗v · ⃗w = v1w1 + v2w2 + · · · + vnwn.

C.2. Bilinear and Quadratic Forms
369
However, one should realize that the following functions Rn × Rn →R are
also bilinear forms:
⟨⃗v, ⃗w⟩1 = v1w2 + v2w1 + v3w3 · · · + vnwn,
⟨⃗v, ⃗w⟩2 = 2v1w1 + v2w2 + · · · + vnwn,
(C.1)
⟨⃗v, ⃗w⟩3 = v1w2.
Despite this variety, bilinear forms can be completely characterized by a
single matrix.
Proposition C.2.2. Let V and W be ﬁnite-dimensional vector spaces, with
dim V = m and dim W = n. Let ⟨·, ·⟩be a bilinear form on V × W. Given
a basis B of V and B′ of W, if we write [⃗v]B as the coordinates of ⃗v in the
basis B and similarly for coordinates of vectors in W, then there exists a
unique m × n matrix M such that
⟨⃗v, ⃗w⟩= [⃗v]T
B M [⃗w]B.
Furthermore, if B = {⃗ei, . . . ,⃗em} and B′ = {⃗ui, . . . , ⃗un}, then the entries
of M are mij = ⟨⃗ei, ⃗uj⟩for 1 ≤i ≤m and 1 ≤j ≤n.
Proof: Let ⃗v ∈V and ⃗w ∈W be vectors with coordinates
[⃗v] =
⎛
⎜
⎝
v1
...
vm
⎞
⎟
⎠
and
[⃗w] =
⎛
⎜
⎝
w1
...
wn
⎞
⎟
⎠.
Then since ⟨·, ·⟩is bilinear,
⟨⃗v, ⃗w⟩=
m

i=1
n

j=1
viwj⟨⃗ei, ⃗uj⟩.
(C.2)
Setting mij = ⟨⃗ei, ⃗uj⟩and the matrix M = (mij), for 1 ≤i ≤m,
n

j=1
⟨⃗ei, ⃗uj⟩wj
are the coordinates of M ⃗w and then Equation (C.2) shows that ⟨⃗v, ⃗w⟩=
[⃗v]T M[⃗w], where [⃗v]T means the coordinates of ⃗v are written in a row vector
as opposed to a column vector.
□
Deﬁnition C.2.3. Let V and W be vector spaces over K, and let ⟨·, ·⟩be a
bilinear form on V × W. Then ⟨·, ·⟩is called

370
C. Multilinear Algebra
1. nondegenerate on the left if for all ⃗v ∈V , there exists ⃗w ∈W such
that ⟨⃗v, ⃗w⟩̸= 0;
2. nondegenerate on the right if for all ⃗w ∈W, there exists ⃗v ∈V such
that ⟨⃗v, ⃗w⟩̸= 0;
3. nondegenerate if it is non-degenerate on the right and on the left.
Furthermore, the rank of ⟨·, ·⟩is the rank of its associated matrix with
respect to any basis on V and W.
Basic facts about the rank of a matrix imply that if a form is nonde-
generate on the left, then the number of rows of its associated matrix M
is equal to the rank of the form. If a form is nondegenerate on the right,
then the number of columns of M is equal to the rank of the form. Hence,
a form can only be nondegenerate if dim V = dim W.
Many applications for bilinear forms arise in geometry or analysis, and
in many situations, one has V = W or at least V isomorphic to W. In
this case, one simply says ⟨·, ·⟩is a bilinear form on V . Suppose that B is
a basis on V and that M is the matrix associated to ⟨·, ·⟩. If L : V →V
is a linear transformation with matrix A associated to the basis B, then
⟨L(⃗v), ⃗w⟩= (A⃗v)T M ⃗w = ⃗vT AT M ⃗w.
There exists a unique linear transformation L† : V →V such that ⟨L(⃗v), ⃗w⟩
= ⟨⃗v, L†(⃗w)⟩for all ⃗v, ⃗w ∈V . We ﬁnd the associated matrix A† of L† by
remarking that if
⃗vT AT M ⃗w = ⃗vT M(A† ⃗w)
for all ⃗v, ⃗w ∈V , then AT M = MA† as matrices. Hence,
A† = M −1AT M.
(C.3)
Deﬁnition C.2.4. Let ⟨·, ·⟩be a nondegenerate form on V , and let L : V →V
be a linear transformation. The linear transformation L† such that
⟨L(⃗v), ⃗w⟩= ⟨⃗v, L†(⃗w)⟩
for all ⃗v, ⃗w ∈V is called the adjoint operator to L with respect to ⟨·, ·⟩.
More generally, let V and W be vector spaces equipped with nonde-
generate bilinear forms ⟨·, ·⟩V and ⟨·, ·⟩W . Let L : V →W be a linear
transformation. Then there exists a unique linear map L† : W →V such
that
⟨L(⃗v), ⃗w⟩W = ⟨⃗v, L†(⃗w)⟩V .

C.2. Bilinear and Quadratic Forms
371
We also call L† the adjoint of L with respect to these forms. In this more
general setting, if M1 is the matrix corresponding to ⟨·, ·⟩V and M2 is the
matrix corresponding to ⟨·, ·⟩W , and if A is the matrix of L, then the adjoint
matrix A† of L† is
A† = M −1
1 AT M2.
A note on terminology is necessary at this point. Some authors refer to
L† as deﬁned above as the transpose of L with respect to a form (or forms)
and use the word adjoint of a linear transformation only in the cases when
V and W are vector spaces over C and when the form ⟨·, ·⟩is sesquilinear.
(A bilinear form on a vector space over C is a form that is linear in the
ﬁrst and conjugate-linear in the second variable. See [30] for a discussion
on sesquilinear forms.) In this book, we use the terminology of adjoint
for any linear transformation L† satisfying Deﬁnition C.2.4 for any type of
nondegenerate form—bilinear or sesquilinear—and we use the terminology
of transpose only to refer to the usual operation on matrices as presented
in a basic linear algebra course. In doing so, we hope to dispel confusion
regarding the word “transpose.”
Example C.2.5. Let L : Rn →Rm be a linear transformation between Eu-
clidean spaces, with a matrix A with respect to the standard bases. For all
⃗v, ⃗w ∈Rn,
L(⃗v) · ⃗w = (A⃗v) · ⃗w = (A⃗v)T ⃗w = ⃗vT AT ⃗w = ⃗v · (AT ⃗w).
Therefore, the transpose AT is the matrix corresponding to the adjoint of
L when we assume Rn and Rm are equipped with the usual dot product.
Proposition C.2.6. Let V , W, and U be vector spaces equipped with nonde-
generate bilinear forms. Then the following formulas hold for the adjoint:
1. (L1 + L2)† = L†
1 + L†
2 for all linear maps L1, L2 : V →W.
2. (cL)† = cL† for all linear L : V →W and c ∈K.
3. (L2L1)† = L†
1L†
2 for all linear L1 : V →W and L2 : W →U.
Proof: (Left as an exercise for the reader.)
□
In many areas of algebra and geometry, one is often lead to consider two
particular types of linear transformations associated to the adjoint operator
of a given nondegenerate bilinear form: automorphisms with respect to the
form and self-adjoint transformations. We describe these in the following
paragraphs.

372
C. Multilinear Algebra
Let V be a vector space, and let f : V × V →K be a nondegenerate
bilinear form, which we write as f(⃗v1,⃗v2) = ⟨⃗v1,⃗v2⟩. By an automorphism
of (V, f), one means a linear transformation L : V →V such that
⟨L(⃗v1), L(⃗v2)⟩= ⟨⃗v1,⃗v2⟩
for all ⃗v1,⃗v2 ∈V .
(C.4)
The set of automorphisms is in general not closed under addition, but one
can easily see that it is closed under composition. Furthermore, since the
form is nondegenerate, the adjoint of L with respect to f exists, and since
Equation (C.4) holds for all pairs of vectors ⃗v and ⃗w, then we deduce the
following proposition.
Proposition C.2.7. A linear transformation L : V →V is an automorphism
of (V, f) if and only if
L†L = Id,
(C.5)
where by Id we mean the identity transformation on V .
Proposition C.2.8. Let (V, f) be a vector space equipped with a nondegenerate
bilinear form.
Then the set S of automorphisms of (V, f) satisﬁes the
following:
1. S is closed under composition: for all L1, L2 ∈S, we have L1◦L2 ∈S.
2. The identity Id is in S.
3. If L ∈S, then L is invertible and L−1 ∈S.
Proof: We have already discussed the ﬁrst property, and the second is ob-
vious. For the third property, note that for all L ∈S, we have L†L = Id
and hence L is invertible with L−1 = L†. Furthermore, for all ⃗v, ⃗w ∈V ,
⟨L†(⃗v), L†(⃗w)⟩= ⟨L ◦L†(⃗v), L ◦L†(⃗w)⟩= ⟨⃗v, ⃗w⟩.
Thus, L† is an automorphism.
□
(Using the language of modern algebra, Proposition C.2.8 shows that
the set of automorphisms of (V, f) is a group. This group is denoted by
Aut(V, f).)
If for a vector space V equipped with a nondegenerate bilinear form f
one also has a basis B = {⃗e1, . . . ,⃗en}, then Equation (C.3) gives a charac-
terization of matrices of automorphisms. Let M be the matrix associated

C.2. Bilinear and Quadratic Forms
373
to the bilinear form f, and let A be the matrix of a linear transforma-
tion L : V →V , all in reference to B. Then by Equation (C.5), L is an
automorphism if and only if
A−1 = M −1AT M.
Before introducing the concept of self-adjoint, we discuss the symmetry
aspects of bilinear forms.
Deﬁnition C.2.9. Let V be a vector space over a ﬁeld K, and let ⟨·, ·⟩be a
bilinear form on V . Then ⟨·, ·⟩is called
1. symmetric if ⟨⃗v, ⃗w⟩= ⟨⃗w,⃗v⟩for all ⃗v, ⃗w ∈V ;
2. antisymmetric if ⟨⃗v,⃗v⟩= 0 for all ⃗v ∈V .
If ⟨·, ·⟩is antisymmetric, then for any two vectors ⃗v, ⃗w ∈V , since ⟨⃗v +
⃗w,⃗v + ⃗w⟩= 0, we also have
0 = ⟨⃗v,⃗v⟩+ ⟨⃗v, ⃗w⟩+ ⟨⃗w,⃗v⟩+ ⟨⃗w, ⃗w⟩= ⟨⃗v, ⃗w⟩+ ⟨⃗w,⃗v⟩.
Thus
⟨⃗v, ⃗w⟩= −⟨⃗w,⃗v⟩.
Example C.2.10. Let V = Rn, use the standard basis, and consider the ex-
amples in Equation (C.1). First, note that the matrix for the dot product
is just the identity matrix
⃗v · ⃗w = ⃗vT ⃗w = ⃗vT In ⃗w.
For the other examples in Equation (C.1), it is easy to ﬁnd that
⟨⃗v, ⃗w⟩1 = ⃗vT
⎛
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
· · ·
0
1
0
0
· · ·
0
0
0
1
· · ·
0
...
...
...
...
...
0
0
0
· · ·
1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
⃗w,
⟨⃗v, ⃗w⟩2 = ⃗vT
⎛
⎜
⎜
⎜
⎝
2
0
· · ·
0
0
1
· · ·
0
...
...
...
...
0
0
· · ·
1
⎞
⎟
⎟
⎟
⎠⃗w,
⟨⃗v, ⃗w⟩3 = ⃗vT
⎛
⎜
⎜
⎜
⎝
0
0
· · ·
0
1
0
· · ·
0
...
...
...
...
0
0
· · ·
0
⎞
⎟
⎟
⎟
⎠⃗w.

374
C. Multilinear Algebra
The dot product, ⟨⃗v, ⃗w⟩1, and ⟨⃗v, ⃗w⟩2 are nondegenerate and symmetric.
The form ⟨⃗v, ⃗w⟩3 is degenerate with rank 1 and is neither symmetric nor
antisymmetric.
An example of an antisymmetric form on R3 is given by
⟨⃗v, ⃗w⟩= ⃗vT
⎛
⎝
0
1
0
−1
0
0
0
0
0
⎞
⎠⃗w.
Example C.2.11. Example C.2.10 indicates that the dot product is a sym-
metric, nondegenerate bilinear transformation with associated matrix In,
and Example C.2.5 shows that the transpose of a matrix is the adjoint of a
matrix with respect to the dot product. However, consider the symmetric
bilinear forms f1 and f2 on R4 given by the matrices
M1 =
⎛
⎜
⎜
⎝
0
1
0
0
1
0
0
0
0
0
1
0
0
0
0
1
⎞
⎟
⎟
⎠
and
M2 =
⎛
⎜
⎜
⎝
0
0
0
1
0
0
1
0
0
1
0
0
1
0
0
0
⎞
⎟
⎟
⎠.
Then a simple calculation using Equation (C.3) shows that the adjoint of
A = (aij) with respect to f1 is
⎛
⎜
⎜
⎝
a22
a12
a32
a42
a21
a11
a31
a41
a23
a13
a33
a43
a24
a14
a34
a44
⎞
⎟
⎟
⎠,
and the adjoint of A with respect to f2 is
⎛
⎜
⎜
⎝
a44
a34
a24
a14
a43
a33
a23
a13
a42
a32
a22
a12
a41
a31
a21
a11
⎞
⎟
⎟
⎠.
Remark C.2.12. If (V, f) is a vector space with a symmetric form, then for
all linear transformations L : V →V , we show that L†† = L. By deﬁnition
of the adjoint operator, f(L(⃗v), ⃗w) = f(⃗v, L†(⃗w)) for all ⃗v, ⃗w ∈V . Since f
is symmetric, f(L(⃗v), ⃗w) = f(⃗w, L(⃗v)). Thus,
f(⃗w, L(⃗v)) = f(L(⃗v), ⃗w) = f(⃗v, L†(⃗w)) = f(L†(⃗w),⃗v) = f(⃗w, L††(⃗v)).
Since these equalities hold for all ⃗v, ⃗w ∈V and since f is nondegenerate,
we conclude that L = L††. In general, this equality does not hold if f is
not symmetric.

C.2. Bilinear and Quadratic Forms
375
We now deﬁne self-adjoint operators.
Deﬁnition C.2.13. Let V be a real vector space with a bilinear form f = ⟨·, ·⟩.
A linear transformation L : V →V is called self-adjoint with respect to
this form if
⟨L(⃗v), ⃗w⟩= ⟨⃗v, L(⃗w)⟩
for all ⃗v and ⃗w in V .
If dim V = n, an n × n matrix A is also called
self-adjoint with respect to ⟨·, ·⟩if ⃗v →A⃗v is a self-adjoint operator.
It is easy to see that this deﬁnition leads to the criterion that L is
self-adjoint if and only if L† = L.
We ﬁnally brieﬂy introduce quadratic forms on a vector space. Let V
be a vector space over a ﬁeld K of characteristic 0. We shall call a function
f : V →K a quadratic form if there exists a symmetric bilinear map
g : V × V →K such that
f(⃗x) = g(⃗x, ⃗x).
Interestingly enough, given a quadratic form, it is possible to recover the
corresponding symmetric bilinear form as
g(⃗x, ⃗y) = 1
2 (f(⃗x + ⃗y) −f(⃗x) −f(⃗y)) .
(C.6)
Problems
C.2.1. Prove Proposition C.2.6.
C.2.2. Prove Equation (C.6).
C.2.3. Let V and W be ﬁnite vector spaces over a ﬁeld K. Suppose that V and
W are equipped with nondegenerate bilinear forms denoted by ⟨, ⟩V and
⟨, ⟩W , respectively. Let L : V →W be a surjective linear transformation,
and let L† be its adjoint, namely, L† : W →V satisﬁes
⟨L(v), w⟩W = ⟨v, L†(w)⟩V
for all v ∈V and w ∈W .
(a) Show that L† is injective.
(b) Assume in addition that for all v ∈V with v ̸= ⃗0, ⟨v, v⟩V ̸= 0. Then
show that ker L and Im L† are orthogonal complements in V , that is:
(1) all v ∈V can be written as v = v1 + v2, where v1 ∈ker L and
v2 ∈Im L†;
(2) ker L ∩Im L† = {⃗0}; and
(3) for all v1 ∈ker L and v2 ∈Im L†, we have ⟨v1, v2⟩V = 0.

376
C. Multilinear Algebra
C.3
The Hom Space and the Dual Space
In this section, let K be a ﬁeld of characteristic 0 (e.g., R or C).
Deﬁnition C.3.1. Let V and W be two real vector spaces over K. Denote the
set of all linear transformations from V to W by HomK(V, W), or simply
Hom(V, W) if the ﬁeld K is understood in the given context.
Deﬁne addition and multiplication by a K-scalar on Hom(V, W) in the
following way. If T1 and T2 are two linear transformations in Hom(V, W),
then T1 + T2 is the linear transformation given by
(T1 + T2)(⃗v) = T1(⃗v) + T1(⃗v)
for all ⃗v ∈V.
Also, if λ ∈K and T ∈Hom(V, W), deﬁne the linear transformation λT by
(λT )(⃗v) = λ(T (⃗v))
for all ⃗v ∈V.
These deﬁnitions lead us to the following foundational proposition.
Proposition C.3.2. Let V and W be vector spaces over K of dimension m
and n, respectively.
Then Hom(V, W) is a vector space over K, with
dim Hom(V, W) = mn.
Proof: We leave it to the reader to check that Hom(V, W) satisﬁes all the
axioms of a vector space over K.
To prove that dim Hom(V, W) = mn, ﬁrst choose a basis {⃗v1, . . . ,⃗vm}
of V and a basis {⃗w1, . . . , ⃗wn} of W. Deﬁne Tij ∈Hom(V, W) as the linear
transformations deﬁned by
Tij(⃗vk) =

⃗wi,
if j = k,
⃗0,
if j ̸= k,
and extended by linearity over all V . We show that {Tij} for 1 ≤i ≤m
and 1 ≤j ≤n form a basis of Hom(V, W).
Because of linearity, any linear transformation L ∈Hom(V, W) is com-
pletely deﬁned given the knowledge of L(⃗vj) for all 1 ≤j ≤m. Suppose
that for each j,
L(⃗vj) =
n

i=1
aij ⃗wi.
Then
L =
n

i=1
m

j=1
aijTij,

C.3. The Hom Space and the Dual Space
377
and hence, the Tij span Hom(V, W). Furthermore, suppose that
n

i=1
m

j=1
cijTij
is the 0-linear transformation for some constants cij. Then for all 1≤k≤m,
n

i=1
m

j=1
cijTij(⃗vk) = ⃗0 ⇐⇒
n

i=1
cik ⃗wi = ⃗0.
However, since {⃗w1, . . . , ⃗wn} is a linearly independent set, given any k,
cik = 0. Hence, for all i and j, the constants cij = 0, which shows that the
linear transformations Tij are linearly independent.
Therefore, we conclude that the linear transformations Tij, for 1 ≤i ≤n
and 1 ≤j ≤m, form a basis of Hom(V, W).
□
The proof of Proposition C.3.2 provides as a standard basis of
Hom(V, W) the set of linear transformation {Tij} that, with respect to
given bases in V and W, have corresponding n × m matrices Eij with
1 ≤i ≤n and 1 ≤j ≤m, where the entries of Eij are all 0 except for a 1
in the (i, j)th entry.
For a vector space V over a ﬁeld K, if we set W = K, then the space
Hom(V, K) is given a special name, the dual space to V , and is denoted
by V ∗. In other words, the dual V ∗is the vector space of all linear func-
tions f : V →K. By Proposition C.3.2, if V is ﬁnite-dimensional, then
dim V ∗= dim V and, by well-known facts from linear algebra, V and V ∗
are isomorphic. If B = {⃗v1,⃗v2, . . . ,⃗vn} is a basis for V , then the linear
functions fi : V →K, with 1 ≤i ≤n such that
fi(⃗v) = ci,
where
[⃗v]B =
⎛
⎜
⎝
c1
...
cn
⎞
⎟
⎠,
(C.7)
form a basis of V ∗. Note that one can give the alternate characterization
that the functions fi are linear and satisfy the property that fi(⃗vi) = 1
if i = j and 0 otherwise. Therefore, the map ϕ : V →V ∗that sends ⃗vi
to fi for all 1 ≤i ≤n (and that is completed by linearity) provides an
isomorphism between V and V ∗.
We sometimes denote the functions fi by ⃗v∗
i , though one must remember
that the entire basis {⃗v∗
1, . . . ,⃗v∗
n} is deﬁned in reference to B: there is no
canonical way to deﬁne a function ⃗v∗∈Hom(V, K) in reference to a single

378
C. Multilinear Algebra
vector ⃗v without reference to a basis of V . The basis {⃗v∗
1, . . . ,⃗v∗
n} is called
the dual cobasis or dual coframe to {⃗v1, . . . ,⃗vn}.
Given any bilinear form ω : V × V →K and a ﬁxed vector ⃗v ∈V ,
the linear function f : V →K deﬁned by f(⃗w) = ω(⃗v, ⃗w) is an element
on the dual space V ∗. We often write f deﬁned in this way as ω(⃗v, ).
Problem C.3.4 asks the reader to ﬁnd the coordinates of f with respect to
{⃗v∗
1, . . . ,⃗v∗
n} in terms of the coordinates of ⃗v in the basis {⃗v1, . . . ,⃗vn}.
In the context of a dual space, we are in a position to explain the mean-
ing of the transpose of a matrix. Suppose that V and W are two vector
spaces over K and that L : V →W is a linear transformation between
them. There is a natural way to deﬁne an associated linear transforma-
tion W ∗→V ∗as follows. Given a linear function g ∈W ∗, the function
⃗v →g(L(⃗v)) is an element of V ∗. Therefore, we call L∗: W ∗→V ∗the
transformation such that L∗(g) is the unique element of V ∗that satisﬁes
L∗(g)(⃗v) = g(L(⃗v)).
(C.8)
It is easy to see that L∗is again linear, and hence, L∗∈Hom(W ∗, V ∗).
This transformation L∗is called the dual of L.
Suppose now that V and W are both ﬁnite-dimensional and have bases
B = {⃗v1 . . . ,⃗vm} and B′ = {⃗w1 . . . , ⃗wn}, respectively, and let A = (aij) be
the matrix of the linear transformation L with respect to these bases so
that
L(⃗vi) =
n

j=1
aij ⃗wj.
Let {⃗v∗
1 . . . ,⃗v∗
m} and {⃗w∗
1 . . . , ⃗w∗
n} be the dual cobases for V ∗and W ∗. Let
⃗u ∈V be written as ⃗u = c1⃗v1 + . . . + cm⃗vm. Then
L∗(⃗w∗
j )(⃗u) = ⃗w∗
j (L(⃗u)) = ⃗w∗
j

L
 m

i=1
ci⃗vi

= ⃗w∗
j
 m

i=1
ciL(⃗vi)

= ⃗w∗
j
 m

i=1
ci
n

k=1
aik ⃗wk

=
m

i=1
ci
n

k=1
aik ⃗w∗
j (⃗wk) =
m

i=1
ciaij
=
m

i=1
aij⃗v∗
i (⃗u).
Therefore, the matrix for L∗is in fact (aji).
We summarize the above
discussion in the following proposition.

C.3. The Hom Space and the Dual Space
379
Proposition C.3.3. Let V and W be ﬁnite-dimensional K-vector spaces with
bases B and B′, respectively. Let L : V →W be a linear transformation
with matrix A with respect to these bases. Then let L∗: W ∗→V ∗be
the linear transformation deﬁned by L∗(g) = g(L( )). Then L∗is a linear
transformation with matrix AT , the transpose of A, with respect to the bases
on V ∗and W ∗associated to B and B′.
Corollary C.3.4. Let V be a vector space with two bases B and B′, and let M
be the transition matrix from coordinates in B′ to coordinates in B. Then
in the dual V ∗, the transition matrix from coordinates in B∗to coordinates
in B
′∗is the transpose M T .
It is a common theme in a ﬁrst course in linear algebra to clearly distin-
guish between a particular element of a vector space and the coordinates
of the vector with respect to a basis. A vector in a vector space (respec-
tively, a linear transformation between vector spaces) exists independently
of its coordinates (respectively, matrix) in any particular basis of the vector
space. Linear transformations that arise naturally in geometry (e.g., pro-
jection on a line or plane, reﬂection through a line or plane, rotation, etc.)
are usually deﬁned without reference to a basis and then ﬁnding a matrix
with respect to some basis is often a simple exercise. On the other hand, a
linear transformation T : V →W can be given by its corresponding matrix
with respect to bases on V and W, but the function T is independent of
any basis.
Deﬁnitions or properties of a vector that can be given without reference
to any particular basis are called canonical. For example, the deﬁnition of
the dual of a vector space or the deﬁnition of a dual of a linear transforma-
tion in Equation (C.8) are canonical deﬁnitions. On the other hand, when
a vector space V has a basis B, there exists an isomorphism ϕ as deﬁned by
Equation (C.7), but this isomorphism is not canonical. In fact, there does
not exist a canonical isomorphism between a vector space and its dual.
However, if we consider the double-dual of V , namely the dual of V ∗,
given any vector ⃗v ∈V , one can deﬁne a linear transformation T⃗v ∈V ∗∗=
Hom(V ∗, K) by
T⃗v(f) = f(⃗v).
(C.9)
This deﬁnes a function ψ : V →V ∗∗by ψ(⃗v) = T⃗v.
Proposition C.3.5. The function ψ deﬁned by Equation (C.9) is an injective
linear transformation. Furthermore, if V is ﬁnite-dimensional, then ψ is a
canonical isomorphism between a vector space V and its double dual V ∗∗.

380
C. Multilinear Algebra
Proof: The function ψ was deﬁned without reference to any basis so is
canonical.
We ﬁrst prove that ψ is a linear transformation. Let ⃗v, ⃗w ∈V , and let
c ∈K. For all f ∈V ∗,
ψ(⃗v + ⃗w)(f) = T⃗v+ ⃗w(f) = f(⃗v + ⃗w) = f(⃗v) + f(⃗w) = T⃗v(f) + T ⃗w(f),
ψ(⃗v + ⃗w)(f) = ψ(⃗v)(f) + ψ(⃗w)(f),
so as functions, ψ(⃗v + ⃗w) = ψ(⃗v) + ψ(⃗w). Similarly,
ψ(c⃗v)(f) = Tc⃗v(f) = f(c⃗v) = cf(⃗v) = cT⃗v(f) = cψ(⃗v)(f),
so again ψ(c⃗v) = cψ(⃗v).
Next we show that ψ is injective.
Let ⃗u1, ⃗u2 ∈V be vectors, and
suppose that ψ(⃗u1) = ψ(⃗u2). Thus, f(⃗u1) = f(⃗u2) for all f ∈Hom(V, K).
Therefore, f(⃗u1 −⃗u2) = 0 for all f ∈V ∗, hence ⃗u1 −⃗u2 = ⃗0, and thus,
⃗u1 = ⃗u2, proving that ψ is injective.
Finally, we prove that if V is ﬁnite-dimensional, then ψ is an isomor-
phism. If V is ﬁnite-dimensional, then it possesses a basis B = {⃗v1, . . . ,⃗vn}.
Call {⃗v∗
1, . . . ,⃗v∗
n}.
Then any element f ∈V ∗can be written as f =
a1⃗v∗
1 + . . . + an⃗v∗
n. Therefore, if ⃗vj is any basis vector of V , we have
T⃗vj(f) = T⃗vj
 n

i=1
ai⃗v∗
i

=
n

i=1
ai⃗v∗
i (⃗vj) =
n

i=1
aiδi
j = aj,
and we conclude that ψ(⃗vj) = ⃗v∗∗
j . This proves that ψ establishes a bi-
jection between B and the associated basis on V ∗∗, and hence gives an
isomorphism between V and V ∗∗.
□
Though a more complete discussion lies beyond what we wish to cover
here, we point out that requiring V to be ﬁnite-dimensional for V and V ∗∗
to be canonically isomorphic is not a limitation of the above proof. If V
is inﬁnite-dimensional with a basis B, it is possible to show that a basis of
V ∗has a strictly greater cardinality than |B|. This suﬃces to show that V
and V ∗and by extension V ∗∗cannot be isomorphic.
However, the beneﬁt of Proposition C.3.5 is that there is no distinction
between V and V ∗∗. This might seem like an innocuous result at ﬁrst,
but it implies that for all functional and linear algebraic reasons one may
always identify V ∗∗with V . As we shall see more clearly here below, for
diﬀerential geometry, this is the ultimate reason why we only have two
types of indices—covariant and contravariant—in tensors (see Section 2.4)
as opposed to an unlimited number of types of indices.

C.4. The Tensor Product
381
Problems
C.3.1. Prove that Hom(V, W ), with the scalar multiplication and addition as
deﬁned in this section satisﬁes all the axioms of a vector space over a ﬁeld
K.
C.3.2. Let V be a vector space with basis {⃗v1, . . . ,⃗vn}. Clearly prove that the
set of functions {fi} deﬁned in Equation (C.7) form a basis of V ∗.
C.3.3. Let U, V , and W be vector spaces over a ﬁeld K. Prove that there exist
canonical isomorphisms
Hom(U ⊕V, W ) ≈Hom(U, W ) ⊕Hom(V, W ),
Hom(U, V ⊕W ) ≈Hom(U, V ) ⊕Hom(U, W ).
C.3.4. Let V be a vector space equipped with a bilinear form ω. Set f ∈V ∗as
the element such that f(⃗w) = ω(⃗v, ⃗w). Let B = {⃗v1, . . . ,⃗vn} be a basis of
V , and let B∗= {⃗v∗
1, . . . ,⃗v∗
n} be the associated cobasis of V ∗. Prove that
in coordinates
[f]B∗= AT [⃗v]B ,
where A is the n × n matrix with entries Aij = ω(⃗vi,⃗vj).
C.4
The Tensor Product
Let V and W be vector spaces over a ﬁeld K. The set V ×W is not a vector
space, but it is possible to deﬁne a vector space associated to V × W.
(The following construction is a little abstract. The casual reader should
feel free to focus his or her attention on the explanations and propositions
following Deﬁnition C.4.2.)
Let U be another vector space over a ﬁeld K. Recall that s function
f : V × W →U is called a bilinear transformation if f is linear in both of
its input variables. More precisely, f satisﬁes
f(⃗v1 + ⃗v2, ⃗w) = f(⃗v1) + f(⃗v2, ⃗w),
f(λ⃗v, ⃗w) = λf(⃗v, ⃗w),
f(⃗v, ⃗w1 + ⃗w2) = f(⃗v, ⃗w1) + f(⃗v, ⃗w2),
f(⃗v, λ⃗w) = λf(⃗v, ⃗w),
for all ⃗v1,⃗v2,⃗v ∈V , for all ⃗w1, ⃗w2, ⃗w ∈W, and all λ ∈K. The deﬁnition
of what we will call the tensor product of two vector spaces relies on the
following proposition.
Proposition C.4.1. Let U, V , and W be vector spaces over a ﬁeld K. There
exists a unique vector space Z over K and a bilinear transformation V ×
W →Z such that for any bilinear transformation f : V × W →U, there
exists a unique linear transformation ¯f : Z →U such that f = ¯f ◦ψ.

382
C. Multilinear Algebra
Proof: We ﬁrst prove the existence of the vector space Z. Consider the set
¯Z of formal ﬁnite linear combinations
c1(⃗v1, ⃗w1) + c2(⃗v2, ⃗w2) + · · · + cl(⃗vl, ⃗wl),
where ⃗vi ∈V , ⃗wi ∈W, and ci ∈K for 1 ≤i ≤l. It is not hard to see that
¯Z is a vector space over K. Consider now the subspace ¯Zlin spanned by
vectors of the form
(⃗v1 + ⃗v2, ⃗w) −(⃗v1, ⃗w) −(⃗v2, ⃗w),
(λ⃗v, ⃗w) −λ(⃗v, ⃗w),
(⃗v, ⃗w1 + ⃗w2) −(⃗v, ⃗w1) −(⃗v, ⃗w2),
(⃗v, λ⃗w) −λ(⃗v, ⃗w).
(C.10)
Deﬁne Z as the quotient vector space Z = ¯Z/ ¯Zlin. The elements of Z are
equivalence classes of elements of ¯Z under the equivalence relation ⃗u ∼⃗v
if and only if ⃗v −⃗u ∈¯Zlin.
Deﬁne ψ : U × V →Z as the composition ψ = π ◦i, where π : ¯Z →Z
is the canonical projection and i : V × W →¯Z is the inclusion. The space
¯Zlin is deﬁned in such a way that the canonical projection π turns ψ into
a bilinear transformation.
Now given any bilinear transformation f : V ×W →U, we can complete
f by linearity to deﬁne a linear transformation ˜f from ¯Z to U via
˜f (c1(⃗v1, ⃗w1) + · · · + cl(⃗vl, ⃗wl)) = c1f(⃗v1, ⃗w1) + · · · + clf(vl, ⃗wl).
If z0 ∈¯Zlin, then z0 is a linear combination of elements of the form in Equa-
tion (C.10). However, every element of the form given in Equation (C.10)
maps to ⃗0 under ˜f, so ˜f(z0). Therefore, if z1, z2 ∈¯Z are such that z1 ∼z2,
then z1 −z2 = z0 ∈¯Zlin, so ˜f(z1 −z2) = ⃗0 and ˜f(z1) = ˜f(z2). Hence, ˜f
induces a function ¯f : ¯Z/ ¯Zlin →U. It is easy to check that ¯f is a linear
transformation and that f = ¯f ◦ψ. Since the image of ψ spans ¯Z/ ¯Zlin, it
follows that the induced map ¯f is uniquely determined. This proves the
existence of Z.
To prove uniqueness of Z, suppose there is another vector space Z′ and
a bilinear transformation ψ′ : V ×W →Z′ with the desired property. Then
there exist ¯ψ and ¯ψ′ such that ψ′ = ¯ψ′ ◦ψ and ψ = ¯ψ ◦ψ′. Then we have
ψ = ¯ψ ◦¯ψ′ ◦ψ. However, ψ = idZ ◦ψ, and since we know that ψ factors
through Z with a unique map, then ¯ψ ◦¯ψ′ = idZ. Similarly, one can show
that ¯ψ′ ◦¯ψ = idZ′. Thus, Z ∼= Z′, and so Z is unique up to a natural
isomorphism.
□
Deﬁnition C.4.2. The vector space Z in the above proposition is called the
tensor product of V and W and is denoted by V ⊗W. The element ψ(⃗v, ⃗w)
in V ⊗W is denoted by ⃗v ⊗⃗w.

C.4. The Tensor Product
383
Elements of V ⊗W are linear combinations of vectors of the form ⃗v ⊗⃗w,
with ⃗v ∈V and ⃗w ∈W. With this notation, it is understood that
(⃗v1 + ⃗v2) ⊗⃗w = ⃗v1 ⊗⃗w + ⃗v2 ⊗⃗w,
(λ⃗v) ⊗⃗w = λ(⃗v ⊗⃗w),
⃗v ⊗(⃗w1 + ⃗w2) = ⃗v ⊗⃗w1 + ⃗v ⊗⃗w2,
⃗v ⊗(λ⃗w) = λ(⃗v ⊗⃗w).
Deﬁnition C.4.3. Any element of a tensor product of two vector spaces is
often simply called a tensor. A tensor in V ⊗W that can be written as
⃗v ⊗⃗w for ⃗v ∈V and ⃗w ∈W is called a pure tensor.
Deﬁnition C.4.2 introduces the concept of a tensor product between
two vector spaces as a vector space with a certain universal property, i.e.,
a property that relates it to all other possible vector spaces. The following
propositions state some of the basic properties of tensor products of vector
spaces. When used in the context of diﬀerential geometry, it is convenient
to understand the tensor product using Proposition C.4.6 with the relations
presented in Equation (C.10). For the sake of brevity, we omit the proofs
of the following propositions and refer the reader to [30, Chapter XVI].
Proposition C.4.4. Let V1, V2, and V3 be three vector spaces over a ﬁeld K.
There exists a unique isomorphism
(V1 ⊗V2) ⊗V3 ∼= V1 ⊗(V2 ⊗V3)
such that
(⃗u ⊗⃗v) ⊗⃗w →⃗u ⊗(⃗v ⊗⃗w)
for all ⃗u ∈V1, ⃗v ∈V2, and ⃗w ∈V3.
In light of Proposition C.4.4, the notation V1 ⊗V2 ⊗V3 is well deﬁned.
Furthermore, one can take the tensor product of any number of vector
spaces. The tensor product of k copies of V is denoted by
V ⊗V ⊗· · · ⊗V = V ⊗k.
We simply state the following two propositions and leave the proofs of
these as simple exercises for the reader.
Proposition C.4.5. Let V and W be two vector spaces over a ﬁeld K. There
exists a unique isomorphism
V ⊗W ∼= W ⊗V
such that ⃗v ⊗⃗w →⃗w ⊗⃗v for all ⃗v ∈V and ⃗w ∈W.

384
C. Multilinear Algebra
Proposition C.4.6. If V and W are ﬁnite-dimensional vector spaces over
a ﬁeld K with dimension m and n, respectively, then V ⊗W is ﬁnite-
dimensional with dimension mn. Furthermore, if {⃗e1, . . . ,⃗em} is a basis of
V and {⃗f1, . . . , ⃗fn} is a basis of W, then
{⃗ei ⊗⃗fj | 1 ≤i ≤m and 1 ≤j ≤n}
is a basis of V ⊗W.
Because of Proposition C.4.6, if ⃗a ∈V ⊗W, it is common to use two
indices to index the coordinates of ⃗a with respect to the basis B = {⃗ei⊗⃗fj}.
Saying that the vector ⃗a has components (aij) with respect to B means that
⃗a =
m

i=1
n

j=1
aij⃗ei ⊗⃗fj.
We have used the superscript notation for the coordinates of ⃗a to be con-
sistent with the Einstein summation convention. Note that if ⃗v ∈V and
⃗w ∈W are vectors with coordinates
⃗v = v1⃗e1 + · · · + vn⃗en
and
⃗w = w1 ⃗f1 + · · · + wm ⃗fm,
then using the linearity properties of the ⊗symbol, the coordinates for
⃗v ⊗⃗w are
⃗v ⊗⃗w =
m

i=1
n

j=1
viwj⃗ei ⊗⃗fj.
The next proposition provides a profound connection between the dual
space of a vector space and linear transformations from this space.
Proposition C.4.7. Let V and W be ﬁnite-dimensional vector spaces over a
ﬁeld K. The space V ∗⊗W is canonically isomorphic to Hom(V, W).
Proof: Consider the function ϕ : V ∗⊗W −→Hom(V, W) deﬁned on each
pure tensor λ ⊗⃗w ∈V ∗⊗W by
λ ⊗⃗w −→(⃗v →λ(⃗v)⃗w)
and extended by linearity. This function ϕ is well deﬁned as a function and
is a linear transformation. This claim follows from the properties of the
tensor product of two vector spaces. (The details are left as an exercise for
the reader.)

C.4. The Tensor Product
385
The kernel of ϕ consists of all linear combinations c1λ1 ⊗⃗w1 + · · · +
cmλm ⊗⃗wm such that the function in Hom(V, W) deﬁned by
c1λ1(·)⃗w1 + · · · + cmλm(·)⃗wm
is identically 0. Because of the properties of the tensor product, without
loss of generality, we can assume that {⃗w1, . . . , ⃗wm} is a linear independent
set of vectors in W. Thus for each ⃗v ∈V , we conclude that for 1 ≤i ≤m,
each ciλi is such that ciλi(⃗v) = 0. Therefore, either ci = 0 in K or λi = 0
in V ∗. From this we conclude that ker ϕ = {0}.
Conversely, let T ∈Hom(V, W) be any linear transformation.
Let
{⃗v1, . . . ,⃗vn} be a basis of V , and consider the linear functions {⃗v∗
1, . . . ,⃗v∗
n}
(see Equation (C.7) and the subsequent explanation). Then the element
n

i=1
⃗v∗
i ⊗T (⃗vi)
maps to T under ϕ. Therefore, ϕ is also surjective.
□
We now come to a point of notation where mathematicians and physi-
cists have often used diﬀerent notations to discuss the same object, a dif-
ference that can lead to some confusion for readers consulting both math-
ematics texts and physics texts. In numerous applications of multilinear
algebra, in particular in diﬀerential geometry, one often has a ﬁnite dimen-
sional vector space V and considers associated vector spaces V ⊗p ⊗V ∗⊗q.
If B = {⃗e1, . . . ,⃗en} is a basis of V , then the basis of V ⊗p ⊗V ∗⊗q associated
to B consists of all vectors of the form
⃗ei1 ⊗· · · ⊗⃗eip ⊗⃗e∗
j1 ⊗· · · ⊗⃗e∗
jq
for ik = 1, 2, . . . n and jl = 1, 2, . . .n. Note that this basis conﬁrms the fact
that dim V ⊗p ⊗V ∗⊗q = np+q. It is common practice in advanced linear
algebra to express the coordinates of a tensor A ∈V ⊗p ⊗V ∗⊗q as
Ai1i2···ip
j1j2···jq
(C.11)
so that
A =
n

i1=1
n

i2=1
· · ·
n

ip=1
n

j1=1
n

j2=1
· · ·
n

jq=1
Ai1i2···ip
j1j2···jq⃗ei1⊗⃗ei2⊗· · ·⊗⃗eip⊗⃗e∗
j1⊗⃗e∗
j2⊗· · ·⊗⃗e∗
jq.
(C.12)
Using superscript and subscript indices allows one to easily distinguish
between the constituent indices from V and from V ∗.
The superscript

386
C. Multilinear Algebra
indices are often called contravariant indices, while the subscript indices
are called covariant indices. Elements of V ⊗p ⊗V ∗⊗q are called tensors of
type (p, q) over V .
The following proposition gives the coordinate-transformation proper-
ties of tensors in V ⊗p ⊗V ∗⊗q under a change of basis in V .
Proposition C.4.8. Let B and B′ be two bases on a ﬁnite dimensional vec-
tor space V . Let (as
r) be the coordinate-change matrix from B to B′. Let
T i1i2···ip
j1j2···jq be the components of a tensor T ∈V ⊗p ⊗V ∗⊗q with respect to B,
and let ¯T i1i2···ip
j1j2···jq be the components of the same tensor T with respect to B′.
Then these two diﬀerent components are related by
¯T k1k2···kp
l1l2···lq
=
n

i1=1
n

i2=1
· · ·
n

ip=1
n

j1=1
n

j2=1
· · ·
n

jq=1
ak1
i1 ak2
i2 · · · akp
ip aj1
l1 aj2
l2 · · · ajq
lq T i1i2···ip
j1j2···jq.
(C.13)
Proof: This result follows easily from the linearity properties of tensors and
Proposition C.3.3. (We leave the details to the reader.)
□
Because of the cumbersome nature of Equation (C.12), mathematicians
introduce the tensor product of vector spaces as we have done here and then
simply use the symbol A (or
 ) to express the above tensor. In contrast,
physicists often introduce tensors using the coordinate symbol in Equa-
tion (C.11) and then say that these coordinate symbols must satisfy the
transformational properties described in Proposition C.4.8. The transfor-
mational property in Equation (C.13) is also quite laborious to write down.
Because of this, one usually uses the Einstein summation convention, as
introduced in Section 2.4. In this convention, one assumes that one takes
a summation over any indices that are repeated in an expression involving
the components of a tensor. Furthermore, due to the linear algebraic inter-
pretation of tensor components, repeated indices most often occur between
a contravariant and a covariant index.
In Section 2.4 and, in particular, Deﬁnition 2.4.6, we introduce tensors
on a surface in the manner that physicists typically do. (We discuss tensors
in Chapter 4 but there we work in the context of vector bundles over
a manifold and not just vector spaces.)
The transformational property
speciﬁed in Equation (2.27) corresponds precisely to Proposition C.4.8,
where the change-of-basis matrix in V = Rn is the Jacobian matrix
∂¯x
∂x

.

C.4. The Tensor Product
387
One should also note that in light of the chain rule and Equation (2.22),
the Jacobian matrix satisﬁes the additional property of being an orthogonal
matrix, i.e., that its inverse is equal to its transpose.
In light of Proposition C.4.7, we can give a linear algebraic interpreta-
tion to tensors of type (p, q) over V . If V is ﬁnite-dimensional, an element
of V ∗⊗p ⊗V ⊗q corresponds to a linear transformation in Hom(V ⊗p, V ⊗q).
For example, a tensor of type (1, 1) over V corresponds to a linear trans-
formation V →V . The components of the tensor given in terms of a basis
on V correspond to the matrix of the linear transformation with respect
to the given basis. As another example, a tensor of type (0, 2) is a linear
transformation from V ⊗V to the base ﬁeld K, or in other words, it is a
bilinear form on V .
We comment now on the linear algebraic meaning of a few common
operations on tensors, in particular, addition, scalar multiplication, multi-
plication, and contraction, which were introduced in Section 2.4.
If Ai1i2···ip
j1j2···jq form the components of a (p, q)-tensor A and Bi1i2···ip
j1j2···jq form
the components of a (p, q)-tensor B, then we can deﬁne the term-by-term
addition
Ci1i2···ip
j1j2···jq = Ai1i2···ip
j1j2···jq + Bi1i2···ip
j1j2···jq.
This operation on components simply corresponds to the addition of A
and B as elements in the vector space V ⊗p ⊗V ∗⊗q. Similarly, given the
components Ai1i2···ip
j1j2···jq of a tensor of type (p, q), the operation of multiplying
all the components by a given scalar c in the base ﬁeld K corresponds to
multiplying the tensor A by the scalar c again as an operation in the vector
space V ⊗p ⊗V ∗⊗q.
It is not hard to check that if Si1i2···ir
j1j2···js and T k1k2···kt
l1l2···lu
are components of
tensors of type (r, s) and (t, u), respectively, then the quantities obtained
by multiplying these components
W i1i2···irk1k2···kt
j1j2···jsl1l2···lu = Si1i2···ir
j1j2···jsT k1k2···kt
l1l2···lu
form the components of another tensor but of type (r + t, s + u) (see Sec-
tion 2.4). This operation of tensor multiplication corresponds to the natural
bilinear transformation
V ⊗r ⊗V ∗⊗s × V ⊗t ⊗V ∗⊗u −→V ⊗(r+t) ⊗V ∗⊗(s+u).
Therefore, the tensor multiplication utilizes the isomorphism
(V ⊗r ⊗V ∗⊗s) ⊗(V ⊗t ⊗V ∗⊗u) ∼= V ⊗r+t ⊗V ∗⊗s+u.

388
C. Multilinear Algebra
Consequently, one typically writes S ⊗T for the multiplication (product)
of two tensors.
(One can describe the operation of tensor multiplication in terms of
an operation in a so-called tensor algebra, but the theory behind tensor
algebras takes us a little too far aﬁeld for this text.)
Finally, the contraction operation on the components of a tensor cor-
responds to setting one contravariant and one covariant index to be the
same and then summing over that index. On the indices involved, this
corresponds to the following linear transformation:
V ⊗V ∗−→K
⃗v ⊗λ −→λ(⃗v).
The contraction operation is similar to the operation of taking the trace of
a matrix along certain speciﬁed indices.
Problems
C.4.1. Let V and W be ﬁnite dimensional vector spaces over a ﬁeld K with
respective bases B = {⃗e,1 , . . . ,⃗en} and B′ = {⃗f1, . . . , ⃗fm}. Let T : V →W
be a linear transformation with matrix A with respect to the bases B and
B′. T determines a linear transformation T ⊗2 : V ⊗V →W ⊗W via
T ⊗2(⃗v1 ⊗⃗v2) = T(⃗v1) ⊗T(⃗v2)
and completed for other elements of V ⊗V by linearity.
(a) If V = W = R2 and the matrix of a linear transformation T with
respect to the standard basis is
A =
2
3
5
7
	
,
ﬁnd the matrix of T ⊗2.
(b) In general, for any ﬁnite dimensional vector spaces V and W and
linear transformation T, if the coeﬃcients of A are (aij), ﬁnd the
coeﬃcients of the matrix for T ⊗2.
C.4.2. Let V be a vector space over C, and let T : V →V be a linear transfor-
mation.
(a) Suppose that the Jordan canonical form of T is J = λI. Find the
Jordan canonical form of T ⊗2.

C.4. The Tensor Product
389
(b) Suppose that the Jordan canonical form of T is
J =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
λ
1
0
· · ·
0
0
0
λ
1
· · ·
0
0
0
0
λ
· · ·
0
0
...
...
...
...
...
...
0
0
0
· · ·
λ
1
0
0
0
· · ·
0
λ
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
Find the Jordan canonical form of T ⊗2.
C.4.3. Let V be a vector space over a ﬁeld K. Let ⃗v1,⃗v2 ∈V . Show that in
V ⊗V , ⃗v1 ⊗⃗v2 = ⃗v2 ⊗⃗v1 if and only if ⃗v1 and ⃗v2 are collinear.
C.4.4. Let V and W be vector spaces over C, and let S : V →V and T : W →
W be linear transformations. Consider the linear transformation S ⊗T
deﬁned by
S ⊗T : V ⊗W −→V ⊗W
⃗v ⊗⃗w −→S(⃗v) ⊗T(⃗w).
(a) Suppose that dim V = 2 and that dim W = 3, with bases {⃗e1,⃗e2}
and {⃗f1, ⃗f2, ⃗f3}, respectively. Suppose also that with respect to these
bases, the matrices for S and T are
1
3
5
2
	
and
⎛
⎝
−1
0
2
1
3
−2
0
1
4
⎞
⎠.
Find the matrix for S ⊗T with respect to the standard basis for
V ⊗W .
(b) Suppose that S and T are diagonalizable with eigenvalues λ1, . . . , λm
and μ1, . . . , μn, respectively. Prove that S ⊗T is diagonalizable and
that the eigenvalues of S ⊗T are λiμj for 1 ≤i ≤m and 1 ≤j ≤n.
C.4.5. Let V , W1, and W2 be ﬁnite dimensional vector spaces over a ﬁeld K.
Show that there exists a canonical (independent of a given basis) isomor-
phism
V ⊗(W1 ⊕W2) ∼= (V ⊗W1) ⊕(V ⊗W2).
C.4.6. Prove Theorem C.4.8.
C.4.7. Let V and W be vector spaces over R with, respectively, bases {⃗e1, . . . ,⃗em}
and {⃗f1, . . . , ⃗fn}. Call {⃗e∗
1, . . . ,⃗e∗
m} the standard associated cobasis for
V ∗. Proposition C.4.7 describes a canonical isomorphism ϕ : V ∗⊗W ∼=
Hom(V, W ).
(a) Show that for all pure tensors ω ⊗⃗w ∈V ∗⊗W , the corresponding
linear transformation T = ϕ(ω ⊗⃗w) is of rank 1.

390
C. Multilinear Algebra
(b) Conversely show that if T ∈Hom(V, W ) has rank 1, then T =
ϕ(ω ⊗⃗w) for some pure tensor ω ⊗⃗w.
(c) Let α ∈V ∗⊗W be a tensor. Prove that if T(α) is a linear trans-
formation of rank k, then in any expression of α as a sum of pure
tensors, one must use at least k terms.
C.5
Symmetric Product and Alternating Product
We now deﬁne two more useful constructions on a ﬁnite dimensional vector
space V over R or C. We will occasionally omit the proofs of the proposi-
tions and leave them as exercises for the reader.
C.5.1
The Symmetric Product
In the tensor product V ⊗V , in general ⃗v1 ⊗⃗v2 ̸= ⃗v2 ⊗⃗v2. It is some-
times useful to have a tensor-like product that is either commutative or
anticommutative.
For example, in the previous section, we pointed out that every bilinear
form on V is an element of V ∗⊗V ∗. However, in geometry and other
areas, one encounters symmetric bilinear forms. If Aij are the components
of an element in A ∈V ∗⊗V ∗with respect to a given basis on V , then the
condition that A be a symmetric bilinear form means that Aij = Aji for all
1 ≤i, j ≤dim V . The set of symmetric bilinear forms is a linear subspace
of V ∗⊗2. The kth symmetric product of a vector space V generalizes this
idea.
Let V be a vector space of dimension n. Let Sk be the set of permuta-
tions on k elements (i.e., bijections on {1, 2, . . ., k}). This set acts on
k times
*
+(
)
V ⊗V ⊗· · · ⊗V
by doing the following on pure tensors:
σ · (⃗v1 ⊗⃗v2 ⊗· · · ⊗⃗vk) = ⃗vσ−1(1) ⊗⃗vσ−1(2) ⊗· · · ⊗⃗vσ−1(k)
(C.14)
and extending by linearity on nonpure tensors. (Taking σ−1 on the indices
means that σ sends the vector in the ith position in the tensor product
⃗v1 ⊗⃗v2 ⊗· · · ⊗⃗vk to the σ(i)th position.)

C.5. Symmetric Product and Alternating Product
391
Deﬁnition C.5.1. Let α ∈V ⊗k be a tensor of rank k over V . We deﬁne the
symmetrization of α to be
Sα =

σ∈Sk
σ · α.
Example C.5.2. Let V be a vector space. We consider tensors in V ⊗V ⊗V .
We will consider permutations in S3, which has 3! = 6 elements.
S(⃗e1 ⊗⃗e2 ⊗⃗e3) = ⃗e1 ⊗⃗e2 ⊗⃗e3 + ⃗e2 ⊗⃗e1 ⊗⃗e3 + ⃗e3 ⊗⃗e2 ⊗⃗e1
+ ⃗e1 ⊗⃗e3 ⊗⃗e2 + ⃗e2 ⊗⃗e3 ⊗⃗e1 + ⃗e3 ⊗⃗e1 ⊗⃗e2.
In contrast,
S(⃗e1 ⊗⃗e1 ⊗⃗e2) = ⃗e1 ⊗⃗e1 ⊗⃗e2 + ⃗e1 ⊗⃗e1 ⊗⃗e2 + ⃗e2 ⊗⃗e1 ⊗⃗e1
+ ⃗e1 ⊗⃗e2 ⊗⃗e1 + ⃗e1 ⊗⃗e1 ⊗⃗e1 + ⃗e2 ⊗⃗e1 ⊗⃗e1
= 2 (⃗e1 ⊗⃗e1 ⊗⃗e2 + ⃗e1 ⊗⃗e2 ⊗⃗e1 + ⃗e2 ⊗⃗e1 ⊗⃗e1) .
By construction, the symmetrization S deﬁnes a linear transformation
S : V ⊗k →V ⊗k.
Deﬁnition C.5.3. The subspace of V ⊗k given as the image of S : V ⊗k →V ⊗k
is called the kth symmetric product of V and is denoted by Symk V .
Proposition C.5.4. Let {⃗e1,⃗e2, . . . ,⃗en} be a basis of V . Then in Symk V , the
elements of S(⃗ei1 ⊗⃗ei2 ⊗· · · ⊗⃗eik) are all distinct for 1 ≤i1 ≤i2 ≤· · · ≤
ik ≤n. Furthermore, the corresponding vectors S(⃗ei1 ⊗⃗ei2 ⊗· · ·⊗⃗eik) form
a basis of Symk V .
Corollary C.5.5. Let V be a vector space of dimension n. Then
dim Symk V =
n + k −1
n
	
.
Proposition C.5.6. The subspace Symk V is invariant under the action of Sk
on V ⊗k.
Proof: Let τ ∈Sk be a permutation. Then on any pure tensor ⃗vi1 ⊗⃗vi2 ⊗
· · · ⊗⃗vik, the action of τ on S(⃗vi1 ⊗⃗vi2 ⊗· · · ⊗⃗vik) gives
τ · S(⃗vi1 ⊗⃗vi2 ⊗· · · ⊗⃗vik) = τ ·
 
σ∈Sk
σ · ⃗vi1 ⊗⃗vi2 ⊗· · · ⊗⃗vik

=

σ∈Sk
τ · (σ · ⃗vi1 ⊗⃗vi2 ⊗· · · ⊗⃗vik)

392
C. Multilinear Algebra
=

σ∈Sk
(τσ) · ⃗vi1 ⊗⃗vi2 ⊗· · · ⊗⃗vik
=

σ∈Sk
σ · ⃗vi1 ⊗⃗vi2 ⊗· · · ⊗⃗vik
= S(⃗vi1 ⊗⃗vi2 ⊗· · · ⊗⃗vik),
where we obtain the second-to-last line because as σ runs through all the
permutations in Sk, for any ﬁxed τ ∈Sk, the compositions τσ also run
through all the permutations of Sk.
□
Corollary C.5.7. For all symmetric tensors α∈Symk V , we have S(α)=k! α.
Proof: The corollary follows immediately from Proposition C.5.6 and Def-
inition C.5.1.
□
Given α ∈Symk V and β ∈Syml V , the tensor product α⊗β is of course
an element of V ⊗(k+l) but is not necessarily an element of Symk+l V . How-
ever, it is possible to construct a new product that satisﬁes this deﬁciency.
Deﬁnition C.5.8. Let α ∈Symk V and β ∈Syml V . Deﬁne the symmetric
product between α and β as
αβ =
1
k! l!S(α ⊗β).
Note that if α and β are tensors of rank 1, then the product αβ is
precisely the symmetrization of α ⊗β. However, a few other properties,
which we summarize in Proposition C.5.10, of this symmetric product also
hold. We need a lemma ﬁrst.
Lemma C.5.9. Let α be a tensor of rank k. If S(α) = 0, then S(α ⊗β) =
S(β ⊗α) = 0 for all tensors β.
Furthermore, if S(α) = S(α′), then
S(α ⊗β) = S(α′ ⊗β) for all tensors β.
Proof: We ﬁrst prove that if S(α) = 0, then S(α ⊗β) = 0 for all tensors β
of rank l, and the result for S(β ⊗α) follows similarly.
Let Sk be the subset of permutations in Sk+l that only permute the ﬁrst
k elements of {1, 2, . . ., k+l} and leave the remaining l elements unchanged.
Deﬁne the relation ∼on Sk+l as τ1 ∼τ2 if and only if τ−1
2 τ1 ∈Sk. Since
Sk is closed under taking inverse functions and composition of functions,
it is easy to see that ∼is an equivalence relation on Sk+l.

C.5. Symmetric Product and Alternating Product
393
Let C be a set of representatives of distinct equivalence classes of ∼.
Then we have
S(α ⊗β) =

σ∈Sk+l
σ · (α ⊗β) =

τ∈C

σ′∈Sk
τσ′ · (α ⊗β)
=

τ∈C
τ ·
 
σ′∈Sk
σ′ · α

⊗β

=

τ∈C
τ · ((Sα) ⊗β)
= 0.
For the second part of the lemma, suppose that S(α) = S(α′). Then
S(α −α′) = 0. Thus, for all tensors β we have S((α −α′) ⊗β) = 0. Hence,
S(α ⊗β) −S(α′ ⊗β) = 0 and the result follows.
□
Proposition C.5.10. Let V be a vector space of dimension n. The following
hold:
1. The symmetric product is bilinear: for all α, α1, α2 ∈Symk V , β, β1,
β2 ∈Syml V , and λ in the base ﬁeld,
(α1 + α2)β = α1β + α2β,
(λα)β = λ(αβ),
α(β1 + β2) = αβ1 + αβ2,
α(λβ) = λ(αβ).
2. The symmetric product is commutative: for all α ∈Symk V and
β ∈Syml V ,
αβ = βα.
3. The symmetric product is associative:
for all α ∈Symr V , β ∈
Syms V , and γ ∈Symt V , as an element of Symr+s+t V , we have
(αβ)γ = α(βγ) =
1
r! s! t!S(α ⊗β ⊗γ).
Proof: We leave part 1 of the proposition as an exercise for the reader.
For part 2, by Proposition C.5.6, S(α ⊗β) is invariant under the ac-
tion of Sk+l. Consider the permutation σ0 ∈Sk+l that maps the n-tuple
(1, 2, . . . , k + l) to (k + 1, . . . , k + l, 1, . . ., k). In each pure tensor in an
expression of α ⊗β, the action σ0(α ⊗β) moves (and keeps in the proper
order) the vector terms coming from β in front of the terms coming from
α. Hence, we see that σ0(α ⊗β) = β ⊗α. Thus, we conclude that
βα = σ0(αβ) = σ0
 1
k! l!S(α ⊗β)
	
=
1
k! l!S(α ⊗β) = αβ.
Thus, the symmetric product is commutative.

394
C. Multilinear Algebra
For part 3, by Corollary C.5.7, since αβ is symmetric,
S(αβ) = (r + s)! αβ = (r + s)!
r! s!
S(α ⊗β).
Therefore, by Lemma C.5.9, for all tensors γ of rank t,
S(αβ ⊗γ) = S
(r + s)!
r! s!
(α ⊗β) ⊗γ
	
.
Consequently,
(αβ)γ =
1
(r + s)! t!S(αβ ⊗γ)
=
1
(r + s)! t!
(r + s)!
r! s!
S ((α ⊗β) ⊗γ)
=
1
r! s! t!S(α ⊗β ⊗γ).
It is easy to follow the same calculation and ﬁnd that
α(βγ) =
1
r! s! t!S(α ⊗β ⊗γ),
which shows that (αβ)γ = α(βγ) for all tensors α, β, and γ.
□
By virtue of associativity, the symmetrization of a pure tensor S(⃗v1 ⊗
⃗v2 ⊗· · · ⊗⃗vk) is in fact
⃗v1⃗v2 · · ·⃗vk.
We think of this element as a commutative “product” between vectors,
which is linear in each term.
With this notation in mind, one usually
thinks of Symk V as a vector space in its own right, independent of V ⊗k,
with basis
{⃗ei1⃗ei2 · · ·⃗eik | 1 ≤i1 ≤i2 ≤· · · ≤ik ≤n}.
Furthermore, analogous to polynomials in multiple variables where the
monomial xyx2z3y = x3y2z3, any symmetric product vector ⃗ei1⃗ei2 · · ·⃗eik is
equal to another expression on which the particular vectors in the product
are permuted.
C.5.2
The Alternating Product
We turn now to the alternating product, also called the wedge product.
Many of the results for the alternating product parallel the symmetric
product.

C.5. Symmetric Product and Alternating Product
395
Let V be a vector space of dimension n and let us continue to consider
the action of Sk on V ⊗k as described in Equation (C.14). Recall the sign
of a permutation described in Deﬁnition 2.4.12.
Deﬁnition C.5.11. Let α ∈V ⊗k be a tensor. We deﬁne the alternation of α
to be
Aα =

σ∈Sk
sign(σ)(σ · α).
Example C.5.12. Let V be a vector space. We consider tensors in V ⊗V ⊗V .
We will consider permutations in S3, which has 3! = 6 elements.
The
identity permutation has a sign of 1, permutations that interchange only
two elements have a sign of −1, and the permutations that cycle through
the three indices have a sign of 1.
A(⃗e1 ⊗⃗e2 ⊗⃗e3) = ⃗e1 ⊗⃗e2 ⊗⃗e3 −⃗e2 ⊗⃗e1 ⊗⃗e3 −⃗e3 ⊗⃗e2 ⊗⃗e1
−⃗e1 ⊗⃗e3 ⊗⃗e2 + ⃗e2 ⊗⃗e3 ⊗⃗e1 + ⃗e3 ⊗⃗e1 ⊗⃗e2
In contrast,
A(⃗e1 ⊗⃗e1 ⊗⃗e2) = ⃗e1 ⊗⃗e1 ⊗⃗e2 −⃗e1 ⊗⃗e1 ⊗⃗e2 −⃗e2 ⊗⃗e1 ⊗⃗e1
−⃗e1 ⊗⃗e2 ⊗⃗e1 + ⃗e1 ⊗⃗e1 ⊗⃗e1 + ⃗e2 ⊗⃗e1 ⊗⃗e1
= 0
Proposition C.5.13. Let ⃗v1 ⊗⃗v2 ⊗· · ·⊗⃗vk be a pure tensor in V ⊗k. If ⃗vi = ⃗vj
for some pair (i, j), where i ̸= j, then
A(⃗v1 ⊗⃗v2 ⊗· · · ⊗⃗vk) = 0.
Proof: Suppose that in the pure tensor ⃗v1 ⊗⃗v2 ⊗· · · ⊗⃗vk, we have ⃗vi = ⃗vj
for some pair i ̸= j. Let f ∈Sk be the permutation that interchanges the
ith and jth entry and leaves all others ﬁxed. Deﬁne the relation ∼on Sk
by σ ∼τ if and only if τ−1σ ∈{1, f}. Note that f 2 = f ◦f = 1 is the
identity permutation, and hence, f = f −1. Because of these properties of
f, one easily checks that the relation ∼is an equivalence relation on Sk.
Let C be a set of representatives for all of the equivalence classes of ∼.
Then if we set α = ⃗v1 ⊗⃗v2 ⊗· · · ⊗⃗vk, we have
A(⃗v1 ⊗⃗v2 ⊗· · · ⊗⃗vk) =

σ∈C
(sign(σ)(σ · α) + sign(σf)((σf) · α))
=

σ∈C
(sign(σ)(σ · α) −sign(σ)(σ · (f · α)))

396
C. Multilinear Algebra
=

σ∈C
(sign(σ)(σ · α) −sign(σ)(σ · α))
= 0
because ⃗vi = ⃗vj, so f · α = α.
□
By construction, the alternation A deﬁnes a linear transformation A :
V ⊗k →V ⊗k.
Deﬁnition C.5.14. The subspace of V ⊗k given as the image of A : V ⊗k →
V ⊗k is called the kth alternating product or the kth wedge product of V
and is denoted by k V .
Proposition C.5.15. Let {⃗e1,⃗e2, . . . ,⃗en} be a basis of V . Then in k V , the
elements A(⃗ei1⊗⃗ei2⊗· · ·⊗⃗eik) are all distinct and nonzero for 1 ≤i1 < i2 <
· · · < ik ≤n. Furthermore, the corresponding vectors A(⃗ei1 ⊗⃗ei2 ⊗· · ·⊗⃗eik)
form a basis of k V .
Corollary C.5.16. Let V be a vector space of dimension n. Then
dim
k
V =
n
k

.
Proposition C.5.17. The subspace k V is skew-invariant under the action
of Sk on V ⊗k, i.e., for all σ ∈Sk and for all tensors α ∈k V , we have
σ · α = sign(σ)α.
Proof: (Left as an exercise for the reader.)
□
Corollary C.5.18. For all alternating tensors α ∈k V , we have A(α) = k! α.
Proof: By Proposition C.5.17,
A(α) =

σ∈Sk
sign(σ)σ · α =

σ∈Sk
sign(σ)2α = k! α.
□
As in the case of the symmetric product, it is not hard to see that
the tensor product of alternating tensors is, in general, not another al-
ternating tensor.
However, it is possible to deﬁne a product between
alternating tensors that produces another alternating tensor.
Deﬁnition C.5.19. Let V be a vector space, and let α ∈k V and β ∈l V .
We deﬁne
α ∧β =
1
k! l!A(α ⊗β)
so that α ∧β ∈k+l V . We call this operation α ∧β the exterior product
or the wedge product of α and β.

C.5. Symmetric Product and Alternating Product
397
Similar properties hold for the exterior product as for the symmetric
product.
Proposition C.5.20. Let V be a vector space of dimension n. The following
hold:
1. The exterior product is bilinear: for all α, α1, α2 ∈1k V , β, β1, β2 ∈
1l V , and λ in the base ﬁeld,
(α1 + α2) ∧β = α1 ∧β + α2 ∧β,
(λα) ∧β = λ(α ∧β),
α ∧(β1 + β2) = α ∧β1 + α ∧β2,
α ∧(λβ) = λ(α ∧β).
2. The exterior product is anticommutative in the sense that for all α ∈
1k V and β ∈1l V ,
β ∧α = (−1)klα ∧β.
3. The exterior product is associative: for all α ∈1r V , β ∈1s V , and
γ ∈1t V , as an element of 1r+s+t V , we have
(α ∧β) ∧γ = α ∧(β ∧γ) =
1
r! s! t!A(α ⊗β ⊗γ).
Proof: Again we leave part 1 as an exercise for the reader.
For part 2, by Proposition C.5.17, A(α ⊗β) is skew-invariant under
the action of Sk+l. As in the proof of Proposition C.5.10, consider the
permutation σ0 ∈Sk+l that maps the n-tuple (1, 2, . . . , k + l) to (k +
1, . . . , k + l, 1, . . . , k). In each pure tensor in an expression of α ⊗β, the
action σ0 · (α ⊗β) moves (and keeps in the proper order) the vector terms
coming from β in front of the terms coming from α. Hence we see that
σ0 · (α ⊗β) = β ⊗α. Also, it is not diﬃcult to see how σ0 can be expressed
using kl transpositions (permutations that interchange only two elements),
and therefore, sign(σ0) = (−1)kl. Thus, we conclude that
β ∧α =
1
k! l!A(β ⊗α)
=
1
k! l!A(σ0 · (α ⊗β))
= sign(σ0) 1
k! l!A(α ⊗β)
= (−1)klα ∧β.
Part 3, follows in a similar manner to the proof of Proposition C.5.10
with appropriate modiﬁcations, including an adaptation of Lemma C.5.9
and using Corollary C.5.18.
□

398
C. Multilinear Algebra
By virtue of the associativity of the exterior product, the alternation of
a pure tensor A(⃗v1 ⊗⃗v2 ⊗· · · ⊗⃗vk) is denoted by
⃗v1 ∧⃗v2 ∧· · · ∧⃗vk,
where we often think of this element as an anticommutative “product”
between vectors. This means that
⃗v1 ∧⃗v2 ∧· · · ∧⃗vk
interchange i,j
= −⃗v1 ∧⃗v2 ∧· · · ∧⃗vk
and also that for all σ ∈Sk,
σ · (⃗v1 ∧⃗v2 ∧· · · ∧⃗vk) = sign(σ)⃗v1 ∧⃗v2 ∧· · · ∧⃗vk.
(C.15)
With this notation in mind, one often thinks of 1k V as a vector space in
its own right, independent of V ⊗k, with basis
{⃗ei1 ∧⃗ei2 ∧· · · ∧⃗eik | 1 ≤i1 < i2 < · · · < ik ≤n}.
Example C.5.21. Let V = R3, and let
⃗v =
⎛
⎝
1
−1
2
⎞
⎠
and
⃗w =
⎛
⎝
3
0
2
⎞
⎠
be vectors in V . Recall that dim 12 V = 3. With respect to the standard
basis in 12 V , we have
⃗v ∧⃗w = (⃗e1 −⃗e2 + 2⃗e3) ∧(3⃗e1 + 2⃗e3)
= 3⃗e1 ∧⃗e1 + 2⃗e1 ∧⃗e3 −3⃗e2 ∧⃗e1 −2⃗e2 ∧⃗e3 + 6⃗e3 ∧⃗e1 + 4⃗e3 ∧⃗e3
= 2⃗e1 ∧⃗e3 + 3⃗e1 ∧⃗e2 −2⃗e2 ∧⃗e3 −6⃗e1 ∧⃗e3
= −4⃗e1 ∧⃗e3 + 3⃗e1 ∧⃗e2 −2⃗e2 ∧⃗e3.
For the symmetric product, recall that dim Sym2 V = 6. With respect to
the standard basis in Sym2 V , we have
⃗v ⃗w = (⃗e1 −⃗e2 + 2⃗e3)(3⃗e1 + 2⃗e3)
= 3⃗e1⃗e1 + 2⃗e1⃗e3 −3⃗e2⃗e1 −2⃗e2⃗e3 + 6⃗e3⃗e1 + 4⃗e3⃗e3
= 3⃗e2
1 + 4⃗e2
3 −3⃗e1⃗e2 + 8⃗e1⃗e3 −2⃗e2⃗e3.
Proposition C.5.22. Let V be an n-dimensional vector space over a ﬁeld. Let
⃗vi for i = 1, . . . , m be m vectors in V where m < n. Let ⃗wj for j = 1, . . . , m
be another set of vectors, with ⃗wj ∈Span(⃗vi) given by ⃗wj = "
i cji⃗vi. Then
⃗w1 ∧⃗w2 ∧· · · ∧⃗wn = (det cji)⃗v1 ∧⃗v2 ∧· · · ∧⃗vn.

C.5. Symmetric Product and Alternating Product
399
Proof: This is a simple matter of calculation, as follows:
⃗w1∧⃗w2∧· · ·∧⃗wm =
 m

i1=1
c1i1⃗vi1

∧
 m

i2=1
c2i2⃗vi2

∧· · ·∧

m

im=1
cmim⃗vim

.
(C.16)
In any wedge product, if there is a repeated vector, the wedge product
is 0. Therefore, when distributing out the m summations, the only nonzero
terms are those in which all the i1, i2, . . . , im are distinct. Furthermore, by
Equation (C.15), any nonzero term can be rewritten as
⃗vi1 ∧⃗vi2 ∧· · · ∧⃗vim = sign(σ)⃗v1 ∧⃗v2 ∧· · · ∧⃗vm,
where σ is the permutation given as a table by
σ =
 1
2
· · ·
m
i1
i2
· · ·
im
	
.
Furthermore, by selecting which integer is chosen for each ik in each term
on the right side of Equation (C.16), we see that every possible permutation
is used exactly once. Thus, we have
⃗w1∧⃗w2∧· · ·∧⃗wm =
⎛
⎝
σ∈Sm
sign(σ)c1σ−1(1)c2σ−1(2) · · · cmσ−1(m)
⎞
⎠⃗v1∧⃗v2∧· · ·∧⃗vm.
The content of the parentheses in the above equation is precisely the de-
terminant of the matrix (cij) and the proposition follows.
□
Example C.5.23. Let V = Rn with standard basis ⃗ei, where i = 1, 2, . . ., n.
By Proposition C.5.22, we have
⃗v1 ∧⃗v2 ∧· · · ∧⃗vn = det
⎛
⎝
|
|
|
⃗v1
⃗v2
· · ·
⃗vn
|
|
|
⎞
⎠⃗e1 ∧⃗e2 ∧· · · ∧⃗en.
By a standard result of linear algebra, the determinant det
⃗v1
⃗v2
· · ·
⃗vn

is the volume of the parallelepiped spanned by {⃗v1,⃗v2, · · · ,⃗vn}.
Furthermore, if we consider the element ⃗e∗
1 ∧· · · ∧⃗e∗
n ∈1n V ∗as an
alternating multilinear function on V , we have
⃗e∗
1 ∧· · · ∧⃗e∗
n(⃗v1, . . . ,⃗vn) =
 
σ∈Sk
sign(σ)σ · (⃗e∗
1 ⊗· · · ⊗⃗e∗
n)

(⃗v1, . . . ,⃗vn)
= det
⎛
⎝
|
|
|
⃗v1
⃗v2
· · ·
⃗vn
|
|
|
⎞
⎠.
Therefore, the element ⃗e∗
1 ∧· · · ∧⃗e∗
n is often called the volume form on V .

400
C. Multilinear Algebra
Problems
C.5.1. Let V = R3, and consider the linear transformation T : V →V given by
T(⃗v) =
⎛
⎝
1
2
3
4
5
6
7
8
9
⎞
⎠⃗v
with respect to the standard basis of R3.
(a) Prove that the function S : 12 V →12 V that satisﬁes
S(⃗v1 ∧⃗v2) = T(⃗v1) ∧T (⃗v2)
extends to a linear transformation.
(b) Determine the matrix of S with respect to the associated basis {⃗e1 ∧
⃗e2, ⃗e2 ∧⃗e3,⃗e1 ∧⃗e3}.
C.5.2. Repeat the above exercise but with Sym2 V and changing the question
accordingly.
C.5.3. Prove Proposition C.5.17.
C.5.4. Prove part 1 of Proposition C.5.10.
C.5.5. Let V be a vector space over C of dimension n, and let T be a linear
transformation T : V →V with eigenvalues λi, where 1 ≤i ≤n. Let
S : 12 V →12 V be deﬁned by S(⃗v1 ∧⃗v2) = T(⃗v1) ∧T(⃗v2).
(a) Prove that the eigenvalues of S are λiλj for 1 ≤i < j ≤n.
(b) Prove that det S = (det T)n−1.
(c) Prove that the trace of S is
1
2
 n

i=1
λi
2
−
n

i=1
λ2
i

.
C.5.6. Let V be a vector space over C of dimension n, and let T be a linear
transformation T : V →V with eigenvalues λi, where 1 ≤i ≤n. Let
S : Sym2 V →Sym2 V be deﬁned by S(⃗v1⃗v2) = T (⃗v1)T(⃗v2).
(a) Prove that the eigenvalues of S are λiλj for 1 ≤i ≤j ≤n.
(b) Prove that det S = (det T)n.
(c) Prove that the trace of S is
1
2
 n

i=1
λi
2
+
n

i=1
λ2
i

.

C.6. The Wedge Product and Analytic Geometry
401
C.6
The Wedge Product and Analytic Geometry
C.6.1
Binet-Cauchy and k-Volume of Parallelepipeds
The article [29] develops the connection between the wedge product of
vectors in Rn and analytic geometry.
Most important for applications
to diﬀerential geometry is a formula for the volume of a k-dimensional
parallelepiped in Rn. The authors of [29] give the following deﬁnition.
Deﬁnition C.6.1.
The dot product of two pure antisymmetric tensors in
1k Rn is
(⃗a1 ∧⃗a2 ∧· · · ∧⃗ak) · (⃗b1 ∧⃗b2 ∧· · · ∧⃗bk) =

⃗a1 ·⃗b1
⃗a1 ·⃗b2
· · ·
⃗a1 ·⃗bk
⃗a2 ·⃗b1
⃗a2 ·⃗b2
· · ·
⃗a2 ·⃗bk
...
...
...
...
⃗ak ·⃗b1
⃗ak ·⃗b2
· · ·
⃗ak ·⃗bk

.
It turns out that this deﬁnition is equivalent to the usual dot product
on 1k Rn with respect to its standard basis, namely,
{⃗ei1 ∧⃗ei2 ∧· · · ∧⃗eik},
with 1 ≤i1 < i2 < · · · < ik ≤n.
The equivalence of these two deﬁnitions is a result of the following combi-
natorial proposition.
Proposition C.6.2 (Binet-Cauchy). Let A and B be two n × m matrices, with
m ≤n. Call I(m, n) the set of subsets of {1, 2, . . ., n} of size m and for
any S ∈I(m, n) ,denote AS as the m×m submatrix consisting of the rows
of A indexed by S (and similarly for B). Then
det(BT A) =

S∈I(m,n)
(det(BS)) (det(AS)) .
Proof: Let A = (aij) and B = (bij), with 1 ≤i ≤n and 1 ≤j ≤m. The
matrix BT A is an m × m-matrix with entries
n

j=1
bjiajk,
indexed by 1 ≤i, k ≤m. Therefore, the determinant of BT A is
det(BT A) =

σ∈Sm
sign(σ)
⎛
⎝
n

j1=1
bj11aj1σ(1)
⎞
⎠
⎛
⎝
n

j2=1
bj22aj2σ(2)
⎞
⎠· · ·
⎛
⎝
n

jm=1
bjmmajmσ(m)
⎞
⎠,

402
C. Multilinear Algebra
where Sm is the set of permutations on the set {1, 2, . . ., m}. Then, after
rearranging the order of summation, we have
det(BT A)
=

σ∈Sm
n

j1=1
n

j2=1
· · ·
n

jm=1
sign(σ)bj11bj22 · · · bjmmaj1σ(1)aj2σ(2) · · · ajmσ(m)
=
n

j1=1
n

j2=1
· · ·
n

jm=1
bj11bj22 · · · bjmm
 
σ∈Sm
sign(σ)aj1σ(1)aj2σ(2) · · · ajmσ(m)

.
Because of the sign of the permutation, any term in the summation where
not all the jl are distinct is equal to 0. Therefore, we only need to consider
the summation over sets of indices j = (j1, j2, . . . , jm) ∈{1, . . . , m}n, where
all of the indices are distinct. We can parametrize this set in an alternative
manner as follows. Let I(m, n) be the set of indices in increasing order,
i.e.,
I(m, n) = {(j1, j2, . . . , jm) ∈{1, . . . , n}m | 1 ≤j1 < j2 < · · · < jm ≤n} .
(C.17)
The set I(m, n) × Sm is in bijection with the set of all m-tuples of indices
that are distinct via
(j, σ) →(jσ(1), . . . , jσ(m)).
We can now write
det(BT A) =

j∈I(m,n)

τ∈Sm
bjτ(1)1bjτ(2)2 · · · bjτ(m)m
 
σ∈Sm
sign(σ)ajτ(1)σ(1)ajτ(2)σ(2) · · · ajτ(m)σ(m)

=

j∈I(m,n)

τ∈Sm
bjτ(1)1bjτ(2)2 · · · bjτ(m)m sign(τ)
 
σ′∈Sm
sign(σ′)aj1σ′(1)aj2σ′(2) · · · ajmσ′(m)

=

j∈I(m,n)

τ∈Sm
sign(τ)bjτ(1)1bjτ(2)2 · · · bjτ(m)m det Aj.
where Aj is the m × m submatrix obtained from A by using only the rows
given in the m-tuple index j. Then we conclude that
det(BT A) =

j∈I(m,n)

τ∈Sm
sign(τ)bj1τ −1(1)bj2τ −1(2) · · · bjmτ −1(m) det Aj
=

j∈I(m,n)

det BT
j

(det Aj) ,

C.6. The Wedge Product and Analytic Geometry
403
and the proposition follows since det(CT ) = det(C) for any square ma-
trix C.
□
Corollary C.6.3. Deﬁnition C.6.1 is equivalent to the dot product on 1k Rn
with respect to the standard basis.
Proof: If ⃗a1,⃗a2, . . . ,⃗ak is a k-tuple of vectors in Rn, call A the n×k-matrix
that has the vector ⃗ai as the ith column. Deﬁne P(n, k) as in Proposition
C.6.2. For any subset S of {1, 2, . . ., n} of cardinality k, deﬁne
⃗eS = ⃗es1 ∧⃗es2 ∧· · · ∧⃗esk,
where S = {s1, s2, . . . , sk}, with the elements listed in increasing order. It
is not hard to check that
a1 ∧⃗a2 ∧· · · ∧⃗ak =

S∈P (n,k)
(det(AS))⃗eS.
(C.18)
The corollary follows immediately from Proposition C.6.2.
□
As with a usual Euclidean vector space Rn, we deﬁne the Euclidean
norm in the following way.
Deﬁnition C.6.4. Let a = ⃗a1 ∧⃗a2 ∧· · · ∧⃗ak ∈1k Rn. The (Euclidean) norm
of this vector is
∥⃗a1 ∧⃗a2 ∧· · · ∧⃗ak∥= √a · a.
Corollary C.6.5. The k-dimensional volume of a parallelepiped in Rn spanned
by k vectors ⃗v1,⃗v2, . . . ,⃗vk is given by
∥⃗v1 ∧⃗v2 ∧· · · ∧⃗vk∥.
Proof: It is a standard fact in linear algebra (see [13, Fact 6.3.7]) that the
k-volume of the described parallelepiped is

det(AT A), where A is the
matrix that has the vector ⃗vi as the ith column. The corollary follows from
Deﬁnitions C.6.1 and C.6.4.
□
C.6.2
The Volume Form Revisited
In Example C.5.23, we introduced the volume form on Rn in reference to the
standard basis. This is not quite satisfactory for our applications because
the standard basis has internal properties, namely that it is orthonormal
with respect to the dot product. The following proposition presents the
volume form on a vector space in its most general context. (Note that,
since the order in which we list basis vectors matters in what follows, we
think of a basis as an n-tuple of vectors as opposed to a set of vectors.)

404
C. Multilinear Algebra
Proposition C.6.6. Let V be an n-dimensional vector space with a symmetric,
positive-deﬁnite, bilinear form ⟨, ⟩. Then there exists a unique form ω ∈
1n V ∗such that ω(⃗e1, . . . ,⃗en) = 1 for all oriented bases (⃗e1, . . . ,⃗en) of V
that are orthonormal with respect to ⟨, ⟩. Furthermore, if (⃗u1, . . . , ⃗un) is
any oriented basis of V , then
ω =
√
det A ⃗u∗
1 ∧· · · ∧⃗u∗
n,
where A is the matrix with entries Aij = (⟨⃗ui, ⃗uj⟩).
Proof: (In this proof, we use superscripts for the coordinates of vectors in
order to mesh with the notation in Section 5.1, where this proposition is
explicitly used.)
Let (⃗u1, . . . , ⃗un) be any basis of V and let ⃗v = "
i ai⃗ui and ⃗w =
"
i bi⃗ui be two vectors in V along with their coordinates with respect to
(⃗u1, . . . , ⃗un). Then by the linearity of the form,
⟨⃗v, ⃗w⟩=
n

i,j=1
aibj⟨⃗ui, ⃗uj⟩= ⃗vT A⃗w.
We remark that det A ̸= 0 because otherwise there would exist some
nonzero vector ⃗v such that A⃗v = 0 and then ⟨⃗v,⃗v⟩= 0, which would
contradict the positive-deﬁnite property of the form.
The existence of an orthonormal basis with respect to ⟨, ⟩follows from
the Gram-Schmidt orthonormalization process.
If (⃗e1, . . . ,⃗en) is an or-
thonormal basis with respect to ⟨, ⟩, then the associated matrix (⟨⃗ei,⃗ej⟩)
is the identity matrix.
Given an orthonormal basis {⃗e1, . . . ,⃗en}, let {⃗e∗
1, . . . ,⃗e∗
n} by the dual
cobasis of V ∗. Set ω = ⃗e∗
1 ∧· · · ∧⃗e∗
n. Obviously, ω(⃗e1, . . . ,⃗en) = 1. Now, if
(⃗u1, . . . , ⃗un) is any other orthonormal basis of V with the same orientation
of (⃗e1, . . . ,⃗en), then det(M T M) = 1, where M is the transition matrix
from coordinates in (⃗u1, . . . , ⃗un) and coordinates in (⃗e1, . . . ,⃗en). Hence,
det(M)2 = 1, and the assumption that (⃗u1, . . . , ⃗un) has the same orienta-
tion as (⃗e1, . . . ,⃗en) means that det(M) is positive. Thus, det M = 1.
By Corollary C.3.4, the transition matrix from coordinates in (⃗e∗
1, . . . ,⃗e∗
n)
to coordinates in (⃗u∗
1, . . . , ⃗u∗
n) is M T . However, by Proposition C.5.22, we
then conclude that
ω = ⃗e∗
1 ∧· · · ∧⃗e∗
n = det(M) ⃗u∗
1 ∧· · · ∧⃗u∗
n = ⃗u∗
1 ∧· · · ∧⃗u∗
n.
Since e∗
1 ∧· · · ∧e∗
n(e1, . . . , en) = 1, then ω evaluates to 1 on all bases of V
that are orthonormal and have the same orientation as {⃗e1, . . . ,⃗en}.

C.6. The Wedge Product and Analytic Geometry
405
Suppose now that {⃗u1, . . . , ⃗un} is a basis of V but not necessarily or-
thonormal. If we write M = (mij), then by deﬁnition of the transition
matrix,
⃗ui =
n

j=1
mji⃗ej.
(Note that it is no longer necessarily true that det M = 1.) Then we can
calculate the coeﬃcients of A as
⟨⃗ui, ⃗uj⟩=
6 n

k=1
mki⃗ek,
n

l=1
mlj⃗el
7
=
n

k=1
n

l=1
mkimlj⟨⃗ei,⃗ej⟩
=
n

k=1
n

l=1
mkimljδij
=
n

k=1
n

l=1
mkimkj.
Hence, we have shown that A = M T M. We conclude that
ω = det(M) ⃗u∗
1 ∧· · · ∧⃗u∗
n =
√
det A ⃗u∗
1 ∧· · · ∧⃗u∗
n.
□
Deﬁnition C.6.7. Let V be equipped with a symmetric, positive-deﬁnite, bi-
linear form. Then the element ω ∈1n V ∗deﬁned in Proposition C.6.6 is
called the volume form of V .
C.6.3
The Hodge Star Operator
We conclude this section by introducing an operator on wedge product
spaces 1k V . In this subsection, we assume throughout that V is a ﬁnite
dimensional inner product space, i.e., that V is a vector space equipped
with a symmetric, positive-deﬁnite, bilinear form ⟨, ⟩.
The inner product ⟨, ⟩deﬁnes a natural linear transformation i : V →
V ∗via
i(⃗v)(⃗w) = i⃗v(⃗w) = ⟨⃗v, ⃗w⟩.
That ⟨, ⟩is positive deﬁnite implies that i is in fact an isomorphism. This
isomorphism i allows us to extend ⟨, ⟩in a natural way to V ∗by setting
⟨η, τ⟩= ⟨i−1(η), i−1(τ)⟩
(C.19)
for all η, τ ∈V ∗. It is easy to see that this gives an inner product on V ∗.
Proposition C.6.8. Let η1, . . . , ηk, τ1, . . . , τk ∈V ∗. Setting
⟨η1 ∧. . . ∧ηk, τ1 ∧. . . ∧τk⟩= det(⟨ηi, τj⟩)
deﬁnes a bilinear form on 1k V ∗that is symmetric and positive deﬁnite.

406
C. Multilinear Algebra
Proof: (Left as an exercise for the reader. See Problem C.6.7.)
□
Deﬁnition C.6.9. Let (V, ⟨, ⟩) be an inner product space of dimension n, and
let ω ∈1n V ∗be the volume form. The Hodge star operator is the operator
⋆: 1k V ∗→1n−k V ∗that is uniquely determined by
⟨⋆η, τ⟩ω = η ∧τ
for all τ ∈1n−k V ∗.
The Hodge star operator has the following nice properties, which we
leave as exercises.
Proposition C.6.10. Let (V, ⟨, ⟩) be an inner product space. Let B = {e1, . . . ,
en} be a basis that is orthonormal with respect to ⟨, ⟩, and let B∗=
{e∗
1, . . . , e∗
n} be the dual cobasis of V ∗.
Set ω as the volume form with
respect to ⟨, ⟩.
1. The Hodge star operator ⋆is well deﬁned and linear.
2. Viewing 1 as an element of R = 10 V , we have ⋆1 = ω.
3. For any k < n, we have ⋆(e∗
1 ∧· · · ∧e∗
k) = e∗
k+1 ∧· · · ∧e∗
n.
4. For any k-tuple (i1, . . . , ik) of increasing indices,
⋆(e∗
i1 ∧· · · ∧e∗
ik) = (sign σ) e∗
j1 ∧· · · ∧e∗
jn−k,
where the jl indices are such that {i1, . . . , ik, j1, . . . , jn−k} = {1, . . ., n}
and σ is the permutation that maps the ordered n-tuple (i1, . . . , ik,
j1, . . . , jn−k) to (1, 2, . . . , n).
The following proposition gives a formula for the coordinates of the ⋆η
in terms of the coordinates of η.
Proposition C.6.11. Let V be a vector space equipped with an inner product
⟨, ⟩. Let B = {⃗u1, . . . , ⃗un} be any basis of V , and denote by {u∗
1, . . . , u∗
n} its
cobasis in V ∗. Let A be the matrix with entries aij = (⟨ui, uj⟩), and label
aij the (i, j)th entry of the inverse A−1. If η ∈1k V ∗, with coordinates
ηi1···ik, so that
η =

1≤i1<···<ik≤n
ηi1···ik u∗
i1 ∧· · · ∧u∗
ik,

C.6. The Wedge Product and Analytic Geometry
407
then the components of ⋆η with respect to B∗are
(⋆η)j1···jn−k =
√
det A
k!
εi1···ikj1···jn−kai1h1 · · · aikhkηh1···hk,
where εh1···hn is the permutation symbol deﬁned in Equation (2.30) and
where on the right-hand side one sums from 1 to n over all indices that are
repeated, namely i1, . . . , ik and h1, . . . , hk.
Proof: By a calculation similar to the one in the proof of Proposition C.6.6
and using the deﬁnition of the inner product on 1-forms given in Equa-
tion (C.19), we determine that
⟨u∗
i , u∗
j⟩= aij,
i.e., the (i, j)th entry of the inverse A−1.
As above, denote by ω the volume form on V associated to ⟨, ⟩.
A few preliminary notations will render the rest of the proof shorter.
Recall the set I(m, n) deﬁned in Equation (C.17). For any sequence i =
(i1, . . . , ik) ∈I(k, n), we denote by u∗
i the wedge product
u∗
i = u∗
i1 ∧· · · ∧u∗
ik.
Denote also by i′ the increasing sequence of length n−k such that {i, i′} =
{1, 2, . . ., n}. We call i′ the complement of i. We deﬁne the permutation
σi ∈Sn by the permutation that maps the sequence (1, 2, . . . , n) to the
sequence (i, i′). Note that the sign of the permutation satisﬁes
sign σi = εi1···iki′
1···i′
n−k.
Consider the kth wedge product u∗
i . According to Deﬁnition C.6.9,
⟨⋆u∗
i , τ⟩ω = u∗
i ∧τ.
(C.20)
We know that {u∗
j } for j ∈I(n −k, n) forms a basis of 1n−k V ∗. Thus, we
can write
⋆u∗
i =

j∈I(n−k,n)
cju∗
j
for some constants cj. However, Equation (C.20) imposes that ⟨⋆u∗
i , u∗
j ⟩= 0
unless j = i′. We denote K = ⟨⋆u∗
i , u∗
i′⟩.
By Deﬁnition C.6.9, ⟨⋆u∗
i , u∗
i′⟩ω = u∗
i ∧u∗
i′ so by Proposition C.6.6,
⟨⋆u∗
i , u∗
i′⟩
√
det A u∗
1 ∧· · · ∧u∗
n = (sign σi) u∗
1 ∧· · · ∧u∗
n,

408
C. Multilinear Algebra
which implies that
⟨⋆u∗
i , u∗
j ⟩= (sign σi)/
√
det A δj,i′,
where δj,i′ = 1 if j = i′ and equals 0 otherwise. On the other hand,
⟨⋆u∗
i , u∗
j ⟩=

l∈I(n−k,n)
cl⟨u∗
l , u∗
j ⟩=

l∈I(n−k,n)
cl det((A−1)lj),
where by Alj we mean the minor of A consisting of the rows l = (l1, . . . , ln−k)
and columns j = (j1, . . . , jn−k). So, we conclude that

l∈I(n−k,n)
cl det((A−1)lj) = sign σi
√
det A
δj,i′.
(C.21)
To ﬁnd the values of cj for a given i, we need to invert the matrix product
in Equation (C.21), or more precisely, ﬁnd the inverse of the
n
k

×
n
k

matrix det((A−1)lj). Though a little tedious to show, the following formula
generalizes the Laplace expansion formula for determinants. For any n × n
matrix B, with notations as above,
det B =

j∈I(k,n)
(sign σi)(sign σj) det Bij det Bi′j′.
A slightly stronger result gives

j∈I(k,n)
det Bij(sign σh)(sign σj) det Bh′j′ =

det B
if h = i,
0
otherwise,
for all h ∈I(k, n). Now multiplying Equation (C.21) by (sign σh)(sign σj)∗
det(A−1)h′j′, summing the result over j ∈I(n −k, n), and taking into
account δj,i′, we obtain
det(A−1)ch = sign σi
√
det A
(sign σh)(sign σj′) det(A−1)h′i,
which implies that
ch = (sign σh)
√
det A det(A−1)h′i.
From this we deduce that
⋆(u∗
i1∧· · ·∧u∗
ik) =
n

j1=1
· · ·
n

jn=1
√
det A
(n −k)!εj1···jnaj1i1 · · · ajkik(u∗
jk+1∧· · ·∧u∗
jn),
(C.22)
and the proposition follows by linearity of the Hodge star operator.
□

C.6. The Wedge Product and Analytic Geometry
409
We point out that one can loosen the conditions on the bilinear form
⟨, ⟩and still deﬁne the Hodge star operator and obtain many of the same
results. If we only assume that ⟨, ⟩is symmetric and nondegenerate, then
all the above propositions hold except that one must replace det A with
| det A| in Proposition C.6.11.
Problems
C.6.1. Use the results of this section to calculate the surface of the parallelogram
in R3 spanned by
⃗v =
⎛
⎝
1
−3
7
⎞
⎠
and
⃗v =
⎛
⎝
4
5
−2
⎞
⎠.
C.6.2. Calculate the 3-volume of the parallelepiped in R4 spanned by
⃗a =
⎛
⎜
⎜
⎝
0,
−2
2
1
⎞
⎟
⎟
⎠,
⃗b =
⎛
⎜
⎜
⎝
3
1
−1
0
⎞
⎟
⎟
⎠,
and
⃗c =
⎛
⎜
⎜
⎝
5
1
−2
−3
⎞
⎟
⎟
⎠.
C.6.3. Using the same vectors ⃗a, ⃗b, and ⃗c in the previous exercise, determine all
vectors ⃗x such that the four-dimensional parallelepiped spanned by ⃗a, ⃗b,
⃗c, and ⃗x has dimension 0.
C.6.4. Verify the claim in Equation (C.18).
C.6.5. This exercise gives an interesting property about the derivative of deter-
minants of square matrices of functions. Let A = (aij(t)) be an n × n
matrix of functions.
(a) Use the standard formula
det A =

σ∈Sn
(sign σ)a1σ(1)a2σ(2) · · · anσ(n).
to show that
d
dt(det A) =
n

i=1
n

j=1
(−1)i+j det(Aij)daij
dt ,
where Aij is the ijth minor of A.
(b) Conclude that if A is a symmetric matrix, then
d
dt(det A) = (det A)
n

i,j=1
aij daij
dt ,
where the aij are the entries of the inverse matrix A−1.

410
C. Multilinear Algebra
C.6.6. A Higher Pythagorean Theorem. Let ⃗a, ⃗b, and ⃗c be three vectors in Rn that
are mutually perpendicular.
(a) Prove that
∥⃗a ∧⃗b + ⃗a ∧⃗c +⃗b ∧⃗c∥2 = ∥⃗a ∧⃗b∥2 + ∥⃗a ∧⃗c∥2 + ∥⃗b ∧⃗c∥2.
(b) Consider the tetrahedron spanned by ⃗a, ⃗b, and ⃗c. Let SC be the
face spanned by ⃗a and ⃗b, SB be the face spanned by ⃗a and ⃗c, SA be
the face spanned by ⃗b and ⃗c, and let SD be the fourth face of the
tetrahedron. Deduce that
S2
A + S2
B + S2
C = S2
D.
C.6.7. Prove Proposition C.6.8.
C.6.8. Prove Proposition C.6.10.
C.6.9. Let (V, ⟨, ⟩) be an inner product space. Prove that the composition ⋆◦⋆:
1k V ∗→1k V ∗is tantamount to multiplication on V ∗by (−1)k(n−k).
Suppose that ⟨, ⟩is a symmetric and nondegenerate bilinear form. Prove
that in this case ⋆◦⋆: 1k V ∗→1k V ∗is tantamount to multiplication
on V ∗by (−1)k(n−k)s, where s is the sign of the determinant of the form
matrix for the form.

Bibliography
[1] Ralph Abraham and Jerrold E. Marsden. Foundations of Mechanics. Read-
ing, MA: Benjamin-Gummings, 1978.
[2] M. A. Armstrong. Basic Topology, Undergraduate Texts in Mathematics.
New York: Springer-Verlag, 1983.
[3] Vladimir I. Arnold. Ordinary Diﬀerential Equations. Cambridge, MA: MIT
Press, 1973.
[4] Andreas Arvanitoyeorgos. An Introduction to Lie Groups and the Geometry
of Homogeneous Spaces, Student Mathematical Library 22. Providence, RI:
American Mathematical Society, 1999.
[5] Thomas F. Banchoﬀand Stephen T. Lovett. Diﬀerential Geometry of Curves
and Surfaces. Natick, MA: A K Peters, Ltd., 2010.
[6] Victor Bangert. “On the Existence of Closed Geodesics on Two-Spheres.”
International J. of Math. 4:1 (1993), 1–10.
[7] Rolf Berndt. An Introduction to Symplectic Geometry, Graduate Studies in
Mathematics 26. Providence, RI: American Mathematical Society, 2001.
[8] Arthur L. Besse. Einstein Manifolds. New York: Springer-Verlag, 2007.
[9] George David Birkhoﬀ. “Dynamical Systems with Two Degrees of Freedom.”
Trans. Am. Math. Soc. 18:2 (1917), 199–300.
[10] Roberto Bonola. Non-Euclidean Geometry: A Critical and Historical Study
of its Developments. New York: Dover Publications, 1955.
[11] Glen E. Bredon. Topology and Geometry, Graduate Texts in Mathematics
139. New York: Springer-Verlag, 1993.
[12] David M. Bressoud. A Radical Approach to Lebesgue’s Theory of Integration.
Cambridge, UK: Cambridge University Press, 2008.
[13] Otto Bretscher.
Linear Algebra with Applications, Third edition.
Upper
Saddle River, NJ: Pearson Prentice Hall, 2005.
[14] Andrew Browder. Mathematical Analysis: An Introduction, Undergraduate
Texts in Mathematics. New York: Springer-Verlag, 1996.
[15] Shiing-Shen Chern. Global Diﬀerential Geometry, MAA Studies 27. Wash-
ington, DC: Mathematical Association of America, 1989.
[16] Manfredo do Carmo. Riemannian Geometry. Boston, MA: Birkh¨auser, 1992.
411

412
Bibliography
[17] P. H. Doyle and D. A. Moran. “A Short Proof That Compact 2-manifolds
Can Be Triangulated.” Invent. Math. 5:2 (1968), 160–162.
[18] C. Henry Edwards and David E. Penny. Diﬀerential Equations: Computing
and Modeling, Third edition.
Upper Saddle River, NJ: Pearson Prentice
Hall, 2004.
[19] Albert Einstein. “Die Feldgleichungen der Gravitation.” Sitzungsberichte der
Preussischen Akademie der Wissenschaften zu Berlin 25 (1915), 844–847.
[20] Albert Einstein. Relativity: The Special and the General Theory. New York:
Crown Publishers, Inc., 1961.
[21] Grant R. Fowles. Analytical Mechanics, Fourth edition. Philadelphia: Saun-
ders College Publishing, 1986.
[22] John Franks. “Geodesics on S2 and Periodic Points of Annulus Diﬀeomor-
phisms.” Invent. Math. 108:2 (1992), 403–418.
[23] Anthony P. French.
Special Relativity.
The M.I.T. Introductory Physics
Series. New York: W. W. Norton & Company, 1968.
[24] Joseph A. Gallian.
Contemporary Abstract Algebra, Sixth. edition.
New
York: Houghton Miﬄin Company, 2006.
[25] George Gamow. My World Line: An Informal Autobiography. New York:
Viking Press, 1970.
[26] Michael C. Gemignani. Elementary Topology, Second edition. New York:
Dover Publications, 1972.
[27] John G. Hocking and Gail S. Young. Topology. New York: Dover Publica-
tions, 1961.
[28] Michel A. Kervaire and John W. Milnor. “Groups of Homotopy Spheres: I.”
Annals of Math. 77:3 (1963), 504–537.
[29] Mehrdad Khosravi and Michael D. Taylor. “The Wedge Product and Ana-
lytic Geometry.” American Mathematical Monthly 115:7 (2008), 623–644.
[30] Serge Lang. Algebra, Third edition. Reading, MA: Addison-Wesley, 1993.
[31] John M. Lee. Riemannian Manifolds: An Introduction to Curvature, Grad-
uate Texts in Mathematics 176. New York: Springer-Verlag, 1997.
[32] John M. Lee. Introduction to Smooth Manifolds, Graduate Texts in Mathe-
matics 218. New York: Springer-Verlag, 2003.
[33] Martin M. Lipshutz. Diﬀerential Geometry, Schaum’s Outline Series. New
York: McGraw-Hill, 1969.
[34] Malcolm Longair. Theoretical Concepts in Physics, Second edition. Cam-
bridge, UK: Cambridge University Press, 2003.
[35] David Lovelock and Hanno Rund. Tensors, Diﬀerential Forms and Varia-
tional Principles. New York: Dover Publications, 1989.
[36] L. Lusternik and L. Schnirelmann. “Sur le probl`eme de trois g´eod´esiques
ferm´ees sur les surfaces de Genre 0.”
C. R. Acad. Sci. S´er. I Math 189
(1929), 269–271.

Bibliography
413
[37] John McCleary. Geometry from a Diﬀerentiable Viewpoint. Cambridge, UK:
Cambridge University Press, 1994.
[38] John W. Milnor and James D. Stasheﬀ. Characteristic Classes, Annals of
Mathematics Studies 76. Princeton, NJ: Princeton University Press, 1994.
[39] Charles W. Misner, Kip S. Thorne, and John Archibald Wheeler. Gravita-
tion. San Francisco: W. H. Freeman and Company, 1973.
[40] Thomas Moore. Six Ideas that Shaped Physics: Unit R, Second edition. New
York: McGraw-Hill, 2002.
[41] James Munkres. Topology. Englewood Cliﬀs, NJ: Prentice Hall, 1975.
[42] Mikio Nakahara. Geometry, Topology and Physics, Second edition. Boca
Raton, FL: Taylor & Francis, 2003.
[43] Barrett O’Neill. Semi-Riemannian Geometry with Applications to Relativity,
Pure and Applied Mathematics 103. New York: Academic Press, 1983.
[44] Edward M. Purcell. Electricity and Magnetism, Berkeley Physics Course 2.
New York: McGraw-Hill, 1985.
[45] G. F. Bernhard Riemann. “On the Hypotheses Which Lie at the Foundation
of Geometry.” In From Kant to Hilbert: A Source Book in the Foundations
of Mathematics, Vol. 2, edited by William Bragg Ewald, pp. 651–652. New
York: Oxford University Press, 1996.
[46] Halsey L. Royden. Real Analysis. Englewood Cliﬀs, NJ: Prentice Hall, 1988.
[47] Walter Rudin. Principles of Mathematical Analysis. New York: McGraw-
Hill, 1976.
[48] Melvin Schwartz. Principles of Electromagnetism. New York: Dover Publi-
cations, 1987.
[49] Michael Spivak. A Comprehensive Guide to Diﬀerential Geometry, Volume
One. Berkeley, CA: Publish or Perish Inc., 1979.
[50] Norman Steenrod. The Topology of Fibre Bundles. Princeton, NJ: Princeton
University Press, 1951.
[51] Hans Stephani. General Relativity. Cambridge, UK: Cambridge University
Press, 1982.
[52] James Stewart. Calculus, Sixth edition. Belmont, CA: Thomson Brooks/
Cole, 2003.
[53] Keith R. Symon. Mechanics. Reading, MA: Addison-Wesley, 1971.
[54] E. R. van Kampen. “On the Argument Functions of Simple Closed Curves
and Simple Arcs.” Compositio Mathematica 4 (1937), 271–275.
[55] Robert Weinstock. Calculus of Variations. New York: McGraw-Hill, 1952.
[56] Hermann Weyl. The Concept of a Riemannian Surface, Third edition. Read-
ing, MA: Addison-Wesley, 1955. Translated from the German by Gerald R.
Maclane.
[57] Barton Zwiebach. A First Course in String Theory. Cambridge, UK: Cam-
bridge University Press, 2005.



