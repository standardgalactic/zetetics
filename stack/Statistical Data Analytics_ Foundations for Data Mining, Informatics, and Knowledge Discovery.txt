
Statistical Data Analytics


Statistical Data Analytics
Foundations for Data Mining, Informatics, and
Knowledge Discovery
Walter W. Piegorsch
University of Arizona, USA

This edition first published 2015
© 2015 John Wiley & Sons, Ltd
Registered office
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United Kingdom
For details of our global editorial offices, for customer services and for information about how to apply for permission to
reuse the copyright material in this book please see our website at www.wiley.com.
The right of the author to be identified as the author of this work has been asserted in accordance with the Copyright, Designs
and Patents Act 1988.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form
or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright,
Designs and Patents Act 1988, without the prior permission of the publisher.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in
electronic books.
Designations used by companies to distinguish their products are often claimed as trademarks. All brand names and product
names used in this book are trade names, service marks, trademarks or registered trademarks of their respective owners. The
publisher is not associated with any product or vendor mentioned in this book.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in preparing this book,
they make no representations or warranties with respect to the accuracy or completeness of the contents of this book and
specifically disclaim any implied warranties of merchantability or fitness for a particular purpose. It is sold on the under-
standing that the publisher is not engaged in rendering professional services and neither the publisher nor the author shall be
liable for damages arising herefrom. If professional advice or other expert assistance is required, the services of a competent
professional should be sought.
Library of Congress Cataloging-in-Publication Data
Piegorsch, Walter W.
Statistical data analytics : foundations for data mining, informatics, and knowledge discovery / Walter W. Piegorsch.
pages cm
Includes bibliographical references and index.
ISBN 978-1-118-61965-0 (cloth : alk. paper) 1. Data mining–Mathematics. 2. Mathematical statistics. I. Title.
QA76.9.D343P535 2015
006.3′12—dc23
2015015327
A catalogue record for this book is available from the British Library.
Typeset in 10/12pt TimesLTStd by SPi Global, Chennai, India
1
2015

To Karen


Contents
Preface
xiii
Part I
Background: Introductory Statistical Analytics
1
1
Data analytics and data mining
3
1.1
Knowledge discovery: finding structure in data
3
1.2
Data quality versus data quantity
5
1.3
Statistical modeling versus statistical description
7
2
Basic probability and statistical distributions
10
2.1
Concepts in probability
10
2.1.1
Probability rules
11
2.1.2
Random variables and probability functions
12
2.1.3
Means, variances, and expected values
17
2.1.4
Median, quartiles, and quantiles
18
2.1.5
Bivariate expected values, covariance, and correlation
20
2.2
Multiple random variables∗
21
2.3
Univariate families of distributions
23
2.3.1
Binomial distribution
23
2.3.2
Poisson distribution
26
2.3.3
Geometric distribution
27
2.3.4
Negative binomial distribution
27
2.3.5
Discrete uniform distribution
28
2.3.6
Continuous uniform distribution
29
2.3.7
Exponential distribution
29
2.3.8
Gamma and chi-square distributions
30
2.3.9
Normal (Gaussian) distribution
32
2.3.10
Distributions derived from normal
37
2.3.11
The exponential family
41

viii
CONTENTS
3
Data manipulation
49
3.1
Random sampling
49
3.2
Data types
51
3.3
Data summarization
52
3.3.1
Means, medians, and central tendency
52
3.3.2
Summarizing variation
56
3.3.3
Summarizing (bivariate) correlation
59
3.4
Data diagnostics and data transformation
60
3.4.1
Outlier analysis
60
3.4.2
Entropy∗
62
3.4.3
Data transformation
64
3.5
Simple smoothing techniques
65
3.5.1
Binning
66
3.5.2
Moving averages∗
67
3.5.3
Exponential smoothing∗
69
4
Data visualization and statistical graphics
76
4.1
Univariate visualization
77
4.1.1
Strip charts and dot plots
77
4.1.2
Boxplots
79
4.1.3
Stem-and-leaf plots
81
4.1.4
Histograms and density estimators
83
4.1.5
Quantile plots
87
4.2
Bivariate and multivariate visualization
89
4.2.1
Pie charts and bar charts
90
4.2.2
Multiple boxplots and QQ plots
95
4.2.3
Scatterplots and bubble plots
98
4.2.4
Heatmaps
102
4.2.5
Time series plots∗
105
5
Statistical inference
115
5.1
Parameters and likelihood
115
5.2
Point estimation
117
5.2.1
Bias
118
5.2.2
The method of moments
118
5.2.3
Least squares/weighted least squares
119
5.2.4
Maximum likelihood∗
120
5.3
Interval estimation
123
5.3.1
Confidence intervals
123
5.3.2
Single-sample intervals for normal (Gaussian) parameters
124
5.3.3
Two-sample intervals for normal (Gaussian) parameters
128
5.3.4
Wald intervals and likelihood intervals∗
131
5.3.5
Delta method intervals∗
135
5.3.6
Bootstrap intervals∗
137
5.4
Testing hypotheses
138
5.4.1
Single-sample tests for normal (Gaussian) parameters
140
5.4.2
Two-sample tests for normal (Gaussian) parameters
142

CONTENTS
ix
5.4.3
Walds tests, likelihood ratio tests, and ‘exact’ tests∗
145
5.5
Multiple inferences∗
148
5.5.1
Bonferroni multiplicity adjustment
149
5.5.2
False discovery rate
151
Part II
Statistical Learning and Data Analytics
161
6
Techniques for supervised learning: simple linear regression
163
6.1
What is “supervised learning?”
163
6.2
Simple linear regression
164
6.2.1
The simple linear model
164
6.2.2
Multiple inferences and simultaneous confidence bands
171
6.3
Regression diagnostics
175
6.4
Weighted least squares (WLS) regression
184
6.5
Correlation analysis
187
6.5.1
The correlation coefficient
187
6.5.2
Rank correlation
190
7
Techniques for supervised learning: multiple linear regression
198
7.1
Multiple linear regression
198
7.1.1
Matrix formulation
199
7.1.2
Weighted least squares for the MLR model
200
7.1.3
Inferences under the MLR model
201
7.1.4
Multicollinearity
208
7.2
Polynomial regression
210
7.3
Feature selection
211
7.3.1
R2
p plots
212
7.3.2
Information criteria: AIC and BIC
215
7.3.3
Automated variable selection
216
7.4
Alternative regression methods∗
223
7.4.1
Loess
224
7.4.2
Regularization: ridge regression
230
7.4.3
Regularization and variable selection: the Lasso
238
7.5
Qualitative predictors: ANOVA models
242
8
Supervised learning: generalized linear models
258
8.1
Extending the linear regression model
258
8.1.1
Nonnormal data and the exponential family
258
8.1.2
Link functions
259
8.2
Technical details for GLiMs∗
259
8.2.1
Estimation
260
8.2.2
The deviance function
261
8.2.3
Residuals
262
8.2.4
Inference and model assessment
264
8.3
Selected forms of GLiMs
265
8.3.1
Logistic regression and binary-data GLiMs
265

x
CONTENTS
8.3.2
Trend testing with proportion data
271
8.3.3
Contingency tables and log-linear models
273
8.3.4
Gamma regression models
281
9
Supervised learning: classification
291
9.1
Binary classification via logistic regression
292
9.1.1
Logistic discriminants
292
9.1.2
Discriminant rule accuracy
296
9.1.3
ROC curves
297
9.2
Linear discriminant analysis (LDA)
297
9.2.1
Linear discriminant functions
297
9.2.2
Bayes discriminant/classification rules
302
9.2.3
Bayesian classification with normal data
303
9.2.4
Naïve Bayes classifiers
308
9.3
k-Nearest neighbor classifiers
308
9.4
Tree-based methods
312
9.4.1
Classification trees
312
9.4.2
Pruning
314
9.4.3
Boosting
321
9.4.4
Regression trees
321
9.5
Support vector machines∗
322
9.5.1
Separable data
322
9.5.2
Nonseparable data
325
9.5.3
Kernel transformations
326
10
Techniques for unsupervised learning: dimension reduction
341
10.1
Unsupervised versus supervised learning
341
10.2
Principal component analysis
342
10.2.1
Principal components
342
10.2.2
Implementing a PCA
344
10.3
Exploratory factor analysis
351
10.3.1
The factor analytic model
351
10.3.2
Principal factor estimation
353
10.3.3
Maximum likelihood estimation
354
10.3.4
Selecting the number of factors
355
10.3.5
Factor rotation
356
10.3.6
Implementing an EFA
357
10.4
Canonical correlation analysis∗
361
11
Techniques for unsupervised learning: clustering and association
373
11.1
Cluster analysis
373
11.1.1
Hierarchical clustering
376
11.1.2
Partitioned clustering
384
11.2
Association rules/market basket analysis
395
11.2.1
Association rules for binary observations
396
11.2.2
Measures of rule quality
397

CONTENTS
xi
11.2.3
The Apriori algorithm
398
11.2.4
Statistical measures of association quality
402
A
Matrix manipulation
411
A.1
Vectors and matrices
411
A.2
Matrix algebra
412
A.3
Matrix inversion
414
A.4
Quadratic forms
415
A.5
Eigenvalues and eigenvectors
415
A.6
Matrix factorizations
416
A.6.1
QR decomposition
417
A.6.2
Spectral decomposition
417
A.6.3
Matrix square root
417
A.6.4
Singular value decomposition
418
A.7
Statistics via matrix operations
419
B
Brief introduction to R
421
B.1
Data entry and manipulation
422
B.2
A turbo-charged calculator
426
B.3
R functions
427
B.3.1
Inbuilt R functions
427
B.3.2
Flow control
429
B.3.3
User-defined functions
429
B.4
R packages
430
References
432
Index
453


Preface
Every data set tells a story. Data analytics, and in particular the statistical methods at their core,
piece together that story’s components, ostensibly to reveal the underlying message. This is the
target paradigm of knowledge discovery: distill via statistical calculation and summarization
the features in a data set/database that teach us something about the processes affecting our
lives, the civilization which we inhabit, and the world around us. This text is designed as an
introduction to the statistical practices that underlie modern data analytics.
Pedagogically, the presentation is separated into two broad themes: first, an introduction
to the basic concepts of probability and statistics for novice users and second, a selection of
focused methodological topics important in modern data analytics for those who have the
basic concepts in hand. Most chapters begin with an overview of the theory and methods
pertinent to that chapter’s focal topic and then expand on that focus with illustrations and
analyses of relevant data. To the fullest extent possible, data in the examples and exercises
are taken from real applications and are not modified to simplify or “clean” the illustration.
Indeed, they sometimes serve to highlight the “messy” aspects of modern, real-world data
analytics. In most cases, sample sizes are on the order of 102–105, and numbers of variables
do not usually exceed a dozen or so. Of course, far more massive data sets are used to achieve
knowledge discovery in practice. The choice here to focus on this smaller range was made so
that the examples and exercises remain manageable, illustrative, and didactically instructive.
Topic selection is intended to be broad, especially among the exercises, allowing readers to
gain a wider perspective on the use of the methodologies. Instructors may wish to use cer-
tain exercises as formal examples when their audience’s interests coincide with the exercise
topic(s).
Readers are assumed to be familiar with four semesters of college mathematics, through
multivariable calculus and linear algebra. The latter is less crucial; readers with only an intro-
ductory understanding of matrix algebra can benefit from the refresher on vector and matrix
relationships given in Appendix A. To review necessary background topics and to establish
concepts and notation, Chapters 1–5 provide introductions to basic probability (Chapter 2),
statistical description (Chapters 3 and 4), and statistical inference (Chapter 5). Readers famil-
iar with these introductory topics may wish to move through the early chapters quickly, read
only selected sections in detail (as necessary), and/or refer back to certain sections that are
needed for better comprehension of later material. Throughout, sections that address more
advanced material or that require greater familiarity with probability and/or calculus are high-
lighted with asterisks (*). These can be skipped or selectively perused on a first reading, and
returned to as needed to fill in the larger picture.

xiv
PREFACE
The more advanced material begins in earnest in Chapter 6 with techniques for supervised
learning, focusing on simple linear regression analysis. Chapters 7 and 8 follow with multiple
linear regression and generalized linear regression models, respectively. Chapter 9 completes
the tour of supervised methods with an overview of various methods for classification. The
final two chapters give a complementary tour of methods for unsupervised learning, focusing
on dimension reduction (Chapter 10) and clustering/association (Chapter 11).
Standard mathematical and statistical functions are used throughout. Unless indicated
otherwise – usually by specifying a different base – log indicates the natural logarithm, so that
log(x) is interpreted as loge(x). All matrices, such as X or M, are presented in bold uppercase.
Vectors will usually display as bold lowercase, for example, b, although some may appear as
uppercase (typically, vectors of random variables). Most vectors are in column form, with the
operator T used to denote transposition to row form. In selected instances, it will be convenient
to deploy a vector directly in row form; if so, this is explicitly noted.
Much of modern data analytics requires appeal to the computer, and a variety of com-
puter packages and programming languages are available to the user. Highlighted herein is
the R statistical programming environment (R Core Team 2014). R’s growing ubiquity and
statistical depth make it a natural choice. Appendix B provides a short introduction to R for
beginners, although it is assumed that a majority of readers will already be familiar with at
least basic R mechanics or can acquire such skills separately. Dedicated introductions to R
with emphasis on statistics are available in, for example, Dalgaard (2008) and Verzani (2005),
or online at the Comprehensive R Archive Network (CRAN): http://cran.r-project.org/. Also
see Wilson (2012).
Examples and exercises throughout the text are used to explicate concepts, both theoretical
and applied. All examples end with a
symbol. Many present sample R code, which is usually
intended to illustrate the methods and their implementation. Thus the code may not be most
efficient for a given problem but should at least give the reader some inkling into the process.
Most of the figures and graphics also come from R. In some cases, the R code used to create
the graphic is also presented, although, for simplicity, this may only be “base” code without
accentuations/options used to stylize the display.
Throughout the text, data are generally presented in reduced tabular form to show only
a few representative observations. If public distribution is permitted, the complete data sets
have been archived online at http://www.wiley.com/go/piegorsch/data_analytics or their
online source is listed. A number of the larger data sets came from from the University of
California–Irvine (UCI) Machine Learning Repository at http://archive.ics.uci.edu/ml (Frank
and Asuncion, 2010); appreciative thanks are due to this project and their efforts to make
large-scale data readily available.
Instructors may employ the material in a number of ways, and creative manipulation is
encouraged. For an intermediate-level, one-semester course introducing the methods of data
analytics, one might begin with Chapter 1, then deploy Chapters 2–5, and possibly Chapter 6
as needed for background. Begin in earnest with Chapters 6 or 7 and then proceed through
Chapters 8–11 as desired. For a more complete, two-semester sequence, use Chapters 1–6
as a (post-calculus) introduction to probability and statistics for data analytics in the first
semester. This then lays the foundations for a second, targeted-methods semester into the
details of supervised and unsupervised learning via Chapters 7–11. Portions of any chapter
(e.g., advanced subsections with asterisks) can be omitted to save time and/or allow for greater
focus in other areas.

PREFACE
xv
Experts in data analytics may canvass the material and ask, how do these topics differ
from any basic selection of statistical methods? Arguably, they do not. Indeed, whole books
can be (and have been) written on the single theme of essentially every chapter. The focus
in this text, however, is to highlight methods that have formed at the core of data analytics
and statistical learning as they evolved in the twenty-first century. Different readers may find
certain sections and chapters to be of greater prominence than others, depending on their own
scholarly interests and training. This eclectic format is unavoidable, even intentional, in a
single volume such as this. Nonetheless, it is hoped that the selections as provided will lead
to an effective, unified presentation.
Of course, many important topics have been omitted or noted only briefly, in order to
make the final product manageable. Omissions include methods for missing data/imputation,
spurious data detection, novelty detection, robust and ordinal regression, generalized additive
models, multivariate regression, and ANOVA (analysis of variance, including multivariate
analysis of variance, MANOVA), partial least squares, perceptrons, artificial neural networks
and Bayesian belief networks, self-organizing maps, classification rule mining, and text min-
ing, to name a few. Useful sources that consider some of these topics include (a) for missing
data/imputation, Abrahantes et al. (2011); (b) for novelty detection, Pimentel et al. (2014);
(c) for generalized additive models, Wood (2006); (d) for MANOVA, Huberty and Olejnik
(2006); (e) for partial least squares, Esposito Vinzi and Russolillo (2013); (f) for neural net-
works, Stahl and Jordanov (2012); (g) for Bayesian belief networks, Phillips (2005); (h) for
self-organizing maps, Wehrens and Buydens (2007); and (i) for text mining, Martinez (2010),
and the references all therein. Many of these topics are also covered in a trio of dedicated
texts on statistical learning – also referenced regularly throughout the following chapters – by
Hastie et al. (2009), Clarke et al. (2009), and James et al. (2013). Interested readers are encour-
aged to peruse all these various sources, as appropriate.
By way of acknowledgments, sincere and heartfelt thanks are due numerous colleagues,
including Alexandra Abate, Euan Adie, D. Dean Billheimer and the statisticians of the Arizona
Statistical Consulting Laboratory (John Bear, Isaac Jenkins, and Shripad Sinari), Susan L. Cut-
ter, David B. Hitchcock, Fernando D. Martinez, James Ranger-Moore, Martin Sill, Debra A.
Stern, Hao Helen Zhang, and a series of anonymous reviewers. Their comments and assistance
helped to make the presentation much more accessible. Of course, despite the fine efforts of
all these individuals, some errors may have slipped into the text and these are wholly my own
responsibility. I would appreciate hearing from readers who identify any inconsistencies that
they may come across.
Most gracious thanks are also due the editorial team at John Wiley & Sons – Prachi Sinha
Sahay, Kathryn Sharples, Heather Kay, and Richard Davies – and their LATEX support staff led
by Alistair Smith. Their patience and professionalism throughout the project’s development
were fundamental in helping it achieve fruition.
Walter W. Piegorsch
Tucson, Arizona
October 2014


Part I
BACKGROUND:
INTRODUCTORY
STATISTICAL ANALYTICS


1
Data analytics and data mining
1.1
Knowledge discovery: finding structure in data
The turn of the twenty-first century has been described as the beginning of the (or perhaps
“an”) Information Age, a moniker that is difficult to dismiss and likely to be understated.
Throughout the period, contemporary science has evolved at a swift pace. Ever-faster
scanning, sensing, recording, and computing technologies have developed which, in turn,
generate data from ever-more complex phenomena. The result is a rapidly growing amount
of “information.” When viewed as quantitative collections, the term heard colloquially is
“Big Data,” suggesting a wealth of information – and sometimes disinformation – available
for study and archiving. Where once computer processing and disk storage were relegated to
the lowly kilobyte (1024 bytes) and megabyte (1024 KB) scales, we have moved past routine
gigabyte- (1024 MB) and terabyte- (1024 GB) scale computing and now collect data on the
petabyte (1024 TB) and even the exabyte (1024 PB) scales. Operations on the zettabyte scale
(1024 EB) are growing, and yottabyte- (1024 ZB) scale computing looms on the horizon.
Indeed, one imagines that the brontobyte (1024 YB) and perhaps geopbyte (1024 BB) scales
are not far off (and may themselves be common by the time you read this).
Our modern society seems saturated by the “Big Data” produced from these technological
advances. In many cases, the lot can appear disorganized and overwhelming – and sometimes
it is! – engendering a sort of “quantitative paralysis” among decision makers and analysts. But
we should look more closely: through clever study of the features and latent patterns in the
underlying information, we can enhance decision- and policy-making in our rapidly changing
society. The key is applying careful and proper analytics to the data.
At its simplest, and no matter the size, data are the raw material from which informa-
tion is derived. This is only a first step, however: the information must itself be studied and its
patterns analyzed further, leading to knowledge discovery. [An earlier term was knowledge dis-
covery in databases, or “KDD” (Elder and Pregibon 1996), because the data often came from a
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

4
STATISTICAL DATA ANALYTICS
Wisdom
Knowledge
Information
Data
Wisdom
Knowledge
Information
Data
Figure 1.1
The DIKW pyramid.
database repository.] A capstone step in the process integrates and synthesizes the knowledge
that has been gained on the phenomenon of interest, to produce true wisdom and advance the
science. Thus from Data we derive Information, gaining Knowledge and producing Wisdom:
D →I →K →W. The effort is sometimes described as a DIKW pyramid or DIKW hierarchy
(Rowley 2007), because each step in the process represents a further refinement on the pre-
vious advance. (Some authors have derided the term, suggesting that it underemphasizes and
misrepresents the complexities of synthesizing knowledge from information. Nonetheless,
the DIKW pyramid still gives a useful framework for conceptualizing the knowledge- and
wisdom-discovery process.) Figure 1.1 abstracts the concept.
The DIKW paradigm is by nature a multidisciplinary endeavor: computer scientists con-
structalgorithmstomanipulateandorganizethedata,aidedbystatisticiansandmathematicians
who instruct on development and application of quantitative methodology. Then, database
experts collect and warehouse the data, software designers write programs that apply the
analytic algorithms to the data, engineers build electronics and hardware to implement the
programming, and subject-matter/domain experts – that is, biologists, chemists, economists,
and social scientists – interpret the findings. To be successful, no one discipline or contributor
can operate in a vacuum: each step in the process is enhanced by the interaction and interplay of
all participants. (Indeed, the more each contributing discipline informs itself about and involves
itselfwiththeothers,themoreproductivetheDIKWeffortbecomes.)Itistrueinterdisciplinarity
at work, driving us to the targets, knowledge and wisdom, at the top of the pyramid.
At the base of the pyramid lies the foundation: the data. To advance successfully through
each DIKW step, we must apply effective data collection, description, analysis, and interpreta-
tion. These all are the purview of statistical science, and it is the methods of modern statistical
analysis that lie at the core of data analytics. Thus experience and familiarity with statistical
data analytics has become a fundamental, necessary skill for any modern scientist dealing
with Big Data. Since these methods are applied at the base of the pyramid – and often also
throughout the advancing steps – this textbook views them as foundations for the DIKW pro-
cess. When applied properly, and within the context of the larger interdisciplinary endeavor,
features and structures in the data are revealed: for example, clinicians identify suscepti-
ble subpopulations in large databases of breast cancer patients, economists study credit card

DATA ANALYTICS AND DATA MINING
5
records for possible trends in purchasing behavior, sociologists track how networks develop
among users of social media, and geographers catalog data on natural hazards and highlight
localities with increased risk.
It is important to warn that domain-aided interpretation is a necessary component in this
process: large data sets can contain structural features which when studied in greater depth rep-
resent nothing more than random noise. Teasing out real patterns from any apparent structure
is often as much art as science. Nonetheless, when the analytics are successful, they facilitate
our ultimate goal of knowledge discovery and advancement in wisdom.
The effort to bore through a large database studying possible patterns of response is often
called data mining. The term conjures imagery of a miner digging through masses of rock
in search of precious stones and is a surprisingly useful metaphor here. A more formal def-
inition is “the process of seeking interesting or valuable information within large data sets”
(Hand et al. 2000, p. 111). Larger still (although the two areas need not overlap) is the field
of informatics, the study and development of quantitative tools for information processing.
Many informatic subfields have emerged as data miners and analysts have specialized their
focus. Examples include bioinformatics and medical informatics, ecoinformatics, geoinfor-
matics, socioinformatics; the list grows daily! In all these areas, the data-analytic effort relies
heavily on proper description, summarization, visualization, and, when necessary, inferential
analysis of the collected data mass. The foundational statistical techniques for doing so are the
basis of the material presented in this textbook. Some of the focus will be on issues associated
with data mining, that is, how one explores collections of data statistically to identify impor-
tant patterns and structure. Where pertinent, however, connections and extensions to larger
applications in informatic science will also gain attention. The material is presented primarily
at an introductory level, although the later chapters also give guidance on intermediate and
(occasionally) advanced topics.
1.2
Data quality versus data quantity
An often-overlooked issue in data mining and data analytics is the need for sufficiently high
quality in the data under study. Data miners regularly remind themselves of the GIGO prin-
ciple: “if Garbage goes In, Garbage come Out” (Hand et al. 2001, Section 2.6). That is, the
quality and value of any data mining or informatic analysis is contingent upon the quality of the
underlying data. In and of itself, a large database is oftentimes an important resource; however,
quantity of data does not always equate with quality of information. The data must themselves
possess a level of quality commensurate with the questions they are asked to address.
This concern is worth emphasizing at an early stage in any data-analytic effort and should
be kept in mind as the calculations and analyses unfold. Many informatic projects utilize data
stores not under the control of the analyst or involve secondary analyses of existing databases.
Thus it may not be possible to avoid or control data entry errors, coding inaccuracies, mea-
surement mistakes, subject misidentifications, etc. If direct access and oversight is available,
then some level of quality assurance/quality control (“QA/QC”) should be imposed on the data
entry and curation process; see, for example, Fong (2001) or Pierchala and Surti (2009). Oth-
erwise, potential data entry missteps or other errors in an existing database can sometimes be
identified via statistical analysis, including single- or multi-dimensional graphical displays,
data summarization techniques, or other forms of comparative statistical testing. (Many of
these methods have more-general uses in statistical data analytics as well; these are described

6
STATISTICAL DATA ANALYTICS
in the following chapters.) Of course, the analyst must also be wary of going too far: over-
correction of, say, missing data by imputing the missing values from the remainder of the
database might just as quickly smooth away the very patterns the mining exercise is intended
to detect.
Hand et al. (2000) distinguish between two general forms of data quality distortion: indi-
vidual and collective. The first (“individual”) occurs when the larger database is generally
sound, but particular records in the database are affected by errors in collection, entry, or some
other form of disruption. Classical examples include misplaced decimal points, transposed
digits, measurement rounding errors, missing data records, and impossible combinations in
classification fields (think: pregnant = “yes”/sex = “male”). These sorts of errors are often
difficult to control, and the problem is common to almost any large collection of data: even
the best data quality assurance program will on occasion let errors slip by. Data miners must
be aware that sometimes, a feature or pattern uncovered in a large database could simply be
the consequence of (a series of) individual-level data distortions. When examined in greater
depth, these likely will be recognized as such and usually are afforded little value. Indeed,
Hand et al. mention, only partly with tongue-in-cheek, that a large database found to be free
of any errors may call into suspicion the quality of the database as a whole! More seriously
though, they also note that certain patterns of distortion may in fact be of actual interest to the
data miner; for example, large blocks of missing data can sometimes indicate a real, predic-
tive classification feature in the population under study. Obviously, a kind of balancing act is
required here: while outlying observations might be the purposeful target of an exercise in,
say, credit-fraud detection, they more often hinder proper pattern detection in a typical data
mining project (Hand et al. 2001, Section 2.7).
The second form of distortion (“collective”) occurs when the larger collection suffers
irregularities in the selection mechanisms under which the data were identified or sampled.
Technically, data scientists define the sampling frame as the population of units from which a
data set or database has been drawn and upon which measurements are taken/recorded. It is to
this population that any inferences made from the data apply. For instance, suppose an analyst
mines a database of patients suffering from a particular respiratory disease, such as asthma, in
the warm and arid US Southwest. Any patterns of disease associated with, say, low-pressure
weather systems gleaned from those records might not – indeed, likely will not – apply to
asthma patients in more-humid, cooler north Britain/Scotland.
More generally, when data are inaccurately registered in a systematic manner, they con-
taminate the database and confuse the underlying sampling frame. A form of collective-scale
data distortion ensues. To help to avoid collective sampling frame distortions, statistical prac-
tice encourages application of formal sampling strategies to the target population in order to
construct the database. Preferred, at least at a basic level, is simple random sampling, where
the units are sampled independently and with equally likely probabilities (see Section 3.1). By
contrast, in selected instances, the database is large enough to contain the entire population of
interest; for example, a grocery chain may collect shopping records of all its customers over
a 6-month period. If so, the data now represent a full census of the population, and issues of
sampling are less urgent. Complete enumerations of this sort are typically necessary if the
informatic goal is one of fine-pattern detection across the target population.
More complex forms of probability-based sampling are also possible, although these
exceed the scope here. For a deeper introduction to the theory and application of sampling
methodology, see Thompson (2012) or Lohr (2010).

DATA ANALYTICS AND DATA MINING
7
Of course, it is not always possible to control the sampling/selection process. In many
cases, the data are recorded simply as the opportunity allows, at the convenience of the team
building the database and with limited or no regard to sampling theory guidelines. This is
called convenience sampling or opportunity sampling. Or, the selection process may by its
very nature favor certain subjects; for instance, patients recruited for a study of genetic sus-
ceptibility to lung cancer may already be in the clinic for other disease-related reasons. (In
the worst case, they all might be cigarette smokers under treatment for another, noncancerous
disease such as emphysema, confounding study of the factors that lead to disease onset or
progression. Upon reflection, it is perhaps obvious here that the subjects are being sampled
preferentially; still, it is also surprising how often a selective process such as this goes unrec-
ognized in practice.) The effect is known as selection bias, where the inclusion of a record
in the database depends on what value(s) the variables take, or on some other, external, non-
random feature (Wei and Cowan 2006). Selection bias can have a substantial confounding or
contaminating effect on a large database.
Other forms of collection-level distortion include drift in the target population’s attributes
over time (e.g., oxygenation levels in an ecosystem’s lakes may exhibit unrecognized changes
due to increasing climate temperature) or overzealous data screening to expunge distortions
that ends up excluding perfectly valid records. In the end, one can control for (some) data
distortions via statistical adjustments in the data and/or in the analyses applied to them, but
this is not always possible. At a minimum, the analyst must be aware of distorting influences
on data quality in order to avoid falling victim to their ills. See Hand et al. (2000, Section 4)
or Hand et al. (2001, Section 2.7) for more details and some instructive examples.
1.3
Statistical modeling versus statistical description
An important component in statistical analytics, and one that has exhibited the power of sta-
tistical science over the past century, is that of statistical inference (Casella and Berger 2002;
Hogg and Tanis 2010). Statistical inference is the derivation of conclusions about a popula-
tion from information in a random sample of that population. In many cases, formal statistical
models are required to implement the inferential paradigm, using probability theory. By con-
trast, statistical description is the process of summarizing quantitative and qualitative features
in a sample or population. The description process is often represented as simpler than the
modeling/inferential process, but in fact, both require a level of skill and expertise beyond
that of simple statistical arithmetic. A better distinction might be that inference is designed
to make deductions about a feature of the population, while description is designed to bring
features of a population to light.
Statistical description and statistical inference are typically applied in tandem, and the
inferential process often contains descriptive aspects. They can also be employed separately,
however, and it is not unusual in a data mining exercise to focus on only one of the two. For
instance, an exploratory investigation of radio-transmitter data from tagged animals of a cer-
tain species may only involve simple description of their tracks and trajectories throughout
a wildlife preserve. Alternatively, an inferential study on how two different species traverse
the preserve might determine if a significant difference was evidenced in their trajectory pat-
terns. In the former case, we call the effort one of exploratory data analysis, or “EDA,” a
statistical archetype popularized by Tukey (1977); more recently, see Gelman (2004) or Buja

8
STATISTICAL DATA ANALYTICS
et al. (2009). The EDA approach shares similarities with many descriptive statistical methods
employed in data analytics, and the two paradigms often overlap (Myatt 2007). As a result, the
focus in this text will be on exploratory aspects of the data mining and knowledge discovery
process, driven by statistical calculation. To provide a broader panorama, however, associated
methods of statistical inference will also be considered. Chapter 2 begins with an introduc-
tion to basic probability models useful in statistical inference. Chapters 3 and 4 follow with an
introduction to methods of statistical description, data manipulation, and data visualization.
On the basis of these methods of probability and data description, Chapter 5 then formally
introduces the inferential paradigm. Readers familiar with introductory concepts in the earlier
chapters may wish to skip forward to Chapter 6 on regression techniques for supervised learn-
ing or on to further chapters where specific foundational statistical methods for data analytics
and selected informatic applications are presented.
Exercises
1.1
Use an online search engine or any other means of textual search to give a list of at
least three more specialized areas of “informatics”, beyond those mentioned (bioinfor-
matics, ecoinformatics, etc.) in Section 1.1.
1.2
Give an application (from your own field of study, as appropriate) where data mining
is used, and indicate instances of knowledge discovery generated from it.
1.3
Describe the nature of the database(s) from Exercise 1.2 on which the data mining was
performed. What quantities were measured? What was the target population? What
was/were the sampling frame/s?
1.4
Give an application (from your own field of study, as appropriate) where data distortion
can occur for
(a) individual-level distortion.
(b) collective-level distortion.
1.5
As mentioned in Section 1.2, a grocery chain constructed a large database from shop-
ping records of all its customers between January 1 and June 30 in a given year. The
data only recorded each customer’s purchase(s) of (i) any cheese products at least once
every month, (ii) any meat products at least once every month, and (iii) any seafood
products at least once every month. It was not recorded whether the customers consid-
ered themselves vegetarians, however. Is this a form of individual-level data distortion
or collective-level distortion? Justify your answer.
1.6
(Hand et al., 2000) A large database was constructed on male adult diastolic blood pres-
sures (in mmHg). When graphed, the data showed that measurements ending in odd
numbers were much more common at higher blood pressure readings. Upon deeper
investigation, it was found that the pressures were taken with a digital instrument that
could only display even values. When a male subject’s reading was exceptionally high,
however, the technician repeated the measurement and recorded the average of the
two readings. Thus although both original readings had to be even, the averaged read-
ing could be odd. Is this a form of individual-level data distortion or collective-level
distortion? Justify your answer.

DATA ANALYTICS AND DATA MINING
9
1.7
To gauge students’ opinions on proposed increases in statewide taxes, a polling firm
sent operatives to every public college or university in their state. At each campus,
the operatives stood outside the Student Union or main cafeteria just before lunch.
For 30 minutes, they asked any student entering the building if he or she supported or
opposed the tax increases. They also recorded the student’s age, sex, and class standing
(freshman, sophomore, etc.). Describe in what way(s) this can be viewed as a form of
convenience sampling. Can you imagine aspects that could be changed to make it more
representative and less opportunistic?
1.8
A financial firm builds a large database of its customers to study their credit card usage.
In a given month, customers who had submitted at least their minimum monthly pay-
ment but less than the total amount due on that month’s statement were included. By
how much, if at all, the monthly payment exceeded the minimum payment level was
recorded. These values were then mined for patterns using the customers’ ages, lengths
of patronage, etc. Is there any selection bias evident in this approach? Why or why not?
1.9
As mentioned in Section 1.2, a physician collected a large database of records on
asthma patients in the US Southwest. He determined whether temporal patterns
occurred in the patients’ asthma onset when low-pressure weather fronts passed
through the region. Is this a question of statistical description or statistical inference?
Justify your answer.
1.10
Return to the asthma study in Exercise 1.9. The physician there also mined the database
for associative patterns of patient proximity to construction sites where large amounts
of airborne particulates were generated. Is this a question of statistical description or
statistical inference? Again, justify your answer.
1.11
A geographer constructs a database on the county-by-county occurrence of natural dis-
asters in the US Southeast over a 40-year period, along with corresponding county-level
information on concurrent property damage (in $). She then uses the database to deter-
mine statistically if a difference exists in property losses due to a particular form of
disaster (floods) among counties in two adjoining US states. Is this a question of sta-
tistical description or statistical inference? Why or why not?

2
Basic probability and statistical
distributions
The elements of probability theory serve as a cornerstone to most, if not all, statistical oper-
ations, be they descriptive or inferential. In this chapter, a brief introduction is given to these
elements, with focus on the concepts that underlie the foundations of statistical informatics.
Readers familiar with basic probability theory may wish to skip forward to Section 2.3 on spe-
cial statistical distributions or farther on to Chapter 3 and its introduction to basic principles
of data manipulation.
2.1
Concepts in probability
Data are generated when a random process produces a quantifiable or categorical outcome. We
collect all possible outcomes from a particular random process together into a set, , called
the sample space or support space. Any subcollection of possible outcomes, including a single
outcome, is called an event, . Notice that an event is technically a subset of the sample space
. Standard set notation for this is ⊂.
Probabilities of observing events are defined in terms of their long-term frequencies of
occurrence, that is, how frequent the events (or combinations of events) occur relative to all
other elements of the sample space. Thus if we generate a random outcome in a repeated
manner and count the number of occurrences of an event , then the ratio of this count to the
total number of times the outcome could occur is the probability of the event of interest. This
is the relative frequency interpretation of probability. The shorthand for P[Observe event ] is
P[] for any ⊂. To illustrate, consider the following simple, if well recognized, example.
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
11
Example 2.1.1 Six-sided die roll. Roll a fair, six-sided die and observe the number of ‘pips’
seen on that roll. The sample space is the set of all possible outcomes from one roll of that die:
= {1, 2, … , 6}. Any individual event is a single number, say, = {6} = {a roll showing 6
pips}. Clearly, the single event = {6} is contained within the larger sample space .
If the die is fair, then each individual event is equally likely. As there are six possible
events in , to find P[], divide 1 (for the single occurrence of ) by 6 (for the six possible
outcomes): P[] = 1
6, that is, in one out of every six tosses, we expect to observe a {6}.
◽
2.1.1
Probability rules
A variety of fundamental axioms are applied in the interpretation of a probability P[]. The
most well known are
(1a) 0 ≤P[] ≤1, and
(1b) P[] = 1.
In addition, a number of basic rules apply for combinations of two events, 1 and 2. These are
(2a) Addition Rule. P[1 or 2] = P[1] + P[2] −P[1 and 2].
(2b) Conditionality Rule. P[1 given 2] = P[1 and 2]∕P[2] for any event 2 such that
P[2] > 0. For notation, conditional probabilities are written with the symbol ‘|’, for
example, P[1|2] = P[1 given 2].
(2c) Multiplication Rule. P[1 and 2] = P[1|2] P[2].
Special cases of these rules occur when the events in question relate in a certain way. For
example, two events 1 and 2 that can never occur simultaneously are called disjoint (or
equivalently, mutually exclusive). In this case, P[1 and 2] = 0. Notice that if two events are
disjoint, the Addition Rule in (2a) simplifies to P[1 or 2] = P[1] + P[2]. Two disjoint
events, 1 and 2, are complementary if the joint event {1 or 2} makes up the entire sample
space . Notice that this implies P[1 or 2] = 1. If two events, 1 and 2, are complementary
so are their probabilities. This is known as the Complement Rule:
(2d) Complement Rule: If, for two disjoint events 1 and 2, the joint event {1 and 2}
equals the entire sample space , then P[1] = 1 −P[2] and P[2] = 1 −P[1].
Example 2.1.2 Six-sided die roll (Example 2.1.1, continued). Return to the roll of a fair,
six-sided die. As seen in Example 2.1.1, the sample space is = {1, 2, … , 6}. As the die
is only rolled once, no two singleton events can occur together, so, for example, observing a
{6} and observing a {4} are disjoint events. Thus from the Addition Rule (2a) with disjoint
events, P[4 or 6] = P[4] + P[6] = 1
6 + 1
6 = 1
3.
More involved constructions are also possible. For instance, from the Complement Rule
(2d), P[not observing a 6] = P[1 or 2 or 3 or 4 or 5] = 1 −P[6] = 1 −1
6 = 5
6.
◽
The case where disjoint events completely enumerate the sample space has a special
name: it is called a partition. One need not be restricted to only two events, however. If a
set of h ≥2 events, {1, 2, … , h}, exists such that (i) all the h events are disjoint from

12
STATISTICAL DATA ANALYTICS
each other – technically, if every pair of events i and j, i ≠j, is disjoint – and (ii) the col-
lective event {1 and 2 and · · · and h} equals , we say the set forms a partition of .
(Mathematically, the partition can even consist of a countably infinite set of pairwise-disjoint
events {1, 2, … } if they satisfy these conditions.) This leads to another important rule from
probability theory:
(2e) The Law of Total Probability. For any event ⊂and any partition, {1, 2,
… , h}, of ,
P[] =
h
∑
i=1
P[|i]P[i].
The Law of Total Probability in (2e) is an important building block in another famous
result from probability theory, known as Bayes’ rule. It describes how the probability of
an event P[] can be ‘updated’ using external information. The result is credited to eigh-
teenth century Presbyterian minister Sir Thomas Bayes (Bayes 1763), although, see Stigler
(1983) regarding the particulars behind that assignment. In its simplest form, Bayes’ rule
also shows how conditional probabilities can be reversed: by recognizing that P[and ] =
P[and ] and manipulating the Multiplication Rule (2c) with this fact in mind, one can
show (Exercise 2.5) that
P[|] = P[|] P[]
P[].
(2.1)
More generally, the result can be applied to full partitions of the sample space:
(2f) Bayes’ Rule. For any event ⊂and any partition, {1, 2, … , h}, of ,
P[i|] = P[|i]P[]
∑h
i=1 P[|i]
for every i = 1, … , h.
A different relationship occurs between two events if they do not impact each other in
any way. Suppose the knowledge that one event 1 occurs has absolutely no impact on the
probability that a second event 2 occurs and that the reverse is also true. Two such events
are called independent. In effect, independent events modify the Conditionality Rule (2b) into
P[1|2] = P[1] and P[2|1] = P[2]. More importantly, for two independent events, the
Multiplication Rule (2c) simplifies to P[1 and 2] = P[1]P[2].
2.1.2
Random variables and probability functions
Suppose a random outcome can be quantified formally, either because (i) it is an actual mea-
surement or count or (ii) it is a qualitative outcome that has been unambiguously coded into a
numeric value. Such a quantified random outcome is called a random variable. Standard nota-
tion for random variables is uppercase Roman letters, such as X or Y. To distinguish between
a conceptual random variable and one that has already been realized in practice, the realized
value is denoted by a lowercase Roman character: x or y. The basic probability rules for events
as discussed in Section 2.1.1 can then be expanded to describe random variables.

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
13
At the core of the operation is the notion of a probability function. Probability functions
are unifying mathematical descriptions of how a random outcome varies. They are used to
characterize two basic types of random variables: discrete and continuous. A discrete ran-
dom variable takes on only discrete values; examples include simple binary variates (say
‘damaged’ = 1 vs. ‘operating’ = 0 in a component reliability study), counts of occurrences
(numbers of customers who purchase a sale item or numbers of adverse-event reports with a
new drug), or even studies that result in an infinite, yet countable, number of outcomes (i.e.,
counts without a clear upper bound, such as the number of different insect species in a tropi-
cal forest). A continuous random variable takes on values over a continuum (mass or length,
stock market averages, blood levels of a chemical, etc.). Discrete random variables often arise
from counting or classification processes, while continuous random variables often arise from
some sort of measurement process. In either case, the probability functions will depend on the
nature of the random outcome.
Suppose the random variable X is discrete and consider the ‘event’ that X takes on some
specific value, say X = m. Then, the values of P[X = m] over all possible values of m describe
the probability distribution of X. Standard notation here is fX(m) = P[X = m]; this is called the
probability mass function (or p.m.f.) of X. As fX(m) is a probability, it must satisfy the various
axioms and rules from Section 2.1.1. Thus, for example, 0 ≤fX(m) ≤1 for all arguments m
and ∑
m∈fX(m) = 1, where the sum is taken over all possible values of m in the sample space
. (The symbol ‘∈’ is read ‘is an element of.’)
Summing the discrete p.m.f. over increasing values up to m produces what is called the
cumulative distribution function (or c.d.f.) of X:
FX(m) = P[X ≤m] =
∑
i≤m
fX(i).
As the c.d.f. is itself a probability, it must also satisfy the probability rules from Section 2.1.1,
in particular, 0 ≤FX(m) ≤1 for any m. Or, from the Complement Rule (2d), P[X > m] =
1 −P[X ≤m] = 1 −FX(m).
As it gives cumulative probabilities, the c.d.f. must be a nondecreasing function. In fact,
for discrete random variables, the c.d.f. will typically have the appearance of a nondecreasing
step function.
Example 2.1.3 Six-sided die roll (Example 2.1.1, continued). Roll a fair, six-sided die and
now formally define the random variable X as the number of ‘pips’ seen on that roll. As seen
in Example 2.1.1, the sample space is = {1, 2, … , 6} and because the die is fair, the p.m.f.
is P[X = m] = fX(m) = 1∕6 for any m ∈.
The c.d.f. FX(m) = P[X ≤m] is simply the cumulative sum of these uniform probabilities
up to and including the argument m. So, for example,
FX(4) =
4
∑
m=1
fX(m) = 1
6 + 1
6 + 1
6 + 1
6 = 2
3.
Notice, however, that if the argument to the c.d.f. is not an element of the sample space ,
then the cumulative probability calculation will stop at the previous element in . So, for
example, FX(4.2) = P[X ≤4.2] is equal to P[X ≤4] = 2∕3, because the event {X ≤4.2} is
identical to the event {X ≤4} for this discrete random variable. This effect gives the c.d.f. here
a ‘step function’ appearance, as in Figure 2.1. Solid dots in the figure indicate the ‘jumps’ in
probability mass at each value of m in .
◽

14
STATISTICAL DATA ANALYTICS
0
1
2
3
4
5
6
0.0
0.2
0.4
0.6
0.8
1.0
X = {# pips on die}
c.d.f. FX(x)
Figure 2.1
Cumulative distribution function (c.d.f.) for a discrete random variable:
X ={number of pips on roll of six-sided die} in Example 2.1.3.
Conversely, for continuous random variables, the probability function, fX(x), describes the
random variable’s continuous density of probability. Thus the terminology changes, and fX(x)
is now called the probability density function (or p.d.f.) of the continuous random variable
X. It is used in expressing probabilities over interval subsets of the real numbers via definite
integrals, for example,
P[a ≤X ≤b] = ∫
b
a
fX(x)dx.
(Readers unfamiliar with concepts of integration and derivatives should refer to introductory
texts in calculus (Hughes-Hallett et al. 2013); readers requiring only a refresher may find tar-
geted texts such as Khuri (2003) helpful.) Similarly, the c.d.f. of a continuous random variable
is the area under the p.d.f. integrated from −∞to the argument, x, of the function:
FX(x) = P[X ≤x] = ∫
x
−∞
fX(u) du.
(2.2)
The definition in (2.2) relates cumulative probabilities to areas under p.d.f. curves. This
produces some interesting consequences: notice that P[a ≤X ≤b] = FX(b) −FX(a) and so
P[X = a] = P[a ≤X ≤a] = FX(a) −FX(a) = 0 for any a. That is, for a continuous random
variable, nonzero probability can only be assigned to events that correspond to intervals of

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
15
values. As a result, if X is continuous, P[X ≤a] = P[X < a] + P[X = a] = P[X < a] + 0 =
P[X < a]. From (2.2), one also finds that a continuous c.d.f. must possess strict limiting
values: limx→−∞FX(x) = 0 and limx→∞FX(x) = 1. For discrete random variables, however,
nonzero probability can be assigned to events that correspond to particular outcomes, so that
statements such as P[Y = a] could evaluate as numbers between 0 and 1, depending on the
underlying p.m.f.
Similar to the case with discrete random variables and their p.m.f.s, the p.d.f. from a
continuous random variable satisfies two basic axioms: (i) fX(x) ≥0 for all arguments x and
(ii) ∫∞
−∞fX(x) dx = 1. Notice also that if FX(x) is a differentiable function, then from the fun-
damental theorem of calculus (Khuri 2003, Section 6.4), its derivative at the point x is the
p.d.f.: dFX(x)∕dx = fX(x).
A probability function (p.m.f. or p.d.f.) that when graphed rises to a single peak and then
falls back is called unimodal. The ‘mode’ of the function is the value of x at which the single
peak is attained. A probability function with more than one mode is called multimodal.
When a probability function is arranged such that its heights are equal both to the left and
right of some central point, say x = b, it has a special structure: a p.m.f. or p.d.f. is symmetric
about a point b if fX(b + 𝜖) = fX(b −𝜖) for all 𝜖> 0. By contrast, unimodal probability func-
tions (and their underlying random variables) that deviate from symmetry are called skewed.
A probability function that tails off faster to the right than to the left is ‘skewed right;’ one that
tails off faster to the left is ‘skewed left.’ Figure 2.2 plots a typical, right-skewed, unimodal
p.d.f. for a positive random variable X, along with a typical, symmetric, unimodal p.d.f. for a
continuous random variable Y.
Right skew is not uncommon with many positive random variables such as blood concen-
trations or income levels: the hard lower bound at x = 0 often causes the probability mass or
density to crowd together as x approaches 0, and/or the open upper range allows for extremely
large values of x, with correspondingly low probabilities of concurrence.
x or y
p.d.f.
fX(x)
fY(y)
Figure 2.2
Unimodal probability density functions (p.d.f.s) for a continuous, right-skewed
random variable X (solid curve,
) and a continuous, symmetric random variable Y (dashed
curve, - - -). Functions are offset such that the central value of Y is twice that of X.

16
STATISTICAL DATA ANALYTICS
In certain instances, it may be desirable to modify a skewed probability function in order
to make it appear more symmetric. To do so, one can manipulate the random variable X via
a functional transformation, say Y = g(X) for some given function, g(⋅). For example, the
(natural) logarithmic transform g(X) = log(X) is often employed to reduce heavy right skew
in a positive-valued random variable.
Bivariate and multivariate extensions of probability functions are also possible. For
instance, with two discrete random variables, X and Y, the joint bivariate p.m.f. is fX,Y(k, m)
= P[X = k and Y = m] and the joint bivariate c.d.f. is FX,Y(k, m) = P[X ≤k and Y ≤m].
Individually, X is itself a random variable; its marginal p.m.f. is derived from the joint
p.m.f. by summing over all possible values of Y = m:
fX(k) =
∑
m∈
fX,Y(k, m).
(For the more general multivariate case, see Section 2.2.)
Using conditional probabilities, one can also introduce the concept of a conditional p.m.f.,
that is, the probability that X takes the value k given that Y = m. Denote this as P[X = k|Y = m]
= fX|Y(k|m). Mimicking the construction from the Conditionality Rule in (2b), the conditional
p.m.f. of X given Y is formally
P[X = k | Y = m] = fX|Y(k|m) =
fX,Y(k, m)
fY(m)
.
(2.3)
For two continuous random variables, analogous results apply. The joint p.d.f. is fX,Y(x, y)
and the joint c.d.f. is FX,Y(x, y) = P[X ≤x and Y ≤y]. The marginal p.d.f. of X is found by
integrating Y out of the joint p.d.f.:
fX(x) = ∫
∞
−∞
fX,Y(x, y) dy.
(Reverse the process for the marginal p.d.f. of Y.) Given the marginal p.d.f.s, the conditional
p.d.f.s follow naturally, for example,
fX|Y(x|y) =
fX,Y(x, y)
fY(y)
.
With this, it is possible to describe a version of Bayes’ rule from (2.1) for p.d.f.s:
fX|Y(x|y) =
fY|X(y|x)fX(x)
fY(y)
.
(2.4)
A similar version of Bayes’ rule is available for p.m.f.s by manipulating the relationships
in (2.3).
One can also extend the concept of independent events to random variables. Suppose two
random variables, X1 and X2, exist such that the value of X1 has absolutely no impact on X2,
and vice versa. If so, the two variables are independent. Extending the Multiplication Rule
in (2c) for independent events, the joint probability function for two independent random
variables factors into the marginal components: fX,Y(x, y) = fX(x)fY(y).

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
17
2.1.3
Means, variances, and expected values
The various probability functions for discrete and continuous random variables introduced in
Section 2.1.2 provide general characterizations of a variable’s probability structure. In many
cases, however, it is useful to construct summary measures of the random variable that encap-
sulate its various features. These can be derived from the underlying p.m.f. or p.d.f. The
general form of such a summary measure is called an expected value, and it is based on a
mathematical construct known as an expectation operator, E[⋅]. In its most general usage, the
expected value of a function of a random variable, g(X), is defined as
E[g(X)] =
∑
m∈
g(m)fX(m)
(2.5)
for a p.m.f. fX(m) and
E[g(X)] = ∫
∞
−∞
g(x)fX(x)dx
(2.6)
for a p.d.f. fX(x). That is, expectation involves summation for discrete random variables and
integration for continuous random variables. In effect, the expected value of a function g(X)
is a weighted average of g(X), with weights taken as the probability mass or density of the
random variable X.
Some special features of the expectation operator include
• if g(X) ≥0 for all X, then E[g(X)] ≥0,
• if g(X) ≤h(X) for two functions g(X) and h(X), then E[g(X)] ≤E[h(X)],
• if a ≤g(X) ≤b for two constants a ≤b, then a ≤E[g(X)] ≤b.
See Casella and Berger (2002, Section 2.2).
The simplest example of an expected value occurs for g(X) = X, that is, the expected value
of X. This is known as the population mean of X and is usually given the special notation
𝜇= E[X]. (This is the Greek lowercase letter 𝜇; Greek letters commonly are employed to
denote special statistical parameters.) From (2.5), for a discrete random variable, this is just
𝜇= ∑
m∈mfX(m), while from (2.6), for a continuous random variable, it is 𝜇= ∫∞
−∞xfX(x) dx.
The mean quantifies the central tendency for X and serves as a single, common, summary
descriptor for the value we expect X to take on. (Of course, X is a random variable, so it
can realize any value in the sample space . Indeed, although the mean 𝜇lies between the
minimum and maximum of , it does not necessarily have to equal any specific value in .)
When necessary, a subscript can be used to further identify a random variable with its mean:
E[X] = 𝜇X.
Example 2.1.4 Six-sided die roll (Example 2.1.3, continued). Roll a fair, six-sided die and
let X be the observed number of ‘pips’ as in Example 2.1.3. The p.m.f. was seen to be fX(m) =
1
6 for any m ∈{1, 2, … , 6}. To find the population mean, appeal to (2.5):
𝜇X =
6
∑
m=1
m fX(m) = 1
6
6
∑
m=1
m.

18
STATISTICAL DATA ANALYTICS
Using the well-known relationship ∑n
m=1 m = 1
2n(n + 1), we find 𝜇X =
(
1
6
) (
1
2
)
(6)(7) = 3.5.
As per its design, 𝜇X gives a measure of the ‘central’ value of X, although here 𝜇X = 3.5 is
not actually an element of . It does, however, rest (precisely) in between the smallest and
largest elements of .
◽
In Exercise 2.7, it is shown that E[a] = a for any constant a and that constants can be
brought out of expected value operations, that is, E[aX] = aE[X] for any random variable
X. Also, for the special case of a linear transformation of X, say g(X) = a + bX for any two
constants a and b, the expectation operator is linear: E[a + bX] = a + bE[X] = a + b𝜇X.
While the mean quantifies central tendency of a random variable X, another special form of
expectation quantifies the inherent variation of X. Given E[X] = 𝜇X, let g(X) = (X −𝜇X)2 be
the squared deviation from the mean. Then, the population variance of X is the expected value
of this squared deviation: Var[X] = E[(X −𝜇X)2]. Notice that because this is an expected value
of a nonnegative function, it must also be nonnegative. Standard notation for Var[X] employs
another Greek letter 𝜎. To emphasize the nonnegative aspect the parameter carries a square:
𝜎2
X = Var[X] = E[(X −𝜇X)2]. In practice, it is often more useful to operate with the equivalent
expression 𝜎2
X = E[X2] −𝜇2
X (see Exercise 2.8).
The expectation E[XN] is called the Nth moment of X, so that the variance can be described
as the difference between the second moment and the first squared moment of a random vari-
able. In Exercise 2.9, it is shown that Var[a] = 0 for any constant a and that when brought out
of variance operations, a constant is squared: Var[aX] = a2Var[X] for any random variable X
whose variance exists.
For measuring variation on the original scale of X, the population standard deviation is
defined as the positive square root of the variance: 𝜎X =
√
𝜎2
X.
An alternative summary descriptor for a distribution’s spread is known as the entropy. If
X has p.m.f. or p.d.f. fX(x), then the entropy for fX is given by
H(fX) = −E[log{fX(X)}],
(2.7)
where the expected value is taken with respect to the probability function fX(x). Thus, for
example, if X is continuous, H(fX) = −∫∞
−∞fX(x) log{fX(x)}dx. The continuous version is
often called differential entropy. The discrete case is similar, although many authors will then
use log2 instead of the natural logarithm in the expected value and drop use of the ‘differential’
adjective. (In this case, the entropy is said to be measured in ‘bits.’ If using the natural log, it
is measured in ‘nats.’)
The term ‘entropy’ was coined by Clausius (1865) to describe the amount of disorder – a
better term might be ‘dispersion’ – in a thermodynamic system. Shannon (1948) later devel-
oped and popularized the information-theoretic features associated with (2.7) for describing
loss of data (‘disorder’) in information transmission.
Entropy can be viewed as the extent to which the probability mass or density of X is
localized at a few separated values or dispersed over a wider range. As defined in (2.7), it
increases as fX(x) increases in variability. Thus is it often taken as a measure of dispersion or
heterogeneity.
2.1.4
Median, quartiles, and quantiles
Another useful summarization for a random variable X relates the values it achieves to the
c.d.f., FX(x), at those values. Consider, for example, the c.d.f. at its middle value, 50%.

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
19
One might ask what value of x in the sample space satisfies FX(x) = 1∕2? The point that
does so is called the median of X. More formally, the median is the quantity Q2 ∈where
P[X ≤Q2] ≥1
2 and P[X ≥Q2] ≥1
2. That is, Q2 is the point below which and above which
at least 50% of the probability mass or probability density rests. (The notation ‘Q2’ will be
explained below.)
For most continuous distributions, the median is unique. For many discrete distributions,
however, its defining equations may not be unambiguously satisfied. For example, for a dis-
crete random variable, X may have two adjacent values m1 < m2 such that P[X ≤m1] = 1
2 and
P[X ≥m2] = 1
2. In this case, any value of X between m1 and m2 could be called the median of
X. If this occurs in practice, Q2 is set equal to the midpoint of the interval, Q2 = 1
2(m1 + m2).
Example 2.1.5 Six-sided die roll (Example 2.1.4, continued). Roll a fair, six-sided die, and
let X be the number of ‘pips’ seen on that roll. As seen in Example 2.1.4, the sample space is
= {1, 2, … , 6} and the p.m.f. is fX(m) = 1
6 for any m ∈.
To find the population median, recognize that P[X ≤3] = P[X = 1 or X = 2 or X = 3]
= P[X = 1] + P[X = 2] + P[X = 3] = 0.5, because the events are disjoint. Similarly, P[X ≥
4] = P[X = 4 or X = 5 or X = 6] = 0.5. Thus any value of Q2 between (but not including)
m = 3 and m = 4 would satisfy the definition of a median for X. Simplest here is to let Q2
be the midpoint: Q2 = 1
2(3 + 4) = 3.5. Notice that this is not an element of . As with the
population mean, the population median need not be an element of the sample space.
◽
As it characterizes the ‘center’ of a distribution, Q2 is used as an alternative to the mean
E[X] to measure X’s central tendency. In fact, the two values can be equal – as in Examples
2.1.4 and 2.1.5 – although this is not guaranteed. When a random variable exhibits a large
skew, the median will be less influenced than the population mean by the extreme values in
the skewed tail of the distribution. Thus it can be particularly useful for measuring central
tendency with skewed distributions.
One can extend the concept of a median – that is, the 50% point of a distribution – to any
desired probability point along the range of FX(x). Two obvious values are the 25% and 75%
points. These are known as the first (or lower) and third (or upper) quartiles of the distribution
and are denoted as Q1 and Q3, respectively. (The second or middle quartile is just the median,
Q2, which explains its notation.) Formally, the first (lower) quartile is defined as the point
Q1 ∈such that P[X ≤Q1] ≥0.25 and P[X ≥Q1] ≥0.75. Similarly, the third (upper) quar-
tile is defined as the point Q3 such that P[X ≤Q3] ≥0.75 and P[X ≥Q3] ≥0.25.
From Q1 and Q3, it is possible to derive another measure of variability in the population,
known as the interquartile range: IQR = Q3 −Q1. This is different in structure and interpre-
tation from the variance, Var[X]: the variance measures average squared deviation from the
center (i.e., the mean) of a distribution, while the IQR gives the length of that portion of the
sample space in which the middle half of the probability mass or density lies. While funda-
mentally different, the two measures do share the feature that as they grow larger, the p.m.f.
or p.d.f. is more dispersed.
Quartiles act to separate the distribution of a random variable into equal-probability
fourths (hence, their name). This concept can be applied to any desired separation, so that,
for example, quintiles separate into fifths, deciles separate into tenths, and percentiles
into hundredths. Fully generalized to any desired probability point, the pth quantile of a
distribution is the point qp that satisfies P[X ≤qp] ≥p and P[X ≥qp] ≥1 −p, for 0 < p < 1.
If the c.d.f. of X, FX(x), is continuous and strictly increasing such that it has an inverse
function F−1
X (⋅), the quantiles can be defined by inverting FX(x): qp = F−1
X (p).

20
STATISTICAL DATA ANALYTICS
2.1.5
Bivariate expected values, covariance, and correlation
The expectation operator also can be applied to pairs of random variables. For instance, sup-
pose two random variables X and Y possess a joint p.m.f. fX,Y(k, m). Then for any bivariate
function g(X, Y), the expected value of g(X, Y) is
E[g(X, Y)] =
∑∑
(k,m)∈
g(k, m)fX,Y(k, m).
If instead X and Y possess a joint p.d.f. fX,Y(x, y), the bivariate expected value is
E[g(X, Y)] = ∫
∞
−∞∫
∞
−∞
g(x, y)fX,Y(x, y) dx dy.
Marginal and conditional expectations can also be defined, based on their respective p.m.f.s
or p.d.f.s. For instance, in the continuous case, the marginal mean of Y can be recovered
from the joint probability function of X and Y: simply take g(X, Y) as the univariate function
g(X, Y) = Y and evaluate the joint expectation. For instance, in the joint continuous case,
E[Y] = ∫
∞
−∞∫
∞
−∞
yfX,Y(x, y) dx dy
= ∫
∞
−∞
y ∫
∞
−∞
fX,Y(x, y) dx dy = ∫
∞
−∞
yfY(y) dy,
(2.8)
because fY(y) = ∫∞
−∞fX,Y(x, y) dx. Equation (2.8) ends with the desired expression for 𝜇Y. (The
joint discrete case is similar: simply replace the integrals with corresponding sums.)
One can also develop conditional expected values. For instance, the mean of Y conditional
on X = x is
E[Y|x] = ∫
∞
−∞
y fY|X(y|x) dy.
(2.9)
(Notice that, by construction, when the calculation in (2.9) is finished, E[Y|x] will be a func-
tion of x.) Similarly, the conditional variance of Y, given X = x, can be found as Var[Y|x] =
E[Y2|x] −E2[Y|x], where E2[Y|x] is the square of the conditional mean. An analogous set of
expressions is available for X|y.
An expected value that quantifies the joint variability between two random variables is
known as the covariance. An extension of the univariate variance, the covariance between
X and Y, is defined as 𝜎XY = Cov[X, Y] = E[(X −𝜇X)(Y −𝜇Y)], where 𝜇X = E[X] and 𝜇Y =
E[Y]. Notice that Cov[X, Y] = Cov[Y, X] and in particular Cov[X, X] = Var[X]. This moti-
vates the use of the notation 𝜎XY. Exercise 2.11 shows that if X and Y are independent,
Cov[X, Y] = 0. When Cov[X, Y] > 0, increasing values of X tend to associate with increasing
values of Y, while when Cov[X, Y] < 0, the reverse is true.
Related to the covariance is the correlation coefficient between two random variables. This
is defined as the ratio of the covariance to the product of the marginal standard deviations:
Corr[X, Y] = Cov[X, Y]
𝜎X 𝜎Y
.
(2.10)
Traditional notation for the correlation coefficient is 𝜌= Corr[X, Y] or, if greater specificity is
required, 𝜌XY. It can be shown that while Cov[X, Y] can take on any real value, 𝜌is restricted
to the interval −1 ≤𝜌≤1 (see Exercise 2.12).

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
21
As it is a scaled covariance, the correlation coefficient 𝜌measures association between
two random variables in much the same way as 𝜎XY: when 𝜌> 0, increasing values of X tend
to associate with increasing values of Y, while when 𝜌< 0, the reverse is true. And, if X and Y
are independent, then 𝜌= 0. The converse is somewhat more complicated, however. The cor-
relation is a linear measure of association, so when 𝜌= 0, no linear association is evidenced
between X and Y. The two variables may still be related in, however, say, a quadratic or other
curvilinear manner and, hence, may not be independent. When employing 𝜌to measure asso-
ciation between two variables, it is best in practice to be aware of both its strengths and its
limitations.
2.2
Multiple random variables∗
All of the operations described in Section 2.1 can be applied to a set of n ≥2 random
variables, X1, X2, … , Xn. Generically, the corresponding joint p.m.f. or p.d.f. is denoted by
fX1,X2, … ,Xn(x1, x2, … , xn). Any subset of these n variates, say X1, … , Xj for j < n, possesses
a corresponding joint marginal p.m.f. or p.d.f. fX1, … ,Xj(x1, … , xj), found by summing or
integrating over the remaining variables. For example, in the continuous case, one has
fX1, … ,Xj(x1, … , xj) = ∫
∞
−∞
· · · ∫
∞
−∞
fX1,X2, … ,Xn(x1, x2, … , xn) dxj+1 · · · dxn.
Joint conditional probability functions are also possible; for example, given X1 = x1, … ,
Xj = xj, the joint conditional p.m.f. or p.d.f. of Xj+1, … , Xn is
fXj+1, … ,Xn|X1, … ,Xj(xj+1, … , xn|x1, … , xj) =
fX1,X2, … ,Xn(x1, x2, … , xn)
fX1, … ,Xj(x1, … , xj)
.
If the n random variables X1, X2, … , Xn are independent, an extension of the Mul-
tiplication Rule (2c) provides their joint p.m.f. or p.d.f., fX1,X2, … ,Xn(x1, x2, … , xn) =
fX1(x1)fX2(x2) · · · fXn(xn), or simply
fX1,X2, … ,Xn(x1, x2, … , xn) =
n
∏
i=1
fXi(xi).
(2.11)
A convenient notational device for multivariate random variables is to employ vector
and matrix notation. (See Appendix A for a refresher on vector and matrix terminology.)
The n-variate random vector is X = [X1 · · · Xn]T and its corresponding vector of population
means is 𝛍= [𝜇1 · · · 𝜇n]T. Here, superscript T denotes the transpose of a vector. For assem-
bling together the collection of n variances 𝜎2
i and the 1
2n(n −1) distinct covariances 𝜎ij(i ≠j),
use the variance–covariance matrix (or just covariance matrix)
Var[X] =
⎡
⎢
⎢
⎢
⎢
⎢⎣
𝜎2
1
𝜎12
· · ·
𝜎1n
𝜎21
𝜎2
2
· · ·
𝜎2n
⋮
⋮
⋱
⋮
𝜎n1
𝜎n2
· · ·
𝜎2
n
⎤
⎥
⎥
⎥
⎥
⎥⎦
,
(2.12)

22
STATISTICAL DATA ANALYTICS
where the variances are placed on the main (left-to-right) diagonal and the covariances lie
in the corresponding off-diagonal positions. Notice that because 𝜎ij = 𝜎ji, V = Var[X] is a
symmetric matrix in that V = VT.
To illustrate selected operations with multiple random variables, consider the following
example with sums of random variables.
Example 2.2.1 Linear combinations.
We often have reason to construct linear combina-
tions of random variables, say L = ∑n
i=1 𝑤iXi , for some real values 𝑤i. For instance, when
the 𝑤is satisfy (i) 𝑤i ≥0 and (ii) ∑n
i=1 𝑤i = 1, L is called a weighted average of the Xis, with
weights 𝑤i. Using standard notation, let 𝜇i = E[Xi] and 𝜎2
i = Var[Xi], i = 1, … , n.
As expectation is a linear operator, any linear combination L has expected value
E[L] = E
[ n
∑
i=1
𝑤iXi
]
=
n
∑
i=1
E[𝑤iXi] =
n
∑
i=1
𝑤iE[Xi] =
n
∑
i=1
𝑤i𝜇i,
(2.13)
When the 𝑤is are weights, this shows that the expected value of a weighted average is a
weighted average of the expected values.
In the special case where n = 2 and 𝑤1 = 𝑤2 = 1, L is simply the sum of two random
variables. Clearly then, (2.13) reduces to E[X1 + X2] = 𝜇1 + 𝜇2. Similarly, if n = 2 and
𝑤1 = −𝑤2 = 1, L is the difference between two random variables, with expected value
E[X1 −X2] = 𝜇1 −𝜇2.
The variance of a linear combination is somewhat more difficult to express. Suppose for the
moment that the covariance between Xi and Xj is Cov[Xi, Xj] = 𝜎ij (i ≠j). Then the variance
of the sum Xi + Xj can be shown (Exercise 2.13) to be
Var[Xi + Xj] = Var[Xi] + Var[Xj] + 2Cov[Xi, Xj] = 𝜎2
i + 𝜎2
j + 2𝜎ij.
(2.14)
Including weights 𝑤i and 𝑤j in (2.14) produces
Var[𝑤iXi + 𝑤jXj] = 𝑤2
i 𝜎2
i + 𝑤2
j 𝜎2
j + 2𝑤i𝑤j𝜎ij,
(2.15)
so that, for example, Var[Xi −Xj] = 𝜎2
i + 𝜎2
j −2𝜎ij (notice where the minus sign enters into
the expression).
Expanding (2.15) to any n-variate linear combination yields
Var[L] = Var
[ n
∑
i=1
𝑤iXi
]
=
n
∑
i=1
𝑤2
i 𝜎2
i + 2
n−1
∑
i=1
n
∑
j=i+1
𝑤i𝑤j𝜎ij.
(2.16)
If the Xis are independent, then we know the covariances are zero: 𝜎ij = 0. Thus (2.16) sim-
plifies to
Var[L] =
n
∑
i=1
𝑤2
i 𝜎2
i .
If 𝑤i = 1 for all i and if the random variables are independent, then we see the variance of a
sum is the sum of the variances.
◽
It is also possible to study the limiting or asymptotic behavior of a sequence of n ran-
dom variables as n →∞. For instance, a sequence of random variables X1, … , Xn is said to

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
23
converge in distribution to a target random variable T if the c.d.f.s of the Xis converge in the
limit to the c.d.f. of T. Formally, this is limn→∞FXn(t) = FT(t) at all real values t where FT(t)
is continuous. This concept can be applied to a weighted average (or any sum) of n random
variables as well, in order to study the distribution of the average as its number of components
grows. This will be explored further in Section 2.3.9.
A full study of convergence in probability is beyond the scope here, and interested readers
are referred to advanced treatments in Lehmann and Casella (1998, Section 1.8) or Casella
and Berger (2002, Section 5.5). Indeed, greater detail on all the concepts reviewed above is
available in textbooks on probability and statistics such as Horgan (2009) or Hogg and Tanis
(2010), or the classic theory text by Feller (1968).
2.3
Univariate families of distributions
When a random variable X occurs with a regular structure, it is useful to refer it to a specific
distributional pattern. If this pattern can be formulated as a mathematical function, the distri-
bution is said to belong to a family of such functions. These families typically possess one
or more unknown parameters, such as the population mean 𝜇and variance 𝜎2, that describe
the distribution’s characteristics. Thus it is common to refer to a parametric family of dis-
tributions when specifying the p.m.f. or p.d.f. of X. This section summarizes some important
families, beginning with discrete distributions and moving on to continuous forms. More gen-
eral descriptions on families of statistical distributions are available in dedicated texts such
as Forbes et al. (2010) or the series by Johnson et al. (2005, 1994, 1995, 1997) and Kotz
et al. (2000).
2.3.1
Binomial distribution
The binomial distribution is a basic discrete family used to describe data in the form of pro-
portions. A binomial random variable X is generically constructed as the number of positive
outcomes (or ‘successes’) among N statistically independent, binary ‘trials.’ Each trial is
assumed to produce a success with (constant) probability 𝜋∈(0, 1). The corresponding p.m.f.
takes the form
fX(m) =
(
N
m
)
𝜋m (1 −𝜋)N−mI{0,1, … ,N}(m),
(2.17)
where
(
N
m
)
=
N!
m! (N −m)!
(2.18)
is the binomial coefficient (the number of ways of selecting m items from a collection of N
elements) and m! is the factorial operator
m! = m(m −1)(m −2) · · · (2)(1).
(2.19)
for any positive integer m. Also, define 0! = 1. Notice in (2.17) use of the notation I(m); this
represents the indicator function over the set ,
I(x) =
{
1
if x ∈
0
otherwise,
(2.20)

24
STATISTICAL DATA ANALYTICS
and is an efficient way to specify in the expression for a p.m.f. or p.d.f. the particular sample
space for X.
The notation to indicate quickly that X possesses a binomial p.m.f. is X ∼Bin(N, 𝜋).
(The tilde symbol, ∼, is read as ‘is distributed as.’) If X ∼Bin(N, 𝜋), its population mean is
E[X] = N𝜋and its population variance is Var[X] = N𝜋(1 −𝜋). A special case occurs with
N = 1, that is, a single trial that produces a dichotomous, binary outcome for X. If so, the
binomial reduces to what is known as a Bernoulli distribution. The singleton event producing
the binary count X is often called a Bernoulli trial.
Calculating the binomial p.m.f. in (2.17) or the c.d.f., FX(m) = P[X ≤m] is not
particularly onerous, although when N is larger than about 20, the factorial computa-
tions can become challenging. To facilitate the effort, some statistics textbooks present
tables of the factorial operator and the p.m.f. or c.d.f. The calculations may also be per-
formed conveniently by computer, however, and many statistical computing languages
and packages provide internal functions for binomial probabilities. Most popular are the
R language (http://cran.r-project.org/), the similar S-Plus language (http://csan.insightful
.com/), the SAS® system (http://www.sas.com/), and the IBM SPSS® package (http://www
.ibm.com/software/analytics/spss/). Each has its own advantages and disadvantages for
statistical data analytics; Tufféry (2011, Chapter 5) gives a useful overview. Throughout this
textbook, the R language (R Core Team 2014) is highlighted. (See Appendix B for a short
introduction to R.)
Example 2.3.1 Binomial distribution.
To compute the binomial p.m.f., R provides the
dbinom(m, size, prob) function, where m is the function argument, size is the sample
size parameter N, and prob is the success probability 𝜋in (2.17). The corresponding binomial
c.d.f. is available via R’s pbinom(m, size, prob) function.
To illustrate, suppose X ∼Bin(50, 0.05). With N as large as 50 and 𝜋= 0.05 so close to
zero, direct calculation here of the quantities in (2.17) is challenging. R’s *binom functions
can facilitate the effort. To compute, say, P[X = 6], use dbinom(6, 50, .05). This gives
P[X = 6] = 0.0260.
◽
For intermediate calculations, R also provides the choose(n,m) function for the binomial
coefficient in (2.18) and the factorial(m) function for m!. With large m, use the lfacto-
rial(m) function, giving log(m!) = exp{log(m!)}.
Example 2.3.2 Purchasing probability. A retail outlet samples N = 1024 of its affinity cus-
tomers to ascertain if temporary price reductions (‘sales’) lead to increased purchases. Let X
be the number of those customers who purchase an item during a sale. Suppose the true prob-
ability that a customer would make a sale purchase is 𝜋= 0.50. The outlet manager wants to
know the probability that at least half of those customers will make a purchase.
Assuming the N = 1024 customers make purchases independently of each other and that 𝜋
remains constant among them, X can be taken as binomial: X ∼Bin(1024, 0.5). The manager
wishes to find P[X ≥512]. From the Complement Rule (2d), this is P[X ≥512] = 1 −P[X <
512]. As X is discrete, however, one cannot calculate this as 1 minus the c.d.f. at m = 512.
Instead, recognize that for a binomial random variable, the event {X < 512} is equivalent to
the event {X ≤511}. As a result,
P[X ≥512] = 1 −P[X ≤511] = 1 −FX(511),
where FX(m) is the c.d.f. of X ∼Bin(1024, 0.5).

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
25
With N so large, the c.d.f. here is unwieldy. The calculation is most efficiently performed
by computer. In R, this is available via the pbinom function for the binomial c.d.f.: for
FX(511), use pbinom(511, 1024, 0.5). Then to find P[X ≥512], subtract the result from
1. This is 1 −0.4875, or about a 51.2% chance that at least half of these 1024 customers will
make a purchase.
If the sample is drawn down to, say, only N = 20 affinity customers, the calculation is
somewhat more manageable: for X ∼Bin(20, 0.5), find P[X ≥10] = 1 −P[X < 10] = 1 −
P[X ≤9] = 1 −FX(9), that is,
P[X ≥10] = 1 −FX(9) = 1 −
9
∑
m=0
(
20
m
) (
1
2
)m (
1 −1
2
)20−m
.
While this could be accomplished by hand, the computer is still a speedier alternative: in R,
use pbinom(9, 20, 0.5) and, to find P[X ≥10], subtract the result from 1. This is 1 −
0.4119, or now about a 58.8% chance that at least half of these 20 customers will make a
purchase.
Figure 2.3 plots the Bin(20, 0.5) p.m.f. and shades in the area corresponding to P[X ≥10].
Notice that the p.m.f. is unimodal and symmetric about m = 10, which here is also the
population mean: 𝜇= N𝜋= (20)(0.5) = 10.
◽
The binomial model is a popular choice for settings in which the response is the num-
ber of Bernoulli trials that exhibit some characteristic of interest – generically, a Bernoulli
‘success’ – such as whether or not a customer will make a sale purchase. (Technically, the
possible outcomes are nonnegative counts bounded above by some known integer, N.) In
most instances, the number of trials, N, is fixed in advance while the number of successes, X,
0
5
10
15
20
0.00
0.05
0.10
0.15
m
fX(m)
Figure 2.3
Probability mass function (p.m.f.) for X ∼Bin(20, 0.5) over possible outcomes
m = 0, 1, … , 20. Shaded area is P[X ≥10] as in Example 2.3.2.

26
STATISTICAL DATA ANALYTICS
is the random variable of interest. The response is often reported as a proportion of successful
outcomes, X∕N.
Binomial random variables exhibit an important feature: the family is closed under
addition. That is, suppose X1, X2, … , Xn are independent binomials with sample-size
parameters Ni and (common) response probability 𝜋. Write this as Xi ∼indep. Bin(Ni, 𝜋),
i = 1, … , n. Then, their sum is also binomial: ∑n
i=1 Xi ∼Bin(N+, 𝜋) where
N+ =
n
∑
i=1
Ni .
(The ‘+’ in the subscript is used throughout as shorthand notation for summation over a sub-
scripted index.)
2.3.2
Poisson distribution
For data in the form of discrete, unbounded counts, that is, observations taken over the space of
nonnegative integers ℤ(≥0) = {0, 1, … }, a popular probability model is known as the Poisson
distribution. Given a random variable X as the number of outcomes of some random process,
X has a Poisson distribution if its p.m.f. takes the form
fX(m) = 𝜆m e−𝜆
m!
I{0,1, … }(m),
(2.21)
where 𝜆> 0 is the rate parameter of the distribution. The Poisson p.m.f. is based on a set of
elementary conditions called the Poisson postulates. These describe how random events occur
or ‘arrive’ in a fixed temporal or spatial region at a rate of 𝜆events per unit time or unit area,
respectively. They are summarized as follows:
• Start with no event occurrences/arrivals in the region.
• Assume occurrences in disjoint temporal/spatial subregions are independent.
• Allow the number of occurrences in disjoint subregions to depend only on each subre-
gion’s length, area, or volume (as appropriate).
• Set the occurrence probability proportional to the temporal/spatial length or area (in a
limiting sense, as the length or area goes to zero).
• Allow for no exactly simultaneous occurrences.
If these five postulates hold, then the Poisson p.m.f. can be derived as the probability of observ-
ing a nonnegative random count, X, of occurrences per unit time or space. (For more on the
Poisson postulates from the temporal or spatial perspective, see, for example, Casella and
Berger (2002, Section 3.8) or Piegorsch and Bailer (1997, Section 1.2.5), respectively.)
The mean and variance of a Poisson distribution are E[X] = Var[X] = 𝜆. (Note the mean-
to-variance equality! This is a stringent consequence of the Poisson sampling assumption.)
The reference notation is X ∼Poisson(𝜆).
As with the binomial model, direct calculation of the Poisson p.m.f. or c.d.f. can grow
difficult for large values of m and one often turns to the computer. In R, the pertinent functions
are dpois(m, lambda) and ppois(m, lambda). These are illustrated in the following
example.

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
27
Example 2.3.3 Microprocessor failure. A computer manufacturer uses microprocessor
chips that fail with a low event rate of 𝜆= 2.7 per 100 000 h (or about 11.4 years) of
use. The manufacturer wishes to know if this will lead to high component failure in its
machines. In particular, find P[X > 0], where X = {number of chip failures in 100 000 h
of use} is assumed to take a Poisson distribution with rate parameter 𝜆= 2.7. To find
P[X > 0], apply the Complement Rule (2d): P[X > 0] = 1 −P[X ≤0] = 1 −P[X = 0].
(The latter equality holds because the events {X ≤0} and {X = 0} are identical – Poisson
random variables are only defined over nonnegative integers.) From (2.21), we have
1 −P[X = 0] = 1 −(2.7)0e−2.7∕0! = 1 −e−2.7 = 0.9328. Thus, there is about a 93% chance
that at least one microprocessor will lead to component failure over 100 000 h of use.
In R, this calculation is available via 1 - dpois(0, 2.7), which again returns a prob-
ability of slightly over 93%.
◽
Similar to the binomial distribution, the Poisson family is closed under addition: if Xi ∼
indep. Poisson(𝜆i), i = 1, … , n, then ∑n
i=1 Xi ∼Poisson(𝜆+), where 𝜆+ = ∑n
i=1 𝜆i.
2.3.3
Geometric distribution
Return to the setting where a series of independent Bernoulli trials are observed, each with
binary outcome equal to either 1 (‘success’) or 0 (‘failure’). Assume that the probability of
‘success’ on any trial is held constant at 𝜋∈(0, 1). Now, however, instead of fixing the number
of trials at a known upper limit N – as in Section 2.3.1 – allow the trials to continue until the
first success is observed. Take X as the number of trials up to (but not including) that first
success. The p.m.f. of X is then
fX(m) = 𝜋(1 −𝜋)m I{0,1, … }(m).
(2.22)
This is known as the geometric distribution. The reference notation is X ∼Geom(𝜋).
The mean of a geometric random variable is E[X] = (1 −𝜋)∕𝜋and the variance is
Var[X] = (1 −𝜋)∕𝜋2. Given the simple form of the p.m.f. in (2.22), the c.d.f. here is
especially easy to derive: FX(m) = P[X ≤m] = ∑m
i=0 𝜋(1 −𝜋)i = 𝜋∑m
i=0 (1 −𝜋)i. Recall,
however, the formula for a finite geometric series:
m
∑
i=0
𝜓i = 1 −𝜓m+1
1 −𝜓
(2.23)
for any |𝜓| < 1. Applying this to the geometric c.d.f., one finds FX(m) = 1 −(1 −𝜋)m+1 for
any m = {0, 1, 2, … }.
A warning: some authors alternatively define the geometric as the number of failures up to
and including the first success. Thus the random variable is now defined as a strictly positive
count. (In effect, it is the transformed variable Y = X + 1 in the notation above.) This changes
the form of (2.22) and all its consequent expressions. See, for example, Casella and Berger
(2002, Section 3.2).
2.3.4
Negative binomial distribution
A natural extension of the geometric sampling construction in Section 2.3.3 is to instead let
X be the number of trials up to (but not including) the rth success, for r ≥1. (Continue to

28
STATISTICAL DATA ANALYTICS
hold the probability of success on any trial constant at 𝜋.) In this case, the p.m.f. in (2.22)
generalizes to
fX(m) =
(
r + m −1
m
)
𝜋r (1 −𝜋)m I{0,1, … }(m).
(2.24)
This is known as the negative binomial distribution. The reference notation is X ∼NB(r, 𝜋).
The negative binomial mean is E[X] = r(1 −𝜋)∕𝜋and the variance is Var[X] = r(1 −𝜋)∕𝜋2.
Clearly, when r = 1, the negative binomial reduces to the simpler geometric form. Viewed
this way, the negative binomial family exhibits a special form of closure under addition: if
Xi ∼indep. Geom(𝜋) = NB(1, 𝜋), then ∑r
i=1 Xi ∼NB(r, 𝜋).
Notice that both the geometric distribution and the negative binomial distribution are
defined for nonnegative counts on unbounded sample spaces. Thus in some sense, they serve
as competitors to the Poisson distribution for modeling unbounded count data. This can be
recognized more clearly by redefining their parametric structure. To wit, in (2.24), write the
negative binomial mean as 𝜇= r(1 −𝜋)∕𝜋and also let 𝛿= 1∕r. Then, the p.m.f. becomes
fX(m) = Γ(𝛿−1 + m)
Γ(𝛿−1) m!
(
𝛿𝜇
1 + 𝛿𝜇
)m
1
(1 + 𝛿𝜇)1∕𝛿I{0,1, … }(m),
(2.25)
where
Γ(a) = ∫
∞
0
ta−1e−t dt
(2.26)
is the gamma function. In passing, it is useful to note a recursive relationship that exists for
Γ(a): one can show (via integration by parts from univariate calculus) that Γ(a + 1) = aΓ(a)
for any a > 0. As a consequence, for any positive integer n the gamma function in (2.26)
relates to the factorial operator: n! = Γ(n + 1).
Clearly, in (2.25), both 𝜇and 𝛿are positive-valued parameters. The population mean of
X is of course now E[X] = 𝜇, and the variance becomes Var[X] = 𝜇+ 𝛿𝜇2. This provides a
more flexible mean-to-variance relationship than that exhibited under the Poisson model. As
𝛿→0, the p.m.f. in (2.25) converges to the Poisson p.m.f. in (2.21).
By the way, at 𝛿= 1 (i.e., r = 1), one recovers a corresponding reexpression for the geo-
metric p.m.f. in (2.22) based on its mean 𝜇:
fX(m) =
(
𝜇
1 + 𝜇
)m
1
(1 + 𝜇) I{0,1, … }(m).
(2.27)
One finds, as expected, that E[X] = 𝜇. More interestingly, however, the geometric variance
now can be expressed as a quadratic function of its mean: under (2.27), Var[X] = 𝜇(𝜇+ 1).
2.3.5
Discrete uniform distribution
One of the simplest discrete distributions is the (discrete) uniform distribution. This occurs
when observations are taken with equal probability over a discrete sample space. In its canon-
ical form, the discrete uniform samples over the first N positive integers, so the p.m.f. assigns
equal probability to every element of = {1, 2, … , N}. This is fX(m) = (1∕N)I{1,2, … ,N}(m).
The reference notation for a discrete uniform p.m.f. is X ∼Unif{N}. The mean is quickly
found to be E[X] = 1
2(N + 1), while the variance is Var[X] = (N2 −1)∕12.

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
29
Referring back to the simple six-sided die roll in Example 2.1.3, we see that the random
variable X ={Number of pips}∼Unif{6}. As in Example 2.14, E[X] = 1
2(6 + 1) = 3.5. Now,
however, we also find Var[X] = (62 −1)∕12 = 35∕12.
2.3.6
Continuous uniform distribution
A continuous analog to the discrete uniform distribution in Section 2.3.5 is the continuous
uniform distribution over any interval (a, b). The interval serves as the sample space. The
associated p.d.f. assigns uniform probability density across all of (a, b):
fX(x) =
1
b −a I(a,b)(x).
The reference notation to indicate that X possesses a continuous uniform p.d.f. is X ∼U(a, b).
The mean is E[X] = 1
2(a + b), while the variance is Var[X] = (b −a)2∕12. A special case of
U(a, b) occurs when a = 0 and b = 1, producing a uniform p.d.f. over the unit interval.
The continuous uniform distribution is also referred to by some authors as the rectangular
distribution.
2.3.7
Exponential distribution
After the uniform, perhaps the simplest continuous p.d.f. is associated with the exponential
distribution:
fX(x) = 1
𝛽e−x∕𝛽I(0,∞)(x),
(2.28)
where the rate parameter 𝛽is constrained to be positive. The exponential is a common choice
for positive random variables that represent waiting times between events or certain forms of
lifetime data. The reference notation is X ∼Exp(𝛽).
The exponential mean is E[X] = 𝛽, while the variance is Var[X] = 𝛽2. Notice that this is
yet another quadratic relationship between variance and mean. An interesting property arises
in this case: the ratio of an exponential variable’s population standard deviation to its popula-
tion mean is constant. (This ratio is known as the coefficient of variation, or often just ‘cv.’)
With the exponential p.d.f., cv =
√
Var[X]∕E[X] =
√
𝛽2∕𝛽= 1.
The exponential distribution possesses an intriguing feature when calculating certain con-
ditional probabilities. Recognize that the exponential c.d.f. is
FX(x) = ∫
x
0
1
𝛽e−t∕𝛽dt
= −e−t∕𝛽||
x
0 = 1 −e−x∕𝛽
for any x > 0. Notice then that the area under the upper tail of the p.d.f. beyond a point
t > 0 – often called an ‘upper-tail probability’ – is
P[X > t] = 1 −P[X ≤t] = 1 −FX(t) = e−t∕𝛽,
(2.29)
by application of the Complement Rule (2d). In many reliability and time-to-event applica-
tions, this is viewed as a kind of ‘survival probability,’ that is, the probability of operating
or surviving past a given time t. Now, consider the further, conditional probability that X

30
STATISTICAL DATA ANALYTICS
exceeds some value t, given that it has already exceeded some lesser value u > 0. This is
P[X > t|X > u] which, from the Conditionality Rule (2b), becomes
P[X > t|X > u] = P[X > t and X > u]
P[X > u]
.
(2.30)
But because t > u, the joint event {X > t and X > u} is identical to the event {X > t}. Thus
(2.30) is just P[X > t|X > u] = P[X > t]∕P[X > u]. Now, if X is exponentially distributed,
(2.29) gives P[X > t] = e−t∕𝛽and the conditional probability reduces to
P[X > t|X > u] = P[X > t]
P[X > u]
= e−t∕𝛽
e−u∕𝛽= e−(t−u)∕𝛽
for any t > u (> 0). Lastly, notice from (2.29) that e−(t−u)∕𝛽is just P[X > t −u], leading to
P[X > t|X > u] = P[X > t −u]
(2.31)
for any X ∼Exp(𝛽). In other words, the probability that an exponential random variable will
exceed some survival time t, given that it has already exceeded an earlier survival time u,
depends on the time between t and u but otherwise not directly on u. In effect, an exponential
random variable ‘forgets’ where it has been when considering how much farther it can operate
or last. This is known as the memoryless property of the exponential distribution.
2.3.8
Gamma and chi-square distributions
An extension of the exponential p.d.f. from Section 2.3.7 into a richer, more flexible family is
known as the gamma distribution. The gamma p.d.f. is
fX(x) =
1
Γ(𝛼)𝛽𝛼x𝛼−1 e−x∕𝛽I(0,∞)(x),
(2.32)
where 𝛼> 0 is the shape parameter of the distribution and 𝛽> 0 is the scale parameter
(Casella and Berger 2002, Section 3.3). The reference notation is X ∼Gamma(𝛼, 𝛽). (Some
authors write the gamma p.d.f. in terms of the scale parameter 𝛼and an alternative rate parame-
ter which in the notation above is 1∕𝛽. Users should be careful to identify the parameterization
under which they operate.)
The gamma extends (i) the exponential mean to E[X] = 𝛼𝛽and (ii) the exponential vari-
ance to Var[X] = 𝛼𝛽2. Clearly, when 𝛼= 1, we recover the simpler exponential form.
Example 2.3.4 Ecological monitoring. Ecologists study the health of organisms that inhabit
coastal marine ecosystems. One measure of aquatic health is the biomass of green algae
(Selenastrum capricornutum) at or near sites of ecological contamination. Signs of deceasing
biomass in the algae can indicate potential ecosystem damage.
Observations on algal biomass are positive valued and can skew to the right, for which a
gamma distribution provides a reasonable model (Bailer and Oris 1997). Suppose that under
static conditions, X = {Green algae biomass (in cells/mm)}∼Gamma(100,12). When algal
biomass drops below a ‘sentinel’ level of about 1000 cells/mm, however, this may indicate

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
31
ecological damage. To find P[X ≤1000], appeal to the gamma c.d.f.:
P[X ≤1000] = FX(1000) = ∫
1000
0
1
Γ(100)(12)100 x99 e−x∕12 dx .
(2.33)
The integral in (2.33) could be evaluated directly, although it is just as effective to call directly
on the computer. In R, the gamma c.d.f. is available via the
> pgamma( x, shape=, scale= )
function, where x is the argument of the c.d.f., shape= specifies the shape parameter 𝛼, and
scale= specifies the scale parameter 𝛽. (The scale= specification should always be written
out; R includes an optional rate= specification, ordered before scale=, for users who wish
to parameterize the gamma in terms of shape and rate, 𝛼and 1∕𝛽, instead of shape and scale.)
As applied to (2.33), pgamma(1000, shape=100, scale=12) gives P[X ≤1000] =
0.0413. Thus there is less than a 5% chance of seeing algal biomass levels this low under
normal conditions. A drop in biomass to these levels may well be indicative of a toxic or
otherwise hazardous ecological impact.
Figure 2.4 plots the corresponding p.d.f. The shaded area in the plot represents the target
probability P[X ≤1000]. The p.d.f. is unimodal, with a slight right skew. (The skew is difficult
to see on this scale, but it is present.)
◽
The gamma family possesses a particular form of closure under addition: if Xi ∼indep.
Gamma(𝛼i, 𝛽), i = 1, … , n, then ∑n
i=1 Xi ∼Gamma(𝛼+, 𝛽), where 𝛼+ = ∑n
i=1 𝛼i. In the
special case of 𝛼i = 1 for all i, the gamma variates collapse to exponentials: Xi ∼indep.
0
500
1000
1500
2000
x
fX(x)
Figure 2.4
Probability density function (p.d.f.) for X ∼Gamma(100, 12) in Example 2.3.4.
Shaded area is P[X ≤1000] = 0.0413.

32
STATISTICAL DATA ANALYTICS
Gamma(1, 𝛽) = Exp(𝛽). Then, sums of independent exponentials are gamma distributed:
∑n
i=1 Xi ∼Gamma(n, 𝛽).
An important special case of the gamma p.d.f. occurs when X ∼Gamma
(
𝜈
2, 2
)
, that is,
when 𝛼= 𝜈∕2 and 𝛽= 2 in (2.32). This is known as the 𝜒2 (or chi-square) distribution, where
the parameter 𝜈is usually a positive integer and is referred to as the degrees of freedom (‘d.f.’)
of the distribution. The reference notation becomes X ∼𝜒2(𝜈). The chi-square mean is simply
the degrees of freedom, E[X] = 𝜈, while the variance is twice the mean, Var[X] = 2𝜈. Also, if
Xi ∼indep. 𝜒2(𝜈i), i = 1, … , n, then ∑n
i=1 Xi ∼𝜒2(𝜈+), for 𝜈+ = ∑n
i=1 𝜈i. This follows from
the additive closure (under fixed scale) of the larger gamma family. The 𝜒2 distribution can
also be derived from another important, continuous random variable known as the normal (or
Gaussian) distribution. The normal is introduced in the next subsection, while further features
of the 𝜒2 are discussed in Section 2.3.10.
2.3.9
Normal (Gaussian) distribution
One of the most important continuous distributions used in data-analytic practice is the normal
distribution, also called the Gaussian distribution. It has p.d.f.
fX(x) =
1
𝜎
√
2𝜋
exp
{
−(x −𝜇)2
2𝜎2
}
I(−∞,∞)(x),
(2.34)
where the parameter 𝜇is any real number and the parameter 𝜎2 is strictly positive. Here, the
fixed constant 𝜋is the ratio of a circle’s circumference to its diameter, 3.14159265 …
The parameters 𝜇and 𝜎2 also explicitly describe the population mean and variance, respec-
tively, of the distribution: E[X] = 𝜇and Var[X] = 𝜎2. Standard reference notation is X ∼
N(𝜇, 𝜎2). (Although, readers should be vigilant: some authors denote the normal as N(𝜇, 𝜎),
i.e., in terms of the mean and the standard deviation, not the mean and variance as done here.)
Normal distributions have unimodal, symmetric p.d.f.s that possess a ‘bell’ shape, centered
at 𝜇, and with spread governed by 𝜎2. They also share an important, unifying feature: any
normal random variable can be standardized into a central form. Specifically, if X ∼N(𝜇, 𝜎2),
then Z = (X −𝜇)∕𝜎∼N(0, 1) for any 𝜇and any 𝜎> 0. In this case, Z is known as the standard
normal random variable. The c.d.f. of the standard normal is given a special notation:
Φ(z) = P[Z ≤z] = ∫
z
−∞
1
√
2𝜋
e−1
2 x2dx .
(2.35)
Unfortunately, the integral in (2.35) is intractable, and so it is obtained via numerical approxi-
mation. The resulting values are quite accurate and widely tabulated in many statistics books.
They are also readily available via most statistical computing programs. For example, in R, the
pnorm(z) function directly calculates Φ(z) in (2.35). In fact, pnorm operates like any general
R p* function for c.d.f.s. Thus it can be extended to calculate the c.d.f of any X ∼N(𝜇, 𝜎2):
simply use pnorm(x, mean=𝜇, sd=𝜎).
Also of value in many calculations is the N(0, 1) upper-tail probability, that is, the area
under the upper tail of the standard normal p.d.f. This is just P[Z > z], which from the Com-
plement Rule (2d) is found as 1 −P[Z ≤z] = 1 −Φ(z) for any real argument z. (Obviously,
for a lower-tail probability, one simply appeals directly to the c.d.f.)

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
33
−3
−2
−1
0
1
2
3
z
fZ(z)
Area = α
Area = 1− α
zα
+
Figure 2.5
Standard normal probability density function (p.d.f.) and upper-𝛼critical
point z𝛼.
The standard normal c.d.f. Φ(z) is continuous and strictly increasing. This allows quantiles
to be found by appealing to the inverse c.d.f., Φ−1(p), as described in Section 2.1.4. Often
more useful in practice, however, are the points above which a specified area under the p.d.f. is
achieved, that is, the points that produce a given upper-tail probability. These values are known
as the upper-𝛼critical points of the standard normal p.d.f. and denoted by z𝛼. Mathematically,
they satisfy the relationship P[Z > z𝛼] = 𝛼for some 𝛼∈(0, 1). Similar to the distribution’s
quantiles, upper-𝛼critical points can be found by inverting the c.d.f., if done carefully: area
equal to 𝛼rests to the right of z𝛼, so from the Complement Rule (2d), area equal to 1 −𝛼must
rest to the left. Thus z𝛼also satisfies z𝛼= Φ−1(1 −𝛼). Figure 2.5 illustrates some of these
features.
In R, upper-𝛼critical points from N(0, 1) are found using the command
> qnorm( 1-alpha, mean=0, sd=1 )
where alpha is the desired upper-tail probability. (mean=0 and sd=1 are the default values
in qnorm, so qnorm(1 - alpha) would be sufficient.) One can also force R to perform the
upper-tail calculation directly, via
> qnorm( alpha, mean=0, sd=1, lower.tail=FALSE )
As all normals are unimodal and symmetric about their means, so is the standard normal.
In particular, it is symmetric about its mean, 0. This allows one to manipulate the c.d.f. and
the critical points in a particular manner. For example, if z𝛼is chosen so that area 𝛼rests to
its right on the z-scale, then from the symmetry about 0, area 𝛼must also rest to the left of
−z𝛼on the z-scale. In effect, this establishes the relationship Φ−1(1 −𝛼) = −Φ−1(𝛼), for any
𝛼∈(0, 1).

34
STATISTICAL DATA ANALYTICS
Another interesting feature of the normal distribution connects its standard deviation 𝜎
with its interquartile range (cf. Section 2.1.4): if X ∼N(𝜇, 𝜎2), then IQRX = 2𝜎Φ−1 (3∕4),
which calculates to IQR ≈1.349𝜎(Stuart and Ord 1994, Section 10.11).
Readers are encouraged to explore other, similar facets of the normal distribution’s sym-
metry, its p.d.f., and its c.d.f.
Example 2.3.5 Women’s heights. Random variation in human height is often normally dis-
tributed, at least to a good approximation. Suppose the heights (in inches) of a population of
college-age women are recorded as the random variable X. Take X ∼N(65, 6.25) with popu-
lation mean 𝜇= 65 inches (5 ft, 5 inches) and population standard deviation 𝜎= 2.5 inches.
If a volleyball coach is recruiting women taller than 72 inches (6 ft) to staff a new team, how
likely is she to encounter such a woman in this population?
To answer this question, the target probability is P[X > 72]. Standardizing gives
Z = (X −𝜇)∕𝜎= (X −65)∕2.5 ∼N(0, 1), so that
P[X > 72] = P
[X −65
2.5
> 72 −65
2.5
]
= P
[
Z > 7
2.5
]
= P[Z > 2.8].
The Complement Rule (2d) then gives P[Z > 2.8] = 1 −P[Z ≤2.8] = 1 −Φ(2.8). In R, this
is simply 1 - pnorm(2.8) which produces P[Z > 2.8] = 0.0026. Alternatively, one can
force R to perform the upper-tail calculation directly, via
> pnorm( 2.8, lower.tail=FALSE )
or even avoid the standardization entirely and use
> pnorm( 72, mean=65, sd=2.5, lower.tail=FALSE )
All these give the same 0.0026 probability. In any case, we find that there is less than a
three-tenths of 1% chance in finding a woman taller than 72 inches within this population.
The coach could also reverse the calculation and ask, for what height would he/she find
women at or above the 95th percentile of this population (the ‘top 5%’)? That is, using the
quantile notation from Section 2.1.4, find q0.95 such that P[X > q0.95] = 0.05. Standardizing
here gives
P[X > q0.95] = P
[
X −65
2.5
> q0.95 −65
2.5
]
= 0.05.
(2.36)
But now, recall the notation for the upper-𝛼critical point for Z: P[Z > z0.05] = 0.05. Applying
this to (2.36) shows z0.05 = (q0.95 −65)∕2.5, and solving for q0.95 gives q0.95 = 2.5z0.05+ 65.
To find z0.05, we can use
> qnorm( 0.05, lower.tail=FALSE )
in R, producing z0.05 = 1.64485. Substituting this into the expression for q0.95 leads to
q0.95 = (2.5)(1.64485) + 65 = 69.1121 inches (or slightly over 5 ft, 9 inches) tall. Women
taller than 69.1121 inches are in the upper 5th percentile of this distribution of heights.
◽
Similar to many of the distributions in this section, normal random variables possess clo-
sure properties under addition:
if Xi ∼indep. N(𝜇i, 𝜎2
i ), i = 1, … , n,
then ∑n
i=1 Xi ∼N (∑n
i=1 𝜇i, ∑n
i=1 𝜎2
i
).

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
35
In fact, this extends to any linear combination of independent normals (cf. Example 2.2.1):
if Xi ∼indep. N(𝜇i, 𝜎2
i ), i = 1, … , n,
then L = ∑n
i=1 𝑤iXi ∼N (∑n
i=1 𝑤i𝜇i, ∑n
i=1 𝑤2
i 𝜎2
i
).
Suppose now that each normal is distributed identically as all the others, so that Xi ∼indep.
N(𝜇, 𝜎2), i = 1, … , n. Then, one has
L =
n
∑
i=1
𝑤iXi ∼N
(
𝜇
n
∑
i=1
𝑤i, 𝜎2
n
∑
i=1
𝑤2
i
)
.
(2.37)
In the special case of an equally weighted average, the 𝑤is are all constant and sum to 1:
𝑤i = 1∕n, ∑n
i=1 𝑤i = 1, and ∑n
i=1 𝑤2
i = 1∕n. L is then the arithmetic average of the Xis. When
associated with a random sample of n observations, this is also known as the sample mean
and is given a special notation:
X = 1
n
n
∑
i=1
Xi.
(2.38)
Clearly then, for n independent, identically distributed (‘i.i.d.’) normals Xi ∼i.i.d. N(𝜇, 𝜎2),
one finds X ∼N(𝜇, 𝜎2∕n). That is, the sample mean from a normal random sample is also
normally distributed, with E[X] equal to the original mean 𝜇but with Var[X] equal to the
original variance divided by n. In effect, X recovers the same location information, 𝜇, as any
member of the sample, but it does so with variability reduced by a factor of n (on the variance
scale).
A powerful feature of the normal distribution, and one reason it is so common in data
analytics, is that normality for the sample mean extends to more than just normal samples.
That is, the sample mean of an i.i.d. sample will often possess a normal distribution, at least
approximately. As seen above, this is exactly true for a sample from the normal itself, but it
also encompasses essentially any i.i.d. sample. The result applies as the number of elements
in the sample, n, grows large and is a form of convergence in distribution as described in
Section 2.2. Known as the Central Limit Theorem or ‘CLT,’ it is best stated formally:
The central limit theorem (Lehmann and Casella 1998, Section 1.8). Take an i.i.d. ran-
dom sample of observations, Xi ∼i.i.d. fX(x), from some p.m.f. or p.d.f. fX(x) with finite mean
E[Xi] = 𝜇and finite variance Var[Xi] = 𝜎2, i = 1, … , n. The distribution of the sample mean
X from (2.38) will converge to that of a normal distribution with mean E[X] = 𝜇and variance
Var[X] = 𝜎2∕n as n →∞.
In practice, the CLT tells us that as n grows large,
X ̇∼N
(
𝜇, 𝜎2
n
)
,
where the symbol ̇∼is read ‘is approximately distributed as.’ The approximation’s quality will
vary case by case; for continuous p.d.f.s that are roughly symmetric and unimodal, the CLT
approximation could be roughly valid for sample sizes as low as n = 10. For parent distribu-
tions that are more skewed and/or discrete, however, n must grow much larger, upwards of

36
STATISTICAL DATA ANALYTICS
50 or 60, for the approximation to take hold. (In extreme cases with extremely skewed parent
distributions or with highly limited, discrete p.m.f.s, the sample size requirement can grow to
over n = 100.)
When a sequence of random variables is known to converge to a normal distribution, it is
often of interest to determine if some functional transformation of that random sequence also
converges to normal. In many cases, the answer is yes, and the result is another convergence
theorem from probability theory known as the delta method.
The delta method (Casella and Berger 2002, Section 5.5). Suppose a sequence of ran-
dom variables Xn exists such that
√
n(Xn −𝜃) converges in distribution to N(0, 𝜎2) for some
constant 𝜃and some positive variance term 𝜎2. Then for any function h(𝜃) whose first deriva-
tive h′(𝜃) exists and is not equal to zero, the distribution of
√
n{h(Xn) −h(𝜃)} converges in
distribution to N(0, {h′(𝜃)}2𝜎2).
In effect, the delta method gives us an asymptotic approximation for any function of a
centrally converging sequence of random variables. It is often useful when considering certain
large-sample estimation problems, as discussed in Section 5.3.5.
The univariate normal distribution can also be extended into bivariate and multivariate forms.
In the bivariate case, if two continuous random variables, X1 and X2, are jointly distributed
as bivariate normal, then their marginal (univariate) distributions are Xi ∼N(𝜇i, 𝜎2
i ), i = 1, 2,
with Cov[X1, X2] = 𝜎12. (The covariance, 𝜎12, can be any real number. When it is zero, X1
and X2 are statistically independent, and, of course, vice versa. The former feature is not true
in general.) Thus the bivariate normal is fully described by these five separate parameters,
𝜇1, 𝜇2, 𝜎2
1, 𝜎2
2, and 𝜎12.
The correlation between X1 and X2 is 𝜌12 = Corr[X1, X2] = 𝜎12∕(𝜎1𝜎2). As such, the
bivariate normal distribution may alternatively be described in terms of the five parameters
𝜇1, 𝜇2, 𝜎2
1, 𝜎2
2, and 𝜌12. The bivariate p.d.f. is then written as
f(x1, x2) =
1
2𝜋𝜎1𝜎2
√
1 −𝜌2
12
× exp
{
−
1
2(1 −𝜌2
12)
[
(x1 −𝜇1)2
𝜎2
1
−2𝜌12
(x1 −𝜇1)(x2 −𝜇2)
𝜎1𝜎2
+ (x2 −𝜇2)2
𝜎2
2
]}
I(−∞,∞)(x1)I(−∞,∞)(x2).
(2.39)
Note that any linear combination of the bivariate normal components, L = 𝑤1X1 + 𝑤2X2,
itself possesses a normal distribution. This is L ∼N(𝜇L, 𝜎2
L) with 𝜇L = 𝑤1𝜇1 + 𝑤2𝜇2 and
𝜎2
L = 𝑤2
1𝜎2
1 + 2𝑤1𝑤2𝜌12𝜎1𝜎2 + 𝑤2
2𝜎2
2, using (2.13) and (2.16), respectively.
As in Section 2.2, one can collect these various components together into the bivariate
vector of variates X = [X1 X2]T, with mean vector 𝜇= [𝜇1 𝜇2]T and covariance matrix
V = Var[X] =
[ 𝜎2
1
𝜎12
𝜎12
𝜎2
2
]
.
The reference notation becomes X ∼N2(𝛍, V). This construction can be extended to
any n-vector of normal random variables, X = [X1 · · · Xn]T, now with mean vector

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
37
𝛍= [𝜇1 · · · 𝜇n]T and covariance matrix
V = Var[X] =
⎡
⎢
⎢
⎢
⎢
⎢⎣
𝜎2
1
𝜎12
· · ·
𝜎1n
𝜎21
𝜎2
2
· · ·
𝜎2n
⋮
⋮
⋱
⋮
𝜎n1
𝜎n2
· · ·
𝜎2
n
⎤
⎥
⎥
⎥
⎥
⎥⎦
.
In this case, we say X has a multivariate normal distribution or n-variate normal distribution
and use the reference notation X ∼Nn(𝛍, V). The joint p.d.f. is written using matrix and vector
notation as
f(x1, x2, … , xn) =
1
(2𝜋)n∕2|V|1∕2 exp
{
−1
2(x −𝛍)TV−1(x −𝛍)
}
,
(2.40)
where |V| is the determinant of V. The external R package mvtnorm is useful for computing
multivariate normal probabilities.
2.3.10
Distributions derived from normal
The normal distribution possesses many important applications in data analytics beyond those
introduced in Section 2.3.9. For instance, it is the basis for a number of other distributions.
Among these is the 𝜒2 random variable mentioned in Section 2.3.8. Although the 𝜒2 is by
definition a special case of the gamma distribution, it can also be constructed from the normal.
To do so, start with a single standard normal variate Z1 ∼N(0, 1). It can be shown that the
square of a standard normal is distributed as 𝜒2 with 1 d.f., that is, Z2
1 ∼𝜒2(1). Extending this
to 𝜈independent, squared, standard normal variates and applying the closure of the 𝜒2 under
addition, one finds X = ∑𝜈
i=1 Z2
i ∼𝜒2(𝜈).
Tail probabilities from the 𝜒2 often prove valuable in data analytics. Calculations with the
𝜒2 c.d.f. can be difficult to manipulate directly, however, so as with the standard normal, the
computer is employed. In the R language, the pchisq(x, df) function gives 𝜒2 c.d.f. val-
ues, P[X ≤x], where df are the pertinent 𝜒2 degrees of freedom. The lower.tail=FALSE
option produces upper-tail areas P[X > x]; equivalently, appeal to the Complement Rule (2d)
leads to
> 1 - pchisq( x, df )
for calculating P[X > x].
One can also invert the upper-tail calculation to find upper-𝛼critical points of the 𝜒2
distribution. Mimicking the notation from the standard normal, denote these as the points
𝜒2
𝛼(𝜈) satisfying P[X > 𝜒2
𝛼(𝜈)] = 𝛼for X ∼𝜒2(𝜈). These 𝜒2 critical points are tabulated in
many statistical sources but are also conveniently available via computer. To find 𝜒2
𝛼(𝜈) in R,
use
> qchisq( alpha, df, lower.tail=FALSE )
Figure 2.6 illustrates these features.
Example 2.3.6 𝝌2 distribution. Suppose X ∼𝜒2(13) and it is of interest to calculate P[X >
20.1520]. In R, this is simply

38
STATISTICAL DATA ANALYTICS
> pchisq( 20.152, df=13, lower.tail=FALSE )
producing P[X > 20.1520] = 0.0915. Or, to find the upper-5% critical point from a 𝜒2 distri-
bution with 13 d.f., the R function is
> qchisq( .05, df=13, lower.tail=FALSE )
This yields 𝜒2
0.05(13) = 22.3620.
◽
By combining the standard normal distribution with the 𝜒2, another important, heavily
used, and historically famous statistical distribution is derived. Suppose a standard normal
variate Z ∼N(0, 1) is independent of a separate 𝜒2 variate W ∼𝜒2(𝜈). Then, the ratio
T =
Z
√
W∕𝜈
(2.41)
is distributed as per the p.d.f.
fT(t) =
Γ
(𝜈+ 1
2
)
Γ
(𝜈
2
) √
𝜈𝜋
(
1 + t2
𝜈
)−(𝜈+1)∕2
I(−∞,∞)(t) ,
where 𝜈are the d.f. of the p.d.f. and here the fixed constant 𝜋is 3.14159265 … This is known
as Student’s t-distribution after the work of W.S. Gosset, who wrote under the pseudonym
‘Student’ (Student 1908). (Gosset’s use of the pseudonym and his larger contribution with the
t-distribution has a colorful history; see Zabell (2008).)
0
10
20
30
40
x
χ2 distribution  p.d.f.
χα2(ν)
Area = α
+
Figure 2.6
𝜒2 probability density function (p.d.f.) and upper-𝛼critical point 𝜒2
𝛼(𝜈).

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
39
Reference notation for the t-distribution is T ∼t(𝜈). The mean and variance are E[T] = 0
(if 𝜈> 1) and Var[T] = 𝜈∕(𝜈−2) (if 𝜈> 2), respectively. The t(𝜈) p.d.f. graphs very similar
to the standard normal p.d.f.; both are centered at zero with symmetric, ‘bell’ shapes. The
t(𝜈) p.d.f. has heavier tails than the standard normal, however. As 𝜈→∞, t(𝜈) converges to
N(0,1).
Extensive tables exist to give probabilities and/or critical points from t(𝜈), although these
may also be calculated using computer software. In R, t-distribution tail areas are available
via the pt(t, df) function, where df are the pertinent d.f. By default, this gives lower-tail
areas P[T ≤t]. For upper-tail areas, P[T > t] insert the lower.tail=FALSE option or appeal
to the Complement Rule (2d) and use
> 1 - pt( t, df )
Upper-𝛼critical points follow similarly. That is, to find the point t𝛼(𝜈) such that
P[T > t𝛼(𝜈)] = 𝛼, one can use
> qt( alpha, df, lower.tail=FALSE )
Here, alpha is the targeted upper-tail area, df are the pertinent d.f., and the lower.tail=
FALSE option forces R to calculate the upper-tail critical point. Figure 2.7 illustrates these
features.
Example 2.3.7 t-distribution. Suppose T ∼t(29) and we wish to find the upper-tail prob-
ability P[T > 1.741] = 1 −P[T ≤1.741]. The R operation for this is simply
> 1 - pt( 1.741, 29 )
−4
−2
0
2
4
t
t-distribution p.d.f.
tα(ν)
+
Area = α
Figure 2.7
t-Distribution probability density function (p.d.f.) and upper-𝛼critical point
t𝛼(𝜈).

40
STATISTICAL DATA ANALYTICS
producing P[T > 1.741] = 0.0461.
For upper-𝛼critical points, suppose again T ∼t(29) and now we wish to find its upper-5%
critical point. Use
> qt( .05, 29, lower.tail=FALSE )
to find t0.05(29) = 1.6991, that is, P[T > 1.6991] = 0.05.
◽
One final derived distribution important in statistical analytics is the ratio of two (scaled)
independent 𝜒2 variates. That is, suppose the random variable W1 ∼𝜒2(𝜈1) is independent of
W2 ∼𝜒2(𝜈2). Then we say the ratio
F = (W1∕𝜈1)
(W2∕𝜈2) = 𝜈2W1
𝜈1W2
has an F-distribution with 𝜈1 and 𝜈2 d.f. The reference notation is F ∼F(𝜈1, 𝜈2).
The mean of an F-distribution is E[F] = 𝜈2∕(𝜈2 −2) for 𝜈2 > 2 and for any 𝜈1 ≥1,
while the variance is Var[F] = 2𝜈2
2(𝜈1 + 𝜈2 −2)∕{𝜈1(𝜈2 −2)2(𝜈2 −4)} for 𝜈2 > 4. The
F-distribution is defined only for positive values and its p.d.f. is skewed to the right.
Note that the ordering of the d.f. is critical here: the d.f. associated with the numera-
tor in the original ratio are listed first (and are sometimes called the numerator degrees of
freedom), while the d.f. associated with the denominator in the original ratio are listed last,
(and are sometimes called the denominator degrees of freedom). In fact, if F ∼F(𝜈1, 𝜈2), then
1∕F ∼F(𝜈2, 𝜈1); see Exercise 2.24.
Standard tables exist to give probabilities and/or critical points from F(𝜈1, 𝜈2), although
as above these may be quickly calculated using the computer. In R, F-distribution tail areas
are available via the pf(x, df1, df2) function, where df1 sets the numerator d.f. and
df2 sets the denominator d.f. As with R’s other p* functions, this gives the c.d.f. P[F ≤x].
So, to find the upper-tail area P[F > x] appeal to the Complement Rule (2d) and employ
1 - pf(x,df1,df2). Alternatively, one can force R to perform the upper-tail calculation
directly via
> pf( x, df1, df2, lower.tail=FALSE )
Upper-𝛼critical points follow similarly: to find the point F𝛼(𝜈1, 𝜈2) such that
P[F > F𝛼(𝜈1, 𝜈2)] = 𝛼, use
> qf( alpha, df1, df2, lower.tail=FALSE )
Here, alpha is the targeted upper-tail area, df1 and df2 are the numerator and denominator
d.f., respectively, and lower.tail=FALSE forces R to calculate the upper-tail critical point.
Figure 2.8 illustrates these features.
Example 2.3.8 F-distribution.
Suppose F ∼F(7, 22) and we wish to find the upper-tail
probability P[F > 3.501] = 1 −P[F ≤3.501]. The R operation for this is simply
> pf( 3.501, 7, 22, lower.tail=FALSE )
This produces P[F > 3.501] = 0.0112.
For upper-𝛼critical points, suppose again F ∼F(7, 22) and now we wish to find its
upper-1% critical point. Then,
> qf( .01, 7, 22, lower.tail=FALSE )
yields F0.01(7, 22) = 3.5867, i.e. P[F > 3.5867] = 0.01.
◽

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
41
0
1
2
3
4
5
6
F-ratio
F-distribution p.d.f.
+
Fα(ν1, ν2)
Area = α
Figure 2.8
F-distribution probability density function (p.d.f.) and upper-𝛼critical point
F𝛼(𝜈1, 𝜈2).
As might be expected with these derived distributions, the F, t, and 𝜒2 random variables
are all interrelated. Besides the obvious connections evident in their definitions, one useful
result is that for T ∼t(𝜈), T2 ∼F(1, 𝜈). Or, as 𝜈2 →∞, (𝜈1F) converges to 𝜒2(𝜈1). For other
interesting connections, see Leemis (1986) or Casella and Berger (2002, p. 627)
2.3.11
The exponential family
Many of the distributions described in this chapter may be assembled within a single class of
probability functions known as the exponential family of distributions. This is a rich collection
that can accommodate both discrete and continuous probability functions.
The exponential family is characterized by a summary form for the probability function.
A random variable belongs to the family if its p.d.f. or p.m.f., fX(x), may be written as
fX(x) = exp
{x𝜃−b(𝜃)
a(𝜑)
+ c(x, 𝜑)
}
,
(2.42)
where a(𝜑), b(𝜃), and c(x, 𝜑) are functions of known form. The parameter 𝜃is the (unknown)
natural parameter of the distribution, and the parameter 𝜑> 0 is an additional dispersion
parameter (sometimes alternatively called a scale parameter).
An important, additional constraint on the class in (2.42) is that the support space,
, of X cannot depend on 𝜃or, if it is not a known constant, 𝜑. This is usually indicated
by incorporating an indicator function (2.20) into c(x, 𝜑), such that the indicator does not
depend on 𝜃.

42
STATISTICAL DATA ANALYTICS
Equation (2.42) is actually a special case of a larger family of probability functions of the
form
fX(x) = exp
{t(x)𝜃−b(𝜃)
a(𝜑)
+ c(x, 𝜑)
}
.
When t(x) = x, as in (2.42), we say the function is in canonical form. Also, for many models
the function a(𝜑) simplifies to a(𝜑) = 𝜑∕𝑤, where 𝑤> 0 is a known constant.
In (2.42), the mean 𝜇= E[X] is related to the natural parameter 𝜃via the partial derivative
𝜇= 𝜕b(𝜃)∕𝜕𝜃. Similarly, the variance is expressible in terms of 𝜃and 𝜑:
Var[X] = a(𝜑) 𝜕2b(𝜃)
𝜕𝜃2 .
When the dispersion parameter 𝜑is known, the quantity 𝜕2b(𝜃)∕𝜕𝜃2 is called the variance
function of X, because it incorporates all the unknown aspects of the variance term. With this,
we write
V(𝜇) = 𝜕2b(𝜃)
𝜕𝜃2
= 𝜕
𝜕𝜃
(𝜕b(𝜃)
𝜕𝜃
)
= 𝜕𝜇
𝜕𝜃
to highlight that the variance can be a function of the mean for an exponential family p.m.f.
or p.d.f. Here are a few examples.
Example 2.3.9 Exponential family: Normal distribution. Given its central position in the
pantheon of statistical distributions, it is natural to ask: is the normal p.d.f. from (2.34) a
member of the exponential family in (2.42)? The answer is yes. To see how, write the p.d.f.
as
fX(x) =
1
𝜎
√
2𝜋
exp
{
−(x −𝜇)2
2𝜎2
}
I(−∞,∞)(x)
= exp
{
−log(2𝜋𝜎2)
2
+ log[I(−∞,∞)(x)]
}
exp
{
−x2 −2x𝜇+ 𝜇2
2𝜎2
}
= exp
{x𝜇−1
2𝜇2
𝜎2
−x2
2𝜎2 −log(2𝜋𝜎2)
2
+ log[I(−∞,∞)(x)]
}
.
Decomposed into this form, the natural parameter is 𝜃= 𝜇and the dispersion parameter is
𝜑= 𝜎2 so that a(𝜑) = 𝜑, b(𝜃) = 1
2𝜃2, and
c(x, 𝜑) = −1
2[𝜑−1x2 + log(2𝜋𝜑)] + log[I(−∞,∞)(x)].
Hence, the normal p.d.f. satisfies the class requirement given by (2.42).
Notice here that E[X] = b′(𝜃) = 𝜃= 𝜇and Var[X] = a(𝜑)b′′(𝜃) = (𝜑)(1) = 𝜎2, as
expected.
◽
In Example 2.3.9, the indicator function describing the support space for X ∼N(𝜇, 𝜎2)
was I(−∞,∞)(x) and was used as an explicit component in the function c(x, 𝜑). This was crucial
for identifying the normal p.d.f. as an exponential family form. More generally, when the
indicator function is used to write the p.m.f. or p.d.f. in its fully expressed form, one can check
quickly whether the support of the p.m.f. or p.d.f. is dependent on any unknown parameters.

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
43
If it is, then as noted above X cannot be part of the exponential family. For example, suppose
X ∼U(0,𝜃) for 𝜃> 0. Then the p.d.f. is fX(x) = 𝜃−1I(0,𝜃)(x) and this cannot be written to satisfy
(2.42). This uniform distribution – indeed, any uniform distribution with an unknown lower
or upper limit – is not a member of the exponential family of distributions.
Example 2.3.10 Exponential family: Binomial distribution.
The exponential family is
not restricted to p.d.f.s. Consider the binomial model from Section 2.3.1. The p.m.f. is
fX(x) =
(N
x
)
𝜋x (1 −𝜋)N−xI{0,1, … ,N}(x)
= exp
{
x log(𝜋) + (N −x) log(1 −𝜋) + log
[(N
x
)
I{0,1, … ,N}(x)
]}
= exp
{
x log
(
𝜋
1 −𝜋
)
+ N log(1 −𝜋) + log
[(N
x
)
I{0,1, … ,N}(x)
]}
.
Decomposed into this form, the binomial’s natural parameter is 𝜃= log{𝜋∕(1 −𝜋)} =
logit(𝜋) and the dispersion parameter is (trivially) fixed at 𝜑= 1. Then, fX(x) does satisfy
(2.42), with a(𝜑) = 1, b(𝜃) = −N log(1 −𝜋) = N log(1 + e𝜃), and
c(x, 1) = log
[(N
x
)
I{0,1, … ,N}(x)
]
.
Hence, the binomial p.m.f. is a member of the exponential family.
A technical caveat: when x is in the set = {0, 1, … , N}, the indicator function
I{0,1, … ,N}(x) equals 1 and the function c(x, 1) in the binomial decomposition is well defined.
When x is not in this support set, however, the indicator function is 0, and thus the function
c(x, 1) here attempts to evaluate the natural logarithm of zero. Although this is technically
impossible, we can appeal to a limiting argument for the evaluation: recognize that as its
argument approaches 0, the natural logarithm approaches −∞. Evaluated in the exponent of
the p.m.f., this drives fX(x) to an infinitesimal value, the limiting value of which is itself 0.
This is precisely what the probability mass should be when x is not in the support set.
◽
The exponential family plays a central role in many statistical calculations, and this brief
introduction only scratches its surface. For further explorations into the family, including ways
to extend it for more complex analytic operations, see Brown (1986) or Casella and Berger
(2002, Section 3.4).
Exercises
2.1
Describe the sample space for the following settings:
(a) Record the number of items purchased during a trip to a grocery store by male
consumers.
(b) Observe thickness (in mm) of eggshells for a certain bird species exposed to a
pesticide.
(c) Sample the annual wages paid to employees in a chain of convenience stores.
(d) Record the longitude and latitude where mobile phone calls are initiated.

44
STATISTICAL DATA ANALYTICS
2.2
Identify if the following random variables are discrete or continuous:
(a) Number of items purchased during a trip to a grocery store by a male consumer.
(b) Thickness (in mm) of eggshells for a bird exposed to a pesticide.
(c) Annual wages paid to an employee in a retail store.
(d) Blood concentration (in mmol/L) of glucose in a diabetic hospital patient.
(e) Whether or not a consumer with a certain credit score is awarded a loan.
(f) Time for an electronic component to recover full operating capacity after exposure
to cold.
(g) Initial longitude and latitude of a mobile phone signal.
2.3
Are the following functions valid p.m.f.s? Explain why or why not.
(a) .
m
1
1.3
1.9
2.1
f(m)
1
3
1
4
1
3
1
4
(b) .
m
−1
1
4
5
9
f(m)
1
4
1
8
1
8
1
4
1
4
(c) .
m
1
3
6
7
13
f(m)
0.2
0.5
−0.3
0.4
0.2
(d) For some positive integer N, f(m) =
6m2
N(N + 1)(2N + 1)I{1,2, … ,N}(m).
(e) For some probability 𝜋∈(0, 1),
f(m) =
−𝜋m
m log(1 −𝜋)I{1,2, … ,∞}(m).
2.4
Are the following functions valid p.d.f.s? Explain why or why not.
(a) f(x) = 1
xI(1,∞)(x).
(b) f(x) = 1
2(x3 −1)I(0,2)(x).
(c) For some positive constant 𝛼> 0,
f(x) = 𝛼(𝛼+ 1)x𝛼−1(1 −x)I(0,1)(x).
(d) For two constants 𝜔> 𝛿> 0,
f(x) = 2
𝛿𝜔x I(0,𝛿](x) +
2
𝜔(𝜔−𝛿)(𝜔−x)I(𝛿,𝜔)(x).
(Hint: Start by plotting the function for some valid pairings of 𝛿and 𝜔.)

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
45
2.5
Verify the simple form of Bayes’ rule in (2.1) by recognizing that P[and ] =
P[and ] and then applying the Multiplication Rule in (2c) to each side of this
equality.
2.6
What happens to the conditional p.m.f. of X|Y when X and Y are independent?
2.7
Suppose a and b are constants that exhibit no random variation and X is a random
variable with p.d.f. fX(x). Verify the following indications from Section 2.1.3:
(a) E[a] = a.
(b) E[bX] = bE[X].
(c) E[a + bX] = a + bE[X].
2.8
Suppose X is a random variable with p.d.f. fX(x), finite population mean 𝜇X, and finite
population variance 𝜎2
X.
(a) Verify the indication from Section 2.1.3 that the variance of X can be written as
𝜎2
X = E[X2] −𝜇2
X.
(b) Show that the second moment of X can be written as E[X2] = 𝜇2
X + 𝜎2
X.
2.9
Suppose a and b are constants that exhibit no random variation and X is a random
variable with finite mean 𝜇X and finite variance 𝜎2
X. Verify the following indications
from Section 2.1.3:
(a) Var[a] = 0.
(b) Var[bX] = b2Var[X].
(c) Use these results to find Var[a + bX].
2.10
Return to the six-sided die roll in Example 2.1.4 and calculate the variance of X directly.
Compare this with that calculated from the expression for the variance of a discrete
uniform random variable in Section 2.3.5.
2.11
Show that if X and Y are two independent random variables, then Cov[X, Y] = 0.
2.12
Show that the correlation coefficient, 𝜌, defined in Section 2.1.5 is contained in the
interval −1 ≤𝜌≤1 via the following steps (Casella and Berger 2002):
(a) Let X and Y be two random variables with means 𝜇X and 𝜇Y, variances 𝜎2
X and 𝜎2
Y,
respectively, and covariance 𝜎XY. Define the function
c(b) = E[{b(X −𝜇X) + (Y −𝜇Y)}2]
and show that c(b) can be expanded into c(b) = b2𝜎2
X + 2b𝜎XY + 𝜎2
Y.
(b) Show why c(b) ≥0 for all real values of b.
(c) Recognize that c(b) is a quadratic function in b. Show that it has at most one real
root, that is, at most one real solution to the equation c(b) = 0.
(d) Recall that a quadratic equation with at most one real root must have a nonpositive
discriminant. Find the discriminant for c(b) and set this less than or equal to 0.
(e) Manipulate the expression in the preceding step into −𝜎X𝜎Y ≤𝜎XY ≤𝜎X𝜎Y and
show that this is equivalent to −1 ≤𝜌≤1.

46
STATISTICAL DATA ANALYTICS
2.13
Verify (2.14). (Hint: Var[Xi + Xj] = E[{(Xi + Xj) −(𝜇i + 𝜇j)}2] = E[{(Xi −𝜇i) + (Xj
−𝜇j)}2]. Now expand the square.)
2.14
Let X ∼Bin(10, 0.2). Use direct calculation, published tables (where available), or a
computer to find the following values:
(a) P[X = 6]
(b) P[X ≤2]
(c) P[X ≥1]
(d) P[2 ≤X ≤6]
(e) P[2 < X < 6]
2.15
Let X ∼Poisson(𝜆) for the values of 𝜆given in the following. Use direct calculation,
published tables (where available), or a computer to find the following values:
(a) Find P[X = 3] for 𝜆= 4.95
(b) Find P[X > 0] for 𝜆= 4.95
(c) Find P[4 < X ≤11] for 𝜆= 13.65
(d) Find P[X ≥8.05] for 𝜆= 13.65
(e) Find P[X ≤4] for 𝜆= 0.55
2.16
Suppose X ∼Geom(𝜋) as in Section 2.3.3. Show that X possesses a similar ‘memory-
less property’ as the exponential distribution in (2.31). That is, show that P[X ≥t|X ≥
u] = P[X ≥t −u] for any positive integers t and u such that t > u.
2.17
Let Z ∼N(0, 1). Use published tables (if available) or a computer to find the following
values:
(a) P[Z ≤2.63]
(b) P[Z > 2.63]
(c) P[ |Z| ≤2.63]
(d) P[ |Z| ≥2.63]
(e) z0.025
(f) z0.05
(g) z0.005
(h) z0.01
2.18
Let X ∼N(𝜇, 𝜎2). Use published tables (if available) or a computer to find the following
values:
(a) Find P[X ≤11.82] for 𝜇= 1.3 and 𝜎2 = 16
(b) Find P[X > 5.39] for 𝜇= −2.5 and 𝜎2 = 9
(c) Find P[ |X| ≤11.82] for 𝜇= 1.3 and 𝜎2 = 16
(d) Find P[ |X| ≥5.39] for 𝜇= −2.5 and 𝜎2 = 9
2.19
The Poisson distribution’s closure under addition allows for some useful probability
calculations. Recall the microprocessor reliability problem in Example 2.3.3. In prac-
tice, it might be acceptable for as many as two chips to fail in every 100 000 h of use.

BASIC PROBABILITY AND STATISTICAL DISTRIBUTIONS
47
Suppose an i.i.d. sample of n = 100 chips is taken to study their actual failure occur-
rences and the sample mean X is calculated using (2.38).
(a) Determine the probability that the mean of the sample exceeds two failures, that
is, find P[X > 2]. (Hint: recognize that P[X > a] = P [∑n
i=1 Xi > na], and use the
closure of the Poisson under addition to find the distribution of ∑n
i=1 Xi.)
(b) Appeal to the CLT in Section 2.3.9 to approximate P[X > 2]. (Hint: here, the pop-
ulation mean of X is 𝜆= 2.7 and the population variance is also 𝜆= 2.7.)
2.20
Find the entropy H(fX) from (2.7) under the following distributions.
(a) X ∼Bin(1, 𝜋) (use log2 in place of the natural logarithm).
(b) X ∼U(0,𝜃) and in particular X ∼U(0, 1).
(c) X ∼Exp(𝛽). Plot H(fX) as a function of 𝛽> 0.
(d) X ∼N(0, 𝜎2). Plot H(fX) as a function of 𝜎> 0.
2.21
Let X ∼𝜒2(𝜈) for the values of 𝜈given in the following. Use published tables (if avail-
able) or a computer to find the following quantities:
(a) Find P[X > 16.38] if 𝜈= 8
(b) Find P[X ≤1.8] if 𝜈= 10
(c) Find P[X > 17.1] if 𝜈= 10
(d) Find P[1.8 ≤X ≤17.1] if 𝜈= 10
(e) Find 𝜒2
0.01(𝜈) if 𝜈= 5
(f) Find 𝜒2
0.05(𝜈) if 𝜈= 5
(g) Find 𝜒2
0.05(𝜈) if 𝜈= 15
(h) Find 𝜒2
0.05(𝜈) if 𝜈= 25
2.22
Let T ∼t(𝜈) for the values of 𝜈given in the following. Use published tables (if avail-
able) or a computer to find the following quantities:
(a) Find P[T ≤2.63] for 𝜈= 4
(b) Find P[T > 2.63] for 𝜈= 4
(c) Find P[ |T| ≤2.63] for 𝜈= 13
(d) Find P[ |T| ≥2.63] for 𝜈= 13
(e) Find t0.025(𝜈) for 𝜈= 4
(f) Find t0.05(𝜈) for 𝜈= 4
(g) Find t0.05(𝜈) for 𝜈= 11
(h) Find t0.05(𝜈) for 𝜈= 33
(i) Find t0.05(𝜈) for 𝜈= 88
2.23
If you only had access to a table or computer program of F-distribution critical points,
how could you use it to find t𝛼(𝜈)?
2.24
Let F ∼F(𝜈1, 𝜈2).
(a) Show that 1∕F ∼F(𝜈2, 𝜈1).
(b) Show that F𝛼(𝜈1, 𝜈2) = 1∕F1−𝛼(𝜈2, 𝜈1).
2.25
Let F ∼F(𝜈1, 𝜈2) for the values of 𝜈given in the following. Use published tables (if
available) or a computer to find the following quantities:

48
STATISTICAL DATA ANALYTICS
(a) Find P[F ≤1.9] if 𝜈1 = 13, 𝜈2 = 28
(b) Find P[F > 3.4] if 𝜈1 = 21, 𝜈2 = 9
(c) Find P[F ≥6.2] if 𝜈1 = 1, 𝜈2 = 4
(d) Find F0.02(1, 4)
(e) Find F0.05(1, 4)
(f) Find F0.05(8, 7)
(g) Find F0.01(3, 49)
2.26
Show that the Poisson p.m.f. in (2.21) is a member of the exponential family in (2.42).
2.27
Return to the negative binomial p.m.f. in Section 2.3.4.
(a) For the standard parameterization in (2.24), set r = 4. Show that the corresponding
p.m.f. is a member of the exponential family in (2.42).
(b) For the standard parameterization in (2.24), assume the parameter r is any known,
positive integer. Show that the corresponding p.m.f. is a member of the exponential
family in (2.42).
(c) What does the result in Exercise 2.27b tell you about the Geometric p.m.f. and how
it relates to the exponential family in (2.42)?
(d) For the redefined parameterization in (2.25), set 𝛿= 2. Is the corresponding p.m.f. a
member of the exponential family in (2.42)?
2.28
It is important to emphasize the difference between the univariate exponential distri-
bution from Section 2.3.7 and the larger exponential family in Section 2.3.11. The
former is a standalone model for a particular continuous p.d.f., while the latter is an
entire class of distributions. Nonetheless, the exponential p.d.f. in (2.28) is a member
of the exponential family in (2.42). Prove this.
2.29
Suppose a continuous random variable X has the following p.d.f.
fX(x) = 𝛽𝛾𝛽
x𝛽+1 I(𝛾,∞)(x)
for 𝛽> 0 and 𝛾> 0. This is known as the Pareto distribution.
(a) Find E[X].
(b) Find Var[X].
(c) Assume 𝛾is a known positive value. Show that the corresponding p.m.f. is a mem-
ber of the exponential family in (2.42).

3
Data manipulation
The probability theory described in Chapter 2 lies at the core of any statistical calculation. It
is, however, only a preliminary step in conducting a data-analytic exercise. In this chapter, a
brief introduction is given to basic data manipulation. The goal is to provide (and/or review)
the fundamental building blocks of statistical summarization. As previously, readers familiar
with these concepts may wish to skip forward to Chapter 4 and its introduction to basics
of data visualization, or on to Chapter 5 and its discussion of the more-advanced aspects of
statistical inference.
3.1
Random sampling
As seen with the various statistical distributions in Section 2.3, a random variable X is typically
characterized in terms of one or more parameters. At its basic level, a parameter is a quantity
that describes a critical feature of a random variable. For example, the normal distribution
in Section 2.3.9 has two parameters: the population mean 𝜇and the population variance 𝜎2.
These are sufficient to completely characterize the normal probability density function (p.d.f).
In most data-analytic settings, the parameters are unknown and must be estimated. To
do so, we take a random sample of observations, X1, X2, … , Xn, from the population. The
sample size, n, is usually known and fixed in advance. In the simplest case, the observa-
tions are all taken from the same ‘identical’ distribution and are statistically independent of
each other. Standard notation for this is Xi ∼i.i.d. fX(x), where fX(x) is the probability mass
function (p.m.f.) or p.d.f. of X. (As in Section 2.3.9, ‘i.i.d.’ is shorthand for ‘independent,
identically distributed’.) If the specific p.m.f. or p.d.f. is from one of the families in Section
2.3, then ‘fX(x)’ is replaced by the further shorthand notation for that distribution; for example,
i.i.d. sampling from a normal distribution is indicated by Xi ∼i.i.d. N(𝜇, 𝜎2), i = 1, … , n.
A fundamental requirement of any random sample is that it be representative of the pop-
ulation under study. For instance, if a data mining study on skin cancer randomly samples
patient records in southern California, would that sample necessarily be representative of can-
cer patients throughout the United States? Perhaps not. Subjects in California often experience
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

50
STATISTICAL DATA ANALYTICS
increased sun exposure; the geographically constrained sampling may provide data on skin
cancer of little value to oncologists in, say, central Alaska.
Left unrecognized, distorted or haphazard sampling can introduce severe biases into the
data, restricting the scope of the corresponding statistical inferences. In order to avoid system-
atic distortion in the sampling process, the population must be sampled in a random, unbiased
manner. The primary focus in this chapter is on the i.i.d. case, often referred to as simple
random sampling or a simple random sample (SRS). An SRS recruits subjects or units from a
larger population such that each subject has an equal chance of being selected. Also, under the
independence criterion, we require each sampled observation to have no impact or influence
on any other sampled observation.
Wherever possible, the SRS must avoid (or find some way to account for) the impact of
peripheral or superfluous factors and should include blinding to avoid potential investigator
bias. Unfortunately, even with maximal care and effort, control of every possible extraneous
factor cannot always be achieved. To balance out uncontrolled systematic effects in a sam-
ple, we employ randomization, the random assignment of subjects or units to the sample,
before any distinguishing conditions or treatments are applied (Fisher 1926). The concept is
straightforward: before imposition of some treatment or intervention, the subjects are allo-
cated so that each subject has the same chance of being assigned to each treatment level. This
process is standard in designed experiments, such as laboratory experiments or prospective
clinical studies, but may be more difficult to implement fully if the study is observational in
nature.
Example 3.1.1 Advertising study. Consumer interest in new-product advertising is often
assessed by evaluating panels of subjects’ interest in different advertising-campaign designs.
In a controlled experiment, the subjects are asked to view one of T > 1 increasingly complex
designs planned for the advertising program.
Assume that, due to an oversight, the subjects are not randomized in their assignment to
the advertising designs. Suppose that the first 10 selected subjects are assigned to the current,
existing design (a form of control group); the next 10 subjects are assigned to the simplest
new design, the next 10 to the next more-complex design, and so on until the last 10 sub-
jects are assigned to the most complex design. Now, suppose further that younger subjects are
selected first and, unbeknownst to the investigator, younger subjects tend to favor the newer,
more-complex advertisements. As a result of this naïve allocation, the youngest subjects will
assess the least-complex designs, slightly older subjects will assess designs of intermediate
complexity, and this will continue in a systematic manner until the older subjects assess the
most complex designs. The likely result of this assignment is that older subjects will respond
less favorably to the newer material and vice versa. This can grossly underestimate the adver-
tising’s impact on the population at large.
Clearly, such a nonrandom allocation can produce misleading inferences. A better scheme
would employ complete randomization, where subjects are assigned randomly to the var-
ious designs, in order to vary the subjects’ particular advertising exposures in a random,
less-systematic manner.
◽
In general, to assign subjects or units randomly to T different treatment levels, one must
use a random device such as a random number generator. From this, one assigns labels or
identifiers to each subject and uses these labels to assign each subject randomly to a treatment
group. Random number generators are common in most statistical packages and programs

DATA MANIPULATION
51
(one can also find hard-copy random number tables in many older statistics textbooks). For
example, in R, the runif(n, min=a, max=b) command produces n uniform random num-
bers in the interval from a to b. The default is a=0 and b=1, producing random numbers
from U(0, 1). (Technically, these are pseudorandom numbers, because the proffered random
variates are computed via a deterministic algorithm. Modern algorithms employ very clever
generators, however; when properly applied, they can mimic true randomness quite effectively
(Gentle 2003, Chapter 1).]
Many other strategies can be applied for forming a random sample. As mentioned in
Chapter 1, some approaches can stratify or otherwise adapt the sampling to meet highly spe-
cific target needs; see Thompson (2012) or Lohr (2010) for greater detail.
3.2
Data types
The types of data available for mining and informatic study are as varied as the underlying
distributions from which they are generated; perhaps even more so. The fundamental feature
of a single data point distinguishes whether or not it is a number: a random observation is
quantitative if it represents a true number, amount, or other quantitative characteristic. Data
that are not quantitative are called qualitative; these often represent categories of some status,
such as ‘healthy versus ‘diseased’, or ‘accepted’/‘pending’/‘declined,’ and so on. Of course,
one can always quantify a qualitative outcome when necessary, for example, use a coding
such as ‘accepted’ = 1, ‘pending’ = 2, and ‘declined’ = 3. These will still be arbitrary labels,
however, if the quantification does not represent a true numerical separation between the val-
ues. For example, if ‘pending’ = 2 and ‘declined’ = 3, does ‘declined’ actually represent
one additional unit of measure (or, for that matter, 50% more) above ‘pending’? Likely not;
indeed, the numerical labels could be reversed here with no loss or disruption of information.
It is important to keep this distinguishing feature in mind when operating with qualitative
data: just because a datum is given as a number does not always mean it imparts quantitative
information.
A data point is called nominal if it describes basic categories and essentially nothing
more. Both qualitative examples in the previous paragraph represent nominal data. In fact,
truly quantitative data are not typically nominal, because the quantitative feature imparts
additional information (see the following text). A quantitative, nominal variate is usually a
qualitative observation that has been coded with an arbitrary numbering scheme, such as the
‘accepted’/‘pending’/‘declined’ illustrated earlier.
A refinement to nominal data occurs when the outcomes occupy some sort of ordered
scale. This is called ordinal data. Ordinal data are common when a complete quantification
is not possible, but a natural ordering nonetheless exists among the outcomes. For instance,
a cancer study may classify patients over progressing stages of a disease, from ‘healthy’ to
‘mild,’ though ‘moderate,’ to ‘severe’ (with possible substages in between). Or, questionnaires
often employ the famous Likert Scale (Likert 1932) on respondent attitudes for a statement or
product: for example, 1 = ‘completely dissatisfied’ to 5 = ‘completely satisfied’, and so on.
One way of distinguishing ordinal data from more-intricate numerical values is that ordinal
outcomes cannot be referenced to any unambiguous ‘absolute zero’ point.
Qualitative categorical data on an ordered scale are often quantified into ordinal quan-
titative values. In some cases, however, the ordinal data can also originate as quantitative
values with no underlying categories other than the actual numbers themselves. In either

52
STATISTICAL DATA ANALYTICS
case, however, assigning more than an ordinal interpretation to a quantitative ordinal variate
is inappropriate.
When ordinal data exist on a scale where the degree of difference between them is mean-
ingful, they are called interval data. A classic example is the Fahrenheit temperature scale:
the one-unit difference between 10∘and 11∘F is interpreted identically as that between 90
and 91∘F, so a unit difference has meaning. There remains no unambiguous ‘absolute zero’
point, however: interval data represent only a ‘next step’ in the numerical progression. (There
is such a thing as 0∘F, but this is just an arbitrary point on that particular scale. Note that
this also holds true for the Celsius temperature scale. There too, 0∘C is essentially arbitrary:
while it is defined as the freezing point of water at standard atmospheric pressure, any other
‘freezing point’ could instead have been employed to define the scale.)
When data are observed as continuous, quantitative measurements on an interval scale
where an unambiguous zero point exists, they are known as ratio data. Technically, a ratio
datum indicates the degree of difference between the recorded value and a unit value of
the same measure. So, for example, 4 g of mass is twice as much as 2 g of mass. Indeed,
most physical measurements such as length, energy, and elapsed time are ratio data. One
can even measure temperature on a ratio scale: use the Kelvin scale where absolute zero is
really absolute zero. Here, for example, 20∘K is truly twice as ‘hot’ as 10∘K, in contrast to
Fahrenheit or Celsius.
For quantitative data, an alternative and more-critical delineation distinguishes between
discrete and continuous variables, mimicking the characterization with probability functions
in Section 2.1.2. Indeed, sampling from a discrete p.m.f. produces a discrete observation,
while sampling from a continuous p.d.f. produces a continuous observation. One can also
mix the two features when sampling bivariate or multivariate data; for example, a subject in a
pharmaceutical study can simultaneously provide the continuous measurements U = {blood
concentration of the drug} and V = {age} along with the discrete indicators W = {disease
status} and X = {sex}. As might be imagined, instances of mixed data are not uncommon in
informatic studies. In the following sections, focus will be on calculations and manipulations
with quantitative data, although some operations with qualitative data will also be mentioned
when pertinent.
3.3
Data summarization
Just as it is useful to construct measures that summarize features of a population’s distribution,
as in Section 2.1.3, it is also useful in data analytics to construct measures that summarize
the data from a random sample. Indeed, many of these measures are sample analogs of the
population quantities seen in Chapter 2. Throughout this section, assume that a random sample
of data is taken as Xi ∼i.i.d. fX(x), i = 1, … , n, from some p.m.f. or p.d.f. fX(x).
3.3.1
Means, medians, and central tendency
Most readers will be familiar with the simple arithmetic average of a set of numbers,
X = 1
n
n
∑
i=1
Xi.
(3.1)

DATA MANIPULATION
53
Table 3.1
A selection (smallest and largest values) of wheat kernel
length (in mm) from a larger data set of n = 210 observations.
4.899
4.902
4.981
· · ·
6.581
6.666
6.675
Source: Charytanowicz et al. (2010).
and indeed, this was introduced as the sample mean in Section 2.3.9 when discussing the
distribution of an average of normal random variables. The sample mean is useful for far more
than normal random samples, however, because it is a sample analog of the population mean
𝜇= E[X]. It is a natural and intuitive way to measure the central value or central tendency of
most any random sample.
Example 3.3.1 Wheat kernels. In an agricultural study of wheat grain characteristics
(Charytanowicz et al. 2010), data were taken on a number of grain measurements, including
X = {kernel length in mm}. A sample of n = 210 wheat kernels produced the data in
Table 3.1. (Owing to the size of the data set, a selection of only the smallest and largest mea-
surements is given in the table. The complete data are available at http://archive.ics.uci.edu/
ml/datasets/seeds.) To find the sample mean of the n = 210 observations, add up the total
and divide by n. Here, this is ∑n
i=1 Xi∕n = 1181.992/210 = 5.6285 mm. For very large data
sets, the calculation can become tedious, however, so we often turn to the computer. In the R
statistical language, the sample mean is available via a number of functions. The simplest of
these is mean(x), where the single set of n observations is collected into the sample vector
x. Applying this to the vector klength containing all 210 observations from this data set
corroborates the calculation above:
> mean( klength )
[1] 5.628533
One can also verify the individual components of the calculation: total/(sample size). In
R, this is simply sum(x)/length(x), where the sum(x) function gives ∑n
i=1 Xi and the
length(x) function gives the number of elements in x.
> sum( klength )
[1] 1181.992
> length( klength )
[1] 210
> sum( klength )/length( klength )
[1] 5.628533
◽
The mean X can be extended into a weighted average of the sample observations. For a
prespecified set of weights 𝑤i ≥0, let
X𝑤=
∑n
i=1 𝑤iXi
∑n
i=1 𝑤i
.
(3.2)

54
STATISTICAL DATA ANALYTICS
If the weights are normed such that ∑n
i=1 𝑤i = 1, this is similar to the linear combination in
Example 2.2.1. One recovers the sample mean in (3.1) when 𝑤i = 1∕n for all i = 1, … , n.
Weighted averages are most useful when the observations have been sampled under some
form of heterogeneity, so that certain Xis should receive more ‘weight’ than others when sum-
marizing central tendency in X.
While it is a standard and accepted way to quantify the center of a data set, the sample
mean X has a substantial weakness: it can be sensitive to excessively large (or excessively
small) values in a random sample. A classic example is with data on household incomes: in
the United States, for example, most households report annual income in a range of about
$25 000–$85 000, but a few households report annual incomes in the millions and even bil-
lions of dollars. This is an example of a large right skew and is a data analog to the population
concept of skew introduced in Section 2.1.2. In that section, a more-resilient (or robust) mea-
sure of central tendency for skewed distributions was given as the population median Q2, that
is, the point below which and above which at least half of the density or mass rests. An analog
for random samples is the sample median, denoted as ̂Q2. (Statisticians often add a circumflex
accent (̂) above a parameter to indicate that it is an estimate based on sample observations.)
If a sample median lies far to the left (right) of the corresponding sample mean, a large right
(left) skew may be evident in the larger sample.
To find the sample median for a set of data, {X1, X2, … , Xn}, first, order the observations
from smallest to largest. The collection of all n-ordered observations is called the order statis-
tics of the random sample. Specialized notation for this is X(1) ≤X(2) ≤· · · ≤X(n); that is,
parentheses around the index on an observation, X(i), indicate that it is the ith-ordered obser-
vation in the sample. For example, the sample minimum is X(1), while the sample maximum
is X(n). The sample median is then that order statistic resting between the lower half and the
upper half of the sample. If n is odd, this is simply the
(
n+1
2
)
th-order statistic. If n is even,
mimic the operation with discrete-population medians in Section 2.1.4 and take the sample
median as the midpoint between the
(
n
2
)
th- and
(
n
2 + 1
)
th-order statistics. That is,
̂Q2 =
⎧
⎪
⎨
⎪⎩
X([n+1]∕2)
if n is even
1
2{X(n∕2) + X([n∕2]+1)}
if n is odd.
(3.3)
Example 3.3.2 Wheat kernels (Example 3.3.1, continued). If an SRS contains only a hand-
ful of observations, ordering the values and finding their median via (3.3) is not difficult. The
effort can grow in complexity as n gets large, however. Again, the computer facilitates the
calculation.
For example, return to the wheat kernel length data in Table 3.1. Direct calculation via (3.3)
requires one to order the data and, because n = 210 is even, find the 210
2 = 105th- and 210
2 + 1
= 106th-ordered values. To create the order statistics for the observations in the vector
klength in R, use sort(klength). From these, we find X(105) = 5.52 and X(106) = 5.527,
with midpoint (5.52 + 5.527)∕2 = 11.047/2 = 5.5235.
To compute ̂Q2 with a single command, however, the pertinent R function is simply
median():
> median( klength )
[1] 5.5235

DATA MANIPULATION
55
Notice that with median(), there is no need to order the observations first – the function
internally instructs to R do it for us!
In any case, we find ̂Q2 = 5.5235 mm for these kernel lengths. This compares with the
slightly larger value of X = 5.6285 mm from Example 3.3.1. These features of the data are
explored in further examples in the following.
◽
Another summary statistic, resilient to skew, for measuring central tendency is called a
trimmed mean. The concept is simple: if the extreme values of the sample are felt to be unstable
or unreliable for determining the central tendency of the data, ‘trim’ the smallest k
2 and largest
k
2 observations from the sample and calculate the mean on the remaining n −k observations:
XT(k) =
1
n −k
n−k
2
∑
i= k
2 +1
X(i).
(3.4)
(The even trimming constant, k, is specified a priori.) Trimming need not be symmetric. If it
is known in advance that the data possess greater instability in one tail of the sample, we may
wish to trim away a larger fraction from that tail. That is, trim the smallest k1 values and the
largest k2 ≠k1 values from the sample before calculating the mean (Barnett and Lewis 1995,
Section 3.2.1).
XT(k) is often referred to in terms of its trimming fraction: the ‘100 k
n% trimmed mean.’
Preference for this fraction varies: common choices usually rest between 1% and 5% trim-
ming, and up to 10% if the data are rich enough to support that much excision. Of course,
when n is small, trimming can remove a nontrivial amount of data; the trimming fraction
must be chosen purposefully.
Example 3.3.3 Wheat kernels (Example 3.3.1, continued). Trimming is accomplished in
R by options in the mean() function: mean(x, trim=), where x is the data vector and the
trim= option accepts any proportion up to 50%. The trimming is symmetric.
For the wheat kernel data in Table 3.1, the sample size is large enough to accommodate
comfortably a trim of up to 10%. For example, with k = 10 in (3.4), we take a 9.5% trimmed
mean:
> mean( klength, trim=0.095 )
[1] 5.599884
We see that XT(10) = 5.5999 mm, which is intermediate to both the standard sample mean
of X = 5.6285 mm and the sample median of ̂Q2 = 5.5235 mm calculated earlier. Roughly
speaking, we find that these wheat kernels average about 5.5–5.6 mm in length.
◽
A modification to trimming that retains a total of n observations in the sample is known
as Winsorizing: rather than simply excising the lower and upper k
2 observations from the data,
replace them with the values of the observation closest to those retained in the data. That
is, if after k-fold trimming the smallest retained observation is X({k∕2}+1) and the largest is
X(n−{k∕2}), the Winsorized mean is
XW({k∕2}) =
kX( k
2 +1
) + ∑n−k
2
i=k∕2+1 X(i) + kX(n−{k∕2})
n
.

56
STATISTICAL DATA ANALYTICS
The Winsorized mean typically decreases variability in the data and buffers the effects of
untoward observations in the tails, while still providing a ‘sample’ of n observations for sum-
marizing central tendency.
It is important to warn that the entire concept of removing or changing portions of the
data should be approached with caution. It is from the information in the data that any con-
clusions will be drawn, and removing or modifying that information will in some way affect
descriptive or inferential quality. Operations such as trimming can improve the stability of an
unstable or substandard data and will in some instances improve the eventual information they
provide. If performed indiscriminately, however, post-sampling data manipulations can just
as often – and, perhaps, more often – detrimentally affect the larger analytic enterprize. One
must always undertake such operations with a clear, conscious understanding of their effects
and consequences.
3.3.2
Summarizing variation
As with measures of central tendency, measures of variability in a random sample mimic
the corresponding population measures in Section 2.1. Thus the sample variance is a sort of
average squared deviation from the mean, using sample information:
S2 =
1
n −1
n
∑
i=1
(Xi −X)2,
(3.5)
where X is given by (3.1). The division by n −1 in the denominator of S2 occurs because
there are, in effect, only n −1 components of quantifiable information in the sum. (See
Exercise 3.7.) For operations on the same scale as the original data, use the sample standard
deviation: the positive square root of the sample variance, S =
√
S2.
The variance in (3.5) has convenient computing forms
S2 =
(∑n
i=1 X2
i
) −nX
2
n −1
=
(∑n
i=1 X2
i
) −1
n
(∑n
i=1 Xi
)2
n −1
,
(3.6)
either of which is usually faster to compute than (3.5). In R, the sample variance of a vector x
is available via the var(x) function; the corresponding sample standard deviation is sd(x).
Example 3.3.4 Wheat kernels (Example 3.3.1, continued). For the wheat kernel length
data in Table 3.1, X = 5.6285. To compute the sample variance, the calculations necessary for
S2 = ∑210
i=1 (Xi −5.6285)2∕209 are not trivial. Using R, however, we quickly find
> var( klength )
[1] 0.1963052
and
> sd( klength )
[1] 0.4430635
that is, S2 = 0.1963, with S = 0.4431. Direct calculation via (3.6) confirms these values.
◽
In some settings, it is useful to quantify an observation, Xi, in terms of how it relates to
the sample mean, for example, is Xi very much larger than X or is it very near but slightly

DATA MANIPULATION
57
smaller than X, and so on. Distance is always relative, however: if variation – as measured by
S2 – is very small, then points very far from X represent real deviations, whereas if S2 is large,
then large deviations from X may not be as meaningful. To represent this quantitatively, data
analysts use what is called a z-score. Given an observation Xi from a sample of size n with
mean X and variance S2
X, the z-score for Xi is the centered and scaled variate
Zi = Xi −X
SX
.
(3.7)
(Notice that this is a unitless quantity.) In effect, Zi measures the standard-deviation-scaled
distance of Xi from its sample mean.
A z-score near zero indicates that the original observation was very near to X; a z-score
near ±1 indicates that the observation is one standard deviation away from X; a z-score near
±2 indicates that the observation is two standard deviations away; and so on. A general
rule-of-thumb with symmetric data concentrated about their mean is that about 68% of a sam-
ple’s z-scores will lie between ±1, about 95% of the z-scores will lie between ±2, and about
99.7% of the z-scores will lie between ±3. (Correspondingly, about 68% of the sample data
will lie between X ± S, 95% will lie between X ± 2S, and 99.7% will lie between X ± 3S.)
This is known as the empirical 68–95–99.7% rule.
Another approach to estimate variation employs ordered measures similar to the sam-
ple median from Section 3.3.1. For example, the sample quartiles mimic their population
counterparts from Section 2.1.4: the first (or lower) sample quartile is the point in the data
below which one-quarter of the Xis lie and above which three-quarters of them lie. Denote
this as ̂Q1. Similarly, the third (or upper) sample quartile is the point in the data below which
three-quarters of the Xis lie and above which one-quarter of them lie. Call this ̂Q3. As might
be expected, the median is also known as the middle quartile, ̂Q2.
Given the sample quartiles, the sample interquartile range (IQR) is then ̂
IQR = ̂Q3 −̂Q1.
This is the sample analog to the population IQR in Section 2.1.4, with comparable interpre-
tation as a measure of spread. Indeed, if Xi ∼i.i.d. N(𝜇, 𝜎2), i = 1, … , n, then ̂
IQR∕1.35 is
an alternative estimator for 𝜎.
A useful way to assemble these various statistics together is known as the five-number
summary. Colloquially, the five-number summary for a set of data, {X1, … , Xn}, is given as
the collection {X(1), ̂Q1, ̂Q2, ̂Q3, X(n)}, that is, the minimum, three quartiles, and maximum of
the data. (This has a useful graphical analog known as a ‘boxplot,’ as seen in Section 4.1.2)
From the five-number summary, the sample IQR is easily calculated. A slight modification to
this construction replaces the two outer quartiles with the so-called hinges: the lower hinge is
the median of the lower half of the data, while the upper hinge is the median of the upper half.
In some instances, the hinges will equal the outer quartiles, but this is not guaranteed. In R,
the command fivenum() returns a five-number summary made up of the minimum, lower
hinge, median, upper hinge, and maximum of the data.
For a more-general pth quantile, ̂qp, one can define the sample (or empirical) quantile
function using the order statistics, although this need not be unique. Perhaps the simplest
definition is
̂qp = X(i)
if
i
n ≤p < i + 1
n
.
(3.8)
Ostensibly, the lower and upper sample quartiles are then ̂Q1 = ̂q0.25 and ̂Q3 = ̂q0.75, respec-
tively. The definition in (3.8) produces a discontinuous function of p, however, and can be

58
STATISTICAL DATA ANALYTICS
difficult to work with in practice. Other competitors can be constructed for defining the sam-
ple quantiles, most of which use a weighted average of adjacent order statistics. Some even
build continuous empirical quantile functions. Thus for the same set of data, one computer
program’s lower quartile may not match exactly with another’s, although they will gener-
ally be very close. (Users should read the help or manual pages for any software to be sure
that they are calculating the desired quantiles.) R includes up to nine different definitions
in its quantile(x, type=) function, each a slight variant of the others. (Use the type=
option to change them; type=7 is the default, giving a continuous quantile function based
on an intricate weighted average of consecutive order statistics. type=1 corresponds to (3.8).
See help(quantile) in R for more information.) A complete survey of quantile estima-
tion exceeds the scope here; interested readers may refer to, for example, Davis and Steinberg
(2006).
Example 3.3.5 Wheat kernels (Example 3.3.1, continued). All these summary quantities
can be produced by a series of commands in R. For instance, the summary() command gives
the (colloquial) five-number summary along with the mean. For the wheat kernel data in
Table 3.1, this is
> summary( klength )
Min. 1st Qu.
Median
Mean
3rd Qu.
Max.
4.899
5.262
5.524
5.629
5.980
6.675
As noted earlier, the fivenum() command replaces the outer quartiles with the hinges (and
drops the sample mean):
> fivenum( klength )
[1] 4.8990
5.2620
5.5235
5.9800
6.6750
The quantile() command reproduces the summary() output but drops the sample mean:
> quantile( klength )
#default is quartiles
0%
25%
50%
75%
100%
4.89900 5.26225 5.52350 5.97975 6.67500
In its more-general form, quantile(x, probs=) gives any pth quantile, via the probs=
option (the argument of which can even be a vector of probabilities). For instance, the four
sample quintiles of the data are
> quantile( klength, probs=c(0.2, 0.4, 0.6, 0.8) )
#quintiles
20%
40%
60%
80%
5.2198 5.3962 5.7014 6.0680
Lastly, for the IQR, use the eponymous IQR() function or calculate directly from any of these
other outputs:
> IQR( klength )
[1] 0.7175
> quantile(klength)[4] - quantile(klength)[2]
0.7175
◽

DATA MANIPULATION
59
3.3.3
Summarizing (bivariate) correlation
One often collects random samples where bivariate pairs of observations are recorded on
each subject, say, (Xi, Yi), i = 1, … , n. In this case, interest may center on summarizing the
association in evidence between the two outcome variables. A measure of bivariate associa-
tion useful for such settings is the population correlation coefficient from Equation (2.10). Its
sample analog is known as the Pearson product-moment correlation coefficient
rXY = SXY
SXSY
=
n∑
i=1
(Xi −X)(Yi −Y)
√
n∑
i=1
(Xi −X)2
n∑
i=1
(Yi −Y)2
,
(3.9)
where the sample covariance is
SXY =
1
n −1
∑n
i=1(Xi −X)(Yi −Y).
Notice that both rXY and SXY are symmetric measures; for example, rXY = rYX.
Numerous computing forms exist for (3.9); these include
rXY =
∑n
i=1 XiYi −nXY
√(∑n
i=1 X2
i −nX
2) (∑n
i=1 Y2
i −nY
2)
and
rXY =
1
n −1
n
∑
i=1
zxizyi ,
where zxi = (Xi −X)∕SX and zyi = (Yi −Y)∕SY are the individual z-scores for each Xi and Yi
in the paired sample (i = 1, … , n).
The sample correlation coefficient rXY was developed by Karl Pearson (1896). It is an
effective measure for quantifying association between paired observations and has become
one of the most widely used (and, not infrequently, misused) summary statistics in data
analytics.
As with the population correlation coefficient, the sample version in (3.9) satisfies −1 ≤
rXY ≤1, and it measures linear association between the paired variables. When rXY = 0, the
data suggest no apparent relationship between Xi and Yi, while when |rXY| = 1, a perfect linear
relationship is in place between them. In the latter case, a plot of Yi versus Xi would produce a
perfectly straight line, with positive slope if rXY = 1 and negative slope if rXY = −1. (Summary
graphics for bivariate relationships are discussed in Section 4.2.)
Example 3.3.6 College admissions. The admissions department of a large US public uni-
versity studied the association between different measures used to evaluate undergraduate
applicants. Among other variables, the admissions officers collected X = {High school class
rank (as a percentile; higher percentiles indicate higher rank)} and Y = {ACT score} in a

60
STATISTICAL DATA ANALYTICS
Table 3.2
Selected data pairs (X, Y) with X = {High school class rank (%)} and Y = {ACT
score}, from a larger set of n = 705 observations.
X = Class rank (%)
61
84
74
95
47
· · ·
97
97
99
Y = ACT score
20
20
19
23
23
· · ·
29
29
32
sample of n = 705 students from its incoming freshman class. (The ACT is a standardized
test provided by the American College Testing program and used by many US colleges and
universities for quantifying academic achievement. The overall score studied here ranges from
a minimum of 1 to a maximum of 36. See http://www.act.org.)
The paired data, from Kutner et al. (2005, Appendix C), appear in Table 3.2. (As above,
a selection of only the smallest and largest measurements is given in the table. The complete
data are available at http://www.wiley.com/go/piegorsch/data_analytics.)
For univariate summary statistics, we find that the average student’s class rank percentile
is X = 76.9532%, while the average ACT score is Y = 24.5433. Also, SX = 18.6339 and
SY = 4.0136.
Of interest with these data is whether and to what degree the two admissions criteria
associate. (If there were very strong association between the two – negative or positive – then
perhaps only one of the two criteria would be needed for admissions decisions. On the other
hand, if the association is weak, the two measures may provide complementary information.)
For such a large data set, computer calculation is indicated: in R, the correlation coefficient
from (3.9) is available via the cor() function. This takes two required inputs, the names of
the two paired variables (order is unimportant). Here, R gives
> cor( rank, ACT )
[1] 0.4425075
Thus for these 705 students, the sample correlation is rXY = 0.4425. This is a moderate level
of positive correlation, suggesting that while the two measures do associate – as might be
expected – the differences between them are substantial enough to warrant continued, joint
consideration.
◽
As the sample correlation is a measure of linear association, it can provide misleading
inferences if the underlying relationship between X and Y is more complex. For example, if
the two variables relate in a tight quadratic or other curvilinear manner, rXY may calculate close
to zero, but this does not mean they are unrelated. Issues of how to explore and understand
bivariate relationships more fully are discussed in Chapter 6.
3.4
Data diagnostics and data transformation
3.4.1
Outlier analysis
As mentioned towards the end of Section 3.3.1, there are occasions when one or more data
points in an SRS may not be truly representative of the population from which the sample was
drawn. This could be due to some sort of contamination in the sample, an unrecognized bias
in the sampling scheme, a recording error, or just an unusual outlier in the data. Indeed, the
term ‘outlier’ is often used for situations such as this, usually to indicate an observation that
appears inconsistent with the pattern(s) in the remainder of the data (Barnett and Lewis 1995,

DATA MANIPULATION
61
Table 3.3
A selection (smallest and largest values) of average daily net
carbohydrate consumption (in grams) from a larger data set of n = 778
observations.
43
64
75
75
· · ·
402
407
437
738
Chapter 1). When associated with detection of anomalies in a data stream – for instance, indi-
cation of false insurance claims or of fraudulent credit card purchases – outlier identification
becomes an important data mining tool (Hand et al. 2001, Section 2.7).
The specification of what constitutes an ‘outlier’ can be made more formal. We say a
potential outlier in a random sample {X1, X2, … , Xn} is any point Xi satisfying
Xi < 1
or
Xi > 3,
where
1 = ̂Q1 −(1.5)(̂
IQR)
and
3 = ̂Q3 + (1.5)(̂
IQR),
(3.10)
are called the inner fences of the data set. (For ‘outer fences,’ replace 1.5 with 3.0 in the
definition.) Essentially, any data point lying outside the ‘fences’ defined by extending the outer
quartiles by 150% of the IQR distance is a potential outlier (Hoaglin et al. 1986). When so
identified, the datum is worthy of closer inspection to determine if it represents an abrogation
in the sampling process or other form of discordant observation.
In passing, note that many alternative conditions can be used to define an ‘outlier.’ Some
authors employ the ‘outer’ fences to define an outlier (then often distinguished as an ‘excessive
outlier’). Others say an outlier is any point whose z-score from (3.7) exceeds 3.0 (or 4.0, or
6.0, etc.) in absolute value. See Barnett and Lewis (1995) for a comprehensive discussion.
Example 3.4.1 Carbohydrate intake. A biomedical research team studied carbohydrate
intake in a population of Caucasian males between the ages of 45 and 55. The sub-
jects were participating in a weight reduction/maintainence program. Recorded was the
average daily intake of net carbohydrates (total carbohydrates −dietary fiber) in grams,
from a simple random sample of n = 778 individuals. The data (sanitized to remove any
identifying information) appear in Table 3.3. (As above, a selection of only the small-
est and largest measurements is given in the table. The complete data are available at
http://www.wiley.com/go/piegorsch/data_analytics.)
Assume that the data are collected into the R vector carbs. For summary statistics, we
find
> mean( carbs )
[1] 221.4357
> quantile( carbs )
0%
25%
50%
75%
100%
43.00 192.00 215.00 245.75 738.00
Clearly, X is larger than ̂Q2 here, but only by about 3%. So, no gross skew in the data is
indicated. Next,
> IQR( carbs )
[1] 53.75

62
STATISTICAL DATA ANALYTICS
> quantile(carbs)[2] - 1.5*IQR(carbs)
#lower fence
25%
111.375
> quantile(carbs)[4] + 1.5*IQR(carbs)
#upper fence
75%
326.375
We find that the outer quartiles are ̂Q1 = 192 and ̂Q3 = 245.75, producing ̂
IQR = 53.75
and (inner) fences of 1 = 192 −(1.5)(53.75) = 111.375 and 3 = 245.75 + (1.5)
(53.75) = 326.375. Current recommendations for average daily carbohydrate intake vary
between 180 and 300 g, so any observations lying outside these fences could be considered
potential outliers in this data set and deserving of further attention.
Indeed, a number of observations exceed these bounds. (A fast way to proceed in R is to use
the language’s subsetting feature, via carbs[ carbs<f1 ] and carbs[ carbs>f3 ].)
Ten observations lie below 1 and 24 lie above 3. Of these, the most excessive outlier is the
largest observation at X(778) = 738 g. This is far above the next-largest observation at X(777) =
437 g, which itself is of some interest: these are the only two data points lying outside of the
‘outer’ fences for these data and, hence, may be worthy of closer inspection.
◽
Once an outlier has been identified, the more-challenging question is, how should it be
treated? Automatized removal of any datum simply because it exceeds a numerical outlier
standard is poor statistical practice and poor analytics. Careful authors use the term ‘potential
outlier’ for good purpose: a point that seems unusual, for whatever reason, deserves additional
study to determine whether and why it disturbs larger patterns in the data. It is this study,
however, that should guide further operations with (or without) a questionable data point. For
instance, in the carbohydrate intake data from Example 3.4.1, the clearly anomalous observa-
tion of 738 g may indicate an individual not suited for the larger weight-loss study, or someone
who is younger than the target age group (and thus consumes more in terms of average daily
carbohydrates), or even just a data entry error. Or, it may be a perfectly reasonable, if extreme,
realization of the process under study and should be retained. Only closer inspection of the
actual datum can clarify the matter. (After further inspection of the potential carbohydrate
intake outliers in Example 3.4.1, no disqualifying features were identified for those subjects:
as far as could be determined they simply consumed larger-than-recommended amounts of
carbohydrates, on average, during the data acquisition period.)
3.4.2
Entropy∗
If a particular data point (or set of points) has been identified for potential removal from
the data set, it may be of interest from an exploratory data analysis (EDA; cf. Section 1.3)
perspective to quantify how much or even whether homogeneity in the sample improves after
excision. To do so, one can turn to sample diagnostics. For instance, the sample entropy is
analogous to the population entropy H(fX) described in Section 2.1.3. Just as H(fX) quantifies
the ‘disorder,’ or more properly, the dispersion or heterogeneity in a distribution, its sample
estimator can be used to quantify the heterogeneity in a set of data. Many different estimators
have been proposed for this process (Beirlant et al. 1997); here the focus is on a nonparametric
estimator based on lagged separations between the sample order statistics. (The estimator

DATA MANIPULATION
63
is ‘nonparametric’ in that, to construct it, no specific parametric family from Section 2.3 is
assumed for the distribution of X.)
Given the order statistics X(1) ≤X(2) ≤· · · ≤X(n), let X(i+m) −X(i−m) be the differences
between the (i + m)th- and (i −m)th-ordered values for some positive lag parameter m < n
2.
With these, Vasicek (1976) described the preliminary estimator
̂HV = 1
n
n
∑
i=1
log
{ n
2m
[X(i+m) −X(i−m)
]}
,
(3.11)
where one truncates to X(j) = X(1) in (3.11) for any index such that j < 1 and to X(j) = X(n)
for any j > n. Vasicek identified a systematic bias with this simple estimator, however, and
proposed a corrected version for practical use. Take
̂H′
V = ̂HV −log
( n
2m
)
−(n −2m)𝜓(2m)
n
+ 𝜓(n + 1) −2
n
m
∑
j=1
𝜓(j + m −1),
(3.12)
where 𝜓(t) is the digamma function
𝜓(t) = d log{Γ(t)}
dt
= Γ′(t)
Γ(t) ,
that is, the derivative of the log of the gamma function in Equation (2.26) (Spouge 1994). For
specifying m, Hampel (2008) recommended a default lag of m = 4. If many ties exist among
the data points, set m to the smallest integer larger than 4 such that X(i+m) −X(i−m) > 0 for all i.
Example 3.4.2 Carbohydrate intake (Example 3.4.1, continued). The ̂H′
V statistic in
(3.12) is complex enough to demand calculation by computer. In R, direct programming is
necessary, although the internal digamma(t) function facilitates the effort. The sort()
command is also useful, with its ability to order the data and produce the order statistics. The
following is a sample R function.
>
HVprime = function( x , m=4 ) {
xo <- sort( x )
n <- length( xo )
n2m <- n/(2*m)
xup <- rep( xo[n],n )
xlo <- rep( xo[1],n )
for ( j in 1:(n-m) ) xup[j] <- xo[j+m]
for ( j in (1+m):n ) xlo[j] <- xo[j-m]
HV <- mean( log(n2m*(xup-xlo)) )
bias <- log(n2m) + (1-(1/n2m))*digamma( 2*m )
- digamma( n+1 ) + (2/n)*sum(digamma( (1:m)+m-1) )
return( HV - bias )
#return HVprime statistic
}
#end function
For the carbohydrate intake data in Table 3.3, numerous ties exist among the interior data
points. For any m ≤6, this produces cases where X(i+m) −X(i−m) = 0 for some values of i.
Thus, take m = 7. The estimated entropy for all n = 778 entries is then found as ̂H′
V = 5.1775.

64
STATISTICAL DATA ANALYTICS
Removing the extreme observation at X(778) = 738 g drops the sample entropy to
̂H′
V = 5.1565 (at m = 7). This is a small decrease in entropy – less than 1% – suggesting that
this single large observation may not substantially affect heterogeneity in the larger sample.
Going further and removing all observations greater than 400 g drops the entropy down to
̂H′
V = 5.1365 (at m = 7). Again, not a large improvement for decreasing sample heterogeneity.
◽
Given its information-theoretic origins (cf. Section 2.1.3), the sample entropy can be
viewed as an index of the information in a sample: as heterogeneity or variation increases,
information decreases. Thus in a certain sense, small values of ̂H′
V indicate greater informa-
tion. Another measure of sample information known as the Fisher information number will
be discussed in Section 5.1.
3.4.3
Data transformation
In some instances, a data set may not contain any extreme outliers or may have potential
outliers that do not detrimentally affect homogeneity of the sample, but it may still exhibit a
large skew that hinders summary calculations. If so, another approach for managing the data
is to transform the observations into a more operable form. For example, as mentioned in
Section 2.1.2, the (natural) logarithmic transform Yi = log(Xi) often is used to reduce heavy
right skew in positive-valued observations.
In general, a data transformation is a mathematical function, Yi = g(Xi), applied to the
original observations in order to achieve a more stable, target characteristic in the trans-
formed data. The log transform is clearly Yi = g(Xi) = log(Xi). Other common transforma-
tions include
• the square root transform, g(Xi) =
√
Xi (for Xi ≥0)
• the reciprocal transform, g(Xi) = 1∕Xi (for Xi ≠0)
• the logit transform, g(Xi) = log
{
Xi
100−Xi
}
(for percentages between 0 and 100)
• the simple arc-sine/square-root transform, g(Xi, Ni) = arcsin
√
Xi∕Ni (for proportions,
Xi∕Ni).
Note that an alternative form of the arc-sine/square-root transform for proportions is
g(Xi, Ni) = arcsin
√
√
√
√
√
Xi + 3
8
Ni + 3
4
.
An entire class of transforms that contains many of these individual forms is known as the
Box–Cox power transformation (Box and Cox 1964)
g𝜆(Xi) =
X𝜆
i −1
𝜆
.
(3.13)
Ignoring constants, the square root and reciprocal transforms obtain at 𝜆= 1
2 and 𝜆= −1,
respectively; the log transform corresponds to the limiting form as 𝜆→0 (Exercise 3.18).
Also useful in certain cases are integer powers such as the quadratic and cubic forms: 𝜆= 2

DATA MANIPULATION
65
and 𝜆= 3 in (3.13). If desired, one can estimate 𝜆directly from the data, although this can
produce unintended instabilities in the transformed variates; users should apply caution when
estimating a power transformation parameter. See Carroll and Ruppert (1988) for more on this
and other issues regarding data transformations.
Example 3.4.3 Carbohydrate intake (Example 3.4.1, continued). For the carbohydrate
intake data in Table 3.3, a transformation could be useful for attenuating the extremely large
observations in the upper tail of the sample. As the data are strictly positive, either the square
root or logarithm are likely candidates. Consider the former: take Yi =
√
Xi. The transformed
data corresponding to the displayed values in Table 3.3 become
6.56, 8.00, 8.66, 8.66, … , 20.05, 20.17, 20.90, 27.17.
Deceases in the large separation among the upper (and lower) observations is clearly
evident. Indeed, the sample entropy is also much smaller with the square-root-transformed
Yis: ̂H′
V = 1.7958 at m = 7.
The logarithmic transform with these data is explored in Exercise 3.16.
◽
Example 3.4.4 College admissions (Example 3.3.6, continued). For the paired College
Admissions data in Table 3.2, X = {Class rank} is recorded as a percentage and, therefore,
is naturally bounded between 0% and 100%. This can at times lead to certain instabilities
in the measure; for example, X = 76.9532 is rather smaller than the sample median ̂Q2 =
81. This suggests a left skew with those data. As noted above, a popular transform with per-
centage data is the logit: g(X) = logit(X) = log{X∕(100 −X)}. Applying the logit transform
to the class ranks provides substantial attenuation in the skew (try it!). The sample entropy
is also much smaller with the logit-transformed Xis than with the original class ranks in Xi:
̂H′
V = 4.087 for the original values versus ̂H′
V = 1.693 for the logits, both at m = 38 due to the
large number of ties.
Recomputing the correlation coefficient between g(X) = logit{Rank} and Y = ACT score
yields a slightly higher sample correlation than that seen in Example 3.3.6: in R, we find
> cor( log(class.rank/(100-class.rank)), ACT )
[1] 0.4596824
◽
3.5
Simple smoothing techniques
Summarization efforts with very large data sets can at times fall victim to the ‘signal-versus-
noise’ problem: high variability (‘noise’) can swamp an underlying trend (‘signal’), making
it difficult to describe or identify pertinent features in the data. Indeed, in the modern age of
ever-increasing, ‘big’ data, noise-to-signal ratios often grow with increasing information, as
the breadth of data can overwhelm our ability to process underlying connections contained
within (Silver 2012). While outlier detection or data transformation may alleviate this concern
in select instances, such adjustments may still prove ineffective with very high noise-to-signal
ratios. A possible solution then involves so-called smoothing of the data, that is, aggregating
or averaging locally similar observations to de-noise (‘smooth’) the data and expose or extract
their core features.

66
STATISTICAL DATA ANALYTICS
3.5.1
Binning
Perhaps the simplest way to smooth a set of data {X1, … , Xn} is to assemble the Xis into
a collection of G > 1 adjacent groups or ‘bins.’ By compressing highly noisy data into G
bin means, Xg, bin medians, ̂Q2g, or frequencies (counts) of observations in each bin, say ̂fg,
one can often obtain smoother summary information on the phenomenon under study. (Here,
g = 1, … , G indexes the individual bins.) Also, some data-analytic methods may only operate
on discretized or categorized data, whereby some form of discrete binning may be necessary
(Myatt 2007, Section 3.4.4). One might also view the binning as an optimization problem,
where the data are distributed among the G bins to minimize the average squared (or absolute,
etc.) distance between each Xi and its bin mean or median. The algorithm for doing so would
be heavily computational, however (Kantardzic 2003, Section 3.5).
A natural concern when aggregating into collective bins is whether a loss of information
occurs, due to apparent discretization of the original data. This is possible in some cases, and
care must be taken not to ‘oversmooth’ or otherwise mask the target features of interest (Kuss
2013). At a certain level, smoothing is as much art as it is science.
If the break points between each bin are clearly indicated from the subject matter, then
the binning is straightforward. (For example, person-age is often separated into 10-year bins.)
If the break points are not prespecified, however, then the simplest and often most-propitious
approach allocates the observations equally among the G bins. Thus, for example, if G = 4,
select breaks at the three sample quartiles ̂Qj (j = 1,2,3).
Binning is perhaps most useful when applied to produce frequency counts, ̂fg, from uni-
variate data. For example, suppose interest centers on estimating distributional features of the
Xis such as their p.d.f. or p.m.f. A familiar graphical device for visualizing a p.d.f. or p.m.f. is
known as a histogram (described more fully in Section 4.1.4), where the ̂fg values are plotted
as a vertical bar chart against the binned categorizations of the Xis. The number of bins, G,
and the break points can be selected in many different ways; however, careful construction
is required, in order to properly visualize the shape of the distribution. A common default is
known as Sturges’ rule (Sturges 1926): G = ⌈log2(n) + 1⌉, where ⌈x⌉is the ceiling operator,
that is, the smallest integer greater than or equal to x. The break points are then constructed to
give G bins of equal width between the smallest and largest observations, usually using round
numbers for simple bin separators.
Sturges’ rule works well with data that center near their mean and vary symmet-
rically around it, that is, unimodal with little skew. An alternative for more general
use is given by Scott (1979) and is known as Scott’s normal reference rule: G = ⌈n1∕3
{X(n) −X(1)}∕(3.49SX)⌉, where SX is the sample standard deviation. Note that this assumes
a desired range of X(1) ≤X ≤X(n) for the spread of the bins. If round numbers are used for
cleaner bin separators, use G = ⌈n1∕3{BU −BL}∕(3.49SX)⌉, where BL is the desired lower
boundary of the first bin and BU is the desired upper boundary of the final bin.
Example 3.5.1 Disease mortality. Among its indicators of national health status, the United
Kingdom collects data on mortality due to coronary heart disease, stroke, and related circu-
latory conditions for persons under 75 years of age. The British government releases their
various data sets online at http://data.gov.uk/dataset.
For example, standardized circulatory-disease mortality rates per 100 000 population for
n = 397 locations throughout the United Kingdom in 2008 appear in Table 3.4. (As above, a
selection of only the smallest and largest measurements is given in the table. The complete
data are available at http://www.wiley.com/go/piegorsch/data_analytics.)

DATA MANIPULATION
67
Table 3.4
A selection (smallest and largest values) of standardized year-2008
circulatory-disease mortality rates per 100 000 population from a larger data set of
n = 397 localities’ rates throughout the United Kingdom.
33.97
36.99
37.42
37.95
· · ·
118.37
122.99
126.47
126.86
Source: http://data.gov.uk/.
To explore these data for possible smoothing and other graphical summarization, begin
with a simple binning operation: application of Sturges’ rule produces a recommended G =
⌈log2(397) + 1⌉= ⌈9.63⌉= 10 bins. Appeal to R facilitates calculation of the corresponding
break points and bin frequencies, since Sturges’ rule is the default binning rule in the pro-
gram’s hist() function for plotting histograms. If the data are collected into, say, the vector
smr08, then use
> hist(smr08)$breaks
[1]
30
40
50
60
70
80
90 100 110 120 130
> hist(smr08)$counts
[1]
7
39 112
83
62
56
16
15
4
3
Notice that R has selected rounded bins, with BL = 30 and BU = 130, slightly below
X(1) = 33.97 and slightly above X(n) = 126.86. (The program employs its internal pretty()
function for selecting equally spaced, ‘round’ values; see help(pretty).) The consequent
G = 10 bins are represented by the values between the breaks; the counts are the binned
frequencies ̂fg (g = 1, … , 10). That is,
Bin 30-40 40-50 50-60 60-70 70-80 80-90 90-100 100-110 110-120 120-130
̂fg
7
39
112
83
62
56
16
15
4
3
Binning here shows that the mortality rates concentrate at about 50–70 deaths per 100 000
population, clearly below the center of the range. A longer tail runs out to a few rates near
110–130 deaths per 100 000. (The sample mean is about 3.5 units larger than the sample
median for these data, corroborating the potential right skew; see Exercise 3.11.) Example
4.1.5 explores how to use these values for graphing a histogram of the data.
Given the slight right skew, application of Scott’s normal reference rule for finding G is
a viable alternative. Exercise 3.19 applies Scott’s rule and builds an alternate set of bins for
these data.
◽
3.5.2
Moving averages∗
A less-coarse approach for clearing noise from a large data set appeals to the inherent sum-
mary/smoothing features of the arithmetic mean. Suppose the data are structured so that values
adjacent to each Xi are felt to be related. For example, the data may be recordings of a quan-
titative outcome over consecutive time periods, called a time series (Box et al. 2008).
Then, taking local averages of related values can act to smooth out small disturbances in
the larger trend. The simplest kind of local smoother averages the m values above and below

68
STATISTICAL DATA ANALYTICS
(and including) each Xi (i = 1, … , n). The consequent subset of 2m + 1 local elements is
called the averaging window or span. As the averaging moves across the data index i, the
result is known as a moving average:
̂Xi =
1
2m + 1
m
∑
j=−m
Xi+j .
(3.14)
Near to the lower data boundary at i = 1 and the upper boundary at i = n, the indexing may
be ill-defined; if so, truncate the averaging window at each boundary. If the data are extensive
enough, one can also simply ignore any ̂Xi for which fewer than 2m + 1 elements are included
in the window. (Another possibility is to ‘reflect’ the smoother at i = 1 and i = n and reuse
data values near each boundary, although this can lead to instabilities in certain cases.)
An extension of (3.14) that allows for heterogeneous weighting within each window is
̂Xi =
∑m
j=−m 𝑤jXi+j
∑m
j=−m 𝑤j
,
(3.15)
similar to (3.2). As there, the weights satisfy 𝑤j ≥0. The principal weight 𝑤0 is usually
the largest; a popular choice for the remaining weights is 𝑤−j = 𝑤j, creating a symmetric
(weighted) moving average. For instance, a symmetric, triangular weighted average employs
the linearly decreasing weights 𝑤0 = m + 1, 𝑤1 = 𝑤−1 = m, 𝑤2 = 𝑤−2 = m −1, … , 𝑤m−1
= 𝑤m−1 = 2, 𝑤m = 𝑤−m = 1.
A special, asymmetric variant for (3.15) takes 𝑤j = 0 for all j = 1, … , m. This is a retro-
spective moving average, the simplest form of which sets 𝑤j = 1 for all j = −m, … , 0. That
is, the previous m observations are averaged equally with the current observation.
Example 3.5.2 Financial moving average. Retrospective moving averages are often seen
with financial time series such as stock market indices, where multiday moving averages are
used to study longer-term features in the index. A typical data set of this sort is the daily
closing prices of the US Dow Jones Industrial (DJI) stock index. DJI data can show highly
irregular patterns in response to the financial market’s day-to-day activities. Applying, say, a
retrospective m-day moving average
̂Xi =
1
m + 1
0
∑
j=−m
Xi+j
smooths out the irregularities and gives a clearer indication of the index’s pattern of movement.
The value taken for m can be 50 days, 100 days, 200 days, and so on, depending on the length
of the larger series under study.
Ley (1996) gave daily closing prices for the DJI, a 20-year selection of which (June
1974–June 1993) is represented in Table 3.5. (Only the earliest and latest measurements are
given in the table. The complete data are available at http://www.wiley.com/go/piegorsch/data
_analytics.)
Figure 3.1 plots the full 20-year data set of n = 4802 DJI closing values, with a retrospec-
tive 200-day moving average superimposed. (The basic graphic is eponymously known as a
times series plot; see Section 4.2.5.) As expected, the jagged pattern of daily index prices is
ameliorated by the longer-term moving average.
◽

DATA MANIPULATION
69
Table 3.5
Closing US Dow Jones Industrial (DJI) stock market average from 17 June 1974
to 14 June 1993; selection from larger collection of n = 4802 index values.
833.23
830.26
826.11
820.79
· · ·
3511.94
3491.72
3505.02
3514.7
Source: Ley (1996).
Month
DJI
500
1000
1500
2000
2500
3000
3500
Jun 1974
Jun 1977
Jun 1980
Jun 1983
Jun 1986
Jun 1989
Jun 1992
Figure 3.1
Dow Jones Industrial (DJI) stock market average (dots, · · ·) over 20-year period
(17 June 1974–14 June 1993) and 200-day moving average (gray curve,
). Source: Data
from Ley (1996).
3.5.3
Exponential smoothing∗
If, in a retrospective moving average, it is felt that data farther from Xi are not as representative
as those closer, the weights can be constructed to dampen exponentially as j →−m (again,
with 𝑤j = 0 for all j = 1, … , m). This is known as exponential smoothing. In this case, rather
than progress a moving window of m previous observations, one includes all the observations
up to and including Xi but dampens earlier observations more severely. This is accomplished,
for example, via
̂Xi ∝𝜆{Xi + (1 −𝜆)Xi−1 + (1 −𝜆)2Xi−2 + · · · + (1 −𝜆)i−1X1}

70
STATISTICAL DATA ANALYTICS
for some 𝜆∈(0, 1). Translated into a weighted, reciprocal, moving average, the weights
become 𝑤hi = 𝜆(1 −𝜆)i−h and the exponential smoother is
̂Xi =
∑i
h=1 𝜆(1 −𝜆)i−hXh
∑i
h=1 𝜆(1 −𝜆)i−h
.
(3.16)
Notice that the weights 𝑤hi are now allowed to evolve over differing values of i. Appeal to the
finite geometric series in (2.23) simplifies (3.16) to
̂Xi = 𝜆∑i
h=1 (1 −𝜆)i−hXh
1 −(1 −𝜆)i
.
(3.17)
Now, as i →∞, the denominator in (3.17) approaches 1 for any 𝜆∈(0, 1). This produces an
asymptotic approximation to (3.17) in the form of a recursive relationship: for very large i,
̂Xi ≈𝜆Xi + (1 −𝜆) ̂Xi−1. Notice that this is a simple weighted average of the current observa-
tion Xi and the past estimate ̂Xi−1. (Also see Exercise 3.25.)
Exponential smoothing is also useful for forecasting the value of a future data point from
the ensemble of current observations. Developed in a slightly different form from (3.17), the
process begins with the recursive relationship
̂X2 = X1
̂Xi = 𝛼Xi−1 + (1 −𝛼) ̂Xi−1
(i > 2),
where 𝛼∈(0, 1). Manipulating this recursive definition for i > 2 produces
̂Xi = (1 −𝛼)i−2X1 + 𝛼
i−2
∑
j=1
(1 −𝛼)j−1Xi−j ,
(3.18)
or simply ̂Xi = ∑i−1
h=1 𝑤hiXi for
𝑤hi =
{
(1 −𝛼)i−2
if h = 1
𝛼(1 −𝛼)(i−1)−h
if h = 2, … , i −1 .
(3.19)
It can be shown (Exercise 3.26) that ∑i−1
h=1 𝑤hi = 1 for the weights in (3.19). Thus (3.18)
is itself a weighted moving average. In the forecasting literature, this simple exponential
smoother is often called an exponential weighted moving average (or EWMA) and is available
in R via the external qcc or forecast packages.
The simple exponential smoother/EWMA is sensitive to underlying trends in the observa-
tions. A modification that adjusts for linear trends is known as ‘double exponential smooth-
ing.’ In fact, a variety of methods that enhance the simple exponential smoother for use with
forecasting applications, many of which connect to autoregressive, integrated moving-average
(ARIMA) models in time series analysis, are available. For more on this, see Hyndman et al.
(2008).
The general concept of a moving average can be extended to locally averaged estima-
tors for a p.d.f., generalizing the discrete binning approach that underlies construction of
histograms. These are known as density estimators and are discussed in Section 4.1.4.

DATA MANIPULATION
71
Exercises
3.1
Use a computer (or any other appropriate vehicle) to generate random samples for the
following scenarios. (Hint: in R, the use the sample() function or any of the general
class of r* random number generator functions, e.g., rnorm() for N(𝜇, 𝜎2).)
(a) A random sample of size n = 50 from the first 1000 positive integers, without
replacement.
(b) A random sample of size n = 100 from the first 1000 positive integers, with replace-
ment.
(c) A random sample of size n = 50 from the first 100 positive integers, without
replacement.
(d) A random sample of size n = 100 from the first 50 positive integers, with replace-
ment.
(e) A random sample of size n = 50 from Bin(20, 0.5).
(f) A random sample of size n = 100 from Bin(100, 0.25).
(g) A random sample of size n = 100 from N(63, 70).
(h) A random sample of size n = 250 from N(97.5, 122).
(i) In Exercise 3.1d, what happens if you sample without replacement? Why?
3.2
Hand et al. (1994, Section 231) listed a random sample of heights for married adults
from the UK Office of Population Censuses and Surveys (OPCS). Data on husbands’
heights (in mm) comprise n = 199 observations, a selection of which follow (download
the full data set at http://www.wiley.com/go/piegorsch/data_analytics):
1809, 1841, 1659, 1779, 1616, … , 1675, 1641, 1743, 1823, 1720
Calculate the sample mean, the sample variance, and the standard deviation for these
199 men’s heights.
3.3
A study examined factors that affect vulnerability to hazardous events among n = 132
of the largest cities in the United States. An index was produced that quantified the
frequency and diversity of natural hazards such as tornados, hurricanes, and floods
on each city; higher values indicated greater urban vulnerability to hazardous events
(Piegorsch et al. 2007). Taken on a natural logarithmic scale, a selection of the values
follows:
0.6768, 0.9712, 1.0202, 1.0460, … , 2.2793, 2.2801, 2.3996, 2.4946
(Download
the
complete
data
set
at
http://www.wiley.com/go/piegorsch/data
_analytics.) Calculate the sample mean, the sample variance, and the standard
deviation for these hazard index values.
3.4
The US National Aeronautics and Space Administration (NASA) via its Solar Radi-
ation and Climate Experiment (SORCE) collects data on extreme ultraviolet (XUV)
solar irradiance. Recordings from one of the experiment’s photometers targeted in

72
STATISTICAL DATA ANALYTICS
the 0.1–7.0 nm range produced n = 3510 observations of median irradiance values (in
W/m2), a selection of which follows:
0.000004, 0.000004, 0.000497, … , 0.001020, 0.001020, 0.001220
(Download
the
complete
data
set
at
http://www.wiley.com/go/piegorsch/data
_analytics.) Calculate the sample mean, the sample variance, and the standard
deviation for these values.
3.5
To explore the resilience/robustness of the sample median, ̂Q2, to extreme observa-
tions, return to the carbohydrate intake data from Table 3.3. For simplicity, create a
randomly selected subset of seven values from the larger data set using, for example,
sample(carbs, size=7) in R. Find the sample mean and the median of this subset.
Comment on the proximity of the two values. Now, include as an additional value the
largest observation in the data set, X(778) = 738, and calculate the sample mean and
sample median of the new subset. By how much have the mean and median changed?
(Use both absolute and relative differences.)
3.6
For the following data sets, calculate the 10% trimmed mean. Comment on how it
compares to the corresponding sample mean.
(a) The carbohydrate intake data in Example 3.4.1.
(b) The hazard vulnerability data in Exercise 3.3.
3.7
To see how the sample variance in (3.5) is based on only n −1 components of infor-
mation, execute the following steps (Casella and Berger 2002, Section 5.3):
(a) Focus on the numerator and write the sum as (X1 −X)2 plus another sum made up
of only n −1 terms. What are these n −1 terms?
(b) Show that ∑n
i=1(Xi −X) = 0.
(c) Write (X1 −X)2 in terms of the same n −1 terms in Exercise 3.7a. (Hint: From the
result in Exercise 3.7b, to what is (X1 −X) equal?)
(d) Argue that the sum ∑n
i=1 (Xi −X)2 is, therefore, made up of only n −1 different
terms. What are these terms?
3.8
Show that the expressions for the sample variance in (3.5) and (3.6) are algebraically
equivalent.
3.9
Return to the husbands’ heights data in Exercise 3.2.
(a) Calculate the five-number summary and IQR. Do the data appear symmetric, or are
they skewed in any way?
(b) Calculate the 10% trimmed mean for these data. How does it compare to the sample
mean in Exercise 3.2? Does this give you further guidance on possible skew in the
data?
(c) Calculate the z-scores from (3.7) for these 199 observations. How many lie between
±1, ±2, and ±3? How does this compare with the 68–95–99.7% rule?

DATA MANIPULATION
73
3.10
Return to the solar observation data in Exercise 3.4.
(a) Calculate the five-number summary and IQR. Do the data appear symmetric, or are
they skewed in any way?
(b) Calculate the 10% trimmed mean for these data. How does it compare to the sample
mean in Exercise 3.4? Does this give you further guidance on possible skew in the
data?
3.11
Return to the circulatory-disease mortality data in Table 3.4 and calculate the following
summary statistics.
(a) The sample mean, sample variance, and sample standard deviation.
(b) The sample median and rest of the five-number summary, along with the IQR.
(c) Compare the mean in Exercise 3.11a to the median in Exercise 3.11b. Use this,
along with the five-number summary, to comment on possible skew in the data.
(d) Calculate the 10% trimmed mean for these data. How does it compare to the sample
mean in Exercise 3.11a? Does this give you further guidance on possible skew in
the data?
3.12
The data set in Exercise 3.2 also contains heights (in mm) for the wives from each
married couple. As each couple constitutes a natural pairing, one can report the data as
bivariate pairs; for example, the pairs (X = {Wife’s height}, Y = {Husbands’s height})
corresponding to the selected values displayed in Exercise 3.2 are
X = Wife’s height:
1590
1560
1620
· · ·
1560
1630
1530
Y = Husband’s height:
1809
1841
1659
· · ·
1743
1823
1720
(Download the full data at http://www.wiley.com/go/piegorsch/data_analytics.) Cal-
culate the Pearson correlation coefficient in (3.9) to quantify any association between
these husbands’ and wives’ heights. What do you find? Is this surprising?
3.13
Show that the various expressions for the Pearson correlation coefficient given in
Section 3.3.3 are all algebraically equivalent.
3.14
For the following data sets, conduct a preliminary outlier analysis. Find the outer
sample quartiles and the sample IQR, and from these, find the fences from (3.10).
Determine if any data points exceed the upper fence or drop below the lower fence and
could be potential outliers. Also find the z-scores from (3.7) for the potential outliers
to study how that metric compares.
(a) The wheat kernel data in Table 3.1.
(b) The circulatory-disease mortality data in Table 3.4.
(c) The husbands’ heights data in Exercise 3.2.
(d) The hazard vulnerability data in Exercise 3.3.

74
STATISTICAL DATA ANALYTICS
3.15
Return to the wheat kernel data in Table 3.1.
(a) Calculate the sample entropy, ̂H′
V, from (3.12) for the full data set.
(b) Apply a square root transformation to the data: take Yi =
√
Xi. Examine the tails
of the data and comment on any attenuation in the spread among the transformed
values.
(c) Calculate the sample entropy, ̂H′
V, for the square-root-transformed data. Comment
on any changes versus the untransformed data set.
(d) Repeat the operation in Exercise 3.15b, but now apply a logarithmic transformation.
Also calculate the sample entropy, ̂H′
V, for the log-transformed data and comment
on any changes.
3.16
For the carbohydrate intake data in Example 3.4.1, apply a logarithmic transformation
to the original observations in Table 3.3. Has a further reduction in spread occurred
among observations in the tails? What happens to the sample entropy, ̂H′
V, after apply-
ing the log transform?
3.17
Recall that the circulatory-disease mortality data in Table 3.4 exhibit a possible skew.
Apply a logarithmic transformation to the Xis. Calculate the five-number summary and
compare it to that found in Exercise 3.11b for the original data. What conclusions might
you draw from the comparison?
3.18
Take the limit of the power transformation in (3.13) as 𝜆→0, and show this produces
a logarithmic transform. (Hint: recall l’Hôpital’s rule from univariate calculus.)
3.19
Return to the circulatory-disease mortality data in Table 3.4 and rebin the values
via Scott’s normal reference rule: calculate the number, G, of bins using Scott’s
rule, construct the bins, and calculate the bin frequencies, ̂fg, g = 1, … , G. In
R, the nclass.scott() function will give Scott’s G if binning is restricted to
X(1) ≤X ≤X(n). To mimic the ‘pretty’ boundaries used in Example 3.5.1, however,
calculate the value directly using BL = 30 and BU = 130. Do the two values for G
differ?
3.20
Smooth the following data sets by binning the observations: (i) select the number, G, of
bins via a reasonable determinator such as Scott’s rule or Sturges’ rule (clearly indicate
which you choose), (ii) construct the bins, and (iii) calculate the bin frequencies, ̂fg, g =
1, … , G.
(a) The wheat kernel data in Table 3.1.
(b) The square-root-transformed wheat kernel data in Exercise 3.15b.
(c) The log-transformed circulatory-disease mortality data in Exercise 3.17.
(d) The husbands’ heights data in Exercise 3.2.
(e) The hazard vulnerability data in Exercise 3.3.
(f) The solar observation data in Exercise 3.4.

DATA MANIPULATION
75
3.21
For the weighted moving average in (3.15), assume the Xis are independent with con-
stant variance Var[Xi] = 𝜎2. Find Var[ ̂Xi].
3.22
Similar to the data in Example 3.5.2, Ley (1996) also studied the Standard and Poor’s
500-stock index (the ‘S&P500’). Closing prices of the S&P500 over the same 20-year
span as pictured in Figure 3.1 are available at http://www.wiley.com/go/piegorsch/data
_analytics. Download these data and plot them over time. Superimpose a 200-day mov-
ing average and compare the pattern(s) with those seen in the figure.
3.23
Piegorsch and Bailer (2005, Exercise 5.11) examined data on yearly snow water
equivalent (SWE, a measure of the amount of water in snow) at a watershed in
the US Yellowstone Park between 1935 and 1996. The data are available online at
http://www.wiley.com/go/piegorsch/data_analytics; a sample follows:
Year:
1935
1936
1937
· · ·
1994
1995
1996
SWE:
8.2
11.7
9.30
· · ·
12.92
14.28
18.13
Examine these data as follows:
(a) Show that an upward trend is evident by plotting SWE versus year. Note the ragged
appearance of the plot.
(b) Smooth the data by calculating a 10-year moving average.
(c) Superimpose the 10-year moving average on the original data plot. Does the moving
average show a smoother trend? What further investigations might be considered
from this analysis?
3.24
Verify that the exponential smoother in (3.16) with weights 𝑤ji = 𝜆(1 −𝜆)i−j simplifies
to the expression in (3.17).
3.25
Show that the exponential smoother in (3.17) satisfies the recursive relationship
̂Xi =
𝜆
𝑤+[i]
Xi +
𝑤+[i−1]
𝑤+[i]
(1 −𝜆) ̂Xi−1,
where 𝑤+[i] = 1 −(1 −𝜆)i.
3.26
Show that the weights in (3.19) sum to 1 for any 𝛼∈(0, 1).

4
Data visualization and statistical
graphics
Readers are likely aware of the famous aphorism that ‘a picture is worth a thousand words’
(ascribed to various sources throughout history, but cementing itself in the lexicon after
appearing in US newspapers and advertisements in the early 1900s). Taken literally, it is often
an exaggeration – a thousand well-placed words can tell a good story! – but it is still a potent
reminder of the power of graphical visualization. (Gelman and Unwin (2013, p. 7) taken it a
step further: “ … a picture may be worth a thousand words, but a picture plus 1000 words is
more valuable than two pictures or 2000 words.”) This is especially true in data analytics: if
properly constructed and interpreted, a statistical graphic can summarize features of a large
data set as well as, and often better than, many summary statistics. It can communicate unseen
or blatant aspects of a data stream and lead to effective knowledge discovery, especially
when exploring very large amounts of data. These two goals, communication and discovery,
often overlap between exploratory data-analytic studies and graphical data investigations;
see Gelman and Unwin (2013) and in particular the thought-provoking discussion therein.
Construction of graphical summaries, like much of data analytics, benefits from transdis-
ciplinary input. Recent terms describing this include information visualization (or ‘infovis’)
and infographics. At first blush, the two sound similar, but in fact, careful distinction is made
between them (Wickham 2013): the former has anchors in the field of computer science and
is an ongoing area of scholarly development, where interdisciplinary inclusion of statistical
input can enhance the broader technological endeavor. The latter exhibits greater focus on
design and creative features of presenting information graphics. Here, data analytics plays a
lesser role. Both infovis and infographics contribute, however, to contemporary advancement
in data visualization.
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

DATA VISUALIZATION AND STATISTICAL GRAPHICS
77
In this chapter, a brief introduction is given to the basic building blocks of statistical data
visualization. As previously, readers familiar with these concepts may wish to skip forward
to Chapter 5 and its presentation on more-advanced aspects of statistical inference.
4.1
Univariate visualization
Throughout this section, assume the data points, Xi, are taken from a simple random sample
(SRS), i = 1, … , n. These can be discrete or continuous, with underlying probability mass
function (p.m.f.) or probability density function (p.d.f.) fX(x), respectively.
4.1.1
Strip charts and dot plots
One of the simplest ways to graph information in the data is to plot a point along a number
line (or ‘strip’) at each observed value. This is known as a strip plot or strip chart and is,
in effect, a one-dimensional plot of the scatter in the data. The strip chart is a very elemental
approach for graphical display, but it can be useful for visualizing univariate data if the number
of observations is not too large.
Example 4.1.1 Disease mortality (Example 3.5.1, continued). Figure 4.1 presents a strip
chart for the circulatory-disease mortality data from Table 3.4. The plot was created in R
from the command stripchart(SMR,pch=1,xlab=‘SMR’), where SMR is the data vector
of mortality rates.
40
60
80
100
120
SMR
Figure 4.1
Simple strip chart in Example 4.1.1 for circulatory-disease mortality data
from Table 3.4. Source: Data from http://data.gov.uk/dataset/ni_121_-_mortality_from_all
_circulatory_diseases_at_ages_under_75.
The main feature in the figure is the long ‘strip’ of data points, which visualizes the scatter.
One sees a greater density of values between about 55 and 80 deaths per 100 000 population,
which tails off slowly as X grows. This corroborates results from the earlier binning exercise
in Example 3.5.1, indicating that the data possess a right skew.
◽
Some variants of the strip chart offset or stack dots on the graphic when multiple obser-
vations have the same (or nearly the same) value. The result is then known as a ‘Wilkinson’

78
STATISTICAL DATA ANALYTICS
Table 4.1
Selection of oldest ages for people above 65 years recalled by a sample of
n = 755 US Internet users.
65
65
65
66
66
66
66
· · ·
117
118
119
119
119
120
120
Source: http://stickers.prudential.com/.
70
80
90
100
110
120
oldest.age
Figure 4.2
Wilkinson dot plot in Example 4.1.2 for oldest-age data from Table 4.1. Source:
Adapted from http://stickers.prudential.com/. Data downloaded on 17 April 2013.
dot plot (Wilkinson 1999). (This differs from an alternative form known as the ‘Cleveland dot
plot’ (Cleveland 1993, Section 2.5), which is not discussed here.) The next example illustrates
the graphic device.
Example 4.1.2 Oldest ages. In a popular US television commercial, the Prudential Insurance
Company asked a sample of individuals to give the age of the oldest person they knew above 65
years (living or dead). They then plotted these data by placing colored dots one above another
on a large wall at the given age. What was produced, in effect, was a dot plot of maximum
ages above 65 among a selected sample of the US population.
The company continued this experiment online at http://stickers.prudential.com/, asking
Internet users to enter the age of the oldest person they knew above 65. The data from n =
755 respondents (as of 17 April 2013; selected values only) appear in Table 4.1.
Figure 4.2 presents a Wilkinson dot plot for these oldest ages. The plot was created in R
via the DOTplot() function from the external UsingR package. One could create a roughly
similar display in R via the command

DATA VISUALIZATION AND STATISTICAL GRAPHICS
79
> stripchart( oldest.age, method=‘stack’, pch=16, cex=.8,
offset=.3 )
The figure presents an interesting picture of how oldest ages are recalled by individuals, with
ages in the mid-to-upper 90s most common among the respondents.
Some qualifications are in order: (i) How the company chose the original participants is
unclear, and the Internet users who participated further are by definition self-selected. Thus
the sample here is likely biased and should be viewed as nonrandom. (ii) Participants are
responding with their memories of ‘oldest’ ages, which themselves may be biased by recall
distortions. (iii) The outcome variable is censored below so that 65 is the lowest possible
value. In addition, it is the oldest age that can be recalled, so these are a form of data known
as extreme values (Coles 2001). Censored extremes such as this often vary differently from
central measures such as a mean.
These caveats warn that the graphic’s value is more illustrative than inferential. Nonethe-
less, it is an interesting and even amusing use of the dot plot as statistical information employed
by the commercial media.
◽
4.1.2
Boxplots
One of the most useful graphical displays for visualizing univariate data is known as the box-
plot. Although it is similar to the strip chart, in that it references along a one-dimensional
number line, the boxplot is designed to provide a much larger amount of information. In
effect, it is a graphical representation of the five-number summary from Section 3.3.2. Given
the sample minimum X(1), the lower quartile ̂Q1, the median ̂Q2, the upper quartile ̂Q3, and the
sample maximum X(n), a boxplot draws bars at each of these five values along their respective
locations on a number line. It then connects (i) the bar at the minimum with the bar at the
lower quartile and (ii) the bar at the maximum with the bar at the upper quartile, each via a
dashed line (the ‘whiskers’). In between the two whiskers, it connects the bars at the lower
and upper quartiles with a box (which by construction will also contain the bar at the median).
Notice that the IQR for the data is then the distance across the ‘box’ portion of the plot.
A number of variations have evolved from this basic design:
(a) Many graphic programs replace the lower quartile with the lower hinge (the median
of the lower half of the data – see Section 3.3.2) and the upper quartile with the upper
hinge (the median of the upper half of the data). This is the default in R’s boxplot()
function.
(b) In order to identify potential outliers, the whiskers can be abridged to end at the lower
and upper fences, 1 and 3 from (3.10), instead of the minimum and maximum,
respectively. A further variant ends the upper (lower) whisker at the largest (smallest)
datum no farther away from the box than the corresponding fence. Any observations
outside of the fences are then plotted as individual points (say, with a dot or circle).
Such points are, by definition, potential outliers within the sample; this device helps to
visualize them quickly. Here again, this is the default setting in boxplot(). (For the
simpler version with whiskers stretched to the data extremes, use the range=0 option
in boxplot().)
(c) Another variant ‘notches’ the box at the median. The notches pinch the box to give
it a ‘waist’ at ̂Q2 and slowly draw up to the original box’s edge below and above the

80
STATISTICAL DATA ANALYTICS
median. In R, the spread of the notch is approximately ±1.58 ̂
IQR∕
√
n from ̂Q2; it
gives a rough location for where the true median may lie, such that any other boxplot’s
notch that overlaps likely has a similarly valued median. (The concept ties in with what
is known as an ‘hypothesis test’ between two medians – statistical testing is introduced
in Section 5.4.) If the notch distance extends past the corresponding hinge, the notch
is drawn back on itself, producing sharply pointed edges on the box (McGill et al.
1978, Fig. F). In R, notched boxplots are available via the notch=TRUE option in
boxplot().
Many other enhancements have appeared for boxplot graphics; see, for example, McGill
et al. (1978) or Benjamini (1988). Figure 4.3 dissects the anatomy of a standard boxplot, given
in a horizontal perspective.
Lower whisker
Lower
fence
(or X(1))
X scale
Upper
fence
(or X(n))
Lower
hinge
Median
Upper
hinge
Box
Upper whisker
Potential
outliers*
Figure 4.3
Anatomy of a box plot. ∗If whiskers end at fences.
The boxplot was originated by Tukey (1972, 1977) who actually developed two variants,
called the box-and-whisker plot and the schematic plot. (The latter moniker has fallen out of
use, while the former term is often used as a synonym for the contemporary version.) The
modern boxplot as described here has evolved as a hybrid of these progenitors. Its primary
value is as a compact-but-efficient graphic for examining the distributional features of the
Xis. Symmetric data exhibit whiskers that extend from the box at roughly equal distances and
with a median bar roughly centered within the box. Asymmetry and skew are evidenced by
asymmetric whiskers, where one whisker is much longer (in the direction of the skew) than
the other and/or the median locates closer to one end of the central box (away from the skew).
Large occurrences of potential outliers in the direction of the skew may also be evident if
using abridged whiskers.
The boxplot also has value for data analytics in that its structure is not adversely hindered
by very large data sets: its design produces essentially the same output for any n. (The number
of outlier points may grow busy with very large n, but this can be adjusted, e.g., by moving to
outer fences in the display – simply use the range=3 option in the boxplot command.)
Example 4.1.3 Disease mortality (Example 4.1.1, continued). Continuing with the
circulatory-disease mortality data from Table 3.4, Figure 4.4 displays a boxplot for the
observations using the R command

DATA VISUALIZATION AND STATISTICAL GRAPHICS
81
> boxplot( x=SMR, horizontal=T, boxwex=.33, xlab=‘SMR’ )
where the data are available in the R vector SMR. (The horizontal=T option produces a hori-
zontal orientation – R’s default is vertical boxplots – while boxwex=.33 tightens the width of
the box for a cleaner horizontal display.) The range= option in boxplot() was left unspeci-
fied, enforcing the default range=1.5. This generated abridged whiskers based on the fences
1 and 3 from (3.10). A rug plot of vertical marks is included at the bottom of the dis-
play, indicating the location of each individual observation. This is generated via the separate
command rug(x=SMR).
40
60
80
100
120
SMR
Figure 4.4
Box plot in Example 4.1.3 for circulatory-disease mortality data from Table 3.4;
‘rug’ at bottom marks individual data values. Source: Data from http://data.gov.uk/dataset
/ni_121_-_mortality_from_all_circulatory_diseases_at_ages_under_75.
From the figure, we observe that the bulk of the data (the 50% inside the box) rests between
about 55 and 80 deaths per 100 000 population. The data also exhibit a right skew: the right
whisker extends farther out than its left counterpart, and the median bar is slightly left of center
within the box. These results corroborate past indications from the binning in Example 3.5.1
and the strip chart in Example 4.1.1.
Commensurate with its design, Figure 4.4 also provides a quick outlier analysis: no
observations appear below the lower fence; however, five observations are marked as
potential outliers above the upper fence at 113.94 deaths per 100 000 population. (As per
the defaults in R, this is the largest Xi that does not exceed 3. Find the actual statistic via
boxplot.stats(SMR)[[1]][5].) The corresponding UK communities would be strong
candidates for further examination as to their high, potentially outlying circulatory-disease
mortality rates. Also see Exercise 3.14b.
◽
4.1.3
Stem-and-leaf plots
Another graphical display – also developed by Tukey (1972) – is known as the stem-and-leaf
plot or just stemplot for short. Different in style and structure from the boxplot, a stemplot is
built from the individual numerals of each datum. This assumes, of course, that the data are
quantitative. The ‘stem’ of each Xi is taken as the collection of its digits up to but not including
its final digit (reading left-to-right). The ‘leaf’ is that final digit. For example, if X = 118.37,

82
STATISTICAL DATA ANALYTICS
then its stem is ‘118.3’ and leaf is ‘7.’ (For simplicity, one usually rounds this to 118.4 and
sets the stem to ‘118.’ and the leaf to ‘4.’ Stemplots are used more for fast graphical display
than highly precise data presentation.)
A stemplot takes this stem–leaf bifurcation and builds the stems along a vertical axis,
smallest at the top. It then displays the leaves for each datum horizontally at each stem. Most
programs also order the leaves at each stem from smallest to largest, left to right, a preferred
option. If a nonproportional-width font (such as ‘courier’) is used to display the leaves, then
the more leaves at each stem, the longer the plot extends. Taken as a whole, this gives an
informative visual of how the data distribute over the sample. (As the stems run vertically, the
visual is rotated 90∘to the right, which takes some getting used to.)
Example 4.1.4 Myocardial infarction. Salzberg (1988) reported on a study of survival for
132 cardiac patients who suffered an acute myocardial infarction or ‘heart attack’. Among
the variables measured was the age (in years) at which patients suffered their (first) attack.
Six patients did not report the age for their attack and so are not included here. Thus the final
sample size is n = 126. Table 4.2 presents the data.
Table 4.2
Ages of (first) acute myocardial infarction (in years) among n = 126
cardiac patients.
35 46 46 47 48 50 50 51 52 52 53 54 54 54 54 54 54 55 55 55 55 55 56 56 56
57 57 57 57 57 57 57 57 58 58 59 59 59 59 59 59 59 60 60 60 60 60 60 61 61
61 61 61 61 61 61 62 62 62 62 62 62 62 62 62 62 62 63 63 63 63 63 63 63 64
64 64 64 64 64 64 65 65 65 65 65 66 66 66 66 66 67 67 67 68 68 68 68 69 69
69 70 70 70 70 70 71 71 72 72 73 73 73 73 74 75 77 78 78 78 79 79 80 81 85 86
Source: Salzberg (1988).
To gain an understanding of the age-at-attack distribution for these subjects, and possibly
uncover new knowledge in the processes underlying myocardial infarction, we can construct
a stemplot in R using the command stem(age), where age is the data vector of ages. This
produces the graphic in Figure 4.5 (with, as recommended, a nonproportional-width font).
Notice how R formats the stem column for these data: each stem is listed twice. The first
placement (closer to the top) of each stem is limited to only those leaves between 0 and 4;
the second (lower) is limited to leaves between 5 and 9. This retains the basic ordering of the
data while expanding the scale of the plot to better visualize the distributional features. (To
reduce the scale into only single tens-digits for the stem here, use stem(age,scale=0.5).
The resulting graphic is visually less informative, however.)
The stemplot in Figure 4.5 reveals that ages in this sample of patients distribute in a
generally symmetric, unimodal manner around a central value of 60–64 years. From sum-
mary(age), we indeed find ̂Q2 = 62 and X = 62.81 years. For purposes of specifying the
statistical distribution of these data, a normal (Gaussian) model from Section 2.3.9 might be
appropriate. (Also see Example 4.1.7.)
◽
Since each datum’s value in a stemplot is also presented via the stem-and-leaf structure,
one can quickly see where values are distributing themselves in the sample – unusually large
or small values will be evident, as will the approximate central tendency of the data.
Stemplots can be extended for comparing two random samples. First find the samples’
common stems, then plot the leaves of the first sample to the right of the stems, and plot

DATA VISUALIZATION AND STATISTICAL GRAPHICS
83
The decimal point is 1 digit(s) to the right of the |
Stem | Leaf
3 | 5
4 |
4 | 6678
5 | 001223444444
5 | 5555566677777777889999999
6 | 000000111111112222222222333333334444444
6 | 55555666667778888999
7 | 00000112233334
7 | 5788899
8 | 01
8 | 56
Figure 4.5
Stemplot in Example 4.1.4 for myocardial infarction data from Table 4.2. Source:
Data from Salzberg (1988).
the leaves of the second sample to the left. This is known as a side-by-side or back-to-back
stemplot.
While the stemplot is an informative summarizing graphic with small-to-moderate-sized
data sets, for very large data sets, the plot can grow very busy. This limits its visual impact.
One solution is to decrease the precision of the data values – for example, work with 118
instead of 118.37 – and many programs automatically apply this sort of strategy, including
R. With very large sample sizes, however, other methods for graphic summarization may be
more useful, as discussed in the next subsection.
4.1.4
Histograms and density estimators
A well-known graphic used to visualize the shape or distribution in a data set is the histogram,
and most readers have likely seen one (or more). The graphic is essentially a simple chart
of G > 0 nonoverlapping bars extending up in proportion to the frequency of occurrence
for different values of the data. With categorical data, this is essentially a bar chart – see
Section 4.2.1 – but for quantitative outcomes, the bar locations must first be selected based
on the data values. The effort is essentially a smoothing operation and can be performed by
binning the data as in Section 3.5.1. The result is a set of disjoint bins – also called class inter-
vals – and a set of corresponding frequencies, ̂fg, counting the number of Xis in each gth bin
(g = 1, … , G).
Choice for the number of bins, G, can be made using any pertinent binning rule. As dis-
cussed in Section 3.5.1, Scott’s (1979) normal reference rule is usually preferred because it is
less sensitive to asymmetries in the data than the earlier rule due to Sturges (1926). Note that
a binning rule may be reexpressed in terms of the bin width, that is, the (constant) separation
distance between bin boundaries. If the lower limit on the first bin is set to BL ≤X(1) and the
upper limit on the final bin is set to BU ≥X(n), then the width of each bin is h = (BU −BL)∕G.
For example, Scott’s rule is often written in terms of the bin width
h = 3.49 SX
n1∕3 ,
where SX is the sample standard deviation.

84
STATISTICAL DATA ANALYTICS
A preferred variant for the histogram, because it allows for comparison with density esti-
mates of fX(x) (see the following text), replaces the frequencies with the relative frequencies
̂fg∕n. In R, this is constructed via the hist(x,freq=FALSE) command, where x is the vec-
tor of data values. Or, one can use truehist(x) from the MASS package. Note that Sturges’
rule is the default in hist(), while Scott’s rule is the default in truehist(). In both cases,
however, R will apply its pretty() function to give cleaner, rounded bin limits. These may
differ from the actual numerical specification for G or h under either rule, unless overridden
by direct specification of the break points via the breaks= option. For more on histograms in
R, see Rizzo (2008, Section 10.1).
Example 4.1.5 Disease mortality (Example 3.5.1, continued). Continuing with the
circulatory-disease mortality data from Table 3.4, the binning operation using Sturges’
rule in Example 3.5.1 produced 10 nonoverlapping bins and a set of corresponding bin
frequencies, ̂fg. Converting these to relative frequencies after dividing by n and plotting
gives the histogram in Figure 4.6. (The shading was created via the col=‘gray’ option in
hist().) Notice the inclusion of a rug plot at bottom marking the individual data values,
similar to that in Figure 4.4.
SMR
Frequency
40
60
80
100
120
0
20
40
60
80
100
Figure 4.6
Histogram for circulatory-disease mortality data from Table 3.4; ‘rug’ at
bottom marks individual data values. Bins produced via application of Sturges’ rule
(Sturges 1926). Source: Data from http://data.gov.uk/dataset/ni_121_-_mortality_from_all
_circulatory_diseases_at_ages_under_75.
The graphical representation of mortality rates in the figure shows a unimodal distribution
with mode near 55 deaths per 100 000 population. There is also a clear skew to the right:
some localities have mortality rates as high 110–130 deaths per 100 000 population, as much
as twice that in the central portion of the distribution.

DATA VISUALIZATION AND STATISTICAL GRAPHICS
85
As in Exercise 3.19, the skew could motivate the use of Scott’s rule in place of Sturges’
rule for specifying the number of bins. Doing so (Exercise 4.7a) gives a slightly different
display but a qualitatively similar graphic.
These results corroborate all previous indications seen with these data and help to visualize
the nature of circulatory disease mortality in these British communities.
◽
The histogram is an effective graphical device for visualizing the distribution of a set of
observations. A common concern often raised with it, however, is that the binning operation
central to its production is highly subjective. That is, the final graphic can depend heavily on
the choice and location of the bins: too many and the bars may become ragged and sparse in
certain regions; too few and the histogram is too coarse. Indeed, the plot can be manipulated,
possibly unethically, to show different distributional shapes for the same set of data. Use of
established binning procedures such as Scott’s rule or Sturges’ rule can avoid this concern,
but then the data analyst should indicate how the bins were produced (as in Figure 4.6).
To alleviate some of these issues, more-advanced methods exist for estimating the under-
lying p.d.f, fX(x), from a set of (continuous) outcomes Xi. Known as kernel density estimators,
the methods are again a form of smoothing that estimates the relative frequency of occurrence
for any value of x in the support space .
The basic formula for a kernel density estimator based on data X1, … , Xn is
̂f(x) = 1
nh
n
∑
i=1
K
(x −Xi
h
)
,
(4.1)
where K(⋅) is a kernel function satisfying ∫∞
−∞K(t)dt = 1 and h > 0 is a smoothing parameter
called the bandwidth or window width. Equation (4.1) is also known in some circles as the
Parzen–Rosenblatt–Whittle window estimator, after three early pioneers of the technology
(Parzen 1962; Rosenblatt 1956; Whittle 1958).
The kernel in (4.1) can be thought of as a weighting function for the smoothing operation.
In most cases, it is chosen as a symmetric p.d.f., although it need have nothing to do with the
p.d.f. estimated in (4.1). Common examples include the standard normal p.d.f. from Section
2.3.9, referred to as the Gaussian kernel
KGauss(t) =
1
√
2𝜋
e−1
2 t2 ,
the triangular kernel
KTr(t) = (1 −|t|) I(−1,1)(t) ,
or the Epanechnikov kernel
KEp(t) = 3
4(1 −t2) I(−1,1)(t) ,
among many others. The Gaussian form appears as the default in many computer programs,
including the density() function in R.
The bandwidth h in (4.1) controls the smoothness of ̂f(x) and is analogous to the bin
width of a histogram: small values intensify the amount of local smoothing around each x
and produce ‘bumpier’ estimates of f, while larger values widen the inclusion window and

86
STATISTICAL DATA ANALYTICS
smooth out local irregularities. For example, under the Gaussian form KGauss(t), the bandwidth
acts as the standard deviation of the underlying kernel (Exercise 4.8): as h grows, the normal
p.d.f. used for the kernel flattens, increasing the impact on ̂f(x) of data points farther from x.
Choice of h can have a substantial impact on the final density estimator. Theoretical con-
siderations (Venables and Ripley 2002, Section 5.6) show that the optimal choice must be
proportional to n−1∕5. A popular rule-of-thumb with the Gaussian kernel sets
h = 𝜏min{SX , ̂
IQR∕1.34}
n1∕5
,
(4.2)
where 𝜏> 0 is a tuning parameter. Standard choices for 𝜏include 𝜏= 0.90 (Silverman 1986)
or 𝜏= 1.06 (Scott 1992). The former is the default for Gaussian kernels in R’s density()
function, although the latter is also very popular. Data-based bandwidth selection is an active
area of research (Ahmad and Ran 2004; Liao et al. 2010), and a variety of more-complex
methods also exist (Jones et al. 1996).
In practice, determining how many values of x at which to calculate the density estimator is
driven by computer resource constraints and/or how smooth a final output the analyst requires.
(Increasing the number of points will typically produce a smoother graphic.) R requires at least
512 points in density() and recommends more – in powers of 2, so next would be 210 =
1024 – if resources permit. Note that this differs from the sample size, n (of course, larger
values of n will produce density estimates more representative of the true fX(x)).
Example 4.1.6 Disease mortality (Example 4.1.5, continued). Return to the circulatory-
disease mortality data from Table 3.4. Recall that Figure 4.6 gave a histogram of the data,
where a unimodal shape with a right skew was evidenced.
Consider enhancing the visual representation here via a kernel density estimator from
(4.1). To explore how the choice of bandwidth, h, can affect the graphic, Figure 4.7 presents
two different density estimators for these data, the first with a relatively small bandwidth of
h = 1 and the second with a much larger bandwidth of h = 10. (In both the cases, the specific
kernel was the Gaussian default in R’s density(), thus h acts as the standard deviation of
the smoothing kernel. R scales all its smoothing kernels in density() to achieve this equiv-
alence. The original histogram from Figure 4.6 is underlaid for reference.) The bandwidth
effect is clear: setting h too low produces a ragged graphic, while driving it too high over-
smooths the estimator (notice how the lower tail at h = 10 extends beyond the lower range of
the data plot).
Figure 4.8 overlays the final kernel density estimator from (4.1) on the histogram, again
employing the Gaussian kernel KGauss(t). Choice of the bandwidth was taken as the R default
using 𝜏= 0.90 in (4.2). With these data, it is h = 4.62 calculated in
> density(SMR,n=1024)$bw
The plot was produced in R via the following code:
> hist( SMR, freq=F, main=‘’ )
#orig. histogram
> lines( density(SMR,n=1024) )
#overlay density() output
> rug( SMR )
#add rug of data values
The figure’s graphical representation of right-skewed mortality rates corroborates indications
seen previously for these data.
◽

DATA VISUALIZATION AND STATISTICAL GRAPHICS
87
SMR
Density
40
60
80
100
120
0.000
0.005
0.010
0.015
0.020
0.025
0.030
(a)
(b)
SMR
Density
40
60
80
100
120
0.000
0.005
0.010
0.015
0.020
0.025
0.030
Figure 4.7
Kernel density estimates (solid curves) in Example 4.1.6, overlaid on histogram
of circulatory-disease mortality data from Table 3.4. (a) Density estimate based on Gaussian
kernel with bandwidth set to h = 1. (b) Density estimate based on Gaussian kernel with band-
width set to h = 10. Histogram bins produced via application of Sturges’ rule (Sturges 1926).
Source:
Data
from
http://data.gov.uk/dataset/ni_121_-_mortality_from_all_circulatory
_diseases_at_ages_under_75.
In similar manner to stemplots, histograms can be extended for comparing two random
samples. First, determine a binning structure and then apply (the same) bins to each samples’
data. Next, score a vertical line according to the chosen bins. Plot the resulting histogram
bars of the first sample to the right of the line, and plot the bars of the second sample to the
left. This is known as a side-by-side or back-to-back histogram. R can produce back-to-back
histograms via, for example, the bi.bars() function in the external psych package.
4.1.5
Quantile plots
The histogram and density estimator in Section 4.1.4 are useful visualizations, but they can
only go as far as suggesting the features of an unknown, true p.d.f. A graphic device that
can assess the validity of a formal specification for fX(x) takes advantage of the information
available in the sample quantiles from (3.8). The concept is simple: start with the cumulative
distribution function (c.d.f.) from the proposed distribution,
FX(x) = ∫
x
−∞
fX(t)dt .
From this, determine the population quantiles, qp = F−1
X (p) from Section 2.1.4 over a selec-
tion of values for p ∈(0, 1). Then, given data Xi ∼i.i.d. fX(x) (i = 1, … , n), find the sample
quantiles ̂qp via (3.8) and compare them to the corresponding population values, qp.

88
STATISTICAL DATA ANALYTICS
SMR
Density
40
60
80
100
120
0.000
0.005
0.010
0.015
0.020
0.025
Figure 4.8
Kernel density estimate (solid curve) based on Gaussian kernel with
data-determined bandwidth (h = 4.62) in Example 4.1.6, overlaid on histogram of circulatory-
disease mortality data from Table 3.4. Rug plot at bottom marks individual data values. His-
togram bins produced via application of Sturges’ rule (Sturges 1926). Source: Data from http:
//data.gov.uk/dataset/ni_121_-_mortality_from_all_circulatory_diseases_at_ages_under_75.
Plotting ̂qp versus qp creates what is known as a quantile plot. (By convention, start with
the smallest p near 0 and plot the points by increasing p towards 1.) If the sample quantiles
approximate the true quantiles from FX(x), the plotted points will collect along a (roughly)
straight line. If the plot shows strong curvature, however, poor correspondence is evidenced
between the quantile information posited under FX(x) and that in the sample. Verzani 2005,
Fig. 3.4) gives an instructive graphic.
Perhaps the most common use of a quantile plot occurs when comparing against a normal
distribution, Xi ∼i.i.d. N(𝜇, 𝜎2). The plot is then called a normal quantile plot or sometimes a
normal probability plot. In this case, the sample quantiles are plotted against the corresponding
normal quantiles. Given its popularity, the normal quantile plot is available as the dedicated
function qqnorm() in R.
Example 4.1.7 Myocardial infarction (Example 4.1.4, continued). Return to the data in
Table 4.2 on ages at myocardial infarction (‘heart attack’) of n = 126 cardiac patients. Recall
from the stemplot of those data in Example 4.1.4 that the age-at-attack distribution appeared
unimodal, symmetric, and possibly normal. To assess this more fully, Figure 4.9 displays a
normal quantile plot for these data using the R command qqnorm(age) for the data in age.
A straight line is overlayed for reference via the R command qqline(age).
The plot shows a typical pattern of fit for normally distributed data: the majority of points
in the middle of the plot coincide with the reference line. While some deviation is evidenced
in the tails – that is, at the lower and upper limits of the plot – they do not bend away so
appreciably as to suggest drastic departure from normality.
◽

DATA VISUALIZATION AND STATISTICAL GRAPHICS
89
−2
−1
0
1
2
40
50
60
70
80
Theoretical quantiles
Sample quantiles
Figure 4.9
Normal quantile plot and reference line (solid line) in Example 4.1.7, for myocar-
dial infarction data from Table 4.2. Source: Data from Salzberg (1988).
Example 4.1.8 Disease mortality (Example 4.1.5, continued). In contrast to Example
4.1.7, Figure 4.10 plots a normal quantile plot (and overlaid reference line) for the
circulatory-disease mortality data from Table 3.4. Recall that our previous analyses with
these data consistently showed a clear right skew, questioning whether a symmetric
distribution such as the normal would provide an appropriate fit; see, for example, Figure 4.8.
The graphic in Figure 4.10 shows clear departure from the reference line and obvious
curvilinearity in the quantile plot. Right skew in the data is evidenced by the plot’s convex-
ity: the sample quantiles are more concentrated than the normal quantiles for p < 0.50 and
less concentrated for p > 0.50, a consequence of the right skew. This forces the plot to bow
downwards in the middle, indicating that these data do not appear normally distributed.
◽
One can extend the quantile plot from a single-sample comparison against a specific dis-
tribution to a pairwise comparison between the quantiles from two random samples. See
Section 4.2.2.
4.2
Bivariate and multivariate visualization
Generalizing the univariate setting described in Section 4.1, data analytics often
involve summarization of data taken on multiple samples or over multiple outcomes.
For continuous responses, the natural progression involves two random samples:
Xi ∼i.i.d. fX(x), i = 1, … , n, independent of Yj ∼i.i.d.Y(y), j = 1, … , m, where fX(x)
and fY(y) are the two samples’ p.d.f.s. (Extensions to more than two samples are similar.)
In addition, categorical data can occur in multivariate form. Suppose observations are
recorded and classified into K > 1 disjoint outcome categories. The counts of how many

90
STATISTICAL DATA ANALYTICS
−3
−2
−1
0
1
2
3
40
60
80
100
120
Theoretical quantiles
Sample quantiles
Figure 4.10
Normal quantile plot and reference line (solid line) in Example 4.1.8,
for
circulatory-disease
mortality
data
from
Table
3.4.
Source:
Data
from
http://
data.gov.uk/dataset/ni_121_-_mortality_from_all_circulatory_diseases_at_ages_under_75.
observation fall into each category, say, Y1, … , Yk (k = 1, … , K), then represent a set of
K multivariate response variables.
In either case – continuous or categorical – graphic summarization of bivariate and multi-
variate data becomes correspondingly more complex than visualization methods for the simple
univariate setting. A number of popular methods for such are described in this section.
4.2.1
Pie charts and bar charts
Of all the statistical graphics seen in practice, the pie chart is possibly the most ubiquitous
and also the most derided. It achieves both feats based on the same feature: its simplicity. Into
a circular ‘pie’, place ‘slices’ of area proportional to the percentage of occurrence for a given
category in a set of data. Thus, for example, if 40% of a company’s customer base come from
the north quadrant of the city, 30% from the south quadrant, 20% from the east quadrant, and
the remaining 10% from the center and west quadrant, the pie has four slices in the proportions
4:3:2:1, respectively. See Figure 4.11.
Pie charts are most appropriate when there are only a handful – literally – of possible cat-
egories being studied and when the proportions they exhibit are not too small. Change the
proportions in the customer base illustration above to 96%, 2%, 1%, and 1% and the graphic’s
message becomes more muddy (other than ‘most customers come from the north quadrant,’
for which no graphic is necessary). Or, delineate the base into 40 regions instead of four, and
the graphic information becomes too busy to interpret at all!
The pie chart’s derision stems from its overuse: being so simple, it is often the first
graphic a novice analyst will employ. It is not always the best choice for a summary graphic,
however, because it forces the viewer to visualize the information in terms of relative area.

DATA VISUALIZATION AND STATISTICAL GRAPHICS
91
North
South
East
West/center
Figure 4.11
Sample pie chart for artificial customer-base illustration; labels are customer
quadrants within a city.
In Figure 4.11, the difference between north quadrant and west/center customer bases is
clear, but south quadrant and east quadrant seem much more similar. Is the 50% (relative)
difference between them easy to discern?
Perhaps R makes it clearest in help(pie): “Pie charts are a very bad way of displaying
information. The eye is good at judging linear measures and bad at judging relative areas. A
bar chart or dot chart is a preferable way of displaying this type of data.”
The dot chart to which R refers can be either the Wilkinson dot pot (suitably modified for
categorical data input) or the Cleveland dot plot, both mentioned in Section 4.1.1. Also given
as an alternative, and favored here, is the bar chart. Generally ascribed to Playfair (1786) as
one of the earliest forms of statistical graphic, a bar chart is a simple chart of K ≥1 nonover-
lapping bars extending up in proportion to the frequency of occurrence for different values
of the data. When the data are continuous and from a single sample, this essentially produces
the histogram from Section 4.1.4. For a set of K categorical outcomes, however, the bars are
simply the frequencies of occurrence for each category/label. If the categories are ordered in
some manner, then the bars are displayed across the increasing (or decreasing, as desired)
category levels. If the categories possess no implicit ordering, then the bars are displayed in
any desired order. Any number of categories may be used, and the frequencies can take on
any values. Properly constructed such that the bar widths are equal, the areas of the bars in a
bar chart will convey information about each category’s relative impact on the phenomenon
under study, as will their heights.
In R, bar charts are implemented via the barplot() function. A barchart() function
is also available, with advanced features for applying trellis graphics.
Example 4.2.1 Driving speeds. From a study of driving speeds, Kadane and Lamberth
(2009) provided data on speeds of motorists when they exceed the speed limit on the US New
Jersey (NJ) Turnpike. The observations comprise n = 6536 recordings by radar of driving
speed, posted speed limit, and vehicles state of origin on the license plate.
The NJ Turnpike is a major artery linking communities along the US East Coast in what is
known as the ‘I-95 corridor.’ I-95 stands for the major interstate highway (Interstate route #95)
creating this transit corridor, from the US state of Florida (FL) to the US state of Maine (ME).

92
STATISTICAL DATA ANALYTICS
Although the larger study collected state-of-origin data from all US states and the District of
Columbia (DC), for simplicity, here the analysis is restricted to only those 15 states and DC
comprising the I-95 corridor: CT, DC, DE, FL, GA, MA, MD, ME, NC, NH, NJ, NY, PA, RI,
SC, and VA. This reduces the data set to n = 6252 individual records.
Speed limits on the NJ Turnpike vary, depending on road conditions and other factors.
Thus for better comparison across drivers, the original data were modified into recorded
speed (in mph) above the posted limit. These appear in Table 4.3. (As previously, only
a selection of the measurements is given in the table. The complete data are available at
http://www.wiley.com/go/piegorsch/data_analytics.)
Table 4.3
Selected driving speeds on the NJ Turnpike from a larger set of n =
6252 observations.
License state of origin
Recorded speed (mph) above limit
CT
2, 2, 2, 3, … , 19, 19, 19, 19
DC
4, 4, 5, 5, … , 19, 21, 21, 22
⋮
⋮
NH
6, 7, 7, 7, … , 15, 15, 16, 18
NJ
1, 1, 2, 2, … , 30, 32, 37, 38
NY
1, 2, 2, 2, … , 29, 29, 31, 35
⋮
⋮
VA
1, 1, 2, 2, … , 24, 26, 26, 31
Source: Kadane and Lamberth (2009).
For an initial analysis of these data, consider a simple breakdown of how (speeding) drivers
distribute across the 16 different states (and DC). If licenseI95 is the character vector of
states-of-origin for the n = 6252 observations, then via the table() function in R one finds
> table( licenseI95 )
licenseI95
CT
DC
DE
FL
GA
MA
MD
ME
214
53
243
164
40
185
702
12
NC
NH
NJ
NY
PA
RI
SC
VA
137
16 2267 1106
539
24
45
505
A bar chart of these summarized data appears in Figure 4.12 from the R command
> barplot( sort(table(licenseI95)), ylab=‘Frequency’, xlab=‘State’ )
The graphic here illustrates that, as might be expected, NJ license plates greatly outnumber the
rest, with the highly populous, adjoining, or nearby states of NY, MD, PA, and VA following
thereafter. States farther away – such as ME or GA – and less-populous states – such as RI
and NH – show smaller bars in the plot. These data will be analyzed further in examples to
follow.
◽
Bar charts can also be used to visualize features between two or more random samples:
the length of the bars is taken as some quantitative measure of interest such as the mean or
median of each sample. Whiskers and/or ticks, known as error bars, can be added above and

DATA VISUALIZATION AND STATISTICAL GRAPHICS
93
ME
NH
RI
GA
SC
DC
NC
FL
MA
CT
DE
VA
PA
MD
NY
NJ
State
Frequency
0
500
1000
1500
2000
2500
Figure 4.12
Bar chart for state-of-origin frequencies in Example 4.2.1, with NJ driving
speeds data from Table 4.3. Source: Data from Kadane and Lamberth (2009).
below the mean to indicate variation in the sample. These are usually displayed as X ± S at
each bar.
Example 4.2.2 Driving speeds (Example 4.2.1, continued). Continuing with the NJ driving
speeds data in Table 4.3, Figure 4.13 plots a bar chart of the mean speeds above limit (in mph)
for the 15 states and DC along the I-95 corridor. Error bars give ±S above and below the bars
at the means for each state of origin. Sample R code for the core graphic follows. The code
can be enhanced in a number of ways, and other programming steps to achieve the plot are
also possible.
> mean <- tapply( overlimI95, factor(licenseI95), mean )
> sdev <- tapply( overlimI95, factor(licenseI95), sd )
> summary.df <- data.frame(overlimI95,table(licenseI95),mean,sdev)
> ord.df <- summary.df[ order(summary.df$Freq), ]
> barplot( ord.df$mean, ylim=c(0,max(ord.df$mean+ord.df$sdev)) )
> xloc <- barplot( ord.df$mean,
ylim=c(0,max(ord.df$mean+ord.df$sdev)), plot=F )
> arrows( xloc, ord.df$mean-ord.df$sdev, xloc,
ord.df$mean+ord.df$sdev, angle=90, code=3 )
Here the graphic highlights the differences among states’ average over-limit speeds – ME
drivers appear to exceed the limit less than the others – although the wide error bars also
show that substantial variability exists in the data.
◽

94
STATISTICAL DATA ANALYTICS
ME
NH
RI
GA
SC
DC
NC
FL
MA
CT
DE
VA
PA
MD
NY
NJ
State
Mean speed (mph) over limit
0
5
10
15
Figure 4.13
Bar chart of mean speeds (in mph) above posted limit and error bars (as
mean ± standard deviation), for NJ driving speeds data in Example 4.2.2. State-of-origin order
displayed as per Figure 4.12. Source: Data from Kadane and Lamberth (2009).
As Figure 4.13 illustrates, bar charts can graphically display sample means (along with
standard deviations, error bars, etc.) and help to visualize similarities and differences among
different samples. The graphic is still very basic, however, and for comparing quantitative
information across multiple samples, other formats can do better; see Section 4.2.2.
An interesting extension of the bar chart places the bars on a polar coordinate grid
so that they circle and extend from a central point. This makes for a visually intriguing
display. Credited to famous nineteenth-century nurse and public health advocate Flo-
rence Nightingale, the graphic is known as a polar area plot, or sometimes a coxcomb
plot or a rose plot. Nightingale used polar area graphics in her ‘Diagram of the Causes
of Mortality in the Army in the East’ to identify mortality in the Crimean War from
various causes, including preventable diseases (Nightingale 1858). The plot’s striking
visual effect adds to its popularity, especially when contrasting colors are employed; see
http://infographicsnews.blogspot.com/2008/01/worth-thousand-words.html or http://www
.sciencenews.org/view/generic/id/38937/description/. Polar area plots can be constructed in
R via the geom_coxcomb() function in the external ggsubplot package.
As an infographic device, the polar area plot is popular; however, its ability to visualize
statistical information is generally not better than a bar chart or other simpler graphic displays
(Gelman and Unwin 2013). For example, a line graph replaces the bars in a bar chart with
lines connecting the tops of the bars. If the heights represent frequencies, this is also known

DATA VISUALIZATION AND STATISTICAL GRAPHICS
95
as a frequency polygon. Such replacement has value if the categories along the horizontal axis
are naturally ordered from left to right. This also makes for easier comparison between two
or more different sets of frequencies over the same set of (horizontal axis) categories, because
overlaying multiple bar charts on top of each other sometimes creates more graphic confusion
than it prevents. (With Nightingale’s mortality data, which occur over time, a better alternative
might also be a time series plot; see Section 4.2.5).
4.2.2
Multiple boxplots and QQ plots
As seen in Section 4.1.2, the boxplot is an effective graphic for summarizing features of a
random sample. Given two independent samples, it is a simple effort to extend the graphic
and include boxplots for both. In fact, this can be applied to any number of K ≥1 inde-
pendent random samples, producing a multiple boxplot. Consider first the two-sample case
with data Xi ∼i.i.d. fX(x), i = 1, … , n, independent of Yj ∼i.i.d. fY(y), j = 1, … , m. From
each random sample, compute the five-number summaries {X(1), ̂QX1, ̂QX2, ̂QX3, X(n)} and
{Y(1), ̂QY1, ̂QY2, ̂QY3, Y(m)}. Include variants such as replacing the outer quartiles with hinges
as in R’s boxplot() – cf. Section 3.3.2 – or abridging the whiskers at the fences and plot-
ting potential outliers, as desired. Then, on the same scale, plot the corresponding boxplots
side by side. These are usually displayed as vertical boxplots, mimicking the format of the
quantitative bar chart, above. Differences between the samples highlighted by the boxplots’
features will become immediately clear.
For K > 2 samples or groups, simply construct the five-number summaries for each sam-
ple and plot all K boxplots on the same scale. (Again, vertical boxplots are common.) For very
large values of K, it can become difficult to compare all groups easily, so use of this graphic
device is best reserved for K in, say, the range 2 ≤K ≤20.
Example 4.2.3 Driving speeds (Example 4.2.2, continued). Continuing with the NJ driving
speeds data in Table 4.3, Figure 4.14 plots a multiple boxplot of the speeds (in mph) over the
posted limit, stratified by licenses state of origin.
The plot was produced in R via the command
> boxplot( overlimI95 ˜ factor(licenseI95, levels=freqorder) )
where overlimI95 is the vector of over-limit speeds and licenseI95 is the corresponding
vector of states-of-origin. The tilde ˜ instructs R to ‘model’ the response variable as over-
limI95 and the explanatory variable as licenseI95. The factor() function builds the
explanatory levels from the different states of origin in the character variable licenseI95
and orders them using the levels= option by the frequencies in Figure 4.12. The sepa-
rate character variable freqorder (not shown) contains the pertinent ordering. Employed
within boxplot(), this creates multiple boxplots for overlimI95 over the different lev-
els of licenseI95. As in Example 4.2.2, consideration is limited to states along the I-95
corridor.
Figure 4.14 enhances the descriptive information provided on state origin-specific
over-limit speeds, compared to the simpler bar chart in Figure 4.13. The similarities in
central tendencies among states remain, as seen by the roughly similar medians in each box’s
center bar. Clearer indications are given, however, as to possible skew in over-limit speed
among many of the states: an (upper) right skew is evidenced by the numerous possible
upper outliers, and no lower outliers, in the plot. Further, there are clear differences in how

96
STATISTICAL DATA ANALYTICS
ME
NH
RI
GA
SC
DC
NC
FL
MA
CT
DE
VA
PA
MD
NY
NJ
0
10
20
30
Figure 4.14
Multiple boxplot of mean speeds (in mph) above posted limit in Example 4.2.3,
for NJ driving speeds data from Table 4.3. State-of-origin order displayed as per Figure 4.12.
Source: Data from Kadane and Lamberth (2009).
the skews distribute themselves among the different states of origin. Some states exhibit
multiple outliers, and a few – such as ME and SC – exhibit none. Indeed, the SC plot is
essentially symmetric. The multiple boxplot graphic exposes a number of interesting features
in these data.
◽
At the core of a boxplot’s construction is its use of quantile information, specifically the
three sample quartiles. Extending the operation to include additional quantiles and then plot-
ting them, as in the quantile plot of Section 4.1.5, produces a further comparison between
two samples or groups. To wit, suppose again that data are generated from two random sam-
ples: Xi ∼i.i.d. fX(x), i = 1, … , n, independent of Yj ∼i.i.d. fY(y), j = 1, … , m. From each
sample, compute the respective sample quantiles, say, ̂qxp and ̂qyp over a range of p ∈(0, 1).
If fX(⋅) and fY(⋅) are the same, one expects the sample quantiles to be roughly equal at each p.
To visualize this, plot ̂qyp versus ̂qxp. This is known as a quantile–quantile (Q–Q) plot.
The Q–Q plot is a useful device for exploring similarities or differences between two
samples’ distributions, although it does take some practice to interpret it correctly. If fX(⋅) and
fY(⋅) are the same, the plotted points will collect along a (roughly) straight line. In fact, this
will be a 45∘line. Deviations between the two distributions will appear as deviations from the
45∘line, either as a straight line with a slope different from 1 (X is linearly related to Y) or as
a curvilinear pattern (X and Y have different patterns of skew).
Example 4.2.4 Residential energy loads. Tsanas and Xifara (2012) presented a study
of factors that affect energy performance in residential buildings. They derived two
response variables, X = {Heating load} and Y = {Cooling load}, over a variety of 768

DATA VISUALIZATION AND STATISTICAL GRAPHICS
97
different building conditions. The paired data appear in Table 4.4. (As above, only a
selection of measurements is given in the table. The complete data are available at
http://www.wiley.com/go/piegorsch/data_analytics.)
The distributional features of each variable are explored in Exercise 4.9. Here, consider
comparison of the two distributions via a Q–Q plot. Figure 4.15 displays the plot, produced
using the R command qqplot(HeatLoad,CoolLoad). A 45∘line is superimposed via the
command abline(0,1).
Table 4.4
Selected data pairs (X, Y) with X = {Heating load} and Y = {Cooling load}, from
a larger set of 768 paired observations.
(6.01, 10.94)
(6.04, 11.17)
(6.05, 11.19)
· · ·
(42.96, 39.56)
(43.10, 39.41)
Source: Tsanas and Xifara (2012).
10
20
30
40
10
20
30
40
Heating load
Cooling load
Figure 4.15
Q–Q plot for X = {Heating load} and Y = {Cooling load} for energy load data
in Example 4.2.4. Source: Data from Tsanas and Xifara (2012).
The points in the Q–Q plot follow a roughly linear pattern, although slight convexity is
seen at the upper quantiles. This suggests that the two distributions’ shapes may be roughly
similar, with possible deviation in the upper tails. As the Q–Q plot is shifted away from the
45∘line, however, there is a likely difference in their means or variances (or in both).
◽
Notice that Example 4.2.4 illustrates the use of the Q–Q plot with paired data; however,
this is not a requirement of the method. Q–Q plots can be constructed for any two random
variables, whether their observations are paired or not. Indeed, the numbers of observations
in each sample, n and m, can be quite different. (The actual plot itself does involve paired
quantiles, but these need not be constructed from paired observations.)

98
STATISTICAL DATA ANALYTICS
4.2.3
Scatterplots and bubble plots
When bivariate data appear as pairs of observations (Xi, Yi), it is natural to plot them on an
(x, y) Euclidean grid. The resulting scatter of points is called a scatterplot, and it is one of the
most effective ways to visualize two-dimensional data. Strong linear or curvilinear relation-
ships between Y and X become evident, wide dispersion or variation in one or both variables is
usually easy to see, and even the lack of any relationship between the variables can be quickly
recognized (as, e.g., a cloud of points with no apparent slope or pattern).
The scatterplot is a mainstay instrument in the data-visualization toolkit. In R, there are a
number of different functions or commands that can produce it in one form or another. The
workhorse is plot(x,y) or plot(y ˜ x). Both forms produce a plot of y (vertical axis)
against x (horizontal axis). The latter is consistent with other uses of the tilde operator in R, as
an indicator of a model relationship of the form response variable ∼explanatory variable(s).
Example 4.2.5 Driving speeds (Example 4.2.1, continued). Return to the NJ driving
speeds data in Table 4.3. Recall from the state-of-origin frequency analysis in Example 4.2.1
that the majority of motorists in these data were identified with NJ license plates. These were
followed by plates from populous states such as NY, PA, MD, and VA. While many of these
states are close to NJ, it is worth asking whether state population could also play a role,
because the observed frequencies were not adjusted for home-state population.
Table 4.5 lists populations in 2009 of the 15 US states, and DC, along the I-95 transit
corridor, corresponding to the license plates recorded in this speeding study. As is well known,
NY and FL are the largest in population, followed by PA, GA, NC, and then NJ.
Table 4.5
Populations in 2009 for US states (and DC) along I-95 transit corridor.
State
Population
State
Population
State
Population
CT
3 518 288
MA
6 593 587
NY
19 541 453
DC
599 657
MD
5 699 478
PA
12 604 767
DE
885 122
ME
1 318 301
RI
1 053 209
FL
18 537 969
NC
9 380 884
SC
4 561 242
GA
9 829 211
NH
1 324 575
VA
7 882 590
NJ
8 707 739
Source: http://www.census.gov/popest/data/historical/2000s/vintage_2009/state.html.
Figure 4.16 plots the state-of-origin frequencies from Figure 4.12 against the populations
in Table 4.5, using the plot() command. The scatterplot shows an increasing band of points
from left to right: the state-of-origin frequencies from Figure 4.12 do appear to change in
proportion to increasing population size, validating the supposition that larger states tend to
supply more license plates for possible identification in this study. The pattern is not very
concentrated, however, suggesting that other factors could be at play as well, at least for the
motorists observed here.
Notice the clear outlier in the plot midway along the population scale but very high on the
frequency scale. As expected, this is NJ, where a ‘home-state’ effect likely is driving the large
number of NJ license plates observed with these data.
◽

DATA VISUALIZATION AND STATISTICAL GRAPHICS
99
0
5
10
15
20
0
500
1000
1500
2000
2009 population (millions)
Frequency
CT
DC
DE
FL
GA
MA
MD
ME
NC
NH
NJ
NY
PA
RI
SC
VA
Figure 4.16
Scatterplot in Example 4.2.5 of Y = {State-of-origin frequency} from
Figure 4.12 versus X = {State population} from Table 4.5. Points are labeled by state of
origin; large outlier at top-middle is NJ. Source: Data from Kadane and Lamberth (2009) and
http://www.census.gov/popest/data/historical/2000s/vintage_2009/index.html.
Example 4.2.5 gives only a brief indication of the power of a scatterplot and how it can
be used for visualizing information in two dimensions. Indeed, the plot() function in R
has a broad array of capabilities for creating two-dimensional graphical displays. Readers are
encouraged to explore its various features in greater depth.
Extension of the scatterplot when K = 3 triplet variables are under study is a more enig-
matic endeavor. Interactive three-dimensional (3D) holographic projections are perhaps in
our future; until then, programs do exist that rotate two-dimensional representations of the
3D relationship for useful visualization, as in the external rgl package. Simpler still are meth-
ods that distill the information down to a standard two-dimensional graphic; see, for example,
the external scatterplot3d package. Suppose the data triples are (Wi, Xi, Yi), i = 1, … , n. One
obvious way to visualize relationships among the three variables is to build all (three) possible
pairwise scatterplots: W versus X, W versus Y, and X versus Y. This strategy can be extended
to any K > 2, however, and so is explored further in the presentation of scatterplot matrices,
later in this section.
Another approach suitable for the K = 3 case is known as a bubble plot (Everitt 2005,
Section 2.4). The concept is fairly simple: begin with a standard scatterplot of Yi versus Xi
but then enhance the plotted points by drawing circles around each point. The circle’s areas
extend in proportion to the value of Wi at that i. (That is, the radius of each circle is propor-
tional to
√
Wi∕𝜋. Some authors alternatively size the circles’ radii directly in proportion to Wi,
although the effect has greater visual perspicuity if the circles are sized by area.) The result is
a scatterplot with varying ‘bubbles’ showing the impact of W on the X, Y pattern. To explore

100
STATISTICAL DATA ANALYTICS
other aspects of the trivariate relationship, switch the combination(s) of scatterplot pairings
and bubble variable.
Numerous variations exist for creating bubble plots: one can change the circles to squares,
or to diamonds, add labels or colors, and so on. If using R, the program’s powerful variety of
graphic capabilities can enhance the visual effect.
A popular infographic offshoot that quantifies text/word frequencies by mimicking the
bubble plot’s approach for relative sizing is in http://www.wordle.net/.
Example 4.2.6 Automobile fuel economy. The US Department of Energy, in concert with
the US Environmental Protection Agency (EPA), reports on automobile fuel economy in terms
of miles driven per gallon of gasoline (MPG). For the 2011 automobile model year, these
agencies gave MPG data on n = 652 automatic-transmission vehicles and included a number
of other possible variables associated with each vehicle’s energy efficiency. Table 4.6 presents
Y = {MPG} for these vehicles (combined highway/city, conventional fuel), along with two
additional variables: X = {Engine displacement (in liters)} and W = {Number of cylinders}.
(As above, only a selection of measurements is given in the table. The complete data are
available at http://www.wiley.com/go/piegorsch/data_analytics.)
Table 4.6
Selected data triplets on automobile fuel economy: (W, X, Y) with W = {No. cylin-
ders}, X = {Engine displacement (L)}, and Y = {MPG}, from a larger set of 652 triplicate
observations.
(2, 1.3, 24.2445)
(4, 2.0, 37.5530)
(4, 1.6, 37.4670)
· · ·
(16, 8.0, 12.4782)
Source: http://www.fueleconomy.gov/feg/download.shtml.
A simple scatterplot of Y = {MPG} versus X = {Engine displacement} (Exercise 4.16)
shows a curvilinear, inverse relationship between the two variables, as might be expected. To
visualize how W = {Number of cylinders} also relates, Figure 4.17 presents a bubble plot
where the bubbles are proportional in area to W. The figure is actually presented to exemplify
two slight alternatives for the plot: (i) Figure 4.17a is a basic bubble plot with the X, Y points
plotted as small dots, and with transparent bubbles overlayed; this helps emphasize the precise
locations of the X, Y points. (ii) Figure 4.17b is a simpler bubble plot only showing the bub-
bles but shading them (here, in grayscale) and adding contrasting borders. This loses location
specificity for the plotted points but counters by emphasizing the bubble effect. (Readers can
decide which gives a more instructive graphic for delivering the visual information.)
R code to produce the plots (not including label enhancement) takes advantage of the
symbols() function:
> #left panel: scatterplot with bubble overlay
> plot( Y ˜ X , pch=‘.’ , xlim=c(1,8.5), ylim=c(10,60) )
> radius <- sqrt( W/pi )
#for area proport’l to W
> symbols( X , Y, circles=radius, inches=.21, add=T )
>
> #right panel: shaded bubble plot
> symbols( X, Y, circles=radius, inches=.21, bg=‘gray’,
fg=‘white’, xlim=c(1,8.5), ylim=c(10,60) )

DATA VISUALIZATION AND STATISTICAL GRAPHICS
101
2
4
6
8
10
20
30
40
50
60
(a)
(b)
Engine displacement (L)
MPG
2
4
6
8
10
20
30
40
50
60
Engine displacement (L)
MPG
Figure 4.17
Two forms of bubble plot from Example 4.2.6 with Y = {MPG} plotted
against X = {Engine displacement (L)}, and with overlayed bubble areas proportional to
W = {No. cylinders} for fuel economy data in Table 4.6. Panel (a) gives specific locations
of X, Y points (dots); panel (b) employs shaded bubbles for contrast. Source: Data from
http://www.fueleconomy.gov/feg/download.shtml.
In the end, both versions of the bubble plot meet the graphical objective: we see that engine
displacement detrimentally affects MPG in an apparently curvilinear manner. There is also
a concomitant, recognizable, detrimental effect of number of cylinders, in that lower MPG
appears related to higher numbers (i.e., bigger bubbles).
◽
When the number of variables grows to K ≥3, the visualization process increases in
complexity. Suppose the jth K-tuplet is (X1j, X2j, … , XKj), j = 1, … , n. As noted above, one
approach for graphing the variable relationships is to construct two-dimensional scatterplots
over all
(
K
2
)
= 1
2K(K −1) possible variable pairings. A popular device for this is known as
a scatterplot matrix: each plot of the pairwise scatter between Xij and Xkj (i ≠k) is drawn as
a small square scatterplot, then all the smaller plots are trellised into a larger K × K square
‘matrix’ of scatterplots. Unique information is only provided in the upper (or lower) triangle
of smaller scatterplot cells, because the (i, k)th cell plots the same variables as the (k, i)th cell.
(The reflected perspective can often yield some useful visualizations, however.)
Notice that the main diagonal of a scatterplot matrix will trivially contain scatterplots of
each Xij against itself that is, straight 45∘patterns – and is, therefore, filled only with the ith
variable’s name. Alternatively, other clever graphics can be inserted into the diagonal cells,
such as univariate histograms of Xij; see (Everitt 2005, Section 2.5).
In R, a scatterplot matrix is produced via the pairs() function, although other alternative
functions in various specialized packages can generate it as well.

102
STATISTICAL DATA ANALYTICS
Example 4.2.7 Wheat kernels (Example 3.3.1, continued). Return to the agricultural study
of wheat grain characteristics (Charytanowicz et al. 2010) from Example 3.3.1. Along with
n = 210 measurements of X1 = {Kernel length} in Table 3.1, the following additional variables
were recorded:
X2 = Kernel width
X3 = Kernel asymmetry
X4 = Kernel groove length
X5 = Kernel area
X6 = Kernel perimeter, and
X7 = Kernel compactness = 4𝜋X5∕X2
6.
Table 4.7 lists a selection of measurements from this larger data set. (The complete data
are available at http://www.wiley.com/go/piegorsch/data_analytics.)
Table 4.7
A selection of wheat kernel data from a larger set of n = 210 observations.
Variables
Index, i
X1i
X2i
X3i
X4i
X5i
X6i
X7i
1
4.899
2.787
4.975
4.794
10.59
12.41
0.8648
2
4.902
2.879
2.269
4.703
11.23
12.63
0.8840
⋮
⋮
⋮
⋮
⋮
⋮
⋮
⋮
209
6.666
3.485
4.933
6.448
18.36
16.52
0.8452
210
6.675
3.763
3.252
6.550
19.94
16.92
0.8752
Variables are defined in Example 4.2.7.
Source: Charytanowicz et al. (2010).
To visualize possible interrelationships among these variables, construct a scatterplot
matrix. For simplicity, limit attention to only the K = 5 variables X1, X2, X3, X4, andX7. (One
might expect a priori that X5, X6, and X7 will be highly interrelated; see Exercise 4.17.) To
produce the matrix in R, the data vectors are collected together into an R data frame (see
Appendix B), say wheat.df, which becomes the single argument of the pairs() function:
pairs(wheat.df). The result appears in Figure 4.18.
The scatterplot matrix here identifies strong relationships between (i) kernel length versus
width; (ii) length versus groove length; and, to a lesser degree, (iii) width versus groove
length. None of these are terribly surprising. A strong, possibly curvilinear relationship
appears between (iv) kernel width versus compactness, while the connections between
(v) kernel length versus compactness and (vi) groove length versus compactness are less
certain; the scatters are broadly dispersed, but possible patterns may be buried therein. These
interrelationships may be worthy of further investigation. Lastly, all four scatterplots based
on the kernel asymmetry measure appear dispersed, with disorderly scatter. As a consistent
pattern, this may represent its own kind of informative feature. The scatterplot matrix here
gives the analyst much to consider.
◽
4.2.4
Heatmaps
Suppose multidimensional data are available in the form of a rectangular matrix, M,
where the rows are categorized by a row variable Ri and the columns are categorized by a

DATA VISUALIZATION AND STATISTICAL GRAPHICS
103
length
2.6 3.0 3.4 3.8
4.5
5.5
6.5
5.0
5.5
6.0
6.5
2.6
3.0
3.4
3.8
width
asym
2
4
6
8
4.5
5.5
6.5
groove
5.0 5.5 6.0 6.5
2
4
6
8
0.82 0.86 0.90
0.82
0.86
0.90
compact
Figure 4.18
Scatterplot matrix in Example 4.2.7 of K = 5 wheat kernel variables from
Table 4.7: X1 = length, X2 = width, X3 = kernel asymmetry, X4 = groove length, and X7 =
compactness. Source: Data from Charytanowicz et al. (2010).
column variable Cj, i = 1, … , I; j = 1, … , J. The categorization variables can be qualitative
or quantitative, although the individual elements, mij, of M are assumed quantitative.
Another enhanced graphic useful for visualizing the relationships among the mijs using the
row–column ordering is known as a heatmap.
The heatmap is a simple display that takes advantage of modern graphical plotting hard-
ware and software. First, from a user-designated color spectrum, it assigns a specific color to
each possible value of mij. Then, it plots these colors in place of the actual m-values in the
same location/order they appear in M. (If true color is not available, the scheme can involve
simple shades of gray.) The effect is a color-based graphic from which it is often easier to
visualize patterns in the data than if one simply examined the raw values of mij.

104
STATISTICAL DATA ANALYTICS
If the R and C variables have a natural order or quantification, the graphic also doubles as
a form of surface plot. Indeed, heatmaps are a common vehicle for displaying topographic or
terrain-based information. More generally, however, they can serve as useful visualization for
exploring patterns in very large matrices. The next example gives a simple illustration.
Example 4.2.8 Heatmap for correlation matrix. Heatmaps are often applied for visualiz-
ing patterns in correlation matrices, that is, matrices that contain the sample correlations, rij,
over multiple pairs of variables Xi and Xj. The resulting matrix R is then square and symmetric
of order J, where J is the number of variables under study. When J is small, visualization of
the correlation structure is usually straightforward. When J grows past about 8–10, however,
teasing out potential patterns among the correlations can become problematic. A heatmap of
R provides a useful visualization tool.
For instance, Lin and Bhattacherjee (2010) presented correlations among J = 19 variables
from a psychometric study of computer-game users. Of interest was the users’ acceptance of
evolving online and interactive gaming/entertainment technologies. The 19 variables covered
a range of measurement constructs, involving Usage Intention (‘UI’ – 3 variables), Attitude
(‘AT’ – 2 variables), Perceived Enjoyment (‘PE’ – 3 variables), Social Image (‘SI’ – 3 vari-
ables), Technical Quality (‘TQ’ – 3 variables), and Interaction Quality (‘IQ’ – 4 variables);
see Lin and Bhattacherjee (2010, Appx. A). The resulting correlations, shown in Table 4.8,
are used to examine how the variables interact with each other. (The ordering in the table is
arbitrary; for optimal use of the heatmap, the different variables are listed together by pertinent
construct groups.)
The heatmap corresponding to the correlation matrix in Table 4.8 appears in Figure 4.19.
The basic heatmap can be constructed in R using the command
> heatmap( R, Rowv=NA, Colv=NA, scale=‘none’, revC=T,
col=heat.colors(256), margins=c(4,4) )
where the input argument R is the correlation matrix of values from Table 4.8. The Rowv=
and Colv= options provide tree-structured diagrams (called dendrograms; see Section 9.4.1
or 11.1.1) that add classification capabilities to the basic heatmap; these are suppressed here.
The revC=T option reverses the column order for plotting, while col= calls for the ‘hot’
red-to-white color spectrum seen in the figure. (Users are encouraged to experiment with the
wide capabilities of the heatmap command.)
The heatmap in Figure 4.19 quickly highlights a number of patterns, including a grouping
of high correlations among the IQ variables and among the TQ variables (but not between
them). Relatively large correlations are also seen among and between the UI and AT variables.
The AT variables – particularly AT3 – also correlate marginally with the TQ variables. Other
patterns are more muted, with correlations closer to 0 (darker color) appearing throughout the
map. Overall, the heatmap gives a rapid and effective visualization of the correlation structure
seen with these variables.
◽
Heatmaps have surprisingly inherent flexibility and come in a number of versions; see
Exercises 4.20 and 11.4c. A closely related graphic is the choropleth map and its many vari-
ants, popular in cartographic displays of statistical data (Stewart and Kennelly 2010). (For
choropleth maps in R, one can use, e.g., the external ggplot2 package.) With increasing avail-
ability of powerful computer graphic and visualization hardware, use of this statistical display
is limited only by the analyst’s imagination.

DATA VISUALIZATION AND STATISTICAL GRAPHICS
105
Table 4.8
A 19 × 19 Correlation matrix among J = 19 variables in Example 4.2.8
reporting gamer acceptance of online/interactive entertainment technologies (upper
triangular portion only; lower triangle is symmetric).
UI1
UI2
UI3
AT1
AT2
AT3
PE1
PE2
PE3
UI1
1
0.73
0.40
0.42
0.43
0.31
0.24
0.28
0.27
UI2
1
0.51
0.48
0.50
0.36
0.27
0.30
0.24
UI3
1
0.38
0.49
0.36
0.25
0.19
0.22
AT1
1
0.64
0.45
0.33
0.38
0.29
AT2
1
0.50
0.32
0.36
0.32
AT3
1
0.25
0.36
0.31
PE1
1
0.56
0.33
PE2
1
0.53
PE3
1
SI1
SI2
SI3
TQ1
TQ2
TQ3
IQ1
IQ2
IQ3
IQ4
UI1
0.22
0.28
0.20
0.18
0.17
0.19
0.17
0.13
0.23
0.18
UI2
0.23
0.37
0.24
0.23
0.23
0.26
0.21
0.22
0.27
0.22
UI3
0.20
0.31
0.30
0.16
0.18
0.17
0.24
0.26
0.30
0.29
AT1
0.41
0.38
0.45
0.36
0.31
0.36
0.17
0.22
0.21
0.28
AT2
0.30
0.34
0.32
0.34
0.33
0.36
0.19
0.23
0.30
0.31
AT3
0.28
0.23
0.25
0.50
0.49
0.49
0.27
0.31
0.32
0.35
PE1
0.39
0.26
0.53
0.15
0.15
0.24
0.19
0.15
0.19
0.27
PE2
0.35
0.20
0.36
0.23
0.24
0.25
0.27
0.28
0.32
0.34
PE3
0.32
0.25
0.25
0.21
0.22
0.19
0.20
0.19
0.19
0.24
SI1
1
0.55
0.52
0.28
0.22
0.27
0.25
0.26
0.18
0.26
SI2
1
0.46
0.21
0.21
0.20
0.16
0.14
0.19
0.20
SI3
1
0.15
0.17
0.17
0.23
0.20
0.25
0.24
TQ1
1
0.74
0.67
0.23
0.26
0.21
0.30
TQ2
1
0.71
0.21
0.24
0.22
0.23
TQ3
1
0.24
0.26
0.19
0.28
IQ1
1
0.74
0.66
0.58
IQ2
1
0.65
0.59
IQ3
1
0.60
IQ4
1
Source: Lin and Bhattacherjee (2010).
4.2.5
Time series plots∗
When observations are taken as a sequence of recordings over consecutive time periods, they
are known as a time series (Box et al. 2008). An illustration of this was seen in Example 3.5.2,
with daily closing prices of the US Dow Jones Industrial stock index. Times series data are
quite common in financial applications; however, they also occur in many other domains, such
as medicine, environmental/ecological science, and sociology.
Time series are distinguished by their connection with a consecutive, increasing time unit,
t. The recordings are usually equally spaced so that the observation Xt may be indexed against

106
STATISTICAL DATA ANALYTICS
UI1
UI2
UI3
AT1
AT2
AT3
PE1
PE2
PE3
SI1
SI2
SI3
TQ1
TQ2
TQ3
IQ1
IQ2
IQ3
IQ4
IQ4
IQ3
IQ2
IQ1
TQ3
TQ2
TQ1
SI3
SI2
SI1
PE3
PE2
PE1
AT3
AT2
AT1
UI3
UI2
UI1
Figure 4.19
Heatmap of gaming user correlation matrix from Example 4.2.8. Lighter colors
indicate correlations near 1, darker colors near 0. Scale at left gives the correlation spec-
trum from 1 (white, top) to 0 (dark, bottom). Source: Data from Lin and Bhattacherjee
(2010).
t = 1, … , n. If so, a graphic device to visualize any temporal patterns in the data is known as a
time series plot, or sometimes also a trace plot, because it traces the progression of the process
for each successive Xt. The concept is essentially a scatterplot of Xt versus t, although the con-
secutive points are usually connected with lines to better explicate the temporal progression.
(If the points are dense enough, as in Figure 3.1, connecting the points may not be necessary.
Figure 3.1 also illustrated overlay of a moving average to visualize smoothed patterns in the
times series.)
Example 4.2.9 Nightingale’s Mortality Data. A famous data set in statistical graphic
design involves deaths due to differing causes in the British Army during the Crimean War,
as reported by Florence Nightingale (1858). Entitled ‘Diagram of the Causes of Mortality in

DATA VISUALIZATION AND STATISTICAL GRAPHICS
107
the Army in the East,’ Nightingale used a novel infographic (now) known as a polar area plot
(see http://www.sciencenews.org/view/generic/id/38937/description/Florence_Nightingale_
The_passionate_statistician) to argue for improved sanitary conditions in hospitals and
medical facilities (not just in war zones); see McDonald (2014).
As mentioned in Section 4.2.1, however, the visualization value of a polar area plot can
often be improved. As the British mortality data were taken over time (months between April
1854 and March 1856), a time series plot provides an effective way to visualize the rates. It
can also compare and highlight differences across causes of death over the same time scale,
as was Nightingale’s intent.
The data in Table 4.9 present the British Army’s annualized mortality rates (per 1000 sol-
diers) during the Crimean War. The three separate causes of death are wounds and injury,
preventable/mitigable disease, and ‘all other,’ where primary comparison is focused on the
former two. (As above, only a selection of measurements is given in the table. The com-
plete data are available at http://www.wiley.com/go/piegorsch/data_analytics.) To compare
the mortality rates in Table 4.9 due to wounds and injury with those due to preventable dis-
eases, Figure 4.20 graphs the two rates over time in the same time series plot. (The different
series are indicated by different line styles, but for greater effect, contrasting colors could
also be used; cf. Gelman and Unwin (2013, Fig. 6).] The difference between the two series
is clear. The complexity other graphic devices have used to illustrate this feature belie their
value, however; the simple time series plot does the job quite effectively. Also see Exercise
4.18.
Table 4.9
Selected data on annualized mortality (per 1000) of British casualties during the
Crimean War from a larger set of 24 monthly observations, made famous by Florence Nightin-
gale (1858).
Month
Wounds & injury
Preventable disease
All other causes
April 1854
0.0
1.4
7.0
May 1854
0.0
6.2
4.6
⋮
⋮
⋮
⋮
January 1855
30.7
1022.8
120.0
February 1855
16.3
822.8
140.1
⋮
⋮
⋮
⋮
March 1856
0.0
3.9
9.1
Source: http://understandinguncertainty.org/node/214
◽
The basic time series plot can be enhanced if additional, multivariate information is avail-
able at each time t. For instance, suppose multiple sites or subjects provide data at each t. If
the number of these replications is large, a simple plot of all the points may hide or distract
from the larger trend(s). Conversely, only plotting the summary location values such as the
median or mean of the replicates loses information about dispersion in the data. Better in this
case is to replace the points with a summary graphic such as a (vertical) boxplot at each t. The
bars at the medians can be replaced with dots or other easy-to-see symbols to help visualize
trend(s), while the boxes and whiskers provide visual information on other facets of the time
series process.

108
STATISTICAL DATA ANALYTICS
0
200
400
600
800
1000
Month
Mortality rate/1000
Apr 1854
Aug 1854 Dec 1854
Apr 1855
Aug 1855 Dec 1855
Figure 4.20
Overlayed time series plots in Example 4.2.9 to contrast mortality rates
during the Crimean War; data from Table 4.9. Dashed line (– – –) is mortality due to
preventable disease; solid gray line (
) is mortality due to wounds and injury. Source:
Data from http://understandinguncertainty.org/node/214. Graphic adapted from Gelman and
Unwin (2013).
Example 4.2.10 Rocky Mountain Front Range rainfall. The US National Center for
Atmospheric Research (NCAR) collects data on various meteorologic phenomena for
climatological modeling. For example, in a precipitation study in the US Rocky Mountains’
Colorado Front Range, data were collected on X = {Total summer (April–October) rainfall}
over the period 1949–2001. The data were taken at a series of 56 separate recording stations,
allowing for study of rainfall totals both within and across time points. Selected values
appear in Table 4.10. (The complete data are available at http://www.wiley.com/go/piegorsch
/data_analytics.)
Table 4.10
A selection of total summer rainfall totals (inches) from a larger study of US
Rocky Mountain Front Range precipitation.
Year
Station 1
Station 2
Station 3
· · ·
Station 54
Station 55
Station 56
1949
17.74
–
12.27
· · ·
10.17
11.27
–
1950
11.99
–
10.37
· · ·
7.49
9.10
–
⋮
⋮
⋮
⋮
⋱
⋮
⋮
⋮
2000
11.50
4.80
–
· · ·
7.40
11.80
8.70
2001
9.90
9.90
–
· · ·
6.40
7.50
10.80
Dashes (–) indicate no (complete) summer data were available at that station.
Source: http://www.image.ucar.edu/∼nychka/FrontrangePrecip/.

DATA VISUALIZATION AND STATISTICAL GRAPHICS
109
1949 1953 1957 1961 1965 1969 1973 1977 1981 1985 1989 1993 1997 2001
0
5
10
15
20
25
Year
Total summer rainfall (inches)
Figure 4.21
Time series plot with boxplot graphics in Example 4.2.10 of total summer rain-
fall (in inches) for US Rocky Mountain Rainfall data from Table 4.10. Source: Data from
http://www.image.ucar.edu/∼nychka/FrontrangePrecip/.
To visualize the rainfall totals over time, Figure 4.21 displays a time series plot, with
embedded boxplots to visualize across-station variation. The graphic was created in R via
> boxplot( X, use.cols=FALSE, names=seq(1949,2001) )
where X is a 53 × 56 matrix whose rows are the 56 summer station totals. The default hori-
zontal bars at the medians were replaced with dots using the options medlty=‘blank’ and
medpch=19. The median dots were connected with repeated use of the lines() function.
The time series plot shows wide variation in rainfall totals, which may prompt the NCAR’s
study of its extremes. The central tendency of the totals is between 7 and 15 inches per summer,
but they can reach as high as 25 inches. The graphic also identifies a few very low points:
see, for example, the possible lower outliers in 1949, 1966, 1985, or 1992. The latter is a
record of 0 inches – that is, no rainfall – at Station #25 that summer. This may be a naturally
occurring drought event, a mechanical/system recording malfunction, or just data entry error.
Exploring which of these possibilities could account for the unusual data point(s) is worth
deeper investigation.
◽
R has a number of ways to construct times series plots, including the powerful, omnibus
plot() function and also ts.plot() and plot.ts(). Users should experiment with the
various options to chose the best routine for their individual needs.
The material in this chapter only begins to illustrate the value statistical visualization can
bring to the data analytic enterprise. Indeed, the static graphics presented here are only a first

110
STATISTICAL DATA ANALYTICS
step: dynamic and interactive data visualization is an evolving area, populated by members
of both the statistical and infovis communities (Henderson 2004; Huang et al. 2012; Young
et al. 2006); also see the web site http://www.gapminder.org (and Exercise 4.15).
To learn more about statistical visualization, see the advanced treatments in, for example,
Murrell (2011) or Wickham (2012) (and the references therein), the seminal works by Cleve-
land (1993) and Wilkinson (2005), or the popular expositions by Tufte (2001) and Yau (2011).
The popularity of graphic design also allows for some humorous, tongue-in-cheek perspec-
tives; see http://xkcd.com/688/ or http://www.theonion.com/articles/americas-most-popular-
charts,7492/ and the associated web sites.
Exercises
4.1
Return to the following data sets and construct a strip chart for each. Comment on the
patterns the graphic exposes, if any.
(a) The wheat kernel data in Table 3.1.
(b) The square-root-transformed wheat kernel data in Exercise 3.15b.
(c) The log-transformed circulatory-disease mortality data in Exercise 3.17.
(d) The husbands’ heights data in Exercise 3.2.
4.2
Recall the circulatory-disease mortality data in Example 4.1.1. Construct a dot plot of
the data and comment on the patterns that emerge. Compare this to the strip chart in
Figure 4.1.
4.3
Return to the following data sets and construct a boxplot for each. Include a rug at
bottom with the data values. What does the graphic indicate about the distribution of
the data?
(a) The hazard vulnerability data from Exercise 3.3.
(b) The solar radiation data from Exercise 3.4.
4.4
Return to the circulatory-disease mortality data in Table 3.4 and construct a stemplot.
(If using R, apply both scale=1 and scale=2 in stem() to see the effect of expanding
the scale.) How does this compare to the histogram for these data plotted in Figure 4.6?
4.5
For the following data, construct a histogram (indicate the bin selection algorithm you
use). Overlay a kernel density estimator and include a rug at bottom with the data
values. Comment on how these compare to the corresponding boxplot in Exercise 4.3
in terms of summarizing features of the data.
(a) The hazard vulnerability data from Exercise 3.3.
(b) The solar radiation data from Exercise 3.4.
4.6
Construct a histogram for the myocardial infarction data in Table 4.2. Include a rug at
bottom with the data values. How does the graphic compare to the stemplot (if viewed
at 90∘rotation) plotted in Figure 4.5?
4.7
Recall that in Examples 4.1.3 and 4.1.5, a right skew was evidenced with the
circulatory-disease mortality data from Table 3.4. With this in mind, explore the
following alternatives:

DATA VISUALIZATION AND STATISTICAL GRAPHICS
111
(a) Apply Scott’s normal reference rule for selecting the number of bins, using the
results in Exercise 3.19. Plot the corresponding histogram (include a rug at bottom
with the data values). Does it differ substantively from the plot in Figure 4.6?
(b) Apply a (natural) logarithmic transformation to the original mortality rates as in
Example 3.17 and construct (i) a box plot, (ii) a stem plot, and (iii) a histogram
for the transformed rates (overlay a kernel density estimator on the histogram and
include a rug at bottom with the data values). Did the transformation act to alleviate
the skew? Also graph (iv) a normal quantile plot. Do the transformed data appear
normal?
4.8
Insert the Gaussian kernel KGauss(t) = (2𝜋)−1∕2 e−(1∕2)t2 into the general expression for
a kernel density estimator in Equation (4.1). Show that the result can be written as
a weighted average of normal p.d.f.s, each with constant variance h2. What are the
weights in this ‘weighted average?’ Can you imagine changing these weights to pro-
duce a different, specialized density estimator? How?
4.9
Return to the energy load data from Table 4.4. To explore distributional features of the
individual variables X = {Heating load} and Y = {Cooling load}, construct the follow-
ing summary graphics. In all cases, comment on the visual similarities or differences
between the variables’ distributions. Compare this with the conclusions reached using
the Q–Q plot in Example 4.2.4.
(a) Construct stemplots for each variable. If you have the programming capability, dis-
play this as a side-by-side stemplot.
(b) Bin each variable using Scott’s normal reference rule and plot the corresponding
histograms. Overlay kernel density estimators and include a rug at bottom for each.
(c) Construct multiple boxplots (on the same plot) for each variable.
4.10
For the NJ driving speeds data in Table 4.3, construct a pie chart for the state-of-origin
frequencies in Example 4.2.1. If in R, use the pie() function. Comment on the quality
of information the plot presents.
4.11
Return to the husbands’ and wives’ heights data in Exercise 3.12.
(a) Construct a multiple boxplot graphic for both variables. How do the two individual
boxplots compare with one another?
(b) Graph a Q–Q plot to compare the two variables. Does the information in the Q–Q
plot corroborate that seen in the multiple boxplot?
(c) Graph a scatterplot for the paired data. Comment on any patterns.
4.12
The solar radiation data in Exercise 3.4 are actually part of a larger collection, where
K = 4 different photometers recorded median extreme ultraviolet (XUV) irradiance
over the 0.1–7.0 nm range. Selected data are

112
STATISTICAL DATA ANALYTICS
Device
XUV irradiance
D1
0.000004
0.000004
· · ·
0.001020
0.001220
D2
0.000123
0.000124
· · ·
0.001140
0.001210
D7
0.000041
0.000044
· · ·
0.001540
0.001560
D9
0.000241
0.000242
· · ·
0.000788
0.000838
(Download the full data set at http://www.wiley.com/go/piegorsch/data_analytics.)
Graph a multiple boxplot for XUV irradiance across all four devices. What patterns
do you discover?
4.13
A novel combination of a boxplot and a kernel density estimate is known as a violin
plot. This essentially overlays the density estimate on the boxplot and then reflects it
around the boxplot to give a stylize graphic that looks somewhat like a violin. Violin
plots are available in R via the external vioplot package. Download this or any other
software that can produce violin plots and use it to build a multiple violin plot for the
NJ driving speed data in Example 4.2.3. Compare the graphic to the multiple boxplot
in Figure 4.14. Does the new plot provide a more descriptive perspective on these data?
4.14
Return to the college admissions data in Example 3.3.6.
(a) Construct a Q–Q plot to compare the paired variables X = {ACT score} and Y =
{Class rank}. How do the patterns of variation compare?
(b) Graph a scatterplot for the paired data. Comment on any patterns.
4.15
The bubble plot gained in popularity after its adroit use by Swedish statistician/
physician Hans Rosling to visualize worldwide life expectancy as a function of
national gross domestic product (GDP); see http://www.gapminder.org/world. To
imitate his famous graphic, consider the variables
W = 2011 Total population,
X = 2011 GDP per capita (in 2000 dollars, inflation-adjusted), and
Y = 2011 Life expectancy at birth
for a given nation. There are n = 149 nations in the archives at http://www
.gapminder.org for which all three values are available. From them, we can form the
triplets (Wi, Xi, Yi):
(86 165, 629.955, 51.093)
(3 215 988, 2255.225, 73.131)
(89 612, 11601.630, 75.901)
(245 619, 5671.912, 74.402)
⋮
⋮
(548 377, 757.401, 75.181)
(13 474 959, 347.746, 51.384)
(Download the full data set at http://www.wiley.com/go/piegorsch/data_analytics.)
(a) Construct a bubble plot: plot Y = {Life expectancy} against X = {GDP per capita}
and build bubbles whose areas are proportional to W = {Total population}. What
pattern(s) emerge?

DATA VISUALIZATION AND STATISTICAL GRAPHICS
113
(b) While the number of nations providing data triplets here is n = 149, some might
argue that the number of ‘subjects’ is much larger: each Wi is (an estimate of) that
nation’s population, as contributing to the bubble sizes in the plot. What is the total,
W+ = ∑149
i=1 Wi ? What does this number represent?
4.16
For the automobile fuel economy data in Table 4.6, build a scatterplot matrix for the
three variables. Does the MPG versus engine displacement subplot reproduce features
of the bubble plot in Figure 4.17? What other patterns become evident in the scatterplot
matrix?
4.17
Recall the full wheat kernel data in Table 4.7. Construct a scatterplot matrix for all K =
7 variables. Do any new or unexpected patterns emerge? Also calculate all pairwise
correlations between the variables and comment on any correspondences.
4.18
For Nightingale’s mortality data in Table 4.9, expand the visual in Figure 4.20 to
include all three causes of mortality. How does this new graphic add to the story?
4.19
A classic set of time series data that led to knowledge discovery involved the
11-year solar sunspot cycle. In 1849, J.R. Wolf of the Berne Observatory proposed
and later published (Wolf 1856) an index, with which he intended to quantify the
relative pattern of dark prominences (‘spots’) seen on the surface of Earth’s sun.
From this, Wolf amassed a database of relative sunspot indices, which has since
been continuously updated. Different versions of the series exist, depending on
which data source one accesses; here, consider a set studied by Piegorsch and Bailer
(2005) with the monthly indices from 1900 to 1977. The data are available online at
http://www.wiley.com/go/piegorsch/data_analytics; a sample follows:
Month/year
Spot index
Month/year
Spot index
Jan 1900
9.4
Feb 1900
13.6
Mar 1900
8.6
Apr 1900
16.0
⋮
⋮
⋮
⋮
Jan 1977
23.1
Feb 1977
8.7
(a) Build a time series plot of the sunspot activity as a function of time. Is the famous
11-year cycle evident?
(b) To visualize the 11-year cycle better, apply an 11-year moving average smoother
(Section 3.5.2) to the raw sunspot data and overlay the smoothed response on the
raw plot.
4.20
An interesting combination of a time series plot with a heatmap occurs when
one of the dimensions of a heatmap’s matrix M is time, producing a temporal
heatmap. For example, the US Energy Information Administration (EIA) pro-
vides data on US energy production and consumption, available at http://www
.eia.gov/electricity/data/browser/. From this source, monthly rates are available for
each US state’s (and the District of Columbia’s) electric power consumption (in 107
MMBtu, by all fuel types) from January 2001 to May 2013. A sample is given as
follows. (The full data are given online at http://www.wiley.com/go/piegorsch/data
_analytics.)

114
STATISTICAL DATA ANALYTICS
Selected monthly energy consumption by US state,
in 107 MMBtu, all fuel types
Region
State
Jan 2001
Feb 2001
· · ·
May 2013
New England
Connecticut
14
11
· · ·
10
Maine
9
8
· · ·
1
⋮
⋮
⋮
⋱
⋮
West South Central
Oklahoma
44
38
· · ·
46
Texas
240
202
· · ·
242
⋮
⋮
⋮
⋱
⋮
Pacific Contiguous
California
100
87
· · ·
57
Oregon
11
11
· · ·
4
Washington
18
17
· · ·
2
Pacific Noncontiguous
Alaska
5
5
· · ·
4
Hawaii
8
7
· · ·
7
View the data as a 51 × 149 matrix with time in months as the column variable
and construct from this a temporal heatmap. (If using R’s heatmap command, do not
standardize the observations across rows, as is the default with that function.) Con-
sider experimenting with color schemes to improve the visualization. Comment on the
patterns that appear.

5
Statistical inference
The power of data analysis is best exemplified when its inferential engine is engaged, extend-
ing the summarizations available from simple statistical description. Statistical inference is
designed to derive conclusions about a population, using information in random samples from
that population. This chapter reviews the basic components of the inferential paradigm, build-
ing on the probability models described in Chapter 2 and on the descriptive methods reviewed
in Chapters 3 and 4. Some readers will find much of this to be a review; others may benefit
from a careful reading. The material begins with basic theoretical definitions and concepts
and then segues to an introduction to some standard inferential methods. The material stands
as a final foundation and gateway of the larger aspects of statistical data analytics that begin
with supervised learning and regression modeling in Chapter 6 and beyond.
5.1
Parameters and likelihood
Suppose that a random sample Xi ∼i.i.d. fX(x), i = 1, … , n has been observed as in
Section 3.1 and that a formal model is taken for the probability mass function (p.m.f.) or
probability density function (p.d.f.) fX(x). The basic tenet of statistical inference is that the
available information about fX(x) is contained within the sample’s data. Although the various
models for fX(x) can vary widely – cf. Section 2.3 – they usually exhibit one common feature:
they depend on one or more parameters to describe variation in X. For example, the normal
distribution in Section 2.3.9 has two parameters: the population mean 𝜇and the population
variance 𝜎2. These are sufficient to completely characterize the normal p.d.f. In most random
samples, however, these values are not known in advance, and so they are referred to as
unknown parameters. The process of statistical description is used to estimate these unknown
quantities, but it is statistical inference that connects the estimates with statements about the
population they describe.
The p ≥1 unknown parameters from a p.m.f. or p.d.f. are assembled together into a param-
eter vector 𝛝= [𝜃1 · · · 𝜃p]T, where 𝜃j is used as generic notation for the jth parameter in the
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

116
STATISTICAL DATA ANALYTICS
model, and superscript T denotes the transpose of a vector. (See Appendix A for a review of
vector and matrix terminology.) Similarly, the random sample can be collected together into
a random vector X = [X1 · · · Xn]T.
To explicate the dependence on 𝛝in the model, the notation for the p.m.f. or p.d.f. is
extended into fX(x|𝛝). The bar ‘|’ is borrowed from the conditional probability notation in
Section 2.1.1 – that is, probability mass or density for X is ‘conditional’ on the particular value
𝛝takes on – although no assumption is made that 𝛝is itself random. Similar to (2.11), under
simple independent, identically distributed (i.i.d.) sampling the corresponding joint p.m.f. or
p.d.f. for the entire random sample can be constructed by appeal to the Multiplication Rule
(2c) from Section 2.1.1. The result is the product of the individual p.m.f.s or p.d.f.s:
fX1,X2, … ,Xn(x1, x2, … , xn|𝛝) =
n
∏
i=1
fX(xi|𝛝) .
(5.1)
If, as introduced above, the available information about 𝛝is contained within the observa-
tions, then the joint probability function represents a statistical model for relating the data
to 𝛝, and vice versa. This fundamental concept was formalized by Fisher (1912, 1922) who
essentially reversed the perspective in (5.1) and viewed the construct as a function of 𝛝given
the information in the observed data x1, x2, … , xn. Fisher called this a likelihood function:
L(𝛝; x1, x2, … , xn) =
n
∏
i=1
fX(xi|𝛝) .
As logarithms of products are sums of logarithms, it is often easier to work with the natural
logarithm of the likelihood, known as the log-likelihood function:
ℓ(𝛝) =
n
∑
i=1
log{fX(xi|𝛝)}.
(5.2)
The log-likelihood in (5.2) quantifies model components represented in the data by 𝛝. This
leads naturally to its use as a measure of information. With only a single unknown parameter 𝜃
(so p = 1), we say the Fisher information number is the expected value of the negative second
derivative of ℓ(𝜃):
(𝜃) = E[−ℓ′′(𝜃)].
(5.3)
Although the notation camouflages it, ℓ′′(𝜃) here is a function of the data, X1, … , Xn. There-
fore, one can calculate its negative expectation with respect to the joint p.m.f. or p.d.f. in (5.1).
If there are p > 1 unknown parameters, the information numbers for each 𝜃j are col-
lected together into a Fisher information matrix. Start with the mixed partials of ℓ(𝛝),
hjk = 𝜕2ℓ(𝛝)∕𝜕𝜃j𝜕𝜃k, and from these, build the Hessian matrix of second partial derivatives
for ℓ(𝛝),
Hℓ=
⎡
⎢
⎢
⎢⎣
h11
h12
· · ·
h1p
h21
h22
· · ·
h2p
⋮
⋮
⋱
⋮
hp1
hp2
· · ·
hpp
⎤
⎥
⎥
⎥⎦
.
(5.4)
Here again, the Hessian elements are functions of the data, X1, … , Xn. Therefore, one can
evaluate their negative expected values, say, jk(𝛝) = E[−hjk] = E[−𝜕2ℓ(𝛝)∕𝜕𝜃j𝜕𝜃k] and

STATISTICAL INFERENCE
117
array them together into the Fisher information matrix
F(𝛝) =
⎡
⎢
⎢
⎢
⎢
⎢⎣
11(𝛝)
12(𝛝)
…
1p(𝛝)
21(𝛝)
22(𝛝)
…
2p(𝛝)
⋮
⋮
⋱
⋮
p1(𝛝)
p2(𝛝)
…
pp(𝛝)
⎤
⎥
⎥
⎥
⎥
⎥⎦
.
(5.5)
In effect, F(𝛝) is the negative expected Hessian of the log-likelihood function. Note that, as is
often the case, jk(𝛝) = kj(𝛝) so that the information matrix F(𝛝) is symmetric. Since it is
based on the expected values of log-likelihood derivatives, F(𝛝) is also known as the expected
information matrix.
5.2
Point estimation
To estimate an unknown parameter vector 𝛝, we use quantitative information in the random
sample, {X1, X2, … , Xn} from fX(x|𝛝). A variety of strategies can be applied to achieve this
goal; what follows is a short review of established approaches. As mentioned in Section 3.3.1,
the default notation for a point estimator of 𝛝places a circumflex accent ( ̂ ) above the param-
eter; thus ̂𝛝is a point estimator for 𝛝(unless a more-specific or traditional notation presents
itself, such as X for a population mean 𝜇).
It is important to note that an estimator, ̂𝛝, is based on data and, therefore, in the abstract is
a function of the random variables X1, X2, … , Xn. Thus it is itself a random variable, with its
own joint p.m.f. or p.d.f., say f ̂𝛝(⋅). The distribution associated with any statistic from a ran-
dom sample is called the sampling distribution of the statistic, and every individual point esti-
mate will possess a consequent mean E[ ̂𝜃j] and variance Var[ ̂𝜃j], along with between-estimate
covariances Cov[ ̂𝜃j, ̂𝜃k] (j ≠k) in the multiparameter case. These are collected together into
the estimator’s covariance matrix
Var[ ̂𝛝] =
⎡
⎢
⎢
⎢
⎢
⎢⎣
Var[ ̂𝜃1]
Cov[ ̂𝜃1, ̂𝜃2]
· · ·
Cov[ ̂𝜃1, ̂𝜃p]
Cov[ ̂𝜃2, ̂𝜃1]
Var[ ̂𝜃2]
· · ·
Cov[ ̂𝜃2, ̂𝜃p]
⋮
⋮
⋱
⋮
Cov[ ̂𝜃p, ̂𝜃1]
Cov[ ̂𝜃p, ̂𝜃2]
· · ·
Var[ ̂𝜃p]
⎤
⎥
⎥
⎥
⎥
⎥⎦
,
(5.6)
as in (2.12).
The standard deviation of a univariate point estimator is the square root of its variance,
√
Var[ ̂𝜃j]. This will often be a function of 𝜃j (and perhaps other elements of 𝛝) and in practice
must be estimated as well. To do so, replace any unknown quantities by their individual point
estimators, that is, replace 𝜃j with ̂𝜃j wherever it appears. The result is called the standard
error of 𝜃j:
se[ ̂𝜃j] =
√
Var[ ̂𝜃j]| 𝛝= ̂𝛝,
(5.7)
where the vertical bar notation in the expression indicates evaluation at 𝛝= ̂𝛝. The standard
error can be interpreted as a measure of uncertainty associated with estimating the population
parameter 𝜃j.

118
STATISTICAL DATA ANALYTICS
5.2.1
Bias
In practice, we would expect the theoretical mean of a point estimator to be (near) the value
it is estimating, that is, E[ ̂𝜃j] = 𝜃j. When this occurs, ̂𝜃j is called an unbiased estimator of 𝜃j.
Conversely, if E[ ̂𝜃j] ≠𝜃j, then ̂𝜃j is a biased estimator. Unbiased estimators exhibit a kind of
“scientific objectivity”. This is considered an optimal feature if it is achieved.
An estimator’s bias is the difference between its expected value and the target param-
eter: Bias[ ̂𝜃j] = E[ ̂𝜃j] −𝜃j. Obviously, an unbiased estimator has Bias[ ̂𝜃j] = 0. To quantify
variation about the target parameter, the mean squared error of an estimator is MSE[ ̂𝜃j] =
E[( ̂𝜃j −𝜃j)2], which can be shown to decompose into MSE[ ̂𝜃j] = Var[ ̂𝜃j] + Bias2[ ̂𝜃j].
Example 5.2.1 Sample mean from Normal population.
Take a random sample from a
normal distribution: Xi ∼i.i.d. N(𝜇, 𝜎2), i = 1, … , n. In Section 2.3.9, it was seen that the
sample mean is then also normally distributed, as X ∼N (𝜇, 𝜎2∕n). Since X can be viewed as
a point estimator of 𝜇, this says that the sampling distribution of X is normal, with expected
value equal to 𝜇and with variance equal to 𝜎2∕n. Further, we see E[X] = 𝜇, so X is an unbiased
estimator of 𝜇.
To find the standard error under (5.7) of the point estimator X, recognize that
√
Var[X] =
√
𝜎2∕n. If 𝜎2 is unknown – as is almost always the case in practice – replace it with its point
estimator: the sample variance from (3.5), S2 = ∑n
i=1 (Xi −X)2∕(n −1). The standard error of
X is then se[X] = S∕
√
n.
◽
Since a point estimator is a function of the sample, X1, … , Xn, it also depends on the
sample size n. (Indeed, some authors write ̂𝛝n to remind the reader of this.) As n grows large,
it is natural to expect that the distributional properties of ̂𝛝n will be affected. It is also natural
to ask whether or how the estimator performs as n →∞. The sampling distribution of ̂𝛝n as
n →∞is known as the asymptotic distribution of the estimator, and in many cases, it has a
common or known form.
Example 5.2.2 Asymptotics for the sample mean.
From the central limit theorem in
Section 2.3.9, we saw that for a random sample from any p.m.f. or p.d.f. fX(x|𝜇, 𝜎2) with
finite mean E[Xi] = 𝜇and finite variance Var[Xi] = 𝜎2, the sampling distribution of X
approaches N (𝜇, 𝜎2∕n) as n →∞. This says that the asymptotic distribution of X has a
normal (Gaussian) form.
Further, because the expected value of this asymptotic distribution is 𝜇, one can say that X
is ‘asymptotically unbiased’ for 𝜇. In very large samples, it is reasonable to expect that X will
come close to 𝜇and that it will do so in a manner approximating normal (Gaussian) variation.
(If the original random sample possesses a parent normal distribution, this will be an exact
relationship, as in Example 5.2.1.)
In practice, every case will differ: the central limit theorem can take effect fairly quickly
for some nonnormal parent distributions, where approximate normality can be valid for n as
small as 10 or 20. Other distributions, especially many discrete or highly asymmetric forms,
can require n upwards of 100 or more before variation in X begins to appear normal.
◽
5.2.2
The method of moments
With only limited assumptions made on the distribution of the Xis, a simple, yet useful esti-
mation approach can be derived. Known as the method of moments (MOM), the procedure

STATISTICAL INFERENCE
119
equates the first p ≥1 population moments of Xi to their corresponding sample moments and
then solves for the p unknown parameters.
Given Xi ∼i.i.d. fX(x|𝛝), i = 1, … , n, suppose that the (common) jth population
moments are E[Xj], j = 1, … , p, and that each of these p moments depends on the elements
of 𝛝. Let the corresponding sample moments be Mj = 1
n
∑n
i=1 Xj
i. Then to find an MOM
estimator for 𝛝, set each E[Xj] equal to Mj and solve for the 𝜃js. This will be a p-dimensional
system of equations defined by each individual estimating equation E[Xj] = Mj, j = 1, … , p.
Technically, all that is required to implement the MOM is knowledge of p distinct population
moments for X. Full specification of fX(x|𝜃) is not necessary.
Notice that the MOM construction for the estimating equations is not unique; one could
instead set E[Xj+1] = Mj for every j, or E[X2j] = Mj, and so on. (However, one would need
substantial motivation for doing so.)
Example 5.2.3 Method of Moments for Bernoulli sample (Example 2.3.2, continued).
Suppose a random sample is taken of i.i.d. Bernoulli observations, each with constant,
unknown probability 𝜋: Xi ∼i.i.d. Bin(1, 𝜋), i = 1, … , n. To find an MOM estimator
for 𝜋, recall that the first moment of a Bernoulli random variable is simply E[Xi] = 𝜋.
Thus the MOM estimator is built from the single estimating equation that equates the first
sample moment with 𝜋. But, the first sample moment is the sample mean, so this becomes
1
n
∑n
i=1 Xi = 𝜋. The solution here is trivial: ‘solve’ for 𝜋and take as the MOM estimator
̂𝜋MOM =
∑n
i=1 Xi
n
.
Notice that ̂𝜋MOM is the sample proportion, that is, the number of Bernoulli successes out of
the total n subjects sampled. This is an intuitively natural estimator to use for this setting.
For instance, recall in Example 2.3.2 the purchasing study of a retail outlet’s n = 1024
affinity customers who might make a purchase during a marketed sale. Suppose after the event
concluded that ∑n
i=1 Xi = 506 of the customers actually made a sale purchase. The MOM
estimator is then ̂𝜋MOM = 506∕1024 or 49.41% of the outlet’s affinity base. (These data will
be studied further in the following examples.)
In passing, recall that the Bernoulli distribution is a special case of the binomial and that
sums of i.i.d. binomials (and Bernoullis) are also binomial. Here, Y = ∑n
i=1 Xi ∼Bin(n, 𝜋),
and so one can also write the MOM estimator as ̂𝜋MOM = Y∕n.
◽
5.2.3
Least squares/weighted least squares
Another traditional method for estimating an unknown parameter is known as the method of
least lquares (LS). The method does not require specification of the likelihood and is thus
quite general. Suppose generically that the observations each have expected value E[Xi] = 𝜃.
Then, the LS method estimates 𝜃by minimizing the objective quantity = ∑n
i=1 (Xi −𝜃)2,
that is, the sum of squared deviations of each observation from 𝜃.
Example 5.2.4 Least squares estimator for a population mean. Take a random sample
Xi ∼i.i.d. fX(x|𝜇) i = 1, … , n, where the unknown, finite population mean is E[Xi] = 𝜇
(and make no other assumptions on Xi). To estimate 𝜇, the LS method minimizes
= ∑n
i=1 (Xi −𝜇)2. Different strategies can be applied here to find the minimum.

120
STATISTICAL DATA ANALYTICS
For instance, expanding the square produces
=
n
∑
i=1
(X2
i −2𝜇Xi + 𝜇2) =
( n
∑
i=1
X2
i
)
−2nX𝜇+ n𝜇2 ,
(5.8)
using the fact that ∑n
i=1 Xi = nX.
Now, view as a function of 𝜇and in particular recognize that (5.8) describes a parabola:
(𝜇) = a𝜇2 + b𝜇+ c, with coefficients a = n, b = −2nX, and c = ∑n
i=1 X2
i . The vertex of
the parabola occurs at 𝜇= −b∕(2a) = −(−2nX)∕(2n) = X. Further, the parabola is convex,
because its quadratic coefficient is strictly positive (a = n > 0). Thus the parabola in (5.8)
attains a minimum at its vertex. But, this says that the LS objective function (𝜇) is mini-
mized at the sample mean, so we take ̂𝜇LS = X. The least squares estimator of a population
mean is the sample mean. (Also see Exercise 5.3.)
◽
The LS method applies more generally when a function of p > 1 parameters, g(𝜃1, 𝜃2,
… , 𝜃p), is chosen to model E[Xi]. Then, to estimate the unknown parameters, the LS approach
minimizes the squared difference between the Xis and g(⋅):
=
n
∑
i=1
{Xi −g(𝜃1, 𝜃2, … , 𝜃p)}2
This results in a p-dimensional system of equations which when solved produces a vector of
estimators ̂𝛝= [ ̂𝜃1 ̂𝜃2 · · · ̂𝜃p]T.
If it is felt that the observations contribute information about 𝛝in a heterogeneous manner,
one can apply weighted least squares (WLS): minimize the weighted objective quantity
w =
n
∑
i=1
𝑤i{Xi −g(𝜃1, 𝜃2, … , 𝜃p)}2 ,
where the weights, 𝑤i ≥0, are chosen to account for the differential quality of each Xi. For
example, suppose Var[Xi] varies with i so that some observations are more variable and, there-
fore, less precise regarding the information they provide on 𝛝. Then, we typically take the
weights proportional to the reciprocals of these variances: 𝑤i ∝1∕Var[Xi]. With this, a more
variable (= less precisely measured) observation has lesser impact on ̂𝛝.
5.2.4
Maximum likelihood ∗
When the full parametric structure of a random sample can be specified via a likelihood func-
tion L(𝛝; x1, x2, … , xn), a powerful estimation method may be brought to bear for estimating
𝛝. Given the likelihood, ones finds the log-likelihood function ℓ(𝛝) and then maximizes it
with respect to the p unknown parameters. This is the method of maximum likelihood (ML).
In most cases, determining the maximum likelihood estimators (MLEs) requires appeal to
differential calculus; that is, set the first partial derivatives of ℓ(𝛝) equal to zero and solve the
resulting system of p estimating equations to find ̂𝛝ML.
Example 5.2.5 MLEs from normal population. Take a random sample of normal (Gaus-
sian) observations, Xi ∼i.i.d. N(𝜇, 𝜎2), i = 1, … , n. To find the MLE for 𝛝= [𝜇, 𝜎2]T, start

STATISTICAL INFERENCE
121
with the likelihood function. This is built from the normal p.d.f. in (2.34):
L(𝜇, 𝜎2; x1, x2, … , xn) =
n
∏
i=1
1
𝜎
√
2𝜋
exp
{
−(xi −𝜇)2
2𝜎2
}
=
1
(2𝜋)n∕2𝜎n exp
{
−1
2𝜎2
n
∑
i=1
(xi −𝜇)2
}
.
It will be convenient to work with the reparameterization 𝜏= 𝜎2 > 0 in order to prevent
confusion when taking derivatives. The log-likelihood is
ℓ(𝜇, 𝜏) = C + log(𝜏−n∕2) −1
2𝜏
n
∑
i=1
(xi −𝜇)2 = C −n
2 log(𝜏) −1
2𝜏
n
∑
i=1
(xi −𝜇)2,
where C is a constant that does not affect the maximization. From this, the ML estimating
equations are found from the partial derivatives of ℓ(𝜇, 𝜏) with respect to each parameter. Set
these equal to zero and then solve for 𝜇and 𝜏:
𝜕ℓ
𝜕𝜇= −2
2𝜏
n
∑
i=1
(xi −𝜇)𝜕(−𝜇)
𝜕𝜇
= 1
𝜏
n
∑
i=1
(xi −𝜇) =
∑n
i=1 xi
𝜏
−n𝜇
𝜏= 0
(5.9)
and
𝜕ℓ
𝜕𝜏= −n
2𝜏+
1
2𝜏2
n
∑
i=1
(xi −𝜇)2 = 0.
(5.10)
Solving (5.9) for 𝜇(assuming 𝜏≠0) leads immediately to
̂𝜇ML =
∑n
i=1 xi
n
= x.
Here again, we find the estimate of the population mean is the sample mean.
Continuing on to the variance, because we know ̂𝜇ML = x in the simultaneous system of
equations, we can substitute this for 𝜇in (5.10) to find
𝜕ℓ
𝜕𝜏= −n
2𝜏+
1
2𝜏2
n
∑
i=1
(xi −x)2 = 0,
that is, n𝜏= ∑n
i=1 (xi −x)2. Solving for 𝜏(= 𝜎2) gives
̂𝜎2
ML = 1
n
n
∑
i=1
(xi −x)2.
Curiously, this is not the sample variance, S2, from (3.5): ̂𝜎2
ML = n−1
n S2. (See Example 5.2.6.)
◽
A technical aside: if applying calculus-based methods to find MLEs, one must verify that
the resulting stationary points of ℓ(𝛝) are maxima, that is, the proposed value for ̂𝛝does indeed
maximize ℓ(𝛝). Generally, the solution is again to apply differential calculus. Construct the
Hessian matrix Hℓ= {hjk} of second partial derivatives, hjk = 𝜕2ℓ(𝛝)∕𝜕𝜃j𝜕𝜃k, from (5.4), and

122
STATISTICAL DATA ANALYTICS
verify that Hℓis negative definite. In the special case of p = 2, this simplifies to verifying
(i) h11 < 0 and (ii) h11h22 −h2
12 > 0 (Khuri 2003, Section 7.7).
One must also verify that the log-likelihood does not reach a true maximum at the bound-
aries of the parameter space. This is usually a simple check: identify the (joint) values of 𝛝
at its boundary and evaluate ℓ(𝛝) at those points. Then, verify that the log-likelihood (or the
full likelihood, if easier to evaluate) achieves a larger value at the proposed MLEs. (Most
likelihood functions are stable enough that their stationary points are indeed global maxima,
although one should always conduct the verification just in case.) Exercise 5.4 verifies that the
MLEs ̂𝜇ML and ̂𝜎2
ML for the normal case in Example 5.2.5 do indeed maximize the likelihood
function.
Example 5.2.6 Variance estimation.
Suppose a random sample produces observations
Xi ∼i.i.d. fX(x|𝜇, 𝜎2) i = 1, … , n. Assume the underlying p.m.f. or p.d.f. has a finite
population mean E[Xi] = 𝜇and a finite population variance 𝜎2. If the sample is obtained
from a normal population, N(𝜇, 𝜎2), then as seen in Example 5.2.5, the MLE for 𝜇is the
sample mean X. Further, from Example 5.2.4, it is also the LS estimator for 𝜇, and from
Example 5.2.1, it is an unbiased estimator for 𝜇. (Hooray for the sample mean!)
Example 5.2.5 also showed that, however, the MLE for the variance 𝜎2 from a normal
random sample is not the sample variance, S2, from (3.5): ̂𝜎2
ML = n−1
n S2. To confuse the issue
further, it can be shown (Exercise 5.6) that for any random sample where the population vari-
ance is finite, E[S2] = 𝜎2. Thus S2 is an unbiased estimator for 𝜎2, but ̂𝜎2
ML is not:
E [̂𝜎2
ML
] = E
[n −1
n
S2]
= n −1
n
E[S2] = n −1
n
𝜎2 ≠𝜎2.
The conundrum here is, which to choose for estimating 𝜎2? Standard practice generally
favors the unbiased estimator S2, and so the sample variance is employed in the various statis-
tical operations in the following. Indeed, in R, application of the var(x) function to a sample
of data in x will produce the unbiased estimator S2. From this, the usual point estimator for 𝜎
is taken as the sample standard deviation, S =
√
S2, available in R via the sd() function.
This has important implications. For example, the standard error of X was found in
Example 5.2.1 as, technically, the square root of an estimator of 𝜎2, divided by
√
n. As
established practice employs S2 as the estimator of 𝜎2, this results in se[X] = S∕
√
n.
To illustrate, recall the data in Table 4.2 on ages at myocardial infarction (‘heart attack’)
among n = 126 cardiac patients. In Example 4.1.7, a normal quantile plot suggested a rea-
sonable fit of the normal model to these data. Thus to estimate the mean age of attack for this
population of subjects, from Example 5.2.5, the MLE/unbiased estimator is simply the sample
mean. In R, this is calculated via mean(x), and for these data, this produces X = 62.8137.
The sample variance is then S2 =
1
125
∑216
i=1 (Xi −62.8137)2. Here, direct calculation is sim-
plified by using the computing formulas from (3.6), although it is even faster to employ the
computer: in R, var(x) gives S2 = 69.5908, while sd(x) produces S = 8.3421. With these,
the standard error of X can be found as se[X] = S∕
√
126 = 8.3421∕11.2250 = 0.7432.
It is worth acknowledging that with very large sample sizes the difference between the
unbiased estimator S2 and the MLE ̂𝜎2
ML will essentially vanish, because as n →∞, the two
estimators converge to the same value.
If, further, the random sample is from a normal distribution, Xi ∼i.i.d. N(𝜇, 𝜎2),
i = 1, … , n, the full sampling distribution of S2 can be determined. Under normal sampling,

STATISTICAL INFERENCE
123
it can be shown (Casella and Berger 2002, Section 5.3) that
(n −1)S2
𝜎2
∼𝜒2(n −1).
This feature will prove useful in making inferences on both 𝜎2 and 𝜇.
◽
For the sorts of complex, multiparameter models often employed in data analytics, con-
struction and maximization of a multidimensional log-likelihood can require extensive cal-
culation and is usually performed by computer. Nonetheless, ample benefits accrue from the
effort: MLEs possess a number of desirable properties. For example, suppose interest focuses
on some known function, h(𝛝), of the parameter vector 𝛝. To find the MLE for h(𝛝), one
can simply evaluate the function at the MLE for 𝛝, that is, ̂
h(𝛝) = h( ̂𝛝). This is known as the
functional invariance property of MLEs (Casella and Berger 2002, Section 7.2).
Another important feature of MLEs is that the form of their asymptotic distribution is
often known, allowing for operable large-sample approximations. Under certain regularity
conditions (Lehmann and Casella 1998, Section 6.3), the ML vector ̂𝛝will possess a p-variate
normal distribution where, individually,
̂𝜃j ̇∼N(𝜃j, Var[ ̂𝜃j]),
(5.11)
j = 1, … , p. (Recall that the symbol ̇∼is read as ‘is approximately distributed as.’ The
approximation improves as n →∞.) The covariance matrix of ̂𝛝is found by inverting the
Fisher information matrix, F(𝛝), from (5.5). For shorthand notation, write E[ ̂𝛝] = 𝛝and
Var[ ̂𝛝] = F−1(𝛝). The large-sample variances are found as the diagonal elements of F−1(𝛝).
In the single-parameter case with p = 1, this is ̂𝜃̇∼N(𝜃, Var[ ̂𝜃]), where Var[ ̂𝜃] is simply
the reciprocal of the Fisher information number
Var[ ̂𝜃] ≈
1
(𝜃).
(5.12)
5.3
Interval estimation
Parameter estimation is a descriptive step for using data to learn about the larger population.
The next step, statistical inference, has two general forms: interval estimation and hypothesis
testing. These are briefly reviewed in this section and the next section, respectively. In the
former case, the concept is an extension of the point estimators introduced earlier, where an
entire interval of plausible values is provided as an estimate for the unknown parameter. When
constructed properly, an interval estimator informs the analyst on uncertainty in estimating 𝜃:
wider intervals suggest greater uncertainty (and less precision), narrower intervals suggest
less uncertainty (more precision).
5.3.1
Confidence intervals
A confidence interval is a form of estimator for the unknown parameter 𝜃that uses the data
to construct an interval within which 𝜃may lie. Formally, a confidence interval is a pair of
values L𝜃(X1, … , Xn) and U𝜃(X1, … , Xn) that satisfy the probability statement
P[L𝜃(X1, … , Xn) < 𝜃< U𝜃(X1, … , Xn)] = 1 −𝛼,
(5.13)
where 1−𝛼is called the confidence coefficient or the confidence level of the interval. Typical
values are 90%, 95%, or 99%, with 95% seen most often in practice. If both L𝜃and U𝜃are

124
STATISTICAL DATA ANALYTICS
finite, the confidence interval is two sided. If either limit is infinite, its finite counterpart is a
one-sided confidence bound on 𝜃.
It is important to recognize that confidence is not probability. The probability that a cal-
culated interval actually contains the true value of 𝜃is not 1−𝛼. It is either 0 or 1. That is,
suppose that we calculate a 95% interval for 𝜃and find 23.08 < 𝜃< 56.26. Clearly, the prob-
ability that 𝜃is included in this interval is 0 (if it is not) or 1 (if it is). As no random variability
is ascribed to 𝜃, no other form of probability can be assigned to the statement 23.08 < 𝜃<
56.26. Thus, instead of a probabilistic interpretation, we say that the interval ‘covers’ 𝜃with
confidence 1−𝛼. This is a frequentist interpretation for coverage: if over repeated sampling
one counts the number of times an interval covers the true 𝜃, one finds that 100(1−𝛼)% of the
intervals cover correctly. Thus confidence is a measure of the interval estimator’s frequency
of correct coverage for the unknown parameter.
5.3.2
Single-sample intervals for normal (Gaussian) parameters
The theory and application of confidence intervals is well established for the normal
(Gaussian) distribution setting. Take a random sample of normal observations, Xi ∼i.i.d.
N(𝜇, 𝜎2), i = 1, … , n. Assume both 𝜇and 𝜎2 are unknown. For a confidence interval on 𝜇,
the goal is to produce a statement of the form in (5.13). To do so, recall that the ML (and LS)
estimator for the population mean here is X, with standard error se[X] = S∕
√
n, and where S
is the sample standard deviation. Statements about 𝜇can be built from the distribution of X,
which from Section 2.3.9 is given by X ∼N (𝜇, 𝜎2∕n). Standardizing to N(0,1) produces
Z = X −𝜇
𝜎∕
√
n
∼N(0, 1).
(5.14)
Now, we know that the standard normal upper- 𝛼
2 critical point, z𝛼∕2, satisfies P[Z > z𝛼∕2]
= 𝛼∕2; cf. Figure 2.5. Manipulating this ‘tail area’ relationship into an interval relationship
leads to P[−z𝛼∕2 < Z < z𝛼∕2] = 1 −𝛼, which for the X standardization in (5.14) is equiva-
lent to
1 −𝛼= P
[
−z𝛼∕2 < X −𝜇
𝜎∕
√
n
< z𝛼∕2
]
= P
[
−z𝛼∕2
𝜎
√
n
< X −𝜇< z𝛼∕2
𝜎
√
n
]
= P
[
X −z𝛼∕2
𝜎
√
n
< 𝜇< X + z𝛼∕2
𝜎
√
n
]
(5.15)
(relying on the fact that 𝜎∕
√
n > 0). This satisfies (5.13) and, hence, is a valid 100(1−𝛼)% con-
fidence interval for 𝜇, if 𝜎were known. (Clearly, 𝜎is a critical component of both the lower and
upper endpoints.) In practice, however, 𝜎is hardly ever known. Instead, as in Example 5.2.6,
the sample standard deviation S is used to estimate it. In effect, the standardization in (5.14)
becomes
T = X −𝜇
S∕
√
n
.
(5.16)
Notice that now, the centered difference X −𝜇has been divided not by the standard deviation
of X,
√
Var[X] = 𝜎∕
√
n, but by the standard error se[X] = S∕
√
n.

STATISTICAL INFERENCE
125
It can be shown (Exercise 5.7) that the quantity T in (5.16) satisfies the relationship in
(2.41) that defines Student’s t-distribution. Here T ∼t(n −1). The degrees of freedom (d.f.),
𝜈= n −1, are derived from the 𝜒2 relationship for the variance estimator S2 introduced in
Example 5.2.6: for a normal random sample, (n −1)S2∕𝜎2 ∼𝜒2(n −1). In this case, we often
say that X in (5.16) has been not ‘standardized,’ but ‘Studentized’ (Hartley 1938).
As a result, t-distribution critical points, t𝛼∕2(n −1), can be substituted for the standard
normal critical points in reconstructing (5.15). Start with P[−t𝛼∕2(n −1) < T < t𝛼∕2(n −1)]
= 1 −𝛼as in Figure 5.1 and find
1 −𝛼= P
[
−t𝛼∕2(n −1) < X −𝜇
S∕
√
n
< t𝛼∕2(n −1)
]
= · · · = P
[
X −t𝛼∕2(n −1) S
√
n
< 𝜇< X + t𝛼∕2(n −1) S
√
n
]
.
(5.17)
This then defines a t-distribution 100(1−𝛼)% confidence interval for the mean from a normal
random sample.
Notice the symmetric form of the interval in (5.17): a point estimator, X, centered between
the same separating quantity, t𝛼∕2(n −1)S∕
√
n. Shorthand for this construction is
X ± t𝛼∕2(n −1) S
√
n
.
t
t-distribution p.d.f.
−4
−2
0
2
4
tα/2(ν)
Tail areas = α
2
+
−tα/2(ν)
+
Central area = 1− α
Figure 5.1
Bounding a t-distribution’s p.d.f. area at critical points ± t𝛼∕2(𝜈).

126
STATISTICAL DATA ANALYTICS
The separating quantity is called the margin of error (MOE) of the confidence interval. (More
technically, the MOE is the half-width of a confidence interval for an unknown parameter.)
If only one-sided limits are desired on 𝜇, the construction simplifies slightly. For an upper
limit, start with the strict lower-tail relationship P[−t𝛼(n −1) < T] = 1 −𝛼and manipulate it
into
1 −𝛼= P
[
−t𝛼(n −1) < X −𝜇
S∕
√
n
]
= P
[
−t𝛼(n −1) S
√
n
< X −𝜇
]
= P
[
𝜇< X + t𝛼(n −1) S
√
n
]
.
(5.18)
Thus the 100(1−𝛼)% one-sided, upper, confidence limit for 𝜇is X + t𝛼(n −1) S
√
n. For the
one-sided lower limit, reverse the construction to find
1 −𝛼= P
[
X −t𝛼(n −1) S
√
n
< 𝜇
]
.
(5.19)
Example 5.3.1 Confidence interval on 𝝁from a normal sample (Example 4.1.7, contin-
ued). Return to the myocardial infarction data in Table 4.2. In Example 4.1.7, a normal quan-
tile plot suggested a possible fit of the normal model to these data. Thus to estimate the mean
age of attack for this population the sample mean is X = 62.8137, calculated in R via mean(x).
In Example 5.2.6, the standard error of X was found as se[X] = S∕
√
126 = 8.3421∕11.2250 =
0.7432, using sd(x) to find S = 8.3421.
These are precisely the components required to construct the t confidence interval for 𝜇
from (5.17). Set the confidence level to 95%. The necessary t critical point is then t𝛼∕2(n −1)
= t0.025(125) = 1.9791, found in R via qt(0.025, 125, lower.tail=FALSE). This gives
an MOE of
t0.025(125) S
√
n
= (1.9791)(0.7432) = 1.4708,
with consequent 95% confidence interval for 𝜇given by 62.8137 ± 1.4708, that is,
61.3429 < 𝜇< 64.2845.
With 95% confidence, we learn that the average age at which this population suffers its (first)
myocardial infarction is between about 61.3 and 64.3 years. (As noted above, however, the
value of P[61.3429 < 𝜇< 64.2846] is either 0 or 1, and it is not 0.95.)
These various operations can be coded directly in R and assembled into a fast routine or
function to find the confidence limits. One example is the standard R routine t.test():
> t.test( age, conf.limit=0.95 )
with output (edited)
One Sample t-test
data:
age
t = 84.5209, df = 125, p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 0

STATISTICAL INFERENCE
127
95 percent confidence interval:
61.34289 64.28456
sample estimates:
mean of x
62.81372
The confidence limits follow after ‘95 percent confidence interval’ and they corrob-
orate the direct calculations above. (Other components of this output are discussed in Example
5.4.1.)
◽
Although the normal population variance 𝜎2 is often viewed as a nuisance parameter,
there are occasions when confidence limits may be desired for it as well. These rely directly
on the 𝜒2 relationship for S2 discussed in Example 5.2.6. Let C2 = (n −1)S2∕𝜎2 such that
C2 ∼𝜒2(n −1). Then using 𝜒2 critical points (see Figure 2.6), the statement
1 −𝛼= P[𝜒2
1−𝛼∕2(n −1) < C2 < 𝜒2
𝛼∕2(n −1)]
(5.20)
can be inverted into
1 −𝛼= P
[
(n −1)S2
𝜒2
𝛼∕2(n −1)
< 𝜎2 <
(n −1)S2
𝜒2
1−𝛼∕2(n −1)
]
.
(5.21)
This defines a 100(1−𝛼)% confidence interval for the variance from a normal random sample.
(For a confidence interval on the standard deviation, 𝜎, first apply the square root across all
sides of the inequalities in (5.20).)
Notice that the interval for 𝜎2 in Equation (5.21) is not symmetric, although it was con-
structed from a symmetric assignment of tail areas. That is, equal 𝛼∕2 area was allocated to
each tail in (5.20). While this is a natural choice, it is not unique: one could assign two times as
much tail area to one side – or three times as much, or four times, and so on – and still produce
a valid confidence statement similar to (5.21). In practice, the choice of tail area assignment
is determined on a case-by-case basis, depending on which of the two limits requires more
attention for the problem under study.
If no a priori information is available to guide the assignment, an equal-area split is sim-
plest but not necessarily optimal. Theoretical constructions for building optimal confidence
limits on a normal variance were given by Tate and Klett (1959). These require additional spec-
ifications to uniquely determine the optimal limits; see the article by Tate and Klett (1959), or
Casella and Berger (2002, Exercise 9.52), for more details.
For the more-common calculation of confidence limits for 𝜇as in (5.17), the issue of
allocating tail areas is not a concern. The equal-tail area allocation used there is known to be
optimal (Casella and Berger 2002, Section 9.3).
The operations used to construct the confidence intervals for both 𝜇and 𝜎2 in (5.17) and
(5.21), respectively, took advantage of a useful feature. The statistics used to form the inter-
vals had familiar reference distributions. For 𝜇, the T ratio in (5.16) was distributed as t(n −1)
while for 𝜎2, C2 in (5.20) was distributed as 𝜒2(n −1). In both cases, the t and 𝜒2 reference
distributions did not depend on any unknown parameters. Statistics that exhibit this useful
feature are given a special name: they are called pivotal quantities (Casella and Berger 2002,
Section 9.2.2). Although not always available, the ‘pivoting’ of such quantities is a popu-
lar strategy to produce confidence limits for many different distributions. See the examples
given in Casella and Berger (2002, Section 9.2), Piegorsch and Bailer (1997, Section 2.6), and
similar sources.

128
STATISTICAL DATA ANALYTICS
5.3.3
Two-sample intervals for normal (Gaussian) parameters
When the random sampling occurs across two independent samples, a confidence interval
can be constructed to compare the means of each sample. In the normal case, suppose data
from the first sample are X1j ∼i.i.d. N(𝜇1, 𝜎2
1), j = 1, … , n1, independent of data from the
second sample X2j ∼i.i.d. N(𝜇2, 𝜎2
2), j = 1, … , n2. Assume both samples’ sets of parameters
𝜇1, 𝜎2
1 and 𝜇2, 𝜎2
2 are unknown. Of interest is interval estimation of the difference in means
Δ = 𝜇1 −𝜇2. An unbiased estimator for Δ is the difference in sample means, X1 −X2, with
corresponding standard error
se[X1 −X2] =
√
S2
1
n1
+
S2
2
n2
,
where the S2
i s are the respective sample variances from each independent random sample
(i = 1, 2). These various statistics are then employed in constructing the pivotal quantity
T = X1 −X2 −Δ
√
S2
1
n1 +
S2
2
n2
.
(5.22)
The pivot T in (5.22) is approximately t-distributed: T ̇∼t(𝜈WS), with approximate d.f.
𝜈WS =
⎢
⎢
⎢
⎢⎣
(U1 + U2
)2
1
n1 −1U2
1 +
1
n2 −1U2
2
⎥
⎥
⎥
⎥⎦
,
(5.23)
and with Ui = S2
i ∕ni (i = 1, 2). (The ‘floor’ notation in 𝜈WS reminds the analyst to round down
to the nearest whole number.) This approximation for the d.f. was proposed by Smith (1936)
and later by Welch (1938) and is a special case of a general method for moment-based d.f. esti-
mation described by Satterthwaite (1946). It is known as the Welch–Satterthwaite correction
for the d.f. of T.
Manipulating the pivot in (5.22) yields the approximate confidence limits (Exercise 5.15)
X1 −X2 ± t𝛼∕2(𝜈WS)
√
S2
1
n1
+
S2
2
n2
.
(5.24)
The Welch–Satterthwaite approximation for the d.f. improves as min{n1, n2} →∞. It is quite
accurate, however, and the approximate confidence level for (5.24) will be very near 1−𝛼for
sample sizes as low as ni = 10.
Example 5.3.2 Two-sample confidence interval for Lung Function data. Taussig et al.
(2003) discussed data on lung function response among 353 asthmatics living in a semi-arid
environment, including possible differences between patients who smoked and those who
did not. Recorded was the patient’s percentage of expected forced expiratory volume in 1 s
(known as ‘FEV1’) at the age of 26 years. Higher values indicate more robust lung function. A
selection of the data appear in Table 5.1. (The full data set is available at http://www.wiley.com
/go/piegorsch/data_analytics.)

STATISTICAL INFERENCE
129
Table 5.1
Percentage of expected forced expiratory volume in 1 s (FEV1)
among asthma patients at the age of 26 years.
Smoking status
FEV1
No
90, 84, 103, 94, 88, 101, … , 78, 76, 81, 101, 93, 111
Yes
104, 82, 79, 93, 86, 69, … , 105, 99, 112, 135, 96, 85
Table 5.2
Summary statistics for FEV1 measurements from Table 5.1.
Smoking status
Sample size, ni
Mean, Xi
Variance, S2
i
No (i = 1)
265
97.5962
121.9386
Yes (i = 2)
88
97.7500
150.7644
To compare the two groups, take X1 as the FEV1 scores for nonsmokers and X2 as those for
smokers. Preliminary examination of the data (Exercise 5.13) indicates that a normal sampling
assumption for both variables is reasonable, so assume X1j ∼i.i.d. N(𝜇1, 𝜎2
1), j = 1, … , n1,
independent of X2j ∼i.i.d. N(𝜇2, 𝜎2
2), j = 1, … , n2.
To determine a plausible range of values for the mean difference Δ = 𝜇1 −𝜇2 under these
assumptions, appeal to the confidence interval in (5.24). Summary statistics for the necessary
calculations appear in Table 5.2. Appeal is made here to the Welch–Satterthwaite correction
for the t reference distribution. From (5.23), this produces 𝜈WS = ⌊136.76⌋= 136 d.f.
Set the confidence level to 90%. The corresponding critical point is t0.10∕2(136) = 1.6561.
From (5.24), the limits are then produces the limits −0.1538 ± (1.6561)
√
2.1734 =
−0.1538 ± 2.4415. That is, with 90% confidence, we state that the difference in mean
FEV1 scores rests in the interval −2.5953 ≤Δ ≤2.2877. As this interval contains Δ = 0,
it is plausible that there is no real difference between the two mean scores. (Also see
Example 5.4.2.)
In R, this analysis can be achieved via the t.test() function (with output edited):
> t.test(X1, X2, conf.level=0.90, var.equal=FALSE)
Welch Two Sample t-test
data:
X1 and X2
t = -0.1043, df = 136.758, p-value = 0.9171
alternative hypothesis: true difference in means is not
equal to 0
90 percent confidence interval:
-2.595218
2.287670
sample estimates:
mean of x
mean of y
97.59623
97.75000
The var.equal=FALSE option institutes the Welsh–Satterthwaite correction for the d.f., indi-
cated by the ‘Welch Two Sample t-test’ output header. The conf.level=0.90 option
calls for the 90% limits, which appear in the R output under ‘90 percent confidence
interval.’ The results corroborate the direct calculations given above. (Slight disparities
occur due to rounding.)
◽

130
STATISTICAL DATA ANALYTICS
In the special case where 𝜎2
1 = 𝜎2
2 = 𝜎2 (say), the independent-samples confidence interval
in (5.24) collapses to an exact construction. As the variance is equal in both groups, we pool
the two sample variances into an (unbiased) estimator for 𝜎2,
S2
p =
(n1 −1)S2
1 + (n2 −1)S2
2
n1 + n2 −2
.
(5.25)
With this, a pivotal quantity is
T = X1 −X2 −Δ
Sp
√
1
n1
+ 1
n2
.
(5.26)
Under the assumption of homogeneous variances for this independent/normal two-sample
setting, the pivot in (5.26) is exactly t-distributed for any values of ni, such that T ∼t(n1 + n2
−2). From this, straightforward manipulation of the pivot produces the exact 100(1−𝛼)%
confidence limits
X1 −X2 ± t𝛼∕2(n1 + n2 −2) Sp
√
1
n1
+ 1
n2
.
(5.27)
In R, if X1 and X2 are vectors containing the data from each independent sample, then the
command t.test(X1, X2, conf.level=1-alpha, var.equal=TRUE) can supply the
confidence limits in (5.27), under “95 percent confidence interval.”
In another special, two-sample case, the t-distribution again produces exact confi-
dence limits. Suppose the observations are recorded in matched pairs, (X1j, X2j), with
Xij ∼i.i.d. N(𝜇i, 𝜎2
i ), i = 1, 2; j = 1, … , n. To build a confidence interval for 𝜇D = 𝜇1 −𝜇2,
we construct the difference variable Dj = X1j −X2j, where now Dj ∼i.i.d. N(𝜇D, 𝜎2
D) and
𝜎2
D is a function of the original variances and of Cov[X1j, X2j]. (The exact expression is
not required for the analysis – note that because the data appear in pairs, the assumption
of independence between the two samples is untenable.) In effect, the differences now act
as a single sample, allowing for appeal to arguments that produce single-sample limits as
in (5.17). Find the mean difference D and the variance S2
D = ∑n
j=1 (Di −D)2∕(n −1), and
construct a pivot similar to (5.16):
TD = D −𝜇D
SD∕
√
n
.
Under the normal sampling assumptions, TD ∼t(n −1) (exactly). Thus a construction similar
to (5.17) can be used to find 100(1−𝛼)% limits on 𝜇D. These are simply
D ± t𝛼∕2(n −1) SD
√
n
.
In R, if X1 and X2 are the vectors containing the matched pairs from each sample, then the
command
> t.test( X1, X2, conf.level=1-alpha, paired=TRUE )
can supply these paired-t limits, under ‘95 percent confidence interval.’

STATISTICAL INFERENCE
131
In the general two-sample case, one can also construct confidence limits for comparing the
two population variances 𝜎2
1 and 𝜎2
2. Suppose again that X1j ∼i.i.d. N(𝜇1, 𝜎2
1), j = 1, … , n1,
independent of X2j ∼i.i.d. N(𝜇2, 𝜎2
2), j = 1, … , n2. Assume all the population parameters are
unknown. It is possible to construct an (exact) confidence interval on the ratio of variances
𝜎2
1∕𝜎2
2, via appeal to the F-distribution from Section 2.3.10. Start with the independent sample
variances S2
1 and S2
2, and recall that C2
i = (ni −1)S2
i ∕𝜎2
i ∼indep. 𝜒2(ni −1) for i = 1, 2. Since
the F-distribution was defined as a scaled ratio of independent 𝜒2 variates, and because the
S2
i s and, hence, the C2
i s are independent, let
F21 =
C2
2∕(n2 −1)
C2
1∕(n1 −1)
=
S2
2
S2
1
×
𝜎2
1
𝜎2
2
.
Under the two-sample normal assumptions, F21 ∼F(n2 −1, n1 −1). Then, in similar form to
(5.21), the probability statement
1 −𝛼= P[F1−𝛼∕2(n2 −1, n1 −1) < F21 < F𝛼∕2(n2 −1, n1 −1)]
can be inverted into
1 −𝛼= P
[
F1−𝛼∕2(n2 −1, n1 −1)
S2
1
S2
2
<
𝜎2
1
𝜎2
2
< F𝛼∕2(n2 −1, n1 −1)
S2
1
S2
2
]
.
(5.28)
Clearly then, (5.28) defines a 100(1−𝛼)% confidence interval for 𝜎2
1∕𝜎2
2.
As with the 𝜒2-based interval for a single variance, the F-based interval for a variance ratio
in (5.28) is not symmetric, although it is constructed from a symmetric assignment of tail areas.
Thus it also suffers from the question of tail area assignment: while convenient, the equal-tail
areas in (5.28) do not produce an optimal confidence interval. Theoretical constructions for
building minimum-length confidence limits on a normal variance ratio were given by Levy
and Narula (1974). These require additional specifications to uniquely determine the optimal
limits; see the article by Levy and Narula (1974), or also Wilson and Tonascia (1971), for
more details.
5.3.4
Wald intervals and likelihood intervals∗
The ‘pivotal’ characteristic of the t-interval in (5.17) overlaps with an omnibus technique for
constructing confidence limits, using MLEs from Section 5.2.4. The limits are known col-
lectively as Wald intervals, based on Abraham Wald’s (1943) pioneering work in this area.
Their key feature relies on the fact that as the sample size grows large, the MLE will often be
approximately normal. From this, a pivotal quantity can be constructed from which approxi-
mate confidence limits on the target parameter may be determined.
Take a random sample Xi ∼i.i.d. fX(x|𝜃), i = 1, … , n. Under suitable regularity condi-
tions, the large-sample distribution of the MLE ̂𝜃is given in (5.11) as ̂𝜃̇∼N(𝜃, Var[ ̂𝜃]), where
the variance Var[ ̂𝜃] is the reciprocal of the Fisher information number, as in (5.12). The cor-
responding standard error, se[ ̂𝜃], is the (estimated) square root of the variance in (5.7). With
these, a pivotal quantity can be constructed similar to (5.16): standardize ̂𝜃by dividing se[ ̂𝜃]
into the centered quantity ̂𝜃−𝜃. This produces the pivot
Z = ̂𝜃−𝜃
se[ ̂𝜃]
.
(5.29)

132
STATISTICAL DATA ANALYTICS
If ̂𝜃̇∼N(𝜃, Var[ ̂𝜃]), then the ratio in (5.29) will be approximately standard normal:
Z = ( ̂𝜃−𝜃)∕se[ ̂𝜃] ̇∼N(0, 1). This large-sample reference distribution does not depend on 𝜃,
thus Z is an approximate pivotal quantity.
To assemble a confidence interval for 𝜃, apply the pivot in an analogous manner to (5.15):
start with P[−z𝛼∕2 < Z < z𝛼∕2] = 1 −𝛼, which for the pivot in (5.29) gives the approximation
1 −𝛼≈P
[
−z𝛼∕2 < ̂𝜃−𝜃
se( ̂𝜃)
< z𝛼∕2
]
= P [−z𝛼∕2se( ̂𝜃) < ̂𝜃−𝜃< z𝛼∕2se( ̂𝜃)]
= · · · = P [ ̂𝜃−z𝛼∕2se( ̂𝜃) < 𝜃< ̂𝜃+ z𝛼∕2se( ̂𝜃)] .
(5.30)
The Wald interval is the resulting, approximate, 100(1−𝛼)% construct
̂𝜃± z𝛼∕2se[ ̂𝜃].
(5.31)
Similar to the application of the central limit theorem in Example 5.2.2, the Wald approx-
imation in (5.30) improves as n →∞, and it applies for a broad range of parent distributions;
however, its quality in practice will vary from case to case. The large-sample features of the
MLE can take effect fairly quickly for some continuous parent distributions, where approxi-
mate normality can be valid for n as small as 10 or 20. Other distributions, especially many
discrete forms, can require n upwards of 100 or more before variation in ̂𝜃begins to appear nor-
mal.
Example 5.3.3 MLE on 𝝅from a binomial sample. An important application of the Wald
approach occurs when sampling from a binomial distribution, with some caveats. Suppose a
random sample is taken with n Bernoulli trials, Xi ∼i.i.d. Bin(1, 𝜋). To estimate 𝜋, consider
the MLE. Recall that the sum, Y = ∑n
i=1 Xi, of these n binary observations is binomially dis-
tributed, Y ∼Bin(n, 𝜋). The associated likelihood function is simply the binomial p.m.f. from
(2.17). This leads to a log-likelihood function of the form
ℓ(𝜋) = C + y log(𝜋) + (n −y) log(1 −𝜋) ,
(5.32)
where C is a constant that does not affect the optimization.
Maximization of (5.32) proceeds by applying differential calculus. The log-likelihood
derivative is
ℓ′(𝜋) = y
𝜋−n −y
1 −𝜋
which when set equal to zero produces the estimating equation (1 −𝜋)y −𝜋(n −y) = 0.
Solving this for 𝜋yields ̂𝜋ML = Y∕n. This is the sample proportion, seen also with the
MOM estimator in Example 5.2.3. To verify that this is a true MLE, find ℓ′′(𝜋) = −𝜋−2y
−(1 −𝜋)−2(n −y). This is negative for any y ∈{0, … , n} and any 𝜋∈(0, 1); hence, the
log-likelihood is strictly concave. Further, the log-likelihood is a minimum at the parameter
boundaries: lim𝜋→0ℓ(𝜋) = lim𝜋→1ℓ(𝜋) = −∞. Thus the stationary point at ̂𝜋ML must be a
maximum.
◽
Example 5.3.4 Confidence intervals on 𝝅from a binomial sample (Example 5.3.3, con-
tinued). The binomial MLE ̂𝜋ML = Y∕n possesses an approximate normal distribution for
large n, with mean equal to 𝜋and with large-sample standard error built from the Fisher
information number. The latter quantity here is (𝜋) = n∕{𝜋(1 −𝜋)} (Exercise 5.1). Taking

STATISTICAL INFERENCE
133
the square root of the reciprocal Fisher information and using ̂𝜋ML to estimate the unknown
value of 𝜋produces the standard error
se[ ̂𝜋ML] =
1
√
( ̂𝜋ML)
=
√
̂𝜋ML(1 −̂𝜋ML)
n
.
(5.33)
With this, the Wald interval from (5.31) is
̂𝜋ML ± z𝛼∕2
√
̂𝜋ML(1 −̂𝜋ML)
n
.
While simple and easy to apply, the actual coverage level for the basic Wald interval here
is known to be highly erratic, even in large samples (Brown et al. 2001, 2002). (Typical rec-
ommendations for ‘large sample’ call for n > 5∕min{𝜋, 1 −𝜋}, although this is not enough
to overcome the erratic behavior of the interval.) Part of the problem arises from the use of
a continuous (normal) distribution to approximate a discrete-valued statistic such as Y∕n. A
‘continuity correction’ has been proposed to alleviate some of this concern – see the review in
Blyth and Still (1983) – however, substantial irregularities still remain with use of this simple
Wald interval for a binomial 𝜋.
Luckily, a wide variety of alternative intervals for 𝜋exist; see the presentation, and the
accompanying discussion, in Brown et al. (2001). Here, two closed-form alternatives are
presented for general use. The first is a very simple modification of the Wald interval given
by Agresti and Coull (1998). It essentially replaces the ML point estimator by a slightly
modified ratio,
̃𝜋=
Y + 1
2z2
𝛼∕2
n + z2
𝛼∕2
,
and then operates under the same estimator ± MOE template. The MOE is again of the form
z𝛼∕2se[ ̃𝜋], where
se[ ̃𝜋] =
√
̃𝜋(1 −̃𝜋)
n + z2
𝛼∕2
.
The final construction is
̃𝜋± z𝛼∕2
√
̃𝜋(1 −̃𝜋)
n + z2
𝛼∕2
.
(5.34)
This is known as an Agresti–Coull (AC) confidence interval for 𝜋.
Similar to the Wald interval, the AC interval in (5.34) is based on asymptotic arguments
and thus is not valid with small samples. Brown et al. (2001) recommend that it be used only
when n > 40.
Notice that for the popular confidence level of 1−𝛼= 0.95, z0.05∕2 = z0.025 = 1.96 ≈2,
from qnorm(0.025, lower.tail=FALSE), and so a quick approximation for (5.34)
employs ̃𝜋′ = (Y + 2)∕(n + 4), with se[ ̃𝜋′] =
√
̃𝜋′(1 −̃𝜋′)∕(n + 4). This was, in fact, part of
Agresti and Coull’s original suggestion for an alternative interval estimator.
It is worth mentioning that the form of the AC point estimator ̃𝜋(and also ̃𝜋′) is chosen
purposefully: the addition of 1
2z2
𝛼∕2 in the numerator and twice of it in the denominator pulls or
shrinks the estimator away from the boundaries, 𝜋= 0 and 𝜋= 1, of the parameter space and

134
STATISTICAL DATA ANALYTICS
towards the central value of 𝜋= 1
2. Near these boundaries, the Wald interval for 𝜋experiences
much of its irregularity. Thus as a point estimator, ̃𝜋is biased and, of course, deviates from the
MLE and the MOM. If point estimation were the lone goal of the analysis, the MLE would
be preferable. For interval estimation, however, the location of the interval’s anchor is less
crucial than the coverage quality of its eventual limits. As a result, the AC interval has gained
substantial support in practical applications with sufficiently large samples.
Although not a likely occurrence in large-data analytics, the question does remain: what
to do if n ≤40? Again, a number of possibilities exist. One favored here is more complicated
than (5.34), but still possesses a closed form. Owing to Wilson (1927), the interval is based on
manipulating the log-likelihood derivative ℓ′(𝜋) to produce valid confidence limits on 𝜋. Both
Agresti and Coull (1998) and Brown et al. (2001) noted that the Wilson interval’s coverage
performance for bounding 𝜋exhibits noticeable stability. Including a continuity correction
provides worthwhile improvement, although it also adds complexity to the final form:
(
Y ± 1
2
)
+ 1
2z2
𝛼∕2
n + z2
𝛼∕2
± z𝛼∕2
√(
Y ± 1
2
)
−1
n
(
Y ± 1
2
)2
+ 1
4z2
𝛼∕2
n + z2
𝛼∕2
,
where the repeated ± notation instructs the user to calculate the lower endpoint whenever a
minus (−) appears and to calculate the upper endpoint whenever a plus (+) appears. (If Y = 0,
set the lower endpoint to 0. If Y = n, set the upper endpoint to 1.) This is known as the Wilson
continuity-corrected (WCC) interval for 𝜋.
In practice, the WCC interval exhibits stable small-sample coverage properties for sample
sizes as low as n ≥5. (With very small sample sizes, the coverage can be slightly conservative.)
For any smaller sample size, the practical recommendation would be to recover more data
before undertaking any statistical inferences on 𝜋.
Computation of these confidence limits is available in a variety of computer pack-
ages/platforms. In R, the AC interval and an uncorrected Wilson interval are available via the
binom.confint() function in the external binom package. The former is recommended
for n > 40. (Although not recommended for use in any circumstance, the Wald interval
is incorporated into the prop.test() function, similar to the t.test() function in
Example 5.3.1.) For smaller samples sizes, direct coding is required for the WCC interval,
although this would not be difficult for moderately experienced R users.
To illustrate, recall from Example 2.3.2 the study of a retail outlet’s n = 1024 customers
who might make a purchase during a sale. In Example 5.2.3, it was determined that y = 506
of the customers actually made a sale purchase, and so the MLE/MOM estimator is ̂𝜋MOM
= 506∕1024 or 49.41% of the customer base. For a 95% confidence interval, the sample size
is large enough here to employ the AC form. In R, this gives
> binom.confint(x=506, n=1024, conf.level=0.95,
method=‘agresti-coull’)
method
x
n
mean
lower
upper
1 agresti-coull 506 1024 0.4941406 0.4635975 0.5247276
The respective 95% AC limits appear under “lower” and “upper” (“mean” gives the MLE).
They indicate that between 46.360% and 52.473% of the customer base was disposed to make
a sale purchase. This information can be used by the outlet’s marketing team when designing
future sales events. (For more on this, see Example 5.4.4.)
◽

STATISTICAL INFERENCE
135
It is important to emphasize that instabilities with the Wald interval for single-sample
binomial data arise primarily from complications when representing discrete binomial
responses through a continuous normal distribution. In many other settings, the Wald
approach operates quite well – at least with sufficiently large samples – and the binomial
case is not representative of the method’s larger applicability. The Wald methodology also
extends easily to the multiparameter case, where it can be quite useful; see Equation (5.49).
An alternative strategy exists for using the likelihood function to build confidence inter-
vals. This evaluates the likelihood at any potential value, 𝜃, for the interval and compares it to
the likelihood at the MLE ̂𝜃. Specifically, start with the likelihood ratio (LR)
Λ(𝜃) = L(𝜃; X1, … , Xn)
L( ̂𝜃; X1, … , Xn)
.
(5.35)
As a function of 𝜃, this ratio is always less than or equal to 1, since ̂𝜃maximizes L. Thus
values of 𝜃that drive Λ(𝜃) sufficiently close to 1 are a ‘most likely’ set of entries for the
interval estimator. To quantify ‘most likely,’ note that (5.35) is a function of the data and,
therefore, is itself a random quantity. In particular, in large samples, the transformation
G2(𝜃) = −2 log{Λ(𝜃)}
satisfies G2(𝜃) ̇∼𝜒2(1), so that P[G2(𝜃) ≤𝜒2
𝛼(1)] ≈1 −𝛼(Wilks, 1938). Now, values of 𝜃for
which G2(𝜃) is sufficiently small become plausible entries for a confidence interval, and thus
this probability can be manipulated into a confidence statement that bounds 𝜃. The result is
called a 100(1−𝛼)% likelihood ratio (LR) interval for 𝜃.
Unfortunately, the LR interval can be difficult to implement, because it does not always
produce closed-form expressions for the confidence limits. Indeed, in the general case, one
cannot write the actual limits L𝜃(X1, … , Xn) < 𝜃< U𝜃(X1, … , Xn) without more specific
details about the nature of the likelihood. As a result, computer calculation is common.
For example, a computationally intensive LR interval exists for the binomial parameter
𝜋in Example 5.3.4 and can be calculated in R via the method=‘profile’ option in
binom.confint(). See Exercise 5.19.
One can also construct confidence intervals on a case-by-case basis, sculpting each pro-
cedure to fit the features of the likelihood under study. For example, if X ∼Poisson(𝜆), an
equivalence relationship between Poisson and 𝜒2 cumulative distribution functions (c.d.f.s)
can be used to build confidence limits for 𝜆. Noted originally by Przyborowski and Wilen-
ski (1935) and also by Garwood (1936), the relationship states that if X ∼Poisson(𝜆), then
P[X > x] = P[V < 2𝜆], where V ∼𝜒2(2x). From this, the following, simple confidence limits
on 𝜆can be derived:
1
2𝜒2
1−(𝛼∕2)(2X) < 𝜆< 1
2𝜒2
𝛼∕2(2X + 2) .
(5.36)
If X = 0 in (5.36), set the lower endpoint to zero. Similar constructs exist for many other
families of distributions; see the exposition in Piegorsch and Bailer (1997, Section 2.6) for
more details.
5.3.5
Delta method intervals∗
When domain-specific interest exists in some function, h(𝜃), of an unknown parameter, the ML
invariance property ensures that h( ̂𝜃) is the corresponding ML estimate. Building confidence

136
STATISTICAL DATA ANALYTICS
limits for h(𝜃) is somewhat more complicated, however. For instance, in the abstract, a 1 −𝛼
Wald interval for h(𝜃) is h( ̂𝜃) ± z𝛼∕2se[h( ̂𝜃)]. This will be a valid interval estimator if (i) h( ̂𝜃)
is (approximately) normal, so that the standard normal critical point z𝛼∕2 may be employed,
and (ii) we can find the standard error of h( ̂𝜃), at least to a good approximation.
More generally, one can appeal to the delta method from Section 2.3.9 for making
inferences on any h(𝜃). In effect, when ̂𝜃̇∼N(𝜃, Var[ ̂𝜃]), the delta method gives h( ̂𝜃) ̇∼
N (h(𝜃), {h′(𝜃)}2Var[ ̂𝜃]). The approximation improves as n →∞. From this, a delta method
confidence interval is straightforward to derive: appeal to (5.7) and find
se[h( ̂𝜃)] =
√
Var[h( ̂𝜃)]|||𝜃= ̂𝜃=
√
{h′( ̂𝜃)}2Var[ ̂𝜃]|||𝜃= ̂𝜃≈|h′( ̂𝜃)| se[ ̂𝜃],
then construct the approximate Wald limits
h( ̂𝜃) ± z𝛼∕2|h′( ̂𝜃)|se[ ̂𝜃].
When there is more than one unknown parameter under study, a multivariate version of the
delta method may be developed. The particulars extend beyond the scope here, but the general
result may be stated: start with the vector of parameters 𝛝= [𝜃1 · · · 𝜃p]T, an unbiased estima-
tor ̂𝛝= [ ̂𝜃1 · · · ̂𝜃p]T, variances Var[ ̂𝜃j], and covariances Cov[ ̂𝜃j, ̂𝜃k] (j ≠k). Assume interest
focuses on some (univariate) function h(𝛝). Then under appropriate regularity conditions, the
delta method gives, to first order, E[h( ̂𝛝)] ≈h(𝛝) and
Var[h( ̂𝛝)] ≈
p
∑
j=1
(
𝜕h
𝜕𝜃j
)2
Var[ ̂𝜃j] + 2
p−1
∑
j=1
p
∑
k=j+1
𝜕h
𝜕𝜃j
𝜕h
𝜕𝜃k
Cov[ ̂𝜃j, ̂𝜃k],
(5.37)
where 𝜕h∕𝜕𝜃j is the partial derivative of h(𝛝) with respect to 𝜃j. For the simplest case where
p = 2, (5.37) simplifies to
Var[h( ̂𝜃1, ̂𝜃2)] ≈
(
𝜕h
𝜕𝜃1
)2
Var[ ̂𝜃1] + 2 𝜕h
𝜕𝜃1
𝜕h
𝜕𝜃2
Cov[ ̂𝜃1, ̂𝜃2] +
(
𝜕h
𝜕𝜃2
)2
Var[ ̂𝜃2].
(5.38)
If, as is common, any 𝜃j remains in the expression for the approximate variance, replace it
with its estimator ̂𝜃j.
In large samples, approximate normality also holds: under suitable regularity conditions,
h(𝛝) ̇∼N
(
h(𝛝), Var[h( ̂𝛝)]
)
,
where the variance is given by (5.37). This leads to approximate 1−𝛼confidence limits via
the Wald construction
h( ̂𝛝) ± z𝛼∕2
√
Var[h( ̂𝛝)]|||𝛝= ̂𝛝.
(5.39)
Example 5.3.5 Confidence interval for a ratio. Many operations in data analytics involve
a ratio of parameters, h(𝜃1, 𝜃2) = 𝜃1∕𝜃2. Ratios are notorious for their instability (especially
if 𝜃2 is near zero), and any calculations using them must be performed with care. With this in
mind, one can use the delta method to construct a confidence interval for 𝜃1∕𝜃2.

STATISTICAL INFERENCE
137
Begin with the MLE, h( ̂𝜃1, ̂𝜃2) = ̂𝜃1∕̂𝜃2. In large samples, this will be approximately nor-
mal, with mean 𝜃1∕𝜃2 and variance from (5.38):
Var [ ̂𝜃1∕̂𝜃2
] ≈
(𝜕{𝜃1∕𝜃2}
𝜕𝜃1
)2
Var[ ̂𝜃1] + 2
(𝜕{𝜃1∕𝜃2}
𝜕𝜃1
) (𝜕{𝜃1∕𝜃2}
𝜕𝜃2
)
Cov[ ̂𝜃1, ̂𝜃2]
+
(𝜕{𝜃1∕𝜃2}
𝜕𝜃2
)2
Var[ ̂𝜃2]
= 1
𝜃2
2
{
Var[ ̂𝜃1] −2𝜃1
𝜃2
Cov[ ̂𝜃1, ̂𝜃2] +
𝜃2
1
𝜃2
2
Var[ ̂𝜃2]
}
.
(5.40)
For use in practice, take the square root of (5.40) and replace the 𝜃js with their estimates to
find se[ ̂𝜃1∕̂𝜃2]. From this, a 1−𝛼Wald interval for the ratio is ̂𝜃1∕̂𝜃2 ± z𝛼∕2se[ ̂𝜃1∕̂𝜃2].
One caveat: this Wald interval for 𝜃1∕𝜃2 can be unstable in small samples, depending on
the values of 𝜃1 and 𝜃2 and on the underlying distribution of the data. In practice, an alternative
confidence interval based on Fieller’s method for a ratio (Fieller 1940) can operate with better
stability and accuracy. The derivation is nontrivial, although manageable; see Buonaccorsi
(2012). Fieller’s theorem gives 1−𝛼limits for 𝜃1∕𝜃2 as
̂𝜃1
̂𝜃2
+
𝛾
1 −𝛾
( ̂𝜃1
̂𝜃2
−̂𝜎12
̂𝜎2
2
)
±
z𝛼∕2
(1 −𝛾)| ̂𝜃2|
√
√
√
√̂𝜎2
1 +
( ̂𝜃1 ̂𝜎2
2
̂𝜃2
−2̂𝜎12
) ( ̂𝜃1
̂𝜃2
)
−𝛾
(
̂𝜎2
1 −
̂𝜎2
12
̂𝜎2
2
)
,
where ̂𝜎2
j = Var[ ̂𝜃j] (j = 1, 2), ̂𝜎12 = Cov[ ̂𝜃1, ̂𝜃2], and 𝛾= z2
𝛼∕2 ̂𝜎2
2∕̂𝜃2
2.
◽
5.3.6
Bootstrap intervals∗
When the parent distribution of the data is unknown, likelihood-based methods for building
confidence intervals are unavailable, because it is impossible to construct a complete like-
lihood for the unknown parameter. An alternative approach in this case approximates the
unknown distribution of the data by simulating random outcomes a large number of times
via computer. This is an application of the Monte Carlo method, a name coined by John von
Neumann and Stanislaw Ulam while both were working at the Los Alamos National Labora-
tory in the 1940s (Anonymous 1949, p. 546). The term associates with the random outcomes
in games of chance seen in Monte Carlo, the capital of Monaco and a well-known center for
gambling. Other names for such random simulation of stochastic outcomes include stochastic
simulation, Monte Carlo simulation, and synthetic data generation.
One specialized form of Monte Carlo simulation useful for building confidence intervals
is the method of bootstrap resampling (Davison and Hinkley 1997). The bootstrap method is
founded on an elegantly simple idea (Efron and Gong 1983): because the sampling distribution
for a statistic is based on repeated samples with replacement – or ‘resamples’ – from the same
population, one can use the computer to simulate repeated sampling, calculating the target
statistic for each simulated sample. The resulting, simulated sampling distribution for the
statistic is used to approximate its underlying, true sampling distribution. In the simplest case,

138
STATISTICAL DATA ANALYTICS
the empirical distribution of the data is used as the basis for the simulated resamples. These
are drawn by computer from a theoretical distribution that matches the empirical distribution,
and the statistic of interest is calculated for each simulated resample. The resampled values of
the statistic provide an approximate distribution from which to construct confidence intervals.
To illustrate the approach, consider the following simple bootstrap confidence interval.
Take a random sample, {X1, … , Xn}, from some population with unknown c.d.f. FX(x). The
goal is to obtain an interval estimate of the unknown parameter 𝜃based on an estimator ̂𝜃
calculated from the data. Start with an estimate of FX(x) by counting how often any of the
data values lie at or below a desired target argument x and dividing by n. This is called the
empirical c.d.f.
̂FX(x) = {Number of Xi ≤x}
n
.
If ̂FX(x) is a good estimate of FX(x), we can generate bootstrap samples as follows:
(1) Generate a bootstrap sample, say, {X∗
1, X∗
2, … , X∗
n} at random from the empirical
c.d.f. ̂FX(x) using standard methods for simulating pseudorandom variates from c.d.f.s
(Gentle 2003, Section 4.1).
(2) Calculate the statistic of interest from the bootstrap sample. Denote this as ̂𝜃∗.
(3) Repeat steps (a) and (b) a large number, B, of times. Babu and Singh (1983) gave the-
oretical arguments for B = n(log n)2, although if n < 60, a common recommendation
is to set at least B = 2000 for building confidence intervals.
(4) Assemble the target statistics from each of the bootstrap samples. Let ̂𝜃∗
1, ̂𝜃∗
2, … , ̂𝜃∗
B
be this collection of the resampled statistic.
This assemblage of bootstrapped statistics ̂𝜃∗
b (b = 1, … , B) represents an empirical estimate
for the sampling distribution of ̂𝜃. From this, we obtain a confidence interval on 𝜃by selecting
specified percentiles from the empirical distribution formed from the collection of ̂𝜃∗
bs. This
is known as the percentile method (DiCiccio and Romano 1988). For example, a 95% confi-
dence interval on 𝜃based on the percentile method takes the 2.5th percentile of the bootstrap
distribution of the ̂𝜃∗
bs for the lower bound and the 97.5th percentile for the upper bound.
In R, a variety of external packages can create bootstrap resamples. (One can also code
the bootstrap directly, as desired.) For instance, the external boot package is recommended
and ties directly to the textbook by Davison and Hinkley (1997).
5.4
Testing hypotheses
A second form of statistical inference useful in data analytics is based on the comparison of
two competing hypotheses that describe the unknown parameter 𝜃. This is known as hypoth-
esis testing. Suppose a particular value of 𝜃, say 𝜃o, defines a standard or objective condition
for the underlying population. Then, the null hypothesis represents the condition that 𝜃is
𝜃o, written as Ho: 𝜃= 𝜃o. This is also called the ‘no-effect hypothesis,’ because 𝜃o gener-
ally indicates lack of some impact or effect such as no response to an external stimulus. The
alternative hypothesis or research hypothesis, Ha, is a specification for 𝜃that represents a
plausible research alternative to Ho. (Some authors denote the alternative hypothesis as H1.)
The goal is determination of whether Ho or Ha represents a credible indication of the nature

STATISTICAL INFERENCE
139
of 𝜃. To achieve this, the process is structured to arrive at a decision regarding Ho, that is,
whether to reject it or accept it in light of the information about 𝜃observed in the data.
Two fundamental probabilities lie at the core of testing these hypothesis statistically. The
first is the probability of a false positive error: 𝛼= P[reject Ho|Ho true]. (Notice how the
probability statement conditions on the unknown event that Ho is true.) This is the Type I or
false positive error rate, and it is usually fixed in advance by the analyst. The second is the
probability of a false negative error: 𝛽= P[accept Ho|Ho false]. This is the Type II or false neg-
ative error rate. Associated with 𝛽is the power or sensitivity of the test: P[reject Ho|Ho false]
= 1 −𝛽. For fixed 𝛼, the goal is to find a test of Ho (vs Ha) that minimizes 𝛽and thus maximizes
power to the greatest extent possible.
Given a set of data, a test statistic that addresses the effect being studied in Ho is cal-
culated. From this, a rejection region – an older term is critical region – is formed where, if
the test statistic falls into this region, Ho is rejected in favor of Ha. (If not, we fail to reject
Ho.) Equivalently, one can find the P-value of the test, defined as the probability under Ho
of observing a test statistic as extreme as or more extreme than that actually observed. Note
that ‘more extreme’ is defined in the context of Ha. For example, when testing Ho: 𝜃= 𝜃o ver-
sus a ‘one-sided’ alternative such as Ha: 𝜃> 𝜃o, ‘more extreme’ corresponds to values of the
test statistic supporting 𝜃> 𝜃o. By contrast, for the ‘two-sided’ alternative Ha: 𝜃≠𝜃o, ‘more
extreme’ corresponds to values of the test statistic supporting either 𝜃> 𝜃o or 𝜃< 𝜃o.
Small P-values indicate departure from Ho; thus for a fixed level of 𝛼, one rejects Ho when
P ≤𝛼. Since the significance of departure from Ho is captured by whether or not 𝛼exceeds
P, 𝛼is often called the significance level of the test. Indeed, the technical terminology has
evolved to focus on the ‘significance’ of the test outcome; see Table 5.3.
Hypothesis tests may be derived under a number of different conditions. These correspond
to analogous criteria for building confidence intervals. Indeed, the two concepts are tautolog-
ically related: suppose one is testing Ho: 𝜃= 𝜃o versus Ha: 𝜃≠𝜃o. It is possible to build a
1 −𝛼confidence region for 𝜃from the complement of the rejection region (often called the
‘acceptance region’): if the confidence region fails to contain 𝜃o, one will reject Ho at the 𝛼
level of significance and vice versa (Casella and Berger 2002, Section 9.2).
It is important to emphasize that the rationale for specifying a one-sided alternative must
be determined prior to examining the data. The analyst cannot first study the observations,
then use the information in, say, X to choose the direction for Ha in order to ‘improve’ the
opportunity to reject Ho. One-sided hypotheses, tests, and confidence limits may be based
only on a priori or existing domain-specific knowledge for the testing or estimation problem
Table 5.3
Terminology for hypothesis test outcomes.
Ho
Ha
Test outcome
Terminology
𝜃= 𝜃o
𝜃≠𝜃o
Reject Ho
𝜃significantly different from 𝜃o
Fail to reject Ho
𝜃insignificantly (or not
significantly) different from 𝜃o
𝜃= 𝜃o
𝜃> 𝜃o
Reject Ho
𝜃significantly greater than 𝜃o
Fail to reject Ho
𝜃not significantly greater than 𝜃o
𝜃= 𝜃o
𝜃< 𝜃o
Reject Ho
𝜃significantly smaller than 𝜃o
Fail to reject Ho
𝜃not significantly smaller than 𝜃o

140
STATISTICAL DATA ANALYTICS
at hand. (For example, a biomedical scientist may question whether proximity to a pollutant
source leads to increased cancer in an exposed population. The underlying scientific rationale
clearly calls for selection of a one-sided, increasing alternative hypothesis.) If no such prior
subject-matter justification is available, however, the default selection is always a two-sided
Ha. Any other use violates the basic probability statements from which the inferences were
formed and will in fact lead to incorrect statistical inferences. While data can certainly be
‘mined’ in an exploratory manner to search for unrecognized features or trends, construc-
tion of formal statistical inferences cannot be based on ‘snooping’ through the concurrent
observations.
5.4.1
Single-sample tests for normal (Gaussian) parameters
Perhaps the most well-known hypothesis test is the t-test, so-named because it is based on
Student’s t-distribution from Section 2.3.10. Many forms exist for the t-test; consider here the
simple case of testing from a normal random sample. Let Xi ∼i.i.d. N(𝜇, 𝜎2), i = 1, … , n,
for unknown 𝜇and 𝜎2. Consider testing Ho: 𝜇= 𝜇o versus Ha: 𝜇≠𝜇o at significance level 𝛼.
For this t-test, estimate 𝜇with its MLE from Example 5.2.5, X, and use the unbiased
sample variance, S2, from (3.5) to estimate 𝜎2. The standard error of X is se[X] = S∕
√
n.
From these, build the test statistic
T = X −𝜇o
S∕
√
n
,
(5.41)
where X ∼N(𝜇, 𝜎2∕n). If Ho is true, the expected value of X is 𝜇= 𝜇o and then the statistic
in (5.41) is a Studentized t random variable. Thus, conditioning on the event that Ho is true
produces T ∼t(n −1).
To build the rejection criterion, consider the region |T| ≥t𝛼∕2(n −1). When Ho is true and
T ∼t(n −1), the corresponding false positive (Type I) error rate is
P[reject Ho|Ho true] = P[|T| ≥t𝛼∕2(n −1)|𝜇= 𝜇o]
= P[|T| ≥t𝛼∕2(n −1)|T ∼t(n −1)],
(5.42)
where the absolute value in the rejection criterion forces consideration of both the lower and
upper tails of the t(n −1) reference distribution. By the symmetry of the t p.d.f., however, the
latter probability in (5.42) is just 2 × P[T ≥t𝛼∕2(n −1)|T ∼t(n −1)], and from the definition
of the t critical point, this becomes (2)(𝛼∕2) = 𝛼. So, (5.42) reduces to P[reject Ho|Ho true]
= 𝛼, satisfying the false positive error requirement. This defines the one-sample t-test of
Ho: 𝜇= 𝜇o versus Ha: 𝜇≠𝜇o.
The corresponding P-value is the probability under Ho that the (null) reference distribution
exceeds the observed value of the test statistic. Denote this latter quantity as tcalc. When Ho is
true, the T statistic is referred to t(n −1), so the P-value is
P = P[|t(n −1)| ≥|tcalc|] = 2P[t(n −1) ≥|tcalc|],
that is, appeal to the symmetry of the t(n −1) p.d.f. and find twice its upper tail area past |tcalc|.
In R, this can be calculated via the command
> 2*pt( abs(tcalc), df=n-1, lower.tail=FALSE )

STATISTICAL INFERENCE
141
The one-sample t-test satisfies the confidence interval/hypothesis test tautology mentioned
earlier. When testing Ho: 𝜇= 𝜇o versus the two-sided alternative Ha: 𝜇≠𝜇o via a level-𝛼
t-test, the corresponding (two-sided) 1−𝛼confidence region for 𝜇from (5.17) will fail to
contain 𝜇o whenever the test rejects Ho and vice versa.
One-sided tests are similar. For instance, when testing Ho: 𝜇= 𝜇o against the one-sided
alternative Ha: 𝜇> 𝜇o, employ the rejection region T ≥t𝛼(n −1), with correspond-
ing P-value P = P[t(n −1) ≥tcalc]. In R, find the P-value via pt(tcalc, df=n-1,
lower.tail=FALSE).
Example 5.4.1 t-test for 𝝁from a normal sample (Example 5.3.1, continued). Consider
again the myocardial infarction data in Table 4.2. The average age of attack for these data
was seen to be X = 62.8137, while the standard error of X was found as se[X] = S∕
√
126 =
8.3421∕11.2250 = 0.7432. In Example 4.1.7, a normal quantile plot suggested a tenable fit of
the normal model to these data. Thus if interest existed in testing a particular value of 𝜇o for
this population’s mean age of attack, the t-test would be an option.
At the time of this writing, US adults are eligible for medical coverage through the Federal
Medicare system after they turn age 65. Interest with this study population includes whether
its true mean age-to-attack, 𝜇, is earlier than the Medicare entry age. This translates to testing
Ho: 𝜇= 65 versus the one-sided alternative Ha: 𝜇< 65. We reject Ho in favor of Ha via the
t-test when T ≤−t𝛼(n −1). At 𝛼= 0.05, the necessary critical point is −t0.05(125) = −1.6571,
found in R via qt(0.05, 125, lower.tail=TRUE).
The pertinent test statistic here is
T = X −𝜇o
S∕
√
n
= 62.8137 −65
8.3421∕
√
126
,
producing tcalc = −2.9418. Since tcalc = −2.9418 < −1.6571 = −t0.05(125), we reject Ho for
these data and conclude that the mean age of attack for this population is significantly lower
than 65 years. Medicare planners can use this knowledge for planning medical coverage and
care needs for this population of patients.
The P-value here is P[t(n −1) ≤tcalc] = P[t(125) ≤−2.9418] = 0.0019, found using the
R command pt(-2.9418, df=125, lower.tail=TRUE). Since P = 0.0019 ≤0.05 = 𝛼,
the decision is again to reject Ho in favor of Ha.
The R routine t.test() provides all these calculations in one combined operation.
Simply use
> t.test( X, alternative="less", mu=65, conf.level=0.95 )
with output (edited)
One Sample t-test
data:
X
t = -2.9418, df = 125, p-value = 0.001945
alternative hypothesis: true mean is less than 65
95 percent confidence interval:
-Inf
64.04526
sample estimates:
mean of x
62.81372

142
STATISTICAL DATA ANALYTICS
In particular, the P-value of P = 0.0019 is given after ‘p-value =’ (the test statistic is listed
after ‘t =’).
Note that with these same data, the t.test() output in Example 5.3.1 exhibited some
marked differences. In that example, the R function was used only to find a 95% confidence
interval for 𝜇and not to construct a targeted hypothesis test. Thus no specific value was given
for 𝜇o, and no alternative= option was supplied. As a result, the t.test() output in that
example used the R defaults of 𝜇o = 0 and a two-sided alternative (settings with little sensible
interpretation with these data). Of course, the 95% confidence limits it provided were valid,
and useful, because they gave a plausible range of values for the population’s mean age of
attack. The inference provided by the hypothesis test here is arguably more limited, if also
more specific.
Speaking of confidence limits, notice in the t.test() output above that a set of
confidence limits is provided after ‘95 percent confidence interval.’ Because the
conf.level=0.95 option was entered, the confidence level was set to 95% while, because
alternative=‘less’ was specified, the limits are one sided. That is, they correspond to
the one-sided upper bound from (5.18). This is indicated by the -Inf output for the lower
confidence limit. As expected, the upper bound does not reach the null value of 𝜇= 65.
◽
If desired, it is also possible to fashion tests of the variance parameter in this single-sample,
normal setting. Suppose interest targets a test of Ho: 𝜎2 = 𝜎2
o versus Ha: 𝜎2 ≠𝜎2
o. The 𝜒2 rela-
tionship employed to build the confidence limits on 𝜎2 in (5.21) also allows for construction
of a test statistic. Under Ho, the statistic
C2 = (n −1)S2
𝜎2
o
is distributed as 𝜒2(n −1). Thus the rejection region
C2 ≤𝜒2
1−𝛼
2
(n −1) or
C2 ≥𝜒2
𝛼∕2(n −1)
can be shown to satisfy a level-𝛼false positive error requirement. Similar to the caveat in
Section 5.3.2, however, allocation here of equal 𝛼∕2 false positive error probability to both
tails in the 𝜒2 reference distribution is done purely for convenience. It is generally suboptimal,
and alternative tail area allocations should be considered on a case-by-case basis.
5.4.2
Two-sample tests for normal (Gaussian) parameters
Hypothesis tests can also be developed for testing across two independent samples. To estab-
lish the concepts, suppose data from the first sample are X1j ∼i.i.d. N(𝜇1, 𝜎2
1), j = 1, … , n1,
independent of data from the second sample X2j ∼i.i.d. N(𝜇2, 𝜎2
2), j = 1, … , n2. Assume
both samples’ sets of parameters 𝜇1, 𝜎2
1 and 𝜇2, 𝜎2
2 are unknown. All of the tests discussed
in this section for testing these various parameters will be constructed essentially by inverting
their corresponding confidence intervals from Section 5.3.3.
Begin with tests for Ho: 𝜇1 = 𝜇2 versus Ha: 𝜇1 ≠𝜇2, that is, testing whether the means of
the two samples are equal. By focusing on the difference Δ = 𝜇1 −𝜇2, the hypotheses become
Ho: Δ = 0 versus Ha: Δ ≠0, for which the pivotal quantity from (5.22) leads directly to the

STATISTICAL INFERENCE
143
Studentized test statistic
T =
X1 −X2
√
1
n1
S2
1 + 1
n2
S2
2
.
(5.43)
If Ho is true, the T statistic in (5.43) has the approximate, null, reference distribution
T ̇∼t(𝜈WS), where the d.f. 𝜈WS are given by the Welch–Satterthwaite correction in (5.23).
Reject Ho when |T| ≥t𝛼∕2(𝜈WS). The corresponding approximate P-value is
P ≈P[|t(𝜈WS)| ≥|tcalc|] = 2P[t(𝜈WS) ≥|tcalc|],
where tcalc is the observed value of the test statistic in (5.43). In R, find the P-value via
2*pt(abs(tcalc),df=nuWS,lower.tail=FALSE), where tcalc is the calculated test
statistic and nuWS are the Welch–Satterthwaite d.f. from (5.23).
As above, the Welch–Satterthwaite approximation for the d.f. improves as min{n1, n2} →
∞. It is quite accurate, however, and the approximate false positive error rate will be very near
𝛼for sample sizes as low as ni = 10.
An aside: readers may wonder what happened to ‘Δo’ in the Studentized test statistic
(5.43). That is, if mimicking the Studentizing operation in the analogous statistic from (5.41),
one would expect the numerator for T in (5.43) to appear as X1 −X2 −Δo. In fact, it does:
the null value Δo is simply 0 here, leading to a numerator of the form X1 −X2 −0 = X1 −
X2. Indeed, to test for any other value of Δo in Ho: 𝜇1 −𝜇2 = Δo, one simply subtracts the
specified value of Δo in the numerator of (5.43).
One-sided tests are similar. For instance, when testing Ho: 𝜇1 = 𝜇2 against the alternative
Ha: 𝜇1 > 𝜇2, reconstruct the hypotheses as Ho: Δ = 0 versus Ha: Δ > 0 and use the rejection
region T ≥t𝛼(𝜈WS), with corresponding P-value P = P[t(𝜈WS) ≥tcalc]. In R, find the P-value
via pt(tcalc, df=nuWS, lower.tail=FALSE).
Example 5.4.2 Two-sample t-test for Lung Function data (Example 5.3.2, continued).
Return to the lung function data from Table 5.1, measuring ‘FEV1’ in 26-year-old asthmatics.
In Example 5.3.2, a 90% confidence interval for the difference in mean FEV1 scores was
seen to contain the value 𝜇1 −𝜇2 = 0, indicating a lack of any difference in mean FEV1
between smokers and nonsmokers. To formalize this, test the hypothesis Ho: 𝜇1 = 𝜇2 against
Ha: 𝜇1 ≠𝜇2 at the 10% significance level.
The necessary statistics were given in Table 5.2, from which the test statistic in (5.43)
evaluates to
97.5962 −97.7500
√
121.9386
265
+ 150.7644
88
= −0.1538
√
2.1734
or simply tcalc = −0.1043. The corresponding Welch–Satterthwaite correction for the d.f. gave
𝜈WS = 136, so the P-value is P ≈2P[t(136) ≥| −0.1043|] = 0.9171. As this is greater than
𝛼= 0.10, we fail to reject Ho and conclude that no significant difference exists between the
mean FEV1 scores. Notice that this is consistent with the inferences reached using the confi-
dence interval in Example 5.3.2, illustrating the tautology between confidence intervals and
hypothesis tests.
To perform these calculations directly in R, use
> t.test( X1, X2, conf.level=0.90, var.equal=FALSE )

144
STATISTICAL DATA ANALYTICS
The output is identical to that seen in Example 5.3.2, where the output statistics corroborate
those calculated earlier. While decreases in FEV1 are often observed among older, lifelong
smokers, the inferences here suggest that such effects are not as evident among individuals in
their 20s, at least for this population of asthmatics.
◽
If one assumes that the two population variances are equal, 𝜎2
1 = 𝜎2
2 = 𝜎2 (say), the
independent-samples t-test in (5.43) collapses to an exact construction. The numerator of
the test statistic remains the same, and the denominator now employs the pooled variance
estimator, S2
p, from (5.25). The result is an appropriate modification of the pivotal quantity in
(5.26) for testing Ho: Δ = 0:
T =
X1 −X2
Sp
√
1
n1
+ 1
n2
.
(5.44)
Under a homogeneous-variance assumption, the null reference distribution for T in (5.44) is
exactly T ∼t(n1 + n2 −2). Reject Ho against Ha: Δ ≠0 when |T| ≥t𝛼∕2(n1 + n2 −2). In R,
if X1 and X2 are vectors containing the data from each independent sample, then the command
t.test(X1, X2, var.equal=TRUE) can conduct the test and supply pertinent statistics.
The corresponding exact P-value is
P = P[|t(n1 + n2 −2)| ≥|tcalc|] = 2P[t(n1 + n2 −2) ≥|tcalc|],
where tcalc is the observed value of the test statistic in (5.44). In R, use
> 2 * pt( abs(tcalc), df=n1+n2-2, lower.tail=FALSE )
One-sided tests are similar. For instance, when testing Ho: 𝜇1 = 𝜇2 against the one-sided
alternative Ha: 𝜇1 > 𝜇2, reconstruct the hypotheses as Ho: Δ = 0 versus Ha: Δ > 0 and
employ the rejection region T ≥t𝛼(n1 + n2 −2). The corresponding P-value is P = P[t(n1
+ n2 −2) ≥tcalc]. For the P-value in R, use pt(tcalc, df=n1+n2-2, lower.tail=
FALSE) or simply call
> t.test( X1, X2, var.equal=TRUE, alt=‘greater’ )
to conduct the test and acquire the pertinent statistics.
A specialized t-test may also be developed for the particular case where observations are
recorded in matched pairs, (X1j, X2j), with Xij ∼i.i.d. N(𝜇i, 𝜎2
i ), i = 1, 2; j = 1, … , n.
As in Section 5.3.3, we translate the paired data to differences Dj = X1j −X2j, where
Dj ∼i.i.d. N(𝜇D, 𝜎2
D). Here again, the differences act as a single sample, allowing for con-
struction of a paired t statistic essentially similar to the single-sample Studentized statistic in
(5.41).
For this paired-samples t-test, the two-sided hypotheses are Ho: 𝜇D = 0 versus Ha: 𝜇D ≠0,
where 𝜇D = 𝜇1 −𝜇2. The test statistic is then
TD =
D
SD∕
√
n
.
(5.45)
Reject Ho when |TD| ≥t𝛼∕2(n −1). The corresponding exact P-value is
P = P[|t(n −1)| ≥|tcalc|] = 2P[t(n −1) ≥|tcalc|],

STATISTICAL INFERENCE
145
where tcalc is the observed value of the test statistic in (5.45). In R, use
> 2 * pt( abs(tcalc), df=n-1, lower.tail=FALSE )
One-sided tests are similar. For instance, when testing Ho: 𝜇1 = 𝜇2 against the one-sided
alternative Ha: 𝜇1 > 𝜇2, reconstruct the hypotheses as Ho: 𝜇D = 0 versus Ha: 𝜇D > 0 and
employ the rejection region TD ≥t𝛼(n −1). The corresponding P-value is P = P[t(n −1) ≥
tcalc]. For the P-value in R, use pt(tcalc, df=n-1, lower.tail=FALSE) or simply call
> t.test( X1, X2, paired=TRUE, alt=‘greater’ )
to conduct the test and acquire the pertinent statistics.
In the general two-sample normal setting, one can also test hypotheses for the two pop-
ulation variances 𝜎2
1 and 𝜎2
2. Suppose again that X1j ∼i.i.d. N(𝜇1, 𝜎2
1), j = 1, … , n1, inde-
pendent of X2j ∼i.i.d. N(𝜇2, 𝜎2
2), j = 1, … , n2. Assume that all the population parameters
are unknown and focus interest on testing Ho: 𝜎2
1 = 𝜎2
2 versus Ha: 𝜎2
1 ≠𝜎2
2. Notice that this is
equivalent to testing the variance ratio: Ho: 𝜎2
1∕𝜎2
2 = 1 versus Ha: 𝜎2
1∕𝜎2
2 ≠1.
Similar to the (exact) confidence interval on 𝜎2
1∕𝜎2
2 in (5.28), one builds the test statistic
from an F ratio
F12 =
S2
1
S2
2
,
where S2
1 and S2
2 are the independent sample variances. Under Ho, F12 ∼F(n1 −1, n2 −1);
therefore, for a level-𝛼test, reject Ho in favor of Ha when
F12 ≤F1−𝛼
2 (n1 −1, n2 −1) or F12 ≥F𝛼∕2(n1 −1, n2 −1).
(5.46)
Using a clever manipulation of the relationship between F12 and 1∕F12 – see
Section 2.3.10 – one can simplify the rejection region here. Let S2
(2) = max{S2
1, S2
2},
S2
(1) = min{S2
1, S2
2}, and denote n∗
(j) as the sample size associated with each S2
(j) (j = 1, 2).
Then, the rejection region in (5.46) is equivalent to
S2
(2)
S2
(1)
≥F𝛼∕2(n∗
(2) −1, n∗
(1) −1).
In either case, the corresponding P-value is P = 2 P[F(n∗
(2), n∗
(1)) ≥S2
(2)∕S2
(1)].
In R, the var.test() function can perform these operations with independent,
two-sample, normal data.
5.4.3
Walds tests, likelihood ratio tests, and ‘exact’ tests∗
The tautologous relationship between confidence intervals and hypothesis tests also applies
to the general Wald statistic in Section 5.3.4, and Wald intervals are easily inverted to produce
hypothesis tests. From a random sample Xi ∼i.i.d. fX(x|𝜃), i = 1, … , n, consider tests on the
generic parameter 𝜃via Ho: 𝜃= 𝜃o versus Ha: 𝜃≠𝜃o. To do so, find the MLE ̂𝜃and its standard
error se[ ̂𝜃]. In large samples, ̂𝜃̇∼N(𝜃, se2[ ̂𝜃]), so consider the standardized test statistic
Z =
̂𝜃−𝜃o
se[ ̂𝜃]
.
(5.47)

146
STATISTICAL DATA ANALYTICS
If Ho is true, the (approximate) expected value of ̂𝜃is 𝜃= 𝜃o, and then this Z statistic is
(approximately) standard normal. That is, conditioning on the event that Ho is true produces
Z ̇∼N(0, 1).
Similar to the t-test construction in (5.42), the consequent rejection region is |Z| ≥z𝛼∕2
and the approximate false positive (Type I) error rate is
P[reject Ho|Ho true] = P[|Z| ≥z𝛼∕2|𝜃= 𝜃o] ≈2{1 −Φ(z𝛼∕2)},
(5.48)
where Φ(z) is the standard normal c.d.f. from (2.35). Using the definition of z𝛼∕2, (5.48)
reduces to
P[reject Ho|Ho true] ≈2
{
1 −
(
1 −𝛼
2
)}
= 𝛼,
satisfying the false positive error requirement, at least approximately. (The approximation
improves as n →∞.) This defines the Wald test of Ho: 𝜃= 𝜃o versus Ha: 𝜃≠𝜃o, via the rejec-
tion region |Z| ≥z𝛼∕2.
The corresponding Wald test P-value is the probability under Ho that the (null) reference
distribution exceeds the observed value of the test statistic. Denote this latter quantity as zcalc.
The P-value is then
P = P[|Z| ≥|zcalc|] ≈2{1 −Φ(|zcalc|)},
that is, twice the upper tail area past |zcalc| from a standard normal reference distribution. In
R, this can be calculated via 2*pnorm(abs(zcalc), lower.tail=FALSE).
It is interesting to note that an equivalent rejection region for the two-sided Wald test can
be constructed from a 𝜒2 critical point. Recall from Section 2.3.10 that if Z ∼N(0, 1), then
Z2 ∼𝜒2(1). Thus the two-sided Wald rejection region can also be written as Z2 ≥𝜒2
𝛼(1). The
corresponding approximate P-value is P[𝜒2(1) ≥z2
calc].
The tautology between confidence intervals and hypothesis tests remains in effect here.
When testing Ho: 𝜃= 𝜃o versus the two-sided alternative Ha: 𝜃≠𝜃o via a level-𝛼Wald test,
the corresponding two-sided 1−𝛼confidence region for 𝜃will fail to contain 𝜃o whenever the
test rejects Ho and vice versa.
For testing Ho: 𝜃= 𝜃o against the one-sided alternative Ha: 𝜃> 𝜃o, employ the rejection
region Z ≥z𝛼, with corresponding approximate P-value P ≈1 −Φ(zcalc). In R, find the
P-value via pnorm(zcalc, lower.tail=FALSE).
In similar fashion, one can invert LR intervals into hypothesis tests of Ho: 𝜃= 𝜃o versus
Ha: 𝜃≠𝜃o. Start with the test statistic
G2 = −2 log
{
L(𝜃o; X1, … , Xn)
L( ̂𝜃; X1, … , Xn)
}
.
Under Ho, G2 ̇∼𝜒2(1), so reject Ho in favor of Ha when G2 ≥𝜒2
𝛼(1). The corresponding,
approximate P-value is P ≈P[𝜒2(1) ≥g2
calc], where g2
calc is the observed value of the LR
statistic.
Alert readers will notice that both Z2 and G2 appeal in large samples to the same ref-
erence distribution, 𝜒2(1). This is no coincidence: in most settings, the limiting value and
large-sample reference distribution for both these likelihood-based methods will be the same.
We call this a form of asymptotic equivalence among the test statistics. As n →∞, the meth-
ods provide essentially the same inference on 𝜃, although, depending on the specific likelihood
under study, they can vary substantially in small samples.

STATISTICAL INFERENCE
147
Extensions to tests for the multiparameter case are also possible with these likelihood-
based approaches; see, for example, Cox (1988).
Example 5.4.3 Wald test and LR test for 𝝁from a normal sample. With a single sample
from a normal distribution, the Wald test collapses to the t-test from Example 5.4.1. To see
this, start with a normal random sample Xi ∼i.i.d. N(𝜇, 𝜎2), i = 1, … , n, for unknown 𝜇and
𝜎2. Set the hypotheses as Ho: 𝜇= 𝜇o versus Ha: 𝜇≠𝜇o and operate at significance level 𝛼.
The Wald statistic from (5.47) takes the MLE of 𝜇, subtracts the null value 𝜇o, and divides
this difference by the standard error of the MLE. In this normal case, however, the MLE of 𝜇
is X, with standard error se[X] = S∕
√
n. The Wald statistic is, therefore,
X −𝜇o
S∕
√
n
.
Clearly, this is the T statistic in (5.41), so that the two test statistics are one and the same.
The Wald procedure approximates the large-sample, null, reference distribution for T as
N(0,1), producing a large-sample rejection region of the form |T| ≥z𝛼∕2. Since we know from
Section 5.4.1 that the exact reference distribution for T is t(n −1), however, the exact rejection
region is |T| ≥t𝛼∕2(n −1). Indeed, as n →∞, t𝛼∕2(n −1) →z𝛼∕2, and thus the two rejection
regions will in fact converge to one another.
It is interesting to note that, here, the LR test for Ho: 𝜇= 𝜇o produces a rejection region
of the form
n(X −𝜇o)2
S2
≥F𝛼(1, n −1)
(Casella and Berger 2002, Section 8.2), where F𝛼(1, n −1) is the upper-𝛼critical point
from an F-distribution with 1 and n−1 d.f. Observe that this rejection region is equivalent to
T2 ≥t2
𝛼(n −1), because a squared t(n −1) variate is distributed as F(1, n −1). Thus in this
simple normal setting, the t-, Wald, and LR tests for 𝜇all coincide.
◽
Example 5.4.4 Tests for 𝝅from a binomial sample. When sampling from a binomial distri-
bution, interest often exists in testing whether the probability parameter 𝜋equals a particular
𝜋o. Suppose a random sample is taken of n Bernoulli trials, Xi ∼i.i.d. Bin(1, 𝜋), such that
the sum, Y = ∑n
i=1 Xi, is Y ∼Bin(n, 𝜋). The basic Wald test of Ho: 𝜋= 𝜋o versus Ha: 𝜋≠𝜋o
can be constructed from the MLE ̂𝜋ML = Y∕n, using the standard error in (5.33). The test
statistic is
W = ̂𝜋ML −𝜋o
se[ ̂𝜋ML] ,
referenced in large samples to W ̇∼N(0, 1). This can be conducted in R via the prop.test()
function.
As seen with the binomial confidence intervals in Example 5.3.4, however, the Wald
approach can exhibit instabilities with a single binomial sample, and alternative methods are
indicated. One could construct an LR test for Ho here (Exercise 5.30), and for sufficiently
large n, this exhibits reasonable properties.
Another option valid for any sample size is a computer-intensive strategy known as an
‘exact test’ of Ho. As the binomial likelihood is defined over a discrete, finite, sample space, it
is feasible in theory to enumerate all possible configurations for Y under Ho and compare these

148
STATISTICAL DATA ANALYTICS
to the outcome actually observed. The corresponding P-value is the probability of recovering
a configuration as extreme as or more extreme (with respect to Ha) than the observed outcome.
In R, the binomial exact test is available via the binom.test() function. To illustrate,
recall from Example 2.3.2 the study of a retail outlet’s n = 1024 affinity customers who
might make a purchase during a sale. In Example 5.2.3, it was determined that Y = 506 of the
customers made a purchase, so the MLE was ̂𝜋ML = 506∕1024 or 49.41% of the affinity base.
The original question raised by the outlet concerned calculations with 𝜋= 50% (Example
2.3.2), so consider testing Ho: 𝜋= 1
2 versus Ha: 𝜋≠1
2 via the binomial exact test. Set
𝛼= 0.05. The R command
> binom.test( x=506, n=1024, p=0.50, conf.level=0.95,
alternative=‘two.sided’ )
produces
Exact binomial test
data:
506 and 1024
number of successes = 506, number of trials = 1024,
p-value = 0.7311
alternative hypothesis: true probability of success is not
equal to 0.5
95 percent confidence interval:
0.4630844 0.5252306
sample estimates:
probability of success
0.4941406
The exact test provides a P-value of P = 0.7311 (following “p-value =”) for testing Ho
against Ha. As this is clearly greater than 𝛼= 0.05, we fail to reject Ho with these data and
conclude that the probability of a customer participating in the sale does not appear to differ
significantly from 50%. One might call this a “50–50 chance,” indicating no swing in the cus-
tomers’ purchasing probability away from 50% in response to the sale. (The outlet’s marketing
department could use this information when planning future sales events such as this.)
◽
Notice in the previous example that the binom.test() output also provided a “95 per-
cent confidence interval” for the unknown value of 𝜋. This corresponds to an interval
given by Clopper and Pearson (1934). It literally inverts the binomial exact test to produce
the confidence limits, taking advantage of the test-interval tautology discussed earlier. The
method is very conservative – Brown et al. (2001) called it “wastefully” so – and is not gener-
ally recommended for modern, practical use. Other possibilities, mentioned in Example 5.3.4,
would be preferred.
Exact tests can also be developed for hypotheses that involve two independent binomial
samples. Discussion of these methods is, however, deferred to the introduction of contingency
tables in Section 8.3.3.
5.5
Multiple inferences∗
An important consideration in data analytics is that of multiple inferences or multiple com-
parisons, that is, when two or more inferences are performed on a single set of data. The
multiplicity problem is common when there are many population parameters under study and

STATISTICAL INFERENCE
149
confidence intervals or hypothesis tests are desired on each. For instance, suppose a hypoth-
esis test is conducted on each of p parameters from the same set of data. Without correction
for the multiplicity of tests being undertaken, there will be p opportunities to make a false
positive error. Thus the experimentwise or familywise false positive error rate (FWER) will
be much larger than the pointwise significance level, 𝛼. (Analogous concerns regarding the
confidence coefficient, 1−𝛼, arise when constructing multiple pointwise confidence intervals.)
To account for this error inflation, some adjustment is required.
The simplest way to adjust for multiple inferences is to build the multiplicity into the esti-
mation or testing scenario; for example, construct joint hypothesis tests or joint confidence
regions that simultaneously contain all p parameters of interest. All of the inferential methods
presented in the preceding sections can be extended in this manner. For instance, a simulta-
neous, p-dimensional, 1−𝛼Wald confidence region for a vector of unknown parameters 𝛝is
the ellipsoid defined by the matrix inequality
( ̂𝛝−𝛝)TF( ̂𝛝)( ̂𝛝−𝛝) ≤𝜒2
𝛼(p),
(5.49)
where F( ̂𝛝) is the Fisher information matrix evaluated at the MLE ̂𝛝.
Joint calculations are not always possible, however, or they may be less useful in prac-
tice than the simpler statements a pointwise construction can provide. For instance, a set of p
intervals on the individual parameters, 𝜃j (j = 1, … , p), of 𝛝is often easier to interpret than
a p-dimensional confidence ellipsoid. In such cases, it is still possible to adjust for the multi-
plicity, using one of a variety of probability inequalities (Galambos and Simonelli 1996). The
next section describes perhaps the most popular of these, the Bonferroni inequality.
5.5.1
Bonferroni multiplicity adjustment
Bonferroni’s inequality (Bonferroni 1936) is in essence a statement relating the joint probabil-
ity of a set of multiple events to the individual event probabilities. Let P[12 · · · p] denote
the probability that the p events j (j = 1, … , p) occur simultaneously. Then, if c
j denotes
the complementary event that j did not occur, Bonferroni’s inequality may be written as
P[12 · · · p] ≥1 −
p
∑
j=1
P[c
j ].
(5.50)
Translated into a multiple inference statement, Bonferroni’s inequality says the familywise
probability that any set of events occurs can be bounded below by 1 minus the sum of the
probabilities that each individual event did not occur. Clearly, in the special case where each
P[c
j ] equals the same constant, say, P[c
j ] = 𝛾for all j = 1, … , p, (5.50) simplifies to
P[12 · · · p] ≥1 −p𝛾.
In the context of a set of p confidence intervals, j corresponds to the event that 𝜃j is
covered correctly by the jth interval (j = 1, … , p). So, if the goal is to set the simultane-
ous confidence level among p individual confidence intervals no smaller than 1−𝛼, using
𝛾= 𝛼∕p produces the desired result. That is, if each complementary event – here, failure to
cover 𝜃j – occurs with probability 𝛼∕p, the simultaneous collection of coverage events occurs
with probability at least 1−𝛼, from (5.50). This is called minimal simultaneous coverage.

150
STATISTICAL DATA ANALYTICS
Example 5.5.1 Simultaneous confidence interval on 𝝁and 𝝈2 (Example 5.3.1, contin-
ued). Suppose a random sample is taken from a normal distribution with unknown mean
𝜇and unknown variance 𝜎2: Xi ∼i.i.d. N(𝜇, 𝜎2), i = 1, … , n. Equations (5.17) and (5.21),
respectively, define pointwise 1 −𝛼confidence limits for each parameter. To construct a joint
confidence region for both 𝜇and 𝜎2 with minimal, simultaneous 1 −𝛼confidence, one can
appeal to the Bonferroni inequality.
To do so, recognize that there are p = 2 parameters here for which a simultaneous inference
is desired. Thus, if we set the confidence level for the 𝜇interval from (5.17) to 1 −𝛼
2 and also
set the confidence level for the 𝜎2 interval from (5.21) to 1 −𝛼
2, the respective probabilities
of noncoverage, that is, the c
j events in (5.50) – are each 𝛾= 𝛼∕2. Thus from Bonferroni’s
inequality, the joint coverage probability will meet or exceed 1 −(2)( 𝛼
2) = 1 −𝛼. This trans-
lates to the joint, minimal 1−𝛼statement
P
[
X −t𝛼∕4(n −1) S
√
n
< 𝜇< X + t𝛼∕4(n −1) S
√
n
and
(n −1)S2
𝜒2
𝛼∕4(n −1)
< 𝜎2 <
(n −1)S2
𝜒2
1−𝛼
4
(n −1)
⎤
⎥
⎥⎦
≥1 −𝛼.
(5.51)
To illustrate, recall the myocardial infarction data in Table 4.2. There, the sample mean was
X = 62.8137 (years) and the sample variance was 𝜎2 = 69.5908. A minimal 95% set of joint
confidence limits for both 𝜇and 𝜎2 here requires the critical points t0.05∕4(125) = t0.0125(125)
= 2.2687, 𝜒2
0.05∕4(125) = 𝜒2
0.0125(125) = 163.0876, and 𝜒2
1−0.05
4
(125) = 𝜒2
0.9875(125) = 92.27.
Applied in (5.51), these lead to the joint limits
62.8137 −(2.2687)8.3421
√
126
< 𝜇< 62.8137 + (2.2687)8.3421
√
126
and
(125)(69.5908)
163.0876
< 𝜎2 < (125)(69.5908)
92.2700
or simply 61.1277 < 𝜇< 64.4997 (years) and 53.3385 < 𝜎2 < 94.2760. Notice that the
interval for 𝜇has widened versus that in Example 5.3.1: by extending joint coverage to
both 𝜇and 𝜎2, the Bonferroni correction here has made each individual inference somewhat
less-precise. This is a common trade-off with multiplicity adjustments; a corollary of what
Clarke et al. (2009, Exercise 1.2) call the ‘no-free-lunch theorem.’ (There is even a website:
http://www.no-free-lunch.org/ !)
The following are some caveats:
(a) The solution in (5.51) is not unique, in that allocation of noncoverage can be modified
to suit the user’s needs. If, for example, greater interest existed in covering 𝜇than
𝜎2, the noncoverage allocation could be varied so that 𝛼∕3 was allocated to the 𝜇
interval – that is, 𝛼∕6 in each tail – and 2𝛼∕3 was allocated to the 𝜎2 interval. The
Bonferroni lower bound is then (still) 1 −
(
2𝛼
3 + 𝛼
3
)
= 1 −𝛼.

STATISTICAL INFERENCE
151
(b) As noted in Section 5.3.2, the equal-tail interval (5.21) for 𝜎2 is known to be subopti-
mal. For greater precision in practice, an interval such as that by Tate and Klett (1959)
could be applied instead within the Bonferroni adjustment.
(c) Many options are available to the analyst when manipulating pointwise intervals for 𝜇
and 𝜎2 into a joint confidence region, and the construction here is only a first step. See,
for example, Casella and Berger (2002, Exercise 9.14) for some other alternatives.
◽
Similar considerations apply in the multiple testing scenario. Suppose m > 1 null hypothe-
ses Hoj (j = 1, … , m) are under study. Define each j as the event that Hoj is (correctly) not
rejected when it is true. Then c
j represents the event that Hoj is rejected when it is true, a false
positive (Type I) error on the jth test. P[c
j ] becomes the associated false positive error rate. If
the goal is to set the simultaneous FWER among the m individual tests no larger than 𝛼, using
the corrected pointwise rate 𝛾= 𝛼∕m in Bonferroni’s inequality produces the desired result.
In effect, one replaces 𝛼with 𝛼∕m in the jth critical point used to test Hoj. (Or, one multiplies
each pointwise P-value by m. A useful R function in this regard is p.adjust().)
In general, for both confidence intervals and hypothesis tests, the multiple events j will
be correlated, because they are derived from the same set of data. This does not affect imple-
mentation of the Bonferroni inequality, endowing it an omnibus quality. Coupled with its
simplicity, this has earned the correction wide popularity for controlling multiplicity with
simultaneous confidence levels or FWERs.
The ‘no-free-lunch theorem’ still applies, however. Convergence to the Bonferroni lower
bound in (5.50) can be poor, depending on the underlying correlation structure among the js,
and thus the actual simultaneous confidence level or FWER can become very conservative
(i.e., much larger than 1−𝛼or much smaller than 𝛼, respectively). One cannot determine the
stringency of the resulting inferences without studying the actual probability structure of the
js, which must be done on a case-by-case basis. Further, even if the bound is fairly tight, as
the number of comparisons grows the eventual inferences become more draconian: confidence
limits widen and critical points grow large compared to the single-comparison, pointwise
case (see Exercise 5.35). The price of controlling for multiplicity is generally lower precision
or lower power in the eventual intervals or tests, respectively. With small numbers of com-
parisons, this trade-off is considered worthwhile. In many large-scale knowledge-discovery
investigations, however, hundreds or even thousands of hypothesis tests are performed on a
single set of data. Pushing m this high exceeds the practicality and the design of the Bon-
ferroni adjustment, requiring a different metric for multiplicity correction. One possibility is
discussed in the next section.
Beyond Bonferroni’s inequality, there is a much larger theory and practice of multiple
comparisons and other simultaneous inferences. A full description exceeds the scope of this
section; interested readers should refer to dedicated texts on the topic such as Liu (2010), Hsu
(1996), or Hochberg and Tamhane (1987). In particular, for executing multiple comparisons
in R, see Bretz et al. (2011), including their description of the external multcomp package, or
Dudoit and van der Laan (2008) and their discussion of the external multtest package.
5.5.2
False discovery rate
As mentioned previously, modern knowledge discovery often takes an exploratory data
analysis (EDA) approach where formal, confirmatory inferences on population parameters

152
STATISTICAL DATA ANALYTICS
become a lesser goal, replaced by issues of exploratory feature assessment (Hastie et al.
2009, Section 18.7). In effect, the hypothesis testing paradigm is coopted into a broad
search-and-discover process, prompting the ‘discovery’ terminology popular in this setting.
For example, in genome-wide association studies (GWAS) or some large-scale astronomy
experiments, many different hypotheses may be tested in order to identify veiled threads or
patterns within a large data set. Here, ‘many’ is not just 10 or even 100; it can be in the many
thousands. This sort of large-scale/high-dimensional multiple testing requires a different
multiplicity adjustment criterion than the FWER in Section 5.5.1.
Towards this end, Benjamini and Hochberg (1995) modified the traditional false positive
FWER approach and asked, can one reduce the conservatism of Bonferroni-type corrections in
order to highlight new ‘discoveries’ (rejected, false, null hypotheses), if one is willing to allow
for more false discoveries (rejected, true, null hypotheses)? Their answer became known as
the problem of controlling the false discovery rate (FDR). Core constituents of this approach
share components with those in formal hypothesis testing, but they are deployed in a different
manner.
To establish the notation: suppose a large number, m, of hypotheses Hoj (j = 1, … , m) are
under study. Of these, let mo (an unknown quantity) be true, representing no new discoveries.
Given a decision rule for rejecting each Hoj, calculate the associated P-value, Pj. Let Rm be
the number of null hypotheses that are actually rejected. Of these, let Vm be the number of
rejected null hypotheses for which Hoj was true, that is, the number of false discoveries. (As
they depend on the data, both Rm and Vm are random variables. Obviously, however, Vm is not
observed in practice.)
The ratio Vm∕Rm is the false discovery proportion, described by Seeger (1968) following
on work by Eklund and Seeger (1965). Benjamini and Hochberg (1995) took this notion and
defined their false discovery rate based on the expected proportion
FDR = E
[
Vm
Rm
|||||
Rm > 0
]
P[Rm > 0],
where the expectation is taken with respect to the joint distribution of the data. (Some authors
write FDR in the simpler form E[Vm∕Rm], although they must then explicitly define Vm∕Rm =
0 when Rm = 0.) By contrast, the FWER is simply P[Vm ≥1], and it can be shown that FDR
≤FWER (Clarke et al. 2009, Section 11.4.1).
The Benjamini and Hochberg (henceforth, BH) strategy replaces the more-stringent
FWER with the FDR. The analyst begins by specifying a maximum FDR, denoted by
convention as 𝛼. (The notation can be confusing: 𝛼here is not the overall false positive rate;
rather, it is the maximum proportion of false discoveries one is on average willing to accept.
In practice, values for it can reach up or past 15%, possibly higher, depending on the goals
of the exploratory study.) Next, order the m P-values into P(1) ≤P(1) ≤· · · ≤P(m) and find
K = min
{
j ∈{1, … , m}|||P(j) ≤j𝛼
m
}
.
(5.52)
Assuming the index K exists for a given data set, reject (or ‘discover’) any hypothesis whose
P(j) ≤P(K). Benjamini and Hochberg showed that if the Pjs are independent, the P-value
threshold based on (5.52) controls the FDR in that
FDR ≤mo
m 𝛼≤𝛼.

STATISTICAL INFERENCE
153
Because mo is unknown, so is this upper bound; however, methods exist to estimate mo and/or
the realized FDR; see, for example, Hastie et al. (2009, Alg. 18.3) or Storey (2011).
It is worth noting that the concept of ordering P-values to control multiple error rates
is a time-tested maneuver in simultaneous inference. For example, an approach similar to
(5.52) was also studied by Seeger (1968) and Simes (1986) to improve the Bonferroni inequal-
ity when testing the global null hypothesis that all the Hojs hold. Indeed, when mo = m, the
method coincides with (5.52).
Example 5.5.2 FDR analysis with gene expression data.
In the analysis of genetic
microarray data, a set of m > 1 genes is often studied to determine if there is some change
in the genes between two distinguishing conditions, for example, two different strains of a
microorganism, or cancer patients versus normal controls, and so on (Nguyen et al. 2002). A
two-sample t-test (or other appropriate statistical test) is conducted to compare the jth gene’s
expression between the two conditions and the corresponding P-value, Pj, is recorded. The
number of genes under study often is very large, and the question of identifying differential
expressions among them is usually more exploratory than confirmatory. As a result, control
of the FDR is popular here.
For example, Table 5.4 presents a selection of m = 1000 ordered P(j)-values from a
two-group gene comparison described in Broberg (2003, Table 2), where roughly 10% of the
genes exhibited differential expression. (The full set of P-values is available at http://www
.wiley.com/go/piegorsch/data_analytics.) The table also lists the corresponding BH thresh-
olds from (5.52) at 𝛼= 0.15, and a marker indicating which of the ordered P-values fall
below the threshold.
In the table, the genes with the 12 smallest P-values lie below the BH threshold. These
would be considered indicators of a potentially ‘significant’ difference between the two con-
ditions, and worth further, targeted study.
◽
A variety of alternative error rates have evolved from (or in some cases, preceded) the
FDR, each allowing for different levels of control to suit different application needs; see,
Table 5.4
Selection of ordered P-values, P(j), from gene expression analysis
of m = 1000 genes in Example 5.5.2, with BH thresholds from (5.52).
Ordered P(j)-value
Threshold at 𝛼= 0.15
Above/below threshold
0.00004
0.00015
✓
0.00010
0.00030
✓
0.00012
0.00045
✓
⋮
⋮
⋮
0.00113
0.00150
✓
0.00135
0.00165
✓
0.00171
0.00180
✓
0.00221
0.00195
0.00258
0.00210
⋮
⋮
⋮
0.99874
0.14985
0.99948
0.15000
Markers indicate genes whose P(j)-values fall below (✓) or above ( ) the threshold.

154
STATISTICAL DATA ANALYTICS
for example, Clarke et al. (2009, Section 11.4.1). Among these, a closely related rate is the
positive false discovery rate or pFDR (Storey 2002, 2003):
pFDR = E
[
Vm
Rm
|||||
Rm > 0
]
.
The pFDR measures the proportion of false discoveries that occur on average, presuming
that rejections (‘discoveries’) do in fact occur. It can be shown that FDR ≤pFDR ≤FWER,
and, indeed, as m →∞, E[FDR] ≈E[pFDR] ≈E[Vm]∕E[Rm]. (The latter ratio is known as
the marginal false discovery rate or mFDR.) Thus for testing problems when the number of
hypotheses is extremely large, these discovery rates will be similar.
Since its introduction, numerous extensions of the basic BH algorithm have appeared.
Typically, these either modify the threshold below which each P(j) marks a discovery, or
first estimate mo and then adaptively modify the selection rule using this estimator. Hwang
et al. (2011) reviewed a number of these and gave guidance on their operating character-
istics in different settings. In fact, applications of the false discovery rate and its sequelae
have become widespread in large-scale data analytics. Instructive reviews are available in
Farcomeni (2008), Goeman and Solari (2011), and Storey (2011).
Exercises
5.1
Suppose a single observation is taken from a binomial distribution: Y ∼Bin(n, 𝜋). Find
the Fisher information number (𝜋) from (5.3) for this single observation.
5.2
Take a random sample of normal observations, Xi ∼i.i.d. N(𝜇, 𝜎2), i = 1, … , n. Find
expressions for the MOM estimators of 𝜇and 𝜎2. How do these compare to the MLEs
from Example 5.2.5?
5.3
In Example 5.2.4, use differential calculus to show that X is the LS estimator for 𝜇. Be
sure to verify that the estimator does indeed minimize the LS objective function.
5.4
Verify that the MLEs for the unknown normal mean and variance in Example 5.2.5 do
in fact maximize the likelihood (or the log-likelihood) function.
5.5
Take a random sample of Poisson observations, Xi ∼i.i.d. Poisson(𝜆), i = 1, … , n.
Find the MLE, ̂𝜆, for 𝜆. Be sure to verify that it is a true maximum. Given ̂𝜆, what is
the MLE of 𝜓= log(𝜆)? Why?
5.6
Let Xi ∼i.i.d. fX(x|𝜇, 𝜎2) i = 1, … , n. Assume the underlying p.m.f. or p.d.f. has a
finite population mean E[Xi] = 𝜇and a finite population variance 𝜎2. Show that the
expected value of the sample variance S2 from (3.5) equals 𝜎2, so that S2 is an unbiased
estimator of 𝜎2 (Casella and Berger 2002, Section 5.2). (Hint: Use the computing form
for S2 from (3.6), and recall that for any random variable W with a finite mean and a
finite variance, E(W2) = Var(W) + E2(W).)
5.7
Show that the Studentized ratio in (5.16) satisfies the relationship in (2.41) that defines
a t-distributed random variable. (Hint: Recall the 𝜒2 relationship for S2 introduced in
Example 5.2.6.)
5.8
Consider again the circulatory-disease mortality data from Table 3.4. (The com-
plete data are available at http://www.wiley.com/go/piegorsch/data_analytics.) In

STATISTICAL INFERENCE
155
Example 4.1.3, it was established that the standardized mortality rates (SMRs)
exhibited a clear right skew, for which a (natural) logarithmic transform can bring the
variation closer to normal, as in Exercise 4.7b. Apply the log transform to the SMRs
and calculate a 90% confidence interval on the mean for these transformed data using
(5.17). How would you translate these confidence limits back into the original SMR
scale? Is there any (theoretical) aspect of this reverse transform that has pragmatic
value?
5.9
Return to the husbands’ heights data in Exercise 3.2.
(a) Plot a histogram (with an overlaid kernel density estimate and accompanying rug
plot) and a normal quantile plot. Do the data appear symmetric and normal?
(b) If the data appear roughly normal, calculate a 99% confidence interval for the mean
height in this population using (5.17).
5.10
Verify that by beginning with (5.20), the confidence statement for 𝜎2 in (5.21) results.
How would you modify this to produce a 100(1−𝛼)% one-sided, upper, confidence
limit for 𝜎2?
5.11
Return to the myocardial infarction data in Table 4.2, as seen in Example 5.3.1. Suppose
interest also exists in calculating a 95% confidence interval on the variance 𝜎2.
(a) Calculate an equal-tail area interval as per (5.21).
(b) Assume that there is twice as much interest in the lower limit as there is in the upper
limit, so that tail areas should be allocated asymmetrically. Make this assignment,
determine the form of the resulting confidence interval, and calculate it for these
data.
(c) Refer to Tate and Klett (1959) and explore how to calculate their ‘minimum length’
confidence interval for these data.
5.12
Repeat the calculations in Exercise 5.11 for the following data sets. Set your confidence
level to 1−𝛼= 0.90.
(a) The hazard vulnerability data from Exercise 3.3.
(b) The log-transformed standardized mortality rates from Exercise 5.8.
5.13
Return to the lung function data in Example 5.3.2. Validate the normal sampling
assumptions made on the data as follows.
(a) Plot histograms for both samples. (Be sure to indicate the binning algorithm you
employ.) Overlay kernel density estimates and rug plots. Do the histograms/density
estimates appear symmetric and bell shaped?
(b) Graph side-by-side boxplots. Do the boxes corroborate indications from the
histograms?
(c) Graph normal quantile plots for both samples. Do the plots appear linear?
(d) If you are familiar with the Shapiro–Wilk test (Royston 1982; Shapiro and Wilk
1965) for normality, conduct the test separately for both samples at 𝛼= 0.05. (In
R, use the shapiro.test() function.)

156
STATISTICAL DATA ANALYTICS
5.14
Toraason et al. (2006) reported data on DNA damage in workers’ peripheral blood
lymphocytes after exposure to the solvent 1-bromopropane (CH3CH2CH2Br) at two
independent workplace facilities, labeled as ‘A’ and ‘B.’ The damage was measured
by a laboratory assay, where higher values indicate greater damage to the DNA. The
data among all workers reporting assay results are as follows:
Facility
Assay scores
A
1739, 2408, 2009, 4023, 2760, 4068, 2218, 2603, 3773, 2598, 2911, 3162,
2570, 3794, 2758, 3348, 2129, 3453, 2821, 2536, 3281, 2766, 2619, 2835,
2913, 4892, 3156, 2675, 3802, 3928, 2408, 3190, 3341, 2956, 3557, 5007,
3693, 3051, 3093, 3615
B
2367, 3113, 2500, 3085, 2557, 2182, 2389, 2671, 2816, 3054, 3486, 4175,
2987, 2909, 2909, 2631, 3031, 2825, 2562, 2374
Assay data such as these often skew to the right, for which a (natural) logarithmic
transform brings variation closer to normal. Thus let X1 = log{Facility A scores}, inde-
pendent of X2 = log{Facility B scores}.
(a) Plot histograms for both samples of log-transformed data. (Be sure to indicate the
binning algorithm you employ.) Overlay a kernel density estimate and include a
rug plot. Do the histograms/density estimates appear symmetric and bell shaped?
(b) Graph side-by-side boxplots. Do the boxes corroborate indications from the his-
tograms?
(c) Graph normal quantile plots for both samples. Do the plots appear linear?
(d) If you are familiar with the Shapiro–Wilk test (Royston 1982; Shapiro and Wilk
1965) for normality, conduct the test separately for both samples at 𝛼= 0.05. (In
R, use the shapiro.test() function.)
(e) Assume that the normal sampling assumption for both variables is tenable and find a
90% confidence interval for the difference in mean log-DNA damage scores. Make
no assumptions about the variances 𝜎2
1 and 𝜎2
2.
5.15
Manipulate the pivot in (5.22) to produce an approximate probability statement that
contains the t-based confidence limits for Δ = 𝜇1 −𝜇2 in (5.24).
5.16
The Australian postal service (Australia Post) performs screening inspections to iden-
tify potentially contaminated items and quarantine them. In a study of parcels entering
the system over a 12-month period n = 2 862 399 parcels were inspected. Of these, Y =
7919 were intercepted as having high biosecurity risk (Decrouez and Robinson 2012).
Assume Y ∼Bin(n, 𝜋). Calculate the following confidence intervals for the binomial
parameter 𝜋. Set your confidence level to 1−𝛼= 0.99.
(a) The Agresti–Coull confidence interval.
(b) The Wilson continuity-corrected confidence interval.
(c) The LR confidence interval.

STATISTICAL INFERENCE
157
5.17
In an ecotoxicological experiment with the potential carcinogen Aflatoxicol, rainbow
trout (Oncorhynchus mykiss) embryos were exposed to 0.250 ppm of the compound
and later examined for development of liver tumors. Out of n = 338 exposed trout,
Y = 286 exhibited the tumor (Roy and Kaiser 2013). Assume Y ∼Bin(n, 𝜋), and
calculate a 95% Agresti–Coull confidence interval for 𝜋.
5.18
Altmetric data are used in scientific publishing to determine patterns of scholarly
and social interest in published articles. The altmetric scores – say, number of Twitter
tweets about an article after its appearance – are seen as potential impact metrics. For
instance, Thelwall et al. (2013) reported data on how often a published article receives a
higher altmetric score than adjacent articles appearing immediately before and immedi-
ately after, using a database of published articles in the biomedical sciences. Those that
did were viewed as ‘successes’ in attracting greater altmetric attention than colocated
articles in the same journal. The result is a proportion: Y = {number of successes} over
n = {number of tested articles}. For the following altmetric scores from this database,
assume Y ∼Bin(n, 𝜋) and find a 90% Agresti–Coull confidence interval for 𝜋.
(a) Using Twitter tweets as the altmetric score, Y = 24 315 and n = 42 891.
(b) Using Facebook posts as the altmetric score, Y = 3229 and n = 5612.
(c) Using Google+ posts as the altmetric score, Y = 426 and n = 804.
5.19
Return to the customer purchasing data in Example 5.3.4, where Y = 506 customers
out of n = 1024 made a purchase during a sale. Calculate the following confidence
intervals for the binomial parameter 𝜋and compare them to the Agresti–Coull interval
presented in the example. As there, set your confidence level to 1−𝛼= 0.95.
(a) The LR confidence interval.
(b) The Wilson continuity-corrected confidence interval.
5.20
From a study of respiratory afflictions similar to that in Example 5.3.2, Taussig et al.
(2003) described data on the numbers of lower respiratory tract illnesses observed in the
first 3 years of life among children with asthma. The counts, presented as a frequency
table, are
Number of illnesses:
0
1
2
3
4
5
6
7
≥8
Frequency:
267
175
79
47
17
13
2
1
0
Thus of the n = 601 children studied, 267 reported Yi = 0 illnesses, 175 reported
Yi = 1 illness, 79 reported Yi = 2 illnesses, and so on.
(a) Construct a bar chart of the observed frequencies. What pattern do you see?
(b) Assume Yi ∼i.i.d. Poisson(𝜆), i = 1, … , 601, and calculate a 95% confidence
interval for the mean number of illnesses, 𝜆, in this population of children, via
(5.36).
5.21
Continuing with the ratio of parameters, 𝜃1∕𝜃2, from Example 5.3.5, notice that
𝜃1∕𝜃2 = exp{log(𝜃1∕𝜃2)}. To derive an alternative confidence interval on the ratio,
consider the function h(𝜃1, 𝜃2) = log(𝜃1∕𝜃2) = log(𝜃1) −log(𝜃2) and use the delta

158
STATISTICAL DATA ANALYTICS
method in Section 5.3.5 to approximate Var[h(𝜃1, 𝜃2)] = Var[log(𝜃1∕𝜃2)]. With this,
develop a 100(1−𝛼)% Wald confidence interval for log(𝜃1∕𝜃2). Use the result to
construct another form of 100(1−𝛼)% Wald confidence interval for 𝜃1∕𝜃2.
5.22
If, for a given set of data, you reject a null hypothesis at the 𝛼= 0.05 significance
level, would you also reject at the 𝛼= 0.10 significance level (with the same set of
data)? Why or why not?
5.23
Return to the hazard vulnerability data from Exercise 3.3 and let 𝜇be the population
mean of the listed index values. Assume Xi ∼i.i.d. N(𝜇, 𝜎2), i = 1, … , n. Test Ho:
𝜇= 1 versus Ha: 𝜇≠1 at 𝛼= 0.05.
5.24
Continuing with the log-transformed circulatory-disease mortality data in Exercise
5.8, assume the transformed data represent a random sample from N(𝜇, 𝜎2). Test Ho:
𝜇= 2.3 versus Ha: 𝜇≠2.3 at 𝛼= 0.01.
5.25
Return to the two-sample data from Exercise 5.14 on DNA damage in workers’ periph-
eral blood lymphocytes. Conduct a test for whether any difference exists in mean (log)
scores between the two facilities. Operate at the 10% significance level. Comment on
how your result compares to the inferences from that earlier exercise.
5.26
Recall the husbands’ and wives’ heights data from Exercise 3.12.
(a) As with the husbands’ heights in Exercise 5.9, plot a histogram (overlay a kernel
density estimate) and normal quantile plot for the wives’ heights. Do the data appear
roughly symmetric and normal?
(b) Assess whether heights of married couples are equal in this population. (The
data are paired, so conduct a paired t-test.) The null hypothesis is obviously Ho:
𝜇D = 0, where 𝜇D = 𝜇male −𝜇female. What choice would you make for the
alternative hypothesis Ha? (Why?) Employ it here. Operate at 𝛼= 0.01.
(c) Take a close look at the wives’ heights. Do you see anything odd with the data?
5.27
Return to the Lung Function data in Examples 5.3.2 and 5.4.2. Recall that no assump-
tions were made there concerning equality of the variances.
(a) Conduct a test for homogeneity of variances on the data: that is, test Ho: 𝜎2
1 = 𝜎2
2
versus Ha: 𝜎2
1 ≠𝜎2
2 at 𝛼= 0.01.
(b) If you conclude that the variances are not significantly different, return to
Example 5.3.2 and calculate a 90% confidence interval for the difference in means,
𝜇1 −𝜇2, using (5.27). Compare the result to that seen in the exercise.
(c) If you conclude that the variances are not significantly different, return to
Example 5.4.2 and perform a test of equality between the means using (5.44). As
there, operate at 𝛼= 0.10. Compare the result to that seen in the example.
5.28
Return to the two-sample DNA damage data from Exercises 5.14 and 5.25. Recall that
no assumptions were made there concerning equality of the variances.
(a) Conduct a test for homogeneity of variances on the log-transformed data: that is,
test Ho: 𝜎2
1 = 𝜎2
2 versus Ha: 𝜎2
1 ≠𝜎2
2 at 𝛼= 0.01.

STATISTICAL INFERENCE
159
(b) If you conclude that the variances are not significantly different, return to Exercise
5.14 and calculate a 90% confidence interval for the mean (log) difference, Δ, using
(5.27). Compare the result to that seen in the example.
(c) If you conclude that the variances are not significantly different, return to Exercise
5.25 and perform a test of equality between the means using (5.44). As there, oper-
ate at a 10% significance level. Compare the result to that seen in the exercise.
5.29
Two-sample hypothesis tests are often employed in online marketing, via what is
referred to as A–B testing. The approach can be applied in a number of ways. Suppose
that a web site designer tests whether a new site layout (variant ‘B’) attracts more
customers, compared to the current site (the ‘control’ variant ‘A’). A volunteer is ran-
domly shown either variant A or variant B and the amount of time spent (in seconds)
viewing the site is recorded. The same volunteer is then shown the other variant and
the amount of time spent on that site is recorded. This is repeated for n = 500 vol-
unteers. The data are matched pairs (XAj, XBj), j = 1, … , 500, producing differences
Dj = XAj −XBj for a paired t-test, as in (5.45). Suppose this experiment produced
a mean difference of D = 12.6306 −17.8446 = −5.2140 s, with sample standard
deviation SD = 4.2749 s. Test whether the new (variant B) web site held volunteers’
attentions significantly longer than the control (variant A). Operate at 𝛼= 0.01.
5.30
Let Y ∼Bin(n, 𝜋). Construct an LR test for Ho: 𝜋= 𝜋o versus Ha: 𝜋≠𝜋o. Use this to
test 𝜋= 1
2 in Example 5.4.4 and compare the result to the conclusions drawn there.
5.31
Let Y ∼Bin(n, 𝜋). Suppose n > 40. How would you modify the recommended 1−𝛼
Agresti–Coull confidence interval for 𝜋into a level-𝛼test of Ho: 𝜋= 𝜋o versus Ha:
𝜋≠𝜋o?
5.32
Recall the postal quarantine study described in Exercise 5.16, where n = 2 862 399
parcels were inspected in a 12-month period, of which Y = 7919 had high biosecurity
risk. Conduct a binomial exact test to determine if the true probability of intercepting
a high-risk parcel is below 1%. Operate at 𝛼= 0.05. What do you determine?
5.33
Follow on the suggestion in Example 5.5.1 about unequal Bonferroni allocations and
modify the adjustment with the myocardial infarction data from Table 4.2. Allocate
𝛼∕3 noncoverage probability to the 𝜇interval and 2𝛼∕3 noncoverage probability to the
𝜎2 interval. For simplicity, continue to use the equal-tail interval from (5.21) for 𝜎2.
How do the results compare with those from the example?
5.34
The ecotoxicological data in Exercise 5.17 were part of a larger experiment on rain-
bow trout carcinogenesis. The full study employed five different exposures (in ppm) of
aflatoxicol. At each exposure, xj, the number of trout with the tumor, Yj, was recorded,
out of nj fish exposed (j = 1, … , 5). From Roy and Kaiser (2013), the data are
Dose, xj
0.010
0.025
0.050
0.100
0.250
Tumor-bearing trout, Yj
25
132
226
281
286
Number exposed, nj
347
346
353
355
338

160
STATISTICAL DATA ANALYTICS
Assume Yj ∼indep. Bin(n, 𝜋j), j = 1, … , 5, and calculate an Agresti–Coull confi-
dence interval for each 𝜋j. As the data come from the same experiment, adjust each
pointwise confidence interval for multiplicity via a Bonferroni correction so that the
overall simultaneous confidence level is no smaller than 1−𝛼= 0.95. What patterns
emerge?
5.35
Consider testing a multiple series of hypotheses Hoj versus Haj, j = 1, … , m. Assume
that for a single comparison, rejection occurs when a test statistic Tj exceeds the point-
wise critical point t𝛼∕2(𝜈) from a t(𝜈) reference distribution. Apply a Bonferroni cor-
rection to the critical point and examine its progression as m changes for the following
cases. Set 𝛼= 0.05. (Notice that, as described here, rejection becomes more demanding
as the corrected critical point grows.)
(a) Set 𝜈= 4 and progress m = 1, 5, 10, 50, 100, 500.
(b) Set 𝜈= 12 and progress m = 1, 5, 10, 50, 100, 500.
(c) Set 𝜈= 25 and progress m = 1, 5, 10, 50, 100, 500.
5.36
For the gene-expression microarray study in Example 5.5.2, data were also
generated from m = 1000 genes where roughly 5% of the genes exhibited
differential
expression.
The
corresponding
P-values
are
available
online
at
http://www.wiley.com/go/piegorsch/data_analytics; a sample follows:
Ordered P(j):
0.00007
0.00009
0.00039
· · ·
0.99807
0.99834
Perform a multiple testing examination of these P-values by applying the Benjamini
and Hochberg (1995) FDR correction, at 𝛼= 0.15. Which genes’ P-values indicate a
potential discovery?

Part II
STATISTICAL LEARNING
AND DATA ANALYTICS


6
Techniques for supervised
learning: simple linear regression
The specialized statistical learning techniques required for large-scale data analytics separate
into one of two basic genera: supervised or unsupervised. Supervised learning techniques will
be described in this and the following three chapters; the current chapter focuses on one of
the simplest forms of supervised learning, linear regression analysis. Unsupervised methods
are described in Chapters 10 and 11.
6.1
What is “supervised learning?”
The term “supervised learning” is rooted in statistical learning/machine learning parlance,
where it describes the analysis of data via a focused structure – sometimes called “learn-
ing with a teacher” (Kantardzic 2003, Section 4.3). The observations may belong to a set
of training data, used to identify a model that relates the inputs, usually a vector or matrix of
predictor/feature variables X, to an output response variable Y. The “teacher” is an optimal-
ity or fitness criterion – such as least squares minimization or likelihood maximization – that
guides selection of the final model. Learning occurs when the input predictors are processed
through the model to discover unknown dependencies between X and Y.
Two basic species of supervised statistical learning are regression analysis, described in
this and the next two chapters, and classification analysis, described in Chapter 9. Both are
predictive in nature: they combine input variables with the eventual model to forecast or clas-
sify future realizations of the outcome variable. The next section begins with the simplest form
of predictive model, simple linear regression (SLR), where a single quantitative variable, x,
is used to describe a single outcome variable, Y.
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

164
STATISTICAL DATA ANALYTICS
6.2
Simple linear regression
6.2.1
The simple linear model
The SLR paradigm involves a single predictor variable, x, also called an input variable or
feature variable, used to describe an output or response variable, Y, via a simple linear model.
Formally, assume data are collected in matched pairs (xi, Yi), i = 1, … , n, where the response
variable is modeled as
Yi ∼indep. N(𝜇(xi), 𝜎2) ,
(6.1)
and where the predictor xi is fixed and nonstochastic. The simple linear model specifies a
linear relationship between E[Yi] = 𝜇(xi) and xi:
𝜇(xi) = 𝛽0 + 𝛽1xi ,
(6.2)
where 𝛽0 and 𝛽1 are regression coefficients representing the Y-intercept and slope, respectively,
of the mean response 𝜇(xi).
The regression coefficients are assumed unknown and must be estimated from the data.
Most analysts employ the method of least squares (LS; see Section 5.2.3): construct the objec-
tive quantity = ∑n
i=1 {Yi −(𝛽0 + 𝛽1xi)}2 and minimize with respect to both 𝛽0 and 𝛽1.
This produces a system of two equations with two unknowns:
n
∑
i=1
Yi = n𝛽0 + 𝛽1
n
∑
i=1
xi
n
∑
i=1
xiYi = 𝛽0
n
∑
i=1
xi + 𝛽1
n
∑
i=1
x2
i .
(6.3)
The equations in (6.3) are known as the normal equations for the SLR model. The solution to
this system of equations produces the unique LS estimators (Exercise 6.1)
̂𝛽1 =
∑n
i=1(xi −x)Yi
∑n
i=1 (xi −x)2 and
̂𝛽0 = Y −̂𝛽1x .
(6.4)
It can be shown that the maximum likelihood estimators (MLEs; see Section 5.2.4) for 𝛽0 and
𝛽1 are identical to these LS estimators. See Exercise 6.1.
To estimate the mean response in (6.2), simply apply the LS estimators for the regression
coefficients to the model equation. This gives
̂Yi = ̂𝛽0 + ̂𝛽0xi ,
(6.5)
which are called the fitted values from the SLR. More generally, the estimated mean response
at any predictor level x is ̂𝜇(x) = ̂𝛽0 + ̂𝛽0x.
Example 6.2.1 UK cancer mortality and local employment. To study national indi-
cators of public health, fiscal, and societal trends, the United Kingdom collects data on
mortality and other associated socioeconomic factors. For example, in a study similar to
that in Example 3.5.1, cancer mortality rates among persons younger than 75 years of

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
165
Table 6.1
Selected data pairs (xi, Yi) with x = {Employment rate} (as average quarterly
percentage of adults over 16 gainfully employed) and Y = {Cancer log-mortality rates per
100 000 population} (3-year running average), from a larger set of n = 342 paired
observations recorded throughout the United Kingdom in 2008.
(90.3, 3.85)
(87.7, 4.45)
(87.6, 4.68)
· · ·
(61.3, 5.05)
(60.7, 4.94)
(58.6, 4.72)
Source: http://data.gov.uk/dataset/ni_151_-_employment_rate.
age (per 100 000 population; averaged over the previous 3 years) were determined for
n = 342 locations – specifically, principal local and regional authorities known as “local
councils” – throughout the United Kingdom in 2008. Also recorded was each locality’s
employment rate (as average quarterly % of persons over 16 years of age in gainful employ-
ment). The employment rate is used here as a surrogate measure for the health of the local
economy.
Study of a possible connection between cancer mortality and a locality’s economic health
is, perhaps, provocative: does higher employment bring lower cancer mortality, on average, to
a community? (And if so, what is the latent connection? Are the citizens more active at work,
better able to access quality health care, and/or more proactive about disease screenings? If a
significant effect were identified, the potential for further knowledge discovery is intriguing.)
To study this possibility, consider an SLR between the two variables via the model in (6.1).
View x = {Employment rate} as the predictor and mortality as the target response. As seen,
for example, in Exercise 3.17, mortality rates often exhibit a right skew, so operate here with
a logarithmic transform of the raw rates: Y = log{Mortality}.
The data values, as (xi, Yi) pairs, appear in Table 6.1. (As above, only a selection of the data
is given in the table. The complete set is available at http://www.wiley.com/go/piegorsch/data_
analytics.)
A first step for any regression analysis is to plot the data. Figure 6.1 presents a scatter-
plot of the (xi, Yi) pairs, with the estimated LS line, ̂𝜇(x) = ̂𝛽0 + ̂𝛽1x (derived later), overlaid.
The plot shows a generally broad scatter; however, as employment increases, the speculated
decrease in log-mortality appears clear. (A number of other interesting features are evident in
the scatterplot; these are explored throughout the remainder of this and the next chapter.)
To quantify the potential effects seen in Figure 6.1, we can calculate LS estimators of
the intercept and slope under the SLR model. For the slope, the data give ∑n
i=1(xi −x)Yi =
−153.2633 and ∑n
i=1 (xi −x)2 = 10 143.4665. Thus
̂𝛽1 = −153.2633
10 143.4665 = −0.0151.
As anticipated, the estimated slope is negative: an increase in employment of 1% relates on
average to a change in log-mortality of −0.0151 units. This translates to e−0.0151 = 0.9850 or
about 1.5% fewer cancer deaths.
We also find Y = 4.6884 and x = 76.3968. Thus the estimated intercept is ̂𝛽0 = 4.6884 −
(−0.0151)(76.3968) = 5.8428.
Statistical software packages readily calculate these quantities. For instance, in R,
the lm() function is used to perform linear regression. The syntax takes the response
variable, say, Y, and regresses against the predictor variable, x, via the call lm(Y˜x). For
Y = log{Mortality} and x = {Employment rate}, this produces the simple output

166
STATISTICAL DATA ANALYTICS
60
65
70
75
80
85
90
4.0
4.2
4.4
4.6
4.8
5.0
Employment(%)
log(Mortality)
Figure 6.1
Scatterplot for UK cancer mortality data in Example 6.2.1. Least squares
regression line ̂𝜇(x) is overlaid. Source: Data from http://data.gov.uk/dataset/ni_151_-_
employment_rate.
Call:
lm( formula = Y ˜ x )
Coefficients:
(Intercept)
x
5.8427513
-0.0151096
The LS estimates are given in the final row: ̂𝛽0 under (Intercept) and ̂𝛽1 under x (for the
predictor variable named in the call to lm()).
◽
The LS estimators in (6.4) possess a number of important qualities. One can show
that E[ ̂𝛽j] = 𝛽j for j = 0, 1; hence, the estimators are unbiased. Further, their sampling
variances are
Var[ ̂𝛽0] = 𝜎2
{
1
n +
x2
∑n
i=1 (xi −x)2
}
, and
Var[ ̂𝛽1] =
𝜎2
∑n
i=1 (xi −x)2
(6.6)
(see Exercise 6.2).
A famous result in linear model theory known as the Gauss–Markov theorem (Christensen
2011, Section 2.3) relates that the variances in (6.6) are the minimum possible among all
unbiased estimators for 𝛽0 and 𝛽1. Thus the ̂𝛽js are known as “best linear unbiased estimators”
(BLUEs).
The complete sampling distribution for each ̂𝛽j also follows from the SLR model in (6.1):
̂𝛽j ∼N(𝛽j, Var[ ̂𝛽j]) , j = 0, 1,
(6.7)

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
167
where Var[ ̂𝛽j] is given in (6.6). The covariance between the two estimators is Cov[ ̂𝛽0, ̂𝛽1] =
−𝜎2x∕∑n
i=1 (xi −x)2.
Using (6.7), one finds ̂𝜇(x) ∼N(𝛽0 + 𝛽1x, Var[ ̂𝜇(x)]) for any x, where
Var[ ̂𝜇(x)] = 𝜎2
{
1
n +
(x −x)2
∑n
i=1 (xi −x)2
}
(6.8)
(again, see Exercise 6.2).
The variance parameter, 𝜎2, in (6.1) is usually unknown. It can be estimated by imi-
tating the strategy employed with the simple sample variance S2 in Section 3.3.2: sum the
squared deviations between the observed values, Yi, and the estimated response under the
posited model. Then, divide by the number of degrees of freedom (d.f.) in the resulting sum
of squares. Under the SLR model, the estimated responses are the fitted values ̂Yi, thus the
pertinent deviations are
ei = Yi −̂Yi
(6.9)
(i = 1, … , n), better known as the residuals from the model fit. Summing the squared resid-
uals produces the aptly named residual sum of squares, also called the error sum of squares
(SSE)
SSE =
n
∑
i=1
e2
i =
n
∑
i=1
(Yi −̂Yi)2.
(6.10)
The SSE here has n−2 d.f. (called error degrees of freedom). In effect, this is the number of
observations minus the number of parameters estimated from 𝜇(x). Dividing the SSE by its
d.f. produces a mean squared error (MSE):
MSE =
∑n
i=1 e2
i
n −2
=
∑n
i=1 (Yi −̂Yi)2
n −2
.
Just as the similarly constructed sample variance S2 estimates the population variance in
a single random sample, the MSE here estimates the population variance for the SLR model.
Importantly, the MSE is an unbiased estimator of 𝜎2; that is, E[MSE] = 𝜎2. This follows from
the larger fact that the MSE has its own sampling distribution:
(n −2)MSE
𝜎2
∼𝜒2(n −2),
(6.11)
where under (6.1) both ̂𝛽0 and ̂𝛽1 are independent of this MSE-based 𝜒2 variable (Casella and
Berger 2002, Section 11.3.4).
The residuals, ei, in (6.9) are useful for more than just constructing the SSE. In particular,
they help in assessing quality of the model fit and in other regression diagnostics. This is
discussed further in Section 6.3.
For quantifying the uncertainty associated with estimation of these various regression
quantities, and for conducting inferences on them, we calculate the estimators’ standard errors.
Recall from (5.7) that the standard error of a point estimator is simply the square root of its
estimated sampling variance. For example, se[ ̂𝛽1] is the square root of Var[ ̂𝛽1] in (6.6), after
inserting an appropriate estimate for the unknown variance parameter 𝜎2. In the latter instance,

168
STATISTICAL DATA ANALYTICS
we use the unbiased estimator MSE, producing
se[ ̂𝛽1] =
√
MSE
∑n
i=1 (xi −x)2 .
(6.12)
Now, from (6.7), ̂𝛽1 is normally distributed so that Z = ( ̂𝛽1 −𝛽1)∕√Var[ ̂𝛽1] ∼N(0, 1).
Further, this Z is independent of the scaled MSE in (6.11). Incorporating the standard error in
(6.12), one can build from these a t-distributed ratio in a similar manner to (5.16):
̂𝛽1 −𝛽1
se[ ̂𝛽1]
∼t(n −2)
(6.13)
(see Exercise 6.5). Using (6.13), confidence regions and hypothesis tests for 𝛽1 are readily
constructed. For instance, similar to the single-sample interval in (5.17), a 1 −𝛼confidence
interval for 𝛽1 is based on
P { ̂𝛽1 −t𝛼∕2(n −2) se[ ̂𝛽1] < 𝛽1 < ̂𝛽1 + t𝛼∕2(n −2) se[ ̂𝛽1]} = 1 −𝛼,
(6.14)
that is, ̂𝛽1 ± t𝛼∕2(n −2) se[ ̂𝛽1], which mimics the popular “Wald” form from (5.31). (Again,
see Exercise 6.5.)
Hypothesis tests on 𝛽1 are similarly developed. Notice in particular that when 𝛽1 = 0,
the mean response in (6.2) simplifies to just 𝜇(x) = 𝛽0, that is, a constant that does not depend
upon x. Thus, many analysts focus attention on testing whether or not 𝛽1 = 0, that is, Ho: 𝛽1 = 0
versus Ha: 𝛽1 ≠0. Again using the t(n −2) relationship in (6.13), a corresponding t-statistic
can be constructed by setting 𝛽1 to its null value of 0:
T1 =
̂𝛽1
se[ ̂𝛽1]
.
(6.15)
When Ho: 𝛽1 = 0 is true, the null reference distribution for (6.15) is T1 ∼t(n −2). Reject
the null hypothesis in favor of the two-sided alternative in Ha when |T1| ≥t𝛼∕2(n −2). The
corresponding, two-sided P-value is
P = 2P[t(n −2) ≥|t1calc|],
where t1calc is the observed value of the test statistic in (6.15). For one-sided tests of Ho: 𝛽1 = 0
against, say, Ha: 𝛽1 > 0, reject Ho in favor of Ha when T1 ≥t𝛼(n −2). The corresponding,
one-sided P-value is then P = P[t(n −2) ≥t1calc]. One-sided tests against Ha: 𝛽1 < 0 are sim-
ilar. (Recall that the rationale for performing a one-sided test must be made prior to examining
the data.)
In R, these various quantities are built into the lm object from the call to lm(Y˜x). For
instance, coef(lm(Y ˜ x)) lists the LS estimates, confint(lm(Y ˜ x)) gives 1 −𝛼con-
fidence intervals, fitted(lm(Y ˜ x)) calculates fitted values, while summary(lm(Y ˜
x)) provides a detailed output with t-statistics, standard errors, and P-values. Of course, users
may also calculate any of these quantities directly in R by appeal to the program’s base arith-
metic functions.
Example 6.2.2 UK cancer mortality and local employment (Example 6.2.1, continued).
Return to the UK cancer mortality data in Table 6.1, where Y = log{Mortality} due to cancer
was regressed on x = {Employment rate}, via the SLR model in (6.1) and (6.2). The LS point

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
169
estimates were found to be ̂𝛽0 = 5.8428 and ̂𝛽1 = −0.0151. These are used to calculate the
fitted values ̂Yi = 5.8428 −0.0151xi, leading to an MSE of
MSE =
∑n
i=1 (Yi −̂Yi)2
n −2
= 6.1428
340
= 0.0181.
Recall also that ∑n
i=1 (xi −x)2 = 10 143.4665.
The parameter of primary interest here is the slope, 𝛽1. To construct its 95% confi-
dence interval, ̂𝛽1 ± t𝛼∕2(n −2) se[ ̂𝛽1], condition the analysis on the observed pattern of
employment rates and start by calculating the standard error se[ ̂𝛽1]. From (6.12), this
is
√
0.0181∕10 143.4665 = 1.336 × 10−3. With t0.025(340) = 1.9670, the 95% interval
calculates to
−0.0151 ± (1.9670)(1.336 × 10−3) = −0.0151 ± 0.0026
or −0.0177 < 𝛽1 < −0.0125. In R, find this as the lower row in the output from the
confint() function:
> confint( lm(Y ˜ x) )
2.5 %
97.5 %
(Intercept)
5.6416923
6.0438103
x
-0.0177347
-0.0124844
One might alternatively perform an hypothesis test here, based on the original speculation
that increasing employment may lead to decreases in cancer mortality. As phrased, this is a
valid a priori rationale for testing Ho: 𝛽1 = 0 versus the one-sided alternative Ha: 𝛽1 < 0.
Set the significance level to 𝛼= 0.05. The test statistic from (6.15) is
t1calc =
̂𝛽1
se[ ̂𝛽1]
=
−0.0151
1.336 × 10−3 = −11.3024.
(Using the higher precision available in, e.g., R, a more-accurate value is t1calc = −11.3214;
see the following output.) The consequent, one-sided P-value is P[t(340) < −11.3214] which
is available in R via pt(-11.3214,df=340). This gives P ≈10−25, which is well below 𝛼.
We therefore reject Ho and conclude, perhaps intriguingly, that log-mortality decreases sig-
nificantly as weekly earnings increase for these UK communities. Further study to understand
what might underly this feature may be warranted.
These various statistics appear throughout the R output (edited) from a call to
summary(lm(Y ˜ x)):
Call:
lm(formula = Y ˜ x)
Coefficients:
Estimate
Std. Error
t value
Pr(>|t|)
(Intercept)
5.8427513
0.1022178
57.1598
< 2.22e-16
x
-0.0151096
0.0013346
-11.3214
< 2.22e-16
Residual standard error: 0.134414 on 340 degrees of freedom
Multiple R-squared:
0.27378
The P-values are given in the final column of the central output, under the header Pr(>|t|).
Notice that the P-value for testing Ho: 𝛽1 = 0 is two sided, as indicated by the absolute-value

170
STATISTICAL DATA ANALYTICS
signs in the header. Dividing by 2 here would recover the one-sided P-value, although, in
either case, the value is so significant that the result is best reported as simply P < 0.0001. ◽
Any of these various operations may also be applied for building inferences on 𝛽0 or on
𝜇(x). For instance, from (6.6), the standard error of ̂𝛽0 is
se[ ̂𝛽0] =
√
MSE
{
1
n +
x2
∑n
i=1 (xi −x)2
}
.
(6.16)
Use this in the 1 −𝛼confidence interval ̂𝛽0 ± t𝛼∕2(n −2) se[ ̂𝛽0]. (Hypothesis tests for 𝛽0 are
similarly developed.) Or, from (6.8), the standard error of ̂𝜇(x) = ̂𝛽0 + ̂𝛽1x is
se[ ̂𝜇(x)] =
√
MSE
{
1
n +
(x −x)2
∑n
i=1 (xi −x)2
}
,
(6.17)
with corresponding 1 −𝛼confidence interval at any (pointwise) value of x given by
̂𝜇(x) ± t𝛼∕2(n −2) se[ ̂𝜇(x)].
(6.18)
Prediction of a future observation, say, ̂Y(x) at a given x is also possible; the point estimate is
again ̂Y(x) = ̂𝛽0 + ̂𝛽1x, but with a correspondingly more complex standard error se[ ̂Y(x)] that
takes into account the prediction feature. See Kutner et al. (2005, Section 2.5) for full details.
A useful summary statistic in regression analysis compares the residual variation with the
total variation in the sample. Recognize that if no regression model were being fit to the Yis,
the sum of squares used to measure variability would be the numerator of the sample variance:
∑n
i=1 (Yi −Y)2. Denote this as the total sum of squares, or SSTo. The analogous quantity under
the SLR model is the SSE from (6.10). Their difference, SSTo −SSE, represents variation in
Yi that is accounted for by the regression; thus the ratio
R2 = SSTo −SSE
SSTo
= 1 −SSE
SSTo .
(6.19)
can be viewed as the percentage variation in Yi accounted for by variation in the xis. (Notice
that 0 ≤R2 ≤1.) This quantity is known as the coefficient of determination. Its intuitive inter-
pretation makes it an often-used (and sometimes, over-used) summary for the value of the
SLR. Numerically, R2 can be shown to equal the squared sample correlation between xi and
Yi from (3.9), motivating use of the R symbol.
It is worth noting that the interpretive quality of the R2 statistic varies from setting to
setting, and will be domain dependent. A linear regression producing R2 near 80% might
indicate a quality fit for an ecological data set; by contrast, the same R2 = 0.80 could border
on substandard for a chemometric regression analyses. “How high is high?” for R2 depends
on the nature of error variation seen in each subject-matter application.
Example 6.2.3 Cancer mortality and local employment (Example 6.2.1, continued).
Return to the UK cancer mortality data in Table 6.1, where Y = log{Mortality} due to cancer
was regressed on x = {Employment rate}, via the SLR model in (6.1) and (6.2). Recall that
the error sum of squares for this model was SSE = 6.1428. As Y was seen to be 4.6884, the

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
171
total sum of squares can be calculated as SSTo = ∑342
i=1 (Yi −4.6884)2 = 8.4586. This gives
R2 = 1 −6.1428
8.4586 = 0.2738
or 27.38%. This is a low value, suggesting that variability in the data remains substantial even
after incorporating the effects of xi. (Small R2 values are not altogether unusual with some
biomedical or, for that matter, socioeconomic studies in humans. Simply put, people tend to be
highly variable in their social, economic, and biological responses.) Recall, however, that the
slope coefficient was seen in Example 6.2.2 to be highly significant: decreasing employment
rates appear to significantly affect cancer (log-)mortality in these communities, although the
large amount of unexplained variation suggests that additional factors may also be at play.
(See Example 7.1.1.)
R2 is routinely displayed by most regression software programs. For example, R pro-
vides it near the bottom of the display from the summary() command (cf. the output in
Example 6.2.2).
◽
6.2.2
Multiple inferences and simultaneous confidence bands
Recall from Section 5.5 that adjustments for multiplicity are prescribed whenever more than
one inference is performed on a single set of data. This issue is a pertinent one in regression
analysis, because there are often many different parameters or model features under study. For
instance, analysts may wish to perform joint inferences on both of the unknown regression
parameters, 𝛽0 and 𝛽1. The various tests and confidence intervals presented in the previous
section were all constructed to be pointwise in nature, however. If multiple inferences are
conducted on the same data, some correction for multiplicity is in order.
Perhaps the simplest way to form multiplicity-adjusted, joint confidence intervals for both
𝛽0 and 𝛽1 is to apply the Bonferroni correction from Section 5.5.1. The approach mimics that
in Example 5.5.1, where joint intervals were constructed for the two unknown parameters,
𝜇and 𝜎2, from an independent, identically distributed (i.i.d.) normal sample. With the SLR
model in (6.1), we again have two unknown parameters, 𝛽0 and 𝛽1, so we apply the Bonferroni
correction by simply replacing 𝛼with 𝛼∕2 wherever it appears in the two sets of confidence
limits. This produces the joint (minimal) 1 −𝛼confidence rectangle
̂𝛽0 ± t𝛼∕4(n −2) se[ ̂𝛽0], and
̂𝛽1 ± t𝛼∕4(n −2) se[ ̂𝛽1].
(6.20)
It is possible to develop a less-conservative, two-dimensional, joint confidence region for
𝛽0 and 𝛽1 by extending the Wald region in (5.49). The resulting 1 −𝛼region takes the form
of an ellipse, defined by the inequality
se2[ ̂𝛽1](𝛽0 −̂𝛽0)2 −2C01(𝛽0 −̂𝛽0)(𝛽1 −̂𝛽1) + se2[ ̂𝛽0](𝛽1 −̂𝛽1)2
se2[ ̂𝛽0]se2[ ̂𝛽1] −C2
01
≤2F𝛼(2, n −2),
(6.21)
where the standard errors se[ ̂𝛽0] and se[ ̂𝛽1] are given in (6.16) and (6.12), respectively, and
C01 is the estimated covariance between ̂𝛽0 and ̂𝛽1:
C01 = ̂
Cov[ ̂𝛽0, ̂𝛽1] =
−x MSE
∑n
i=1 (xi −x)2

172
STATISTICAL DATA ANALYTICS
(cf. Exericse 6.2). The estimated variance–covariance terms may be accessed in R by applying
the vcov() function to the lm object from the model fit. This produces the full 2 × 2 estimated
covariance matrix for ̂𝛽0 and ̂𝛽1; cf. (5.6).
Although more complex in both its construction and its implementation, the joint confi-
dence ellipse in (6.21) often provides far better precision for joint inferences on the regression
coefficients than the conservative Bonferroni rectangle.
Example 6.2.4 Cancer mortality and local employment (Example 6.2.1, continued).
Return to the UK cancer mortality data in Table 6.1, where Y = log{Mortality} due to
cancer was regressed on x = {Employment rate}, via the SLR model in (6.1) and (6.2). As
𝛽0 represents the log-mortality when a community’s employment rate is zero – an arguably
undesirable level – inferences on the intercept may be of limited socioeconomic interest here.
Nonetheless, construction of a joint confidence region for both 𝛽0 and 𝛽1 is instructive.
Begin by setting the confidence level to 1 −𝛼= 0.95. The various quantities required for
the calculation of both the Bonferroni rectangle in (6.20) and the confidence ellipse in (6.21)
were previously calculated or displayed in Examples 6.2.1 and 6.2.2, save for the estimated
covariance C01. Direct calculation gives
C01 =
−x MSE
∑n
i=1 (xi −x)2 = −(76.3968)(0.0181)
10 143.4665
= −1.361 × 10−4.
Alternatively, one can apply R’s vcov() function:
> vcov( lm(Y ˜ x) )
(Intercept)
x
(Intercept)
0.010448485
-1.36075e-04
x
-0.000136075
1.78116e-06
Notice that this gives a correlation between ̂𝛽0 and ̂𝛽1 close to its lower limit of −1:
Corr[ ̂𝛽0, ̂𝛽1] =
C01
se[ ̂𝛽0]se[ ̂𝛽1]
=
−1.361 × 10−4
√
1.045 × 10−2√
1.781 × 10−6 = −0.9975.
For a twofold multiplicity adjustment, the (minimal) 95% Bonferroni limits require
t0.05∕4(340) = 2.2514, leading to the joint confidence rectangle
5.8428 −(2.2514)(0.1022) = 5.6127 < 𝛽0
< 6.0729 = 5.8428 + (2.2514)(0.1022),
−0.0151 −(2.2514)(0.0013) = −0.0181 < 𝛽1
< −0.0121 = −0.0151 + (2.2514)(0.0013).
The alternative, joint 95% confidence ellipse requires F0.05(2, 340) = 3.0223. The ellipse
bounds a region containing all values of 𝛽0 and 𝛽1 that satisfy
(1.78 × 10−6)(𝛽0 −5.8428)2
(0.0104)(1.78 × 10−6) −(−0.000136)2
−(2)(−0.000136)(𝛽0 −5.8428)(𝛽1 + 0.0151)
(0.0104)(1.78 × 10−6) −(−0.000136)2
+
(0.0104)(𝛽1 + 0.0151)2
(0.0104)(1.78 × 10−6) −(−0.000136)2 ≤(2)(3.0223).

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
173
5.5
5.6
5.7
5.8
5.9
6.0
6.1
6.2
−0.020
−0.018
−0.016
−0.014
−0.012
−0.010
β0
β1
Figure 6.2
95% joint confidence regions for (𝛽0, 𝛽1) with UK cancer mortality data in
Example 6.2.4: joint confidence ellipse from (6.21) (solid curve) and joint Bonferroni rectan-
gle from (6.20) (dashed lines). Least squares point estimate marked by a cross (+). Source:
Data from http://data.gov.uk/dataset/ni_151_-_employment_rate.
Figure 6.2 overlays the two regions in (𝛽0, 𝛽1) space. The especially strong correlation
Corr[ ̂𝛽0, ̂𝛽1] approaching −1 induces heavy eccentricity (narrow elongation) in the confidence
ellipse. By contrast, the Bonferroni rectangle remains relatively wide, illustrating its extreme
conservatism with these data.
Construction of the confidence ellipse in R is facilitated by the use of the ellipse()
function in the external ellipse package.
◽
Issues of multiplicity adjustment become even more complex when considering inferences
on the mean response in (6.2). While estimation of 𝜇(x) via ̂𝜇(x) at any x is straightforward,
construction of, say, confidence limits on 𝜇(x) requires more care. The pointwise limits in
(6.18) are valid for any value of x, but for only a single such value. If inferences are desired
at more than one x, correction for multiplicity is required.
The simplest approach for making such corrections is to again apply a Bonferroni correc-
tion from Section 5.5.1. Suppose interest centers on joint 1 −𝛼confidence bounds for 𝜇(x)
at an a priori set of H > 1 predictor values xh, h = 1, … , H. (These can include any of the
original predictors, xi, or an entirely new set of values, or any combination thereof.) Then, the
Bonferroni adjustment simply modifies the pointwise limits in (6.18) by dividing H into 𝛼at
every xh:
̂𝜇(xh) ± t𝛼∕(2H)(n −2) se[ ̂𝜇(xh)]
(6.22)
(h = 1, … , H), where se[ ̂𝜇(xh)] is given by (6.17). It is straightforward to verify that for fixed
𝛼, t𝛼∕(2H)(n −2) grows as H increases, forcing the joint intervals to widen. This again rein-
forces the potential conservatism of the Bonferroni bounds.
A useful alternative to the Bonferroni adjustment for multiple inferences on 𝜇(x) applies
a method for constructing confidence bounds at every value of x on the real line. Owing to
Working and Hotelling (1929) and Scheffé (1953), the approach creates a 1 −𝛼simultane-
ous confidence band around the line represented by 𝜇(x). For purposes of calculation, the

174
STATISTICAL DATA ANALYTICS
Working–Hotelling–Scheffé (WHS) bands are almost identical to the Bonferroni confidence
bounds; the only change from (6.22) is to modify the critical point:
̂𝜇(x) ±
√
2F𝛼(2, n −2) se[ ̂𝜇(x)].
(6.23)
The properties of the confidence band in (6.23) are worth reemphasizing: the band pro-
vides simultaneous 1 −𝛼coverage on 𝜇(x) for all values of x. This includes any predictor
values that were not originally considered by the analyst. Thus, while the Bonferroni bounds
in (6.22) require specification of the H target xhs prior to viewing the data, the WHS bands
can be applied in either an a priori or an a posteriori manner to any values of x.
Of course, the “no-free-lunch theorem” (Section 5.5.1) still applies: the width of the WHS
bands in (6.23) is usually larger than that of the Bonferroni bounds in (6.22). That is,
t𝛼∕(2H)(n −2) <
√
2F𝛼(2, n −2)
for most values of H. This need not always be the case, however (Schwager 1984). For any
known set of H target predictors xh, analysts should always check that the Bonferroni bounds
are indeed tighter than the WHS band for their intended application. Of course, when interest
centers on the entire mean response line, or on any post hoc confidence statements, the WHS
band is the proper inference vehicle.
Example 6.2.5 Cancer mortality and local employment (Example 6.2.1, continued).
Return to the UK cancer mortality data in Table 6.1, where Y = log{Mortality} due to
cancer was regressed on x = {Employment rate}, via the SLR model in (6.1) and (6.2).
Construction of a WHS simultaneous band for 𝜇(x) from (6.23) is relatively straightforward,
since essentially all its components have been calculated in the previous examples. The LS
estimator for the mean response is ̂𝜇(x) = 5.8428 −0.0151x, with
se[ ̂𝜇(x)] =
√
0.0181
{
1
342 + (x −76.3968)2
10 143.4665
}
from (6.17). For a 95% band, the pertinent WHS critical point is
√
2F0.05(2, 340) =
√
(2)(3.0223) = 2.4586. The resulting simultaneous bands are represented by
5.8428 −0.0151x ± 2.4586
√
0.0181
{
1
340 + (x −76.3968)2
10 143.4665
}
for all x. Figure 6.3 plots the bands, along with the centering LS regression line, overlayed on
the original scatterplot. Notice that the bands are hyperbolic in shape, with minimum width at
x = x = 76.3968. (This is also recognizable from their mathematical expression, above.) They
also fail to admit a horizontal line between their upper and lower bounds, consistent with the
earlier inference from Example 6.2.2 that 𝛽1 is negatively valued.
◽
The observation in Example 6.2.5 that the WHS confidence bands for 𝜇(x) achieve min-
imum width at x = x also extends to the Bonferroni bounds in (6.22). Both have widths
proportional to se[ ̂𝜇(x)], and as can be seen in (6.17), this standard error (i) is minimized
at x = x, and (ii) expands hyperbolically and symmetrically as x diverges from x. This has the
effect of widening either confidence statement for choices of x farther away from the center of

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
175
60
70
80
90
3.8
4.0
4.2
4.4
4.6
4.8
5.0
5.2
Employment(%)
log(Mortality)
Figure 6.3
Ninety-five percent WHS simultaneous confidence bands (solid curves) for
𝜇(x) with UK cancer mortality data in Example 6.2.5. Least squares estimator ̂𝜇(x) given
by dashed line. Original scatterplot from Figure 6.1 superimposed. Source: Data from
http://data.gov.uk/dataset/ni_151_-_employment_rate.
the x-range. In a certain sense, this is a desirable quality: as x draws away from x, the strength of
the regression relationship imparted by the larger data cloud weakens. The resulting standard
errors represent this by widening the confidence intervals/bands.
Indeed, values of x outside the range of the observed predictor values represent true extrap-
olations and are subject to question. It is typically unclear whether any given model will apply
outside the range of the observations, because by definition, no data are available to certify
the model’s use. Unless the analyst is assured that the model is valid at points away from the
data cloud, any such extrapolated inferences – either simultaneous or pointwise – may have
questionable validity.
6.3
Regression diagnostics
The quality of an SLR fit can vary, and it is good analytic practice to examine the fit for
potential violations of the model assumption, outlying observations, or other unusual features.
A variety of diagnostic tools are available for this task. One of the most basic, and also most
useful, is analysis of the residuals, ei, from (6.9). In effect, the residuals estimate variation in
the Yis that remains – hence their name – after the SLR effect has been accounted for via the
model fit.
A simple tool to study residual variation graphs ei against the fitted values ̂Yi. (For the SLR
model, one could equivalently plot ei against the predictor variable xi.) Known as a residual

176
STATISTICAL DATA ANALYTICS
plot, this graphic very effectively visualizes the quality of an SLR fit. When the model is
correct, E[ei] = 0 (Exercise 6.3d). Thus we expect the residual plot to display essentially
random scatter about e = 0. In fact, under (6.1) the residuals should imitate random normal
(Gaussian) noise. Figure 6.4 gives a prototypical example.
−10
−5
0
5
10
Fitted value
Residual
Figure 6.4
Prototypical residual plot with desired random scatter about e = 0. Horizontal
line at e = 0 included for reference.
By contrast, if a residual plot displays a clear pattern – of which there are many different
types – then some model violation or discrepancy may be indicated. A common violation is
heterogeneous variance, i.e. a departure from the assumption that Var[Yi] = 𝜎2 is constant.
This can be quickly recognized in a residual plot by variation(s) in the width of the residual
pattern. For instance, if Var[Yi] increases with increasing xi, a plot of ei vs. xi will show a
broadening of the residuals around e = 0 as xi increases. If the variation drops with increasing
xi, then the pattern will be reversed. Plots against the fitted values, ̂Yi, will show similar effects;
Figure 6.5 illustrates the phenomenon. In this case, some remedial action to account for the
heterogeneous variability would be required. (Possible remedies are discussed in Section 6.4.)
Another form of model violation observable in a residual plot is departure from linear-
ity, where the mean response takes some form of nonlinear function. For instance, if E[Y]
is quadratic in x but an SLR model is fit, the fitted values will under- and overestimate the
true response in a recognizable, alternating pattern; Figure 6.6 gives a typical example. Here
again, some remedial action would be required, such as expanding the SLR model to include
a quadratic term in x; see Section 7.2.
The residuals are also valuable in assessing the quality of the normality assumption in
(6.1). If normality is valid, the raw residuals should display normal variation, at least to a good
approximation. Histograms, density estimates, or stemplots of the eis may help in visualizing
the effect. Or, the normal quantile plot discussed in Section 4.1.5 could be applied: under

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
177
−10
−5
0
5
10
Fitted value
(a)
(b)
Residual
−10
−5
0
5
10
Fitted value
Residual
Figure 6.5
Residual plots indicating heterogeneous variation: (a) increasing variation or
(b) decreasing variation as fitted values increase. Horizontal lines at e = 0 included for
reference.
0
5
10
15
20
0
2
4
6
8
10
12
14
x
Y
0
5
10
−1.0
−0.5
0.0
0.5
1.0
1.5
Fitted value
Residuals
(a)
(b)
Figure 6.6
(a) Scatterplot and (b) corresponding residual plot indicating curvilinear mean
response. LS line for SLR model overlaid on the scatterplot, and horizontal e = 0 line overlaid
on the residual plot, for reference.
normality, a normal quantile plot of the residuals will appear roughly linear. If the plot deviates
strongly from normal variation, however, a transformation of the original Yis such as the power
transform in (3.13) (or, if the data justify it, consideration of a generalized linear model as in
Chapter 8) may be warranted.
Residual plots can also aid in the identification of potential outliers. Extending the concept
of a univariate outlier from Section 3.4.1, a residual that is far away from e = 0 in either a

178
STATISTICAL DATA ANALYTICS
positive or negative direction, and also lies far afield from the bulk of the residual cloud, may
indicate an outlying data pair with respect to the SLR model.
Raw residuals are scale/measurement dependent, however. In one data set, an absolute
residual of |ei| = 8.2 may be less egregious than another data set’s residual of |ei| = 0.7. To
adjust for this, we stabilize the raw residuals to a commensurate scale. The operation essen-
tially “Studentizes” the residual, much like that seen with the sample mean in (5.16): subtract
the true mean and divide by the standard error. The former is trivial: E[ei] = 0. To find the
latter, start with the variance. Under the SLR model,
Var[ei] = (1 −hii)𝜎2
(Kutner et al. 2005, Section 6.4), where hii is taken from a special matrix known as a hat
matrix. (The hat matrix is described in more detail in Section 7.1.1.) For use with the SLR
model here, one can show (Exercise 7.8)
hii =
nx2
i −2xi
∑n
ℓ=1 xℓ+ ∑n
ℓ=1 x2
ℓ
n ∑n
ℓ=1 x2
ℓ−
(∑n
ℓ=1 xℓ
)2
(6.24)
(i = 1, … , n). As 𝜎2 is unknown in Var[ei], we estimate it with the MSE. Taking square roots
then produces the standard error se[ei] =
√
(MSE)(1 −hii).
With this, the Studentized residual becomes
ei
√
(MSE)(1 −hii)
.
A further refinement for stabilizing the residuals involves examination of how case
deletion of individual observations affects the model fit. Clearly, one would expect the SLR
fit to change more drastically when extreme, outlying observations are deleted, than when
observations more in line with the rest of the data are removed. Formally then, denote the pre-
dicted value under the SLR model when the ith observation is removed from the data as ̂Yi[−i].
If this value deviates particularly far from Yi, the original observation could be a potential
outlier. To quantify this, the (raw) deleted residual is Yi −̂Yi[−i] and its Studentized version is
ti = (Yi −̂Yi[−i])∕se[Yi −̂Yi[−i]]. Extremely large values of ti identify potential outlying
response values.
At first glance, it appears that calculation of the tis requires recalculation of the SLR fit
for every deleted observation. A series of important computing formulae avoid this drawback,
however. First, the deleted residual Yi −̂Yi[−i] can be shown to equal ei∕(1 −hii), where hii
is given in (6.24). From this, the standard error is simpler to derive. In the end, the complete
Studentized deleted residual becomes
ti = ei
√
n −p −2
(SSE)(1 −hii) −e2
i
(6.25)
(Kutner et al. 2005, Section 10.2), where p is the number of predictor/feature variables fit in
the model. (This general notation for p will be useful in future chapters. For the SLR setting,
obviously, p = 1.)
Studentized deleted residuals follow t-distributions, with ti ∼t(n −p −2). Thus if an indi-
vidual observation’s Studentized deleted residual exceeds, say, the upper- 𝛼
2 critical point from

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
179
this reference distribution, it can be viewed as a possible outlier. Applying a Bonferroni cor-
rection (Section 5.5.1) when testing across all n observations leads to the following outlier
criterion for an SLR model fit: consider the ith observed response to be a potential outlier if
|ti| ≥t𝛼∕(2n)(n −p −2),
(6.26)
where the Studentized deleted residual ti is given in (6.25).
Example 6.3.1 UK cancer mortality and employment (Example 6.2.1, continued).
Return to the UK cancer mortality data in Table 6.1, where Y = log{Mortality} due to
cancer was regressed on x = {Employment rate}, via the SLR model in (6.1) and (6.2). The
residuals ei from the SLR fit can be computed in R by applying the resid() function to the
lm object, for example, resid(lm(Y ˜ x)). As an initial diagnostic, Figure 6.7 displays
a normal quantile plot (Section 4.1.5) for the raw residuals. This is achieved in R via the
sample commands
> qqnorm( resid(lm(Y ˜ x)) )
> qqline( resid(lm(Y ˜ x)) )
The qqline() command supplies a reference line passing through the first and third quartiles.
Visually, departures from this line seen over the bulk of the data range help to indicate a lack
of normality in the residuals.
The quantile pattern appears generally consistent with a normal distribution: for the most
part, the quantiles line up and coincide with the overlaid normal reference line. Three unusual
−3
−2
−1
0
1
2
3
−0.6
−0.4
−0.2
0.0
0.2
Theoretical normal quantiles
Sample quantiles
Figure 6.7
Normal quantile plot for raw residuals, ei, from SLR fit with UK cancer
mortality data in Example 6.3.1. Solid line is normal reference line. Source: Data from
http://data.gov.uk/dataset/ni_151_-_employment_rate.

180
STATISTICAL DATA ANALYTICS
60
65
70
75
80
85
90
−4
−2
0
2
4
Employment(%)
Studentized deleted residuals
Figure 6.8
Studentized deleted residual plot from SLR fit with cancer mortality data in
Example 6.3.1. Dashed lines are 5% exceedance levels, ±t0.05∕684(339) = 3.8411. Solid hor-
izontal line at t = 0 included for reference. Source: Data from http://data.gov.uk/dataset/ni_
151_-_employment_rate.
points appear in the lower tail of the distribution, however. These help to prompt the next step
in the diagnostic process, outlier detection.
As a diagnostic tool, residual visualization with these data provides an opportunity for
some intriguing knowledge discovery. The raw residual plot and the Studentized deleted
residual plot present roughly the same pattern (Exercise 6.8), although the latter is preferred
because it adjusts for possible scale issues. The Studentized deleted residuals, ti, may be
accessed in R via rstudent(lm(Y ˜ x)). These are plotted in Figure 6.8 against employ-
ment rate xi, where the scatter around t = 0 appears generally random. No troublesome
patterns are evidenced in the general scatter.
The plot also displays exceedance limits from (6.26) at 𝛼= 0.05 for identifying potential
outliers: with n = 342 and p = 1, ±t0.05∕684(339) = ±3.8411. Three communities drop below
the lower limit in the plot (none go above the upper limit), suggesting potential outlying status.
The fact that the three potential outliers in Figure 6.8 all drop below the exceedance limit is
intriguing: these three communities all exhibit lower-than-expected log-mortality. They corre-
spond to the three lowest-lying points in Figure 6.1 and are the three unusual, lower-tail points
marked by the normal quantile plot in Figure 6.7. Closer inspection of the data shows that the
three localities are (starting with the lowest residual): the City of London, the Royal Borough
of Kensington & Chelsea (RBKC), and the City of Westminster. All are London Boroughs
(LBs: administrative districts that make up the larger London metropolitan area). In fact, the
three adjoin at the core of central London. The City of London is the smallest, barely over a
square mile in area and with only about 7000 residents. It is, however, the business and com-
mercial heart of London and a leading center of global finance. (This may explain its elevated

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
181
employment rate of 90.3%, the largest in the UK sample.) The other two localities are larger
and have much higher population densities. The three councils also report some of the highest
median earnings in London and across the United Kingdom: for 2008, the City of London was
first in median earnings, Westminster was fifth, and the RBKC was forty-seventh. (Example
7.1.1 explores some of these factors in more detail.) Given these three communities’ similar
geographic and socioeconomic features, their potential outlying status may be motivation for
further study by British sociologists, economists, and medical researchers.
◽
An observation need not be an “outlier” to affect the fit of a regression model. For instance,
if a single xi rests far from the bulk of the other predictor values, it can literally lever the
regression fit. We say that an xi’s leverage is its ability to strongly influence the fit of the
regression. (This is usually seen as a detriment.) Some instructive online applications that
illustrate the leverage effect are available at
http://www.amstat.org/publications/jse/v6n3/applets/regression.html
and
http://www.stat.tamu.edu/jhardin/applets/signed/Outreg.html.
Readers are encouraged to experiment with these and see how a single xi can manipulate an
SLR fit.
The Studentized deleted residual in (6.25) gives guidance on ways to quantify leverage,
via its component “hat” value hii from (6.24). These hat values can be shown to lie between 0
and 1. Recognize then that as hii →1, |ti| will grow larger, while the reverse occurs as hii →0.
Other factors can affect the value of |ti|, of course, but this nonetheless indicates the impact
hii has on an observation’s status: small values of hii will associate with observations closer
in some manner to the core of the data, while larger values may act otherwise.
A practical rule-of-thumb for gauging the leverage of a predictor value is to flag any xi
whose hat value exceeds twice the average of all the hiis. In fact, the sum ∑n
i=1 hii always
equals 1 plus the number of predictor/feature variables fit in the model (Kutner et al. 2005,
Section 10.3), or simply p + 1 using the notation introduced in (6.25). Thus, the leverage
rule-of-thumb simplifies to
hii > 2(p + 1)
n
,
which reduces to hii > 4∕n for the p = 1 SLR setting.
Notice that the hat values from (6.24) do not depend on the Yis – that is, they are functions
only of the predictor values – and thus this leverage criterion can be examined for any xi prior
to observing the response values. This can be a useful exercise if the design of the predictor
spacings is determined before collecting the data.
Example 6.3.2 UK cancer mortality and employment (Example 6.2.1, continued).
Return to the UK cancer mortality data in Table 6.1, where Y = log{Mortality} due to cancer
was regressed on x = {Employment rate}, via the SLR model in (6.1) and (6.2). To study
the potential leverage of the employment rate values, we calculate the hiis in (6.24). In R,
employ the hatvalues() function. Combined with R’s logical indexing capability – i.e.,
using square brackets to identify selected elements of an array or object – we can isolate
those observations with high-leverage predictors. For these data, 4∕n = 4∕342 = 0.0117, and
we find 30 predictors in the sample that exhibit high leverage (output edited):
> hii <- hatvalues( lm(Y ˜ x) )
> p <- length( coef(lm(Y ˜ x)) ) - 1
> n <- length(x)

182
STATISTICAL DATA ANALYTICS
> cbind( x[ hii > 2*(p+1)/n ], hii[ hii > 2*(p=1)/n ] )
[,1]
[,2]
[,1]
[,2]
1
90.3 0.02198052
61
64.2 0.01758973
12
64.4 0.01711270
64
64.7 0.01641194
14
64.8 0.01618230
71
64.8 0.01618230
20
66.9 0.01181531
79
64.4 0.01711270
25
58.6 0.03414856
85
64.7 0.01641194
26
66.7 0.01219375
87
64.0 0.01807464
28
66.6 0.01238593
123 86.4 0.01278888
30
60.7 0.02721439
184 86.1 0.01220605
33
63.3 0.01983395
195 86.7 0.01338946
36
61.3 0.02539291
222 66.6 0.01238593
44
65.2 0.01528346
225 64.6 0.01664356
45
62.9 0.02088265
253 66.0 0.01358040
46
66.7 0.01219375
286 86.1 0.01220605
54
65.2 0.01528346
307 87.7 0.01551954
58
62.7 0.02141883
318 87.6 0.01529766
The initial (unlabeled column) indices in the above output are the values of i that satisfy the
selection criterion hii > 2(p + 1)∕n = 4∕n. The second column under [,1] gives the selected
xis, and the third column under [,2] lists the corresponding hat values hii.
Figure 6.9 reproduces the original scatterplot and LS line, now with the high-leverage
points highlighted. As expected, high-leverage points lie at the extremes of the predictor range.
Notice, however, that many of the point fall generally in line with the overall trend in the data;
indeed, the only points that stand out blatantly are the same three LB localities identified by
the outlier analysis in Example 6.3.1. (How coincidental this is would require deeper study
into the nature of those localities’ individual socioeconomic and health factors. Outliers need
not be high-leverage points and vice versa.)
◽
Beyond simple outlier status of Yi or the leverage potential of xi, data pairs in an SLR may
also be studied for their specific influence on the model fit. Such “influential observations”
combine features such as high leverage, separation from the general trend, or other compelling
features that may be worthy of identification. A useful comprehensive measure that quanti-
fies the influence of the ith observation on the collection of ̂Yis is known as Cook’s distance
(Cook 1977). The statistic compares every fitted value ̂Ym (m = 1, … , n) with its prediction
̂Ym[−i] when the ith observation is deleted. Squaring and summing their differences gives a
case-deletion measure of discrepancy at each i:
n
∑
m=1
( ̂Ym −̂Ym[−i])2.
Scaling by the product (p + 1)MSE then produces Cook’s distance measure for the ith
observation:
Di =
∑n
m=1 ( ̂Ym −̂Ym[−i])2
(p + 1)MSE
.
For comparative purposes, Di is often referred to an F random variable. (Quality of the
F-approximation can fluctuate but it suffices for use as a diagnostic tool.) An observation
is viewed as “influential” if the probability P[F(p + 1, n −p −1) ≤Di] exceeds a defined
cutoff. Recommendations for this threshold vary, but a common theme is that the probability

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
183
60
70
80
90
3.8
4.0
4.2
4.4
4.6
4.8
5.0
5.2
Employment (%)
log(Mortality)
Figure 6.9
Scatterplot for UK cancer mortality data in Example 6.3.2, with high-leverage
points identified by solid circles. (Gray circles are points below the leverage thresh-
old.) Least squares regression line ̂𝜇(x) is overlaid. Source: Data from http://data.gov.uk/
dataset/ni_151_-_employment_rate.
should exceed at least 1∕2 for an observation to be influential. For small n, the threshold is
sometimes pushed down as low as 1∕5 or even 1∕10. (Recall for the SLR case that p = 1.)
Although the Di measure is based on case deletion, it does not require repeated recalcula-
tion of the SLR fit. As with the Studentized deleted residuals, a convenient computing formula
is available:
Di =
(
e2
i
[p + 1]MSE
) (
hii
[1 −hii]2
)
.
(6.27)
The formula illustrates the distance’s features: both ei and hii enter into its computation and
as either (or both) depart from zero, Di grows larger. Thus departure from the central trend
and/or high leverage will produce a high influence value.
Example 6.3.3 UK cancer mortality and employment (Example 6.2.1, continued).
Return to the UK cancer mortality data in Table 6.1, where Y = log{Mortality} due to cancer
was regressed on x = {Employment rate}, via the SLR model in (6.1) and (6.2). To study the
influence of the various data points using Cook’s distance in (6.27), one can compute Di for
each data pair and compare it to an F(p + 1, n −p −1) = F(2, 340) distribution. In R, this
employs the eponymous cooks.distance() function.
As with the leverage measure in Example 6.3.2, calculation of every Di can grow tedious
for large data sets, so it is more valuable to identify only those points with high influence.
Here, use as the criterion P[F(2, 340) > Di] > 1
2. Sample R code is
> Di = cooks.distance( lm(Y ˜ x) )
> which( pf(Di, p+1, n-p-1, lower=T) > .5)

184
STATISTICAL DATA ANALYTICS
(The which() function identifies which indices satisfy a logical query. The pf() func-
tion calculates F probabilities.) Interestingly, this results in no observations meeting the
high-influence criterion. The largest value of Di in the data set is D1 = 0.2476, corresponding
to the most extreme outlier in Example 6.3.1 (the City of London LB), and also the point
farthest along and lowest in Figure 6.1. These extreme-appearing features are not sufficient,
however, to flag it – or any other data point – as highly influential under Cook’s distance:
P[F(2, 340) ≤0.2476] is only 0.22 and not near the threshold of 1
2.
◽
In practice, when influential observations are flagged by any of these regression diag-
nostics, it is common to repeat the analysis without the questionable data points. Then,
examine whether and how the LS estimates and other analytic outcomes change as a result.
(Exercise 6.8 applies this strategy to the UK mortality versus employment data in Examples
6.3.1–6.3.3.) Very large differences suggest that the outliers, influential points, and so on
may require special attention, or at least a reexamination of how they were sampled, to verify
that their inclusion in the analysis is warranted. In the process, further study of these unusual
observations may provide for new knowledge discovery.
A variety of other diagnostic tools are available for assessing influence in regression anal-
ysis, and the presentation here is intended only as a brief introduction. For more on regression
diagnostics, see Kutner et al. (2005, Section 10.4) or the classic texts by Belsley et al. (1980)
and Cook and Weisberg (1982).
6.4
Weighted least squares (WLS) regression
An important, and not uncommon, violation of the basic SLR assumptions is heterogeneous
variation in the Yis, that is, departure from the assumption that Var[Yi] = 𝜎2 is constant. This
was illustrated conceptually by the residual diagnostic plots in Figure 6.5. If left unaccounted,
differential variation in Var[Yi] can detrimentally affect the inferences from an SLR fit.
When heterogeneous variation is present, a common remedial tactic is to transform
the original responses. As noted in Section 3.4.3, moving from Yi to g(Yi) will change
the distributional properties of the observations, a possible consequence of which can be
reversion to homogeneous variation (at least to a good approximation). Of course, this
may also change the underlying parent distribution of the Yis: data that began as normal or
approximately normal may be driven away from this status, creating another model violation.
(In large samples, normality may still be valid to a good approximation, as per the delta
method theorem in Section 2.3.9.) On the other hand, if the variance heterogeneity exists
concomitantly with nonnormal variation, then the transformation may act to address both
problems. The natural logarithm is a popular choice, as seen in Example 6.2.1. The various
alternatives in Section 3.4.3 can provide similar remediation along these lines.
When a transformation to homogeneous variation cannot be found, or when the desired
variance-stabilizing transformation disrupts normality in the data, the standard remedy is
to apply weighted least squares (WLS) from Section 5.2.3. That is, include heterogeneous
weights, 𝑤i, in the LS normal equations (6.3) to account for the differential variation. The
weighted normal equations in the SLR setting are
n
∑
i=1
𝑤iYi = 𝛽0
n
∑
i=1
𝑤i + 𝛽1
n
∑
i=1
𝑤ixi
n
∑
i=1
𝑤ixiYi = 𝛽0
n
∑
i=1
𝑤ixi + 𝛽1
n
∑
i=1
𝑤ix2
i .
(6.28)

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
185
For the remainder of this section, assume Var[Yi] = 𝜎2
i across all i = 1, … , n. A propitious
choice for 𝑤i in (6.28) weights each observation inversely proportional to its variation, so
take 𝑤i ∝1∕𝜎2
i . Thus the more precision – that is, lower variance – an observation exhibits,
the higher its weight.
Solving the weighted normal equations for the unknown regression parameters produces
WLS estimators
̃𝛽1 =
∑n
i=1 𝑤ixiYi −𝑤−1
+
∑n
i=1 𝑤ixi
∑n
j=1 𝑤jYj
∑n
i=1 𝑤ix2
i −𝑤−1
+
(∑n
i=1 𝑤ixi
)2
and
̃𝛽0 =
∑n
i=1 𝑤iYi −̃𝛽1
∑n
i=1 𝑤ixi
𝑤+
,
where 𝑤+ = ∑n
i=1 𝑤i.
The standard error of ̃𝛽1 is
se[ ̃𝛽1] =
√
̃
MSE
√∑n
i=1 𝑤ix2
i −𝑤−1
+
(∑n
i=1 𝑤ixi
)2 ,
where ̃
MSE is the weighted mean square ∑n
i=1 𝑤i(Yi −̃Yi)2∕(n −2) and ̃Yi = ̃𝛽0 + ̃𝛽1xi are the
WLS fitted values (i = 1, … , n). An approximate 1 −𝛼confidence interval for 𝛽1 is then
̃𝛽1 ± t𝛼∕2(n −2)se[ ̃𝛽1].
When the 𝜎2
i values are unknown, the weights must be estimated or calculated under some
sensible supposition. For instance, the variance may increase proportionally with some pos-
itive function h(xi). Simple examples include h(xi) = x2
i or h(xi) = xi for xi > 0. If so, set the
weights inversely proportional to variance: 𝑤i = 1∕h(xi) for all i. More complex strategies are
also available; see, for example, Kutner et al. (2005, Section 11.1).
Example 6.4.1 Baseball batting averages. Statistical analytics increasingly support
strategic planning with professional sports teams. Popular examples such as the motion
picture Moneyball (http://www.sonypictures.com/movies/moneyball/) and the associated
book (Lewis 2003) advertise and encourage this phenomenon. Indeed, North American
Major League Baseball is a heavy user of “sabermetrics” – the use of statistical methods to
analyze and predict baseball player and team performance (http://sabr.org/sabermetrics) – as
Moneyball helped to illustrate.
For example, Friendly (1991, Section A.2) discusses data on performance of n = 322
Major League Baseball players, not including pitchers, after the 1986 season. Included among
the data are the players’ career batting averages, along with the number of years they had
played in the major leagues. One might question whether increased tenure in the major leagues
leads to a higher batting average. To study this, let Y = {Career batting average × 1000} and
x = {Years played}. Table 6.2 presents a selection of the original data. (The complete set
is available at http://www.wiley.com/go/piegorsch/data_analytics. The larger database is also
available as part of the external vcd package in R.)
Conditioning the analysis on the observed pattern of years played, a scatterplot of the
data (Figure 6.10a) indicates a clear upward trend. The raw residuals, Yi −̂Yi, from the
unweighted SLR fit suggest a pattern of decreasing variance with increasing response,
however (Figure 6.10b). To adjust for this, consider a WLS–SLR fit of these data.

186
STATISTICAL DATA ANALYTICS
Table 6.2
Selected data from a larger set of n = 322 observations on
Major League Baseball players’ career batting performance through 1986
season (see text for details).
Player code
Years played
Hits
At bats
Batting average
AN
2
42
214
0.196
AT1
10
1300
4631
0.281
⋮
⋮
⋮
⋮
⋮
WW
11
1457
4908
0.297
Source: Friendly (1991).
0
5
10
15
20
25
Years played
(a)
(b)
Batting average × 1000 
150
200
250
300
350
250
260
270
280
290
−100
−50
0
50
100
Fitted values
Residuals
Figure 6.10
(a) Scatterplot for baseball batting average data in Example 6.4.1 with
unweighted LS regression line overlaid and (b) raw residual plot from unweighted
LS regression. Source: Data from http://vincentarelbundock.github.io/Rdatasets/doc/vcd/
Baseball.html. Graphic adapted from Friendly (1991).
A plausible assumption to make on the heterogeneous variation here is that the variance
is inversely proportional to the (positive) number of years played, that is, Var[Yi] ∝1∕xi. This
leads to weights of the form 𝑤i ∝xi. The following are the sample R code; note the use of the
weights= option in the lm command:
> Y <- 1000*hits/atbat; x <- years
> baseballW.lm <- lm( Y ˜ x, weights = x )
> summary( baseballW.lm )
> confint( baseballW.lm )
This produces (output edited)
Call:
lm(formula = Y ˜ x, weights = x)

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
187
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
253.7371
2.7194
93.308
< 2e-16
x
1.2405
0.2295
5.405 1.27e-07
and
2.5 %
97.5 %
(Intercept)
248.3870012
259.087166
x
0.7889613
1.692028
where, in particular, an approximate (pointwise) 95% confidence interval on the slope param-
eter is given by 0.7890 ≤𝛽1 ≤1.6920. Besides the implication that a significant relationship
appears between x and Y, the interval indicates that every additional year of experience yields
on an average an increase of between 0.8 and 1.7 points in batting average for these profes-
sional baseball players.
In passing, it is worth speculating on possible factors that underlie this analysis. A natural
conclusion is that a player’s skill in batting improves as he become more mature, and the
data seem to support this. Other factors may also be at work, however. For instance, is there a
selective effect as careers develop? Figure 6.10a shows fewer observations for larger values of
x, particularly past x ≥19 years or so. Players whose batting skills do not advance sufficiently
as they age – and, for example, as they negotiate for greater salary and benefits – may face
release from team owners who insist on increased batting performance. Indeed, the figure also
shows that few players exhibit batting averages below 0.200 – a minimum level to perform at
the major-league level – and of these, all are young players with no more than x = 3 years of
major-league experience. Failure to improve low batting averages may contribute to limited
tenure in baseball’s major leagues.
Similar statistics are now actively collected and explored by baseball sabermetricians,
leading to further knowledge discoveries regarding players’ performance. Examples include,
but are not limited to, Kvam (2011) and Piette and Jensen (2012).
Exercises 6.10 and 7.16a explore further aspects of the WLS fit with these data.
◽
6.5
Correlation analysis
The SLR model is best suited for assessing the (linear) aspects of the mean response, E[Y],
and predicting its value(s) from the explanatory x-variable. When the analytic focus is not one
of predictor-versus-response, however, a different model may be appropriate. For instance,
if interest exists in simply evaluating the association between two paired random variables
X and Y, data analysts often study the (linear) correlation between the two variables. The
analysis is more in keeping with the approach in Section 3.3.3 than with the regression focus
of this chapter; however, the similarities between two model formulations make it useful to
present further details on correlation analysis here.
6.5.1
The correlation coefficient
It is important to emphasize that the underlying model for bivariate correlation analysis dif-
fers technically from the SLR structure in (6.1). Formally, the paired observations (Xi, Yi),
i = 1, … , n, are viewed as realizations of a bivariate random vector with joint probability

188
STATISTICAL DATA ANALYTICS
density function fX,Y(x, y). Interest centers on estimation and inferences for the correlation
between X and Y,
𝜌= Cov[X, Y]
𝜎X𝜎Y
,
where 𝜎X and 𝜎Y are the population standard deviations of X and Y, respectively. The specific
form taken for fX,Y is usually the bivariate normal from (2.39). Then, the MLE of 𝜌is precisely
the product-moment correlation coefficient from Section 3.3.3:
rXY =
∑n
i=1(Xi −X)(Yi −Y)
√∑n
i=1 (Xi −X)2 ∑n
i=1 (Yi −Y)2
.
(6.29)
As discussed in Chapter 3, −1 ≤rXY ≤1 and the statistic measures linear association between
the paired variables. When rXY = 0, the data suggest no apparent relationship between Xi and
Yi, while as |rXY| →1, a linear relationship between them is indicated. Directionality (positive
or negative association) is taken from the sign of rXY.
A natural hypothesis to test under this bivariate normal model is whether Ho: 𝜌= 0 versus
either two-sided (Ha: 𝜌≠0) or one-sided departures (e.g., Ha: 𝜌> 0 for positive association),
depending on any a priori subject-matter input. To do so, construct the t-statistic
T =
rXY
√
n −2
√
1 −r2
XY
.
(6.30)
Under Ho: 𝜌= 0, T ∼t(n −2). Reject Ho in favor of Ha: 𝜌≠0 when |T| ≥t𝛼∕2(n −2). The
corresponding P-value is 2P[t(n −2) ≥|tcalc|], where tcalc is the observed value of the statistic
in (6.30). One-sided tests are similar.
Example 6.5.1 College admissions (Example 3.3.6, continued). Recall the paired college
admissions data in Example 3.3.6 relating high school class rank (%) to reported ACT score
from a set of n = 705 admissions files at a large US public university.
After transformation of the class ranks via a logit function, the correlation between
X = logit{Class rank} and Y = {ACT score} was found in Example 3.4.4 as rXY = 0.4597.
For example, use cor(log(rank/(100-rank),ACT) in R. It was implied that an observed
correlation near 0.45 represented a moderate level of positive association. To formalize this,
assume the bivariate normal model holds for the (X, Y) pairs, and consider testing Ho: 𝜌= 0.
For the alternative, one assumes these two admissions measures would correlate positively,
if at all, so set Ha: 𝜌> 0. Let 𝛼= 0.01.
Given rXY = 0.4597, the test statistic in (6.30) calculates to
tcalc = 0.4597
√
703
√
1 −0.45972 = 13.7247.
Using the greater accuracy available in, for example, R, a more-precise value is
tcalc = 13.7240; see the following output. The corresponding one-sided P-value is
P = P[t(703) > 13.7240], which is vanishingly small. (In R, pt(13.7240,df=703,
lower=F) gives 1.89 × 10−38.) Thus P is far less than 𝛼and we reject Ho; conclude that a
significantly positive correlation is indeed evidenced between these two admissions measures
(after suitable transformation of the ranks).

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
189
The complete analysis is available in R via the cor.test() function (output edited):
> X = log( rank/(100-rank) )
> cor.test( x=X, y=ACT, alternative =‘greater’ )
Pearson’s product-moment correlation
data:
X and ACT
t = 13.7240, df = 703, p-value < 2.2e-16
alternative hypothesis: true correlation is greater than 0
sample estimates:
cor
0.4596824
◽
Confidence intervals for 𝜌require additional effort, as the unknown parameter does not
appear explicitly in the t-statistic from (6.30). The established approach, due to Fisher (1921),
applies a transformation to the observed correlation:
Z = 1
2 log
(1 + rXY
1 −rXY
)
,
(6.31)
which is an alternative expression for the inverse hyperbolic tangent of rXY: Z = tanh−1(rXY).
Fisher used this to find
Z ̇∼N
(
𝜁,
1
n −3
)
,
where 𝜁= tanh−1(𝜌). (Fisher determined that the approximation is very accurate, even for
sample sizes as small as n = 8.) From this, an approximate 1 −𝛼confidence interval on 𝜁is
simply
ZL = Z −
z𝛼∕2
√
n −3
< 𝜁< Z +
z𝛼∕2
√
n −3
= ZU,
where z𝛼∕2 is the upper- 𝛼
2 critical point from the standard normal distribution. Applying the
inverse transformation
𝜌= tanh(Z) = e2Z −1
e2Z + 1
to the limits ZL and ZU produces an approximate 1 −𝛼confidence interval on 𝜌:
e2ZL −1
e2ZL + 1 < 𝜌< e2ZU −1
e2ZU + 1 .
Example 6.5.2 College admissions (Example 6.5.1, continued).
Continuing with the
paired college admissions data, recall that the observed correlation between X = logit{Class
rank} and Y = {ACT score} was found to be rXY = 0.4597. For a 95% confidence interval
on the true correlation 𝜌under the bivariate normal model, apply Fisher’s Z-transformation
from (6.31):
Z = 1
2 log
(1 + 0.4597
1 −0.4597
)
= 1
2 log(2.7015) = 0.4969.
With this, the upper and lower Z-transformed 95% limits are
ZL = 0.4969 −
z0.025
√
705 −3
= 0.4229

190
STATISTICAL DATA ANALYTICS
and
ZU = 0.4969 +
z0.025
√
705 −3
= 0.5709.
Next, apply the inverse Z-transform (the hyperbolic tangent) to each limit to achieve the
final 95% limits on 𝜌:
e(2)(0.4229) −1
e(2)(0.4229) + 1 = 0.3994 < 𝜌< 0.5160 = e(2)(0.5709) −1
e(2)(0.5709) + 1.
We again see that a clear positive correlation exists between the two admission measures.
These confidence limits are available in R as part of the cor.test() output (previously
suppressed), under “95 percent confidence interval:”
> X = log( rank/(100-rank) )
> cor.test( x=X, y=ACT, alternative=‘two-sided’ )
Pearson’s product-moment correlation
data:
X and ACT
t = 13.724, df = 703, p-value < 2.2e-16
95 percent confidence interval:
0.3993997 0.5160072
sample estimates:
cor
0.4596824
A variety of external R packages also perform Fisher’s Z-transformation and the associ-
ated correlation analysis, including the CIr() function in the psychometric package and the
fisherz() suite in the psych package.
◽
6.5.2
Rank correlation
When the assumption of bivariate normality for the random pair (X, Y) is questionable or
untenable, a rank-based alternative exists for testing Ho: No association between the two
variables. Known as Spearman’s rank correlation (Spearman 1904b), the method essentially
replaces the observations with their ranks (largest to smallest, within each variable) and
then computes the product moment correlation (6.29) on these ranks. The resulting rank
correlation, rS, can then be assessed against a reference distribution based on how the ranks
should permute if no correlation is present, that is, under Ho (Kvam and Vidakovic 2007,
Section 7.3). The calculations are straightforward, if tedious, and most analysts employ the
computer for estimation and testing. For example, in R, simply apply the cor() function
with the method=“spearman” option.
The Spearman rank test can suffer if many ties exist among the observations, which is
not uncommon with the large data sets seen in modern data analytics. To compensate, a
t-approximation is available if the sample size exceeds about 10: simply calculate the rank
correlation rS and apply it in the t-statistic from (6.30):
TS =
rS
√
n −2
√
1 −r2
S
.
(6.32)

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
191
Under Ho, TS ̇∼t(n −2). Reject Ho in favor of some association when |TS| ≥t𝛼∕2(n −2).
The corresponding P-value is 2P[t(n −2) ≥|tScalc|], where tScalc is the observed value of the
statistic in (6.32). One-sided tests are similar.
Example 6.5.3 College admissions (Example 6.5.1, continued).
Continuing with the
paired college admissions data from Table 3.2, recall that concern over a large skew led to
consideration of a logit transformation of the n = 705 original class ranks, because the skew
would affect the quality of the bivariate normal-based analysis. Moving to Spearman’s rank
correlation can assuage this concern, however. So, consider estimating the rank correlation
between the original variables X = {Class rank} and Y = {ACT score}. This is quickly
accomplished in R:
> cor( x=class.rank, y=ACT, method=‘spearman’ )
[1] 0.4392875
We find rS = 0.4393, which is, again, a moderate level of positive association.
To test for significant, one-sided, positive association, set 𝛼= 0.01. The Spearman rank
test is available as a part of the cor.test() output after including the method=“spearman”
option (and, because this large data set contains many tied ranks, the exact=FALSE option to
institute the large-sample t-approximation):
> cor.test( x=class.rank, y=ACT, method=‘spearman’,
alternative =‘greater’, exact=FALSE )
Spearman’s rank correlation rho
data:
class.rank and ACT
S = 32745789, p-value < 2.2e-16
alternative hypothesis: true rho is greater than 0
sample estimates:
rho
0.4392875
Notice that the test statistic is given here as S = 32745789, which is the (very large!)
Spearman rank-based test statistic (the sum of the squared differences in the ranks). As
the exact=FALSE option was included, however, the P-value is instead based on the
t-approximation. Moving to the exact=TRUE option would produce the same test statistic
but would incur the R warning “Cannot compute exact p-values with ties.”
From the R output, the one-sided P-value is again vanishingly small: P < 2.2 × 10−16,
which best reported as simply P < 0.0001. A strongly significant, positive correlation is evi-
denced between the two (original) variables.
◽
Exercises
6.1
Return to the SLR model from (6.1) and (6.2).
(a) Verify the forms of the normal equations in (6.3) by differentiating the LS objective
quantity = ∑n
i=1 {Yi −(𝛽0 + 𝛽1xi)}2 with respect to both 𝛽0 and 𝛽1.
(b) Show that the solution of the LS normal equations in (6.3) produces the point
estimators
̂𝛽0 and
̂𝛽1 in (6.4). (Hint: Show that ∑n
i=1 (xi −x)2 = ∑n
i=1 x2
i −
1
n
(∑n
i=1 xi
)2.)

192
STATISTICAL DATA ANALYTICS
(c) Construct the log-likelihood function (see Section 5.2.4) for 𝛽0 and 𝛽1 under the
SLR model.
(d) Show that by jointly maximizing the log-likelihood, the MLEs for 𝛽0 and 𝛽1 are
identical to the LS estimators derived in Exercise 6.1.
6.2
Show that for the SLR model in (6.1) and (6.2), the LS estimators from (6.4) possess
the following sampling qualities.
(a) Both estimators are unbiased, that is, E[ ̂𝛽j] = 𝛽j for j = 0, 1. (Hint: Show that ̂𝛽0 and
̂𝛽1 can each be written in the form ∑n
i=1 𝜅iYi for some constants 𝜅i, and then take
advantage of features for sums of normally distributed random variables discussed
in Section 2.3.9. Use this result as necessary throughout the exercise.)
(b) Verify the forms of the sampling variances in (6.6).
(c) Cov[ ̂𝛽0, ̂𝛽1] = −𝜎2x∕∑n
i=1 (xi −x)2.
(d) Both estimators are normally distributed: ̂𝛽j ∼N(𝛽j, Var[ ̂𝛽j]) , j = 0, 1, where the
Var[ ̂𝛽j] quantities are given by (6.6).
(e) ̂𝜇(x) ∼N(𝛽0 + 𝛽1x, Var[ ̂𝜇(x)]) for any x, where Var[ ̂𝜇(x)] is given by (6.8). (Hint:
As above, show that ̂𝜇(x) can be written in the form ∑n
i=1 𝜅iYi for some constants
𝜅i, and then take advantage of features for sums of normally distributed random
variables discussed in Section 2.3.9.)
6.3
Show that for the SLR model in (6.1) and (6.2), the fitted values ̂Yi from (6.5) and
the residuals ei from (6.9) satisfy the following relationships. (Hint: Where necessary,
appeal to the relationships defined by the normal equations from (6.3).)
(a) ∑n
i=1 ei = 0 and from this, ∑n
i=1 Yi = ∑n
i=1 ̂Yi.
(b) ∑n
i=1 xiei = 0.
(c) ∑n
i=1 ̂Yiei = 0.
(d) E[ei] = 0.
6.4
For the SLR model in (6.1) and (6.2), are the LS estimators from (6.4) necessarily
independent (in the statistical sense, as per Section 2.1.2)? Why or why not? If not, is
there a feature or condition that ensures independence?
6.5
Use the normality of ̂𝛽1 from (6.7), the 𝜒2 feature of the scaled MSE from (6.11), and
the independence between these two random variables to establish that T = ( ̂𝛽1 −𝛽1)∕
se[ ̂𝛽1] ∼t(n −2). With this, manipulate the relationship P[−t𝛼∕2(n −2) < T <
t𝛼∕2(n −2)] = 1 −𝛼into the two-sided confidence interval in (6.14). (Hint: Imitate
the operations in (5.17).) Can you achieve similar results for ̂𝛽0?
6.6
Vise (2012) reported data on business characteristics of n = 250 US trucking and
delivery companies for calendar year 2011. Among the characteristics studied were
the number of drivers and the number of power units each company employed. The
data are available online at http://www.wiley.com/go/piegorsch/data_analytics; a sam-
ple follows:

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
193
Drivers:
102 000
125 497
24 335
· · ·
392
386
Power units:
98 908
52 287
15 602
· · ·
386
390
Examine these data as follows:
(a) Consider the application of the SLR model in (6.1) and (6.2). As the variables
are counts, and because they range over many degrees of magnitude, it is natural
to employ some form of transformation. Here, apply a logarithmic transform to
both variables and plot Y = log{Units} versus x = log{Drivers}. Does the resulting
relationship appear linear? What is the coefficient of determination (R2) with the
transformed data?
(b) Regress Y = log{Units} on x = log{Drivers} via the SLR model. Use the results to
test whether an increase in (log-)number of drivers increases the mean (log-)number
of power units. Operate at 𝛼= 0.01.
(c) Under the SLR model, construct a 90% confidence region for 𝛽1.
(d) Under the SLR model, construct a joint 90% confidence region for 𝛽0 and 𝛽1. How
does your inference on 𝛽1 compare with the pointwise inference in Exercise 6.6c?
(e) Under the SLR model, construct and plot a 90% simultaneous WHS confidence
band on the mean response for all x.
(f) Find the raw residuals ei from the SLR fit in Exercise 6.6b, and plot them against xi.
Also construct a normal quantile plot of the eis. Do any untoward patterns emerge?
(g) Calculate the Studentized deleted residuals ti from the SLR fit in Exercise 6.6b and
plot these against xi. Do any points appear to be potential outliers at 𝛼= 0.10?
(h) Find the diagonal elements hii (i = 1, … , n) from the hat matrix under the SLR fit
in Exercise 6.6b. Use these to identify if any of the xis appear to be high leverage
points.
(i) Find Cook’s distance Di (i = 1, … , n) using (6.27), and identify if any data pairs
appear influential, using the criterion P[F(p + 1, n −p −1) ≤Di] > 1
2 with p = 1.
(j) If a new company with 530 drivers were to be formed, how many power units would
you expect the company would require, based on the SLR fit in Exercise 6.6? Also
give a 99% confidence interval for this value.
6.7
Return to the UK cancer mortality data in Table 6.1, and construct a set of
multiplicity-adjusted confidence bounds on the mean log(Mortality) response at the
following H = 5 values of x = {Employment rate}: xh = 50, 60, 70, 80, 90%. Operate
at a familywise confidence level of 90%.
6.8
Return to the UK cancer mortality data in Table 6.1, and perform the following diag-
nostic operations.
(a) Calculate the raw residuals ei and the Studentized deleted residuals ti from the
SLR fit. Plot both against (i) the fitted values ̂Yi and (ii) the predictor values xi.

194
STATISTICAL DATA ANALYTICS
How do the patterns differ among these various plots? For the raw residual plots in
particular, would the outlying features discussed in Example 6.3.1 be as evident?
(b) Supplement the normal quantile plot in Figure 6.7 by constructing a boxplot and
a histogram of the raw residuals (inserting rug plots at bottom). What patterns
emerge? Do these corroborate the indications in Example 6.3.1?
(c) Remove the three potential outliers identified in Example 6.3.1, and fit a new SLR
model to the remaining 339 paired observations. Calculate the new LS estimates
for 𝛽0 and 𝛽1. Plot the remaining points and overlay the new LS line. How do the
results differ from those seen using the full 342-point data set? Repeat the effort
by removing only the most extreme potential outlier (the City of London LB). Do
your calculations bring into question any of the three points?
(d) Remove the 30 leverage points identified in Example 6.3.2, and fit a new SLR
model to the remaining 312 paired observations. Calculate the new LS estimates
for 𝛽0 and 𝛽1. Plot the remaining points and overlay the new LS line. How do the
results differ from those seen using the full 342-point data set?
6.9
A famous exercise in data mining involves airline on-time performance for US
commercial flights (see http://stat-computing.org/dataexpo/2009/). The data set is
massive, with almost 120 million records; for illustration purposes, consider here
only data on flights reporting actual arrival delays (positive times, in minutes) from
calendar year 2008, and only over distances less than 3000 miles. These arrival delay
times exhibit a right skew, so take the response variable as Y = log{Arrival delay}
and regress this against x = {Flight distance}. The data are available online at
http://www.wiley.com/go/piegorsch/data_analytics; a sample is as follows:
Distance (miles):
810
515
515
· · ·
215
533
533
Arrival delay (min):
2
14
34
· · ·
2
14
9
Examine these data as follows:
(a) Even when reduced as described above, the sample size here is almost 3 × 106.
To simplify the exercise, select from the data set a random sample of n =
2000 data pairs. Calculate Y = log{Arrival delay} and plot this against x =
{Flight distance}. What pattern emerges? (In R, use the sample() function
with the replace=F option. If you cannot download or access the origi-
nal file, download a sample file with 2000 randomly selected data pairs at
http://www.wiley.com/go/piegorsch/data_analytics. Note that the sample file has
already applied the logarithmic transform to the delay times.)
(b) Regress Y on x via the SLR model in (6.1) and (6.2). Test whether increasing flight
distance increases the mean (log-)arrival delay. Operate at 𝛼= 0.05.
(c) Under the SLR model, construct a 90% confidence region for 𝛽1.
(d) Under the SLR model, construct and plot a 90% simultaneous WHS confidence
band on the mean response for all x.

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
195
(e) Find the raw residuals ei from the SLR fit in Exercise 6.9b and plot them against
the fitted values ̂Yi. What patterns emerge?
(f) Calculate the Studentized deleted residuals ti from the SLR fit in Exercise 6.9b
and plot these against ̂Yi. Determine if any points are potential outliers by assess-
ing the Studentized deleted residuals against the exceedance limits ±t𝛼∕(2n)(n −
p −2), where p = 1. (Set 𝛼= 0.10. Can you see any possible problems with these
Bonferroni-adjusted limits here?)
(g) Find Cook’s distance Di (i = 1, … , n) using (6.27) and identify if any data pairs
appear influential, using the criterion P[F(p + 1, n −p −1) ≤Di] > 1
2 at p = 1.
(h) Suppose you were meeting a flight from Miami, FL, to Seattle, WA (distance =
2734 miles), and you were informed the flight’s arrival was delayed. How long
a delay would you expect to encounter based on the analysis of your sample in
Exercise 6.9b? Also give a 99% confidence interval for this value.
(i) Repeat your analysis in Exercise 6.9b with a new random sample of n = 2000 data
pairs. (If computer resources permit, do so a number of times.) Do you recover the
same results, at least qualitatively?
(j) What factors might contribute to model-assumption violations with these data?
6.10
Return to the baseball batting average data from Table 6.2 and consider the following,
additional aspects of the WLS analysis for these data.
(a) What is the interpretation of the estimated intercept term, ̂𝛽0? From the Example,
find a 95% confidence interval for the intercept and interpret the results. (Should
any adjustment be made to these quantities?)
(b) Recover the residuals, ̃ei = Yi −̃Yi, from the WLS–SLR fit and construct (i) a his-
togram of the ̃eis (include a rug plot at bottom) and (ii) a normal quantile plot of
the ̃eis. Do these suggest strong departures from the normal parent assumptions?
(c) Plot the residuals, ̃ei, against xi. Notice that the pattern of variance heterogeneity
has not changed much, which is not unreasonable: the WLS solution is not designed
to remove the heterogeneity, merely to adjust the estimators for it.
(d) Find the Studentized deleted residuals from the WLS–SLR fit (in R, use rstu-
dent()). Plot these against xi. Determine if any points are potential outliers by
assessing the Studentized deleted residuals against the approximate exceedance
limits ±t𝛼∕(2n)(n −p −2), where p = 1. Set 𝛼= 0.05. Do any points appear to be
potential outliers?
6.11
In 2013, the online site payscale.com released data on median, annual, full-time earn-
ings (in dollars, to the nearest hundred) among n = 606 private colleges and universities
in the United States; see http://www.payscale.com/college-salary-report-2013. For
each college, reported were x = {Starting salaries} and Y = {Mid-career salaries}
of their alumni. (“Starting” employees were defined as having 5 years or less
of experience in their field, and “Mid-career” employees were defined as hav-
ing 10 years or more of experience.) The data values are available online at

196
STATISTICAL DATA ANALYTICS
http://www.wiley.com/go/piegorsch/data_analytics; a sample is as follows:
x = Starting salary ($):
34 200
30 200
· · ·
49 700
58 300
Y = Mid-career salary ($):
42 300
43 400
· · ·
111 000
137 000
Analyze these data as follows:
(a) Plot Y against x. Does the relationship appear linear? Do any features of the plot
stand out?
(b) Conditioning the analysis on the observed pattern of starting salaries, regress Y on
x via the SLR model in (6.1) and (6.2). Find the raw residuals ei = Yi −̂Yi from the
SLR fit and plot them against xi. What pattern emerges?
(c) Apply a WLS SLR fit to these data. For your weights, set 𝑤i = 1∕xi. Use the results
to test if increasing one’s starting salary also increases the associated mid-career
salary, on average. Operate at 𝛼= 0.05.
(d) Estimate the mean increase in annual mid-career salary associated with a $1000
rise in annual starting salary for these private-college graduates, based on your
WLS–SLR fit in Exercise 6.11c. Also give an approximate, 95% confidence interval
for this value.
6.12
In a review of annual operating characteristics of US natural gas utilities for
calendar year 2011, data on n = 245 natural gas suppliers were collected (Anony-
mous 2012). Among the features recorded were Y = {Miles of gas mains} and
x = {Numbers of customers} for each company. The data are available online at
http://www.wiley.com/go/piegorsch/data_analytics; the following is a sample:
x = Customers:
5 801 000
4 461 363
4 295 741
· · ·
2183
1077
Y = Mains (miles):
497 738
80 159
6567
· · ·
100
48
Examine these data as follows:
(a) Plot Y against x. Does the relationship appear linear? Do any features of the plot
stand out?
(b) Regress Y on x via the SLR model in (6.1) and (6.2). Find the raw residuals
ei = Yi −̂Yi from the SLR fit and plot them against xi. What pattern emerges?
(c) Apply a WLS–SLR fit to these data. For your weights, set 𝑤i = 1∕xi. Use the results
to test if increasing the number of customers increases mean miles of gas mains.
Operate at 𝛼= 0.01.
(d) If two other companies, the first with x = 20 000 customers and the second
with x = 100 000 customers, were to be studied, how many miles of gas mains
would you expect each company would require, based on your WLS–SLR fit in

TECHNIQUES FOR SUPERVISED LEARNING: SIMPLE LINEAR REGRESSION
197
Exercise 6.12c? Also give multiplicity-adjusted, approximate, 99% confidence
intervals for these values.
6.13
Under the bivariate normal model in Section 6.5, find the conditional p.d.f. of Y | X = x
(see Section 2.1.2), and show that this is Y | X = x ∼N(𝜔+ 𝜓x, 𝜏2), with 𝜔= 𝜇Y −
𝜇X𝜓, 𝜓= 𝜌𝜎Y∕𝜎X, 𝜏2 = 𝜎2
Y(1 −𝜌2), and where 𝜇X, 𝜇Y and 𝜎X, 𝜎Y are the population
means and standard deviations, respectively, and 𝜌is the correlation of X and Y.
6.14
As part of an environmental management effort, the UK records and archives data
on residual waste collected per household for n = 383 nationwide communities
(http://data.gov.uk/dataset/ni_151_-_employment_rate).
Reported
are
the
paired
variables X = {Collected waste for calendar year 2008} and Y = {Collected waste
for calendar year 2009} (both in kg/household) for each community. The data are
available online at http://www.wiley.com/go/piegorsch/data_analytics; the following
is a sample:
2008 waste:
1006
995
992
· · ·
386
361
345
2009 waste:
991
835
857
· · ·
346
387
340
To assess potential association between the patterns of generated waste between the
years 2008 and 2009, examine these data as follows:
(a) Plot the paired data. Is any pattern apparent?
(b) Calculate the correlation coefficient (6.29) between the 2008 and 2009 data. What
does it suggest? Is this surprising?
(c) Conduct a test of whether or not the corresponding population correlation coeffi-
cient under the bivariate normal model is zero. Operate at 𝛼= 0.01.
(d) Calculate a 99% confidence interval for the population correlation coefficient under
the bivariate normal model. What does the interval indicate?
6.15
From an ecological study of longleaf pine (Pinus palustris) characteristics, Chen
et al. (2004, Section 8.6) discussed data from a sample of n = 396 trees in a North
American forest. Recoded were the paired variables X = {Diameter at breast height
(in cm)} and Y = {Height (in ft)} for each tree. The data are available online at
http://www.wiley.com/go/piegorsch/data_analytics; The following is a sample:
Diameters (cm):
15.9
22.0
56.9
· · ·
6.0
3.8
8.0
Heights (ft):
28.0
26.0
119.0
· · ·
12.0
3.5
9.0
(a) Examine the original variables X and Y to determine if they appear to exhibit
normal-distribution features. Plot histograms (include rug plots) and construct nor-
mal quantile plots for each variable. Do either or both appear normal?
(b) Calculate the Spearman rank correlation, rS, for these data. One might expect that
diameter and height of the tree are positively associated, so use rS to test this pro-
posal with these data. Operate at 𝛼= 0.01.

7
Techniques for supervised
learning: multiple linear
regression
7.1
Multiple linear regression
The simple linear regression (SLR) model in (6.1) and (6.2) can be extended to accommodate
multiple predictor variables, creating a multiple linear regression (MLR) model. The response
variable Yi remains normally distributed, with
Yi ∼indep. N(𝜇i, 𝜎2),
(7.1)
but now the mean response E[Yi] = 𝜇i is expanded into a function of p fixed predictor variables
xij (j = 1, … , p):
𝜇i = 𝜇(xi1, … , xip) = 𝛽0 + 𝛽1xi1 + · · · + 𝛽pxip.
(7.2)
Here, 𝜇i is modeled as a linear predictor, that is, a linear combination of the p predictor vari-
ables along with a constant ‘intercept’ term 𝛽0. For the latter, we often view its corresponding
‘predictor’ as the constant regressor xi0 = 1.
The 𝛽js now represent p + 1 unknown regression coefficients. For j ≥1, each 𝛽j is inter-
preted as the change in E[Yi] for a unit increase in the corresponding xij – the ‘slope’ of the
jth predictor – assuming all the other x-variables are held fixed. (If it is not possible to vary
one predictor while holding all others constant, this interpretation may not make sense. An
example occurs when the xijs are taken as polynomial terms: xij = x j
i . See Section 7.2.) As
previously, 𝜎2 = Var[Yi] is the unknown, constant variance term.
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
199
7.1.1
Matrix formulation
Some simplification in the description of the MLR model is available by moving to vector
and matrix notation, similar to that presented in Section 2.2. (See Appendix A for a review
of vector and matrix terminology.) The Yis are collected into the n × 1 response vector
Y = [Y1 · · · Yn]T, while the p + 1 predictor values for each ith observation are placed into
individual 1 × (p + 1) row vectors xi = [1 xi1 xi2 · · · xip] (i = 1, … , n). (Notice inclusion
of a leading constant, xi0 = 1, to represent the intercept term.) These are collected into the
design matrix
X =
⎡
⎢
⎢
⎢⎣
x1
x2
⋮
xn
⎤
⎥
⎥
⎥⎦
=
⎡
⎢
⎢
⎢⎣
1
x11
· · ·
x1p
1
x21
· · ·
x2p
⋮
⋮
⋱
⋮
1
xn1
· · ·
xnp
⎤
⎥
⎥
⎥⎦
.
(7.3)
The use of matrix notation also simplifies the larger model formulation. Define the expecta-
tion operator so that it applies to a vector componentwise, that is, E[Y] = [E(Y1) · · · E(Yn)]T.
Similarly, define the covariance matrix of Y as Var[Y] from (2.12). Note that this provides
E[AY] = AE[Y] and Var[AY] = AVar[Y]AT for any conformable, nonstochastic matrix A.
Using this matrix notation, E[Y] is simply [𝜇1 · · · 𝜇n]T. Under (7.2), this translates to the
matrix expression
E[Y] = X𝜷,
(7.4)
where 𝜷= [𝛽0 𝛽1 · · · 𝛽p]T is the (p + 1)-vector of unknown regression coefficients. Each ele-
ment of the expectation vector in (7.4) is simply the mean response (7.2) at the ith observation.
In vector notation, these can be expressed as the linear combination
𝜇(xi) = xi𝜷.
Also, for the MLR model in (7.1), Var[Y] is simply 𝜎2I. (Recall that I = diag{1, 1, … , 1} is
the identity matrix from Section A.1.)
Point estimation again proceeds via least squares (LS). The LS estimator ̂𝜷satisfies the
MLR normal equations, which are (p + 1)-variate extensions of (6.3):
XTX ̂𝜷= XTY
(7.5)
(Kutner et al. 2005, Section 6.3). When the rank of the n × (p + 1) design matrix X is equal
to p + 1, the (p + 1) × (p + 1) inverse matrix (XTX)−1 is well defined. This is used in the LS
solution to (7.5):
̂𝜷= (XTX)−1XTY.
(7.6)
As in the SLR case, under (7.1), this LS estimator corresponds to the maximum likeli-
hood estimate for 𝜷and is again ‘best linear unbiased’ via an extension of the Gauss–Markov
theorem from Section 6.2.1. Also, its sampling distribution under the normal parent model is
again normal: ̂𝜷∼Np(𝜷, 𝜎2(XTX)−1).
The fitted values ̂Yi are collected into the n × 1 column vector ̂Y = X ̂𝜷. Notice, however,
that from (7.6), this is
̂Y = X(XTX)−1XTY,
that is, another linear combination of the Yis (seen also with the SLR case). Write this as
̂Y = HY, where the n × n matrix

200
STATISTICAL DATA ANALYTICS
H = X(XTX)−1XT
(7.7)
is called the hat matrix of the LS fit. Its diagonal elements are denoted by hii, explaining the
notation in (6.24). The hat matrix possesses many interesting qualities; see Kutner et al. (2005,
Section 5.11). For instance, its trace equals the number of 𝛽-parameters: tr(H) = p + 1.
To estimate 𝜎2, mimic the approach taken in the SLR case and start with the residuals
ei = Yi −̂Yi. In vector notation, this is e = Y −̂Y for e = [e1 · · · en]T. Notice that the residuals
may also be written as
e = (I −H)Y,
that is, another linear combination of the Yis. Next, write the residual (error) sum of squares
as SSE = ∑n
i=1 (Yi −̂Yi)2 = eTe = YT(I −H)Y and divide by the corresponding error degrees
of freedom (d.f.), say, dfE, to find the mean squared error (MSE):
MSE = SSE
dfE
=
eTe
n −p −1
(see Exercise 7.7). Notice the change in dfE from the SLR model: in effect, estimation of each
new 𝛽j results in a loss of one additional d.f. for error, so we have gone from dfE = n −2 to
dfE = n −p −1.
The MSE remains unbiased for estimating the variance parameter: E[MSE] = 𝜎2. Also, it
remains statistically independent of ̂𝜷and 𝜒2-distributed (after appropriate scaling):
(n −p −1)MSE
𝜎2
∼𝜒2(n −p −1).
(7.8)
As the covariance matrix of the LS estimator for 𝜷has the particularly simple form
Var[ ̂𝜷] = 𝜎2(XTX)−1, substituting the MSE for 𝜎2 produces the estimated covariance matrix
̂
Var[̂𝜷] = MSE (XTX)−1.
(7.9)
The jth diagonal element of ̂
Var[ ̂𝜷] is the square of the individual standard error se[ ̂𝛽j], while
the (j, k)th off-diagonal element is the estimated covariance ̂
Cov[ ̂𝛽j, ̂𝛽k] (j ≠k).
An MLR extension also can be constructed for the coefficient of determination from
(6.19). Called the coefficient of multiple determination, it has the same structure and form:
R2 = 1 −SSE
SSTo ,
(7.10)
where SSTo = ∑n
i=1 (Yi −Y)2. Similar to its SLR counterpart, R2 here represents the percent-
age variation in Yi that is accounted for by variation in the aggregate collection of p predictor
variables.
7.1.2
Weighted least squares for the MLR model
If each observation Yi in the MLR model can be assigned a heterogeneous weight 𝑤i > 0,
as in Section 6.4, we apply weighted least squares (WLS). In matrix notation, the weights
are collected into a diagonal weight matrix: W = diag{𝑤1, 𝑤2, … , 𝑤n}. For known W, the
weighted normal equations – generalizing (6.28) – are
(XTWX)̃𝜷= XTWY.

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
201
The solution (when the inverse exists) is clearly
̃𝜷= (XTWX)−1XTWY,
(7.11)
extending the LS result in (7.6). The estimated covariance matrix is
̂
Var[̃𝜷] = MSE (XTWX)−1.
As in the unweighted case, the WLS estimator ̃𝜷corresponds to the maximum likelihood
estimator for 𝜷under a normal error assumption and is again ‘best linear unbiased.’ Kutner
et al. (2005, Section 11.1) give further details on WLS estimation for the MLR model.
7.1.3
Inferences under the MLR model
Similar to the SLR setting, inferences on any individual 𝛽j may be constructed to assess
whether the corresponding predictor variable xj is important in modeling E[Yi]. The statistical
features of the LS estimators induce marginal t-distributions for the ̂𝛽js:
̂𝛽j −𝛽j
se[ ̂𝛽j]
∼t(n −p −1),
(7.12)
where se[ ̂𝛽j] is the square root of the jth diagonal element from the estimated covariance matrix
in (7.9). From this, a pointwise 1 −𝛼confidence interval on 𝛽j has the recognizable ‘Wald’
form
̂𝛽j ± t𝛼∕2(n −p −1) se[ ̂𝛽j],
(7.13)
for any single j = 0, … , p.
For simultaneous confidence statements on a prescribed subset of the 𝛽js, extensions of
the Bonferroni approach from Section 6.2.2 may be applied. Suppose the indices comprising
the desired subset of 𝛽js are j1, j2, … , jq (q ≤p + 1). To apply a q-fold Bonferroni correction
to the pointwise limits in (7.13), simply construct the q joint, minimal, 1 −𝛼confidence
statements
̂𝛽j ± t𝛼∕(2q)(n −p −1) se[ ̂𝛽j], for all j = j1, j2, … , jq.
(7.14)
(In effect, this produces a q-dimensional, hyperrectangular, confidence region.)
If the target collection of the 𝛽js cannot be prespecified, so that post hoc inference are
likely, construct the Bonferroni bounds with p + 1 replacing q in (7.14) and then select any
subset of q ≤p + 1 confidence limits. Alternatively, a joint 1 −𝛼confidence ellipsoid can be
built for the entire (p + 1)-vector of regression parameters, extending the two-dimensional
ellipse in (6.21). This is defined by a matrix inequality similar to (5.49)
( ̂𝜷−𝜷)T ̂
Var[ ̂𝜷]−1( ̂𝜷−𝜷) ≤(p + 1)F𝛼(p + 1, n −p −1).
Both approaches are conservative and will produce minimal 1 −𝛼joint confidence bounds on
the post hoc subset. (The 1 −𝛼confidence ellipsoid will be exact when q = p + 1, however.)

202
STATISTICAL DATA ANALYTICS
For a confidence interval on the mean response in (7.2) at any given row vector x =
[1 x1 x2 · · · xp], begin with the LS point estimate
̂𝜇(x) = x ̂𝜷= ̂𝛽0 + ̂𝛽1x1 + · · · + ̂𝛽pxp.
Then, mimic the construction in (6.18) and construct the pointwise 100(1 −𝛼)% t-interval
̂𝜇(x) ± t𝛼∕2(n −p −1) se[ ̂𝜇(x)],
(7.15)
where se[ ̂𝜇(x)] =
√
(MSE)x(XTX)−1xT. For multiple intervals at a prescribed collection of
H ≥1 vectors xh (h = 1, … , H), apply a Bonferroni adjustment to (7.15). This gives the joint,
minimal, 100(1 −𝛼)% hyperrectangle
̂𝜇(xh) ± t𝛼∕(2Hq)(n −p −1) se[ ̂𝜇(xh)],
for all h = 1, … , H. Extending this to all possible vectors x, the WHS method from
Section 6.2.2 gives a simultaneous 1 −𝛼confidence surface as
̂𝜇(x) ±
√
(p + 1)F𝛼(p + 1, n −p −1) se[ ̂𝜇(x)].
This WHS construction may also be applied for post hoc 1 −𝛼confidence statements on any
subcollection of 𝜇(x)s.
Hypothesis tests on the 𝛽j parameters may be similarly extended from the SLR setting.
For testing Ho:𝛽j = 0 against Ha: 𝛽j ≠0, appeal to the t-distribution relationship in (7.12) and
reject Ho when the test statistic
Tj =
̂𝛽j
se[ ̂𝛽j]
(7.16)
exceeds the two-sided critical point t𝛼∕2(n −p −1) in absolute value. The corresponding
P-value is
P = 2P[t(n −p −1) ≥|tjcalc|],
where tjcalc is the observed value of Tj in (7.16). (One-sided tests are similar.) Note that this
tests the significance of the jth predictor variable given that all the other predictor variables
are present in the model. This is termed a partial test of Ho.
The partial t-test from (7.16) is a pointwise test, in that it does not adjust for any other
inferences being performed on the same set of data. For joint, coordinated testing on whole
subsets or subcollections of the 𝛽-parameters, a general strategy is available. Suppose a null
hypothesis fixes a subset of q ≤p + 1 𝛽js equal to zero. For convenience, arrange the predictors
so that this is the final group of q regression coefficients, that is,
Ho∶𝛽p−q+1 = 𝛽p−q+2 = · · · = 𝛽p = 0.
(7.17)
The alternative hypothesis here is that at least one of these 𝛽js is nonzero.
To test Ho in (7.17), we construct a discrepancy measure that quantifies the fit of the
full model (FM) with all p + 1 𝛽-parameters, versus the fit of the reduced model (RM) under

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
203
Ho with only p + 1 −q (nonzero) 𝛽-parameters. A useful discrepancy measure for the MLR
model involves the sum of squared errors (SSE) under the FM. Denote this as
SSE(FM) =
n
∑
i=1
{Yi −̂Yi(FM)}2.
Next, compare SSE(FM) to the SSE calculated when the RM under (7.17) holds:
SSE(RM) =
n
∑
i=1
{Yi −̂Yi(RM)}2.
The two SSEs quantify the relative quality of each model’s fit to the data: if Ho is false, we
expect SSE(RM) to greatly exceed SSE(FM), because the model under which it is fit fails to
include important predictor variables. If Ho is true, we expect SSE(RM) to be roughly equal to
SSE(FM). Notice that SSE(RM) will always exceed SSE(FM), because adding one (or more)
predictor variables to the model always drops the SSE. The issue here is whether the drop
from RM to FM is so large as to call Ho in question.
Corresponding to these terms, also write the error d.f. as dfE(FM) and dfE(RM), respec-
tively. The difference between the two is ΔE = dfE(RM) −dfE(FM). Here, this is ΔE = (n +
q −p −1) −(n −p −1) = q, that is, the number of parameters constrained by Ho.
The test statistic built from this discrepancy measure is
F = {SSE(RM) −SSE(FM)}∕ΔE
SSE(FM)∕dfE(FM)
.
(7.18)
Under Ho in (7.17), F ∼F(ΔE, dfE[FM]). Reject Ho in favor of any departure when Fcalc ≥
F𝛼(ΔE, dfE[FM]). The corresponding P-value is P{F(ΔE, dfE[FM]) ≥Fcalc}.
If the null hypothesis is Ho: 𝛽1 = 𝛽2 = · · · = 𝛽p = 0, the RM is simply E[Yi] = 𝛽0, that is,
no effect due to any regressor variables. This is known as the ‘full’ F-test for the MLR. R
list the full F-test statistic and its associated P-value at the bottom of the summary() output
when applied to an lm object (previously suppressed in output above).
For the discrepancy approach based on (7.18) to be valid, the parameters represented under
RM must be a true subset of those under FM. We say then that the models are ‘nested.’ If the
relationship between RM and FM does not satisfy a nested hierarchy, (7.18) under Ho may
not follow (or even approximate) an F-distribution. The family of models is then said to be
‘separate’ (Cox 1961; 1962); inferences for testing separate families is still a developing area
of data analytic research (Cox 2013).
Notice that when testing Ho in (7.17), one could alternatively apply a simple Bonfer-
roni correction to the collection of q individual t-statistics based on (7.16), with j = p −q +
1, p −q + 2, … , p. This has the advantage of identifying which of the specific 𝛽js are signif-
icant (if any), something the F-statistic in (7.18) cannot supply. Of course, the ‘no-free-lunch
theorem’ (Section 5.5.1) still applies: the q Bonferroni-based inferences will be conservative,
with familywise false positive error (FWE) exceeding 𝛼(and, greatly exceeding it for large
q). If appropriate, consideration could instead be applied to controlling the false discovery
rate (FDR; see Section 5.5.2). Analysts must decide which of these approaches will be most
suitable on a case-by-case basis in consultation with the domain-specific expert(s).
Example 7.1.1 Cancer mortality multiple regression. Consider again the UK cancer mor-
tality data from Example 6.2.1. Recall that the SLR fit on Y = log{Mortality} with the single

204
STATISTICAL DATA ANALYTICS
predictor variable x = {Employment rate} produced a highly significant regression coefficient
and also a relatively low value of R2. This suggested that additional factors might be impor-
tant in modeling E[Y]. Towards that end, a series of further locality-specific predictor variables
were also recorded as part of the UK government’s data collecting effort. Including the original
employment predictor, these were
x1 = Employment rate (average quarterly percentage of persons over 16 gainfully
employed)
x2 = Median weekly earnings (£)
x3 = Unpaid volunteering (percentage of citizenry participating once or more per month)
x4 = Civic activity (percentage of citizenry participating in local decision-making)
x5 = Smoking cessation (persons per 100 000 over 16 who stopped smoking in past 4
weeks)
x6 = Emergency bed-days (number of beds × days in use at hospital emergency rooms)
Predictors x1 and x2 represent economic factors, predictors x3 and x3 represent lifestyle factors,
and predictors x5 and x6 represent health factors, all associated with socioeconomic activity in
each locality. Table 7.1 presents the data. (As above, only a selection of the data is given in the
table. The complete set is available at http://www.wiley.com/go/piegorsch/data_analytics.)
Table 7.1
Selected data from a larger set of n = 342 observations recorded
in localities throughout the United Kingdom in 2008.
Yi
xi1
xi2
xi3
xi4
xi5
xi6
47.20
843.8
25.7
90.3
524
23.6
2 419
152.27
535.9
13.4
67.7
1277
16.0
102 700
⋮
⋮
⋮
⋮
⋮
⋮
⋮
92.75
375.8
16.2
78.9
684
30.6
51 450
106.72
418.5
12.3
72.6
684
23.6
51 603
See Example 7.1.1 for Y- and x- variable descriptions.
An initial step in the analysis of these data is visual inspection: Figure 7.1 displays a
scatterplot matrix with all the variables. A variety of patterns are evidenced, some of which
suggest strong potential for the additional variables to impact the mean response.
To study this further, apply the MLR model in (7.1) to Yi with the p = 6 predictor variables
in Table 7.1. In R, the command for an MLR fit is again lm(), with syntax extended after the
tilde (˜) to include the additive variables. For example,
> mlrFM.lm <- lm( Y ˜ employmt + earnings + volunteer
+ civic + stopsmk + emergbeds )
> summary( mlrFM.lm )
The call to summary() produces the following R output (edited):
Call:
lm(formula = Y ˜ employmt + earnings + volunteer + civic
+ stopsmk + emergbeds)

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
205
Coefficients:
Estimate
Std. Error
t value
Pr(>|t|)
(Intercept)
5.714e+00
1.411e-01
40.489
< 2e-16
employmt
-7.500e-03
1.689e-03
-4.439
1.23e-05
earnings
-4.369e-04
1.063e-04
-4.108
5.01e-05
volunteer
-1.142e-02
2.415e-03
-4.730
3.32e-06
civic
-5.110e-03
3.095e-03
-1.651
0.099656
stopsmk
1.015e-04
2.956e-05
3.433
0.000671
emergbeds
6.320e-08
5.844e-08
1.082
0.280229
logMortality
60
75
90
15
25
35
500
1500
4.0
4.4
4.8
60
70
80
90
employmt
earnings
300
500
700
15
25
35
volunteer
civic
10
15
20
25
500
1500
stopsmk
4.0
4.6
300
600
10
20
0e + 00
6e + 05
0e + 00
4e + 05
emergbeds
Figure 7.1
Scatterplot matrix for data in Example 7.1.1. Variables are Y = log{Mortality},
x1 = {Employment rate}, x2 = {Weekly earnings}, x3 = {Volunteering %}, x4 = {Civic par-
ticipation %}, x5 = {Smoking cessation rate}, and x6 = {Emergency bed-days}. Source: Data
from http://data.gov.uk/dataset/ni_122_-_mortality_from_all_cancers_at_ages_under_75.

206
STATISTICAL DATA ANALYTICS
Residual standard error: 0.1163 on 335 degrees of freedom
Multiple R-squared: 0.4639,
Adjusted R-squared: 0.4543
F-statistic: 48.31 on 6 and 335 DF,
p-value: < 2.2e-16
The analysis identifies a significant regression effect: the full F-test of
Ho∶𝛽1 = 𝛽2 = · · · = 𝛽6 = 0
(7.19)
shows a highly significant P < 2.2 × 10−16 (see the bottom of the output). Indeed, most of the
predictor variables appear to significantly impact log-mortality: two-sided P-values from the
(pointwise) partial t-tests on each variable are given in the final column of the Coefficients
output, under Pr(>|t|).
To formalize this indication, consider testing (7.19) via partial t-tests on each Ho:𝛽j =
0, j = 1, … , 6 (i.e., ignoring the intercept), including a Bonferroni correction for the sixfold
multiplicity. To do so, simply multiply the pointwise partial P-values by the number of com-
parisons, here q = 6. (If the multiplication drives an adjusted P-value past 1.0, report it as P
= 1.0.) The consequent, Bonferroni-adjusted P-values appear in Table 7.2, calculated using
the R code
> p1 <- length( coef(mlrFM.lm) )
> p.adjust( summary(mlrFM.lm)$coefficients[,4][2:p1],method=‘bonf’ )
Table 7.2
Bonferroni-adjusted P-values for testing
Ho:𝛽j = 0 (j = 1, … , 6) with cancer mortality data
from Example 7.1.1.
Predictor variable
Adjusted P-value
x1 = Employment rate
P1 = 7.36 × 10−5
x2 = Weekly earnings
P2 = 0.0003
x3 = Volunteering %
P3 = 1.99 × 10−5
x4 = Civic activity %
P4 = 0.5979
x5 = Smoking cessation rate
P5 = 0.0040
x6 = Emergency bed-days
P6 = 1.0
At a 5% false positive rate, only the adjusted P-values for x4 = {Civic activity} and x6 =
{Emergency bed-days} in Table 7.2 fail to show significant differences from Ho:𝛽j = 0. Thus
we conclude that x2 = {Weekly earnings}, x3 = {Volunteering %}, and x5 = {Smoking ces-
sation} show significant effects, along with x1 = {Employment rate}.
For finer resolution, we refit the MLR model with just the p = 4 significant variables. In
order to keep the subscripts consistent, redesignate x5 as x′
4 = {Smoking cessation}.
In R, the MLR fit is achieved via
> mlrRM.lm=lm( Y ˜ employmt + earnings + volunteer + stopsmk )
or, apply the update() function. The consequent output (edited) from summary() is
Call:
lm(formula = Y ˜ employmt + earnings + volunteer + stopsmk)

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
207
Coefficients:
Estimate Std. Error t value Pr(>|t|)
(Intercept)
5.672e+00
1.324e-01
42.834
< 2e-16
employmt
-6.544e-03
1.517e-03
-4.315 2.10e-05
earnings
-5.041e-04
9.939e-05
-5.072 6.53e-07
volunteer
-1.439e-02
1.673e-03
-8.601 3.02e-16
stopsmk
1.082e-04
2.940e-05
3.679 0.000272
Residual standard error: 0.1167 on 337 degrees of freedom
Multiple R-squared: 0.4573,
Adjusted R-squared: 0.4508
F-statistic: 70.99 on 4 and 337 DF,
p-value: < 2.2e-16
Notice that the R2 value for the MLR fit has risen considerably from the earlier SLR fit in
Example 6.2.3: 45.73% versus 27.38%, respectively.
From the signs of the estimated regression coefficients, the economic variables, Employ-
ment rate and Weekly earnings, and the remaining lifestyle variable, Volunteering %, decrease
log-mortality as each rises (with all others held fixed), as might be expected. The remaining
health variable, Smoking cessation, increases log-mortality as it rises, however. This seems
counterintuitive: as more individuals stop smoking, one might expect the log-mortality to
drop. One possible speculation here is that a high smoking cessation rate may indicate a
community with a larger inceptive population of smokers, and with more smokers, there
are more cancer deaths. Also, the crude rate does not take account for recidivism among
individuals who quit but then return to smoking, which could eventually increase observed
cancer mortality. The perplexing result from this data-mining exercise is cause for further
investigation (and possible knowledge discovery) into how various factors affect cancer
mortality for these British communities.
The next example continues study of these data with analysis of the residuals from the
four-predictor MLR fit.
◽
Example 7.1.2 Cancer mortality multiple regression (Example 7.1.1, continued).
Continue with the MLR analysis of the UK cancer mortality data from Example 7.1.1. No
analysis is complete without a check of the model fit via study of the residuals. Figure 7.2
displays a normal quantile plot of the raw residuals, along with a plot of Studentized deleted
residuals, from the final four-variable fit in Example 7.1.1. Overlayed in the Studentized
residual plot are exceedance bounds that mark the t-critical points ±t𝛼∕(2n)(n −p −2)
from (6.26) to identify unusual observations. At 𝛼= 0.05, the pertinent critical points are
±t0.05∕684(336) = 3.8415.
The normal quantile plot in Figure 7.2 mimics that seen with its p = 1 SLR cousin in
Figure 6.7. The pattern appears generally consistent with a normal distribution, but three
unusual points appear in the lower tail of the distribution. As might be expected, these are
the same three potential outliers identified in Example 6.3.1: the City of London, the RBKC,
and the City of Westminster. The corresponding Studentized deleted residual plot gives similar
corroboration, although now only two points drop below the lower 5% exceedance level: the
City of London and the RBKC. The City of Westminster’s Studentized deleted residual now
lies just above the lower exceedance line. Nonetheless, the indications from Example 6.3.1
for these three communities remain valid: they appear to exhibit different, and potentially
intriguing, features regarding log-cancer mortality.
◽

208
STATISTICAL DATA ANALYTICS
−3
−2
−1
0
1
2
3
−0.4
−0.2
0.0
0.2
Theoretical normal quantiles
(a)
(b)
Sample quantiles
4.4
4.5
4.6
4.7
4.8
4.9
5.0
−4
−2
0
2
4
Fitted value
Studentized deleted residual
Figure 7.2
(a) Normal quantile plot for raw residuals with overlaid normal reference
line and (b) Studentized deleted residual plot from MLR fit in Example 7.1.2. Vari-
ables are Y = log{Mortality}, x1 = {Employment rate}, x2 = {Weekly earnings}, x3 =
{Volunteering %}, and x′
4 = {Smoking cessation rate}. Dashed lines in (b) are 5%
exceedance levels for Studentized deleted residuals, ±t0.05∕684(336) = 3.8415. Source: Data
from http://data.gov.uk/dataset/ni_122_-_mortality_from_all_cancers_at_ages_under_75.
7.1.4
Multicollinearity
An important consideration when employing MLR models is the differing contributions of
the various x-variables. It can sometimes be the case that information in one of the pre-
dictors closely duplicates or overlaps with information in another. The effect is known as
multicollinearity. In the extreme, one predictor may be a strict linear function of another, say,
xim = 𝛾0 + 𝛾1xij at all i for some j ≠m. Then in the MLR model equation (7.2),
𝜇i = 𝛽0 + 𝛽1xi1 + · · · + 𝛽jxij + · · · + 𝛽mxim + · · · + 𝛽pxip
= 𝛽0 + 𝛽1xi1 + · · · + 𝛽jxij + · · · + 𝛽m(𝛾0 + 𝛾1xij) + · · · + 𝛽pxip.
But then, the mean response becomes
𝜇i = (𝛽0 + 𝛽m𝛾0) + 𝛽1xi1 + · · · + (𝛽j + 𝛽m𝛾1)xij + · · · + 𝛽pxip,
that is, xim drops from the model, changing the intercept to 𝛽0 + 𝛽m𝛾0 and the regression coeffi-
cient for xij to 𝛽j + 𝛽m𝛾1. Thus there is no need to fit xim. Its contribution is already represented
in 𝜇i. (The same effect occurs if xim is a linear combination of two or more other predictors
already in the model. In matrix terms, this sort of perfect collinearity drops the column rank
of X below p + 1, making XTX singular and impossible to invert.) When this occurs, we say
xim is aliased with the other predictor(s), because it replicates existing information already
contained in 𝜇i. Most modern linear regression programs will identify an aliased predictor
and remove it before fitting the model.

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
209
Even when no aliasing is present, one (or more) of the predictors may closely approximate
information in the others. When this occurs, the strong multicollinearity leads to a variety of
undesirable instabilities with the LS estimators for ̂𝜷(Kutner et al. 2005, Section 7.6).
A number of strategies exist to identify multicollinearity in a set of posited predictor vari-
ables. A convenient way to visualize overlaps between the predictor variables is to build a
scatterplot matrix among all the predictors. Clear redundancies will appear as strong patterns
in the individual pairwise plots. Correspondingly, calculation of the pairwise correlations
between the predictor variables – via, for example, the cor() function in R – will give a
similar numerical indication, at least for any linear associations.
A more-formal numerical diagnostic is known as the variance inflation factor (VIF). When
heavy multicollinearity exists among a set of predictor variables, the variances (and their
square roots, the standard errors) of the LS estimators inflate. This increases the width of
any confidence regions on 𝜷and 𝜇(x) and decreases the power of any associated hypothesis
tests. The VIFs are used to measure how much each predictor contributes to variance inflation
(Kutner et al. 2005, Section 10.5). A VIF is calculated for each jth predictor as
VIFj =
1
1 −R2
j
,
(7.20)
where R2
j is the coefficient of multiple determination found from regressing the jth predictor,
xij, on the other p −1 predictor variables (j = 1, … , p). In R, the vif() function from the
external car package computes VIFs.
In practice, a set of predictors whose maximum VIF exceeds 10 is felt to be highly multi-
collinear. Also check the mean, VIF = ∑p
j=1 VIFj∕p, which should not grow far above 1. (In
the latter case, recommendations vary, although a VIF greater than about 6 or 7 is usually
cause for concern.)
Example 7.1.3 Cancer mortality multiple regression (Example 7.1.1, continued).
Consider again the UK cancer mortality data from Example 7.1.1. The MLR fit on
Y = log{Mortality} employed the p = 4 predictor variables x1 = {Employment rate}, x2 =
{Weekly earnings}, x3 = {Volunteering %}, and x′
4 = {Smoking cessation rate}. Focusing on
pertinent components of the scatterplot matrix in Figure 7.1 shows that these four predictors
display a variety of pairwise patterns, with mostly diffuse pairwise spreads. The pair x1 and
x′
4 exhibit perhaps the most substantial linear relationship in the scatterplot – suggesting the
largest potential overlap – but even here the dispersion is fairly large.
Turning to the VIFs, the following sample R code and output give the individual values
and their average
> library( car )
> vif( mlrRM.lm )
employmt
earnings volunteer
stopsmk
1.712690
1.086523
1.372481
1.438172
> mean( vif(mlrRM.lm) )
[1] 1.402467
None of the VIFs exceed 1.8, so their maximum is well below the practical action limit of 10.
Their mean is also well below 6, with VIF = 1.4. In general, this analysis indicates no serious
concerns regarding multicollinearity with this suite of predictor variables.
◽

210
STATISTICAL DATA ANALYTICS
Remedies for multicollinearity are primarily related to predictor variable selection and
design (and to a certain extent, simple common sense):
• To the best extent possible, identify predictors that provide separate domain-specific
information (this may seem obvious, but is nonetheless worth stating).
• Avoid inclusion of predictors that reproduce very similar information (e.g., hospital
admissions and hospital bed-days in a health care utilization study).
• Always try to limit the number, p, of predictor variables to a manageable total. As more
variables are added to an MLR model, opportunities for multicollinearity will naturally
increase.
One can also try transforming highly collinear predictor variables (cf. Section 3.4.3) to
decrease their VIFs. The logarithm is fairly popular in this regard, although its success rate
can vary. Alternatively, applying methods of multivariate data reduction to the set of predictors
may make them more amenable for regression modeling. A popular data reduction approach is
principal component analysis (PCA), discussed in Section 10.2. This leads to a methodology
known as principal component regression (Hastie et al. 2009, Section 3.5.1). Or, if the focus is
on prediction of 𝜇i, a form of penalized optimization known as ridge regression can be applied
to address effects of multicollinearity; this is discussed in Section 7.4.2.
Note that the VIFs are determined computationally from only the xijs, so they can be
computed before acquisition of the responses Yi. Thus, where possible, it is good practice to
check the VIFs for multicollinearity before final data acquisition. Predictors exhibiting high
multicollinearity can be reassessed as to their anticipated value to the analysis and removed
or transformed if felt to be marginal or nonessential.
7.2
Polynomial regression
A special case of the MLR mean response in (7.2) that employs only a single predictor, x, is
the polynomial regression model
𝜇i = 𝜇(xi) = 𝛽0 + 𝛽1(xi −x) + 𝛽2(xi −x)2 + · · · + 𝛽p(xi −x)p.
(7.21)
The polynomial form is useful when the mean response is affected by x in a manner more
complex than described by simple linear relationships. Centering the predictor about its mean
x helps to avoid potentially high multicollinearity in the multiple regression (Bradley and Sri-
vastava 1979) and is commonly instituted. If the values of xi can be chosen in advance, special
forms called orthogonal polynomials exist, which can eliminate multicollinearity among the
polynomial regressors. See, for example, the review by Narula (1979).
Continue to assume Yi ∼indep. N(𝜇[xi], 𝜎2), i = 1, … , n. Also assume that p + 1 < n
and that at least p + 1 distinguishable values exist among the xis. (These latter assumptions
are not unreasonable, because values of p greater than 3 are often difficult to motivate from
subject-matter arguments.) In practice, it usually does not make sense to include a higher-order
term without also including all the lower orders. Thus whenever a certain order of polynomial,
p, is specified in a regression model, all the lower order terms at p −1, p −2, … , 2, and 1
are generally also included.
Under these assumptions, the statistical machinery employed for fitting and analyzing
an MLR becomes available for fitting and analyzing polynomial regression models, with

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
211
xij = (xi −x)j. LS estimators, standard errors, tests and confidence intervals, regression
diagnostics, and so on all follow using the general MLR equations described in the previous
section. This underlies much of the appeal for the use of polynomial models: although
curvilinear regression relationships are often more complex than can be described by a
simple polynomial function, (7.21) may provide an approximation to the true nonlinear
relationship over the range of x under study (Piegorsch and Bailer, 2005, Section 1.5).
This applies even for the SLR case of p = 1. Accepting the value of the approximation,
the various methods of MLR analysis may then be applied for studying the curvilinear
response. For instance, to test if there is any effect on E[Y] due to the x-variable, assess Ho∶
𝛽1 = 𝛽2 = · · · = 𝛽p = 0 via an F-statistic as in (7.18). Or, partial tests on the individual
regression coefficients follow from standard t-statistics. For instance, if p = 2, a test of
Ho:𝛽2 = 0 using T2 = | ̂𝛽2|∕se[ ̂𝛽2] assesses whether quadratic curvature is required in a
model that already has a linear term present. As mentioned at the beginning of Section 7.1,
however, in the polynomial context, 𝛽j loses its interpretation as change in response with the
jth predictor when holding the other predictors fixed. Clearly, one cannot vary (xi −x)j while
holding (xi −x), … , (xi −x)j−1, (xi −x)j+1, … , (xi −x)p fixed.
Exercises 7.10–7.12 explore features of polynomial regression in more detail.
7.3
Feature selection
In many informatic studies with multiple predictor variables, the number of potential x-
variables can become quite large. It is natural in such studies to search for possible predictors
that represent pertinent features associated with the supervised learning effort and include
these in the multiple regression equation. Also called attribute section or variable selection,
the identification of important predictors is a common component of the exploratory
model-building exercise.
Combinations of the predictors can also be assessed, including higher-order polynomial
terms such as x2
j and cross-product terms such as xjxk. (The cross-products are often called
‘interactions.’) Note that, as in Section 7.2, it does not usually make sense to include a
higher-order term in a model without also including all its lower-order siblings. Thus, for
example, given predictors x1, x2, and x3, if next including x4 = x2x3, one should typically
retain x2 and x3. Similarly, if including x5 = (x3 −x3)2, one should retain x3, and so on. Many
of these may turn out to be uninteresting, however, in that they contribute insignificantly
to modeling the mean response. Substantial multicollinearity may also exist among large
collections of x-variables, as they can often overlap in the predictive information they provide.
Thus, the selection effort must not be conducted in a cavalier manner.
Clearly, when prior, domain-specific knowledge indicates that certain predictor variables
are expected to impact the mean response, they should be included and assessed in the MLR
model fit. When reason for doubt exists about some of the potential predictors, however, and/or
when the analysis has more of an exploratory and less of a confirmatory character, specifica-
tion and selection of predictor variables can itself become part of the exploratory effort. Of
course, determination of the final set of regressors should be made via careful and purposeful
analysis of the data; selecting predictor variables for further study is only a first step in con-
structing the MLR model. (Any consequent inferences must also be viewed as conditional on
the selection effort. If the selection process involves some form of formal inferences, these
can disrupt the final, unconditional (familywise) error rates or confidence levels. Berk et al.
(2013) discussed some of the theoretical complexities associated with making inferences after
variable selection; also see Potscher (1991).)

212
STATISTICAL DATA ANALYTICS
7.3.1
R2
p plots
Perhaps the simplest strategy for regression variable selection is to quantify the value of each
potential variable configuration and then choose that configuration that optimizes some fixed
score or metric, say, the coefficient of multiple determination from the MLR fit, R2, in (7.10).
Suppose there are Q > 1 distinct variable configurations under consideration, each with a
different combination of one or more potential predictors. Assume an intercept term, 𝛽0, is
always included in the model, so that the total number of regression coefficients being esti-
mated is p + 1 ≥2. (The requirement to always include 𝛽0 is made for convenience and if
relaxed, does not substantively affect the methods described later.) For example, at p = 1, the
models all contain an intercept and a single predictor xj; at p = 2, they all contain an intercept
and either two predictors xj and xk (j ≠k) or if desired, a predictor xj and its mean-adjusted
square (xj −xj)2; at p = 3, they contain three predictors, xj, xk, andxℓ, or two predictors and
one of their squares, or two predictors and their cross-product, and so on.
Let R2
[p] denote the optimum score for R2 among all variable configurations under con-
sideration with exactly p terms based on the predictor variables and any functions of them,
along with an intercept 𝛽0, in the model for 𝜇i. (For R2, the optimum is always the largest
R2 at that p, but other measures may require the smallest value; see the following text.) By
adding more terms and increasing p, we expect R2
[p] also to rise because as noted in Section
7.1.3, the corresponding SSE[p] in (7.10) will always drop. For large-enough p, however, the
drop in SSE and consequent rise in R2 will likely become slight, even infinitesimal. This will
appear on a plot of R2
[p] versus p as a leveling of the R2
[p] curve with increasing p. If the point of
this ‘diminishing return’ is visually obvious, select the variable configuration corresponding
to the maximum R2 at that p.
Example 7.3.1 Cancer mortality multiple regression (Example 7.1.1, continued). Return
to the UK cancer mortality data in Example 7.1.1. The MLR fit on Y = log{Mortality}
previously employed the p = 4 predictor variables x1 = {Employment rate}, x2 = {Weekly
earnings}, x3 = {Volunteering %}, and x′
4 = {Smoking cessation rate}. Now consider as
additional predictors the associated quadratic terms (xj −xj)2 (j = 1, … , 4), along with
all six second-order cross-products (interactions) xjxk (j ≠k). This produces a suite of 14
possible predictor variables and, including the intercept, up to p = 15 regression parameters
to be estimated from the data.
For selecting among the consequent variable configurations, apply the R2 measure as
described earlier and construct an R2
[p] plot to inspect the pattern as p increases. To facilitate
the computations, R provides the leaps() function in the external leaps package. This
finds all possible R2
p values from input of the observation vector and the design matrix
of all constructed predictor variables. Here, given the additional, second-order predictors
x1sq=(x1-mean(x1))^2, … , x12=x1*x2, … , and so on, sample R code is
> require( leaps )
> Xmtx <- cbind( x1,x2,x3,x4,x1sq,x2sq,x3sq,x4sq,
x12,x13,x14,x23,x24,x34 )
> mlrBaseFM.r2 <- leaps( x=Xmtx, y=Y, method=‘r2’ )
> plot( mlrBaseFM.r2$r2 ˜ I(mlrBaseFM.r2$size - 1) )
which yields the R2
p plot. (The size attribute in a leaps object is the total number of
regression parameters, p + 1, so the code in the plot command above subtracts 1 from it.

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
213
Also included is the ‘Identity’ function I() to protect the direct subtraction operation.) The
consequent maximum values R2
[p] can be isolated via, for example,
by( data=mlrBaseFM.r2$r2,
INDICES=factor(mlrBaseFM.r2$size - 1), FUN=max )
Connecting these with line segments produces Figure 7.3. As expected, one sees a sharp rise
when moving from p = 1 to multiple predictors and then a flattening as p increases further. By
about p = 5, R2
[p] has clearly leveled out. This suggests that an MLR model with five predictors
(and an intercept, so size = p + 1 = 6) produces about as much practical improvement in R2
as can be extracted from these data.
0.1
0.2
0.3
0.4
0.5
p
R2
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Figure 7.3
Variable selection plot of R2
p versus p for MLR data in Example 7.3.1. Variables
are Y = log{Mortality}, x1 = {Employment rate}, x2 = {Weekly earnings}, x3 = {Volun-
teering %}, and x′
4 = {Smoking cessation rate}, plus all quadratic terms and second-order
cross-products. Solid line connects values of maximum values R2
[p]. Source: Data from
http://data.gov.uk/dataset/ni_122_-_mortality_from_all_cancers_at_ages_under_75.
Closer inspection of the data set identifies R2
[5] = 0.4827, with corresponding predictors
x3, x′
4, (x2 −x2)2, x1x2, and x1x′
4. (Find this selection using
> maxR2 <- max( mlrBaseFM.r2$r2[mlrBaseFM.r2$size==6])
> index <- which( mlrBaseFM.r2$r2 == maxR2 )
> mlrBaseFM.r2$which[index,]
from the leaps object.) As x2
2 and x1x2 are components of the selected configuration, one
should override removal of the original x1 and x2 variables and also include these in any

214
STATISTICAL DATA ANALYTICS
further MLR fit. In fact, from the mlrBaseFM.r2 object, we find that this configuration has
the highest R2 at p = 7, with R2
[7] = 0.4871.
Exercise 7.13 explores this MLR analysis further, now with the seven predictor variables
x1, x2, x3, x′
4, (x2 −x2)2, x1x2, and x1x′
4.
◽
The R2 measure’s predisposition to increase as variables are added to the model is often
viewed as a drawback, because it allows a naïve analyst to increase the measure simply by
adding irrelevant or arbitrary variables. To correct for this, we can adjust R2 to use mean
squares instead of sums of squares. Known colloquially as the adjusted R2, the corrected
measure divides each sum of squares in (7.10) by its corresponding d.f.:
R2
A = 1 −SSE∕(n −p −1)
SSTo∕(n −1) .
(7.22)
With this, let R2
A[p] be the largest value of R2
A among all variable configurations under con-
sideration containing p regression parameters (and an intercept). The adjustment in (7.22) no
longer increases strictly with p, necessarily, although it often follows a roughly increasing
pattern when plotted against p. As with the R2
[p] plot, visual inspection of R2
A[p] versus p can
identify a point of ‘diminishing return.’ The corresponding collection of regressor variables
may then be selected for further study. (See Exercise 7.13.) In R, use the leaps() function
with the method=‘adjr2’ option.
A variety of other, more-sophisticated diagnostic scores have been constructed to measure
the quality of an MLR fit to which a graphical inspection strategy may be applied. For example,
suppose the collection of potential predictor variables contains P > 1 candidates, including
polynomial powers and cross-product interactions. Mallows (1973) suggested a measure using
the MSE from a baseline, ‘full’ model with all the P predictors (and the intercept), MSEP. He
compared MSEP with the SSE of an aspirant RM (which may or may not contain an intercept)
with p ≤P + 1 terms, SSEp, via what we now call Mallows’ Cp statistic
Cp =
SSEp
MSEP
−(n −2p).
Predictor configurations that describe the mean response well correspond to E[Cp] ≈p, while
poor-fitting configurations produce large values well above p; the measure decreases towards
p as the explanatory quality improves. Thus a value of Cp close to but not greatly exceeding
p identifies a collection of model terms for further study. Plotting Cp versus p again gives a
visual diagnostic for such selection: choose predictor configuration(s) at the smallest p such
that Cp ≈p, without greatly exceeding it. To find Cp in R, use the leaps() function with the
method=‘Cp’ option.
Or, focusing on prediction of a future Yi via the case-deletion strategy mentioned in
Section 6.3, Allen (1974) constructed a prediction sum of squares,
PRESSp =
n
∑
i=1
(Yi −̂Yi[−i]
)2,
where ̂Yi[−i] is the ith predicted value under the aspirant configuration of p terms when the ith
observation is deleted from the data. If prediction is an important goal of the MLR analysis,
variable configurations with a low PRESSp become candidates for more-focused analysis. As
above, plotting PRESSp against p provides a facilitative visual device. For a further discussion
on this and related aspects of MLR variable selection, see Kutner et al. (2005, Section 9.3).

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
215
7.3.2
Information criteria: AIC and BIC
A popular criterion in contemporary data analytics for variable and model selection focuses on
maximizing statistical information in the fitted model. Here ‘information’ is used in a formal
sense and is different from (but related to) the sample entropy in Section 3.4.2 and the Fisher
information in Section 5.1. For selecting models with p predictor variables, define the Akaike
information criterion (AIC; Akaike 1973) as
AICp = −2̂ℓp + 2𝜈p,
(7.23)
where ̂ℓp = ℓ( ̂𝜷p) is the value of the maximized log-likelihood function (Section 5.1) under
the model with (p + 1) × 1 parameter vector 𝜷p, and 𝜈p ≥1 is the number of unknown param-
eters being estimated when fitting that model. For example, if the model contains p predictors
along with an intercept and an unknown variance term, 𝜈p = p + 2. Then, let AIC[p] denote
the optimum value of AICp (here, the smallest) among all considered variable configurations
containing p regressors.
In passing, note that (7.23) is the ‘lower-is-better’ form of the AIC. Some authors multiply
(7.23) by −1 or −1
2 to achieve a ‘higher-is-better’ form. Analysts must be careful to expressly
identify which form of AIC under which they operate.
In effect, the AIC begins by assessing the maximized log-likelihood for each competitor
model (a natural criterion for defining the quality of a parametric model). Then, it multiplies
by −2 to view the process as a minimization and finally adds a positive ‘penalty’ term that
accounts for the number of parameters used to achieve the model fit. In most cases – not lim-
ited to the MLR setting – adding variables to a model improves the fit and increases the max-
imized likelihood, but sometimes only slightly. The AIC penalizes those models that achieve
high likelihoods simply by including a ‘kitchen sink’ full of inconsequential parameters.
Given a set of models representing different variable configurations, an AIC-based
model-selection strategy computes the optimal (smallest) AIC, AIC[p], from (7.23) at each
p and then selects that model for which it is lowest across all p. For the MLR setting, AICp
will order potential variable configurations identically to Mallows’ Cp. No form of sampling
distribution is involved, so there are no formal hypotheses to test or P-values to calculate.
A wide variety of other measures exist that compete with the AIC, most of which
modify the penalty term in (7.23) for different levels of selectivity. A simple adjustment for
small-samples known as the corrected AIC, or AICc, takes
AICcp = AICp +
2𝜈p(𝜈p + 1)
n −𝜈p −1
(7.24)
(Hurvich and Tsai 1989). For the large data sets seen in modern data analytics, n will often
exceed 𝜈p by many orders of magnitude, so that the AICp term in (7.24) will dominate. If so,
AIC and AICc produce essentially the same results with very large n.
Another popular competitor is the Bayesian information criterion (BIC) due to
Schwarz (1978):
BICp = −2̂ℓp + 𝜈p log(n).
(Some authors use the alternative moniker Schwarz’s Bayesian Criterion, or SBC. Notice that
BICp = AICp + 𝜈p(log{n} −2).) Since its penalty term weights 𝜈p more heavily than the AIC
for most values of n, the BIC tends to favor models with smaller numbers of parameters. This
is seen by many to be a favorable property.

216
STATISTICAL DATA ANALYTICS
Many other ‘information criterion (IC)’ variants exist, with sometimes-different and
sometimes-overlapping features for assessing the quality of a model. These include the
Takeuchi information criterion (TIC; Takeuchi 1976), the Kullback information criterion
(KIC; Cavanaugh 1999), the deviance information criterion (DIC; Spiegelhalter et al. 2002),
the focused information criterion (FIC; Claeskens and Hjort 2003), and many more. (One
might say that an entire ‘alphabet soup’ of ICs has been, and continues to be, developed.)
Final determination of which IC to employ is in the hands of the data analyst and should be
made with the eventual, specific target inference(s) in mind. Either of the originals, AIC or
BIC, can serve as useful defaults when no domain-specific motivation guides this choice.
In R, the AIC() function computes AIC for a variety of model classes and R objects. A
similar BIC() function exists for finding the BIC. Objects may even be nested in the command
to compare their AICs. As help(AIC) warns, however,
The log-likelihood and hence the AIC/BIC is only defined up to an
additive constant. Different constants have conventionally be used
for different purposes...
Thus, analysts should study the function structure and output carefully to ensure that the
desired quantities are being produced. The next section discusses ways to approach this via
automated selection.
7.3.3
Automated variable selection
As the number of possible regressors, p, increases, the number of possible variable configu-
rations, 2p, grows with it exponentially. Analysts run the risk of overfitting the data: a model
with too many predictor variables will suitably accommodate many of the particular features
in a given set of training data, but usually at the cost of poor predictive ability with future
data sets.
To assuage these concerns, we can turn to automated or semiautomated variable selection
procedures. These systematically study all possible models – or some prespecified subgroup,
such as all possible second-order models – available from a given set of original predictor
variables. Then, an operating subset of the larger set of variables is chosen that optimizes some
preselected score. The result is one (or more) putative models that the analyst can evaluate
further in detail.
One such strategy aims to find a few (usually one or two) of the best-performing predictor
configurations at each p, so that the analyst can focus attention on these various subsets. In
this sense, the process is known as (best) subset selection. ‘Best’ here is usually defined as
minimization of the SSE for a fixed p; the largest desired value of p and the maximum number
of output configurations per p are specified in advance.
In a sense, the R2
p plot and R’s leaps() function from Section 7.3.1 imitate a subset
selection strategy, although, as presented above, only one configuration is chosen per p (cor-
responding to the largest R2 at each p) and the analyst participates more directly in selection
via the visualized plot.
To limit the (likely) large number of calculations required in automated subset selection,
clever computational algorithms have evolved to flag the best subsets more efficiently. One
of the best known is leaps and bounds (Furnival 1971; Furnival and Wilson 1974), a form of
branch-and-bound algorithm (see Gatu and Kontoghiorghes 2006). Computationally, it can be
quite efficient for values of p up to about 35 or 40.

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
217
Subset selection is available in R via a number of external packages. For instance, the leaps
package offers its regsubsets() function, which provides graphical outputs as a pathway
for visualizing large configurations of potentially best models.
Example 7.3.2 Cancer mortality multiple regression (Example 7.3.1, continued). Return
to the UK cancer mortality data and the variable selection operations in Example 7.3.1. Recall
that for the MLR fit on Y = log{Mortality}, the single predictor variables x1 = {Employ-
ment rate}, x2 = {Weekly earnings}, x3 = {Volunteering %}, and x′
4 = {Smoking cessation
rate} were augmented with the additional, second-order predictors (xj −xj)2 (j = 1, … , 4),
and xjxk (j ≠k). Suppose we wish to conduct subset selection on this collection of 14 predic-
tor variables, where the subsets contain no more than p = 7 predictors (for comparison with
the results in Example 7.3.1). Also, we ask the computer to provide the two best subsets at each
p. (As above, we always include the intercept term.) Sample R code using the regsubsets()
function from the external leaps package is
> require( leaps )
> mlrBaseFM.rss <- regsubsets( x=Xmtx, y=Y, nbest=2, nvmax=7 )
> summary( mlrBaseFM.rss )
(where the corresponding R components from Example 7.3.1 remain in place). The various
options to the function specify the design matrix X with the 14 predictors (x=), the response
variable (y=), the number of ‘best’ subsets per p (nbest=), and the maximum number of
variables per configuration (nvmax=). This produces the output (edited) in Figure 7.4.
In the regsubsets output, the first column indicates the number of predictors, p, and
the second column indicates the best (1) and the next-best (2) variable configuration
at that value of p. The asterisks (*) mark which of the predictors are selected for that
configuration. For example, the best seven-variable configuration contains the predictors
x1, x2, x3, x′
4, (x2 −x2)2, x1x2, and x1x′
4, which corresponds to results found using the R2
p plot
in Example 7.3.1. For a given p, the analyst can decide which of these various configurations
deserve deeper study in the MLR model. Additional visualization is available via the plot()
command in leaps, which orders variable subsets by a selection criterion such as BIC or R2.
(Try it: plot(mlrBaseFM.rss,scale=‘BIC’).)
Of additional interest in the regsubsets output above is the patterns that appear among
the various best subsets. Notice that the (x1 −x1)2, (x3 −x3)2, (x′
4 −x′
4)2, and x3x′
4 variables
are never selected, suggesting that they would have limited value if employed in the MLR
model. Further, x2 is only selected once p has risen to 7, although its corresponding quadratic
term (x2 −x2)2 appears in configurations with p as low as 3. As we usually require lower-order
terms to accompany their higher-order companions when the latter are entered into an MLR
model, this interesting anteposition could call for deeper analysis. Indeed, a variety of lesser
patterns also surface in the output; their further study could make for intriguing knowledge
discovery into the connections between these variables and log(Mortality) in these British
communities.
◽
Another automated strategy approaches the feature selection process in a ‘stepwise’ man-
ner. Rather than branch through the space of potential variable configurations, stepwise strate-
gies take a single path based on a set of step-up and step-down criteria. Although less efficient
than subset section, they can be useful in practice when p is very large and the subset approach
becomes computationally infeasible.

218
STATISTICAL DATA ANALYTICS
Subset selection object
14 Variables
(and intercept)
2 subsets of each size up to [p =] 7
Selection Algorithm: exhaustive
x1
x2
x3
x4 x1sq x2sq x3sq x4sq x12 x13 x14 x23 x24 x34
1 ( 1 )
*
1 ( 2 )
*
2 ( 1 )
*
*
2 ( 2 ) *
*
3 ( 1 )
*
*
*
3 ( 2 )
*
*
*
4 ( 1 )
*
*
*
*
4 ( 2 )
*
*
*
*
5 ( 1 )
*
*
*
*
*
5 ( 2 )
*
*
*
*
*
6 ( 1 )
*
*
*
*
*
*
6 ( 2 ) *
*
*
*
*
*
7 ( 1 ) *
*
*
*
*
*
*
7 ( 2 )
*
*
*
*
*
*
*
Figure 7.4
R output from best subset selection analysis for UK cancer mortality
data in Example 7.3.2. Source: Data from http://data.gov.uk/dataset/ni_122_-_mortality_
from_all_cancers_at_ages_under_75.
A popular stepwise variant is forward stepwise regression, which applies both step-up and
step-down criteria to achieve the recommended predictor list for further examination. As in
Section 7.3.1, suppose there are a total of P potential variables under study. From a simple
progenitor model – usually just the lone intercept – the forward stepwise algorithm proceeds
as follows (Kutner et al., 2005, Section 9.4):
FS.0 Fix an ‘entry’ significance level, 𝛼en, for admission of aspirant variables, and an ‘exit’
significance level, 𝛼ex, for deletion of unnecessary ones. To avoid cycling, 𝛼en < 𝛼ex
is required.
FS.1 Fit P SLR models, one for each jth predictor variable (j = 1, … , P). From each
fit, compute the t-statistics for testing that the individual jth predictor is significant,
via (7.16):
Tj −̂𝛽j
se[ ̂𝛽j].
Find the largest |Tj| and calculate its corresponding two-sided P-value Pj. Admit the
associated xj if Pj ≤𝛼en. (Notice that no adjustments are made here for multiplicity.)
If no |Tj| can satisfy Pj ≤𝛼en, terminate the stepwise procedure.
FS.2 Denote the selected x-variable by xj∗. Fit all the P −1 possible MLR models at p = 2,
using the two predictors xj∗and xj (j ≠j∗). Compute the partial t-statistics Tj for
testing that the newly added jth predictor (j ≠j∗) in each MLR is significant via
(7.16). Find the largest |Tj| and calculate its corresponding two-sided P-value Pj.
Admit the associated xj if Pj ≤𝛼en. If no |Tj| can satisfy Pj ≤𝛼en, terminate the
stepwise procedure.

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
219
FS.3 Now check for step-down variable exiting: in the selected two-variable MLR model,
compute the partial t-statistic Tj∗via (7.16) for testing that the earlier j∗th predic-
tor remains significant. Find its two-sided P-value Pj∗and remove xj∗if Pj∗> 𝛼ex.
Otherwise, continue with step-up variable admission.
FS.4 In the general case, suppose J < P variables currently reside in the selected configu-
ration, indexed by j∗= 1, … , J. Fit all possible MLR models at p = J + 1, using the
J selected predictors xj∗and a newly added xj (j ≠j∗). Compute the partial t-statistics
Tj for testing that the newly added jth predictor (j ≠j∗) in each MLR is significant
via (7.16). Find the largest |Tj| and calculate its corresponding two-sided P-value
Pj. Admit the associated xj if Pj ≤𝛼en. If no |Tj| can satisfy Pj ≤𝛼en, terminate the
stepwise procedure.
FS.5 Check for step-down variable exiting. Denote the newly added predictor in the cur-
rent (J + 1)-variable MLR model as xj′. Compute the J partial t-statistics Tj∗via
(7.16) for j∗≠j′. Find the smallest |Tj∗| and calculate its corresponding two-sided
P-value Pj∗. Remove xj∗if Pj∗> 𝛼ex. Then, continue with variable addition in Step
FS.4.
In practice, choices for the entry and exit levels can vary greatly. Values in the 5–15%
range are not uncommon, although increasing into the 20–35% range may be reasonable if
the goal is to produce a large list of putative predictors for deeper examination.
A number of other strategies exist for automated variable section. A simple variant of
forward stepwise regression is forward selection, where no step-down exiting is performed
(i.e., Steps FS.3 and FS.5 are omitted from the above FS algorithm). Or, one can start with all
P predictor variables in the MLR model and perform backward elimination: find the largest
two-sided partial P-value among all the P predictors, and if that P-value exceeds a predeter-
mined 𝛼ex, remove the corresponding x-variable from the model. Then, repeat the operation
on the remaining P −1 predictors. Continue with step-down elimination until all remaining
predictors fail the exit test.
Backward elimination is favored by some analysts because it tends to retain more of the
pertinent predictors. (In early stages of forward stepwise algorithms, many of the important
predictors have yet to enter into the model. This inflates the SSE and MSE, which in turn drives
the entry t-statistics closer to zero. Consequently, step-up admission becomes more difficult
and some important predictors may get lost along the way. Such is the lot when performing
complex feature selection.) See, for example, Kutner et al. (2005, Section 9.4) or Hastie et al.
(2009, Section 3.3) for further commentary.
Example 7.3.3 Cancer mortality multiple regression (Example 7.3.1,continued). Return
to the UK cancer mortality data and the variable selection operations in Examples 7.3.1 and
7.3.2. Recall that for the MLR fit on Y = log{Mortality}, the single predictor variables x1 =
{Employment rate}, x2 = {Weekly earnings}, x3 = {Volunteering %}, and x′
4 = {Smoking
cessation rate} were augmented with the additional, second-order predictors (xj −xj)2 (j =
1, … , 4) and xjxk (j ≠k).
To illustrate feature selection, consider simple backward elimination on the full set of 14
potential predictor variables. A convenient way to conduct the calculations is via R’s drop1()
function. This fits all the individual predictors in a supplied MLR model and computes a table
of the changes in fit when any term is singly dropped from the model. A test=‘F’ option
applies a partial F-test; with one numerator d.f., this is equivalent to the desired partial t-test

220
STATISTICAL DATA ANALYTICS
and gives identical P-values. Repeated application of drop1() to the serially reducing models
mimics the backward elimination strategy.
For example, start with the full, 14-predictor model via the sample code
> mlrBaseFM.lm <- lm( Y ˜ x1 + x2 + x3 + x4 + x1sq + x2sq +
x3sq + x4sq + x12 + x13 + x14 +
x23 + x24 + x34 )
> mlr14.drop <- drop1( mlrBaseFM.lm, test=‘F’ )
> Pvals14 <- mlr14.drop[,6]
> row.names(mlr14.drop)[which(Pvals14==max(Pvals14,na.rm=T))]
The mlr14.drop object contains the table of pointwise partial P-values for testing each indi-
vidual variable’s impact on the model. The remaining commands simply output the name
of the predictor associated with the largest P-value (output suppressed). If its corresponding
P-value is greater than 𝛼ex, remove that predictor from the model. (If not, stop and retain the
entire set of 14 predictors.) Here, we operate with the elimination level set to 𝛼ex = 0.15.
Next, eliminate the single predictor recommended for removal by taking advantage of R’s
update() function, and ‘back’ down to the next set of 13 P-values. Sample code with these
data is
> mlr13.drop <- drop1( update(mlrBaseFM.lm, .˜.-x34), test=‘F’ )
> Pvals13 <- mlr13.drop[,6]
> row.names(mlr13.drop)[which(Pvals13==max(Pvals13,na.rm=T))]
Continue until no further predictor variables are flagged for elimination.
Table 7.3 summarizes the results of the backward elimination steps for these data. As seen
therein, selected second-order cross-products are the first to be eliminated, followed by many
of the quadratic terms. By the time p = 7 variables remain, the first-order term x2 = {Weekly
earnings} is marked for elimination.
At this point, with 𝛼ex = 0.15, backward elimination produces a quandary: variable x2
presents the highest remaining P-value at P2 = 0.163. As this is (just) greater than 𝛼ex, the
algorithm calls for elimination of x2. It still retains, however, the predictors (x2 −x2)2 and
x1x2. Recall that standard practice requires retention of any lower-order terms that support
their corresponding higher-order terms when left in a model. Thus x2 should be retained, even
though backward elimination now flags it for removal.
Consequently, the analyst has a number of options:
1. stop here and study the seven-parameter configuration x1, x2, x3, x′
4, (x2 −x2)2, x1x2,
and x1x′
4 in greater detail. (A familiar collection for this set of features/regressors!);
2. ignore the call to eliminate x2 until such time – if ever – that both (x2 −x2)2 and x1x2
are removed, and continue backward elimination by removing the highest P-values not
associated with the single x2 variable (there is no keep= option in drop1(), which
would simplify such a strategy); or
3. blindly remove x2 and continue with the elimination; should either or both second-order
terms with x2 be retained when elimination concludes; attempt to interpret the model
without the supporting x2 lower-order term.
Option (i) is perhaps most reasonable, because the larger goal of this analysis is to iden-
tify a feature configuration worthy of continued study, and this subset seems to be popular.

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
221
Table 7.3
Summary of backward elimination steps for variable selection with cancer
mortality MLR analysis in Example 7.3.3.
No. variables
Model
Maximum
Targeted
Action
in model
components
P-value
x-variable
14
(Full model)
P14 = 0.998
x3x′
4
Eliminate x3x′
4
13
x1, … , x′
4,
P13 = 0.984
x2x′
4
Eliminate x2x′
4
all squares,
x1x2, … , x2x′
4
12
x1, … , x′
4,
P12 = 0.967
x2x3
Eliminate x2x3
all squares,
x1x2, … , x2x3
11
x1, … , x′
4,
P8 = 0.927
(x′
4 −x′
4)2
Eliminate (x′
4 −x′
4)2
all squares,
x1x2, … , x1x′
4
10
x1, … , x′
4,
P5 = 0.751
(x1 −x1)2
Eliminate (x1 −x1)2
(x1 −x1)2,
(x2 −x2)2,
(x3 −x3)2,
x1x2, … , x1x′
4
9
x1, … , x′
4,
P6 = 0.715
(x3 −x3)2
Eliminate (x3 −x3)2
(x2 −x2)2,
(x3 −x3)2,
x1x2, … , x1x′
4
8
x1, … , x′
4,
P7 = 0.571
x1x3
Eliminate x1x3
(x2 −x2)2,
x1x2, … , x1x′
4
7
x1, … , x′
4,
P2 = 0.163
x2
(see text)
(x2 −x2)2,
x1x2, x1x′
4
Note: Elimination level set to 𝛼ex = 0.15.
(Indeed, a sensible omnibus strategy would apply a variety of different selection algorithms
to the collection of potential feature variables and examine how/where they agree or dis-
agree in their recommendations.) Option (ii) could also be considered if one were willing
to make the required adjustment to the backward elimination strategy. Option (iii) is not
recommended.
◽
As mentioned in the forward stepwise regression algorithm, stepwise methods that rely
on P-values or test statistics typically do not adjust for multiplicity throughout their many (!)
decision points/hypotheses regarding variable retention or deletion. As a result, little, if any,
inferential value exists in the resulting quantities: the P-values are essentially being used as

222
STATISTICAL DATA ANALYTICS
objective scores for making the variable selection decisions. A reasonable alternative measure
from which to select features that has gained some favor replaces the P-values (or partial t-tests
or other traditional devices) with information criteria such as the AIC or BIC. One still applies
forward selection, backward elimination, or a combination of the two, but now variables enter
or exit based on how small the values of AIC or BIC can become.
In R, for example, the step() function can conduct forward selection (via its direc-
tion=‘forward’ option), backward elimination (direction=‘backward’), or both
(direction=‘both’) with the goal of minimizing the AIC in the final reported model.
(AIC corresponds to the function’s k=2 option. To minimize BIC, input instead k=log(n).)
As with any feature-selection output, the final, reported variable configuration is then used as
a springboard for greater examination of how the variables affect the observed response.
Example 7.3.4 Cancer mortality multiple regression (Example 7.3.1, continued). Return
to the UK cancer mortality data and the variable selection operations in Examples
7.3.1–7.3.3. Recall that for the MLR fit on Y = log{Mortality}, the single predictor variables
x1 = {Employment rate}, x2 = {Weekly earnings}, x3 = {Volunteering %}, and x′
4 =
{Smoking cessation rate} were augmented with the additional, second-order predictors
(xj −xj)2 (j = 1, … , 4) and xjxk (j ≠k).
To illustrate feature selection via IC-based stepwise methods and to compare with the
results in Example 7.3.3, consider again simple backward elimination on the full set of 14
potential predictor variables. Now, however, employ minimum-AIC as the selection criterion
using step() in R. Given an lm object, say, mlr14.lm containing the full set of 14 predictors,
the command is simply
> step( mlr14.lm, direction=‘backward’, k=2 )
The consequent output is a thread of model fits that reports the values of AIC as terms are
dropped singly from the full base model in mlr14.lm. The complete output is condensed and
edited here for space considerations: the initial, base fit is reported as
Start:
AIC=-1464.19
Y ˜ x1 + x2 + x3 + x4 + x1sq + x2sq + x3sq + x4sq + x12 + x13
+ x14 + x23 + x24 + x34
Df
Sum of Sq
RSS
AIC
- x34
1 0.000000055 4.3312160 -1466.1851
- x24
1 0.000004861 4.3312208 -1466.1848
- x23
1 0.000017250 4.3312332 -1466.1838
- x4sq
1 0.000064731 4.3312807 -1466.1800
- x1
1 0.001205777 4.3324217 -1466.0900
- x1sq
1 0.001234972 4.3324509 -1466.0876
- x3sq
1 0.002005920 4.3332218 -1466.0268
- x13
1 0.005742240 4.3369582 -1465.7320
- x2
1 0.006244338 4.3374603 -1465.6924
- x3
1 0.012891450 4.3441074 -1465.1687
- x12
1 0.015313809 4.3465297 -1464.9781
<none>
4.3312159 -1464.1852
- x14
1 0.026781625 4.3579976 -1464.0769
- x4
1 0.026818793 4.3580347 -1464.0740
- x2sq
1 0.097974879 4.4291908 -1458.5351
The output indicates that greatest reduction in AIC occurs with elimination of x3x′
4.

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
223
The program next automatically eliminates x3x′
4 and steps down to assess whether further
single-variable elimination can decrease the AIC. After nine step-down eliminations (output
suppressed), we achieve
Step:
AIC=-1478.73
Y ˜ x3 + x4 + x2sq + x12 + x14
Df
Sum of Sq
RSS
AIC
<none>
4.3752500 -1478.7257
- x12
1 0.07259252 4.4478425 -1475.0979
- x2sq
1 0.15528962 4.5305396 -1468.7976
- x14
1 0.16254693 4.5377969 -1468.2502
- x4
1 0.27616605 4.6514161 -1459.7925
- x3
1 0.92637882 5.3016288 -1415.0445
at which point the operation terminates. R finds that no additional single-variable deletions
can further decrease the AIC.
The reported minimum-AIC configuration from step() lists the five predictors x3, x′
4,
(x2 −x2)2, x1x2, and x1x′
4. Here again, however, the inclusion of selected second-order terms
requires us to also include their supporting first-order terms. Thus the selected model presents
x1, x2, x3, x′
4, (x2 −x2)2, x1x2, and x1x′
4 for further examination (see Exercise 7.13). We once
again recover the same seven-predictor model seen in previous variable selection calculations
with these data.
◽
The philosophy of applying an automated search method for variable selection in an
MLR analysis is not without controversy. Certainly, the automated approach allows for faster
and more-efficient study of a large collection of potential predictor variables. A primary
argument against it, however, is its commissioning of the decision-making process to the
computer – which operates without any domain-specific experience – and away from a data
analyst who brings pertinent subject-matter knowledge to the selection process.
Indeed, automated selection necessarily induces selection bias in the eventual model cho-
sen for the MLR (Freedman 1983), often substantially so. P-values for the final regression
coefficients may be overstated, and associated predictor variables can appear statistically sig-
nificant when they do not in fact contribute meaningfully to the regression. These concerns
are difficult to dismiss. Clearly, blind or inattentive assignment to the computer for final deci-
sion(s) on which MLR predictors to employ is foolhardy; no automated variable selection
procedure should ever replace informed scientific judgment. In the end, the analyst must
ensure that use of computer-driven, automated, feature selection is intended only as a tool
and not as the guiding force in the regression modeling effort.
7.4
Alternative regression methods∗
When the basic assumptions of the MLR model in (7.1) – statistical independence of the data,
normal parent distributions, linear relationships, additive effects, and so on – are in doubt, the
quality of the resulting estimators and/or inferences can suffer. A number of alternative fitting
strategies can be applied in such cases to provide remedial or robust estimates of the various
regression features. A selected assortment is presented in this section.

224
STATISTICAL DATA ANALYTICS
7.4.1
Loess
If the actual shape/linearity of the mean response 𝜇(x) is in doubt, a form of nonparametric
regression can be applied that draws a smooth curve or surface through the data. Information
on 𝜇(x) is taken from those Yis at xis located near to x, producing a ‘locally weighted smoother’
for 𝜇(x). For the single-predictor (p = 1) setting, Cleveland (1979) introduced locally weighted
scatterplot smoothing or ‘lowess.’ This later evolved for the case of p ≥2 predictor variables
into a form of locally weighted polynomial regression (Cleveland and Devlin 1988) to estimate
the mean response surface. The modern term for the method is loess, after the slit-like deposits
formed along river banks that resemble ‘surfaces’ of sorts (Cleveland et al. 1992).
To introduce the concepts, begin with the p = 1 case, where Yi is regressed on a single pre-
dictor variable xi, i = 1, … , n. Take 𝜇(x) = E[Y] and assume 𝜇(x) is a smooth-but-unknown
function of x. Similar to the moving average smoother from Section 3.5.2, loess constructs a
local window or neighborhood around each x and fits a straight line to the data pairs (xi, Yi)
within that window via weighed least squares (WLS, from Section 7.1.2). The weights
𝑤i = 𝑤i(x) vary locally for any given x, and xis close to x in the window are given heavier
weight. Each local window is defined as a fixed proportion q ∈(0, 1) of those original xis
closest to x. The local linear fits are then connected across a fine grid of x-values. This
produces a smoothed predictor, ̃𝜇(x), for the mean response at any value of x in the range of
the original data.
In effect, q serves as a smoothing parameter: small values of q draw the local windows
tighter and produce a ‘bumpy’ loess fit; larger values smooth out the fit over the broader range
of the data. In practice, q is usually taken between q = 0.2 and q = 0.8, depending on the
particular visualization needs of the analysis. If desired, the first-order linear fit can be replaced
with a higher-order polynomial in each local window; local quadratic smoothing is common.
In essence, loess assumes that the mean response 𝜇(x) can be approximated accurately in
some small neighborhood of x via a linear or quadratic (or other polynomial) function. As it
is relatively simple to fit low-order polynomials via the MLR approach, the loess algorithm’s
increased complexity is less onerous than it may at first appear.
Formally, let dq[x] be the distance from x to the farthest predictor – the ⌊qn⌋th ‘near-
est neighbor’ – in its local window. Extending the triangular kernel smoother seen in
Section 4.1.4, define the tricube kernel as
KCu(t) = (1 −|t|3)3 I(−1,1)(t).
(7.25)
From this, the local weights at the given x are taken as
𝑤i(x) =
{
KCu
(|x −xi|∕dq[x])
if |x −xi| ≤dq[x]
0
otherwise
.
(7.26)
The loess predicted value ̃𝜇(x) is then found by calculating local linear WLS estimates ̃𝛽0x and
̃𝛽1x using the weights in (7.26) and setting ̃𝜇(x) = ̃𝛽0x + ̃𝛽1xx. Repeating this procedure over
a dense collection of x values produces smoothed loess predicted values, ̃𝜇(x). Plotting these
against x helps to visualize the shape, perturbations, and overall nature of the mean response,
without call to any specific parametric assumptions on 𝜇(x).

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
225
The loess algorithm extends to local quadratic smoothing by simply applying WLS with a
parabolic response 𝛽0 + 𝛽1x + 𝛽2x2 at each x. The local WLS estimates become ̃𝜇(x) = ̃𝛽0x +
̃𝛽1xx + ̃𝛽2xx2. Local quadratic smoothing can add flexibility to the loess predicted values, with
only a slight increase in computational burden.
If the distribution of Yi can be assumed normal (Gaussian) with homogeneous variation,
the loess fitting procedure stops after one WLS pass. If, however, the variation is thought to
exhibit heavier-than-normal tail behavior (while remaining symmetric) or if greater robustness
is desired in the predicted values, the fit is iteratively updated: from the WLS-loess fitted values
̃Yi = ̃𝜇(xi), define the initial loess residuals as ̃ei = Yi −̃Yi. The goal is to update the original
weights 𝑤i(x) at each x to new values, ̃𝑤i(x), that diminish the effects of outlying or extreme
observations.
First, calculate the absolute residuals |̃ei| and find their median, denoted by ̃Q|2|. Then,
for the bisquare kernel
KSq(t) = (1 −t2)2I(−1,1)(t),
(7.27)
define the updated weights at each x as
̃𝑤i(x) =
{
KSq
(̃ei∕6 ̃Q|2|
)
if ̃ei ≤6 ̃Q|2|
0
otherwise
.
(7.28)
With the new weights in (7.28), perform an additional WLS fit to find updated, robust estimates
̌𝛽0x and ̌𝛽1x for the SLR parameters in the local window. From these, calculate the updated
predicted value as ̌𝜇(x) = ̌𝛽0x + ̌𝛽1xx. (If a quadratic polynomial was employed in the initial
WLS fit, continue to use it for the regression model in each local window.) To ensure that this
robust extension properly downweights any extreme observations, repeat the reweighting a
second time to produce the final, robust predicted values ̌𝜇(x). Plot these against x to visualize
the pattern of mean response in the data.
Loess is most appropriate when the observations are dense and numerous, so that the local
smoother can adequately estimate the mean response over the range of the data. It may not
perform as well if the sample size is small or if the Yis are sparsely arranged. (This can be
especially problematic when p > 1; see the following text.)
Example 7.4.1 Type Ia supernovae cosmology. In astronomy, exploding stars known as
Type Ia supernovae (SNe Ia) possess very stable luminosity patterns. This allows astronomers
to use the objects as a sort of ‘standard candle’ to calibrate and measure astronomical
distances.
As part of a larger study into characteristics of n = 580 Type Ia Supernovae, Suzuki et al.
(2012) gave data on the relationship between an SNe Ia’s redshift and its distance modulus.
A stellar object’s redshift is the lengthening of its emitted radiation due to differential motion
in space: redder if receding, bluer if approaching. In effect, this allows its use as a surro-
gate measure for the object’s relative velocity. By contrast, an object’s distance modulus is
the difference between its apparent and absolute magnitudes, which produces a measure of
‘distance.’ Scatterplots of the distance modulus against the redshift are known as ‘Hubble
diagrams,’ from which predictions on an object’s distance can be made.
The data appear in Table 7.4. (As above, only a selection is given in the table. The complete
set is available at http://www.wiley.com/go/piegorsch/data_analytics.) Figure 7.5 displays the
corresponding Hubble diagram. The scatterplot indicates a concave-increasing trend in the

226
STATISTICAL DATA ANALYTICS
distance modulus. To model curvilinearity in the plot, one could attempt a quadratic polyno-
mial fit as per Section 7.2. For these data, however, the quadratic’s strict form of curvature
cannot adequately accommodate the concave response in Figure 7.5 (see Exercise 7.17). As
an alternative, consider here construction of a loess fit to provide a smooth prediction curve.
Table 7.4
Selected data from a larger set of n = 580 paired observations on Type Ia super-
novae characteristics.
Supernova code
1993ah
1993ag
1993o
· · ·
2002kd
2002ki
x = Redshift
0.02849
0.05004
0.05293
· · ·
0.73500
1.14000
Y = Distance modulus
35.34658
36.68237
36.81769
· · ·
43.09184
44.19695
Source: Suzuki et al. (2012).
0.0
0.5
1.0
1.5
34
36
38
40
42
44
46
Redshift
Distance modulus
Figure 7.5
Scatterplot of Y = {Distance modulus} versus x = {Redshift} for n =
580 Type Ia supernovae from Example 7.4.1. Solid curve overlays robust quadratic
loess prediction for 𝜇(x) with smoothing parameter set to q = 0.5. Source: Data from
http://supernova.lbl.gov/Union/. Graphic adapted from Suzuki et al. (2012).
For the loess predicted values, work with a second-order smoother, that is, impose no
shape assumptions on the mean response E[Y] = 𝜇(x) but build the loess smooth based on
local quadratic regression within each window. In R, the loess() function can perform
second-order loess regression. Sample R code is given as follows:
> Ytilde.loess <- loess( Y ˜ x, span=.5, degree=2,
family=‘symmetric’ )
> Ysmooth <- predict( Ytilde.loess, data.frame(x=seq(0,1.5,.01)) )

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
227
In the call to loess(), the span=.5 option sets the smoothing parameter to q = 0.5 (the
default is q = 0.75), the degree=2 option employs a quadratic fit within each window and the
family=‘symmetric’ option calls for multiple iterations with updated weights as in (7.28)
(the default is a single iteration, via family=‘gaussian’). The family=‘symmetric’
option was employed here to diminish the effects of possible outlying observations in the data.
The smoothed predicted values ̌𝜇(x) reside in Ysmooth. Plotted against x, this gives a
smooth prediction curve and also helps to visualize the trend in the data without forcing any
parametric assumptions on 𝜇(x). This is displayed as an overlay (solid curve) in Figure 7.5.
Exercise 7.17 further explores the loess fit with these data.
◽
It is fairly straightforward to extend the loess approach to more than one predictor vari-
able: essentially, all the components described above for the single-predictor case apply with
multiple predictors. The only substantial change is a need to choose a priori the formal met-
ric, d[xi, xh], that defines ‘distance’ between two p-dimensional points xi and xh in each local
window. (Many distance metrics exist. Table 9.8 lists some of the options.) Loess operations
usually default to Euclidean, ‘as-the-crow-flies’ distance
d[xi, xh] =
√∑p
j=1(xij −xhj)2.
If the predictor variables differ substantially in their scales or units, it is common to divide
them first by a scaling quantity such as their individual standard deviations.
Given a specification for the distance metric, d[xi, xh], multivariable loess smoothing con-
tinues to apply a local WLS–MLR fit to the data in each (p −1)-dimensional window or span
around the target predictor vector x. The local window is again defined as the fixed proportion
q of original xis closest to x, where q is typically taken in the range 0.2 ≤q ≤0.8.
Let dq[x] now be the distance, defined by the choice for d[⋅, ⋅], from x to the farthest
predictor vector in its local window. The weights for the WLS–MLR fit in the window around
x are then taken as
𝑤i(x) =
{
KCu
(d[x, xi]∕dq[x])
if d[x, xi] ≤dq[x]
0
otherwise,
where KCu(⋅) is the tricube kernel from (7.25). The loess predicted value ̃𝜇(x) is then found by
calculating the (first-order) WLS–MLR parameter estimates ̃𝜷x from (7.11) in the local win-
dow about x and setting ̃𝜇(x) = x̃𝜷x. If desired, one can move to second-order MLR models by
including quadratic terms and linear cross-products in the regression equation. For example,
with two predictor variables x1 and x2, the full second-order response surface
𝛽0 + 𝛽1x1 + 𝛽2x2 + 𝛽3x2
1 + 𝛽4x2
2 + 𝛽5x1x2
(7.29)
can be fit within each local window. Graphing ̃𝜇(x) against x using surface plots, contour plots,
or other dimension-reducing displays can help to visualize the smoothed surface.
If concern exists over potential outliers or heavy-tailed (symmetric) variation, apply the
robust extension to the loess fit. As above, find the WLS-loess fitted values ̃Yi = ̃𝜇(xi) and
calculate the absolute initial loess residuals |̃ei| = |Yi −̃Yi|. Find the corresponding median
absolute residual ̃Q|2| and set the updated weights at each x to
̃𝑤i(x) =
{
KSq
(̃ei∕6 ̃Q|2|
)
if ̃ei ≤6 ̃Q|2|
0
otherwise,
(7.30)

228
STATISTICAL DATA ANALYTICS
where KSq(t) is the bisquare kernel in (7.27). With the new weights in (7.30), recover the
updated predicted value ̌𝜇(x) = x ̌𝜷x. (If a second-order response surface was employed in the
initial WLS fit, continue to use it for the MLR model in each local window.) To ensure that
this robust extension properly downweights any extreme observations, repeat the reweighting
for second time to produce the final, robust predicted values ̌𝜇(x).
A warning: when p > 1, loess smoothing can suffer from a ‘curse of dimensionality’
(Clarke et al. 2009, Section 1.0.1). For instance, moving from one x-variable to two only dou-
bles the number of predictors, but it in effect squares the space within which each predictor
vector x lies. A dense collection of n points in one-dimensional space may appear decidedly
more sparse in two (or more) dimensions, so a higher-dimensional space must be filled with
many more points if the analytic goal includes estimation/visualization of the mean response
that progresses through it. The ‘curse’ only exacerbates as p grows: unless n is very large
and the observations densely fill the prediction space, it is usually advisable to apply loess
smoothing to data sets with just a limited number of predictor variables.
Example 7.4.2 Automobile fuel economy (Example 4.2.6, continued). To illustrate appli-
cation of loess smoothing with p = 2 predictor variables, consider again the automobile fuel
economy data in Table 4.6. Recall that these data gave automobile miles driven per gallon
of gasoline (MPG) performance for n = 652 automatic-transmission vehicles from the 2011
model year.
In Example 4.2.6, focus was on visualizing the data; here, consider extending this to how
Y = MPG relates to number of cylinders and engine displacement (L) when the latter are
now viewed as two predictor variables. Rather than assuming any specific form for the mean
response 𝜇(x1, x2), however, employ a multidimensional loess smooth. The two scales of mea-
surement for the predictors are somewhat different, so normalize the variables first by their
standard deviations:
x1 = Number of cylinders
1.7604
and
x2 = Engine displacement
1.3268
.
One typically starts with a simple scatterplot matrix of the data; however, the plots with
Y = MPG essentially replicate information already seen in the bubble plots from Figure 4.17.
Thus a simple x1, x2 scatterplot will suffice (see Figure 7.6). The scatterplot shows that the
two scaled predictors vary over a band or swath of values from lower left to upper right. (The
correlation between the two predictors is 0.908. This suggests possible multicollinearity, but
in fact the interrelationship between x1 and x2 here is not deleterious; see Exercise 7.5.)
For the loess fit, the potential complexity in the relationships among all the variables argues
for use of robust, second-order smoothing. Set the smoothing parameter to q = 0.7. Sample
R code is
> MPG.loess <- loess( Y ˜ x1 + x2 + x1:x2, span=0.7,
degree=2, family=‘symmetric’ )
> x1grid <- seq( 0.75, 9, length=100 )
> x2grid <- seq( 0.75, 6, length=100 )
> Ysmooth <- matrix(0, nrow=100, ncol=100)
> for(i in 1:100) {
for(j in 1:100) {
Ysmooth[i,j] <- predict( MPG.loess,
newdata=data.frame(x1=x1grid[i], x2=x2grid[j] )
)

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
229
2
4
6
8
1
2
3
4
5
6
Scaled cylinders
Scaled displacement
Figure 7.6
Scatterplot of x2 = {scaled Engine Displacement} against x1 ={scaled Number
of Cylinders}, from Example 7.4.2. Scaling is by individual standard deviations. Source: Data
from http://www.fueleconomy.gov/feg/download.shtml.
if ( x2grid[j] < max(0.8, -.75+0.7*x1grid[i]) ||
x2grid[j] > min(6.1, 1.1*x1grid[i]) )
Ysmooth[i,j] <- NA
} # end for j loop
}
# end for i loop
> contour( x=x1grid, y=x2grid, z=Ysmooth, levels=seq(12,44) )
In the originating call to loess(), the model formula now includes the two predictor vari-
ables and, for completeness, an interaction term x1:x2 (a shorter, more efficient syntax in R
is Y ˜ x1*x2). The code lays out a polygonal grid of (x1, x2) points, over which the 𝜇(x1, x2)
surface is to be estimated. The complicated for statement restricts the construction to that
swath of points. This corresponds roughly to the two-dimensional range of the predictor val-
ues. (A simpler rectangular grid would produce some extreme extrapolations: no automobiles
are produced with, say, 16 cylinders but only 2 L of engine displacement.)
Figure 7.7 presents the subsequent plot from contour(). An overall decreasing pattern in
estimated MPG is seen from bottom to top and, to a lesser extent, from right to left in the plot.
This is consistent with our general expectations; however, the contours also appear to show
a ridge-like structure arching down the surface as displacement increases. The relationship
between mean MPG and the two predictors exhibits some intriguing complexities that may
require additional study. (Exercise 7.20 explores the loess fit with these data in further detail.)
Readers are encouraged to experiment with R’s other plotting routines to better visualize
the predicted contours/surface. For example,

230
STATISTICAL DATA ANALYTICS
Scaled cylinders
Scaled displacement
14 
15 
16 
17 
17 
18 
19 
20 
21 
22 
23 
23 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
38 
2
4
6
8
1
2
3
4
5
6
Figure 7.7
Contours of estimated mean MPG using robust, second-order, loess pre-
diction in Example 7.4.2. Smoothing parameter is set to q = 0.7. Source: Data from
http://www.fueleconomy.gov/feg/download.shtml.
> filled.contour( x=x1grid, y=x2grid, z=Ysmooth,
color.palette=terrain.colors )
gives a colored plot (not shown) that may improve the view of the changing surface
features.
◽
A substantial literature exists on loess and its use in data analytics; useful sources include
Gijbels and Prosdocimi (2010), Givens and Hoeting (2013, Section 11.4.1), and the early
introduction by Cleveland et al. (1992).
7.4.2
Regularization: ridge regression
In some cases, the basic regression model may exhibit ill-posed or otherwise unstable fea-
tures, such as high multicollinearity/correlations among the predictor variables, or very large
numbers of predictors with p close to or even exceeding n. In situations such as these, the XTX
matrix at the core of the ordinary LS estimator in (7.6) can be driven to a state of ill condition
(Gentle 2007, Section 9.4.1). The pejorative term here is meaningful. If XTX is ill-conditioned,
it can appear almost singular. The resulting inverse matrix is numerically unstable and detri-
mentally affects the LS estimator in (7.6): small changes in the data can lead to wild swings
in the estimated 𝛽-parameters or in the predicted values.
An approach for reducing the effects of ill-conditioning is known under the larger moniker
of regularization (Lukas 2012). The goal is to stabilize the regression parameter estimates by
penalizing those that grow too large or unwieldy. This essentially drives the point estimates

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
231
closer to each other and eventually toward zero. Since the estimates literally shrink to the
origin, the process with MLR data is often called shrinkage regression (Sundberg 2012).
Formally, to regularize a multicollinear or otherwise ill-conditioned set of MLR predictors,
we apply a form of penalized least squares: add a penalization term to the usual error sum of
squares (SSE) and minimize the resulting objective quantity
(𝜷) =
n
∑
i=1
{
Yi −
(
𝜷0 +
p
∑
j=1
𝛽jxij
)}2
+ 𝜅
p
∑
j=1
𝒬(𝛽j).
(7.31)
Here 𝒬(𝛽j) is a positive penalty function and 𝜅≥0 is an associated tuning or regularization
parameter. The added penalty term is sometimes referred to as a Tikhonov factor, and the
general approach as Tikhonov regularization, after Tikhonov (1963). Properly constructed, the
added factor acts to penalize large (absolute) values of the regression parameters and shrink
the resulting estimates toward zero. A common choice employs the quadratic penalty function
𝒬(𝛽j) = 𝛽2
j .
Readers familiar with constrained minimization will recognize the expression in (7.31)
as a form of objective quantity for an optimization method known as Lagrange multipliers
(Hughes-Hallett et al. 2013, Section 15.3), where 𝜅is the ‘multiplier.’ An equivalent formu-
lation calls for minimization of the SSE subject to an upper-bound constraint involving 𝜷.
That is, minimize ∑n
i=1 {Yi −(𝛽0 + ∑p
j=1 𝛽jxij)}2 subject to ∑p
j=1 𝒬(𝛽j) ≤t𝜅. (The notation on
the bound t𝜅indicates that it and the tuning parameter/multiplier 𝜅will be interrelated. The
technical details exceed the scope here; interested readers may refer, for example, to Clarke
et al. (2009, Section 10.3.1).)
In practice, it is common to standardize the predictors so that any differences in scale do
not interfere with the regularization. Thus we work with the z-scores zij = (xij −xj)∕sj, where
xj is the arithmetic mean of the jth predictor and sj is the corresponding standard deviation.
(Some programs alternatively employ zij
√
n∕(n −1) or zij∕
√
n −1; the latter gives predictors
whose squares sum to 1.)
When the predictors are centered, the LS estimator for the intercept term 𝛽0 is simply Y.
Indeed, because relocating the response variable by adding an arbitrary constant can affect
the results, it is also common to center the Yis: say, Ui = Yi −Y. This adds further numerical
simplification, since the intercept term no longer enters into the regularization adjustment.
(Alternatively, one can simply choose not to include 𝛽0 in the regularization penalty term.)
The subsequent equations simplify somewhat after applying matrix notation, as in
Section 7.1.1. Denote the corresponding design matrix (now without a leading column of
ones for the intercept) as
Z =
⎡
⎢
⎢
⎢⎣
z11
z12
· · ·
z1p
z21
z22
· · ·
z2p
⋮
⋮
⋱
⋮
zn1
zn2
· · ·
znp
⎤
⎥
⎥
⎥⎦
and the corresponding, centered, response vector as U = [U1 U2 · · · Un]T. The revised MLR
model takes the form E[U] = Z𝜷where the p × 1 vector of regression parameters becomes
𝜷= [𝛽1 𝛽2 · · · 𝛽p]T. With these, the objective function with a quadratic penalty is simply
(𝜷) = (U −Z𝜷)T(U −Z𝜷) + 𝜅𝜷T𝜷.

232
STATISTICAL DATA ANALYTICS
The resulting normal equations reduce to
(ZTZ + 𝜅I) ̂𝜷= ZTU,
where I is a p × p identity matrix. Notice that these have essentially the same form as the
original MLR normal equations in (7.5), except that the (constant) tuning parameter 𝜅has
been added to the diagonal of the (potentially unstable) ZTZ matrix. This diagonal ‘ridge’
gives the method its name in statistical parlance: ridge regression.
No matter what the condition of ZTZ, for any 𝜅> 0, the diagonal increment in ZTZ + 𝜅I
stabilizes the matrix so effectively that it always possesses an inverse. Thus the solution to the
ridge regression normal equations is always defined:
̂𝜷𝜅= (ZTZ + 𝜅I)−1ZTU.
(7.32)
The universal existence of the ridge regression estimator and its consequent ability to address
issues of multicollinearity for any MLR model were some of the original motivators for its
use (see Hoerl and Kennard 1970).
The tuning parameter 𝜅in (7.32) controls the amount of regularization and shrinkage seen
in the eventual parameter estimates. The ordinary LS estimator in (7.6) obtains as 𝜅→0, while
as 𝜅→∞, ̂𝜷𝜅shrinks to a zero vector.
It can be shown (Exercise 7.22) that E[ ̂𝜷𝜅] = A𝜅𝜷where
A𝜅= (I + 𝜅[ZTZ]−1)−1.
Clearly, if 𝜅> 0, then A𝜅≠I and, therefore, the ridge estimator is biased! The bias vanishes
as 𝜅→0, but it conversely grows as 𝜅→∞. The associated variance of ̂𝜷𝜅is proportional
to A𝜅ZTZAT
𝜅, producing a mean squared error (MSE, as variance plus squared bias; see
Section 5.2.1) that depends on 𝜅. Hoerl and Kennard (1970) showed that there exists some
𝜅> 0 such that the MSE of the ridge estimator in (7.32) is strictly smaller than the ordinary LS
estimator in (7.6). Unfortunately, the value of this minimizing 𝜅depends on the true value of
𝜷, so it is usually unknown. (Selection of 𝜅is discussed later.) The existence theorem nonethe-
less indicates a trade-off between increased bias and decreased MSE that can play to the ridge
estimator’s advantage (Hastie et al. 2009, Section 7.3): by accepting a (small amount of) bias
in the point estimator, the overall precision can be enhanced and in the process can improve
prediction of E[U]. For problems where some form of regularization is indicated, this gives
the ridge estimator and the larger strategy significant validity.
The ridge predicted values using (7.32) are
̂U = Z ̂𝜷𝜅= Z(ZTZ + 𝜅I)−1ZTU
(7.33)
which for
H𝜅= Z(ZTZ + 𝜅I)−1ZT
(7.34)
gives ̂U = H𝜅U. Thus H𝜅can be viewed as a form of ridge ‘hat’ matrix, analogous to the MLR
hat matrix H in (7.7). Recall there that the trace of H, tr(H), is the number of parameters, p. We
often say this represents the ‘parameter d.f.’ being modeled by the MLR equation. Similarly,
the trace of H𝜅can be shown to equal
tr(H𝜅) =
p
∑
j=1
𝜆j
𝜆j + 𝜅,
(7.35)

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
233
where the 𝜆js are the ordered, decreasing eigenvalues of ZTZ. (Eigenvalues are reviewed in
Section A.5.) Borrowing the ‘trace as d.f.’ concept for the ridge regression setting, (7.35) is
called the effective d.f. of the ridge model fit. Notice that this expression is monotone deceasing
in 𝜅and, in particular, as 𝜅→∞the effective d.f. drop to zero.
In practice, selection of 𝜅is conducted in an exploratory manner. Where possible, one
‘trains’ the choice of 𝜅with a set of training data – using methods discussed later – and then
applies this choice to a separate target or test data set. Separate training data are not always
available, however. For choosing 𝜅from a single set of data, a variety of algorithms exist. A
popular, standalone estimate was given by Hoerl et al. (1975), who suggested a summary
value that divides the residual mean square (MSE) by the harmonic mean of the regres-
sion coefficients:
𝜅HKB = p MSE
̂𝛽T ̂𝛽
.
(7.36)
One can modify this by iterating over successive values of 𝜅, if desired. Also see Cule and De
Iorio (2013).
A more-computational, subjective and also more-participatory approach was given by
Hoerl and Kennard (1970): they recommended graphing each individual ̂𝛽j𝜅as a function
of 𝜅≥0 on the same plot. Called a ridge trace, the plot will show the point estimates shrink-
ing toward each other and toward zero as 𝜅grows. For small 𝜅, the divergence and disparities
among the estimated recession coefficients will appear clear, but typically a value of 𝜅will
present where the various traces begin to converge. Hoerl and Kennard recommended stabiliz-
ing value of 𝜅for use in (7.32). Some authors alternatively plot the traces against the effective
d.f. in (7.35), which produces a reversed graphic but otherwise provides similar information.
Friendly (2013) gives an extension of the ridge trace using ellipsoids to better visualize the
bias-variance trade off.
Perhaps most common in current practice is a form of cross-validation (CV) to select 𝜅
(Golub et al. 1979). CV removes an observation from the original data set and then uses the
remaining data to estimate the value of that excised observation under the proffered model. The
concept is similar to the case-deletion strategy with the PRESS statistic in Section 7.3.1. As
there, the squared difference between the actual observation and its cross-validated prediction
gives a measure of prediction error. Reapplying the single-deletion effort across all n data
points and averaging produces a CV error, which can be minimized to select a value of 𝜅. In
practice, a number of convenient simplifications and approximations allow the analyst to avoid
repeated model fitting to find the case-deleted prediction errors. The result is a generalized
cross-validation (GCV) error:
GCV(𝜅) = 1
n
n
∑
i=1
{
Ui −̂Ui(𝜅)
1 −n−1tr(H𝜅)
}2
,
(7.37)
where for a given 𝜅, ̂Ui(𝜅) is the ith element of (7.33) and tr(H𝜅) are the effective d.f. from
(7.35). Select the GCV estimator 𝜅GCV ≥0 to minimize (7.37).
In R, a number of packages provide functions that conduct ridge regression. The most
basic is the lm.ridge() function in the MASS package. Others include the ridge() function
in the external genridge package, the linearRidge() function in the external ridge package,
and the glmnet() function in the external glmnet package.

234
STATISTICAL DATA ANALYTICS
Example 7.4.3 Ridge regression with genetic SNP data. Regularization methods such as
ridge regression can be useful for analyzing large data sets in modern genetics. The response
variable might be a phenotypic outcome that is regressed against a variety of genotypic input
variables. The goal is to identify potential associations between the genotypic variants and the
phenotypic response. A popular genetic regressor variable involves different single nucleotide
polymorphisms (SNPs) identified along a species’ genome (Austin et al. 2013; Cule et al.
2011). A typical SNP has three levels coded as 0, 1, or 2 to represent copies of the minor allele.
It is not uncommon for a set of p SNP predictors to exhibit high multicollinearity
and/or represent many regressor variables relative to the number of subjects n (the so-called
‘large-p/small-n’ problem, which can nonetheless involve a not-so-small n). Standard MLR
analysis is contraindicated. The ridge penalty in (7.32) can still produce a viable regression
estimator, however, endowing the methodology with substantive applicability for this
estimation problem.
For a moderate-scale illustration, take the sample GenCont data provided in R’s external
ridge package. The observations are given as n = 500 outcomes for a continuous phenotypic
response, Yi, generated via the Fregene software program (Chadeau-Hyam et al. 2008). A
set of p = 11 SNPs are reported as the regressor variables. A selection of the data appear in
Table 7.5. (The complete set is available by loading the ridge package, or at http://www.wiley
.com/go/piegorsch/data_analytics.)
Table 7.5
Selected phenotypic responses (Yi) and SNP genetic predictors
from a larger set of n = 500 observations generated from the Fregene software
program (Chadeau-Hyam et al. 2008).
Yi
SNPi1
SNPi2
SNPi4
· · ·
SNPi,11
SNPi,12
1.4356316
1
1
0
· · ·
1
0
2.9226960
1
0
0
· · ·
1
0
0.5669319
0
0
0
· · ·
2
0
4.8515051
1
0
0
· · ·
1
0
⋮
⋮
⋮
⋮
⋱
⋮
⋮
0.4047265
0
0
0
· · ·
2
0
-0.1720107
0
0
0
· · ·
1
0
Note: The SNP3 variable replicates existing information in the data set and is not listed. Source:
data(GenCont) from R ridge package.
Application of a standard MLR model is possible with these data, although one quickly
sees that multicollinearity is present. The sample R code
> require( car )
#load external car package for vif() function
> vif( lm(Phenotypes ˜ SNP1 + SNP2 + SNP4 + SNP5 + SNP6 + SNP7
+ SNP8 + SNP9 + SNP10 + SNP11 + SNP12) )
produces the following VIFs from (7.20) for the individual SNP predictors:
SNP1
SNP2
SNP4
SNP5
SNP6
SNP7
19.113512 45.075246
1.024255 12.613304 33.850875
1.010278
SNP8
SNP9
SNP10
SNP11
SNP12
1.311907
1.007684 1.244746
1.814737
1.011572
A number of these VIFs exceed the action limit of 10, some substantially so, and indeed, even
their mean exceeds 10: VIF = 10.825.

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
235
To address this obvious multicollinearity, a regularization method such as ridge regression
could be applied. For instance, the lm.ridge() function in the MASS package provides a
basic ridge analysis. Start by centering the response variable and standardizing the predictors.
(To correspond with the default scaling in lm.ridge(), the following sample R code sets the
predictors to zij =
√
n∕(n −1)(xij −xj)∕sj.)
> U <- Phenotypes - mean(Phenotypes)
> z1 <- sqrt(n/(n-1)) * scale( SNP1 )
> z2 <- sqrt(n/(n-1)) * scale( SNP2 )
⋮
> z11 <- sqrt(n/(n-1)) * scale( SNP12 )
Next, set a range of values for 𝜅(here, 0 < 𝜅≤50), and call the lm.ridge() function.
(The lambda= option calculates the regularized fit over each of the values for 𝜅.)
> kappa = seq(.01,50,.01)
> GenCont11.ridgelm = lm.ridge( U ˜ z1 + z2 + z4 + z5 + z6
+ z7 + z8 + z9 + z10 + z11 + z12, lambda=kappa )
The GenCont11.ridgelm object now contains the various summary statistics used
for selecting an operative value for 𝜅. In particular, a quick summary is available via
select(GenCont11.ridgelm), producing (edited) output
modified HKB estimator is 9.40075
smallest value of GCV
at 22.05
That is, the HKB estimate from (7.36) (‘HKB’ stands for Hoerl, Kennard, and Baldwin, the
authors of the originating 1975 article) is roughly 𝜅HKB = 9.4, while the minimum-GCV error
occurs at 𝜅GCV = 22.05. A ridge trace plot helps to visualize the former suggestion: simply use
plot(GenCont11.ridgelm) for a stock graphic or construct the plot directly by overlaying
the individual ̂𝛽j𝜅values available in the R vectors
GenCont11.ridgelm$coef[j,]
(j = 1, … , p). The latter strategy produces the trace plot in Figure 7.8. Notice how the
trace curves flatten and stabilize as 𝜅grows large, with a clear effect occurring after about
𝜅HKB = 9.4. Similarly, a graph of the GCV error created by plotting
GenCont11.ridgelm$GCV
against 𝜅clearly shows the minimum at 𝜅GCV = 22.05 (see Figure 7.9).
From this analysis, either 𝜅HKB = 9.4 or 𝜅GCV = 22.05 could be taken as reasonable oper-
ating values for 𝜅. Using the latter, reimplement lm.ridge():
> GenCont.ridgelm <- lm.ridge( U ˜ z1 + z2 + z4 + z5 + z6
+ z7 + z8 + z9 + z10 + z11 + z12, lambda=22.05 )
> GenCont.ridgelm$coef
The resulting output gives the estimated regression coefficients:
z1
z2
z4
z5
z6
z7
0.3651 -0.1016
0.0273
0.7736 -0.1706 -0.0179
z8
z9
z10
z11
z12
-0.0707
0.0169
0.0280
0.0414 -0.0367

236
STATISTICAL DATA ANALYTICS
0
10
20
30
40
50
−0.2
0.0
0.2
0.4
0.6
0.8
κ
βj
Figure 7.8
Ridge trace plot over values of tuning parameter 𝜅for the Genetic SNP data in
Example 7.4.3. Dashed horizontal line marks location of HKB estimator 𝜅HKB = 9.4. Source:
Data from data(GenCont) in R ridge package.
From these, the minimum-GCV predicted values ̂Yi(𝜅GCV) can be constructed using (7.33):
> Zmtx <- as.matrix( cbind(z1,z2,z4,z5,z6,z7,z8,z9,z10,z11,z12) )
> Yhat <- Zmtx %*% GenCont.ridgelm$coef + mean(Phenotypes)
(The sample R code here adds back Y to recover the original scale.) Study of the
̂Yi(𝜅GCV) values may identify potential associations with the SNP predictors. For example,
Figure 7.10 gives single-variable prediction plots of ̂Yi(𝜅GCV) against z1 (Figure 7.10a) and
z5 (Figure 7.10b). Positive associations are evidenced between the predicted phenotypic
outcome and the individual SNP predictors, suggesting the need for further investigation.
Exercise 7.24 explores other features of the ridge regression analysis with these data.
◽
Ridge regression is employed primarily to improve prediction of the mean response in the
face of high multicollinearity and/or when the number of predictor variables is very large. In
the latter case, it becomes a useful strategy to help avoid overfitting. At its core, however, it is
primarily a prediction/estimation methodology. Construction of standard errors or inferences
such as confidence limits is less common, due to the bias in ̂𝜷𝜅. (Standard errors for biased
point estimators can give an indication of the estimation uncertainty, but they may also give a
distorted impression of that uncertainty because they may not take the bias into account.)
If confidence intervals/statements on, say, the regression coefficients are of absolute neces-
sity, suggestions have been made to apply bootstrap methods with the ridge fit (Crivelli et al.
1995), as in Section 5.3.6. Obenchain (1977) discussed some other possibilities. In general,
caution is advised: any confidence region method must carefully incorporate all sources of

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
237
0
10
20
30
40
50
0.002104
0.002108
0.002112
κ
GCV
Minimum GCV at κ = 22.05
Figure 7.9
Generalized cross validation (GCV) error over values of tuning parameter 𝜅for
the genetic SNP data in Example 7.4.3. Minimum GCV identified at 𝜅GCV = 22.05. Source:
Data from data(GenCont) in R ridge package.
−1.0
−0.5
0.0
0.5
(a)
(b)
1.0
1.5
2.0
1
2
3
4
5
z1
Predicted value
0
1
2
3
1
2
3
4
5
z5
Predicted value
Figure 7.10
Individual predicted value plots against selected SNP predictors (a) z1 and (b) z5
for the genetic SNP data in Example 7.4.3. Source: Data from data(GenCont) in R ridge
package.

238
STATISTICAL DATA ANALYTICS
uncertainty when constructing statements about the model components in (biased) shrinkage
regression.
7.4.3
Regularization and variable selection: the Lasso
Another popular penalty term for use in the regularization objective function (7.31) is the
(absolute) first-order penalty
𝒬(𝛽j) = |𝛽j|,
often called an ‘L1’ penalty because it mimics a first-order length measure (or ‘norm’) in
mathematics (Vidaurre et al. 2013). (With its appeal to sums of squared terms, ridge regression
corresponds to an L2 penalty.) Thus the new objective function to be minimized is
(𝜷) =
n
∑
i=1
{
Yi −
(
𝛽0 +
p
∑
j=1
𝛽jxij
)}2
+ 𝜅
p
∑
j=1
|𝛽j|.
(7.38)
Tibshirani (1996) proposed (7.38) for use with MLR data when a large number of predictors
leads to problems with unstable or overdetermined structure. Similar to the L2 ridge setting,
minimizing (7.38) is equivalent to a constrained optimization:
minimize
n
∑
i=1
{
Yi −
(
𝛽0 +
p
∑
j=1
𝛽jxij
)}2
subject to
p
∑
j=1
|𝛽j| ≤t𝜅,
(7.39)
where again, the positive bound t𝜅will be related to the tuning parameter 𝜅.
The penalized optimization operates somewhat differently here than in ridge regression.
There is no closed-form expression for the estimator ̂𝜷, and a numerical solution is required.
The result is a more-abrupt form of shrinkage, with some ̂𝛽j coefficients driven exactly to zero
as 𝜅grows. Thus the L1-penalized fit is more sparse, that is, it contains only a subset, and
sometimes a substantially reduced subset, of the original p predictors. The regularization,
therefore, serves both as a form of shrinkage regression and as a sort of variable selector,
because it literally zeros out the contribution of low-impact or low-consequence predictor
variables. In effect, the variable selection is conducted by penalizing on the regression coef-
ficients rather than on the number of parameters (Chen et al. 2014). Tibshirani called this
procedure the least absolute shrinkage and selection operator or ‘Lasso’ for short.
When substantial multicollinearity is present among the predictors, ridge regression will
usually be more effective in terms of its predictive capability (Tibshirani 1996); however, the
ability to combine shrinkage regression with de facto variable selection makes the Lasso a
useful addition to the MLR toolkit. As with ridge regression, the data are usually assumed to
have been centered, Ui = Yi −Y, and the predictor variables to have been standardized into
z-scores zij = (xij −xj)∕sj. As a result, the intercept term 𝛽0 estimates exactly as zero.
Selection of 𝜅in (7.38) – or equivalently t𝜅in (7.39) – is usually conducted via appeal to
CV, similar to the approach taken with the ridge regression tuning parameter in Section 7.4.2.
In the simplest case, one removes each observation from the original data set and then uses the
remaining data to estimate the value of that excised observation under a series of candidate val-
ues for 𝜅. With very large n, this becomes computationally costly, so an efficient modification
breaks the data into K equal subsamples. Over k = 1, … , K, the kth subsample is sequestered
for validation purposes and the remaining K −1 subsamples are collected into a training set

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
239
to estimate the responses in that kth subsample. The squared differences between the actual
observations and their cross-validated predictions measure the prediction error. Reapplying
this K-fold deletion across all K subsamples and averaging produces a CV error. Select the 𝜅
that minimizes this error estimate. This method is known as K-fold cross-validation. If K = n,
it collapses to single-case deletion (‘leave-one-out’) CV. For further introductions to CV in
statistical learning, see, for example, Clarke et al. (2009, Section 1.3.2) or James et al. (2013,
Section 5.1).
As one might imagine, application of the Lasso when n and/or p is large is not trivial.
Strategies for numerical implementation vary, although ongoing advances have drastically
decreased the computational burden. A preferred algorithm is known as ‘coordinate descent’
(Friedman et al. 2007; Wu and Lange 2008). In R, this is available via the glmnet() function
in the external glmnet package.
Example 7.4.4 Ridge regression with genetic SNP data (Example 7.4.3, continued).
Continuing with the genetic SNP data from Example 7.4.3, consider application of the Lasso
to the regression with all p = 11 SNP predictors. The nontrivial multicollinearity here would
normally call for regularization via ridge regression, but it is instructive to examine how the
Lasso fit operates with these data. Indeed, there is value here in viewing the exercise from a
variable-selection perspective.
To allow for comparison with Example 7.4.3, the phenotypic outcome is centered
into the response variable Ui and the various predictors are centered and scaled via
zij =
√
n∕(n −1)(xij −xj)∕sj. To apply the Lasso in R, invoke glmnet(). Sample code is
> require( glmnet )
> Zmtx <- cbind( z1,z2,z4,z5,z6,z7,z8,z9,z10,z11,z12 )
> GenCont11.glmnet <- glmnet( x=Zmtx, y=U,
family=‘gaussian’, alpha=1 )
where U and the z-vectors are as given in Example 7.4.3. The call to glmnet() requires direct
input of the design matrix Z, here as x=Zmtx. Notice that no intercept is included in the matrix.
The specification y=U identifies the (centered) response variable. The family=‘gaussian’
option indicates that the data are continuous measurements suitable for modeling with a nor-
mal distribution. (Other options exist for nonnormal data; see help(glmnet).) The alpha=1
option institutes the Lasso L1 penalty.
The available outputs from glmnet are substantial and span a range of uses; simplest for
our purposes here is a stock graphic of the ̂𝛽j coefficient profiles:
> plot( GenCont11.glmnet, xvar=‘lambda’, label=T )
> abline( h=0, lwd=3 )
The xvar=‘lambda’ option plots the estimated coefficients against log(𝜅) (the log scale is
recommended for better visualization), while the label=T option inserts (small) markers at
the end of each trace indicating the associated predictor. The subsequent call to abline()
overlays a thick horizontal line at 𝛽= 0 (see Figure 7.11).
The coefficient profile plot illustrates the expected shrinkage as log(𝜅) grows, along with
the Lasso’s characteristic contraction of each ̂𝛽j to exactly zero with increasing penalization.
Viewed from a variable-selection perspective, most of the SNP predictors drop fairly quickly,
with only three variables remaining by log(𝜅) = −3, that is, 𝜅≈0.05. (The digits at the top of
the plot help in this regard: they indicate how many nonzero predictors remain in the model at

240
STATISTICAL DATA ANALYTICS
−6
−5
−4
−3
−2
−1
0
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
log(κ)
Coefficients
10
10
9
3
1
1
1
Figure 7.11
Lasso-penalized regression coefficient profiles for the genetic SNP data in
Example 7.4.4, plotted as a function of log(𝜅). Solid horizontal line indicates 𝛽j = 0. Source:
Data from data(GenCont) in R ridge package.
the corresponding value of log(𝜅) on the horizontal axis.) These three predictors are, in order
of retention, z4 and then (essentially tied) z7 and z9. At, for example, 𝜅= 0.05, the nonzero
coefficients can be estimated via the R command coef(GenCont11.glmnet,s=0.05),
where the s=0.05 option dictates the specific value of 𝜅at which glmnet generates the
estimates. The resulting output (edited) is
(Intercept)
2.984784e-16
V4
1.036733e+00
V7
-1.643679e-02
V9
1.952590e-02
(Notice that the intercept, as expected, estimates as essentially machine zero.) Further inves-
tigation would be warranted to determine which genetic features these three predictors were
marking.
Selection of 𝜅via K-fold CV is available using glmnet’s cv.glmnet() subfunction:
> GenCont11.cv.glmnet <- cv.glmnet( x=Zmtx, y=U, nfolds=20 )
The CV is conducted over an internally selected range for 𝜅> 0; the nfolds=20
option here sets K = 20. Upon output, the CV error is contained in the vector Gen-
Cont11.cv.glmnet$cvm; the corresponding values for 𝜅are in GenCont11.cv.glmnet$
lambda. A plot of the error against log(𝜅) appears in Figure 7.12. (One can alterna-
tively generate a pre-supplied stock graphic via plot(GenCont11.cv.glmnet). See
help(plot.cv.glmnet).)

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
241
−6
−5
−4
−3
−2
−1
0
1.0
1.2
1.4
1.6
1.8
2.0
2.2
log(κ)
Cross-validation error
Minimum CV error at
log(κ) = −2.613
Figure 7.12
K-fold cross-validation error at K = 20 for the genetic SNP data in
Example 7.4.4, plotted as a function of log(𝜅). Dashed vertical line indicates minimum error
at log(𝜅) = −2.6130. Source: Data from data(GenCont) in R ridge package.
As indicated in the figure, the CV error is minimized with these data at log(𝜅) = −2.6130,
that is, 𝜅= 0.0733. (This is also calculated via GenCont11.cv.glmnet$lambda.min.)
Referring back to Figure 7.11, this corresponds to shrinkage of all but one coefficient to
zero: the only SNP predictor selected by the Lasso at the minimum (20-fold) CV is z4. Its
estimated regression coefficient can be conveniently accessed via
> coef( GenCont11.cv.glmnet, s=‘lambda.min’ )
producing (output edited)
(Intercept)
2.909530e-16
V4
1.015418e+00
◽
A variety of extensions on the regularization concept have appeared in the statistical
learning literature. For example, explicitly incorporating a power parameter q ≥0 into the
penalization term produces the objective quantity
q(𝜷) =
n
∑
i=1
{
Yi −
(
𝛽0 +
p
∑
j=1
𝛽jxij
)}2
+ 𝜅
p
∑
j=1
|𝛽j|q
(Frank and Friedman 1993), now referred to as bridge regression (Fu 1998). Or, an expanded
class of shrinkage estimators that includes both the Lasso and ridge regression is known as
elastic net regularization (Zou and Hastie 2005). This extends the penalty function in (7.38)

242
STATISTICAL DATA ANALYTICS
into a convex combination of L1 and L2 terms:
𝛼(𝜷) =
n
∑
i=1
{
Yi −
(
𝛽0 +
p
∑
j=1
𝛽jxij
)}2
+ 𝜅
{
𝛼
p
∑
j=1
|𝛽j| + (1 −𝛼)
p
∑
j=1
𝛽2
j
}
for some selected 𝛼∈[0, 1]. At the extremes, 𝛼= 0 produces ridge regression while 𝛼= 1
yields the Lasso. The glmnet() function in the external glmnet package performs elastic net
regularization via its alpha= option.
For more on regularization/shrinkage in linear regression problems, see Hastie et al. (2009,
Section 3.4) or Sundberg (2012).
7.5
Qualitative predictors: ANOVA models
When the predictors thought to affect the mean response are qualitative rather than
quantitative – for example, sex or ethnic status in a marketing survey, or patient condition
when categorizing medical records – modifications are required in the MLR analysis. A
natural approach is to assign a set of codes to the different levels of the qualitative variable(s)
and build from these a series of quantitative predictor variables. For instance, set xi1 = 1 if
the ith observation corresponds to the first level of the qualitative predictor (zero otherwise),
xi2 = 1 if the ith observation corresponds to the second level (zero otherwise), and so on. In
this manner, multiple linear predictors can be constructed to account for qualitative as well
as quantitative predictor variables.
With qualitative predictors, however, interpretation of the regression coefficients as
slopes/changes-in-effect becomes ambiguous. As a result, the model is usually written with
a more traditional parameterization, called an ANOVA structure. (The name comes from
use of an analysis of variance to assess differences between the levels of the qualitative
factor. How variation compares among these levels is analyzed to identify the differences;
see Table 7.6.) To each qualitative factor, the model assigns certain effect parameters. Thus
if a single factor ‘A’ has a > 1 categories or levels, write the effect parameters as 𝛼i over
the i = 1, … , a levels. A second index, j, is included to account for replicate observations
at each combination of this single factor. Similar to (7.1), the statistical model becomes
Yij ∼indep. N(𝜇i, 𝜎2), where now the mean response 𝜇i = E[Yij] is taken as
𝜇i = 𝜃+ 𝛼i,
(7.40)
over i = 1, … , a levels of the single qualitative factor and j = 1, … , ni independent replicates
per factor level. The standalone parameter 𝜃may be interpreted as the grand mean of the
model, while the 𝛼i parameters are viewed as deviations from 𝜃due to the effects of factor
A. This is a one-factor ANOVA model. The total sample size is n+ = ∑a
i=1 ni. If ni is constant
at each level of i, that is, ni = n, then the ANOVA is balanced. Deviations from a balanced
design are called unbalanced.
Unfortunately, this factor-effect parameterization does not provide sufficient informa-
tion to estimate every parameter. As currently written, (7.40) describes a + 1 parameters,
𝜃, 𝛼1, … , 𝛼a; however, only a different factor levels are sampled from which to estimate
these values. For instance, one could estimate all a values of 𝛼i, but not 𝜃. A simple solution
to overcome this imposes an estimability constraint on the 𝛼is. Many possible constraints

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
243
exists; two popular versions are the zero-sum constraint ∑a
i=1 𝛼i = 0 and the corner-point
constraint 𝛼1 = 0.
If applied properly, constraints on the 𝛼is do not affect tests of the effects among levels
of the factor; however, they do affect interpretation of the parameter estimates. For example,
under the zero-sum constraint, 𝜃is the mean of all the a group means and 𝛼i is the difference
between the ith factor level mean and the overall mean. By contrast, under the corner-point
constraint, 𝜃is the mean of the first factor level, while 𝛼i is the difference between the ith
level’s mean and this first level’s mean.
The predicted response within each ith factor level is estimated as the per-level mean:
̂Yij = Yi = 1
ni
a
∑
i=1
Yij.
Differential variation between levels of factor A is quantified by summing the squared differ-
ences between these Yis and the overall mean Y = ∑a
i=1
∑ni
j=1 Yij∕ni. This is
SSA =
a
∑
i=1
ni
∑
j=1
(Yi −Y)2.
As it is a sum of squares, SSA has an associated number of d.f.; here, dfA = a −1. As
such, we can divide SSA by its d.f. to produce a mean square for between-factor effects:
MSA = SSA∕(a −1).
Variation within factor A may be similarly quantified as the sum of squared differences
between Yij and its predicted value ̂Yij = Yi. Pooled over all levels of A, this is essentially a
residual sum of squares, as in (6.10), so we write
SSE =
a
∑
i=1
ni
∑
j=1
(Yij −Yi)2.
The associated d.f. here are dfE = n+ −a, with MSE = SSE∕(n+ −a). As previously, the MSE
is unbiased for estimating the unknown variance 𝜎2 (Exercise 7.6).
It can be shown (Kutner et al. 2005, Section 16.5) that the two sums of squares SSA and
SSE will themselves add to the total sum of squares
SSA + SSE = SSTo =
a
∑
i=1
ni
∑
j=1
(Yij −Y)2.
This decomposition of observed variation is collected together for convenient calculation.
The result is called an analysis of variance (ANOVA) table, as given in Table 7.6. Notice the
inclusion of an F-statistic in the table: F = MSA∕MSE. This is essentially the discrepancy
measure in (7.18), which is used here to test the RM Ho: 𝛼1 = 𝛼2 = · · · = 𝛼a, that is, factor A
has no effect on the mean response. Under this null hypothesis, F ∼F(a −1, n+ −a). Reject
Ho in favor of any departure at false positive level 𝛼when the calculated statistic Fcalc exceeds
the critical point F𝛼(a −1, n+ −a). The corresponding P-value is P[F(a −1, n+ −a) ≥Fcalc].
In the simplest case of a = 2 levels with a single factor, this construction collapses to the
two-sample t-test from section 5.4.2. The F-statistic in Table 7.6 equates exactly to the square

244
STATISTICAL DATA ANALYTICS
Table 7.6
Schematic for single-factor analysis of variance (ANOVA) table.
Source
d.f.
SS
MS
F
Factor A
a −1
SSA = ∑a
i=1
∑ni
j=1 (Yi −Y)2
MSA = SSA
a −1
MSA/MSE
Residual
n+ −a
SSE = ∑a
i=1
∑ni
j=1 (Yij −Yi)2
MSE =
SSE
n+ −a
Total
n+ −1
SSTo = ∑a
i=1
∑ni
j=1 (Yij −Y)2
of the t-statistic in Equation (5.44), and a similar relationship holds between their reference
distributions. Thus the (two-sided) inferences from both tests will always coincide.
A second, qualitative factor ‘B’ can be added to the study design, expanding the nota-
tion for the observations to Yijk, with indices i = 1, … , a for factor A, j = 1, … , b for fac-
tor B, and k = 1, … , nij for replicate observations at each combination of the two factors.
Equation (7.40) becomes
𝜇ij = 𝜃+ 𝛼i + 𝛽j.
This is a two-factor, main-effects ANOVA model, so named because it contains effects due to
only the two main factors. Allowing for the possibility that the two factors may interact leads
to addition of cross-classified interaction parameters
𝜇ij = 𝜃+ 𝛼i + 𝛽j + 𝛾ij.
(7.41)
In either case, the total sample size is n+ + = ∑a
i=1
∑b
j=1 nij.
As in (7.40), estimability constraints are required under this factor-effects parameteriza-
tion. The zero-sum constraints are ∑a
i=1 𝛼i = ∑b
j=1 𝛽j = 0, ∑b
j=1 𝛾ij = 0 for all i, and ∑a
i=1 𝛾ij =
0 for all j. The alternative corner-point constraints are 𝛼1 = 𝛽1 = 0, 𝛾i1 = 0 for all i, and 𝛾1j = 0
for all j.
Table 7.7 displays an archetypal two-factor ANOVA table. The table gives ‘sequential’
sums of squares (SS) for testing the significance of model components using the sequential
order in which they are fit. Similar to the partial t-tests in Section 7.1.3, the last sequential
is known as the ‘partial’ SS; it tests the significance of a model component when fitted last
in sequential order. Ratios of the specific mean squares to the MSE produce F-statistics to
assess the significance of any factor or any multifactor interaction, using the general approach
embodied in (7.18). Extensions to other multifactor models are also possible (Kutner et al.
2005, Chapter 24).
Table 7.7
Schematic for two-factor analysis of variance (ANOVA) table.
Source
d.f.
SS
MS
F
Factor A
a −1
SSA
MSA
MSA/MSE
Factor B
b −1
SSB
MSB
MSB/MSE
A × B interaction
(a −1)(b −1)
SSAB
MSAB
MSAB/MSE
Residual
n+ −ab
SSE
MSE
Total
n+ −1
SSTo

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
245
Similar to earlier comments on model construction with higher-order quantitative pre-
dictors, it does not usually make sense to include a higher-order qualitative term in a model
without also including all associated lower-order terms. Thus, for example, inclusion in (7.41)
of a two-factor interaction using the 𝛾ijs also requires inclusion of both main effects terms via
the 𝛼is and 𝛽js. This suggests a natural ordering for hypothesis testing: test the interaction first
as Ho: 𝛾ij = 0 for all i, j. If the interaction is insignificant, follow back up the sequential order
with a separate test of the next main effect via, for example, Ho: 𝛽j = 0 for all j.
Example 7.5.1 Per capita income in US counties. (Kutner et al. 2005, Appendix C) present
data on per capita income (in $) across the n = 440 largest counties in the United States, as
related to a variety of demographic measures. Here, consider the two factors A = {Geographic
region (Northeast, North central, South, or West) of the county} and B = {% college (bac-
calaureate) degrees (Low = less than 15%, Middle = between 15% and 30%, or High = greater
than 30%) among county residents}.
Table 7.8 gives cell mean summaries of the untransformed data. (The complete data set is
available at http://www.wiley.com/go/piegorsch/data_analytics. Notice the lack of balance in
the design.) As might be expected, per capita income rises with increasing college experience,
but it also shows some disparities among the different geographic regions. The differential
pattern among the four regions is a question of interest here, which translates to testing (i)
whether a significant interaction exists in these data and then (ii) testing for differences among
the regions. (To find the means in R, use
> aggregate( PerCapInc ˜ A+B, FUN=‘mean’ )
where PerCapInc is the original income variable, and where A and B are the two indicators,
each of R class ‘factor’.)
As per capita incomes often exhibit a large skew, apply a logarithmic transform and take
Y = log{income} as the response variable. With this, assume the full two-factor model from
(7.41). In R, this is fit by appealing to the lm() function, but with the special introduction of
‘factor’ variables for building the linear predictor. To do so, the factor() function can create
qualitative factors from any appropriately structured variable. Sample R code is
> Ex751.lm <- lm( Y ˜ factor(B) + factor(A) + factor(B):factor(A) )
Table 7.8
Summary cell means (and sample sizes) of per capita income (standardized to
1990 $) among n = 440 US counties.
Factor B: % college degrees
Low: <15%
Middle: 15%–30%
High: >30%
Northeast
15 916.05
20 585.29
27 021.29
(n11 = 19)
(n12 = 70)
(n13 = 14)
North central
16 578.25
18 600.18
21 974.17
(n21 = 36)
(n22 = 60)
(n23 = 12)
Factor A: Region
South
14 332.97
17 581.20
21 899.95
(n31 = 31)
(n32 = 101)
(n33 = 20)
West
15 045.75
17 978.56
24 652.64
(n41 = 16)
(n42 = 50)
(n43 = 11)

246
STATISTICAL DATA ANALYTICS
The linear predictor here calls for sequential fit of first the factor B main effect, then the
factor A main effect, and finally the B × A interaction. (The ordering places B first because
the college factor here is an expected source of variation – called a ‘blocking variable’ in
the experimental design literature – and, hence, should have its effects accounted for first in
the sequential order.) The interaction is indicated in R via the colon (:) operator. A shorthand
syntax for the combination of all three terms is simply
> formula = Y ˜ factor(B)*factor(A)
The * operator, when employed in an lm linear predictor, is not to be confused with sim-
ple multiplication. Here, it produces the cross-classification necessary to fit (7.41), which is
something quite different.
In constructing the consequent ANOVA, R operates under corner-point constraints. To
display the ANOVA table, use anova( Ex751.lm ), which produces the following (edited)
output
Analysis of Variance Table
Response: Y
Df
Sum Sq
Mean Sq
F value
Pr(>F)
factor(B)
2
6.0454
3.02272
120.9328
< 2.2e-16
factor(A)
3
1.6253
0.54177
21.6750
4.366e-13
factor(B):factor(A)
6
0.3934
0.06557
2.6232
0.01652
Residuals
428
10.6979
0.02500
(R does not display SSTo in its standard ANOVA tables.) The P-values in the final column
correspond to sequential F-tests of that factor’s or interaction’s effect.
From the R output, begin with the test for no A × B interaction via Ho∶
𝛾ij = 0 for all i, j. The pertinent F-statistic is the partial (i.e., last sequential) Fcalc = 2.6232.
Referred to F(6, 428), this is significant at, say, the 10% level: P = 0.0165 < 0.10. For these
data, the pattern of response among different regions changes significantly as the rate of
college education varies.
In the presence of a significant interaction, it is inappropriate to test for a main effect,
because the main effect pools over levels of its factor. This can mask true differences across
each individual factor level when interaction is present (Kutner et al. 2005, Section 19.7). For
these data, this requires us to examine the ‘simple effects’ among the factor A cell means
within each level of factor B. (As factor B is a known source of variation here, there is no
need to test its effects. If we were to do so, however, we would also study ‘simple effects’
among the factor B cell means within each level of factor A.)
The comparisons can be performed in a variety of ways (Kutner et al. 2005, Section 23.3).
For example, we can build hypothesis tests of the simple pairwise comparisons Hii′j: 𝜇ij =
𝜇i′j (i < i′) at each individual j = 1, 2, 3. The test statistics have the form
Tii′j =
Yij −Yi′j
√(
1
nij +
1
ni′j
)
MSE
,
(7.42)
where Yij = ∑nij
k=1 Yijk∕nij. Under Hii′j, (7.42) is referenced to a t-distribution: Tii′j ∼t(dfE)
where dfE = n+ −ab. To correct for multiplicity, compare each of the resulting 3 × (4
2
) =

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
247
Table 7.9
Test statistics (7.42) and Bonferroni-corrected P-values (7.43) from simple
effects/pairwise comparison analysis of factor A (Geographic region) in Example 7.5.1.
Factor B: % college degrees
Comparison
Low: <15%
Middle: 15–30%
High: >30%
Northeast vs North central
T121 = −0.9017
T122 = 3.3927
T123 = 3.3113
P121 = 1
P122 = 0.0136
P123 = 0.0181
Northeast vs South
T131 = 2.5503
T132 = 6.4345
T133 = 3.8586
P131 = 0.2000
P132 = 2.1 × 10−7
P133 = 0.0024
Northeast vs West
T141 = 1.1409
T142 = 4.6611
T143 = 1.3797
P141 = 1
P142 = 0.0001
P143 = 1
North central vs South
T231 = 4.0760
T232 = 2.4774
T233 = 0.1148
P231 = 0.0010
P232 = 0.2451
P233 = 1
North central vs West
T241 = 2.1394
T242 = 1.3901
T243 = −1.7890
P241 = 0.5934
P242 = 1
P243 = 1
South vs West
T341 = −1.1562
T342 = −0.7959
T343 = −2.1010
P341 = 1
P342 = 1
P343 = 0.6520
Corrected P-values above 10% FWE rate are deemphasized via gray tone.
18 statistics to a Bonferroni-adjusted critical point: reject Hii′j in favor of any (two-sided)
departure when |Tii′j| ≥t𝛼∕(2×18)(428). At an FWE rate of 10%, the Bonferroni critical point is
t0.10∕36(428) = t0.0038(428) = 2.7871. Equivalently, reject Hii′j when its Bonferroni-corrected
P-value
Pii′j = min{(18)(2)P[t(428) ≥|Tii′j|], 1}
(7.43)
exceeds 𝛼= 0.10.
Table 7.9 gives the t-statistics from (7.42) and the associated adjusted P-values. As
expected, the pattern of pairwise differences among levels of factor A (geographic region)
differs across the three levels of factor B (college degrees). Regions at the low percentage of
college degrees only differ significantly between the North central and South regions (and,
this is the only instance where those two regions differ significantly).
By contrast, Regions at the middle and high percentages of college experience show con-
sistent, significant differences for both the Northeast versus North central and Northeast versus
South comparisons. The Northeast versus West comparison at the middle level of college
experience is also significant. (None of the North central versus West or South versus West
comparisons is significant.) Further investigation into how these patterns relate per capita
income and college experience could lead to intriguing knowledge discovery.
A residual plot of Yij −̂Yij = Yij −Yij against ̂Yij = Yij is presented in Figure 7.13. No seri-
ous outliers are detected, although an indication of nontrivial variance heterogeneity appears.
Perhaps a transformation of the original per capita incomes other than the natural logarithm
may be more appropriate; this is explored in Exercise 7.30.
◽

248
STATISTICAL DATA ANALYTICS
9.6
9.8
10.0
10.2
−0.4
−0.2
0.0
0.2
0.4
Predicted value
Residual
Figure 7.13
Raw residual plot from two-factor ANOVA fit in Example 7.5.1. Source: Data
from Kutner et al. (2005, Appendix C).
Exercises
7.1
Return to the MLR analysis of the cancer mortality data in Example 7.1.1. Corroborate
the indication in the example that the two predictor variables x4 = {Civic activity} and
x6 = {Emergency bed-days} are insignificant by applying a single 2 d.f. test via the
F-statistic discrepancy measure in (7.18). Operate at 𝛼= 0.05.
7.2
Yeh (1998) described data on the compressive strength (in MPa) of high-performance
concrete, as related to a variety of component predictors. A selection of the n = 1030
observations follows. (Download the complete data set at http://www.wiley.com/go
/piegorsch/data_analytics.) All predictor variables are measured as kilogram per cubic
meter unless otherwise specified.
Replicate index, i
Outcome variable
i = 1
i = 2
i = 3
· · ·
i = 1029
i = 1030
Y = Strength (MPa)
79.99
61.89
40.27
· · ·
32.77
32.40
x1 = Age (days)
28
28
270
· · ·
28
28
x2 = Cement
540.00
540.00
332.50
· · ·
159.10
260.90
x3 = Furnace slag
0.00
0.00
142.50
· · ·
186.70
100.50
x4 = Superplasticizer
2.50
2.50
0.00
· · ·
11.30
8.60
x5 = Water
162.00
162.00
228.00
· · ·
175.60
200.60
x6 = Fly ash
0.00
0.00
0.00
· · ·
0.00
78.30
x7 = Coarse aggregate
1040.00
1055.00
932.00
· · ·
989.60
864.50
x8 = Fine aggregate
676.00
676.00
594.00
· · ·
788.90
761.50

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
249
(a) It was recognized that the compressive strength outcome variable was skewed and
would require a transformation, as in Section 3.4.3. Here, use Y =
√
Strength as the
response. Fit the MLR model to these data with this transformed response variable
and the p = 8 predictors given above. Use partial t- or F-tests to determine if any of
the predictors significantly affects the response. (Include a Bonferroni correction
to adjust for multiplicity.) Operate at 𝛼= 0.05.
(b) If you found any of the predictors in Exercise 7.2a to be insignificant, remove them
in a RM to fine-tune the fit. Corroborate your choice by testing the RM against the
eight-predictor FM via an F-statistic as in (7.18). Operate at 𝛼= 0.05.
(c) Find the raw residuals from your RM in Exercise 7.2b and plot them against the
fitted values. Also construct a normal quantile plot of these residuals. Comment on
the quality of the diagnostic plots.
(d) Find the Studentized deleted residuals from your RM in Exercise 7.2b and plot them
against the fitted values. Determine if any points are potential outliers by assessing
these residuals against the exceedance limits ±t𝛼∕(2n)(n −p −2). (Set 𝛼= 0.05. Can
you see any possible problems with these Bonferroni-adjusted limits here?) Do any
points appear to be potential outliers?
(e) Expand on the transformation in Exercise 7.2a and investigate whether a different
transformation may further stabilize the fit. Do so via the Box–Cox power trans-
formation from (3.13). To implement a Box–Cox search in R, apply the boxcox()
function to the lm object containing the eight-variable MLR fit of the data. The
function will search for a value of the power parameter, 𝜆, that maximizes the
log-likelihood along the 𝜆direction. Here, restrict 𝜆to the range −2 ≤𝜆≤2. Trans-
form Y according the recommended value of 𝜆(to the nearest reasonable number;
for example, if the function recommends 𝜆= 0.71, set 𝜆= 3
4). Repeat the analyses
above using the transformed response variable.
7.3
The study on admissions data from Example 6.5.3 also included the response vari-
able Y = {Student GPA (Grade Point Average)} in college. An additional question of
interest with these data was the predictive ability of the the two preliminary scores
x1 = {ACT} and x2 = {Class rank}. Conditioning the analysis on the observed pat-
tern of response in both ACT and Class rank scores, fit the MLR model with E[Y] =
𝛽0 + 𝛽1x1 + 𝛽2x2 to these data.
(a) Find the residuals ei = Yi −̂Yi from the LS fit and plot them against the fitted values
̂Yi. What patterns appear that bring into question the quality of the fit?
(b) To identify a possible transformation that may stabilize the fit for these data, con-
sider the Box–Cox power transformation from (3.13). To implement a Box–Cox
search in R, apply the boxcox() function to the lm object containing your
two-variable MLR fit of the data. The function will search for a value of the power
parameter, 𝜆, that maximizes the log-likelihood along the 𝜆direction. Here, restrict
𝜆to the range −3 ≤𝜆≤3. Transform Y according to the recommended value of 𝜆
(to the nearest reasonable number; e.g., if the function recommends 𝜆= 2.03, set
𝜆= 2). Fit the appropriate MLR model using the transformed response variable.
(c) Plot the residuals from the transformed fit. Does the pattern appear more stable?

250
STATISTICAL DATA ANALYTICS
(d) Find the LS estimators for 𝛽1 and 𝛽2 from the data-transformed fit. What interpre-
tation does each have?
(e) Test if either predictor variable significantly affects the transformed college GPA
value. Use partial t- or F-tests. Operate at an FWE rate of 𝛼= 0.01 and adjust
your inferences for multiplicity. (Technically, this is also conditional on the selected
value of 𝜆.) What do you conclude?
(f) Calculate the hat matrix diagonals, hii, for the (xi1, xi2) pairs and determine if any
points exhibit high leverage. (Hint: For visualization purposes, plot x2 versus x1
and mark the high-leverage points, if any, on the plot.)
(g) Add a cross-product/interaction predictor, x1x2, to the (transformed) MLR. Use a
partial t- or F-test to determine if it significantly improves the fit. Operate at 𝛼=
0.01. If the term is significant, also plot the residuals to check for any improvement
or deterioration in their pattern.
7.4
A cadre of modern recording artists had their Twitter activity examined to determine
if the data could relate to first week sales of a new album. p = 3 predictor variables
were taken: x1 = {Number of Twitter followers (thousands)}, x2 = {Average tweets per
day}, and x3 = log{Previous album’s first week sales}. The response was Y = log{New
album’s first week sales}. The data were
Artist
Y
x1
x2
x3
Asher Roth
8.7160
118.5
2.1
11.0880
Ciara
11.3022
202.3
10.0
12.7321
Fabolous
11.6440
228.0
12.1
11.9767
Jordin Sparks
10.7579
350.8
17.4
11.6869
Maxwell
12.6635
70.0
1.1
12.5994
Trey Songz
11.9250
352.0
14.4
11.1982
(a) Fit an MLR model to these data. Test if the overall three-predictor model is signif-
icant. Operate at a false positive rate of 10%.
(b) Use partial t-tests to assess whether each individual predictor variable significantly
affects mean (log-)new-album sales. Adjust for multiplicity at a false positive FWE
of 10%.
(c) What concerns might exist about the quality of the model fit with this data set?
7.5
Return to the automobile fuel economy in Example 7.4.2. Verify the indication there
that multicollinearity is elevated, but not serious for these data by calculating VIFs for
the two (scaled) predictor variables x1 and x2.
7.6
Use (7.8) to prove that the MSE under the MLR model is an unbiased estimator of the
population variance 𝜎2.
7.7
Verify the indication in Section 7.1.1 that the SSE under the MLR model can be written
as SSE = eTe = YT(I −H)Y. (Hint: What is (I −H)T(I −H)?) Is this also true for the
SLR model?

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
251
7.8
Apply the matrix formulation in Section 7.1.1 to the SLR model in Section 6.2.1. In
particular,
(a) Find the X matrix and from this the (XTX) and (XTY) matrices. From (7.5), verify
the normal equations in (6.3).
(b) Find the (XTX)−1 matrix and from (7.6) verify the equations for the LS estimators
in (6.4).
(c) Verify the expression for the hat matrix diagonal elements hii in (6.24).
7.9
Similar to the study discussed in Exercise 6.11, the online site payscale.com released
2013 data on median, annual, full-time earnings (in $, to the nearest hundred) among
n = 452 public/state colleges and universities in the United States; see http://www.
payscale.com/college-salary-report-2013. For each college, reported were x = {Start-
ing salaries} and Y = {Mid-career salaries}. The data are available online at http://www
.wiley.com/go/piegorsch/data_analytics; a sample is given as follows:
x = Starting salary:
35 400
33 800
44 900
· · ·
76 000
72 200
Y = Mid-career salary:
43 600
44 400
81 800
· · ·
120 000
122 000
(a) Plot Y versus x. Does a curvilinear pattern appear?
(b) Condition the analysis on the observed pattern of starting salaries and fit a quadratic
regression model to these data via LS. Report the estimated mean response ̂𝜇(x) and
overlay it on the scatterplot of the data. Comment on the quality of the fit.
(c) Construct a 2 d.f. test to determine if a significant effect exists due to the predictor
variable, x. Operate at 𝛼= 0.01.
(d) Test if the quadratic term significantly improves on the model fit, above and beyond
the linear term. Operate at 𝛼= 0.01.
(e) Recall that in Exercise 6.11, a concern was raised over possible variance hetero-
geneity. Assess if this is also a concern with these data.
7.10
Moore (2010, Compan. Chapter 27) listed data on how the price (in $) of a diamond
relates to the weight of the stone (in carats). Of interest is estimating the mean price for
differently sized diamonds. The observations comprise n = 351 data pairs and are avail-
able online at http://www.wiley.com/go/piegorsch/data_analytics; a sample is given as
follows:
x = Carats:
3.35
3.17
3.01
· · ·
0.31
0.31
Y = Total price:
56454.40
54884.60
53191.20
· · ·
979.30
544.10
(a) Plot Y = Price against x = Carats. Is the pattern for the mean response linear or
curvilinear?
(b) Assume a quadratic regression model as in Section 7.2 for these data: set
𝜇(xi) = 𝛽0 + 𝛽1(xi −x) + 𝛽2(xi −x)2 and work under the normal (Gaussian) model
in (7.1). Calculate LS estimates for the 𝛽-parameters and also report the LS
estimate for 𝜇(x).

252
STATISTICAL DATA ANALYTICS
(c) Find the raw residuals Yi −̂Yi from your LS fit and plot them against the fitted
values ̂Yi. What pattern emerges? (Hint: Focus away from the small handful of
diamonds with very large fitted values.) Does this call into question any of the
assumptions made during this analysis?
7.11
To illustrate the effect centering the x variable can have when performing polynomial
regression, suppose n = 1000 observations are to be taken at equally spaced values of
xi = 1, 2, … , n. As ∑n
x=1 x = n(n + 1)∕2, we know x = (n + 1)∕2.
(a) Calculate the correlation between xi and x2
i , and compare it to the correlation
between xi −x and (xi −x)2.
(b) Find the VIFs for xi and x2
i in the linear model E[Yi] = 𝛽0 + 𝛽1xi + 𝛽2x2
i . (If you
appeal to the computer, recall that VIF calculations do not depend on the Yis. Thus
if your program requires Yis, just generate any arbitrary set of n values.) Does the
maximum VIF exceed the recommended threshold of 10?
(c) Now find the VIFs for xi −x and (xi −x)2 in the linear model E[Yi] = 𝛽0 +
𝛽1(xi −x) + 𝛽2(xi −x)2. How do they compare?
(d) The use of integer-valued predictors is irrelevant here. Illustrate that the same VIF
effects appear if you divide xi by some positive constant.
7.12
For the diamond data in Exercise 7.10 recognize that the residual pattern indicates
clear departure from homogeneous variance, so apply a WLS fit (Section 7.1.2) under
a quadratic model. For your weights, take 𝑤i = 1∕xi, where xi is the carat variable. Find
the WLS estimates for the 𝛽parameters, their standard errors, and the WLS estimate
for 𝜇(xi) = 𝛽0 + 𝛽1(xi −x) + 𝛽2(xi −x)2. What changes do you find versus the LS fit?
7.13
Return to the feature selection analysis with the UK cancer mortality data in Examples
7.3.1–7.3.4. Examine the data further, as follows. Throughout, operate at 𝛼= 0.05.
(a) Calculate an MLR fit using the seven predictors suggested in Example 7.3.1: the
four x-variables along with (x2 −x2)2, x1x2, and x1x′
4. Test to see if any of the three
new predictors are significant. (Remember to include a Bonferroni adjustment if
you apply simple 1 d.f. partial t-tests.) Also examine the residuals from your fit to
ascertain if any unusual patterns emerge.
(b) Replace the simple R2
[p] measure in Example 7.3.1 with the adjusted R2
A[p]. Plot
R2
A[p] against p and determine if a point of diminishing increase in R2
A[p] suggests
a possible collection of predictor variables for further study. Do your results differ
from those achieved in the example?
(c) Replace the simple R2
[p] measure with Mallows’ Cp. Plot Cp against p; you should
see a general decrease toward Cp = p as p increases. Determine the smallest p such
that Cp ≈p (but not substantially greater), and find the associated predictor config-
uration for further study. Do your results differ from those achieved in the example?
(d) In the regsubsets output from Example 7.3.2, what other interesting patterns
emerge among the 14 potential predictor variables?
(e) In Example 7.3.4, replace the AIC with the BIC as the information-based optimality
criterion. Do the results change qualitatively?

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
253
7.14
Return to the compressive strength data in Exercise 7.2 and apply a feature selection
search to identify a possible RM among the eight predictor variables with this data
set. Use the square-root-transformed response variable Y =
√
Strength. Employ back-
ward elimination and take minimum-BIC as your selection criterion. How does the
recommended set of variables compare to the RM you chose in Exercise 7.2b?
7.15
Return to the following data sets to experiment with loess smoothing. Begin by
(re)constructing scatterplots of the data. Then, apply a robust quadratic loess smooth
at q = 1
2. Overlay the loess predicted curve on your scatterplot. Also apply other
smoothing parameters in the range 0.2 ≤q ≤0.8 to visualize how changing the span
affects the loess smooth in each case.
(a) The financial moving average data on the Dow Jones Industrial index in Example
3.5.2. Overlay the original moving average smooth in Figure 3.1 and comment on
any differences.
(b) The Rocky Mountain Rainfall data in Example 4.2.10. Take x = {Year} as the
predictor variable and view each station’s rainfall as a replicated observation Y
at each x.
(c) The (random sample of) airline on-time performance data in Exercise 6.9a. Overlay
the SLR fitted line from the exercise and comment on any differences.
(d) The public college salary data in Exercise 7.9. Overlay the fitted parabola from the
exercise and comment on any differences.
7.16
Cleveland (1979) suggested an intriguing use of loess smoothing for enhancing resid-
ual diagnostics. The approach can be used to verify, or perhaps call into question,
indications of variance heterogeneity in a residual plot. From a regression fit (of any
sort: SLR, MLR, loess, etc.), find the absolute residuals |ei|, i = 1, … , n. To these,
apply a loess smooth against the fitted values ̂Yi. If the loess curve for the |ei|s exhibits
departure from a horizontal line, variance heterogeneity is indicated/validated. If the
smooth appears relatively flat, however, the loess diagnostic suggests that variation is
not heterogeneous. To illustrate, apply this strategy to the following data sets. In each
case, indicate whether or not the loess smooth substantiates the earlier indication of
heterogeneous variances.
(a) The baseball batting average data in Example 6.4.1.
(b) The private college salary data in Exercise 6.11.
(c) The diamond data in Exercise 7.12.
7.17
Explore the loess fit for the supernovae data in Example 7.4.1 as follows:
(a) Calculate the final raw residuals ̌𝜖= Yi −̌Yi and plot them against the fitted val-
ues ̌Yi. Does any untoward pattern appear? Do some points appear to be possible
outliers? If so, identify the supernovae and research them for knowledge discovery
purposes to see if they have any features in common.
(b) Plot the absolute residuals | ̌𝜖i| from Exercise 7.17a against ̌Yi and overlay a loess
smooth of the | ̌𝜖|s as in Exercise 7.16. Use a smoothing parameter of q = 0.75.
How does the loess smooth inform your residual diagnostic(s)?

254
STATISTICAL DATA ANALYTICS
(c) Recalculate the local quadratic loess fit in the example by varying the smoothing
parameter over q = 0.2, 0.4, 0.6, 0.8. Indicate whether or not these changes affect
the smoothed predicted values in a substantive manner.
(d) Recalculate the local quadratic loess fit in the example by switching to
single-iteration (in R, use the family=‘gaussian’ in loess()) for the
local quadratic smoother, retaining q = 1
2. Does this affect the final loess curve in
a substantive manner?
(e) In Exercise 7.17d, apply instead a local linear smoother (with the robust extension)
and vary the smoothing parameter over q = 0.25, 0.50, 0.75. Indicate whether or
not these changes affect the final loess curve in a substantive manner.
(f) Fit a quadratic polynomial without loess smoothing, as in Section 7.2. Overlay the
predicted quadratic curve on the original scatterplot in Figure 7.5, and comment on
the visual quality of the fit.
7.18
Gammon (2009) reported a study on body mass index (BMI) of US photographer’s
models from the 1950s to the late 2000s. (BMI is a standardized measure that combines
a person’s weight in inches and height in pounds: BMI = 703 × weight/height2.) While
most Western populations have seen increases in BMI over that time span, these models
show a different pattern. The data comprise n = 609 data pairs and are available online
at http://www.wiley.com/go/piegorsch/data_analytics; a sample is given as follows:
Date:
Dec. 1953
Mar. 1954
Nov. 1954
· · ·
Dec. 2008
Jan. 2009
BMI:
19.6341
19.0436
20.4825
· · ·
17.4838
18.9492
(a) Plot Y = BMI against x = time. Does the pattern deviate from the rise seen in most
Western populations?
(b) Calculate a robust, linear, loess fit with soothing parameter set to q = 0.5. Overlay
the loess fit on the scatterplot. Does this improve visualization of the pattern?
(c) Calculate the final raw residuals ̌𝜖= Yi −̌Yi, and plot them against the fitted values
̌Yi. Does any pattern appear? Do some points appear to be possible outliers?
(d) Apply instead a robust quadratic smoother, again with smoothing parameter
q = 0.5. Overlay the loess fit on the scatterplot. Is there much change from the
local linear fit?
7.19
Cleveland et al. (1992, Section 8.2.4) described an astrometric data set with n = 323
observations on the radial velocity of the NGC7531 spiral galaxy, taken from different
east/west and north/south positions. The data are available from a variety of R sources,
for example, as the galaxy data frame in the external ElemStatLearn package associ-
ated with Hastie et al. (2009). Of interest is studying how Y = Velocity is affected by
the two predictor variables x1 = East/West position and x2 = North/South position, via
a loess fit. Download these data and mimic Cleveland et al.’s analysis as follows.
(a) Plot the north/south versus east/west positions to visualize the measurement
locations.

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
255
(b) Calculate a robust, quadratic loess fit for Y against the two x-variables. Include a
cross-product term x1x2 in the model formula as in (7.29) to account for a possible
interaction. (As the two predictors are of similar magnitude, there is no need to
normalize them for this analysis.) Set the smoothing parameter to q = 0.35.
(c) Plot the contours of the resulting loess surface against x1 and x2. Comment on the
features that appear.
(d) Calculate the final raw residuals ̌𝜖i = Yi −̌Yi and plot them against the fitted values
̌Yi. Does any pattern appear? Do some points appear to be possible outliers? Also
construct a normal probability plot of the ̌𝜖is. Does the plot suggest that a simpler,
one-step, ‘Gaussian’ loess fit would be appropriate?
(e) Plot the absolute residuals | ̌𝜖i| from Exercise 7.19d against ̌Yi and overlay a loess
smooth of the | ̌𝜖|s as in Exercise 7.16. Use a smoothing parameter of q = 0.75.
How does the loess smooth inform your residual diagnostic(s)?
7.20
Explore the loess fit for the MPG data in Example 7.4.2, as follows.
(a) Calculate the final raw residuals ̌𝜖= Yi −̌Yi, and plot them against the fitted values
̌Yi. Does any pattern appear? Do some points appear to be possible outliers?
(b) Plot the absolute residuals | ̌𝜖i| from Exercise 7.20a against ̌Yi and overlay a loess
smooth of the | ̌𝜖|s as in Exercise 7.16. Use a smoothing parameter of q = 0.75.
How does the loess smooth inform your residual diagnostic(s)?
(c) Recalculate the local quadratic loess fit in the example by varying the smoothing
parameter over q = 0.4, 0.5, 0.6. Indicate whether or not these changes affect the
smoothed predicted values in a substantive fashion.
7.21
Under the MLR model in (7.1) and (7.2), show that by centering the response variable
and all the predictor variables about their means, the LS estimate for the intercept 𝛽0
will be zero.
7.22
Under the MLR model in (7.1) and (7.2), let the ridge regression shrinkage estimator
for 𝜷be given by (7.32). Find E[ ̂𝜷𝜅] and Var[ ̂𝜷𝜅]. (Hint: Recall the matrix relationships
given in Section 7.1.1.)
7.23
Return to the college admissions data studied in Exercise 7.3. Using the three predictors
x1 = {ACT}, x2 = {Class rank}, and the interaction x1x2, assess if multicollinearity is a
concern with the MLR fit. If so, apply a ridge regression analysis. (Remember to center
the response variable and standardize the three predictors for the ridge analysis.) Find
the GCV error and minimize it to determine the tuning parameter 𝜅and use this to
calculate ridge-regression predicted values. Report the final values on their original
scale.
7.24
Return to the genetic SNP data in Example 7.4.3 and further examine the ridge regres-
sion fit, as follows.
(a) Verify the shrinkage effect by extending the ridge trace plot over much a larger
range of 𝜅, say, 0 ≤𝜅≤105 or greater. Does the expected ‘shrinkage’ toward zero
appear?

256
STATISTICAL DATA ANALYTICS
(b) Plot the predicted values ̂Yi(𝜅GCV) against the other predictors. Do any interesting
(or uninteresting) associations appear?
(c) Replace 𝜅GCV with 𝜅HKB and recalculate the ridge regression coefficients. Is there
a substantial change?
(d) Replace the trace plot in Figure 7.8 with the alternative trace of each ̂𝛽j plotted as
a function of the effective d.f. from (7.35). Notice how the plot now begins with
the traces tightly grouped and then shows greater variation and instability as the
d.f. increase. At what effective d.f. do the traces appear to stabilize? How does this
relate to the values for 𝜅GCV and 𝜅HKB chosen in the example? (Hint: Operate over
a very wide range of 𝜅, say, 0 ≤𝜅≤106 or greater, in order to capture the effects
as the d.f. →0. You may find the ridge() function from the external genridge
package convenient for calculation of the d.f.)
7.25
Return to the compressive strength data in Exercise 7.2 and apply the Lasso from
Section 7.4.3 for purposes of variable selection. Use all eight original predictor vari-
ables and Y =
√
Strength as the response. (Remember to center Y and standardize the
eight predictor variables.) Compare the variables retained by the Lasso at its minimum
CV 𝜅against the models selected in Exercises 7.2b and 7.14. Comment on whether
and how any differences appear between the Lasso regularization and those previous
analyses.
7.26
Return to the genetic SNP data in Example 7.4.4 and, in particular, the Lasso regular-
ization calculation from that example. Reperform the K-fold CV for a variety of other
values of K, say, K = 5, 10, 25, 50. Do the results change substantively from those seen
in the example? In particular, at each minimum-CV value for 𝜅, how many and which
predictors are retained in the model?
7.27
Kutner et al. (2005, Appendix C) reported data on health care claims (in total
$ over 24 mos.) generated by an insurance company’s female subscribers with
ischemic heart disease, as related to a series of seven predictor variables. A selec-
tion of the n = 608 observations follows. (Download the complete data set at
http://www.wiley.com/go/piegorsch/data_analytics.)
Subscriber index, i
Outcome variable
i = 1
i = 2
· · ·
i = 607
i = 608
Y = Claims ($)
179.10
319.00
· · ·
1282.20
586.00
x1 = Age (years)
63
59
· · ·
58
56
x2 = Procedures (no.)
2
2
· · ·
7
4
x3 = Prescribed drugs (no.)
1
0
· · ·
2
4
x4 = Emerg. Room visits (no.)
4
6
· · ·
2
6
x5 = Complications (no.)
0
0
· · ·
0
0
x6 = Comorbidities (no.)
3
0
· · ·
7
3
x7 = Duration (days)
300
120
· · ·
244
336
Claim costs are notoriously skewed, so begin by transforming the response variable
to Y = log{Claims} and calculate the centered response U = Y −Y. Also center the

TECHNIQUES FOR SUPERVISED LEARNING: MULTIPLE LINEAR REGRESSION
257
seven predictor variables and scale them to have unit variance. Apply the Lasso from
Section 7.4.3 for purposes of variable selection with these data by proceeding as fol-
lows.
(a) Begin with an assessment of multicollinearity among the (standardized) predictors.
Does the maximum VIF exceed 10?
(b) Fit the seven predictors to the centered response via the Lasso. Construct an esti-
mated coefficient profile plot. Comment on the features in the plot. Does the typical
Lasso-shrinkage pattern appear?
(c) Apply K-fold CV at K = 10 to select a single value for 𝜅. What value is selected?
Which predictors are retained in the model and what are their estimated ̂𝛽j coeffi-
cients?
7.28
Blæsild and Granfeldt (2002, Exercise 4.6) presented data on egg development of
sand lizards (Lacerta agilis). Across a series of a = 20 different incubation treatment
regimes, incubation times of lizard’s eggs were recorded in days for n+ = 119
hatchlings. The data are available online at http://www.wiley.com/go/piegorsch/data
_analytics; a sample is given follows:
Treatment code:
1
2
· · ·
20
Incubation times:
92.9, 93.9, 83.9
59.2, 58.0, … , 55.9
· · ·
26.0, 26.4, … , 26.7
Temporal data often exhibit skewness and/or variance heterogeneity, so operate with
Y = log{Incubation time}. Apply a one-factor ANOVA model to these data and assess
whether any difference in log-incubation was seen across the different treatments.
Include pertinent diagnostic assessments. What do you conclude at 𝛼= 0.01?
7.29
Illustrate the claim in Section 7.5 that when a single factor has only a = 2 levels, the
ANOVA F-test and the pooled, two-sample t-test are equivalent, as follows. Return to
the lung function data in Exercise 5.27c and fit a one-factor ANOVA model. Test for
equality between the two means against a two-sided alternative. As there, operate at
𝛼= 0.10. Show that the two test statistics equate via t2 = F and that the corresponding
P-values are identical.
7.30
Return to the per capita income data from Example 7.5.1 and investigate whether a
different transformation may further stabilize the fit. Do so via the Box–Cox power
transformation from (3.13). To implement a Box–Cox search in R, apply the boxcox()
function to the lm object containing the full two-factor fit to the data. The function
will search for a value of the power parameter, 𝜆, that maximizes the log-likelihood
along the 𝜆direction. Here, restrict 𝜆to the range −2 ≤𝜆≤2. Transform Y according
the recommended value of 𝜆(to the nearest reasonable number; e.g., if the function
recommends 𝜆= −0.44, set 𝜆= −1
2). Repeat the two-factor ANOVA in the example
using this newly transformed response variable.

8
Supervised learning: generalized
linear models
A core assumption underlying the regression analyses in Chapters 6 and 7 was that the
independent observations Yi were continuous with constant variances. Normality via
Yi ∼N(𝜇i, 𝜎2) was also often included, possibly after a stabilizing transformation (as in
Section 3.4.3). While applicable to a wide variety of settings, these assumptions need not
hold for every regression data set. Observations may have variances Var[Yi] that change in
relation to the mean response, 𝜇i = E[Yi]. They may be restricted to only positive outcomes,
be bounded over a fixed interval, or be discrete such as counts and proportions. In such cases,
the normal linear regression model will not hold. It can be generalized, however, to include
nonnormal parent distributions for the data, allow for heterogeneous variances among the Yis,
and incorporate nonlinear relationships between the mean response and the linear predictor.
This chapter presents a class of generalized linear models (GLiMs) that achieves these goals.
8.1
Extending the linear regression model
8.1.1
Nonnormal data and the exponential family
As in Chapters 6 and 7, assume the data Yi (i = 1, … , n) are recorded along with a series of
concomitant predictor variables xij (j = 1, … , p). GLiMs continue to view the mean response
𝜇i = E[Yi] as a function of these p predictors but accept that the distribution of Yi may differ
from normal. To do so, they appeal to the larger exponential family of distributions from
Section 2.3.11. Recall that the exponential family is a rich collection that can accommodate
both discrete and continuous probability functions, fY(y), via the general expression
fY(y) = exp
{y𝜃−b(𝜃)
a(𝜑)
+ c(y, 𝜑)
}
,
(8.1)
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
259
where a(𝜑), b(𝜃), and c(y, 𝜑) are functions of known form. The unknown parameter 𝜃is called
the natural parameter of the distribution and is related to the population mean via E[Y] = 𝜇
= b ′(𝜃). The dispersion or scale parameter 𝜑> 0 relates to the variance via the relationship
Var[Y] = a(𝜑) 𝜕2b(𝜃)
𝜕𝜃2 .
When the dispersion parameter 𝜑is known, the quantity 𝜕2b(𝜃)∕𝜕𝜃2 is called the variance
function of Y, because it incorporates all the unknown aspects of the variance term. Because
𝜕2b(𝜃)∕𝜕𝜃2 = 𝜕𝜇∕𝜕𝜃is usually a function of 𝜇, the variance function is denoted by V(𝜇).
Example 2.3.9 showed that the normal distribution probability density function (p.d.f.)
does satisfy (8.1) and, therefore, is a member of the larger exponential family. Other
well-known examples include the binomial (Example 2.3.10) and Poisson (Exercise 2.26)
models. As discussed in Section 2.3.11, however, a notable exception is the uniform
distribution. As seen there, the uniform distribution, or for that matter any distribution whose
support depends on unknown parameters, cannot be included in the exponential family.
8.1.2
Link functions
A second extension to the classical linear model made in a GLiM expands the link between
the mean of the ith response, 𝜇i, and the linear predictor. For notational convenience, write
the latter as
𝜂i = 𝛽0 + 𝛽1xi1 + · · · + 𝛽pxip.
(8.2)
Under a classical simple linear regression (SLR) or multiple linear regression (MLR) model,
the mean and linear predictor are identically related: as modeled in (7.2), 𝜇i = 𝜂i. GLiMs
extend this into a functional relationship, say, g(𝜇i) = 𝜂i. The function g(⋅) is assumed mono-
tone and is called the link function or link between 𝜇and 𝜂. The link function defines the scale
over which the systematic effects represented by 𝜂are modeled as additive.
In some cases, the link is trivial; for example, 𝜇i = 𝜂i represents an identity link,
g(𝜇i) = 𝜇i. In other cases, the link can be used to model a necessary relationship for
𝜇= E[Y]. For instance, in Exercise 2.26, the Poisson mean is seen to be positive, so the link
should relate the strictly positive quantity 𝜇i > 0 to a linear predictor of any sign. A common
choice then is the natural logarithm, g(𝜇i) = log(𝜇i).
As the link function is assumed monotone, it has an inverse link function, g−1(𝜂i), which
characterizes the mean as a function of the linear predictor. For simplicity, one often sees the
notation h(𝜂i) = g−1(𝜂i), so that 𝜇i = h(𝜂i). For example, with the Poisson distribution, the
inverse link to g(𝜇i) = log(𝜇i) = 𝜂i is 𝜇i = h(𝜂i) = e𝜂i.
The exponential family based on (8.1) and the link function g(⋅) relating 𝜇to 𝜂are the
two key generalizations that make up a GLiM. Within this context, a number of technical,
theoretical developments that allow for regression modeling from any member of the parent
class in (8.1) are possible. These are discussed in the next section.
8.2
Technical details for GLiMs∗
From a broad perspective, estimation and inference for GLiMs are similar to that for the SLR
and MLR models in Chapters 6 and 7, respectively. The extended nature of the model class

260
STATISTICAL DATA ANALYTICS
does require some technical manipulation, however. In what follows, advanced familiarity
with differential calculus and matrix algebra will be necessary. Introductory readers may wish
to skip ahead to Section 8.3 and study applications of the models before taking up the details
that follow here.
8.2.1
Estimation
By relaxing the homogeneous variance assumption on Yi, GLiMs suppose the observa-
tions contribute heterogeneous information on the regression parameters in (8.2). As in
Section 5.2.3, this then calls for estimation of 𝜷= [𝛽0 𝛽1 · · · 𝛽p]T via weighted least
squares. The weights will typically depend on 𝜇i and thus, through the link, on 𝜷. Iteration,
therefore, becomes necessary, and the estimation proceeds by iterating through the weighted
least squares equations. This iteratively (re)weighted least squares (IWLS) approach can
be shown to correspond to a maximum likelihood (ML) solution (Nelder and Wedderburn,
1972). Thus the estimation process may be viewed as maximizing the log-likelihood
ℓ(𝜷) =
n
∑
i=1
exp
{
Yi𝜃i(𝜷) −b (𝜃i(𝜷))
a(𝜑)
+ c(Yi, 𝜑)
}
,
(8.3)
where the notation 𝜃i(𝜷) emphasizes that the natural parameters are modeled as functions of
the regression parameters in 𝜷. Note that this assumes, for the moment, that the dispersion
parameter 𝜑is known.
The IWLS/ML algorithm is not trivial (Myers et al. 2012, Section A.6) and has no closed-
form solution; however, it can be shown to produce a set of iterative ML estimating equations
for the 𝛽js (Piegorsch and Bailer 2005, Section 3.2.1):
n
∑
i=1
xijh′(𝜂i)
a(𝜑)V(𝜇i)(Yi −𝜇i),
for j = 0, … , p,
with 𝜇i = h(𝜂i) and 𝜂i dependent on 𝜷through (8.2). From these, the iterative solution is easily
programmed. For instance, R’s glm() function can perform the IWLS/ML fit for most GLiMs
seen in practice. This is illustrated in the examples throughout Section 8.3.
The IWLS/ML estimator ̂𝜷exhibits typical ML optimality properties, as discussed in
Section 5.2.4. Thus ̂𝜷possesses an approximate (p + 1)-variate normal distribution with mean
vector 𝜷and covariance matrix found by inverting the Fisher information matrix, F(𝜷), from
(5.5). For shorthand notation, write Var[ ̂𝜷] ≈F−1(𝜷). (The approximation improves as n →
∞.) The individual variances Var[ ̂𝛽j] are taken from the diagonal elements of Var[ ̂𝜷]. These
often depend on 𝜷and so are estimated by replacing 𝛽j with its estimator ̂𝛽j wherever it appears.
Denote the estimated variances as ̂𝜎2
j , with corresponding standard errors se[ ̂𝛽j] = ̂𝜎j. The
estimated off-diagonal covariances ̂𝜎jk (j ≠k) are calculated similarly by replacing 𝛽j with ̂𝛽j.
Note that while E[ ̂𝜷] ≈𝜷, in small samples, the approximation’s quality can vary. Thus
for small n, analysts may wish to apply a bias correction. This will depend on the nature of
the linear predictor, the link function, and the underlying parent distribution (Cordeiro and
McCullagh 1991; Firth 1993).
If the dispersion parameter 𝜑is unknown, it may also be estimated. One can apply ML
or the method of moments (Section 5.2.2). In the latter case, one finds a quantity whose first

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
261
moment is a function of 𝜑and manipulates the relationship to produce the estimator ̂𝜑(see
the following text).
8.2.2
The deviance function
A fundamental quantity useful for testing hypotheses, assessing model adequacy, and estimat-
ing dispersion in a GLiM is known as the deviance function. It is constructed to measure the
discrepancy of a posited model when fit to a data vector Y = [Y1 Y2 · · · Yn]T. The deviance
uses the log-likelihood ℓ(𝜷) = log{L(𝜷; Y)} from (8.3) to quantify discrepancy in that model’s
fit. This is defined as the deviation in ℓ(⋅) from the fullest possible model that can be fit to the
data. The latter quantity is found by fitting a separate parameter to each observation, giving a
log-likelihood of the form log{L(Y; Y)}.
The notation here is avowedly awkward. Technically, one estimates each value of 𝜇i by
its corresponding Yi wherever 𝜇i appears in the log-likelihood. But because 𝜇i is related to
𝜃i(𝜷) via 𝜇i = b ′(𝜃i) and because 𝜃i = 𝜃i(𝜷) is a function of 𝜷, one can in the extreme case
view 𝜷as being comprised of n ‘parameters,’ each corresponding to a value of Yi. The term
for such an effect is saturation. The model and, hence, the log-likelihood has been saturated
by n parameters, and we write ̂𝜃i(Y) to indicate that 𝜃i(𝜷) is estimated under this saturation.
The corresponding value of ℓ(𝜷) is denoted by ℓ(Y) = log{L(Y; Y)}.
Compared with the saturated model, a reduced model (RM) allocating only p + 1 < n
parameters to 𝜷will have a smaller maximized log-likelihood. Evaluated at the maximum
likelihood estimate (MLE) of 𝜷, this is ℓ( ̂𝜷) = log{L( ̂𝜷; Y)}. The quality of the model fit can
be quantified by twice the difference of the two values: 2{ℓ(Y) −ℓ( ̂𝜷)}. (Multiplying by 2 has
some useful consequences; see the following text.) The closer this difference is to zero, the
more the RM using only p + 1 parameters mimics that of a saturated model.
Applied to the exponential family in (8.1), suppose a(𝜑) is known. Then,
ℓ(Y) =
n
∑
i=1
{
Yi ̂𝜃i(Y) −b ( ̂𝜃i(Y))
a(𝜑)
+ c(Yi, 𝜑)
}
,
while
ℓ( ̂𝜷) =
n
∑
i=1
{
Yi𝜃i( ̂𝜷) −b (𝜃i( ̂𝜷))
a(𝜑)
+ c(Yi, 𝜑)
}
.
Twice their difference is 2{ℓ(Y) −ℓ( ̂𝜷)}. Write this as
D∗( ̂𝜷) =
2 ∑n
i=1
[Yi
{ ̂𝜃i(Y) −𝜃i( ̂𝜷)} −{b( ̂𝜃i(Y)) −b(𝜃i( ̂𝜷))}]
a(𝜑)
.
(8.4)
The numerator of (8.4) is traditionally denoted as D( ̂𝜷) and called the deviance function. The
entire quotient, D∗( ̂𝜷) = D( ̂𝜷)∕a(𝜑), is called the scaled deviance.
One special case of D( ̂𝜷) is notable. When Yi ∼N(𝜇i, 𝜎2), the deviance simplifies to the
residual sum of squares: D( ̂𝜷) = ∑n
i=1 (Yi −̂𝜇i)2, where ̂𝜇i = b′[𝜃i( ̂𝜷)] is the estimated mean
response at the ith observation (Myers et al. 2012, Section 5.7).
The deviance is useful for testing discrepancies between models, as in (7.18). That
is, suppose the ‘full’ model (FM) is represented by the (p + 1)-vector 𝜷= [𝛽0 · · · 𝛽q−1
𝛽q · · · 𝛽p]T and a putative ‘reduced’ model (RM) contains only the first q + 1 < p + 1

262
STATISTICAL DATA ANALYTICS
elements of 𝜷. In terms of a hypothesis test, the null hypothesis corresponding to RM
constrains p −q of the 𝛽js to zero. The alternative hypothesis corresponds to FM, where no
constraints are imposed on the 𝛽js. Denote the reduced vector as 𝜷RM = [𝛽0 · · · 𝛽q 0 · · · 0]T.
For consistency, also write the FM vector as 𝜷FM. Fit both models to the data and calculate their
scaled deviances, D∗( ̂𝜷RM) and D∗( ̂𝜷FM), respectively. The difference between these is simply
D∗( ̂𝜷RM) −D∗( ̂𝜷FM) = 2 {ℓ( ̂𝜷FM) −ℓ( ̂𝜷RM)} .
(8.5)
Notice that this is twice the log of the ratio of the likelihoods. Recall from Section 5.4.3,
however, that a ratio of likelihoods between a reduced (or null) model and the FM forms the
likelihood ratio (LR) test of the RM. That is, for known 𝜑, to test if the p −q parameters set
to zero under RM are significant, the LR statistic is
G2 = −2 log
{
L( ̂𝜷RM; Y)
L( ̂𝜷FM; Y)
}
= −2 [log {L( ̂𝜷RM); Y)} −log {L( ̂𝜷FM); Y)}]
= −2 {ℓ( ̂𝜷RM) −ℓ( ̂𝜷FM)} ,
which is simply (8.5). Thus the difference in scaled deviances is an LR statistic for assessing
the RM. Recall that under appropriate regularity conditions, G2 ̇∼𝜒2(p −q), where the
symbol ̇∼is read ‘is approximately distributed as.’ The 𝜒2 approximation here improves as
n →∞. This result is useful for testing the significance of RMs in a nested GLiM hierarchy
(see Section 8.2.4).
A 𝜒2 approximation also holds for the scaled deviances themselves: D∗( ̂𝜷) ̇∼
𝜒2(n −p −1). Unfortunately, in small samples, this approximation can be poor near the
tails of the distribution and it is not recommended for unregulated use (see McCullagh and
Nelder 1989, Section 4.4.3). One case where it is useful, however, is when the dispersion
parameter 𝜑is unknown. Then, equating the expected value of D∗( ̂𝜷) under the 𝜒2(n −p −1)
approximation produces the estimating equation
E[D∗( ̂𝜷)] = E[D( ̂𝜷)∕a(𝜑)] ≈n −p −1.
Solving for 𝜑yields, in effect, a method of moments estimator for 𝜑. In the special case
where a(𝜑) = 𝜑, this is just
̂𝜑D =
D( ̂𝜷)
n −p −1,
a deviance-based moment estimator for 𝜑.
8.2.3
Residuals
Assessing model adequacy is as important for GLiMs as it is for classical SLRs and MLRs.
Residual analysis is a natural component of this process. With GLiMs, however, a variety of
possible formulations exists for how to define a ‘residual.’ Given predicted values ̂Yi, i = 1,
… , n, the raw residual is the usual quantity ei = Yi −̂Yi. Plotting the eis against ̂Yi can provide
useful a diagnostic, as seen in Section 6.3. For many distributional models in the exponential
family, however, the variance of Yi varies with the mean 𝜇i. When this occurs, plots of the raw
residuals can create spurious patterns and/or mask truly important relationships in the data.

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
263
To adjust for differential variation, it is common to scale the eis by the (estimated) standard
deviation of Yi:
Yi −̂Yi
√
Var[Yi]
=
Yi −̂Yi
√
a(𝜑)V( ̂𝜇i)
,
where V( ̂𝜇i) is the variance function evaluated at the predicted value for 𝜇i.
If, as is common, a(𝜑) = 𝜑∕𝑤i for some known weights 𝑤i > 0, this scaled residual
becomes 𝜑−1∕2(Yi −̂Yi)∕
√
V( ̂𝜇i)∕𝑤i. For use as a diagnostic, we can ignore the constant
dispersion parameter 𝜑, producing
ci =
Yi −̂Yi
√
V( ̂𝜇i)∕𝑤i
.
These quantities are known as Pearson residuals, honoring the statistician Karl Pearson who
proposed squaring and summing the cis for use in analyzing tabular count data (Pearson 1900).
The resulting summary measure of residual variation is the Pearson 𝜒2 statistic:
X2 =
n
∑
i=1
c2
i =
n
∑
i=1
(Yi −̂Yi)2
V( ̂𝜇i)∕𝑤i
.
Scaling X2 by 𝜑leads to X2∕𝜑̇∼𝜒2(n −p −1). Thus E[X2∕𝜑] ≈n −p −1. When 𝜑
is unknown, simple appeal to the method of moments, therefore produces ̂𝜑P = X2∕
(n −p −1), a 𝜒2-based moment estimator for 𝜑. This is preferred over ̂𝜑D when V(𝜇) is not
constant with respect to 𝜇, because the latter can be unstable in such cases.
An adjusted residual similar to ci is based on the deviance function D( ̂𝜷). From (8.4),
the ith contribution to the deviance is 2[Yi{ ̂𝜃i(Y) −𝜃i( ̂𝜷)} −{b( ̂𝜃i(Y)) −b(𝜃i( ̂𝜷))}]. Then, the
deviance residual is
di = sgn(ei)
√
2 [Yi
{ ̂𝜃i(Y) −𝜃i( ̂𝜷)} −{b ( ̂𝜃i(Y)) −b (𝜃i( ̂𝜷))}],
(8.6)
where sgn(e) = −I(−∞,0)(e) + I(0,∞)(e) is the signum function that reports the sign of its argu-
ment. Deviance residuals tend to be somewhat more stable than Pearson residuals (McCullagh
and Nelder 1989, Section 2.4), although they do possesses a nonzero bias in small samples.
Jørgensen (2012) suggests some modifications to di that can help to correct for this.
GLiM residuals can be standardized, similar to the strategies discussed in Section 6.3. As
there, one begins with the design matrix X, whose columns are the individual predictor vari-
ables. Unless otherwise specified, an additional column of ones for the ‘intercept’ is included
as in (7.3). Next, define the matrix Q = diag{q11, … , qnn} with elements
qii =
√
𝑤i∕V( ̂𝜇i)
|g′( ̂𝜇i)|
,
where g′( ̂𝜇i) is 𝜕g(𝜇)∕𝜕𝜇evaluated at ̂𝜇i. These are used in constructing the hat matrix for the
GLiM as
H = QTX(XTQQTX)−1XTQ
To achieve a standardized GLiM residual whose variance is approximately constant, divide
di or ci by
√
1 −hii, where hii is the ith the diagonal element of H.

264
STATISTICAL DATA ANALYTICS
8.2.4
Inference and model assessment
Inferences in a GLiM generally enlist the large-sample features of the ML estimators
(Section 5.2.4). For example, if se[ ̂𝛽j] is the standard error of ̂𝛽j (from Section 8.2.1),
then the Wald ratio Wj = ( ̂𝛽j −𝛽j)∕se[ ̂𝛽j] will be approximately standard normal:
Wj ̇∼N(0, 1), j = 0, … , p. From this, large-sample confidence intervals and hypoth-
esis tests may be constructed. A (pointwise) 1 −𝛼Wald confidence interval for 𝛽j is
̂𝛽j ± z𝛼∕2se[ ̂𝛽j], where z𝛼∕2 is the upper- 𝛼
2 standard normal critical point.
Similarly, a Wald test of Ho: 𝛽j = 0 versus Ha: 𝛽j ≠0 employs the statistic
Wj =
̂𝛽j
se[ ̂𝛽j]
and rejects Ho when |Wj| ≥z𝛼∕2 or, equivalently, when W2
j ≥𝜒2
𝛼(1). The approximate
P-value is P ≈1 −2Φ(|Wj|), where Φ(⋅) is the standard normal cumulative distribution
function (c.d.f.) from (2.35). Notice that this is equivalent to P ≈P[𝜒2(1) ≥W2
j ]. For a
one-sided test against, say, Ha: 𝛽j > 0 reject Ho when Wj ≥z𝛼. The approximate P-value
becomes 1 −Φ(Wj).
Stability of Wald tests/intervals varies greatly among members of the exponential family.
For example, Wald tests for many Poisson-based GLiMs are usually quite stable, while those
for some binomial-based GLiMs can be so unstable as to be expressly contraindicated (see
Section 8.3.2).
Hypotheses involving multiple predictors can be assessed via matrix-based extensions of
the Wald test (Cox 1988) or by LR methods. The latter strategy is especially attractive with
GLiMs, because it corresponds to the use of the deviance function: recall that the difference in
scaled deviances between a FM and an RM nested within FM corresponds to an LR statistic.
This concept carries forward into a nested hierarchy of, say, r ≤p + 1 models from FM down
to the simplest RM, where a series of LR statistics can be constructed for each of the scaled
deviance differences. (The degrees of freedom (d.f.) for the corresponding test will equal the
number of parameters that are constrained to reduce FM to RM. Generally, this is simply the
difference in d.f. between the two scaled deviances.) Arranged in tabular form, this produces
an analysis of deviance table, as in Table 8.1. In the table, RMk refers to the kth (sub)model
being fit, while ‘Resid. d.f.’ (residual d.f.) is n minus the number of nonzero parameters fit for
that (sub)model and all previous RMs.
In an analysis of deviance, one sequentially assesses the significance of each nested RM by
referring the corresponding LR statistic G2 from Table 8.1 to an appropriate 𝜒2 distribution.
Table 8.1
Schematic for an analysis of deviance table.
Source
Resid. d.f.
D∗(⋅)
Δd.f.
G2
RM1
df1
D∗(RM1)
–
–
RM2
df2
D∗(RM2)
df1 −df2
D∗(RM1) −D∗(RM2)
⋮
⋮
⋮
⋮
⋮
RMr−1
dfr−1
D∗(RMr−1)
dfr−2 −dfr−1
D∗(RMr−2) −D∗(RMr−1)
FM
dfFM
D∗(FM)
dfr−1 −dfFM
D∗(RMr−1) −D∗(FM)

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
265
Reject the null hypothesis that the additional components in a larger nested model are unim-
portant at false positive rate 𝛼if G2 ≥𝜒2
𝛼(Δd.f.). The corresponding approximate P-value is
P[𝜒2(Δd.f.) ≥G2].
As with classical linear modeling, the assessments in an analysis of deviance are made
sequentially. Inferences on any nested RM may depend on the terms that precede it in the
model and on the sequential order in which they are fit.
If 𝜑is unknown in an analysis of deviance, adjustments are required. For example, suppose
a(𝜑) = 𝜑and employ a moment-based estimator such as ̂𝜑P. Then, the difference in estimated
scaled deviances between two nested models, RM2 and RM1 (with corresponding residual d.f.
df2 < df1), is calculated as {D(RM1) −D(RM2)}∕̂𝜑P. Unfortunately, using an estimate of 𝜑
degrades the 𝜒2 approximation for G2. To adjust for this, divide G2 by the corresponding
Δd.f. = df1 −df2. This results in an F-statistic:
F = D(RM1) −D(RM2)
(df1 −df2) ̂𝜑P
.
Under certain regularity conditions on ̂𝜑P, F ̇∼F(df1 −df2, df2), and we reject the null
hypothesis that the additional components in RM2 are insignificant at false positive rate 𝛼
when the calculated F-statistic Fcalc exceeds the upper-𝛼critical point F𝛼(df1 −df2, df2). The
corresponding approximate P-value is P[F(df1 −df2, df2) ≥Fcalc].
An important question here is, under which model – RM1, RM2, FM, etc. – should ̂𝜑be
determined? (As regression variables are added to or deleted from the model, ̂𝜑will usually
change.) Many strategies are possible, each varying in quality based on the underlying parent
distribution, linear predictor, link function, etc., and it is difficult to make an omnibus rec-
ommendation. The default used in R’s glm() function is, however, a useful standard: when
employing an estimator of 𝜑, R fits the maximal model (FM) to determine ̂𝜑and uses this
same value for all its sequential P-values.
Note that these differences in deviances can also be used to construct approximate (point-
wise) 1 −𝛼confidence intervals on any 𝛽j by inverting the corresponding LR statistic from
(5.35). These are called profile likelihood confidence intervals. The intervals are not usually
available as closed-form expressions, but they can be acquired from a GLiM computer output.
For instance, the R function confint(obj.glm) can compute profile likelihood intervals
from the GLiM object obj.glm constructed via the glm() function. (Be sure to load the
MASS package before applying confint() to a glm object.)
8.3
Selected forms of GLiMs
This section illustrates GLiMs for a few of the more popular members from the exponential
family. In some cases, allied methods of analysis are included, depending on the nature of the
parent distribution and the link function of interest. For a wider introduction to different forms
of GLiMs, see, for example, Faraway (2006, Chapter 7), Myers et al. (2012, Chapter 5), or
the classic text by McCullagh and Nelder (1989).
8.3.1
Logistic regression and binary-data GLiMs
When discrete data are observed as binary observations (generically: ‘success’ vs ‘failure’)
leading to proportion responses, it is natural to consider use of the binomial probability mass

266
STATISTICAL DATA ANALYTICS
function (p.m.f.) from Section 2.3.1 as the parent distribution. Take Y as the number of suc-
cesses recorded from M independent, binary ‘trials’ for the outcome. Then Y ∼Bin(M, 𝜋),
where 𝜋= P[success]. (The proportion response is Y∕M.)
As seen in Example 2.3.10, this is a member of the exponential family in (8.1), and hence,
GLiMs may be applied. The natural parameter for the binomial is the log-odds or ‘logit’ of
success: 𝜃= logit(𝜋) = log{𝜋∕(1 −𝜋)}. Also, the dispersion parameter is a constant, 𝜑= 1,
along with the function a(𝜑) = 1. Thus there is no unknown dispersion parameter to estimate
with a binary-data GLiM.
While the p.m.f. can be written as a function of the binomial mean E[Y] = 𝜇= M𝜋, it is
more common in practice to model 𝜋, rather than 𝜇, directly. Thus a minor adjustment with
the GLiM notation for binary data is to use 𝜋instead of 𝜇as the argument of the link function,
that is, write and model g(𝜋) rather than g(𝜇).
The link function most often employed with binomial data is the logit link: g(𝜋) = logit(𝜋)
= log{𝜋∕(1 −𝜋)}, connecting with the natural parameter 𝜃. The corresponding inverse link
is h(𝜂) = 1∕(1 + e−𝜂), which transforms a linear predictor over −∞< 𝜂< ∞to a probability
0 < 𝜋< 1. This feature is particular to binary data: because 𝜋is a probability, every inverse
link should return a value between 0 and 1. And since g(𝜋) is monotone, so is its inverse h(𝜂).
Notice then that if h(𝜂) is nondecreasing in 𝜂, it exhibits the properties of a c.d.f.: it is between
0 and 1, and it is a monotone, nondecreasing function. For example, with the logit link, the
inverse is the c.d.f. from the standard logistic distribution (Johnson et al. 1995, Chapter 23).
The corresponding GLiM is often called a logistic regression.
Connecting the inverse link to c.d.f.s opens a broad variety of possibilities for modeling
h(𝜂) and, hence, g(𝜋). For example, if we appeal to the standard normal c.d.f. in (2.35), we
find h(𝜂) = Φ(𝜂) and thus g(𝜋) = Φ−1(𝜋). This is known as a probit link. The corresponding
GLiM is called a probit regression model. Another useful c.d.f. is h(𝜂) = 1 −exp{−e𝜂}, which
produces the complementary log–log link g(𝜋) = log{−log(1 −𝜋)}.
When the subject matter does not provide an obvious choice for the link function, many
analysts default to the logit. Among other features, the logit link has appeal due to the inter-
pretability of its regression parameters: note that log{𝜋∕(1 −𝜋)} represents the log-odds
that a ‘success’ occurs. Modeling 𝜋via the logit and fitting, say, the multiple linear pre-
dictor 𝜂= 𝛽0 + 𝛽1x1 + 𝛽2x2 allows 𝛽2 to represent the unit change in log-odds – a log-odds
ratio – for a unit change in x2 when holding x1 constant.
One can also choose the link function empirically by applying a series of candidate links
to the data and selecting that link associated with the most favorable model fit. Useful in
this regard is the scaled deviance function, D∗( ̂𝜷) from Section 8.2.2. Recall from the 𝜒2
approximation for D∗( ̂𝜷) that E[D∗( ̂𝜷)] ≈n −p −1, so we expect E[D∗( ̂𝜷)∕(n −p −1)] ≈1.
Reasonable model fits will, therefore, display values of D∗( ̂𝜷)∕(n −p −1) near 1. When
D∗( ̂𝜷)∕(n −p −1) is much larger than 1, however, potential model inadequacy is indicated.
A common diagnostic rule-of-thumb (whenever 𝜑= 1) is to view a fit as questionable if
D∗( ̂𝜷)
n −p −1 > 1 +
2.8
√
n −p −1
(8.7)
(McCullagh and Nelder 1989, Section 4.4.3).
Example 8.3.1 Logistic regression in joint-action toxicology. Modern cell-screening tech-
niques provide toxicologists with rapid-throughput screening of hazardous chemicals. In a
single toxicity study, tens of thousands of cells can be examined. For example, Shi et al. (2010)

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
267
Table 8.2
Proportions of human hepatic cells exhibiting micronuclei
after exposure to DDT (in μmol/L) and nano-TiO2 (in μg/L).
Nano-TiO2 concentration
0
0.01
0.1
1.0
0
59
3000
65
3000
70
3000
67
3000
0.001
67
3000
75
3000
83
3000
84
3000
DDT concentration
0.01
76
3000
87
3000
96
3000
83
3000
0.1
94
3000
107
3000
110
3000
117
3000
Source: Deutsch and Piegorsch (2012, Table 1).
studied the joint action of two potential toxins, dichlorodiphenyltrichloroethane (DDT) and
titanium dioxide nanoparticles (nano-TiO2) in human hepatic cells. The first agent, DDT, was
a heavily used pesticide and is a known environmental carcinogen. The second, nano-TiO2, is a
strongly reacting oxidizer in particulate form. Combination of the two may produce enhanced
toxicity (called ‘synergy’), prompting study of their joint action.
For this experiment, the outcome variable was a binary indicator of cellular damage,
as represented by formation of extranuclear, damaged chromosome fragments known as
‘micronuclei.’ Specifically, Y = 1 if a cell exhibited chromosomal damage and Y = 0 if it did
not. The data in Table 8.2 present the observed proportions of damaged cells across a variety
of single- and joint-exposure combinations.
Interest with these data centers on assessing whether and how the two agents significantly
impact the rate of cellular damage. As the data are proportions, it is natural to apply the bino-
mial distribution: Yi ∼indep. Bin(Mi, 𝜋i), i = 1, 2, … , 16, where 𝜋i = 𝜋(xi1, xi2) is modeled
as a function of the two exposures. Notice that Mi is held constant at 3000 cells/exposure
combination.
For the predictor variables, the geometric spacing of the two exposure regimes argues for
use of logarithmic transforms. From Table 8.2, base-10 logs are indicated, so set xi1 = log10
(DDTi) + 4 and xi2 = log10(TiO2i) + 3. (The added constants ensure that the final exposure
levels are nonnegative.) For the control levels, apply consecutive-dose average spacing: from
a set of roughly equispaced nonzero doses x2 < x3 < · · · < xn, set the ‘control’ dose to
x1 = x2 −xn −x2
n −1
(8.8)
(Margolin et al. 1986). Here, applied to the log10-transformed doses, this conveniently pro-
duces x11 = 0 at the DDT control and x12 = 0 at the nano-TiO2 control.
For the linear predictor, include an intercept 𝛽0, terms for each single log-exposure, and
also a cross-product term, xi1xi2 to allow for possible synergism via the interaction of the two
agents. This gives
𝜂i = 𝛽0 + 𝛽1xi1 + 𝛽2xi2 + 𝛽3xi1xi2.

268
STATISTICAL DATA ANALYTICS
Consider use of the logit link function for this analysis. (This is a reasonable default,
although the probit or complementary log–log links could alternatively be applied; see
Exercise 8.6.) The following sample R code uses the glm() function to conduct the analysis.
(The variables are Y for Yi, Trials for Mi, x1 for the log-transformed DDT exposure, and
x2 for the log-transformed TiO2 exposure.)
> Ex831logit.glm <- glm( cbind(Y,Trials-Y) ˜ x1 + x2 + I(x1*x2),
family=binomial(‘logit’) )
> anova( Ex831logit.glm, test=‘Chisq’ )
In the R code, some features require explanation:
• To indicate that the response is a proportion, Yi∕Mi, R requires separate presentation
of the two component vectors, as Yi and Yi −Mi. This is performed by binding the two
columns together via cbind(). Do not present R with the calculated proportions Yi∕Mi.
• In the linear predictor, the ‘Identity’ function I() protects the direct multiplication of
x1 and x2. Without it, R would interpret the call to x1*x2 as a more-complex series of
terms not germane to the model being fit here, as in Example 7.5.1. See Dalgaard (2008,
Section 12.5).
• The family=binomial(‘logit’) option instructs R to fit a binomial likelihood with
the logit link.
• The call to anova(Ex831logit.glm,test=‘Chisq’) produces the analysis of
deviance table. The test=‘Chisq’ option conveniently adds a final column with the
LR P-values
The resulting R output (edited) is just the analysis of deviance table:
Analysis of Deviance Table
Model: binomial, link: logit
Response: cbind(Y, Trials - Y)
Terms added sequentially (first to last)
Df
Deviance
Resid.Df
Resid.Dev
Pr(>Chi)
NULL
15
53.723
x1
1
43.944
14
9.779
3.379e-11
x2
1
5.550
13
4.229
0.01848
I(x1*x2)
1
0.022
12
4.207
0.88211
R employs a slightly different ordering than that seen in Table 8.1, with the ‘residual’ quanti-
ties Resid.Df and D∗( ̂𝛽) (as Resid.Dev) appearing after the other two columns. The analy-
sis reports information for model terms when they are ‘added sequentially (first to
last)’ as per their order in the call to glm().
From the analysis of deviance, one finds that when fitted last, the interaction term
I(x1*x2) is insignificant (P = 0.8821). Alternatively, a 95% profile likelihood confidence
interval for 𝛽3 can be produced using the following sample R command
> confint( Ex831logit.glm, level=0.95, parm=‘x1x2’ )
which gives
2.5 %
97.5 %
-0.04066755
0.04730930

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
269
The interval contains 𝛽3 = 0, indicating that no significant synergy between the two exposures
is evidenced. As it provides no additional explanatory value in the model, the xi1xi2 interaction
term can be removed. The resulting RM is sometimes referred to as an ‘additive model,’
because its linear predictor only contains additive terms in x1 and x2. Sample R code is
> Ex831RM.glm <- glm( cbind(Y,Trials-Y) ˜ x1 + x2,
family=binomial(‘logit’) )
> anova( Ex831RM.glm, test=‘Chisq’ )
> summary( Ex831RM.glm )
The code is similar to that for the FM seen earlier, with a few additions. The call to sum-
mary(Ex831RM.glm) will display the IWLS/ML point estimates, their standard errors, and
the corresponding single-d.f. Wald tests. The output (edited) for the analysis of deviance
becomes
Analysis of Deviance Table
Model: binomial, link: logit
Response: cbind(Y, Trials - Y)
Terms added sequentially (first to last)
Df
Deviance
Resid.Df
Resid.Dev
Pr(>Chi)
NULL
15
53.723
x1
1
43.944
14
9.779
3.379e-11
x2
1
5.550
13
4.229
0.01848
The output (edited) from the call to summary() is
Coefficients:
Estimate
Std.Error
z value
Pr(>|z|)
(Intercept) -3.90377
0.06394
-61.050
< 2e-16
x1
0.16521
0.02506
6.593
4.31e-11
x2
0.05845
0.02483
2.354
0.0186
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 53.723
on 15
degrees of freedom
Residual deviance:
4.229
on 13
degrees of freedom
AIC: 109.75
From the RM analysis of deviance, the remaining two terms representing each log-exposure
variable are significant: when fitted last, the TiO2 term gives a partial LR test P-value for Ho:
𝛽2 = 0 of P = 0.018. Reversing the order – try it! – to place x1 last gives a partial LR P-value
for Ho: 𝛽1 = 0 of P ≈3 × 10−11. Even when adjusting for multiplicity via, for example, a
Bonferroni correction, both terms are significant at a familywise false positive error (FWE)
rate of 𝛼= 0.05.
A more coherent, consolidated, 2 d.f. test of Ho: 𝛽1 = 𝛽2 = 0 can be performed by relating
the deviance under the NULL model (with just 𝛽0) to the current RM fit:
> anova( glm(cbind(Y,Trials-Y) ˜ 1, family=binomial(‘logit’)),
Ex831RM.glm, test=‘Chisq’ )
which yields a P-value of 1.788 × 10−11; again, highly significant.
One might also consider including additional quadratic terms, x2
i1 and x2
i2, in the linear
predictor, but these do not significantly improve the model fit (see Exercise 8.6).

270
STATISTICAL DATA ANALYTICS
Other features of the output include the point estimates and their standard errors (under
Estimate and Std.Error, respectively), along with a reminder that the dispersion parameter
𝜑was ‘taken to be 1,’ as desired for the binomial model. The eventual predicted response
probability as a function of x1 and x2 is then
̂𝜋(x1, x2) =
1
1 + exp{3.9038 −0.1652x1 −0.0585x2}.
(8.9)
A few final calculations can generate the ratio D∗( ̂𝜷)∕(n −p) and the rule-of-thumb in
(8.7), respectively, for informal model adequacy assessment:
> residDF <- Ex831RM.glm$df.residual
> Ex831RM.glm$deviance/residDF
# stability measure
> 1 + ( 2.8/sqrt(residDF) )
# rule-of-thumb
These give
[1] 0.3253062
[1] 1.77658
We find D∗( ̂𝜷)∕(n −p −1) = 0.3253, while 1 + 2.8∕
√
n −p −1 = 1.7766. As the former is
much smaller than the later, no concerns with the model fit are indicated.
Further diagnostics may be conducted by examining the deviance residuals, di, from (8.6).
For example, a residual plot of di versus ̂𝜋(xi1, xi2) from (8.9) is available via the sample R
code
> di <- residuals( Ex831RM.glm, type=‘deviance’ )
> pihat <- predict( Ex831RM.glm, type=‘response’ )
> plot( di ˜ pihat, pch=19 ); abline( h=0 )
The residual plot appears in Figure 8.1. The pattern is generally stable, except for one large
negative residual at bottom, corresponding to the observed proportion 83/3000 = 2.767% at
exposure combination DDT = 0.01, TiO2 = 1.0. (A similar pattern emerges when plotting the
standardized residuals against the predicted probabilities; see Exercise 8.6.) Referring back to
the data in Table 8.2, this value clearly dips lower than the trend in its neighboring responses.
Further investigation is necessary to determine if this is just a naturally low response or if
some form of contamination or other disturbance has led to an outlying data point.
The overall conclusion to take from this analysis is that the logit link appears to fit the data
reasonably well. In doing so, it indicates that both exposure variables do produce significant
genotoxic effects in these cell systems.
◽
As the logistic (and probit, etc.) GLiM represents a form of supervised regression, it is
amenable to the various diagnostics and exploratory techniques from Chapter 7. Beyond the
adequacy rule-of-thumb and residual analysis illustrated in the previous example, full-scale
regression diagnostics can be conducted (Hosmer and Lemeshow 2013, Section 5.3) and
adjustments are possible when the suite of predictor variables is highly multicollinear (Schae-
fer 1986).
In high-dimensional problems with large p, it may be of interest to conduct logistic regres-
sion on a reduced subset of predictors. If so, regularized logistic regression via the least
absolute shrinkage and selection operator (Lasso) from Section 7.4.3 can be performed (Hastie
et al. 2009, Section 4.4.4), producing a form of sparse logistic regression. Like the Lasso,

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
271
0.020
0.025
0.030
0.035
0.040
−1.5
−1.0
−0.5
0.0
0.5
Predicted probability
Deviance residual
Figure 8.1
Deviance residual plot for reduced model fit in Example 8.3.1.
regularization again produces a form of regression a variable selector. For application in
R, the glmnet() function from the external glmnet package includes an option for regu-
larized/sparse logistic regression. (In fact, glmnet has capabilities that handle any logistic
regression with extremely large data sets.)
8.3.2
Trend testing with proportion data
For some binary-data regressions, only a sole predictor variable, xi, is recorded. Inferences
can then be distilled down to a single question: do increases in xi lead to significant increases
in the corresponding response probability, 𝜋i, when the latter is viewed as a function of xi?
This is often called the quantal response problem. The no-trend, null hypothesis here is Ho:
𝜋1 = · · · = 𝜋n.
A powerful test procedure that detects departures from Ho is known as the Cochran–
Armitage (CA) trend test (for proportions); its test statistic can be written as
ZCA =
∑n
i=1(xi −x)Yi
√
p(1 −p) ∑n
i=1 Mi(xi −x)2
,
(8.10)
where p = ∑n
i=1 Yi∕∑n
i=1 Mi is a pooled estimator of the common 𝜋under Ho and
x = ∑n
i=1 Mixi∕∑n
i=1 Mi.
In large samples, the null reference distribution for the CA statistic is approximately
standard normal. (The approximation is valid when every Mi is at least 10 and ∑n
i=1 Mi
≥50.) This allows for straightforward construction of the trend test: reject Ho in favor of
an increasing trend when ZCA ≥z𝛼. The corresponding, approximate, one-sided P-value is
P ≈1 −Φ(ZCA). To test against a decreasing trend, reject if ZCA ≤−z𝛼. The corresponding,

272
STATISTICAL DATA ANALYTICS
approximate, one-sided P-value is P ≈Φ(ZCA). To test against any (two-sided) departure
from Ho, reject if |ZCA| ≥z𝛼∕2.
The test based on (8.10) was developed by Cochran (1954) and Armitage (1955) and also
given in a more general form by Yates (1948). It possesses many favorable properties. ZCA is
invariant to linear transformations of xi, and it has a form of omnibus optimality: it has locally
highest power against any form of twice-differentiable, monotone function for 𝜋(xi) (Tarone
and Gart 1980). If the true regression relation is logistic, so that 𝜋(xi) = 1∕(1 + exp{−𝛽0
−𝛽1xi}), the CA trend test is even more powerful. The no-effect, null hypothesis now sim-
plifies to Ho: 𝛽1 = 0, versus the increasing-effect alternative of Ha: 𝛽1 > 0. In this setting, the
CA trend statistic has uniformly highest power for testing Ho against Ha. The same is true if
we test Ho: 𝛽1 = 0 against Ha: 𝛽1 < 0 (Cox 1958). In R, the basic CA trend statistic (actually,
its square) can be found via the prop.trend.test() function.
A caveat: computation of (8.10) assumes that the xis are reasonably symmetric about x.
When this is not the case, and if a simple transformation of the xis does not achieve approx-
imate symmetry, a skewness correction is advocated (Tarone 1986): calculate the skewness
measure
̂𝛾=
m3(1 −2p)
√
M+ −1
(M+ −2)
√
m3
2 p (1 −p)
,
(8.11)
where M+ = ∑n
i=1 Mi and mk = ∑n
i=1 Mi(xi −x)k∕M+ for k = 2, 3. If ̂𝛾deviates appreciably
from 0, reject Ho in favor of an increasing trend when ZCA ≥z𝛼+ ̂𝛾(z2
𝛼−1)∕6.
To illustrate, consider the following example from Piegorsch and Bailer (2005,
Example 3.6), using data originally discussed in Huang and Smith (1999).
Example 8.3.2 Trend test for ozone exceedance. Ozone (O3) is an important constituent
of the Earth’s upper atmosphere. At ground level, however, it is an irritant and can become a
health hazard. Public health officials typically base ozone air quality standards on the number
of exceedances over some safety level of O3 concentration. For example, the data in Table 8.3
give Y = {Number of surface O3 concentration exceedances above an air quality limit of 120
ppb} across a series of 2349 recordings at monitoring stations near Chicago, IL. The table
also lists the associated x = {Average ozone concentration (in ppb)}.
A plot of these data (not shown) indicates an increasing, sigmoidal trend in exceedance
rate with increasing average surface ozone. To verify this indication statistically, one can apply
the CA trend test via (8.10). (One data point had only a handful of recordings, where Mi < 10
and was removed from the presentation.)
These surface O3 data provide a weighted mean of x = 71.7080, with no large deviation
from symmetry in the design: ̂𝛾= 0.0543 from (8.11). For the sake of illustration, however,
consider inclusion of Tarone’s skewness adjustment here. The calculations are not difficult to
complete by hand, although a simple R function can be written to find the various statistics.
For example:
>
CAtrendSkew.stat <- function(x , Trials , Y) }
sumM <- sum(Trials);
pbar <- sum( Y )/sumM
crrctx <-
x - (sum( x*Trials )/sumM)
m2 <- sum( Trials*(crrctx^2) )/sumM
m3 <- sum( Trials*(crrctx^3) )/sumM
Zdenom <- sqrt( pbar*(1-pbar)*sumM*m2 )
gtop <- (1 - 2*pbar)*sqrt( sumM-1 )*m3

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
273
gbot <- (sumM-2)*Zdenom*m2/sqrt(sumM)
gamma = gtop/gbot
ZCA <- sum(crrctx*Y) / Zdenom
return ( list( statistic=ZCA, gamma=gamma) )
}
# end function
Applying this to the data in Table 8.3 gives the following output:
$statistic
[1] 20.3481
$gamma
[1] 0.054322
Notice the verification that ̂𝛾= 0.0543. From these, we reject the null hypothesis of no
trend against an alternative of increasing trend at, say, 𝛼= 0.01 if the ZCA statistic exceeds
z0.01 + ̂𝛾(z2
0.01 −1)∕6 = 2.326 + 0.0399 = 2.366. This clearly occurs, so a significant,
increasing trend in exceedance is evidenced.
◽
Table 8.3
Exceedance rates over an air quality threshold of 120 ppb for surface ozone (O3)
as related to average O3 surface concentrations (in ppb).
x = average O3 concentration
39.31
49.42
56.89
64.2
65.1
74.67
80.46
85.05
Y = exceedances
0
2
0
2
2
8
11
3
M = recordings
184
230
315
285
191
292
309
95
x = average O3 concentration
87.62
99.43
106.3
114
125.4
143.6
Y = exceedances
7
27
15
28
18
16
M = recordings
110
148
62
77
33
18
Owing to its many favorable properties, the CA trend test is the method of choice for
identifying an increasing (or decreasing) trend in a set of quantal response data. It is particu-
larly important in one special case: testing individual predictor variables in logistic regression.
For example, in a logistic regression with a single quantitative predictor variable – that is,
𝜋(xi) = 1∕(1 + exp −𝛽0 −𝛽1xi) – the Wald test of Ho: 𝛽1 = 0 against either a one- or two-sided
alternative hypothesis is known to behave aberrantly, even in large samples. The test statistic
W1 = ̂𝛽1∕se[ ̂𝛽1] can decrease to zero as ̂𝛽1 grows far from zero, giving weak indications of
departure from Ho (Hauck and Donner 1977). Væth (1985) gave general conditions for the
Wald test to exhibit this poor performance. The CA trend statistic does not suffer from this
instability and is recommend for use in this setting.
8.3.3
Contingency tables and log-linear models
When discrete data are observed as unbounded counts, it is natural to consider a Poisson
distribution as the parent p.m.f. for the probability model. Take Yi as the ith observed count
in a random sample, with mean rate of occurrence E[Yi] = 𝜇i, i = 1, … , n. As recognized in
Exercise 2.26, the Poisson is member of the exponential family in (8.1) and, hence, may be
employed in a GLiM. The natural parameter is 𝜃= log(𝜇). As with binomial-based GLiMs
in Section 8.3.1, the dispersion parameter for a Poisson GLiM is set to 𝜑= 1, along with the
function a(𝜑) = 1.

274
STATISTICAL DATA ANALYTICS
The link function most often employed with a Poisson GLiM is the natural logarithm
g(𝜇) = log(𝜇). The corresponding inverse link is h(𝜂) = e𝜂. This is known as a (Poisson)
log-linear regression model, because the log of the mean is being modeled as linear. Notice
here that because 𝜇> 0, any choice for the inverse link, including e𝜂, must be chosen to return
a value greater than 0.
Example 8.3.3 Contingency table analysis of hardwood tree associations.
Digby and
Kempton (1987, Chapter 6) presented a classic ecological data set on spatial associations
between 2076 different trees in a hardwood forest. The association is defined as whether or
not any tree’s nearest neighbor is of the same species. If not, no association is evidenced
regarding how different hardwoods distribute themselves spatially in this forest.
Five specific types of tree were studied: hickory, maple, black oak, red oak, and white oak.
The data were taken as Y = {Number of times a given type of tree was a nearest neighbor to
itself or another type}; they conveniently collect into a cross-classified table of counts, as in
Table 8.4.
The presentation in Table 8.4 represents the predictor variables as two separate qualitative
factors that contribute to the observed pattern of counts: factor A = {Marker tree} and factor
B = {Neighbor tree}. To assess the association between nearest neighbors, the table in effect
asks whether the two factors act as independent contributors. If not, counts identified with
one factor are contingent on the other factor. Thus this tabular display is often called an R × C
contingency table of count data. In Table 8.4, R = C = 5. The analytic question is one of
association versus independence between the R levels of the row factor and the C levels of the
column factor.
Many approaches exist for assessing association between two factors in an R × C con-
tingency table. A full survey exceeds the scope here; more details are available in sources
such as Agresti (2013). For analytic purposes, it is sufficient to recognize that associations in
a contingency table may be modeled via a Poisson GLiM by making use of the table’s R × C
factorial structure. That is, view the row and column factors as qualitative predictors as in
Section 7.5, such that i = 1, … , R indexes the row factor’s levels and j = 1, … , C indexes
the column factor’s levels. Assuming each count is Yij ∼indep. Poisson(𝜇ij), the mean 𝜇ij is
then related to a two-factor ANOVA-type linear predictor
𝜂ij = 𝜃+ 𝛼i + 𝛽j + 𝛾ij,
where the 𝛼is represent the row factor main effect, the 𝛽js represent the column factor main
effect, and the 𝛾ijs represent the row × column interaction. No association between row and
Table 8.4
Cross-classified counts of nearest-neighbor trees in a hardwood forest.
Neighbor tree
Hickory
Maple
Black oak
Red oak
White oak
Hickory
355
71
48
105
108
Maple
79
242
21
74
70
Marker tree
Black oak
51
25
27
12
20
Red oak
95
64
20
104
59
White oak
117
95
14
62
138
Source: Digby and Kempton (1987, Chapter 6).

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
275
column factors is indicated when 𝛾ij = 0 for all combinations of i and j. Since 𝜇ij must be
positive under the Poisson assumption, apply a logarithmic link in the log-linear model:
log(𝜇ij) = 𝜃+ 𝛼i + 𝛽j + 𝛾ij.
(8.12)
Problematically, the linear predictor in (8.12) involves more parameters, 1 + R + C + RC,
than the RC observations available to fit them. As with the ANOVA-type models in Section
7.5, an estimability constraint must be imposed. Examples include the corner-point con-
straints, 𝛼1 = 𝛽1 = 𝛾i1 = 𝛾1j = 0, or the zero-sum constraints ∑a
i=1 𝛼i = ∑b
j=1 𝛽j = ∑b
j=1 𝛾ij
= ∑a
i=1 𝛾ij = 0. The choice between these is usually arbitrary. For the full two-factor model
in (8.12), the constraints reduce the set of parameters down to a set of 1 + (R −1) + (C −1)
+ (R −1)(C −1) = RC parameters. Notice, however, that for the RC independent observa-
tions in the R × C table, even under the estimability constraints each independent count is
essentially fit to one parameter; this saturates the model fit as described in Section 8.2.2.
Nonetheless, this still allows for unambiguous estimation of all pertinent parameters in 𝜂ij.
Some important caveats are required when modeling contingency table data via a Poisson
model. In essence, different sampling constraints on how the counts are obtained can affect
the Poisson assumption, although in most cases the resulting test statistics – including those
discussed here – all have the same form and interpretation. For the technical details, interested
readers are referred, for example, to Agresti (2013, Chapter 9) or the discussion in Piegorsch
and Bailer (2005, Section 3.3.4).
To test for association, fit the saturated ‘FM’ model (8.12) and also an RM under the
no-association null hypothesis Ho: 𝛾ij = 0 for all i, j. The difference in scaled deviances
between the two models produces an LR test statistic for testing Ho. Using an analysis of
deviance from Table 8.1, the difference in deviances will be approximately distributed as 𝜒2
with (R −1)(C −1) d.f.
Unfortunately, this LR statistic exhibits poor small-sample stability and can incur
false-positive errors too often. It is not recommended for standard use. Instead, an asymp-
totically equivalent approach for R × C contingency tables call on the Pearson 𝜒2 statistic
mentioned in Section 8.2.3. Start with the predicted values under the reduced, Ho-restricted
model, which can be found to be
̂Yoij =
(∑R
k=1 Ykj
) (∑C
ℓ=1 Yiℓ
)
∑R
k=1
∑C
ℓ=1 Ykℓ
.
(8.13)
Next, calculate the Pearson statistic, which simplifies to
X2 =
R
∑
i=1
C
∑
j=1
(Yij −̂Yoij
)2
̂Yoij
.
(8.14)
In large samples, X2 ̇∼𝜒2[(R −1)(C −1)]. This is asymptotically equivalent to the LR statis-
tic, but far more stable if every expected cell frequency, ̂Yoij, is at least 1 and the overall average
expected cell frequency is at least 5, that is,
1
RC
R
∑
i=1
C
∑
j=1
̂Yoij ≥5.

276
STATISTICAL DATA ANALYTICS
If the table satisfies these rules-of-thumb, reject Ho in favor of some association between row
and column factors when the calculated statistic X2
calc exceeds the critical point 𝜒2
𝛼[(R −1)
(C −1)]; the corresponding approximate P-value is P{𝜒2[(R −1)(C −1)] ≥X2
calc}. (R × C
tables that do not satisfy the large-sample rules-of-thumb are called sparse. Piegorsch and
Bailer (1997, Section 9.3.4) gave some guidance on how to analyze sparse contingency tables.)
For the nearest-neighbor data in Table 8.4, rejection of the no-association hypothesis
Ho∶𝛾ij = 0, for all i, j,
implies that some ecological effect causes the trees to associate with or disassociate from one
another. To perform the fit in R, input the data as 5 × 5 = 25 rows of per-cell information, for
example, as the respective variables Marker, Neighbor, and Y in
hickory
hickory
355
hickory
maple
71
⋮
⋮
⋮
whiteoak
redoak
62
whiteoak whiteoak
138
Then, fit the saturated model via the sample R code
> A = factor(Marker)
> B = factor(Neighbor)
> neighbor.glm <- glm( Y ˜ A*B, family=poisson(‘log’) )
> anova( neighbor.glm, test=‘Chisq’ )
This produces an analysis of deviance table in the (edited) output
Analysis of Deviance Table
Model: poisson, link: log
Response: Y
Terms added sequentially (first to last)
Df
Deviance
Resid.Df
Resid.Dev
Pr(>Chi)
NULL
24
1301.65
A
4
430.83
20
870.82
<2.2e-16
B
4
451.73
16
419.10
<2.2e-16
A:B
16
419.10
0
0.00
<2.2e-16
Notice the value of zero under Resid.Df for the interaction term, indicating that the model fit
has been saturated, as expected. The LR statistic for testing Ho here is given as the difference
G2 = 419.10 −0 = 419.10, although as mentioned earlier this is not recommended for gen-
eral use.
To find the Pearson X2 statistic, simply fit the RM without the interaction terms, recover
the Pearson residuals, and sum their squares:
> neighborRM.glm <- glm( Y ˜ A+B, family=poisson(‘log’) )
> ci <- residuals( neighborRM.glm, type=‘pearson’ )
> sum( ci^2 )
> pchisq( sum(ci^2), df=neighborRM.glm$df.residual, lower=F )
The final two calculations give the X2 statistic and its approximate P-value:
[1] 460.1468
[1] 8.401886e-88

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
277
A check shows that the rules-of-thumb for the 𝜒2 approximation are satisfied with this 5 ×
5 table (left to reader). Thus the 𝜒2 P-value, which reports as essentially zero, provides a valid
large-sample inference. We see that a significant association exists among nearest-neighbor
species with these trees.
It is worth noting that R has a self-contained function (in fact, many) that calculates Pear-
son’s X2: chisq.test(). The data must be structured or manipulated into an R × C table or
matrix, as in the following sample R command:
> chisq.test( xtabs( Y ˜ factor(Marker)+factor(Neighbor) ) )
This gives (edited)
Pearson’s Chi-squared test
X-squared = 460.1468, df = 16, p-value < 2.2e-16
which corroborates the above glm-based calculations.
Further discovery is available here by asking in what direction(s) any significant associ-
ation in the table lies. For instance, recall that the Pearson (and deviance) residuals may be
standardized to give them roughly equal variances and thus greater comparability. In R, use
> chisq.test( xtabs( Y ˜ factor(Marker)+factor(Neighbor) ) )$stdres
or equivalently (with a call to round the reported values)
> zci = rstandard( neighborRM.glm, type=‘pearson’ )
> round( xtabs( zci ˜ factor(Marker)+factor(Neighbor) ), digit=2 )
The (edited) output from the latter is
factor(Neighbor)
factor(Marker)
blackoak hickory
maple
redoak whiteoak
blackoak
6.81
1.07
-1.53
-2.65
-1.29
hickory
0.96
12.28
-10.22
-1.62
-2.70
maple
-2.02
-9.24
15.26
-1.32
-2.97
redoak
-0.35
-2.48
-2.48
7.09
-0.92
whiteoak
-2.84
-3.00
-0.89
-1.62
7.88
R defaults to order the variables alphabetically, differing from that seen in Table 8.4 (if desired,
this can be easily changed). Nonetheless, the pattern here is revealing. Strong positive resid-
uals are seen along the diagonal, while the off-diagnol residuals are almost always negative.
(Only hickory and black oak combine to show positive off-diagonal values, and these are of
relatively low magnitude for standardized residuals.) This indicates that many more trees were
observed on the diagonals than would be expected under the null hypothesis of no association,
with the reverse predominantly true for off-diagonals. It appears that trees of similar type tend
to cluster near each other, at least for those studied in this hardwood forest.
Digby and Kempton (1987, Section 6.1) gave suggestions for other advanced methods to
tease out subtleties in the nearest-neighbor associations with these data.
◽
A special case of the general R × C table occurs when R = C = 2. This gives a 2 × 2
contingency table, as per the schematic in Table 8.5.
For testing the null hypothesis of no association between Factors A and B in a 2 × 2
contingency table, (8.13) and (8.14) remain valid. These can be applied if the large-sample

278
STATISTICAL DATA ANALYTICS
Table 8.5
Schematic for 2 × 2 contingency table.
Factor B
Level B1
Level B2
Row total
Level A1
Y11
Y12
Y1+
Factor A
Level A2
Y21
Y22
Y2+
Column total
Y+1
Y+2
Y+ +
rules-of-thumb hold: all four ̂Yoijs are at least 1 and ∑2
i=1
∑2
j=1 ̂Yoij∕4 ≥5. If so, reject the null
hypothesis of no association when X2 ≥𝜒2
𝛼(1).
The 2 × 2 table can be interpreted in a number of ways, however. In particular, suppose
the study design has fixed the column totals Y+1 and Y+2, for example, if the numbers of
sample elements at levels B1 and B2 were chosen before determining their Factor A status.
The sampling then reverts to binomial, with Y1j ∼indep. Bin(Y+j, 𝜋j) for j = 1, 2. Focus is now
on Factor B and whether differences exist between its two levels when ‘success’ is defined
(generically) by achieving Factor A’s level A1, versus ‘failure’ at level A2. (If, alternatively,
the row totals are fixed, the interpretation is similar: just transpose the perspective for which
variates take on the binomial parent.)
The no-effect null hypothesis can, therefore, be written as Ho: 𝜋1 = 𝜋2, and this will in
fact be equivalent to the no-association null hypothesis from the broader 2 × 2 perspective.
Now, however, one-sided (along with two-sided) alternatives can conveniently be studied; for
example, Ha: 𝜋1 < 𝜋2 asks whether moving from level B1 to level B2 increases the probability
of A1 success.
A different sampling constraint may instead force the table total Y++ to be fixed but
allow all cells to otherwise vary. This induces what is known as a multinomial distribution
structure – an extension of the binomial – on the table. When properly constructed, however,
the various hypotheses and statistics for testing association in the table will all coincide.
Inferences for the 2 × 2 table can be developed using exact calculations, that is, without
call to a large-sample reference distribution such as 𝜒2. This is called exact testing, as dis-
cussed in Section 5.4.3, and dates back to the theories of Fisher (1935). To calculate the exact
P-value, Fisher fixed the row and column margins in the 2 × 2 table. He then recognized that
the P-value associated with the 1 d.f. in a 2 × 2 table is the conditional probability of recov-
ering a tabular configuration as extreme or more extreme – in the direction of Ha – than that
actually observed. If this P-value drops below the nominal, target, 𝛼level, reject Ho in favor
of Ha: 𝜋1 < 𝜋2.
The conditional probabilities that make up the exact P-value may be indexed by Y11.
Conditioning on the margin totals drives Y11 to take a hypergeometric distribution, with con-
ditional p.m.f.
P[Y11 = y | Y1+, Y2+, Y+1, Y+2] =
(
Y+1
y
) (
Y+2
Y1+ −y
)
(Y+ +
Y1+
)
(8.15)

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
279
for y = 0, … , 𝜐1 and where 𝜐1 = min{Y1+, Y+1}. The one-sided P-value is then
P =
𝜐1
∑
i=yo
P[Y11 = i | Y1+, Y2+, Y+1, Y+2],
where yo is the observed value of Y11. Similar calculations are available if the one-sided alter-
native is reversed.
This construction with hypergeometric probabilities is known as the Fisher exact test
(FET), or the Fisher–Irwin exact test to acknowledge the contributions by Irwin (1935). In
some circles, it is also referred to as a ‘hypergeometric test.’
The FET is available for any sample size and is ‘exact’ in the sense that the false positive
rate of the test, say, 𝛼e is never larger than the prespecified nominal rate of 𝛼. (Hence, the FET
is also ‘conservative.’ This results from the discrete nature of the hypergeometric p.m.f.). The
test can suffer loss of power, however, when the design is highly unbalanced, that is, when Y+1
is far larger (or smaller) than Y+2, say, Y+k∕Y+ℓ> 20 (k ≠ℓ). Kang and Ahn (2008) offered
some alternatives for this extreme case.
For computing two-sided P-values, the calculations grow complex, because departure
from Ho now involves both directions. Strategies for doing so vary. One possibility, used in
the R function fisher.test(), is to find those tables whose hypergeometric probabilities
(8.15) rest less than or equal to the observed table and sum these values. Or, one could modify
the core test by identifying some two-sided measure of departure such as X2 and comput-
ing that measure for all possible 2 × 2 tables with the same row and column totals as those
observed. The modified P-value is then the sum of those hypergeometric probabilities cor-
responding to tables whose measures are larger than that of the observed table. Hirji (2005,
Section 7.6) and Lydersen et al. (2009) discussed the FET and exact testing in depth; also see
Berger (1996).
Note that, in principle, nothing prevents us from extending the 2 × 2 FET to any R × C
table, except perhaps that the underlying computations can grow burdensome. For moderate
tables/sample sizes, this is a shrinking concern, given ongoing advancements in computing
technology. (R’s fisher.test usually handles moderately sized tables. Very large tables
may benefit by specifying its simulate.p.value=T option.) For large sample sizes, the
𝜒2 approximation to Pearson’s X2 exhibits sufficient accuracy and, at least with two-sided
alternatives, can be reasonably employed instead.
Example 8.3.4 Asthma/allergy association.
Halonen et al. (1997) explored occurrence
of respiratory ailments in 755 children (aged 6) who were sensitized to specific allergens.
Among the data they reported was a 2 × 2 comparison between positive allergic reactions via
skin-prick tests to the common mold Alternaria alternata and prevalence of asthma, as given
in Table 8.6.
Here the focus is on differences in asthma response between the two Alternaria categories
in Table 8.6. In particular, are children who respond positively to the Alternaria skin test more
likely to be asthmatic? This translates to testing whether Ho: 𝜋1 = 𝜋2 versus Ha: 𝜋1 < 𝜋2,
where
𝜋1 = P[Asthma pos.|Alternaria neg.]
and
𝜋2 = P[Asthma pos.|Alternaria pos.].
The sample proportions ̂𝜋1 = Y11∕Y+1 = 42/624 = 6.73% and ̂𝜋2 = Y12∕Y+2 = 41/131
= 31.30% are certainly suggestive, but a formal inference is still needed.

280
STATISTICAL DATA ANALYTICS
Table 8.6
A 2 × 2 contingency table data for association between Alternaria
alternata allergic response and asthma status for 6-year old children.
Alternaria status
Allergic negative
Allergic positive
Row total
Positive
42
41
83
Asthma status
Negative
582
90
672
Column total
624
131
755
Source: Halonen et al. (1997).
Sample code to enter the data in R is
> table2x2.mtx <- matrix( c(42, 582, 41, 90), nrow=2, byrow=F )
> colnames( table2x2.mtx ) <- c( ‘Alt neg’, ‘Alt pos’ )
> rownames( table2x2.mtx ) <- c( ‘Asthma pos’, ‘Asthma neg’ )
The table2x2.mtx object is then
> table2x2.mtx
Alt neg
Alt pos
Asthma pos
42
41
Asthma neg
582
90
mimicking Table 8.6. The FET is implemented via the standalone command
> fisher.test( table2x2.mtx, alternative=‘less’ )
Note the use of the alternative=‘less’ option to perform the one-sided test in the appro-
priate direction. The consequent FET output contains a variety of components; for our use,
the pertinent information is
Fisher’s Exact Test for Count Data
data:
table2x2.mtx
p-value = 5.014e-13
alternative hypothesis: true odds ratio is less than 1
(The output highlights that technically fisher.test tests against the alternative that the
odds ratio
𝜋1∕(1 −𝜋1)
𝜋2∕(1 −𝜋2)
is less than 1. This is equivalent to testing 𝜋1 < 𝜋2, as desired.) The reported, one-sided
P-value is P = 5.014 × 10−13, which is highly significant at any reasonable false positive
rate. We find that a positive Alternaria skin test does indeed suggest a significantly higher
prevalence of asthma in these children.
◽
Of course, not every study with Poisson count data is as complex as the R × C table illus-
trated in Example 8.3.3. Some analyses assess the effect of just a single quantitative predictor,
xi, via a simple log-linear regression: log(𝜇i) = 𝛽0 + 𝛽1xi. Interest then centers on inferences
for the log-linear slope parameter 𝛽1, including its MLE, 1 −𝛼confidence limits, and/or tests

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
281
of the null hypothesis Ho: 𝛽1 = 0. Both the Wald and LR tests (and their related confidence
intervals) perform adequately in this case. (All these operations are available via R’s glm()
function.) A CA trend test for Poisson counts is also available, as described in Exercise 8.15.
8.3.4
Gamma regression models
A common form of data in large-scale analytics involves positively valued outcomes, Y > 0,
with nonconstant variances. Data in this form may be modeled with the gamma distribution
from Section 2.3.8. There, the p.d.f. was given in (2.32) via its traditional form:
fY(y) =
1
Γ(𝛼)𝛽𝛼y𝛼−1 e−y∕𝛽I(0,∞)(y).
Exercise 8.16 shows, however, that by reparameterizing 𝛼and 𝛽in terms of the population
mean E[Y] = 𝜇= 𝛼𝛽and of the dispersion parameter 𝜑= 1∕𝛼, the p.d.f. satisfies the expo-
nential family criterion in (8.1). The exercise further shows that Var[Y] = 𝜇2𝜑(so the variance
is quadratic in 𝜇) and that the coefficient of variation,
√
Var[Y]∕E[Y] from Section 2.3.7, is,
therefore, constant with respect to 𝜇. Thus positive-valued data with constant coefficients of
variation can be analyzed on their original scale of measurement via a gamma GLiM (also
called a ‘gamma regression.’)
A caveat: some confusion exists on what to call 𝜑with this model. Many sources refer to
𝜑as the exponential family scale parameter. In the general literature, however, most authors
refer to 𝛽(or sometimes 1∕𝛽) as the ‘scale parameter’ under a gamma model, with 𝛼as the
‘shape parameter.’ Analysts must ensure they know precisely which parameter is being used
for which purpose when fitting gamma GLiMs to data.
As Y > 0 and, therefore, E[Y] = 𝜇> 0, a useful link function to employ with gamma
GLiMs is the natural logarithm: 𝜂= g(𝜇) = log(𝜇), with strictly positive inverse link 𝜇= h(𝜂)
= e𝜂. This is a form of log-linear model for continuous measurements, because it relates a
linear predictor to the mean response via a logarithmic link.
Given a set of predictor variables for the linear predictor 𝜂i, estimation proceeds via the
IWLS/ML approach in Section 8.2.1. To estimate the dispersion parameter, the Pearson-based
𝜒2 moment estimator ̂𝜑P is recommended. One could also extend the ML calculations by
assuming 𝜑is unknown in (8.3) if desired – see the gamma.dispersion() function from
the MASS package – although both ̂𝜑ML and the deviance-based moment estimator ̂𝜑D are
known to exhibit instabilities with the gamma model (Faraway 2006, Section 7.1).
Example 8.3.5 US emergency room expenditures.
The US Department of Health and
Human Services studies patterns in US health care costs through its Medical Expenditure
Panel Survey (MEPS); see http://meps.ahrq.gov/mepsweb/. For instance, Table 8.7 gives data
on total expenditures (in $) from a 2009 sample of the US population. The sample contains
nonzero expenditures from n = 6438 emergency room (ER) visits. (As previously, the table
gives only a portion of the data. The complete data is available online at http://www.wiley.com
/go/piegorsch/data_analytics.) The data are stratified by whether the ER visit involved an
actual ‘emergency’ (accident or injury) versus any nonemergency event.
Of interest with these data is whether total expenditures differ significantly between
the two types of ER visits. This is a basic two-sample comparison, as seen in Section
5.4.2. The methods in that section require, however, that the parent distribution for Y be
normal (Gaussian). Monetary expenditure data are notoriously skewed – as is true here; see

282
STATISTICAL DATA ANALYTICS
Table 8.7
Total nonzero expenditures (in $) for n = 6438 ER
visits sampled throughout the United States in 2009.
Visit category
Total expenditure
Nonemergency
0.92, 1.23, 1.68, 2.68, 3.00, … , 25 413.91,
25 413.91, 25 413.91, 25 683.96, 34 053.42
Emergency
0.18, 3.00, 4.20, 6.89, 7.68, … , 25 413.91,
25 413.91, 31 057.14, 37 165.86, 37 492.08
Abbreviation: ER, emergency room.
Source: http://meps.ahrq.gov/mepsweb/.
Exercise 8.17 – and not directly amenable to normal-based analyses. One could consider a
stabilizing transformation, as in Section 3.4.3, although it is instructive to first examine the
data more closely.
In particular, if the variable Y contains the data and the variable visit indicates the visit
category (0 for nonemergency; 1 for emergency), then sample R code to calculate the coeffi-
cient of variation for each subsample is
> sd(Y[visit==0])/mean(Y[visit==0])
#nonemergency
> sd(Y[visit==1])/mean(Y[visit==1])
#emergency
which produces
[1] 2.0956
[1] 2.0723
respectively. The two coefficients are almost identical, suggesting appeal to a gamma par-
ent distribution. The two-sample comparison can then be performed by employing a gamma
GLiM, using the single qualitative predictor variable visit to delineate the visit categories.
Sample R code is
> expER.glm = glm( Y ˜ factor(visit), family=Gamma(‘log’) )
> anova( expER.glm, test=‘F’ )
Notice the use of factor() to produce the qualitative predictor. Because the gamma disper-
sion parameter must be estimated, the call to anova() includes the test=‘F’ option. This
employs an F-statistic to calculate the sequential P-values, as described in Section 8.2.4.
The use of a single qualitative predictor generates predicted values ̂Yi from the glm fit
which on the response scale are simply the corresponding subsample means. Find these via
> Yhat = predict( expER.glm, type=‘response’ )
> round( unique(Yhat), digit=2 )
This gives average expenditures of $ 914.61 for non-ER visits and $ 977.34 for ER visits.
Output (edited) from the call to anova() for the analysis of deviance table is
Analysis of Deviance Table
Model: Gamma, link: log
Response: Y
Terms added sequentially (first to last)
Df
Deviance
Resid.Df
Resid.Dev
F
Pr(>F)
NULL
6437
10710
factor(visit)
1
7.0261
6436
10703
1.616
0.2037

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
283
The approximate P-value to test for differences between the two levels of the visit factor is
seen to be P = 0.2037. This is insignificant at any reasonable false positive rate. We conclude
that total expenditures do not appear to vary significantly between these two types of ER visits.
In passing, recall that the Pearson estimate for 𝜑is found by summing the squares of the
Pearson residuals and dividing by the residual d.f. Sample R code is
> ci = resid( expER.glm, type=‘pearson’ )
> phihat.P = sum( ci^2 )/expER.glm$df.residual
from which we find ̂𝜑P = 4.3480. This estimate is also found as a component of the output
(not shown) from summary(expER.glm).
◽
Exercises
8.1
As noted in Section 8.1.1, the normal (Gaussian) distribution is a member of the expo-
nential family. Thus, for example, the SLR model from Section 6.2.1 qualifies as a
GLiM. For illustration, fit this SLR as a GLiM to the data on UK mortality in Table 6.1.
(a) Appeal to R’s glm() function using the gaussian family. Produce a summary
output of the resulting glm object and compare the results to the output from sum-
mary(lm(Y ˜ x)) as displayed in Example 6.2.2. Comment on any differences.
(b) Also construct an analysis of deviance table via a call to anova(). As the disper-
sion parameter 𝜑= 𝜎2 here is estimated from the data, use the test=‘F’ option
as in Section 8.2.4. What inferences can you make using this table? What is the
estimate of 𝜑?
(c) One can construct an ANOVA table to study variation in any SLR (or MLR) model,
mimicking the format in Table 7.6. Do so with these data, via the R command
anova(lm(Y ˜ x)). Compare this to the output in Exercise 8.1b.
8.2
Upadhyay (2014) presented proportion data, Yi∕Mi, describing a large bank’s loan
defaults as a function of the borrower’s age group. The predictor values, xi, are taken
as the mid-points of the group ranges. The complete data set is available online at http:
//www.wiley.com/go/piegorsch/data_analytics; a portion of it is given as follows:
Age group (years)
21–24
24–27
27–30
30–33
33–36
· · ·
57–60
Midpoint (years)
22.5
25.5
28.5
31.5
34.5
· · ·
58.5
Yi = Defaults
14
20
172
169
188
· · ·
9
Mi = Loans
310
511
4000
4568
5698
· · ·
788
(a) Plot the proportions Yi∕Mi against xi and comment on the result.
(b) View load default as a ‘success’ and analyze these data via a binomial GLiM.
Use a simple linear predictor, 𝜂i = 𝛽0 + 𝛽1xi. Apply each of (i) the logit
link, (ii) the probit link, and (iii) the complementary log-log link. (In R, use
family=binomial(‘probit’)
and
family=binomial(‘cloglog’),
as
needed, in the call to glm().) Identify which of the three links (logit, probit, or
complementary log–log) gives the best fit, as gauged by smallest stability measure
D∗( ̂𝜷)∕(n −p −1).

284
STATISTICAL DATA ANALYTICS
(c) For the best-fitting link found in Exercise 8.2b, find a 90% profile likelihood confi-
dence interval on the regression parameter, 𝛽1. (Hint: Recall the confint() func-
tion in R. Be sure to load the MASS package first.) Interpret your finding; in partic-
ular, does the interval contain 𝛽1 = 0 and if so/if not, what does this suggest about
loan default rates at this bank?
(d) For the best-fitting link found in Exercise 8.1b, find the standardized deviance
residuals di∕
√
1 −hii from the model fit and plot these against the predicted proba-
bilities. (Hint: In R, the hatvalues() and/or rstandard() functions may prove
useful.) Comment on any patterns in the plot.
8.3
Return to the rainbow trout carcinogenesis data in Exercise 5.34. Of additional interest
here is assessing whether or not a significantly increasing trend in cancer occurrence
is observed over the exposure dose xj. Consider the following.
(a) Plot the observed proportions Yj∕nj against the dose xj. What pattern appears?
(b) Apply the CA trend test for proportions to these data, using the ZCA statistic from
(8.10). As the doses are geometrically spaced, include possible adjustment for skew
in the predictor variable via (8.11). Operate at 𝛼= 0.01. What do you conclude?
How does this compare to the inferences achieved in Exercise 5.34?
(c) Another way to adjust for skew in the dose/exposure regime here is to apply a
logarithmic transform to xj. As adjacent doses are essentially doubled, calculate
uj = log2(xj). Replace xj with uj in your calculations from Exercise 8.3b. Does the
skewness adjustment appear necessary? (What is ̂𝛾?) Do the conclusions change in
any meaningful way?
8.4
The following data, modified from Silvapulle (1981), give proportions of male psychi-
atric patients’ responses to a general health questionnaire (GHQ). The predictor vari-
able, x, is an integer-valued score quantifying decreasing health status as x increases.
The response Y is number of patients (‘cases’) exhibiting psychiatric disorders, out of
a total M at that value of x.
GHQ score
0
1
2
4
5
7
10
Yi = ‘Cases’
0
0
1
1
3
2
1
Mi = Total tested
18
8
2
1
3
2
1
(a) Plot the proportions Yi∕Mi against xi. Is there a visible increase in the response as
x grows?
(b) Fit a logistic regression model with a simple linear predictor 𝜂i = 𝛽0 + 𝛽1xi to these
data. Find the MLE of 𝛽1. Also find its standard error and construct the Wald statis-
tic for testing Ho: 𝛽1 = 0 against Ho: 𝛽1 > 0. What do you conclude at 𝛼= 0.05?
Does this seem unusual?
(c) The strange result in Exercise 8.4b results from an instability in the MLE with
this sort of response pattern. Return to your data plot and notice that no overlap

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
285
exists across the predictor scale between proportions exhibiting only nonresponses
(Y∕M = 0) and proportions exhibiting complete responses (Y∕M = 1). When this
occurs in logistic regression the data are separated, and the likelihood will not have
a finite maximum for any value of 𝛽1 (Hosmer and Lemeshow 2013, Section 4.4).
The MLE of 𝛽1 is, therefore, infinite or, more properly, does not exist. (R’s attempt
to report a finite MLE is a consequence of this instability. This effect is not limited
to completely separated data as given here; Silvapulle (1981) discussed a form of
quasi-complete separation that can lead to infinite parameter estimates as well.)
From your output, calculate the log odds ratio for a unit change in x and comment
on the result.
(d) Use the CA statistic to test for an increasing trend. Does this test exhibit any insta-
bilities? If not, what do you conclude at 𝛼= 0.05?
8.5
Show that the CA trend statistic for proportions is invariant to linear transformations
of the predictor variable; that is, show algebraically that replacing xi with ui = a + bxi
in (8.10), for known constants a and b, does not change the value of the test statistic.
8.6
Return to the GLiM analysis of the joint-action toxicology data in Example 8.3.1.
(a) Under the logit link, verify that by adding quadratic terms in x1 and x2 to the liner
predictor provides no significant improvement in the model fit. Operate at a false
positive rate of 5%.
(b) Find the standardized deviance residuals di∕
√
1 −hii from the RM fit using only x1
and x2, and plot these against the predicted probabilities. (Hint: In R, the hatval-
ues() and/or rstandard() functions may prove useful.) Verify that the pattern
does not differ drastically from that in Figure 8.1.
(c) Repeat the analysis given in the example, now using (i) a probit link and (ii) a com-
plementary log–log link. (In R, use the family=binomial(‘probit’) and fam-
ily=binomial(‘cloglog’) options, respectively.) Identify which of the three
links (logit, probit, or complementary log–log) gives the best fit, as gauged by
smallest stability measure D∗( ̂𝜷)∕(n −p). (One could alternatively employ an infor-
mation criterion such as the Akaike information criterion (AIC) as in Section 7.3.2,
although here either measure will give the same qualitative result.) For the best
model, do the final results change drastically from those in the example?
8.7
Lindsay and Liu (2009) present a contingency table illustrating an archetypal biological
association: hair color and eye color in humans. The data, over 592 subjects, form a
4 × 4 table:
Eye color
Brown
Blue
Hazel
Green
Black
68
20
15
5
Brunette
119
84
54
29
Hair color
Red
26
17
14
14
Blonde
7
94
10
16

286
STATISTICAL DATA ANALYTICS
(a) Check the large-sample rules-of-thumb for application of Pearson’s X2 statistic
(8.14) to this table. If valid, use X2 to assess whether hair color and eye color exhibit
a significant association. Operate at a false positive rate of 10%.
(b) Calculate the standardized Pearson residuals from the 𝜒2 analysis and determine
if patterns exist that help to describe any association in the table.
8.8
Similar to the Altmetric data in Exercise 5.18, a study was conducted of social media
posts for a sample of 13 023 scientific articles appearing in a database collected by
researchers at altmetric.com. The articles were grouped by general subject area and
then the number of Facebook, Twitter, and general blog entries associated with each
article were counted. The data, kindly provided by Mr. Euan Adie of altmetric.com,
form a 6 × 3 contingency table, as follows:
Altmetric source
Facebook
Twitter
Blogs
Biology and Medicine
2476
25192
1036
General
873
11053
620
Humanities
34
514
23
Mathematics and Information Science
49
800
51
Physical Science and Engineering
336
3104
336
Social Science and Business
239
4140
242
(a) One might ask whether the pattern of social media posts differs between the dif-
ferent subject areas. This translates to an interaction between article subject area
and Altmetric source. To assess this, check the large-sample rules-of-thumb for the
application of Pearson’s X2 statistic (8.14) with this table. If valid, use X2 to assess
if a significant interaction exists. Operate at a false positive rate of 1%.
(b) Calculate the standardized Pearson residuals from the 𝜒2 analysis and determine
if patterns exist that help to describe any interaction in the table.
8.9
Ezzikouri et al. (2008) reported on a study of liver cancers among 318 hepatitis patients,
relating their cancer status (present or absent) to the human manganese superoxide
dismutase MnSOD gene. Of interest was whether any association exists between cancer
status and different genotypic polymorphisms of MnSOD. The data comprise a 3 × 2
table, as follows:
Liver cancer status
Absent
Present
Val/Val
21
81
MnSOD polymorphism
Val/Ala
45
101
Ala/Ala
30
40

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
287
(a) Check the large-sample rules-of-thumb for the application of Pearson’s X2 statistic
(8.14) to this table. If valid, use X2 to assess if cancer status exhibits any association
with the genetic factor. Operate at a false positive rate of 5%.
(b) Calculate the standardized Pearson residuals from the 𝜒2 analysis and determine
if patterns exist that help to describe any association in the table.
8.10
Faraway (2006, Section 4.1) displayed data on semiconductor wafer manufacturing
in an industrial plant, relating the presence of contaminant particles on machine dies
to quality of the wafers produced by those dies. Of interest was whether any associ-
ation exists between contamination status and wafer quality. A sample of 450 wafers
produced the following 2 × 2 table:
Wafer quality
Good
Bad
Row total
Absent
320
80
400
Contamination
Present
14
36
50
Column total
334
116
450
Use the FET to assess if any association is evidenced between the two factors in this
table. Also apply Pearson’s X2 statistic from (8.14). (Verify first if the large-sample
rules-of-thumb are valid for the application of X2.) How do the two inferences compare
at a nominal false positive rate of 5%?
8.11
Recall Exercise 5.29, where the concept of ‘A–B testing’ was discussed: a web site
designer tests whether a new site (variant ‘B’) attracts more customers, compared to
the current site (variant ‘A’). When such testing involves two independent groups of
Internet users, the data essentially fall into a 2 × 2 table for which the FET would be
appropriate. A new design (‘B’) for a web site’s signup page was tested on Y+2 = 2000
volunteers, along with the original design (‘A’) on an independent set of Y+1 = 2000
volunteers. Y12 = 793 of the 2000 volunteers clicked on the B-design’s
Sign Up Now
link, compared with Y11 = 610 for the A-design’s link. Use an FET to assess if
the B-group exhibited a significant increase in sign-up activity, compared with the
A-group. Operate at a nominal false positive rate of 1%.
8.12
In a US Census Bureau study of annual incomes (in 1994 $), data were collected on
whether or not US residents reported incomes above $ 50 000 (Kohavi 1996). Stratified
by sex of the respondent, the data showed that Y11 = 1179 out of Y+1 = 10 771 women
exceeded the $ 50K threshold, while Y12 = 6662 out of Y+1 = 21 790 men exceeded
the threshold. Do these data indicate that US women whose 1994 incomes exceeded $
50 000 did so significantly less often than US men? Justify your answer. (Operate at a
5% false positive rate.)
8.13
Verify the indication in Example 8.3.4 that the null hypotheses Ho: 𝜋1 = 𝜋2 and
Ho ∶𝜋1∕(1 −𝜋1)
𝜋2∕(1 −𝜋2) = 1

288
STATISTICAL DATA ANALYTICS
are equivalent. Also verify that the alternative hypotheses Ha: 𝜋1 < 𝜋2 and
Ha ∶𝜋1∕(1 −𝜋1)
𝜋2∕(1 −𝜋2) < 1
are equivalent. (A similar equivalence will hold for one-sided upper alternatives.)
8.14
From a study of species biodiversity, Carroll (1998) gave data on numbers of different
bird species observed throughout the Indian subcontinent, as a function of altitudinal
relief (i.e., the difference between the highest and lowest elevations above sea level in
a study region of fixed size). To predict Y = {Number of species in a 1 km2 region},
take x = log{Relief} + 2. (2 is added to make all log-relief values positive.) The data
are available online at http://www.wiley.com/go/piegorsch/data_analytics; a sample is
given as follows:
x = log{Relief} + 2
4.050
3.452
3.662
3.917
· · ·
2.571
2.916
Y = Number of species
261
224
171
326
· · ·
225
193
(a) Plot the species counts against x. Does any pattern appear?
(b) Fit a log-linear Poisson regression model to the data, using the simple linear pre-
dictor 𝜂i = 𝛽0 + 𝛽1xi. Calculate the stability measure D∗( ̂𝜷)∕(n −p −1) for this fit.
Verify that it violates the rule-of-thumb in (8.7).
(c) A common reason for D∗( ̂𝜷)∕(n −p −1) to violate (8.7) with counts is the pres-
ence of overdispersion in the data, that is, where Var[Yi] grows larger than that
predicted under the parent Poisson assumption. A useful alternative p.m.f. for Y
in this case is the negative binomial distribution from Section 2.3.4, using its 𝜇, 𝛿
parameterization in (2.25). When 𝛿is unknown, this is not technically a GLiM;
however, regression estimators can still be calculated using ML via, for example,
R’s glm.nb() function from the MASS package. Apply this to the species counts:
continue to use the log link and a simple linear predictor. Does the stability measure
D∗( ̂𝜷)∕(n −p −1) improve under the negative binomial parent?
(d) Build an analysis of deviance table from the negative binomial log-linear regression
fit in Exercise 8.14c. Is the x variable’s contribution significant at 𝛼= 0.01? Include
a 99% confidence interval for 𝛽1 in your calculations.
(e) Find the standardized deviance residuals from the fit in Exercise 8.14c and plot
these against the predicted response. Do any untoward patterns appear?
8.15
A CA trend test also exists for independent count data. Suppose Yij ∼Poisson(𝜇[xi]),
i = 1, … , n; j = 1, … , Ji. The no-trend null hypothesis is Ho:𝜇(x1) = · · · = 𝜇(xn). At
each xi, let the ith sample mean be Yi+ = ∑Ji
j=1 Yij∕Ji. If Ho is true, each Yi+ will approx-
imate the pooled mean Y+ + = ∑n
i=1
∑Ji
j=1 Yij∕∑n
i=1 Ji. Using these quantities, the CA
test quantifies departure from Ho via the statistic
ZCA =
∑n
i=1 Ji(xi −x)Yi+
√
Y+ +
∑n
i=1 Ji(xi −x)2
,

SUPERVISED LEARNING: GENERALIZED LINEAR MODELS
289
where x = ∑n
i=1 Jixi∕∑n
i=1 Ji. Reject Ho in favor of an increasing (decreasing) trend
when ZCA is greater (less) than or equal to z𝛼(−z𝛼). (Two-sided testing is also possible.)
(a) Show that this CA trend statistic for counts is invariant to linear transformations of
the predictor variable; that is, show that replacing xi with ui = c + dxi, for known
constants c and d, does not change the value of ZCA.
(b) In the presence of overdispersion, as in Exercise 8.14c, ZCA will be unstable and
reject Ho too often. A adjustment given by Astuti and Yanagawa (2002) constructs
the generalized CA statistic
ZGCA =
∑n
i=1 Ji(xi −x)Yi+
√∑n
i=1 (xi −x)2 ∑Ji
j=1 (Yij −Y+ +)2
.
Reject Ho in favor of an increasing (decreasing) trend when ZGCA is greater (less)
than or equal to z𝛼(−z𝛼). Reject against a two-sided alternative if |ZGCA| ≥z𝛼∕2.
Apply this test to the data in Exercise 8.14; test against two-sided departure from
Ho. How do the conclusions compare at 𝛼= 0.01?
8.16
Let Y ∼Gamma(𝛼, 𝛽) from (2.32).
(a) Show that the gamma p.d.f. can be written in the form of an exponential family
density, as in (8.1), by reparameterizing the traditional parameters 𝛼and 𝛽in terms
of the mean 𝜇= 𝛼𝛽and the dispersion parameter 𝜑= 1∕𝛼.
(b) Show that the population variance is Var[Y] = 𝜇2𝜑.
(c) Consider the population coefficient of variation,
√
Var[Y]∕𝜇, defined in
Section 2.3.7. Show that this is constant with respect to 𝜇.
(d) Can you find another distribution whose coefficient of variation is constant with
respect to its mean?
8.17
Return to the ER cost data in Example 8.3.5 and verify the skew in the data: plot his-
tograms for each subsample’s total expenditures. (Use Scott’s normal reference rule
from Section 4.1.4 for bin selection.) Include rug plots and overlay a kernel density
estimator on each histogram.
8.18
Similar to the ER cost data in Example 8.3.5, the US MEPS also sampled total
nonzero expenditures (in $) with n = 3272 in-patient hospital stays for calendar year
2009; see http://meps.ahrq.gov/mepsweb/. Almost half (46.7%) of the stays were
preceded by an ER visit which then resulted in hospital admission. The rest were
direct admissions. Using this as a stratifying marker produces another two-sample
data set, a portion of which is given as follows (the complete data are available online
at http://www.wiley.com/go/piegorsch/data_analytics):
Route of admission
Total expenditure ($)
ER admission
12.47, 20.03, 24.74, 29.36, … , 14 1122.60,
15 3802.80, 15 4155.70, 15 4758.80, 29 4472.60
Direct admission
5.96, 13.28, 16.10, 24.26, 25.10, … , 20 0351.20,
20 7024.10, 22 4619.70, 24 0235.40, 28 9176.20

290
STATISTICAL DATA ANALYTICS
Perform a two-sample analysis analogous to that seen in Example 8.3.5. To visualize
the variation, start with histograms, rug plots, and overlayed kernel density estimates
for each subsample’s total expenditures. Next, calculate the two subsamples’ means
and standard deviations, and from these, determine if the two coefficients of variation
are roughly equal. If so, invoke a gamma GLiM and assess if any significant difference
exists between the two admission routes. Operate at a false positive rate of 5%. What
do you conclude? Also estimate the unknown dispersion parameter, 𝜑.
8.19
The study of in-patient hospital expenditures in Exercise 8.18 also recorded x = {Num-
ber of nights} associated with each in-patient stay in 2009. The paired data are are avail-
able online at http://www.wiley.com/go/piegorsch/data_analytics; a sample is given
follows:
x = Number of nights:
3
1
· · ·
20
5
Y = Expenditure ($):
5041.81
12 748.45
· · ·
20 563.31
9556.04
(a) Plot Y against x. Comment on the observed pattern.
(b) Fit a gamma GLiM using Y as the response and x as a single quantitative predictor.
Construct an analysis of deviance table and test if x has a significant impact on the
mean response. Operate at a false positive rate of 1%. Also find an estimate of the
unknown dispersion parameter, 𝜑.
(c) Calculate a 99% profile likelihood confidence interval on the regression parameter
𝛽1 associated with x. Comment on the statistical inference being produced.
(d) Determine the predicted response ̂𝜇(x) and overlay the predicted curve on your
scatterplot in Exercise 8.19a. Comment further on the pattern(s) observed.
(e) Find the standardized deviance residuals di∕
√
1 −hii from the model fit and
plot these against x. (Hint: In R, use the rstandard() function with option
type=‘deviance’.) Do you see a possible outlier? If so, excise it and reanalyze
the remaining data with the gamma GLiM. Include a new (standardized) residual
analysis. What changes, if any, are evident?

9
Supervised learning:
classification
Along with the regression-based approaches seen in Chapters 6–8, another important form of
supervised learning involves methods of classification. From a broad perspective, regression
and classification are similar: both employ a set of p inputs quantified as a vector of predictor or
feature variables Xi = [Xi1 Xi2 · · · Xip]T to explain the value of an outcome variable Yi. For the
classification setting, however, Yi represents a series of discrete categories, {C1, C2, … , CQ},
exclusively and exhaustively defining the possible states within which the ith subject or ele-
ment lies. For example, in a cancer classification study, the Cqs could be different stages or
types of cancer identified in an ith patient’s tissue sample and Xi could represent microarray
expression levels from p different genes that potentially relate to cancer status. The goal is to
determine an accurate classification scheme for stages of the cancer based on the various pre-
dictor inputs (cf. Exercise 9.3). It is prior known specification of the Cq categories that makes
this a ‘supervised’ effort. (When no predefined categories exist, the process is ‘unsupervised’
and may appear as a form of cluster analysis, as described in Chapter 11.)
This chapter gives an overview of some standard methods for supervised classification. In
most cases, the topics are mature enough to demand their own standalone chapters; hence, the
descriptions in this single chapter are intended only as an introduction. Selected references
are provided throughout for readers interested in pursuing the methods in more detail.
The n observed data pairings (Xi, Yi) are often viewed as a training set of data from which
the classification scheme is estimated. The given scheme may then be applied to a separate
test set, where independent values of Xi are sequestered or collected with the aim of predicting
their associated categories Yi. In contrast to the regression setting, however, the statistical goals
in classification may be more exploratory and less inferential in nature. This then follows the
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

292
STATISTICAL DATA ANALYTICS
general strategy of an exploratory data analysis (EDA) often seen in statistical learning (cf.
Section 1.3).
It is useful to differentiate the exercise into two slightly different forms: discrimination
(or discriminant analysis) and focused classification. The former is more traditional, where
interest centers on identifying patterns and structure in the data by discriminating between
separable classes. The latter is more prevalent when the focus is on prediction of the class or
category within which a new test case may lie. In many settings, the distinction between the
two is ambiguous, however, and the terms are increasingly applied interchangeably.
Discriminant analyses are sometimes conducted by calculating discriminant scores,
denoted generically as 𝛿q(X). The scores are used to quantify the association between X and
category or class Cq. Usually, a high value of 𝛿q(X) favors inclusion in Cq. Indeed, it can
be convenient for the score vector [𝛿1(X) · · · 𝛿Q(X)]T to itself represent the class inclusion
probabilities. Then, we assign a datum X to the class for which it has the highest probability
of inclusion (assuming the Q classes are afforded equal a priori weight or cost).
If the inputs exhibit stochastic variation, the vector X is assigned a distributional structure,
for example, via some p-variate joint probability function f(x). When this stochastic feature is
not required, any statistical inferences are constructed conditionally on the observed pattern
of the Xis.
9.1
Binary classification via logistic regression
In the simplest form of supervised classification, only two possible categories exist, so Q = 2.
This is the simple binary classifier/discriminant problem. The outcome variable is taken as
Yi = 1 when subjects populate the target, ‘success’ category C1, and as Yi = 0 if they populate
the remaining reference or ‘background’ category C2. (If no target or background categories
exist, then we arbitrarily choose either category as the ‘success’ and delineate it consistently
so throughout the training set.)
9.1.1
Logistic discriminants
Binary outcomes lead naturally to consideration of a binomial parent distribution for Y.
Assume each observation represents an independent Bernoulli trial such that Yi ∼indep.
Bin(1, 𝜋i). Here 𝜋i = 𝜋(xi) models the success probability as a function of xi, conditional
on the observed realizations of the predictor values Xi = xi (i = 1, … , n). In effect, 𝜋(x)
operates as a potential score function for classification purposes. From Section 8.3.1, the
ubiquitous logistic function is then a popular choice for 𝜋(⋅):
𝜋(xi) = 𝜋(xi1, … , xip) =
1
1 + e−𝜂i ,
where, as in Chapter 8,
𝜂i = 𝛽0 + 𝛽1xi1 + · · · + 𝛽pxip
denotes the linear predictor. The conditional probability of category residence is modeled via
P[Yi = 1 | Xi = xi] =
1
1 + exp{−𝛽0 −𝛽1xi1 −· · · −𝛽pxip} ,
(9.1)
a form of logistic regression for Yi.

SUPERVISED LEARNING: CLASSIFICATION
293
Given data, estimation proceeds via the iteratively (re)weighted least squares (IWLS)/
maximum likelihood (ML) algorithm from Section 8.2.1, producing a maximum likelihood
estimate (MLE), ̂𝜷= [ ̂𝛽0 · · · ̂𝛽p]T, for the vector of regression coefficients. Appealing to ML
invariance (Section 5.2.4), the ̂𝛽qs are substituted into the corresponding expressions for 𝜂i
and 𝜋(xi) to yield MLEs for the ith linear predictor and success probability, respectively.
When predictive classification is the goal, the estimator ̂𝜋(x) provides a sense of how com-
mon or likely the ‘success’ category is to be populated by a new/test observation with feature
vector x. Indeed, formal inferences are possible: a (pointwise) conditional, large-sample, 1 −𝛼
confidence interval for 𝜋(x) starts with the Wald interval on 𝜂: for any given x = [x1 · · · xp]T,
find ̂𝜂= ̂𝛽0 + ̂𝛽1x1 + · · · + ̂𝛽pxp along with its standard error se[̂𝜂]. Take z𝛼∕2 as the upper- 𝛼
2
standard normal critical point, and form the 1 −𝛼Wald confidence interval for 𝜂as
̂𝜂± z𝛼∕2se[̂𝜂]. Then, apply this in the logistic model to produce the interval for 𝜋(x):
1
1 + exp{−̂𝜂∓z𝛼∕2se[̂𝜂]} .
(9.2)
Notice that se[̂𝜂] is the square root of the (estimated) variance of ̂𝜂. This involves a sum of
the variances and covariances of the ̂𝛽js, as in (2.16), which are found from the estimated
covariance matrix for ̂𝜷. These are usually taken from the output of a logistic regression
program. One could alternatively find se[ ̂𝜋(x)] directly and then report the Wald interval
̂𝜋(x) ± z𝛼∕2se[ ̂𝜋(x)]; however, this is not guaranteed to lie wholly within the unit interval,
unlike (9.2), and is, therefore, less preferred.
For discriminant calculations, the logistic regression estimators can be used to character-
ize separation between the two categories. Given a feature input x, the discriminant function
favors Y = 1 when ̂𝜋(x) ≥1
2, and Y = 0 otherwise (again, assuming the two categories are
assigned equal a priori weight or cost). Under the conditional logistic model, this simplifies to
̂𝜂= ̂𝛽0 + ̂𝛽1x1 + · · · + ̂𝛽pxp ≥0.
(9.3)
When the feature inputs are all of first order, (9.3) corresponds to a linear discriminant func-
tion for the boundaries between categories. Higher-order terms such as quadratic predictors
or interactions – for example, x2
i1 or xi1xi2, respectively – lead to curvilinear discriminants.
Graphing these relationships is often a useful way to visualize the discriminant outcome.
Example 9.1.1 Wholesale distribution channel study. Cardoso (2013) described a busi-
ness study of the client base for a Portuguese wholesale food distributor. The database includes
information on annual client spending (given as generic monetary units, m.u.) on food prod-
ucts, and the distribution channel – Retail versus Service (Hotel/Restaurant/Cafe) – in which
the client resides. For the purposes of illustration, we work here with a subset of the data,
restricting attention to the cohort of clients outside of the two largest Portuguese cities of
Lisbon and Porto. This gives a data set containing n = 316 client records.
Of interest is discrimination between the two classes of distribution channels the clients
occupy and in particular, whether (Y = 1) or not (Y = 0) a client’s channel is Retail. Focus on
two key annual spending markers: (i) fresh food products and (ii) grocery products. Because
spending outcomes are notoriously skewed, use the natural logarithms of these inputs:
X1 = log{Fresh-product spending} and X2 = log{Grocery spending}. A third predictor
representing the interaction X1X2 is also calculated. Table 9.1 presents a selection of the log-
transformed data. (The original set is available at http://www.wiley.com/go/piegorsch/data
_analytics.)

294
STATISTICAL DATA ANALYTICS
Table 9.1
Selected data from a larger set of n = 316 observations recorded on distribution
channel (Retail channel: Y = 1; Service channel: Y = 0) and annual spending (in generic
monetary units, m.u.) for clients of a wholesale food distributor.
Y = Distribution channel:
1
1
1
· · ·
0
0
X1 = log{Fresh-product spending}:
9.447
8.862
8.757
· · ·
9.239
7.933
X2 = log{Grocery spending}:
8.931
9.166
8.947
· · ·
7.711
7.828
For these data, the conditional logistic model is
P[Yi = 1 | Xi1 = xi1, Xi2 = xi2] =
1
1 + exp{−𝛽0 −𝛽1xi1 −𝛽2xi2 −𝛽3xi1xi2} .
(9.4)
Similar to that seen in Section 8.3.1, sample R code for fitting this model to the data is
> X12 <- X1 * X2
> wholesale.glm <- glm( Y ˜ X1+X2+X12, family=binomial(‘logit’) )
> summary( wholesale.glm )
This produces output (edited) indicating
Coefficients:
Estimate Std. Error z value Pr(>|z|)
(Intercept)
-109.8416
32.2516
-3.406 0.000660
X1
8.7513
3.3309
2.627 0.008606
X2
12.2642
3.5842
3.422 0.000622
X12
-0.9805
0.3704
-2.647 0.008123
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 401.81
on 315
degrees of freedom
Residual deviance: 174.50
on 312
degrees of freedom
from which the estimated discriminant function is found to be
̂𝜂(x1, x2) = −109.8416 + 8.7513x1 + 12.2642x2 −0.9805x1x2.
This is a curve in (x1, x2)-space providing a discriminant boundary between Retail clients
(above the curve) and Service clients (below the curve). Figure 9.1 plots the actual points
from this sample (solid circles indicate Retail clients, open circles Service clients), along
with the curvilinear discriminant. The graph suggests that higher annual spending tends to
associate with the Retail distribution channel, particularly along the grocery metric. This is
useful knowledge discovery for a distributor wishing to identify client spending patterns. The
discriminant curve also appears to do a reasonable job of separating the two classes: more
retail clients tend to lie above the curve – as desired – and more service clients tend to lie
below it. (Also see Section 9.1.2.)
Although not as crucial in this discriminant exercise, we can also estimate the probabil-
ity that a new client spending x1 log-m.u. on fresh foods and x2 log-m.u. on groceries would
classify into the Retail channel. To do so, simply substitute the estimated regression coeffi-
cients into (9.4). For example, to estimate the Retail channel probability at x1 = log(13 500)
and x2 = log(11 000), use the sample R code

SUPERVISED LEARNING: CLASSIFICATION
295
2
4
6
8
10
12
2
4
6
8
10
log(Fresh-product spending)
log(Grocery spending)
Figure
9.1
Observed
(x1 = log{Fresh-product spending}, x2 = log{Grocery spending})
pairs for wholesale food clients in Example 9.1.1 indicating retail clients (solid circles) and
service clients (open circles). Solid curve gives discriminant function from logistic regression
model containing predictors x1, x2, and an x1x2 interaction. Source: Data from Cardoso
(2013).
> new.df <- data.frame( X1=log(13500), X2=log(11000),
X12=log(13500)*log(11000) )
> p.new <- predict( wholesale.glm, type=‘response’, newdata=new.df )
> round( p.new, digits=4 )
This gives P[Y = 1 | X1 = log(13 500), X2 = log(11 000)] = 0.6765 (output not shown), or
slightly more than 2
3. As the estimate exceeds 1
2, the putative client would be classified into
the Retail channel. For a conditional 95% large-sample confidence interval, appeal to (9.2) via
> z025 <- qnorm( 0.025, lower=F )
> eta.new <- predict( wholesale.glm, type=‘link’, newdata=new.df,
se.fit=T)
> p.lower <- 1/( 1 + exp(-(eta.new$fit - z025*eta.new$se.fit)) )
> p.upper <- 1/( 1 + exp(-(eta.new$fit + z025*eta.new$se.fit)) )
> round( cbind(p.lower, p.new, p.upper), digits=4 )
to find
p.lower
p.new
p.upper
[1]
0.5657
0.6765
0.7705
that is, 56.57% < P[Y = 1 | X1 = log(13 500), X2 = log(11 000)] < 77.05%.
◽
As a discriminant/classification routine, logistic regression exhibits the typical flexibility
associated with regression models. It often works well in practice; however, it can destabilize

296
STATISTICAL DATA ANALYTICS
if the categories are widely separated. Extensions of the basic logistic model to accommo-
date multiple categories with Q > 2 are also possible; see, for example, Hastie et al. (2009,
Section 4.4).
9.1.2
Discriminant rule accuracy
Readers may notice in Figure 9.1 that not all the retail clients (solid circles) lie above the
decision boundary and that not all the service clients (open circles) lie below it. This is a
realization of misclassification error. One of the goals in predictive analytics is minimization
of misclassification error to as propitious an extent as possible.
To formalize this, consider the two-category setting (Q = 2) and assume a discrimi-
nant/classification rule has been applied to a training set of n observations. Let the number
of correct classifications into category Cq be 𝜈qq (q = 1, 2) and the number of category Cm
elements misclassified into category Cq be 𝜈qm (q ≠m). These counts can be arranged in
a 2 × 2 cross-classification, similar to the 2 × 2 structure in Table 8.5. Here, however, the
table’s counts represent the correct and incorrect classifications in the training data. Table 9.2
displays a schematic version, colorfully known as a confusion matrix.
Table 9.2
A 2×2 confusion matrix from classification rule
outcomes with Q = 2 categories.
True category
C1
C2
Row total
C1
𝜈11
𝜈12
𝜈1+
Prediction
C2
𝜈21
𝜈22
𝜈2+
Column total
𝜈+1
𝜈+2
n
A number of useful predictive measures can be derived from the confusion matrix. The
correct classification rate, or accuracy, is obviously (𝜈11 + 𝜈22)∕n, also known in statistical
parlance as the concordance of the 2 × 2 table. Conversely, the misclassification (error) rate
is (𝜈12 + 𝜈21)∕n.
An additional set of measures often seen in biomedical screening and similar applications
are the sensitivity 𝛾1 = 𝜈11∕𝜈+1 and specificity 𝛾2 = 𝜈22∕𝜈+2, that is, the correct proportion of
C1 predictions and C2 predictions, respectively. (These are sometimes called the true posi-
tive and true negative rates, respectively, when applied in a formal screening setting. Their
complements then have connections to the false positive and false negative rates, respectively,
from hypothesis testing in Section 5.4.)
Example 9.1.2 Wholesale distribution channel study (Example 9.1.1, continued).
For
the wholesale channel study in Example 9.1.1, the confusion matrix can be computed by
counting the numbers of clients that fall into each of the four categories in Table 9.2. Doing
so produces the result in Table 9.3.
We see that the logistic discriminant’s overall accuracy is (84 + 193)/316 = 87.7%, while
its overall misclassification error is (18 + 21)/316 = 12.3%. Sensitivity is good: 𝛾1 = 84/105 =
80%, although specificity is higher: 𝛾2 = 193/211 = 91.5%. The rule picks up Service channel

SUPERVISED LEARNING: CLASSIFICATION
297
Table 9.3
A 2 × 2 confusion matrix from logistic discriminant rule
outcomes in Examples 9.1.1 and 9.1.2.
True category
Retail channel
Service channel
Row total
Retail channel
84
18
102
Prediction
Service channel
21
193
214
Column total
105
211
316
clients slightly better than it predicts Retail channel clients. Indeed, it is not unusual for classi-
fication rules to make implicit trade-offs in sensitivity for specificity, or vice versa, depending
on the structure of the observed response categories in the population.
◽
9.1.3
ROC curves
A popular way to visualize the interrelationship between sensitivity and specificity is known
as a receiver operating characteristic (ROC) curve (Pepe 2003, Chapter 4). (The curious name
stems from the method’s origins in radio signal detection during World War II.) The curve is
a plot of 𝛾1 against 1 −𝛾2 within the unit square. A ‘perfect’ ROC curve will rise up the 𝛾1
sensitivity axis from the origin to 𝛾1 = 1, then move horizontally across to 1 −𝛾2 = 1. The
area under this perfect curve will be 1. In practice, however, sample ROC curves do not fill
the unit square. Instead, they take a concave shape over a series of upwards steps connecting
the square’s lower left and upper right corners. (Some programs offer options to smooth the
curve.) A 45∘line connecting those reference corners is often included; below this line, the
ROC curve is substandard. Satisfactory performance is indicated by an area under the curve
(AUC) well above 1
2 and preferably close to 1.
External R packages that can construct ROC curves include pROC and ROCR. Figure 9.2
gives a sample ROC curve from pROC, using the wholesale channel data in Example 9.1.2.
The corresponding AUC is 0.708.
The ROC curve and its AUC are sometimes applied as classification rule diagnostics;
however, they can suffer from selected irregularities. Concerns over this, and a pertinent alter-
native, are detailed by Hand (2009a). Indeed, a variety of measures are available to quantify
association in a 2 × 2 confusion matrix (Myatt and Johnson 2009, Section 4.1.5), although the
ways in which these actually represent a rule’s performance can vary dramatically. Analysts
must apply careful attention when employing and interpreting any measure of classification
fidelity (Hand 2012). While simplistic, the overall accuracy and misclassification error given
above are often useful starting points and will be the focus throughout this chapter.
9.2
Linear discriminant analysis (LDA)
9.2.1
Linear discriminant functions
Linear discriminant functions such as (9.3) can be fashioned in a variety of ways. A clas-
sic formulation is due to Fisher (1936). He developed a linear combination of the feature

298
STATISTICAL DATA ANALYTICS
1 − γ2
γ1
0.0
0.2
0.4
0.6
0.8
1.0
1.0
0.8
0.6
0.4
0.2
0.0
Figure 9.2
Sample ROC curve from logistic classification rule in Examples 9.1.1–9.1.2.
Source: Data from Cardoso (2013).
variables, say,
𝜁(X) = aTX = a1X1 + a2X2 + · · · + apXp,
(9.5)
where a = [a1 · · · ap]T. The ajs are chosen to maximize the ratio of between-category vari-
ance to within-category variance. This is a natural objective: maximize ‘signal’ relative to
‘noise’ across the differential categories. Fisher showed that the target quantity becomes a
ratio of quadratic forms aTBa∕aTWa, where B is the between-category covariance matrix
and W is the within-category covariance matrix.
In particular, one can calculate B and W from their component vectors and matrices. Let G
be a category indicator matrix with elements giq = I{q}(Yi) identifying whether the ith subject
resides in category Cq. Here I(Y) is the indicator function from (2.20). Also, let M be the
matrix of category means with elements
̂𝜇qj = 1
Nq
n
∑
i=1
giqXij ,
where Nq = ∑n
i=1 giq is the qth category’s sample size (q = 1, … , Q). Lastly, let X =
[X+1 X+2 · · · X+p]T be the vector of per-variable means X+j = ∑n
i=1 Xij∕n. With these, write
the full predictor matrix as
𝚵=
⎡
⎢
⎢
⎢
⎢
⎢⎣
X11
X12
· · ·
X1p
X21
X22
· · ·
X2p
⋮
⋮
⋱
⋮
Xn1
Xn2
· · ·
Xnp
⎤
⎥
⎥
⎥
⎥
⎥⎦
=
⎡
⎢
⎢
⎢
⎢
⎢⎣
XT
1
XT
2
⋮
XT
n
⎤
⎥
⎥
⎥
⎥
⎥⎦
.

SUPERVISED LEARNING: CLASSIFICATION
299
Then
W = (𝚵−GM)T(𝚵−GM)
n −Q
and
B = (GM −hX
T)T(GM −hX
T)
Q −1
,
where h = [1 1 · · · 1]T is an n × 1 column vector made up entirely of ones.
One can show, analogous to the sums of squares in an analysis of variance (ANOVA) table
(Section 7.5), that the sum (n −Q)W + (Q −1)B adds to a scaled matrix of ‘total’ variation
which is independent of Q. But because this sum is constant with respect to Q, maximizing
aTBa∕aTWa becomes, essentially, a constrained optimization problem. Appeal is then made
to an optimization method known as Lagrange multipliers (Hughes-Hallett et al. 2013, Section
15.3). The solution employs the eigenvalues and eigenvectors of the sample covariance matri-
ces and is built into various discriminant-rule computer programs; see the following text. (The
technical details exceed the scope here; interested readers may refer, for example. to Clarke
et al. (2009, Section 5.2.1). For a refresher on eigenanalysis, see Section A.5.)
For the simple two-category case with Q = 2, the optimal solution sets a = W−1( ̂𝝁1 −̂𝝁2),
where ̂𝝁q is the mean vector for the qth group (the qth row of M taken as a column vector).
Fisher’s linear discriminant function 𝜁(X) = aTX is then designed to classify an observation
with discriminant score 𝜁(X) to category q = 1 if 𝜁(X) > 1
2(𝜁1 + 𝜁2), where 𝜁q is the mean
discriminant score for category q and the categories have been organized such that 𝜁1 > 𝜁2,
assuming equal prior probability of allocation to both groups (Everitt 2005, Section 7.2.2).
Example 9.2.1 Bank depositor marketing study. Moro et al. (2011) described a southern
European bank’s marketing campaign for subscribing customers to a new term deposit. Their
database includes information on customer characteristics (education, age, loan status, etc.)
as related to likelihood of subscribing to the new deposit vehicle. For purposes of illustration,
we work here with a subset of the data, focusing on the cohort of married bank customers
who had completed secondary school and had not been contacted by the bank regarding any
previous deposit campaigns. This gives a data set containing n = 1172 customer records.
Of interest is characterization of the eventual category decisions, Y = q, made by these
customers, regarding whether (q = 1) or not (q = 2) each subscribed to the new deposit vehi-
cle. Two predictor variables were found to be important in predicting subscriber success for
this cohort, X1 = {Age of depositor} and X2 = log{Duration of marketing phone call (in s)}.
A selection of the data appear in Table 9.4. (The complete set is available at http://www.wiley
.com/go/piegorsch/data_analytics.)
Table 9.4
Selected data from a larger set of n = 1172 observations on bank customer
subscription classifications (subscribed: Y = 1; unsubscribed: Y = 2) after a marketing
campaign.
Y = Subscription class:
2
2
2
· · ·
2
1
X1 = Age (years):
59
39
39
· · ·
33
56
X2 = log{Duration (seconds)}:
5.4205
5.0173
5.6095
· · ·
5.7236
7.1412
Source: Moro et al. (2011).

300
STATISTICAL DATA ANALYTICS
For purposes of discriminating between subscribers and nonsubscribers, consider here
application of Fisher’s linear discriminant analysis (LDA). To find the linear discriminant
function in (9.5), we can appeal to R’s lda() function from the external MASS package.
Assume equal prior weights for both categories. Sample code is
> Bank.lda <- lda( Y ˜ X1+X2, prior=c(0.5,0.5) )
> print( Bank.lda )
Notice the specification of equal prior weights via the prior= option. This produces the
summarizing output (edited)
Call:
lda(Y ˜ X1 + X2, prior = c(0.5, 0.5))
Prior probabilities of groups:
1
2
0.5 0.5
Group means:
X1
X2
1 44.4800 6.223444
2 42.1103 5.113879
Coefficients of linear discriminants:
LD1
X1 -0.02172443
X2 -1.12114596
The listed Group means are the per-variable category means ̂𝜇qj (q = 1, 2; j = 1, 2). The
linear discriminant function is given by 𝜁(X1, X2) = a1X1 + a2X2, where the ajs are found
under ‘Coefficients of linear discriminants.’ They are also available for direct
computation as Bank.lda$scaling. Here, 𝜁(X1, X2) = −0.0217X1 −1.1211X2.
To construct the classification rule, find the category indicator matrix G via, for example,
> G <- as.matrix( cbind(2-Y, Y-1) )
and from this, calculate the discriminant means 𝜁q:
> a <- Bank.lda$scaling
> zeta <- a[1]*X1 + a[2]*X2
> zeta1bar <- sum( zeta*G[,1] )/sum( G[,1] )
> zeta2bar <- sum( zeta*G[,2] )/sum( G[,2] )
This yields 𝜁1 = −7.9437 and 𝜁2 = −6.6482 (output not shown). As 𝜁1 < 𝜁2, the linear
discriminant rule predicts a depositor will subscribe (into category q = 1) when
𝜁(X1, X2) = −0.0217X1 −1.1211X2 < 1
2(𝜁1 + 𝜁2) = −6.6482 −7.9437
2
= −7.2960.
A plot of the data coded by observed subscription outcomes (solid circles indicate new
subscribers, open circles nonsubscribers) helps to visualize the complexity of the intertwined
categories here (see Figure 9.3). In the figure, the linear discriminant decision boundary, corre-
sponding to the line −0.0217X1 −1.1211X2 = 1
2(𝜁1 + 𝜁2), is superimposed. Depositors lying

SUPERVISED LEARNING: CLASSIFICATION
301
30
40
50
60
70
80
2
3
4
5
6
7
Age (years)
log(Duration)
Figure 9.3
Observed (X1 = Age, X2 = log {Duration}) pairs for bank depositors in Example
9.2.1 indicating subscribers (solid circles) and nonsubscribers (open circles) to new deposit
vehicle. Solid line gives linear discriminant decision boundary. New depositor with X1 = 45
and X2 = log(900) = 6.8024 marked by an × (see arrow). Source: Data from Moro et al.
(2011).
above the boundary are predicted into category C1 (subscribers); those below are predicted
into category C2 (nonsubscribers).
The pattern in Figure 9.3 provides useful knowledge discovery. Apparently, longer-
duration marketing calls are required to predict successful new subscriptions with younger
depositors: calls of duration shorter than about 7 min (i.e., log{x2} < log{420} = 6.04)
produce few successes. Subscriptions appear slightly more likely with older depositors over
shorter durations, although with very senior customers, the discriminant function may have
limited predictive value due to the small number of observations in that upper age range.
In any case, a substantial portion of the feature space in Figure 9.3 is below the discrimi-
nant line and, therefore, allocated to nonsubscribers. Indeed, most depositors did not respond
successfully to the marketing campaign, illustrating the limited success rate of the marketing
effort.
The confusion matrix for this linear discriminant rule is given in Table 9.5. We see that the
discriminant’s overall accuracy is (59 + 811)/1172 = 74.2%, while its misclassification error
is (286 + 16)/1172 = 25.8%. Sensitivity is moderate at 𝛾1 = 59/75 = 78.7%, as is specificity at
𝛾2 = 811/1097 = 73.9%.
To predict the category of, say, a new 45-year old depositor contacted for 15 min
(900 s), calculate the linear discriminant as 𝜁(X1, X2) = −(0.0217)(45) −(1.121)(log{900})
= −8.604. As this is below −7.296, predict that this customer will indeed subscribe to the
new deposit product.

302
STATISTICAL DATA ANALYTICS
Table 9.5
The 2 × 2 confusion matrix from linear discriminant rule
outcomes in Example 9.2.1.
True category
Subscriber
Nonsubscriber
Row total
Subscriber
59
286
345
Prediction
Nonsubscriber
16
811
827
Column total
75
1097
1172
In R, the lda() function can perform these various calculations internally and report the
category prediction via the predict() function. For instance, the sample command
> predict( Bank.lda, newdata=data.frame(X1=45, X2=log(900)) )$class
gives the class prediction as
[1] 1
Levels: 1 2
That is, the predicted category membership for a depositor with X1 = 45 and X2 = log(900)
has label 1, corresponding to subscription category C1, as expected. In Figure 9.3, the point
(marked by an ×) lies clearly above the decision boundary.
◽
In high-dimensional problems with large p, it may be of interest to conduct the supervised
classification on a reduced subset of predictors. Similar to the sparse logistic regression
approach mentioned in Section 8.3.1, a type of sparse discriminant analysis may be applied to
address this issue (Clemmensen et al. 2011). The method embeds a regularizing L1 penalty into
a set of discriminant scoring criteria, harkening back to the Lasso approach of Section 7.4.3.
As a result, the discrimination effort is combined with a form of variable selection/
feature selection; this can greatly improve the discriminant outcomes in high-dimensional
classification.
For more on Fisher’s linear discriminant function, see Everitt (2005, Section 7.2),
Izenman (2008, Chapter 8), or the larger treatment in Huberty and Olejnik (2006).
9.2.2
Bayes discriminant/classification rules
If the analyst is able or willing to assign probability densities to the input variables in X,
discriminant functions can be built from the probability models that ensue. For simplicity, it
was assumed above that any prior interest in the categories was constant. Now, suppose that the
qth category is assigned a formal prior probability, 𝜋q, containing an observed predictor vector
X = [X1 X2 · · · Xp]T, where ∑Q
q=1 𝜋q = 1. Suppose further that a probability density function
(p.d.f.) f(x | Y = q) is modeled to describe the probability that X classifies in category Cq for
each q = 1, … , Q. Then from Bayes’ rule in (2.4), the posterior probability of observing an
element in category Cq, given the vector X = x, is
P[Y = q | X = x] =
f(x | Y = q) 𝜋q
f(x)
,
(9.6)

SUPERVISED LEARNING: CLASSIFICATION
303
where f(x) = ∑Q
q=1 f(x | Y = q) 𝜋q is the marginal p.d.f. of X. Clearly, f(x) is independent of
q and so (9.6) is often expressed simply as the proportional relationship P[Y = q | X = x] ∝
f(x | Y = q) 𝜋q.
With this, the Bayes classification rule predicts a future or test-case observation X will lie
in that category Cq′ whose P[Y = q′ | X = x] is largest among all Q posterior probabilities.
In this form, Bayes discriminant rules can be shown to minimize the expected number of
misclassifications they induce (Clarke et al. 2009, Section 5.2.2) and, hence, serve as a form
of optimal classifier in supervised learning.
In effect, the posterior probabilities from (9.6) partition the feature space into Q disjoint
regions, each with highest classification probability in that region. For example, when com-
paring two categories Cq and Cm, X is classified into category Cq when P[Y = q | X = x] >
P[Y = m | X = x]. That is, when
log f(x | Y = q)
f(x | Y = m) + log
𝜋q
𝜋m
> 0.
(9.7)
Of course, this assumes that the cost of misclassification is essentially equal between the two
categories. If this is not the case – say, the domain expert indicates that misclassifying into Cq
is twice as costly as misclassifying into Cm – one can incorporate a cost parameter and modify
the decision rule accordingly; cf. James et al. (2013, Section 4.4.3).
For many specifications of f(x | Y = q), implementation of the Bayes rule can be prob-
lematic in practice, especially with high-dimensional/multicategory classifications. One case
where it simplifies into a convenient form, however, is under a normal (Gaussian) assumption.
This is discussed in the next section.
9.2.3
Bayesian classification with normal data
Suppose the analyst is willing to adopt a normal (Gaussian) assumption on the input variables.
For instance, with the multivariate normal p.d.f. from (2.40), we have
f(x | Y = q) =
1
(2𝜋)n∕2|V|1∕2 exp
{
−1
2(x −𝝁q)TV−1(x −𝝁q)
}
,
where 𝝁q = E[X | Y = q] and V = Var[X] is the covariance matrix of X. Notice here that
V is assumed constant across all categories and is, therefore, independent of q. Under this
multivariate normal assumption, the first term in (9.7) simplifies to
log f(x | Y = q)
f(x | Y = m) = (𝝁q −𝝁m)TV−1x −
𝝁T
qV−1𝝁q −𝝁T
mV−1𝝁m
2
.
(9.8)
This has the form 𝛂Tx −𝜃, where the constant 𝜃is independent of x. Thus this Gaussian
discriminant rule is, once again, a linear (in x) decision boundary for discriminating between
Cq and Cm.
In practice, the unknown quantities in (9.8) are estimated from the training data: use ̂𝝁q
for 𝝁q, the within-group covariance matrix W for V, and unless some known prior specifica-
tion is available, ̂𝜋q = Nq∕n for 𝜋q. Then, the estimator of the coefficient vector for x in the
Gaussian classifier will have the same form as Fisher’s LDA counterpart from Section 9.2.1:
a = W−1( ̂𝝁q −̂𝝁m).

304
STATISTICAL DATA ANALYTICS
This convergence is worth emphasizing: in developing Fisher’s LDA, we made no
parametric assumptions on the distribution of X. By appealing formally to the normal model
for f(x | Y = q) and applying the theoretical constructs from Bayes’ rule, however, we
recover essentially the same linear discriminator.
Applied to all Q categories, the Gaussian classifier allocates an observed X to category Cq
when
𝛿q(X) = log f(X | Y = q) + log(𝜋q)
is a maximum across all q = 1, … , Q. In the special case with X | {Y = q} ∼Np(𝝁q, V),
this is
𝛿q(X) = 𝜃′ + 𝝁T
qV−1X −1
2𝝁T
qV−1𝝁q + log(𝜋q),
(9.9)
where the initial constant 𝜃′ is comprised of terms independent of q and may be ignored when
maximizing across q. Replacing unknown quantities with their sample estimators produces
the Gaussian classification function
dq(X) = ̂𝝁T
qW−1X −1
2 ̂𝝁T
qW−1 ̂𝝁q + log
(Nq
n
)
,
(9.10)
a hyperplane in the feature space. (A hyperplane is a multidimensional extension of a flat
plane from three-dimensional space. It represents a flat (p −1)-dimensional subspace of the
p-dimensional space in which it resides, separating the larger space into two half-spaces.
Allowing for a nonzero intercept, a hyperplane takes the general formula a0 + a1X1 + · · · +
apXp = 0. A special case is a line in (X1, X2)-space: a0 + a1X1 + a2X2 = 0.)
Under this model, allocate X to category Cq when dq(X) is a maximum for all q. The
decision borders between any two categories Cq and Cm are the intersecting points for which
dq(X) = dm(X). As the classification functions dq(X) produce hyperplanes in X-space, these
decision boundaries will themselves be linear.
Useful graphics for visualizing the category predictions and decision boundaries are avail-
able in the partimat() and drawparti() functions from the external klaR package.
Example 9.2.2 Urban terrorism vulnerability. A study was conducted to examine
vulnerability to terrorist attacks in n = 132 of the largest cities in the United States. Data were
collected on whether each city had experienced a terrorism event over the 35-year period
1970−2004, and if so, whether any human casualties (injuries or deaths) were recorded
(Piegorsch et al. 2007). The classification scheme was defined as follows:
Category
Category outcome
Event status
C1
Y = 1
No events
C2
Y = 2
One or more noncasualty events
C3
Y = 3
One or more events with casualties
This produced Q = 3 categories in which C1 contained cities where no events were observed,
C2 contained cities where an event occurred with no casualties, and C3 contained cities where
an event occurred with casualties.

SUPERVISED LEARNING: CLASSIFICATION
305
A pair of predictor variables was used to index (i) the frequency and diversity of natural
hazards – tornados, floods, etc. – the cities had experienced and (ii) the nature and diversity
of each city’s infrastructure. The former was used as a surrogate for community experience
in responding to extreme events, while the latter quantified vulnerable characteristics of the
built urban environment such as transportation infrastructure and age of housing. Higher
index values indicated greater vulnerability to adverse outcomes. Taken on a logarithmic
scale, the index data, along with the terrorism outcome classifications Y, appear in Table 9.6.
(The X1 log-hazard scores were seen previously in Exercise 3.3. As previously, only a selec-
tion of the data is given in the table; the complete set is available at http://www.wiley.com
/go/piegorsch/data_analytics.)
Table 9.6
Selected data on terrorism events from n = 132 US cities (no event: Y = 1; non-
casualty event(s): Y = 2; event(s) with casualties: Y = 3) and vulnerability log-indices.
Urban area
Y
X1 = log {Natural hazard}
X2 = log {Infrastructure}
Albany, NY
3
1.854
1.713
Albuquerque, NM
2
1.078
0.739
Allentown/Bethlehem, PA
1
1.577
1.773
⋮
⋮
⋮
⋮
Worcester.MA
1
1.221
1.540
Youngstown, OH
1
1.641
1.871
Source: Piegorsch et al. (2007).
Assuming a bivariate normal distribution with equal category covariance matrices for the
(X1, X2) pairs, Gaussian classification functions for these data can be constructed as per (9.10).
In R, appeal again to the lda() function from the MASS package. For example, the sample
commands
> cities132.lda <- lda( Y ˜ X1+X2 )
> print( cities132.lda$prior )
list the calculated prior probabilities, ̂𝜋q = Nq∕n, as
1
2
3
0.4924242
0.2348485
0.2727273
We see that almost half (49.2%) of the cities in this study escaped any terrorism events, while
the remainder split about evenly between casualty and noncasualty events.
For visualization, the R command plot(X2 ˜ X1, pch=as.character(Y)) pro-
duces a scatterplot of the (X1, X2) pairs with their category labels. More useful, however,
may be the partimat() function from the external klaR package. For instance, the sample
commands
> require( klaR )
> partimat( factor(Y) ˜ X2+X1, imageplot=F, col.wrong=‘gray’ )
plot the (X1, X2) pairs with their category labels, overlay the linear decision boundaries (the
intersections of the classification functions), mark in gray those points misclassified in each
partitioned category, and if desired, plot the category means. Other options are also available;

306
STATISTICAL DATA ANALYTICS
3
2
1
1
1
3
1
1
1
2
2
1
2
3
1
2
3
3
3
1
2
1
1
1
1
1
3
2
3
1
1
3
1
1
1
3
1
3
1
3
1
1
1
1
2
1
1
3
1
1
1
1
2
1
1
2
2
1
1
2
1
1
2
2
1
2
2
3
1
1
3
1
13
2
2
3
1
1
2
2
3
3
3
1
3
2
3
1
2
1
3
3
3
1
3
2
2
1
1
2
2
1
2
3
2
3
1
3
3
1
1
1
2
1
2
1
1
3
1
1
1
3
1
3
3
1
2
3
3
1
1
1.0
1.5
2.0
2.5
0.0
0.5
1.0
1.5
2.0
2.5
X1
X2
Figure 9.4
Scatterplot of X2 = log {Infrastructure index} versus X1 = log {Natural hazard
index} for 132 US cities in Example 9.2.2. The plot labels indicate category membership (1 for
no terrorism event, 2 for noncasualty event, 3 for event with casualties). Grayed labels indicate
misclassifications. Solid lines give linear discriminant decision boundaries and delineate the
category structure: C1 to lower left, C2 to lower right; C3 to top. Source: Data from Piegorsch
et al. (2007).
see help(partimat). Figure 9.4 gives a version (stylized for presentation) of the result.
In the figure, the no-event category C1 locates down and to the left, corresponding to low
values of both X1 and X2. Viewing C1 as the least vulnerable category suggests, as expected,
that low values on both log-indices imply lower urban vulnerability to detrimental outcomes
from terrorism events.
By contrast, the high-vulnerability category C3 locates at the top and somewhat to the
right in the graphic. This suggests that high values of both log-indices predict this extreme
classification and also that large log-infrastructure values X2 drive the effect somewhat more
than large log-hazard values X1. The mid-vulnerability, no-casualty event category, C2, lies
primarily to the lower right in the figure and is sparsely populated: the classification schema,
at least under the normal model for these data, does not emphasize the C2 outcomes to as great
an extent.
The R command
> Yhat <- predict(cities132.lda)$class
generates (and stores in Yhat) the predicted classifications for the 132 cities in the database,
from which some intriguing knowledge discovery is possible. For instance, the location
names for the 132 metropolitan areas reside in the database variable urbName. Calling
urbName[Yhat==3] displays 23 metropolitan areas assigned to category C3; these are
predicted as high-vulnerability locations. Table 9.7 displays the predicted C3 category

SUPERVISED LEARNING: CLASSIFICATION
307
Table 9.7
US metropolitan areas in Example 9.2.2 assigned to high-vulnerability category
C3 by Gaussian classification rule, ordered by posterior probability P[Y = 3 | X1, X2].
Urban area
P[Y = 3|X1, X2]
Urban area
P[Y = 3|X1, X2]
New York, NY/Newark, NJ
0.699
New Orleans, LA
0.667
St. Louis, MO
0.523
Washington, DC
0.646
Trenton, NJ
0.511
Philadelphia, PA
0.640
Mission Viejo, CA
0.487
Norfolk/Chesapeake, VA∗
0.619
Milwaukee, WI†
0.458
Chicago, IL
0.608
Detroit/Warren, MI
0.438
Richmond, VA†
0.598
Baton Rouge, LA†
0.436
Cleveland/Akron, OH
0.575
Tampa/St. Petersburg, FL
0.435
Baltimore/Annapolis, MD†
0.574
Columbia, SC∗
0.424
Boston, MA
0.566
Charlotte, NC∗
0.422
Toledo, OH∗
0.549
San Francisco/Oakland, CA
0.413
Charleston, SC∗
0.546
Houston, TX†
0.406
All areas observed as Y = 3 unless otherwise indicated.
∗Observed with Y = 1.
† Observed with Y = 2.
members by order of their posterior probabilities P[Y = 3 | X1, X2] from (9.6) found in R
using predict(cities132.lda)$posterior.
Most locations in Table 9.7 experienced previous C3 events, although a number reported
Y = 2 (noncasualty events) or even Y = 1 (no events), as identified in the table. Other inter-
esting patterns exist as well (see Exercise 9.8). Despite past outcome(s), however, a predicted,
high-vulnerability classification under this model could (or should) encourage a C3 com-
munity to study more closely its vulnerability status. For example, emergency managers in
coastal cities such as Norfolk, VA, or Charleston, SC, might consider new or updated forms
of shoreline antiterrorist protection, because their C3 classification indicates high predicted
vulnerability to terrorist events at the community level. Similar sorts of targeted, place-based
efforts would be pertinent for the other locations in Table 9.7 as well.
◽
Gaussian classification as introduced here can be effective when the data satisfy the nor-
mal parent supposition, at least to a good approximation. It also extends easily to multiclass
settings where Q > 2, as seen in Example 9.2.2. The method is often criticized, however,
for its relatively inflexible linear decision boundaries, a consequence of the stringent model
assumptions. It can also be detrimentally affected by large outliers. When there are only Q = 2
categories under study and for nonnormal X-variables with sufficiently large sample sizes, the
logistic approach in Section 9.1 often proves more adaptable (Press and Wilson 1978).
When normality is valid, some flexibility can be achieved by relaxing the supposition
of equal covariance matrices in the multivariate normal p.d.f.s. The resulting classification
functions will involve quantities of the form 1
2(X −̂𝝁q)TW−1
q (X −̂𝝁q). These now contain
quadratic terms in X that vary with q, creating curvilinear decision boundaries. The result is a
type of quadratic discriminant analysis (QDA), which can add a level of pliancy to the clas-
sification process (James et al. 2013, Section 4.4.4). QDA comes with a concomitant increase
in complexity, however, as each category’s covariance matrix must now be estimated. The
qda() function in the MASS package proves useful in this case.

308
STATISTICAL DATA ANALYTICS
9.2.4
Naïve Bayes classifiers
When modeling the posterior probability via (9.6), the analyst might assume that the individual
Xj variables are independent. If so, the joint p.d.f. of X simplifies using the Multiplication
Rule as in (2.11). This gives
P[Y = q | X = x] =
𝜋q
p∏
j=1
fj(xj | Y = q)
f(x)
,
where the fj(xj | Y = q) terms are the individual, independent p.d.f.s and f(x) is again the
marginal p.d.f. of X.
The independence assumption further simplifies the model: estimation of unknown param-
eters in the posterior probabilities now involves a series of univariate operations, rather than
a more complicated, multivariate calculation. And, there is no need to estimate any of the
covariances because these are all assumed to be zero. The method can also be easier to apply
when some of the Xj variables are discrete.
Whether or not the independence assumption is valid in practice is usually open to ques-
tion, however. Perhaps as a result, this approach is known as Naïve Bayes classification or
sometimes more descriptively Idiot’s Bayes classification. Despite the obvious caution such
nomenclature implies, naïve Bayes classifiers can prove useful. Their simplicity makes them
easier to implement and interpret, particularly if the dimensionality of the problem is very
large. Indeed, selected studies have shown that the ‘naïve’ independence model can produce
competitive classification procedures in certain cases (Hand and Yu 2001); a popular appli-
cation is with spam filtering for Internet communications (Seewald 2007). For an instructive
introduction, see Hand (2009b).
9.3
k-Nearest neighbor classifiers
The methods in the previous sections often rely on parametric model assumptions to under-
take the discrimination/classification exercise. When such reliance is felt to be a concern,
nonparametric techniques can be applied instead. Perhaps the simplest of these involves the
k-nearest neighbor (k-NN) approach, a concept reaching back to the work of Fix and Hodges
(1951). Suppose a set of training data contains n feature vectors, Xi, each unambiguously
identified with one of the Q categories under study. Given a new or test feature vector Xo, the
k-NN method finds those k > 1 observations in the training set that lie nearest to Xo in the
p-dimensional feature space. It then classifies Xo into that category represented most often
among the k neighbors (a majority voting scheme).
To avoid ties, k is usually assumed odd; if any ties still occur, these are broken at random.
Increasing k can also help to avoid ties; a possible suggestion along this vein is to set k as
the smallest odd integer greater than Q (why?). This comes with subsequent compromises,
however. For instance, large values of k can increase bias in the classification rule and also
heighten the computing burden. On the other hand, the increased bias often associates with
decreased variance; a ‘bias-variance trade-off’ that may be to the analyst’s advantage (Hastie
et al. 2009, Section 7.3). In a certain sense, k acts as a tuning parameter that modulates the
amount of this trade-off.

SUPERVISED LEARNING: CLASSIFICATION
309
The method’s simplicity belies its computational needs; very large training sets can push
the calculations to their proverbial limits. So too can high-dimensional feature spaces, that is,
including many X-variables. The issue is similar to the ‘curse of dimensionality’ mentioned
in Section 7.4.1: in high dimensions, distances between feature vectors can become sparse,
making detection of classifying relationships much more difficult.
Along with selection of k, the analyst must also specify a metric to define ‘distance’ in
the feature space. Most common is Euclidean distance, that is, the usual ‘as-the-crow-flies’
measure. Table 9.8 lists this along with a variety of other popular choices. Notice that the
Minkowski form is actually a general class of metrics, characterized by its positive parameter
𝛾. Special cases include Euclidian (𝛾= 2), Manhattan (𝛾= 1, sometimes called ‘Hamming’
distance), and Maximum (𝛾→∞) distances. For Canberra distance, terms with zero numer-
ator and denominator are omitted from the sum; Canberra distance is often intended for use
with nonnegative counts.
Table 9.8
Selected metrics for defining ‘distance’ between two vec-
tors, Xo = [Xo1
Xo2 · · · Xop]T and Xi = [Xi1
Xi2 · · · Xip]T, in a
p-dimensional feature space.
Name
Distance
Euclidean
√∑p
j=1 (Xoj −Xij)2
Manhattan (‘city block’)
∑p
j=1 |Xoj −Xij|
Maximum/Tchebychev
max j=1, … ,p {|Xoj −Xij|}
Minkowski
{∑p
j=1 |Xoj −Xij|𝛾}1∕𝛾
(𝛾> 0)
Canberra
∑p
j=1 |Xoj −Xij|∕|Xoj + Xij|
If the feature variables differ greatly in their scales of measurement and/or variances, they
are usually, if somewhat arbitrarily, divided by their standard deviations before any distance
calculations. This may help assuage problems with wildly differential scales or arbitrary units
of measurement.
Given k and the chosen distance metric, the calculations for k-NN classification are fairly
straightforward, if tedious. In R, the knn() function from the external class package is a stal-
wart choice when using Euclidean distance. Also available are the knnVCN() function from
the external knnGarden package and the gknn() function from the external scrime package,
either of which can institute any of the distance metrics in Table 9.8.
Example 9.3.1 Remote sensing of tree disease.
In a study to classify arboreal disease
progression, Johnson et al. (2013) reported data on diseased oak trees in a Japanese forest.
Recorded were the status of the trees in a high-resolution satellite image segment (Y = 1
for diseased trees, Y = 2 for other land cover), along with p = 5 predictor variables repre-
senting features of each image. A selection of the data, comprising n = 4339 training obser-
vations and no = 500 test observations, appears in Table 9.9 (download both data sets at
http://www.wiley.com/go/piegorsch/data_analytics).

310
STATISTICAL DATA ANALYTICS
Table 9.9
Selected data from a training set of n = 4339 satellite observations and a test set
of no = 500 satellite observations on oak tree disease (diseased trees: Y = 1, other land
cover: Y = 2) in a Japanese forest.
Training set
Y = Disease status
1
1
· · ·
2
X1 = Mean gray-level (pansharp. band)
120.362
124.740
· · ·
125.172
X2 = Mean green band
205.500
202.800
· · ·
559.048
X3 = Mean red band
119.395
115.333
· · ·
365.968
X4 = Mean near-infrared band
416.581
354.333
· · ·
439.272
X5 = Std. deviation (pansharp. band)
20.676
16.707
· · ·
15.392
Test set
Y0 = Disease status
1
2
· · ·
2
X01 = Mean gray-level (pansharp. band)
121.383
109.829
· · ·
119.732
X02 = Mean green band
218.357
183.700
· · ·
182.238
X03 = Mean red band
112.018
82.950
· · ·
74.286
X04 = Mean near infrared band
426.607
251.750
· · ·
301.690
X05 = Std. deviation (pansharp. band)
19.083
16.079
· · ·
22.944
Source: Johnson et al. (2013).
To construct a classifier for predicting disease status from a satellite image, consider the
use of the k-NN method. We train the learning algorithm on the larger set of n = 4339 observa-
tions, then apply it to the test set of no = 500 observations, and examine the various accuracy
and misclassification rates. The training variables are coded as X1, … , X5, Y and the test vari-
ables as X01, … , X05, Y0. (The feature variables will be scaled by their standard deviations as
part of the analysis.)
Set k = 7 and employ Euclidean distance as the separation metric. With this, sample R
code for the k-NN analysis employs functions from the external class package:
> trainset <- as.matrix( cbind(X1,X2,X3,X4,X5) )
> testset <- as.matrix( cbind(X01,X02,X03,X04,X05) )
> require( class )
> Yhat.knn <- knn( train=scale(trainset, center=F),
test=scale(trainset, center=F), cl=Y, k=7 )
> classify.knn <- knn( train=scale(trainset, center=F),
test=scale(testset, center=F), cl=Y, k=7 )
In the code, the knn() function is the workhorse: its train= and test= subcommands define
the feature variables from the (scaled) training and test sets, respectively. The cl= subcom-
mand indicates the category labels in the training set that correspond to the feature variables
defined by train=. All these terms are required. Lastly, the k= option sets the number of ele-
ments in the nearest-neighbor window. (The default is k=1.) Notice that by listing the training
set in both the train= and test= subcommands, the function will give predicted values for
the training responses; this is assigned to the Yhat.knn object. The consequent confusion
matrix for the training data can be determined simply as
> table( Yhat.knn, Y )

SUPERVISED LEARNING: CLASSIFICATION
311
Using similar code, the confusion matrix for the test data is available via
> table( classify.knn, Y0 )
The results appear in Table 9.10, where the confusion matrices provide some intriguing
insights. First, the overall accuracy of the seven-neighbor rule in the training set is (34 +
4265)/4339 = 99.1%; the complementary misclassification rate is 40/4339, or less than 1%.
While striking, these values only provide validation that the rule has accurately learned the
extant patterns in the training data. The challenge comes with the test data: accuracy there is a
more modest (6 +313)/500 = 63.8%, while misclassification error grows to 181/500 = 36.2%.
It is not uncommon to see drops in accuracy and consequent rises in misclassification rates as
the ‘test’ of the rule is conducted. Here the shrinkage is substantial, but it nonetheless affords
some useful information: notice that for both data sets the specificity is perfect at 100%. The
seven-neighbor rule apparently has strong ability to accurately identify nondiseased remote
images. It might be anticipated, however, that accurate identification of diseased images is
the more important goal here; the sensitivity of 34/74 = 45.9% in the training data confusion
matrix is far smaller. (Test data sensitivity is even worse, barely over 3%.)
Table 9.10
A 2 × 2 confusion matrices from k-nearest neighbor classifications in Example
9.3.1.
Training set
Test set
Observed
Observed
Diseased
Other
Row total
Diseased
Other
Row total
Diseased
34
0
34
6
0
6
Predicted
Other
40
4265
4305
181
313
494
Column total
74
4265
4339
187
313
500
Notice from Table 9.10 that the bulk of the training data, 4265/4339 = 98.3%, are
nondiseased images. The rule appears to do well on nondiseased images with this substantial
amount of information but has clear difficulty learning from the far fewer diseased images.
Whether changes to the k-neighbor window (Exercise 9.12) or other classification strategies
can improve the error rates here is a question explored further in the following.
◽
If the choice of the tuning parameter k is in doubt, many analysts select it adaptively from
the data using leave-one-out cross-validation. As mentioned in Section 7.4.2, cross-validation
removes an observation from the data set and then uses the remaining data to estimate the
value of that excised observation under the proffered model. The knn.cv() function in the
external class package can perform leave-one-out cross-validation on a set of training data.
A k-NN classifier can only classify a test vector Xo from among the categories presented
to it by the k-NN, thus it does not actively search out other possible classification options.
This is often referred to as ‘lazy learning.’ Also, as with many simple nonparametric tech-
niques, simplicity trumps capability for this very basic classification methodology. Extensions
are available that can enhance the nearest-neighbor strategy but still retain the nonparametric
flavor; see, for example, Hastie et al. (2009, Section 13.4).

312
STATISTICAL DATA ANALYTICS
9.4
Tree-based methods
9.4.1
Classification trees
An alternative and somewhat more advanced nonparametric method for constructing
supervised classification rules is based on application of decision trees: simple, branching
flowcharts that are both straightforward to plot and easy to intuit. (They are part of a larger
class of tree-structured diagrams known as dendrograms.) This makes them very popular in
decision analysis; de Ville (2013) provided an engaging review.
The decision tree approach apportions the feature space into a series of disjoint
hyper-rectangles by recursively partitioning larger rectangles into smaller ones. This grows
the tree, , hierarchically from an originating ‘root’ collection 1 – traditionally located
at the top of the display – down to the final classification of M > 1 terminal ‘nodes.’ (The
arboresque terminology here becomes rather imaginative: the terminal nodes are the ‘leaves,’
while bifurcations from existing, intermediate nodes are ‘branches’ of the tree.) Each node
is examined to determine the quality of its classification regime: as more and more elements
or fewer and fewer elements from a single category populate the node, we say it has greater
‘purity’ for classifying the category. Statistical operations are usually conducted on the
complementary ‘impurity’ scale, so the goal becomes one of finding rules that decrease
impurity as the tree grows.
In its simplest form, this recursive partitioning evolves by applying a series of dichotomous
splitting criteria to the feature variables. These take the form Xj ≥𝜏versus Xj < 𝜏for some
threshold 𝜏, and they create a pair of new child nodes branching out from that parent node.
The routine searches through the set of feature variables to determine a splitting rule that
provides greatest decrease in impurity at each new branch; so-called greedy search algorithms
are commonly applied, focusing on local improvement to rapidly progress the optimization
(Kantardzic 2003, Section 7.1). The tree continues growing until a predetermined stopping
rule is attained, say, when every node reaches a minimum size or when no further decrease in
impurity is possible from any existing node. The leaves are then defined as the terminal nodes
at which the tree stops.
The result in the feature space is a series of recursively embedded (hyper)rectangles that
define the terminal nodes and that contain the predicted classifications. Figure 9.5 gives an
idealized example with two input variables X1 and X2 under a dichotomous classification
scheme (Y = 1 for ‘Positive’ and Y = 2 for ‘Negative’).
To define the impurity measure at any mth node (m = 1, … , M) denote by Nqm the number
of observations classified into the qth category (q = 1, … , Q). Let N+m = ∑Q
q=1 Nqm be the
sum over all Q categories. Consider first the simplest case of Q = 2. Conditional on the Xjs and
on the previous branchings, Nqm ∼Bin(N+m, 𝜋qm) where 𝜋qm is the probability of classifying
into category q at node m. Clearly, as 𝜋1m →0 or 𝜋1m →1, the node’s purity for classify-
ing category q = 1 improves. (Similarly for category q = 2.) Away from these extremes, the
impurity grows.
A possible measure to quantify impurity here is (proportional to) the variance of Nqm,
𝜋qm(1 −𝜋qm), because binomial variation also increases as 𝜋qm departs from 0 or 1. Summing
over q yields the Gini impurity
𝒢m = 𝜋1m(1 −𝜋1m) + 𝜋2m(1 −𝜋2m) = 2𝜋1m(1 −𝜋1m),

SUPERVISED LEARNING: CLASSIFICATION
313
(a)
(b)
X2 ≥ 5.5
X1 ≥ 42
X2 ≥ 6.1
X2 < 5.5
X1 < 42
X2 < 6.1
Neg.
Neg.
Pos.
Pos.
Neg.
Neg.
Neg.
1
2
4
8
9
5
3
X2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
 Neg.
 Neg.
 Neg.
 Pos.
30
3
4
5
6
7
40
50
60
70
X1
Figure 9.5
Idealized four-leaf classification tree (a) with Q = 2 outcome categories (Y =
1 for ‘Pos.’ and Y = 2 for ‘Neg.’) and (b) corresponding partitions in the (X1, X2) feature
space.
where the latter equality recognizes that 𝜋2m = 1 −𝜋1m in this binomial case. Alternatively,
another measure for variation/disorder is based on the information-theoretic entropy from
(2.7). For a discrete binomial variable, this produces
m = −𝜋1m log2(𝜋1m) −𝜋2m log2(𝜋2m) = −𝜋1m log2
(
𝜋1m
1 −𝜋1m
)
−log2(1 −𝜋1m).
(Many authors use the more-popular natural log instead of the base-2 log in m, because the
result differs by only a constant multiple.)
A third alternative involves the node’s misclassification error. Let successful classification
in the mth node be defined as achieving maximal containment for the observations in the qth
category (James et al. 2013, Section 8.1.2). Then the misclassification error is
ℳm = 1 −max{𝜋1m, 𝜋2m} = 1 −max{𝜋1m, 1 −𝜋1m}.
In practice, we estimate 𝜋qm by the per-node proportion pqm = Nqm∕N+m. With this, and
extending to Q > 2 categories, the estimated Gini impurity becomes
𝒢m =
Q
∑
q=1
pqm(1 −pqm) = 1 −
Q
∑
q=1
p2
qm,
while the estimated entropy (also called cross-entropy) is
m = −
Q
∑
q=1
pqm log2(pqm).
(9.11)

314
STATISTICAL DATA ANALYTICS
If one replaces the base-2 log with the natural log when computing (9.11), the impurity
becomes
m = −
Q
∑
q=1
pqm log(pqm) = −1
N+m
Q
∑
q=1
Nqm log(pqm),
(9.12)
where (0)log(0) is defined as 0. Now, the extension from the binomial case with Q = 2 to
the general case with Q > 2 induces a multinomial probability structure in the Nqms. (The
multinomial distribution is a multiple-category generalization of the binomial model, also
mentioned in Section 8.3.3.) In this case, Venables and Ripley (2002, Section 9.1) showed,
in effect, that for fixed m the impurity in (9.12) differs by only a constant from the deviance
(defined in Section 8.2.2) under the multinomial model:
−2
Q
∑
q=1
Nqm log(pqm).
As a result, many authors refer to the entropy and the deviance interchangeably.
Lastly, the estimated misclassification error is
ℳm = 1 −
max
q=1, … ,Q{pqm}.
Among the three impurities 𝒢m, m, and ℳm, misclassification error is generally the least
sensitive, while the Gini and entropy impurities operate in a roughly similar manner. A variety
of additional measures can also be constructed (Linoff and Berry 2011, Chapter 7), although
the three impurities presented above are common in statistical learning.
This bifurcating, two-branch splitting strategy can be broadened to allow for multiway
branching from each node and/or to use linear combinations of the Xjs in the splitting rules.
Some of these features are embodied in a famous series of tree-based algorithms known pro-
gressively as the ID3, C4.5, and C5.0 routines; see Quinlan (1996). For more on these and
other extensions of the basic classification tree paradigm, Loh (2010) and Linoff and Berry
(2011, Chapter 7), among many others, provided instructive expositions.
9.4.2
Pruning
Simple stopping rules for tree-based classifiers can vary, leading to potentially wide differ-
ences in how a tree is actually used for classification. One common problem is overfitting, the
inclusion when a tree grows out too far of classifications that overaccommodate the particular
features in the training data. This decreases bias, but typically at a cost of increased variance
and poor predictive ability. (As in Sections 7.4 and 9.3, another bias-variance trade-off.)
A strategy for combatting overfitting involves pruning a tree as or after it is formed.
View this as the culling of possibly superfluous nodes from , not unlike pruning overgrown
branches from a real tree. To do so, denote by || the number of terminal nodes of the full
tree and index these nodes via m′ = m1, m2, … , m||. Define the cost-complexity risk as
R𝛼() = R() + 𝛼||,
(9.13)
where 𝛼is a complexity parameter that tunes the risk and R() is some function describing
the total cost or risk associated with the full tree. A common choice for R() is the terminal
misclassification error ∑
m′ℳm′, although other impurity measures such as terminal entropy
are also possible.

SUPERVISED LEARNING: CLASSIFICATION
315
Notice the similarity with the objective functions in regression regularization from
Section 7.4: 𝛼acts as a form of penalty parameter that controls the complexity of overfitted
models (here, trees). As 𝛼→0, the pruned tree approaches the full tree, while as 𝛼→∞,
the pruning drives towards the original root node. It can be shown that as 𝛼increases, the
full tree can be cut back to some optimal subtree that minimizes (9.13). This is known as
‘weakest-link cutting’ or ‘weakest-link pruning’ (Breiman et al. 1984, Section 3.3).
In R, the external rpart package provides functions to perform the recursive partitioning
algorithm, including weakest-link pruning, as does the external tree package. The latter is
slightly simpler to use, although the former is popular for construction of classification trees. A
variety of other packages also offer enhanced output graphics for classification trees, including
rpart.plot and maptree.
The complexity parameter 𝛼is often chosen in a data-dependent manner from the training
set. As in Section 9.3, cross-validation is a favored approach, here applied in an L-fold manner.
(At L = n, we recover the leave-one-out form of cross-validation mentioned earlier, although
for very large training sets, this can become computationally prohibitive.) When applied to
classification trees, typical default choices for L lie in the range 5 ≤L ≤10.
To apply L-fold cross-validation, randomly separate the training data into L disjoint subsets
of roughly equal size. Then, remove the ℓth subset and use the remaining data to grow a full
tree (ℓ= 1, … , L). Apply the predicted classifications to the removed ℓth subset and record
the misclassification error. Repeat across all ℓfolds and average the result. (Also include the
standard error of the mean for comparison purposes.) Finally, perform these operations over
a range of possible values for 𝛼, and choose that 𝛼with minimum average error. Return to the
full training set and apply cost-complexity pruning with that chosen 𝛼.
The cross-validation error often drops sharply from 𝛼at ∞and then may flatten as 𝛼→0.
Thus there may be a number of possible candidates for 𝛼with essentially equal errors. We
then turn to the colloquially named one-standard-error rule: prune to the highest 𝛼(i.e., the
most parsimonious tree) whose average cross-validation error is no larger than the minimum
average error plus its standard error. In R, the external rpart package provides a number of
functions to assist in this effort.
Example 9.4.1 Remote sensing of tree disease (Example 9.3.1, continued). Return to the
oak tree disease study in Example 9.3.1 and consider growing a classification tree from that
example’s training data set. Recursive partitioning as implemented in the R rpart package is
featured here. Given the classification variable Y (Y = 1 for diseased trees, Y = 2 for other
land cover) and the predictor variables X1, X2, X3, X4, X5 from Table 9.9, sample R code for
constructing the full tree is
> Yfac <- factor( Y, levels=1:2, labels=c(‘Diseased’,‘Other’) )
> require( rpart )
> control.set <- rpart.control( cp=0, xval=10, minbucket=5 )
> set.seed( 941 )
> classify.rpart <- rpart( Yfac ˜ X1+X2+X3+X4+X5, method=‘class’,
parms=list(split=‘gini’), control=control.set )
In the code, notice the following:
• The response variable is rebuilt into the factor variable Yfac so that the rpart() func-
tion will properly construct a classification tree (also see method= in the following).

316
STATISTICAL DATA ANALYTICS
• The preliminary call to rpart.control() sets various control parameters for con-
structing the tree:
∘to build a full tree, cp=0 initially sets the complexity parameter 𝛼to 0;
∘xval=10 calls for 10-fold cross-validation (in order to later select a value for 𝛼); and
∘minbucket=5 sets the stopping rule to allow no fewer than five observations in a
terminal node/leaf.
• Because 10-fold cross-validation populates the folds randomly, it is good practice to set
the seed for the random number generator explicitly – here this is set.seed(941).
• In the call to rpart(),
∘the leading formula has a familiar structure, similar to that seen in earlier
regression-type functions such as lm() and glm();
∘method=‘class’ instructs R to build a classification tree (because the response vari-
able is a factor, the function would do so by default; still, it is instructive to present it
here);
∘parms=list(split=‘gini’) calls for use of Gini impurity (this is also the default;
split=‘information’ would call for entropy impurity); and
∘control=control.set draws in the control settings from the earlier use of
rpart.control().
The resulting object classify.rpart contains all the information necessary to build
the tree. For instance, simply calling the object prints out the splitting criteria and node
containment details. The corresponding full tree can be visualized by applying the com-
mands plot(classify.rpart) and text(classify.rpart). These plot the tree and
add informative labels, respectively. Or, summary(classify.rpart) provides a highly
detailed output (all results left to reader).
As described earlier, however, pruning the tree is a logical next step. Useful in this regard
is rpart’s printcp(classify.rpart) command. This constructs a so-called CP table for
choosing the complexity parameter 𝛼. Here, the table lists the average 10-fold cross-validation
errors along with their standard errors over a range of 𝛼values. The output (edited) is
Classification tree:
rpart(formula = Yfac ˜ X1 + X2 + X3 + X4 + X5, method = "class",
parms = list(split = "gini"), control = control.set)
Variables actually used in tree construction:
[1] X2 X3 X4
Root node error: 74/4339 = 0.017055
n = 4339
CP
nsplit
rel error
xerror
xstd
1
0.2432432
0
1.00000
1.00000
0.115252
2
0.0405405
2
0.51351
0.54054
0.085072
3
0.0270270
3
0.47297
0.50000
0.081848
4
0.0225225
5
0.41892
0.48649
0.080744
5
0.0202703
8
0.35135
0.48649
0.080744
6
0.0067568
10
0.31081
0.44595
0.077333
7
0.0000000
12
0.29730
0.41892
0.074971

SUPERVISED LEARNING: CLASSIFICATION
317
The output reveals a number of important features. Perhaps the most interesting thing we see
under ‘Variables actually used’ is that only X2, X3, and X4 were employed in form-
ing the full tree. (This would have been evident when displaying the full tree graphically as
well.) Thus the two pansharpening band variables from Table 9.9, X1 ={Mean gray-level} and
X5 ={Std. deviation}, were not seen to add value in constructing the tree.
Next is the data in the displayed CP table: minimum cross-validation error (xerror col-
umn) occurs at 𝛼= 0 (CP column). Applying the one-standard error (s.e.) rule, however,
shows that the cross-validation error at 𝛼= 0.0225 is within one standard error of this min-
imum (0.48649 < 0.41892 + 0.07497 = 0.49389), so choose a marginally larger value, say
𝛼= 0.025, as the complexity parameter for pruning the tree. (Similar determinations can be
viewed graphically via rpart’s plotcp(classify.rpart) command.) The pruned tree is
then computed using
> prune.rpart <- prune( classify.rpart, cp=0.025 )
(One could return to the rpart command and first fit the reduced model using only the three
variables, X2 = {Mean green band}, X3 ={Mean red band}, and X4 ={Mean near-infrared
band}, that is, with formula = Yfac ˜ X2 + X3 + X4. Pruning to the same 𝛼= 0.025
would not change the final decision tree, however.)
Visualization is possible via a number of functions and routines. For instance, by restrict-
ing attention to the three variables X2, X3, and X4, an intriguing graphic can be produced:
Figure 9.6 is a three-dimensional scatterplot of the data using only these three feature vari-
ables. ‘Diseased’ image points are colored differently than the ‘Other’ image points. (The
figure was created using routines from the external scatterplot3d package.) A clear pattern
emerges: the diseased images locate in a restricted swath near smaller values of the X2 and X3
inputs. Clearly, strong potential exists for developing a useful classification rule from these
training data.
To view the pruned tree, print(prune.rpart) details the splitting rules and pertinent
per-node statistics (output edited):
node), split, n, loss, yval, (yprob)
* denotes terminal node
1) root 4339 74 Other (0.01705462 0.98294538)
2) X3>=114.6723 1221 51 Other (0.04176904 0.95823096)
4) X2< 222.4286 48
6 Diseased (0.87500000 0.12500000)
8) X4>=317.4018 43
2 Diseased (0.95348837 0.04651163) *
9) X4< 317.4018 5
1 Other (0.20000000 0.80000000) *
5) X2>=222.4286 1173
9 Other (0.00767263 0.99232737) *
3) X3< 114.6723 3118 23 Other (0.00737652 0.99262348)
6) X2< 184.8764 155 14 Other (0.09032258 0.90967742)
12) X3>=87.83333 14
5 Diseased (0.64285714 0.35714286) *
13) X3< 87.83333 141
5 Other (0.03546099 0.96453901) *
7) X2>=184.8764 2963
9 Other (0.00303746 0.99696254) *
A more visually digestible representation, however, displays the tree graphically. As noted
above, plot(prune.rpart) and text(prune.rpart) are available, although these are
somewhat rudimentary. The external rpart.plot package and its powerful prp() function can
build more engaging tree graphics; for example, the sample R code

318
STATISTICAL DATA ANALYTICS
0
100
200
300
400
500
600
700
800
0
200
400
600
800
1000
1200
0
200
400
600
800
1000
X2
X3
X4
Figure 9.6
Three-dimensional scatterplot of oak tree disease training data in Example 9.4.1,
using feature variables X2, X3, and X4 from Table 9.9. Points are distinguished by observed
classification: open circles are ‘Diseased’ tree images and filled circles are ‘Other’ images.
Data from Johnson et al. (2013).
> require( rpart.plot )
> prp( prune.rpart, type=4, extra=104, clip.right.labs=F, nn=T,
fallen.leaves=T )
generates a tree with a variety of enhanced features; see help(prp) for details. Figure 9.7
displays the pruned tree with the root node (no. 1) at the top and branches eventually ending
in six leaves (terminal nodes’ numbers 8, 9, 5, 12, 13, 7, as per the numbers in the small boxes
above each node). The graphic gives the following information:
• The splitting criteria used to form each new branch; for example, the first split was
created by dichotomizing between X3 ≥115 (left branch) and X3 < 115 (right branch).
• The predicted classifications in each node. Focus on the terminal leaves; for example,
proceeding down the far left branches to terminal node/leaf no. 8 predicts an observed
image’s trees into the ‘Diseased’ category when (i) X3 ≥115, then (ii) X2 < 222, and
finally (iii) X4 ≥317.
• Each node’s containment statistic. For example, terminal node/leaf no. 8 contains only
43/4339 = 1% of the training set observations, the percentage given at the bottom of the
leaf box. A few leaves list 0% containment, but this is due to rounding. For instance,
leaf no. 9 contains 5/4339 = 0.1% of the training observations. (Find these values in the
print(prune.rpart) output.)
• Labeling that gives each node’s summary classification probabilities, also called ‘pos-
terior probabilities.’ In terminal node/leaf no. 8, for example, 95% of the observations

SUPERVISED LEARNING: CLASSIFICATION
319
lie in the ‘Diseased’ category. As this exceeds 50%, the fitted/predicted category is
‘Diseased’ as labeled in the leaf box. Obviously, the remaining 5% is ‘Other.’ Find
these values via unique(predict(object=prune.rpart, type=‘prob’)) Cor-
responding statistics are displayed in the other leaf boxes.
The pruned tree can also be used for predictive analytics. In R, if train.df is the original
training set data frame, the confusion matrix for the training data can be calculated via
> Yhat.train = predict( object=prune.rpart, newdata=train.df,
type=‘class’ )
> table( Yhat.train, Yfac )
Similarly, suppose test.df contains the test set data frame with a column Y0 giving the
observed categories for each observation (Y0 = 1 for diseased trees; Y0 = 2 for other land
X3 ≥ 115
X2 < 222
X4 ≥ 317
X2 < 185
X3 ≥ 88
X3 < 115
X2 ≥ 222
X4 < 317
X2 ≥ 185
X3 < 88
Other
0.02 0.98
100%
Other
0.04 0.96
28%
Diseased
0.88 0.12
1%
Diseased
0.95 0.05
1%
Other
0.20 0.80
0%
Other
0.01 0.99
27%
Other
0.01 0.99
72%
Other
0.09 0.91
4%
Diseased
0.64 0.36
0%
Other
0.04 0.96
3%
Other
0.00 1.00
68%
1
2
4
8
9
5
3
6
12
13
7
Figure 9.7
Classification tree for training data in oak tree disease study from Example 9.4.1,
using feature variables listed in Table 9.9. Displayed tree gives six terminal modes, pruned
from the full classification tree using a complexity parameter (‘cp’) of 𝛼= 0.025. See text for
descriptions of the individual node labels. Source: Data from Johnson et al. (2013).

320
STATISTICAL DATA ANALYTICS
cover) and with five additional columns giving the corresponding feature variables X01, X02,
X03, X04, and X05. Then, the confusion matrix for the test data is available via
> testYfac = factor( Y0, levels=1:2, labels=c(‘Diseased’,‘Other’) )
> Yhat.test = predict( object=prune.rpart, newdata=test.df,
type=‘class’ )
> table( Yhat.test, testYfac )
The results appear in Table 9.11.
Table 9.11
2 × 2 confusion matrices from pruned classification tree in Example 9.4.1.
Training set
Test set
Observed
Observed
Diseased
Other
Row total
Diseased
Other
Row total
Diseased
50
7
57
74
3
77
Predicted
Other
24
4258
4282
113
310
423
Column total
74
4265
4339
187
313
500
From the confusion matrices in Table 9.11, we see the pruned tree’s overall accuracy for
the training set is high: mean(Yhat.train==Yfac) gives (50 + 4258)/4339 = 99.3%. The
corresponding misclassification error is only 31/4339 = 0.7%. Compare these to their coun-
terparts from the k-NN fit in Example 9.3.1: there the accuracy was incrementally lower
at 99.1% while misclassification was higher at 0.9%. These values are arguably indistin-
guishable, suggesting roughly similar capacities between the two methods for classifying the
training data. (The pruned tree does classify more training images as diseased, both correctly
and incorrectly.)
For the test data, however, more substantive improvement is seen: the test accuracy in
Table 9.11 is (74 + 310)/500 = 76.8%, much larger than the 63.8% seen with the k-NN anal-
ysis. Similarly, the test misclassification error of pruned tree is only 116/500 = 23.2%, well
below the k-NN error of 36.2%.
Recall that sensitivity – that is, the true positive rate – was felt to be a pertinent target
quantity with these data. From Table 9.11, the sensitivity for the training data is 50/74 =
67.6%, while for the test data, it is 74/187 = 39.6%. Both values are increased over the k-NN
results of 45.9% and 3.2%, respectively. At least for these remote sensing data, the pruned
decision tree analysis has clearly improved on these various summary measures. Exercise
9.16 explores this analysis further.
◽
Tree-based methods are often favored in classification analytics, due to their relative sim-
plicity, natural graphic output, and fairly intuitive interpretation(s). Their nonparametric fla-
vor, not requiring a formal parametric model for implementation, also drives much of their
popularity. Indeed, they can operate with both continuous and categorical predictor/feature
variables, and nothing in their construction stymies them by missing values in the data. They
do possess some disadvantages, however, not the least of which is their tendency to overfit,
producing high variances. Associated with this is a tendency to propagate early classification
errors down the tree, due to their fundamental, hierarchical nature. An adjustment known as

SUPERVISED LEARNING: CLASSIFICATION
321
bagging (Breiman 1996) employs a form of bootstrap resampling (Section 5.3.6) to average
over many different trees and help to alleviate these concerns. Such averaging is often called
a form of ensemble learning. See Hastie et al. (2009, Section 8.7).
For further discussions on recursive partitioning and classification trees, see the presenta-
tions in Hand et al. (2001, Section 10.5) and Clarke et al. (2009, Section 5.3).
9.4.3
Boosting
Another technique useful for improving the classification capacity of a decision tree is known
as boosting (Schapire 1989). Often discussed in association with bagging (above) – but
based on a different core technology – the method attempts to ‘boost’ the performance of
weak classifiers. (A ‘weak’ classifier exhibits predictive ability only marginally better than
random guessing.) In boosting, one constructs an averaged classification rule by combining
sequentially updated classifiers from the same training data. This is sometimes referred to as
‘classification by committee’ and is another form of ensemble learning.
A highly effective algorithm to perform boosting is AdaBoost (Freund and Schapire 1996,
1997). AdaBoost starts by equally weighting each training observation and constructing an
associated classification tree. It then updates the weights to increase the influence of points
that were misclassified. The new weights determine the next classifier. The process repeats and
the final classification rule is itself based on a weighted average of the updated classifiers. The
algorithm can drastically improve the performance of weak classification trees. In R, boosting
is available in a number of external packages, including ada, adabag, gbm, and mboost.
As a concept, boosting is naturally extensible and can be employed with more than just
classification trees. It has been applied across a rich panoply of data mining contexts. For more
details, see Hastie et al. (2009, Chapter 10).
9.4.4
Regression trees
Tree-based methods may also be applied in supervised learning when the response variable Y
is quantitative. This technically places us back in the regression setting from Chapters 6 to 8,
although it is useful to briefly discuss the approach here.
If one applies basic recursive partitioning to a continuous response variable Y, the result
is, in effect, a form of nonparametric regression. The splitting criteria continue to take the
form Xj ≥𝜏versus Xj < 𝜏for some threshold 𝜏. The predicted response will then be a series
of flat (hyper)rectangles lying above the rectangular partitions of the feature space produced
by each m′th terminal node. The height of the predicted response will be equal to the mean
of the Yis contained in that node, say, Yi(m′). In this sense, the regression tree is often referred
to as a local model: it captures local characteristics within each terminal node but does not
attempt to make global statements across the entire feature space.
The splitting criterion is once again based on a quantitative impurity measure. Most natural
with a continuous outcome is reduction in the residual sum of squares when splitting the
mth node into two child nodes (Hastie et al. 2009, Section 9.2.2). As overfitting remains a
problem, pruning is also conducted using the cost-complexity approach and incorporating
L-fold cross-validation where necessary.
Taken together, regression trees and classification trees comprise a larger class of
tree-based methods known as classification and regression trees (CART), developed by
Breiman et al. (1984). A modern introduction is available, for example, in James et al. (2013,

322
STATISTICAL DATA ANALYTICS
Section 8.1). In R, rpart fits regression trees using the same rpart() function: simply
invoke the method=‘anova’ option. See help(rpart).
As they are sister methodologies, regression trees share both the strengths and the weak-
nesses of classification trees from Section 9.4.1. Bagging (bootstrapped aggregation) can
again be applied to improve predictive accuracy. A further enhancement, known as random
forests, extends the bootstrapping by randomly sampling from the feature variables at each
split (Breiman 2001). The result, which may seem counterintuitive, can actually reduce cor-
relation in the eventual regression tree. In R, the external randomForest package can conduct
both bagging and random forests for CARTs (see James et al. 2013, Sections 8.2–8.3).
9.5
Support vector machines∗
A powerful, modern, supervised learning technology is based on support vector (SV) methods
(Vapnik 1995, 1998). SV classifiers are increasingly applied with high-dimensional data in, for
example, genetic research with microarray or single nucleotide polymorphism data (O’Fallon
et al. 2013; Zhang et al. 2006), or in large-scale signal processing studies (Salcedo-Sanz et al.
2014). The calculations involve fairly sophisticated statistical and mathematical techniques,
so this section provides only a brief introduction; Mammone et al. (2009) gave a larger, con-
temporary review. Focus herein is on two classes (Q = 2). It will be convenient to reformulate
the categorical response as now Y = 1 for category q = 1 and Y = −1 for category q = 2.
SV methods are best conceptualized by distinguishing between two disjoint cases: data
that are completely separable in the input-variable space and those that exhibit some nonsep-
arable overlap. The next section begins with the former case.
9.5.1
Separable data
Separable data occur when the observations lie in clearly disjoint regions of the input space
and when no overlap is evident between them. In particular, assume that the separation is
linear such that a roughly straight band cleaves the two classes. While uncommon in practice,
this sort of compete linear separation is useful for laying the foundations of the SV approach.
As in previous sections, the goal is to determine a linear decision boundary that efficiently
discriminates between the two categories. When the data are linearly separated, however, there
will be infinitely many such boundaries. (Why?) To find an optimal separator when Q = 2,
Vapnik in his 1995 textbook showed that a unique separating line exists that maximizes the
distance between itself and the closest points in both classes; see Vapnik (2000, Section 5.4)
in the book’s second edition. The separating distance between the two classes is called the
margin; this optimal separating line lies in the middle of the margin and acts to maximize its
width. (It is, therefore, often described as a maximum margin classifier.) In higher dimensions,
the line becomes a hyperplane – cf. Section 9.2.3 – and we refer to the decision boundary
generically as the optimal separating hyperplane. For consistency, the generic terminology is
used here. Denote this hyperplane by 0.
The two borders at the edge of the margin are called supporting hyperplanes, and any
points that lie directly on these borders – there must be at least one for each hyperplane – are
called support vectors. (The terminology harkens to the geometric interpretation where a point
in space defines a vector to the origin.) Denote the supporting hyperplane for category q = 1

SUPERVISED LEARNING: CLASSIFICATION
323
X1
X2
Support vectors
Separating hyperplane
Margin
Figure 9.8
Fully separable data: idealized display of supporting hyperplanes 1 (upper
dashed line) and 2 (lower dashed line) and optimal separating hyperplane 0 (solid line) in
(X1, X2) plane with Q = 2 categories (Y = 1 for upper circles; Y = −1 for lower diamonds).
Support vectors are highlighted in gray.
by 1 and that for category q = 2 by 2. Figure 9.8 gives an idealized illustration. (Therein,
and in what follows, assume without loss of generality that the ‘positive’ category at q = 1 is
defined to lie above 0.)
Mathematically, the separating hyperplane 0 in Figure 9.8 satisfies
0 = {𝛽0, 𝜷∶𝛽0 + 𝛽1X1 + 𝛽2X2 = 0}
for some coefficient vector 𝜷= [𝛽1
𝛽2]T and some constant 𝛽0. This gives a rule for clas-
sification: assign Yi into category q = 1 if 𝛽0 + 𝛽1Xi1 + 𝛽2Xi2 > 0 and into category q = 2 if
𝛽0 + 𝛽1Xi1 + 𝛽2Xi2 < 0. Since Yi = ±1, this gives
Yi(𝛽0 + 𝛽1Xi1 + 𝛽2Xi2) > 0,
for all i = 1, … , n.
(9.14)
Notice that the farther a point is from the optimal separating hyperplane, the more certain
we are of its classification. This gives distances from 0 intrinsic interpretation. Indeed, one
can show that the distance from 0 to either supporting hyperplane is 1∕|| 𝜷|| (Clarke et al.
2009, Section 5.4.1), where the notation || 𝜷|| represents the L2 norm of 𝜷:
|| 𝜷|| =
√
𝛽2
1 + 𝛽2
2 .
(9.15)
To find the 𝛽js, given the data, we appeal to optimization arguments. The goal is to identify
that separating hyperplane whose margin is pushed as far apart as possible but still supports

324
STATISTICAL DATA ANALYTICS
the separated classes. That is, we wish to maximize the width of the margin subject to (9.14).
It will be convenient to work with just the margin half-width, which we write as some positive
value ℳ> 0. To contain the separated classes, this then requires
Yi(𝛽0 + 𝛽1Xi1 + 𝛽2Xi2) ≥ℳ,
for all i = 1, … , n.
(9.16)
In effect, we aim to maximize ℳsubject to (9.16). Unfortunately, in its present form, this
problem cannot be solved uniquely: if the quantities 𝛽j satisfy (9.16), so will 𝜓𝛽j for any
𝜓≥1. To compensate, impose the additional constraint || 𝜷|| = 1. Rearranging terms and
recognizing that ℳ= 1∕|| 𝜷||, the problem then simplifies into the following constrained
minimization: find the separating hyperplane 0 of the form 𝛽0 + 𝛽1X1 + 𝛽2X2 to
(a) minimize the objective quantity 1
2 || 𝜷||2, subject to
(b) Yi(𝛽0 + 𝛽1Xi1 + 𝛽2Xi2) ≥1 for all i = 1, … , n.
This inequality-constrained minimization falls into the class of convex optimization problems
(Lange 2013, Chapter 14). The specifics exceed the scope of discussion here, although the
result is fairly simple to express. The coefficients for the optimal separating hyperplane 0
are
𝛽j =
n
∑
i=1
aiYiXij
(9.17)
(j = 1, 2), where the ai terms satisfy
ai = 0 if Xij lies strictly within its classifying region, or
ai > 0 if Xij lies on its supporting hyperplane.
Notice here that only the SVs affect construction of the separating hyperplane: observations
wholly within the classification region contribute no weight (ai = 0) to 0. This gives the SV
approach a certain robustness to extreme, outlying observations.
To find the ai coefficients in (9.17), dualities in the convex optimization can be exploited
to write the problem equivalently as a second-order optimization. Maximize the objective
quantity
(a1, … , an) =
n
∑
i=1
ai −1
2
n
∑
i=1
n
∑
h=1
aiahYiYhXT
i Xh
(9.18)
subject to
n
∑
i=1
aiYi = 0
(9.19)
and ai ≥0 for all i = 1, … , n. Standard quadratic optimization algorithms then complete the
calculations (Karatzoglou et al. 2006).
With the 𝛽js given by (9.17), 𝛽0 is found by solving Yi(𝛽0 + 𝛽1Xi1 + 𝛽2Xi2) = 1 for all train-
ing tuples (Xi, Yi) satisfying ai > 0. As there will likely be many solutions, one can average
the results or, following Clarke et al. (2009, Section 5.4.4), take
𝛽0 = −1
2
min
{i∶Yi=1}
{𝛽1Xi1 + 𝛽2Xi2
} −1
2
max
{i∶Yi=−1}
{𝛽1Xi1 + 𝛽2Xi2
} .

SUPERVISED LEARNING: CLASSIFICATION
325
Using these quantities, the SV classifier can be written as a simple function of any putative
or test feature vector X = [X1
X2]T:
𝛿SV(X) = sgn
(
𝛽0 +
n
∑
i=1
aiYiXT
i X
)
,
(9.20)
where sgn(x) = −I(−∞,0)(x) + I(0,∞)(x) is the signum function that reports the sign of its argu-
ment. Notice that a majority of the terms in (9.20) will likely correspond to ai = 0; this
sparseness can reduce much of the computational burden in the calculation.
Generalization of the SV classifier to p > 2 feature variables is conceptually straightfor-
ward: simply extend [Xi1 Xi2]T to Xi = [Xi1 Xi2 · · · Xip]T and [𝛽1 𝛽2]T to 𝜷= [𝛽1 · · · 𝛽p]T,
and write the separating hyperplane in the general form 𝛽0 + XT
i 𝜷. Make corresponding
changes to (9.17), (9.18), and (9.20).
9.5.2
Nonseparable data
When the data fail to separate, that is, when some observations with Yi = 1 and Yh = −1 cross
in the (X1, X2) input space, we say that they are nonseparable. This creates what is known as a
‘soft margin’ problem, because imposition of the ‘hard’ margin constraints as in Section 9.5.1
no longer admits an optimal solution. The linear separator is now called a soft margin line, or
more generally a soft margin hyperplane.
To accommodate nonseparable data, we admit the existence of a slack variable, 𝜉i ≥0,
that incorporates location information for each Xi = [Xi1 Xi2]T. If Xi is correctly contained
within its classification margin (above it if Yi = 1 and below if Yi = −1), set 𝜉i = 0. If Xi is
misclassified such that it incorrectly lies beyond its opposing margin, set 𝜉i > 1. In between,
set 0 < 𝜉i < 1. The misclassification error is then conveniently written as ∑n
i=1 I(1,∞)(𝜉i)∕n,
where I(⋅) is the indicator function from (2.20).
We continue to write the separating hyperplane 0 as
0 = {𝛽0, 𝜷∶𝛽0 + 𝛽1X1 + 𝛽2X2 = 0}.
By employing the slack variables, the classification rule now assigns Yi into category q = 1
if 𝛽0 + 𝛽1Xi1 + 𝛽2Xi2 > 1 −𝜉i and into category q = 2 if 𝛽0 + 𝛽1Xi1 + 𝛽2Xi2 < −(1 −𝜉i). As
Yi = ±1, a unifying relationship again obtains
Yi(𝛽0 + 𝛽1Xi1 + 𝛽2Xi2) > 1 −𝜉i,
for all i = 1, … , n.
The constrained optimization problem evolves into the following form:
(a) minimize the objective quantity 1
2 || 𝜷||2+C ∑n
i=1 𝜉i, subject to
(b) Yi(𝛽0 + 𝛽1Xi1 + 𝛽2Xi2) ≥1 −𝜉i for all i = 1, … , n, and
(c) 𝜉i ≥0 for all i = 1, … , n,
where C ≥0 is a tuning parameter that incorporates the cost of misclassification. Notice that
∑n
i=1 𝜉i∕n is an upper bound on the misclassification error (Exercise 9.20). In effect, this ‘bud-
gets’ for misclassification/training errors: as C→∞, the margin narrows and misclassification
violations are viewed as more ‘expensive.’ Conversely, as C →0, the margin widens, || 𝜷||

326
STATISTICAL DATA ANALYTICS
shrinks, and violations are viewed as more tolerable. This is another example of bias-variance
trade-off – small C drives bias down but increases variance – similar to that seen with regu-
larized regression in Section 7.4. Indeed, the cost reciprocal 𝜆= 1∕C is often called the SV
regularization parameter.
If no prior choice or external validation data are available for choosing C, some authors
choose a large default value such as C= 102 or higher; this tightens the margin and mini-
mizes overlap but can sometimes lead to overfitting. One could instead compare results across
a selection of values for C and subjectively select that which provides the best predictive
performance. Or, Hastie et al. (2009, Section 12.3.5) described an algorithm to compute a
piecewise-linear regularization path of the support vector machine (SVM) across all values
of 𝜆. Cross-validation can also be employed, using the current training set.
The corresponding solution leads to similar relationships as those found for the separable
case in Section 9.5.1. In particular, 𝛽j takes the same form as in (9.17) where the ai coefficients
now satisfy
ai = 0
for Yi(𝛽0 + 𝛽1Xi1 + 𝛽2Xi2) ≥1
and 𝜉i = 0, or
0 < ai < C
for Yi(𝛽0 + 𝛽1Xi1 + 𝛽2Xi2) = 1
and 𝜉i = 0, or
ai = C
for Yi(𝛽0 + 𝛽1Xi1 + 𝛽2Xi2) ≤1
and 𝜉i ≥0
(Clarke et al. 2009, Section 5.4.5). Once again, observations wholly within their (correct)
classification region contribute no weight (ai = 0) to the separating hyperplane 0. Those
that do affect 0 now either lie on the (correct) margin or lie within the margin on the correct
side of 0. To distinguish these, we say the former are margin support vectors, while the latter
are nonmargin support vectors. Points lying in the incorrect classification region – which also
affect construction of 0 – are of course viewed as misclassifications. Figure 9.9 presents an
idealized illustration.
With 𝛽j given by (9.17), 𝛽0 is found by solving Yi(𝛽0 + 𝛽1Xi1 + 𝛽2Xi2) = 1 over all training
tuples (Xi, Yi) satisfying ai > 0 at 𝜉i = 0 and averaging the results (Hastie et al. 2009, Section
12.2.1).
The dual optimization criteria (9.18) and (9.19) also continue to hold, now with 0 ≤ai ≤C
for all i = 1, … , n. Computer routines are used to conduct the calculations (Karatzoglou et al.
2006).
9.5.3
Kernel transformations
When classification data do not present linear separations the effort to find effective
classification rules becomes more challenging. Classes that can appear difficult to separate
in the initial X-input space may be easily distinguished in some higher-dimensional space,
however, after appropriate transformation of the input variables. It is within such settings
that SV technology becomes particularly useful. Figure 9.10 gives an abstract representation,
motivated by an example from Schölkopf and Smola (2002, Section 2.1). The (X1, X2)
points in Figure 9.10a illustrate a classic ‘bull’s-eye’ pattern where the two groups are
clearly distinct, but where no linear classifier can completely capture the separation. As
seen in Figure 9.10b, however, transforming (X1, X2) to the three-dimensional vector
(X2
1, X2
2,
√
2X1X2) now easily distinguishes the two classes.
We can take advantage of this potential for effective separation by moving to transformed
spaces. Return to the SV objective quantity in (9.18) and notice that the input vectors Xi only

SUPERVISED LEARNING: CLASSIFICATION
327
Margin support vectors
Nonmargin support vectors
Misclassifications
X1
X2
Margin
Figure 9.9
Nonseparable data: idealized display of separating hyperplane 0 (solid line)
and associated margin (bounded by dashed lines) in (X1, X2) plane with Q = 2 categories
(Y = 1 for upper circles; Y = −1 for lower diamonds). Margin and nonmargin support vectors
are indicated in gray. Misclassified points are indicated in black.
−2
−1
0
(a)
(b)
1
2
−2
−1
0
1
2
0.0
 1
2X1X2
0.5 1.0 1.5
2.5
2.0
3.0
−2
−1
 0
 2
 3
0
1
2
3
4
X1
X1
2
X2
2
X2
Figure 9.10
Idealized effect of nonlinear separation and kernel transforms. (a) Nonseparable
(X1, X2) data (Y = 1 as gray dots versus Y = −1 as black dots) in classic ‘bull’s-eye’ pattern.
(b) Transformation into three dimensions via (X2
1, X2
2,
√
2X1X2) produces clear separation.

328
STATISTICAL DATA ANALYTICS
enter into the expression via the Euclidean inner product XT
i Xh. A similar effect is evident
with the SV classification rule in (9.20): 𝛿SV(X) depends on the inputs only through the inner
product XT
i X. Thus we could consider some vector-valued transformation of X to
U = U(X) =
⎡
⎢
⎢
⎢
⎢⎣
U1(X)
U2(X)
⋮
Ur(X)
⎤
⎥
⎥
⎥
⎥⎦
,
where r > p is the dimension of the transformed feature space. Suppose U successfully sepa-
rates the two classes in this transformed space, and we apply it in place of X in (9.18). That is,
optimize the objective quantity (a1, … , an) with XT
i Xh replaced by UT
i Uh. This calculation
will then yield a valid, r-dimensional, optimal separating hyperplane 0.
For example, the three-dimensional transformation in Figure 9.10 takes XT
i Xh = Xi1Xh1 +
Xi2Xh2 to
UT
i Uh = X2
i1X2
h1 + 2Xi1Xi2Xh1Xh2 + X2
i2X2
h2
= (Xi1Xh1 + Xi2Xh2)2 = (XT
i Xh)2.
Notice that we can write this transformation as a function of the two input vectors Xi and Xh,
say, K(Xi, Xh) = (XT
i Xh)2. The function K(⋅, ⋅) is called a kernel. In the general case, when a
kernel satisfies appropriate regulatory conditions (Clarke et al. 2009, Section 5.4.6), it can be
used as a separation-inducing, dimension-transforming replacement in the SV constructions.
For instance, the optimization problem generalizes to maximizing the objective quantity
K(a1, … , an) =
n
∑
i=1
ai −1
2
n
∑
i=1
n
∑
h=1
aiahYiYhK(Xi, Xh)
(9.21)
subject to
n
∑
i=1
aiYi = 0
and 0 ≤ai ≤C for all i = 1, … , n. The ais are used in the classifier
𝛿SV(X) = sgn
{
𝛽0 +
n
∑
i=1
aiYiK(Xi, X)
}
,
for any putative or test feature vector X. With these, the constant 𝛽0 is found as described
in Section 9.5.2. The resulting construction is known formally as a support vector machine,
although the term is sometimes used more generally to describe the general methods described
throughout in this section.
A wide variety of kernel functions can be employed in (9.21). Table 9.12 lists some of
the more popular forms. The linear kernel at top serves as an ‘identity’ transform, while the
succeeding polynomial kernels of order d are popular in practice; indeed, the homogeneous,
second-order kernel was employed in Figure 9.10. Another common choice is the Gaussian
radial basis function (RBF), due to its obvious connections with the normal (Gaussian) p.d.f.
from Section 2.3.9.

SUPERVISED LEARNING: CLASSIFICATION
329
Table 9.12
Selected kernel functions K(A,B) of two p-dimensional vector
inputs A and B, for use with support vector machines in Section 9.5.3.
Name
K(A,B)
Linear
ATB
Homogeneous polynomial
(𝛾ATB)d
Inhomogeneous polynomial
(𝜙o + 𝛾ATB)d
Gaussian RBF
exp{−𝛾|| A −B ||2}
(𝛾> 0)
Laplace RBF
exp{−𝛾|| A −B ||}
(𝛾> 0)
Cauchy
1
𝜋(1 + || A −B ||2)−1
Sigmoid
tanh(𝜙o + 𝛾ATB)
Thin-plate spline
|| A −B || log(|| A −B ||)
𝛾and 𝜙o are kernel-specific tuning parameters.
Abbreviation: RBF, radial basis function.
Note: || B || = (B2
1 + B2
2 + · · · + B2
p)1∕2 is the L2 norm, extending (9.15).
Notice that some kernels contain additional parameters. For many, a quantity 𝛾is included
as an additional tuning parameter. If no prior choice or external validation data are available
for determining the kernel parameter(s), cross-validation can be employed using the current
training set.
It is worth remarking that most analysts normally avoid adding dimensions to a problem,
because this can have enigmatic consequences with, for example, the curse of dimension-
ality mentioned in Section 7.4.1. The ‘kernel trick’ employed with SVMs often overcomes
this, however, due to computational advantages implicit in application of the kernel transform
(Schölkopf and Smola 2002, Section 2.2).
In R, a number of external packages provide SVMs in one form or another. These include
the svm() function in the e1071 package, the ksvm() function in the kernlab package, and for
regularization path analysis, the svmpath() function in the svmpath package. Karatzoglou
et al. (2006) give a useful review, include training time benchmarks.
Example 9.5.1 Remote sensing of tree disease (Example 9.3.1, continued). Return to the
oak tree disease data in Example 9.3.1 and now apply SVMs to the classification exercise. As
per the results in Example 9.4.1, restrict attention to the three input variables X2, X3, and X4
from Table 9.9. Also, recode the output to Y = 1 for ‘Diseased’ tree images and Y = −1 for
‘Other’ images.
Consider use of the Gaussian RBF kernel from Table 9.12. Application here is via the
ksvm() function in the external kernlab package. Given the ±1 category outcomes in the
vector Y, the function requires construction of a corresponding category factor, along with a
matrix of the input variables. Sample R code is
> Yfac <- factor( Y, levels=c(1,-1), labels=c(‘Diseased’,‘Other’) )
> train.mtx <- as.matrix( cbind(X2,X3,X4) )
To set the cost parameter, a search over the range C = 10−2, 10−1, 100, 101, 102 with the
kernel parameter 𝛾initially fixed at 1 (not shown) indicates that training set misclassification
error can be driven to zero with C as low as 1. Note that a training error of zero need not
be optimal: the underlying classification rule may be overfitting the training set and may not
perform well on future test data (as seen later in this example). Nonetheless, it serves here

330
STATISTICAL DATA ANALYTICS
as a useful starting point, so set C= 1. With this, to select 𝛾for the Gaussian RBF, the kern-
lab package offers an internal estimation scheme based on the package’s sigest function.
This estimates the kernel parameter from the empirical quantiles of || Xi −Xj || (i ≠j) and is
implemented via the kpar=‘automatic’ option. Employing these settings in the full ksvm
command gives
> require( kernlab )
> train.ksvm <- ksvm( x=train.mtx, y=Yfac, scaled=F, type=‘C-svc’,
kernel=‘rbfdot’, kpar=‘automatic’, C=1 )
where the R object train.ksvm is assigned the SVM classifier information based on the
training data. The various options operate as follows:
• The matrix of input variables is specified via the x= option and the corresponding vector
of category factors is specified via the y= option.
• scaled=F instructs R to refrain from centering and scaling the various data vec-
tors/matrices. (The default is scaled=T.) Such scaling can be useful, but for simplicity,
it is overridden here.
• type=‘C-svc’ institutes the basic SVM classification scheme described in this section.
(The function offers a variety of SV methods for more complex classification and regres-
sion settings.)
• The Gaussian RBF kernel is chosen via kernel=‘rbfdot’, with kpar=‘automatic’
calling for automated section of 𝛾, as noted above. To specify a predetermined value
for 𝛾, say, 𝛾= 1, use kpar=list(sigma=1). (The function’s notation for 𝛾in the
Gaussian RBF is sigma.)
• C=1 sets the value for the cost parameter at C= 1. (This is also the default.)
The resulting classification results can be determined from components of the train.ksvm
object. Note that ksvm objects are rendered using R’s S4 object system, as opposed to the
older S3 objects primarily seen herein. By default, S4 components are extracted with the @
operator and do not exclusively use the $ operator; see help(ksvm) and help(‘@’). For
instance, the internally chosen value for the kernel parameter when C = 1 is available in
> train.ksvm@kernelf@kpar$sigma
which is found here to be 𝛾= 2.4657 × 10−4. The corresponding misclassification rate is
found from
> train.ksvm@error
as 0.004148.
The confusion matrices for both the training and the test data can be calculated via the
following sample commands:
> YhatSVM.train <- predict( object=train.ksvm, newdata=train.mtx )
> table( YhatSVM.train, Yfac )
> testY <- factor( Y0, levels=c(1,-1), labels=c(‘Diseased’,‘Other’) )
> test.mtx <- as.matrix( cbind(X02,X03,X04) )
> YhatSVM.test <- predict( object=train.ksvm, newdata=test.mtx )
> table( YhatSVM.test, testY )

SUPERVISED LEARNING: CLASSIFICATION
331
where the notation for the test data variables is taken from Example 9.4.1. The results
appear in Table 9.13. We see that further improvement in overall training accuracy
(4321/4339 = 99.6%) and misclassification error (0.4%, above) are evidenced, relative to
their counterparts in Examples 9.3.1 and 9.4.1. Improvements are also seen with the rates for
the test data; in particular, ‘Diseased’ image classification is clearly strengthened.
Table 9.13
Confusion matrices from SVM classification using Gaussian RBF with C = 1
and 𝛾= 2.4657 × 10−4 in Example 9.5.1.
Training set
Test set
Observed
Observed
Diseased
Other
Row total
Diseased
Other
Row total
Diseased
56
0
56
107
3
110
Predicted
Other
18
4265
4283
80
310
390
Column total
74
4265
4339
187
313
500
Table 9.14 summarizes the results from these various analyses, including pertinent com-
parisons for the sensitivity 𝛾1, that is, the true positive rate. Recall that 𝛾1 was felt to be
an important target measure with this application. For these remote sensing data, at least,
improvements are seen with all measures as the classification strategy moves from the k-NN
to SVM methods.
Table 9.14
Comparison of summary analytic measures for Oak Tree Disease data.
Training set
Classification approach
Accuracy
Misclassification error
Sensitivity
k-NN (Example 9.3.1)
0.991
0.009
0.459
Classification tree∗(Example 9.4.1)
0.993
0.007
0.676
SVM∗(Example 9.5.1)
0.996
0.004
0.757
Test set
Classification approach
Accuracy
Misclassification error
Sensitivity
k-NN (Example 9.3.1)
0.638
0.362
0.032
Classification tree∗(Example 9.4.1)
0.768
0.232
0.396
SVM∗(Example 9.5.1)
0.834
0.166
0.572
∗Pruned to employ only X2, X3, and X4 input variables from Table 9.9.
Finally, it is worth returning to the initial SVM fit with C = 𝛾= 1 to illustrate issues with
overfitting. Recall that when applying a Gaussian RBF kernel at these settings, the misclas-
sification rate was exactly 0%. This can be seen in the corresponding confusion matrices for
both the training and test data given in Table 9.15. (The table was constructed using similar
R commands as those that produced Table 9.13.) Therein, the training data are fit perfectly:

332
STATISTICAL DATA ANALYTICS
Table 9.15
Confusion matrices from SVM classification using Gaussian RBF kernel with
C = 𝛾= 1 in Example 9.5.1.
Training set
Test set
Observed
Observed
Diseased
Other
Row total
Diseased
Other
Row total
Diseased
74
0
74
1
0
1
Predicted
Other
0
4265
4265
186
313
499
Column total
74
4265
4339
187
313
500
overall accuracy is 100% and no misclassifications occur. The results for the test data are
less encouraging, however. The accuracy drops precipitously to 314/500 = 62.8%, with com-
plementary increase in misclassification error to 37.2%. Perhaps more daunting, sensitivity
plummets from 100% to
1
187, barely one-half of 1%! This is classic overfitting: the SVM
classifier has overtrained and has locked in patterns from the training data. It fails miserably,
however, to predict forward from these on to the test data. The diseased-tree images are par-
ticularly misevaluated. By moving, in this case, to a smaller kernel parameter 𝛾, the classifier
generates a few more training misclassifications and also vastly improves its ability to predict
the separate test data outcomes.
Exercise 9.19 explores further use of the SVM classifier with these data.
◽
Although the discussion here has focused on binary classification with Q = 2, strategies do
exist for applying SVMs in the multiclass setting with any Q ≥2. Popular are manipulations
where multiple SVMs are constructed and then compared in a pairwise or compound manner.
Observations are then assigned into a class based on some optimal containment measure. For
more, see Clarke et al. (2009, Section 5.4.10) and James et al. (2013, Section 9.4).
SV methods can also be applied when the response Y is quantitative instead of categorical,
similar to the case with classification trees and CART models in Section 9.4.4. This leads to
a form of support vector regression. The error structure is (re)developed to view the linear
regression function as a kind of decision boundary, to which SV-based optimization can be
adapted (see Hastie et al. 2009, Section 12.3.6).
Modern applications of SVMs in classification and regression are broad and evolving;
they cannot be captured easily by the short introduction given here. Besides the various sources
referenced throughout this section, interested readers may benefit from the further expositions
given in Moguerza and Munoz (2006) or Izenman (2008, Chapter 11). Also see Dixon and
Brereton (2009) and Kruppa et al. (2014) for comparative studies of various classification
methods, including many of those discussed in this chapter.
Exercises
9.1
The bank marketing study in Example 9.2.1 also queried n = 361 never-married cus-
tomers with a college (‘tertiary’) education who had not been contacted in any previous
marketing campaign. For this cohort, only X = log{Duration} of the call was seen to be
important for predicting whether (Y = 1) or not (Y = 0) a depositor subscribed in the

SUPERVISED LEARNING: CLASSIFICATION
333
new campaign. A selection of the (Xi, Yi) data pairs follows. (Download the complete
set at http://www.wiley.com/go/piegorsch/data_analytics.)
Y = Subscribed:
1
0
0
· · ·
1
1
X = log{Duration}:
6.8648
2.7726
4.6634
· · ·
6.7991
5.8833
(a) Plot Y against X. What does the pattern reveal?
(b) Fit a simple linear, conditional, logistic model to the data: P[Y = 1 | X = x] =
𝜋(x) = 1∕(1 + e−𝛽0−𝛽1x). Find the estimated success probability ̂𝜋(x) and overlay
it on your scatterplot.
(c) Construct the confusion matrix and from it calculate the overall accuracy, misclas-
sification error, sensitivity, and specificity for these data under the logistic discrim-
inant rule.
(d) Estimate the probability 𝜋(log{420}) that a depositor in this cohort will subscribe
after a call lasting 7 min (420 s). Also find a 95% confidence interval for this value.
(e) At what call duration do the bank’s marketers ‘break even;’ that is, how long a call
is needed to meet or exceed a 50% probability of garnering a subscription? (This
is known generically as a median effective stimulus; here it would be a ‘median
effective duration.’) Interpret this quantity within the context of two-class discrim-
ination.
9.2
Suppose a conditional logistic regression based on p = 2 predictors is used for
two-category discrimination/classification with estimated linear predictor ̂𝜂= ̂𝛽0
+ ̂𝛽1x1 + ̂𝛽2x2.
(a) Verify algebraically that classifying Y = 1 when ̂𝜋(x) > 1
2 leads to (9.3).
(b) Derive an expression for the perpendicular distance from any observed point
(x10, x20) to the discrimination boundary.
(c) Can you extend this to the general case with p > 2 predictors in ̂𝜂= ̂𝛽0 + ̂𝛽1x1 +
· · · + ̂𝛽pxp?
9.3
From a bioinformatic study of gene expression profiles in carcinogenesis, Dagliyan
et al. (2011) provided prostate cancer outcomes in n = 102 male patients, along with
their expression data from a variety of potential genetic markers. The response was
binary: Y = 1 if a patient’s prostate tissue was identified as positive for the tumor or
Y = 0 if it was not. Consider here p = 3 genes to represent important predictors of the
tumor outcome as listed in the following selection of data. (Download the complete set
at http://www.wiley.com/go/piegorsch/data_analytics)
Y = tumor positive:
1
1
1
· · ·
0
0
0
x1 = serum protease hepsin (X07732):
203
182
117
· · ·
14
46
38
x2 = ao89h09.x1 (AI207842):
165
109
215
· · · 881
809
240
x3 = GSTM4 (M96233):
54
34
29
· · ·
96
236
138

334
STATISTICAL DATA ANALYTICS
(a) Construct a 3 × 3 scatterplot matrix that pairs all three predictors. Code the plotted
points as to whether or not the patient had a positive tumor outcome. Comment on
any observed patterns.
(b) Fit a conditional multiple logistic model to these data with all three predictor vari-
ables. Use this to build a linear discriminant function for classifying cancer in this
population. In particular, give the equation of the estimated discriminant function.
Also find the confusion matrix and from it calculate the overall accuracy, misclas-
sification error, sensitivity, and specificity of the discriminant rule.
(c) Find the predicted probabilities ̂𝜋(x1, x2, x3) for each patient and plot them against
the observed value of Y. Comment on the pattern in the plot.
(d) From the fitted logistic model, estimate the probability that a new patient will be
classified with a positive tumor outcome if he/she presents the following predictor
values: x1 = 100, x2 = 300, x3 = 50. Also include a 95% confidence interval for
this value.
9.4
A well-known database for illustrating classification with a binary outcome involves
diabetes diagnoses among the Native America Pima (Akimel O’odham) peo-
ple in central-southern Arizona (Smith et al. 1988). The response was Y = 1 if
diabetes was diagnosed or Y = 0 if it was not, among n = 724 female Pimas at
least 21 years old. Following Myatt and Johnson (2009, Section 4.5), consider
p −1 = 5 predictor variables from the larger database as listed in the following
(x4 = Diabetes Pedigree is a score incorporating family history of diabetes; higher
values suggest higher risk), along with a selection of the data after curation to
remove missing or ambiguous observations. Download this complete set of data at
http://www.wiley.com/go/piegorsch/data_analytics.
Y = Diabetes:
0
0
0
· · ·
1
0
1
x1 = Age (years):
21
32
22
· · ·
33
31
25
x2 = Glucose (mg/dL):
102
87
90
· · ·
137
197
180
x3 = Body mass index:
25.1
23.2
27.3
· · ·
43.1
36.7
59.4
x4 = Diabetes pedigree:
0.078
0.084
0.085
· · ·
2.288
2.329
2.42
x5 = Blood pressure (diastolic):
52
80
70
· · ·
40
70
78
(a) Construct a 5 × 5 scatterplot matrix that pairs all five predictors. Code the plotted
points as to whether or not the subject had a diabetes diagnosis. Comment on any
observed patterns.
(b) Fit a conditional multiple logistic model to these data with all five predictor vari-
ables. Use this to build a linear discriminant function for classifying diabetes in this
population. In particular, give the equation of the estimated discriminant function.
Also find the confusion matrix and from it calculate the overall accuracy, misclas-
sification error, sensitivity, and specificity of the discriminant rule.
(c) Find the predicted probabilities ̂𝜋(x1, x2, x3, x4, x5) for each subject and plot them
against the observed value of Y. Comment on the pattern in the plot.
(d) From the fitted logistic model, estimate the probability that a Pima female will
be classified with diabetes if she presents the following predictor values: x1 = 30,

SUPERVISED LEARNING: CLASSIFICATION
335
x2 = 110, x3 = 44.4, x4 = 0.500, and x5 = 85. Also include a 95% confidence inter-
val for this value.
9.5
Suppose a logistic regression model using (9.1) is employed for two-category
classification with estimated linear predictor
̂𝜂= ̂𝛽0 + ̂𝛽1x1 + · · · + ̂𝛽pxp. Verify
algebraically that the Wald confidence interval for 𝜂satisfying P(̂𝜂−z𝛼∕2se[̂𝜂] ≤𝜂≤
̂𝜂+ z𝛼∕2se[̂𝜂]) = 1 −𝛼can be manipulated into the confidence statement
P
[
1
1 + exp{−̂𝜂+ z𝛼∕2se[̂𝜂]} ≤𝜋(x) ≤
1
1 + exp{−̂𝜂−z𝛼∕2se[̂𝜂]}
]
= 1 −𝛼.
9.6
If the logistic model in (9.1) is replaced by a probit regression, as in Section 8.3.1, what
eventual changes occur in (9.2) and (9.3)?
9.7
The Bank Marketing Study in Example 9.2.1 also queried n = 449 married customers
who had not completed any secondary school education and who had not been con-
tacted in any previous marketing campaign. For this cohort, the variables X1 ={Age of
depositor} and X2 = log{Duration of marketing phone call (in s)} were again felt to be
important for predicting whether (Y = 1) or not (Y = 2) a depositor subscribed in the
new campaign. A selection of the data is given as follows. (Download the complete set
of data at http://www.wiley.com/go/piegorsch/data_analytics.)
Y = Subscribed:
2
2
2
· · ·
2
1
X1 = Age (years):
30
55
26
· · ·
37
60
X2 = log{Duration (s)}:
4.3694
5.5094
6.0521
· · ·
6.2422
6.7044
(a) Plot the two predictor variables and code the plotted points as to whether or not the
customer subscribed. Comment on any observed patterns.
(b) Apply a LDA to these data, as in Section 9.2.1. Use this to build a linear dis-
criminant function for classifying subscribers in this cohort. In particular, give the
equation of the estimated discriminant function. Overlay this function on your plot
in Exercise 9.7a and remark on any pattern.
(c) Find the corresponding confusion matrix and from it calculate the overall accuracy,
misclassification error, sensitivity, and specificity of the discriminant rule.
(d) From the LDA, predict the subscription status of a future, 40-year-old customer
(X1 = 40) if contacted for the following durations: 3 min (180 s), 4 min (240 s),
5 min (300 s), 6 min (360 s), 7 min (420 s), and 8 min (480 s). Display or plot these
as a function of the duration time. Comment on the result.
9.8
Return to the urban vulnerability data in Example 9.2.2.
(a) Construct a confusion matrix for the classifications given in the example. (Extend
the 2 × 2 matrix from Table 9.2 into a 3 × 3 matrix for these data.) From this cal-
culate the overall accuracy and the misclassification error rate. Comment on the
results.
(b) The supplied database contains the longitude and latitude for each city.
If you have access to geoinformatic mapping software – the external map

336
STATISTICAL DATA ANALYTICS
package in R is a rudimentary version – plot the 132 cities on a US map and
overlay the predicted class assignments from the example (found, e.g., in pre-
dict(cities132.lda)$class). Use different colors or symbols to distinguish
the three predicted classes. Comment on the patterns that are observed.
9.9
Under the multivariate normal model, verify the equalities in (a) (9.8) and (b) (9.9).
(Hint: Recall that the linear combination aTb is a scalar, and therefore, aTb = (aTb)T =
bTa.)
9.10
Kohavi (1996) reported on a US Census Bureau database of income (in 1994 $) among
US residents. Studied was propensity to exceed an annual income of $ 50 000. Focus
here is on a cohort of 17 936 males employed in private industry or self-employed.
Following suggestions in the study, a training set comprising two-thirds of these data
was randomly created, with sample size n = 11 957. The categorical response was set
to Y = 1 if a subject’s income exceeded $ 50 000 and Y = 2 if not. Quantitative input
variables for consideration are X1 ={Age}, X2 ={Hours worked/week}, X3 ={Years
of education}, and X4 ={Capital gains (or losses) from previous year}. A sample of
the training data follows:
Training set
Y = Exceed $ 50K:
2
2
2
· · ·
1
2
2
X1 = Age (years):
35
30
31
· · ·
27
54
46
X2 = Hours worked:
48
40
45
· · ·
42
40
40
X3 = Education (years):
13
9
9
· · ·
7
9
13
X4 = Capital gain/loss ($):
0
0
0
· · ·
0
0
−1590
This left a test set of the remaining no = 5979 data points, a sample of which fol-
lows (download both complete data sets at http://www.wiley.com/go/piegorsch/data
_analytics):
Test set
Y0 = Exceed $ 50K:
2
1
2
· · ·
2
1
2
X01 = Age (years):
39
54
25
· · ·
49
81
57
X02 = Hours worked:
80
40
45
· · ·
55
45
25
X03 = Education (years):
9
9
9
· · ·
9
2
10
X04 = Capital gain/loss ($):
0
0
3325
· · ·
0
0
0
(a) Apply a k-NN analysis to these data. Set k = 5 and employ Euclidean distance.
Scale the input variables in X by their standard deviations before any calculations.
Construct the confusion matrices for both the training data and the test data and
from these report the associated accuracy and misclassification error rates.
(b) Repeat your analysis with k = 3 and comment on any differences.
9.11
If applying a k-NN analysis with k = 1, what form would you expect the confusion
matrix to take when the training data is applied to itself (i.e., when you ‘predict’ the

SUPERVISED LEARNING: CLASSIFICATION
337
categories of the training observations using only the same observations to train the
learning process)? Can you imagine a setting where this might not occur?
9.12
Return to the k-NN classification for the oak tree disease data in Example 9.3.1 and
explore how the (a) misclassification error and (b) sensitivity change for the test data
as k varies over a range of values, say, k = 3, 5, 15, 25. Comment on your results.
9.13
One can explore differences among the various impurity measures used to build classi-
fication trees in Section 9.4.1 using graphical comparisons (Hastie et al. 2009, Section
9.2.3). For simplicity, let Q = 2. As a function of 𝜋1 over the range 0 < 𝜋1 < 1, plot
the Gini measure 2𝜋1(1 −𝜋1), the entropy −1
2𝜋1log2(𝜋1) −1
2(1 −𝜋1)log2(1 −𝜋1), and
the misclassification error
𝜋1I[0, 1
2 ](𝜋1) + (1 −𝜋1)I( 1
2 ,1](𝜋1),
where I(⋅) is the indicator function from (2.20). (The entropy here is scaled to reach
the same maximum as the others.) Comment on the patterns you observe.
9.14
Return to the following data sets and view each as a standalone set of training data.
Construct a classification tree from the associated collection of input variables. Operate
with the Gini impurity measure. Start with a full tree and use L-fold cross-validation to
help to identify a complexity parameter for pruning the tree. To select the complexity
parameter 𝛼, appeal to a plot of the cross-validation error versus 𝛼(available, e.g., via
the plotcp() function in the external rpart package). Employ the one-standard error
rule: select the largest 𝛼whose cross-validation error does not exceed the minimum
error plus its standard deviation. Plot the pruned tree. Also find the confusion matrix
for these training data, and calculate the consequent accuracy and misclassification
rates.
(a) The bank marketing data in Example 9.2.1. Set L = 8.
(b) The Pima diabetes data in Exercise 9.4. Set L = 5.
9.15
Mansouri et al. (2013) reported a study of quantitative structure–activity relationships
(QSARs) among 1055 chemical molecules, in order to classify whether or not the
chemicals were readily biodegradable. The category response was set to Y = 1 if a
chemical’s biodegradation status was viewed as ‘ready’ and to Y = 0 if not. An associ-
ated set of quantitative input variables is given here as X1 = {Number of heavy atoms in
the molecule}, X2 = {Number of substituted benzene carbons}, X3 ={Number of ring
tertiary carbons}, and X4 ={Intrinsic state pseudoconnectivity (ISP) index}. A training
set comprising 705 chemicals from the full data set was randomly generated, a sample
of which follows:
Training set
Y = Ready status
0
1
0
· · ·
0
0
1
X1 = Heavy atoms
0
0
0
· · ·
2
0
0
X2 = Substituted benzene C
2
0
0
· · ·
0
18
0
X3 = Ring tertiary C
0
0
2
· · ·
0
0
1
X4 = ISP index
0.004
0.425
−0.007
· · ·
−0.022
0.000
0.004

338
STATISTICAL DATA ANALYTICS
This left a test set of the remaining no = 350 data points, a sample of which fol-
lows (download both complete data sets at http://www.wiley.com/go/piegorsch/data_
analytics):
Test set
Y0 = Ready status
1
1
1
· · ·
0
0
0
X01 = Heavy atoms
0
2
1
· · ·
1
0
2
X02 = Substituted benzene C
0
0
0
· · ·
3
5
9
X03 = Ring tertiary C
0
0
0
· · ·
0
0
0
X04 = ISP index
0.011
−0.271
0.000
· · ·
−0.025
0.000
0.000
(a) Construct a classification tree from the training data, as per the methods in Section
9.4. Operate with the Gini impurity measure. Start with a full tree and use 10-fold
cross-validation to help to identify a complexity parameter for pruning the tree.
Plot the resulting tree. Also find the confusion matrix for both the training data and
the test data, and calculate the consequent accuracy and misclassification rates.
(b) Repeat your analysis with five-fold cross-validation and comment on any differ-
ences.
9.16
Return to the pruned classification tree for the oak tree disease data in Example 9.4.1.
(a) Verify the indication in the example that fitting only the three input variables X2,
X3, and X4 produces the same pruned classification tree (at complexity parameter
𝛼= 0.025) as given in Figure 9.7.
(b) Reproduce the three-dimensional scatterplot in Figure 9.6 and study the pattern by
changing the viewing/perspective angle. If your 3D plotter has interactive capabil-
ities (as, e.g., in the external R package rgl), rotate the display in real time to better
visualize the data cloud.
(c) Repeat the 3D visualization in Figure 9.6 now with the test data. Comment on any
similarities or differences in the 3D plot. In particular, do the test data lie in a similar
range of X2, X3, and X4 values as the training data? If not, and based on the patterns
you observe in both plots, how might this affect the classification analysis?
(d) One might be tempted to use hypothesis tests to compare the test data sensitivity of
the pruned classification tree (74/187) to that from the corresponding k-NN analysis
(6/187) from Example 9.3.1. For practice, conduct such a test; that is, assess the
equality of the two proportions against any difference. Employ the Fisher exact
test from Section 8.3.3. While perhaps informally useful for gauging differences
between these two proportions, the formal inferences here are invalid. Why?
9.17
Return to the urban vulnerability study from Example 9.2.2 and view the collection as
a standalone set of training data.
(a) Construct a classification tree from the two feature variables X1 and X2. (Notice that
this is a multiclass problem with more than two categories.) Operate with the Gini

SUPERVISED LEARNING: CLASSIFICATION
339
impurity measure and use 10-fold cross-validation to help to identify a complexity
parameter 𝛼for pruning. Plot the resulting tree.
(b) Find the confusion matrix associated with the pruned tree, and calculate the con-
sequent accuracy and misclassification rates with these data. Compare these to the
results in Exercise 9.8.
(c) Imitate the display in Figure 9.4 by finding the splits given by the pruned tree in the
(X1, X2) plane and overlaying on them the original data. Label the data points by
their observed categories (1, 2, or 3). Include a label in each separate classification
region demarcating its predicted category. Comment on the differences between
the two graphics.
(d) Identify which of the 132 cities are classified by the tree into the high-vulnerability
category. How does this list compare with Table 9.7?
9.18
Return to the following data sets and apply SVMs to perform the classification exercise
with the associated collection of input variables. Employ a Gaussian RBF kernel; if
you use the ksvm() function in R, find the kernel parameter 𝛾via its automatic section
routine. Set the cost parameter C as instructed in the following.
(a) The bank marketing data in Example 9.2.1, viewed as a standalone set of training
data. Start with a selection of cost parameters: C = 10−1, 100, 101, … , 105. Choose
that C for which the associated misclassification error is a minimum; report the
associated value for 𝛾. Find the resulting confusion matrix for the full data set and
compare it, along with the accuracy and misclassification error, to that from the
classification tree in Exercise 9.14a.
(b) The Pima diabetes data in Exercise 9.4, viewed as a standalone set of training data.
Start with a selection of cost parameters: C = 10−1, 100, 101, … , 105. Choose that
C for which the associated misclassification error is a minimum; report the asso-
ciated value for 𝛾. Find the resulting confusion matrix for the full data set and
compare it, along with the accuracy and misclassification error, to that from the
classification tree in Exercise 9.14b.
(c) The chemical structure–activity data in Exercise 9.15. Start with the training data
and vary the cost parameter over C = 10−2, 10−1, 100, 101, … , 105. Choose that C
for which the associated misclassification error is a minimum; report the associated
value for 𝛾. Find the resulting confusion matrices for the training set and the test set.
Compare these, along with the associated accuracies and misclassification errors,
to those from the classification tree results in Exercise 9.15.
9.19
Return to the oak tree disease data in Example 9.5.1, and for illustrative purposes, apply
the following kernels from Table 9.12. Use the suggested cost and kernel parameters
as indicated. Find the confusion matrices for both the training set and the test set, along
with the associated accuracies, misclassification rates, and sensitivities. Comment on
any differences from the results seen in the example.
(a) The homogeneous polynomial kernel with 𝛾= 1 and degree d = 2. Set C = 100.

340
STATISTICAL DATA ANALYTICS
(b) The inhomogeneous polynomial kernel with 𝜙o = 𝛾= 1 and degree d = 2. Set
C = 10−4.
(c) The Laplace RBF kernel. In R, use automated estimation for 𝛾via the
kpar=‘automatic’ option within the ksvm() function. Set C = 103 and
report the estimated value of 𝛾.
(d) The sigmoid kernel with 𝜙o = 𝛾= 1. Set C = 1.
9.20
Prove the assertion in Section 9.5.2 that ∑n
i=1 𝜉i∕n is an upper bound on the misclassi-
fication error ∑n
i=1 I(1,∞)(𝜉i)∕n.

10
Techniques for unsupervised
learning: dimension reduction
10.1
Unsupervised versus supervised learning
In contrast to the supervised learning procedures presented in Chapters 6–9, the unsupervised
learning paradigm applies when only input/feature data are available; no formal ‘output’ vari-
ables are derived from the inputs. In effect, this is ‘learning without a teacher’ where the
discovery process is self-organized, and learning occurs wholly from the input information
(Kantardzic 2003, Section 4.3). In effect, the goals shift to the discovery of hidden or latent
structure among unlabeled groupings, classes, or clusters.
For unsupervised learning, the inputs are themselves treated as the observations. Often
they are multivariate in nature, with data recorded on p > 1 different variables of interest,
x1, x2, … , xp. When p is very large, the terminology often refers to ‘high-dimensional data
analytics.’ However, the methods described in the following will apply to any p > 1.
Suppose each variable is measured on a sample or training set of n subjects, produc-
ing individual column vectors xj = [x1j x2j … xnj]T (j = 1, … , p), and collected into the data
matrix X = [x1 · · · xp]. (Unsupervised learning calculations rely heavily on matrix opera-
tions; where needed, see the vector and matrix refresher in Appendix A.) It will be useful to
center the feature/input variables by their means xj = ∑n
i=1 xij∕n, producing the corrected vec-
tors xj −xjh, where h = [1 1 · · · 1]T is a vector of ones. Next, arrange these into the column
matrix of (mean-corrected) values
X∗= [x1 −x1h · · · xp −xph] =
⎡
⎢
⎢
⎢⎣
x11 −x1
· · ·
x1p −xp
x21 −x1
· · ·
x2p −xp
⋮
⋱
⋮
xn1 −x1
· · ·
xnp −xp
⎤
⎥
⎥
⎥⎦
(10.1)
as in (A.11).
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

342
STATISTICAL DATA ANALYTICS
Despite the unsupervised aspect of the analysis, statistical models are still required for
describing the characteristics of X or X∗. It is under these models that the original observations
are analyzed for their underlying structure, and features are extracted. In this chapter, ana-
lytic procedures are described for the problem of dimension reduction. (Chapter 11 presents
methods of unsupervised learning for the alternative issues of clustering and association.)
Dimension reduction aims to abridge the p observed variables into a smaller, more manage-
able group from which latent structures may be more apparent. As a first step, this can be
accomplished simply by studying each set of variables and applying domain-specific knowl-
edge. For example, if a study includes the variables x1 = weight (lbs) and x2 = height (in.)
among the recorded variables, removal of one or the other may be useful because these are
often highly correlated/multicollinear, as in Section 7.1.4. (Or perhaps, replacement with a
single integrating variable such as the body mass index (BMI), x3 = BMI = 703.07x1∕x2
2,
would be appropriate instead.) Once this level of domain knowledge is exhausted, however,
statistical procedures can be applied to possibly reduce the dimension of the problem further
and advance the discovery process.
10.2
Principal component analysis
In practice, one often encounters substantial intercorrelation and multicollinearity among
an original set of inputs x1, x2, … , xp, producing overlapping information (as in the
height-vs-weight example above). The focus of the dimension-reduction effort involves
reduction of the variables under consideration to a smaller number, q < p, of representative
indices or consolidating measures. We wish to retain as much of the original information in
the xjs as possible, but with no intercorrelation among the new variables.
One of the oldest and most popular methods of statistical dimension reduction based on
correlations among the data is known as principal component analysis (PCA). The technique
was presented by Pearson (1901) and also given independently by Hotelling (1933), who
developed the implementation in greater detail and from whom the name apparently derives.
For a short historical review, see the introduction in Abdi and Williams (2010).
10.2.1
Principal components
As a device for reducing dimension, PCA transforms the original xjs, or more typically
their mean-centered versions in x∗= [x1 −x1 · · · xp −xp]T, to a reduced set of variables
y1, y2, … , yq. The strategy is to calculate yj as a linear combination, yj = uT
j x∗, for some
suitably chosen coefficient vector uj = [u1j u2j · · · upj]T, j = 1, … , q. The yjs are formulated
to provide the maximum amount of explainable variation for describing information in the
x∗
j s, while also exhibiting zero correlation. Note, however, that one can always increase
variation in yj simply by multiplying uj by a constant greater than 1. Thus some additional
constraint on the maximization is required. A standard choice is to normalize the coefficients
such that their sum of squares is 1: uT
j uj = 1.
The transformed linear combinations, yj = uT
j x∗, produced in a PCA are known as the
principal components (PCs). These are constructed in a stepwise manner:
• The first PC, y1 = uT
1x∗, is chosen to maximize explainable variation under the
constraint that uT
1u1 = 1.

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
343
• Next, the second PC, y2 = uT
2x∗, is taken to maximize the remaining variation such that
uT
2u2 = 1 and such that y1 and y2 are uncorrelated. This is achieved by imposing the
additional constraint that uT
1u2 = 0 (an ‘orthogonality’ restriction).
• The third PC is y3 = uT
3x∗where uT
3u3 = 1 and uT
1u3 = uT
2u3 = 0.
• The process continues with y4 = uT
4x∗, and so on, until a sufficient number, q, of yjs
have been constructed. Each yj explains a decreasing amount of variation, while being
mutually uncorrelated with all that came before.
To visualize the matter, consider the following illustration with p = 2.
Example 10.2.1 PCs in two dimensions: husbands’ and wives’ ages.
Similar to the
observations on husbands’ and wives’ heights in Exercise 3.12, additional data in that
study were recorded on the couples’ ages in years. (Ages were not recorded for some
wives, so the data are restricted to n = 170 couples.) Let x1 be the wives’ ages and x2 be
the husbands’ ages. The data appear in Table 10.1. (As previously, a selection of only the
smallest and largest measurements is given in the table. The complete data are available at
http://www.wiley.com/go/piegorsch/data_analytics.) For a study such as this with p = 2, one
would not normally consider a PCA to reduce two variables to a single, summary, linear
combination. (Although, it is not unheard-of.) The example can help with illustrating the
concepts, however.
Figure 10.1 plots the husbands’ ages against the wives’ ages on the original, uncentered
scale. The plot indicates a tight, increasing trend and a high, positive correlation: r = 0.9386
from (3.9). As might be expected, younger women tend to marry younger men, and older
women tend to marry older men. Superimposed on the scatterplot are the two PCs: the largest
(first) is the linear combination of x1 and x2 that maximizes the variance of its projection on to
the (x1, x2) plane, while the smallest (here, the second) ‘sweeps’ up the remaining variation.
Notice that the two PC lines are perpendicular, that is, orthogonal in the plane. This is expected,
because the two variables are constructed to be uncorrelated.
In the figure, the lengths of the PC lines are scaled relative to the component standard
deviations to represent the information they provide: the first PC line is far longer and stretches
across a more substantial portion of (x1, x2) space. The limited amount of information in the
second PC direction indicates that little additional, uncorrelated information is being supplied
once the first PC has been constructed. Were the goal to find a single measure that captured
maximal information as represented by variation in the data cloud, the first PC would be the
solution.
◽
For most applications of PCA, the original data typically represent a variety of different
outcomes which, for that matter, often exist across different, arbitrary measurement scales
(e.g., length as inches or centimeters). Such differences will affect the nature of the vari-
ances and covarainces/correlations in the original data and thus impact the results of the PCA.
Table 10.1
Selected data pairs with x1 = {Wife’s age} and x2 = {Husband’s age}, from a
larger set of n = 170 married couples.
x1 = Wife’s age (years)
18
21
21
21
22
23
· · ·
64
64
x2 = Husband’s age (years)
20
20
22
27
20
31
· · ·
63
64
Source: Hand et al. (1994, Section 231).

344
STATISTICAL DATA ANALYTICS
10
20
30
40
50
60
70
10
20
30
40
50
60
70
Wife’s age (years)
Husband’s age (years)
Figure 10.1
Visualization of principal component lines from Example 10.2.1 for husband-
wifes age data in Table 10.1. The longer line represents the first principal component and the
shorter line represents the second (and last) principal component. Source: Data from Hand
et al. (1994).
Indeed, if some of the x∗
j s have particularly large variances, these will tend to dominate the
initial PCs (Everitt 2005, Section 3.2). This may or may not lead to difficulties when inter-
preting results of the final PCA, but it nonetheless warrants attention. To avoid vagaries with
wildly differential variances and problems with arbitrary units of measurement, a modifica-
tion usually applied in practice scales each x∗
j to have unit variance before constructing the
PCs, similar to previous suggestions in Section 7.4. That is, the original xijs are standardized to
z-scores as in (3.7): zij = (xij −xj)∕sj = x∗
ij∕sj, i = 1, … , n; j = 1, … , p. (Some authors warn
that the standardization treats every original variable with a kind of equal ‘attention,’ which is
itself an arbitrary determination. If the scaling robs certain variables of a stature that is impor-
tant to the analysis, then the decision to standardize the observations should be reconsidered.)
Unless otherwise indicated, the default here is to operate with the standardized data zij. These
then make up a target data matrix Z with elements zij.
10.2.2
Implementing a PCA
To calculate the PCs as described earlier, the method essentially implements sequenced, con-
strained optimization. The constraints – sums of squares equals 1, mutual orthogonality –
apply to the coefficient vector u. This can be achieved with an optimization method known as
Lagrange multipliers (Hughes-Hallett et al., 2013, Section 15.3), similar to the construction
in Section 9.2. As there, the technical details exceed our scope; interested readers may refer,
for example, to Clarke et al. (2009, Section 9.1). The solution employs the eigenvalues and

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
345
eigenvectors of the sample correlation or covariance matrices. (For a refresher on eigenanaly-
sis, see Section A.5.) Note that because Z contains the standardized data, its covariance matrix
is identical to its correlation matrix and simplifies to R = ZTZ∕(n −1) (see Section A.7).
The first PC is built using u1 as the orthonormal eigenvector corresponding to the first
(largest) eigenvalue, 𝜆1, of R. (If working instead with the centered data in X∗, use the covari-
ance matrix (X∗)TX∗∕(n −1).) The next PC is built with u2 as the eigenvector corresponding
to the next-largest eigenvalue, 𝜆2, of R. The process continues: build the jth PC with uj as the
eigenvector corresponding to the jth eigenvalue 𝜆j of R, and so on.
Since the correlation matrix for Z is real valued and symmetric, its eigenvalues and eigen-
vectors can be found via appeal to the singular value decomposition (SVD) from Section
A.6.4 or, equivalently here, the spectral decomposition from Section A.6.2. These return the
(ordered) eigenvalues in a diagonal matrix 𝚲= diag{𝜆1, 𝜆2, … , 𝜆p}, along with their corre-
sponding eigenvectors in a p × p orthogonal matrix U = [u1u2 · · · up]. The PCs are then calcu-
lated as linear combinations of the standardized variables: y1 = uT
1z, y2 = uT
2z, … , yp = uT
pz.
This is sometimes called the Hotelling transformation, or also the Karhunen–Loéve transfor-
mation, of z. For computing purposes, R can conduct a PCA via its prcomp() function (see
Example 10.2.2). The alternative princomp() function may also be used, although it can be
numerically less accurate because it does not take advantage of the SVD.
Beyond supplying constituent calculations for each PC, this eigendecomposition pos-
sesses other useful features. In particular, the variance of the jth PC is equal to the jth eigen-
value 𝜆j. With this, the total variation in the sample is equal to the sum of all p eigenvalues:
𝜆+ = ∑p
j=1 𝜆j = p. (As the data are standardized, the sample variance of each variable is
always 1. Thus the sum of all p variances is simply p. If working instead with the centered
data in X∗, the sum of the corresponding eigenvalues will equal s2
1 + s2
2 + · · · + s2
p.) Thus the
percentage variance explained by the jth PC is 𝜆j∕𝜆+. This can be helpful in selecting the final
PCs for dimension reduction.
Another feature of the eigendecomposition is that the eigenvectors uj are forced to satisfy
uT
j uj = 1. Thus their individual coefficients must lie between −1 and 1. This allows the kth
coefficient ukj in the jth PC yj to quantify the amount and direction the kth original, standard-
ized variate zk contributes to yj. Sometimes called loadings of each PC (although this invites
confusion with similar terms in factor analysis from Section 10.3), these coefficients are use-
ful for interpreting how the original data enter into the PCs. Coefficients near zero indicate
limited contributions by the corresponding zks, while coefficients near ±1 indicate strong con-
tributions. Differences in sign also provide contrast: variables with different signs contribute
with opposite effect to the overall component.
Notice that as constructed, the decomposition returns a full complement of p PCs yj. For
the purposes of dimension reduction, only the first q PCs of this collection are retained. The
available reduction can be substantial: obviously, some information will be lost for q < p,
but hopefully a large percentage of the explainable variation from the original data will be
contained in the few retained PCs.
A number of different strategies have evolved to formalize selection of q. One can
• Choose only those PCs whose 𝜆j exceeds the average 𝜆= 𝜆+∕p. When using the
standardized data in Z to perform the PCA, 𝜆+ = p so 𝜆is just 1. Thus, one would
retain components whose eigenvalue exceeds 1 (Kaiser 1958), because they represent
transformations whose variances exceed those of any original (standardized) variables.

346
STATISTICAL DATA ANALYTICS
For more-accurate retention in practice, however, Jolliffe (1972) recommended
dropping the cutoff to about 0.7.
• Choose q such that the total retained percentage variation, ∑q
j=1 𝜆j∕𝜆+ exceeds a
large, pre-set value. The pre-set is typically in the range 70–90%, with 80% a popular
midpoint.
• Plot the 𝜆js against j to visualize the decrease in explainable variation. If, as is
common, the plot flattens after a certain index j, a diminishing return in accumulated
variation occurs at that point, often called an ‘elbow’ or ‘knee,’ in the plot. The
remaining components beyond that j are discarded. The graphic is known as a scree
plot – associating the flat portion of the plot to flattened rubble at the bottom of a
sharply sloping mountain – after a suggestion by Cattell (1966).
No single one of these criteria is uniformly favored in the literature, and analysts often combine
them informally for final selection of the retained PCs.
Applied to the data, the PCA produces PC scores for each observation when calculated
from the retained q components, as linear combinations of the rows of Z:
Y = [y1 y2 · · · yq] = Z[u1 u2 · · · uq].
To visualize the extant relationships between the PCs, one can plot the scores pairwise via, for
example, a scatterplot matrix (see Section 4.2.3) or perhaps build 3D plots of various triplets.
Indeed, another aim of the PCA is to build informative graphics of the interrelationships as
an aid to feature extraction; the first few PC scores often provide a useful low-dimensional
mapping of how the data points relate to one another (Everitt 2005, Section 3.3).
Example 10.2.2 PCA for California natural hazards. To study the socioeconomic effects
of natural hazards – floods, wildfires, and so on – social and environmental scientists collect
various forms of data after such events. An important endpoint is property losses, that is,
financial losses to property after a natural hazard such as a flood impacts a community.
Property loss data from a variety of natural hazard events are available at the Spatial Haz-
ard Events and Losses Database for the US (SHEDLUS): http://www.sheldus.org (Hazards &
Vulnerability Research Institute 2013). Table 10.2 presents selected SHELDUS data on eco-
nomic losses (in $100000) from 3509 hazard events over the period 2000–2012 in the US state
of California, collected at the per-county level. (As previously, only a selection of the data is
given in the table. The complete set of data is available at http://www.wiley.com/go/piegorsch
/data_analytics.) The table lists losses from p = 7 variables corresponding to a variety of com-
mon hazards. (Because geographic considerations affect a county’s potential for loss, the data
are somewhat sparse. For example, losses due to coastal/tsunami events are not evident – or
likely – for interior counties such as Alpine or Yuba. Also, earthquakes losses are not included,
because they represent an extraordinary, and uncommon, form of economic impact.)
To explore potential interrelationships among the seven hazards as relates to property
losses, consider a PCA analysis on the p = 7 variables. (Not surprisingly, the $-losses skew
heavily to the right, so we conduct the analysis on xj = log10{jth county’s loss}, which remain
positive throughout the data. A zero reported loss is still treated as zero.)
As for any multiple-variable analysis, the first step is to plot the data. Figure 10.2 displays
a scatterplot matrix of the log-transformed values, where a variety of patterns appear between
the pairs of variables. To help to quantify this, Table 10.3 gives the corresponding correlation

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
347
Table 10.2
Selected data on property losses (in $100 000) from specific natural hazards in
California during the period 2000–2012, from a larger set across all 58 California counties.
County
Hazard variable
Alameda
Alpine
· · ·
Los Angeles
· · ·
Yolo
Yuba
Flood
176.0
10.0
· · ·
13.10
· · ·
0.04
98.06
Landslide
63.97
0.005
· · ·
–
· · ·
–
–
Wind
24.22
13.71
· · ·
0.50
· · ·
0.10
0.09
Wildfire
–
1250.0
· · ·
350.03
· · ·
–
–
Severe storm
0.65
–
· · ·
250.0
· · ·
0.22
0.16
Coastal
0.13
–
· · ·
–
· · ·
–
–
Winter weather
0.29
2.40
· · ·
–
· · ·
0.57
0.03
Landslide includes avalanche losses; wind includes tornado losses; severe storm includes hail losses; coastal events
include tsunami losses. Dashes indicate no losses reported.
Source: http://www.sheldus.org.
Table 10.3
Correlation matrix, R, for log10 hazard loss data in Table 10.2; upper triangular
portion only (lower triangle is transpose of upper triangle).
Flood
Landslide
Wind
Wildfire
Storm
Coastal
Winter
Flood
1
0.466
0.283
−0.043
0.327
0.370
0.397
Landslide
1
0.582
0.045
0.119
0.665
0.245
Wind
1
−0.031
0.099
0.342
0.266
Wildfire
1
0.232
−0.119
0.230
Storm
1
0.169
0.253
Coastal
1
0.144
Winter
1
matrix found in R using the cor() function. Moderate correlations are seen throughout most
of the table, the highest of which is between Landslide and Coastal (log) losses at r26 = 0.665.
Some correlations are close to zero and, interestingly, only a few negative correlations appear
(all with the Wildfire variable; see the following text).
To conduct the PCA, we standardize the log-transformed values into z-scores. In R, this
can be managed automatically in the prcomp() function by including its scale.=TRUE
option:
> prcomp( Xmatrix, center=TRUE, scale.=TRUE )
where Xmatrix is the original log-transformed data matrix. The consequent output is summa-
rized in Table 10.4. For purposes of dimension reduction, the upper portion of the table gives
the ordered eigenvalues, 𝜆j. The first two exceed 1.0, which by Kaiser’s criterion qualifies their
corresponding PCs, PC1 and PC2, for retention. Jolliffe’s adjustment drops this cut-off to 0.7,
which would then retain PC3 and PC4 as well. Indeed, the next two eigenvalues, 𝜆3 = 0.88 and
𝜆4 = 0.78, bring the cumulative explainable variation up past 80%, also arguing for retention.
The scree plot, given in Figure 10.3, is less generous, however. A clear ‘elbow’ in the plot is
evidenced at j = 3, suggesting that any PCs past the first three may be of limited value.

348
STATISTICAL DATA ANALYTICS
Flood
0 2 4 6
0 2 4 6 8
0 2 4 6
0
2
4
6
8
0
2
4
6
Landsld
Wind
0
2
4
6
0
2
4
6
8
Wildfire
Storm
0
2
4
6
8
0
2
4
6
Coastal
0 2 4 6 8
0 2 4 6
0 2 4 6 8
0
2
4
6
0
2
4
6
Winter
Figure 10.2
Scatterplot matrix for full log10 hazard loss data from Table 10.2. Originating
variable names listed along diagonal. Source: Data from http://www.sheldus.org.
Suppose we center our attention on the first three PCs. (Exercise 10.1 explores further
graphical visualizations with this PCA output.) In particular, Figure 10.4 gives a 3D drop
plot of the three PC scores (constructed in R using the plot3d() function from the external
rgl package). One highlight in the plot is the point for Trinity county, which ‘distances’ itself
above the rest on the third component scale. Trinity is a rural, mountainous county in northern
California, set inland to the east of coastal Humboldt county. It has no incorporated cities,
few major roads, and, if informal sources are to be believed, no traffic lights! Its 2000–2012
property losses were confined to only five separate events (out of the 3509 for all of California),
the most severe of which was a 2005 landslide that led to almost $14M in losses. (Next was a
2001 wildfire that incurred $3.5M in losses. From Table 10.4, we see that these two hazards
are positively weighted by the third PC.) Otherwise, the county was one of the least affected by

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
349
Table 10.4
Results from principal component analysis (PCA) on log10 hazard loss data
based on Table 10.2.
PC1
PC2
PC3
PC4
PC5
PC6
PC7
Eigenvalues (ordered)
Variance, 𝜆j
2.66
1.37
0.88
0.78
0.61
0.48
0.22
Percentage of explainable
variation
38.0
19.6
12.5
11.1
8.8
6.8
3.2
Cumulative percentage of
variation
38.0
57.6
70.1
81.3
90.0
96.8
100.0
Coefficients (‘loadings’)
Flood
−0.441
0.082
−0.449
0.251
0.225
0.656
−0.229
Landslide
−0.514
−0.221
0.256
−0.192
0.143
0.139
0.738
Wind
−0.414
−0.186
0.420
0.181
−0.687
0.084
−0.326
Wildfire
−0.044
0.636
0.559
−0.342
0.283
0.247
−0.216
Storm
−0.251
0.490
−0.483
−0.414
−0.484
−0.180
0.160
Coastal
−0.441
−0.303
−0.077
−0.409
0.360
−0.441
−0.465
Winter
−0.331
0.416
0.070
0.642
0.193
−0.504
0.094
natural hazards in all of California during the period under study. The potential for knowledge
discovery here is intriguing: does Trinity county’s geography and rural status singularly affect
its susceptibility to losses from natural hazards? Further investigation may be warranted.
The coefficients from Table 10.4 can provide insights on how variables enter into the
PCs. For example, the first PC incorporates essentially all the variables in roughly equal
manner – note the constant sign among the eigenvector’s elements – except for the Wild-
fire variable, which receives a smaller coefficient. Thus PC1 appears to be a general ‘non-
fire’ hazard indicator. The second PC picks up the Wildfire variable – with a large positive
coefficient – and then includes the other variables in a mixed manner. This second PC is a
good example for not overinterpreting small differences between coefficients (Everitt 2005,
Section 3.3); past the strong coefficient for Wildfire, interpreting how the other variables enter
here hinges on the analyst’s reading. The third PC is similar: the Wildfire coefficient is large
and positive, followed by Wind and Landslide. By contrast, Storm and Flood have nontriv-
ial negative coefficients. PC3 may be some form of land-vs-water contrast here, although
domain-expert interpretations may differ.
◽
Besides its use for dimension reduction and summary visualization with complex mul-
tivariate data, PCA can also be employed as a springboard to more-advanced analyses. For
example, in so-called sparse PCA certain coefficients are driven to zero – making the eigen-
vector matrix more ‘sparse’ – in order to highlight and help to interpret the contributions of
higher-impact variables (Hastie et al. 2009, Section 14.5.5). (The external PMA package can
perform sparse PCA in R, via its SPC() function.) Another application, mentioned previ-
ously in Section 7.1.4, is PC regression. This employs the retained PC scores as a reduced
set of regressors in a multiple linear regression for modeling or predicting the outcomes of
an associated response variable (Hastie et al. 2009, Section 3.5.1). In-depth discussions on
these uses and other features of PCA are available in the various sources cited throughout this
section or in the textbook by Jolliffe (2002).

350
STATISTICAL DATA ANALYTICS
1
2
3
4
5
6
7
0.5
1.0
1.5
2.0
2.5
Index, j
ʎj
Elbow/break at j = 3
Figure 10.3
Scree plot of ordered eigenvalues, 𝜆j, from PCA in Table 10.4.
2
1
0
PC2
−1
−2
Calaveras
Yolo
Inyo
Mono
−3−2−1 0 1 2 3
SantaBarbara
Siskiyou
Shasta
LosAngeles
Tehama
SanLuisObisp
Plumas
Alpine
Nevada
ElDorado
Tuolumne
Stanislaus
SanJoaquin
Merced
Sacramento
Placer
Fresno
SanBernardino
RiversideMadera
Tulare
Mariposa
0
1
Kern
SanDiego
−1
2
Orange
Solano
Lassen
Imperial
Yuba
SanFrancisco
SanMateo
Butte
Ventura
Sierra
Amador
Modoc
Sutter
Napa
Kings
Alameda
Sonoma
SantaCruz
Trinity
Glenn
Colusa
Lake
Mendocino
PC1
PC3
Monterey
DelNorte
Humboldt
SanBenito
SantaClara
Marin
ContraCosta
Figure 10.4
Three-dimensional visualization of first three principal component (PC) scores
from PCA in Table 10.4.

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
351
10.3
Exploratory factor analysis
Similar to the analysis of PCs, and sometimes confused with it, is the method of exploratory
factor analysis (EFA). As presented in Section 10.2, PCA is a dimension-reduction technique
built essentially from algebraic and geometric principles. It is an effective way to reduce a
large set of variables to a smaller grouping and, in the process, explore some of the geometric
relationships extant among them. PCA makes very few statistical assumptions on that set of
variables, however. By contrast, EFA employs a formal statistical model, the consequences of
which can be exploited for knowledge discovery. As with PCA, implicit in the method is the
reduction in dimension among the original variables; however, its goals and interpretations
differ.
10.3.1
The factor analytic model
To construct the EFA model, begin with a set of p random variables Yj, j = 1, … , p, and
assume that the variables have been centered about their mean so that E[Yj] = 0. As with the
PCA approach in Section 10.2, it is common to further standardize the data into z-scores via
division by their standard deviations as in (3.7). Thus, we work with the standardized variables
Zj such that E[Yj] = 0 and Var[Zj] = 1 for all j. (Standardization is not required here, however,
and one could operate simply with the centered variables.)
The factor analytic concept hinges on the existence of a latent variable: an unobservable
random factor, Fk, whose effects are captured by the observed (often called ‘manifest’) vari-
ables Zj. The model relates the p manifest variables to a reduced set of q < p latent factors
via
Zj = bj1F1 + bj2F2 + · · · + bjqFq + 𝜖j
(10.2)
(j = 1, … , p), where Fk represents the kth latent factor (k = 1, … , q), the bjks are coefficients
that ‘load’ each manifest observation Zj on to the unobserved factor, and 𝜖j is a residual term.
These latter quantities serve as a source of variation specific to each Zj after modeling the Fks.
By contrast, each kth factor Fk represents a source of variation that affects all the observations
via its loading coefficients bjk. Dimension reduction occurs in the use of q < p factors to model
the underlying effects among the p observed variables.
The concepts underlying factor analysis were introduced by Spearman (1904a) for use
in psychometric testing with human subjects. In many psychometric applications, measure-
ments are taken on a wide variety of outcomes, but it is often felt that a smaller set of latent
factors can explain the observed variation in a more compact manner. Spearman’s example
involved intelligence testing with school children: test scores on mathematics, English, for-
eign language, music, and so on were recorded, but it was hypothesized that these all could be
explained by a smaller set of factors – in fact, just a single factor representing overall ‘intel-
ligence.’ Thurstone (1931, 1947) later generalized the model to multiple factors. From those
roots, EFA has become a popular, if very specific technique for dimension reduction, and sees
wide application in the behavioral, social, economic, and physical sciences.
In matrix terms, we write
Z =
⎡
⎢
⎢
⎢⎣
Z1
Z2
⋮
Zp
⎤
⎥
⎥
⎥⎦
,
B =
⎡
⎢
⎢
⎢⎣
b11
b12
· · ·
b1q
b21
b22
· · ·
b2q
⋮
⋮
⋱
⋮
bp1
bp2
· · ·
bpq
⎤
⎥
⎥
⎥⎦
,
F =
⎡
⎢
⎢
⎢⎣
F1
F2
⋮
Fq
⎤
⎥
⎥
⎥⎦
,
and
𝝐=
⎡
⎢
⎢
⎢⎣
𝜖1
𝜖2
⋮
𝜖p
⎤
⎥
⎥
⎥⎦
.

352
STATISTICAL DATA ANALYTICS
With these, (10.2) becomes the matrix expression
Z = BF + 𝝐.
(10.3)
Notice that this is a linear model, similar in form to the multiple regression models in Section
7.1. As applied here, however, its construction and interpretation are quite different. The goals
of an EFA are to detect structure – represented by the modeled factors – within the manifest
variables, while reducing the dimension of the data. The structure(s) are identified by studying
patterns among the loadings in B (see the following text); feature extraction often results
through detection of this factor structure.
Since all three constituents, B, F, and 𝝐, in (10.3) are unknown and unobserved, the model
is indeterminate and requires further specification to be useful. Typically, both F and 𝝐are
assumed random with zero means; that is, E[Fk] = E[𝜖j] = 0 for all k and j. The factors are
assigned unit variances, Var[Fk] = 1, while the residuals are given variable-specific variances
Var[𝜖j] = 𝜓j ≥0.
Further, all variables are taken to be uncorrelated such that their covariances are zero:
Cov[Fk, Fm] = Cov[𝜖j, 𝜖i] = 0 for all k ≠m, j ≠i, and Cov[Fk, 𝜖j] = 0 for all k, j. Again in
matrix terms, this gives E[F] = 0, E[𝝐]= 0, Var[F] = I, and Var[𝝐]= diag{𝜓1, 𝜓2, … , 𝜓p},
where each 0 is an appropriately dimensioned vector of zeros and I is the identity matrix. For
convenience, denote D𝝍= diag{𝜓1, 𝜓2, … , 𝜓p}. Under these moment assumptions, it can
be shown (Exercise 10.5) that E[Zj] = 0 (as designed) and
Var[Zj] = h2
j + 𝜓j,
(10.4)
where h2
j = ∑q
k=1 b2
jk.
Equation (10.4) shows that Var[Zj] is modeled as a sum of two components. The first, h2
j ,
represents the contribution of the common loading coefficients to Var[Zj] and is called the
communality of Zj. The second, 𝜓j, is called the specific variance of Zj, so-named because
it gives the variable-specific contribution of each 𝜖j to Var[Zj]. (One also sees the somewhat
awkward term ‘uniquenesses’ for the 𝜓js.) By design, Var[Zj] = 1. Thus in concert with 𝜓j ≥
0, (10.4) requires b2
jk ≤1, that is,
−1 ≤bjk ≤1.
(Indeed, (10.4) also gives h2
j = 1 −𝜓j for all j.) In addition, the covariance assumptions on F
and 𝝐lead to Cov[Zj, Zm] = ∑q
k=1 bjkbmk for j ≠m (see Exercise 10.5). Collected together in
matrix form, this is
Var[Z] = BBT + D𝝍.
(10.5)
As the Zjs are standardized to have unit variance, the correlation between any two
is
Corr[Zj, Zm] = Cov[Zj, Zm]∕(1),
using
(2.10).
Thus
the
factor-analytic
construc-
tion from (10.3) produces a prescriptive recipe for modeling the correlation struc-
ture of the Zjs, via (10.5). This has substantive consequences. For instance, we see
Corr[Zj, Zm] = Cov[Zj, Zm] = ∑q
k=1 bjkbmk and because b2
jk ≤1, this correlation will be large
(in absolute value) only if the bjks are themselves large on many of the same factors. Similarly,
loadings close to zero indicate limited or possibly meaningless relationships between Fk and
Zj. As a result, loading patterns with selected bjks near ±1 and many others near zero offer

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
353
very convenient identification of the underlying factors. This, in turn, provides for potential
knowledge discovery into the phenomenon under study.
Despite the careful model construction here, there remains an issue of identifiability.
Suppose we take a q × q orthogonal matrix Q and modify (10.3) into Z = BQTF + 𝝐. Then,
Var[Z] = BQTQBT + D𝜓= BBT + D𝜓,
because QTQ = I. Thus despite rotation of the loadings by QT, Var[Z] is unchanged. We
see that (infinitely) many different loading patterns can be constructed to represent the same
correlation structure for Z. This is both a blessing and a curse: it affords the analyst great
flexibility to modify a given collection of loadings by rotating them into BQT. One can search
repeatedly for as clear an explication as possible on how the factors affect Z. The definition
of ‘clear’ is open to interpretation, however, and this subjectivity can easily be overdone,
even abused. Indeed, the ‘E’ in EFA partly represents this exploratory aspect – also see the
following text – and can make the technique seem as much an art as a science. This leads some
analysts to view it with skepticism.
10.3.2
Principal factor estimation
To utilize the factor-analytic model (10.3) in practice, estimates must be determined for the
components of Var[Z] in (10.5). Assume the number of factors is fixed at some q < p. The aim
is to employ information in the sample covariance or correlation matrix to estimate BBT and
D𝝍. Write these latter estimators as ̂B ̂B
T and ̂D𝝍, respectively. If Zi = [Zi1 Zi2 · · · Zip]T is
the ith vector of observations from a p-variate random sample (i = 1, … , n), then the sample
correlation matrix from Section A.7 is R =
1
n−1
∑n
i=1 ZiZT
i .
A common method for estimating the correlation structure mimics the eigenanalysis used
in PCA and, hence, is called principal factor, or also principal axis factor, estimation. (The
overlapping nomenclature may contribute to some of the confusion seen between EFA and
PCA.) The general strategy is to modify the sample correlation matrix R by replacing its
diagonal elements (all 1) with estimates of the communalities. Many possibilities exist for
these replacements, including some clever manipulations of multiple regressions or multiple
correlations among the Zjs (Everitt 2005, Section 4.2). One simple approach appeals to the
core relationship in (10.5): viewing R as an empirical estimator for Var[Z] = BBT + D𝝍,
estimate the communalities as ̂h2
j = 1 −̂𝜓j, where the ̂𝜓js are taken from the diagonal of the
matrix R −̂B ̂B
T for some estimate ̂B ̂B
T.
Now, to find ̂B ̂B
T, begin with the spectral decomposition of R but retain only the largest q
eigenvalues, along with their corresponding unit eigenvectors. Denote these as 𝜆k and uk (k =
1, … , q), respectively. Clearly, if q = p, this reproduces the full PCA decomposition, in effect
forcing 𝜓j = 0. For q < p, however, collect this reduced set of eigencomponents into the col-
umn matrix [u1
√
𝜆1 u2
√
𝜆2 · · · uq
√𝜆q] and view the reduced spectral decomposition
[
u1
√
𝜆1 u2
√
𝜆2 · · · uq
√
𝜆q
] [
u1
√
𝜆1 u2
√
𝜆2 · · · uq
√
𝜆q
]T
=
q
∑
k=1
𝜆jukuT
k
as the initial estimate ̂B ̂B
T (cf. Section A.6.4). From this, recover ̂𝜓j from the diagonal of
R −̂B ̂B
T.

354
STATISTICAL DATA ANALYTICS
We usually iterate this process: replace the recovered estimates 1−̂𝜓j into the correspond-
ing diagonal elements of R and recompute the spectral decomposition for this updated R.
Continue until some prespecified convergence criterion is achieved, for example, iterate until
tr(R) (the sum of R’s diagonal elements; see Section A.1) stabilizes over successive updates.
For computing purposes, this form of principal factor estimation is available in R via the fa()
function of the external psych package (use the factoring option fm=‘pa’).
Once iteration is complete, the estimated percentage variation explained by the kth factor
is ∑p
j=1 ̂b2
jk∕p (assuming the data have been standardized), where the ̂bjks are taken from the
final estimate ̂B. The estimated communalities are ̂h2
j = ∑q
k=1 ̂b2
jk.
As an iterative procedure, convergence with principal factors is usually rapid. (Indeed,
some authors apply it or its variants with only one iteration.) Like any iterative approach,
however, it can suffer certain instabilities. One important example occurs when the iterative
estimate of a communality, ̂h2
j = 1 −̂𝜓j, exceeds 1. This forces the corresponding specific
variance ̂𝜓j below its lower limit of zero, which is senseless. Known as ‘Heywood cases’
(Heywood 1931), the iterative algorithm must identify and adapt to such untoward occur-
rences, possibly by resetting the initial conditions or applying some other adjustment. For
more on Heywood cases, see Dillon et al. (1987) and Kolenikov and Bollen (2012).
10.3.3
Maximum likelihood estimation
Principal factor estimation can be viewed as ‘distribution free,’ because it makes no assump-
tions on the particular distributions of F or 𝝐. This lends it a certain level of robustness for use
in practice.
If the analyst is willing to make, say, normal (Gaussian) distribution assumptions on Fk
and 𝜖j, the full power of parametric likelihood analysis (Section 5.1) can be brought to bear
on estimating B and D𝝍. To wit, suppose [Fk 𝜖j]T are independent bivariate normal vectors
with zero means and diagonal covariance matrices diag{1, 𝜓j}, j = 1, … , p; k = 1, … , q.
Assuming the data have been standardized into z-scores Zij with sample correlation matrix
R =
1
n−1
∑n
i=1 ZiZT
i , the log-likelihood may be written as
ℓ(BBT, D𝝍) = C −1
2{n log |BBT + D𝝍| + (n −1)tr([BBT + D𝝍]−1R)},
where |BBT + D𝝍| is the determinant of BBT + D𝝍and C is a constant not related to the
unknown parameters (Anderson and Olkin 1985). Maximizing ℓ(⋅) produces a maximum
likelihood estimate (MLE) for BBT + D𝝍. Unfortunately, rotation indeterminacy remains a
problem here, and so some additional constraint must be imposed to complete the optimiza-
tion. For instance, Lawley and Maxwell (1962) required BD−1
𝝍BT to be diagonal and then
conducted the constrained maximization via computer iteration. Details are available in, for
example, Bartholomew et al. (2011, Section 3.4), Clarke et al. (2009, Section 9.2.1), and
Lawley and Maxwell’s (1962) own early work.
Given MLEs for ̂bjk and ̂𝜓j from ̂B ̂B
T and ̂D𝝍, the MLEs for the communalities are
̂h2
j = ∑q
k=1 ̂b2
jk. The estimated percentage variation explained by the kth factor is ∑p
j=1 ̂b2
jk∕p
(again, assuming the data have been standardized).
One can also appeal to the normality assumptions and construct a likelihood ratio (LR)
test for model adequacy. The test compares the likelihood under the given q-factor model to
that under a full p-factor model: if the two are essentially indistinguishable – quantified by

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
355
an insignificant LR P-value – the q-factor model may be viewed as adequately reducing the
dimension of the problem. Conversely, small P-values from the test indicate that the explain-
able variability provided by the q factors may not be sufficient to supplant all p factors, and
so an additional factor (or factors) might be necessary (Everitt 2005, Section 4.3).
The LR test is useful when determining the number of factors is part of the exploratory
process – see the next section – because it can guide selection of q. This necessarily involves
calculation of repeated P-values on the same set of data, however, and so some correction for
multiple testing (Section 5.5) or other adjustment may be appropriate to avoid overfitting or
‘overfactoring’ the final model (Hayashi et al. 2007). The LR test, along with the full max-
imum likelihood (ML) estimation procedure, may be conducted in R via the factanal()
function.
Iteration and convergence with the ML approach can be fairly rapid, although the MLEs
are more sensitive to Heywood cases (de Winter and Dodou 2012). Analysts will often employ
ML factors in concert with principal factors, in order to study if and how the estimators differ
between the two methods. Roughly similar results will generate greater confidence in any
achieved dimension reduction.
10.3.4
Selecting the number of factors
In practice, the number of factors, q, may not be known a priori, and if so a large portion of
the ‘exploratory’ nature of the EFA involves determining the number, and final nature, of the
reported factors. In the former case, a number of possible strategies exists for choosing q. For
instance, if employing principal factor estimation as in Section 10.3.2, one could mimic the
Kaiser criterion from PCA and set q equal to the number of eigenvalues greater than 1. As
there, a factor whose eigenvalue 𝜆k is below 1 can be viewed as providing less explainable
variability than (at least) one of the original variables. Begin with a full p-factor analysis,
initially forcing 𝜓j = 0 for all j. Then, simply find that q < p such that 𝜆k ≥1 for k = 1, … , q.
Given this q, recompute the EFA with just q factors in the model.
Similarly, one might select q such that a predetermined percentage of variation is provided
by the q-factor model. That is, for a fixed percentage 100𝜋% (0 < 𝜋< 1), and assuming the
data have been standardized, select q such that
1
p
p
∑
j=1
q
∑
k=1
̂b2
jk ≥𝜋.
(10.6)
(Notice that ∑q
k=1 ̂b2
jk = ̂h2
j are the estimated communalities under the q-factor model.) Com-
mon choices for the percentage threshold are often seen in the 50–80% range, depending on
the domain-specific application.
To select q when employing ML factor estimation, it is common to apply the LR test for
adequacy mentioned in Section 10.3.3. Begin with, say, qo < p factors (typical suggestions
start at qo = 4, 5 or 6) and apply the LR test. If its corresponding large-sample P-value is
below a predetermined significance level, 𝛼, increase to qo+ 1 factors and retest. Continue until
P exceeds 𝛼. Conversely, if with qo factors P is already larger than the desired significance
level, drop to qo−1 factors and retest. Continue reducing by one factor until P is just larger
than 𝛼. (As mentioned earlier, this ignores the multiplicity inherent in such a repeated testing
strategy. Strict false positive error control is not always an issue here; when it is, some form
of multiplicity adjustment should also be applied.)

356
STATISTICAL DATA ANALYTICS
10.3.5
Factor rotation
Recall from Section 10.3.1 that the EFA model suffers (or benefits, depending on one’s
point-of-view) from a form of rotation indeterminacy: the base model in (10.3) can be
modified via an orthogonal rotation matrix Q into Z = BQTF + 𝝐without changing the core
variance relationship Var[Z] = BBT + D𝝍in (10.5). Analysts can take advantage of this
indeterminacy to rotate a given q-factor model’s fit and explore how the new factor loadings
change under the chosen rotation. (Again, an element of the ‘exploratory’ nature in EFA.)
If a rotation can be found which loads each variable heavily on as few factors as possible,
while drawing most of the other loading coefficients to zero, each variable is connected
with only one or a few factors in a mutually exclusive manner. The result is called ‘simple
structure’ (Thurstone, 1947) where, hopefully, domain-intelligent patterns emerge in the
factor loadings. Interpretability of the factors’ effects is then eased, even enhanced (see
Everitt, 2005, Section 4.5). Beyond dimension reduction, an associated goal in EFA is to
affect the pattern of rotated loadings to achieve this simplified interpretability.
As might be expected, many rotation strategies exist. For simplicity, continue to let ̂bjk rep-
resent the final (estimated) factor loadings over the j = 1, … , p variables and k = 1, … , q
factors. Perhaps the most popular rotation strategy is known as the varimax approach, pro-
posed by Kaiser (1958). As its name suggests, the goal is to maximize the (sample) variance
of each factor’s squared loadings, summed over all variables: in effect, for fixed q, maximize
the objective quantity
̂B =
p
∑
j=1
⎧
⎪
⎨
⎪⎩
q
∑
k=1
̂b4
jk −1
q
( q
∑
k=1
̂b2
jk
)2⎫
⎪
⎬
⎪⎭
.
When ̂B is large, the loading coefficients will migrate towards 1 or 0. Varimax rotation,
therefore, aims to produce focused factors with a small number of high loadings and many
loadings at or close to zero. It leans away from production of broad, general factors. For
computing purposes, R offers varimax rotation through a variety of conduits. Simplest perhaps
is the rotation=‘varimax’ option in factanal(), or the rotate= ‘varimax’ option in
fa() from the external psych package.
Another alternative is quartimax rotation (Neuhaus and Wrigley 1954), where variables
load heavily on just a single factor and are loaded trivially or not at all on all the others. The
fa() function from psych can also provide quartimax rotation, via its rotate=‘quartimax’
option.
If the analyst is willing to abandon the assumption that the factors are uncorrelated, that
is, Cov[Fk, Fm] = 0, then the options for rotation widen. The requirement that the factors
are uncorrelated was applied in (10.3) more for convenience than any other reason, and for
many data-analytic scenarios, its imposition is not strictly required. Rotations that allow factor
covariances to vary from zero are called oblique, and they produce a wide variety of loading
patterns. On the one hand, this adds increased flexibility to the analyst’s EFA toolkit; on the
other hand, it requires more care in loading interpretation and more complicated bookkeeping
to manage the now-nonzero factor correlations. Also, with greater flexibility comes greater
opportunity for hyperinterpretation of factor significance. As discussed above, this has led
some to debate the value of factor rotation and EFA in general.
One of the most popular oblique rotation strategies is known as promax rotation (Hen-
drickson and White 1964). The method initializes with a varimax rotation and then uses

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
357
increasing exponents on the loadings to push the higher coefficients towards 1 while shrinking
lower values towards 0. In R, promax rotation is performed by factanal() via its rota-
tion=‘promax’ option, or via the rotate=‘promax’ option in fa() from the external
psych package. For more on oblique rotations in EFA, see Everitt (2005, Section 4.3).
10.3.6
Implementing an EFA
As many authors note, calculation of an EFA (or a PCA, for that matter) is of little value if the
correlations among the variables are all at or near zero: the operation then essentially returns
the original observed variables. A test statistic for whether the sample correlation matrix R
differs from an identity matrix I was given by Bartlett (1950):
B2
p = −
{
(n −1) −1
6(2p + 5)
}
log |R| ,
(10.7)
where |R| is the determinant of R. In large samples, B2 ̇∼𝜒2( 1
2p{p −1}) so, for example, an
approximate P-value for testing if R differs from I is
P
[
𝜒2 (1
2p{p −1}
)
≥B2
p
]
.
This is known as Bartlett’s test for sphericity; Dziuban and Shirkey (1974) discussed its oper-
ating characteristics. In R, the test for sphericity is available via the cortest.bartlett()
function in the external psych package, or of course the test statistic can be calculated directly
via use of the det() function to find |R|.
Another index that provides a check for nonzero correlation is known as the
Kaiser-Meyer-Olkin (KMO) measure. Proposed by Kaiser (1970) and modified by Kaiser
and Rice (1974), the index gauges the amount of departure from zero among the various
correlations in the sample. Begin with the sample correlation matrix R =
1
n−1
∑n
i=1 ZiZT
i and
its elements rjk. Let diag{R−1} be a diagonal matrix built from the diagonal elements of R−1.
Then, find Q = diag{R−1}−1∕2R−1diag{R−1}−1∕2, with elements qjk. The KMO index is
∑∑
j≠k
r2
jk
(
∑∑
j≠k
r2
jk + ∑∑
j≠k
q2
jk
)
In practice, index values above about 80% show strong correlations where an EFA can be
effective, values between 50% and 80% show reasonable correlations, while values below
50% indicate little value in pursuing the calculations. Dziuban et al. (1979) gave details on
the measure and on its operating characteristics.
If the intricate model formulation embodied by (10.3) and (10.5) is felt to be valid with
a set of multivariable data, dimension reduction via EFA is a fairly straightforward process,
given modern-day computing capabilities. One standardizes the data; plots the variables for
visual overview and inspection; checks for valid correlation via, for example, Bartlett’s test
for sphericity; decides whether (or not) a normal distribution model can be imposed for ML
factor (or principal factor) estimation; fixes a value for the number of factors q or uses the
strategies described above for estimating it; and applies a rotation if desired to enhance factor

358
STATISTICAL DATA ANALYTICS
interpretation, structure detection, and possible knowledge discovery. The method can allow
for much more in the way of advanced multivariate data analysis (Bartholomew et al. 2011),
but these basic steps constitute the standard features of an EFA.
Example 10.3.1 EFA for psychometric test outcomes. A classic data set in psychometric
testing presented by Birren and Morrison (1961) concerns responses of n = 933 Caucasian
adults, aged 25–64, on the Wechsler Adult Intelligence Scale (WAIS). As conducted in that
article, the WAIS test reported outcome scores on 11 different cognitive ‘subtests’ (listed
in Table 10.5), scores on which were taken as the variables for an EFA. Two additional
standardized variables, Z12 = {Age} and Z13 = {Years of education}, were also included.
One question of interest was whether the p = 13 variables could be reduced to a smaller
number of factors, along with any insights as to how those factors might affect the original
variables.
Table 10.6 presents the correlation matrix, R, for these data as given in Birren and Morri-
son (1961). Most striking is the consistently negative correlation of Z12 = {Age} with all the
other variables. Many of these values are so close to zero as to likely be insignificant, but for
a few at least, increasing age appears to associate with decreasing outcome response.
Suppose the factor analytic model in (10.3) applies, and for convenience, assume a nor-
mal (Gaussian) distribution for the standardized data. A check of the correlation structure via
Bartlett’s test for sphericity gives B2
13 = 7007.2 on (13)(12)∕2 = 78 d.f. (degrees of freedom).
The P-value here is well below 10−4. This helps to validate application of an EFA to these data.
To estimate BBT and D𝝍, apply ML via, for example, the factanal() function in R.
Suppose the correlation matrix from Table 10.6 has been entered as the numeric matrix Rmx.
Start with qo = 5 factors (and no rotation) via the command
> factanal( covmat=Rmx, n.obs=933, factors=5, rotation=‘none’ )
The consequent output (not shown) produces an LR test for adequacy with P-value of
7 × 10−5, indicating a poor fit. Moving then to qo = 6 factors requires the command
> factanal( covmat=Rmx, n.obs=933, factors=6, rotation=‘none’ )
with consequent LR P-value P = 0.143. Thus a six-factor model seems minimally acceptable.
Table 10.7 displays the estimated loadings and summary information from the fit.
The loading pattern in Table 10.7 is unclear. Perhaps use of a varimax rotation via
> factanal( covmat=Rmx, n.obs=933, factors=6,
rotation=‘varimax’ )
can help to improve factor interpretation. Table 10.8 gives the new factor loadings and
other summary information from the fit (notice that the communalities ̂h2
j are unchanged,
Table 10.5
Standardized variables, Zj, from WAIS study in Example 10.3.1.
Z1 = Information
Z5 = Digit span
Z10 = Picture arrangement
Z2 = Comprehension
Z6 = Vocabulary
Z11 = Object assembly
Z3 = Arithmetic
Z7 = Digit symbol
Z12 = Age
Z4 = Similarities
Z8 = Picture completion
Z13 = Education
Z9 = Block design
Source: Birren and Morrison (1961).

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
359
Table 10.6
Correlation matrix, R, for WAIS study data in Example 10.3.1; upper
triangular portion only (lower triangle is transpose of upper triangle).
Variable Z1
Z2
Z3
Z4
Z5
Z6
Z7
Z8
Z9
Z10
Z11
Z12
Z13
Z1
1
0.67 0.62 0.66 0.47 0.81 0.47 0.60 0.49 0.51 0.41 −0.07
0.66
Z2
1
0.54 0.60 0.39 0.72 0.40 0.54 0.45 0.49 0.38 −0.08
0.52
Z3
1
0.51 0.51 0.58 0.41 0.46 0.48 0.43 0.37 −0.08
0.49
Z4
1
0.41 0.68 0.49 0.56 0.50 0.50 0.41 −0.19
0.55
Z5
1
0.45 0.45 0.42 0.39 0.42 0.31 −0.19
0.43
Z6
1
0.49 0.57 0.46 0.52 0.40 −0.02
0.62
Z7
1
0.50 0.50 0.52 0.46 −0.46
0.57
Z8
1
0.61 0.59 0.51 −0.28
0.48
Z9
1
0.54 0.59 −0.32
0.44
Z10
1
0.46 −0.37
0.49
Z11
1
−0.28
0.40
Z12
1
−0.29
Z13
1
Source: Birren and Morrison (1961).
Table 10.7
Loadings, ̂bjk; communalities, ̂h2
j ; and other summary results from ML
exploratory factor analysis for WAIS study data in Example 10.3.1.
Variable, Zj
Factor 1
Factor 2
Factor 3
Factor 4
Factor 5
Factor 6
̂h2
j
Z1
0.996
−0.015
−0.057
−0.016
<0.001
<0.001
0.995
Z2
0.690
0.137
0.188
0.269
0.068
−0.120
0.622
Z3
0.658
−0.227
0.674
−0.034
−0.011
−0.003
0.940
Z4
0.680
0.243
0.183
0.154
0.006
−0.047
0.580
Z5
0.492
0.131
0.318
−0.032
−0.100
−0.012
0.371
Z6
0.829
0.133
0.117
0.421
−0.031
0.022
0.898
Z7
0.495
0.500
0.287
−0.078
−0.272
0.127
0.673
Z8
0.620
0.388
0.211
−0.024
0.210
−0.120
0.638
Z9
0.516
0.418
0.348
−0.135
0.310
0.100
0.687
Z10
0.533
0.433
0.262
−0.021
0.035
−0.185
0.576
Z11
0.432
0.409
0.265
−0.093
0.232
0.173
0.517
Z12
−0.085
−0.567
−0.204
0.362
0.236
0.119
0.571
Z13
0.675
0.255
0.150
0.002
−0.235
0.052
0.602
∑p
j=1 ̂b2
jk
5.121
1.481
1.092
0.441
0.396
0.138
% variation
0.394
0.114
0.084
0.034
0.030
0.011
Cumul. % var.
0.394
0.508
0.592
0.626
0.656
0.667

360
STATISTICAL DATA ANALYTICS
Table 10.8
Loadings, ̂bjk; communalities, ̂h2
j ; and other summary results from ML
exploratory factor analysis after varimax rotation for WAIS study data in Example 10.3.1.
Variable, Zj
Factor 1
Factor 2
Factor 3
Factor 4
Factor 5
Factor 6
̂h2
j
Z1
0.794
0.256
0.059
0.274
0.470
−0.002
0.995
Z2
0.682
0.286
0.055
0.249
−0.034
−0.092
0.622
Z3
0.373
0.245
0.053
0.857
0.049
−0.004
0.940
Z4
0.625
0.323
0.193
0.217
0.022
−0.015
0.580
Z5
0.355
0.211
0.260
0.362
0.042
0.025
0.371
Z6
0.882
0.246
0.019
0.223
−0.032
0.092
0.898
Z7
0.387
0.298
0.592
0.189
<0.001
0.219
0.673
Z8
0.471
0.541
0.259
0.157
0.062
−0.167
0.638
Z9
0.264
0.708
0.251
0.224
0.049
−0.006
0.687
Z10
0.436
0.408
0.402
0.171
−0.024
−0.168
0.576
Z11
0.235
0.614
0.236
0.138
0.033
0.093
0.517
Z12
0.043
−0.201
−0.725
−0.008
−0.016
0.058
0.571
Z13
0.574
0.190
0.374
0.236
0.147
0.138
0.602
∑p
j=1 ̂b2
jk
3.548
1.921
1.477
1.314
0.257
0.152
% variation
0.273
0.148
0.114
0.101
0.020
0.012
Cumul. % var.
0.273
0.421
0.534
0.635
0.655
0.667
as expected). To find the specific variances for each variable, label the output object from
factanal() and examine its uniquenesses attribute:
> wais6vmx.fa <- factanal( covmat=Rmx, n.obs=933, factors=6,
rotation=‘varimax’ )
> wais6vmx.fa$uniquenesses
[1] 0.0050 0.3785 0.0601 0.4197 0.6288 0.1022 0.3270
[8] 0.3619 0.3130 0.4237 0.4833 0.4287 0.3979
In the rotated loadings from Table 10.8, the first factor loads heavily with Z1 = {Information},
Z2 = {Comprehension}, Z4 = {Similarities}, Z6 = {Vocabulary} (all verbal ability measures),
and Z13 = {Education}. Thus it may relate to some level of verbal acuity. The second fac-
tor loads heavily with Z8 = {Picture completion}, Z9 = {Block design}, and Z11 = {Object
assembly}, suggesting perhaps a visual perception skill factor. The fourth factor loads Z3 =
{Arithmetic} (and little else) heavily and may be a mathematical factor.
The other factors, especially the fifth and sixth, give few large loadings. Accounting also
for the small percentage of variation they explain (see the lower rows of Table 10.8), the
latter two at least may be superfluous. Indeed, Everitt (2005, Section 4.7) notes that for very
large data sets such as seen here, the LR test can be unnecessarily sensitive: it overreacts to
slight differences between the observed and predicted correlations and rejects more often than
necessary. Reduced models with fewer factors may be quite reasonable and possibly easier to
interpret. Exercise 10.6 explores possible avenues along these lines with these data.
◽
In some settings, the data analyst may wish to find estimates for the common factors,
known as ‘Factor Scores,’ analogous to the PC scores from a PCA. The issue is more deceptive

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
361
than it may at first seem, however: the EFA model in (10.2) expresses the manifest variables in
terms of the (unknown) factors, not vice versa. Some form of inversion is, therefore, necessary.
Indeed, because the factors are assumed random the issue is actually one of prediction, not
parameter estimation, and one must proceed carefully. Everitt (2005, Section 4.6) discusses
the prediction of Factor Scores in more detail.
Employed strictly as a data reduction strategy EFA has both proponents and detractors,
as has been noted throughout this section. It enlists a very particular model to describe the
observed variables and requires extensive numerical computation to fit that model. Every time
the method is considered for use, the data analyst must ask: does the latent factor model make
sense, and is it appropriate for the data at hand? If so, EFA can provide valuable dimension
reduction, especially given its additional ability to supply possible interpretation(s) for the
reduced factors’ impact on the phenomenon under study. It can also provide a springboard
to confirmatory factor analysis (Jöreskog 1969), where more specific constraints are placed
on the manifest variables and the postulated factors. If the latent factor model is inappro-
priate, however, use of EFA may be simple ‘overkill’ or worse, could produce misleading
interpretations of the data. Analysts should approach its use with informed caution.
10.4
Canonical correlation analysis∗
Often viewed as an extension of PCA due to similarities in its dimension-reduction technique,
the method of canonical correlation analysis (CCA) can be applied when two multivariate sets
of variables are observed. Rather than focus on observed variation in the data, however, the
technique operates on the correlations between the two sets of variables.
CCA was developed by Hotelling (1935, 1936). Its basic data structure involves n
observations taken on the two sets of variables x1, … , xp and y1, … , yq, producing separate
data matrices
X =
⎡
⎢
⎢
⎢
⎢
⎢⎣
x11
· · ·
x1p
x21
· · ·
x2p
⋮
⋱
⋮
xn1
· · ·
xnp
⎤
⎥
⎥
⎥
⎥
⎥⎦
and
Y =
⎡
⎢
⎢
⎢
⎢
⎢⎣
y11
· · ·
y1q
y21
· · ·
y2q
⋮
⋱
⋮
yn1
· · ·
ynq
⎤
⎥
⎥
⎥
⎥
⎥⎦
.
(10.8)
As with the other dimension-reduction techniques studied in this chapter, it is common for the
observations to be standardized by centering about their means and dividing by their standard
deviations. Denote these here as zij = (xij −xj)∕sxj and 𝑤ij = (yik −yk)∕syk, i = 1, … , n;
j = 1, … , p; k = 1, … , q. Collect the results into data matrices Z and W, respectively,
corresponding to (10.8). From these, find the correlation matrices RZZ = ZTZ∕(n −1) and
RWW = WTW∕(n −1), along with the cross-product matrices RZW = RT
WZ = ZTW∕(n −1).
For convenience of notation, let 𝜍= min{p, q}.
The goal in a CCA is to find linear combinations of Z and W whose correlation is a
maximum; that is, find Za and Wb that maximize r = Corr[Za,Wb]. The quantities Za and
Wb are called the (pair of) canonical variates of Z and W. In effect, CCA finds the direction
of maximal correlation between a pair of matrices. It involves a multivariate extension of the
simpler pairwise correlation coefficient from Section 3.3.3.
Often, a single pair of canonical variates will not be sufficient, so the goal expands to find
a collection of canonical variates Zam and Wbm with correlations rm such that r1 > r2 > · · · >
r𝜏for some 𝜏< 𝜍. This requires additonal constraints, so similar to the approach in a PCA,

362
STATISTICAL DATA ANALYTICS
we force the within-group covariances to zero and set
Cov[Zam, Zah] = Cov[Wbm, Wbh] = 0,
along with
Cov[Zam, Wbh] = 0,
for all m ≠h.
This is a constrained maximization problem, the solution to which results in yet another
eigenanalysis (Timm 2002, Section 8.7). We set
R−1
ZZRZWR−1
WWRWZa = 𝜆a
and solve for the eignevalue/eigenvector pair 𝜆and a. One can construct a similar eigenequa-
tion to find b, although it is often simpler to apply the equivalency relationship
b =
1
√
𝜆
R−1
WWRWZa .
From these, find the (ordered) eigenvalues 𝜆1 > 𝜆2 > · · · > 𝜆𝜍and set rm =
√
𝜆m for
m = 1, … , 𝜍. The rm values are the canonical correlation coefficients of Z and W. (Notice
that these are the positive square roots; canonical correlations are constructed to be non-
negative.) The largest eigenvalue corresponds to the maximal correlation, with decreasing
correlation as m grows. As is common, the squared canonical correlations r2
m represent the
proportion of variation in Zam explained by that in Wbm.
As applied in a CCA, the dimension reduction strategy translates to operations with a
reduced set of canonical variates. That is, choose a collection of canonical variates Zam and
Wbm with their corresponding canonical correlations rm over m = 1, … , 𝜏for some 𝜏< 𝜍,
such that the collection retains a large proportion of the correlation structure available in the
data. For instance, given a fixed percentage 100𝜋% (0 < 𝜋< 1) and assuming the data have
been standardized, one might select 𝜏such that
∑𝜏
m=1 r2
m
∑𝜍
h=1 r2
h
≥𝜋.
(10.9)
Practical values for the threshold 100𝜋% will vary, depending on the domain-specific problem
under study; common choices are often in the 50–80% range.
The CCA model also allows for tests of the null hypothesis
Ho𝜏∶𝜌1 ≠0, … , 𝜌𝜏≠0, 𝜌𝜏+1 = · · · = 𝜌𝜍= 0,
(10.10)
for any 𝜏= 1, … , 𝜍−1, where 𝜌m is the true, underlying correlation estimated by rm. If Ho𝜏
is true, then only the first 𝜏correlations are significantly different from zero and there is no
need to retain more than those 𝜏(pairs of) canonical variates.
For testing Ho𝜏, Bartlett (1938, 1941) gave the statistic
G2
𝜏= −
{
(n −1) −1
2(p + q + 1)
}
𝜍∑
m=𝜏+1
log(1 −r2
m).
(10.11)

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
363
In large samples, reject Ho𝜏at significance level 𝛼when G2
𝜏≥𝜒2
𝛼(𝜈), with 𝜈= (p −𝜏)(q −𝜏)
d.f. The corresponding large-sample P-value is P[𝜒2{(p −𝜏)(q −𝜏)} ≥G2
𝜏].
If a specific value for 𝜏is not known or anticipated, testing can proceed sequentially:
begin at 𝜏= 𝜍−1 and test Ho𝜏via (10.11). If the test rejects, stop and conclude the entire
collection of 𝜍canonical variates is required. If the test fails to reject, however, decrease 𝜏by
1 and retest using (10.11). If this next test rejects, stop and conclude the reduced collection
of 𝜍−1 canonical variates is required. If it fails to reject, continue decreasing 𝜏by one unit
and retest, and so on. When the test finally rejects at a given 𝜏, stop and operate with the
reduced collection of those 𝜏+ 1 canonical variates. If the test fails to reject for all 𝜏, operate
with only the largest canonical correlation at 𝜏= 1. (Clearly, this strategy involves multiple
testing, similar to the approach for finding the number of factors in an EFA. If control of
the consequent false positive error is important for the problem under study, some form of
multiplicity adjustment may be required; see Section 5.5.)
In small samples, rm possesses a bias for estimating 𝜌m. (Timm 2002, Section 8.7.c), fol-
lowing Lawley (1959), gave a correction that operates on the eigenvalues/squared correlations.
Modify r2
m into ̃r2
m = 𝜐2
mr2
m with
𝜐2
m = 1 −
1
n −1
𝜏∑
h=1
m≠h
r2
h
r2
m −r2
h
−
𝜍−𝜏
(n −1)r2
m
,
valid for the largest 𝜏< 𝜍correlations. If the correction factor calculates to a negative value,
leave it undefined. The resulting values of ̃rm are known as adjusted canonical correlations.
In R, CCA is performed via the cancor() function, although its extent is lim-
ited. A variety of external packages extend the basic features of cancor(). See
http://cran.r-project.org/web/packages/.
Example 10.4.1 Canonical correlation in the Million-Song Dataset.
The Million-Song
Data Set is a publicly available online database (http://labrosa.ee.columbia.edu/millionsong/)
cataloging the audio features of a million popular music tracks from 1922 to 2011
(Bertin-Mahieux et al. 2011). The data provide a useful testbed for a variety of research
studies with massive data. Here, consider a formal ‘test’ subset of the larger database
comprising audio features from n = 51 630 tracks, available from the UCI Machine Learning
Repository at http://archive.ics.uci.edu/ml. The data are quantifications of 90 audio attributes,
p = 12 based on timbre average (the X domain) and q = 78 based on timbre covariance (the
Y domain). A selection of the data is displayed in Table 10.9. (The complete set of values is
given at http://www.wiley.com/go/piegorsch/data_analytics.)
As an exercise in CCA, the p + q = 90 attribute variables in Table 10.9 can be reduced to a
smaller set of 𝜏< 𝜍= min{12, 78} = 12 canonical variates. Begin by standardizing the data
in the Z and W matrices, respectively. (Recall that R’s scale() function standardizes the
columns of a numeric matrix.) From these, find the correlation matrices RZZ = ZTZ∕(n −1),
RWW = WTW∕(n −1), and RZW = RT
WZ = ZTW∕(n −1). In R, use cor(Z), cor(W), and
cor(Z,W), respectively.
The correlation matrix for the complete data can be built from the individual component
matrices as a partition:
R =
[
RZZ
RZW
RWZ
RWW
]
.
(10.12)

364
STATISTICAL DATA ANALYTICS
Table 10.9
Selected audio track quantifications from the Million-Song Data Set stratified
by timbre average (X domain) and timbre covariance (Y domain) variables; selection from
larger collection of n = 51630 tracks.
X: timbre average
Y: timbre covariance
TAvg01
TAvg02
· · ·
TAvg12
TCov01
TCov02
· · ·
TCov78
45.442
−30.750
· · ·
−2.289
17.902
1377.122
· · ·
7.173
52.678
−2.889
· · ·
3.229
5.668
702.254
· · ·
−5.215
⋮
⋮
⋱
⋮
⋮
⋮
⋱
⋮
51.857
59.117
· · ·
−6.198
20.166
598.453
· · ·
12.174
Source: UCI Machine Learning Repository (http://archive.ics.uci.edu/ml).
The 90 × 90 matrix of correlations here is too large and complex to read easily. However, it
can be visualized via a heatmap display as described in Section 4.2.4. Here, the heatmap is a
90 × 90 graphic whose coloration or grayscaling represents the correlations in R. Figure 10.5
displays such a heatmap for (10.12) built from the heatmap() function in R. Lighter ‘hotter’
scale indicates values closer to 1, while darker ‘cooler’ scale indicates values closer to −1;
the color-bar along the left of the graphic gives the spectrum from 1 (bottom) to −1 (top).
As might be expected, the multiple correlation patterns in Figure 10.5 are mixed, although
a clear ‘hotspot’ of increased correlation is evident for the first dozen or so timber covariance
variables (see labels along the right/bottom). Note that because R is a symmetric matrix, the
heatmap pattern mirrors above and below the white diagonal.
To conduct the CCA, appeal to cancor(Z,W) in R produces the (ordered) canonical
correlations displayed in Table 10.10. The table also gives the corresponding squared correla-
tions, r2
m, along with the cumulative percentage variation explained by each (paired) canonical
variate. As can be seen, the first 𝜏= 3 (pairs of) canonical variates explain over 50% of the
variation from the correlation structure. If we increase to the first 𝜏= 6 canonical variates, we
exceed 80%. This suggests that a smaller group of between three and six canonical variates
could be sufficient to represent the correlation structure in this set of data.
We can also consider the individual null hypotheses from (10.10) that as a group the first
𝜏correlations are significantly different from zero (and that the remaining 𝜍−𝜏= 12 −𝜏are
not). Table 10.11 displays Bartlett’s G2
𝜏statistic (10.11) to test each null hypothesis, for 𝜏=
11, … , 1. We see each G2
𝜏statistic is highly significant when compared to its corresponding
𝜒2 reference distribution. This is not altogether surprising: when applied to extremely large
sample sizes – like those here – 𝜒2 tests of this sort can be overly sensitive. They overreact to
small deviations from the null event and reject more often than necessary. This appears to be
the case with these data.
◽
Just as in PCA, examination of the CCA loading coefficients am and bm for each canonical
variate can provide some understanding of how the original (standardized) variates associate
with each other. One may also extend the analysis to a sparse CCA, where many loading
coefficients are driven to zero in order to highlight contributions of higher-impact variables
(Parkhomenko et al. 2009; Witten and Tibshirani 2009). This shares features with the similar
‘sparse’ strategy for PCA. (The external PMA package can perform sparse CCA in R, via its

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
365
tavg01
tavg02
tavg03
tavg04
tavg05
tavg06
tavg07
tavg08
tavg09
tavg10
tavg11
tavg12
tcov01
tcov02
tcov03
tcov04
tcov05
tcov06
tcov07
tcov08
tcov09
tcov10
tcov11
tcov12
tcov13
tcov14
tcov15
tcov16
tcov17
tcov18
tcov19
tcov20
tcov21
tcov22
tcov23
tcov24
tcov25
tcov26
tcov27
tcov28
tcov29
tcov30
tcov31
tcov32
tcov33
tcov34
tcov35
tcov36
tcov37
tcov38
tcov39
tcov40
tcov41
tcov42
tcov43
tcov44
tcov45
tcov46
tcov47
tcov48
tcov49
tcov50
tcov51
tcov52
tcov53
tcov54
tcov55
tcov56
tcov57
tcov58
tcov59
tcov60
tcov61
tcov62
tcov63
tcov64
tcov65
tcov66
tcov67
tcov68
tcov69
tcov70
tcov71
tcov72
tcov73
tcov74
tcov75
tcov76
tcov77
tcov78
tcov78
tcov77
tcov76
tcov75
tcov74
tcov73
tcov72
tcov71
tcov70
tcov69
tcov68
tcov67
tcov66
tcov65
tcov64
tcov63
tcov62
tcov61
tcov60
tcov59
tcov58
tcov57
tcov56
tcov55
tcov54
tcov53
tcov52
tcov51
tcov50
tcov49
tcov48
tcov47
tcov46
tcov45
tcov44
tcov43
tcov42
tcov41
tcov40
tcov39
tcov38
tcov37
tcov36
tcov35
tcov34
tcov33
tcov32
tcov31
tcov30
tcov29
tcov28
tcov27
tcov26
tcov25
tcov24
tcov23
tcov22
tcov21
tcov20
tcov19
tcov18
tcov17
tcov16
tcov15
tcov14
tcov13
tcov12
tcov11
tcov10
tcov09
tcov08
tcov07
tcov06
tcov05
tcov04
tcov03
tcov02
tcov01
tavg12
tavg11
tavg10
tavg09
tavg08
tavg07
tavg06
tavg05
tavg04
tavg03
tavg02
tavg01
Figure 10.5
Heatmap of 90 × 90 correlation matrix for standardized data from Table 10.9.
Lighter colors indicate correlations near 1, darker colors near −1. Scale at left gives the spec-
trum from 1 (white, bottom) to −1 (dark, top). Source: Data from http://labrosa.ee.columbia
.edu/millionsong/ via http://archive.ics.uci.edu/ml.
CCA() function.) Given the multivariate nature of these sorts of analyses, however, interpre-
tation can be tricky and often must mature with the analyst’s practice and experience.
A standard recommendation for conducting a CCA is that the sample size minimally sat-
isfy n ≥p + q + 1 (Eaton and Perlman 1973). When n is small relative to p and q, however,
the method is hampered. One approach that can adjust for excessive numbers of variables
is known as regularized canonical correlation analysis (RCCA), which in effect introduces
penalty terms into the correlation matrices to adjust for excess variables. See González et al.
(2008), who also provided the external CCA package in R for performing RCCA.
The short introduction to CCA given here only scratches the surface of this complex
method of multivariate data analytics. Readers interested in further details may refer to

366
STATISTICAL DATA ANALYTICS
Table 10.10
Ordered canonical correlations from CCA of testbed data
in Table 10.9.
m
rm
r2
m
Cumlulative percentage of variance (%)∗
1
0.9240
0.8538
20.0
2
0.8735
0.7631
37.9
3
0.7936
0.6298
52.7
4
0.7450
0.5490
65.5
5
0.6255
0.3913
74.7
6
0.4957
0.2457
80.5
7
0.4629
0.2143
85.5
8
0.4494
0.2020
90.2
9
0.3617
0.1308
93.3
10
0.3407
0.1161
96.0
11
0.2967
0.0880
98.1
12
0.2854
0.0815
100
∗Canonical variates’ accumulated percentage contribution to total explainable variability:
∑m
h=1 r2
h∕∑12
h=1 r2
h.
Table 10.11
Bartlett test statistics, G2
𝜏, from CCA of testbed data in
Table 10.9.
𝜏
G2
𝜏
d.f.
P-value
11
4 383.63
67
<10−4
10
9 137.17
136
<10−4
9
15 501.58
207
<10−4
8
22 734.47
280
<10−4
7
34 373.48
355
<10−4
6
46 811.48
432
<10−4
5
61 356.48
511
<10−4
4
86 960.00
592
<10−4
3
128 040.46
675
<10−4
2
179 304.87
760
<10−4
1
253 586.41
847
<10−4
the advance discussions in, for example, Everitt (2005, Section 8.3) and Izenman (2008,
Section 7.3).
Exercises
10.1
Return to the PCA for the hazard-loss data in Table 10.2, and explore the final PC
scores in greater detail, as follows.
(a) Apply Bartlett’s test for sphericity to these data using (10.7). Is there an indication
that the correlation structure was rich enough to perform the PCA?

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
367
(b) Construct a scatterplot matrix for the first three PC scores studied in Example
10.2.2. What patterns emerge, if any?
(c) As suggested in the example, include the next (fourth) PC. Now, construct a scat-
terplot matrix display for the four retained PC scores. Also construct all (4 – why
is it 4?) possible 3D scatterplots using the four PC scores. (In R, the external
scatterplot3d package may prove useful.)
(d) Do any additional patterns emerge?
10.2
Return to the full wheat kernel data from Table 4.7 with all p = 7 variables. Perform
a PCA on the seven-variable, standardized data set and determine the extent of any
available dimension reduction using the following criteria:
(a) Kaiser’s (1958) criterion where 𝜆j ≥1 for j = 1, … , q.
(b) Jolliffe’s (1972) criterion where 𝜆j ≥0.7 for j = 1, … , q.
(c) Selection of the first q eigenvalues to achieve at least 80% explainable variation.
(d) Construct a scree plot and choose q such that the plot ‘elbows’ at 𝜆q.
10.3
To explore the geometry of PCs in more detail, return to the p = 2 case illustrated in
Figure 10.1 using husband’s and wife’s ages from Table 10.1. With those same data,
construct the following alternative graphs.
(a) Center each original variable: find x∗
1 = (x1 −x1) as the centered wife’s age and
x∗
2 = (x2 −x2) as the centered husband’s age. Now plot x∗
2 versus x∗
1. Compare this
to the plot in Figure 10.1.
(b) Calculate the z-scores via (3.7) for each original variable: find z1 = (x1 −x1)∕s1
as the standardized wife’s age and z2 = (x2 −x2)∕s2 as the standardized husband’s
age. Now plot z2 versus z1. Compare this to the plot in Figure 10.1.
(c) Perform a PCA on the standardized data and find the 2 PC scores, PC1 for the
wives and PC2 for the husbands. Plot PC2 versus PC1. Compare this to your other
plots of these data. Do you see any patterns among them? (Hint: Try an overlay
of the PC lines on the standardized plot.)
10.4
To explore the mathematics of PCs, set p = 2 with standardized variables z = [z1 z2]T
(Everitt 2005, Section 3.2). Suppose n data pairs are recorded and the sample corre-
lation matrix is found as
R =
[
1
r
r
1
]
.
(a) Start with the two eigenvalues, 𝜆1 and 𝜆2, of R. From (A.4), these solve the
determinant equation |R−𝜆I| = 0. Show that this reduces to solving the quadratic
equation (1 −𝜆)2 −r2 = 0. What are the two solutions?
(b) Next find the corresponding eigenvectors. Start with 𝜆1 and from (A.3), solve
the matrix equation Ru1 = 𝜆1u1 for u1 = [u11 u21]T. Show that this leads to the
system of two equations (with two unknowns)
u11 + ru21 = (1 + r)u21

368
STATISTICAL DATA ANALYTICS
and
ru11 + u21 = (1 + r)u21.
Solve this system for u11 and u21. What interesting result do you achieve? To estab-
lish a unique solution, impose the constraint uT
1u1 = 1. Do any arbitrary features
still remain? If so, force u11 > 0.
(c) Mimic Exercise 10.4b to find the eigenvector u2 corresponding to 𝜆2.
(d) Use the results above to find the PCs y1 and y2 in terms of z1 and z2.
10.5
Under the factor analysis model for Zj from (10.2) or (10.3), and assuming E[F] = 0,
E[𝝐] = 0, Var[F] = I, and Var[𝝐] = diag{𝜓1, 𝜓2, … , 𝜓p}, verify the following for
any j = 1, … , p:
(a) E[Zj] = 0.
(b) Var[Zj] = h2
j + 𝜓j, where h2
j = ∑q
k=1 b2
jk are the communalities. Since Var[Zj] is
constructed to be 1, verify the relationship between h2
j and 𝝍j.
(c) Assume Cov[Fk, 𝜖j] = 0 for all k, j. For m ≠j, show that Cov[Zj, Zm] =
∑q
k=1 bjkbmk.
(d) Verify that this leads to the matrix expression Var[Z] = BBT + D𝝍.
10.6
Return to the WAIS test data in Example 10.3.1 and as there employ ML estimation
with a varimax rotation. Explore the fit of different q-factor models as follows (Everitt
2005, Section 4.7).
(a) Set q = 6 and perform the fit. This should reproduce the results in Table 10.8.
From this, compute and plot the residual correlations R −( ̂B ̂B
T + ̂D𝝍). (Only
the (13
2
) = 78 entries in the upper or lower triangular portion are necessary. The
upper.tri() function in R may prove helpful.) Do the differences appear very
small and close to zero?
(b) Now set q = 3 and reperform the ML fit with varimax rotation. Again compute
and/or plot the residual correlations. Do these seem much different from those
from the q = 6 fit? If not, are the rotated factors simpler to interpret?
10.7
As part of a study to develop digitized cell imaging techniques for diagnosing cancer
status, Street et al. (1995) reported data from n = 151 female cancer patients over
a series of digitized variables meant to categorize certain target cell tissues. Con-
sider the p = 11 cell feature variables listed in the following display, along with their
recorded outcomes from these 151 patients. (Only a selection of the measurements
appears here. Download the complete data set at http://www.wiley.com/go/piegorsch
/data_analytics.)
The first five variables (Size, Radius, Perimeter, Area, and Compactness) may be
viewed as ‘size’ characteristics, the next two (Texture and Smoothness) as ‘texture’
characteristics, and the last four (Concavity, Concave Points, Symmetry, and Fractal
Dimension) as ‘shape’ characteristics.

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
369
Size
Radius
Perimeter
Area
Compact
Texture
Smooth
5.0
18.02
0.1086
0.0633
0.0142
1013.0
3.972
3.0
17.99
0.3001
0.0787
0.0490
1001.0
8.589
⋮
⋮
⋮
⋮
⋮
⋮
3.5
16.70
0.1012
0.0604
0.0263
885.4
4.243
Concavity
Concave Points
Symmetry
Fractal Dimension
0.0169
37.08
0.120
0.117
0.0300
17.33
0.162
0.265
⋮
⋮
⋮
⋮
0.0196
34.92
0.133
0.132
(a) Plot the data: use a scatterplot matrix to visualize any potential (pairwise) patterns
among the original 11 variables.
(b) Standardize the original data across the p = 11 variables and find their sample
correlation matrix R.
(c) Apply Bartlett’s test for sphericity using (10.7) to these data. Is the correlation
structure rich enough to perform an EFA?
(d) Apply an EFA to explore if dimension reduction from the original 11 variables is
possible. Suppose the factor analytic model in (10.3) applies, and for convenience,
assume a normal distribution structure on the standardized data. Estimate the ele-
ments of the matrices BBT and D𝝍via ML. Set the number of factors to qo = 4
and with no rotation test for model adequacy via an LR statistic. Continue testing
until an adequate reduction in dimension (if any) is found. Make no adjustment
for test multiplicity. (Hint: To improve iterative stability, use the ̂𝜓j ‘uniquenesses’
from the previous fit as starting values in each new fit.)
(e) Find the loadings from your chosen model. Is any interpretable pattern evident?
If not, consider a varimax rotation and reexamine the loading pattern for inter-
pretable features.
(f) Contrast use of the LR test to reduce the dimension of the model with simple
calculation of percentage of explainable variability using the squared loading
coefficients as in (10.6). Set your threshold to 70%. How does this change your
results?
10.8
Fayers and Machin (2007, Section 6.2) reported summary correlations from a
study on psychological factors affecting chronic-disease therapy. A sample of
n = 1952 patients were surveyed for their responses on p = 14 different anxiety- and
depression-related features of the therapy. Of these variables, the first seven were
anxiety related (coded A1, A3, … , A13) and the final seven were depression related
(coded D2, D4, … , D14). The observed correlation matrix, R, was found to be the
following (upper triangular portion only; lower triangle is symmetric):

370
STATISTICAL DATA ANALYTICS
A1
A3
A5
A7
A9
A11
A13
D2
D4
D6
D8
D10
D12
D14
A1
1
0.54
0.56
0.50
0.49
0.34
0.54
0.31
0.34
0.41
0.28
0.30
0.33
0.30
A3
1
0.60
0.41
0.58
0.32
0.60
0.26
0.36
0.37
0.23
0.27
0.32
0.27
A5
1
0.43
0.52
0.32
0.56
0.27
0.36
0.41
0.27
0.27
0.31
0.30
A7
1
0.38
0.34
0.43
0.41
0.43
0.47
0.33
0.34
0.44
0.40
A9
1
0.30
0.57
0.20
0.28
0.32
0.17
0.23
0.26
0.30
A11
1
0.38
0.15
0.18
0.19
0.16
0.17
0.17
0.19
A13
1
0.26
0.33
0.37
0.23
0.33
0.30
0.31
D2
1
0.47
0.50
0.52
0.40
0.59
0.42
D4
1
0.58
0.33
0.38
0.54
0.44
D6
1
0.37
0.41
0.52
0.47
D8
1
0.36
0.44
0.31
D10
1
0.45
0.36
D12
1
0.42
D14
1
(a) Apply Bartlett’s test for sphericity using (10.7) to these data. Is the correlation
structure rich enough to perform an EFA?
(b) Apply an EFA to explore if dimension reduction from the original 14 variables
is possible. Suppose the factor analytic model in (10.3) applies. Estimate the
elements of the matrices BBT and D𝝍via principal factors (i.e., principal axis
factoring). Set the number of factors to qo = 5, and with no rotation, determine
if this five-factor model is adequate via a percentage of explainable variability
metric, as in (10.6). Set your threshold to 90%. (You may need to increase the
maximum number of iterations in your computer code. Consider using at least
1000 iterations.) If not, increase or decrease the number of factors until an ade-
quate reduction in dimension (if any) is found.
(c) Find the loadings from your chosen model. Is any interpretable pattern evident?
If not, consider a varimax rotation and reexamine the loading pattern for
interpretable features.
10.9
Hambrick et al. (2010) gave summary correlations from a psychometric study of work
multitasking. A sample of n = 109 participants were administered tests of multitask-
ing capacity/speed under a suite of either four Baseline (‘Bblock’) work conditions
Bblock1 Bblock2 Bblock3 Bblock4 Eblock1 Eblock2 Eblock3 Eblock4
Bblock1
1
0.73
0.68
0.70
0.54
0.53
0.42
0.39
Bblock2
1
0.69
0.63
0.46
0.48
0.45
0.35
Bblock3
1
0.58
0.53
0.47
0.40
0.38
Bblock4
1
0.45
0.42
0.31
0.24
Eblock1
1
0.78
0.68
0.61
Eblock2
1
0.81
0.77
Eblock3
1
0.85
Eblock4
1

TECHNIQUES FOR UNSUPERVISED LEARNING: DIMENSION REDUCTION
371
or four increased-pace Emergency (‘Eblock’) conditions. Responses under the
p = 8 conditions represented the outcome variables. The observed correlation matrix,
R, was reported as the matrix displayed above (upper triangular portion only; lower
triangle is symmetric).
(a) Apply Bartlett’s test for sphericity using (10.7) to these data. Is the correlation
structure rich enough to perform an EFA?
(b) Apply an EFA to explore if dimension reduction from the original eight variables
is possible. Suppose the factor analytic model in (10.3) applies, and for conve-
nience, assume a normal distribution structure on the standardized data. (Estimate
the elements of the matrices BBT and D𝝍via ML.) As there are two broad testing
conditions (‘B’ and ‘E’), set the number of factors to qo = 2 and with no rotation
test for model adequacy via an LR statistic. Continue testing until an adequate
reduction in dimension (if any) is found. Make no adjustment for test multiplicity.
(c) Find the loadings from your chosen model. Is any interpretable pattern evident?
If not, consider a varimax rotation and reexamine the loading pattern for
interpretable features.
(d) Contrast use of the LR test to reduce the dimension of the model with simple
calculation of percentage of explainable variability using the squared loading
coefficients as in (10.6). Set your threshold to 80%. How does this change your
results?
10.10
CCA is increasingly applied when studying genetic outcomes between different
populations, disease states, and so on. Consider the following data, which are
expression levels in 19672 genes from n = 89 samples of breast cancer tissue
described by Witten et al. (2009). CCA can be employed to help identify coregulated
sets of genes on different chromosomes. For illustrative use, expression levels
are listed here for a subset of p = 4 genes from human chromosome 1 (the X
domain) and a subset of q = 3 genes from human chromosome 13 (the Y domain).
A selection of the measurements follows. (Download the complete data set at
http://www.wiley.com/go/piegorsch/data_analytics.)
X: Chromosome 1 genes
Y: Chromosome 13 genes
SF3B4
HSPC003
GNPAT
NDUFS2
WASF3
BRCA2
PCCA
9.7058
9.0607
10.626
10.1618
7.6545
5.2597
5.5841
9.7582
8.8349
10.607
10.1444
6.0809
5.0343
6.6120
9.0349
8.7048
10.033
9.5802
5.9279
5.6758
8.1687
⋮
⋮
⋮
⋮
⋮
⋮
⋮
9.5462
9.7489
10.485
10.9294
8.2027
4.9079
7.0484
(a) Perform a CCA. Start by standardizing the data: compute a Z matrix from X and
a W matrix from Y.
(b) Calculate the 𝜍= 3 canonical correlations, rm, and their corresponding canonical
variates from the correlation matrices of Z and W.

372
STATISTICAL DATA ANALYTICS
(c) Find the squared correlations r2
m, and identify the percentage of explainable
variation each canonical term provides, via (10.9). If 70% of total variation is
considered a sufficient threshold, what do you conclude about the need for any of
the terms?
(d) Calculate the Bartlett statistic from (10.11) at 𝜏= 2 and 𝜏= 1. Operate at 𝛼=
0.05. (Do not adjust for multiplicity.) What does this suggest about the need for
any of the terms?
(e) Construct a plot of the leading canonical variables, that is, plot Za1 against Wb1.
Do any informative patterns emerge?

11
Techniques for unsupervised
learning: clustering and
association
11.1
Cluster analysis
An important data analytic technology involves the study of how observations cluster into dis-
tinct groups or categories. When the categories are known and delineated in advance, statistical
learning about the clustering/categorization is a supervised process, as in the classification
problem from Chapter 9. When the categories or clusters are unknown, however, learning is
unsupervised. Cluster analysis is the broad term for this effort: the analysis of multivariate
data to identify patterns of clustering they exhibit. The concept evolved in the mid-twentieth
century after early works of Driver and Kroeber (1932) and Tryon (1939) established many
of the basic themes, with applications in anthropology and psychology. Cluster analysis has
since become a popular technique in data mining and informatics.
As in Chapter 10, assume the data are numerical measurements on p > 1 different vari-
ables of interest taken over a sample or training set of n subjects. Each variable is associated
with an individual column vector xj = [x1j x2j · · · xnj]T (j = 1, … , p). The corresponding
n × p data matrix is X = [x1 · · · xp]. Here again, it will be useful to center the observations
about their means, say, x∗
ij = xij −xj, producing a column matrix of (mean-corrected) values
X∗from (10.1).
Generally, differences in scale between the p variables can impact the clustering rela-
tionship: variables with especially large scales can dominate or skew the pattern relative
to those with smaller scales, destabilizing the analysis. To avoid this, we further divide the
x∗
ijs by their corresponding standard deviations, sj. This produces standardized z-scores as
in (3.7): zij = x∗
ij∕sj = (xij −xj)∕sj, i = 1, … , n; j = 1, … , p, with consequent, standardized,
n × p data matrix Z.
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

374
STATISTICAL DATA ANALYTICS
The fundamental goal of a cluster analysis is to create a set of clusters {C1, C2, … , CK}
from
the
p-variate
information
provided
by
the
standardized
observation
vectors
zi = [zi1 zi2 · · · zip] making up the n rows of Z. Ostensibly, observations within a
cluster Ck should be more similar to each other and less similar to those in any other cluster
Ck′. (Technically, the notation Ck is used here to contain those indices i corresponding to row
vectors zi located in the kth cluster. Where no confusion exists, however, the terminology
will also refer loosely to an observation zi as being ‘in’ Ck.)
How to define similarity within – or, for that matter, dissimilarity between – clusters is,
of course, the challenge: many different clustering strategies have been proposed, each with
its own selective value and use(s). At their core, however, the various methods all attempt to
replicate what the human eye does expertly in two (and sometimes three) dimensions: iden-
tify where and how clusters of data group together. Indeed, when p = 2 or 3, appeal to 2D or
3D scatterplots and/or scatterplot matrices often provides sufficient visual guidance on which
clusters the zijs inhabit. (Further study of how and what each identified cluster represents
then becomes part of the knowledge discovery process.) In higher dimensions, humans sel-
dom visualize patterns as effectively, however, and we turn to statistical techniques for cluster
identification. Indeed, these can also prove useful in the simpler two- and three-dimensional
settings. For illustrative purposes, consider the following small-scale example.
Example 11.1.1 Automobile characteristics. From a study of US automobile efficiency
similar to that in Example 4.2.6, Myatt (2007, Table 6.8) gave a small data set with selected
physical characteristics of n = 8 eight-cylinder passenger cars from model year 1970. For
simplicity of visualization, focus here is restricted to the p = 2 variables x1 = {Engine dis-
placement (cu. in.)} and x2 = {Acceleration (s, from 0 to 60 mph)}. The data appear in
Table 11.1; the corresponding z-scores are plotted in Figure 11.1. The figure presents a simple
scatterplot (Figure 11.1a) alongside a labeled plot with the model names (Figure 11.1b).
The relationship in Figure 11.1 illustrates the expected drop (improvement) in acceleration
times with larger engine displacement for these eight-cylinder US passenger cars. Beyond
this, however, two simple clusters quickly visualize: a low-displacement/slow-acceleration
cluster to the upper left and a large-displacement/fast-acceleration cluster to the lower right.
Translating to the original scale, one sees that large displacements above 400 cu. in. appear
to definitively lower acceleration times for these vehicles. Such a ‘knowledge discovery’ may
seem obvious, but it is nonetheless instructive to see the data illustrate this phenomenon. How
to replicate this sort of visual identification statistically is the goal of a cluster analysis.
We will return to these data in order to explicate features of the various clustering methods
in this section.
◽
Every cluster analysis depends on how ‘distance’ is defined in the feature space. The dis-
tance metric, also called a dissimilarity measure, between any two observations zi and zh is
given by a functional d(zi, zh) that satisfies standard conditions of a ‘distance’ for all i and h:
• d(zi, zh) ≥0,
• d(zi, zh) = d(zh, zi) (‘symmetry’),
• d(zi, zi) = 0, and
• d(zi, zh) ≤d(zi, zℓ) + d(zℓ, zh) for all ℓ= 1, … , n (the triangle inequality).

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
375
Table 11.1
Data on selected eight-cylinder US automobiles from 1970
model year: Engine Displacement (cu. in.) and Acceleration (s, from 0 to
60 mph).
Make and model
Displacement (cu. in.)
Acceleration (s)
Chevrolet Chevelle
307
12.0
Plymouth Fury
440
8.5
Ford Galaxie 500
429
10.0
Chevrolet Impala
454
9.0
AMC Rebel SST
304
12.0
Plymouth Satellite
318
11.0
Buick Skylark
165
11.5
Ford Torino
140
10.5
Source: Myatt (2007, Table 6.8).
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
Standardized displacement
Standardized acceleration
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
Standardized displacement
(a)
(b)
Standardized acceleration
Chevelle
Fury
Galaxie
Impala
Rebel
Satellite
Skylark
Torino
Figure 11.1
Scatterplots of automobile characteristics in Table 11.1 from Example 11.1.1.
(a) z2 = {Standardized Acceleration} plotted against z1 = {Standardized Engine Displace-
ment}. Two clusters are apparent at upper left and lower right. (b) Points labeled by auto
model (slight jitter added to distinguish adjacent points). Source: Data from Myatt (2007,
Table 6.8).
We collect the observed distances together into a n × n distance matrix, also called a dissimi-
larity matrix, D, with (i, h)th element d(zi, zh). Notice then that D = DT and that the diagonal
of D contains only zeros. In many cases, numerical or graphical examination of D (via, e.g., a
heatmap from Section 4.2.4) can give useful input on the affinity patterns exhibited in the data.
In passing, it is worth noting that some analysts prefer to operate instead with a proximity
or similarity matrix S that captures how ‘close’ observations are to each other. The proximity

376
STATISTICAL DATA ANALYTICS
elements s(zi, zh) are constructed to increase as d(zi, zh) decreases, with the typical conven-
tion that s(zi, zh) = 1 when d(zi, zh) = 0. Thus a simple choice for S is I −D after scaling
the d(zi, zh) values to have maximum ‘distance’ 1. Of course, other forms are possible. For
example, one might compute the sample correlation between zi and zh, averaging across the
p variables see Hastie et al. (2009, Section 14.3.2).
The distance metric is often taken as Euclidean, that is, the usual ‘as-the-crow-flies’ dis-
tance. Similar to the options for k-nearest neighbor learning from Section 9.3, a number of
alternatives can be considered for the distance; Table 11.2 reproduces the options in Table 9.8.
Notice that the Minkowski form is actually a general class of metrics, characterized by its posi-
tive parameter 𝛾. Special cases include Euclidian (𝛾= 2), Manhattan (𝛾= 1, sometimes called
‘Hamming’ distance when the xijs are quantitative measurements), and Maximum (𝛾→∞)
distances. For Canberra distance, terms with zero numerator and denominator are omitted
from the sum; Canberra distance is often intended for use with nonnegative counts. For some
additional metrics useful with other forms of data such as binary observations, see Myatt and
Johnson (2009, Section 3.6).
Table 11.2
Selected metrics for defining the distance, d(zi, zh), between two
p-dimensional row vectors zi = [zi1
zi2 · · · zip] and zh = [zh1
zh2 · · · zhp].
Name
Distance, dih
Euclidean
d(zi, zh) =
√∑p
j=1 (zij −zhj)2
Manhattan (‘city block’)
d(zi, zh) = ∑p
j=1 |zij −zhj|
Maximum/Tchebychev
d(zi, zh) =
max
j=1, … ,p {|zij −zhj|}
Minkowski
d(zi, zh) =
{∑p
j=1 |zij −zhj|𝛾}1∕𝛾
(𝛾> 0)
Canberra
d(zi, zh) = ∑p
j=1 |zij −zhj|∕|zij + zhj|
In R, the dist() function computes distances between the rows of a data matrix. The
function can accommodate any of the specific metrics listed in Table 11.2, via its method=
option; method=‘euclidean’ is the default.
Focus in this section is on how the information in D can be used to cluster the n
observations in a quantitative manner. Two distinct clustering strategies are examined:
(i) hierarchical methods where convenient or evolving subgroups/subcategories appear as
the clusters develop and (ii) data segmentation that partitions the observations into a set of
K ≥1 clearly separated clusters. The latter approach is how most users imagine clustering
and cluster analysis – segmenting the data into groups as suggested in Figure 11.1 – while
the former process has more of a branching, tree-like nature similar to the decision trees in
Section 9.4.1. Each approach is discussed in turn, beginning with hierarchical clustering.
11.1.1
Hierarchical clustering
Hierarchical clustering is based on a simple concept: start with as many clusters as there
are observations (so K = n) and amalgamate clusters in a stepwise, hierarchical manner.

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
377
Add observations to clusters based on how the ‘distances’ between the clusters change as
they grow. This is known as agglomerative hierarchical clustering. When operating hierar-
chically, observations are not to be swapped or moved between clusters: once an observation
is added to an agglomerating cluster, it remains there until completion of the process. (One
can alternatively reverse the process and start with a single large cluster containing all n
observations and then divide clusters apart by removing observations based on the changing
distances between clusters. This divisive case is aptly called divisive hierarchical clustering.
Agglomerative clustering is more common in practice, however, and is highlighted here.
Some comments on divisive clustering appear at the end of the section.)
Given the interobservation distances in the dissimilarity matrix D – and therefore, a fixed
choice for the distance metric – a further definition is required for the linkage between two
clusters Ck and Cm. Linkage here refers to how separation between Ck and Cm is calcu-
lated, given the choice of distance metric d(zi, zh) from Table 11.2. Many authors also refer
to the linkage descriptor as (another) ‘dissimilarity measure,’ now measuring dissimilarities
between clusters rather than between observations. (The terminology varies and can be con-
fusing; in practice, users must be careful to recognize which term is being used for which cal-
culation.) To help distinguish the two, denote the linkage dissimilarity measure as 𝛿(Ck, Cm).
Here again, a number of different forms are possible. One of the simplest is known as single
linkage or single-link clustering, where the closest observations between two clusters define
the linkage distance (a type of nearest-neighbor relationship):
𝛿Sngl(Ck, Cm) = min
i∈Ck
h∈Cm
{d(zi, zh)}.
Contrastingly, one can take the distance between the farthest two observations, called complete
linkage or complete-link clustering:
𝛿Comp(Ck, Cm) = max
i∈Ck
h∈Cm
{d(zi, zh)}.
Both single-link and complete-link clustering are invariant to monotone transformations of
d(zi, zh), and when the underlying clusters are well delineated and tightly contained, the two
approaches often yield similar outcomes. In other cases, however, they can produce distinctly
different results. Since it focuses on only the closest points between two clusters, single link-
age can form clusters that are fairly diffuse, elongated, or irregularly shaped (think ‘barbells’).
Known as the chaining problem, this is sometimes viewed as a deficiency of the method. As
Clarke et al. (2009, p. 416) noted, however, unusually shaped clusters do occur in nature, so
‘ … it’s unclear in general whether such properties are features or bugs.‘
At the other extreme, complete linkage tends toward small, tightly contained clusters.
Since it is driven by the largest possible distance between two clusters, however, it can be sensi-
tive to outlying observations. Thus it sometimes produces clusters with elements less like each
other and more like those in an adjoining cluster, violating a basic premise of cluster analysis.
A compromise between the two strategies is average linkage or average-link clustering,
defined via
𝛿Avg(Ck, Cm) =
1
𝜈k𝜈m
∑
i∈Ck
∑
h∈Cm
d(zi, zh),
(11.1)
where 𝜈k and 𝜈m are the numbers of elements in clusters Ck and Cm, respectively. Average link-
age can be shown to approximate a measure of integrated cluster dissimilarity as n grows large

378
STATISTICAL DATA ANALYTICS
(a)
(b)
(c)
Figure 11.2
Different idealized forms of linkage for hierarchical clustering. (a) Single link-
age, (b) complete linkage and (c) average linkage.
(Hastie et al. 2009, Section 14.3.12) and represents a middle ground between the single- and
complete-link extremes. Its mediating features make it a popular choice in many applications.
Figure 11.2 illustrates these three linkage scenarios, using two abstracted clusters with the
automobile data from Table 11.1.
A number of other options exist for characterizing the linkage distance 𝛿(Ck, Cm), for use
in selected or specialized applications. For instance, define the centroid of a cluster as the
p-variate arithmetic mean (row) vector across all its elements: zk = [z+1k
z+2k · · · z+pk],
where z+jk = ∑
i∈Ckzij∕𝜈k. Then, centroid linkage employs
𝛿Cent(Ck, Cm) = d(zk, zm)
(Clarke et al. 2009, Section 8.2.1). Here, d(zk, zm) indicates application of the selected metric
from Table 11.2 to find the ‘distance’ between the centroids zk and zm. Other forms of link-
age distance include the use of medians to define the location of each cluster, or of flexible,
parameterized functions to define the intercluster distances (Lance and Williams 1967); see
Hubert (2006).
Changing the linkage specification also changes the underlying definition of ‘closeness’
or ‘affinity’ between the developing clusters. Thus different choices for the linkage result in
different – sometimes substantially different – hierarchical patterns (see Example 11.1.3).
From an exploratory perspective, however, this can be as much a blessing as a curse:
application of alternative linkage specifications to the same data sometimes uncovers
interesting or unexpected features, which in turn helps to advance the knowledge discovery
process.
With the architecture defined for the interobservation dissimilarity metric d(zi, zh) and the
intercluster linkage 𝛿(Ck, Cm), the formal algorithm for agglomerative hierarchical clustering
is actually fairly straightforward (Everitt 2005, Section 6.2):
AH.1 Start with K = n clusters, each containing a single, distinct observation: Ck = {zk},
k = 1, … , n.

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
379
AH.2 Find the two clusters Ck∗and Cm∗exhibiting the smallest linkage separation
min
k,m {𝛿(Ck, Cm)}, and collapse them together. Consequently decrease K by 1.
AH.3 If K = 1 stop; otherwise return to step AH.2.
The result of an agglomerative clustering operation is typically displayed as a tree-like
classification, called a dendrogram, similar to the decision trees in Section 9.4. The agglomer-
ated K = 1 ‘root’ cluster is traditionally located at the top, with the smaller clusters branching
down hierarchically and ending in the K = n single-‘leaf’ clusters. (One can also reverse the
display with the root at the bottom of the dendrogram, or with it at the side from left-to-right,
etc.) A sliding scale alongside marks the linkage distances, and the dendrogram is constructed
so that clusters separated by a given distance 𝛿all align transversely at that 𝛿on the distance
scale (see Figure 11.3).
Example 11.1.2 Automobile characteristics (Example 11.1.1, continued). To illustrate
the construction of a simple dendrogram, return to the automobile characteristics data from
Table 11.1. Recall that this small data set has only n = 8 observations, with p = 2 outcome
variables, z2 = {Standardized acceleration} plotted against z1 = {Standardized engine dis-
placement} as in Figure 11.1. In the figure two clear clusters appeared evident.
Expanding on this visualization, consider application of agglomerative, hierarchical clus-
tering. For the underlying distance metric, let d(zi, zh) be standard Euclidean distance from
Table 11.2. To calculate the underlying distances and collect them into a dissimilarity matrix
D, one can use the R command
> D <- dist( cbind(z1,z2), method=‘euclidean’, diag=TRUE )
where z1 and z2 are individual 8 × 1 column vectors with the standardized displacements and
accelerations, respectively, bound together into an 8 × 2 Z matrix via the cbind() function.
Given the interobservation distances in D, R can then perform agglomerative hierarchical
clustering via its hclust() function. To construct the corresponding dendrogram, simply
apply R’s plot() function to the hclust() object:
> plot( hclust(D, method=‘average’),
ylab=expression(delta), labels=Names )
where the method=‘average’ option calls for average-link clustering as in (11.1) and Names
is a character vector containing the model names of the eight automobiles to label the dendro-
gram leaves. Notice that hclust() does not employ the actual (standardized) observations
zij to perform the cluster analysis; only information in the dissimilarity matrix D is required.
The resulting dendrogram appears in Figure 11.3. Working up from 𝛿= 0 in the figure,
we see {Chevelle, Rebel} aggregate quickly into a two-element cluster, followed by {Impala,
Fury}, and then by {Satellite, Torino}. Skylark and Galaxie remain as singletons until
about 𝛿= 0.76, when Skylark joins the original {Chevelle, Rebel} cluster. At approximately
𝛿= 0.92, this three-element cluster merges with the {Satellite, Torino} pair, creating the
low-displacement/slow-acceleration cluster visualized in Figure 11.1. Skylark remains a
singleton until about 𝛿= 1, when it agglomerates with the {Impala and Fury} pair to produce
the large-displacement/fast-acceleration cluster in Figure 11.1. These two clusters remain
distinct until 𝛿exceeds 2.6, when the entire collection merges into a single agglomerated
cluster. Thus for values of an intercluster linkage distance between approximately 1.0 and

380
STATISTICAL DATA ANALYTICS
Satellite
Torino
Skylark
Chevelle
Rebel
Galaxie
Fury
Impala
0.0
0.5
1.0
1.5
2.0
2.5
δ
Figure 11.3
Average linkage dendrogram for Automobile Characteristics data in Example
11.1.2. Vertical scale is intercluster linkage distance, 𝛿(Ck, Cm). Underlying distance metric
is Euclidean, from Table 11.2. Source: Data from Myatt (2007, Table 6.8).
2.6 (a substantial portion of the 𝛿scale here), the two-cluster visualization from Figure 11.1
is validated by this average-linkage dendrogram.
Exercise 11.1 explores how the dendrogram and the corresponding clusters change if
single-link or complete-link clustering is instead employed with these data.
◽
Example 11.1.3 Clustering in gene expression data. An important application of cluster
analysis occurs with gene expression levels and DNA microarray data (Hastie et al. 2009,
Section 14.3.12); cf. Example 5.5.2. Clustering often focuses on the n different genes that
represent the observations, where the genes’ expression patterns are examined across p >
1 conditions such as different disease states and organ/tissue types, ecological species.
Alternatively, n individual subjects may be clustered using expression data from p different
genes felt to represent presumptive genetic markers. For the latter case, Uhlmann et al. (2012)
described a study where n = 118 colorectal cancer patients had their relative gene expression
ratios sampled for p = 4 different genes thought contribute to cancer progression: osteopontin
(OPN), cyclooxygenase-2 (COX-2), transforming growth factor 𝛽(TGF-𝛽), and matrix
metalloproteinase-2 (MMP-2). Among the questions of interest to the investigators was
whether clustering among the patients’ expression patterns could reveal tumor subgroups or
other carcinogenic effects. The data, sanitized to mask personal identifying information and
kindly provided by Dr. Martin Sill of the Deutsches Krebsforschungszentrum in Heidelberg,
Germany, appear in Table 11.3. (As previously, only a selection of the data is given in the table.
The complete set of data is available at http://www.wiley.com/go/piegorsch/data_analytics.)

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
381
Table 11.3
Selected values for relative gene expression levels of p = 4 putative tumor
progression genes, from n = 118 colorectal cancer patients.
Gene
Patient code no.
OPN
COX-2
Tgf-𝛽
MMP-2
1
8.0528
60.1337
0.5105
6.0677
2
34.6786
221.1466
19.9325
3.4757
3
0.2449
1.8196
0.3016
1.0040
⋮
⋮
⋮
⋮
⋮
117
0.1259
1.3478
0.1125
0.5449
118
1.3660
8.8011
0.1983
1.9355
Source: Uhlmann et al. (2012).
Consider here agglomerative hierarchical clustering of these 118 patients’ relative gene
expression patterns using the p = 4 genes in Table 11.3. Begin by standardizing the original
xijs in the table to z-scores: zij = (xij −xj)∕sj, i = 1, … , 118; j = 1, … , 4. Set the distance
metric from Table 11.2 to Euclidean, and for now, employ average linkage.
For these settings, a dendrogram in R can be produced using commands similar to those
from Example 11.1.2:
> D <- dist( Z, method=‘euclidean’, diag=TRUE )
> plot( hclust(D, method=‘average’),
ylab=expression(delta), labels=Patient )
where Patient is a vector containing the patients’ reference code numbers.
The average linkage dendrogram appears in Figure 11.4, where substantial initial coupling
is evidenced. That is, for small values of 𝛿most early clusters are simple pairs or triplets of
patients whose expressions patterns are very close. The agglomeration accelerates, however,
so that by only about 𝛿= 0.15 a large cluster of 46 patients has developed – delineated by
the lower dashed cut-line in the figure. (See the small rectangle towards the lower center of
the dendrogram.) To find the cluster membership labels for each patient, use R’s cutree()
function, for example,
> cutree( hclust(D, method=‘average’), h=0.15 )
As 𝛿increases, this core cluster continues to grow. At approximately 𝛿= 2.75 – delineated
by the upper dashed cut-line in the figure – it has accumulated all but 10 of the patients under
study. (See the larger rectangle in the dendrogram.) Specifics of the tumors and other patient
characteristics from either this smaller/earlier cluster or the larger/agglomerated cluster – or,
for that matter, from the remaining patients outside the larger cluster(s) – would be natural
targets of further knowledge discovery with these data.
To contrast how other forms of linkage perform with these gene expression data,
Figure 11.5 displays the results of single-link and complete-link clustering (still using
the Euclidian distance metric). Comparing to Figure 11.4, clear structural differences are
apparent. Most evident is the disparate pattern given by single linkage, where clusters congeal
much faster at small levels of 𝛿. Complete linkage gives a clustering pattern with some
similarities to the average linkage result in Figure 11.4; however, obvious differences also

382
STATISTICAL DATA ANALYTICS
2
29
72
47
23
38
35
40
46 1
9437
17
32
51
114
56
57
14
54
11148
21
87
68
64
8
88
89
65
84
62
106
20
95
42
937
77
86
90
67
80
113
103
71
25
104
63
30
117
61
70
74
91
1074
36
76
59
66
78
16
55
100
1153
345
82
112
50
105
13
118
101
24
81
58
92
110
15
109
108
6
9783
28
69
11698
43
39
26
99
44
9
12
41
79
11
52
31
60
10
85
96
73
75
22
45
102
49
19
27
53
18
33
0
2
4
6
8
10
δ
Figure 11.4
Average linkage dendrogram for gene expression data from Example 11.1.3. Underlying distance metric is Euclidean. Dashed
lines at 𝛿= 0.15 and 𝛿= 2.75 suggest possible clustering breaks (see text). Corresponding rectangles mark the clusters. Dendrogram leaf digits
(tiny terminal numbers) are patient codes. Source: Data from Uhlmann et al. (2012).

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
383
0
1
2
3
4
5
6
7
(a)
0
2
4
6
8
10
(b)
Figure 11.5
(a) Single linkage and (b) complete linkage hierarchical clustering dendrograms for gene expression data from Example 11.1.3.
Underlying distance metric is Euclidean. Note difference in linkage distance vertical scales. For clarity of presentation, dendrogram leaf labels
are suppressed. Source: Data from Uhlmann et al. (2012).

384
STATISTICAL DATA ANALYTICS
emerge. In particular, notice the group of patients towards the far left that remains separate
from the larger cluster through much higher levels of 𝛿.
On balance, this analysis corroborates the usual caveat that the three linkage meth-
ods can exhibit substantial differences in their hierarchical clustering patterns. (Also see
Exercise 11.5.)
◽
As noted above, rather than hierarchically agglomerating from an initial set of K sin-
gleton clusters (starting with a single observation each), one can alternatively reverse the
process and begin with a single cluster containing all n observations. Such divisive clustering
hierarchically divides clusters based on widening linkage distances. Algorithms for divisive
clustering tend to require more computation than those for agglomerative clustering, and no
clear evidence has emerged that the divisive strategy creates more-precise clusters than the
agglomerate approach. As a result, agglomerative clustering is often favored in practice. For
more on divisive hierarchical clustering, see Clarke et al. (2009, Section 8.2.2).
11.1.2
Partitioned clustering
An alternative strategy for performing cluster analysis is to partition the observations into K ≥
1 distinctly encapsulated clusters within the p-variate space under study. (Some users apply
the alternative term segmentation, when the goal is to partition the observations into a clear
set of K focused, if unknown clusters.) As noted above, partitioned clustering is often how a
novice analyst envisions the process of finding unsupervised clusters in multivariate data.
Partitioned clustering is somewhat distinct from the hierarchical methodology in the pre-
ceding section, and as a result, partitioning/segmentation algorithms are quite different. A
critical assumption is that the number of clusters, K, be known in advance. For example, a
business may wish to partition its customer base into K segments, corresponding to the K
known members of its salesforce (Hastie et al. 2009, Section 14.3.11). Or, preliminary anal-
yses may have identified K clear groupings in the data, such as K general body sizes for a
manufacture’s clothing lines, and the goal is to specify the cluster memberships for product
manufacturing planning (Hand et al. 2001, Section 9.3). As with any cluster analysis, how-
ever, the goal remains to form clusters where the data are more similar to each other within a
cluster and more different from each other between the K clusters.
Denote a given collection of clusters again as {C1, … , CK}. For a measure of cumulative
within-cluster variation, a common suggestion is
W(C1, … , CK) =
K
∑
k=1
∑
i∈Ck
h∈Ck
d2(zi, zh) ,
(11.2)
where d(zi, zh) is a distance metric taken, for example, from Table 11.2. (Note that some
authors employ the original distance d(zi, zh), rather than its square, in (11.2).) The corre-
sponding cumulative between-cluster variation is
B(C1, … , CK) =
K
∑
k=1
∑
i∈Ck
h∉Ck
d2(zi, zh) ,

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
385
where W(C1, … , CK) and B(C1, … , CK) sum to the total variation
T = W(C1, … , CK) + B(C1, … , CK).
The goal then is to minimize W(C1, … , CK) or, equivalently, maximize B(C1, … , CK) for
a given K. (If the domain-specific application calls for optimization of a more specialized
target ‘score’ related to within- and between-cluster variation, a number of possibilities exist,
for example, minimize W(C1, … , CK)∕B(C1, … , CK). See Hand et al. (2001, Section 9.4)
for an instructive discussion.)
Clarke et al. (2009, Section 8.1.1), following Hastie et al. (2009, Section 14.3.6), noted
that if, in (11.2), d(zi, zh) is taken as Euclidean distance, then the cumulative within-cluster
measure can be written as a sum of squared deviations:
W(C1, … , CK) = 2
K
∑
k=1
∑
i∈Ck
(zi −zk)2 ,
(11.3)
where, as given earlier, zk is a row vector representing the kth cluster’s centroid
zk = [z+1k
z+2k · · · z+pk]
(11.4)
with elements z+jk = ∑
i∈Ckzij∕𝜈k and where 𝜈k is the number of observations in the kth cluster.
Use of Euclidean distance is common here, and focus for the remainder of this section is on
minimization of the cumulative within-cluster sum of squares in (11.3).
Ostensibly, minimizing (11.3) for fixed K is a straightforward combinatorial exercise:
calculate the cumulative within-cluster sum of squares W(C1, … , CK) for all possible assign-
ments of the n observations zi into K different clusters, and choose that assignment minimizing
W(C1, … , CK). This conceptual simplicity conceals some potentially extensive computa-
tions, however. The number of possible assignments is
1
K!
K
∑
k=1
(−1)K−k (K
k
)
kn
(Jensen 1969), and for even moderately sized values of n, this quickly becomes intractable. For
example, with n = 25 and K = 3, the number is roughly 1011. In practice, analysts search for
the minimum only over a portion of the assignment space. A popular algorithm that achieves
this goal is known as K-means clustering (Hastie et al. 2009, Section 14.3.6):
KM.1. Designate a group of K ≥2 different locations in the p-dimensional space under
study to serve as the initial cluster ‘centroids.’ These can be a selection of K indi-
vidual observations or other well-separated locations suggested by prior knowl-
edge. (Avoid placing the initial points too close, as this can lead to unstable cluster
outcomes.)
KM.2. For each observation zi, i = 1, … , n, find the closest centroid zk and assign zi to
the cluster centered about that zk.
KM.3. Calculate the new centroids zk from (11.4) for the resulting K clusters.
KM.4. Iterate through Steps KM.2 and KM.3 until the cluster assignments do not change.

386
STATISTICAL DATA ANALYTICS
In Step KM.2, ‘closeness’ between the point zi and the centroid zk is usually quantified by
Euclidean distance
[ p
∑
j=1
(zij −z+jk)2
]1∕2
.
Notice that in vector notation, this can be written as [(zT
i −zT
k )T(zT
i −zT
k )]1∕2 for the row vector
zi. If strongly heterogeneous correlation exists among the underlying observations, one can
adjust by appealing instead to Mahalanobis distance
[
(zT
i −zT
k )T ̂Σ−1(zT
i −zT
k )
] 1
2 ,
(11.5)
where ̂Σ is the sample covariance matrix with (j, q)th element
sjq =
1
n −1
n
∑
i=1
(zij −zj)(ziq −zq) ,
j, q = 1, … , p (Clarke et al. 2009, Section 8.1.1). When the zijs are z-scores, however, we
know zj = 0 for all j and so some simplification is possible. Indeed, as noted above, the sample
covariance matrix for a matrix Z of standardized z-scores zij is just the correlation matrix
R = ZTZ∕(n −1). Thus (11.5) reduces to [ziR−1zT
i ]1∕2.
K-means is a form of ‘greedy search’ or ‘greedy descent’ optimization; it attempts to step
towards improved values of the cumulative within-cluster sum of squares as it iterates through
the developing clusters. From a computational efficiency standpoint, K-means can drastically
reduce the execution time required to report a K-cluster solution, and especially for very large
p (high dimension) or large n (‘big’ data), it is often the only practical choice for performing
partitioned clustering.
Unfortunately, the K-means algorithm cannot guarantee an optimal solution, and it does
occasionally get caught at a local minimum. A common suggestion is to run the algorithm
from a number of starting points and choose the clustering solution with the lowest cumulative
within-cluster sum of squares. Clarke et al. (2009, Section 8.1.1) recommended using at least
min{10, n} different starting positions.
As a computing algorithm, K-means developed in the mid-twentieth century from multiple
sources and over a variety of disciplines. As a result, the algorithm exists in different forms
and has many modified versions, each focusing on a specific manipulation of the basic greedy
descent strategy. Early examples include suggestions by Lloyd (1957, 1982), Forgy (1965),
and MacQueen (1967); also see Jain and Dubes (1988, Section 3.3) and Hand et al. (2001,
Section 9.7). A version often favored by data analysts is given by Hartigan and Wong (1979),
where an observation is prevented from moving between two clusters if the resulting solution
increases the cumulative within-cluster sum of squares. The Hartigan and Wong algorithm is
in fact the default in R’s kmeans() function.
Example 11.1.4 Automobile characteristics (Example 11.1.2, continued). To illustrate
the K-means algorithm for partitioned cluster analysis, return again to the automobile
characteristics data in Table 11.1. Recall that this simple data set has only n = 8 obser-
vations, with p = 2 outcome variables, z2 = {Standardized Acceleration} plotted against
z1 = {Standardized Engine Displacement} as in Figure 11.1. In the figure, two clear clusters

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
387
appeared evident, so set K = 2. To perform a K-means cluster analysis in R, we can apply
the commands
> Z <- cbind( z1, z2 )
> kmeans( Z, centers=2, nstart=8, algorithm=‘Hartigan-Wong’ )
where Z is the 8 × 2 matrix of standardized observations built from the individual-variable
(column) vectors. The option centers=2 instructs R to set K = 2; because this is a number
(as opposed to a vector of initial centroid values) R chooses a random set of rows from Z
as the initial cluster centers. To avoid having this result in a local minimum, the nstart=8
option calls for eight different initial centroids. The resulting output from kmeans() (edited
for presentation) is
K-means clustering with 2 clusters of sizes 5, 3
Cluster means:
z1
z2
1 -0.702148
0.6338996
2
1.170247 -1.0564993
Clustering vector:
[1] 1 2 2 2 1 1 1 1
Within cluster sum of squares by cluster:
[1] 1.3297430 0.7390515
(between_SS / total_SS =
85.2 %)
The output provides the final cluster centroids as z1 = (−0.702, 0.634) and z2 = (1.170,
−1.056). The corresponding individual, within-cluster sums of squares are given as
∑
i∈C1(zi −z1)2 = 1.330 and ∑
i∈C2(zi −z2)2 = 0.739, so that W(C1, C2) = 1.330 + 0.739 =
2.069. (This latter quantity is available as the tot.withinss attribute from the R object
created by the call to kmeans.)
Cluster membership is given by the location of indices in the Clustering vector. The
sequence 1 2 2 2 1 1 1 1 indicates that cluster C1 contains automobiles with indices
i = 1, 5, 6, 7, 8, while cluster C2 contains those with indices i = 2, 3, 4. The data were entered
into R such that these indices correspond to the top-to-bottom order presented in Table 11.1;
thus C1 contains {Chevelle, Rebel, Satellite, Skylark, Torino} and C2 contains {Fury, Galaxie,
Impala}. This is, of course, the same low-displacement/slow-acceleration versus large-
displacement/fast-acceleration delineation identified in Example 11.1.1. It is also the seg-
mentation available in the hierarchical average-linkage dendrogram from Figure 11.3 when
values of the linkage lie between 𝛿= 1 and 𝛿= 2.6.
A further partitioning into K = 3 clusters is worth a brief investigation here. The effort
is simple enough: in R, simply change to centers=3 when invoking kmeans. The resulting
output (edited for presentation) is
K-means clustering with 3 clusters of sizes 3, 3, 2
Cluster means:
z1
z2
1 -0.6401349
0.9618874
2
1.1702466 -1.0564993
3 -0.7951676
0.1419178

388
STATISTICAL DATA ANALYTICS
Clustering vector:
[1] 1 2 2 2 1 3 1 3
Within cluster sum of squares by cluster:
[1] 0.3936573 0.7390515 0.1004234
(between_SS / total_SS =
91.2 %)
The cumulative within-cluster sum of squares has now dropped to W(C1, C2, C3) = 0.3934
+ 0.739 + 0.100 = 1.233 (which is not surprising: W generally decreases with increasing
K), and the ratio of cumulative between-cluster sum of squares to total sum of squares has
increased from 85.2% to 91.2%.
From the Clustering vector output of 1 2 2 2 1 3 1 3, we see the trifurcation
into K = 3 clusters has moved autos with indices i = 6, 8 out of C1 into their own, separate
cluster C3. It has also left the large-displacement/fast-acceleration cluster C2 unchanged. That
is, C1 now contains only {Chevelle, Rebel, Skylark}, C2 continues to harbor {Fury, Galaxie,
Impala}, and the new C3 houses {Satellite, Torino}. Referring to Figure 11.1, this has further
segmented the low-displacement/slow-acceleration automobiles along an acceleration gradi-
ent: C1 is a very slow-acceleration cluster, while C3 contains low-displacement vehicles with
moderate acceleration.
Intriguingly, this three-cluster partitioning differs from the three-cluster arrangement sug-
gested by the average-linkage dendrogram in Figure 11.3: there, the hierarchical construction
retains the five-element low-displacement/slow-acceleration cluster {Chevelle, Rebel, Satel-
lite, Skylark, Torino} but separates Galaxie into a singleton cluster, leaving {Fury, Impala} as a
paired high-displacement/very fast-acceleration cluster. This illustrates the common warning
that the different strategies for cluster analysis can sometimes produce conflicting results. Still,
disparities such as these serve as useful springboards for exploring the underlying features of
the data and may lead to new knowledge discoveries.
A graphical display of the separated clusters is known as a Voronoi tessellation or a Voronoi
diagram (Schoenberg 2012). This is a formal construction that uses optimally placed line
segments to partition the feature space into disjoint ‘cells,’ where any point in the ith cell is
closer to that cell’s centroid than to any other centroid. Obviously, this overlaps with many of
the goals of a partitioned cluster analysis.
In R, the external deldir package can produce Voronoi tessellations (also called Dirichlet
tessellations) via its deldir() function. Sample code for the automobile characteristics data
is
> auto8.km <- kmeans( cbind(z1,z2), centers=3, nstart=8,
algorithm=‘Hartigan-Wong’ )
> require( deldir )
> auto8.dd <- deldir( auto8.km$centers[,1],
auto8.km$centers[,2] )
> plot( auto8.dd, wlines=‘tess’, lty=‘solid’ )
Figure 11.6 displays the tessellation for the three-cluster partitioning, using information from
the three-cluster kmeans() output. The original data are overlaid for enhanced visualization.
Tessellations are particularly useful in geographic analytics for partitioning cartographic
clusters, because the vertices of the tessellation’s polygons represent optimized distances from
cluster centroids. They can be used to service nearest-neighbor queries when, for example,

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
389
Standardized displacement
Standardized acceleration
+
+
+
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
Figure 11.6
Voronoi tessellation for automobile characteristics data in Table 11.1. Three
clusters from the K-means analysis in Example 11.1.4 are separated by tessellated line seg-
ments to produce cluster boundaries. Crosses (+) indicate cluster centroids. Dark circles (•)
are overlaid scatterplot of z2 = {Standardized acceleration} against z1 = {Standardized engine
displacement} (slight jitter added to distinguish adjacent points). Source: Data from Myatt
(2007, Table 6.8).
planning mass-transit circuits, separating water supplies from hazardous waste sites, locating
mobile telephone towers. For more details, see Linoff and Berry (2011, Chapter 13).
◽
When K is unknown, it may still be possible to perform a partitioned cluster analysis,
although the effort becomes even more exploratory in nature. One simply conducts the par-
titioning for each value of K = 1, … , n −1 and saves the cumulative within-cluster sum of
squares at each K, say, WK(C1, … , CK). Since WK is generally decreasing in K (indeed, Wn =
0), so is any monotonic measure based on WK, such as log{WK} or
WK
T −WK
.
A plot of WK or log{WK} versus K often shows an abrupt change in the decreasing pattern as
K →n. Similar to the scree plot in principal components analysis (Section 10.2.2), if such a
precipitous ‘elbow’ appears, that point may represent a putative choice for the unknown K.

390
STATISTICAL DATA ANALYTICS
Another popular measure is the ‘pseudo-F’ statistic (Cali´nski and Harabasz 1974):
F = (T −WK)∕(K −1)
WK∕(n −K)
,
(11.6)
so-named because it imitates the F-ratio in (7.18) from an analysis of variance. When plotted
against K, a large local spike in (11.6) indicates that between-cluster variation is high rela-
tive to within-cluster variation at that K. This becomes another possibility for the unknown
number of clusters and can validate a choice of K when, for example, both an elbow plot
and a pseudo-F plot suggest the same value. The pseudo-F is straightforward to calculate
directly and is also available in R via, for example, the index.G1() function of the external
clusterSim package.
Selection of the number of clusters in an exploratory cluster analysis is an active area
of analytic and statistical research, and advances occur on a regular basis. See, for example,
Koepke and Clarke (2013), Gopal et al. (2012), and the reference therein.
Example 11.1.5 Home center financial performance. Cluster analysis is a popular
methodology for unsupervised learning with business and financial data. For example, Clark
(2010) compiled business performance data for calendar year 2009 on n = 497 US companies
engaged in home center/building supply sales. Recorded were the following variables:
x1 = Annual sales (in $M)
x2 = Sales growth (percentage of increase or decrease from previous calendar year)
x3 = Number of store units (dedicated to or including home/building supply)
x4 = Number of employees (staffing home/building supply)
Table 11.4 displays the data. (As previously, only a selection of observations is provided
in the table. The complete set of data is available at http://www.wiley.com/go/piegorsch/data
_analytics.) Notice that many companies reported decreases in sales from calendar 2008 to
2009, as measured by variable x2, commensurate with concurrently tight conditions in the US
economy. This was part of the focus in this particular study.
Consider here a partitioned cluster analysis via the standard K-means algorithm, to deter-
mine if any special (or obvious) patterns exist in these data. Begin by standardizing the p = 4
Table 11.4
Selected business performance data for calendar year 2009 on p = 4 financial
variables, from n = 497 US companies engaged in home center/building supply sales.
Outcome variables
Company
x1 = Sales ($M)
x2 = % change
x3 = Units
x4 = Staff
Home Depot
66 176.0
−7.2
2244
375000
Lowe’s
47 220.0
−2.0
1710
238000
Wal-Mart
25 700.0
−1.2
3708
143800
⋮
⋮
⋮
⋮
⋮
Peek’s Carpet & Tile
20.5
61.4
9
30
Theut Products
20.0
0.0
7
105
Source: Clark (2010).

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
391
variables from Table 11.4 into z1, z2, z3, and z4. To set a value for the number of clusters, take
a series of K-means fits over increasing K up to, say, K = 6 and explore some of the measures
mentioned earlier. A typical R command for each fit could be
> Z <- cbind( z1, z2, z3, z4 )
> kmeans( Z, centers=4, nstart=100, algorithm=‘Hartigan-Wong’ )
for K = 4. As the number of observations is reasonably large here, the command includes
nstart=100 starting values to help to ensure a valid clustering solution. Varying this over
other values of K and extracting the overall within-cluster sum of squares WK (in R, this is
the tot.withinss attribute from the kmeans() object) leads to an elbow plot of WK versus
K. Figure 11.7 displays the plot, along with an alternative pseudo-F plot for determining K.
Both graphics indicate selection of K = 4 for the number of clusters with these data.
The four-cluster solution from kmeans() gives the following R output (edited for presen-
tation):
K-means clustering with 4 clusters of sizes 2, 260, 5, 230
Cluster means:
[,1]
[,2]
[,3]
[,4]
1
14.49536999
0.07695562
5.5630081 14.34938073
2
-0.07905351
0.85948194 -0.1080104 -0.07761037
3
2.45768662 -0.79002619
8.9206856
2.50025098
4
-0.09010983 -0.95508298 -0.1202032 -0.09139704
Within cluster sum of squares by cluster:
[1] 35.12519 66.59292 62.08312 48.57776
(between_SS / total_SS =
89.3 %)
Notice the very small cluster sizes for cluster C1 (two elements) and C3 (five elements). The
corresponding 497-element Clustering vector (not shown; also available as the clus-
ter component from the kmeans() object) gives the four-cluster delineation as summarized
1
2
3
4
5
6
500
1000
1500
2000
K
WK
Elbow/break at K = 4 
2
3
4
5
6
600
800
1000
1200
1400
K
(a)
(b)
Pseudo-F
Sharp jump at
K = 4
Figure 11.7
(a) Cumulative within-cluster sum of squares WK and (b) pseudo-F statistic
plotted against number of clusters, K, for home center data in Example 11.1.5. Both plots
indicate selection of K = 4.

392
STATISTICAL DATA ANALYTICS
Table 11.5
Four-cluster K-means solution for business performance data from Table 11.4.
Cluster
Cluster size
Companies (for larger clusters, only selected elements are given)
C1
𝜈1 = 2
Home Depot, Lowe’s
C2
𝜈2 = 260
Menards, Dai-Tile, … , Peek’s Carpet & Tile, Theut Products
C3
𝜈3 = 5
Wal-Mart, CCA Global Partners, Sears, Sherwin-Williams,
Fastenal
C4
𝜈4 = 230
ProBuild Holdings, Sutherland Lumber, … , 41 Lumber, Allen
& Allen
in Table 11.5. We see that the two megastores, Home Depot and Lowe’s, are the only compo-
nents of cluster C1, which is not surprising given their substantial presence in this market. The
other small cluster, C3, is made up of very large North American companies that either have
extensive, specialized commitments to building and hardware supply, such as Fastenal (fas-
teners and hardware) and Sherwin-Williams (paints/paint supplies), or are large businesses
with extensive hardware/home center subunits (Wal-Mart, CCA Global Partners, Sears).
The remaining two clusters in Table 11.5, C2 and C4, are almost evenly split in terms of
size; each makes up slightly less than half of the total collection. Cluster C2 appears to house
some of the larger companies in terms of physical resources. For example, the following R
commands and output summarize the separate store staff counts (from Table 11.4) for each
cluster, with accompanying output:
> HomeK4.km <- kmeans( Z, centers=4, nstart=100 )
> summary( Staff[which(HomeK4.km$cluster == 2)] )
Min. 1st Qu.
Median
Mean
3rd Qu.
Max.
14.0
100.0
175.0
660.6
333.8 40000.0
> summary( Staff[which(HomeK4.km$cluster == 4)] )
Min. 1st Qu.
Median
Mean
3rd Qu.
Max.
20.0
85.0
130.0
368.4
250.0 11320.0
Almost every comparison measure is greater for cluster C2, and these become substantial for
the higher measures such as the upper quartile and maximum. Similar results occur for the
separate store unit counts:
> summary( Units[which(HomeK4.km$cluster == 2)] )
Min. 1st Qu.
Median
Mean
3rd Qu.
Max.
1.00
4.00
7.00
24.55
16.00
930.00
> summary( Units[which(HomeK4.km$cluster == 4)] )
Min. 1st Qu.
Median
Mean
3rd Qu.
Max.
1.00
3.00
5.00
20.35
10.00
474.00
To visualize the four-cluster solution’s effect on the original data, Figure 11.8 presents a
scatterplot matrix with clusters labeled in each panel. The plot was constructed using the
R command
> pairs( Home.df,
panel=function(x,y) {text(x,y,HomeK4.km$cluster)} )

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
393
Sales
−60
−20 0 20 40 60
1
1
3
32
33 2
43 2
2
2
2 444 42
42
2
42
4
2
4
2
4
2
4
2
4
4
2222
24 2
2
4
2
42
42
4 442
22
4 2
4 44
2
4
4 4444
4 22
4 2
4
2
22
2
2
4
4 2
22
42
444
4 44 2
4
2
22
42
42
44
2
2
2
2
4 22
2
2
4 22
222
22 22
4444
4
4
2
422
4 4422
2
22
444
4 2
4
2
4
2
2
4422
2
22
2
42
44422
4 24
4 22
22
42
4 2
44 2
4
22
22
2
2 4
22
2
22
2 44
4222
4 2
4 2
4 4
444
2
4
444
2
4
442
444 22
2
44 2
22
424 22
4
4 2
42
4
2
2
2
44 4
4222
2
2
2
4
22
2
4244
2
2
2
2
2
422
222
4 24
42
4 2 2
4 2 2
4
4442
4
2
224
2
4
4 2
4 22 2
4 22
42
4
2
24
22
2
4
2
4
2
44 24
2 2
2
442
4
4
2
2
442
4
4
2
4 44 24
2
24
42222 2
2
24
2 2
4
4
4 2422
2
4444
4 2
44442222
44
444 42
44 2
4 442422
422
4 2
4
2
222
4 22
2
2
44444 2
2 44 2
2 2
2
2
42
4
2
2
2
2 2
4
44 22
4
44
4 4
4 22
4 2
2
2
2
44
4 242
4
4
2
44444 22
22
4
4 22
2
4
444 2
1
1
3
3
2
3
3
2
4
3
2
2224
44
424224
242
424
2424422
2224224
242424 4
4222
4244424444444224242222244222422444444242224242442222422224222222222444444242244422222244442424224422222242444224244222242424424222222422222444222424244444244442442444222422242422442424222444422222242224244222224222224244242422444242222424442442224224242242224242442422224242244244244424224422222224244424224444244442224444424424424242224242224222224444224422222242422222444422444442242222244424244424444422244222244442
0e+00
2e+05
0
20000
40000
60000
1
1
3
323
3
243222244442422424242424244222224224242424442224244424444444422424222224422242244444424222424244222242222422222222244444424224442222224444242422442222224244422424422224242442422222422244422242424444244442442444222422242422442424224444222224222424222224222224244242424424222424442444222422424224224242442422224424422442442444242422222224224424224444442444422224424424242242242422242222444224422222242422224442444422422244424244424442242244442
−60
−20
0
20
40
60
1
1
3
32
3
3
2
43
22
2
2
444
42
42
2
42
4
2
424
2
4
2
4
4
2222
24
224242
4
2
4
44
2224
2
4
4424
4
44444
224
24
2
22
2
2
44
2
22
42
444
4
44
24
2
22
4242
44
2
2
2
24
2
2
2
4
2222222
22
4
444
44
2
422
4
4422
2
224444
24
2
4
22
44222
22
242
4
4422
4
244
222424
2
44
2
4
22222
2
4
22
2
22
2
444222
4
24
2
4
4
444
2
4444
2
4442444
222
44
222
424
22
44
2
424
2
2
2
44
4422
2
2
2
2
422
242442
2
2
2
2
422222
4
24
42
4
2
2
4
2
2
4
4442
4
2
24244
244
2
2
4
2
42
4
2
24
22
2
4
2
4
2
4
242
2
2
44244
2
2
4424
4
2
4
44
24
2
2442222
2
2
24
2
2
444
24222
4444
4
2
44442222
4444
42
44
2
4
442422
4224
2
4
2222
4
22
2
44444
2
2
44
22
2
2
2
424
2
2
2
2
2
4
44
2444
4
44
2
4
22
2
2
444
24244
2
44444
222
4
4
22
2
4444
2
PrcntChange
1
1
3
3
2
3
3
2
4
3
2
2
2
2
4
44
42
42
2
4
2
4
2
424
2
4
2
4
4
22
22
24
224
242
4
2
4
4
4
222
4
2
4
44244
4
44444
224
24
2
22
2
2
44
2
22
422
444
4
44
24
2
22
4242
44
2
2
2
24
22
2
2
4
2222222
22
4
444
44
2
422
4
44222
2
224444
24
2
4
22
44222
22
242
4
4422
4
244
2222424
2
44
2
4
22222
2
4
22
2
22
2
444222
4
24
2
4
4
444
2
4444
2
4442444
222
44
222
424
22
44
2
424
2
2
2
444
4422
2
2
2
2
422
242442
2
2
2
2
422222
4
24
42
4
2
2
4
2
2
4
4442
4
2
22242444
244
22
2
4
22
42
4
2
24
22
2
4
2
4
2
44
242
2
22
44244
2
2
4424
4
2
4
44
24
2
2442222
2
2
24
2
2
444
24222
44444
4
2
44442222
44444
42
44
2
4
442422
42224
2
4
2222
4
2222
2
44444
2
2
44
222
2
2
2
424
2
2
2
2
2
44
44
22444
4
44
22
4
222
2
2
444
242444
2
44444
222
4
4
222
2
4444
2
1
1
3
32
3
3
2
43
22
2
2
444
42
42
2
42
4
2
424
2
4
2
4
4
2222
24
224242
4
2
4
44
2224
2
4
44244
4
44444
224
24
2
22
2
2
44
2
22
422
444
4
44
24
2
22
4242
44
2
2
2
24
22
2
2
4
2222222
22
4
444
44
2
422
4
44222
2
224444
24
2
4
22
44222
22
242
4
4422
4
244
2222424
2
44
2
4
22222
2
4
22
2
22
2
444222
4
24
2
4
4
444
2
4444
2
4442444
222
44
222
424
22
44
2
424
2
2
2
444
4422
2
2
2
2
422
242442
2
2
2
2
422222
4
24
42
4
2
2
4
2
2
4
4442
4
2
22242444
2444
22
2
4
22
42
4
2
24
22
2
4
2
4
2
44
242
2
22
44244
2
2
4424
4
2
4
44
24
2
2442222
2
2
24
2
2
444
24222
444444
4
2
44442222
4444
42
44
2
4
442422
4224
2
4
2222
4
222
2
44444
2
2
44
222
2
2
2
424
2
2
2
2
2
444
44
2444
4
44
22
4
222
2
2
444
242444
2
44444
2222
4
4
22
2
4444
2
1
1
3
3
2
3
3
2
4
3
222244442422424242424244222224224242424
4
4222424442444444422424222224422242244444242224242442222422224222222222444444242244422222244442424224422222242444224244222242424424222222422222444222424244444244442442444222422242422442424222444422222242224244222224222224244242422444242222424442442224224242242224242442422224242244244244424224422222224244424224444244442224444424424424242224242224222224444224422222242422222444422444442242222244424244424444422244222244442
1
1
3
3
2
33
2
4
3
2
2
2
2 444 42
42
2
42
4
2
4
2
4
2
4
2
4
4
2222
24 2
2
4
2
42
42
4
4
42
22
4 2
4 44
2
44
4 4444
4 22
4 2
4
2
22
2
2
4
4 2
22
422
444
4 44 2
4
2
22
42
42
44
2
2
2
2
4 22
2
2
4 22
222
22 22
4444
4
4
2
422
4 44222
2
22
444
4 2
4
2
4
2
2
4422
2
22
2
42
44422
4 24
4 22
22
42
4 2
44 2
4
22
22
2
2 4
22
2
22
2 44
4222
4 2
4 2
4 4
444
2
4
444
2
4
442
444 22
2
44 2
22
424 22
4
4 2
42
4
2
2
2
444 4
4222
2
2
2
4
22
2
4244
2
2
2
2
2
422
222
4 24
42
4 2 2
4 2 2
4
4442
4
2
2224
2
44
4 2
44 22 2
4 22
42
4
2
24
22
2
4
2
4
2
44 24
2 2
22
442
4
4
2
2
442
4
4
2
4 44 24
2
24
42222 2
2
24
2 2
4
4
4 2422
2
44444
4 2
44442222
44
444 42
44 2
4 442422
4222
4 2
4
2
222
4 222
2
2
44444 2
2 44 2
22 2
2
2
42
4
2
2
2
2 2
44
44 22
4
44
4 4
4 22
4 2
22
2
2
44
4 242
44
4
2
44444 2
22
4
4 222
2
4
444 2
Units
0
1000
2000
3000
1
1
3
3
2
3
3
2
4
3
222244442422424242424244222224224242424
4
42224244424444444422424222224422242244444424222424244222242222422222222244444424224442222224444242422442222224244422424422224242442422222242222244422242424444424444244424442224222424224424242224444422222242224244222224222224244242422444424222242444244422242242422422242424424222244244224424424442422442222222422444242224444442444422224444424424424224222424222422222444422442222224242222244444224444422422222444242444244444222244222244442
0
200004000060000
0e+00
1e+05
2e+05
3e+05
1
1
3
323
3243222244442422424242424244222224224242424442224244424444444422424222224422242244444242224242442222422224222222222444444242244422222444424242244222222424442242442222424244242222224222244422242424444244442444244222422242422442424224444422222242224244222224222224244242424444242242444244422242242422422424244242222424422442442444242242222222422442422244442444422224442442442422422424222422222444442244222224242222444422444422242222444242444244422242222442
1
1
3
32
33 2
43 2
2
2
2 444 42
42
2
42
4
2
4
2
4
2
4
2
4
4
2222
24 2
2
4
2
42
42
4 442
22
4 2
4 44
2
44
4 4444
4 22
4 2
4
2
22
2
2
4
4 2
22
422
444
4 44 2
4
2
22
42
42
44
2
2
2
2
4 22
2
2
4 22
222
22 22
4444
4
4
2
422
4 44222
2
22
444
4 2
4
2
4
2
2
4422
2
22
2
42
44422
4 24
4 22
22
42
4 2
44 2
4
22
22
2
2 4
22
2
22
2 44
4222
4 2
4 2
4 4
444
2
4
444
2
4
442
444 22
2
44 2
22
424 22
4
4 2
42
4
2
2
2
444 4
4222
2
2
2
4
22
2
4244
2
2
2
2
2
422
222
4 24
42
4 2 2
4 2 2
4
4442
4
2
2224
2
44
4 2
444 22 2
4 22
42
4
2
24
22
2
4
2
4
2
44 24
2 2
22
442
4
4
2
2
442
4
4
2
4 44 24
2
24
42222 2
2
24
2 2
4
4
4 2422
2
44444
4 2
44442222
44
444 42
44 2
4 442422
422
4 2
4
2
222
4 222
2
2
44444 2
2 44 2
2 2
2
2
42
4
2
2
2
2 2
444
44 22
4
44
4 4
4 222
4 2
22
2
2
44
4 242
44
4
2
44444 22
22
4
4 222
2
4
444 2
0
1000 2000 3000
1
1
3
3
2
3
3
2
4
3
2
2224
44
424224
242
424
2424422
2224224
242424 4
4222
4244424444444422424222224422242244444424222424244222242222422222222244444424224442222224444242422442222224244422424422224242442422222242222224442224242444442444424442444222422242422442424222444442222224222424422222422222424424242244442422224244424442224224242242224242442422224244224424424442422442222222422444242224444442444422224444424424424224222424222242222244444224422222242422222444442244444222422222444242444244444222244222244442
Staff
Figure 11.8
Scatterplot matrix for home center data in Example 11.1.5 (original scales) with
points labeled via K-means cluster assignments from four-cluster solution. Source: Data from
Clark (2010).
where Home.df is the original dataframe containing the p = 4 input variables and HomeK4.km
is the four-cluster K-means solution object (see above).
Figure 11.8 shows that the clustering has evident validity: points associated with clusters
C1 and C3 separate visibly from the rest in essentially every pairwise plot panel. Some points
from cluster C2 are also visible, although the skewed scale for the Sales, Units, and Staff
variables tends to compact the C2 and C4 observations. Exercise 11.8 analyzes the data using
logarithmic scales to improve resolution with these tightly condensed groupings.
◽

394
STATISTICAL DATA ANALYTICS
It is important to warn that as with any analytic method that employs arithmetic means,
the K-means algorithm is susceptible to outliers. To compensate, alternative algorithms call
on more-robust measures to calculate the cluster centers. For example, the K-medoids algo-
rithm replaces the centroid zk in (11.3) with the medoid, mk, of each kth cluster. The medoid
(not to be confused with the median; see next paragraph) of a cluster is defined as the indi-
vidual observation within the cluster whose average distance from all other observations in
the cluster is a minimum. Distance can be Euclidean, Mahalanobis, or any other desired mea-
sure of dissimilarity. The medoids are then used to represent the cluster centers, in place of
the outlier-vulnerable centroids. K-medoid clustering is available in a number of external R
packages, including cluster and fpc.
Another alternative to K-means takes a step further. The K-medians algorithm employs
a target score based on absolute-value distance, W∗
K = ∑K
k=1
∑
i∈Ck| zi −̃Q2k|, where ̃Q2k is
a cluster median (Clarke et al. 2009, Section 8.1.2). Expressions for ̃Q2k can vary, because
there is no unique definition of a multidimensional median (see Small 1990). The external
R package depth can calculate various forms of multivariate median, via its med() function.
A form of K-medians clustering is available in the R package flexclust, through its kcca()
function.
If the analyst is willing to adopt a formal probability model for the multivariate obser-
vations in X, the cluster analysis can be expanded to incorporate this information. In the
process, the results can become much more powerful – but much more dependent on the
underlying model – and expand the basic clustering paradigm past its exploratory roots. Such
model-based clustering often assumes that the multivariate observations are derived from a
mixture of K probability density functions (p.d.f.s), where each kth p.d.f. models variation in
observations from the kth cluster (k = 1, … , K). If the forms of the p.d.f.s are known, such as
K p-variate normal densities (Section 2.3.9) with possibly different means, variances, and/or
covariances, then maximum likelihood estimation (Section 5.2.4) can be applied to estimate
the unknown parameters. If K is itself assumed unknown it can also be estimated, using various
model selection procedures. As might be expected, the calculations for model-based cluster-
ing appeal to some rather sophisticated statistical techniques, and a full discourse exceeds the
scope here. Interested readers can find details in Everitt (2005, Section 6.4) or Clarke et al.
(2009, Section 8.3.1).
Clearly, many different techniques are available to perform exploratory cluster analysis.
No matter the strategy taken, however, it is important to warn that no catch-all method exists.
Each approach has its own strengths and weaknesses. In practice, the underlying clusters can
sometimes involve complex forms and shapes: think of a circular cluster completely sur-
rounded by a separate, ringed cluster (often called ‘bull’s-eye’ clustering; cf. Figure 9.10a)
or a pair of separate crescents pushed together so that one end of each crescent tucks into
the bowl of the other, creating a larger, sigmoidal pattern (‘half-moon’ clustering). Unusual
shapes such as these can be difficult to identify, especially if the number of dimensions p
grows very large. (Another consequence of the curse of dimensionality mentioned in Section
7.4.1: in high-dimensional space, the data points become more sparse and our ability to iden-
tify connections among them drops precipitously; see Clarke et al. (2009, Section 1.0.1).)
There may even be cases where no true clusters exist at all, but the clustering operation still
reports potential segmentations in the data. Cluster analysts must approach the calculations
with due caution – in some cases, even with a modicum of skepticism! – and be conscious

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
395
of the unsupervised aspects the learning effort entails. Hand et al. (2001, p. 296) provided a
useful, succinct philosophy (with original emphasis):
The important lesson … is that we must match the method to the objectives.
In particular, we must adopt a cluster analytic tool that is effective at detecting
clusters that conform to the definition of what is meant by ‘cluster’ in the problem
at hand. It is perhaps worth adding that we should not be too rigid about [cluster
identification]. Data mining, after all, is about discovering the unexpected, so
we must not be too determined in imposing our preconceptions on the analysis.
Perhaps a search for a different kind of cluster structure will throw up things we
have not previously thought of.
For more on cluster analysis, see the various references throughout this section, the modern
compendium by Everitt et al. (2011), and the classic text by Kaufman and Rousseeuw (1990).
11.2
Association rules/market basket analysis
As we have seen, many different methods exist to describe, summarize, and quantify associa-
tions among a series of multidimensional observations. Clustering (as in Section 11.1) studies
patterns in terms of distances between objects, classification (Chapter 9) allocates objects to
known categories based on observed similarities, correlation (Section 3.3.3) quantifies lin-
ear relationships between two variables, scatterplots and other graphical devices (Section
4.2) visualize relationships in two or more dimensions, and so on. Another analytic device
for divining how objects connect with one another is known as association rule learning; it
attempts to identify interesting or informative connections among a set of items or events as
they relate in an implicating (‘if then ’) manner.
The fundamental concept for association rules arose in studies of customer buying
patterns; hence, the coadunate moniker market basket analysis when association rules
are applied in consumer studies. An oft-cited scenario is purchasing behavior in grocery
stores, where customers buy ‘market baskets’ of various grocery items – eggs, milk, lettuce,
and so on – in assorted patterns. Some purchasing patterns are obvious – say, (almost)
always buying salad dressing when buying lettuce – while others can prove surprising and
instructive. For instance, a famous example identified a pattern with discount department
megastore Wal-Mart’s customers: apparently, purchasers of Barbie® dolls often also bought
one of three kinds of candy bars. While the underlying motivations for such an association
were ambiguous (Palmeri, C., ‘Believe in yourself, believe in the merchandise,’ Forbes 160
(5), 8 September 1997, pp. 118–124), the connotations were clear to Wal-Mart’s managers:
locate more candy bar displays either closer or on a pathway to the Barbie aisle. (Another,
infamous example related late-afternoon purchases of diapers with concomitant acquisitions
of beer, although this appears to be more of an urban myth than a success of association rule
mining; see Rao, S.S., ‘Birth of a legend,’ Forbes 161 (7), 6 April 1998, pp. 128–130.)
Originated by Agrawal et al. (1993) for the consumer setting, association rules have
expanded to many application areas, including biomedicine and genetics, architectural design,
geoinformatics, and communication and information systems, to name just a few. In this
section, an introduction is given to this rapidly evolving methodology in its most basic form.

396
STATISTICAL DATA ANALYTICS
11.2.1
Association rules for binary observations
Suppose a database of n ‘transactions’ (i.e., observations) = {T1, T2, … , Tn} is recorded
over p possible items = {I1, I2, … , Ip}. The items could be specific purchases such as milk
and lettuce (or diapers and beer!), or other characteristics such as purchaser’s sex, new/repeat
customer, high-school graduate, and so on. Item categories may also be extended to other set-
tings; for example, in medical studies, one might query disease status, prescribed medications,
insured/uninsured, and so on. Collect the data into an n × p matrix X, where the component
observations are binary: xij = 1 if transaction i resulted in purchase/activation of item j, and 0
otherwise.
As a simple touchstone example, consider the hypothetical transaction database for gro-
cery shopping excursions given by the matrix in Figure 11.9. Notice how the n = 12 transac-
tions differ across the p = 9 items (although transactions T3 and T7 are in fact identical).
Patterns of concomitant purchases among the rows of X are called itemsets. For
example, an itemset in a grocery store market analysis could be {lettuce, salad dressing}
or {eggs, lettuce, salad dressing}. Generically, an itemset is any collection of k ≥1 items
k = {Ij1, Ij2, … , Ijk} ⊂thought to exhibit potential associations among the n transac-
tions. The goal is to mine the transaction database and uncover useful relationships – called
association rules – among items that are (or are not) purchased together.
In the simple setting described here, a formal association rule connects a k-fold itemset
k with a singleton itemset 1 via an implicative association: {k ⇒1}. That is, pur-
chase/activation of k is presumed to induce purchase/activation of 1. The left-hand side
(‘LHS’) of the rule is called the antecedent and the right-hand side (‘RHS’) is the consequent.
(Actually, the directed implication here is artificial, because the database does not provide a
causal relationship for the purchase of k leading to purchase of 1. Technically, the data
themselves only indicate whether the two itemsets are purchased in the same transaction.
Causal motivations must be taken from external knowledge or assumptions on the customers’
Items
Items
I1 = Eggs
Transactions
I1
I2
I3
I4
I5
I6
I7
I8
I9
I2 = Milk
T1
0
0
1
1
1
1
1
1
0
I3 = Cheese
T2
1
0
0
1
0
0
0
0
0
I4 = Lettuce
T3
0
1
0
0
0
0
0
0
0
I5 = Salad dressing
T4
1
0
0
1
1
0
0
0
0
I6 = Onions
T5
0
1
1
1
1
1
0
0
1
I7 = Pepper
T6
0
1
0
1
1
0
1
0
0
I8 = Mustard
T7
0
1
0
0
0
0
0
0
0
I9 = Ice cream
T8
0
1
1
0
0
0
1
0
0
T9
1
1
0
1
0
1
0
0
0
T10
1
1
1
1
1
1
0
0
1
T11
0
1
0
1
1
0
0
1
1
T12
0
1
1
0
0
1
1
0
1
Figure 11.9
Hypothetical market basket transaction database for n = 12 grocery store
customers.

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
397
purchasing behaviors. To remain consistent with the association rule literature, however, the
‘⇒’ notation is retained here.) For simplicity, 1 is restricted here to a single item, although
the framework can be extended to include m-fold consequents, m.
11.2.2
Measures of rule quality
Viewing the xijs as realizations of some binary random variable, the itemsets k and 1
become ‘events.’ Thus, the corresponding association rule {k ⇒1} has some probabil-
ity of occurrence P[k ∧1], where the ∧symbol indicates a ‘logical and’ operation. An
estimate of this probability is available from the database: simply divide the number of times
{k ⇒1} is observed by the total number of transactions:
S(k ⇒1) = # occurrences of {k ⇒1}
n
.
This is known as the support of the association rule {k ⇒1}. (The term is not to be con-
fused with the support space of a probability function (p.m.f. or p.d.f.) from Section 2.1.)
From this, the accuracy of the rule {k ⇒1} is the ratio of its support to the support of
the antecedent k:
A(k ⇒1) = S(k ⇒1)
S(k)
.
(This should not be confused with the similarlynamed accuracy measure from classification
analytics in Section 9.1.2.) In effect, accuracy can be viewed as an estimate of the condi-
tional probability P[1|k]. Strong association rules will have accuracies as large as possible,
preferably close to 1.
Accuracy for an association rule is also called confidence by many in the machine learning
literature. This is, of course, unfortunate usage because it has nothing to do with the confidence
intervals of Section 5.3.1.
The support and accuracy of an association rule are measures for its overall quality. Rules
that appear often in the transaction database have high S(k ⇒1) and, therefore, will involve
large numbers of customers. Acting on a rule whose support is high, say, exceeding a min-
imum threshold so ∈(0, 1), has the potential to affect a large portion of the customer base.
Further, rules with high accuracy appear better at predicting consequent purchases from their
antecedents. Here too, acting on rules where A(k ⇒1) exceeds some minimum accuracy
threshold co ∈(0, 1) brings the potential to affect more customers and increase future trans-
actions (and, in market applications, revenues; see Section 11.2.3.
For example, with the hypothetical transactions in Figure 11.9, consider the simple asso-
ciation rule {lettuce ⇒salad dressing}. That is, set {1} = {I4} and {1} = {I5}. Inspection
of the transaction matrix shows that the support here is S(I4 ⇒I5) = 6/12 or 50%, a fairly
common association in this database. (Support thresholds vary greatly in practice; values of
so can range from as low as 1% up to 20% or even 50%, depending on the domain-specific
targets.) Accuracy of the rule is then
A(I4 ⇒I5) = S(I4 ⇒I5)
S(I4)
=
(
6
12
)
(
8
12
) ,

398
STATISTICAL DATA ANALYTICS
or 75%. In practice, an accuracy threshold of at least co = 50% is typical; thus the rule associat-
ing {lettuce ⇒salad dressing} is fairly accurate. (Indeed, in this hypothetical database, shop-
pers never purchased salad dressing without also purchasing lettuce.) Or, let {2} = {I2, I3}
and {1} = {I9}, that is, the rule {milk, cheese ⇒ice cream}. Then S(I2, I3 ⇒I9) = 3/12 =
25% is less often observed, although its accuracy
A(I2, I3 ⇒I9) = S(I2, I3 ⇒I9)
S(I2, I3)
=
(
3
12
)
(
4
12
)
is also 75%. Thus this latter rule is a similarly interesting candidate for further study.
Of additional use is appeal to measures of association/correlation for which objective
break-points exist. A popular construct in association rule mining is the lift (also called
improvement). The lift of a rule is the ratio of its accuracy to the support of its consequent:
L(k ⇒1) = A(k ⇒1)
S(1)
= S(k ⇒1)
S(k)S(1) .
(11.7)
As a measure of rule quality, lift expands on the accuracy A(k ⇒1) by quantifying how
‘independent’ the antecedent is from the consequent when the latter is viewed in a standalone
sense. That is, if the ‘event’ k were statistically independent from 1, we would expect
the final numerator in (11.7) to equal the final denominator, so that L(k ⇒1) ≈1. When
L(k ⇒1) > 1, the proposed rule appears to improve over simple independence, while if
L(k ⇒1) < 1, the rule is less informative than assuming the two itemsets are independent.
For example, in Figure 11.9, while both {I4 ⇒I5} and {I2, I3 ⇒I9} have similar accura-
cies of 75%, their lifts differ, providing more discrimination. Using (11.7), L(I4 ⇒I5) calcu-
lates to
L(I4 ⇒I5) =
(
6
12
)
(
8
12
) (
6
12
) = 1.5 ,
a good level of improvement. However,
L(I2, I3 ⇒I9) =
(
3
12
)
(
4
12
) (
4
12
) = 2.25
is even higher: a strong indicator for this latter rule’s potential associative value.
11.2.3
The Apriori algorithm
Given these various measures of quality, an exercise in association rule learning would, in
theory, simply canvass the target database and identify all those rules whose support or accu-
racy exceed predetermined thresholds, or whose lifts exceed 1 (or some other improvement
threshold), and so on. In practice, however, the size of most transaction databases precludes
such a strategy. Databases on the order of n = 104 are considered small in some circles, and
transaction ‘warehouses’ with n exceeding 108 are not uncommon, representing terabytes of

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
399
data. Coupling this with item lists containing thousands or even tens of thousands of candi-
date entries produces transaction matrices that are very large and very sparse. The necessary
storage and computational requirements quickly move out of range for many practical users.
Some simplification may be available if the various items collect naturally into taxonomies,
for example, milk, cheese, and ice cream could all be grouped into ‘dairy’ and vegetables and
fruits into ‘produce’ (Tufféry 2011, Section 10.2). Of course, this also increases granularity
in the item structure, and some associations can become blurred. Item taxonomies are best
applied when domain-specific requirements motivate their use.
To compensate for sparseness in the database, itemsets are restricted to meet minimum
support thresholds, usually at least so ≥1%, although this can vary depending on the analytic
objectives. (At times, the threshold is given in terms of minimum total number of itemsets,
i.e., a threshold of nso.) The results are often called frequent itemsets and represent a focused
subset for closer examination. Of course, this can eliminate interesting associations among
uncommon items, particularly with low-occurrence consequents. For example, Hastie et al.
(2009, Section 14.2.2) suggested, perhaps partly with tongue-in-cheek, how easily one might
overlook an intriguing if infrequent association such as {vodka ⇒caviar} among premium
marketers.
Additional minimum thresholds on the accuracy – usually at least co ≥50% – are next
imposed, often along with the natural requirement that lift exceed 1. Many analysts also restrict
k to keep the eventual associations from becoming too unwieldy. Upper bounds of k ≤4 or
k ≤5 are not unheard-of.
Restriction to frequent itemsets can still produce massive data sets, however, and machine
learning analysts have developed clever algorithms for managing transaction databases and
searching for association rules. One of the earliest is the Apriori algorithm of Agrawal and
Srikant (1994). Along with the usual minimum threshold requirements, Apriori takes advan-
tage of a key feature of itemset architecture. It recognizes that addition of an additional item to
an existing itemset cannot increase the number of occurrences in the database of the new larger
itemset. That is, by adding a new item Ij ∉k to k and enlarging the itemset to k+1, we
always find S(k+1) ≤S(k). Apriori employs this downward-closure property to improve
execution time when searching frequent itemsets, starting with k = 1 and working upwards
from there (Hand et al. 2001, Section 13.3.2). Its combination of simplicity and effectiveness
makes Apriori a very popular rule mining algorithm. In R, Apriori is implemented via the
apriori() function of the external arules package.
Example 11.2.1 Bank depositor associations. In the bank marketing study of a European
bank’s depositors previously discussed in Example 9.2.1, a large database was constructed
on the depositors’ backgrounds – education, loan status, and so on – and banking activities
(Moro et al. 2011). The large collection of data here allows for unsupervised learning into pos-
sible associations among the depositors’ banking characteristics. After removing depositors
with missing/unknown observations, the database used here comprises a total of n = 43 193
depositor records (with any personal, identifying information expunged).
From this, an individual depositor’s observation is derived as a set of binary indicators
on a series of pertinent outcome variables. For an association rule analysis, each depositor
is viewed as a ‘transaction,’ and their binary indicators for each outcome are the individual
‘items.’ Table 11.6 lists the items/variables, and Table 11.7 gives the corresponding transaction
matrix X. (As previously, only a selection of observations is provided in the data table. The
complete set of data is available at http://www.wiley.com/go/piegorsch/data_analytics.)

400
STATISTICAL DATA ANALYTICS
Table 11.6
Binary outcome variables (‘items’) for association rule analysis of depositor
characteristics, from n = 43 193 customers of a large European bank.
Items
Binary feature(s)
I1 = Adult
0 = Below 21 years
1 = At/above 21 years
I2 = Working
0 = Not working/retired
1 = Working
I3 = Marital
0 = Single/divorced
1 = Married
I4 = Post-secondary Education
0 = No college
1 = Some college/degree
I5 = Credit default
0 = Yes/previous
1 = None
I6 = Positive average balance
0 = No
1 = Yes
I7 = Active home loan
0 = No
1 = Yes
I8 = Active personal loan
0 = No
1 = Yes
Source: Moro et al. (2011).
Table 11.7
Selected elements of binary transaction matrix X for association rule analysis
of depositor characteristics, from n = 43 193 customers of a large European bank.
Customer code
Items
I1
I2
I3
I4
I5
I6
I7
I8
T1
1
1
0
0
0
0
0
1
T2
1
1
1
1
0
0
0
1
⋮
⋮
⋮
⋮
⋮
⋮
⋮
⋮
⋮
T43192
1
1
1
1
1
1
0
0
T43193
1
1
0
1
1
1
0
0
Source: Moro et al. (2011).
Items are defined in Table 11.6.
To conduct an association rule analysis via the Apriori algorithm, the X matrix in
Table 11.7 can be entered into R and manipulated to accommodate operations by the external
arules package and its apriori() function. The following code gives sample commands,
after having imported X as the dataframe bank.df:
> require( arules )
> bank.tr <- as( as.matrix(bank.df), ‘transactions’ )
> bank.ru <- apriori( bank.tr, parameter =
list(supp=0.5, conf=0.8,
minlen=2, maxlen=6, target=‘rules’) )
> inspect( bank.ru )
These commands perform the following operations:
1. load the arules package;
2. apply the as() function with the ‘transactions’ option to coerce bank.df into a
‘transactions’ object bank.tr for use with apriori();
3. apply apriori() to the transactions object bank.tr and recover the association rules;
and
4. inspect the resulting bank.ru ‘rules’ object to study the association rules.

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
401
Notice in the call to apriori() that various options are supplied as a list; these specify
minimum levels for the uncovered rules:
• supp=0.5 sets the minimum support level to 50% (the default is 10%)
• conf=0.8 sets the minimum accuracy (‘confidence’) to 80% (the default is also 80%)
• minlen=2 sets the minimum length of a rule, that is, the number of items (antecedents
+ consequents), to 2 (the default is minlen=1, which can produce rules with empty
antecedents)
• maxlen=6 sets the maximum length of a rule to 6, which in effect forces k ≤5 because
apriori() only allows for singleton consequents
We find that bank.ru gives a set of 55 rules, as detailed by the inspect() command.
The output (not shown) also gives values of support, accuracy (called ‘confidence’ by the
program), and lift. These quality measures can alternatively be accessed as three columns,
respectively, from the arules quality() function. As expected, all supports are at least
50% and all accuracies are at least 80%. In fact, one of the accuracies is exactly 100%:
A(Marital, Positive Avg. Balance ⇒Adult) = 1.000. (This is known as a ‘trivial’ rule: it is not
very surprising that every depositor in this database who was both married and carried a posi-
tive average balance was always over 21 years of age.) The lifts for these 55 rules vary between
min(quality(bank.ru)[3]) = 0.994 and max(quality(bank.ru)[3]) = 1.051. For
instance, L(Home Loan ⇒Adult) is 1.001, barely above 1 (another trivial rule).
We can restrict attention only to rules with lifts above 1 by subsetting the rule object:
> bankLift1.ru <- bank.ru[quality(bank.ru)[3] > 1]
> inspect( bankLift1.ru )
This produces 39 rules. Many remain trivial, although a few interesting results – called
‘actionable’ rules – appear. These are generally related to I5 = Credit Default. (Recall that
I5 indicates if a depositor has no previous default on record.) The following output is edited
from inspect(bankLift1.ru):
index
lhs
rhs
support
conf.
lift
5
{Marital}
=> {CrDefault}
0.59074
0.98343
1.0016
7
{PosBalanc} => {CrDefault}
0.83213
0.99307
1.0114
23
{Adult,
Marital}
=> {CrDefault}
0.59072
0.98343
1.0016
24
{Working,
PosBalanc} => {CrDefault}
0.74714
0.99289
1.0112
25
{Working,
CrDefault} => {PosBalanc}
0.74714
0.84314
1.0062
27
{Adult,
PosBalanc} => {CrDefault}
0.83074
0.99305
1.0114
28
{Adult,
CrDefault} => {PosBalanc}
0.83074
0.84737
1.0113
38
{Adult, Working,
PosBalanc} => {CrDefault}
0.74702
0.99289
1.0112
39
{Adult, Working,
CrDefault} => {PosBalanc}
0.74702
0.84316
1.0062

402
STATISTICAL DATA ANALYTICS
The general message is that working, married adults appear likely to carry a positive average
balance and/or avoid defaulting on credit. (Carrying a home loan also enters into the mix.)
Perhaps more surprisingly, no other depositor characteristics – education, personal loan, and
so on – appear to affect these rule associations as strongly with these data.
We find that this database identifies strong associations between common characteristics
of job and personal stability – married, working, and so on – when considering depositors for
their credit-worthiness and/or likelihood of carrying positive balances.
Some might argue that such associations border on ‘trivial,’ because experts in the banking
industry surely have recognized this already. It could have been just this sort of analysis,
however, that led to those discoveries.
◽
Evolution of alternative market basket algorithms, along with enhancements to the basic
Apriori concept, has produced a variety of frequent itemset search procedures (Motoda and
Ohara 2009; Nath et al. 2013). One approach simply samples from the frequent itemsets, using
methods of statistical random sampling (Section 3.1). If conducted properly, the sample will
represent the associative information in the larger transaction database and can reduce the
computational burden (Hand et al. 2001, Section 13.3). Indeed, algorithm design for associa-
tion rule learning is an active research area, with many avenues open for further advancement
(Borgelt 2012).
11.2.4
Statistical measures of association quality
As a predictive measure of association rule quality, the lift in (11.7) represents a more objective
quantification than the simpler concepts of support and accuracy. Taking this a step further,
one can view a rule’s {antecedent ⇒consequent} association structure in a cross-classified
manner, similar to the confusion matrices for classification rules in Section 9.1.2. This allows
for formal statistical measures to be applied.
Frame the construction as follows: given an antecedent itemset k and a consequent item-
set 1, calculate how many transactions in X satisfy both itemsets – that is, those transactions
where all items in both k and 1 are purchased. Denote this by Y11. Similarly, calculate how
many transactions satisfy neither itemset; denote this by Y00. Lastly, calculate
Y10 = the number of transactions where k is satisfied but 1 is not,
and
Y01 = the number of transactions where 1 is satisfied but k is not.
(The total should equal n, because these four cases fully partition X.) Collect the four counts
together into a cross-classified 2 × 2 contingency table, as in Section 8.3.3. Table 11.8 gives
a schematic representation.
We can quantify the association in this 2 × 2 table via the Pearson X2 statistic from (8.14).
Here, we have
X2 =
1
∑
i=0
1
∑
j=0
(Yij −̂Yoij)2
̂Yoij
,
(11.8)
where
̂Yoij =
(∑1
j=0 Yij
) (∑1
i=0 Yij
)
∑1
i=0
∑1
j=0 Yij
=
Yi+Y+j
Y++

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
403
Table 11.8
A 2 × 2 contingency table for
cross-classification of association rule outcomes
Consequent
1 fail
1 met
k fail
Y00
Y01
Antecedent
k met
Y10
Y11
is the expected (i, j)th cell count when no association exists between the antecedent and con-
sequent conditions (cf. Section 8.3.3). We use the ability of the X2 statistic to quantify high
association in this 2 × 2 contingency table. As the observed pattern in Table 11.8 deviates
from the no-association state, the statistic in (11.8) grows large. (Note, however, that there
is no formal hypothesis test being performed here, so that appeal to significance levels and
false-positive errors is irrelevant.)
A caveat: the X2 statistic in (11.8) is unaffected by column transposition. If we were to
permute the columns in Table 11.8, X2 remains the same. (Try it!) As applied to associa-
tion rules, this translates to a 𝜒2-equivalence between the rule {k ⇒1} and {k ⇒c
1},
where c
1 is the complement of 1 (‘not’ 1). In most settings, however, the rules {k ⇒1}
and {k ⇒c
1} will represent different associations, so this 𝜒2-equivalence will be spurious.
The X2 measure still has value in detecting strong associations, but analysts should not rely
on it exclusively. At a minimum, always also calculate an additional measure, such as the lift.
(Although, some transaction rules with equal X2 values can also have equal lifts.) In fact, any
other measure designed to detect association in 2 × 2 tables could also be calculated to similar
effect, such as the (two-sided) hypergeometric P-value from the Fisher exact test in Section
8.3.3. In R, the external arules package can calculate X2 for any rule via its interestMea-
sure() function as of course can many other R functions, such as chisq.test().
Example 11.2.2 Bank depositor associations (Example 11.2.1, continued). In the bank
depositor data from Table 11.7, we can calculate the X2 statistic for each of the highlighted
rules with lifts greater than 1. Applied to the same database and R objects from Example
11.2.1, the commands
> require( arules )
> indeces <- c(5, 7, 23, 24, 25, 27, 28, 38, 39)
> interestMeasure( bankLift1.ru[indeces],
method=‘chiSquare’, transactions=bank.tr )
produce the X2 values summarized in Table 11.9.
As might be expected, all of the X2 values in Table 11.9 are large. The smaller values for
the rules
{Marital ⇒(no) Credit Default},
{Adult, Marital ⇒(no) Credit Default},
{Working, (no) Credit Default ⇒Positive Avg. Balance}, and
{Adult, Working, (no) Credit Default ⇒Positive Avg. Balance}

404
STATISTICAL DATA ANALYTICS
Table 11.9
X2 statistics from (11.8) for selected association rules in Example 11.2.2 with
bank depositor data from Table 11.7.
Index
Rule: {Antecedent ⇒Consequent}
X2
5
{Marital ⇒(no) Cred. Default}
8.578
7
{Pos. Balance ⇒(no) Cred. Default}
1567.356
23
{Adult, Marital ⇒(no) Cred. Default}
8.570
24
{Working, Pos. Balance ⇒(no) Cred. Default}
893.382
25
{Working, (no) Cred. Default ⇒Pos. Balance}
66.896
27
{Adult, Pos. Balance ⇒(no) Cred. Default}
1548.262
28
{Adult, (no) Cred. Default ⇒Pos. Balance}
1414.184
38
{Adult, Working, Pos. Balance ⇒(no) Cred. Default}
892.649
39
{Adult, Working, (no) Cred. Default ⇒Pos. Balance}
67.396
suggest lesser strength of association. For the others, added potential may lie in exploring their
particular associations further.
◽
Owing in part to their ease of use, association rules are extremely popular in machine
learning and knowledge discovery. Consequently, a large literature has developed; see
Höppner (2010); Zhang and Wu (2011). For particular emphasis on business and commerce,
Linoff and Berry (2011, Chapter 15) provided an extensive introduction.
Exercises
11.1
Return to the automobile characteristics data in Example 11.1.2 and now apply (a)
single linkage and (b) complete linkage to construct the dendrogram. (Continue to
employ a Euclidean distance metric.) Does either clustering pattern change substan-
tively for this simple data set?
11.2
What happens if you change to Manhattan distance in Exercise 11.1?
11.3
The basic structure of a cluster analysis may be transposed, so that clustering among
the variables is explored instead of clustering among the observations. The data
remain the same, although now the target goal is to find clusters among the p columns
of Z.
(a) Write these column vectors as zj for j = 1, … , p and consider two different
column vectors zj and zm. How would the technical details change for defining
a dissimilarity metric d(zj, zm) in this case? What changes would be necessary
to apply a corresponding hierarchical agglomerative cluster analysis with, say,
average-link clustering?
(b) How would you apply this reformulated method in practice using, for example,
hclust() and/or plot() in R?

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
405
11.4
From a bioinformatic
study of gene expression profiles in carcinogenesis,
Golub et al. (1999) analyzed expression data for 7129 human genes poten-
tially associated with hematopoietic cancers. The profiles were taken from
a group of 47 patients with acute lymphoblastic leukemia. A selection of
the reported expression levels follows. (Download the complete data set at
http://www.wiley.com/go/piegorsch/data_analytics.)
Patient code no.
Gene descr./code
1001
1032
1113
1124
· · ·
6371
6672
AF1q
399
252
1588
27
· · ·
79
267
apoargC
−290
−274
−337
−256
· · ·
−116
−182
ard-1
426
371
420
587
· · ·
244
569
⋮
⋮
⋮
⋮
⋮
⋱
⋮
⋮
Zyxin
298
307
309
693
· · ·
509
417
(a) Conduct an agglomerative hierarchical cluster analysis on these gene expression
data, with genes viewed as the n observations and patients viewed as the p vari-
ables. (Remember to standardize the data across genes within each jth patient
before proceeding with the calculations.) Use Euclidean distance for the under-
lying distance metric. Separately apply each of (i) single linkage, (ii) complete
linkage, and (iii) average linkage for the linkage metric, and comment on any
differences you see in the resulting dendrograms.
(b) Transpose the perspective from Exercise 11.4a and now view patients as the n
observations and genes as the p variables, as per Exercise 11.3. (Notice that you
will have to return to the original expression values and now standardize across
patients within each gene.) Continue to employ Euclidean distance, but only con-
struct an average linkage dendrogram.
(c) Use the results from Exercises 11.4a and 11.4b to build a heatmap (cf. Figure
10.5) of the gene expression outcomes based on the hierarchical clusterings. That
is, reorder the original data matrix X by setting an observation’s row index equal
to the ordered location given by clustering genes hierarchically, and its column
index is equal to the ordered location given by clustering patients hierarchically.
(The heatmap() function in R may be useful, particularly with its Rowv= and
Colv= options activated. See help(heatmap).) A popular color scheme with
gene expression data is to set negative (underexpressed) observations to green
and positive (overexpressed) observations to bright red. If you have such capabil-
ities, try it; however, be aware that this particular scheme may cause problems for
viewers with dichromatism.
11.5
Return to the gene expression data in Example 11.1.3 and examine the hierarchical
clustering solutions from each of the three linkage strategies in greater detail. In partic-
ular, compare the ‘early’ clustering patterns at small values of 𝛿for the single linkage

406
STATISTICAL DATA ANALYTICS
and complete linkage solutions in Figure 11.5 with that identified in Figure 11.4. For
clusters that develop quickly, is there large overlap in the cluster memberships? (If so,
this would indicate possible subgroups of patients with the tumor.)
11.6
In a study of calendar year 2012 business activities for hotel management compa-
nies (Anonymous 2013), data were collected for n = 147 firms on the number of
guest rooms each had under management, along with the firm’s total 2012 revenue
(in $M). This gave p = 2 outcome variables for study. The data are available online
at http://www.wiley.com/go/piegorsch/data_analytics; a sample is given as follows:
Company
Guest rooms
Revenue ($M)
Interstate Hotels & Resorts
61 205
2600.00
Aimbridge Hospitality
25 908
700.13
GF Management
21 971
520.00
White Lodging Services
20 232
746.64
John Q Hammons
19 000
50.00
⋮
⋮
⋮
Chartwell Hospitality
107
1.75
Allen & O’Hara
96
2.45
Both outcome variables exhibit a substantial right skew (plot histograms or boxplots
to verify this), so perform a logarithmic transformation on each and operate with
x1 = log(Guest rooms) and x2 = log(Revenue). Study potential clustering with these
log-transformed data as follows:
(a) Plot x2 versus x1. Does visual inspection suggest any apparent clustering?
(b) Standardize the data and construct a scatterplot of the resulting z-scores. Do any
substantial changes appear from the plot in Exercise 11.6a?
(c) Apply K-means partitioning with Euclidean distance to identify potential clusters
in the standardized data. Use an elbow plot of the cumulative within-cluster sum of
squares, WK, to select a value for K. Also plot log{WK} and the pseudo-F statistic
against K to verify if your choice appears warranted.
(d) Return to the scatterplot of the standardized data in Exercise 11.6a and modify
the plot to mark the K-means cluster solution from Exercise 11.6c. (Use different
colors, or different plot symbols, etc. for each observation’s cluster membership.)
Overlay a Vonoroi tessellation to clearly segment the clusters.
11.7
In a study of diagnostic factors for human cancer, Wolberg and Mangasarian (1990)
reported on p = 9 different prognostic variables for a sample of breast cancer
patients in the US state of Wisconsin. The data, corresponding to a version of this
database discussed in Sugar and James (2003), comprise n = 683 observations.
A selection of the data is given as follows. (Download the complete data set at
http://www.wiley.com/go/piegorsch/data_analytics.)

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
407
Patient code no.
Outcome variable
1000025
1002945
1015425
· · ·
888820
897471
x1 = Clump thickness
5
5
3
· · ·
5
4
x2 = Cell size uniformity
1
4
1
· · ·
10
8
x3 = Cell shape uniformity
1
4
1
· · ·
10
8
x4 = Marginal adhesion
1
5
1
· · ·
3
5
x5 = Epithelial cell size
2
7
2
· · ·
7
4
x6 = Bare nuclei
1
10
2
· · ·
3
5
x7 = Bland chromatin
3
3
3
· · ·
8
10
x8 = Normal nucleoli
1
2
1
· · ·
10
4
x9 = Mitoses
1
1
1
· · ·
2
1
(a) Standardize the nine outcome variables into z-scores. Apply K-means partitioning
with Euclidean distance to identify potential clusters in the data. Use an elbow plot
of the cumulative within-cluster sum of squares, WK, to select a value for K. Also
plot the pseudo-F statistic against K to verify if your choice seems warranted.
Does anything unusual appear with the pseudo-F plot?
(b) An additional variable recorded for each patient was whether their tumor was
benign or malignant. This variable (labeled as x10 = ‘Class’) is also included in
the downloadable data set: x10 = 2 for benign tumors (for 444 of the patients)
and x10 = 4 for malignant tumors (for 239 of the patients). Recode these as ‘B’
and ‘M’, respectively. Determine how many of each tumor type are identified in
each of the K clusters from the K-means clustering solution you found in Exercise
11.7a. Present the counts in a tabular cross-classification, with rows as clusters and
columns as tumor type. Comment on the results.
11.8
Return to the home center data in Example 11.1.5. The skew in the original data sug-
gests the use of a (natural) logarithmic transform to help to improve analytic traction.
So, apply a log transform to the x1 = Sales, x3 = Units, and x4 = Staff variables.
(Leave the x2 = percentage of change variable unaltered.)
(a) Reconstruct the labeled scatterplot matrix (Figure 11.8), using the log-Sales,
log-Units, and log-Staff variables (and with the original % change variable). Use
the cluster labels from the analysis in Example 11.1.5. What patterns appear in
the new scatterplots?
(b) Using the log-Sales, log-Units, and log-Staff variables (and with the original %
change variable), conduct a new cluster analysis. (Remember to standardize the
log-transformed data first.) Apply K-means partitioning with Euclidean distance.
Use an elbow plot of the cumulative within-cluster sum of squares, WK, to select
a value for K. Also plot the pseudo-F statistic against K to verify if your choice
seems warranted. With your given choice of K, construct another labeled scatter-
plot matrix to examine the clustering solution. How does this compare to the plot
in Exercise 11.8a?

408
STATISTICAL DATA ANALYTICS
11.9
Association rule learning is often applied in biomedical applications. To illustrate, the
following data from n = 270 patients on factors affecting heart disease come from the
famous ‘Statlog’ project in machine learning (Michie et al. 1994, Section 9.4.2). For
purposes of assessing possible associations in the data, consider the following p = 10
dichotomized outcome variables:
Items
Binary feature(s)
I1 = Age
0 = Less than 60 years
1 = At/over 60 years
I2 = Sex
0 = Female
1 = Male
I3 = Blood pressure (systolic)
0 = Less than 140 mmHg
1 = At/over 140 mmHg
I4 = Cholesterol
0 = Less than 240 mg/dL
1 = At/over 240 mg/dL
I5 = Blood sugar
0 = Less than 120 mg/dL
1 = At/over 120 mg/dL
I6 = Maximum heart rate
0 = Less than 150 bpm
1 = At/over 150 bpm
I7 = Induced angina
0 = No
1 = Yes
I8 = Fluoroscopy indicator
0 = None
1 = Some
I9 = Thallium scan
0 = Normal
1 = Some defect
I10 = Disease state
0 = Absent
1 = Present
A selection of the corresponding binary transaction matrix X is given as follows (the
full data are available online at http://www.wiley.com/go/piegorsch/data_analytics):
Items
Patient code
I1
I2
I3
I4
I5
I6
I7
I8
I9
I10
T1
1
1
0
0
0
0
0
0
0
1
T2
1
0
0
0
1
0
0
0
0
0
T3
1
0
1
0
0
0
0
0
0
0
⋮
⋮
⋮
⋮
⋮
⋮
⋮
⋮
⋮
⋮
⋮
T269
0
1
1
1
0
0
1
1
1
1
T270
1
0
1
1
0
1
0
1
1
1
(a) Apply the Apriori algorithm to this transaction database and identify any possible
association rules. Set your minimum support to 2% and your minimum accuracy
(‘confidence’) to 50%, and only consider itemset antecedents of size k = 5 or less.
What association rules does this uncover? How do their lifts compare?
(b) Also calculate the X2 statistic from (11.8) for each of these rules. Does this provide
additional distinguishability among the rules?
(c) Experiment with the algorithm settings (minimum support, minimum accuracy,
etc.) in Exercise 11.9a. What patterns emerge?

TECHNIQUES FOR UNSUPERVISED LEARNING: CLUSTERING AND ASSOCIATION
409
11.10
Return to the bank depositor data in Example 11.2.2, and verify the individual calcu-
lations for the X2 statistic with the following rules from Table 11.9. In particular, find
the underlying 2 × 2 table and perform the X2 calculations directly without appeal to
the arules package.
(a) {Marital ⇒(no) Cred. Default}, X2 = 8.578.
(b) {Adult, Marital ⇒(no) Cred. Default}, X2 = 8.570.
(c) {Working, (no) Cred. Default ⇒Pos. Balance}, X2 = 66.896.
11.11
Brijs et al. (1999) discussed a market basket analysis for a Belgian retail supermarket
involving n = 88 162 customer transactions. These data give each transaction as a row
that lists which of the store’s products, coded by stock-keeping units (SKUs), were
purchased. As is typical, the total number of SKUs for this supermarket is large: p =
16 470. The data have been sanitized: the SKUs are coded to between 0 and 16 469,
and all customer information is masked. A selection of the data is given as follows
(the complete set of data is available online at http://fimi.ua.ac.be/data/retail.dat):
Transaction number
Item SKUs purchased
1
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
2
30
31
32
3
33
34
35
⋮
⋮
88160
2310
4267
88161
39
48
2528
88162
32
39
205
242
1393
(a) Apply the Apriori algorithm to identify any possible association rules in this
transaction database. (If using R and the apriori() function, the data’s input
format will require manipulation. Try constructing it as a list via the com-
mand strsplit(readLines(‘retail.csv’),‘,’), where retail.csv is
the comma-separated data file.) Set your minimum support to 10% and your min-
imum accuracy (‘confidence’) to 50%, and only consider itemset antecedents of
size k = 5 or less. What association rules does this uncover? How do their lifts
compare?
(b) Also calculate the X2 statistic from (11.8) for each of these rules. Does this provide
additional distinguishability among the rules?

410
STATISTICAL DATA ANALYTICS
(c) Experiment with the algorithm settings (minimum support, minimum accuracy,
etc.) in Exercise 11.11a. Especially for this large and sparse transaction matrix,
what patterns emerge?
11.12
Return to the heart disease data in Exercise 11.9b, and verify the individual calcu-
lations for the X2 statistic with the rule whose lift is a maximum among all those
generated. In particular, find the underlying 2 × 2 table and perform the X2 calcula-
tions without appeal to the arules package.
11.13
Prove algebraically the contention in Section 11.2.4 that if the rows of the 2 × 2
contingency table in Table 11.8 are transposed, the X2 statistic in (11.8) is unaffected.

Appendix A
Matrix manipulation
Matrix and vector operations are a necessary component of many data analytic calculations.
This brief appendix reviews the terminology, notation, and selected algebraic aspects most
useful in this regard. All quantities are assumed to be real valued. For a complete introduction,
including background on many of the results stated in the following, see complete textbook
sources such as Gentle (2007) and Schay (2012).
A.1
Vectors and matrices
A vector or a matrix is a collection of numbers assembled together into a structured, rectan-
gular (including square) array. By contrast, a scalar is a unidimensional number, denoted by
a simple Latin or Greek letter, such as b or 𝛽. A vector is a single row or column of n scalars,
denoted by a lowercase, boldfaced symbol b. In special cases, such as vectors of random
variables, uppercase notation X is used.
The scalar elements of a vector are assigned subscripts to identify their location in the
array. A row vector arranges these subscripted elements in a row: [b1 b2 · · · bn]. A column
vector arranges them in a column:
⎡
⎢
⎢
⎢
⎢⎣
b1
b2
⋮
bn
⎤
⎥
⎥
⎥
⎥⎦
.
(A.1)
The transposition operator, denoted by superscript T at the end of the vector, transposes a row
vector into a column vector, and vice versa. Thus the transposed row vector [b1 b2 · · · bn]T
is equivalent to the column vector in (A.1).
A matrix is an n × p rectangular collection of scalars into n rows and p columns, denoted
with an uppercase Roman or Greek letter, B or Θ, and indexed first by the rows and then by
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

412
STATISTICAL DATA ANALYTICS
the columns:
B =
⎡
⎢
⎢
⎢
⎢⎣
b11
b12
· · ·
b1p
b21
b22
· · ·
b2p
⋮
⋮
⋱
⋮
bn1
bn2
· · ·
bnp
⎤
⎥
⎥
⎥
⎥⎦
.
For shorthand notation, we use B = {bij}. Now, transposition takes an n × p matrix into a p × n
matrix: if B has elements {bij}, the transpose BT has elements {bji}. Repeated transposition
reproduces the original matrix: (BT)T = B.
It is often convenient to write an n × p matrix as a collection of its columns viewed as
vectors, for example, B = [b1 b2 · · · bp], where bj = [b1j b2j · · · bnj]T is the jth column.
(Alternatively, one can also view B in terms of its row vectors.) If a column (or row) of a
matrix is a linear combination of any other columns (or rows) – for example, bj = 𝑤qbq +
𝑤rbr for j ≠q, r and any scalars 𝑤q and 𝑤r – the column (row) is linearly dependent. If not,
it is linearly independent. The number of linearly independent columns of a matrix B is called
its rank, denoted as rank(B). One can equivalently define rank in terms of the number of
linearly independent rows, because the two quantities are in fact equal (Gentle 2007, Section
3.3). Indeed, rank(B) = rank(BT). If rank(B) = min{n, p}, B is said to have full rank.
If n = p the matrix is square of order n. The elements bii then constitute the diagonal of
the matrix. The trace of an n × n square matrix is the sum of its diagonal elements:
tr(B) =
n
∑
i=1
bii.
Elements above the diagonal are the upper diagonal elements; those below are the lower
diagonal elements.
If all the lower (upper) diagonal elements of a square matrix are identically zero, the matrix
is called upper (lower) triangular. More generally, for any rectangular (n × p) matrix M, if
all elements below the leading diagonal m11, m22, … , mqq are zero (for q = min{n, p}), the
matrix is called (upper) trapezoidal.
If the elements of a square matrix satisfy bij = bji (i ≠j), the matrix is symmetric (and
then B = BT). A symmetric matrix with bij = bji = 0 for all i ≠j is called a diagonal matrix,
denoted as diag{b11, b22, … , bnn} or simply diag{bii} if the context is clear.
The special n × n diagonal matrix I = diag{1, 1, … , 1} is the identity matrix. The notation
includes the order as a subscript, In, if greater specificity is required. Viewed as a matrix of col-
umn vectors, In is written as In = [e1 e2 · · · en], where ej is the coordinate unit vector made
up entirely of zeros except for a 1 in its jth element: e1 = [1 0 0 · · · 0]T, e2 = [0 1 0 · · · 0]T,
and so on. Similar in nature is the column vector made up entirely of ones, denoted herein as
h = [1 1 1 · · · 1]T.
A.2
Matrix algebra
Vector and matrix addition is conducted elementwise, as is scalar multiplication. So, for
example,
b + 𝑤d =
⎡
⎢
⎢
⎢⎣
b1 + 𝑤d1
b2 + 𝑤d2
⋮
bn + 𝑤dn
⎤
⎥
⎥
⎥⎦
,

MATRIX MANIPULATION
413
where 𝑤is any scalar. The operations are commutative, so that b + 𝑤d = 𝑤d + b = b + d𝑤.
Notice that the two vectors or matrices being added must have the same dimensions. Subtrac-
tion is similar.
Matrix/vector multiplication is more complex. Often called ‘Cayley multiplication’ after
its development by nineteenth century British mathematician Arthur Cayley (see Cayley
1858), the standard definition multiplies elementwise across a column and then down a
row, adding the resulting scalar product(s). The operation depends on the row and column
dimensions of the product’s factors and on the order in which they appear. Thus, multiplying
a 1 × n row vector bT by an n × 1 column vector d produces a scalar:
bTd =
n
∑
i=1
bidi.
(This is also called a dot product, with notation b ⋅d, or also a Euclidean inner product.) The
number of elements in each of the two vectors must match. Conversely, multiplying an n × 1
column vector by a 1 × p row vector is possible, producing an n × p matrix:
⎡
⎢
⎢
⎢⎣
b1
b2
⋮
bn
⎤
⎥
⎥
⎥⎦
[d1 d2 · · · dp]
=
⎡
⎢
⎢
⎢⎣
b1d1
b1d2
· · ·
b1dp
b2d1
b2d2
· · ·
b2dp
⋮
⋮
· · ·
⋮
bnd1
bnd2
· · ·
bndp
⎤
⎥
⎥
⎥⎦
.
Clearly, vector multiplication is not commutative.
Matrix multiplication is an extension of this row × column operation: the ‘product’ of
an n × p matrix B and a p × m matrix D has (i, j)th element ∑p
h=1 bihdhj for i = 1, … , n and
j = 1, … , m. The resulting matrix product, BD, is an n × m matrix. For the product to exist, the
number of columns in B must equal the number of rows in D. If so, B and D are conformable
(for multiplication). Here again, matrix multiplication is not commutative: BD need not equal
DB. In fact, one or both may not exist, depending on whether or not the individual matrices
are conformable. If they are, then (BD)T = DTBT. For two conformable matrices Bn×p and
Dp×n, tr(BD) = tr(DB).
Because order makes a difference in matrix multiplication, special terminology is required.
For instance, if B and D are conformable to produce the product BD, we say D has been
premultiplied by B. Similarly, in DB, D is postmultiplied by B. In ether case, multiplication
by the (appropriate) identity matrix always returns the original n × p matrix: InB = B and
BIp = B.
In passing, note that other alternative definitions exist for the concept of a matrix product
(see Gentle 2007, Section 3.2.9).
When two vectors b and d satisfy bTd = 0, they are said to be orthogonal. (This has a
geometric interpretation: orthogonal vectors are perpendicular and cross at right angles in
Euclidean space.) A set of nonzero vectors that are mutually orthogonal are also linearly
independent.
When two orthogonal vectors b and d are scaled to have unit length such that bTb = 1
and dTd = 1, they are called orthonormal. Notice that the coordinate unit vectors, ej from
Section A.1, are all orthonormal.
Applied to matrices, a square matrix, Q, is orthogonal if it satisfies QQT = QTQ = I. (The
definition of orthogonality can be extended to any n × p matrix, but this will not be required
herein.)

414
STATISTICAL DATA ANALYTICS
A.3
Matrix inversion
There is no simple definition for matrix division. For a square matrix, there is the concept
of a reciprocal called an inverse matrix. To define this, some preliminary constructions are
required.
Given a square n × n matrix B = {bij}, the determinant of the matrix is a summary scalar
useful for various multiplicative operations. The notation is |B| or sometimes det(B). Note
that the former has nothing to do with the absolute value operator.
The simplest way to define |B| is in terms of the conditions that it satisfies. Let B =
[b1 b2 · · · bp], where bj = [b1j b2j · · · bnj]T is B’s jth column. Define the determinant |B|
such that
(a) |I| = 1 for B = I,
(b) |b1 · · · vbj + 𝑤d · · · bp| = v|B| + 𝑤|b1 · · · d · · · bp| for scalars v and 𝑤and any
n×1 vector d,
(c) | · · · bj · · · bk · · · | = 0 if bj = bk, and
(d) |b1 · · · bj bj+1 · · · bp| = −|b1 · · · bj+1 bj · · · bp|
(Schay 2012, Section 6.1). Rules for calculating |B| from these defining conditions can then
be developed. In the simplest case, the determinant of a 2×2 matrix is
||||
b11
b12
b21
b22
||||
= b11b22 −b12b21.
For larger n, the determinant is best calculated via computer. For instance, in R, use the
det(X) function where X is an n × n array of class matrix.
The following are some special cases for an n × n matrix B:
• |𝑤B| = 𝑤n|B| for any scaler 𝑤.
• |BT| = |B|.
• If B is upper or lower triangular, |B| = ∏n
i=1 bii.
• Suppose B is partitioned into
B =
[
B11
0
0
B22
]
for square submatrices B11 and B22, where 0 is a matrix of zeros. Then |B| = |B11||B22|.
• For the n × n matrix D, |BD| = |B||D|.
• If Q is a square, orthogonal matrix, |Q| = ±1.
Note, however, that |A + B| ≠|A| + |B|.
The determinant can be used to identify the inverse of a matrix. A square matrix B whose
determinant |B| equals zero is called singular. If |B| ≠0, then B is nonsingular. Every square
nonsingular matrix B has an inverse matrix B−1 that satisfies
B−1B = I = BB−1.
(Singular matrices do not possess an inverse.) Nonsingular matrices must have full rank.

MATRIX MANIPULATION
415
As with the determinant, expressions for finding an inverse matrix B−1 can become com-
plex (see Schay 2012, Section 2.5). In the simple 2×2 case, the result is
[
b11
b12
b21
b22
]−1
=
1
b11b22 −b12b21
[
b22
−b12
−b21
b11
]
.
(A.2)
Notice that the denominator in (A.2) is simply |B|. For larger n, the inverse is often determined
via computer. For instance, in R, use the solve(X) function where X is an n × n array of class
matrix.
If B is nonsingular, then
• (BT)−1 = (B−1)T.
• To solve the (conformable) system of linear equations Bx = d, premultiply by B−1 to
find x = B−1d.
• If the conformable matrix D is also nonsingular, (BD)−1 = D−1B−1.
• If Q is (square and) orthogonal, Q−1 = QT.
• And obviously, I−1 = I.
A.4
Quadratic forms
Given an n × 1 vector d and a symmetric n × n matrix B, the scalar quantity
dTB d =
n
∑
i=1
n
∑
j=1
dibijdj
is called a quadratic form. When B = I, the quadratic form is just the sum of squares dTd =
∑n
i=1 d 2
i , sometimes called the inertia of the vector d.
If dTB d ≥0 for any real vector d, B is termed a nonnegative definite matrix (some authors
use ‘positive semidefinite’). If, further, dTB d > 0 for any real vector d ≠0, B is positive
definite. Alternatively, if dTB d < 0 for any real vector d ≠0, B is negative definite.
A special case occurs when B has the form XTX, where X is an m × n matrix. Then, the
quadratic form dTB d is
dTXT X d = (Xd)TXd.
Notice, however, that Xd is an m×1 vector, say a = Xd. Thus dTB d = aTa = ∑m
i=1 a2
i , which
is clearly nonnegative for any a (and, hence, for any d). Therefore, we conclude that XTX is
always nonnegative definite. If, further, ai > 0 for all i, XTX will be positive definite, with
inverse matrix (XTX)−1. (A symmetric, positive definite matrix is always nonsingular; see the
following sections.)
A.5
Eigenvalues and eigenvectors
Suppose B = [b1 b2 · · · bn] is a square n × n matrix with jth column bj. Let u ≠0 be any
nonzero n×1 vector. Notice that the product Bu is itself an n×1 vector. In fact, it is a linear

416
STATISTICAL DATA ANALYTICS
combination of the columns of B, Bu = ∑n
i=1 biui. Suppose further that there exists a scalar
𝜆≠0 relating the vector Bu to the vector u via
Bu = 𝜆u .
(A.3)
If (A.3) holds, we say 𝜆is an eigenvalue and u is an eigenvector of the square matrix B.
Other names for the eigenvalue are ‘characteristic value’ or ‘latent value.’ Notice that if u
satisfies (A.3), so will 𝑤u for any scalar 𝑤≠0. Thus eigenvectors are not unique. To establish
a standard, eigenvectors are often scaled to uTu = 1. These are then called unit eigenvectors.
It can be shown that under (A.3) with u ≠0, the matrix B −𝜆I must be singular. That is,
|B −𝜆I| = 0.
(A.4)
This is another equation – called the characteristic polynomial of B – which can be solved
for 𝜆. As such, some authors use (A.4) as the definition of an eigenvalue. As the equation
defines the roots of an nth order polynomial in 𝜆, from the Fundamental Theorem of Algebra
(Borwein and Erdélyi 1995, Section 1.1), there are at most n real, distinct eigenvalues for B.
(Note that some roots may be imaginary and/or repeated.)
Other results for an n × n matrix B with eigenvalue 𝜆and corresponding eigenvector u are
as follows:
• 𝑤𝜆is an eigenvalue of 𝑤B for any nonzero scalar 𝑤≠0.
• 𝜆2 is an eigenvalue of B2 with corresponding eigenvector u.
• If B is nonsingular, 1∕𝜆is an eigenvalue of B−1, with corresponding eigenvector u.
• If B is symmetric, all its eigenvalues are real.
• If B is symmetric with two distinct eigenvalues 𝜆1 and 𝜆2, then the eigenvectors, u1 and
u2, corresponding to these eigenvalues are orthogonal: uT
1u2 = uT
2u1 = 0.
• If B has eigenvalues 𝜆1 ≥𝜆2 ≥· · · ≥𝜆n, then tr(B) = ∑n
i=1 𝜆i and |B| = ∏n
i=1 𝜆i.
The latter result shows that if B possesses a zero-valued eigenvalue, it is singular. Further, a
symmetric, nonnegative definite matrix has only real, nonnegative eigenvalues, while a sym-
metric, positive-definite matrix has only real, positive eigenvalues (Gentle 2007, Section 3.8).
Notice then that a symmetric, positive-definite matrix has determinant |B| = ∏n
i=1 𝜆i > 0 and,
therefore, must be nonsingular.
Eigenanalysis is possible by hand, but the computer makes the effort less daunting. For
instance, in R, use the eigen(X) function where X is an n × n array of class matrix.
A.6
Matrix factorizations
Given a matrix with certain features – nonnegative definite, symmetric, and so on – a number
of factorizations and decompositions have evolved to characterize its structure. A few of these
useful for data analytics are presented in this section. For deeper expositions, see, for example,
(Gentle 2007, Ch. 5) or Skillicorn (2007).

MATRIX MANIPULATION
417
A.6.1
QR decomposition
A simple, yet very useful decomposition of an n × p matrix B is
B = QR,
where Q is a n × n orthogonal matrix and R is trapezoidal. If n = p so that B is square, R will
be upper triangular.
This QR decomposition has many uses; one often sees it employed when solving the sys-
tem of equations Bx = d. Computing is facilitated in R via the qr() function. For more on
the QR decomposition, see Gentle (2007, Section 5.7).
A.6.2
Spectral decomposition
Suppose that a square n × n matrix B has ordered eigenvalues 𝜆1 ≥𝜆2 ≥· · · ≥𝜆n and that
these are collected into a diagonal matrix 𝚲= diag{𝜆1, 𝜆2, · · · , 𝜆n}. Take U as the matrix
whose columns are the corresponding eigenvectors: U = [u1 u2 · · · un]. Then, from (A.3),
we can write BU = U𝚲. From this, if U is nonsingular, postmultiplying by U−1 achieves the
diagonal factorization of B:
B = U𝚲U−1.
(A.5)
It can be shown that tr(B) = tr(𝚲) and |B| = |𝚲|. Further, the number of nonzero eigenvalues
in 𝚲is equal to rank(B).
When B is symmetric, (A.5) will always apply. If the ujs are then assumed to be unit
eigenvectors, the matrix U will be orthogonal. That is, UTU = I, and we can write U−1 = UT.
Replacing this in (A.5) produces the spectral decomposition of a symmetric matrix B:
B = U𝚲UT.
(A.6)
Many authors automatically default to eigenvectors in unit form and thus, at least for a sym-
metric matrix, view (A.5) and (A.6) as reformulations of each other. Indeed, because the
eigenvalues/vectors play critical roles in both, one often sees the term ‘eigendecomposition’
used for either factorization. For computing purposes, in R, the various components of the
spectral decomposition are easily extracted from the eigen() function.
A.6.3
Matrix square root
Cases occur in data analytics where a matrix B
1
2 is required to satisfy the relationship
(B
1
2 )2 = B,
(A.7)
for some given matrix B. One is tempted to call B
1
2 the ‘square root’ of B. As matrix multipli-
cation is more complex than scalar multiplication, however, the notion of a matrix square root
requires careful development. For the discussion here, focus is on n × n symmetric matrices
where BT = B.

418
STATISTICAL DATA ANALYTICS
Start with a simple case. Suppose B is a diagonal matrix of the form B = diag{b1, b2, … ,
bn} with bi ≥0 for all i. Then clearly
B
1
2 = diag
{√
b1,
√
b2, … ,
√
bn
}
(A.8)
always exists and satisfies (A.7). Define (A.8) as the nonnegative definite square root of a
diagonal matrix.
Building from this, suppose the symmetric matrix B is nonnegative definite, with non-
negative, (decreasing) ordered eigenvalues in 𝚲= diag{𝜆1, … , 𝜆n} and with corresponding
matrix of (unit) eigenvectors U. From (A.6), B has spectral decomposition B = U𝚲UT. But
then, the matrix
B
1
2 = U𝚲
1
2 UT,
will satisfy (A.7), using the definition of a diagonal square root in (A.8) for 𝚲
1
2 . It can be
shown that B
1
2 is also symmetric and nonnegative definite.
If B is positive definite, then so is B
1
2 , with its own inverse matrix B−1
2 = U𝚲−1
2 UT. Here,
𝚲−1
2 = diag{𝜆−1∕2
1
, … , 𝜆−1∕2
n
}.
For more on square roots of matrices, see Gentle (2007, Section 3.8).
A.6.4
Singular value decomposition
A more general decomposition, of which the spectral decomposition in Section A.6SD can
be viewed as a special case, applies to any n × p matrix X. Known as the singular value
decomposition, or SVD, it gives a factorization of X in the form
X = QDVT,
(A.9)
where Q and V are n × n and p × p orthogonal matrices, respectively, and D is an n × p ‘diago-
nal’ matrix with entries d11 ≥d22 ≥· · · drr > 0 along its leading diagonal up to r = rank(X) ≤
min{n, p} and with all other entries as zero. The diis from D are known as the singular values
of X. For computing purposes, in R, the various components of the SVD are available via the
svd() function.
A useful application of the SVD occurs for the matrix product B = XTX. Notice here that
B is square (of order p) and symmetric. Then from (A.9),
B = (QDVT)TQDVT = VDTDVT,
appealing to the orthogonality of Q. Because B is symmetric, we can compare this with (A.6):
clearly, V = U is the (column) matrix of unit eigenvectors from XTX, while
DTD = diag{d 2
11, d 2
22, … , d 2
rr , 0, … , 0}
(A.10)
is the diagonal matrix of its eigenvalues. (If X has full column rank such that rank(XTX) = p,
then D will contain no zero entries on its diagonal. Recall that the number of nonzero eigen-
values is the rank of a symmetric matrix.) Clearly then, the eigenvalues are the squares of the
singular values in D: 𝜆i = d 2
ii . These are all nonnegative, reminding us that XTX is nonnega-
tive definite (positive definite if p = r).

MATRIX MANIPULATION
419
From DTD in (A.10), let
W = UDT = [u1d11 u2d22 · · · updpp]T = [u1
√
𝜆1
u2
√
𝜆2
· · · up
√
𝜆p ]T,
where ui is the ith (unit) eigenvector of XTX. One possible realization of the SVD for XTX
is then the product WWT, where W is the column matrix of eigenvectors multiplied (up to a
sign) by the square roots of their eigenvalues.
A.7
Statistics via matrix operations
Many summary statistics can be represented in terms of matrix operations. To see this, suppose
a univariate random sample from Section 3.1, Xi, i = 1, … , n, is collected into a column vec-
tor X = [X1 X2 · · · Xn]T. Recall that the column vector made up entirely of ones is denoted
by h = [1 1 · · · 1]T. Then, the sample mean is a scaled dot product of the two: X = 1
nh ⋅X =
1
nhTX. With this, the corrected (by the mean) vector of observations is
X∗= X −hTX
n h =
⎡
⎢
⎢
⎢
⎢⎣
X1 −X
X2 −X
⋮
Xn −X
⎤
⎥
⎥
⎥
⎥⎦
.
The corresponding sample variance is the sum of squares of these elements, divided by n −1:
S2 =
1
n −1(X∗)TX∗.
A p-variate sample, where each component is represented by the column vector Xj =
[X1j X2j · · · Xnj]T, j = 1, … , p, can be similarly collected into an n × p matrix
X = [X1 X2 · · · Xp] =
⎡
⎢
⎢
⎢⎣
X11
X12
· · ·
X1p
X21
X22
· · ·
X2p
⋮
⋮
⋱
⋮
Xn1
Xn2
· · ·
Xnp
⎤
⎥
⎥
⎥⎦
.
The corresponding matrix of mean-corrected observations is
X∗=
⎡
⎢
⎢
⎢
⎢
⎢
⎢⎣
X11 −X1
X12 −X2
· · ·
X1p −Xp
X21 −X1
X22 −X2
· · ·
X2p −Xp
⋮
⋮
⋱
⋮
Xn1 −X1
Xn2 −X2
· · ·
Xnp −Xp
⎤
⎥
⎥
⎥
⎥
⎥
⎥⎦
.
(A.11)
For summarizing variation, the sample analog to the population covariance matrix in (2.12)
is the sample covariance matrix. This locates the sample variances S2
j along the diagonal, and
the sample covariances from Section 3.3.3
Sjk =
1
n −1
n
∑
i=1
(Xij −Xj)(Xik −Xk)

420
STATISTICAL DATA ANALYTICS
(j ≠k) as the off-diagonal elements:
̂𝚺=
⎡
⎢
⎢
⎢⎣
S2
1
S12
· · ·
S1p
S21
S2
2
· · ·
S2p
⋮
⋮
⋱
⋮
Sp1
Sp2
· · ·
S2
p
⎤
⎥
⎥
⎥⎦
.
(A.12)
Note that because Sjk = Skj (j ≠k), ̂𝚺is symmetric. Indeed, let C = (n −1)−1
2 X∗. Then (A.12)
can be written compactly as ̂𝚺= CTC =
1
n−1(X∗)TX∗.
From (A.12), one can construct the corresponding sample correlation matrix:
R =
⎡
⎢
⎢
⎢⎣
1
r12
· · ·
r1p
r21
1
· · ·
r2p
⋮
⋮
⋱
⋮
rp1
rp2
· · ·
1
⎤
⎥
⎥
⎥⎦
,
where rjk = Sjk∕(SjSk) is the sample correlation coefficient between Xj and Xk, as in (3.9).
Again, because rjk = rkj (j ≠k), R is symmetric. If one collects the z-scores zij = (Xij −Xj)∕Sj
from (3.7) into the matrix Z = {zij}, then R = ZTZ∕(n −1). Note that some authors use
ZTZ∕n for the correlation matrix; clearly, this will make little difference when n is very large.
In R, cor(X) calculates a correlation matrix when the input argument X is a numeric vector,
matrix, or data frame. The cor() function uses n −1 in its denominator.

Appendix B
Brief introduction to R
R is an integrated suite of operators, functions, and user-contributed packages that conducts
statistical and mathematical calculations (R Core Team 2014). The program is an open
source computational environment developed by Ihaka and Gentleman (1996) and built upon
the S language. (For background on S, see Becker (1994).) The goal of this appendix is
to provide a brief overview of R and to establish guidelines for how it is used throughout
the rest of this textbook. It is not intended to supplant the many quality sources on R,
including introductions such as Dalgaard (2008) and Verzani (2005), the concise online
guide by Owen (http://cran.r-project.org/doc/contrib/Owen-TheRGuide.pdf), and of course
the main Comprehensive R Archive Network (CRAN) web site and its online manual at
http://cran.r-project.org/doc/manuals/R-intro.html. Readers should refer to sources such as
these for a deeper introduction to the language.
The program operates on Windows®, Apple OS, and Linux systems. At the time of this
writing, the current version number is 3.1.0, available for both 32-bit and 64-bit platforms.
It is that version, using 64-bit format, from which the bulk of the R material in this text was
prepared.
R’s open source architecture makes it highly popular and, of course, free. It is a highly
effective-yet-flexible form of modern statistical software, provides a powerful graphical sub-
system, and has an extensive user network that contributes subordinate external packages to
the program’s ongoing evolution (see Section B.4).
On most systems, when base R is started, it opens a window in which the various com-
mands can be entered and the (nongraphical) output is reported. This is known as the R
workspace. Separate windows will open to display output from the graphic subsystem. The
workspace and all R variables and objects created during the session can be saved for future
retrieval. As with any computer software, it is advisable to save workspace results regularly
and often.
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

422
STATISTICAL DATA ANALYTICS
Base R is command-line driven, and in this textbook, code presented to generate statistical
and mathematical calculations employs the command-line format. For distinguishability, this
is shown in standard typewriter text, either as inline commands or as standalone displays.
In the latter case, code to be entered at the command line will have a leading entry mark using
R’s default ‘greater than’ sign, >. Thus a command to give the sample mean, say, of a vector
of numbers X will be displayed as
> mean( X )
Note that all R code is case sensitive. mean(X) and mean(x) will give different values unless
X and x have exactly the same arithmetic mean.
R output will also be presented in typewriter text, again either inline or as a stan-
dalone display. In the latter case, to distinguish from commands/code, no leading > will appear,
although, in many cases, R will lead with a bracketed numeral to indicate what element of its
output stream immediately follows. For instance, if there is only one element to follow, the out-
put leads with [1] (illustrated later). Where necessary, output may be edited for presentation
purposes.
The basic function to learn about any R feature or function is help(), where the particular
command of interest is placed between the parenthesis marks. For instance,
> help( mean )
gives a help page on the mean() function to calculate arithmetic means. An equivalent short-
cut places the command of interest after a questions mark, as in
> ?mean
If R cannot interpret the query, it gives a No documentation error.
To quit or end an R session, the base command is
> q()
although each separate operating system may have equivalent menu bar or mouse shortcuts.
R uses special expressions to indicate certain types of values. A number that cannot be
calculated, for example, 0/0, is ‘not a number,’ or NaN. Calculations that include an NaN will
themselves result in an NaN. This differs, however, from a number whose limit is (unambigu-
ously) ∞, say, 1/0. If R recognizes the infinite value, it will assign it the special expression
Inf. Any calculations that involve an Inf attempt to parse the limiting infinite value. This
can result in another Inf, a finite value, or even an NaN, depending on the particulars. Missing
values are possible and are expressed as NA. Note that this differs from NaN, above.
As any R user will affirm, the best way to learn R is to use R: experiment with the various
commands (and options), sort through the help() files (some of which can be admittedly
opaque), and apply the functions and methods to data. Besides its use as a computing envi-
ronment, R is also an effective tool for learning and understanding data-analytic thinking.
The following sections give some examples to help get started.
B.1
Data entry and manipulation
Data can be entered into the current workspace in various ways. The simplest is with the c()
(for ‘concatenate’) command, for example,
> c( 1, -1.2, pi )
#these are three numbers

BRIEF INTRODUCTION TO R
423
creates a vector with three entries, the last of which is the internally held constant 𝜋=
3.14159265... (The # hash symbol is R’s comment character: all text after a # is ignored until
the next carriage return.) This vector can be assigned to an object or variable that retains
the three values for future use. R’s default assignment operator is the left arrow ←, that is,
assign the quantity on the right to the quantity on the left (a very programming-oriented
construction). Most keyboards do not carry a left-arrow key, however, so R uses the two
keystrokes <- (i.e., ‘less than’ and ‘minus’) with no space between them. As each of these
symbols carries its own (different!) meaning when applied separately in R, users must apply
caution when entering them. In its newer versions, R allows equivalent use of the equal sign
= for assignment. (The two expressions can act differently in certain cases, so caution is
advised. See the R online manual.) Thus, for example, to assign the three numbers above to
the variable X, use either of the following:
> X <- c(1, -1.2, pi)
> X =
c(1, -1.2, pi)
#same assignment
To display the contents of an existing object in the workspace, use the print() function.
To list the current contents of the workspace, including all assigned variables, type ls().
It is good practice to keep the workspace compact, so that excess/temporary objects do not
build up. The remove() function, also simply rm(), deletes unwanted variables, objects, and
functions. (Needless to say, this should be used with caution: there is no ‘unremove’ function.)
Larger data sets can be entered using other, more powerful functions. The scan() function
scans in keyboard input continuously until two successive carriage returns are entered. This is
useful for entering long streams of data. It can also be applied in conjunction with file system
paths that direct to an external file, for example, in Windows, the command
> X <- scan( "C:/datafile.txt" )
finds the ASCII text file datafile.txt located on the user’s C: drive and assigns its contents
into the variable X. (Notice use of the forward slash.) If the file is located deep in the file system,
dialog-box navigation can be invoked via file.choose():
> X <- scan( file.choose() )
Inbuilt functions can perform common assignment tasks. For instance, seq(from=,to=,
by=) produces sequences of numbers, while rep() repeats blocks of material a given number
of times. To wit,
> seq( from=1, to=13 )
#default is by=1
[1]
1
2
3
4
5
6
7
8
9 10 11 12 13
> 1:13
#shortcut for seq( 1,13 )
[1]
1
2
3
4
5
6
7
8
9 10 11 12 13
> seq( from=13, to=4, by=-1 )
[1]
13 12 11 10
9
8
7
6
5
4
> seq( from=3, to=7.5, by=0.5 )
[1] 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5
> rep( 13, times=4 )
[1] 13 13 13 13

424
STATISTICAL DATA ANALYTICS
For multiple-variable databases or tables of data, a series of read functions can be used.
For instance, read.table(file.choose()) will read a variety of different database for-
mats, here using dialog-box navigation; see help(read.table). R can, in fact, read and
write data in many forms, including interfaces with other popular software programs. For
more details, see the CRAN introduction at http://cran.r-project.org/doc/manuals/R-intro.html
#Reading-data-from-files or any of the introductory sources mentioned earlier.
Once entered, variables can be manipulated in a variety of ways. Summary functions
or other operations applied to vectors produce either scalar- or vector-valued results. For
instance, given the three numbers in X, above, we find
> length( X )
[1] 3
> sum( X )
[1] 2.941593
> round( sum(X), digits=4 )
[1] 2.9416
> mean( X )
[1] 0.9805309
> rep( X, times=2 )
[1]
1.000000 -1.200000
[3]
3.141593
1.000000
[5] -1.200000
3.141593
R operates seamlessly with vector- and matrix-valued objects; indeed, it is an
object-oriented programming environment. Where necessary, a single variable contain-
ing a stream of numbers is treated as a column vector (even though it displays as a row). To
create an n × p matrix, use the matrix() command; see help(matrix). For example,
> M <- matrix( seq( 2,7 ), nrow=2, ncol=3 )
produces the 2 × 3 matrix
[,1] [,2] [,3]
[1,]
2
4
6
[2,]
3
5
7
Notice that data entry is by column; this can be modified via options in matrix(). R can also
‘bind’ two or more vectors together into a matrix, via the cbind() and rbind()commands.
The former binds columns, the latter binds rows. To coerce an R object into a matrix, use
as.matrix().
A special kind of array object in R is known as a data frame. Data frames are used to
collect p different n × 1 vectors and/or n × p matrices together when their ith rows relate to one
another. This is particularly useful in statistical applications: suppose a response variable Y is
associated with two predictor variables X1 and X2. A data frame connects the ith observation in
each vector in order to facilitate future statistical calculations. (Thus, any data frame’s column
vectors must all have the same length. Missing values, as NA, are permitted.)
In R, the data.frame() command constructs data frames. For example,
> Y <- c( 3, 9, 5.5, -1, -1.1 )
> X1 <- seq( from=3, to=5, by=0.5 )
> X2 <- c( 540.4, 100.2, 221.0, 490.1, NA )
> data.df <- data.frame( Y, X1, X2 )

BRIEF INTRODUCTION TO R
425
yields
Y
X1
X2
1
3.0
3.0
540.4
2
9.0
3.5
100.2
3
5.5
4.0
221.0
4 -1.0
4.5
490.1
5 -1.1
5.0
NA
The first column of the output is the row number. The remainder is the actual data frame with
the columns aligned by row. Notice that column names correspond to the entry variables.
Elements within R vectors, matrices, data frames, etc. are referenced by square brackets.
For instance, in the matrix M from above, the (2,3)rd element lies in the second row and the
third column and is referenced as M[2,3]. Notice that row indices precede column indices.
The entire third column is simply M[,3], while the entire second row is M[2,]. Indexing for
a data frame is similar: for example, calling data.df[4,3] above reports the value 490.1.
A minus sign used in a bracketed index indicates deletion; for example, to remove the
second and third rows from data.df, use
> data.df[ -c(2,3), ]
Y
X1
X2
1
3.0
3.0
540.4
4
-1.0
4.5
490.1
5
-1.1
5.0
NA
R’s object-oriented structure allows for creative use of the bracketed indexing feature
to created subsetted objects; see http://cran.r-project.org/doc/manuals/R-intro.html#Index-
vectors. For instance, to identify those elements in a vector below a specific threshold, embed
the threshold as a logical query (cf. Table B.1):
> Y[ Y<0 ]
[1] -1.0
-1.1
Any R object, including data frames, possesses a series of internal attributes that describe
and summarize its contents. To view these, apply the attributes() command, for example,
> attributes( data.df )
$names
[1] "Y"
"X1" "X2"
$row.names
[1] 1 2 3 4 5
$class
[1] "data.frame"
Users can access subcomponents from any R object for further calculation. For most of
the objects seen herein, the $ symbol acts as a separator between the object name and the
subcomponent name:

426
STATISTICAL DATA ANALYTICS
>
data.df$Y
[1]
3.0
9.0
5.5 -1.0 -1.1
Combine this with bracketed indexing, for example, to access individual elements of any mul-
ticomponent object:
>
data.df$X2[ c(2,4) ]
[1] 100.2
490.1
B.2
A turbo-charged calculator
Many authors affirm, sometimes tongue-in-cheek, that at its core R is ‘a turbo-charged calcu-
lator.’ That is, given a series of mathematical operations, R will report their result as reliably as
a typical hand calculator – with, often, higher floating-point accuracy. Consider the following
commands, functions (which should be obvious), and their resulting outputs:
> 13 + 4
[1] 17
> 13 * 4
[1] 52
> 13^2
[1] 169
> sqrt( 13 )
[1] 3.605551
> abs( 4 - 13 )
[1] 9
> cos( pi )
[1] -1
> log( 13 - 4 )
[1] 2.197225
> factorial(4)
[1] 24
Here, log() is the natural logarithm unless specified otherwise with an optional
argument – see help(log) – and the factorial() function simply applies (2.19).
Notice that the (scalar) multiplication operator is the usual *.
Vector and matrix arithmetic is performed with special commands. Matrix addition and
subtraction is elementwise, as in Section A.2, so just apply the + and - operators, respectively.
Matrix multiplication requires a special operator, however: %*%. (In all cases, of course, the
operands must be conformable.) Inverses of square, nonsingular matrices are found with the
solve() function. When needed, transposition is achieved with the t() function. Thus, for
example, with the 3 × 1 vector X and the 2 × 3 matrix M seen earlier, we have
>
M %*% X
[,1]
[1,] 16.04956
[2,] 18.99115
> X %*% M
Error in X %*% M : non-conformable arguments
> t(X) %*% t(M)
[,1]
[,2]
[1,] 16.04956 18.99115

BRIEF INTRODUCTION TO R
427
The diag() function creates diagonal matrices from input values (among other features). So,
for example,
> D <- diag(X)
> print(D)
[,1]
[,2]
[,3]
[1,]
1
0.0
0.000000
[2,]
0
-1.2
0.000000
[3,]
0
0.0
3.141593
> solve(D)
[,1]
[,2]
[,3]
[1,]
1
0.00000
0.00000
[2,]
0
-0.83333
0.00000
[3,]
0
0.00000
0.31831
Readers may verify that solve(D)%*%D is indeed the 3 × 3 identity matrix I.
Besides its ability to produce basic mathematical calculations, R also provides an interface
to a powerful graphics subsystem. (Almost all the plots in this text were constructed using R’s
graphical capabilities.) At its core is the basic plot() command. For example, to plot an
ordinate variable Y against an abscissa variable X, use plot(X,Y) or equivalently plot(Y ˜
X). In either case, order is important.
Plot output in R can be manipulated in a variety of ways, and this allows for some imagi-
native graphical displays. For more details, see http://cran.r-project.org/doc/manuals/R-intro
.html#Graphics and Murrell (2011).
B.3
R functions
B.3.1
Inbuilt R functions
As illustrated earlier, R contains a large collection of inbuilt functions for mathematical and
statistical calculations. These all use parentheses to sequester their arguments, although some
functions such as ls() or q() may not require any arguments. The parentheses are still nec-
essary, however. In this textbook, an R function will, in most cases, be referred to with its
parentheses, but if the use is generic no actual arguments or argument placeholders will be
included. When specific arguments or inputs are required, these will be listed.
To learn about any function, simply apply help(), as suggested above. This usually
reports the required and/or optional arguments each functions takes and gives further detail on
how these are used. If a particular option is specified formally in the function’s help screen,
the specification represents the default value for that option. Argument or option position is
assumed as given. For example, help(log) reports that the log() function has the complete
form log(x, base=exp(1)). That is, the first argument to log() is some value x, and if
a second argument is given, it is the desired base of the logarithmic calculation. If no second
argument is given, the function defaults to the natural log with base=exp(1).
Some help screens will show an ellipsis (...) in a function’s description. This simply indi-
cates that further arguments can be passed to the function from other functions or operations,
giving the programmer added flexibility.
Position in a function call can be overridden by specifying the arguments or options by
their individual names, in any order. The default position is imposed only if the argument(s)

428
STATISTICAL DATA ANALYTICS
is given as a standalone value. Thus with the log() function, the following calls are all
equivalent:
> log( pi )
[1] 1.14473
> log( x=pi )
#from help(log): specify the operand via "x="
[1] 1.14473
> log( base=exp(1), x=pi )
[1] 1.14473
A semicolon (;) in R acts as a separator, allowing for multiple commands or operations to be
given before a carriage return. Thus, for example,
> x <- sqrt( pi ); print( x+1 )
and
> x <- sqrt( pi )
> print( x+1 )
both produce the same result (2.772454, for the record).
Special forms of logical operations can also be performed, either within a function that
calls for a logical argument or simply as a standalone logical query. The basic logical operators
are given in Table B.1.
The outcome of a logical operation is either TRUE or FALSE. This may seem obvious,
but in fact, R treats these as actual values: TRUE or FALSE. Shorthand equivalents are T or F.
(Thus, it is usually unwise to name a separate variable as T or F – popular letters in statistical
parlance! – because these could confuse future logical operations.) For instance, to determine
if the fourth element of a vector X2 equals a certain number, say 14.2, write X2[4]==14.2.
Include a logical OR to expand the query:
> print( X2 )
[1] 540.4
100.2
221.0
490.1
NA
> ( X2[4] == 14.2 ) | ( X2[4] < 2.72 )
#logical OR via "|"
[1] FALSE
Combining logical operators with element referencing via square brackets gives R pro-
gramming particular pliancy. For example, the is.na() function reports the indices of those
elements missing (‘NA’) in an object. These can then be applied to that object (or any other
object) to excise the missing observations:
Table B.1
Logical operators in R.
Operator
Operation
Operator
Operation
==
Equal to
!=
Not equal to
<
Less than
<=
Less than or equal to
>
Greater than
>=
Greater than or equal to
&
Logical AND
|
Logical OR

BRIEF INTRODUCTION TO R
429
> mean( X2 )
[1] NA
> is.na( X2 )
[1] FALSE FALSE FALSE FALSE
TRUE
> X2[ is.na(X2)==F ]
[1] 540.4 100.2 221.0 490.1
> mean( X2[is.na(X2)==F] )
[1] 337.925
By the way, this particular calculation can be accomplished simply by using the na.rm= option
(for ‘NA remove’):
> mean( X2, na.rm=TRUE )
[1] 337.925
See help(mean).
B.3.2
Flow control
Users can apply standard loops and conditional execution for flow control of R calcula-
tions. The program has the usual for(), while(), and repeat() loop capabilities. It also
allows for if() statements, where the argument to the if() command is a logical query; for
example, to set the fifth element of the vector Y equal to zero if the corresponding element of
X2 is missing, use
> if( is.na(X2[5])==T ) { Y[5]=0 }
(The braces contain the operation to be executed if the argument is TRUE. These are not nec-
essary if the executable operation can be contained on a single input line.)
The else command can be combined with if() to enhance flow control. A special
ifelse() command is also available; see http://cran.r-project.org/doc/manuals/R-intro.html#
Loops-and-conditional-execution.
B.3.3
User-defined functions
R allows programmers to construct their own user-defined functions via the function()
command. The standard syntax is
> functionName <- function( arg_1, arg_2, ... ) {
executable_1
executable_2
⋮
}
#end of function
where functionName is the user’s choice for the name of the function, each arg is an
input argument to the function, and the executable statements contained by braces
are various lines of R code to be executed. For the function to actually return a result,
the desired output quantity must be written as the last line of the executable statements,

430
STATISTICAL DATA ANALYTICS
or be reported via the return() subfunction. See http://cran.r-project.org/doc/manuals/
R-intro.html#Writing-your-own-functions for details.
One can also take advantage of R’s capabilities as a programming language, although
details on this are generally outside the scope of this text. Advanced users may gain from the
treatments by Jones et al. (2009) and Gentleman (2009). Also see Venables and Ripley (2000).
B.4
R packages
Another powerful feature in R is its use of libraries and packages to assemble specialized
functions. These are stored as separate compendiums for more efficient access. For example,
the core stats package contains the basic statistical functions; it loads automatically when R
starts. (Packages are referenced in this text using italic naming. Use these to refer to the CRAN
web site at http://cran.r-project.org/doc/manuals/R-intro.html#Packages for full information.)
Other packages can be more exotic, such as the lattice package for trellis graphics. To see the
full list of available packages for a local installation, type library() after starting R.
Except for the base collection, most packages do not load automatically and must
be explicitly invoked. Either of the commands library() or require() can load an
installed package. For example, to load the lattice package, type library(lattice) or
require(lattice). To detach the loaded package, type detach(package:lattice).
Beside the base offerings, a wealth of external, add-on packages has been devel-
oped by the R user community. Most are available from the CRAN web site at
http://cran.r-project.org/web/packages/. Each package provides a targeted and some-
times highly specific set of statistical and mathematical functions, many of which are useful
in modern data analytics. Where appropriate, specific examples are highlighted throughout
the main chapters of this textbook.
External packages must first be installed (just once) on the computer’s local storage device
before they can be loaded into an active workspace. To install an external package, follow the
instructions in help(install.packages). (The program may first require that a specific
CRAN repository or mirror site be specified for online access.) The process is facilitated
on certain platforms with menu-driven options; for example, modern Windows installations
provide a ‘Packages’ menu for package retrieval and management. Once installed locally, a
package is loaded for case-by-case use with library() or require(), as above.
A few distinctive packages are worth particular mention. First is a series designed for use
with exceptionally large data sets, as would be common in large-scale data analytics (the ‘big
data’ setting). When R is employed on desktop computers and standalone workstations, it
will generally be limited to the machine’s available RAM. Depending on the local setup, a
massive data set can tax the program’s abilities. In response, developers have contributed spe-
cialized functions that break the data into smaller portions and then map these to RAM in a
sequential or segmented manner. An archetypal example is the external ff package; its exten-
sion ffbase provides an extensive toolkit of functions for use on massive data sets, including
arithmetic operators, data manipulation, summary statistics, and data transformations. Also
useful is the external biglm package, which performs linear (Chapters 6 and 7) and general-
ized linear (Chapter 8) regression with massive data. Other advances along these lines include
the external bigmemory package and its extension, biganalytics. Development of packages
such as these for massive data continues apace and will likely prove a valuable asset in R’s
ongoing evolution.

BRIEF INTRODUCTION TO R
431
For users uncomfortable with the command-line nature of base R, the external Rcmdr
package – also known as ‘R Commander’ – provides graphical user interface (GUI) capabil-
ity for an R session. The package is popular for its intuitive feel, its selection of statistical
functions, and, especially, its value as a tool for introducing basic statistical methods with R.
See Wilson (2012) for a comparative review. Rcmdr can also integrate with another useful tool,
RStudio
®. Although not itself an R package, RStudio is an application programming interface
(API) that oversees R sessions; manages the workspace; debugs, edits, and executes R code;
and produces pertinent graphical output. It is particularly useful for integrated code/software
development and can be accessed in either open source or commercial editions. More detail
is available at http://www.rstudio.com/.
Readers are cautioned that any references herein to external R packages represent neither
endorsements for nor approbation of those packages. As with any application of an open
source product, users operate at their own gain and risk.

References
Abdi H and Williams LJ (2010). Principal component analysis. Wiley Interdisciplinary
Reviews: Computational Statistics 2(4), 433–459.
Abrahantes JC, Sotto C, Molenberghs G, Vromman G and Bierinckx B (2011). A comparison
of various software tools for dealing with missing data via imputation. Journal of Statistical
Computation and Simulation 81(11), 1653–1675.
Agrawal R, Imielinski T and Swami AN (1993). Mining association rules between sets of
items in large databases. In Proceedings of the 1993 ACM SIGMOD International Con-
ference on Management of Data – SIGMOD ’93, Washington, DC, May 26–28, 1993 (eds.
Buneman P and Jajodia S), pp. 207–216. ACM Press, New York.
Agrawal R and Srikant R (1994). Fast algorithms for mining association rules in large
databases. In VLDB ’94, Proceedings of 20th International Conference on Very Large Data
Bases, September 12-15, 1994, Santiago, Chile (eds. Bocca JB, Jarke M and Zaniolo C),
pp. 487–499. Morgan Kaufmann, San Francisco, CA.
Agresti A (2013). Categorical Data Analysis, 3rd edn. John Wiley & Sons, Inc., Hoboken,
NJ.
Agresti A and Coull BA (1998). Approximate is better than “exact” for interval estimation of
binomial proportions. American Statistician 52(2), 119–126.
Ahmad IA and Ran IS (2004). Data based bandwidth selection in kernel density estima-
tion with parametric start via kernel contrasts. Journal of Nonparametric Statistics 16(6),
841–877.
Akaike H (1973). Information theory and an extension of the maximum likelihood principle.
In Proceedings of the 2nd International Symposium on Information Theory (Tsahkadsor,
1971) (eds. Petrov BN and Csáki B), pp. 267–281. Akadémiai Kiadó, Budapest.
Allen DM (1974). The relationship between variable selection and data augmentation and a
method for prediction. Technometrics 16(1), 125–127.
Anderson TW and Olkin I (1985). Maximum-likelihood estimation of the parameters of a
multivariate normal distribution. Linear Algebra and its Applications 70, 147–171.
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics

REFERENCES
433
Anonymous (1949). News. Mathematical Tables and Other Aids to Computation 3(28),
544–547.
Anonymous (2012). P&GJ’s annual pipeline report: 500 leading gas distribution utilities.
Pipeline & Gas Journal 239(11), 22–30.
Anonymous (2013). 2013 hotel management survey: top third-party management companies.
Hotel Management 288(3), 26–36.
Armitage P (1955). Tests for linear trends in proportions and frequencies. Biometrics 11(3),
375–386.
Astuti E and Yanagawa T (2002). Trend test for count data with extra-Poisson variability.
Biometrics 58(2), 398–402.
Austin E, Pan W and Shen X (2013). Penalized regression and risk prediction in genome-wide
association studies. Statistical Analysis and Data Mining 6(4), 315–328.
Babu GJ and Singh K (1983). Inference on means using the bootstrap. Annals of Statistics
11(3), 999–1003.
Bailer AJ and Oris JT (1997). Estimating inhibition concentrations for different response
scales using generalized linear models. Environmental Toxicology and Chemistry 16(7),
1554–1560.
Barnett V and Lewis T (1995). Outliers in Statistical Data, 3rd edn. John Wiley & Sons, Ltd,
Chichester.
Bartholomew D, Knott M and Moustaki I (2011). Latent Variable Models and Factor Analysis:
A Unified Approach, 3rd edn. John Wiley & Sons, Ltd, Chichester.
Bartlett MS (1938). Further aspects of the theory of multiple regression. Proceedings of the
Cambridge Philosophical Society: Mathematical and Physical Sciences 34(1), 33–40.
Bartlett MS (1941). The statistical significance of canonical correlations. Biometrika 32(1),
29–37.
Bartlett MS (1950). Tests of significance in factor analysis. British Journal of Statistical
Psychology 3(2), 77–85.
Bayes T (1763). An essay towards solving a problem in the doctrine of chances. Philosophical
Transactions of the Royal Society of London 53, 370–418.
Becker RA (1994). A brief history of S. In Computational Statistics: Papers Collected on
the Occasion of the 25th Conference on Statistical Computing at Schloß Reisensburg (eds.
Dirschedl P and Ostermann R), pp. 81–110. Physica-Verlag, Heidelberg.
Beirlant J, Dudewicz EJ, Györfi L and van der Meulen EC (1997). Nonparametric entropy
estimation: an overview. International Journal of Mathematical and Statistical Sciences
6(1), 17–39.
Belsley DA, Kuh E and Welsch RE (1980). Regression Diagnostics: Identifying Influential
Data and Sources of Collinearity. John Wiley & Sons, Inc., New York.
Benjamini Y (1988). Opening the box of a boxplot. American Statistician 42(4), 257–262.
Benjamini Y and Hochberg Y (1995). Controlling the false discovery rate: a practical and
powerful approach to multiple testing. Journal of the Royal Statistical Society, Series B
(Methodological) 57(1), 289–300.
Berger RL (1996). More powerful tests from confidence interval p values. American Statisti-
cian 50(4), 314–318.
Berk R, Brown LD, Buja A, Zhang K and Zhao L (2013). Valid post-selection inference.
Annals of Statistics 41(2), 802–837.

434
REFERENCES
Bertin-Mahieux T, Ellis DP, Whitman B and Lamere P (2011). The million song dataset. In
Proceedings of the 12th International Society for Music Information Retrieval Conference
(ISMIR 2011), Miami, FL, October 24–28, 2011 (eds. Klapuri A and Leider C),
pp. 591–596. University of Miami, Miami, FL.
Birren JE and Morrison DF (1961). Analysis of the WAIS subtests in relation to age and
education. Journal of Genontology 16(4), 363–369.
Blæsild P and Granfeldt J (2002). Statistics with Applications in Biology and Geology. Chap-
man & Hall/CRC Press, Boca Raton, FL.
Blyth CR and Still HA (1983). Binomial confidence intervals. Journal of the American Sta-
tistical Association 78(381), 108–116.
Bonferroni CE (1936). Teoria statistica delle classi e calcolo delle probabilityà. Pubblicazioni
del R Istituto Superiore di Scienze Economiche e Commerciali di Firenze 8, 3–62.
Borgelt C (2012). Frequent item set mining. Wiley Interdisciplinary Reviews: Data Mining
and Knowledge Discovery 2(6), 437–456.
Borwein P and Erdélyi T (1995). Polynomials and Polynomial Inequalities. Springer-Verlag,
New York.
Box GEP and Cox DR (1964). An analysis of transformations (with discussion). Journal of
the Royal Statistical Society, Series B (Methodological) 26(2), 211–252.
Box GEP, Jenkins GM and Reinsel GC (2008). Time Series Analysis: Forecasting and Control,
4th edn. John Wiley & Sons, Inc., Hoboken, NJ.
Bradley RA and Srivastava SS (1979). Correlation in polynomial regression. American
Statistician 33(1), 11–14.
Breiman L (1996). Bagging predictors. Machine Learning 24(2), 123–140.
Breiman L (2001). Random forests. Machine Learning 45(1), 5–32.
Breiman L, Friedman J, Olshen RA and Stone CJ (1984). Classification and Regression Trees.
Wadsworth, Belmont, CA.
Bretz F, Hothorn T and Westfall P (2011). Multiple Comparisons Using R. Chapman & Hall,
Boca Raton, FL.
Brijs T, Swinnen G, Vanhoof K and Wets G (1999). The use of association rules for product
assortment decisions: A case study. In Proceedings of the Fifth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining – SIGKDD ’99, San Diego, CA,
August 15–18, 1999 (eds. Fayyad U, Chaudhuri S and Madigan D), pp. 254–260. ACM
Press, New York.
Broberg P (2003). Statistical methods for ranking differentially expressed genes. Genome
Biology 4(6), Article no. R41.
Brown LD (1986). Fundamentals of Statistical Exponential Families, with Applications in Sta-
tistical Decision Theory, Institute of Mathematical Statistics Lecture Notes – Monograph
Series, vol. 9. Institute of Mathematical Statistics, Hayward, CA.
Brown LD, Cai TT and DasGupta A (2001). Interval estimation for a binomial proportion
(with discussion). Statistical Science 16(2), 101–133.
Brown LD, Cai TT and DasGupta A (2002). Confidence intervals for a binomial proportion
and asymptotic expansions. Annals of Statistics 30(1), 160–201.
Buja A, Cook D, Hofmann H, Lawrence M, Lee EK, Swayne DF and Wickham H (2009).
Statistical inference for exploratory data analysis and model diagnostics. Philosophi-
cal Transactions of the Royal Society of London, Series A: Mathematical, Physical and
Engineering Sciences 367(1906), 4361–4383.

REFERENCES
435
Buonaccorsi JP (2012). Fieller’s theorem. In Encyclopedia of Environmetrics, Vol. 2, 2nd
edn. (eds. El-Shaarawi AH and Piegorsch WW), pp. 1015–1017. John Wiley & Sons, Ltd,
Chichester.
Cali´nski T and Harabasz J (1974). A dendrite method for cluster analysis. Communications
in Statistics 3(1), 1–27.
Cardoso MG (2013). Logical discriminant models. In Quantitative Modeling in Marketing and
Management (eds. Moutinho L and Huarng KH), pp. 223–253. World Scientific Publishing,
Singapore.
Carroll RJ and Ruppert D (1988). Transformation and Weighting in Regression. Chapman &
Hall, New York.
Carroll SS (1998). Modelling abiotic indicators when obtaining spatial predictions of species
richness. Environmental and Ecological Statistics 5(3), 257–276.
Casella G and Berger RL (2002). Statistical Inference, 2nd edn. Duxbury Press, Pacific Grove,
CA.
Cattell RB (1966). The Scree test for the number of factors. Multivariate Behavioral Research
1(2), 245–276.
Cavanaugh JE (1999). A large-sample model selection criterion based on Kullback’s symmet-
ric divergence. Statistics & Probability Letters 42(4), 333–343.
Cayley A (1858). A memoir on the theory of matrices. Philosophical Transactions of the
Royal Society of London 148(1), 17–37.
Chadeau-Hyam M, Hoggart CJ, O’Reilly PF, Whittaker JC, De Iorio M and Balding DJ
(2008). Fregene: Simulation of realistic sequence-level data in populations and ascertained
samples. BMC Bioinformatics 9(9), Article No. 364.
Charytanowicz M, Niewczas J, Kulczycki P, Kowalski PA, Łukasik S and ̇Zak S (2010).
A complete gradient clustering algorithm for features analysis of X-ray images. In
Information Technologies in Biomedicine, Vol. 2, AISC 69 (eds. Pi
¸
etka E and Kawa J),
pp. 15–24. Springer-Verlag, Berlin.
Chen Z, Bai Z and Sinha BK (2004). Ranked Set Sampling: Theory and Applications, Lecture
Notes in Statistics, Vol. 176. Springer, New York.
Chen Y, Du P and Wang Y (2014). Variable selection in linear models. Wiley Interdisciplinary
Reviews: Computational Statistics 6(1), 1–9.
Christensen R (2011). Plane Answers to Complex Questions. The Theory of Linear Models,
4th edn. Springer, New York.
Claeskens G and Hjort NL (2003). The focused information criterion (with discussion). Jour-
nal of the American Statistical Association 98(464), 900–945.
Clark K (2010). Top 500: the race to recovery. Home Channel News 36(6), 23–45.
Clarke B, Fokoué E and Zhang HH (2009). Principles and Theory for Data Mining and
Machine Learning. Springer, Dordrecht.
Clausius R (1865). Über verschiedene für die Anwendung bequeme Formen der Hauptgle-
ichungen der mechanischen Wärmetheorie. Annalen der Physik 201(7), 353–400.
Clemmensen L, Hastie T, Witten D and Ersbøll B (2011). Sparse discriminant analysis. Tech-
nometrics 53(4), 406–413.
Cleveland WS (1979). Robust locally weighted regression and smoothing scatterplots. Journal
of the American Statistical Association 74(368), 829–836.
Cleveland WS (1993). Visualizing Data. Hobart Press, Summit, NJ.

436
REFERENCES
Cleveland WS and Devlin SJ (1988). Locally weighted regression: an approach to regression
analysis by local fitting. Journal of the American Statistical Association 83(403), 596–610.
Cleveland WS, Grosse E and Shyu WM (1992). Local regression models. In Statistical Models
in S (eds. Chambers JM and Hastie TJ), pp. 309–376. Wadsworth & Brooks/Cole, Pacific
Grove, CA.
Clopper CJ and Pearson ES (1934). The use of confidence or fiducial limits illustrated in the
case of the binomial. Biometrika 26(4), 404–413.
Cochran WG (1954). Some methods for strengthening the common 𝜒2 tests. Biometrics 10(4),
417–451.
Coles S (2001). An Introduction to Statistical Modeling of Extreme Values. Springer-Verlag,
London.
Cook RD (1977). Detection of influential observations in linear regression. Technometrics
19(1), 15–18.
Cook RD and Weisberg S (1982). Residuals and Influence in Regression. Chapman & Hall,
London.
Cordeiro GM and McCullagh P (1991). Bias reduction of maximum likelihood estimates.
Journal of the Royal Statistical Society, Series B (Methodological) 53(3), 629–643.
Cox DR (1958). The regression analysis of binary sequences (with discussion). Journal of the
Royal Statistical Society, Series B (Methodological) 20(2), 215–242.
Cox DR (1961). Tests of separate families of hypotheses. In Proceedings of the Fourth
Berkeley Symposium on Mathematical Statistics and Probability, Vol. I (ed. Neyman J),
pp. 105–123. University California Press, Berkeley, CA.
Cox DR (1962). Further results on tests of separate families of hypotheses. Journal of the
Royal Statistical Society, Series B (Methodological) 24(2), 406–424.
Cox DR (1988). Some aspects of conditional and asymptotic inference: A review. Sankhy¯a,
Series A 50(3), 314–337.
Cox DR (2013). A return to an old paper: ‘Tests of separate families of hypotheses’ (with
discussion). Journal of the Royal Statistical Society, Series B (Statistical Methodology)
75(2), 207–215.
Crivelli A, Firinguetti L, Montaño R and Muñóz M (1995). Confidence intervals in ridge
regression by bootstrapping the dependent variable: a simulation study. Communications
in Statistics – Simulation and Computation 24(3), 631–652.
Cule E and De Iorio M (2013). Ridge regression in prediction problems: automatic choice of
the ridge parameter. Genetic Epidemiology 37(7), 704–714.
Cule E, Vineis P and De Iorio M (2011). Significance testing in ridge regression for genetic
data. BMC Bioinformatics 12(9), Article No. 372.
Dagliyan O, Uney-Yuksektepe F, Kavakli IH and Turkay M (2011). Optimization based tumor
classification from microarray gene expression data. PLoS ONE 6(2), Article No. e14579.
Dalgaard P (2008). Introductory Statistics with R, 2nd edn. Springer-Verlag, New York.
Davis CE and Steinberg SM (2006). Quantile estimation. In Encyclopedia of Statistical Sci-
ences, Vol. 10 (eds. Kotz S, Read CB, Balakrishnan N and Vidakovic B), pp. 6704–6707.
John Wiley & Sons, Inc, Hoboken, NJ.
Davison AC and Hinkley DV (1997). Bootstrap Methods and Their Application, Cambridge
Series in Statistical and Probabilistic Mathematics, Vol. 1. Cambridge University Press,
Cambridge.

REFERENCES
437
Decrouez G and Robinson AP (2012). Confidence intervals for the weighted sum of two
independent binomial proportions. Australian & New Zealand Journal of Statistics 54(3),
281–299.
Deutsch RC and Piegorsch WW (2012). Benchmark dose profiles for joint-action quantal data
in quantitative risk assessment. Biometrics 68(4), 1313–1322.
DiCiccio TJ and Romano JP (1988). A review of bootstrap confidence intervals (with discus-
sion). Journal of the Royal Statistical Society, Series B (Methodological) 50(3), 338–370
(corr. vol. 51(3), 470).
Digby P and Kempton R (1987). Multivariate Analysis of Ecological Communities. Chapman
& Hall, London.
Dillon WR, Kumar A and Mulani N (1987). Offending estimates in covariance structure anal-
ysis: comments on the causes of and solutions to Heywood cases. Psychological Bulletin
101(1), 126–135.
Dixon SJ and Brereton RG (2009). Comparison of performance of five common classifiers
represented as boundary methods: Euclidean Distance to Centroids, Linear Discriminant
Analysis, Quadratic Discriminant Analysis, Learning Vector Quantization and Support
Vector Machines, as dependent on data structure. Chemometrics and Intelligent Laboratory
Systems 95(1), 1–17.
Driver HE and Kroeber AL (1932). Quantitative expression of cultural relationships. Univer-
sity of California Publications in American Archaeology and Ethnology 31(4), 211–256.
Dudoit S and van der Laan MJ (2008). Multiple Testing Procedures with Applications to
Genomics. Springer, New York.
Dziuban CD and Shirkey EC (1974). When is a correlation matrix appropriate for factor anal-
ysis? Some decision rules. Psychological Bulletin 81(6), 358–361.
Dziuban CD, Shirkey EC and Peeples TO (1979). An investigation of some distributional
characteristics of the measure of sampling adequacy. Educational and Psychological Mea-
surement 39(3), 543–549.
Eaton ML and Perlman MD (1973). The non-singularity of generalized sample covariance
matrices. Annals of Statistics 1(4), 710–717.
Efron B and Gong G (1983). A leisurely look at the bootstrap, the jackknife, and
cross-validation. American Statistician 37(1), 36–48.
Eklund G and Seeger P (1965). Massignifikansanalys. Statistisk Tidskrift (Statistical Review),
Series 3 4(5), 355–365.
Elder JF and Pregibon D (1996). A statistical perspective on knowledge discovery in
databases. In Advances in Knowledge Discovery and Data Mining (eds. Fayyad UM,
Piatetsky-Shapiro G, Smyth P and Uthurusamy R), pp. 83–113. American Association for
Artificial Intelligence/MIT Press, Cambridge, MA.
Esposito Vinzi V and Russolillo G (2013). Partial least squares algorithms and methods. Wiley
Interdisciplinary Reviews: Computational Statistics 5(1), 1–19.
Everitt BS (2005). An R and S-Plus® Companion to Multivariate Analysis. Springer-Verlag,
London.
Everitt BS, Landau S, Leese M and Stahl D (2011). Cluster Analysis, 5th edn. John Wiley &
Sons, Ltd, Chichester.

438
REFERENCES
Ezzikouri S, El Feydi AE, Chafik A, Afifi R, El Kihal L, Benazzouz M, Hassar M, Pineau P
and Benjelloun S (2008). Genetic polymorphism in the manganese superoxide dismutase
gene is associated with an increased risk for hepatocellular carcinoma in HCV-infected
Moroccan patients. Mutation Research 649(1-2), 1–6.
Faraway JJ (2006). Extending the Linear Model with R: Generalized Linear, Mixed Effects
and Nonparametric Regression Models. Chapman & Hall/CRC, Boca Raton, FL.
Farcomeni A (2008). A review of modern multiple hypothesis testing, with particular attention
to the false discovery proportion. Statistical Methods in Medical Research 17(4), 347–388.
Fayers PM and Machin D (2007). Quality of Life: The Assessment, Analysis and Interpretation
of Patient-Reported Outcomes, 2nd edn. John Wiley & Sons, Ltd, Chichester.
Feller W (1968). An Introduction to Probability Theory and its Applications, Volume I, 3rd
edn. John Wiley & Sons, New York.
Fieller EC (1940). The biological standardization of insulin (with discussion). Supplement to
the Journal of the Royal Statistical Society 7(1), 1–64.
Firth D (1993). Bias reduction of maximum likelihood estimates. Biometrika 80(1), 27–38
(corr. 82(3), 667).
Fisher RA (1912). On an absolute criterion for fitting frequency curves. Messenger of
Mathematics 41, 155–160.
Fisher RA (1921). On the “probable error” of a coefficient of correlation deduced from a small
sample. Metron 1(1), 3–32.
Fisher RA (1922). On the mathematical foundations of theoretical statistics. Philosoph-
ical Transactions of the Royal Society of London, Series A: Containing Papers of a
Mathematical or Physical Character 222(594–604), 309–368.
Fisher RA (1926). The arrangement of field experiments. Journal of the Ministry of Agricul-
ture of Great Britain 33(6), 503–515.
Fisher RA (1935). The logic of inductive inference (with discussion). Journal of the Royal
Statistical Society 98(1), 39–82.
Fisher RA (1936). The use of multiple measurements in taxonomic problems. Annals of
Eugenics 7(2), 179–188.
Fix E and Hodges JL (1951). Discriminatory analysis, nonparametric discrimination: Con-
sistency properties. Technical Report No. 4, Project 21-49-004, US Air Force School of
Aviation Medicine, Randolph Field, TX.
Fong DYT (2001). Data management and quality assurance. Drug Information Journal 35(3),
839–844.
Forbes C, Evans M, Hastings N and Peacock B (2010). Statistical Distributions, 4th edn. John
Wiley & Sons, Inc., Hoboken, NJ.
Forgy EW (1965). Cluster analysis of multivariate data: Efficiency vs. interpretability of clas-
sifications. Biometrics 21(3), 768–769.
Frank A and Asuncion A (2010). UCI Machine Learning Repository http://archive.ics.uci.
edu/ml, School of Information and Computer Sciences, University of California – Irvine.
Frank IE and Friedman JH (1993). A statistical view of some chemometrics regression tools
(with discussion). Technometrics 35(2), 109–148.
Freedman DA (1983). A note on screening regression equations. American Statistician 37(2),
152–155.

REFERENCES
439
Freund Y and Schapire RE (1996). Experiments with a new boosting algorithm. In Machine
Learning, Proceedings of the 13th International Conference (ICML ’96), Bari, Italy, July
3-6, 1996 (ed. Saitta L), pp. 487–499. Morgan Kaufmann, San Francisco, CA.
Freund Y and Schapire RE (1997). A decision-theoretic generalization of on-line learning and
an application to boosting. Journal of Computer and System Sciences 55(1), 119–139.
Friedman JH, Hastie T, Höfling H and Tibshirani R (2007). Pathwise coordinate optimization.
Annals of Applied Statistics 1(2), 302–332.
Friendly M (1991). SAS System for Statistical Graphics. SAS Institute, Inc., Cary, NC.
Friendly M (2013). The generalized ridge trace plot: visualizing bias and precision. Journal
of Computational and Graphical Statistics 22(1), 50–68.
Fu WJ (1998). Penalized regressions: the bridge versus the Lasso. Journal of Computational
and Graphical Statistics 7(3), 397–416.
Furnival GM (1971). All possible regressions with less computation. Technometrics 13(2),
403–408.
Furnival GM and Wilson RW (1974). Regressions by leaps and bounds. Technometrics 16(4),
499–511.
Galambos J and Simonelli I (1996). Bonferroni-Type Inequalities with Applications. Springer-
Verlag, New York.
Gammon K (2009). Belle Curves. Wired Magazine 17(17.02), Article No. 12.
Garwood F (1936). Fiducial limits for the Poisson distribution. Biometrika 28(3/4), 437–442.
Gatu C and Kontoghiorghes EJ (2006). Branch-and-bound algorithms for computing the
best-subset regression models. Journal of Computational and Graphical Statistics 15(1),
139–156.
Gelman A (2004). Exploratory data analysis for complex models (with discussion). Journal
of Computational and Graphical Statistics 13(4), 755–787.
Gelman A and Unwin A (2013). Infovis and statistical graphics: different goals, different
looks (with discussion). Journal of Computational and Graphical Statistics 22(1), 2–49.
Gentle JE (2003). Random Number Generation and Monte Carlo Methods, 2nd edn. Springer-
Verlag, New York.
Gentle JE (2007). Matrix Algebra: Theory, Computations, and Applications in Statistics.
Springer, New York.
Gentleman R (2009). R Programming for Bioinformatics. Chapman & Hall/CRC Press, Boca
Raton, FL.
Gijbels I and Prosdocimi I (2010). Loess. Wiley Interdisciplinary Reviews: Computational
Statistics 2(5), 590–599.
Givens GH and Hoeting JA (2013). Computational Statistics, 2nd edn. John Wiley & Sons,
Inc., Hoboken, NJ.
Goeman JJ and Solari A (2011). Multiple testing for exploratory research (with discussion).
Statistical Science 26(4), 584–612.
Golub GH, Heath M and Wahba G (1979). Generalized cross-validation as a method for choos-
ing a good ridge parameter. Technometrics 21(2), 215–223.
Golub TR, Slonim DK, Tamayo P, Huard C, Gaasenbeek M, Mesirov JP, Coller H, Loh ML,
Downing JR, Caligiuri MA, Bloomfield CD and Lander ES (1999). Molecular classification
of cancer: class discovery and class prediction by gene expression monitoring. Science
286(5439), 531–537.

440
REFERENCES
González I, Déjean S, Martin PGP and Baccini A (2008). CCA: an R package to extend
canonical correlation analysis. Journal of Statistical Software 23(12), 1–14.
Gopal V, Fuentes C and Casella G (2012). bayesclust: an R package for testing and searching
for significant clusters. Journal of Statistical Software 47(14), 1–21.
Halonen M, Stern DA, Wright AL, Taussig LM and Martinez FD (1997). Altemaria as a
major allergen for asthma in children raised in a desert environment. American Journal of
Respiratory and Critical Care Medicine 155(4), 1356–1361.
Hambrick DZ, Oswald FL, Darowski ES, Rench TA and Brou R (2010). Predictors of
multitasking performance in a synthetic work paradigm. Applied Cognitive Psychology
24(8), 1149–1167.
Hampel D (2008). Estimation of differential entropy for positive random variables and its
application in computational neuroscience. In Mathematical Modeling of Biological Sys-
tems, Volume II (eds. Deutsch A, Bravo de la Parra R, de Boer R, Diekmann O, Jagers P,
Kisdi E, Kretzschmar M, Lansky P and Metz H), pp. 213–224. Birkhäuser, Basel.
Hand DJ (2009a). Measuring classifier performance: a coherent alternative to the area under
the ROC curve. Machine Learning 77(1), 103–123.
Hand DJ (2009b). Naïve Bayes. In The Top Ten Algorithms in Data Mining (eds. Wu X and
Kumar V), pp. 163–177. Chapman & Hall/CRC, Boca Raton, FL.
Hand DJ (2012). Assessing the performance of classification methods. International Statisti-
cal Review 80(3), 400–414.
Hand DJ, Blunt G, Kelly MG and Adams MN (2000). Data mining for fun and profit. Statis-
tical Science 15(2), 111–131.
Hand DJ, Daly F, McConway K, Lunn AD, and Ostrowski, E (eds.) (1994). A Handbook of
Small Data Sets. Chapman & Hall, New York.
Hand DJ, Mannila H and Smyth P (2001). Principles of Data Mining. MIT Press, Cambridge,
MA.
Hand DJ and Yu K (2001). Idiot’s Bayes – Not so stupid after all? International Statistical
Review 69(3), 385–398.
Hartigan JA and Wong MA (1979). A K-means clustering algorithm. Journal of the Royal
Statistical Society. Series C (Applied Statistics) 28(1), 100–108.
Hartley HO (1938). Studentization and large-sample theory. Supplement to the Journal of the
Royal Statistical Society 5(1), 80–88.
Hastie T, Tibshirani R and Friedman J (2009). The Elements of Statistical Learning: Data
Mining, Inference, and Prediction, 2nd edn. Springer, New York.
Hauck WW and Donner A (1977). Wald’s test as applied to hypotheses in logit analysis.
Journal of the American Statistical Association 72(360, part 1), 851–853 (corr. vol. 75(370),
482).
Hayashi K, Bentler PM and Yuan KH (2007). On the likelihood ratio test for the number of
factors in exploratory factor analysis. Structural Equation Modeling: A Multidisciplinary
Journal 14(3), 505–526.
Hazards & Vulnerability Research Institute (2013). The Spatial Hazard Events and Losses
Database for the United States (SHELDUS), Version 10.0 http://www.sheldus.org, Hazards
& Vulnerability Research Institute, University of South Carolina, Columbia, SC.
Henderson HV (2004). Interactive and dynamic graphics in statistical consulting. Journal of
Agricultural, Biological, and Environmental Statistics 9(4), 402–431.

REFERENCES
441
Hendrickson AE and White PO (1964). Promax: a quick method for rotation to oblique simple
structure. British Journal of Statistical Psychology 17(1), 65–70.
Heywood HB (1931). On finite sequences of real numbers. Proceedings of the Royal Society of
London: Series A, Containing Papers of a Mathematical or Physical Character 134(824),
486–501.
Hirji KF (2005). Exact Analysis of Discrete Data. Chapman & Hall/CRC Press, Boca Raton,
FL.
Hoaglin DC, Iglewicz B and Tukey JW (1986). Performance of some resistant rules for outlier
labeling. Journal of the American Statistical Association 81(396), 991–999.
Hochberg Y and Tamhane AC (1987). Multiple Comparison Procedures. John Wiley & Sons,
Inc., New York.
Hoerl AE and Kennard RW (1970). Ridge regression: Biased estimation for nonorthogonal
problems. Technometrics 12(1), 55–67.
Hoerl AE, Kennard RW and Baldwin KF (1975). Ridge regression: some simulations. Com-
munications in Statistics 4(2), 105–123.
Hogg RV and Tanis EA (2010). Probability and Statistical Inference, 8th edn. Pearson Prentice
Hall, Upper Saddle River, NJ.
Höppner F (2010). Association rules. In Data Mining and Knowledge Discovery Handbook
(ed. Maimon O and Rokach L), pp. 299–319. Springer, New York.
Horgan JM (2009). Probability with R: An Introduction with Computer Science Applications.
John Wiley & Sons, Inc., Hoboken, NJ.
Hosmer D and Lemeshow S (2013). Applied Logistic Regression, 3rd edn. John Wiley & Sons,
Inc., New York.
Hotelling H (1933). Analysis of a complex of statistical variables into principal components.
Journal of Educational Psychology 24(6,7), 417–441, 498–520.
Hotelling H (1935). The most predictable criterion. Journal of Educational Psychology 26(2),
139–142.
Hotelling H (1936). Relations between two sets of variates. Biometrika 28(3 and 4), 321–377.
Hsu JC (1996). Multiple Comparisons: Theory and Methods. Chapman & Hall, Boca Raton,
FL.
Huang B, Cook D and Wickham H (2012). tourGui: a gWidgets GUI for the tour to explore
high-dimensional data using low-dimensional projections. Journal of Statistical Software
49(6), 1–12.
Huang LS and Smith RL (1999). Meteorologically-dependent trends in urban ozone. Envi-
ronmetrics 10(1), 103–118.
Hubert LJ (2006). Hierarchical cluster analysis. In Encyclopedia of Statistical Sciences, Vol.
5 (eds. Kotz S, Read CB, Balakrishnan N and Vidakovic B), pp. 3142–3148. John Wiley &
Sons, Inc., Hoboken, NJ.
Huberty CJ and Olejnik S (2006). Applied MANOVA and Discriminant Analysis, 2nd edn.
John Wiley & Sons, Inc., Hoboken, NJ.
Hughes-Hallett D, Gleason AM, McCallum WG, Connally E, Flath DE, Kalayciôglu S,
Lahme B, Frazer Lock P, Lomen DO, Lovelock D, Lozano GI, Morris J, Mumford D,
Osgood BG, Patterson CL, Quinney D, Rhea K, Spiegler AH, Tecosky-Feldman J and
Tucker TW (2013). Calculus: Single and Multivariable, 6th edn. John Wiley & Sons, Inc.,
Hoboken, NJ.

442
REFERENCES
Hurvich CM and Tsai CL (1989). Regression and time series model selection in small samples.
Biometrika 76(2), 297–307.
Hwang YT, Chu SK and Ou ST (2011). Evaluations of FDR-controlling procedures in multi-
ple hypothesis testing. Statistics and Computing 21(4), 569–583.
Hyndman RJ, Koehler AB, Ord JK and Snyder RD (2008). Forecasting with Exponential
Smoothing: The State Space Approach. Springer, Berlin.
Ihaka R and Gentleman R (1996). R: a language for data analysis and graphics. Journal of
Computational and Graphical Statistics 5(3), 299–314.
Irwin JO (1935). Tests of significance for differences between percentages based on small
numbers. Metron 12, 83–94.
Izenman AJ (2008). Modern Multivariate Statistical Techniques. Regression, Classification,
and Manifold Learning. Springer, New York.
Jain AK and Dubes RC (1988). Algorithms for Clustering Data. Prentice Hall, Englewood
Cliffs, NJ.
James G, Witten D, Hastie T and Tibshirani R (2013). An Introduction to Statistical Learning
with Applications in R. Springer, New York.
Jensen RE (1969). A dynamic programming algorithm for cluster analysis. Operations
Research 17(6), 1034–1057.
Johnson NL, Kemp AW and Kotz S (2005). Univariate Discrete Distributions, 3rd edn. John
Wiley & Sons, Inc., New York.
Johnson NL, Kotz S and Balakrishnan N (1994). Continuous Univariate Distributions, Vol-
ume 1, 2nd edn. John Wiley & Sons, Inc., New York.
Johnson NL, Kotz S and Balakrishnan N (1995). Continuous Univariate Distributions, Vol-
ume 2, 2nd edn. John Wiley & Sons, Inc., New York.
Johnson NL, Kotz S and Balakrishnan N (1997). Discrete Multivariate Distributions. John
Wiley & Sons, Inc., New York.
Johnson BA, Tateishia R and Hoana NT (2013). A hybrid pansharpening approach and mul-
tiscale object-based image analysis for mapping diseased pine and oak trees. International
Journal of Remote Sensing 34(20), 6969–6982.
Jolliffe IT (1972). Discarding variables in a principal component analysis. I. Artificial data.
Journal of the Royal Statistical Society, Series C (Applied Statistics) 21(2), 160–173.
Jolliffe IT (2002). Principal Component Analysis, 2nd edn. Springer-Verlag, New York.
Jones O, Maillardet R and Robinson A (2009). Introduction to Scientific Programming and
Simulation Using R. Chapman & Hall/CRC Press, Boca Raton, FL.
Jones MC, Marron JS and Sheather SJ (1996). A brief survey of bandwidth selection for
density estimation. Journal of the American Statistical Association 91(433), 401–407.
Jöreskog KG (1969). A general approach to confirmatory maximum likelihood factor analysis.
Psychometrika 34(2), 183–202.
Jørgensen B (2012). Generalized linear models. In Encyclopedia of Environmetrics, Volume
3, 2nd edn. (eds. El-Shaarawi AH and Piegorsch WW), pp. 1152–1159. John Wiley & Sons,
Ltd, Chichester.
Kadane JB and Lamberth J (2009). Are blacks egregious speeding violators at extraordinary
rates in New Jersey? Law, Probability & Risk 8(2), 139–152.
Kaiser HF (1958). The varimax criterion for analytic rotation in factor analysis. Psychometrika
23(3), 187–200.

REFERENCES
443
Kaiser HF (1970). A second generation little jiffy. Psychometrika 35(4), 401–415.
Kaiser HF and Rice J (1974). Little Jiffy, Mark IV. Educational and Psychological Measure-
ment 34(1), 111–117.
Kang SH and Ahn CW (2008). Tests for the homogeneity of two binomial proportions in
extremely unbalanced 2 × 2 contingency tables. Statistics in Medicine 27(14), 2524–2535.
Kantardzic M (2003). Data Mining: Concepts, Models, Methods, and Algorithms. IEEE Press,
Piscataway, NJ.
Karatzoglou A, Meyer D and Hornik K (2006). Support vector machines in R. Journal of
Statistical Software 15(9), 1–28.
Kaufman L and Rousseeuw PJ (1990). Finding Groups in Data. An Introduction to Cluster
Analysis. John Wiley & Sons, Inc., Hoboken, NJ.
Khuri AI (2003). Advanced Calculus with Applications in Statistics, 2nd edn. John Wiley &
Sons, Inc., Hoboken, NJ.
Koepke H and Clarke B (2013). A Bayesian criterion for cluster stability. Statistical Analysis
and Data Mining 6(4), 346–374.
Kohavi R (1996). Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid.
In Proceedings of the 2nd International Conference on Knowledge Discovery and Data
Mining (eds. Simoudis E, Han J and Fayyad U), pp. 202–207. American Association for
Artificial Intelligence Press, Menlo Park, CA.
Kolenikov S and Bollen KA (2012). Testing negative error variances: Is a Heywood case a
symptom of misspecification? Sociological Methods & Research 41(1), 124–167.
Kotz S, Balakrishnan N and Johnson NL (2000). Continuous Multivariate Distributions, Mod-
els and Applications, Volume 1, 2nd edn. John Wiley & Sons, New York.
Kruppa J, Liu Y, Biau G, Kohler M, Knig IR, Malley JD and Ziegler A (2014). Probability
estimation with machine learning methods for dichotomous and multicategory outcome:
theory. Biometrical Journal 56(4), 534–563.
Kuss O (2013). The danger of dichotomizing continuous variables: A visualization. Teaching
Statistics 35(2), 78–79.
Kutner MH, Nachtsheim CJ, Neter J and Li W (2005). Applied Linear Statistical Models, 5th
edn. McGraw-Hill-Irwin, Boston, MA.
Kvam PH (2011). Comparing Hall of Fame baseball players using most valuable player ranks.
Journal of Quantitative Analysis in Sports 7(3), Article No. 19.
Kvam PH and Vidakovic B (2007). Nonparametric Statistics with Applications to Science and
Engineering. John Wiley & Sons, Inc., Hoboken, NJ.
Lance GN and Williams WT (1967). A general theory of classificatory sorting strategies. 1.
Hierarchical systems. Computer Journal 9(4), 373–380.
Lange K (2013). Optimization, 2nd edn. Springer, New York.
Lawley DN (1959). Tests of significance in canonical analysis. Biometrika 46(1 and 2), 59–66.
Lawley DN and Maxwell AE (1962). Factor analysis as a statistical method. Journal of the
Royal Statistical Society, Series D (The Statistician) 12(3), 209–229.
Leemis LM (1986). Relationships among common univariate distributions. American Statis-
tician 40(2), 143–146.
Lehmann EL and Casella G (1998). Theory of Point Estimation, 2nd edn. Springer-Verlag,
New York.

444
REFERENCES
Levy KJ and Narula SC (1974). Shortest confidence intervals for the ratio of two normal
variances. Canadian Journal of Statistics 2(1), 83–87.
Lewis M (2003). Moneyball. The Art of Winning an Unfair Game. W.W. Norton & Company,
New York.
Ley E (1996). On the peculiar distribution of the U.S. stock indexes’ digits. American Statis-
tician 50(4), 311–313.
Liao JG, Wu Y and Lin Y (2010). Improving Sheather and Jones’ bandwidth selector for
difficult densities in kernel density estimation. Journal of Nonparametric Statistics 22(1-2),
105–114.
Likert R (1932). A technique for the measurement of attitudes. Archives of Psychology no.
140, 1–55.
Lin CP and Bhattacherjee A (2010). Extending technology usage models to interactive hedo-
nic technologies: a theoretical model and empirical test. Information Systems Journal 20(2),
163–181.
Lindsay B and Liu J (2009). Model assessment tools for a model false world. Statistical Sci-
ence 24(3), 303–318.
Linoff GS and Berry MJ (2011). Data Mining Techniques: For Marketing, Sales, and Cus-
tomer Relationship Management 3rd edn. Wiley Publishing, Inc., Indianapolis, IN.
Liu W (2010). Simultaneous Inference in Regression, Monographs on Statistics and Applied
Probability, Volume 118. Chapman & Hall/CRC, Boca Raton, FL.
Lloyd SP (1957). Quantization for least mean squares error (abstract). Annals of Mathematical
Statistics 28(4), 1059.
Lloyd SP (1982). Least squares quantization in PCM. IEEE Transactions on Information The-
ory IT-28(2), 129–137.
Loh WY (2010). Tree-structured classifiers. Wiley Interdisciplinary Reviews: Computational
Statistics 2(3), 364–369.
Lohr SL (2010). Sampling: Design and Analysis, 2nd edn. Duxbury Press, Pacific Grove, CA.
Lukas MA (2012). Regularization methods. In Encyclopedia of Environmetrics, Volume 5,
2nd edn. (eds. El-Shaarawi AH and Piegorsch WW), pp. 2181–2185. John Wiley & Sons,
Ltd, Chichester.
Lydersen S, Fagerland MW and Laake P (2009). Recommended tests for association in 2 × 2
tables. Statistics in Medicine 28(7), 1159–1175.
MacQueen J (1967). Some methods for classification and analysis of multivariate observa-
tions. In Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Prob-
ability, Volume I (eds. Le Cam LM and Neyman J), pp. 281–297. University of California
Press, Berkeley, CA.
Mallows CL (1973). Some comments on CP. Technometrics 15(4), 661–675.
Mammone A, Turchi M and Cristianini N (2009). Support vector machines. Wiley Interdisci-
plinary Reviews: Computational Statistics 1(3), 283–289.
Mansouri K, Ringsted T, Ballabio D, Todeschini R and Consonni V (2013). Quantitative
structure-activity relationship models for ready biodegradability of chemicals. Journal of
Chemical Information and Modeling 53(4), 867–878.
Margolin BH, Resnick MA, Rimpo JY, Archer P, Galloway SM, Bloom AD and Zeiger E
(1986). Statistical analyses for in vitro cytogenetic assays using Chinese hamster ovary
cells. Environmental Mutagenesis 8(2), 183–204.

REFERENCES
445
Martinez AR (2010). Natural language processing. Wiley Interdisciplinary Reviews: Compu-
tational Statistics 2(3), 352–357.
McCullagh P and Nelder JA (1989). Generalized Linear Models, 2nd edn. Chapman & Hall,
London.
McDonald L (2014). Florence Nightingale, statistics and the Crimean War. Journal of the
Royal Statistical Society, Series A (Statistics in Society) 177(3), 569–586.
McGill R, Tukey JW and Larsen WA (1978). Variations of box plots. American Statistician
32(1), 12–16.
Michie D, Spiegelhalter D and Taylor, CC (ed.) (1994). Machine Learning. Neural and Sta-
tistical Classification. Ellis Horwood, Upper Saddle River, NJ.
Moguerza JM and Muñoz A (2006). Support vector machines with applications. Statistical
Science 21(3), 322–336.
Moore DS (2010). The Basic Practice of Statistics, 5th edn. W.H. Freeman & Co., New York.
Moro S, Laureano R and Cortez P (2011). Using data mining for bank direct marketing: an
application of the CRISP-DM methodology. In Proceedings of the European Simulation
and Modelling Conference – ESM ’2011 (eds. Novais P, Machado J, Analide C and Abelha
A), pp. 117–121. EUROSIS, Guimaräes, Portugal.
Motoda H and Ohara K (2009). Apriori. In The Top Ten Algorithms in Data Mining (eds. Wu
X and Kumar V), pp. 61–92. Chapman & Hall/CRC, Boca Raton, FL.
Murrell P (2011). R Graphics, 2nd edn. Chapman & Hall/CRC Press, Boca Raton, FL.
Myatt GJ (2007). Making Sense of Data. A Practical Guide to Exploratory Data Analysis and
Data Mining. John Wiley & Sons, Inc., Hoboken, NJ.
Myatt GJ and Johnson WP (2009). Making Sense of Data II: A Practical Guide to Data Visu-
alization, Advanced Data Mining Methods, and Applications. John Wiley & Sons, Inc.,
Hoboken, NJ.
Myers R, Montgomery D, Vining G and Robinson T (2012). Generalized Linear Models with
Applications in Engineering and the Sciences. John Wiley & Sons, Inc., Hoboken, NJ.
Narula SC (1979). Orthogonal polynomial regression. International Statistical Review 47(1),
31–36.
Nath B, Bhattacharyya DK and Ghosh A (2013). Incremental association rule mining: a
survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 3(3),
157–169.
Nelder JA and Wedderburn RWM (1972). Generalized linear models. Journal of the Royal
Statistical Society, Series A (General) 135, 370–384.
Neuhaus JO and Wrigley C (1954). The Quartimax method. An analytic approach to orthog-
onal simple structure. British Journal of Statistical Psychology 7(2), 81–91.
Nguyen DV, Arpat AB, Wang N and Carroll RJ (2002). DNA microarray experiments: bio-
logical and technological aspects. Biometrics 58(4), 701–717.
Nightingale F (1858). Notes on Matters Affecting the Health, Efficiency, and Hospital
Administration of the British Army. Harrison & Sons, London.
Obenchain RL (1977). Classical F-tests and confidence regions for ridge regression. Techno-
metrics 19(4), 429–439.
O’Fallon BD, Wooderchak-Donahue W and Crockett DK (2013). A support vector machine
for identification of single-nucleotide polymorphisms from next-generation sequencing
data. Bioinformatics 29(11), 1361–1366.

446
REFERENCES
Palmeri C (1997). Believe in yourself, believe in the merchandise. Forbes 160(5), 118–124.
Parkhomenko E, Tritchler D and Beyene J (2009). Sparse canonical correlation analysis with
application to genomic data integration. Statistical Applications in Genetics and Molecular
Biology 8(1), Article No. 1.
Parzen E (1962). On estimation of a probability density function and mode. Annals of
Mathematical Statistics 33(3), 1065–1076.
Pearson K (1896). Contributions to the mathematical theory of evolution. – III. Regression,
heredity, and panmixia. Philosophical Transactions of the Royal Society of London, Series
A: Containing Papers of a Mathematical or Physical Character 187, 253–318.
Pearson K (1900). On the criterion that a given system of deviations from the probable in the
case of a correlated system of variables is such that it can be reasonably supposed to have
arisen from random sampling. The London, Edinburgh and Dublin Philosophical Magazine
and Journal of Science, 5th series 50(302), 157–175.
Pearson K (1901). On lines and planes of closest fit to systems of points in space. The London,
Edinburgh and Dublin Philosophical Magazine and Journal of Science, 6th series 2(11),
559–572.
Pepe MS (2003). The Statistical Evaluation of Medical Tests for Classification and Prediction.
Oxford University Press, Oxford.
Phillips LD ((2005).). Bayesian belief networks. In Encyclopedia of Statistics in Behavioral
Science, Volume 1 (eds. Everitt BS and Howell D), pp. 130–134. John Wiley & Sons, Ltd,
Chichester.
Piegorsch WW and Bailer AJ (1997). Statistics for Environmental Biology and Toxicology.
Chapman & Hall/CRC Press, Boca Raton, FL.
Piegorsch WW and Bailer AJ (2005). Analyzing Environmental Data. John Wiley & Sons,
Ltd, Chichester.
Piegorsch WW, Cutter SL and Hardisty F (2007). Benchmark analysis for quantifying urban
vulnerability to terrorist incidents. Risk Analysis 27(6), 1411–1425.
Pierchala CE and Surti J (2009). Control charts as a tool for data quality control. Journal of
Official Statistics 25(2), 167–191.
Piette J and Jensen ST (2012). Estimating fielding ability in baseball players over time. Journal
of Quantitative Analysis in Sports 8(3), Article No. 1463.
Pimentel MA, Clifton DA, Clifton L and Tarassenko L (2014). A review of novelty detection.
Signal Processing 99, 215–249.
Playfair W (1786). The Commercial and Political Atlas: Representing, by Means of Stained
Copper-Plate Charts, the Exports, Imports, and General Trade of England, at a Single
View. To Which are Added, Charts of the Revenue and Debts of Ireland, Done in the Same
Manner by James Correy, 1st edn. Debrett, Robinson, and Sewell, London.
Potscher BM (1991). Effects of model selection on inference. Econometric Theory 7(2),
163–185.
Press SJ and Wilson S (1978). Choosing between logistic regression and discriminant analysis.
Journal of the American Statistical Association 73(364), 699–705.
Przyborowski J and Wilenski H (1935). Statistical principles of routine work in testing clover
seed for dodder. Biometrika 27(3/4), 273–292.
Quinlan JR (1996). Improved use of continuous attributes in c4.5. Journal of Artificial Intel-
ligence Research 4, 77–90.

REFERENCES
447
Rao SS (1998). Birth of a legend. Forbes 161(7), 128–130.
R Core Team (2014). R: A Language and Environment for Statistical Computing. R Founda-
tion for Statistical Computing, Vienna, Austria.
Rizzo ML (2008). Statistical Computing with R. Chapman & Hall/CRC Press, Boca Raton,
FL.
Rosenblatt M (1956). Remarks on some nonparametric estimates of a density function. Annals
of Mathematical Statistics 27(3), 832–837.
Rowley J (2007). The wisdom hierarchy: representations of the DIKW hierarchy. Journal of
Information Science 33(2), 163–180.
Roy V and Kaiser MS (2013). Posterior propriety for Bayesian binomial regression models
with a parametric family of link functions. Statistical Methodology 13, 25–41.
Royston JP (1982). An extension of Shapiro and Wilk’s W test for normality to large samples.
Journal of the Royal Statistical Society, Series C (Applied Statistics) 31(2), 115–124.
Salcedo-Sanz S, Rojo-Álvarez JL, Martínez-Ramón M and Camps-Valls G (2014). Support
vector machines in engineering: an overview. Wiley Interdisciplinary Reviews: Data Mining
and Knowledge Discovery 4(3), 234–267.
Salzberg S (1988). Exemplar-based learning: theory and implementation. Technical Report
TR-10-88, Center for Research in Computing Technology, Aiken Computation Laboratory,
Harvard University, Cambridge, MA.
Satterthwaite FE (1946). An approximate distribution of estimates of variance components.
Biometrics 2(6), 110–114.
Schaefer RL (1986). Alternative estimators in logistic regression when the data are collinear.
Journal of Statistical Computation and Simulation 25(1-2), 75–91.
Schapire RE (1989). The strength of weak learnability. In 30th Annual Symposium on Foun-
dations of Computer Science (FOCS), Oct. 30 – Nov. 1, 1989, Research Triangle Park, NC,
pp. 28–33. IEEE Computer Society Press, Los Alamitos, CA.
Schay G (2012). A Concise Introduction to Linear Algebra. Birkhäuser, Boston, MA.
Scheffé H (1953). A method for judging all contrasts in the analysis of variance. Biometrika
40(1/2), 87–104.
Schoenberg FP (2012). Tessellations. In Encyclopedia of Environmetrics, Volume 6, 2nd
edn. (eds. El-Shaarawi AH and Piegorsch WW), pp. 2707–2709. John Wiley & Sons, Ltd,
Chichester.
Schölkopf B and Smola AJ (2002). Learning with Kernels: Support Vector Machines, Regu-
larization, Optimization, and Beyond. MIT Press, Cambridge, MA.
Schwager SJ (1984). Bonferroni sometimes loses. American Statistician 38(3), 192–197.
Schwarz G (1978). Estimating the dimension of a model. Annals of Statistics 6(2), 461–464.
Scott DW (1979). On optimal and data-based histograms. Biometrika 66(3), 605–610.
Scott DW (1992). Multivariate Density Estimation. Theory, Practice, and Visualization. John
Wiley & Sons, Inc., New York.
Seeger P (1968). A note on a method for the analysis of significance en masse. Technometrics
10(3), 586–593.
Seewald AK (2007). An evaluation of Naive Bayes variants in content-based learning for spam
filtering. Intelligent Data Analysis 11(5), 497–524.
Shannon CE (1948). A mathematical theory of communication. Bell System Technical Journal
27(3,4), 379–423, 623–656.

448
REFERENCES
Shapiro SS and Wilk MB (1965). An analysis of variance test for normality: complete samples.
Biometrika 52(3/4), 591–611.
Shi Y, Zhang JH, Jiang M, Zhu LH, Tan HQ and Lu B (2010). Synergistic genotoxicity caused
by low concentration of titanium dioxide nanoparticles and p,p′-DDT in human hepato-
cytes. Environmental and Molecular Mutagenesis 51(3), 192–204.
Silvapulle MJ (1981). On the existence of maximum likelihood estimators for the binomial
response models. Journal of the Royal Statistical Society, Series B (Methodological) 43(3),
310–313.
Silver N (2012). The Signal and the Noise: Why So Many Predictions Fail – But Some Don’t.
Penguin Press, New York.
Silverman BW (1986). Density Estimation for Statistics and Data Analysis. Chapman & Hall,
London.
Simes RJ (1986). An improved Bonferroni procedure for multiple tests of significance.
Biometrika 73(3), 751–754.
Skillicorn D (2007). Understanding Complex Datasets: Data Mining with Matrix Decompo-
sitions. Chapman & Hall/CRC, Boca Raton, FL.
Small CG (1990). A survey of multidimensional medians. International Statistical Review
58(3), 263–277.
Smith HF (1936). The problem of comparing the results of two experiments with unequal
errors. Journal of the Council of Scientific and Industrial Research 9, 211–212.
Smith JW, Everhart JE, Dickson WC, Knowler WC and Johannes RS (1988). Using the ADAP
learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the 12th
Annual Symposium on Computer Applications and Medical Care (November 6–9, 1988,
Washington D.C.) (ed. Greenes RA), pp. 261–265. IEEE Computer Society Press, Los
Alamitos, CA.
Spearman C (1904a). “General intelligence,” objectively determined and measured. American
Journal of Psychology 15(2), 201–292.
Spearman C (1904b). The proof and measurement of association between two things. Ameri-
can Journal of Psychology 15(1), 72–101.
Spiegelhalter DJ, Best NG, Carlin BP and van der Linde A (2002). Bayesian measures of
model complexity and fit (with discussion). Journal of the Royal Statistical Society, Series B
(Statistical Methodology) 64(4), 583–639.
Spouge JL (1994). Computation of the gamma, digamma, and trigamma functions. SIAM
Journal on Numerical Analysis 31(3), 931–944.
Stahl F and Jordanov I (2012). An overview of the use of neural networks for data mining tasks.
Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 2(3), 193–208.
Stewart J and Kennelly PJ (2010). Illuminated choropleth maps. Annals of the Association of
American Geographers 100(3), 513–534.
Stigler SM (1983). Who discovered Bayes theorem? American Statistician 37(4), 290–296.
Storey JD (2002). A direct approach to false discovery rates. Journal of the Royal Statistical
Society, Series B (Statistical Methodology) 64(3), 479–498.
Storey JD (2003). The positive false discovery rate: a Bayesian interpretation and the q-value.
Annals of Statistics 31(6), 2013–2035.
Storey JD (2011). False discovery rate. In International Encyclopedia of Statistical Science
(ed. Lovric M), pp. 504–508. Springer-Verlag, Berlin.

REFERENCES
449
Street WN, Mangasarian OL and Wolberg WH (1995). An inductive learning approach to
prognostic prediction. In Machine Learning. Proceedings of the 12th International Confer-
ence on Machine Learning, Tahoe City, CA, July 9–12, 1995 (eds. Prieditis A and Russell
SJ), pp. 522–530. Morgan Kaufmann, San Francisco, CA.
Stuart A and Ord JK (1994). Kendall’s Advanced Theory of Statistics, Volume 1. Distribution
Theory, 6th edn. Arnold, London.
Student (1908). The probable error of a mean. Biometrika 6(1), 1–25.
Sturges HA (1926). The choice of a class interval. Journal of the American Statistical Asso-
ciation 21(153), 65–66.
Sugar CA and James GM (2003). Finding the number of clusters in a dataset: an
information-theoretic approach. Journal of the American Statistical Association 98(463),
750–763.
Sundberg R (2012). Shrinkage regression. In Encyclopedia of Environmetrics, Volume 5, 2nd
edn. (eds. El-Shaarawi AH and Piegorsch WW), pp. 2450–2453. John Wiley & Sons, Ltd,
Chichester.
Suzuki N, Rubin D, Lidman C, Aldering G, Amanullah R, Barbary K, Barrientos L, Botyan-
szki J, Brodwin M, Connolly N, Dawson K, Dey A, Doi M, Donahue M, Deustua S,
Eisenhardt P, Ellingson E, Faccioli L, Fadeyev V, Fakhouri H, Fruchter A, Gilbank D,
Gladders M, Goldhaber G, Gonzalez A, Goobar A, Gude A, Hattori T, Hoekstra H, Hsiao
E, Huang X, Ihara Y, Jee M, Johnston D, Kashikawa N, Koester B, Konishi K, Kowalski
M, Linder E, Lubin L, Melbourne J, Meyers J, Morokuma T, Munshi F, Mullis C, Oda T,
Panagia N, Perlmutter S, Postman M, Pritchard T, Rhodes J, Ripoche P, Rosati P, Schlegel
D, Spadafora A, Stanford S, Stanishev V, Stern D, Strovink M, Takanashi N, Tokita K,
Wagner M, Wang L, Yasuda N, Yee H and The Supernova Cosmology Project (2012). The
Hubble space telescope cluster supernova survey. V. Improving the dark-energy constraints
above z > 1 and building an early-type-hosted supernova sample. Astrophysical Journal
746(1), Article No. 85.
Takeuchi K (1976). Distribution of informational statistics and a criterion of model fitting (in
Japanese). Suri-Kagaku (Mathematical Sciences) 153, 12–18.
Tarone RE (1986). Correcting tests for trend in proportions for skewness. Communications in
Statistics – Theory and Methods 15(2), 317–328.
Tarone RE and Gart JJ (1980). On the robustness of combined tests for trends in proportions.
Journal of the American Statistical Association 75(369), 110–116.
Tate RF and Klett GW (1959). Optimal confidence intervals for the variance of a normal
distribution. Journal of the American Statistical Association 54(287), 674–682.
Taussig LM, Wright AL, Holberg CJ, Halonen M, Morgan WJ and Martinez FD (2003).
Tucson Children’s Respiratory Study: 1980 to present. Journal of Allergy and Clinical
Immunology 111(4), 661–675.
Thelwall M, Haustein S, Larivière V and Sugimoto CR (2013). Do altmetrics work? Twitter
and ten other social web services. PLoS ONE 8(5), Article No. e64841.
Thompson SK (2012). Sampling, 3rd edn. John Wiley & Sons, Inc., Hoboken, NJ.
Thurstone LL (1931). Multiple factor analysis. Psychological Review 38(5), 406–427.
Thurstone LL (1947). Multiple-Factor Analysis: A Development and Expansion of “The Vec-
tors of the Mind”. University of Chicago Press, Chicago.
Tibshirani R (1996). Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society, Series B (Methodological) 58(1), 267–288.

450
REFERENCES
Tikhonov AN (1963). Solution of incorrectly formulated problems and regularization method.
Soviet Mathematics–Doklady 4, 1035–1038.
Timm NH (2002). Applied Multivariate Analysis. Springer-Verlag, New York.
Toraason M, Lynch DW, DeBord DG, Singh N, Krieg E, Butler MA, Toennis CA and
Nemhauser JB (2006). DNA damage in leukocytes of workers occupationally exposed to
1-bromopropane. Mutation Research 603(1), 1–14.
Tryon RC (1939). Cluster Analysis. Edwards Bros., Ann Arbor, MI.
Tsanas A and Xifara A (2012). Accurate quantitative estimation of energy performance of
residential buildings using statistical machine learning tools. Energy and Buildings 49,
560–567.
Tufféry S (2011). Data Mining and Statistics for Decision Making. John Wiley & Sons, Ltd,
Chichester.
Tufte E (2001). The Visual Display of Quantitative Information, 2nd edn. Graphics Press,
Cheshire, CT.
Tukey JW (1972). Some graphic and semigraphic displays. In Statistical Papers in Honor of
George W. Snedecor (ed. Bancroft TA), pp. 293–316. Iowa State University Press, Ames,
IA.
Tukey JW (1977). Exploratory Data Analysis. Addison-Wesley, Reading, MA.
Uhlmann ME, Georgieva M, Sill M, Linnemann U and Berger MR (2012). Prognostic value of
tumor progression-related gene expression in colorectal cancer patients. Journal of Cancer
Research and Clinical Oncology 138(10), 1631–1640.
Upadhyay R (2014). A Case Study from Banking (Part 3) – Logistic Regression. http://www.
bigdatanews.com/profiles/blogs/a-case-study-from-banking-part-3-logistic-regression-1.
Accessed 20 March 2014.
Væth M (1985). On the use of Wald’s test in exponential families. International Statistical
Review 53(2), 199–214.
Vapnik VN (1995). The Nature of Statistical Learning Theory. Springer-Verlag, New York.
Vapnik VN (1998). Statistical Learning Theory. John Wiley & Sons, Inc., New York.
Vapnik VN (2000). The Nature of Statistical Learning Theory, 2nd edn. Springer-Verlag, New
York.
Vasicek O (1976). A test for normality based on sample entropy. Journal of the Royal Statis-
tical Society, Series B (Methodological) 38(1), 54–59.
Venables WN and Ripley BD (2000). S Programming. Springer-Verlag, New York.
Venables WN and Ripley BD (2002). Modern Applied Statistics with S-Plus, 4th edn.
Springer-Verlag, New York.
Verzani J (2005). Using R for Introductory Statistics. Chapman & Hall/CRC, Boca Raton, FL.
Vidaurre D, Bielza C and Larrañaga P (2013). A survey of L1 regression. International Sta-
tistical Review 81(3), 361–387.
de Ville B (2013). Decision trees. Wiley Interdisciplinary Reviews: Computational Statistics
5(6), 448–455.
Vise A (2012). CCJ Top 250: getting stronger. For the largest trucking companies, revenue
growth outpaces capacity growth. Commercial Carrier Journal 169(8), 60–92.
Wald A (1943). Tests of statistical hypotheses concerning several parameters when the num-
ber of observations is large. Transactions of the American Mathematical Society 54(3),
426–482.

REFERENCES
451
Wehrens R and Buydens LMC (2007). Self- and super-organizing maps in R: The kohonen
package. Journal of Statistical Software 21(5), 1–19.
Wei LJ and Cowan CD (2006). Selection bias. In Encyclopedia of Statistical Sciences, Volume
11 (eds. Kotz S, Read CB, Balakrishnan N and Vidakovic B), pp. 7524–7526. John Wiley
& Sons, Inc., Hoboken, NJ.
Welch BL (1938). The significance of the difference between two means when the population
variances are unequal. Biometrika 29(3/4), 350–362.
Whittle P (1958). On the smoothing of probability density functions. Journal of the Royal
Statistical Society, Series B (Methodological) 20(2), 334–343.
Wickham H (2012). Statistical graphics. In Encyclopedia of Environmetrics, Volume 6, 2nd
edn. (eds. El-Shaarawi AH and Piegorsch WW), pp. 2649–2658. John Wiley & Sons, Ltd,
Chichester.
Wickham H (2013). Graphical criticism: Some historical notes. Journal of Computational and
Graphical Statistics 22(1), 38–44.
Wilkinson L (1999). Dot plots. American Statistician 53(3), 276–281.
Wilkinson L (2005). The Grammar of Graphics, 2nd edn. Springer, New York.
Wilks SS (1938). The large-sample distribution of the likelihood ratio for testing composite
hypotheses. Annals of Mathematical Statistics 9(1), 60–62.
Wilson EB (1927). Probable inference, the law of succession, and statistical inference. Journal
of the American Statistical Association 22(158), 209–212.
Wilson J (2012). Statistical computing with R: selecting the right tool for the job – R Com-
mander or something else? Wiley Interdisciplinary Reviews: Computational Statistics 4(6),
518–526.
Wilson PD and Tonascia J (1971). Tables for shortest confidence intervals on the standard
deviation and variance ratio from normal distributions. Journal of the American Statistical
Association 66(336), 909–912.
de Winter JCF and Dodou D (2012). Factor recovery by principal axis factoring and maximum
likelihood factor analysis as a function of factor pattern and sample size. Journal of Applied
Statistics 39(4), 695–710.
Witten DM and Tibshirani RJ (2009). Extensions of sparse canonical correlation analysis with
applications to genomic data. Statistical Applications in Genetics and Molecular Biology
8(1), Article No. 28.
Witten DM, Tibshirani RJ and Hastie T (2009). A penalized matrix decomposition, with appli-
cations to sparse principal components and canonical correlation analysis. Biostatistics
10(3), 515–534.
Wolberg WH and Mangasarian OL (1990). Multisurface method of pattern separation for
medical diagnosis applied to breast cytology. Proceedings of the National Academy of Sci-
ences of the United States of America 87(23), 9193–9196.
Wolf JR (1856). Mittheilungen über die Sonnenflecken? Vierteljahrsschrift der Naturforschen
Gesellschaft in Zurich 1, 151–161.
Wood SN (2006). Generalized Additive Models: An Introduction with R. Chapman &
Hall/CRC Press, Boca Raton, FL.
Working H and Hotelling H (1929). Applications of the theory of error to the interpretation
of trends. Journal of the American Statistical Association, Supplement: Proceedings of the
American Statistical Association 24, 73–85.

452
REFERENCES
Wu TT and Lange K (2008). Coordinate descent algorithms for lasso penalized regression.
Annals of Applied Statistics 2(1), 224–244.
Yates F (1948). The analysis of contingency tables based on quantitative characters.
Biometrika 35(1/2), 176–181.
Yau N (2011). Visualize This: The FlowingData Guide to Design, Visualization, and Statistics.
Wiley Publishing, Inc., Indianapolis, IN.
Yeh IC (1998). Modeling of strength of high performance concrete using artificial neural net-
works. Cement and Concrete Research 28(12), 1797–1808.
Young FW, Valero-Mora PM and Friendly M (2006). Visual Statistics: Seeing Data with
Dynamic Interactive Graphics. John Wiley & Sons, Inc., Hoboken, NJ.
Zabell SL (2008). On Student’s 1908 article “The probable error of a mean” (with discussion).
Journal of the American Statistical Association 103(481), 1–20.
Zhang HH, Ahn J, Lin X and Park C (2006). Gene selection using support vector machines
with non-convex penalty. Bioinformatics 22(1), 88–95.
Zhang S and Wu X (2011). Fundamentals of association rules in data mining and knowledge
discovery. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 1(2),
97–116.
Zou H and Hastie T (2005). Regularization and variable selection via the elastic net. Journal
of the Royal Statistical Society, Series B (Statistical Methodology) 67(2), 301–320 (corr.
67(5), 768).

Index
+ subscript, 26
68–95–99.7% rule, 57
A-B testing, 159, 287
accuracy
in association rule learning, 397
in classification analytics, 296
AdaBoost algorithm, 321
addition rule, see under probability, rules
adjusted R2, see under coefficient of
determination
agglomerative hierarchical clustering, see
under cluster analysis
Agresti-Coull confidence interval, 133
Akaike Information Criterion (AIC), see
under information criteria
alias, see under predictor variable
alternative hypothesis, 138
one-sided, 139
two-sided, 139
altmetrics, 157
analysis of deviance table, 264
analysis of variance, 242
multivariate, xv
one-factor, 242–243
two-factor, 244
ANOVA, see analysis of variance
Apriori algorithm, 399
artificial neural networks, xv
association rules
accuracy, 397
minimum threshold, 399
Statistical Data Analytics: Foundations for Data Mining, Informatics, and Knowledge Discovery,
First Edition. Walter W. Piegorsch.
© 2015 John Wiley & Sons, Ltd. Published 2015 by John Wiley & Sons, Ltd.
www.wiley.com/go/piegorsch/data_analytics
antecedent, 396
𝜒2 statistic, 402–403
consequent, 396
itemsets, 396
frequent, 399
sampling from, 402
lift, 398
support, 397
transaction database, 396
asymptotic distribution, 23
of estimator, 118
of sample mean, 35, 118
attribute section, see variable selection
backward elimination, see under variable
selection
bagging, 321, 322
balanced design, 242
bar chart, 83, 91–94
with error bars, 92–94
Bayes classification rule, 303
Bayes’ Rule, 12
for p.d.f.s, 16
Bayes, T., 12
Bayesian belief network, xv
Bayesian Information Criterion (BIC), see
under information criteria
Bernoulli trial, 24
best linear unbiased estimator, 166,
199
best subset selection, see under variable
selection

454
INDEX
bias
of estimator, 118
sampling, 50
bias-variance trade-off
in classification, 308, 314, 326
in ridge regression, 232, 233
Big Data, 3, 65, 386, 430
binning, 66–67
for histogram, 83
normal reference rule, 66
Sturges’ rule, 66
binomial coefficient, 23
binomial distribution, 23–26
closure under addition, 26, 119
entropy of, 47
in exponential family, 43
p.m.f., 23
biodiversity, 288
bioinformatics, 5, 333, 405
bivariate normal distribution, see normal
(Gaussian) distribution, bivariate
BLUE, see best linear unbiased estimator
body mass index, 342
Boltzman-Shannon entropy, see entropy
Bonferroni correction, see under
multiplicity adjustment
Bonferroni’s inequality, 149
boosting, 321
bootstrap interval, see confidence interval,
bootstrap
bootstrap resampling, 137
box-and-whisker plot, 80
boxplot, 57, 79–81
multiple, 95–96
notched, 79
whiskers in, 79
branch-and-bound algorithm, 216
bridge regression, 241
bubble plot, 99–101, 112
bull’s-eye cluster, 326, 394
c.d.f., see cumulative distribution function
canonical correlation analysis, 361–363
adjusted canonical correlations, 363
Bartlett statistic, 362
bias correction, 363
canonical variates, 361
eigendecomposition, 362
regularized, 365
canonical correlation coefficient, 362
CART, see classification and regression
trees
case deletion, 178, 183, 214, 233, 239
Cayley multiplication, see matrix
multiplication
Cayley, A., 413
CCA, see canonical correlation analysis
Central Limit Theorem, 35–36
central tendency, 53
centroid, 378
characteristic polynomial, see under
matrix
characteristic value, see eigenvalue
𝜒2 distribution, 32
closure under addition, 32
dervied from normal, 37
upper-𝛼critical point, 37, 127
𝜒2 statistic, 263, 275
for 2 × 2 table, 277, 402
choropleth map, 104
classification analytics, 292–328
accuracy, 296
Bayesian, 302–303
naive, 308
confusion matrix, 296, 336
decision trees, 312–315
k-nearest neighbor, 308–309
tuning parameter for, 308
logistic regression in, 292
misclassification error, 296, 325
nonparametric, 308, 312
nonseparable data, 325
slack variable, 325
soft margin hyperplane, 325
sensitivity, 296
separable data, 322
classification rule, 323
margin between, 322
specificity, 296
support vector machines, 328
support vector methods, 322
tree-based methods, 312, 320
classification and regression trees, 321
Clausius, R., 18

INDEX
455
CLT, see Central Limit Theorem
cluster analysis, 373–394
agglomerative hierarchical, 376–379
average-link, 377, 378
centroid link, 378
chaining problem, 377
complete-link, 377, 378
dendrogram, 379
single-link, 377, 378
between-cluster variation, 384,
390
dissimilarity matrix, 375
distance metric, 376, 384
divisive hierarchical, 377, 384
exploratory, 389–390
model-based, 394
partitioned, 384–385
elbow plot, 389
K-means, 385–386
K-medians, 394
K-medoids, 394
pseudo-F plot, 390
pseudo-F statistic, 390
similarity matrix, 375
within-cluster sum of squares, 385,
386, 391
Cochran-Armitage trend test
for counts, 288
for proportions, 271–272
generalized, 289
Tarone skewness adjustment, 272
coefficient of determination (R2), 170
adjusted, 214
multiple, 200
coefficient of variation, 29, 281
collinearity, see multicollinearity
confidence band, 173–175
confidence bound
one-sided, 124
confidence coefficient, see confidence
level
confidence ellipse
for regression coefficients, 171–172,
201
Wald form, 149
confidence interval, 123
bootstrap, 138
percentile method, 138
for binomial probability, 133–134,
148
Agresti-Coull, 133
Wilson type, 134
with continuity correction, 133
for classification probability, 293
for correlation coefficient, 189
for difference in normal means
with equal variances, 130
with paired samples, 130
with unequal variances, 128–129
for mean response
joint, 202
multiple linear, 202
simple linear, 170
for normal mean, 124–127
for normal mean and variance
( joint), 150–151
for normal variance, 127
optimal, 127, 155
ratio of, 131
for Poisson mean, 135
for ratio, 136–137, 158
for regression coefficient
joint, 201
multiple linear, 201
simple linear, 168, 170
frequentist interpretation, 124
from Delta method, 136
from pivotal quantity, 127
likelihood ratio, 135
profile, 265
tautology, 139
Wald, 131–132
in generalized linear model, 264
confidence level, 123
simultaneous, 149
confidence rectangle
for regression coefficients, 171
confidence surface, 202
confidence vs. probability, 124
confusion matrix, see under classification
analytics
consecutive-dose average spacing, 267
contingency table, 274
2 × 2, 277

456
INDEX
contingency table (continued)
for association rules, 402
𝜒2 statistic, 275
implementaion rules, 276
sparse, 276
standardized residuals
from, 277
continuous data, 52
contour plot, 229
control group, 50
convenience sampling, see under sampling
convergence
in distribution, 23
in probability, 23
of sample mean, see under asymptotic
distribution
of transformed variable, see Delta
method
convex optimization, 324
coordinate descent algorithm, 239
correlation coefficient
as measure of association, 21, 59, 187
population, 20–21, 188
sample, 59, 188
computing formulae, 59
correlation matrix, 104, 420
for standardized variates, 420
covariance
population, 20
of independent variables, 45
sample, 59
covariance matrix, 21–22
for bivariate normal, 36
of estimator, 117
of matrix transformation, 199
sample, 419
coxcomb plot, see polar area plot
Cp statistic, 214
critical point
𝜒2, 37, 127
F, 40
standard normal, 33, 124
t, 39, 125
critical region, see rejection region
cross-validation, 233, 329
generalized, 233
K-fold, 239, 240
L-fold, 315
leave-one-out, 239, 311
cumulative distribution function, 13
joint, 16
curse of dimensionality, 309, 329, 394
CV, see cross-validation
cv, see coefficient of variatione
data analysis, 52
exploratory, 7
inferential, 115
data analytics, xiii, 4
data mining, 5
data quality, 5
disortion of, 6–7
data transformation, see transformation
data, types of
continuous, 52
discrete, 52
interval, 52
nominal, 51
ordinal, 51
qualitative, 51
quantitative, 51
ratio, 52
decile, 19
decision boundary, 296, 303, 304, 322, 332
decision trees, 312–315
branches and leaves, 312
cost-complexity pruning, 314
entropy impurity, 313, 337
Gini impurity, 312, 313, 337
ID3/C4.5/C5.0 algorithms, 314
misclassification impurity, 313, 314,
337
one-standard error rule, 315
pruning, 314–315
weakest-link, 315
splitting criteria, 312
tuning parameter for, 314
degrees of freedom, 32, 37, 38
effective, 233
error, 167, 200
F ratio, 40
Delta method, 36
multivariate, 136
dendrogram, 312, 379

INDEX
457
as heatmap add-on, 104
density estimation, see kernel density
estimator
density function, see probability density
function
design matrix, 199
deviance function, 261
as impurity measure, 314
as residual sum of squares, 261
𝜒2 approximation, 262
diagnostics, see regression diagnostics
diagonal matrix, see under matrix
DIKW pyramid, 4
dimension reduction, 342
via canonical correlation analysis, 361
via exploratory factor analysis, 351
via principal components, 345–346
Dirichlet tessellation, see Voronoi
tessellation
discrepancy measure
for hypothesis testing, 202–203
via deviance function, 261–262
discrete data, 52
discriminant analysis, 292
Bayesian, 302–303
Gaussian, 303–304
linear, 297–299
logistic, 292
quadratic, 307
scores in, 292
sparse, 302
discriminant function
linear, 293, 299
discrimination, see discriminant analysis
disjoint events, see under events
dispersion
as entropy, 18, 62
parameter, 41, 259
dissimilarity measure, 374
distance metric, 374–375
Canberra, 309, 376
Euclidean, 309, 376
Hamming, 309, 376
Mahalanobis, 386
Manhattan, 309, 376
maximum, 309, 376
Tchebychev, 309, 376
distribution, 13
Bernoulli, 24
binomial, 23–26
𝜒2, 32
exponential, 29–30
exponential family, 41–43
F, 40–41
gamma, 30–32
geometric, 27
hypergeometric, 278
multinomial, 278, 314
negative binomial, 28
normal (Gaussian), 32–35
bivariate, 36
multivariate, 37, 303, 336
Pareto, 48
Poisson, 26–27
t, 38–40
uniform
continuous, 29
discrete, 28
univariate, 23
dot plot
Cleveland-type, 78
Wilkinson-type, 77–79
dot product, 413
ecoinformatics, 5
EDA, see exploratory data analysis
EFA, see exploratory factor analysis
eigenanalysis, 415–416
eigendecomposition, see under matrix
eigenvalue, 416
and singular matrix, 416
eigenvector, 416
unit, 416
elastic net regularization, see under
regularization
ensemble learning, 321
entropy, 18
in decision trees, 313
sample, 62–63
error bars, see under bar chart
error inflation, 149
error sum of squares, see residual sum of
squares
estimability constraint

458
INDEX
estimability constraint (continued)
corner-point
one-factor, 243
two-factor, 244, 275
zero-sum
one-factor, 243
two-factor, 244, 275
estimating equation, 119, 260
estimator
asymptotically unbiased, 118
best linear unbiased, 166, 199
biased, 118
interval, 123
least squares, 119
maximum likelihood, 120
moment (method of), 119
nonparametric, 63
point, 117
unbiased, 118
Euclidean distance, see under distance
metric
events, 10
complementary, 11
disjoint, 11
independent, 12
mutually exclusive, 11
EWMA, see smoothing, exponential
weighted average
exact test
for 2 × 2 table, 278–279
for binomial probability, 147
for R × C table, 279
expectation operator, see expected value
expected value, 17
as linear operator, 18
as weighted average, 17
bivariate, 20
conditional, 20
marginal, 20
of linear combination, 22
population mean, 17
experimentwise error, see false positive
error, familywise
exploratory data analysis, 151, 292
exploratory factor analysis, 351–358
Bartlett test for sphericity, 357
communality, 352
eigendecomposition, 353
factor scores, 360
factor selection
Kaiser criterion, 355
likelihood ratio test, 354–355
percent explainable variance, 355
Kaiser-Meyer-Olkin measure, 357
latent factor in, 351
loading coefficient, 351, 352
manifest variable, 351
maximum likelihood estimation
for, 354
principal factor estimation, 353–354
rotation
oblique, 356
promax, 356
quartimax, 356
varimax, 356
specific variances, 352
uniquenesses, 352
exponential distribution, 29–30
c.d.f., 29
coefficient of variation, 29
entropy of, 47
in exponential family, 48
memoryless property, 30
p.d.f., 29
survival function, 30
exponential family, 41–43, 258
dispersion parameter, 41, 259
mean relationship, 42
natural parameter, 41, 259
binomial, 43, 266
normal (Gaussian), 42
Poisson, 273
probability function, 41, 259
extended, 42
scale parameter, 41, 259
variance function, 42, 259
extrapolation, 175
extreme values, 79
F distribution, 40–41
dervied from normal, 40
relation to 𝜒2, 41
relation to t, 41
upper-𝛼critical point, 40

INDEX
459
F ratio, 40
F-test
for ANOVA main effect, 243
for regression coefficient, 203
polynomial, 211
in analysis of deviance, 265
Facebook, 157, 286
factor analysis
confirmatory, 361
exploratory, 351
factorial operator, 23
false discovery rate, 152–153
marginal, 154
positive, 154
false negative error, 139, 296
false positive error, 139, 296
familywise, 149
vs. false discovery rate, 152
pointwise, 149
FDR, see false discovery rate
feature assessment, 152
feature extraction, 346, 352
feature selection, 211, 302
feature variable, 164, 291
centered, 341
scaled, 309
standardized, 344
fences (inner), 61
in boxplot, 79
fences (outer), 61
FET, see Fisher Exact Test
Fieller’s theorem, 137
Fisher Exact Test, 279
with unbalanced design, 279
Fisher information matrix, 116–117, 123,
149
expected, 117
Fisher information number, 116
for binomial sample, 132, 154
Fisher Z transform
for correlation coefficient, 189
Fisher, R.A., 116
Fisher-Irwin test, see Fisher exact test
fitted values
in multiple linear regression, 199–200
in simple linear regression, 164
five-number summary, 57, 79, 95
forecasting, 70
forward stepwise regression, see under
variable selection
fraud detection, 6
frequency polygon, 95
FWER, see false positive error, familywise
gamma distribution, 30–32
closure under addition, 31
in exponential family, 281, 289
p.d.f., 30, 281
alternative form, 281, 289
gamma function, 28
recursive relationship, 28
Gauss–Markov theorem, 166, 199
Gaussian distribution, see normal
distribution
Gaussian radial basis function, see under
kernel function
GCV, see cross-validation, generalized
gene expression data, 153, 291, 333, 371,
380, 405
generalized additive model, xv
generalized linear model, 258
analysis of deviance, 264–265
with unknown dispersion, 265
deviance function, 261
scaled, 261
dispersion parameter
𝜒2-based estimator, 263, 281
deviance-based estimator, 262
for simple linear regression, 283
gamma, 281
inverse link, 259
link function, 259
complementary log-log, 266
diagnostic rule-of-thumb, 266
identity, 259
logarithmic, 259, 274, 281
logit, 266
probit, 266
log-linear, 274, 280, 281
logistic, 266
residual
deviance, 263
standardized, 263
variance function, 259, 263

460
INDEX
genome-wide association studies, 152
geoinformatics, 5, 395
geometric distribution, 27
c.d.f., 27
memoryless property, 46
p.m.f., 27
geometric series (finite), 27, 70
GIGO principle, 5
GLiM, see generalized linear model
Gosset, W.S., 38
greedy search algorithm, 312, 386
Ho, see null hypothesis
Ha, see alternative hypothesis
hat matrix, 200
diagonal elements, 181, 200
in generalized linear model, 263
in ridge regression, 232
trace of, 200
heatmap, 103–104
for correlation matrix, 104, 364
temporal, 113
Hessian matrix, 116
Heywood cases, 354
high-dimensional data, 270, 302, 341, 386
hinges, 57
histogram, 83–85
back-to-back, 87
bin width, 83
class intervals, 83
normal reference rule for, 83
Sturges’ rule for, 83
Hotelling transformation, see under
principal component analysis
hyperbolic tangent, 190
hypergeometric distribution, 278
hyperplane, 304
hypothesis test, 138–140
for ANOVA interaction, 244
for ANOVA main effect, 243, 244
sequential, 245
for binomial probability, 147–148
for canonical correlations, 362
for correlation coefficient, 188
rank-based, 190–191
for difference in normal means
with equal variances, 144
with paired data, 144–145
with unequal variances, 142–143
for normal mean, 140–141
for normal variance, 142
for ratio of normal variances, 145
for regression coefficient
joint, 202–203
multiple linear, 202
partial, 202
polynomial, 211
simple linear, 168
likelihood ratio, 146–147
in generalized linear
model, 264–265
instablity with contingency
tables, 275
via deviance difference, 262
one-sided, 139
P-value for, 139
power of, 139
rejection region, 139
sensitivity, 139
significance level, 139
tautology, 139
two-sided, 139
Wald, 145–146
in generalized linear model,
264
instability with binomial
data, 273
with contingency tables, 275–276
i.i.d., 35, 49
identity matrix, see under matrix
Idiot’s Bayes classification, see naïve Bayes
classification
imputation, see under missing data
indicator function, 23
inference, statistical, 7, 115
infographics, 76
informatics, 5
medical, 5
information age, 3
information criteria, 215–216
Akaike (AIC), 215
corrected, 215
Bayesian (BIC), 215

INDEX
461
information matrix, see Fisher information
matrix
infovis, 76, 110
inner product, Euclidean, 413
input variable, see predictor variable or
feature variable
interquartile range
population, 19
sample, 57
inverse hyperbolic tangent, 189
inverse matrix, see under matrix
IQR, see interquartile range
itemsets, see under association rules
IWLS, see least squares, weighted,
iterative
K-means clustering, see under cluster
analysis, partitioned
k-nearest neighbor classifier, see under
classification analytics
Kaiser criterion, see under principal
component analysis
Karhunen-Loéve transformation, see under
principal component analysis
KDD, see under knowledge discovery
kernel density estimator, 85–86
bandwidth, 85
as standard deviation, 86
optimal, 86
kernel function, 85
kernel function
bisquare, 225
Cauchy, 329
dimension-transforming, 328
Epanechnikov, 85
Gaussian, 85, 86, 111,
328
Laplace, 329
linear, 328
second-order, 328
sigmoid, 329
thin-plate spline, 329
triangular, 85
tricube, 224
tuning parameter for, 329
kernel transformation, see under support
vector methods
knowledge discovery, xiii, 3
in databases (KDD), 3
Lagrange multiplier, 231, 299, 344
Laplace radial basis function, see under
kernel function
Lasso, 238–239
as variable selector, 238
coefficient profile plot, 239–240
cross-validation plot, 240
tuning parameter for, 238
cross-validation, 238
selection of, 238
latent factor, see under exploratory factor
analysis
latent value, see eigenvalue
leaps and bounds algorithm, 216
learning with a teacher, see supervised
learning
learning without a teacher, see
unsupervised learning
least squares
method of, 119–120
penalized, 231
weighted, 120
for linear regression, 184–185,
200–201
for loess smoothing, 224
iterative, 260
least squares estimator
in multiple linear regression, 199
covariance matrix for, 200
sampling distribution, 199
standard error, 200
unbiased, 199
in simple linear regression, 164, 191
sampling distribution, 166, 192
standard error, 167
unbiased, 166, 192
weighted
in multiple linear regression,
201
in simple linear regression,
185
leverage, see under regression diagnostics
lift, see under association rules
likelihood function, 116

462
INDEX
likelihood ratio, 135
asymptotic distribution, 135
likelihood ratio test, see under hypothesis
test
Likert scale, 51
line graph, 94
linear combination
of random variables, 22
linear discriminant analysis, see under
discriminant analysis
linear discriminant function, see under
discriminant function
linear predictor
in generalized linear model, 259
in multiple linear regression, 198
one-factor, 242
two-factor, 244, 274
link function, see under generalized linear
model
locally weighted scatterplot smoothing, see
loess
loess, 224–225
bisquare kernel, 225
robust iteration, 225, 227–228
smoothing parameter for, 224
tricube kernel, 224
variance diagnostic, 253
with multiple predictors, 227–228
log-likelihood function, 116
log-linear model, see regression, log-linear
log-odds ratio, 266
logarithmic transformation, see under
transformation
logistic regression, see regression, logistic
logit, 43, 266
lowess, see loess
LR, see likelihood ratio
Mahalanobis distance, 386
Mallows’ Cp statistic, 214
matrix, 411
characteristic polynomial of, 416
conformable, 413
determinant, 414
diagonal, 412
diagonal factorization of, 417
eigendecomposition for, 417
full rank, 412
identity (I), 412
ill conditioned, 230
inverse, 414–415
nonnegative definite, 415
nonsingular, 414
orthogonal, 413
positive definite, 415
rank of, 412
singular, 414
singular values of, 418
square, 412
square root of, 417–418
symmetric, 412
trace of, 412
transposition of, 412
trapezoidal, 412
triangular, 412
matrix multiplication, 413
maximum likelihood, 120–123
invariance property, 123, 135
maximum likelihood estimator, 120–122
asymptotic distribution, 123
covariance matrix, 123
for binomial probability, 132
for normal mean, 121
for normal variance, 121
for Poisson mean, 154
in generalized linear models, 260
in linear regression, 164, 192, 199,
201
maximum, sample, 54, 79
mean
population, 17
sample, 35, 53
mean squared error
for regression model, 167, 200
in analysis of variance, 243
of estimator, 118
median
population, 19, 54
for discrete distribution, 19
Q2 notation for, 19
sample, 54–55, 79
robust to outliers, 72
medical informatics, see under informatics
method of moments, 118–119

INDEX
463
mFDR, see false discovery rate, marginal
microarray data, 153, 291, 322, 380
Million Song Dataset, 363
minimal simultaneous coverage, 149
minimum, sample, 54, 79
misclassification error
in decision trees, 313
in supervised classification, 296
missing data, xv, 6
imputation, 6
MLE, see maximum likelihood estimator
MLR, see regression, multiple linear
MOM, see method of moments
Monte Carlo method, 137
moving average, see under smoothing
MSE, see mean squared error
multicollinearity, 208–210, 342
in polynomial regression, 210
remedies for, 210
Lasso, 238
ridge regression, 232
variance inflation factor for, 209
multinomial distribution, 278, 314
multiple comparisons, see multiple
inferences
multiple inferences, 148–151
multiple linear regression, see regression,
multiple linear
multiplication rule, 11
for independent events, 12
for probability functions, 21
multiplicity adjustment, 149–153
Bonferroni, 149–151
in linear regression, 171, 203, 206,
247
for mean response, 173
P-values, 151
Simes’, 153
multiplicity correction, see multiplicity
adjustment
multivariate normal, see normal (Gaussian)
distribution, multivariate
natural parameter, see under exponential
family
naïve Bayes classification, 308
negative binomial distribution, 28
closure under addition, 28
in exponential family, 48
p.m.f., 28
alternative form, 28
nested model, 203
neural networks, xv
Nightingale, F., 94
no-effect hypothesis, see null hypothesis
no-free-lunch theorem, 150, 174, 203
and multiplicity adjustment, 151
noise-to-signal ratio, 65
nonsingular matrix, see under matrix
norm, 238
L2, 323, 329
normal (Gaussian) distribution, 32–35
bivariate, 36
conditional p.d.f.s, 197
p.d.f., 36
c.d.f. Φ(z), 32
closure under addition, 34
entropy of, 47
in exponential family, 42
interquartile range, 34
multivariate, 37
log-likelihood ratio, 303, 336
p.d.f., 37, 303
p.d.f., 32
quantiles, 33
standard, N(0, 1), 32
upper-𝛼critical point, 33, 124
normal equations
for multiple linear regression, 199
for simple linear regression, 164
weighted, 184
normal probability plot, see quantile plot,
normal
normal quantile plot, see quantile plot,
normal
normal reference rule, see under binning
notches, see under boxplot
novelty detection, xv
null hypothesis, 138
one-standard error rule, see under decision
trees
opportunity sampling, see under sampling
order statistics, 54

464
INDEX
orthogonal matrix, see under matrix
orthogonal polynomials, 210
outlier, 60–62
in boxplot, 79, 81
overdispersion, 288
overfitting, 216, 315, 326, 329, 355
oversmoothing, 66
P-value, 139
p.d.f., see probability density function
p.m.f., see probability mass function
parameter, 23, 49
parameter vector, 115
Pareto distribution, 48
in exponential family, 48
partial test, see under hypothesis test, for
regression coefficient
partition, see under sample space
Parzen-Rosenblatt-Whittle estimator, see
kernel density estimator
PCA, see principal component analysis
Pearson product-moment correlation, see
correlation coefficient
Pearson, K., 59
percentile, 19
percentile method, see under confidence
interval, bootstrap
pFDR, see false discovery rate, positive
pie chart, 90–91
pivotal quantity, see under confidence
interval
Poisson distribution, 26–27
closure under addition, 27
in exponential family, 48
p.m.f., 26
Poisson postulates, 26
polar area plot, 94
polynomial regression, see regression,
polynomial
positive definite matrix, see under matrix
posterior classification probability, 302
power, see under hypothesis test
prediction sum of squares, 214
predictor variable, 164, 198, 291
aliased, 208
centered, 210, 231, 255
qualitative, 242
standardized, 231, 235, 238
PRESS, see prediction sum of squares
principal component analysis, 342–346
eigendecomposition, 345, 367
Hotelling transformation, 345
Kaiser criterion, 345
Jolliffe adjustment, 346
Karhunen-Loéve transformation, 345
scree plot, 346
sparse, 349
principal component regression, 210, 349
principal components, 342
loadings, 345
percent explainable variance, 345, 346
variances equal to eigenvalues, 345
visualized, 344, 367
prior classification probability, 302
probability
axioms, 11
frequency interpretation, 10
rules, 11–12
addition rule, 11
Bayes’ Rule, 12
complement rule, 11
conditionality rule, 11
Law of Total Probability, 12
multiplication rule, 11
theory of, 10
probability density function, 14
probability distribution, see distribution
probability function, 13
bivariate, 16
conditional, 16
joint, 21
joint, 16, 21
for random sample, 116
marginal, 16
joint, 21
skewed, 15–16
symmetric, 15
unimodal, 15
probability mass function, 13
probit regression, see regression, probit
profile likelihood interval, see under
confidence interval, likelihood
ratio

INDEX
465
promax rotation, see under exploratory
factor analysis, rotation
pruning, see under decision trees
psychometric testing, 104, 351, 358, 370
Q-Q plot, see quantile-quantile plot
QR decomposition, 417
quadratic discriminant analysis, see under
discriminant analysis
quadratic form, 415
quality assurance/quality control, 5
quantal response, 271
quantile
function, 57, 58
population, 19, 34, 87
sample, 57–58, 87
standard normal, 33
quantile plot, 88–89
normal, 88
quantile-quantile (Q-Q) plot, 96–97
quantitative structure–activity relationship
(QSAR), 337
quartile
population, 19
first (lower), 19
third (upper), 19
sample
first (lower), 57, 79
third (upper), 57, 79
quartimax rotation, see under exploratory
factor analysis, rotation
quintile, 19
R, 24, 421–422
assignment keystrokes (<-), 423
bracketed indexing, 425–426, 428
comment character (#), 423
data entry, 422–425
data frame, 424
flow control, 429
functions
inbuilt, 427–428
user-defined, 429–430
graphics subsystem, 427
Inf, 422
logical operators, 428
NA, 422
NaN, 422
packages, 430–431
arules, 399, 400, 403
biglm, 430
biganalytics, 430
bigmemory, 430
binom, 134
boot, 138
car, 209
CCA, 365
class, 309–311
clusterSim, 390
deldir, 388
e1071, 329
ElemStatLearn, 254
ellipse, 173
ffbase, 430
ff, 430
forecast, 70
genridge, 233, 256
ggplot2, 104
ggsubplot, 94
glmnet, 233, 239, 240, 242, 271
kernlab, 330
klaR, 304, 305
knnGarden, 309
lattice, 430
leaps, 212, 217
map, 336
maptree, 315
MASS, 84, 233, 235, 265, 281,
288, 300, 305, 307
multcomp, 151
multtest, 151
PMA, 349, 364
psych, 87, 190, 354, 356, 357
psychometric, 190
qcc, 70
randomForest, 322
rgl, 99, 338, 348
ridge, 233, 234
rpart, 315–317, 322, 337
rpart.plot, 315, 317
scatterplot3d, 99, 317, 367
scrime, 309
stats, 430
svmpath, 329

466
INDEX
R packages (continued)
tree, 315
UsingR, 78
vcd, 185
vioplot, 112
R Commander, 431
RStudio, 431
workspace, 421
random forests, 322
random sample, 49
simple, 50
vector notation for, 116
random sampling, see sampling, random
random variable, 12
asymptotic behavior, 22
continuous, 13
discrete, 13
independent, 16
linear combinations of, 22
moment of, 18, 45
multivariate, 21
sequence of, 22
sum of, 22
transformation of, 16
vector notation for, 21
weighted average of, 22
randomization, 50
rank correlation, 190
rank of a matrix, see under matrix
receiver operating characteristic, see ROC
curve
regression
bridge, 241
gamma, 281
log-linear, 274, 280
logistic, 266
for supervised
classification, 292,
333, 335
regularized, 270
sparse, 270
multiple linear, 198
matrix formulation, 199
nonparametric, 321
Poisson, 274
polynomial, 210–211
probit, 266
ridge, 232
simple linear, 163–164
matrix formulation, 251
support vector, 332
with constant coefficient of
variation, 281
regression coefficients, 164, 198
vector of, 199
regression diagnostics, 175
Cook’s Distance, 182–183
hat value, 181, 200
influential observation, 182–183
leverage, 181
Mallows’ Cp, 214
outlier detection, 177–179
prediction sum of squares, 214
raw deleted residual, 178
residual loess plot, 253
residual plot, 176
residual Q-Q plot, 177
Studentized deleted residual, 178–179
exceedance level, 179
Studentized residual, 178
regression tree, 321
regularization, 230
elastic net, 241–242
L1 penalty, 238
Tikhonov, 231
via Lasso, 238–239
via ridge regression, 232–233
regularization parameter
decision tree, 315
via cross-validation, 315
penalized least squares, 231
support vector, 326
regularization path, see under support
vector methods
rejection region, 139
research hypothesis, see alternative
hypothesis
residual, 262
deviance, 263
in multiple linear regression, 200
in simple linear regression, 167
Pearson, 263
raw deleted, 178
standardized

INDEX
467
in generalized linear model, 263
Studentized, 178
Studentized deleted, 178–179
exceedance level, 179
residual plot, 176
heterogeneous variance in, 176
nonlinear pattern in, 176
residual sum of squares, 167, 200, 243
response variable, 164
response vector, 199
ridge regression, 232–233
bias-variance trade-off, 232
hat matrix, 232
ridge trace, 233
standard errors in, 236
tuning parameter for, 232
selection of, 233
ridge trace plot, 233
ROC curve, 297
rose plot, see polar area plot
R2, see coefficient of determination
R2
p plot, see under variable selection
rug plot, 81, 84
S language, 421
S-plus, 24
sabermetrics, 185
sample mean, 35, 53
asymptotic distribution, 35
asymptotic distribution of, 118
from normal distribution, 35, 118
sensitive to outliers, 54
standard error of, 118
sample size, 49
sample space, 10
partition of, 11
sample variance, 56
computing formulae, 56
sampling
convenience, 7, 9
opportunity, 7
probability-based, 6
random, 49–51
selection bias in, 7
systematic bias in, 50
sampling distribution, 117
sampling frame, 6
SAS, 24
Satterthwaite correction, see
Welch-Satterthwaite correction
saturated model, 261, 275
scalar, 411
scatterplot, 98–99
three-dimensional, 99
scatterplot matrix, 101–102
schematic plot, 80
Schwarz’s Bayesian Criterion, see
information criteria, Bayesian
Scott’s normal reference rule, see under
binning
scree plot, see under principal component
analysis
selection bias, see under sampling
self-organizing maps, xv
sensitivity
of classification rule, 296
of hypothesis test, 139
separate families of hypotheses, 203
Shannon entropy, see entropy
Shapiro-Wilk test, 155, 156
shrinkage regression, 231, 238
signal processing, 322
signal-vs.-noise problem, 65, 298
significance level, 139
signum function, 325
Simes’ adjustment, see under multiplicity
adjustment
simple linear model, see regression, simple
linear
simple linear regression, see regression,
simple linear
simple random sampling, see random
sample, simple
simultaneous confidence region, 149
single nucleotide polymorphism
data, 234–236, 239,
322
singular matrix, see under matrix
singular value decomposition, 345,
418–419
skew, see probability function, skewed
in probability function, 15
in random sample, 54
SLR, see regression, simple linear

468
INDEX
smoothing, 65
averaging window, 68
double exponential, 70
exponential, 69–70
exponential weighted average, 70
loess, 224
with multiple
predictors, 227–228
moving average, 68
retrospective, 68
symmetric, 68
via kernel density estimator, 85
SNP, see single nucleotide polymorphism
socioinformatics, 5
sparse discriminant analysis, see under
discriminant analysis
sparse logistic regression, see under
regression, logistic
sparse PCA, see under principal component
analysis
Spearman’s rank correlation, see rank
correlation
specificity, 296
spectral decomposition, 345, 353,
417
SPSS, 24
SRS, see random sample, simple
SSE, see residual sum of squares
standard deviation
population, 18
sample, 56
standard error, 117
of binomial proportion, 133
of sample mean, 118
standard normal, see normal (Gaussian)
distribution, standard
standardized variate, 32
statistical inference, 7, 115
statistical learning
supervised, 163
unsupervised, 341
Statlog project , 408
stem-and-leaf plot, see stemplot
stemplot, 81–83
back-to-back, 83
stochastic simulation, see Monte Carlo
Method
strip chart, 77, 79
strip plot, see strip chart
Student, see Gosset, W.S.
Student’s t-distribution, 38–40
Student’s t-test, 140
Studentized deleted residual, see under
regression diagnostics
Studentized variate, 125
Sturges’ rule, see under binning
subset selection, see variable selection,
(best) subset
sum of squares
for ANOVA main effect, 243
partial, 244
residual, 167, 200, 243
sequential, 244
supervised learning, 163
classification analytics, 291
regression analysis, 163
support space, see sample space
support vector, 322, 326
nonmargin, 326
support vector machines, 328
support vector methods, 322
classification rule, 325
kernel transformation, 326–329
kernel trick, 329
maximum margin classifier, 322
optimal separating hyperplane, 322,
324, 328
regularization path, 326
robust to outliers, 324
supporting hyperplane, 322
tuning parameter in, 325
support vector regression, 332
survival probability, 29
SVD, see singular value decomposition
SVM, see support vector machines
symmetric matrix, see under matrix
synergy, 267
synthetic data generation, see Monte Carlo
Method
t distribution, 38–40
dervied from normal, 38
upper-𝛼critical point, 39,
125

INDEX
469
t-test, 140
for difference in normal means
F-test equivalent, 243
with equal variances, 144
with paired data, 144–145
with unequal variances, 142–143
for normal mean, 140–141, 147
for regression coefficient
multiple linear, 202
polynomial, 211
simple linear, 168
tessellation, see Voronoi tessellation
test data, 233, 291
text mining, xv
Tikhonov regularization, see under
regularization
time series, 67, 105
ARIMA model, 70
financial, 68
plot, 106–109
Total Probability, Law of, see under
probability, rules
total sum of squares, 170
trace of a matrix, see under matrix
trace plot, 106
training data, 163, 233, 291
transformation, 16, 64–65
arc-sine/square-root, 64
Box-Cox, 64–65, 74, 249,
257
logarithmic, 64
to reduce skew, 64
logit, 64
power, 64–65, 74, 249, 257
square root, 64
transposition operator, 411
trellis graphics, 430
trend test, see Cochran-Armitage trend test
triangular matrix, see under matrix
trimmed mean, 55
tuning parameter
for kernel function, 329
for Lasso, 238
in penalized least squares, 231
in ridge regression, 232
with decision trees, 314
with support vector methods, 325
Twitter, 157, 250, 286
Type I error, see false positive error
Type II error, see false negative error
UCI Machine Learning Repository, xiv
unbalanced design, 242, 245
unbiased estimator, see estimator,
unbiased
uncertainty
of estimator, 117, 167
uniform distribution
continuous, 29
entropy of, 47
discrete, 28
unsupervised learning, 341
association rules, 395
cluster analysis, 373
dimension reduction, 342
exploratory factor analysis, 351
market basket analysis, 395
principal component analysis,
342
upper-𝛼critical point
𝜒2, 37, 127
F, 40
standard normal, 124
t, 39, 125
variable, see random variable
variable selection, 211–216
(best) subset, 216–217
backward elimination, 219
forward selection, 219
forward stepwise, 218–219
inference after, 211
R2
p plot in, 212
stepwise, 217
via AIC/BIC, 215, 222
via Lasso, 238
variance
of estimator, 117
of linear combination, 22
of sum of independent variables,
22
population, 18
sample, 56
variance inflation factor, 209

470
INDEX
variance-bias tradeoff, see bias-variance
tradeoff
variance-covariance matrix, see covariance
matrix
varimax rotation, see under exploratory
factor analysis, rotation
vector, 411
column, 411
coordinate unit (e), 412
intertia of, 415
linearly independent, 412
orthogonal, 413
orthonormal, 413
row, 411
transposition operator for, 411
vector addition, 413
vector multiplication, 413
VIF, see variance inflation factor
violin plot, 112
visualization
multivariate, 90
univariate, 76
Voronoi tessellation, 388–389
Wald interval, see under confidence
interval
Wald test, see under hypothesis test
Wald, A., 131
weakest-link pruning, see under decision
trees
weighted average
expected value of, 22
of observations, 53
of random variables, 22
weighted least squares, see least squares,
weighted
weighting inversely to variance, 120, 185
Welch-Satterthwaite correction, 128, 143
whiskers, see under boxplot
Wilk-Shapiro test, see Shapiro-Wilk test
Wilson interval, see under confidence
interval, for binomial probability
window width, 85
Winsorized mean, 55–56
wordle plot, 100
z-score, 57

