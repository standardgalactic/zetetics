Understanding AI alignment research: A
Systematic Analysis
Jan H. Kirchner∗
kirchner.jan@icloud.com
Logan Smith∗
logansmith5@gmail.com
Jacques Thibodeau∗
thibo.jacques@gmail.com
Kyle McDonell
kyle@conjecture.dev
Laria Reynolds
laria@conjecture.dev
Abstract
AI alignment research is the ﬁeld of study dedicated to ensuring that artiﬁcial intelligence
(AI) beneﬁts humans.
As machine intelligence gets more advanced, this research is
becoming increasingly important. Researchers in the ﬁeld share ideas across diﬀerent
media to speed up the exchange of information. However, this focus on speed means that
the research landscape is opaque, making it diﬃcult for young researchers to enter the
ﬁeld. In this project, we collected and analyzed existing AI alignment research. We found
that the ﬁeld is growing quickly, with several subﬁelds emerging in parallel. We looked
at the subﬁelds and identiﬁed the prominent researchers, recurring topics, and diﬀerent
modes of communication in each. Furthermore, we found that a classiﬁer trained on
AI alignment research articles can detect relevant articles that we did not originally
include in the dataset. We are sharing the dataset with the research community and
hope to develop tools in the future that will help both established researchers and young
researchers get more involved in the ﬁeld.
Introduction
AI alignment research is a nascent ﬁeld of
research concerned with developing ma-
chine intelligence in ways that achieve de-
sirable outcomes and avoid adverse out-
comes1,2. While the term alignment prob-
lem was originally proposed to denote the
problem of "pointing an AI in a direction"3,
the term AI alignment research is now used
as an overarching term referring to the en-
tire research ﬁeld associated with this prob-
lem2,4–9.
Associated lines of research in-
clude the question of how to infer human
values as revealed by preferences10, how to
prevent risks from learned optimization11,
or how to set up an appropriate structure
of governance to facilitate coordination12.
∗These authors contributed equally.
As machine intelligence becomes increas-
ingly capable13,14, AI alignment research
becomes increasingly important. There is a
risk that if machine intelligence is not care-
fully designed, it could have catastrophic
consequences for humanity15–17. For ex-
ample, if machine intelligence is not de-
signed to take human values into account,
it could make decisions that are harmful
to humans15. Alternatively, if machine in-
telligence is not designed to be transpar-
ent and understandable to humans, it could
make decisions that are opaque to humans
and diﬃcult to understand or reverse18. As
machine intelligence rapidly becomes more
powerful14, the stakes associated with the
AI alignment problem only grow. Conse-
quently, the ﬁeld receives considerable at-
1
arXiv:2206.02841v1  [cs.CY]  6 Jun 2022

Understanding AI alignment research: A Systematic Analysis
tention from philanthropic organizations
searching to increase the speed and scope
of research19,20.
One interesting feature of AI alignment
research is how the researchers commu-
nicate: to increase the speed and band-
width of information exchange, novel in-
sights and ideas are exchanged across var-
ious media.
Beyond the traditional re-
search article published as a preprint or
conference article, a substantial portion of
AI alignment research is communicated on
a curated community forum: the Align-
ment Forum21.
Other channels of com-
munication include formal and informal
talks22, semi-publicly shared manuscripts
and notes17,23, and informal exchanges via
instant messaging24.
The strong focus on increased speed and
bandwidth of communication comes at the
cost of a diﬀuse research landscape, mak-
ing it diﬃcult for newcomers to orient
themselves25,26. These diﬃculties are ex-
acerbated by the short time the ﬁeld has
existed and the resulting lack of unify-
ing paradigms27,28.
Previous attempts to
catalog and classify existing AI alignment
research29–32 do not include all relevant
sources, are not kept up-to-date, and do
not provide easy access to the data in a
machine-readable format. Given the po-
tential importance of AI alignment re-
search and the attempts to increase the size
of the ﬁeld19,20, the lack of a coherent
overview of the research landscape repre-
sents a major bottleneck.
In this project, we collected and cataloged
AI alignment research literature and ana-
lyzed the resulting dataset in an unbiased
way to identify major research directions.
We found that the ﬁeld is growing rapidly,
with several subﬁelds emerging naturally
over time. By analyzing the emerging sub-
ﬁelds, we can identify the prominent re-
searchers working in the subﬁeld, recur-
ring topics and questions speciﬁc to each
subﬁeld, and diﬀerent modes of commu-
nication dominating each subﬁeld. Finally,
training a classiﬁer to distinguish AI align-
ment research from more general AI re-
search can automatically detect relevant ar-
ticles published too recently to be included
in our dataset. We make our dataset and the
analysis publicly available to interested re-
searchers to enable further analysis and fa-
cilitate orientation to the ﬁeld.
Results
To capture the current state of AI align-
ment research, we collected research ar-
ticles from various sources (Tab. 1). Be-
yond the full-length manuscript published
on arXiv (𝑁
= 707), we also included
shorter communications published on the
Alignment Forum (𝑁= 2, 138), blogs, and
personal websites (𝑁
= 1, 326), publicly
available, full-length books (𝑁
= 23), a
popular AI alignment research newsletter
with summaries of articles (𝑁= 420), full-
length manuscripts not published on arXiv
(𝑁= 372), transcripts of lectures and inter-
views (𝑁= 494), and entries from public
wikis (𝑁= 582). To establish a baseline for
our analysis, we also collected research ar-
ticles from adjacent (𝑁= 1, 679) and un-
related (𝑁= 1, 000) areas of research, as
well as shorter communications published
on the LessWrong Forum (𝑁= 28, 259).
For details about our collection procedure,
2

Understanding AI alignment research: A Systematic Analysis
see the Methods section.
Rapid growth of AI alignment
research from 2012 to 2022 across
two platforms.
There was substantial heterogeneity in the
form and quality of articles in the dataset.
We decided to focus on articles published
on the Alignment Forum and as preprints
on the arXiv server (see Methods for arXiv
inclusion criteria). These sources contain
a large portion of the entire published AI
alignment research (Tab. 1) and are struc-
tured in a consistent form that allows auto-
mated analysis.
To quantify the ﬁeld’s growth, we visual-
ized the number of articles published on
either platform as a function of time. We
found a rapid increase from 20173 to 2022
(present) from less than 20 articles per year
to over 400 (Fig. 1a). When calculating the
number of articles published per researcher,
we observed a long-tailed distribution with
most researchers publishing less than ﬁve
articles and some publishing more than 60
(Fig. 1b).
Finally, when comparing the
number of researchers per article on the
Alignment Forum and the arXiv, we no-
ticed that articles on the Alignment Forum
tend to be written by either just a single
author or by a small team of fewer than
ﬁve researchers (Fig. 1c; purple). In con-
trast, the distribution of authors on arXiv
articles is long-tailed and includes articles
with more than 60 authors35–37 (Fig. 1c;
green).
This asymmetry partially results
from the late introduction of the multi-
3We note that the Alignment Forum was created
in 201834.
ple authors feature to the Alignment Fo-
rum1, but might also reﬂect the Align-
ment Forum’s focus on speed of communi-
cation, which disincentivizes large collabo-
rations38. Alternatively, the larger number
of authors on arXiv articles might also re-
ﬂect inﬂation of (unjustiﬁed) authorship on
research articles39,40.
Thus, AI alignment research is a rapidly
growing ﬁeld, driven by many researchers
contributing individual articles and a few
publishing proliﬁcally.
Unsupervised decomposition of
AI alignment research into dis-
tinct clusters.
Given the collected AI alignment research
articles from the Alignment Forum and
arXiv, we were curious whether we could
use the text to understand the current state
of research. To this end, we used the Allen
SPECTER model41 to compute a sentence
embedding, followed by a UMAP projec-
tion42 to obtain a low-dimensional repre-
sentation (Fig. 2a). While there is a ten-
dency for articles from diﬀerent sources to
occupy diﬀerent regions of the embedding,
the transition between Alignment Forum
and arXiv is ﬂuent (Fig. 2b). Interestingly,
when visualizing the publication date, we
noticed that the embedding captures part of
the temporal evolution of the ﬁeld (Fig. 2c).
Due to the relative youth of the ﬁeld,
there is no universally-accepted decompo-
sition of AI alignment research into sub-
ﬁelds22,28,43. To see if we can produce a
1The feature to add multiple authors didn’t become
available to all users until 2019, and many people may
still not be aware of how to do it.
3

Understanding AI alignment research: A Systematic Analysis
source
domain
# of articles
Alignment Forum
alignmentforum.org
2,138
lesswrong.com
28,252
arXiv
AI alignment research (level-0)
707
AI research (level-1)
1,679
arXiv.org/search/?query=quantum
1,000
arXiv.org/list/cs.AI (filtered)
4,621
Books
(available upon request)
23
Blogs
aiimpacts.org
227
aipulse.org
23
aisafety.camp
8
carado.moe
59
cold-takes.com
111
deepmindsafetyresearch.medium.com
10
generative.ink
17
gwern.net
7
intelligence.org
479
jsteinhardt.wordpress.com
39
qualiacomputing.com
278
vkrakovna.wordpress.com
43
waitbutwhy.com
2
yudkowsky.net
23
Newsletter
rohinshah.com/alignment-newsletter/ summaries
420
Reports
pdf-only articles
323
distill.pub
49
Audio transcripts
youtube.com playlist 1 & 2
457
Assorted transcripts
25
interviews with AI researchers33
12
Wikis
arbital.com
223
lesswrong.com (Concepts Portal)
227
stampy.ai
132
Total:
Total token count: 89,240,129
Total word count: 53,550,146
Total character count: 351,757,163
Table 1: Diﬀerent sources of text included in the dataset alongside the number
of articles per source. Color of row indicates that data was analyzed as AI alignment
research articles (green) or baseline (gray), or that the articles were added to the dataset
as a result of the analysis in Fig. 4 (purple). Deﬁnition of level-0 and level-1 articles in
Fig. 4c. For details about our collection procedure see the Methods section.
4

Understanding AI alignment research: A Systematic Analysis
2000
2010
2020
time (years)
0
200
400
count
alignment
forum
arxiv
0
60
articles per researcher
1
10
2
10
4
> 60
S. Armstrong
S. Garrabrant
A. Demski
J. Wentworth
P. Christiano
S. Levine
count
a
b
0
30
60
researchers per article
1
102
104
count
arxiv
>60
Bommasani et al. 2021
Brundage et al. 2020
Chen et al. 2021
c
alignment
forum
Figure 1: Alignment research across a community forum and a preprint server. (a)
Number of articles published as a function of time on the Alignment Forum (AF; purple)
and the arXiv preprint server (arXiv; green). (b) Histogram of the number of articles
per researcher published on either AF or arXiv. Inset shows names of six researchers
with more than 60 articles. Note the logarithmic y-axis. (c) Histogram of the number
of researchers per article on AF (purple) and arXiv (green). Note the logarithmic y-axis.
a
title
+
abstract
sentence
embedding
(768 dim.)
UMAP
embedding
(2 dim.)
dim 1
dim 2
alignment
forum
arxiv
b
2014
2022
time
c
clu. 1
clu. 2
clu. 3
clu. 4
clu. 5
d
0
20
# comp.
Figure 2: Dimensionality reduction and unsupervised clustering of alignment
research.
(a) Schematic of the embedding and dimensionality reduction.
After
concatenating title and abstract of articles, we embed the resulting string with the
Allen SPECTER model41, and then perform UMAP dimensionality reduction with
n_neighbors=250. (b) UMAP embedding of articles with color indicating the source
(AF, purple; arXiv, green). (c) UMAP embedding of articles with color indicating date
of publication. Arrows superimposed to indicate direction of temporal evolution. (d)
UMAP embedding of articles with color indicating cluster membership as determined
with k-means (k=5). Inset shows sum of residuals as a function of clusters k, with an
arrow highlighting the chosen number of clusters.
5

Understanding AI alignment research: A Systematic Analysis
useful, unbiased decomposition of the re-
search landscape, we applied k-means clus-
tering to the SPECTER embedding to ob-
tain ﬁve distinct clusters (see Methods for
details).
In summary, combined semantic embed-
ding and dimensionality reduction produce
a compact visualization of AI alignment re-
search.
Research dynamics vary across
the identiﬁed clusters.
Having identiﬁed ﬁve distinct research
clusters, we asked ourselves if we could
ﬁnd natural descriptions of research topics
and prominent researchers. Therefore, we
inspected which researchers tend to pub-
lish the highest number of articles in each
cluster (Tab. 2). Even though the names
of researchers did not enter into the Allen
SPECTER sentence embedding (Fig. 2a),
we observed that diﬀerent researchers tend
to dominate diﬀerent research clusters. The
distribution of researchers across clusters
lead us to assign putative labels to the clus-
ters (Fig. 3a):
• cluster one : Agent alignment is concerned
with the problem of aligning agentic sys-
tems, i.e. those where an AI performs ac-
tions in an environment and is typically
trained via reinforcement learning.
• cluster two : Alignment foundations is con-
cerned with deconfusion research, i.e. the
task of establishing formal and robust con-
ceptual foundations for current and future
AI alignment research.
• cluster three :
Tool alignment is con-
cerned with the problem of aligning non-
agentic (tool) systems, i.e. those where an
AI transforms a given input into an out-
put. The current, prototypical example of
tool AIs is the "large language model"35,44.
• cluster four : AI governance is concerned
with how humanity can best navigate the
transition to advanced AI systems. This
includes focusing on the political, eco-
nomic, military, governance, and ethical
dimensions12.
• cluster ﬁve : Value alignment is concerned
with understanding and extracting human
preferences and designing methods that
stop AI systems from acting against these
preferences.
To corroborate these putative labels, we
computed a word cloud representation of
the articles (Sup. Fig. 1). We found the re-
curring words speciﬁc to each cluster to be
in good agreement with the labels. We also
note that our labels are consistent with our
observation that alignment foundations re-
search is the historical origin of AI align-
ment research (Fig. 2c, Fig. 3b,c). Further-
more, we observe that theoretical research
(alignment foundations, value alignment,
AI governance) tends to be published on
the Alignment Forum. In contrast, applied
research (agent alignment, tool alignment)
tends to be published on arXiv (Fig. 2b,
Fig. 3d).
Finally, we note that in the
alignment foundations cluster, a few indi-
vidual researchers tend to produce a dis-
proportionate number of research articles
(Fig. 3e).
In combination, these arguments make us
hopeful that our unsupervised decomposi-
tion of AI alignment research mirrors rel-
evant structures existing in the ﬁeld. We
hope to leverage the decomposition to pro-
6

Understanding AI alignment research: A Systematic Analysis
cluster 1; 𝑁= 567
cluster 2; 𝑁= 988
cluster 3; 𝑁= 593
cluster 4; 𝑁= 383
cluster 5; 𝑁= 670
(agent alignment)
(alignment
founda-
tions)
(tool alignment)
(AI governance)
(value alignment)
S. Levine (55)
S. Armstrong (154)
J. Steinhardt (20)
D. Kokotajlo (21)
S. Armstrong (54)
P. Abbeel (34)
S. Garrabrant (95)
D. Hendrycks (17)
A. Dafoe (19)
S. Byrnes (32)
A. Dragan (29)
A. Demski (94)
E. Hubinger (14)
G. Worley III (11)
P. Christiano (29)
S. Russell (23)
J. Wentworth (57)
P. Christiano (13)
J. Clarck (10)
R. Ngo (25)
S. Armstrong (22)
"Diﬀractor" (44)
P. Kohli (11)
S. Armstrong (9)
R. Shah (25)
Table 2: Researchers with the highest number of articles per cluster. Clusters as
determined in Fig. 2, with number of articles per cluster 𝑁. Number in brackets be-
hind researcher name indicates number of articles published by that researcher. Note:
"Diﬀractor" is an undisclosed pseudonym.
2015
2018
2021
time (years)
0
1
fraction
2015
2018
2021
time (years)
0
2.5k
count
clu. 1
clu. 2
clu. 3
clu. 4
clu. 5
b
c
0
1
fraction
clu. 1
clu. 2
clu. 3
clu. 4
clu. 5
d
alignment
forum
arxiv
0
1
GINI coefficient
e
clu. 1
clu. 2
clu. 3
clu. 4
clu. 5
a
“agent
alignment”
“tool
alignment”
“alignment
foundations”
“AI
governance”
“value
alignment”
Figure 3: Characteristics of research clusters corroborate potential usefulness of
decomposition. (a) UMAP embedding of articles with color indicating cluster mem-
bership as in Fig. 2d. Labels assigned to each cluster are putative descriptions of a common
research focus across articles in the cluster. (b) Number of articles published per year, col-
ored by cluster membership. (c) Fraction of articles published by cluster membership as a
function of time. (d) Fraction of articles from AF or arXiv as a function of cluster mem-
bership. (e) GINI inequality coeﬃcient of articles per researcher as a function of article
cluster membership.
7

Understanding AI alignment research: A Systematic Analysis
vide researchers structured access to the ex-
isting literature in future work.
Leveraging dataset to train an AI
alignment research classiﬁer.
When quantifying the number of articles
across diﬀerent sources, we noticed a dra-
matic drop-oﬀin articles published on the
arXiv after 2019 (Fig. 1a).
Especially in
contrast with the continued strong increase
in articles published on the Alignment Fo-
rum, we suspected that our data collection
might have missed some more recent, rele-
vant work1.
To automatically detect articles published
more recently, we decided to train a lo-
gistic regression classiﬁer on the seman-
tic embeddings of arXiv articles. Besides
the AI alignment research articles already
included in our dataset ("arXiv level-0";
Fig. 4a green), we also collected all arXiv
articles cited by level-0 articles, which
were not level-0 articles themselves ("arXiv
level-1"; Fig. 4a blue). We trained the clas-
siﬁer on a training set (80%) to distinguish
level-0 from level-1 articles and evaluated
performance on a separate test set (20%).
The classiﬁer achieved good performance
(AUC= 0.75; Fig. 4b inset), reliably re-
jecting level-1 articles and correctly iden-
tifying a large portion of level-0 articles
(Fig. 4b). To test whether the classiﬁer ro-
bustly generalizes beyond AI research, we
tested it on 1000 recently published articles
on quantum physics and the Alignment Fo-
rum. We found that the classiﬁer reliably
1In particular, for our dataset, we manually ex-
tended an existing collection of arXiv articles from
202031, see Methods section for details.
rejects quantum physics and accepts Align-
ment Forum articles (Fig. 4b,d).
Most AI alignment research articles on the
arXiv are published in the cs.AI section.
Therefore we used the arXiv API45 to col-
lect all articles from that section (Fig. 4c).
When applying our classiﬁer to the se-
mantic embeddings of the cs.AI articles,
we observed a slightly bimodal distribution
with most articles receiving a score close
to 0%, and some articles receiving a score
close to 100% (Fig. 4d). Motivated by the
distribution of scores of Alignment Forum
articles and by individual inspection, we
chose a threshold at 75% and considered ar-
ticles above that threshold as AI alignment
research-relevant and added them to our
dataset. As anticipated, we found that the
number of AI alignment-relevant arXiv ar-
ticles increases as rapidly over time as the
articles published on the Alignment Forum
(Fig. 4e). Finally, to verify that the addi-
tion of AI alignment-relevant arXiv articles
does not aﬀect our unsupervised decompo-
sition, we repeated the UMAP dimension-
ality reduction on the updated dataset. We
found that cluster structure is not disrupted
(Fig. 4f ).
In conclusion, our analysis demonstrates
that semantic embedding can capture rel-
evant characteristics of AI alignment re-
search and that automatic ﬁltering of new
publications might be feasible.
Discussion
The ﬁeld of AI alignment research is grow-
ing quickly, with many researchers pub-
lishing articles on diverse topics.
We
found that semantic embedding and di-
8

Understanding AI alignment research: A Systematic Analysis
0
1
FP rate
0
1
TP rate
AUC
0.75
level-0 level-1
train
test
b
0
100
0
fraction
classifier AF (%)
0.1
0.3
alignment
forum
d
arxiv
cs.AI
cutoff
2000
2010
2020
time (years)
0
count
alignment
forum
arxiv
updated
1500
e
0
0.2
0
100
0.6
classifier AF (%)
arxiv
level-0
arxiv
level-1
quantum
physics
arxiv
level-0
arxiv
level-1
cite
a
c
fraction
clu. 1
clu. 2
clu. 3
clu. 4
clu. 5
f
arxiv
updated
adding
arxiv
Figure 4: An AI alignment research classiﬁer for ﬁltering new publications. (a)
Top: Illustration of arXiv level-0 articles (alignment research; green) and level-1 ar-
ticles (cited by alignment research articles; blue). Bottom: Schematic of test-train split
(20%-80% for training of a logistic regression classiﬁer. (b) Fraction of articles as a function
of classiﬁer score for arXiv level-0 (green), level-1 (blue), and arXiv articles on quantum
physics (grey). (c) Illustration of procedure for ﬁltering arXiv articles. After querying
articles from the cs.AI section of arXiv, the logistic regression classiﬁer assigns a score
between 0 and 1. (d) Fraction of articles as a function of classiﬁer score for articles from
the cs.AI section of arXiv (grey) and AF (purple). Dashed line indicates cutoﬀfor clas-
sifying articles as arXiv level-0 (75%). (e) Number of articles published as a function of
time on AF (purple) and arXiv (green), according to the cutoﬀin panel d. (f) Left inset:
Original UMAP embedding from Fig. 2d. Right: UMAP embedding of all original ar-
ticles and updated arXiv articles with color indicating cluster membership as in Fig. 2d
or that the article is ﬁltered from the arXiv (gray).
9

Understanding AI alignment research: A Systematic Analysis
mensionality reduction can produce a com-
pact visualization of AI alignment research.
This decomposition of AI alignment re-
search mirrors known structures in the
ﬁeld, demonstrating that semantic embed-
ding can capture relevant characteristics of
AI alignment research. Furthermore, we
demonstrate the possible feasibility of au-
tomatically detecting new publications rel-
evant to AI alignment research. In the fu-
ture, we hope that our decomposition can
provide researchers with structured access
to the existing literature.
Tools for alignment researchers.
Our
presented research suggests several excit-
ing possible applications for improving
the research landscape in AI alignment
research.
We have begun to explore
this potential by developing several pro-
totypes that use the collected dataset to
interactively explore semantic embeddings
(Sup. Fig. 2), to provide summaries of long
articles (Sup. Fig. 3), or to search and com-
pare articles (Sup. Fig. 4). Thanks to the
focus on speed and the openness to inno-
vation of the AI alignment research com-
munity, we believe that tools tailored to
this community might reach broad adop-
tion and help accelerate research eﬀorts.
Paradigmatic AI alignment research. In
the language of Thomas Kuhn27, the suc-
cessive transition from one paradigm to an-
other via revolution is the usual develop-
mental pattern of mature science.
Some
researchers argue that AI alignment re-
search is pre-paradigmatic, meaning that it
has not yet converged on a single, dom-
inant paradigm or approach.
While our
research demonstrates that decomposition
of AI alignment research into meaningful
subﬁelds is possible, we note that the choice
of the number of subﬁelds has a subjective
component (Fig. 2d). Furthermore, the se-
mantic similarity between articles in a clus-
ter does not imply similarity in methodol-
ogy or underlying research agenda. How-
ever, we do not believe that this implies the
impossibility of progress. In fact, the cur-
rent exploratory nature of AI alignment re-
search might be a strength, as exploration
helps to avoid ossiﬁcation.
Limitations. Especially due to the rapid
expansion of the ﬁeld (Fig. 1), classiﬁca-
tions and descriptions of the state-of-the-
art might become inaccurate soon after
publication.
While the observation that
our clustering remains stable after includ-
ing many articles not used for the original
clustering (Fig. 4) makes us hopeful, we still
plan to carefully monitor the ﬁeld and pub-
lish regular updates to our analysis.
The decision to focus on the two largest,
non-redundant sources of articles (Align-
ment Forum and arXiv) might systemati-
cally exclude certain lines of research and
thus bias our analysis. However, as a sub-
stantial fraction of blog posts, reports and
the alignment newsletter tend to be cross-
posted or announced on the Alignment Fo-
rum we think a strong bias is unlikely.
In summary, by collecting a comprehen-
sive dataset of published AI alignment re-
search literature, we demonstrate rapid
growth of the ﬁeld over the last ﬁve years
and identify emerging directions of re-
search through unbiased clustering.
10

Understanding AI alignment research: A Systematic Analysis
Methods
Data collection and inclusion criteria.
• Alignment Forum & LessWrong: We
extracted all posts on the forum viewer
website
GreaterWrong.com
on
March
21st, 2022 (dataset used for the analysis
in this article) and June 4th (dataset pub-
lished). We excluded articles with the tag
"event", which are published for coordi-
nating meetups.
• arXiv: We extended an existing collec-
tion of AI alignment research arXiv ar-
ticles31 from 2020 with relevant publica-
tions published since then ("arXiv Level-
0"). We started with an existing bibliog-
raphy of alignment literature31 and aug-
mented that collection with two other
bibliographies46,47, articles mentioned in
the alignment newsletter, and articles we
identiﬁed. We excluded articles that were
not about AI alignment research.
• Books: We converted ebooks into plain
text ﬁles with pandoc. No text was ex-
cluded.
• Blogs: We extracted individual articles
from AI alignment research-relevant (as
determined by the authors) blogs with the
requests and the BeautifulSoup pack-
ages. No text was excluded.
• Newsletter:
We extracted summaries
from the publicly available list of sum-
maries and matched them with the respec-
tive original articles.
• Reports: We extracted additional pub-
lished articles that were only available as
pdf ﬁles, by converting these ﬁles with
grobid and cleaning the resulting ﬁles.
No text was excluded.
• Audio transcripts: We were able to lo-
cate some transcripts of interviews avail-
able online.
For the rest, we used a
voice-to-text service (otter.ai) to extract
transcripts from AI alignment research-
relevant (as determined by the authors)
recordings. We hired contractors to clean
the resulting transcripts to correct format-
ting problems and spelling mistakes. After
cleaning, no text was excluded.
• Wikis: We extracted articles from two
open Wikis on AI alignment research
(arbital.com, (lesswrong.org’s Con-
cepts Portal and stampy.ai) through the
export option on the website.
Data analysis. We performed the dataset
collection with Python 3.7 on commodity
hardware and Google Colab and all data
analysis with Python 3.7 in Google Colab.
We created plots with the seaborn pack-
age48 and post-processed them in Adobe Il-
lustrator.
Semantic embedding. We used the Allen
SPECTER model41 through the hugging-
face sentence transformer library49 for em-
bedding articles into a 768 dimensional
vector space.
The SPECTER model re-
quires each article as <Title> + <SEP> +
<Abstract>, where <SEP> is the separator
token of the tokenizer. For articles from
the arXiv, we used the author-submitted
abstract as the <Abstract>. As articles from
the Alignment Forum do not always have
an author-submitted abstract, we instead
used the ﬁrst 2-5 paragraphs of the article
as the <Abstract>.
Dimensionality reduction. To compute
a two-dimensional representation of the se-
mantic embedding, we used the python
11

Understanding AI alignment research: A Systematic Analysis
UMAP package50 with a neighborhood
parameter of n_neighborhood=250. Using
a smaller or larger neighborhood did not
aﬀect the results, but at very small neigh-
borhood values (n_neighborhood<40) the
embedding became unstable.
Unsupervised clustering. While we ex-
plored diﬀerent clustering algorithms, we
eventually converged on the k-means im-
plementation of the scikit-learn package51,
which is straightforward to interpret while
producing robust clustering across multiple
instantiations.
Statistics.
All statistics were computed
with the seaborn package48 in python, with
the exception of the GINI coeﬃcient in
Fig. 3, which we computed as half of the
relative mean absolute diﬀerence52,
𝑛
∑︁
𝑖=1
𝑛
∑︁
𝑗=1
𝑥𝑖−𝑥𝑗

2𝑛2 ¯𝑥
,
where 𝑥𝑖is the number of articles of each
researcher and ¯𝑥is the average number of
articles across all researchers.
Logistic regression classiﬁer.
To train
the AI alignment research classiﬁer, we
used the LogisticRegression model of the
scikit-learn package in Python51 with an
increased number of maximum iterations,
max_iter=1000. For training, we used 80%
of level-0 and level-1 arXiv papers.
For
evaluation in Fig. 4b we used the remain-
ing 20% of level-0 and level-1 arXiv pa-
pers as well as 1000 arbitrarily chosen ar-
ticles on quantum physics. For the anal-
ysis in Fig. 4c-f, we used the arXiv API45
to collect all articles published in the cs.AI
section since its inception.
Code
and
data
availability.
The
dataset
and
all
code
for
collecting
the
dataset
is
available
on
Github,
https://github.com/moirage/alignment-
research-dataset.git.
Code for the data
analysis is available upon request.
Acknowledgments
JK and LR were supported by funding
from the Longterm Future Fund.
We
thank Daniel Clothiaux for help with writ-
ing the code and extracting articles. We
thank Remmelt Ellen, Adam Shimi, and
Arush Tagade for feedback on the research.
We thank Chu Chen, Ömer Faruk Şen,
Hey, Nihal Mohan Moodbidri, and Trinity
Smith for cleaning the audio transcripts.
References
1.
Yudkowsky, E. The AI alignment
problem: why it is hard, and where
to start. Symbolic Systems Distinguished
Speaker (2016).
2.
Christian, B. The alignment problem:
Machine learning and human values
(WW Norton & Company, 2020).
3.
Yudkowsky, E. The Rocket Alignment
Problem en-US. 2018.
4.
Russell, S. in Human-Like Machine
Intelligence 3–23 (Oxford University
Press Oxford, 2021).
5.
Gabriel, I. Artiﬁcial intelligence, val-
ues, and alignment. Minds and ma-
chines 30, 411–437 (2020).
12

Understanding AI alignment research: A Systematic Analysis
6.
Ouyang,
L.,
Wu,
J.,
Jiang,
X.,
Almeida,
D.,
Wainwright,
C.
L.,
Mishkin, P., Zhang, C., Agarwal, S.,
Slama, K., Ray, A., et al. Training lan-
guage models to follow instructions
with human feedback. arXiv preprint
arXiv:2203.02155 (2022).
7.
Kenton, Z., Everitt, T., Weidinger, L.,
Gabriel, I., Mikulik, V. & Irving, G.
Alignment of language agents. arXiv
preprint arXiv:2103.14659 (2021).
8.
Dafoe, A., Bachrach, Y., Hadﬁeld, G.,
Horvitz, E., Larson, K. & Graepel, T.
Cooperative AI: machines must learn to
ﬁnd common ground 2021.
9.
Askell, A., Bai, Y., Chen, A., Drain,
D., Ganguli, D., Henighan, T., Jones,
A., Joseph, N., Mann, B., DasSarma,
N., et al. A General Language As-
sistant as a Laboratory for Align-
ment. arXiv preprint arXiv:2112.00861
(2021).
10.
Christiano, P. F., Leike, J., Brown,
T., Martic, M., Legg, S. & Amodei,
D. Deep reinforcement learning from
human preferences. Advances in neural
information processing systems 30 (2017).
11.
Hubinger,
E.,
van
Merwijk,
C.,
Mikulik, V., Skalse, J. & Garrabrant,
S. Risks from learned optimization
in advanced machine learning sys-
tems. arXiv preprint arXiv:1906.01820
(2019).
12.
Dafoe, A. AI governance: a research
agenda. Governance of AI Program, Fu-
ture of Humanity Institute, University
of Oxford: Oxford, UK 1442, 1443
(2018).
13.
Grace, K., Salvatier, J., Dafoe, A.,
Zhang, B. & Evans, O. When will
AI exceed human performance? Evi-
dence from AI experts. Journal of Arti-
ﬁcial Intelligence Research 62, 729–754
(2018).
14.
Sevilla, J., Heim, L., Ho, A., Be-
siroglu, T., Hobbhahn, M. & Vil-
lalobos, P. Compute trends across
three eras of machine learning. arXiv
preprint arXiv:2202.05924 (2022).
15.
Bostrom, N. Superintelligence (Dunod,
2017).
16.
Ord, T. The precipice: Existential risk
and the future of humanity (Hachette
Books, 2020).
17.
Carlsmith, J. Is Power-Seeking AI an
Existential Risk? (2021).
18.
Christiano, P. What failure looks like.
Alignment Forum (2019).
19.
Beckstead, N. & Muehlhauser, L.
Potential Risks from Advanced Arti-
ﬁcial Intelligence https : / / www .
openphilanthropy . org / focus /
global - catastrophic - risks /
potential - risks - advanced -
artificial-intelligence (2022).
20.
Foundation, F. Potential Risks from Ad-
vanced Artiﬁcial Intelligence https://
ftxfuturefund.org/ (2022).
21.
Infrastructure, L. Alignment Forum
https://www.alignmentforum.org/
(2022).
22.
Christiano,
P.
Current
work
in
AI alignment https : / / forum .
effectivealtruism . org / posts /
63stBTw3WAW6k45dY
/
paul
-
13

Understanding AI alignment research: A Systematic Analysis
christiano - current - work - in -
ai-alignment (2022).
23.
Cotra, A. Draft report on AI timelines
https : / / www . lesswrong . com /
posts / KrJfoZzpSDpnrv9va / draft -
report-on-ai-timelines (2022).
24.
Institute,
M.
I.
R.
Late
2021
MIRI
Conversations
https : / /
intelligence . org / late - 2021 -
miri-conversations/ (2022).
25.
Hyvärinen, A.-M. How I failed to
form views on AI safety. Eﬀective Al-
truism Forum (2022).
26.
Wentworth, J. S. How To Get Into
Independent
Research
On
Align-
ment/Agency.
Alignment
Forum
(2021).
27.
Kuhn, T. S. The structure of scien-
tiﬁc revolutions (Chicago University of
Chicago Press, 1970).
28.
Shimi, A. Epistemological Framing
for AI Alignment Research. Alignment
Forum (2021).
29.
Miles, R. Stampy’s Wiki https : / /
stampy.ai/wiki/Stampy%5C%27s_
Wiki (2022).
30.
Shah, R. Alignment Newsletter https:
/ / rohinshah . com / alignment -
newsletter/ (2022).
31.
Riedel, J. & Deibel, A. AI Safety
Papers https : / / ai - safety -
papers . quantifieduncertainty .
org/ (2022).
32.
Ought. Elicit: The AI research assistant
https://elicit.org (2022).
33.
Gates,
V.
Transcripts
of
inter-
views
with
AI
researchers
https :
/ / www . lesswrong . com / posts /
LfHWhcfK92qh2nwku / transcripts -
of - interviews - with - ai -
researchers (2022).
34.
Arnold,
R.
Announcing
Align-
mentForum.org
Beta
https : / /
www . lesswrong . com / posts /
JiMAMNAb55Qq24nES / announcing -
alignmentforum-org-beta (2022).
35.
Bommasani, R., Hudson, D. A., Adeli,
E., Altman, R., Arora, S., von Arx, S.,
Bernstein, M. S., Bohg, J., Bosselut,
A., Brunskill, E., et al. On the oppor-
tunities and risks of foundation mod-
els. arXiv preprint arXiv:2108.07258
(2021).
36.
Brundage, M., Avin, S., Wang, J.,
Belﬁeld, H., Krueger, G., Hadﬁeld,
G., Khlaaf, H., Yang, J., Toner, H.,
Fong, R., et al. Toward trustworthy
AI development: mechanisms for sup-
porting veriﬁable claims. arXiv preprint
arXiv:2004.07213 (2020).
37.
Chen, M., Tworek, J., Jun, H., Yuan,
Q., Pinto, H. P. d. O., Kaplan,
J., Edwards, H., Burda, Y., Joseph,
N., Brockman, G., et al. Evaluat-
ing large language models trained on
code. arXiv preprint arXiv:2107.03374
(2021).
38.
Moshontz, H., Ebersole, C. R., We-
ston, S. J. & Klein, R. A. A guide for
many authors: Writing manuscripts in
large collaborations. Social and Person-
ality Psychology Compass 15, e12590
(2021).
14

Understanding AI alignment research: A Systematic Analysis
39.
Põder, E. Let’s correct that small mis-
take. Journal of the American Society for
Information Science and Technology 61,
2593–2594 (2010).
40.
Lozano, G. A. The elephant in the
room: multi-authorship and the as-
sessment of individual researchers.
Current Science 105, 443–445 (2013).
41.
Cohan, A., Feldman, S., Beltagy,
I.,
Downey,
D.
&
Weld,
D.
S.
Specter:
Document-level
repre-
sentation
learning
using
citation-
informed transformers. arXiv preprint
arXiv:2004.07180 (2020).
42.
McInnes, L., Healy, J. & Melville,
J.
Umap:
Uniform
manifold
ap-
proximation
and
projection
for
dimension reduction. arXiv preprint
arXiv:1802.03426 (2018).
43.
Critch, A. Some AI research areas
and their relevance to existential safety
https : / / www . alignmentforum .
org / posts / hvGoYXi2kgnS3vxqb /
some - ai - research - areas - and -
their-relevance-to-existential-
1 (2022).
44.
Weidinger, L., Mellor, J., Rauh, M.,
Griﬃn, C., Uesato, J., Huang, P.-S.,
Cheng, M., Glaese, M., Balle, B.,
Kasirzadeh, A., et al. Ethical and social
risks of harm from Language Mod-
els. arXiv preprint arXiv:2112.04359
(2021).
45.
University, C. arXiv API https : / /
arxiv.org/help/api/ (2022).
46.
Krakovna,
V.
AI
safety
resources
https : / / vkrakovna . wordpress .
com/ai-safety-resources/ (2022).
47.
Larks. 2021 AI Alignment Literature
Review and Charity Comparison https:
/ / www . alignmentforum . org /
posts / C4tR3BEpuWviT7Sje / 2021 -
ai-alignment-literature-review-
and-charity-comparison (2022).
48.
Waskom, M. L. seaborn: statistical
data visualization. Journal of Open
Source Software 6, 3021 (2021).
49.
Reimers, N. & Gurevych, I. Sentence-
BERT:
Sentence
Embeddings
using
Siamese BERT-Networks in Proceedings
of the 2019 Conference on Empirical
Methods in Natural Language Process-
ing (Association for Computational
Linguistics, 2019).
50.
Sainburg, T., McInnes, L. & Gentner,
T. Q. Parametric UMAP Embed-
dings for Representation and Semisu-
pervised Learning. Neural Computa-
tion 33, 2881–2907 (2021).
51.
Pedregosa, F., Varoquaux, G., Gram-
fort, A., Michel, V., Thirion, B.,
Grisel,
O.,
Blondel,
M.,
Pretten-
hofer, P., Weiss, R., Dubourg, V.,
Vanderplas, J., Passos, A., Courna-
peau, D., Brucher, M., Perrot, M. &
Duchesnay, E. Scikit-learn: Machine
Learning in Python. Journal of Ma-
chine Learning Research 12, 2825–2830
(2011).
52.
Wikipedia contributors. Gini coeﬃ-
cient — Wikipedia, The Free Encyclope-
dia [Online; accessed 27-May-2022].
2022.
53.
Wang,
B.
Mesh-Transformer-JAX:
Model-Parallel
Implementation
of
Transformer Language Model with JAX
15

Understanding AI alignment research: A Systematic Analysis
https://github.com/kingoflolz/
mesh-transformer-jax. 2021.
54.
Wang, B. & Komatsuzaki, A. GPT-
J-6B: A 6 Billion Parameter Autore-
gressive Language Model https : / /
github . com / kingoflolz / mesh -
transformer-jax. 2021.
16

Understanding AI alignment research: A Systematic Analysis
Appendix
reward
reinforcement learning
policy
environment
learning
task
reward function
state
action
learn
use
algorithm
paper
agents
using
robot
method
training
goal
demonstration
approach
system
behavior
way
game
based
value
humans
given
work
make
two
performance
preference
need
time
new
data
well
policies
take
distribution
show
set
information
without
want
imitation learning
different
trained
first
even
train
world
objective
case
feature
problems
may
instead
observation
Inverse Reinforcement
find
used
author
learned
exploration
safety
setting
good
result
feedback
function
allow
provide
many
might
better
user
possible
dynamic
require
expert
single
see
order
consider
space
lead
must
much
particular
trajectories
mean
learning algorithm
propose
think
trajectory
approaches
play
rather
simple
control
framework
perform
level
part
change
planning
give
real world
IRL
able
often
likely
image
evaluate
technique
assumption
solution
assume
whether
idea
cluster 1 - reinforcement learning
cluster 2 - agent foundations
paper
neural network
task
training
dataset
performance
data
method
image
use
work
GPT
new
using
make
different
network
might
distribution
system
think
question
machine learning
function
input
find
result
language
way
first
well
two
time
show
feature
value
good
algorithm
point
even
many
better
trained
compute
set
train
output
learn
much
alignment
research
accuracy
see
examples
given
author
classifier
want
case
based
large
seem
prediction
humans
still
may
robustness
change
take
information
look
benchmark
simple
text
particular
provide
parameter
architecture
learning
thing
weight
approach
word
sample
need
technique
used
possible
often
cluster 3 - language modeling
system
research
alignment
work
risk
safety
think
AGI
might
people
make
question
many
researcher
year
way
time
value
may
important
future
different
scenario
argument
paper
world
first
want
good
much
even
impact
idea
Benefit
take
two
seem
project
use
well
problems
need
progress
likely
new
help
humans
part
field
current
thing
lot
comment
see
better
less
policy
governance
goal
now
useful
lead
hard
particular
whether
result
mean
don
case
technology
point
three
change
discussion
development
posts
find
probably
reason
information
relevant
view
give
group
topic
link
related
capabilities
provide
cluster 4 - AI Governance
system
humans
think
alignment
way
might
goal
AGI
work
world
want
value
make
thing
agents
use
seem
different
question
time
may
good
first
even
safety
two
task
environment
many
well
possible
need
case
problems
using
part
action
point
idea
something
algorithm
given
take
people
much
argument
future
training
see
research
particular
behavior
paper
mean
reward
learn
useful
know
say
important
answer
build
aligned
understand
whether
approach
actually
design
still
don
new
better
rather
brain
find
preference
now
learning
consider
help
concept
understanding
thinking
general
risk
look
trying
information
action
way
set
world
value
time
use
agents
make
think
given
preference
two
want
thing
function
Let
probability
first
different
case
might
mean
take
question
say
system
see
utility function
output
seem
player
possible
work
definition
consider
decision theory
counterfactual
using
give
state
need
even
now
algorithm
part
result
environment
good
information
true
point
humans
proof
problems
answer
know
may
well
utility
goal
define
Note
argument
fact
idea
game
much
outcome
policy
something
look
many
better
theory
people
show
don
instead
sequence
distribution
situation
prediction
belief
whether
paper
actually
assume
choose
new
particular
reward
still
universe
sentence
variable
bit
must
observation
find
general
exist
defined
happen
following
Suppose
doesn
rather
without
change
reason
term
assumption
number
oracle
map
least
concept
Thus
behavior
theorem
simple
used
version
every
second
another
start
space
going
allow
prior
predict
choice
future
similar
data
examples
program
cluster 5 - value learning
Supplementary Figure 1: Word frequency visualization for diﬀerent clusters. Word-
cloud representation (word_cloud package in Python) of the most commonly used words
in articles of the ﬁve identiﬁed clusters in Fig. 2. The following words occurred in all
clusters very often and were thus removed from the wordcloud: "will", "post", "problem",
"example", "one", "SEP", "AI", "agent", "human", "model", and "models".
17

Understanding AI alignment research: A Systematic Analysis
Supplementary Figure 2: Interactive embedding of AI alignment literature. An in-
teractive plot (plotly.com) of an UMAP projection of AI alignment research that displays
the title of a selected article.
18

Understanding AI alignment research: A Systematic Analysis
Supplementary Figure 3: Summarization tool. An early prototype of a summarization
service for AI alignment research articles. We ﬁnetuned a 6B GPT-J language model53,54
on the collected dataset and designed a prompt that produces a short summary of a pro-
vided AI alignment research article.
19

Understanding AI alignment research: A Systematic Analysis
Supplementary Figure 4: Prototype of semantic search engine. After entering the
URL of an Alignment Forum post (top left), the article is extracted (bottom left) and em-
bedded with the Allen SPECTER model41. The resulting embedding is compared with
all embeddings with a vector database search service (Pinecone.io) to retrieve similar
articles (middle column). By clicking the "Explain" button on a search result, a query
with the abstract of the original article and the search result is sent to the OpenAI API
to generate an analysis of similarities and diﬀerences (right column).
20

