Association for Information Systems 
Association for Information Systems 
AIS Electronic Library (AISeL) 
AIS Electronic Library (AISeL) 
ACIS 2023 Proceedings 
Australasian (ACIS) 
12-2-2023 
What the Lone Banana Problem reveals about the Nature of 
What the Lone Banana Problem reveals about the Nature of 
Generative AI 
Generative AI 
Kai Riemer 
The University of Sydney Business School, Australia, kai.riemer@sydney.edu.au 
Sandra Peter 
The University of Sydney Business School, Australia, sandra.peter@sydney.edu.au 
Follow this and additional works at: https://aisel.aisnet.org/acis2023 
Recommended Citation 
Recommended Citation 
Riemer, Kai and Peter, Sandra, "What the Lone Banana Problem reveals about the Nature of Generative AI" 
(2023). ACIS 2023 Proceedings. 86. 
https://aisel.aisnet.org/acis2023/86 
This material is brought to you by the Australasian (ACIS) at AIS Electronic Library (AISeL). It has been accepted for 
inclusion in ACIS 2023 Proceedings by an authorized administrator of AIS Electronic Library (AISeL). For more 
information, please contact elibrary@aisnet.org. 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
1 
What the Lone Banana Problem reveals about the Nature 
of Generative AI 
Full research paper  
 
Kai Riemer 
Discipline of Business Information Systems 
The University of Sydney 
Sydney, NSW, Australia 
Email: kai.riemer@sydney.edu.au  
Sandra Peter 
Discipline of Business Information Systems 
The University of Sydney 
Sydney, NSW, Australia 
Email: sandra.peter@sydney.edu.au   
 
Abstract 
Generative AIs have mushroomed, exhibiting astonishing text and image generation abilities. At the 
same time, these systems also show surprising weaknesses in terms of information accuracy, ability to 
solve simple maths problems, or the visualisation of simple objects. In this essay we take the so-called 
“lone banana problem”, the inability of Midjourney to generate a picture of a single banana (instead of 
a bunch), as our starting point to problematise our common-sense understanding, and related 
expectations, of generative AI. Through problematisation we highlight that many of the traditional IS 
assumptions of how systems represent real-world phenomena, and how algorithms work, need to be set 
aside for understanding generative AI more authentically. We suggest conceiving of generative AI as 
style engines that encode all aspects of the world - objects, properties, appearances - as styles available 
for creation. We discuss what this alternative conception affords, and implications for the IS discipline. 
Keywords Generative AI, Artificial Intelligence, Probabilistic Computing, Styles, Problematisation. 
 
 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
2 
1 Introduction 
In July 2023, at the height of the generative AI hype, we came across a curious article titled “The Lone 
Banana Problem”, published by Digital Science CEO Daniel Hook on the company’s blog (Hook 2023). 
The article describes in detail the author’s frustration and fascination with the inability of Midjourney, 
a popular image generation AI, to generate a picture of a single banana (as opposed to bunches or pairs 
of bananas), and the author’s attempts to find a prompt that would do so.  
In this essay we take the lone banana problem as a starting point to investigate and illustrate the nature 
of generative AI systems. We argue that the important question is not to ask, why Midjourney was unable 
to generate a lone banana, but why we are surprised by its inability to do so. More precisely, what does 
this surprise reveal about our understanding of the nature of generative AI? In investigating these 
questions we draw on problematisation, a method described by Alvesson and Kärreman (2007). 
Problematisation takes surprising observations or phenomena as an opportunity to surface and question 
taken-for-granted assumptions that underpin widely held beliefs or pre-understandings in a field of 
research, or society more broadly. The approach takes surprise as resulting from a mismatch between 
tacit expectations and concrete observations. It posits that, normally, surprises are either resolved by 
finding a way to explain it away from within existing understandings of a subject matter, or they are set 
aside as curious oddities or problems to be dealt with at a later time. By contrast, the idea of 
problematisation is to ask critically whether the cause for the surprise - instead of being located in the 
phenomenon observed - stems from the deeply held assumptions that underpin normal expectation of 
what was supposed to happen (e.g. how hard can it be to generate just one single banana?). By 
questioning and replacing said assumptions, it then becomes possible to arrive at a novel, changed 
understanding of the subject matter. 
Utilising a problematisation approach, we use the surprise presented by the lone banana problem to 
arrive at a new understanding of generative AI that renders the lone banana no longer a problem with 
the tool but a typical, to be expected, example of how generative AI works. Hence, our research objective 
is to surface assumptions underpinning the accepted narratives about generative AI, and to challenge 
these as a way of advancing our understanding of it as a type of system. Note that we draw on the highly 
visual lone banana problem, and thus predominantly on image-generation AI, to illustrate both the 
surprise and revised understanding of generative AI. We will however show that it also applies to text-
generation AI, and large language models (LLM), and the often-heard surprise that these systems 
‘hallucinate’ or get relatively simple answers ‘wrong’ (e.g. Alkaissi and McFarlane 2023, Marcus 2022).   
AI algorithms are created by training large mathematical, probabilistic models, using artificial neural 
networks, with large amounts of training data (Goodfellow et al. 2016). In doing so, the neural networks 
encode aspects of our world from this data, a process called “learning”, in the form of either image or 
text patterns, depending on the kind of network (Bengio et al. 2013). This allows these networks to 
recognise objects in pictures, or language patterns. With generative AI this process is reversed, as the 
encoded patterns are used to generate new content. This is typically done via prompting, which triggers 
the generation of images or text, corresponding with what was described in the prompt.  
We begin with a brief introduction to problematisation and introduce in more detail the surprise at hand. 
We use this surprise to reveal widely held tacit assumptions about generative AI that stem from deeply 
held understandings about the nature of computing more broadly, both in terms of how algorithms 
work, and the nature of data in representing phenomena and entities in the world – assumptions that 
no longer hold true with AI. Instead of deterministic, rule-based systems, AI are probabilistic systems 
that make inferences from probabilistic data structures (Feuerriegel et al. 2020). 
Moreover, as we will show, the kinds of representations encoded by neural networks are very different 
to traditional forms of data representation, as aspects of the world become transcribed as patterns into 
high-dimensional numerical spaces (Asperti and Tonelli 2023). It is this feature that is at the heart of 
the astonishing capabilities of generative AIs, but also what makes them strikingly different in nature 
from traditional computing.  
Consequently, we argue that we need to set aside conventional understandings of computing and related 
expectations when dealing with generative AI. Instead, generative AI might be better understood as what 
we call “style engines”. As style engines, these systems, by their very nature, do not encode any kind of 
representations of our world, at least not in any traditional sense that is customary to our common 
understanding of computer systems. Rather, they encode the world purely as patterns that can act as 
styles in the generation of content (and synthesising entities). 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
3 
A style-based understanding of generative AI opens up a different realm of possibilities and different 
trajectories for optimising such systems, not for accuracy or veracity, but for creativity or generativity in 
a genuine sense of the word. Rather than trying to tie down generative AI to weed out hallucinations and 
inaccuracies, we suggest leaning into the “weirdness” of these systems, as this will open up new creative 
spaces for exploration. As a result, we contribute to IS a different approach to using and investigating 
generative AI, which we hope will itself prove generative of new knowledge and insights. 
2 Problematisation and the surprise of the ‘lone banana problem’ 
Our objective is to surface assumptions underpinning the common narratives in IS, and society at large, 
concerning the nature of generative AI, and to challenge these as a way of advancing our understanding 
of generative AI systems. For doing so, we draw on ‘problematisation’, a dialectic research approach 
championed in organisation studies by Alvesson and co-authors (Alvesson and Kärreman 2007; 
Alvesson and Sköldberg 2009; Alvesson and Sandberg 2011). The aim of problematisation is to uncover 
the normally unstated assumptions that give rise to ‘problematic’, or ‘surprising’ phenomena.  
Surprises are known to arise from a mismatch between expectation (informed by implicit assumptions) 
and actuality (empirical observations) (Casati and Pasquinelli 2007; Maguire et al. 2011). This makes 
surprising observations a trigger candidate to engage in problematisation, with the aim to surface those 
tacit assumptions that rendered this experience surprising in the first place, and then to analyse and 
identify why these assumptions were inadequate to understand what occurred.  
Problematisation has been employed in IS and Organisational Studies previously for several reasons: to 
challenge existing assumptions and develop interesting new theory (Alvesson and Kärreman 2007; 
Riemer and Johnston 2019; Hafermalz at al. 2020), to derive new research questions (Alvesson and 
Sandberg 2013), or more generally to think outside the box (Alvesson and Sandberg 2011). Here we 
follow the first application, using principles outlined in Alvesson and Kärreman (2007), to theorise the 
nature of generative as a particular kind of system, what we will term ‘style engines. We begin by 
recounting in detail the surprise presented by the lone banana problem, before we use it to problematise 
our collective, widely held understanding of generative AI. 
2.1 The surprise presented by the lone banana problem 
The inability of Midjourney, one of the most popular and widely used image generation AIs, to generate 
a picture of a single banana, came as a surprise to both the author Daniel Hook, who discovered and 
described the problem (Hook 2023), and to the author team. How difficult can it be to get an image 
generation AI, which is known for its ability to create wonderful, intricate, complex and artistic images 
(Ivanenko 2022; Heidorn 2022), to generate something as simple as a single banana?  
As it turns out – very! We share the results of our attempts in table 1. Pictures (a) and (b) show the 
images for the simple prompts “a banana” and “a single banana”. As can be seen, in each instance the 
system generated multiple bananas, either bunches or at least pairs of bananas, but not ‘a’ or ‘a single’ 
banana, as requested. Hook, in his article, outlines in much detail his quest to formulate various more 
complex prompts, experimenting with requesting the AI to generate various backgrounds or shadows, 
or even to have an invisible monkey holding a banana, assuming that monkeys would typically hold just 
one banana, all to no avail (the monkeys were neither invisible nor holding just one banana). 
We followed in his footsteps and reasoned that more forceful, precise prompts, reinforcing the intent to 
receive just one banana, e.g. by using multiple different ways of describing said intent, would yield a 
better result. Picture (c) shows the output for the prompt “a photo of a single banana, just one banana, 
not many, ONE”. At first glance the images look promising, but closer inspection reveals that what looks 
like single bananas appears to suffer from a form of ‘split personality’, as in each instance it appears that 
either a second banana is located behind the prominent first one, or the banana is indeed split in two in 
weird and unnatural ways. It was almost like the AI was actively resisting the attempt to produce just 
one single banana1. 
Before we discuss what the lone banana problem reveals about our assumptions regarding the nature 
and working of generative AI, we want to note that the lone banana problem is not an outlier. It appears 
equally difficult, or impossible, to create a single grape (see picture (d)), or a single spear of asparagus, 
 
1 The author eventually found, rather randomly, a prompt that generated a single banana. We were also able to produce a single 
banana, by deriving a different kind of prompt, grounded in a new understanding of generative AI as style engines (see section 5). 
We also want to note that we created these images in July 2023, and that particular issues, once known, will likely be addressed 
by model fine-tuning. Issues like the lone banana problem nonetheless give us a unique insight into the nature of generative AI. 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
4 
among other things. And the issue is not confined to image generators either, as text-generating AIs 
come with their own surprising issues, such as their inability to solve simple maths problems (e.g. 
Zumbrun 2023) or their propensity to hallucinate (Alkaissi and McFarlane 2023). 
 
(a) “a banana” 
(b) “photo of a single banana” 
 
(c)“a photo of a single banana, just one 
banana, not many, ONE” 
(d) “a single grape” 
Figure 1: Visualising the lone banana problem 
2.2 Problematising generative AI 
The surprising results recounted above prompt us to examine assumptions that underpin how 
generative AI is currently understood in both the literature and more generally. In keeping with the 
problematisation approach (Alvesson and Kärreman 2007) and the nature of surprise (Casati and 
Pasquinelli 2007; Maguire et al. 2011) we reason that the surprise arises from certain widely held 
preconceptions about how generative AI works, and what it does, or rather should be doing. 
Generative AIs2 have made rapid and astounding advances in the recent past. Image generation AIs, 
such as Midjourney or Dall-E 2, have been shown to be able to create highly realistic, imaginative, or 
artistic images in ways that were unimaginable only a short while ago (Ivanenko, 2022, Heidorn, 2022). 
Similarly, OpenAI’s ChatGPT has seen phenomenal success and growth, attracting one million users in 
just 5 days after its release in November 2022 (Heaven 2023). What makes ChatGPT so attractive is the 
way in which it engages in seemingly human-like conversation using natural, every-day language, and 
its ability to be useful in a wide variety of text-related tasks (Teubner et al. 2023). The capabilities of 
generative AIs have surprised even their developers (Roose and Metz 2023; Roberts et al. 2020). 
 
2 We refer to AIs in the plural when referring to concrete instances of AI, or tools, and in the singular when talking about the 
abstract concept of artificial intelligence. 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
5 
Yet, while commentators have been in awe about the ability of these systems to produce plausible and 
useful text answers (Roose 2023a), or realistic images (Hsu and Meyers 2023), much has also been said 
about their propensity to “hallucinate” falsehoods in text or inaccuracies in images (Alkaissi and 
McFarlane 2023, Marcus 2022), such as the lone banana problem. And yet, there seems to be an 
unwavering belief that, while such shortcomings “still” exist, generative AI systems will improve over 
time as issues with accuracy are progressively weeded out (Roose 2023b).  
Contrary to this belief, here we ask, what if expectations of accuracy are ill-founded and grounded in a 
misguided understanding of how these systems work? It is this question that is at the heart of our 
problematisation of generative AI. Expectations of accuracy appear to be grounded in a tacit belief that 
these neural networks, somehow, and much has been written about their inscrutability and black box 
nature (e.g. Castelvecchi 2016; Berente et al. 2021), encode in their probabilistic structures a 
representation of our world. An image model that can recognise with very high accuracy a banana in a 
picture, will therefore have encoded an understanding of what the object we label a banana looks like. It 
is against this understanding that the surprise arises, because an image generation AI should have no 
problem generating a picture of said object, in particular given its otherwise amazing abilities to create 
all kinds of objects, landscapes, sceneries. 
In response, we argue that the issue lies rather with our tacitly held assumptions regarding what kind of 
“understanding” of our world these networks encode. What if these networks did not encode any 
understanding of objects at all? What if they had a very alien way of appropriating our world, a way that 
does not have a concept of objects, or a distinction between objects and their properties? And what if, 
similarly, LLMs did not encode knowledge in ways we as humans conceptually make sense of the world? 
What if these networks did something not comparable at all to how we understand the world? 
We will show that understanding the difference in “representation” matters, and that accuracy or 
veracity might be the wrong goals to optimise for, because as probabilistic systems generative AIs will 
never attain the kind of accuracy we commonly expect from computer systems in representing aspects 
of our world in data. In turn however, deriving a different understanding, as style engines, will open up 
a different trajectory for optimisation – for creativity, or generativity, with new opportunities of utilising 
generative AI, as style engines, in conjunction with other, data or knowledge-based systems. 
2.3 Expectations derived from common-sense conceptions of computing 
Our established pre-understanding governs how we normally encounter phenomena in the world 
(Alvesson and Sandberg 2022), setting aside of which is required to construct phenomena as truly novel 
(Alvesson and Sandberg 2023), in particular relevant when the world change or new technologies 
emerge. Our pre-understanding of computing comes with certain tacit assumptions about the nature of 
algorithms and data, which in turn results in expectations about how computer tools ought to behave. 
We argue that these tightly held assumptions get in the way of understanding (generative) AI on its own 
terms, as a different kind of computing, even though much has been written about its probabilistic, 
rather than deterministic, nature (e.g. Gustavsson and Ljungberg 2021; Berente et al. 2021). 
Traditionally, at a foundational level, computing is understood as a combination of instructions or logic 
(in the form of algorithms) and certain representations of the world (in the form of data structures) that 
are being acted on, or manipulated by, said instructions (Dourish 2016). 
And traditionally, the term algorithm refers to “any well-defined computational procedure that takes 
some value, or set of values, as input and produces some value, or set of values, as output. An algorithm 
is thus a sequence of computational steps that transform the input into the output.” (Cormen et al 2009) 
Hence, “every step of the procedure is explicitly specified by its human designers and written down in a 
general-purpose programming language” (Kearns and Roth 2019, p. 6). As a result, algorithms are 
associated with accuracy, consistency and auditability (Gustavsson and Ljungberg 2021). Yet, given its 
probabilistic nature, artificial intelligence breaks with this understanding of ‘algorithms’, as AIs make 
predictions or generate content based on probability, not logic, with no ability to reason or judge based 
on knowledge (Cantwell Smith 2019; Dourish 2016). 
Similarly, the term data, or information, is associated with accuracy, veracity and faithful representation 
of (real-world) phenomena (Burton-Jones et al. 2017). In fact, it has been argued that “the provision of 
representations” is the very purpose of an information systems (ibid.), whereby IS are said to be built to 
represent “someone’s or some group’s perception of the real-world system that the information system 
has been built to model” (Wand and Weber 1995, p. 206). Hence, it is in the nature of an IS that the 
information it stores corresponds with the conceptual understanding humans have of their world. As we 
will show, AI, with its well-known black-box characteristics (Castelvecchi 2016), breaks with this 
conception quite radically (and thus would technically not even qualify as an IS on the above definition). 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
6 
Consequently, on the one hand we argue that, while much has been said about the different nature of AI 
as a form of computing (e.g. Castelvecchi 2016; Berente et al. 2021), we still tacitly hold it to the same 
yardstick as traditional computing, that is veracity, accuracy and faithfulness. On the other hand, we will 
show how exactly AI differs from traditional computing, and how setting aside traditional expectations 
might open up a conversation about what exactly is novel about generative AI, and how this novelty can 
be exploited beyond attempts to reign in its alienness in the pursuit of traditional goals of accuracy. We 
begin by explaining how generative AIs work, before characterising them as style engines. 
3 How generative AIs work 
Generative AIs have been defined as a class of artificial intelligence algorithms that are capable of 
creating new and diverse content, such as text, images, audio, or video, after being trained on extensive 
sets of data of the respective types (Goodfellow et al. 2016). These algorithms, through their ability to 
discern and encode underlying patterns, structures, and associations within their training data (Bengio 
et al. 2013), can thus generate new, plausible, and often surprisingly useful results. 
Unlike AI systems trained to accomplish particular tasks in organizational contexts, generative AI, such 
as ChatGPT, Google BARD, DALL-E or Midjourney, are based on large, task-agnostic neural networks 
called foundation models. Foundation models are “trained on broad data (generally using self-
supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks” 
(Bommasani et al., 2021), that is without specifically or explicitly being trained to do so. 
Structurally they are based on artificial neural networks, often referred to as deep learning models, 
because of their deep layered structures. This structure enables these models to encode hierarchical 
features, with each layer capturing different levels of abstraction (LeCun et al., 2015), and thus different 
features, aspects, or indeed styles, encoding them into what is commonly referred to as the “latent space” 
(Kingma and Welling 2013; Asperti and Tonelli 2023). As we will argue, it is this latent space, the layered 
structure in which any feature of the training data becomes encoded, that is at the heart of generative 
AIs’ ability to encode, (re)generate and combine intricately detailed patterns, or styles.  
At the same time, it becomes clear that these AI models do not store, in any traditional sense of the word, 
any data. They do not hold any faithful representation of the text or images they are trained on, nor the 
content or concepts contained in those, in any way humans would understand or represent themselves. 
Rather, they transform any aspect or features of the content into patterns, as they become mapped into 
the latent space, a purely probabilistic, numerical space.  
One consequence of this kind of mapping is that the resulting probabilistic representation in these 
models cannot be mapped back onto understandable features, or aspects of our world, such as labels or 
attributes (Asperti and Tonelli 2023). It is this feature that renders these models, with their underlying 
neural networks, into black boxes in terms of how exactly they “learn” any meaningful features or 
characteristics from the data (ibid.). In the following we first outline briefly how image and text 
generation AIs work, before we introduce why we refer to generative AIs as sophisticated style engines. 
3.1 Image-generation AIs: GANs and Diffusion models 
Image generation AIs, such as Midjourney, create new images, or modify existing ones, based on more-
or-less elaborate text inputs. They combine the power of large language models with techniques to 
generate images from textual descriptions (Ramesh et al. 2020), such as generative adversarial networks 
(GANs) or, more commonly used today, so-called diffusion models. 
GANs are an early technique for creating image generators; they were introduced by Goodfellow et al. 
(2014) and first widely popularized by OpenAI’s first DALL-E release. GANs consist of two neural 
networks, a generator, and a discriminator, which are trained simultaneously in a process called 
adversarial training. The generator creates synthetic images, while the discriminator evaluates their 
quality to distinguish them from real ones. The generator and discriminator engage in competition, with 
the generator trying to create images that the discriminator cannot differentiate from real ones, and the 
discriminator striving to become better at identifying fake images, in the process inscribing ever more 
sophisticated patterns into the generator’s layered latent space by way of back-propagation. This process 
continues until the generator produces synthetic images that are indistinguishable from real ones. 
As a more recent, and now widely adopted technique, diffusion-based image synthesis, as found in 
DALL-E 2, Midjourney or Stable Diffusion, is based on the concept of denoising (Ho et al. 2020). Such 
diffusion processes involve gradually corrupting an original image with noise over a series of steps, 
resulting in a completely noisy image, in the process “learning” its features. The idea behind diffusion 
models is to reverse this process, starting from a noisy image and removing the noise step by step to 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
7 
generate a new one. Diffusion models have demonstrated highly sophisticated results across various 
applications, such as image synthesis or inpainting (Karras et al. 2019). 
Regardless of approach, in each case a complex network with a deep layered structure encodes various 
aspects of an image. As such, it is important to note again that these AI models do not store any image 
data. Rather, the features and styles of images get encoded into the latent space, the multiple, specialized 
layers of neural networks. The deeper the layer and the wider the network structure, the better a network 
is able to discern between detailed and nuanced patterns (Devlin et al. 2018). 
3.2 Text-generation AIs: LLMs and Transformers 
Large language models (LLMs), such as GPT (Generative Pre-trained Transformer), work by leveraging 
the above-described deep learning techniques to encode (or “learn”), and in turn generate human-like 
text. GPT is pre-trained on a massive corpus of textual data from various sources to encode the statistical 
relationships between words and phrases during the (generally unsupervised) training process (Radford 
et al., 2018). A detailed exposition of the functioning of LLMs can be found in Lee and Trott (2023). 
It is important to note that, naming-wise, GPT refers to the LLM, or foundation model, while ChatGPT 
refers to the fine-tuning layer that sits on top of the foundation model with the aim to improve its 
conversational abilities, such as answering questions, providing recommendations, or offering support. 
This is made possible by additional techniques such as Reinforcement Learning from Human Feedback 
(RLHF) (Kreutzer et al. 2021). RLHF employs human testers to quality-rank responses generated by the 
foundation model to derive a reward function for reinforcement learning. This feedback loop allows the 
AI system to better align its outputs with human expectations. 
A significant innovation that made possible the current performance (and the ‘T’ in GPT) is the 
transformer architecture, introduced by Vaswani et al. (2017). It employs a so-called self-attention 
mechanism that allows the model to weigh the importance of different words and their relationships in 
a given context, enabling it to capture complex and contextual relationships within the input prompt. 
This allows discerning contextually ambiguous word relationships, such as in the sentence “hot dogs 
need water”, a problem that traditionally proved difficult to solve in NLP (Turney and Pantel 2010). 
Most relevant for our argument is the structure and nature of the underlying foundation model, namely 
its multi-layer architecture which encodes language patterns. Like image models, the transformer 
architecture consists of multiple layers that enable the model to learn increasingly complex and abstract 
features of its textual training data encoded into a numerical, high-dimensional latent space (Vaswani 
et al. 2017). With multiple layers, the model can capture and “represent” increasingly higher levels of 
abstraction, effectively learning hierarchical features, such as linguistic, semantic, formatting styles, 
from the data. As training data is fed through the layers, lower layers encode basic patterns, while higher 
layers combine these patterns to encode more complex and abstract stylistic elements, such as tone, 
writing styles, or genre-specific patterns. The depth of the network, determined by the number of layers, 
influences its capacity to discern and represent various stylistic patterns in the data (Devlin et al. 2018). 
Importantly again, no text is stored in these models. Rather, when a prompt is fed to the model, each 
word becomes represented as a numerical, high-dimensional, vector. For example, “the most powerful 
version of GPT-3 uses word vectors with 12,288 dimensions—that is, each word is represented by a list 
of 12,288 numbers.” (Lee and Trott, 2023) Conceptually, this means that words do not possesses any 
textual content, but become characterised purely as numerical ‘nearness relationships’ with other words; 
for example, ‘banana’ might be characterized as yellow-ness, fruit-ness, healthy-ness, sweet-ness, 
kitchen-ness, fruit-basket-ness, and much more. We can then begin to see the alienness of this form of 
representation, which is purely relational, or stylistic, where each word is constituted as a mix of styles, 
and each style can potentially be applied to any other to constitute or generate new text sequences3. 
Loosely speaking, as input words are processed by the model they become progressively characterized 
by a mix of styles as they are fed through the so-called attention layers of the language model (forming 
their word vectors as near-nesses with other words), whereby each word collects all the styles it is closely 
related to (first within the context of the input prompt, and then against the encoded patterns in the 
neural network). In a second step the so-called feed-forward layers preform style-matching between 
those input vectors and the patterns encoded in the network, to generate as a response those words (or 
sequences of words) that are most alike with what was requested in the input prompt. 
 
3 Lee describes how this feature enables interesting “computations” on text, such as “king minus man plus woman equals queen” 
or “Berlin minus Germany plus France equals Paris” – it is evident in these examples how concepts are encoded as styles – in 
these examples “gender-ness” and “royal-ruler-ness”, or “capital-city-ness” and “country-ness”. 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
8 
4 Generative AIs as style engines 
As we have shown, artificial intelligence in general, and generative AI in particular, present a very 
different kind of computing, a different way of representing and encoding aspects of our world. Artificial 
neural networks do not represent the world like we do, as objects with properties, but as complex, 
layered patterns. In this section we link this kind of representation more explicitly to the idea of styles. 
We begin by outlining how styles, in a narrow sense, have already been a topic in generative AI, before 
we take styles beyond a common-sense understanding, and ask what happens when all aspects of the 
world are encoded as styles, available for creation, where the distinction between objects and styles, 
between substance and appearance dissolves. We end by discussing the usefulness of generative AI. 
4.1 Styles in the field of generative AI 
In art and film, a style is defined as a distinctive and recognizable way in which an artifact is created 
(Gombrich 1968), expressed in the (audio-)visual characteristics or features of an artwork or film that 
can be attributed to a particular artist/maker, culture, period, or movement, and thus permits 
recognition and grouping of said artifacts (Fernie 1995). Style encompasses a wide range of aspects, 
including composition, colour, lighting, texture, camera angles, editing techniques, pacing, dialogue, 
and storytelling. It is a way of expressing a unique perspective on a subject matter. 
Styles are a known concept in the context of AI generally and generative AI specifically. Research in 
computer science has already demonstrated in much detail that the neural networks underpinning 
generative AI excel at encoding styles (Radford et al. 2015). It has been shown for example that it is 
possible to distil style from one image to transfer it to and blend with the content of another (e.g. Gatys 
et al. 2016; Huang and Belongie 2017). Styles are also a much-discussed topic in the context of prompt 
engineering, the art of instructing generative AI models to produce specific kinds of content (e.g. Scharth 
2023), whereby prompts contain certain keywords to deliberately evoke particular styles. 
As such, for GPT, 'styles' refer to writing styles, such as formal, informal, poetic, academic, or journalistic 
styles, or different genres such as letter text, tables, scripts, conversations, or even computer code. As 
GPT is trained on a massive corpus of text from various sources, it learns to identify and reproduce these 
styles by recognizing patterns in the training data. When prompted, GPT can generate text in a specific 
style or apply a style to a text submitted to it, e.g. such as organising text into a table. 
In the case of image generation AI, 'styles' in a narrow sense refer to the visual patterns, textures, and 
artistic elements (such as the expressions used by certain recognized artists or movements) present in 
the input images, that can be evoked via prompting, such as “in the style of” impressionism, cubism, or 
modern art. Further building on the ability to encode and generate with styles, a new experimental 
generative AI model, StyleGAN, has been proposed to explicitly separate high-level style attributes (e.g., 
pose, identity) from lower-level variations (e.g., colour, texture) to better control the generation process 
and produce images with very detailed artistic styles (Karras et al. 2019). 
4.2 Dissolving the distinction between content and style 
Implicit in the above understanding of styles is the distinction between style and content. In the most 
general sense, a style is not the thing itself, but the way in which the thing is made or represented, thus 
merely a feature of the thing. For example, in everyday language, we commonly make a distinction 
between the content of a picture (e.g. a banana) and its style (e.g., depicted in the style of Picasso). 
But as we’ve already shown above, neural network AI models, neither LLMs nor image generation AI, 
store any content at all, be it text or images, in the traditional sense. Instead, they “learn” to represent 
any underlying patterns, structures, and features that characterize the input data, and encode these 
patterns in the form of complex, high-dimensional representations within the numerical weights of their 
very large and layered neural networks. We thus assert that what these models are in fact sophisticated 
‘style engines’; they turn everything, every aspect of their input data, into likenesses available for 
creation and combination. We propose that there is conceptual value in conceiving of all patterns that 
AI models encode as styles, as they capture the inherent structure, relationships, and characteristics 
present in the training data. In this sense, 'styles' serve as a broader term to describe the various features 
and regularities that AI models learn from their training data, regardless of domain or modality. 
In language models like GPT, the patterns encoded from training data thus include not only writing 
styles in a traditional sense but all aspects of grammar, syntax, semantics, and structure. These patterns 
help the model generate coherent, contextually appropriate text that reflects the diversity of human 
language with all its intricacies. In this sense, the model learns language as a network of 'styles' that 
capture the essence of the input data, that can then be generalized across domains and tasks. 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
9 
Similarly, in image models, the patterns learned from the input data encompass a wide range of visual 
features, such as shapes, textures, colours, and spatial relationships. These patterns can be considered 
as 'styles' that define the visual characteristics of the learned and generated images. In addition to artistic 
styles, the model learns to generate images that adhere to the 'style' of specific object classes, such as 
faces, animals, fruit, or landscapes, by capturing the common patterns and structures inherent in these 
entities. Thus, even banana is turned into a style – banana-ness. 
5 Discussion 
Understanding generative AIs as style engines will allow us to foreground the creative, and indeed 
generative, uses of this new technology and move beyond attempts to map it on established 
understandings and categories, which has rendered its most defining characteristics into deficiencies. 
As we have argued, generative AIs by their very nature do not store exact representations of input data, 
which would enable them to reproduce learned knowledge with precision and accuracy. On such an 
understanding generative AIs naturally ‘make mistakes’ or ‘hallucinate’. Once understood as style 
engines, precision and accuracy will no longer be expected, and its creative potential can be appreciated.  
5.1 What does it mean when every thing becomes a style? 
If neural networks do not store any content, but turn everything into styles, what does this mean? When 
everything, every aspect of a text or image, is turned into styles, the very ‘essence’ of what makes an 
entity, or thing, is encoded into the network as a kind of like-ness, or “thing-ness”, akin to a style. And 
this is our main point. We assert that the way in which large foundation models work, the way in which 
they encode everything as patterns, means that much of what is traditionally thought of as the thing 
itself, as content, is turned into styles as well.  
For instance, when trained on images of wood, an image model will learn 'wood-ness' by capturing the 
visual patterns, textures, and colour variations that characterize (different kinds of) wood. When trained 
on bananas, the model will learn banana-ness. Similarly, when trained on a dataset of poems, GPT will 
learn the 'poem-ness' or the characteristics that define the structure, language, and style of poetry; when 
trained on text written by management consultants it will encode “consultant-ness” as a style, or voice, 
that can be evoked explicitly when generating content. These models thus use these learned 
representations, or styles, to generate new content that exhibits the same characteristics or essences, 
such as generating a new poem, consulting report or a wood-like texture, in ways that make them 
creative in new and interesting ways. 
What makes this conception of generative AI so powerful is the way in which styles behave. It is in the 
nature of a style that it is applied to something. But if every thing becomes a style, if everything is just 
thing-ness, those styles can be endlessly combined to bring into being new things, in that styles are being 
evoked to synthesize, or in the literal sense of the word, generate an object, be that a picture or a text, 
that might go beyond what might naturally occur in our world. We assert that this is what happens when 
patterns are mapped from the latent space back into our world, via prompting. 
5.2 Return to the banana 
Against traditional assumptions underpinning computing, and how data is commonly represented, the 
lone banana problem appears as a surprising problem. A single banana should be the simple case, from 
which more complex cases are combined, e.g. by adding more bananas to make bunches. But as we have 
seen, this is not how AI works. Neural networks do not possess an object ontology; this is not how ‘their 
world’ is encoded, they do not possess the capacity to grasp the concept of an object or a single banana 
as such; it encodes banana-ness. But if banana-ness is taken to be a style, instead of an object, 
Midjourney’s output makes more sense. Average banana-ness would then be bunches of banana, as this 
is how ‘banana’ mostly occurs. Moreover, as the pictures in figure 1 show, banana-ness also comes with 
other style aspects, such as colour, shape and positioning (all bananas seem oddly upright and floating). 
And yet, from within the traditional understanding, this phenomenon would be diagnosed as a problem 
of biased data. And indeed, this is where Hook (2023) locates the issue initially, as a matter of bias; too 
many pictures in the training data set show banana bunches, not single bananas, so the network fails to 
learn what a banana is. But the author also points out, much in line with our overall argument, that AIs 
“don’t perceive objects in the way that we do – they understand commonly occurring patterns. Their 
reality is fundamentally different to ours – it is not born in the physical world but in a logical world.” 
What we argue is that we need to go one step further and understand what exactly this “logical world” 
looks like. We have shown that this is a world purely made of, endlessly combinable, styles. Note that 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
10 
our argument is not to say that it is not worthwhile, or indeed useful, to strive for getting these systems 
to produce singular bananas (and indeed the creators of these systems will likely fine-tune the image 
model to be able to do so), but that we need to approach this issue on the basis of a more appropriate 
understanding of how these systems work; their particular nature as style engines.  
And at this point we return to exploring banana-ness with Midjourney. We want to offer three more 
examples to illustrate the nature of banana as a style. The first is a curious effect that Hook already 
mentioned in passing in his article, but could not make sense of, as he observed that “slightly bizarrely, 
several of the monkeys ended up wearing bananas or being banana-coloured”. We encountered the 
same issue (see figure 2 (a)), which no longer is as surprising when we understand generative AIs as 
style engines. In our observation, these style engines tend to have issues with what we term ‘style bleed’, 
where styles defy our expectations of entities and their appearance, grounded in how we understand the 
objects in our world, where banana-ness combines with (or bleeds into) monkey-ness in weird ways.  
However, this effect of style bleed, or the ability to almost endlessly combine styles, is also what makes 
generative AI so truly generative or creative. Our second and third examples (in figures 2 (b) and (c)) 
show what happens when banana-ness is combined with car-ness or office-ness. It is this ability to use 
the ‘thing-ness’ of just about everything (such as banana) as styles, in addition to aspects that are 
traditionally understood as styles, which makes generative AI, as style engines, powerful new creative 
tools with never-seen-before abilities, useful in a wide variety of creative tasks or industries. 
(a) “a monkey with a banana” 
(b) “a car shaped like a banana” 
(c) “an office in the style of bananas, with desk 
in banana shape, banana coloured” 
(d) “a sausage that looks like a banana” 
Figure 2: Visualising “banana-ness” and combining styles to create a single banana 
How then would we create a single banana within this new-found understanding? Could we combine 
banana-ness with another style that would get us the desired outcome? How about ‘sausage-ness’? 
Sausages often occur in singles and they are similar in style to bananas. If we were able to ‘bleed’ banana-
ness into ‘sausage-ness’, might we be able to generate a single banana? And indeed this approach yielded 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
11 
the intended result. Figure 2 (d) shows the result for the prompt “a sausage that looks like a banana”. As 
we can see, some results combine the styles in rather weird ways (one even yields the same old two-
banana output), but one of the images clearly shows a single banana. 
5.3 On the usefulness of style engines 
Understanding generative AIs as style engines problematises common conceptions, in particular those 
of text-based ones, where LLMs are often enacted as knowledge-based assistants or process automation 
systems (Chui et al. 2022), as such applications require a high degree of accuracy and reliability that 
style engines by their very nature are not able to deliver.  
We have shown that generative AI technically do not store any content, in the sense of facts or actual 
images, but trade exclusively in likenesses, that become encoded, as patterns or styles, in their latent 
space. And yet, it is this very characteristic, the transposition of entities as representations into the latent 
space, that is behind the widely discussed problems these systems have with accuracy or veracity, 
whereby their models “get things wrong”, or “hallucinate” (Alkaissi and McFarlane 2023, Marcus 2022).  
Yet, when we set aside traditional computing expectations, in terms of accuracy and reliability, and 
instead come to understand generative AIs as style engines, these “problems” are no longer surprising 
but rather the result of misplaced expectations. Given that these systems have no ability to recall exact 
information, but rather provide probabilistic renditions that are ‘like’ what is requested (e.g. image 
models are good at creating hand-like objects, having encoded hand-ness in their latent space, but they 
struggle with the exact number of fingers), they are fundamentally unreliable systems (Dixit 2023). 
But because likenesses get encoded at a deeply detailed level, they are also fundamentally creative 
systems. Again, we argue that this is one of the most unique and most useful characteristics of generative 
AIs. And in this respect it becomes clear that generative AIs are no replacements for expert systems, nor 
for search engines (Gozalo-Brizuela and Garrido-Merchan 2023), but rather that their unique creative 
features make them useful in conjunction with such systems.  
For example, when combined with a live search index, large language models can be useful for both 
deriving better search queries and presenting search results in more creative and useful ways (Mehdi 
2023). Given that ‘query-ness’ will be one of the many encoded styles in the model, it can be exploited 
to create improved search queries from user prompts. Conversely, any results returned from the live 
index can then be transformed by the model and presented in line with user expectations, without the 
user having to trawl through websites to retrieve information themselves. Similarly, generative AI can 
be combined with a range of corporate systems, or with bespoke data sets (e.g. Anghel 2023), both text 
or images, to create new kinds of systems that exploit the creative qualities of generative AIs without 
having to suffer from its inherent lack of “accuracy”. 
6 Conclusion 
We used a surprising observation, the inability of the image-generation AI Midjourney to create a picture 
of a single banana, to problematise our common conceptions of generative AI. We argue that common-
sense conceptions of computing get in the way of understanding AI, as probabilistic systems, on their 
own terms. We propose to view generative AIs as style engines that encode very alien representations of 
the world, challenged by traditional expectations of accuracy and veracity, but excelling at novel kinds 
of creativity, as they make available all kinds of aspects of the world as styles, to be endlessly combinable 
in the creation of new content, be it images or text. 
For IS, generative AIs pose an interesting challenge to traditional assumptions and our common notions 
of ‘information systems’. In a strict sense of the word, AIs are not information systems, as they do not 
faithfully represent aspects of the world. This insight will challenge many established conceptions of 
how systems are designed, deployed and used, and their relationship to real-world phenomena.  
Future research is required to understand better the inner workings of generative AI to deepen our 
understanding as style engines. When viewed as such, we expect its usefulness will lie in its ability to 
add novel, generative capabilities to existing systems. When combined with systems that harness the 
strengths of traditional computing (accuracy and reliability) generative AI can usefully bring creative 
and imaginative qualities to the equation, with the prospect of developing new kinds of systems. Given 
its novelty, this offers a plethora of new research opportunities for the field, research questions such as, 
‘how are established notions of computational creativity challenged by generative AI?’, ‘what are the 
most promising areas of enterprise application, given its inherent unreliability in a traditional sense?’, 
or, more philosophically, ‘what can we learn from generative AI, as an alien form of intelligence, about 
technology-mediated forms of perception and reality construction?’. 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
12 
7 References 
Alkaissi, H., & McFarlane, S. I. 2023. "Artificial Hallucinations in ChatGPT: Implications in Scientific 
Writing," Cureus (15:2), pp. e35179. doi: 10.7759/cureus.35179.  
Alvesson, M., and Kärreman, D. 2007. "Constructing Mystery: Empirical Matters in Theory 
Development," Academy of Management Review (32:4), pp. 1265-1281. 
Alvesson, M., and Sandberg, J. 2011. "Generating Research Questions through Problematisation," 
Academy of Management Review (36:2), pp. 247-271. 
Alvesson, M., and Sandberg, J. 2013. Constructing Research Questions: Doing Interesting Research. 
London: SAGE. 
Alvesson, M., and Sandberg, J. 2022. “Pre-understanding: An interpretation-enhancer and horizon-
expander in research,” Organization Studies, (43:3), pp. 395-412. 
Alvesson, M. and Sandberg, J. 2023,” The Art of Phenomena Construction: A Framework for Coming 
Up with Research Phenomena beyond ‘the Usual Suspects’,” Journal of Management Studies, 
https://doi.org/10.1111/joms.12969 
Anghel, I. 2023. “PwC Introduces AI Chatbot for 4000 Lawyers to Speed Up Work.” 
(https://www.afr.com/companies/professional-services/pwc-introduces-ai-chatbot-for-4000-
lawyers-to-speed-up-work-20230322-p5cu72, accessed 14-08-2023). 
Asperti, A., and Tonelli, V. 2023. "Comparing the Latent Space of Generative Models," Neural 
Computing and Applications (35:4), pp. 3155-3172. 
Bengio, Y., Courville, A., and Vincent, P. 2013. "Representation Learning: A Review and New 
Perspectives," IEEE Transactions on Pattern Analysis and Machine Intelligence (35:8), pp. 1798-
1828. 
Berente, N., Gu, B., Recker, J., and Santhanam, R. 2021. "Special Issue Editor’s Comments: Managing 
Artificial Intelligence," MIS Quarterly (45:3), pp.1433-1450. 
Bommasani, R., et al. 2021. "On the Opportunities and Risks of Foundation Models," arXiv:2108.07258. 
Burton-Jones, A., Recker, J., Indulska, M., Green, P., and Weber, R. 2017. "Assessing Representation 
Theory with a Framework for Pursuing Success and Failure," MIS Quarterly (41:4), pp.1307-1333. 
Cantwell Smith, B. 2019. The Promise of Artificial Intelligence: Reckoning and Judgement. Cambridge, 
MA: The MIT Press. 
Casati, R., and Pasquinelli, E. 2007. "How Can You Be Surprised? The Case for Volatile Expectations," 
Phenomenology and the Cognitive Sciences (6:1-2), pp. 171-183. 
Castelvecchi, D. 2016. “Can We Open the Black Box of AI?,” Nature (538:7623), pp. 20-23. 
Chui, M., Roberts, R., and Yee, L. 2022. “Generative AI is Here: How Tools Like ChatGPT Could Change 
Your Business.” (mckinsey.com, accessed 14-08-2023). 
Cormen, T.H., Leiserson, C., Rivest, R., et al. 2009. Introduction to Algorithms. Third ed. Massachusetts 
London, England: MIT Press. 
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. 2018. BERT: Pre-training of Deep Bidirectional 
Transformers for Language Understanding, arXiv:1810.04805. 
Dixit, 
P. 
2023. 
“Why 
Are 
AI-Generated 
Hands 
So 
Messed 
Up?.” 
(https://www.buzzfeednews.com/article/pranavdixit/ai-generated-art-hands-fingers-messed-
up, accessed 14-08-2023). 
Dourish, P. 2016. “Algorithms and Their Others: Algorithmic Culture in Context,” Big Data & Society 
(3:2), pp. 1–11. 
Fernie, E. 1995. Art History and Its Methods: A Critical Anthology. London: Phaidon. 
Feuerriegel, S., Dolata, M., & Schwabe, G. 2020. "Fair AI," Business & Information Systems Engineering 
(62:4), pp. 379-384. 
Gatys, L. A., Ecker, A. S., and Bethge, M. 2016. "Image Style Transfer Using Convolutional Neural 
Networks," Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 
pp. 2414-2423. 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
13 
Gombrich, E. 1968. "Style," In International Encyclopedia of the Social Sciences, ed. D. L. Sills, xv. New 
York. 
Goodfellow, I., Bengio, Y., and Courville, A. 2016. Deep Learning. Cambridge, MIT Press. 
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and 
Bengio, Y. 2014. "Generative Adversarial Nets," Advances in Neural Information Processing 
Systems, pp. 2672-2680. 
Gozalo-Brizuela, R., and Garrido-Merchan, E. C. 2023. "ChatGPT is Not All You Need. A State of the Art 
Review of Large Generative AI Models," arXiv:2301.04655. 
Gustavsson, M., and Ljungberg, J. 2021. "Algorithms and Their Work: A Performativity Perspective." 
(https://aisel.aisnet.org/scis2021/7, accessed 14-08-2023). 
Hafermalz, E., Johnston, R., Hovorka, D., and Riemer, K. 2020. "Beyond 'Mobility': A New 
Understanding of Moving with Technology," Information Systems Journal (30:4), pp. 762-786. 
Heaven, 
W. 
D. 
2023. 
“ChatGPT 
is 
Everywhere. 
Here’s 
Where 
It 
Came 
From.” 
(https://www.technologyreview.com/2023/02/08/1068068/chatgpt-is-everywhere-heres-
where-it-came-from, accessed 14-08-2023). 
Heidorn, 
C. 
2022. 
“Mind-Boggling 
Midjourney 
Statistics 
in 
2022.” 
(https://tokenizedhq.com/midjourney-statistics/, accessed 14-08-2023). 
Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. 2020. "Denoising Diffusion Probabilistic Models," 
arXiv:2006.11239. 
Hook, D. 2023. “The Lone Banana Problem. Or, the New Programming: ‘Speaking’ AI.” 
(https://www.digital-science.com/tldr/article/the-lone-banana-problem-or-the-new-
programming-speaking-ai/, accessed 14-08-2023). 
Hsu, T., and Myers, S. L. 2023. “Can We No Longer Believe Anything We See?.” 
(https://www.nytimes.com/2023/04/08/business/media/ai-generated-images.html, accessed 
14-08-2023). 
Huang, X., and Belongie, S. 2017. "Arbitrary Style Transfer in Real-Time with Adaptive Instance 
Normalization," Proceedings of the IEEE International Conference on Computer Vision, pp. 1501-
1510. 
Ivanenko, N. 2022. “Midjourney v4: An Incredible New Version of the AI Image Generator.” 
(https://mezha.media/en/2022/11/11/midjourney-v4-is-an-incredible-new-version-of-the-ai-
image-generator/, accessed 14-08-2023). 
Karras, T., Laine, S., and Aila, T. 2019. "A Style-Based Generator Architecture for Generative Adversarial 
Networks," Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern 
Recognition, pp. 4401-4410. 
Kearns, M., and Roth, A. 2019. The Ethical Algorithm: The Science of Socially Aware Algorithm Design. 
New York: Oxford University Press. 
Kingma, D. P., and Welling, M. 2013. "Auto-Encoding Variational Bayes," arXiv:1312.6114v11. 
Kreutzer, J., Raffel, C., and Shazeer, N. 2021. "Fine-Tuning Language Models from Human Preferences," 
arXiv:2103.00020. 
LeCun, Y., Bengio, Y., and Hinton, G. 2015. "Deep Learning," Nature (521:7553), pp. 436-444. 
Lee, T. B., and Trott, S. 2023. “A Jargon-Free Explanation of How AI Large Language Models Work.” 
(https://arstechnica.com/science/2023/07/a-jargon-free-explanation-of-how-ai-large-
language-models-work/, accessed 14-08-2023). 
Maguire, R., Maguire, P., and Keane, M. T. 2011. "Making Sense of Surprise: An Investigation of the 
Factors Influencing Surprise Judgments," Journal of Experimental Psychology: Learning, 
Memory, and Cognition (37:1), p. 176. 
Marcus, G. 2022. “AI Platforms Like ChatGPT are Easy to Use but Also Potentially Dangerous.” (x/, 
accessed 14-08-2023). 

Australasian Conference on Information Systems 
 
Riemer & Peter 
2023, Wellington 
 
The lone banana problem and the nature of generative AI 
 
 
14 
Mehdi, Y. 2023. “Reinventing Search with a New AI-Powered Microsoft Bing and Edge, Your Copilot for 
the Web.” (https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-
powered-microsoft-bing-and-edge-your-copilot-for-the-web/, accessed 14-08-2023). 
Radford, A., Metz, L., and Chintala, S. 2015. "Unsupervised Representation Learning with Deep 
Convolutional Generative Adversarial Networks," arXiv:1511.06434. 
Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. 2018. Improving Language Understanding 
by Generative Pre-Training. OpenAI. 
Ramesh, A., Pavlov, M., Goh, G., Ramachandran, S., Gray, S., Voss, C., Radford, A., Chen, M., and 
Sutskever, I. 2021. "Zero-Shot Text-to-Image Generation," arXiv:2102.12092. 
Riemer, K., and Johnston, R. 2019. "Disruption as Worldview Change: A Kuhnian Analysis of the Digital 
Music Revolution," Journal of Information Technology (34:4), pp. 350-370. 
Roberts, A., Raffel, C., and Shazeer, N. 2020. "How Much Knowledge Can You Pack into the Parameters 
of a Language Model?," arXiv:2002.08910. 
Roose, 
K. 
2023a. 
“GPT-4 
is 
Exciting 
and 
Scary.” 
(https://www.nytimes.com/2023/03/15/technology/gpt-4-artificial-intelligence-openai.html, 
accessed 14-08-2023). 
Roose, 
K. 
2023b. 
“A.I. 
Poses 
'Risk 
of 
Extinction,' 
Industry 
Leaders 
Warn.” 
(https://www.nytimes.com/2023/05/30/technology/ai-threat-warning.html, accessed 14-08-
2023). 
Roose, K., and Metz, C. 2023. “How to Become an Expert on A.I.,” On Tech Newsletter, The New York 
Times, March 27, 2023. 
Scharth, M. 2023. “How to Perfect Your Prompt Writing for ChatGPT, Midjourney and Other AI 
Generators,” The Conversation, (https://theconversation.com/how-to-perfect-your-prompt-
writing-for-chatgpt-midjourney-and-other-ai-generators-198776, accessed 14-08-2023). 
Teubner, T., Flath, C. M., Weinhardt, C., van der Aalst, W., and Hinz, O. 2023. "Welcome to the Era of 
ChatGPT et al.," Business & Information Systems Engineering (65:2), pp. 95-101. 
Turney, P. D., and Pantel, P. 2010. "From Frequency to Meaning: Vector Space Models of Semantics," 
Journal of Artificial Intelligence Research (37), pp. 141-188. 
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, 
I. 2017. "Attention is All You Need," Advances in Neural Information Processing Systems, pp. 
5998-6008. 
Wand, Y., and Weber, R. 1995. “On the Deep Structure of Information Systems,” Information Systems 
Journal (5:3), pp. 203-223. 
Zumbrun, J. 2023. “ChatGPT Needs Some Help With Math Assignments,” The Wall Street Journal, 
(https://www.wsj.com/articles/ai-bot-chatgpt-needs-some-help-with-math-assignments-
11675390552, accessed 15-08-2023) 
 
Copyright  
Copyright © 2023 [Kai Riemer, Sandra Peter]. This is an open-access article licensed under a Creative 
Commons Attribution-Non-Commercial 3.0 Australia License, which permits non-commercial use, 
distribution, and reproduction in any medium, provided the original author and ACIS are credited. 
 

