Reinforcement Learning in Low-Rank MDPs
with Density Features
Audrey Huang*†
Jinglin Chen*†
Nan Jiang†
Abstract
MDPs with low-rank transitions—that is, the transition matrix can be factored into the product of
two matrices, left and right—is a highly representative structure that enables tractable learning. The
left matrix enables expressive function approximation for value-based learning and has been studied
extensively. In this work, we instead investigate sample-efﬁcient learning with density features, i.e.,
the right matrix, which induce powerful models for state-occupancy distributions. This setting not only
sheds light on leveraging unsupervised learning in RL, but also enables plug-in solutions for convex
RL. In the ofﬂine setting, we propose an algorithm for off-policy estimation of occupancies that can
handle non-exploratory data. Using this as a subroutine, we further devise an online algorithm that
constructs exploratory data distributions in a level-by-level manner. As a central technical challenge,
the additive error of occupancy estimation is incompatible with the multiplicative deﬁnition of data
coverage. In the absence of strong assumptions like reachability, this incompatibility easily leads to
exponential error blow-up, which we overcome via novel technical tools. Our results also readily extend
to the representation learning setting, when the density features are unknown and must be learned from
an exponentially large candidate set.
1
Introduction
The theory of reinforcement learning (RL) in large state spaces has seen fast development. In the model-free
regime, how to use powerful function approximation to learn value functions has been extensively studied
in both the online and the ofﬂine settings (Jiang et al., 2017; Jin et al., 2020b,c; Xie et al., 2021), which also
builds the theoretical foundations that connect RL with (discriminative) supervised learning. On the other
hand, generative models for unsupervised/self-supervised learning—which deﬁne a sampling distribution
explicitly or implicitly—are becoming increasingly powerful (Devlin et al., 2018; Goodfellow et al., 2020),
yet how to leverage them to address the key challenges in RL remains under-investigated. While prior works
on RL with unsupervised-learning oracles exist (Du et al., 2019; Feng et al., 2020), they often consider
models such as block MDPs, which are more restrictive than typical model structures considered in the
value-based setting such as low-rank MDPs.
In this paper, we study model-free RL in low-rank MDPs with density features for state occupancy
estimation. In a low-rank MDP, the transition matrix can be factored into the product of two matrices, and
the left matrix is known to serve as powerful features for value-based learning (Jin et al., 2020b), as it can be
used to approximate the Bellman backup of any function. On the other hand, the right matrix can be used to
represent the policies’ state-occupancy distributions, yet how to leverage such density features (without the
knowledge of the left matrix) in ofﬂine or online RL is unknown. To this end, our main research question is:
*The two authors contributed equally to this work.
†Department of Computer Science, University of Illinois Urbana-Champaign.
Email:
audreyh5@illinois.edu,
jinglinc@illinois.edu, nanjiang@illinois.edu.
1
arXiv:2302.02252v1  [cs.LG]  4 Feb 2023

Is sample-efﬁcient ofﬂine/online RL with density features possible in low-rank MDPs?
We answer this question in the positive, and below is a summary of our contributions:
1. Ofﬂine:
Section 3 provides an algorithm for off-policy occupancy estimation. It bears similarity to
existing algorithms for estimating importance weights (Hallak and Mannor, 2017; Gelada and Bellemare,
2019), but our setting gives rise to a number of novel challenges. Most importantly, our algorithm enjoys
guarantees under arbitrary ofﬂine data distributions, when the standard notion of importance weights
are not even well-deﬁned. We introduce a novel notion of recursively clipped occupancy and show
that it can be learned in a sample-efﬁcient manner. The recursively clipped occupancy always lower
bounds the true occupancy, and the two notions coincide when the data is exploratory. Such a guarantee
immediately enables an ofﬂine policy learning result that only requires “single-policy concentrability”,
which is comparable to the most recent advances in value-based ofﬂine RL (Jin et al., 2020c; Xie et al.,
2021).
2. Online: Using the ofﬂine algorithm as a subroutine, in Section 4, we design an online algorithm that
builds an exploratory data distribution (or “policy cover” (Du et al., 2019)) from scratch in a level-by-
level manner. At each level, we estimate each policy’s state-occupancy distribution and construct an
approximate cover by choosing the barycentric spanner of such distributions. A critical challenge here
is that the additive ℓ1 error in occupancy estimation destroys the multiplicative coverage guarantee of the
barycentric spanner, so the constructed distribution is never perfectly exploratory. Worse still, standard
algorithm designs and analyses for handling such a mismatch easily lead to an exponential error blow-up.
We overcome this by a novel technique, where two inductive error terms are maintained and analyzed in
parallel, with delicate interdependence that still allows for a polynomial error accumulation (Figure 1).
3. Representation learning: We also extend our ofﬂine and online results to the representation learning
setting (Agarwal et al., 2020), where the true density features are not given but must also be learned from
an exponentially large candidate feature set.
4. Implications: Our online algorithm is automatically reward-free (Jin et al., 2020a; Chen et al., 2022b)
and deployment-efﬁcient (Huang et al., 2022). Further, since we can accurately estimate the occupancy
distribution for all candidate policies, our results enable plug-in solutions for settings such as convex RL
(Mutti et al., 2022; Zahavy et al., 2021), where the objectives and/or constraints are functions over the
entire state distributions (see Appendix C).
2
Preliminaries
Markov Decision Processes (MDPs)
We consider a ﬁnite-horizon episodic MDP (without reward) de-
ﬁned as M = (X, A, P, H), where X is the state space, A is the action space, P = (P0, . . . , PH−1) with
Ph : X × A →∆(X) is the transition dynamics, H is the horizon, and d0 ∈∆(X) is the known ini-
tial state distribution.1 We assume that X is a measurable space with possibly inﬁnite number of elements
and A is ﬁnite with cardinality K. Each episode is a trajectory τ = (x0, a0, x1, . . . , xH−1, aH−1, xH),
where x0 ∼d0, the agent takes a sequence of actions a0, . . . , aH−1, and xh+1 ∼Ph(· | xh, ah). We use
π = (π0, . . . , πH−1) ∈(X →∆(A))H to denote a (non-stationary) H-step Markov policy, which chooses
ah ∼πh(·|xh). (We will also omit the subscript h and write π(·|xh) when it is clear from context.) We use
ρ to refer to non-Markov policies that can choose ah based on the history x0:h, a0:h−1, which often arises
from the probability mixture of Markov policies at the beginning of an trajectory. Once a policy π is ﬁxed,
the MDP becomes an Markov chain, with dπ
h(xh) being its h-th step distribution. As a shorthand, we use
the notation [H] to denote {0, 1, . . . , H −1}.
1We assume the known initial state distribution for simplicity. Our results easily extend to the unknown version.
2

Low-rank MDPs
We consider learning in a low-rank MDP, deﬁned as:
Assumption 1 (Low-rank MDP). M is a low-rank MDP with dimension d, that is, ∀h ∈[H], there ex-
ist φ∗
h : X × A →Rd and µ∗
h : X →Rd such that ∀xh,xh+1 ∈X, ah ∈A : Ph(xh+1|xh, ah) =
⟨φ∗
h(xh, ah), µ∗
h(xh+1)⟩. Further,
R
∥µ∗
h(x)∥1(dx) ≤Bµ and ∥φ∗
h(·)∥∞≤1.2
Notation
We use the convention 0
0 = 0 when we deﬁne the ratio between two functions. Deﬁne a ∧
b = min(a, b), and we treat ∧as an operator with precedence between “×/” and “+−”. When clear
from the context, {□h} = {□h}H−1
h=0 , and we refer to state “occupancies,” “distributions,” and “densities”
interchangeably. Finally, letter “d” has a few different versions (with different fonts): d is the low-rank
dimension, d(x) is a density, and (dx) is the differential used in integration. Further, while dπ
h and dD
h refer
to true densities, dh (without superscripts) is often used for optimization variables.
Learning setups
We provide algorithms and guarantees under a number of different setups (e.g., ofﬂine
vs. online). The result that connects all pieces together is the setting of online reward-free exploration with
known density features µ∗= (µ∗
0, . . . , µ∗
H−1) and a policy class Π ⊆(X →∆(A))H (Section 4). Here,
the learner must explore the MDP and form accurate estimations of dπ
h for all π ∈Π and h ∈[H], that is,
output {bdπ
h}h∈[H],π∈Π such that with probability at least 1 −δ, ∀π ∈Π, h ∈[H], ∥bdπ
h −dπ
h∥1 ≤ε, by only
collecting poly(H, K, d, log(|Π|), 1/ε, log(1/δ)) trajectories. Two remarks are in order:
1. Such a guarantee immediately leads to standard guarantees for return maximization when a reward func-
tion is speciﬁed. More concretely (with proof in Appendix F.2),
Proposition 1. Given any policy π and reward function3 R = {Rh} with Rh : X × A →[0, 1], deﬁne
expected return as vπ
R := Eπ[PH−1
h=0 Rh(xh, ah)] = PH−1
h=0
RR
dπ
h(xh)Rh(xh, ah)π(ah|xh)(dxh)(dah).
Then for {bdπ
h} such that ∥bdπ
h −dπ
h∥1 ≤ε/(2H) for all π ∈Π and h ∈[H], we have vbπR
R ≥maxπ∈Π vπ
R−
ε, where bπR = argmaxπ∈Π bvπ
R, and bvπ
R is the expected return calculated using {bdπ
h}.
Moreover, the result can be extended to more general settings, where the optimization objective is some
function of the state (and action) distribution that cannot be written as cumulative expected rewards; e.g.,
entropy as in max-entropy exploration (Hazan et al., 2019), or ∥dπ
h −dπE
h ∥2
2, where πE is an expert policy,
used in imitation learning (Abbeel and Ng, 2004). A detailed discussion is deferred to Appendix C.
2. The introduction of Π and the dependence on K = |A| are both necessary, since low-rank MDPs can
emulate general contextual bandits where the density features µ∗become useless; see Appendix B for
more details.
To enable such a result, a key component is to estimate dπ
h using ofﬂine data (Section 3). Later in
Section 5, we also generalize our results to the representation-learning setting (Agarwal et al., 2020; Modi
et al., 2021; Uehara et al., 2021b), where µ∗is not known but must be learned from an exponentially large
candidate set.
2This is w.l.o.g. as the norm of φ∗
h can be absorbed into Bµ. In a natural special case of low-rank MDPs with “simplex features”
(Jin et al., 2020b, Example 2.2), Assumption 1 holds with Bµ = d. Our sample complexities only have polylogarithmic dependence
on Bµ which will be suppressed by eO.
3We assume known and deterministic rewards, and can easily handle unknown/stochastic versions (Appendix D.2).
3

3
Off-policy occupancy estimation
In this section, we describe our algorithm, FORC, which estimates the occupancy distribution dπ
h of any
given policy π using an ofﬂine dataset. Note that this section serves both as an important building block for
the online algorithm in Section 4 and a standalone ofﬂine-learning result in its own right, so we will make
remarks from both perspectives.
We start by introducing our assumption on the ofﬂine data.
Assumption 2 (Ofﬂine data). Consider a dataset D0:H−1 = D0
S . . . S DH−1, where Dh = {(x(i)
h , a(i)
h ,
x(i)
h+1)}n
i=1.
For any ﬁxed h, we assume that tuples in Dh are sampled i.i.d. from ρh−1 ◦πD
h , where
a0, . . . , ah−1 ∼ρh−1 is an arbitrary (h −1)-step (possibly non-Markov) policy4 and ah ∼πD
h is a single-
step Markov policy. Further, ρh−1, πD
h can be a function of D0:h−1, and πD
h is known to the learner.
The dataset consists of H parts, where the h-th part consists of (xh, ah, xh+1) tuples, allowing us to
reason about the transition dynamics at level h. In practice (as well as in Section 4), such tuples will be
extracted from trajectory data. We use dD
h (xh, ah, xh+1), dD
h (xh), dD,†
h
(xh+1) to denote the joint and the
marginal distributions, respectively. Importantly, we do not assume that dD,†
h
(xh+1) = dD
h+1(xh+1), i.e.,
the next-state distribution of Dh and the current-state distribution of Dh+1 (which are both over X) may
not be the same, as we will need this ﬂexibility in Section 4. The H parts can also sequentially depend on
each other, though samples within each part are i.i.d. While this setup is sufﬁcient for Section 4 and already
weaker than the fully i.i.d. setting commonly adopted in the ofﬂine RL literature (Chen and Jiang, 2019;
Yin and Wang, 2021), in Appendix D we discuss how to relax it to handle more general situations in ofﬂine
learning.
3.1
Occupancy estimation via importance weights
Recall that value functions satisfy the familiar Bellman equations, allowing us to learn them by approxi-
mating Bellman operators via squared-loss regression. The occupancy distributions {dπ
h} also satisfy the
Bellman ﬂow equation: let Pπ
h denote the Bellman ﬂow operator, where for any given dh : X →R and
policy π, (Pπ
hdh)(xh+1) :=
RR
Ph(xh+1|xh, ah)π(ah|xh)dh(xh)(dxh)(dah).5 dπ
h can be then recursively
deﬁned via the Bellman ﬂow equation dπ
h = Pπ
h−1dπ
h−1, with the base case dπ
0 = d0. (One difference is
that value functions are deﬁned bottom-up, whereas occupancies are deﬁned top-down.) Furthermore, in a
low-rank MDP, Pπ
hdh is always linear in µ∗
h (Lemma 16), just like the image of Bellman operators for value
is always in the linear span of φ∗
h.
Given the similarity, one might think that we can also approximate Pπ
h−1 by regressing directly onto the
occupancies, hoping to obtain dπ
h via
argmin
dh
EdD
h−1


 
dh(xh) −dπ
h−1(xh−1)πh−1(ah−1|xh−1)
πD
h−1(ah−1|xh−1)
!2
,
(1)
where πh−1(ah−1|xh−1)
πD
h−1(ah−1|xh−1) is the standard importance weighting to correct the mismatch on actions between
πh−1 and data policy πD
h−1. Unfortunately, this does not work due to the “time-reversed” nature of ﬂow
4h on the superscript of a policy distinguishes identities and does not refer to the h-th step component (which is indicated by
the subscript), that is, ρh and ρh′ for h′ ̸= h can be completely unrelated policies.
5In this deﬁnition, we do not require dh to be a valid distribution. Even π is allowed to be unnormalized; see the deﬁnition of
pseudo-policy in Deﬁnition 1.
4

operators (Liu et al., 2018). In fact, the Bayes-optimal solution of Eq. (1) is
dh(xh) = (Pπ
h−1(dD
h−1dπ
h−1))(xh)
dD,†
h−1(xh)
̸= (Pπ
h−1dπ
h−1)(xh).
However, the fractional form of the solution indicates that we may instead aim to learn a related function—
the importance weight, or density ratio (Hallak and Mannor, 2017). If we use wπ
h−1 = dπ
h−1/dD
h−1 to replace
dπ
h−1 as the regression target in Eq. (1), the population solution would be
(Pπ
h−1dπ
h−1)(xh)
dD,†
h−1(xh)
=
dπ
h(xh)
dD,†
h−1(xh)
=: wπ
h(xh).
The occupancy can then be straightforwardly extracted from the weight via elementwise multiplication, i.e.,
dπ
h = wπ
h · dD,†
h−1, where dD,†
h−1 can be estimated via MLE from the dataset itself.
While this is promising, the approach uses importance weight wπ
h(xh) as an intermediate variable, whose
very existence and boundedness rely on the assumption that the data distribution dD,†
h−1 is exploratory and
provides sufﬁcient coverage over dπ
h. We next consider the scenario where such an assumption does not
hold. Perhaps surprisingly, although we would like to construct exploratory datasets in Section 4 and feed
them into the ofﬂine algorithm, being able to handle non-exploratory data turns out to be crucial to the online
setting, and also yields novel ofﬂine guarantees of independent interest.
3.2
Handling insufﬁcient data coverage
Because we make no assumptions about data coverage, the true occupancy dπ
h may be completely unsup-
ported by data, in which case there is no hope to estimate it well. What kind of learning guarantees can we
still obtain?
To answer this question, we introduce one of our main conceptual contributions, a novel learning target
for occupancy estimation under arbitrary data distributions.
Deﬁnition 1 (Pseudo-policy and recursively clipped occupancy). Given a Markov policy π, data distri-
butions {dD
h }, and state and action clipping thresholds {Cx
h}, {Ca
h}, the recursively clipped occupancy,
{d
π
h}, is deﬁned as follows. Let d
π
0 := dπ
0 = d0. Deﬁne πh(ah|xh) := πh(ah|xh) ∧Ca
hπD
h (ah|xh) (or
πh = πh ∧Ca
hπD
h for short), and for 1 ≤h ≤H −1, inductively set 6
d
π
h(xh) :=

Pπ
h−1

d
π
h−1 ∧Cx
h−1dD
h−1

(xh).
(4)
We also call objects like π a pseudo-policy, which can yield unnormalized distributions over actions.
The above deﬁnition ﬁrst clips the previous-level d
π
h−1 to have at most Cx
h−1 ratio over the data distri-
bution dD
h−1 and the policy π to have at most Ca
h−1 ratio over πD
h−1, then applies the Bellman ﬂow operator.
This guarantees that d
π
h is always supported on the data distribution (unlike dπ
h), and d
π
h ≤dπ
h because
poorly-supported mass is removed from every level (and hence d
π
h is generally an unnormalized distribu-
tion). Further, when we do have data coverage and the original importance weights on states and actions are
always bounded by {Cx
h} and {Ca
h}, it is easy to see that d
π
h = dπ
h, since the clipping operations will have
no effects and Deﬁnition 1 simply coincides with the Bellman ﬂow equation for {dπ
h}.
6Note that d
π
h depends on hyperparameters Cx
h and Ca
h, which is omitted in the notation. Appendix E.1 discusses the relationship
between Cx
h, Cx
a and the missingness error, namely, that ∥dπ
h −d
π
h∥1 is Lipschitz in, and thus insensitive to misspeciﬁcations of,
the clipping thresholds.
5

Algorithm 1 Fitted Occupancy Iteration with Clipping (FORC)
Input: policy π, density feature µ∗, dataset D0:H−1, sample sizes nmle and nreg, clipping thresholds {Cx
h}
and {Ca
h}.
1: Initialize bdπ
0 = d0.
2: for h = 1, . . . , H do
3:
Randomly split Dh−1 to two folds Dmle
h−1 and Dreg
h−1 with sizes nmle and nreg, respectively.
4:
Estimate marginal data distributions bdD
h−1(xh−1) and bd D,†
h−1(xh) by MLE on dataset Dmle
h−1:
bdD
h−1 = argmax
dh−1∈Fh−1
1
nmle
nmle
X
i=1
log

dh−1(x(i)
h−1)

and bd D,†
h−1 = argmax
dh∈Fh
1
nmle
nmle
X
i=1
log

dh(x(i)
h )

,
(2)
where Fh =

dh = ⟨µ∗
h−1, θh⟩: dh ∈∆(X), θh ∈Rd, ∥θh∥∞≤1
	
.
# ∥θh∥∞≤1 guarantees
dD
h ∈Fh
5:
Deﬁne LDreg
h−1(wh, wh−1, πh−1) :=
1
nreg
Pnreg
i=1

wh(x(i)
h ) −wh−1(x(i)
h−1)
πh−1(a(i)
h−1|x(i)
h−1)
πD
h−1(a(i)
h−1|x(i)
h−1)
2
, and es-
timate
bwπ
h = argmin
wh∈Wh
LDreg
h−1

wh,
bdπ
h−1∧Cx
h−1 bdD
h−1
bdD
h−1
, πh−1 ∧Ca
h−1πD
h−1

,
(3)
where Wh =

wh =
⟨µ∗
h−1,θup
h ⟩
⟨µ∗
h−1,θdown
h
⟩: ∥wh∥∞≤Cx
h−1Ca
h−1, θup
h , θdown
h
∈Rd

.
6:
Set the estimate bdπ
h = bwπ
h bd D,†
h−1.
7: end for
Output: estimated state occupancies {bdπ
h}h∈[H].
As we will see below in Section 3.3, {d
π
h} becomes a learnable target and the ℓ1 estimation error of our
algorithm goes to 0 when the sample size n →∞. The thresholds {Cx
h} and {Ca
h} reﬂect a bias-variance
trade-off: higher thresholds ensure that less “mass” is clipped away (i.e., d
π
h will be closer to dπ
h), but result
in a worse sample complexity as the algorithm will need to deal with larger importance weights. Below we
provide more ﬁne-grained characterization on the bias part, i.e., how d
π
h is related to dπ
h, and the proof is
deferred to Appendix E.2.
Proposition 2 (Properties of d
π
h).
1. d
π
h ≤dπ
h.
2. d
π
h = dπ
h when data covers π (i.e., ∀h′ < h we have dπ
h′ ≤Cx
h′dD
h′ and πh′ ≤Ca
h′πD
h′).
3. ∥d
π
h −dπ
h∥1 ≤∥d
π
h−1 −dπ
h−1∥1 + ∥d
π
h−1 −d
π
h−1 ∧Cx
h−1dD
h−1∥1 + ∥Pπ
h−1dπ
h−1 −Pπ
h−1dπ
h−1∥1.
The 3rd claim shows how the bias term ∥d
π
h −dπ
h∥1 (i.e., how much mass d
π
h is missing from dπ
h)
accumulates over the horizon: the RHS of the bound consists of 3 terms, where the ﬁrst is missing mass
from the previous level, and the other terms correspond to the mass being clipped away from states and
actions, respectively, at the current level.
6

3.3
Algorithm and analyses
We are now ready to introduce our algorithm, FORC, with its analyses and guarantees. See pseudocode
in Algorithm 1. The overall structure of the algorithm largely follows the sketch in Section 3.1: we use
squared-loss regression to iteratively learn the importance weights (line 5), and convert them to densities by
multiplying with the data distributions (line 6) estimated via MLE (line 4).
The major difference is that we introduce clipping in line 5 (in the same way as Deﬁnition 1) to guarantee
that the regression target is always well-behaved and bounded, and below we show that this makes bdπ
h a good
estimation of d
π
h. In particular, we will bound the regression error ∥bdπ
h −d
π
h∥1 as a function of sample size
nreg. A key lemma that enables such a guarantee is the following error propagation result:
Lemma 1. For every h ∈[H], the error between estimates bdπ
h from Algorithm 1 and the clipped target d
π
h
is decomposed recursively as
bdπ
h −d
π
h

1 ≤
bdπ
h−1 −d
π
h−1

1
+ 2Cx
h−1
bdD
h−1 −dD
h−1

1+ Cx
h−1Ca
h−1
bd D,†
h−1 −dD,†
h−1

1
+
√
2
 bwπ
h −Eπ
h−1

dD
h−1
bdπ
h−1∧Cx
h−1 bdD
h−1
bdD
h−1

2,dD,†
h−1
,
where (Eπ
hdh) := (Pπ
hdh)/dD,†
h
.
The proof can be found in Appendix E.2. The bound consists of 3 parts: the ﬁrst line is the error at
the previous level h −1, showing that the regression error accumulatives linearly over the horizon. The
second line captures errors due to imperfect estimation of the data distributions, since we use the estimated
bdD
h−1 and bd D,†
h−1, instead of the groundtruth distributions, to set up the weight regression problem and extract
the density; these errors can be reduced by simply using larger nmle. The last line represents the ﬁnite-
sample error in regression, which is the difference between the estimated weight bwπ
h and the Bayes-optimal
predictor. We set the constraints in the hypothesis class in a way to guarantee the Bayes-optimal predictor
is in the class (see the deﬁnition of Wh below Eq. (3)), so the regression is realizable.
Bounding the complexities of Fh and Wh
The last challenge is in controlling the statistical complexities
of the function classes used in learning, Fh and Wh, both of which are inﬁnite classes. For Fh, we construct
an optimistic covering to bound its covering number (Chen et al., 2022a). For Wh, however, its hypothesis
takes the form of ratio between linear functions,
⟨µ∗
h−1,θup
h ⟩
⟨µ∗
h−1,θdown
h
⟩, where standard covering arguments, which
discretize θup
h and θdown
h
, run into sensitivity issues, as θdown
h
is on the denominator where small perturba-
tions can lead to large changes in the ratio. We overcome this by recalling a technique from Bartlett and
Tewari (2006): we bound the pseudo-dimension of Wh, which is equal to the VC-dimension of the corre-
sponding thresholding class. Then, using Goldberg and Jerrum (1993), the VC-dimension is bounded by
the syntactic complexity of the classiﬁcation rule, written as a Boolean formula of polynomial inequality
predicates. The pseudo-dimension of Wh further implies ℓ1 covering number bounds, for which Dong et al.
(2020); Modi et al. (2021) provide fast-rate regression guarantees.
Sample complexity of FORC
We now provide the guarantee for FORC, with its proof deferred to Ap-
pendix E.2.
Theorem 2 (Ofﬂine dπ estimation). Fix δ ∈(0, 1). Suppose Assumption 1 and Assumption 2 hold, and µ∗
is known. Then, given an evaluation policy π, by setting nmle = ˜O(d(P
h∈[H] Cx
hCa
h)2 log(1/δ)/ε2) and
7

nreg = ˜O(d(P
h∈[H] Cx
hCa
h)2 log(1/δ)/ε2), with probability at least 1 −δ, FORC (Algorithm 1) returns
state occupancy estimates {bdπ
h}H−1
h=0 satisfying
∥bdπ
h −d
π
h∥1 ≤ε, ∀h ∈[H].
The total number of episodes required by the algorithm is
˜O

dH
P
h∈[H] Cx
hCa
h
2
log(1/δ)/ε2

.
This result can also be used to establish a guarantee for ∥bdπ
h −dπ
h∥1, simply by decomposing ∥bdπ
h −
dπ
h∥1 ≤∥bdπ
h −d
π
h∥1 + ∥d
π
h −dπ
h∥1. The regression error in the ﬁrst term is controlled by Theorem 2. The
second term is a one-sided missingness error due to insufﬁcient coverage of data, which we have character-
ized in Proposition 2. Note that we split ∥bdπ
h −dπ
h∥1 into two terms using d
π
h as an intermediate quantity
and analyze how their errors accumulate over the horizon separately; alternatively, one can directly try to
analyze how ∥bdπ
h −dπ
h∥1 depends on ∥bdπ
h−1 −dπ
h−1∥1. In general, we ﬁnd the latter can yield signiﬁcantly
worse bounds—in fact, exponentially worse, as will be seen in Section 4.
Ofﬂine policy optimization
Theorem 2 provides learning guarantees for d
π
h, which is a point-wise lower
bound of dπ
h. When we consider standard return maximization with a given reward function, having access
to bdπ
h ≈d
π
h immediately enables pessimistic policy evaluation (Jin et al., 2020c; Xie et al., 2021), and we
are only ε-suboptimal compared to the maximal value computed over covered parts of the data, i.e., with
respect to d
π
h. The immediate implication is that we can compete with the best policy fully covered by data
(satisfying property 2 of Proposition 2); see Appendix E.3 for the full statement and proof.
Theorem 3 (Ofﬂine policy optimization). Fix δ ∈(0, 1) and suppose Assumption 1 and Assumption 2 hold,
and µ∗is known. Given a policy class Π, let {bdπ
h}h∈[H],π∈Π be the output of running Algorithm 1. Then
with probability at least 1 −δ, for any reward function R and policy selected as bπR = argmaxπ∈Π bvπ
R, we
have
vbπR
R ≥argmax
π∈Π
vπ
R −ε,
where vπ
R and bvπ
R are deﬁned in Proposition 1, and vR is deﬁned similarly for {d
π
h}. The total number of
episodes required by the algorithm is
˜O

dH3 P
h∈[H] Cx
hCa
h
2
log(|Π|/δ)/ε2

.
Computation
We remark that our policy optimization result only enjoys statistical efﬁciency and does not
guarantee computational efﬁciency, as Theorem 3 assumes that we can enumerate over candidate policies
and run FORC for each of them; similar comments apply to our later online algorithm as well. Since the
optimization variable is a policy, the most promising approach is to come up with off-policy policy-gradient
(OPPG) algorithms to approximate the objective. However, existing model-free OPPG methods all rely
on value-function approximation (Nachum et al., 2019b; Liu et al., 2019), which is not available in our
setting. Studying OPPG with only density(-ratio) approximation will be a pre-requisite for investigating the
computational feasibility of our problem, which we leave for future work.
8

4
Online policy cover construction
We now consider the online setting where the learner explores the MDP to collect its own data. The hope is
that we will collect exploratory datasets that provide sufﬁcient coverage for all policies in Π (so that we can
estimate their occupancies accurately), which is measured by the standard deﬁnition of concentrability.
Deﬁnition 2 (Concentrability Coefﬁcient (CC)). Given a policy class Π and any distribution d ∈∆(X),
the concentrability coefﬁcient at level h relative to d is
CCh(d) = inf
n
c ∈R : maxπ∈Π

dπ
h
d

∞≤c
o
.
To achieve this goal, we ﬁrst recall the following result, which shows the existence of an exploratory
data distribution that satisﬁes the above criterion and hints at how to construct it.
Proposition 3 (Adapted from Chen and Jiang (2019), Prop. 10). Given a policy class Π and h, let {dπh,i
∗
h
}d
i=1
be the barycentric spanner (Deﬁnition 4 in Appendix I.2) of {dπ
h}π∈Π. Then, CCh

1
d
Pd
i=1 dπh,i
∗
h

≤d.
Proposition 3 shows that for each level h, an exploratory distribution that has d concentrability always
exists. It is simply the mixture of {dπh,i
∗
h
} for i ∈[d], which can be identiﬁed if we have access to dπ
h for
all π ∈Π. Of course, we can only estimate dπ
h if we have exploratory data, so the estimation of dπ
h and the
identiﬁcation of {πh,i
∗} need to be interleaved to overcome this “chicken-and-egg” problem (Agarwal et al.,
2020; Modi et al., 2021): suppose we have already constructed policy cover at h −1. We can construct it
for the next level as follows:
1. Collect a dataset Dh−1 by rolling in to level h −1 with the policy cover, with CCh−1(dD
h−1) ≤d, then
taking a uniformly random action, thereby CCh(dD,†
h−1) ≤dK.
2. Use FORC to estimate dπ
h for all π ∈Π based on Dh−1.
3. Choose their barycentric spanner as the policy cover for level h, with CCh(dD
h ) ≤d.
The idea is that, since we have an exploratory distribution at level h −1, taking a uniform action afterwards
will give us an exploratory distribution at level h, though the degree of exploration will be diluted by a factor
of K. We collect data from this distribution to estimate dπ
h and compute the barycentric spanner for level h,
which will bring the concentrability coefﬁcient back to d, so that the process can repeat inductively.
The above reasoning makes an idealized assumption that dπ
h can be estimated perfectly. In such a
case, the constructed distribution will provide perfect coverage, so that the clipping introduced in Section 3
becomes completely unnecessary: all clipping operations would be inactive (by setting Cx
h = d and Ca
h =
K), and d
π
h ≡dπ
h. Unfortunately, when the estimation error of dπ
h is taken into consideration, the reasoning
breaks down seriously.
The ﬁrst problem is that our estimate bdπ
h from FORC is not necessarily linear due to its product form.
However, that is not a concern as we can linearize it (corresponding to line 7 in Algorithm 2); we also have
an alternative procedure for FORC that directly produces linear bdπ
h (see Appendix D.3), so in this section
we will ignore this issue and pretend that bdπ
h is linear (thus is the same as edπ
h in Algorithm 2) for ease of
presentation.
7MLE only needs to be done once and not for every π ∈Π.
9

Algorithm 2 FORC-guided Exploration (FORCE)
Input: policy class Π, density feature µ∗, n = nmle + nreg.
1: Initialize bdπ
0 = d0 and edπ
0 = d0, ∀π ∈Π.
2: for h = 1, . . . , H do
3:
Construct {edπh−1,i
h−1
}d
i=1 as the barycentric spanner of {edπ
h−1}π∈Π, and set Πexpl
h−1 = {πh−1,i}d
i=1.
4:
Draw a tuple dataset Dh−1 = {(x(i)
h−1, a(i)
h−1, x(i)
h )}n
i=1 using unif(Πexpl
h−1) ◦unif(A).
5:
for π ∈Π do
6:
Estimate bdπ
h using the h-level loop7 of Algorithm 1 (lines 4-6) with Dh−1, bdπ
h−1, Cx
h−1 = d,
Ca
h−1 = K.
7:
Find the closest linear approximation edπ
h = ⟨µ∗
h−1, eθh⟩where eθh = argminθh∈Rd ∥⟨µ∗
h−1, θh⟩−
bdπ
h∥1.
8:
end for
9: end for
Output: estimated state occupancy measure {bdπ
h}h∈[H],π∈Π.
4.1
Taming error exponentiation
Now that the issue of (non-)linear bdπ
h is out of the way, we are ready to see where the real trouble is: note
that the barycentric spanner computed from {bdπ
h}π∈Π satisﬁes

bdπ
h
1
d
Pd
i=1 bdπh,i
h

∞
≤d,
∀π ∈Π.
(5)
However, the actual distribution induced by the policy cover {πh,i}d
i=1 is dD
h = 1
d
Pd
i=1 dπh,i
h
. Suppose for
now we have nmle = ∞for perfect estimation of dD
h ; even then, the regression target in Eq. (3) will no
longer be bounded without clipping, as the boundedness of bd/bd does not imply that of bd/d, and the latter
can be very large or even inﬁnite.
While the unbounded regression target can be easily controlled by clipping, analyzing the algorithm and
bounding its error still prove to be very challenging. A natural strategy is to inductively bound ∥bdπ
h −dπ
h∥1
using ∥bdπ
h−1 −dπ
h−1∥1. Unfortunately, this approach fails miserably, as directly analyzing ∥bdπ
h −dπ
h∥1 yields
∥bdπ
h −dπ
h∥1 ≤(1 + d)∥bdπ
h−1 −dπ
h−1∥1 + · · · ,
(6)
implying an O(d)H exponential error blow-up. (The concrete reason for this failure will be made clear
shortly.) In Appendix D.4, we also discuss an alternative approach that “pretends” data to be perfectly ex-
ploratory, which only addresses the problem superﬁcially and still suffers O(d)H error exponentiation, just
in a different way. Issues that bear high-level similarities are commonly encountered in level-by-level ex-
ploration algorithms, which often demand the so-called reachability assumption (Du et al., 2019, Deﬁnition
2.1), which we do not need.
As all the earlier hints allude to, the key to breaking error exponentiation is to split the error using d
π
h into
its two sources with very different natures: a “two-sided” regression error ∥bdπ
h −d
π
h∥1, and a “one-sided”
missingness error ∥d
π
h −dπ
h∥1 (in the sense that d
π
h ≤dπ
h). Because the ofﬂine occupancy estimation module
of Algorithm 2 is the same as that of Algorithm 1, Lemma 1 still holds (left ×1 chain of Figure 1), implying
that ∥bdπ
h −d
π
h∥1 can be bounded irrespective of the data distribution.
This observation disentangles the regression error from the rest of the analysis, allowing us to focus on
bounding the missingness error. For the latter, Proposition 2 also exhibits linear error propagation, as it
takes the form of Ah ≤Ah−1 + Bh−1 where Ah = ∥d
π
h −dπ
h∥1. However, it still remains to show that the
10

d⇡
h
A
AB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1ItQ9OKxov2ANpTNdtMu3WzC7kQoT/BiwdFvPqLvPlv3LY5aOuDgcd7M8zMCxIpDLrut1NYWV
1b3yhulra2d3b3yvsHTROnmvEGi2Ws2wE1XArFGyhQ8naiOY0CyVvB6Hbqt564NiJWjzhOuB/RgRKhYBSt9DC8dnvlilt1ZyDLxMtJBXLUe+
Wvbj9macQVMkmN6Xhugn5GNQom+aTUTQ1PKBvRAe9YqmjEjZ/NTp2QE6v0SRhrWwrJTP09kdHImHEU2M6I4tAselPxP6+TYnjlZ0IlKXLF5
ovCVBKMyfRv0heaM5RjSyjTwt5K2JBqytCmU7IheIsvL5PmWdW7qJ7fn1dqN3kcRTiCYzgFDy6hBndQhwYwGMAzvMKbI50X5935mLcWnHzmE
P7A+fwBwQGNdg=</latexit>h = 0
h = 1
h = 2
h = H −1
⇥1
⇥1
⇥1
⇥1
bd⇡
h
V
CUIAWMFC2OR6ENqQuQ4TmvVsSPbAVWhn8LCAEKsfAkbf4PTZoCWI1k6Ouce3esTpowq7TjfVmVldW19o7pZ29re2d2z6/tdJTKJSQcLJmQ/RIowyklHU81IP5UEJSEjvXB8Xfi9ByIVFfxOT1LiJ2jIaUwx0kYK7LonjF2kYRS
M7r2UBnbDaTozwGXilqQBSrQD+8uLBM4SwjVmSKmB6Taz5HUFDMyrXmZIinCYzQkA0M5Sojy89npU3hslAjGQprHNZypvxM5SpSaJKGZTJAeqUWvEP/zBpmOL/2c8jThOP5ojhjUAtY9AjKgnWbGIwpKaWyEeIYmwNm3V
TAnu4peXSfe06Z43z27PGq2rso4qOARH4AS4AK0wA1ogw7A4BE8g1fwZj1ZL9a79TEfrVhl5gD8gfX5AziMk/w=</latexit>
d
⇡
h
kbd⇡
h −d
⇡
hk1
kd
⇡
h −d⇡
hk1
⇥O(d)
9
x40IRt/6HO/GaZqFth4YOJxzL/fM8RLOpLKsb2NpeWV1b20Ud7c2t7ZNf2zJOBaEtEvNYdD0sKWcRbSmO0mguLQ47Tja6nfueBCsni6F6NE+qGeBCxgBGstNQ3Dx3FQirRbdUJsRrKIPMnp32zYtWsHGiR2AWpQIFm3/xy/JikIY0U4VjKnm0lys2wUIxwOik7qaQJiM8oD1NI6xPulmefoJOtOKjIBb6RQrl6
u+NDIdSjkNPT+YR572p+J/XS1Vw6WYsSlJFIzI7FKQcqRhNq0A+E5QoPtYE8F0VkSGWGCidGFlXYI9/+VF0j6r2e1+l290rgq6ijBERxDFWy4gAbcQBNaQOARnuEV3own48V4Nz5mo0tGsXMAf2B8/gAEwpT1</latexit>⇥O(d)
Figure 1:
Error propagation diagram for FORCE. “• →•” with ×c means (•) ≤c × (•) + (other
instantaneous errors that do not accumulate over horizon), and multiple incoming arrows imply sum of
errors. The left ×1 chain is from Lemma 1, the right ×1 chain from Proposition 2, and the ×O(d) edges
from Lemma 4.
additional error (“Bh−1”) has no dependence on the inductive error (“Ah−1”), otherwise we would still have
error exponentiation.8 This is shown in the following key lemma:
Lemma 4. For any h ∈[H] and π ∈Π in Algorithm 2,
∥d
π
h −dπ
h∥1 ≤∥d
π
h−1 −dπ
h−1∥1 + 4d max
π′∈Π ∥bdπ′
h−1 −d
π′
h−1∥1.
To understand this lemma, recall that the additional error in Proposition 2 characterizes the mass clipped
away at the current level. This mass can be bounded by the regression error of the previous level (maxπ′∈Π ∥bdπ′
h−1−
d
π′
h−1∥1): intuitively, had we had perfect estimation of bdπ′
h−1 = d
π′
h−1, our barycentric spanner would also be
perfect and we would not need any clipping at all in level h, implying 0 additional error in the bound. More
generally, the closer bdπ′
h−1 is to d
π′
h−1, the less mass we need to clip away.
That said, this term is not instantaneous and depends inductively on quantities in the previous time
step, still raising concerns of error exponentiation. To see why this is not a problem, we visualize error
propagation in Figure 1: it can be clearly seen that such a dependence corresponds to a “cross-edge”, and
appears at most once along any long chain. This also explains the destined failure of directly analyzing
∥bdπ
h −dπ
h∥1 in Eq. (6), as that corresponds to merging the two chains into one, where every edge along the
only chain acquires an O(d) multiplicative factor.
With this, we can now state the formal guarantee for our algorithm, FORCE. See Algorithm 2 for its
pseudo-code, and the proof of the guarantee is deferred to Appendix F.1.
Theorem 5 (Online dπ estimation). Fix δ ∈(0, 1) and consider an MDP M that satisﬁes Assumption 1, and
µ∗is known. Then by setting nmle = eO
 d3K2H4 log(1/δ)/ε2
,nreg = eO
 d5K2H4 log(|Π|/δ)/ε2
, n =
nmle + nreg, with probability at least 1 −δ, FORCE returns state occupancy estimates {bdπ
h}h∈[H],π∈Π satis-
fying that
bdπ
h −dπ
h

1 ≤ε, ∀h ∈[H], π ∈Π.
The total number of episodes required by the algorithm is
eO(nH) = eO
 d5K2H5 log(|Π|/δ)/ε2
.
8For example, if Bh−1 can only be bounded as Bh−1 ≤Ah−1, we would still have Ah ≤2Ah−1.
11

Theorem 5 also immediately translates to a policy optimization guarantee when combined with Propo-
sition 1:
Theorem 6 (Online policy optimization). Fix δ ∈(0, 1) and suppose Assumption 1 and Assumption 2 hold,
and µ∗is known. Given a policy class Π, let {bdπ
h}h∈[H],π∈Π be the output of running FORCE. Then with
probability at least 1 −δ, for any reward function R and policy selected as bπR = argmaxπ∈Π bvπ
R, we have
vbπR
R ≥argmax
π∈Π
vπ
R −ε,
where vπ
R and bvπ
R are deﬁned in Proposition 1. The total number of episodes required by the algorithm is
˜O
 d5K2H7 log(|Π|/δ)/ε2
.
The proof is deferred to Appendix F.2. We remark that Theorem 6 is a reward-free learning guarantee
(Jin et al., 2020a; Chen et al., 2022b), and it is easy to see that Algorithm 2 is deployment efﬁcient (Huang
et al., 2022).
5
Representation learning
In this section, we extend the ofﬂine (Section 3) and online (Section 4) results to the representation learning
setting. Here, the true density feature µ∗is unknown, but the learner has access to a realizable density
feature class Υ, deﬁned formally below. For simplicity, we consider ﬁnite and normalized Υ, as is standard
in the literature (Agarwal et al., 2020; Modi et al., 2021; Uehara et al., 2021b).
Assumption 3. We have a ﬁnite density feature class Υ = S
h∈[H] Υh such that µ∗
h ∈Υh for each h ∈[H],
thus µ∗∈Υ. Further, for any µh ∈Υh, we have
R
∥µh(x)∥1(dx) ≤Bµ.
The algorithms and analyses for the representation learning case mostly follow the same template as the
known feature case, so we restrict our discussion to their differences. Recall that, in order to have realizable
function classes for regression and MLE in Section 3, we constructed Fh, Wh using functions linear in the
known µ∗
h−1. In order to maintain this realizability when µ∗
h−1 is unknown, we instead construct Fh, Wh
using the union of all functions linear in some candidate µh−1 ∈Υh−1, i.e., S
µh−1∈Υh−1{⟨µh−1, θh⟩, θh ∈
Rd} (see Eq. (26) and Eq. (27) for their formal deﬁnitions).
While such union classes allow most of Section 3 and Section 4 to straightforwardly extend to the
representation learning setting, a nontrivial modiﬁcation must be made to the online algorithm. Recall in
line 7 of Algorithm 2, we constructed our policy cover using the barycentric spanner of {edπ
h}π∈Π, the set of
linearized approximations to the density estimates. Importantly, this guaranteed a concentrability coefﬁcient
of d because all edπ
h are linear in the same feature µ∗
h−1. This is no longer the case with unknown features
because, if linearized in the same way (but over all feasible µh−1 ∈Υh−1), each edπ
h can be composed of a
different µh−1 feature, resulting in a CC linear in |Π|. To overcome this issue, we replace line 7 with the
following “joint linearization” step (see line 8 in Algorithm 4):
bµh−1 =
min
µh−1∈Υh−1 max
π∈Π min
θh∈Rd ∥⟨µh−1, θh⟩−bdπ
h∥1,
where all density estimates are linearized using a single feature bµh−1, whose linear span approximates all
bdπ
h well. We provide theorems for ofﬂine/online dπ estimation with representation learning below.
12

Theorem 7 (Ofﬂine dπ estimation with representation learning). Fix δ ∈(0, 1). Suppose Assumption 1,
Assumption 2, and Assumption 3 hold. Then, given an evaluation policy π, by setting
nmle = ˜O(d(P
h∈[H] Cx
hCa
h)2 log(|Υ|/δ)/ε2) and nreg = ˜O(d(P
h∈[H] Cx
hCa
h)2 log(|Υ|/δ)/ε2), with prob-
ability at least 1 −δ, FORCRL (Algorithm 3) returns state occupancy estimates {bdπ
h}H−1
h=0 satisfying that
bdπ
h −d
π
h

1 ≤ε, ∀h ∈[H].
The total number of episodes required by the algorithm is
˜O

dH
P
h∈[H] Cx
hCa
h
2
log(|Υ|/δ)/ε2

.
Theorem 8 (Online dπ estimation with representation learning). Fix δ ∈(0, 1) and suppose Assumption 1
and Assumption 3 hold. Then by setting nmle = eO(d3K2H4 log(|Υ|/δ)/ε2), nreg = eO(d5K2H4 log(|Π||Υ|/δ)/ε2),
n = nmle +nreg, with probability at least 1−δ, FORCRLE (Algorithm 4) returns state occupancy estimates
{bdπ
h}H−1
h=0 satisfying that
∥bdπ
h −dπ
h∥1 ≤ε, ∀h ∈[H], π ∈Π.
The total number of episodes required by the algorithm is
eO
 d5K2H5 log(|Π||Υ|/δ)/ε2
.
The detailed proofs of these two theorems are given in Appendix G. We also present the theorems and
proofs for ofﬂine/online policy optimization with representation learning as well as the formal representation
learning algorithms in Appendix G.
6
Conclusion
We have shown how to leverage density features for statistically efﬁcient state occupancy estimation and
reward-free exploration in low-rank MDPs, culminating in policy optimization guarantees. An important
open problem lies in investigating the computational efﬁciency of our algorithms (e.g., through off-policy
policy gradient).
Acknowledgements
The authors thank Akshay Krishnamurthy and Dylan Foster for discussions related to MLE generalization
error bounds. NJ acknowledges funding support from NSF IIS-2112471 and NSF CAREER IIS-2141781.
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship Learning via Inverse Reinforcement Learning. In Pro-
ceedings of the 21st International Conference on Machine learning, page 1. ACM, 2004.
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the
monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine
Learning, pages 1638–1646, 2014.
13

Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and
representation learning of low rank mdps. Advances in Neural Information Processing Systems, 2020.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge univer-
sity press, 2009.
Baruch Awerbuch and Robert Kleinberg. Online linear optimization and adaptive routing. Journal of Com-
puter and System Sciences, 74(1):97–114, 2008.
Peter Bartlett and Ambuj Tewari. Sample complexity of policy search with known dynamics. Advances in
Neural Information Processing Systems, 19, 2006.
Fan Chen, Song Mei, and Yu Bai. Uniﬁed algorithms for rl with decision-estimation coefﬁcients: No-regret,
pac, and reward-free learning. arXiv preprint arXiv:2209.11745, 2022a.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In Inter-
national Conference on Machine Learning, 2019.
Jinglin Chen and Nan Jiang. Ofﬂine reinforcement learning under value and density-ratio realizability: The
power of gaps. In Conference on Uncertainty in Artiﬁcial Intelligence, 2022.
Jinglin Chen, Aditya Modi, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. On the statistical
efﬁciency of reward-free exploration in non-linear rl. In Advances in Neural Information Processing
Systems, 2022b.
Christoph Dann and Emma Brunskill. Sample complexity of episodic ﬁxed-horizon reinforcement learning.
In Advances in Neural Information Processing Systems, pages 2818–2826, 2015.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Kefan Dong, Jian Peng, Yining Wang, and Yuan Zhou. √n-regret for learning in Markov decision processes
with function approximation and low Bellman rank. In Conference on Learning Theory, 2020.
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Prov-
ably efﬁcient rl with rich observations via latent state decoding. In International Conference on Machine
Learning, 2019.
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-learning. In
Learning for Dynamics and Control, pages 486–489. PMLR, 2020.
Fei Feng, Ruosong Wang, Wotao Yin, Simon S Du, and Lin Yang. Provably efﬁcient exploration for re-
inforcement learning using unsupervised learning. Advances in Neural Information Processing Systems,
33:22492–22504, 2020.
Carles Gelada and Marc G Bellemare. Off-policy deep reinforcement learning by bootstrapping the covariate
shift. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 3647–3655,
2019.
Paul Goldberg and Mark Jerrum. Bounding the vapnik-chervonenkis dimension of concept classes parame-
terized by real numbers. In Proceedings of the sixth annual conference on Computational learning theory,
pages 361–369, 1993.
14

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):
139–144, 2020.
Assaf Hallak and Shie Mannor. Consistent on-line off-policy evaluation. In International Conference on
Machine Learning, pages 1372–1383. PMLR, 2017.
Elad Hazan, Sham M Kakade, Karan Singh, and Abby Van Soest. Provably efﬁcient maximum entropy
exploration. In International Conference on Machine Learning, 2019.
Audrey Huang and Nan Jiang. Beyond the return: Off-policy function estimation under user-speciﬁed error-
measuring distributions. In Advances in Neural Information Processing Systems, 2022.
Jiawei Huang, Jinglin Chen, Li Zhao, Tao Qin, Nan Jiang, and Tie-Yan Liu. Towards deployment-efﬁcient
reinforcement learning: Lower bound and optimality. In International Conference on Learning Repre-
sentations, 2022.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E. Schapire. Contextual
decision processes with low Bellman rank are PAC-learnable. In International Conference on Machine
Learning, 2017.
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for rein-
forcement learning. In International Conference on Machine Learning, 2020a.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efﬁcient reinforcement learning with
linear function approximation. In Conference on Learning Theory, pages 2137–2143. PMLR, 2020b.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efﬁcient for ofﬂine rl? arXiv preprint
arXiv:2012.15085, 2020c.
Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim. Optidice: Ofﬂine policy
optimization via stationary distribution correction estimation. In International Conference on Machine
Learning, pages 6120–6130. PMLR, 2021.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Inﬁnite-horizon
off-policy estimation. In Advances in Neural Information Processing Systems, pages 5356–5366, 2018.
Qinghua Liu, Alan Chung, Csaba Szepesv´ari, and Chi Jin. When is partially observable reinforcement
learning not scary? In Conference on Learning Theory, pages 5175–5220. PMLR, 2022.
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient with state
distribution correction. arXiv preprint arXiv:1904.08473, 2019.
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free represen-
tation learning and exploration in low-rank mdps. arXiv:2102.07035, 2021.
Mehryar Mohri and Afshin Rostamizadeh. Rademacher complexity bounds for non-iid processes. Advances
in Neural Information Processing Systems, 21, 2008.
Mirco Mutti, Riccardo De Santi, Piersilvio De Bartolomeis, and Marcello Restelli. Challenging common
assumptions in convex reinforcement learning. arXiv preprint arXiv:2202.01511, 2022.
Oﬁr Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted
stationary distribution corrections. Advances in Neural Information Processing Systems, 32, 2019a.
15

Oﬁr Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policy
gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019b.
Asuman Ozdaglar, Sarath Pattathil, Jiawei Zhang, and Kaiqing Zhang. Revisiting the linear-programming
framework for ofﬂine rl with general function approximation. arXiv preprint arXiv:2212.13861, 2022.
Tongzheng Ren, Tianjun Zhang, Lisa Lee, Joseph E Gonzalez, Dale Schuurmans, and Bo Dai. Spectral
decomposition representation for reinforcement learning. arXiv preprint arXiv:2208.09515, 2022.
Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and Tengyang Xie. Finite sam-
ple analysis of minimax ofﬂine reinforcement learning: Completeness, fast rates and ﬁrst-order efﬁciency.
arXiv preprint arXiv:2102.02981, 2021a.
Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and ofﬂine RL in
low-rank MDPs. In International Conference on Learning Representations, 2021b.
Sara A Van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.
Vladimir Vapnik. Statistical learning theory, volume 2. Wiley New York, 1998.
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pes-
simism for ofﬂine reinforcement learning. Advances in neural information processing systems, 34, 2021.
Ming Yin and Yu-Xiang Wang. Towards instance-optimal ofﬂine reinforcement learning with pessimism.
Advances in neural information processing systems, 34:4065–4078, 2021.
Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough for
convex mdps. Advances in Neural Information Processing Systems, 34:25746–25759, 2021.
Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Ofﬂine reinforcement learning with
realizability and single-policy concentrability. In Conference on Learning Theory, pages 2730–2775.
PMLR, 2022.
Tong Zhang. From ε-entropy to kl-entropy: Analysis of minimum information complexity density estima-
tion. The Annals of Statistics, 34(5):2180–2210, 2006.
16

A
Related works
In this section, we discuss a few lines of related work in detail.
First, the closest related works involve RL with unsupervised-learning oracles (Du et al., 2019; Feng
et al., 2020). Instead of investigating low-rank MDPs, they consider more restricted block MDPs and need
stronger assumptions such as reachability, identiﬁability, and separatability (we refer the reader to their
works for the deﬁnitions). Their notion of “decoder” looks like density features in low-rank MDPs, but
they are incomparable. The crucial property of “decoder” is that it is a map from the X space to the low
d dimensional space. This map itself no longer exists in low-rank MDPs. In addition, the density feature
serves a different purpose in our paper, as its primary purpose is for constructing the weight function class.
A second line of related work is model-based representation learning in low-rank MDPs (Agarwal et al.,
2020; Uehara et al., 2021b; Ren et al., 2022), which assumes that both a realizable left feature class Φ ∋φ∗
and realizable density (right) feature class Υ ∋µ∗are given to the learner, essentially inducing a realizable
dynamics model class. The learned model (features) are subsequently used for downstream planning. In
comparison, we utilize a much weaker inductive bias as we only require a realizable density feature class Υ,
and we do not try to learn a dynamics model. Though we additionally need a policy class Π, this is a very
basic and natural function class to include. It can be immediately obtained from the (Q-)value function class
in the value-based approach, and from the dynamics model class (given a reward function) in the model-
based approach above. In terms of the algorithm design, we also use MLE, but for a different objective (the
data distribution, instead of the dynamics model).
The importance weight (density-ratio) learning used within our algorithms is related to the marginalized
importance sampling of the ofﬂine RL algorithms in Nachum et al. (2019a); Lee et al. (2021); Uehara et al.
(2021a); Zhan et al. (2022); Chen and Jiang (2022); Huang and Jiang (2022); Ozdaglar et al. (2022). These
works do not make the low-rank MDP assumption and study the problem in general MDPs, and require both
a weight function class and value function class for learning. We leverage the true density µ∗or density
feature class Υ to construct the realizable weight function class, allowing us to achieve statistically faster
rates in the low-rank MDP setting. We do not need a value function class and instead only need a weaker
(as discussed in the previous paragraph) policy class Π. Lastly, we note that the aforementioned works
all learn weights, while our goal is to learn the densities. Extracting the densities from the weights allows
us to efﬁciently explore the MDP using its low-dimensional structure, and additionally enables our return
maximization guarantees of Proposition 1 by separating them from the underlying data distribution.
B
Hardness result without the policy class
In this section, we show that without policy class Π, learning in low-rank MDPs (or an easier simplex
feature setting) is provably hard even when the true density feature µ∗is known to the learner. The crux is
that low-rank MDPs can readily emulate a fully general contextual bandit problem, where µ∗is useless. For
the hardness result, we adapt Theorem 2 of Dann and Brunskill (2015) to our case by only keeping their
second to third level to get a contextual bandit problem.
To provide speciﬁcs for the reward and transition functions, we ﬁrst note that the subscript of the re-
ward/transition function denotes which level it applies to (e.g., P0 are the transitions to x1 from x0). Level
h = 0 is composed of |X| −3 states with zero reward, i.e., x0 ∈{1, . . . , |X| −3} and R0(i) = 0, ∀i ∈
{1, . . . , |X|−3}. Level h = 1 is composed of 2 states, i.e., x1 ∈{+, −}, where R1(+) = 1 and R1(−) = 0.
Lastly, at level h = 2 we have a single null absorbing state x2.
For the transition functions, in level h = 0 the transitions P0 are Bernoulli distributions where for any
state i ∈{1, . . . , |X| −3} and action a0 ∈A, we have P0(+|i, a0) =
1
2 + ε′
i(a0) and P0(−|i, a0) =
1
2 −ε′
i(a0). Here, ε′
i is deﬁned in a per-state manner given a parameter ε. We have ε′
i(a0) = ε/2 if a0 = a∗
0,
17

where a∗
0 is a ﬁxed action; ε′
i(a0) = ε if a0 = ai,∗
0 where ai,∗
0 is an unknown action deﬁned per state i; and
ε′
i(a0) = 0 otherwise. In level h = 1, the transitions P1 simply transmit deterministically to the absorbing
state x2, i.e., P1(x2|x1, a1) = 1 for all x1 ∈{+, −} and a1 ∈A.
It is easy to see that the dynamics of this contextual bandit can be modeled using simplex features, thus
it is an instantiation of low-rank MDPs. Since we only have two levels (H = 2), we only need to verify
that P0 and P1 can be written in the desired form (Assumption 1). In level h = 0, we add two latent states
corresponding to the rewarding and non-rewarding state, thus d = 2. Then in level h = 0, we have right
features µ∗
0(+) = [1, 0] and µ∗
0(−) = [0, 1], and left features φ∗
0(x0, a0) = [P1(+|x0, a0), P1(−|x0, a0)]
for any (x0, a0), corresponding to the original Bernoulli distribution. It is easy to see that this satisﬁes
Assumption 1, i.e., for any (x0, a0, x1) we have P0(x1|x0, a0) = ⟨φ∗
0(x0, a0), µ∗
0(x1)⟩. In level h = 1 we
can simply set a single latent state representing the singleton x2, and observe that Assumption 1 is trivially
satisﬁed with µ∗
1(x2) = 1, and φ∗
1(x1, a1) = 1 for any (x1, a1).
Finally, from Theorem 2 of Dann and Brunskill (2015), we know that the sample complexity of learning
in this contextual bandit problem is Ω(|X|), demonstrating that efﬁcient learning is impossible in low-rank
MDPs (or the simplex feature setting) given only µ∗.
The necessity of K = |A| dependence
It is well known that learning contextual bandits with just a
policy class requires a dependence on |A| in regret and sample complexity; see Agarwal et al. (2014) and
the references therein. This can also be reproduced in the above hardness result: ﬁrst, we can scale up the
construction by adding more actions, and show an Ω(|X|K) lower bound. Second, we now provide the
learner with a policy class that contains all Markov deterministic policies. The size of the class is O(K|X|),
and the log-size is O(|X| log(K)). Given the logarithmic dependence on K, no polynomial dependence on
log(|Π|) can explain away the linear-in-K dependence in the lower bound, and we must introduce K as a
separate factor in the sample complexity.
C
RL with objectives on state distributions
Proposition 1 also extends to general optimization objectives f({dh}) that are Lipschitz in the input {dh}
(note the Lipschitz property does not require the input to be a valid distribution). This Lipschitzness property
is key for many recent results in convex RL (Zahavy et al., 2021; Mutti et al., 2022), and also holds for return
maximization where f({dπ
h}) = vπ
R, in which case the Lipschitz constant is related to the maximum reward
maxh,x,a Rh(x, a). While we write the objective f({dh}) using state densities dh(xh) as input for simplicity,
it is straightforward to instead use state-action densities dh(xh)π(ah|xh) formed by directly composing the
state density dh with the policy π. If f is Lipschitz in state-action densities, it will still be Lipschitz in the
state-action densities in the ℓ1 norm, which is the exactly the case in return maximization, since any input
density will be composed with same π. Lastly, we note that constraints can also be added to the objective
and to result in a similar statement.
Proposition 4. Suppose the optimization objective is f({dh}), where f is Lipschitz in {dh} under the ℓ1
norm, i.e., there exists a constant L > 0 such that for any {d′
h} and {d′′
h}
f({d′
h}) −f({d′′
h})
 ≤L
X
h∈[H]
∥d′
h −d′′
h∥1.
Then for {bdπ
h} such that ∥bdπ
h −dπ
h∥1 ≤
ε
2H for all π ∈Π and h ∈[H], and bπ maximizing the plug-in
estimate of the objective:
bπ = argmax
π∈Π
f({bdπ
h}),
18

we have
f({dbπ
h}) ≥max
π∈Π f({dπ
h}) −Lε.
Proof. For any π ∈Π, from the Lipschitz assumption,
f({dπ
h}) −f({bdπ
h})
 ≤L
X
h∈[H]
∥dπ
h −bdπ
h∥1 ≤Lε/2.
Then, letting π∗= argmaxπ∈Π f({dπ
h}) denote the maximizer of the true objective and using the above
inequality,
f({dbπ
h}) −f({dπ∗
h }) = f({dbπ
h}) −f({bdbπ
h}) + f({bdbπ
h}) −f({bdπ∗
h }) + f({bdπ∗
h }) −f({dπ∗
h }) ≥−Lε.
On bdπ
h being invalid distributions
One potential issue is that some of the objective functions f considered
in the literature are only well deﬁned for valid probability distributions (e.g., entropy). This is easy to deal
with in the online setting, as we can simply project bdπ
h onto the probability simplex, which picks up a
multiplicative factor of 2 in ∥bdπ
h −dπ
h∥1 (c.f. the analysis of the linearization step in Algorithm 2).
For the ofﬂine setting, however, the situation can be trickier. For example, the above projection idea is
clearly bad for return maximization, since after projection all bdπ
h satisfy ∥bdπ
h∥1 = 1 and we lose pessimism.
From an analytical point of view, pessimistic approaches (e.g., Theorem 3) only pays one factor of the
missingness error ∥d
π
h −dπ
h∥1 by leveraging its one-sidedness, and a factor of 2 introduced by projection is
simply unacceptable. Therefore, the question is whether we can generalize the pessimism in Theorem 3 to
general objective functions. We only answer this question with a rough sketch and leave the full investigation
to future work: roughly speaking, since we know ∥bdπ
h −d
π
h∥1 ≤ε′ (for some appropriate value of ε′ from
our analysis), we can form a version space for dπ
h as:
dπ
h ∈{dh : ∃d′
h, s.t. dh ≥d′
h and ∥d′
h −bdπ
h∥1 ≤ε′}.
Then we can simply come up with pessimistic evaluation of f({dπ
h}) by minimizing f({dh}) over the above
set. It is not hard to see that such an approach will provide similar guarantees to Theorem 3 when applied to
return maximization.
D
Alternative setups, algorithm designs, and analyses
D.1
Ofﬂine data assumptions
As mentioned in Section 3, our ofﬂine data assumption allows sequentially dependent batches, where in-
batch tuples are i.i.d. samples. This is already weaker than the standard fully i.i.d. settings considered in the
ofﬂine RL literature, and here we further comment on how to handle various extensions.
Trajectory data
One simple setting is when data are i.i.d. trajectories sampled from a ﬁxed policy. (This
setting does not ﬁt our need for the online algorithm, but is a representative setup for the purpose of ofﬂine
learning.) While our protocol directly handles it (we can simply split the data in H chunks and call them
D0, D1, . . .), it seems somewhat wasteful as we only extract 1 transition tuple per trajectory, potentially
worsening the sample complexity by a factor of H. This is because in our analysis of the regression step
(Algorithm 1, line 5), we treat the regression target (which depends on bdπ
h) as ﬁxed and independent of the
current dataset. If we want to use all the data, we would need to union bound over the target as well; see
19

similar considerations in the work of Fan et al. (2020). A slow-rate analysis follows straightforwardly, and
we leave the investigation of fast-rate analysis to future work. We also remark that our current ofﬂine setup
(Assumption 2) is the most natural protocol for the data collected from the online algorithm (Section 4), and
using full trajectory data does not seem to improve the theoretical guarantees of the online setting.
Fully adaptive data
A more general setting than Assumption 2 is that the data is fully adaptive, i.e., each
trajectory is allowed to depend on all trajectories that before it. To handle such a case, we will need to
replace the i.i.d. concentration inequalities with their martingale versions. Some special treatment in the
concentration bounds will also be needed to handle the random data-splitting step in Algorithm 1, line 3
(c.f. Mohri and Rostamizadeh, 2008); alternatively, if we union bound over regression targets (see previous
paragraph), the data splitting step will no longer be needed.
Unknown and/or non-Markov πD
In Assumption 2 we assume that the last-step policy in the data-
collecting policy is Markov and known, as we need it to form the importance weights on actions. When πD
is still Markov and unknown, we can use behavior cloning to back it out from data, which would require
some additional assumptions (e.g., having access to a policy class that realizes πD), and we do not further
expand on such an analysis. When πD is non-Markov, it is well known that the action in the data tuple
(xh, ah, xh+1) can be still treated as if it were generated from a Markov policy—one can compute the
state-action occupancy for (xh, ah) (which is well-deﬁned even if πD is non-Markov) and then obtain the
equivalent Markov policy by conditioning on xh. Incidentally, the algorithmic solution is the same as the
case of unknown Markov πD, i.e., behavior cloning.
D.2
Stochastic and/or unknown reward functions
When the reward function is stochastic but still known, Proposition 1 and all policy optimization guarantees
extend straightforwardly, since we can still directly compute the return. The more nontrivial case is when
the reward function R is unknown and comes as part of the data, i.e., we have the usual format of data
tuples that include (possibly) stochastic reward signals, {(x(i)
h , a(i)
h , r(i)
h )}nret
i=1 ∼dD
h . Then given estimates
{bdD
h } (from MLE) and {bdπ
h} (from Algorithm 1 or Algorithm 2), the expected return can be estimated by
reweighting the rewards according to the importance weight bdπ
h/bdD
h , and assuming this ratio is well-deﬁned:
bvπ
R =
1
nret
nret
X
i=1
X
h∈[H]
bdπ
h(x(i)
h )
bdD
h (x(i)
h )
πh(a(i)
h |x(i)
h )
πD
h (a(i)
h |x(i)
h )
r(i)
h .
It can be shown that we then have |bvπ
R −vπ
R| ≤ε + (additive terms), where the additive terms correspond to
the statistical error of return and MLE estimation, which is O((nret)−1/2). If bdD
h does not cover bdπ
h, which
may generally be the case, clipping (e.g., according to thresholds Cx
h, Ca
h) can again be used, which will
lead to additional error corresponding to clipped mass.
D.3
Algorithm design and analyses
In this section, we discuss alternative designs of the ofﬂine density learning algorithm (Algorithm 1), as
well as their downstream impacts on the online and representation learning algorithms, which use the ofﬂine
module in their inner loops. For simplicity, most discussions are in the case of ofﬂine density learning with
known representation µ∗.
20

Point estimate in denominator
First, we discuss alternative parameterizations of the weight function
class. To enable more “elementary” ℓ∞covering arguments, one may consider instead parameterizing the
weight function class as a ratio of linear functions over a ﬁxed function vh : X →R, speciﬁcally
Wh(vh) =

wh = ⟨µ∗
h−1, θh⟩
vh
: ∥wh∥∞≤Cx
h−1Ca
h−1, θh ∈Rd

.
When µ∗consists of simplex features, it can be shown that an ℓ∞covering with scale γ of size (1/γ)d can
be constructed for Wh(vh), because it can be induced by an ℓ∞covering of the low-dimensional parameter
space that has scale adaptively chosen according to how much the weight can be perturbed with respect to
the denominator, thus ﬁxed size. It is unclear how to construct such ℓ∞coverings for “linear-over-linear”
function classes such as Wh of Algorithm 1. One may consider compositions of standard ℓ∞coverings gen-
erated separately for the linear numerator and denominator, but bounding the covering error is challenging
due to sensitivity of the denominator to perturbations.
As we will see, however, the key issue with such ﬁxed-denominator parameterizations is that the Bayes-
optimal solution is no longer realizable. To handle this in the analysis, we can introduce an additional
approximation error (similar to Chen and Jiang (2019, Assumption 3) in the value learning setting) that will
appear in the ﬁnal bound, corresponding to how well the Bayes-optimal solution is approximated by the
function class. Depending on the choice of denominator, the approximation error may not be controlled, or
may lead to a slower rate of estimation; loosely, it is deﬁned as
εapprox
h
=
max
wh−1:∥wh−1∥∞≤Cx
h−1
min
wh∈Wh(vh)
wh −Eπ
h−1(dD
h−1wh−1)

2,dD,†
h−1 .
One obvious choice for the ﬁxed denominator is vh = bd D,†
h−1, since it is immediately available from the
MLE data estimation step, plus the linear numerator can then be extracted exactly through the elementwise
multiplication bdπ
h = bwπ
h bd D,†
h−1. However, the Bayes-optimal predictor Eπ
h−1(dh−1) is no longer realizable,
since Eπ
h−1(dh−1) = Pπ
h−1(dh−1)/dD,†
h−1 is a linear function over the true data distribution dD,†
h−1. In this case,
using Lemma 19 gives a more interpretable upper bound on the approximation error involves the difference
between the ratio of any linear dh covered on dD,†
h−1 and the corresponding ratio over bd D,†
h−1:
εapprox
h
≤
max
dh=⟨µ∗
h−1,θh⟩:
dh≤Cx
h−1Ca
h−1dD,†
h−1

dh
bd D,†
h−1
−
dh
dD,†
h−1

2,dD,†
h−1
.
However such approximation error may be difﬁcult to control even with small data estimation error due to
sensitivity of the denominator (for example if ∥bd D,†
h−1 −dD,†
h−1∥1 ≤εmle but they have disjoint support).
Barycentric spanner in denominator
To avoid the above support issue and control the approximation
error, we can instead consider a denominator function upon which dD,†
h−1 is supported. This is satisﬁed by
the barycentric spanner of the version space of the estimate bd D,†
h−1,
Vh =
n
vh = ⟨µ∗
h−1, θh⟩: ∥vh −bd D,†
h−1∥1 ≤εmle, θh ∈Rdo
,
noting that dD,†
h−1 ∈Vh with high probability due to the MLE guarantee. Then letting evh denote the spanner,
Lemma 15 guarantees that
dD,†
h−1
evh
≤d, and the approximation error of Wh(evh) can be controlled by the error
of MLE estimation, since for any dh ≤Cx
h−1Ca
h−1dD,†
h−1 we have

dh
evh
−
dh
dD,†
h−1

2
2,dD,†
h−1
≤(Cx
h−1Ca
h−1)2
Z dD,†
h−1(x)
evh(x)
 
1 + dD,†
h−1(x)
evh(x)
! evh(x) −dD,†
h−1(x)
 (dx)
21

≤2(Cx
h−1Ca
h−1d)2∥evh −dD,†
h−1∥1
which implies that εapprox
h
≤2Cx
h−1Ca
h−1d√εmle by the deﬁnition of Vh. However, since εmle is O(n−1/2
mle ),
this results in a slow rate of 1/ε4 total sample complexity for ofﬂine density estimation, and from a compu-
tational standpoint, introduces another barycentric spanner construction step in the algorithm which can be
expensive. The representation learning setting has the additional challenge that there will be approximation
error if the wrong representation bµh−1 ∈Υh−1 is chosen for bd D,†
h−1, since dD,†
h−1 /∈Vh(bµh) (we extend the
deﬁnition to Vh(µh−1) =
n
vh = ⟨µh−1, θh⟩: ∥vh −bd D,†
h−1∥1 ≤εmle, θh ∈Rdo
), which, as in the ﬁrst case
above, may be difﬁcult to bound.
Clipped function class with point estimate in denominator
Generalizing and improving upon the pre-
vious analyses, using a clipped version of the function class Wh(vh)
Wclip
h
(vh) =

wh = ⟨µ∗
h−1, θh⟩∧Cx
h−1Ca
h−1vh
vh
: θh+1 ∈Rd

will allow us to bound the approximation error for general denominator functions vh. For any dh such that
dh ≤Cx
h−1Ca
h−1dD,†
h−1, we can approximate the ratio
dh
dD,†
h−1
with
dh∧Cx
h−1Ca
h−1vh
vh
∈Wclip
h
(vh), and separate
the approximation error into two terms, based on whether dD,†
h−1 is covered by vh according to a threshold
C ≥1:

dh ∧Cx
h−1Ca
h−1vh
vh
−
dh
dD,†
h−1

2
2,dD,†
h−1
≤

 
dh ∧Cx
h−1Ca
h−1vh
vh
−
dh
dD,†
h−1
!
· 1
"
dD,†
h−1(x)
vh(x)
≤C
#
2
2,dD,†
h−1
(“covered”)
+

 
dh ∧Cx
h−1Ca
h−1vh
vh
−
dh
dD,†
h−1
!
· 1
"
dD,†
h−1(x)
vh(x)
> C
#
2
2,dD,†
h−1
(“not covered”)
Bounding the two terms individually, for the “covered” term, we have
(“covered”) ≤
Z
x:
dD,†
h−1(x)
vh(x) ≤C
dD,†
h−1(x)
 
dh(x)
vh(x) −
dh(x)
dD,†
h−1(x)
!2
(dx)
≤(Cx
h−1Ca
h−1)2
Z
x:
dD,†
h−1(x)
vh(x) ≤C
dD,†
h−1(x)
vh(x)
(dD,†
h−1(x) −vh(x))2
vh(x)
(dx)
≤(Cx
h−1Ca
h−1)2C(1 + C)
Z
x:
dD,†
h−1(x)
vh(x) ≤C
dD,†
h−1(x) −vh(x)

≤(Cx
h−1Ca
h−1)2C(1 + C)
dD,†
h−1 −vh

1 .
For the “not covered” term, noticing that both parenthesized ratios are bounded on [0, Cx
h−1Ca
h−1], we
have
(“not covered”) ≤(Cx
h−1Ca
h−1)2
Z
dD,†
h−1(x) · 1
"
dD,†
h−1(x)
vh(x)
> C
#
(dx)
22

≤(Cx
h−1Ca
h−1)2

1 −1
C
−1 dD,†
h−1 −vh

1 ,
where the second inequality is because

1 −1
C
 Z
x:
dD,†
h−1(x)
vh(x) >C
dD,†
h−1(x)(dx) <
Z
x:
dD,†
h−1(x)
vh(x) >C
(dD,†
h−1(x) −vh(x))(dx) ≤
dD,†
h−1 −vh

1
since
dD,†
h−1
C
> vh. Thus in total, we have
εapprox
h
≤Cx
h−1Ca
h−1

C + C2 +
C
C −1
 rdD,†
h−1 −vh

1.
The bound depends on how close the point estimate vh is to the true dD,†
h−1, as well as the threshold C. In
the case where vh = bd D,†
h−1 is the point estimate, we are now able to bound εapprox
h
≤Cx
h−1Ca
h−1(C + C2 +
C
C−1)√εmle, which results in a slower rate than our results in the main text. If vh = evh is the barycentric
spanner of the version space, then it sufﬁces to set C = d, in which case only the “covered” part of the error
is nonzero, and we recover the analysis in the previous paragraph.
In general, the best choice of threshold C is not obvious because dD,†
h−1 is not known, and will trade off
between the two errors. When C is large, the “covered” error will be large since it is proportional to C2,
while if C is too small (too close to 1), the “not-covered” error will be large since it is proportional to
C
C−1.
Direct extraction of the estimate
Putting aside the discussion of point estimates in the denominator, we
now present an alternative to pointwise multiplication + linearization used to extract bdπ
h from Algorithm 1.
Instead, we can directly extract the numerator, which will already be a linear function (in µ∗), from weight
ratio and use it as the estimate for bdπ
h. The regression objective might then be (replacing line 5 in Algo-
rithm 1)
, bdπ
h = argmin
vh∈Vh
argmin
dh∈Fh(vh)
LDreg
h−1
 
dh
vh
,
bdπ
h−1 ∧Cx
h−1 bdD
h−1
bdD
h−1
, πh−1 ∧Ca
h−1πD
h−1
!
,
where the version space of denominator functions Vh is deﬁned above, and Fh(vh) = {dh = ⟨µ∗
h−1, θh⟩:
∥dh/vh∥∞≤Cx
h−1Ca
h−1, θh ∈Rd} represents linear numerator functions covered by vh. It is necessary to
constrain the denominator functions to the version space in order to ensure that the numerator is close to the
true density, since regression only guarantees quality of estimated weight. For example, even if bwπ
h = wπ
h,
if the denominator function is c · dD,†
h−1 then the numerator will be c · dπ
h, leading to large bdπ
h estimation error.
In terms of the analysis, this is quantiﬁed as the error between the denominator and true dD,†
h−1 in Eq. (9),
which is controlled by εmle when the denominator is constrained to the version space Vh, and will result in
the same guarantee as we have for Algorithm 1 and Algorithm 2 in the known feature setting. In the online
setting with known features, direct extraction has the advantage of no longer requiring the linearization step
(line 7 in Algorithm 2), though it is computationally more expensive because the function classes are jointly
optimized, and the version space must be maintained. This advantage is lost in the representation learning
setting because the estimates {bdπ
h}π∈Π must be jointly re-linearized with the same representation in order to
construct the policy cover (line 9 of Algorithm 4).
23

MLE instead of regression
An alternative to using regression to estimate the occupancy is instead using
MLE-type estimation. Along similar veins as the regression algorithm, (a clipped version of) the previous-
level estimate bdπ
h−1 must be reused to reweight the data distribution in order to estimate bdπ
h:
bdπ
h = argmin
fh∈Fh
1
n
n
X
i=1
bdπ
h−1 ∧Cx
h−1 bdD
h−1
bdD
h−1
πh−1 ∧Ca
h−1πD
h−1
πD
h−1
log(fh).
where Fh is some linear function class. One possible advantage of such an approach is that a linear density
estimate can be directly learned, but establishing formal guarantees for an MLE-type algorithm remains
future work. After separating the missingness error ∥dπ
h −d
π
h∥1 in the same way as in Section 3, similar
methods as classical MLE analysis (Appendix H) might be used to control ∥bdπ
h −d
π
h∥1. The challenge is that
such MLE analyses require Fh to include only valid densities ∈∆(X), but this is at odds with reweighted
MLE objectives such as the one above, since the weights
bdπ
h−1∧Cx
h−1 bdD
h−1
bdD
h−1
generally will not induce a valid
density when multiplied with the data distribution.
D.4
Discussion of other approaches for controlling error exponentiation in the online setting
Barycentric spanner in regression target (without clipping)
In Section 4 we controlled the error ex-
ponentiation arising from having only approximately exploratory data by ﬁrst clipping the regression target
bdπ
h/bdD
h (since the MLE estimate bdD
h does not necessarily cover bdπ
h), then separating the error ∥bdπ
h −dπ
h∥1 into
the “two-sided regression error” and “one-sided missingness error”. It will be instructive to also look at an
alternative approach that avoids clipping and “pretends” that data is perfectly exploratory, which provides
interesting insights on the underlying issue and the delicacy of error propagation in our problem from a
different perspective.
The seemingly feasible solution is based on the observation that 1
d
Pd
i=1 bdπh,i
h
, the barycentric spanner of
{bdπ
h}π∈Π in the denominator of Eq. (5), is a good approximation of dD
h . So instead of using MLE to estimate
dD
h = 1
d
Pd
i=1 dπh,i
h
, we could simply use 1
d
Pd
i=1 bdπh,i
h
, which will keep the regression target bounded in
Algorithm 1 without any clipping.
However, a closer look reveals that this only sweeps the issue under the rug. The problem does not
go away, and only appears in a different form: recall from Lemma 1 that the bound includes a term of
2d
bdD
h −dD
h

1, and when we use 1
d
Pd
i=1 bdπh,i
h
to replace bdD
h , we obtain
bdD
h −dD
h

1 =

1
d
d
X
i=1
bdπh,i
h
−1
d
d
X
i=1
dπh,i
h

1
≤max
π∈Π ∥bdπ
h −dπ
h∥1
which, in addition to merging the two inductive chains, gives us ∥bdπ
h−dπ
h∥1 ≤(1+d) maxπ∈Π ∥bdπ
h−dπ
h∥1+
. . ., resulting in O(d)H error. In other words, because the error of the denominator distribution depends on
the quality of regression, even with full coverage we will suffer the same error exponentiation issues.
Reachability-based approach
Error exponentiation can be avoided if a reachability assumption (Du et al.,
2019; Modi et al., 2021) is satisﬁed in the underlying MDP. Formally, this assumption requires that there
exists a constant ηmin such that ∀h ∈[H], z ∈Zh+1 we have maxπ∈Π Pπ[zh+1 = z] ≥ηmin, where Zh+1
correspond to the latent states of the MDP. For example, in the case where µ∗
h is full-rank and composed of
simplex features, Zh+1 = {1, . . . , d} and θh[i] directly corresponds to Pπ[zh+1 = i] for i ∈{1, . . . , d}. The
direct implication is that we can construct a fully exploratory policy cover that reaches all latent states (and
thus covers all π ∈Π) as long as we ﬁnd, for each latent state, the policy that reaches it with probability
24

at least ηmin. This policy can be found as long as bdπ
h is estimated sufﬁciently well, which when backed up
implies the latent state visitation is estimated sufﬁciently well.
Speciﬁcally, in the ofﬂine module used in Algorithm 2, we can instead set nreg such that ∥bdπ
h −dπ
h∥1 ≤
σmin(µ∗
h−1)ηmin/4 for all π ∈Π, which implies that when backed up to latent states the error of estimation
is ∥bθπ
h −θπ
h∥∞≤ηmin/4. Then the exploratory policy cover can be chosen as Πexpl
h
= {πh,i}d
i=1 where
for each i ∈{1, . . . , d}, πh,i is such that bθπh,i
h
[i] ≥ηmin/4, which implies θπh,i
h
[i] ≥ηmin/2 with high
probability, and such a policy is guaranteed to exist from the reachability assumption. Since the policy
cover is fully exploratory, a single induction chain in the error analysis (instead of the two in Figure 1) will
sufﬁce.
E
Off-policy occupancy estimation proofs (Section 3)
E.1
Discussion of clipping thresholds for ¯dπ
As we have previously mentioned, the clipped occupancy d
π
h depends on clipping thresholds {Cx
h} and
{Ca
h} that are hyperparameter inputs to the ofﬂine estimation algorithm (Algorithm 1). To better understand
the effects of Cx
h, Ca
h on d
π
h and downstream analysis, we highlight three properties below, which we have
written only for Cx
h (but that take analogous forms for Ca
h).
Importantly, property 3 shows that the missingness error ∥d
π
h −dπ
h∥1 is Lipschitz in the clipping thresh-
olds {Cx
h}, indicating that small changes in Cx
h will only lead to small changes in the missingness error, and
thus the result of Theorem 2. For practical purposes, this serves as a reassurance that, within some limit,
misspeciﬁcations of Cx
h, Ca
h in the algorithm do not have catastrophic consequences.
Proposition 5. For two sets of clipping thresholds {Cx
h}, {(Cx
h)′}, following Deﬁnition 1, for each h =
1, . . . , H let their corresponding clipped occupancies be deﬁned recursively as
d
π
h = Pπ
h−1

d
π
h−1 ∧Cx
h−1dD
h−1

(d
π
h)′ = Pπ
h−1

(d
π
h−1)′ ∧(Cx
h−1)′dD
h−1

with d
π
0 = (d
π
0)′ = d0. Then the following two properties hold for each h ∈[H]:
1. (Monotonicity) d
π
h ≤(d
π
h)′ if Cx
h′ ≤(Cx
h′)′ for all h′ < h. The relationship also holds in the other
direction, i.e., replacing “≤” with “>”.
2. (Clipped occupancy Lipschitz in thresholds) ∥(d
π
h)′ −d
π
h∥1 ≤P
h′<h |(Cx
h′)′ −Cx
h′|.
3. (Missingness error Lipschitz in thresholds)
∥dπ
h −(d
π
h)′∥1 −∥dπ
h −d
π
h∥1
 ≤P
h′<h |(Cx
h′)′ −Cx
h′|.
Proof. We prove these three claims one by one.
Proof of Claim 1
We will prove Claim 1 via induction. Suppose d
π
h′−1 ≤(d
π
h′−1)′ for some h′ ≤h. This
holds for the base case h′ = 1 since d
π
0 = (d
π
0)′. Then since Cx
h′−1 ≤(Cx
h′−1)′,
d
π
h′ = Pπ
h′−1

d
π
h′−1 ∧Cx
h′−1dD
h′−1

≤Pπ
h′−1

(d
π
h′−1)′ ∧(Cx
h′−1)′dD
h′−1

= (d
π
h′)′.
Then by induction we have that d
π
h ≤(d
π
h)′.
25

Proof of Claim 2
For Claim 2, using Lemma 20, we have
∥(d
π
h)′ −d
π
h∥1
≤


d
π
h−1 ∧Cx
h−1dD
h−1

−

(d
π
h−1)′ ∧(Cx
h−1)′dD
h−1

1
≤


d
π
h−1 ∧Cx
h−1dD
h−1

−

(d
π
h−1)′ ∧Cx
h−1dD
h−1

1 +


(d
π
h−1)′ ∧Cx
h−1dD
h−1

−

(d
π
h−1)′ ∧(Cx
h−1)′dD
h−1

1
≤
d
π
h−1 −(d
π
h−1)′
1 +
Cx
h−1dD
h−1 −(Cx
h−1)′dD
h−1

1
=
d
π
h−1 −(d
π
h−1)′
1 +
Cx
h−1 −(Cx
h−1)′ .
Unfolding this recursion from level h −1 through level 0 gives the result.
Proof of Claim 3
For Claim 3, we have
∥dπ
h −(d
π
h)′∥1 −∥dπ
h −d
π
h∥1
 =

Z
|dπ
h(x) −(d
π
h)′(x)| −|dπ
h(x) −d
π
h(x)|(dx)

≤
Z |dπ
h(x) −(d
π
h)′(x)| −|dπ
h(x) −d
π
h(x)|
 (dx)
≤
Z (d
π
h)′(x) −d
π
h(x)
 (dx)
(since ||x| −|y|| ≤|x −y|)
=
(d
π
h)′ −d
π
h

1
Then applying Claim 2 gives the stated claim.
E.2
Proof of occupancy estimation
Proposition (Restatement of Proposition 2). We have the following properties for d
π
h:
1. d
π
h ≤dπ
h.
2. d
π
h = dπ
h when data covers π, i.e., ∀h′ < h we have dπ
h′ ≤Cx
h′dD
h′ and πh′ ≤Ca
h′πD
h′.
3. ∥d
π
h −dπ
h∥1 ≤∥d
π
h−1 −dπ
h−1∥1 + ∥d
π
h−1 −d
π
h−1 ∧Cx
h−1dD
h−1∥1 + ∥Pπ
h−1dπ
h−1 −Pπ
h−1dπ
h−1∥1.
Proof. We prove these three claims one by one.
Proof of Claim 1
Firstly, we have d
π
h = dπ
h = d0. Assuming the claim holds for h′ −1, then we have
d
π
h′ = Pπ
h′−1(d
π
h′−1 ∧Cx
h′−1dD
h′−1) ≤Pπ
h′−1(d
π
h′−1 ∧Cx
h′−1dD
h′−1) ≤Pπ
h′−1(dπ
h′−1 ∧Cx
h′−1dD
h′−1) ≤
Pπ
h′−1dπ
h′−1 = dπ
h′. By induction, we complete the proof.
Proof of Claim 2
It is easy to see that dπ
h′ ≤Cx
h′dD
h′ together with Claim 1 implies d
π
h′ ≤Cx
h′dD
h′, thus
∥d
π
h′−d
π
h′∧Cx
h′dD
h′∥1 = 0. In addition, πh′ ≤Ca
h′πD
h′ gives us πh′ = πh′, therefore
Pπ
h′−1dπ
h′−1 −Pπ
h′−1dπ
h′−1

1 =
0. Now we can prove Claim 2 inductively. For h′ = 0, we know the claim holds since d
π
0 = dπ
0 = d0. As-
suming the claim holds for h′ −1, by Claim 3 we have that
0 ≤∥d
π
h′−dπ
h′∥1 ≤∥d
π
h′−1−dπ
h′−1∥1+∥d
π
h′−1−d
π
h′−1∧Cx
h′−1dD
h′−1∥1+∥Pπ
h′−1dπ
h′−1−Pπ
h′−1dπ
h′−1∥1 = 0.
This means the claim holds for h′. By induction, we complete the proof.
26

Proof of Claim 3
For the third part, we have the following decomposition
∥d
π
h −dπ
h∥1 =
Pπ
h−1

d
π
h−1 ∧Cx
h−1dD
h−1

−Pπ
hdπ
h−1

1
≤
Pπ
h−1

d
π
h−1 ∧Cx
h−1dD
h−1

−Pπ
h−1dπ
h−1

1 +
Pπ
h−1dπ
h−1 −Pπ
hdπ
h−1

1
≤
d
π
h−1 ∧Cx
h−1dD
h−1 −dπ
h−1

1 +
Pπ
h−1dπ
h−1 −Pπ
hdπ
h−1

1
(Lemma 20)
≤
d
π
h−1 ∧Cx
h−1dD
h−1 −d
π
h−1

1 +
d
π
h−1 −dπ
h−1

1 +
Pπ
h−1dπ
h−1 −Pπ
hdπ
h−1

1 .
Lemma (Restatement of Lemma 1). For every h ∈[H], the error between estimates bdπ
h from Algorithm 1
and the clipped target d
π
h is decomposed recursively as
bdπ
h −d
π
h

1 ≤
bdπ
h−1 −d
π
h−1

1 + 2Cx
h−1
bdD
h−1 −dD
h−1

1 + Cx
h−1Ca
h−1
bd D,†
h−1 −dD,†
h−1

1
+
√
2
 bwπ
h −Eπ
h−1
 
dD
h−1
bdπ
h−1 ∧Cx
h−1 bdD
h−1
bdD
h−1
!
2,dD,†
h−1
,
where (Eπ
hdh) := (Pπ
hdh)/dD,†
h
.
Proof. We start by separating out the recursive term
bdπ
h −d
π
h

1 =
bdπ
h −Pπ
h−1

d
π
h−1 ∧Cx
h−1dD
h−1

1
≤
bdπ
h −Pπ
h−1

bdπ
h−1 ∧Cx
h−1 bdD
h−1

1 +
Pπ
h−1

bdπ
h−1 ∧Cx
h−1 bdD
h−1

−Pπ
h−1

d
π
h−1 ∧Cx
h−1 bdD
h−1

1
+
Pπ
h−1

d
π
h−1 ∧Cx
h−1 bdD
h−1

−Pπ
h−1

d
π
h−1 ∧Cx
h−1dD
h−1

1
≤
bdπ
h −Pπ
h−1

bdπ
h−1 ∧Cx
h−1 bdD
h−1

1 +
bdπ
h−1 ∧Cx
h−1 bdD
h−1 −d
π
h−1 ∧Cx
h−1 bdD
h−1

1
+
d
π
h−1 ∧Cx
h−1 bdD
h−1 −d
π
h−1 ∧Cx
h−1dD
h−1

1
≤
bdπ
h −Pπ
h−1

bdπ
h−1 ∧Cx
h−1 bdD
h−1

1 +
bdπ
h−1 −d
π
h−1

1 + Cx
h−1
bdD
h−1 −dD
h−1

1 .
(7)
Here, we apply Lemma 20 in the second inequality. The last inequality is due to | min(x, y) −min(x, z)| ≤
|y −z| for x, y, z ∈R.
Now, we consider the ﬁrst term in Eq. (7) and get
bdπ
h −Pπ
h−1

bdπ
h−1 ∧Cx
h−1 bdD
h−1

1
≤

bdπ
h −Pπ
h−1
 bdπ
h−1 ∧Cx
h−1 bdD
h−1
bdD
h−1
dD
h−1
!
1
+
Pπ
h−1
 bdπ
h−1 ∧Cx
h−1 bdD
h−1
bdD
h−1
dD
h−1
!
−Pπ
h−1
 bdπ
h−1 ∧Cx
h−1 bdD
h−1
bdD
h−1
bdD
h−1
!
1
≤

bdπ
h −Pπ
h−1
 bdπ
h−1 ∧Cx
h−1 bdD
h−1
bdD
h−1
dD
h−1
!
1
+ Cx
h−1
dD
h−1 −bdD
h−1

1 .
(8)
In the last inequality, we notice

bdπ
h−1∧Cx
h−1 bdD
h−1
bdD
h−1

∞
≤Cx
h−1 by our convention 0
0 = 0 and apply Lemma 20
again.
27

Let ewh−1 :=
bdπ
h−1∧Cx
h−1 bdD
h−1
bdD
h−1
for short. Since ∥ewh−1∥∞≤Cx
h−1, Lemma 19 guarantees (Pπ
h−1(dD
h−1 ewh−1))
dD,†
h−1
≤
Cx
h−1Ca
h−1, thus the ratio is well-deﬁned. Then we can further upper-bound the ﬁrst term in Eq. (8) as
bdπ
h −Pπ
h−1
 dD
h−1 ewh−1

1 =
 bwπ
h bd D,†
h−1 −Pπ
h−1
 dD
h−1 ewh−1

dD,†
h−1
dD,†
h−1

1
≤
 bwπ
h bd D,†
h−1 −bwπ
h dD,†
h−1

1 +
 bwπ
h dD,†
h−1 −Pπ
h−1
 dD
h−1 ewh−1

dD,†
h−1
dD,†
h−1

1
=
 bwπ
h bd D,†
h−1 −bwπ
h dD,†
h−1

1 +
 bwπ
h −Pπ
h−1
 dD
h−1 ewh−1

dD,†
h−1

1,dD,†
h−1
≤∥bwπ
h∥∞
bd D,†
h−1 −dD,†
h−1

1 +
 bwπ
h −Pπ
h−1
 dD
h−1 ewh−1

dD,†
h−1

1,dD,†
h−1
≤Cx
hCa
h
bd D,†
h−1 −dD,†
h−1

1 +
 bwπ
h −Pπ
h−1
 dD
h−1 ewh−1

dD,†
h−1

2,dD,†
h−1
.
(9)
Combining Eq. (7), Eq. (8), and Eq. (9) and noticing the deﬁnition of Eπ
h and ewh−1 completes the
proof.
Theorem (Restatement of Theorem 2). Fix δ ∈(0, 1). Suppose Assumption 1 and Assumption 2 hold, and
µ∗is known. Then, given an evaluation policy π, by setting
nmle = ˜O

d

X
h∈[H]
Cx
hCa
h


2
log(1/δ)/ε2

and nreg = ˜O

d

X
h∈[H]
Cx
hCa
h


2
log(1/δ)/ε2

,
with probability at least 1 −δ, FORC (Algorithm 1) returns state occupancy estimates {bdπ
h}H−1
h=0 satisfying
bdπ
h −d
π
h

1 ≤ε, ∀h ∈[H].
The total number of episodes required by the algorithm is
˜O

dH

X
h∈[H]
Cx
hCa
h


2
log(1/δ)/ε2

.
Proof. We ﬁrst make two claims on MLE estimation and error propagation.
Claim 1
Our estimated data distributions satisfy that with probability 1 −δ/2, for any h ∈[H]
bdD
h −dD
h

1 ≤εmle and
bd D,†
h
−dD,†
h

1 ≤εmle,
(10)
where
εmle := 6
s
d log(16HBµnmle/δ)
nmle
.
28

Claim 2
Under the high-probability event that Eq. (10) holds, we further have that with probability at least
1 −δ/2, for any 1 ≤h ≤H,
bdπ
h −d
π
h

1 ≤
bdπ
h−1 −d
π
h−1

1 + 3Cx
h−1Ca
h−1εmle +
√
2εreg,h−1,
(11)
where
εreg,h−1 :=
s
221184d(Cx
h−1Ca
h−1)2 log (2Hnreg/δ)
nreg
.
Now we establish the ﬁnal error bound with these two claims. Notice that the total failure probability is
less than δ. Unfolding Eq. (11) from h′ = h to h′ = 1 and noticing that bdπ
0 = d
π
0 = d0 yields that for any
h ∈[H]
bdπ
h −d
π
h

1 ≤
h−1
X
h′=0

3Cx
h′Ca
h′εmle +
√
2εreg,h′

.
(12)
Substituting in the expressions for εmle and εreg,, we have
bdπ
h −d
π
h

1 ≤
h−1
X
h′=0

18Cx
h′Ca
h′
s
d log(16HBµnmle/δ)
nmle
+ 666Cx
h′Ca
h′
s
d log (2Hnreg/δ)
nreg

.
(13)
It is easy to see that if we set
nmle = ˜O

d

X
h∈[H]
Cx
hCa
h


2
log(1/δ)/ε2

and nreg = ˜O

d

X
h∈[H]
Cx
hCa
h


2
log(1/δ)/ε2

,
then we have
bdπ
h −d
π
h

1 ≤ε, ∀h ∈[H].
In the following, we provide the proof of these two claims respectively.
Proof of Claim 1
We start with a ﬁxed h ∈[H] and bounding ∥bdD
h −dD
h ∥1, where we recall that bdD
h is
the MLE solution in Eq. (2). By Lemma 22, we know that function class Fh has an ℓ1 optimistic cover
with scale 1/nmle of size (2⌈Bµnmle⌉)d. It is easy to see that the true marginal distribution dD
h ∈Fh
from Lemma 17 and any dh ∈Fh is a valid probability distribution over X. From Assumption 2, we
know that once conditioned on prior dataset D0:h−1, the current dataset Dmle
h
is drawn i.i.d. from the ﬁxed
distribution denoted as dD
h . Thus, Lemma 12 tells us that when conditioned on D0:h−1, with probability at
least 1 −δ/(4H)
∥bdD
h −dD
h ∥1 ≤
1
nmle
+
s
12 log(4H (2⌈Bµnmle⌉)d /δ)
nmle
+
6
nmle
(14)
≤
1
nmle
+
s
12d log(16HBµnmle/δ)
nmle
+
6
nmle
≤6
s
d log(16HBµnmle/δ)
nmle
= εmle.
(15)
29

Since Eq. (14) holds for any such ﬁxed D0:h−1, applying the law of total expectation gives us this that
Eq. (14) holds with probability 1 −δ/(4H) without conditioning on D0:h−1.
Similarly, with probability at least 1 −δ/(4H), for the MLE solution bd D,†
h
we have ∥bd D,†
h
−dD,†
h
∥1 ≤
εmle. Union bounding these two high-probability events and further union bounding over h ∈[H] gives us
that Eq. (10) holds with probability 1 −δ/2.
Proof of Claim 2
Notice that the proof in this part is under the high-probability event that Eq. (10) holds.
We consider a ﬁxed h ∈[H]. From Lemma 1, we have the error propagation result that
bdπ
h −d
π
h

1 ≤
bdπ
h−1 −d
π
h−1

1 + 2Cx
h−1
bdD
h−1 −dD
h−1

1 + Cx
h−1Ca
h−1
bd D,†
h−1 −dD,†
h−1

1
+
√
2
 bwπ
h −Pπ
h−1
 dD
h−1 ewh−1

dD,†
h−1

2,dD,†
h−1
,
(16)
where ewh−1 :=
bdπ
h−1∧Cx
h−1 bdD
h−1
bdD
h−1
.
Since bwπ
h ∈Wh, we have ∥bwπ
h∥∞≤Cx
hCa
h. The last term on RHS isolates the ﬁnite-sample error
of regression, involving the difference between the empirical minimizer bwπ
h and the population minimizer
Pπ
h−1(dD
h−1 ewh−1)
dD,†
h−1
of the regression objective. To bound this error, we apply Lemma 13 and Lemma 14, which
give us that, with probability at least 1 −δ/(2H),
 bwπ
h −Pπ
h−1
 dD
h−1 ewh−1

dD,†
h−1

2
2,dD,†
h−1
= E
h
LDreg
h−1 ( bwπ
h, ewh−1, π)
i
−E
"
LDreg
h−1
 
Pπ
h−1
 dD
h−1 ewh−1

dD,†
h−1
, ewh−1, π
!#
≤2
 
LDreg
h−1 ( bwπ
h, ewh−1, π) −LDreg
h−1
 
Pπ
h−1
 dD
h−1 ewh−1

dD,†
h−1
, ewh−1, π
!!
+ 2ε2
reg,h−1
(17)
where
εreg,h−1 :=
s
221184 · d(Cx
h−1Ca
h−1)2 log (2Hnreg/δ)
nreg
The ﬁrst term in Eq. (17) compares the empirical regression loss of the empirical minimizer bwπ
h against the
population solution. In order to show that this is ≤0, we ﬁrst need to check that
Pπ
h−1(dD
h−1 ewh−1)
dD,†
h−1
∈Wh.
As we have previously seen, we have
Pπ
h−1(dD
h−1 ewh−1)
dD,†
h−1
≤Cx
h−1Ca
h−1 from Lemma 19, thus satisfying the
norm constraints of Wh. Further, Lemma 16 guarantees that both the numerator and denominator are linear
functions of µ∗
h−1, i.e., Pπ
h−1
 dD
h−1 ewh−1

= ⟨µ∗
h−1, θup
h ⟩and dD,†
h−1 = ⟨µ∗
h−1, θdown
h
⟩for some θup
h , θdown
h
∈
Rd. Then since bwπ
h minimzes the empirical regression loss Eq. (3), we have
LDreg
h−1
 bwπ
h−1, ewh−1, π

−LDreg
h−1
 
Pπ
h−1
 dD
h−1 ewh−1

dD,†
h−1
, ewh−1, π
!
≤0.
(18)
Combining Eq. (16), Eq. (17), Eq. (18) with the MLE bound of Eq. (10), with probability at least
1 −δ/(2H) we have
∥bdπ
h −d
π
h∥1 ≤∥bdπ
h−1 −d
π
h−1∥1 + 2Cx
h−1εmle + Cx
h−1Ca
h−1εmle +
√
2εreg,h−1
30

≤∥bdπ
h−1 −d
π
h−1∥1 + 3Cx
h−1Ca
h−1εmle +
√
2εreg,h−1.
Finally, union bounding over h ∈[H], plugging in the deﬁnition of εmle, and rearranging gives that
Eq. (11) holds with probability at least 1 −δ/2.
E.3
Proof of ofﬂine policy optimization
Theorem (Restatement of Theorem 3). Fix δ ∈(0, 1) and suppose Assumption 1 and Assumption 2 hold.
Given a policy class Π, let {bdπ
h}h∈[H],π∈Π be the output of running Algorithm 1. Then with probability at
least 1 −δ, for any deterministic reward function R and policy selected as bπR = argmaxπ∈Π bvπ
R, we have
vbπR
R ≥argmax
π∈Π
vπ
R −ε,
where bvπ
R := PH−1
h=0
RR bdπ
h(xh)R(xh, ah)π(ah|xh)(dxh)(dah) and vR is deﬁned similarly for {d
π
h}. The
total number of episodes required by the algorithm is
˜O

dH3

X
h∈[H]
Cx
hCa
h


2
log(|Π|/δ)/ε2

.
Additionally, deﬁne the set of policies fully covered by the data to be
Πcovered =
n
π ∈Π : dπ
h = d
π
h, ∀h ∈[H]
o
.
Then with the same total number of episodes required by the algorithm, for any reward function R and
policy selected as bπR = argmaxπ∈Πcovered bvπ
R, with probability at least 1 −δ, we have
vbπR
R ≥argmax
π∈Πcovered vπ
R −ε.
Proof. Firstly, Theorem 2 states that, with probability at least 1−δ/|Π|, ˜O

dH3 P
h∈[H] Cx
hCa
h
2
log(|Π|/δ)/ε2

samples are sufﬁcient for learning {bdπ
h} such that ∥bdπ
h −d
π
h∥1 ≤
ε
2H for all h ∈[H] and each π ∈Π. Taking
a union bound over π ∈Π, with probability at least 1 −δ, we have that for all h ∈[H], π ∈Π,
∥bdπ
h −d
π
h∥1 ≤
ε
2H .
Then since the R is bounded on [0, 1], for any π ∈Π we have
|bvπ
R −vπ
R| =
H−1
X
h=0
ZZ
(bdπ
h(xh) −d
π
h(xh))R(xh, ah)π(ah|xh)(dxh)(dah)
≤
H−1
X
h=0
Z
|bdπ
h(xh) −d
π
h(xh)|
Z
π(ah|xh)(dah)

(dxh)
=
H−1
X
h=0
∥bdπ
h −d
π
h∥1 ≤ε/2.
Denote π∗
R = argmaxπ∈Π vπ
R, and recall that we pick bπR = argmaxπ∈Π bvπ
R. Then
vbπR
R −max
π∈Π vπ
R = vbπR
R −v
π∗
R
R ≥vbπR
R −v
π∗
R
R = vbπR
R −bvbπR
R + bvbπR
R −bv
π∗
R
R + bv
π∗
R
R −v
π∗
R
R ≥−ε,
31

where the ﬁrst inequality follows from the fact that dπ
h ≥d
π
h, thus vπ
R ≥vπ
R. The second inequality results
from the fact that bvbπR
R ≥bv
π∗
R
R and |bvπ
R −vπ
R| ≤ε/2 for all π ∈Π.
The result for Πcovered is a straightforward from the observation that for each π ∈Πcovered, we have
dπ
h = d
π
h for all h ∈[H] and vπ
R = vπ
R.
F
Online policy cover construction proofs (Section 4)
F.1
Proof of occupancy estimation
Lemma (Restatement of Lemma 4). For any h ∈[H] and π ∈Π in Algorithm 2,
d
π
h −dπ
h

1 ≤
d
π
h−1 −dπ
h−1

1 + 4d max
π′∈Π
bdπ′
h−1 −d
π′
h−1

1 .
Proof. Firstly, from the third claim of Proposition 2, we have that for any h ∈[H], π ∈Π
∥d
π
h −dπ
h∥1 ≤∥d
π
h−1 −dπ
h−1∥1 + ∥d
π
h−1 −d
π
h−1 ∧Cx
h−1dD
h−1∥1 + ∥Pπ
h−1dπ
h−1 −Pπ
h−1dπ
h−1∥1.
(19)
Now we further simplify the latter two error terms on the RHS of Eq. (19) by noticing that Cx
h = d and
Ca
h = K for all h ∈[H]. For the last term, πD = unif(A) gives us
π(ah−1|xh−1) = min{π(ah−1|xh−1), Ca
h−1πD(ah−1|xh−1)} = min{π(ah−1|xh−1), 1)} = π(ah−1|xh−1)
and thus
Pπ
h−1dπ
h−1 −Pπ
h−1dπ
h−1

1 = 0. For the middle term, we expand the expression as
d
π
h−1 −d
π
h−1 ∧ddD
h−1

1 =
Z
d
π
h−1(xh−1) −

d
π
h−1 ∧ddD
h−1

(xh−1)(dxh−1).
Consider a ﬁxed xh−1 ∈X. Note that d
π
h−1(xh−1) −

d
π
h−1 ∧ddD
h−1

(xh−1) is nonzero only if
ddD
h−1(xh−1) < d
π
h−1(xh−1), for which we have
d
π
h−1(xh−1) −

d
π
h−1 ∧ddD
h−1

(xh−1) = d
π
h−1(xh−1) −ddD
h−1(xh−1)
≤bdπ
h−1(xh−1) −ddD
h−1(xh−1) +
d
π
h−1(xh−1) −bdπ
h−1(xh−1)
 .
To bound bdπ
h−1(xh−1) −ddD
h−1(xh−1), we have
bdπ
h−1(xh−1) −ddD
h−1(xh−1)
≤edπ
h−1(xh−1) −ddD
h−1(xh−1) +
bdπ
h−1(xh−1) −edπ
h−1(xh−1)

≤
d
X
i=1
edπh−1,i
h−1
(xh−1)
 −ddD
h−1(xh−1) +
bdπ
h−1(xh−1) −edπ
h−1(xh−1)

≤
d
X
i=1
bdπh−1,i
h−1
(xh−1)
 −ddD
h−1(xh−1) + (d + 1) max
π′∈Π
bdπ′
h−1(xh−1) −edπ′
h−1(xh−1)

≤
d
X
i=1
d
πh−1,i
h−1
(xh−1)
 −ddD
h−1(xh−1) + (d + 1) max
π′∈Π
bdπ′
h−1(xh−1) −edπ′
h−1(xh−1)

+ d max
π′∈Π
bdπ′
h−1(xh−1) −d
π′
h−1(xh−1)

32

=
d
X
i=1
d
πh−1,i
h−1
(xh−1) −ddD
h−1(xh−1) + (d + 1) max
π′∈Π
bdπ′
h−1(xh−1) −edπ′
h−1(xh−1)

+ d max
π′∈Π
bdπ′
h−1(xh−1) −d
π′
h−1(xh−1)

≤
d
X
i=1
dπh−1,i
h−1
(xh−1) −ddD
h−1(xh−1) + (d + 1) max
π′∈Π
bdπ′
h−1(xh−1) −edπ′
h−1(xh−1)

+ d max
π′∈Π
bdπ′
h−1(xh−1) −d
π′
h−1(xh−1)

= (d + 1) max
π′∈Π
bdπ′
h−1(xh−1) −edπ′
h−1(xh−1)
 + d max
π′∈Π
bdπ′
h−1(xh−1) −d
π′
h−1(xh−1)
 .
In the second inequality, we use that Πexpl
h−1 = {πh−1,1, . . . , πh−1,d} are the policies corresponding to the
barycentric spanner, which Lemma 15 guarantees to be of cardinality no larger than d. The ﬁrst equality is
because d
π
h−1(xh−1) ≥0, ∀π, which can be seen by the induction deﬁnition in Eq. (4) and the non-negativity
of d0. The ﬁfth inequality is due to d
π
h−1(xh−1) ≤dπ
h−1(xh−1), ∀π, which can be shown inductively by
noticing d
π
0 ≤dπ
0 and the deﬁnition of d
π
h in Eq. (4). The last equality can be seen from that dD
h−1(xh−1) is
the marginal distribution of Dh−1 and Dh−1 is rolled in with unif(Πexpl
h−1).
Integrating over xh−1 yields
d
π
h−1 −d
π
h−1 ∧ddD
h−1

1 ≤(d + 1) max
π′∈Π
bdπ′
h−1 −edπ′
h−1

1 + (d + 1) max
π′∈Π
bdπ′
h−1 −d
π′
h−1

1 .
Since d
π′
h−1 = Pπ′
h−2(d
π
h−2∧Cx
h−2dD
h−2) = Pπ′
h−2(d
π
h−2∧ddD
h−2) is linear in the features µ∗
h−2 (Lemma 16),
and edπ′
h−1 is the closest linear approximation in the ℓ1 norm to bdπ′
h−1 (line 7), for any π′ ∈Π we have
bdπ′
h−1 −edπ′
h−1

1 ≤
bdπ′
h−1 −d
π′
h−1

1
(20)
and thus
d
π
h−1 −d
π
h−1 ∧ddD
h−1

1 ≤2(d + 1) max
π′∈Π
bdπ′
h−1 −d
π′
h−1

1 .
(21)
Then combining Eq. (19) with Eq. (21) gives
d
π
h −dπ
h

1 ≤
d
π
h−1 −dπ
h−1

1 + 4d max
π′∈Π
bdπ′
h−1 −d
π′
h−1

1 .
Theorem (Restatement of Theorem 5). Fix δ ∈(0, 1) and consider an MDP M that satisﬁes Assumption 1,
where the right feature µ∗is known. Then by setting
nmle = eO
d3K2H4 log(1/δ)
ε2

, nreg = eO
d5K2H4 log(|Π|/δ)
ε2

, n = nmle + nreg,
with probability at least 1 −δ, FORCE returns state occupancy estimates {bdπ
h}H−1
h=0 satisfying that
∥bdπ
h −dπ
h∥1 ≤ε, ∀h ∈[H], π ∈Π.
The total number of episodes required by the algorithm is
eO(nH) = eO
d5K2H5 log(|Π|/δ)
ε2

.
33

Proof. From Algorithm 2, we know that dataset D0:H−1 satisﬁes Assumption 2 and for each π ∈Π, bdπ
h is
estimated in the same way as that in Algorithm 1. Therefore, we can follow the same steps as the proof of
Theorem 2. By setting Cx
h = d and Ca
h = K for all h ∈[H] in Eq. (13), with probability at least 1 −δ, for
any policy π ∈Π, we get that
bdπ
h −d
π
h

1 ≤18hd3/2K
s
log(16HBµnmle/δ)
nmle
+ 666hd3/2K
s
log (2|Π|Hnreg/δ)
nreg
.
(22)
The primary difference between the above results and the corresponding statements in Theorem 2 is
that the regression error in Eq. (22) includes an additional union bound over all π ∈Π. This is because
Algorithm 2 performs estimation for all policies, while Algorithm 1 only concerns a single ﬁxed policy. We
note that this change in the proof occurs only through application of Lemma 14, which is stated generally
and already includes a union bound over all policies of interest. Because MLE estimation occurs only for the
data distribution and is policy-agnostic, the MLE error (second term) does not require such a union bound.
Next, to bound the missingness error, from Lemma 4, we have
d
π
h −dπ
h

1 ≤
d
π
h−1 −dπ
h−1

1 + 4d max
π′∈Π
bdπ′
h−1 −d
π′
h−1

1 .
(23)
Unfolding Eq. (23) yields
d
π
h −dπ
h

1 ≤4d
h−1
X
h′=0
max
π′∈Π
bdπ′
h′ −d
π′
h′

1 .
(24)
Plugging the bound for
bdπ′
h′ −d
π′
h′

1 from Eq. (22) into Eq. (24) gives
d
π
h −dπ
h

1 ≤72h2d3/2K
s
log(16HBµnmle/δ)
nmle
+ 2664h2d5/2K
s
log (2|Π|Hnreg/δ)
nreg
.
(25)
Combining Eq. (22) and Eq. (25) via triangle inequality and simplifying, we have
bdπ
h −dπ
h

1 ≤90h2d3/2K
s
log(16HBµnmle/δ)
nmle
+ 3330h2d5/2K
s
log (2|Π|Hnreg/δ)
nreg
.
Finally, noticing that nmle = eO

d3K2H4 log(1/δ)
ε2

, nreg = eO

d5K2H4 log(|Π|/δ)
ε2

, n = nmle + nreg
completes the proof.
F.2
Proof of online policy optimization
First, we prove Proposition 1, from which our online policy optimization guarantee (Theorem 6) follows
when combined with Theorem 5.
Proposition 6 (Restatement of Proposition 1). Given any policy π and reward function9 R = {Rh} with
Rh : X×A →[0, 1], deﬁne expected return as vπ
R := Eπ[PH−1
h=0 Rh(xh, ah)] = PH−1
h=0
RR
dπ
h(xh)Rh(xh, ah)
π(ah|xh)(dxh)(dah). Then for {bdπ
h} such that ∥bdπ
h −dπ
h∥1 ≤ε/(2H) for all π ∈Π and h ∈[H], and policy
chosen as
bπR = argmax
π∈Π
bvπ
R,
9We assume known & deterministic rewards, and can easily handle unknown/stochastic versions (Appendix D.2).
34

we have
vbπR
R ≥max
π∈Π vπ
R −ε,
where bvπ
R = PH−1
h=0
RR bdπ
h(xh)Rh(xh, ah)π(ah|xh)(dxh)(dah) is the expected return calculated using
{bdπ
h}.
Proof. Since the R is bounded on [0, 1], for any π ∈Π we have
|bvπ
R −vπ
R| =
H−1
X
h=0
ZZ
(bdπ
h(xh) −dπ
h(xh))R(xh, ah)π(ah|xh)(dxh)(dah)
≤
H−1
X
h=0
Z
|bdπ
h(xh) −dπ
h(xh)|
Z
π(ah|xh)(dah)

(dxh)
=
H−1
X
h=0
∥dπ
h −bdπ
h∥1 ≤ε/2.
Next, recall we pick bπR = argmaxπ∈Π bvπ
R, and denote π∗
R = argmaxπ∈Π bvπ
R. Then using the above
inequality, we have
vbπR
R −max
π∈Π vπ
R = vbπR
R −v
π∗
R
R = vbπR
R −bvbπR
R + bvbπR
R −bv
π∗
R
R + bv
π∗
R
R −v
π∗
R
R ≥−ε
since bvbπR
R ≥bv
π∗
R
R , completing the proof.
Theorem 9 (Restatement of Theorem 6). Fix δ ∈(0, 1) and suppose Assumption 1 and Assumption 2 hold,
and µ∗is known. Given a policy class Π, let {bdπ
h}h∈[H],π∈Π be the output of running FORCE. Then with
probability at least 1 −δ, for any reward function R and policy selected as bπR = argmaxπ∈Π bvπ
R, we have
vbπR
R ≥argmax
π∈Π
vπ
R −ε,
where vπ
R and bvπ
R are deﬁned in Proposition 1. The total number of episodes required by the algorithm is
˜O
d5K2H7 log(|Π|/δ)
ε2

.
Proof. The proof takes similar steps as the proof of Theorem 3. From Theorem 5, w.p. ≥1 −δ, we
obtain estimates {bdπ
h} such that ∥dπ
h −bdπ
h∥1 ≤
ε
2H for all π ∈Π with ˜O

d5K2H7 log(|Π|/δ)
ε2

total number of
samples, where we use the union bound over π ∈Π. Combining this with Proposition 1 gives the result.
G
Representation learning
In this section, we present the detailed algorithms and results for the representation learning setting (Sec-
tion 5), where the true density features are not given but must also be learned from an exponentially large
candidate feature set. The algorithms and analyses mostly follow that of the known density feature case
(Section 3 and Section 4), therefore, we mainly discuss the difference here.
35

G.1
Off-policy occupancy estimation
We start with describing our algorithm FORCRL (Algorithm 3), which estimates the occupancy distribution
dπ
h of any given policy π using an ofﬂine dataset D0:H−1 when the true density feature µ∗is unknown and
the learner is given a realizable density feature class Υ ∋µ∗(see Assumption 3).
As discussed in Section 5, instead of using µ∗to construct the function classes, a natural choice here is
to use the union of all linear function classes. Since now the feature comes from candidate feature classes
Υh−2, Υh−1, in line 4 of Algorithm 3, we use different function classes Fh−1(Υh−2), Fh(Υh−1) as deﬁned
in Eq. (26) for the MLE objective. In addition, in line 5 of Algorithm 3, now we run regression with a
different function class Wh(Υh−1) as deﬁned in Eq. (27).
Algorithm 3 Fitted Occupancy Iteration with Clipping and Representation Learning (FORCRL)
Input: policy π, density feature class Υ, dataset D0:H−1, sample sizes nmle and nreg, clipping thresholds
{Cx
h} and{Ca
h}.
1: Initialize bdπ
0 = d0, ∀π ∈Π.
2: for h = 1, . . . , H do
3:
Randomly split Dh−1 to two folds Dmle
h−1 and Dreg
h−1 with sizes nmle and nreg respectively.
4:
Estimate marginal data distributions bdD
h−1(xh−1) and bd D,†
h−1(xh) by MLE with dataset Dmle
h−1.
bdD
h−1 =
argmax
dh−1∈Fh−1(Υh−2)
1
nmle
nmle
X
i=1
log

dh−1(x(i)
h−1)

and bd D,†
h−1 =
argmax
dh∈Fh(Υh−1)
1
nmle
nmle
X
i=1
log

dh(x(i)
h )

where
Fh(Υh−1) =
n
dh = ⟨µh−1, θh⟩: dh ∈∆(X), µh−1 ∈Υh−1, θh ∈Rd, ∥θh∥∞≤1
o
.
(26)
5:
Deﬁne LDreg
h−1(wh, wh−1, πh−1) :=
1
nreg
Pnreg
i=1

wh(x(i)
h ) −wh−1(x(i)
h−1)
πh−1(a(i)
h−1|x(i)
h−1)
πD
h−1(a(i)
h−1|x(i)
h−1)
2
and es-
timate
bwπ
h =
argmin
wh∈Wh(Υh−1)
LDreg
h−1
 
wh,
bdπ
h−1 ∧Cx
h−1 bdD
h−1
bdD
h−1
, πh−1 ∧Ca
h−1πD
h−1
!
where
Wh(Υh−1) =

wh =
⟨µh−1, θup
h ⟩
⟨µh−1, θdown
h
⟩: ∥wh∥∞≤Cx
h−1Ca
h−1, µh−1 ∈Υh−1, θup
h , θdown
h
∈Rd

.
(27)
6:
Set the estimate bdπ
h = bwπ
h bd D,†
h−1.
7: end for
Output: estimated state occupancies {bdπ
h}h∈[H].
Similar as in the known feature case counterpart (Theorem 2), we have the following guarantee for
estimating dπ.
Theorem (Restatement of Theorem 7). Fix δ ∈(0, 1). Suppose Assumption 1, Assumption 2, and Assump-
36

tion 3 hold. Then, given an evaluation policy π, by setting
nmle = ˜O

d

X
h∈[H]
Cx
hCa
h


2
log(|Υ|/δ)/ε2

and nreg = ˜O

d

X
h∈[H]
Cx
hCa
h


2
log(|Υ|/δ)/ε2

,
with probability at least 1−δ, FORCRL (Algorithm 3) returns state occupancy estimates {bdπ
h}H−1
h=0 satisfying
that
bdπ
h −d
π
h

1 ≤ε, ∀h ∈[H].
The total number of episodes required by the algorithm is
˜O

dH

X
h∈[H]
Cx
hCa
h


2
log(|Υ|/δ)/ε2

.
Proof. The proof for this theorem largely follows its counterpart for the known feature case (Theorem 2),
and we mainly discuss the different steps here. We now make the following two slightly different claims on
MLE estimation and error propagation. Based on them, the ﬁnal error bound is obtained in the same way as
Theorem 2.
Claim 1
Our estimated data distributions satisfy that with probability 1 −δ/2, for any h ∈[H]
bdD
h −dD
h

1 ≤εmle and
bd D,†
h
−dD,†
h

1 ≤εmle,
(28)
where
εmle := 6
s
d log(16H|Υ|Bµnmle/δ)
nmle
.
Claim 2
Under the high-probability event that Eq. (28) holds, we further have with probability at least
1 −δ/2, for any 1 ≤h ≤H, we have
bdπ
h −d
π
h

1 ≤
bdπ
h−1 −d
π
h−1

1 + 3Cx
h−1Ca
h−1εmle +
√
2εreg,h−1,
where
εreg,h−1 :=
s
221184d(Cx
h−1Ca
h−1)2 log (2H|Υ|nreg/δ)
nreg
.
(29)
Proof of Claim 1
Notice that for the term εmle in Eq. (28), we now have an additional |Υ| factor inside
the log. The reason is that here we use Fh−1(Υh−2), Fh(Υh−1) instead of Fh−1, Fh. By Lemma 22, the
two function classes considered here have ℓ1 optimistic covers with scale 1/nmle of size |Υ| (2⌈Bµnmle⌉)d.
In addition, we still have that dD
h−1 ∈Fh−1(Υh−2), dD,†
h−1 ∈Fh(Υh−1) from Lemma 18, and any dh−1 ∈
Fh−1(Υh−2), Fh(Υh−1) is a valid probability distribution over X.
37

Proof of Claim 2
This proof mostly follows the proof of Claim 2 in Theorem 2. The difference is that
the function class Wh(Υh−1) now consists of all features in Υh−1 instead of only the true feature µ∗
h−1.
Therefore, in Eq. (29), the term εreg,h−1 has an additional |Υ| inside the log, which is from the counterpart
of Eq. (17). It is also easy to see that
Pπ
h−1(dD
h−1 ewh−1)
dD,†
h−1
∈Wh(Υh−1) by following the same logic before.
Further noticing that µ∗
h−1 ∈Υh−1, we again have Eq. (18) holds here.
Theorem 10 (Ofﬂine policy optimization with representation learning). Fix δ ∈(0, 1) and suppose As-
sumption 1, Assumption 2, and Assumption 3 hold. Given a policy class Π, let {bdπ
h}h∈[H],π∈Π be the output
of running Algorithm 3. Then with probability at least 1 −δ, for any deterministic reward function R and
policy selected as bπR = argmaxπ∈Π bvπ
R, we have
vbπR
R ≥argmax
π∈Π
vπ
R −ε,
where vπ
R and bvπ
R are deﬁned in Proposition 1, and vR is deﬁned similarly for {d
π
h}. The total number of
episodes required by the algorithm is
˜O

dH3

X
h∈[H]
Cx
hCa
h


2
log(|Π||Υ|/δ)/ε2

.
Additionally, deﬁne the set of policies fully covered by the data to be
Πcovered =
n
π ∈Π : dπ
h = d
π
h, ∀h ∈[H]
o
.
Then with the same total number of episodes required by the algorithm, for any reward function R and
policy selected as bπR = argmaxπ∈Πcovered bvπ
R, with probability at least 1 −δ, we have
vbπR
R ≥argmax
π∈Πcovered vπ
R −ε.
Proof. The proof follows the same steps as that of Theorem 3. Notice that now we will apply Theorem 7
rather than Theorem 2 to get the bound ∥bdπ
h −d
π
h∥1, which leads to the additional log(|Υ|) factor.
G.2
Online policy cover construction
Now we present the algorithm FORCRLE (Algorithm 4), which estimates the occupancy distribution dπ
h of
any given policy π with the access of online interaction. Again the true density feature µ∗is unknown and
the learner is given a realizable density feature class Υ (µ∗∈Υ).
Similar as the know feature case online algorithm (Algorithm 2), we use the ofﬂine algorithm (Algo-
rithm 3) as a submodule. However, as discussed in the main text, the crucial different step is to select a
representation bµh−1 in Eq. (30) in line 8 before setting edπ
h. This guarantee the cardinality of the barycentric
spanner is at most d. Then the state occupancy edπ
h is set as the linear estimate using bµh−1 (rather than using
µ∗
h−1 in the known feature case) in line 9.
Similar as in the known feature case counterpart (Theorem 5), we have the following guarantee for
estimating dπ.
Theorem (Restatement of Theorem 8). Fix δ ∈(0, 1) and suppose Assumption 1 and Assumption 3 hold.
Then by setting
nmle = eO
d3K2H4 log(|Υ|/δ)
ε2

, nreg = eO
d5K2H4 log(|Π||Υ|/δ)
ε2

, n = nmle + nreg,
38

Algorithm 4 FORCRL-guided Exploration (FORCRLE)
Input: policy class Π, density feature class Υ, n = nmle + nreg
1: Initialize bdπ
0 = d0 and edπ
0 = d0, ∀π ∈Π.
2: for h = 1, . . . , H do
3:
Construct {edπh−1,i
h−1
}d
i=1 as the barycentric spanner of {edπ
h−1}π∈Π, and set Πexpl
h−1 = {πh−1,i}d
i=1.
4:
Draw a tuple dataset Dh−1 = {(x(i)
h−1, a(i)
h−1, x(i)
h )}n
i=1 using unif(Πexpl
h−1) ◦unif(A).
5:
for π ∈Π do
6:
Estimate bdπ
h using the h-level loop10of Algorithm 3 (lines 4-6) with Dh, bdπ
h−1, Cx
h = d, Ca
h = K.
7:
end for
8:
Select feature bµh−1 according to
bµh−1 =
min
µh−1∈Υh−1 max
π∈Π min
θh∈Rd ∥⟨µh−1, θh⟩−bdπ
h∥1.
(30)
9:
For all π ∈Π, set the closest linear approximation to bdπ
h with feature bµh−1 as edπ
h = ⟨bµh−1, eθh⟩, where
eθh = argminθh∈Rd ∥⟨bµh−1, θh⟩−bdπ
h∥1.
10: end for
Output: estimated state occupancy measure {bdπ
h}h∈[H],π∈Π.
with probability at least 1 −δ, FORCRLE (Algorithm 4) returns state occupancy estimates {bdπ
h}H−1
h=0 satis-
fying that
∥bdπ
h −dπ
h∥1 ≤ε, ∀h ∈[H], π ∈Π.
The total number of episodes required by the algorithm is
eO(nH) = eO
d5K2H5 log(|Π||Υ|/δ)
ε2

.
Proof. The proof for this theorem largely follows its counterpart for the known feature case (Theorem 5),
and we only discuss the different steps here.
Firstly, Lemma 4 still holds. However, since we use “joint linearization” in line 8 and line 9, we need
to modify the proof of Eq. (20) as the following. Again, we have d
π′
h−1 = Pπ′
h−2(d
π
h−2 ∧Cx
h−2dD
h−2) =
Pπ′
h−2(d
π
h−2 ∧ddD
h−2) is linear in the true feature µ∗
h−2 (Lemma 16). Together with the feature selection
criteria Eq. (30), we have that
max
π′∈Π ∥edπ′
h−1 −bdπ′
h−1∥1 = max
π′∈Π
min
θh−1∈Rd ∥⟨bµh−2, θh−1⟩−bdπ′
h−1∥1
≤max
π′∈Π
min
θh−1∈Rd ∥⟨µ∗
h−2, θh−1⟩−bdπ′
h−1∥1 ≤max
π′∈Π ∥d
π′
h−1 −bdπ′
h−1∥1.
For Eq. (22), we will have an additional |Υ| factor inside the log as
εmle := 6
s
d log(16H|Υ|Bµnmle/δ)
nmle
.
The reason is that here we use Fh−1(Υh−2), Fh(Υh−1) instead of Fh−1, Fh. By Lemma 22, the two
function classes considered here have ℓ1 optimistic covers with scale 1/nmle of size |Υ| (2⌈Bµnmle⌉)d.
In addition, we still have that dD
h−1 ∈Fh−1(Υh−2), dD,†
h−1 ∈Fh(Υh−1) Lemma 18, and any dh−1 ∈
Fh−1(Υh−2), Fh(Υh−1) is a valid probability distribution over X.
The remaining part of the proof is the same as that of Theorem 5.
39

Theorem 11 (Online policy optimization with representation learning). Fix δ ∈(0, 1) and suppose As-
sumption 1 and Assumption 3 hold. Given a policy class Π, let {bdπ
h}h∈[H],π∈Π be the output of running
Algorithm 4. Then with probability at least 1 −δ, for any deterministic reward function R (as per Proposi-
tion 1) and policy selected as bπR = argmaxπ∈Π bvπ
R, we have
vbπR
R ≥argmax
π∈Π
vπ
R −ε,
where bvπ
R := PH−1
h=0
RR bdπ
h(xh)R(xh, ah)π(ah|xh)(dxh)(dah). The total number of episodes required by
the algorithm is
˜O
d5K2H7 log(|Π||Υ|/δ)
ε2

.
Proof. The proof follows the same steps as that of Theorem 6. Notice that now we will apply Theorem 8
rather than Theorem 5 to get the bound ∥dπ
h −bdπ
h∥, which leads to the additional log(|Π|) factor.
H
Maximum likelihood estimation
In this section, we adapt the standard i.i.d. results of maximum likelihood estimation (Van de Geer, 2000) to
our setting, and in particular, to our (inﬁnite) linear function class. We consider the problem of estimating a
probability distribution over the instance space X, and note that we abuse some notations (e.g., n, L, D, F)
in this section, as they have different meanings in other parts of the paper. Given an i.i.d. sampled dataset
D = {x(i)}n
i=1 and a function class F, we optimize the MLE objective
bf = argmin
f∈F
1
n
n
X
i=1
log

f(x(i))

.
(31)
We consider the function class F to be inﬁnite, and as is common in statistical learning, our result will
depends on its structural complexity. In particular, this will be quantiﬁed using the ℓ1 optimistic cover,
deﬁned below:
Deﬁnition 3 (ℓ1 optimistic cover). For a function class F ⊆(X →R), we call function class F an ℓ∞
optimistic cover of F with scale γ, if for any f ∈F there exists f ∈F, such that ∥f −f∥1 ≤γ and
f(x) ≤f(x), ∀x ∈X. Notice that here we do not require the cover to be proper, i.e., we allow F ̸⊆F.
Now we are ready to state the MLE guarantee formally.
Lemma 12 (MLE guarantee). Let D = {x(i)}n
i=1 be a dataset, where x(i) are drawn i.i.d. from some ﬁxed
probability distribution f∗over X. Consider a function class F that satisﬁes: (i) f∗∈F, (ii) each function
f ∈F is a valid probability distribution over X (i.e., f ∈∆(X)), and (iii) F has a ﬁnite ℓ1 optimistic cover
(Deﬁnition 3) F with scale γ and F ⊆(X →R≥0). Then with probability at least 1 −δ, the MLE solution
bf in Eq. (31) has an ℓ1 error guarantee
∥bf −f∗∥1 ≤γ +
s
12 log(|F|/δ)
n
+ 6γ.
Proof. Our proof is based on Zhang (2006); Agarwal et al. (2020); Liu et al. (2022) and is simpler since we
assume the D here is drawn i.i.d. instead of adaptively. We ﬁrst deﬁne L(f, D) = 1
2
Pn
i=1 log

f(x(i))
f∗(x(i))

.
By Chernoff’s method, for a ﬁxed f ∈F we have that
P
 L(f, D) −log(ED[exp(L(f, D))]) ≥log(|F|/δ)

40

≤exp(−log(|F|/δ))ED [exp (L(f, D) −log(ED[exp(L(f, D))]))]
= δ/|F|.
Union bounding over f ∈F, with probability at least 1 −δ, for any f ∈F we have
−log(ED[exp(L(f, D))]) ≤−L(f, D) + log(|F|/δ).
(32)
Let f ∈F be the γ-close ℓ1 optimistic approximator of the MLE solution bf ∈F. Since f(x) ≥
bf(x), ∀x ∈X due to the optimistic covering construction and bf is the MLE estimator, for the RHS of
Eq. (32). we have
−L(f, D) = 1
2
n
X
i=1
log
 
f∗(x(i))
f(x(i))
!
≤1
2
n
X
i=1
log
 
f∗(x(i))
bf(x(i))
!
= 1
2
 n
X
i=1
log(f∗(x(i))) −
n
X
i=1
log( bf(x(i)))
!
≤0.
Next, consider the LHS of Eq. (32). From the deﬁnition of dataset D and L(f, D), we get
−log(ED[exp(L(f, D))]) = −log
 
ED
"
exp
 
1
2
n
X
i=1
log
 
f(x(i))
f∗(x(i))
!!#!
= −n log

ED

exp
1
2 log
 f(x)
f∗(x)

= −n log

ED


s
f(x)
f∗(x)



.
Furthermore, by −log(y) ≥1 −y, ℓ1 optimistic cover deﬁnition, and f∗, bf are valid distributions over
x ∈X, we have
−n log

ED


s
f(x)
f∗(x)



≥n

1 −ED


s
f(x)
f∗(x)



= n

1 −
Z q
f(x)f∗(x)(dx)

= n
2
Z p
f∗(x) −
q
f(x)
2
(dx) + n
2

1 −
Z
f(x)(dx)

= n
2
Z p
f∗(x) −
q
f(x)
2
(dx) + n
2
Z 
bf(x) −f(x)

(dx)
≥n
2
Z p
f∗(x) −
q
f(x)
2
(dx) −nγ
2 .
Then notice that
R p
f∗(x) +
q
f(x)
2
(dx) ≤2
R  f∗(x) + f(x)

(dx) ≤2
R
(f∗(x) + bf(x) +
|f(x) −bf(x)|)(dx) ≤6 and the Cauchy-Schwarz inequality, we obtain
n
2
Z p
f∗(x) −
q
f(x)
2
(dx) −nγ
2
≥n
12
 Z p
f∗(x) −
q
f(x)
2
(dx)
!  Z p
f∗(x) +
q
f(x)
2
(dx)
!
−nγ
2
≥n
12
Z
|f(x) −f∗(x)|(dx)
2
−nγ
2 = n
12∥f −f∗∥2
1 −nγ
2 .
41

Combining the above inequalities and rearranging yields
∥f −f∗∥2
1 ≤12 log(|F|/δ)
n
+ 6γ.
Finally, by the triangle inequality and the deﬁnition of the ℓ1 optimistic cover, we get
∥bf −f∗∥1 ≤∥bf −f∥1 + ∥f −f∗∥1 ≤γ +
s
12 log(|F|/δ)
n
+ 6γ ,
which completes the proof.
I
Auxiliary lemmas
In this section, we provide detailed proofs for auxiliary lemmas.
I.1
Squared loss regression results
Lemma 13 (Squared loss decomposition). For any wh, wh+1 : X →R, dataset Dreg
h
= {(xh, ah, xh+1)} ∼
dD
h , and a pseudo-policy π, we have
wh+1 −Pπ
h
 dD
h wh

dD,†
h

2
2,dD,†
h
= E
h
LDreg
h (wh+1, wh, π)
i
−E
"
LDreg
h
 
Pπ
h
 dD
h wh

dD,†
h
, wh, π
!#
.
(33)
Proof. We introduce a new notation
(Eπ
hwh)(xh+1) :=
 Pπ
h
 dD
h wh

(xh+1)
dD,†
h
(xh+1)
=
RR
Ph(xh+1|xh, ah)π(ah|xh)dD
h (xh)wh(xh)(dxh)(dah)
dD,†
h
(xh+1)
,
(34)
which represents the conditional expectation. Then we have the decomposition
E
h
LDreg
h (wh+1, wh, π)
i
=
ZZZ
dD
h (xh, ah, xh+1)

wh+1(xh+1) −π(ah|xh)
πD(ah|xh)wh(xh)
2
(dxh)(dah)(dxh+1)
=
ZZZ
dD
h (xh, ah, xh+1)

wh+1(xh+1) −(Eπ
hwh)(xh+1) + (Eπ
hwh)(xh+1) −π(ah|xh)
πD(ah|xh)wh(xh)
2
(dxh)(dah)(dxh+1)
=
Z
dD,†
h
(xh+1)(wh+1(xh+1) −(Eπ
hwh)(xh+1))2(dxh+1)
+
ZZZ
dD
h (xh, ah, xh+1)

(Eπ
hwh)(xh+1) −π(ah|xh)
πD(ah|xh)wh(xh)
2
(dxh)(dah)(dxh+1)
+ 2
ZZZ
dD
h (xh, ah, xh+1)(wh+1(xh+1) −(Eπ
hwh)(xh+1))

(Eπ
hwh)(xh+1) −π(ah|xh)
πD(ah|xh)wh(xh)

(dxh)(dah)(dxh+1)
42

= ∥wh+1 −(Eπ
hwh)∥2
2,dD,†
h
+ E
h
LDreg
h (Eπ
hwh, wh, π)
i
+ 2
Z
dD,†
h
(xh+1)(wh+1(xh+1) −(Eπ
hwh)(xh+1))(Eπ
hwh)(xh+1)(dxh+1)
−2
Z
dD,†
h
(xh+1)(wh+1(xh+1) −(Eπ
hwh)(xh+1))
·
ZZ
dD
h (xh, ah|xh+1) π(ah|xh)
πD(ah|xh)wh(xh)(dxh)(dah)

(dxh+1)
= ∥wh+1 −(Eπ
hwh)∥2
2,dD,†
h
+ E
h
LDreg
h (Eπ
hwh, wh, π)
i
+ 2
Z
dD,†
h
(xh+1)(wh+1(xh+1) −(Eπ
hwh)(xh+1))((Eπ
hwh)(xh+1) −(Eπ
hwh)(xh+1))(dxh+1)
= ∥wh+1 −(Eπ
hwh)∥2
2,dD,†
h
+ E
h
LDreg
h (Eπ
hwh, wh, π)
i
.
Lemma 14 (Deviation bound for regression with squared loss). For h ∈[H], consider a dataset D0:h that
satisﬁes Assumption 2 and a function wh : X →[0, Cx
h] that only depends on D0:h−1
S Dmle
h
. Consider a
ﬁnite feature class Υh and a ﬁnite policy class Π′ such that any π ∈Π′ is a pseudo-policy (Deﬁnition 1)
satisfying πh(ah|xh) ≤Ca
hπD
h (ah|xh), ∀xh ∈X, ah ∈A. Then with probability 1 −δ, for any wh+1 ∈
Wh+1(Υh) and π ∈Π′, we have
E
h
LDreg
h (wh+1, wh, π) −LDreg
h (Eπ
hwh, wh, π)
i
−

LDreg
h (wh+1, wh, π) −LDreg
h (Eπ
hwh, wh, π)

≤1
2E
h
LDreg
h (wh+1, wh, π) −LDreg
h (Eπ
hwh, wh, π)
i
+ 221184d(Cx
hCa
h)2 log (nreg|Π′||Υh|/δ)
nreg
where the function class Wh+1(Υh) is deﬁned in Algorithm 1 as in Eq. (26) and the operator Eπ
h is deﬁned
in Eq. (34).
Proof. We ﬁrst ﬁx the datasets D0:h−1
S Dmle
h
and prove the desired bound when conditioned on these
datasets, in which case wh, dD,†
h
, πD are ﬁxed. In the following, the expectation E and variance V are
w.r.t. (xh, ah, xh+1) ∼dD
h , i.e., the data distribution from which the samples in Dreg
h
are drawn i.i.d. from
(Assumption 2), when conditioned on D0:h−1
S Dmle
h
.
Consider a single π ∈Π′ and feature µh ∈Υh, and consider the hypothesis class
Y(Wh+1(µh), wh, π) = {Y (wh+1, wh, π) : wh+1 ∈Wh+1(µh)} .
where the random variable Y (wh+1, wh, π) (suppressing the dependence on the (xh, ah, xh+1) tuple) is
deﬁned for convenience as
Y (wh+1, wh, π) :=

wh+1(xh+1) −wh(xh) π(ah|xh)
πD(ah|xh)
2
−

(Eπ
hwh)(xh+1) −wh(xh) π(ah|xh)
πD(ah|xh)
2
,
and we use Yi(wh+1, wh, π) to denote its realization on the i-th tuple data (x(i)
h , a(i)
h , x(i)
h+1) ∈Dreg
h . The
function class Wh+1(µh) is deﬁned as in Eq. (27), i.e.,
Wh+1(µh) =
(
wh+1 = ⟨µh, θup
h+1⟩
⟨µh, θdown
h+1 ⟩: ∥wh+1∥∞≤Cx
hCa
h, θup
h+1, θdown
h+1 ∈Rd
)
.
It can be seen that |Y (wh+1, wh, π)| ≤4(Cx
hCa
h)2 from the following. From their respective deﬁni-
tions, we know ∥wh∥∞≤Cx
h, ∥π
πD ∥∞≤Ca
h, and ∥wh+1∥∞≤Cx
hCa
h. We also have (Eπ
hwh)(xh+1) =
(Pπ
h(dD
h wh))(xh+1)
dD,†
h
(xh+1)
∈[0, Cx
hCa
h] from Lemma 19.
43

Further, for any Y (wh+1, wh, π) ∈Y(Wh+1(µh), wh, π), we can bound the variance V[Y (wh+1, wh, π)]
as
V[Y (wh+1, wh, π)] ≤E

Y (wh+1, wh, π)2
= E


 
wh+1(xh+1) −wh(xh) π(ah|xh)
πD(ah|xh)
2
−

(Eπ
hwh)(xh+1) −wh(xh) π(ah|xh)
πD(ah|xh)
2!2

= E
"
(wh+1(xh+1) −(Eπ
hwh)(xh+1))2

wh+1(xh+1) −2wh(xh) π(ah|xh)
πD(ah|xh) + (Eπ
hwh)(xh+1)
2#
≤4(Cx
hCa
h)2E

(wh+1(xh+1) −(Eπ
hwh)(xh+1))2
= 4(Cx
hCa
h)2E [Y (wh+1, wh, π)] .
(Lemma 13)
Next, we show that the uniform covering number N1(γ, Y(Wh+1(µh), wh, π), m) (see Deﬁnition 7) for
any γ ∈R, m ∈N can be bounded by the covering number of Wh+1(µh). Let Zm = (x(i)
h , a(i)
h , x(i)
h+1)m
i=1
denote m i.i.d. samples from dD
h , and denote Xm = (x(i)
h+1)m
i=1 the corresponding xh+1 samples. For any
Zm and Y (wh+1, wh, π), Y (w′
h+1, wh, π) ∈Y(Wh+1(µh), wh, π),
1
m
m
X
i=1
Yi(wh+1, wh, π) −Yi(w′
h+1, wh, π)

= 1
m
m
X
i=1

 
wh+1(x(i)
h+1) −wh(x(i)
h ) π(a(i)
h |x(i)
h )
πD(a(i)
h |x(i)
h )
!2
−
 
w′
h+1(x(i)
h+1) −wh(x(i)
h ) π(a(i)
h |x(i)
h )
πD(a(i)
h |x(i)
h )
!2
= 1
m
m
X
i=1
wh+1(x(i)
h+1) −2wh(x(i)
h ) π(a(i)
h |x(i)
h )
πD(a(i)
h |x(i)
h )
+ w′
h+1(x(i)
h+1)
 ·
wh+1(x(i)
h+1) −w′
h+1(x(i)
h+1)

≤4Cx
hCa
h
m
m
X
i=1
wh+1(x(i)
h+1) −w′
h+1(x(i)
h+1)
 .
Thus any γ/(4Cx
hCa
h)-covering of Wh+1|Xm in ℓ1 is a γ-covering of Y (Wh+1, wh, π)|Zm in ℓ1, and
N1(γ, Y (Wh+1(µh), wh, π), Zm) ≤N1(γ/(4Cx
hCa
h), Wh+1(µh), Xm)
which implies the same relationship for the uniform covering numbers:
N1(γ, Y (Wh+1(µh), wh, π), m) = max
Zm N1(γ, Y (Wh+1(µh), wh, π), Zm)
≤max
Xm N1(γ/(4Cx
hCa
h), Wh+1(µh), Xm) = N1(γ/(4Cx
hCa
h), Wh+1(µh), m).
Then using this inequality and b = 4(Cx
hCa
h)2 in Lemma 26 and conditioning on D0:h−1
S Dmle
h
, for
any wh+1 ∈Wh+1(µh), we have
P
 E[Y (wh+1, wh, π)] −
1
nreg
n
X
i=1
Yi(wh+1, wh, π)
 ≥ε
!
≤36N1

ε3
10240(Cx
hCa
h)4 , Y(Wh+1(µh), wh, π), 640nreg(Cx
hCa
h)4
ε2

· exp

−
nregε2
128V[Y (wh+1, wh, π)] + 2048ε(Cx
hCa
h)2

44

≤36N1

ε3
40960(Cx
hCa
h)5 , Wh+1(µh), 640nreg(Cx
hCa
h)4
ε2

· exp

−
nregε2
512(Cx
hCa
h)2E[Y (wh+1, wh, π)] + 2048ε(Cx
hCa
h)2

.
Then setting the RHS equal to δ′, we have
nreg =
512(Cx
hCa
h)2 (E[Y (wh+1, wh, π)] + 4ε) log

36N1

ε3
40960(Cx
hCa
h)5 , Wh+1(µh), 640nreg(Cx
hCa
h)4
ε2

/δ′
ε2
implying
ε ≤
v
u
u
t512(Cx
hCa
h)2E[Y (wh+1, wh, π)] log

36N1

ε3
40960(Cx
hCa
h)5 , Wh+1(µh), 640nreg(Cx
hCa
h)4
ε2

/δ′

nreg
+
2048(Cx
hCa
h)2 log

36N1

ε3
40960(Cx
hCa
h)5 , Wh+1(µh), 640nreg(Cx
hCa
h)4
ε2

/δ′
nreg
.
From Lemma 23 and Lemma 25, and noting that nreg ≥2048(Cx
hCa
h)2
ε
, we have that
log

36N1

ε3
40960(Cx
hCa
h)5 , Wh+1(µh), 640nreg(Cx
hCa
h)4
ε2

/δ′

≤4(d + 1) log(8e) log
655360e2(Cx
hCa
h)6
ε3δ′

≤96d log
nreg
δ′

.
Thus with probability at least 1 −δ′,
E[Y (wh+1, wh, π)] −
1
nreg
nreg
X
i=1
Yi(wh+1, wh, π)

≤
s
49152d(Cx
hCa
h)2E[Y (wh+1, wh, π)] log
  nreg
δ′

nreg
+ 196608d(Cx
hCa
h)2 log
  nreg
δ′

nreg
.
Then invoking the AM-GM inequality,
E[Y (wh+1, wh, π)] −
1
nreg
nreg
X
i=1
Yi(wh+1, wh, π)

≤1
2E[Y (wh+1, wh, π)] + 221184 · d(Cx
hCa
h)2 log
  nreg
δ′

nreg
.
Recall that this result holds for a ﬁxed π and Wh+1(µh) deﬁned using a ﬁxed µh. Then setting δ′ =
δ
|Π′||Υh|
and taking a union bound over Π and Υh, we have that with probability at least 1 −δ that for any π ∈Π′
and wh+1 ∈Wh+1(Υh) that
E[Y (wh+1, wh, π)] −
1
nreg
nreg
X
i=1
Yi(wh+1, wh, π)

45

≤1
2E[Y (wh+1, wh, π)] +
221184d(Cx
hCa
h)2 log

nreg|Π′||Υh|
δ

nreg
.
Finally, since this result holds for any ﬁxed D0:h−1
S Dmle
h
, by the law of total expectation, it also holds
with probability at least 1−δ′ without conditioning on D0:h−1
S Dmle
h
. Using Lemma 13 with the deﬁnitions
of Y (wh+1, wh, π) and Yi(wh+1, wh, π) completes the proof.
I.2
Barycentric spanner
In this section we ﬁrst deﬁne the barycentric spanner (Awerbuch and Kleinberg, 2008, Deﬁnition 2.1), then
prove that a spanner of size d always exists for a set of functions linear in a feature µh−1, from which Propo-
sition 3 follows straightforwardly. The proof is adapted from Awerbuch and Kleinberg (2008, Proposition
2.2), which only applies to square matrices, and we extend it to rectangular matrices for completeness. We
close with a discussion of the computational complexity of ﬁnding the barycentric spanner.
Deﬁnition 4 (Barycentric spanner). Let V be a vector space over the real numbers, and S ⊆V a subset
whose linear span is a m-dimensional subspace of V . A set X = {x1, . . . , xm} ⊆S is a barycentric
spanner of S if every x ∈S may be expressed as a linear combination of elements of X using coefﬁcients
in [−1, +1].
Lemma 15 (Barycentric spanner for linear functions). For a feature µh−1 ∈Υh−1 with rank d, any set of
linear functions U ⊆{⟨µh−1, θh⟩: θh ∈Rd} has a barycentric spanner of cardinality min(|U|, d).
Proof. We prove the proposition when rank(µh−1) = d is full rank (the argument should be the same when
rank(µh−1) < d), and |U| > d (otherwise we can satisfy the lemma statement by picking all of U to be the
spanner). First, U is a compact subset of R|X| because it is closed and bounded. Because U is linear in µh−1,
its linear span is a d-dimensional subspace of R|X|, and any u ∈U can be written as the linear combination
of a subspace basis.
We claim the barycentric spanner is any subset B = {b1, . . . , bd} ⊆U with B ∈Rd×|X| that maximizes
the volume | det(BB⊤)|. By compactness, the maximum is obtained by at least one subset of U. Since
det(BB⊤) = (Qd
i=1 σi(B))2, the maximizing B will have d singular values and full row rank (otherwise
the determinant will be 0). As a result, any u ∈U will be a linear combination of the rows of B, i.e., there
exists {ci}d
i=1 such that u = Pd
i=1 cibi. We will prove that |ci| ≤1 by contradiction.
W.l.o.g, suppose there exists u with coefﬁcient |c1| > 1. Then consider a new matrix eB = {u, b2, . . . , bd},
which can be expressed as eB = CB, where C ∈Rd×d is the coefﬁcient matrix. Then eB has determinant
| det( eB eB⊤)| = | det(C)|2| det(BB⊤)| = |c1|2| det(BB⊤)| ≥det(BB⊤).
Then we have a contradiction because B was volume-maximizing, and |ci| ≤1.
Computation of barycentric spanner
Lastly, we discuss computation of the barycentric spanner. In the
main results of the paper we assume that we can perfectly compute the barycentric spanner in an efﬁcient
manner. When this is not the case, the algorithm in Figure 2 in Awerbuch and Kleinberg (2008) (with
similar adaptations to handle rectangular matrices as in the proof of Lemma 15) can be used to compute a
C-approximate barycentric spanner, where C > 1, with O(d2 logC d) calls to a linear optimization oracle
(Awerbuch and Kleinberg, 2008, Proposition 2.5). A C-approximate barycentric spanner is deﬁned similarly
as Deﬁnition 4, except that the coefﬁcients are in the range [−C, +C]. This will only change our main results
by increasing them by a factor of C, and we may simply set C = 2 with minimal effects on our sample
complexity guarantees.
46

I.3
Properties of low-rank MDPs
Lemma 16. In the low-rank MDP (Assumption 1), for any h ∈[H], function dh−1 : X →R, and pseudo-
policy π (Deﬁnition 1), we have
(Pπ
hdh)(xh+1) =
ZZ
Ph(xh+1|xh, ah)πh(ah|xh)dh(xh)(dxh)(dah) = ⟨µ∗
h(xh+1), θh+1⟩
for some θh+1 ∈Rd with ∥θh+1∥∞≤∥dh∥1.
Proof. By the deﬁnition of low-rank MDPs (Assumption 1), we have
Pπ
hdh =
ZZ
Ph(xh+1|xh, ah)πh(ah|xh)dh(xh)(dxh)(dah)
=
ZZ
⟨µ∗
h(xh+1), φ∗
h(xh, ah)⟩πh(ah|xh)dh(xh)(dxh)(dah)
= ⟨µ∗
h(xh+1), θh+1⟩,
where θh+1 =
RR
φ∗
h(xh, ah)πh(ah|xh)dh(xh)(dxh)(dah) ∈Rd. In addition,
∥θh+1∥∞≤
ZZ
∥φ∗
h(xh, ah)∥∞πh(ah|xh)|dh(xh)|(dxh)(dah)
≤
Z Z
πh(ah|xh)(dah)

|dh(xh)|(dxh)
≤
Z
|dh(xh)|(dxh) = ∥dh∥1
where we use Lemma 21 in the last inequality.
Lemma 17. In low-rank MDPs (Assumption 1), given a dataset Dh satisfying Assumption 2 for h ∈[H],
let dD
h and dD,†
h
be the corresponding current-state and next-state data distributions. Then for the function
class
Fh =
n
dh = ⟨µ∗
h−1, θh⟩: dh ∈∆(X), θh ∈Rd, ∥θh∥∞≤1
o
,
we have that dD
h ∈Fh and dD,†
h
∈Fh+1.
Proof. Recall that under Assumption 2, Dh is collected by ρh−1 ◦πD
h where a0:h−1 ∼ρh−1, an (h−1)-step
non-Markov policy, and ah ∼πD
h , a Markov policy.
First we prove the lemma statement for dD,†
h
. Since dD
h is a valid distribution and πD
h is a valid Markov
policy, from Lemma 16 we know that dD,†
h
= P
πD
h
h (dD
h ) can be written as ⟨µ∗
h, θh+1⟩with ∥θh+1∥∞≤1.
Finally, since dD,†
h
is a valid marginal distribution, dD,†
h
∈∆(X), thus satisfying all constraints of Fh+1.
To prove the lemma statement for dD
h , we ﬁrst prove a variant of Lemma 16 for non-Markov policies.
With some overload of notation, let dD
h−1(xh−1) denote the marginal distribution of xh−1 induced by rolling
the non-Markov policy ρh−1 to level h −1. Then
dD
h (xh) =
ZZ
Ph(xh|xh−1, ah−1)ρh−1(ah−1|x0:h−1)dD
h−1(xh−1)(dxh−1)(dah−1).
Using similar steps as the proof of Lemma 16, we have that
dD
h (xh) =
ZZ
Ph(xh|xh−1, ah−1)ρh−1(ah−1|x0:h−1)dD
h−1(xh−1)(dxh−1)(dah−1)
47

=
ZZ
⟨φ∗
h−1(xh−1, ah−1), µ∗
h−1(xh)⟩ρh−1(ah−1|x0:h−1)dD
h−1(xh−1)(dxh−1)(dah−1)
= ⟨µ∗
h−1(xh), θh⟩,
where θh =
RR
φ∗
h−1(xh−1, ah−1)ρh−1(ah−1|x0:h−1)dD
h−1(xh−1)(dxh−1)(dah−1) ∈Rd. Since dD
h−1 and
ρh−1(·|x0:h−1) are valid probability distributions over states xh and actions ah, respectively, it is easy to see
that
∥θh∥∞≤
ZZ
∥φ∗
h−1(xh−1, ah−1)∥∞ρh−1(ah−1|x0:h−1)dD
h−1(xh−1)(dxh−1)(dah−1) ≤1
since ∥φ∗
h−1(·)∥∞≤1 from Assumption 1. Finally, since dD
h is a valid distribution, we have dD
h ∈Fh.
Lemma 18. In low-rank MDPs (Assumption 1), given a dataset Dh satisfying Assumption 2 for h ∈[H],
let dD
h and dD,†
h
be the corresponding current-state and next-state data distributions. Then for the function
class
Fh(Υh−1) =
n
dh = ⟨µh−1, θh⟩: dh ∈∆(X), µh−1 ∈Υh−1, θh ∈Rd, ∥θh∥∞≤1
o
,
we have that dD
h ∈Fh(Υh−1) and dD,†
h
∈Fh+1(Υh).
Proof. From Lemma 17 we know that dD
h ∈Fh (where Fh is linear in the true features µ∗
h−1, as deﬁned
in the Lemma 17), and dD,†
h
∈Fh+1. Noting that Fh ⊆Fh(Υh−1) and Fh+1 ⊆Fh+1(Υh) completes the
proof.
Lemma 19. For h ∈[H], suppose we have a dataset Dh satisfying Assumption 2, with corresponding data
distributions dD
h and dD,†
h
. Given a function wh : X →[−Cx
h, Cx
h] and pseudo-policy π (Deﬁnition 1) with
πh(a|x)
πD
h (a|x) ≤Ca
h, ∀x ∈X, a ∈A, we have

Pπ
h(dD
h wh)
dD,†
h

∞
≤Cx
hCa
h.
Proof. For any xh+1 ∈X, we have
 Pπ
h
 dD
h wh

(xh+1) ≤Cx
h
 Pπ
hdD
h

(xh+1)
= Cx
h
ZZ
Ph(xh+1|xh, ah)πh(ah|xh)dD
h (xh)(dxh)(dah)
≤Cx
hCa
h
ZZ
Ph(xh+1|xh, ah)πD
h (ah|xh)dD
h (xh)(dxh)(dah)
= Cx
hCa
hdD,†
h
(xh+1).
The last equality follows from the Bellman ﬂow equation and Assumption 2. The convention that 0
0 = 0
gives the lemma statement.
Lemma 20. For any two state distributions dh, d′
h and a pseudo-policy π (Deﬁnition 1), we have the fol-
lowing inequality
∥Pπ
hdh −Pπ
hd′
h∥1 ≤∥dh −d′
h∥1,
where we recall that (Pπ
hdh)(xh+1) =
RR
Ph(xh+1|xh, ah)π(ah|xh)dh(xh)(dxh)(dah).
48

Proof. From deﬁnition of Pπ
h and Lemma 21, we have
∥Pπ
hdh −Pπ
hd′
h∥1 =
ZZ Ph(xh+1|xh, ah)π(ah|xh)
 dh(xh) −d′
h(xh)

(dxh)(dah)
 (dxh+1).
≤
Z 
|dh(xh) −d′
h(xh)|
ZZ
π(ah|xh)Ph(xh+1|xh, ah)(dxh+1)(dah)

(dxh)
≤
Z
|dh(xh) −d′
h(xh)|(dxh) = ∥dh −d′
h∥1.
Lemma 21. For any pseudo-policy π (Deﬁnition 1), we have
Z
πh(ah|xh)(dah) ≤1
∀xh ∈X, h ∈[H].
Proof. Recall πh(ah|xh) = min

πh(ah|xh), Ca
hπD
h (ah|xh)
	
where πh is a valid Markov policy. Then
Z
πh(ah|xh)(dah) =
Z
min

πh(ah|xh), Ca
hπD
h (ah|xh)
	
(dah) ≤
Z
πh(ah|xh)(dah) = 1.
I.4
Covering lemmas
In this subsection, we provide the ℓ1 optimistic cover lemma used in MLE (Lemma 22) and pseudo-
dimension bound for the weight function class (Lemma 23) respectively.
Lemma 22. Suppose Assumption 3 holds. Then for the function class
Fh(Υh−1) = {dh = ⟨µh−1, θh⟩: µh−1 ∈Υh−1, θh ∈Rd, ∥θh∥∞≤1, dh ∈∆(X)},
there exists an ℓ1 optimistic cover Fh(Υh−1) (according to Deﬁnition 3) with scale γ of size |Υh−1| (2⌈Bµ/γ⌉)d
and Fh(Υh−1) ⊆(X →R≥0).
Proof. The ideas of this proof are adapted from the proof of Proposition H.15 in Chen et al. (2022a). Let
Θh = {θh : ∃µh−1 ∈Υh−1, s.t., ⟨µh−1, θh⟩∈Fh(Υh−1)} ⊆{θh : θh ∈Rd, ∥θh∥∞≤1} be the set of
θh parameters associated with Fh(Υh−1). Then any dh ∈Fh(Υh−1) can be written as ⟨µh−1, θh⟩for some
µh−1 ∈Υh and θh ∈Θh. Deﬁne the γ′-neighborhood of θh to be B(θh, γ′) := γ′⌊θh/γ′⌋+ [0, γ′]d, and
construct the optimistic covering function for each dh = ⟨µh−1, θh⟩as
fµh−1,θh(x) =
max
θ∈B(θh,γ′)
⟨µh−1(x), θ⟩
∀x ∈X.
Note that fµh−1,θh ≥dh pointwise, thus fµh−1,θh ≥0, though it is not necessarily a valid distribution.
Further,
∥fµh−1,θh −dh∥1 ≤
Z
max
θ∈B(θh,γ′)
|⟨θ −θh, µh−1(x)⟩|(dx)
≤
Z
max
θ∈B(θh,γ′)
∥θ −θh∥∞∥µh−1(x)∥1(dx)
≤γ′
Z
∥µh−1(x)∥1(dx)
≤γ′Bµ
49

using Assumption 3 in the last line. Observe that there are at most (2⌈1/γ′⌉)d unique γ′-neighborhoods in
the set {B(θh, γ′)}θh∈Θh. This implies that there are at most |Υh−1| (2⌈1/γ′⌉)d unique functions in the set
{fµh−1,θh}⟨µh−1,θh⟩∈Fh(Υh−1), which forms an ℓ1-optimistic cover of Fh(Υh−1) of scale γ′. Finally, setting
γ′ = γ/Bµ gives us an ℓ1-optimistic covering of Fh(Υh−1) of scale γ with size |Υh−1| (2⌈Bµ/γ⌉)d.
Lemma 23. For any h ∈[H] and density feature µh−1 ∈Υh−1, the function class
Wh(µh−1) =

wh =
⟨µh−1, θup
h ⟩
⟨µh−1, θdown
h
⟩: ∥wh∥∞≤Cx
h−1Ca
h−1, θup
h , θdown
h
∈Rd

.
has pseudo-dimension (Deﬁnition 6) bounded as Pdim(Wh(µh−1)) ≤4(d + 1) log(8e).
Proof. For any h and µh, consider the unconstrained version W′
h(µh−1) of Wh(µh−1):
W′
h(µh−1) =

w =
⟨µh−1, θup
h ⟩
⟨µh−1, θdown
h
⟩: θup
h , θdown
h
∈Rd

.
Clearly, Wh(µh−1) ⊆W′
h(µh−1), thus Pdim(Wh(µh−1)) ≤Pdim(W′
h(µh−1)), and Pdim(W′
h(µh−1)) =
VCdim(HW′
h(µh−1)), where HW′
h(µh−1) = {h = sign(w −c) : w ∈W′
h(µh−1), c ∈R}. We will use
Lemma 24 to bound VCdim(HW′
h(µh−1)). Any h(x) ∈HW′
h(µh−1) may be written as the following Boolean
formula
Φ = 1
 ⟨µh−1(x), θup
h ⟩
⟨µh−1(x), θdown
h
⟩−c ≥0

=
 
1
" d
X
i=1
µh−1(x)[i]θup
h [i] −c
d
X
i=1
µh−1(x)[i]θdown
h
[i] ≥0
#
1 ∧
" d
X
i=1
µh−1(x)[i]θdown
h
[i] ≥0
#!
∨
 
1
" d
X
i=1
µh−1(x)[i]θup
h [i] −c
d
X
i=1
µh−1(x)[i]θdown
h
[i] ≤0
#
∧1
" d
X
i=1
µh−1(x)[i]θdown
h
[i] < 0
#!
which involves k = 2d + 1 real variables, a polynomial degree of at most l = 1 in these variables, and
s = 4 atomic predicates. Then from Lemma 24, Pdim(Wh(µh−1))) ≤VCdim(HW′
h(µh−1)) ≤4(d +
1) log(8e).
Lemma 24 (Theorem 2.2 of Goldberg and Jerrum (1993)). Let Ck,m be a concept class where concepts
and instances are represented by k and m real values, respectively. Suppose that the membership test for
any instance c in any concept C of Ck,m can be expressed as a Boolean formula Φk,m containing s distinct
atomic predicates, each predicate being a polynomial inequality over k + m variables of degree at most l.
Then the VC dimension of Ck,m is bounded as VCdim(Ck,m) ≤2k log(8els).
I.5
Probabilistic tools
In this section, we deﬁne standard tools from statistical learning theory (Anthony and Bartlett, 2009; Vapnik,
1998) that we use in our proofs. We note that, for convenience, we may override some notations from the
main paper, e.g., ε does not refer to the same thing as in other sections.
Deﬁnition 5 (VC-dimension). Let F ⊆{−1, +1}X and xm
1 = (x1, . . . , xm) ∈X m. We say xm
1 is shattered
by F if ∀b ∈{−1, +1}m, ∃fb ∈F such that (fb(x1), . . . , fb(xm)) = (b1, . . . , bm) ∈Rm. The Vapnik-
Chervonenkis (VC) dimension of F is the cardinality of the largest set of points in X that can be shattered
by F, that is, dim(F) = max{m ∈N | ∃xm
1 ∈X m, s.t. xm
1 is shattered by F}.
50

Deﬁnition 6 (Pseudo-dimension). Let F ⊆RX and xm
1 = (x1, . . . , xm) ∈X m. We say xm
1 is pseudo-
shattered by F if ∃c = (c1, . . . , cm) ∈Rm such that ∀y = (y1, . . . , ym) ∈{−1, +1}m, ∃fy ∈F
such that sign(fy(xi −ci) = yi ∀i ∈[m]. The pseudo-dimension of F is the cardinality of the largest
set of points in X that can be pseudo-shattered by F, that is, Pdim(F) = max{m ∈N | ∃xm
1
∈
X m, s.t. xm
1 is pseudo-shattered by F}.
Deﬁnition 7 (Uniform covering number). For p = 1, 2, ∞, the uniform covering number of H w.r.t. the
norm ∥· ∥p is deﬁne as
Np(ε, H, m) = max
xm
1 ∈X m Np(ε, H, xm
1 )
where Np(ε, H, xm
1 ) is the ε-covering number of H|xm
1 w.r.t. ∥· ∥p, that is, the cardinality of the smallest set
S such that for every h ∈H|xm
1 , ∃s ∈S such that ∥h −s∥p < ε.
Lemma 25 (Bounding uniform covering number by pseudo-dimension, Corollary 42 of Modi et al. (2021)).
Given a hypothesis class H ⊆(Z →[a, b]), for any m ∈N we have
N1(ε, H, m) ≤
4e2(b −a)
ε
Pdim(H)
.
Lemma 26 (Uniform deviation bound using covering number, adapted from Corollary 39 of Modi et al.
(2021)). For b ≥1, let H ⊆(Z →[−b, b]) be a hypothesis class and Zn = (z1, . . . , zn) be i.i.d. samples
drawn from some distribution P(z) supported on Z. Then
P
 E[h(z)] −1
n
n
X
i=1
h(zi)
 ≥ε
!
≤36N1

ε3
640b2 , H, 40nb2
ε2

exp

−
nε2
128V[h(z)] + 512εb

.
51

