Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 728–736
Hong Kong, China, November 3-4, 2019. c⃝2019 Association for Computational Linguistics
728
Memory Graph Networks
for Explainable Memory-grounded Question Answering
Seungwhan Moon, Pararth Shah, Anuj Kumar, Rajen Subba
Facebook Assistant
{shanemoon, pararths, anujk, rasubba@}fb.com
Abstract
We introduce Episodic Memory QA, the
task of answering personal user questions
grounded on memory graph (MG), where
episodic memories and related entity nodes
are connected via relational edges. We cre-
ate a new benchmark dataset ﬁrst by gen-
erating synthetic memory graphs with simu-
lated attributes, and by composing 100K QA
pairs for the generated MG with bootstrapped
scripts.
To address the unique challenges
for the proposed task, we propose Memory
Graph Networks (MGN), a novel extension of
memory networks to enable dynamic expan-
sion of memory slots through graph traver-
sals, thus able to answer queries in which con-
texts from multiple linked episodes and exter-
nal knowledge are required. We then propose
the Episodic Memory QA Net with multiple
module networks to effectively handle various
question types.
Empirical results show im-
provement over the QA baselines in top-k an-
swer prediction accuracy in the proposed task.
The proposed model also generates a graph
walk path and attention vectors for each pre-
dicted answer, providing a natural way to ex-
plain its QA reasoning.
1
Introduction
The task of question and answering (QA) has been
extensively studied, where many of the existing
applications and datasets have been focused on
the fact retrieval task from a large-scale knowl-
edge graph (KG) (Bordes et al., 2015), or machine
reading comprehension (MRC) approaches given
unstructured text (Rajpurkar et al., 2018). In this
work, we introduce the new task and dataset for
Episodic Memory QA, in which the model an-
swers personal and retrospective questions based
on memory graphs (MG), where each episodic
memory and its related entities (e.g. knowledge
graph (KG) entities, participants, ...) are repre-
Figure 1: Illustration of Episodic Memory QA with
user queries and memory graphs (MG) with knowledge
graph (KG) entities. Relevant memory nodes are pro-
vided as initial memory slots via graph search lookup.
The Memory Graph Network walks from the initial
nodes to attend to relevant contexts and expands the
memory slots when necessary. The main QA model
takes these graph traversal paths and expanded memory
slots as input, and predicts correct answers via multiple
module networks (e.g. COUNT, CHOOSE, etc.)
sented as the nodes connected via corresponding
edges (Figure 1). Examples of such queries in-
clude “Where did we go after we had brunch with
Jon?”, “How many times did I go to jazz concerts
last year?”, etc. For Episodic Memory QA, a ma-
chine has to understand the contexts of a question
and navigate multiple MG episode nodes as well
as KG nodes to gather comprehensive information
to match the query requirement.
While the ability of querying a personal
database could lead to many potential applica-
tions, previous work in this domain (Jiang et al.,
2018) is limited due to the lack of a large-scale

729
Figure 2: Overall architecture of the Episodic Memory QA Network. For an input query q, candidate memory
nodes m = {m(k)} are provided as input memory slots for the Memory QA Network. The Memory Graph
Network then traverses the memory graph to expand the initial memory slots and activate other relevant entity and
memory nodes based on the input queries. The Answer Module Networks execute the predicted neural programs
to decode answers given the memory graph network outputs.
dataset and a unique set of challenges unseen in
other tasks: For example, we observe that 1) Mem-
ory QA queries often include ambiguous and in-
complete descriptions of reference memory (as
opposed to many conventional Fact QAs with
unambiguous mentions, e.g.
“Who painted the
Mona Lisa?”), hence requiring extensive candi-
date memory generation. Another challenge we
observe is the case where 2) target memory is only
indirectly linked to reference memory or entities
(e.g. “Where did we go after brunch?”), which
makes the conventional information retrieval (IR)
approaches for generating answer candidates inef-
fective. In addition, 3) queries are not conﬁned to
retrieval tasks, but include various types of ques-
tions such as counting, set comparing, etc., many
of which remain unsolved or not considered in
many QA tasks.
To this end, we propose a new model called the
Memory Graph Network (MGN) to address the
speciﬁc challenges stated above that come with
Memory QA. While Memory Networks have suc-
cessfully been used in QA applications, typical
limitations are that memory slots are limited to
the ﬁxed number of slots, often in sentence or
bag-of-symbols forms. MGN extends the popular
memory networks by storing graph nodes as mem-
ory slots and by allowing the network to dynam-
ically expand memory slots through graph traver-
sals. We then implement the main Episodic Mem-
ory QA Network with multiple module networks
such as CHOOSE, COUNT, etc., to effectively han-
dle various question types not easily handled via
graph networks.
To bootstrap a large-scale dataset collection for
Episodic Memory QA, we ﬁrst build a synthetic
memory graph generator, which creates multiple
episodic memory graph nodes connected with real
entities (e.g. locations, events, public entities) ap-
pearing on common-fact KGs. By creating a real-
istic memory graph that is synthetically generated,
we avoid the need for inferring memory graphs
from other structured data (e.g.
photo albums)
which are often limited in size. We then generate
100K QA pairs for each memory node with tem-
plates composed by human annotators, combined
with 1K manual paraphrasing steps. More details
for dataset collection are provided in Section 3.
2
Method
Figure 2 illustrates the overall architecture and
the model components that make up the Episodic
Memory QA Net. We examine each module in de-
tail and provide our rationale about its formulation
in the following sections.
Input Module (Section 2.1): For a given query
q, its relevant memory nodes m = {m(k)}K
k=1
for slot size K are given as initial memory slots.
At test time, relevant memory nodes can be re-
trieved from a graph search engine that measures
textual similarity (e.g. n-gram TF-IDF) between
its connected node contexts and query. The Query
Encoder then encodes the input query with a lan-
guage model, and the Memory Encoder encodes
each memory slot for both structural and semantic
properties of each memory.

730
Figure 3: Memory Graph Network (MGN) Walker. Input query q and candidate memory nodes m = {m(k)}
are encoded with the query encoder and the memory encoder, respectively (left). The decoder (right) predicts both
the optimal paths and the ﬁnal nodes p = {pt}T
t=1 based on their relevance scores as well as soft-attention based
walk paths, which expands the initial memory slots.
Memory Graph Networks (MGN) (Section 2.2):
Many previous work in QA or MRC systems
use memory networks to evaluate multiple an-
swer candidates with transitive reasoning, and typ-
ically store all potentially relevant raw sentences
or bag-of-symbols as memory slots.
However,
naive increase of memory slot size or retention-
based sequential update of memory slots often in-
crease search space for answer candidates, lead-
ing to poor precision especially for the Episodic
Memory QA task. To overcome this issue, with
MGN we store memory graph nodes as initial
memory slots, where additional contexts and an-
swer candidates can be succinctly expanded and
reached via graph traversals. For each (q, m(k))
pair, MGN predicts optimal memory slot expan-
sion steps: p(k) = {[p(k)
e,t ; p(k)
n,t]}T
t=1 for edge paths
pe and corresponding node paths pn (Figure 3).
QA Modules (Section 2.3, 2.4): An estimated an-
swer ˆa = QA(m, q) is predicted given a query
and MGN graph path output from initial memory
slots. Speciﬁcally, the model outputs a module
program {u(k)} for several module networks (e.g.
CHOOSE, COUNT, ...) via module selector, each
of which produces an answer vector . The aggre-
gated result of module network outputs determines
the top-k answers.
2.1
Input Encoding
Query encoder: We represent each textual query
with an attention-based Bi-LSTM language model
(Conneau et al., 2017) with GloVe (Pennington
et al., 2014) distributed word embeddings trained
on the Wikipedia and the Gigaword corpus with a
total of 6B tokens.
Memory encoder: We represent each memory
node based on both its structural features (graph
embeddings) and contextual multi-modal features
from its neighboring nodes (e.g. attribute values).
Structural contexts of each memory node (ms)
are encoded via graph embeddings projection ap-
proaches (Bordes et al., 2013), in which nodes
with similar relation connectivity are mapped
closer in the embeddings space. The model for
obtaining embeddings from a MG (composed of
subject-relation-object (s, r, o) triples) can be for-
mulated as follows:
P(Ir(s, o)=1|θ) = score
 e(s), er(r), e(o)

(1)
where Ir is an indicator function of a known re-
lation r for two entities (s,o) (1: valid relation, 0:
unknown relation), e is a function that extracts em-
beddings for entities, er extracts embeddings for
relations, and score(·) is a function (e.g. multi-
layer perceptrons) that produces a likelihood of a
valid triple.
For contextual representation of memories
(mc), we compute attention-weighted sum of tex-
tual representation of neighboring nodes and at-
tributes (connected via rj ∈R), using the same

731
language model as the query encoder:
mc =
X
γjmc,j
γ = σ(Wqγq)
Note that the query attention vector γ attenuates
or ampliﬁes each attribute of memory based on a
query vector to better account for query-memory
compatibility accordingly. We then concatenate
the structural features with semantic contextual
features to obtain the ﬁnal memory representation
(m = [ms; mc]).
2.2
Memory Graph Network
Inspired by the recently introduced graph traversal
networks (Moon et al., 2019) which output dis-
crete graph operations given input contexts, we
formulate our MGN as follows. Given a set of ini-
tial memory slots (m) and a query (q), the MGN
model outputs a sequence path of walk steps (p)
within MG to attend to relevant nodes or expand
initial memory slots (Figure 3):
{p(k)} = MGN(q, {m(k)})
(2)
Speciﬁcally,
we deﬁne the attention-based
graph decoder model which prunes unattended
paths, which effectively reduce the search space
for memory expansion. We formulate the decod-
ing steps for MGN as follows (bias terms for gates
are omitted for simplicity of notation):
it = σ(Whiht−1 + Wcict−1)
ct = (1 −it) ⊙ct−1
+ it ⊙tanh(Wzczt + Whcht−1)
ot = σ(Wzozt + Whoht−1 + Wcoct)
ht = WALK(x, zt) = ot ⊙tanh(ct)
(3)
where zt is a context vector at decoding step t,
produced from the attention over graph relations
which is deﬁned as follows:
x = Wqmx[q; m(k)]
αt = σ(Whαht−1 + Wxαx)
zt = ht−1 +
X
rj∈R
αt,jrj
(4)
where αt ∈R|R| is an attention vector over the re-
lations space, rk is relation embeddings, and zt is
a resulting node context vector after walking from
its previous node on an attended path.
The graph decoder is trained with the ground-
truth walk paths by computing the combined loss
of Lwalk(m, q, p) = P
i,t Le + Ln between pre-
dicted paths and each of {pe, pn}, respectively
(Le: loss for edge paths, and Ln for node paths):
X
˜
pe̸=p(i)
e,t
max[0, ˜pe · pe,t(i)−αtr · (p(i)
e,t−˜ye)⊤]
+
X
˜
pn̸=p(i)
n,t
max[0, ˜
pn · pn,t(i)−ht(i) · (p(i)
n,t−˜
pn)⊤]
At test time, we expand the memory slots by
activating the nodes along the optimal paths based
on the sum of their relevance scores (left) and soft-
attention-based output path scores (right) at each
decoding step:
p(k)
n,t = argmax
p(k)
n ∈VR,1(p(k)
n,t−1)
ht · p(i)
n
⊤+
X
αt,jrj · p(k)
e
⊤
(5)
2.3
Module Networks
MGN outputs are then passed to module networks
for the ﬁnal stage of answer prediction. We ex-
tend the previous work in module networks (Kot-
tur et al., 2018), often used in VQA tasks, to
accommodate for graph nodes output via MGN.
We ﬁrst formulate the module selector which out-
puts the module label probability {u(k)} given in-
put contexts for each memory node, trained with
cross-entropy loss Lmodule:
{u(k)} = Softmax(MLP(q, {m(k)}))
(6)
We then deﬁne the memory attention to attenu-
ate or amplify all activated memory nodes based
on their compatibility with query, formulated as
follows:
β = MLP(q, {m(k)}, {p(k)})
(7)
α = Softmax(W⊤
β β) ∈RK
(8)
For this work, we propose the following four
modules: CHOOSE, COUNT, CONFIRM, SET OR,
and SET AND, hence u(k) ∈R5. Note that the
formulation can be extended to the auto-regressive
decoder in case sequential execution of modules is
required.
CHOOSE module outputs answer space vector
by assigning weighted sum scores to nodes along
the MGN soft-attention walk paths.
End nodes
with the most probable walk paths thus get the

732
highest scores, and their node attribute values are
considered as answer candidates. COUNT mod-
ule counts the query-compatible among the ac-
tivated nodes, a = W⊤
K([α; max{α}; min{α}]).
CONFIRM uses a similar approach to COUNT, ex-
cept it outputs a binary label indicating whether
the memory nodes match the query condition:
a = W⊤
b ([α; max{α}; min{α}]). SET modules
either combine or ﬁnd intersection among answer
candidates by updating the answer vectors with
a = max{W⊤
s {a(k)}} or a = min{W⊤
s {a(k)}}.
2.4
Answer Decoding
Answers from each module network (Section 2.3
are then aggregated as weighted sum of answer
vectors with module probability (Eq.6), guided by
memory attention (Eq.7). Predicted answers are
evaluated with cross-entropy loss Lans
We observe that the model performs better when
the MGN component of the model is pre-trained
with ground-truth paths. We thus ﬁrst train the
MGN network with the same training split (with-
out answer labels), and then train the entire model
with module networks, fully end-to-end super-
vised with L = Lwalk + Lmodule + Lans.
3
Dataset
To empirically evaluate the proposed approach
for the Episodic Memory QA task, we create a
new dataset, MemQA, of 100K question and an-
swer pairs composed based on synthetic memory
graphs that are artiﬁcially generated. We speciﬁ-
cally use the synthetically generated MG to avoid
the need for inferring memory graphs from other
structured data (such as publicly available photo
albums such as Flicker, etc.)
which are often
limited in size and domains.
We bootstrap our
large-scale realistic memory graph dataset with the
following procedures: ﬁrst, we construct a syn-
thetic social graph with a set number of artiﬁ-
cial users, each with randomly generated interest
embeddings. We then create a realistic memory
graph by randomly choosing participants within
the synthetic social graph as well as activities and
associated entities from the curated list (of lo-
cations, events, public entities, etc.), which is a
subset of common-fact Freebase knowledge base
(Bast et al., 2014). Each generated memory node
thus has connections to entities appearing in KGs,
comprising the memory graph together. Finally,
given a set of reference memory nodes and neigh-
Answer Type
% Examples
Location
18 Mt. Rainier, AMC Theater
KG entities & events 17 Iron Man, Coachella
Common nouns, etc. 16 skiing, movie
Person / Group
16 Mark, Jon
Count
15 zero, three
Date / Time
10 01-02-2018, 7 PM
Yes/No
6 -
Miscellaneous
2 -
Table 1: Types of Answers in MemQA
boring attributes, we pragmatically generate tar-
get ground-truth answer samples (e.g. single-hop
/ multi-hop node value, count, set comparison,
yes/no, etc.) We then collect QA pairs for each
sample with templates composed with human an-
notators, which are combined with 1K manual
paraphrasing steps for added variety and conﬁrma-
tion. We randomly split the QA corpus into into
train (70%), validation (15%), and test sets (15%).
4
Empirical Evaluation
Task: Given a query and a set of initial mem-
ory graph nodes via graph search, we evaluate the
model on the open-ended question answering pre-
diction task.
4.1
Baselines
We choose as baselines the following QA systems
(see Section 5 for details), and modify them ac-
cordingly to make comparisons with our task:
• MemN2N (Sukhbaatar et al., 2016): uses the
end-to-end memory networks with the static
set of initial memory slots.
Each memory
slot is represented with a bag-of-symbols for
surrounding attributes and nodes. We use a
single Softmax layer for answer classiﬁca-
tion. Memory slot size is tuned as a hyper-
parameter.
• MemexNet (Jiang et al., 2018): uses the tex-
tual representation for multi-modal attributes,
and frames the problem into a classiﬁcation
problem via text kernel match approaches
to predict approximate answers.
Since the
dataset does not contain images, we omit the
CNN representations.
We also consider several conﬁgurations of our
proposed approach to examine contributions of

733
Components
Model
Precision@k
k=1
3
5
10
A
MemN2N (Sukhbaatar et al., 2016) 29.7 43.8 51.0 50.5
A
Memex (Jiang et al., 2018)
32.6 50.3 58.2 59.1
G
MemQANet (ablation)
36.7 56.8 63.5 67.9
G + N
MemQANet (ablation)
41.1 65.8 75.5 80.0
G + N + E
MemQANet (proposed)
45.8 68.1 75.7 80.9
Table 2: QA performance on the MemQA dataset (metric: precision@k). Our proposed model is compared against
the selected baseline models as well as several ablation variations of the proposed model (model components (G):
Memory Graph Network, (N): Module Networks, (E): graph embeddings).
each component (model components (G): Mem-
ory Graph Network, (N): Module Networks, (E):
structural graph embeddings).
• (Proposed; G+N+E): is the proposed ap-
proach as described in Figure 2.
• (G+N): does not structural graph embeddings
for MGN, and relies on semantic representa-
tion of surrounding nodes.
• (G): does not use any of the module networks
(e.g. COUNT, ...), and predicts MGN graph
output as answers instead.
4.2
Results
Parameters:
We tune the parameters of each
model with the following search space (bold in-
dicate the choice for our ﬁnal model): graph em-
beddings size: {64, 128, 256, 512}, Bi-LSTM hid-
den states for the language model: {64, 128, 256,
512}, MGN hidden states: {64, 128, 256, 512},
word embeddings size: {100, 200, 300}, and max
memory slots: {1, 5, 10, 20, 40, 80}. We opti-
mize the parameters with Adagrad (Duchi et al.,
2011) with batch size 10, learning rate 0.01, ep-
silon 10−8, and decay 0.1.
Main Results: Table 2 shows the results of the
top-k predictions of the proposed model and the
baselines. It can be seen that the proposed Mem-
ory QA model outperforms other QA baselines for
precision at all ks.
Speciﬁcally, with the MGN walker model, the
MemQA model learns to condition its walk path
on query contexts and attend and expand mem-
ory nodes, thus outperforming the baseline mod-
els that simply rely on their initial memory slots,
typically large in size to maintain reasonable re-
call.
The node expansion via MGN allows the
model to keep the initial memory slots small (10)
and expand only when necessary (e.g. for queries
that require references to related memories, ”...
where did I go after ...”), thus improving the preci-
sion performance. Note that memory slot sizes for
baselines are tuned for their performance on the
validation set.
In addition, it can be seen that the neural module
components (G+N and G+N+E) greatly outper-
form the ablation model (G) and the baselines by
aggregating answers with the modules speciﬁcally
designed for various types of questions.
These
neural modules allow the model to answer ques-
tions that are typically hard to answer (e.g. count,
set comparison, etc.) by explicitly reducing the
answer space accordingly.
Note also that the graph embeddings (G+N+E)
improve the performance over the ablation model
that does not use structural contexts (G+N), in-
dicating that the model learns to better leverage
knowledge graph contexts to answer questions.
Error Analysis: Table 3 shows some of the ex-
ample output from the proposed model, given the
input question and memory graph nodes. It can be
seen that the model is able to predict answers by
combining answer contexts from multiple compo-
nents (walk path, node attention, neural modules,
etc.) In general, the MGN walker successfully ex-
plores the respective single-hop or multi-hop re-
lations within the memory graph, while keeping
the initial memory slots small enough. The acti-
vated nodes via graph traversals are then used as
input for each neural module, the aggregated re-
sults of which are the ﬁnal top-k answer predic-
tions. There are some cases where the ﬁnal answer
prediction is incorrect, whereas its walk path is
correctly predicted. This is due to inaccurate pre-
diction of memory attention vector given a query
and initial memory slots, which requires compre-

734
Question and Answer
Model Preidction
Walk Path
Neural Module
Top-k Answers
Q: Where did Jon and I go after we watched m1 →(NEXT ID) →m′
1
CHOOSE
Symphony Hall,
Avengers? // A: Symphony Hall
→(LOCATION)
AMC Theatre, ...
Q: Who did I go skiing with last year?
m1 →(PARTICIPANT)
SET OR
{Emma, Jacob},
A: {Emma, Jacob}
m3 →(PARTICIPANT)
{Emma, Noah}, ...
Q: How many sci-ﬁmovies have I watched
m1 →(ENTITY) →e′
1
COUNT
Three,
last year? // A: Two
→(GENRE) →a′
1
Two, Four, ...
Table 3: Error Analysis: Model predictions of walk paths (with expanded memory nodes noted with ’) are partially
shown for each question and ground-truth answer pair. The full reference memory graphs are not shown here due
to space constraints. Initial memory slot size = 10.
hensive understanding of surrounding knowledge
nodes in the context of the query.
5
Related Work
Memory Networks:
Weston et al. (2014);
Sukhbaatar et al. (2016) propose Memory Net-
works with explicit memory slots to contain aux-
iliary multi-input, now widely used in many QA
and MRC tasks for its transitive reasoning capabil-
ity. Traditional limitations are that memory slots
for storing answer candidates are ﬁxed in size,
and naively increasing the slot size typically de-
creases the precision.
Several work extend this
line of research, for example by allowing for dy-
namic update of memory slots given streams of
input (Kumar et al., 2016; Tran et al., 2016; Xu
et al., 2019), reinforcement learning based reten-
tion control (Jung et al., 2018), etc. By allowing
for storing graph nodes as memory slots and for
slot expansion via graph traversals, our proposed
Memory Graph Networks (MGN) effectively by-
pass the issues.
Structured QA systems: often answer ques-
tions based on large-scale common fact knowl-
edge graphs (Bordes et al., 2015; tau Yih et al.,
2015; Xu et al., 2016; Jain, 2016; Yin et al., 2016;
Dubey et al., 2018), typically via an entity linking
system and a QA model for predicting graph oper-
ations through template matching approaches, etc.
Our approach is inspired by this line of work, and
we utilize the proposed module networks and the
MGN walker model to address unique challenges
to Episodic Memory QA.
Machine Reading Comprehension (MRC)
systems: aim at predicting answers given evidence
documents, typically in length of a few paragraphs
(Seo et al., 2017; Rajpurkar et al., 2016, 2018; Cao
et al., 2019; tau Yih et al., 2015). Several recent
work address multi-hop reasoning within multi-
ple documents (Yang et al., 2018; Welbl et al.,
2018; Bauer et al., 2018; Clark et al., 2018) or
conversational settings (Choi et al., 2018; Reddy
et al., 2018), which require often complex reason-
ing tools. Unlike in MRC systems that typically
rely on language understanding, we effectively uti-
lize structural properties of memory graph to tra-
verse and highlight speciﬁc attributes or nodes that
are required to answer questions.
Visual QA systems: aim to answer questions
based on contexts from images (Antol et al., 2015;
Wang et al., 2018; Wu et al., 2018). Recently, neu-
ral modules (Kottur et al., 2018) are proposed to
address speciﬁc challenges to VQA such as visual
co-reference resolutions, etc. Our work extends
the idea of neural modules for Episodic Memory
QA by implementing modules that can take graph
paths as input for answer decoding. Jiang et al.
(2018) proposes visual memex QA which tackles
similar problem domains given a dataset collected
around photo albums. Instead of relying on meta
information and multi-modal content of a photo
album, our work explicitly utilizes semantic and
structural contexts from memory and knowledge
graphs. Another recent line of work for VQA in-
cludes graph based visual learning (Hudson and
Manning, 2019), which aims to represent each im-
age with a sub-graph of visual contexts. While
graph-based VQA operates on a graph constructed
from a single scene, Episodic Memory QA oper-
ates on a large-scale memory graph with knowl-
edge nodes. We therefore propose memory graph
networks to handle ambiguous candidate nodes, a
main contribution of the proposed work.

735
6
Conclusions
We introduce Episodic Memory QA, the task
of answering personal user questions grounded
on memory graph (MG). The dataset is gener-
ated with synthetic memory graphs with simu-
lated attributes, and accompanied with 100K QA
pairs composed via bootstrapped scripts and man-
ual annotations.
Several novel model compo-
nents are proposed for unique challenges for the
Episodic Memory QA: 1) Memory Graph Net-
works (MGN) extends the conventional memory
networks by enabling dynamic expansion of mem-
ory slots through graph traversals, which also nat-
urally allows for explainable predictions. 2) Sev-
eral neural module networks are proposed for the
proposed task, each of which takes queries and
memory graphs as input to infer answers. 3) The
main Episodic Memory QA Net aggregates an-
swer prediction from each neural module to gener-
ate ﬁnal answer candidates. The empirical results
demonstrate the efﬁcacy of the proposed model in
the Memory QA reasoning.
References
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,
and Devi Parikh. 2015. VQA: Visual Question An-
swering. In ICCV.
Hannah Bast, Florian Baurle, Bjorn Buchhold, and El-
mar Haussmann. 2014. Easy access to the freebase
dataset. In WWW.
Lisa Bauer, Yicheng Wang, and Mohit Bansal. 2018.
Commonsense for generative multi-hop question an-
swering tasks. EMNLP.
Antoine Bordes, Nicolas Usunier, Sumit Chopra, and
Jason Weston. 2015.
Large-scale simple question
answering with memory network. arxiv.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In NIPS.
Nicola De Cao, Wilker Aziz, and Ivan Titov. 2019.
Question answering by reasoning across documents
with graph convolutional networks. NAACL.
Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-
tau Yih, Yejin Choi, Percy Liang, and Luke Zettle-
moyer. 2018. Quac: Question answering in context.
EMNLP.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018.
Think you have solved question
answering?
try arc, the ai2 reasoning challenge.
arXiv:1803.05457.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo¨ıc
Barrault, and Antoine Bordes. 2017.
Supervised
learning of universal sentence representations from
natural language inference data. In EMNLP.
Mohnish Dubey, Debayan Banerjee, Debanjan Chaud-
huri, and Jens Lehmann. 2018. Earl: Joint entity and
relation linking for question answering over knowl-
edge graphs. ESWC.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. JMLR.
Drew A Hudson and Christopher D Manning. 2019.
Gqa: A new dataset for real-world visual reasoning
and compositional question answering. CVPR.
Sarthak Jain. 2016. Question answering over knowl-
edge base using factual memory networks. NAACL.
Lu Jiang, Junwei Liang, Liangliang Cao, Yannis Kalan-
tidis, Sachin Farfade, and Alexander Hauptmann.
2018. Memexqa: Visual memex question answer-
ing. arXiv.
Hyunwoo Jung, Moonsu Han, Minki Kang, and Sungju
Hwang. 2018. Learning what to remember: Long-
term episodic memory networks for learning from
streaming data. arXiv preprint arXiv:1812.04227.
Satwik Kottur, Jos´e MF Moura, Devi Parikh, Dhruv
Batra, and Marcus Rohrbach. 2018. Visual corefer-
ence resolution in visual dialog using neural module
networks. In ECCV.
Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit
Iyyer, James Bradbury, Ishaan Gulrajani, Victor
Zhong, Romain Paulus, and Richard Socher. 2016.
Ask me anything: Dynamic memory networks for
natural language processing. In ICML.
Seungwhan Moon, Pararth Shah, Anuj Kumar, and Ra-
jen Subba. 2019. Opendialkg: Explainable conver-
sational reasoning with attention-based walks over
knowledge graphs. ACL.
Jeffrey Pennington, Richard Socher, and Christo-
pher D. Manning. 2014. Glove: Global vectors for
word representation. In EMNLP.
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
Know what you don’t know: Unanswerable ques-
tions for squad. ACL.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. arXiv:1606.05250.
Siva Reddy, Danqi Chen, and Christopher D Manning.
2018. Coqa: A conversational question answering
challenge. arXiv preprint arXiv:1808.07042.

736
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and
Hannaneh Hajishirzi. 2017. Bidirectional attention
ﬂow for machine comprehension. ICLR.
Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
and Rob Fergus. 2016.
End-to-end memory net-
works. NIPS.
Ke Tran, Arianna Bisazza, and Christof Monz. 2016.
Recurrent memory networks for language modeling.
NAACL.
Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and
Anton van den Hengel. 2018. Fvqa: Fact-based vi-
sual question answering. PAMI.
Johannes Welbl, Pontus Stenetorp, and Sebastian
Riedel. 2018. Constructing datasets for multi-hop
reading comprehension across documents. TACL.
Jason Weston,
Sumit Chopra,
and Antoine Bor-
des. 2014.
Memory networks.
arXiv preprint
arXiv:1410.3916.
Qi Wu, Chunhua Shen, Peng Wang, Anthony Dick, and
Anton van den Hengel. 2018. Image captioning and
visual question answering based on attributes and
external knowledge. PAMI.
Kun Xu, Yuxuan Lai, Yansong Feng, and Zhiguo
Wang. 2019. Enhancing key-value memory neural
networks for knowledge based question answering.
NAACL.
Kun Xu, Siva Reddy, Yansong Feng, Songfang Huang,
and Dongyan Zhao. 2016. Question answering on
freebase via relation extraction and textual evidence.
ACL.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W Cohen, Ruslan Salakhutdinov, and
Christopher D Manning. 2018. Hotpotqa: A dataset
for diverse, explainable multi-hop question answer-
ing. EMNLP.
Wen tau Yih, Ming-Wei Chang, Xiaodong He, and
Jianfeng Gao. 2015.
Semantic parsing via staged
query graph generation: Question answering with
knowledge base. ACL.
Wenpeng Yin, Mo Yu, Bing Xiang, Bowen Zhou, and
Hinrich Sch¨utze. 2016. Simple question answering
by attentive convolutional neural network.
COL-
ING.

