An Overview of Fast Multipole Methods
Alexander T. Ihler
April 23, 2004
Sing, Muse, of the low-rank approximations
Of spherical harmonics and O(N) computation...
Abstract
We present some background and results from the body of work collectively referred to as fast multipole
methods (FMM). These comprise a set of techniques for speeding up so-called N-body problems, in which
a potential function composed of a sum of pairwise interaction terms from N points is evaluated at an
equally large number of locations.
We present these methods from the viewpoint of low-rank block
matrix approximations, ﬁrst discussing a heuristic block-matrix approximation method [1], then moving
into the analytic expansions which form the basis of both the original and new versions of the fast
multipole method [2].
We attempt to provide suﬃcient background to understand and compute all
the relevant results, yet present the material in suﬃcient generality that it is easy to understand the
relationship to similar algorithms such as the fast Gauss transform [3].
1
Introduction
In many problem domains and applications, simulation of physics-based interaction models or other direct
methods result in substantial computational overhead. One such class of problems are described by pairwise
interaction models, in which the function of interest (perhaps a cost or energy function) can be decomposed
into sums of pairwise interactions with a set of “source points” xi:
f(y) =
N
X
i=1
qiK(y −xi)
(1)
where the qi are scalar values. In particular, these types of problems are found in such wide-ranging ﬁelds
as physics (computation of electrostatic or gravitational potentials and ﬁelds), ﬂuid and molecular dynamic
simulations, and density estimation. They are sometimes called “N-body” problems, since they involve N
“bodies” of charge, mass, or probability.
We have not speciﬁed the dimensionality of the variables y, xi; in typical applications for physics they
are three- (or possibly two-) dimensional values. Although this is not necessarily required it is certainly
convenient for gaining intuition. We will use the variables x, y, z to denote vectors of arbitrary dimension,
and use |y| to denote the length of y. Otherwise (unless speciﬁed) all variables refer to scalar values. In
order to provide intuition we will often refer to the xi as “point charges” and qi as their associated charge,
and give examples couched in terms of computing an electrostatic potential. However, precisely the same
equations apply equally well to e.g. the gravitational potential (with the additional requirement that the qi
be positive), and the development for many other problems is quite similar.
Typically the applications of interest involve evaluating f(·) at an equally large number of target locations
{yj}.
For many cases, we desire to evaluate f at the same locations xi (with the self-interaction term
omitted):
f(xj) =
X
i̸=j
K(xj −xi)
(2)
1

≈
Figure 1: The far-ﬁeld eﬀect from several point charges can be approximated as a single function capturing
their net eﬀect, rather than requiring that each contribution be calculated individually.
Notationally, it is convenient to assume at least that the evaluation locations {yj} and point charge locations
{xi} are approximately equal in size. This makes a direct, exact evaluation of the function f parameterized by
point charges {qi, xi}N
i=1 at locations {yj}N
j=1 nominally a quadratic time operation, O(N 2). Fast multipole
methods (FMM) can be used to approximate this calculation to some pre-speciﬁed level of accuracy in only
O(N) time.
2
Far-ﬁeld approximation
Although the function f may be quite complex in the near-ﬁeld (when y is close to the set of charges at
{xi}), if the function K is smooth far from zero, f will be well-behaved in the far-ﬁeld1. In particular, this
means that if the location y is suﬃciently far from a set of charges at {xi}, we may compute the aggregate
eﬀect of the charges, and not need to resort to computing every interaction.
As an example, consider the electrostatic ﬁeld due to several point charges examined from very far away.
In particular, we assume here that the origin of our coordinate system is near to the point charge locations
(|xi| is small for each i) and far from the location of evaluation (|y| is large). This situation is depicted
in Figure 1. We may recall from classical physics that, suﬃciently far away and up to ﬁrst order, we may
approximate their net eﬀect by a single point with total charge given by the sum of the qi. We make this
approximation more precise in later sections.
It is generally the case that for any value of y, the sum (2) is dominated by only a few of the xi’s terms
(those nearby). The rest of the charges’ contributions are suﬃciently far that an aggregate approximation
is reasonable. By approximating increasingly larger sets as their distance grows, we may greatly reduce the
total computational burden. Such “divide-and-conquer” style calculations underlie many computationally
eﬃcient algorithms.
3
Matrix Formulation
To construct a precise generalization of such far-ﬁeld approximations and to gain additional intuition, we
ﬁrst describe the generic problem in terms of matrix multiplication. Let us denote the N × N matrix of
pairwise interactions by M, i.e.
Mji = K(yj −xi)
(3)
3.1
Low-rank matrix approximations
We are interested in the computational cost of matrix-vector products Mv for some vector v; in particular, the
values of the function f as discussed in Section 1 at the locations {yj} are exactly Mq where q = [q1, . . . , qN].
If M is an arbitrary matrix, these matrix-vector products are O(N 2) to compute. However, for certain
types of matrices they may be performed more eﬃciently. For example, this is the case if M is low-rank.
The rank of M (which we denote by r) is the number of linearly independent rows or columns in M [4].
Considering the singular value decomposition of M,
M = U ′SV
(4)
1Often, the functions K are symmetric and approach zero as distance increases; however, the techniques applied do not
require these conditions and we do not impose them. We will, however, require the function to become arbitrarily smooth with
increasing distance; this rules out, for example, purely oscillatory kernel functions.
2

where U and V are size N × N unitary matrices (UU ′ = V V ′ = IN, the N × N identity matrix) and S is
a diagonal matrix of the singular values, the rank of M is the number of non-zero elements of S. Typically,
the elements of s are ordered from largest to smallest, s1 ≥s2 ≥. . . ≥sr (the rest being zero). The limited
number of independent rows means that any matrix-vector product may be taken in only O(rN) time.
Often, a real-world matrix M will be technically full rank; in this event we may instead consider the
eﬀective rank of M. The eﬀective rank is deﬁned in terms of some numerical tolerance ϵ, as the number
of singular values which are greater than ϵ · s1; we denote this quantity by rϵ. While the cost of exact
computations scale with the rank r, the cost of approximate computations depends on the eﬀective rank rϵ.
Given a matrix M we may construct a low-rank approximation Mϵ by retaining only those singular values
greater than rϵ, i.e. Mϵ = U ′SϵV , where Sϵ is a thresholded version of S. For such a matrix Mϵ, one may
derive error bounds on an arbitrary matrix-vector product Mv versus the (computationally easier) Mϵv. Let
|v| be the standard vector norm, |v| =
qPN
i=1(vi)2; then we have that
|Mv −Mϵv| ≤rϵ|v|
(5)
See e.g. [4] for more detail on low-rank matrix approximation.
3.2
Block approximations
Unfortunately, it is typical that the matrices of interest in real-world N-body problems also have full eﬀective
rank. However, they often have blocks which are low rank (or low eﬀective rank); this corresponds to the
observation that, while y is unlikely to be in the far-ﬁeld for all the xi, it will be far from many of them. For
example, several clustered points xI (where I ⊂{1, . . . , N} may have nearly identical inﬂuence on another
set of far-away locations yJ. Then, the block MJI is nearly constant (eﬀective rank one), and computations
involving it are correspondingly less expensive.
This leads us to consider a natural tradeoﬀin the computational eﬃciency of the matrix-vector products.
In particular, we may choose the size of the blocks we approximate; we would like each block’s size to be
large enough that there are few total blocks, but small enough that each block needs only a relatively low
rank to achieve the required tolerance level.
In [1], this tradeoﬀis optimized by heuristically joining blocks of the (pre-speciﬁed) matrix M, and
approximating the block with a low-rank matrix which meets some error tolerance ϵ. This joining procedure
continues until the amount of work required to perform a matrix-vector product is at its local optimum.
Algorithmically, this can be done reasonably eﬃciently:
1. Sort the points so as to put strongly interacting points close together (near the diagonal), and weaker
interactions farther apart (oﬀ-diagonal). One is free to choose precisely how this is done.
2. Grid the matrix into k × k blocks (where k is a parameter of the algorithm), and let l = 0. Compute
rank rϵ approximations of each block, and associate with each block an optimal level (initialized to l)
at which its products will be computed.
3. Re-grid the matrix with block size 2k; call these blocks parents, and the four sub-blocks enclosed the
children. Increment l = l + 1.
4. For each parent block all of whose children have optimal level l −1,
(a) Compute a low-rank approximation to the parent block. Note that this can be done more eﬃciently
by ﬁrst merging pairs horizontally, then vertically (Figure 2); see [1] for more detail.
(b) Compare the computational burden of using the parent to that of using all four children — if the
rank of the parent is less than one-half the summed ranks of the children, set the optimal level of
this block to l; otherwise, set it to l −1.
5. Otherwise, set the parent’s optimal level to the largest level of its children.
6. Set k ←2k and repeat from Step 3.
3

U S V
1 1 1 U S V
2 2 2
U S V
4 4 4
U S V
3 3 3
U   S   V
34 34 34
U   S   V
12 12 12
U      S       V
1234 1234 1234
Figure 2: Low-rank approximations for larger blocks may be constructed eﬃciently by merging the approx-
imations of the constituent smaller blocks; from [1].
Figure 3: Block-wise low-rank approximation to a 256 × 256 inductance matrix M from [1]; the number
displayed indicates the retained rank of each block to precision ϵ = .001.
4

This yields a hierarchy of block matrix approximations with bounded error (see Figure 3). While the arbitrary
nature of M means that it is diﬃcult to say anything analytic about the algorithm’s performance, in practice
it has been observed [1] to be slightly super-linear (compared to quadratic for the naive computation).
Of course, none of the presented analysis takes into account the amount of eﬀort required to ﬁnd low-rank
approximations to the blocks of M. For the heuristic joining algorithm discussed, this construction cost is
about O(N 2). Thus, if we need to evaluate many matrix-vector products Mv successively for various vectors
v, pre-processing M can make these much more eﬃcient. However, if we only need to perform the evaluation
a few times, it is no better (and potentially worse) than direct computation.
However, we have as yet done nothing to exploit the known analytic structure of the kernel function K.
In fact, we may ﬁnd a low-rank approximation directly from an analytic series expansion of K, relieving us
of the relatively high cost involved in the matrix decomposition step above.
4
Analytic approximation of the kernel
4.1
Degenerate kernel functions
Suppose that our kernel function K is in fact degenerate, i.e.
K(y −x) =
p−1
X
k=0
Φk(x)Ψk(y)
(6)
Then the sum (1) becomes
f(y) =
N
X
i=1
p−1
X
k=0
Φk(xi)Ψk(y)
=
p−1
X
k=0
Ã N
X
i=1
Φk(xi)
!
Ψk(y)
(7)
eﬀectively decoupling the inﬂuence of the particle locations xi and evaluation locations yj, and allowing all
N evaluations at {yj} to be performed in O(pN) time. Equation (6) indicates a rank-deﬁciency inherent in
the function K, and thus present in the matrix M; speciﬁcally, M has rank at most p.
Just as in Section 3, the kernels of interest are not likely to be exactly or approximately degenerate for
arbitrary values of x and y. However, once again we expect that the far-ﬁeld eﬀects of K will be smooth, and
thus may be analytically shown to be approximately degenerate so long as x and y are suﬃciently far apart.
We describe some of these analytic expansions in the next section; we then describe some data structures
which may be used to determine appropriate groupings of points, analogous to the choice of which potential
indices to group for low-rank block approximation. This leads directly to an O(N log N) algorithm; however,
with some additional eﬀort we may improve this to O(N).
4.2
Expansion of the electrostatic potential
We next describe one analytic series expansion of a particular kernel and its associated error residual bounds,
corresponding to a multipole approximation to the electrostatic potential due to a number of point charges.
The electrostatic potential due to a set of charges qi located at xi is given by
f(y) =
X
i
qi
1
|y −xi|
(8)
We would like to ﬁnd a summation expansion of the kernel function
K(y −x) =
1
|y −x| ≈
∞
X
k=0
Φk(x)Ψk(y)
(9)
5

θ
x
y
0
Figure 4: Constructing a series expansion of the function
1
|y−x|; we consider their positions in terms of the
spherical coordinates around some origin 0.
If (for a given set of xi and yj) this summation is well-approximated by a small number of terms, we have
a situation similar to (7); we will be able to sum the contributions of the xi separately and decouple the
otherwise quadratic cost.
It will turn out to be convenient to perform this approximation in spherical coordinates, essentially
because our selected kernel is (trivially) separable in a spherical coordinate frame. Consider, therefore, the
situation depicted in Figure 4 — a point charge located at x (taken to be near the origin), whose inﬂuence
we wish to compute at another location y (taken to be far from the origin). If we denote the angle between
the two points by θ, by simple trigonometry we have
|y −x|2 = |x|2 + |y|2 −2|x||y| cos θ
(10)
and thus
1
|y −x| = 1
|y|
1
√
1 −2uv + v2
where
u = cos θ
,
v = |x|
|y|
(11)
We may use this to deﬁne a generating function g(u, v) which we expand it into its power series in v
g(u, v) =
1
√
1 −2uv + v2 =
∞
X
n=0
Pn(u)vn
(12)
which deﬁnes the Legendre polynomials Pn(·); speciﬁcally our kernel function is given by
1
|y −x| = 1
|y|
∞
X
n=0
Pn(cos θ)
µ|x|
|y|
¶n
(13)
The series approximation (12) is convergent for |v| < 1, corresponding to the distance assumption |x| < |y|
made at the beginning; thus the series converges very quickly in the far-ﬁeld. We will typically make this
assumption, that the charges xi are near to the origin of our coordinate system and the evaluation point
y is far. However, sometimes it will be useful to consider the opposite, that the origin is near y, and the
contributing points xi are far away. For reasons which will become clear, approximations made under the
ﬁrst assumption are called multipole expansions, while approximations under the latter will be referred to as
local expansions. We concentrate initially on multipole expansions (the coordinate system is centered near
the charge locations xi), and discuss later why local expansions are important and how to construct them.
Unfortunately, although Equation (13) is close to our goal of a separable expansion, we have not yet
achieved it — the two radii |x| and |y| are coupled by the cos θ argument to Pn.
However, it will be
instructive to linger for a moment on the Legendre polynomial expansion form before constructing a truly
separated expansion.
4.2.1
Legendre polynomials
The Legendre polynomials Pn(·) are in fact interesting in their own right. For example, the polynomials
form a complete orthogonal basis of functions deﬁned on the interval [−1, 1]. They arise in a number of
contexts. However, leaving aside these broader contexts, we consider only what we need for the FMM. The
polynomials themselves may be described analytically in a number of diﬀerent ways, some of which may be
more or less convenient than others. The ﬁrst few are listed in Figure 5.
6

P0(u) = 1
P1(u) = u
P2(u) = 1
2(3u2 −1)
P3(u) = 1
2(5u3 −3u)
P4(u) = 1
8(35u4 −30u2 + 3)
Figure 5: The ﬁrst several Legendre polynomials Pn(u).
Perhaps the most numerically convenient form is given by the recurrence relations,
(2n + 1)uPn(u) = (n + 1)Pn+1(u) + nPn−1(u)
(14)
which allows the n + 1st polynomial to be evaluated in terms of the nth and n −1st. Another convenient
form is given by Rodrigues’ formula,
Pn(u) =
1
2nn!
∂n
∂un (u2 −1)n
(15)
The series (13) (and its further expansion into spherical harmonics, described in the next section) is
known as a multipole expansion. This nomenclature comes from the physical meaning behind each term in
the series — suppose that we have a number of charges qi arrayed in some small region compared to |y|. For
suﬃciently large |y| this sum is dominated by its ﬁrst non-zero term. Retaining only the ﬁrst term of (13)
we have that the ﬁeld is given by
f(y) ≈1
|y|
X
i
qi
(16)
This is precisely the ﬁeld due to the qi’s monopole moment, the eﬀect of treating the charge distribution
deﬁned by the qi as a single point charge. If the qi are chosen such that their sum is zero and we examine
the second term of the sum, we have
f(y) ≈
1
|y|2
X
i
cos θi qi |x|,
(17)
the ﬁeld due to the dipole moment of the qi. If, for instance, we have a two-charge symmetric conﬁguration,
i.e. q1 = −q2 and x1 = −x2, we obtain the classic electrostatic dipole potential. Similarly, if the dipole
moment is zero, the third term dominates and is known as the quadrupole moment, and so on.
4.3
Spherical harmonics
While Equation (13) hasn’t quite produced the separation of variables we initially hoped, it has come close.
As it turns out, we may further expand the Legendre polynomial into its own sum:
Pn(cos θ) =
n
X
m=−n
Y −m
n
(x)Y m
n (y)
(18)
giving the series expansion
1
|y −x| =
∞
X
n=0
n
X
m=−n
|x|nY −m
n
(x)Y m
n (y)
|y|n+1
(19)
The functions Y m
n
are called the spherical harmonics; speciﬁcally, Y m
n (y) |y|−n−1 is the spherical harmonic
of degree −n −1. Although spherical harmonics may be deﬁned for higher dimensions, they are typically
developed for three dimensional space. We will proceed in as general terms as possible, but the reader should
note that the deﬁnition of Y must be modiﬁed in higher dimensions. In d ≤3 dimensions, of course, the
formulae hold as presented.
7

For the same reasons discussed in Section 4.2.1, the spherical harmonics of negative degree are also called
multipoles and their coeﬃcients (in this case given by |x|nY −m
n
(x)) are called the moments of the expansion.
In particular, the moments M m
n due to a set of charges {xi} is given by
M m
n =
X
i
qi|xi|nY −m
n
(xi)
(20)
Note that in fact, the functions Y depend only on the angular portion (in spherical coordinates) of their
argument; however we sometimes suppress this independence for convenience in the notation. Interestingly,
the spherical harmonic functions Y m
n themselves form an orthogonal and complete basis for functions on the
sphere, subject to “suﬃcient continuity properties” [5].
The spherical harmonics Y are deﬁned in terms of the associated Legendre functions P m
n (u). Speciﬁcally,
if θy denotes the polar angle of y, and φy denotes its azimuthal angle, then
Y m
n (y) = (−1)m
s
2n + 1
4π
(n −m)!
(n + m)!P |m|
n
(cos θy)eimφy
(21)
(where i, when not used in a sub- or super-script, denotes the imaginary number, i2 = −1). Sometimes
(as in [2]) the normalization constant 2n+1
4π
is discarded. The associated Legendre functions P m
n are in turn
deﬁned by
P m
n (u) = (1 −u2)
m
2 ∂m
∂um Pn(u)
(22)
Note that for m = 0 we recover the Legendre polynomials, P 0
n(u) = Pn(u), and since the highest-order
term of Pn is un, for m > n we have P m
n
≡0. Although it might appear from their deﬁnition that m
should be positive, the associated Legendre functions for negative m, denoted P −m
n
, may be deﬁned by using
Rodrigues’ formula (15) as the deﬁnition of Pn; one may then show that
P −m
n
(u) = (−1)m (n −m)!
(n + m)!P m
n (u)
(23)
As before, it may be convenient (particularly numerically) to deﬁne the associated Legendre functions
(and thus the spherical harmonics) in terms of a recurrence relation. Since the associated Legendre functions
are doubly indexed, there are in fact a wide variety of recurrences to choose from [5]. A suﬃcient set is given
by the two formulae
(2n + 1)uP m
n (u) = (n + m)P m
n−1(u) + (n −m + 1)P m
n+1(u)
(24)
(2n + 1)(1 −u2)
1
2 P m
n (u) = P m+1
n+1 (u) + P m+1
n−1 (u)
(25)
and that P 0
0 (u) = P0(u) = 1.
Returning to our primary concern of creating a separable expansion for the kernel function, we see that
the expansion (19) is ﬁnally beginning to resemble the degenerate form (7); when the distance |y| is far
compared to the |xi| this series will converge quickly and we can hope to neglect all but a few terms. To do
so, we need only bound the error incurred by a ﬁnite truncation of the series. It can be shown [2] that if the
radii |xi| are bounded by some value |x| and we approximate f using p terms,
ˆfp(y) =
p−1
X
n=0
n
X
m=−n
M m
n
Y m
n (y)
|y|n+1
(26)
(with moments M m
n deﬁned as before) then we have that the residual error is bounded by
¯¯¯f(y) −ˆfp(y)
¯¯¯ ≤
Q
|y| −|x|
µ|x|
|y|
¶p
(27)
where Q = P
i |qi| is the total magnitude of charge located at the set of source points xi.
8

(a)
(b)
(c)
Figure 6: Equal-size hierarchical division of space — a binary tree (a) in one dimension, a quadtree (b) in
two dimensions, and an octree (c) in three dimensions.
Although the details of this derivation have been fairly complicated, the general picture should be quite
intuitive. Equation (26) is a Taylor series expansion of the function f which is nearly degenerate (has low
eﬀective rank, as expressed by (27)). Although the coeﬃcients themselves are less than intuitive, they may
be computed using relatively simple formulae and recurrence relations.
This approximation leads directly to an O(N log N) algorithm, similar in many ways to our previous
low-rank matrix approximation method but with the retained (eﬀective) rank determined analytically from
location constraints.
Note that (27) demonstrates the same tradeoﬀdiscussed earlier for block matrix
approximation. Speciﬁcally, precision of our approximation ˆf may be achieved in two ways — increasing
the number of terms retained (p), or increasing the minimum distances involved (reducing |x|
|y|). We will
consider this tradeoﬀfurther shortly. However, before enumerating the algorithm, we must describe some
data structure for grouping sets of points which we hope to approximate together.
5
Multi-resolution data structures
As was the case with the block matrix approximation of Section 3, we require some means of deciding which
points to group together.
The fact that our kernel function becomes increasingly smooth with distance
suggests that we should group points by spatial proximity. Similarly, we would like to create groupings at a
variety of scales, since we can accurately approximate large sets so long as they are suﬃciently far.
One simple way of creating such a hierarchical description of locale is given by successively dividing the
space in half along each dimension, giving a tree-structured sequence of increasingly ﬁne partitions. In one
dimension, this leads to a binary tree; in two dimensions, a quad-tree (see Figure 6). In three dimensions, it
becomes an octree (and in d dimensions, a tree with 2d children per node).
Of course, not all of these regions will be equally populated. There is clearly no reason to consider further
reﬁnement of an empty partition (a region of space in which no points are located); if we stop the reﬁnement
of each sub-tree when it contains one or no points we obtain an adaptive binary tree (and quadtree/octree
in higher dimensions) [6]. More generally, one may stop when the number is below some ﬁxed value N0.
Similar schemes for creating adaptive tree structures include [7, 8].
Another possibility for adaptive reﬁnement of space is the k-dimensional tree (kd-tree for short) [9].
This method again consists of a binary tree decomposition of space, but adaptively chooses both a single
dimension along which to divide the space and a location at which to do so, and keep track of bounding
boxes or spheres for the contained points.
Typically, this requires fewer spatial subdivisions to achieve
the same level of clustering than an spatially equally divided tree. However, the cost of constructing such
a tree is asymptotically O(N log N) (due to a sorting procedure on the points), which means the overall
algorithm cannot be less than this. In practice, the asymptotic regime where this cost dominates is well
beyond most reasonable data set sizes, but there has been relatively little work in adapting the fast multipole
approximations to make use of their advantages.
9

6
An O(N log N) algorithm
At this point, we have all the tools necessary to develop an O(N log N) algorithm for computing the in-
teraction potential f at all N locations. The basic idea is to subdivide our domain space using one of the
hierarchical descriptions of Section 5, and for all particles which are “suﬃciently far” from a given particle,
use the multipole expansion to summarize their total interaction.
For analysis of the (typical) binary/quad/octree case, a convenient deﬁnition of “suﬃciently far” is a
binary relationship between pairs of boxes at a particular level, and speciﬁed in terms of the length of each
box’s sides (denoted w). Speciﬁcally, suﬃcient distance is a statement about the ratio of |x| to |y|; for all
points xi in a given d-dimensional box with side length w we know that no point is further than
√
d·w/2 from
an origin placed at the box’s center (the diagonal distance to a corner). Furthermore, if the box containing
y is separated by s boxes (i.e. s = 0 if y is in a neighboring box, s = 1 in the next farther set, and so forth)
then |y| is at least (s + 1/2)w (the distance perpendicular to a box side). Any pair of boxes which are far
enough apart (more than s boxes) will be called “well separated”, while those which are not will be called
“near neighbors”.
We begin by describing the algorithm under the common assumption of uniformly distributed points,
then discuss relaxing this assumption. First, notice that the error bound (27) depends on the ratio of total
charge to box size:
Q
|y|−|x|. If |y| and |x| are both speciﬁed in terms of the box length w, this depends on the
density of charge within a given box. If the distribution of point charges is relatively uniform, this becomes
a constant and we may essentially ignore it.
Next we consider the number of terms required. This depends on the number of boxes we would like to
be able to summarize at a given level of the tree hierarchy. A typical choice is that only the box itself and
its immediate neighbors comprise the near neighbor set (so that well-separated boxes have s ≥1). In this
case (and for d = 3 dimensions), it is easy to see that |x|/|y| ≤
√
3
2 / 3
2 = 1/
√
3. Thus precision ϵ may be
achieved using p ∝−log ϵ/ log
√
3 (where the proportionality constant depends only on the charge density
Q/w). More generally, it will be
p ∝−log ϵ/ log(2s + 1
√
d
).
(28)
Typically this overestimates the number of terms required by about a factor of two; the required number
of terms for various accuracies cited in [2] seem to be chosen so as to remove this gap. However, for the
calculations in this paper we will conﬁne ourselves to the stated, bounding values of p.
The algorithm then operates as follows. We begin with a single box, the region of space in which all points
lie. This space is subdivided into 2d more boxes (e.g. 8 in three dimensions); for each box, we determine
which (if any) other boxes may be dealt with immediately using the multipole expansion. After only one
subdivision, no boxes are suﬃciently far, but after two subdivisions there will be several box pairs which
may be summarized. We then proceed recursively by subdividing each box; for each (new, smaller) box b1
we process any box b2 whose parent was not processed (the parents of b1 and b2 were not well-separated)
but which is now well-separated from b1 (see Figure 7). This recursion proceeds for approximately log2d(N)
steps, since at this point (by our assumption of uniformity) there will be approximately one point per box
and we may simply evaluate the remaining near neighbors directly.
The cost of this algorithm is approximately N log N: to evaluate the expansion of a single box b requires
O(p2Nb) operations, where Nb is the number of points inside the box, since we compute the triple sum
Pp
n=0
Pn
m=−n
P
i∈b Y m
n (xi).
Computing this for all boxes at a given level requires O(p2N) work; since
there are approximately log2d(N) levels, we have O(p2N log N). In fact however, we may do more detailed
accounting to evaluate the computational cost.
To estimate the constant in front, we may ask how many boxes which were near neighbors of a given box
become well-separated at the next level. Again, denoting by s the required separation in number of boxes, it
is easy to see that the number of near neighbors of a box at any level is approximately (2s+1)d. Then, these
boxes are subdivided into 2d(2s + 1)d subregions, of which (2s + 1)d are once again not well-separated from
a given subregion. At the ﬁnest level, we compute the (2s + 1)d interactions directly. Thus the algorithm
requires approximately
(2d −1)(2s + 1)dp2N log2d(N) + (2s + 1)dN
(29)
10

Figure 7: An illustration of the near-neighbors and well-separated boxes in two dimensions. From a given
box b’s perspective (black), there are (2s + 1)d = 9 near-neighbors for s = 1, shown in dark gray. At each
level, we compute the interaction terms from the children of the near neighbors of b’s parent which are not
near neighbors of b; these boxes are shown in light gray, and there are (2d −1)(2s + 1)d = 27 of them. The
boxes in white are well-separated from b’s parent box, and thus their contribution is calculated at a previous
scale.
operations. For d = 2 and s = 1 (the most commonly considered version) this becomes approximately
27p2N log N; for d = 3 it is ≈189p2N log N.
This analysis has been predicated on the fact that the distribution of points is approximately uniform.
If it is not, using an adaptive hierarchical structure (such as one which does not continue to reﬁne regions
which have fewer than some N0 points contained) becomes imperative. It also becomes much more diﬃcult to
analyze the resulting algorithmic performance. Although [10] demonstrates similar behavior to the improved
FMM developed in Section 7 for kd-tree structures, its proof of asymptotic performance appears ﬂawed. In
practice, however, the performance of adaptive data structures on non-uniform data is quite similar to the
above analysis.
6.1
How can this be improved?
In the Introduction, we promised an O(N) algorithm; however we have only yet seen an O(N log N) one.
How may the above algorithm be improved so as to require only O(N) computations?
The diﬀerence in performance can be seen as arising from the fact that each point must be visited at each
level of the tree structure, both to create a box’s series expansion and evaluate the expansion from each well
separated box. If we could instead visit only every box at each level, caching suﬃcient information about
the expansion, we would require only O(1 + 2d + 4d + . . . + N) = O(N) operations.
The dependence on the actual point locations enters in two places. The ﬁrst is in computing the source
expansion — naively, we compute moments for each box b by evaluating a sum Nb terms. We may avoid this
by designing a means of converting the ﬁner scale’s moment expansions to an expansion at the next coarser
scale. The second dependence is in evaluating at the target locations. Evaluating at a target location depends
on the angle from the expansion’s origin to the target. We would like to accumulate terms from several source
boxes and combine them so that they may be evaluated at the target only once. This requires two things —
a translation of several expansions’ origins to a common point (enabling them to be summed), typically the
target box center, and a translation of an expansion about the target box center to expansions about each
of its subregions (enabling expansions from diﬀerent levels to be combined easily). In the next section, we
present three theorems which enable this decoupling, and show that it results in an O(N) algorithm.
11

7
An O(N) algorithm
7.1
Local translation operations
As discussed, in order to develop an O(N) fast multipole algorithm, we require three things:
1. A means of combining ﬁne-scale multipole expansions into a single coarse-scale expansion. This will
be used to compute the multipole expansions of each box in the tree using a single ﬁne-to-coarse pass,
and takes the form of a translation from child box centers to parent box centers.
2. A means of combining several multipole expansions into a single expansion about a local origin in the
target box. This again takes the form of a translation, but this time between source boxes and target
boxes at the same scale.
3. A means of translating the target box’s local series expansion to an origin within each of the box’s
subdivisions at the next level of the tree. This will be used to combine the approximations at all levels
in a coarse-to-ﬁne pass.
Each of these is addressed with a corresponding theorem.
7.1.1
Translation of the multipole expansion
This step allows us to combine several multipole expansions about diﬀerent origins at one scale to a single
expansion at the next coarser scale (the boxes’ parent), by translating each of them to a common origin.
Suppose that we have a multipole expansion about the origin, and we shift the expansion to be located
about some point z, for example the center of the parent box (see Figure 8). Then, our original multipole
expansion looks like
ˆf =
∞
X
n=0
n
X
m=−n
M m
n
|y|n+1 Y m
n (y)
(30)
This may be expanded into the coarser scale form
ˆf +(y) =
∞
X
n=0
n
X
m=−n
˜
M m
n
|y −z|n+1 Y m
n (y −z)
(31)
where
˜
M m
n =
n
X
j=0
j
X
k=−j
M m−k
n−j
i|m|
i|k|i|m−k|
Ak
j Am−k
n−j
Am
n
|z|jY k
j (z)
(32)
where the constant Am
n is deﬁned by
Am
n =
(−1)n
p
(n −m)!(n + m)!
.
(33)
However, our error bounds are loosened slightly by the coarser expansion, from the original error bound
¯¯¯f(y) −ˆfp(y)
¯¯¯ ≤
Q
|y| −|x|
µ|x|
|y|
¶p
to
¯¯¯f(y) −ˆf +
p (y)
¯¯¯ ≤
Q
|y −z| −|x| −|z|
µ|x| + |z|
|y −z|
¶p
(34)
Note also that, since there are p2 moment terms ˜
M m
n , each of which is a linear function of the p2 original
moments M m
n , this operation takes about p4 operations.
12

y
y-z
z
|x|+|z|
|x|
Figure 8: A multipole expansion around the source points (small circle) is translated to an expansion about
the point z, in order to fuse several ﬁne-scale expansions into a single coarse-scale one.
y
z-y
z
|x|
|x|
Figure 9: A multipole expansion around the source points (left) is translated to a local expansion about the
point −z (in the original coordinate system), in order to fuse several multipole expansions into a single local
one.
7.1.2
Conversion to a local expansion
The next step is to convert each source box’s multipole expansion at a particular tree level into a single
expansion about the target box center. Given a multipole expansion of the form (30) which we intend to
evaluate at some point y, it may be converted to a local expansion about the point −z (see Figure 9). We
assume that |z| is large compared to the bounding sphere of radius |x| about the sources xi, speciﬁcally that
there exists some constant c > 1 such that |z| > (c + 1)|x|, and also that |z −y| < |x| (that y is contained
in an equivalent-sized bounding sphere about z).
This local expansion has the form
ˆf ∗(y) =
X
n
n
X
m=−n
Lm
n |z −y|nY m
n (z −y)
(35)
where
Lm
n =
∞
X
j=0
j
X
k=−j
M k
j
(−1)j
i|m−k|
i|k|i|m|
Ak
j Am
n
Ak−m
j−n
Y k−m
j+n (z)
|z|j+n+1
(36)
and Ak
j is deﬁned as before.
This is not quite the result we need — speciﬁcally, it does not tell us what additional error is introduced in
this transformation by the fact that our moments M k
j have been truncated (retaining p2 terms). According
to [2] it is “straightforward but tedious” to derive a precise bound on the error. One may think of this as
a slight accumulation of error; using only p2 moments causes slight errors in even the retained values of
Lm
n . Luckily only this step introduces additional error, since the upward and downward translations of the
multipole expansion are exact, and this step occurs only once for each contributing box.
Furthermore, ﬁnite truncation of this series to p ≥1 terms conforms to the error bound
¯¯¯f(y) −ˆf ∗
p (y)
¯¯¯ ≤
Q
(c −1)|x|
µ1
c
¶p
.
(37)
As with the multipole translation, the construction of a truncated local expansion requires p4 operations.
13

y
y-z
z
Figure 10: A local expansion create for the coarse scale region can be converted into local expansions at each
ﬁner scale child by translating the expansions’ origins to the centers of the child regions.
Notice that if this expression is used for converting expansions from well-separated boxes (with number
of separating boxes s ≥1 and box width w), then |z| ≥2w and |x| ≤
√
d · w/2, and thus we know that
c ≥
4
√
d
−1
(38)
Unfortunately this is a considerably weaker rate of convergence (in p) than we obtained for the original
N log N algorithm. For example, in d = 3 dimensions our original error bound was slightly faster than (.6)p,
but it is now about (.75)p. One possible solution [11] is to require further separation between approximated
boxes, but (as we shall see) this may be ineﬀective since it also impacts the number of boxes which must be
processed at each level.
7.1.3
Translation of a local expansion
Finally, we would like to shift the local expansion at a box center to a local expansion at the center of each
of its children (see Figure 10). Let a ﬁnite local expansion be given by
ˆf ∗
p (y) =
p−1
X
n=0
n
X
m=−n
Lm
n |y|nY m
n (y)
(39)
Then we may equivalently write
ˆf ∗
p (y) =
p−1
X
n=0
n
X
m=−n
˜Lm
n |y −z|nY m
n (y −z)
(40)
where
˜Lm
n =
p−1
X
j=n
j
X
k=−j
Lk
j
(−1)j+n
i|k|
i|k−m|i|m|
Ak−m
j−n Am
n
Ak
j
Y k−m
j−n (−z)|z|j−n
(41)
where Ak
j is deﬁned as before. This entails no additional approximation (merely a translation of origin), and
again requires p4 operations.
7.2
The (original) fast multipole algorithm
We are now ready to describe the original Fast Multipole Method (FMM). As outlined previously, this will
proceed as follows:
1. Form multipole expansions (moments) at the tree’s ﬁnest scale
2. Merge expansions to form the expansions at the next coarser scale until the coarsest level is reached
3. Starting at the coarsest level, for each (target) region convert the multipole expansions at any well-
separated (source) regions into a local expansion around the target center.
14

4. For each region, translate the local expansion to the center of each of its 2d children.
5. Go to (3) until the ﬁnest scale is reached; then evaluate the series expansion at each target location
and compute the remaining near neighbor interactions directly.
Once again, let us assume uniformity of the source and target points, and construct a tree hierarchy with
log2d(N/N0) levels (so that each ﬁnest-scale box has approximately N0 points each). This means that there
are approximately N/N0 total boxes in the hierarchy. Analyzing the costs of each of the above steps, we see
that the ﬁrst requires pN computations, the second and fourth each require at most p4(N/N0) (p4 for each
node in the tree), the third p4(2d −1)(2s + 1)d(N/N0), and the last (2s + 1)dNN0 + pN computations (the
remaining pairwise interactions plus the expansion’s sum at each target). Thus we have a total number of
computations given by
p4((2d −1)(2s + 1)d + 2)(N/N0) + 2pN + (2s + 1)dNN0
(42)
If, as in [2], we choose N0 ≈2p2 and take d = 3 dimensions and s = 1 as our neighborhood region, this
becomes ≈150p2N.
Notice that the increase in required number of terms p from Section 7.1.2 is unfortunate, as the “constant”
in front of our O(N) algorithm depends on p2. To give some intuition, for ϵ = 10−4 and unit charge density,
we need p = 17 terms for the O(N log N) algorithm but p = 32 terms for the O(N) one. However, the O(N)
version is almost always preferable, since for this example, we can compute the break-even point between
the two by 189(17)2N log8 N ≈150(32)2N, which is attained around N ≈400. In this regime the direct,
quadratic computation is actually more eﬃcient; we are primarily interested in much higher values of N.
Furthermore, consider increasing the required separation distance s to, say, s = 2 as proposed in [11].
Then for ϵ = 10−4 we requires only about p = 11 terms, but the constant in front increases, making our cost
689p2N. For this choice of ϵ this gives only about a factor of 2 improvement. In order to be practical for
the three-dimensional problems of interest, we require something more.
8
Improvements in three dimensions
Unfortunately, although the asymptotic performance is O(N), the high value of the constant factor means
that the standard FMM becomes much slower with increasing dimension. In particular, three dimensional
problems are particularly common, yet the breakpoint for FMM versus direct computation in three dimen-
sions is quite high. For example, if ϵ = 10−4 we have p ≈32 and the cost is ≈150 ∗p2N, so the break-even
point versus direct N 2 computation is around N=150,000.
One possible way to reduce this cost is by applying coordinate system rotations to translate the multipole
expansion in a source box to the center of a target box. Naively, the multipole to local translation cost p4
operations; by rotational methods it can be performed in only 3p3 operations. For details on how this may
be done, see e.g. [2]. However, this is still not a suﬃcient improvement to make the method truly practical,
and we omit details.
8.1
Exponential plane waves
Recent work by Greengard and Rokhlin [2] uses somewhat more complicated mathematical apparatus and
improves the FMM’s performance for reasonable values of N. The essential observation is that (in three
dimensions) the multipole expansion may be converted into six directional plane-wave expansions (one in
each cartesian direction, ±x, ±y, ±z). These plane-wave expansions can be translated very easily, and may
then be converted into local expansions as before. Although mathematically more diﬃcult, this results in a
computationally lower complexity procedure.
8.1.1
Integral expansion
We require a diﬀerent expansion for each of the six cartesian directions. For example, letting r be a vector,
the positive z direction can be derived using the integral identity [2]
1
|r| = (r2
x + r2
y + r2
z)−1
2 =
Z ∞
0
e−λrzJ0(λ
q
r2x + r2y)dλ
(43)
15

where J0 is a Bessel function of integral order zero. This Bessel function has a number of nice integral
representations [5], for example
J0(u) = 1
2π
Z 2π
0
cos(u sin α)dα
(44)
or
J(λ
q
r2x + r2y) = 1
2π
Z 2π
0
eiλ(rx cos α+ry sin α)dα
(45)
Clearly, these integrals may be estimated using two summations, such as
1
|r| ≈
X
k
wke−λkrzJ0(λk
q
r2x + r2y)
(46)
J0(λk
q
r2x + r2y) = 1
2π
X
j
ωjkeiλk(rx cos αjk+ry sin αjk)
(47)
For any ﬁnite number of terms in each sum, the problem of determining a good set of coeﬃcients wk, λk, ωjk, αjk
falls into the category of quadrature approximation.
8.1.2
Quadrature approximation
Quadrature is the well-studied ﬁeld of approximating the deﬁnite integral of an unknown function by a
weighted sum of n evaluations at pre-speciﬁed locations. While a full explanation of quadrature approxi-
mations goes beyond the scope of this document, we brieﬂy describe some of the relevant background for
applying quadrature methods to the double integral form (43).
Quadrature approximations [5] come in many forms. The most straightforward involve evaluating the
integrand at a number of equally-spaced locations.
Equally weighting these evaluations is often called
“trapezoidal” quadrature, since it approximates the integral by the area under a linear interpolation. The
error in this estimate is bounded by a constant factor times the second derivative of the integrand (as seems
intuitive for such an approximation).
By optimizing the n weights, one may create a better ﬁt and make the quadrature estimate exact for low-
degree polynomial functions. Even more sophisticated methods optimize both the weights and evaluation
locations in the sum, 2n parameters in all, selecting the best possible interpolating polynomial. Collectively,
these 2n parameter methods are known as Gaussian quadrature, and can be shown to be exact for all
polynomials up to degree 2n −1. Their estimate error is at most proportional to the 2nth derivative of the
integrand.
Of course, we have a double integration problem, and thus must worry about the interaction between
the two sources of error.
Luckily, the bounded nature of the Bessel function J0 means that our errors
cannot interfere too catastrophically. Speciﬁcally, if we have a function of two variables f(u, v) and both
approximations (individually) have errors bounded by ϵ:
Z
f(u, v)du −
nu
X
i=1
wu
i f(ui, v) < ϵ
Z
f(u, v)dt −
nv
X
i=1
wv
i f(u, vi) < ϵ
(48)
and at least one function’s evaluates have bounded absolute sum, e.g. Pn
i=1 |f(u, vi)| < η, then the total
error in the double quadrature approximation is bounded by
Z Z
f(u, v)dudv −
nu
X
i=1
nv
X
j=1
wu
i wv
j f(ui, vj) < ϵ(1 + η).
(49)
If we then choose the number of terms in our quadrature approximation to be high enough, we may
achieve any level of desired accuracy. We are free to choose the exact nature of this quadrature formula as
we please; for example, one might choose trapezoidal quadrature for the inner integral (44) (involving α),
16

since it is on a ﬁnite range; all weights are equal and locations equally spaced in [0, 2π]. One may easily
bound the second derivative of J0 to obtain the required number of terms.
As for the outer integral (43) (involving λ), an appealing choice is Gauss-Laguerre quadrature. The
locations are then given by the n roots of the nth Laguerre polynomial [5]
Ln(λ) =
n
X
s=0
(−1)n−s n!
s!
λn−s
(n −s)!(n −s)!
(50)
and weighting by wi where
wi =
λi
(n + 1)2 (Ln+1(λi))2 .
(51)
Again, the error is proportional to the 2nth derivative of the integrand.
Although we list these options as reasonable possibilities, they may not be the optimal choices. Indeed,
recently methods have been proposed by [12, 13] which numerically optimize the weights and locations for a
pre-speciﬁed class of possible functions in the integrand. This may reap considerable beneﬁts in the FMM,
since we know a considerable amount about each function.
Speciﬁcally, we know not only the analytic
structure of the function, but also about the possible range of its arguments (since in the FMM, the boxes
being considered at any point are neither extremely close nor far). In [13], a numerical search is proposed
and demonstrated by optimizing coeﬃcients to minimize potential error (whose maximum, they show, may
be evaluated for arguments in the pre-speciﬁed region). While this involves a procedurally complex search
which must be repeated if the desired tolerance level changes, it results in an extremely eﬃcient quadrature
formulation for the speciﬁc problem under consideration. Tables for several reasonable choices of ϵ are given
in [2]. Note, however, that because these estimates are obtained by assuming a particular argument range
(the values of which are appropriate for a unit-length cube), we will need to rescale each box before converting
it to a plane wave expansion, then rescale again after converting back to a local expansion.
Finally, the exact number of terms required in the inner integral’s trapezoidal approximation, denoted
mk, depends on the value of the particular λk in the outer integral’s quadrature sum (46). Given mk, of
course, the locations αjk of (47) are simply 2πj
mk and the weights ωjk ≡1.
The double integral expressing r−1 is thus approximated by the sum
s
X
k=1
mk
X
j=1
wk
mk
e−λkrze−iλk(rx cos αjk+ry sin αjk)
(52)
resulting in an overall approximation to f (in the +z direction)
ˆf +z =
s
X
k=1
mk
X
j=1
Wjke−λkrze−iλk(rx cos αjk+ry sin αjk)
(53)
where the Wjk will be deﬁned shortly in terms of the multipole expansion.
8.1.3
Exponential translation operations
At this point it might appear that we have merely created more work for ourselves; however, the plane wave
expansion has one extremely nice property — that translation (step 3 of the original FMM, Section 7.1.2)
becomes a diagonal operation and thus may be performed in only O(p2) time (rather than O(p4) for the
original algorithm or O(p3) by coordinate system rotation).
This requires three steps:
1. Conversion of a multipole expansion to (six) plane-wave expansions
2. Translation of a plane-wave expansion
3. Conversion of a plane-wave expansion to a local expansion
17

The ﬁrst is accomplished by expressing the spherical harmonics as derivatives of r−1; this gives the result
that the coeﬃcients of the exponential plane wave may be calculated by ﬁrst rescaling the box to unit volume
(if the box has side length b, we set M m
n ←M m
n /bn+1). We may then apply the transformation
Wjk = wk
mk
∞
X
n=0
n
X
m=−n
(−i)|m|eimαjM m
n (λk)n
p
(n −m)!(n + m)!
(54)
As was the case in (36) for the original algorithm, although this involves an inﬁnite sum its truncation to
the p2 terms typically retained by the multipole expansion introduces only a small additional error to the
computation.
The translation step (to any suﬃciently distant box in the +z direction) is quite simple: to translate the
coeﬃcients Wjk for an expansion at the origin to coeﬃcients W d
jk for an expansion centered at d = (dx, dy, dz)
we again rescale by the box size (d ←d/b), then simply apply the diagonal operation
W d
jk = Wjke−λkdze−iλk(dx cos αjk+dy sin αjk)
(55)
Finally, converting the plane wave expansions back into local expansion coeﬃcients Lm
n at the target box
can be done by the following transformation:
Lm
n =
(−i)|m|
p
(n −m)!(n + m)!
s
X
k=1
(−λk)n
mk
X
j=1
Wjkeimαj
(56)
We may then rescale back to our original box size b by setting Lm
n ←Lm
n bn.
In [2] it is asserted that s ≈p and that mk ≈p on average, and thus the total cost of performing the
multipole to plane wave and plane-wave to local conversions is approximately 3p3. Computationally, this
is a small additional price, and the translation to every well-separated region is correspondingly cheaper,
costing only p2(2d −1)(2s + 1)d = 189p2.
Choosing the minimum number of particles per box to be
N0 ≈2p, our operation count becomes ≈150pN + 5p2N. For ϵ ≈10−4 we have a break-even point of
N ≈150 ∗32 + 5 ∗(32)2 = 105, more than an order of magnitude better than the original FMM.
As it turns out, a few more tricks including close examination of symmetry relationships can speed things
up even further. However, we do not explore these options; for a short list of the possibilities see [2].
9
Gaussian kernel expansion (the fast Gauss transform)
While most of the previous discussion has been in terms of the kernel K(z) =
1
|z|, an almost identical
development may be performed for other kernel functions using their series expansions. For example, the
fast Gauss transform (FGT) [3] can be used for eﬃcient simulation of diﬀusion models and more generally
evaluation of Gaussian sum models. Using the same toolkit, it is relatively easy to derive. Suppose that our
kernel function is described by a Gaussian distribution,
K(z) =
1
(2πσ2)
d
2 e−|z|2
2σ2
(57)
We are aided considerably in this case by the fact that the Gaussian distribution (57) is separable in cartesian
coordinates, since if z is d-dimensional, z = (z1, . . . , zd), we have K(z) = Q
i Ki(zi). Then, a separable kernel
in d dimensions may be constructed by the product of separable kernels in each individual dimension:
K(y −x) =
d
Y
i=1
Ãp−1
X
k=0
Φk(xi)Ψk(yi)
!
=
p−1
X
k1=0
. . .
p−1
X
kd=0
Ã d
Y
i=1
Φki(xi)Ψki(yi)
!
Note, however, that for a given rank p the above sum has pd terms, exponential in the problem’s dimension.
Although we will see that the residual error decays very rapidly in p, this is still the main computational
18

H0(u) = 1
H1(u) = 2u
H2(u) = 4u2 −2
H3(u) = 8u3 −12u
H4(u) = 16u4 −48u2 + 12
Figure 11: The ﬁrst several Hermite polynomials Hn(u).
limitation of the fast Gauss transform for high dimension. Once again, exponential plane waves may be used
to speed up the process somewhat [3]; but here will we simply cover the original FGT.
Therefore we may consider only the one-dimensional case, for which (y −x)2 = y2 + x2 −2xy. In parallel
with Equation (12), deﬁne the Hermite polynomials Hn by their generating function
g(u, v) = e−v2+2uv =
∞
X
n=0
Hn(u)vn
n!
(58)
Then clearly the Gaussian kernel is given by u =
x
√
2σ2 , v =
y
√
2σ2 , and
K(y −x) = 1
Zσ
∞
X
n=0
Hn(u)e−u2 vn
n!
(59)
where Zσ is the Gaussian normalization constant. Again, it is usually more convenient to express the Hermite
polynomials in terms of a recurrence relation,
Hn+1(z) = 2zHn(z) −2nHn−1(z)
(60)
with H0(z) = 1, H1(z) = 2z, and so on; the ﬁrst several Hermite polynomials are listed in Figure 11.
Assuming the box size is suﬃciently small (≤
√
2σ), the series’ ﬁnite truncation error is bounded by [3]
¯¯¯¯¯f(y) −
p−1
X
k=0
Φi(y)Ψi(xi)
¯¯¯¯¯ ≤2.75d Q
µ 1
p!
¶d/2 µ1
2
¶(p+1)d/2
(61)
where (as previously) Q = P |qi| and d is the dimension. This error bound decreases much more rapidly
in p than the corresponding bound in the original FMM, but does not make use of the separation distance
between source and target boxes, perhaps indicating that even tighter bounds might be derived.
In principle, there is nothing to stop one from deriving similar algorithms for any other kernel functions
using a series expansion (if known); however, the FMM and FGT are each widely applicable and thus the
best studied. For another example, see e.g. [14], which extends the FMM to kernels of the form
K(y −x) = e−λ|y−x|
|y −x|
(62)
called “screened” Coulombic potentials.
10
Conclusions
We have given an overview of the fast multipole method and related algorithms, which are used for approxi-
mate computation for otherwise quadratic complexity problems. The key to these algorithms lies in creating
low-rank block approximations to the N × N matrix of interactions. This may be done directly, but not
without incurring the quadratic cost; thus it is only useful if the matrix is to be reused many times.
19

For many analytic interaction potentials, however, the function itself can be shown to have low eﬀective
rank under certain distance constraints. Exploiting these low-rank forms results in an O(N) algorithm,
where the constant depends on the desired accuracy.
Unfortunately, the size of this constant is quite important and may dominate even for the large data sets
of interest. Reducing its eﬀect can be quite involved. Directional exponential expansions can lower the cost
considerably, due to the simplicity of translating their series from source to destination regions (the main
computational bottleneck of the original FMM).
Although we have discussed the algorithms mainly in terms of the Coloumbic potential function r−1, the
same ideas may be applied to other potentials as well. Gaussian sums, for instance, may be evaluated using
the fast Gauss transform, which is developed from precisely the same principles. Other functions may be
readily used given a series expansion and a bound on its ﬁnite truncation error.
References
[1] Sharad Kapur and Jinsong Zhao. A fast method of moments solver for eﬃcient parameter extraction of MCMs. In Design
Automation Conference, pages 141–146, 1997.
[2] L. Greengard and V. Rokhlin. A new version of the fast multipole method for the laplace equation in three dimensions.
Acta Numerica 6, pages 229–269, 1997.
[3] L. Greengard and X. Sun.
A new version of the fast Gauss transform.
Documenta Mathematica, Extra Volume
ICM(III):575–584, 1998.
[4] G. Strang. Linear Algebra and its Application, 3rd. Ed. Academic Press, 1985.
[5] George Arfken. Mathematical Methods for Physicists. Academic Press, Boston, 3 edition, 1985.
[6] J. Barnes and P. Hut. A Hierarchical O(NlogN) Force Calculation Algorithm. Nature, 324:446–449, 1986.
[7] J. Carrier, L. Greengard, and V. Rokhlin. A fast adaptive multipole algorithm for particle simulations. SIAM Journal of
Scientiﬁc and Statistical Computing, 9(4), 1988. Yale University Technical Report, YALEU/DCS/RR-496 (1986).
[8] Leon van Dommelen and Elke A. Rundensteiner. Fast, adaptive summation of point forces in the two-dimensional poisson
equation. J. Comput. Phys., 83(1):126–147, 1989.
[9] J. L. Bentley. Multidimensional binary search trees used for associative searching. Communications of the ACM, 18, 1975.
[10] A. G. Gray and A. W. Moore. Very fast multivariate kernel density estimation via computational geometry. In Joint Stat.
Meeting, 2003.
[11] L. Greengard and V. Rokhlin.
The rapid evaluation of potential ﬁelds in three dimensions.
In C. Greengard (Eds.)
C. Anderson, editor, Vortex Methods, volume 1360 of Lecture Notes in Mathematics, pages 121–? Springer-Verlag, Berlin,
1988.
[12] J. Ma, V. Rokhlin, and S. Wandzura. Generalized gaussian quadrature rules for systems of arbitrary functions. SIAM J.
Numer. Anal., 33(3):971–996, 1996.
[13] N. Yarvin and V. Rokhlin. Generalized gaussian quadratures and singular value decompositions of integral operators.
SIAM J. Sci. Comput., 20(2):699–718, 1998.
[14] J. Huang and L. Greengard.
A new version of the fast multipole method for screened coulomb interactions in three
dimensions. J. Comput. Physics, 180(2):642–658, 2002.
20

