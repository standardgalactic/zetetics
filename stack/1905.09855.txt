Distributional Policy Optimization:
An Alternative Approach for Continuous Control
Chen Tessler∗, Guy Tennenholtz∗and Shie Mannor
∗Equal Contribution
chen.tessler@campus.technion.ac.il, guytenn@gmail.com, shie@ee.technion.ac.il
Technion Institute of Technology, Haifa, Israel
Abstract
We identify a fundamental problem in policy gradient-based methods in continu-
ous control. As policy gradient methods require the agent’s underlying probability
distribution, they limit policy representation to parametric distribution classes. We
show that optimizing over such sets results in local movement in the action space
and thus convergence to sub-optimal solutions. We suggest a novel distributional
framework, able to represent arbitrary distribution functions over the continuous
action space. Using this framework, we construct a generative scheme, trained us-
ing an off-policy actor-critic paradigm, which we call the Generative Actor Critic
(GAC). Compared to policy gradient methods, GAC does not require knowledge
of the underlying probability distribution, thereby overcoming these limitations.
Empirical evaluation shows that our approach is comparable and often surpasses
current state-of-the-art baselines in continuous domains.
1
Introduction
Model-free Reinforcement Learning (RL) is a learning paradigm which aims to maximize a cumu-
lative reward signal based on experience gathered through interaction with an environment [Sutton
and Barto, 1998]. It is divided into two primary categories. Value-based approaches involve learning
the value of each action and acting greedily with respect to it (i.e., selecting the action with highest
value). On the other hand, policy-based approaches (the focus of this work) learn the policy directly,
thereby explicitly learning a mapping from state to action.
Policy gradients (PGs) [Sutton et al., 2000b] have been the go-to approach for learning policies
in empirical applications. The combination of the policy gradient with recent advances in deep
learning has enabled the application of RL in complex and challenging environments. Such domains
include continuous control problems, in which an agent controls complex robotic machines both in
simulation [Schulman et al., 2015, Haarnoja et al., 2017, Peng et al., 2018] as well as real life
[Levine et al., 2016, Andrychowicz et al., 2018, Riedmiller et al., 2018]. Nevertheless, there exists a
fundamental problem when PG methods are applied to continuous control regimes. As the gradients
require knowledge of the probability of the performed action P(a | s), the PG is empirically limited
to parametric distribution functions. Common parametric distributions used in the literature include
the Gaussian [Schulman et al., 2015, 2017], Beta [Chou et al., 2017] and Delta [Silver et al., 2014,
Lillicrap et al., 2015, Fujimoto et al., 2018] distribution functions.
In this work, we show that while the PG is properly deﬁned over parametric distribution functions,
it is prone to converge to sub-optimal exterma (Section 3). The leading reason is that these distri-
butions are not convex in the distribution space1 and are thus limited to local improvement in the
1As an example, consider the Gaussian distribution, which is known to be non-convex.
33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.
arXiv:1905.09855v2  [cs.LG]  25 Nov 2019

action space itself. Inspired by Approximate Policy Iteration schemes, for which convergence guar-
antees exist [Puterman and Brumelle, 1979], we introduce the Distributional Policy Optimization
(DPO) framework in which an agent’s policy evolves towards a distribution over improving actions.
This framework requires the ability to minimize a distance (loss function) which is deﬁned over two
distributions, as opposed to the policy gradient approach which requires an explicit differentiation
through the density function.
DPO establishes the building blocks for our generative algorithm, the Generative Actor Critic2. It is
composed of three elements: a generative model which represents the policy, a value, and a critic.
The value and the critic are combined to obtain the advantage of each action. A target distribution is
then deﬁned as one which improves the value (i.e., all actions with negative advantage receive zero
probability mass). The generative model is optimized directly from samples without the explicit
deﬁnition of the underlying probability distribution using quantile regression and Autoregressive
Implicit Quantile Networks (see Section 4). Generative Actor Critic is evaluated on tasks in the
MuJoCo control suite (Section 5), showing promising results on several difﬁcult baselines.
2
Preliminaries
We consider an inﬁnite-horizon discounted Markov Decision Process (MDP) with a continuous
action space. An MDP is deﬁned as the 5-tuple (S, A, P, r, γ) [Puterman, 1994], where S is a
countable state space, A the continuous action space, P : S × S × A 7→[0, 1] is a transition kernel,
r : S × A →[0, 1] is a reward function, and γ ∈(0, 1) is the discount factor. Let π : S 7→B(A) be
a stationary policy, where B(A) is the set of probability measures on the Borel sets of A. We denote
by Π the set of stationary stochastic policies. In addition to Π, often one is interested in optimizing
over a set of parametric distributions. We denote the set of possible distribution parameters by Θ
(e.g., the mean µ and variance σ of a Gaussian distribution).
Two measures of interest in RL are the value and action-value functions vπ ∈R|S| and
Qπ ∈R|S|×|A|, respectively. The value of a policy π, starting at state s and performing action a
is deﬁned by Qπ(s, a) = Eπ [P∞
t=0 γtr(st, at) | s0 = s, a0 = a]. The value function is then de-
ﬁned by vπ = Eπ[Qπ(s, a)]. Given the action-value and value functions, the advantage of an action
a ∈A at state s ∈S is deﬁned by Aπ(s, a) = Qπ(s, a) −vπ(s). The optimal policy is deﬁned by
π∗= arg maxπ∈Π vπ and the optimal value by v∗= vπ∗.
3
From Policy Gradient to Distributional Policy Optimization
Current practical approaches leverage the Policy Gradient Theorem [Sutton et al., 2000b] in order
to optimize a policy, which updates the policy parameters according to
θk+1 = θk + αkEs∼d(πθk)Ea∼πθk (·| s)∇θ log πθ(a | s) |θ=θk Qπθk (s, a) ,
(1)
where d (π) is the stationary distribution of states under π. Since this update rule requires knowl-
edge of the log probability of each action under the current policy log πθ(a | s), empirical methods
in continuous control resort to parametric distribution functions. Most commonly used are the Gaus-
sian [Schulman et al., 2017], Beta [Chou et al., 2017] and deterministic Delta [Lillicrap et al., 2015]
distribution functions. However, as we show in Proposition 1, this approach is not ensured to con-
verge, even though there exists an optimal policy which is deterministic (i.e., Delta) - a policy which
is contained within this set.
The sub-optimality of uni-modal policies such as Gaussian or Delta distributions does not occur due
to the limitation induced by their parametrization (e.g., the neural network), but is rather a result of
the predeﬁned set of policies. As an example, consider the set of Delta distributions. As illustrated
in Figure 1, while this set is convex in the parameter µ (the mean of the distribution), it is not convex
in the set Π. This is due to the fact that (1−α)δµ1 +αδµ2 results in a stochastic distribution over two
supports, which cannot be represented using a single Delta function. Parametric distributions such
as Gaussian and Delta functions highlight this issue, as the policy gradient considers the gradient
w.r.t. the parameters µ, σ. This results in local movement in the action space. Clearly such an
approach can only guarantee convergence to a locally optimal solution and not a global one.
2Code provided in the following anonymous repository: github.com/tesslerc/GAC
2

(a) Policy vs. Parameter Space
(b) Delta
(c) Gaussian
Figure 1: (a): A conceptual diagram comparing policy optimization in parameter space Θ (black
dots) in contrast to distribution space Π (white dots). Plots depict Q values in both spaces. As
parameterized policies are non-convex in the distribution space, they are prone to converge to a
local optima. Considering the entire policy space ensures convergence to the global optima. (b,c):
Policy evolution of Delta and Gaussian parameterized policies for multi-modal problems.
Proposition 1. For any initial Gaussian policy π0 ∼N(µ0, Σ) and L ∈[0, v∗
2 ) there exists an
MDP M such that π∞satisﬁes
∥v∗−vπ∞∥∞> L ,
(2)
where π∞is the convergent result of a PG method with step size bounded by α. Moreover, given M
the result follows even when µ0 is only known to lie in some ball of radius R around ˜µ0, BR(˜µ0).
Proof sketch. For brevity we prove for the case of a ∈R, such that BR is a ﬁnite interval [a, b].
We also assume [a, b] ⊆[µ0 −2α, µ0 + 2α], and σ →0. The general case proof can be found in
the supplementary material. Let ϵ > 0. We consider a single state MDP (i.e., x-armed bandit) with
action space A = R and a multi-modal reward function (similar to the illustration in Figure 1b),
deﬁned by
r(a) =
cos
2π
8α(a −µ0)
 (ϵWµ0−2α,µ0+2α + (1 −ϵ)Wµ0+2α,µ0+6α) ,
where Wx,y(z) =
1
z ∈[x, y]
0
else
is the window function.
In PG, we assume µ is parameterized by some parameters θ.
Without loss of general-
ity, let us consider the derivative with respect to θ
=
µ.
At iteration k the deriva-
tive can be written as
d
dµ log πµ(a) |µ=µk= −1
2σ2 (µk −a) . PG will thus update the pol-
icy parameter µ by µk+1 = µk + αk

Ea∼N(µk,σ)
1
2σ2 (a −µk) r(a)
	
. As σ
→
0, it holds
that sign

Ea∼N(µk,σ) (a −µk) r(a)
	
= sign
 d
d ar(a) |a=µk
	
. It follows that if ϵ
<
1
3 and
µk ∈[µ0 −2α, µ0 + 2α] then so is µk+1. Then, µ∞∈[µ0 −2α, µ0 + 2α]. That is, the pol-
icy can never reach the interval [µ0 + 2α, µ0 + 6α] in which the optimal solution lies. Hence,
∥v∗−vπ∞∥∞= 1 −2ϵ and the result follows for ϵ < 1
3.
3.1
Distributional Policy Optimization (DPO)
In order to overcome issues present in parametric distribution functions, we consider an alterna-
tive approach. In our solution, the policy does not evolve based on the gradient w.r.t. distribution
parameters (e.g., µ, σ), but rather updates the policy distribution according to
πk+1 = Γ (πk −αk∇πd(Dπk
Iπk , π) |π=πk) ,
where Γ is a projection operator onto the set of distributions, d : Π × Π →[0, ∞) is a dis-
tance measure (e.g., Wasserstein distance), and Dπ
Iπ(s) is a distribution deﬁned over the support
Iπ(s) = {a : Aπ(s, a) > 0} (i.e., the positive advantage). Table 1 provides examples of such distri-
butions.
3

Algorithm 1 Distributional Policy Optimization (DPO)
1: Input: learning rates αk ≫βk ≫δk
2: πk+1 = Γ

πk −αk∇πd(Dπ′
k
Iπ′
k , π) |π=πk

3: Qπ′
k+1(s, a) = Qπ′
k (s, a) + βk

r(s, a) + γvπ′
k (s) −Qπ′
k (s, a)

4: vπ′
k+1(s) = vπ′
k + βk
R
A

Qπ′
k (s, a) −vπ′
k (s)

5: π′
k+1 = π′
k + δk(πk −π′
k)
Table 1: Examples of target distributions over the set of improving actions
Argmax
Dπ
Iπ(s)(a | s) = δarg maxa∈I(π) Aπ(s,a)(a | s)
Linear
Dπ
Iπ(s)(a | s) = 1{a∈Iπ}
Aπ(s,a)
R
Iπ(s) Aπ(s,a′)d a′
Boltzmann (β > 0)
Dπ
Iπ(s)(a | s) = 1{a∈Iπ}
exp( 1
β Aπ(s,a))
R
Iπ(s) exp( 1
β Aπ(s,a′))d a′
Uniform
Dπ
Iπ(s)(a | s) = Uniform(Iπ(s))
Algorithm 1 describes the Distributional Policy Optimization (DPO) framework as a three time-
scale approach to learning the policy. It can be shown, under standard stochastic approximation
assumptions [Borkar, 2009, Konda and Tsitsiklis, 2000, Bhatnagar and Lakshmanan, 2012, Chow
et al., 2017], to converge to an optimal solution. DPO consists of 4 elements: (1) A policy π on a fast
timescale, (2) a delayed policy π′ on a slow timescale, (3) a value and (4) a critic, which estimate
the quality of the delayed policy π′ on an intermediate timescale. Unlike the PG approach, DPO
does not require access to the underlying p.d.f. In addition, π which is updated on the fast timescale
views the delayed policy π′, the value and critic as quasi-static, and as such it can be optimized using
supervised learning techniques3. Finally, we note that in DPO, the target distribution Dπ′
Iπ′ induces
a higher value than the current policy π′, ensuring an always improving policy.
The concept of policy evolution using positive advantage is depicted in Figure 2. While the policy
starts as a uni-modal distribution, it is not restricted to this subset of policies. As the policy evolves,
less actions have positive advantage, and the process converges to an optimal solution. In the next
section we construct a practical algorithm under the DPO framework using a generative actor.
4
Method
In this section we present our method, the Generative Actor Critic, which learns a policy based on
the Distributional Policy Optimization framework (Section 3). Distributional Policy Optimization
requires a model which is both capable of representing arbitrarily complex distributions and can be
optimized by minimizing a distributional distance. We consider the Autoregressive Implicit Quantile
Network [Ostrovski et al., 2018], which is detailed below.
4.1
Quantile Regression & Autoregressive Implicit Quantile Networks
As seen in Algorithm 1, DPO requires the ability to minimize a distance between two distributions.
The Implicit Quantile Network (IQN) [Dabney et al., 2018a] provides such an approach using the
Wasserstein metric. The IQN receives a quantile value τ ∈[0, 1] and is tasked at returning the
value of the corresponding quantile from a target distribution. As the IQN learns to predict the
value of the quantile, it allows one to sample from the underlying distribution (i.e., by sampling
τ ∼U([0, 1]) and performing a forward pass). Learning such a model requires the ability to estimate
the quantiles. The quantile regression loss [Koenker and Hallock, 2001] provides this ability. It is
given by ρτ(u) = (τ −1{u ≤0})u, where τ ∈[0, 1] is the quantile and u the error.
3Assuming the target distribution is ’ﬁxed’, the policy π can be trained using a supervised learning loss,
e.g., GAN, VAE or AIQN.
4

(a) π0
(b) π1
(c) π2
(d) πk
Figure 2: Policy evolution of a general, non-parametric policy, where the target policy is a distribu-
tion over the actions with positive advantage. The horizontal dashed line denotes the current value
of the policy, the colored green region denotes the target distribution (i.e., the actions with a positive
advantage) and πk denotes the policy after multiple updates. As opposed to Delta and Gaussian
distributions, the ﬁxed point of this approach is the optimal policy.
Nevertheless, the IQN is only capable of coping with univariate (scalar) distribution functions. Os-
trovski et al. [2018] proposed to extend the IQN to the multi-variate case using quantile autore-
gression [Koenker and Xiao, 2006]. Let X = (X1, . . . , Xk) be an n-dimensional random variable.
Given a ﬁxed ordering of the n dimensions, the c.d.f. can be written as the product of conditional
likelihoods FX(x) = P
 X1 ≤x1, . . . , Xn ≤xn
= Πn
i=1FXi|Xi−1,...,X1(xi) . The Autoregres-
sive Implicit Quantile Network (AIQN), receives an i.i.d. vector τ ∼U([0, 1]n). The network
architecture then ensures each output dimension xi is conditioned on the previously generated val-
ues x1, . . . , xi−1; trained by minimizing the quantile regression loss.
4.2
Generative Actor Critic (GAC)
Next, we introduce a practical implementation of the DPO framework. As shown in Section 3,
DPO is composed of 4 elements: an actor, a delayed actor, a value, and an action-value estimator.
The Generative Actor Critic (GAC) uses a generative actor trained using an AIQN, as described
below. Contrary to parametric distribution functions, a generative neural network acts as a universal
function approximator, enabling us to represent arbitrarily complex distributions, as corollary of the
following lemma.
Lemma (Kernels and Randomization [Kallenberg, 2006]). Let π be a probability kernel from a mea-
surable space S to a Borel space A. Then there exists some measurable function f : S × [0, 1] →A
such that if θ is U(0, 1), then f(s, θ) has distribution π(a | s) for every s ∈S.
Actor: DPO deﬁnes the actor as one which is capable of representing arbitrarily complex policies.
To obtain this we construct a generative neural network, an AIQN. The AIQN learns a mapping
from a sampled noise vector τ ∼U([0, 1]n) to a target distribution.
As illustrated in Figure 3, the actor network contains a recurrent cell which enables sequential gen-
eration of the action. This generation schematic ensures the autoregressive nature of the model.
Each generated action dimension is conditioned only on the current sampled noise scalar τ i and the
previous action dimensions ai−1, . . . , a1. In order to train the generative actor, the AIQN requires
the ability to produce samples from the target distribution Dπ′
Iπ′. Although we are unable to sample
from this distribution, given an action, we are able to estimate its probability. An unbiased estima-
tor of the loss can be attained by uniformly sampling actions and then multiplying them by their
corresponding weight. More speciﬁcally, the weighted autoregressive quantile loss is deﬁned by
X
aj∼U(A)
Dπ′
Iπ′(aj | s)
n
X
i=1
ρk
τ i
j (ai
j −πφ(τ i
j| ai−1
j
, . . . , a1
j)) ,
(3)
where ai
j is the ith coordinate of action aj, and ρk
τ i
j is the Huber quantile loss [Huber, 1992, Dabney
et al., 2018b]. Estimation of Iπ′ in the target distribution is obtained using the estimated advantage.
5

Figure 3: Illustration of the actor’s
architecture.
⊗is the hadamard
product, ⊕a concatenation opera-
tor, and ψ a mapping [0, 1] 7→Rd.
Delayed Actor: The delayed actor, also known as Polyak aver-
aging [Polyak, 1990], is an appealing requirement as it is com-
mon in off-policy actor-critic schemes [Lillicrap et al., 2015].
The delayed actor is an additional AIQN πθ′, which tracks πθ.
It is updated based on θ′
k+1 = (1−α)θ′
k +αθk and is used for
training the value and critic networks.
Value and Action-Value: While it is possible to train a critic
and use its empirical mean w.r.t. the policy as a value esti-
mate, we found it to be noisy, resulting in bad convergence.
We therefore train a value network to estimate the expectation
of the critic w.r.t. the delayed policy. In addition, as suggested
in Fujimoto et al. [2018], we train two critic networks in par-
allel. During both policy and value updates, we refer to the
minimal value of the two critics. We observed that this indeed
reduced variance and improved overall performance.
To summarize, GAC combines 4 elements. The delayed actor
tracks the actor using a Polyak averaging scheme. The value
and critic networks estimate the performance of the delayed
actor. Provided Q and v estimations, we are able to estimate
the advantage of each action and thus propose the weighted
autoregressive quantile loss, used to train the actor network.
We refer the reader to the supplementary material for an exhaustive overview of the algorithm and
architectural details.
5
Experiments
In order to evaluate our approach, we test GAC on a variety of continuous control tasks in the
MuJoCo control suite [Todorov et al., 2012]. The agents are composed of n joints: from 2 joints
in the simplistic Swimmer task and up to 17 in the Humanoid robot task. The state is a vector
representation of the agent, containing the spatial location and angular velocity of each element.
The action is a continuous n dimensional vector, representing how much torque to apply to each
joint. The task in these domains is to move forward as much as possible within a given time-limit.
We run each task for 1 million steps and, as GAC is an off-poicy approach, evaluate the policy
every 5000 steps and report the average over 10 evaluations. We train GAC using a batch size
of 128 and uncorrelated Gaussian noise for exploration. Results are depicted in Figure 4. Each
curve presented is a product of 5 training procedures with a randomly sampled seed. In addition to
our raw results, we compare to the relevant baselines4, including: (1) DDPG [Lillicrap et al., 2015],
(2) TD3 [Fujimoto et al., 2018], an off-policy actor critic approach which represents the policy using
a deterministic delta distribution, and (3) PPO [Schulman et al., 2017], an on-policy method which
represents the policy using a Gaussian distribution.
As we have shown in the previous sections, DPO and GAC only require some target distribution to
be deﬁned, namely, a distribution over actions with positive advantage. In our results we present
two such distributions: the linear and Boltzmann distributions (see Table 1). We also test a non-
autoregressive version of our model 5 using an IQN. For completeness, we provide additional dis-
cussion regarding the various parameters and how they performed, in addition to a pseudo-code
illustration of our approach, in the supplementary material.
Comparison to the policy gradient baselines: Results in Figure 4 show the ability of GAC to
solve complex, high dimensional problems. GAC attains competitive results across all domains,
often outperforming the baseline policy gradient algorithms and exhibiting lower variance. This is
somewhat surprising, as GAC is a vanila algorithm, it is not supported by numerous improvements
apparent in recent PG methods. In addition to these results, we provide numerical results in the
supplementary material, which emphasize this claim.
4We use the implementations of DDPG and PPO from the OpenAI baselines repo [Dhariwal et al., 2017],
and TD3 [Fujimoto et al., 2018] from the authors GitHub repository.
5Theoretically, the dimensions of the actions may be correlated and thus should be represented using an
auto-regressive model.
6

Figure 4: Training curves on continuous control benchmarks. For the Generative Actor Critic ap-
proach we present both the Autoregressive and Non-autoregressive approaches, the exact hyperpa-
rameters for each domain are provided in the appendix.
Table 2: Relative best GAC results compared to the best policy gradient baseline
Environment
Humanoid-v2
Walker2d-v2
Hopper-v2
HalfCheetah-v2
Ant-v2
Swimmer-v2
Relative Result
+3447 (+595%)
+533 (+14%)
+467 (+17%)
−381 (−4%)
−444 (−8%)
+107 (+81%)
Parameter Comparison: Below we discuss how various parameters affect the behavior of GAC in
terms of convergence rates and overall performance:
1. At each step, the target policy is approximated through samples using the weighted quan-
tile loss (Equation (3)). The results presented in Figure 4 are obtained using 32 (256 for
HalfCheetah and Walker) samples at each step. 32 (128) samples are taken uniformly over
the action space and 32 (128) from the delayed policy π′ (a form of combining exploration
and exploitation). Ablation tests showed that increasing the number of samples improved
stability and overall performance. Moreover, we observed that the combination of both
sampling methods is crucial for success.
2. Not presented is the Uniform distribution, which did not work well. We believe this is due
to the fact that the Uniform target provides an equal weight to actions which are very good
while also to those which barely improve the value.
3. We observed that in most tasks, similar to the observations of Korenkevych et al. [2019],
the AIQN model outperforms the IQN (non-autoregressive) one.
6
Related Work
Distributional RL: Recent interest in distributional methods for RL has grown with the introduction
of deep RL approaches for learning the distribution of the return. Bellemare et al. [2017] presented
the C51-DQN which partitions the possible values [−vmax, vmax] into a ﬁxed number of bins and
estimates the p.d.f. of the return over this discrete set. Dabney et al. [2017] extended this work by
representing the c.d.f. using a ﬁxed number of quantiles. Finally, Dabney et al. [2018a] extended the
QR-DQN to represent the entire distribution using the Implicit Quantile Network (IQN). In addition
to the empirical line of work, Qu et al. [2018] and Rowland et al. [2018] have provided fundamental
theoretical results for this framework.
Generative Modeling: Generative Adversarial Networks (GANs) [Goodfellow et al., 2014] com-
bine two neural networks in a game-theoretic approach which attempt to ﬁnd a Nash Equilbirium.
This equilibrium is found when the generative model is capable of “fooling” the discriminator (i.e.,
the discriminator is no longer capable of distinguishing between samples produced from the real
distribution and those from the generator). Multiple GAN models and training methods have been
introduced, including the Wasserstein-GAN [Arjovsky et al., 2017] which minimizes the Wasser-
stein loss. However, as the optimization scheme is highly non-convex, these approaches are not
proven to converge and may thus suffer from instability and mode collapse [Salimans et al., 2016].
7

Policy Learning: Learning a policy is generally performed using one of two methods. The Policy
Gradient (PG) [Williams, 1992, Sutton et al., 2000a] deﬁnes the gradient as the direction which
maximizes the reward under the assumed policy parametrization class. Although there have been a
multitude of improvements, including the ability to cope with deterministic policies [Silver et al.,
2014, Lillicrap et al., 2015], stabilize learning through trust region updates [Schulman et al., 2015,
2017] and bayesian approaches [Ghavamzadeh et al., 2016], these methods are bounded to para-
metric distribution sets (as the gradient is w.r.t. the log probability of the action). An alternative
line of work formulates the problem as a maximum entropy [Haarnoja et al., 2018], this enables the
deﬁnition of the target policy using an energy functional. However, training is performed via mini-
mizing the KL-divergence. The need to know the KL-divergence limits practical implementation to
parametric distributions functions, similar to PG methods.
7
Discussion and Future Work
In this work we presented limitations inherent to empirical Policy Gradient (PG) approaches in
continuous control. While current PG methods in continuous control are computationally efﬁcient,
they are not ensured to converge to a global extrema. As the policy gradient is deﬁned w.r.t. the log
probability of the policy, the gradient results in local changes in the action space (e.g., changing the
mean and variance of a Gaussian policy). These limitations do not occur in discrete action spaces.
In order to ensure better asymptotic results, it is often needed to use methods that are more complex
and computationally demanding (i.e., “No Free Lunch” [Wolpert et al., 1997]). Existing approaches
attempting to mitigate these issues, either enrich the policy space using mixture models, or discretize
the action space. However, while the discretization scheme is appealing, there is a clear trade-off
between optimality and efﬁciency. While ﬁner discretization improves guarantees, the complexity
(number of discrete actions) grows exponentially in the action dimension [Tang and Agrawal, 2019].
Similar to the limitations inherent in PG approaches, these limitations also exist when considering
mixture models, such as Gaussian Mixtures. A mixture model of k-Gaussians provides a categorical
distribution over k Gaussian distributions. The policy gradient w.r.t. these parameters, similarly to
the single Gaussian model, directly controls the mean µ and variance σ of each Gaussian indepen-
dently. As such, even a mixture model is conﬁned to local improvement in the action space.
In practical scenarios, and as the number of Gaussians grows, it is likely that the modes of the mix-
ture would be located in a vicinity of a global optima. A Gaussian Mixture model may therefore
be able to cope with various non-convex continuous control problems. Nevertheless, we note that
Gaussian Mixture models, unlike a single Gaussian, are numerically unstable. Due to the summation
over Gaussians, the log probability of a mixture of Gaussians does not result in a linear representa-
tion. This can cause numerical instability, and thus hinder the learning process. These insights lead
us to question the optimality of current PG approaches in continuous control, suggesting that, al-
though these approaches are well understood, there is room for research into alternative policy-based
approaches.
In this paper we suggested the Distributional Policy Optimization (DPO) framework and its empir-
ical implementation - the Generative Actor Critic (GAC). We evaluated GAC on a series of con-
tinuous control tasks under the MuJoCo control suite. When considering overall performance, we
observed that despite the algorithmic maturity of PG methods, GAC attains competitive performance
and often outperforms the various baselines. Nevertheless, as noted above, there is “no free lunch”.
While GAC remains as sample efﬁcient as the current PG methods (in terms of the batch size during
training and number of environment interactions), it suffers from high computational complexity.
Finally, the elementary framework presented in this paper can be extended in various future research
directions. First, improving the computational efﬁciency is a top priority for GAC to achieve de-
ployment in real robotic agents. In addition, as the target distribution is deﬁned w.r.t. the advantage
function, future work may consider integrating uncertainty estimates in order to improve exploration.
Moreover, PG methods have been thoroughly researched and many of their improvements, such as
trust region optimization [Schulman et al., 2015], can be adapted to the DPO framework. Finally,
DPO and GAC can be readily applied to other well-known frameworks such as the Soft-Actor-Critic
[Haarnoja et al., 2018], in which entropy of the policy is encouraged through an augmented reward
function. We believe this work is a ﬁrst step towards a principal alternative for RL in continuous
action space domains.
8

8
Acknowledgement
We thank Yonathan Efroni for his fruitful comments that greatly improved this paper.
References
Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pa-
chocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous
in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018.
Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, pages 214–223, 2017.
Marc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pages 449–458. JMLR. org, 2017.
Shalabh Bhatnagar and K Lakshmanan. An online actor–critic algorithm with function approxima-
tion for constrained markov decision processes. Journal of Optimization Theory and Applications,
153(3):688–708, 2012.
Vivek S Borkar. Stochastic approximation: a dynamical systems viewpoint, volume 48. Springer,
2009.
Po-Wei Chou, Daniel Maturana, and Sebastian Scherer. Improving stochastic policy gradients in
continuous control with deep reinforcement learning using the beta distribution. In Proceedings
of the 34th International Conference on Machine Learning-Volume 70, pages 834–843. JMLR.
org, 2017.
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained re-
inforcement learning with percentile risk criteria. The Journal of Machine Learning Research, 18
(1):6070–6120, 2017.
Will Dabney, Mark Rowland, Marc G Bellemare, and R´emi Munos. Distributional reinforcement
learning with quantile regression. arXiv preprint arXiv:1710.10044, 2017.
Will Dabney, Georg Ostrovski, David Silver, and R´emi Munos.
Implicit quantile networks for
distributional reinforcement learning. arXiv preprint arXiv:1806.06923, 2018a.
Will Dabney, Mark Rowland, Marc G Bellemare, and R´emi Munos. Distributional reinforcement
learning with quantile regression. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence,
2018b.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov.
Openai baselines.
https:
//github.com/openai/baselines, 2017.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Mohammad Ghavamzadeh, Yaakov Engel, and Michal Valko. Bayesian policy gradient and actor-
critic algorithms. The Journal of Machine Learning Research, 17(1):2319–2371, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pages 2672–2680, 2014.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pages 1352–1361. JMLR. org, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, pages 1856–1865, 2018.
9

Peter J Huber. Robust estimation of a location parameter. In Breakthroughs in statistics, pages
492–518. Springer, 1992.
Olav Kallenberg. Foundations of modern probability. Springer Science & Business Media, 2006.
Roger Koenker and Kevin Hallock. Quantile regression: An introduction. Journal of Economic
Perspectives, 15(4):43–56, 2001.
Roger Koenker and Zhijie Xiao. Quantile autoregression. Journal of the American Statistical Asso-
ciation, 101(475):980–990, 2006.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pages 1008–1014, 2000.
Dmytro Korenkevych, A Rupam Mahmood, Gautham Vasan, and James Bergstra. Autoregressive
policies for continuous control deep reinforcement learning. arXiv preprint arXiv:1903.11524,
2019.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. The Journal of Machine Learning Research, 17(1):1334–1373, 2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Georg Ostrovski, Will Dabney, and R´emi Munos. Autoregressive quantile networks for generative
modeling. arXiv preprint arXiv:1806.05575, 2018.
Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne.
Deepmimic:
Example-guided deep reinforcement learning of physics-based character skills. arXiv preprint
arXiv:1804.02717, 2018.
Boris T Polyak. New stochastic approximation type procedures. Automat. i Telemekh, 7(98-107):2,
1990.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 1994.
Martin L Puterman and Shelby L Brumelle. On the convergence of policy iteration in stationary
dynamic programming. Mathematics of Operations Research, 4(1):60–69, 1979.
Chao Qu, Shie Mannor, and Huan Xu. Nonlinear distributional gradient temporal-difference learn-
ing. arXiv preprint arXiv:1805.07732, 2018.
Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Wiele,
Vlad Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playing solving sparse
reward tasks from scratch. In International Conference on Machine Learning, pages 4341–4350,
2018.
Mark Rowland, Marc G Bellemare, Will Dabney, R´emi Munos, and Yee Whye Teh. An analysis of
categorical distributional reinforcement learning. arXiv preprint arXiv:1802.08163, 2018.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems,
pages 2234–2242, 2016.
Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a
scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pages 1889–1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
10

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In ICML, 2014.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction, volume 1. MIT
press Cambridge, 1998.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems, pages 1057–1063, 2000a.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in neural informa-
tion processing systems, pages 1057–1063, 2000b.
Yunhao Tang and Shipra Agrawal. Discretizing continuous action space for on-policy optimization.
arXiv preprint arXiv:1901.10500, 2019.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages
5026–5033. IEEE, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229–256, 1992.
David H Wolpert, William G Macready, et al. No free lunch theorems for optimization. IEEE
transactions on evolutionary computation, 1(1):67–82, 1997.
11

A
Proof of Proposition 1
Let ϵ > 0. We consider a single state MDP (i.e., x-armed bandit) with action space A = Rd and a
multi-modal reward function deﬁned by
r(a) = ϵδ˜µ0(a) + (1 −ϵ)δ˜µ0+D·1(a),
where D = D(R, ϵ) will be deﬁned later, and δx(a) is the Dirac delta function satisfying
R
a g(a)d(δx(a)) = g(x) for all continuous compactly supported functions g.
Denote by fµ,Σ(a) the multivariate Gaussian distribution, deﬁned by
fµ,Σ(a) = (2π|Σ|)−k
2 e−(a −µ)T Σ−1(a −µ).
In PG, we assume µ is parameterized by some parameters θ. Without loss of generality, let us
consider the derivative with respect to θ = µ. At iteration k the derivative can be written as
∇µ log πµ(a) |µ=µk= Σ−1 (a −µk) .
PG will thus update the policy parameter µ by
µk+1 = µk + αk

Ea∼N(µk,Σ)Σ−1 (a −µk) r(a)
	
.
Notice that given a Bernoulli random variable B
=
0
, w.p. ϵ
D
, w.p. 1 −ϵ, one can write
r(a) = Eδ˜µ0+B·1(a). Then by Fubini’s theorem we have
Ea∼N(µk,Σ) (a −µk) r(a)
= EBEa∼N(µk,Σ) (a −µk) δ˜µ0+B·1(a)
= EB (˜µ0 + B · 1 −µk) fµk,Σ(˜µ0 + B · 1).
We wish to show that the gradient has a higher correlation with the direction of ˜µ0 −µk rather than
˜µ0 + D · 1 −µk. That is we wish to show that
 Ea∼N(µk,Σ)Σ−1 (a −µk) r(a)
T  ˜µ0 −µk
∥˜µ0 −µk∥

>
 Ea∼N(µk,Σ)Σ−1 (a −µk) r(a)
T  ˜µ0 + D · 1 −µk
∥˜µ0 + D · 1 −µk∥

.
Substituting r(a) the above equation is equivalent to
(EB (˜µ0 + B · 1 −µ0) fµ0,Σ(˜µ0 + B · 1))T
 ˜µ0 −µk
∥˜µ0 −µk∥

> (EB (˜µ0 + B · 1 −µ0) fµ0,Σ(˜µ0 + B · 1))T
 ˜µ0 + D · 1 −µk
∥˜µ0 + D · 1 −µk∥

.
(4)
Proving Equation (4) for all k ≥0 will complete the proof.
We continue the proof by induction on k.
Base case (k = 0):
Recall that µ0 ∈BR(˜µ0). Writing Equation (4) explicitly we get
LHS = ϵ∥˜µ0 −µ0∥fµ0,Σ(˜µ0) + (1 −ϵ)fµ0,Σ(˜µ0 + D · 1) (˜µ0 −µ0 + D · 1)T
˜µ0 −µ0
∥˜µ0 −µ0∥,
RHS = ϵfµ0,Σ(˜µ0) (˜µ0 −µ0)T
˜µ0 −µ0 + D · 1
∥˜µ0 −µ0 + D · 1∥+ (1 −ϵ)∥˜µ0 −µ0 + D · 1∥fµ0,Σ(˜µ0 + D · 1).
Since fµ0,Σ(˜µ0 + D · 1) ∝exp {−D · 1} we only need to show that for large enough D (which
depends on the constants ϵ and R)
∥˜µ0 −µ0∥> (˜µ0 −µ0)T · 1
D
∥˜µ0 −µ0 + D · 1∥,
as all other values tend to zero.
If (˜µ0 −µ0)T 1 < 0 then we are done. Otherwise, if (˜µ0 −µ0)T 1 ≥0 then
(˜µ0 −µ0)T · 1
D
∥˜µ0 −µ0 + D · 1∥≤∥˜µ0 −µ0∥
D
∥˜µ0 −µ0 + D · 1∥≤∥˜µ0 −µ0∥,
12

Algorithm 2 Generative Actor Critic
1: Input: number of time steps T, policy samples K, minibatch size N
2: Initialize critic networks Qθ1, Qθ2, value network vψ and actor network πφ with random param-
eters θ1, θ2, ψ, φ
3: Initialize target networks θ′
1 ←θ1, θ′
2 ←θ2, ψ′ ←ψ, φ′ ←φ
4: Initialize replay buffer B
5: for t = 0, 1, ..., T do
6:
Select action with exploration noise a ∼πφ(s) + ϵ,
7:
ϵ ∼N(0, σ) and observe reward r and new state s′
8:
Store transition tuple (s, a, r, s′) in B
9:
Sample mini-batch of N transitions (s, a, r, s′) from B
10:
yQ ←r + γvψ′(s′)
11:
Update critics:
12:
θ ←θ −1
N ∇θi
X
(yQ −Qθi(s, a))2
13:
˜aj ←πφ′(τ| s), ∀1 ≤j ≤K, τ ∼U([0, 1]n)
14:
yv ←mini=1,2
PK
j=1 Qθ′
i(s, ˜aj)
15:
Update value:
ψ ←ψ −N −1∇ψ
X
(yv −vψ(s))2
16:
Sample actions ˆa1, . . . , ˆaK from sampling policy σ(πφ′, A)
17:
ˆ
Ak ←{ˆaj : 1 ≤j ≤K, mini=1,2 Qθ′
i(sk, ˆaj) > vψ′(sk)}
18:
Update actor:
φ ←φ −1
N ∇φ
N
X
n=1
X
ˆa∈ˆ
Ak
action dim
X
i=1
ρk
τi

ˆai −πφ(τi|ˆai−1, . . . , ˆa1, sk)

Dπ′
k
Iπ′
k
19:
Update target networks:
θ′
i ←τθi + (1 −τ)θ′
i
ψ′ ←τψ + (1 −τ)ψ′
φ′ ←τφ + (1 −τ)φ′
where in the ﬁrst step we used the Cauchy–Schwarz inequality, and in the second step we used the
fact if a vector x satisﬁes xT 1 ≥0 then for any constant C > 0, ∥x + C · 1∥≥C.
Induction step:
Assume Equation (4) holds from some k ≥0. Then by the gradient procedure we know that
µk ∈BR(˜µ0), and thus we can use the same proof as the base case. Hence, ∥v∗−vπ∞∥∞= 1 −2ϵ
and the result follows for ϵ < 1
3.
B
Experimental Details
Our approach is depicted in Algorithm 2. In addition, we provide a numerical comparison of the
various approaches in Table 3. These results show a clear picture.
Target policy estimation:
To estimate the target policy, for each state s, we sample 128 actions
uniformly from the action space A, 128 samples from the target policy πφ′ and the per-sample loss is
weighted by the positive advantage A(s, ·)+. This can be seen as a form of ‘exploration-exploitation’
- while uniform sampling ensures proper exploration of the action set, sampling from the policy has
a higher probability of producing actions with positive advantage.
13

Table 3: Comparison of the maximal attained value across training.
Environment
DDPG
TD3
PPO
GAC AIQN
GAC IQN
Hopper-v2
638 ± 477
2521 ± 1429
2767 ± 421
3234 ± 122
1473 ± 421
Humanoid-v2
519 ± 44
184 ± 67
579 ± 30
4056 ± 878
3547 ± 572
Walker2d-v2
364 ± 223
3824 ± 995
3694 ± 765
4357 ± 160
1390 ± 651
Swimmer-v2
75 ± 46
60 ± 20
131 ± 1
238 ± 3
45 ± 0
Ant-v2
−399 ± 323
5508 ± 191
2899 ± 973
5064 ± 208
4784 ± 895
HalfCheetah-v2
−395 ± 81
9681 ± 908
3787 ± 2249
9300 ± 515
6807 ± 98
Table 4: AIQN Hyperparameters
Humanoid-v2, Hopper-v2,
Ant-v2, Swimmer-v2
Walker2d-v2
HalfCheetah-v2
Distribution
max{exp Q(s, a) −v(s), 20}
softmax(Q(s, a) −v(s))
Q(s, a) −v(s)
π LR
1e−4
1e−3
1e−3
Q/v LR
1e−3
1e−3
1e−3
π grad clip
1
∞
∞
Q/v grad clip
5
∞
∞
# of samples
64
256
256
The loss is thus the weighted quantile loss. We do note that while one would want to deﬁne the
target policy as the linear/Boltzmann distribution over the positive advantage, this is not possible in
practice. As actions are sampled, we can only construct such a distribution on a per-batch instance.
This approach does provide higher weight for better performing actions, but does result in a different
underlying distribution. In addition, in order to ensure stability, we normalize the quantile loss
weights in each batch - this is to ensure that very small (high) advantage values do not incur a
near-zero (huge) gradients which may harm model stability.
Architectural Details:
Actor: As presented in Figure 3, our architecture incorporates a recurrent cell. The recurrent cell
ensures that each dimension i of the action is a function of the state s, the sampled quantile τi and
the previous predicted action dimensions a1, . . . , ai−1. Notice that using this architecture, the pre-
diction of ai is not affected by τ1, . . . , τi−1. This approach is a strict requirement when considering
the autoregressive approach.
We believe other, potentially more efﬁcient architectures can be explored. For instance, a fully
connected network, similar to the non-autoregressive approach, with attention over the previous
action dimensions may work well [Vaswani et al., 2017]. Such evaluation is out of the scope of this
work and is an interesting investigation for future work.
Value & Critic: While the actor architecture is a non-standard approach, for both the value and
critic networks, we use the classic MLP network. Speciﬁcally, we use a two layer fully connected
network with 400 and 300 neurons in each layer, respectively. Similarly to Fujimoto et al. [2018],
the critic receives a concatenated vector of both the state and action as input.
C
Discussion and Common Mistakes
As shown in the body of the paper, there exist alternative approaches. We take this section in order
to provide some additional discussion into how and why we decided on certain approaches and what
else can be done.
14

C.1
Alternative Gradient Approaches
Going back to the policy gradient approach, speciﬁcally the deterministic version, we can write the
value of the current policy of our generative model (policy) as:
vπ(s) =
Z
τ∈[0,1]n Q(s, F −1(s |τ))dτ ,
or an estimation using samples
vπ(s) = 1
N
N
X
i=1
Q(s, F −1(s |τi)) |τi∼U([0,1]n) .
It may then be desirable to directly optimize this objective function by taking the gradient w.r.t.
the parameters of F −1. However, this approach does not ensure optimality. Clearly, the gradient
direction is provided by the critic Q for each value of τ. This can be seen as optimizing an ensemble
of DDPG models whereas each τ value selects a different model from this set. As DDPG is a uni-
modal parametric distribution and is thus not ensured to converge to an optimal policy, this approach
suffers from the same caveats.
However, Evolution Strategies [Salimans et al., 2017] is a feasible approach. As opposed to the
gradient method, this approach can be seen as directly calculating ∇πvπ, i.e., it estimates the best
direction in which to move the policy. As long as the policy is still capable of representing arbitrarily
complex distributions this approach should, in theory, converge to a global maxima. However, as
there is interest in sample efﬁcient learning, our focus in this work was on introducing an off-policy
learning method under the common actor-critic framework.
C.2
Target Networks and Stability
Our empirical approach, as shown in Algorithm 2, uses a target network for each approximator
(critic, value and the target policy). While the critic and value target networks are mainly for stability
of the empirical approach, they can be disposed of, the policy target network is required for the
algorithm to converge (as shown in Section 3).
The quantile loss, and any distribution loss in general, is concerned with moving probability mass
from the current distribution towards the target distribution. This leads to two potential issues
when lacking the delayed policy: (1) non-quasi-stationarity of the target distribution, and (2) non-
increasing policy.
The ﬁrst point is important from an optimization point of view. As the quantile loss is aimed to esti-
mate some target distribution, the assumption is that this distribution is static. Lacking the delayed
policy network, this distribution potentially changes at each time step and thus can not be properly
estimated using sample based approaches. The delayed policy solves this problem, as it tracks the
policy on a slower timescale it can be seen as quasi-static and thus the target distribution becomes
well deﬁned.
The second point is important from an RL point of view. In general, RL proofs evolve around two
concepts - either you are attempting to learn the optimal Q values and convergence is shown through
proving the operator is contracting towards a unique globally stable equilibrium, or the goal is to
learn a policy and thus the proof is based on showing the policy is monotonically improving. As the
delayed policy network slowly tracks the policy network, the multi-timescale framework tells us that
“by the time” the delayed policy network changes, the policy network can be assumed to converge.
As the policy network is aimed to estimate a distribution over the positive advantage of the delayed
policy, this approach ensures that the delayed policy is monotonically improving (under the correct
theoretical step-size and realizability assumptions).
C.3
Sample Complexity and Policy Samples
When considering sample complexity in its simplest form, our approach is as efﬁcient as the base-
lines we compared to. It does not require the use of larger batches nor does it require more environ-
ment samples. However, as we are optimizing a generative model, it does require sampling from the
model itself.
15

As opposed to Dabney et al. [2018a], we found that in our approach the number of samples does af-
fect the convergence ability of the network. While using 16 samples for each transition in the batch
did result in relatively good policies, increasing this number affected stability and performance pos-
itively. For this reason, we decided to run with a sample size of 128. This results in longer training
times. For instance, training the TD3 algorithm on the Hopper-v2 domain using two NVIDIA GTX
1080-TI cards took around 3 hours, whereas our approach took 40 hours to train. We argue that as
often the resulting policy is what matters, it is worth to sacriﬁce time efﬁciency in order to gain a
better ﬁnal result.
C.4
Generative Adversarial Policy Training
Our approach used the AIQN framework in order to train a generative policy. An alternative method
for learning distributions from samples is using the GAN framework. A discriminator can be trained
to differentiate between samples from the current policy and those from the target distribution; thus,
training the policy to ‘fool’ the discriminator will result in generating a distribution similar to the
target.
However, while the GAN framework has seen multiple successes, it still lacks the theoretical guar-
antees of convergence to the Nash equilibrium. As opposed to the AIQN which is trained on a
supervision signal, the GAN approach is modeled as a two player zero-sum game.
D
Distributional Policy Optimization Assumptions
We provide the assumptions required for the 3-timescale stochastic approximation approach, namely
DPO, to converge.
The ﬁrst assumption is regarding the step-sizes. It ensures that the policy moves on the fastest time-
scale, the value and critic on an intermediate and the delayed policy on the slowest. This enables the
quasi-static analysis in which the fast elements see the slower as static and the slow view the faster
as if they have already converged.
Assumption 1. [Step size assumption]
∞
X
n=0
αk =
∞
X
n=0
βk = ∞=
∞
X
n=0
δk = ∞,
∞
X
n=0
 α2
k + β2
k + δ2
k

< ∞,
αk
βk
→0 and
βk
δk
→0 .
The second assumption requires that the action set be compact. Since there exists a deterministic
policy which is optimal, this assumption ensures that this policy is indeed ﬁnite and thus the process
converges.
Assumption 2. [Compact action set] The action set A(s) is compact for every s ∈S.
The ﬁnal two assumptions (3 and 4) ensure that π, moving on the fast time-scale, converges. The
Lipschitz assumption ensures that the action-value function and in turn the target distribution DIπ′
are smooth.
Assumption 3. [Lipschitz and bounded Q] The action-value function Qπ(s, ·) is Lipschitz and
bounded for every π ∈Π and s ∈S.
Assumption 4. For any D ∈Π and θ ∈Θ, there exists a loss L such that ∇θL(πθ, D) →0 as
πθ →D.
Finally, it can be shown that DPO converges under these assumptions using the standard multi-
timescale approach.
16

