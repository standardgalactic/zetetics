Fast Segment Anything
Xu Zhao 1,3
Wenchao Ding 1,2
Yongqi An 1,2
Yinglong Du 1,2
Tao Yu 1,2
Min Li 1,2
Ming Tang 1,2
Jinqiao Wang 1,2,3,4
Institute of Automation, Chinese Academy of Sciences, Beijing, China1
School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China2
Objecteye Inc., Beijing, China3
Wuhan AI Research, Wuhan, China4
{xu.zhao,yongqi.an,tangm,jqwang}@nlpr.ia.ac.cn
{dingwenchao2021,duyinglong2022,yutao2022,limin2021}@ia.ac.cn
Abstract
The recently proposed segment anything model (SAM)
has made a significant influence in many computer vision
tasks. It is becoming a foundation step for many high-level
tasks, like image segmentation, image caption, and image
editing. However, its huge computation costs prevent it from
wider applications in industry scenarios. The computation
mainly comes from the Transformer architecture at high-
resolution inputs. In this paper, we propose a speed-up al-
ternative method for this fundamental task with compara-
ble performance. By reformulating the task as segments-
generation and prompting, we find that a regular CNN de-
tector with an instance segmentation branch can also ac-
complish this task well. Specifically, we convert this task
to the well-studied instance segmentation task and directly
train the existing instance segmentation method using only
1/50 of the SA-1B dataset published by SAM authors. With
our method, we achieve a comparable performance with
the SAM method at 50× higher run-time speed. We give
sufficient experimental results to demonstrate its effective-
ness. The codes and demos will be released at https:
//github.com/CASIA-IVA-Lab/FastSAM.
1. Introduction
Recently, the Segment Anything Model (SAM) [19] is
proposed. It is regarded as a milestone vision foundation
model. It can segment any object within the image guided
by various possible user interaction prompts. SAM lever-
ages a Transformer model trained on the extensive SA-1B
dataset, which gives it the ability to deftly handle a wide
range of scenes and objects. SAM opens the door to an ex-
citing new task known as Segment Anything. This task, due
to its generalizability and potentiality, has all the makings
of becoming a cornerstone for a broad spectrum of future
vision tasks.
FastSAM
SAM
40ms/img
2099ms/img
50×Faster
(a)
(b)
(c)
Figure 1. Comparative analysis of FastSAM and SAM. (a) Speed
comparison between FastSAM and SAM on a single NVIDIA
GeForce RTX 3090. (b) Comparison on the BSDS500 dataset
[1, 28] for edge detection. (c) Box AR@1000 evaluation of Fast-
SAM and SAM on the COCO dataset [25] for the object proposal.
Both SAM and FastSAM are tested using PyTorch for inference,
except FastSAM(TRT) uses TensorRT for inference.
However, despite these advancements and the promising
results shown by SAM and subsequent models in handling
the segment anything task, its practical applications are still
challenging. The glaring issue is the substantial computa-
tional resource requirements associated with Transformer
(ViT) models, the main part of SAM’s architecture. When
compared with their convolutional counterparts, ViTs stand
out for their heavy computation resources demands, which
presents a hurdle to their practical deployment, especially in
real-time applications. This limitation consequently hinders
the progress and potential of the segment anything task.
Motivated by the high demand from the industrial appli-
arXiv:2306.12156v1  [cs.CV]  21 Jun 2023

cations for the segment anything model, in this paper we
design a real-time solution for the segment anything task,
FastSAM. We decouple the segment anything task into two
sequential stages which are all-instance segmentation and
prompt-guided selection. The first stage hinges on the im-
plementation of a Convolutional Neural Network (CNN)-
based detector. It produces the segmentation masks of all in-
stances in the image. Then in the second stage, it output the
the region-of-interest corresponding the prompt. By lever-
aging the computational efficiency of CNNs, we demon-
strate that a real-time segment of anything model is achiev-
able without much compromising on performance quality.
We hope the proposed method would facilitate the indus-
trial applications of the foundational task of segmenting
anything.
Our proposed FastSAM is based on YOLOv8-seg [16],
an object detector equipped with the instance segmenta-
tion branch, which utilizes the YOLACT [4] method. We
also adopt the extensive SA-1B dataset published by SAM.
By directly training this CNN detector on only 2% (1/50)
of the SA-1B dataset, it achieves comparable performance
to SAM, but with drastically reduced computational and
resource demands, thereby enabling real-time application.
We also apply it to multiple downstream segmentation tasks
to show its generalization performance. On the object pro-
posal task on MS COCO [13], we achieve 63.7 at AR1000,
which is 1.2 points higher than SAM with 32× 32 point-
prompt inputs, but running 50 times faster on a single
NVIDIA RTX 3090.
The real-time segment anything model is valuable for in-
dustrial applications. It can be applied to many scenarios.
The proposed approach not only provides a new, practical
solution for a large number of vision tasks but also does so
at a really high speed, tens or hundreds of times faster than
current methods.
It also offers new views for the large model architecture
for the general vision tasks. We think for specific tasks, spe-
cific models still take advantage to get a better efficiency-
accuracy trade-off.
Then, in the sense of model compression, our approach
demonstrates the feasibility of a path that can significantly
reduce the computational effort by introducing an artificial
prior to the structure.
Our contributions can be summarized as follow:
• A novel, real-time CNN-based solution for the Seg-
ment Anything task is introduced, which significantly
reduces computational demands while maintaining
competitive performance.
• This work presents the first study of applying a CNN
detector to the segment anything task, offering insights
into the potential of lightweight CNN models in com-
plex vision tasks.
• A comparative evaluation between the proposed
method and SAM on multiple benchmarks provides
insights into the strengths and weaknesses of the ap-
proach in the segment anything domain.
2. Preliminary
In this section, we give a review of the segment anything
model and a clear definition of the segment anything task.
Segment Anything Model. In the evolving field of image
segmentation, the Segment Anything Model (SAM) [19] is
a significant innovation due to its proposed training method-
ology and performance on large-scale visual datasets. SAM
provides a high-precision, class-agnostic segmentation per-
formance, exhibiting distinct capabilities in zero-shot tasks.
As a foundation model, it expands the horizons of computer
vision by showing not just powerful interactive segmenta-
tion methods, but also exceptional adaptability across a va-
riety of segmentation tasks. SAM is a striking example of
the potential of foundation models for open-world image
understanding. However, while the model’s performance
is satisfying, it is worth noting that SAM faces a significant
limitation – the lack of real-time processing capability. This
restricts its wide application in scenarios where immediate
segmentation results are critical.
Segment Anything Task. The Segment Anything task is de-
fined as a process whereby an effective segmentation mask
is produced given any form of the prompt. These prompts
range from foreground/background point sets, rough boxes
or masks, free-form text, or any information that indicates
the content to be segmented within an image. We have dis-
covered that the segment anything task can be effectively
broken down into two stages in the majority of practical
applications.
The first stage involves detecting and seg-
menting all objects in the image, like a panoptic segmen-
tation [18] process. The second stage depends on the pro-
vided prompts to separate the specific object(s) of interest
from the segmented panorama. The decoupling of this task
significantly reduces its complexity, thus providing the pos-
sibility to propose a real-time segment of anything model.
3. Methodology
3.1. Overview
Fig. 2 gives the overview of the proposed method, Fast-
SAM. The method consists of two stages, i.e.
the All-
instance Segmentation and the Prompt-guided Selection.
The former stage is a basis and the second stage is essen-
tially the task-oriented post-processing. Different from the
end-to-end transformers [7,8,19], the overall method intro-
duces many human priors which match the vision segmen-
tation tasks, like the local connections of convolutions and

ProtoNet
Detect
Detect
Detect
Mask Coeff.
Mask Coeff.
Mask Coeff.
NMS
Crop
Pred
Threshold
Detect Branch
·
Text
Point-prompt
Box-prompt
Text-prompt
P5
P4
P3
T1
I1
I2
···
IN
I1·T1
···
I2·T1
IN·T1
CLIP
“The black dog”
Mask Branch
···
···
-
+
=
···
+0.0137
-0.0342
+0.6846
-1
+1
-
-
Mask Coeff.
FPN
CNN
Backbone
Image
Encoder
Text
Encoder
···
...
Figure 2. The framework of FastSAM. It contains two stages: All-instance Segmentation (AIS) and Prompt-guided Selection (PGS). We
use YOLOv8-seg [16] to segment all objects or regions in an image. Then we use various prompts to identify the specific object(s) of
interest. It mainly involves the utilization of point prompts, box prompts, and text prompt. The text prompt is based on CLIP [31].
the receptive-field-relevant object assigning strategies. This
makes it tailored for the vision segmentation task and can
converge faster on a smaller number of parameters.
3.2. All-instance Segmentation
Model Architecture. The architecture of YOLOv8 [16] de-
velops from its predecessor, YOLOv5 [15], integrating key
design aspects from recent algorithms such as YOLOX [10],
YOLOv6 [22], and YOLOv7 [35]. YOLOv8’s backbone
network and neck module substitute YOLOv5’s C3 module
with the C2f module. The updated Head module embraces a
decoupled structure, separating classification and detection
heads, and shifts from Anchor-Based to Anchor-Free.
Instance
Segmentation.
YOLOv8-seg
applies
YOLACT [4] principles for instance segmentation.
It
begins with feature extraction from an image via a
backbone network and the Feature Pyramid Network
(FPN) [24], integrating diverse size features. The output
consists of the detection and segmentation branches.
The detection branch outputs category and bounding
box, while the segmentation branch outputs k prototypes
(defaulted to 32 in FastSAM) along with k mask coef-
ficients.
The segmentation and detection tasks are com-
puted in parallel. The segmentation branch inputs a high-
resolution feature map, preserves spatial details, and also
contains semantic information.
This map is processed
through a convolution layer, upscaled, and then passed
through two more convolution layers to output the masks.
The mask coefficients, similar to the detection head’s clas-
sification branch, range between -1 and 1. The instance seg-
mentation result is obtained by multiplying the mask coef-
ficients with the prototypes and then summing them up.
YOLOv8 can be used in a variety of object detection
tasks. With the instance segmentation branch, YOLOv8-
Seg is born suitable for the segment anything task, which
aims to accurately detect and segment every object or region
in an image, regardless of the object category. The proto-
types and mask coefficients provide a lot of extensibility for
prompt guidance. As a simple example, a simple prompt

encoder and decoder structure is additionally trained, with
various prompts and image feature embeddings as input and
mask coefficients as output. In FastSAM, we directly use
the YOLOv8-seg method for the all-instance segmentation
stage. The more artificial design might bring additional im-
provements, but we regard it as out of the scope of this work
and leave it for future study.
3.3. Prompt-guided Selection
Following the successful segmentation of all objects or
regions in an image using YOLOv8, the second stage of the
segment anything task is to use various prompts to identify
the specific object(s) of interest. It mainly involves the uti-
lization of point prompts, box prompts, and text prompts.
Point prompt.
The point prompt consists of matching
the selected points to the various masks obtained from the
first phase. The goal is to determine the mask in which
the point is located.
Similar to SAM, we employ fore-
ground/background points as the prompt in our approach.
In cases where a foreground point is located in multiple
masks, background points can be utilized to filter out masks
that are irrelevant to the task at hand. By employing a set of
foreground/background points, we are able to select multi-
ple masks within the region of interest. These masks will be
merged into a single mask to completely mark the object of
interest. In addition, we utilize morphological operations to
improve the performance of mask merging.
Box prompt. The box prompt involves performing Inter-
section over Union (IoU) matching between the selected
box and the bounding boxes corresponding to the various
masks from the first phase. The aim is to identify the mask
with the highest IoU score with the selected box and thus
select the object of interest.
Text prompt. In the case of text prompt, the correspond-
ing text embeddings of the text are extracted using the
CLIP [31] model. The respective image embeddings are
then determined and matched to the intrinsic features of
each mask using a similarity metric. The mask with the
highest similarity score to the image embeddings of the text
prompt is then selected.
By carefully implementing these prompt-guided selec-
tion techniques, the FastSAM can reliably select specific
objects of interest from a segmented image. The above ap-
proach provides an efficient way to accomplish the segment
anything task in real-time, thus greatly enhancing the util-
ity of the YOLOv8 model for complex image segmentation
tasks. A more effective prompt-guided selection technique
is left for future exploration.
4. Experiments
In this section, we first analysis the run-time efficiency
of FastSAM. Then we experiment with four zero-shot tasks,
along with applications in real-world scenarios, efficiency,
and deployment. In the first part of the experiments, our
goal is to test the similarity in capabilities between Fast-
SAM and SAM. Following SAM, we also experiment with
four tasks with different levels: (1) low-level: edge detec-
tion, (2) mid-level: object proposal generation, (3) high-
level: instance segmentation, and finally, (4) high-level:
segmenting objects with free-form text input. Our exper-
iments also further validate FastSAM’s capabilities with
real-world applications and speed.
Implementation Details.
Unless stated otherwise, the
following conditions apply:
(1) FastSAM employs a
YOLOv8-x [16] model as the main part of its architecture,
with the input size of 1024; (2) FastSAM’s training was car-
ried out on 2% of the SA-1B dataset [19]; (3) We train the
model for 100 epochs using the default hyper-parameter set-
tings except that the reg max in the bounding box regres-
sion module is changed from 16 to 26 for predicting large
instances.
4.1. Run-time Efficiency Evaluation
SAM uses the Transformer architecture to construct an
end-to-end algorithm. The Transformer is a universal ar-
chitecture that can represent many forms of mapping func-
tions of various tasks. To segment anything, SAM learns
the vision-oriented inductive bias through the learning pro-
cess on large-scale data. On the contrary, with the human
priori knowledge in structure designing, FastSAM obtains
a relatively compact model. From Figure 3, The FastSAM
generates relatively satisfying results.
In Table 1, we report the running speed of SAM and Fast-
SAM on a single NVIDIA GeForce RTX 3090 GPU. It can
be seen that FastSAM surpasses SAM at all prompt num-
bers. Moreover, the running speed of FastSAM does not
change with the prompts, making it a better choice for the
Everything mode.
4.2. Zero-Shot Edge Detection
Approach. FastSAM is assessed on the basic low-level task
of edge detection using BSDS500 [1, 28]. Specifically, we
select the mask probability map from the results of Fast-
SAM’s all-instance segmentation stage. After that, Sobel
filtering [33] is applied to all mask probability maps to
generate edge maps. Finally, we conclude with the edge
NMS [6] step.
Results. The representative edge maps are illustrated in
Figure 4.
Upon qualitative observation, it becomes evi-
dent that despite FastSAM’s significantly fewer parameters
(only 68M), it produces a generally good edge map. In com-
parison to the ground truth, both FastSAM and SAM tend
to predict a larger number of edges, including some logi-
cal ones that aren’t annotated in the BSDS500. This bias

Running Speed under Different Point Prompt Numbers (ms)
method
params
1
10
100
E(16×16)
E(32×32*)
E(64×64)
SAM-H [20]
0.6G
446
464
627
852
2099
6972
SAM-B [20]
136M
110
125
230
432
1383
5417
FastSAM (Ours)
68M
40
Table 1. Running Speed (ms/image) of SAM and FastSAM under different point prompt numbers. E: Everything Mode of SAM. *: 32×32
is the default setting of SAM for many tasks1.
Figure 3. Segmentation Results of FastSAM
method
year
ODS
OIS
AP
R50
HED [37]
2015
.788
.808
.840
.923
EDETR [30]
2022
.840
.858
.896
.930
zero-shot transfer methods:
Sobel filter
1968
.539
-
-
-
Canny [6]
1986
.600
.640
.580
-
Felz-Hutt [9]
2004
.610
.640
.560
-
SAM [19]
2023
.768
.786
.794
.928
FastSAM
2023
.750
.790
.793
.903
Table 2. Zero-shot transfer to edge detection on BSDS500. Eval-
uation Data of other methods is from [20].
is quantitatively reflected in Table 2. Table 2 shows that
we achieve similar performance with SAM, specifically a
higher R50 and a lower AP.
4.3. Zero-Shot Object Proposal Generation
Background. Object proposal generation has long been a
basic pre-processing step for many computer vision tasks,
including general object detection, few-shot object detec-
tion, and image understanding. Many famous proposal gen-
eration methods witness the evolution of visual recognition
in the past decades, as a role of the basic step of visual
recognition methods. These proposal generation methods
includes EdgeBox [38], Geodesic [21], Selective Search
1https://github.com/facebookresearch/segment-
anything
[34], MCG [2].
These years, many deep learning-based
methods are proposed like DeepMask [29], OLN [17]. For
example, RCNN-series object detection methods [11, 12]
adopts the Seletive Search method, and the recently pro-
posed open world detector, UniDetector [36], adopts the
OLN method. Though RPN [32] is used by most exist-
ing object detectors, it can only generate object proposals
of the learned categories, limiting its application in open-
vocabulary recognition tasks. Therefore, zero-shot object
proposal generation is rather important. A good proposed
method is important for the good performance of these vi-
sual recognition tasks.
We directly use the generated bounding boxes of the first
stage of FastSAM as the object proposals. To evaluate the
performance, we test on LVIS [13] and COCO [25] dataset,
following the existing evaluating strategies. Besides this,
following the experimental settings of SAM, we also test
the mask proposal accuracy by using the all-instance masks
of the first stage.
Details. We report the results of SAM, ViTDet [23], and our
FastSAM on the LVIS dataset. As SAM does not publicize
its detailed evaluation codes, we reproduced the category-
agnostic mask and box recall using the official LVIS eval-
uation code [13]. However, we fail to reproduce the Mask
AR results of ViTDet and SAM presented in the SAM’s pa-
per [20]. Nevertheless, we think our evaluation results still
reflect several features of FastSAM compared with SAM.

image
ground truth
SAM
FastSAM
Figure 4. Zero-shot edge prediction on BSDS500. FastSAM achieves comparable results to SAM.
Results. The results is shown in Table 3, 4, and 5. The re-
sults show that our method has a significant advantage on
the box proposal generation tasks. Table 3 presents the Av-
erage Recall (AR) of various methods on the COCO vali-
dation set. Among these, EdgeBoxes [38], Geodesic [21],
Sel.Search [34], and MCG [2] are methods that do not
require training, whereas DeepMask [29] and OLN [17]
are supervised methods that are trained on VOC categories
within the COCO training set, and then tested across all cat-
egories. In contrast, our approach and SAM [20] implement
a fully zero-shot transfer. From the table, it can be seen
that our method and SAM [20] do not perform as well in
AR@10 precision compared to previous supervised meth-
ods such as OLN [17]. However, in AR@1000, our method
significantly outperforms OLN [17]. The reason for this is
that previous methods were trained on certain categories in
COCO, leading to a higher confidence level in these cat-
egories during inference. However, since our method and
SAM are zero-shot, this results in balanced confidence lev-
els across different categories, thereby recalling more cate-
gories not present in COCO. More comparisons can be seen
in Figure 5.
In Table 4, we report the bbox AR@1000 results of
VitDet-H [23], SAM [20], and our method on the LVIS
v1 dataset. Our method substantially surpasses the most
computationally intensive model of SAM, SAM-H E64, by
over 5%. However, it falls short compared to VitDet-H [23],
which was trained on the LVIS dataset.
The reason for
these results is that during our training process, we used
the ground truth (gt) bounding box (bbox) information as a
supervisory signal. SAM [20], on the other hand, only used
the mask as a supervisory signal, and its bbox at inference
is generated by extracting the outer box from the mask.
From Table 5, our mask proposal generation is relatively
lower on Recall.
We find this mainly results from that
our segmentation mask on small-sized objects is not fine-
AR10
AR100
AR1000
AUC
EdgeBoxes [38]
7.4
17.8
33.8
13.9
Geodesic [21]
4.0
18.0
35.9
12.6
Sel.Search [34]
5.2
16.3
35.7
12.6
MCG [2]
10.1
24.6
39.8
18.0
DeepMask [29]
13.9
28.6
43.1
21.7
OLN-Box [17]
27.7
46.1
55.7
34.3
SAM-H E64
15.5
45.6
67.7
32.1
SAM-H E32
18.5
49.5
62.5
33.7
SAM-B E32
11.4
39.6
59.1
27.3
FastSAM (Ours)
15.7
47.3
63.7
32.2
Table 3. Comparison with learning-free methods on All Categories
of COCO. We report average recall (AR) and AUC of learning
free methods, deep learning methods (trained on VOC), and ours
vs SAM on All generalization. The scores of competing methods
are taken from [17], which test object proposal methods on all 80
COCO classes.
Figure 5. Comparison with OLN [17] and SAM-H [20]. We test
object proposal methods on all 80 COCO classes
grained enough. We give more detailed discussions in Sec-
tion 6.

bbox AR@1000
method
all
small
med.
large
ViTDet-H [23]
65.0
53.2
83.3
91.2
zero-shot transfer methods:
SAM-H E64
52.1
36.6
75.1
88.2
SAM-H E32
50.3
33.1
76.2
89.8
SAM-B E32
45.0
29.3
68.7
80.6
FastSAM (Ours)
57.1
44.3
77.1
85.3
Table 4. Object proposal generation on LVIS v1. FastSAM and
SAM is applied zero-shot, i.e. it was not trained for object proposal
generation nor did it access LVIS images or annotations.
mask AR@1000
method
all
small
med.
large
freq.
com.
rare
results reported in the SAM paper:
ViTDet-H [23]
63.0
51.7
80.8
87.0
63.1
63.3
58.3
SAM [20] – single out. 54.9
42.8
76.7
74.4
54.7
59.8
62.0
SAM [20]
59.3
45.5
81.6
86.9
59.1
63.9
65.8
results after our replication:
ViTDet-H [23]
59.9
48.3
78.1
84.8
-
-
-
SAM-H E64
54.2
39.6
77.9
83.9
SAM-H E32
51.8
35.2
78.7
85.2
-
-
-
SAM-B E32
45.8
31.1
70.5
73.6
-
-
-
FastSAM (Ours)
49.7
35.6
72.7
77.6
-
-
-
Table 5. Object proposal generation on LVIS v1. FastSAM and
SAM are applied zero-shot, i.e. it was not trained for object pro-
posal generation nor did it access LVIS images or annotations.
Figure 6. Segmentation results with text prompts
4.4. Zero-Shot Instance Segmentation
Approach. Similarly to the SAM method, we accomplish
the instance segmentation task by utilizing the bounding
box (bbox) generated by ViTDet [23] as the prompt. As de-
scribed by Section 3.3, we choose the mask with the highest
Intersection over Union (IoU) with the bbox as the predicted
mask.
Results. Table 6 gives the evaluation results. On this task,
we fail to achieve a high AP. We infer that this mainly be-
cause of the segmentation mask accuracy or the box-based
mask selection strategy. Section 6 gives several examples.
4.5. Zero-Shot Object Localization with Text
Prompts
Approach. Finally, we consider an even high-level task, i.e.
segmenting objects by free-form texts. This experiment is
to show the FastSAM ability of processing text prompts like
SAM. Different wit SAM, FastSAM doesn’t need to modify
COCO [26]
LVIS v1 [13]
method
AP
APS
APM
APL
AP
APS
APM
APL
ViTDet-H [23] 51.0
32.0
54.3
68.9
46.6
35.0
58.0
66.3
zero-shot transfer methods (segmentation module only):
SAM
46.5
30.8
51.0
61.7
44.7
32.5
57.6
65.5
FastSAM
37.9
23.9
43.4
50.0
34.5
24.6
46.2
50.8
Table 6. Instance segmentation results. Fastsam is prompted with
ViTDet boxes to do zero-shot segmentation. The fully-supervised
ViTDet outperforms SAM, but the gap shrinks on the higher-
quality LVIS masks.
the training procedure. It directly runs text through CLIP’s
text encoder and then uses the resulting text embedding to
find the most similar mask at inference time.
Results. We show qualitative results in Fig. 6. FastSAM
can segment objects well based on the text prompts. Never-
theless, the running speed of the text-to-mask segmentation
is not satisfying, since each mask region is required to be
fed into the CLIP feature extractor. How to combine the
CLIP embedding extractor into the FastSAM’s backbone
network remains an interesting problem with respect to the
model compression.
5. Real-world Applications
In this section, we evaluate the performance of FastSAM
across different application scenarios and analyze its advan-
tages and limitations. We showcase visualizations of Fast-
SAM’s segmentation using point-prompt, box-prompt, and
everything modes, and compare it with SAM and ground
truths.
Anomaly Detection.
As detailed in [3], anomaly detection is a task that aims
to distinguish between defective and normal samples in the
manufacture. FastSAM is evaluated using the MVTec AD
dataset [3], with results displayed in Fig. 7.
Under ev-
erything mode, FastSAM can segment nearly all regions
similar to SAM, but with a lower level of precision com-
pared to SAM. In addition, the mask for the background
does not completely cover the entire background, which
is an inherent characteristic of YOLACT [4].
By fore-
ground/background points (yellow and magenta points in
FastSAM-point respectively) or box-guided selection, Fast-
SAM can segment on the exact defective regions.
Salient Object Segmentation.
The aim of salient object segmentation [5] is to segment
the most attention-grabbing objects from an image. This
task is class-agnostic, setting it apart from semantic seg-
mentation. We apply FastSAM to the well-known saliency
dataset, ReDWeb-S [27]. As presented in Fig. 8, FastSAM
exhibited only a minor difference from SAM under every-
thing mode, as it segment fewer background objects which
are irrelevant to the task. By points-guided selection, such

original image
ground truth
FastSAM-point
FastSAM-box
FastSAM-everything
SAM-point
SAM-box
SAM-everything
Figure 7. Application on anomaly detection, where SAM-point/box/everything means using point-prompt, box-prompt, and everything
modes respectively.
original image
ground truth
FastSAM-point
FastSAM-box
FastSAM-everything
SAM-point
SAM-box
SAM-everything
Figure 8. Application on salient object segmentation, where SAM-point/box/everything mean using point-prompt, box-prompt, and
everything modes respectively.
as yellow points in FastSAM-point, we can obtain masks of
all objects of interest. The segmentation result of FastSAM-
point is nearly identical to that of the SAM-point and the
ground truth, with only minor details lost at the edges. The
object of interest can also be selected by box prompt, such
as the green box in FastSAM-box. However, it is impossi-
ble to select multiple objects with a single box, which even
SAM-box cannot realize.
Building Extracting.
Building Extracting from optical remote sensing im-
agery has a wide range of applications, such as urban
planning. We evaluate FastSAM on the dataset proposed
by [14].
As demonstrated in Fig.9, FastSAM performs
well in segmenting regularly shaped objects, but segments
fewer regions related to shadows compared to SAM. We can
also select regions of interest with point-prompt and box-
prompt, as presented in FastSAM-point and FastSAM-box.
It is worth noting that we position a point in a shadow region
in FastSAM-point. However, the correct mask for the build-
ing can still be obtained by merging based on this point.
This indicates that our method can resist the interference of
noise to some extent.

original image
ground truth
FastSAM-point
FastSAM-box
FastSAM-everything
SAM-point
SAM-box
SAM-everything
Figure 9. Application on building extracting, where SAM-point/box/everything means using point-prompt, box-prompt, and everything
modes respectively.
Figure 10. FastSAM generates finer segmentation masks on the narrow region of the large objects
6. Discussion
Generally, the proposed FastSAM achieves a compara-
ble performance with SAM and runs 50x faster than SAM
(32×32) and 170x faster than SAM (64×64). The running
speed makes it a good choice for industrial applications,
such as road obstacle detection, video instance tracking, and
image manipulation. On some images, FastSAM even gen-
erates better masks for large objects, as shown in Figure 10.
Weakness. However, as presented in the experiments, our
box generation has a significant advantage, but our mask
generation performance is below SAM. We visualize these
examples in Figure 11. We find that FastSAM has the fol-
lowing features.
• The low-quality small-sized segmentation masks have
large confidence scores.
We think this is because
the confidence score is defined as the bbox score of
YOLOv8, which is not strongly related to the mask
quality. Modifying the network to predict the mask
IoU or other quality indicators is a way to improve that.
• The masks of some of the tiny-sized objects tend to
be near the square. Besides, the mask of large objects
may have some artifacts on the border of the bounding
boxes. This is the weakness of the YOLACT method.
By enhancing the capacity of the mask prototypes or
reformulating the mask generator, the problem is ex-
pected to solve.
Moreover, since we only use 1/50 of all SA-1B datasets,
the model’s performance can also be further enhanced by
utilizing more training data.
7. Conclusion
In this paper, we rethink the segment of anything task
and the model architecture choosing, and propose an al-
ternative solution with 50 times faster running speed than

Figure 11. Some examples for the bad case of FastSAM.
SAM-ViT-H (32×32).
The experiments show that Fast-
SAM can solve multiple downstream tasks well.
Still,
there are several weaknesses that can be improved for Fast-
SAM, like the scoring mechanism and the instance mask-
generating paradigm. These problems are left for future
study.
References
[1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-
tendra Malik. Contour detection and hierarchical image seg-
mentation. IEEE transactions on pattern analysis and ma-
chine intelligence, 33(5):898–916, 2010. 1, 4
[2] Pablo Arbel´aez, Jordi Pont-Tuset, Jonathan T Barron, Fer-
ran Marques, and Jitendra Malik. Multiscale combinatorial
grouping. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pages 328–335, 2014.
5, 6
[3] Paul Bergmann, Michael Fauser, David Sattlegger, and
Carsten Steger.
Mvtec ad–a comprehensive real-world
dataset for unsupervised anomaly detection. In CVPR, pages
9592–9600, 2019. 7
[4] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee.
Yolact: Real-time instance segmentation. In Proceedings of
the IEEE/CVF international conference on computer vision,
pages 9157–9166, 2019. 2, 3, 7
[5] Ali Borji, Ming-Ming Cheng, Qibin Hou, Huaizu Jiang, and
Jia Li. Salient object detection: A survey. Computational
Visual Media, 5:117–150, 2019. 7
[6] John Canny. A computational approach to edge detection.
IEEE Transactions on pattern analysis and machine intelli-
gence, (6):679–698, 1986. 4, 5
[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part I 16, pages 213–229.
Springer, 2020. 2
[8] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-
pixel classification is not all you need for semantic segmen-
tation. Advances in Neural Information Processing Systems,
34:17864–17875, 2021. 2
[9] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient
graph-based image segmentation. International journal of
computer vision, 59:167–181, 2004. 5
[10] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian
Sun. Yolox: Exceeding yolo series in 2021. arXiv preprint
arXiv:2107.08430, 2021. 3
[11] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1440–1448,
2015. 5
[12] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra
Malik.
Region-based convolutional networks for accurate
object detection and segmentation.
IEEE transactions on
pattern analysis and machine intelligence, 38(1):142–158,
2015. 5
[13] Agrim Gupta, Piotr Dollar, and Ross Girshick.
Lvis: A
dataset for large vocabulary instance segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pages 5356–5364, 2019. 2, 5, 7
[14] Shunping Ji, Shiqing Wei, and Meng Lu. Fully convolutional
networks for multisource building extraction from an open
aerial and satellite imagery data set. IEEE Transactions on
Geoscience and Remote Sensing, 57(1):574–586, 2018. 8
[15] Glenn Jocher.
Yolov5 by ultralytics, 2020.
https://
github.com/ultralytics/yolov5. 3
[16] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Yolo by ultra-
lytics, 2023. https://github.com/ultralytics/
ultralytics. 2, 3, 4
[17] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon,
and Weicheng Kuo. Learning open-world object proposals
without learning to classify. IEEE Robotics and Automation
Letters, 7(2):5453–5460, 2022. 5, 6

[18] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Doll´ar. Panoptic segmentation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 9404–9413, 2019. 2
[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643, 2023. 1, 2, 4, 5
[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643, 2023. 5, 6, 7
[21] Philipp Kr¨ahenb¨uhl and Vladlen Koltun.
Geodesic object
proposals. In Computer Vision–ECCV 2014: 13th European
Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part V 13, pages 725–739. Springer, 2014. 5, 6
[22] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng
Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangx-
iang Chu. Yolov6 v3.0: A full-scale reloading, 2023. 3
[23] Jingjing Li, Tianyu Yang, Wei Ji, Jue Wang, and Li
Cheng. Exploring denoised cross-video contrast for weakly-
supervised temporal action localization.
In CVPR, pages
19914–19924, 2022. 5, 6, 7
[24] Tsung-Yi Lin, Piotr Doll´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie.
Feature pyra-
mid networks for object detection.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 2117–2125, 2017. 3
[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13, pages 740–755. Springer, 2014. 1, 5
[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13, pages 740–755. Springer, 2014. 7
[27] Nian Liu, Ni Zhang, Ling Shao, and Junwei Han. Learning
selective mutual attention and contrast for rgb-d saliency de-
tection. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 44(12):9026–9042, 2021. 7
[28] David Martin, Charless Fowlkes, Doron Tal, and Jitendra
Malik.
A database of human segmented natural images
and its application to evaluating segmentation algorithms and
measuring ecological statistics. In Proceedings Eighth IEEE
International Conference on Computer Vision. ICCV 2001,
volume 2, pages 416–423. IEEE, 2001. 1, 4
[29] Pedro O O Pinheiro, Ronan Collobert, and Piotr Doll´ar.
Learning to segment object candidates. Advances in neural
information processing systems, 28, 2015. 5, 6
[30] Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan,
and Haibin Ling. Edter: Edge detection with transformer.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 1402–1412, 2022. 5
[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages
8748–8763. PMLR, 2021. 3, 4
[32] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. Advances in neural information process-
ing systems, 28, 2015. 5
[33] Irwin Sobel, Gary Feldman, et al. A 3x3 isotropic gradient
operator for image processing. a talk at the Stanford Artifi-
cial Project in, pages 271–272, 1968. 4
[34] Jasper RR Uijlings, Koen EA Van De Sande, Theo Gev-
ers, and Arnold WM Smeulders. Selective search for ob-
ject recognition. International journal of computer vision,
104:154–171, 2013. 5, 6
[35] Chien-Yao
Wang,
Alexey
Bochkovskiy,
and
Hong-
Yuan Mark Liao. YOLOv7: Trainable bag-of-freebies sets
new state-of-the-art for real-time object detectors.
arXiv
preprint arXiv:2207.02696, 2022. 3
[36] Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Tor-
ralba, Hengshuang Zhao, and Shengjin Wang. Detecting ev-
erything in the open world: Towards universal object detec-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 11433–11443,
2023. 5
[37] Saining Xie and Zhuowen Tu. Holistically-nested edge de-
tection. In Proceedings of the IEEE international conference
on computer vision, pages 1395–1403, 2015. 5
[38] C Lawrence Zitnick and Piotr Doll´ar.
Edge boxes: Lo-
cating object proposals from edges. In Computer Vision–
ECCV 2014: 13th European Conference, Zurich, Switzer-
land, September 6-12, 2014, Proceedings, Part V 13, pages
391–405. Springer, 2014. 5, 6

