s
OF CODING 

FOUNDATIONS 
OF CODING 
Theory and Applications 
of Error-Correcting Codes 
with an Introduction to 
Cryptography and Information Theory 
Jifi Adamek 
Czech Technical 
University 
in 
Prague 
A Wiley-Interscience Publication 
JOHN WILEY & SONS, INC. 
Chichester · New York · Brisbane · Toronto · Singapore 

A NOTE TO THE READER 
This book has been electronically reproduced from 
digital information stored at John Wiley & Sons, Inc. 
We are pleased that the use of this new technology 
will enable us to keep works of enduring scholarly 
value in print as long as there is a reasonable demand 
for them. The content of this book is identical to 
previous printings. 
In recognition of the importance of preserving what has been 
written, it is a policy of John Wiley & Sons, Inc., to have books 
of enduring value published in the United States printed on 
acid-free paper, and we exert our best efforts to that end. 
Copyright © 1991 by John Wiley & Sons, Inc. 
All rights reserved. Published simultaneously in Canada. 
Reproduction or translation of any part of this work 
beyond that permitted by Section 107 or 108 of the 
1976 United States Copyright Act without the permission 
of the copyright owner is unlawful. Requests for 
permission or further information should be addressed to 
the Permissions Department, John Wiley & Sons, Inc. 
Library of Congress Cataloging in Publication Data: 
Ad&mek, Jiff, ing. 
Foundations of coding: theory and applications of error-
correcting codes, with an introduction to cryptography and 
information theory / Jif ί Adamek. 
p. 
cm. 
"A Wiley-Interscience publication'' 
Includes bibliographical references and index. 
ISBN 0-471-62187-0 
1. Coding theory. 
I. Title. 
QA268A36 1991 
003'.54-dc20 
90-20905 
CIP 
Rev. 
10 9 8 
7 
6 
5 
4 

To Honza, Kuba, Jirka, and 
Ondfej 

Preface 
Coding theory is a fascinating field combining elegant mathematical theo-
ries with constructions of a major practical impact. 
This book is devoted to constructions of 
(1) error-correcting codes, 
(2) secrecy codes, and 
(3) codes used in data compression. 
The stress is on the first direction: we introduce a number of important 
classes of error-detecting and error-correcting codes, and we present their 
decoding methods. Some of these constructions require a deep background 
in modern algebra, and we carefully provide such background. Secret codes 
are treated only briefly; we mainly explain the role of error-correcting codes 
in modern cryptography. Data compression and other topics related to 
information theory are briefly discussed in the first part of the book. 
The material is presented in a way making it possible to appreciate both 
the beauty of the theory and the scope of practical applications. We use 
the definition-theorem-proof style usual in mathematical texts (since the 
reader can thus skip a proof to keep continuity of the text and return to it 
later), but formalism is avoided as much as possible. 
The book evolved from a series of lectures I held at the Czech Technical 
University in Prague in 1985-1990. 
They were based primarily on the 
following excellent textbooks which the reader may use for further reading: 
Information Theory and Coding, Abramson (1963),* Theory and Practise 
of Error Control Codes, Blahut (1983), The Theory of Error-Correcting 
Codes, MacWilliams and Sloane (1981), and An Introduction to Cryptology, 
van Tilborg (1988). 
Jiff Adamek 
*A name followed by a year in parentheses refers to the list of references at the end 
of the book. 
vii 

Contents 
Introduction 
1 
Part I 
Coding and Information Theory 
3 
1 
Coding and Decoding 
5 
1.1 
Coding 
5 
1.2 
Unique Decoding 
6 
1.3 
Block Codes and Instantaneous Codes 
7 
1.4 
Some Important Block Codes 
9 
1.5 
Construction of Instantaneous Codes 
11 
1.6 
Kraft's Inequality 
12 
1.7 
McMillan's Theorem 
13 
Exercises 
14 
Notes 
16 
2 
Huffman Codes 
17 
2.1 
Information Source 
17 
2.2 
Huffman Codes 
17 
2.3 
Construction of Binary Huffman Codes 
18 
2.4 
Example 
21 
2.5 
Construction of General Huffman Codes 
22 
Exercises 
24 
Notes 
24 
3 
Data Compression and Entropy 
25 
3.1 
An Example of Data Compression 
25 
3.2 
The Idea of Entropy 
26 
3.3 
The Definition of Entropy 
28 
3.4 
An Example 
29 
ix 

x 
CONTENTS 
3.5 
Maximum and Minimum Entropy 
30 
3.6 
Extensions of a Source 
32 
3.7 
Entropy and Average Length 
33 
3.8 
Shannon's Noiseless Coding Theorem 
34 
3.9 
Concluding Remarks 
36 
Exercises 
36 
Notes 
38 
4 
Reliable Communication Through Unreliable Channels 
39 
4.1 
Binary Symmetric Channels 
40 
4.2 
Information Rate 
42 
4.3 
An Example of Increased Reliability 
44 
4.4 
Hamming Distance 
46 
4.5 
Detection of Errors 
48 
4.6 
Correction of Errors 
49 
4.7 
Channel Capacity 
50 
4.8 
Shannon's Fundamental Theorem 
56 
Exercises 
58 
Notes 
60 
Part II 
Error-Correcting Codes 
61 
5 
Binary Linear Codes 
63 
5.1 
Binary Addition and Multiplication 
63 
5.2 
Codes Described by Equations 
64 
5.3 
Binary Linear Codes 
65 
5.4 
Parity Check Matrix 
67 
5.5 
Hamming Codes—Perfect Codes for Single Errors 
69 
5.6 
The Probability of Undetected Errors 
75 
Exercises 
77 
Notes 
78 
6 
Groups and Standard Arrays 
79 
6.1 
Commutative Groups 
79 
6.2 
Subgroups and Cosets 
81 
6.3 
Decoding by Standard Arrays 
84 
Exercises 
87 
Notes 
89 

CONTENTS 
xi 
7 
Linear Algebra 
91 
7.1 
Fields and Rings 
91 
7.2 
The Fields Z p 
93 
7.3 
Linear Spaces 
95 
7.4 
Finite-Dimensional Spaces 
98 
7.5 
Matrices 
101 
7.6 
Operations on Matrices 
· 
105 
7.7 
Orthogonal Complement 
108 
Exercises 
Ill 
Notes 
114 
8 
Linear Codes 
115 
8.1 
Generator Matrix 
115 
8.2 
Parity Check Matrix 
119 
8.3 
Syndrome 
121 
8.4 
Detection and Correction of Errors 
122 
8.5 
Extended Codes and Other Modifications 
125 
8.6 
Simultaneous Correction and Detection of Errors 
128 
8.7 
Mac Williams Identity 
130 
Exercises 
133 
Notes 
135 
9 
Reed-Muller Codes: Weak Codes with Easy Decoding 
137 
9.1 
Boolean Functions 
138 
9.2 
Boolean Polynomials 
140 
9.3 
Reed-Muller Codes 
144 
9.4 
Geometric Interpretation: Three-Dimensional Case 
147 
9.5 
Geometric Interpretation: General Case 
151 
9.6 
Decoding Reed-Muller Codes 
154 
Exercises 
159 
Notes 
160 
10 Cyclic Codes 
161 
10.1 Generator Polynomial 
161 
10.2 Encoding Cyclic Codes 
167 
10.3 Parity Check Polynomial 
171 
10.4 Decoding Cyclic Codes 
175 
10.5 Error-Trapping Decoding 
180 
10.6 Golay Code: A Perfect Code for Triple Errors 
182 
10.7 Burst Errors 
185 
10.8 Fire Codes: High-Rate Codes for Burst Errors 
188 

xii 
CONTENTS 
Exercises 
192 
Notes
 
l 9 4 
11 Polynomials and Finite Fields 
197 
11.1 Zeros of Polynomials 
197 
11.2 Algebraic Extensions of a Field 
201 
11.3 Galois Fields 
206 
11.4 Primitive Elements 
207 
11.5 The Characteristic of a Field 
211 
11.6 Minimal Polynomial 
213 
11.7 Order 
216 
11.8 The Structure of Finite Fields 
219 
11.9 Existence of Galois Fields 
221 
Exercises 
223 
Notes 
227 
12 B C H Codes: Strong Codes Correcting Multiple Errors 
229 
12.1 Hamming Codes as Cyclic Codes 
230 
12.2 Double-Error-Correcting BCH Codes 
232 
12.3 BCH Codes 
239 
12.4 Reed-Solomon Codes and Derived Burst-Error-Correcting 
Codes 
245 
12.5 Generalized Reed-Muller Codes 
246 
12.6 Goppa Codes: Asymptotically Good Codes 
248 
Exercises 
255 
Notes 
256 
13 Fast Decoding of B C H Codes 
257 
13.1 Error Location and Error Evaluation 
258 
13.2 Euclidean Algorithm 
260 
13.3 The Decoding Algorithm 
263 
Exercises 
266 
Notes 
267 
14 Convolutional Codes 
269 
14.1 Linear Codes and Convolutional Codes 
269 
14.2 Generator Polynomials and Generator Matrices 
274 
14.3 Maximum-Likelihood Decoding of Convolutional Codes 
. . 279 
14.4 The Viterbi Decoding Algorithm 
283 
Exercises 
288 
Notes 
290 

xiii 
Part III 
Cryptography 
291 
15 Cryptography 
293 
15.1 A Noisy Wiretap 
294 
15.2 Secret-Key Encryption 
296 
15.3 Public-Key Encryption 
303 
15.4 Encryption Based on Large Prime Numbers 
305 
15.5 Encryption Based on Knapsack Problems 
307 
15.6 Data Encryption Standard 
309 
Exercises 
316 
Notes 
317 
Appendixes 
319 
A Galois Fields 
321 
Β 
B C H Codes and Reed-Muller Codes 
325 
Bibliography 
327 
List of Symbols 
331 
Index 
333 

FOUNDATIONS 
OF CODING 

Introduction 
Data transmision and data storage suffer from errors created by noise. Tech-
niques for combatting noise have been used for a long time. They range 
from simple ones, e.g., adding a parity check symbol to every byte, to mod-
ern complex error-correcting techniques described in this book. The basic 
idea of error correction by a block code (i.e., a code in which all code words 
have the same length) is simple: code words must be "wide apart" from 
each other. That is, two distinct code words have a large Hamming dis-
tance, which means the number of symbols in which the words differ. Then 
the code corrects errors as follows: the word received is corrected to the 
nearest code word (in the sense of the Hamming distance). If the number of 
errors created by noise is smaller than one-half of the minimum Hamming 
distance of code words, then the correction is well done. Thus, the theory 
of error-correcting block codes is concerned with a construction of "good" 
codes with large Hamming distances. "Good" means that (1) the num-
ber of code words is as high as possible (to keep the redundancy low) and 
(2) an efficient technique for error correction is known (to make the search 
for the nearest code word fast). 
Besides block codes, there is another class of error-correcting codes, 
called convolutional codes, in which memory plays a role: the message is 
again divided into blocks, but each block sent depends on a certain number 
of preceding blocks. The theory of convolutional codes is less rich than 
that of block codes: whereas good convolutional codes have been found 
by computer search, good block codes result from the algebraic theory 
presented in this book. However, the importance of convolutional codes in 
practical applications is ever increasing. 
The theory of error-correcting codes is closely related to the theory of 
information, and the first part of this book is devoted to the foundations 
of information theory. Both of these theories were initiated by the pio-
neering paper of Claude Shannon (1948) in which he introduced entropy 
as a measure of information contained in an average symbol of a message. 
Shannon proved, inter alia, that entropy gives a precise estimate of how 
1 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

2 
INTRODUCTION 
much can be achieved by data compression. Combined with the famous 
HufTmann construction of the shortest code, this result of Shannon leads 
to a simple technique of data compression, presented in Chapters 2 and 3. 
(However, data compression is restricted to the case of information sources 
without memory.) The fourth chapter discusses the Fundamental Theorem 
of Shannon, which states that for every channel there exist error-correcting 
codes which remove noise while keeping the redundancy within the channel 
capacity. This result is purely theoretical: no algorithm for finding such 
codes has ever been found. The theory of error-correcting codes today has 
a lesser goal, viz., constructing codes with a reasonable redundancy and a 
fast decoder. 
Constructions of efficient error-correcting and error-detecting codes with 
fast decoders are presented in the second part of the book. Some of the 
constructions require a deeper background in modern algebra and geometry, 
and we provide a thorough presentation of the relevant topics. The most 
important classes of error-correcting codes are the following: 
Hamming codes (Chapter 5), perfect codes for single errors; 
Reed-Muller codes (Chapter 9), multiple-error-correcting codes with a par-
ticularly efficient and easily implemented decoder; 
Golay code (Chapter 10), the unique perfect code for triple errors; 
BCH codes (Chapters 12 and 13), strong multiple-error-correcting codes 
with a fast decoder; 
Convolutional codes (Chapter 14), multiple-error-correcting codes with 
memory. 
The last part of the book is a short introduction to modern cryptogra-
phy, stressing the role which error-correcting codes play here. Some of the 
well-known secret codes used in cryptography are based on constructions of 
error-correcting codes (e.g. the cryptosystem of McEliece, see 15.3). How-
ever, the main relation between cryptography and error-correcting codes is 
that, since noise is fatal for decryption, secret codes are usually combined 
with error-correcting codes. Furthermore, since encryption is costly, secret 
codes are usually combined with data compression. 
The book is organized in chapters numbered consecutively throughout 
the three parts. Each chapter is divided into sections, and cross-references 
are always related to the number of section. For example, Theorem 3.2 
means (the only) theorem in Section 3.2 of Chapter 3. 

Part I 
Coding and Information 
Theory 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

Chapter 1 
Coding and Decoding 
We are often faced with the task of converting a message, i.e., a sequence of 
symbols from a finite set (called a source alphabet), into a binary message, 
i.e., a sequence of O's and l's. The most common method is to translate each 
source symbol into a binary word. [ Word means precisely a finite sequence; 
we often write a\ai.. .an instead of the more precise (αχ,αι,..., 
a„). Thus, 
00101 is an example of a binary word.] Then the message is encoded symbol 
by symbol: we simply concatenate the words corresponding to the first, 
second, etc., symbol of the source message. The question of how to encode 
the source symbols is very important. There are two major criteria: we 
want to compress data, i.e., we want the resulting binary message to be as 
concise as possible, and we want to protect information against noise. These 
two requirements are rather contradictory, since by compressing data in the 
presence of noise, we are apt to increase, rather than decrease, the loss of 
information. In the present part, we therefore disregard noise: assuming 
that no errors occur, we try to find a concise code. 
In the present chapter we introduce the important class of instantaneous 
codes, i.e., codes which can be decoded letter by letter, and we show how 
to construct such codes. The construction of the shortest code will be 
presented in Chapter 2. 
1.1 
Coding 
Definition. Given finite sets A (source alphabet) and Β (code alphabet), 
a coding is a rule assigning to each source symbol exactly one word in the 
code alphabet, i.e., a function from A to the set of all words in B. We speak 
about binary coding if the code alphabet Β has two symbols. 
5 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

6 
CHAPTER 1. CODING AND DECODING 
Source Symbol 
Code Word 
1 
11000 
2 
10100 
3 
01100 
4 
10010 
5 
01010 
6 
00110 
7 
10001 
8 
01001 
9 
00101 
0 
00011 
Decoding 
01247 
Figure 1: 2-out-of-5 code 
Example of Binary Coding. 
Observe that among binary words of 
length 5, the number of those having two l's is (*) = 10. This can be 
used to the 2-out-of-5 code of decimal digits, see Figure 1. 
The message "173" has the following code: 110001000101100. Observe 
that no space is left between the code words since "space" is a code symbol 
too. Thus, for example, the famous Morse code has code alphabet Β = 
{ · , — , space}. 
How do we decode the 2-out-of-5 code? Of course, the first five binary 
digits correspond to the first decimal one, and after decoding them, we 
proceed to the second group of five binary digits, etc. A helpful mnemonic 
rule: use 01247 as a "weight" of the five columns, and add the weights of 
all l's in your words. Examples: 11000 ι—• 0 + 1 = 1 and 011001—• 1+2 = 
3. Unfortunately, 0 is an exception. 
Remark. Given a coding (i.e., a function Κ from A = {αϊ,.. . ,a„} to the 
set of all words in B), the words K(a\) 
K(a„) are called code words, 
and the set of all code words is called a code. When the concrete symbol 
in A is not important, "code" and "coding" are usually identified. 
1.2 
Unique Decoding 
Definition. For each coding Κ (of source symbols), we define the coding 
of source messages as the rule K
m, which to each word x j x 2 . . . a r m in the 

1.3. BLOCK CODES AND INSTANTANEOUS 
CODES 
7 
source alphabet assigns the word K*(xiXn 
.. x m ) = K(xi)K(x2) 
• • 
.K(xm) 
obtained by concatenation of the code words K(xi), i = 1, ... 
,m. 
The coding Κ is said to be uniquely decodable provided that arbitrary 
two distinct source messages have distinct codes. In other words, provided 
that K* is one-to-one. 
For example, the 2-out-of-5 code is uniquely decodable. The assignment 
of a binary word to "173" is a sample of coding source messages. 
In contrast, the following coding 
αι—>00 
6 ι—• 10 
ci—.101 
d>—.110 
e 
1001 
is not uniquely decodable: try to decode 10110. 
We now introduce two important types of uniquely decodable codes. 
1.3 
Block Codes and Instantaneous Codes 
We now introduce two important types of codes: instantaneous codes, 
which are codes of variable word lengths decodable symbol per symbol, 
and block codes, which are the special case of instantaneous codes with 
constant word length: 
Definition. (1) A coding using only pairwise distinct code words of a cer-
tain length η is called a block coding of length n. 
(2) A coding is called instantaneous provided that no code word is a 
prefix of another code word; i.e., if a source symbol has a code 6χ62 · · bn 
then no other source symbol has a code 6162 · b„bn+1.. 
.bm. 
Remark. Block codes (e.g., the 2-out-of-5 code above) are very convenient 
for decoding since we know in advance which code symbols correspond to 
the first (second, third, etc.) source symbol. And they are certainly efficient 
whenever all source symbols appear with equal frequency. However, if the 
frequencies of various source symbols differ substantially, then block codes 
become clumsy, and it is preferable to use instantaneous codes of variable 
lengths of words. 
Examples 
(1) The famous Morse code is exhibited in Figure 2. This is an instan-
taneous code with the code alphabet { · , — , space}. Since "space" 
is only used at the end of each code word, the decoding procedure is 
simple: we always look for the first "space". There is an obvious reason 
for not using a block code: the frequency of, say, "E" in the English 
language is much higher than that of "F". 

8 
CHAPTER 1. CODING AND DECODING 
A 
· — 
Ν 
Β 
0 
C 
— 
Ρ 
D 
Q 
Ε 
R 
F 
S 
G 
Τ 
Η 
υ 
I 
ν 
J 
· 
w 
Κ 
χ 
L 
Υ 
Μ 
ζ 
Figure 2: Morse code 
(2) An important example of a block code is the octal code: 
0 
000 
4 
100 
1 
001 
5 
101 
2 
010 
6 
110 
3 
011 
7 
111 
(3) Suppose that we are to find a binary coding for the alphabet {0,1,2,3}, 
and we observe that 0 appears much more often in source messages than 
any other symbols. Then the following coding seems reasonable: 
0,—•Ο 
li—»01 
2i—•Oil 
3·—• 111. 
We can decode quite easily: count the number of l's among the last 
three symbols of the encoded message. If the number is i, then the last 
source symbol is t. However, the above coding is not instantaneous. 
Indeed, when receiving a long message 
011111111111 
we will not know whether the first source symbol is 0, 1, or 2 until the 
message stops. 

1.4. SOME IMPORTANT 
BLOCK 
CODES 
9 
1.4 
Some Important Block Codes 
Long binary codes are difficult to handle. It is thus often suitable to group 
the binary symbols: we get shorter codes in more complex alphabets. For 
example, by forming groups of three symbols, we obtain the octal code, 
see 1.3. Representation by the octal code is usually indicated by the sub-
script 8. Example: 
(01) 8 = 000001. 
By forming groups of four binary symbols, we obtain the hexadecimal code 
in Figure 3. 
Binary 
Hexadecimal 
Binary 
Hexadecimal 
0000 
0 
1000 
8 
0001 
1 
1001 
9 
0010 
2 
1010 
A 
0011 
3 
1011 
Β 
0100 
4 
1100 
C 
0101 
5 
1101 
D 
0110 
6 
1110 
Ε 
0111 
7 
1111 
F 
Figure 3: Hexadecimal code 
A very important code used for a standard binary representation of 
alphabetic and numeric symbols is the ASCII code* (Figure 4). It has 
2
7 = 128 source symbols encoded into binary words of length 8: the first 
seven symbols carry the information, and the eighth one is set in such a way 
that the parity is even (i.e., each code word contains an even number of l's). 
The role of this eighth symbol is to enable detection of single errors—we 
explain this in detail in Chapter 5. For example, the letter A has code 
A 
<—• 
1000JM0, 
inform, check 
symbols 
symbol 
which in Figure 4 is represented by its seven information bits: l(01)s = 
1000001. 
'American Standard Code for Information Interchange 

10 
CHAPTER 
1. CODING AND 
DECODING 
Source 
Code 
Source 
Code 
Source 
Code 
Source 
Code 
Symbol 
Symbol 
Symbol 
Symbol 
@ 
1(00) 8 
> 
1(40)8 
NUL 
0(00)e 
SP 
0(40) e 
A 
1(01) 8 
a 
1(41)8 
SOH 
0(01) 8 
! 
0(41) 8 
Β 
1(02) 8 
b 
1(42)8 
STX 
0(02) 8 
0(42) 8 
C 
1(03)8 
c 
1(43)8 
ETX 
0(03) 8 
# 
0(43) 8 
D 
1(04)8 
d 
1(44)8 
EOT 
0(04) 8 
$ 
0(44) 8 
Ε 
1(05)8 
e 
1(45)8 
ENQ 
0(05) 8 
% 
0(45) 8 
F 
1(06)β 
f 
1(46)8 
ACK 
0(06) 8 
k 
0(46) 8 
G 
1(07) 8 
g 
1(47)8 
BEL 
0(07)8 
I 
0(47) 8 
Η 
1(10)8 
h 
1(50)8 
BS 
0(10)8 
( 
0(50) 8 
I 
1(11)β 
i 
1(51)8 
HT 
0(ll)e 
) 
0(51) 8 
i 
1(12)8 
J 
l(52)e 
LF 
0(12)8 
* 
0(52) 8 
Κ 
1(13)8 
k 
1(53)8 
VT 
0(13)8 
+ 
0(53) 8 
L 
1(14)8 
1 
1(54)8 
FF 
0(14) 8 
1 
0(54)8 
Μ 
1(15)8 
m 
1(55)8 
CR 
0(15) 8 
-
0(55) 8 
Ν 
1(16)8 
η 
1(56)8 
SO 
0(16)8 
0(56) 8 
0 
1(17)8 
0 
1(57)8 
SI 
0(17)8 
/ 
0(57) 8 
Ρ 
1(20)8 
Ρ 
1(60)8 
DLE 
0(20)8 
0 
0(60) 8 
Q 
1(21)8 
q 
1(61)8 
DC1 
0(21)8 
1 
0(61)8 
R 
1(22)8 
r 
1(62) 8 
DC2 
0(22)e 
2 
0(62) 8 
S 
1(23)8 
s 
1(63)8 
DC3 
0(23)8 
3 
0(63) 8 
Τ 
1(24)8 
t 
1(64)8 
DC4 
0(24) 8 
4 
0(64) 8 
υ 
1(25)8 
u 
1(65)8 
NAK 
0(25) 8 
5 
0(65) 8 
ν 
1(26)8 
V 
1(66)8 
SYN 
0(26)8 
6 
0(66) 8 
w 
1(27)8 
w 
1(67)8 
ETB 
0(27)8 
7 
0(67)8 
χ 
1(30)8 
X 
1(70) 8 
CAN 
0(30) 8 
8 
0(70)8 
Υ 
1(31)8 
y 
1(71) 8 
EM 
0(31)8 
9 
0(71)8 
ζ 
1(32)8 
ζ 
1(72)8 
SUB 
0(32) 8 
0(72)8 
[ 
1(33)8 
{ 
1(73)8 
ESC 
0(33) 8 
t 
0(73) 8 
\ 
1(34)8 
1 
1(74)8 
FS 
0(34)8 
< 
0(74) 8 
] 
1(35)8 
} 
1(75)8 
GS 
0(35) 8 
= 
0(75) 8 
1(36)8 
• 
1(76)8 
RS 
0(36)8 
> 
0(76) 8 
-
1(37)8 
DEL 
1(77)8 
US 
0(37)8 
? 
0(77) 8 
Figure 4: ASCII code (7 information bits) 

1.5. 
CONSTRUCTION 
OF INSTANTANEOUS 
CODES 
11 
Let us finally mention an "everyday" code we meet in most textbooks. 
It is called the international standard book number, ISBN, and it is a block 
code of length 10. (Various hyphens are often inserted between the symbols, 
but we can ignore them here since they are used just for optical orientation.) 
The code alphabet has 11 symbols: 0, 1, . . . , 9 and X (read: ten). For 
example, the book of Lin and Costello (1983) has 
ISBN 0 13-283796-X. 
The first number 0 denotes the country (USA), 13 denotes the publisher 
(Prentice-Η all), and the next six digits are assigned by the publisher as 
an identification number of the book. The last symbol is a check symbol 
(analogously as in the ASCII code above). It is set in such a way that for 
each ISBN code word 010203 . . .09010, the sum 
10 
^ i a i i _ i = 10oi + 9a 2 + 8a 3 + 
h 2a 9 + a 1 0 
• = 1 
be divisible by 11. For example, in the ISBN above: 
1 0 x 0 + 9 x 1 + 8 x 3 + 7 x 2 + 6 x 8 + 
5 x 3 + 4 x 7 + 3 x 9 + 2 x 6 + 1 x 1 0 
= 
132 = 1 1 x 1 2 . 
Some publishers have a three-digit identification (e.g., Wiley-Intersci-
ence has 471) and then they assign a five-digit number to each publication. 
1.5 
Construction of Instantaneous Codes 
Suppose you want to construct a binary instantaneous code of the source 
alphabet { α ι , . . . , ο „ } . It is sufficient to specify the lengths d\ 
d„ of 
the expected code words. In fact, we can certainly assume that di < 0*2 < 
·•• < d„. Then we choose an arbitrary binary word K(ai) of length d%. 
Next we choose a binary word /ί(α 2) of length d 2, but we avoid all those 
which have the prefix Κ(αχ). 
This is possible: the number of all binary 
words of length d2 is 2
d j . The number of those having the prefix K(ai) is 
2
dl~
dl 
(because you can choose the 0*2 — di digits remaining after the prefix 
K(ai) 
arbitrarily). Since 2
d* > 2
d7~
dl 
+ 1, we have at least one choice of 
K(a2). 
Next, we want to choose a word of length 0*3 which has neither prefix 
K(a\) 
nor K(o.2). 
Thus, from the 2
d* possible words, we must avoid all 
the 2
d3-
dl 
words with the prefix Λ'(αι) and all the 2
d*~
d3 
words with the 
prefix K(a2). 
This is possible if (and only if) 

12 
CHAPTER 1. CODING AND DECODING 
Dividing the last inequality by 2
d >, we obtain 
1 > 2~
dl + 2~
dl + 2~
di. 
Analogously, we can see that the following inequality 
1 > 2~
dl + 2~
d3 + · ·• + 2
_ d -
makes it possible to fulfil our task. It turns out that this inequality is both 
necessary and sufficient for a construction of instantaneous codes. 
Example. We want to find an instantaneous code with the same lengths 
of code words as that in Example 1.3(3) above. This is possible since 
1 > 2"
1 + 2 -
2 + 2 -
3 + 2
- 3 . 
Here is such a code: 
0 ι—• 0 
I . — - 10 
2 . — 110 
3 H - . 1 1 1 . 
1.6 
Kraft's Inequality 
Theorem. Given a source alphabet of η symbols and a code alphabet of 
k symbols, then an instantaneous code with given lengths d%, d-ι, ... , dn of 
code words exists, whenever the following Kraft's inequality 
k~
dl + k~
i3 + • • • + k-
d" < 1 
is fulfilled. 
PROOF. We can assume that the source alphabet {αχ, a j , . . . , a„} is pre-
sented in the order imposed by the lengths of the expected code words; i.e., 
d\ < di < · · < dn. 
We define instantaneous coding Κ by the following 
induction: 
(1) Choose an arbitrary word K(a.\) of length d\. 
(2) Suppose K(ai), 
K(a^), 
K(a,-i) 
have been chosen. 
Then 
choose an arbitrary word K(a,) of length d, with no prefix among K(ai), 
... , K(a,-i). 
This is possible: the number of words with the prefix K(ai) 
is k
d,~
di, 
and thus we have a choice of 

1.7. McMILLAN'S 
THEOREM 
13 
words. From Kraft's inequality, we get 
and multiplying by k
di, we conclude 
j - l 
k
d--S"k
d--
d' 
> 1. 
i=l 
• 
1.7 
McMillan's Theorem 
McMillan's Theorem. Every uniquely decodable coding satisfies Kraft's 
inequality. 
Remark. We see that Kraft's inequality is not only sufficient, but also 
necessary for the construction of an instantaneous code. However, McMil-
lan's Theorem says much more: instantaneous codes are just as efficient 
as uniquely decodable codes. More precisely, for every uniquely decod-
able code there exists an instantaneous code with the same lengths of code 
words. 
PROOF. Let AT be a uniquely decodable coding. Denote by d< the length 
of the code word K(ai), 
i = 1, 2 , . . . , n. Observe that for each number 
j = 1, 2, 3 , . . . , we can form exactly k> words of length j in the (i-symbol) 
code alphabet. 
By unique decodability, the number of source messages 
aj,a,- a.. .aj r whose code has length j cannot exceed lb'. The length of the 
code is d,, + dj 3 + • • · + dj r. Thus, we observe that the number of all sums 
of the form 
d<, + d > 3 + · · · + ύ 
is smaller or equal to W. 
It is our task to prove that the number 
+ dir = j 
(1.7.1) 
η 
is smaller or equal to 1. For this, we will verify that the numbers £ are 
bounded for all r = 1, 2, 3, ... . In fact, each number c > 1 clearly fulfils 

14 
CHAPTER 1. CODING AND DECODING 
limr_oo y = oo, and, therefore, the theorem will then be proved. Let us 
compute the powers of c: 
\ = 1 
' 
\ ' = 1 
' 
i,j = l 
and, in general, 
c
r = 
£ 
*-<*,+*.,+•••+*,.). 
(
1
7
2
) 
<i,. . , i , = l 
We can reorder the last sum by collecting all the summands ib
-', where j 
satisfies (1.7.1). The largest possible j is j = d + d + 
r d = rd, where 
d = m&x(di,..., 
</„). As observed above, the number of all summands k~* 
in sum (1.7.2) is smaller or equal to Jb
J. Thus, 
rd 
rd 
c
r < Σ V · 
1 = rd. 
Consequently, 
< d, which proves that c < 1. 
D 
Exercises 
1A 
What is the smallest length of a block code with the same source 
alphabet {A, B, ..., Z) and the same code alphabet { · , — , space} as 
the Morse code? 
1 
01 
A 
. 
1010 
2 
. 
Oil 
Β 
. . 
001 
3 
. 
10 
C 
. 
101 
4 
. 
1000 
D 
. 
0001 
5 
. . 
1100 
Ε 
. 
1101 
6 
. . 
0111 
F 
.. 
1011 
Figure 5 
Figure 6 

EXERCISES 
15 
I B 
Is the code in Figure 5 uniquely decodable? Is it instantaneous? Can 
you find an instantaneous code with the same lengths of code words? 
1C 
Is the code in Figure 6 uniquely decodable? If not, exhibit two source 
messages with the same code. 
I D 
Is the code in Figure 7 uniquely decodable? 
0 
.. . 
AA 
1 
.. . 
AABAB 
2 
. 
ABBBBB 
ABABA 
3 
.. 
. 
ABBBBB 
ABABA 
A 
. . 
001 
A 
. 
00 
4 
.. . 
ABBAA 
Β 
. 
1001 
Β 
. 
10 
5 
.. 
BABBA 
C 
. . 
0010 
c 
. 
011 
6 
. 
BBBAB 
D 
. 
1110 
D 
. 
101 
7 
. 
AAAABB 
Ε 
1010 
Ε 
. 
111 
8 
. 
AAAABA 
F 
. 
o n i o 
F 
110 
9 
. 
AAAAAB 
G 
. . 
0101 
G 
. 
010 
Figure 7 
Figure 8 
I E 
Can you decide unique decodability of the two codes in Figure 8 by 
using Kraft's inequality? 
I F 
Construct a binary instantaneous code for the following source al-
phabet with the prescribed lengths of code words: 
Symbol 
Length 
A
B
C
D
E
F
G
H
I
J
K
L 
2
4
7
7
3
4
7
7
3
4
7
7 
1G 
Construct a ternary (three code symbols) instantaneous code for the 
following source alphabet with the prescribed lengths of code words: 
Symbol 
Length 
1
2
3
4
5
6
7
8
9
0 
1
3
3
3
3
2
2
2
2
2 

16 
CHAPTER 1. CODING AND DECODING 
1H 
How many code symbols are needed if the following source alphabet 
is to be encoded into an instantaneous code with the prescribed lengths of 
code words: 
A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P 
1
2
2
2
1
2
2
2
1
2
2
2
2
2
1
2 
II 
Prove that for each instantaneous code for which Kraft's inequality 
is not an equality, it is possible to add a new source symbol and extend 
the given code to an instantaneous code (with the same code alphabet). 
Demonstrate this on the code found in Exercise 1.6. 
Notes 
Coding theory and information theory have their origin in the fundamental 
work of Claude E. Shannon (1948), reprinted in Slepian (1974), although 
many of the ideas were understood and used before. The small interesting 
book of Richard N. Hamming (1980), one of the founders of coding theory, 
provides further historical remarks. 
Kraft's inequality was formulated by Kraft (1949). The source of McMil-
lan's Theorem is McMillan (1956), the present proof is from Karush (1961). 

Chapter 2 
Huffman Codes 
We have mentioned that if the frequencies of source symbols vary, then 
instantaneous codes can be preferable to block codes: the most frequent 
symbols will be encoded into the shortest code words. We now make these 
considerations more precise. If the frequencies of source symbols are known 
exactly (i.e., if the probability distribution of source symbols in messages 
has been determined), we want to find the most efficient coding. 
2.1 
Information Source 
Definition. An information source is a source alphabet together with a 
probability distribution; 
i.e., a set {(ii,... 
,an) 
together with numbers 
Ρ(αι),..., 
P(a„) satisfying £ J L , P ( a ; ) = 1 and 0 < P(a.) < 1. 
More precisely, we should speak about a discrete, zero-memory infor-
mation source: discrete because we have discrete source symbols, and zero-
memory because we assume that the source symbols appear independently. 
In other words, the probabilities Ρ(α,) fully describe the statistics of the 
source messages, and the probability of a message di,a%3 • • • o, r can be com-
puted from the probabilities of the individual symbols: 
P(aitah 
... air) = P(ah) 
P ( a < a ) . . . P ( a < r ) . 
2.2 
Huffman Codes 
Let AT be a coding of an information source. That is, for each source 
symbol α<, we have a code word Λ"(α,) and we know the probability P(aj) 
17 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

18 
CHAPTER 2. HUFFMAN 
CODES 
of α,·. Denoting by d{ the length of the word K(o,), we can compute the 
average length L of code words: 
L = J2diP(ai). 
The most efficient coding is the one which has the smallest average length 
(because this means that the resulting code messages are as compressed as 
possible in the long run). By McMillan's Theorem 1.7 no harm is done if 
we confine ourselves to instantaneous codes. 
Definition. Given an information source S and a code alphabet, by a Huff-
man code is meant an instantaneous code with the minimum average length 
of code words; the minimum average length is denoted by 
Lmm(S). 
Example. Find a binary Huffman code for the source alphabet A, B, C, 
D, E, F if A appears twice as often as Ε and Ε twice as often as any 
consonant. 
Thus, we have the following information source: 
Symbol 
A 
Ε 
Β 
C 
D 
F 
Probability 
0.4 
0.2 0.1 
0.1 
0.1 
0.1 
We can assign a word of length 1 to A and a word of length 2 to E. Then 
the remaining lengths are 4 by Kraft's inequality: 1 + ^ + ψ = 1. Here is 
a such code: 
Α 
Ε 
Β 
C 
D 
F 
0 
10 1100 1101 1110 1111 
Its average length is 
L = 0.4 + 2 (0.2) + 4.4 (0.1) = 2.4. 
Is this Lmm(S)7 
Or would it be better to assign a word of length 3 to Ε 
in order to escape the length 4? The following algorithm answers such 
questions. 
2.3 
Construction of Binary Huffman Codes 
A source of two symbols has an obvious Huffman code with the code words 
0 and 1 [and Lmin(S) 
= 1]. A source with three symbols, a\, aj, as, of 
which αχ is the most probable, can be reduced to the case of two symbols, 

2.3. 
CONSTRUCTION 
OF BINARY HUFFMAN 
CODES 
19 
a\ and a 2 ( 3 , where P ( a 2 , 3 ) = P{o.i) + P(a 3). We find a Huffman code for 
the reduced source 
Ol 
<»2,3 
0 
1 
and then we "split" the code word 1 into two words, 10 and 11, thus ob-
taining a Huffman code for the original source: 
αχ 
a 2 
0 3 
0 
10 11 
In general, let S be an information source with symbols a\, o 2 , . . . , a„ 
ordered by probability, i.e., 
Ρ(αι) > P(o 2) > · · · > P ( a n ) . 
The reduced source S* has symbols a\, o 2 
a„_j and a new sym-
bol 
<*n-i,ni
 a n ( l 
probabilities are Ρ ( α ι ) , . . . , P(a„_ 2) (as before), whereas 
Ρ(α„_ι ι η) = P(a f i_i) + P ( a n ) . Suppose that we are able to find a Huffman 
code K* for the reduced source 5*. Then the following code 
"ι 
<»2 
··· 
α„_ 2 
a„-i 
an 
Κ·(αι) 
K*(a2) 
... 
A-(a„_ 2) 
/ f ( a „ _ , , B ) 0 
K'(an.1<n)l
 
( '
Λ
Ι
) 
[obtained by "splitting" the last code word ft"*(an_iin) into two words] is 
a Huffman code for S. This will be proved below. 
If we cannot find a Huffman code for S*, we continue in the same way, 
finding a reduction of 5* and a reduction of this reduction, etc. Eventually, 
we end up with two source symbols. 
Caution! Before further reducing the reduced source 5*, we must move 
the new symbol α η _ ι η in order to maintain the ordering by probability. 
Remark. Observe that the average length L(K) of the code in (2.3.1) is 
related to the average length L(K*) of the original code by 
L(K) = L(K') + Ρ(α„_ι) + P(a„). 
(2.3.2) 
In fact, if the lengths of code words of K* are d\, d 2,..., d n _ 2 ) d*, then 
the lengths of code words of Κ are d\, d 2,..., d„-2i d* + 1, d* + 1. Thus, 
n - 2 
L(K) 
= 
^diPia^ 
+ id'+ 
l)P(an^)-r(d*+ 
l)P(an) 
•=1 
n - 2 
= 
Σ 
diP(ai) + d" [Ρ(α„_,) + P(a n)] + P(a„_!) + P(a„) 
i = l 
= 
L(K') + P(an-i) 
+ P(a„). 

20 
CHAPTER 2. HUFFMAN 
CODES 
Theorem. Suppose thai K* is a Huffman code for the reduced informa-
tion source S*, then the code (S.S.I) is a Huffman code for the original 
information source S. 
PROOF. 
Let 
aii 
• · ·>
 an be all source symbols ordered by probability. We 
assume Ρ(α„) > 0 since the theorem is obvious if P(an) = 0. 
I. S has a Huffman code KQ with nondecreasing word lengths, i.e., 
di < d
2 < - ··<</„, 
where d, denotes the length of the word Κο(α,). 
To prove this, we start with an arbitrary Huffman code Κ for S. If there 
exists a symbol a,- such that d,- > di+i, denote by K' the code obtained 
from Κ by switching the code words for α,· and α<+ι. Then K' is certainly 
an instantaneous code, and the difference of the average lengths L = £ , m j n 
(of K) and V (of K') is 
Lmi„ — L' 
- 
[diP(ai) + d j + 1 P ( a 1 + , ) ] - [*+ιΡ(α,-) + *Ρ(α<+1)1 
= 
(άι-ά(+ι)[Ρ(α{)-Ρ(αί+ι)]. 
The last number is a product of a positive and a nonnegative number— 
thus, Lmi„ > L', and from the minimality we conclude that Lmin 
= V. 
In other words, K' is another Huffman code. We continue the process 
of switching "unsuitable" pairs of neighbor code words until we obtain a 
Huffman code KQ as above. 
II. S has a Huffman code Κ χ such that the last code words Λ*ι(α„_ι) 
and K\{an) 
differ only in the last symbol. 
In fact, let Ko be a Huffman code with nondecreasing word lengths (as 
in I) and let KQ be the code obtained from KQ by deleting the last symbol of 
the last code word K0(an). 
The average length of Ko is clearly smaller than 
that of Ko [since P(an) > 0] and, therefore, AO cannot be instantaneous. 
The word K(a{) — Κ(αΐ) (i < η — 1) is not a prefix of any code word; thus, 
K(a„) 
must be a prefix of some K(ai), i < η — 1. This is not possible if 
di < d„; therefore, d, — d n (since K0 has nondecreasing word lengths). 
The fact that A*(a„) is a prefix of Α"(α,·) implies that ΑΓ(α,) differs from 
K(a„) 
in the last symbol only. If i = η - 1, we put Κχ = KQ. If i < η - 1, 
observe that d,- = d„ implies d< = d,-+i = dj +2 = · · · = d„. Therefore, we 
can switch the code words of K0 for a,- and α„-χ. This gives us a code Κ χ 
of the same average length as Ko; thus, a Huffman code. 
III. We are ready to prove the theorem. Suppose that a Huffman 
code K* of the reduced source S* is given, and define a code Κ for S 
by (2.3.1). The average lengths L(K) and L(K*) of those codes are related 
by (2.3.2). 

2.4. 
EXAMPLE 
21 
Let us now use the Huffman code K\ of II above. Since the last two code 
words differ only in the last symbol, K\ can be obtained from an (obvious) 
code K* of S* by "splitting" the last code word. Moreover, K\ is clearly 
instantaneous. The same computation as in the remark above shows that 
L(Kt) 
- L(K\) = Ρ(α„_ι) + P(a„). 
Thus, the two differences are equal, and we conclude that 
L(K) = L(K\) - L(K{) + 
L(K*). 
Now L(K') 
= Lmin(S*) 
and, hence, -L(K*X) + L(K*) < 0. This proves 
that L(K) < L(Ki) - L m i„(S). Thus, Κ is a Huffman code. 
• 
2.4 
Example 
Let us compute a Huffman code for the source of Example 2.2. We start 
by reducing the symbols D and F to one symbol of probability 0.2. We 
must put the new symbol between A and Ε or betweeen Ε and B. Let us 
choose, for example, the former, and continue with the reduction process 
(see Figure 1). We finally obtain two symbols (of probabilities 0.6 and 0.4), 
1»' Reduction: 
2
n d Red. 
A ... 0.4 
0.4 
Ε . . . 0.2 
0.2 
Β . . . 0.1 
0.2 
C ... 0.1 
0.1 1 
D . . . 0.1 1 
0.1 j 0.2 
F . . . 0.1 ί 0.2 
0.4 
0.2 
0 
0 
• *4 
.2 J 0. 
0 
0 
0.2 
.4*" 
I 
0.6 \ 
nJ «Μ 
.2 ί 0.6 
Figure 1: Iterated reduction 
which are encoded by 0 and 1. We now proceed backwards by "splitting" 
each code word w assigned to a sum of two probabilities to two words, wO 
and wl (see Figure 2). 
The resulting Huffman code is the following: 
Α 
Ε 
Β 
C 
D 
F 
00 
11 010 011 
100 101 

22 
CHAPTER 2. HUFFMAN 
CODES 
1»' Splitting: 
2
n d Splitting: 
0.6 
0.4 
0.4 
0.4 
0.2 
2
n d Splitting: 
3
r d Splitting: 
4
t h Splitting: 
0.4 . . 00 
0.4 ... 00 
0.4 . . 00 
0.2 . . 01 —*| 
0.2 ... 10 — 
0.2 . . 11 
0.2 . . 10 
0.2 ... 11 
0.1 . . 010 
0.2 . . 11 
1 / 0 . 1 ... 010 
0.1 . . Oil 
\ 0 . 1 ... Oil 
fO.l . . 100 
. 101 
Figure 2: Iterated splitting 
Its average length is Lmin(S) 
= 2 (0.4)+ 2 (0.2)+ 3 (0.4) = 2.4. We conclude 
that the code found in Example 2.2 was indeed a Huffman code. The fact 
that Huffman codes of one source can be substantially different, as illus-
trated by the present example, is due to the possibility of various orderings 
of the reduced symbols by probability. 
2.5 
Construction of General Huffman Codes 
The idea of constructing a Huffman code in, say, the code alphabet 0, 1, 2 
is quite analogous to the binary case: reduce the three least important 
symbols to one, and, in the opposite direction, "split" the code word of 
the reduced symbol by concatenating 0, 1, 2 at the end. There is just 
one exception: the first (least important) reduction is performed on either 
three or just two symbols. For example, the source alphabet aj, aj, aa, a« 
requires a reduction of two symbols, whereas m, aj, as, 04, as requires a 
reduction of three. The rule is as follows: 
(1) order the η source symbols by probability, 

2.5. 
CONSTRUCTION 
OF GENERAL HUFFMAN 
CODES 
23 
(2) reduce the three least important symbols (n odd), or the two least 
important symbols (n even), and re-order by probability, 
(3) in the next reductions, always reduce the three least important sym-
bols and re-order by probability. 
Observe that after each reduction, the resulting number of symbols is odd 
(since by reducing the three least important symbols to one symbol, the 
number of source symbols is decreased by two). Thus, we eventually end 
up with one symbol of probability 1. Then we start the splitting process. 
In general, given a code alphabet of Jb symbols, we construct a Huffman 
code by repeated reductions of the k least important symbols. The only 
exception is the first reduction, when we reduce Jbo = 2, 3 , . . . , Jb symbols 
in such a way that Jb — 1 is a divisor of the number η — ko (of nonreduced 
symbols). 
Example. A ternary Huffman code for the information source in Exam-
ple 2.2 is constructed as shown in Figures 3 and 4. 
A 
Ε 
Β 
C 
D 
F 
Figure 3: Iterated reduction 
0.4 ... 0 —η 
0.4 ... 1 
0.2 ... 2 
0.4 
0.2 
1 
2 
0.4 
0.2 
0.1 
0.1 
1 
2 
01 
02 
Figure 4: Iterated splitting 

24 
CHAPTER 2. HUFFMAN 
CODES 
Symbol 
A 
Β 
C 
D 
Ε 
F 
G 
Η 
Prob. (l
a t source) 
Prob. ( 2
n d source) 
Prob. ( 3
r d source) 
1 
1 
0.1 
0.15 
1 
8 
0.2 
0.15 
1 
8 
0.1 
0.15 
1 
8 
0.3 
0.15 
1 
8 
0.05 
0.1 
1 
8 
0.1 
0.1 
1 
8 
0.05 
0.1 
1 
8 
0.1 
0.1 
Figure 5 
2 B 
Find ternary and quaternary (three and four code symbols) Huffman 
codes for the sources shown in Figure 5. 
2C 
Find the smallest number of code symbols necessary to construct an 
instantaneous code of average length L < 1.5 for each of the three sources 
of Figure 5. 
2D 
Find all binary Huffman codes for the source consisting of symbols 
A, B, C, D if A is twice more frequent than Β and Β is twice more frequent 
than either of C and D. 
2 E 
Describe binary Huffman codes of sources with equiprobable symbols 
(1) if the number of source symbols is a power of 2, 
(2) if it is not a power of 2. 
2 F 
Describe binary Huffman codes of sources satisfying p\ < pn-\ 
+Pn 
(where pi > p a > · · · > p„ are the source probabilities). Compare with 
Exercise 2E. 
Notes 
Huffman codes have been introduced by Huffman (1952) reprinted in Ber-
lekamp (1974). 
Exercises 
2A 
Find binary Huffman codes for the sources shown in Figure 5. 

Chapter 3 
Data Compression and 
Entropy 
Huffman codes can be used for data compression, in particular, when com-
bined with extensions of the information source (i.e., with dividing mes-
sages into bytes). We first show an example of how this compression is 
performed, and then we discuss its effectiveness. The discusion uses the 
concept of entropy, or amount of information in the message, introduced by 
Claude Shannon. We prove the famous Shannon's Noiseless Coding The-
orem, which states that Huffman codes of extensions represent the most 
effective data compression possible. (The reader should be aware of our 
standing assumption that we work with zero memory sources, see 2.1. Data 
compression for sources with memory is much more subtle.) 
3.1 
An Example of Data Compression 
Suppose we obtain a very large binary message, and we observe that it 
contains almost no l's. Say, there are nine times more O's than l's. This 
means, of course, that the information contained in the message is some-
what "thin". There are many methods of "compressing" the information. 
One is to divide the source message into bytes and encode the individual 
bytes. 
For example, let us use bytes per two symbols. Thus, the message is 
now presented in the following symbols: 00, 01, 10, 11. Since 0 has proba-
bility 0.9 and 1 has probability 0.1, we can compute the new probabilities 
(using our assumption of zero memory, see 2.1): 
25 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

26 
CHAPTER 3. DATA COMPRESSION AND 
ENTROPY 
Symbol 
00 
01 
10 
11 
Probability 
0.81 
0.09 
0.09 
0.01 
The Huffman code for this new information source is obvious: 
Symbol 
00 01 
10 
11 
Code 
0 
10 
110 
111 
Its average length is 
imin = 0.81 + 2 (0.09) + 3 (0.1) = 1.29. 
Thus, we need 1.29 bits to express a pair of source symbols; in other words, 
we need 
1 29 
— 
= 0.645 bits/symbol. 
This is a dramatic improvement to the 1 bit/symbol used without any cod-
ing. 
This whets our appetite to try larger bytes. 
For example, in byte 
length 3, we get the following Huffman code (which can be guessed, or 
constructed as in the previous chapter): 
Symbol 
000 
100 
010 
001 
110 
101 
110 
111 
Probab. 
0.729 
0.081 
0.081 
0.081 
0.009 
0.009 
0.009 
0.001 
Code 
0 
100 
101 
110 
11100 
11101 
11110 
11111 
Here L m i n = 0.729+ 3(0.243) + 5(0.028) = 1.598, which is the average 
number of bits per three symbols, and thus we need approximately 
^ — = 0.533 bits/symbol, 
ο 
Using this simple technique, we compress data to slightly more than one-
half of their original size! 
How much further can we compress? Can we really achieve one-half? 
Or even one-quarter? We now try to give clear answers to such questions. 
3.2 
The Idea of Entropy 
In the late 1940s, Claude E. Shannon introduced the concept of the "amount 
of information" contained in a message. We must be very careful when 
trying to understand the idea well: "information" has a semantic meaning 

3.2. 
THE IDEA OF ENTROPY 
27 
(what does the message say? what does it imply?) and a syntactic one (how 
does the message look?). We are absolutely disinterested in the semantics 
here because it is far beyond our aim of transmitting messages. Thus, our 
concept of "information" is purely syntactical. One can say that we regard 
messages in the same way a computer would: as sequences of symbols. In 
order to avoid misunderstanding, Shannon used the name entropy rather 
than information. 
Thus, for each information source S, we want to introduce a num-
ber H(S), called the entropy of the source S, which would express the 
intuitive idea of the "average amount of information" contained in one 
source symbol (or, from the opposite direction, the average uncertainty, 
i.e., the amount of information needed to specify one symbol of the source 
message). The main observation about the entropy H(S) is that it de-
pends only on the statistics of the source S, not on the names of source 
symbols. Since we assume that 5 is a zero-memory source (2.1), we can 
consider H(S) as a function of the probabilities pi,..., p„ of the individual 
source symbols. Thus, H(pi,... 
,pn) is a function assigning to each prob-
ability distribution (pi,... ,p„) (i.e., each η-tuple of nonnegative numbers 
summing up to 1) a number H(pi,... 
,p„) — the entropy of any source of 
information with the given probability distribution of source symbols. 
Now, the entropy function H(p\,.. 
.,pn) can be expected to have the 
following properties: 
(1) Positivity. H(pi,... 
,p„) is greater or equal to zero, and for some 
sources, it is positive. 
(2) Continuity. 
A small change in the probabilities causes only a small 
change in the entropy; i.e., the function i/(pi 
p n ) is continuous. 
(3) Symmetry. A change in the ordering of the source symbols does not 
change the entropy, i.e., 
H(pi, 
. . . , Ρ η ) = 
# ( P < ( 1 ) , - . · ι Ρ ί ( η ) ) 
for each permutation t of the numbers 1,..., n. 
(4) Coherence. The entropy of a source of η symbols οχ,..., an can be 
computed from entropies of smaller sources for the following reason. 
Suppose we read the message twice: the first time we fail to dis-
tinguish between the symbols ax and a 2. This means that we work 
with a source of η — 1 symbols of probabilities p, pa,..., p n . where 
ρ = pi + pj. On the second reading, we can ignore the symbols 
a3,...,a„ 
because we only want to distinguish oi from a 2 . Thus, we 
read a p-times smaller message consisting of a\'s (with probability 

28 
CHAPTER 3. DATA COMPRESSION AND 
ENTROPY 
and α2*8 (with probability 
= 1 — 
The first reading gives us 
the entropy H(p,pa,P4, • · • ,pn) and the second one, when a p-times 
smaller message is read, the entropy pH (^-, 
The two readings, 
then, give us the entropy of the original source: 
Η(ρ\,Ρ2,Ρ3,...,ρη) 
= 
Η(ρι + p 2, P 3 , · · ·, Pn) 
• 
( ρ , + ρ ^ - ^ , - ^ - ) . 
\ P l + P 2 
P 1 + P 2 / 
Remark. We have listed four "obvious" properties of entropy. Nobody is 
forced to accept them. But everyone who believes, as we do, that entropy 
must satisfy the properties above, obtains a unique formula for entropy: 
Theorem. There exists, up to a multiplicative constant k > 0, precisely 
one positive, continuous, symmetric, and coherent function, viz.* 
H(pi,...,pn) 
= * ^ P <
l o g j p 
where we add over all ρ, φ 0. 
We refrain from presenting the (very technical) proof due to Fade-
iev (1956) improving an analogous result of Shannon (1948). 
3.3 
The Definition of Entropy 
Theorem 3.2 is a clue to the following definition of entropy. The multiplica-
tive constant k is, obviously, just the choice of a unit of entropy. Let us 
express k in the form 
* = - ? - 
(forr = e-±). 
logr 
' 
Then, by virtue of the formula 
logjr 
logr χ = 
-
logr 
for logarithm of radix r, we get 
H(pi,... 
,p„) = ]T]p, logr j . 
'The natural logarithm is denoted by log, and the logarithm of radix r by logr. 

3.4. AN 
EXAMPLE 
29 
or, equivalently, 
H{pi, · · ·, p n) = - Σ,Ρχ logr Pi • 
The most common choice of radix is r = 2. The corresponding unit is called 
a bit. Observe that if S has two equiprobable symbols, then 
H(S)= 
i l o g 2 2 + i l o g 2 2 = lbit. 
Thus, one bit is the entropy of flipping an ideal coin once. 
The (unpleasant) condition of adding over all nonzero probabilities only 
is overcome by a (pleasant but imprecise) convention: 
Ologi = OlogO = 0. 
Summarizing, we get the following: 
Definition. By the entropy of an information source S with symbol prob-
abilities p\,. • •, pn is meant the number 
H(
s) = ^2
pi , o g 2 — = - Σ
 p«
log2 pi 
(
bit8)-
1 = 1
 
P * 
» = 1 
3.4 
An Example 
The entropy of the information source S in Example 3.1 is 
H(S) = -0.1 log2 0.1 - 0.9 log2 0.9 « 0.469 bits. 
Thus, by reading one source symbol, we obtain, in the average, less than 
half a bit of information. In 3.1, we have seen a concrete coding requiring, 
in the average, 0.533 bits per source symbol. It can be expected that no 
coding can compress data to below the 0.496 bits/symbol expressed by the 
entropy. We show below that this is indeed the case. 
More generally, for an arbitrary information source with two symbols, 
we have a probability distribution ρ, 1 — p, where ρ £ [0,1], and the 
corresponding entropy is 
H(p, 1 - ρ) = ρlog 2 - + (1 - p) log2 — — 
(bits). 
ρ 
1 - ρ 
Its graph is shown in Figure 1; observe how the positivity, symmetry, and 
continuity of entropy are demonstrated by the graph. 

30 
CHAPTER 3. DATA COMPRESSION AND ENTROPY 
3.5 
Maximum and Minimum Entropy 
Theorem. (1) The minimum entropy is H(S) = 0, and it is reached by 
precisely those sources S which have a symbol of probability 1. 
(2) The maximum entropy on an η-symbol source S is H(S) — log2 η 
bits, and it is reached by precisely those sources S in which all symbols are 
equiprobable, ρ,· = i. 
PROOF. (1) We have H(S) = £ > i
l o 8 a ^ - and both p< and log2 j - are 
nonnegative (since j - > 1). Thus, H(S) > 0, and if H(S) = 0, then for 
each i, either log2 ^- = 0 (which happens exactly if ρ,· = 1) or ρ,· = 0. We 
conclude that H(S) = 0 takes place precisely when all probabilities are 0 
or 1; the latter must then happen exactly once. 
(2) If all symbols are equiprobable, i.e., p { = i , then 
" 1 
#(S) 
= £ ~
,
o « 2
n 
= 
,
0 « 2
n 
(bits). 
i = l 
In order to prove that this is the maximum value, we use the following (eas-
ily established) fact from calculus: the natural logarithm log χ lies entirely 

3.5. 
MAXIMUM AND MINIMUM 
ENTROPY 
31 
below its tangent line in χ = 1 (which is the line y = χ — 1), see Fig. 2. 
y = χ - 1 
y = log a; 
Figure 2 
Thus, 
log χ < χ — 1 with equality only for χ = 1. 
(3.5.1) 
We now compute the difference between the entropy of any source 5 and 
the value log2 η = ΣΓ=ι Pi
 
n '• 
H(S) - log2 η 
= 
Σ ρ, log2 — - Σ Pi
 l o82
 
n 
i = l
 
P i 
i=l 
log 2 ( 1 - 1 ) = 0. 

32 
CHAPTER 3. DATA COMPRESSION AND ENTROPY 
Thus, H(S) < log 2n, and the equality takes place only if ^ 
= 1, i.e., 
p< = i , for all f. 
' 
• 
3.6 
Extensions of a Source 
In Example 3.1 we have seen the procedure of dividing messages into bytes 
of k (= 2 or 3) symbols. We now formalize this procedure by introducing 
the corresponding information source S
k of all Jb-tuples of source symbols. 
Then we will show that this process does not change the entropy per symbol. 
More precisely, the entropy of 5* is k times that of S (because it is related 
to a it-tuple of symbols of S). 
Definition. Let S be an information source with the source symbols 
αχ,..., 
a„. By the kth extension of S is meant the information source 5* 
of symbols α^α,,.. . a u with the probabilities 
P(ahaia 
...aik) 
= P(Qil) 
P(a< 3).. .P(a<J 
(for ι'ι, t ' 2 ( . . . , it taking arbitrary values 1,1, ... , n). 
For example, in 3.1, we have seen S
2 and S
3 for the binary source S 
with probabilities 0.9 and 0.1. 
Theorem. For each information source S, 
H(S
k) 
= 
kH(S). 
PROOF. We prove just that H(S
2) = 2H(S) and leave it to the reader to 
perform the (completely analogous) computation for Jb = 3, 4, ... . The 
source S
2 has symbols α,α;' with probabilities ρ,-pj, where p; denotes the 
ith probability in S (i = 1, . . . , n). Thus, 
H(S
2) 
= 
- έ $ > Ρ , 1 ο 8 3 ( ρ , ρ , ) 
i = l j = l 
η 
η 
= 
~ Σ 
Σ
 
PiP> 
(
Ι ο « 2 W +
 
1 θ82 Ρ) ) 
. = 1 j = l 
η 
η 
η 
η 
= - Σ &
 , ο « 2 Ρ ' Σ
 
p j
_ Σ
ρ ί Σ
 ρ>
 1ο&
 ρί 
ι=1 
j = l 
i = l 
j = l 
η 
= 
H(S) + ^2PiH(S) 
= 
2H(S). 
ι'=1 
• 

3.7. ENTROPY AND AVERAGE 
LENGTH 
33 
3.7 
Entropy and Average Length 
We now compare the average length L of a binary instantaneous code Κ 
with the entropy H(S). The number L tells us how many bits are required 
per symbol by the code K. The entropy H(S) expresses the average number 
of bits "contained" in a symbol. Whatever the construction of the code Κ 
may be, we expect that the actual number L of bits per symbol will not be 
smaller than H(S). This is what we prove now. 
Theorem. Every binary instantaneous code of a source S has the average 
length larger or equal to the entropy of S (in bits), i.e., 
L > H(S). 
PROOF. Denote by di the length of the ith code word. Then 
η 
η 
i = l 
i = l 
and we compute the difference 
η 
- 
η 
H(S)-L 
= 
£ > l o g
2 - - ] r > , l o g
2 2 * 
We now use the inequality (3.5.1): 
H(S)-L 
< 
log 2 ^ 
1 
η 
j 
Σ>(ϊ!>Γ-
1 
log 2 
2* 
By Kraft's inequality (1.6), we conclude that H(S) - L < 0. 
Q 

34 
CHAPTER 3. DATA COMPRESSION AND ENTROPY 
3.8 
Shannon's Noiseless Coding Theorem 
Let us once again return to Example 3.1. The source S has entropy H(S) = 
0.469 bit (per symbol), see 3.4. By coding the second extension S
2, we have 
obtained a code of average length 1.29 (bits per two symbols), thus getting 
imin(5
2)/2 = 0.645 bits per symbol. A better result is obtained by coding 
the third extension, which yields Lm\n(S
3)/Z 
= 0.533 bits per symbol. (See 
3.1.) This process will never compress data to below the entropy of the 
source. In fact, by applying Theorem 3.7 to the extended source S
k, we get 
Lmln(S
k)>H(S
k) 
= 
k-H(S) 
and, thus, 
L m i n
J b
( 5 t ) > H(S). 
One of the crucial results of information theory is that the compression can 
be arbitrarily close to the entropy H(S): 
Shannon's Noiseless Coding Theorem. For each information source, 
the average length Lmin(S) 
of a binary Huffman code is related to the en-
tropy H(S) by 
H(S) < Lmin(S) 
< H(S) + 1 
and, thus, the kth extensions S
k of the source fulfil 
Lmin(S
k) 
rj/c\ 
t 
l 
- —• Η (S) 
for k —• oo. 
k 
Remark. Two mistakes live on in the literature on information theory 
[ever since Shannon's paper (1948)]. One is minor: it is usually claimed 
that Lmin(S) < H(S) + 1; however, if S is a source of two symbols of 
probabilities 1 and 0, then Lmin(S) 
= 1 = H(S) + 1. The other one is 
presenting the simpler proof without assuming nonzero probabilities. 
PROOF. The latter statement follows from the former one by applying it 
to the source S
k [of entropy kH(S), see 3.6]: 
kH(S) < Lmin(S") 
< kH(S) + 1, 
In fact, divide the inequalities through ib: 
H(S) <
 I m i n
j f c
( 5 K ) < H(S) + I 
and use the sandwich theorem for limits to conclude that 
—• H(S). 

3.8. 
SHANNON'S NOISELESS 
CODING 
THEOREM 
35 
In view of Theorem 3.7, it remains to prove that 
Lmin(S) 
< H(S) + 1. 
(3.8.1) 
Let ai, 0 2 , • • • , an be the source symbols of 5 and denote by 
P(ai) = pi 
for / = 1, 2, ... , η 
their probabilities. 
(1) Suppose that pi,φ 0 for i = 1, 2, ... , n. For each i, there exists a 
(unique) natural number di such that 
l o g 2 - < < * ; < l + l o g 2 - . 
(3.8.2) 
Pi 
Pi 
We can construct an instantaneous code with code word lengths d\, di, 
... , d„. In fact, Kraft's inequality is satisfied: 
i=l 
i = l & 
'' 
i = l 
Pi 
i = l 
That instantaneous code has the following average length: 
l = Σ 
< Σ p<
i + Σ «
1 οβ2 - =
1 + # (
5 ) · 
i = l 
1 = 1 
i = l
 
Ρ ' 
This proves (3.8.1). 
(2) The case of an arbitrary source S (with pi — 0 for some i) can be 
derived from the above case as follows: choose a sequence S
k of sources 
on the same source symbols as 5 and with positive probabilities p\, p 2, 
• · · ι P*> which tend to the probabilities of S: 
lim pf = pi 
and 
pf > 0 
for all i = 1,2,..., n. 
[Such sources exist: if, say, p\ > 0, then put p* = p< + £ (i = 2 , . . . , n) and 
p* = P
l - 
for 
all k > ^-.] 
By (1) we know that for each it, 
W S * ) < l + 
H(S
k). 
Thus, to prove (3.8.1), it is sufficient to verify that 
Lmtn(S
k) 
-> Lmin(S) 
and 
H(S
k) 
- . H(S) 
for i t o o . 
The first limit is clear: the Huffman codes of sources S
k will not change 
infinitely many times. More precisely, there exists ito such that a Huffman 
code for S
k° will also be a Huffman code for all S
k with it > ito (and, thus, 
for S too). The latter limit follows from the continuity of entropy. 
• 

36 
CHAPTER 3. DATA COMPRESSION AND 
ENTROPY 
3.9 
Concluding Remarks 
1. For each source 5, we have introduced the entropy H(S), which is, 
roughly speaking, the average number of bits contained in a source symbol. 
No data compression can represent S by a smaller amount of bits symbol 
(see Theorem 3.6). 
2. An optimum data compression can be achieved by using Huffman codes 
of the extension S
k of the source S. 
3. We have, so far, restricted ourselves to binary codes. 
However, the 
same results hold for coding by r code symbols for any r > 2, except that 
logarithms of radix r must be used. That is, for the radix-r entropy, 
Hr(S) 
= Σ Pi
 l°8r Pi. 
one has, as in 3.8, 
Hr(S)<Lmin(S)<Hr(S) 
+ l, 
and hence 
ib 
Here L mi n(S*) is the minimum average length of an r-symbol code for the 
ibth extension of S. 
The proof is quite analogous to that of 3.8, and we leave it to the reader. 
Exercises 
3A 
A message is written in source symbols A, B, C, D, and A appears 
seven times more often than any of the other symbols. Find a binary coding 
which does not require more than 1.4 bits per symbol in the average. (Use 
extensions.) 
3 B 
Find the entropy of the following information source: 
Symbol 
Probability 
1 
2 
3 
4 
5 
6 
0.1 
0.1 
0.45 
0.05 
0.2 
0.1 
3C 
A shooter who hits the target with probability | shoots twice. An-
other shooter has the probability g of hitting the target, and he shoots 
three times. Whose target carries "more information" (i.e., has a larger 
entropy)? 

EXERCISES 
37 
3D 
Given information sources 5 t and S 2, denote by Si χ S 2 the informa-
tion source of all pairs of symbols (sj, « 2) with «< a symbol of Si (i = 1, 2). 
Prove that 
x S 2) < tf(S,) + / / ( S 2 ) 
and the equality takes place iff S\ and 5 2 are independent [i.e., the prob-
ability of (si,s 2) is the product of the probability of «i (in Si) and e 2 
(in 5 2)]. 
3 E 
A channel transmits equiprobable symbols 0 and 1. What is the 
probability of receiving the message 01101? What is the entropy of five-
symbol messages? 
3 F 
An information source consists of 128 equiprobable symbols. What 
is the length of a message of entropy 42 bits? 
3G 
What is the entropy of a message consisting of three words of length 8 
each, if the source alphabet has symbols A, B, C which 
(1) are equiprobable? 
(2) have probabilities 0.58, 0.33, and 0.09? 
3H 
The effectivity of an information source is defined as the ratio between 
the entropy (in bits) and the average length of a binary Huffman code. 
Prove that the effectivity lies between 0 and 1, and discuss the two extremal 
values. Find the effectivity of the three sources of Figure 5 in Chapter 2. 
31 
Find the effectivity of 
(1) the source A, B, C, D in which A has double the frequency of the 
other three symbols, 
(2) the second extension of that source. 
3 J 
What is the effectivity of a binary source S in which 0 has probabil-
ity 0.89? Find an extension of 5 with effectivity at least 90%. 
3 K 
Find binary instantaneous codes for the sources in Figure 4 and com-
pute their effectivity. 

38 
CHAPTER 3. DATA COMPRESSION AND ENTROPY 
Symbol 
A 
Β 
C 
D 
Ε 
F 
G 
Η 
Prob. ( I
s t source) 
Prob. ( 2
n d source) 
Prob. ( 3
r d source) 
6% 
1 
4 
0.2 
15% 
1 
4 
0.18 
15% 
1 
S 
0.16 
7% 
1 
8 
0.14 
5% 
1 
16 
0.1 
30% 
1 
16 
0.1 
18% 
1 
16 
0.6 
4% 
1 
16 
0.4 
Figure 4 
3L 
Suppose a huge binary message contains twice more O's than l's. 
Find a coding which uses 
(1) at most 0.94 bits per symbol 
(2) at most 0.9 bits per symbol. 
Notes 
Entropy in information theory has been introduced by Shannon (1948), who 
was inspired by entropy in physics (where logarithms of the probabilities 
of states had been used by Boltzmann already in 1896). Foundations of 
information theory are presented, e.g., in the excellent textbook of Abram-
son (1963), where some generalizations of Shannon's Noiseless Coding The-
orem can also be found. 

Chapter 4 
Reliable Communication 
Through Unreliable 
Channels 
So far we have not taken into consideration the role of noise in commu-
nication. In practice, communication channels and storage media add a 
certain level of noise to messages. We can use coding to restore the original 
information. Codes designed for this task are called error-correcting (or 
error-detecting) codes. They are typically block codes. 
The construction of good error-correcting codes goes, in a sense, in 
the opposite direction to the data compression studied in the preceding 
chapters: there we tried to minimize the redundancy, whereas here we 
create it. Observe that all natural languages contain a certain redundancy 
which is used for correction of errors. That is, our words contain more 
letters than what would be absolutely necessary for communication, and 
we are thus usually able to correctly read a sentence even if one or two 
letters are corrupted. In artificial languages (i.e., codes), designed without 
redundancy, this error-correcting capability is missing. (When we receive a 
binary word, there is no telling whether this word has actually been sent.) 
We thus extend code words by redundant symbols, making it possible to 
detect or even correct errors. These new symbols, called check symbols, 
make the coding less effective, and the fundamental task of the theory of 
error-correcting codes is to find good codes which would correct sufficiently 
many errors while remaining sufficiently effective. 
In the present chapter, we introduce the basic parameters of error-
correcting codes, and we introduce the surprising result of Claude Shannon, 
39 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

40 
CHAPTER 4. RELIABLE 
COMMUNICATION 
stating that each unreliable channel can be encoded with a specified level 
of effectivity in such a way that communication becomes reliable, i.e., prac-
tically error-free. Concrete constructions of good error-correcting codes are 
presented in subsequent chapters. 
4.1 
Binary Symmetric Channels 
We now turn to communication through channels which add a certain level 
of noise. This means that when we send a symbol through the channel, we 
can receive a different symbol. (We disregard the possibility of synchroniza-
tion errors, i.e., of receiving a symbol when none was sent, or not receiving 
any symbol although some was sent. In most of the usual applications, 
synchronization errors are less probable than corruptions of symbols.) 
For simplicity, we confine ourselves to binary symmetric channels, which 
means that 
(1) our communication is binary, i.e., the inputs and outputs of the chan-
nel are the symbols 0 and 1, 
(2) the probability ρ that when sending 1 we will receive 0 is equal to the 
probability that when sending 0 we will receive 1, 
(3) the channel has no memory, i.e., when sending a symbol, the proba-
bility of receiving 0 (or 1) does not depend on the preceding symbols 
sent. 
Although all results of the present chapter hold for a much wider variety of 
channnels, binary symmetric channels form the most frequently met model 
of communication (in data transmission, storage, etc.), and the idea of 
reliable communication is much easier to understand in such a simple model. 
The probability ρ of (2) above is called the error probability of the binary 
symmetric channel. The opposite probability of a correct transmission of a 
symbol is denoted by 
q = 1 - p. 
We can assume that 
0 < ρ < 0.5 
because if ρ > 0.5, then it is sufficient to switch the roles of 0 and 1 at the 
receiving end. 
As a concrete example, consider a binary symmetric channel which 
corrupts one symbol in a thousand (on the average), i.e., ρ = 0.001 and 
q = 0.999. In many applications, this is not a sufficient reliability. Let us 
assume that we cannot afford more than two corrupted symbols per ten 

4.1. 
BINARY SYMMETRIC 
CHANNELS 
41 
thousand. The first of idea of how to increase the reliability is to repeat 
each symbol several times. This is called a repetition code. For example, 
when we repeat three times, we use the following code A 3 : 
Thus, to each information symbol we, add two check symbols. Let us com-
pute the increase in reliability. When a\aia3 
is received, we determine 
the word sent by a majority vote: if at least two of the symbols α,· are 0, 
then 000 was sent; if at least two are 1, then 111 was sent. This decision is 
incorrect only if the noise has either corrupted three bits in a row (probabil-
ity p
3) or two in one word (probability 3pq
2, see 4A). Thus, the probability 
of error when K3 is applied is 
This is not yet enough, so we can try five repetitions. (Observe that four 
repetitions clearly would not help.) 
Again, we decode by majority vote. This is incorrect only if noise has cor-
rupted five bits (probability p
6) or four bits (probability 5p
4q) or three bits 
[probability ( ^ p
3 ?
2 ] . Thus, the code A"s has the following error probabil-
ity: 
0 
000 
1 
111 
Repetition code K3 
Perr(K3) 
= ρ
3 + 3p?
2 = (0.001)
3 + 3(0.001)(0.999)
2 « 0.003. 
0 
00000 
1 
11111 
Repetition code A"s 
= 
(0.001)
5 + 5(0.001)
40.999 + 10(0.001)
3(0.999)
2 
ss 
1 0
- 8 . 
This certainly is a major improvement. 

42 
CHAPTER 
4. RELIABLE 
COMMUNICATION 
It is obvious that repetition codes can make communication arbitrarily 
reliable. That is, the repetition codes K„ of length η have the property 
that Pcrr(Κn) tends to zero with increasing n. This simple technique is, 
however, too costly (from the point of view of time, space, or equipment). 
Therefore, we will try to find better codes. Before continuing, observe that 
an application of error-correcting codes makes the scheme of communica-
tions more complex (see Figure 1). We first turn the source message to 
Information 
source 
Source 
encoder 
Cha 
enc 
nnel 
oder 
Noise 
ι ι 
I 
I 
t 
Chanel 
User 
Source 
decoder 
Channel 
decoder 
Figure 1: Block diagram of a communication system 
the binary form (source encoder) and add the check bits (channel encoder). 
Then we send the message through the channel, where noise is added. In 
the opposite direction, we use the redundancy to estimate what code word 
has been sent (channel decoder), then we delete the redundant symbols 
(source decoder) and, finally, we translate to the form required by the user. 
Beware: when speaking of decoding in the realm of error-correcting 
codes, what is usually meant is the channel decoder (estimating the code 
word sent). 
4.2 
Information Rate 
In the case of the repetition code, we send a word of length n, but only 
one bit carries information, and the other η — 1 bits just repeat it—thus, 
they are redundant. In a more efficient code designed for noise protection, 

4.2. 
INFORMATION 
RATE 
43 
we often have the η bits of each code word divided into lb information 
bits which carry information (i.e., which can be chosen arbitrarily), and 
η — Jb cAecJb bits which are redundant (i.e., are completely determined by 
the information bits). The arbitrary choice of Jb bits then implies that the 
number of code words is 2
f c. Conversely, whenever a binary block code Κ 
of length η has 2* code words, we can say that each word "contains" Jb 
information bits and η — Jb check bits. More precisely, since the number 
of all binary words of length Jb is 2*, we have a bijection between those 2* 
words and all code words. Any such bijection provides an encoder which 
turns a /b-bit information word (a binary word of Jb symbols) into a code 
word (of length n). 
The ratio 
«*>-: 
of the number of information bits to the number of all bits sent is called the 
information rate of the code K. This important concept actually need not 
be restricted only to the binary case. Recall that a block code of length η 
in a given alphabet Β is a nonempty set Κ of words of length η in B. 
We call Κ trivial if it has one code word only; all other codes are called 
nontrivial. 
Definition. A block code Κ of length η in a code alphabet of r symbols is 
said to have the information rate 
provided that the number of all code words is r*. 
Remark. The information rate lies between 0 and 1. The two extreme 
values correspond to the trivial codes [Jb = 0, R(K) = 0], which carry no 
information, and to the codes in which every word is a code word [k = n, 
R(K) — 1], which provide no noise protection. In general, information rates 
near to 0 are typical for codes which are not very efficient from the point 
of view of time and/or space they require, but which often provide good 
noise protection. A code with information rate near to 1 is a very efficient 
code, but it often provides weaker noise protection. 
Examples 
(1) The repetition code has an information rate 1/n. 
(2) One of the most frequently used codes for error control is the even-
parity code which has η — 1 information bits and one parity check bit 
such that every code word has even parity (i.e., an even number of l's). 

44 
CHAPTER 4. RELIABLE 
COMMUNICATION 
Information Bits 
Code Word 
Information Bits 
Code Word 
000 
0000 
110 
1100 
100 
1001 
101 
1010 
010 
0101 
011 
0110 
001 
0011 
111 
1111 
Figure 2: The even-parity code of length 4 
The information rate is (n — l)/n = 1 — 1/n, and the code detects single 
errors. The encoding in the case η = 3 is shown in Figure 2. 
4.3 
An Example of Increased Reliability 
Let us return to the situation of a binary symmetric channel with the error 
probability ρ = 0.001, and to our requirement of at most two errors per 
ten thousand of symbols. Suppose our channel can transmit two bits per 
second, and the source message is a continuous flow of information of one 
bit per second. This means that we can add redundant bits, but the amount 
of the information bits must be at least one-half of the transmitted bits. In 
other words, we can only use codes Κ of information rates R(K) > 1/2. 
In this situation, repetition codes are no good, of course. In order to 
achieve the required increase in reliability, we use the same trick as for data 
compression in Chapter 2: we organize the source message into bytes. For 
example, if we use bytes of length k = 2, we can send words of length 
η = 4. A simple idea is to repeat the second bit three times; i.e., to use the 
code K\ in Figure 3. We decode as follows: we assume that the first bit is 
Information Bits 
Code Word 
00 
0000 
01 
0111 
10 
1000 
11 
m i 
Figure 3: Code K\ 

4.3. 
AN EXAMPLE OF INCREASED 
RELIABILITY 
45 
correct, and for the second bit we have a majority vote (like in the case of 
the repetition code K3 in 4.1). Thus, our decoding will be correct whenever 
all four bits are correct (probability q
A) or the first bit is correct and there 
is at most one corrupted bit in the word (probability 3pq
3, see 4A). The 
average error probability Pm(Kl) 
of our decoding then fulfils 
1 - PerT(Kl) 
= q
A + 3pq
3 = (0.999)
4 + 3(0.001)(0.999)
3 w 0.9997, 
i.e., 
PerrTO « 0.0003. 
This is not sufficient. 
To find a better code, we choose Jb = 3 and η = 6. We can use the 
code K* in Figure 4, which has the property that any two distinct code 
words differ in three bits or more. It follows that this code can correct any 
Information Bits 
Code Word 
000 
000000 
100 
100011 
010 
010101 
001 
001110 
011 
011011 
101 
101101 
110 
110110 
111 
111000 
Figure 4: Code K* 
single error: the received word differs in one bit from exactly one of the 
code words, the code word sent. Thus, we know which symbol to correct. 
In other words, we decode by finding the most similar code word. This 
decoding is correct whenever no error has occurred (probability q
6) or just 
one bit has been corrupted (probability 6pg
5, see 4A). Thus, 
1 - Perr(^e) > 9
6 + 6p<Z
5 * 0.99985, 
i.e., 
Perr(A"e) < 0.00015. 
We see that the percentage of computed bytes is better than required: 1.5 
per ten thousand. (The percentage of corrupted bits is, then, even slightly 
smaller, since a corrupted byte can have some correct bits.) 

46 
CHAPTER 4. RELIABLE 
COMMUNICATION 
We can ask whether an arbitrary reliability can be achieved this way. 
That is: do there exist codes A'* of information rate 5 such that the de-
coding error probabilities PeTT(K*) 
tend to zero with increasing n? We will 
see from Shannon's Fundamental Theorem that the answer is affirmative. 
This might whet our appetite to ask for more: can an arbitrary re-
liability be achieved with the information rate 0.9? (yes, by Shannon's 
Fundamental Theorem), or even 0.99? (no, by the Converse of Shannon's 
Fundamental Theorem). 
4.4 
Hamming Distance 
The main idea of error correction is that we want to have code words 
sufficiently "wide apart" in order to recognize the corrupted positions of 
the received word. For that reason, the following concept of distance of two 
words (not necessarily binary, but necessarily of the same length) plays a 
fundamental role:* 
Definition. Given two words a = a^-.-dn
 
and 
b = b\b2...bn, 
their 
Hamming distance is defined as the number of positions in which a and b 
differ. We denote it by d(a,b); 
d(a, b) = number of indices i = 1,2,..., η with α, φ 6<. 
For example, the repetition code has the property that the two distinct 
code words have Hamming distance n. In 4.3, we introduced the code Κζ 
in which arbitrary two code words have distance at least 3. We deduced 
that the code is capable of correcting single errors. We will see below that 
this is no coincidence. 
Proposition. For each alphabet and each number n, the Hamming distance 
is a metric on the set of all words of length n. That is: 
(1) d(a, a) = 0 and d(a, b) > 0 whenever a φ b, 
(2) d(a,b) = d(b,a), 
(3) d(a, b) + d(b, c) > d(a, c) 
(triangle inequality) 
for arbitrary words a, b, c of length n. 
'This i> the distance function suitable for the white noise, i.e., for the random distri-
bution of corrupted symbols (as opposed, say, to burst errors). 

4.4. 
HAMMING 
DISTANCE 
47 
PROOF. (1) and (2) follow immediately from the definition of the Ham-
ming distance. To verify (3), put d(a, b) = ν and d(b,c) 
= w. Thus, 
we have indices i j , . . . , iv in which a differs from b, and indices j \ , . . . , j
w 
in which b diners from c. Moreover, o, = 6,· whenever i φ i\ 
i„, 
and 6,· = Cj whenever i φ j \ , . . . , j
w . 
Consequently, α,· = c< whenever 
» φ ι'ι,..., iv, ji,..., 
j
w · 
Therefore, the number of indices in which a 
differs from c is at most ν + w. 
This proves that d(a, c) < υ + w = 
Example. Let us illustrate the role of the Hamming distance in the case of 
binary codes of length 3. There are altogether 8 binary words of length 3, 
and we can depict them as vertices of the corresponding cube (see Figure 5). 
The Hamming distance is just the number of sides of the cube on the path 
from one word to another. If all eight words are code words, then no noise 
protection is obtained, and the information rate is R = 1. The repetition 
code has code words 000 and 111 of Hamming distance 3 (see Figure 6). 
This code has information rate | , and it corrects single errors. Further, we 
have the even-parity code (see Figure 7). Arbitrary two code words have 
Hamming distance 2. Therefore, the code detects a single error, but cannot 
correct it. Its information rate is | . 
Remark. Now we can make precise the decoding rule used throughout 
the book* for error-correcting codes (which has already been illustrated in 
d(a,b) + d(b,c). 
• 
Figure 5: Binary words of length 3 
•The only exceptions are codes for burst-error protection, see 10.7. 

48 
CHAPTER 
4. RELIABLE 
COMMUNICATION 
Oil 
Figure 6: Repetition code 
Figure 7: Even-parity code 
several examples above). Suppose we are sending words of a block code Κ 
through a noisy channel. If we receive a code word, we assume that no error 
has occurred. In case the received word is not a code word, we find the code 
word a in X which has the smallest Hamming distance from the received 
word, and we assume that a has been sent. (If there are more choices of a, 
we pick one at random.) This is called the maximum 
likelihood 
decoding. 
4.5 
Detection of Errors 
The idea of error detection is clear: if we receive a word which is no code 
word, then we know that an error has been made. Recall that a block code 
of length η in the code alphabet Β is a nonempty set of words of length η 
in that alphabet. A block code Κ is said to detect t errors provided that 
for each code word a and each word a' obtained from a by corrupting 1, 2, 
t symbols, a' is not a code word. In other words, no two code words 
have Hamming distance 1,2 
t. This is an important property of codes 
used in situations where communication can be repeated whenever errors 
are detected. 
Definition. The minimum distance d(K) of a nontrivial 
block code Κ is 
the smallest Hamming 
distance of two distinct code words, i.e., 
d(K) = min { d(a, b) | a, b are words in Κ and a φ b } . 
Examples. The repetition code of length η has d — n. The even-parity 
code has d = 2. The code Κ* of 4.3 has d = 3. 

4.6. 
CORRECTION 
OF ERRORS 
49 
Proposition. A code detects t errors if and only if its minimum 
distance 
is greater than t. 
PROOF. If d(K) > t, then Κ detects 1 errors. In fact, suppose a is a code 
word and a' is a word which satisfies 1 < rf(a, a') < t. Then a' cannot be 
a code word, since else d(K) < d(a, a') < t. 
Conversely, if d(K) < t, then Κ does not detect ί errors. In fact, let a, a' 
be code words with d(a,a') = d(K). Then d(a, a') < t and the error which 
changes the originally sent code word a to the received word a' escapes 
undetected. 
• 
The concept of error correction is somewhat more subtle. A block code Κ 
is said to correct t errors provided that for each code word a and each 
word a' obtained from a by corrupting 1,2, ... ,t symbols, the maximum 
likelihood decoding (4.4) leads uniquely to a. In other words, the Hamming 
distance d(a, a') is strictly smaller than the Hamming distance from a' to 
any other code word. In symbols: for each code word a in A" and each 
word a' such that 1 < <i(a, a') < t, it follows that d(a,a') < d(b,a') for all 
words b in K, b φ a. 
Proposition. A code corrects t errors if and only if its minimum 
distance 
is greater than 2t. 
PROOF. Let d(K) < 2t. Then we will show that Κ cannot correct t arbi-
trarily distributed errors. In other words, we will find a code word a and a 
word a' which has Hamming distance t or less from a and yet its Hamming 
distance from a different code word is even smaller. To this end, let a, b be 
two code words with d(a, b) = d(K). 
Let i\, t j , . . . , i r be all the indices 
in which a differs from b. Then r < 2t. Suppose that we send a and the 
noise changes all the symbols α,· with i — ii, Μ, »e> •·· (i.e., all the even 
indices in which a and b differ) to the values in b. That is, we receive the 
word a', where 
Then, obviously, d(a,a') < § < < and yet cf(a',b) < d(a',a). This can lead 
to decoding a' incorrectly as b. 
Conversely, let d(K) > 2t. Then Κ corrects ί errors. In fact, suppose 
that we send a word a and we receive a word a' of Hamming distance 
4.6 
Correction of Errors 
if * φ h, «2 
ir, 
if »' = «1, »3, ··· , 
if i = t'2) 14, 

50 
CHAPTER 4. RELIABLE 
COMMUNICATION 
d(a,a') < t. We have the following situation with any code word b φ a : 
d(&, b) > d(K) > 2t and, by the triangle inequality (4.4), 
rf(a, a') + d(a', b) > d{a, b) > 21. 
Thus, 
d(a', b) > 2i - d(a, a') > 2t - r = t > d(a, a'). 
We conclude that the maximum likelihood decoding will (correctly) lead 
to a. 
• 
Remark. We see that error correction is more demanding than error de-
tection. For example, a code with the minimum distance d = 4 corrects 
single errors only, but it detects triple errors. However, in situations when 
no re-transmission is possible (e.g., in information storage), error-correcting 
codes have to be applied. In 8.6, we study the combination of detection 
and correction of errors. 
4.7 
Channel Capacity 
In this section, we introduce an important parameter of the binary symmet-
ric channel: its capacity. We first take a broader view and discuss discrete 
channels with an arbitrary number of inputs. Suppose a channel has inputs 
ii, i j , ... , x„ and outputs yi, j/2. · · • , Vm- As in the case of binary sym-
metric channels (4.1), we assume that the channel has no memory, i.e., the 
present output y, only depends on the present input z, and not on the pre-
vious inputs. Then the full function of the channel is known when we test 
what happens if XJ is put to the input for all j = 1, 2 
n. The result of 
such tests is the conditional probability distribution P(yi |*>), P(yi \
 
xj), 
• • · ι P(Vm I Xj) of the output under the condition that the input is Xj. This 
leads to the following mathematical model of a discrete channel: 
Definition. A discrete memoryless information channel consists of α fi-
nite set {x%,x2,. 
• •, x„ } (of input symbols), α finite set { y\, 1/2,...,y m } 
(of output symbols), and, for each j = 1, 2, ... , n, a probability distribu-
tion P(yi I Xj) on the set of output symbols. That is, P(y, | xj) are numbers 
between 0 and 1 (ii = 1, 2, ... , m and j = 1, 2, ... , n) such that for 
each j , we have 521=ι ^(ΐΛ I
 xj) — 
Examples 
(1) The case η = m = 2 and P(lto\x\) 
— P(yi |xa) = Ρ is the binary 
symmetric channel of Section 4.1. 

4.7. CHANNEL 
CAPACITY 
51 
(2) Suppose a channel with inputs 0, 1 has outputs 0, 1, E. Statistical test-
ing shows that 1% of the inputs results in the output Ε and 99% are 
transmitted correctly. Then the channel is described by the two prob-
ability distributions in Figure 8. 
0 
1 
Ε 
^(wio) 
0.99 
0 
0.01 
P{iH\i) 
0 
0.99 
0.01 
Figure 8: The description of a channel 
Remark. 
(1) Suppose we know the probability P(xi) of the input x, of an 
information channel. More succinctly, we know what information source: 
input 
x\ 
x2 
... 
xn 
probability 
P(xx) 
P(x2) 
... 
P(x„) 
is being fed into the channel (Figure 9). Then we can compute the prob-
S 
Pi 
x\ 
Pa 
* 2 
P3 
X3 
P4 
* 4 
channel 
channel 
channel 
channel 
channel 
channel 
channel 
channel 
Figure 9: An information channel with an information source 
ability f ( x ; , y , ) of the input/output pair of symbols by the well-known 
formula 
P(xi,yi) 
= P(xj)P(yi\xj). 
(4.7.1) 
The (marginal) probability distribution of the outputs is then given by 
%) = E ^ i j ) . 
(4-7.2) 

52 
CHAPTER 
4. RELIABLE 
COMMUNICATION 
We can use the formula (4.7.1) also in the reverse order: P(xj,y%) 
= 
P(.Vi,Xj) = P(Vi)P(xj I to)- Thus, we obtain the following expression for 
the probability Ρ(χ, |y,) that, when receiving j/j, the input is z,-: 
P
(
I
j
|
y
,
) - 
p(») 
-TrMPi*k)P{jH\>kY
 
(
4
7 '
3
) 
Example. 
(2) (continued) An information source at the input consists of 
the probabilities p 0 = P(0) and p t = P(i) = 1 - po- The probabilities 
P(zj,yi) 
and P(XJ |y,) are listed in Figure 10. Observe that P(XJ |y,) has 
Vi 
0 
1 
Ε 
0 
1 
Ε 
0.99p 0 
0 
O.Olpo 
^ ( 0 | » ) 
1 
0 
Po 
0 
0.99pi 
O.Olp! 
Ρ(ΐ\ν>) 
0 
1 
Pi 
Ρ(νύ 
0.99p 0 
0.99p! 
0.01 
Figure 10: Probabilities P(xj,t/i) and P(XJ |y<) 
the obvious interpretation: when receiving 0, the input must be 0; when 
receiving 1, the input must be 1; and when receiving E, the probabilities of 
0 and 1 have their original values, po, pj. 
Remark. 
(2) We want to introduce a measure of the amount of informa-
tion lost in the channel. In a noiseless channel, when receiving y,, we know 
what the input has been. In the presence of noise, some uncertainty re-
mains about the input, and we can measure its amount: the input symbols 
have probabilities P(zi|y,), P(x2\yi), 
... , P(zm\Vi), 
thus, the entropy 
of the input alphabet under the condition of receiving y< is 
η 
Η(Χ\1Η) 
= -ΣP(Xj 
I,«)log,P(xj I V i) 
(bits). 
i=i 
Taking the mean value over all output symbols y,, we obtain the expected 
uncertainty about the input after reading the output. This mean value is 
denoted by H(X \ Y). Let us compute it: 
η 
H(X\Y) 
= 
I»)/>(») 
<=1 
m 
η 
i=lj=l 

4.7. CHANNEL 
CAPACITY 
53 
η 
m 
Definition. Given an information channel and an information source S 
at its input, the number 
m 
η 
H(X\Y) = - Σ Σ
p ( * j '
l
o
g 2 ι
y
i
) 
( b i t 8 ) 
<=1 ; = 1 
is called the conditional input-output entropy. The number 
I(X,Y) 
= 
H(S)-H(X\Y) 
is called the mutual information. 
Example. 
(2) (continued) Prom the tables in Figure 10, we see that the 
conditional input-output entropy is 
H(X\Y) 
= 
0.99j> 0log 2l + 0 + 0.01polog2po 
+ 0 + 0.99p t log2 1 + O.Olpi log2 
P
l 
= 
0.01(p 0 log 2p 0 + pi log 2pi) 
= 
OMH(S). 
We can say that 1% of information is lost in the channel. The mutual 
information is 
I(X, Y) = H(S) - 0.01#(S) = 0.99ft(5). 
Remarks 
(3) The mutual information is the difference of the uncertainties about the 
input before and after reading the output. It depends on the informa-
tion channel and on the type of messages that are put into the channel, 
i.e., on the information source S. 
(4) A more compact formula for the mutual information is 
(Observe the symmetry of the inputs and outputs in the formula, which 
explains the term "mutual".) In fact, 
η 
πι 
η 
I(X, Y) 
= 
- £ 
P(ZJ)log2 
P ( x ; ) + 
P(x3, 
W ) log2 P(XJ 
| 
W) 
j=i 
«=ii=i 

54 
CHAPTER 4. RELIABLE 
COMMUNICATION 
= - Σ Σ ^ · * )
1 0 * * ^ 
i = l j = l 
m 
η 
η / 
\ 
+ Σ Σ ^ - ^ ΐ ο 8 2 ^ 
ΓΠ 
Π 
r j / 
\ 
(5) The maximum mutual information obtained by taking the most conve-
nient source S for the given channel is called capacity: 
Definition. By the capacity C of an information channel is meant the 
maximum value of the mutual information (over all information sources at 
the input). 
Theorem. The capacity of a binary symmetric channel of error probabili-
ty ρ is 
£ 7 = 1 - H(p, 1 - p) = 1 + Ρ bg 2 ρ + (1 - ρ) log 2(l - ρ) (bits). 
PROOF. Let S be an arbitrary information source of the input, given by 
the probabilities p0 = P(0) and pi = P(l). Using (4.7.1) and (4.7.2), we 
can compute the output probabilities p0 and p t of 0 and 1, respectively (see 
Figure 11), and we obtain an output information source S given by p0 = 
P(0) and pj = P(l). 
From the symmetrical formula for I(X,Y) 
in Remark (4), we conclude 
that 
I(X, Y) = H(S) - H(X IY) = H(S) - H(Y \ X), 
Figure 11: A binary symmetric channel with an input source S 
and an output source 3 

4.7. CHANNEL 
CAPACITY 
55 
where Η (Υ \ X) is the output-input conditional entropy: 
m 
η 
i=l j = l 
The probabilities P(y, | χ ; ) and P(yi,Xj) 
= P(xj,Vi) 
for the binary sym-
metric channel are shown in Figure 12. (Recall that q = 1 — p.) 
y. 
0 
1 
Vi 
0 
1 
9 
Ρ 
P{a,Vi) 
Po? 
POP 
Ρ 
9 
PiP 
PiQ 
Figure 12: Binary symmetric channel 
Thus, 
H(Y\X) 
- 
- p o 9 l o g 2 g - p o p l o g 2 p - p 1 p l o g 2 p - p 1 g l o g 2 o 
= 
- ( P o + P i ) 9 l o g 2 o - (Po+Pi)plog 2p 
= 
- « l o g 2 9 - p I o g 2 p 
= 
H(p,l-p). 
We conclude that H(X \ Y) is independent of the source S, thus, 
C = (max//(5)) - H(p, 1 - p). 
The value of ft(S) cannot exceed 1 bit. On the other hand, the value of 1 bit 
can be achieved by choosing po = pi = 0.5. In fact, using (7.4.1) and (7.4.2), 
we obtain p 0 = pj = 0.5, thus, maxH(S) 
= 1 bit. Consequently, 
( 7 = 1 - H(p, 1 - p) 
(bit). 
• 
Examples 
(3) The binary symmetric channel with error probability ρ = 0.001 has 
capacity 
C = 1 + 0.001 log2 0.001 -I- 0.999 log2 0.999 w 0.989 bits. 

56 
CHAPTER 4. RELIABLE 
COMMUNICATION 
(4) The binary symmetric channel with error probability ρ = 0.5 has ca-
pacity 
C = 1-7/(0.5,0.5) = Obits. 
This is a channel which mixes up 0's and l's completely, thus, it loses 
all the information. 
(5) The channel of Example (2) fulfils 
I(X,Y) 
= 
0.99H(S). 
The maximum value of H(S) is 1 bit; thus, the channel has capacity 
C = 0.99 bits. 
4.8 
Shannon's Fundamental Theorem 
We have observed in 4.1 that repetition codes K„ make communication in a 
binary symmetric channel reliable in the sense that their error probabilities 
Perr(Kn) tend to zero with increasing n. However, the information rates 
also tend to zero. We now present a surprising result of Claude Shannon 
concerning the probability of reliable communication with quite large in-
formation rates. Given a binary block code Κ of length n, we denote by 
Pm(K) 
the average error probability of the maximum likelihood decoding 
introduced in 4.4. In more detail, suppose we send a certain code word a 
in Κ. We can receive, in general, any binary word of length n. Some of 
the received words will be correctly decoded as a. The remaining words 
(e.g., all the other code words) will be decoded incorrectly, and the sum 
of the probabilities of receiving those words when a was sent is called the 
error probability related to a. Then Perr(K) 
is the mean value of the error 
probability related to all code words a of AT. (See Exercise 4B for a useful 
estimate.) 
Shannon's Fundamental Theorem. 
Every binary symmetric channel 
of capacity C > 0 can be encoded with an arbitrary reliability and with 
information rate arbitrarily close to C. That is, there exist codes K\, 
K2, 
Ka, ... 
such that Pm(Kn) 
tends to zero and R(Kn) 
tends to C with 
increasing n. In symbols: 
lim 
Perr(A'„) = 0 
and 
lim 
R(Kn) 
= 
C. 
n-»oo 
n-»oo 
SKETCH OF PROOF. It is our task to show that for an arbitrarily small 
number εχ > 0, there is a code Kn of length η with the information rate 

4.8. 
SHANNON'S FUNDAMENTAL 
THEOREM 
57 
R(K„) 
= C — ει and with an arbitrarily small value of Pm(.Kn)- 
The 
first requirement is met whenever we choose, out of the 2" possible binary 
words, Μ code words for Kn, where 
Μ = 2
n <
c -
i l ) . 
In fact, by the definition of the information rate (4.2), we then have 
η 
(We must be careful to choose our η in such a way that n(C — ει) is an 
integer. No problem: we can easily guarantee that C — ει is a rational 
number, say, C — ε χ — u/v, and then we choose η to be a multiple of v.) 
From now on, Ci is a constant, and we choose η and Μ binary words of 
length n. Whether or not the chosen words will form codes satisfying the 
second condition that the error probability tends to 0 depends on the choice, 
of course. 
The surprising and ingenious move in Shannon's proof is that, instead 
of trying to invent a clever way of choosing the Μ code words, he has 
shown that a random choice works. More precisely, given any number n, 
we can pick Μ out of the 2" binary words in a random way, and we obtain 
a random code K„. Each of these random codes satisfies R(Kn) 
= C — ει, 
as observed above, but the value of Perr(Kn) 
is now a random variable. 
Denote by Pe„ the mean value of this random variable (for a fixed value 
of n, but a completely random choice of Μ code words). The technically 
difficult part of the proof is to estimate the value of P
e
r
r in order to verify 
that 
Pen- — • 0 
as 
η —• oo. 
We omit the details [which the interested reader can find, e.g., in Ham-
ming (1980), where a very careful exposition is presented]. Now the proof 
is finished: given arbitrarily small numbers £i > 0 and £2 > 0, we can find 
η such that Perr 
< £2- Since the mean value is smaller than ε 2, then at 
least one realization PeTr(Kn) 
is also smaller than ε 2. Thus, K„ is a code 
with 
R(Kn) 
= C-ei 
and 
Perr(Kn) 
< ε2. 
• 
Remarks 
(1) Observe that the proof of Shannon's Theorem says more than the state-
ment, and this "more" is highly surprising: in every symmetric binary 

58 
CHAPTER 4. RELIABLE 
COMMUNICATION 
channel of capacity C, by choosing a random code of length η and infor-
mation rate C — ej, we can be sure that reliability increases arbitrarily 
with increasing n. 
However, this feature of the proof makes it absolutely nonconstructive: 
nobody would seriously advise a code designer to choose a good code 
at random. 
(2) No practical coding scheme realizing the parameters promised by 
Shannon's Theorem has ever been presented. Thus, coding theory (and 
practice) concentrates today on a smaller but practically very impor-
tant goal: finding good codes which are capable of the correction of a 
lot of errors. Here "good" means that the information rate is as high 
as possible (but not related to any concrete channel), and that there 
is an efficient decoding method. This is the subject of the subsequent 
chapters. 
(3) The Converse of Shannon's Fundamental Theorem is the following 
statement (which further explains the role of capacity): In every bi-
nary symmetric channel of capacity C, whenever codes K„ of lengths η 
have information rates at least C + ε (where ε > 0 is an arbitrarily 
small number), then the codes tend to be totally unreliable, i.e., their 
error probability tends to 1 with increasing n. In symbols: 
Λ(/ν„) > C + ε = > 
lim Pert(Kn) 
= 1. 
This has been proved by Wolfowitz (1959). 
Example. The binary symmetric channel with error probability ρ = 0.001 
has a capacity C « 0.989. It follows that there exist arbitrarily reliable 
codes with information rate R = 0.9. From the above Converse of Shan-
non's Fundamental Theorem, it further follows that codes with information 
rate R — 0.99 cannot be arbitrarily reliable (in fact, with increasing length, 
the reliability always decreases to zero). 
Exercises 
4A 
Suppose that a memorylese binary information source (2.1) has prob-
abilities ρ (for 0) and q = 1 - ρ (for 1). Let η symbols be emitted. 
(1) Prove that the probability that the binary word of length η has 1 on 
positions »Ί, » 2, ... , ι * and 0 on the remaining ones is 
p
kq
n~
k. 
(2) Conclude that the probability that the word has 1 on exactly ib posi-
tions is 
(l)p
kq
n~
k. 

EXERCISES 
59 
4 B 
Verify the following error estimates for codes in a binary symmetric 
channel of error probability p: 
(1) Prove that the probability that in a word of length n, precisely i bits 
are corrupted is (1)ρ^
η~'-
(2) Let A" be a binary block code of length η and minimum distance 2i 
or 2t + 1. Prove that 
[Hint: if t or less bits are corrupted, the maximum likelihood decoding 
is correct; this gives a lower bound on 1 — 
PeTr(K)] 
(3) Verify that the estimate in (2) holds with equality for the repetition 
codes. 
4C 
In a binary symmetric channel of error probability ρ = 0.1: 
(1) find the length of a repetition code Κ with PeTr(K) 
< 1 0
- 4 ; 
(2) find Perr(A'|) for the code A"' of 4.3. 
4D 
Prove that the mutual information I(X,Y) 
has the following prop-
(1) I(X, Y) > 0, with equality holding only if the inputs and outputs are 
stochastically independent. 
(2) I(X, Y) < H(S), with equality holding only if the outputs completely 
determine the inputs. 
4 E 
Let 5 be an information source at the input of an information chan-
nel, and S the information source at the output, given by (4.7.2). Prove 
that 
where H(S,H>) is the entropy of the input-output pairs [with probabilities 
given in (4.7.1)]. 
erties: 
I(X,Y) 
= H(S) + H(S~) - tf(S,S~), 

βο 
CHAPTER 4. RELIABLE 
COMMUNICATION 
Notes 
The first known error-correcting code was the (7,4)-Hammingcode invented 
by Richard Hamming, and mentioned already in the fundamental work of 
Shannon (1948). The progress which the theory of error-correcting codes 
has made in the 40 years of its existence is enormous. A thorough treatise 
devoted to various aspects of the theory is the monograph of Mac Williams 
and Sloane (1981). 
Shannon's Fundamental Theorem was proved by Claude E. Shannon 
in 1948. In our sketch of the proof, we have followed the detailed presenta-
tion in Hamming (1980). 

Part II 
Error-Correcting Codes 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

Chapter 5 
Binary Linear Codes 
For construction of effective error-correcting codes, and for good encod-
ing and decoding techniques of such codes, it is desirable to add more 
"structure" to the code alphabet. In this chapter, we discuss an algebraic 
structure of the binary alphabet. In subsequent chapters, we will take a 
broader view by structuring other code alphabets, and we will show how 
coding is related to linear algebra. 
We first show how binary codes can be described by equations, and then 
we use this description to introduce the important class of Hamming codes 
for single-error correction. 
5.1 
Binary Addition and Multiplication 
We define the following operations + (addition modulo 2) and · (multipli-
cation modulo 2) on the binary symbols 0, 1: 
+ 
0 
1 
0 
1 
0 
0 
1 
0 
0 
0 
1 
1 0 
1 0 
1 
Observe that multiplication is defined as if 0 and 1 were ordinary numbers, 
whereas the only "interesting" entry in the table of addition is 
1 + 1 = 0. 
This can be inerpreted as 
1 = - 1 , 
i.e., in binary computations, subtraction coincides with addition. 
In the subsequent chapter, we will see that the operations + and • turn 
the binary alphabet into a field Z 2 (and we will also explain why they have 
been chosen this way). 
63 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

64 
CHAPTER 5. BINARY LINEAR CODES 
5.2 
Codes Described by Equations 
Important binary codes can be described by systems of linear equations 
with binary coefficients (which practically means without coefficients: a 
variable is either present, if the coefficient is 1, or absent, if the coefficient 
is 0). For example, the repetition code (4.1) is described by the following 
equations 
xi 
= 
xa, 
Xl 
= 
X3, 
Xl 
= 
x„, 
or, which is the same, by the following homogenous system: 
X j + X
2 
= 
0, 
* l + * 3 
= 
0, 
Xi + x„ 
= 
0. 
In fact, the system has just two solutions, 000.. .0 and 111... 1, and those 
are precisely all code words. 
Another example: the even-parity code (4.2) is described by a single 
equation, viz., 
XI+X2+ 
r X
n = 0. 
In fact, every solution of that equation has an even number of l's (recall 
that 1 + 1 = 0!) and, conversely, every word with an even number of l's is 
a solution. 
Let us mention another useful binary code: the rectangular code. This 
is a binary code of length η = re, whose words are written (or, at least, 
considered) as r χ s matrices. Each row has r — 1 information bits and 
one parity check bit—thus, all rows have even parity. Analogously, each 
column has s — 1 information bits and one parity check bit. A typical code 
word for r = 4 and s = 3 is shown in Figure 1. This code corrects single 
errors: the error corrupts the parity of a single row (say, the tth one) and 
a single column (the jth one), and then we correct the entry aij. However, 
the theory of error-correcting codes provides us with better codes than the 
rectangular code. For example, the 8 x 4 rectangular code has length 32 
(21 information bits and 11 check bits). In comparison, the extended Ham-
ming code of length 32, which also corrects single errors, has 26 information 
bits (see 8.6), and the extended BCH code of length 32 has 21 information 

5.3. 
BINARY LINEAR 
CODES 
65 
/ 
1 
1 
1 
1 ' 
0 
1 
1 0 
1 0 
0 
1 
row parity check 
column parity check 
Figure 1: A code word of the 4 x 3 rectangular code 
bits and corrects double errors (see 12.2). Rectangular codes are suitable, 
e.g., for error correction of punch cards. 
Rectangular codes can also be described by equations. For example, 
the 4 x 3 code is described by the system of equations in Figure 2 (where 
the last equation is actually redundant: if all parities of rows and all but 
one parities of columns are even, then the last column parity must also be 
even). 
« π + αΐ2 + ai3 + a i 4 
= 
0 
021 + <>22 + <«23 + °24 
= 
0 
Ο31 + «32 + θ33 + a 3 4 
= 
0 
απ + β2ΐ + a 3 t 
= 
0 
012 + 022 + 032 
= 
0 
013 + 023 + Ο33 
= 
0 
014 + 024 + a 3 4 
= 
0 
Figure 2: Equations describing the 4 x 3 rectangular code 
5.3 
Binary Linear Codes 
Whenever a binary code Κ can be described by a system of homogenous 
linear equations, then its code words form a linear space, i.e., 
(1) a sum a + b of code words a and b (i.e., the word whose ith symbol 
is a, + bi) is a code word, 
(2) a scalar multiple Aa of a code word (whose ith symbol is λα,·) is a 
code word, for λ = 0,1. 

66 
CHAPTER 5. BINARY LINEAR CODES 
Observe that (1) implies that the zero word, 0 = 000.. .00, is a code word: 
in fact, a + a = 0, for any code word a. Further, observe that condition (2) is 
redundant since Aa is either a or 0a = 0. Thus, we can define an important 
class of binary codes using (1) alone: 
Definition. A binary block code is said to be linear provided that the sum 
of arbitrary two code words is a code word. 
Examples. The repetition code, the even-parity code, and the rectangular 
code are examples of linear codes. In contrast, the 2-out-of-5 code (1.1) is 
not linear. 
Remarks 
(1) As observed above, every homogenous system of linear equations defines 
a linear code (consisting of all the solutions). Conversely, as we will 
prove in 7.7, every linear code can be described by a homogenous system 
of linear equations. The importance of such descriptions for decoding 
will be seen in many instances of codes studied below. 
(2) If we send a code word ν and receive a word w, then by the error 
pattern, we mean the word e which has 1 in precisely those positions 
in which ν and w disagree: 
Γ 1 
if w. / ω,·, 
' ~ \ 0 
if ν,· = wi. 
It then follows that 
w = ν + e 
(i.e., we receive the sum of the code word which was sent and the error 
pattern). 
(3) We saw in 4.5 the importance of the minimum distance of a block code. 
We now prove that this concept can be simplified in the case of linear 
codes: 
Definition. By a Hamming weight of a (not necessarily binary) word is 
meant the number of symbols distinct from 0. For each nontrivial block 
code K, the smallest Hamming weight of a code word distinct from 0 = 
000.. .00 is called the minimum weight of K. 
Example. The word 11000 has Hamming weight 2. It is a code word of 
the even-parity code of length 5. Every code word of that code, except 
00000, has Hamming weight either 2 or 4. Thus, the minimum weight, of 
the even-parity code is 2. 

5.4. PARITY CHECK 
MATRIX 
67 
Proposition. For each nonirivial binary linear code the minimum distance 
is equal to the minimum weight. 
PROOF. Denote by d(K) the minimum distance and by w(K) the minimum 
weight. 
Let a be a code word of Κ of Hamming weight w(K). 
Then 
the Hamming distance of the code words a and 0 is, clearly, equal to the 
Hamming weight of a, thus, 
u)(A-) = if(a,0)><i(A-). 
To prove the opposite inequality, we choose code words a and b of 
Hamming distance d(K). Then a + b is also a code word (since Κ is linear) 
and it has O's precisely on those positions on which a and b agree. Thus, 
the Hamming distance of a and b is equal to the Hamming weight of a + b: 
d(K) = 
d(a,b)>w(K). 
Consequently, d(K) = 
w(K). 
a 
Corollary. A linear code corrects (or detects) t errors if and only if its 
minimum weight is larger than It (or larger than t, respectively). 
This follows from 4.6 and 4.5. 
• 
Example. The rectangular code (5.2) has the minimum distance 4. In 
fact, if a code word (a,j) has some nonzero entry, say, a j 0 j 0 = 1, then the 
t'oth row must contain at least two l's, and the same is true about the 
joth column. Thus, the code word of the smallest weight has the following 
form: 
0 
0 
Jo 
I 
0 
1 
0 
0 
1 
0 
0 
1 0 
0 
1 0 
0 
0 
Consequently, the rectangular code detects triple errors. 
5.4 
Parity Check Matrix 
Given a linear code Κ described by a system of homogenous equations,we 
denote by Η the matrix of coefficients of that system. That is, the t'th row 

68 
CHAPTER 5. BINARY LINEAR CODES 
of Η expresses the tth equation (in the sense that the jth entry is 1 if the 
jth variable is included, and 0 otherwise). As we will see in 7.5 below, the 
matrix notation of the system of equations is the following 
Η 
For example, the equations in 5.2 describing the repetition code take the 
following form: 
' 0 ' 
— 
0 
_ 0 . 
1 1 0 
0 0 
1 0 
1 0 
0 
1 0 
0 
1 0 
0 
0 
0 
0 
0 
0 
1 0 
0 0 0 
1 0 
0 0 0 
1 0 
0 
1 
" 0 ' 
X2 
0 
* 3 
— 
0 
Xn-l 
0 
0 
Definition. A binary matrix Η is called a parity check matrix of a linear 
binary code Κ of length η provided thai the code words of Κ are precisely 
those binary words x\x? .. .xn which fulfil 
Η 
Example. The even-parity code has a one-row parity check matrix: 
Η = [1111...1]. 
The 4 x 3 rectangular code (see Figure 2 in 5.2) has the following parity 
check matrix (provided that the code words are now written as words of 
length 12 by concatenating the three rows): 
' xi 
' 0 " 
X2 
= 
0 
0 
H = 
1 1 1 1 
0 
0 
0 
0 
0 0 
0 
0 
0 
0 
0 
0 
1 
1 1 1 
0 0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 1 
1 
1 
1 0 
0 
0 
1 0 
0 
0 
1 0 
0 
0 
0 
1 0 
0 
0 
1 0 
0 
0 
1 0 
0 
0 
0 
1 0 
0 
0 
1 0 
0 0 
1 0 
0 
0 
0 
1 
0 
0 
0 
1 
0 0 
0 
1 
\row parity 
checks 
column parity 
checks 

5.5. HAMMING 
CODES 
69 
Since the last equation in Figure 2 is redundant, another parity check matrix 
of the same code is obtained by deleting the seventh row of Η. 
Remark. Let A" be a linear code described by a system of linear homoge-
nous equations, none of which is redundant. Then each of the equations 
determines one bit of a code word. Thus, the number of equations (or rows 
of H) is the number of check bits, see 4.2. This will be made precise in 
Chapter 7, where we will also see that, conversely, each linear code with 
jb information symbols and η — Jb check symbols can be described by η — k 
equations. Thus, Η is an η by η — Jb matrix. 
5.5 
Hamming Codes—Perfect Codes for 
Single Errors 
We are now going to introduce an important class of binary linear codes, 
the Hamming codes, whose basic properties are listed in Figure 3. In Sec-
tion 8.6, we shall investigate the closely related class of extended Hamming 
codes with properties listed in Figure 4. 
Length : 
Information symbols: 
Minimum distance: 
Error control capability 
η = 2
m - 1 
Jb = 2
m - m - 1 
d = 3 
Perfect codes for 
correcting single errors 
Figure 3: Properties of Hamming codes 
Length: 
Information symbols: 
Minimum distance: 
Error control capability: 
Corrects single errors and 
detects double errors (8.6) 
η = 2
m 
Jb = 2
m - m - 1 
d = 4 
Figure 4: Properties of extended Hamming codes 

70 
CHAPTER 5. BINARY LINEAR 
CODES 
Proposition. A binary linear code Κ corrects single errors if and only if 
every parity check matrix of Κ has nonzero and pairwise distinct columns. 
PROOF. Denote by di the binary word with all zeros except a 1 in the 
ith position, and by d,j the binary word with exactly two l's in the itb and 
jth positions. (All nonzero binary words except dj and d,j have Hamming 
weight at least 3.) 
I. Let Κ correct single errors, i.e., have minimum weight at least 3 
(see 5.3). Let Η be a parity check matrix for K. If the ith column of Η 
is an all-zero column, then the product of Η and d, (written as a column) 
is 0: 
Η 
' 0 ' 
' 0 ' 
0 
0 
0 
1 
i 
0 
0 
0 
This implies that the word d, is a code word of K, in contradiction to the 
minimum weight at least 3. Similarly, if the ith column of Η is the same as 
the jth column, j φ i, then the word 
is a code word—a contradiction 
again. 
II. Let Η be a parity check matrix for a code K, and let Η have nonzero 
and pairwise distinct columns. Then none of the words d, can be a code 
word: if it were, we would have 
" 0 ' 
' 0 " 
0 
0 
0 
1 
0 
0 
0 
However, the left-hand side is clearly equal to the ith row of H, which is 
nonzero by assumption. Also none of the words dij can be a code word 
(because by multiplying Η with the transpose of dy, we clearly obtain the 
sum of the ith and jth columns of Η, which is nonzero by assumption). 

5.5. HAMMING 
CODES 
71 
Consequently, every nonzero code word has Hamming weight at least 3. 
This implies that d(K) > 3 by 5.3. Thus, Κ corrects single errors. 
Ο 
Remark. The above proposition suggests a procedure for constructing ef-
ficient codes correcting single errors: design the parity check matrix as a 
matrix with the largest number of nonzero pairwise distinct columns. In 
this way, we obtain single-error-correcting codes of the largest number of 
information symbols (see Remark 5.4), and, thus, with the best information 
rate. 
Observe that if the parity check matrix has m rows (i.e., its columns 
are binary words of length m), then the number of columns is smaller or 
equal to 2
m — 1. In fact, of the 2
m possible binary words, we consider all 
but one, the all-zero word. This leads us to the following: 
Definition. A binary linear code is called a Hamming code provided that 
it has, for some number m, a parity check matrix Η of m rows and 2
m — 1 
columns such that each nonzero binary word of length m is a column of Η. 
Example. 
(1) For m = 3, the parity check matrix has 3 rows and 7 col-
umns. It is not uniquely determined by the above definition: we can choose 
any ordering of the columns. A convenient approach is to use as columns 
the binary expansions of the numbers 1, 2, ... , 7: 
Η = 
0 
0 
0 
1 
1 
1 1 
0 
1 1 0 
0 
1 1 
1 0 
1 0 
1 0 
1 
This means that the code is determined by the following equations 
X4 + XS + X6 + X7 
= 
0, 
X2 + X3+ 
X6 + X7 
= 
0, 
(5.5.1) 
X\ + 
Xz + 
Xb + 
X7 
= 
0. 
It is easy to see that (5.5.1) is equivalent to the following: 
x& = 
x? + xa +
 
χ4, 
X6 
= 
X1 + X3 + X4, 
(5.5.2) 
Χγ 
= 
Χι + X2 + X4. 
We conclude that the symbols xj, ... , 1 4 can be chosen arbitrarily (i.e., 
these are the information symbols), and the last three symbols are the check 
symbols. Thus, this Hamming code has the information rate R = 4/7. 

72 
CHAPTER 5. BINARY LINEAR CODES 
information 
bits 
Up for 
the first 
4 bits 
Down for 
the last 
3 bits 
Figure 5: An encoder for the Hamming code of length 7 
Encoding. Given a binary word of length 4, we encode it to a code word by 
concatenating the three symbols 2 5 , xe, and X7 computed as in (5.5.2). 
A shift-register encoder is shown in Figure 5, where φ denotes a binary 
adder (i.e., an exclusive-or gate) and 
a shift register stage (i.e., a 
flip-flop). All code words of the Hamming code are listed in Figure 6. 
Information 
Code word 
Information 
Code word 
Symbol 
Symbol 
0000 
0000000 
0 1 1 0 
0 1 1 0 0 1 1 
1000 
1000011 
0101 
0101010 
0100 
0100101 
0011 
0011001 
0010 
0010110 
1 1 1 0 
1110000 
0001 
0 0 0 1 1 1 1 
1 1 0 1 
1 1 0 1 0 0 1 
1100 
1 1 0 0 1 1 0 
1 0 1 1 
1 0 1 1 0 1 0 
1010 
1 0 1 0 1 0 1 
0 1 1 1 
0 1 1 1 1 0 0 
1001 
1001100 
1 1 1 1 
1 1 1 1 1 1 1 
Figure 6: Code words of the Hamming code of length 7 

5.5. HAMMING 
CODES 
73 
Decoding. The method of correcting single errors when a Hamming code 
is used is very simple. When receiving a word w = W1W2 • • .107, compute 
the left-hand sides of the equations (5.5.1) above (binary addition). The 
results are denoted *i, s 2, and s 3, respectively, and the binary word «182*3 
is called a syndrome of the received word. 
(a) If the syndrome is 000, then the received word is a code word (and 
there is nothing to correct). 
(b) If the syndrome is different from 000, then it is a binary expansion 
of some number i = 1, 2, ... , 7. Then correct the ith bit «ν,· of the 
received word w. 
For example, we send 1111111 and receive 1110111. The syndrome is 
«i 
= 
u/4 + U J 5 + we + W7 
— 
1, 
«2 
= 
«>2 + UJ3 + U>6 + W7 
= 
0, 
«3 
= 
W\ + W3 + 
U> 5 + U>7 
= 
0. 
Since «ιβ2«3 = 100 is the binary expansion of 4, we correct u>4, and we 
conclude that 1111111 has been sent. 
In order to explain why this decoding technique works, observe that in 
the matrix notation, we have 
«1 
«2 
«3 
= Η 
U>2 
ID 7 
Suppose we send a code word ν and one bit is corrupted. Then we receive 
the word w = ν + d,, where d, is the word with all zeros except one 1 in 
the tth position. Since ν is a code word, its product with the matrix Η is 
the all-zero word, and, thus, 
/ 
«1 
s 2 
«3 
Η 
" «ι ' 
" 0 " 
" 0 ' 
0 
0 
va 
+ 
1 
i 
= Η 
1 
; 
.
 
W 7 . 
0 
1 
. 0 
= ith column of H. 
Finally, the ith column of Η is just the binary expansion of i; thus, we 
correct the corrupted bit. 

74 
CHAPTER 5. BINARY LINEAR CODES 
Examples 
(2) For m = 4, we have the following parity check matrix based on the 
binary expansions of 1, 2, ... , 15: 
"
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
' 
„ _ 
0
0
0
1
1
1
1
0
0
0
0
1
1
1
1 
0
1
1
0
0
1
1
0
0
1
1
0
0
1
1 
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1 
This Hamming code has length η = 15 and the number of check symbols 
is 4. Thus, there are 11 information symbols and the information rate 
is R = 11/15 « 0 . 7 3 . 
The error correction is performed quite analogously to the case when 
m = 3: by multiplying the matrix Η with the received word (as a 
column), we obtain a 4-symbol syndrome βιβ2«3*4· If the syndrome is 
nonzero, then it is a binary expansion of a number i = 1,2, ... , 15, 
and we correct the ith bit. 
(3) For η = 5, the Hamming code has length 31, with 5 check symbols and 
26 information symbols. Here R rn 0.81. 
Concluding Remarks 
(1) For every m = 2, 3, 4, ... , we have a Hamming code of m check 
symbols. It has 
length η = 2
m — 1 (= the number of all nonzero binary words 
of length m, i.e., columns of the matrix H), 
number of information bits k = 2
m - 1 - m, 
minimum distance d = 3. 
The last can be seen as follows: choose the parity check matrix whose 
columns are the binary expansions of the numbers 1, 2, ... , 2
m — 1. 
Then the word 11100...00 is clearly a code word; thus, d < 3, see 
Proposition 5.3. We have seen a simple decoding technique correcting 
single errors above; thus, d > 3, see Corollary 5.3. 
(2) Hamming codes are perfect for single-error correction because they have 
the following property: every binary word of length η is either a code 
word or it has a Hamming distance 1 from (precisely one) code word. 
In fact, this property can be readily deduced from the above decoding 
technique. And this implies that Hamming codes use the absolutely 
smallest possible redundancy for correction of single errors. We will 

5.6. 
THE PROBABILITY 
OF UNDETECTED 
ERRORS 
75 
explain the idea of perfect codes in more detail in 10.6, but let us re-
mark here that there is a shocking lack of perfect binary codes: besides 
Hamming codes and some trivial cases, only one perfect code exists (a 
triple-error correcting-code of length 23). 
(3) The information rates of Hamming codes 
tend rapidly to 1 with increasing m. On the other hand, the larger m 
we choose, the smaller error protection we obtain, of course. 
5.6 
The Probability of Undetected Errors 
Let us return to the binary symmetric channel (4.1). When a binary linear 
code Κ of length η is used, what is the probability that an error escapes 
undetected? In other words, what is the probability that a code word ν is 
sent and a different code word w is received? 
To answer this question, denote by 
the error-pattern word (5.3). Then an undetected error occurs if and only if 
e is a nonzero code word. (In fact, if e φ 0 is a code word, then w = ν + e 
is a code word, and w φ v. Conversely, if w is a code word, then so is 
e = w — v = w + v, and w φ ν implies e φ 0.) Now, suppose that the 
Hamming weight of e is i, i.e., i bits have been corrupted. The probability 
of this error pattern is p
lq
n~', where ρ is the error probability of the binary 
symmetric channel, and q = 1 — ρ (see Exercise 4A). Thus, if we put 
then the probability P u nd of an undetected error is the sum of the probabil-
ities p
,q"~
i, where each of these summands appears Ai times for i = 1, 2, 
... , n. That is, 
w — ν 
Ai = the number of code words of Hamming weight i, 
η 
(5.6.1) 
Observe that A\ = A2 = · · · = Ad-ι = 0, where d is the minimum weight 
of the code—thus, P u n d = 
Y^-dAip
iq
n-
i. 

76 
CHAPTER 5. BINARY LINEAR CODES 
Example. The Hamming (7,4)-code in Figure 6 (see 5.5) has one code 
word of Hamming weight 0 (viz. 0), seven code words of each of Hamming 
weights 3 and 4, and one code word of Hamming weight 7. Thus, 
fund = 7 p V + 7 p Y + p
7. 
Suppose we use this code in a binary symmetric channel of error probability 
ρ =0.01. Then 
Pund = 7(0.01)
3(0.99)
4 + 7(0.01)
4(0.99)
3 + (0.01)
7 μ 7 χ ΙΟ
- 6, 
which means that seven words in a milion are incorrectly taken to be error-
free (on the average). 
Definition. // Κ is a block code with exactly Ai code words of Hamming 
weight i (i = 0 , 1 , . . . , n), then the polynomial 
Λ(*) = £ > χ < 
i=0 
is called the weight enumerator of the code K. 
Proposition. Let Κ be a binary linear code of length η with a weight 
enumerator A(x). 
The probability of an undetected error when using the 
code Κ in a binary symmetric channel is 
where ρ is the error probability of the channel, and q = 1 - p. 
PROOF. The above expression (5.6.1) for P u nd can be multiplied and di-
vided by q
n to obtain 
Pund = q
n Σ 
= ί" Σ
 
Α ' (
£)' 
· 
1=1 
ϊ=1 
' 
Since Αο = 1 (the only code word of Hamming weight 0 is 0), we can rewrite 
the last sum as follows 
η 
Pund = q" Σ*(ί)'-·]-νΚί)-Ί· 
Ί=ο 
*
 
J 
* 
Α 

EXERCISES 
77 
Example (continued). The Hamming (7,4)-code has the weight enumera-
tor 
Exercises 
5A 
Is the binary code of all palindromes (i.e., words which are the same 
when read backward or forward) linear? Describe it by equations, and 
determine the number of errors it detects. 
5 B 
Describe an algorithm for detection of triple errors when the rectan-
gular code (5.2) is used. 
5C 
Let Κ be the binary code of all words of length 7 such that (a) the 
third bit is a parity check for the first two bits, (b) the sixth bit is a parity 
check for the fourth and fifth bits, and (c) the last bit is an overall parity 
check. Describe Κ by equations and determine the number of errors it can 
correct or detect. 
5D 
The rectangular code (5.2) can also be called a two-dimensional even-
parity code. Define, more generally, m-dimensional even-parity codes. For 
which m is such a code capable of correcting double errors? And triple 
errors? 
5 E 
Hamming codes: 
(1) Compute the error probability PEN(K) 
(4.8) of the Hamming code Κ 
of length 7 used in a binary symmetric channel corrupting 1 bit per 
hundred on the average. (Hint: use the error estimate in 4A.) 
(2) More in general, express the error probability of the Hamming code of 
length 2
m — 1 as a function of m. 
(3) What is the probability Pund(K) 
of an undetected errror of the Ham-
ming code of length 2
m — 1? 
A(x) = 1 + 7x
3 + 7 x
4 + x
7. 
Thus, 
7 p V + 7 p V + p
7. 

78 
CHAPTER 5. BINARY LINEAR CODES 
5F 
Prove that every single-error-correcting code of length 2
m — 1 has at 
least m check symbols. 
5G 
Prove that the decoding of Hamming codes is always incorrect if two 
bits are corrupted in a code word. 
5H 
A binary linear code of length 8 is described by the following equa-
tions: 
Ϊ 5 
= 
Xi + *3 + 
Xl, 
xe 
= 
X1+X2 
+ xa, 
X7 
= 
X\ + X2 + 
Xi, 
xe 
= 
x\ + xa + 
ΧΛ· 
Find its parity check matrix and verify that the minimum distance is 4. 
51 
Find the weight enumerator (5.6) of the code in 5H. 
5 J 
Draw a scheme of a shift-register encoder of the Hamming code of 
length 15. 
5K 
Draw a scheme of a shift-register encoder of the code in 5H. 
Notes 
The theory of binary linear codes was initiated by the discovery of Ham-
ming codes by Golay (1949) and Hamming (1950). The formalism of linear 
algebra was introduced by Slepian (1956, 1960). Further historical notes 
can be found in Berlekamp (1974). 

Chapter 6 
Groups and Standard 
Arrays 
The algebraic structure of commutative groups is used extensively in the 
theory of error-correcting codes. In this chapter, we introduce groups and 
cosete, and we show how algebraic ideas can be used for a very thorough 
(but, unfortunately, very slow) decoding of binary linear codes by so-called 
standard arrays. 
6.1 
Commutative Groups 
Definition. A commutative group (G, +) is a set G together with a binary 
operation + (i.e., a rule assigning to every pair x, y of elements of G a 
new element χ + y of G) satisfying the following axioms: 
(1) associative law: χ + (y + z) = (x + y) + ζ for all x, y, ζ in G, 
(2) commutative law: χ + y — y + χ for all x, y in G, 
(3) existence of a neutral element: there exists an element 0 of G such 
that χ -f 0 = χ for all χ in G, 
(4) existence of opposite elements: for every element χ in G, there exists 
an element —x in G such thai χ + (—χ) = 0. 
Remark. The binary operation is often denoted by juxtaposition (xy in-
stead of χ + y) and then the neutral element is denoted by 1 and the op-
posite element by x
-
1 (and it is called the inverse element of x). 
79 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

80 
CHAPTER 6. GROUPS AND STANDARD 
ARRAYS 
Examples 
(1) The set Ζ of all integers is a commutative group under the usual addi-
tion. 
(2) The set R of all real numbers is a commutative group under the usual 
addition. But not under the usual multiplication. The multiplication 
of real numbers satisfies (1), (2), (3), and "almost" satisfies (4): 0 is the 
only element which fails to have an inverse. A group can be obtained 
by discarding 0. Since a product of two nonzero numbers is always 
different from zero, the multiplication defines a binary operation on 
the set R - {0} of all nonzero real numbers. It is easy to check that 
(R - {0}, χ ) is a commutative group. 
(3) Denote by { 0,1 }
n the set of all binary words of length n. With the 
addition considered in 5.3, this is a commutative group: the associa-
tive and commutative laws are obvious, 0 = 0 0 0 . . . 00 is the neutral 
element, and each word a is its own opposite, since a + a = 0. 
(4) For each natural number p, put 
Z p = { 0 , 1 , . . . , p - 1 } , 
and define the following addition on the set Z p: 
x +
 y 
ifx + y < P , 
X®V-\ 
χ + y-p 
i{x + 
y>p. 
Then (Z p, φ) is a commutative group: the associative and commutative 
laws are obvious, 0 is the neutral element, and the opposite element to χ 
is ρ - χ (for all χ in Z p except χ = 0), and —0 = 0. 
In particular, in Z 2 = { 0 , 1 } , the addition φ agrees with that in 5.1: 
1 φ 1 = 1 + 1 - 2 = 0. When no misunderstanding is possible, the 
addition in Zp is denoted by +. For example, the addition in Z3 is 
shown in Figure 1. 
+ 
0 
1 
2 
0 
0 
1 
2 
1 
1 
2 
0 
2 
2 
0 
1 
Figure 1: Addition in Z 3 

6.2. SUBGROUPS AND 
COSETS 
81 
Remark. In every group, we have, besides addition, the operation of sub-
traction: χ — y — χ + (—y). In the multiplicative notation, we call this 
operation division: χ: y = 
xy~
l. 
Observe that the associative law means that we do not have to use 
parentheses for the group operation: the meaning of χ + y + z is unam-
biguous. 
6.2 
Subgroups and Cosets 
Definition. Let G be a commutative group. 
( 1 ) By a subgroup of G is meant a subset Κ closed under the group 
operations, i.e., satisfying the following conditions: 
(a) if χ and y lie in K, then χ -f y lies in Κ, 
(b) 0 lies in K, 
(c) if χ lies in K, then —x lies in K. 
(2) By the coset of an element xofG 
modulo the subgroup Κ is meant the 
set x + Κ of all sums χ + Jb, where k are elements of K. In symbols: 
x+K 
= {x + 
k\kGK}. 
Examples 
( 1 ) In the group ( Z , + ) of integers, the set Z e of all even integers is a 
subgroup. The coset of the number 1 is the set Z„ of all odd numbers: 
1 + Z e = { 1 + Jb | Jb even } = Z 0 . 
The same is true about any odd number: 
Z„ = 1 + Z e = 3 + Z e = - 1 + Z e ... . 
The coset of the number 0 is Z e itself: 
0 + Z e = Z e , 
and the same is true about any even number: 
Z e = 2 + Ze = 4 + Z e = - 2 + Z e ... . 
Thus, there are precisely two cosets of integers modulo Z„, viz., Z„ 
and Z 0 . 

82 
CHAPTER 6. GROUPS AND STANDARD 
ARRAYS 
(2) Every binary linear code A" of length η is a subgroup of the group 
{ 0 , 1 }
n [see Example ( 3 ) of 6 . 1 ] . In fact, (a) if χ and y are code words, 
then χ + y is a code word, by definition of linearity, (b) 0 = χ + χ 
is always a code word, and (c) every code word is opposite to itself. 
Cosets modulo a linear code form a basis of an important decoding 
method, which we study in the next section. 
We will now show that the structure of all cosets is pleasantly regular: 
no two cosets overlap, they cover the given group, and they have the same 
size. 
Proposition. Cosets in a group G modulo an arbitrary subgroup Κ have 
the following properties: 
( 1 ) every element of G lies in some cosei, 
(2) no two distinct cosets have a common element, 
(3) two elements χ and y lie in the same cosei if and only if their differ-
ence χ — y is an element of K, 
(4) if the subgroup Κ has r elements, then each coset has r elements. 
PROOF. 
( 1 ) Each element χ of G lies in the coset χ + AT: in fact, Κ con-
tains 0, and χ = χ + 0. 
(2) If the cosets χ + Κ and y+ Κ have a common element, say, z, then 
we can express ζ in two ways: 
z = x + k' = y+k", 
for ib', k" in K. 
Now we will prove that every element t of the coset χ + Κ lies in y+ K; by 
symmetry, we then conclude that χ + Κ = y + Κ. Express t as t = χ + ib 
for some ib in K. Then 
t =
x + k = (y + k"-k') 
+ k = y + (k" -k' + k). 
The subgroup Κ contains k" - k' + k, thus, t lies in y + K. 
(3) If χ and y belong to the same coset, say, to ζ + Κ, then χ = ζ + ib 
for some ib in A" and y = ζ + ib' for some ib' in A". Then Κ contains k — k', 
and we have 
χ - y = ζ + k - (z + k') = k - k' Ε K. 
Conversely, if χ — y lies in AT, then put ib = χ — y, in other words χ — y-\-k. 
We conclude that χ and y lie in the same coset y + K. 

6.2. SUBGROUPS AND 
COSETS 
83 
(4) Let Κ = { ibi, &2,..., kr } with Jb, φ kj for i 5ε j . Every coset has 
the form 
x + K = {x + kux 
+ 
k2,...,x+kr}, 
and 1 + Jb,- φ χ + Jbj for i ^ j . (In fact, if χ + ib, = χ + fc,-, then we can 
subtract χ and obtain Jb, = kj.) Thus, χ + Κ has r elements. 
• 
Remarks 
(1) The above proposition tells us that cosets form a very nice decomposi-
tion of the original group G: the group decomposes into disjoint cosets 
of the same size. Thus, if G is a finite group of s elements and A" is a 
subgroup of r elements, then the number of cosets is s/r. 
(2) An interesting construction of a new group (from a given group G and 
its subgroup A") can be performed as follows. Let us choose in each 
coset an element, called the coset leader. The set of all coset leaders is 
denoted by G/K, and the following operation can be defined on G/K: 
χ + y is the coset leader of the coset χ + y + A". 
Then G/K becomes a commutative group: 
(a) the associative law is clear since (x + y) + ζ is the coset leader of 
the coset (x + y) + ζ + Κ, and χ + (y + ζ) is the coset leader of 
(the same) coset χ + (y + 2) + K, 
(b) the commutative law is clear since x + y + A" = y + x-|-A*, 
(c) the neutral element is the coset leader of 0 -f Κ = Κ, 
(d) the opposite element of χ is the coset leader of —χ + Κ. 
The concrete form of the group G/K depends, of course, on the choice 
of coset leaders. However, all the resulting groups are essentially the 
same (see Exercise 6F). 
Example. 
(3) For each natural number p, let pZ denote the set of all 
multiples of ρ (i.e., all integers divisible by p). This is a subgroup of (Z, + ) . 
There are precisely ρ cosets modulo pZ: the coset of 0 is 0 + pZ' = pZ, 
the coset of 1 is the set 1 + pZ of all integers which, divided by p, have 
remainder 1, etc. For each » = 0, 1,2, ... , ρ — 1, the coset t + pZ consists 
of all integers of the form t + pn (η € Z), in other words, all integers whose 
(integer) division by ρ has the remainder »'. For t = p, we get the first coset: 
ρ + pZ = pZ, for t = ρ + 1; the second one: p + l + p Z = 1 + pZ, etc. Also 
— 1 +pZ = (ρ— l)+pZ, etc. We usually choose the numbers 0, 1, ... , ρ— 1 
as coset leaders. In that case, 
Z/pZ = Z p 

84 
CHAPTER β. GROUPS AND STANDARD 
ARRAYS 
is the group of Example (4) in 6.1. In fact, χ + y in Z p is just the usual 
addition if the result is smaller then ρ (i.e., it is one of the coset leaders), and 
otherwise we just subtract p. However, an important example of everyday 
life uses somewhat different coset leaders (see Figure 2). If someone comes 
Figure 2: Z12 
three hours late to a meeting scheduled for eleven, then the person arrives 
at 
11 + 3 = 2. 
Here we compute in Z/12Z, but the coset leaders are 1, 2, ... , 12 (instead 
of 0, 1, ... , 11). Nevertheless, if time is measured modulo 24Z (e.g., as in 
airline schedules), the usual choice of coset leaders is 0, 1, ... , 23. 
Definition. We say that two integers χ and y are congruent modulo p, in 
notation 
x = y 
(mod p) 
provided that they lie in the same coset modulo pZ; in other words, provided 
that the difference χ — y is divisible by p. 
Example. 
(4) Two integers are congruent modulo 2 if and only if they 
have the same parity. 
Congruence modulo 12 is well known from (old-fashioned) watches: 14 
is congruent to 2 and to 26 and to —10, etc. 
6.3 
Decoding by Standard Arrays 
Let A" be a binary linear code of length n. As observed in 6.2, Κ is a 
subgroup of the group { 0 , 1 } " of all binary words of length n. It follows 

6.3. DECODING BY STANDARD 
ARRAYS 
85 
that { 0 , 1 } " decomposes into cosets modulo K. In each coset, we choose 
a coset leader of the smallest Hamming weight (5.3); thus, e.g., the coset 
leader of 0 + Κ = Κ is 0. The standard array is a rectangular array 
containing all binary words of length η arranged so that 
(1) each row is one coset modulo K\ the first one is Κ itself, 
(2) the first column is formed by the chosen coset leaders, 
(3) the word in the ith row and the jth column is the sum of the ith coset 
leader and the jth code word. 
The first row, then, is the code K, arranged arbitrarily except that the first 
code word is 0. Every other row is obtained from the first one by adding 
the coset leader. 
Example. (1) The even-parity code of length 4 is a subgroup Κ of { 0,1 }
4 . 
One coset is Κ with the coset leader 0000. The word 1000 does not lie in 
the coset K, and it has the smallest Hamming weight. Thus, we can choose 
it as a coset leader of 1000 + K. This leads to the rectangle in Figure 3. 
Coset 
Leader 
Code Κ 
0000 
1000 
1100 
0100 
1010 
0010 
1001 
0001 
0101 
1101 
0110 
1110 
0011 
1011 
1111 
0111 
Figure 3: Standard array of the even-parity code 
Remark. If a linear code Κ has k information symbols, and thus 2* code 
words, then the number of cosets modulo Κ is 2
n~* (see Remark 6.2). Thus, 
we know that we have to choose 2
n~* coset leaders. The choice is simple: 
choose any word of length η which is not contained in any of the preceding 
cosets, and take care to keep the Hamming weight to the minimum. 
Example. 
(2) Let A"5 be the linear code of length 5 in which the fourth 
bit checks the parity of the first two bits, and the last bit is an overall parity 
check. Thus, A"e is given by the following equations: 
X\ 
= 
X\ + X2, 

86 
CHAPTER 6. GROUPS AND STANDARD 
ARRAYS 
In the standard array, we start with the code AV Since A"5 has 3 informa-
tion bits, it has 2
3 = 8 code words. Therefore, we have 2
5
-
3 = 4 cosets. 
Next, we choose as a coset leader some word of Hamming weight 1, for 
example, 10000. This is not a code word, thus, we can choose it as a coset 
leader of 10000 + AV See Figure 4. We now want to choose another coset 
Coset 
Leader 
00000 
10000 
10010 
00010 
01010 
11010 
00101 
10101 
11000 
01000 
10111 
00111 
01111 
11111 
11101 
01101 
Figure 4: Code K& and one of the cosets 
leader of Hamming weight 1. But beware: 01000 cannot be chosen since it 
lies in the coset of 10000: 
10000 + /C5 = 01000 + K6 
(verify!). 
The only word of weight 1 not contained in Figure 4 is 00001. Thus, we 
choose it as a coset leader. The last coset leader already has Hamming 
weight 2; we can choose, for example, 10001. See Figure 5. 
Coset 
Leader 
00000 
10010 
01010 
00101 
11000 
10111 
01111 
11101 
10000 
00010 
11010 
10101 
01000 
00111 
l l l l l 
01101 
00001 
10011 
01011 
00100 
11001 
10110 
OHIO 
11100 
10001 
00011 
11011 
10100 
01001 
00110 
11110 
01100 
Figure 5: A standard array of the code A'5 
Decoding by a standard array. When receiving a word, we assume that 
the word sent is that code word in whose column the received word lies. 
In symbols, we receive a word y and we find its position in the standard 
array: 
y = x + e, 
where e is the coset leader of the row of y, and χ is the code word of the 
column of y. Then we assume that χ has been sent. 

EXERCISES 
87 
When is this decoding correct? Whenever the error pattern (5.3) is one 
of the coset leaders. In fact, if we send a code word χ and the error pattern 
is e, then we receive y = x + e. If e is a coset leader, we correctly decode x. 
However, if the error pattern is not a coset leader, then we (always!) decode 
incorrectly. 
Example. 
(3) Suppose that when using the above code K$, we received 
11111. We use the standard array in Figure 5: 11111 lies below 01111; 
thus, we decode 01111. This decoding is correct if (and only if) the error 
pattern is one of the coset leaders. That is, the first or the fifth bit can 
be corrupted, or both. But when some of the middle bits is corrupted, the 
decoding is wrong. For example, if we send 01111 and receive 00111, we 
decode 10111. 
Remark. Decoding by standard arrays is very thorough, since it corrects 
the largest possible collection of error patterns. (Observe, for example, that 
the code K*, has minimum weight 2 and, thus, it cannot correct single errors. 
Yet, with the standard array above, we correct any errors occurring on the 
first and last bits only.) However, this decoding is very slow. A substantial 
simplification will be presented in 8.3; yet, even then, the decoding is quite 
slow, and we will present more powerful decoding techniques for concrete 
classes of codes later. We have seen one such example in 5.5 already: for the 
Hamming code of length 7, we presented a fast decoding. In comparison, 
decoding by the standard array would require checking the array of 2
7 = 128 
words per each received word. 
Exercises 
6A 
Verify that every subgroup of the group (Z, + ) has the form pZ for 
some ρ = 0, 1, 2, . . . . 
6 B 
Describe all finite subgroups of the groups (R, + ) and ( R — { 0 } , X ). 
Verify that the set of all rational numbers is a subgroup of both the groups. 
6C 
Find all subgroups of the group (Zi 2, +). 
6D 
Verify that each commutative group G has precisely one neutral ele-
ment, and each element of G has precisely one inverse element. 

88 
CHAPTER 6. GROUPS AND STANDARD 
ARRAYS 
6 Ε 
Let Μ be a finite set. Denote by exp Μ the collection of all sub-
sets of Μ (including Μ and the empty set). The operation of symmetric 
difference on subsets of Μ is defined by 
Α Δ Β = { m I m lies in A or B, but not in both of them}. 
Verify that (exp Μ, A) is a commutative group. If A U Β denotes the usual 
operation of union, is (exp M,U) a commutative group? 
6 F 
Two commutative groups G and G' are said to be isomorphic pro-
vided that there exists a bijective correspondence between their elements, 
f:G 
—* G', which preserves the group operations [i.e., f(x + y) = f(x) + 
f(y)< 
/ ( Ο ) = 
Οι
 
a
n
a
< f(~x) 
= —f(x)]- Such groups can be considered as 
"essentially the same": the name of an element χ of G is just changed 
to / ( * ) . 
( 1 ) Prove that there exists essentially just one commutative group of 1 
element (i.e., any two one-element groups are isomorphic). 
(2) Prove that there exists essentially just one commutative group of 2 
elements [i.e., any two-element group is isomorphic to (Zj,+)]. 
( 3 ) Let Κ be a subgroup of a commutative group G. Prove that there is 
essentially just one group G/K of coset leaders (i.e., various choices 
of coset leaders lead to isomorphic groups). 
Describe the group Z4/A", where Κ = { 0 , 2 } . 
(4) Prove that the group (exp Μ, Δ) of 6E is isomorphic to (ZJ, + ) . 
6G 
Describe the standard array for the repetition codes. 
6H 
Find a standard array of the code for Exercise 5E. 
61 
Find a standard array for the Hamming code of length 7. 
6 J 
Prove that the binary code of length 5 described by the following 
equations 
Xa = 
x\ + X2, 
XA = 
x\, 
x 5 
= 
x i + z a , 
corrects single errors. Find a standard array and observe that the corre-
sponding decoding corrects more than single errors. 

NOTES 
89 
6K 
Let Κ be the linear code obtained by all possible sums of the following 
words: 101011,011101, 011010. 
(1) Find a parity check matrix. 
(2) Find a standard array, and decode 111011. 
Notes 
More on classical algebraic topics like groups and cosets can be found, 
e.g., in Birkhoff and MacLane (1953). Standard arrays were introduced 
by Slepian (1956, 1960). 

Chapter 7 
Linear Algebra 
The reader has probably already encountered the concepts of a linear (or 
vector) space, matrix, and solutions of a system of linear equations. How-
ever, these topics are often restricted to the field of real numbers, whereas 
in coding theory they are used over finite fields. We therefore present lin-
ear algebra over a finite field, and prove all the results needed later. In the 
sections devoted to basis, matrices, and linear equations, the exposition is 
somewhat quicker than in other chapters since all the material is actually 
quite analogous to the case of real vector spaces, and we only present it for 
sake of easy reference. 
7.1 
Fields and Rings 
Finite fields represent one of the most basic concepts of the algebraic theory 
of error-correcting codes. We introduce here the simple p-element fields Z p , 
and later (in Chapter 11) the more complex Galois fields. 
The aim of the abstract concept of a field is to express all the basic 
properties of real numbers, the "prototype" field. Thus, in a field, we 
have addition, subtraction, multiplication, and division, and we require 
enough axioms to be sure that the computation techniques we are used to 
with real numbers can be applied in any field. Recall that the addition of 
real numbers forms a commutative group, and the multiplication forms a 
commutative group on the set R — { 0 } of all nonzero real numbers. 
Definition. A field is a set F together with binary operations + and • such 
that 
(1) (F,+) 
is a commutative group with the neutral element 0, 
91 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

92 
CHAPTER 
7. LINEAR 
ALGEBRA 
(2) ( F — { 0 }, ·) is a commutative group, 
(3) distributive law: x(y + z) = xy + xz holds for all x, y, ζ in F. 
Remarks 
(1) In every field, we have subtraction: a — b = a + (—6), and, if 6 φ 0, 
division: a:b = ab~
l. 
(2) A product of two nonzero elements is a nonzero element (because mul-
tiplication is a group operation on the set F — { 0 } of all nonzero 
elements). In other words: 
xy = 0 
implies 
χ = 0 or y = 0. 
(3) Since the axiom (2) in the above definition deals only with products of 
nonzero elements, we must specify the multiplication by 0: Every field 
fulfils 
χ · 0 = 0 
for all x. 
In fact, since χ = χ + 0, by the distributive law, we obtain χ · χ = 
x(x + 0) = χ · χ + χ·0 and subtracting χ • χ, we get 0 = χ • 0. 
(4) In every field, we can cancel by any nonzero element: 
ax = ay 
implies 
χ = y 
(whenever α φ 0). 
In fact, a(x - y) = 0 implies either a = 0 or χ - y = 0, by (2). 
(5) Although the definition of a field is quite concise, it represents a large 
number of axioms: 
(a) associative law for both addition and multiplication, 
(b) commutative law for both addition and multiplication, 
(c) distributive law, 
(d) the existence of the neutral elements 0 and 1, 
(e) each element has an opposite, 
(f) each nonzero element χ has an inverse. 
The last axiom is the most subtle one: there are lots of natural algebraic 
structures which satisfy all the axioms but the last one. An algebraic 
structure satisfying the axioms (a)-(e) is called a ring (in more detail: 
a commutative unitary ring). Thus, a field can be defined as a ring in 
which all nonzero elements have inverses. 

7.2. THE FIELDS Z p 
93 
Examples 
(1) Real numbers form a field. Also rational numbers and complex numbers 
are fields. 
(2) The set { 0 , 1 } with the operations introduced in 5.1 is a field. It is 
easy to check the axioms (a)-(f) mechanically, but a more elegant proof 
is given below. 
(3) Integers do not form a field; they fail badly with respect to (f): 1 
and —1 are the only integers with an inverse. However, integers form 
a ring. 
7.2 
The Fields Z p 
Let ρ be a nonzero element of a ring F. We can create a new ring (in fact, 
often a field) by requiring that the equation 
p = 0 
(7.2.1) 
become true. This obscure statement will be made precise below. However, 
observe first that the above equation implies pit = OJb = 0 for each element 
Jfc of F, and, thus, 
x = χ + pk, 
for each χ and Jb in F. 
(7.2.2) 
Now denote [as in Example (3) of 6.2] by pF the set of all multiples of p: 
PF = {pk\ke 
F). 
This is a subgroup of (F, +), and (7.2.2) tells us that each coset modulo 
the subgroup pF (see 6.2) shrinks to a single element—the coset leader. 
Cosets modulo the subgroup pF are shortly called cosets modulo p. By 
Proposition 6.1, two elements x, y of F are congruent modulo ρ (i.e., lie in 
the same coset modulo p) if and only if χ — y is a multiple of p. Shortly: if 
χ — y is divisible by p. The vague idea of forcing the equation ρ = 0 can be 
formalized as follows: 
Definition. If ρ is a nonzero element of a ring F, denote by 
Ff mod ρ 
the ring whose elements are (arbitrarily chosen) coset leaders modulo p, 
and the operations are as follows: 
χ + y 
is the coset leader of χ + y + pF, 
xy 
is the coset leader of xy + pF 
(for arbitrary coset leaders χ and y). 

94 
CHAPTER 
7. LINEAR 
ALGEBRA 
Example. If Ζ is the ring of integers, we can choose, for each ρ > 0, the 
coset leaders 0, 1 
ρ — 1. The operation + has been described in 6.2. 
We now also have the operation of multiplication: 
xy is the remainder of the usual product xy divided by p. 
We use the shorter notation 
Zp — Z/ mod p. 
For example, Z 2 is the set { 0 , 1 } together with the operations introduced 
in 5.1. The operations of Z 3 are given in Figure 1. We see that Z 3 is a 
+ 
0 
1 
2 
0 
1 
2 
0 
0 
1 
2 
0 
0 
0 
0 
1 
1 
2 
0 
1 
0 
1 
2 
2 
2 
0 
1 
2 
0 
2 
1 
Figure 1: Operations of Z 3 
field. In contrast, Z i 2 is not a field because 3-4 = 0 (whereas in a field, the 
product of nonzero elements cannot be zero, see Remark 7.1). In general, 
Z p is not a field for any composite number. 
Remark. To show that the above definition is correct, we must verify that 
F/mod ρ satifies all the axioms (a)-(e) of 7.1. In Remark 6.2, we verified 
that the operation + defines a commutative group. The verification of 
the associative and comutative laws for the multiplication are equally easy: 
(xy)z is the coset leader of (xy)z+pF, 
and x(yz) is the coset leader of (the 
same) coset x(yz)+pF; 
analogously with xy and yx. Also the distributive 
law is clear. The neutral element for the multiplication is the coset leader 
of 1 + pF. 
Theorem. For every prime p, Z p is a field. 
PROOF. Since Z p is a ring (see Remark above), we only have to prove that 
every element i = 1,2, ... , ρ — 1 has an inverse. We proceed by induction, 
denoting multiplication in Zp by ®. 
(1) i = l has an inverse element: 1
_ 1 = 1. 
(2) Suppose ι > 1 and all the inverse elements l"
1, 2
_ L , ... , (i — 1 )
_ 1 
exist. Perform the integer division ρ: i, denoting the quotient by q and the 
remainder by r. Then 
ρ - r = iq, 

7.3. LINEAR 
SPACES 
85 
which clearly implies that 
—r = i ® q 
in Z p . 
Moreover, r is smaller then i (being the remainder of p:i) and is nonzero: 
ρ is a prime and j lies between 2 and ρ - 1, thus ρ φ iq. By the induction 
hypothesis, r
-
1 exists. It follows that the inverse to i is the element — q ® 
r-h 
i ® ( - 9 Θ r
-
1 ) = -(»' ® q) <8> r "
1 = - ( - r ) ® r
_
1 = 1. 
• 
Concluding Remark. We are now equipped with a number of finite fields: 
Zj, Z 3 , Z 5 , Z 7 , . . . . Later we will construct fields of p* elements for any 
prime p, for example, a four-element field (see also Exercise 7F). However, 
no six-element field exists: every finite field has a number of elements which 
is a power of a prime (this will be proved in 11.8). 
7.3 
Linear Spaces 
The reader has certainly encountered the three-dimensional Euclidean 
space and has used vectors in that space. A vector is described by a triple 
of real numbers or a word of length 3 over the alphabet R. The basic oper-
ations on vectors are addition (which turns R
3 into a commutative group) 
and scalar multiplication. The concept of a linear space is a generalization 
which admits an arbitrary word length and substitutes R by an arbitrary 
alphabet with field operations defined on it. 
Definition. Let F be afield (the elements of which are called scalars J . By 
a linear space, or vector space, is meant a set L (the elements of which 
are called vectorsJ together with operations +, called addition, and ·, called 
scalar multiplication, satisfying the following axioms: 
(1) ( £ , + ) is a commutative group, 
(2) · asigns to each scalar t and each vector a a unique vector t a, 
(3) associative law: («t)a = s(i a) for all scalars s, t and all vectors a, 
(4) distributive laws <(a + b) = i a - M b and (« + i)a = s a + i a for all 
scalars s, t and all vectors a, b, 
(5) la = a for all vectors a (where 1 is the unit of F). 

96 
CHAPTER 
7. LINEAR 
ALGEBRA 
Remark. Some further properties of linear spaces can be derived from the 
above ones, e.g., 
(6) Oa = 0 for each vector a, 
(7) (—l)a = —a for each vector a, 
(8) 10 = 0 for each scalar i. 
In fact, (6) follows from (3), (4), and 1 = 1 + 0 (in F): 
a = (l + 0)a = a + 0a; 
now subtract a from both sides. (7) follows from (3), (4), and the definition 
of the opposite vector - a : 
a + ( - l ) a = (1 - l)a = Oa = 0. 
Finally, from (7), we obtain (—t)a = — (ta), and then (8) follows: 
ι0 = ί ( 0 - 0 ) = * 0 - ( ί 0 ) = 0. 
Examples 
(1) The set F
n of all words of length η in the alphabet F is a linear space 
with coordinatewise addition: 
αια 2 . . . a n + fcj62 ... 6„ = cic 3 ...c„, 
where 
c,- = α, + 6j, 
and coordinatewise scalar multiplication: 
i (αια 2 .. .a„) = c j c 2 . . .c„, 
where 
c< = ta,-. 
In particular, 
ζ; 
is the linear space of all binary words of length η (the fundamental 
linear space of coding theory). 
(2) The usual (real) plane R
2 is a linear space over the field R of real 
numbers. Since points in the plane are described by their coordinates 
(which are words of length 2 over R), this linear space is a special case 
of the space F
n above. 
Analogously, the usual (real) three-dimensional space R
3 is a linear 
space over the field R. 

7.3. LINEAR 
SPACES 
97 
Definition. Lei L be a linear space. A nonempty subset Κ of L is called 
a linear subspace provided that it is closed under addition and scalar mul-
tiplication, i.e., 
( 1 ) if two vectors a, b lie in K, then a + b also lies in K, and 
(2) if a vector a lies in K, then each scalar multiple ta lies in K. 
Remark. 
( 1 ) Every subspace contains the zero vector because, given a 
vector a in K, then 0 = Oa lies in K. Thus, the smallest subspace of any 
linear space is { 0 } —this subspace is called trivial. 
Examples 
(3) Binary linear codes of length η are precisely the linear subspaces of the 
space Z"-
(4) Linear subspaces of the real plane R
2 are, beside the trivial subspace 
and R
2 itself, precisely all the lines passing through the origin. 
Linear subspaces of the three-dimensional space R
3 are, beside { 0 } 
and R
3 , precisely all the lines and all the planes passing through the 
origin. 
Definition. By a linear combination of vectors &\, ... , a
m is meant any 
vector of the form t\*\ + <2«2 Η 
h tm&m, where t\, ... , tm 
are scalars. 
Proposition. Let ai, ... , a™ be vectors of a linear space L. 
Then all 
linear combinations of those vectors form a subspace of L, which is the 
smallest subspace containing the given vectors. 
PROOF. We must first prove that the collection Κ of all linear combinations 
ΣΖ<=ι *»'**»' °f the given vectors is a linear subspace of L. In fact, Κ is closed 
under addition: 
m 
m 
m 
i = l 
ί=1 
i = l 
and scalar multiplication: 
m 
m 
s ^ i . a , = ^ ( s i , ) a , . 
i = l 
i = l 
Further, if K' is a linear space containing each a, (» = 1 
m), then 
K' contains each linear combination: I,a, lies in K' for each i and each 
scalar *,·, hence, Σ!=ι <ia,- also lies in K'. Thus, .ft* is a subspace of K'. 
• 

98 
CHAPTER 
7. LINEAR 
ALGEBRA 
Remark. 
(2) Vectors a j , ... , a„ of a linear space L are said to span 
the space L provided that every vector in L can be expressed as a linear 
combination of aj 
a„. The above proposition says that every m-tuple 
of vectors of L spans a subspace of L. 
Examples 
(5) 
(6) The vector ( 1 , 0 , - 1 ) in the three-dimensional space R
3 spans the line 
Κ = t (1,0, - 1 ) consisting of all vectors (ί, 0, - i ) , t 6 R. 
The vectors ( 1 , 0 , - 1 ) and (0,1,1) span the plane Κ = ί ( 1 , 0 , - 1 ) + 
«(0,1,1). 
(7) The even-parity code of length 4 is spanned by the vectors 1100, 1010, 
and 1001. In fact, all the remaining code words are linear combinations 
of those three words: 
0000 
= 
1100+ 1100, 
0110 
= 
1100+1010, 
0101 
= 
1100+ 1001, 
0011 
= 
1010+ 1001, 
m i 
= 
noo+ioio+iooi. 
Observe that in the binary field Zj, the only scalars are t = 0, 1 and 
they just determine whether a given vector is present (i = 1) or absent 
(t = 0) in a linear combination. 
7.4 
Finite-Dimensional Spaces 
Definition. Let L be a linear space. 
(1) Nonzero vectors ai, ... , a m are said to be linearly independent pro-
vided thai none of them is a linear combination of the others. 
(2) By a basis of L is meant a linearly independent collection of vectors 
which span L [see Remark (2) of 7.3]. If L has a finite basis, then it 
is said to be finite-dimensional; more specifically, L is /b-dimensional 
provided that it has a basis of k vectors. 
Examples 
(1) The real plane R
J is two-dimensional: the vectors (1,0) and (0,1) form 
a basis. The three-dimensional space R
3 deserves its name: (1,0,0), 
(0,1,0), and (0,0,1) form a basis. 

7.4. FINITE-DIMENSIONAL 
SPACES 
99 
(2) The linear space F
n of all words of length η is η-dimensional. It is easy 
to see that the η words of Hamming weight 1 form a basis of F". 
(3) The even-parity code of length 4 is three-dimensional: the three vectors 
of Example (6) in 7.3 clearly form a basis. 
(4) As an example of a linear space which is not finite-dimensional, consider 
the set of all sequences a = αια 2α 3 . . . of O's and l's. This is a linear 
space over Z 2 with coordinatewise addition and scalar multiplication. 
Remarks 
(1) Vectors ai, ... , a m are linearly independent if and only if none of their 
linear combinations except the trivial one (with all-zero scalars) is equal 
to the zero vector. In symbols: 
m 
U&i — 0 
implies 
t, = 0 for all i = 1, . . . , m. 
i = l 
In fact, whenever Y^tia% = 0 and U0 φ 0, then a, 0 is a linear combina-
tion of the other vectors, viz., 
m 
•*. = Σί-'-'Γο
1)*·· 
t=l 
m 
Conversely, whenever &i0 = "ζ
 
s«
a«)
 t
n
e
n we have the following non-
1 = 1 
trivial linear combination: 
siai + 
1- S j 0 _ i a , 0 _ i - a
i
o + s, 0+iai 0 +i + 
1- s m a m = 0. 
(2) For finite fields F, the following holds: a linear space over F is finite-
dimensional if and only if it has a finite number of vectors. 
More concretely, let F have r elements. Then each Jb-dimensional linear 
space has r* elements. This follows from the next result: 
Proposition. Let ei 
e m be a basis of a linear space L. Then for 
each vector a of L, there exists a unique word ti.. .tm of scalars (i.e., an 
element of F
m) with a = Σ™=1 i.e.. 
PROOF. Since the basis spans L, the existence of such a word is clear. To 
prove the uniqueness, let si ...sm 
be another word with a = "ζ^χ e^e,-. 
Then 5Ζ<=ι(
β*
 — *·')
β· = a — b = 0 is a linear combination which, by 
Remark (1) above, is trivial. Thus, «j = <,· for each i. 
• 

100 
CHAPTER 
7. LINEAR 
ALGEBRA 
Remark. 
(3) By choosing a basis elt 
... , e m in a finite-dimensional 
space L, we obtain a bijective correspondence between vectors in the linear 
space F
m and vectors in L: 
m 
<1 · · · <m < 
• 
Υ Ί < ί β ι · 
This correspondence preserves the operations of linear space: it assigns to 
a sum t + t' in the space F
m 
the sum £<je, + Σ*ί
β»
 = Σ)(*· + 'ί)
6· *
n 
the space L, and to a scalar product rt in the space F
m the scalar product 
r£)<,e, = XXr<,)e, 
L. 
Thus, the linear spaces F
m and L are essentially the same. 
Theorem. Any linearly independent vectors aj, ... , a m in a k-dimen-
sional linear space form a part of some basis aj, ... , am, b m+i, ... , bt 
of that space. 
PROOF. We proceed by induction on the number m. 
(1) m = 1. Given a vector a which is linearly independent (i.e., non-
zero) in a ib-dimensional space L, we first choose an arbitrary basis of 
k vectors in L: bi, ... , bt. We can express a as a linear combination 
a = Σί=ι *<°·· Some of the coefficients i,- is nonzero (since a φ 0). Suppose, 
for example, that U φ 0. Then we will show that a , b 2, ... , bi form a 
basis of L. In fact: 
(a) The vectors a , b 2, ... , bt span the space L because each of the 
vectors bi (which span //) is a linear combination of those vectors. This is 
obvious for ι = 2, ... , ib, and for i = 1, we have 
m 
b, = «γ
1·- Σ^γ
1**)»»*. 
i=2 
(b) To prove linear independence, we use Remark (1) above. Consider 
any linear combination 
«ia + s 2 b 2 -) 
h «*bt = 0. 
Since a = Y^=\ 'ibi, we have 
«i<ibi + (s 2 + eii 2)b 2 + ··· + («* + s i t t ) b t = 0. 
From the linear independence of bi, ... , b», we conclude that 8\ti = 0 
(thus, « i = 0 because t\ φ 0 by assumption) and «, + *i<. = 0, which 
implies «i = 0 (i = 2 
k). Therefore, the above linear combination is 
trivial, which was to be proved. 

7.5. 
MATRICES 
101 
(2) Induction step: Let the theorem hold for a j , ... , a m _ i , i.e., let 
there be a basis ai 
a m _ i , b m , ... , bt. We are to prove that the 
theorem holds for ai 
a m . We can express a m as a linear combination 
a m = tiai Η 
H i m - i a m - i + smbm 
Η 
h s*bt. 
Some of the coefficients s< is nonzero (since am is not a linear combina-
tion of ai, ... , a m _ i ) . Suppose, for example, that s m φ 0. Then ai, 
... , a m _ i , a m , b m + i , ... , bt is a basis of L. This follows, analogously 
to the above step m = 1, from the fact that b m is a linear combination of 
those vectors: 
m - l 
k 
b m = Y^(-S^U)&i 
+ ^ Ι Ι „ + 
Σ 
( - S - ' s . J b i . 
ί=1 
i = m + l 
• 
Corollary. Every k-dimensional space L has the following properties: 
(1) Each basis of L has k vectors. 
(2) Each k-tuple of linearly independent vectors forms a basis. 
(3) Jb is the largest number of linearly independent vectors in L. 
(4) Every linear subspace of L, except L itself, has dimension smaller 
than Jb. 
In fact, (1) follows from the above Proposition and the obvious fact 
that no basis contains a smaller basis; (2) and (3) follow directly from the 
Proposition. For (4), let A" be a linear subspace of L and let m be the 
largest number of linearly independent vectors in K. 
By (3), we know 
that m < Jb. Any linearly independent collection a t , ... , a m of vectors 
in A" is a basis of AT. (In fact, for each vector a φ 0, the collection a, ai, 
... , a,„ is linearly dependent, thus, there is a nontrivial linear combination 
t a+">2 tja,- = 0. The linear independence of ai, ... , am implies that t φ 0, 
and then a = Σ(—i
_1<,)a,-. 
Therefore, ai, ... , am span A".) If m = Jb, 
then, by (2), ai, ... , a m form a basis of L, and thus, L — K. In other 
words, if Κ φ L, then m < Jb. 
• 
7.5 
Matrices 
We are now going to show that solutions of a system of linear homogenous 
equations can be found over any field F analogously to the well-known 

102 
CHAPTER 
7. LINEAR 
ALGEBRA 
O l l l l + 
012*2 + 
H a i n a n 
= 
0 
021*1 + 
<"22*2 Η 
r 02n*n 
= 
0 
o mi*i + amixi 
Η 
Yamnxn 
= 
0 
Figure 2: A system of linear homogenous equations 
case of real numbers. A system of linear homogenous equations over F is 
a system as in Figure 2, where the coefficients α ι ; are elements of F. It is 
obvious that the all-zero vector solves the system, but we want to describe 
all solutions. We first rewrite the system more succinctly, using matrices. 
An m χ η matrix over F is an (m n)-tuple of elements of F arranged in a 
rectangular array of m rows and η columns. For example, the coefficients 
of the system in Figure 2 form the following matrix 
Γ O N 
O N 
013 
· • · 
Oi„ 1 
* 
I 021 
022 
023 
· · 
0 2 „ 
L 0ml 
0
m 2 
O
m
3 
· · · 
a
m „ 
J 
Every vector a of the linear space F
n is considered as a one-row matrix: 
a = [ ax 
a 2 
a 3 
· · a„ ] . 
Conversely, every row of an m χ η matrix is a vector of F
n. 
The linear 
subspace of F" spanned by all rows of the matrix A is called the raw space 
of A. The dimension of the row space is called the rank of the matrix A. It 
is obvious that the solutions of the system of equations in Figure 2 depend 
on the row space of A only, and not on the concrete form of the matrix. 
In order to find all solutions of the system of equations in Figure 2, we 
perform so-called elementary row operations on the matrix A. These are 
the following three types of operations, which change the matrix A, but do 
not change the row space of A: 
(1) An interchange of two rows. 
(2) A scalar multiplication of a row by a nonzero scalar. 
(3) A replacement of a row by the sum of itself and a scalar multiple of 
another row. 

7.5. 
MATRICES 
103 
By a successive application of elementary row operations, every matrix can 
be put into row-echelon form, which means that 
(a) each row with nonzero terms has the leading term (i.e., the first 
nonzero term) equal to 1; 
(b) the leading term of each nonzero row lies to the right of the leading 
terms of all higher rows; 
(c) each all-zero row lies below all nonzero rows. 
Observe that (a), (b), and (c) imply that all entries below a leading term 
are zeros. 
Example. Let us solve the following system of equations over the field Z 3: 
x 4 + 2*5 
= 
0, 
2 x 2 + 
*4 + 2χ 5 
= 
0, 
2 x 2 + 2 x 4 + 
x 5 
= 
0, 
2 x 2 + 
x 4 
= 0 . 
We first write down the matrix of coefficients: 
' 0 
0 
0 
1 2 ' 
0 
2 
0 
1 2 
A ~ 
0 
2 
0 
2 
1 
0 
2 
0 
1 0 
Next, we interchange the first and fourth rows and divide the new first row 
by 2. (In the field Z3, division by 2 is the same as multiplication by 2, since 
2 x 2 = 1 . ) 
0 
1 0 
2 
1 ' 
0 
2 
0 
1 2 
0 
2 
0 
2 
1 
0 
0 
0 
1 2 
We now add the first row to the third and fourth rows: 
' 0 1 0 2 1 " 
0 
0 
0 
0 
0 
" 
0 
0 
0 
1 2 
0 
0 
0 
1 2 
0 
2 0 
1 0 
0 
2 
0 
1 2 
0 
2 
0 
2 
1 
0 
0 0 
1 2 

104 
CHAPTER 
7. LINEAR 
ALGEBRA 
Finally, we add the third row to the fourth one, and interchange the second 
and third rows: 
" 0 1 0 2 1 ' 
0 
0 0 
1 2 
~~' 
o o o o o 
· 
0 
0 0 0 0 
Thus, we obtain a new system of equations in the row-echelon form: 
X 2 + 2x 4 + 
X5 
= 
0, 
x 4 + 2x 5 
= 
0. 
Its solutions are precisely all vectors (p, q, r, 2q, 2a), where p, q, r are scalars 
from Z 3. 
Putting a matrix into row-echelon form. Let A be a matrix. If the en-
tries are all zeros, then A is in row-echelon form. Otherwise perform the 
following steps: 
(1) Interchange the first row with the row having the leftmost leading 
term of all rows. After the interchange, the leading coefficient a\j has 
the property that either j = 1 or every row has the first j — 1 terms 
equal to zero. 
(2) Multiply the first row by a"",
1. The leading coefficient of the first row 
becomes 1. 
(3) For all i > 1, add the scalar multiple of the first row by the scalar — o,j 
to the ith row. All entries of the j'th column become zero except for 
aij = 1. 
If m = 1, we are finished, and if m > 1, we now delete the first row of A, 
and perform the same steps on the smaller matrix. After the smaller matrix 
is put (recursively) into row-echelon form, we return the first row. 
Remarks 
(1) The row-echelon form has the property that the nonzero rows are lin-
early independent. In fact, let ai, ... , a* be all the nonzero rows, and 
let 23 i =i i<a, = 0 be a linear combination. Then 
ii<"y
 = 
u> where 
j denotes the column in which the first row has its leading coefficient. 
Since ay = 1 and a2j- = · · · = a*j = 0, we get ti = 0. Analogously, 
from "C*_2iia,' = 0, we conclude that i 2 = 0, etc. This proves the 
linear independence by Remark (1) of 7.4. 
Therefore, the row rank of a matrix A is equal to the number of nonzero 
rows of the row-echelon form of A. 

7.6. OPERATIONS 
ON 
MATRICES 
105 
(2) The row-echelon form always enables an easy characterization of all 
solutions of the corresponding homogenous system as illustrated by 
the above example. 
Theorem. Every matrix of rank k has k linearly independent columns. 
PROOF. If a matrix A is in row-echelon form, then it has ib nonzero rows, 
and we can pick the ib columns in which the leading coefficients of the rows 
lie. 'those columns are linearly independent because if we write them as 
rows (from the top downward), we clearly get a matrix of ib nonzero rows 
in row-echelon form. 
Since every matrix can be put into row-echelon form by a succession 
of elementary row operations, it is sufficient to show that none of the row 
operations changes the linear independence of columns. We present the 
proof for the case of the interchange of two rows (the other two cases of 
elementary row operations are analogous). Consider ib columns of the ma-
trix A; the columns number ji, j
2 , • • • , jk- If we write them as rows, we 
get ib vectors by,, . . . , b J J k. Now we interchange the ith and i'th rows of the 
matrix A. The corresponding columns by,, ... , byk of the new matrix are 
obtained from the original vectors by,, . . . , b J k by interchanging the ith 
and i'th positions. It is our task to show that the vectors by,, ... , byk are 
linearly independent if and only if so are the new vectors by,,... , by k. This 
follows from Remark (1) of 7.4 since, given scalars <i, . . . , it, we clearly 
have 
f iby, + 
h tkbjk 
= 0 
if and only if tibjl 
+••• + tkbjk 
= 0. 
• 
7.6 
Operations on Matrices 
Recall the following operations on matrices. 
Sum A + Β is defined for two m χ η matrices componentwise: 
A + B = [oij + bij]. 
Scalar multiple is also defined componentwise: 
< A = 
[tai3]. 
Matrix product A B of an m χ η matrix A by an η χ ib matrix Β is the 
following m χ k matrix: 
η 
A B = [ c < ; ] , 
where 
ci} 
=^
ai'
b*i-
»=i 

106 
CHAPTER 7. LINEAR 
ALGEBRA 
In other words, the (i, j)th position of the matrix A B is the inner 
product of the ith row of A with the j'th column of B , where the inner 
product of two vectors a and b of F
n is the scalar denned by 
a · b = aj6i + 0262 + 
1- o„6n-
In particular, the product of a matrix A by a row is just the linear 
combination of the row vectors aj, ... , a m of A: 
[<ι<2· · <m]A = iiai + r 2 a 2 + • · -M mam- 
(7.6.1) 
Analogously, the product of a matrix A by a column is just the linear 
combination of the column vectors of A: 
" «1 
an 
Oi2 
Oln 
«2 
021 
022 
02n 
A 
= «1 
+ «2 
+ ·+*„ 
. °"»1 
«m2 
o m n 
(7.6.2) 
Transpose of an m χ η matrix A is the η χ m matrix A
t r obtained from A 
by writing the columns as rows, i.e., 
A" =[«,·,]. 
In particular, the transpose of a vector a of the linear space F" (considered 
to be a one-row matrix a = [ αϊ 02 · · · a„ ]) is the one-column matrix 
αϊ 
L
 
A « 
J 
Returning to the system of linear equations in Figure 2, we see that it 
takes the following matrix form: 
' 
A N 
012 
• · 
O I „ 
* Xl 
' 0 ' 
021 
«22 
• · 
02„ 
*2 
= 
0 
. o M I 
0m2 
o m n 
0 _ 
Or, more succinctly, 
A x
t r = 0
T R. 
(7.6.3) 
where χ denotes the vector of unknowns. 

7.6. OPERATIONS ON 
MATRICES 
107 
An η χ η matrix is called a square matrix. A square matrix whose terms 
are given by 
Α 
-
Ί
1 
I
F
I 
= 
> ' 
a»-\o 
0 
else, 
is called an identity matrix, and is denoted by I. For example, if η = 3, 
then 
" 1 0 
0 
1 = 
0 
1 0 
0 
0 1 
The name stems from the fact that whenever the matrix product with I is 
defined, then I acts as a unit: AI = A and IB = B . 
Let A be a square matrix. A is said to be regular if it has linearly 
independent rows. Then A has an inverse matrix, i.e., a matrix A
-
1 such 
that 
A A "
1 = A
_ 1 A = I. 
In fact, A
-
1 can be found as follows: denote by [ A |l] the η by 2n matrix 
obtained by adjoining the identity matrix to the right of the matrix A. 
Perform the elementary row operations on the matrix [ A j I ] in order to 
obtain a matrix of the form [ I j Β ]. Then Β = A
- 1 . 
Remark. Let us consider a nonhomogenous system of linear equations: 
a n x i + 
1- ai„xn 
= 
dit 
0 2 1 * 2 + 
h 02n*n 
= 
0*2. 
^ 
^ 
^ 
a m i * i + 
r amnx„ 
= 
dm. 
Or, in the matrix form: 
A x
t r = d
t r. 
(1) If A is a regular square matrix (m = n), then the above system 
has a unique solution, viz., x
t r = A
_ 1 d
t r . Let us remark that every square 
matrix A with linearly independent columns is regular (by Theorem 7.5 
applied to A
t r: since A
t r has linearly independent columns, A has linearly 
independent rows). 
(2) More in general, if A has linearly independent rows (i.e., if it has 
rank m), then the above equations (7.6.4) have at least one solution. 
In fact, by Theorem 7.5, A has m linearly independent columns. If, for 
example, the first m columns are linearly independent, than we can choose 
Xi = 0 for i = m + 1, ... , η and solve the following equations 
a n i i Η 
H a i m x m 
= 
d\, 
am\x\ + 
H A
m
m x
m 

108 
CHAPTER 
7. LINEAR 
ALGEBRA 
They have a unique solution [by (1)], and by adding the n - m zeros, we get 
a solution of the system above. In the case that columns of another m-tuple 
are linearly independent, we proceed in the same way, putting χ,· = 0 for 
all i's except in the m chosen columns. 
7.7 
Orthogonal Complement 
Remark. 
(1) Recall from 7.6 the concept of an inner product in the linear 
space F
n: 
a · b = αχδχ + · · · + 
a„b„. 
Observe the following properties of the inner product: 
(a) a · b = b · a for all vectors a and b, 
(b) a · (b + c) = a · b + a · c for all vectors a, b, and c, 
(c) a · (t b) = t (a · b) for all vectors a and b and all scalars t. 
Definition. 
(1) Two vectors of F
n are said to be orthogonal if their inner 
product is equal to zero. 
(2) Let L be :, linear subspace of F". By the orthogonal complement 
of L is meant the -.et of all vectors orthogonal to each vector in L. This is 
denoted by L^: 
L
L 
= { a € F
n I a . b = 0 for all b £ L } . 
Examples 
(1) If L is a line in Hie plane R
2 , then L
x is the perpendicular line passing 
through the origin. For example, the orthogonal complement of the 
x-axis is the y-axis. 
(2) The orthogonal complement of the even-parity code Κ is the repetition 
code of the same length. In fact, since each code word a fulfils αχ + 
μ an = 0, both 000.. .00 and 111... 11 are orthogonal to every code 
word of K. Conversely, if a word c = cxC2 • • .c„ is orthogonal to every 
code word, then c, = c}- for all indices i, j . (For i φ j , consider the 
word a which has 1 precisely on the ith and jth positions. Then a is a 
code word of K; thus, a · c = c< + CJ = 0.) 
Remark. 
(2) In the theory of real vector spaces, an important fact con-
cerning orthogonal complements is that a linear space and its orthogonal 
complement have only the zero vector 0 in common: Κ Π Κ
1- = { 0 } . 

7.7. 
ORTHOGONAL 
COMPLEMENT 
109 
This is not true in general over finite fields. In fact, there are linear spaces 
which coincide with their orthogonal complements (e.g., the repetition code 
of length 2). 
Proposition. Lei Κ be a linear subspace of F
n. 
(1) The orthogonal complement K
1 
is also a linear subspace of F
n. 
(2) If Κ is spanned by vectors ai, ... , A m , then every vector orthogonal 
to each a,-, t = 1, ... , m, lies in Λ"
χ. 
PROOF. 
(1) Follows immediately from the linearity properties (b) and (c) 
in Remark (1) above. 
(2) Let b be orthogonal to each a,-. Every vector of A" is a linear com-
bination a = "CZLi 'ί
8» °f 'he given vectors, and from the above linearity 
properties, we get 
tn 
m 
m 
b · a = £ b · (t,a<) = £ t i ( b · m) = £ ^ 0 = 0. 
t = l 
1 = 1 
i = l 
Thus, b lies in K
x. 
• 
Example. 
(3) Every solution of a system of linear homogenous equations 
(see Figure 2 of 7.5) is orthogonal to all rows of the corresponding matrix A. 
Thus, it is orthogonal to all vectors of the row space L of A. Conversely, 
every vector in the orthogonal complement L
x is a solution of the system 
of equations. 
Thus, we see that all solutions of a system of equations with the coeffi-
cient matrix A form precisely the orthogonal complement of the row space 
of A. 
Theorem. The orthogonal complement of a k-dimensional subspace of the 
linear space F
n has dimension n — k. In symbols, 
dim L
x = η — dimi. 
PROOF. Let ai, ... , at be a basis of a linear subspace L of F
n. 
Those 
vectors, written as rows, form a k χ η matrix A. A vector b lies in the 
orthogonal complement of L if and only if A b
t r = 0
T R. 
Since A has rank Jb, it has Jb linearly independent columns (see Theo-
rem 7.5). The length of the columns is Jb, and thus it follows from Corol-
lary (4) in 7.4 that the columns span the linear space F
k (provided that 
we write vectors of F
k as columns). Thus, every vector ν in F
k is a linear 

110 
CHAPTER 
7. LINEAR 
ALGEBRA 
combination of the columns of A; in other words, ν has the form ν = Aw" 
for some vector w in F" [see formula (7.6.2)]. 
Let r denote the dimension of Z/
A. We choose a basis bi, . . . , b r of L
x 
and we complete it to a basis bj, . . . , b r, c r + i , . . . , c„ of the space F
n 
(see Theorem 7.4). We are going to show that the η — r vectors 
A r
t
r 
A r
t
r 
form a basis of the space F
k. This will prove that k = n — r; thus, r = n — k, 
whereby the proof will be finished. 
(1) The vectors above span F
k. In fact, every vector ν of F
k has the 
form ν = Aw
t r. 
We can express the vector w as a linear combination 
w = J2<=i *«
D« + I3j=r+i
 8J
ci>
 
a n d
 
t
n
e
n Ab}
r = 0 implies 
\ = 1 
j=r+l
 
7 
j=r+l 
(2) The vectors above are linearly independent. 
In fact, consider a 
linear combination ] C " _ r + 1 ijAcj
r = 0
t r [see Remark (1) in 7.4]. The vector 
c = 5 3 j _ r + 1 tjCj then fulfils A c
t r = 0
t r; i.e., c lies in L
1. Thus, c is a linear 
combination of the vectors bi 
b r (forming a basis of I
x ) as well as 
a linear combination of the vectors c r + i , ... , c„. Since b\, . . . , b r , cv +i, 
... , c„ form a basis of F
n, Proposition 7.4 implies that c = 0. Now, by 
the linear independence of c r + i , . . . , c n , we conclude that t, = 0 for all t, 
which proves the linear independence of Ac'+j, . . . , AcJ,
r. 
• 
Corollary. For an arbitrary subspace L of the linear space F
n, we have 
(L
X)
X 
= I-
In fact, L
x = Κ implies that L is a subspace of K
x 
(since each vector 
in L is orthogonal to all vectors of X = L
x). 
If dim L = ib, then dim A" = 
η — ib and, hence, dim A"-
1- = η — (η — k) = ib. Thus, L and K
x 
have the 
same dimension. By Corollary (4) of 7.5, L = K
1. 
• 
Remarks 
(3) If a system of linear homogenous equations in η unknowns has matrix 
of rank k, then all solutions form a linear space of dimension n — k. 
This follows from the above Theorem and Example (3). 
(4) Every linear subspace L of the space F" can be described by a system 
of linear homogenous equations. That is, there exists a system as in 

EXERCISES 
111 
Figure 2 of 7.5 such that L is precisely the set of all solutions. In fact, 
let ai, . . . , a
m be a basis of the orthogonal complement of L, and let 
A be the matrix whose rows are ai, . . . , a m . Then vectors χ of 
(L
1)
1-
are precisely the solutions of the system A x
t r = 0
t r , and (L-
L)-
L = L. 
Exercises 
7A 
Prove that multiplication in every ring has a unique neutral element. 
7B 
Verify that in every field, ( χ
-
1 )
-
1 = χ for all χ φ 0. 
7C 
Prove that in every ring, all elements x, for which x
-
1 
exists, form 
a group under multiplication. Apply this to Ζ12· 
7D 
Write down the operation tables of the field Z5. Find x
-
1 for each 
χ φ 0. 
7E 
Is Z4 a field? Can you re-define the multiplication (leaving the addi-
tion) to obtain a field? 
7F 
Verify that the following tables 
+ 
0 
1 Ρ 
9 
0 
1 Ρ 
9 
0 
0 
1 Ρ 
9 
0 
0 
0 
0 
0 
1 
1 
0 
9 
Ρ 
1 
0 
1 Ρ 
9 
Ρ 
Ρ 
9 
0 
1 
Ρ 
0 
Ρ 
9 
1 
9 
9 
Ρ 
1 
0 
9 
0 
9 
1 Ρ 
define a four-element field F = { 0 , l,p, 9 } . 
7G 
How many vectors are there in the linear space ZjJ? In Z3? 
7H 
Verify that the linear space F
n over an r-element field F has precisely 
(1) r" — 1 linear subspaces of dimension 1. [Hint: the number of all 
vectors, including 0, is r".] 
(2) r" — 1 linear subspaces of dimension η — 1. [Hint: apply (1) to the 
orthogonal complement.] 

112 
CHAPTER 
7. 
LINEAR 
ALGEBRA 
71 
Prove that the set of all η χ τη matrices with the addition and scalar 
multiplication of 7.6 form a linear space. What is its dimension? 
73 
Prove that the linear space of Example (4) in 7.4 is not finite-dimen-
sional. 
7K 
Find a row-echelon form of the following matrix 
(1) over the field Z 2 ) 
(2) over the field Z 3 . 
In both cases, determine the rank. 
7L 
Find all solutions of the following system of equations over Z 2 and 
over Z3: 
1 0 1 
1 
1 0 
0 
1 1 
0 
0 
0 
1 0 1 
1 
1 0 
0 
1 1 
X1 + X2 + X4 
= 
0, 
*3 + X4 + *5 
= 
0, 
xi + xa + xs 
= 
0. 
7M 
Find an inverse matrix to the following matrix 
0 
0 
1 
1 1 
1 0 
(1) over the field Z 2 , 
(2) over the field Z 3 . 
7N 
Find an inverse matrix of the following matrix 
A = 
1 
1 
1 
0 
0 
1 1 
1 0 
1 
1 
1 0 
0 
1 1 

EXERCISES 
113 
(1) over the field Z 2 , 
(2) over the field Z 3 . 
In both cases, discuss the solutions of the system of equation A x
t r = b
t r, 
where b = 1100. 
7 0 
Given a solution a = a\.. .a„ of a nonhomogenous system of linear 
equations A x
t r = d
t r (see Remark 7.6), prove that all solutions of that 
system have the form a + b, where b solves the corresponding homogenous 
system (i.e., A b
t r = 0
t r ) . In other words, the collection of all solutions is 
the coset (6.2) 
a + 
A '
x 
modulo the linear space X
x which is the orthogonal complement of the row 
space Κ of the matrix A. 
7P 
Find all solutions of the following system of equations over Z 5: 
Xl + 3x 2 + 2x 3 
= 
1, 
x t + 2 x 2 + X3 = 
2. 
7Q 
Find the orthogonal complement of the Hamming code of length 7 
(see 5.5). 
7 R 
What property of the field F of real numbers guarantees that for each 
linear subspace L of F
n, one has L Π h
L = { 0 }? Does any of the fields Z p 
have this property? Do you know any other field which does? 
7S 
Let A be an τη χ η matrix of rank m. Prove the following: 
(1) A row-echelon form of A has l's in the main diagonal (a,, = 1) and 
zeros under it. 
(2) If m = n, then it is possible to find elementary row operations which 
transform A into an identity matrix. [Hint: use (1) and start the 
operations on the row-echelon form from the bottom up.] 
7T 
Prove the following generalization of Theorem 7.2: each number i = 
0, 1, ... , n —1 relatively prime with η has an inverse element in the ring Z„; 
i.e., there exists j with ij = 1 
(mod n). [Hint: The proof is analogous to 
that in 7.2. Whenever i is relatively prime with n, then the remainder of 
the integer division η :» is also relatively prime with n.] 

114 
CHAPTER 
7. 
LINEAR 
ALGEBRA 
7U 
Prove that if a matrix A is obtained by concatenating matrices 
Ai and A 2 row by row (notation: A = [ Ai i A 2 ] ) and a matrix Β is ob-
tained by concatenating matrices Bi and B 2 column by column (notation: 
Bi 
Β = I — 
) , then for the matrix product, the following formula holds: 
B 2 
A B = [ Ai i A 2 
2i 
B 2 
= A i B i + 
A
2 B
2
l 
whenever the number of rows of A, is equal to the number of columns of Β,· 
for » = 1, 2. 
Notes 
This chapter is just a brief presentation of standard linear algebra; for more 
information, the interested reader can consult a number of good textbooks, 
e.g., Birkhoff and Mac Lane (1953). 

Chapter 8 
Linear Codes 
We now extend the concepts of generator matrix, parity check matrix, and 
syndrome decoding from binary codes to codes over an arbitrary alpha-
bet. Then we turn to modifications of a given code: extension, puncturing, 
augmentation, and expurgation. This chapter prepares ground for the in-
troduction of important classes of codes in subsequent chapters. 
8.1 
Generator Matrix 
Linear codes over the binary field Z 2 were introduced in 5.3. We now 
generalize the concept to any finite field F. This means that our code 
alphabet carries the structure of addition and multiplication of a field; 
this somewhat restricts the code alphabet we can choose, but it has the 
advantage of structuring block codes in order to obtain good error control. 
Definition. Let F be a finite field. By a linear code is meant a linear 
subspace of the linear space F" of all words of length η (n = 1 , 2 , 3 , . . . ) . 
In more detail, a k-dimensional 
linear subspace of F" is called a linear 
(n,k)-code 
in the alphabet F. 
Remarks 
(1) Thus, a linear code of length η is a set of words of length η such that 
(a) if a and b are code words, then a + b is a code word 
and 
(b) if a is a code word, then each scalar multiple t a is a code word. 
115 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

116 
CHAPTER 8. LINEAR CODES 
Observe that every linear code contains the all-zero word 0 = Oa. 
(2) A linear (n, ib)-code has Jb information symbols (4.2) and n — k check 
symbols. In fact, let Κ be a. Jb-dimensional subspace of F
n, and let 
e i , e 2 , .. . , e * 
be a basis of Κ. Then every code word ν has the form 
(8.1.1) 
i = l 
for a unique Jb-tuple of scalars u, (see Proposition 7.4). In other words, 
the k information symbols uiu 2 .. . 
determine a code word uniquely 
by (8.1.1), and, conversely, every code word (8.1.1) determines the Jb in-
formation symbols uniquely. 
(3) Observe that if the code alphabet F has r symbols, then there are 
r* code words in K. 
Definition. Let Κ be α linear code with a basis e i , e 2 , . . . , β * . The ib χ η 
matrix G with the rows ei, ... , ek: 
G = 
is called a generator matrix of the code K. 
Examples 
(1) The Hamming (7,4)-code [see Example (1) of 5.5] has the following 
generator matrix: 
G = 
1 0 0 0 0 1 1 
0 
1 0 
0 1 0 
1 
0 0 1 0 
1 1 0 
0 0 0 1 1 1 1 
In fact, each row of the matrix G is a code word, since we can find it in 
Figure 3 of 5.5. The first four columns of G form an identity matrix, 
thus, G is of rank 4. The dimension of the code is also 4, therefore, it 
is spanned by the rows of G. 

8.1. 
GENERATOR 
MATRIX 
117 
(2) The even-parity code of length 4 has the following generator matrix 
[see Example (3) of 7.4]: 
G = 
1 1 0 
0 
1 0 
1 0 
1 0 
0 
1 
Every matrix obtained from G by elementary row operations (7.5) is a 
generator matrix of the same code. For example, the following matrix 
(in row-echelon form) 
G' = 
1 0 
0 1 
0 
1 0 
1 
0 
0 
1 1 
is a generator matrix. 
(3) The following matrix over the field Z3 
G = 
1 
1 0 
0 0 
0 
0 
0 
2 
2 0 
0 
1
1
1
1
1
1 
is a generator matrix of a linear (6,3)-code. If we put it into row-echelon 
form, we get 
' 1 
1 0 
0 
0 
0 
G ' = 
0 
0 
1 1 0 
0 
0 
0 
0 
0 
1 1 
We see that this is the "stammering" code of all words in which each 
symbol is repeated twice. 
Remark. 
(4) If G is a generator matrix of a code, then the above rule 
(8.1.1), assigning to the information bits u = U1U2 .. .«t the code words v, 
can be put into matrix form 
ν = 11G, 
see formula (7.6.1). This, then, is the encoding rule of the code: from the 
information symbols u, we obtain the code word uG. 
By changing the generator matrix (i.e., the basis), we change the en-
coding rule. 
The most convenient encoding rule is writing down the information 
symbols and supplementing them by the check symbol. 
That is, given 
information symbols ujuj · · •
 u*i
 
w
e send a code word uiu2 . • • «*υ*+ι · · • vn. 
This corresponds to the generator matrices of the form 
G = 
[ I J B ] , 

118 
CHAPTER 8. LINEAR CODES 
where I is the identity Jb χ ib matrix, and Β is a ib by η - Jb matrix. 
Definition. 
(1) A linear code is called systematic provided thai it has a 
generator matrix of the form G = [ί | Β ], where I is an identity matrix. 
(2) Two linear codes Κ and K' of the same length η are said to be 
equivalent provided that they differ only in the ordering of symbols in a 
code word, i.e., provided thai there exists a permutation (ρι,Ρΐ, • • • ,pn) of 
the numbers 1, 2, ... , η such that for each word V\V2 • • • v„, we have: 
v\V2 • • .vn is a code word in Κ <=> vPlvf3.. 
.vPn 
is a code word in K'. 
Example. 
(4) The Hamming (7,4)-code in Example (1) is systematic. 
The even-parity code of length 4 is also systematic since the generator 
matrix G' above has the required form 
' 1 
0 
0 
1 ' 
G' = 
0 
1 0 
1 
0 
0 
1 1 
In contrast, the stammering code Κ above is not systematic. However, 
the following equivalent code K* is systematic: switch the second and 
the fourth symbols, and switch the third and the sixth symbols. That 
is, use the permutation (1,4,6,2,5,3). From the generator matrix G', this 
permutation yields 
' 1 0 
0 
1 0 
0 
G* = 
0 
1 0 
0 0 
1 
0 
0 
1 0 
1 0 
Proposition. Every linear code is equivalent to a systematic linear code. 
PROOF. The generator matrix G of a linear (n, ib)-code Κ has rank Jb; thus, 
it has Jb linearly independent columns (Theorem 7.5). 
I. Suppose that the first Jb columns of G are linearly independent. That 
is, G = [ A j Β ], where A is a regular η χ η matrix. There is a succession 
of elementary row operations which transform A into an identity matrix I 
(see Exercise 7S). If the same elementary row operations are performed 
on G, we get a matrix G' = [ί ί B ' ] . Since G' is also a generator matrix 
of the code K, it follows that Κ is systematic. 
II. In general, given Jb independent columns of G, let (pi.pa 
p„) 
be a permutation which shifts them into the first k positions. From the 
matrix G, we obtain another matrix G*. Let K* be the linear code with 
G* as a generator matrix. Then Κ and K* are equivalent, and K* is 
systematic by I above. 
• 

8.2. 
PARITY CHECK 
MATRIX 
119 
8.2 
Parity Check Matrix 
The concept of a parity check matrix is a direct generalization of the binary 
case (5.4): 
Definition. Let Κ be a linear code of length η over a field F. An n-column 
matrix Η over F is called a parity check matrix for Κ provided that the 
following holds for words ν = v\v2 .. • v„ in F
n: ν is a code word of Κ if 
and only i / H v
t r = 0
T R; shortly: 
ν lies in Κ 
Η 
'
 
v l 
' 0 ' 
— 
0 
0 
Proposition. A systematic code with a generator matrix 
G = [ I j Β ], 
where I is an identity matrix, 
has the following parity check matrix: 
Η = [ — B
t r i I' ], 
where I' is an identity matrix. 
PROOF. Let Κ be an (n, lb)-code with the above generator matrix (where 
I is the Jb χ k identity matrix). If L denotes the row space of the matrix Η 
above, then it is our task to show that Κ is the orthogonal complement 
of L. First, observe that the rank of the matrix Η is η — Jb, since I' is an 
identity matrix. Consequently, the dimension of L
1 is η — (η — k) = k (by 
Theorem 7.7), the same as the dimension of K. Thus, it is sufficient to 
show that every row g of the matrix G fulfils Hg
t r = 0
T R: it then follows 
that every code word ν of Κ also fulfils H v
t r = 0
T R, i.e., that A* is a 
subspace of L
1. 
The equality of dimensions then implies that Κ — 
[by 
Corollary (4) in 7.4]. 
Thus, we want to prove that H G
t r is the all-zero matrix 0. This is an 
easy application of the formula in Exercise 7 U : 
HG tr 
- B
t
r if' 
I 
B
t r 
= 
- B
t r I + I ' B
t r 
= 
- B
t
r + B
t r = 0. 
• 

120 
CHAPTER 8. LINEAR CODES 
Corollary. Every linear (n, k)-code has an n — k by η parity check matrix 
of rank n — k. 
In fact, this has been shown for each systematic code Κ in the proposi-
tion above, and for an equivalent code (see Proposition 8.1), it is sufficient 
to perform the given permutation on the columns of the parity check matrix 
of Κ. 
α 
Example. 
(1) Let us find a parity check matrix of the code Κ of Exam-
ple (3) in 8.1. First, let K* be the equivalent code of Example (4) in 8.1: 
" 1 0 
0 
1 0 
0 ' 
G* = 
0 
1 0 
0 0 
1 
0 
0 
1 0 
1 0 
By the above proposition, the following is a parity check matrix of K*\ 
" - 1 
0 
0 
1 0 
0 " 
' 2 0 
0 
1 0 
0 
H* = 
0 
0 
- 1 
0 
1 0 
0 
0 
2 0 
1 0 
0 
- 1 
0 
0 
0 
1 
0 
2 
0 
0 
0 
1 
We now perform the inverse permutation on the columns of H* in order to 
get a parity check matrix of the original code K: 
Η = 
0 
0 
0 
0 
1 
2 
0 
0 
1 2 
0 
0 
Definition. By the dual code of a linear code Κ is meant the code K
L 
of 
all words orthogonal to all code words of K. (That is, the dual code is just 
another name for the orthogonal complement, see 7.7) 
Observe that the generator matrix of a code Κ is the parity check matrix 
of the dual code K
1 
(and vice versa). 
Examples. 
(2) The dual of the even-parity code is the repetition code. In fact, the 
generator matrix of the latter is [111.. .1], and this is a parity check 
matrix of the former (Example 5.4). 
(3) The dual of the Hamming (7,4)-code of Example (1) in 5.5 is the (7,3)-
code with the following generator matrix: 
0 
0 0 
1 1 1 1 
0 
1 1 0 
0 
1 1 
1 0 
1 0 
1 0 
1 

8.3. 
SYNDROME 
121 
It has minimum weight 4. In fact, each row of G has even weight and 
thus each code word (a linear combination of the rows) also has even 
weight. The weight cannot be equal to 2 because we know that the 
parity check matrix of our code is the matrix G of Example (1) in 8.1. 
By inspection, we see that its rows are nonzero and pairwise distinct 
and thus (by Proposition 5.5), d > 3. Since d is even, d > 4. 
(4) Dual codes of binary Hamming codes are called simplex codes. Thus, 
for each m = 2, 3, 4 
we have a binary ( 2
m — 1, m) simplex code. 
Remark. We know that a linear code of minimum distance d detects 
d — 1 errors and corrects less than d/2 errors (see 4.5). The minimum 
distance is equal to the minimum weight—the proof is the same as that of 
Proposition 5.3. Thus, a linear code 
(a) detects t errors if and only if its minimum weight is > t + 1, 
(b) corrects t errors if and only if its minimum weight is > 2t + 1. 
We see that the theory of error-control codes is not just a linear algebra: 
the crucial question we ask is that of the minimum weight. 
8.3 
Syndrome 
Recall from 5.3 that the difference between the received word w and the 
code word ν is called the error pattern and is denoted by e: 
e = w — v. 
We know only what we have received, and our task is to guess the error 
pattern, from which we then deduce what was sent: ν = w — e. Since we 
use the maximum likelihood decoding (4.4), we always look for the error 
pattern e of the smallest Hamming weight—this yields a code word v, which 
is the most likely. 
An important piece of information about the error pattern which we 
can derive directly from the received word is the syndrome: 
Definition. Let Κ be a linear code with a specified m χ η parity check 
matrix H. By the syndrome of each word w of length n, we understand the 
word s = si«2 ·. « m defined by s
t r = Hw
t r, i.e., 
«1 
«2 
= Η 

122 
CHAPTER 8. LINEAR CODES 
Example. In Example (1) of 5.5, we have used syndromes for decoding: 
each code word w has the syndrome β = 000, each word with the first bit 
corrupted has the syndrome s = 001, etc. 
Proposition. The error pattern has the same syndrome as the received 
word. That is, for each code word ν and each (error-pattern) word e, the 
word w = ν + e fulfils 
He
t r = Hw
t r. 
PROOF. Since the code word ν fulfils Hv
t r = 0
T R, we have 
Hw
t r 
= 
H ( v
t r + e
t r ) 
= 
H v
t r + H e
t r 
= 
O
t r + He
t r 
= 
He
t r. 
Ο 
Remark. No matter how trivial this proposition is, it forms the basis of 
most of the decoding techniques introduced below: We receive a word, find 
its syndrome, and try to find a word with a small Hamming weight having 
that syndrome. 
8.4 Detection and Correction of Errors 
So far we have just moved in the realm of linear algebra: a linear code 
is simply a linear space, the dual code is the orthogonal complement, etc. 
However, the theory of error-control codes is only interested in linear codes 
with good properties for error detection and error correction. 
Recall that a code Κ detects t errors if and only if it has minimum 
weight larger than t. Thus, a "good" linear code is a linear space with a 
large minimum weight d and, of course, a large number k of information 
bits. Unfortunately, these two requirements are contradictory: 
Proposition. The minimum weight d of a linear (n,k)-code satisfies 
d< 
n-jfc + 1. 
PROOF. 
I. Let A" be a systematic linear (n,ik)-code. Then we can choose 
the first k symbols in every code word arbitrarily. Let ν be the code word 
obtained by choosing 1 followed by k — 1 zeros: 
ν = 1000.. .0«t +it;t +j ...«„. 
Then ν φ 0 and the weight of ν is at most η - k + 1. Thus, d < η — k + 1. 

8.4. 
DETECTION 
AND CORRECTION 
OF ERRORS 
123 
II. Let Κ be an arbitrary linear code, and let K' be an equivalent 
systematic linear code (see Proposition 8.1. Then Κ and K' have the 
same parameters n, Jb, and d. Thus, from I, we get the required inequality 
Concerning error correction, we know that a linear code corrects t errors 
if and only if d > It + 1 (4.6). However, more can be said: in Chapter 6, 
we saw a thorough decoding technique based on cosets. Since every linear 
code Κ is a subgroup of the additive group F
n, we can generalize (and 
simplify) the procedure of 6.3 as follows: 
Syndrome Decoding. Let A" be a linear (n, Jb)-code. Let us fix a parity 
check matrix Η with linearly independent rows (see in Corollary 8.2). For 
each word e of length n, we form the coset 
which we interpret as the set of all possible received words w = e-f- ν when 
a code word ν is sent and the channel noise adds the error pattern e. All 
words in the coset have the same syndrome (Proposition 8.3). 
Conversely, for each syndrome, i.e., each word s of length η — Jb, we 
now want to choose a coset leader, i.e., a word e of the smallest Hamming 
weight has the syndrome s. That is, we solve the system 
of linear equations (which has a solution since the rows of Η are linearly 
independent, see Remark 7.6). We choose one of the solutions with the 
smallest Hamming weight and call it the coset leader of the coset of syn-
drome s. This leads to the following decoding procedure: 
(a) When receiving a word w, compute the syndrome s: Hw
t r = s
t r. 
(b) Find the coset leader e of the coset with syndrome β. 
(c) Assume that the code word sent is ν = w — e. 
In the binary case, this is a simplification of the decoding by a standard 
array (see 6.3). The decoding is correct if and only if the actual error 
pattern is one of the chosen coset leaders. (Prove it!) 
Examples 
(2) In Example (2) of 6.3, we introduced a binary code K& by a system of 
equations with the following matrix coefficients: 
again. 
• 
e + A" = { e + v | v a code word } , 
H e
t r = s
t r 
Η = 
1 1 0 
1 0 
1
1
1
1
1 

124 
CHAPTER 8. LINEAR CODES 
Instead of the standard array of Figure 4 in 6.3, we can use syndrome 
decoding, see Figure 1. Suppose we receive 11111 [see Example (3) 
Syndrome 
Coset Leader 
00 
00000 
11 
10000 
01 
00001 
10 
10001 
Figure 1: Choice of coset leaders 
of 6.3]. We find the syndrome 
!] 
and so we decide that the error pattern is e = 10000. Thus, the code 
word 
ν = w - e = 01111 
has been sent. 
(3) Let us construct a coset decoding for the code over the field Z3 given 
by the following parity check matrix: 
Η = 
1 0 
2 
1 0 
0 
1 2 
1 2 
Syndromes are words of length 2; in the alphabet Z3, this means 
nine words. For the syndrome 00, we choose the coset leader 00000. 
Next, we choose coset leaders of Hamming weight 1. For example, all 
the words with a single symbol 1 have pairwise different syndromes 
(see Figure 2), thus, we can choose them as coset leaders. The next 
word of Hamming weight 1 is 20000, and its syndrome is a new word; 
in contrast, 02000 has the same syndrome as 00001, etc. Thus, af-
ter 20000, we have to choose two coset leaders of Hamming weight 2. 
By choosing 10001 and 20002, we complete the list of all syndromes 
in Figure 2. 

8.5. EXTENDED 
CODES AND OTHER MODIFICATIONS 
125 
Syndrome 
Coset Leader 
00 
00000 
10 
10000 
01 
01000 
22 
00100 
11 
00010 
02 
00001 
20 
20000 
12 
10001 
21 
20002 
Figure 2: Choice of coset leaders 
Suppose we send the code word 21122 and receive 
11122. The syn-
drome is 
1 0 
2 
1 0 
0 
1 2 
1 2 
thus, e = 20000, and we decode 
v = w - e = 11122-20000 = 21122. 
The decoding is correct beacause the error pattern made was one of 
the chosen coset leaders. 
8.5 
Extended Codes and Other 
Modifications 
It is often impossible to use a good code simply because of some restrictions 
imposed, e.g., on the code word length or on the information rate, which 
have to be respected. Here we introduce some simple modifications of linear 
codes: extension, puncturing, augmentation, and expurgation. 
Definition. By the extension of a linear (n,k)-code Κ is meant the linear 
(n + l,k)-code 
K* obtained from Κ by concatenating to each code word 
χ = x\x2 .. ,xn 
a new symbol z„+i »n such a way that 
x% = 0. 

126 
CHAPTER 8. LINEAR CODES 
Remark. 
(1) If the code Κ has a parity check matrix H, then the ex-
tended code K* has the following parity check matrix 
H* = 
0 
0 
Η 
0 
1 1 1 · · · 
1 1 
1 
In fact, the last line expresses the equation ΣΓ=ι
 x<
 = 1>
 a n ( * we keep all the 
parity equations expressed by the rows of Η (without the unknown x n+i! 
thus, the last entry of these rows of H* is 0). 
Examples 
(1) The extended Hamming code is the ( 2
m , 2
m - m — l)-code of all words 
χι X2 • • • X2
m of even parity whose first 2
m — 1 symbols form a code word 
in the Hamming code. 
For example, by extending the Hamming (7,4)-code of Example (1) 
in 5.5, we obtain the (8,4)-code K* with the following parity check 
matrix: 
• 0 0 
0 
1 1 
1 
1 0 
0 
1 1 0 0 
1 
1 0 
— 
1 0 
1 0 
1 0 
1 0 
. 1 
1 
1 1 1 
1 1 
1 
By inspecting Figure 6 of 5.5 [containing all the code words of the Ham-
ming (7,4)-code], we conclude that every row of the above matrix H* 
is a code word of the extended Hamming code K*. Since H* has rank 4 
(and ib = 4), it follows that H* is also a generator matrix of A'*. This 
means that the extended Hamming (8,4)-code is self-dual. 
(2) The linear space 
is itself an (uninteresting) code of length n. Its 
extension is the even parity code of length n + 1. 
Remark. 
(2) If a binary linear code Κ has an odd minimum weight d, 
then the extended code has the minimum weight d + 1. In fact, let χ be 
a code word of Κ of the Hamming weight d. Then 5Z?=i
x«
 
= 
I< thus, 
x„ +i = 1. The word x\x2 .. .x„l is a code word of the extended code of 
the minimum weight (which is d + 1). 

8.5. EXTENDED 
CODES AND OTHER MODIFICATIONS 
127 
For example, the extended Hamming codes have minimum distance 4. 
The importance of these codes will be seen in the next section. 
Definition. Let Κ be a linear code of length n. 
(1) By the puncturing of Κ is meant the linear code Κ of length η — 1 
obtained by deleting ike last symbol from each code word. 
(2) By the augmentation of Κ is meant the expansion of Κ by the all-ones 
code word, i.e., the code of all words 
v\v2...vn 
and 
(111... 11) + (υιϋ 2 . . . t > „ ) , 
where v\V2 • • .vn is a code word of K. 
(3) By the expurgation of Κ is meant the code whose code words are 
precisely the even-weight code words of K. 
Remarks. 
(3) Puncturing is the reverse process to extension. Thus, the puncturing 
of the extended Hamming (8,4)-code is the Hamming (7,4)-code. 
(4) Augmenting a binary code means that we get new code words by re-
versing, in each code word, O's to l's and vice versa. 
Proposition. Every linear code either has all code words of even weight 
or it has the same number of even-weight and odd-weight code words. 
PROOF. Let A" be a linear code with a word vi of odd weight. Let vi, 
... , v r be all code words of K. Then νχ + vi, Vi + V2, . . . , Vi + v r are 
code words (by linearity) which are pairwise distinct (since Vi+ν,· = v i + v ; 
implies Vj = ν ; ) . In other words, these are all the code words of K. Now 
whenever a code word v, has even weight, then vi + ν,· has an odd weight, 
and vice versa. Consequently, the addition of vi is a binary correspondence 
between all even-weight code words and all odd-weight ones, which proves 
the proposition. 
• 
Corollary. The expurgation of a linear code Κ is either Κ itself or a code 
of half the size of K. 
Example. 
(3) By expurgating the Hamming (7,4)-code in Figure 6 in 5.5, 
we obtain the code in Figure 3. Conversely, the Hamming (7,4)-code is the 
augmentation of the code in Figure 3. 

128 
CHAPTER 8. LINEAR CODES 
0000000 
0001111 
1100110 
1010101 
0110011 
1101001 
1011010 
0111100 
Figure 3: Expurgated Hamming (7,4)-code 
8.6 
Simultaneous Correction and Detection 
of Errors 
Let us start with an example. The Hamming (7,4)-code has minimum dis-
tance d — 3 (see 5.5); thus, it can either correct single errors or detect double 
errors. However, it cannot do both simultaneously. In other words, when-
ever the Hamming code is used as an error-correcting code, double errors 
escape undetected. Example: we send 0000000 and receive 1010000. The 
syndrome is 010, i.e., we correct the second bit, and assume (incorrectly) 
that 1110000 has been sent. However, it is frequently required that a code 
correct single errors and detect double errors simultaneously. This means, 
of course, that each code word ν has the following property: whenever a 
word w (the word received) has Hamming distance d(w,v) < 2 from v, 
then the Hamming distance of w to all other code words is greater than 1. 
In fact, we then proceed as follows when receiving a word w: we find the 
nearest code word v; if d(v,w) < 1, then we correct w to v, whereas if 
d(v, w) > 1, we announce a large error (i.e., double error, or worse). More 
in general: 
Definition. A block code Κ is said to correct t errors and detect β errors 
simultaneously provided that every code word ν has the following property: 
each word w of Hamming distance d(w,v) < s has Hamming distance 
greater than t from all code words different from v. 
Remark. If a code Κ has the property of this definition, then the simul-
taneous correction and detection are performed as follows: when receiving 
a word w, find the nearest code word ν (in the sense of Hamming dis-
tance). If d(w, v) < t, then correct the received word to the code word v. 
If d(w, v) > t, announce that at least β symbols are incorrect. 
This really works: if at most t symbols have been corrupted, we know 
that d(w,v) < i, while d(w,v') > t for any code word ν' φ ν (see the 

8.6. SIMULTANEOUS CORRECTION 
AND DETECTION 
129 
definition). Thus, we announce nothing and correctly guess the sent word v. 
If more than t symbols, but no more than s symbols, have been corrupted, 
we know that d(w, v) > i, and so our announcement of a "big error" warns 
the user correctly. 
Proposition. A code corrects t errors and detects s errors simultaneously 
if and only if its minimum distance fulfils 
d>t 
+ 
s+l. 
PROOF. 
I. Suppose d > t + s + 1. Let ν be a code word and w be a word 
with d(v,w) < β. For every code word v' φ ν, we have d(v,v') > d > 
t + s + 1. Thus, from the triangle inequality (4.4) 
d(v, w) + d(w, v') > d(v, ν') > ί + s + 1 
we conclude 
a"(w,v') >t + s+ 1 -d"(v,w) >t + s+l-a 
= i + l. 
Therefore, the condition of the above definition is satisfied. 
II. Suppose d < t + s + 1. Let ν and ν' be two different code words of 
Hamming distance d(v,v') = d < t + s. Let w be the word obtained from 
ν by changing the first β symbols in which it diners from the word ν' to 
their values in ν'. Then d(v,w) = s and 
d(v',w) <t + s - s = t, 
which contradicts the condition of the definition above. 
• 
Examples 
(1) Extended Hamming codes correct single errors and detect double errors 
simultaneously. 
The algorithm is simple: receiving a word u>i... wn 
(n = 2
m ) , compute the syndrome s of the shortened word wy.. 
.wn-\ 
using the parity check matrix of the Hamming code. 
(a) If s = 0, then either we received a code word or we received a 
word of odd parity, in which case wn must be corrected. 
(b) If β φ 0, then in case w has even parity, announce that at least two 
bits are corrupted, and in case of odd parity,-correct the ith bit u>j, 
where i has the binary expansion s. 

130 
CHAPTER 8. LINEAR 
CODES 
(2) The repetition code of length 7 can either 
correct 3 errors, or 
detect 6 errors, or 
correct 2 errors and detect 4 errors simultaneously. 
8.7 
Mac Williams Identity 
We have seen in 5.6 how the error-detection probability of a binary code 
can be computed from the weight enumerator 
Λ ( ζ ) 
= 
where Ai is the number of code words of Hamming weight i. We now 
prove a surprising result which makes it possible to compute the weight 
enumerator Ακ±(χ) of the dual K
x of a linear code directly from the 
weight enumerator A%(x) of K: 
Theorem. For each linear (n,k)-code the following MacWilliams identity 
holds: 
t
K
t
( .
) . i i + p .
A , f c i ) . 
(
e . „ ) 
Remark. Some authors use a different concept of weight enumerator, viz., 
the following polynomial of two indeterminates: 
Β(χ,ν) = Σ**
ίν
η-
ί· 
i=0 
In this notation, the MacWilliams identity is nicer: 
BK±(x,y) 
= ±;BK(y-xly 
+ x). 
(8.7.2) 
[Since A(x) = B(x,l) 
and, conversely, B(x,y) = y"A 
it is easy to 
derive (8.7.1) from (8.7.2), and vice versa.] 
PROOF of Theorem. I. We first show that for each word w of length n, 
the scalar products ν · w with all code words ν of A" have the following 
property: 
if w lies in K
x, 
else. 

8.7. M&cWILLIAMS 
IDENTITY 
131 
The case of w in K
x 
is clear: for each code word ν of A", we have ( — l )
v '
w = 
(—1)° = 1, thus, the above sum is equal to the number of code words of K, 
which is 2*. 
Suppose w does not lie in A'
x, i.e., some code word vo of Κ fulfils 
vo · w = 1. Then we will show that the number of all code words of Κ 
orthogonal to w is equal to the number of those nonorthogonal to w—thus, 
the above sum is 0. Let vi, ... , v r be precisely all the code words of Κ 
orthogonal to w. Then we claim that vi + vo, ... , v P + vo are precisely 
all the code words which are not orthogonal to w: 
(a) for each «' = 1 
r, we have (v, + vo) · w = v, · w + Vo · w = 
0 + 1 
= 1; 
(b) whenever a code word ν of Κ fulfils ν · w = 1, then ν — vo is a code 
word orthogonal to w, and, thus, ν — vo = Vj for (precisely one) i. 
II. We are going to verify (8.7.2). The weight enumerator B{x,y) can 
be expressed by means of the Hamming weight |v| of code words as follows: 
BK(*,V) = Y,Aix
iy
n-
i 
= £ x'V-
| v |. 
t=0 
v€K 
Consequently, the right-hand side of the MacWilliams identity is 
BK(V 
- * , » + *) 
= 
X > - * )
|
v
| ( * + y ) " -
| v | 
= 
Σ Π Ι » + ( -
ι Μ · 
v e x ·=ι 
Analogously, the left-hand side can be expressed as 
BKj.(x,y)= 
£ 
*
| w | i /
n -
| w | -
Using I, we can rewrite the last sum as follows: 
BK,(x,y)= 
^ 
f i E i -
1 )
1 " * 
[2* 
In fact, the expression in the brackets is 0 for all words w which do not lie 
in K
x. 
Thus, 
ΒΚ±(*,ν) 
= φΣ, Σί-
1)""*
1"
1*"
-1*
1 
νζκ 
we ζ ; 

132 
CHAPTER 8. LINEAR CODES 
Now, the inner sum ranges over all binary words w. We can arrange the 
summands by the Hamming weight of w: for w = 0, the summand is y"; 
for the η words w of weight 1, we get [(—l)*" + ( — l )
e a Η 
Η (—I)""]*»*
1, 
etc.; finally, for the weight n, we get [(-I)*· + · · + ( - l )
v ' ] x
n . The sum of 
all these summands 
+ · · · + (-Ι)"*]**»""* is easily seen to be 
equal to [ ν + ( - 1 ) · . « ] [y+(-l)-»«] · · · [y+(-l)--«] = Π? β 1[»+(-1)-*]. 
Thus, 
Βκφ,ν) 
= i Σ 
n [ y + ( - 1 ) " . ] = ^ a * ( y - *,» + *)· 
This proves (8.7.2), and (8.7.1) follows. 
Ο 
Examples 
(1) Let us find the weight enumerator of the even-parity code Κ of even 
length n. The dual code Α"
χ is the repetition code [Example (2) of 8.2] 
and its weight enumerator is, of course, 
AKJ.(X) 
= 
l + x
n . 
From the Mac Williams identity, 
A
M 
. 
i L
V £ [
I 
+ 
( - ^ ) - ] . i l ±
! ! ^ l = £ C 
• 
•
+ G ) -
,
+ G ) *
,
+ 
·
+ - " 
(2) The weight enumerator of the Hamming (7,4)-code Κ is 
= 1 + 7 I
3 + 7X
4 + I
7 
[Example (2) in 5.6]. The dual code has the weight enumerator 
It easy to see that this expression can be simplified to 
Λ
κ χ ( χ ) = 1 + 7χ
4. 
Thus, the dual code [Example (3) of 8.2] has the remarkable property 
that all nonzero code words have Hamming weight 4. 

EXERCISES 
133 
Exercises 
8A 
A linear code over Z 5 has the following generator matrix: 
G = 
1 2 
3 
1 2 
2 
2 4 
1 0 
1 1 2 
2 
1 
Find the parity check matrix. 
8 B 
Perform 8A in Z 7 . 
8C 
atic: 
Find out whether or not the following binary linear code is system-
G = 
If not, find a systematic equivalent code. 
8D 
Encode the information bits 101 in both of the codes of Exercise 8C. 
8 E 
Find a generator matrix of a binary code with the following parity 
check matrix: 
' 1 0 
1 
1 0 
0 
0 
1
1
1
0 
1 0 
0 
1 
1 0 
0 
0 
1 0 
0 
0 
1 0 
0 
0 
1 
8 F 
Prove the converse of Proposition 8.2: whenever a linear code has 
a parity check matrix Η = [ A j I' ], where I' is an identity matrix, then 
G = [ I i — A ] is a generator matrix. Which linear codes have such a parity 
check matrix? 
8G 
Which extended Hamming codes are self-dual? 
8H 
Describe the dual of the binary (6,3)-code described by the following 
equations: 
Xl 
= 
* 4 , 
Xl 
= 
X5, 
X3 
= 
*6-

134 
CHAPTER 8. LINEAR CODES 
81 
Construct a table of syndrome decoding for 
(1) the repetition code of length 7, 
(2) the Hamming code of length 7, 
(3) the code of 8H, 
(4) the code over Z 3 with the following generator matrix: 
8 J 
Describe the extension and the puncturing of the code in 8H. 
8K 
Describe the augumentation and the expurgation of the following 
binary code: 
8L 
Nonbinary Hamming codes. 
(1) Generalize Proposition 5.5 to the 
following: a linear code corrects single errors if and only if every parity 
check matrix has the property that no column is a scalar multiple of a 
different column. 
By a Hamming code is meant a code with a parity check matrix Η such 
that 
(a) no column of Η is a scalar multiple of a different column, 
(b) Η is maximal w.r.t. (a), i.e., every word of length m is a scalar mul-
tiple of some column of H. 
Prove that condition (a) can be safeguarded by choosing as columns all 
words with the leading term (i.e., the first nonzero term) equal to 1. 
(2) Verify that the following is a parity check matrix of a Hamming 
code over Z 3 : 
0 
1 1 1 
1 0 
1 2 
Find a table of syndrome decoding that code. 
G = 
1 0 
0 2 2 
0 
1 0 
0 1 
0 
0 
1 1 0 
G = 
1
1
1
0 
0 
0 
0 
1 1 1 
1
1
1
1
0 

NOTES 
135 
(3) Find a parity check matrix of m = 3 rows for a Hamming code 
over Z 3 . 
(4) Prove that for each m, there is a Hamming (
 3" 2~
1, ^"g"
1 — m ) -
code over Z 3 . 
(5) Generalize (4) to any finite field. 
8 M 
Describe how the repetition code of length 7 corrects two errors and 
detects four errors simultaneously. What number of errors can it detect 
while correcting single errors? 
8N 
Find the minimum distance of the (15,4)-simplex code [Example (4) 
of 8.2]. 
8O 
Prove that syndrome decoding is optimal in the sense that no other 
decoding of a linear code Κ can correct a larger collection of error patterns. 
More precisely: suppose a decoding d of Κ is given [i.e., a rule assigning to 
each word w of the considered length a code word d(w)] which corrects all 
error patterns corrected by a syndrome decoding [i.e., for each code word ν 
and each of the chosen coset leaders e, we have d(v + e) = v]. Then d does 
not correct any other error pattern [i.e., whenever e is not a coset leader, 
then there is a code word ν with d(v + e) φ ν]. 
Notes 
The theory of linear error-correcting codes was first restricted to the binary 
case (as in Chapter 5), but the extension of all the basic ideas to any finite 
field is rather obvious. 
The identity of 8.5 is due to MacWilliams (1963). 

Chapter 9 
Reed-Muller Codes: 
Weak Codes with Easy 
Decoding 
We now introduce an interesting class of multiple-error-correcting binary 
codes whose prime importance lies in an easily implementable decoding 
technique: the Reed-Muller codes. Their parameters are listed in Figure 1. 
Length: 
η = 2
m 
Information symbols: 
* = ELo (7) 
Minimum distance: 
d = 2
m ~
r 
Error-control capacity: 
Corrects 2
m
_
r
_
1 — 1 errors by a 
technique based on majority logic 
which is easy to implement 
Figure 1: Parameters of the Reed-Muller code 72(r, m) 
There are closely related punctured Reed-Muller codes, the parameters of 
which are presented in Figure 2. 
One of these codes, the Reed-Muller code 72(1,5), was used by the 
1969 Mariner to transmit pictures of the Moon. The code 72(1,5) has 
137 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

138 
CHAPTER 9. REED-MULLER 
CODES 
Length: 
η = 2
m - 1 
Information symbols: 
*=Σ: = 0 (?) 
Minimum distance: 
d = 2
m -
r - 1 
Error-control capacity: 
Corrects errors as 72(r, m) does, 
but has a better information rate 
and is a cyclic code 
Figure 2: Parameters of the punctured Reed-Muller code 
length 32 with 6 information bits and it corrects 7 errors. Each dot of the 
transmited picture was assigned one of 2
6 = 64 degrees of greyness, and 
these 6 information bits were then encoded into a word of length 32. 
We introduce Reed-Muller codes by means of Boolean polynomials, 
which we first discuss in some detail. To understand the decoding of Reed-
Muller codes, it is more convenient to work with finite geometries, where 
code words become characteristic functions of flats. 
9.1 
Boolean Functions 
Reed-Muller codes are best described by means of Boolean polynomials. 
We first show how binary words translate to Boolean functions and Boolean 
polynomials. Then we introduce the codes and present their basic proper-
ties. However, the decoding is better explained in a different, geometrical, 
presentation of binary words, which we introduce later. 
Definition. A Boolean function f = f(xo,xi 
xm~i) 
of m variables 
is a rule which assigns to each m-tuple (XQ, xit..., 
xm-i) 
of 0's and l's a 
value f(xo,xi,.. 
. , x m _ i ) = 0 or 1. In other words, f is a function from 
to Z 2 . 
Truth Table. A simple way of presenting a Boolean function is to list all 
of its values. That is, we write down all the 2
m combinations of the values 
of all variables XQ, x\, ... , x m-i> &nd then to each combination, we assign 
the value f(xo,x\,... 
,xm-i)- 
F°r notational convenience, we proceed in 
such a way that the columns form binary expansions of the numbers 0, 1, 
2, ... 
(from the first row downward). This presentation is called the truth 
table of the function f. 

9.1. 
BOOLEAN 
FUNCTIONS 
139 
An example of a truth table of a Boolean function of three variables is 
presented in Figure 3. Observe that a truth table of a Boolean function 
xo 
0 
1 
0 
1 
0 
1 
0 
1 
xi 
0 
0 
1 
1 
0 
0 
1 
1 
X2 
0 
0 
0 
0 
1 
1 
1 
1 
f 
0 
1 
1 
0 
1 
1 
1 
0 
Figure 3: An example of a Boolean function of three variables 
of three variables yields a binary word of length 8, and, conversely, every 
binary word of length 8 is the truth table of some Boolean function. We 
thus can (and will) identify Boolean function of three variables with binary 
words of length 8. For example, the word 01101110 is the same as the 
Boolean function in Figure 3. 
More in general, every binary word f of length 2
m is considered as a 
Boolean function of m variables. If we write the indices starting with zero, 
then the binary word 
f = foil 
• · · /2™-l 
is the Boolean function with 
f(0,0,...,0,0) 
= 
/o, 
f(0,0,...,0,l) 
= 
fu 
f(0,0,...,l,0) 
= 
/ a , 
f(l,l,...,l,l) = /,-_,. 
fi = f(im-l, · · ·ι»1ι«θ), 
In general, 
where the number ι has the binary expansion im-i 
• • • *i»o (i.e., i = 
Examples 
(1) There are two constant Boolean functions 
1 = 1 1 . . . 11 
and 
0 = 0 0 . . . 0 0 . 

140 
CHAPTER 9. REED-MULLER 
CODES 
(2) Every variable is a Boolean function. For example, XQ is the Boolean 
function which assigns to each m-tuple ( x 0 , x i , . . . , x m - i ) the first co-
ordinate value xo. Thus, the value is 0 for all even numbers and 1 for 
all odd ones: 
x 0 = 0101010... 1. 
(See Figure 3 for the case m = 3.) In general, 
χ,- is the binary word whose fcth position equals 1 precisely 
when the binary expansion of Jb has 1 in the ith position 
(* = 0, 1, ... , 2"· - 1). 
This follows from the way we are writing the truth tables. For example, 
xi = 00110011...0011, and 
x m _ i = pOO.. .00,111.. .11,. 
For m = 4, the four variables are shown on Figure 4. 
xo 
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1 
X ! 
0
0
1
1
0
0
1
1
0
0
1
1
0
0
1
1 
x a 
0
0
0
0
1
1
1
1
0
0
0
0
1
1
1
1 
x 3 
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1 
Figure 4: The four variables as Boolean functions (m = 4) 
9.2 
Boolean Polynomials 
We now introduce the basic logical operations on Boolean functions of 
m variables. If f is a Boolean function, we denote by /j (t = 0 , 1 , . . . , 2
m —1) 
the ith position of its truth table. 
The logical sum (also called "exclusive or") of Boolean functions f 
and g is the function 
' + «, 
whose value is 1 precisely when either f or g has value 1, but not both. That 
is, the ith coordinate of f + g is /< + ff< (addition modulo 2). The logical 
sum is nothing new: it is just the usual addition in the linear space Z 2 , 
where η = 2
m . 

9.2. BOOLEAN 
POLYNOMIALS 
141 
The logical product (also called "and") of Boolean functions f and g is 
the function 
fg. 
whose value is 1 precisely when both f and g have value 1. That is, the 
ith coordinate of fg is /,</j (multiplication modulo 2). This is a new oper-
ation imposed on Z 2 for η = 2
m , viz., the coordinatewise multiplication 
(/ 2m_i . . ./l/o)(S2»-l •••gigo) - 
(/2·»-102"·-ΐ)···(/ΐ0ΐ)(/θ0θ)· 
Remarks 
(1) Observe that the logical product fulfils 
ff = f. 
Thus, no exponents higher then 1 are ever needed. 
(2) There are other natural operations which, however, can be expressed 
by means of the logical sum and logical product. For example, the 
negation f (which has value 1 precisely when f has value 0) is simply 
expressed as a sum with the constant function 1 = 1 1 . . . 11: 
f = 1+f. 
The operation "or" (for g), whose value is 1 precisely when f or g (or 
both) have value 1, can be expressed as 
f or g = f-(- g + fg. 
(3) We have seen that 1 and each variable Xi are Boolean functions. Other 
Boolean functions can be obtained by addition and multiplication. For 
example: x , X j , l + x , + X j , etc. These are called Boolean polynomials: 
Definition. By a Boolean polynomial in m indeterminates is meant a sum 
of (some of the) following Boolean functions: 1, χ,·, X i X j , . . . , X i , X i g . . .Xi„ 
(where the indices range overO, 1, ... , m—1). The zero function 0 is called 
a Boolean polynomial of degree —1, the function 1 is called a Boolean poly-
nomial of degree 0, and a Boolean polynomial f has degree k > 1 provided 
that k is a maximum number of factors in a summand oft (i.e., f has a 
summand x , , x , a . . . x , k , and no summand has more than k factors). 
Examples 
(1) The Boolean polynomial l + x i x 2 of three indeterminates has degree 2. 
It is the negation of the polynomial 
x i x 2 = (01010101)(OOT10011) = 00010001. 

142 
CHAPTER 9. REED-MULLER 
CODES 
Thus, 
l + xjxa = 11101110. 
The Boolean polynomial 1 + Xixj considered as a function of four 
indeterminates is the word 
1110111011101110. 
(2) The polynomial x,Xj ( t φ j) is the binary word whose Arth position 
is 1 precisely when k has 1 in positions t and j . The number of such 
Jb's is 2
m
-
2 (because we can choose the m — 2 remaining positions of k 
arbitrarily). Thus, the binary word x,Xj has Hamming weight 2
m ~
2 . 
Analogously, X.XJX* has Hamming weight 2
m ~
3 , and, in general, 
X j , x , - 3 . . . X j , has Hamming weight 2
m~* 
for arbitrary pairwise distinct indices t'j, ... , i, chosen between 0 
and m — 1. 
Remark. 
(4) A Boolean polynomial never uses exponents, simply because 
x
2 = χ,·. Thus, there are just four Boolean polynomial of one indetermi-
nate x, viz.: χ, χ + 1, 0, and 1. 
We will later introduce (non-Boolean) polynomials of one indetermi-
nate—those are fundamentally different from the Boolean polynomials since 
all the information is checked by the exponents. 
Translation between words and Boolean polynomials. Every Boolean 
polynomial of m-variables yields a binary word of length 2
m : for a single 
indeterminate, see Example (2) in 9.1, and, further, we perform the required 
additions and multiplications. 
Conversely, every binary word f = /o/i.. /2·»-ι can be translated into 
a Boolean polynomial as follows. First, observe that the last indetermi-
nate x m _ i is the word 
x m _ i = 00...0011...11, 
whose first half is the constant function 0 and the other half is the con-
stant function 1 (now both considered in m — 1 indeterminates, i.e., in 
the length ~2
m = 2
m _ 1 ) . As a consequence, we see that for each Boolean 
function /(xo, x\,..., 
x m _ i ) , the first half of the corresponding word f is 
/ ( x 0 , x i , . . . ,x m_2>0) and the other half is /(xo, xi, · · ·, Xm-a. 1) (both con-
sidered as functions of m — 1 indeterminates). 
The following trivial proposition, applied recursively, then yields an 
algorithm for converting a word into a Boolean polynomial: 

9.2. 
BOOLEAN 
POLYNOMIALS 
143 
Proposition. Every Boolean function f of m variables can be expressed 
[by means of the two halves / ( * o , . . . , *
m - 2 > 0) and f(xo,.. 
•, xm-2, 
1) of 
the corresponding binary word] as follows: 
/ ( * 0 , . . • , Xm-2, 
I m - l ) 
= 
f(x0, 
*
m - 2 , 0) + [f(XQ, 
Xm-2, 0) + f(xo, ••·, Xm-2, 
l)]xm-l-
PROOF. Since xm-\ 
can only take one of the two possible values, 0 or 1, it 
is sufficient to verify that the identity holds for both of them. For xm-\ 
— 0, 
the identity is clear, and for xm-\ 
— 1. we get 
f(x0, 
. . .,Xm-2, 
1) 
= 
f(xo,--,Xm-2,0) 
+ [f(x0, 
· · ·, Xm-2, 0) + f(xi, • • • , Xm-2, 1)] · 
• 
Example. 
(3) Let us translate f = 01101110 into a Boolean polynomial 
(of three variables). We apply the preceding proposition: 
f 
= 
0110+ [0110+ 1110]* 2 
= 
OIIO+IOOO12. 
Next we apply the same proposition to the two words 0110 and 1000 (of 
two indeterminates): 
f 
= 
(01 + [01+ 10]*!) + ( 1 0 + [10+ 001*0*2 
= 
01 + ll*i + 10*2 + 10*!*2-
Finally, an application of the proposition to the words of length 2 (in the 
indeterminate x0) yields: 
f 
= 
(0 + [ 0 + l ] * o ) + (l + [l + l]*o)*i + (l + [l + 0]*o)*a 
+ (1 + [1 + 0]*o)*i* 0 
= 
*o + * 1 + *2 + XQX2 + XlX2 + 
ΧθΧΐΧ2· 
Theorem. The linear space ZJ, where η = 2
m, has a basis formed by all 
one-summand Boolean polynomials, i.e., by the following polynomials 
1, 
Xi 
(i = 0, 1, ... , m - 1 ) , 
x . X j 
(»'· j = 0, 1, ... , m - 1 and i φ j), 
X Q X I . . 
x m - i 
· 

144 
CHAPTER 9. REED-MULLER 
CODES 
PROOF. Every word of length η = 2
m is a Boolean function of m indeter-
minates, and, hence, can be expressed as a Boolean polynomial. It follows 
that the one-summand Boolean polynomials span the whole space ZJ. In 
order to prove that they form a basis, it is sufficient to show that their num-
ber is η = 2
m , the dimension of the space. In fact, we have 1 polynomial 
of degree 0, m polynomials of degree 1, (™) polynomials of degree 2, etc. 
For each k = 0, l , . . . , m — 1, there are (™) one-summand polynomials of 
degree ib, and, thus, the total number of the one-summand polynomials is 
(9.2.1) 
[The last equation is easily derived from the binomial theorem applied to 
(l + l)
m.] 
Ο 
9.3 
Reed-Muller Codes 
Definition. By the Reed-Muller code of length η = 2
m and degree r 
(= 0 , 1 , . . . , m ) is meant the binary code 72(r,m) of all binary words of 
length n, which have, as Boolean polynomials, degree ai most r. 
Examples 
(1) 72(0, m) consists of the polynomials of degree at most 0, i.e., of 0 and 1. 
Thus, 72(0, m) is the repetition code of length 2
m . 
(2) 72(1, m) has basis 1, xo, ... , x m - i - In fact, each polynomial of degree 
at most 1 is a sum of (some of) those m + 1 polynomials, and they are 
linearly independent by Theorem 9.2. Thus, 72(1, m) is a ( 2
m , m + 1)-
code. 
For example, 72(1,3) has the following generator matrix: 
G = 
1 
" 1 
1 1 
1 1 
1 1 
1 ' 
Xo 
0 
1 0 
1 0 
1 0 
1 
xi 
0 
0 
1 
1 0 
0 
1 
1 
.
 X 2 . 
0 
0 
0 
0 
1 
1 
1 
1 
We will see that 72(1,3) is the extended Hamming code (see 8.5). 
72(1,4) is a (16,5)-code and 72(1,5) is a (32,6)-code, which was used 
by the 1969 Mariner, as mentioned in the introduction. 
(3) 72(2, m) has basis 1, x 0 , . . . , x m _ i , x 0 x i , xoXa,... , x
m - 2 X
m - i - Thus, 
this is a ( 2
m , (™) + m + l)-code. For example, 72(2,3) is a (8,7)-code. 

9.3. 
REED-MULLER 
CODES 
145 
It is easy to see that this is just the even-parity code of length 8. 72.(2,4) 
is a (16, ll)-code. We will see that 11(2,4) 
is the extended Hamming 
code. 
(4) Ti(m — l,m) is the even-parity code of length 2
m. 
In fact, every code 
word of Tl(m — 1,m) is a sum of the polynomials X j , x l : ) . . .χ,·,, where 
s < m — 1. Each of these polynomials has an even Hamming weight 
[see Example (2) of 9.2]. Thus, 7l(m — l,m) is a subspace of the even-
parity code. Moreover, H(m — 1, m) has dimension 2
m — 1, because it 
contains all of the basis polynomials in Theorem 9.2 except the last one, 
XoXi.. x m - i - Since the even-parity code also has dimension 2
m — 1, 
the two codes are identical by Corollary (4) of 7.4. 
Theorem. The Reed-Muller 
code Tl(r, m) has 
*=έ(7) 
1=0
 
x 
' 
information 
symbols, and its dual code is Ti(m — r — l,m). 
PROOF. I. The space Tl(r, m) is spanned by all the polynomials χ,·, .. .χ,·,, 
where 0 < s < r, and for a given s, we have (™) such polynomials. By 
Theorem 9.2, all these polynomials are linearly independent, thus, they 
form a basis of H(r, m). We conclude that 7c(r, m) has dimension Σ«=ο (Τ) · 
II. The dimension of ^(r.m)-
1- is 2
m - £ i = o (7) 
(by Theorem 7.7). 
Using Equation (9.2.1) and the well-known identity 
( β ) ~ 
( m - s ) ' 
we see that the dimension of Tl(r, m )
1 can be re-written as follows: 
t ( : ) - ± ( : ) 
»=o
 v 
' 
»=o
 x 
' 
ί ο 
dimft(r, m )
x 
= 

146 
CHAPTER 9. REED-MULLER 
CODES 
We conclude that linear spaces H(r, πι)
λ 
and %(m - r - l,m) have the 
same dimension. By Corollary (4) of 7.4, it is now sufficient to show that 
H(m — r — 1, m) is a subspace of the space 7J(r, m )
χ . 
Thus, it is our task to verify that each Boolean polynomial f of degree 
p < m - r - l i s orthogonal to all Boolean polynomials g of degree q < r 
[and, thus, f lies in K(r, m)
x]. 
Since the scalar product f · g is the sum of 
the coordinates 
of the logical product fg, we just have to show that fg, 
represented as a binary word, has an even Hamming weight. The degree of 
the polynomial fg is at most p+q < m — r - l + r = m— 1 (see Exercise 9A). 
Thus, fg is a code word of H(m-1, 
m), which by Example (4) above implies 
that fg has an even Hamming weight. 
Q 
Example. 
(5) H(m - 2, m) is the extended Hamming code of length 2
m 
(see 8.5). In fact, the dual code is 
H(m - 2,m)-
L =n(l,m) 
and, thus, H(m — 2, m) has the following parity check matrix: 
1 
" 1 
1 1 1 · • 1 
1 
1 
1 " 
xo 
0 
1 0 
1 
• 0 
1 0 
1 
Η = 
xi 
= 
0 
0 
1 1 · • 0 0 
1 
1 
Xm 
. 0 0 
0 
0 
· • 1 
1 
1 
1 . 
We can add the first row to all the other rows, and then interchange it with 
the last row. This yields another parity check matrix: 
Η ~ 
' 1 0 
1 0 
· • 1 0 
1 0 " 
1 
1 0 
0 
· • 1 
1 0 
0 
1 1 1 
1 · • 0 0 
0 
0 
1 
1 1 
1 · • 1 
1 1 
1 . 
By deleting the last column and the last row from the new matrix, we 
obtain a 2
m — 1 by m matrix Ho with pairwise distinct, nonzero columns. 
Thus, Ho is a parity check matrix of the Hamming code. Consequently, 
our matrix 
0 ' 
0 
Ho 
0 
. 1 1 
1 1 
··· 1 
1 1 
1 . 
is a parity check matrix of the extended Hamming code. 

9.4. GEOMETRIC 
INTERPRETATION: 
3-DIMENSIONAL CASE 
147 
Remarks 
(1) Encoding of the Reed-Muller codes can be performed in the usual way 
by multiplying the information word by the generator matrix [see Re-
mark (4) of 8.1]. In other words, the information bits become the 
coefficients of the corresponding Boolean polynomial. 
For example, 
in 72(1, m), we encode the m + 1 information bits as follows: 
[ « 1 , « 2 , • • • . " r n + l 
1 
x o 
x i 
X m - l 
= «1 + U 2Xo + · · · + 
U
m
+ i X
m _ i . 
(2) The minimum weight of the Reed-Muller code 72(r, m) is 
d = 2
m - \ 
In fact, the code word x o X i . . . x
r - i has a Hamming weight 2
m
-
r [see 
Example (2) of 9.2], thus, d < 2
m
-
r . Since 72(r, m) is a subspace of the 
even-parity code 72(m— 1, m), we know that d is even. In 9.6, we will see 
that 72(r, m) can correct 2
m
-
r
_
1 — 1 errors. Thus, by Proposition 4.6, 
d > 2 ( 2
m -
r _ l - 1), and we conclude that 
2
m
_
r < d < 2
m
-
r - 2. 
Since d is even, this proves d = 2
m
-
r . 
(3) In some respect, it is more suitable to work with the punctured Reed-
Muller codes 72(r, m) (see 8.6). This means that in 72(r, m), we delete 
the last symbol from each code word. The resulting code 72(r, m) has 
length 2
m — 1, and the number of information symbols can be shown 
to be equal to that in 72(r, m). The minimum Hamming distance 
of 72(r,m) is 2
m
-
r - 1 and, thus, it can correct 2
m ~
r ~
l — 1 errors, 
the same number as the original code can. We list all punctured Reed-
Muller codes of lengths 7, ... , 127 in Appendix B. 
For example, 72(0, m) is the repetition code of length 2
m — 1 and 72(m — 
1, m) is the Hamming code of that length. 
9.4 
Geometric Interpretation: 
Three-Dimensional Case 
In order to explain the decoding of Reed-Muller codes, it is convenient 
to introduce a new interpretation: instead of with Boolean functions, we 

148 
CHAPTER 9. REED-MULLER 
CODES 
work with flats (or affine subspaces). We first present the case of the three-
dimensional geometry, which corresponds to the codes of length 8, and then 
the general geometry of Ζψ. 
Recall that the conventional three-dimensional Euclidean geometry op-
erates within the linear space R
3, whose points (or vectors) are triples 
a = 0 ^ 2 0 3 of real numbers. We have lines in R
3 , which can be described 
as follows: 
a + <b 
( a , b in R
3 , b φ 0). 
Here t denotes a real parameter, thus, the line a + t b is the set of all points 
{ a + i b | t € R } . Further, we have planes in R
3 : 
a + t b + s c 
(a, b, c in R
3 , b and c linearly independent). 
Here, again, t and s are real parameters. 
Now, the binary Euclidean three-dimensional geometry can be intro-
duced quite analogously. Its points are the vectors of the linear space Z
3 . 
In contrast to the real case above, there are precisely eight points. We can 
enumerate them by the corresponding binary expansions of their indices: 
po = 000, pi = 001, etc., see Figure 5. 
Point 
Characteristic Function 
Po 
000 
00000001 
Pi 
001 
00000010 
P2 
010 
00000100 
P 3 = o n 
00001000 
Ρ4 = 100 
00010000 
Ρ 5 
101 
00100000 
P 6 = 110 
01000000 
Ρ 7 = 111 
10000000 
Figure 5: Points of the binary three-dimensional Euclidean geometry 
Lines of the Euclidean geometry can be described as follows: 
a + ib 
( a , b in Z\, b φ 0), 
where ί is a binary parameter, t = 0, 1. Thus, the line has precisely two 
points: a and a + b. Conversely, every pair a , a ' of points constitutes a 
line, viz., 
a + < ( a ' - 
a ) . 

9.4. GEOMETRIC 
INTERPRETATION: 
3-DIMENSIONAL CASE 
149 
It follows that lines are just all two-point subsets: 
{ P 0 . P l } > 
{ P 0 , P 2 } , 
· · · 
. 
{ P 6 , P 7 } -
The number of lines is 
© • » • 
since a line is just a choice of an (unordered) pair from among the eight 
points. 
Finally, planes of the Euclidean geometry are described as 
a - M i b j - M 2 D 2 
( a , bi, bj in Zf,; bi.bj linearly independent), 
where t\ and <2 are binary parameters. The plane, then, consists of the 
following four points: a , a + bi, a + b?, and a + bi + b 2 . All planes are 
listed in Figure 6. 
Plane 
Characteristic Function 
Boolean Polynomial 
{Pl>P3,P5,P7} 
1 0 1 0 1 0 1 0 
xo 
{ P 2 , P 3 , P 6 , P 7 } 
1 1 0 0 1 1 0 0 
x\ 
{P4,P5,P6,P7} 
1 1 1 1 0 0 0 0 
X2 
{ P 0 , P 2 , P 4 . P 6 } 
0 1 0 1 0 1 0 1 
1 + xo 
{ P 0 , P l , P 4 . P 5 } 
0 0 1 1 0 0 1 1 
{ P 0 , P l , P 2 , P 3 } 
0 0 0 0 1 1 1 1 
1 + Z 2 
{ P l . P 2 , P 5 , P 6 } 
0 1 1 0 0 1 1 0 
xo + xi 
{ P l . P 3 , Ρ4,Ρβ} 
0 1 0 1 1 0 1 0 
Xo + Xl 
{ P 2 , P 3 , P 4 > P 5 } 
0 0 1 1 1 1 0 0 
Xl 
+ * 2 
{ P l 1 P 2 . P 4 . P 7 } 
1 0 0 1 0 1 1 0 
xo + xi + xi 
{ P 0 , P 3 , P4.P7} 
10011001 
l + io + zi 
{ P 0 . P 2 , P 5 . P 7 } 
10100101 
1 + X0 + X2 
{ P 0 , P l , P 6 , P 7 } 
11000011 
1 + 
XI+X2 
{ P 0 , P 3 , P 5 , P 6 } 
0 1 1 0 1 0 0 1 
1 + xo + xi + xi 
Figure 6: Planes in the binary three-dimensional Euclidean geometry 
Observe that the planes passing through the origin ( a = 0) are precisely 
those which can be described by a homogenous equation 
hoxo + hixi 
+ njia = 0. 

150 
CHAPTER 9. REED-MULLER 
CODES 
In fact, such a plane is precisely a two-dimensional subspace of Z\, now see 
Remark (4) of 7.7. It follows that a general plane can always be described 
by a (possibly nonhomogenous) equation 
ΛοΧο + hiXi + h2x2 
— c. 
(Given a plane a + ijbj +/_>b2 and describing the parallel plane t\hi + t2b2 
by the equation ΛοΧο + ΛχΧι + h2x2 = 0, put c = Λοαο + Λιαι + h2a2, where 
a = 
aoaia2.) 
Furthermore, every line can be described by a pair of nonhomogenous 
equations 
hoxo + Λ1Χ1 + h2x2 
— c, 
h'0xo + h[xi + h'2x2 
= 
c'. 
This follows from the fact that every line a + t b is an intersection of two 
planes: choose a basis b, d, d' of the space Z\ and consider the planes 
a + t b + s d and a +t b + s'd'. 
Lines and planes are examples of flats (also called affine spaces). A flat 
in the space Z2 is a coset (6.2) of a linear subspace of the space Z2. That 
is, a flat has the form 
a + A ' = { a + b | b i 8 a point of Κ } , 
where a is a point of Z2, and AT is a linear subspace. If the dimension of Κ 
is s, we call the coset an s-flat. Thus, lines are preisely the 1-flats, and 
planes are the 2-flats. For each point pj, we have a 0-flat { p< }, and there 
is precisely one 3-flat, viz., the whole space Z\. 
Every flat L can be described by the binary word {L — fr .. • fifo denned 
by 
1 
if the point p* lies in L, 
0 
otherwise. 
The word fx, (or the correspondig Boolean function of three variables) is 
called the characteristic function of the flat L. (For 1-flats and 2-flats, see 
the above figures.) 
Given flats L and V, their intersection Lf)L' is obviously characterized 
by the logical product fcft/. For example, the first two planes in Figure 6 
intersect in the line { p a . p ? } . The logical product of their characteristic 
functions is 
x 0 x i = 10001000, 
which is indeed the characteristic function of { p a , P 7 } . 

9.5. GEOMETRIC INTERPRETATION: 
GENERAL CASE 
151 
Remark. The Reed-Muller code 72(1,3) is spanned by the characteristic 
functions of all planes. In fact, we see in Figure 6 that the characteristic 
functions of planes are precisely all Boolean polynomials of degree 1 in three 
variables. 
The Reed-Muller code 72(2,3) is spanned by the characteristic functions 
of all planes and all lines. In fact, every line L is an intersection of two 
planes. Hence, the characteristic function fj, is a product of two Boolean 
polynomials of degree 1, which is a Boolean polynomial of degree 2—a code 
word of 72(2,3). 
9.5 
Geometric Interpretation: General Case 
We now pass to the Euclidean geometry of the m-dimensional binary linear 
space Z
m. 
The points, or vectors, of Z
m 
can, again, be ordered by the 
binary expansions of the numbers 0, 1, ... , 2
m — 1. That is, we put Ζψ = 
{ Po>Pi.P2, · • · , P 2 » - i }> where 
po 
= 
000...00, 
pi 
= 
000...01, 
P 2 
= 
000 ... 10, 
P 2 n . _ i 
= 
111...11. 
Definition. Let Κ be an r-dimensional linear subspace of the space Ζψ. 
Every coset 
a + Κ - { a + b I b lies in Κ } 
of Κ is called an r-flat in the binary m-dimensional Euclidean geometry. 
An (m — l)-flat is also called a hyperplane. 
Notation. Given a basis bi, ... , b r of the subspace K, the r-flat a + Κ 
is also denoted by 
a + i,b, + ... + f rb P. 
It has 2
r points (given by the r choices U = 0,1 for t = 1 , . . . , r). 
Another notation of the r-flat a + Κ is by means of a nonhomogenous 
system of linear equations: whenever the space Κ is described by equations 
H
x t r _ 
0 t r 
Remark (4) 
G f 7 . 7 ^ then the r-flat a + Κ is described by 
the following equations: 
H x
t r = c
t r , 
where 
c
t r = H a
t r . 

152 
CHAPTER 9. REED-MULLER 
CODES 
The number of these equations is m — r. For example, each hyperplane is 
described by a single equation: 
Examples 
(1) Every 0-flat is a one-point set. Thus, there are 2
m different 0-flats, viz., 
{ p o } , ··· , { P 2 - - l } -
(2) Every 1-flat (or line) is a two-point set, 
and, conversely, every two-point set is a 1-flat. Therefore, there are 
(
2
2") 1-flats. 
(3) Let P( denote the flat described by the equation n = 1. That is, P, is 
the set of all points p* which have a 1 in the ith position. For example, 
Po = { Pi,P3.Ps, · · · , P 2 » - l }· 
Each Pi is a hyperplane. In fact, the point p 2. has precisely one nonzero 
position (the ith one), and, thus, 
where Κ is the linear space determined by the equation ib,- = 0. It is 
clear that the dimension of Κ is m — 1. 
The number of all hyperplanes is 2(2
m - 1). In fact, the space Z™ has 
precisely 2
m - 1 subspaces Κ of dimension m — 1 (see Exercise 7H). 
Each of these subspaces Κ has 2
m
_
1 points and, thus, by Remark 6.2, 
there are 2
m / 2
m ~
1 cosets modulo K. 
(4) For i φ j , the intersection Pi Π Pj (i.e., the set of all points with 
a 1 in the ith and jth positions) is an (m — 2)-flat. In fact, for the 
point a = p 2 i + 2 i (with l's just on the ith and jth positions), we have 
where Κ is determined by the equation i,- = kj = 0 . The dimension 
of Κ is m - 2. 
Definition. By the characteristic function of an r-flat L is meant the bi-
nary word tΊ, = / 2 » - i . . ./i/o defined by 
hoio + hxxi + 
(- n m _ i x m _ i = c. 
a + <b = { a , a + b } , 
Pi = p 2. + K, 
ΡίΠΡ}=& 
+ Κ, 
1 
0 
if the point p ; lies in L , 
otherwise. 

9.5. 
GEOMETRIC 
INTERPRETATION: 
GENERAL 
CASE 
153 
Remark. The characteristic function ft can be interpreted as a Boolean 
polynomial / t ( i o , * i , · · - , X m - i ) - It follows from 9.1 that L consists of pre-
cisely those points p< = xoXi · · · X m - i which satisfy /f,(xoi χι,..·, 
x m - i ) = 
1. Shortly: 
x0xi.. 
.xm-i 
lies in L «==> h(xo, xi, • • •, X m - i ) = 1· 
Examples 
(5) The only m-flat, i.e., the space Z
m , has the characteristic function 
1 = 1 1 1 . . . 11. For the hyperplane P< above fpi = χ,·. 
(6) Every hyperplane has a characteristic function which is a Boolean poly-
nomial of degree 1: if the hyperplane L is described by the equation 
hoxo + hixi + 
h hm-\xm-i 
— c, 
then 
/l-(X0, · · · , X m - l ) = Λ 0Χ 0 + /»iXj Η 
h hm-\Xm-l 
+ c + 1. 
In fact, a point xoxi . . . x m _ i lies in the hyperplane precisely when 
ftoxo + 
r hm-iXm-i 
= c. i.e., when /r,(xo,xi, · • - , X m - i ) = 1-
(7) For two flats L and L', the function ftft' is the characteristic function 
of the intersection L(~\ L'. 
For example, the polynomial χ,-χ^ is the characteristic function of the 
(m — 2)-flat, which is the intersection of the hyperplanes P, and Pj. 
More in general: the Boolean polynomial 
X|'| X | J · · · X | , 
is the characteristic function of an (m — s)-flat. 
Theorem. The characteristic function of an r-flat is a Boolean polynomial 
of degree m — r. 
PROOF. In the notation above we have described each r-flat L by m — r 
equations H x " = c
t r , or 
m - 1 
hijXj 
— c, 
for i = 1, 2, . . . , m — r. 
We can rewrite the equations as follows: 
m - 1 
(hijXj 
+ c, + 1) = 1 
for i = 1, 2, . . . , m — r. 

154 
CHAPTER 9. REED-MULLER 
CODES 
Then the following Boolean polynomial of degree m — r 
m—r m—1 
f(xu 
.. .,xm-i) 
= 
^ ( n . j X j + c , · + 1 ) 
is the characteristic function of L: the equation /(xo, · · ·. * m - i ) = 1 holds 
precisely when Y^^-Q
1 (hijXj + c, + 1) = 1 for each %=\, ... ,m — r. 
Π 
Corollary. 
(1) Reed-Muller codes can be characterized geometrically as 
follows: 72(r, m) is spanned by all characteristic functions of flats of di-
mension at least m — r in the binary m-dimensional geometry over Z 2 . 
(2) Every characteristic function of an ( r + l)-flat lies in the dual code 
ofH(r, m). 
In fact, H(r, m) contains all characteristic functions of s-flats, s > m — r, 
by the preceding theorem. That those functions span the space 
H(r,m) 
follows from Example (7). Thus, (1) is true, and (2) follows from Theo-
rem 9.3. 
• 
9.6 
Decoding Reed-Muller Codes 
We now present an interesting and easily implementable decoding tech-
nique for the code 72(r, m). It is based on majority logic, and it can correct 
2"j-r-i _ j errors. In contrast to other decoding techniques, the present 
method does not use syndromes, rather it directly computes the corrupted 
bits from the properties of the received word. The idea is as follows. We 
receive a binary word w = ω 2·»-ι ...w\Wo 
of length 2
m . Assuming that 
less then 2
m - r " "
1 bits are corrupted, we want to determine, for each i = 0, 
... , 2
m — 1, whether or not ω, should be corrected. This can be reformu-
lated by asking whether the position of w corresponding to the 0-flat { ρ,· } 
should be corrected. 
Instead of answering this question directly, we take a broader point of 
view: for each s-flat L, where s = 0, 1, ... , r + 1 , we determine whether the 
positions of the received word w corresponding to the points of L (i.e., those 
bits u>j such that the point p, lies in L) are corrupted or not. Well, not 
exactly. We just distinguish between "even" and "odd" s-flats: an s-flat L 
is called even if the number of all corrupted positions u>< corresponding to 
points p, of L is even, otherwise L is odd. If we are able to determine the 
parity of each s-flat L, the decoding is clear: we correct a bit ui< if and only 
if the 0-flat { ρ,· } has odd parity. The trick is that we start with s = r + 1 
and then proceed to the lower dimensions. 

9.6. DECODING REED-MULLER 
CODES 
155 
Thus, the first step of decoding the word w is to determine, for each 
(r + l)-flat L, whether L is odd or even. This is performed by computing 
the scalar product of w with the characteristic function f/,: 
L is even <=> w · fi = 0. 
In fact: if w is a code word, then w · fx = 0 by Corollary (2) in 9.5. Now, 
if precisely two of the bits of w corresponding to points of L are corrupted, 
the value w · ft will not be changed. Analogously with 4, 6, ... bits. But 
if an odd number of bits of w corresponding to points of L are corrupted, 
then the received word fulfils w · fL — 1. 
For e-flats L, where s < r, we proceed by majority logic: suppose we 
already know the parity of every ( « + l)-flat containing L, then we say that 
L is odd if a majority of these (s+ l)-flats is odd, and L is even if a majority 
of them is even. The reason why this procedure works is that each s-flat is 
contained in a large number of (s + l)-tlats: 
Theorem. Every s-flat L in the binary m-dimensional geometry is con-
tained in exactly 2
m~' 
— 1 different (s + \)-flats. Furthermore, each point 
outside of L lies in precisely one of these (s 4- l)-flats. 
PROOF. 
I. We prove first that every s-dimensional linear subspace Κ 
of Z
m is contained in precisely 2
m
- ' — 1 different subspaces of dimen-
sion s + 1. 
Every (s + l)-dimensional space Κ containing Κ has the form Κ = 
Κ + 1 b for some point b outside of K, where 
Κ + t b = { a + t b I a € Κ and t = 0,1 }. 
This follows immediately from the fact that every basis of Κ can be ex-
tended (by a single vector) to a basis of Κ—see Theorem 7.4. 
Next observe that for two points b, b' outside of K, the linear subspaces 
Κ +1 b and Κ +1 b' coincide if and only if b and b' lie in the same coset 
modulo Κ (6.2). In fact, if Κ + tb = Κ -Mb', then b can be expressed as 
a + t b', where a lies in Κ—thus, t φ 0 and we get 
b - b ' = a € 
K. 
By Proposition 6.2, b and b' lie in the same coset. Conversely, if b — b' = a 
is a point of K, then b = a + b' lies in Κ + t b', and b' = - a + b lies 
in K+tb; 
thus, K + tb = K + tb'. By Remark6.2, there are 2
m / 2 * cosets 
modulo K. One of them is Κ itself, and all other cosets contain only points 
outside of K. Consequently, there are 2
m
_
J — 1 different spaces Κ +1 b for 
b outside of K. 

156 
CHAPTER 9. REED-MULLER 
CODES 
II. Every s-flat 
L = a + Κ 
(dim A = a) 
is contained in 2
m~* — 1 different (s+l)-flats a+K', where K' is an (e+1)-
dimensional space containing Κ (this follows from I.). It remains to prove 
that every (« + l)-flat a' + K' containing L has the mentioned form. That 
is, we want to show that a + K' = a' + K'. In fact, since a lies in a' + K', 
the difference a —a' is in K' and, hence, the points a and a' lie in the same 
coset modulo K. 
III. Every point b outside of the s-flat L = a + K lies in an (s + l)-flat 
containing L, viz., the flat a + K', where K' = Κ +1(b — a). In fact, b lies 
in a + [K -M(b — a)] because by choosing t — 1 and 0 £ K, we have 
To verify that K' has dimension s + 1, it is sufficient to show that b — a 
lies outside of K: in fact, if b - a lies in K, then a + (b - a) = b lies in L. 
Finally, we prove that the (β + l)-flat is unique. 
Every (s + l)-flat 
containing a + Κ has the form a+K', 
where K' is an (« + l)-dimensional 
linear space containing K. If b lies in such a flat a + K', then b - a is also 
a point of K'\ thus, K' contains the linear space Κ + (b — a). The last two 
spaces have both dimensions 8+1, hence, they coincide. 
• 
Corollary. If the number of errors in α received word is less then 2
m
-
r
-
1 , 
then for each s-flat L, 0 < a < r, the majority of(s + l)-flais containing L 
have the same parity of errors as L. 
In fact, let t < 2
m
_
r
-
1 bits of the received word w be corrupted. We know 
that L is contained in 
(s+l)-flats V, and each such flat V is determined by one point outside of L. 
Let us begin with all points pj outside of L such that u>,- is a corrupted bit. 
There are at most t corresponding (s+ l)-flats V. All the remaining flats V 
have the property that they contain no point p< outside of L such that u>,-
is incorrect. Thus, V has the same error parity as L. The number of the 
latter flats is at least ( 2
m
-
r - 1) - 1 > t; thus, they form a majority. 
Ο 
b = a + [ 0 + (b-a)]. 
2' 
(m—r - 1 > 2t 

9.6. 
DECODING REED-MULLER 
CODES 
157 
Decoding Algorithm for the Reed-Muller Code 72(r, m) 
First step: Receiving a word w, call each (r + l)-flat L odd if the scalar 
product of its characteristic function ft with w is 1, otherwise call L even. 
That is, for (r + l)-flats L: 
r . 
f odd if w · ft = 1, 
L is < 
even if w · ft = 0. 
Recursive steps: For all s = r, r — 1, ... , 0 such that each (s + l)-flat 
has already been called odd or even, call an s-flat L odd if a majority of 
(s + l)-flats containing L is odd, otherwise call L even. 
Last step: Correct the ith bit of w if and only if the 0-flat { p, } has been 
called odd. 
Example. Working with 72(1,3), we have received 
11101010. 
The first step is to decide which planes (see Figure 6 in 9.4) are odd and 
which are even. For example, the plane L — { P 1 . p 3 . p 5 . p 7 } is even since 
w ft = 11101010· 10101010 = 0. See Figure 7. 
Next we must decide, 
Plane 
Parity 
Plane 
Parity 
{ P l . P 3 , P 5 , P 7 } 
even 
{ P l . P 3 , P 4 , P 6 } 
odd 
{ P 2 . P 3 . P e . P 7 } 
odd 
{ p 2 . P 3 . P 4 . P 5 } 
even 
{ p 4 . P s . P 6 . P 7 } 
odd 
{ p o , P 3 , P 4 . P 7 } 
even 
{ P 0 , P 2 , P 4 , P 6 } 
odd 
{ P 0 . P 2 . P 5 . P 7 } 
even 
{ P 0 . P l . P 4 . P 5 } 
even 
{ P 0 , P l , P 6 , P 7 } 
odd 
{ P 0 . P 1 . P 2 . P 3 } 
even 
{ P 1 . P 2 . P 4 . P 7 } 
even 
{ P l . P 2 , P 5 , P 6 } 
odd 
{ P 0 , P 3 , P 5 , P 6 } 
odd 
Figure 7: First step of decoding 11101010 
for each line L, whether L is odd or even. For example, the line { po.Pi } 
is contained in three planes (see Figure 6 of 9.4): 
{ P 0 . P 1 . P 4 . P 5 } 
- 
even, 
{ P 0 . P 1 . P 2 . P 3 } 
- 
even, 
{ p o . P 1 . P e . P 7 } 
- 
odd. 

158 
CHAPTER 9. REED-MULLER 
CODES 
Line 
Pariry 
Line 
Pariry 
{Ρο,Ρι } 
even 
{ P 2 . P 4 } 
even 
{ Ρ ο , Ρ ϊ } 
even 
{ P 2 , P 5 > 
even 
{Po.Pa} 
even 
{Pa.Pe} 
odd 
{ P 0 , P 4 J 
even 
{ P 2 , P 7 } 
even 
{Po.Ps} 
even 
{ P 3 , P 4 } 
even 
{ρο.Ρβ} 
odd 
{ Ρ 3 , Ρ β } 
even 
{ P 0 , P 7 } 
even 
{ P s . P e } 
odd 
{ P 1 . P 2 } 
even 
{ P 3 , P 7 } 
even 
{ P i . P a } 
even 
{ P 4 , P s } 
even 
{ Ρ 1 . Ρ 4 } 
even 
{ Ρ 4 , Ρ β } 
odd 
{ P i . P s } 
even 
{P4,P7} 
even 
{ P i . P e } 
even 
{Ρβ,Ρβ} 
odd 
{ Ρ ι . Ρ τ } 
even 
{ P 5 . P 7 J 
even 
{ P 2 , P 3 } 
even 
{ Ρ β , Ρ 7 } 
odd 
Figure 8: Second step of decoding 11101010 
By majority vote, the line { ρ ο , Ρ ι } is even. We must go through all the 
lines and perform such a majority vote. The result is seen in Figure 8. 
Finaly, we are prepared to correct the individual bits. The point po is 
contained in seven lines, one odd and six even; thus u>o will not be corrected. 
Also pi is contained in one odd and six even lines, etc. The only bit to 
correct is u>e because ρβ is contained in seven odd lines. The word sent 
is 
10101010 
[which is the polynomial 1 + x\, a code word of 72(1,3)]. 
Concluding Remark. 
We have presented a method by which the Reed-
Muller code 72(r, m) can correct 2
m
-
r
_
1 — 1 errors. As explained in Re-
mark (1) of 9.3, this proves that the minimum distance is 2
m ~
r . 
For example, the code 72(1,5) is a (32,6)-code [see Example (1) in 9.3], 
which corrects 2
5
-
1
_
1 — 1 = 7 errors, as mentioned in the introduction. 

EXERCISES 
159 
Exercises 
9A 
Prove the following about degrees of Boolean polynomials: 
(1) For nonzero Boolean polynomials f and g, the degree of fg is the sum 
of the degrees of f and g. 
(2) The degree of f + g is the maximum of the degrees of f and g. 
9 B 
Find a binary (15,5)-code correcting triple errors. (Hint: use a punc-
tured Reed-Muller code.) 
9C 
When using 72(1,3), decode 01111100. Verify the correctness of your 
decoding. Encode information bits 1011. 
9D 
When using 72(2,3), decode 0111100. 
9 E 
When using 72(2,4), decode 1111111011111111. 
9 F 
What Boolean polynomial has the truth table 
(1) 10100110? 
(2) 1010011010100110? 
9G 
Find the truth table of the Boolean polynomial 1 + zo + *ι*2 
(1) as a function of three variables, 
(2) as a function of four variables. 
9H 
What is the relationship between simplex codes (8.2) and the Reed-
Muller codes 72(1, m)? 
91 
Compute the number of 2-flats in the Euclidean geometry in Ζψ • 
93 
Prove that each hyperplane L in the Euclidean geometry in Z™ has 
the property that its complement Ζ™ — L is a hyperplane too. (Hint: see 
the proof of Theorem 9.6.) 
9K 
Is every Boolean function a characteristic function of some flat? 
Characterize such functions! (Hint: express each flat as an intersection 
of hyperplanee.) 

160 
CHAPTER 9. REED-MULLER 
CODES 
Notes 
Reed-Muller codes are named after Reed (1954) and Muller (1954). The 
former paper is the source of the decoding method. 

Chapter 10 
Cyclic Codes 
The search for efficient error-correcting codes has been very successful 
within the class of linear codes satisfying the additional requirement that 
a cyclic shift of a code word be a code word. Such codes are called cyclic. 
It turns out that a strong algebraic structure is added to all cyclic codes: 
besides the addition and scalar multiplication, we have an important oper-
ation of multiplication, which is best understood if code words are regarded 
as (coefficients of) polynomials. 
We introduce polynomials and show how they are used for a succint 
presentation of a cyclic code. Then we present encoding and decoding 
methods specific for cyclic codes (in particular, for the fundamental Golay 
code). We further introduce correction of bursts of errors, and we construct 
good cyclic burst-error-correcting codes called Fire codes. 
Additional important properties of polynomials will be treated in Chap-
ter 11. 
10.1 
Generator Polynomial 
Definition. A linear code is called cyclic if for each code wordvov\ .. 
.v„-i, 
the cyclic shift υη_ινονι · • υ η-2 
α'*ο a code word. 
Examples. The even-weight code is a cyclic code, and so is the repetition 
code. In contrast, the Hamming code with the parity check matrix in 5.5 
is not a cyclic code: whereas 1110000 is a code word, 0111000 is not. 
Although the definition of cyclic codes is very simple, the property of 
being cyclic is of prime importance both for the theory and for implemen-
tation. 
161 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

162 
CHAPTER 
10. CYCLIC 
CODES 
We use indices starting from 0 rather than 1 (as in vovi ... vn-\) 
because 
we are going to use a description of words via polynomials. For example, 
instead of 101200 we, write 1 + x
2 + 2x
3. The advantage is that the cyclic 
shift, 010120, is now expressed by a product with the indeterminate: (1 + 
x
2 + 2x
3)x. Analogously, the second shift 001012 is (1 + x
2 + 
2x
a)x
2. 
Definition. By α polynomial in one indeterminate χ over a field F is 
meant an expression of the form 
a0 + a\X + a2x
2 + 
h akx
k, 
where ao, ... , α* are elements of F; the summands α,χ' with α,· = 0 can 
be omitted from the expression. 
The polynomial is said to have degree k if at φ 0; the polynomial 0 is 
said to have degree - 1 . / / a t = 1, inen the polynomial is called monic. 
Caution! A polynomial is not a function of the form / ( x ) = αο + αχχ + 
r atx*. For example, the following polynomials over Z2 
1 + χ + x
2 
and 
1 + x
2 + x
4 
are certainly different (in fact, their degrees are different). Nevertheless, 
they define the same function, viz., the constant function of value 1. 
A polynomial can be understood as a word over F, aoai.. .at, except 
that the length of the word is not specified: the same polynomial corre-
sponds to the words a o . . a t 0 , ao...at00, etc. However, whenever the 
length η of words is given, then polynomials of degree at most η — 1 can 
be identified with words by the bijective correspondence 
α0αι ...α η_! <—m 0 + αιχ+ · · · + αη-ι*"
- 1· 
From now on, polynomials and words will be freely interchanged, provided 
the word length is given. 
Operation on polynomials. Given polynomials a(x) = ao + ajx + 
(-
a„x
n and 6(x) = 6o + bix + 
h bmx
m 
over a field, we define: 
(1) Addition by 
a(x) + 6(x) = (a 0 + 6 0) + (ai + ij)* + (a 2 + b2)x
2 + ••· , 
i.e., a(x) + 6(x) = c(x), where c, = α,· + 6,· [i = 1, 2, ... , max(n,m)]. 
This is precisely the addition of words introduced in 7.3. For example, if 
a(x) = 1 + χ -(- 2x
4 and 6(x) = 1 + 2x + x
2 over Z 3 , then 
a(x)+b(x) 
= 2 + x
2 + 2x
4. 
(In another notation: 11002+ 12100 = 20102.) 

10.1. 
GENERATOR 
POLYNOMIAL 
163 
(2) Multiplication by 
a(x)6(x) = a0b0 + (a\b0 + a0bi)x + (a 26 0 + ai6i + a o ^ ) *
2 + · · · , 
i.e., a(x)6(x) = c(x), where 
Ci;= a,6o + ai_i6j -f 
(-ai6,_i + aofc,- 
for 
t = 0 , l , . . . , n m . 
For example, if a(x) — 1 + χ + 2x
4 and 6(x) = 1 + 2x + x
2 over Z 3 , then 
a(x)6(x) = 1 + x
3 + 2x
4 + x
5 + 2x
6. 
Observe that, in particular, the product xa(x) corresponds to the shift of 
the word a(x) to the right. 
(3) Division of a(x) by 6(x), provided that 6(x) φ 0, as the following 
pair of polynomials q(x) (the quotient) and r(x) (the remainder): 
a(x) = g(x)6(x) + r(x) 
and 
degree r(x) < degree 6(x) 
(10.1.1) 
It is not difficult to verify that (10.1.1) determines both the quotient and 
the remainder uniquely, see Excercises 10A and 10B. Let us perform the 
long division on the polynomials a(x) = 1 + χ + 2x
4 and 6(x) = 1 + 2x + x
2 
over Z 3 : 
2x
2 + 2x 
= 
q(x) 
x
2 + 2x + 1) 2 x
4 
+ χ + 1 
2 x
4 + 
x
3 + 2x
2 
2 x
3 + χ
2 + χ + 1 
2 x
3 + x
2 + 2x 
χ + 1 = 
r(x) 
Remarks 
(1) For two nonzero polynomials o(x) and 6(x), the degree of a(x)6(x) is 
the sum of the degrees of a(x) and 6(x). In fact, let o(x) have degree η 
and b(x) degree m. Then for c< = α,·6ο + 
h α06,·, we see that 
Cn+m 
= 
(a„+ m6o Η 
1- an+\bm-i) 
+ 
anbm 
+ (an-ibm+x 
+ 
r 
a0bm+„), 
where the summands in the first set of paretheses are 0 because a, = 0 
for all i > n, and those in the second set of parentheses are 0 because 
bj = 0 for all j > m. Thus, 
c m + m
 
= " n ' m φ 0 
(since o„ φ 0 φ 
bm). 

164 
CHAPTER 
10. CYCLIC 
CODES 
For each k > n + m, the summands of 
Cfc = (atbo + 
h α η +ι6*_„_ι) + (a„6t_„ + 
h a 0 6 t ) 
are all zeros. Thus, c(x) has degree n + m. 
(2) Polynomials can be cancelled: 
a(x)c(x) = b(x)c(x) 
implies a(x) — 6(x), 
whenever e(x) φ 0. 
In fact, [a(x) — 6(x)]c(x) = 0 has degree - 1 . If c(x) φ 0 and a(x) -
4(x) / 0, then, by (1), the degree could not be negative. Thus, a(x) -
6(x) = 0. 
Proposition. Every cyclic code Κ of length η has the following property: 
g(x) lies in Κ => q(x)g(x) lies in Κ 
for all polynomials q(x) such thai the degree of q(x)g(x) is smaller than n. 
PROOF. Let β be the degree of g(x). If β = —1, there is nothing to prove: 
q(x)g(x) = 0 lies in A". Suppose s > 0, then the degree r of q(x) satisfies 
r < η — 8. Put g(x) = g0 + g\x + · · · + 9,x'• 
For each i < η — s, the 
polynomial x'g(x) = p 0x' + gix'
+l 
+ 
r <7»x*
+' corresponds to the word 
of length η obtained by i cyclic shifts of the word gog\ .. .g, to the right. 
Since our code is cyclic, x'g(x) is a code word. Consequently, any linear 
combination of those code words is a code word. It follows that 
q(x)g(x) = q0g(x) + qixg(x) + •·• + 
qrx
rg(x) 
is a code word. 
• 
It turns out that the whole cyclic code can be described by means of a 
single polynomial. Recall that the code { 0 } is called trivial. 
Theorem. Every nontrivial cyclic (n,k)-code contains a code word g(x) 
of degree η — k. Then code words are precisely all the multiples 
q(x)g(x), 
where q(x) is any polynomial of degree smaller than k. The code has the 
following generator matrix: 

10.1. 
GENERATOR 
POLYNOMIAL 
165 
9(x) 
xg(x) 
G = 
x^gix) 
go 
gi 
32 
0 
go 
gi 
gi 
0 
0 
g0 
gi 
gi 
gn-k 
0 
0 
gn-k 
0 
0 0 0 
0 0 0 
0 
0 0 
9n-k 
0 
0 
0 
0 
0 
SO 
9i 
</2 
9n-k J 
PROOF. Let <jf(x) be the nonzero code word of the smallest degree a. We 
will prove that a = n — k. 
(1) By the preceding proposition, all the multiples q(x)g(x) 
are code 
words. Conversely, we will prove that every code word u/(x) is a multiple 
of g(x) by a polynomial of degree smaller than n - s . Dividing w(x) by g(x), 
we obtain the quotient q(x) and the remainder r(x) such that 
Since both u>(x) and q(x)g(x) are code words, their difference r(x) = 
w(x)~ 
q(x)g(x) 
is also a code word. But the degree of r(x) is smaller than a, the 
smallest degree of a nonzero code word. Thus, r(x) = 0. Consequently, 
U J ( X ) = q(x)g(x). 
By Remark (1), the degree of q(x) is either —1, or it 
is equal to the difference degw(x) — degg(x), 
which is smaller than η — s 
[since the code word ui(x) has degree smaller than n]. 
(2) The η — a code words g(x), xg(x), • •. , x
n~'~
1g(x) 
form a basis of 
the code: by (1) every code word is a linear combination of those words. By 
Remark (2), those words are linearly independent, since every linear combi-
nation has the form q(x)g(x) [where q(x) is a polynomial], and q(x)g(x) = 0 
implies q(x) = 0. 
(3) Thus, we see that the dimension of the code is Jb = η — a, in other 
words, a — η — Jb. By substituting η — k for a above, we conclude the proof 
of the theorem. 
Ο 
Definition. By α generating polynomial of α nonirivial cyclic (n, k)-code 
is meant a code word of degree n — k. 
Examples 
(1) The even-parity code of length η is cyclic. The code word of degree 
u»(x) = q(x)g(x) + r(x) 
and 
degree r(x) < β. 

166 
CHAPTER 
10. CYCLIC 
CODES 
η — k = 1 is just one: 1 + x. The generator matrix is 
"1 
1 0 
0 0 
··· 
0 " 
0 
1 1 0 
0 
··· 
0 
0 
0 
1 1 0 
··· 
0 
0 
0 
0 
·•· 
O i l 
(2) The repetition code is a cyclic code whose generator polynomial is 
g(x) = 1 + χ + χ
2 + ··· + χ"
- 1. 
(3) Consider the binary cyclic code Κ of length 7 with the generator poly-
nomial g(x) = 1 + χ + x
3. It has the following generator matrix: 
1 1 0 
1 0 
0 
0 
0 
1 1 0 
1 0 
0 
0 
0 
1 1 0 
1 0 
0 
0 0 
1 1 0 1 
It is easy to find a parity check matrix: 
Η = 
1 0 
1
1
1
0 
0 
1
1
1
0 
0 
1 0 
0 
1
1
1
0 
0 1 
Since the columns are pairwise distinct and nonzero, we see that Κ 
is a Hamming (7,4)-code. Thus, a different ordering of the columns 
of Η from that used in 5.5 leads to an (equivalent) cyclic code. Cyclic 
Hamming codes are systematically presented in 12.1. 
(4) Consider a cyclic code of length 6 in the alphabet Z 3 with the generator 
polynomial g(x) = 2 + x
2. It has the following generator matrix: 
2 
0 
1 0 0 0 
0 
2 
0 
1 0 0 
0 
0 
2 0 
1 0 
0 
0 
0 
2 0 1 
This is a (6,4)-code. 
Remark. 
(3) We see that every cyclic code can be described by a single 
polynomial, the generator polynomial g(x), substituting the generator ma-
trix. Later we will find another polynomial, substituting the parity check 
matrix. 

10.2. 
ENCODING CYCLIC 
CODES 
167 
Observe that the generator polynomial is unique up to a scalar multi-
ple. More precisely: every cyclic code has a unique monic generator poly-
nomial g(x) [and all other generator polynomials are precisely the scalar 
multiples tg(x) for t φ 0]. In fact, if g(x) is a code word of degree η — Jb, 
then so is tg(x). 
On the other hand, two generating polynomials of the 
same code have the same degree, and one is a multiple of the other one (by 
the theorem above), thus, one must be a scalar multiple of the other one. 
From now on, generator polynomials are supposed to be monic. 
One of the important properties of cyclic codes is the simplicity of encoding 
information symbols. There are two encoding methods: one, more direct 
but nonsystematic, is based on multiplication by the generator polynomial, 
and the other one, which is systematic, is based on division by that poly-
nomial. 
Nonsysiematic 
encoding. This is just a special case of the general en-
coding method: multiply the information symbols uouj... 
by the gen-
erator matrix (see Theorem 10.1): 
In other words the ib information symbols are used as coefficients of the 
polynomial u(x) = uo + u\x + • · • + u t _ i x
i _ 1 , and then the code word 
u(x)g(x) 
is sent. 
The operation of multiplication by a given polynomial g(x) can be real-
ized by a simple shift-register circuit. Observe that in the case g(x) = 
the multiplication of a polynomial u(x) by g(x) is just the scalar multi-
plication by y, followed by i shifts to the right. For g(x) = 5ΖΓ^ο*Λ
χ<' 
these operations are simply added. A multiply-by-(7(x) circuit is shown in 
Figure 1, where -*£]-*• denotes a shift-register stage, and 
10.2 
Encoding Cyclic Codes 
[u 0ui · · -Ut-l] 
g(x) 
= (u0 + uix + 
r-ut-ix*
 
l)g(x). 
X
k 
lg(x) 
b 
and 
a 
denote the adder and scaler (in the field under consideration), respectively. 

168 
CHAPTER 
10. CYCLIC 
CODES 
u(x)g(x) 
Figure 1: A circuit for multiplication u(x) H+ 
u(x)g(x) 
Examples 
(1) The Hamming cyclic code with the generator polynomial g(x) = 1 + 
χ + x
3 [see Example (3) of 10.1] has the following encoder: 
. 
• © 
<+)— «(«)*(*) 
« 3 « 2 « 1 « 0 
• ! 
«• 
» 
—Λ 
• 
• 
The information bits 1001 are encoded as follows: 
u(x)g(x)= 
( l + i
3 ) ( l + x + x
3 ) = l + x + 
x*+x
6. 
The code word 1100101 is sent; this encoding is not systematic. 
(2) The cyclic code over Z 3 with the generator polynomial g(x) = 2 + x
2 
[see Example (4) of 10.1] has the following encoder: 
u(x)g(x) 
U3U2U1U0 
Systematic encoding. The following encoding method is systematic, pro-
vided that we read the polynomials from the highest degree to the lowest 
one (opposite to our custom otherwise). The idea is simple: use the in-
formation symbols as coefficients of a polynomial starting with the highest 
degree; i.e., form the polynomial 
« ( * ) = U n - l * "
-
1 + Un-2X
n~
2 
+ ••• + t i n - i * "
- * . 

10.2. 
ENCODING CYCLIC 
CODES 
169 
Divide u(i) by the generator polynomial g(x), and denote by q(x) the 
quotient (which is not used) and by r(x) the remainder: 
u(x) = q(x)g(x) + r(x) 
and 
degree r(x) < n — k. 
Then send the polynomial 
u(x) — r(x). 
This is a code word, since it is a multiple of g(x): 
u(x) - r(x) = 
q(x)g(x) 
(see Theorem 10.1). 
For example, if we use the above Hamming code with g(x) = 1 + χ + χ
3, 
then the information bits 1001 are encoded as follows: divide u(x) = x
6 + x
3 
by g(x): 
χ
3 
+ χ 
= 
q(x) 
x
3 + x + l ) 
x
6 
+ x
3 
x
6 + x
4 + x
3 
X
4 
+ x
2 + x 
x
2 + χ 
= 
r(x) 
The remainder is r(x) = x
2 + x. Thus, we send the polynomial 
u(x) - r(x) = x
6 + x
3 + x
2 + x, 
or the word 1001110 (provided that we read polynomials from degree 6). 
Observe that whereas the highest coefficient of the remainder is at 
j.n-t-1 [ s m c e 
Γ ( χ ) has degree smaller than η - k], the lowest coefficient 
of the polynomial u(x) is that at x
n
_
t . Thus, the two polynomials u(x) 
and r(x) are not mixed at all, and this explains why the encoding is sys-
tematic. 
Fortunately, digital circuits (unlike human beings) divide by a given 
polynomial as easily as they multiply. A divide-by-y(x) feedback circuit, 
based on the long division of polynomials, is shown in Figure 2. It takes 
η shifts for the quotient to pass out of the shift register; at that moment, 
the contents of the shift register are the coefficients of the remainder r(x) = 
ro + r\x + 
1- r n _ t _ i x
n " "
f c - 1 . By using this idea (and the fact that we 
do not need the quotient), a simple encoding circuit for cyclic codes can 
be constructed. For example, a systematic encoder for the Hamming code 
of length 7 [with g(x) = 1 + χ + χ
3] is shown in Figure 3: for the first 
4 shifts, the information bits u6u5u4u3 
are sent both to the channel and the 

170 
CHAPTER 10. CYCLIC 
CODES 
t l „ _ t . . . U „ _ l 
quotient 
Figure 2: A circuit for division u(x) t-> u(x) :g(x) 
divide-by-y(x) circuit, and since at that moment the shift register contains 
the remainder (i.e., the check bits ujUiUi), we disconnect the feedback by 
opening the gate and send the contents of the shift register to the channel. 
Open: the last 3 shifts 
U 3 U 4 U 5 U 6 
Up: the last 3 shifts 
code word 
Down: the first 4 shifts 
Figure 3: A systematic encoder for the Hamming code 
Example. 
(3) Jumping ahead a little, we consider a double-error-cor-
recting code which belongs to the important class of BCH codes studied in 
Chapter 12. The following polynomial 
g(x) = 1 + z
4 + x
6 + x
7 + x
6 
generates a linear cyclic (15,7)-code. We prove later that the code has 
minimum distance 5, and thus corrects double errors. (Compare this code 
with the single-error-correcting Hamming code with 11 information bits, 
and the shortened triple-error-correcting Reed-Muller code 72(1,4) with 
5 information bits.) 
An encoder for this code is shown in Figure 4. 

10.3. 
PARITY CHECK 
POLYNOMIAL 
171 
# \ O p e n : shifts 7 to 14 
code word 
U 8 « 9 · · « 1 2 « 1 3 
Down: shifts 0 to 6 
Figure 4: An encoder for the (15,7)-BCH code correcting double errors 
10.3 
Parity Check Polynomial 
We have defined the generator polynomial of a cyclic code, and now we 
show which polynomials can serve as generator polynomials, and how the 
parity check matrix can be substituted by a polynomial. 
Observe first that if g(x) = yo + g\x + 
\- 
g„-t-ix n-k-l 
n - i 
IS 
a monic generator polynomial [see Remark (3) of 10.1] of a cyclic code K, 
then g{x), xg(x), 
• • • , x
k~
1g(x) 
are code words of K, in fact, cyclic shifts 
of the word g(x). However, x
kg(x) 
is not a code word (since its degree is 
not smaller than n). In order to "produce" a code word we have to shift 
the highest-degree summand of x
kg(x), 
which is x
n, to the lowest-degree 
position; in other words, we have to subtract x
n and add 1. Thus, 
x
kg(x) 
- x
n + 1 = 100.. .OOyofli · · - S n - t - i 
(10.3.1) 
is a code word of Κ. 
Theorem. Every generator polynomial of a cyclic code of length η di-
vides χ" — 1. Conversely, every proper divisor of x
n — 1 generates a cyclic 
code of length n. 
PROOF. I. Let g(x) be a monic generator polynomial of a cyclic code [see 
Remark (3) of 10.1]. The degree of g(x) is η — ib (with it > 0, since a cyclic 
code is nontrivial), and when dividing ζ" — 1 by g(x), we get 
x" — 1 = q(x)g(x) -+- r(x) 
and 
degree r(x) < η — Jb. 
In order to prove that r(x) = 0, it is sufficient to verify that r(x) is a code 
word: by Theorem 10.1, r(x) is a multiple of g(x), but since it has a degree 

172 
CHAPTER 
10. CYCLIC 
CODES 
smaller than that of g(x), we conclude that r(x) = 0. Put q(x) = go+9i* + 
Nt**, then q(x)g(x) 
= (io + 9i* + · · +qk-ix
k~
l)g(x) 
+ qkX
ig{x) 
and 
the first summand is a code word 
u>(x) = (q0 + qix + • • • + 4t_j**
- 1)o(a;). 
We will further use the code word w'(x) = x
kg(x) 
— x" + 1 of (10.3.1). 
From the equality 
x
kg(x) 
- w'(x) - ι" - 1 = w(x) + x
kg(x) 
+ r(x), 
we get 
r(x) = —w'(x) — w(x). 
Since —u>(x) and —ti/(x) are code words, it follows that r(x) is a code 
word—hence, r(z) = 0. 
II. Let g(x) be a divisor of χ" — 1 of degree β < n. The code Κ 
of all multiples q(x)g(x), 
where q(x) is any polynomial of degree smaller 
then η — β, is easily seen to be linear. To prove that Κ is cyclic, consider 
a code word w(x) = 
q(x)g(x): 
(a) If the degree of q(x) is smaller than η — s — 1, then the cyclic shift 
of w(x) to the right is xw(x) = [xq(x)]g(x), 
which is a code word. 
(b) If the degree of q(x) is η — s — 1, then the cyclic shift w(x) of w(x) 
to the right is obtained from xq(x)g(x) 
by shifting the highest coeffi-
cient anx
n 
to the lowest position: 
w(x) - xq(x)g(x) 
- anx
n 
+ o„ = xq(x)g(x) 
- an(x
n 
- l ) . 
Since g(x) divides χ" — 1, we can write x" — 1 = h(x)g(x) 
and we 
obtain 
w(x) = [xq(x) - 
anx
nh(x)]g(x), 
which is a code word. 
Thus, A" is a cyclic code with basis g(x), xg(x), 
··· , x
n~'~
ig(x). 
It 
follows that the dimension of Κ is k = η — β. Therefore, the degree of g(x) 
is η — k. Hence, p(x) is a generator polynomial of Κ. 
Ο 
Definition. For each cyclic code of length η with generator 
polynomi-
al g(x), we call the polynomial 
x
n 
— 1 
a parity check polynomial of the code. 

10.3. 
PARITY CHECK 
POLYNOMIAL 
173 
Example. 
(1) We are going to list all binary cyclic codes of length 7. 
Since the generator polynomials are precisely the divisors of χ
7 — 1 (except 
of χ
7 — 1 itself), we first decompose that polynomial: 
x
7 - 1 = (χ + l)(x
3 + χ + l)(x
3 + x
2 + 1). 
It is not difficult to verify that no further decomposition of the polynomials 
χ
3 + χ + 1 and x
3 + x
2 + 1 in the field Z 2 is possible. Thus, the gen-
erator polynomials are precisely all the combinations of the polynomials 
decomposing x
7 — 1—their number is 2
3 — 1 = 7. See Figure 5. 
Generator 
Parity Check 
Name of the Code 
Polynomial 
Polynomial 
x+ 1 
(χ
3 + χ + 1 ) ( χ
3 + χ
2 + 1) 
Even-parity code 
[Ex. (1) in 10.1] 
χ
3 + χ + 1 
( x + l ) ( x
3 + x
2 + l ) 
Hamming code 
[Ex. (3) in 10.1] 
x
3 + x
2 + 1 
( x + l ) ( x
3 + x - r l ) 
Hamming code 
[Exercise 10C] 
(x + l)(x
3 + χ + 1) 
x
3 + x
2 + 1 
Simplex code 
[Exercise 10C] 
(x + l)(x
3 + x
2 + l) 
χ
3 + χ + 1 
Simplex code 
(x
a + x + 1) 
x + l 
Repetition code 
x ( x
3 + x
2 + 1) 
[Ex. (4) of 10.1] 
1 
x
7 - l 
All of Z\ 
Figure 5: All binary cyclic codes of length 7 
Remark. 
(1) The parity check polynomial is a monic polynomial (pro-
vided that, as agreed, we restrict ourselves to monic generator polynomials) 
of degree ib. 
Proposition. A cyclic code with the parity check polynomial h(x) = Λ0 + 
Λιχ + 
1- ftt_ix*
-1 + x* has the following parity check matrix: 
0 
0 
- 0 
0 
0 
1 
A t_i 
··· 
Λ2 
hi 
h0 
0 
0 
··· 
0 
0 
1 
hk-i 
··· 
hi 
hi 
h0 
Q 
H = I 0 
0 
· 0 
1 
hk-i 
••• 
hi 
hx 
h0 
0 
0 
1 Λ*-ι 
··· 
A] 
fti 
Ao 
0 
0 
··· 
0 
0 
0 

174 
CHAPTER 
10. 
CYCLIC 
CODES 
PROOF. The above matrix Η has, obviously, η — Jb linearly independent 
rows. Thus, its rank is η - k, and the linear space L of all vectors w 
satisfying Hw
t r = 0
T R has dimension η - (η - k) = Jb, see Theorem 7.7. It 
is our aim to prove that the code Κ with the parity check polynomial h(x) 
is equal to L. Since the two linear spaces have the same dimension, it is 
sufficient to prove that Κ is a subspace of L [by Corollary (4) of 7.7]. 
We first observe that the generator polynomial g(x) lies in the space L, 
i.e., that Hg
t r = 0
T R. In fact, the inner product of the first row of Η 
with g is ΧΧΓο' ΛΙ'0Π-Ι-<· This is the coefficient of the polynomial 
h(x)g(x) 
at a;"
- 1, which is zero because h(x)g(x) 
— x
n — 1. The inner product of 
the second row with g is Y^ll 
Λ<</«—a—«, i.e., the coefficient of 
h(x)g(x) 
at x
n - 2 , which is also zero, etc. Thus, g(x) lies in L. For the same reason, 
the shifts xg(x), x
ig(x), 
... , x
k~
1g(x) 
of the generator polynomial lie in L. 
Therefore, L contains a basis of the linear space K. 
It follows that L 
contains K, hence, Κ = L. 
• 
Examples 
(2) The Hamming cyclic code of length 7 with the generator polynomial 
g(x) — 1 + χ + x
3 has the parity check polynomial h(x) = ( χ
7 — 1) -r 
( χ
3 + χ + 1) = χ
4 + χ
2 + χ + 1. Thus, it has the following parity check 
matrix: 
0 
0 
1 0 
1 1 1 ' 
H = 
0 
1 0 
1
1
1
0 
1 0 
1
1
1
0 
0 
A different parity check matrix was found in Example (3) of 10.1. 
(3) The polynomial 1 + χ + x
4 divides χ
1 5 - 1 in the field Z 2: 
x
1
5 - l = ( 1 + χ + χ
4)(1 + χ + χ
2 + χ
3 + χ
5 + χ
7 + χ
8 + 
*
η ) . 
Thus, there is a cyclic code of length 15 with the parity check polyno-
mial A(x) = 1 + χ + x
a + x
3 + X
8 + x
7 + x
8 + x
1 1 . The code has the 
following parity check matrix: 
Η = 
0
0
0
1
0
0
1
1
0
1
0
1
1
1
1 
0
0
1
0
0
1
1
0
1
0
1
1
1
1
0 
0
1
0
0
1
1
0
1
0
1
1
1
1
0
0 
1
0
0
1
1
0
1
0
1
1
1
1
0
0
0 
Since the 2
4 — 1 columns of Η are nonzero and pairwise distinct, this 
is a Hamming (15, ll)-code. 

10.4. 
DECODING 
CYCLIC 
CODES 
175 
Remarks 
(2) In the preceding examples, we have seen two Hamming codes as cyclic 
codes. This is no coincidence, as is explained in 12.1. 
(3) The dual of a cyclic code AT is a cyclic code. The generator polynomial 
of K
x 
is the parity check polynomial of A" "read backwards" (i.e., from 
the highest-degree position to the lowest one). More precisely, let Λ ( χ ) 
be the parity check polynomial of K, h(x) = ft0 + h\x + 
V Λ * χ * . 
Then the dual code K
x 
has the generator polynomial g*(x) — Λ* + 
hk-ix + 
h 
h0x
k. 
In fact, the polynomial Λ(χ) generates a cyclic (η, η — fc)-code Κ (since 
it divides x
n — 1, and has degree ib). It is then obvious that the above 
polynomial g* (x) generates the cyclic code K* of all words in Κ read 
backwards. This code K* is also an (n, n —ib)-code, and from the above 
proposition, we see that any code word of K* lies in K
x. 
Since K* 
and K
1 
have the same dimension n—k, 
we conclude that they are 
equal (see 7.7). 
Example. 
(4) In order to obtain a (15,4)-simplex code as a cyclic code, 
we use the polynomial h(x) of Example (3) above, and read it backwards: 
the following polynomial 
g*(x) = 1 + x
3 + x
4 + x
6 + x
9 + i
1 0 + x
1 1 
generates a simplex code. 
10.4 
Decoding Cyclic Codes 
We have seen in 8.3 that an important piece of information about the error 
pattern, obtainable directly from the received word, is the syndrome. In the 
case of cyclic codes it is often more convenient to work with the syndrome 
polynomial: 
Definition. Let Κ be a cyclic code of length η with a generator polyno-
mial g(x). 
By the syndrome polynomial «(x) of a (received) word w of 
length η is meant the remainder of the division of the corresponding poly-
nomial w(x) by g(x). 
Remarks 
(1) Analogously to the situation with syndrome words (8.3), the received 
word has the same syndrome polynomial as the error-pattern word. In 
more detail: we send a code word q(x)g(x) 
and receive 
ui(x) = q(x)g(x) + e(x) 

176 
CHAPTER 
10. CYCLIC 
CODES 
[where e(x) is the error pattern]. Then the remainders of the divisions 
of u>(x) and e(x), respectively, by the polynomial g(x) are the same, 
since w(x) — e(x) is divisible by g(x). 
(2) The syndrome decoding that we saw in 8.3 can be realized by means 
of polynomials: the syndromes are precisely all polynomials of degrees 
smaller than n — k, and to each syndrome s(x), we choose an error pat-
tern e(x) of the smallest Hamming weight which has that syndrome. 
Fortunately, the algebraic structure of cyclic codes can be used to ob-
tain a much more efficient decoding method. We first illustrate the 
syndrome decoding on a simple example. 
Example. 
(1) Let Κ be the Hamming code of length 7 with the generator 
polynomial g(x) — 1 + χ + χ
3. 
The syndrome of w = 1011001 is the 
remainder of (x
6 + x
3 + x
2 + 1) 
(χ
3 + χ + 1), which is s(x) = χ + 1. We 
know that Hamming codes correct single errors, thus, it is worthwhile to 
find the syndromes of all the corresponding error patterns e(x) = x*, see 
Figure 6. Since χ + 1 is the syndrome of e(x) = x
3 , we conclude that the 
third bit is corrupted, and we decode 
w(x) - e(x) = 1011001 - 0001000 = 1010001. 
Error-Pattern Polynomial 
Syndrome Polynomial 
0 
0 
1 
1 
X 
X 
X
2 
X
2 
X
3 
x + 1 
X
4 
X
2 + x 
X
5 
x
2 + x + 1 
X
6 
x
2 
+ 1 
Figure β: Syndrome decoding for the cyclic Hamming code of length 7 
The computation of the syndrome can be realized by a circuit analogous 
to the encoding circuit of 10.2, see Figure 7. 
Meggitt decoder. The most complicated part of syndrome decoding is 
listing of error patterns corresponding to the syndromes, and searching 
this list every time a word is received. A substantial simplification can be 
achieved by using the cyclic character of the code: We can concentrate on 

10.4. 
DECODING CYCLIC 
CODES 
1 7 7 
WQW\ 
. . . 
Figure 7: Syndrome computing circuit for the cyclic Hamming code 
the last (i.e., highest-degree) position of each received word w, which we 
correct or not, according to its syndrome. Then we make a cyclic shift of w, 
and again study the last position, etc. After η cyclic shifts, all positions 
will have been corrected. 
The advantage of this decoding method, discovered by Meggitt in 1960, 
is that we only list syndromes of those correctable error patterns which 
have degree exactly η — 1. (For example, of the seven syndromes of the 
Hamming code in Figure 6 only one has the degree 6.) The computation 
of syndromes need not be repeated for each shift: we just compute the 
syndrome s of the received word w, and by the cyclic shift of s within the 
divide-by-<y(x) circuit, we obtain the syndrome of the cyclic shift of w. This 
follows from the next proposition. 
Proposition. If a wordvr has a syndrome polynomial s(x), then the cyclic 
shift of w has the syndrome polynomial s^(x), 
which is the remainder of 
the division of xs(x) by the generator polynomial g(x). 
PROOF. The syndrome polynomial s(x) fulfils 
U J ( X ) = e(x) + 
q(x)g(x), 
where q(x) is the quotient of the division of w(x) by g(x). 
The cyclic 
shift of w(x) is obtained from the (noncyclic) shift xw(x) by shifting the 
highest-degree summand u> n-ix
n to the first position, i.e., the cyclic shift 
is 
u/
x)(x) 
= 
xtu(x) — wn-ix
n 
+ 
wn-i 
= 
xw(x) - u)„_i(x
n - 1) 
= 
xw{x) - 
w„-xg(x)h(x) 
= 
xe(x) + xq{x)g(x) 
- κ>„_ι(/(χ)Λ(χ) 
= 
xs(x) + g(x)[xq(x) 
- 
wn-ih(x)]. 
It follows that the remainders of the division of w^(x) 
and xs(x) by the 
polynomial g(x) are the same. 
• 

178 
CHAPTER 
10. CYCLIC 
CODES 
Remark. 
(3) The polynomial 8^ in the proposition above is obtained 
from s(x) [which is the contents of the shift register in the divide-by-p(x) 
circuit] by shifting the register once with no input. 
Thus, we can summarize the algorithm of the Meggitt decoder (for bi-
nary codes, just to simplify the presentation) as follows: 
Meggitt decoding algorithm for binary cyclic (n, k))-codes 
Step 1. Make a list of all syndromes whose chosen coset leader has 1 in the 
rightmost position (i.e., the coset leader is an error-pattern polynomial 
of degree η — k — 1). 
Step 2. When receiving a word u/(x), feed it into the divide-by-o(x) circuit 
(in η clock times). 
Step 3. If the resulting syndrome is on the list above, correct the rightmost 
bit of the received word. 
Step 4. Shift cyclically both the received word and the contents of the 
divide-by-o(x) circuit, and repeat step 3. After η — 1 such cyclic shifts, 
all bits of the received word are corrected; a final cyclic shift of the 
received word concludes the decoding. 
Example. 
(2) The Meggitt decoder of the Hamming code of Example (1) 
proceeds as follows. 
Step 1. The only syndrome on our list is x
2 + 1. 
Step 2. When receiving, say, w = 1011001, feed it into the circuit in 
Figure 7: in 7 clock times, one obtains the syndrome 110: 
1011001 
Η+ΗΪ 
0 
Step 3. Do not correct u>e, since the syndrome 110 is not on our list. 
Step 4. Shift the received word to wetvoWi .. .w& = 1101100 and the 
contents of the above shift register with no input: 

10.4. 
DECODING CYCLIC 
CODES 
179 
The syndrome is, again, not on our list. Thus, w& will not be corrected, 
etc. The contents of the shift register during steps 7 to 13 are listed in 
Figure 8. It follows that the only bit to correct is W3 because the tenth step 
of decoding is the only one for which the shift-register contents is 100 (i.e., 
1 + x
2), which is on our list. Finally, make the last cyclic shift, and obtain 
the decoded word 1010001 [as in Example (1)]. 
Step of Decoding 
Bit Considered 
Syndrome 
7 
U>6 
110 
8 
«>5 
011 
9 
U>4 
111 
10 
U> 3 
101 
11 
U> 2 
100 
12 
U>! 
010 
13 
W0 
001 
Figure 8: Decoding the word 1011001 
Example. 
(3) Consider the double-error-correcting (15,7)-code of Ex-
ample (3) in 10.2. We can correct double errors either by the syndrome-
decoding technique [which requires listing syndromes for all error patterns 
of weight 0, 1 , 2 , the number of which is 1 + 15 + (") = 121] or by the 
Meggitt decoder. Observe that there are just 15 syndromes to be listed 
for the latter, viz., the syndromes of e(x) = x
1 4 and e(x) — x* + x
1 4 for 
i = 0, 1, . . . , 13. The table of syndromes is shown in Figure 9. Thus, 
receiving a word w of length 15, we compute its syndrome, and if it can be 
found among the syndromes of Figure 9, we correct the last bit, w^. Then 
we shift cyclically the received word and correct bits w\3, wu, ... , wo-
Remark. Observe that in the last example, the encoding is not complete: 
the Meggitt decoder will correct all single errors and all double errors, which 
(together with the no-error pattern) means 121 error patterns. However, 
the code Κ has 2
7 code words, and thus there are 2
1 5 / 2
7 = 256 classes 
modulo K, see Remark 6.2. This is a typical situation: for codes correcting 
t errors, we want to find a simple decoder correcting all error patterns of 
Hamming weight at most t, ignoring the possibility of other corrections. 
Well, it would actually be best to have no other correctable errors, but, 
unfortunately, this is a very rare property of codes, as will be seen in 10.6. 

180 
CHAPTER 
10. CYCLIC 
CODES 
Error-Pattern 
Syndrome 
Polynomial 
Polynomial 
X
1
4 
x
7 + x
6 + X
5 
+ X
3 
1 + x
1 4 
x
7 + x
6 + X
5 
+ X
3 
+ 1 
x + x
1 4 
x
7 + x
6 + X
5 
+ X
3 
+ X 
x
2 + x
1 4 
x
7 + x
e + x
5 
+ X
3 + X
2 
x
3 + x
1 4 
x
7 + x
6 + X
6 
x
4 + x
1 4 
x
7 + x
e + X
5 + X
4 + X
3 
x
5 + x
1 4 
x
7 + x
e 
+ X
3 
x
6 + x
1
4 
X
7 
+ X
5 
+ X
3 
x
7 + *
1 4 
X
6 + X
B 
+*
3 
x
8 + x
1 4 
X
5 + X
4 + X
3 
+ 1 
x
9 + x
1 4 
X
7 
+ X
4 + X
3 
+ x + 1 
x
1 0 + x
1 4 
X
3 + X
2 + X 
x
u + x
1
4 
x
7 + x
6 + X
5 + X
4 
+ X
2 + X 
x
1 2 + x
1 4 
x
7 + x
6 
+ X
4 
+ X 
x
1 3 + x
1 4 
X
7 
+ X
4 + X
3 + X
2 
Figure 9: Syndrome list for the Meggitt decoder of a double-error-
correcting code with g(x) = 1 + χ
4 + χ
6 -I- x
7 + x
8 
In the following section, we present a better decoder of the BCH code 
under study. 
10.5 
Error-Trapping Decoding 
A powerful decoding method, particularly useful for single-error- and dou-
ble-error-correcting codes (and, as we will see later, for burst-error cor-
rections) is based on the possibility of "trapping" all errors within the 
syndrome circuit. 
Let A* be a cyclic (n, &)-code capable of correcting t errors. The error-
trapping decoding we are going to describe corrects ί errors whenever they 
are confined to η - λ consecutive positions (including end-around). We 
first make the last concept precise, and then we prove the basic proposition 
which shows how error trapping works. 
Definition. 
(I) By a (cyclic) burst-error pattern of length I is meant a 
nonzero word e0e\... 
e„-i such that all nonzero components are confined 

20.5. ERROR-TRAPPING 
DECODING 
181 
to I consecutive positions in the cyclic sense, i.e., they are among e^i, 6,-4.2, 
• · · 1 ei+i for some i — 0, 1, . . . , η—1, with modulo η addition in the indices. 
(2) / / a word ν is sent and a word w is received, we say that errors are 
confined to / consecutive positions provided that the error pattern e = w — ν 
is a burst-error pattern of length I. 
Proposition. Let w be a word received when a cyclic (n,k)-code is used, 
and assume that at most t symbols have been corrupted. 
(1) / / the syndrome polynomial has Hamming weight at most t, then the 
error-pattern polynomial is equal to the syndrome polynomial. 
(2) Whenever the errors are confined to n — k consecutive positions, then 
some cyclic shift of w has the syndrome polynomial of Hamming 
weight at most t. 
PROOF. 
(1) The syndrome polynomial s(x) is the remainder of the divi-
sion of the error-pattern polynomial e(x) by the generator polynomial g(x). 
Thus, 
e(x) = q(x)g(x) + s(x). 
By hypothesis, e(x) has Hamming weight at most t (since at most t symbols 
are corrupted), and e(x) — s(x) = q(x)g(x) is a code word of K. Whenever 
s has Hamming weight at most t, then the code word e(x) — s(x) has Ham-
ming weight at most 2t. However, since Κ corrects t errors, the minimum 
distance of Κ is 2t + 1 or more. Thus, no nonzero code word has Hamming 
weight smaller than It + 1 —consequently, e(i) — s(x) = 0. 
(2) It is obvious that whenever the word w has errors confined to η — ib 
consecutive positions, then some cyclic shift w' has errors confined to the 
first n—k consecutive positions (i.e., to the positions w'0,... , w'n_k_l). 
This 
implies that the error pattern e shifted cyclically in the same manner yields 
a word e' with ej = 0 for all i > η — ib. In other words, the corresponding 
polynomial e'(x), which is the error-pattern polynomial of the word w', has 
degree at most η — ib — 1. The syndrome of w' is the remainder s'(x) of 
the division of e'(x) by the generator polynomial g(x). Since the degree 
of e'(x) is smaller than that of g(x), it follows that the remainder is e'(x); 
i.e., s'(x) = e'(x). Thus, s' has Hamming weight at most i. 
D 
Error-trapping decoding algorithm. Let Κ be a cyclic code correcting 
t errors. When receiving a word w, find its syndrome s, and if the Hamming 
weight of s is smaller or equal tot, decode as w—8. If s has Hamming weight 
larger than t, shift cyclically w (to the right) and find the syndrome s^
1) 
of the shifted word w^
1). If the Hamming weight of e^
1^ is smaller than t, 

182 
CHAPTER 10. CYCLIC 
CODES 
then decode the word as w^
1^ - s ^ shifted cyclically to the left. If s(
l) has 
Hamming weight larger than t, shift cyclically the word w^) to w^
2\ etc. 
This algorithm corrects t errors whenever they are confined to η — Jb 
consecutive positions. In fact, by (2) in the above proposition, one of the 
shifted words w^) has syndrome s ^ of Hamming weight at most t. Then 
by (1) of that proposition, β^') is the ith shift of the error pattern. 
Examples 
(1) Error trapping is an optimal decoding method for Hamming codes. The 
received word w is either a code word or we shift it until a syndrome of 
weight 1 is found. The unique 1 in that syndrome then indicates which 
bit should be corrected. 
For example, let us have another look at Example (2) of 10.4. In 
Figure 8, we see that at the eleventh step of decoding (when the received 
word w = 1011001 has been shifted to w3w4 ... wjWoW\w-i = 1001101) 
the syndrome has Hamming weight 1. Thus, the syndrome 100 is the 
error pattern, therefore, the bit u»s must be corrected. 
(2) Error trapping is also very suitable for the double-error-correcting BCH 
code of Example (3) in 10.2. Observe that whenever two symbols are 
corrupted (say, u>j and Wj, j > i), then errors are confined to η — Jb = 8 
consecutive positions. In fact: 
(a) if i + 7 > j , then errors are confined to the eight positions i, i + 1, 
... , i + 7, and 
(b) if i + 7 < j , then errors are confined to the eight positions j , 
j+l, 
. . . , 1 4 , 0 , 1 
i. 
Thus, we compute the syndromes of all 15 cyclic shifts of the received 
word, and whenever we find a syndrome of Hamming weight 1 or 2, we 
know which bit, or pair of bits, to correct. 
10.6 
Golay Code: A Perfect Code for Triple 
Errors 
In this section, we introduce one of the most fascinating error-correcting 
codes: a triple-error-correcting binary (23,12)-code, discovered by Golay 
in 1949. This is a code important both for practical applications and in 
theory, since it has a rich algebraic and combinatorical structure. Besides, 
it is the only perfect code we have not yet introduced. We first turn to the 
concept of "perfectness". 

10.6. 
GOLAYCODE 
183 
What is the perfect solution to the problem of finding a code of length η 
which corrects ί errors? The best code would correct t errors and nothing 
else, since then it would have the least redundancy necessary for the task 
required. In other words, in syndrome decoding (6.3), it would have as 
coset leaders precisely all words of Hamming weights at most t. This is 
equivalent to the following property: every word of length η has distance 
at most t from precisely one code word. 
Definition. A linear code of length η is called perfect for t errors provided 
that for every word w of length n, there exists precisely one code word of 
Hamming distance t or less from w. 
Remarks 
(1) A perfect code for t errors has minimum distance 2i + l; thus, it corrects 
t errors. 
(2) If a binary (n, ib)-code is perfect for t errors, then 
In fact, the left-hand side is the number of cosets modulo Κ (see He-
mark 6.2), and the right-hand side is the number of coset leaders, i.e., 
words of Hamming weight i = 0, 1, . . . , i. [The number of words of 
weight t is just the number of combinations of order t among the (bit) 
numbers 1, 2, . . . , n, which is (").] 
(3) Conversely, every t-error-correcting binary (n, ib)-code such that 2"""* = 
ΣΖί=ο (?) '
8 perfect. In fact, since the code corrects t errors, we can 
choose all words of Hamming weight at most t as coset leaders. And 
these will be all the coset leaders beause their number is equal to the 
number of cosets, which is 2
n~*. 
Examples 
(1) The binary repetition code of length 2t + 1 is perfect for t errors. In 
fact, every word has either a majority of O's (and then its Hamming 
distance from 00.. .0 is at most t) or a majority of l's (and then its 
Hamming distance from 11... 1 is at most t). 
(2) Hamming codes are perfect for single errors, see Remark (2) of 5.5. 
(3) The double-error-correcting BCH code of Example (3) in 10.2 is not 
perfect: it corrects, in addition to all double errors, also a lot (but 

184 
CHAPTER 10. CYCLIC 
CODES 
by no means all) of error patterns of three errors. This follows from 
the fact that this (15,7)-code has 2
1
5
-
7 = 256 coset leaders of which 
only ]C?=o (V)
 
= 
I***
 n
a
v
e Hamming weight at most 2. (In contrast, 
a triple-error-correcting code must have at least $2,_ 0 (*?) = 451 coset 
leaders.) 
Definition. The binary cyclic code of length 23 with the generator polyno-
mial g(x) = 1 + x
2 + χ
 4 + x
5 + x
6 + x
1 0 + x
1 1 is called Golay code. 
Theorem. The Golay code has minimum distance 7. 
The proof, which is very technical, is omitted. The interested reader 
can find it in Blahut (1983). 
Corollary. The Golay code is a (23,\2)-code which is perfect for triple 
errors. 
In fact, since the generator polynomial has degree 11, the code has 
Jb = 23 — 11 = 12 information bits. It is easy to verify that 
Thus, the Golay code is perfect for triple errors by Remark (3) above. 
Decoder for the Golay code. The following decoder is a variation on 
error-trapping decoding (10.5). Suppose we send a code word and at most 
three bits are corrupted. If all the errors are confined to 11 consecutive 
positions, then we can trap them: some cyclic shift of the received word 
has syndrome of Hamming weight at most 3, and the syndrome is then 
equal to the error pattern (which we correct). Next suppose that errors are 
not confined to 11 consecutive positions. Then if we happen to correct one 
"suitable" bit, we can achieve that the remaining errors (at most two) are 
confined to 11 consecutive positions. More precisely, if the corrupted bits 
are ω,·, Wj, and u>*, then one of the pairs, (wi,Wj), (u>,-,u>t), or (u)j,u>i), is 
confined to 11 consecutive positions. This suggests the following algorithm: 
Step 1: Compute the syndrome of the received word w. 
Step 2: Find the syndrome s ^ of the cyclic shifts w* of the received word 
(to the right) for i = 0, 1, ... , 22. If the Hamming weight of some s ^ 
is at most 3, the decoded word is the ith cyclic shift of w*') — s ^ to 
the left. 

10.7. 
BURST 
ERRORS 
185 
Step 3: If all the syndromes in step 2 have Hamming weight larger than 3, 
change one bit of the received word, say, the bit wo- Repeat step 2, 
searching for a syndrome of Hamming weight at most 2. If such a 
syndrome is found, the decoding is finished: the syndrome plus the 
bit wo form the error pattern to be corrected. If no syndrome of weight 
at most 2 is found, then wQ is correct. Reset it to its original value, 
and repeat step 3 with other bits changed. 
Remarks 
(4) One of the deepest results of coding theory is the following theorem 
proved by Tietavainen (1974) and Van Lint (1975): Every perfect bi-
nary code is either a repetition code (of odd length), or a Hamming 
code (of length 2
m — 1), or a code equivalent to the Golay code (of 
length 23). 
(5) The situation in the three-element alphabet Z3 is analogous. Every 
perfect ternary code is either a repetition code, or a Hamming code, or 
a code equivalent to the ternary Golay code, which is a perfect triple-
error-correcting (ll,6)-code. 
(6) In alphabets of more than three elements, the situation is even simpler: 
the only perfect codes are the repetition codes and the Hamming codes. 
10.7 
Burst Errors 
We usually assume that the corrupted symbols in a transmitted code word 
are distributed randomly (white noise). In some applications, however, 
errors are typically grouped in bursts. This is the case, for example, of 
telephone lines disturbed by a stroke of lightening, and of mechanical dis-
turbances of floppy discs, etc. Codes designed for protection against white 
noise are less effective in combatting burst errors than codes specifically 
designed for burst-error detection or burst-error correction. The most ef-
ficient burst-error control codes have been found among cyclic codes, and 
we thus restrict our attention to those. For this reason, we also extend the 
concept of burst error to include the end-around case (as defined in 10.5). 
Remarks 
(1) A linear code is said to detect burst errors of length I provided that no 
burst-error pattern (10.5) of length / is a code word. This means, then, 
that if we send a code word ν and receive a word w = ν + e, where the 
error pattern e is a burst of length /, then w is not a code word. (If 
it were, e = w — ν would also be a code word!) Thus, we detect the 
error. 

186 
CHAPTER 
10. CYCLIC 
CODES 
(2) If a linear (n, Jb)-code detects burst errors of length /, then it must have 
at least / parity check symbols: 
n-k>l. 
In fact, in the given r-symbol alphabet, there are precisely r' words, all 
with nonzero components confined to the first / positions. No two of 
these words lie in the same coset modulo the code (6.2) because their 
difference is a burst of length / (which is not a code word). Thus, the 
number r
n~
k 
of classes modulo the code (see Remark 6.2) is larger or 
equal to r'. Hence, η — k > I. 
The following proposition thus shows that all cyclic codes are very 
effective in detecting burst errors: 
Proposition. Every cyclic (n, k)-code delects burst errors of length n — k. 
PROOF. Every burst e(z) of length η - k can be obtained by a cyclic shift 
(to the right) of a burst which has all nonzero components confined to the 
first n — k positions, i.e., 
e(x) = z'a(z), 
degree a(z) < η — k. 
Since a(x) is a nonzero polynomial of degree smaller than that of the gen-
erator polynomial, it follows that a(z) is not a code word. Then neither 
is e(z), because a(z) can be obtained by a cyclic shift from e(z). 
• 
Example. 
(1) The cyclic Hamming (7,4)-code detects burst errors of 
length 3. Recall that it detects two (random) errors. 
Remarks 
(3) A linear code is said to correct burst errors of length I provided that 
two different bursts of length / always lie in different cosets modulo 
the code (6.2). We can then correct analogously as in 6.3 by forming 
an array in which all bursts of length / are chosen as coset leaders. 
(Conversely, if two bursts, ej and e 2, lie in the same coset, then ν = 
ei — β2 is a code word. If we send ν and receive w = ν + e 2 = ej, we 
cannot correct this error: we do not know whether ν or 0 was sent.) 
(4) If a linear (n, &)-code corrects burst errors of length /, then it must have 
at least 2/ parity check symbols: 
η - * > 2/. 
In fact, no burst of length 21 can be a code word. Thus, the number of 
cosets is at least r
2' [compare Remark (2)]. 

10.7. BURST 
ERRORS 
187 
Examples 
(2) The expurgated cyclic Hamming (7,3)-code corrects burst errors of 
length 2. This cyclic code [consisting of the even-parity code words of 
the cyclic Hamming (7,4)-code] has the following generator polynomial: 
g(x) = (χ
3 + χ + l)(x - 1) = x
4 + x
3 + x
2 + 1. 
The minimum distance of this code is 4. Thus, in order to verify that 
no two bursts of length 2 lie in the same coset, it is sufficient to show 
that none of the following words 
1100000 - 0011000 = 1111000, 
1100000-0001100= 1101100, 
1100000 - 0000110 = 1100110, 
1100000-0000011 = 1100011, 
is a code word. (In fact, all the other cases yielding a word of Hamming 
weight 4 can be obtained by cyclic shifts of some of the four cases.) The 
verification that these four words are no code words can be performed 
mechanically. 
(3) A number of good binary burst-error-correcting codes have been found 
by computer search. Some are listed in Figure 10. 
Information 
Corrects 
Length η 
Symbols ib 
Bursts / 
Generator Polynomial 
15 
10 
2 
x
6 + x
4 + x
2 + 1 
15 
9 
3 
χ
6 + χ
5 -I- χ
4 -1- x
3 + 1 
31 
25 
2 
x
6 + x
5 + x
4 + 1 
63 
56 
2 
x
7 + x
6 + x
5 + x
3 + x
2 + 1 
63 
55 
3 
χ
8 + χ
7 + x
e + x
3 + 1 
Figure 10: Some binary burst-error-correcting codes 
Interleaving is a simple but powerful technique for creating new cyclic 
burst-error-correcting codes from a given one. For each (n, /b)-code K, we 
form a (jn,jk)-code 
by taking j arbitrary code words in Κ and merging 
them to a word of length jn by alternating the symbols. If the original 
code Κ corrects burst errors of length /, then the new code will, obviously, 

188 
CHAPTER 
10. CYCLIC 
CODES 
correct burst errors of length jl. For example, the expurgated Hamming 
code yields a (14,6)-code correcting bursts of length 4, a (21,9)-code cor-
recting bursts of length 6, etc. 
The interleaving of a cyclic code Κ produces a cyclic code: if we in-
terleave the code words vi, vj, ... , Vj and then make a cyclic shift to 
the right, we get the same result as when interleaving the words v?, vj, 
• • • ι
 vj-i> 
where the asterisk denotes the cyclic shift to the right. The 
generator polynomial of the new code is g(x')\ 
it is obtained by inter-
leaving g(x) = go9\ • • .p„_t00.. .0 and j - 1 zero words. The result is 
g000.. 
.Ορ,ΟΟ.. . 0 . . .Opa ... o„_ t00.. .0 = g{x>). 
Example. By interleaving the expurgated Hamming code above we obtain 
a cyclic (14,6)-code correcting burst errors of length 4 with the generator 
polynomial 
g(x) = χ
β + χ
6 + χ
4 + 1. 
Decoding. Burst-error-correcting cyclic codes are efficiently decoded by 
the error trapping described in 10.5. 
10.8 
Fire Codes: High-Rate Codes for Burst 
Errors 
Besides the burst-error-correcting codes found by computer search and in-
terleaving, some analytic methods have also been developed for a construc-
tion of good codes for burst errors. Here we introduce the Fire codes, see 
Figure 11, and in 12.4, we will see the class of Reed-Solomon codes. 
Specified: 
By an irreducible polynomial p(x) 
of degree ι 
Length: 
η = LCM(h,2t 
- 1), where ft is 
the period of p(x) 
Information symbols: 
k = η — 3ί + 1 
Generator polynomial: 
g(x) = ( x
a , _ 1 — l)p(x) 
Error-control capability: 
Corrects burst errors of length t 
Figure 11: Properties of Fire codes 
We first present an interesting concept concerning polynomials: 

10.8. FIRE 
CODES 
189 
Definition. A polynomial p(x) over a field is said to have period η pro-
vided thai η is the smallest number such that p(x) divides x
n — 1 [i.e., such 
thai p(x) generates a cyclic code of length n]. 
Example. 
(1) The binary polynomial χ
2 + χ + 1 has period 3; the binary 
polynomial χ
3 + χ + 1 has period 7. The binary polynomial x
a does not 
have a period. 
Remarks 
(1) If p(x) is a polynomial of period n, then p(x) divides χ* — 1 if and only 
if η divides the number Jb. In fact: 
(a) If η divides Jb, then χ" — 1 divides χ* — 1: 
x * - l = ( x
n - ΐ ) ( ΐ + χ" + χ
2 η + ··· + χ - ) . 
Thus, p(x) divides χ* — 1. 
(b) Let p(x) divide χ* — 1. Then ib > n, and we prove that ib is divisible 
by η by induction on Jb. The case ib = η is clear. Suppose Jb > n. 
We know that p(x) divides both 
χ" - 1 = (x - 1)(1 + χ + · · · + χ""
1) 
and 
x* - 1 = (x - 1)(1 + X + ... + χ*"
1). 
Since p(x) is irreducible, it divides both 1 + χ + 
h x
n
_
1 and 
1+xH 
l-x*
- 1; thus, it also divides the difference of the last two 
polynomials, x
n ( l + xH 
h x
n - f c - 1 ) . It follows that p(x) divides 
1 + χ Η 
1- x
n
-
t
_
1 ; finally, p(x) also divides 
x k - n 
_ 
l 
_ 
^ 
_ 
j ^ j 
+ 
χ 
+ 
. . . + 
x n - k - l y 
By the induction hypothesis, η divides ib — n; consequently, η di-
vides ib. 
(2) In particular, since p(x) = χ" — 1 has period n, we conclude that 
χ" — 1 divides x* — 1 ^=> η divides ib. 
Definition. A Fire code is a cyclic code with a generator polynomial 
g(x) = (χ
2*"
1 - l)p(x) 
for some irreducible polynomial p(x) of degree at least t and of period not 
divisible by 2t — 1. The length of the code is the period of g(x). 

190 
CHAPTER 
10. CYCLIC 
CODES 
Remark. 
(3) If p(x) has period ft, then the length of the Fire code is the 
least common multiple of ή and 2t — 1: 
η = LCM(n,2t- 
1). 
In fact, the period of g(x) is a multiple of both η and 2t — 1 [by Remarks 
(1) and (2)]. Thus, it is a multiple of n. Conversely, χ" — 1 is divisible both 
by x
2 '
- 1 — 1 and by p(x). Since 2t — 1 does not divide ή, the polynomials 
x
2 , _ l - 1 and p(x) are relatively prime. Therefore, χ" - 1 is divisible by 
their product, i.e., by g(x). It follows that the period of g(x) is n. 
Examples 
(2) The irreducible binary polynomial p(x) = x
2 + x + l of period ft = 3 does 
not allow the choice t = 2 (since 2t — 1 = 3 divides ή), and for t = 1, 
we get the repetition code of length 3. 
(3) The irreducible binary polynomial p(x) = χ
3 + χ + 1 of period 7 defines 
a Fire code generated by 
(/(x) = ( x
5 - l ) ( x
3 + x + l ) . 
Its length is η = 7 χ 5 = 35. This (35,28)-code corrects burst errors of 
length 3, as we presently prove. 
Theorem. The Fire code generated by g(x) = ( x
2 '
- 1 —l)p(x) corrects burst 
errors of length t. 
PROOF. We are going to prove that whenever two bursts, ej(x) and β2(χ), 
of length t lie in the same coset modulo the Fire code, then βχ(χ) = β2(χ). 
By a certain number r of cyclic shifts, we can turn e\(x) into the position 
in which the first (lowest-degree) component is nonzero and all the nonzero 
components are confined to the first t positions. Then ej(x) = χ
Γα(χ), 
where 
degreea(x)<< 
and a0 φ 0, 
(10.8.1) 
Analogously, e 2(x) = x'6(x) for some polynomial 6(x) with 
degree6(x)<i 
and b0 φ 0. 
(10.8.2) 
It is our task to show that if e\(x) — e 2(x) is a code word, then it must 
be 0. We can restrict ourselves to the case r = 0 because the Fire code is a 
cyclic code (and the general case is obtained by r cyclic shifts to the right 
from the special case). Thus, our task is to prove the following implication: 
o(x) - χ'ό(χ) is a code word ==> a(x) = x*6(x) 
(10.8.3) 

10.8. FIRE 
CODES 
191 
Let us divide s through 2t - 1: 
s = g(2i - 1) + r 
and 
r < 2ί - 1. 
(10.8.4) 
It follows that 
a(x) - **&(*) = a(x) - x
Tb(x) - z
rb(x)(x^
2t-^ 
- l). 
The code word a(x) — x* b(x) is a multiple of the generator polynomial g(x). 
Hence, x
2 '
- 1 — 1 divides the polynomial a(x) — x'b(x). 
It also divides 
a;«(2t-i) _ ι [ b y Remark (2)]; thus, the difference 
a(x) - x
rb(x) 
= [a(x) - x'b(x)] - x
rb(x)(x«
2,-V 
- l) 
is also divisible by x
2t~
x 
— 1. 
We now prove that 
a(x) = x
Tb(x). 
(10.8.5) 
As observed above, we can write 
a(x) - x
rb(x) 
= d{x)(x
2t-
1 
- 1) 
(10.8.6) 
for some polynomial d(x) of degree k. Suppose that Equation (10.8.5) is 
false, i.e., k > 0. We will derive a contradiction. The right-hand side 
of (10.8.6) has degree 2t — 1 + k. This is larger or equal to <, and since 
by (10.8.1), the degree of a(x) is smaller than t, it follows that the degree 
of the left-hand side of (10.8.6) is determined by the summand x
r6(x) alone. 
By (10.8.2), we conclude that r + 1 > 2t - 1 + k; thus, 
r>t 
+ k. 
(10.8.7) 
This implies that the degree of a(x) is smaller than t < r. We conclude 
that the coefficient of the left-hand side of (10.8.6) at x
r is &o. Recall that 
to Φ 0 [see (10.8.2)]; thus, the right-hand side also contains x
r . However, 
from (10.8.7) and (10.8.4), we see that r fulfils k < r < 2t — 1; consequently, 
neither the polynomial d ( x ) x
2 '
- 1 nor — d(x) has nonzero coefficients at x
r — 
a contradiction. This proves (10.8.5). 
Since a0 φ 0, see (10.8.1), Equation (10.8.5) can hold only if r = 0 and 
a(x) = 6(x). It remains to prove that s — 0. The code word of (10.8.3) has 
the form 
α ( χ ) - χ ' ό ( χ ) = - 6 ( χ ) ( χ ' - 1 ) . 
Every code word is divisible by g{x), hence, by p(x). The irreducible poly-
nomial p(x) has degree at least t > degree [—6(x)]; thus, p(x) must di-
vide x' — 1. It follows from Remark (1) that β is a multiple of the period η 
of p(x). Since r = 0 in (10.8.4), β is also a multiple of 2t — 1. Thus, s is 
a multiple of η = LCM(n, 2t — 1). However, β is one of the numbers 0, 1, 
... , η—1. This proves 8 = 0, whereby the proof of (10.8.3) is concluded. 
• 

192 
CHAPTER 
10, 
CYCLIC 
CODES 
Example. 
(4) To construct a binary Fire code correcting burst errors of 
length 4, we consider an irreducible polynomial of degree 4, e.g., p(x) = 
χ
4 + χ -f 1. Its period is 15 (not divisible by 7); thus, we get a Fire code 
with the generator polynomial 
S ( x ) = (χ
7 - l)(x
4 + χ + 1) 
and of length η = 7 χ 15 = 105. This (105,94)-code corrects burst errors 
of length 4. 
Remark. 
(3) Fire codes typically have high information rates. 
Some 
examples of binary Fire codes are listed in Figure 12. 
Length 
Information Symbols 
Corrects Bursts of Length 
35 
27 
3 
105 
94 
4 
297 
265 
5 
693 
676 
6 
1651 
1631 
7 
Figure 12: Parameters of binary Fire codes 
Exercises 
10A 
Verify that the division of a polynomial a(x) by a polynomial b(x) φ 
0 can be performed by the following algorithm: 
(a) If 6(x) has a degree larger than a(x), then the quotient is 0, and the 
remainder is a(x). 
(b) If 6(x) has a degree smaller or equal to that of a(x), divide the leading 
coefficient a„ of o(x) by the leading coefficient bm of 6(x). The result 
ίη-m = an +bm 
is the coefficient of the quotient at the power x
n
-
m . 
(c) Put a(x) := a(x) - q„-mx
n~
mb(x) 
and perform recursively steps (a), 
(b), and (c). [Observe that the new polynomial a(x) has a smaller 
degree than the original one. Thus, the algorithm stops after η repe-
titions of steps (a), (b), and (c).] 

EXERCISES 
193 
1 0 B 
Prove that polynomial division is unique, i.e., given polynomials 
a(x) and b(x) φ 0, then whenever 
a(x) = b(x)q(x) + r(x) = b(x)q'(x) + r'(x) 
and the degrees of r(x), r'(x) are both smaller than that of b(x), then 
q(x) = i'(x) and r(x) = r'(x). [Hint. Since the polynomial r'(x) — r(x) — 
b(x)[q(x) — q'(x)] has degree smaller than that of 6(x), it follows that 
q(x)-
q'(x) = 0; see Remark (1) in 10.1.] 
IOC 
Verify that the binary cyclic code of length 7 with the generator 
polynomial g(x) = 1 + x
2 + x
3 is a Hamming code, and conclude that the 
code with the generator polynomial ( x
7 — 1): g(x) — x
4 + x
3 + x
2 + 1 is 
the dual (simplex) code (8.2). [Hint: use remark (3) of 10.3 and the fact 
that by reading a Hamming code backwards one obtains another Hamming 
code.] 
10D 
Find all binary cyclic codes of length 5. Find all cyclic codes of 
length 5 over Z 3 . 
10E 
Describe the dual code of the binary cyclic code of length 5 with 
the generator polynomial g(x) = x
3 + 2 x
2 + 3x + 1. 
10F 
In systematic coding, encode the information bits 1101 into a code 
word of the cyclic binary code of length 7 with the generator polynomial 
g(x) = 1 + x
2 + x
3 . 
10G 
Describe the Meggitt decoder for the binary code of length 7 with 
the generator polynomial g(x) — (1 + x
2 + x
3)(x + 1). Can you decode by 
error trapping? 
10H 
Prove that every double-error-correcting (n,Jt)-code with η > 2k 
can be corrected by error trapping. What about the case η < 2Jb? 
101 
What is the smallest length of a binary code with the generator 
polynomial g(x) = x
4 + x
3 + x
2 + 1? Design an encoder and decoder for 
the code. How many errors does it correct? Can error-trapping decoding 
be used? 
10J 
Can error-trapping decoding be used for the Golay code? Design 
an encoder and decoder for that code. 

194 
CHAPTER 
10. CYCLIC 
CODES 
10K 
Find a modification of the error-trapping decoding which can decode 
the double-error-correcting binary code of length 31 with the generator 
polynomial g(x) = x
1 0 + x
9 + x
8 + x
e + x
5 + x
3 + 1. 
10L 
Prove that whenever a cyclic code detects an error pattern e, then 
it also detects all error patterns obtained by cyclic shifts of e. 
10M 
Design an encoder for the Hamming (15, ll)-code of Example (3) 
in 10.3. 
ION 
Are the linear binary codes with the following generator matrices 
cyclic? 
1 0 
1
1
1
0 
0 
1 1 0 
1 0 
0 0 
1 1 0 
0 
1 0 
1 
0 
0 
1 0 
1 1 1 
1
1
1
1
0 
0 
0 
0 
1
1
1
1
0 
0 
0 
0 
1
1
1
1
0 
0 
0 
0 
1 1 
1 1 
10O 
If the generator polynomial of a binary code is divisible by χ + 1, 
verify that all code words have even weight. Does the converse hold? 
10P 
What is the necessary and sufficient condition on a generator poly-
nomial of a cyclic code of odd length in order that 111... 11 be a code 
word? 
10Q 
Describe the generator polynomial of the expurgation of a given 
cyclic code. 
1 0 R 
If p(x) generates a cyclic (n,Jb)-code, verify that g(x*) generates a 
cyclic (in, ii)-code. Describe the code for g(x) = x + 1 and g(x) = x
3 + x + l 
with η = 7. 
10S 
Verify that the intersection of two cyclic codes of the same length 
is a cyclic code. What generator polynomial does it have? 
Notes 
Cyclic codes were introduced by Prange (1957), who discovered their rich al-
gebraic structure and prepared ground for the major development of coding 

NOTES 
195 
theory in the early 1960s. The Golay code is one of the first known error-
correcting codes, see Golay (1949). The Meggitt decoder was described 
by Meggitt (1960), and the origin of error trapping is usually connected 
with Prange. The decoder of the Golay code in 10.6 is based on an idea of 
Kasami (1964). 

Chapter 11 
Polynomials and Finite 
Fields 
In this chapter, we introduce further properties of polynomials and fields 
needed for the presentation of the most important class of error-correcting 
codes, the BCH codes. The reader may decide to skip a part of this chapter 
(in particular the later sections which are rather abstract) and to return 
to some topics whenever necessary. However, the material presented in the 
early sections is indespensable for understanding the BCH codes, which are 
defined by means of zeros of polynomials (found in algebraic extensions of 
the code alphabet). 
The crucial idea of the theory of finite fields is that each polynomial over 
a field F has a zero either in F or in some field F* extending F. Conversely, 
given an extension F* of the given field, then every element α of F* is a zero 
of some polynomial with coefficients in F. The smallest-degree polynomial 
with that property is called the minimal polynomial of a. Using these ideas, 
it is possible to present a full description of all finite fields: each finite field F 
can be obtained by extending the field Z p for some prime p. The extension 
is performed via the minimal polynomial of a primitive element of F. 
11.1 
Zeros of Polynomials 
Recall that a polynomial over a field F is a formal expression 
/ ( * ) 
= 
/o + / l * + - - - + 
/ m *
m . 
Although f(x) cannot be identified with a function on the field F, it cer-
tainly does define a function: the function assigning to each element a of 
197 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

198 
CHAPTER 11. POLYNOMIALS AND FINITE 
FIELDS 
the field F the element f(a) = fo + f\a + · 
+ fma
m. 
Here 
a m = α χ a χ • · · χ a 
(m factors); 
we also define, for each nonzero element a, 
Observe that in a finite field, there always exist infinitely many polynomials 
defining the same function. 
An element a of the field F is called a zero of the polynomial / ( x ) 
provided that / ( a ) = 0. 
Proposition. An element a is a zero of a polynomial f(x) if and only if 
the polynomial χ — a is a divisor of /(x). 
PROOF. 
I. Let a be a zero of /(x). Dividing f(x) by the polynomial ι —a, 
we obtain a quotient q(x) and a remainder r, the latter of degree smaller 
than 1 (i.e., a constant r), such that f(x) = q(x)(x — a) + r. Substituting a 
for x, we get 0 = q(a) χ 0 + r = r. Thus, f(x) = q(x)(x — a), which proves 
that χ — α divides f(x). 
II. Let χ - a be a divisor of f(x), i.e., / ( x ) = q(x)(x — a). By substi-
tuting α for x, we get f(a) = q(a) χ 0 = 0. 
Ο 
Corollary. Every polynomial with pairwise distinct zeros, ai, a2, ... , a„, 
is divisible by the polynomial (χ — a,)(x — a2) · · (x — a„). 
In fact, f(x) is divisible by χ - αϊ, i.e., / ( χ ) = /(x](x - αχ). Since 
/(α 2) = 0 = /(α 2)(α2 - αϊ) implies that α 2 is a zero of /(x), we see that 
/ ( x ) is divisible by χ - α 2. Thus, / ( χ ) is divisible by (χ - a\)(x - a 2 ) , 
etc. 
Ο 
Remark. 
(1) A polynomial of degree η has at most η pairwise distinct 
zeros. This follows directly from the above corollary. 
Examples 
(1) The polynomial x
3 + 1 has a zero in 1 in the field Z 2 . Thus, x
3 + 1 can 
be divided by χ + 1: 
This is a complete factorization of x
3 + 1: since χ
2 + χ + 1 has no zero, 
it cannot be factored into a product of polynomials of degree 1. 
x
3 + l = (x + l)(x
2 + x + l ) . 

11.1. 
ZEROS OF POLYNOMIA LS 
199 
(2) The same polynomial x
3 -f 1 over the field Z 3 has a zero in 2; thus, it 
can be divided byx — 2 = x + l. In fact, in that field, we have 
x
3 + 1 = (x + l )
3 . 
We conclude that factorization of a polynomial depends on the field 
under consideration. 
Definition. A polynomial f(x) over a given field is called irreducible pro-
vided that it cannot be factored as a product of two polynomials of degrees 
smaller than that of / ( x ) . 
Remarks 
(2) Every linear polynomial is irreducible. For polynomials / ( x ) of degree 
2 or more, the above proposition tells us that 
irreducible ==> no zeros. 
The reverse implication is false in general. For example, the polynomial 
/ ( x ) = (χ
2 + χ + l )
2 is certainly reducible, but it has no zeros in Z 2 . 
[Observe that the degree of f(x) is 4, and the reverse implication is 
true for polynomials of degree 2 or 3: such a polynomial is irreducible 
if and only if it has no zeros.] 
(3) In the field R of real numbers, x
2 + 1 is irreducible. 
However, no 
polynomial / ( x ) of degree η > 3 is irreducible over R: if η is odd, then 
/ ( x ) has a zero; if η is even, then / ( x ) has either a real zero or two 
complex zeros α ± i'6. In the latter case, by the above corollary / ( x ) 
can be divided by 
[χ - (a + ib)] [x - (a - ib)] = (x - a )
2 + 6
2, 
which is a real polynomial of degree 2. 
(4) In the field of complex numbers, the fundamental theorem of algebra 
tells us that no polynomial of degree at least 2 is irreducible. 
In a sharp contrast to the two last examples, we will later see that every 
finite field has the property that irreducible polynomials of all degrees exist. 
The following result is analogous to the well-known factorization theo-
rem for natural numbers as products of primes: 
Unique Factorization Theorem. Every nonzero polynomial f(x) has a 
factorization 
/(χ) = α / 1 ( χ ) / 2 ( χ ) . . . / , ( χ ) , 
where a is a scalar, and fi(x) are monic irreducible polynomials of positive 
degrees. The factorization is unique up to the order of the factors. 

200 
CHAPTER 
11. POLYNOMIALS AND FINITE 
FIELDS 
PROOF. 
I. Existence. Let / ( x ) be a nonzero polynomial with leading coef-
ficient a. Then / ( χ ) = a/(x), where / ( x ) = a~
lf(x) 
is a monic polynomial. 
We will find a factorization of f(x) into a product of monic irreducible poly-
nomials by induction on the degree η of / ( x ) . 
If η = 0, then / ( x ) = 1 is irreducible. 
Let η > 0, and suppose that a factorization exists for all polynomials 
of smaller degrees. If / ( x ) is irreducible, then the factorization of / ( x ) is 
/(χ) = a/(x). In the case / ( x ) = g(x)h(x), 
where g(x) and Λ(χ) are monic 
polynomials of degrees smaller than n, we use the induction hypothesis 
on both g(x) and ft(x): there are monic irreducible polynomials /i(x), 
··· . /n(x), /n+i(x), ··· , /m(x) with g(x) = / i ( x ) . . . /„(x) and Λ(χ) = 
Λ » + Ι ( Χ ) · · ·ίτη{χ)· Then the required factorization is 
/ ( * ) = β / ! 
( « ) . . . / „ ( « ) /
B
+
1 ( * ) . . . / m ( « ) . 
II. Uniqueness. It is certainly sufficient to restrict ourselves to monic 
polynomials [since for any polynomial /(x), there is a unique monic polyno-
mial / ( x ) and a unique scalar α with /(χ) = a/(x): in fact, a must be the 
leading coefficient of / ( x ) and/(x) = a~
1f(x)]. 
Let / ( x ) be a monic poly-
nomial of degree n. We will prove that f(x) has at most one factorization 
into irreducible factors. 
If η = 0, then f(x) — 1 has no nontrivial factorization. 
Let η > 0, and suppose that / ( x ) has the following two factorizations 
into monic, irreducible factors: 
/ ! ( * ) . . · / . ( * ) = βι(*)·..«»(*). 
( H . 1 . 1 ) 
We will first prove that /i(x) can be found on the right-hand side, i.e., 
fi(
x) 
= 9i{
x) f°r some i. We can assume that /i(x) has degree smaller or 
equal to the degree of gi(x) [since else we just interchange the two sides 
of (11.1.1)]. If/i(x) = Pi(x), put i = 1. In case /ι(χ) φ gi{x), divide gi(x) 
by /i(x) to get a quotient q(x) and a remainder r(x) such that 
0 i ( x ) = g(x)/i(x) + r(x) and 
degree r(x) < degree/i(x) < degreegi(x). 
Since gi(x) is irreducible, r(x) is a nonzero polynomial, and because both 
/i(x) and ffi(x) are monic [thus, the quotient q(x) is monic too], we conclude 
that r(x) = gi(x) — g(x)/i(x) is also monic. We can substitute for 0 i ( x ) 
in (11.1.1) to get 
/ i ( x ) / 2 ( x ) · · · / . ( * ) = 9(x)/i(x)ya(x) · · -0«(x) + r ( x ) 0 2 ( x ) • ·-9t(
x)-
Consequently, 
/i(x)[/a(x) · · ·Λ(χ) - i(x)02(x) • · -0«(x)] = r ( x ) j f 2 ( x ) . . 
.g,(x). 

11.2. 
ALGEBRAIC 
EXTENSIONS 
OF A 
FIELD 
201 
By I, the bracket on the left-hand side has a factorization into monic irre-
ducible factors h\(x).. 
.hm(x). 
Thus, 
/i(«)M«) · · - M*) = r(x)g2(x) 
• ..g,(x). 
( Π . 1 . 2 ) 
We now apply the induction hypothesis to the polynomial whose two fac-
torizations are expressed by (11.1.2). That polynomial has degree smaller 
than η because r(x) has degree smaller than gi(x) [and the polynomial 
f(x) 
= gi(x)g2(x)...gt(x) 
has degree n]. Consequently, the polynomial 
of (11.1.2) has a unique factorization. In particular, fi(x) can be found on 
the right-hand side of (11.1.2). Since fi(x) cannot be equal to r(x) [because 
the latter polynomial has degree smaller then that of fi(x)], we conclude 
that /i(x) = giix) for some i = 2 
i. 
Thus, we can cancel fi(x) from both sides of (11.1.1) and we get two 
factorizations of a polynomial of degree smaller than η as follows: 
fiix) ...f,(x) 
= gi(x) • • • gi-i(x)gi+\(x) 
• • • gt(x)-
By applying the induction hypothesis again, we conclude that the two sides 
of the last equation differ only in the order of their factors. Consequently, 
the same is true about (11.1.1). 
• 
11.2 
Algebraic Extensions of a Field 
The idea of algebraic extension is well known from the case of the field R 
of real numbers: the polynomial x
2 + 1 has no zero in that field, but we 
can extend the field by a new element i, which is a zero of x
2 + 1, and we 
"obtain" the field of complex numbers. (A side remark: do you really know 
what complex numbers are? One approach of introducing this fundamental 
concept is to say that we adjoin the element t = 
to the reals. Well, 
this is too obscure to be a reasonable way to introduce a mathematical 
object! Another approach is to say, quite formally, that complex numbers 
are pairs of reals which instead of (a, 6) are denoted by a + »'&. Then formal 
addition and formal multiplication are introduced. Well, this is too formal 
to be a reasonable way of explaining complex numbers—for example, why, 
then, do we not multiply them coordinatewise? In the present section, 
although our prime aim is different, we will find a precise and suggestive 
way of introducing complex numbers.) 
Example. 
(1) We want to extend the field Z2 by a zero of the polyno-
mial p(x) = χ
2 + χ + 1. This new zero, say, a, will satisfy a
2 + a + 1 = 0, 
i.e., 
a
2 = a + 1. 

202 
CHAPTER 11. POLYNOMIALS AND FINITE 
FIELDS 
Thus, we have to add at least two new elements: a and a + 1. Denote the 
latter by β, i.e., β = a + 1. Then we get four elements, 0, 1, a, and β, and 
this is all we need. In fact: 
(a) Addition is clear because in any extension of Z?, we have ζ + ζ = 0 
for each χ (since 1 + 1 = 0 , and this equation can be multipled by x). 
Thus, 
α + /? = α + α + 1 = 1, 
/3+1 = α + 1 + 1 = α, 
etc. 
(b) Multiplication follows from a
2 = a + 1 = β: we have 
αβ 
= 
α ( « + 1) = β + α = 1, 
β
2 
= 
( α + 1 ) ( α + 1 ) = /3 + α + α + 1 = / ? + 1 = α, 
etc. The tables of addition and multiplication are shown in Figure 1. 
+ 
0 
1 
a 
β 
0 
0 
1 
a 
β 
1 
β 
0 
1 
a 
α 
a 
β 
0 
1 
β 
1 
a 
β 
0 
Figure 1: Extension of Z 2 
X 
0 
1 
a 
β 
0 
0 
0 
0 
0 
1 
0 
1 
α 
β 
a 
0 
a 
β 
1 
β 
0 
β 
1 
α 
a zero of p(z) = ζ
2 + χ + 1 
Still on the informal level, let us consider a polynomial 
P(x) 
= Po + Pix 
+ 
· · + P m *
m 
( P m ^ 
0) 
over a field F. Suppose we want to "extend" the field F by a zero a of the 
polynomial p(x). Thus, in the extended field, we have p(a) = 0. Now, p(a) 
is just the same polynomial, but with a new name for the indeterminate. 
The equation p(a) = 0 implies, of course, that each polynomial q(x), which 
is a multiple of p(z), also fulfils q(a) = 0. Now recall that all polynomials 
form a ring F[x] (10D) and that p(z) is an element of that ring. Thus, 
we can consider the ring F[x]/mod 
p(z) of all coset leaders modulo p(z) 
(see 7.2). 
Observe that if two polynomials q(x) and q'(x) lie in the same coset mod-
ulo p(z) [i.e., if their diference q(x) — q'(x) is a multiple of p(z), see 6.2], 

11.2. 
ALGEBRAIC 
EXTENSIONS 
OF A 
FIELD 
203 
then q(a) = q'(a). In fact, q(a) - q'(a) is a multiple of p(a) = 0. Thus, it 
is not surprising that a formalization of the idea of "creating a zero of p(x)" 
can be realized by the ring F[x]/mod p(x) of coset leaders modulo p(x). In 
Example (1), the coset leaders were 0, 1, x, and χ + 1. In general, there is 
always a canonical choice of the coset leaders: 
Proposition. Let p(x) be a polynomial of degree m > 0 over a field. All 
polynomials of degrees smaller than m are coset leaders modulo p(x). That 
is, every polynomial f(x) over the field lies in the coset of precisely one 
polynomial of degree smaller than m, viz., the remainder of the division 
off(x) 
by p(x). 
PROOF. The above division is expressed by 
/ ( x ) = g(x)p(x) + r(x), 
degreer(x) < m. 
Then / ( x ) and r(x) lie in the same coset modulo p(x) because / ( x ) — r(x) 
is a multiple of p(x), see Proposition 6.2. 
It remains to prove that such a polynomial is unique, i.e., if r(x) and r'(x) 
are two polynomials of the same coset and having degrees smaller than m, 
then r(x) = r'(x). In fact, by Proposition 6.2, we have 
r(x) - r'(x) = i(x)p(x) 
for some polynomial q(x). In the last equation, the degree of the left-hand 
side is smaller than that of p(x). Consequently, q(x) = 0 [see Remark (1) 
of 10.1]. Thus, r(x) = r'(x). 
• 
Definition. By an algebraic extension of a field F is meant the ring 
F[x]/mod p(x), 
where p(x) is a polynomial of degree m > 1. The elements of the extension 
are all polynomials f(a) 
of degree less than m in indeterminate a over F. 
The algebraic operations are defined as follows: the addition is the usual 
addition of polynomials: 
f(a) + g(a) = h(a) <=> /(«) + g(x) = A(x), 
and the multiplication is given by 
f(a)g(a) 
= h(a) ·<=> ή(χ) is the remainder of 
the division of f(x)g(x) 
by p(x). 

204 
CHAPTER 11. POLYNOMIALS AND FINITE 
FIELDS 
Examples 
(1) The algebraic extension of the field Z 2 by the polynomial p(x) = x
2 + 
χ + 1 is the ring 
Z 2[x]/mod x
2 + x + l. 
Its elements are 
0, 1, α, a + 1 ( = ' / ? ) . 
The addition and multiplication are presented in Figure 1. Observe 
that we have (finally!) obtained a four-element field: here l
- 1 = 1, 
α
- 1 = β, and / Η = α. 
(2) The algebraic extension of Z 2 by p(x) = x
2 + 1 also has four elements, 
0, I, a, and o +1 = β. The table of addition is the same as in Figure 1. 
However, multiplication is different: since a
2 + 1 = 0, we see that 
a
2 
= 
1, 
αβ 
= 
a ( a + 1) = a
2 + a = 1 + ο = β, 
β
2 
= 
(a + l )
2 = 1 + 1 = 0, 
etc. 
From the last equation, we conclude, of course, that this extension is 
not a field. 
(3) The algebraic extension of the field R of real numbers by the polyno-
mial p(x) = x
2 +1 consists of all real polynomials of degree at most 1 in 
an indeterminate which (rather then by a) is traditionally denoted by t. 
That is, the elements of R[x]/mod x
2 + 1 are all polynomials α + 6t, 
addition is componentwise as usual, and multiplication is determined 
by i
2 + 1 = 0, i.e., i
2 = - 1 . 
We see that the algebraic extension of R by a zero of x
2 + 1 is just the 
field of complex numbers. This definition of complex numbers is both 
precise and suggestive. 
Remarks 
(2) The algebraic extension F[x]/modp(x) actually does extend the field F: 
all elements of F are contained in the extension (as the polynomials 
of degrees - 1 and 0), and the addition and multiplication of those 
elements are the same in F and in F[x]/modp(x). 
(3) We have seen above that the algebraic extension is sometimes, but not 
always, a field. Whenever the polynomial p(x) under consideration can 
be factorized as p(z) = a(x)6(x), where the degrees of a(x) and 6(x) are 
smaller than that of p(x), then the extension F[z]/modp(x) cannot be 

11.2. 
ALGEBRAIC 
EXTENSIONS 
OF A FIELD 
205 
a field. In fact, the nonzero elements α(α) and b(a) of the extension 
fulfil 
a(a)b(a) = 0, 
which cannot happen in a field. Conversely, in analogy to Theorem 7.2, 
we have the following: 
Theorem. For each irreducible polynomial p(x) over a field F, the algebraic 
extension F[x]/modp(x) is afield. 
PROOF. Let m be the degree of p(x). We will prove by induction on 
k = 0, 1, 
m — 1 that every element a(o) of degree k in the exten-
sion F[x]/modp(x) has an inverse element. 
If Jb = 0, then a(a) lies in F, and it has an inverse there. 
Let k > 0 and suppose that all nonzero elements of smaller degrees 
have inverse elements. Divide p(x) by a(x) to get a quotient q(x) and a 
remainder r(x). The latter has degree smaller than Jb and, since p(x) is 
irreducible, it is nonzero. Thus, the element r(o) has an inverse element 
in F[x]/mod p(x); i.e., there is a polynomial s(x) such that s(a)r(a) = 1. 
In other words, the product s(x)r(x) has remainder 1 when divided by p(x): 
s(x)r(x) = /(x)p(x) -I-1 
for some <(x). 
When multiplying the equation p(x) = q(x)a(x) -f- r(x) by the polyno-
mial s(x), we get 
s(x)p(x) = s(x)q(x)a(x) + <(x)p(x) + 1. 
Thus, 
[- S(x)g(x)]o(x) = 1 + p(x)[t(x) - .(*)]. 
Consequently, the remainder of the right-hand side polynomial divided 
by p(x) is 1; hence, the same is true of the left-hand side: 
[—s(a)q(a)]a(a) 
— 1 
in 
F[x]/mod p(x). 
We conclude that —s(a)q(a) is an inverse element of a(a). 
• 
Concluding Remark. Every polynomial p(x) over a field F defines a ring 
F[x]/mod p(x) 
of cosets of polynomials modulo p(x). Two polynomials, / ( x ) and g(x), 
lie in the same coset precisely when their difference / ( x ) — y(x) is divisible 
by p(x). In notation: 
/ ( x ) = g(x) 
(mod p(x)). 

206 
CHAPTER 11. POLYNOMIALS AND FINITE 
FIELDS 
In particular, every polynomial / ( x ) lies in the same coset as the remainder 
of the division of / ( x ) by p(x). Consequently, all polynomials of degrees 
smaller than that of p(x) can be chosen as coset leaders. 
For irreducible polynomials p(x), the resulting extension of F is a field. 
When extending the field F = Z p in this way, we obtain the important 
Galois fields studied in the next section. 
11.3 
Galois Fields 
Definition. An algebraic extension of the field Z p by an irreducible poly-
nomial of degree m is called a Galois field, and is denoted by 
GF(p
m). 
Examples 
(1) We have constructed the field GF(4) in 11.2, see Figure 1. 
(2) GF(8) is obtained by extending the field Z 2 by an irreducible polyno-
mial of degree 3, for example, by p(x) = χ
3 + χ + 1. Thus, the new 
zero α of p(x) fulfils 
α
3 = a+ 1. 
It follows that 
a
4 
= 
a
2 + a, 
a
5 
= 
o
3 + a
2 = a
2 + a + l , 
a
6 
= 
a
4 + a
3 = a
2 + l , 
a
7 
= 
a
5 + a
4 = l. 
Consequently, all nonzero elements of GF(8) can be expressed as powers 
of a, see Figure 2. Observe that the double expression of each nonzero 
0 
1 
ο 
1 + α α
2 α
2 + 1 α
2 + α α
2 + α + 1 
- 
α° = α
7 
α
1 
α
3 
α
2 
α
6 
α
4 
α
5 
Figure 2: Galois field GF(8) = Z 2/mod χ
3 + χ + 1 
element (as a polynomial in α and as a power of a) yields a complete 
and simple description of the field. In fact, the upper row of Figure 2 
tells us how to add, and the lower row how to multiply. For example: 
( a
2 + l ) ( a
2 + a + 1) = a
4 o
B = a
9 = a
2 a
7 = a
2 . 

11.4. 
PRIMITIVE 
ELEMENTS 
207 
(3) The Galois field GF(9) is obtained by extending the field Z 3 by an 
irreducible polynomial of degree 2, e.g., by p(z) = ζ
2 + ζ + 2. Then 
a
2 + a + 2 = 0 implies 
a
2 
= 
2 a + 1, 
a
3 
= 
2 a
2 + a = 2a + 2, 
a
4 
= 
2 a
2 + aa = 2, 
a
5 
= 
2a, 
a
6 
= 
2 a
2 = a + 2, 
a
7 
= 
a
2 + 2 a = a + l, 
a
8 
= 
a
2 + a = 12. 
A complete description of GF(9) is presented in Figure 3. 
0 
1 
2 
a 2 a a + l a + 2 2a + 1 2 a + 2 
- 
a
0 = a
8 
a
4 
a
1 
a
5 
a
7 
a
6 
a
2 
a
3 
Figure 3: Galois field GF(9) = Z 3[x]/mod ζ
2 + χ + 2 
Remarks 
(1) All Galois fields up to GF(32) are listed in Appendix A. 
(2) The Galois field GF(p
m) 
has p
m elements: this is the number of all 
polynomials ao + a\a + 
h α η - ι «
η _ 1 , where each aj has ρ possible 
values. 
Later we will see that irreducible polynomials of all degrees exist over Z p 
for any prime p. Therefore, a field of p
m elements exists for any prime ρ 
and any m. (Conversely, every finite field has a number of elements 
which is a power of some prime.) 
(3) The presentation of the fields GF(8) and GF(9) above is based on the 
fact that all nonzero elements are powers of a "special" element. In 
the next section, we will see that such an element exists in every finite 
field. 
11.4 
Primitive Elements 
We have seen in the above examples of fields the importance of finding an 
element α whose powers cover all the field except 0. Such elements are 

208 
CHAPTER 11. POLYNOMIALS AND FINITE 
FIELDS 
called primitive, and we will now prove that they can be found in each 
finite field. The characteristic property of a primitive element α is that the 
equation a" = 1 does not hold for "small" n. We thus first pay attention 
to this type of equation: 
Definition. An element a of a field is said to have order η = 1, 2, 3, ... 
provided thai a" = 1 while a
k φ 1 for all k = 1, 2, ... , η - 1. 
Examples 
(1) In the field Z 5 , the element 2 has order 4: 2
a = 4, 2
3 = 3, and 2
4 = 1. 
The element 4 has order 2, and the element 1 has, of course, order 1. 
The order of 0 is undefined. 
(2) In the field of real numbers, 1 has order 1, and —1 has order 2. All 
other orders are undefined. 
Proposition. In a finite field, every nonzero element a has an order η = 
1, 2, 3, ... . Then the elements a, a
2, ... , a
n are pairwise distinct, and 
a
k = 1 if and only if η is a divisor of k. 
PROOF. Let α be a nonzero element. Since the elements a, a
2, a
3, 
... 
cannot be pairwise distinct in a finite field, we can choose the smallest 
η = 1, 2, 3, ... for which a
l+n 
= a' (for some i). We can divide the 
equation by a' (φ 0), and we get a" = 1. It follows that a, a
2, ... , a
n are 
pairwise distinct elements, hence, η is the order of a. 
Whenever η is a divisor of Jb, i.e., Jb = in, then a
k = (α")' = 1' = 1. 
Conversely, suppose that a* = 1. Performing the integer division of Jb by n, 
we get a quotient q and a remainder r < η such that k = qn + r. Then 
a
qn = 1 implies 
1 = α * = α«
ηα
Γ = α
Γ. 
Since r < η, we conclude that r = 0. 
Ο 
Definition. An element a of a field is said to be primitive provided that 
every nonzero element is equal to some power of a. 
Example. 
(3) The number 2 is primitive in Zs because it has order 4, 
thus, all the four nonzero elements of Z5 are powers of 2: 
2 = 2
1, 
3 = 2
3, 
4 = 2
2, 
and 
1 = 2
4. 
Remark. 
(1) Let F be a finite field of r elements. Then an element is 
primitive if and only if it has order r - 1. 

U.4. 
PRIMITIVE 
ELEMENTS 
209 
In fact, if α has order r — 1, then we have nonzero, pairwise distinct ele-
ments a, a
2, ... , o
r - 1—these must be all the nonzero elements. Conversely, 
if a is primitive, then we can express 1 as 1 = a". The smallest such η is 
the order of a. The number of all powers of α is η (because a
n + 1 = a, 
a n + 2 _ 
a 2
( 
e t c ) , thus, η = r — 1. 
Theorem. Every finite field has a primitive element. 
PROOF. Let F be a, field of r elements. We can choose an element a of the 
largest order, say, n. It is clear that η < r— 1 because the η elements, a, a
2, 
... , a", are nonzero and pairwise distinct (see Proposition above) and there 
are just r — 1 nonzero elements. It is our task to prove that η > r— 1; then 
a is primitive by Remark (1) above. We will show that for each nonzero 
element b, the order of 6 is a divisor of n. This will conclude the proof: 
by Proposition above, 6 is then a zero of the polynomial χ" — 1, and, thus, 
x
n — 1 is a polynomial of degree η with r — 1 zeros. By Corollary 11.1, this 
proves η > r — 1. 
Let s be the order of b. Consider the prime factorization of s: 
We will prove that p' is a divisor of n. By symmetry, also qi (and all the 
other prime-power factors of s) are divisors of n, which proves that s is also 
a divisor of η. 
The number η can be factored as 
where n' is a number not divisible by ρ (and t can possibly be zero). It is 
sufficient to find an element c of order 
In fact, since η = p'n' is the largest order, we then have p'n' < p'n', which 
proves i < t, hence, p* is a divisor of η = ρ'η'. 
Put s' = s/p' (i.e., 8 = p*s'); we will prove that the following element 
has order m. Firstly, c
m = 1 is demonstrated by a simple computation: 
* = p V . . . 
(p, q, ... 
primes). 
m — p'n'. 
c ,m 
_ 
ap'mb,'m 
= («")'>) 
= 
( ι ό 0
n') 
= 
1. 

210 
CHAPTER 11. POLYNOMIALS AND FINITE 
FIELDS 
Secondly, whenever c
m ' = 1, we are to prove that m' > m. It is sufficient 
to verify that both p* and n' are divisors of m': since the two divisors are 
relatively prime (recall that the prime ρ does not divide n'), it then follows 
that their product m = p*n' is also a divisor of m'. The trick is to compute 
the powers of c
m (= 1) to n' and to p': 
1 = (c
m')"' = </"'
mV
m' = (a")
mV
m' = 6
,
W . 
By Proposition above, the order s = p V of 6 divides s'm'; thus, p' di-
vides m'. Next: 
1 = (c
m')
P' 
= a P V m ' j p ' . W _ 
ap'p'm'(b^
m' 
= 
aP'p'm' 
We see that the order η — ρ*η' of a divides p*p'm'; thus, n' divides p'm'. 
• 
Remark. 
(2) Given an element a of order n, we can factor x" — 1 as 
follows: 
i" - 1 = (x - a)(x - a
2 ) . . . ( x - 
a
n). 
In fact, each a* is a zero of x
n - 1, and since the zeros a, a
2, ... , a" are 
pairwise distinct (by Proposition above), the right-hand side polynomial 
divides χ" - 1, see Corollary 11.1. The equality now follows from the fact 
that both of the polynomials considered are monic and have degree n. 
Corollary (Fermat's Theorem). In a field Fofr 
elements, every element 
is a zero of x
r — x; thus, 
x
r - x = JJ(x-a) 
(11.4.1) 
and 
x
r~
l-l= 
Yl 
(x-a). 
(11.4.2) 
In fact, let 6 be a primitive element. By Remark (2), 
χ
Γ -
1 - 1 = ( χ - 6 ) ( χ - 6
2 ) · · · ( χ - ό
Γ -
1 ) = 
JJ 
(x-a). 
a£F,a?0 
Multiplying the last equation by x, we obtain (11.4.1); thus, o
r - a = 0. 
Example. In Z 3 , we have 
χ
3 - χ = x(x - l)(x - 2) = Yl (x - a). 
a€Z, 
Remark. 
(3) Several theorems bear the name of Pierre Fermat. (For 
example, the famous theorem on the nonsolvability of x
n + y
n = z
n for η > 
3, which is not yet known to be true.) In this book, only the corollary 
above is called Fermat's Theorem. 

11.5. 
THE CHARACTERISTIC 
OF A FIELD 
211 
11.5 
The Characteristic of a Field 
In a finite field the elements 1, 1 + 1, 1 + 1 + 1, ... cannot be pairwise 
distinct — thus, for some number ρ of summands we have 1+1Η 
1-1 = 0. 
The smallest such ρ is an important feature of the field, since the field then 
is an extension of Z p . It is called the characteristic: 
Definition. The characteristic of a field is the smallest number η of sum-
mands in the equation 1 + 1 + 
r 1 = 0; if no such number exists, the 
field is said to have characteristic oo.* 
Examples 
(1) Zj has characteristic 2. All extensions of Z 2 ) e.g., GF(4) (see Figure 1 
in 11.2), have characteristic 2. 
(2) Z3 and all of its extensions have characteristic 3. 
(3) R has characteristic 00. 
Proposition. Every finite field has a characteristic which is a prime num-
ber. 
PROOF. Since the elements α,· = 1 +1Η 
h 1 ( t summands) cannot be pair-
wise distinct, let ρ be the smallest number such that α,· = a< + p for some t. 
It follows that ap — α, + ρ - α,· = 0. Thus, the field has characteristic p. 
To prove that ρ is a prime, consider a factorization ρ = st: we will prove 
that if β < ρ, then s = 1 and t = p. Obviously, 
0 = a,t — a, + a, + · · · + a, 
(t summands). 
Since s < p, we know that α, φ 0. Dividing the last equation by a,, we get 
0 = 1 + 1 + 
h 1 = a,. Thus, t = p. 
Ο 
Remarks 
(1) Every field F of characteristic ρ is an extension of the field Z p : the 
elements 
0, 
1, 
2 = 1 + 1, 
p—1 = 1 + 1 + 
hi 
(p — 1 summands) 
'Some authon say, instead of 00, that the characteristic is zero. 

212 
CHAPTER 11. POLYNOMIALS AND FINITE 
FIELDS 
of F are pairwise distinct, and they are added as in Z p because (p— 1)+ 
1 = 1 + H 
1-1 = 0. They are also multiplied as in Z p because from 
a product of two elements of the type above we can always substract 
ρ summands 1. Thus, the product of two elements nra of the type 
above is equal to the remainder of the division of the usual product nm 
by p. 
(2) Every binary polynomial can be regarded as a polynomial over the 
field GF(2
m) 
whose coefficients happen to be restricted to 0 and 1. 
More in general, by (1), every polynomial over Z p is also a polynomial 
over any field of characteristic p. In particular, it makes sense, given a 
polynomial over Z p , to ask about its zeros in any field of characteristic p. 
Theorem. Every field F of characteristic ρ has the following properties: 
(1) An element a fulfils a
p = a precisely when a 6 Z p . 
(2) i4ritirary elements a, b fulfil (a + b)
p = a? + bP. 
(3) Given a polynomial f(x) 
over Z p , which has a zero a in F, then 
a
p, a
p , a
p , ... 
are also zeros of f(x). 
PROOF. 
(1) Every element a e Z p fulfils a
p = a by Fermat's Theo-
rem (11.4.1). Since the polynomial x
p — χ cannot have more than ρ zeros 
(see Corollary 11.1), it follows that a
r φ a for any a G F - Z p. 
(2) By the binomial theorem, we have (a + b)
p = ΣΛ=Ο (
p
k)a
kb
p~
k 
(see 
11B). However, in a field of characteristic p, the element ρ is equal to 0, 
and each (J), where 0 < ib < p, is a multiple of this element, i.e., (£) = 0. 
Therefore, 
(a + b)
p = (£ja
p 
+ 
= a" + 6*. 
(3) Observe first that from (2), we immediately get (a 0 + ai + · · • + 
a » )
p = °o +
 α ι Η 
r a
p. Let f(x) = /ο + /i* + 
1-/,!
1 be a polynomial 
with a zero in a. Then f(a) = 0 implies [/(a)]
P = 0, i.e., 
/S + / X + - " + tfa" = 0. 
Now, assuming that the coefficients of f(x) lie in Z p , we have / f = /,· 
by (1). Thus, the last equation implies f(a
p) = £ V _ 0 f{a
ip 
= 0. 
By applying the same rule, we see that f((a
pY) = 0, i.e., a
p' is a zero 
of f(x), etc. 
D 

11.6. 
MINIMAL 
POLYNOMIAL 
213 
Example. In GF(2
m), 
we have (a + 6)
2 = a
3 + 6
2. Any binary polyno-
mial f(x) with a zero in a has zeros in a
2, a
4, a
8, . . . (many of which are 
mutually equal; e.g., a? = a by Fermat's Theorem). 
Remarks 
(4) Analogously to (2) in the above theorem, we can see that in a field of 
characteristic p, 
(x + af = x
p + a
p. 
More in general, for every polynomial a(x) = ao + a\x + 
V α,χ', 
[a(x)]
p 
= α
ρ
0 + 
α
ρχ>'+·+α'>,χ'". 
(5) Note that by applying (2) of the above theorem twice, we get (α+δ)
ρ 
= 
(a
p + 6
P )
P = 
+ V . In general, whenever q is a power of p, then 
(a+6)« = a« + 6». 
11.6 
Minimal Polynomial 
We have already remarked that a binary polynomial f(x) can be regarded 
as a polynomial over GF(2
m) 
and that we can study the zeros of / ( z ) 
in the extended field. An important algebraic concept results from the 
opposite point of view: given an element β in the extended field 
GF(2
m), 
we are interested in binary polynomials having a zero in β. By Fermat's 
Theorem (11.4), such a polynomial always exists, e.g., z
r—x, where r = 2
m . 
The lowest-degree binary polynomial with a zero in β is called the minimal 
polynomial of β. More in general: 
Definition. Let F be a field and lei β be an element of some extension 
of F. By the minimal polynomial of β (with respect to F) is meant the 
lowest-degree monic polynomial over F with a zero in β. 
Example. 
(1) In the field 
GF(8) = Z 2[x]/mod x
3 + x + l, 
the element a (see Figure 1 in 11.2) has the minimal polynomial M(x) = 
z
3 -f χ + 1. In fact, since a
3 -f a + 1 = 0 in GF(8), 
M(x) has a zero in a, 
and no binary polynomial of degree less than 3 has a zero in α (except the 
zero polynomial which is not monic). 
By Theorem (2) of 11.5, M(z) also has zeros in a
2 and a
4 . It is clear 
that Μ (ζ) is the minimal polynomial of both of these elements. 
The element 1 has the minimal polynomial ζ + 1 and 0 has the minimal 
polynomial x. 

214 
CHAPTER 11. POLYNOMIALS AND FINITE 
FIELDS 
Remarks 
(1) Observe the importance of the requirement that the minimal polyno-
mial be over F. In fact, each element β of, say, GF(2
m) 
is a zero 
of the linear polynomial χ — β. But we are only interested in binary 
polynomials with a zero in β. 
(2) Let us point out that the requirement that the minimal polynomial be 
monic excludes the zero polynomial. This requirement serves otherwise 
to guarantee the uniqueness. 
Proposition. Each element β of a finite extension of a field F has a unique 
minimal polynomial Μ(x). Moreover, M(x) divides each polynomial over F 
with a zero in β. 
PROOF. By Fermat's Theorem (11.4), β is a zero of x
r - x, which is a 
polynomial over F. Let M(x) be the lowest-degree monic polynomial over F 
with a zero in β. We will prove that whenever a polynomial / ( x ) has a zero 
in β, then M(x) is a divisor of / ( x ) . It is then clear that the minimal 
polynomial is unique (since whenever two monic polynomials divide each 
other, then they are equal). 
We divide / ( x ) by M(x) over F and obtain a quotient q(x) and a re-
mainder r(x), 
/(x) = q(x)M(x) + r(x) 
and 
degr(x) < degAf(x). 
Since r(/?) = / ( / ? ) - g(/?)M(/?) = 0 - o(/?)0 = 0, we see that r(x) is a 
polynomial over F of degree smaller than that of M(x) with a zero in β. 
We can factor r(x) as r(x) = kr'(x), 
where ib is a scalar, and r'(x) is a 
monic polynomial. If ib φ 0, then r'(x) is a monic polynomial of degree 
smaller than that of M(x) and with a zero in β—a contradiction. Thus, 
ib = 0, and we conclude that f(x) = q(x)M(x). 
• 
Corollary. Every minimal polynomial is irreducible. 
In fact, if M(x) = /(x)o(x) is the minimal polynomial of β, then 
Μ(β) 
= 0 implies /(/?) = 0 or ρ(β) = 0. Thus, M(x) divides either 
/(x) or g(x), which proves that / ( x ) and g(x) cannot both have degrees 
smaller than that of M(x). 
• 
Theorem. The minimal polynomial of each element β of the Galois field 
GF(p
m) 
with respect to Z p can be computed as follows: 
Μ(Χ) 
= 
(χ-β)(χ-βΡ)(χ-βΡ
3)...(χ-βΡ'), 
where i is the smallest number with / J
p '
+ l = β. 

11.6. 
MINIMAL 
POLYNOMIAL 
2 1 5 
PROOF. 
( 1 ) Such an i exists. In fact, by Fermat's Theorem ( 1 1 . 4 ) , one 
candidate is t = η — 1 since βΡ" = β. 
(2) The elements β, β?, βΡ , ... , βΡ' are pairwise distinct: suppose 
β
9 
= β
9 
for some β < ι, then we divide the equation by βΡ' 
and get 
β — β
9 
. Thus, t — s > i; hence, s, t cannot be among 0, 1, 2, ... , i. 
(The only possibility would be s — 0, t = i, but β φ β
9 
by the choice of ».) 
(3) M(x) divides all polynomials over Z p with a zero in β. This follows 
from Corollary 1 1 . 1 and Theorem (3) of 1 1 . 5 . Thus, it remains to prove 
that M(x) has coefficients in Z p . Put M(x) = roo + mixH 
|-m rx
r. We 
will prove that mf = m, for each i. Then m, lies in Z p by Theorem ( 1 ) 
in 1 1 . 5 . 
Observe that the pth power of M(x) is Μ(x
p). 
In fact, by Remark (4) 
of 1 1 . 5 , 
[ M ( X ) ]
P 
= 
(χ-βγ(χ-βργ...(χ-βρ'γ 
= 
(x
p - β*) (x
p - β"*) • • • (x
p - /J*') ( χ
ρ - 
βΡ'
+>). 
Since β?'
+ι 
= /?, we conclude (shifting the last term forward) that 
[M(x)]
p = (χΡ - β) (χΡ - β*) (χΡ - 
/ ? P
3 ) · · · (χΡ - ρ ' ) = Μ ( χ Ρ ) . 
On the other hand, by Remark (4) of 1 1 . 5 , we have 
[M(x)]
p = m
p
0 + m
px
p 
+ ••• + 
m
px
pr. 
Consequently, m
P, the coefficient of [M(x)]
p at x
p', is equal to m<, the 
coefficient of M(x
p) 
at x
pi. 
• 
Examples 
(2) Let us find the minimal polynomials of all elements of C?F(8) (see Fig-
ure 4). We already know them for 0, 1, a, a
2, a
A, see Example ( 1 ) . 
Element 
Minimal Polynomial 
0 
X 
1 
x + 1 
a , a
2 , a
4 
χ
3 + χ + 1 
a
3 , a
5 , a
6 
x
3 + x
2 + 1 
Figure 4 : Minimal polynomials of elements of GF(8) 

216 
CHAPTER 11. POLYNOMIALS AND FINITE 
FIELDS 
For a
3 , we form ( a
3 )
2 = a
e , ( a
6 )
2 = a
1 3 = a
5 , and ( a
5 )
2 = a
1 0 = a
3 ; 
thus, here the process stops. Consequently, the minimal polynomial 
of a
3 (and a
6 and a
5 ) is 
M(x) = (x- 
a
3)(x - a
6)(x - a
5 ) = χ
3 + x
3 + 1. 
(3) Binary minimal polynomials of all elements of GF( 16) (see Appendix A) 
are listed in Figure 5. 
Element 
Minimal Polynomial 
0 
χ 
1 
z + 1 
a, α
2, a
4 , a
8 
x
4 + x + l 
a
3, a
6, a
9, a
12 
x
4 + x
3 + x
2 + x + 1 
a
5 , a
1 0 
x
3 + z + l 
a
7 , a " , a
1 3 , a
1 4 
z
4 + z
3 + l 
Figure 5: Minimal polynomials of elements of GF(16) 
(4) In GF(9) (see Figure 3 in 11.3), the element a has the minimal poly-
nomial χ
2 + χ + 2. To find the minimal polynomial of a
3 , we compute 
( a
3 )
3 = a
6 , ( a
6 )
3 = a
1 8 = a
3 ; thus, 
M(x) = (x - a
2) (x - a
6 ) = x
3 + 1. 
11.7 Order 
In this section, we show how to find elements of prescribed orders in finite 
fields. 
Proposition. In a field of r elements, 
(1) the order of each element divides r — 1, 
(2) every divisor of r — 1 is an order of some element. 
PROOF. Let ο be a primitive element (11.4) of a field of r elements. Then 
each nonzero element has the form a*, and since (o*)
r 
1 = (a
r~
1)* 
= 

11.7. 
ORDER 
217 
1* = 1, it follows that the order of a' is a divisor of r — 1 (see Propo-
sition 11.4). Conversely, whenever i is a divisor of r— 1, then the ele-
ment 6 = α (
Γ - 1 ) / ' has order t: 
(1) 6'' = α**-
1*!* 
= a
1""
1 = 1, 
(2) f 
ji 1 for all j < i because a' φ I for all s < r [in particular 
for s = j ( r - l)/i]. 
• 
Examples 
(1) In GF(16), we can find elements of orders 1, 3, 5, and 15. In fact, the 
primitive element a has order 15, a
3 has order 5, and a
6 has order 3. 
(2) Is there an extension of Z 2 in which some element has order 9? In other 
words, is there a number η such that 9 divides 2" — 1? Sure: η = 6. 
This is so since tp(9) = 6 for the following function φ: 
Definition. The Euler function assigns to each number η = 1, 2, 3, ... 
the number <p(n) of all i = 1, 2, ... , η — 1 such that η and i are relatively 
prime. 
Examples 
(3) φ(ρ) = ρ — 1 for every prime ρ since each i < ρ is relatively prime 
with p. 
(4) (p(jpq) = (p — l)(g — 1) for two distinct primes p, q. In fact, all ι < pq 
except i = sq (s = 1, 2, . . . , ρ — 1) and i = pi (t = 1, 2, . . . , q — 1) have 
the property that pq and i are relatively prime. Thus, 
f(pq) = pq -1 - (β - i) - (p - 1 ) = (p - 1 ) ( ? - 1 ) . 
(5) y>(p*) = p* — p *
- 1 for each prime p. In fact, all t < p
k except i = sp 
(s = 1, 2, . . . , p *
- 1 ) are relatively prime with p*. 
Using these rules, we can compute ip(n) for all η < 11, see Figure 6. 
η 
1
2
3
4
5
6
7
8
9 
10 11 
y>(r») 
0
1
2
2
4
2
6
4
6 
4 
10 
Figure 6: The Euler function ψ{ή) for η < 11 

218 
CHAPTER 11. POLYNOMIALS AND FINITE 
FIELDS 
Euler-Fermat Theorem. // q and η are relatively prime natural num-
bers, then η is a divisor of 
— 1, i.e., 
= 1 (modn). 
PROOF. We will work in the ring Z„ = Z/mod η of coset leaders modulo η 
(see 7.2). Observe first that whenever a number i is relatively prime with n, 
then all elements of the coset i + nZ are numbers i + nk relatively prime 
with n. (In fact, any common divisor of η and i+nk divides i = i+nk — nk, 
thus, it is equal to 1.) Let αχ, a2, • • • , αψ{η) be all the numbers between 1 
and η - 1 which are relatively prime with n. Each of the numbers οα< is, 
obviously, relatively prime with η too. The coset oo, + nZ contains a coset 
leader which, as observed above, must be relatively prime with n, i.e., is 
equal to α* for some ib. Moreover, the cosets of qai are pairwise distinct 
for t = 1, ... , n: suppose qai and qa} lie in the same coset, i.e., η di-
vides q(ai — Oj). Then η must divide o, — Oj; however, |α< — a,| < η—thus, 
α,· = Oj. We conclude that the cosets of the numbers qai, qa2, • • • , 
qav(n) 
are the same as those of the numbers aj, a2, ... , αφ(„γ Consequently, the 
two products 6 = oio 2 .. .a v(„) and q^^b 
= (qai)(qa2) • • (qav(n)) 
lie in 
the same coset: 
q*
nh 
= b (modn). 
That is, η divides q^
n^b — b = b(q'
fi^ — l). However, 6 is relatively prime 
with n; thus, η divides q^
n) 
- 1. 
• 
Corollary. The Galois field GF(q*^) 
contains an element of order η 
whenever q and η are relatively prime. 
In fact, the field has r = qf^ 
elements and η divides r — 1. Apply 
Proposition (2) above. 
• 
Examples 
(6) We want to find an extension of Z2 with an element of order 11. Since 
V>(11) = 10, we can find such an element in G F ( 2
1 0 ) . 
(7) An element of order 7 exists, by the above corollary, in GF{2*^) 
= 
GF(64). 
However, it also exists in GF(S). Thus, the corollary does not 
always provide the best answer. 
(8) No element of GF(2
m) 
has an even order. In fact, 2
m - 1 is odd, thus, 
it is not divisible by an even number. This shows that the hypothesis 
that g and η be relatively prime is essential. 

11.8. 
THE STRUCTURE 
OF FINITE 
FIELDS 
219 
11.8 
The Structure of Finite Fields 
The aim of the next two sections is to prove that every finite field is a 
Galois field, and that the number of elements determines essentially the 
field. The reader may skip the remainder of the chapter without breaking 
the continuity of the text. 
Let us first discuss the "essential determination" of a finite field. We can 
always create a new field by simply re-labeling the elements (e.g., in Z j , we 
can use · , — instead of 0, 1). By giving each element α of a field F a new 
label ρ(α), we will get another field F' in which the algebraic operations 
are defined as in F, i.e., 
0{a) + e(b) = β(α + b), 
-ρ(α) = g(-a), 
, 
„ 
k > 
P 
< ν 
for all a, 6 in F. 
(*) 
ρ(ο)β(6) = g(ab), 
ρ(α)~
ι 
= ρ(α~
ι) if α φ 0,
 
1 ' 
The fields F and F' are essentially the same. Conversely, if we can find, 
for two fields F and F', a bijective correspondence α ι—• ρ(α) between the 
elements α (of F) and ρ(α) (of F') such that the algebraic operations agree 
in the sense of (*), the two fields can be identified. 
Definition. Two fields F and F' are said to be isomorphic provided that 
there exists a bijective function ρ from F onto F' satsfying (*). 
Theorem. Every finite field is isomorphic to a Galois field. 
Remark. More in detail: given a finite field F of characteristic p, find a 
primitive element a of F and its minimal polynomial M(x) (of degree m). 
Then F is isomorphic to the Galois field 
GF{p
m) 
= Z p[x]/mod M(x). 
PROOF (of the theorem and remark). Since F has a primitive element a 
(Theorem 11.2) and a has an irreducible minimal polynomial M(x) (see 
11.6), the statement of the remark above makes sense. Recall that the 
elements of the Galois field GF(p
m) 
are polynomials / ( a ) , where f(x) 
is a polynomial over Z p of degree smaller than m. 
By computing the 
polynomials in the primitive element a, we obtain a function 
g:GF(p
m)^F, 
e[ff(a)] 
= / ( a ) . 
We will show that ρ is a bijection satisfying (*). 
(1) ρ is one to one since β[/(α;)] = β[ρ(ο)] means that / ( a ) = g(a), 
i.e., a is a zero of the polynomial f (x) — g(x). 
By Corollary 11.6, this 
implies that M(x) divides f(x) — g(x), but the degree of f(x) — g(x) is 
smaller than m; thus, f(x) — g(x) = 0. Consequently, / ( a ) = 
g(ct). 

220 
CHAPTER 11. POLYNOMIALS AND FINITE 
FIELDS 
(2) Q is onto because each element 6 of F has the form 6 = / ( a ) for 
some polynomial / ( x ) over Z p of degree smaller then m. In fact, if 6 = 0, 
choose f(x) — 0. For nonzero elements, we have 6 = α', since a is primitive. 
Now the situation is clear if t < m: put }{x) = χ*. For i = m, use that 
fact that α is a zero of the (monic) polynomial M(x) = Mo + Mix + 
h 
a
m = -(Mo + Mia + · · · + M m _ 1 a
m "
1 ) . 
Thus, for / ( x ) = -(Mo + Mxx + ••• + M m _ i x
m _ 1 ) , we have 6 = / ( a ) . 
Analogously, if ι = m -f 1, we can express a
m
+
1 by the powers 1, a, a
3, 
... , a
m
_
1 (using the above expression for a
m twice), etc. 
(3) Q satisfies (*). In fact, there is no problem with the addition and the 
opposite element, since f(a) + g(a) — A(a) holds precisely if / ( x ) + g(x) = 
Λ(χ): 
e[f(a) 
+ g(a)} = f(a) + g(a) = g[f(a)\ 
+ 
e[g(a)), 
e[-f(o)]=-f(a) 
= 
-g[f(a)]. 
For the multiplication, recall that f(at)g(a) — h(a) implies that the poly-
nomial M(x) divides /(x)o(x) — A(x); say, f(x)g(x) 
— Λ(χ) = 
q{x)M(x). 
Then M(a) = 0 implies 
e[f(a)]e[g(a)] 
= 
f(a)g(a) 
= 
q(a)M(a) + h(a) 
= 
h(a) 
= <?[M«)] 
= 
g[f(a)g(a)}. 
Moreover, since ρ(1) = 1, it follows that β[/(α)~
ι] = e[/(o)]
 
l: divide 
the equation β[/(α)]β[/(α)]~ = 1 through 
tf[/(ar)]. 
D 
Proposition. Let F be a field of characteristic p. For each power q of p, 
all elements a of F satisfying a
1 = a form a field Fo whose extension 
is F. If the number of elements of F is a power of q, then Fo has precisely 
q elements. 
PROOF. 
I. Denote by Fo the set of all elements of F satisfying a
9 = a. 
We are to prove that Fo forms a field whose extension is F , i.e., that Fo is 
closed under the algebraic operations in F: 
(1) Addition: given a, b in Fo, then by Remark (5) of 11.5, 
(o + 6)» = α» + b< = a + b. 

11.9. 
EXISTENCE 
OF GALOIS 
FIELDS 
221 
Thus, α + 6 lies in Fo. 
(2) Opposite element: if ο is odd, then for α in Fo, 
(-o)« = -(a*) = -a. 
If q is even, then it is necessarily a power of 2, and there is nothing 
to prove, since —a = a. 
(3) Multiplication: given a, b in Fo, then 
(ab)
9 = aH" = ab. 
(4) Inverse element: ( a
- 1 ) ' = (α')
 
1 = a~
l for any a in Fo. 
II. Suppose F has q
m elements. Since q — 1 divides q
m — 1 = (ο — 1) χ 
( i
m
-
1 + q
m~
2 
+ ··· + l), there is an element c of order q — 1 in F (see 
Proposition 11.7). Each of the elements 0, c, c
2, ... , c
,
_
1 (= 1) is clearly a 
zero of x
1 — x. These q elements are pairwise distinct by Proposition 11.2; 
thus, these are all zeros of x' — x. In other words, 
F 0 = { 0 , c , C
2 , . . . , c « -
1 } . 
• 
Corollary. The Galois field GF(p
m) 
is an extension of GF(p*) whenever 
k is a divisor of m. 
In fact, put q = p
k in the above proposition: if ib divides m, then p
m is 
a power of q. 
• 
Example. GF(16) extends GF(4): in the table of GF(16) in Appendix A 
the elements 0, 1, a
5 , a
1 0 (i.e., the zeros of x
4 — x) form the field GF(4). 
Observe that GF(16) does not extend GF(8): no element of GF(16) 
has order 7. 
11.9 
Existence of Galois Fields 
In the preceding section, we proved that the only finite fields are the Galois 
fields. We now prove that they exist: 
Theorem. For each prime ρ and each number m, there exists a Galois 
field 
GF(p
m). 

222 
CHAPTER 11. POLYNOMIALS AND FINITE 
FIELDS 
PROOF . 
(!) We will first prove that there exists a field F of characteristic ρ 
in which the polynomial x
p" - χ factors into linear factors. We will find F 
by approximations: F = F< for some i, where F< are the following fields. 
Put Fo = Z p and factor x
p" - χ in Fo using the unique factorization 
theorem (11.1): 
* > " - * = / i ( * ) / 3 ( x ) . . . / . „ ( * ) . 
If each of the irreducible polynomials, /,(x) has degree 1, then put F = Fo. 
Else, find i with /i(x) of degree at least 2 and use /,(z) to extend the 
field F 0: 
F, = F 0[z]/mod/i(z). 
By Theorem 11.2, Fi is a field extending Fo. Therefore, we can factor 
x
p 
— χ over F\ using the unique factorization theorem again: 
x
p" - χ = gi(x)92(x). 
• fft,(x)-
Observe that every linear factor fj(x) = χ — a of the above factorization 
in Fo appears again in the factorization in F\ (because this means precisely 
that α is a zero of x
p 
— x). The number k\ of factors is now strictly larger 
than the previous number Jfco because the above polynomial fi(x) has a 
zero α in Fi; thus, x
p 
— χ has a new linear factor: χ — a. Now continue 
in the same manner: if each of the irreducible polynomial </;(x) has degree 1, 
put F = Fi, else Fj = Fi[x]/mod g}{x), 
etc. Since the number of factors 
is strictly increasing, after at most p
m steps, we will find » such that the 
field F = Fi has the required property. 
(2) The polynomial x
p" — χ has no multiple zeros (11D) in any field 
of characteristic ρ because its formal derivative 
(*>" - ζ ) ' = ρ - χ Γ "
1 - 1 = - 1 
has no zero. Thus, in the field F constructed in (1), the polynomial x
p" — χ 
has p
m distinct zeros. By Proposition 11.8, these zeros form a field of 
p
m elements. That field is isomorphic to G F ( p
m ) by Theorem 11.8. 
• 
Corollary. For each prime p, there exist irreducible polynomials of all de-
grees over Z p . 
Concluding Remarks 
(1) For each power of a prime q = p
m, 
there exists an essentially unique 
field of q elements: the Galois field GF(q). 
It can be constructed by 
finding an irreducible polynomial r(x) over Zp of degree m: GF(q) = 
Zp[x]/mod r(x). 

EXERCISES 
223 
(2) Each Galois field GF(q
k) 
is an extension of GF(q) [see Corollary 11.8]. 
(3) For each finite field F and each number η relatively prime with the 
number of elements of F, some extension of F contains an element of 
order n. [In fact, F = GF(q) for some q and then such an element 
can be found, by Corollary 11.7, in GF(q'
p^), 
which is an extension 
of GF(q).] 
(4) Given an element β of some extension of a finite field F, there exists 
a unique minimal polynomial M(x) of β. In case β has order n, the 
minimal polynomial is a divisor of χ" — 1. [In fact, χ" — 1 is a poly-
nomial over F with a zero in β. It follows that M(x) divides it, see 
Proposition 11.6.] 
Exercises 
11A 
Let F be a field of characteristic p. For each natural number n, 
denote 
n = l + lH 
hi 
(n summands). 
In particular, in F, we have ρ = 0, ρ + 1 = 1, etc. 
Verify that for all η = 0, 1, ... , ρ (= 0), we can define binomial coefi-
cients in F in the usual manner: 
(.)-*"~»(«^ 
if 0 < k < p, and 
Verify that the following well-known formula is valid: 
11B 
Prove that a field of characteristic ρ has the following properties. 
(1) The usual binomial theorem: 
(a + br = 
±fya
kb»-
k 

224 
CHAPTER 
11. POLYNOMIALS AND FINITE 
FIELDS 
holds for all η = 1, 2, ... , ρ and arbitrary elements a, 6; beware that 
the exponents are interpreted as ordinary natural numbers, not as 
field elements: α' = α χ α χ · · · χ α ( t factors). [Hint: use mathematical 
induction on n, applying the formula in 11 Α.] 
(2) For each polynomial f(x) over Z p we have 
/(*") = [/(*)]"· 
[Hint: analogous to 1 IB (1).] 
11C 
The formal derivative of a polynomial a(x) = do + αιχ + α 2χ
2 + 
(- anx
n 
is defined by a'(x) = a\ + 2a 2x + 3a3X
2 Η 
h n a „ x "
- 1 (where 
2 = 1 + 1 , etc.). Prove that the "usual" formulas for derivatives are true: 
(1) [α(χ) + 6(χ)]' = α'(χ) + 6'(χ), 
(2) [α(χ)6(χ)]' = α'(χ)6(χ) + o(x)6'(x). 
11D 
Multiple zeros. An element a is called a multiple zero of a polyno-
mial / ( x ) provided that (x — a)
k divides / ( x ) for some k > 2. 
(1) Prove that each multiple zero of f(x) is a zero of the formal deriva-
tive /'(x). 
(2) Find a polynomial over Z 2 with no multiple zeros whose derivative 
has a zero. 
(3) Prove that every common zero of / ( x ) and /'(x) is a multiple zero 
of/(x). 
H E 
Prove that in a field of characteristic 2: 
(1) Every element has a square root (i.e., for each a, there exists 6 with 
a = 6
2). [Hint: if c is primitive in GF(2
m), 
then c
2"* = c; thus, c has 
the square root c
1 
, and then each c* has a square root too.] 
(2) The formal derivative of every polynomial is a perfect square (i.e., for 
each polynomial / ( x ) , there exists a polynomial g(x) with / ' ( x ) = 
[g(x)] ). [Hint: since ( x
n ) ' = n x "
- 1 = 0 for η even, the formal 
derivative has only even powers. Using (1), write f'(x) = /
2 + /
2 x
2 + 
f\x* 
Η 
and then put g(x) = f0 + / 2 x + f4x
7 
+ ···.] 

EXERCISES 
225 
1 1 F 
Verify that orders of elements of GF(2
m) 
are always odd numbers. 
More in general: if a field of q elements has the property that some of its 
extensions contain an element of order n, verify that q and η are relatively 
prime. Compare with Corollary 11.7. 
11G 
Formal power series. 
Just as (finite) words can be expressed by 
polynomials, infinite words correspond to a so-called formal power series. 
In other words, given a field F, a formal power series over F in one inde-
terminate χ is an expression of the form Σ^ο"·**" where α 0, α,, a 2 , . . . 
are elements of F. The summands α,χ' with α,- = 0 can be omitted from 
the expression. (Thus, in particular, every polynomial is a formal power 
series.) The question of convergence, discussed so much in the case F = R, 
never arises in finite fields; thus, a formal power series is not a function. 
(1) Verify that all formal power series form a ring under the usual addition 
oo 
oo 
oo 
Σ α,χ* + ]Γ] 6,χ' = £ ( α , + ό,)** 
1=0 
ι'=0 
1=0 
and multiplication 
(
OO 
ν 
y OO 
\ 
OO 
Σ***) (Σ^**) = Σ(
α'
6°
+α·-
161 + · · ·+
αο*<)**'. 
»=0 
' \ = 0 
' 
ι'=0 
(2) A remarkable fact: the well-known formula for the sum of a geometric 
series is valid even in the formal setting: for each element a, 
oo 
, 
More precisely, the product of the power series 
α'χ' and 1 — ax is 1. 
Prove it. [Hint: multiply £ > V ( 1 - ax) = 
- Σ >
< + 1 χ
< + 1 . ] 
11H 
Prove that polynomials over any field have the following properties: 
(1) The following formula 
i - l 
x
i
m - 1 = ( x
m - l ) ^ ( x
m y 
1=0 
holds. Therefore, 
m divides η 
= > 
x
m — 1 divides χ" — 1. 

226 
CHAPTER 
11. POLYNOMIALS AND FINITE 
FIELDS 
(2) Conversely, whenever x
m - 1 divides x
n — 1, prove that m divides n. 
[Hint: given x" - 1 = ( x
m - l)q(x) 
= ( x
m - ΐ)(ϊο + 9ι* + · • + o,x
r), 
multipy out the last product.] 
H I 
Let / ( x ) be an irreducible polynomial over a field F. Prove that 
/ ( x ) is the minimal polynomial (with respect to F) of any zero / ( x ) has in 
any extension of F. Use this fact to describe the minimal polynomials of 
all elements of GF(16) w.r.t. GF(4). Can you apply Theorem 11.6? 
11J 
Prove the following: 
(1) χ
8 — 1 factors as the product of all binary irreducible polynomials 
whose degree divides 3 (see Figure 4 of 11.6). 
(2) More in general, the product of all binary irreducible polynomials 
whose degree divides η is equal to χ
2 
— 1. [Hint: if α is a primitive 
element of GF(2"), then χ
2* — χ factors as x(x — a)(x — a
2 ) · · (x — 
a
2 
_
1 ) by Fermat's Theorem. Use the fact that minimal polynomials 
are irreducible and vice versa, and apply Theorem 11.6.] 
(3) Generalize (2) from Z 2 to any finite field. 
11K 
Find a primitive element of Za[x]/x
2 + 1. 
11L 
What is an algebraic extension F[x]/mod p(x) in the case p(x) has 
degree 1? 
11M 
Prove that in a field of characteristic p, no element has an order 
divisible by p. 
U N 
Prove that in a field of characteristic p, every polynomial / ( x ) fulfils 
[/(«)]'" = jt"+/rv-+...+is-«»'". 
Conclude that for polynomials / ( x ) over Z p > 
[/(«)]'" = / ( « 0 . 
H O 
Find an extension of Z 2 in which χ
9 — 1 factors into linear factors. 
H P 
Prove that if GF(2
f c) extends GF(2"), then k is divisible by n. Find 
all fields which are extended by GF(16). 

NOTES 
227 
H Q 
Show that the polynomial χ
2 — 1 has more than two zeros in Z15. 
Compare this with Remark (1) in 11.1. 
1 1 R 
What are the orders of all elements of GF(27) and GF(32)? 
11S 
An irreducible polynomial p(x) over a field F is called primitive if 
the indeterminate α is a primitive element of F[x]/mod p(x). 
(1) Find a nonprimitive irreducible polynomial of degree 2 over Z3, and 
find a primitive polynomial of degree 2 over Z3. 
(2) Prove that every finite field has primitive polynomials of all degrees. 
[Hint: use Theorems 11.2 and 11.9.] 
11T 
Construct GF(16) 
as an algebraic extension of GF(4). 
N o t e s 
Many textbooks on modern algebra contain material concerning finite fields. 
A good reference is, e.g., Berlekamp (1968), Chapter 4. 

Chapter 12 
BCH Codes: Strong 
Codes Correcting 
Multiple Errors 
In this chapter, we introduce an important class of cyclic codes: the Bose-
Chaudhuri-Hocquenghem codes (BCH codes). Their properties are sum-
marized in Figure 1. The main distinctive feature of the BCH codes is the 
possibility of choosing the number of errors they are to correct. In the next 
chapter, we present a fast decoder of BCH codes. 
Specified: 
By zeros β, β
2, ... , β
2* of all code 
words 
Length: 
η = order of β 
Minimum distance: 
d > It + 1 
Generator polynomial: 
Least common multiple of the min-
imal polynomials of β, β
2, ... , β
2* 
Information symbols: 
k = η — degree g(x) 
Error-control capability: 
Corrects t errors; a fast decoder is 
presented in Chapter 13 
Figure 1: Properties of BCH codes 
229 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

230 
CHAPTER 
12. BCH CODES 
The most important BCH codes are, undoubtadly, the binary codes. 
However, the nonbinary Reed-Solomon codes, which are BCH codes of 
length η such that the code alphabet has η + 1 letters, are tools for a 
construction of good burst-error correcting binary codes. We also present 
a class of codes generalizing BCH codes, the Goppa codes, which have in-
teresting theoretical properties, and are used in cryptography, as will be 
explained in 15.3 below. 
We first revisit. Hamming codes, which turn out to be special BCH codes, 
then we present the double-error-correcting BCH codes, and, finally, we 
turn to the general BCH codes. We also introduce nonbinary BCH codes, 
called Reed-Solomon codes, from which important binary burst-error-cor-
recting codes are derived. The last section is devoted to Goppa codes, a 
generalization of BCH codes used, e.g., in cryptography. 
12.1 
Hamming Codes as Cyclic Codes 
We presented Hamming codes in 5.5 via binary expansions of numbers as 
columns of the parity check matrix. Those codes are not cyclic. We now in-
troduce (equivalent) cyclic codes via a prescribed zero of polynomials. This 
is both an important view of Hamming codes and a prelude to BCH codes, 
which are denned by zeros. 
Suppose that a fixed element β of some extension of the field Z 2 is given. 
If the order of β is n, we can consider all binary words WQW\ . . , U » „ _ I of 
length η characterized by the equation 
χυ(β) = 0, 
i.e, all binary polynomials of degree smaller than η with β as a zero. All 
these words form a cyclic code. In fact: 
(1) Let Μ (χ) be the minimal polynomial of β. Then M(x) is a divisor 
of x
n - 1. (Proof: since β" = 1, β is a zero of x
n - 1, thus, M(x) 
divides χ" - 1 by Proposition 11.6.) 
(2) M(x) generates a cyclic code of length η—see Theorem 10.3. 
(3) A binary polynomial of degree smaller than η has β as a. zero if and 
only if it is a multiple of M(x). 
Consequently, the binary code determined by β is the cyclic code of length η 
whose generator polynomial is M(x). 
In particular, if β is a primitive element of GF(2
m), 
we get a binary 
cyclic code of length 2
m — 1. Nothing new: 

12.1. 
HAMMING CODES AS CYCLIC 
CODES 
231 
Proposition. Lei β be a primitive element ofGF(2
m). 
Then the binary 
cyclic code of length 2
m - 1 given by the equation νυ(β) = 0 is a Hamming 
code. 
PROOF. The equation determining the code is η>ο + νιβ-\ 
r-u> n_i/?"
- 1 = 
0 (where η = 2
m — 1). In matrix form, 
[ 1 
β 
β
2 
β"-
1 
Thus, the one-row matrix 
Η = [ 1 β 
β
2 
Wo 
Wi 
W2 
U)„_l 
βη-Χ ] 
= 0. 
could be called a parity check matrix of the code, except that this is a 
matrix over GF(2
m), 
not a binary matrix. However, we can translate 
Η into binary: each element of GF(2
m) 
is a binary polynomial / ( a ) = 
fo 
· · ·+ fm-i<x
m~
l 
of degree smaller than m, which we can identify 
with the binary column of its coefficients: 
/<«) 
/ 
fo 
\ 
fx 
\ U-x 
) 
If each β* in the matrix Η is translated in this manner, we obtain an 
m χ η binary matrix which is the parity check matrix of the given code. 
Now the columns β* of that matrix are nonzero and pairwise distinct (see 
Proposition 11.2); thus, our code is a Hamming code by definition (5.5). 
• 
Examples 
(1) Let α be a primitive element of GF(8) in Figure 2 of 11.3. The binary 
cyclic code of length 7 given by the equation w(a) = 0 has the following 
parity check matrix: 
~2 
„ 3 
Λ 4 
„ 5 
Η = [ 1 
In binary: 1 is the polynomial l + 0 x a + 0 x a
2 o r the column 

232 
CHAPTER 
12. BCH CODES 
Analogously, o = 0 + l x o + 0 x a
2 i s the column 
etc. The translation of Η into binary is the following matrix: 
Η = 
1 0 
0 
1 0 
1 1 
0 
1 0 
1
1
1
0 
0 
0 
1 0 
1 1 1 
Another presentation of the same cyclic Hamming code is by means of 
its generator polynomial: g(x) is the minimal polynomial of a, which 
(by 11.7) is 
$(x) = x
3 + x + l . 
(2) The primitive element a of GF(16) in Appendix A determines the 
binary cyclic code of the parity check matrix: 
Η 
= 
[ 1 a 
.13 
a
1
4 
] 
1
0
0
0
1
0
0
1
1
0
1
0
1
1
1 
0
1
0
0
1
1
0
1
0
1
1
1
1
0
0 
0
0
1
0
0
1
1
0
1
0
1
1
1
1
0 
0
0
0
1
0
0
1
1
0
1
0
1
1
1
1 
The generator polynomial (see Figure 5 of 11.7) is 
y(x) = x
4 + x + l . 
12.2 
Double-Error-Correcting BCH Codes 
Before introducing BCH codes in general, we present an important special 
case (with an important special decoding technique): the binary BCH codes 
correcting double errors. Observe that (except for the repetition code of 
length 5) no double-error-correcting codes have been introduced so far. 
In 12.1, we saw that one zero β determines the (perfect) single-error-
correcting Hamming code. With some optimism, we can expect that two 
zeros (say, two different powers of the element β) will determine a double-
error-correctig code. This is indeed the case. However, the second zero 
cannot be β
2: we know that every binary polynomial with β as a zero also 
has β
2 as a zero (Theorem 11.5). The next element to try is β
3. We now 
show that this works. 

12.2. 
DOUBLE-ERROR-CORRECTING 
BCH CODES 
233 
Definition. By the binary double-error-correcting BCH code is meant the 
code of all binary words w of length η satisfying 
w(P) = υ>(β*) = 0, 
where β is an element of order η in some algebraic extension of the field Z 2 . 
Remarks 
(1) The name "double-error-correcting BCH code" naturally requires jus-
tification: we have to prove that the code corrects double errors. We 
will do this below by presenting a powerful decoding algorithm. 
(2) The BCH code is cyclic. This is proved exactly as in 12.1 for the 
Hamming codes: if M\(x) is the minimal polynomial of β and 
M3(x) 
the minimal polynomial of/?
3, then g(x) = Mi(x)M3(x) 
is a generator 
polynomial of the BCH code. 
(3) The length η of the BCH code can be any odd number: by Corol-
lary 11.7, an element of order η exists in some 
GF(2
m). 
Example. 
(1) Let β = α be the primitive element of GF(16) 
in Ap-
pendix A. The binary code of length 15 given by the equations 
w(a) = w(a
3) = 0 
can equivalently be described by the matrix equation 
1 a 
or 
or 
1 
a
3 
( a
3 )
2 ( a
3 )
3 
a 
13 
( a
3 )
1 3 
( a
3 ) 
a 
14 
14 
U>0 
U)13 
U/14 
Thus, we have the parity check matrix 
1 a 
Η = 
α
2 
α
3 
,13 
_39 
This can be translated into binary as we saw in 12.1: each element a' 
of GF(16) is a binary column of its four coefficients (when considered as a 

234 
CHAPTER 
12. BCH CODES 
polynomial in α of degree smaller then 4). Thus: 
'
1
0
0
0
1
0
0
1
1
0
1
0
1
1
1 
0
1
0
0
1
1
0
1
0
1
1
1
1
0
0 
0
0
1
0
0
1
1
0
1
0
1
1
1
1
0 
0_ 0 _ 0 _ 1 _0 0_ 1 
1 0 
0^ 1_ 1_1 
J 
1
0
0
0
1
1
0
0
0
1
1
0
0
0
1 
0
0
0
1
1
0
0
0
1
1
0
0
0
1
1 
0
0
1
0
1
0
0
1
0
1
0
0
1
0
1 
0
1
1
1
1
0
1
1
1
1
0
1
1
1
1 
This BCH code is a (15,7)-code, because the last matrix Η has linearly 
independent rows. 
Since ο has the minimal polynomial Mj(x) = χ
 4 + χ + 1, and a
3 the 
minimal polynomial Λ/ 3(χ) = χ
4 +x
3 + x
2 + χ + 1 (see Figure 5 of 11.7), it 
follows that every code word is a polynomial w(x) divisible by both M\(x) 
and M 3(x). The latter polynomials are irreducible; thus, w(x) must be di-
visible by Mi(x)Ma(x) (by the unique factorization theorem 11.1). Conse-
quently, the lowest-degree nonzero code word (= the generator polynomial) 
is 
All code words of the BCH code are the multiples q(x)g(x) 
of the generator 
polynomial g(x) = x
8 + x
7 + x
6 + x
4 + 1 by a polynomial q(x) of degree 
lower than 7. 
Decoding. Suppose that we receive a word w = woWi.. 
. U J „ _ I and 
that at most two positions are corrupted, say, the positions u>,- and Wj. We 
compute the syndrome based on the parity check matrix 
Si(x) 
= 
Mi(x)M 3(x) 
= 
(χ
4 + χ + 1) (χ
4 + χ
3 + χ
2 -I- χ + 1) 
= 
x
8 + x
7 + x
6 + x
4 + 1. 
Η = 
' 1 β 
β
2 
β
3 
1 β
3 
(β
3)
2 
(β
3)
3 
That is, we compute 
υ>ι 
η>0 + υ>ιβ + • • • + 
Wn-ιβ"-
1 
+ χνιβ
3 + • • • + 
wn.i(p)
n 
Η 
, η - 1 
υ>(β) 
_ Γ βι ' 

12.2. 
DOUBLE-ERROR-CORRECTING 
BCH CODES 
235 
The error-pattern word e(x) = ι* + χϊ has the same syndrome (see Propo-
sition 8.3): 
' «1 
.
 
fi3 . 
β
3ί + β
3) 
Thus, it is our task to compute the unknown elements β* and β* (from 
which i and j are easily determined) from the known syndrome (si.sa) 
using the following equations 
β*+β* 
= 
«ι, 
β*+β* 
= 
83. 
To solve these equations, we consider the third power of the first one: 
s
3 
= 0 * + 3β
2ψ+ 
3β*β
2> + β * 
= 
(β
3ί + β
3^ + 0β
ί(β
ί 
+ Ρ) 
= 
aa + 
pptai. 
Thus, we know both the sum of the unknown elements: β* + β* = «ι, and 
their product: 
^ 
= fLZfi = 
ea + „ e r». 
The sum and the product of two unknown numbers allow us to write a 
quadratic equation for those numbers: 
ι
2 - (β< + ρ)χ 
+ β
ίΡ = 0. 
That is, 
χ
2 + β ι χ + (s^ + saej-
1) = 0 . 
This leads to the following 
Decoding algorithm 
Step I: Compute the syndrome «ι = νυ(β) and S3 = w(p). 
If sj = « 3 = 0, 
the received word w is a code word. 
Step II: If «ι = 0 φ « 3, announce that more than two bits are corrupted. 
If 81 φ 0, solve the quadratic equation 
X
2 + 8\X + β
2 + 8a8\~
l 
= 0. 
(The solutions can be found by brute force: try all β*, i = 0, 1, 
. . . , n - l . ) 

236 
CHAPTER 
12. BCH CODES 
Step ΠΙ: If the quadratic equation has roots β* and β>, correct w,- and Wj. 
If it has roots 0 and β*, correct u><. If it has no roots, announce that 
more than 2 bits are corrupted. 
The proof of correctness of the above algorithm has essentially been 
performed above. There are four possibilities: 
(1) Two different bits u>,- and Wj have been corrupted. Then, as we have 
seen, β
1 and ft are the roots of the above quadratic equation. 
(2) One bit has been corrupted. Then si = ε(β) = β* and «3 = e ( ^ ) = 
β
3*. Thus, sj + ssi]"
1 = 0, and the quadratic equation takes the form 
χ
2 + 8χχ = 0. 
It has roots 0 and 8\ = β*, and we must correct ω,·. 
(3) No bit has been corrupted, i.e., «i = S3 = 0. 
(4) More than two bits have been corrupted—this takes place precisely 
when none of (l)-(3) does. 
Examples 
(1) (continued) When receiving 
w = 0000000111000000 = x
6 + x
7 + x
8, 
we decode as follows: 
Step I: The syndrome is 
ai 
= 
a
6 + a
7 + a
8 = a, 
s 3 
= 
a
1 8 + a
2
1 + a
M = a
3 + a
6 + a
9 = a
1 1 . 
Step II: Since s\ + S3SJ"
1 = a
2 + a
1 0 = a
4 , solve the equation 
x
2 + ax + a
4 = 0. 
Its roots are found by inserting χ = 1, a, a
2, ... : they are 
x\ = 1 
and 
xj = a
4 . 
Step III: Correct wQ (since X\ = a
0) and u>4 and obtain 
v = 100010111000000. 
It is easy to verify that v(ct) = v(a
3) = 0. Thus, the decoding is correct: 
ν is a code word of Hamming distance 2 from the received word. 

12.2. 
DOUBLE-ERROR-CORRECTING 
BCB 
CODES 
237 
(2) Let us construct the double-error BCH code of length 9. We must find 
an element of order 9 in some extension of Z 2 . Since y>(9) = 6 (see 
Figure 6 of 11.7), such an element can be found in GF(2
6) 
= 
GF(6A). 
In fact, the primitive element a in Appendix A has order 63 = 9 χ 7; 
thus, β = α
7 has order 9. The BCH code is therefore determined by 
the zeros β and β
3. We compute the minimal polynomials by means of 
Theorem 11.6. For β = α
7, we have 
Μι(χ) = (x - a
7)(x - a
1 4 ) ( x - a
2 8 ) ( x - a
8 6 ) ( x - a
1 1 2 ) ( x - a
2 2 4 ) , 
since a
2 2 4 = a
3 5 a n d ( a
3 5 )
2 = a
7 . For β
3 = a
2 1 , we have 
Μ 3(χ) = ( χ - α
2 1 ) ( χ - α
4 2 ) , 
since ( a
4 2 )
2 = a
2 1 . We conclude that the generator polynomial 
g(x) = Afi(x)A/ 3(x) 
has degree 8. Thus, this BCH code is simply the repetition code of 
length 9. 
(3) For the length η = 31, we choose the primitive element β = a of GF(32) 
in Appendix A. Then a has, obviously, the minimal polynomial 
Mi(x) = x
5 + x
2 + 1 
[used for defining GF(32)], and a
3 has the minimal polynomial 
M 3(x) 
= 
(χ - a
3)(x - a
6)(x - a
1 2 ) ( x - a
2 4 ) ( x - a
1 7 ) 
= 
x
5 + x
4 + x
3 + x
2 + 1. 
Thus, we obtain a cyclic (31,21)-code with the generator polynomial 
g(x) = ( x
5 + x
2 + l)(x
5 + x
4 + x
3 + x
2 + 1). 
The definition of double-error-correcting BCH codes contains some un-
certainty. In which extension of Z 2 are we working? Which element β are 
we choosing? We will now show that the answers to such questions are 
irrelevant: 
Independence Theorem. The double-error-correcting BCH code does 
not depend, up to equivalence, on the choice of the element β. 
That is, 
if β and β are two elements of order η in two extensions ofLi, 
then the 
two corresponding BCH codes: K, defined by the zeros β and β
3, and K, 
defined by the zeros β and β , are equivalent. 

238 
CHAPTER 
12. BCH 
CODES 
PROOF. If β lies in GF(2
m) 
and β lies in GF(2
m), 
then forj = mm, the 
field GF(2
k) 
is a common extension of G F ( 2
m ) and GF(2
m), 
see Corol-
lary 11.8. Thus, both β and β lie in GF(2
k). 
In that field, x
n - 1 factors 
as follows: 
x
n - \ = (χ~β)(χ-β
2) 
- ( χ - η , 
see Remark (1) in 11.4. Therefore, each element of order η is a power of β; 
in particular, β = β*. 
We will prove that the following numbers π» = 0, 1, ... , η 
πι = ifc (mod n) 
for Jb = 0, 1, ... , η 
form a permutation such that a word vo .. .v„_i is a code word of Κ if and 
only if the word vWo.. .υ»„_, is a code word of K. In fact, to show that 
το,..., τ η_ι is a permutation, it is sufficient to verify that the numbers τ * 
are pairwise distinct. If irt = 7Γ/, then iib — i/ is divisible by n, hence, 
P
k~
il 
= 1. This means (/?')* = (/?*')', i.e., J? =jf. 
Since β has order n, 
it follows that k = I (see Proposition 11.4). Therefore, το, xi,...,π„_ι is 
a permutation of the numbers 0 , 1 , . . . , η - 1. We conclude the proof by 
showing that for each word v, 
v0v\...vn-i 
lies in Κ 
... 
lies in JV. 
In fact, ν lies in A' precisely when £
 
vifi - 12
 ν'β
3'
 
= 
υ· The index i can 
be substituted by ir,-, and these equations read 5
3
= Σ
υ»·^
3*' = "· 
Since β** = tf
k = β , the last equations can be written as Σ,ν,,β 
= 
12 ντβ 
= 0. Those characterize the code words v„0vXl .. v„n_1 of K. 
• 
Concluding Remarks 
(1) For every odd number n, there is a binary double-error-correcting BCH 
code of legth n. It is determined by choosing an elementj? of order η 
in some Galois field G F ( 2
m ) and requiring that β and β be zeros of 
code words. The concrete choice of β is irrelevant: different choices 
give equivalent codes. 
(2) The BCH code is determined either by its generator polynomial g(x) = 
Mi(x)Ma(x) 
[where Mi(x) is the minimal polynomial of β*] or by the 
parity check matrix 
_ \ 1 β 
β
2 
•• 
β"-
χ 
H ~ 
1 β
3 β
6 ••• β
3(
η~
ι) 
' 

12.3. 
BCH CODES 
239 
(3) The code corrects double errors by solving a specific quadratic equation. 
It follows that its minimum distance is at least 5. It can be larger: for 
length 9, the BCH code is just the repetition code with d = 9. 
(4) Consider the extended double-error-correcting BCH code (see 8.5). It 
has minimum distance 6 (or more), thus, it can correct single errors and 
detect triple errors smultaneously (see 8.6). For example, the extended 
double-error-correcting BCH code of length 32 is a (32,21)-code. 
12.3 
BCH Codes 
Recall that for each r-element fied F (which will now be the code alphabet) 
and each number η such that r and η are relatively prime, there exists an 
element β of order η in some extension of F (Corollary 11.7). Recall further 
that each power β* (in the same extension) has a minimal polynomial M,(x) 
which is the smallest-degree polynomial over F with β* as a zero. 
Definition. Let F be a finite field, and let η be a number relatively prime 
with the number of elements of F. By a ί-error-correcting BCH code of 
length η in the alphabet F is meant the cyclic code of all words w in F" 
satisfying the equations 
w(J3) = u,(/?
2) = υ)(β
3) = ••• = u,(/?
2') = 0, 
where β is an element of order η in some algebraic extension of F. That is, 
code words are polynomials over F of a degree smaller than η with β, 
β
2, 
β
3, ... , β
2* as zeros. 
Examples 
(1) If the alphabet F is Z 2, we can delete the even powers of β in the above 
definition: any polynomial with β
1'as a zero also has β
2* as a zero [by 
Theorem (2) in 11.5]. Thus, the case t — 2 reduces to 
υ>(β) = υ>(β
3) = 0 
studied in the previous section. 
(2) A binary triple-error-correcting BCH code is given by 
υι(β\ = w(^) 
= ιυ(β*) = 0. 
For example, let β = a be the primitive element of GF( 16) in Ap-
pendix A. The minimal polynomials Μ,(«Ε) of the elements a' are listed 

240 
CHAPTER 
12. BCH 
CODES 
in Figure 5 of 11.6: 
Mj(x) 
= 
x
4 + x + l, 
M 3(x) 
= 
x
4 + x
3 + x
2 + x + l, 
M6(x) 
= 
x
2 + x + l. 
A binary polynomial has a, a
3 , a
5 as zeros precisely when it can be 
divided by each of M\(x), 
Λί3(χ), and Ms(x). Since those three poly-
nomials are irreducible (as are all minimal polynomials), divisibility by 
each of them is equivalent to divisibility by their product. Thus, the 
generator polynomial of the BCH code is 
g(x) = Λ/ 1(χ)Λ/ 3(χ)Μ 5(χ) = ( x
4 + x + l ) ( x
4 + x
3 + x
2 + x + l ) ( x
3 + x + l ) . 
It has degree 10; thus, we get a cyclic (15,5)-code. 
(3) Let F = Z3. A ternary double-error correcting BCH code of length 26 
is obtained by choosing β = α, the primitive element of GF(27) 
in 
Appendix A. The equations 
can be reduced by discarding w(a
3) = 0, since every polynomial with 
a zero in a has a zero in a
3 [by Theorem (2) in 11.5]. Let us compute 
the minimal polynomials: 
The code words are precisely the polynomials divisible by each of these 
(irreducile) polynomials, hence, divisible by their product. Thus, the 
generator polynomial is 
g(x) = Μι(χ)Λ/ 2(χ)Μ 4(χ) = ( x
3 + 2 + l ) ( x
3 + 2 x
2 + x + 2 ) ( x
3 + 2 x
2 + l ) . 
We get a cyclic ternary (26,17)-code correcting double errors. 
Remark. 
(1) Usually the best choice of the element β determining a 
BCH code is a primitive element of some extension of the code alphabet. 
Such BCH codes are called primitive. 
For example, primitive binary BCH codes are those of lengths η = 
2
m - 1. Compare the primitive codes of length 15 and 31 in Examples (1) 
and (3) of 12.2 with the nonprimitive (repetition) code of length 9 in Ex-
ample (2). 
u>(«) = w(a
2) = w(a
3) = u)(a
4) = 0 
M,(x) 
A/ 2(x) 
M4(x) 
(χ - a)(x - a
3)(x - a
9 ) = x
3 + 2x + 1, 
(x - a
2)(x - o
6)(x - a
1 8 ) = x
3 + 2x
2 + x + 2, 
(x - a
4)(x - a
l 2 ) ( x - a
1 0 ) = x
3 + 2x
2 + 1. 

12.3. 
BCH CODES 
241 
Proposition. The t-error-correcting BCH code is a cyclic code of length η 
whose generator polynomial is the least common multiple* of the minimal 
polynomials Mi(x) of β* for i = 1, 2, ... , It. In short: 
g(x) = LCM[ Mi(x), M 2 ( s ) , . . . . 
M2t(x)}. 
PROOF. The above polynomial g(x) is a code word since Λί<(/7*) = 0 implies 
g(fi) = 0 for i = 1, 2, ... ,2t. Since the order of β is n, we have β
η = 1; 
thus, (/?·')" = (/?")' = 1. It follows that 
is a zero of χ" - 1; thus, x
n - 1 
is a common multiple of all M,(x) (by Proposition 11.6). Consequently, 
x
n — 1 is a multiple of g(x). This implies that g(x) generates a cylic code 
of length η (see Theorem 10.3). 
Now, each multiple u»(x) = q(x)g(x) of g(x) fulfils 
= ΐΨ) x 0 = 0 
for i = 1, 2 
2t, 
thus, w(x) is a code word of the BCH code. Conversely, if w(x) is a code 
word of the BCH code, then w(x) is divisible by each M{{x) (by Propo-
sition 11.6 again). Since all Mi(x) are irreducible (see Corollary 11.6), it 
follows from the unique factorization theorem (11.1) that w(x) is divisible 
by the least common multiple g(x) of the polynomials Af,(x). Consequently, 
the cyclic code generated by g(x) is precisely the BCH code. 
Q 
Parity check matrix. Analogous to the case of double-error-correcting codes 
in 12.2, the following matrix is a parity check matrix of the /-error-correcting 
BCH code: 
Η = 
1 
β 
β
2 
ι β
2 
(β
2)
2 
ι ^ 
(βη
2 
β
3 
(β
2)
3 
(β
3)
3 
βη-Ι 
(β
2Γ
1 
ι β
2* (β
2,Ϋ (β
21)
3 • 
In fact, for each word w of length n, we have 
n - l 
" W0 + 
Μιβ 
\™(β) ι 
H w
t r = 
WQ + U)i/?
2 + u> 2(/?
2)
2 + • .+ 
w
n ^ )
n ~
1 
— *>(β
2) 
. WQ + li/i/?
2* + u> 2(/*
2')
2 
· + «, η_ 1(/?
2«)
η-
1 . 
Μβ
2*). 
'The least common multiple is, in general, the lowest-degree polynomial divisible by 
each of the polynomials under consideration. Since each M,(x) is irreducible, the least 
common multiple is simply the product of all Λ/,(χ)'β from which duplicate factors are 
deleted. 

242 
CHAPTER 
12. BCH CODES 
Thus, Hw
t r = O
t r precisely when w is a code word of the BCH code. 
The above matrix Η is expressed (instead of in the code alphabet F) 
in an extension of F. This can be translated to F, as explained in 12.2: 
suppose the algebraic extension in which β lies is F[x]/modp(x). The ele-
ments β* are then polynomials f(a) of degree smaller than m = degree p(x) 
in the indeterminate a. Each such polynomial is substituted by the column 
of its coefficients: 
ft = 
/(*) 
fo 
fx 
\ 
/ m - 1 
/ 
This turns the 2/ by η matrix Η above into a 2im by η matrix over F, 
which is a parity check matrix of the BCH code. 
Remark. 
(2) In order to prove that the ι-error-correcting BCH code does 
correct t errors, we apply a classical result concerning determinants of ma-
trices of the following form: 
• 1 
1 
1 
·· 
1 
αϊ 
a 2 
a 3 
• • a n 
«? 
«i 
ai 
• • a
2 
U-
1 
• r
1 
· 
Here <n, a 2, ... , a„ are elments of some field, and such matrices are called 
Vandermonde matrices. 
We assume here that the reader is familiar with the concept of deter-
minant and with the Laplace expansion formula for determinants: det A = 
12k
 aik(—i)
,+kAn, 
where An is the minor of a,*. 
Observe that for η = 2, the Vandermonde determinant is 
det 
1 
1 
αϊ 
a 2 
= a 2 - 
ax. 
For η = 3, we have 
det 
1 
1 
1 
αχ 
α 2 
as 
*i 
a 3 J 
= det 
1 1 
0 
a
2 - αχ 
1 
as - αχ 
0 
— aia 2 
a
2, — aj<i3 

12.3. 
BCH CODES 
243 
where, from the second and third rows, we subtracted the scalar multiple 
of the preceding row by ax. Thus: 
det 
1 
1 
1 
a 2 
a 3 
det 
o 2 - αχ 
a 3 - αχ 
(a 2 - ai)(a 3 - ai)det 
1 
1 
a 2 
a 3 
= 
(β2 - ai)(a3 - ai)(a3 - a 2 ) . 
In general, the Vandermonde determinant is 
= Π(
α<-
α;)· 
»>i 
• 1 
1 
1 
·• 
1 
αχ 
a 2 
a 3 
• · a„ 
det 
a? 
a? 
4 
• • 
4 
L a ? "
1 
«γ
1 «γ
1 · 
(The proof by induction on η is easy; we perform the same algebraic mod-
ifications as in the above case, η = 3.) In particular, the Vandermonde 
determinant is nonzero precisely when the elements αχ, a 2 , ... , a„ are 
pairwise distinct. 
Theorem. The t-error-correcting BCH code has minimum distance at least 
2t + 1 (and, thus, it actually corrects t errors). 
PROOF. Given a code word w of weight at most 2t, we will show that 
w = 0. We can find 2t indices, i\, i 2, ... , i 2 (, such that u>i = 0 for all 
* τ* *i. »2. · · · . *2t- (No claim is made about uij,, w^, ... , Wi3i.) 
Since w 
is a code word, for the parity check matrix Η above, we have 
Hw
t r = 0
T R. 
We can rewrite this system of equations by deleting all columns of Η corre-
sponding to the zeros u»,- = 0 (i φ ϊχ,..., i 2 <) and leaving only the columns 
ii, . . . , i 2t. Observe that the ith column of Η consists of the ith powers of 
a, a
2, ... , a
2*. Thus, we get 
α' 
(<*
ilY 
( a - )
2 
( a - )
3 
( α ' » < )
2 
( a " < )
3 
( a ' ' )
2 ' 
( a » ' )
2 < 
. . . 
( o > ) 2< 
' 0 " 
0 
wia 
— 
0 
_ 0 

244 
CHAPTER 
12. BCH CODES 
We will prove that this system of equations has a unique (all-zero) solution. 
It is sufficient to verify that the determinant of the matrix of that system 
is nonzero. We can divide the first column by a*
1, the second one by a*
1, 
etc., and we obtain the Vandermonde determinant: 
det 
(ο
4·)
2 
(a
1'
3)
2 
( a ' ' )
3 
(α<>)
3 
a 
'31 
( a ' " ) ' 
( a " )
2 ' 
(a*>)
2t 
··· 
( a * ' " )
2 ' 
= 
o
< lo
< 3...a
, :"det 
a ' 
( a
4 ' )
2 ' " ' 
( a
<
3 ) 
a 
( a " )
2 
( a ' ' )
5 
2<-l 
v»3< 
( o > ) 2 « - l 
= 
a^a*'...a'" J J ^ ' -a'*). 
P>« 
Since the elements a'
1, « '
2 , ... , a'
3' are pairwise distinct (see Proposi-
tion 11.1) and nonzero, we conclude that the above determinant is nonzero. 
Thus, w = 0. 
Q 
Concluding Remarks 
(1) Given numbers η and i, we can construct a t-error-correcting BCH code 
of length η over any finite field whose number of elements is relatively 
prime with n. The code is determined by the zeros β, β
2, ... , β
2*, 
where β is an element of order η in some extension of the code al-
phabet. The choice of β is irrelevant: different choices give equivalent 
codes, which is proved in the same manner as the independence theorem 
of 12.2. 
It is possible to define BCH codes more generally by using the zeros 
β
1, β*
+1, 
... , β*+
2ί~
ι 
for an element β of order η and for some ί = 
0, 1 
We restrict our attention to t = 1 for simplicity. 
(2) The <-error-correcting BCH code has minimum distance 2i + 1 or more. 
In Example (2) of 12.2, we saw a double-error-correcting BCH code 
of minimum distance 9. A fast decoder which performs the actual 
correction of t errors will be presented in Chapter 13. 

12.4. REED-SOLOMON 
CODES 
245 
12.4 
Reed-Solomon Codes and Derived 
Burst-Error-Correcting Codes 
Reed-Solomon codes are special BCH codes, namely, those BCH codes for 
which the prescribed zeros β* are found in the code alphabet, not in an 
extension. These codes are never binary. However, important binary burst-
error-correcting codes are obtained by a "translation" of Reed-Solomon 
codes over 
GF(2
m). 
Definition. A Reed-Solomon code is a BCH code of length η in a code 
alphabet of η + 1 symbols. 
Proposition. A t-error-correcting Reed-Solomon code has the generator 
polynomial g(x) = (χ — β)(χ — β
2) • • (χ — β
2%) and its minimum distance 
is precisely 2i + 1 • 
PROOF. A Reed-Solomon code over a field F is determined by the zeros 
β, β
2, 
... , β
2*, where the order η of β is the number of the elements 
of F minus 1—thus, β is a primitive element of F. 
Consequently, the 
minimal polynomial of β* is χ—β*, and Proposition 12.3 implies that g(x) = 
(χ - β)(χ — β
2) • • • (χ - β
2*). The weight of the generator polynomial g(x) 
is at most 2t + 1, and since by Theorem 12.3 the minimum weight is 2t + 1 
or more, we conclude d = 2t + 1. 
• 
Examples 
(1) Since 3 is a primitive element of Z7, we can define a double-error-
correcting Reed-Solomon code of length 6 over Z7 by the generator 
polynomial 
g(x) = (x - 3)(x - 3
2)(x - 3
3)(x - 3
4) = (x - 3)(x - 2)(x - 6)(x - 4). 
(2) Choose the primitive element α of GF(8) in Appendix A. The following 
polynomial 
g(x) = (χ - a)(x - a
2)(x - α
3)(χ - a
4 ) = a
3 + ax + x
2 + a
3 x
3 + x
4 
generates a double-error-correcting Reed-Solomon code of length 7 over 
GF(8). 
Derived burst-error correcting codes. Given a <-error-correcting Reed-Solo-
mon code Κ over GF(2
m), 
we can form a new code by interpreting each of 
the nonbinary symbols f[a) = /ο + / ι « + · · · + /
m _
1 a
m
_
1 of GF(2
m) 
as the 
binary m-tuple / 0 / i . . . / m - i - Thus, from the nonbinary code Κ of length 

246 
CHAPTER 
12. BCH CODES 
η = 2
m - 1, we derive a binary code K* of length nm by substituting for 
each symbol the corresponding binary m-tuple. 
For example, the generator polynomial of the Reed-Solomon code in 
Example (2) is the word α
3α1α
3100 of length 7 over GF(8). 
Since a
3 = 
a -(- 1 corresponds to 110, a corresponds to 010, etc., the derived binary 
code of length 21 contains the following word: 
110010100110100000 000. 
From the (7,2)-Reed-Solomon code over GF(8), we derive a binary (21,6)-
code. 
The importance of those derived codes lies in their capability of correct-
ing burst errors of length (< - l)m + 1. In fact, such a burst in the derived 
binary word can corrupt at most t (adjacent) symbols of the original word 
over G F ( 2
m ) , see Figure 2. 
I 
I 
I 
I 
I original word over 
GF(2
m) 
m 
ΙΑιΛίΛίΛίΛΙ 1 Μ I I 
derived binary word 
" 
» 
' 
buret—error 
Figure 2: A burst error on a word derived from a Reed-Solomon code 
For example, the above double-error-correcting Reed-Solomon code over 
GF(8) yields a binary (21,6)-code which corrects burst errors of length 3 + 
1 = 4 : any such burst affects at most two adjacent symbols of the original 
word. 
12.5 
Generalized Reed-Muller Codes 
Recall that puncturing (8.5) is a modification of a code consisting in delet-
ing the last symbol from each code word. We now return to the Reed-
Muller codes 7t(r, m) of length η = 2
m studied in Chapter 9. Since the 
code K(r,m) 
has even minimum distance d = 2
m
-
r (Remark 9.6), the 
punctured code TC(r, m) has the same error-correcting capacity: it corrects 
2
m
-
r
-
1 — 1 errors. The punctured code has a better information rate and, 
moreover, it is cyclic. 

12.5. 
GENERALIZED 
REED-MULLER 
CODES 
247 
Notation. For each natural number s, we denote by wt(s) 
the Hamming 
weight of the g-ary expansion of s. Examples: u>2(l) = w2(2) 
= 1 and 
u>2(3) = 2. 
Theorem. The punctured Reed-Muller code V.(r, m) is a binary cyclic code 
consisting of all binary polynomials of degree less than 2
m — 1 which have 
β' as zeros, where β is a fixed primitive element of GF(2
m), 
and s = 1, 2, 
... , 2
m — 1 is any number with w2(s) < m — r. In notation: 
w(x) lies in 7e(r, m) ·<==> ν{β') = 0 for all i<2
m, 
0 < w2{i) < m - r. 
The proof, which is quite technical, is omitted. The reader can find it 
in the monograph of MacWilliams and Sloane (1981). 
Definition. Let β be a primitive element of GF(q
m). 
By a generalized 
Reed-Muller code of length η = q
m — 1 and order r over the alphabet GF(q) 
is meant the cyclic code of all polynomials with β' as zeros for all i = 1, 2, 
... , η such that w9(i) < m — r. 
Remark. The case q = 2 leads to the punctured Reed-Muller code Tl(r, m) 
by the preceding theorem. Thus, Reed-Muller codes form a subclass of the 
extended generalized Reed-Muller codes: 7i(r, m) is obtained from Tl(r, m) 
by extension (8.5). 
Examples 
(1) Take q = 2, m = 4, and r = 2. All the numbers i = 1, 2 
2
4 - 1 
with u)2(») < 2 are 1, 2, 4, and 8. Thus, we obtain a cyclic binary code 
of length 15 with zeros at β, β
2, β
4, and β
6. Equivalently, with a zero 
at β. This is the Hamming (16, ll)-code. 
(2) For q = 2, m = 4, and r = 1, we consider all the numbers i = 1, 2, 
. . . , 2
4 — 1 with w2(i) < 3: besides the four numbers above, these are 
3, 5, 6, 9, 10, and 12. The code is determined by the zeros β, 
β
3, 
and β
5. This is the BCH (15,5)-code. 
(3) Take q = 3, m = 3, and r = 1. All the numbers i = 1, 2, ... , 3
3 - 1 
with u ) 3 ( t ) < 2 are 1, 3, and 9. The resulting ternary code of length 26 
is determined by a single zero β. 
Proposition. The binary generalized Reed-Muller code has minimum dis-
tance at least 2
m
_
r - 1. 
PROOF. It is sufficient to prove that the generalized Reed-Muller code is a 
subcode of the t-error-correcting BCH code, where t = 2
m~
r~
x 
— 1. Since 

248 
CHAPTER 
12. BCH CODES 
the latter code has minimum distance d > 2t + 1 (Theorem 12.3), it follows 
that the former code has this property too. Observe that the first number 
1 with u)2(i) > m - r is i = 2
m
_
r - 1. Thus, each i = 1, 2, ... , 2
m~
T 
-
2 satisfies Wi(i) < m - r. It follows that every code word u;(ar) of the 
generalized Reed-Muller code has β* as zeros for» = 1, 2 , . . . , 2
m
-
r - 2 = 2t. 
Therefore, w(x) is a code word of the i-error-correcting BCH code. 
Ο 
12.6 
Goppa Codes: Asymptotically Good 
Codes 
We now introduce a class of noncyclic binary linear codes, discovered by 
Goppa in 1970, which generalize binary BCH codes and contain a number 
of interesting codes (e.g., a double-error-correcting code of length 8). Later 
we will also use these codes for a construction of secret codes. Parameters 
of the important subclass of irreducible binary Goppa codes are listed in 
Figure 3. 
Specified: 
By an irreducible polynomial of de-
gree t over 
GF(2
m) 
Length: 
η = 2
m 
Information symbols: 
k > η — mi 
Minimum distance: 
d > 2i + 1 
Error-control capability: 
Corrects t errors by a technique 
analogous to that for BCH codes 
Figure 3: Parameters of irreducible binary Goppa codes 
Notation. Let r(x) be a fixed polynomial over a field F. Given an element a 
with r(a) φ 0, we denote by 
the following polynomial over F: 
_ L _ 
= _ r ( x ) - r ( a ) 
, 
χ — a 
χ — a 
(The right-hand side is well-defined: since the polynomial r(x) — r(o) has 
α as a zero, it is divisible by χ — a, see 11.1.) 

12.6. GOPPA CODES: ASYMPTOTICALY 
GOOD 
CODES 
249 
This notation expresses the fact that in the algebraic extension 
F[x]/r(x) 
(see 11.2), the above polynomial 
is inverse to χ — a: 
(χ - e
)
^ = - [ r ( x ) - r(a)]r(a)-
1 
= 1 + tr(z) 
[t = - r ( a ) "
1 ] 
and r(x) is the zero element of F[x]/r(x). 
Definition. Let F be a finite field. A Goppa code of length η in the al-
phabet F is determined by (i) a polynomial r(x) of degree > 2 over some 
extension of F and (ii) η elements αϊ, . . . , o„ of that extension, in none 
of which r(x) has a zero. The code words are precisely those words vi .. .vn 
in F
n which satisfy 
Σ —
= o. 
Before presenting examples, we mention an alternative definition with-
out the notation of 
Recall that the BCH code is characterized by the 
condition v(a') = 0 for s = 1, 2, ... , 2i; in other words, by ΣΓ=ι "ia" = 0 
for s = 1 2, ... , It. Rather analogously: 
Proposition. The code words of the Goppa code given by a polynomial r(x) 
are precisely those words v\ ... vn satisfying 
η 
53Μ«ίΓ
Χ«ί = 0 
/or β = 0, 1, ... , [degr(x)] - 1. 
»=ι 
PROOF. We first compute 
explicitly. Given 
r(x) = r0 + rix + 
rrtx' 
(rt^0), 
it is easy to verify the following equation [for each element a with r(a) φ 0]: 
Κ») ~
 
r (
f l ) 
_ 
r 
x ' - i 
+ ( a r < 
+ 
r 
1 )
I « - 2 
+ 
(
a 2
r
( + ar <_i + r t _ 2 ) x
1 _ 3 
χ — a 
+ · · · + (α'-
ιη 
+ a ' -
2 r t _ i + • · · + n ) . 
Thus, the coefficient of the polynomial 
_ 1 _ = _ Κ ^ ) - Κ α ) 
χ 
x-a 
χ — a 
at x *
- 1 (ib = 1, ... , t) is the following: 
- r ( a ) - V ~ S + α*"*
- 1»·.-! + · · • + r t ) = - r ( a ) -
1 ^ V " ^ . 

250 
CHAPTER 
12. BCH CODES 
Now the characteristic equation of the Goppa code: 
η 
implies that for each k = 1, ... , t, the coefficient at x
k 
1 is equal to zero. 
That is: 
]T>,r(a,)-
1 
= 0 
for * = 1, .... <. 
i=l 
j=k 
The case k = t yields 
η 
Γ , ^ υ ι Κ α , · ) -
1 = 0 
i=l 
and since rt φ 0, this is equivalent to $3" = 1 ^r(ai) = 0. The case k = 
t+l 
yields 
η 
η 
η^κ,-Κα,·)
- 1^· + Γι_ι ^ υ , τ ( α ί )
- 1 = 0, 
i=l 
i=l 
and since we already know that the second summand is zero and rt φ 0, 
this equation is equivalent to ]Γ)Γ=ι
 υ ι '
Γ (
α « )
_
1
α » = 0ι etc. Thus, we see that 
η 
η 
Σ~Γ~7
 = 0 
Σ
ν·
Γ(
α<)~
1αί
 = 0 
f o r f i = 0.
1 <-!· 
1 = 1
 X 
a' 
i = l 
• 
Corollary. The Goppa code determined by a polynomial r(x) of degree t 
has the following parity check matrix: 
1 
1 
1 
1 Ί 
r ( a i ) 
r ( a 3 ) 
" 
r ( a . ) 
αϊ 
aa 
OS 
α» 
r ( o i ) 
r ( a a ) 
'(«») 
' ' 
r(»„) 
-2 
«Ϊ 
• i 
<·(<·! ) 
r ( a a ) 
r ( a . ) 
· ' γ(ο,) 
„1-1 
fa 
« i -
• ί - ' 
r(a.) 
r(o 3) 
' 
Γ ( α Λ ) J 
Examples 
(1) The polynomial 
r ( x ) = χ
2 + χ + 1 

12.6. GOPPA CODES: ASYMPTOTICALY 
GOOD 
CODES 
251 
is irreducible over the field GF(S), 
since it has no zero (see Figure 2 
of 11.3). If we choose as a'a all elements of GF(8), we obtain a Goppa 
code of length 8 with the following parity check matrix: 
Η 
= 
1 
1 
1 
1 
1 
rTSj 
?ΓΠ 
7ζη 
7ζ&) 
··· 
u 
r7Ti 
7(Tj 
ΤζΓη 
· · · 
r 7 P 7 
That is, in binary: 
Η 
The code has precisely four code words: 
00000000, 
00111111, 
11001011, 
11110100. 
Its minimum distance is 5. Thus, we have obtained a double-error-
correcting (8,2)-code. Observe that this code is not cyclic. 
(2) BCH codes form a subclass of the class of Goppa codes: choose r(x) — 
x
2> and as ai's, consider all the nonzero elements of some extension F* 
of our alphabet F. That is, all the elements 
- 2 
- ( n - 1 ) 
where α is a primitive element of F*. The resulting Goppa code has 
the following parity check matrix: 
' 1 a
2 ' 
( a
2 )
2 ' 
· ·· 
( a " "
1 )
2 ' 
Η = 
1 
( a
2 )
2 * "
1 
• ·· 
( a " "
1 )
2 ' -
1 
1 a 
a
2 
This is, up to the order of rows, the parity check matrix of the i-error-
correcting BCH code (see 12.3). 

252 
CHAPTER 
12. BCH CODES 
Remarks 
(1) The binary Goppa code of length η determined by a polynomial of 
degree t over GF(2
m) 
has 
k >n — mt 
information symbols. In fact, the above parity check matrix has t rows 
in G F ( 2
m ) , i.e., tm rows in the binary. Thus, the number η — k of 
check symbols is smaller or equal to mt. 
(2) Let r(x) be an irreducible polynomial over some extension F* of the 
alphabet F. The Goppa code determined by r(x) and all the elements 
of F* is called irreducible. Example (1) is an irreducible Goppa code. 
(3) The minimum distance of a Goppa code determined by a polynomial of 
degree t is at least t + 1. This can be proved exactly as Theorem 12.3. 
However, a much better result holds for the binary irreducible codes: 
Theorem. The irreducible binary Goppa code determined by a polynomial 
of degree t corrects t errors. 
PROOF. Let r(x) be an irreducible polynomial of degree t over some ex-
tension F of Z 2 , F = { a\,..., 
a„ }. We are to prove that the Hamming 
weight q of an arbitrary nonzero coce word « i . . . v„ of the irreducible Goppa 
code fulfils g > 2i + 1. Let υ,·,, i>,a, ... , 
be all the l's of v. Since ν is a 
code word, we have 
(12.6.1) 
The following polynomial 
f(x) = (x - α,·,)(* - o l 3) · · · (x - a,,) 
(12.6.2) 
is not divisible by the polynomial r(x). In fact, since r(x) is an irreducible 
polynomial of degree t > 2, this follows from the unique factorization the-
orem (11.1). Thus, f(a) φ 0 in the algebraic extension Za[x]/r(x), and 
since r(x) is irreducible, / ( a ) has an inverse element (by Theorem 11.2). 
That is, there exists a polynomial g(x) with 
/(o)e(o) = 1 
(12.6.3) 
We can compute the formal derivative f'(x), 
see 11D: it is, obviously, 
the sum of the following polynomials: 
(χ - a„) · · • (χ - α,,.,Ηχ - o,, + 1) · · · (x - aik) 
(s = 1 
q). 

12.6. GOPPA CODES: ASYMPTOTICALY 
GOOD CODES 
253 
The latter can be more succinctly written as f(x)/x 
— α,·,. Thus, 
Substituting α for χ and multiplying both sides by g(a), we get, by (12.5.3) 
and (12.5.1), 
/ ' ( « ) * ( « ) = έ 
=
έ 
-
1 - = ° · 
J = l 
'• 
j=l 
'· 
This means that f'(x)g(x) 
is divisible by r(x). Now r(x) is irreducible and 
does not divide g(x) [since g(a) φ 0, see (12.5.3)]. It follows that r(x) 
divides /'(x). 
We know that / ' ( x ) is a perfect square, / ' ( x ) = o
2(x) (see Exercise HE). 
Since r(x) is an irreducible divisor of 6
2(x), it must be a divisor of b(x) (by 
the unique factorization theorem again). Thus, r
2 ( x ) divides b
2(x) = 
f'(x). 
Since f'(x) φ 0 (in fact, the degree of / ' is q — 1 and q > 1), it follows that 
the degree of f'(x) is larger or equal to that of r
2(x): 
q - 1 > 2i. 
• 
Examples 
(3) The polynomial r(x) = x
3 + x + l has three zeros in GF(8) = Z2[x]/mod 
χ
3 + χ + 1. Consequently, it has no roots in GF(32) 
[GF(32) is not an 
extension of GF(8)]. 
It follows that r(x) is irreducible over 
GF(32). 
The corresponding irreducible Goppa code has the following parame-
ters: 
η 
= 
32, 
k 
> 
3 2 - 5 x 3 = 17 
(actually, Jb = 17), 
d 
> 
7. 
This is a triple-error-correcting (32,17)-code. Compare it with the 
(31,16) triple-error-correcting BCH code whose extension is a (32,17)-
code with d = 8. 
(4) So far, we have considered only binary polynomials r(x). However, r(x) 
can have coefficients in the extended field. For example, r(x) = x
2 + 
αχ + 1 is an irreducible polynomial over GF(8) of Figure 2 in 11.3. The 

254 
CHAPTER 
12. BCH CODES 
corresponding irreducible Goppa code has the following parity check 
This is another triple-error-correcting binary (8,2)-code. 
Concluding Remarks 
(1) Goppa codes form a generalization of BCH codes. The decoding (which 
we do not describe here) can also be performed by a technique analogous 
to that for BCH codes. A fast decoder has been described by Patter-
son (1975). 
(2) No short Goppa code is known today which has parameters improving 
those of short BCH codes (as listed in Appendix B). 
(3) A remarkable property of Goppa codes is that they are "asymptotically 
good". Recall that Shannon's Fundamental Theorem (4.8) promises 
codes Ki (i = 1, 2, ... ) with information rates ^ converging to some 
positive number (the capacity) while the error probability converges to 
zero. Let di denote the minimum distance of the code Ki. We call the 
sequence of codes K\, K2, Ka, ... asymptotically good provided that 
both ^ and ^ converge to positive limits (with increasing i). In other 
words, the codes K\, K2, Ka, ... 
are asymptotically good provided 
that, for a given information rate R > 0 and a given percentage Ρ > 0 of 
correctable symbols per code word, we have £»· —• R and jjf —» P. The 
proof of the following statements can be found, e.g., in Mac Williams 
and Sloane (1981): 
(a) BCH codes are asymptotically bad, that is, there does not exist 
any asymptotically good sequence of BCH codes. (Consequently, 
BCH codes cannot ever be used for a realization of the parameters 
of Shannon's Fundamental Theorem.) 
(b) Goppa codes are asymptotically good: there exists an asymptoti-
cally good sequence of irreducible binary Goppa codes. 
However, the proof of (b) is purely existentional. It is not known how 
to produce irreducible polynomials giving asymptotically good Goppa 
codes. 
matrix: 
Η = 
1 α
6 
1 
α
5 
α
6 
a
5 
α 
α 
0 
α
6 
α
2 
α 
α
2 
α
2 
α
6 
1 

EXERCISES 
255 
Exercises 
12A 
Using the BCH code of Example (1) in 12.2, decode the words 
(1) 00000111000000, 
(2) 11000100000001. 
1 2 B 
Describe the binary double-error-correcting code of length 11. 
12C 
What is the information rate of the double-error-correcting BCH 
code of length 31 
(1) over Z 2 ? 
(2) over Z 3 ? 
12D 
Find the generator and parity check polynomials for the binary 
triple-error-correcting BCH code of length 
(1) 15, 
(2) 31. 
Encode the information word consisting of all l's. 
12E 
Prove the independence theorem (see 12.2) for the i-error-correcting 
BCH codes. 
1 2 F 
Using Reed-Solomon codes, construct a binary (75,28)-code cor-
recting burst errors of length 11 and a (75,40)-code correcting burst errors 
of length 6. 
12G 
For r(x) = x
2 t, show that ^ 
= Σ**=ι a
-
J V
-
1 . Use this to verify 
again that the Goppa code determined by x
2t 
and all nonzero elements 
a
0 , a
- 1 , ... , a
- * "
- 1 ) of the extension field is precisely the BCH code. 
12H 
Find the minimum distance of the binary Goppa code determined 
by r(x) = x
2 + 1 and by the elements 0, z, z
2 of GF(4). 
Compare this with 
Theorem 12.5. 
121 
Find a parity check matrix of the irreducible binary (16,8)-Goppa 
code given by r(x) = χ
2 + χ + a
3, where a is primitive in 
GF(16). 

256 
CHAPTER 
12. BCH CODES 
12J 
Verify that the polynomial χ
2 + χ + 1 is irreducible over 
GF(32). 
Find the parameters and parity check matrix of the corresponding irre-
ducible Goppa code. 
Notes 
Binary BCH codes were discovered by Bose and Ray-Chaudhuri (1960) 
and Hocquenhem (1959). They were generalized to other alphabets by 
Gorenstein and Ziegler (1961), who also observed that the codes earlier 
discovered by Reed and Solomon (1960) represent a special case. Goppa 
codes were published by Goppa (1970). 

Chapter 13 
Fast Decoding of BCH 
Codes 
We have described two decoding techniques for the binary double-error-
correcting BCH codes: error trappnig (10.5) and quadratic equations (12.2). 
For the general BCH codes, several decoding methods are known. 
We 
present a decoding algorithm based on the procedure of finding the great-
est common divisor of two polynomials (the famous Euclidean algorithm, 
which we first introduce in detail). The decoder we present is fast, and of 
all the known fast decoders, it is the simplest one to understand. Theo-
retical results show that the (more complicated) Berlekamp decoder, see 
Chapter 7 in Berlekamp (1968), is faster for very large lengths n. However, 
the speed of the two decoders is comparable for all η < 10
6. 
Throughout this chapter, we assume that a ί-error-correcting BCH code 
of length η over an arbitrary finite field F is used. The code words are the 
polynomials v{x) satisfying ν(β*) = 0 for i = 1, 2, . . . , 2t. We send a code 
word ν and receive a different word w of Hamming distance 
d(v, w) = ρ < t. 
In other words, the actual number ρ of corrupted symbols lies between 1 
and t. (The case ρ = 0 need not be considered: whenever the syndrome 
is 0, we stop decoding since we have received a code word.) It is our aim 
to find the error-pattern word 
e = w — v. 
257 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

258 
CHAPTER 13. FAST DECODING OF BCH CODES 
13.1 
Error Location and Error Evaluation 
Our task is to determine the error-pattern word β from the received word w. 
Since e has Hamming weight ρ (= the number of actual errors made), it is 
characterized by its ρ nonzero components e<,, ej 3, ... , e^. For each k = 
1, 2, ... ,p, put 
at = β'" 
(error-location number), 
(13.1.1) 
bit = e, t 
(error-evaluation number). 
(13.1.2) 
It is sufficient to determine the 2p elements a\, ... ,ap and b\, ... ,bp: we 
then know the location of errors: 
ε,· φ 0 
iff /?* is one of οι, ... , ap, 
and in the case e, φ 0, we can evaluate the error: 
e, = bit 
whenever β' = α*. 
More succinctly: 
bk 
i{?=ak, 
0 
if β' φ 
aua2,...,ap. 
(13.1.3) 
In order to find the error-location numbers, it is sufficient to determine 
the following error-location polynomial: 
σ(χ) = (1 - αι*)(1 - α 2χ) · · · ( ! - apx). 
(13.1.4) 
It has zeros in aj"
1, a j
1 , ... , ap
x. 
Thus, once we know <r(x), we know the 
error-location numbers as inverses of all the zeros of σ(χ). [Since we work 
within a finite field, the zeros can be found by brute force: compute all the 
values of σ(χ).] 
Once the error-location numbers have been established, the error-eval-
uation numbers can be found by means of the following error-evaluation 
polynomial: 
ρ 
«(») = Σ Μ
1 " «ι») - (Ι - α*-ιχ)(1 - a i + 1 x ) · -(1 - apx) 
(13.1.5) 
k = l 
In fact: 
Proposition. The error-evaluation numbers are given by 
* i = - o j W ( a ;
1 ) [ < T ' ( a i -
1 ) ] -
,
> 
(13.1.6) 
where <r'(x) is the formal derivative (11C) of σ(χ). 

13.1. 
ERROR LOCATION AND ERROR 
EVALUATION 
259 
PROOF. From (13.1.5), it clearly follows that for each j = 1, 2, . . . , p, 
ω(α~
ι) 
= 6,(1 - 
aiaJ
l) 
· · (1 - α , - ^ χ ΐ 
- aHlaJ
l).. 
.(1 - 
a,aj
l). 
On the other hand, the formal derivative of the polynomial in (13.1.4) is, 
obviously, the sum of the polynomials: 
- a * ( l - aix)(l 
- a 2 x ) · · (1 - a t_ix)(l - at+ix) 
· (1 - 
apx) 
for k = 1, 2, . . . , p. Thus, 
σ ' ί α Γ
1 ) = _ e j ( l - aiu"
1) · · · (1 - α , - κ ^ χ ΐ - a^aj
1) 
· · (1 - α ρ α -
χ ) . 
Dividing α»(a"
1) by σ'(α~
ι), 
we get (13.1.6). 
• 
Remark. In subsequent sections, we will see how to compute the poly-
nomials σ(χ) and ω(χ) from the syndrome 
s(x) of the received word w(x) 
defined by 
s(x) = s0 + six Η 
(- s 2 ( _ i x
2 t - 1 , 
where s< = w(a'). 
(13.1.7) 
Then the decoding is easy to conclude: we find the error locations at as in-
verse zeros of the polynomial σ(χ), and the error evaluations bt by (13.1.6). 
The error-pattern word is then given by (13.1.3). 
We now list some crucial properties of the known polynomial s(x) and 
the unknown polynomials σ(χ) and ω(χ) to be used later. Recall from 11.2 
that the expression 
f(x) 
= g(x) 
(mod x') 
means that f(x) — g(x) is divisible by x'; in other words, that the polyno-
mials / ( x ) and g(x) have the same coefficients at 1, x, x
2 , . . . , x '
- 1 . 
Theorem 
(1) ω(χ) = σ(χ)β(χ) 
(mod x
2 ' ) . 
(2) ω(χ) has degree smaller than t, and σ(χ) has degree at most t. 
(3) β(χ) φ 0 
(mod x'), i.e., β< φ 0 for some t < t. 
PROOF. 
(1) The received word w has the same syndrome as the error-
pattern word e: in fact, w —e is the code word sent, thus, tu(a') — e(ar') = 0 
for « = 1 , 2 
2t (by the definition of the BCH code). Therefore, 
8 i = e(a*) = eit a"
1 + ••• + 
e^a"'. 

260 
CHAPTER 
13. FAST DECODING OF BCH CODES 
By (13.1.1) and (13.1.2), 
Si = bia\ + ··· + 
bpa'p. 
(13.1.8) 
On the other hand, we can express the sumands of (13.1.5) as bka(x)/(l 
— 
α*χ): 
t=i 
' akx 
Using formal power series (11G), we have 1/(1 - akx) = Y^oio-kx)', 
thus, 
ρ 
u(x) 
= a(x)J2bkY^(akx)' 
k=l 
i=0 
= 
έ 
******* 
ι'=0 k-l 
oo 
= 
o-(x)'^2(bla\+b2a2+--rbpap)x
i. 
1=0 
Comparing the last expression with (13.1.8), we see that ω(χ) and σ(χ)β(χ) 
have the same coefficients at x' for all t = 0, 1, ... , 2t — 1. In other words, 
ω(χ) 
Ξ < T ( X ) S ( X ) 
(mod 
x
2'). 
(2) By assumption, the number ρ of actual errors is at most t. The 
degree of σ(χ) is ρ and that of u>(x) is ρ - 1. 
(3) Suppose that, to the contrary, S{ - 0 for all i = 0, 1, 
Since ρ < t, we see from (13.1.8) that the following equations 
t - 1. 
1 
1 
1 
α Γ
1 
α Γ
1 
nP-l 
Γ *i 1 
• 0 " 
0 
63 
= 
0 
. 0 . 
hold. 
This is impossible since the matrix has a nonzero Vandermonde 
determinant [see Remark (2) in 12.3] and the ό,-'s are nonzero. 
• 
13.2 
Euclidean Algorithm 
More than two thousand years ago, Euclid formulated the following algo-
rithm for finding the greatest common divisor (GCD) of two numbers, a0 

13.2. 
EUCLIDEAN 
ALGORITHM 
2 6 1 
and αχ: Suppose a0 > αχ; perform the integer division of aQ by αχ, and 
denote the quotient by qx and the remainder by a 2: 
ao = ςχαχ + a2, 
ai < ax. 
If α 2 = 0, then αχ = 000(00,0!). If α 2 φ 0, then perform the integer 
division αχ 4- α 2 and denote the quotient by g2 and the remainder by a 3: 
αχ = q2a2 + 03, 
α 3 < α 2, 
etc. Since αχ > α 2 > 03 > 
· ·, we eventually get α*+ι = 0 for some Jb, and 
then o* = 
GCD(ao,ax). 
The same procedure can be used to find the greatest common divisor 
of two nonzero polynomials, a 0 ( x ) and ax(x), 
over any field. By defini-
tion, this is a polynomial of the highest degree which divides both oo(x) 
and aj(x). Assuming that degreeao(x) > degreeoj(x), we can define fur-
ther polynomials a 2(x), 03(1), ... and qi(x), q2(x), 
· · · by the following 
recursion: 
a t _ i ( x ) = g t(x)a f c(x) + a t + i ( x ) , 
d e g a l + i ( x ) < dega t(x). 
(13.2.1) 
In other words, qt(x) is the quotient and α* +ι(χ) is the remainder of the 
division of α*_ι(χ) by α*(χ). Since the degrees of ai(x), a 2(x), ... are 
decreasing, we eventually get α*+ι(χ) = 0. Then at(x) is the greatest 
common divisor of ao(x) and αι(χ). [In fact: (1) α*(χ) divides at_i(x) since 
the remainder of that division is αι+ι(χ) = 0. It follows that α*(χ) divides 
α*_ 2(χ) = g/t_i(x)afc_i(x) + a c(x); hence, it divides a»_3(x), etc. We 
conclude that α*(χ) divides both aj(x) and ao(x). (2) Any polynomial 6(x) 
which divides both ao(x) and aj(x) divides the polynomial α 2(χ) = a 0(x) — 
0ι(χ)θ!(χ). Thus, 6(x) divides 03(1) = ai(x)—g 2(x)a 2(x), etc. We conclude 
that 6(x) divides a*(x) and, thus, it has a degree smaller or equal to that 
of a t(x).] 
Example. Find a greatest common divisor of the polynomials χ
2 + χ and 
x
2 + 2 over Z 3 . 
In the first step, we divide αο(χ) = χ
2 + χ by αχ(χ) = χ
2 -f 2 and obtain 
flj(x) = 1 
and 
α 2(χ) = χ + 1. 
Then we divide ai(x) by a 2(x). The remainder is a 3 ( x ) = 0; thus, a 2(x) = 
χ + 1 is the greatest common divisor. 
Observe that 2x + 2 is the greatest common divisor too. (The concept 
of greatest common divisor is only unique up to a scalar multiple, see 13A.) 

262 
CHAPTER 13. FAST DECODING OF BCH CODES 
Remarks 
(1) Define polynomials uk(x) and vk(x) by the following recursion, using 
the quotients qk(x) of (13.2.1): 
u o(x) = 0, 
u i ( x ) = l , 
u i + i ( x ) = qk(x)uk(x) 
+ 
uk-i(x), 
t> 0(x)=l, 
i>i(x) = 0, 
vk+i(x) 
= qk(x)vk(x) 
+ t>t_i(x). 
Then each of the steps of the Euclidean algorithm is the following linear 
combination of the two given polynomials ao(x) and aj(x): 
«*(*) = (-l)
k[vk(x)a0(x) 
- « » ( * ) « , ( * ) ] . 
(13.2.3) 
In fact, this trivially holds for k = 0, 1, and for higher Jb's, it readily 
follows by mathematical induction. 
In the above example we have 
uo(x) = 0, 
t > 0 ( x ) = l , 
« ι ( χ ) = 1 , 
υι(χ) = 0, 
u 2(x) = 1, 
υ 2(χ) = 1, 
u 3(x) = x + l , 
υ 3(χ) = χ. 
It is easy to check that 
α 2(χ) = χ + 1 = ( - l )
2 [ ( x
2 + x) - (x
2 + 2)], 
β 3 ( * ) = 
0 = ( - l )
3 [ x ( x
2 - r x ) - ( x + l ) ( x
2 + 2 ) ] . 
(2) If two polynomials aa(x) and ai(x) are relatively prime, i.e., have the 
greatest common divisor 1, then each polynomial can be expressed as 
a linear combination of those polynomials. That is, for each polyno-
mial / ( x ) , there exist polynomials /o(x) and f\(x) with 
/ ( * ) = /o(x)a 0(x) + 
h(x)ai(x). 
In fact, there exists ib such that ak(x) 
is a nonzero scalar [i.e., the 
greatest common divisor of a 0(x) and ai(x), see 13A]. For this scalar, 
say, c, we have by (13.2.3) polynomials po(x) and pi(x) with 
c = po(x)a 0(x) + p!(x)ai(x). 
Put fi(x) = c-
l
Pi{x)f{x) 
for t = 0, 1. 
(3) The polynomials uk(x) and t/*(x) above fulfill 
«*(xK+i(x) - vk(x)uk+i(x) 
- ( - 1 ) *
+ 1 . 
(Verify by induction.) It follows that uk(x) and υ*(χ) are relatively 
prime: every common divisor divides (—1)*
+ 1. 

13.3. 
THE DECODING 
ALGORITHM 
263 
(4) The quotient g*(x) ofα*_ι(χ) -j- α*(χ) has the following degree: 
dego t(x) = dega t_i(x) - deg a t ( x ) . 
It is easy to verify by induction that 
degu t(x) = dega 0(x) - dega t_i(x). 
13.3 
The Decoding Algorithm 
Using the <-error-correcting BCH code and receiving a word w(x) which is 
not a code word, we correct it as follows: 
Step I. Compute the syndrome β,- = u>(a') of the received word w(x). [By 
assumption, s(x) φ 0.] 
Step II. Perform the Euclidean algorithm with the polynomials 
a0(x) = x
2t 
and 
oi(x) = s(x). 
(13.3.1) 
Denote by Jb the first index such that a*(x) has degree smaller than t. 
That is: 
dega f c(x)<< 
and 
dega k-i(x) > t. 
(13.3.2) 
For this k, put d = [«t(0)]
 
1 and define 
a(x) = duk(x) 
and 
u(x) = ( - l ) *
+ 1 d a t ( x ) . 
(13.3.3) 
We will prove below that these are the actual error-locator and error-
evaluator polynomials, respectively. 
Step III. Correct those symbols Wi, i' = 0, 1, ... , η — 1, for which σ(χ) 
has a zero in β ~ ' . The correct value is 
[This is the necessary correction by (13.1.3) and (13.1.6).] 
Remark. The second step requires that (1) there exists ib such that the 
degree of α*(χ) is lower than t, and (2) ut(0) φ 0. It follows from the 
next theorem that both are true whenever the actual number ρ of errors 
is smaller or equal to t. Thus, if the second step cannot be performed, we 
stop decoding and announce that an incorrectable error has been made. 

264 
CHAPTER 13. FAST DECODING OF BCH CODES 
Theorem. Under our standing 
assumption 
that ρ < t, the 
polynomials 
of (13.3.3) are the true error-location 
and error-evaluation 
polynomials, 
respectively. 
PROOF. Throughout the proof, we use the symbols σ(χ) and w(x) for the 
true polynomials, denned by (13.1.4) and (13.1.5), respectively. Observe 
that in step II such k as required exists: the greatest common divisor of x
2 ' 
and s(x) certainly has the form x* for some i. However, s(x) is not divisible 
by x* [see Theorem (3) in 13.1]; thus, i < t. In the Euclidean algorithm, 
the final step is the polynomial ako(x) 
= χ', and since i < t, we can choose 
(the largest) Jb with degree α*(χ) < t. We are going to prove that there is a 
scalar d such that 
ff(x) = du t(x) 
and 
ω(χ) = d ( - l ) *
+ 1 a * ( x ) . 
(13.3.4) 
This will conclude the proof: it remains then to verify that d = [u*(0)] 
\ 
but by inserting 0 into (13.3.4), we see that σ(0) = 1 = 
duk(0). 
To prove (13.3.4), we first show that the polynomials 
W(x) = uk(x) 
and 
ω(χ) = ( - l )
t + 1 a t ( x ) 
have the properties (1), (2) of Theorem 13.1. In fact, (1) follows from the 
equation (13.2.3): we have 
U(x) 
= 
(-l)W(-l)*[vk(x)x»-W(x)s(x)} 
= 
<f(x)s(x) + x
2«[-v t(x)] =W(x)s(x) 
(modx
2 <). 
Concerning (2), the degree of ω(χ) is smaller than t by the choice of ib, 
and for <f(x), we use Remark (4) of 13.2: 
deg^(x) = 2i - degat_i(x). 
By the choice of ib, deg<n_i(x) > t; thus, degtf(x) < t. 
We conclude that 
σ(χ)ω(χ) = σ(χ)ω(χ). 
(13.3.6) 
In fact, both sides are polynomials of degree smaller than It, and since 
property (1) of Theorem 13.1 holds for both of the pairs of polynomials, we 
have 
ω
(
χ
) 
_ 
/ 
χ _ " ( Χ ) 
, 
, 
3
Ι . 
more precisely, ω(χ)σ(χ) = ω(χ)σ(χ) 
(mod x
2 ' ) . This establishes (13.3.6). 
It follows that σ(χ) is a divisor of <?(x): in fact, <r(x) can be factored 

13.3. 
THE DECODING 
ALGORITHM 
265 
into linear polynomials 1 — akx [see (13.1.4)] none of which divides ω(χ) 
[because ω ( χ ) does not have a zero in ajf
1, see Proposition 11.1]. By the 
unique factorization theorem (11.1), since <r(x) divides σ(χ)ω(χ), 
it must 
divide <r(x). Put 
6
(
x
) " 
WY 
Then from (13.3.6), we get 
σ ( χ ) — 6 ( χ ) σ ( χ ) 
and 
ω ( χ ) = 
6 ( χ ) ω ( χ ) . 
The proof of (13.3.4) will be concluded when we show that 6(x) is actually 
a nonzero scalar: just let d be the inverse of that scalar, then σ ( χ ) = 
da(x) 
and ω(χ) = 
du(x). 
It is sufficient to verify that b(x) divides both uk(x) and vk(x): 
since 
the latter two polynomials are relatively prime [see Remark (3) of 13.2], 
it follows that 6(x) is a scalar. The polynomial 6(x) certainly divides 
6 ( χ ) σ ( χ ) = σ ( χ ) = ut(x). To prove that it also divides Ufc(x), use (13.3.5): 
x
2tVk{x) 
= 
σ ( χ ) β ( χ ) — ω ( χ ) 
= 
σ ( χ ) β ( χ ) — 6 ( χ ) ω ( χ ) . 
By Theorem (1) of 13.1, we have a polynomial c(x) such that ω(χ) = 
σ ( χ ) β ( χ ) + x
2*c(x). Then 
x
2ivk(x) 
= 
σ ( χ ) β ( χ ) - b(x)a(x)s(x) 
- 6(x)x
2'c(x) 
= 
σ ( χ ) β ( χ ) - σ ( χ ) β ( χ ) — 6(x)x
2 ,c(x) 
= 
-6(x)x
2'c(x). 
Dividing by x
2', we see that vk(x) 
is divisible by b(x). 
Thus, 6(x) is a 
scalar.
 
n 
Example. Suppose we use the binary double-error-correcting BCH code 
of length 15, and we receive 
001000100000000 = x
2 + x
6. 
Step I. The syndrome s, =
 
w (
z * ) = °
2 ' +
 
a
6
i 
i e computed in G.F(16) 
(Appendix A) as follows: 
βο 
= 
0, 
«ι 
= 
α
3, 
β 2 
= 
a
6 , 
S3 
= 
α
2. 

266 
CHAPTER 
13. FAST DECODING OF BCH CODES 
Thus, 
s(x) = a
2 x
3 + a
6 x
2 + a
3 x . 
Step II. The Euclidean algorithm with ao(x) = x
4 and αι(χ) = s(x), 
performed until degree α*(χ) < 2. 
The division of ao(x) by aj(x) yields 
<7!(x) = a
1 3 x - ( - a
2 
and 
o 2(x) = a
1 0 *
2 + a
5 x . 
Since a 2(x) has degree > 2, we further divide a\(x) by a 2(x): 
g 2(z) = a
7 x + a
9 
and 
aa(x) = *· 
Thus, in (13.3.3), we have ib = 3. Next, we compute u 3(x): 
u 0(x) 
= 
0, 
uj(x) 
= 
1, 
"2(1) 
= 
« i ( * ) = ot
13x + a
2 , 
u 3(x) 
= 
u2(x)qi(x) 
+ ui(x) = a
5 + χ + α
1 2. 
We see that in (13.3.3), d = u 3 ( x )
_ 1 = a
-
1
2 = a
3 and the error-
locator polynomial is 
σ(χ) = du 3(x) = a
8 x
2 + a
3 x + 1. 
Step III. We find the zeros of σ(χ) (by brute force): they are a
9 , a
1 3 (or 
a
- 6 , a "
2 ) . We thus correct u>2 and we'- the word sent is 0. 
Exercises 
13A 
Prove that if 6(x) is a greatest common divisor of two polynomials, 
then 
(1) for each scalar c φ 0, the multiple c6(x) is also a greatest common 
divisor of those polynomials, and 
(2) each of the greatest common divisors of those two polynomials is a 
scalar multiple of 6(x). 
[Hint: As observed above, in the Euclidean algorithm, every divisor of oo(x) 
and oj(x) divides each α*(χ).] 

NOTES 
267 
1 3 B 
Let a0(x) and αι(χ) be relatively prime polynomials. Find an algo-
rithm which produces polynomials po(x) and pi(x) satisfying po(x)ao(x) + 
ρι(χ)αχ(χ) = 1. [Hint: In the Euclidean algorithm above, there exists Jb 
with a t ( x ) of degree 0. Use (13.2.3).] 
13C 
Analogously to 13B, find an algorithm which, for relatively prime 
numbers do and αχ, produces numbers po and pi with po<>o + pi<ii = 1. 
Conclude that for each number η (= αϊ), every element of Z„ relatively 
prime with η has an inverse element which can be found by the Euclidean 
algorithm. 
13D 
Suppose we use the binary triple-error-correcting BCH code of 
length 15. Decode 101011001000000. 
1 3 E 
Suppose we use the double-error-correcting BCH code over Z 3 of 
length 8. Decode 00120000. 
Notes 
The first decoder for (the binary) BCH codes was proposed by Peter-
son (1960). 
The decoder based on the Euclidean algorithm is due to 
Sugiyamaet al. (1975). 

Chapter 14 
Convolutional Codes 
Whereas linear codes encode information symbols without memory (i.e., 
each code word only depends on the present Jb-tuple of information sym-
bols), there is an important error-correcting technique based on coding 
with memory: the convolutional codes. A convolutional code breaks the 
input message into frames of length ib and encodes them into code frames 
of length n, but each code frame depends not only on the last information 
frame, but also on m preceding information frames. 
We first describe convolutional codes by means of generator polynomials 
and generator matrices, and then we present an error-correcting procedure 
called the Viterbi algorithm. 
14.1 
Linear Codes and Convolutional Codes 
Before defining convolutional codes, let us take a new, global look at linear 
codes. We have so far considered a linear (n.ib)-code locally: each word u 
of length Jb is encoded into a code word ν = uG of length n. However, we 
can also view a linear code as a function which transforms source messages 
(of any length iib for j = 1, 2, 3, . . . ) into code messages (of length in). For 
example, the even-parity code of length 4 is a function given as follows: 
U0U1U2U3U4W5 · ··
 
1 — • 
« ο « ι « 2 α ο « 3 « 4 « 5 α ι · 
· , 
βθ = «0 -f Ul + « 2 , αϊ = « 3 + «4 + « 5 , 
Such a function is best expressed by means of polynomials (because these 
correspond to words, except that the length is not specified). Denote by 
u(z) = ua+uix 
+ U2X
2-\ 
the polynomial representing the source message 
and by v(x) = » 0 + "ι' + v2x
2 
+ ·•• the encoded message. Then a linear 
269 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

270 
CHAPTER 14. CONVOLUTIONAL 
CODES 
code is a function C assigning to each polynomial u(x) the encoded poly-
nomial v(x) = C[tz(x)]. It is obvious that this function has the following 
properties: 
(1) C is linear, i.e., 
C[u(x) + «'(*)] = C[u(x)] + 
C[u'(x)), 
C[tu(x)] 
=tC[u(x)]. 
(2) C is time-invariant, i.e., by delaying the source message for ib shifts 
[which is expressed by the polynomial x*u(x)], we only delay the 
reeponce for η shifts: 
C[x*u(x)] 
=x
nC[u(x)]. 
(3) C has no memory, i.e., the response to a given t-tuple does not depend 
on the preceding Jb-tuples. In view of the time-invariance, this is 
equivalent to the following: by changing the first Jb symbols of the 
source message u, we can only change the first η symbols of the 
response v. 
Conversely, every function C on polynomials satisfying (l)-(3) expresses 
a linear code. In fact, denote by v<(x) the response to x* for i = 0, 1, ... , ifc. 
By (3), the degree of v,(x) is less than n; thus, we have Jb words v 0, νχ, 
... , v*_i of length n. The matrix 
G = 
v 0 
Vl 
V * - l 
defines a linear code. The usual ecoding 
u ι—• [u 0tii.. .tifc_i]G = 52 ti,Vj 
t - i 
1=0 
corresponds to the function C (applied to polynomials of degree less than Jb): 
k-l 
t - 1 
C[uo + u,x + -.- + u t_ix*-
1] =^«,·ί7[χ
,'] =]Γ\,·«<(χ). 
ι=0 
1=0 
The time-invariance and linearity (of both the function C and the linear 
code defined by the matrix G) then guarantee that C is the global descrip-
tion of the linear code generated by G. 

14.1. 
LINEAR CODES AND CONVOLUTIONAL 
CODES 
271 
By dropping the condition (3) above, we get the concept of convolutional 
code: 
Definition. Lei F be a finite field (the source alphabet). By a convolu-
tional (n, Jfc)-code is meant a function C assigning to each polynomial u(x) 
over F a (response) polynomial C'[u(x)], which is linear: 
C [ u ( x ) + «'(*)] = C[ti(x)] + C[u'(x)], 
C[tu(x)}=tC[u(x)], 
and time-invariant: 
C[x
ku(x)] 
= x
nC[u(x)]. 
Example. 
(1) Consider the following-shift register sequence: 
input 
•«> 
It consists of two shift registers, three binary adders, and a two-bit output 
buffer which emits two bits in every shift. The input-output function of this 
shift-register sequence is linear (because only linear elements are involved) 
and time-invariant: a delay of 1 shift at the input causes a delay of 2 shifts 
at the output. Thus, we get a convolutional (2, l)-code. 
The response to a single 1 is 11 at the first shift, 11 at the second shift, 
and 01 at the third shift (followed by 0000...), thus, 
C[l] = 111101 = 1 + χ + x
2 + x
3 + x
5 . 
Consequently, by time-invariance, 
C[01] = 
00111101, 
C[001] 
= 
0000111101, 
etc. By linearity, the single response C[l] determines the whole code. For 
example, the response to 101 is 
CflOl] = C[l] + <7[001] = 1111101101. 
Convolutional (n, \)-codes. In the special case t = 1, the whole code is 
determined by the single response to 1. We call 
C[l] = ffo(x) 

272 
CHAPTER 14. CONVOLUTIONAL 
CODES 
the generator polynomial of the convolutional (n, l)-code. Observe that, by 
the time-invariance, 
C[x] 
= 
x
ng0(x), 
C[x
2] 
= 
x
2 n o 0 ( * ) , 
etc. Thus, for each polynomial u(x) = UQ + u\x + u 2 x
2 + • · ·, we have 
C [u(x)] 
= 
u0C[l] + UlC[x] 
+ u7C [x
2] + · · · 
= 
u 0o 0(x) + uix
ng0(x) 
+ u 2x
2 noo(x) + · · · 
= 
(u 0 + uix
n + u
2 x
2 n + - ) i o ( i ) 
= 
u{x
n)g0(x). 
We conclude that a convolutional (n, l)-code is determined by its generator 
polynomial go(x) via the following rule: 
u(x) ι—• 
u(x
n)g0(x). 
Conversely, for every polynomial go(x), this rule defines a convolutional 
(n, l)-code. 
Remark. 
(1) Generator polynomials play here a different role from that 
in the realm of cyclic codes. For example, every polynomial go(x) generates 
a convolutional (n, l)-code by the rule 
u(x)i—• 
U ( X " ) O Q ( X ) . 
For each polynomial go(x) — <*o + "ι* + 
r o$x', there is a simple rule 
of how to describe a circuit which realizes the encoding. For simplicity, we 
will describe it just in the binary case. Use the first η summands of go(x) 
to decide whether the individual η bits of the output buffer are connected 
directly with the input (if α,· = 1, then the ith output bit is connected; if 
a; = 0, then it is not). The next η summands decide whether the individual 
η bits are connected directly with the output of the first shift-register stage, 
etc. Thus, we write 
9o(x) 
= 
(αο + αιχ+••• + a„_jx"~
1) 
+ (a„x
n + o n + 1 x
n + 1 + · · · + oat,.!*
2""
1) 
+ · · · + ( < W
m
n + a m n + i *
m n + 1 + · • · + a
m
n
+
n _
1 x
m
n
+ " -
1 ) . 
The number m + 1 of parentheses indicates that we need a shift-register 
sequence of length m. The (i + l)st set of parentheses determines the 
connection of the output of the ith shift-register stage with the individual 
bits of the output buffer. 

14.1. 
LINEAR CODES AND CONVOLUTIONAL 
CODES 
273 
Examples 
(2) Let us sketch an encoder of the convolutional (2, l)-code with the gen-
erator polynomial g(x) = 1 + χ + χ
2 + χ
5 = (1 + x ) + ( x
2 + 0) + (0 + x
5). 
The first expression in parentheses, 1 + x, means that both bits of the 
output buffer are directly connected with the input; the next expres-
sion in parentheses, x
2 + 0, tells us that only the first output bit is 
connected with the first memory; and, finally, from 0 + x
8 , we see that 
only the second output bit is connected with the second memory. See 
Figure 1. 
Figure 1: An encoder of a convolutional (2, l)-code 
(3) Consider the convolutional (3, l)-code with go(x) = 1 + x
2 + x
3 + x
8 + 
x
6 + x
7 + x
1 2 + x
1 3 + x
1 4 . We rewrite the generator polynomial as follows: 
g0(x) = (1 + 0 + x
2 ) + ( x
3 + 0 + x
5 ) + ( x
6 + x
7 + 0) + ( x
1 2 + x
1 3 + x
1 4 ) . 
The encoder is shown in Figure 2. 
input 
Figure 2: An encoder of a convolutional (3, l)-code 
Remark. From the generator polynomial, we can form a generator matrix 
analogously to the case of cyclic codes: Let G be the matrix whose first 

274 
CHAPTER 
14. CONVOLUTIONAL 
CODES 
row is go(r), second row is x
ngo(x) 
(i.e., the first row shifted η times to the 
right), third row is x
2ngo(x), 
etc. The number of rows of G is not fixed: for 
each »' = 1, 2, 3, ... , we have an t χ η generator matrix serving to encode 
messages u = UQUI .. . u,_i of length t. The encoding rule is as usual: 
u I — • [u 0«i · · - U | - l ] G. 
For example, let C be the binary convolutional (3, l)-code with generator 
polynomial oo(x) = 1 + x + x
3 + x
4 + x
5 + x
8- For messages of length 3, we 
have the following generator matrix: 
G = 
1 
1 0 
1 1 0 
0 1 
(where the missing entries are all zeros). The response of that code to 101 
is 
[ 1 0 
1 ] G = [ 1 1
0
0
0
1
0
0
0
1 
1 0 0 0 1 ] . 
14.2 
Generator Polynomials and Generator 
Matrices 
In the preceding section, we have seen how a convolutional (n, l)-code is 
described by its generator polynomial. We now present an analogous de-
scription of convolutional (n, Jb)-codes. Let us begin with an example. 
Example. Consider the two shift-register sequences in Figure 3 encoding 
input 
output 
Figure 3: An encoder of a convolutional (2,2)-code 

14.2. 
GENERATOR 
POLYNOMIALS 
275 
convolutional (2, l)-codes, which only share the output buffer. At each clock 
time, the input buffer is filled by two bits, giving the inputs to both of the 
shift-register sequences. The two outputs are then added, and the resulting 
contents of the output buffer is sent out. The input-output function of this 
circuit is linear and time-invariant: a delay of the input by two shifts results 
in a delay of the output by 2 shifts. That is, we obtain a convolutional 
(2,2)-code. 
Observe that the response to 10 is 110101 (the upper shift-register se-
quence is activated alone), and the response to 01 is 010111 (activating the 
lower sequence alone). That is, 
C[l] 
= 
1 + χ + x
3 + x
s = 
g0(z), 
C[x] 
= 
χ + x
3 + x
4 + x
6 = 
gi(x). 
By the time invariance, we can compute other responses: 
C[x
2} 
= 
x*g0(x) 
C[x
3] 
= 
x*9l(z) 
C[x
4} 
= 
x
4g0(x) 
C[x*] 
= 
x*gi(x) 
Thus, the general response can be expressed as follows: 
C[u0 
+ uxx + u 2*
2 + U 3 «
3 + «ι»
4 + · · ] = 
= 
(u 0 + ti 2C[x
2] +tt4C[x
4] 
+ ...) + («,+ 
u3C[x
3] 
+ u 5C[x
5] + · · ·) 
= 
uW(x
2)g0(x) 
+ 
uW(x
2)9l(x). 
Here u^°\x) and υί
ι\χ) 
are the polynomials representing the even and odd 
symbols of the word Μ Ο « Ι « 2 « 3 · · ·, respectively: 
uW(x) 
= 
u0+ 
u 2x 
+ 
U 4 X
2 + 
· · · , 
U ^ ( x ) 
= 
tlj + U 3 X + u 5 x
2 + · · · . 
More in general: 
Definition. For each convolutional (n, k)-code, the polynomials 
y , ( x ) = C[x
(] 
i = 0, 1 
k - 1 
are called the generator polynomials. 

276 
CHAPTER 
14. CONVOLUTIONAL 
CODES 
Theorem. A convolutional (n, k)-code with generator polynomials 
go(x), 
gi(x), ... , </*_i(x) is determined by the following formula: 
«(«)•—Σ
u(,) (*")«(*). 
(
14·
21) 
i=0 
where for each word u = unuiu2 ..., 
we put 
u
( , ) = u,«, +t«<+2t · · · 
(»' = 0,1, 
1). 
Conversely, given polynomials go(x), 9ι(
χ), 
··· , fft-i(x) and a num-
bern, the formula (14.2.1) determines a convolutional 
(n,k)-code. 
PROOF. We first prove the latter statement. Observe that the transforma-
tion 
«(,) ·_•«<<>(,»),,(>) 
is linear and (n, ib)-time-invariant (for each i = 0, 1, ... , A — 1). The 
former follows from the fact that the transformation is composed from 
linear transformations, viz., 
u(x) ι—• u*(x), 
v(x)r-+v(x
n), 
and 
w(x) ι—• 
w(x)gi(x). 
To prove the time invariance, observe that for each word u, 
[x*«(x)]
( < ) =xu<
<>(x). 
Thus, the response to x*u(x) in our transformations is x
nu'(x
n)go(x). 
It follows that the formula (14.2.1) defines a linear (n, ib)-time-invariant 
transformation, i.e., a convolutional (n,Jb)-code. 
Next we prove that each convolutional (n,i)-code C is equal to the 
transformation C"[u(x)] = ]£u'(x
n)tfi(x), provided that j/,-(x) = C[x*] are 
the generator polynomials. Since both C and C are linear, it is sufficient 
to prove that C[x
r] = C"[x
r] for each r = 0, 1, 2, . . . : we then have 
C[ii 0 + uix + u 2 x
2 - r ···] 
= 
] £ u r C [ x
r ] 
= 
Σ « ^ ' Μ 
= 
C"[u0 + tiix + u
2 x
2 + - ] . 

14.2. GENERATOR 
POLYNOMIALS 
277 
Further, since both C and C' are (n, Jfc)-time-invariant, we can restrict 
ourselves to r < k - 1. For u = x
T = 0 0 . . .01000..., we clearly get 
u(
r> = 1000 . . . and u<'> = 0 0 0 . . . for t φ r; therefore, 
C'[x')=gr(x) 
= C[x
r}. 
D 
Remark. 
(1) Analogously to the case of (n, l)-codes in 14.1, the for-
mula (14.2.1) can be expressed in matrix form: the following matrix 
G = 
9o(x) 
9i(x) 
0 
* 
0 
9k-i{ 
η 
9o(x) 
9i(x) 
9k -1 
η 
9o(x) 
u 
η 
of r lines (where r is a fixed number) is a generator 
matrix. 
That is, the 
response of the convolutional code to a word uo«i... u r_ j of length r is the 
word [uotii.. .u r_i]G. 
In fact, the lines of G are g0(x), 
gi(x), 
••• , 9k-i(x), 
x
n9o{x), 
x
n9i{x), 
... , x
ngk-1(x), 
x
2ngo{x), 
... ; thus, 
[«o«i · · .u r_i]G 
= 
u0g0(x) 
+ 
1- 
Uk-\9k-i(x) 
+ x" (u«oo(x) + · · · + 
v2k-igk-i(x)] 
+ χ
2η [x2kgo(x) 
+ · · · + »3k-igk-i(x)] 
+••• 
= 
(u 0 + ukx
n 
+ u 2 * x
2 n + · · 
)g0(x) 
+ ••• + (ut-i + u2k-ix
n 
+ »3k-\x
2n 
+ · · 
)gk-i(x) 
= 
£ « < < v )
f
t ( x ) . 

278 
CHAPTER 
14. CONVOLUTIONAL 
CODES 
Example (continued). The generator matrix of the above convolutional 
(2,2)-code with g0(x) = 1 + χ + χ
3 + χ
6 and gx{x) = χ + χ
3 + χ
4 + χ
6 is 
1 1 0 
1 0 
1 
0 
1 0 
1 1 1 
G = 
1 1 0 
1 0 
1 
0 
1 0 
1 1 1 
1 1 0 
1 0 
1 
0 
1 0 
1 1 1 
(where the empty entries are all zeros). For example, the response to the 
word 101 is computed as follows: 
1 0 
1 
1 1 0 
1 0 
1 0 
0 
0 
1 0 
1
1
1
0 
0 
0 
0 
1 1 0 
1 0 
1 
= [ 1 1 1 0 0 0 0 1 ] 
In other words, C[l + x
3] = 1 + χ + χ
3 + χ
8. 
Remark. 
(3) In the preceding section, we have seen how an encoder of 
each (n, l)-code is constructed directly from the generator polynomial go(x). 
Analogously, the encoder of a convolutional (n.i)-code is constructed as 
follows: 
(1) Design the encoder of the (n, l)-code u(x) t-+ u(x
n)gi(x) 
for each i = 
0, 1. 
1. 
(2) Use a common (η-symbol) output buffer for all the ib shift registers 
by simply adding the corresponding output symbols. 
(3) Use a i-symbol input buffer to multiplex the input string u into the 
k input strings n^°\ υί
ι\ ... , ut
k~
l\ 
For example, let us construct an encoder for the binary convolutional (2,2)-
code with the generator polynomials 
9o(x) 
= 
1 + x + x
2 + x
4 + x
5 + x
6 + χ
7, 
Ji(x) 
= 
x + x
3 + x
4 + x
5. 
We write gQ(x) = ( l + x ) + ( x
2 + 0 ) + ( x
4 + x
5 ) + ( x
6 + x
7 ) ; the corresponding 
shift-register sequence has length 3. Further, 0i(x) = (0 + x) + (0 + x
3 ) + 
( x
4 + x
s ) , which leads to a shift-register sequence of length 2. The encoder 
is shown in Figure 4. Observe that this convolutional code has memory of 
length 3: for the computation of the present output frame, it remembers, 
besides the present input frame, symbols of three preceding input frames. 

14.3. MAXIMUM-LIKELIHOOD 
DECODING 
279 
input 
I 
output 
t 
Figure 4: An encoder of a convolutional (2,2)-code 
Definition. A convolutional (n, k)-code is said to have memory m provided 
that the degrees of its generator polynomials fulfil 
deggi(x) 
< m(n + 1 ) 
for i = 0, 1, ... * - 1. 
The number m(n + 1) is called the constraint length of the code. 
From Remark 2, we see that a convolutional code of memory m can be 
encoded by a combination of shift-register sequences of length m. Thus, 
the code remembers m input frames. The constraint length is the number 
of output symbols, which can be changed by the change of a single input 
symbol. 
Observe that the use of the symbols η and ib is only partially analogous 
when we speak about linear (n, Jfc)-codes and convolutional (n, ib)-codes: in 
some respect, it is the constraint length m(n + 1) which plays the role that 
η does in case of linear codes. 
14.3 
Maximum-Likelihood Decoding of 
Convolutional Codes 
The basic idea of decoding is the same for linear codes and convolutional 
codes: receiving a word, we decode it as the code word of maximum like-
lihood, i.e., of the minimal Hamming distance from the word received. By 
code words we understand all words which are responses of the code C to 

280 
CHAPTER 
14. CONVOLUTIONAL 
CODES 
information words u(x), i.e., all words of the form v(x) = C[u(x)]. We do 
not decode the whole word at once, but step by step, each step correcting 
one code frame, which means the contents of the output buffer. Thus, we 
encode the information word u(x) into a code word v(x) = C[u(x)], 
and 
we receive a word w(x). In the first step, we find the code word v(x) of 
maximum likelihood, and we correct the first code frame WQWX .. .wn-i to 
the word vovi.. .v„_j. If our decoding was correct (i.e., if the maximum-
likelihood code word is the word actually sent), then we have correctly 
established the first code frame, and we know the corresponding first in-
formation frame u 0Ui . ..u*_i (the contents of the input buffer). Let us 
subtract the response to U Q « J .. ut_j from the received word, i.e., let us 
define 
w'(x) = w(x) - C[u0 + U\X + · · · + Uk-\x
k~
x\ 
• 
Then we can discard the first η symbols from the word w'(x), and the 
second step will be the same as the first one: find the maximum-like-
lihood word for the word u'„u'n+i
u'n+2 • · •
 a
n
a correct the second code 
frame u)„uin+i... ω 2 η - 1 to the prefix of that code word of length n. In fact, 
the above subtraction means that instead of the code word v(x) = C[u(x)], 
we now work with 
v(x) - C[u0 + U\X + 
h Ujk-iZ*""
1] 
= 
C[u{x)]-C[uQ 
+ 
v.lx+-+v.k-ix
k-
1] 
= 
C[ukx
k + u t + 1 x
t + 1 + ·••] 
= 
C[x
k(uk 
+ uk+ix 
+ uk+7x
2 
+ ···)] 
= 
x
nC[uk 
+ u t + i x + u t + 1 x
2 + ··•]· 
Thus, by discarding the first η zeros, we obtain the response to the infor-
mation symbols ukuk+iuk+2 
. . . . The third step is quite analogous: put 
w"(x) = u)'(x) - x"C[u* + u t + i x + · · · + u j t - x x "
- 1 ] 
and discard the first 2n symbols from w"(x). 
Then find the maximum-
likelihood code word and correct ui2nW2n+i · · u>3r»-ii etc. 
Example. Consider the binary convolutional (2, l)-code with the genera-
tor polynomial go(x) = 1 + χ + χ
3. Its code words are 
go(x) 
11010000..., 
x
2go(x) 
001101000..., 
(l-rx
2)go(x) 
111001000..., 

14.3. MAXIMUM-LIKELIHOOD 
DECODING 
281 
etc. When receiving, say, 
w = 11110001, 
we find the maximum-likelihood code word, which is (1 + χ
2 + χ
4)σο(χ) = 
11101001. The first code frame is 11 and, thus, the first information frame 
is 1. Put 
w' = w - C[l] = 11110001 - 11010000 = 00100001. 
Discard the first two symbols of w' and find the maximum-likelihood code 
word for 100001. It is 000.. .0; thus, the second code frame is 00, and the 
second information frame is 0. Put 
w" = w' - x
2C[0] = 00100001. 
Discard the first four symbols of w" and find the maximum-likelihood code 
word for 0001. It is 0 0 0 . . . again. Thus, we correct w to 
C [ l + 0 x + 0x
2] = 11010000. 
Remark. A convolutional code is said to correct t errors provided that 
the above decoding can find the code word υ(χ) actually sent whenever the 
Hamming distance of v(x) from the received word w(x) is at most t. Since 
the subsequent steps are just repetitions of the first step (after the "dead" 
symbols are discarded), a convolutional code corrects t errors precisely when 
the first decoding step is successful in all situations with at most t corrupted 
symbols. This leads to the following analogy of the minimum distance of 
linear codes: 
Definition. By the free distance of a convolutional (n, k)-code C is meant 
the smallest Hamming distance d f r e e of code words C[u(x)] and C[u'(x)] 
such that the first information frames of the inputs u(x) and ti'(x) are 
different. In symbols: 
df r ee = min j dist (C[u(x)], C[u'(x)]) | u 0ui... u*_i φ u'0u\ ... u't_i } . 
Remark. Analogous to the case of linear codes (Remark 8.2), the free 
distance can be substituted by the minimum Hamming weight of a code 
word C[u*(x)] such that «3
υΙ · · ·
 u t - i Φ
 
υ ·
 
m 
^
a
c * i g i
v e n 
c°de words 
C[u(x)] and C[u'(x)] of the Hamming distance dfree, consider C[u*(x)], 
where u*(x) = u(x) — u'(x). 
Example (continued). The above convolutional (2, l)-code has 
dfree = 3. 

282 
CHAPTER 14. CONVOLUTIONAL 
CODES 
In fact, the code word go(x) has Hamming weight 3, and an arbitrary code 
word C[uo + u\x + • • ·] with tin φ 0 has Hamming weight at least 3. Thus, 
that code corrects single errors, since we have the following analogy of 
Proposition 4.6: 
Proposition. A convolutional code corrects t errors if and only if Us free 
distance is greater than 2t. 
PROOF. Suppose d f r e e > 2t. Receiving a word w(x) with at most t er-
rors, we find the code word v(z) = C[u(x)] of the smallest Hamming 
distance, say, s, from w(x). It is our task to show that the first information 
frame until.. . ut_i agrees with that of the code word u*(x) = 
C[u*(x)] 
actually sent. 
The Hamming distance of u>(x) and v*(x) is at most t, 
by assumption. Therefore, the Hamming distance of v(x) and w*(x) is 
at most s + t (by the triangle inequality, see 4.4). Since s is the small-
est Hamming distance, we have s < t; hence, the Hamming distance of 
v(x) = C[u(x)] 
and υ*(χ) - C[«*(*)] is s + ί < 2t < dfree- It follows from 
the definition of the free distance that u(x) and u*(x) agree in the first 
information frame. This proves that the first step of decoding is successful. 
The proof for the subsequent steps is analogous. 
Conversely, suppose d^ 
< 2t, and let v(x) = C[u(x)] and t/(x) = 
C[u'(x)] be code words of Hamming distance d f r e e with the first information 
frames different. Let i\, i2, ... , 
be all the indices i for which w,- φ v\. 
Let w(x) bo the following word: 
V{ 
whenever Vi = v'it or i = im, m odd, 
v\ 
if i = i m, m even. 
The Hamming distance of u>(x) and v(x) is smaller or equal to the Hamming 
distance of w'[x) and v'(x), and the latter is at most t (since dfree < 2t). 
Assume that v'(x) is sent and u>(x) is received. This error, corrupting at 
most t symbols, can be decoded incorrectly: in the first step, we choose 
the code word i(x) as the word of maximum likelihood, and this leads to 
a wrong estimate of the first information frame. Thus, the code does not 
correct t errors. 
• 
Concluding Remark. For a convolutional code, an important parameter 
is the free distance which specifies the number of errors correctable by 
the code. No analytic method of constructing good convolutional codes 
(comparable, say, to the BCH codes) is known. However, a number of good 
convolutional codes have been found by computer search. We list some of 
them in Figure 5. 

14.4. 
THE VTTERBl DECODING 
ALGORITHM 
283 
η 
dfree 
ffo(x) 
2 
5 
110111 
= 1 + ι + ι
3 + ι
4 + ι
ί 
2 
6 
11110111 
2 
7 
1101011011 
2 
8 
110111011011 
2 
10 
11011111001011 
2 
10 
111011110111 
3 
8 
111011111 
3 
10 
111011101111 
3 
12 
111011101011111 
4 
10 
111101111111 
4 
13 
1111011110011111 
Figure 5: Some good binary convolutional (n, l)-codes 
14.4 
The Viterbi Decoding Algorithm 
We now describe in detail a decoder realizing the maximum-likelihood de-
coding of a convolutional code. This algorithm was applied, for example, 
by the Voyager spacecraft in 1974 for its mission to Mars, Jupiter, and Sat-
urn. [The spacecraft used the convolutional (2, l)-code with the generator 
polynomialg0{x) - 1 + x + x
2+x*+ 
x
6+ x
7 -rx* + x
10+ 
x
n+x
12.] 
In order 
to explain the Viterbi algorithm, we introduce another representation of a 
convolutional code called the trellis diagram. For the sake of simplicity, we 
restrict ourselves now to the binary convolutional (n, l)-codes. However, 
the reader will have no difficulty in generalizing the Viterbi algorithm to 
arbitrary convolutional codes. 
A trellis diagram depicts each state of the code, i.e., each state of the 
shift-register encoder, at each clock time. The state S at the clock time > is 
connected by an edge with two states at the clock time t + 1: the state S° 
resulting from the input 0 (at the clock time i) and the state S
1 resulting 
from the input 1. We draw the edges in such a way that the edge S —• S° 
always lies under the edge S —» S
1. 
Every edge is labeled by the output 
frame corresponding to the state and the input. 
Examples 
(1) The convolutional (2, l)-code with the following encoder 

284 
CHAPTER 14. CONVOLUTIONAL 
CODES 
output 
- H l r - © - ^ 
has two states: 0 and 1. In the state 0, the output is 
00 
if the input is 0, 
11 
if the input is 1. 
In the state 1, the ouput is 
01 
if the input is 0, 
10 
if the input is 1. 
The trellis diagram is shown in Figure 6. 
State 
1 >-
10 
10 
10 
10 
State 
1 >-
0 0 
00 
00 
00 
00 
00 
Time 
0 
1 
2 
3 
4 
5 ·· 
Figure 6: The trellis diagram of a (2, l)-code of memory m = 1 
(2) The convolutional (2, l)-code with the encoder in Figure 7 has four 
states: 00, 01, 10, and 11. 
From the state 5 = 00, we get to 5° = 00 if the input is 0 or to S
l = 10 
if the input is 1. The corresponding outputs are 00 and 11, respectively. 
This yields the following part of the trellis diagram: 

14.4. 
THE VITERBI 
DECODING 
ALGORITHM 
285 
input— 
Figure 7: An encoder of a (2, l)-code of memory m = 2 
Analogously, if S = 10, then 5° = 01 (input 0, output 10) and S
1 = 
11 (input 1, output 01), etc. The whole trellis diagram is shown in 
Figure 8. 
State 
11 
10 
10 
10 
11 
0 l / \ 
ο ι / χ 
/ 
ο ι \ ν οι 
0 l / \ 
\ 7 
oi 
0 1 / 
01 
/ u p * 
10 
2 > o x p o 2 $ \ \ p o / 2 <"\po 
00 
00 
00 
00 
00 
00 
00 
Time 
0 
1 
2 
3 
4 
5 ... 
Figure 8: The trellis diagram of a (2, l)-code of memory m = 2 
Observe that every path through the trellis starting at time 0 yields 
a code word. For example, the path in Figure 9 corresponds to the 
code word 111000011001 = C[110110]. Conversely, every code word 
represents a path in the trellis. 
Viterbi algorithm. Receiving a word w = WQWIW? ..., we try to find 
the maximum-likelihood path through the trellis. For each time » and each 

286 
CHAPTER 
14. CONVOLUTIONAL 
CODES 
Time 
0 
Figure 9: A path through the trellis 
state S, we list the active path (or paths) through the trellis to the state S at 
time i. A path is called active if its discrepancy is minimal, the discrepancy 
being the Hamming distance between the word generated by the path in the 
trellis and the word (of the same length) obtained by reading the received 
symbols. To simplify the computation of discrepancies, we label each state 
at each time t by the discrepancy of its active path. Then at the next 
time i + 1, we have 
discrepancy of a path = discrepancy of the last edge 
+ discrepancy of the last-but-one state. 
(14.4.1) 
More in detail, the first frame wowi ...ui„_i of the received word w is 
decoded in the following steps (illustrated in Figure 10): 
Initial step: The leftmost state of the trellis (i.e., the all-zero state at 
time 0) is labeled by 0, since there is no discrepancy. 
Step i + 1: Suppose that each state at the clock time i has been labeled 
by the discrepancy of (all) active paths leading to it, and that all 
active paths of length t have been listed. 
Given a state S at the 
clock time t + 1, find all active paths in the trellis leading to S [using 
the previously listed active paths, see (14.4.1)]. Label S by the dis-
crepancy of those paths. Make a list of all active paths at the clock 
time t. 
Final step: The procedure terminates at time b provided that all the ac-
tive paths at that time have the same first edge. If the label of that 
edge is vavi.. 
.w„_j, then the first code frame wotvi.. 
. ω„-ι is cor-
rected to VQV\ .. 
.ν„-χ. 

14.4. 
THE VITERBI 
DECODING 
ALGORITHM 
287 

288 
CHAPTER 
14. CONVOLUTIONAL 
CODES 
In the case when there always are two active first edges, the error is not 
correctable. 
In practical applications of the Viterbi algorithm, it is not possible to 
use an arbitrary word length at the receiving end. Instead, a number 6 is 
specified (the length of the "decoding window") and the algorithm always 
stops after 6 steps: either the unique first edge has been found at that time 
or the error is declared to be incorrectable. 
After correcting the first code frame, the decoding window is moved 
η steps to the right, and the correction of the second code frame is per-
formed, etc. 
Example (2) (continued). Suppose we receive 
and we use a decoding window of length 6 = 7. 
To correct the first 
frame wotvi, we start the Viterbi algorithm. Step 1 assigns to the lower 
state (00) the discrepancy 1 because the corresponding edge has value 00, 
whereas woW\ = 10. Analogously, it assigns 1 to the upper state (10) 
because the value of the corresponding edge is 11: 
The discrepancies in Step 2 are easy to compute from those in Step 1 using 
the fact that ιυ2υ>3 = 01: 
The whole procedure is shown in Figure 10: after 6 = 7 steps, all the active 
paths pass through the first edge labeled by 11. Thus, we correct waw\ 
to 11, and continue analogously with the next frame w^w^, etc. (If our 
decoding window had length 5, the error would not be correctable.) 
Exercises 
14A 
Design an encoder of the binary convolutional (2, l)-code with the 
generator polynomial go(x) = 1 + χ + x
a + x
4 + x
e + x
7 + x
9. Encode 110. 
10010100101100000... 
Step 1: 
Step 2: 

EXERCISES 
289 
1 4 B 
Find a five-row generator matrix for the code in 14A. 
14C 
Design an encoder of a binary convolutional (3,2)-code of memory 
m = 2. 
14D 
The encoder in Figure 4 in 14.2 uses five shift registers. Design an 
encoder of the same code using four shift register states only. [Hint: the 
two shift-register sequences can share the last register.] 
1 4 E 
Describe the generator polynomials of the convolutional code in 
Figure 11. 
Figure 11: An encoder of a (3,3)-code 
14F 
Find a five-row generator matrix for the code in 14E. Encode 11001. 
14G 
Design an encoder for the (2,3)-code with the generator polynomials 
ffo(x) = 1 + x + x
3 + x
5 + x
6, gi(x) = χ + x
2 + x
3 + i
4 , and g2(x) 
= 
1 + x
2 + x
3 + x
5 + x
6 . 
14H 
Use the Viterbi algorithms to decode w = 01000001000... provided 
that the code of Example 2 of 14.4 is used (and b = 7). 

290 
CHAPTER 14. CONVOLUTIONAL 
CODES 
output 
<+K 
input 
Figure 12: An encoder of a (2,2)-code 
141 
Draw the trellis of the (2, l)-code with g0(x) = I + x
2 + x
a + z
&. 
Use the Viterbi algorithm to decode w = 1000001000001000.... 
14J 
Generalize the concept of a trellis diagram to (n, t)-codes. Draw 
the trellis of the (2,2)-code in Figure 12. 
Notes 
Convolutional codes were introduced by Elias (1954); see also Wozencraft 
(1957). The polynomial treament is due to Masey and Sain (1968) and 
Forney (1970), who also presented the description by a trellis. The decoding 
algorithm is due to Viterbi (1967). 

Part III 
Cryptography 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

Chapter 15 
Cryptography 
In this last chapter, we provide a brief introduction to cryptography, stress-
ing the role error-correcting codes play in this field. In cryptography, we 
assume that a sender wants to send a message to a user in such a way that 
an enemy with a wiretap on the channel will not be able to understand the 
message. For that reason, the sender applies a special code (we speak about 
encryption instead of encoding here) and the user decrypts (i.e., decodes) 
to get the original message. See Figure 1. 
Sender 
User 
channel + noise 
Encryption 
channel + noise 
Decryption 
Encryption 
•f 
Decryption 
j wiretap 
i 
, 
Enemy 
Figure 1: A scheme of decoding for secrecy 
We present several modern cryptosystems, some of which directly use 
error-correcting codes. The main connection between the two theories is 
that cryptography is typically very sensitive to noise. A small change in 
the encrypted message will usually cause a big damage after decryption. 
For this reason, it is advisable to combine encryption and error correction, 
293 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

294 
CHAPTER 15. 
CRYPTOGRAPHY 
Sender 
User 
Encryption 
Decryption 
Channel + Noise 
Encoding 
Channel + Noise 
Decoding 
Encoding 
'? 
Decoding 
-1_ 
Enemy 
Figure 2: A scheme of cryptography with error-correcting coding 
as indicated in Figure 2. 
It should also be noted that since encryption and decryption are usu-
ally quite demanding (a simple encryption would be simple to break), it is 
always worthwhile to apply data compression before encrypting, see Chap-
ter 3. 
We assume for simplicity that the source messages are binary. This is 
unimportant: for the general source alphabet, we can first use some binary 
encoder and then compress data to get rid of redundancy. 
15.1 
A Noisy Wiretap 
If the wiretap (see Figure 2) is noisy, we can find a simple and effective 
encryption by means of error-correcting codes. We use them in two ways: 
first, we apply an error-correcting code to eliminate the noise of our channel. 
Thus, we will now assume that our channel is noiseless, whereas the enemy 
has a noisy wiretap. The next application of an error-correcting code will 
first be illustrated by the simple example of the even-parity code: 
Example. Suppose that the wiretap is a binary symmetric channel of 
error-probability p. We encrypt as follows: we choose a number n, and 
instead of 0, we send a random binary word of length η and even parity; 
instead of 1, a random word of odd parity. In other words, if K„ denotes 

15.1. 
A NOISY 
WIRETAP 
295 
the even-parity code of length n, then 0 is encrypted as a random code 
word and 1 as a random noncode word. 
The decryption (in the absence of noise) is easy: decrypt the first η sym-
bols as 0 or 1, according to the parity. On the other hand, the enemy 
receives an η-tuple which either has the original parity or the wrong par-
ity. The latter happens whenever one bit has been corrupted [probability 
np(l - p)"
- 1] or three bits [probability (a)p
3(l - p )
n - 3 ] , etc. Thus, the 
enemy decrypts incorrectly with probability 
It is easy to verify that Pn converges to | with inreasing n. If we choose 
η with P„ « | , then the enemy is completely baffled: half of his O's are 
actually l's, and vice versa. 
Remarks 
(1) More in general, every binary linear (n.ib)-code Κ can be used for 
encryption as follows. Let Η be a parity check matrix. For each word w 
of length n, we can compute its syndrome s (of length η — Jb) by 
Hw" = s
t r. 
Conversely, all the words w with a given syndrome s form a coset, 
and the number of such words is equal to 2* (= the number of all 
code words), see 6.2. We divide the source message into blocks of 
length η - ifc. Given a block s, we choose a random word w with the 
syndrome s, and we encrypt s as w. The information rate R of this 
encryption is 
η 
because the source message s of length η — Jb is encrypted by a word of 
length n. 
The decryption (in the absence of noise) is easy: given w, decrypt by 
finding the syndrome s of w. 
(2) The enemy who uses a noisy wiretap will lose a certain amount of in-
formation. In fact, Wyner (1975) proved that it is possible to find 
codes which (like in the above example) lead to a complete loss of in-
formation by the enemy, but (unlike the above example) keep a certain 
information rate. More precisely, suppose that the enemy uses a bi-
nary symmetric channel of error probability ρ > 0. We know that the 

296 
CHAPTER 15. 
CRYPTOGRAPHY 
maximum amount of information per symbol which can pass through 
the channel is the capacity C = 1 - H(p, 1 - p), see 4.7, and the sum-
mand H(p, 1 — p) represents the loss of information per symbol. Wyner 
proved that there exist codes K„ such that the information rates of the 
above encryption method tends to H(p, 1 — p), and the information the 
enemy receives about the source symbol tends to 0 with increasing n. 
15.2 
Secret-Key Encryption 
The simplest method of encryption which, moreover, is absolutely secure 
consists in choosing a random key, i.e., a long random binary word known 
only to the sender and the user. The sender adds the key to the message 
(the length of which must be smaller or equal to the length of the key) and 
sends 
ν = u + k 
(u = message, k = key). 
The user also adds the (same) key to the received word v. Thus, the user 
gets 
v + k = u + k + k = u, 
the message sent (see Figure 3). The wiretap brings no information: since 
k is a random word (obtained, say, by tossing a coin a number of times), 
the word ν is also random. 
key 
key 
Φ-
110010 
message 
Τ 
I 
_L 
4> 
message out 
wiretap 
Figure 3: The one-time key 
The serious disadvantage of this scheme is the necessity to find a secure 
manner of delivering the (usually extremely long) key to the sender and the 
user. One way out of this difficulty is to use a small random key ko as a germ 
for a large "pseudo-random" key k, i.e., a word whose properties resemble 
the properties of random words. We are going to describe such a procedure 

15.2. 
SECRET-KEY 
ENCRYPTION 
297 
which constructs from a random binary word ko of length m a word of 
length 2
m — 1 having the basic features of "randomness". Thus, a key germ 
of length 100 yields a "pseudo-random" key of length 2
1
0
0 - 1 « 1.3 x 10
3 0. 
The trick is to use the ( 2
m — 1, m)-simplex code S m [see Example (4) 
of 8.2]: each key germ of length m, considered as an m-tuple of infor-
mation symbols, is encoded into a code word of length 2
m — 1. We will 
explain why the code words of the simplex code can be considered to be 
"pseudo-random". Let h(x) be the minimal polynomial of a primitive el-
ement of GF(2
m). 
Then Λ(χ) is the generator polynomial of a Hamming 
code of length 2
m — 1 (see 12.1), which is the dual code to S m . 
Conse-
quently, Λ(χ) is a parity check polynomial of S m . 
Example. 
(1) The minimal polynomial 
Λ(χ) = χ
3 + χ + 1 
of the primitive element of GF(8) found in 11.6 is a parity check polynomial 
of the simplex code S 3 . The corresponding parity check matrix is 
Η = 
0 
0 
0 
1 
1 0 
1 
0 
0 
1 1 0 
1 0 
0 
1 1 0 
1 0 
0 
1 1 0 
1 0 
0 
0 
It follows that code words are those words voviv^vsv^v^ve which satisfy 
H v " = 0
T R, or, equivalently, 
Vi + Vi+i + Vi+3 = 0 
for ι = 0, 1, 2, 3. 
In other words, the following recursion formula 
Vi+3 = Vi + υ,·+ι 
determines the code S 3. This formula can be "implemented" by the follow-
ing shift-register sequence: 
output 
VQ 
V2 
That is, given any initial state (or key germ) V0V1V2 of the registers, the 
output word after 8 stages is a code word of S3 (and, conversely, each code 
word of S3 can be obtained in this manner, since S 3 has precisely 2
3 code 
words). As a concrete example, let us choose the key germ 101. We obtain 
the code word 
1011000. 

298 
CHAPTER 15. 
CRYPTOGRAPHY 
Remark. More in general, let h(x) = ho + h\x-\ 
rhmx
m 
(Λο = hm = 1) 
be the minimal polynomial of a primitive element of GF(2
m). 
Then h(x) 
is the parity check polynomial of the simplex ( 2
m - l,m)-code S m . The 
nonzero code words can be obtained from the shift-register sequence in 
Figure 4 as follows: insert any nonzero initial word (key germ) VQVI • • • v m - i 
into the shift register, then the output after η = 2
m — 1 stages is a code 
word of S m . In fact, the output clearly satisfies the recursion formula 
m - 1 
v
i
+
m = Σ 
htvi+t 
(i = 0 , 1 , 2 , . . . ) 
(15.5.1) 
or, equivalently, 
m 
= ° 
( t = 0 , 1 , 2 , . . . ) 
(15.5.2) 
k=0 
Figure 4: An encoder of the simplex code 
It follows that 
' 0 
0 
·•· 
0 
0 
hm 
•·• 
hi 
Λο " 
vo 
" 0 " 
0 
0 
0 
hi 
h0 
0 
= 
0 
«1 
Λο 
0 
0 
0 
0 
Thus, v0vi.. 
.vn-i 
is a code word of S, 
Proposition. The shift-register sequence in Figure 4 has η — 2
m — 1-
nonzero states. Given an arbitrary nonzero initial state, each of the η states 
is taken precisely once during the first η shifts. 

15.2. 
SECRET-KEY 
ENCRYPTION 
299 
PROOF. Since we have m two-state shift registers, the number of all non-
zero states is 2
m — 1. 
Denote by voV\v2 
• • • the infinite output of the shift-register sequence, 
and assume that the initial state (which is voV\ .. . v m _ i ) is nonzero. Then 
we are going to prove that the sequence VQV\V2 . . . is periodic with period n, 
i.e., η is the smallest number such that Vi+n 
= t>, for all i. It then follows 
that the shift-register sequence does not return to any of the previous states 
during the first η shifts, and since η = 2
m — 1 is the number of all possible 
nonzero states, each state is taken precisely once. To prove that VQV\V2 • • • 
has period n, we form the power series (11G) 
v i
x ) - Σ
υ,χ'· 
i=0 
and we prove that η is the smallest number such that V(x) can be expressed 
as 
V(x) 
= a(x)(l 
+ x" + x
2
n + ···), 
degreea(x) < n. 
For each polynomial a(x) = ao + a\x + 
(- arx
r, 
denote by a*(x) = 
ar + ar-\x 
+ · · · + OQX
T the reciprocal polynomial and observe that this 
operation respects multiplication, i.e., [a(r)6(x)] = 
a(x)*b(x)*. 
We first observe that the product 
f(x) 
= 
V(x)h'(z) 
is a polynomial of a degree lower than m. In fact, since 
oo 
fix) = 
ihm+hm-1x+-+hQx
m)Y^vix
i, 
i=0 
fix) 
has the following coefficient at x', j > m: 
m 
hmVj 
+ hm-iVj-i 
+ 
r h0Vj-m 
= 
y^/ui>(j-,n) +fc. 
*=0 
The output sequence V 0 V 1 V 2 . . . satisfies the recursion formula (15.5.2); 
thus, the last expression is 0. 
Next, from the equation 
g(x)h(x) 
= x
n - l , 
where g(x) is the generator polynomial of the code S m , we get 
0·(χ)Λ·(*)= ( *
n - l ) * = l - * » . 

300 
CHAPTER 15. 
CRYPTOGRAPHY 
Consequently, 
The formula for geometric series (11G) implies that 
K(x) = /(x) S*(x)(l + x
n + x
2
n + ··•)· 
The degree of / ( x ) is smaller then m, thus, f(x)g*(x) 
has degree smaller 
than m + η - m = n. Consequently, the polynomial /(x)o*(x) represents 
a word of length n, and the last equation tells us that V(x) is a (cyclic) 
repetition of that word: V(x) = f(x)g*(x) + x
nf(x)g*(x) 
+ - · ·· Thus, V(x) 
is a periodic sequence. 
Let Jb be the period of V(x), i.e., the smallest number such that V(x) = 
a(x) + x
ka(x) 
Η 
for some polynomial a(x) of degree lower than Jb. We 
clearly have k < n, and we will prove that Jb > n. In fact, since 
j f l ^ . , .
( . , - ^ ,
(
1
+ ^
+ ^
+ . . . ) -
1 s a
r . 
we conclude that 
/ ( x ) ( l - x
k ) = f c ' ( x ) a ( x ) . 
The reciprocal of that equation is 
Γ ( χ ) ( χ * - ΐ ) = Λ(χ)α·(χ). 
By the unique factorization theorem (11.1), the irreducible polynomial h(x) 
divides either /*(x) or χ* — 1. The first is impossible since /*(x) has degree 
smaller than m. The latter implies Jb > n: in fact, h(x) is the minimal 
polynomial of a primitive element β of GF(2
m). 
Since Λ(χ) divides χ* - 1, 
it follows that /?* — 1 = 0 . Because the order of β is 2
m — 1 = n, we conclude 
that Jb > n. 
• 
Pseudo-random sequences. In the following theorem, we list some prop-
erties of the nonzero code words of the simplex code, due to which we can 
consider each of these words as a pseudo-random sequence. We leave it 
to the reader's intuition to conclude why these properties are sufficient—a 
detailed explanation can be found in the monograph of Golomb (1967). 
Some concepts first. By a run of length Jb in a word v n V i . . . v n_i is 
meant a word v,... 
with 
Vi Φ υ,+ι = υ ί + 2 = • · · = vi+k φ v i + k + i 

15.2. 
SECRET-KEY 
ENCRYPTION 
301 
(i = 0, 1, ... , η — Jb — 2). Thus, runs are of two types: 
a run of l's: 0.111... 11,0, 
a run of O's: 1000.. .00,1. 
k 
k 
By the autocorrelation of the word ν is meant the function 
1 m—1 
= η Σ 
( " Ι Γ ( - 1 ) "
< + * 
(* = 1,2,..., η - 1) 
i = 0 
with addition in indices modulo n. 
Example. The code S 4 can be generated by the following shift-register 
sequence [corresponding to h(x) = 1 + χ + χ
4]: 
output 
From the key germ, say, 1000, we get the key word 
100010011010111. 
It has 
(1) eight l's and seven O's, 
(2) eight runs: four of length 1, two of length 2 and 3 each, 
(3) the autocorrelation function p(k) = — ^ for all Jb. 
Theorem. Each nonzero code word of the simplex code of length η has the 
following properties: 
(1) The number ofl's 
is essentially equal to the number of O's: they are 
and
 
a f ^ , respectively. 
(2) Of all the runs, one-half have length 1, one-quarter have length 2, 
one-eighth have length 3, etc. (as long as the fractures are integers). 
Except for the two longest runs, the number of runs ofl's 
of a given 
length is equal to the number of runs of O's of that length. 
(3) The autocorrelation has a (very small) constant value: 
p(k) = ~- 
fork = l, 2, , . . , η - l . 
η 

302 
CHAPTER 
15. 
CRYPTOGRAPHY 
PROOF. By the above proposition, the shift-register sequence generating 
an (arbitrary) nonzero code word vnt>i ...v n-i takes each nonzero state 
precisely once. The ith output υ,· is the leftmost bit of the ith state. This 
implies the properties (l)-(3) as follows. 
(1) Half of all states have the leftmost bit 1, thus, υ,· = 1 for j 2
m in-
dices i. Clearly, ^ 2
m =
 
a 4p-. Since the all-zero state is not taken, the 
number of all states with the leftmost bit 0 is | 2
m — 1 = ^y^-. 
(2) Consider first all runs of O's. A run of 0's of length ib < m — 2 
results from a state whose ib + 2 leftmost bits form the word 1000.. .001. 
The remaining m — k — 2 bits are arbitrary, thus, the number of such states 
is 2
m - * -
2 . It follows that the word has 2
m - f c -
2 runs of O's of length ib = 
1,2,... , m - 2. One run of O's has length m - 1 : it results from the unique 
state of the form 1000.. .00 (which, as can be easily verified, is followed in 
the next stage by the state 000.. .001). Thus, the number of all runs of O's 
is 
m - 2 
m - 3 
1 + Σ 
2
m - * -
2 = 1 + Σ
 
2< =
 1 +
 2
m ~
2 ~
 
1 =
 
2m"
2-
k=l 
1=0 
It is clear that no run of O's has a length larger than m — 1 since the all-zero 
state is never taken. 
Analogously, the number of runs of l's of length ib = l,2, . . . , m — 2 
is 2
m - * ~
2 . However, no run of l's has a length m — 1: The unique state 
of the form 0111... 11 is followed by the all-one state [which easily follows 
from the fact that, since Λ(0) φ 0, the number of summands of h(x) is odd}. 
The all-one state is followed by 111... 110 [for the same reason]. This leads 
to a run of l's of length m. The number of all runs of Γ is again 2
m
-
2 . 
Consequently, we have 2
m
_
1 — 2 x 2
m ~
2 runs. Of these, one-half have 
length 1: 
± 2
m ~
l = 2 x 2
m - * -
2 
for ib = 1. 
One-quarter, 2
m ~
3 , have length ib = 2, etc. 
(3) Put u>, = v, + v<+t in Z 2 (addition in the index modulo n). Then 
v 
; 
1 
' 
I - 1 ifu>, = l. 
Now observe that w = wow\.. .u)„_i is a code word of S m : it is the sum of 
the code word ν and the same word shifted cyclically ib times. Obviously, 
w φ 0 (since the output ν of the shift-register sequence would otherwise 
have period ib, in contradiction to the proposition above). Thus, by (1), 

15.3. PUBLIC-KEY 
ENCRYPTION 
303 
wi = 0 for 
indices i and 
= 1 for
 a |
l
i indices i. Therefore, 
,m=ι £(-')··(-.)···· - i ( i ± i - i i i ) - -ι. 
ο 
ι=0
 
x 
' 
Concluding Remark. One possibility of encryption is to use a secret 
key k whose length is larger or equal to the length of the encrypted mesage. 
If k is a random word, the encryption method is completely secure 
(provided that the key can be sent through a completely secure channel). 
More realistic, but less secure, is the method of using a random key 
germ ko of length m from which a pseudo-random key k of length 2
m — 1 is 
created by means of a linear shift-register sequence. The weak point of this 
method is that the enemy, once deciding that this encryption was applied, 
can break the code whenever he learns a source message of the (small!) 
length 2m together with its encryption, see Exercise 15A. In this sense, 
linear shift-register sequences are insecure, and an application of nonlinear 
elements is advisible. An example of such a method is the DES encryption 
described in 15.5. 
15.3 
Public-Key Encryption 
Modern cryptosystems are often such that the method of encryption is not 
kept secret. Thus, anyone can send the ueer an encrypted message. On 
the other hand, decryption is extremely complicated for everyone except 
the user—so complicated that it can be considered impossible. Here we 
describe one public-key cryptosystem based on Goppa codes, and in the 
next two sections, we will show two other public-key cryptosystems. The 
following cryptosystem was discovered by McEliece (1978). 
Recall from 12.5 that each irreducible polynomial r(x) of degree t over 
GF(2
m) 
defines a Goppa (n.Jt)-code with 
η = 2"* 
and 
k>n-mt. 
This code corrects t errors (and a fast decoder is known). Let us choose at 
random 
(1) an irreducible polynomial r(x), 
(2) an invertible binary A; χ Jb matrix S, and 
(3) a permutation η χ η matrix Ρ (i.e., each row and each column of Ρ 
has Hamming weight 1). 

304 
CHAPTER 15. 
CRYPTOGRAPHY 
The encryption is performed by means of the following matrix: 
G* = SGP, 
where G is a Jb χ η generator matrix of the Goppa code. To encrypt, divide 
the message into words ν of length ib, compute vG*, and change t randomly 
chosen bits. Thus, the encrypted message is 
w = vG* + e, 
where e is a random word of Hamming weight t. The matrix G* is public, 
whereas the matrices S, G, and Ρ are kept secret. 
At the receiving end, we decrypt w as follows. First, compute wP"
1. 
Observe that 
w P
- 1 = (vS)G-l-eP-
1. 
Now, (vS)G is a code word of the Goppa code (given by the information 
bits vS), and since P
-
1 is a permutation matrix, the Hamming weight 
of e P
- 1 is t. Thus, the word wP"
1 has Hamming distance t from a code 
word. Since the Goppa code corrects i errors, we can recover the word 
(vS)G by applying the decoding technique to wP"
1. 
This establishes 
the information bits vS, and the final step is a multiplication by S
_ 1 : 
ν = (vS)S-
1. 
Example. Choose a random irreducible polynomial of degree t = 50 over 
GF(2
1 0). It can be proved that there exist more than 1 0
1 4 9 such polyno-
mials. We have 
η = 1024, 
t = 50, 
ib > 1024 - 50 x 10 = 524. 
Next choose a permutation η χ η matrix Ρ (we have 1024! possibilities) 
and an invertible ib χ ib matrix S (from among an astronomical number of 
possibilities). Keep S, P, and the generator matrix G secret while publish-
ing G* = SGP. 
The encryption is performed by blocks of length ib. From each such 
block v, compute the code word vSG, change it in 50 random bits, and 
send the resulting word w. 
The decryption of the word w received consists in correcting the 50 er-
rors of the word wP"
1, thus, retrieving the code word (vS)G. The infor-
mation bits vS are then multiplied by S
- 1 . 
The enemy, who knows w and G* and knows the encryption method, 
will not be able to decrypt w unless he discovers the secret matrices (which 
were chosen randomly from an extremely large number of possibilities). 
Moreover, the random change of 50 out of 1024 bits makes the decryption 
even more difficult. 

15.4. 
ENCRYPTION 
BASED ON LARGE PRIME NUMBERS 
305 
15.4 
Encryption Based on Large Prime 
Numbers 
Riveet, Shamir, and Adelman (1978) found a simple public-key cryptosys-
tem based on the following two facts: 
(1) A fast algorithm for finding a random prime number of a prescribed 
size is known. 
(2) No fast algorithm for a factorization of a large number into a product 
of primes is known. 
Recall from 11.7 that for two relatively prime numbers q and n, we have 
q«
a) 
= 1 (mod n), 
where φ is the Euler function. Further, if η = pq is a product of primes ρ 
and q, then 
V>(n) = ( p - l ) ( q - l ) . 
Encryption. Choose two large random prime numbers ρ and q. Put 
n = pq 
and use the numbers 0, 1, ... , η — 1 as the source alphabet. (One can, 
of course, use the binary expansion of those numbers to remain in the 
binary.) Further, choose a random number i = 0, 1, ... , η — 1 relatively 
prime with φ(η). Then, by 13C, we can find a number j = 0, 1, . . . , η — 1 
such that 
ij = 1 (mod <p(n)). 
Publish η and i, keeping p, q, and j secret. 
A source symbol ν = 0, 1, ... , η — 1 is encrypted as follows: compute 
the ith power v' and find the remainder w of its division through n. That 
is, the symbol sent is the number ω = 0, 1, ... , η - 1 such that 
w = v' 
(mod n). 
Decryption. The received symbol w is raised to the jth power and the 
remainder of w> divided through η is computed: 
υ = w
3 
(mod n). 

306 
CHAPTER 
15. 
CRYPTOGRAPHY 
Proposition. The above decryption is correct, i.e., for each number ν = 
0, 1, . . . , n - 1 , 
w = ν' 
(mod n) 
implies 
ν = w> (mod n). 
PROOF. The statement is clear if ν = 0; thus, we can assume ν φ 0. Since 
ij =. 1 (mod φ(η)), there exists an integer r such that ij = 1 + rip(n). 
Then w = v' (mod n) implies 
= 
vvrv(») 
( m o d 
„). 
It remains to prove that 
i / "
( n ) = l 
(modn), 
from which it follows that t>
r^(
n) = l
r 
(mod n). Then we conclude that 
= ν χ 1 (mod n). 
Suppose ν and η are relatively prime. Then »*<") Ξ 1 (mod n) by the 
Euler-Fermat Theorem (11.7). If ν and η are not relatively prime, then υ is 
divisible either by ρ or q; say, by p. Then t/^") is also divisible by p; thus, 
the remainder of the division of ν
ψ^ 
through η = pq is the same as the 
remainder of the division through q. Therefore, it remains to prove that 
vv>(.n) = ι 
(mod q). This follows from the Euler-Fermat Theorem again: 
ν is relatively prime with q (since else it would be divisible by η = pq, but 
0 < υ < n); therefore, 
ΞΞ 1 (mod q). 
Raising both sides of the last equation to <p(p), we conclude [since <p(n) = 
( Ρ - 1 ) ( ? - 1) = ¥>(ρΜϊ)]
 
t
h
a
t 
„„<») = [yKf)]*rt = ! 
(
m
o
d 
9 ) . 
Ο 
Example. Choose two random 50-digit prime numbers ρ and q. Do not 
be worried whether there is enough to choose from: for the number ir(Jb) of 
all prime numbers smaller than Jb, it it well known that 
*(ib)log* 
um 
; 
= 1. 
*-.oo 
ib 
If we estimate ir(10
5 0) by ^""n = 5 χ 10
4 7, we conclude that the choice is 
rich enough. The basic fact is that there are fast algorithms for testing pri-
mality; thus, the random choice can be practically performed: choose a ran-
dom fifty-digit number and test it for primarity. See Solovay and Strassen 
(1977) for details. 

15.5 
ENCRYPTION 
BASED ON KNAPSACK 
PROBLEMS 
307 
Once ρ and q have been chosen, we have a 100-digtt number η = pq 
which we publish. We further publish a number i for which we have found 
a (secret) number j such that ij = 1 (mod n). We encrypt by the public 
key υ *-* v* (mod n) and decrypt by the secret key w *-* 
(mod n). In 
order to guess the secret number j , it would be sufficient to know the secret 
number ψ(η): then j is determined from the equation ij Ξ 1 
(mod <p(n)). 
However, guessing <p{n) is as difficult as factoring n, see Exercise 15B. 
No algorithm capable of factoring a random 100-digit number sooner 
than in hundreds of years is known today, and there are good reasons to 
believe that such an algorithm will not be found in the future. 
15.5 
Encryption Based on Knapsack 
Problems 
Another public-key cryptosystem was found by Merkle and Hellman (1978). 
It is based on the following knapsack problem: given a pile of objects, can 
your knapsack be filled exactly by some of those objects? More precisely: 
given a pile of natural numbers αϊ, a 2, ... , o„, can a fixed number 8 be 
expressed as a sum β = α,·, + α,·, + 
1- 
of some of them? This can 
be reformulated by asking whether s is equal to the scalar product of the 
vector a = ( 0 1 , 0 2 , . . . ,o„) and some binary word ν of length n. In fact, 
the expression 
s = ν · a = υιοί + υ 2α 2 Η 
h v„a„ 
is exactly the sum we were searching for. 
The knapsack problem can be used for encryption as follows: encrypt 
binary words ν of length η by the formula ν »-» β = ν · a. To decrypt, 
solve the knapsack problem of the number s, i.e., find ν such that s = ν · a. 
Unfortunately, if the knapsack problem is simple, i.e., if the numbers αϊ, a 2, 
... , a n are such that one can simply find the word ν for the given number s, 
then this method is useless because the enemy can simply decrypt our 
message. 
If the knapsack problem is difficult, then this method is also 
useless because we will not be able to encrypt. What we need is a method 
of turning an easy knapsack problem into a problem which is difficult for 
the enemy. We first describe some simple knapsack problems. 
Definition. A sequence a\, a 2 ) ... , a„ of natural numbers is called su-
perincreasing provided that for each i = 1, 2, ... , η — 1, 
> « ι + 0 2 + ••• + α,·. 

308 
CHAPTER 
15. 
CRYPTOGRAPHY 
Remarks 
(1) The knapsack problem of every superincreasing sequence αϊ, a 2 l . - • 
,an 
is easy: given a number β, define a binary vector ν as follows: 
vn = 1 
<=> 
a„ < s, 
u„_i = 1 
<=> 
α„_ι + vna„ < β, 
v„_2 = 1 
<=> 
a „ _ 2 + t>„_ia„_i + v„a n < β, 
etc. Then either β = ν · a or the knapsack problem has no solution 
for β. 
(2) A simple knapsack problem (01,0.3,... ,an) can be turned into a difficult 
one as follows. Choose relatively prime numbers t and m with m > 
Σ"=1α< and m > i > 1. Then find j = 1,2, ... , m such that ij = 
1 (mod m), see 13C. For each of the given numbers at, find the 
remainder &t of the integer division of tat through m: 
bt = iais 
(mod m). 
The numbers ( 6 j , 6 2 , . . . , b„) constitute a knapsack problem looking dif-
ficult to the enemy. But it is a simple one for us: given a number s, 
find the remainder β' of the integer division of js through m: 
s' = js 
(mod m). 
Solve the easy knapsack problem for «': β' = ν · a. The same vector ν 
then solves the "difficult" knapsack problem, as we prove presently. 
Proposition. For each numbers 
= vibi+vib?-] 
h v n 6 n , the numbers' 
= 
0, 1, . . . , m—1 with e' = js (mod m) satisfies β' = viaj -rt^fljH 
\-vnan. 
PROOF. Since ij — 1 (mod m) and &ι Ξ tat (mod m), we have jbt = at 
(mod m). Therefore, 
e' 
= 
j(vibi + «262 + 
h t>„6„) 
(mod m) 
= 
v\Oi + vjaj Η 
1- v„a„ 
(mod m). 
Our choice of m was such that m > ΣΊι=ι α* > υχοχ + vja^ + 
r 
υηαη. 
Also, m > β'. Thus, from the equation β' = ΣΊί=ι
 
υ *
α * (mod m), it follows 
t h a t e ' i r ^ . i v t a t . 
Π 

15.6. 
DATA ENCRYPTION 
STANDARD 
309 
Encryption. 
Choose 
(1) an easy knapsack problem 
"1 ι "2> · · · ι " m 
(2) a number m > ΣΊ=ι 
at, 
(3) numbers i, j — 1, 2, . . . , m — 1 with ij = 1 (mod m). 
Compute numbers bt = 0, 1, ... , τη - 1 satisfying 6* = tat 
(mod m). 
Publish 6i, δ 2, ... , b„, keeping i, j , at, and m secret. 
Binary messages are divided into words ν of length n. Each word is 
encoded as the number 
s = t>i6i -|- υ 2δ 2 + 
h v„b„. 
Decryption. Find the number s' = 0, 1, ... , m — 1 with «' = je 
(mod m). The solution of the easy knapsack problem for β' is the encrypted 
message. 
Concluding Remark. The security of the knapsack encryption depends 
on the difficulty of solving the new knapsack problem 6j, δ 2, ... , 6„. Al-
though it is known that solving a random knapsack problem is really diffi-
cult (as difficult as factoring large numbers), the trouble is that our knap-
sack problem is not random. 
To increase the security, the method can be iterated: from a superin-
creasing sequence αχ, α 2, ... , a„, we create a difficult knapsack problem 
δι, δ 2, ... , δ„, which, however, is simple for us. Starting from the latter 
problem, we create, in the same manner, a new knapsack problem c\, c 2, 
... , c„, which we publish (keeping δι, δ 2, . . . , δ„ secret), etc. The advan-
tage of the knapsack encryption over, say, the prime-number method is the 
simplicity of its implementation. 
15.6 
Data Encryption Standard 
We now describe the cryptosystem which has become the Data Encryption 
Standard (DES) of the National Bureau of Standards in 1977, and which 
is available on chips suitable for a number of applications. Although theo-
retical attacks against the security of DES have been launched, no effective 
way of breaking it has been published yet. 
The encryption of DES is based on a key chosen by the user. The key 
is a binary word of length 64, with 56 arbitrarily chosen bits and 8 parity 
check bits. We now describe the scheme in detail (see Figure 5). 

CHAPTER 15. 
CRYPTOGRAPHY 
input v, length 64 
h = r 0 
1 2 = ri 
permutation e 
r\ = l o + / ( r
0
) k i ) 
r 2 = h + / ( r i . k a ) 
lie = r j 5 
Γιβ = lie + / ( r i s . k i e ) 
Γΐβ 
lie 
permutation e~
l 
encrypted word 
Figure 5: DES encryption of a source word ν 

15.6. 
DATA ENCRYPTION 
STANDARD 
311 
The scheme uses various nonlinear elements, e.g., those of the following 
type, S: let S be a 4 χ 16 matrix, the rows «ο, «i, «2, «3 of which are 
permutations of the numbers 0, 1, . . . , 15 (see Figure 6). The nonlinear 
function S maps words of length 6 to words of length 4 as follows: the 
response to vivivsv+v&ve is the matrix entry 5,j = 0, 1, ... , 15 expressed 
in the (4-bit) binary expansion, where 
t is the number whose binary expansion is vjve, 
j is the number whose binary expansion is V2V3V4V5. 
For example, if S = Si in Figure 6, then §1 gives the following responses: 
000000 H -- 
14 
= 
1100, 
000001 
H --* 
0 
= 
0000, 
100000 
1 -- 
4 
= 
0100, 
100001 H -
15 
= 
1 1 0 1 , 
etc. 
DES Encryption 
Divide the source message into binary words ν of length 64. They are 
encrypted by means of a key of length 64. The key, chosen by the user, 
produces 16 keys, ki, k 2 ) . . . , ki 6, of length 48 by means of the key schedule 
described below (see Figure 7). The encryption is performed in the following 
three steps (see Figure 5). 
Step 1: Permute the letters of ν by means of the permutation e in Figure 8. 
That is, the 58th bit goes to the first position, the 50th one to the 
second position, etc. 
Denote by lo and Vo the two halves of the 
resulting word e(v), i.e., lo and ro are words of length 32 such that 
e(v) = l 0 r 0 . 
Step 2: Compute words Ι1Γ1, l 2 r 2 , ... , ΙιβίΊβ as follows: the left half of 
each word is just the right half of the preceding one: 
l, = r;_, 
(1 = 1 , 2 , . . . , 16), 
whereas the right half is computed using the key k, as follows: 
r, = li-i + / ( r , _ i , k.) 
(addition in 
Zf), 
where / is the function described below. 

312 
CHAPTER 
15. 
CRYPTOGRAPHY 
so 
Si 
«i 
«2 
«3 
14 
4 13 
1 
2 15 11 
8 
3 10 
6 12 
5 
9 
0 
7 
0 15 
7 
4 14 
2 13 
1 10 
6 12 11 
9 
5 
3 
8 
4 
1 14 
8 13 
6 
2 11 15 12 
9 
7 
3 10 
5 
0 
15 12 
8 
2 
4 
9 
1 
7 
5 11 
3 14 10 
0 
6 13 
«0 
S 2 
*i 
«2 
«3 
15 
1 
8 14 
6 11 
3 
4 
9 
7 
2 13 12 
0 
5 10 
3 13 
4 
7 15 
2 
8 14 12 
0 
1 10 
6 
9 11 
5 
0 14 
7 11 10 
4 13 
1 
5 
8 12 
6 
9 
3 
2 15 
13 
8 10 
1 
3 15 
4 
2 11 
6 
7 12 
0 
5 14 
9 
«0 
S 3 
«ι 
«2 
«3 
10 
0 
9 14 
6 
3 15 
5 
1 13 12 
7 11 
4 
2 
8 
13 
7 
0 
9 
3 
4 
6 10 
2 
8 
5 14 12 11 15 
1 
13 
6 
4 
9 
8 15 
3 
0 11 
1 
2 12 
5 10 14 
7 
1 10 13 
0 
6 
9 
8 
7 
4 15 14 
3 11 
5 
2 12 
«0 
S 4
 
e> 
«2 
«3 
7 13 14 
3 
0 
6 
9 10 
1 
2 
8 
5 11 12 
4 15 
13 
8 11 
5 
6 15 
0 
3 
4 
7 
2 12 
1 10 14 
9 
10 
6 
9 
0 12 11 
7 13 15 
1 
3 14 
5 
2 
8 
4 
3 15 
0 
6 10 
1 13 
8 
9 
4 
5 11 12 
7 
2 14 
«0 
s5 ·> 
«2 
«3 
2 12 
4 
1 
7 10 11 
6 
8 
5 
3 15 13 
0 14 
9 
14 11 
2 12 
4 
7 13 
1 
5 
0 15 10 
3 
9 
8 
6 
4 
2 
1 11 10 13 
7 
8 15 
9 12 
5 
6 
3 
0 14 
11 
8 12 
7 
1 14 
2 13 
6 15 
0 
9 10 
4 
5 
3 
«0 
S 6
 
e i 
«2 
«3 
12 
1 10 15 
9 
2 
6 
8 
0 13 
3 
4 14 
7 
5 11 
10 15 
4 
2 
7 12 
9 
5 
6 
1 13 14 
0 11 
3 
8 
9 14 15 
5 
2 
8 12 
3 
7 
0 
4 10 
1 13 11 
6 
4 
3 
2 12 
9 
5 15 10 11 14 
1 
7 
6 
0 
8 13 
«0 
S 7
 
fli 
β 2 
«3 
4 11 
2 14 15 
0 
8 13 
3 12 
9 
7 
5 10 
6 
1 
13 
0 11 
7 
4 
9 
1 10 14 
3 
5 12 
2 15 
8 
6 
1 
4 11 13 12 
3 
7 14 10 15 
6 
8 
0 
5 
9 
2 
6 11 13 
8 
1 
4 10 
7 
9 
5 
0 15 14 
2 
3 12 
«0 
S 8 
«i 
«2 
«3 
13 
2 
8 
4 
6 15 11 
1 10 
9 
3 14 
5 
0 12 
7 
1 15 13 
8 10 
3 
7 
4 12 
5 
6 11 
0 14 
9 
2 
7 11 
4 
1 
9 12 14 
2 
0 
6 10 13 15 
3 
5 
8 
2 
1 14 
7 
4 10 
8 13 15 12 
9 
0 
3 
5 
6 11 
Figure 6: Matrices Si, S 2, ... , Se of permutations 

15.6. 
DATA ENCRYPTION 
STANDARD 
key k, length 64 
a(k), length 56 
lo 
ro 
1 shift 
1 shift 
li 
1 shift 
2 shifts 
1 shift 
function b 
Γ2 
Γ2 
2 shifts 
function b 
Γ3 
function 6 
Figure 7: Key schedule 

314 
CHAPTER 15. 
CRYPTOGRAPHY 
58 50 42 
60 52 44 
62 54 
64 56 
57 49 
59 51 
61 53 45 
63 55 47 
10 
46 
48 
41 
43 
34 26 18 
36 28 20 12 4 
38 30 22 14 6 
40 32 24 16 8 
33 25 17 
35 27 19 11 
37 29 21 
39 31 23 
13 
15 
40 8 
39 7 
38 6 
37 5 
36 
35 
34 
33 
48 16 56 24 64 32 
47 15 55 23 63 31 
46 14 54 22 62 30 
45 13 53 21 61 29 
44 12 52 20 60 28 
43 11 51 19 59 27 
42 10 50 18 58 26 
41 
9 49 17 57 25 
permutation e 
inverse permutation c
-
1 
Figure 8: Permutations e and e
_ 1 
Step 3: Interchange the two halves of the last word, i.e., form the word 
rieli6> and apply the permutation e
- 1 (see Figure 8). 
The function f. We are going to describe a function / which assigns to 
a binary word r of length 32 and a key k of length 48 a binary word / ( r , k) 
of length 32. It is shown in Figure 9. 
word r, length 32 
function g 
g(r), length 48 
S i 
S
3 
word k, length 48 
S
4 
S
8 
permutation h 
output / ( r , k ) , length 32 
Figure 9: Function / 

15.6. 
DATA ENCRYPTION 
STANDARD 
315 
32 
1 
2 
3 
4 
5 
4 
5 
6 
7 
8 
9 
8 
9 
10 
11 
12 
13 
12 
13 
14 
15 
16 
17 
16 
17 
18 
19 
20 
21 
20 
21 
22 
23 
24 
25 
24 
25 
26 
27 
28 
29 
28 
29 
30 
31 
32 
1 
Figure 10: Function g 
Step 1: Turn the word r into a word g(r) of length 48 by the function g 
in Figure 10. That is, the 32nd bit of r goes to the first position, the 
first bit to the second position, etc. 
Step 2: Add o(r) and k in Z
4,
8. Divide the resulting word into eight words 
of length 6. 
Step 3: Apply to the eight words of Step 2 the following eight functions 
Si, S 2, ... , Sg, respectively, in order to compute a word of length 8 χ 
4 = 32. The functions S, are denned by means of the matrices S, in 
Figure 6 (whose rows are permutations of the numbers 0, 1, . . . , 15) 
in the manner described in the introduction of the present section. 
Step 4: Perform the permutation h of Figure 11. That is, the 16th bit 
goes to the first position, the 7th one to the second position, etc. 
17 
7 
20 
21 
29 
12 
28 
17 
1 
15 
23 
26 
5 
18 
31 
10 
2 
8 
24 
14 
32 
27 
3 
9 
19 
13 
30 
6 
22 
11 
4 
25 
Figure 11: Permutation ft 
Key schedule. The user chooses a key k of 64 bits: 56 bits are chosen 
at random and 8 bits are parity check bits, serving to detect an error per 
8 bits. This is performed by setting every eighth bit in such a way that, 
together with the seven preceding bits, it forms a word of odd parity. The 
key k is used to compute words ki, k 2, ... , ki 6 as follows (see Figure 10). 

316 
CHAPTER 15. 
CRYPTOGRAPHY 
Step 1: Apply the function a in Figure 12 (which is a permutation of the 
56 information bits of the key k): the 57th bit of k goes to the first 
position, the 49th bit to the second one, etc. Split the result a(k) in 
two halves, a(k) = lorn. 
Step 2: Form k, = 6(l,r,), i = 1, 2, ... , 16, where b is the function of 
Figure 13 assigning to each word of length 56 a word of length 48, and 
lirj, ^ r j , ... , 1]6Γΐ6 are defined as follows: 1,- is obtained from 
by n,- cyclic shifts to the left, and r,- is obtained from r,_j by n, cyclic 
shifts to the left, where 
_ J 1 if t = 1, 2, 9, 16, 
"* ~ \ 2 
else. 
The key schedule is indicated in Figure 7. 
57 49 41 33 25 17 
9 
1 58 50 42 34 26 18 
59 51 43 35 27 
3 60 52 44 36 
63 55 47 39 31 23 15 
7 62 54 46 38 30 22 
6 61 53 45 37 29 
28 20 12 
4 
10 
19 
2 
11 
14 
21 13 
14 17 11 24 
1 
5 
3 28 15 
6 21 10 
23 19 12 
4 26 
8 
16 
7 27 20 13 
2 
41 52 31 37 47 55 
30 40 51 45 33 48 
44 49 39 56 34 53 
46 42 50 36 29 32 
Figure 12: Function a 
Figure 13: Function 6 
Exercises 
15A 
In the encryption method using linear shift sequences (15.2), prove 
that the following system of equations 
V*+2 
Vk+m-1 
1>k+m 
Vk+m+l 
V*+m+2 
" * + 2 m - 2 
Λι 
— 
Vk+m+l 
. t>i+2m-l -
holds and makes it possible to find h(x) whenever we know v* .. njfc+2 m-i. 

NOTES 
317 
15B 
Prove that the prime-number method in 15.4 has the property that 
anyone who determines the secret number φ(η) knows the factorization 
of n: since pq = η is known, and ρ + q = η — φ(η) + 1, it is easy to find ρ 
and q. 
Notes 
Using a secret key for cryptography is an obvious and old method. The 
trick of using error-correcting codes in the case of a noisy wiretap is due 
to Wyner (1975). Pseudo-random keys are discussed by MacWilliams and 
Sloane (1976). 
A pioneering paper in cryptography, which introduced the idea of a 
public key, is Diffie and Hellman (1976). The first satisfactory public-key 
cryptosystem, based on large primes, was presented by Rivest, Shamir, and 
Adelman (1978). Soon later the knapsack-problem cryptosystem was pro-
posed by Merkle and Hellman (1978). The cryptosystem based on Goppa 
codes is due to McEliece (1978). The Data Encryption Standard was cre-
ated in 1977 by IBM, and then adopted by the National Bureau of Stan-
dards. 
For more information about the topics of this chapter, the reader is 
referred to the excellent textbook of van Tilborg (1988). 

Appendixes 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

Appendix A 
Galois Fields 
We present fields GF(p
m) 
= Z p[x]/mod r(x) (see 11.3), where r(x) is an 
irreducible polynomial of degree η > 1. Every nonzero element is expressed 
(a) as a polynomial in indeterminate a (of degree lower than n) and (b) as 
a power of a. All finite fields of at most 32 elements are represented here 
(except the trivial case of Z p , ρ a prime). 
GF(4) = Z 2[x]/mod χ
2 + χ + 1 
0 
1 
a 
l + a 
- 
a
0 = a
3 
a
1 
a
2 
GF(8) = Z 3[x]/mod χ
3 + χ + 1 
0 
1 
ο 
a
2 
l + a 
a + a
2 
1 + a + a
2 
l + a
2 
- 
a
0 = a
7 
a
1 
a
2 
a
3 
a
4 
a
5 
a
6 
321 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

322 
APPENDIX 
A. GALOIS 
FIELDS 
GF(9) = Z 3I>]/ mod χ
2 + χ + 2 
0 
1 
α 
1 + 2α 
2 + 2α 
2 
2 α 2 + α 1 + α 
- 
ο° = α
8 
α
1 
β
2 
α
3 
α
4 
α
5 
α
β 
α
7 
GF(16) = Z 2[x]/mod χ
4 + χ + 1 
0 
1 
α 
α
2 
-
α 0 
_ 
α 1 5 
α
1 
α
2 
α
3 
1 + α 
α + ο
2 
α
2 + α
3 
α
3 
α
4 
α
5 
α
6 
1 + α + α
3 
1 + α
2 
α + α
3 
1 + α + α
2 
α
7 
α
8 
α
9 
α
1 0 
α + α
2 + α
3 
1 + α + α
2 + α
3 
1 + α
2 + α
3 
1 + α
3 
αιι 
α
1 2 
α
1 3 
α
1 4 
GF(25) = Ζ 6[χ]/ mod χ
2 + χ + 2 
0 
1 
α 
4 α + 3 
4α + 2 
3 α + 2 
4 α + 4 
-
α° = α
2 4 
α
1 
α
2 
α
3 
α
4 
α
5 
2 
2α 
3α + 1 
3 α + 4 
α + 4 
3 α + 3 
4 
α
β 
α
7 
α
8 
α
9 
ο
1 0 
α
1 1 
α
1 2 
4α 
α + 2 
α + 3 
2 α + 3 
α + 1 
3 
3α 
α
1 3 
α
1 4 
α
1 5 
α
1 6 
α
1 7 
α
1 8 
α
1 9 
2 α + 4 
2α + 1 
4α + 1 
2 α + 2 
α
2 0 
α
2 1 
α
2 2 
α
23 

APPENDIX 
A. GALOIS FIELDS 
3 2 3 
GF(21) = Z3[x]/ 
m o d x
3 + 2x + 1 
0 
1 
a 
a
2 
a + 2 
-
a
0 = a
2
6 
a
1 
a
2 
a
3 
a
2 + 2a 
2 a
2 + a + 2 
a
2 + a + 1 
a
2 + 2 a + 2 
2 a
2 + 2 
a
4 
a
5 
a
6 
a
7 
a
8 
a + 1 
a
2 + a 
a
2 + a + 2 
a + a
2 
2 
a
9 
a
1
0 
a
1
1 
a
1
2 
a
1
3 
2 a 
2 a
2 
o a + 1 
2 a
2 + a 
a
2 + 2 a + 1 
a
1
4 
a
1
5 
a
1
6 
a
1
7 
a
1
8 
2 a
2 + 2 a + 2 
2 a
2 + a + 1 
a
2 + l 
2 a + 2 
2 a
2 + 2 a 
a
1
9 
a
2
0 
a
2
1 
a
2
2 
a
2
3 
2 a
2 + 2 a + 1 
2 a
2 + 1 
a
2
4 
a
2
5 

324 
APPENDIX A. GALOIS 
FIELDS 
GF(32) = Z 2[x]/mod z
8 + x
2 + 1 
0 
1 
a 
a
2 
-
a
0 = a
3 1 
a 
a
2 
a
3 
a
4 
a
2 + l 
a
3 + a 
a
3 
a
4 
a
5 
a
6 
a
4 + a
2 
1 + a
2 + a
3 
a + a
3 + a
4 
1 + a
4 
a
7 
a
8 
a
9 
a
1 0 
1 + a + a
2 
a + a
2 + a
3 
a
2 + a
3 + a
4 1 + a
2 + a
3 + a
4 
a
1 1 
a
1 2 
a
1 3 
a
1 4 
1 + a + a
2 + a
3 + a
4 1 + a + a
3 + a* 
1 + a + a
4 
1 + a 
a
1 6 
a
1 7 
a
1 8 
a + a
2 
a
2 + a
3 
a
2 + a
4 
1 + a
2 + a
4 
a
1 9 
a
2 0 
a " 
a
2 2 
1 + a + a
2 + a
3 
a + a
2 + a
3 + a
4 1 + a
3 + a
4 
1 + a + a
2 + a
4 
a " 
a
2 4 
a
2 8 
a
2 6 
1 + a + a
3 
a + a
2 + a
4 
1 + a
3 
a + a
4 
a
2 7 
a
2 8 
a
2 9 
a
3 0 

Appendix Β 
BCH Codes and 
Reed-Muller Codes 
We list parameters of all BCH codes (Chapter 12) and all punctured Reed-
Muller codes 7c(r,m) (see 9.3) of lengths η = 7, 15, 31, 63, 127. The 
number of information symbols is denoted by ib. 
Length 
Minimum 
BCH codes 
tt(r, m) 
distance 
ib 
Jb 
Γ 
7 
3 
4 
4 
1 
7 
1 
1 
0 
15 
3 
11 
11 
2 
5 
7 
-
-
7 
5 
5 
1 
15 
1 
1 
0 
31 
3 
26 
26 
3 
5 
21 
-
-
7 
16 
16 
2 
11 
11 
-
-
15 
6 
6 
1 
31 
1 
1 
0 
325 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

326 
APPENDIX 
Β. BCB CODES AND REED-MULLER 
CODES 
Length 
Minimum 
BCH codes 
m) 
distance 
k 
ib 
r 
63 
3 
57 
57 
4 
5 
51 
-
-
7 
45 
42 
3 
7 
39 
-
-
11 
36 
-
-
13 
30 
-
-
15 
24 
22 
2 
21 
18 
-
-
23 
16 
-
-
27 
10 
-
-
31 
7 
7 
1 
63 
1 
1 
0 
127 
3 
120 
120 
5 
5 
113 
-
-
7 
106 
99 
4 
9 
99 
-
-
11 
92 
-
-
13 
85 
-
-
15 
78 
64 
3 
19 
71 
-
-
21 
64 
-
-
23 
57 
-
-
27 
27 
-
-
31 
43 
-
-
31 
36 
29 
2 
43 
29 
-
-
47 
22 
-
-
55 
15 
-
-
63 
8 
8 
1 
127 
1 
1 
0 

Bibliography* 
Abramson, Ν. (1963) Information Theory and Coding. McGraw-Hill, New York 
[Ch. 1-4]. 
Berlekamp, E. R. (1968) Algebraic Coding Theory. 
McGraw-Hill, New York 
[Ch. 5-14]. 
Berlekamp, E. R., editor. (1974) Key Papers in the Development of Coding The-
ory. IEEE Press, New York [Ch. 5-14]. 
Birkhoff, G., and MacLane, S. (1953) A Survey of Modern Algebra. Macmillan, 
New York [Ch. 6, 7, 11]. 
Blahut, R. E. (1983) Theory and Practise of Error Control Codes. 
Addison-
Wesley, Reading, Mass. [Ch. 5-14]. 
Bose, R. C , and Ray-Chaudhuri, D. K. (1960) On a Class of Error Correcting 
Binary Group Codes. Inform. Control 3, pp. 68-79 [Ch. 12]. 
Diffie, W., and Hellman, Μ. E . (1976) New Directions in Cryptography. 
IEEE 
Trans. Inf. Theory IT-22, pp. 644-654 [Ch. 15]. 
EUas, P. (1954) Error-Free Coding. IEEE Trans. Inf. 
Theory IT-4, pp. 29-37 
[Ch. 14]. 
Fadeiev, D. K. (1956) To the Concept of Entropy of a Finite Probability Distri-
bution (Russian), Uspechy Matem. Nauk 11, pp. 1-10. 
Forney, G. D., Jr. (1970) Convolutional Codes I: Algebraic Structure. 
IEEE 
Trans. Inf. Theory IT-16, pp. 720-738 [Ch. 14]. 
Golay, M. J . E. (1949) Notes on Digital Coding. Proc. IEEE 37, p. 657 [Ch. 5]. 
Golomb, S. W. (1967) Shift Register Sequences. 
Holden-Day, San Francisco 
[Ch. 15]. 
Goppa, V. C. (1970) A New Class of Linear Error-Correcting Codes. 
Prob/. 
Peredach. Inf. 6, pp. 24-30 [Ch. 12]. 
Gorenstein, D. C , and Ziegler, N. (1961) A Class of Error-Correcting Codes in 
p
m Symbols. /. Soc. Indus. Applied Math. 9, pp. 207-214 [Ch. 12]. 
* At the end of each reference, we put in brackets the chapter(s) to which it is related. 
327 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

328 
BIBLIOGRAPHY 
Hamming, R. W. (1950) Error Correcting and Error Detecting Codes. Bell 
Syst. 
Tech. 
J . 29, pp. 147-160 [Ch. 5]. 
Hamming, R. W. (1980) 
Coding 
and Information 
Theory. 
Prentice-Hall, Engle-
wood Cliffs, New Jersey [Ch. 1-4]. 
Hocquenghem, A. (1959) Codes corecteure d'erreurs. 
Chiffres 
2, pp. 147-156 
[Ch. 12]. 
Karush, J . (1961) A Simple Proof of an Inequality of McMillan. 
I E E E 
Trani. 
Inform. 
Theory 
IT-7, p. 118 [Ch. 1]. 
Kasami, T. (1964) A Decoding Procedure for Multiple-Error-Correcting Cyclic 
Codes. IEEE 
Tram. 
Inf. Theory 
IT-10, pp. 134-138 [Ch. 10]. 
Kraft, L. G. (1949) A Device 
for Quantizing, 
Grouping, 
and Coding 
Amplitude 
Modulated 
Pulses. 
M.S. thesis, Electrical Engineering Department, Mas-
sachusetts Institute of Technology, March [Ch. 1]. 
Lin, S., and Costello, D. J. (1983) Error-Control 
Coding. 
Prentice-Η all, Engle-
wood Cliffs, New Jersey [Ch. 5-14]. 
Mac Williams, F. J . (1963) A Theorem on the Distribution of Weights in a Sys-
tematic Code. Bell 
Syst. 
Tech. 
J . 42, pp. 79-94 [Ch. 8]. 
MacWilliame, F. J . , and Sloane, N. J. A. (1976) Pseudo-Random Sequences and 
Arrays. Proc. 
IEEE 
64, pp. 1715-1729 [Ch. 15]. 
MacWilliame, F. J . , and Sloane, N. J . A. (1981) T h e Theory 
of 
Error-Correc-
ting 
Codes 
( 3
r d printing). North-Holland, Amsterdam [Ch. 5-14]. 
Massey, J . L., and Sain, Μ. K. (1968) 
Inverses of Linear Sequential Circuits. 
IEEE 
Trans. 
Comp. 
C-17, pp. 330-337 [Ch. 14]. 
McEliece, R. J . (1978) A Public-Key Cryptosystern Based on Algebraic Coding 
Theory. Deep Space Network Progress Report 42-44, Jet Propulsion Labo-
ratories, Pasadena, California, pp. 114-116 [Ch. 15]. 
McMillan, B. (1956) Two Inequalities Implied by Unique Decipherability. I R E 
Trans. 
Inform. 
Theory 
IT-2, pp. 115-116 [Ch. 1]. 
Meggit, J . E. (1960) Error-Correcting Codes for Correcting Bursts of Errors, 
IBM 
J . Res. Develop. 
4, pp. 329-334 [Ch. 10]. 
Merkle, R. C , and Hellman, Μ. E. (1978) Hiding Information and Signatures in 
Trapdoor Knapsacks. I E E E Trans. 
Inf. Theory 
IT-24, pp. 525-530 [Ch. 15]. 
Muller, D. E. (1954) Application of Boolean Algebra to Switching Circuit Design 
and to Error Detection. IEEE 
Trans. 
Comp. 
3, pp. 6-12 [Ch. 9]. 
Patterson, N. J . (1975) The Algebraic Decoding of Goppa Codes. I E E E 
Trans. 
Inf. 
Theory 
21, pp. 203-207 [Ch. 12]. 
Peterson, W. W. (1960) Encoding and Error-Correcting Procedures for the Bose-
Chaudhuri Codes. I E E E 
Trans. 
Inf. Theory 
IT-16, pp. 459-470 [Ch. 12]. 

BIBLIOGRAPHY 
329 
Prange, Ε. (1957) 
Cyclic 
Error-Correcting 
Codes 
in Two Symbols. 
A F C R C - T N -
57-103, Aii Force Cambridge Research Center, Cambridge, Massachusetts 
[Ch. 10]. 
Reed, I. S. (1954) A Class of Multiple-Error Correcting Codes and the Decoding 
Scheme. IEEE 
Trans. 
Inf. Theory 
4, pp. 38-49 [Ch. 9]. 
Reed, I. S., and Solomon, G. (1960) Polynomial Codes over Certain Finite Fields. 
J. Soc. Indus. 
Applied 
Math. 
8, pp. 300-304 [Ch. 12]. 
Rivest, R. L., Shamir, Α., and Adelman, L. M. (1978) A Method for Obtaining 
Digital Signatures and Public-Key Cryptosystems. Comm. 
ACM 21, pp. 
120-126 [Ch. 15]. 
Shannon, C. E. (1948) A Mathematical Theory of Communication. 
Bell 
Syst. 
T e c h . J . 27, pp. 379-423 and 623-656 [Ch. 1-4]. 
Slepian, D. (1956) A Class of Binary Signalling Alphabets. 
Bell 
Syst. 
T e c h . 
J . 35, pp. 203-234 [Ch. 5]. 
Slepian, D. (1960) Some Further Theory of Group Codes. Bell 
Syst. 
T e c h . J . 39, 
pp. 1219-1252 [Ch. 5]. 
Slepian, D., editor. (1974) Key Papers 
in the Development 
of Information 
The-
ory. 
IEEE Press, New York [Ch. 1-4]. 
Solovay, R., and Strassen, V. (1977) A Fast Monte-Carlo Test for Primality. 
SIAM 
J. Comp. 
6, pp. 84-85 [Ch. 15]. 
Sugiyama, Y., Kasahara, M., Hirasawa, S., and Namekawa, T. (1975) A Method 
for Solving Key Equation for Decoding Goppa Codes. Inf. and Control 
27, 
pp. 87-99 [Ch. 13]. 
Sugiyama, Y., Kasahara, M., Hirasawa, S., and Namekawa, T. (1976) An Era-
sures-and-Errors Decoding Algorithm for Goppa Codes, I E E E 
Trans. 
Inf. 
Theory 
IT-22, pp. 238-241. 
Tietavainen, A. (1974) A Short Proof for the Nonexistence of Unknown Perfect 
Codes ofer GF(q), 
q > 2, Annates 
Acad. 
Scient. 
Fenniciae, 
ser. A, 580, 
pp. 1-6 [Ch. 10]. 
van Lint, J . H. (1982) Introduction 
to Coding 
Theory. 
Springer, New York-
Heidelberg-Berlin [Ch. 5-14]. 
van Tilborg, H. C. A. (1988) An Introduction 
to Cryptology. 
Kluwer, Boston-
Dordrecht-Lancaster [Ch. 15]. 
Viterbi, A. J . (1967) Error Bounds for Convolutional Codes and an Asymptot-
ically Optimum Decoding Algorithm. 
I E E E 
Trans. 
Inf. Theory 
IT-13, 
pp. 260-269 [Ch. 14]. 
Wolfowitz, J . (1959) Strong Converse of the Coding Theorem for Semi-continu-
ous Channels, Illinois 
J . Math. 
3, pp. 477-489. 
Wozencraft, J . M. (1957) 
Sequential Decoding for Reliable Communication. 
Nat. 
IRE Conv. 
Rec. 5, pp. 11-25 [Ch. 14]. 
Wyner, A. D. (1975) The Wire-Tap Channel. Bell 
Syst. 
T e c h . J . 54, pp. 1355-
1387 [Ch. 15]. 

List of Symbols 
d48 
dfree 281 
e 66 
G 116 
g(x) 165 
Η 29, 53 
h(x) 172 
/ 53 
I 107 
L
x 
108 
Pund 75 
pZ 83 
R 43 
η 137 
R 8 0 
β 121 
s(z) 175 
Ζ 80 
Z p 80 
= 
(mod p) 84, 205 
331 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

Index 
active path 286 
adder 167 
addition modulo 2 63 
algebraic extension 203 
asymptoticaly good codes 254 
augumentation 127 
autoregulation 301 
average length 18 
basis 98 
BCH code 239 
binary adder 72 
binary operation 79 
binary symmetric channel 40 
binomial coefficients 223 
binomial theorem 223 
bit 29 
block code 43 
Boolean function 138 
Boolean polynomial 141 
burst-error 180, 185, 245 
capacity 54 
channel 40, 50 
characteristic 211 
characteristic function 150, 152 
check bit 43 
check symbol 39 
code 6 
ASCII 9 
BCH 239 
block 7 
code (continued) 
burst-error-correcting 
180, 
189, 245 
convolutional 271 
cyclic 161 
dual 120 
even-parity 43 
expurgated 125 
extended 125 
Fire 189 
generalized Reed-Muller 246 
Golay 184, 185 
Goppa 248, 303 
Hamming 69, 134, 230 
hexadecimal 9 
instantaneous 7 
irreducible Goppa 252 
ISBN 11 
linear 66, 115 
Morse 7 
nontrivial 43 
octal 8 
perfect 74, 183 
punctured 127 
Reed-Muller 24 
Reed-Solomon 245 
simplex 121, 297 
systematic 117 
2-out-of-5 5 
trivial 43 
uniquely decodable 7 
code alphabet 5 
code frame 280 
333 
Foundations of Coding: Theory and Applications of Error-Correcting Codes 
with an Introduction to Cryptography and Information Theory 
by Jiri Adämek 
Copyright © 1991 John Wiley & Sons, Inc. 

334 
INDEX 
code word 6, 279 
coding 5 
of source messages 6 
coherence 27 
commutative group 79 
conditional entropy 53 
congruent modulo 84, 93 
consecutive positions 181 
constraint length 277 
continuity 27 
convolutional code 271 
correct errors 49, 279 
correct burst errors 186 
coset 81 
coset leader 83, 123 
coset modulo ρ 93 
cryptography 29 
cyclic code 161 
Data Encryption Standard 309 
decryption 293 
degree 141, 162 
derivative 224 
detect errors 48 
detect burst errors 185 
dimension 98 
discrepancy 286 
division of polynomial 163 
dual code 121 
effectivity 37 
elementary row operation 102 
encoding 72 
encryption 293 
entropy 29 
equivalent codes 118 
error evaluation 258 
error location 258 
error pattern 66, 121 
error probability 40 
error trapping 181 
Euclidean algorithm 260 
Euclidean geometry 148, 151 
Euler-Fermat Theorem 218 
Euler function 217 
even-parity code 43 
expurgation 127 
extension 125 
of a source 32 
Fermat's Theorem 210 
field 91 
finite-dimensional 98 
Fire code 198 
fiat 150, 151 
free distance 279 
Galois field 206 
generalized Reed-Muller code 245 
generating polynomial 165, 275 
generator matrix 116, 271, 275 
Golay code 184 
Goppa code 248, 303 
greatest common divisor 261 
group 79 
Hamming code 69, 134, 230 
Hamming distance 46 
Hamming weight 66 
hexadecimal code 9 
Huffman code 18 
hyperplane 151 
identity matrix 107 
independence theorem 237 
information 26 
bit 23 
frame 280 
rate 43 
information source 17 
inner product 105, 108 
instantaneous 7 
interleaving 187 
inverse 79 

INDEX 
335 
inverse matrix 107 
irreducible polynomial 198 
irreducible Goppa code 252 
ISBN 11 
isomorphic 88, 219 
key 296 
knapsack problem 307 
Kraft's inequality 12 
linear 
code 115 
combination 97 
space 65 
subspace 96 
linearly independent 98 
logarithm 28 
MacWilliams identity 130 
matrix 101 
maximum-likelihood decoding 48 
McMillan's Theorem 13 
Meggitt decoder 176 
memory 279 
metric 46 
minimal polynomial 213 
minimum distance 48 
minimum weight 66 
monic polynomial 162 
Morse code 7 
multiple zero 224 
multiplication modulo 2 63 
mutual information 53 
neutral element 79 
octal code 8 
opposite element 79 
order 207 
orthogonal 108 
complement 108 
parity 9 
parity check 
matrix 68, 119 
polynomial 172 
perfect code 74, 183 
period 189 
polynomial 162 
Boolean 141 
positivity 27 
power series 225 
primitive 
BCH code 240 
element 208 
polynomial 226 
pseudo-random key 297 
public key 303 
puncturing 127 
quotient 163 
rectangular code 64 
reduced source 19 
redundancy 39 
Reed-Muller code 144 
generalized 246 
Reed-Solomon code 245 
regular matrix 107 
remainder 163 
repetition code 41 
ring 92 
row space 102 
row-echelon form 102 
run 300 
scalar 95, 167 
Shannon's Fundamental Theorem 
56 
Shannon's Noiseless Coding The-
orem 34 
shift-register stage 72, 167 
simultaneous detection and cor-
rection 128 

336 
INDEX 
simplex code 121, 297 
span 97 
square matrix 106 
standard array 84 
state 281 
source alphabet 5 
subgroup 81 
symmetry 27 
syndrome 72, 121, 257 
decoding 123 
polynomial 175 
systematic 117 
transpose 106 
trellis diagram 283 
triangle inequality 46 
trivial code 43 
trivial subspace 96 
unique factorization theorem 199 
uniquely decodable 7 
Vandermonde matrices 242 
vector 95 
Viterbi algorithm 283 
weight enumerator 76 
word 5 
zero 198 
zero word 66 

