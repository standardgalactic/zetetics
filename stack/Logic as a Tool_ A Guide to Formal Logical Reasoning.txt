

LOGIC AS A TOOL


LOGIC AS A TOOL
A GUIDE TO FORMAL LOGICAL REASONING
Valentin Goranko
Stockholm University, Sweden

This edition first published 2016
© 2016 by John Wiley & Sons, Ltd
Registered office
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United
Kingdom
For details of our global editorial offices, for customer services and for information about how to apply
for permission to reuse the copyright material in this book please see our website at www.wiley.com.
The right of the author to be identified as the author of this work has been asserted in accordance with
the Copyright, Designs and Patents Act 1988.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or
transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or
otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior
permission of the publisher.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print
may not be available in electronic books.
Designations used by companies to distinguish their products are often claimed as trademarks. All
brand names and product names used in this book are trade names, service marks, trademarks or
registered trademarks of their respective owners. The publisher is not associated with any product or
vendor mentioned in this book.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts
in preparing this book, they make no representations or warranties with respect to the accuracy or
completeness of the contents of this book and specifically disclaim any implied warranties of
merchantability or fitness for a particular purpose. It is sold on the understanding that the publisher is
not engaged in rendering professional services and neither the publisher nor the author shall be liable
for damages arising herefrom. If professional advice or other expert assistance is required, the services
of a competent professional should be sought.
Library of Congress Cataloging-in-Publication Data
Names: Goranko, Valentin, author.
Title: Logic as a tool : a guide to formal logical reasoning / Valentin
Goranko.
Description: Chichester, UK ; Hoboken, NJ : John Wiley & Sons, 2016. |
Includes bibliographical references and index.
Identifiers: LCCN 2016010458 (print) | LCCN 2016014532 (ebook) | ISBN 9781118880005 (cloth) |
ISBN 9781118880050 (pdf) | ISBN 9781118880043 (epub)
Subjects: LCSH: Logic–Textbooks.
Classification: LCC BC71 .G67 2016 (print) | LCC BC71 (ebook) | DDC
511.3–dc23
LC record available at http://lccn.loc.gov/2016010458
A catalogue record for this book is available from the British Library.
Set in 10.5/12pt, TimesLTStd by SPi Global, Chennai, India.
1
2016

This book is dedicated to those from whom I have learned
and to those who will learn from it.


Contents
Preface
xi
Acknowledgements
xv
Introduction
xvii
An Appetizer: Logical Paradoxes and Self-Reference
xxi
1
Understanding Propositional Logic
1
1.1
Propositions and logical connectives: truth tables and tautologies
1
1.1.1
Propositions
1
1.1.2
Propositional logical connectives
2
1.1.3
Truth tables
3
1.1.4
The meaning of the connectives in natural language and in logic
4
1.1.5
Computing truth values of propositions
5
1.1.6
Propositional formulae and their truth tables
6
1.1.7
Tautologies
11
1.2
Propositional logical consequence: logically correct inferences
18
1.2.1
Propositional logical consequence
18
1.2.2
Logically sound rules of propositional inference and logically
correct propositional arguments
21
1.2.3
Fallacies of the implication
23
1.3
Logical equivalence: negation normal form of propositional formulae
28
1.3.1
Logically equivalent propositional formulae
28
1.3.2
Basic properties of logical equivalence
29
1.3.3
Some important logical equivalences
29
1.4
Supplementary: Inductive definitions and structural induction and recursion
34
1.4.1
Inductive definitions
34
1.4.2
Induction principles and proofs by induction
36
1.4.3
Basics of the general theory of inductive definitions and principles
37
1.4.4
Inductive definitions and proofs in well-founded sets
39
1.4.5
Recursive definitions on inductively definable sets
40

viii
Contents
2
Deductive Reasoning in Propositional Logic
47
2.1
Deductive systems: an overview
47
2.1.1
The concept and purpose of deductive systems
47
2.1.2
Brief historical remarks on deductive systems
48
2.1.3
Soundness, completeness and adequacy of deductive systems
50
2.2
Axiomatic systems for propositional logic
52
2.2.1
Description
52
2.2.2
Derivations in the axiomatic system H
54
2.3
Semantic Tableaux
58
2.3.1
Description of the deductive system ST of Semantic Tableaux
59
2.3.2
Some derivations in ST
61
2.3.3
Unsigned version of the system of Semantic Tableaux
64
2.4
Natural Deduction
68
2.4.1
Description
69
2.4.2
Examples of derivations in Natural Deduction
71
2.5
Normal forms and Propositional Resolution
77
2.5.1
Conjunctive and disjunctive normal forms of propositional formulae
77
2.5.2
Clausal Resolution
79
2.5.3
Resolution-based derivations
80
2.5.4
Optimizing the method of resolution
82
2.6
Supplementary: The Boolean satisfiability problem and NP-completeness
86
2.7
Supplementary: Completeness of the propositional deductive systems
88
3
Understanding First-order Logic
96
3.1
First-order structures and languages: terms and formulae of first-order logic
97
3.1.1
First-order structures
97
3.1.2
First-order languages
99
3.1.3
Terms and formulae
100
3.2
Semantics of first-order logic
108
3.2.1
The semantics of first-order logic: an informal outline
108
3.2.2
Interpretations of first-order languages
111
3.2.3
Variable assignment and evaluation of terms
112
3.2.4
Truth of first-order formulae
112
3.2.5
Evaluation games
114
3.2.6
Translating first-order formulae to natural language
117
3.3
Basic grammar and use of first-order languages
123
3.3.1
Translation from natural language to first-order languages:
warm-up
123
3.3.2
Restricted quantification
124
3.3.3
Free and bound variables, and scope of a quantifier
125
3.3.4
Renaming of a bound variable in a formula and clean formulae
127
3.3.5
Substitution of a term for a variable in a formula, and capture of a
variable
128
3.3.6
A note on renamings and substitutions in a formula
130

Contents
ix
3.4
Logical validity, consequence, and equivalence in first-order logic
135
3.4.1
More on truth of sentences in structures: models and
counter-models
135
3.4.2
Satisfiability and validity of first-order formulae
136
3.4.3
Logical consequence in first-order logic
137
3.4.4
Using equality in first-order logic
140
3.4.5
Logical equivalence in first-order logic
142
3.4.6
Logical equivalences involving quantifiers
143
3.4.7
Negating first-order formulae: negation normal form
144
3.5
Syllogisms
151
4
Deductive Reasoning in First-order Logic
159
4.1
Axiomatic system for first-order logic
160
4.1.1
Axioms and rules for the quantifiers
160
4.1.2
Derivations from a set of assumptions
160
4.1.3
Extension of the axiomatic system H with equality
161
4.2
Semantic Tableaux for first-order logic
167
4.2.1
Some derivations in Semantic Tableaux
168
4.2.2
Semantic Tableaux for first-order logic with equality
171
4.2.3
Discussion on the quantifier rules and on termination
173
4.3
Natural Deduction for first-order logic
180
4.3.1
Natural Deduction rules for the quantifiers
180
4.3.2
Derivations in first-order Natural Deduction
181
4.3.3
Natural Deduction for first-order logic with equality
183
4.4
Prenex and clausal normal forms
187
4.4.1
Prenex normal forms
187
4.4.2
Skolemization
189
4.4.3
Clausal forms
190
4.5
Resolution for first-order logic
194
4.5.1
Propositional Resolution rule in first-order logic
194
4.5.2
Substitutions of terms for variables revisited
195
4.5.3
Unification of terms
196
4.5.4
Resolution with unification in first-order logic
197
4.5.5
Examples of resolution-based derivations
199
4.5.6
Resolution for first-order logic with equality
201
4.5.7
Optimizations and strategies for the method of Resolution
202
4.6
Supplementary: Soundness and completeness of the deductive systems for
first-order logic
210
4.6.1
First-order theories
211
4.6.2
Soundness
212
4.6.3
Herbrand structures and interpretations
212
4.6.4
Henkin theories and Henkin extensions
214
4.6.5
Completeness theorem
217
4.6.6
Semantic compactness of first-order logic
217

x
Contents
5
Applications: Mathematical Proofs and Automated Reasoning
222
5.1
Logical reasoning and mathematical proofs
223
5.1.1
Proof strategies: direct and indirect proofs
223
5.1.2
Tactics for logical reasoning
227
5.2
Logical reasoning on sets, functions, and relations
231
5.2.1
Zermelo–Fraenkel axiomatic theory of sets
231
5.2.2
Basic operations on sets and their properties
234
5.2.3
Functions
236
5.2.4
Binary relations and operations on them
237
5.2.5
Special binary relations
239
5.2.6
Ordered sets
240
5.3
Mathematical Induction and Peano Arithmetic
246
5.3.1
Mathematical Induction
247
5.3.2
Peano Arithmetic
250
5.4
Applications: automated reasoning and logic programming
254
5.4.1
Automated reasoning and automated theorem proving
254
5.4.2
Logic programming and Prolog
255
6
Answers and Solutions to Selected Exercises
263
Answers and solutions: Section 1.1
263
Answers and solutions: Section 1.2
266
Answers and solutions: Section 1.3
268
Answers and solutions: Section 1.4
270
Answers and solutions: Section 2.2
270
Answers and solutions: Section 2.3
272
Answers and solutions: Section 2.4
281
Answers and solutions: Section 2.5
287
Answers and solutions: Section 3.1
293
Answers and solutions: Section 3.2
296
Answers and solutions: Section 3.3
297
Answers and solutions: Section 3.4
299
Answers and solutions: Section 3.5
305
Answers and solutions: Section 4.1
306
Answers and solutions: Section 4.2
308
Answers and solutions: Section 4.3
325
Answers and solutions: Section 4.4
328
Answers and solutions: Section 4.5
329
Answers and solutions: Section 4.6
338
Answers and solutions: Section 5.1
339
Answers and solutions: Section 5.2
339
Answers and solutions: Section 5.3
344
Answers and solutions: Section 5.4
347
References
348
Index
351

Preface
Unlike most books and textbooks on logic, this one purports to teach logic not so much
as a subject to study, but rather as a tool to master and use for performing and structuring
correct reasoning. It introduces classical logic rather informally, with very few theorems
and proofs (which are mainly located in the supplementary sections). Nevertheless, the
exposition is systematic and precise, without compromising on the essential technical and
conceptual issues and subtle points inherent in logic.
Aims
This textbook covers only the core of classical logic, which itself is just the heart of the
vast and growing body of modern logic. The main aims of the book are:
1. to explain the language, grammar, meaning, and formal semantics of logical formulae,
to help the reader understand the use of classical logical languages and be able both to
formalize natural language statements in them and translate back from logical formulae
to natural language;
2. to present, explain, and illustrate with examples the use of the most popular deductive
systems (namely, axiomatic systems, Semantic Tableaux, Natural Deduction, and Res-
olution with the only notable exclusion being Sequent Calculus, which is essentially
inter-reducible with Natural Deduction) for mechanizing and “computing” logical rea-
soning both on propositional and on first-order level, and to provide the reader with the
necessary technical skills for practical derivations in them; and
3. to offer systematic advice and guidelines on how to organize and perform a logically
correct and well-structured reasoning using these deductive systems and the reasoning
techniques that they provide.
Summary of the content and main features
The structure of the book reflects the two levels of expression and reasoning in classical
logic: propositional and first-order.
The first two chapters are devoted to propositional logic. In Chapter 1 I explain how
to understand propositions and compute their truth values. I then introduce propositional

xii
Preface
formulae and their truth tables and then discuss logical validity of propositional argu-
ments. The fundamental notion here is that of propositional logical consequence. Then,
in Chapter 2, I present several deductive systems used for deriving logical consequences
in propositional logic and show how they can be used for checking the logical correct-
ness of propositional arguments and reasoning. In a supplementary section at the end of
the chapter I sketch generic proofs of soundness and completeness of the propositional
deductive systems.
The exposition of propositional logic is uplifted to first-order logic in the following two
chapters. In Chapter 3 I present first-order structures and languages and then the syntax
and semantics (first informally, and then more rigorously) of first-order logic. Then I focus
on using first-order languages and translations between them and natural languages. In
the last section of this chapter I present and discuss the fundamental semantic concepts
of logical validity, consequence, and equivalence in first-order logic. Deductive systems
for first-order logic are introduced in Chapter 4 by extending the respective propositional
deductive systems with additional rules for the quantifiers. Derivations in each of these
are illustrated with several examples. Again in a supplementary section, I sketch generic
proofs of soundness and completeness of the deductive systems for first-order logic.
Chapter 5 contains some applications of classical logic to mathematical reasoning and
proofs, first in general and then specifically, for sets functions, relations, and arithmetic.
It consists of concise presentations of the basic theories of these, where the proofs are left
as exercises. The chapter ends with applications of classical logic to automated reasoning
and theorem proving, as well as to logic programming, illustrated briefly with Prolog.
The book ends with a comprehensive set of detailed solutions or answers to many of
the exercises.
The special features of this book include:
• concise exposition, with semi-formal but rigorous treatment of the minimum necessary
theory;
• emphasis both on conceptual understanding by providing many examples, and on devel-
oping technical skills and building experience by providing numerous exercises, most
of them standard, yet non-trivial, as well as full solutions or answers for many of them;
• solid and balanced coverage of semantic, syntactic, and deductive aspects of logic;
• some refreshing extras, such as a few logic-related cartoons scattered around, as well
as many biographical boxes at the end of each section with photos and short texts on
distinguished logicians, providing some background to their lives and contributions;
• selected references to other books on logic, listed at the end of each section, which are
suitable for further reading on the topics covered in the section; and
• a supplementary website with slides, additional exercises, more solutions, and errata,
which can be viewed at https://logicasatool.wordpress.com
For the instructor
The textbook is intended for introductory and intermediate courses in classical logic,
mainly for students in both mathematics and computer science, but is also suitable and
useful for more technically oriented courses for students in philosophy and social sciences.

Preface
xiii
1.1
1.2
1.3
1.4
2.1
2.2
2.3
2.4
2.5
2.6
2.7
3.1
3.2
3.3
3.4
4.1
4.2
4.3
4.4
4.5
4.6
5.1
5.3
5.2
5.4
Dependancy chart
Some parts of the text and some exercises are much more relevant to only one of the main
target audiences, and I have indicated them by using Mathematics Track
and Computer
Science Track
markers in the text. Everything else which is not explicitly included in
either of these tracks should be suitable for both groups. Likewise, some specific topics
and exercises are somewhat more advanced and are indicated with an Advanced Track
marker
. These are, of course, only indications.
The whole book can be covered in one or two semester courses, depending on the back-
ground and technical level of the audience. It assumes almost no specific prior knowledge,
except some general background in college maths for specific topics and examples, usu-
ally indicated in Mathematics or Advanced tracks. A dependency chart of the different
sections is provided in the figure above.


Acknowledgements
This textbook grew out of lecture notes that I have compiled for introductory logic courses
for students in mathematics and computer science since the late 1990s. Many people
have contributed in various ways to this book over these years. I am grateful to former
colleagues who have helped with valuable comments, technical or editorial corrections,
and some solutions to exercises, including Thomas Bolander, Willem Conradie, Ruaan
Kellerman and Claudette Robinson, as well as to many students at the University of Johan-
nesburg, the University of the Witwatersrand, and the Technical University of Denmark,
who have sent me useful feedback and have noticed some errors in the lecture notes from
which this book has evolved.
I also thank Christopher Burke, Mike Cavers, and Andreï Kostyrka for generously
allowing me to include some of their cartoons in the book.
I gratefully acknowledge Wikipedia, the Stanford Encyclopedia of Philosophy, and the
MacTutor History of Mathematics archive of the School of Mathematics and Statistics
University of St Andrews as the main sources of the historical and biographical informa-
tion provided.
The core content of the present book is a substantially extended version of the two
chapters in logic in the recently published Logic and Discrete Mathematics: A Concise
Introduction (written by Willem Conradie and myself, published by Wiley). I am grate-
ful to Wiley for the continued collaboration. In particular, I thank everyone employed or
contracted by Wiley who took part in the different stages of the technical preparation of
this book.
Lastly, I owe very special thanks to my life partner Nina for her personal support and
understanding during the work on this book, as well as for her invaluable technical help
with producing many figures and diagrams and collecting photos and references for many
of the historical boxes and the cover of the book.

xvi
Acknowledgements
Attributions for the photos used in the book
Most of the photos are public domain or under Creative Commons licence with free per-
missions. The photos of Cook, Fitting, Prawitz and Hodges are obtained from their owners
or used with their permission. Attributions for the rest are listed below.
Davis
Author of photo: David Monniaux
Gentzen
Author of photo: “Eckart Menzler-Trott”, permission obtained from the MFO “Archives
of the Mathematisches Forschungsinstitut Oberwolfach”
Henkin
Author of photo: George M. Bergman
Herbrand
Author of photo: Journal des Sciences
Ja´skowski
Author of photo: Alojzy Czarnecki
Kleene
Author of photo: Konrad Jacobs, Erlangen, permission obtained from the MFO “Archives
of the Mathematisches Forschungsinstitut Oberwolfach”
Levin
Author of photo: Wikipedia user Sergio01
Maltsev
Author of photo: Shiltsev
Robinson
Author of photo: David Monniaux
Putnam
Author of photo: Hilary Putnam
Skolem
Author of photo: Oslo Museum: image no. OB.F06426c (Byhistorisk samling/City his-
toric collection), via oslobilder.no
Tarski
Author of photo: George M. Bergman, permission obtained from the MFO “Archives of
the Mathematisches Forschungsinstitut Oberwolfach”
Wittgenstein
Author of photo: Moritz Nähr
Zeno
Author of photo: Wikipedia user Shakko

Introduction
What is logic about? What does it study and what does it offer? A usual definition found
in the encyclopedia is that it is the branch of philosophy that studies the laws and rules
of human reasoning. Little of this is actually correct. First, logic left the cradle of philos-
ophy long ago and is now a truly interdisciplinary area, related and relevant not only to
philosophy but also to mathematics, computer science, artificial intelligence, linguistics,
social sciences, and even economics. Second, is logic really about how we reason? If that
were the case, as a professional logician for many years I should already know quite well
how exactly humans reason. Alas, the more experience I gain in life, the less I understand
that. One thing is certain: most people use in their everyday reasoning emotions, analo-
gies, clichés, ungrounded beliefs and superstitions, that is, everything but logic. But then,
maybe logic studies the reasoning of the rational human, for whom reasoning is a purely
rational brain activity? Well, while many (but far from all) of us humans reason with their
brains, this is not sufficient to understand how we do it. As the American scientist Emerson
M. Pugh brilliantly put it: “If the human brain were so simple that we could understand
it, we would be so simple that we couldn’t.”
What does logic tell us, after all, if not how we reason? A better answer is: it tells us how
we can – and ideally should – reason in a systematic and well-structured way that would
guarantee that we always derive true and correct conclusions, providing we only use true
assumptions and only apply logically correct rules of reasoning. Logic is therefore not
just concerned with what is true and what is false, but rather with the correctness of our
argumentation of what implies what and with the validity of our reasoning. What exactly
does all that mean? This book aims to answer this question, beginning with some food for
thought here.
The famous Greek philosopher Aristotle (384–322 BC), regarded as the founding father
of formal logic, was the first who systematically studied and classified logically correct

xviii
Introduction
and incorrect forms and rules of reasoning. Aristotle studied specific patterns of arguments
called syllogisms. Here is a typical example (mine, not Aristotle’s) of a syllogism:
All logicians are clever.
All clever people are rich.
All logicians are rich.
The way we actually read this is as follows.
If all logicians are clever and all clever people are rich, then all logicians are rich.
This sounds intuitively like a correct piece of reasoning, and it is, but it does not mean
that the conclusion is necessarily true. (In fact, unfortunately, it is not.) What then makes
it correct?
Here is another example:
All natural numbers are integers.
Some integers are odd numbers.
Some natural numbers are odd numbers.
Note that all statements above are true. So, is this a correct argument? If you think so,
then how about taking the same argument and replacing the words “natural numbers”
by “mice,” “integers” by “animals,” and “odd numbers” by “elephants.” This will
not change the logical shape of the argument and, therefore, should not change its logical
correctness. The result speaks for itself, however.
All mice are animals.
Some animals are elephants.
Some mice are elephants.
So what makes an argument logically correct? You will also find answers to this ques-
tion in this book.
Let me say a few more concrete words about the main aspects and issues of classical
logic treated in this book. There are two levels of logical discourse and reasoning in classi-
cal logic. The lower level is propositional logic, introduced and discussed in the first two
chapters of this book, and the higher level is first-order logic, also known as predicate
logic, treated in the rest of the book.
Propositional logic is about reasoning with propositions, sentences that can be assigned
a truth value of either true or false. They are built from simple, atomic propositions by
using propositional logical connectives. The truth values propagate over all propositions
through truth tables for the propositional connectives.
Propositional logic can only formalize simple logical reasoning that can be expressed
in terms of propositions and their truth values, but it is quite insufficient for practical
knowledge representation and reasoning. For that, it needs to be extended with several
additional features, including constants (names) and variables for objects of any nature
(numbers, sets, points, human beings, etc.), functions and predicates over objects, as
well as quantifiers such as “for all objects x( . . . x . . . ),” and “there exists an object x
such that ( . . . x . . . ).” These lead to first-order languages, which (in many-sorted ver-
sions) are essentially sufficient to formalize most common logical reasoning. Designing

Introduction
xix
appropriately expressive logical languages and using them to capture fragments of natural
languages and reasoning is one of the main tasks of modern logic.
There are three major aspects of a logical system: semantic; syntactic; and deduc-
tive. The former deals mostly with the semantic notions of truth, validity and logical
consequence, whereas the latter two deal respectively with the syntax and grammar of
logical languages and with systems for logical deduction and derivations and deductive
consequences. Deductive systems are purely mechanical procedures designed to derive
(deduce) logical validities and consequences by means of formal rules of inference and
possibly some postulated derived formulae called axioms. Thus, a deductive system does
not refer explicitly to the meaning (semantics) of the formulae but only treats them as spe-
cial strings of symbols and acts on their shape (syntax). In principle, a deductive system
can be used successfully without any understanding of what formulae mean, and deriva-
tions in a deductive system can be performed not only by humans but also by artificial
“agents” or computers. However, deductive systems are always meant to capture (or even
determine) logical consequence so, ideally, semantic logical consequence and deductive
consequence should precisely match each other. If that is the case, we say that the deduc-
tive system is sound and complete, or just adequate. Design and study of adequate and
practically useful deductive systems is another major logical task.
The main syntactic, semantic, and deductive aspects of classical logic are discussed in
detail in the book; there is much more that is not treated here however, both inside and
outside of classical logic. In particular, logic is deeply related to: the foundations of math-
ematics, via axiomatic theories of sets; mathematics itself via model theory; the important
notions of algorithmic decidability and computability via recursion theory; and the funda-
mentals and limitations of the deductive approach via proof theory. All of these are major
branches of logic that I will only mention briefly in the text, but much more can be seen in
the references. Furthermore, there is a rich variety of other, more specialized non-classical
logical languages and systems that are better suited for specific modes and aspects of rea-
soning, such as intuitionistic, modal, temporal, epistemic, deontic, and non-monotonic
logics that will not (except briefly intuitionistic logic) be discussed at all in this book.
References to relevant publications covering these topics are provided throughout.
Finally, a few final words on the role of logic in the modern world. As I mentioned
earlier, contemporary logic has become a highly interdisciplinary area with fundamen-
tal applications to a wide variety of scientific fields including mathematics, philosophy,
computer science, artificial intelligence, and linguistics. Today logic not only provides
methodology for correct human reasoning, but also techniques and tools for automated
reasoning of intelligent agents. It also provides theoretical foundations for basic concepts
in computer science such as computation and computability, algorithms and complexity,
and semantics of programming languages, as well as practical tools for formal specifi-
cation, synthesis, analysis, and verification of software and hardware, development and
management of intelligent databases, and logic programming. The impact of logic on
computer science nowadays is often compared to the impact of differential and integral
calculus on natural sciences and engineering from the 17th century.
I end this introduction with a humble hope that this book will help the reader understand
and master the use of this great intellectual tool called Logic. Enjoy it!
Valentin Goranko
Stockholm, November 2015


An Appetizer: Logical Paradoxes
and Self-Reference
Dear reader,
The sentence that you are reading now is not true.
Is this claim true or false? If true, then it truly claims that it is not true, so it can’t be
true. But then, it is not true that it is not true, it must be true! Or . . . ?
This is a version of probably the oldest known logical paradox since antiquity, also
known as the liar’s paradox which refers to the quote “I am lying now.”
What is a logical paradox? It is a statement or an argument that presents an apparent
logical contradiction, either with well-known and accepted truths or simply with itself.
Unlike a fallacy, a paradox is not due to an incorrect reasoning, but it could be based on
wordplay or on a subtle ambiguity in the assumptions or concepts involved. Most com-
monly however, logical paradoxes arise when using self-reference, such as in the opening
sentence above. Logicians love playing with self-reference. For instance, I have added
this sentence in order to make a reference to itself. And, this one, which does not make a
reference to itself. (Or, does it . . . ?)
A variation of the liar’s paradox is Jourdain’s card paradox, which does not rely on
immediate self-reference but on a circular reference. Here is a simple version:
The next sentence is true. The previous sentence is false.
I end this appetizer two more paradoxes which are not exactly logical but semantic,
again a self-referential play but now with natural language.
The first is known as Berry’s paradox. Clearly every natural number can be defined in
English with sufficiently many words. However, if we bound the number of words to be
used, then only finitely many natural numbers can be defined. Then, there will be numbers
that cannot be defined with that many words. Hence, there must be a least so undefinable
natural number. Now, consider the following sentence “The least natural number that is
not definable in English with less than twenty words.” There is a uniquely determined
natural number that satisfies this description, so it is a definition in English, right? Well,
count how many words it uses.

xxii
An Appetizer: Logical Paradoxes and Self-Reference
The second is the Grelling–Nelson paradox. Divide all adjectives into two
groups: autological, if and only if it describes itself, such as “English,” “short,” and
“fourteen-letter;” and heterological, if and only if it does not describes itself, such
as “wet,” “white,” and “long.” Now, is the adjective “heterological” autological or
heterological?

1
Understanding Propositional Logic
Propositional logic is about reasoning with propositions. These are sentences that can be
assigned a truth value: true or false. They are built from primitive statements, called atomic
propositions, by using propositional logical connectives. The truth values propagate over
all propositions through truth tables for the propositional connectives. In this chapter I
explain how to understand propositions and compute their truth values, and how to reason
using schemes of propositions called propositional formulae. I will formally capture the
concept of logically correct propositional reasoning by means of the fundamental notion
of propositional logical consequence.
1.1
Propositions and logical connectives: truth tables and tautologies
1.1.1
Propositions
The basic concept of propositional logic is proposition. A proposition is a sentence that
can be assigned a unique truth value: true or false.
Some simple examples of propositions include:
• The Sun is hot.
• The Earth is made of cheese.
• 2 plus 2 equals 22.
• The 1000th decimal digit of the number π is 9.
(You probably don’t know whether the latter is true or false, but it is surely either true
or false.)
The following are not propositions (why?):
• Are you bored?
• Please, don’t go away!
• She loves me.
• x is an integer.
• This sentence is false.
Logic as a Tool: A Guide to Formal Logical Reasoning, First Edition. Valentin Goranko.
© 2016 John Wiley & Sons, Ltd. Published 2016 by John Wiley & Sons, Ltd.

2
Logic as a Tool
Here is why. The first sentence above is a question, and it does not make sense to
declare it true or false. Likewise for the imperative second sentence. The truth of the third
sentence depends on who “she” is and who utters the sentence. Likewise, the truth of the
fourth sentence is not determined as long as the variable x is not assigned a value, integer
or not. As for the last sentence, the reason is trickier: assuming that it is true it truly claims
that it is false – a contradiction; assuming that it is false, it falsely claims that it is false,
hence it is not false – a contradiction again. Therefore, no truth value can be consistently
ascribed to it. Such sentences are known as self-referential and are the main source of
various logical paradoxes (see the appetizer and Russell’s paradox in Section 5.2.1).
1.1.2
Propositional logical connectives
The propositions above are very simple. They have no logical structure, so we call them
primitive or atomic propositions. From primitive propositions one can construct com-
pound propositions by using special words called logical connectives. The most com-
monly used connectives are:
• not, called negation, denoted ¬;
• and, called conjunction, denoted ∧(or sometimes &);
• or, called disjunction, denoted ∨;
• if . . . then . . . , called implication, or conditional, denoted →;
• . . . if and only if . . . , called biconditional, denoted ↔.
Remark 1 It is often not grammatically correct to read compound propositions by simply
inserting the names of the logical connectives in between the atomic components. A typical
problem arises with the negation: one does not say “Not the Earth is square.” A uniform
way to get around that difficulty and negate a proposition P is to say “It is not the case
that P.”
In natural language grammar the binary propositional connectives, plus others like but,
because, unless, although, so, yet, etc. are all called “conjunctions” because they “con-
join”, that is, connect, sentences. In logic we use the propositional connectives to connect
propositions. For instance, given the propositions
“Two plus two equals five” and “The Sun is hot”
we can form the propositions
• “It is not the case that two plus two equals five. ”
• “Two plus two equals five and the Sun is hot.”
• “Two plus two equals five or the Sun is hot.”
• “If two plus two equals five then the Sun is hot.”
• “Two plus two equals five if and only if the Sun is hot.”

Understanding Propositional Logic
3
For a more involved example, from the propositions (we assume we have already
decided the truth value of each)
“Logic is fun”, “Logic is easy”, and “Logic is boring”
we can compose a proposition
“Logic is not easy or if logic is fun then logic is easy and logic is not
boring.”
It sounds better smoothed out a bit:
“Logic is not easy or if logic is fun then it is easy and not boring.”
1.1.3
Truth tables
How about the truth value of a compound proposition? It can be computed from the truth
values of the components1 by following the rules of ‘propositional arithmetic’:
• The proposition ¬A is true if and only if
the proposition A is false.
• The proposition A ∧B is true if and only if
both A and B are true.
• The proposition A ∨B is true if and only if
either of A or B (possibly both) is true.
• The proposition A →B is true if and only if
A is false or B is true, that is, if the truth of A implies the truth of B.
• The proposition A ↔B is true if and only if
A and B have the same truth values.
We can systematize these rules in something similar to multiplication tables. For that
purpose, and to make it easier for symbolic (i.e., mathematical) manipulations, we intro-
duce a special notation for the two truth values by denoting the value true by T and
the value false by F. Another common notation, particularly in computer science, is to
denote true by 1 and false by 0.
The rules of the “propositional arithmetic” can be summarized by means of the follow-
ing truth tables (p and q below represent arbitrary propositions):
p
¬p
p
q
p ∧q
p ∨q
p →q
p ↔q
T
F
T
T
T
T
T
T
F
T
T
F
F
T
F
F
F
T
F
T
T
F
F
F
F
F
T
T
1 Much in the same way as we can compute the value of the algebraic expression a × (b −c) + b/a as soon as we
know the values of a, b, c.

4
Logic as a Tool
1.1.4
The meaning of the connectives in natural language and in logic
The use and meaning of the logical connectives in natural language does not always match
their formal logical meaning. For instance, quite often the conjunction is loaded with a
temporal succession and causal relationship that makes the common sense meanings of
the sentences “The kid threw the stone and the window broke” and “The window
broke and the kid threw the stone” quite different, while they have the same truth
value by the truth table of the conjunction. Conjunction in natural language is therefore
often non-commutative, while the logical conjunction is commutative. The conjunction
is also often used to connect not entire sentences but only parts, in order to avoid repeti-
tion. For instance “The little princess is clever and beautiful” logically means “The
little princess is clever and the little princess is beautiful.” Several other conjunctive
words in natural language, such as but, yet, although, whereas, while etc., translate into
propositional logic as logical conjunction.
The disjunction in natural language also has its peculiarities. As for the conjunction, it
is often used in a form which does not match the logical syntax, as in “The old stranger
looked drunk, insane, or completely lost”. Moreover, it is also used in an exclusive
sense, for example in “I shall win or I shall die”, while in formal logic we use it by
convention in an inclusive sense, so “You will win or I will win” will be true if we both
win. However, “exclusive or”, abbreviated Xor, is sometimes used, especially in com-
puter science. A few other conjunctive words in natural language, such as unless, can
translate into propositional logic as logical disjunction, for instance “I will win, unless
I die.” However, it can also equivalently translate as an implication: “I will win, if I do
not die.”
Among all logical connectives, however, the implication seems to be the most debat-
able. Indeed, it is not so easy to accept that a proposition such as “If 2+2=5, then the
Moon is made of cheese”, if it makes any sense at all, should be assumed true. Even
more questionable seems the truth of the proposition “If the Moon is made of chocolate
then the Moon is made of cheese.” The leading motivation to define the truth behav-
ior of the implication is, of course, the logical meaning we assign to it. The proposition
A →B means:
If A is true, then B must be true,
Note that if A is not true, then the (truth of the) implication A →B requires nothing
regarding the truth of B. There is therefore only one case where that proposition should
be regarded as false, namely when A is true, and yet B is not true. In all other cases we
have no reason to consider it false. For it to be a proposition, it must be regarded true. This
argument justifies the truth table of the implication. It is very important to understand the
idea behind that truth table, because the implication is the logical connective which is
most closely related to the concepts of logical reasoning and deduction.
Remark 2 It helps to think of an implication as a promise. For instance, Johnnie’s father
tells him: “If you pass your logic exam, then I’ll buy you a motorbike.” Then con-
sider the four possible situations: Johnnie passes or fails his exam and his father buys
or does not buy him a motorbike. Now, see in which of them the promise is kept (the
implication is true) and in which it is broken (the implication is false).
Some terminology: the proposition A in the implication A →B is called the
antecedent and the proposition B is the consequent of the implication.

Understanding Propositional Logic
5
The implication A →B can be expressed in many different but “logically equivalent”
(to be defined later) ways, which one should be able to recognize:
• A implies B.
• B follows from A.
• If A, B.
• B if A.
• A only if B.
• B whenever A.
• A is sufficient for B.
(Meaning: The truth of A is sufficient for the truth of B.)
• B is necessary for A.
(Meaning: The truth of B is necessary for A to be true.)
1.1.5
Computing truth values of propositions
It can be seen from the truth tables that the truth value of a compound proposition does not
depend on the meaning of the component propositions, but only on their truth values. To
check the truth of such a proposition, we merely need to replace all component proposi-
tions by their respective truth values and then “compute” the truth of the whole proposition
using the truth tables of the logical connectives. It therefore follows that
• “It is not the case that two plus two equals five” is true;
• “Two plus two equals five and the Sun is hot” is false;
• “Two plus two equals five or the Sun is hot” is true; and
• “If two plus two equals five, then the Sun is hot” is true (even though it does not
make good sense).
For the other example, suppose we agree that
“Logic is fun” is true,
“Logic is boring” is false,
“Logic is easy” is true.
Then the truth value of the compound proposition
“Logic is not easy or if logic is fun then it is easy and not boring.”
can be determined just as easily. However, in order to do so, we first have to analyze the
syntactic structure of the proposition, that is, to determine how it has been composed,
in other words in what order the logical connectives occurring therein have been applied.
With algebraic expressions such as a × (b −c) + b/c that analysis is a little easier, thanks
to the use of parentheses and the established priority order among the arithmetic opera-
tions. We also make use of parentheses and rewrite the sentence in the way (presumably)
we all understand it:
“(Logic is not easy) or ((if logic is fun) then ((logic is easy) and (logic is
not boring))).”

6
Logic as a Tool
The structure of the sentence should be clear now. We can however go one step fur-
ther and make it look exactly like an algebraic expression by using letters to denote the
occurring primitive propositions. For example, let us denote
“Logic is fun” A,
“Logic is boring” B, and
“Logic is easy” C.
Now our compound proposition can be neatly rewritten as
(¬C) ∨(A →(C ∧¬B)).
In our rather informal exposition we will not use parentheses very systematically, but
only whenever necessary to avoid ambiguity. For that purpose we will, like in arithmetic,
impose a priority order among the logical connectives, namely:
• the negation has the strongest binding power, that is, the highest priority;
• then come the conjunction and disjunction;
• then the implication; and
• the biconditional has the lowest priority.
Example 3 The proposition ¬A ∨C →A ∧¬B is a simplified version of
((¬A) ∨C) →(A ∧¬B).
The last step is to compute the truth value. Recall that is not the actual meaning of
the component propositions that matters but only their truth values, so we can simply
replace the atomic propositions A, B, and C by their truth values and perform the formal
computation following the truth tables step-by-step:
(¬T) ∨(T →(T ∧¬F)) = F ∨(T →(T ∧T)) = F ∨(T →T) = F ∨T = T.
So, logic is easy after all! (At least, so far.)
1.1.6
Propositional formulae and their truth tables
If we only discuss particular propositions our study of logic would be no more useful than
a study of algebra based on particular equalities such as 2 + 3 = 5 or 12345679 × 9 =
111111111. Instead, we should look at schemes of propositions and their properties, just
like we study algebraic formulae and equations and their properties. We call such schemes
of propositions propositional formulae.
1.1.6.1
Propositional formulae: basics
I first define a formal language in which propositional formulae, meant to be templates
for composite propositions, will be special words. That language involves:
• propositional constants: special fixed propositions ⊤, that always takes a truth value
true, and ⊥, that always takes a value false;

Understanding Propositional Logic
7
• propositional variables p, q, r . . . , possibly indexed, to denote unspecified proposi-
tions in the same way as we use algebraic variables to denote unspecified or unknown
numbers;
• the logical connectives that we already know; and
• auxiliary symbols: parentheses (and) are used to indicate the order of application of
logical connectives and make the formulae unambiguous.
Using these symbols we can construct propositional formulae in the same way in which
we construct algebraic expressions from variables and arithmetic operations. Here are a
few examples of propositional formulae:
⊤, p, ¬⊥, ¬¬p, p ∨¬q,
p1 ∧¬(p2 →(¬p1 ∧⊥))
There are infinitely many possible propositional formulae so we cannot list them all
here. However, there is a simple and elegant way to give a precise definition of proposi-
tional formulae, namely the so-called inductive definition (or recursive definition). It
consists of the following clauses or formation rules:
1. Every propositional constant or variable is a propositional formula.
2. If A is a propositional formula then ¬A is a propositional formula.
3. If A, B are propositional formulae then each of (A ∨B), (A ∧B), (A →B), and
(A ↔B) is a propositional formula.
We say that a propositional formula is any string of symbols that can be constructed by
applying – in some order and possibly repeatedly – the rules above, and only objects that
can be constructed in such a way are propositional formulae.
Note that the notion of propositional formula that we define above is used in its own
definition; this is the idea of structural induction. The definition works as follows: the
first rule above gives us some initial stock of propositional formulae; as we keep applying
the other rules, we construct more and more formulae and use them further in the defini-
tion. Eventually, every propositional formula can be obtained in several (finitely many!)
steps of applying these rules. We can therefore think of the definition above as a construc-
tion manual prescribing how new objects (here, propositional formulae) can be built from
already constructed objects. I discuss inductive definitions in more detail in Section 1.4.5.
From this point, I omit the unnecessary pairs of parentheses according to our earlier
convention whenever that would not lead to syntactic ambiguity.
The formulae that are used in the process of the construction of a formula A are called
subformulae of A. The last propositional connective introduced in the construction of A
is called the main connective of A and the formula(e) to which it is applied is/are the
main subformula(e) of A. I make all these more precise in what follows.
Example 4 (Construction sequence, subformulae and main connectives) One con-
struction sequence for the formula
(p ∨¬(q ∧¬r)) →¬¬r
is
p, q, r, ¬r, ¬¬r, q ∧¬r, ¬(q ∧¬r), p ∨¬(q ∧¬r), (p ∨¬(q ∧¬r)) →¬¬r

8
Logic as a Tool
For instance, the subformula (q ∧¬r) has main connective (the only occurrence of) ∧in it,
and its main subformulae are q land ¬r; the first occurrence of ¬ is the main connective of
¬(q ∧¬r) and its only main subformula is (q ∧¬r); and the only occurrence of →is the
main connective of the whole formula, the main subformulae of which are (p ∨¬(q ∧¬r))
and ¬¬r.
1.1.6.2
Construction tree and parsing tree of a formula
A sequence of formulae constructed in the process of applying the definition and ending
with A is called a construction sequence of a formula A. A formula has many con-
struction sequences and a construction sequence may contain many redundant formulae.
A better notion for capturing the construction of a formula is the construction tree of
that formula. A construction tree is a tree-like directed graph with nodes labeled with
propositional constants, variables, and propositional connectives, such that:
1. Every leaf is labeled by a propositional constant or variable.
2. Propositional constants and variables label only leaves.
3. Every node labeled with ¬ has exactly one successor node.
4. Every node labeled with any of ∧, ∨, →or ↔has exactly two successor nodes: left
and right successor.
A construction tree therefore looks like:
Leaves:
unary connective (¬)
binary connective (∨, ∧, →, ↔)
the main connective
Every construction tree defines a formula C, built starting from the leaves and going
towards the root, by applying at every node the formula construction rule corresponding
to the label at that node. The formulae constructed in the process are precisely the subfor-
mulae of C, and the propositional connective labeling the root of the construction tree of
a formula C is the main connective of C.
Example 5 (Construction tree) The formula (p ∨¬(q ∧¬r)) →¬¬r has the follow-
ing construction tree:

Understanding Propositional Logic
9
r
¬
∧
q
¬
∨
p
→
¬
¬
r
The parsing tree of a formula looks the same as its construction tree but is produced
in inverse order, starting from the main connective (if any), drawing edges to the main
components, and then recursively producing the parsing trees for each of them.
1.1.6.3
Truth assignments: satisfaction of propositional formulae
A propositional formula is a scheme that becomes a proposition whenever we substitute
propositions for all occurring propositional variables. I, of course, mean uniform substi-
tutions, that is, the same variables are replaced by the same propositions throughout the
formula.
We cannot attribute a truth value to a propositional formula before we assign concrete
propositions to all occurring propositional variables, for the same reason that we cannot
evaluate x(y + z) before we have assigned values to x, y, z. However, remember that in
order to evaluate the truth value of a compound proposition, we only need to know the
truth values of the occurring atomic propositions and not the propositions themselves.
For instance, if we substitute the propositions “0.5 is an integer” for p, “2 is less
than 3” for q, and “the Moon is larger than the Sun” for r in the propositional formula
(p ∨¬(q ∧¬r)) →¬¬r
we find that the resulting proposition is true:
(F ∨¬(T ∧¬F)) →¬¬F = (F ∨¬(T ∧T)) →¬T = (F ∨¬T) →
F = (F ∨F) →F = F →F = T.
If, however, we substitute any true propositions for p and q and a false proposition for r,
then the resulting proposition will be false:
(T ∨¬(T ∧¬F)) →¬¬F = (T ∨¬(T ∧T)) →¬T = (T ∨¬T) →
F = (T ∨F) →F = T →F = F.

10
Logic as a Tool
Definition 6 A function that assigns truth values to propositional variables in a given
set is called truth assignment for that set of variables. If a truth assignment τ renders
a formula A true, we say that τ satisfies A, denoted τ |= A. A propositional formula is
satisfiable if it is satisfied by some truth assignment.
For instance, the formula p ∧¬q ∧r is satisfiable by the assignment p : T, q : F, r : T,
while the formula p ∧¬p is not satisfiable.
1.1.6.4
Truth tables of propositional formulae
Clearly, the truth of a given propositional formula only depends on the truth values
assigned to the variables occurring in that formula. We can therefore think of propo-
sitional formulae as functions from truth assignments to truth values. We can tabulate
the “behavior” of any propositional formula in a truth table where we list all possible
truth assignments of the occurring variables, and for each of them we compute the
corresponding truth value of the formula. We can do that by successively computing the
truth values of all occurring subformulae, as we did just now. For example, the truth table
for the above formula is compiled as follows:
p q
r
¬r
¬¬r
q ∧¬r
¬(q ∧¬r)
p ∨¬(q ∧¬r)
(p ∨¬(q ∧¬r)) →¬¬r
T T
T
F
T
F
T
T
T
T T
F
T
F
T
F
T
F
T F
T
F
T
F
T
T
T
T F
F
T
F
F
T
T
F
F T
T
F
T
F
T
T
T
F T
F
T
F
T
F
F
T
F F
T
F F
F
Exercise 7 Complete the last two rows.
Truth tables can be somewhat simplified if we notice that every occurrence of a logical
connective in a propositional formula determines a unique subformula where that occur-
rence is the main connective of that subformula. We can now simplify the truth table of
the formula by listing only the occurring variables and the whole formula, and then com-
puting the truth table of every subformula in the column below the corresponding main
connective:
p
q
r
(p ∨
¬
(q ∧
¬
r))
→
¬
¬
r
T
T
T
T T
T
T F
F
T
T
T
F
T
T
T
F
T T
F
T T
T
F
F
F
T
F
T
F
T
T T
T
F F
F
T
T
T
F
T
T
F
F
T T
T
F F
T
F
F
F
T
F
F
T
T
F T
T
T F
F
T
T
T
F
T
F
T
F
F F
F
T T
T
F
T
F
T
F
F
F
T
· · ·
F
F
F
· · ·

Understanding Propositional Logic
11
We therefore see that a propositional formula can represent a true or a false proposition,
depending of the choice of the propositions substituted for the occurring propositional
variables, or rather on their truth values.
1.1.7
Tautologies
Definition 8 A propositional formula A is a tautology if it obtains a truth value T for
any assignment of truth values to the variables occurring in A.
The claim that A is a tautology is denoted
|= A.
Tautologies are also called (logically) valid formulae.
Thus, tautology always renders a true proposition; it represents a logical law in the
same way as the identity x + y = y + x represents a law of arithmetic and, therefore,
holds no matter what values we assign to x and y.
Here are a few simple tautologies that represent some important features of proposi-
tional logic:
• (p ∨¬p): the law of excluded middle, which states that every proposition p is either
true or false (and, in the latter case, ¬p must be true);
• ¬(p ∧¬p): the law of non-contradiction, which states that it cannot be the case that
both p and ¬p are true;
• (p ∧q) →p: this is always true by the very meaning (and truth table) of ∧;
• likewise, p →(p ∨q) is always true by the very meaning and truth table of ∨; and
• ((p ∧(p →q)) →q): if p is true and it is true that p implies q, then q is true. This
reflects the meaning of the implication.
1.1.7.1
Checking tautologies with truth tables
How can we determine if a propositional formula A is a tautology? Quite easily: complete
the truth table of A and check if it always takes a truth value true. Let us check some of
the examples mentioned above:
p
¬p
(p ∨¬p)
(p ∧¬p)
¬(p ∧¬p)
T
F
T
F
T
F
T
T
F
T
p q
(p →q)
(p ∧(p →q))
((p ∧(p →q)) →q)
T T
T
T
T
T F
F
F
T
F T
T
F
T
F F
T
F
T

12
Logic as a Tool
The opposite concept of a tautology is a contradictory formula, that is, a formula that
always takes a truth value false. For example, (p ∧¬p) is a contradictory formula. A
formula that is not contradictory is called falsifiable.
How are the concepts of tautology and contradictory formula, and the concepts of sat-
isfiable and falsifiable formula related?
1. A formula A is a tautology precisely when its negation ¬A is a contradictory formula,
and A is contradictory precisely when its negation ¬A is a tautology.
2. A formula A is satisfiable precisely when its negation ¬A is falsifiable, and A is falsi-
fiable precisely when its negation ¬A is satisfiable.
1.1.7.2
Checking tautologies by searching for falsifying assignments
Checking tautologies with truth tables is straightforward but rather laborious and – let’s
admit – not very exciting. There is a somewhat streamlined and more intelligent method
whereby we attempt to show that the formula is not a tautology by searching for an
appropriate falsifying truth assignment, that is, a combination of truth values of the
propositional variables that renders it false. If we succeed in finding such an assignment,
then the formula is indeed not a tautology. If, however, when exploring a possible case
we reach a state where some variable is required to be both true and false, that is clearly
a contradiction, meaning that the case we are exploring is impossible and we must aban-
don that case. If all possible attempts to produce a falsifying assignment end up with a
contradiction, then we have actually proved that the formula cannot be falsified; it must
therefore be a tautology.2
The systematic search for a falsifying assignment is based on a step-by-step decompo-
sition of the formula by using the truth tables of the propositional connectives occurring
in it. Let us see how this works on some examples.
To make it more succinct I will use signed formulae, that is, expressions of the type
A : T, meaning “A must be true”, and A : F, meaning “A must be false”.
1. Consider the formula ¬(p →¬q) →(p ∨¬r).
To falsify it, it must be the case that ¬(p →¬q) :T and p ∨¬r : F.
For the former, it must be the case that p →¬q : F, hence p : T and ¬q : F.
For the latter, it must be the case that p : F and ¬r : F.
This implies that p must be both true and false, which is impossible. Our attempt to
falsify the formula has failed, and it is therefore a tautology.
2. Consider now ¬p →¬(p ∨¬q).
To falsify it, it must be the case that ¬p : T while ¬(p ∨¬q) : F.
Therefore p : F and p ∨¬q : T.
For the latter, p : T or ¬q : T. Let us consider both cases:
Case 1: p : T. This contradicts p : F.
Case 2: ¬q : T. Then q : F. In this case we have not reached any contradiction and
there is nothing more we can do in order to obtain one. Indeed, we can check that p : F
and q : F renders the formula false.
3. A contradiction in the truth values can be reached on any subformula, not necessarily a
variable. For instance, take (p∨¬q) →(¬p →(p∨¬q)). For it to be false, (p∨¬q) : T
2 This method of proof is called a proof by contradiction and is discussed in more detail in Section 5.1.

Understanding Propositional Logic
13
and (¬p →(p ∨¬q)) : F must be the case. From the latter, ¬p : T and (p ∨¬q) : F,
which contradicts the former.
4. Finally, take ((p ∧¬q) →¬r) ↔((p ∧r) →q). For it to be false, there are two pos-
sible cases:
Case 1: ((p ∧¬q) →¬r) : T and ((p ∧r) →q) : F. The latter implies (p ∧r) : T and
q : F, hence p : T, q : F, r : T. For the former, two sub-cases are possible:
Case 1a: ¬r : T. Then r : F, which is a contradiction with r : T.
Case 1b: (p ∧¬q) : F. Then:
Case 1bi: p : F, a contradiction with p : T.
Case 1bii: ¬q : F and q : T, again a contradiction but now with q : F.
Note that the consideration of these sub-cases could have been avoided, if we had
noticed that the assignment p : T, q : F, r : T renders ((p ∧¬q) →¬r) : F.
Case 2: (((p ∧r) →q) : T and (p ∧¬q) →¬r) : F. The former implies (p ∧¬q) : T
and ¬r : F, that is, p : T, q : F, r : T. Again, we can either notice that this assignment
renders ((p ∧r) →q) : F, or consider the cases for ((p ∧r) →q) : T and see that all
lead to a contradiction.
The formula cannot be falsified in either case, so it is a tautology.
This method can be formalized and mechanized completely into a kind of deductive
system, called Semantic Tableaux, which is presented in Section 2.3.
References for further reading
For helpful and accessible introductions to propositional logic and discussions of the
issues covered here plus more, see Tarski (1965), Gamut (1991), Jeffrey (1994), Barwise
and Echemedy (1999), Nederpelt and Kamareddine (2004), Boole (2005), Bornat (2005),
Hodges (2005), Chiswell and Hodges (2007), Makinson (2008), Ben-Ari (2012), and van
Benthem et al. (2014).
Some suitable books on philosophical logic, old and new, include Carroll (1897), Kalish
and Montague (1980), Smith (2003), Copi et al. (2010), and Halbach (2010).
For more technical books on mathematical logic, see Shoenfield (1967), van Dalen
(1983), Hamilton (1988), Mendelson (1997), Enderton (2001), and Hedman (2004),
For books on computational and applied logic the reader is referred to Gallier (1986),
Nerode and Shore (1993), and Fitting (1996).
Last but not least, some fun logic books include Carroll (1886) and Smullyan (1998,
2009a, b, 2011, 2013, 2014).
Exercises
1.1.1
Which of the following are propositions? (Assume that John, Mary and Eva are
concrete individuals.)
(a)
23 + 32 = 19
(b) 23 + 32 = 91
(c)
23 + 32 = x
(d) Will you marry me?

14
Logic as a Tool
(e)
John married on 1 January 1999.
(f)
John must marry Mary!
(g) I told her about John.
(h) Mary is not happy if John mar-
ried Eva.
(i)
Who is Eva?
(j)
Mary is not happy if 23 + 32 = 19.
(k) This sentence refers to itself.
(l)
This sentence is true.
(m) If you are reading this sentence
now, then it is not true.
1.1.2
If A and B are true propositions and C and D are false propositions, deter-
mine the truth values of the following compound propositions without using truth
tables.
(a) A ∧(B ∨C)
(b) (C →A) →D
(c) C →(A →D)
(d) ¬(¬A ∨C) ∧B
(e) ¬(¬(¬D ∧(B →¬A)))
(f) ¬(C →A) ∨(C →D)
(g) (C ↔¬B) ∨(A →¬A)
(h) (A ↔¬B) ↔(C ↔¬D)
1.1.3
Determine the antecedent and the consequent in each of the following
implications.
(a) Whenever John talks everyone else listens.
(b) Everyone else listens if John talks.
(c) John talks only if everyone else listens.
(d) If everyone else listens, John talks.
(e) An integer is positive if its cube is positive.
(Hint: To make it easier to reason, introduce a name for the object in question,
for example “An integer n is positive if the cube of n is positive.”)
(f) An integer is positive only if its cube is positive.
(g) A function is continuous whenever it is differentiable.
(h) The continuity of a function is necessary for it to be differentiable.
(i) The continuity of a function is sufficient for it to be differentiable.
1.1.4
A positive integer n is called prime if n > 1 and n is divisible only by 1 and by
itself. Which of the following conditions are sufficient and which are necessary
for the truth of “n is not prime”, where n is some (given) positive integer?
(a) n is divisible by 3.
(b) n is even.
(c) n is divisible by 6.
(d) n has at least two different factors.
(e) n has more than two different
factors.
(f) n = 15.
(g) n has a factor different from n.
(h) n has a prime factor.
(i) n has a prime factor different
from n.
1.1.5
Write each of the following composite propositions in a symbolic form by identi-
fying its atomic propositions and logical structure. Then determine its truth value.
(a) The Earth rotates around itself and, if the Moon rotates around the
Earth, then the Sun rotates around the Moon.
(b) If the Sun rotates around the Earth or the Earth rotates around the
Moon then the Sun rotates around the Moon.
(c) The Moon does not rotate around the Earth if the Sun does not rotate
around the Earth and the Earth does not rotate around the Moon.

Understanding Propositional Logic
15
(d) The Earth rotates around itself only if the Sun rotates around the Earth
or the Moon does not rotate around the Earth.
(e) The Earth rotates around itself if and only if the Moon does not rotate
around the Earth or the Earth does not rotate around the Moon.
1.1.6
Determine the truth value of the proposition A in each of the following cases,
without using truth tables. (Hint: if necessary, consider the possible cases.)
(a) B and B →A are true.
(b) A →B is true and B is false.
(c) ¬B and A ∨B are true.
(d) Each of B →¬A, ¬B →¬C, and C is true.
(e) Each of ¬C ∧B, C →(A ∨B), and ¬(A ∨C) →C is true.
1.1.7
Let P, Q, R, and S be propositions. Show that:
(a) If the propositions P and P →Q are true, then Q is true.
(b) If the propositions (P ∨Q) →R and P ∨R are true, then R is true.
(c) If P →Q, Q →R, and P ∧S are true, then R ∧S is true.
(d) If ¬P →¬Q, ¬(P ∧¬R), ¬R are true, then Q is false.
(e) If P →Q, R ∨(S ∧¬Q), and ¬R are true, then P is false.
(f) If Q →(R ∧S) is true and Q ∧S is false, then R ∧Q is false.
(g) If P →Q and Q →(R ∨S) are true and P →R is false, then ¬R →S
is true.
(h) If ¬P →(¬Q ∨¬R) and Q ∧(P ∨R) are true, then P is true.
(i) If P →Q and Q →(R ∨S) are true and P →R is false, then S is true.
(j) If Q →(R ∧S) is true and Q ∧S is false, then Q is false.
1.1.8
Construct the truth tables of the following propositional formulae, and determine
which (if any) of them are tautologies and which are contradictory formulae.
(a) ¬(p →¬p)
(b) p ∨(p →¬p)
(c) p ∧(q ∨¬q)
(d) (p ∧¬p) →q
(e) ((p →q) →p) →p
(f) ¬p ∧¬(p →q)
(g) (p ∨¬q) →¬(q ∧¬p)
(h) (p →q) ∧(q →r) ∧¬(¬p ∨r)
(i) ¬(¬p ↔q) ∧(r ∨¬q)
(j) ¬((p ∧¬q) →r) ↔(¬(q ∨r) →¬p)
1.1.9
Determine which (if any) of the following propositional formulae are tautologies
by searching for falsifying truth assignments.
(a) q →(q →p)
(b) p →(q →p)
(c) ((p →q) ∧(p →¬q)) →¬p
(d) ((p →q) ∨(p →¬q)) →¬p
(e) (p ∨¬q) ∧(q →¬(q ∧¬p))
(f) ((p →q) ∧(q →r)) →(r →p)
(g) ((p →q) ∧(q →r)) →(p →r)
(h) ((p →q) ∧(p →r)) →(p →(q ∧r))
(i) ((p ∧q) →r) →((p →r) ∧(q →r))
(j) ((p ∨q) →r) →((p →r) ∧(q →r))

16
Logic as a Tool
(k) ((¬p ∧q) →¬r) →(¬q →¬(p ∧¬r))
(l) ((p →r) ∨(q →r)) →((p ∨q) →r)
(m) ((p →r) ∧(q →r)) →((p ∨q) →r)
(n) p →((q →r) →((p →q) →(p →r)))
1.1.10
Lastly, some logical puzzles3. On the remote planet Nologic there are two types
of intelligent creatures:
• truth-tellers, who always tell the truth, and
• liars, who (you guessed it) always lie.
It is not possible to distinguish them by appearance, but only by the truth or falsity
of the statements they make.
(a)
A space traveler visited Nologic and met two inhabitants, P and Q. He asked
them: “Is any of you a liar?” “At least one of us is a liar”, replied P. Can you
find out what P and Q are?
(b)
Next, the stranger met two other inhabitants and asked one of them “Is any
of you a liar?”. He got a “yes” or “no” answer and from that answer was
able to determine for each of them whether he is a liar or not. What was the
answer and what was the stranger’s conclusion?
(c)
Walking about Nologic, the stranger met two other inhabitants A and B and
asked A, “Is any of you a truth-teller?” “If B is a liar, then I am a liar too”,
replied A. What are A and B?
(d)
The stranger went on and met three locals X, Y, and Z and asked X: “Are
you a liar?” X answered something which the stranger did not hear, so he
asked Y: “What did X say?” “He said that he is a liar”, replied Y. Then Z
added “Don’t believe Y, he is a liar”. Can you identify all liars?
(e∗) The stranger went on. In the evening, he began to look for a shelter for the
night, but was very cautious because he knew that some of the inhabitants
were man-eaters and it was not possible to recognize them by appearance.
He met three inhabitants, C, D, and E. He asked C, “How many of you
are truth-tellers?” “Flam flim” answered C in her language. “What did she
say?” asked the stranger D. “Just one”, replied D. “Do not trust D, he is a
liar. Come with me, I’m not a man-eater” said E. “No, come with me, I’m
not a man-eater” countered D.
What should the stranger do?
(f∗) The stranger decided to go back to the port where his spaceship was, but he
got lost. After a long walk he got to a fork of the road. He knew that one
of the two roads would take him to the spaceship port, but did not know
which one. Luckily, he saw two of the inhabitants, one on each road. He
had met them before so he knew that one of them was a liar and the other a
truth-teller, but could not remember who was who.
Can the stranger ask just one question to either of these inhabitants in
order to find out which is the correct road to the spaceship port? If so, what
question should he ask?
3 For many more puzzles of this type, I warmly recommend to the reader the marvellous logical puzzle books by
Raymond Smullyan (1998, 2009a, b, 2011, 2013, 2014).

Understanding Propositional Logic
17
1.1.11
.
The early origins of propositional logic
The Megarian school of philosophy
The Megarian school of philosophy was founded by Euclid of Megara (c. 430–360
BC). (This is not the famous geometer Euclid of Alexandria.) He was a disciple of
Socrates and, following on his ideas, claimed that there is one universal Good in
the world, sometimes also called Wisdom, God, or Reason, and that nothing that is
not Good exists. Euclid used logic, in a dialogue form, to defend his ideas and win
arguments. He applied extensively reductio ad absurdum (see Section 2.4) in his
argumentation.
Eubulides (4th century BC) was a pupil of Euclid of Megara and a strong oppo-
nent of Aristotle. He was most famous for inventing several paradoxes, still boggling
the mind today. The most popular of them is the Liar’s paradox, also attributed to
Epimenides (6th century BC), a Cretan, who is claimed to have said that “ All Cre-
tans are liars” (which is not a paradox yet, just a necessarily false statement).
Diodorus Cronus (?–c. 284 BC) was another prominent philosopher from the
Megarian Dialectical school. He made important early contributions to logic, espe-
cially on the theory of conditionals and the concepts of “possible” and “necessary”,
thus following Aristotle in laying the foundations of modal logic. He is most famous
for his Master argument in response to Aristotle’s discussion of future contin-
gents, such as “There will be a sea battle tomorrow”. Diodorus’ argument implied
that whatever is possible is actually necessary, hence there are no contingencies.
Philo of Megara (c. 400 BC) was a disciple of Diodorus Cronus. He was also
his most famous opponent in their disputes concerning the modal notions of “possi-
ble” and “necessary” and on the criteria for truth of conditional statements. Notably,
Philo regarded a conditional as false only if it has both a true antecedent and a false
consequent, essentially inventing the truth-functional implication which we now
use in classical propositional logic, also known as material implication.

18
Logic as a Tool
The Stoic school of philosophy
Greek philosopher Zeno of Citium (c. 335–265 BC), a pupil of
Diodorus Cronus, founded the Stoic school in the early 3rd cen-
tury BC. Zeno and his school had an elaborated theory of philos-
ophy as a way of life, and also made influential contributions to
physics, cosmology, epistemology and ethics.
Zeno taught that the Universal Reason (Logos, from which the
word Logic originated) was the greatest good in life and living
in accordance with it was the purpose of human life. The Stoic
school was engaged in logical argumentation and essentially laid
the foundations of propositional logic as an alternative to the Aris-
totelian logic of Syllogisms (see Section 3.5).
Chrysippus (c. 280–207 BC) was a philosopher and logician
from the Stoic School. He wrote over 300 books (very few of sur-
vived to be studied) on many fundamental topics of logic, includ-
ing propositions and propositional connectives (he introduced
the implication, conjunction and exclusive disjunction), logical
consequence, valid arguments, logical deduction, causation, and
logical paradoxes, and on the most popular non-classical logics,
including modal, tense, and epistemic logics. Chrysippus is often
regarded as the founder of propositional logic, and is one of the
most important early formal logicians along with Aristotle.
1.2
Propositional logical consequence: logically correct inferences
The central problem of logic is the study of correct argumentation and reasoning. In this
section I define and discuss what it means for an argument to be logically correct or not by
formalizing the fundamental logical concept of logical consequence. This is done here just
for a simple type of logical arguments that only involve propositional reasoning, called
propositional arguments.
1.2.1
Propositional logical consequence
The intuition behind logically correct reasoning is simple: starting from true premises
should always lead to true conclusions. Let us first make this intuition precise.
Definition 9 A propositional formula B is a logical consequence of the propositional
formulae A1, . . . , An, denoted 4
A1, . . . , An |= B
4 Note that I use here the same symbol we used to indicate tautologies. This will be justified soon.

Understanding Propositional Logic
19
if B is true whenever all A1, . . . , An are true. That means: if every truth assignment to
the variables occurring in A1, . . . , An, B for which the formulae A1, . . . , An is true,
then the formula B is also true.
When A1, . . . , An |= B, we also say that B follows logically fromA1, . . . , An, or
that A1, . . . , An imply logically B.
In the context of A1, . . . , An |= B, the formulae A1, . . . , An are called assumptions
while the formula B is called a conclusion.
When A1, . . . , An |= B is not the case, we write A1, . . . , An ⊭B.
If A1, . . . , An |= B then every substitution of propositions for the variables occurring
in A1, . . . , An, B which turns the formulae A1, . . . , An into true propositions also turns
the formula B into a true proposition.
In order to check whether A1, . . . , An |= B we can simply complete the truth tables
of A1, . . . , An, B and check, row by row, if the following holds: whenever all formulae
A1, . . . , An have a truth value T in that row,B must also have a truth value T. (Of course,
it is possible for B to be true without any of A1, . . . , An being true.) If that holds in every
row in the table, then B does follow logically from A1, . . . , An; if that fails in at least
one row, then B does not follow logically from A1, . . . , An.
Thus, B does not follow logically from A1, . . . , An, just in case there is a truth assign-
ment which renders all formulae A1, . . . , An true and B false.
Example 10 (Some simple cases of logical consequences)
1. Any formula B follows logically from any set of formulae that contains B. (Why?)
2. Any tautology follows logically from any set of formulae, even from the empty set!
3. For any formulae P and Q we claim that P, P →Q |= Q.
Note first that, whatever the formulae P and Q, any truth assignment eventually ren-
ders each of them true or false and all combinations of these truth values can be
possible, so we can treat P and Q as propositional variables and consider the truth
tables for P, Q and P →Q:
P
Q
P
P→Q
Q
T
T
T
T
T
T
F
T
F
F
F
T
F
T
T
F
F
F
T
F
Indeed, in every row where the 3rd and 4th entries are T, the 5th entry is also T.
4. P →R, Q →R |= (P ∨Q) →R for any formulae P, Q, R.
Likewise, it suffices to show that p →r, q →r |= (p ∨q) →r, for propositional vari-
ables p, q, r.
Indeed, in every row of the truth table where the truth values of p →r and q →r
are T, the truth value of (p ∨q) →r is also T.

20
Logic as a Tool
p
q
r
p →r
q →r
p ∨q
(p ∨q) →r
T
T
T
T
T
T
T
T
T
F
F
F
T
F
T
F
T
T
T
T
T
T
F
F
F
T
T
F
F
T
T
T
T
T
T
F
T
F
T
F
T
F
F
F
T
T
T
F
T
F
F
F
T
T
F
T
5. Is it true that p ∨q, q →p |= p ∧q? We check the truth table:
p
q
p ∨q
q →p
p ∧q
T
T
T
T
T
T
F
T
T
F
F
T
. . .
. . .
. . .
F
F
. . .
. . .
. . .
and see that in the 2nd row both assumptions are true while the conclusion is false.
This suffices to conclude that p ∨q, q →p ⊭p ∧q, so there is no need to fill in the
truth table any further.
Recall that |= A means that A is a tautology. Tautologies and logical consequences are
closely related. First, note that a formula A is a tautology if and only if (iff) it follows
logically from the empty set of formulae. Indeed, we have already noted that if |= A
then ∅|= A. Now, to see that if ∅|= A then |= A, suppose ∅|= A and take any truth
assignment. Note that it satisfies every formula from ∅. Why? Well, there can be no formula
in ∅which is not satisfied, because there are no formulae in ∅at all! Since ∅|= A, that
truth assignment must also satisfy A. Thus, A is satisfied by every truth assignment5.
In general, we have the following equivalences.
Proposition 11 For any propositional formulae A1, . . . , An, B, the following are equiv-
alent:
1. A1, . . . , An |= B
2. A1 ∧· · · ∧An |= B
3. |= (A1 ∧· · · ∧An) →B
4. |= A1 →(· · · →(An →B) · · ·)
I leave the proofs of these equivalences as easy exercises.
Checking logical consequences can be streamlined, in the same way as checking tau-
tologies, by organizing a systematic search for a falsifying assignment. In order to check
5 Here we did some logical reasoning based on the very same concepts of truth and logical consequence that we are
discussing. When reasoning about logic, this kind of bootstrapping reasoning is inevitable!

Understanding Propositional Logic
21
if B follows logically from A1, . . . , An we look for a truth assignment to the variables
occurring in A1, . . . , An, B that renders all A1, . . . , An true and B false. If we succeed,
then we have proved that B does not follow logically from A1, . . . , An; otherwise we
want to prove that no such assignment is possible by showing that the assumption that it
exists leads to a contradiction.
For example, let us check again that p →r, q →r |= (p ∨q) →r. Suppose that for
some assignment (p →r) : T, (q →r) : T and ((p ∨q) →r) : F. Then (p ∨q) : T and
r : F , hence p : T or q : T.
Case 1: p : T. Then (p →r) : F, that is, a contradiction.
Case 2: q : T. Then (q →r) : F, again, a contradiction.
Thus, there is no assignment that falsifies the logical consequence above.
1.2.2
Logically sound rules of propositional inference and logically correct
propositional arguments
We now apply the notion of logical consequence to define and check whether a given
propositional argument is logically correct. Let us first introduce some terminology.
Definition 12 A rule of propositional inference (inference rule, for short) is a scheme:
P1, . . . , Pn
C
where P1, . . . , Pn, C are propositional formulae. The formulae P1, . . . , Pn are called
premises of the inference rule, and C is its conclusion.
An instance of an inference rule is obtained by uniform substitution of concrete propo-
sitions for the variables occurring in all formulae of the rule. Every such instance is called
a propositional inference, or a propositional argument based on that rule.
Definition 13 An inference rule is (logically) sound if its conclusion follows logically
from the premises. A propositional argument is logically correct if it is an instance of a
logically sound inference rule.
Example 14
1. The following inference rule
p, p →q
q
is sound, as we have already seen. This rule is known as the Detachment rule or Modus
Ponens, and is very important in the logical deductive systems called axiomatic sys-
tems which we will study in Chapter 2.
The inference
Alexis is singing.
If Alexis is singing, then Alexis is happy.
Alexis is happy.
is therefore logically correct, being an instance of that inference rule.

22
Logic as a Tool
2. The inference rule
p, q →p
q
is not sound: if p is true and q is false, then both premises are true while the conclusion
is false.
Therefore the inference
5 is greater than 2.
5 is greater than 2 if 5 is greater than 3.
5 is greater than 3.
is not logically correct, despite the truth of both premises and the conclusion, as it is
an instance of an unsound rule.
Some remarks are in order here.
• It is very important to realize that the logical correctness of an inference does not always
guarantee the truth of the conclusion, but only when all premises are true. In other
words, if at least one premise of a logically correct inference is false, then the conclusion
may also be false. For instance, the inference
5 divides 6.
If 5 divides 6, then 5 divides 11.
5 divides 11.
is logically correct (being an instance of the rule Modus Ponens) in spite of the falsity of
the conclusion. This does not contradict the idea of logical correctness of an inference,
because the first premise is false. (What about the second premise?)
• Conversely, if the conclusion of an inference happens to be true, this does not neces-
sarily mean that the inference is logically correct as in the second example above.
• Moreover, it may happen that the truth of the premises of an inference does imply the
truth of the conclusion, and yet the inference is not logically correct. For instance, the
correctness of the inferences
Today is Monday.
Tomorrow will be Tuesday.
or
a = 2, a + b = 5
b = 3
is based not on logical consequence but, in the first case, on the commonly known fact that
Tuesday always follows after Monday, and in the second case on some laws of arithmetic.
Indeed, the first inference is based on the rule
p
q

Understanding Propositional Logic
23
and the second inference (although the statements are not really propositions) on
p, q
r ,
both of which are clearly unsound.
To summarize, the meaning and importance of logically correct inferences is that only
such inferences guarantee that if all premises are true, then the conclusions will also be
true. That is why only logically correct inferences are safe to be employed in our reasoning.
Let us now look at a few more examples.
1. The inference rule
q, p ∨¬q
p
is logically sound. You can check this in two ways: by applying the definition or by
showing that the corresponding formula
(q ∧(p ∨¬q)) →p
is a tautology.
Consequently, the inference
Olivia is crying or Olivia is not awake.
Olivia is awake.
Olivia is crying.
is logically correct, being an instance of the rule above.
2. Now, take the argument
If a divides b or a divides c, then a divides bc.
a divides bc.
a does not divide b.
Therefore a divides c.
where a, b, c are certain integers. This argument is an instance of the following
inference rule:
(p ∨q) →r, r, ¬p
q
.
Let us see if we can invalidate this rule. For that we need an assignment such that
((p ∨q) →r) : T, r : T, and ¬p : T, hence p : F and q : F. Indeed, the assignment
p : F, q : F, and r : T renders all premises true and the conclusion false. (Check this.)
The rule is therefore not logically sound, hence the argument above is not logically
correct.
I develop the method behind the last argument above in the next chapter.
1.2.3
Fallacies of the implication
As an application let us analyze some very common forms of correct and incorrect rea-
soning related to implications. Given the implication
A →B

24
Logic as a Tool
we can form the so-called derivative implications:
• the converse of A →B is B →A;
• the inverse of A →B is ¬A →¬B; and
• the contrapositive of A →B is ¬B →¬A.
Now, suppose we know that A →B is true. What can we say about the truth of its
derivatives? To answer that question, look at each of the inferences:
A →B
B →A,
A →B
¬A →¬B ,
A →B
¬B →¬A
Exercise 15 Show that the first two of these inferences are incorrect, while the third infer-
ence is correct.
The truth of an implication A →B therefore only implies the truth of its contrapositive,
but not the truth of the converse or inverse. These are mistakes that people often make,
respectively called the fallacy of the converse implication and the fallacy of the inverse
implication. For example, the truth of the implication “If it has just rained, then the
tennis court is wet” does not imply that either of “If the tennis court is wet, then it
has just rained” and “If it has not just rained, then the tennis court is not wet” is
true – someone may have just watered the court on a clear sunny day – but it certainly
implies that “If the court is not wet, then it has not just rained.” In fact, it can easily
be checked that
¬B →¬A
A →B
is also logically correct. This is the basis of the method of proof by contraposition, which
is discussed further in Section 2.5.
References for further reading
Propositional logical consequence, as well as propositional arguments, inference rules
and their logical correctness, are treated in more details in Carroll (1897), Tarski (1965),
Kalish and Montague (1980), Gamut (1991), Nerode and Shore (1993), Jeffrey (1994),
Barwise and Echemendy (1999), Smith (2003), Boole (2005), Bornat (2005), Chiswell
and Hodges (2007), Copi et al. (2010), Halbach (2010), Ben-Ari (2012), and van Benthem
et al. (2014).
Exercises
1.2.1
Prove Proposition 11. (Hint: you do not have to prove all pairs of equivalences. It
is sufficient to show, for instance, that claim 1 implies 2, which implies 3, which
implies 4, which implies 1.)
1.2.2
Show that the first two of the following inference rules, corresponding to the
derivative implications, are logically unsound while the third rule is sound.

Understanding Propositional Logic
25
(a)
p →q
q →p
(b)
p →q
¬p →¬q
(c)
p →q
¬q →¬p
1.2.3
Using truth tables, check if the following inference rules are sound.
(a)
p →q, ¬q ∨r, ¬r
¬p
(b)
¬p →¬q, q, ¬(p ∧¬r)
r
(c)
p →q, p ∨¬r, ¬r
¬q ∨r
(d)
((p ∧q) →r), ¬(p →r)
q →r
1.2.4
Write down the inference rules on which the following arguments are based and
check their logical soundness, using truth tables.
(a) In the following argument, X is a certain number.
X is greater than 3.
X is greater than or equal to 3.
(b) In the following argument, Y is a certain number.
If Y is greater than –1, then Y is greater than –2.
Y is not greater than –2.
Y is not greater than –1.
(c)
If the triangle ABC has a right angle, then it is not equilateral.
The triangle ABC does not have a right angle.
Therefore, the triangle ABC is equilateral.
(d)
If Victor is good at logic, then he is clever.
If Victor is clever, then he is rich.
Therefore, if Victor is good at logic, then he is rich.
(e) In the following argument n is a certain integer.
If n is divisible by 2 and n is divisible by 3, then n is divisible by 6.
If n is divisible by 6, then n is divisible by 2.
n is not divisible by 3.
Therefore, n is not divisible by 6.
1.2.5
For each of the following implications construct the converse, inverse and the con-
trapositive, phrased in the same way.
(a) If a is greater than –1, then a is greater than –2.
(b) x is not prime if x is divisible by 6.
(c) x is positive only if its square is positive.

26
Logic as a Tool
(d) The triangle ABC is equilateral whenever its medicentre and orthocen-
tre coincide.
(e) For the function f to be continuous, it is sufficient that it is differentiable.
(f) For a function not to be differentiable, it is sufficient that it is discontin-
uous.
(g) For the integer n to be prime, it is necessary that it is not divisible by 10.
George Boole (2.11.1815–8.12.1864) was an English mathe-
matician who first proposed and developed an algebraic approach
to the study of logical reasoning. Boole’s first contribution to
logic was a pamphlet called Mathematical Analysis of Logic,
written in 1847. He published his main work on logic, An
Investigation of the Laws of Thought, on which are Founded
the Mathematical Theories of Logic and Probabilities, in
1854. In it Boole developed a general mathematical method
of logical inference, laying the foundations of modern mathematical logic. His sys-
tem proposed a formal algebraic treatment of propositions by processing only their
two possible truth values: yes–no, true–false, zero–one. In Boole’s system, if x stands
for “white things” then 1 −x stands for “non-white things;” if y stands for “sheep”,
then xy stands for “white sheep”, etc. x(1 −x) denotes things that are both white
and non-white, which is impossible. A proposition of the shape x(1 −x) is therefore
always false, that is, has a truth value 0. The algebraic law x(1 −x) = 0 therefore
emerges. The resulting algebraic system is known today as (the simplest) Boolean
algebra.
Boole also argued that symbolic logic is needed in other mathematical disciplines,
especially in probability theory. He wrote: “· · · no general method for the solution of
questions in the theory of probabilities can be established which does not explicitly
recognise those universal laws of thought which are the basis of all reasoning · · ·”
Propositional logic today is somewhat different from Boole’s system of logic 150
years ago, but the basic ideas are the same. That is why propositional logic is also
often called Boolean logic in recognition of Boole’s ground-breaking contribution.
It is not only a fundamental system of formal logical reasoning, but it also provides
the mathematical basis of the logical circuits underlying the architecture of modern
digital computers.

Understanding Propositional Logic
27
William Stanley Jevons (1.09.1835–13.08.1882) was an English
economist and logician known for his pioneering works on polit-
ical and mathematical economics, including the theory of utility.
As well as contributing to the early development of modern logic,
in 1869 he designed one of the first mechanical computers which
he called the logic piano.
Jevons studied natural sciences and moral philosophy at the
University College of London and, in 1866, was appointed
Professor of Logic, Mental and Moral Philosophy and Professor
of Political Economy at Owens College. His book A General Mathematical Theory
of Political Economy (1862) is one of the first works on mathematical methods in
economics which, being concerned with quantities, he regarded as an essentially
mathematical science.
Jevons’ most important work on scientific methods is his Principles of Science
(1874). In 1870 he published Elementary Lessons on Logic, which soon became
the most widely read elementary textbook on logic in the English language, later
supplemented by his Studies in Deductive Logic.
Jevons developed a general theory of induction, which he regarded as an inverse
method of deduction; he also developed and published his own treatments of Boole’s
approach to logic and on the general theory of probability, and studied the relation
between probability and induction.

28
Logic as a Tool
1.3
Logical equivalence: negation normal form of propositional
formulae
1.3.1
Logically equivalent propositional formulae
Definition 16 The propositional formulae A and B are logically equivalent, denoted
A ≡B, if for every assignment of truth values to the variables occurring in them they
obtain the same truth values.
Being a little imprecise (you’ll see why), we can say that A and B are logically equiv-
alent if they have the same truth tables.
• Every tautology is equivalent to ⊤. For example, p ∨¬p ≡⊤.
• Every contradiction is equivalent to ⊥. For example, p ∧¬p ≡⊥.
• ¬¬p ≡p: a double negation of a proposition is equivalent to the proposition itself.
• ¬(p ∧q) ≡(¬p ∨¬q) and ¬(p ∨q) ≡(¬p ∧¬q). These are known as De Morgan’s
laws. Let us check the first, using simplified truth tables:
p
q
¬
(p ∧
q)
(¬ p
∨
¬
q)
T
T
F
T T
T
F T
F
F
T
T
F
T
T F
F
F T
T
T
F
F
T
T
F F
T
T F
T
F
T
F
F
T
F F
F
T F
T
T
F
• (p ∧(p ∨q)) ≡(p ∧p). There is a small problem here: formally, the truth tables of
these formulae are not the same, as the first contains two variables (p and q) while the
second contains only p. However, we can always consider that q occurs vacuously in
the second formula and include it in its truth table:
p
q
(p ∧
(p ∨
q))
(p ∧
p)
T
T
T T
T T
T
T T
T
T
F
T T
T T
F
T T
T
F
T
F F
F T
T
F F
F
F
F
F F
F F
F
F F
F
Checking logical equivalence can be streamlined, just like checking logical validity
and consequence, by systematic search for a falsifying assignment, as follows. In order
to check if A ≡B we try to construct a truth assignment to the variables occurring in A
and B which renders one of them true while the other false. If such an assignment exists,
the formulae are not logically equivalent; otherwise, they are. For example, let us check
the second De Morgan’s law: ¬(p ∨q) ≡(¬p ∧¬q). There are two possibilities for an
assignment to falsify that equivalence:
(i) ¬(p ∨q) : T and (¬p ∧¬q) : F. Then p ∨q : F hence p : F, and q : F, but then
(¬p ∧¬q) : T: a contradiction.

Understanding Propositional Logic
29
(ii) ¬(p ∨q) : F, and (¬p ∧¬q) : T. Then ¬p : T and ¬q : T, hence p : F, and q : F. But
then ¬(p ∨q) : T: a contradiction again.
There is therefore no falsifying assignment and the equivalence holds.
1.3.2
Basic properties of logical equivalence
Caution: do not confuse the propositional connective ↔and logical equivalence between
formulae. These are different things: the former is a logical connective, a symbol in our
object language, whereas the latter is a relation between formulae, that is, a statement in
our metalanguage. However, as I show below, there is a simple relation between them.
1. Logical equivalence is reducible to logical validity:
A ≡B
iff
|= A ↔B.
Indeed, they mean the same: that A and B always take the same truth values.
2. Logical equivalence is likewise reducible to logical consequence:
A ≡B
iff
A |= B
and
B |= A
3. The relation ≡is an equivalence relation, that is, for every formulae A, B, C it is:
(a) reflexive: A ≡A;
(b) symmetric: if A ≡B then B ≡A;
(c) transitive: if A ≡B and B ≡C then A ≡C.
4. Moreover, ≡is a congruence with respect to the propositional connectives, that is:
(a) if A ≡B then ¬A ≡¬B;
(b) if A1 ≡B1 and A2 ≡B2 then (A1 • A2) ≡(B1 • B2), for • ∈{∧, ∨, →, ↔}.
5. The following property of equivalent replacement holds. For any propositional for-
mulae A, B, C and a propositional variable p, presumably occurring in C, if A ≡B
then C(A/p) ≡C(B/p), where C(X/p) is the result of simultaneous substitution of
all occurrences of p by X.
Logical equivalence between propositional formulae can therefore be treated just like
equality between algebraic expressions.
1.3.3
Some important logical equivalences
I now briefly present and discuss some important and useful logical equivalences. Verify-
ing all of these are easy, but useful, exercises.
1.3.3.1
Algebraic laws for the logical connectives
I begin with some important logical equivalences which are used, for example, for equiv-
alent transformations of propositional formulae to so-called conjunctive and disjunctive
normal forms that I introduce later.

30
Logic as a Tool
• Idempotency: p ∧p ≡p; p ∨p ≡p.
• Commutativity: p ∧q ≡q ∧p; p ∨q ≡q ∨p.
• Associativity: (p ∧(q ∧r)) ≡((p ∧q) ∧r); (p ∨(q ∨r)) ≡((p ∨q) ∨r).
This law allows parentheses to be omitted in multiple conjunctions and disjunctions.
• Absorption: p ∧(p ∨q) ≡p; p ∨(p ∧q) ≡p.
• Distributivity: p ∧(q ∨r) ≡(p ∧q) ∨(p ∧r); p ∨(q ∧r) ≡(p ∨q) ∧(p ∨r).
1.3.3.2
Equivalences mutually expressing logical connectives
The following logical equivalences, the proofs of which are left as easy exercises, can be
used to define logical connectives in terms of others:
• ¬A ≡A →⊥. We sometimes use this equivalence in colloquial expressions, such as
“If this is true then I can fly”, when we mean “This cannot be true.”
• A ↔B ≡(A →B) ∧(B →A). This equivalence allows us to consider the bicondi-
tional as definable connective, which we will often do.
• A ∨B ≡¬(¬A ∧¬B).
• A ∧B ≡¬(¬A ∨¬B).
• A →B ≡¬A ∨B.
• A →B ≡¬(A ∧¬B).
• A ∨B ≡¬A →B.
• A ∧B ≡¬(A →¬B).
We therefore see that each of ∧, ∨, and →can be expressed by means of any other of
these using negation.
1.3.3.3
Some simplifying equivalences
Other useful logical equivalences can be used to simplify formulae:
• A ∨¬A ≡⊤, A ∧¬A ≡⊥
• A ∧⊤≡A, A ∧⊥≡⊥
• A ∨⊤≡⊤, A ∨⊥≡A
• A →⊤≡⊤, A →⊥≡¬A
• ⊤→A ≡A, ⊥→A ≡⊤.
• ¬A →¬B ≡B →A (Every implication is equivalent to its contrapositive.)
1.3.3.4
Negating propositional formulae: negation normal form
In mathematical and other arguments we sometimes have to negate a formalized statement
and then use the result in further reasoning. For that, it is useful to transform, up to logi-
cal equivalence, the formula formalizing the statement in negation normal form, where
negation may only occur in front of propositional variables. Such transformation can be

Understanding Propositional Logic
31
done by step-by-step importing of all occurrences of negations inside the other logical
connectives using the following equivalences, some of which we already know:
• ¬¬A ≡A
• ¬(A ∧B) ≡¬A ∨¬B
• ¬(A ∨B) ≡¬A ∧¬B
• ¬(A →B) ≡A ∧¬B
• ¬(A ↔B) ≡(A ∧¬B) ∨(B ∧¬A).
Example 17 Equivalent transformation to negation normal form:
¬((A ∨¬B) →(¬C ∧D))
≡(A ∨¬B) ∧¬(¬C ∧D)
≡(A ∨¬B) ∧(¬¬C ∨¬D)
≡(A ∨¬B) ∧(C ∨¬D).
References for further reading
To read more on propositional equivalence and negation normal form of propositional
formulae see Nerode and Shore (1993), Jeffrey (1994), Barwise and Echemendy (1999),
Hedman (2004), Nederpelt and Kamareddine (2004), Boole (2005), Chiswell and Hodges
(2007), and Ben-Ari (2012).
Exercises
1.3.1
Verify the following logical laws:
(a) Idempotency: p ∧p ≡p, p ∨p ≡p
(b) Commutativity: p ∧q ≡q ∧p, p ∨q ≡q ∨p
(c) Associativity: (p ∧(q ∧r)) ≡((p ∧q) ∧r), (p ∨(q ∨r)) ≡((p ∨q) ∨r)
(d) Absorption: p ∧(p ∨q) ≡p, p ∨(p ∧q) ≡p
(e) Distributivity: p∧(q ∨r) ≡(p∧q)∨(p∧r), p∨(q ∧r) ≡(p∨q) ∧(p ∨r)
(f) De Morgan’s laws ¬(p ∧q) ≡(¬p ∨¬q), ¬(p ∨q) ≡(¬p ∧¬q)
1.3.2
Prove the following logical equivalences:
(a) ¬(p ↔q) ≡(p ∧¬q) ∨(q ∧¬p)
(b) (p →q)∧(p →r) ≡p →(q ∧r)
(c) (p →q)∨(p →r) ≡p →(q ∨r)
(d) ¬(p ↔q) ≡(p ∧¬q) ∨(q ∧¬p)
(e) ¬(p ↔q) ≡¬p ↔q ≡p ↔¬q
(f) p →(q →r) ≡(p ∧q) →r
(g) p →(q →r) ≡q →(p →r)
(h) (p →r) ∧(q →r) ≡(p∨q) →r
(i) p ↔q ≡q ↔p
(j) p ↔(q ↔r) ≡(p ↔q) ↔r
1.3.3
Determine which of the following pairs of formulae are logically equivalent:
(a) p →q and ¬p ∨q
(b) ¬(p →q) and p ∧¬q
(c) ¬p →¬q and q →p
(d) p →¬q and q →¬p
(e) ¬(p →¬q) and p ∧q
(f) ((p →q) →q) →q and p ∨q

32
Logic as a Tool
(g) (p →r)∧(q →r) and (p∧q) →r
(h) (p →r)∨(q →r) and (p∨q) →r
(i) ((p∧q)→r) and (p→r)∨(q→r)
(j) p →(q →r) and (p →q) →r
(k) p ↔(q ↔r) and q ↔(p ↔r)
(l) p→(q→r)and(p→q) →(p →r)
1.3.4
Negate each of the following propositional formulae and transform the result to
an equivalent formula in a negation normal form.
(a) (p ∨¬q) ∧¬p
(b) (p →q) ∨(¬p →¬q)
(c) (p →¬q) →p
(d) p →(¬q →p)
(e) (p ↔¬q) →¬r
(f) p →(¬q ↔r)
Augustus De Morgan (27.6.1806–18.3.1871) was a British
mathematician, logician and a popularizer of mathemat-
ics. Influenced by George Boole, he pioneered the appli-
cation of algebraic methods to the study of logic in the
mid-19th century, becoming one of the founding fathers
of modern mathematical logic. In particular, he was the
first to formulate the logical equivalences now known as
de Morgan’s laws.
De Morgan was born in Madura, India and became blind
in one eye soon after his birth. He graduated from Trinity

Understanding Propositional Logic
33
College, Cambridge and in 1828 became the first Professor of Mathematics at the
newly established University College of London, where he taught for most of his
academic life.
He was an enthusiastic and prolific writer of over 700 popular articles in mathe-
matics for the Penny Cyclopedia, aiming to promote the education of mathematics
in Britain. In an 1838 publication he formally introduced the term “mathematical
induction” and developed the so-far informally used method of mathematical induc-
tion into a precise mathematical technique. He also wrote the books Trigonometry
and Double Algebra and The Differential and Integral Calculus. In 1847 he pub-
lished his main work on mathematical logic, Formal Logic: The Calculus of Infer-
ence, Necessary and Probable, which was used for a very long time and was last
reprinted in 2003. De Morgan was also a passionate collector of mathematical puz-
zles, curiosities, and paradoxes, many of which he included in his book A Budget of
Paradoxes published in 1872 (now digitalized and available on the internet).
In 1866 De Morgan became one of the founders and the first president of the
London Mathematical Society. There is a crater on the Moon named after him.
Hugh MacColl (1831–1909) was a Scottish mathematician,
logician, and novelist who made some important early contri-
butions to modern logic.
MacColl grew up in a poor family in the Scottish
Highlands
and
never
obtained
a
university
education
because he could not afford it and refused to accept to
take orders in the Church of England, a condition under
which William Gladstone was prepared to support his
education at Oxford. Consequently, he never obtained a
regular academic position; he was a highly intelligent person
however.
During 1877–1879 MacColl published a four-part article establishing the
first-known variant of the propositional calculus, which he called the “calculus of
equivalent statements”, preceding Gottlob Frege’s Begriffschrifft. Furthermore,
MacColl’s work on the nature of implication was later credited by C.I. Lewis as
the initial inspiration of his own innovative work in modal logic. MacColl also
promoted logical pluralism by exploring on a par ideas for several different logical
systems such as modal logic, logic of fiction, connexive logic, many-valued logic,
and probability logic, establishing himself as a pioneer in the field known as
non-classical logics today.

34
Logic as a Tool
1.4
Supplementary: Inductive definitions and structural induction
and recursion
In section 1.1 I defined propositional formulae using a special kind of definition, which
refers to the very notion it is defining. Such definitions are called inductive. They are very
common and important, especially in logic, because they are simple, elegant, and indis-
pensable when an infinite set of structured objects is to be defined. Moreover, properties
of an object defined by inductive definitions can be proved by a uniform method, called
structural induction, that resembles and extends the method of mathematical induction
used to prove properties of natural numbers. Here I present the basics of the general the-
ory of inductive definitions and structural induction. Part of this section, or even all of it,
can be skipped, but the reader is recommended to read it through.
1.4.1
Inductive definitions
Let us begin with well-known cases: the inductive definition of words in an alphabet and
then natural numbers as special words in a two-letter alphabet. Note the pattern.
1.4.1.1
The set of all finite words in an alphabet
Consider a set A. Intuitively, a (finite) word in A is any string of elements of A. We
formally define the set of (finite) words in the alphabet A inductively as follows.
1. The empty string ϵ is a word in A.
2. If w is a word in A and a ∈A, then wa is word in A.
The idea of this definition is that words in A are those, and only those, objects that can
be constructed following the two rules above.
1.4.1.2
The set of natural numbers
We consider the two-letter alphabet {0, S}, where 0, S are different symbols, and formally
define natural numbers to be special words in that alphabet, as follows.
1. 0 is a natural number.
2. If n is a natural number then Sn is a natural number.
The definition above defines the infinite set {0, S0, SS0, SSS0, · · · } .
Hereafter we denote S· · · n times · · ·S0 by n and identify it with the (intuitive notion of)
natural number n.
1.4.1.3
The set of propositional formulae
Let us denote the alphabet of symbols used in propositional logic AP L. Note that it
includes a possibly infinite set PVAR of propositional variables.

Understanding Propositional Logic
35
We now revisit the inductive definition of propositional formulae as special words in the
alphabet of symbols used in propositional logic, by paying closer attention to the structure
of the definition. I emphasize the words is a propositional formula so we can see shortly
how the definition transforms into an explicit definition and an induction principle.
Definition 18 The property of a word in AP L of being a propositional formula is defined
inductively as follows.
1. Every Boolean constant (i.e., ⊤or ⊥) is a propositional formula.
2. Every propositional variable is a propositional formula.
3. If (the word) A is a propositional formula then (the word) ¬A is a propositional
formula.
4. If each of (the words) A and B is a propositional formula then each of (the words)
(A ∧B), (A ∨B), (A →B), and (A ↔B) is a propositional formula.
The meaning of the inductive definition above can be expressed equivalently by the
following explicit definition, which essentially repeats the definition above but replaces
the phrase “is a propositional formula” with “is in (the set) FOR.”
Definition 19 The set of propositional formulae FOR is the least set of words in the alpha-
bet of propositional logic such that the following holds.
1. Every Boolean constant is in FOR.
2. Every propositional variable is in FOR.
3. If A is in FOR then ¬A is in FOR.
4. If each of A and B is in FOR then each of (A ∧B), (A ∨B), (A →B), and (A ↔B)
is in FOR.
This pattern of converting the inductive definition into an explicit definition is general
and can be applied to each of the other inductive definitions presented here. However, we
have not yet proved that the definition of the set FOR given above is correct in the sense
that the least (by inclusion) set described above even exists. Yet, if it does exist, then it is
clearly unique because of being the least set with the described properties. We will prove
the correctness later.
1.4.1.4
The subgroup of a given group, generated by a set of elements
I now provide a more algebraic example. The reader not familiar with the notions of groups
and generated subgroups can skip this safely.
Let G = ⟨G, ◦,−1, e⟩be a group and X be a subset of G. The subgroup of G gener-
ated by X is the least subset [X]G of G such that:
1. e is in [X]G.
2. Every element from X is in [X]G.

36
Logic as a Tool
3. If a ∈[X]G then a−1 ∈[X]G.
4. If a, b ∈[X]G then a ◦b ∈[X]G.
Exercise: re-state the definition above as an inductive definition.
1.4.2
Induction principles and proofs by induction
With every inductive definition, a scheme for proofs by induction can be associated. The
construction of this scheme is uniform from the inductive definition, as illustrated in the
following.
1.4.2.1
Induction on the words in an alphabet
We begin with a principle of induction that allows us to prove properties of all words in a
given alphabet. Given an alphabet A, let P be a property of words in A such that:
1. The empty string ϵ has the property P.
2. If the word w in A has the property P and a ∈A, then the word wa has the property P.
Then, every word w in A has the property P.
1.4.2.2
Induction on natural numbers
We can now formulate the well-known principle of mathematical induction on natural
numbers in terms of the formal definition of natural numbers given above.
Let P be a property of natural numbers such that:
1. 0 has the property P.
2. For every natural number n, if n has the property P then Sn has the property P.
Then every natural number n has the property P.
Here is the same principle, stated in set-theoretic terms:
Let P be a set of natural numbers such that:
1. 0 ∈P.
2. For every natural number n, if n ∈P then Sn ∈P.
Then every natural number n is in P, that is, P = N.
1.4.2.3
Structural induction on propositional formulae
Following the same pattern, we can now state a principle of induction that allows us to
prove properties of propositional formulae. Note that this principle is obtained almost

Understanding Propositional Logic
37
automatically from the inductive definition of propositional formulae by replacing
throughout that definition the words is a propositional formula with satisfies the
property P. Let P be a property of propositional formulae such that:
1. Every Boolean constant satisfies the property P.
2. Every propositional variable satisfies the property P.
3. If A satisfies the property P then ¬A satisfies the property P.
4. If each of A and B satisfy the property P then each of (A ∧B), (A ∨B), (A →B),
and (A ↔B) satisfy the property P.
Then every propositional formula satisfies the property P.
Again, the same principle can be formulated in set-theoretic terms by treating the prop-
erty P as the set of those propositional formulae that satisfy it, and then replacing the
phrase satisfies the property P with is in the set P.
The induction principle can likewise be formulated for the elements of a subgroup of a
given group, generated by a given set of elements. I leave that as an exercise.
1.4.3
Basics of the general theory of inductive definitions and principles
1.4.3.1
An abstract framework for inductive definitions
We extract the common pattern in the examples above to formulate a uniform abstract
framework for inductive definitions and proofs by induction. The necessary ingredients
for an inductive definition are:
• A universe U.
In our examples, universes were sets of words in a given alphabet and the set of all
elements of a given group.
• A subset B ⊆U of initial (basic) elements.
In our examples, the sets of initial elements were: {ϵ}; {0}; {⊤, ⊥} ∪PVAR; and
the set X of generators of a subgroup.
• A set F of operations (constructors) in U.
In our examples, these were: the operation of appending a symbol to a word; the oper-
ation of prefixing S to a natural number; the propositional logical connectives regarded
as operations on words; and the group operations.
We fix the sets U, B, F arbitrarily thereafter. Our aim is to define formally the set of ele-
ments of U inductively defined over B by applying the operations in F , denoted C(B, F).
Intuitively, this will be the set defined by the following inductive definition:
1. Every element of B is in C(B, F).
2. For every operation f ∈F such that f : U n →U, if every x1, . . . , xn is in C(B, F)
then f(x1, . . . , xn) is in C(B, F).

38
Logic as a Tool
We give the set C(B, F) a precise mathematical meaning by defining it in two different,
yet eventually equivalent, ways.
1.4.3.2
Top-down closure construction
Definition 20 A set C ⊆U is:
1. closed under the operation f ∈F, such that f : U n →U, if f(x1, . . . , xn) ∈C for
every x1, . . . , xn ∈C.
2. closed, if it is closed under every operation f ∈F.
3. inductive, if B ⊆C and C is closed.
Remark 21 The elements of B can be regarded as constant (0-argument) functions in U,
and the condition B ⊆C can therefore be subsumed by closedness.
Proposition 22 Intersection of any family of inductive sets is an inductive set.
Proof. I leave this as an exercise.
Definition 23 C∗is the intersection of the family of all inductive sets.
By Proposition 22, C∗is the smallest inductive set.
1.4.3.3
Bottom-up inductive construction
Definition 24 A construction tree for an element x ∈U is a finite tree T(x), every node
of which is labeled with an element of U and the successors of every node are ordered
linearly, satisfying the following conditions:
1. Every leaf in T(x) is labeled by an element of B.
2. If a node in T(x) labeled with y has k successors labeled by elements listed in the
order of successors y1, . . . , yk, then there is a k-ary operation f ∈F such that y =
f(y1, . . . , yk).
3. The root of T(x) is labeled by x.
Definition 25 The height of a finite tree is the length (number of nodes minus 1) of the
longest path in the tree.
Definition 26 The rank of an element x ∈U is the least height r(x) of a construction
tree for x if it exists; otherwise, the rank is ∞.
Definition 27 We define a hierarchy of sets C0 ⊆C1 ⊆· · · ⊆C∗as follows.
1. Cn is the set of all elements of U with rank ≤n.
2. C∗:= 
n∈N
Cn.

Understanding Propositional Logic
39
Proposition 28 The following holds.
1. C0 = B.
2. Cn+1=Cn ∪{f(x1,. . ., xn)|f ∈F is an n-ary operation on U and x1,. . ., xn∈Cn}
Proof. Easy induction on the rank n, which I leave as an exercise for the reader. ■
1.4.3.4
Inductive definitions
Proposition 29 C∗is an inductive set.
Proof. Exercise. ■
Corollary 30 C∗⊆C∗, since C∗is the least inductive set.
Proposition 31 C∗⊆C∗.
Proof. We prove by induction on n that Cn ⊆C∗. Exercise. ■
Definition 32 C∗(= C∗) is the set inductively defined over B and F , denoted C(B, F).
1.4.3.5
Induction principle for inductively defined sets
We can easily generalize the ordinary principle of mathematical induction on natural num-
bers to induction in the set C(B, F).
Proposition 33 (Induction principle for C(B, F)) Let P be a property of elements of
U, such that:
1. Every element of B has the property P.
2. For every operation f ∈F, such that f : U n →U, if every x1, . . . , xn has the prop-
erty P then f(x1, . . . , xn) has the property P.
Then every element of C(B, F) has the property P.
The proof is straightforward. The two conditions above state precisely that the set of
elements of C(B, F) that have the property P is inductive; it therefore contains the least
inductive set, that is, C(B, F).
It is quite easy to see that the induction principle above generalizes all those formulated
earlier for our examples.
1.4.4
Inductive definitions and proofs in well-founded sets
Here I briefly generalize the induction principle from inductively defined sets to the more
abstract notion of well-founded sets.

40
Logic as a Tool
1.4.4.1
Well-founded sets
Definition 34 A partially ordered set (poset) (X, <) is well-founded if it contains no
infinite strictly descending sequences x1 > x2 > . . . .
A well-founded linear ordering is called a well-ordering.
Example 35
• Every finite poset is well-founded.
• ⟨N, <⟩is well-ordered, while ⟨Z, <⟩and ⟨Q, <⟩are not.
• The poset (P(X), ⊆), where X is any infinite set, is not well-founded.
• The lexicographic ordering in N2 defined by ⟨x1, y1⟩≤⟨x2, y2⟩iff x1 < x2 or (x1 =
x2 and y1 ≤y2) is a well-ordering in N2.
• The relation “A is a strict subformula of B” in the set FOR is a well-ordering.
Proposition 36 A poset ⟨X, <⟩is well-founded iff every non-empty subset of X has
a minimal element. Respectively, a linear ordering ⟨X, <⟩is a well-ordering iff every
non-empty subset of X has a least element.
1.4.4.2
Induction principle for well-founded sets
Let (X, <) be a well-founded poset. The induction principle for (X, <) states the fol-
lowing:
Let P ⊆X be such that for every x ∈X, if all elements of X less than x belong
to P then x itself belongs to P. Then P = X.
Proof. Assume the contrary, that is, X −P ̸= φ. Then X −P has a minimal element x.
Then all elements of X less than x belong to P, hence x must belong to P: a contradiction.
■
Example 37 Let P be a property (set) of propositional formulae such that for every for-
mula A ∈FOR, if all strict subformulae of X have the property (belong to the set) P,
then A itself has the property (belongs to the set) P.
Then every formula A ∈FOR has the property (belongs to the set) P.
1.4.5
Recursive definitions on inductively definable sets
We now consider the following general problem: given an inductively defined set C(B, F),
how should we define a function h on that set by using the inductive definition? The idea
is to first define h on the set B, and then provide rules prescribing how the definition of
that function propagates over all operations. Formally, in order to define by recursion a
mapping h : C(B, F ) →X where X is a fixed target set, we need:
1. A mapping h0 : B →X.
2. For every n-ary operation f ∈F a mapping Ff : X2n →X.

Understanding Propositional Logic
41
We now define the mapping h as follows:
1. If a ∈B then h(a) := h0(a).
2. For every n-ary operation f ∈F:
h(f(a1, . . . , an)) := Ff(a1, . . . , an, h(a1), . . . , h(an)).
We will soon discuss the meaning and correctness of such definitions, but let us first
look at some important particular cases.
1.4.5.1
Primitive recursion on natural numbers
Functions on natural numbers can be defined by so-called primitive recursion using the
inductive definition provided earlier in this section.
1. The basic scheme of primitive recursion is:
h(0) = a
h(n + 1) = h(Sn) = FS(n, h(n)).
For example, the scheme
h(0) = 1
h(Sn) = (n + 1)h(n)
defines the factorial function h(n) = n!.
2. The more general scheme of primitive recursion with parameters is:
h(m, 0) = F0(m).
h(m, Sn) = FS(m, n, h(m, n)).
For example, the scheme
h(m, 0) = m,
h(m, n + 1) = h(m, n) + 1
defines the function addition h(m, n) = m + n.
1.4.5.2
Truth valuations of propositional formulae
Recall that the set of propositional formulae FOR is built on a set of propositional vari-
ables PVAR and a truth assignment is a mapping s : PVAR →{T, F}. Now, given any
truth assignment s : PVAR →{T, F} we can define a mapping α : FOR →{T, F} that
extends it to a truth valuation, a function computing the truth values of all formulae in
FOR by recursion on the inductive definition of FOR as follows:

42
Logic as a Tool
1. α(t) = T, α(f) = F.
2. α(p) = s(p) for every propositional variable p.
3. α(¬A) = F¬(α(A)),
where F¬ : {T, F} →{T, F} is defined as follows: F¬(T) = F, F¬(F) = T.
4. α(A ∧B) = F∧(α(A), α(B)),
where F∧: {T, F}2 →{T, F} is defined as follows:
F∧(T, T) = T and F∧(T, F) = F∧(F, T) = F∧(F, F) = F.
(That is, F∧computes the truth table of ∧.)
5. α(A ∨B) = F∨(α(A), α(B)),
where F∨: {T, F}2 →{T, F} is defined as follows:
F∨(T, T) = F∨(T, F) = F∨(F, T) = T and F∨(F, F) = F.
6. α(A →B) = F→(α(A), α(B)),
where F→: {T, F}2 →{T, F} is defined according to the truth table of →.
7. α(A ↔B) = F↔(α(A), α(B)),
where F↔: {T, F}2 →{T, F} is defined according to the truth table of ↔.
The mapping α so defined is called the truth valuation of the propositional formulae
generated by the truth assignment s.
Using such recursive definitions, we can likewise define various other natural functions
associated with propositional formulae such as length, number of occurrences of logical
connectives, and set of occurring propositional variables. I leave these as exercises.
1.4.5.3
Homomorphisms on freely generated groups
Given a group G with a set of free generators B and any group H, every mapping h0 :
B →H can be (uniquely) extended to a homomorphism h : G →H. The definition of h
is essentially by recursion on the inductive definition of G as generated by B, and I leave
it as an exercise.
1.4.5.4
Other inductive definitions and recursion on natural numbers
Consider the following inductive definitions:
Definition 38
1. 0 is a natural number.
2. If n is a natural number then 2n + 1 is a natural number.
3. If n is a natural number and n > 0 then 2n is a natural number.
Definition 39
1. 0 is a natural number.
2. If n is a natural number then n + 2 is a natural number.
3. If n is a natural number then 2n + 1 is a natural number.

Understanding Propositional Logic
43
I leave it as exercises for the reader to show that each of these inductive definitions
defines the set of all natural numbers.
We now consider the recursive definitions:
1. h1(0) = 1;
2. h1(2n + 1) = 3n + h1(n);
3. h1(2n) = h1(n) + 1, for n > 0.
and
1. h2(0) = 0;
2. h2(n + 2) = 2h2(n) + 3;
3. h2(2n + 1) = h2(n) + 1;
They look similar, and yet there is something wrong with the second definition. What?
First, note that h2(1) = h2(2 × 0 + 1) = h2(0) + 1 = 1. Now, let us compute h2(3). On
the one hand, h2(3) = h2(1 + 2) = 2h2(1) + 3 = 5. On the other hand, h2(3) = h2(2 ·
1 + 1) = h2(1) + 1 = 2. Thus, we have obtained two different values, which is definitely
bad. The problem comes from the fact that the second definition allows for essentially
different generations of the same object, leading us to define the notion of unique gener-
ation.
1.4.5.5
Unique generation
For the correctness of recursive definitions it should be required that every element of
C(B, F) can be constructed uniquely (up to the order of the steps). Otherwise, definitions
can lead to problems as above. More formally, the elements of C(B, F) are represented
by expressions (terms) built from the elements of B by applying the operations from F.
Unique generation means that every element of C(B, F) can be represented by a unique
expression.
Example 40
1. The standard definition of natural numbers and Definition 1 above have the unique
generation property. These can be proved by induction on natural numbers.
2. Definition 2 given above does not satisfy the unique generation property.
3. The set FOR of propositional formulae satisfies the unique generation property, also
known as unique readability property.
4. If the subgroup H of a group G is freely generated by a set of generators B, then it
satisfies the unique generation property.
5. If the subgroup H of a group G is not freely generated by a set of generators B, then
it does not satisfy the unique generation property.
Theorem 41 If C(B, F) satisfies the unique generation property then for every mapping
h0 : B →X and mappings {Ff : X2n →X | f ∈F} there exists a unique mapping h :
C(B, F) →X defined by the recursive scheme in Section 1.4.5.
I do not give a proof here, but refer the reader to Enderton (2001).

44
Logic as a Tool
References for further reading
For further reading in inductive definitions, structural induction, and recursion see Tarski
(1965), Shoenfield (1967), Barwise and Echemendy (1999), Enderton (2001), Hedman
(2004), Nederpelt and Kamareddine (2004), and Makinson (2008).
Exercises
1.4.1
Given a set Z, give an inductive definition of the set of lists of elements of Z.
Formally, these are special words in the alphabet Z ∪{[ , ; , ]} of the type
[· · · []; z1]; z2]; · · · zn], where [] is the empty list and z1, z2, . . . , zn ∈Z.
Then formulate the induction principle for the set of lists of elements of Z, both
for properties of lists and for sets of lists, and use it to prove that every list of
elements of Z has equal numbers of occurrences of [and of].
1.4.2
Rephrase the definition of the subgroup [X]G of a group G generated by a set
X, given in this section, as an inductive definition.
Then formulate the induction principle for the elements of [X]G, both for prop-
erties and for sets of elements of the group.
1.4.3
Prove by structural induction that every propositional formula has equal numbers
of occurrences of (and).
1.4.4
Prove Proposition 22.
1.4.5
Prove Proposition 28.
1.4.6
Prove Proposition 31.
1.4.7
Use addition and primitive recursion to define multiplication on natural numbers.
1.4.8
Use multiplication and primitive recursion to define exponentiation on natural
numbers.
1.4.9
Complete the definition of the mappings F→and F↔in the recursive definition
of truth valuations of propositional formulae.
1.4.10
Given a group G with a set of free generators B, a group H, and a mapping h0 :
B →H, define the unique homomorphism h : G →H extending h0 by recursion
on the inductive definition of G as generated by B.
1.4.11
Show that each of the alternative inductive definitions of the set of natural num-
bers given in Section 1.4.5.4 is correct.
1.4.12
Prove by induction on natural numbers that each of the standard definitions of
natural numbers and Definition 1 in Section 1.4.5.4 satisfy the unique generation
property.
1.4.13
Prove by structural induction on FOR that the set FOR of propositional formulae
satisfies the unique generation property.

Understanding Propositional Logic
45
1.4.14
Using the inductive definition of FOR, give recursive definitions of the following
functions associated with propositional formulae:
(a) the length of a formula, being the number of symbols occurring in it;
(b) the number of occurrences of logical connectives in a formula;
(c) the set of propositional variables occurring in a formula;
(d) the set of logical connectives occurring in a formula; and
(e) the nesting depth of a formula, being the largest number of logical connec-
tives occurring in the scope of one another in the formula.
Ludwig Wittgenstein (26.04.1889–29.04.1951) was one of
the most prominent philosophers and logicians of the 20th
century. His work spanned logic, philosophy of mathemat-
ics, philosophy of mind, and philosophy of language. Dur-
ing his lifetime he published just one book in philosophy, the
extremely influential (and quite enigmatic) 75-page long Trac-
tatus Logico-Philosophicus in 1921, but he produced and wrote
much more, mostly published only in 1953 after his death, in
the book Philosophical Investigations.
Wittgenstein was born in Vienna into one of Europe’s richest
families. He completed high school in Linz, Austria and went on to study mechanical
engineering in Berlin and then aeronautics in Manchester. He gradually developed
a strong interest in logic and the foundations of mathematics, influenced by Rus-
sell’s Principia Mathematica and Frege’s Grundgesetze der Arithmetik. In 1911 he
visited Frege in Jena and wanted to study with him; they did not quite match each
other however. He went on to Cambridge to study logic with Russell, who quickly
recognized Wittgenstein as a genius. However, Wittgenstein became depressed in
Cambridge and in 1913 went to work in isolation in Skjolden, Norway, where he
conceived much of Tractatus. When World War I broke out, Wittgenstein returned
to Vienna to join the army and fight in the war for 4 years. During that period he
received numerous military decorations for his courage.
During the war, Wittgenstein completed the manuscript of his Tractatus, where
he presented his philosophical theory on how logic, language, mind, and the real
world relate. In a simplified summary, he argued that words are just representations
of objects and propositions are just words combined to make statements (pictures)
about reality, which may be true or false, while the real world is nothing but the
facts that are in it. These facts can then be reduced to states of affair (later leading
to the concept of “possible worlds”), which in turn can be reduced to combinations
of objects, thus eventually creating a precise correspondence between language and
the world. Wittgenstein believed he had solved all philosophical problems in his
Tractatus. In any case, he had essentially invented propositional formulae and truth
tables. In his later period, Wittgenstein deviated from some of his views in Tractatus.
In particular, he came to believe that the meaning of words was entirely in their use.

46
Logic as a Tool
Wittgenstein sent his Tractatus to Russell from Italy in 1918, where he was
detained as a prisoner of war, but only submitted it as his doctoral thesis when he
returned to Cambridge in 1929. During a period of depression after the war in 1919,
he gave the fortune that he had inherited from his father to his brothers and sisters.
He then he had to support himself by working at various jobs, including as maths
teacher in a remote Austrian village, a gardener at a monastery, and an architect for
his sister’s house. He remained in Cambridge to teach during 1929–1947, which was
his longest period in academia. Interrupted by World War II, he decided that it was
not morally justifiable to stay in the comfortable academic world in such troubled
times, worked anonymously as a hospital porter in London (where he advised
patients not to take the drugs they were prescribed). He resigned from Cambridge
in 1947 and retired in isolation to a remote cottage on the coast of Ireland, where he
died of cancer in 1951. Despite his worsening health he worked until the very end,
which he is said to have accepted quite willingly as he was apparently never quite
happy with his life and felt he did not really fit into this world.
To the present day, Wittgenstein remains one of the most studied and discussed,
but also probably one of the least understood, modern thinkers.

2
Deductive Reasoning in
Propositional Logic
This chapter presents the deductive side of propositional logic: deductive systems and for-
mal derivations of logical consequences in them. First, I explain the concept and purpose
of a deductive system and provide some historical background. I then introduce, discuss,
and illustrate with examples the most popular types of deductive systems for classical
logic: Axiomatic Systems, Semantic Tableaux, Natural Deduction, and Resolution. In a
supplementary section at the end of the chapter I sketch generic proofs of soundness
and completeness of these deductive systems. In another supplementary section I dis-
cuss briefly the computational complexity of the Boolean satisfiability problem and the
concept of NP-completeness.
The deductive systems introduced in this chapter are specifically designed for clas-
sical propositional logic, but the concept is universal and applies to almost all logical
systems that have been introduced and studied. In Chapter 4 I extend each of these deduc-
tive systems with additional axioms and rules for the quantifiers, so that they also work
for first-order logic.
2.1
Deductive systems: an overview
2.1.1
The concept and purpose of deductive systems
The fundamental concept in logic is that of logical consequence. It extends the concept
of logical validity and is the basis of logically correct reasoning. Verifying logical con-
sequence in propositional logic is conceptually simple and technically easy (although
possibly computationally expensive), but this is no longer the case for full first-order
logic, which I introduce in Chapter 3, or for the variety of non-classical logics which I
do not discuss here. In fact, verifying logical consequence (in particular, logical validity)
in first-order logic is usually an infinite task, as it generally requires checking infinitely
many possible models rather than a finite number of simple truth assignments. A dif-
ferent approach for proving logical validity and consequence, which is not based on the
semantic definition, is therefore necessary. Such a different approach is provided by the
notion of a deductive system, a formal, mechanical – or, at least mechanizable – procedure
Logic as a Tool: A Guide to Formal Logical Reasoning, First Edition. Valentin Goranko.
© 2016 John Wiley & Sons, Ltd. Published 2016 by John Wiley & Sons, Ltd.

48
Logic as a Tool
for derivation (inference, deduction) of formulae or lists (“sequents”) of formulae, by
applying precise inference rules and possibly using some formulae called axioms that are
postulated as derived. Deductive systems therefore substitute the semantic notion of logi-
cal consequence with the formal, syntactic notion of deductive consequence, based only on
the syntactic shape of the formulae to which inference rules are applied, but not explicitly
on their meaning. This means that, in theory, someone without knowledge and under-
standing of the meaning of logical formulae and logical consequence, or even a suitably
programmed computer, should be able to perform derivations in a given deductive system.
The purpose and fundamental importance of a deductive system is not only to serve
as a mechanical procedure for deriving logical consequences, but also to provide simple
and intuitively acceptable principles and rules of logical inference which can be used to
demonstrate or verify logically correct reasoning. The underlying idea is that if we start
with premises that are known or accepted as true, and apply only rules of inference that are
known to preserve truth, then we are guaranteed that every reasoning performed within
such a deductive system is logically correct and that every conclusion reached by a chain
of inferences within that system must also be true.
There are different types of deductive systems based on different ideas, but they all share
several common features and principles. To begin with, a deductive system works within a
formal logical language where derivations are performed on formal expressions – usually
formulae or “sequents” (finite sequences of formulae) – which are special “words”, that
is, finite strings of symbols in the formal language, having precise syntax1. In this chapter
I discuss deductive systems acting on propositional formulae. In Chapter 4 I extend these
to full first-order logic.
The main component of a deductive system is the notion of derivation (inference,
deduction) from a given set of assumptions, based on a set of precisely specified rules
of inference. The idea is that by systematically applying these rules, we can derive (infer,
deduce) formulae from other formulae that are already derived, or assumed as given,
called assumptions (premises). In addition, axiomatic systems also allow for an initial
set of formulae, called axioms, to be accepted as derived without applying any rules of
inference. The axioms can therefore always be used as premises in derivations.
If a formula A can be derived from a set of assumptions Γ in a deductive system D, we
denote this
Γ ⊢D A
and say that A is a deductive consequence from Γ in D.
In particular, if Γ = ∅, we write ⊢D A and say that A is a theorem of D.
2.1.2
Brief historical remarks on deductive systems
The idea of a proof or derivation as demonstrative formal argumentation and of the concept
of logical deductive systems go back to antiquity, and most notably to the book Organon of
the ancient Greek philosopher Aristotle (384–322 BC), founding father of (among many
scientific disciplines) formal logic. The first formal logical system, called Syllogistic (see
Section 3.5), was developed by Aristotle, but perhaps the first real prototype of a deductive
system can be found in the fundamental work Elements of the “Father of Geometry”,
the great Ancient Greek mathematician Euclid (c. 300 BC), who provided a systematic
1 In this aspect a deductive system is much like a programming language.

Deductive Reasoning in Propositional Logic
49
development of elementary geometry based on several simple assumptions about points
and lines (such as “every two different points determine exactly one line”, “for every line
there is a point not belonging to that line”, etc.). Using these postulates and some informal
logical reasoning, other geometric facts are derived and thus the entire body of Euclidean
geometry is eventually built.
The logical concept of the deductive system gradually emerged much later, notably
through the ideas of Gottfried Leibniz (1646–1716), one of the greatest philosophers and
mathematicians of all times, of a characteristica universalis (universal language) and a
calculus ratiocinator (calculus of reasoning). The first formal deductive system for mod-
ern logic, however, was only constructed in the late 19th century by the mathematician and
philosopher Gottlob Frege (1848–1925) in his seminal book Begriffsschrift (1879), where
he developed the prototype of classical first-order logic. Later, David Hilbert (1862–1943),
one of the leading mathematicians of the late 19th and early 20th century, reworked
Euclid’s system of geometry developed in the Elements into a rigorous and mathemat-
ically precise treatment that resulted in his book Foundations of Geometry (1899). The
mathematician Giuseppe Peano (1858–1932) developed a formal system of arithmetic,
still known as Peano’s axiomatic system, in his most important work Formulario Math-
ematico. The concept of formal deductive system was further developed by the philoso-
phers Bertrand Russell (1872–1970) and Alfred North Whitehead (1861–1947) in their
three-volume book Principia Mathematica (1910–1913), and by David Hilbert and Wil-
helm Ackermann (1896–1962) in their book Principles of Mathematical Logic (1928).
These books were most influential for the development of logic and foundations of math-
ematics in the first half of the 20th century. In particular, Hilbert strongly promoted the
idea of building the whole body of mathematics as a formal axiomatic system, a deductive
system mainly based on axioms and on very few and simple rules of inference. Hilbert
was the leading proponent of the development of the axiomatic approach in mathematics,
meant to replace the semi-formal notion of mathematical proof by the completely formal-
ized notion of derivation in an axiomatic system. A major purpose of the formal axiomatic
approach was to avoid the occurrence of any paradoxes and contradictions in mathematics
by performing all mathematical proofs within such a formal system of deduction which
has been proved to be consistent, that is, free from contradictions. Such a system, based
on first-order logic, was proposed by Hilbert and Ackermann in Principles of Mathemat-
ical Logic, and Hilbert’s ultimate goal was to prove that that system was both consistent
and complete, that is, capable of deriving every mathematical truth, but not deriving any
contradictions. Hilbert’s idea of a deductive system was further developed by several logi-
cians, including Jacques Herbrand, Emil Post, Alfred Tarski, and others.
In the late 1920s–early 1930s the concept of a deductive system was extended in the
works of Gerhard Gentzen and Stanisław Ja´skowski by adding the possibility of introduc-
ing and withdrawing assumptions in derivations, that is, formulae that are not axioms and
have not been derived. Thus, the more intuitive and practically more efficient rule-based
type of deductive system, called Natural Deduction and the closely related Sequent Calcu-
lus, were developed, essentially founding the major field of logic known as proof theory. In
the 1950s–1960s, the the refutation-based system of Semantic Tableaux emerged, imple-
menting the idea of a systematic and exhaustive search for a falsifying model of the
assumptions plus the negation of the desired conclusion, where a derivation consists of
an established failure to construct such a falsifying model. The idea of refutation-based
deductive systems led to the development in the 1960s–1970s of the method of resolution,
which turned out to be very suitable for computer-aided automated deduction.

50
Logic as a Tool
Meanwhile, in the early 1930s some groundbreaking results in logic were announced
by a young logician having just completed his doctoral studies, Kurt Gödel. Those results,
that made Gödel the most famous logician of the 20th century, showed that Hilbert’s
idea was only partly realizable, in a sense that purely logical validity and consequence
in first-order logic can be axiomatized completely by Gödel’s completeness theorem for
first-order logic; it was not, however, realizable for the richer notion of mathematical
consequence, even in the relatively simple mathematical systems of arithmetic of nat-
ural numbers with addition and multiplication. More precisely, in 1931 he proved his
two celebrated Gödel’s incompleteness theorems. The first of these stated that no suffi-
ciently expressive and reasonably axiomatized (with an effectively recognizable notion
of axioms) deductive system, such as Peano’s system of arithmetic, can be complete. The
second incompleteness theorem claimed that such a theory cannot even prove its own con-
sistency – suitably encoded as a formula in that system – unless it is inconsistent (in which
case it can derive any formula by using the sound logical rule Ex Falso Quodlibet). No
absolute and finitary proof of consistency of such system is therefore possible. Hilbert’s
dream of formalizing the whole of mathematics in a provably consistent way turned out
to be unattainable. Still, deductive systems and their proof theory have remained one of
the main directions for development of modern logic.
2.1.3
Soundness, completeness and adequacy of deductive systems
A very important aspect of deductive systems is that derivations in them are completely
mechanizable procedures that in principle do not require any intelligence or understand-
ing of the meaning of the formulae or rules involved; in fact, such a meaning need not
be specified at all. Derivations in a given deductive system can therefore be performed
by a mechanical device such as a computer without any human intervention, as long
as the axioms and rules of inference of the deductive system have been programmed
into it.
While deductive systems are not explicitly concerned with the meaning (semantics) of
the formulae they derive, they are designed with the purpose of deriving only valid logi-
cal consequences from the assumptions. A deductive system with this property is called
sound. In particular, every theorem of a sound deductive system must be a valid formula,
that is, in the case of propositional logic it must be a tautology.
Formally, a deductive system D is sound (or correct) for a given logical semantics (that
is, well-defined notions of logical validity and consequence) if D can only derive logically
valid consequences, that is:
A1, . . . , An ⊢D C implies A1, . . . , An |= C.
In particular: ⊢D C implies |= C.
A deductive system D is complete for a given logical semantics if D can derive every
valid logical consequence (as defined in that semantics), that is:
A1, . . . , An |= C implies A1, . . . , An ⊢D C.
In particular, |= C implies ⊢D C, that is, a complete deductive system can derive every
logically valid formula.

Deductive Reasoning in Propositional Logic
51
A deductive system D is adequate for a given semantics if it is both sound and complete
for it, i.e.:
A1, . . . , An |= C
if and only if
A1, . . . , An ⊢D C
In particular: |= C
if and only if
⊢D C.
The soundness of a deductive system can be guaranteed and proved easily in principle
as long as the following two conditions hold:
(i) All axioms (if any) must be true.
(ii) All rules of inference must be sound, that is, they must always produce true con-
clusions when applied to true assumptions. The truth therefore propagates from the
axioms to all theorems.
I present here some of the most popular types of deductive systems for classical logic,
namely Axiomatic Systems, Semantic Tableaux, Natural Deduction, and Resolution.2 Each
of these bears a different idea and has advantages and shortcomings compared to the oth-
ers. The use of each of these is illustrated with several examples and many more exercises,
and in Chapter 5 they will be put to real work for performing concrete mathematical rea-
soning.
References for further reading
For further general discussion and historical notes on deductive systems, see the classical
masterpiece Tarski (1965) and well as Nerode and Shore (1993), Jeffrey (1994), Fitting
(1996), Barwise and Echemendy (1999), Hodges (2001), and Ben-Ari (2007).
Aristotle (384–322 BC) was a great Ancient Greek philoso-
pher, “the first genuine scientist in history” according to Ency-
clopaedia Britannica.
He was born in the Macedonian city of Stagira in the
Chalkidiki peninsula, and joined Plato’s Academy in Athens
at the age of 18 where he carried out scholarly work for
around 20 years. After the death of Plato he left Athens and, at
the request of King Philip of Macedonia, became the private
teacher of his son Alexander the Great during 356–323 BC.
It is believed that only about one-third of Aristotle’s original
works have survived. In these he made important contributions to just about every
field of science and arts that existed in his time or was founded by him: philosophy,
poetry, theatre, music, rhetoric, politics, government and ethics, physics, geology
and biology. Aristotle’s legacy, not only in philosophy, continues to be the object
of active academic study today. For instance, the idea of systematic classification of
plants and animals, developed in the 18th century by Linnaeus, originate from him.
2 I do not present Sequent Calculus in this book, but it is easily reducible to Natural Deduction. See Ebbinghaus et al.
(1996) and Boolos et al. (2007).

52
Logic as a Tool
Among his many other seminal contributions, Aristotle is regarded as the found-
ing father of logic as a scientific discipline. He was the first to propose the use of
formal logical languages in the study of reasoning and to undertake and accomplish
a systematic study of the principles of correct reasoning, unsurpassed for more than
2000 years. His treatises were collected by his followers under the name Organon
(from the Greek word meaning “instrument” or “tool”), consisting of six books:
Categories, On Interpretation, Prior Analytics, Posterior Analytics, Topics and On
Sophistical Refutations. In Organon Aristotle developed a theory of deduction for a
special kind of logical arguments called syllogisms (see Section 3.5) built on several
forms of expressions of the type “all As are (not) Bs”, “some As are (not) Bs”, or “no
As are Bs”. A typical example of a syllogism is “All humans are mortal. All Greeks
are humans. Therefore, all Greeks are mortal.” In his logical system, now called the
Syllogistic, Aristotle introduced the so-called square of opposition, identified and
classified all syllogistic forms into logically valid and invalid forms, proposed a sys-
tem for deriving some valid syllogistic forms from others, and provided convincing
counter-examples to the invalid forms. The main rival of Aristotelian logic, which in
modern terms is a proper fragment of first-order logic, was the logic of propositions
of the Stoic school.
Aristotle also studied different modes of truth – possible, necessary, and
contingent – laying the foundations of modal logic. He also proved many
“meta-properties” of the correct logical reasoning, anticipating the rigorous,
mathematical study of modern logic.
2.2
Axiomatic systems for propositional logic
2.2.1
Description
Axiomatic systems are deductive systems that are mainly based on axioms and use very
few and simple rules of inference. Axiomatic systems are the oldest and simplest to
describe (but not to use!) of all deductive systems. The idea of basing the mathematical
reasoning on formal axiomatic systems was strongly promoted by David Hilbert in the
early 20th century. In his honor, axiomatic systems are also commonly called Hilbert
(-style) systems.
Here we build a hierarchy of several axiomatic systems for propositional logic by grad-
ually adding new logical connectives and axioms for these. Before that, we need the
notion of formula scheme which is used in these axiomatic systems. This is the set of all
instances of propositional formulae obtained from a given formula by applying uniform
substitution as follows.
Let F = F(p1, . . . , pn) be a formula built over the propositional variables p1, . . . , pn
and let B1, . . . , Bn be any formulae. Then F ′ = F [B1/p1, . . . , Bn/pn] is the formula
obtained from F by replacing simultaneously every occurrence of pi by Bi for each
i = 1, . . . , n. We say that F ′ is a substitution instance (or just an instance) of F. For
example,
((p ∧q) ∧¬¬p) →(¬(p ∧q) ∨¬p)

Deductive Reasoning in Propositional Logic
53
is the substitution instance of
(p ∧¬q) →(¬p ∨q)
where p is substituted by (p ∧q) and q is substituted by ¬p. The scheme of all substitution
instances of the formula F(p1, . . . , pn) can be written simply as F [B1/p1, . . . , Bn/pn],
where B1, . . . , Bn are regarded not as specific formulae but as metavariables (i.e., vari-
ables in our metalanguage) for formulae. The formulae of the scheme are the instances
obtained by substituting formulae for these metavariables. The scheme generated from
the formula (p ∧¬q) →(¬p ∨q) can therefore be written (B1 ∧¬B2) →(¬B1 ∨B2).
We are now ready to start introducing axiomatic systems for propositional logic. First,
we assume that the only logical connectives are →and ¬, and all others are definable in
terms of these. The axiomatic system for that language comprises the following axioms
and rules.
Axiom schemes
(→1) A →(B →A);
(→2) (A →(B →C)) →((A →B) →(A →C));
(→3) (¬B →¬A) →((¬B →A) →B).
The only rule of inference is Modus ponens:
A, A →B
B
which now reads: “If the formulae A and A →B are derived, then the formula B is also
derived.”
We denote this axiomatic system H(→, ¬).
If we also consider the conjunction as a primitive, rather than definable connective, then
it turns out that the following axiom schemes completely capture its deductive properties.
(∧1) (A ∧B) →A;
(∧2) (A ∧B) →B;
(∧3) (A →B) →((A →C) →(A →B ∧C)).
Added to H(→, ¬), these schemes produce the system H(→, ¬, ∧).
Likewise, the following axiom schemes suffice for the disjunction:
(∨1) A →A ∨B;
(∨2) B →A ∨B;
(∨3) (A →C) →((B →C) →(A ∨B →C)).
These schemes, added to H(→, ¬) produce the system H(→, ¬, ∨).
Finally, the axiomatic system H(→, ¬, ∧, ∨) combining H(→, ¬, ∧) and H(→, ¬, ∨)
is simply denoted H.

54
Logic as a Tool
2.2.2
Derivations in the axiomatic system H
I only consider the full system H here, but all definitions apply to all subsystems defined
above.
A derivation in H from a set of assumptions Γ is a finite sequence of formulae
C1, . . . , Cn such that for every i ∈{1, . . . , n}, either Ci ∈Γ or Ci is an axiom of H, or
Ci is obtained by applying the rule Modus ponens to previously derived formulae, that is,
there are j, k < i such that Ck is Cj →Ci. We write Γ ⊢H Cn and say that Cn is derived
in H from the set of assumptions Γ.
Note that every initial subsequence of a derivation is a derivation, so every formula
occurring in a derivation is derived and not just the last one.
We often list the assumptions explicitly and write A1, . . . , Ak ⊢H C.
A formula A is said to be derivable in H or a theorem of H, denoted ⊢H A, if ∅⊢H A.
The notion of derivation from a set of assumptions is meant to be the deductive analog
in H of a logical consequence. In order to prove that “A1, . . . , An logically imply B”,
we add the assumptions A1, . . . , An to the set of axioms of H and try to derive B from
these. Here is an example of a derivation in H. We list the formulae in the derivation and
provide a brief justification of how they have been derived.
Example 42 ⊢H (p ∧(p →q)) →q:
1. (p ∧(p →q)) →p,
instance of Axiom (∧1);
2. (p ∧(p →q)) →(p →q),
instance of Axiom (∧2);
3. ((p ∧(p →q)) →(p →q)) →(((p ∧(p →q)) →p) →((p ∧(p →q)) →q)),
instance of Axiom (→2);
4. ((p ∧(p →q)) →p) →((p ∧(p →q)) →q),
by 2,3 and Modus ponens;
5. (p ∧(p →q)) →q,
by 1,4 and Modus ponens.
This example looks overly complicated for the simple tautology that it derives, but it
is illustrative of how complex derivations in axiomatic systems can be. Still, we prove in
Section 2.7 that H and each of its extensions above can derive every valid logical conse-
quence. More precisely, the following holds.
Theorem 43 (Adequacy theorem for H) The axiomatic system H is sound and com-
plete, that is:
A1, . . . , An ⊢H B
if and only if
A1, . . . , An |= B.
In particular, ⊢H B if and only if B is a tautology.
The same holds for each of the subsystems H(→, ¬), H(→, ¬, ∧), and H(→, ¬, ∨).
Despite being adequate, the axiomatic system H is not very suitable for practical deriva-
tions, as it can be judged from the example above. The derivations in it are not very well
structured and often difficult to construct, because even simple cases may require involved
derivations. For instance, as a challenge, try deriving the tautology p →p.
Still, the derivations in H can be simplified significantly by using the following impor-
tant result which allows us to introduce, and later eliminate, auxiliary assumptions in the
derivations.

Deductive Reasoning in Propositional Logic
55
Theorem 44 (Deduction theorem) For any formulae A, C, and set of formulae Γ:
Γ ∪{A} ⊢H C iff Γ ⊢H A →C.
One direction of this theorem is very easy. If Γ ⊢H A →C, then Γ ∪{A} ⊢H A →C.
We also have that Γ ∪{A} ⊢H A. Now, by application of the rule Modus ponens, we
obtain that Γ ∪{A} ⊢H C.
The proof of the other direction (from right to left) requires a more involved argument
by induction on derivations, associated with the inductive definition of derivations in H
as in Section 1.4. It goes as follows.
Lemma 45 (Principle of induction on derivations in H) Let P be a property of propo-
sitional formulae and Γ be a set of formulae, such that:
1. Every axiom of H has the property P.
2. Every formula in Γ has the property P.
3. Whenever the formulae Q, R are such that both Q and Q →R have the property P,
then R also has the property P.
Then every formula C such that Γ ⊢H C has the property P.
The proof is left as an exercise. It is an application of the general observation about
inductive definitions and proofs by induction, made in Section 1.4.
The direction from right to left of the Deduction Theorem can now be proved as follows.
Let the property P state for a formula C that “For every set of formulae Γ and a
formula A, if Γ ∪{A} ⊢H C then Γ ⊢H A →C.”
We leave the proof as an exercise of application of the principle of induction on deriva-
tions in H. Note that you will need to use the axioms for the implication.
Using the Deduction Theorem, derivations such as ⊢H p →p become straightforward.
Example 46 The derivation in Example 46 can also be substantially simplified.
1. p ∧(p →q) ⊢H p,
by Axiom (∧1) and the Deduction Theorem;
2. p ∧(p →q) ⊢H p →q,
by Axiom (∧2) and the Deduction Theorem;
3. p ∧(p →q) ⊢H q,
by 1,2, and Modus ponens;
4. ⊢H (p ∧(p →q)) →q,
by 3 and the Deduction Theorem.
Here is another example of a derivation in H using the Deduction Theorem.
Example 47 p, ¬p ⊢H q:
1. ¬p ⊢H ¬q →¬p,
by Axiom (→1) and the Deduction Theorem;
2. ¬p ⊢H (¬q →p) →q,
by 1, Axiom (→3), and Modus ponens;
3. p ⊢H ¬q →p,
by Axiom (→1) and the Deduction Theorem;
4. p, ¬p ⊢H q,
by 2,3, and Modus ponens.
Note that adding more premises does not affect the validity of derivations, but can
possibly add more derivable formulae. This justifies step 4 above.

56
Logic as a Tool
References for further reading
For further discussion and examples on derivations in axiomatic systems for propositional
logic, see Tarski (1965), Shoenfield (1967), Hamilton (1988), Fitting (1996), and Mendel-
son (1997).
Exercises
2.2.1
Show that the result of uniform substitution of a propositional formula for a
propositional variable in a propositional formula is again a propositional formula.
(Hint: use induction on the formula in which the substitution is performed.)
2.2.2
Derive the following in H without using the Deduction Theorem.
(NB: some of these are simple, others are quite tricky. The purpose of this exercise
is to make you appreciate the Deduction Theorem.)
(a) p ⊢H p ∨q; q ⊢H p ∨q
(b) p, q ⊢H p ∧q
(c) ⊢H (p ∧q) →(q ∧p)
(d) ⊢H (p ∨q) →(q ∨p)
(e) ⊢H p →p
(f) ⊢H (p →q) →((q →r)
→(p →r))
(g) ⊢H (¬p →¬q) →(q →p)
(h) ⊢H (q →p) →(¬p →¬q)
2.2.3
Prove the principle of induction on derivations in H by using induction on lengths
of derivations.
2.2.4
Complete the proof of the Deduction Theorem by using the principle of induction
on derivations in H.
2.2.5
Prove that if Γ, A ⊢H B and Γ, B ⊢H C then Γ, A ⊢H C.
(Hint: use the Deduction Theorem.)
2.2.6
If A ⊢H B then B →C ⊢H A →C.
2.2.7
Prove, by induction on derivations in H, that every theorem of H is a tautology.
2.2.8
More generally, using induction on derivations in H, prove that H is sound, that
is, for every set of formulae Γ and a formula A, if Γ |= A then Γ ⊢H A.
2.2.9
Derive the following in the axiomatic system H using the Deduction Theorem,
where P, Q, R are any formulae.
(Hint: for some of these exercises you may use the previous exercises.)
(a) P ⊢H P ∨Q; Q ⊢H P ∨Q
(b) P, Q ⊢H P ∧Q
(c) ⊢H (P ∧Q) →(Q ∧P)
(d) ⊢H (P ∨Q) →(Q ∨P)
(e) P →(Q →R) ⊢H Q →(P →R)
(f) (P ∧Q) →R ⊢H P →(Q →R)
(g) P →(Q →R) ⊢H (P ∧Q) →R
(h) If ¬P ⊢H ¬Q then Q ⊢H P
(i) (¬P →¬Q) ⊢H (Q →P)
(j) If ¬P, Q ⊢H ¬Q then Q ⊢H P
(k) If Q ⊢H P then ¬P ⊢H ¬Q
(l) (Q →P) ⊢H (¬P →¬Q)
(m) ¬¬P ⊢H P
(n) P ⊢H ¬¬P
(o) ⊢H P ∨¬P
(p) ⊢H ¬(P ∧¬P)

Deductive Reasoning in Propositional Logic
57
(q) ¬P ∨Q ⊢H P →Q
(r) P →Q ⊢H ¬P ∨Q
(s) ¬(P ∨Q) ⊢H ¬P ∧¬Q
(t) ¬P ∧¬Q ⊢H ¬(P ∨Q)
(u) ¬(P ∧Q) ⊢H ¬P ∨¬Q
(v) ¬P ∨¬Q ⊢H ¬(P ∧Q)
(w) ⊢H (Q →P) →((Q →¬P) →¬Q)
(x) ⊢H (¬Q →P) →((¬Q →¬P) →Q)
(y) If P ⊢H Q and P ⊢H ¬Q then ⊢H ¬P.
(z) If P, Q ⊢H R and P, ¬Q ⊢H R then P ⊢H R.
2.2.10
Using the Deduction Theorem, show that if any of ∧, ∨, or →is considered
definable in terms of the others, the corresponding axioms in H can be derived
from the others.
Euclid of Alexandria (c. 325–265 BC) was the most promi-
nent and influential mathematician of ancient Greece. He wrote a
monumental 13-book work, known as the Elements, in which he
laid the systematic foundations of both geometry and arithmetic.
Elements encompassed the system of postulates and theorems for
what is now called Euclidean geometry and is probably the most
influential book written in the history of mathematics. It was still
used as the classic textbook for the study of geometry until the end of the 19th cen-
tury, when David Hilbert revised and modernized it.
Furthermore, the book is written in a distinctly axiomatic style, so Euclid’s sys-
tem of geometry can be regarded as the first axiomatic system of a mathematical
theory. The first book of Elements contains five postulates for points and lines in the
plane. The fifth postulate is the famous Parallel Postulate, stating that for every line
and for every point that does not lie on that line there exists a unique line through
the point that is parallel to the given line. Since it was not as simple to state or as
obvious as the other postulates, Euclid himself and many mathematicians after him
tried for two millennia to derive it from the other four postulates. However, all these
attempts were futile. It was only in the early 19th century that the mathematicians
Gauss, Lobachevski, and Bolyai showed independently that negations of this postu-
late lead to the development of consistent, alternative Non-Euclidean geometries,
such as elliptic and hyperbolic geometries. The axiomatic approach of Euclid there-
fore turned out to be much more than just a matter of style and illustrated the great
importance of the axiomatic method in mathematics, taken up later by Dedekind,
Peano, Frege, Russell, Whitehead, Hilbert, Bernays, and their followers.
Besides geometry, Euclid also made fundamental contributions to number theory
in Elements, including the study of prime numbers, the celebrated proof of existence
of infinitely many prime numbers, Euclid’s lemma on factorization, and, of course,
the Euclidean procedure for computing the greatest common divisor of two integers.
That procedure was one of the earliest and most famous instances of what is today
called an algorithm.
See on the following page a copy of the first few definitions and postulates
from an 1838 edition of Euclid’s Elements, published by Robert Simson in
Philadelphia, USA.

58
Logic as a Tool
2.3
Semantic Tableaux
Here I develop the idea of testing the validity of propositional formulae and inferences
by systematically searching for a falsifying truth assignment into a formal deductive sys-
tem called Semantic Tableaux (hereafter denoted ST). The first versions of Semantic
Tableaux were designed in the 1950s independently by Evert Beth and Jaakko Hintikka,
and further developed in the 1960s and later by Raymond Smullyan, Melvin Fitting,

Deductive Reasoning in Propositional Logic
59
and others. Since then, systems of Semantic Tableaux have also been developed for a
great variety of non-classical logical systems. Read more on Beth and Hintikka in the
biographic boxes at the end of this section, and on Smullyan and Fitting at the end of
Section 4.2.
In order to prove the validity of the consequence A1, . . . , An |= B, the method of
Semantic Tableaux involves demonstrating that there is no truth assignment which falsifies
that consequence, which is equivalent to showing that the formulae A1, . . . , An and ¬B
cannot be satisfied simultaneously.
2.3.1
Description of the deductive system ST of Semantic Tableaux
The deductive system of ST is based on formula decomposition rules which reduce the
truth or falsity of a formula to the truth or falsity of its main subformulae. These rules can
be extracted from the truth tables of the propositional connectives as follows.
1. ¬ :
(a) For ¬A to be true, A must be false.
(b) For ¬A to be false, A must be true.
2. ∧:
(a) For A ∧B to be true, A must be true and B must be true.
(b) For A ∧B to be false, A must be false or B must be false.
3. ∨:
(a) For A ∨B to be true, A must be true or B must be true.
(b) For A ∨B to be false, A must be false and B must be false.
4. →:
(a) For A →B to be true, A must be false or B must be true.
(b) For A →B to be false, A must be true and B must be false.
5. ↔:
(↔T) For A ↔B to be true, A and B must be true or A and B must be false.
(↔F) For A ↔B to be false, A must be true and B must be false or A must be false
and B must be true.
These rules are formalized in Table 2.1, presented as rules for the signed version of the
system of Semantic Tableaux. The signed version works with formulae that are labeled
with the truth values they are required to obtain.
Using these rules, one can search systematically for an assignment falsifying a given
formula or a set of formulae. In the process of this systematic application of the rules we
build a “search tree” called tableau. If we are to derive the consequence A1, . . . , An |=
B, that is, to prove its validity, we start by placing the signed formulae
A1 : T, . . . , An : T, B : F
at the root of the tableau and then extend it downward by repeatedly applying the decom-
position rules to signed formulae appearing on the tree as follows.
1. A branch on the tree is selected.
2. A signed formula occurring on that branch, to which a decomposition rule has not yet
been applied, is selected.

60
Logic as a Tool
Table 2.1
Rules for Semantic Tableaux in propositional logic: signed version
Non-branching rules (α-rules)
Branching rules (β-rules)
(∧T)
A ∧B : T
A : T, B : T
(∧F)
A ∧B : F
A : F
B : F
(∨F)
A ∨B : F
A : F, B : F
(∨T)
A ∨B : T
A : T
B : T
(→F)
A →B : F
A : T, B : F
(→T)
A →B : T
A : F
B : T
(¬T)
¬A : T
A : F
(↔T)
A ↔B : T
A : T, B : T
A : F, B : F
(¬F)
¬A : F
A : T
(↔F)
A ↔B : F
A : T, B : F
A : F, B : T
3. The decomposition rule that corresponds to that signed formula is applied to it. As a
result, one (for the α-rules) or two (for the β-rules) successor nodes are added, labeled
with the signed formulae which that rule introduces.
We repeat this procedure along every branch of the tableau until:
• either a contradictory pair of signed formulae of the type A : T, A : F appears on the
branch, in which case we declare that branch closed (meaning that no assignment can
ever be found which satisfies all signed formulae on that branch), mark it × and do not
extend it any further;
• or no new applications of decomposition rules are possible on that branch (i.e., every
signed formula appearing on the branch has already been decomposed further on that
branch); we then say that the branch is saturated and, if a saturated branch is not yet
closed, we declare it open and mark it ⃝.
Note that an open branch defines a truth assignment for the variables occurring on that
branch as follows: a variable p is true under that assignment if p : T appears on the branch,
and is false otherwise (in which case, p : F may or may not appear on the branch). It is
easy to check that such an assignment will satisfy all signed formulae on the branch. That
can be done step-by-step by going up the branch. All signed formulae at the root will
eventually be satisfied, which will falsify the logical consequence A1, . . . , An |= B and
therefore prove it invalid.

Deductive Reasoning in Propositional Logic
61
On the other hand, if all branches of the tableau close, then the entire tableau is declared
closed. That means that the systematic search for an assignment satisfying the formulae
at the root has failed. We then say that B is derived from A1, . . . , An in ST, which
we denote by A1, . . . , An ⊢ST B. In particular, if ∅⊢ST B then we say that B is a
theorem of ST.
Theorem 48 (Adequacy Theorem for ST) The system of Semantic Tableaux is sound
and complete, that is:
A1, . . . , An ⊢ST B
if and only if
A1, . . . , An |= B.
In particular, ⊢ST B if and only if B is a tautology.
I only sketch the proof idea here. The soundness of ST means that if the input formula
is satisfiable, then the tableau remains open. This can be proved by fixing a satisfying
truth assignment at the beginning and tracing the branch that corresponds to it. For the
completeness, it is sufficient to note that if the tableau remains open then a satisfying
truth assignment can be extracted from any saturated open branch. For a more detailed,
generic proof of soundness and completeness see Section 2.7.
2.3.2
Some derivations in ST
Let us demonstrate the method of Semantic Tableaux on some of the examples included
informally in Section 1.1.
1. Show that ¬(p →¬q) →(p ∨¬r) is a tautology:
¬ (p→¬q) →(p∨¬r) : F
¬ (p→¬q) : T,(p∨¬r) : F1
p→¬q : F
p : T,¬ q : F
q : T
p : F,¬ r : F
×
Thus, ⊢ST ¬(p →¬q) →(p ∨¬r) and hence |= ¬(p →¬q) →(p ∨¬r).
Note that no branching rules were applicable in this example; the tableau consists
of only one branch which closes because the complementary pair p : T, p : F appears

62
Logic as a Tool
on it. The superscript 1 indicates where the decomposition rule is applied to the formula
(p ∨¬r) : F. This is not needed for the derivation, but has only been included for
completeness and to make the tableau easier to read, and will not be used further.
2. Check if ¬p |= ¬(p ∨¬q).
¬p : T,¬ (p∨¬q) : F
p : F
p∨¬q : T
p : T
¬ q : T
q : F
⃝
The left-hand branch closes but the right-hand branch does not, and no more decom-
position rules are applicable on it which means it is open. The whole tableau is therefore
open, which means that ¬p ⊭¬(p ∨¬q). Indeed, the right-hand branch provides a fal-
sifying valuation for that consequence: p : F, q : F. (Check it!)
3. Check whether (¬p →q), ¬r |= p ∨¬q.
(¬p→q) : T,¬ r : T, p ∨¬q : F
r : F
p : F,¬ q : F
q : T
¬p : F
q : T
⃝
p : T
×
The tableau is again open, and a falsifying valuation is provided by the open branch
p : F, q : T, r : F. Note that we choose to first decompose the subformulae ¬r : T and
(p ∨¬q) : F because they are non-branching (α-formulae) and then ¬p →q : T which
is branching (β-formula). This does not make a difference to the outcome of the tableau
but saves some work, so it is generally the preferable strategy.

Deductive Reasoning in Propositional Logic
63
4. Check whether ((p ∧¬q) →¬r) ↔((p ∧r) →q) is a tautology.
((p∧¬q)→¬r) ↔((p∧r) →q) : F
(p∧¬q)→¬r : T,(p∧r) →q : F
(p∧r) →q : T,(p∧¬q)→¬r : F
p∧r : T, q : F
p : T, r : T
p∧¬q : F
¬ r : T
r : F
×
p : F
×
¬ q : F
q : T
×
p∧¬q : T,¬ r : F
p : T,¬ q : T
q : F
r : T
r : T
q : T
×
p : F
r : F
×
×
The tableau closes, and so the formula is a tautology.
I end this section with a few remarks.
1. At any stage during the construction of a Semantic Tableau, more than one decomposi-
tion rule may be applicable. Depending on the order of their application the tableau tree
may develop differently, but the final result – whether the tableau closes or not – will
always be the same. The order of rule application is therefore unimportant. However, it
may affect the size of the tableau tree. As a general guideline, it is better to first apply
the non-branching rules before applying the branching rules (if there is a choice), as
this would defer the branching and reduce the size of the tableau.
2. Note that no node in a tableau can have more than two children nodes. If a node in your
tableau has three or more children, then there is something wrong!
3. Although we can use equivalences such as p ∧(q ∨p) ≡p and ¬(p ∧q) ≡¬p ∧¬q
when reasoning informally, these equivalences do not form part of the rules of the
deductive system ST and should not be used in the construction of the tableaux graphs.
As emphasized in Section 2.1, derivations within a deductive system may only use the
rules of that system in order to be completely mechanizable.

64
Logic as a Tool
2.3.3
Unsigned version of the system of Semantic Tableaux
Unlike the signed version presented above, in the unsigned version of Semantic Tableaux
all formulae appearing on the branches are assumed to be true; there is therefore no need
to assign truth values. For that purpose, every signed formula A : T is replaced simply by
A, whereas A : F is transformed to ¬A.
The rules for the unsigned version can be produced by a simple modification of the rules
for the signed version, by using the basic equivalences used for importing the negation
inside the other propositional connectives. They are listed in Table 2.1.
Non-branching rules (α-rules)
Branching rules (β-rules)
(∧)
A ∧B
A, B
(¬∧)
¬(A ∧B )
¬A
¬B
(¬∨)
¬(A ∨B )
¬A, ¬B
(∨)
A ∨B
A
B
(¬ →)
¬(A →B )
A, ¬B
(→)
A →B
¬A
B
(¬¬)
¬¬ A
A
(↔)
A ↔B
A, B
¬A, ¬B
(¬ ↔)
¬(A ↔B )
A, ¬B
¬A, B
Figure 2.1
Rules for Semantic Tableaux in propositional logic: unsigned version
A branch of the unsigned tableau is closed if a “complementary pair” of formulae
A, ¬A appears on it; otherwise, it remains open. The rest is the same as for the signed
version. The two versions are essentially equivalent and it is only a matter of preference
or convenience when deciding which to use.
References for further reading
D’Agostino et al. (1999) is a comprehensive handbook on tableaux methods, not only
in classical logic. Fore more details on theory and examples of derivations in Semantic
Tableaux see Nerode and Shore (1993), Jeffrey (1994, who used the expression “analytic
trees”), Smullyan (1995, one of the first versions of Semantic Tableaux), Fitting (1996),
Smith (2003, who called them “trees”), Ben-Ari (2012), and van Benthem et al. (2014).

Deductive Reasoning in Propositional Logic
65
Exercises
For each of the following exercises use Semantic Tableaux (the version you prefer).
2.3.1
Check which of the following formulae are tautologies.
(a) ((p →q) →q) →q
(b) ((q →p) →q) →q
(c) ((p →q) ∧(p →¬q)) →¬p
(d) ((p ∨q) →¬r) →¬(¬q ∧r)
(e) ((p →q) ∧(p →r)) →(p →(q ∧r))
(f) ((p →r) ∧(q →r)) →((p ∨q) →r)
(g) ((p →r) ∨(q →r)) →((p ∨q) →r)
(h) ((p →r) ∧(q →r)) →((p ∧q) →r)
(i) p →((q →r) →((p →q) →r))
(j) (p →(q →r)) →((p →q) →(p →r))
(k) ((¬p ∧q) →¬r) →(¬q →¬(p ∧¬r))
(l) (p →(q ∨¬r)) →(((q →¬p) ∧r) →¬p)
2.3.2
Check which of the following logical consequences are valid.
(a) ¬p →q, ¬p →¬q |= p
(b) (¬p ∧q) →¬r, r |= p ∨¬q
(c) (p ∧q) →r |= (p →r) ∨(q →r)
(d) (p ∧q) →r |= (p →r) ∧(q →r)
(e) (p ∨q) →r |= (p →r) ∧(q →r)
(f) p →(q ∨r) |= (p →q) ∨(p →r)
(g) p →(q ∧r) |= (p →q) ∧(p →r)
(h) p →q, r →s |= (p ∨r) →(q ∨s)
(i) (p →q) ∧(r →s) |= (p ∧r) →(q ∧s)
(j) (p →q) ∨(r →s) |= (p ∨r) →(q ∨s)
(k) (p →q) ∨(r →s) |= (p →s) ∨(r →q)
(l) (p →q) ∧(r →s) |= (p →s) ∧(r →q)
(m) ¬((p →r) ∨(q →r)) |= ¬((p ∨q) →r)
(n) ¬((p ∨q) →r) |= ¬((p →r) ∨(q →r))
2.3.3
Let P, Q, R, and S be propositions. Check whether:
(a) if P →Q, Q →R, and P ∧S are true, then R ∧S is true;
(b) if R and (P ∧¬R) are false and ¬P →¬Q is true, then Q is false;
(c) if Q →(R ∧S) is true and Q ∧S is false, then R ∧Q is false.
(d) if P →Q and Q →(R ∨S) are true and P →R is false, then S is true;
(e) if Q →(R ∧S) is true and Q ∧S is false, then Q is false.
2.3.4
Check the soundness of each of following rules of propositional inference.
(a)
(p ∨q) →r, p ∨r
r
(b) p →q, ¬q ∨r, ¬r
¬p
(c) p →q, p ∨¬r
q →r
(d) ¬p →¬q, ¬(p ∧¬r), ¬r
¬q

66
Logic as a Tool
(e)
p →q, r ∨(s ∧¬q), q
¬p
(f)
p →q, q →(r ∨s), ¬.(p →r)
s
2.3.5
Check the logical correctness of each of the following propositional arguments.
(a)
If the triangle ABC has a right angle, then it is not equilateral.
The triangle ABC does not have a right angle.
Therefore, the triangle ABC is equilateral.
(b) In the following argument n is a certain integer.
If n is divisible by 2 and n is divisible by 3, then n is divisible by 6.
If n is divisible by 6, then n is divisible by 2.
n is not divisible by 3.
Therefore n is not divisible by 6.
(c)
If Thomas likes logic, then he is clever.
If Thomas is clever, then he is rich.
Therefore, if Thomas likes logic, then he is rich.
(d)
Socrates is wise or not happy.
If Socrates is not wise then Socrates is not a philosopher.
Therefore, if Socrates is a philosopher then Socrates is not happy.
(e)
Alexis smiles if she sings and is happy.
Alexis cries if she is not happy.
Therefore, if Alexis sings, then she smiles or cries.
(f)
Victor will go to a university if he studies hard and he is clever.
Victor will not get a good job if he does not study hard.
Therefore, if Victor is clever, then he will go to a
university or will not get a good job.
(g) Replace the second premise in the previous argument with
“Victor will not get a good job only if he does not study hard”
and determine if the resulting argument is logically correct.
(h)
Johnnie will not learn logic if he skips lectures.
Johnnie will pass the exam if he learns logic and does not skip lectures.
Therefore, if Johnnie skips lectures or does not pass the exam,
then he will not learn logic.

Deductive Reasoning in Propositional Logic
67
(i) Replace the second premise in the previous argument with
“Johnnie will pass the exam only if he learns logic and does
not skip lectures”
and determine if the resulting argument is logically correct.
(j)
If Bonnie does not come to the party, then Clyde will not come,
either, or Alice will come.
If Clyde comes to the party, then Alice will not come.
Therefore, if Clyde comes to the party, then Bonnie will come, too.
(k) Replace the second premise in the previous argument with
“Alice will come to the party if Clyde does not come.”
and determine if the resulting argument is logically correct.
Evert Willem Beth (7.7.1908–12.4.1964) was a Dutch philoso-
pher and logician who made some seminal contributions to math-
ematical logic.
Beth studied mathematics and physics at Utrecht University, as
well as philosophy and psychology, and obtained a PhD in philos-
ophy in 1935. He became Professor of Logic and the Foundations
of Mathematics at the University of Amsterdam in 1946 – the first
such professor in the Netherlands – and worked there until his
death with two brief interruptions (the first was in 1952, when
he was a Research Associate at the University of California in
Berkeley with Alfred Tarski).
Beth is best known for the so-called Beth definability theorem (stating that
implicit and explicit definability of functions and predicates in first-order logic
are equivalent), for his construction of what is now called Beth models for the
intuitionistic logic, and for the development of the method of semantic tableaux,
originally devised independently by him (1955), Hintikka (1955), and Schütte
(1956). The tableau method is a systematic search for refutations, usually presented
in an upside-down tree form, similar to Gentzen’s method for natural deduction (a
systematic search for proofs presented in a tree-like form).
During his last years, Beth tried to relate his research in logic to various applica-
tions including automated theorem proving, mathematical education, and heuristics,
as well as translation methods in natural languages.
An E. W. Beth Dissertation Prize named in his honour is awarded annually by the
Association for Logic, Language and Information (FoLLI) to outstanding interdis-
ciplinary PhD theses in these fields.

68
Logic as a Tool
Jaakko Hintikka (12.01.1929–12.08.2015) was a Finnish
philosopher and logician, well known for several important
contributions to a very wide range of topics including mathe-
matical and philosophical logic, philosophy of logic and math-
ematics, philosophy of language philosophy of science, episte-
mology, and history of philosophy.
Hintikka was a philosophy student of G. H. von Wright at
the University of Helsinki and obtained his PhD in 1953 on
distributive normal forms. Following three years as a post-doctoral researcher at
Harvard, his 60-year-long academic career included professorial positions at the Uni-
versity of Helsinki, the Academy of Finland, Florida State University, and finally
Boston University from 1990.
Hintikka introduced or discovered independently and developed several now
fundamental logical concepts and methods, including: the method of semantic
tableaux (which he referred to as “tree-method”) independently of Beth in 1955;
possible-worlds semantics for modal logic in 1957; epistemic logic (logic of
knowledge and belief) in his ground-breaking 1962 book Knowledge and Belief;
game-theoretical semantics; infinitary logics; and independence-friendly logic.
He was active in research until his final days, and participated in the Congress of
Logic, Methodology and Philosophy of Science in Helsinki just a few days before
his death at the age of 86.
2.4
Natural Deduction
The system of Natural Deduction aims to provide a well-structured way of formal rea-
soning that represents a systematic, intuitively natural, and, of course, logically correct
argumentation. It was originally developed in the late 1920s – early 1930s by Gerhard
Gentzen and a version of it independently by Stanisław Ja´skowski3.
Building on Hilbert’s idea of axiomatic system as a paradigm of a deductive system,
Natural Deduction was the first rule-based deductive system, adding the possibility
for making and withdrawing additional assumptions in derivations, that is, formulae
that are neither axioms nor have been derived. It involves two main kinds of inference
rules – introduction rules and elimination rules – for each logical connective, an
idea that turned out to be very important in proof theory. Gentzen’s motivation for
developing Natural Deduction was to provide a purely syntactic, combinatorial proof
of the consistency of Peano’s axiom system of arithmetic, and he was partly successful
in that pursuit. In order to analyze derivations better and to prove a special property of
derivations that could be used for the proof of consistency, Gentzen also introduced
Sequent Calculus, a deduction system based not just on formulae but on sequents – pairs
of lists of formulae, being assumptions on the left and conclusions on the right – with a
set of rules for manipulating and deriving such sequents.
3 Read more on Gentzen’s and Ja´skowski’s work in the biographic boxes at the end of this section.

Deductive Reasoning in Propositional Logic
69
2.4.1
Description
The deductive system of Natural Deduction, hereafter denoted ND, is based on a
set of inference rules which naturally reflect the logical meaning of the propositional
connectives.
ND has no axioms, but several inference rules (listed further) including a pair of rules
for each logical connective: an introduction rule which produces a conclusion containing
that connective as the main one; and an elimination rule in which the connective occurs
as the main connective of a premise. To reduce the number of rules, we will assume ↔
to be defined in terms of →and ∧. Note that, since ¬A ≡A →⊥, the rules for ¬ can be
regarded as particular cases of the corresponding rules for →. There are also two additional
rules, (⊥) and (RA), which will be discussed later in this section.
The derivation in ND consists of successive application of the inference rules, using
as premises the initial assumptions or already derived formulae as well as additional
assumptions which can be added at any step of the derivation. Some of the rules allow
for cancellation (or discharge) of assumptions, which is indicated by putting them in
square brackets. The idea of the additional assumptions is that they only play an auxiliary
role in the derivation; when no longer needed they are canceled, but only with an applica-
tion of an appropriate rule which allows such a cancellation. Note that the cancellation of
an assumption, when the rule allows it, is an option, not an obligation, so an assumption
can be re-used several times before being canceled. However, all assumptions which have
not been canceled during the derivation must be declared in the list of assumptions from
which the conclusion is proved to be a logical consequence. If we want to prove that a
formula C is a logical consequence from a set of assumptions Γ, then any assumption
which is not in Γ must therefore be canceled during the derivation.
Here is a brief explanation of the rules of ND presented in Table 2.2.
(∧I) To prove the truth of a conjunction A ∧B, we have to prove the truth of each of
A and B.
(∧E) The truth of a conjunction A ∧B implies the truth of each of the conjuncts.
(∨I) The truth of a disjunction A ∨B follows from the truth of either disjunct.
(∨E) If the premise is a disjunction A ∨B we reason per cases, that is, we consider
separately each of the two possible cases for that disjunction to be true: Case 1:
A is true; and Case 2: B is true. If we succeed in proving that in each of these
cases the conclusion C follows, then we have a proof that C follows from A ∨B.
(→I) To prove the truth of an implication A →B, we assume that (besides all
premises) the antecedent A is true and try to prove that the consequent B is true.
(→E) This is the detachment rule (Modus ponens) which we have already discussed in
Section 2.2.
(¬I) To prove a negation ¬A we can apply proof by contradiction, that is, assume A
(which is equivalent to the negation of ¬A) and try to reach a contradiction.
(¬E) The falsum follows from any contradiction.
(⊥) “Ex falso sequitur quodlibet”: from a false assumption anything can be derived.
(RA) This rule is called ‘Reductio ad absurdum’. It formalizes the method of proof
by contradiction or proof from the contrary: if the assumption that A is false
(i.e., ¬A is true) leads to a contradiction, then A must be true, formalized as
¬¬A →A.

70
Logic as a Tool
Table 2.2
Rules for Natural Deduction in propositional logic.
The vertical dots indicate derivations
Introduction rules
Elimination rules
(∧I)
A, B
A ∧B
(∧E) A ∧B
A
, A ∧B
B
(∨I)
A
A ∨B ,
B
A ∨B
(∨E)
A ∨B
[A]
...
C
[B]
...
C
C
(→I)
[A]
...
B
A →B
(→E) A, A →B
B
(¬I)
[A]
...
⊥
¬A
(¬E) A, ¬A
⊥
(⊥) ⊥
A
(RA)
[¬A]
...
⊥
A
The rule RA formalizes a typical pattern of non-constructive reasoning. The formula
¬¬A →A is a tautology in the classical logic that we study here, but not in an important
non-classical logical system called intuitionistic logic, developed in the early 20th century
and meant, inter alia, to capture and formalize constructive reasoning where RA is not an
admissible inference rule. In fact, a sound and complete system of Natural Deduction
for the intuitionistic logic can be obtained from ND by simply removing the rule RA.
The intuitionistic logic also rejects the validity of other classical tautologies, expressing
non-constructive claims and derived by using RA, such as the law of excluded middle
A ∨¬A and the implication ¬(A ∧B) →¬A ∨¬B, hence also rejecting the respective
Morgan’s law ¬(A ∧B) ≡¬A ∨¬B.
Theorem 49 (Adequacy Theorem for ND) The system of Natural Deduction is sound
and complete, that is:
A1, . . . , An ⊢ND B if and only if A1, . . . , An |= B.
In particular, ⊢ND B if and only if B is a tautology.
The soundness of ND can be proved by induction on the derivations, and is a fairly
routine exercise. The proof of completeness is more difficult, but it can easily be derived

Deductive Reasoning in Propositional Logic
71
from the completeness of H by showing that every logical consequence that is derivable
in H is also derivable in ND. This will be sketched in Section 2.7.
2.4.2
Examples of derivations in Natural Deduction
1. Commutativity of the conjunction A ∧B ⊢ND B ∧A:
(∧I)
(∧E)A ∧B
B
(∧E)A ∧B
A
B ∧A
Note that an assumption A ∧B was used twice.
2. ⊢ND A →¬¬A :
(→I)
(¬I)
(¬E)[A]2, [¬A]1
⊥
¬¬A
1
A →¬¬A
2
In this derivation two additional assumptions have been made: A and ¬A. The label
1 indicates the application of the rule which allows the cancellation of the assumption
¬A, while label 2 indicates the cancellation of A.
3. ⊢ND ¬¬A →A :
(→I)
(RA)
(¬E)[¬¬A]2, [¬A]1
⊥
A
1
¬¬A →A
2
Note the application of (RA) in this derivation. It can be proved that the derivation
cannot be made without this or an equivalent rule.
4. ⊢ND (A →(B →C)) →((A ∧B) →C) :
(→E)
(∧E)[A ∧B]1
B
(→E)
(∧E)[A ∧B]1
A
, [A →(B →C)]2
B →C
(→I)
(→I)
C
(A ∧B) →C
1
(A →(B →C)) →((A ∧B) →C)2
From this point onward I will usually omit the rule labels of the steps in the derivations,
but the reader should be able to figure out which rule is being applied at each step.
5. A →B ⊢ND ¬B →¬A :
[¬B]1A →B
B
, [¬B]2
⊥
¬A
¬B →¬A
2
1

72
Logic as a Tool
6. ¬B →¬A ⊢ND A →B:
[¬B]1, ¬B →¬A
[¬A]
, [A]2
⊥
(→I)
B
A →B
2
1
Again, this derivation requires the use of (RA).
7. A ∨B ⊢ND ¬A →B:
A ∨B
[¬A]1, [A]3
⊥
B
¬A →B 1 [¬A]2, [B]3
¬A →B 2
¬A →B
3
Note the application of the rule (∨E) (reasoning per cases) in the last step of this
derivation.
References for further reading
For more details on the theory and examples of derivations in Natural Deduction see van
Dalen (1983), Jeffrey (1994) (who referred to “synthetic trees”), Smullyan (1995), Fitting
(1996), Huth and Ryan (2004), Nederpelt and Kamareddine (2004), Prawitz (2006, the
original development of the modern version of Natural Deduction), Chiswell and Hodges
(2007), and van Benthem et al. (2014), as well as Kalish and Montague (1980) and Bor-
nat (2005), who present Natural Deduction derivations in a boxed form rather than in
tree-like shape.
Exercises
2.4.1
Prove that all inference rules of ND are logically sound.
2.4.2
Construct logically sound rules for the introduction and elimination of the
biconditional ↔.
2.4.3
Show that the following tautologies are theorems of ND.
(Hint: for some of these you will have to use the rule (RA).)
(a) (p ∧(p →q)) →q
(b) p →¬¬p
(c) ¬¬p →p
(d) p ∨¬p
(e) ((¬p →q) ∧(¬p →¬q)) →p
(f) ((p →q) ∧(p →¬q)) →¬p
(g) ((p →q) ∧(q →r)) →(p →r)
(h) ((p →q) ∧(p →r)) →(p →(q ∧r))
(i) ((p →r) ∧(q →r)) →((p ∨q) →r)
(j) p →((q →r) →((p →q) →(p →r)))
(k) (p →(q ∨¬r)) →(((q →¬p) ∧r) →¬p)

Deductive Reasoning in Propositional Logic
73
2.4.4
Show that for any formulae A, B:
(a) If A ⊢ND B then ¬B ⊢ND ¬A.
(b) If ¬B ⊢ND ¬A then A ⊢ND B.
(c) If ¬B ⊢ND ¬A then A ⊢ND B.
2.4.5
Derive each of the following for any formulae A, B, C.
(Hint: for some of these exercises use derivations of previous exercises.)
(a) A →C, B →C ⊢ND (A ∨B) →C
(b) (A →B) ∨C ⊢ND A →(B ∨C)
(c) (A ∨B) →C ⊢ND (A →C) ∧(B →C)
(d) A →(B ∨C) ⊢ND (A →B) ∨C
(e) A →¬A ⊢ND ¬A
(f) ¬A →A ⊢ND A
(g) ¬A →B, ¬A →¬B ⊢ND A
(h) ¬A →B ⊢ND A ∨B
(i) (¬A ∧B) →¬C, C ⊢ND A ∨¬B
(j) ¬(A ∧B) ⊢ND ¬A ∨¬B
(k) ¬(A ∨B) ⊢ND ¬A ∧¬B
(l) ¬A ∨¬B ⊢ND ¬(A ∧B)
(m) ¬A ∧¬B ⊢ND ¬(A ∨B)
(n) ¬(A →B) ⊢ND A ∧¬B
(o) A ∧¬B ⊢ND ¬(A →B)
2.4.6
Derive the following logical consequences in Natural Deduction.
(a) p →¬q, ¬r ∨p |= (¬p →r) →¬q
(b) ¬p →¬q, ¬(p ∧¬r), ¬r |= ¬q
(c) ¬p ∨¬r, r →¬q, ¬q →p |= ¬r
(d) (¬p ∧q) →¬r, r |= p ∨¬q
2.4.7
Formalize the following propositional arguments and prove their logical correct-
ness by using Natural Deduction.
(a)
Alice is at home or at work.
Alice is not at home.
Therefore, Alice is at work.
(b)
Nina will go to a party or will not go to the office.
Nina will not go to a party or will not go to the office.
Therefore, Nina will not go to the office.
(c)
Victor is lazy or Victor is clever.
If Victor is not successful then Victor is lazy.
Victor is not lazy.
Therefore, Victor is clever and successful.

74
Logic as a Tool
(d)
Socrates is happy or not stupid.
If Socrates is happy then Socrates is not a philosopher.
Therefore, if Socrates is a philosopher then Socrates is not stupid.
(e)
If the court is wet then it rains or the sprinkler is on.
The court is not closed if the sprinkler is on.
Therefore, if the court is closed and wet then it rains.
(f)
Olivia is not sleeping if she is not smiling.
Olivia is not eating or she is smiling.
If Olivia is sleeping whenever she is not eating, then Olivia is smiling.
(g)
Alexis cries if she is sad.
Alexis sulks or does not cry.
Alexis does not sulk.
Therefore, Alexis is not sad.
(h)
If Bill smokes regularly, then Bill is a heavy smoker.
It is not true that if Bill smokes regularly then Bill will quit smoking.
If Bill is a heavy smoker then Bill will quit smoking or get lung cancer.
Therefore, Bill will get lung cancer.
(i)
Bonnie was an accomplice if Alice is innocent and
Alice witnessed the robbery.
Clyde was not an accomplice if Alice is not innocent.
Therefore, if Alice witnessed the robbery,
then Bonnie was an accomplice or Clyde was not an accomplice.
(j)
If Kristina is not a good student, then she is not clever or she is lazy.
Kristina is not clever or she is not lazy.
Therefore, if Kristina is clever, then she is a good student.
(k)
It will not rain if it is sunny.
It will be misty if it rains and is not sunny.
Therefore, if it is sunny or is not misty then it will not rain.

Deductive Reasoning in Propositional Logic
75
Gerhard Karl Erich Gentzen (24.11.1909–4.8.1945) was a Ger-
man mathematician and logician who made pioneering contri-
butions to the foundations of mathematics, especially in proof
theory of which he is one of the founders.
Gentzen was a student at the University of Göttingen of Paul
Bernays, a Swiss logician and collaborator of David Hilbert.
Gentzen completed his doctoral thesis in 1933 under Bernays’
supervision, but when Bernays was fired in April 1933 for
being “non-Aryan” Hermann Weyl formally acted as Gentzen’s
supervisor until its defence. Nevertheless, Gentzen kept in contact with Bernays
until the beginning of the World War II. During 1935–1939 he was an assistant of
Hilbert in Göttingen.
In the early 1930s Gentzen developed his logical deductive systems natural
deduction and sequent calculus for both classical and intuitionistic logic. In his
thesis Untersuchungen über das logische Schliessen (Investigations into Logical
Inference), he wrote that his aim was formal analysis of mathematical proofs that
occur in practice. The main result he proved there, known as Gentzen’s Hauptsatz,
was the cut-elimination property of his sequent calculus, that is, elimination of
the rule “cut” (essentially, the sequent version of Modus Ponens), which allows
for easier proofs of consistency. Gentzen’s ultimate goal was to provide a purely
syntactic, combinatorial proof of the consistency of Peano’s axiom system of
arithmetic, and he succeeded in that to some extent (by invoking transfinite
induction). However, later that goal was proved unattainable by Gödel’s Second
Incompleteness Theorem, stating that no sufficiently expressive deductive system,
such as Peano’s arithmetic, can prove its own consistency. Still, proof theory
flourished as one of the main branches of mathematical logic after Gentzen’s
ground-breaking work, and it has achieved much in analyzing the notion of formal
derivation and in proving relative consistency results.
Gentzen joined the Nazi paramilitary wing Sturmabteilung in November 1933 and
the NSDAP in 1937. In April 1939 he swore the oath of loyalty to Hitler as part of
his academic appointment. Under a contract from the SS, Gentzen evidently worked
for the V-2 project.
From 1943 Gentzen lectured at the University of Prague. Towards the end of the
war, on 7 May 1945, he was arrested with all other Germans in Prague, and starved
to death in early August at the age of 35.

76
Logic as a Tool
Stanisław Ja´skowski (22.04.1906–16.11.1965) was a Polish logician
who made important contributions to proof theory and formal seman-
tics. He was a student of Jan Łukasiewicz and a member of the famous
Lwów-Warsaw School of Logic.
Ja´skowski was one of the founders of the system of natural deduc-
tion, which he developed independently from (and possibly before)
Gerhard Gentzen in the late 1920s, developed in his doctoral thesis
defended in 1932 and published in the 1934 paper On the Rules of
Suppositions in Formal Logic. Gentzen’s approach eventually became more pop-
ular, in particular because Gentzen proved the cut-elimination property for it, but
Ja´skowski’s method was considered by some logicians to be closer to the way that
proofs are made in practice.
Ja´skowski was one of the first logicians to propose a formal calculus of
inconsistency-tolerant logic (now also known as paraconsistent logic) in his 1948
paper A propositional Calculus for Inconsistent Deductive Systems. He was also
one of the pioneers in the study of both intuitionistic logic, for which he developed
adequate semantics based on logical matrices, and free logic, which allows empty
domains.
Ja´skowski was a President (Rector) of the Nicolaus Copernicus University in
Toru´n.

Deductive Reasoning in Propositional Logic
77
2.5
Normal forms and Propositional Resolution
The deductive systems presented so far are, more or less, suitable for human use but not
for computer-based derivations (with the exception of Semantic Tableaux). Here I present
yet another deductive system which is particularly suitable for automation as it is based
on no axioms and uses only one simple rule of inference, called (Propositional) Resolu-
tion. Originally introduced by Martin Davis and Hilary Putnam4 around 1960, it was later
improved and refined into the Davis–Putnam–Logemann–Loveland (DPLL) algorithm.
This backtracking-based search algorithm for determining the satisfiability of proposi-
tional formulae in conjunctive normal form (see below) is still the basis of most efficient
complete Boolean satisfiability problem (SAT-) solving algorithms (see more on these in
Section refLB12:sect6:AddBoolSat). As for the previously studied deductive systems, the
method of Propositional Resolution is sound and complete for propositional logic.
The rule of Propositional Resolution is very simple:
A ∨C,
B ∨¬C
A ∨B
,
where A, B, C are any propositional formulae. The formula A ∨B is called a resolvent
of A ∨C and B ∨¬C, and we write A ∨B = Res(A ∨C, B ∨¬C).
This rule is sound in the usual sense: A ∨C, B ∨¬C |= A ∨B. The proof of this is
left as an easy exercise for the reader.
2.5.1
Conjunctive and disjunctive normal forms of propositional formulae
The resolution rule is particularly efficient when applied to formulae that are pre-processed
in so-called clausal form, which can be obtained immediately from conjunctive normal
form. Normal forms are also useful for other technical purposes, and I devote this subsec-
tion to them. First, I provide some basic definitions.
Definition 50
1. A literal is a propositional variable or its negation.
2. An elementary disjunction (respectively, elementary conjunction) is any literal or a
disjunction (respectively, conjunction) of two or more literals.
3. A disjunctive normal form (DNF) is any elementary conjunction or a disjunction of
two or more elementary conjunctions.
4. A conjunctive normal form (CNF) is any elementary disjunction or a conjunction of
two or more elementary disjunctions.
Examples:
• p, ¬q, p ∨¬q, p ∨¬p ∨q ∨¬r are elementary disjunctions;
• p, ¬q, ¬p ∧q, ¬p ∧q ∧¬r ∧¬p are elementary conjunctions;
4 Read more on Davis and Putnam in the biographic boxes at the end of this section.

78
Logic as a Tool
• p, ¬q, p ∧¬q, p ∨¬q, (p ∧¬p) ∨¬q, (r ∧q ∧¬p) ∨(¬q ∧p) ∨(¬r ∧p) are dis-
junctive normal forms;
• p, ¬q, p ∧¬q, p ∨¬q, p ∧(¬p ∨¬q), (r ∨q ∨¬r) ∧¬q ∧(¬p ∨r) are conjunctive
normal forms.
The normal forms are convenient for various symbolic manipulations and logical com-
putations. They are also used for construction and optimization of logical circuits in com-
puter systems design, which is why the following fact is very important.
Theorem 51 Every propositional formula is equivalent to a disjunctive normal form and
to a conjunctive normal form.
We give two methods for construction of such equivalent normal forms.
The first method is based on the following algorithm which transforms any formula into
a DNF, respectively CNF, using some of the logical equivalences listed above.
1. Eliminate all occurrences of ↔and →using the logical equivalences
A →B ≡¬A ∨B,
A ↔B ≡(A →B) ∧(B →A).
2. Import all negations in front of the propositional variables, using the logical equiva-
lences listed above.
3. For a DNF: distribute all conjunctions over disjunctions using p ∧(q ∨r) ≡(p ∧q) ∨
(p ∧r).
4. Respectively, for a CNF: distribute all disjunctions over conjunctions using p ∨(q ∧
r) ≡(p ∨q) ∧(p ∨r).
Throughout this process the formulae can be simplified by using commutativity, asso-
ciativity, and idempotency of ∨and ∧as well as:
p ∨¬p ≡⊤;
p ∧¬p ≡⊥;
p ∧⊤≡p;
p ∧⊥≡⊥;
p ∨⊤≡⊤;
p ∨⊥≡p.
Example 52 (p ∧¬r) →(p ↔¬q)
≡¬(p ∧¬r) ∨((p →¬q) ∧(¬q →p))
≡(¬p ∨¬¬r) ∨((¬p ∨¬q) ∧(¬¬q ∨p))
≡¬p ∨r ∨((¬p ∨¬q) ∧(q ∨p))
For a DNF we further distribute ∧over ∨and simplify:
≡¬p ∨r ∨(((¬p ∨¬q) ∧q) ∨((¬p ∨¬q) ∧p))
≡¬p ∨r ∨((¬p ∧q) ∨(¬q ∧q)) ∨((¬p ∧p) ∨(¬q ∧p))
≡¬p ∨r ∨((¬p ∧q) ∨⊥) ∨(⊥∨(¬q ∧p))
≡¬p ∨r ∨(¬p ∧q) ∨(¬q ∧p).

Deductive Reasoning in Propositional Logic
79
For a CNF we distribute ∨over ∧and simplify:
≡(¬p ∨r ∨¬p ∨¬q) ∧(¬p ∨r ∨q ∨p)
≡(¬p ∨r ∨¬q) ∧(⊤∨r ∨q)
≡(¬p ∨r ∨¬q) ∧⊤
≡¬p ∨r ∨¬q.
As can be seen, in this case the CNF also turns out to be a DNF, even simpler than
the one we obtained above. The problem of minimization of normal forms, which is of
practical importance, will not be discussed here.
The second method constructs the normal forms directly from the truth table of the
given formula. I outline it for a DNF.
Given the truth table of the formula A we consider all rows (i.e., all assignments of
truth values to the occurring variables) where the truth value of A is true. If there are
no such rows, the formula is a contradiction and a DNF for it is, for example, p ∧¬p.
Otherwise, with every such row we associate an elementary conjunction in which all vari-
ables assigned value T occur positively, while those assigned value F occur negated. For
instance, the assignment F, T, F to the variables p, q, r is associated with the elementary
conjunction ¬p ∧q ∧¬r. Note that such an elementary conjunction is true only for the
assignment with which it is associated.
As an exercise, show that the disjunction of all elementary conjunctions associated with
the rows in the truth table of a formula A is logically equivalent to A.
Example 53 The formula p ↔¬q has a truth table
p
q
p ↔¬q
T
T
F
T
F
T
F
T
T
F
F
F
The corresponding DNF is (p ∧¬q) ∨(¬p ∧q). Check this!
As an exercise, outline a similar method for construction of a CNF from the truth table
of a formula and prove your claim.
2.5.2
Clausal Resolution
Definition 54
1. A clause is essentially an elementary disjunction l1 ∨. . . ∨ln but written as a set of
literals {l1, . . . , ln}.
2. The empty clause {} is a clause containing no literals; a unit clause is a clause con-
taining only one literal.
3. A clausal form is a (possibly empty) set of clauses, written as a list: C1 . . . Ck. It
represents the conjunction of these clauses.

80
Logic as a Tool
Every CNF can therefore be rewritten in a clausal form, and every propositional formula
is therefore equivalent to a formula in a clausal form.
As an example, the clausal form of the CNF formula (p ∨¬q ∨¬r) ∧¬p ∧(¬q ∨r)
is {p, ¬q, ¬r}{¬p}{¬q, r}.
The resolution rule can be rewritten for clauses as follows:
{A1, . . . , C, . . . , Am}{B1, . . . , ¬C, . . . , Bn}
{A1, . . . , Am, B1, . . . , Bn}
.
The rule above is called Clausal Resolution. The clause {A1, . . . , Am, B1, . . . , Bn} is
a resolvent of the clauses {A1, . . . , C, . . . , Am} and {B1, . . . , ¬C, . . . , Bn}.
The soundness of the Clausal Resolution rule is again left as an easy exercise.
Example 55 Here are some examples of applying Clausal Resolution:
{p, q, ¬r}{¬q, ¬r}
{p, ¬r}
,
{¬p, q, ¬r}{r}
{¬p, q}
,
{¬p}{p}
{}
.
Note that two clauses can have more than one resolvent, for example:
{p, ¬q}{¬p, q}
{p, ¬p}
,
{p, ¬q}{¬p, q}
{¬q, q}
.
However, it is wrong to apply the resolution rule for both pairs of complementary literals
concurrently and obtain
{p, ¬q}{¬p, q}
{}
.
Why? Because the conclusion here is the empty clause, which is false, does not follow
logically from the premises.
2.5.3
Resolution-based derivations
The method of resolution works similarly to Semantic Tableaux. In order to determine
whether a logical consequence A1, . . . , An |= B holds using the method of resolution,
we negate the conclusion B and transform each of the formulae A1, . . . , An, ¬B into
clausal form. We then test whether the resulting set of clauses is unsatisfiable by looking
for a resolution-based proof of the empty clause from that set of clauses. Formally:
Definition 56 A resolution-based derivation of a formula B from a list of formulae
A1, . . . , An is a derivation of the empty clause {} from the set of clauses obtained from
A1, . . . , An, ¬B by successive applications of the rule of Propositional (Clausal) Reso-
lution. We will denote the system of Propositional Resolution by RES and the claim that
there is a resolution-based derivation of B from A1, . . . , An by A1, . . . , An ⊢RES B.
Example 57 Check whether p →q, q →r ⊢RES p →r.
First, transform p →q, q →r, ¬(p →r) to clausal form:
C1 = {¬p, q}, C2 = {¬q, r}, C3 = {p}, C4 = {¬r}.

Deductive Reasoning in Propositional Logic
81
Now, applying resolution successively:
C5 = {q} = Res(C1, C3);
C6 = {r} = Res(C2, C5);
C6 = {} = Res(C4, C6).
The derivation of the empty clause is a proof that p →q, q →r ⊢RES p →r.
Example 58 Check whether ¬p →q, ¬r ⊢RES p ∨(¬q ∧¬r).
First, transform (¬p →q), ¬r, ¬(p ∨(¬q ∧¬r)) to clausal form:
C1 = {p, q}, C2 = {¬r}, C3 = {¬p}, C4 = {q, r}.
Now, applying resolution successively:
C5 = Res(C1, C3) = {q};
C6 = Res(C2, C4) = {q}.
At this stage no new applications of the Clausal Resolution rule are possible; the empty
clause is therefore not derivable. We therefore have ¬p →q, ¬r ⊬RES p ∨(¬q ∧¬r).
Deletion of repeating literals
Recall that clauses are sets of literals but are recorded as lists, meaning that if more than
one copy of a given literal is listed in a clause, the extra copies should be deleted. From
this point onwards we assume that such deletion of repeating literals is done automatically
in every new resolvent. For example, instead of
{p, ¬q, ¬r}{q, ¬r}
{p, ¬r, ¬r}
we derive directly
{p, ¬q, ¬r}{q, ¬r}
{p, ¬r}
.
Theorem 59 (Adequacy theorem for RES) The system of Clausal Resolution is sound
and complete, that is:
A1, . . . , An ⊢RES B
if and only if
A1, . . . , An |= B.
In particular, ⊢RES B if and only if B is a tautology.
The soundness of RES means that if the empty clause is derivable then the input set of
formulae is unsatisfiable. It follows easily from the soundness of the Clausal Resolution
rule: every truth assignment satisfying the input set of formulae must also satisfy the initial
set of clauses produced from them, but then, by soundness of the rule, it must eventually
satisfy the empty clause.
A more detailed generic proof of completeness is sketched in Section 2.7.

82
Logic as a Tool
2.5.4
Optimizing the method of resolution
The method of resolution is amenable to various optimizations by means of special addi-
tional rules and derivation strategies that go beyond the scope of this book. I only mention
some simple rules that remove redundant clauses.
• Tautology deletion: if a clause contains a complementary pair of literals, then it is
a tautology and is therefore of no use for deriving a contradiction, that is, the empty
clause, so it can be removed.
• Subsumption deletion: if a clause C contains (as a set of literals) a clause C ′, then it is
subsumed by C ′, and hence can be safely removed from the set of clauses because every
derivation of the empty clause using C can be reduced to a possibly shorter derivation
using C ′ instead.
• Removal of clauses with mono-polar literals: if a clause C contains a literal l such
that its complementary literal ¯l does not occur in any clause of the current set, then
C can be removed from the set because every derivation of the empty clause using C
can be simplified to a possibly shorter derivation not using C. The reason is simple:
the literal l will be inherited in every resolvent clause produced by using C as a parent
clause because the complementary literal ¯l can never appear in a clause produced from
the current clause set.
References for further reading
Fore more details on the theory and examples of derivations in Propositional Resolution,
see Nerode and Shore (1993), Ebbinghaus et al. (1996), Fitting (1996), Chang and Lee
(1997), Hedman (2004), and Ben-Ari (2012).
Exercises
2.5.1
Prove that the rule of Propositional Resolution is logically sound.
2.5.2
Show that the second method for constructing DNF, presented here and illus-
trated in Example 53, is correct in the sense that the disjunction of all elementary
conjunctions associated with the rows in the truth table of a formula A is logically
equivalent to A.
2.5.3
Develop a method for construction of a CNF from the truth table of a formula,
analogous to that for DNF. Prove your claim.
2.5.4
Construct a DNF and a CNF, equivalent to each of the following formulae, using
both methods.
(a) ¬(p ↔q)
(b) ((p →q) ∧¬q) →p
(c) (p ↔¬q) ↔r
(d) p →(¬q ↔r)
(e) (¬p ∧(¬q ↔p)) →((q ∧¬p) ∨p)

Deductive Reasoning in Propositional Logic
83
2.5.5
Using the method of Clausal Resolution check which of the following formulae
are tautologies.
(a) ((p →q) →q) →q
(b) ((q →p) →q) →q
(c) ((p →q) ∧(p →¬q)) →¬p
(d) ((p ∨q) →¬r) →¬(¬q ∧r)
(e) ((p →q) ∧(p →r)) →(p →(q ∧r))
(f) ((p →r) ∧(q →r)) →((p ∧q) →r)
(g) ((p →r) ∨(q →r)) →((p ∨q) →r)
(h) ((p →r) ∧(q →r)) →((p ∨q) →r)
(i) p →((q →r) →((p →q) →r))
(j) (p →(q →r)) →((p →q) →(p →r))
2.5.6
Using the method of Clausal Resolution, check the validity of the following log-
ical consequences:
(a) ¬p →q, ¬p →¬q |= p
(b) p →r, q →r |= (p ∨q) →r
(c) (p ∨q) →r |= (p →r) ∧(q →r)
(d) (p ∧q) →r |= (p →r) ∨(q →r)
(e) p →q, p ∨¬r, ¬q |= ¬r
(f) (¬p ∧q) →¬r, r |= p ∨¬q
(g) p →q, r ∨(s ∧¬q) |= ¬r →¬p
(h) p →q, q →(r ∨s), ¬(p →r) |= s
(i) p →(q ∧r) |= (p →q) ∧(p →r)
(j) (p ∧q) →r |= (p →r) ∧(q →r)
2.5.7
Using Clausal Resolution, check the logical correctness of the following propo-
sitional arguments. If not logically correct, find a falsifying truth assignment.
(a)
If Socrates is a philosopher then Socrates is not happy.
If Socrates is wise and happy then Socrates is not a philosopher.
Therefore, if Socrates is a philosopher then Socrates is wise.
(b)
Nina wears a red dress or will not wear a silk scarf at the dinner.
If Nina wears a red dress at the dinner, then she will wear high heels.
Nina does not wear a red dress at the dinner.
Therefore, Nina will wear high heels and will not wear a
silk scarf at the dinner.
(c)
If Olivia is not sleeping, then she is crying or she is not hungry.
If Olivia is crying, then she is hungry.
Therefore, if Olivia is not crying, then she is sleeping.

84
Logic as a Tool
(d)
The property prices increase if the interest rates go down.
The interest rates go down or the economy is not doing well.
The property prices do not increase.
Therefore the economy is not doing well.
(e) Replace the first premise of the propositional argument in the previous ques-
tion with
“The property prices increase only if the interest rates go
down”
and check the correctness of the revised argument.
(f)
Alice will not come to the party if Bonnie comes to the party.
Clyde will come to the party only if Alice comes and
Bonnie does not come.
Therefore Alice will not come to the party if Bonnie comes or
Clyde does not come.
(g) Replace the second premise of the propositional argument in the previous
question with
“Clyde will come to the party if Alice comes and Bonnie
does not come”
and check the correctness of the revised argument.
(h)
Hans will be promoted if he is clever and does his job well.
Hans will be fired if he does not do his job well.
Therefore, if Hans is clever, then he will be promoted or will be fired.
(i) Replace the second premise of the propositional argument in the previous
question with
“Hans will be fired only if he does not do his job well”
and check the correctness of the revised argument.
2.5.8
Prove that each of the simplification rules – deletion of repeating literals, tau-
tology deletion, subsumption deletion, and removal of clauses with mono-polar
literals – is based on a logically sound rule.

Deductive Reasoning in Propositional Logic
85
Martin Davis (b. 1928) is an American mathematician known
for several important ideas and results in mathematics, logic,
and computability theory.
Davis was an undergraduate student of Emil Post in New
York and later completed his doctoral study under the supervi-
sion of Alonso Church at Princeton University in 1950. He is
currently a Professor Emeritus at New York University.
One of Davis’ most important work, together with Hilari
Putnam, led to the solution of Hilbert’s tenth problem, namely the proof of the
algorithmic unsolvability of Diophantine equations. That proof was eventually com-
pleted by the young Russian mathematician Yuri Matiyasevich in 1970, essentially
using results by Davis, Putnam, and Julia Robinson.
In around 1960, Davis and Putnam invented the Davis–Putnam algorithm for
checking the validity of a first-order logic formula (which only terminates on valid
formulae), using a resolution-based decision procedure for propositional logic.
Later, their algorithm was improved to the Davis–Putnam–Logemann–Loveland
(DPLL) algorithm which is still the basis for the currently most efficient complete
SAT-solvers.
Davis is also known for inventing his models of Post-Turing machines.
Hilary Whitehall Putnam (31.07.1926 – 13.03.2016) is an
American philosopher, mathematician, and computer scientist,
a leading figure in analytic philosophy since the 1960s, espe-
cially in philosophy of mind, of language, of logic, of mathe-
matics, and of science.
Putnam studied mathematics and philosophy at the Uni-
versity of Pennsylvania and received his PhD in philosophy
at UCLA in 1951 for a dissertation on The Meaning of
the Concept of Probability in Application to Finite Sequences, working with
Reichenbach. He then taught philosophy at Northwestern University, Princeton
University, MIT and at Harvard University from 1965 until the end of his active
academic career in 2000.
Putnam is well known for his many influential works on theories of mind and
meaning and, in particular, for his arguments in defence of scientific realism and
objectivity of truth, knowledge, and mathematical reality.
Putnam also made important contributions to mathematics and logic. In around
1960, he and Martin Davis developed the Davis–Putnam algorithm for solving the
Boolean satisfiability problem. Putnam also contributed to the eventual solution of
Hilbert’s tenth problem.

86
Logic as a Tool
2.6
Supplementary: The Boolean satisfiability problem
and NP-completeness
The Boolean satisfiability problem (SAT) is the problem of deciding whether a given input
propositional (Boolean) formula is satisfiable, that is, whether there is a truth assignment
to the propositional (Boolean) variables occurring in the formula that makes it true. It is
the most popular NP-complete decision problem, the meaning of which I explain in the
following.
First, NP stands for “non-deterministic polynomial” time. An algorithmic decision
problem (that requires an answer Yes/No for every input) is in the complexity class NP if,
intuitively, for every input where the answer is Yes there is an evidence (proof or witness)
that can be verified “quickly and efficiently”, which is assumed to mean in polynomial
time, that is, in a number of steps that is bounded by a fixed polynomial in the length of the
input. Equivalently, and again intuitively, a problem is in NP if there is an algorithm that
can run on a hypothetical non-deterministic computing device (e.g., non-deterministic
Turing machine) to always solve the problem and, when the answer is Yes, to produce that
answer in polynomial time. Simply put, an NP problem may be computationally difficult
(slow) to find a solution, but it is easy (quick) to check whether a proposed, or guessed,
solution is correct.
Of course, every decision problem that can be solved (for any answer) by an algorithm
that works in deterministic polynomial time (i.e., one that runs on a usual digital com-
puter and always takes a number of steps that is bounded by a fixed polynomial in the
length of the input) is in NP. Besides, there may be problems that are in NP but for which
there is no algorithm that can solve them in polynomial time. The SAT problem seems
to be a case in point. Indeed, it is in NP because whenever a Boolean formula is satisfi-
able, any given satisfying truth assignment can be verified very quickly in a number of
steps which is linear in the length of the input formula. Alternatively, the SAT problem
can be solved efficiently with a non-deterministic computing device that guesses a sat-
isfying assignment, if there is one, and then verifies in linear time that it indeed works.
Equivalently, we can think of solving SAT by using a hypothetical unboundedly parallel
computing device, simply by determining simultaneously all truth assignments whether
any of them renders the formula true.
There is a number of practically very efficient algorithms for solving the SAT problem,
known as SAT solvers, which usually solve most of the input SAT problems involving
thousands of variables and millions of clauses within seconds. On the other hand, there
is currently no known algorithm run on a digital computer that can always solve the
SAT problem efficiently, that is, in polynomial time. Indeed, all methods for solving the
SAT problem that are known so far – including truth tables, Semantic Tableaux, Propo-
sitional Resolution, and many more modern, extremely clever and practically efficient
algorithms – require in the worst case a number of steps which is exponential in the num-
ber of variables occurring in the input formula, and therefore exponential in the length
of the formula. This is because there are 2n possible truth assignments for a set of n
propositional variables.

Deductive Reasoning in Propositional Logic
87
There are many hundreds of other very important algorithmic problems that are in NP,
including problems of scheduling, coloring of maps with a given number of colors, or
finding the prime factorization of a given integer.
Some of the problems in NP, such as SAT, are also known to be NP-complete. An
algorithmic decision problem is NP-complete if solving any problem that is in NP can
be reduced efficiently (i.e., in a number of steps bounded by some fixed polynomial in
the length of the input) to solving the problem P. Any algorithm solving an NP-complete
problem can therefore be transformed in an at most “polynomially slower” (i.e., not much
slower) algorithm solving any given problem in NP. If an efficient algorithm (always taking
a number of steps polynomially bounded in the size of the input) solving SAT is found, it
can therefore be suitably modified to efficiently solve any other problem in NP.
Can there be an efficient algorithm solving SAT, or any other NP-complete problem?
This has been an open question since 1971 when it was first stated by Stephen Cook
in his seminal paper The Complexity of Theorem Proving Procedures. It is literally a
million-dollar question, being one of the seven Millennium Prize Problems stated by the
Clay Mathematics Institute in 2000, each bearing a 1,000,000 US$ prize tag for the first
solution which is officially accepted as correct by the scientific community. The problem
is known as “P=NP?” and is currently considered to be the most challenging and impor-
tant unsolved problem in computer science. Most computer scientists believe that such
a miracle algorithm, solving NP-complete problems in polynomial time, does not exist
at all, but there are some who have justified reasons to be more optimistic. I can only
conjecture here that we will know the answer before the turn of this century.
References for further reading
For more on the Boolean satisfiability problem and NP-completeness from a logical per-
spective, see Nerode and Shore (1993), Fitting (1996), Hedman (2004), and Ben-Ari
(2012).

88
Logic as a Tool
Stephen Cook (b. 1939) is an American–Canadian computer sci-
entist and mathematician, one of the founders of the theory of
computational complexity and the study of proof complexity.
Cook completed his PhD at Harvard in 1966 as a student of
Hao Wang. In his seminal 1971 paper The Complexity of Theorem
Proving Procedures he formalized the notions of polynomial-time
reduction and NP-completeness and proved that the Boolean
satisfiability problem (SAT) is NP-complete. He also formulated the most famous
problem in computer science, P v. NP, and conjectured that there are no polynomially
fast algorithms solving NP-complete problems, that is, that P̸= NP.
In 1982, Cook received the prestigious ACM Turing award for his fundamental
contributions to complexity theory.
Leonid Levin (b. 1948) is a Soviet–American computer scien-
tist, known for his work in the theory of computing, algorithmic
complexity, and intractability.
Levin studied at Moscow University in 1970 where he
completed doctoral studies under the supervision of Andrey
Kolmogorov in 1972. In 1977 he emigrated to the US, where
he completed another PhD at the Massachusetts Institute of
Technology (MIT) in 1979 under the supervision of Albert R. Meyer.
During his doctoral studies he discovered the existence of NP-complete problems
independently of Stephen Cook, and published this result in 1973. It is now often
called the Cook–Levin Theorem. Levin was awarded the prestigious Knuth Prize
in 2012 for his discovery of NP-completeness and the development of the theory of
average-case complexity.
2.7
Supplementary: Completeness of the propositional deductive
systems
Here I summarize the important concepts and claims related to proving soundness and
completeness of a deductive system for propositional logic and outline a generic proof
of completeness which can be applied mutatis mutandis to each of the deductive systems
introduced here. The proofs of most of the claims are left as exercises but they can be
found in many excellent textbooks on logic, some of which are listed as references at the
end of this chapter. For some of these claims the proofs are generic, that is, essentially
the same for each deductive system, while for others the proofs are essentially different
as they make use of the specific deductive machinery – axioms (if any) and inference
rules – of the system.

Deductive Reasoning in Propositional Logic
89
Hereafter, D denotes any Axiomatic System (H), Semantic Tableau (ST), Natural
Deduction (ND), or Resolution (RES). Derivability in D is denoted ⊢D.
First, we need some more terminology.
By a (propositional) theory I mean any set of propositional formulae. Recall that a set
of formulae is satisfiable if there is a truth assignment that makes all formulae in the set
true; otherwise it is unsatisfiable. Here are some important properties relating satisfiability
and logical consequence that we will need.
Proposition 60 (Satisfiability and logical consequence) For any formula A:
1. Γ ∪{A} is satisfiable iff Γ ⊭¬A.
2. Γ |= A iff Γ ∪{¬A} is unsatisfiable.
3. If Γ ∪{A} is unsatisfiable and Γ ∪{¬A} is unsatisfiable then Γ is also unsatisfiable.
I leave the proofs of these as easy exercises.
Definition 61 (Deductive consistency) A theory Γ is:
1. consistent in D (or simply D-consistent) if there is no formula A such that Γ ⊢D A
and Γ ⊢D ¬A; otherwise, Γ is D-inconsistent; or
2. a maximal D-consistent theory if it is D-consistent and cannot be extended to a larger
D-consistent theory, that is, adding to Γ any formula that is not already in Γ results in
a D-inconsistent theory.
Proposition 62 (D-inconsistency) The following are equivalent for any theory Γ:
1. Γ is D-inconsistent.
2. Γ ⊢D ⊥.
3. Γ ⊢D A for every formula A.
4. There are formulae A1, . . . , An ∈Γ such that ⊢D ¬(A1 ∧· · · ∧An).
The proofs differ for each deductive system as they use the specific notion of derivation
in each, but every one of them is a useful and not very difficult exercise.
Let us now observe that deductive consequence in each of our systems D and the notion
of D-consistency have the properties of logical consequence and satisfiability (i.e., seman-
tic consistency), respectively, stated earlier in Proposition 60:
Proposition 63 (Consistency and deductive consequence) For any formula A:
1. Γ ∪{A} is D-consistent iff Γ ⊬D ¬A.
2. Γ ⊢D A iff Γ ∪{¬A} is D-inconsistent.
3. If
Γ ∪{A}
is
D-inconsistent
and
Γ ∪{¬A}
is
D-inconsistent
then
Γ
is
D-inconsistent.

90
Logic as a Tool
The proofs differ again for each deductive system as they use the specific notion of
derivation in each of them. I leave them as useful exercises.
Next, let us revisit the notion of soundness by redefining it in two different ways.
Definition 64 (Soundness 1) A deductive system D is sound1 if for every theory Γ and
a formula A,
Γ ⊢D A implies Γ |= A.
Definition 65 (Soundness 2) A deductive system D is sound2 if for every theory Γ,
if Γ is satisfiable then Γ is D-consistent.
We therefore now have two different definitions of soundness: one in terms of logical
and deductive consequence, and the other in terms of satisfiability and deductive consis-
tency. These definitions are in fact equivalent and the proof of that, left as an exercise, is
quite generic for all deductive systems using Propositions 60 and 63.
As I have already stated in the respective sections, each of our deductive systems is
sound (in either sense). I leave the proofs of these as exercises. Let us now turn to com-
pleteness.
Definition 66 (Completeness 1) A deductive system D is complete1 if for every theory
Γ and a formula A,
Γ |= A implies Γ ⊢D A.
Definition 67 (Completeness 2) A deductive system D is complete2 if for every theory Γ,
if Γ is D-consistent then Γ is satisfiable.
Again, we have two different definitions of completeness, one in terms of logical and
deductive consequence and the other in terms of satisfiability and deductive consistency.
As for soundness, these definitions are equivalent and the proof is again generic for all
deductive systems, using Propositions 60 and 63. These proofs are left as an exercise.
Proposition 68 (Maximal consistent theory 1) Every maximal D-consistent theory Γ
is closed under deductive consequence in D, that is, for any formula A, if Γ ⊢D A then
A ∈Γ.
Proof: Exercise.
Definition 69 (D-completeness) A theory Γ is D-complete if it is D-consistent and for
every formula A, Γ ⊢D A or Γ ⊢D ¬A.
Proposition 70 (Maximal consistent theory 2) A theory Γ is a maximal D-consistent
theory iff it is closed under deductive consequence in D and is D-complete.
The proof is generic, similar for each D, and left as an easy exercise.

Deductive Reasoning in Propositional Logic
91
The next theorem shows that membership of a given maximal consistent theory has the
same properties as a truth assignment (just replace membership in the theory with truth in
each of the clauses below.)
Theorem 71 (Maximal consistent theory 3) For every maximal D-consistent theory Γ
and formulae A, B, the following hold:
1. ¬A ∈Γ iff A /∈Γ.
2. A ∧B ∈Γ iff A ∈Γ and B ∈Γ.
3. A ∨B ∈Γ iff A ∈Γ or B ∈Γ.
4. A →B ∈Γ iff A ∈Γ implies B ∈Γ (i.e., A /∈Γ or B ∈Γ).
The proof is specific to each deductive system as it uses its specific deductive machinery.
Each proof is left as an exercise.
Given a theory Γ, consider the following truth assignment:
SΓ(p) :=

T, if p ∈Γ;
F, otherwise. for every propositional variable p.
The truth assignment SΓ extends to a truth valuation of every formula by applying a
recursive definition according to the truth tables (see Section 1.4.5.2). The truth valuation
is denoted SΓ.
Lemma 72 (Truth Lemma) If Γ is a maximal D-consistent theory, then for every for-
mula A, SΓ(A) = T iff A ∈Γ.
Proof. Exercise. Use Theorem 71. ■
Corollary 73 Every maximal D-consistent theory is satisfiable.
Lemma 74 (Lindenbaum’s Lemma) Every D-consistent theory Γ can be extended to
a maximal D-consistent theory.
Proof. Let A0, A1,... be a list of all propositional formulae. (NB: they are countably many,
so we can list them in a sequence.) I will define a chain by inclusion of theories Γ0 ⊆Γ1 ⊆
... defined by recursion on n as follows:
• Γ0 := Γ;
• Γn+1 :=

Γn ∪{An},
if Γn ∪{An} is D-consistent;
Γn ∪{¬An}, otherwise.
Note that every Γn is an D-consistent theory. Prove this by induction on n, using the
properties of the deductive consequence from Proposition 63.
Now, we define
Γ∗:=

n∈N
Γn.
Clearly, Γ ⊆Γ∗. Γ∗is a maximal D-consistent theory. Indeed:

92
Logic as a Tool
• Γ∗is D-consistent. Otherwise, Γ∗⊢D A and Γ∗⊢D ¬A. Since a derivation in each D
uses only finitely many assumptions (we say that the deductive consequence in D is
compact), it follows that Γn ⊢D A and Γn ⊢D ¬A for some large enough index n,
which contradicts the consistency of Γn (fill in the details here).
• Γ∗is maximal D-consistent. Indeed, take any formula A. Let A = Am. Then Am ∈
Γm+1 or ¬Am ∈Γm+1, so A ∈Γ∗or ¬A ∈Γ∗; hence, Γ∗cannot be extended to a
larger D-consistent theory. ■
Theorem 75 (Completeness of D) The axiomatic system D is complete.
Proof. Let Γ be a D-consistent theory. Then, by Lindenbaum’s Lemma 74, Γ can be
extended to a maximal D-consistent theory Γ∗. That theory is satisfiable by the Truth
Lemma 72. ■
The outlined proof above applies to each of the deductive systems that we have
studied. However, in order to prove their completeness we do not have to follow the same
scheme of steps for each of them. It is sufficient to prove the soundness and completeness
for one of them, say D, and then reduce the proofs for any of the other to this result,
as follows.
Proposition 76 (Relative soundness and completeness) For any deductive systems for
propositional logic D and D′, the following hold.
1. To prove the soundness of D′ given the soundness of D, it is sufficient to show that for
every theory Γ and a formula A,
Γ ⊢D′ A implies Γ ⊢D A.
2. To prove the completeness of D′ given the completeness of D, it is sufficient to show
that for every theory Γ and a formula A,
Γ ⊢D A implies Γ ⊢D′ A.
I leave the proof as an easy exercise.
Using the above proposition, the soundness and completeness of any of our deductive
systems can be reduced to the soundness and completeness of any other (see exercises).
References for further reading
For more details and complete proofs of soundness and completeness of propositional
deductive systems, see van Dalen (1983) for a completeness proof for ND; Hamilton
(1988) for completeness of H; Nerode and Shore (1993) and Fitting (1996) and for com-
pleteness of ST and RES; Smullyan (1995) for completeness of ST and ND; Ebbinghaus
et al. (1996), Hedman (2004), and Ben-Ari (2012) for completeness of RES; and Tarski
(1965) for a general discussion and methodology.

Deductive Reasoning in Propositional Logic
93
Exercises
No solutions are provided for these exercises, but most of these proofs can be found in the
references listed above.
2.7.1
Prove Proposition 60.
2.7.2
Prove Proposition 62 for H.
2.7.3
Prove Proposition 62 for ST.
2.7.4
Prove Proposition 62 for ND.
2.7.5
Prove Proposition 62 for RES.
2.7.6
Prove Proposition 63 for H.
2.7.7
Prove Proposition 63 for ST.
2.7.8
Prove Proposition 63 for ND.
2.7.9
Prove Proposition 63 for RES.
2.7.10
Using Propositions 60 and 63, prove that the two definitions of soundness are
generically equivalent for each deductive system D.
2.7.11
Using Propositions 60 and 63, prove that the two definitions of completeness are
generically equivalent for each deductive system D.
2.7.12
Prove Proposition 68 generically for each deductive system D.
2.7.13
Prove Proposition 70 generically for each deductive system D.
2.7.14
Prove Theorem 71 for H.
2.7.15
Prove Theorem 71 for ST.
2.7.16
Prove Theorem 71 for ND.
2.7.17
Prove Theorem 71 for RES.
2.7.18
Show that if the construction in Lindenbaum’s lemma is modified as follows:
Γn+1 :=

Γn ∪{An}, if Γn ∪{An} is D-consistent;
Γn,
otherwise.
then the resulting theory Γ∗is still a maximal D-consistent theory.
2.7.19
Prove Proposition 76.

94
Logic as a Tool
2.7.20
Assuming the soundness and completeness of H, prove the soundness and com-
pleteness of each of ST, ND, and RES, by using Proposition 76.
2.7.21
Assuming the soundness and completeness of ST, prove the soundness and com-
pleteness of each of H, ND, and RES by using Proposition 76.
2.7.22
Assuming the soundness and completeness of ND, prove the soundness and
completeness of each of ST, H, and RES by using Proposition 76.
2.7.23
Assuming the soundness and completeness of RES, prove the soundness and
completeness of each of H, ST, and ND by using Proposition 76.
Jan Leopold Łukasiewicz (21.12.1878–13.02.1956) was a Polish
logician and philosopher who introduced mathematical logic in
Poland and made notable contributions to analytical philosophy,
mathematical logic, and history of logic.
Łukasiewicz studied first law and then mathematics and phi-
losophy at the University of Lwów where he achieved a PhD in
1902 under the supervision of Kazimierz Twardowski for a disser-
tation On induction as the inverse of deduction. He taught at the
University of Lwów before WW I then joined the University of Warsaw in 1915,
where he held the position of rector in 1922–23 and 1931–32. He also served as
a minister of education in 1919. Together with another prominent logician, Stanis-
law Lesniewski, Łukasiewicz founded the world-famous Warsaw School of Logic.
Alfred Tarski, a student of Lesniewski but also strongly influenced by Łukasiewicz,
also contributed to the reputation of the school. Łukasiewicz fled from Poland dur-
ing WW II. In 1946 he was appointed Professor of Mathematical Logic at the Royal
Irish Academy in Dublin, where he worked until his retirement in 1953.
Łukasiewicz did important work in modernizing formal logic. He developed
propositional logic and its implicational and equivalential fragments, for all of
which he obtained some elegant short axiomatizations. Notably, he introduced
many-valued logics (partly as an alternative to the Aristotelian 2-valued logic)
in 1917. Łukasiewicz also introduced the Polish notation which allowed logical
formulae to be written unambiguously without the use of brackets. For instance,
the formula (p →(¬p →q)) is written in Polish notation as CpCNpq. He also
developed a theory of axiomatic rejection, wrote a book on Logical Founda-
tions of Probability Theory, and conducted very important work in the history
of logic by studying and popularizing both Aristotle’s syllogistic and Stoic’s
propositional logic.

Deductive Reasoning in Propositional Logic
95
Emil Leon Post (11.02.1897–21.04.1954) was a Polish-born
American mathematician and logician, regarded as one of the
founders of both computability and recursion theory, along
with Alan Turing and Alonso Church, and of proof theory.
Post was born in Augustów, then in the Russian Empire (now
in Poland), into a Polish–Jewish family that emigrated to Amer-
ica when he was 7 years old. When he was a child he lost an arm
in an accident in New York, which prevented him from studying
astronomy (his favourite subject at that time). He graduated from City College of
New York in 1917 and completed his PhD in mathematics at Columbia University
in 1920. He then completed a post-doctorate at Princeton, where he anticipated the
incompleteness phenomena that Gödel discovered several years later, as well as the
undecidability results of Church and Turing. He wrote about his incompleteness
ideas to Church but did not publish them, planning to analyze them better. It was
only when Kurt Gödel published his famous proof in 1931 that Post shared his ideas
with him. It is believed that the excitement from these discoveries probably triggered
Post’s manic-depressive attacks, a condition from which he suffered all his life.
After returning from Princeton, Post became a very successful and popular high
school mathematics teacher at the City College of New York.
Post published some mathematical works as an undergraduate student, but his
first publication in logic was a shortened version of his doctoral dissertation. This
contained a precise formulation and systematic study of the propositional fragment
of Russell and Whitehead’s Principia Mathematica, and provided the first published
proof of its completeness and decidability by showing that any formula in the system
is provably equivalent to one in a full disjunctive normal form. Notably, Post intro-
duced truth tables for the propositional connectives even though, as he noted, they
were implicit in the Principia. He then generalized his two-valued truth-table method
to one which had an arbitrary finite number of truth values. He also promoted in his
thesis the pioneering idea of the development of systems for logical inference based
on a finite process of manipulation of symbols. Such systems produce, in today’s ter-
minology, recursively enumerable sets of words in a finite alphabet, just like formal
systems of deduction, so Post can also be credited with laying the foundations of both
proof theory and recursion theory in his thesis.
In 1936 he developed and published, independently of Turing, an abstract
mathematical model of computation, now known as Post machine or Post–Turing
machine, essentially equivalent to the Turing machine.
Post is also well known for his work on polyadic groups, recursively enumer-
able sets, and degrees of unsolvability, as well as for his results on the algorithmic
unsolvability of certain combinatorial problems, notably the Post correspondence
problem he introduced in 1946. It is equivalent to the Halting problem for Turing
machines, but simpler to use in proofs of undecidability. In 1947 Post showed that
the word problem for semigroups was recursively unsolvable, therefore solving a
problem posed by Thue in 1914.
Post continued to suffer from manic-depressive illness throughout his life, and
died in 1954 at the age of 57 from a heart attack induced by electro-shock treatment
at a mental hospital in New York.

3
Understanding First-order Logic
Propositional logic can only formalize some patterns of reasoning, but it cannot grasp the
logical structure or the truth behavior of very simple sentences such as:
• “x + 2 is greater than 5;”
• “there exists y such that y2 = 2;”
• “for every real number x, if x is greater than 0 then there exists a real number
y such that y is less than 0 and y2 equals x;” or, for a non-mathematical example;
• “every man loves a woman.”
Indeed, note that an expression such as “x + 2 is greater than 5” is not a proposition,
for it can be true or false depending on the choice of x. Neither is “there exists y such
that y2 = 2” a proposition until the range of possible values of y is specified: if y is an
integer, then the statement is false; but if y can be any real number, then it is true. As for the
third sentence above, it is a proposition but its truth depends heavily on its internal logical
structure and mathematical meaning of all phrases involved, and those are not tractable
on a propositional level.
All these sentences take us out of the simple world of propositional logic into the realm
of first-order logic (the term “first-order” will be explained soon), also known as classi-
cal predicate logic or just classical logic, the basic concepts of which we introduce and
discuss here.
First-order logic (just like every formal logical system) has two major aspects:
• precise syntax, involving a formal language called a first-order language, that enables
us to express statements in a uniform way by means of logical formulae;
• formal logical semantics, specifying the meaning of all components of the language by
means of their interpretation into suitable models called first-order structures and
formal truth definitions, extending the truth tables for the propositional connectives1.
Here we discuss the basic components of the syntax and semantics of first-order logic
and their relevance to mathematical reasoning.
1 The reader who has some experience with programming languages should find the concepts of formal syntax and
semantics familiar.
Logic as a Tool: A Guide to Formal Logical Reasoning, First Edition. Valentin Goranko.
© 2016 John Wiley & Sons, Ltd. Published 2016 by John Wiley & Sons, Ltd.

Understanding First-order Logic
97
3.1
First-order structures and languages: terms and formulae
of first-order logic
3.1.1
First-order structures
First-order logic is a formal logical language for formalizing statements and reasoning
about universes represented as so-called first-order structures. Here we first discuss the
basic components of a first-order structure and then give a more formal definition.
Domains
When we reason and make statements about objects, we have in mind a certain domain of
discourse. In mathematics, that domain includes mathematical objects such as numbers
(integers, rationals, reals, etc.), vectors, geometric figures (points, lines, triangles, circles,
etc.), sets and graphs. A domain in a non-mathematical discourse may consist of material
objects, human beings, ideas, or anything else. In either case, it is important that we have
specified our domain of discourse, so that we know what we are talking about. More-
over, as one of the sentences mentioned above suggests, a statement can be true or false
depending on the domain in which it is considered.
Predicates
When we reason about the objects from our domain of discourse, we usually make state-
ments about properties they have or do not have. For instance, talking about integers,
we may discuss properties such as being “positive”, “divisible by 3”, “not greater
than 1999”, etc. Triangles can be “obtuse”, “equilateral”, etc. A human being can be
“female”, “male”, “young”, “blue-eyed”, etc.
The logical term for a property is predicate. Predicates need not only concern one
object, as for the examples mentioned above which are called unary predicates. We also
deal with binary predicates, relating two objects such as: “_ is less than _” or “_ is
divisible by _” (for numbers), or “_ is a son of _” or “_ loves _” (for humans), etc.
Ternary predicates relate three objects, such as “_ is between _and _” (for points on a
line) or “_ and _ are the parents of _” (for humans), etc. In general, we can talk about
n-ary predicates, relating n objects at a time.
As long as we specify the meaning (semantics) of a predicate and the objects which it
relates, that predicate becomes true or false, that is, represents a proposition. For example,
the propositions “12 is divisible by 6” and “13 is divisible by 7” are both instances of
the binary predicate “x is divisible by y.” Furthermore, we can connect predicates by
using propositional connectives in compound statements, just like we did earlier with
propositions.
Functions
Typically in mathematics (and not only there) we use functions to represent operations
which, applied to one or several objects, determine an object. Depending on the number of
arguments, we talk about unary functions, binary functions, etc. Standard examples are
all arithmetic operations and, more generally, all algebraic functions that we have studied
at school. Examples such as “the mother of _” or “the father of _” are in the domain of
humans.
Note that in our logical framework all functions will be considered total, that is, defined
for all possible values of the argument (or all possible tuple values of the arguments) in
the domain. This is quite often not the case, for example subtraction in the domain of

98
Logic as a Tool
natural numbers, division in the domain of reals, or the function “the daughter of _”
in the domain of humans. This problem has an easy (albeit artificial) fix, good enough
for our formal purposes: we designate an element u (for undefined) from the domain and
make the function total by assigning u to be its value for all (tuples of) arguments for
which it is not defined. For instance, we can extend division by 0 in the domain of reals
by putting x/0 = 17 for any real x. This may sound reckless, but if proper care is taken
when reasoning about division, it will not lead to confusion or contradiction.
Constants
Some objects in the domain can be distinguished in a way that would allow us to make
direct references to them, by giving them names. Such distinguished objects are called
constants2. Examples in the domain of real numbers are 0, 1, 3/7,
√
2, π, e, etc. Note that
the notion of a constant pertains to the language rather than to the domain. For instance,
in common calculus we do not have a special name for the least positive solution of
the equation cos x = x, but if for some purposes we need to refer directly to it then we
can extend our mathematical language by giving it a name, that is, make it a constant in
our domain.
We are now ready to define the general notion of a first-order structure.
Definition 77 A first-order structure (hereafter, just structure) consists of a non-empty
domain and a family of distinguished functions, predicates, and constants in that domain.
Example 78 Here are some examples of structures that will be used further.
• N, with domain being the set of natural numbers N, the unary function s (successor, i.e.,
s(x) = x + 1), the binary functions + (addition) and × (multiplication), the predicates
=, <, and >, and the constant 0.
• Z: as for N, but with the domain being the set of integers Z and the additional func-
tion −(subtraction).
• With the same functions and predicates we take the domain to be the set of rational
numbers Q or the reals R. The resulting structures are denoted Q and R, respectively.
(For those familiar with some abstract algebra: algebraic structures such as groups,
rings, and fields are all examples of first-order structures. Algebraic structures usually
only involve functions and constants but no predicates, except = and possibly <.)
• H: the domain is the set of all humans, with functions m (for “the mother of”) and f (for
“the father of”); unary predicates M (for “man”) and W (for “woman”); binary pred-
icates P (for “parent of”), C (for “child of”), L (for “loves”); and constants (names),
for example John, Mary, Adam, Eve.
• G: the domain is the set of all points and lines in the plane, with unary predicates P for
“point” L for “line”, and the binary predicate I for “incidence” between a point and
a line.
• S: the domain is the collection of all sets3, with binary predicates = for “equality” and
∈for “membership”.
2 Note that the use of the word “constant” in first-order logic is not exactly the same as in the common mathematical
language.
3 Strictly speaking, this is not a structure because the collection of all sets is not a proper set, but we can obtain a
structure if we relativize it to the family of all subsets of a given set, a “universe.” See more in Section 5.2.1.

Understanding First-order Logic
99
3.1.2
First-order languages
In order to refer to objects, functions and predicates in our domain, we give them names.
In mathematics they are typically symbols or abbreviations such as 5, π, +, √, sin, = or
<; for humans they are proper names. Together with logical connectives, variables and
some auxiliary symbols, these symbolic names determine formal logical languages called
“first-order languages”. In particular, with every structure S we can associate a first-order
language LS for that structure, containing:
1. Functional, predicate, and constant symbols, used as names for the functions, predi-
cates, and constants we consider in the structure. All these are referred to as non-logical
symbols.
I emphasize that the non-logical symbols are just names for functions, predicates, and
constants; in principle, we should use different symbols to denote objects and their
names. Later on I introduce a generic notation that will take care of that, but mean-
while I allow a little sloppiness and use the same symbols for functions, predicates,
and constants in our example structures and for their names in the first-order languages
designed for them.
2. Individual variables. Quite often, particularly in mathematics, we deal with unknown
or unspecified objects (individuals) from the domain of discourse. In order to be able to
reason and make statements about such objects, we use individual variables to denote
them. For instance, talking about numbers, we use phrases such as “Take a positive
integer n”, “Every real number x greater than
√
2 . . . ”, etc. In natural language
instead of variables we usually use pronouns or other syntactic constructions but that
often leads to awkwardness and ambiguity (e.g. “If a man owes money to another
man then that man hates the other man”). The use of variables is therefore indis-
pensable, not only in mathematics, and it is very important to learn how to use them
properly.
We also use variables as placeholders for the arguments of the various predicates and
functions we deal with. We can therefore talk about the (unary) functions f(x) and
“the mother of z”, or the (binary) predicates P (x, y) and “x loves y”, etc.
We will assume that any first-order language contains an infinite set of individual vari-
ables, denoted VAR. The letters u, v, w, x, y, z, possibly indexed, are used to denote
individual variables.
3. Auxiliary symbols, such as “(”, “,”, “)” etc.
4. Last but most important, logical symbols, including:
(a) Propositional connectives, which we already know: ¬, ∧, ∨, →, ↔; and
(b) Quantifiers. Often we use special phrases to quantify objects of our discourse,
such as:
“(for) every (objects) x . . . ”,
“there is (an object) x such that . . . ”,
“for most (objects) x . . . ”,
“there are at least 5 (objects) x such that . . . ”,
“there are more (objects) x than y such that . . . ”,
etc. (Note the use of variables here.)

100
Logic as a Tool
The first two quantifiers are particularly important and many others can be expressed
by means of them, so they are given special names and notation:
The quantifier “for every” is called the universal quantifier, denoted ∀.
The quantifier “there exists” is called existential quantifier, denoted ∃.
These are the only quantifiers used in first-order languages, so they are called
first-order quantifiers. The term “first-order” refers to the fact that variables and
quantifications in these languages are only permitted to range over individuals in the
universe of discourse, called “first-order objects”4.
As well as the phrases above, the universal quantifier is usually represented by “all”,
“for all”, and “every”, while the existential quantifier can appear as “there is”, “a”, some”,
and “for some”, particularly in a non-mathematical discourse.
Sometimes, recognizing the correct quantification in natural language can be quite
tricky, or even confusing. Take for example: “a dog ate my homework”, meaning “some
dog ate my homework” (existential quantification) v. “a dog is an animal”, meaning
“every dog is an animal” (universal quantification). Alternatively, the well-known
expression5 “All that glitters is not gold” is actually meant to mean “Not all that
glitters is gold”, something logically quite different, as we will see in Section 3.4.
So, watch out!
3.1.3
Terms and formulae
Using the symbols in a given first-order language and following certain common syntac-
tic rules, we can compose formal expressions which allow us to symbolically represent
statements, to reason about them, and to prove them in a precise, well-structured, and
logically correct way. There are two basic syntactic categories in a first-order language:
(first-order) terms and (first-order) formulae.
3.1.3.1
Terms
Terms are formal expressions (think of algebraic expressions) built from constant symbols
and individual variables, using functional symbols. Terms are used to denote specified or
unspecified individuals, that is, elements of the domain.
Here is the formal inductive definition of terms in a first-order language L.
Definition 79 (Terms) Let L be any first-order language.
1. Every constant symbol in L is a term in L.
2. Every individual variable in L is a term in L.
4 First-order logic can be extended to second-order logic, where there are second-order variables and quantifiers
ranging over sets, relations, and functions; then further to third-order logic with variables and quantifiers over more
complex objects definable in second-order logic, etc. In this book we will not go beyond first-order logic.
5 From Shakespeare’s play The Merchant of Venice.

Understanding First-order Logic
101
3. If t1, . . . , tn are terms in L and f is an n-ary functional symbol in L, then
f(t1, . . . , tn) is a term in L.
We denote the set of terms of L by TM(L).
The set of variables occurring in a term t is denoted by VAR(t).
Terms that do not contain variables are called constant terms or ground terms.
Example 80 Some examples of terms in the first-order languages for some of the struc-
tures we have seen include the following.
1. In the language LN :
First, 0, s(0), s(s(0)), etc. are constant terms. The term s( . . . s(0) . . . ), where s
occurs n times, is denoted n. That term is called the numeral for n6.
We are less formal from this point onward, and allow ourselves to use the common,
infix notation, as well as omitting outermost parentheses in terms whenever that does
not affect the correct reading. With that in mind, other examples of terms include:
• +(2, 2), which in a more familiar notation is written as 2 + 2;
• ×(3, y), written in the usual notation as 3 × y;
• (x2 + x) × 5, where x2 is an abbreviation of x × x;
• x1 + s((y2 + 3)×s(z)), etc.
2. In the ‘human’ language LH:
• Mary;
• x;
• m(John) (meant to denote “the mother of John”);
• f(m(y)) (meant to denote “the father of the mother of y”).
The inductive definition of terms generates the respective principle of induction fol-
lowing the general scheme presented in Section 1.4.
Proposition 81 (Induction on terms) Let L be any first-order language and P a prop-
erty of terms in L, such that:
1. every constant symbol in L has the property P;
2. every individual variable in L has the property P;
3. if t1, . . . , tn are terms in L that have the property P and f is an n-ary functional
symbol in L, then the term f(t1, . . . , tn) has the property P.
Then every term in L has the property P.
Let us now illustrate definitions by recursion (recall Section 1.4) on the inductive defi-
nition of terms by formally defining for every term t the set of variables VAR(t) occurring
in that term.
6 The term n, which is a syntactic object, that is, just a string of symbols, must be distinguished from the number n,
which is a mathematical entity.

102
Logic as a Tool
Definition 82 (The set of variables in a term) Let L be any first-order language. We
define a mapping VAR : TM(L) →P(VAR) recursively on the structure of terms in L as
follows.
1. If t is a constant symbol in L, then VAR(t) = ∅.
2. If t is the individual variable x in L, in L, then VAR(t) = {x}.
3. If t = f(t1, . . . , tn), where t1, . . . , tn are terms in L and f is an n-ary functional
symbol in L, then VAR(t) = VAR(t1) ∪. . . ∪VAR(tn).
With every term we can associate a construction tree and a parsing tree in the same
way as for propositional formulae. For instance, here is the parsing tree of the term t =
+(×(x, +(1, y)), ×(s(s(x)), y)) (recall that 1 = s(0)):
+
×
x
+
y
s
0
×
y
s
s
x
Respectively, for every term t we define the set sub(t) of subterms being all terms
used in the construction of that term, that is, all terms with construction trees rooted
as subtrees at nodes of the construction tree of t. The definition is by recursion on the
inductive definition of terms. For instance, for the term t in the example above, sub(t) =
{t, ×(x, +(1, y)), x, +(1, y), 1, 0, y, ×(s(s(x)), y), s(s(x)), s(x)}.
3.1.3.2
Atomic formulae
By applying predicate symbols to terms we can build atomic formulae, the simplest
first-order formulae. They have no internal logical structure and correspond to atomic
propositions in propositional logic. They are formally defined as follows.
Definition 83 (Atomic formulae) Let L be any first-order language. An atomic formula
in L is any string p(t1, . . . , tn) where t1, . . . , tn are terms in L and p is an n-ary pred-
icate symbol in L.
We denote the set of atomic formulae of L by AFOR(L).
Example 84 Some examples of atomic formulae include the following:
1. In LN :
• < (1 , 2), or in traditional notation: 1 < 2;
• x = 2;
• 5 < (x + 4);

Understanding First-order Logic
103
• 2 + s(x1) = s(s(x2));
• (x2 + x) × 5 = 0;
• x × (y + z) = x × y + x × z, etc.
2. In LH:
• x = m(Mary) (meant to say “x is the mother of Mary”).
• L(f(y), y) (meant to say “the father of y loves y”).
3.1.3.3
Formulae
Finally, using atomic formulae and logical connectives we can build (compound) formu-
lae like in propositional logic, but now we can also use quantifiers.
The inductive definition of formulae of a first-order language L naturally extends the
definition of propositional formulae as follows.
Definition 85 (Formulae) Let L be any first-order language.
1. Every atomic formula in L is a formula in L.
2. If A is a formula in L then ¬A is a formula in L.
3. If A and B are formulae in L then each of (A ∨B), (A ∧B), (A →B), and (A ↔
B) is a formula in L.
4. If A is a formula in L and x is an individual variable, then each of ∀xA and ∃xA is
a formula in L.
We denote the set of formulae of L by FOR(L).
The set of variables occurring in a formula A is denoted VAR(A).
Formulae that do not contain variables are called ground formulae.
Example 86 Some examples of formulae include the following.
1. In LZ:
• (5 < x ∧x2 + x −2 = 0);
• ∃x(5 < x ∧x2 + x −2 = 0);
• ∀x(5 < x ∧x2 + x −2 = 0);
• (∃y(x = y2) →(¬x < 0));
• ∀x(∃y(x = y2) →(¬x < 0)).
2. In LH:
• John = f(Mary) →∃xL(x, Mary);
• ∃x∀z(¬L(z, y) →L(x, z));
• ∀x(∃y(x = m(y)) →∃z(L(z, x))).
Think about the intended meaning of each of these formulae.
As in propositional logic, we will allow ourselves to omit parentheses wherever possible
without resulting in ambiguity. For that purpose we will, again, impose a priority order
among the logical connectives:

104
Logic as a Tool
• the unary connectives – negation and quantifiers – have the strongest binding power,
that is, the highest priority;
• then come the conjunction and disjunction;
• then the implication; and
• the biconditional has the lowest priority.
Example:
∀x((∃y(x = y2)) →((¬(x < 0)) ∨(x = 0)))
can be simplified to
∀x(∃y(x = y2) →¬x < 0 ∨x = 0).
On the other hand, sometimes we can use redundant parentheses in order to improve
the readability.
With every first-order formula we can associate a construction tree and a parsing tree
in the same way as for propositional formulae. The only differences are that the leaves
of the construction/parsing tree of a formula are labeled with atomic formulae rather than
propositional variables, and that internal nodes can also be labeled with pairs ⟨quantifier,
individual variable⟩, such as ∀x or ∃x, and these nodes have single successor nodes.
For every first-order formula A we define its main connective as the connective label-
ing the root of the construction/parsing tree of the formula, and the set sub(A) of subfor-
mulae of A being all formulae with construction trees rooted as subtrees at nodes of the
construction tree of A.
The inductive definition of first-order formulae generates the respective principle of
induction, following the general scheme presented in Section 1.4.
Proposition 87 (Induction on formulae) Let L be any first-order language and P a
property of formulae in L, such that:
1. every atomic formula in L has the property P;
2. if A is a formula in L that has the property P, then ¬A has the property P;
3. if A and B are formulae in L that have the property P, then each of (A ∨B), (A ∧B),
(A →B), and (A ↔B) has the property P;
4. if A is a formula in L that has the property P and x is an individual variable, then
each of ∀xA and ∃xA has the property P.
Then every formula in FOR(L) has the property P.
3.1.3.4
Unique readability of terms and formulae
Natural languages do not provide a reliable medium for precise reasoning because they are
ambiguous: the same phrase or sentence may have several different – yet grammatically
correct – readings, and therefore several different meanings. Eliminating ambiguity is one
of the main reasons for using formal logical languages instead. In particular, the first-order
languages introduced here are unambiguous in their formal syntax. In particular, it can be
proved (although the proof is long and somewhat tedious) that the terms and formulae

Understanding First-order Logic
105
of any first-order language L have the unique readability property that every term or
formula has an essentially unique construction tree, respectively parsing tree, up to the
order of listing of the successor nodes. More precisely, for any first-order language L the
following hold.
1. Every occurrence of a functional symbol in a term t from TM(L) is the beginning of
a unique subterm of t.
2. Every occurrence of a predicate symbol ¬, ∃, or ∀in a formula A from FOR(L) is
the beginning of a unique subformula of A.
3. Every occurrence of any binary connective ◦∈{∧, ∨, →, ↔} in a formula A from
FOR(L) is in a context (B1 ◦B2) for a unique pair of subformulae B1 and B2 of A.
3.1.3.5
First-order instances of propositional formulae
Definition 88 Given a propositional formula A, any uniform substitution of first-order
formulae for the propositional variables in A produces a first-order formula, called a
first-order instance of A.
For example, substituting (5 < x) for p and ∃y(x = y2) for q in the propositional
formula
(p ∧¬q) →(q ∨p)
produces its first-order instance
((5 < x) ∧¬∃y(x = y2)) →(∃y(x = y2) ∨(5 < x)).
3.1.3.6
Many-sorted first-order structures and languages
Often the domain of discourse involves different sorts of objects, for example: integers
and reals; scalars and vectors; or points, lines, triangles, and circles.
The notions of first-order structures and languages can be extended naturally to
many-sorted structures and languages, with suitable inter-sort and cross-sort functions
and predicates. An example is a two-sorted language for a simple geometric reasoning in
the plane involving two sorts, one for points and another for lines. Each sort has its own
domain and these domains are disjoint (no point is a line). The only non-logical symbol
is a binary relation of incidence between a point and a line.
Using several sorts for individuals comes with a technical overhead, as it requires dif-
ferent sorts of individual variables and various syntactic restrictions. Instead, in this book
we will use unary predicates to identify the different sorts within a universal domain.
References for further reading
For many more examples (mostly mathematical) and further discussion of first-order
structures and languages, see Tarski (1965), Kalish and Montague (1980), van Dalen
(1983), Hamilton (1988), Ebbinghaus et al. (1996), Barwise and Echemendy (1999),
Enderton (2001), Hodges (2001), Hedman (2004), Nederpelt and Kamareddine (2004),
Bornat (2005), Chiswell and Hodges (2007), Ben-Ari (2012), and van Benthem et al.
(2014).

106
Logic as a Tool
Exercises
3.1.1
Define suitable first-order structures and introduce appropriate first-order lan-
guages for the following types of mathematical structures: graphs, groups, rings,
matrices, and vector spaces.
3.1.2
Define suitable first-order structures and introduce appropriate first-order lan-
guages for the following data types: Booleans, strings, lists, and trees.
3.1.3
Which of the following are syntactically correct terms in the language LN ?
Re-write them in the usual infix notation.
(a) ×(x, y)
(b) ×(2, x, y)
(c) +xy
(d) +(5, ×(x, +(3, y))
(e) +(×(x, +(3, y)), ×(+(3, y), x))
(f) ×(s(×(2, x), +(3, y)))
(g) s(×(×(2, x), +(3, s)))
(h) s(×(×(2, x), +(3, y)))
(i) ×(s(×(2, x)), s(+(s(3), y)))
(j) s(×(s(s(x)), s(s(s(2)))), s(2))
3.1.4
Construct the parsing tree of each of the syntactically correct terms from the
previous exercise and determine the set of its subterms.
3.1.5∗Prove that every occurrence of a functional symbol in a term t is the beginning
of a unique subterm of t. (Hint: use induction on the definition of terms and the
fact that a subterm of a subterm is a subterm.)
3.1.6
Which of the following are syntactically correct formulae in the language LN ?
Assume that terms are written in the usual infix notation and parentheses can be
omitted whenever possible or added for improved readability.
(a) ¬∀x(x = 0)
(b) ∀¬x(x = 0)
(c) ∀x(¬x = 0)
(d) ∀x(x¬ = 0)
(e) ∀x(x = ¬0)
(f) ¬∀x(¬x = 0)
(g) ¬∃x(¬x = 0.5)
(h) ∀x(x ∨¬x)
(i) ∀x∀y((x + y) →(y + x))
(j) ∀x((x < 0) →∀x(x > 0))
(k) ∀x((x + 0) →∀x(x > 0))
(l) ∀x(∀x(x > 0) →x < 0)
(m) ∀y(x < x →∀x(x < x))
(n) ∀y((y < y)¬∀x(x < x))
3.1.7
Construct the parsing tree of each of the syntactically correct formulae from
the previous exercise and determine its main connective and the set of its
subformulae.
3.1.8∗Prove that every occurrence of a predicate symbol, ¬, ∃, or ∀, in a formula A from
FOR(L) is the beginning of a unique subformula of A. (Hint: use induction on
the definition of formulae and the fact that a subformula of a subformula is a
subformula.)
3.1.9∗Prove that every occurrence of a binary connective ◦∈{∧, ∨, →, ↔} in a for-
mula A is in a context (B1 ◦B2) for a unique pair of subformulae B1 and B2 of
A. (Hint: use induction on the definition of the formula A.)

Understanding First-order Logic
107
3.1.10
Determine the first-order instances of the propositional formula
A = (¬p →(q ∨¬(p ∧q)))
where:
(a) ¬(f(x) = y) is substituted for p and ∃x(x > y) is substituted for q; and
(b) ¬(x = y) is substituted both for p and q.
3.1.11
Is the formula (¬(x = y) →(∃x(x > y) ∨¬((y = x) ∧∃x(x > y)))) a first-
order instance of the propositional formula A from the previous exercise?
Friedrich Ludwig Gottlob Frege (8.11.1848–26.7.1925)
was a German mathematician, logician, and philosopher who
conducted groundbreaking studies on philosophy of math-
ematics, logic, and language. He is one of the founders of
modern first-order logic, where his contribution is regarded
by many as the most important development in logic since
Aristotle.
Frege’s first major scientific work, which was also the
most influential in the future development of mathematical
logic, was his book Begriffsschrift, eine der Arithmetischen
Nachgebildete Formelsprache des reinen Denkens (Concept
Notation, a Formal Language for Pure Thought, Modeled
on that of Arithmetic) published in 1879. In that book he systematically developed
a system of logic with rather unusual notation which is very close in spirit to
modern-day first-order logic (also known as “predicate calculus”). He explicitly
introduced quantifiers, quantification, and logical formulae, and formalized the
notion of a “proof” in a way that is still used today. Frege had realized that natural
everyday language is often imprecise and ambiguous, and therefore unreliable
for performing rigorous mathematical reasoning. He wanted to develop a precise,
unambiguous language for expressing mathematical statements and a formal logical
system for mathematical reasoning. Frege had a logicistic view, that the whole of
mathematics should be reducible to logic. He stated in the preface to Begriffsschrift
that he wanted to prove all basic results in arithmetic “by means of pure logic.” He
pursued this idea further in his next major work, The Foundations of Arithmetic,
published in 1884, and made huge achievements in formalizing much arithmetic in
purely logical terms.
The reaction to Frege’s work by the mathematicians and philosophers of his time
was disappointingly weak and shallow, as very few of them were able to understand
his groundbreaking ideas. Particularly devastating for Frege was the criticism of Can-
tor, even though Cantor’s views on the foundations of mathematics were very close
to Frege’s philosophical views. Nevertheless, Frege went on developing and elab-
orating his philosophy of logic and mathematics and was about to publish the 2nd
volume of The Foundations of Arithmetic when, in 1902, he received a letter from

108
Logic as a Tool
the young philosopher and mathematician Bertrand Russell. Russell pointed out a
contradiction in Frege’s system of axioms, revealed by Russell’s paradoxical set of
all sets that are not elements of themselves. This was a heavy blow to Frege, who
attempted (as it turned out later, unsuccessfully) to fix the problem in an appendix to
his book by amending one of his axioms, but he was so strongly shaken by that dis-
covery that he stopped working on that project and never published the planned 3rd
volume. Frege never succeeded in reducing the entire mathematics to logic. Much
later, in the early 1920s, he came to the conclusion that this goal was impossible and
decided, instead, that the whole of mathematics should be based on geometry, but
never managed to develop his ideas until the end of his life.
Frege strongly influenced the views and work of many prominent followers,
including Peano, Wittgenstein, Husserl, Carnap, and Russell. After his death
his revolutionary works in the philosophy of mathematics, logic, and language
gradually received due recognition and have since been regarded as being of major
scientific importance.
3.2
Semantics of first-order logic
We now consider an arbitrarily fixed first-order language L. The formulae of L are meant
to express statements about structures “matching” that language L. The meaning of the
formulae is relative to the given structure of discourse and is “computed” composition-
ally following the structure of the formula, the values of the occurring variables, and the
meaning of the logical and non-logical symbols occurring in it. The precise meaning of
logical formulae is determined by their formal semantics, described in this section.
3.2.1
The semantics of first-order logic: an informal outline
In order to determine the meaning of the statement expressed by a given formula of
our first-order language L we first need to fix its interpretation. It is a first-order
structure – the structure of discourse – that corresponds to the language L, in a sense
that all distinguished functions, predicates, and constants in the structure should have
names in L, that is, respective functional, predicate, and constant symbols. Conversely,
all non-logical symbols of the language L should correspond to explicitly defined
functions, predicates, and constants in the structure.
Next, we need to assign values to the individual variables of the language, or at least
to those occurring in the formula (but only to those that are not quantified over). These
values are elements of the structure of discourse, so every variable refers to the element
which is its value.
As a running example, consider the following formula from the language LN of arith-
metic: ∃x(¬(x < y + z) ∧(z × x < (x × (y + 1))) →(x = z + 2)). (What it says is
not really important for our purposes.)
We consider the intended interpretation, namely the structure N with the standard
meanings of <, +, ×, 1, 2. Take an assignment of values in N to the variables as follows:
x = 5, y = 3, z = 2 (we are not interested in the values of the other variables).

Understanding First-order Logic
109
Once the structure of discourse is fixed and the individual variables are assigned values
in that structure, the value of every term in L can be computed step-by-step following
the structure of the term, in the same way as evaluating arithmetic expressions using the
addition and multiplication tables. Eventually, every term in L is evaluated as a unique
element of the structure, and that element is its value.
In our example, the terms occurring in the formula are evaluated as follows: y + z = 5;
z × x = 10; x × (y + 1) = 20; and z + 2 = 4.
We are now ready to “compute” the meaning of every atomic formula: all we need
to do is apply the predicate in the formula to the values of the terms appearing as its
arguments. That meaning is a truth value – true or false – just as for propositional
logic. Once these truth values are computed, we can treat atomic formulae in the same
way as propositional variables.
In our example, the atomic subformulae of the formula acquire truth values as follows:
x < (y + z) becomes 5 < 5, the truth value of which is false; z × x < (x × y + 1)
becomes 10 < 20, which has truth value true; and x = (z + 2) becomes 5 = 4, which
has truth value false.
Once we have computed the truth values of the atomic formulae, we can compute the
truth value of any first-order formula following the structure of that formula, in the same
way as computing truth values of propositional formulae; the only difference is that we
also now have to deal with the quantifiers.
Let us compute the truth values of the subformulae in our example. Since x < (y + z) is
false for the given assignment of values to the variables, we find that ¬(x < y + z) is true.
Likewise, since z × x < (x × (y + 1)) is true for that assignment, we find that ¬(x <
y + z) ∧(z × x < (x × (y + 1))) is true. Hence, since x = z + 2 is false, ¬(x < y +
z) ∧(z × x < (x × (y + 1))) →(x = z + 2) is false.
We now come to the most essential new step: the quantifiers. This is where computing
the meaning, that is, the truth value, of a formula becomes generally difficult. Concep-
tually, there is no problem as we understand quite well what the quantifiers intuitively
mean:
• for the existential quantifier: ∃xA is true if there is a possible value of the variable x in
the given structure of discourse, that is, there is an element of that structure assigned as
a value to x that renders the formula A true; and
• for the universal quantifier: ∀xA is true if every possible value of the variable x in the
given structure of discourse, that is, any element of that structure assigned as a value to
x renders the formula A true.
In the long run, we have informally defined the notion of a formula A true in a structure
S under a variable assignment v, denoted
S, v |= A.
Well, that sounds simple and, in a sense, it is simple. The difficulty is only technical:
the structure may have many (possibly infinitely many) elements, and checking that all
of them (respectively, some of them) make the formula A true can be a long (possibly
infinitely long) and hard task. In practice, we therefore guess a suitable value of x in the

110
Logic as a Tool
case of existential quantification, or come up with a uniform argument that works for all
possible values of x in the case of universal quantification.
In order to determine the truth value of the entire formula in our example, we need to
find out whether there is any natural number that, assigned as a value to x while y and
z keep the same values as before, will render the subformula ¬(x < y + z) ∧(z × x <
(x × (y + 1))) →(x = z + 2) true. In this case it is not difficult to guess such a value; it
is sufficient to find one that makes the antecedent of the implication false. For that, we can
take any value that makes ¬(x < y + z) false, that is, x < y + z true, for instance x = 0.
The formula ∃x(¬(x < y + z) ∧(z × x < (x × (y + 1))) →(x = z + 2)) is therefore
true in N.
Let us now change the existential quantifier into a universal quantifier. Is the formula
∀x(¬(x < y + z) ∧(z × x < (x × (y + 1))) →(x = z + 2)) still true in N? To deter-
mine that, in principle we have to consider all natural numbers as possible values of x and
determine the truth value of ¬(x < y + z) ∧(z × x < (x × (y + 1))) →(x = z + 2)
for each of them. However, this is not necessary in our case since we already know a
value of x that makes that subformula false; we therefore find that ∀x(¬(x < y + z) ∧
(z × x < (x × (y + 1))) →(x = z + 2)) is false in N.
What if the existentially quantified formula was false or the universally quantified for-
mula was true, however? Would we have to perform the infinitely many checks then? Usu-
ally a uniform argument can be found that proves the case. For instance, we can prove that
∀x((x + 1)2 = x2 + 2x + 1) by straightforward calculations, or that ∃x(x2 + x + 1 = 0)
is false by using some high-school algebra.
In general, however, this question is much deeper than it looks, so I will leave it there.
I will only mention that it was proved in 1973 (by the collective effort of several logicians
and mathematicians, notably Martin Davis, Yuri Matiyasevich, Hilary Putnam, and Julia
Robinson) that there is no general algorithm that can answer such questions, even when the
formula is a very simple one of the type ∃x(P(x) = 0) where P(x) is a polynomial with
integer coefficients7. The truth of some formulae in the language LN or simple extensions
of it has been settled with a formal proof only after hundreds of years of futile attempts of
many great minds. One famous example is Fermat’s Last Theorem, which can be stated
by a simple one-line formula using exponentiation in N:
∀x∀y∀z∀n((x ̸= 0 ∧y ̸= 0 ∧z ̸= 0 ∧n > 2) →xn + yn ̸= zn).
The truth of other such formulae is still unsettled after centuries of attempts, for example
the famous Goldbach conjecture which states that “every even integer greater than 2
equals the sum of two prime natural numbers”8. Formalizing this statement in LN is
given later as an exercise. Although we (believe to) understand very well what this state-
ment says, “computing” its formal “meaning”, that is, its truth value, appears to be a
formidable task.
This is the end of the quick and informal exposition. We now embark on the more
detailed and formal version.
7 This was a negative answer to Hilbert’s Tenth Problem, one of the 23 famous open challenges to the mathematics of
the 20th century posed by David Hilbert at the World Congress of Mathematics in 1900.
8 A positive integer greater than 1 is prime if it has no other positive integer divisors but 1 and itself.

Understanding First-order Logic
111
3.2.2
Interpretations of first-order languages
An interpretation of a first-order language L is a matching first-order structure S, that is,
a structure with a family of distinguished functions, predicates, and constants that cor-
respond to (and match the respective numbers of arguments of) the non-logical symbols
in L.
Some first-order languages, like all those that we have considered so far, are designed
for specific structures which are their intended or standard interpretations. Other
first-order languages are designed for classes of structures. For instance, the first-order
language containing one binary relational symbol R (plus equality) can be regarded as
the language of directed graphs, where the intended interpretation of R in any directed
graph is the edge relation in that graph. Likewise, the first-order language of (algebraic)
groups contains the following non-logical symbols: one binary functional symbol ◦,
with intended interpretation being the group operation; one unary functional symbol ′,
with intended interpretation being the inverse operation; and one constant symbol e, with
intended interpretation the identity element.
Note, however, that every first-order language may have many unintended interpre-
tations. For instance, the language for directed graphs can be interpreted in the domain
of integers, with R interpreted as “divisible by”, or in the domain of humans, with
R interpreted as “is a friend of.” Indeed, most of the unintended interpretations are
practically meaningless. For instance, the language LH can be interpreted in the domain
of integers where, for example, the functional symbol m is interpreted as “m(n) = 2n”,
f is interpreted as “f(n) = n5 −1”, the unary predicate M is interpreted as “is prime”,
and the unary predicate W is interpreted as “is greater than 2012”, the binary predicate
symbols P, C, and L are interpreted respectively as “is greater than”, “is divisible by”,
and “has the same remainder modulo 11”, and the constant symbols John, Mary, Adam,
Eve are interpreted respectively as the numbers −17, 99, 0, and 10. Of course, such
unintended interpretations are not interesting, but they must be taken in consideration
when judging whether a given first-order formula is logically valid, that is, true in
every possible interpretation. That will be discussed later in Section 3.4 however, and
we now come back to the meaning of first-order terms and formulae under a given
interpretation.
Once a given first-order language L is interpreted, that is, a matching first-order
structure S is fixed, the value in S of every term t from TM(L) can be “computed” as
soon as all individual variables occurring in t are assigned values in S, that is, elements
of S. The meaning of every formula A in FOR(L) can also then be “computed”, just
as for propositional logic, from the values of the terms and the interpretation of the
predicate symbols occurring in A and the standard meaning of the logical connectives.
The rules for computing this meaning determine the semantics of first-order logic.
I will spell out these rules without going into more technical detail than is really
necessary.

112
Logic as a Tool
3.2.3
Variable assignment and evaluation of terms
As I discussed above, in order to compute the truth value of a formula in a given structure
S we first have to evaluate the terms occurring in it, that is, determine the elements of S
denoted by these terms. For that, we must first assign values in S to the individual variables
by means of a variable assignment in S, which is a mapping v : VAR →|S| from the
set of individual variables VAR to the domain of S.
The evaluation of a term is now done as for the evaluation of an algebraic expression
that we know from primary school: starting with the values of the variables and constant
symbols we systematically apply the functions in S which interpret the respective func-
tional symbols occurring in the term. That is, when evaluating a term f(u1, . . . , um),
we first compute recursively the values of the arguments u1, . . . , um and then apply the
interpretation of f in S to these values.
Formally, due to the unique readability of terms, every variable assignment v : VAR →
|S| in a structure S can be uniquely extended to a mapping vS : TM(L) →|S|, called
term evaluation, such that for every n-tuple of terms t1, . . . , tn and an n-ary functional
symbol f:
vS(f(t1, . . . , tn)) = f S(vS(t1), . . . , vS(tn))
where f S is the interpretation of f in S.
In this way, once a variable assignment v in the structure S is fixed, every term t in
TM(L) is evaluated as a unique element vS(t) of S (or just v(t) when S is fixed), called
the value of the term t for the variable assignment v.
The following proposition essentially says that the value of any term only depends on
the assignment of values to the variables occurring in that term. We leave the proof as an
easy exercise.
Proposition 89 For any given first-order language L, term t ∈TM(L), and an
L-structure S, if v1, v2 are variable assignments in S that assign the same values to all
variables in VAR(t), then vS
1 (t) = vS
2 (t).
Example 90
• Let v be a variable assignment in the structure N such that v(x) = 3 and v(y) = 5.
Here is a step-by-step computation of the value of the term s(s(x) × y):
vN(s(s(x) × y))= sN (vN (s(x) × y)) = sN (vN(s(x))×NvN(y)) = sN (sN(vN (x))
×NvN (y)) = sN (sN (3) × N 5) = sN ((3 + 1)×N 5) = ((3 + 1) × 5) + 1 = 21.
• Likewise, vN (1 + (x × s(s(2)))) = 13.
• If v(x) = “Mary” then vH(f(m(x))) = “the father of the mother of Mary”.
3.2.4
Truth of first-order formulae
Eventually, we want to formally define the notion of a formula A being true in a struc-
ture S for a variable assignment v, denoted
S, v |= A.

Understanding First-order Logic
113
The definition will be recursive, following the inductively defined structure of the
formula A.
3.2.4.1
Atomic formulae
We begin with the simplest case, where A is an atomic formula. The truth value of the
atomic formula p(t1, . . . , tn) is determined by the interpretation of the predicate symbol
p in S, applied to the tuple of arguments vS(t1), . . . , vS(tn):
S, v |= p(t1, . . . , tn) iff pS holds true for vS(t1), . . . , vS(tn).
If S, v |= p(t1, . . . , tn) does not hold, we write S, v ⊭p(t1, . . . , tn).
Example 91 If L is a binary predicate symbol interpreted in N as “less than”, and the
variables x and y are assigned values as above, then:
• N, v |= L(1 + (x × s(s(2))), s(s(x) × y)) iff
LN ((1 + (x × s(s(2))))N, (s(s(x) × y))N )
iff 13 < 21, which is true.
• likewise, N, v |= 8 × (x + s(s(y))) = (s(x) + y) × (x + s(y)) iff
(8 × (x + s(s(y))))N = ((s(x) + y) × (x + s(y)))N
iff 80 = 81, which is false.
3.2.4.2
Propositional connectives
The truth values propagate over the propositional connectives according to their truth
tables, as in propositional logic:
• S, v |= ¬A iff S, v ⊭A;
• S, v |= (A ∧B) iff S, v |= A and S, v |= B;
• S, v |= (A ∨B) iff S, v |= A or S, v |= B;
• S, v |= (A →B) iff S, v ⊭A or S, v |= B;
• S, v |= (A ↔B) iff (S, v ⊭A if and only if S, v |= B).
3.2.4.3
Quantifiers
Finally, the truth of formulae ∀xA(x) and ∃xA(x) is computed according to the meaning
of the quantifiers and the truth values of A. We first define a technical notion. Given a
structure S and two variable assignments v and v′ in S, we say that v′ is an x-variant of
v if v′ coincides with v on every variable except possibly x. Equivalently, v′ is an x-variant
of v if there exists an element a ∈S such that v′ = v[x := a], where v[x := a] is obtained
from v by redefining v(x) to be a.
• ∀xA(x) is true if every object a from the domain of S, assigned as a value of x, satisfies
(i.e., renders true) the formula A.
Formally, S, v |= ∀xA(x) if S, v′ |= A(x) for every variable assignment v′ that is an
x-variant of v.

114
Logic as a Tool
• ∃xA(x) is true if there is an object a from the domain of S which, assigned as a value
of x, satisfies the formula A.
Formally, S, v |= ∀xA(x) if S, v′ |= A(x) for some variable assignment v′ that is an
x-variant of v.
3.2.4.4
Computing the truth of first-order formulae
We can now (at least theoretically) compute the truth of any first-order formula in a given
structure for a given variable assignment, step-by-step, following the logical structure of
the formula and applying recursively the respective truth condition for the main connective
of the currently evaluated subformula.
It is not difficult to show that the truth of a formula in a given structure for a given
variable assignment only depends on the assignment of values to the variables occurring
in that formula. That is, if we denote the set of variables occurring in the formula A by
VAR(A) and v1, v2 are variable assignments in S such that v1 |VAR(A)= v2 |VAR(A), then
S, v1 |= A iff S, v2 |= A.
Still, note that the truth conditions for the quantifiers given above are not really prac-
tically applicable when the structure is infinite because they require taking into account
infinitely many variable assignments. Evaluating the truth of a first-order formula in an
infinite structure is, generally speaking, an infinite procedure. Nevertheless, we can often
perform that infinite procedure as finite by applying uniform (yet ad hoc) arguments to
the infinitely many arising cases.
Example 92 Consider the structure N and a variable assignment v such that v(x) = 0,
v(y) = 1, and v(z) = 2. The following then holds.
• N, v |= ¬(x > y).
• However, N, v |= ∃x(x > y). Indeed, N, v[x := 2] |= x > y.
• In fact, N, v |= ∃x(x > y) holds for any assignment of value to y, and therefore
N, v |= ∀y∃x(x > y).
• On the other hand, N, v |= ∃x(x < y), but N, v ⊭∀y∃x(x < y). Why?
• What about N , v |= ∃x(x > y ∧z > x)? This is false; there is no natural number
between 1 and 2.
• However, for the same variable assignment in the structure of rationals Q, we have that
Q, v |= ∃x(x > y ∧z > x).
Does this hold for every variable assignment in Q?
3.2.5
Evaluation games
There is an equivalent, but somewhat more intuitive and possibly more entertaining, way
to evaluate the truth of first-order formulae in given structures. This is done by playing

Understanding First-order Logic
115
a special kind of a two-player game, called (formula) evaluation game9. These games
go back to Lorenzen’s work in the 1950s (if not much earlier), but were first introduced
explicitly for first-order logic by Henkin and Hintikka.
The two players are called the Verifier and the Falsifier10.
The game is played on a given first-order structure S, containing a variable assignment
v, and a formula A, the truth of which is to be evaluated in the structure S for the assign-
ment v. As suggested by the names of the players, the objective of Verifier is to defend
and demonstrate the claim that S, v |= A, while the objective of Falsifier is to attack and
refute that claim.
The game goes in rounds and in each round exactly one of the players, depending on
the current “game configuration”, has to make a move according to rules specified below
until the game ends. The current game configuration (S, w, C) consists of the structure
S, an assignment w in S, and a formula C (the truth of which is to be evaluated in S for
the assignment w). The initial configuration is (S, v, A). We identify every such game
with its initial configuration.
At every round, the player to make a move as well as the possible move are determined
by the main connective of the formula in the current configuration (S, w, C), by rules that
closely resemble the truth definitions for the logical connectives.
The rules are as follows.
• If the formula C is atomic, the game ends.
If S, w |= C then Verifier wins, otherwise Falsifier wins.
• If C = ¬B then Verifier and Falsifier swap their roles and the game continues
with the configuration (S, w, B). Swapping the roles means that Verifier wins the
game (S, w, ¬B) iff Falsifier wins the game (S, w, B), and Falsifier wins the game
(S, w, ¬B) iff Verifier wins the game (S, w, B).
Intuition: verifying ¬B is equivalent to falsifying B and vice versa.
• If C = C1 ∧C2 then Falsifier chooses i ∈{1, 2} and the game continues with the con-
figuration (S, w, Ci).
Intuition: for Verifier to defend the truth of C1 ∧C2 he should be able to defend the
truth of any of the two conjuncts, so it is up to Falsifier to question the truth of either
of them.
• If C = C1 ∨C2 then Verifier chooses i ∈{1, 2} and the game continues with the con-
figuration (S, w, Ci).
Intuition: for Verifier to defend the truth of C1 ∨C2, it is sufficient to be able to defend
the truth of at least one of the two disjuncts; Verifier can choose which one.
• If C = C1 →C2 then Verifier chooses i ∈{1, 2} and, depending on that choice, the
game continues with the configuration (S, w, ¬C1) or (S, w, C2).
Intuition: C1 →C2 ≡¬C1 ∨C2.
• If C = ∃xB then Verifier chooses an element a ∈S and the game continues with the
configuration (S, w[x := a], B).
Intuition: by the truth definition of ∃xB, verifying that S, w |= ∃xB amounts to veri-
fying that S, w[x := a] |= B for some suitable element a ∈S.
9 Also known as model checking game.
10 Also known by various other names, for example Proponent and Opponent, Eloise and Abelard, Eve and Adam.
For the sake of convenience, and without prejudice, here we will assume that both players are male.

116
Logic as a Tool
• If C = ∀xB then Falsifier chooses an element a ∈S and the game continues with the
configuration (S, w[x := a], B).
Intuition: by the truth definition of ∀xB, falsifying S, w |= ∀xB amounts to falsifying
S, w[x := a] |= B for some suitable element a ∈S.
It is easy to see that any formula evaluation game always ends after a finite number
of steps; this is because the number of logical connectives in the formula in the current
configuration strictly decreases after every move until an atomic formula is reached. It is
also obvious from the rules that the game always ends with one of the players winning.
Clearly the winner of such a game depends not only on the truth or falsity of the claim
S, v |= A, but also on how well the players play the game; we assume they always play
a best possible move. The game will therefore be won by the player who has a winning
strategy for that game, that is, a rule that, for every possible configuration from which
that player is to move assigns such a move, that he is guaranteed to eventually win the
game, no matter how the other player plays. It is not quite obvious that one of the players
is sure to have a winning strategy in every such game, but it follows from a more general
result in game theory. That also follows from the following claim, which we state here
without proof11, relating the existence of a winning strategy to the truth of the formula in
the initial configuration.
Theorem 93 For every configuration (S, v, A):
1. S, v |= A iff Verifier has a winning strategy for the evaluation game (S, v, A).
2. S, v ⊭A iff Falsifier has a winning strategy for the evaluation game (S, v, A).
Example 94 Consider the structure N and the variable assignment v such that v(x) = 0,
v(y) = 1, and v(z) = 2.
1. Verifier has a winning strategy for the game (N, v, ∀y∃x(x > y + z)).
Indeed, the first move of the game is by Falsifier who has to choose an integer n, and
the game continues from configuration (N, v[y := n], ∃x(x > y + z)).
Now, Verifier has to choose an integer m. For every given n ∈N Verifier can choose
m = n + 3, for example. He then wins the game (N, v[y := n][x := m], (x > y +
z)) because n + 3 > n + 2. Verifier therefore has a winning strategy for the game
(N, v[y := n], ∃x(x > y + z)), for any n ∈N.
Hence, Verifier has a winning strategy for the game (N, v, ∀y∃x(x > y + z)).
Therefore, N, v |= ∀y∃x(x > y + z). In fact, the above winning strategy for Verifier
is easy to generalize for any assignment of value to z, demonstrating that N, v |=
∀z∀y∃x(x > y + z).
2. Verifier has a winning strategy for the game (N, v, ∀x(y < x ∨x < z)).
Indeed, the first move in the game is by Falsifier who has to choose an integer n, and the
game continues from configuration (N, v[x := n], (y < x ∨x < z)) in which Verifier
must choose one of the disjuncts y < x and x < z.
11 See references at the end of this section.

Understanding First-order Logic
117
The strategy for Verifier is as follows: if Falsifier has chosen n > 1 then Verifier
chooses the disjunct y < x and wins the game (N, v[x := n], y < x); if Falsifier
has chosen n ≤1 then Verifier chooses the disjunct x < z and wins the game
(N, v[x := n], x < z).
Therefore, N, v |= ∀x(y < x ∨x < z).
3. Falsifier has a winning strategy for the game (N, v, ∀x(x < z →∃y(y < x))).
Indeed, let Falsifier choose 0 in the first move. The game then continues from con-
figuration (N, v[x := 0], (x < z →∃y(y < x))) and now Verifier is to choose the
antecedent or the consequent of the implication.
If Verifier chooses the antecedent, the game continues from configuration (N, v[x :=
0], ¬(x < z)) which is won by Falsifier because the game (N, v[x := 0], x < z) is
won by Verifier (since 0 < 2). If Verifier chooses the consequent of the implication,
then the game continues from configuration (N, v[x := 0], ∃y(y < x)) and Verifier
chooses a suitable value for y. However, whatever n ∈N Verifier chooses, he loses
the game (N, v[x := 0][y := n], y < x) because n < 0 is false for every n ∈N.
Thus, Verifier has no winning move after the first move of Falsifier choosing 0 as a value
for x. Therefore N, v ⊭∀x(x < z →∃y(y < x)). Furthermore, the winning strategy
for Falsifier in the game (N, v, ∀x(x < z →∃y(y < x))) is a winning strategy for
Verifier in the game (N, v, ¬∀x(x < z →∃y(y < x))). Therefore N, v |= ¬∀x(x <
z →∃y(y < x)).
3.2.6
Translating first-order formulae to natural language
First-order formulae formalize statements about first-order structures, and these state-
ments can be translated back into natural language. A formula is of course just a string
of symbols, so a formal translation could simply consist of writing all symbols in words.
However, that would be of no use for understanding the meaning of the formula. While
evaluating the truth of the formula in a given structure formally does not require trans-
lating that formula to natural language and understanding its intuitive meaning, that is
essential for practically carrying out the truth evaluation procedure. Indeed, the meaning
of (the statement expressed by) a first-order formula in a given interpretation is closely
related to its truth in that interpretation. In fact, it could be argued that to understand the
logical meaning of a formula, spelled out in a natural language, and to determine its truth
are the two sides of the same coin. (This is not really true; recall for instance Goldbach’s
conjecture.) In any case, a sensible translation of the formula would surely help the player
who has a winning strategy in the evaluation game to come up with the right strategy, and
therefore to establish the truth of the formula in the given interpretation.
Example 95 Let us look at some examples of translating to natural language and eval-
uating the truth of first-order formulae, now interpreted in the structure of real numbers
R. Note that a good translation is usually not word-for-word, but it takes some polishing
and rephrasing in the target language so that it eventually sounds natural and makes good
sense.

118
Logic as a Tool
1.
∃x(x = x2)
—“There is a real number which equals its square.” (True, take x = 0.)
2.
∀x(x < 0 →x3 < 0)
—“Every negative real number has a negative cube.” (True.)
3.
∀x∀y(xy > 0 →(x > 0 ∨y > 0))
—“If the product of (any) two real numbers is positive, then at least one of
them is positive.” (False: take x = y = −1.)
4.
∀x(x > 0 →∃y(y2 = x))
—“Every positive real number is a square of a real number.” (True: algebraic
fact.)
5.
∃x∀y(xy < 0 →y = 0)
—“There is a real number x such that, for every real number y, if xy is negative,
then y is 0.” (True or false?)
Now, some examples of formulae in LH:
1.
John = f(Mary) ∧L(John, Mary)
—“John is the father of Mary and he loves her.”
2.
(John = f(Mary)) →∃xL(x, Mary)
—“If John is the father of Mary then (there is) someone (who) loves Mary.”
3.
∃x∀z(¬L(z, y) →L(x, z))
—“There is someone (x) who loves everyone who does not love y.”
(Note that y stands for an unspecified person here, so this is not a proposition.)

Understanding First-order Logic
119
4.
∀x(∃y(x = m(y)) →∀z(x = m(z) →L(x, z)))
—“Every mother loves all her children.”
References for further reading
For further discussion and more details on semantics of first-order logic, see Tarski (1965),
Kalish and Montague (1980), Hamilton (1988), Ebbinghaus et al. (1996), Barwise and
Echemendy (1999), Enderton (2001), Hodges (2001), Smith (2003), Hedman (2004),
Bornat (2005), Chiswell and Hodges (2007), Ben-Ari (2012), and van Benthem et al.
(2014).
For more on evaluation games, see Ebbinghaus et al. (1996), Hodges (2001), and van
Benthem et al. (2014).
Exercises
3.2.1
Evaluate the occurring terms and determine the truth of the following atomic for-
mulae in R:
(a) ×(s(s(x)), 2) = ×(x, s(s(2))), where x is assigned value 3.
(b) +(5, ×(x, +(3, y))) > s(×(3, ×(2, x))), where x is assigned the value
−1/2 and y is assigned the value −5.
(c) (s(×(×(x, x), x))) = ×(s(x), +(×(x, x), −(1, x))), for each of the follow-
ing values of x: 2,
√
2, π.
3.2.2
Show by induction on the definition of terms in any given first-order language L
that if v1, v2 are variable assignments in S such that v1 |VAR(t)= v2 |VAR(t), then
vS
1 (t) = vS
2 (t). The value of any term therefore only depends on the assignment
of values to the variables occurring in that term.
3.2.3
Show by induction on the definition of formulae in any given first-order language L
that, if v1, v2 are variable assignments in S such that v1 |VAR(A)= v2 |VAR(A), then
S, v1 |= A iff S, v2 |= A. The truth value of any formula therefore only depends
on the assignment of values to the variables occurring in that formula. (We will
see later, in Proposition 98, that not all of these variables matter.)
3.2.4
Consider the structure N and a variable assignment v such that v(x) = 1, v(y) =
2, v(z) = 3. Determine the truth in N for v of each of the following formulae,
using either the semantic definition or evaluation games. Many of these look sim-
ilar, but you will see that they say different things:
(a) z > y →x > y
(b) ∃x(z > y →x > y)
(c) ∃y(z > y →x > y)
(d) ∃z(z > y →x > y)
(e) ∀y(z > y →x > y)
(f) ∃x∀y(z > y →x > y)
(g) ∃x∀z(z > y →x > y)
(h) ∃z∀y(z > y →x > y)
(i) ∃z∀x(z > y →x > y)
(j) ∃y∀x(z > y →x > y)

120
Logic as a Tool
(k) ∃y∀z(z > y →x > y)
(l) ∀x∃y(z > y →x > y)
(m) ∀x∃z(z > y →x > y)
(n) ∀y∃x(z > y →x > y)
(o) ∀y∃z(z > y →x > y)
(p) ∀z∃x(z > y →x > y)
(q) ∀z∃y(z > y →x > y)
(r) ∃x∀y∀z(z > y →x > y)
(s) ∃y∀x∀z(z > y →x > y)
(t) ∃z∀x∀y(z > y →x > y)
(u) ∀y∃x∀z(z > y →x > y)
(v) ∀x∃y∀z(z > y →x > y)
(w) ∀z∃x∀y(z > y →x > y)
(x) ∀x∀y∃z(z > y →x > y)
(y) ∀y∀z∃x(z > y →x > y)
(z) ∀z∀x∃y(z > y →x > y)
3.2.5
Translate into English the following first-order formulae and determine which of
them represent true propositions when interpreted in R. Use either the semantic
definition or evaluation games.
(a) ¬∀x(x ̸= 0)
(b) ∀x(x3 ≥x)
(c) ∀x(x = x2 →x > 0)
(d) ∃x(x = x2 ∧x < 0)
(e) ∃x(x = x2 →x < 0)
(f) ∀x(x > 0 →x2 > x)
(g) ∀x(x = 0 ∨¬x + x = x)
(h) ∀x((x = x2 ∧x > 1) →x2 < 1)
(i) ∀x∀y(x > y ∨y > x)
(j) ∀x∃y(x > y2)
(k) ∀x∃y(x > y2 ∨y > 0)
(l) ∀x(x ≥0 →∃y(y > 0∧x = y2))
(m) ∀x∃y(x > y →x > y2)
(n) ∀x∃y(¬x = y →x > y2)
(o) ∃x∀y(x > y)
(p) ∃x∀y(x + y = x)
(q) ∃x∀y(x + y = y)
(r) ∃x∀y(x > y ∨−x > y)
(s) ∃x∀y(x > y ∨¬x > y)
(t) ∃x∀y(y > x →y2 > x)
(u) ∃x∀y(x > y →x > y2)
(v) ∃x∃y(xy = x + y)
(w) ∀x∃y∀z(xy = yz)
(x) ∃x∀y∃z((x + y)z = 1)
(y) ∀x∀y(x > y →∃z(x > z ∧z > y))
(z) ∀x∃z∀y(x > z →z > y)
3.2.6
Translate into English the following first-order sentences of the language LN and
determine for each of them whether it is true when interpreted in the structure of
the natural numbers N. Justify your answers. Use either the semantic definition
or evaluation games. (Note that this is an exercise in first-order logic, not in math-
ematics. No special knowledge of arithmetic, going beyond standard high school
curriculum, is needed.)
(a) ∃x(x > 5 →¬ x > 6)
(b) ∀x¬(x > 6 ∨¬ x > 5)
(c) ¬∃x(x > 5 ∧¬ x > 6)
(d) ∀x(¬ x > 4 →¬ x > 3)
(e) ∀x(¬ x < 7 →¬ x < 8)
(f) ∀x(((¬ x = 1) ∧(¬ x = 31)) →∀y(¬ x × y = 31))
(g) ∀x((¬ x > 31 ∧∃y(x × y = 31 ∧y < 31)) →x < 31)
(h) ∀x∃z(x < z ∧∀y(x < y →(z < y ∨z = y)))
(i) ∃x(x > 0 ∧∀z(z > x →∃y(x < y ∧y < z)))
(j) ¬∀x(x > 0 →¬∃z(z > x ∧∀y(x < y →z ≤y)))
(k) ∀x∃z(x < z ∧∀y(x < y →z < y))
(l) (∀x(∀y(y < x →P(y)) →P(x)) →∀xP(x)),
where P is an uninterpreted (arbitrary) unary predicate.

Understanding First-order Logic
121
(m) The same sentence as above, but interpreted in the structure of the integers Z.
(n) ∃xP(x) →∃x(P (x) ∧∀y(P(y) →(x < y ∨x = y))),
where P is an uninterpreted (arbitrary) unary predicate.
(o) The same sentence as above, but interpreted in the structure of the integers Z.
(p) (∃xP(x)∧∀x(P(x)→x > 0)) →∃x(P (x)∧∀y(P(y)→(x < y∨x = y))),
where P is an uninterpreted (arbitrary) unary predicate.
3.2.7
Translate into English the following first-order sentences of the language LH, and
determine their truth in the domain of human beings.
(a) ∀x((M(x) ∧∃zC(z, x)) →∀z(C(z, x) →K(x, z)))
(b) ∀x(W(x) →∀z(C(z, x) →m(z) = x))
(c) ¬∃x(W(x) ∧∀z(C(z, x) →¬K(x, z)))
(d) ∃x(W(x) ∧∀z(C(z, x) →¬m(z) = x))
(e) ∀x(M(x) →∀z(C(z, x) →f(z) = x))
(f) ∀x(∃y f(y) = x →¬∃z m(z) = x)
(g) ∀x∃y(¬P(y, x) ∧∀z(P(x, z) →∃u(P(u, z) ∧P(y, u)))),
where P(x, y) means “x is a parent of y.”
3.2.8
Formalize and justify the (propositional) reasoning in the cartoon below.

122
Logic as a Tool
Alfred Tarski (born Alfred Teitelbaum, 14.1.1901–
26.10.1983) was a Polish–American logician and mathe-
matician who made seminal contributions to many areas of
mathematics including set theory, universal algebra, topol-
ogy, geometry, universal algebra, and several branches of
mathematical logic, including theories of truth, logical con-
sequence and definability, algebraic logic, model theory,
and metamathematics (proof theory), as well as in method-
ology of science.
At age of 22 Tarski became the youngest student awarded a PhD from the Univer-
sity of Warsaw. At that time he changed his religion from Jewish to Roman Catholic
and his surname from Teitelbaum to Tarski to reaffirm his Polish identity, but also
to make it possible to obtain an academic position which, at that time, was almost
impossible for Jews in Poland. Even though Tarski did not have a university position
in Warsaw and taught mostly in high schools there until 1939, he made important
contributions to logic and set theory which brought him international recognition. In
1933 Tarski published his groundbreaking paper The concept of truth in formalised
languages where he rigorously defined formal semantics of logical languages and the
notion of truth of a logical formula. In 1936 Tarski published another seminal work,
On the concept of logical consequence, where he precisely defined logical conse-
quence to hold when the conclusion is true in every model in which all premises
are true. Yet another work of fundamental importance was A decision method for
elementary algebra and geometry, which he essentially completed in around 1930
but only published in the USA in 1948. In it Tarski showed, using the method of
quantifier elimination, that the first-order theory of the real numbers under addition
and multiplication is algorithmically decidable, one of the most important results
on decidability of a mathematical theory. In 1936 he also produced his very popu-
lar and influential undergraduate textbook on logic, Introduction to Logic and to the
Methodology of Deductive Sciences.
During Tarski’s visit to the USA for a conference in 1939, the Nazi army invaded
Poland and World War II began; Tarski could not return so remained in the USA,
where he became a naturalized citizen in 1945. Since 1949 until his retirement in
1971 Tarski was a Professor of Mathematics at the University of California at Berke-
ley, where he organized his famous logic seminar and created a very strong research
school. Famous for his extreme productivity and precision, both in his teaching and
his scientific writings, he was also an energetic and charismatic teacher whose lec-
tures were meticulously prepared and presented. Tarski was also an inspiring and
demanding supervisor of 26 doctoral students, many of whom also became distin-
guished mathematicians. Tarski was notorious for his all-night working sessions with
his students in his smoke-filled study in the basement of his home in Berkeley. He had
a strong and colorful personality; he was a heavy smoker and drinker, a womanizer,
very ambitious and self-righteous, yet very civilized and polite.
With his enormous legacy in scientific works, students, followers, and general
scientific influence, Tarski is widely regarded as one of the greatest logicians of all
times, along with Aristotle, Frege, and Gödel.

Understanding First-order Logic
123
Wilfrid Augustine Hodges (b. 27.05.1941) is a British
logician and mathematician, a prominent expert in clas-
sical logic and model theory, an enthusiastic explorer of
the history of logic, particularly of Arabic logic, author
of some of the most popular logic textbooks, and pas-
sionate popularizer of logic in general.
Hodges studied at Oxford during 1959–65, where he
received degrees in both Literae Humaniores and Theol-
ogy. Then he did doctoral studies in logic (model theory)
with John Crossley, and in 1970 was awarded a doctor-
ate. He then lectured in both Philosophy and Mathemat-
ics at Bedford College, University of London. In 1984 he moved to Queen Mary
College, University of London, where he was a Professor of Mathematics until his
retirement in 2008; he is now an Emeritus Professor.
Hodges has written several very influential textbooks in logic in a lively,
rather informal, yet stimulating and thought-provoking style. His 1997 Logic: An
Introduction to Elementary Logic is still one of the best popular expositions of the
subject, while his 1993 monograph Model Theory and the 1997 version A Shorter
Model Theory are top references in that major field of logic. His 2007 Mathematical
Logic written with I. Chiswell has become one of the most appreciated textbooks
on the subject. Hodges also developed compositional semantics for the so called
Independence-Friendly logic.
Since the early 2000s Hodges has also been deeply involved in exploring and
interpreting the work in logic and algorithmics of the Persian encyclopedic thinker
of the Islamic Golden Age: Ibn Sin¯a (Avicenna).
Hodges was, inter alia, President of the British Logic Colloquium, of the
European Association for Logic, Language, and Information, and of the Division
of Logic, Methodology, and Philosophy of Science of the International Union of
History and Philosophy of Science. In 2009 he was elected a Fellow of the British
Academy.
3.3
Basic grammar and use of first-order languages
The best (and only) way to learn a new language is by using it, in particular, by practicing
translation between that language and one that you know well. In this section I discuss
how to use first-order languages. For that you will have to understand their basic grammar,
but first let us look at some examples of translating statements from natural language to
first-order languages.
3.3.1
Translation from natural language to first-order languages: warm-up
1. Let us start with translating
“There is an integer greater than 2 and less than 3.”

124
Logic as a Tool
This is surely false but now we are not concerned with its truth, only with its translation
to LZ. It is immediate:
∃x(x > 2 ∧x < 3).
Note that 2 and 3 here are terms, not the numbers 2 and 3.
2. Does the same translation work for LQ? No, because the quantification is over all
elements of the domain of discourse, so the same formula would say in LQ “There
is a rational number greater than 2 and less than 3”, which is true, but not what
we wanted to say. So, what now? If we want to refer to integers while the domain of
discourse is a larger set, namely all rational numbers, we need to restrict somehow the
scope of quantification. A standard method of doing that is to extend the language with
an additional unary predicate, say I, which states that the element of the domain is of
the type we want to quantify over, in this case an integer. Now, the translation is easy:
∃x(I(x) ∧x > 2 ∧x < 3).
We will come back to this trick later again.
3. Let us now translate to LR
“There is no real number, the square of which equals −1.”
How about
∃x(¬x2 = −1)?
Although the sentence begins with “There is . . . ”, do not be confused: this is not an
existential statement, but a negation of such. It actually states
“It is not true that there is a real number the square of which
equals −1”,
so the correct translation is
¬∃x(x2 = −1).
3.3.2
Restricted quantification
Take again the sentence
“Every man loves a woman.”
In order to formalize it in the language of first-order logic, we need to conduct some
preparation. It looks deceptively simple, yet it presents a problem we have seen before:
the quantifiers in our formal language range over the whole domain of discourse, in our
case over all human beings. So, whenever we write ∀x, in this context it will mean “every
human being”, not “every man”; likewise, using ∃x we say “there is a human being”,
but we cannot say “there is a woman.” On the other hand, we usually quantify not over
all individuals from the domain, but over a specified family of them, for example “there
is a positive integer x such that . . . ”, “every child . . . ”, etc. To resolve this problem
we use (again) a little trick with the predicates, called restricted quantification. After

Understanding First-order Logic
125
thinking a little on the sentence we realize that it can be rephrased in the following way,
which is somewhat awkward but more convenient to formalize in first-order logic: “For
every human, if he is a man then there is a human who is a woman and the man
loves that woman”. Now the translation into LH is immediate:
∀x(M(x) →∃y(W(y) ∧L(x, y))).
In general, we introduce unary predicates for the type of objects we want to quantify
over and use logical connectives to express the restricted quantification schemes, as we
did in the example above. For instance, in order to quantify over positive numbers in the
domain of all integers, we can introduce a predicate P(x), stating that “x is a positive
number”, and then write:
∃x(P(x) ∧. . . x . . . )
saying that there exists an object x which is a positive number and which satisfies
. . . x . . . . Likewise,
∀x(P(x) →. . . x . . . )
says that all objects x, which are positive numbers, satisfy . . . x . . . .
Note, that sometimes we do not really need to introduce a new predicate in the language,
if it is already definable there. For instance, in the latter case the predicate for (or the set of)
positive numbers is definable in the language LZ on the structure Z as 0 < x. The schemes
∃x(0 < x ∧. . . x . . . ) and ∀x(0 < x →. . . x . . . ) will therefore have the same effect
as those above.
An alternative way to formalize restricted quantification is to use, as suggested earlier,
many-sorted domains. For instance, in the domain of real numbers we can have sorts for
integers, rational numbers, etc.; in the domain of humans, there are sorts for male, female,
child, etc.. Respectively, we would need different and disjoint stocks of variables for each
sort12. While the former method is more commonly used in informal reasoning, the latter
is a more universal method. In usual mathematical practice however, the following conve-
nient combination of the two methods is used. For the most important families of individu-
als we introduce some standard notation, for example the set of natural numbers is usually
denoted N, the set of integers Z, the set of rational numbers Q, and the set of reals R. Now,
if we want to quantify over integers, we can say: “for all x in Z”, respectively “ there is
x in Z such that”, or symbolically ∀x ∈Z( . . . x . . . ),
and
∃x ∈Z( . . . x . . . ). For
instance, ∀x ∈Z(x2 ≥x) states (truly) that the square of every integer is greater than or
equal to that integer, while ∀x ∈Q ∃z ∈Z(x < z) says that for every rational number
there is an integer greater than it.
3.3.3
Free and bound variables, and scope of a quantifier
There are essentially two different ways in which we use individual variables in first-order
formulae.
12 In fact, this is a common practice in mathematical discourse and writings. For instance, it is a tradition to denote a
real number x, y, z etc. while an integer is denoted i, j, k, n, m, a function f, g, h, etc. Of course, there is no inherent
reason (apart from the risk of confusion) not to use f for an integer or j for a real number. When working with complex
numbers, i of course becomes something quite different.

126
Logic as a Tool
1. First, we use them to denote unknown or unspecified objects, as in
(5 < x) ∨(x2 + x −2 = 0).
We say that the variable x occurs free in that formula, or simply that x is a free variable
in it. As long as a formula contains free variables, it cannot (in general) be assigned
a truth value until these free variables are assigned values. A formula containing free
variables therefore cannot be regarded as a proposition, for its truth may vary depending
on the different possible values of the occurring free variables. For example, assigning
value 1 or 6 to x in the formula above turns it into a true proposition in R, while
assigning value 2 to x turns it into a false proposition.
2. Second, we use individual variables in order to quantify over individuals, as in
∃x((5 < x) ∨(x2 + x −2 = 0)) and ∀x((5 < x) ∨(x2 + x −2 = 0)).
In these formulae the variable x is said to occur bound (or simply, to be a bound
variable) by the quantifier ∀(i.e., existentially bound) in the formula on the left and,
respectively, by ∃(i.e., universally bound) in the formula on the right.
Note that the same variable can be both free and bound in a formula, for example x in
the formula 0 < x ∧∃x(5 < x). That is why we talk about free and bound occurrences
of a variable in a formula.
To make the notion of a bound occurrence of a variable more precise, note that every
occurrence of a quantifier Q in a formula is the beginning of a unique subformula QxA,
called the scope of that occurrence of the quantifier. For example, in the formula
∀x((x > 5) →∀y(y < 5 →(y < x ∧∃x(x < 3))))
the scope of the first occurrence of ∀is the whole formula, the scope of the second occur-
rence of ∀is the subformula ∀y(y < 5 →(y < x ∧∃x(x < 3))), and the scope of the
occurrence of ∃is the subformula ∃x(x < 3).
Every bound occurrence of a variable x is bound by the innermost occurrence of a
quantifier Q over x in which scope that occurrence of x lies. In other words, an occurrence
of a variable x is bound by the first occurrence of the quantifier Q in a formula QxA, if
and only if that occurrence of x is free in the subformula A. For example, in the formula
above the first three occurrences of x are bound by the first occurrence of ∀, while the last
two are bound by the occurrence of ∃.
I now define, for every formula in A ∈FOR(L), the set FVAR(A) of all individual
variables that are free in A, that is, that have a free occurrence in A. The definition is by
recursion on the inductive definition of formulae in L (refer to Section 1.4).
Definition 96 (The set of free variables in a formula) Let L be any first-order lan-
guage. We define a mapping FVAR : FOR(L) →P(VAR) recursively on the structure of
formulae in L as follows.
1. For every atomic formula A = p(t1, . . . , tn), where t1, . . . , tn are terms in L and p
is an n-ary predicate symbol in L, FVAR(A) := VAR(t1) ∪. . . ∪VAR(tn).
2. If A is a formula in L then FVAR(¬A) := FVAR(A).

Understanding First-order Logic
127
3. If A and B are formulae in L then FVAR(A ∧B) := FVAR(A) ∪FVAR(B) and
the same holds for FVAR(A ∨B), FVAR(A →B), and FVAR(A ↔B).
4. If A is a formula in L that has the property P and x is an individual variable, then
FVAR(∀xA) = FVAR(A) \ {x} and FVAR(∃xA) = FVAR(A) \ {x}.
As an exercise, define in a similar way the set of bound variables, that is, variables with
a bound occurrence BVAR(A) for every formula A.
Definition 97 A formula with no bound variables is called an open formula. A formula
with no free variables is called a closed formula or a (first-order) sentence.
Once interpreted in a structure, first-order sentences have a determined meaning and
represent propositions about that structure.
The following important observation allows us to only take into account the assignment
of values to the free variables occurring in a formula in order to determine the truth of that
formula.
Proposition 98 The truth of a formula in a given first-order structure under a given vari-
able assignment only depends on the assignment of values to the free variables occurring
in that formula. That is, if v1, v2 are variable assignments in S such that v1 |FVAR(A)=
v2 |FVAR(A), then
S, v1 |= A iff S, v2 |= A.
In particular, if a variable x does not occur free in A, then S, v |= A if and only if
S, v′ |= A for every x-variant v′ of v and, therefore, if and only if S, v |= ∀xA.
3.3.4
Renaming of a bound variable in a formula and clean formulae
Note that a bound variable in a formula plays an auxiliary rôle and does not have its own
meaning (it is also called a dummy variable) in the sense that we can replace it throughout
the formula by another variable, not occurring in the formula, without altering the meaning
of that formula. For example, it should be intuitively clear that ∃x(5 < x∨x2 +x−2 = 0)
means exactly the same as ∃y(5 < y ∨y2 + y −2 = 0); in particular, they are equally
true. Likewise, ∀x(5 < x ∨x2 + x −2 = 0) means the same as ∀y(5 < y ∨y2 +
y −2 = 0). On the other hand, 5 < x ∨x2 + x −2 = 0 is essentially different from
5 < y ∨y2 + y −2 = 0; depending on the values of the free variables x and y, one of
these can be true while the other is false.
We should distinguish very well between the meaning and the use of free and bound
variables. For example, the free and the bound occurrences of the variable x in the formula
(x > 5) ∧∀x(x < 2x) have nothing to do with each other.
Furthermore, different occurrences of the same variable can be bound by different quan-
tifiers, and that may also be confusing.
Example 99 Here are some examples of different uses of the same variable:
• ∃x(x > 5) ∨∀x(2x > x). Clearly, the occurrences of x, bound by the first quantifier,
have nothing to do with those bound by the second.

128
Logic as a Tool
• ∃x(x > 5) ∧∃x(x < 3). Likewise, the two x’s claimed to exist here need not (and,
in fact, cannot) be the same, so this formula has the same meaning as ∃y(y > 5) ∧
∃x(x < 3) or ∃x(x > 5) ∧∃z(z < 3) or ∃y(y > 5) ∧∃z(z < 3).
• ∀x((x > 5) →∃x(x < 3)). Again, the occurrences of x in the subformula ∃x(x < 3)
are bound by ∃and not related to the first two occurrences of x bound by ∀, so this for-
mula has the same meaning as ∀x((x>5) →∃y(y<3)) or ∀z((z>5)→∃x(x<3))
or ∀z((z > 5) →∃y(y < 3)).
The best way to avoid confusions like these in formal arguments and proofs is to always
use different variables for different purposes. In particular, never use the same variable as
both free and bound, or as bound by two different quantifiers in the same formula or proof.
Definition 100 A formula A is clean if no variable occurs both free and bound in A and
every two occurrences of quantifiers bind different variables.
Definition 101 The uniform replacement of all occurrences of a variable x bound by the
same occurrence of a quantifier in a formula A with a variable not occurring in A is
called a renaming of the variable x in A.
Example 102 The formula ∃x(x > 5) ∧∃y(y < z) is clean, while
∃x(x > 5) ∧∃y(y < x) and ∃x(x > 5) ∧∃x(y < x) are not.
The formula (x > 5) ∧∀x(x > 5 →¬∃x(x < y)) is not clean, either.
One correct renaming of that formula is (x > 5) ∧∀x(x > 5 →¬∃z(z < y)),
but neither (z > 5) ∧∀x((x > 5) →¬∃x(x < y)),
nor (x > 5) ∧∀z((z > 5) →¬∃z(z < y)),
nor (x > 5) ∧∀x(x > 5 →¬∃y(y < y))
is a correct renaming. (Why?)
Every formula can be transformed into a clean formula by means of several consecutive
renamings of variables. For example, the formula
(x > 5) ∧∀x((x > 5) →¬∃x(x < y))
can be transformed into the clean formula
(x > 5) ∧∀z1((z1 > 5) →¬∃z2(z2 < y)).
3.3.5
Substitution of a term for a variable in a formula, and capture of a
variable
Given a formula A, a variable x, and a term t, we can obtain a formula A[t/x] by substi-
tuting simultaneously t for all free occurrences of x in A. The formula A[t/x] is called
the result of substitution of t for x in A. The semantic meaning of such substitution
is to assign the value of the term t to the variable x when evaluating the truth of A. For
instance, given the formula
A = ∀x(P(x, y) →(¬Q(y) ∨∃yP(x, y))),

Understanding First-order Logic
129
we have
A[f(y, z)/y] = ∀x(P(x, f(y, z)) →(¬Q(f(y, z)) ∨∃yP(x, y))),
while
A[f(y, z)/x] = A,
because x does not occur free in A.
Intuitively, what the formula A[t/x] is supposed to say about the individual denoted t
is the same as what A says about the individual denoted by x. Is that always the case? No!
An unwanted capture effect can occur after such a substitution if we substitute a term t for
a variable x in a formula A, where x is in the scope of a quantifier over another variable y
which occurs in t. For example, substituting y + 1 for x in the formula ∃y(x < y), which
is true in N for every value assigned to x, will produce the false sentence ∃y(y + 1 < y)
since the occurrence of y in the term y + 1 is captured by the quantifier ∃y; this has
happened because the substitution mixed free and bound occurrences of y.
Similarly, in the formula ∀x∃z(x < z), meaning that for every number there is a greater
one, renaming x with the variable z will produce ∀z∃z(z < z), which clearly distorts the
meaning of the formula. The reason is that when substituting z for x it is captured by
the quantifier ∃z.
Formally, capture happens when new occurrences of some variable, say y, are intro-
duced in the scope of a quantifier Qy in a formula A as a result of substitution of a term
t containing y for another variable x in that formula. The following definition is intended
to disallow substitutions that cause capture.
Definition 103 A term t is free for (substitution for) a variable x in a formula A, if
no variable in t is captured by a quantifier as t is substituted for any free occurrence of
x in A.
Example 104
• Every ground term (not containing variables), in particular every constant symbol, is
free for substitution for any variable in any formula.
• More generally, every term that does not contain variables with bound occurrences in
a given formula is free for substitution for any variable in that formula.
• The term f(x, y) is free for substitution for y in the formula
A = ∀x(P(x, z) ∧∃yQ(y)) →P(y, z), resulting in
A[f(x, y)/y] = ∀x(P(x, z) ∧∃yQ(y)) →P(f(x, y), z).
• However, the same term is not free for substitution for z in A, resulting in
A[f(x, y)/z] = ∀x(P(x, f(x, y)) ∧∃yQ(y)) →P(y, f(x, y)), because a capture
of the variable x occurs in the first occurrence of f(x, y).
To avoid the problems arising from capture mentioned above, from now on we only
allow a substitution t/x in a formula A when t is free for x in A. Hereafter, when t is a
constant symbol c, I will often write A(c) instead of A[c/x].

130
Logic as a Tool
3.3.6
A note on renamings and substitutions in a formula
Note that renaming and substitution are very different operations; renaming always acts
on bound variables, while substitution always acts on free variables.
Also, as we will see in Theorem 119, renamings preserve the formula up to logical
equivalence while substitutions do not.
On the other hand, a suitable renaming of a formula can prepare it for a substitution by
rendering the term to be substituted free for such substitution in the renamed formula. For
instance, the term f(x, y) is not free for substitution for y in
A = ∀x(P(x, y) ∧∃yQ(y)),
but it becomes free for such a substitution after renaming A, for example to:
A′ = ∀x′(P(x′, y) ∧∃yQ(y)).
References for further reading
For more examples and further discussion on grammar and use of first-order languages
and translations to and from natural language, see Tarski (1965), Kalish and Montague
(1980), Barwise and Echemendy (1999), Hodges (2001), Smith (2003), Nederpelt and
Kamareddine (2004), Bornat (2005), Chiswell and Hodges (2007), and van Benthem et al.
et al. (2014).
Exercises
3.3.1
Using the additional predicate I(x) for “x is an integer”, formalize the following
sentences in the first-order language for the structure of real numbers. R.
(a) Every square of an integer is greater than 0.
(b) Every square of a real number which is not an integer is greater than 0.
(c) Some real numbers are not integers.
(d) Every integer is even or odd.
(e) No integer is both even and odd.
(f) For every integer there is a greater integer.
(g) Every positive integer is a square of some negative real number.
(h) Not every real number is greater than an integer.
(i) There is an integer such that not every real number is greater than it.
(j) No real number is greater than every integer.
(k) Every real number which is not zero has a reciprocal.
(l) There is a real number which, when multiplied by any real number,
produces that number.
(m) Between every two different real numbers there is an integer.
3.3.2
Using unary predicates T(x) for “x talks” and L(x) for “x listens”, formalize
the following sentences in a first-order language for the domain of all humans.
(a) Everybody talks or everybody listens.
(b) Everybody talks or listens.
(c) If John talks everybody listens.
(d) If somebody talks everybody listens.

Understanding First-order Logic
131
(e) If somebody talks everybody else listens.
(f) Nobody listens if everybody talks.
3.3.3
Use binary predicates T2(x, y) for “x talks to y” and L2(x, y) for “x listens to
y” to formalize the following sentences in a first-order language for the domain
of all humans. (Note the ambiguity of some of these.)
(a) Not everybody talks to somebody.
(b) Nobody listens to anybody.
(c) John listens to everybody who talks to somebody.
(d) John listens to nobody who talks to somebody else.
(e) John talks to everybody who does not listen to him.
(f) Somebody does not talk to anybody who does not listen to him.
(g) Nobody listens to anybody who does not listen to him.
(h) Not everybody listens if everybody who talks to somebody.
(i) If everybody talks to somebody then nobody listens to anybody.
3.3.4
Translate the following sentences into the first-order language LH for the struc-
ture of all humans H. Remember that the language LH does not have a unary
predicate for “ . . . is a child”, but a binary predicate “ . . . is a child of . . . .”
(a) Some men love every woman.
(b) Every woman loves every man who loves her.
(c) Every man loves some woman who does not love him.
(d) Some women love no men who love them.
(e) Some men love only women who do not love them.
(f) No woman loves a man who loves every woman.
(g) Every woman loves her children.
(h) Every child loves his/her mother.
(i) Everyone loves a child. (Note the ambiguity here . . . )
(j) A mother loves every child. ( . . . and here.)
(k) Every child is loved by someone.
(l) Some woman love every child.
(m) All children love at least one of their parents.
(n) Some children love every man who loves their mother.
(o) No child loves any mother who does not love all her children.
(p) Not every man loves some woman who loves all his children.
3.3.5
Translate the following sentences into the first-order language LH for humans,
with functions m for “the mother of” and f for “the father of”; the unary
predicates M for “man” and W for “woman”; and the binary predicates P(x,y)
meaning “x is a parent of y”, K(x,y) meaning “x knows y”, L(x,y) meaning “x
likes y”, and R(x,y) meaning “x respects y”. Some of these exercises require
the use of equality.
(a) Every parent is respected by some mother.
(b) Every human knows his/her mother.
(c) Some people do not know any of their parents.
(d) Every woman knows a man who is not her parent.
(e) Some children do not respect any of their parents.

132
Logic as a Tool
(f) Some fathers respect every mother whom they know.
(g) No woman likes a man who does not know all of her children.
(h) Not every woman likes no man who does not know some of her
children.
(i) No man respects a woman who does not know all of her children.
(j) Nobody respects any man who does not respect both his parents.
(k) If somebody is a child then every mother likes him/her.
(l) No man, except Adam, knows Eve.
(m) Only possibly Adam loves every woman he knows.
(n) Not every woman loves exactly one man.
(o) Only John’s mother loves him.
(p) Not only John’s mother loves him. (Note the implicature here!)
(q) Henry Ford likes every color, as long as it is black.
(r) Any customer can have a car painted any color that he wants, so long
as it is black. (Use suitable predicates here.)
3.3.6
Translate the following sentences into the first-order language (with equality)
LR for the structure of real numbers ⟨R; <, +, ×, 0⟩, with an additional unary
functional symbol f and a unary predicate Q(x) for x “is a rational number.”
(a) Every rational zero of the function f is positive.
(b) No irrational zero of the function f is non-negative.
(c) No negative real number is greater than every zero of the function f.
(d) Not every negative rational number is greater than every zero of f.
(e) Some positive zero of f is not less than every irrational zero of f.
(f) Every non-positive zero of f is less than some irrational zero of f.
3.3.7
Recall that a natural number is prime if it is greater than 1 and has no other
positive integer divisors but 1 and itself. Express the following statements in the
language LN . For some of these you need to use equality (see Section 3.4.4).
(a) x divides y, denoted x|y.
(b) The number (denoted) x is prime.
(c) There are infinitely many primes. (Hint: this is equivalent to saying that for
every natural number there is a greater prime number.)
(d) (Goldbach’s conjecture)
Every even integer greater than 2 equals the sum of two primes.
(e) (The twin primes conjecture)
There are infinitely many pairs of primes that differ by 2.
(f) Every natural number n has a prime divisor not greater than √n.
(NB: the function √· is not in the language.)
(g) The numbers x and y have no common prime divisor.
(h) z is a greatest common divisor of x and y.
(i) Every two natural numbers x, y which are not both 0 have a greatest common
divisor, denoted gcd(x, y).
(j) (The fundamental theorem about division with remainder)
For any natural numbers x > 0 and y, there exist unique natural numbers q
and r such that y = q × x + r and 0 ≤r < x.

Understanding First-order Logic
133
3.3.8
Express the following statements in the language of the structure of real num-
bers LR, using the predicate I(x) to mean “x is an integer.”
(a) The number (denoted) z is rational.
(b) Every rational number can be represented as an irreducible fraction.
(c) Between every two different real numbers there is a rational number.
(d)
√
2 is not a rational number.
(e) Every quadratic polynomial with real coefficients which has a non-zero value
has at most two different real zeros.
3.3.9
Define by recursion on the inductive definition of formulae, for every formula in
A ∈FOR(L), the set BVAR(A) of all individual variables that are bound in A,
that is, have a bound occurrence in A.
3.3.10
Determine the scope of each quantifier and the free and bound occurrences of
variables in the following formulae where P is a unary and Q a binary predicate.
(a) ∃x∀z(Q(z, y) ∨¬∀y(Q(y, z) →P(x))),
(b) ∃x∀z(Q(z, y) ∨¬∀x(Q(z, z) →P(x))),
(c) ∃x(∀zQ(z, y) ∨¬∀z(Q(y, z) →P(x))),
(d) ∃x(∀zQ(z, y) ∨¬∀yQ(y, z)) →P(x),
(e) ∃x∀z(Q(z, y) ∨¬∀yQ(x, z)) →P(x),
(f) ∃x∀zQ(z, y) ∨¬(∀yQ(y, x) →P(x)),
(g) ∃x(∀zQ(z, y) ∨¬(∀zQ(y, z) →P(x))),
(h) ∃x(∀x(Q(x, y) ∨¬∀z(Q(y, z) →P(x)))),
(i) ∃x(∀y(Q(x, y) ∨¬∀xQ(x, z))) →P(x).
3.3.11
Rename the bound variables in each formula above to obtain a clean formula.
3.3.12
Show that every formula can be transformed into a clean formula by means of
several consecutive renamings of variables.
3.3.13
For each of the following formulae (where P is a unary predicate and Q is a
binary predicate), determine if the indicated term is free for substitution for the
indicated variable. If so, perform the substitution.
(a) Formula: ∃x(∀zP(y) ∨¬∀y(Q(y, z) →P (x))); term: f(x); variable: z.
(b) Same formula; term: f(z); variable: y.
(c) Same formula; term: f(y); variable: y.
(d) Formula:
∀x((¬∀yQ(x, y) ∨P(z)) →∀y¬∃z∃xQ(z, y));
term: f(y);
variable: z.
(e) Same formula; term: g(x, f(z)); variable: z.
(f) Formula:
∀y(¬(∀x∃z(¬P(z) ∧∃yQ(z, x))) ∧(¬∀xQ(x, y) ∨P (z)));
term f(x); variable: z.
(g) Same formula; term: f(y); variable: z.
(h) Formula:
(∀y∃z¬P(z) ∧∀xQ(z, x)) →(¬∃yQ(x, y) ∨P(z));
term:
g(f(z), y); variable: z.
(i) Same formula; term: g(f(z), y); variable: x.

134
Logic as a Tool
Bertrand Arthur William Russell (18.5.1872–2.2.1970)
was a famous British philosopher, logician, mathemati-
cian, intellectual, and political activist with strong pacifist
views, who made seminal contributions to the foundations
of mathematics and to the advancement of modern formal
logic, also regarded as one of the founders of modern ana-
lytic philosophy.
Russell studied mathematics and philosophy at Trinity
College in Cambridge, from where he graduated with dis-
tinction and later held academic positions there with several short interruptions,
during which he worked at several universities in USA.
Russell’s first major work was his book The Principles of Mathematics published
in 1903, where he presented and discussed his famous paradoxical definition of the
set of all sets that are not members of themselves, which became known as Russell’s
paradox (see details in Section 5.2.1). That discovery dealt a devastating blow to
Cantor’s set theory and to Frege’s formalization of logic, and played a crucial role in
the arising of a major crisis in the foundations of mathematics at the beginning of the
20th century. In this book and in the following three-volume Principia Mathemat-
ica, written with Whitehead and published during 1910–1913, Russell advanced the
theory of logicism which aimed to found the entire field of mathematics on purely
logical principles (one of the three main philosophical approaches for reforming the
foundations of mathematics, prompted by the foundational crisis). According to the
logicist theory, first put forward by Frege, all mathematical truths can be translated
into logical truths and all mathematical proofs can be transformed into logical proofs;
all mathematical results are therefore theorems of logic. In Russell’s words, “The fact
that all Mathematics is Symbolic Logic is one of the greatest discoveries of our age;
and when this fact has been established, the remainder of the principles of math-
ematics consists in the analysis of Symbolic Logic itself.” With these books and
later philosophical works, Russell became immensely influential to many prominent
philosophers and mathematicians, including his genius student Wittgenstein.
Russell was a staunch pacifist and, shortly after World War I, was convicted
and imprisoned for 6 months for his anti-war views and speeches, where he wrote
his extremely popular Introduction to Mathematical Philosophy. He pursued his
anti-war activities and kept a high political profile, which brought him worldwide
fame well beyond academic circles until his death in 1970 at age of 97.
Russell also had a very eventful personal life, with four marriages and a number of
intermittent affairs. His often revolutionary views and writings on marriage, morals,
and society were often controversial, leading, inter alia, to his professorship appoint-
ment at the City College New York being revoked in 1940. However, 10 years later
Russell received the Nobel Prize for literature, in particular for his book Marriage
and Morals.
In summary, Russell was one of the most influential logicians and philosophers of
the 20th century and one of the intellectually strongest personalities of his time. He
said of himself: “Three passions, simple but overwhelmingly strong, have governed
my life: the longing for love, the search for knowledge, and unbearable pity for the
suffering of mankind.”

Understanding First-order Logic
135
3.4
Logical validity, consequence, and equivalence in first-order logic
Here I introduce and discuss the fundamental logical notions of logical validity, conse-
quence, and equivalence for first-order formulae.
3.4.1
More on truth of sentences in structures: models and counter-models
Recall that a sentence is a formula with no free variables.
As we noted earlier, the truth of a sentence in a given structure does not depend on the
variable assignment. For a structure S and sentence A we can simply write
S |= A
if S, v |= A for any (hence, every) variable assignment v.
We then say that S is a model of A and that A is true in S, or that S satisfies A or
A is satisfied by S. Otherwise we write S ⊭A and say that A is false in S, S does not
satisfy A, or S is a counter-model for A.
True sentences express properties of structures.
Example 105 (Sentences expressing properties of structures)
• R |= ∀x∀y(x + y = y + x) (commutativity of the addition of real numbers),
R |= ∀x∀y∀z((x + y) × z = (x × z) + (y × z)) (distributivity of multiplication over
addition of real numbers).
• N |= ∀x∃y(x < y) (for every natural number there is a larger one), while
N ⊭∀x∃y(y < x) (not for every natural number is there a smaller one).
Hence, N |= ¬∀x∃y(y < x). However, Z |= ∀x∃y(y < x).
• Q |= ∀x∀y(x < y →∃z(x < z ∧z < y))
(the ordering of the rationals is dense), but
Z ⊭∀x∀y(x < y →∃z(x < z ∧z < y))
(the ordering of the integers is not dense).
• Q |= ∃x(3x = 1), but
Z ⊭∃x(3x = 1) (1/3 is a rational number, but is not an integer).
• R |= ∀x(x > 0 →∃y(y2 = x)) (every positive real is a square of a real), but
Q ⊭∀x(x > 0 →∃y(y2 = x))
(not every positive rational number is a square of a rational number).
• R ⊭∀x∀y(xy > 0 →(x > 0 ∨y > 0)). (For example, x = y = −1).
• R |= ∃x∀y(xy < 0 →y = 0): true or false?
• H |= ∀x∀y(∃z(x = m(z) ∧y = m(z)) →x = y).
What does this sentence mean? Is it true only for this interpretation?
• N |= (∀x(∀y(y < x →P(y)) →P(x)) →∀xP(x)),
where P is any (uninterpreted) unary predicate.
Why is this true? What does this sentence say about N? Is it also true in Z?

136
Logic as a Tool
3.4.2
Satisfiability and validity of first-order formulae
A first-order formula A is:
• satisfiable if S, v |= A for some structure S and some variable assignment v in S; or
• (logically) valid, denoted |= A, if S, v |= A for every structure S and every variable
assignment v in S.
If A is a sentence, the variable assignment in these definitions is not relevant and can
be omitted.
Validity of any formula can actually be reduced to validity of a sentence, as follows.
Definition 106 Given a first-order formula A with free variables x1, . . . , xn, the sen-
tence ∀x1 . . . ∀xnA is called a universal closure of A.
Note that, depending on the order of the quantified variables, a formula can have several
universal closures; as we will see later in Section 3.4.6 they are all essentially equivalent.
The following is an easy exercise to prove.
Proposition 107 A formula A is logically valid if and only if any of its universal closures
is logically valid.
Example 108 Some examples of logically valid and non-valid first-order formulae
include the following.
• Every first-order instance of a tautology is logically valid.
For instance, |= ¬¬(x > 0) →(x > 0) and |= P(x) ∨¬P(x), for any unary predi-
cate P.
• If |= A and x is any variable then we also have |= ∀xA.
Thus, |= ∀x(P(x) ∨¬P(x)) and |= ∀x(P(y) ∨¬P (y)).
• |= ∀x(x = x), because of the fixed meaning of the equality symbol =.
• |= ∀x∀y(∃z(x = f(z) ∧y = f(z)) →x = y), where f is any unary functional sym-
bol, because of the very meaning of the notion of a function.
• The sentence ∃xP(x) is not valid: take for instance P(x) to be interpreted as the empty
set in any non-empty domain. This will give a counter-model for that sentence. However,
∃xP(x) is satisfiable; a model is any structure where the interpretation of P(x) is
non-empty.
• Likewise, the sentence ∀xP(x) ∨∀x¬P(x) is not valid, but it is satisfiable.
(Find a model and a counter-model.)
• The sentence ∃x(P(x) ∧¬P(x)) is not satisfiable. Why?
• |= ∃x∀yP(x, y) →∀y∃xP(x, y).
It is a good exercise to check this from the definition; we will discuss it again later in
Section 3.4.6
• However, ⊭∀y∃xP(x, y) →∃x∀yP(x, y). Find a counter-model!

Understanding First-order Logic
137
3.4.3
Logical consequence in first-order logic
The notion of logical consequence is fundamental in logic. In propositional logic we
defined logical consequence A1, . . . , An |= B as preservation of the truth from the
premises to the conclusion. The same idea applies in first-order logic but, instead of truth
tables, we now use truth in a structure.
Definition 109 A first-order formula B is a logical consequence from the formulae
A1, . . . , An, denoted A1, . . . , An |= B, if for every structure S and variable assign-
ment v in S that each satisfy the formulae A1, . . . , An, in S by v, the formula B is also
satisfied in S by v. Formally:
If S, v |= A1, . . . , S, v |= An then S, v |= A.
In particular, if A1, . . . , An, B are sentences, then A1, . . . , An |= B means that B
is true in every structure in which all A1, . . . , An are true, that is, every model of each
of A1, . . . , An is also a model of B.
If A1, . . . , An |= B, we also say that B follows logically fromA1, . . . , An, or that
A1, . . . , An logically imply B.
In particular, ∅|= A iff |= A.
Some easy observations (left as exercises to prove) are listed below. As in propositional
logic, the following are equivalent for the first-order logical consequence.
1. A1, . . . , An |= B.
2. A1 ∧. . . ∧An |= B.
3. |= A1 ∧· · · ∧An →B.
4. |= A1 →(A2 →· · · (An →B) . . . ).
Logical consequence in first-order logic is therefore reduced to logical validity.
Example 110 Here are some examples and non-examples of first-order logical conse-
quences.
1. If A1, . . . , An, B are propositional formulae such that A1, . . . , An |= B, and
A′
1, . . . , A′
n, B′ are first-order instances of A1, . . . , An, B obtained by the
same uniform substitution of first-order formulae for propositional variables, then
A′
1, . . . , A′
n |= B′.
For instance, ∃xA, ∃xA →∀yB |= ∀yB.
2. ∀xA(x) |= A[t/x] for any formula A and term t free for x in A.
Indeed, if ∀xA(x) is true in a structure S for some assignment, then A is true for every
possible value of x in S, including the value of t for that assignment.
To put it in simple words: if everybody is mortal then, in particular, John is mortal, and
the mother of Mary is mortal, etc.
3. ∀x(P(x) →Q(x)), ∀xP(x) |= ∀xQ(x). Why?
(Note that this is not an instance of a propositional logical consequence.)

138
Logic as a Tool
4. ∃xP(x) ∧∃xQ(x) ⊭∃x(P(x) ∧Q(x)).
Indeed, the structure N ′ obtained from N where P(x) is interpreted as ‘x is even’
and Q(x) is interpreted as ‘x is odd’ is a counter-model:
N ′ |= ∃xP(x) ∧∃xQ(x), but N ′ ⊭∃x(P(x) ∧Q(x)).
Example 111 Here we illustrate “semantic reasoning”, based on the formal semantics
of first-order formulae, for proving or disproving first-order logical consequences.
1. Show the logical validity of the following argument:
“If Tinkerbell is a Disney fairy and every Disney fairy has blue eyes, then some-
one has blue eyes.”
Proof. Let us first formalize the argument in first-order logic by introducing predicates:
P(x) meaning “x is a Disney fairy”, Q(x) meaning “x has blue eyes”, and a
constant symbol c interpreted as “Tinkerbell.”
The argument can now be formalized as follows:
1. P(c) ∧∀x(P(x) →Q(x)) |= ∃yQ(y).
To show its validity take any structure S for the first-order language introduced
above and any variable assignment v in S. Now, suppose
2. S, v |= (P(c) ∧∀x(P(x) →Q(x)).
We have to show that:
3. S, v |= ∃yQ(y).
From the assumption (2) we have, by the truth definition of ∧, that:
4. S, v |= P (c) and
5. S, v |= ∀x(P(x) →Q(x)).
Let v′ be a variable assignment obtained from v by redefining it on x as follows:
v′(x) = cS. (Recall that cS is the interpretation of the constant symbol c in S.)
Then:
6. S, v′ |= P(x).
According to the truth definition of ∀, it follows from (5) that:
7. S, v′ |= P(x) →Q(x).
From (6) and (7) it follows that:
8. S, v′ |= Q(x).
Now, let v′′ be a variable assignment obtained from v by redefining it on y as fol-
lows: v′′(y) = v′(x) = cS. We then have:
9. S, v′′ |= Q(y).
According to the truth definition of ∃, it follows from (9) that:
(3) S, v |= ∃yQ(y), so we are done. ■
2. Prove that the following argument is not logically valid:
“If everything is black or white then everything is black or everything is white.”

Understanding First-order Logic
139
Proof. Again, we first formalize the argument in first-order logic by introducing
predicates:
B(x) meaning “x is black” and W (x) meaning “x is white.”
Now the argument can be formalized as follows:
∀x(B(x) ∨W (x)) |= ∀xB(x) ∨∀xW(x).
To falsify this logical consequence it is sufficient to find any counter-model, that is, a
structure S and a variable assignment v such that
S, v |= ∀x(B(x) ∨W(x)) and S, v ⊭∀xB(x) ∨∀xW(x).
Note that the interpretations of the predicates B and W need not have anything to
do with black, white, or any colors in that structure. We therefore choose to take the
structure S with domain the set Z of all integers, where the interpretation of B is the
predicate Even, where Even(n) means “n is even”, and the interpretation of W is
the predicate Odd, where Odd(n) means “n is odd.”
We already know that the variable assignment is irrelevant here because there are no
free variables in this argument, but we will nevertheless need it in order to process the
truth definitions of the quantifiers. Consider any variable assignment v in S. Then take
any variable assignment v′ that differs from v possibly only on x. The integer v′(x) is
even or odd, therefore S, v′ |= B(x) or S, v′ |= W(x), hence (by the truth definition
of ∨):
1. S, v′ |= B(x) ∨W (x).
Thus, by the truth definition of ∀, it follows that:
2. S, v |= ∀x(B(x) ∨W(x)).
On the other hand, consider the variable assignment v1 obtained from v by redefin-
ing it on x as follows: v1(x) = 1. Then:
3. S, v1 ⊭B(x).
By the truth definition of ∀, it follows that:
4. S, v ⊭∀xB(x).
Likewise, consider the variable assignment v2 obtained from v by redefining it on x as
follows: v2(x) = 2. Then:
(3) S, v2 ⊭W(x).
By the truth definition of ∀, it follows that:
(4) S, v ⊭∀xW(x).
By the truth definition of ∨, it follows that
(5) S, v ⊭∀xB(x) ∨∀xW(x).
Therefore, by the truth definition of →, it follows from (2) and (5) that
(6) S, v ⊭∀x(B(x) ∨W(x)) →(∀xB(x) ∨∀xW(x)). ■

140
Logic as a Tool
As noted earlier, logical consequence in first-order logic satisfies all basic properties of
propositional logical consequence. Further, some important additional properties related
to the quantifiers hold. They will be used as rules of inference in deductive systems for
first-order logic, so I give them more prominence here.
Theorem 112 For any first-order formulae A1, . . . , An, A, B, the following hold.
1. If A1, . . . , An |= B then ∀xA1, . . . , ∀xAn |= ∀xB.
2. If A1, . . . , An |= B and A1, . . . , An are sentences, then A1, . . . , An |= ∀xB, and
hence A1, . . . , An |= ¯B, where ¯B is any universal closure of B.
3. If A1, . . . , An |= B[c/x] where c is a constant symbol not occurring in A1, . . . , An,
then A1, . . . , An |= ∀xB(x).
4. If A1, . . . , An, A[c/x] |= B where c is a constant symbol not occurring in
A1, . . . , An, A, or B, then A1, . . . , An, ∃xA |= B.
5. For any term t free for substitution for x in A:
(a) ∀xA |= A[t/x].
(b) A[t/x] |= ∃xA.
The proofs of these are easy exercises using the truth definition.
3.4.4
Using equality in first-order logic
First-order languages (especially those used in mathematics) usually contain the equality
symbol =, sometimes also called identity. This is regarded as a special binary relational
symbol, and is always meant to be interpreted as the identity of objects in the domain of
discourse.
Example 113 The equality is a very useful relation to specify constraints on the size of
the model, as the following examples in the first-order language with = show.
1. The sentence
λn = ∃x1 · · · ∃xn


1≤i̸=j≤n
¬xi = xj

states that the domain has at least n elements.
2. The sentence μn = ¬λn+1 or, equivalently,
μn = ∀x1 · · · ∀xn+1


1≤i̸=j≤n+1
xi = xj

states that the domain has at most n elements.
3. The sentence σn = λn ∧μn states that the domain has exactly n elements.
The proofs of these claims are left as an exercise (Exercise 9 in Section 3.4.8).

Understanding First-order Logic
141
Example 114 The equality is often an indispensable relation to express important math-
ematical properties as shown in the following examples.
1. The sentences in Example 113 are easily relativized for every formula A(x, ¯z) con-
taining, among others, the free variable x, to the subset of the domain consisting of
the elements satisfying A. For instance, we can combine these to say things like “of
all the students in the class, at most two scored distinctions in at least five
exams each.”
2. In particular, for any formula A(x, ¯z), the formula
∃!xA(x, ¯z) = (A(x, ¯z) ∧∀y(A(y, ¯z) →x = y))
states that there is a unique element in the domain of discourse satisfying A, for the
current values of the parameters ¯z.
3. The sentence ∀x∃!y(R(x, y) in the language with = and a binary relational symbol R
therefore states that the relation R is functional, that is, every element of the domain
is R-related to a unique element.
4. The sentence ∀x∀y(f(x) = f(y) →x = y)) in the language with = and a unary
functional symbol f states that the function f is injective, that is (by contraposition),
assigns different values to different arguments.
Sometimes the equality is implicit or even hidden in natural language expressions such
as “Everyone, except possibly John, understood the joke.” It becomes explicit and
readily translatable (exercise) to first-order logic when rephrased as “Everyone, who is
not (equal to) John, understood the joke.” Likewise, “No-one, but John and Mary,
enjoyed the party” can be rephrased as “Everyone, who is not (equal to) John and
not (equal to) Mary, enjoyed the party.”
The following proposition captures the characteristic properties of the equality express-
ible in a first-order language.
Proposition 115 The following sentences in an arbitrary first-order language L are log-
ically valid:
(Eq1) x = x;
(Eq2) x = y →y = x;
(Eq3) x = y ∧y = z →x = z;
(Eqf) x1 = y1 ∧. . . ∧xn = yn →f(x1, . . . , xn) = f(y1, . . . , yn) for n-ary func-
tional symbol f in L;
(Eqr) x1 = y1 ∧. . . ∧xn = yn →(p(x1, . . . , xn) →p(y1, . . . , yn)) for n-ary pred-
icate symbol p in L.
However, these axioms cannot guarantee that the interpretation of any binary relational
symbol = satisfying them is equality, but only that it is a congruence (see Section 4.6.3)
in the given structure.
Lastly, the following theorem states an important generalization of the equality axioms
listed above stating that equal terms can be equivalently replaced for each other in any
formula.

142
Logic as a Tool
Theorem 116 (Equivalent replacement) For any formula A(x) and terms s, t free for
x in A, the following holds:
|= s = t →A[s/x] ↔A[t/x].
The proof is by structural induction on the formula A(x), and is left as an exercise for
the reader.
3.4.5
Logical equivalence in first-order logic
Logical equivalence in first-order logic is based on the same idea as in propositional logic:
the first-order formulae A and B are logically equivalent if always one of them is true if
and only if the other is true. This is defined formally as follows.
Definition 117 The first-order formulae A and B are logically equivalent, denoted A ≡
B, if for every structure S and variable assignment v in S:
S, v |= A if and only if S, v |= B.
In particular, if A and B are sentences, A ≡B means that every model of A is a model
of B, and every model of B is a model of A.
The following theorem summarizes some basic properties of logical equivalence.
Theorem 118
1. A ≡A.
2. If A ≡B then B ≡A.
3. If A ≡B and B ≡C then A ≡C.
4. If A ≡B then ¬A ≡¬B, ∀xA ≡∀xB, and ∃xA ≡∃xB.
5. If A1 ≡B1 and A2 ≡B2 then A1 ◦A2 ≡B1 ◦B2 where ◦is any of ∧, ∨, →, ↔.
6. The following are equivalent:
(a) A ≡B;
(b) |= A ↔B; and
(c) A |= B and B |= A.
Theorem 119 The result of renaming of any variable in any formula A is logically equiv-
alent to A. Consequently, every formula can be transformed into a logically equivalent
clean formula.
Example 120 Some examples of logical equivalences between first-order formulae are
as follows.
• Any first-order instance of a pair of equivalent propositional formulae is a pair of log-
ically equivalent first-order formulae. For example:
¬¬∃xQ(x, y) ≡∃xQ(x, y) (being an instance of ¬¬p ≡p);
∃xP(x) →Q(x, y) ≡¬∃xP(x) ∨Q(x, y) (being an instance of p →q ≡¬p ∨q).

Understanding First-order Logic
143
• ∃x(5< x ∨x2 + x −2 =0) ≡∃y(5 < y ∨y2 + y −2 = 0), as the formula on the
right is the result of renaming of the variable x in the formula on the left.
• ¬∃xP(x) ̸≡∃x¬P(x). For example, “No student passed the exam” should not
be equivalent to “There is a student who did not pass the exam.”
• “The integer x is not less than 0” is not logically equivalent to “The integer x is
greater than or equal to 0.” Mathematically these mean the same (due of the math-
ematical property of the ordering of integers called trichotomy, which arranges all
integers in a line) but not because of logical reasons. Likewise, 2+2=4 is a mathemat-
ical but not a logical truth.
To put it simply: Logic does not know any mathematics. It is important to distinguish
logical from non-logical truths, and logical from non-logical equivalences.
3.4.6
Logical equivalences involving quantifiers
Here is a summary of the most important logical properties relating the quantifiers.
1. To begin with, the negation swaps the quantifiers as follows:
¬∀xA ≡∃x¬A;
¬∃xA ≡∀x¬A.
For example:
• “Not every student wrote the test”
means the same as
“There is a student who did not write the test.”
• “There is no natural number less than 0” means
“Every natural number is not less than 0.”
2. By negating both sides of the equivalences above we find that each of the universal and
existential quantifier is definable in terms of the other:
∀xA ≡¬∃x¬A;
∃xA ≡¬∀x¬A.
3. Universal quantifiers distribute over conjunctions: ∀xP ∧∀xQ ≡∀x(P ∧Q).
4. Existential quantifiers distribute over disjunctions: ∃xP ∨∃xQ ≡∃x(P ∨Q).
5. Assuming that x does not occur free in Q, the following distributive equivalences
hold:
(a) ∀xP ∧Q ≡Q ∧∀xP ≡∀x(P ∧Q);
(b) ∀xP ∨Q ≡Q ∨∀xP ≡∀x(P ∨Q);
(c) ∃xP ∨Q ≡Q ∨∃xP ≡∃x(P ∨Q);
(d) ∃xP ∧Q ≡Q ∧∃xP ≡∃x(P ∧Q).
6. Two nested quantifiers of the same type commute:
∀x∀yA ≡∀y∀xA;
∃x∃yA ≡∃y∃xA.

144
Logic as a Tool
7. Consequently, every two universal closures of a formula are logically equivalent.
8. The case of two different nested quantifiers is more delicate: while
∃x∀yA →∀y∃xA
is valid (check that yourself!), the converse implication
∀y∃xA →∃x∀yA
is not. For instance,
“For every integer x there is an integer y such that x + y = 0”,
which is true, certainly does not imply that
“There is an integer y such that for every integer x it holds that
x + y = 0”,
which is false.
Likewise,
“Every man loves a woman”
does not imply that
“There is a woman whom every man loves.”
Therefore, ∃x∀yA and ∀y∃xA are not equivalent. It is important to remember which
of these imply the other, and why.
3.4.7
Negating first-order formulae: negation normal form
Using the first pair of equivalences listed in Section 3.4.6, in addition to those from
propositional logic, we can now systematically negate not only propositions but also
any first-order sentences. We can transform any first-order sentence into negation nor-
mal form, where negation only occurs in front of atomic formulae. For example, the
sentence
“For every car, there is a driver who, if (s)he can start it, then (s)he can stop it”
can be formalized in a suitable first-order language as
∀x(Car(x) →∃y(Driver(y) ∧(Start(x, y) →Stop(x, y)))).

Understanding First-order Logic
145
Now, we negate:
¬∀x(Car(x) →∃y(Driver(y) ∧(Start(x, y) →Stop(x, y))))
≡∃x¬(Car(x) →∃y(Driver(y) ∧(Start(x, y) →Stop(x, y))))
≡∃x(Car(x) ∧¬∃y(Driver(y) ∧(Start(x, y) →Stop(x, y))))
≡∃x(Car(x) ∧∀y¬(Driver(y) ∧(Start(x, y) →Stop(x, y))))
≡∃x(Car(x) ∧∀y(¬Driver(y) ∨¬(Start(x, y) →Stop(x, y))))
≡∃x(Car(x) ∧∀y(¬Driver(y) ∨(Start(x, y) ∧¬Stop(x, y)))).
Since ¬A ∨B ≡A →B, the last formula is equivalent to
∃x(Car(x) ∧∀y(Driver(y) →(Start(x, y) ∧¬Stop(x, y)))).
The negation of the sentence above is therefore equivalent to:
“There is a car such that every driver can start it and cannot stop it.”
Note that the restricted quantifiers satisfy the same equivalences (and non-equivalences)
mentioned in Section 3.4.6. In particular,
¬∀x(P(x) →A) ≡∃x(P(x) ∧¬A),
¬∃x(P (x) ∧A) ≡∀x(P(x) →¬A),
and hence:
¬∀x ∈X(A) ≡∃x ∈X(¬A),
¬∃x ∈X(A) ≡∀x ∈X(¬A).
For instance, the negation of the definition of a limit of a function:
¬(∀ϵ > 0∃δ > 0(0 < |x −c| < δ →|f(x) −L| < ϵ))
is logically equivalent to
∃ϵ > 0∀δ > 0(0 < |x −c| < δ ∧¬|f(x) −L| < ϵ).
References for further reading
For more examples, discussion and properties of truth, logical validity, consequence, and
equivalence in first-order logic, see Tarski (1965), Kalish and Montague (1980), van Dalen
(1983), Hamilton (1988), Ebbinghaus et al. (1996), Barwise and Echemendy (1999),
Hodges (2001), Smith (2003), Nederpelt and Kamareddine (2004), Chiswell and Hodges
(2007), Ben-Ari (2012), and van Benthem et al. (2014). More specifically, on first-order
logic with equality see van Dalen (1983) and Hamilton (1988).

146
Logic as a Tool
Exercises
In all exercises below, “semantic reasoning” refers to reasoning based on the formal
semantics presented in the advanced track. For the basic track, read “informal reasoning”
instead.
3.4.1
Use semantic reasoning to decide which of the following first-order formulae are
logically valid. For each of those which are not, give a counter-model, that is, a
structure which falsifies it.
(a) ∀x(P(x) ∨¬P(x))
(b) ∀xP(x) ∨∀x¬P (x)
(c) ∀xP(x) ∨¬∀xP(x)
(d) ∃x(P(x) ∨∃x¬P(x))
(e) ∃xP(x) →∀xP(x)
(f) ∃xP(x) →∃yP(y)
(g) ∃x(P(x) →∀yP(y))
(h) ∃x(P(x) →∀xP(x))
(i) ∀x(P(x) →∃yP(y))
(j) ∀x(P(x) →∀yP(y))
(k) ∀x∃yQ(x, y) →∀y∃xQ(y, x)
(l) ∀x∃yQ(x, y) →∀y∃xQ(x, y)
(m) ∀x(∃yQ(x, y) →∃yQ(y, x))
(n) ∀x(∀yQ(x, y) →∃yQ(y, x))
(o) ∃x¬∃yP(x, y) →∀y¬∀xP(x, y)
(p) ∀y¬∀xP(x, y) →∃x¬∃yP(x, y)
(q) (∀x∃yP(x, y) ∧∀x∀y(P(x, y) →P(y, x))) →∃xP(x, x).
(r) (∀x∃yP(x, y) ∧∀x∀y∀z((P(x, y) ∧P(y, z)) →P (x, z))) →
∃xP(x, x).
3.4.2
Show that for any first-order formulae A1, . . . , An, B, the following are equiv-
alent:
(a) A1, . . . , An |= B.
(b) A1 ∧. . . ∧An |= B.
(c) |= A1 ∧· · · ∧An →B.
(d) |= A1 →(A2 →· · · (An →B) . . . ).
3.4.3
Prove Theorem 112 using the truth definition.
3.4.4
Prove Theorem 118 using the truth definition.
3.4.5
Prove Theorem 119.
3.4.6
Let A, B be any first-order formulae. Using semantic arguments show that the
following logical consequences hold.
(a) ∀xA(x) |= ¬∃x¬A(x)
(b) ¬∃x¬A(x) |= ∀xA(x)
(c) ∃xA(x) |= ¬∀x¬A(x)
(d) ¬∀x¬A(x) |= ∃xA(x)
(e) ∃x∃yB(x, y) |= ∃y∃xB(x, y)
(f) ∃x∀yB(x, y) |= ∀y∃xB(x, y).
3.4.7
Let A, B be any first-order formulae and assume x is not free in B. Show that
the following logical consequences hold by giving semantic arguments.
(a) ∀x(A(x) ∨B) |= ∀xA(x) ∨B
(b) ∀xA(x) ∨B |= ∀x(A(x) ∨B)
(c) ∃x(A(x) ∧B) |= ∃xA(x) ∧B
(d) ∃xA(x) ∧B |= ∃x(A(x) ∧B)
(e) ∀x(B →A(x)), B |= ∀xA(x)
(f) ∃xA(x) →B |= ∀x(A(x) →B)
(g) ∃x(A(x) →B), ∀xA(x) |= B
(h) ∃x(B →A(x)), B |= ∃xA(x).

Understanding First-order Logic
147
3.4.8
Using semantic reasoning, determine which of the following logical conse-
quences hold. For those that do, try to give a semantic argument. For those that
do not, construct a counter-model, that is, a structure in which all premises are
true, while the conclusion is false.
(a) ∀xA(x), ∀xB(x) |= ∀x(A(x) ∧B(x))
(b) ∀x(A(x) ∧B(x)) |= ∀xA(x) ∧∀xB(x)
(c) ∀xA(x) ∨∀xB(x) |= ∀x(A(x) ∨B(x))
(d) ∀x(A(x) ∨B(x)) |= ∀xA(x) ∨∀xB(x)
(e) ∀xA(x) →∀xB(x) |= ∀x(A(x) →B(x))
(f) ∀x(A(x) →B(x)) |= ∀xA(x) →∀xB(x)
(g) ∃x(A(x) ∧B(x)) |= ∃xA(x) ∧∃xB(x)
(h) ∃xA(x), ∃xB(x) |= ∃x(A(x) ∧B(x))
(i) ∃x(A(x) ∨B(x)) |= ∃xA(x) ∨∃xB(x)
(j) ∃xA(x) ∨∃xB(x) |= ∃x(A(x) ∨B(x))
(k) ∃x(A(x) →B(x)) |= ∃xA(x) →∃xB(x)
(l) ∃xA(x) →∃xB(x) |= ∃x(A(x) →B(x)).
3.4.9
Prove the claims in Example 113, Section 3.4.4, by semantic reasoning:
(a) The sentence λn = ∃x1 · · · ∃xn(∧1≤i̸=j≤n¬xi = xj) is true in a structure iff
its domain has at least n elements.
(b) The sentence μn = ¬λn+1 ≡μn = ∀x1 · · · ∀xn+1(∨1≤i̸=j≤n+1xi = xj) is
true in a structure iff its domain has at most n elements.
(c) The sentence σn = λn ∧μn is true in a structure iff its domain has exactly
n elements.
3.4.10
Prove that for any formula A(x) containing, among others, the free variable x, the
formula ∃!xA(x) = (A(x) ∧∀y(A(y) →x = y)) states that there is a unique
element in the domain of discourse satisfying A.
3.4.11
Prove Theorem 116 for equivalent replacement by structural induction on the
formula A(x).
3.4.12
Prove each of the following equivalences (listed in Section 3.4.6).
(a) ∀xA ≡¬∃x¬A
(b) ¬∀xA ≡∃x¬A
(c) ∃xA ≡¬∀x¬A
(d) ¬∃xA ≡∀x¬A
(e) ∀xP ∧∀xQ ≡∀x(P ∧Q)
(f) ∃xP ∨∃xQ ≡∃x(P ∨Q)
(g) ∀x∀yA ≡∀y∀xA
(h) ∃x∃yA ≡∃y∃xA.
3.4.13
Use semantic arguments to show that the following logical equivalences hold,
given that x does not occur free in Q.
(a) ∀xP ∧Q ≡Q∧∀xP ≡∀x(P ∧Q)
(b) ∀xP ∨Q ≡Q∨∀xP ≡∀x(P ∨Q)
(c) ∃xP ∨Q ≡Q∨∃xP ≡∃x(P ∨Q)
(d) ∃xP ∧Q ≡Q∧∃xP ≡∃x(P ∧Q)
(e) ∀x(Q →P) ≡Q →∀xP
(f) ∀x(P →Q) ≡∃xP →Q
(g) ∃x(P →Q) ≡∀xP →Q
(h) ∃x(Q →P) ≡Q →∃xP.

148
Logic as a Tool
3.4.14
Use semantic reasoning to decide which of the following logical equivalences
hold. For each of those that do not hold, give a counter-model, that is, a structure
in which one formula is true while the other false.
(a) ∀x(P(x) ∨Q(x)) ≡∀xP(x) ∨∀xQ(x)
(b) ∃x(P(x) ∧Q(x)) ≡∃xP(x) ∧∃xQ(x)
(c) ∀x(P(x) →Q(x)) ≡∀xP(x) →∀xQ(x)
(d) ∃x(P(x) →Q(x)) ≡∃xP(x) →∃xQ(x)
(e) ∀xQ(x, x) ≡∀x∃yQ(x, y)
(f) ∀x∃yQ(x, y) ≡∃xQ(x, x)
(g) ∃x∀yQ(x, y) ≡∃yQ(y, y)
(h) ∀x∀yQ(x, y) ≡∀x∀yQ(y, x)
(i) ∃xP(x) →∀xP(x) ≡∃x¬P(x) →∀x¬P(x).
3.4.15
Negate each of the following formulae and import the negations inside all other
logical connectives.
(a) ∀x(x = x2 →x > 0)
(b) ∀x((x = x2 ∧x > 1) →x2 < 1)
(c) ∀x∃y(x > y →x > y2)
(d) ∀x(x = 0 ∨∃y¬(xy = x))
(e) ∀x∃y(¬x = y →x > y)
(f) ∃x∃y(x > y ∨−x > y)
(g) ∃x(P(x) →∀yP(y))
(h) ∀x(P(x) →Q(x)) →(∀xP(x) →∀xQ(x))
(i) (∃xP(x) →∃xQ(x)) →∃x(P (x) →Q(x)).
3.4.16
For each of the following pairs of statements, check whether either of them logi-
cally implies the other by formalizing them in first-order logic and using semantic
reasoning. If either of the logical consequences does not hold, give an appropriate
counter-model.
(a) A: “Not everybody who is a lawyer is greedy.”
B: “There exists a lawyer and not everybody is greedy.”
(b) A: “No man is happy only when he is drunk.”
B: “There is a happy man and there is a man who is not drunk.”
3.4.17
Formalize the following arguments in FOL by introducing suitable predicates.
Using semantic reasoning, then determine which of them are logically correct.
(a)
All philosophers are humans.
All humans are mortal.
All philosophers are mortal.
(b)
No work is fun.
Some entertainment is work.
Some entertainment is not fun.
(c)
Some negative numbers are rationals.
All integers are rationals.
Some integers are negative numbers.
(d)
All bank managers are rich.
No bank managers are teachers.
No teachers are rich.

Understanding First-order Logic
149
(e)
No mathematicians are stupid.
Some mathematicians are bachelors.
Some bachelors are not stupid.
(f)
No poor people are politicians.
Some politicians are crooks.
Some crooks are not poor people.
(g)
Some penguins are white.
All penguins are birds.
Therefore, some white bird is a penguin.
(h)
Every mother loves some child.
Every mother is a woman.
Some mothers hate watching TV.
Some woman loves some child.
(i) As in (m), but replacing
“Some mothers hate watching TV”
with “All mothers like watching TV.”
(j)
No successful politicians are poor.
Someone is poor only if he is a politician.
Therefore, nobody who is poor is
successful.
(k)
No penguins are white.
Some penguins are fat.
All penguins are birds.
Therefore, no fat bird is white.
(l)
Every bachelor loves a woman he knows.
Every bachelor is a man.
Eve is a woman whom every man knows.
Some bachelor loves some woman.
(m)
No food is better than a good steak.
Stale bread is better than no food.
Stale bread is better than a good steak.
(n) As above, but adding a new premise:
For any x, y, z, if x is better than y and y is better than z then x is
better than z.
(Watch out for the quantifiers!)
Alonzo Church (14.6.1903–11.8.1995) was an American
mathematician and one of the leading logicians of the 20th
century, who made profound contributions to mathematical
logic and the theoretical foundations of computer science.
Church studied and had his academic career at the Mathe-
matics Department of Princeton University where he worked
until 1967. He then formally retired and moved to the Univer-
sity of California, Los Angeles, as Flint Professor of Philos-
ophy and Mathematics for a further 23 years, until his final
retirement at age 87 in 1990.
In the early 1930s Church created lambda-calculus as a formal system of defining
functions that are effectively computable. In connection with his two-part work A Set
of Postulates for the Foundations of Logic, published in 1932–1933, he intended to
develop a set of axioms that “would lead to a system of mathematical logic free of

150
Logic as a Tool
some of the complications entailed by Bertrand Russell’s theory of types, and would
at the same time avoid the well-known paradoxes, in particular the Russell paradox.”
While it later turned out that the original system of lambda-calculus did contain an
inconsistency, the idea flourished and became very influential in computer science
as the basis of functional programming.
In 1936 Church published two papers of fundamental importance to logic and
computability theory. In the first, An Unsolvable Problem of Elementary Number
Theory, he defined the notion of “effective calculability” by identifying it with the
notion of recursive function, or – as it had turned out to be equivalent – of the
lambda-definable function of natural numbers. He then proved that the problem of
deciding whether a given first-order formula of the language with addition and mul-
tiplication is true of the arithmetic of natural numbers is not effectively calculable.
By equating effective calculability with algorithmic solvability, Church put forward
what was later called by Kleene Church’s Thesis. Church essentially proved that
the truth in the elementary arithmetic is algorithmically unsolvable.
In his other historic 1936 paper, A Note on the Entschiedungsproblem (just two
pages long, followed by a two-page correction), Church proved what is now called
Church’s Undecidability Theorem: the problem of deciding validity of formulas
in first-order logic is not effectively calculable, which means – assuming Church’s
Thesis – it is algorithmically unsolvable.
Church had an extremely long and fruitful scientific career, spanning over more
than 70 years since his first publication as an undergraduate student in 1924 until
his last paper, A Theory of the Meaning of Names, published in 1995. In 1956 he
published his classic textbook Introduction to Mathematical Logic, which educated
and influenced generations of logicians. Church was also an extremely popular and
successful supervisor and had 31 doctoral students, many of whom became distin-
guished logicians and computer scientists, including Martin Davis, Leon Henkin,
John Kemeny, Steven Kleene, Michael O. Rabin, Nicholas Rescher, Hartley Rogers,
J. Barkley Rosser, Dana Scott, Reymond Smullyan, and Alan Turing.
Stephen Cole Kleene (05.01.1909–25.01.1994) was an
American mathematician and logician, the founder (along
with Church, Gödel, Turing, and Post) of recursion theory,
one of the main branches of mathematical logic, providing
the logical foundations of the theory of algorithms and com-
putability and of theoretical computer science in general.
Kleene received a PhD in mathematics from Princeton
University in 1934, for the thesis A Theory of Positive
Integers in Formal Logic supervised by Alonzo Church. He also made important
contributions to Church’s lambda-calculus. In 1935, Kleene joined the Mathematics
Department at the University of Wisconsin-Madison, where he spent most of his
academic career. In 1939–40, while he was a visiting scholar at the Institute for

Understanding First-order Logic
151
Advanced Study in Princeton, he laid the foundation for recursion theory. During
WW II, Kleene served in the United States Navy as a lieutenant commander. He
was later an instructor of navigation and a project director at the Naval Research
Laboratory.
Several fundamental concepts in the theories of computability and recursion were
invented by Kleene and named after him, including Kleene’s recursion theorems,
Kleene–Mostowski’s arithmetical hierarchy of sets of natural numbers, Kleene
fixpoint theorem, the Kleene star operation, the Kleene algebra of regular expres-
sions, and Kleene’s theorem of the equivalence between regular expressions and
finite automata.
Kleene wrote, inter alia, two very popular and influential books from which gen-
erations of logicians learned mathematical logic: Introduction to Metamathematics
(1952) and Mathematical Logic (1967). He also made significant contributions to
the foundations of mathematical intuitionism (founded by L. Brouwer) and wrote,
together with Vesley, the book The Foundations of Intuitionistic Mathematics
in 1965.
3.5
Syllogisms
Formal logic was founded by the famous Greek philosopher Aristotle some 2400 years
ago. (See more about Aristotle in the biographical box devoted to him at the end of Section
2.1.) Aristotle’s logical system was not based on propositional logic, which was originally
developed a little later, again in ancient Greece, mainly by Chrysippus from the Stoic
school of logic. Instead, Aristotle’s system of logic was based on a fragment of monadic
first-order logic, consisting of a special kind of logical arguments called syllogisms (from
the Greek word syllogismos, meaning conclusion, inference). Aristotle developed a very
systematic logical theory of syllogisms, now called the Syllogistic. This earliest known
formal logical system remained the gold standard for logical reasoning until the mid-19th
century when the modern algebraic treatment of logic began with the works of Boole,
de Morgan, Schröder, and others. Even though it was later subsumed by first-order logic,
the syllogistic has nevertheless remained an important and useful logical system, not only
for historical but also for methodological and practical computational reasons; I therefore
present it briefly here.
3.5.0.1
Basics of categorical syllogisms
The best-known syllogism is probably:
All men are mortal.
Socrates is a man.
(Therefore) Socrates is mortal.
Even though the notion of syllogism introduced by Aristotle was more general, he
mostly focused on the so-called categorical syllogisms that formally exclude that above
(but can still accommodate it). Categorical syllogisms consist of three propositions, each

152
Logic as a Tool
All S are P
No S are P
Some S are P
Not all S are P
Figure 3.1
The Square of Opposition
of them cast in one of the following four patterns, respectively denoted by the letters A,
E, I, and O, where S and P are unary predicates:
A: “All S are P”, or, in first-order logic: ∀x(S(x) →P(x)).
E: “No S are P”. Equivalently, “All S are not P”, that is, ∀x¬(S(x) ∧P(x)).
I: “Some S are P”, that is, ∃x(S(x) ∧P(x)).
O: “Not all S are P”. Equivalently, “Some S are not P”, that is, ∃x(S(x) ∧¬P(x)).
These four patterns can be arranged in terms of their logical relationships in a so-called
Square of Opposition, presented in Figure 3.1. For further discussion and explanation,
see the references on philosophical logic.
Example 121 For some examples of categorical syllogisms see Exercise 17(a–f) from
Section 3.4.8. Some other examples include:
1.
All humans are mortal.
All logicians are humans.
All logicians are mortal.
2.
No philosophers are crooks.
Some politicians are crooks.
No politicians are philosophers.
3.
No philosophers are crooks.
Some philosophers are politicians.
No politicians are crooks.
4.
All philosophers are clever.
Some clever people are not happy.
Some happy people are not philosophers.
Up to logical equivalence, every categorical syllogistic proposition therefore has the
form:
“Some/All S are/are_not P.”
Syllogistic propositions are therefore simply quantified sentences involving only unary
predicates (and possibly constant symbols), but no functions.
Some standard terminology and conditions of categorical syllogisms are defined in the
following.
• The S and P in any syllogistic proposition are called terms; S is called its subject and
P is called its predicate.

Understanding First-order Logic
153
• A categorical syllogism consists of three syllogistic propositions, the fist two of which
are premises and the third which is the conclusion of the syllogism.
• The predicate of the conclusion is called the major term of the syllogism.
• The subject of the conclusion is called the minor term of the syllogism.
• One of the premises contains the major term and is called the major premise. Usually
this is the first premise.
• The other premise contains the minor term and is called the minor premise. Usually
this is the second premise.
• The two premises must share a third, common term called the middle term, which does
not appear in the conclusion.
• Any combination of the three propositions, each of one of the four types above, is
called a syllogistic form. These forms are named by listing the type letters of the major
premise, the minor premise, and the conclusion. For instance: AAA denotes the form
where each of these is of type A; EIO is the form where the major premise is of type E,
the minor premise is of type I, and the conclusion is of type O, etc.
Furthermore, all syllogistic forms are grouped in four figures, each referring to the
positions of the middle term in the major premise (M) and minor premise (m):
• Figure 1: the middle term is first (the subject) in M and second (the predicate) in m.
• Figure 2: the middle term is second (the predicate) both in M and m.
• Figure 3: the middle term is first (the subject) both in M and m.
• Figure 4: the middle term is second (the predicate) in M and first (the subject) in m.
Syllogism type is now specified as XYZ-N, where each of X,Y, Z is one of the letters
A,E,I,O and N∈{1, 2, 3, 4} indicates the figure. For instance, in each of the examples
above the major premise is the first and the minor premise is the second, so they are of the
type AAA-1, EIE-2, EIE-3, and AOO-4 (check!).
Since in the categorical syllogism XYZ-N each of X,Y,Z, and N can independently
take four different values, there are 44 = 256 possible syllogistic forms. Some of them
represent logically valid inferences, while others do not. Aristotle already inspected most
of the 256 categorical syllogistic forms, identified and classified all valid forms among
them, and constructed an obviously incorrect argument (i.e., a counter-model) for each of
the invalid forms.
3.5.0.2
Testing validity of syllogisms by Venn diagrams
How can we determine which of the syllogistic forms represent logically valid arguments?
An easy method, much more modern than that of Aristotle, is based on the so-called Venn
diagrams13. Venn diagrams represent sets as intersecting circles in a rectangular box, so
the basic operations (union, intersection, complementation) on these sets and the basic
relations of equality, inclusion, and disjointness between them can be easily visualized.
A three-set Venn diagram is depicted in Figure 3.2, where the three circles represent sets
A, B, C and the box represents the whole domain of discourse (the “universal set”). Each
13 Named after the British logician and philosopher John Venn who invented them in around 1880. Read more about
him in the biographical box at the end of the section.

154
Logic as a Tool
A
C
B
C\(A∪B)
A∩B∩C
(A∩B)\C
Figure 3.2
Three-set Venn diagram
S
P
×
S
P
×
S
P
•
S
P
•
Figure 3.3
Venn diagrams depicting the four types of categorical syllogistic propositions. From left to
right: “All S are P”, “All S are not P”, “Some S are P”, and “Some S are not P”.
of the eight regions represents a particular set-theoretic combination of the three sets,
for example, the region in the middle represents A ∩B ∩C while the region above this
represents C \ (A ∪B), etc.
Venn diagrams can be used to represent the terms in a categorical syllogism by sets
and indicate the relations between them expressed by the premises and the conclusion.
In particular, each of the four types of categorical syllogistic propositions can be easily
illustrated by Venn diagrams as in Figure 3.3. The circles correspond to the three terms;
the barcoded and shaded regions indicate those referred to by the premises; a cross × in
a region means that that region is empty; a bullet • in a region means that that region
is non-empty; and no cross or bullet implies that the region could be either empty or
non-empty.
Each of the three propositions of a categorical syllogism can therefore be represented
on a three-set Venn diagram, corresponding to the three terms. For instance, the syllo-
gisms in Example 121(1) and 121(3) are represented on Figure 3.4 as follows. In the
Venn diagram on the left corresponding to Example 121(1), the circle M represents the
set of mortals (the major term), the circle L represents the set of all logicians (the minor
term), and the circle H represents the set of all humans (the middle term). The diagram
depicts the two premises of Example 121(1) by indicating the regions H \ M that corre-
sponds to “humans who are not mortals”, and L \ H that corresponds to “logicians who
are not humans”, as empty. Likewise, in the Venn diagram on the right corresponding to
Example 121(3), C represents the set of crooks (the major term), Po represents the set of
all politicians (the minor term), and Ph represents the set of all philosophers (the middle
term). The diagram depicts the two premises of Example 121(3) by indicating the region
Ph ∩C corresponding to all “philosophers who are crooks” as empty, while indicating
the region Ph ∩Po corresponding to all “philosophers who are politicians” as non-empty
(the bullet).

Understanding First-order Logic
155
L
H
M
×
×
×
×
Ph
C
Po
•
×
?
×
Figure 3.4
Venn diagrams for the syllogisms in Exercise 121(1) (left) and 121(3) (right)
Once the Venn diagram representing the relations stated by the two premises of the
syllogism is constructed, the testing for validity is immediate: check, by inspection of the
diagram, whether the relation stated by the conclusion must also hold. If so, the syllogism
is valid; otherwise, it is not. It is as simple as that!
For example, in Example 121(1), the conclusion claims that in the diagram on the left
on Figure 3.4 it must be the case that L ⊆M, that is, the non-barcoded part of L (where
all elements of L must live), which is the shaded triangle in the middle, must be included
in M. It is included in M, and so this syllogism is valid.
On the other hand, in Example 121(3) the conclusion claims that Po ∩C = ∅but in
the diagram the right (shaded) part of that region is not depicted as empty and therefore it
need not be empty. Hence, this syllogism is invalid.
The proof of correctness of the Venn diagrams method is not very difficult and is left
as an exercise.
Non-categorical syllogisms of the “Socrates is mortal” type mentioned earlier can also
be easily treated here. By regarding proper names, such as Socrates, as singleton predi-
cates/sets, the individual Socrates can be identified with the property of “being Socrates.”
I leave the easy details of that to the reader.
It turns out that of the 256 categorical syllogistic forms, there are 24 valid forms under
the natural assumption of the meaning of the types A and E discussed further. In the Middle
Ages these were given special Latin names (some of which appear in the literature with
slight variations), each containing three vowels which were used as mnemonics of the
types of the respective forms. (Some of the other letters in these mnemonic words also
contain some information.) The 24 valid forms are as follows.
Figure 1
Figure 2
Figure 3
Figure 4
1. AAA-1 Barbara
2. EAE-1 Celarent
3. AII-1 Darii
4. EIO-1 Ferio
5. AAI-1 Barbari
6. EAO-1 Celaront
1. AEE-2 Camestres
2. EAE-2 Cesare
3. AOO-2 Baroko
4. EIO-2 Festino
5. EAO-2 Cesaro
6. AEO-2 Camestrop
1. AII-3 Datisi
2. AAI-3 Darapti
3. IAI-3 Disamis
4. EAO-3 Felapton
5. EIO-3 Ferison
6. OAO-3 Bocardo
1. AAI-4 Bramantip
2. AEE-4 Camenes
3. IAI-4 Dimaris
4. EAO-4 Fesapo
5. EIO-4 Fresison
6. AEO-4 Camenop

156
Logic as a Tool
It is important to note that the validity of some of these forms depends on an assump-
tion which Aristotle made about the interpretation of the quantifiers in the types A and
E, known as Existential Import: in the propositions “All S are P” and “No S are P” the
predicate S is non-empty, that is, there are S objects. Under this assumption, these univer-
sal proposition types, which are at the top of the Square of Opposition, logically imply
respectively their existential counterparts at the bottom, “Some S are P” and “Some S are
not P.”
The Existential Import assumption is compliant with the normal natural language usage
of such expressions, but is not formally justified by the semantics of first-order logic that
we have assumed; we must therefore handle it with special care and will only apply it
in the context of syllogisms. Without this assumption, nine of the syllogistic forms listed
above cease to be valid. I leave it as an exercise for the reader to identify these.
Note that the syllogistic forms are not formulae but inference rules. It is however pos-
sible to develop a deductive system for deriving some valid syllogisms from others; see
the references for further details.
To summarize, categorical syllogisms capture a natural and important fragment of log-
ical reasoning in first-order logic involving only unary predicates and constant symbols.
They do not capture simple sentences involving binary predicates, such as “Every man
loves a woman”, but have one great technical advantage: the simple decision procedure
for testing valid syllogisms given by the method of Venn diagrams. As I explain in the
next chapter, no such decision procedure exists for first-order logic in general, even when
the language involves a single binary predicate.
References for further reading
For more details on syllogisms see Carroll (1897), Barwise and Echemendy (1999), and
van Benthem et al. (2014), plus many books on philosophical logic.
Exercises
3.5.1
For each of the syllogisms listed in Example 121, verify its type and figure as
claimed in the text.
3.5.2
For each of the following syllogisms (taken from Section 3.4.8, Exercise 17(a–f))
identify its major, minor, and middle terms and its type and figure.
(a)
All philosophers are humans.
All humans are mortal.
All philosophers are mortal.
(b)
No work is fun.
Some entertainment is work.
Some entertainment is not fun.
(c)
Some negative numbers are rationals.
All integers are rationals.
Some integers are negative numbers.
(d)
All bank managers are rich.
No bank managers are teachers.
No teachers are rich.

Understanding First-order Logic
157
(e)
No mathematicians are stupid.
Some mathematicians are bachelors.
Some bachelors are not stupid.
(f)
No poor people are politicians.
Some politicians are crooks.
Some crooks are not poor people.
3.5.3
Using Venn diagrams, show that each of the 24 syllogistic forms listed in Section
3.5 represent logically valid arguments.
3.5.4
Identify the 9 syllogistic forms among those 24 which are only valid conditionally
on the assumption of Existential Import.
3.5.5
Prove the correctness of the method of Venn diagrams for testing validity of cate-
gorical syllogisms.
3.5.6
Using Venn diagrams, determine whether each the syllogisms listed in Section
3.5.1, Exercise 2 is valid.

158
Logic as a Tool
John Venn (4.08.1834–4.04.1923) was an English logician and
philosopher, mostly known for introducing the Venn diagrams
used in set theory, probability, logic, statistics, and computer
science.
Venn studied at Gonville and Caius College, Cambridge and
worked there as a lecturer, and later professor, of moral science,
logic, and probability theory during most of his career. In 1903
he was elected President of the College and held that post until
his death.
In 1866 Venn published his book The Logic of Chance, in which he developed the
frequency theory of probability. He developed further George Boole’s theory in his
1881 book Symbolic Logic, where he applied his diagrams which he had invented
for illustration in his teaching.
Lewis
Carroll
(Charles
Lutwidge
Dodgson,
7.1.1832–
14.1.1898) was an English writer, mathematician, logician,
and photographer, most famous for his children’s books Alice’s
Adventures in Wonderland and Through the Looking-Glass,
published under the pen name Lewis Carroll.
Lewis Carroll was talented and creative since his early
childhood in many areas, including mathematics, poetry, sto-
rytelling, and designing various new games. After completing
his university studies in Oxford he became a lecturer at Christ Church College
which was his main job for life, although he did not like it much.
Stammering and other medical problems since early age made him feel less at ease
among adults, but he was quite relaxed with children for whom he enjoyed creating
and telling stories, including his famous books about Alice written for the daughter
of the dean of his college.
Lewis Carroll did some research in geometry, algebra, logic, and recreational
mathematics, producing several books and articles under his real name. In The
Alphabet-Cipher (1868) he described a well-known cryptographic scheme, and in
a paper on The Principles of Parliamentary Representation (1884) he proposed a
voting system, still known as Dodgson’s method. He was also an avid popularizer
of logic, on which he wrote the book The Game of Logic (1887, still popular today)
and the textbook Symbolic Logic (1896), where he explains the theory of syllogisms
with many examples. In the story What the Tortoise Said to Achilles (1895), he
uses a dialogue between the Tortoise and Achilles to demonstrate the problem with
infinite regress in logical deduction.

4
Deductive Reasoning in First-order
Logic
Validity, logical consequence, and equivalence in first-order logic can be established by
using deductive systems, like for propositional logic.
In this chapter I present and illustrate with examples extensions of the propositional
deductive systems introduced in Chapter 2, obtained by adding additional axioms (for
Axiomatic Systems) and inference rules (for Propositional Semantic Tableaux and Natural
Deduction) for the quantifiers or additional procedures handling them (for Resolution),
that are sound and complete for first-order logic.
That such sound and complete deductive systems for first-order logic exist is not a
priori obvious at all. The first proof of completeness of (an axiomatic) deductive system
for first-order logic was first obtained by Kurt Gödel1 in his doctoral thesis in 1929, and is
now known as Gödel’s Completeness Theorem. In supplementary Section 4.6, generic
proofs of soundness and completeness of the deductive systems are sketched for first-order
logic introduced here.
Differently from the propositional case, however, none of these deductive systems can
be guaranteed to always terminate their search for a derivation, even if such a derivation
exists. This can happen, for instance, when the input formula is not valid but can only be
falsified in an infinite counter-model.
This is not an accidental shortcoming of the deductive systems. In fact, one of the most
profound and important results in mathematical logic, proved in 1935 by Alonso Church
and independently by Alan Turing in 19362, states that the problem of whether a given
first-order sentence is valid is not algorithmically decidable. It is now known as Church’s
Undecidability Theorem.
Deciding logical consequence in first-order logic is therefore also not possible by a
purely algorithmic procedure. Therefore, no sound, complete, and always terminating
deductive system for first-order logic can be designed.
1 See more about Gödel and his groundbreaking results in the biographical box at the end of this chapter.
2 Read more about Church in the biographical box at the end of Section 3.4 and Turing in the box at the end of
Section 5.4.
Logic as a Tool: A Guide to Formal Logical Reasoning, First Edition. Valentin Goranko.
© 2016 John Wiley & Sons, Ltd. Published 2016 by John Wiley & Sons, Ltd.

160
Logic as a Tool
4.1
Axiomatic system for first-order logic
We now extend the propositional axiomatic system presented in Section 2.2 to first-order
logic by adding axioms and rules for the quantifiers. I denote the resulting axiomatic sys-
tem again by H.
In what follows, ∀is regarded as the only quantifier in the language and ∃is definable
in terms of it. An equivalent system can be obtained by regarding both quantifiers present
in the language and adding the axiom ∃xA ↔¬∀x¬A.
4.1.1
Axioms and rules for the quantifiers
Additional axiom schemes
(Ax∀1) ∀x(A(x) →B(x)) →(∀xA(x) →∀xB(x));
(Ax∀2) ∀xA(x) →A[t/x] where t is any term free for substitution for x in A;
(Ax∀3) A →∀xA where x is not free in the formula A.
As an easy exercise, show that all instances of these axiom schemes are valid.
Additional rule
We must also add the following rule of deduction, known as Generalization:
A
∀xA
where A is any formula and x any variable. Note that x may occur free in A, but need not
occur free there in order to apply that rule. Note also that this rule does not read as
“Assuming A is true, conclude that ∀xA is also true”
but rather as
“If A is valid in the given model (respectively, logically valid) then conclude
that ∀xA is also valid in the given model (respectively, logically valid)”
that is, the rule preserves not truth but validity (in a model, or logical validity).
Respectively, the syntactic/deductive reading of the Generalization rule is:
“If A is derived, then derive ∀xA.”
4.1.2
Derivations from a set of assumptions
Note that if the Generalization rule is used unrestrictedly in derivations from a set of
assumptions, it would derive A ⊢H ∀xA for example, which should not be derivable
because A ⊭∀xA. To avoid such unsound derivations when adding the Generalization

Deductive Reasoning in First-order Logic
161
rule to the definition of derivation in H from a set of assumptions Γ, we include the pro-
viso: if the formula A is already in the sequence and x does not occur free in any of the
formulae of Γ, then ∀xA can be added to the sequence.
More generally, we can extend the Generalization rule to work with assumptions on the
left, as follows:
A1, . . . , An ⊢H A
∀xA1, . . . , ∀xAn ⊢H ∀xA.
In the case where x does not occur free in A1, . . . , An, the vacuous quantification on the
left can be achieved by using Axiom (Ax∀3), thus justifying the definition above.
With the refined definition of derivations from a set of assumptions, the Deduction
Theorem still holds for the extension of H to first-order logic. The proof of this claim is
left as an exercise.
Here is an example of derivations in H. Check that the rules for the quantifiers have
been applied correctly.
Example 122 Derive ∀x(Q →P(x)), ∃x¬P(x) ⊢H ¬Q, where x is not free in Q.
Eliminating ∃, we are to derive ∀x(Q →P(x)), ¬∀x¬¬P(x) ⊢H ¬Q.
1. ⊢H P(x) →¬¬P(x)
derived in the Propositional H.
2. ⊢H ∀x(P(x) →¬¬P(x))
from 1 and Generalization.
3. ∀x(P(x) →¬¬P(x)) ⊢H ∀xP(x) →∀x¬¬P(x)
by Axiom (Ax∀1).
4. ⊢H ∀xP(x) →∀x¬¬xP(x)
by 2, 3, Deduction Theorem, and Modus Ponens.
5. ∀x(Q →P(x)) ⊢H ∀xQ →∀xP(x)
by Axiom (Ax∀1).
6. ∀x(Q →P(x)), ∀xQ ⊢H ∀xP(x)
by 5 and Deduction Theorem.
7. Q ⊢H ∀xQ
by Axiom (Ax∀3).
8. ∀x(Q →P(x)), Q ⊢H ∀xP(x)
by 6,7 and Deduction Theorem.
9. ⊢H ¬∀x¬¬P (x) →¬∀xP(x)
by 4 and contraposition
(derived in the propositional H).
10. ∀x(Q →P(x)), ¬∀xP(x) ⊢H ¬Q
by 8 and contraposition.
11. ∀x(Q →P(x)), ¬∀x¬¬P(x) ⊢H ¬Q
by 9, 10, and Deduction Theorem.
4.1.3
Extension of the axiomatic system H with equality
Recall that the equality symbol = is a special binary relational symbol, always to be inter-
preted as the identity of objects in the domain of discourse. For that standard meaning of
the equality to be captured in the axiomatic system, the following additional axioms for
the equality, mentioned in Section 3.4.4, are needed (though, as noted there, it is not suffi-
cient to express the claim that it is the identity relation), where all variables are implicitly
universally quantified.
Axioms for the equality
(Ax=1) x = x;
(Ax=2) x = y →y = x;

162
Logic as a Tool
(Ax=3) x = y ∧y = z →x = z;
(Axf)
x1 = y1 ∧. . . ∧xn = yn →f(x1, . . . , xn) = f(y1, . . . , yn)
for
n-ary
functional symbol f;
(Axr) x1 = y1 ∧. . . ∧xn = yn →(p(x1, . . . , xn) →p(y1, . . . , yn)) for n-ary predi-
cate symbol p.
An example of a derivation in the extension H with equality is provided in the
following.
Example 123 Derive ∀x∀y(x = y →(P(f(x)) ↔P(f(y)))) where P is a unary pred-
icate symbol and f is a unary function symbol.
1. ⊢H ∀x∀y(x = y →(P (x) →P(y)))
by Axiom Axr.
2. ⊢H ∀y(f(x) = y →(P (f(x)) →P (y)))
by 1, Axiom Ax∀2, and MP.
3. ⊢H f(x) = f(y) →(P(f(x)) →P(f(y)))
by 2 and Ax∀2.
4. ⊢H ∀x∀y(x = y →y = x)
by Axiom Ax=2.
5. ⊢H ∀y(f(x) = y →y = f(x))
by 4 and Axiom Ax∀2.
6. ⊢H f(x) = f(y) →f(y) = f(x)
by 5 and Axiom Ax∀2.
7. ⊢H ∀y∀x(y = x →(P (y) →P(x)))
by Axiom Axr.
8. ⊢H ∀x(f(y) = x →(P(f(y)) →P (x)))
by 7, Axiom Ax∀2, and MP.
9. ⊢H f(y) = f(x) →(P(f(y)) →P(f(x)))
by 8 and Ax∀2.
10. f(x) = f(y) ⊢H (P(f(x)) →P(f(y)))
by 3 and Deduction Theorem.
11. f(x) = f(y) ⊢H f(y) = f(x)
by 6 and Deduction Theorem.
12. f(x) = f(y) ⊢H (P(f(y)) →P(f(x)))
by 9, 11, and MP.
13. f(x) = f(y) ⊢H (P(f(x)) →P(f(y))) ∧(P(f(y)) →P (f(x)))
by 10, 12,
and Propositional logic.
14. f(x) = f(y) ⊢H (P(f(x)) ↔P(f(y)))
by 13 and definition of ↔.
15. ⊢H f(x) = f(y) →(P(f(x)) ↔P(f(y)))
by 14 and Deduction Theorem.
16. ⊢H ∀y(f(x) = f(y) →(P(f(x)) ↔P(f(y))))
by 15 and Generalization.
17. ⊢H ∀x∀y(f(x) = f(y) →(P(f(x)) ↔P(f(y))))
by 16 and Generalization.
QED.
Example 124 An old jazz song written by Jack Palmer says
“Everybody loves my baby, but my baby don’t love nobody but me.”
Properly formalized (after correcting the grammar) in first-order logic, it states:
Me-and-my-baby = ∀xL(x, MyBaby) ∧∀y(¬y = Me →¬L(MyBaby, y))
I will derive in H with equality that the claim above implies “I am my baby,” that is,
∀xL(x, MyBaby) ∧∀y(¬y = Me →¬L(MyBaby, y)) ⊢H MyBaby = Me.

Deductive Reasoning in First-order Logic
163
1. Me-and-my-baby ⊢H ∀xL(x, MyBaby)
by Propositional logic.
2. Me-and-my-baby ⊢H L(MyBaby, MyBaby)
by 1 and Axiom Ax∀2.
3. Me-and-my-baby ⊢H ∀y(¬y = Me →¬L(MyBaby, y))
by Propositional logic.
4. Me-and-my-baby ⊢H ¬MyBaby = Me →¬L(MyBaby, MyBaby)
by 3 and
Axiom Ax∀2.
5. Me-and-my-baby ⊢H L(MyBaby, MyBaby) →MyBaby = Me
by 4 and
Propositional logic.
6. Me-and-my-baby ⊢H MyBaby = Me QED.
by 2, 5, and MP.
The following is a deductive analogue in H of the Equivalent replacement theorem
116 and a generalization of the equality axioms, stating that equal terms can provably be
replaced for each other in any formula.
Theorem 125 (Equivalent replacement) For any formula A(x) and terms s, t free for
x in A, the following is derivable in H:
⊢H s = t →A[s/x] ↔A[t/x].
The proof is by structural induction on the formula A(x), left as an exercise.
Theorem 126 (Soundness and completeness of H) Each of the axiomatic systems H
for first-order logic with and without equality is sound and complete, that is, for every
first-order formula A1, . . . , An, C of the respective language (respectively, with or with-
out equality):
A1, . . . , An, ⊢H C iff A1, . . . , An, |= C.
The proof for H with equality is outlined in Section 4.6.
References for further reading
For further discussion and examples on derivations in axiomatic systems for first-order
logic, see Tarski (1965), Shoenfield (1967), Hamilton (1988), Fitting (1996), Mendelson
(1997), Enderton (2001). For discussion and proofs of Church’s Undecidability Theorem,
see Shoenfield (1967), Jeffrey (1994), Ebbinghaus et al. (1996), Enderton (2001), Hedman
(2004), and Boolos et al. (2007).
Exercises
4.1.1
Show that all instances of the axiom schemes listed in Section 4.1.1 are logically
valid, and that the Generalization rule preserves validity in a structure and hence
logical validity.

164
Logic as a Tool
4.1.2
Prove the logical correctness of the inference rule
A1, . . . , An ⊢A
∀xA1, . . . , ∀xAn ⊢∀xA
by showing that if A1, . . . , An |= A then ∀xA1, . . . , ∀xAn |= ∀xA.
4.1.3
Prove the Deduction Theorem for the extension of H to first-order logic, with
respect to the definition of derivations from a set of assumptions given in the text.
For each of the following exercises on derivations in the axiomatic system H,
you may use the Deduction Theorem.
4.1.4
Derive the following deductive consequences in H, where P is a unary predicate.
(a) ⊢H ∀xP(x) →∀yP(y)
(b) ⊢H ∃xP(x) →∃yP(y)
(c) ∀xA(x) ⊢H ¬∃x¬A(x)
(d) ¬∃x¬A(x) ⊢H ∀xA(x)
(e) ∀x∀yA(x, y) ⊢H ∀y∀xA(x, y)
(f) ∃x∃yA(x, y) ⊢H ∃y∃xA(x, y).
4.1.5
Suppose x is not free in Q. Prove the validity of the following logical consequences
by deriving them in H.
(a) ∀x(P(x) ∨Q) ⊢H ∀xP(x) ∨Q
(b) ∀xP(x) ∨Q ⊢H ∀x(P(x) ∨Q)
(c) ∃x(P(x) ∧Q) ⊢H ∃xP(x) ∧Q
(d) ∃xP(x) ∧Q ⊢H ∃x(P(x) ∧Q)
(e) ∃xP(x) →Q ⊢H ∀x(P(x) →Q)
(f) ∀x(Q →P (x)), Q ⊢H ∀xP(x)
(g) ∃x(Q →P (x)), Q ⊢H ∃xP(x)
(h) ∃x(P(x) →Q), ∀xP(x) ⊢H Q
(i) ∃x(¬P(x) ∨Q) ⊢H ∀xP(x) →Q
(j) ∃x(P(x) ∨¬Q) ⊢H Q →∃xP(x).
4.1.6
Determine which of the following logical consequences hold by searching for a
derivation in H. For those that you find not derivable in H, consider A and B as
unary predicates and look for a counter-model, that is, a structure and assignment
in which all premises are true while the conclusion is false.
(a) ∀xA(x) ∧∀xB(x) |= ∀x(A(x) ∧B(x))
(b) ∀xA(x) ∨∀xB(x) |= ∀x(A(x) ∨B(x))
(c) ∀x(A(x) ∧B(x)) |= ∀xA(x) ∧∀xB(x)
(d) ∀x(A(x) ∨B(x)) |= ∀xA(x) ∨∀xB(x)
(e) ∀xA(x) →∀xB(x) |= ∀x(A(x) →B(x))

Deductive Reasoning in First-order Logic
165
(f) ∀x(A(x) →B(x)) |= ∀xA(x) →∀xB(x)
(g) ∀x(A(x) →B(x)) |= ∃xA(x) →∃xB(x)
(h) ∃xA(x) ∧∃xB(x) |= ∃x(A(x) ∧B(x))
(i) ∃x(A(x) ∧B(x)) |= ∃xA(x) ∧∃xB(x)
(j) ∃x(A(x) ∨B(x)) |= ∃xA(x) ∨∃xB(x)
(k) ∃xA(x) ∨∃xB(x) |= ∃x(A(x) ∨B(x))
(l) ∃xA(x) →∃xB(x) |= ∃x(A(x) →B(x))
(m) ∃x(A(x) →B(x)) |= ∃xA(x) →∃xB(x)
(n) ∀y∃x(P(x) →Q(y)) |= ∀xP(x) →∀zQ(z)
(o) ∃y∀x(P(x) →Q(y)) |= ∃xP(x) →∃zQ(z)
(p) |= ∃x(P(x) →∀yP(y)).
4.1.7
Formalize the following arguments in first-order logic and try to prove their logical
correctness by deriving them in H. For those that you find not derivable in H, look
for a counter-model.
(a)
All logicians are clever.
All clever people are rich.
All logicians are rich.
(b)
All penguins are birds.
No penguins can fly.
Every white bird can fly.
Therefore, no penguin is white.
(c)
No lion eats birds.
Every penguin is a bird.
Simba is a lion.
Therefore some lion eats no penguins.
4.1.8
Prove the validity of all axioms for equality.
4.1.9
Derive the following in the extension of H with equality (universal quantification
is assumed wherever omitted):
(a) x1 = y1 ∧x2 = y2 →g(f(x1, f(x2))) = g(f(y1, f(y2)));
(b) ∀x(x = f(x) →(P(f(f(x))) →P(x))); and
(c) ∀x∀y(f(x) = y →g(y) = x) →∀z(g(f(z)) = z).
For more exercises on derivations with equality in set theory, see Section 5.2.7.

166
Logic as a Tool
David Hilbert (23.01.1862–14.02.1943) was one of the most
prominent and influential mathematicians of the late 19th and
early 20th centuries.
Hilbert studied at the University of Königsberg and in
1895 became Professor in Mathematics at the University of
Göttingen – the world-leading research center for mathematics
at that time – where he remained for the rest of his life. He
established a famous school there and mentored a great number
of students and collaborators who later also became prominent mathematicians,
including the chess champion Emanuel Lasker, Ernst Zermelo, Otto Blumenthal,
Felix Bernstein, Hermann Weyl, Richard Courant, Erich Hecke, Hugo Steinhaus,
and Wilhelm Ackermann. At some period John von Neumann was his assistant and
Alonzo Church joined him as a visiting researcher.
Hilbert’s knowledge of mathematics was encyclopedic and his contributions were
wide ranging from mathematical logic and foundations of mathematics to the theory
of invariants, algebraic number fields, the introduction of (now known as) Hilbert
spaces, and the founding of the field of functional analysis as the main instrument of
modern mathematical physics.
At the end of the 19th century Hilbert analyzed and revised systematically
Euclid’s postulates of geometry and their consequences. He modernized and re-cast
Euclid’s seminal work in his book Foundations of Geometry (1899), with which
he put the foundations of geometry in a formal axiomatic setting. Starting with
this work Hilbert also promoted the formal axiomatic approach to mathematics,
essentially proclaimed in the famous Hilbert’s program for formalizing the entire
body of mathematics as the most influential development in the foundations
of mathematics throughout the 20th century. By launching and pursuing that
program, Hilbert became one of the founders of proof theory, which he called
metamathematics, a major branch of mathematical logic.
In his historical speech The Problems of Mathematics delivered at the Second
International Congress of Mathematicians in Paris in 1900, Hilbert presented his
famous list of 23 unsolved mathematical problems, the solutions of which would,
according to Hilbert, guide and chart the development of mathematics in the forth-
coming 20th century. Many of Hilbert’s 23 problems were solved during the 20th
century and their solutions did influence much of the development of mathematics
at the time. Others were foundational problems, such as the Continuum Hypothesis
and the existence of well-ordering of the reals, the solutions of which were proved
to essentially depend on the axiomatic foundations of set theory. Hilbert’s list also
included some still famously unsolved problems, such as Goldbach’s conjecture and
the Riemann hypothesis.
Hilbert was baptized and raised in the Reformed Protestant Church, but he later left
the church and became an agnostic. He argued that mathematical truth was indepen-
dent of the existence of God or other a priori assumptions. In response to the Latin
maxim Ignoramus et ignorabimus (We do not know, we shall not know), he stated his
mathematician’s famous credo in his retirement address in 1930: Wir müssen wissen.
Wir werden wissen. (We must know. We will know.). This credo became the epitaph
on his tombstone in Göttingen.

Deductive Reasoning in First-order Logic
167
4.2
Semantic Tableaux for first-order logic
We now extend the method of Propositional Semantic Tableaux to first-order logic by
adding rules for the quantifiers.
Quantifier rules for Semantic Tableaux
(∀T)
∀xA(x) : T
A[t/x] : T
for any ground term(∗) t
for any ground term(∗) t
(∃T)
∃xA(x) : T
A[c/x] : T
for a new constant symbol c
not yet occurring on this branch(∗∗)
for a new constant symbol c
not yet occurring on this branch(∗∗)
(∃F)
∃xA(x) : F
A[t/x] : F
(∀F)
∀xA(x) : F
A[c/x] : F
(∗)Recall that a ground term is a term with no variables. In order to always enable
this rule we assume that there is at least one constant symbol in the language.
(∗∗)This rule may only be applied once for the given formula on each branch.
The rules are quite natural and simple. Their correctness, which will be stated more
formally further, follows from some semantic properties of quantifiers and logical con-
sequence that are listed in Theorem 112. However, some discussion is in order. I defer
the discussion on the quantifier rules (∀T) and (∃F) to the end of this section. Let me first
explain the proviso (∗∗). It is only needed to prevent “useless” redundant applications of the
rules (∃T) and (∀F). Note that without that proviso, each of these rules could be applied
in principle infinitely many times: once for every constant symbol in the language that
does not yet occur on the current branch. Intuitively, however, such applications should
be redundant, because we only need one witness of the truth (respectively, of falsity) of
the formula assumed to be true in the rule (∃T) (respectively, false in (∀F)), if such a wit-
ness exists. We therefore only need to introduce one name for such a witness in any given
branch of the tableau. If the assumption of the existence of such a witness can lead to a
contradiction and close the branch, then one application of the rule should suffice; other-
wise, repeated applications of the rule will have no effect, except to make it impossible to
declare the branch, and the tableau construction, saturated.
We treat Semantic Tableaux for first-order logic as a deductive system, denoted again as
ST. Now, the idea of derivation in Semantic Tableaux for first-order logic is essentially the
same as in propositional logic: in order to prove that A1, . . . , An |= C, we search system-
atically for a counter-model, that is, a structure where A1, . . . , An and ¬C are satisfied
simultaneously. Closed and open branches and tableaux are defined as in Propositional
Semantic Tableaux, but with the more refined notion of saturation of first-order tableaux

168
Logic as a Tool
as discussed above. A closed tableau with the formulae A1, . . . , An, ¬C at the root certi-
fies that there cannot be such a counter-model, and that constitutes a derivation in Semantic
Tableaux of the logical consequence A1, . . . , An |= C, denoted A1, . . . , An ⊢ST C. In
particular, a closed tableau with the formula ¬C at the root, denoted ⊢ST C, certifies that
C is not falsifiable; it is therefore valid. On the other hand, a tableau with an open and
saturated branch contains the information needed for the construction of a counter-model
for the formula or set of formulae in the label of the root.
Unlike propositional logic, where every tableau either closes or else saturates and ter-
minates as open, a third case is also possible where the tableau does not close or produce
an open and saturated branch. This case will be illustrated in the last example below and
termination is discussed at the end of the section.
Finally, note that the unsigned version of Semantic Tableaux for first-order logic can
easily be obtained by modifying the quantifier rules to handle negated quantifiers and
adding these to the unsigned version of propositional Semantic Tableaux.
4.2.1
Some derivations in Semantic Tableaux
Example 127 Using Semantic Tableaux, check if ∀x(Q →P (x)) ⊨Q →∀xP(x),
where x does not occur free in Q.
∀x(Q →P (x)) : T,Q →∀xP (x) : F
∀x(Q →P (x)) : T,Q : T, ∀xP (x) : F
p : F, ¬q : F
P (c): F
Q →P (c) : T
Q : F
P (c) : T
×
×
The tableau above closes, implying that ∀x(Q →P(x)) ⊢ST Q →∀xP(x), hence
∀x(Q →P(x)) ⊨Q →∀xP(x) holds.
Where was the assumption that x is not free in Q used?
Example 128 Using Semantic Tableaux, show that ⊨¬∀x∀y(P(x) ∧¬P(f(y))), where
P is a unary predicate symbol and f is a unary functional symbol.

Deductive Reasoning in First-order Logic
169
¬∀x∀y(P (x) ∧¬ P (f (y))) : F
∀x∀y(P (x) ∧¬ P (f (y))) : T
∀y(P (f (c)) ∧¬ P (f (y))) : T
P (f (c)) ∧¬ P (f (c)) : T
P (f (c)) : T, ¬P (f (c)) : T
P (f (c)) : T, P (f (c)) : F
×
The tableau closes, so ⊢ST ¬∀x∀y(P(x) ∧¬P(f(y))), hence ⊨¬∀x∀y(P(x) ∧
¬P(f(y))). In this example I first applied the rule (∀T) in a somewhat artificial way, by
introducing a new constant symbol and guessing the ground term f(c) to substitute for x.
Such guessing would be avoided in the Free Variable version of the Semantic Tableaux
method, mentioned at the end of this section.
Example 129 Using Semantic Tableaux, check if
∃x(A(x) →B(x)), ∃xA(x) |= ∃xB(x).
∃x(A(x) →B(x)) : T, ∃xA(x) : T, ∃xB (x) : F
A(c1) →B(c1) : T
A(c2) : T
B(c1) : F
B(c2) : F
A(c1) : F
B(c1) : T
⃝
×

170
Logic as a Tool
Note that, due to the proviso (∗), the left-hand branch in this tableau can only be extended
further by applying the rule ∃F to the signed formula ∃xB(x) : F to produce instances
of the type B(c) : F for new constant symbols c (as there are no function symbols in the
formula); these clearly cannot produce a contradictory pair of formulae, so we can treat
such applications of the rule as redundant. The left-hand branch therefore remains open,
and the tableau does not close; ∃x(A(x) →B(x)), ∃xA(x) |= ∃xB(x) therefore cannot
be derived in ST, and this consequence is not valid.
Indeed, the necessary information for building a counter-model can be collected from
the open branch of the tableau. It must have at least two elements, say a1 and a2, interpret-
ing c1 and c2, respectively, and these should suffice because no other names of elements
are mentioned in the tableau. The predicate A must then be false for a1 and true for a2, that
is, its interpretation in the domain {a1, a2} is {a2}; the predicate B must be false for both
a1 and a2, that is, its interpretation in the domain {a1, a2} is ∅. Indeed, it is easy to check
that the resulting model falsifies the logical consequence ∃x(A(x) →B(x)), ∃xA(x) |=
∃xB(x).
Example 130 Using Semantic Tableaux, check if ∀x∃yA(x, y) |= ∃xA(x, c0), for a
constant symbol c0.
∀x∃yA(x, y ) : T, ∃xA(x, c 0) : F
A(c0, c0) : F
∃yA(c0, y) : T
A(c0, c1) : T
A(c1, c0) : F
∃yA(c1, y) : T
A(c1, c2) : T
A(c2, c0) : F
...
It should now be evident that this tableau can be extended forever without either clos-
ing or saturating. That indicates that ∀x∃yA(x, y) |= ∃xA(x, c0) cannot be derived in
Semantic Tableaux, hence this consequence is not valid. Indeed, a simple counter-model
can be defined in the structure of natural numbers N where A(x, y) is interpreted as x < y

Deductive Reasoning in First-order Logic
171
and c0 is interpreted as 0. However, this fact cannot be formally established at any finite
stage of the extension of the tableau. We therefore see that the construction of a first-order
tableau does not always terminate.
4.2.2
Semantic Tableaux for first-order logic with equality
The system of Semantic Tableaux presented here does not involve rules for the equality,
but such rules can be added to produce a sound and complete system for first-order logic
with equality.
Semantic Tableaux rules for the equality
(=T)
A[t/x] : T
A[s/x] : T
for any terms s, t free for x in A such that
s = t : T or t = s : T occurs on the branch
(=F)
A[t/x] : F
A[s/x] : F
for any terms s, t free for x in A such that
s = t : T or t = s : T occurs on the branch
As well as these rules, the following condition for closing a branch is added: a branch
closes if for some term t the signed formula t = t : F occurs on it.
Example 131 Using Semantic Tableaux with equality, check whether
∀x(P(x) ↔x = c) |= P (c), for a constant symbol c.
The tableau is constructed as follows.
∀x(P (x) ↔x = c): T, P(c) : F
P (c) ↔c = c : T
P (c) : T, c = c : T, P(c) : F
P (c): F, c = c : F, P(c) : F
×
×
Note that the right-hand branch closes due to the new closure rule.

172
Logic as a Tool
Example 132 Using Semantic Tableaux with equality, derive
⊢ST ∀x∀y(x = y →(P(f(x)) ↔P(f(y)))),
where P is a unary predicate symbol and f is a unary function symbol:
∀x∀y(x = y → (P (f(x)) ↔ P (f(y)))) : F
∀y(c = y → (P (f(c)) ↔ P (f(y)))) : F
(c = d → (P (f(c)) ↔ P (f(d)))) : F
c = d : T, (P (f(c)) ↔ P (f(d))) : F
c = d : T, (P (f(c)) → P (f(d)): F
c = d : T, P (f(c)) : T,  P (f(d)) : F
P(f(d)) : T
P(f(d)) : F
= T
×
c = d : T, P (f(d)) → P (f(c)) : F
c = d : T, P (f(d)) : T, P (f(c)) : F
= F
×
The tableaux close. The applications of the rules equality are indicated. In both cases,
we had a choice to apply either of these rules.
Example 133 As a last example we prove once more, now using Semantic Tableaux with
equality, that the line in the old jazz song
“Everybody loves my baby, but my baby don’t love nobody but me”
implies that “I am my baby,” that is,
∀xL(x, MyBaby) ∧∀y(¬y = Me →¬L(MyBaby, y)) ⊢ST MyBaby = Me.
Theorem 134 (Soundness and completeness of ST) Each of the Semantic Tableaux
systems ST for first-order logic, with and without equality, is sound and complete, that is,
for all first-order formulae A1, . . . , An, C of the respective language (respectively, with
or without equality):
A1, . . . , An, ⊢ST C iff A1, . . . , An, |= C.
The proof for ST with equality is outlined in Section 4.6.

Deductive Reasoning in First-order Logic
173
∀xL(x,MyBaby)∧∀y(¬y=Me→¬L(MyBaby,y)):T,
MyBaby=Me:F
∀xL(x,MyBaby):T,∀y(¬y=Me→¬L(MyBaby,y)):T,
MyBaby=Me:F
L(MyBaby,MyBaby):T, ∀y(¬y=Me→¬L(MyBaby,y)):T,
MyBaby=Me:F
L(MyBaby,MyBaby):T, ¬MyBaby=Me→¬L(MyBaby,MyBaby):T,
MyBaby=Me:F
L(MyBaby,MyBaby):T, ¬ MyBaby=Me:F,
MyBaby=Me:F
L(MyBaby,MyBaby):T, MyBaby=Me:T,
MyBaby=Me:F
L(MyBaby,MyBaby):T, ¬ L(MyBaby,MyBaby):T,
MyBaby=Me:F
L(MyBaby,MyBaby):T, L(MyBaby,MyBaby):F,
MyBaby=Me:F
4.2.3
Discussion on the quantifier rules and on termination
We end this section with some discussion on the quantifier rules (∀T) and (∃F) and on
the termination of the Semantic Tableaux method for first-order logic.
1. When the language has at least one constant and one functional symbol, there are
infinitely many ground terms that can be instantiated in the rules (∀T) and (∃F). All,
except for finitely many of these instantiations, would be “useless” for the closure of
the branch of the tableau. The rules as they stand therefore appear to be “wasteful”, in
the sense of allowing many useless applications. As we saw in some of the examples,
this could lead to unnecessary non-termination but will not affect the soundness and
completeness of the system, as stated in Theorem 134. Still, in automated theorem
provers, these rules can be used quite efficiently by employing suitable heuristics and
strategies on how to use them.
2. On the other hand, these rules can be strengthened by allowing instantiation with any
term t that is free for x in A. The resulting rules would still be sound, but even more
wasteful in the sense of allowing useless applications.
3. The problem of identifying and preventing useless applications of the rules (∀T) and
(∃F) is non-trivial. As we will see soon, this problem is in fact algorithmically unsolv-
able in any sound and complete version of the Semantic Tableaux method. Intuitively,
it may seem sufficient to only apply these rules for terms that have already appeared on
the branch, if a contradiction is to be reached on that branch. However, such a restric-
tion would be too strong and could prevent the tableau from closing, for example for
the unsatisfiable input formula ∀x∀y(P(x) ∧¬P(f(y))).

174
Logic as a Tool
4. There is a variation of the quantifier rules leading to the so-called Free Variable
Tableaux version of the method, which defers the term instantiations applied with the
rules (∀T) and (∃F) to a later stage, where it may become clear exactly what instantia-
tions are needed in order to close the branch, if possible. The idea is (1) to modify the
rules (∀T) and (∃F) to instantiate not with ground terms but with new free variables,
and (2) to modify the rules (∀F) and (∃T) to instantiate not with a new constant but
with a term f(v1, . . . , vk), where f is a new functional symbol and v1, . . . , vk are
free variables on the branch that may have appeared there by applications of the rules
(∀T) and (∃F). The technique behind this modification of the rules is called Skolem-
ization and is presented and discussed in Section 4.4.2. In the case when there are no
such free variables, the modified rules act like the present rules by instantiating with
new constant symbols.
For example, the guessing in Example 128 could be avoided in the Free Variable
Tableaux version by instantiating x with a new free variable v1, then instantiating y
with another free variable v2, and finally instantiating v1 with f(v2). This is a simple
case of the term unification technique, which I discuss in detail in Section 4.5.3.
Finally, some words on termination. We have seen that the system of Semantic Tableaux
presented here does not always terminate, even in cases where a suitable application of the
rules would close the tableau. The question therefore arises: can the rules of the system be
modified so that it remains sound and complete, but also always terminates? The answer
is an emphatic no, at least not if the rules are to be algorithmically implementable. More
precisely, in any sound and complete system of Semantic Tableaux it is not possible to
determine algorithmically whether the construction of the tableau for any given first-order
formula can terminate (as open or closed) or not. This impossibility is a direct consequence
from Church’s Undecidability Theorem stated in the introduction to this chapter.
References for further reading
For a comprehensive handbook on tableaux methods, not only in classical logic, see
D’Agostino et al. (1999). For more details on the theory and examples of derivations
in Semantic Tableaux for first-order logic, see Nerode and Shore (1993), Jeffrey (1994,
who referred to “analytic trees”), Smullyan (1995, one of the first versions of Semantic
Tableaux), Fitting (1996), Smith (2003, who called them “trees”), Ben-Ari (2012), and
van Benthem et al. (2014).
Exercises
For each of the following exercises use Semantic Tableaux.
4.2.1
Prove the following logical validities and consequences.
(a) |= ∀xP(x) →∀yP(y)
(b) |= ∃xP(x) →∃yP(y)
(c) ∀xA(x) |= ¬∃x¬A(x)
(d) ¬∃x¬A(x) |= ∀xA(x)
(e) ∃xA(x) |= ¬∀x¬A(x)
(f) ¬∀x¬A(x) |= ∃xA(x)
(g) |= ∃x∃yQ(x, y) →∃y∃xQ(x, y).

Deductive Reasoning in First-order Logic
175
4.2.2
Suppose x is not free in Q. Prove the following logical consequences.
(a) ∀x(P(x) ∨Q) |= ∀xP(x) ∨Q
(b) ∀xP(x) ∨Q |= ∀x(P(x) ∨Q)
(c) ∃x(P(x) ∧Q) |= ∃xP(x) ∧Q
(d) ∃xP(x) ∧Q |= ∃x(P(x) ∧Q)
(e) ∀x(Q →P(x)), Q |= ∀xP(x)
(f) ∃xP(x) →Q |= ∀x(P(x) →Q)
(g) ∃x(P(x) →Q), ∀xP(x) |= Q
(h) ∃x(Q →P(x)), Q |= ∃xP(x).
4.2.3
Check which of the following first-order formulae are logically valid. For each of
those which are not, construct a counter-model, that is, a structure which falsifies it.
(a) ∃x(P(x) →∀yP(y))
(b) ∀x(P(x) →∃yP(y))
(c) ∀x∃yQ(x, y) →∀y∃xQ(x, y)
(d) ∀x∃yQ(x, y) →∀y∃xQ(y, x)
(e) ∀x∃yQ(x, y) →∃y∀xQ(x, y)
(f) ∀x∃yQ(x, y) →∃xQ(x, x)
(g) ∃y∀xQ(x, y) →∃xQ(x, x)
(h) ∀x(∃yQ(x, y) →∃yQ(y, x))
(i) ∃x¬∃yP(x, y) →∀y¬∀xP(x, y)
(j) (∀x∃yP(x, y) ∧∀x∀y(P(x, y) →P (y, x))) →∃xP(x, x).
4.2.4
Check which of the following logical consequences hold. For those which do not,
use the tableau to construct a counter-model, that is, a structure and assignment in
which all premises are true, while the conclusion is false.
(a) ∀xA(x), ∀xB(x) |= ∀x(A(x) ∧B(x))
(b) ∀x(A(x) ∧B(x)) |= ∀xA(x) ∧∀xB(x)
(c) ∀xA(x) ∨∀xB(x) |= ∀x(A(x) ∨B(x))
(d) ∀x(A(x) ∨B(x)) |= ∀xA(x) ∨∀xB(x)
(e) ∀xA(x) →∀xB(x) |= ∀x(A(x) →B(x))
(f) ∀x(A(x) →B(x)) |= ∀xA(x) →∀xB(x)
(g) ∀x(A(x) →B(x)) |= ∃xA(x) →∃xB(x)
(h) ∃xA(x), ∃xB(x) |= ∃x(A(x) ∧B(x))
(i) ∃x(A(x) ∧B(x)) |= ∃xA(x) ∧∃xB(x)
(j) ∃x(A(x) ∨B(x)) |= ∃xA(x) ∨∃xB(x)
(k) ∃xA(x) ∨∃xB(x) |= ∃x(A(x) ∨B(x))
(l) ∃x(A(x) →B(x)) |= ∃xA(x) →∃xB(x)
(m) ∃xA(x) →∃xB(x) |= ∃x(A(x) →B(x)).
4.2.5
Determine which of the following logical equivalences hold. For each of those that
do not hold, construct a counter-model, that is, a structure in which one formula
is true while the other is false.
(a) ∀x(P(x) ∧Q(x)) ≡∀xP(x) ∧∀xQ(x)
(b) ∀x(P(x) ∨Q(x)) ≡∀xP(x) ∨∀xQ(x)
(c) ∀x(P(x) →Q(x)) ≡∀xP(x) →∀xQ(x)
(d) ∃x(P(x) ∧Q(x)) ≡∃xP(x) ∧∃xQ(x)

176
Logic as a Tool
(e) ∃x(P(x) →Q(x)) ≡∃xP(x) →∃xQ(x)
(f) ∃x(P(x) ∨Q(x)) ≡∃xP(x) ∨∃xQ(x)
(g) ∀xQ(x, x) ≡∀x∃yQ(x, y)
(h) ∃x∀yQ(x, y) ≡∃yQ(y, y)
(i) ∀x∃yQ(x, y) ≡∃xQ(x, x)
(j) ∀x∀yQ(x, y) ≡∀x∀yQ(y, x)
(k) ∃xP(x) →∀xP(x) ≡∃x¬P(x) →∀x¬P (x).
4.2.6
In each of the following cases formalize both statements in first-order logic and
check which, if any, logically implies the other. If any of the logical consequences
do not hold, construct an appropriate counter-model.
(a) A: “Not every lawyer is greedy.”
B: “There exists a lawyer and not everybody is greedy.”
(b) A: “Some men are happy only when they are drunk.”
B: “There is a drunk man or there is an unhappy man.”
(c) A: “ If somebody is talking then everybody is listening.”
B: “ There is somebody, such that if he is not listening then nobody is
talking.”
4.2.7
Formalize the following arguments in FOL by introducing suitable predicates.
Then check for each of them whether it is logically valid. If not valid, use the
tableau to construct a counter-model.
(a)
All logicians are clever.
All clever people are rich.
All logicians are rich.
(b)
All natural numbers are integers.
Some integers are odd numbers.
Some natural numbers are odd numbers.
(c)
No man loves every woman.
Some men are bachelors.
Therefore, some bachelor does not love some woman.
(d)
If everybody is selling then somebody is buying or negotiating.
No seller is buying.
Therefore somebody is selling only if he is negotiating.

Deductive Reasoning in First-order Logic
177
(e)
No driver is a pedestrian.
Every child is a pedestrian.
Johnnie is a child.
Therefore somebody is not a driver.
(f)
All penguins are birds.
Some penguins are white.
No white bird eats butterflies.
Therefore, not all penguins eat butterflies.
(g)
Everybody likes some black gmuck.
Nobody likes any white gmuck.
Therefore no gmuck is black and white.
(h)
Some bachelors love every woman they know.
Every bachelor is a man.
Eve is a woman whom every man knows.
Some bachelor loves a woman.
(i) Replace the conclusion in the previous example by
“Every bachelor loves a woman,”
and check whether the resulting argument is logically valid.
4.2.8
Using Semantic Tableaux with equality, derive each of the following (the first five
are the axioms of H for the equality):
(a) (Ax=1) x = x
(b) (Ax=2) x = y →y = x
(c) (Ax=3) x = y ∧y = z →x = z
(d) (Axf)
x1 = y1 ∧. . . ∧xn = yn →f(x1, . . . , xn) = f(y1, . . . , yn)
for
n-ary functional symbol f
(e) (Axr) x1 = y1 ∧. . . ∧xn = yn →(p(x1, . . . , xn) →p(y1, . . . , yn)) for
n-ary predicate symbol p
(f) s = t →A[s/x] ↔A[t/x] for any formula A(x) and terms s, t free for x
in A.
(This is the Theorem 125 for equivalent replacement.)
4.2.9
Using Semantic Tableaux with equality, check whether each of the following
validities and logical consequences holds.
(a) |= x1 = y1 ∧x2 = y2 →g(f(x1, f(x2))) = g(f(y1, f(y2)))
(Universal quantification is assumed but omitted.)

178
Logic as a Tool
(b) |= ∀x(x = f(x) →(P(f(f(x))) →P(x)))
(c) ∀x∀y(f(x) = y →g(y) = x) |= ∀z(g(f(z)) = z)
(d) ∀x∀y(f(x) = y →g(y) = x) |= ∀z(f(g(z)) = z)
(e) ∀x∀y(f(x) = y →g(y) = x), ∀x∀y(g(x) = g(y) →x = y) |=
∀z(f(g(z)) = z).
For more exercises on derivations with equality, on sets, functions, and relations,
see Section 5.2.7.
Raymond Merrill Smullyan (born 25.5.1919) is an Ameri-
can mathematician, logician, magician, Taoist philosopher, and
piano player, best known as an avid popularizer of logic through
his series of scientific (mathematical logic) and recreational
books with enchanting logical puzzles.
In his childhood Smullyan showed passion and strong talent
for music (as a piano player), chess, and mathematics, but was
not quite happy at school so he left to study on his own. He
was keen on problem solving, both in maths and chess where
he composed many problems of his own. He also started learning magic tricks and
became a very good magician, earning money by performing magic acts in night-
clubs in Greenwich Village. During the early 1950s he studied at the University
of Chicago (where one of his teachers was the prominent philosopher and logi-
cian Rudolf Carnap), while himself teaching at Dartmouth College. He achieved his
PhD at Princeton University under Alonzo Church and published his first paper on
logic in 1957, where he showed that Gödel’s incompleteness phenomena based on
self-reference also apply to much simpler formal systems than Peano arithmetic and
gave new, very intuitive explanations of Gödel’s theorems.
Smullyan has been for many years a Professor of Mathematics and Philosophy at
Yeshiva University, New York, and at Lehman College and the Graduate Center of
the City University of New York, where he is still a Professor Emeritus. He is also
an Oscar Ewing Professor of Philosophy at Indiana University.
Smullyan has published several well-known books on mathematical logics,
including Theory of Formal Systems (1959), First-Order Logic (1968), Gödel’s
Incompleteness Theorems (1992), Recursion Theory for Metamathematics (1993),
Diagonalization and Self-reference (1994), Set Theory and the Continuum Problem
(1996, together with Melvin Fitting), and A Beginner’s Guide to Mathematical
Logic (2014). In his book First-Order Logic Smullyan developed the method of
analytic tableaux as a variant of the Semantic Tableaux of Beth and Hintikka, and
related it to Gentzen’s proofs systems.
Smullyan also wrote a few books on Taoist philosophy, but he is most popular for
his fascinating books with logical, mathematical, and chess puzzles, including What
Is the Name of This Book? (1978), The Chess Mysteries of Sherlock Holmes (1979)

Deductive Reasoning in First-order Logic
179
The Lady or the Tiger? (1982), Alice in Puzzle-Land (1982), To Mock a Mockingbird
(1985), Forever Undecided (1987), Satan, Cantor, and Infinity (1992), The Riddle of
Scheherazade (1997), King Arthur in Search of his Dog (2010), The Gödelian Puzzle
Book: Puzzles, Paradoxes and Proofs (2013), and many others.
As well as teaching and writing about logic, Smullyan is an accomplished piano
player and has produced many recordings of his favorite classical piano pieces.
In 2004 the filmmaker Tao Ruspoli produced a documentary film about him called
This Film Needs No Title: A Portrait of Raymond Smullyan.
Melvin Fitting (born 24.1.1942) is an American logician,
mathematician, philosopher, and computer scientist with very
broad interests, and has made many contributions to mathe-
matical, philosophical, and computational logic.
Fitting obtained his doctorate in mathematics from
Yeshiva University, New York under the supervision of
Raymond Smullyan. From 1968 to 2013 he was a professor
at City University of New York, Lehman College and at the
Graduate Center, in the departments of Computer Science,
Philosophy, and Mathematics. He is currently an Emeritus
Professor there.
Fitting’s philosophy of logic can be formulated succinctly as follows (quoting
Wikipedia): “There are many logics. Our principles of reasoning vary with context
and subject matter. Multiplicity is one of the glories of modern formal logic. The
common thread tying logics together is a concern for what can be said (syntax), what
that means (semantics), and relationships between the two. A philosophical position
that can be embodied in a formal logic has been shown to be coherent, not correct.
Logic is a tool, not a master, but it is an enjoyable tool to use.”
Fitting has worked in numerous diverse fields of logic, an impressive testimony
of which is the long list of well-known books he has written over the years including
Intuitionistic Logic, Model Theory, and Forcing (1969), Fundamentals of General-
ized Recursion Theory (1981), Proof Methods for Modal and Intuitionistic Logics
(1983), First-Order Logic and Automated Theorem Proving (1990), Set Theory and
the Continuum Problem, with Raymond Smullyan (1996), First-Order Modal Logic,
with Richard Mendelsohn (1998) Types, Tableaus, and Gödel’s God (2002), and
Incompleteness in the Land of Sets (2007). In addition, he has carried out important
work on tableaux methods of proof for modal logics, bilattices and the theory of
truth, many-valued modal logics, and justification logic.

180
Logic as a Tool
4.3
Natural Deduction for first-order logic
I now extend propositional Natural Deduction to first-order logic by adding rules for the
quantifiers.
4.3.1
Natural Deduction rules for the quantifiers
Introduction rules
Elimination rules
(∀I)*
A[c/x]
∀xA(x)
(∀E)**
∀xA(x)
A[t/x]
(∃I)**
A[t/x]
∃xA(x)
(∃E)***
∃xA(x)
[A[c/x]]
...
C
C
*where c is a constant symbol, not occurring in A(x) or in any open assumption used in the derivation
of A[c/x];
**for any term t free for x in A;
***where c is a constant symbol, not occurring in A(x), C, or in any open assumption in the derivation
of C, except for A[c/x].
Let us start with some discussion and explanation of the quantifier rules.
(∀I) : If we are to prove a universally quantified sentence, ∀xA(x), we reason as fol-
lows. We say “Let c be any object from the domain (e.g., an arbitrary real num-
ber).” However, the name c of that arbitrary object must be new – one that has
not yet been used in the proof – to be sure that it is indeed arbitrary. We then
try to prove that A(c) holds, without assuming any specific properties of c. If we
succeed, then our proof will apply to any object c from the structure, so we will
have a proof that A(x) holds for every object x, that is, a proof of ∀xA(x).
(∃I) : If we are to prove an existentially quantified sentence ∃xA(x), then we try to
find an explicit witness, an object in the domain, satisfying A. Within the formal
system we try to come up with a term t that would name such a witness.
(∀E) : If a premise is a universally quantified sentence ∀xA(x), we can assume A(a) for
any object a from the structure. Within the formal system, instead of elements of
a structure, we must use syntactic objects which represent them, namely, terms3.
(∃E) : If a premise is an existentially quantified sentence ∃xA(x) then we introduce a
name, say c, for an object that satisfies A. That is, we say “if there is an x such
3 Note that this sounds a little weaker because in general not every element of the domain has a name, or is the value
of a term. However, it is sufficient because ad hoc names can be added whenever necessary.

Deductive Reasoning in First-order Logic
181
that A(x), then take one and call it c.” We then replace the premise by A[c/x].
If we succeed in deriving the desired conclusion C from that new premise, then it
can be derived from ∃xA(x), provided c does not occur in A or C, or in any other
assumption used in the derivation of C from A[c/x]. This means that the name
c must be a new one that has not yet been used in the proof. At that moment we
can discard the assumption A[c/x], as it was only an auxiliary assumption used
to justify the derivation of C from ∃xA(x).
From this point onward I write A(t) instead of A[t/x] whenever it is clear from the
context which variable is substituted by the term t.
Remark 135 A common mistake is to use this simpler rule for elimination of ∃:
(∃E) ∃xA(x)
A(c)
where c is a new constant symbol. Although simple and natural looking, this rule is not log-
ically sound! For instance, using it I can derive the invalid implication ∃xA(x) →A(c)
which can be interpreted as, for example, “If anyone will fail the exam then ⟨YourName⟩
will fail the exam,” which you certainly do not wish to be valid.
4.3.2
Derivations in first-order Natural Deduction
Derivations in first-order Natural Deduction are organized as for derivations in Proposi-
tional Natural Deduction: as trees growing upward (but usually constructed downward);
with nodes decorated by formulae, where the leaves are decorated with given or introduced
assumptions; every node branching upward according to some of the derivation rules; and
the formula decorating that node is the conclusion of the rule, while those decorating the
successor nodes are the premises. The root of the derivation tree is decorated with the
formula to be derived. In particular, the Natural Deduction derivation tree for the logical
consequence A1, . . . , An |= B has leaves decorated with formulae among A1, . . . , An
or with subsequently canceled additional assumptions, while the root is decorated with B.
If such a derivation tree exists, then we say that A1, . . . , An |= B is derivable in Natural
Deduction and write A1, . . . , An ⊢ND B.
Here are some examples of derivations in Natural Deduction. Check that the rules for
the quantifiers have been applied correctly.
1. ⊢ND ∀x∀yP(x, y) →∀y∀xP(x, y) :
(→I)
(∀I)
(∀I)
(∀E)
(∀E)[∀x∀yP(x, y)]1
∀yP(c1, y)
P (c1, c2)
∀xP(x, c2)
∀y∀xP(x, y)
∀x∀yP(x, y) →∀y∀xP(x, y) 1

182
Logic as a Tool
2. ∀x(P(x) ∧Q(x)) ⊢ND ∀xP(x) ∧∀xQ(x) :
(∧I)
(∀I)
(∧E)
(∀E)∀x(P(x) ∧Q(x))
P(c) ∧Q(c)
P(c)
∀xP(x)
(∀I)
(∧E)
(∀E)∀x(P(x) ∧Q(x))
P(c) ∧Q(c)
Q(c)
∀xQ(x)
∀xP(x) ∧∀xQ(x)
3. ¬∃x¬A(x) ⊢ND ∀xA(x) :
¬∃x¬A(x)
[¬A(c)]1
∃x¬A(x)
⊥
A(c)
1
∀xA(x)
4. ¬∀x¬A(x) ⊢ND ∃xA(x) :
¬∀x¬A(x)
[¬∃xA(x)]2
[A(c)]1
∃xA(x)
⊥
¬A(c)
1
∀x¬A(x)
⊥
∃xA(x)
2
5. Suppose x is not free in P. Then ⊢ND ∀x(P →Q(x)) →(P →∀xQ(x)) :
(→I)
(→I)
(∀I)
(→E)
(∀E)[∀x(P →Q(x))]1
P →Q(c)
,[P]2
Q(c)
∀xQ(x)
P →∀xQ(x)
2
∀x(P →Q(x)) →(P →∀xQ(x))
1
6. Suppose x is not free in P. Then ∀x(Q(x) →P ), ∃xQ(x) ⊢ND P :
(∃E)
∃xQ(x) (→E)
(∀E)∀x(Q(x) →P)
Q(c) →P
,
[Q(c)]1
P
P
1

Deductive Reasoning in First-order Logic
183
4.3.3
Natural Deduction for first-order logic with equality
The system of Natural Deduction presented so far does not involve rules for the equality.
Such rules can be added to produce a sound and complete system of Natural Deduc-
tion for first-order logic with equality, as follows. In each of the rules below, t, ti, si
are terms.
(Ref)
t = t
(Sym)
t1 = t2
t2 = t1
(Tran)
t1 = t2, t2 = t3
t1 = t3
(ConFunc)
s1 = t1, . . . , sn = tn
f(s1, . . . , sn) = f(t1, . . . , tn)
for every n-ary functional symbol f.
(ConRel)
s1 = t1, . . . , sn = tn
p(s1, . . . , sn) →p(t1, . . . , tn)
for every n-ary predicate symbol p.
Example 136 Using Natural Deduction with equality, derive
∀x∀y(f(x) = y →g(y) = x) ⊢ND ∀z(g(f(z)) = z)
where f, g are unary function symbols.
(∀I)
(→E)f(x) = f(x)
(∀E)
(∀E)∀x∀y(f(x) = y →g(y) = x)
∀y(f(x) = y →g(y) = x)
f(x) = f(x) →g(f(x)) = x
g(f(x)) = x
∀z(g(f(z)) = z)
Finally, two important general results are as follows.
Theorem 137 [Equivalent replacement] For any formula A(x) and terms s, t free for x
in A, the following is derivable in ND:
s = t ⊢ND A[s/x] ↔A[t/x].
The proof can be done by induction on A and is left as an exercise.
The proof of the following fundamental result, for ND with equality, will be outlined
in Section 4.6.

184
Logic as a Tool
Theorem 138 (Soundness and completeness of ND) Each of the systems ND of Natu-
ral Deduction for first-order logic, with and without equality, is sound and complete, that
is, for all first-order formulae A1, . . . , An, C of the respective language (respectively,
with or without equality):
A1, . . . , An, ⊢ND C
iff
A1, . . . , An, |= C.
References for further reading
For more details on the theory and examples of derivations in Natural Deduction, see van
Dalen (1983), Jeffrey (1994) (who referred to “synthetic trees”), Smullyan (1995), Fitting
(1996), Huth and Ryan (2004), Nederpelt and Kamareddine (2004), Prawitz (2006, the
original development of the modern version of Natural Deduction), Chiswell and Hodges
(2007), and van Benthem et al. (2014), as well as Kalish and Montague (1980) and Bornat
(2005), where Natural Deduction derivations are presented in a boxed form rather than in
tree-like shape.
Exercises
Use Natural Deduction for all exercises below. In all derivations, indicate the rules applied
and the succession of all steps by numbering them. Check that the provisos for all appli-
cations of rules are satisfied.
4.3.1
Prove the following logical validities and consequences.
(a) |= ∀xA(x) →∀yA(y)
(b) |= ∃xA(x) →∃yA(y)
(c) ∀xA(x) |= ¬∃x¬A(x)
(d) ∃xA(x) |= ¬∀x¬A(x).
4.3.2
Suppose x is not free in Q. Prove the validity of the following logical consequences
by deriving them with Natural Deduction.
(a) ∀x(P(x) ∨Q) |= ∀xP(x) ∨Q
(b) ∀xP(x) ∨Q |= ∀x(P (x) ∨Q)
(c) ∃x(P(x) ∧Q) |= ∃xP(x) ∧Q
(d) ∃xP(x) ∧Q |= ∃x(P (x) ∧Q)
(e) ∃xP(x) →Q |= ∀x(P(x) →Q)
(f) ∀x(Q →P(x)), Q |= ∀xP(x)
(g) ∃x(Q →P(x)), Q |= ∃xP(x)
(h) ∃x(P(x) →Q), ∀xP(x) |= Q
(i) ∃x(¬P(x) ∨Q) |= ∀xP(x) →Q
(j) ∃x(P(x) ∨¬Q) |= Q →∃xP(x).
4.3.3
Determine which of the following logical consequences hold by searching for a
derivation in Natural Deduction. For those that you find not derivable in ND, con-
sider A and B as unary predicates and look for a counter-model, that is, a structure
and assignment in which all premises are true while the conclusion is false.
(a) ∀xA(x) ∧∀xB(x) |= ∀x(A(x) ∧B(x))
(b) ∀xA(x) ∨∀xB(x) |= ∀x(A(x) ∨B(x))
(c) ∀x(A(x) ∧B(x)) |= ∀xA(x) ∧∀xB(x)

Deductive Reasoning in First-order Logic
185
(d) ∀x(A(x) ∨B(x)) |= ∀xA(x) ∨∀xB(x)
(e) ∀xA(x) →∀xB(x) |= ∀x(A(x) →B(x))
(f) ∀x(A(x) →B(x)) |= ∀xA(x) →∀xB(x)
(g) ∃xA(x) ∧∃xB(x) |= ∃x(A(x) ∧B(x))
(h) ∃x(A(x) ∧B(x)) |= ∃xA(x) ∧∃xB(x)
(i) ∃x(A(x) ∨B(x)) |= ∃xA(x) ∨∃xB(x)
(j) ∃xA(x) ∨∃xB(x) |= ∃x(A(x) ∨B(x))
(k) ∃xA(x) →∃xB(x) |= ∃x(A(x) →B(x))
(l) ∃x(A(x) →B(x)) |= ∃xA(x) →∃xB(x)
(m) ∃x∃yA(x, y) |= ∃y∃xA(x, y)
(n) ∀y∃x(P(x) →Q(y)) |= ∀xP(x) →∀zQ(z)
(o) ∃y∀x(P(x) →Q(y)) |= ∃xP(x) →∃zQ(z)
(p) ∀y(P(y) →Q(y)) |= ∃yP(y) →∃yQ(y)
(q) ∀y(P(y) →Q(y)) |= ∃x¬Q(x) →∃x¬P(x)
(r) |= ∃x(P(x) →∀yP(y)).
4.3.4
Formalize the following arguments in first-order logic and try to prove their logical
correctness by deriving them in Natural Deduction. For those that you find not
derivable in ND, look for a counter-model.
(a)
No yellow and dangerous reptiles are lizards.
Some reptile is dangerous or is not a lizard.
Therefore, not every reptile is a yellow lizard.
(b)
All penguins are birds.
No penguin can fly.
Every white bird can fly.
Therefore, no penguin is white.
(c)
All penguins are birds.
Some penguins are white.
All penguins eat some fish.
Therefore, there is a white bird that eats some fish.
(d)
No lion eats birds.
Every penguin is a bird.
Simba is a lion.
Therefore some lion eats no penguins.

186
Logic as a Tool
4.3.5
Prove Theorem 137 for equivalent replacement in Natural Deduction with equality
by structural induction on the formula A.
4.3.6
Derive each of the following in Natural Deduction with equality.
(a) ⊢ND x1 = y1 ∧x2 = y2 →g(f(x1, f(x2))) = g(f(y1, f(y2)))
(Universal quantification is assumed but omitted.)
(b) ⊢ND ∀x(x = f(x) →(P(f(f(x))) →P(x)))
(c) ∀x∀y(f(x) = y →g(y) = x), ∀x∀y(g(x) = g(y) →x = y) ⊢ND
∀z(f(g(z)) = z)
4.3.7
Prove again, now using Natural Deduction with equality, that the line in the old jazz
song “Everybody loves my baby, but my baby don’t love nobody but me” implies
that “I am my baby,” that is,
∀xL(x, MyBaby) ∧∀y(¬y = Me →¬L(MyBaby, y)) ⊢ND MyBaby = Me.
For more exercises on derivations with equality, on sets, functions, and relations,
see Section 5.2.7.
Dag Prawitz (born 16.05.1936) is a Swedish philosopher and
logician who has made seminal contributions to proof theory
as well as to the philosophy of logic and mathematics.
Prawitz was born and brought up in Stockholm. He studied
theoretical philosophy at Stockholm University as a student of
Anders Wedberg and Stig Kanger, and obtained a PhD in phi-
losophy in 1965. After working for a few years as a docent
(associate professor) in Stockholm and in Lund, and as a visit-
ing professor in US at UCLA, Michigan and Stanford, in 1971 Prawitz took the chair
of professor of philosophy at Oslo University for 6 years. Prawitz returned to Stock-
holm University in 1976 as a professor of theoretical philosophy until retirement in
2001, and is now a Professor Emeritus there.
While still a graduate student in the late 1950s, Prawitz developed his algorithm
for theorem proving in first-order logic, later implemented on one of the first comput-
ers in Sweden (probably the first computer implementation of a complete theorem
prover for first-order logic) and published in his 1960 paper with H. Prawitz and N.
Voghera A mechanical proof procedure and its realization in an electronic computer.
In his doctoral dissertation Natural deduction: A proof-theoretical study Prawitz
developed the modern treatment of the system of Natural Deduction. In particular, he
proved the Normalization Theorem, stating that all proofs in Natural deduction can
be reduced to a certain normal form, a result that corresponds to Gentzen’s celebrated
Hauptsatz for sequent calculus. Prawitz’ Normalization Theorem was later extended
to first-order arithmetic as well as to second-order and higher-order logics.
As well as his pioneering technical work in proof theory, Prawitz has conducted
important studies on the philosophical aspects of proof theory, on inference and

Deductive Reasoning in First-order Logic
187
knowledge, and on analyzing the relations between classical and intuitionistic logic.
In particular, following Gentzen and Dummett, he has argued that there is a deep
relation between proofs rules and the meaning of the logical connectives, and that
the meaning of a statement can be seen as determined by what arguments would
establish that statement as true.
Prawitz is a member of the Swedish and Norwegian Academies of Science and of
several other distinguished scientific organizations.
4.4
Prenex and clausal normal forms
In this section I present the first-order extensions of conjunctive normal form and clausal
form of a formula, needed for the extension of the method of Resolution to first-order
logic.
4.4.1
Prenex normal forms
Definition 139
• A first-order literal is an atomic formula or a negation of an atomic formula.
• A first-order elementary conjunction/disjunction is a first-order literal or a conjunc-
tion/disjunction of (two or more) first-order literals.
• A first-order disjunctive normal form (DNF) is a first-order elementary conjunction
or a disjunction of (two or more) first-order elementary conjunctions.
• Respectively, a first-order conjunctive normal form (CNF) is a first-order elementary
disjunction or a conjunction of (two or more) first-order elementary disjunctions.
Definition 140 A first-order formula Q1x1 . . . QnxnA, where Q1, . . . , Qn are quan-
tifiers and A is an open formula, is said to be in a prenex form. The quantifier string
Q1x1 . . . Qnxn is called the prefix, and the formula A the matrix of the prenex form.
If A is a first-order DNF (respectively, CNF) then Q1x1 . . . QnxnA is in a prenex
disjunctive(respectively, conjunctive) normal form.
Example 141
1. The formula
∀x∃y(¬x > 0 ∨(y > 0 ∧¬x = y2))
is in a prenex DNF, but not a prenex CNF.

188
Logic as a Tool
2. The formula
∀x∃y(x > 0 →(y > 0 ∧x = y2))
is in a prenex form but not in DNF or in CNF.
3. The formula
∀x(x > 0 →∃y(y > 0 ∧x = y2))
is not in a prenex form.
Theorem 142 Every first-order formula is equivalent to a formula in a prenex disjunctive
normal form (PDNF) and to a formula in a prenex conjunctive normal form (PCNF).
I only provide a brief outline of an algorithm for construction of these normal forms,
for a given any formula A.
1. Eliminate all occurrences of →and ↔as in the propositional case.
2. Import all negations inside all other logical connectives and transform the formula to
negation-normal form.
3. Pull all quantifiers in front and therefore transform the formula into a prenex form. For
that, use the equivalences
(a) ∀xP ∧∀xQ ≡∀x(P ∧Q) and
(b) ∃xP ∨∃xQ ≡∃x(P ∨Q)
to pull some quantifiers outward, after renaming the formula wherever necessary. Then
use the following equivalences, where x does not occur free in Q, until the formula is
transformed to a prenex form:
(c) ∀xP ∧Q ≡Q ∧∀xP ≡∀x(P ∧Q)
(d) ∀xP ∨Q ≡Q ∨∀xP ≡∀x(P ∨Q)
(e) ∃xP ∨Q ≡Q ∨∃xP ≡∃x(P ∨Q)
(f) ∃xP ∧Q ≡Q ∧∃xP ≡∃x(P ∧Q).
4. Finally, transform the matrix into a DNF or CNF, as for a propositional formula.
A simplified, but more inefficient procedure for getting the quantifiers out first trans-
forms the formula into an equivalent clean formula by suitable renaming of bound vari-
ables. It then pulls out all quantifiers by maintaining the order, tacitly using the last four
equivalences (c–f) above. However, this procedure often introduces unnecessary new vari-
ables, which makes the result not optimal for the Skolemization step of the procedure,
described in the following section.
Example 143 Let A = ∃z(∃xQ(x, z) ∨∃xP(x)) →¬(¬∃xP(x) ∧∀x∃zQ(z, x)).
1. Eliminating →: A ≡¬∃z(∃xQ(x, z) ∨∃xP(x)) ∨¬(¬∃xP(x) ∧∀x∃zQ(z, x)).
2. Importing the negation:
A ≡∀z(¬∃xQ(x, z) ∧¬∃xP(x)) ∨(¬¬∃xP(x) ∨¬∀x∃zQ(z, x))
≡∀z(∀x¬Q(x, z) ∧∀x¬P(x)) ∨(∃xP(x) ∨∃x∀z¬Q(z, x)).

Deductive Reasoning in First-order Logic
189
3. Using the equivalences (a) and (b):
A ≡∀z∀x(¬Q(x, z) ∧¬P(x)) ∨∃x(P(x) ∨∀z¬Q(z, x)).
4. Renaming: A ≡∀z∀x(¬Q(x, z) ∧¬P(x)) ∨∃y(P(y) ∨∀w¬Q(w, y)).
5. Using the equivalences (c–f) and pulling the quantifiers in front:
A ≡∀z∀x∃y∀w((¬Q(x, z) ∧¬P(x)) ∨P(y) ∨¬Q(w, y)).
6. The resulting formula is in a prenex DNF. For a prenex CNF we have to distribute the
∨over ∧:
A ≡∀z∀x∃y∀w((¬Q(x, z) ∨P(y) ∨¬Q(w, y)) ∧(¬P(x) ∨P (y) ∨¬Q(w, y))).
Note that we could have renamed the formula A into a clean formula right from the
beginning. That would have made the resulting prenex formula much larger, however.
4.4.2
Skolemization
Skolemization, named after the logician Thoralf Skolem who first developed it (read more
about him in the biographical box at the end of the section), is a procedure for systematic
elimination of the existential quantifiers in a first-order formula in a prenex form, by way
of uniform replacement of all occurrences of existentially quantified individual variables
with terms headed by new functional symbols, called Skolem functions. In the traditional
version of Skolemization, these Skolem functions take as arguments all variables (if any)
which are bound by universal quantifiers in the scope of which the given existential quan-
tifier sits. In particular, existentially quantified variables not in the scope of any universal
quantifiers are replaced by constant symbols, called Skolem constants.
The Skolemization procedure is explained by the following examples.
Example 144
1. The result of Skolemization of the formula
∃x∀y∀z(P(x, y) →Q(x, z))
is
∀y∀z(P(c, y) →Q(c, z)),
where c is a new constant symbol called a Skolem constant.
Intuitively, c names an element of the domain witnessing the truth of ∃xA( . . . x . . . )
such that A( . . . c/x . . . ) is true.
2. More generally, the result of Skolemization of the formula
∃x1 · · · ∃xk∀y1 · · · ∀ynA(x1, . . . , xk, y1, . . . , yn)
is
∀y1 · · · ∀ynA(c1, . . . , ck, y1, . . . , yn),
where c1, . . . , ck are new Skolem constants.

190
Logic as a Tool
3. The result of Skolemization of the formula
∃x∀y∃z(P (x, y) →Q(x, z))
is
∀y(P(c, y) →Q(c, f(y))),
where c is a new Skolem constant and f is a new unary function, called a Skolem
function.
Intuitively, assuming the formula ∀y∃zA(y, z) is true in a given structure, f names
a function in its domain of discourse, which for every possible value of y assigns a
suitable value for z that makes the formula A(y, z) true.
4. More generally, the result of Skolemization of the formula
∀y∃x1 · · · ∃xk∀y1 · · · ∀ynA(y, x1, . . . , xk, y1, . . . , yn)
is
∀y∀y1 · · · ∀ynA(y, f1(y), . . . , fk(y), y1, . . . , yn),
where f1, . . . , fk are new Skolem functions.
5. The result of Skolemization of the formula
∀x∃y∀z∃uA(x, y, z, u)
is the formula
∀x∀zA(x, f(x), z, g(x, z)),
where f is a new unary Skolem function and g is a new binary Skolem function.
Proposition 145 The result of Skolemization of any first-order formula A is a formula
which is generally not logically equivalent to A, but is equally satisfiable with A.
The proof of this claim is not difficult, but it requires use of a special set-theoretic
principle called the Axiom of Choice.
Thus, for the purposes of checking satisfiability of a formula, Skolemization is an
admissible procedure.
4.4.3
Clausal forms
Recall that a literal is an atomic formula or a negation of an atomic formula, and a clause
is a set of literals (representing their disjunction). An example of a clause is as follows:
{P(x), ¬P(f(c, g(y))), ¬Q(g(x), y), Q(f(x, g(c)), g(g(g(y))))}.
Note that all variables in a clause are implicitly assumed universally quantified.
A clausal form is a set of clauses (representing their conjunction), for example:

Deductive Reasoning in First-order Logic
191
{
{P(x)},
{¬P (f(c)), ¬Q(g(x, x), y)},
{¬P (f(y)), P(f(c)), Q(y, f(x))}
}.
Theorem 146 Every first-order formula A can be transformed to a clausal
form {C1, . . . , Ck} such that A is equally satisfiable with the universal closure
(C1 ∧· · · ∧Ck) of the conjunction of all clauses, where the clauses are considered as
disjunctions.
The algorithm for transforming a formula into clausal form is as follows.
1. Transform A into a prenex CNF.
2. Skolemize all existential quantifiers.
3. Remove all universal quantifiers.
4. Write the matrix (which is in CNF) as a set of clauses.
Example 147 Consider the formula
A = ∃z(∃xQ(x, z) ∨∃xP(x)) →¬(¬∃xP(x) ∧∀x∃zQ(z, x)).
from Example 143.
1. Transforming A to a prenex CNF:
A ≡∀z∀x∃y∀w((¬Q(x, z) ∨P(y) ∨¬Q(w, y)) ∧(¬P(x) ∨P(y) ∨¬Q(w, y))).
2. Skolemizing all existential quantifiers:
Skolem(A) = ∀z∀x∀w((¬Q(x, z) ∨P(f(z, x)) ∨¬Q(w, f(z, x)))∧
(¬P(x) ∨P(f(z, x)) ∨¬Q(w, f(z, x)))).
3. Removing all universal quantifiers and writing the matrix as a set of clauses:
Clausal(A) = {{¬Q(x, z), P(f(z, x)), ¬Q(w, f(z, x))},
{¬P (x), P(f(z, x)), ¬Q(w, f(z, x))}}.
References for further reading
For more technical details, discussions, and examples of prenex normal forms,
Skolemization, and clausal forms, see van Dalen (1983), Hamilton (1988), Nerode and
Shore (1993), Smullyan (1995), Ebbinghaus et al. (1996), Fitting (1996), Barwise and
Echemendy (1999), Hedman (2004), and Ben-Ari (2012).

192
Logic as a Tool
Exercises
Transform each of the following formulae into a prenex DNF and a prenex CNF. Then
Skolemize the resulting formula and transform it into clausal form.
4.4.1
∀x((∃yP(y) ∧∀zQ(z, x)) →∃yQ(x, y))
4.4.2
∃z(∃xQ(x, z) →(∃xP(x) ∨¬∃zP(z)))
4.4.3
∀y¬(P(y) ↔∃xQ(y, x))
4.4.4
∀z(∃yP(y) ↔Q(z, y))
4.4.5
(∃xP(x) →¬∃xQ(x)) →∀x(P(x) →¬Q(x))
4.4.6
∀x(¬∀yQ(x, y) ∧P(z)) →∃z(∀yQ(z, y) ∧¬P(x))
4.4.7
¬∃x(∀y(∃zQ(y, z) ↔P(z)) ∧∀zR(x, y, z))
4.4.8
¬(∀y(∀zQ(y, z) →P (z)) →∃z(P(z) ∧∀x(Q(z, y) →Q(x, z))))
4.4.9
¬(¬∃z(P(z) ∧∀x(Q(z, y) →Q(x, z))) →¬∀y(∀zQ(y, z) →P(z)))
4.4.10
¬(∀y(¬∃zQ(y, z) →P(z)) →∃z((P(z) →Q(z, y)) ∧¬∃xR(x, y, z))).
Thoralf Albert Skolem (23.5.1887–23.3.1963) was a Nor-
wegian mathematician and logician who made important
contributions to mathematical logic, set theory, and alge-
bra.
Skolem was extremely productive and published around
180 papers on Diophantine equations, mathematical logic,
algebra, and set theory. However, many of his results were
not well known until being rediscovered by others, as they
were published in Norwegian journals which were not
widely accessible.
In 1922 Skolem refined and made precise Zermelo’s
axioms for set theory, essentially in the form in which
they are known today. In 1923 he created a formal theory
of primitive recursive arithmetic meant to avoid the paradoxes of the infinite, and
formally developed a great deal of number theory in it. His system can now be
regarded as a programming language for defining functions on natural numbers, so
he made an early contribution to computer science. His other work that later became
very important in computer science and instrumental in resolution-based automated
theorem proving was on what later became known as Skolemization and Skolem
normal form.

Deductive Reasoning in First-order Logic
193
In 1930 Skolem proved that the subsystem of Peano arithmetic involving only
axioms for multiplication but not addition, now called Skolem arithmetic in his
honor, was complete and decidable, unlike the full system of Peano arithmetic
(including both addition and multiplication), which was proven to be incomplete by
Gödel in 1931 and undecidable by Church and Turing in 1936.
The completeness of first-order logic, first published by Gödel in 1930, follows
from results Skolem obtained in the early 1920s and published in 1928. He was
apparently not aware of that fact at the time however, because the problem of proving
completeness of an axiomatic system for first-order logic was only stated explicitly in
1928 in the first edition of Hilbert–Ackermann’s Principles of Mathematical Logic.
The most influential and widely recognized work of Skolem was his pioneering
research in model theory, the study of applications of logic to mathematics. The
most well-known result associated with him is the Löwenheim–Skolem Theorem,
an extension of an earlier work by Löwenheim published in 1920. It states that if
a first-order theory, that is, a set of sentences of first-order logic, has a model (a
structure that satisfies them all), then it has a countable model. This theorem has
some consequences that sound paradoxical, giving rise to what became known as the
Skolem paradox: assuming that the axiomatic system ZFC of set theory is satisfied
in some model, then it must have one with a countable universe, despite the fact
that it can prove the existence of uncountable sets. Likewise, the first-order theory
of the field of reals, which is uncountable, also has a countable model. (However,
according to some sources, Skolem did not believe in the existence of uncountable
sets, so some of the results associated with such sets may have been falsely attributed
to him.) Skolem also provided some of the first constructions of non-standard models
of arithmetic and set theory.
Anatoly Ivanovich Maltsev (14.11.1909–7.06.1967) was a
Russian–Soviet mathematician, well known for his work in
universal algebra and mathematical logic. He is recognized as
one of the founders, along with Löwenheim and Skolem, of
model theory, one of the main branches of mathematical logic
which studies the interaction between mathematics and logic.
Maltsev studied at Moscow University, completed his grad-
uate work there under A. N. Kolmogorov and obtained his PhD
in mathematics in 1941 from the Steklov Institute of Math-
ematics, with a dissertation on the Structure of isomorphic representable infinite
algebras and groups. During 1932–1960 Maltsev taught mathematics at the Ivanovo
Pedagogical Institute near Moscow. In 1960 he moved to Novosibirsk, where he was
head of the department of algebra at the Mathematical Institute of the Siberian branch
of the academy, as well as head of the chair of algebra and mathematical logic at the
University of Novosibirsk.
Maltsev obtained very important results on decidability and undecidability of
the first-order theories of various important classes of groups and other algebraic

194
Logic as a Tool
structures, and also made important contributions to the theory of Lie algebras. His
most influential legacy, however, was in mathematical logic, where in 1936 and
1941 he proved in full generality the two most fundamental results of model the-
ory: the compactness theorem, stating that if every finite subset of a first-order
theory (set of sentences) is satisfiable then the entire theory is satisfiable; and the
Löwenheim–Skolem Theorem (see the box on Skolem).
4.5
Resolution for first-order logic
We are now almost ready to extend the method of Resolution to first-order logic. One
more technical topic – unification – is needed for that, described in the following.
4.5.1
Propositional Resolution rule in first-order logic
The Propositional Resolution rule extended to first-order logic reads:
Res0 :
C ∨Q(s1, . . . , sn),
D ∨¬Q(s1, . . . , sn)
C ∨D
.
This rule, however, is not strong enough. For example, the clause set
{{P (x)}, {¬P (f(y))}}
is not satisfiable, as it corresponds to the unsatisfiable formula
∀x∀y(P(x) ∧¬P(f(y))).
However, the resolution rule above cannot produce an empty clause, because it cannot
unify the two clauses in order to resolve them. We therefore need a stronger derivation
mechanism that can handle cases like this. There are two natural solutions4:
1. Ground resolution: generate sufficiently many ground instances of every clause (over
the so-called Herbrand universe of the language) and then apply the standard Reso-
lution rule above.
In the example, ground resolution would generate the ground clauses {P (f(c))} and
{¬P(f(c))} for some constant symbol c.
This method is sound and complete but inefficient, as it leads to the generation of too
many unnecessary clauses. It will not be discussed further.
2. Resolution with unification introduce a stronger Resolution rule that first tries to
match a pair of clauses by applying a suitable substitution that would enable that pair
to be resolved. We present this method in further detail in the following.
4 Both proposed by John Alan Robinson; read more at the end of the section.

Deductive Reasoning in First-order Logic
195
4.5.2
Substitutions of terms for variables revisited
Recall that substitution of a term t for a variable x in a term s, denoted s[t/x], is the result
of simultaneous replacements of all occurrences of x in s by t. For instance:
f(g(x), f(y, x))[g(a)/x] = f(g(g(a)), f(y, g(a))).
This generalizes to simultaneous substitutions of several terms for different variables,
denoted s[t1/x1, . . . , tk/xk]. For instance:
f(g(x), f(y, x))[g(y)/x, f(x, b)/y] = f(g(g(y)), f(f(x, b), g(y))).
Note that the condition of simultaneity is important: if we first substituted g(y) for x
and then f(x, b) for y, the result would have been different (and wrong).
We can apply a substitution to several terms simultaneously, for example, when a sub-
stitution acts on an atomic formula P(t1, . . . , tn). For instance:
P(x, f(x, y), g(y))[g(y)/x, a/y] = P (g(y), f(g(y), a), g(a)).
Given the substitution σ = [t1/x1, . . . , tk/xk], the set of variables {x1, . . . , xk} is
called the domain of σ, denoted dom(σ).
Substitutions can be composed by consecutive application: the composition of substi-
tutions τ and σ is the substitution τσ obtained by applying τ followed by applying σ to
the result, that is, tτσ = (tτ)σ (note the order). For instance:
f(g(x), f(y, x))[f(x, y)/x][g(a)/x, x/y] =
f(g(f(x, y)), f(y, f(x, y)))[g(a)/x, x/y] =
f(g(f(g(a), x)), f(x, f(g(a), x))).
The composition of two substitutions τ = [t1/x1, . . . , tk/xk] and σ can be computed
as follows.
1. Extend dom(τ) to cover dom(σ) by adding to τ substitutions [x/x] for every variable
x in dom(σ) \ dom(τ).
2. Apply the substitution σ simultaneously to all terms [t1, . . . , tk] to obtain the substi-
tution [t1σ/x1, . . . , tkσ/xk].
3. Remove from the result all cases xi/xi, if any.
For instance:
[f(x, y)/x, x/y][y/x, a/y, g(y)/z] =
[f(y, a)/x, y/y, g(y)/z] =
[f(y, a)/x, g(y)/z].

196
Logic as a Tool
4.5.3
Unification of terms
A substitution σ unifies two words s and s′ if σ(s) = σ(s′). A substitution σ that uni-
fies the respective arguments of the literals Q(s1, . . . , sn) and Q(t1, . . . , tn), that is,
such that σ(s1) = σ(t1), . . . , σ(sn) = σ(tn), is called a unifier of Q(s1, . . . , sn) and
Q(t1, . . . , tn). For instance, the substitution σ = [f(c)/x, c/y, c/z] unifies the literals
Q(x, c, f(f(z))) and Q(f(y), z, f(x)).
Two terms are unifiable if they have a unifier; otherwise they are non-unifiable.
A unifier τ of two terms (or literals) is more general than a unifier ρ, if there is a
substitution σ such that ρ = τσ. (Note that, in the sense of this definition, every unifier
is more general than itself!) For instance, ρ = [c/x, f(c)/y] is a unifier of the literals
P(f(x)) and P(y), but τ = [f(x)/y] is a more general unifier because ρ = τσ, where
σ = [c/x].
A unifier of two terms (or literals) is their most general unifier (MGU) if it is more
general than any unifier of these terms (literals).
A most general unifier (if it exists) need not be unique. For instance, in the example
above, τ is a most general unifier, as well as [z/x, f(z)/y] for any variable z.
We are eventually interested in unifying literals, that is, (possibly negated) atomic
formulae. Note that two atomic formulae P(t1, . . . , tm) and Q(s1, . . . , sn) cannot be
unified unless P = Q and m = n. If these are satisfied, in order to unify the formulae we
need to unify the respective pairs of arguments, that is, we are looking for a (most general)
unifier of the system
t1 = s1,
...
tn = sn.
Informally, we do that by computing a most general unifier (if one exists) of the first
pair, then applying it to both lists of terms, then computing a most general unifier of the
next pair (if there is one) in the resulting lists, and applying it to both resulting lists of
terms. In order to unify the current pair of terms we apply the algorithm recursively to the
pair of lists of their respective arguments. The composition of all most general unifiers
computed as above, if they all exist, is a most general unifier of the two input lists.
A simple recursive algorithm for computing a most general unifier of two lists of terms
p and q is provided in pseudocode as follows.
procedure mgu(p, q)
θ := ϵ (the empty substitution).
Scan p and q simultaneously, left-to-right,
and search for the first corresponding subterms
where p and q disagree (mismatch).
If there is no mismatch, return θ.
Else, let s and t be the first respective subterms
in p and q where mismatch occurs.
If variable (s) and s /∈Var(t) then
θ := compose (θ, [t/s]);

Deductive Reasoning in First-order Logic
197
θ := compose (θ, mgu(pθ, qθ)).
Else, if variable (t) and t /∈Var (s) then
θ := compose (θ, [s/t]);
θ := compose (θ, mgu(pθ, qθ)).
Else, return failure.
end
Example 148 Some examples of unification of atomic formulae using the algorithm above
(where Bill and Jane are constant symbols, father, mother are unary functional
symbols and parents is a ternary predicate symbol) are as follows.
1. Literal 1: parents(x, father(x), mother(Bill)),
Literal 2: parents(Bill, father(Bill), y),
Most general unifier: [Bill/x, mother(Bill)/y].
2. Literal 1: parents(x, father(x), mother(Bill)),
Literal 2: parents(Bill, father(y), z),
Most general unifier: [Bill/x, Bill/y, mother(Bill)/z].
3. Literal 1: parents(x, father(x), mother(Jane)),
Literal 2: parents(Bill, father(y), mother(y)),
The procedure mgu starts computing: [ Bill/x, Bill/y, . . . failure].
The algorithm fails. Therefore, a unifier does not exist.
4. Literal 1: g(x,f(x)).
Literal 2: g(f(y),y).
The procedure mgu starts computing: [f(y)/x, . . . failure].
MGU does not exist. Indeed, any unifier would have to unify f(f(x)) with x, which is
impossible.
4.5.4
Resolution with unification in first-order logic
The first-order resolution rule is combined with a preceding unification of the clausal set
in order to produce a complementary pair of literals in the resolving clauses:
Res :
C ∨Q(s1, . . . , sn),
D ∨¬Q(t1, . . . , tn)
σ(C ∨D)
where:
(i) the two clauses have no variables in common (achieved by renaming of variables);
and
(ii) σ is a most general unifier of the literals Q(s1, . . . , sn) and Q(t1, . . . , tn).
Note that we require that the unifier applied in the rule is a most general unifier of
the respective literals in order not to weaken the resolvent unnecessarily. This is impor-
tant because that resolvent may have to be used in further applications of resolution. For
instance, resolving {P(a, y), Q(x)} with {¬P(a, x)} by using the unifier [a/x, a/y]

198
Logic as a Tool
would produce {Q(a)} which cannot be resolved with {¬Q(b)}, while using a most gen-
eral unifier, for example [y/x], would produce {Q(x)} which can then be further unified
and resolved with {¬Q(b)} by using [b/y].
Some examples of resolution with unification include the following.
{P (x)}, {¬P (f(y))}
{}
(MGU : [f(y)/x])
{P (a, y), Q(y)}{¬P(x, b), ¬Q(x)}
{Q(b), ¬Q(a)}
(MGU : [a/x, b/y])
{P(a, y), Q(y)}{¬P(x, f(x)), Q(f(x))}
{Q(f(a))}
(MGU : [a/x, f(a)/y]).
Note that both literals in the resolvent become equal.
{P (a, y), P(x, f(x))}{¬P(x, f(a))}
{}
(MGU : [a/x, f(a)/y]).
Note that, after unification, both literals in the first clause become P(a, f(a)), so they
become identified before the resolution rule is applied.
Factoring
In order for the method of Resolution for first-order logic as defined here to be complete,
it has to incorporate an auxiliary rule called Factoring which unifies two literals in the
same clause, whenever possible:
Fac:
{ . . . P(¯s), P(¯t) . . . }
σ{ . . . P(¯s) . . . }
,
(σ = MGU(¯s, ¯t))
For example,
{P(x), P(c), ¬Q(x, y)}
{P(c), ¬Q(c, y)}
(MGU : [c/x]).
As an exercise, prove that Factoring is a logically sound rule. Moreover, it is sometimes
necessary to be applied. For instance, consider the set of clauses
{P(x), P(c)}, {¬P(y), ¬P(c)}.
The empty clause cannot be derived from it without using Factoring, whereas if Factoring
is applied first to both clauses the empty clause is immediately derivable.
Hereafter, we assume that Factoring is incorporated into the Resolution method. Note
that it extends the deletion of repeated literals from Propositional Resolution.
Definition 149 A resolution-based derivation of a formula C from a list of formulae
A1, . . . , An, denoted A1, . . . , An, ⊢RES C, is a derivation of the empty clause {} from
the set of clauses obtained from A1, . . . , An, ¬C by successive applications of the rules
Res and Fac.

Deductive Reasoning in First-order Logic
199
Unlike Propositional Resolution, in some cases first-order Resolution may run forever,
that is, never terminate, when the conclusion does not follow logically from the premises.
It may also run forever even when the conclusion does follow logically from the premises,
if unnecessary resolvents are produced recurrently in the process. To avoid this, special
strategies or additional mechanisms for disposal of used-up clauses need to be applied.
4.5.5
Examples of resolution-based derivations
Example 150 Using the method of first-order Resolution, prove that:
∀x(P (x) →Q(x)) |= ∀xP(x) →∀xQ(x).
1. Transform
{∀x(P(x) →Q(x)), ¬(∀xP(x) →∀xQ(x))}
to clausal form:
{¬P(x), Q(x)}, {P(y)}, {¬Q(c)}
for some Skolem constant c.
2. Successive applications of Res:
(a) Unify P (x) and P(y) with MGU [y/x].
Then resolve {¬P(x), Q(x)} and {P(y)} to obtain {Q(y)}.
(b) Unify Q(c) and Q(y) with MGU [c/y].
Then resolve {¬Q(c)} and {Q(y)} to obtain {}.
Example 151 Show that
if Everybody loves somebody
and Everybody loves a lover
then Everybody loves everybody.
1. Formalize the assumptions and the goal conclusion, using the predicate L(x, y) mean-
ing “x loves y.”
Everybody loves somebody: ∀x∃yL(x, y).
Everybody loves a lover: ∀x(∃yL(x, y) →∀zL(z, x)).
Everybody loves everybody: ∀x∀yL(x, y).
2. Transform the set
{∀x∃yL(x, y), ∀x(∃yL(x, y) →∀zL(z, x)), ¬(∀x∀yL(x, y))}
to a clausal form:
{L(x, f(x))}, {¬L(x1, y), L(z, x1)}, {¬L(a, b)}
for some Skolem constants a, b and a Skolem function f.

200
Logic as a Tool
3. Successive applications of Res:
(a) Unify L(z, x1) and L(a, b) with MGU [b/x1, a/z] and resolve {¬L(x1, y),
L(z, x1)} with {¬L(a, b)} to obtain {¬L(b, y)}.
(b) Unify L(x, f(x)) and L(b, y) with MGU [b/x, f(b)/y] and resolve {L(x, f(x))}
with {¬L(b, y)} to obtain {}.
Example 152 Verify the logical consequence
∀x∃yR(x, y), ∀x∀y∀z((R(x, y) ∧R(y, z)) →R(x, z)) |= ∃xR(x, x).
1. Transform
{∀x∃yR(x, y), ∀x∀y∀z((R(x, y) ∧R(y, z)) →R(x, z)), ¬(∃xR(x, x))}
to a clausal form:
C1 = {R(x, f(x))}, C2 = {¬R(u, y), ¬R(y, z), R(u, z)}, C3 = {¬R(v, v)}
for some unary Skolem function f.
2. Successive applications of Res to the clausal set
C1 = {R(x, f(x))}, C2 = {¬R(u, y), ¬R(y, z), R(u, z)}, C3 = {¬R(v, v)}
(wherever necessary, we rename variables in the derived clauses to keep all clauses
with disjoint sets of variables):
(a) Unify R(u, z) in C2 and C3, with MGU [v/u, v/z] and resolve: C4 =
{¬R(v, y), ¬R(y, v)}.
(b) Unify R(v, y) in C4 and R(x, f(x)) in C1 with MGU [x/v, f(x)/y] and resolve:
C5 = {¬R(f(x), x)}.
After renaming x: C5 = {¬R(f(x1), x1)}.
(c) Unify R(u, z) in C2 and R(f(x1), x1) in C5 with MGU [f(x)/u, x/z] and
resolve: C6 = {¬R(f(x), y), ¬R(y, x)}.
After renaming: C6 = {¬R(f(x′), y′), ¬R(y′, x′)}.
(d) Unify R(f(x′), y′) in C6 and R(x, f(x)) in C1 with MGU [f(x′)/x, f(f(x′))/y′]
and resolve: C7 = {¬R(f(f(x′)), f(x′))}.
After renaming, C7 = {¬R(f(f(x2)), f(x2))}.
Etc.
Thus, an infinite set of clauses can be generated:
{¬R(f(x1), x1)}, {¬R(f(f(x2)), f(x2))}, {¬R(f(f(f(x3))), f(f(x3)))}, . . .
but the empty clause cannot be derived.
In fact, the logical consequence does not hold. A counter-model is the set of natural
numbers with R(x, y) interpreted as x < y.

Deductive Reasoning in First-order Logic
201
4.5.6
Resolution for first-order logic with equality
As we have seen in the previous sections on deductive systems, logical deduction in
first-order logic with equality requires special attention in the form of either additional
axioms or inference rules. The method of Resolution presented so far does not involve
rules for the equality. Such rules can be added to produce a sound and complete
Resolution-based deductive system for first-order logic with equality. To begin with, we
can take all axioms for the equality for the respective first-order language from Section
4.1, transform them all into clausal form and add the resulting clauses to the set of
clauses to which we apply Resolution. This approach, however, can be rather inefficient.
A more efficient way to extend the method of first-order Resolution to handle equality
and equational reasoning is to keep only the identity unit clauses t = t for any term t,
and add the following inference rule, called Paramodulation5:
Par :
C ∨s1 = s2,
D ∨Q(t, t1, . . . , tn)
σ(C ∨D ∨Q(s2, t1, . . . , tn))
(σ = MGU(s1, t))
or
Par :
C ∨s2 = s1,
D ∨Q(t, t1, . . . , tn)
σ(C ∨D ∨Q(s2, t1, . . . , tn))
(σ = MGU(s1, t))
where the parent clauses are assumed to have no variables in common. The resulting clause
is called a paramodulant of the parent clauses.
This rule enables direct substitution of terms by equal terms deep inside literals, rather
than deriving such substitutions every time by using the clauses for equality.
Example 153 Here are some simple examples of applications of Paramodulation, where
a, b, c are constants and P, Q are unary predicates.
1.
a = b, P(a)
P(b)
2. The set of literals {a = b, P(a), ¬P(b)} is not satisfiable. Indeed, the empty clause
is derivable from the clause set {{a = b}, {P (a)}, {¬P(b)}} using the previous
derivation.
3.
P(y) ∨a = f(b), Q(f(x), x)
P(y) ∨Q(a, b)
Here we have used [b/x] = MGU(f(b), f(x)).
4.
P(a) ∨f(g(a)) = g(b), Q(f(x), x) ∨¬P(x)
P(a) ∨Q(g(b), g(a)) ∨¬P(g(a))
,
where we have used σ = [g(a)/x], and
P(a) ∨f(g(a)) = g(b), Q(f(x), x) ∨¬P(x)
P(a) ∨Q(f(g(b)), f(g(a))) ∨¬P(g(b))
,
5 Originally introduced by G. Robinson and L. Wos in the 1969 paper Paramodulation and theorem-proving in
first-order theories with equality.

202
Logic as a Tool
where we have used σ = [g(b)/x],
5. Applications of Paramodulation to equalities of terms sharing a variable may lead to
generation of an infinite series of new clauses, for instance:
f(x) = x, C ∨Q(a)
C ∨Q(f(a))
,
f(x) = x, C ∨Q(f(a))
C ∨Q(f(f(a)))
,
. . .
For more examples and some applications of Resolution with equality, see Exercises
10, 11, and 14.
The proof of the following theorem for the case of Resolution with equality is outlined
in Section 4.6.
Theorem 154 (Soundness and completeness of RES)
1. The system RES of Resolution with Unification and Factoring for first-order
logic without equality is sound and complete, that is, for all first-order formulae
A1, . . . , An, C of the language without equality:
A1, . . . , An, ⊢RES C iff A1, . . . , An, |= C.
2. Likewise, the system RES= of Resolution with Unification, Factoring, and Paramodu-
lation for first-order logic with equality is sound and complete, that is, for all first-order
formulae A1, . . . , An, C of the language with equality:
A1, . . . , An, ⊢RES= C iff A1, . . . , An, |= C.
Sometimes, it is said that the method of Resolution is refutation-complete.
4.5.7
Optimizations and strategies for the method of Resolution
Like Propositional Resolution, the method of Resolution for first-order logic is amenable
to various optimizations by means of special additional inference rules and derivation
strategies that go beyond the scope of this book. Such optimizations are much more impor-
tant for first-order Resolution because if the search for a derivation of the empty clause is
not cleverly organized, it may not terminate even if such a derivation exists. The techniques
of Tautology Deletion, Subsumption Deletion, and Removal of Clauses with Mono-polar
Literals still apply to first-order Resolution. Moreover, they can be combined with each

Deductive Reasoning in First-order Logic
203
other and with suitable unification pre-processing, for example to produce more specific
optimization rules and techniques.
In particular, widely used additional optimizing techniques are based on special strate-
gies for application of the Resolution rule that, instead of removing redundant clauses,
prevent the generation of useless clauses. For instance, the Set-of-support resolution strat-
egy is based on the idea that some of the clauses are of higher importance and should be
treated with priority. That leads to the idea of designating a subset of the given clause set
S as a support set T if its complement S \ T is known to be satisfiable, and to require
that Resolution is only applied when at least one of the resolved clauses comes from T as
there is no chance to produce the empty clause from within S \ T.
Other very successful strategies include Hyperresolution, used to reduce the number
of intermediate resolvents by combining several resolution steps into a single inference
step, and Linear Resolution, requiring always that one of the resolved clauses is the
most recently derived resolvent, therefore organizing the deduction in a simple linear
shape, which is easily and efficiently implementable. All strategies mentioned above are
logically sound and preserve the (refutation) completeness of the Resolution method. A
special version of the Linear Resolution strategy, called Selective Linear Definite Clause
Resolution, or just SLD-resolution, is the derivation strategy of the logic programming
language Prolog, briefly discussed in Section 5.4.
There are also some practically very efficient but generally not completeness-preserving
strategies, such as Unit Resolution, where one of the resolved clauses must always be a
literal, and Input Resolution, where one of the resolved clauses is always selected from
the set of clauses generated from the initial assumptions (axioms). Other important types
of incomplete, yet practically efficient, strategies are the ordering strategies based on
some partial ordering imposed on the predicate symbols, terms, literals, or clauses used
in the deduction, based on various heuristics. Ordered resolution treats clauses not as sets
but as (ordered) lists of literals and chooses candidate clauses for resolving in a priority
order according to these lists.
For more on practical applications of the method of Resolution, see Section 5.4.
References for further reading
For more technical details, discussions, and examples of first-order Resolution with uni-
fication, see Gallier (1986), Nerode and Shore (1993), Ebbginhaus et al. (1996), Fitting
(1996), Chang and Lee (1997), Barwise and Echemendy (1999), Robinson and Voronkov
(2001), Hedman (2004), and Ben-Ari (2012).
Exercises
4.5.1
Using the algorithm, compute a most general unifier (MGU) (if one exists) for
the following pairs of literals:
(a) P (f(x), g(y)), P(y, g(x)); and
(b) P (x, f(y), y), P(f(y1), f(g(z)), g(a)).
For each of the following exercises use first-order Resolution with unification.
4.5.2
Prove the following logical validities and consequences.

204
Logic as a Tool
(a) |= ∀xP(x) →∀yP(y)
(b) |= ∃xP(x) →∃yP(y)
(c) |= ∃x(P(x) →∀yP(y))
(d) ∀xA(x) |= ¬∃x¬A(x)
(e) ¬∃x¬A(x) |= ∀xA(x)
(f) ∃xA(x) |= ¬∀x¬A(x)
(g) ¬∀x¬A(x) |= ∃xA(x)
(h) ∃x∃yA(x, y) |= ∃y∃xA(x, y).
4.5.3
Suppose x is not free in Q. Prove the following logical consequences.
(a) ∀x(P(x) ∨Q) |= ∀xP(x) ∨Q
(b) ∀xP(x) ∨Q |= ∀x(P(x) ∨Q)
(c) ∃x(P(x) ∧Q) |= ∃xP(x) ∧Q
(d) ∃xP(x) ∧Q |= ∃x(P(x) ∧Q)
(e) ∀x(Q →P(x)), Q |= ∀xP(x)
(f) ∃xP(x) →Q |= ∀x(P(x) →Q)
(g) ∃x(P(x) →Q), ∀xP(x) |= Q
(h) ∃x(Q →P(x)), Q |= ∃xP(x).
4.5.4
Check which of the following logical consequences hold. For those which do
not, construct a counter-model, that is, a structure and assignment in which all
premises are true while the conclusion is false.
(a) ∀xA(x), ∀xB(x) |= ∀x(A(x) ∧B(x))
(b) ∀x(A(x) ∧B(x)) |= ∀xA(x) ∧∀xB(x)
(c) ∀x(A(x) ∨B(x)) |= ∀xA(x) ∨∀xB(x)
(d) ∀xA(x) ∨∀xB(x) |= ∀x(A(x) ∨B(x))
(e) ∀xA(x) →∀xB(x) |= ∀x(A(x) →B(x))
(f) ∃x(A(x) ∧B(x)) |= ∃xA(x) ∧∃xB(x)
(g) ∃xA(x), ∃xB(x) |= ∃x(A(x) ∧B(x))
(h) ∃xA(x) ∨∃xB(x) |= ∃x(A(x) ∨B(x))
(i) ∃x(A(x) ∨B(x)) |= ∃xA(x) ∨∃xB(x)
(j) ∃x(A(x) →B(x)) |= ∃xA(x) →∃xB(x)
(k) ∃xA(x) →∃xB(x) |= ∃x(A(x) →B(x)).
4.5.5
Formalize the following arguments in first-order logic by using suitable predi-
cates, interpreted in a suitable domain. Check whether each is logically correct.
(a)
All logicians are clever.
All clever people are rich.
Therefore all logicians are rich.
(b)
All natural numbers are integers.
Some integers are odd numbers.
Therefore some natural numbers are odd numbers.

Deductive Reasoning in First-order Logic
205
(c)
Every child is a pedestrian.
No driver is a pedestrian.
Johnie is a child.
Therefore somebody is not a driver.
(d)
All penguins are birds.
Some penguins are white.
All penguins eat some fish.
Therefore, there is a white bird that eats some fish.
(e)
All penguins are birds.
No penguins can fly.
All seagulls can fly.
All fat birds are penguins.
Some fat birds are white.
Therefore, there is a fat white bird that is not a seagull.
(f)
No yellow plonks are qlinks.
Some object is a plonk or not a qlink.
Therefore, there is an object which is not a yellow qlink.
(g)
Victor is a hungry student.
Every student either studies or parties.
Every student who is hungry does not study.
Therefore, some student parties.
4.5.6
Formalize each of the following statements in first-order logic and check whether
it logically implies the other. If any of the logical consequences do not hold, give
an appropriate counter-model.
A: “No man is happy only when he is drunk.”
B: “There is a happy man and there is a man who is not drunk.”
4.5.7
Using the predicates: D(x) for “x is a dragon,” E(x) for “x is an elf,” F(x) for
“x is a fairy,” Fl(x) for “x can fly,” G(x) for “x is a goblin,” H(x) for “x is a
hobbit,” T(x) for “x is a troll,” U(x) for “x is ugly,” and L(x, y) for “x likes
y,” formalize each of the following arguments in first-order logic, in the domain
of all “mythic creatures.” Check whether each is logically valid. If it is not valid,
give an appropriate counter-model.

206
Logic as a Tool
(a)
All fairies are elves. Hobbits do not like any elves.
Kermit likes a goblin or a fairy. Everyone who likes
any goblins likes no trolls.
Therefore, if Kermit likes any trolls then Kermit is not a hobbit.
(b)
All trolls are elves or goblins. No troll can fly, unless it is an elf.
All dragons can fly.
All ugly goblins are trolls. Some goblins are ugly.
Therefore, there is an ugly mythic creature that is not a dragon.
(c) Change the first premise in the previous argument to
“All trolls are either elves or goblins, but not both.”
Formalize it and check if the resulting argument is logically valid.
4.5.8
Formalize the following arguments in first-order logic by using suitable predi-
cates interpreted in suitable domains. Check whether each is logically valid. If it
is not valid, give an appropriate counter-model.
(a)
Some bachelors love every woman they know.
Every bachelor is a man.
Eve is a woman whom every man knows.
Some bachelor loves a woman.
(b) Replace the conclusion in the previous example by
“Every bachelor loves a woman”
and check again whether the resulting argument is logically valid.
(c)
Every man admires some brave woman.
Juan is a man who admires every pretty woman.
Lara is a brave or pretty woman.
Some man admires Lara.
4.5.9
Formalize the following arguments in first-order logic by using suitable predi-
cates, interpreted in a suitable domain. Check whether each is logically valid. If
it is not valid, give an appropriate counter-model.
(a)
All talking robots can walk.
Some walking robot can construct every talking robot.
Therefore, some talking robot can construct some walking
robot and every talking robot.

Deductive Reasoning in First-order Logic
207
(b)
Some devices are robots.
All robots can talk or walk or reason logically.
No non-talking robot can reason logically.
All talking robots can construct every logically reasoning device.
A device can construct a robot only if it is itself a robot.
Some logically reasoning devices can construct
every non-walking robot.
Therefore, some talking robot can construct itself.
(c) Change the last premise in the argument above
“Some logically reasoning devices can construct every non-walking
robot”
to:
“All logically reasoning devices can construct every non-walking
robot.”
Formalize it and check if the resulting argument is logically valid.
(d) Add to the original argument in item (a) the premise
“Creepy is a robot that cannot walk.”
Formalize it and check if the resulting argument is logically valid.
4.5.10
Prove that the following is a logically sound rule:
¬s = t ∨P(s)
σ(P(t))
where the terms s and t are unifiable and σ = MGU(s, t).
4.5.11
Transform all axioms of equality from Section 4.1 for the first-order language
with one constant symbol c, one unary predicate symbol P, and one binary func-
tional symbol f into clausal form. Then use Resolution with Paramodulation to
prove their validity.
4.5.12
Using Resolution with Paramodulation, derive each of the following (the first five
in the list are the axioms of H for the equality):
(a) (Ax=1) x = x
(b) (Ax=2) x = y →y = x
(c) (Ax=3) x = y ∧y = z →x = z
(d) (Axf) x1 = y1 ∧. . . ∧xn = yn →f(x1, . . . , xn) = f(y1, . . . , yn) for
n-ary functional symbol f
(e) (Axr) x1 = y1 ∧. . . ∧xn = yn →(p(x1, . . . , xn) →p(y1, . . . , yn)) for
n-ary predicate symbol p.
4.5.13
Using Resolution with Paramodulation, check whether each of the following
validities and logical consequences holds.
(a) |= x1 = y1 ∧x2 = y2 →g(f(x1, f(x2))) = g(f(y1, f(y2)))
(Universal quantification is assumed but omitted.)

208
Logic as a Tool
(b) |= ∀x(x = f(x) →(P(f(f(x))) →P(x)))
(c) ∀x∀y(f(x) = y →g(y) = x) |= ∀z(g(f(z)) = z)
(d) ∀x∀y(f(x) = y →g(y) = x) |= ∀z(f(g(z)) = z)
(e) ∀x∀y(f(x) = y →g(y) = x), ∀x∀y(g(x) = g(y) →x = y) |=
∀z(f(g(z)) = z)
For more exercises on derivations with equality, on sets, functions, and relations,
see Section 5.2.7.
4.5.14
Consider the first-order theory of groups G, consisting of the following
axioms, where e is the constant for group identity, ◦is the (binary) group
operation, and ′ is the (unary) inverse operation, and all variables below are
assumed universally quantified.
G1: (x ◦y) ◦z = x ◦(y ◦z) (associativity of ◦)
G2: x ◦e = x (e is a right identity)
G3: x ◦x′ = e (′ is a right inverse operation)
Using Resolution with Paramodulation, derive the following as logical conse-
quences from the axioms G1–G3. Again, all variables in the formulae below are
assumed universally quantified.
(a)
x′ ◦x = e (′ is a left inverse operation)
(b)
e ◦x = x (e is a left identity)
(c)
(x′)′ = x
(d)
(x ◦y)′ = y′ ◦x′
(e)
x = y ◦z →y = x ◦z′ ∧z = y′ ◦x
(f)
Adding to G the axiom x ◦x = e, derive the commutativity law x ◦y =
y ◦x.
(g∗∗) Adding to G the axiom x ◦x ◦x = e, derive the identity ((x, y), y) = e
where (x, y) = x ◦y ◦x′ ◦y′.
Jacques Herbrand (12.02.1908–27.07.1931) was a French math-
ematician and logician who made groundbreaking contributions to
logic and automated reasoning, even though he died at the age of
only 23 years old.
Herbrand worked in mathematical logic in particular and in
what would become computability theory, where he essentially
introduced recursive functions. He also proved what is now called
the Herbrand Theorem, informally stating that a first-order
formula in a prenex form containing existential quantifiers is only
valid (respectively, provable) in first-order logic if and only if some disjunction of
ground substitution instances of the quantifier-free subformula of A is a tautology
(respectively, derivable in propositional logic). Herbrand’s Theorem therefore
reduces in a precise sense validity and theoremhood from first-order logic to
propositional logic.

Deductive Reasoning in First-order Logic
209
Using his theorem, he also contributed to Hilbert’s program in the foundations
of mathematics by providing a constructive consistency proof for a weak system of
arithmetic.
Herbrand completed his doctorate at École Normale Supérieure in Paris under
Ernest Vessiot in 1929 and presented his doctoral thesis Investigations in proof the-
ory. He then joined the army and only defended his thesis at the Sorbonne in the
following year. He was awarded a Rockefeller fellowship that enabled him to study
in Germany in 1931, first with John von Neumann in Berlin, then with Emil Artin in
Hamburg, and finally with Emmy Noether in Göttingen.
Herbrand submitted his principal study of proof theory and general recursive func-
tions On the consistency of arithmetic early in 1931. While the essay was under
consideration, Gödel published his celebrated paper announcing the impossibility of
formalizing within any sufficiently expressive theory that theory’s consistency proof.
Herbrand studied Gödel’s paper and wrote an appendix to his own study explain-
ing why Gödel’s result did not contradict his own. In July of that year he died in a
mountain-climbing accident in the French Alps. His other very important work in
logic, On the consistency of arithmetic, was published posthumously.
In 1992 the International Association for Automated Reasoning’s International
Conference on Automated Deduction (CADE) established the annual Herbrand
Award for Distinguished Contributions to Automated Reasoning.
John Alan Robinson (b. 1928) is a British–American philoso-
pher, mathematician, and computer scientist, best known for
developing the method of resolution for first-order logic.
Robinson was born in England and moved to the US in 1952.
He obtained a PhD in philosophy from Princeton University
in 1956 for a thesis on Causation, probability and testimony.
In the 1960s he worked as an operations research analyst at
Du Pont and as a visiting researcher at the Argonne National
Laboratory, while working at Rice University. He joined Syracuse University as a
Distinguished Professor of Logic and Computer Science in 1967 and has held the
position of a Professor Emeritus there since 1993.
Robinson introduced the method of resolution with unification for first-order
logic in his seminal 1965 paper A machine-oriented logic based on the Resolution
Principle. For his pioneering contributions he received the 1996 Herbrand Award for
Distinguished Contributions to Automated Reasoning. His work was also instrumen-
tal in the development of the concept of logic programming and, in particular, for the
programming language Prolog, for which the Association for Logic Programming
awarded him the honorary title Founder of Logic Programming in 1997.

210
Logic as a Tool
4.6
Supplementary: Soundness and completeness of the deductive
systems for first-order logic
Here I provide a detailed outline of the proof of the most important technical result of this
course: the soundness and completeness of each of the deductive systems for first-order
logic presented in this book. I outline the proof for a generic deductive system called
D which will apply, mutatis mutandis, to each of the concrete deductive systems for
first-order logic introduced here, namely H, ND, ST, and RES. The proof is based
on several claims which are left as exercises and listed in the exercises section. Some of
them are proved generically for any of these deductive systems, while the proofs of others
are specific to each of them and sometimes require quite different arguments based on the
specific notions of derivation. I will indicate that in the exercises.
The first such completeness result for a certain axiomatic system for first-order logic,
presented in Hilbert and Ackermann’s 1928 book Grundzüge der Theoretischen Logik,
was proved by K. Gödel in the late 1920s. The proof presented here is more modern and
elegant, based on L. Henkin’s completeness proof (around 1950)6. The proof will build
on the completeness of the propositional deductive systems outlined in Section 2.7. Extra
work and several substantial additional steps are, however, needed because, instead of a
6 Read more on Gödel and Henkin in the biographical boxes at the end of this section.

Deductive Reasoning in First-order Logic
211
satisfying truth assignment, we now have to build a whole first-order structure as a model
for our consistent theory. There is at least one additional problem: the maximal consistent
theory constructed by Lindenbaum’s Lemma may not be “rich” enough to provide all the
information needed for the construction of such a model. In particular, it may occur that a
formula ∃xA(x) belongs to the maximal consistent theory, while for every term t in the
language that is free for x in A, not A(t/x) but its negation is in that theory; the theory
therefore contains no “witness” of the truth of ∃xA(x). We will resolve that problem with
a few extra technical lemmas, the proofs of which require the deductive power of D.
4.6.1
First-order theories
First, we need to adjust and extend our terminology from the propositional case.
A (first-order) theory is any set of first-order sentences. A theory Γ is satisfiable if it
has a model, that is, a structure S such that S |= Γ; otherwise it is unsatisfiable (or not
satisfiable). Proposition 60 also applies to first-order theories.
4.6.1.1
Semantically closed, maximal, and complete theories
The notions of semantically closed, maximal, and complete first-order theories are defined
as for propositional logic in Section 2.7.
4.6.1.2
Deductively consistent, maximal, and complete theories
The notions of D-consistent, D-maximal, deductively closed in D, and D-complete
first-order theories are defined as for propositional logic; see Definitions 61 and 69. In
particular, Propositions 62 and 63 also apply here.
Proposition 155 For any first-order theory Γ and a sentence B, the following hold.
1. Γ ∪{B} is D-consistent iff Γ ⊬D ¬B.
2. Γ ⊢D B iff Γ ∪{¬B} is D-inconsistent.
3. If Γ ∪{B} is D-inconsistent and Γ ∪{¬B} is D-inconsistent then Γ is
D-inconsistent.
Lemma 156 A first-order theory Γ is a maximal D-consistent theory iff it is deductively
closed in D and D-complete.
Theorem 157 (Maximal consistent theories) For every maximal D-consistent
first-order theory Γ and sentences A, B the following hold.
1. ¬A ∈Γ iff A /∈Γ.
2. A ∧B ∈Γ iff A ∈Γ and B ∈Γ.
3. A ∨B ∈Γ iff A ∈Γ or B ∈Γ.
4. A →B ∈Γ iff A ∈Γ implies B ∈Γ (i.e., A /∈Γ or B ∈Γ).

212
Logic as a Tool
5. If ∀xA(x) ∈Γ then A[t/x] ∈Γ for any term t free for x in A.
6. If A[t/x] ∈Γ for any term t free for x in A then ∃xA(x) ∈Γ.
Proof. The Boolean cases are as for propositional logic. For the quantifier cases, we need
to prove that the following hold for each of the deductive systems H, ND, ST, and RES:
1. ∀xA(x) ⊢D A[t/x], and
2. A[t/x] ⊢D ∃xA(x).
I leave these proofs as exercises for the reader. ■
4.6.2
Soundness
Hereafter we fix an arbitrary first-order language (with equality) L.
Theorem 158 (Soundness) For every first-order theory Γ and a formula A:
if Γ ⊢D A then Γ |= A.
Proof. This is performed by induction on derivations in D. Exercise. ■
Corollary 159 For any first-order theory Γ, formula A, and structure S: if S |= Γ and
Γ ⊢D A then S |= A.
Corollary 160 Every satisfiable theory Γ is D-consistent.
4.6.3
Herbrand structures and interpretations
Definition 161 (Herbrand structure) Let L be a first-order language (with equality),
containing at least one constant symbol.
1. The Herbrand universe of L is the set HL of all ground terms in L.
2. The Herbrand structure is any L-structure H with domain HL, where all functional
symbols are interpreted as follows. For any ground terms t1, . . . , tn ∈HL:
f H(t1, . . . , tn) := f(t1, . . . , tn).
Such an interpretation is called a Herbrand interpretation. See a note on their importance
at the end of the section.
The interpretation of constant and functional symbols in a given structure S naturally
extends to an interpretation tS of every ground term t. Note that for every such ground
term, we have tH = t.
Definition 162 Let S be any L-structure with domain X. An equivalence relation ∼on
X is a congruence in S if the following hold.

Deductive Reasoning in First-order Logic
213
1. For every n-ary functional symbol f in L and a1, . . . , an, b1, . . . , bn ∈X:
if a1 ∼b1, . . . , an ∼bn then f S(a1, . . . , an) ∼f S(b1, . . . , bn).
2. For every n-ary predicate symbol r in L and a1, . . . , an, b1, . . . , bn ∈X:
if a1 ∼b1, . . . , an ∼bn then rS(a1, . . . , an) holds iff rS(b1, . . . , bn) holds.
Definition 163 Let S be any L-structure with domain X and ∼be a congruence in S.
The quotient-structure of S with respect to ∼is the L-structure S∼with domain
X∼= {[a]∼| a ∈X}
where [a]∼is the ∼-equivalence class of a and the non-logical symbols are interpreted in
S∼as follows.
1. For every n-ary functional symbol f in L and [a1]∼, . . . , [an]∼∈X∼:
f S∼([a1]∼, . . . , [an]∼) := [f S(a1, . . . , an)]∼.
2. For every n-ary predicate symbol r in L and [a1]∼, . . . , [an]∼∈X∼:
rS∼([a1]∼, . . . , [an]∼)
iff
[rS(a1, . . . , an)].
Lemma 164 The definitions above are correct, that is, independent of the representatives
of the elements of X∼.
Proof. This is immediate from the definition of congruence. Exercise. ■
4.6.3.1
Canonical structures for first-order theories
From this point onward we assume that the language L has at least one constant symbol.
If not, such can always be added to the language conservatively, that is, without affecting
the derivations in the original language.
Definition 165 (Herbrand structure for a theory) Given a first-order theory Δ, we
define the Herbrand structure H(Δ) for Δ as follows. For every n-ary predicate symbol
r in L, other than the equality, and ground terms t1, . . . , tn ∈HL:
rH(t1, . . . , tn)
iff
Δ ⊢D r(t1, . . . , tn).
We now define the following binary relation in HL:
t1 ∼Δ t2
iff
Δ ⊢D t1 = t2.
Lemma 166 For any first-order theory Δ, the relation ∼Δ is a congruence in H(Δ).
Proof. This follows from the axioms and rules for the equality in D. Exercise. ■

214
Logic as a Tool
Definition 167 (Canonical structure for a theory) Given a first-order theory Δ, the
canonical structure for Δ is
S(Δ) := H(Δ)∼Δ.
Lemma 168 For any first-order theory Δ and an atomic sentence A:
Δ ⊢D A iff S(Δ) |= A.
Proof. Exercise. ■
4.6.4
Henkin theories and Henkin extensions
4.6.4.1
Henkin theories
Definition 169 (Henkin theory) A first-order theory Δ is a Henkin theory if for
every formula with one free variable B(x) the following holds: if Δ ⊢D ∃xB(x) then
Δ ⊢D B[t/x] for some ground term t, called a witness for ∃xB(x) in Δ.
Lemma 170 For any first-order theory Δ, formula B = B(x1, . . . , xn), ground terms
t1, . . . , tn, structure S, and a variable assignment v in S such that v(xi) = tS
i for i =
1, . . . , n, the following holds:
S, v |= B(x1, . . . , xn) iff S |= B[t1/x1, . . . , tn/xn].
Proof. This is proved by induction on the number of logical connectives in B.
Exercise. ■
Theorem 171 (Henkin models) For every maximal D-consistent Henkin theory Δ,
S(Δ) |= Δ.
Proof. It is sufficient to prove that, for every sentence A,
Δ ⊢D A iff S(Δ) |= A.
The proof is by induction on the number of occurrences of logical connectives in A.
1. The case of atomic sentence follows from Lemma 168.
2. The cases A = ¬B, A = A1 ∧A2, A = A1 ∨A2, and A = A1 →A2 are straightfor-
ward, using Theorem 157.
3. Let A = ∃xB. If x is not free in B then B is also a sentence. In that case, |= B ↔∃xB
and ⊢D B ↔∃xB (exercise), and therefore Δ ⊢D ∃xB iff Δ ⊢D B iff (by inductive
hypothesis) S(Δ) |= B iff S(Δ) |= ∃xB.

Deductive Reasoning in First-order Logic
215
Now, suppose x occurs free in B, that is, B = B(x).
If S(Δ) |= ∃xB then S(Δ), v |= B for some variable assignment v. Let v(x) = [t]∼Δ,
where t is a ground term. Then, S(Δ) |= B[t/x] by Lemma 170, hence, by the induc-
tive hypothesis Δ ⊢D B[t/x]. Since ⊢D B[t/x] →∃xB, we obtain Δ ⊢D ∃xB.
Conversely, let Δ ⊢D ∃xB. Since Δ is a Henkin theory, we have Δ ⊢D B[t/x]
for some ground term t. Then S(Δ) |= B[t/x] by the inductive hypothesis, hence
S(Δ), v |= B for any variable assignment v such that v(x) = [t]∼Δ by Lemma 170.
Therefore, S(Δ) |= ∃xB.
4. The case A = ∀xB(x) is analogous. It can also be reduced to the previous cases, using
|= ∀xB(x) ↔¬∃¬B and ⊢D ∀xB(x) ↔¬∃¬B. ■
Thus, every maximal Henkin theory has a model. The model S(Δ) is called the Henkin
model of such a theory Δ.
4.6.4.2
Henkin extensions of theories
We now show that every D-consistent theory Γ can be extended to a maximal D-consistent
Henkin theory.
Lemma 172 (Lemma about constants) For every first-order theory Γ, formula A and
a constant symbol c not occurring in Γ ∪A:
Γ ⊢D A[c/x] iff Γ ⊢D ∀xA(x).
Proof. The one direction is straightforward, since ⊢D ∀A(x) →A[c/x]. The other is spe-
cific to each deductive system D. Exercise. ■
Corollary 173 For any theory Γ and a sentence ∃xA(x), if Γ ∪∃xA(x) is D-consistent
then Γ ∪A[c/x] is D-consistent for any constant symbol c not occurring in Γ ∪A.
Proof. Suppose Γ ∪A[c/x] ⊢D ⊥. Then Γ ⊢D ¬A[c/x], therefore Γ ⊢D ∀x¬A(x)
by Lemma 172, hence Γ ⊢D ¬∃xA(x), which contradicts the D-consistency of
Γ ∪∃xA(x). ■
Lemma 174 (Conservative extensions) Let Γ be any theory in a first-order language
L and let L+ be the extension of L obtained by adding countably many new constants
c1, . . . , cn, . . . Then for any formula A in L, Γ ⊢D A in L iff Γ ⊢D A in L+.
In particular, Γ is D-consistent in L iff Γ is D-consistent in L+.
Proof. Clearly, if Γ is D-consistent in L+ then Γ is D-consistent in L. The converse
direction is specific to each deductive system D. See exercises. ■
Lemma 175 (Compactness of the deductive consequence) The deductive consequence
in D is compact in the following sense: for any theory Γ and a sentence A, it holds that
Γ ⊢D A iff Γ0 ⊢D A for some finite Γ0 ⊆Γ.

216
Logic as a Tool
This property is equivalent (exercise) to the following: a first-order theory Γ is
D-consistent iff every finite subset of Γ is D-consistent.
Theorem 176 (Lindenbaum–Henkin Theorem) Let Γ be D-consistent theory in a
countable language L and let L+ be the extension of L obtained by adding countably
many new constants c1, . . . , cn, . . . Then Γ can be extended to a maximal Henkin
theory H(Γ) in L+.
Proof. Let A0, A1, . . . be a list of all sentences in L+. (NB: these are still only countably
many!)
We define a chain by inclusion of theories Γ0, Γ1, . . . by recursion on n as follows.
• Γ0 := Γ.
• Γn+1 :=
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
Γn ∪{An}
if Γn ⊢D An and An is not of the type ∃xB;
Γn ∪{An} ∪{B[c/x]} if Γn ⊢D An and An = ∃xB, where c is the
ﬁrst new constant that does not occur in
Γn ∪{An};
Γn ∪{¬An}
if Γn ⊬D An.
Note that every Γn is an D-consistent theory. (Exercise: prove this by induction on n,
using Corollary 173.)
We now define
H(Γ) :=

n∈N
Γn.
Clearly, Γ ⊆H(Γ). We claim that H(Γ) is a maximal D-consistent Henkin theory.
Indeed:
• H(Γ) is D-consistent. To prove this, suppose otherwise. Then H(Γ) ⊢D A and
H(Γ) ⊢D ¬A. Since the deductive consequence is compact by Lemma 175, it follows
that Γn ⊢D A and Γn ⊢D ¬A for some large enough index n (exercise: complete the
details here), which contradicts the consistency of Γn.
• H(Γ) is maximal D-consistent. Indeed, take any sentence A. Let A = Am. Then
Am ∈Γm+1 or ¬Am ∈Γm+1, hence A ∈H(Γ) or ¬A ∈H(Γ), hence H(Γ) cannot
be extended to a larger D-consistent theory.
• H(Γ) is a Henkin theory. The proof of this is left as an exercise. ■
The theory H(Γ) defined above is called a maximal Henkin extension of Γ.
Remark: The result above also applies to uncountable languages, by using Zorn’s
Lemma.

Deductive Reasoning in First-order Logic
217
4.6.5
Completeness theorem
We are now ready to state and prove the completeness theorem generically for any of the
deductive systems D introduced here.
Theorem 177 (Model existence) Every D-consistent first-order theory Γ has a model.
Proof. Given a D-consistent theory Γ, construct a maximal Henkin theory H(Γ)
extending Γ, using Theorem 176. We then take the Henkin model for H(Γ). That model,
restricted to L, is a model of Γ. ■
Corollary 178 (Completeness Theorem) For every first-order theory Γ and a formula
A, if Γ |= A then Γ ⊢D A.
Proof. If Γ |= A then Γ ∪{¬A} is unsatisfiable; by the model existence theorem, Γ ∪
{¬A} is then D-inconsistent. Therefore, Γ ⊢D A by Proposition 155. ■
4.6.6
Semantic compactness of first-order logic
An immediate consequence of the completeness theorem is the compactness of the logical
consequence as follows.
Theorem 179 (Semantic Compactness Theorem 1) For any first-order theory Γ and a
formula A, if Γ |= A then Γ0 |= A for some finite Γ0 ⊆Γ.
Equivalently, we also have the following theorem.
Theorem 180 (Semantic Compactness Theorem 2) A first-order theory Γ is satisfiable
iff every finite subset of Γ is satisfiable.
This is a fundamental result about the semantics of first-order logic which can be proved
independently of the deductive completeness theorem. It has numerous important appli-
cations which we do not discuss here, but see the references.
Finally, we take note of another very important result on first-order logic, known as
Herbrand’s Theorem. First, it follows from the proof of the completeness theorem for
RES that, if any interpretation satisfies a given set of clauses S, then there is a Herbrand
interpretation that satisfies them. Herbrand’s Theorem states that, moreover, if a set of
clauses S is unsatisfiable then there is a finite set of ground instances of these clauses,
instantiated with elements from the Herbrand universe, which is unsatisfiable. Since such
a set is finite and consists of ground clauses, where no substitutions are possible, its unsat-
isfiability can be verified in a finite number of steps. This sounds like a way to check
any set of first-order clauses for satisfiability; isn’t this in a contradiction with Church’s

218
Logic as a Tool
Undecidability Theorem? Alas, no. The catch is that there may be an infinite number of
such candidate sets of ground instances to check.
References for further reading
For more detailed proofs of soundness and completeness of deductive systems for
first-order logic, see: van Dalen (1983) and Chiswell and Hodges (2007) for a com-
pleteness proofs for ND; Shoenfield (1967), Hamilton (1988), Mendelson (1997), and
Enderton (2001) for completeness of H; Nerode and Shore (1993), Fitting (1996), and
Ben-Ari (2012) for completeness of ST and RES; Smullyan (1995) and Smith (2003)
for completeness of ST; Ebbinghaus et al. (1996) and Hedman (2004) for completeness
of RES; and Boolos et al. (2007) for completeness of the Sequent calculus presented
there.
For more on Herbrand’s Theorem see Shoenfield (1967), Nerode and Shore (1993), and
Fitting (1996). For expositions and discussions of Gödel’s Incompleteness Theorems, see
Jeffrey (1994), Ebbinghaus et al. (1996), Enderton (2001), Hedman (2004), and Boolos
et al. (2007).
Exercises
In the following exercises, FO stands for first-order logic, H, ND, ST, and RES refer
to the respective deductive systems for FO introduced here, and D refers to any of these
deductive systems. All references to definitions and results in Section 2.7 now refer to the
respective definitions and results for FO.
4.6.1
Prove Proposition 60 for FO.
4.6.2
Prove Proposition 62 for:
(a) H;
(b) ST;
(c) ND;
(d) RES.
4.6.3
Prove Proposition 63 for:
(a) H;
(b) ST;
(c) ND;
(d) RES.
4.6.4
Prove Proposition 68 generically for each deductive system D.
4.6.5
Prove Proposition 70 generically for each deductive system D.
4.6.6
Prove Theorem 157 for:
(a) H;
(b) ST;
(c) ND;
(d) RES.
4.6.7
Prove Theorem 158 for:
(a) H;
(b) ST;
(c) ND;
(d) RES.
4.6.8
Prove Lemma 164.

Deductive Reasoning in First-order Logic
219
4.6.9
Prove Lemma 166 for:
(a) H;
(b) ST;
(c) ND;
(d) RES.
4.6.10
Prove Lemma 168 generically for any deductive system D.
4.6.11
Prove Lemma 170.
4.6.12
Complete the proof of Lemma 172 for:
(a) H;
(b) ST;
(c) ND;
(d) RES.
4.6.13
Prove Lemma 174 for:
(a) H;
(b) ST;
(c) ND;
(d) RES.
(Hint for ND: suppose Γ ⊢D ⊥in L+ by a derivation Ξ. Let Ξ be the result of
replacing each free occurrence of a new constant ci in Ξ by a new variable xi not
occurring in Ξ. It can be shown, by inspection of the rules of D, that Ξ is a valid
derivation of Γ ⊢D ⊥in L.)
4.6.14
Prove Lemma 175 for:
(a) H;
(b) ST;
(c) ND;
(d) RES.
4.6.15
Prove that the deductive compactness property stated in Lemma 175 is equivalent
to the following: a first-order theory Γ is D-consistent iff every finite subset of Γ
is D-consistent.
4.6.16
Complete the generic proof details of Theorem 176 for any deductive system D.
4.6.17
Assuming soundness and completeness of H, prove soundness and completeness
of each of ST, ND, and RES by using a first-order analog of Proposition 76.
4.6.18
Assuming soundness and completeness of ST, prove soundness and complete-
ness of each of H, ND, and RES by using a first-order analog of Proposition 76.
4.6.19
Assuming soundness and completeness of ND, prove soundness and complete-
ness of each of ST, H, and RES by using a first-order analog of Proposition 76.
4.6.20
Assuming soundness and completeness of RES, prove soundness and complete-
ness of each of H, ST, and ND by using a first-order analog of Proposition 76.
4.6.21
Prove the equivalence of the semantic compactness theorems 179 and 180.

220
Logic as a Tool
Kurt Friedrich Gödel (28.4.1906–14.1.1978) was an
Austrian–American logician, mathematician, and philoso-
pher, regarded as the most influential logician of the 20th
century.
Gödel’s first interest in logic was sparked when he
attended a seminar on Russell’s book Introduction to
Mathematical Philosophy. Later he attended a lecture
by David Hilbert on completeness and consistency of
mathematical systems, where Hilbert posed the question
of whether there is a consistent formal system of axioms
of first-order logic which can derive every valid – that is,
true in all models – statement of first-order logic. Gödel chose this problem as the
topic of his doctoral work and he completed his doctoral dissertation under the
supervision of Hans Hahn in 1929, at the age of 23. In his thesis he proposed an
axiomatic system for the first-order predicate logic and established its completeness.
This was his first famous result, known as Gödel’s Completeness Theorem.
In 1931 Gödel published his most important and groundbreaking work, On For-
mally Undecidable Propositions of “Principia Mathematica” and Related Systems,
where he proved his famous incompleteness theorems.
Gödel’s First Incompleteness Theorem states that any axiomatic system of
first-order logic which is consistent (i.e., no contradiction can be derived in it),
has an effectively recognizable (recursive) set of axioms, and is expressive enough
to describe the arithmetic of the natural numbers with addition and multiplication
must be incomplete; that is, there are true arithmetical statements that can be
stated in the language of that system but cannot be derived from its axioms. In
particular, this result applies to the Peano system of axioms of the arithmetic and
to Zermelo–Fraenkel set theory. Gödel’s Second Incompleteness Theorem states
that no such consistent system can derive a statement, formalized in the language of
arithmetic, claiming its consistency.
Gödel’s basic idea of the proof of the incompleteness theorems was conceptu-
ally simple but extremely original. It involved using, for any given formal axiomatic
system of the arithmetic with an effectively enumerable set of axioms, a specially
developed technique of encoding of the notions of formulae, axioms, and derivations
in the language of the arithmetic, to eventually construct a formula of that language
that claims that it is unprovable in the given formal system. Such a formula cannot
be provable in the system, for that would imply its inconsistency, and therefore what
it states must be true.
Gödel’s incompleteness theorems had a shattering effect on the attempts to find an
effectively enumerable set of axioms sufficient to derive all true statements in math-
ematics, beginning with the work of Frege half a century earlier, and culminating in
Principia Mathematica and Hilbert’s program for formalizing of the mathematics.
Gödel’s theorems also had a profound effect on mathematical logic and
the foundations and philosophy of mathematics. They are one of the greatest
achievements of 20th century mathematics; they are of fundamental importance
not only for the foundations of mathematics but also for computer science, as they
imply that a computer can never be programmed to derive all true mathematical
statements, not even all those about the arithmetic of natural numbers.

Deductive Reasoning in First-order Logic
221
Leon Albert Henkin (19.04.1921–1.11.2006) was an Ameri-
can logician and mathematician, best known for the Henkin
Completeness Proof for axiomatic deductive systems of
first-order logic. This proof is much simpler and intuitive than
the original Gödel proofs, and has become the standard method
for semantic completeness proofs since then.
After studying mathematics and philosophy at Columbia Col-
lege, Henkin was a doctoral student of Alonzo Church at Prince-
ton University, where he received his PhD in 1947. He then
became Professor of Mathematics at the University of Califor-
nia, Berkeley in 1953 where he worked until his retirement in 1991. He was a col-
laborator of Alfred Tarski and a keen popularizer of logic and of mathematics.
In his doctoral dissertation The completeness of formal systems, Henkin origi-
nally proved the completeness of Church’s higher-order logic using the now-called
general Henkin semantics, where the higher types are interpreted not by the full
space of functions but by a suitably rich subset of it. He then adapted his method
to obtain a new completeness proof for first-order logic, introducing the now-called
Henkin constants and Henkin models. His proof method is now used for complete-
ness proofs of a wide variety of logical systems. Henkin also studied extensions of
first-order logic with branching quantifiers and with infinite strings of quantifiers
and proposed game-theoretic semantics for them.
Henkin was not only a distinguished academic and promoter of logic and math-
ematics, but also a passionate social activist, instrumental in designing special pro-
grams and scholarships for talented students from disadvantaged minorities.

5
Applications: Mathematical Proofs
and Automated Reasoning
Logical systems are used in practice to perform formal reasoning that goes beyond pure
logic. Most often they are used for mathematical reasoning, but they have also found
numerous other applications in artificial intelligence, especially the field of knowledge
representation and reasoning (e.g. various description logics for ontologies), as well as in
many areas of computer science including database theory, program analysis, and deduc-
tive verification. In each of these areas the use of deductive systems is guided by the
specific applications in mind, but the underlying methodology is the same: the purely log-
ical engine of the deductive system, which consists of the general, logical axioms and
inference rules is extended with specific, non-logical axioms and rules describing the
particular subject area. The logical and non-logical axioms and rules together constitute a
formal theory in which the formal reasoning is performed by means of derivations from a
set of assumptions in the chosen deductive system. These assumptions are usually the rel-
evant non-logical axioms, plus other specific ad hoc assumptions applying to the concrete
domain or situation for which the reasoning is conducted.
Most typical mathematical theories describe classes of important relational structures
such as sets, partial or linear orders, directed graphs, trees, equivalence relations, var-
ious geometric structures, etc., algebraic structures, such as lattices, Boolean algebras,
groups, rings, fields, etc., or combined, for example ordered rings and fields. Other impor-
tant mathematical theories are intended to describe single structures of special interest, for
instance: an axiomatic theory for the arithmetic of natural numbers, the most popular one
being Peano Arithmetic; the subsystem of Presburger Arithmetic (involving only addi-
tion); or the first-order theory of the field of reals R or of the field of rational numbers
Q, etc.
In this chapter I first discuss generally the logical structure, strategies and tactics of
mathematical reasoning and proofs and then illustrate these with several examples in
Logic as a Tool: A Guide to Formal Logical Reasoning, First Edition. Valentin Goranko.
© 2016 John Wiley & Sons, Ltd. Published 2016 by John Wiley & Sons, Ltd.

Applications: Mathematical Proofs and Automated Reasoning
223
Section 5.1. I then present and briefly discuss some of the basic theories mentioned above
in Section 5.2 where I give just a minimum background on sets, functions, and relations
needed for performing meaningful mathematical reasoning in them. The actual proofs will
be left as exercises for the reader however, the main purpose of the chapter. Section 5.3.1
is supplementary, intended to provide a basic background on Mathematical Induction to
the reader who needs it. Section 5.3.2 focuses on deductive reasoning in the axiomatic
system of Peano Arithmetic, again mainly by means of exercises.
Logical deductive reasoning is carried out not only manually but also – with increasing
popularity and success – using computers. That use has lead to the active development of
automated reasoning, including automated and interactive theorem proving. On the other
hand, logic has made a strong methodological contribution to the theory and practice of
programming and computing by suggesting the paradigm of logic programming, realized
in several programming languages (the most popular of these being Prolog). I discuss
these topics very briefly in Section 5.4.
5.1
Logical reasoning and mathematical proofs
In mathematics the truth of a statement is established by proving it. A proof may consist
of a simple argument or of much complicated calculations, but essentially no logical rea-
soning. Alternatively, it may consist of a long and intricate argument involving a number
of other already-proven statements, conjectures, or logical inferences.
This section is about the logical aspects and structure of mathematical reasoning and
proofs. I discuss strategies and tactics for proofs, essentially based on the rules of Natural
Deduction, and illustrate them with a few examples. I end this section with some brief
remarks on Resolution-based automated reasoning.
5.1.1
Proof strategies: direct and indirect proofs
A typical mathematical statement is of the form:
if P1, . . . , Pn then C
where P1, . . . , Pn (if any) are premises or assumptionsand C is a conclusion.
While mathematical arguments are very specific to the subject area and the concrete
statement, the structure of the logical arguments in a proof only depend on the adopted
logical strategy of proof and the logical forms of the premises and the conclusion. Here
I provide some proof tactics, describing how to go about specific, local steps of the
proof, but I first discuss possible proof strategies, describing how proofs are organized
globally.

224
Logic as a Tool
I. Direct proofs With this proof strategy we assume that all premises are true, and then
try to deduce the desired conclusion by applying a sequence of correct – logical or
substantially mathematical
– inference steps. Schematically, a direct proof takes the
following form.
Assume P1, . . . , Pn.
...
(a sequence of valid inferences)
...
Conclude C.
Typical patterns of direct proofs are derivations in Natural Deduction without using the
rule of Reductio ad Absurdum.
Example 181 As an example, we provide a direct proof of the statement
if n is an odd integer, then n2 is an odd integer.
Proof.
1. Suppose n is an odd integer.
2. Then there is an integer k such that n = 2k + 1.
3. Therefore, n2 = (2k + 1)(2k + 1) = 4k2 + 4k + 1 = 2(2k2 + 2k) + 1.
4. Therefore, n2 is odd.
5. That proves the statement.
In this proof only the first and last steps are logical, roughly corresponding to the deriva-
tion of A →B by assuming A and deducing B.
II. Indirect proofs Indirect proofs are also known as proofs by assumption of the con-
trary or proofs by contradiction. Typical patterns of indirect proofs are derivations in
Semantic Tableaux, and also those derivations in Natural Deduction that use an applica-
tion of the rule of Reductio ad Absurdum. The idea of the indirect proof strategy is to
assume that all premises are true while the conclusion is false (i.e., the negation of the
conclusion is true), and try to reach a contradiction based on these assumptions, again
by applying only valid inferences. A contradiction is typically obtained by deducing a
statement known to be false (e.g., deducing that 1 + 1 = 3) or by deducing a statement
and its negation. We can often reach a contradiction by deducing the negation of some of
the premises (see the example below) which have been assumed to be true. The rationale
behind the proof by contradiction is clear: if all our assumptions are true and we only
apply valid inferences, then all our conclusions must also be true. By deducing a false
conclusion we therefore show that, given that all original assumptions are true, the addi-
tional one that we have made (i.e., the negation of the conclusion) must have been wrong,
that is, that the conclusion must be true.

Applications: Mathematical Proofs and Automated Reasoning
225
The scheme of a proof by contradiction is of the following form.
Assume P1, . . . , Pn.
Assume ¬C
...
(a sequence of valid inferences)
...
Derive both A and ¬A
for some proposition A.
Conclude that ¬C cannot be true.
We therefore conclude C.
Example 182 Here is an example of a proof by contradiction of the statement:
If x and y are integers, x is odd and xy is even, then y is even.
Proof.
1. Suppose x, y are integers, x is odd, and xy is even.
2. Suppose also that y is not even.
3. Therefore, y is odd.
4. Then there is an integer k such that x = 2k + 1, since x is odd.
5. There is also an integer m such that y = 2m + 1, since y is odd.
6. Therefore, xy = (2k + 1)(2m + 1) = . . . = 2(2km + k + m) + 1.
7. Therefore, xy is odd.
8. On the other hand, we have assumed that xy is even and, therefore, xy is not odd.
9. xy is therefore odd and xy is not odd.
10. This is a contradiction, which completes the proof.
Example 183 Euclid’s proof by contradiction that
√
2 is irrational.
Proof.
1. Suppose it is not the case that
√
2 is irrational.
2. Therefore,
√
2 is rational.
3.
√
2 can therefore be represented as an irreducible fraction, that is, there are relatively
prime integers m and n such that
√
2 = m
n .
4. Therefore, 2 = m2
n2 , hence 2n2 = m2.

226
Logic as a Tool
5. m2 is therefore even.
6. m must therefore be even since, if it is odd, then m2 would be odd (see the example
of a direct proof).
7. There is therefore an integer a such that m = 2a.
8. Hence, m2 = 4a2.
9. Therefore 2n2 = 4a2, hence n2 = 2a2.
10. n2 is therefore even.
11. n must therefore be even, by the same argument as above.
12. There is therefore an integer b such that n = 2b.
13. The fraction m
n = 2a
2b is therefore not irreducible.
14. This contradicts the assumption that m
n is irreducible.
15. This contradiction completes the proof.
A variation of proof by contradiction is proof by contraposition. This strategy is usu-
ally applied when the statement to be proved is of the type “if P, then C,” that is, there
is only one premise1. This proof strategy is based on the fact that the implication “if P,
then C” is logically equivalent to its contrapositive “if ¬C, then ¬P.” If we prove the
latter, we therefore have a proof of the former. Schematically, the proof by contraposition
of “if P, then C” has the following form.
Assume ¬C.
...
(a sequence of valid inferences)
...
Derive ¬P.
We can also think of this strategy as obtained from the proof by contradiction strategy,
by assuming P together with ¬C at the beginning and then proving ¬P to produce the
desired contradiction.
We usually apply the proof by contraposition when it is easier to prove the contraposi-
tive of the original implication.
Example 184 Here is a proof by contraposition of the statement
if n is an integer such that n2 is even, then n is even.
Proof.
1. Suppose n is not even.
2. Therefore, n is odd.
3. Then (see the example of a direct proof) n2 is odd.
4. Therefore, n2 is not even.
5. This completes the proof.
1 It can in fact be applied in the general case, as we can always replace all premises by their conjunction.

Applications: Mathematical Proofs and Automated Reasoning
227
Note that while Semantic Tableaux and Resolution are very convenient and concep-
tually easy-to-use systems of deduction, they are only designed for proofs by contradic-
tion and not for direct proofs. However, reasoning by contradiction is often unnecessary
and sometimes unnatural. Proof by contradiction requires that we have a specified goal
(conclusion) to prove, and is of no use if we simply want to derive logical consequences
from the knowledge that we have and the assumptions that we may have made. In such
cases, direct reasoning has no good alternative.
5.1.2
Tactics for logical reasoning
Here we discuss tactics of proof that are applicable to any of the methods discussed above.
These tactics can be extracted from the rules of Natural Deduction. The choice of tactics
depends both on the logical structure of the conclusion and on the logical structure of the
premises. Let us look at these separately.
Tactics based on the conclusion to be proved
A specific tactic can be used according to the logical form of the conclusion C to be
proved, as follows.
• If the conclusion is a negation, C = ¬A, then apply a proof by contradiction by assum-
ing A (which is equivalent to the negation of ¬A) and trying to reach a contradiction.
Alternatively, import the negation in ¬A inside A and try another tactic.
• If the conclusion is a conjunction, C = A ∧B, prove each of A and B.
• If the conclusion is a disjunction, C = A ∨B, try to prove one of A or B. This works
sometimes, but not always. If it does not work, then try a proof by contradiction, that
is, assume ¬C which is equivalent to ¬A ∧¬B, hence assume each of ¬A and ¬B,
and try to reach a contradiction.
The equivalence P ∨Q ≡¬P →Q can also be used to transform the disjunction into
implication; see below.
• If the conclusion is an implication, C = A →B, assume A in addition to all premises,
and try to prove B.
Alternatively, attempt a proof by contraposition by tackling a conclusion ¬B →¬A
instead.
Lastly, try a proof by contradiction by assuming ¬(A →B) or, equivalently, A and
¬B.
• If the conclusion is a biconditional, C = A ↔B, then it is equivalent to the conjunc-
tion (A →B) ∧(B →A) and we follow the tactics mentioned above.
• If the conclusion is a universally quantified statement, C = ∀xA(x), typically rea-
son by saying “Let c be a (name of an) arbitrary object from the domain” (e.g., an
arbitrary real number). Then you try to prove that A(c) holds with a general argu-
ment, without assuming any specific properties of c. In particular, the name c must be
a new one which has not yet been mentioned in the proof for the argument to be really
general.
If you have proved that A(c) holds independently of c, then that proof will apply to any
object c from the domain. That will produce a proof that A(x) holds for every object
x, that is, a proof of ∀xA(x).

228
Logic as a Tool
• If the conclusion is an existentially quantified statement, C = ∃xA(x), then either
try to find an explicit witness c (which can be any concrete object in the domain or
just a term in the language) such that A(c) holds, or attempt a proof by contradiction,
that is, a proof that it cannot be the case that there does not exist x for which
A(x) holds. The latter is called a non-constructive proof of existence, common in
mathematics.
Tactics based on the premises
A specific tactic can be used for each premise P, according to its logical form as follows.
• If the premise is a negation, P = ¬A, then import that negation inside all other logical
connectives in A, if any. Alternatively, try to use it for deriving a contradiction if A is
assumed or can be derived.
• If the premise is a conjunction, P = A ∧B, it can be replaced it by two new premises,
namely A and B.
• If the premise is a disjunction, P = A ∨B, and it is not known which of A and B is
true, then reason per case, that is, consider separately each of the two possible cases:
Case 1: A is true; and Case 2: B is true. If it can be proven that the conclusion C follows
in each of these cases, then this provides a proof that C follows from A ∨B. (Why?)
• If the premise is an implication, P = A →B, assume or try to derive A and then
conclude B and use it further.
Alternatively, replace A →B by the equivalent disjunction ¬A ∨B and reason as in
the previous case.
• If the premise is a biconditional, P = A ↔B, replace it by the pair of premises (A →
B) and (B →A).
• If the premise is a universally quantified statement, P = ∀xA(x), then you can
obtain from it new premises (possibly infinitely many) A(c) for each object c from the
domain. More generally, deduce A(t) for any term that is free for substitution for x in
A(x) and use these further in your reasoning.
• If the premise is an existentially quantified statement, P = ∃xA(x), then introduce
a new name, say c, for an object that satisfies A (“if there is an object x such that A(x),
then let us take one and give it a name, say c”). Then replace the premise by A(c).
Note that the name c must be a new one that is not mentioned in the proof or target
conclusion, and no assumptions about c can be made other than A(c).
References for further reading
For further discussions and detailed illustrations with many examples of how to perform
logically correct and well-structured mathematical reasoning and proofs, see Kalish and
Montague (1980, including many formal proofs), Sollow (1990, including many solu-
tions), Garnier and Taylor (1996), Barwise and Echemendy (1999), Devlin (2004, 2012),
Nederpelt and Kamareddine (2004), Bornat (2005), Velleman (2006), Makinson (2008),
and Day (2012).

Applications: Mathematical Proofs and Automated Reasoning
229
Exercises
5.1.1
For each of the examples of proofs given in this section determine which steps in
the proof above are based on logical inferences, and which are based on substantial
mathematical arguments.
5.1.2
Prove each of the following statements. Clearly indicate the method of proof and
explain the steps. Distinguish the logical from the mathematical steps.
(a) If the sum of two integers is even, then so is their difference.
(b) If a, b, x are real numbers, a < b, and (x −a)(x −b) < 0, then a < x < b.
(c) If a and b are rational numbers and a < b, then there exists a rational number
r such that a < r < b.
(d) For every integers m, n, if mn is even, then m is even or n is even.
(e) For every natural number n the number n2 + n is even.
(f) For every two integers a, b, if ab is odd then a + b is even.
(g) The number
3√
3 is irrational.
(h) No positive integers m and n exist such that 1
m + 1
n > 2.
(i) If x + y is an irrational number, then x is an irrational number or y is an
irrational number.
Charles Sanders Peirce (10.09.1839–19.04.1914) was an
American universal scientist, philosopher, logician, and math-
ematician. He made pioneering scientific contributions to an
extremely wide range of disciplines including logic, mathemat-
ics, statistics, philosophy, metrology, chemistry, experimen-
tal psychology, economics, linguistics, scientific methodology,
and the history of science. In particular, he is regarded as a
founding father of pragmatism, a philosophical theory and
method of scientific research, of modern statistics, and of semiotics, the study of
the meaning of symbols.
While he trained and worked as a chemist and geodesist, Peirce regarded himself
mainly as a logician, philosopher, and mathematician. When Alfred Tarski and his
students were developing their theory of relation algebras in the 1940s, they redis-
covered many ideas of Peirce’s pioneering work on the algebraic and logical study of
relations, starting with his 1870 paper Logic of Relatives. Moreover, Peirce’s system
of relational logic laid the foundations for the relational model of databases devel-
oped by Edgar F. Codd (a doctoral student of a student of Peirce’s) for which Codd
received a Turing Award in 1981. Peirce developed further Boole’s work on alge-
braic logic and showed, in particular, that a single binary logical operator (logical
NOR, now called Peirce’s arrow) is sufficient to express all other Boolean connec-
tives. He also introduced first-order and second-order quantification independently
of Frege and Peano.
Peirce also made contributions to cardinal arithmetic for infinite numbers,
years before Cantor’s first attempts, as well as to the theory of infinitesimals and

230
Logic as a Tool
mathematics of the continuum. He also conducted important work in several other
branches of pure and applied mathematics and statistics. Notably, as early as in
1886 – more than 50 years before the first digital computers were constructed – he
showed how Boolean logical operations could be carried out by electrical switching
circuits.
Peirce’s personal life and professional career were quite troubled. From early age
he suffered from a nervous condition now called “facial neuralgia” which made him
appear unfriendly and also caused sudden outbursts of bad temper. Together with
some other features of his character, this contributed to the fact that he never held
a stable employment; the only academic job he had was a non-tenure position as
lecturer in logic at the Johns Hopkins University. However, he was fired from this
post in 1884 after a scandal following the discovery that, while still legally married
pending divorce, he had an affair with another woman (of unknown origin) before
they married later and lived together to his end. Peirce spent the last 20–30 years of
his life in utmost poverty and constant debt, and died destitute.
While Peirce was largely unrecognized and rejected during his life, gradually he
received the recognition he deserved after his death. Russell and Whitehead were
not aware of his work when they published Principia Mathematica in 1910 and did
not mention him there, but in 1959 Russell wrote “Beyond doubt [ . . . ] he was one
of the most original minds of the later nineteenth century, and certainly the greatest
American thinker ever.” Much earlier in 1918, the logician C. I. Lewis wrote “The
contributions of C.S. Peirce to symbolic logic are more numerous and varied than
those of any other writer – at least in the 19th century.” In 1934, the philosopher Paul
Weiss called Peirce “the most original and versatile of American philosophers and
America’s greatest logician” and Karl Popper considered him “one of the greatest
philosophers of all times.”
Clarence Irving Lewis (12.04.1883–3.02.1964) was an Ameri-
can logician, epistemologist, and moral philosopher, one of the
founders of modern philosophical logic. Lewis is best known
in logic for his work exposing and criticizing the paradoxical
features of the truth functional, material implication, as used in
Russell and Whitehead’s Principia Mathematica, which declares,
for instance, that any true consequent follows from any false
antecedent. Lewis proposed to replace that material implication
with a strict implication which avoids that. Lewis’ strict implication was not primi-
tive, but defined as in terms of negation, conjunction, and a prefixed unary intensional
modal operator ♦, where ♦A reads “A is possibly true.” Lewis then defined “A
strictly implies B” as ¬♦(A ∧¬B). Lewis later devised the formal systems S1 to S5
of modal logic, adopting different principles for the modal operator ♦and its dual □
(where □A read as “A is necessarily true”), included in his 1932 book with Langford
Symbolic Logic, thus laying the foundations of modern, formal modal logic.

Applications: Mathematical Proofs and Automated Reasoning
231
5.2
Logical reasoning on sets, functions, and relations
The purpose of this section is not to teach the basics of sets, functions, and relations.
Here I assume that the reader already has some basic background on these; if necessary,
consult the references at the end of the section. In this section I summarize the relevant
basic concepts and list a number of simple properties to be proved as exercises on basic
logical reasoning in mathematical proofs.
5.2.1
Zermelo–Fraenkel axiomatic theory of sets
The concept of a set is fundamental in contemporary mathematics, but it only developed
as a precise mathematical concept in the mid-late 19th century through the work of sev-
eral mathematicians, most notably Georg Cantor. Soon afterwards, some paradoxes were
discovered as inevitably arising in the “naive” theory of sets, where any conceivable col-
lection of objects that can be described in our language was declared as a set. The most
famous of these is Russell’s paradox. Using that unrestricted possibility to define sets,
Russell proposed the definition of the set of all sets that are not elements of themselves.
Written in a set-builder notation, that set can be “defined” as follows:
R = {x | x /∈x}.
Now, any yes/no answer to the question of whether R is an element to itself leads to a
contradiction, thus creating the paradox. Note the role of self-reference here.
These paradoxes showed that a more conservative and systematic approach was
needed when building sets and their theory, and several axiomatic systems were proposed
to capture the notion of sets abstractly and consistently. The most popular of them is
Zermelo–Fraenkel axiomatic set theory (ZF). It is a formal logical theory in a very
simple first-order language for sets LZF, containing only equality plus just one additional
binary predicate symbol ∈of membership of an object to a set as its element. I will not
list all axioms of ZF here, as we will need only very few of them in what follows. The
most fundamental axiom that we use very often is the Extensionality axiom:
EXT: ∀x∀y(x = y ↔∀z(z ∈x ↔z ∈y)).
This essentially states that two sets are declared equal just in case they have the same
elements, so a set is determined completely by the collection of its elements.
We also say that a set A is included in a set B, denoted A ⊆B, just in case every
element of A is also an element of B. Formally, we can define the relation inclusion in
the language LZF as follows:
x ⊆y := ∀z(z ∈x →z ∈y).
This definition plus the axiom EXT and a basic logical inference provide a proof of the
following very simple claim, which I will present in a very detailed way as an illustration
of logic-based mathematical reasoning. The reader may find it too detailed, and I agree
that it is so, but my purpose here is not just to prove the claim, but also to explain explicitly
every single step, logical or mathematical, involved in the proof. You are advised to work
through a few more proofs on that level of detail until you feel sufficiently experienced to
skip or leave implicit most of these details.

232
Logic as a Tool
Proposition 185 For all sets A and B, A = B if and only if A ⊆B and B ⊆A.
Proof. We have to prove an equivalence: “ . . . if and only if . . . ” We do that by proving
the implications in both directions.
1. To prove the implication “If A = B, then A ⊆B and B ⊆A” we assume the
premise “A = B” and try to derive the conclusion “A ⊆B and B ⊆A.” This is a
conjunction, so we have to prove both “A ⊆B” and “B ⊆A.” To prove that A ⊆B, by
definition of set inclusion we must show that every element of A is an element of B. Let
x be any (arbitrary) element of A. Then, because A = B, we also have x ∈B. Since
x ∈A was arbitrary, we conclude that A ⊆B.
Likewise, to show that B ⊆A, let x be any element of B. Because A = B, it follows
that x ∈A. Since x ∈B was arbitrary, we therefore conclude that B ⊆A.
2. To prove “If A ⊆B and B ⊆A, then A = B,” we assume the premise “A ⊆B and
B ⊆A” and try to deduce A = B. By the axiom EXT, we have to show that A and B have
exactly the same elements. Let x be an arbitrary element. If x ∈A, then since A ⊆B, we
have x ∈B. Similarly, if x ∈B, then since B ⊆A, we have x ∈A. We have therefore
shown that, for any x, it holds that x ∈A iff x ∈B, that is, that A and B have the same
elements, hence A = B, by EXT. ■
Note that the proof above is almost formalized in Natural Deduction. As an exercise,
formalize it completely in ND or in any deductive system of your choice for first-order
logic for LZF. This can be done in two ways: without explicitly using the symbol ⊆or
using ⊆and adding its defining equivalence as an assumption:
∀x∀y(x ⊆y ↔∀z(z ∈x →z ∈y)).
Most of the other axioms of ZF are just statements of existence of some special sets or
can be obtained by applying basic operations to already existing sets.
The empty set axiom states the existence of an “empty set”:
EMPTY: ∃x∀y(¬y ∈x).
As an exercise, using EXT prove that there is only one empty set, denoted ∅.
The pair set axiom states the existence of a set with elements being a given pair of
sets:
PAIR: ∀x∀y∃z∀u(u ∈z ↔u = x ∨u = y).
As an exercise, using EXT prove that for any sets x, y there is only one pair set for them,
denoted {x, y}.
The union set axiom states the existence of the union of any pair of sets:
UNION: ∀x∀y∃z∀u(u ∈z ↔u ∈x ∨u ∈y).
As an exercise, using EXT prove that for any sets x, y there is only one union set for them,
denoted x ∪y.
The powerset axiom states the existence of the set of all subsets of any given set:
POWER: ∀x∃y∀z(z ∈y ↔z ⊆x).

Applications: Mathematical Proofs and Automated Reasoning
233
As an exercise, using EXT prove that for any sets x there is only one powerset of x,
denoted P(x).
A natural operation on sets, definable in LZF, is the successor set operation which,
applied to any set x, produces the set
x′ := x ∪{x}.
The infinity axiom states the existence of a set containing the empty set and closed under
the successor set operation:
INF: ∃x(∅∈x ∧∀y(y ∈x →y′ ∈x)).
As an exercise, using the other axioms (some are yet to come), prove that x′ ̸= x for any
set x. It then follows that any set x satisfying the formula above must indeed be infinite.
“Infinite” here means that it is bijective (see Section 5.2.3) with a proper subset of itself,
in this case the subset obtained by removing ∅.
We next have the regularity axiom or the foundation axiom:
REG: ∀x(x ̸= ∅→∃y∀z(z ∈y →z /∈x))
which states that every non-empty set x has a disjoint element (i.e., an element having no
common elements with x), therefore preventing the existence of an infinite descending
chain of set memberships. In particular, this axiom forbids any set to be an element of
itself (show this as an exercise).
The next axiom of ZF is actually a scheme, called the Axiom Scheme of Separation
or Axiom Scheme of Restricted Comprehension: for any formula φ(¯x, u) from
LZF, where ¯x is a tuple of free variables x1, . . . , xn (to be treated as parameters),
it states:
SEP: ∀y∀x1 . . . ∀xn∃z∀u(u ∈z ↔u ∈y ∧φ(¯x, u)).
Intuitively, this axiom states that, given any set y, for any fixed values of the parameters
¯x there exists a set consisting of exactly those elements of u that satisfy the property of
sets defined by the formula φ(¯x, u).
The last axiom in ZF is again a scheme, called the Axiom Scheme of Replacement:
for any formula φ(¯x, y, z) from LZF, where ¯x is a tuple of free variables x1, . . . , xn (to
be treated as parameters), it states:
REP:
∀x1 . . . ∀xn∀u(∀y(y ∈u →∃!zφ(¯x, y, z)) →
∃v∀z(z ∈v ↔∃y ∈uφ(¯x, y, z))).
Intuitively, this axiom states that if, for any fixed values of the parameters ¯x, the formula
φ(¯x, y, z) defines a functional relation fφ,¯x(y, z), then the image of any set u under that
relation is again a set (v).
Another important axiom was added to ZF later, namely the axiom of choice, stating
that for every set x of non-empty sets there exists a “set of representatives” of these sets,
which has exactly one element in common with every element of x:
AC: ∀x(∀y(y ∈x →y ̸= ∅) →∃z∀y(y ∈x →∃!u(u ∈y ∧y ∈z))).

234
Logic as a Tool
This axiom is sometimes stated only for sets x consisting of pairwise disjoint elements,
and then proved for all sets.
The axiomatic system ZF is a very rich theory within which most of the contemporary
mathematics can be built. ZF with the added axiom AC is denoted ZFC. While AC looks
very natural and almost obvious, it turns out to have some quite unintuitive, even paradox-
ical, consequences, so it has always been treated with special care. For more on ZF and
ZFC, see the references. I do not delve into the foundations of set theory here, but only
use that theory to illustrate logical reasoning and mathematical proofs.
5.2.2
Basic operations on sets and their properties
I hereafter denote some sets with capital letters A, B, C, . . . , X, Y, Z and elements of
those sets with lower-case letters a, b, c, . . . , x, y, z. This should hopefully be intuitive
and help to avoid confusion.
The existence of the empty set ∅and some of the basic operations on sets, namely
union ∪and powerset of a set P, are postulated with axioms of ZF. The other basic oper-
ations – intersection ∩, difference −, and Cartesian product × – can also be defined and
justified within ZF.
I list a number of properties of these operations, to be formalized in first-order logic and
proved as exercises. First, we make some important general remarks on defining objects
in ZF. In order to formally define an operation or relation on sets, we can write its defining
equivalence in LZF as we did for ⊆above. For instance, we can define the proper subset
relation ⊂as
∀x∀y(x ⊂y ↔x ⊆y ∧x ̸= y)
and the intersection ∩by:
∀x∀y∀z(z ∈(x ∩y) ↔(z ∈x ∧z ∈y)).
As an exercise, use the separation axiom SEP to prove the existence of x ∩y. For that,
write its definition in a set-builder notation and note that it can now be relativized to
already “existing” sets.
Likewise, the Cartesian product of two sets A, B can formally be defined in
set-builder notation as
A × B := {(a, b) | a ∈A, b ∈B}
where (a, b) is the ordered pair consisting of a as a first element and b as a second
element. A strange-looking but formally correct (see exercise) and commonly accepted
way to define an ordered pair as a set is
(a, b) := {{a}, {a, b}}.
As an exercise, using axioms of ZF show that the Cartesian product of any two sets exists.
In the next four propositions, note the close parallel between the properties of the logical
connectives ⊥, ∧, ∨, and −, where A −B := A ∧¬B, and of their set-theoretic coun-
terparts ∅, ∩, ∪, and −.

Applications: Mathematical Proofs and Automated Reasoning
235
Proposition 186 (Properties of the intersection) The following hold for any sets
A, B, C, and X:
(a) A ∩∅= ∅
(b) A ∩A = A
(c) A ∩B = B ∩A
(d) A ∩(B ∩C) = (A ∩B) ∩C
(e) A ⊆B if and only if A ∩B = A
(f) A ∩B ⊆A and A ∩B ⊆B
(g) If X ⊆A and X ⊆B, then
X ⊆A ∩B.
Proposition 187 (Properties of the union) The following hold for any sets A, B, C,
and X:
(a) A ∪∅= A
(b) A ∪A = A
(c) A ∪B = B ∪A
(d) A ∪(B ∪C) = (A ∪B) ∪C
(e) If A ⊆B if and only if A ∪B = B
(f) A ⊆A ∪B and B ⊆A ∪B
(g) If A ⊆X and B ⊆X, then
A ∪B ⊆X.
Proposition 188 (Properties of set theoretic difference) The following hold for any
sets A, B, and C:
(a) A −A = ∅
(b) ∅−A = ∅
(c) A −∅= A
(d) A −B ⊆A
(e) (A −B) ∩B = ∅
(f) A −B = ∅if and only if A ⊆B
(g) (A −B) ∩(B −A) = ∅
(h) (A −B) ∪B = A ∪B
(i) A −(A ∩B) = A −B
(j) A −(B ∩C) = (A −B) ∪(A −C)
(k) A −(B ∪C) = (A −B) ∩(A −C)
(l) A∪B =(A−B)∪(B−A)∪(A∩B).
Proposition 189 (Interaction between union and intersection) For all sets A, B, and
C the following hold:
(a) A ∩(B ∪C) = (A ∩B) ∪(A ∩C) (distributivity of ∩over ∪)
(b) A ∪(B ∩C) = (A ∪B) ∩(A ∪C) (distributivity of ∪over ∩)
(c) A ∩(A ∪B) = A (absorption)
(d) A ∪(A ∩B) = A (absorption).
Proposition 190 (Properties of the powerset) The following hold for any sets A, B, C:
(a) A ∈P(B) iff A ⊆B
(b) ∅∈P(A)
(c) A ∈P(A)
(d) If A ⊆B then P(A) ⊆P(B)
(e) If A ∈P(X) then X −A ∈P(X)
(f) If A ∈P(X) and B ∈P(X) then A ∩B, A ∪B, and A −B are in P(X)
(g) If X has n elements then P(X) has 2n elements.

236
Logic as a Tool
These properties of the powerset imply that, if U is a universal set in which all sets
of our interest are included, then ∪, ∩, and ′ are operations on P(U). (We write A′ for
U −A.) Thus, we obtain an algebraic structure ⟨P(U); ∪, ∩,′, ∅, U⟩called the powerset
Boolean algebra of U.
Proposition 191 (Properties of the Cartesian product) The following hold for any sets
A, B, C.
(a) A × (B ∪C) = (A × B) ∪(A × C), (B ∪C) × A = (B × A) ∪(C × A).
(b) A × (B ∩C) = (A × B) ∩(A × C), (B ∩C) × A = (B × A) ∩(C × A).
(c) A × (B −C) = (A × B) −(A × C).
5.2.3
Functions
First, we recall the basic terminology: a function (or mapping) from a set A to a set
B is a rule denoted f : A →B which assigns to each element a ∈A a unique element
f(a) ∈B. The element f(a) is called the value of a under f, or the image of a under
f. If f(a) = b then a is called a pre-image of b under f.
The set A is called the domain of f, denoted A = dom(f), andB is called the
co-domain, or the target set, of f, denoted B = cod(f).
The notion of image can be generalized from elements to subsets of the domain as
follows. For any subset X ⊆A of the domain of f, the image of X under f is the set
f[X] = {f(a) | a ∈X}.
The image of the whole domain A under f, that is, the set f[A] = {f(a) | a ∈A} of
all values of f, is called the range or image of f, also denoted rng(f).
Two functions f and g are equal iff dom(f) = dom(g), cod(f) = cod(g), and
f(a) = g(a) for every a ∈dom(f).
The graph of a function f is the set of ordered pairs {(a, f(a)) | a ∈dom(f)}.
A function f : A →B is:
• injective or into if for every a1, a2 ∈A, f(a1) = f(a2) implies a1 = a2;
• surjective or onto if rng(f) = B; or
• bijective or one-to-one if it is both injective and surjective.
If f is injective then the inverse of f is the function f −1 : rng(f) →dom(f), defined
by f −1(f(a)) = a for every a ∈dom(f).
Proposition 192 For every bijective function f:
1. f −1 is a bijection; and
2. (f −1)−1 = f.
If f : A →B and g : B →C then the composition of f and g is the mapping gf :
A →C defined by gf(a) = g(f(a)) for each a ∈A.

Applications: Mathematical Proofs and Automated Reasoning
237
Whenever we refer to a composition gf of two mappings f and g, we will assume that
rng(f) ⊆dom(g).
Proposition 193 Composition of mappings is associative, that is, f(gh) = (fg)h, when-
ever either of these is defined.
Proposition 194 Let f : A →B, g : B →C be any mappings. Then the following hold.
(a) If f and g are injective then gf is also injective.
(b) If f and g are surjective then gf is also surjective.
(c) In particular, the composition of bijections is a bijection.
(d) If gf is injective then f is injective.
(e) If gf is surjective then g is surjective.
Proposition 195 If f : A →B, g : B →C are bijective mappings then gf has an
inverse, such that (gf)−1 = f −1g−1.
Proposition 196
1. A mapping f is injective iff for every two mappings g1 and g2 with dom(g1) = dom(g2)
and cod(g1) = dom(f) = cod(g2), the following left cancellation property holds:
if fg1 = fg2 then g1 = g2.
2. A mapping f is surjective iff for every two mappings g1 and g2 with dom(g1) =
dom(g2) = cod(f), the following right cancellation property holds:
if g1f = g2f then g1 = g2.
5.2.4
Binary relations and operations on them
We have already discussed relations (predicates) of any number of arguments. Being sub-
sets of a given universal set, sets can be regarded as unary relations. Here we focus on
binary relations and operations on them.
First, let us summarize the basic terminology. Given sets A and B, a binary relation
between A and B is any subset of A × B. In particular, a binary relation on a set
A is any subset of A2 = A × A. Given a binary relation R ⊆A × B, if (a, b) ∈R, we
sometimes also write aRb and say that a is R-related to b.
Given sets A and B and a binary relation R ⊆A × B:
• the domain of R is the set dom(R) = {a ∈A | ∃b ∈B(aRb)}; and
• the range of R is the set rng(R) = {b ∈B | ∃a ∈A(aRb)}.
More generally, given subsets X ⊆A and Y ⊆B, we define
• the image of X under R: R[X] = {b ∈B | ∃x ∈X(xRb)}; and
• the inverse image of Y under R: R−1[Y ] = {a ∈A | ∃y ∈Y (aRy)}.
Notice that dom(R) = R−1[B] and rng(R) = R[A].

238
Logic as a Tool
Some special relations on any given set A include:
• the empty relation ∅;
• the equality(identity, diagonal) relation EA = {(a, a) | a ∈A}; and
• the universal relation A2.
We can also restrict the domain of a relation as for that of a function as follows.
If R is a binary relation on a set A and B ⊆A, then the restriction of R to B is the
relation R |B= R ∩(B × B) = {(x, y) | (x, y) ∈R and x, y ∈B}.
5.2.4.1
Operations on binary relations
Boolean operations
Since binary relations are sets themselves, the set operations ∪, ∩, and −also apply
to them. Besides, we define the complementation of a relation R ⊆A × B as
R′ = (A × B) −R.
Proposition 197 For any relations R, S ⊆A × B the following hold:
(a) dom(R ∪S) = dom(R) ∪dom(S);
(b) rng(R ∪S) = rng(R) ∪rng(S);
(c) dom(R ∩S) ⊆dom(R) ∩dom(S);
(d) rng(R ∩S) ⊆rng(R) ∩rng(S).
Inverse of a relation
Let R ⊆A × B. The inverse of R is the relation R−1 ⊆B × A defined as:
R−1 = {(b, a) | (a, b) ∈R}. Thus, aRb if and only if bR−1a.
Proposition 198 For any relations R, S ⊆A × B the following hold:
(a) dom(R−1) = rng(R);
(b) rng(R−1) = dom(R);
(c) (R−1)−1 = R;
(d) (R ∪S)−1 = R−1 ∪S−1;
(e) (R ∩S)−1 = R−1 ∩S−1;
(f) (R′)−1 = (R−1)′.
Composition of relations
Let R ⊆A × B and S ⊆B × C. The composition of R and S is the binary relation
R ◦S ⊆A × C defined:
R ◦S = {(a, c) | ∃b ∈B(aRb ∧bSc)}.
In particular, when R ⊆A2, R ◦R is defined and denoted R2.
Note that we can always form the composition of two relations, even if their domains
and ranges do not match, by suitably expanding these: if R ⊆A × B1 and S ⊆B2 × C,
then R ⊆A × (B1 ∪B2) and S ⊆(B1 ∪B2) × C.
Proposition 199 The composition of binary relations is associative, that is,
(R ◦S) ◦T = R ◦(S ◦T).
Note, however, that the composition of binary relations is not commutative.
Proposition 200 For any binary relations R and S: (R ◦S)−1 = S−1 ◦R−1.

Applications: Mathematical Proofs and Automated Reasoning
239
5.2.5
Special binary relations
Definition 201 A binary relation R ⊆X2 is called:
• reflexive if it satisfies ∀x(xRx);
• irreflexive if it satisfies ∀x¬(xRx), that is, if X2 −R is reflexive;
• serial if it satisfies ∀x∃y(xRy);
• functional if it satisfies ∀x∃!y(xRy), where ∃!y means “there exists a unique y;”
• symmetric if it satisfies ∀x∀y(xRy →yRx);
• asymmetric if it satisfies ∀x∀y(xRy →¬yRx);
• antisymmetric if it satisfies ∀x∀y(xRy ∧yRx →x = y);
• connected if it satisfies ∀x∀y(xRy ∨yRx ∨x = y);
• transitive if it satisfies ∀x∀y∀z((xRy ∧yRz) →xRz);
• an equivalence relation if it is reflexive, symmetric, and transitive;
• euclidean if it satisfies ∀x∀y∀z((xRy ∧xRz) →yRz);
• a pre-order (or quasi-order) if it is reflexive and transitive;
• a partial order if it is reflexive, transitive, and antisymmetric, that is, an antisymmetric
pre-order;
• a strict partial order if it is irreflexive and transitive;
• a linear order (or total order) if it is a connected partial order; or
• a strict linear order (or strict total order) if it is a connected strict partial order.
Proposition 202 For any set X and binary relation R ⊆X2:
(a) R is reflexive iff EX ⊆R;
(b) R is symmetric iff R−1 ⊆R iff R−1 = R;
(c) R is asymmetric iff R−1 ∩R = ∅;
(d) R is antisymmetric iff R−1 ∩R ⊆EX;
(e) R is connected iff R ∪R−1 ∪EX = X2;
(f) R is transitive iff R2 ⊆R.
Functions can be regarded as special type of relations by means of their graphs: the
graph of a function f : A →B can be defined as the binary relation Gf ⊆A × B where
Gf = {(a, f(a)) | a ∈A}. A relation R ⊆A × B is therefore functional iff it is the
graph of a function from A to B (exercise).
Let R ⊆X × X be a binary relation on a set X. Then
• the reflexive closure of R is the smallest by inclusion reflexive binary relation Rref on
X such that R ⊆Rref;
• the symmetric closure of R is the smallest by inclusion symmetric binary relation Rsym
on X such that R ⊆Rsym; and
• the transitive closure of R is the smallest by inclusion transitive binary relation Rtran
on X such that R ⊆Rtran.
As an exercise, show that each of these closures always exists.

240
Logic as a Tool
Proposition 203 Let R ⊆X × X. Then
(a) Rref =  {S | R ⊆S ⊆X2, and S is reflexive}
(b) Rsym =  {S | R ⊆S ⊆X2, and S is symmetric}
(c) Rtran =  {S | R ⊆S ⊆X2, and S is transitive}.
5.2.5.1
Equivalence relations, quotient-sets, and partitions
Recall that a binary relation R ⊆X × X on a set X is called an equivalence relation on
X if it is reflexive, symmetric, and transitive. If R ⊆X × X is an equivalence relation
on X and x ∈X, the subset [x]R = {y ∈X | xRy} of X is called the equivalence class
(or the cluster) of x generated by R.
Proposition 204 For every equivalence relation R on a set X and x, y ∈X the following
hold:
(a) x ∈[x]R
(b) x ∈[y]R implies [x]R = [y]R
(c) x /∈[y]R implies [x]R ∩[y]R = ∅
(d) if [x]R ̸= [y]R then [x]R ∩[y]R = ∅.
Let R ⊆X × X be an equivalence relation. The set X/R = {[x]R | x ∈X}, consist-
ing of all equivalence classes of elements of X under R, is called the quotient set of X
by R. The equivalence class [x]R, also denoted x/R, is the quotient-element of x by R.
The mapping ηR : X →XR defined by ηR(x) = [x]R is the canonical mapping of X
onto X/R.
A partition of a set X is any family P of non-empty and pairwise disjoint subsets of
X, the union of which is X.
Proposition 205 If R ⊆X2 is an equivalence relation then X/R is a partition of X.
Proposition 206 If P is a partition of X then the relation ∼P⊆X × X defined by x ∼P
y iff x and y belong to the same member of P is an equivalence relation on X.
Equivalence relations and partitions of a set are therefore two faces of the same coin.
5.2.6
Ordered sets
5.2.6.1
Pre-orders and partial orders
Recall that a binary relation R on a set X is called a pre-order if it is reflexive and
transitive. If R is also anti-symmetric, then it is called a partial order. If R is a partial
order on X, then the pair (X, R) is called a partially ordered set or a poset.
An arbitrary poset is typically denoted (X, ≤). Given a poset (X, ≤) the following
notations are standard: x ≥y means y ≤x; x < y means x ≤y and x ̸= y; and x > y
means y < x.

Applications: Mathematical Proofs and Automated Reasoning
241
Proposition 207 Let R be a pre-order on X. Then:
(a) The relation ∼on X defined by
x ∼y iff xRy and yRx
is an equivalence relation on X.
(b) The relation ˜R on X/∼, defined by
[x]∼˜R [y]∼iff xRy.
is a well-defined partial order on X/∼, called the partial order induced by R.
5.2.6.2
Lower and upper bounds; minimal and maximal elements
Let (X, ≤) be a poset and Y ⊆X. An element x ∈X is:
• a lower bound for Y in X if x ≤y for every y ∈Y ;
• the greatest lower bound, also called infimum, of Y in X if x is a lower bound for Y
in X and x′ ≤x for every lower bound x′ of Y in X;
• an upper bound for Y in X if x ≥y for every y ∈Y ; or
• the least upper bound, also called supremum, of Y in X if x is an upper bound for
Y in X and x ≤x′ for every upper bound x′ of Y in X.
Proposition 208 Let (X, ≤) be a poset and Y ⊆X.
(a) If Y has an infimum in X, then it is unique.
(b) If Y has a supremum in X, then it is unique.
Let (X, ≤) be a poset and Y ⊆X. An element x ∈Y is called:
• minimal in Y if there is no element of Y strictly less than x, that is, for every y ∈Y ,
if y ≤x then x = y; or
• maximal in Y if there is no element of Y strictly greater than x, that is, for every y ∈Y ,
if y ≥x then x = y.
Note that the least (respectively, greatest) element of Y , if it exists, is the only minimal
(respectively, maximal) element of Y .
5.2.6.3
Well-ordered sets
An ascending (respectively, strictly ascending) chain in a poset (X, ≤) is any finite or
infinite sequence x1, x2, x3, . . . of elements of X such that x1 ≤x2 ≤x3 ≤· · · (respec-
tively, x1 < x2 < · · ·). A descending (respectively, strictly descending) chain in a poset

242
Logic as a Tool
(X, ≤) is any finite or infinite sequence x1, x2, x3, . . . of elements of X such that x1 ≥
x2 ≥· · · (respectively, x1 > x2 > · · ·).
A poset is called well-founded if it contains no infinite strictly descending chains. For
instance, every finite poset is well-founded. On the other hand, the poset (P(X), ⊆), where
X is any infinite set, is not well-founded (exercise).
A well-founded linear order is called a well-order.
Proposition 209 The lexicographic order in N2 defined by
⟨x1, y1⟩≤⟨x2, y2⟩iff x1 < x2 or (x1 = x2 and y1 ≤y2)
is a well-order on N2.
Proposition 210 A poset (X, ≤) is well-founded if and only if every non-empty subset of
X has a minimal element.
In particular, a linear order (X, ≤) is a well-order if and only if every non-empty subset
of X has a least element.
We now generalize the principle of Mathematical Induction to induction on arbitrary
well-founded sets; see also Section 1.4.4.
Theorem
211 (Induction
principle
for
well-founded
sets) Let
(X, ≤)
be
a
well-founded set and P ⊆X be such that for every x ∈X, if all elements of X less than
x belong to P, then x itself belongs to P. Then P = X.
References for further reading
Most of the books listed in the previous section provide many concrete examples –
including many of the exercises listed further below – of mathematical reasoning and
proofs on sets, functions, and relations. See in particular: Sollow (1990), Garnier and
Taylor (1996), Devlin (2004, 2012), Nederpelt and Kamareddine (2004), Velleman
(2006), Makinson (2008), Day (2012), van Benthem et al. (2014), and Conradie and
Goranko (2015).
See also Bornat (2005), Makinson (2008), and Conradie and Goranko (2015) for
mathematical reasoning and proofs on some other topics arising in computer science
(loop invariants, arrays, trees, etc.), as well as in number theory, combinatorics,
probability, and graph theory. For a more involved treatment and discussion of axiomatic
set theory and ZF, see Shoenfield (1967), Hamilton (1988), Mendelson (1997), and
Enderton (2001).
Exercises
Do each of the following proofs either by formalizing the claims in LZF and using any
deductive system for first-order logic, or using semi-formal mathematical reasoning. In the
latter case, indicate the method of proof and explicate the logical structure of the argument.
Distinguish the logical steps from the mathematical steps.

Applications: Mathematical Proofs and Automated Reasoning
243
Exercises on sets and operations on sets
5.2.1
Using the axioms of ZF and logical reasoning, prove the following.
(a) There is only one empty set, that is, every two empty sets are equal.
(b) ∅⊆x for any set x.
(c) For any sets x, y there is only one pair set {x, y} for them.
(d) For any sets x, y there is only one union set x ∪y for them.
(e) For any sets x, y there exists exactly one intersection set x ∩y for them.
(f) For any sets x, y there exists exactly one Cartesian product x × y for them.
(g) For any set x there is only one powerset P(x).
(h) Prove that x′ ̸= x for any set x, where x′ := x ∪{x}. (Hint: use REG.)
(i) Prove that no set may be an element of itself.
5.2.2
Prove that there is a unique least “infinite” set, that is, a set ω which satisfies
the axiom INF when substituted for x and is included in all sets satisfying
INF.
Then prove also that ω is indeed infinite, in the sense that there is a bijection
between ω and its proper subset ω −{∅}.
5.2.3
Recall the definition of an ordered pair as a set: (a, b) := {{a}, {a, b}}.
Prove that it is formally correct, in a sense that for any objects (sets) a1, b1, a2, b2:
(a1, a2) = (b1, b2) iff (a1 = b1 ∧a2 = b2)
5.2.4
Formalize in LZF each of the properties of the set intersection listed in Proposition
186 and prove them.
5.2.5
Formalize in LZF each of the properties of the set union listed in Proposition 187
and prove them.
5.2.6
Formalize in LZF each of the properties of the set difference listed in Proposition
188 and prove them.
5.2.7
Formalize in LZF each of the properties of the interaction between union and
intersection listed in Proposition 189 and prove them.
5.2.8
Formalize in LZF each of the properties of the powerset listed in Proposition 190
and prove them.
5.2.9
Formalize in LZF each of the properties of the Cartesian product listed in Propo-
sition 191 and prove them.
Exercises on functions
For each of the following exercises consider a suitable first-order language extending LZF
with the functional symbols mentioned in the exercise. Then formalize the respective prop-
erty in that language and attempt to prove it using a deductive system of your choice.
Provide semi-formal mathematical proofs, as required, by indicating the method of proof

244
Logic as a Tool
and explicating the logical structure of the argument. Distinguish the logical steps from
the mathematical steps.
5.2.10
Prove Proposition 192.
5.2.11
Prove Proposition 193.
5.2.12
Prove Proposition 194.
5.2.13
Prove Proposition 195.
5.2.14∗Prove Proposition 196.
Exercises on binary relations
For each of the following exercises consider a suitable first-order language extending LZF
with the relational symbols mentioned in the exercise. Then formalize the respective prop-
erty in that language and attempt to prove it using a deductive system of your choice.
Provide semi-formal mathematical proofs, as required, by indicating the method of proof
and explicating the logical structure of the argument. Distinguish the logical from the
mathematical steps.
5.2.15
Prove Proposition 197.
5.2.16
Prove Proposition 198.
5.2.17
Prove Proposition 199.
5.2.18
Prove Proposition 200.
5.2.19
Prove Proposition 202.
5.2.20
Show that a relation R ⊆A × B is functional iff it is the graph of a function
from A to B.
5.2.21
Show that each of the reflexive, symmetric, and transitive closures of any given
binary relation exists.
5.2.22
Prove Proposition 203.
5.2.23∗Prove each of the following for any binary relations R, S, T ⊆X2.
(a) If R and S are transitive and R ◦S = S ◦R = T, then T is also transitive.
(b) If R and S are transitive, R is symmetric, and R ∪S = X2, then R = X2
or S = X2.
(c) If R or S is reflexive and transitive and R ◦S = EX, then R = S = EX.
5.2.24
Prove Proposition 204.
5.2.25
Prove Proposition 205.
5.2.26
Prove Proposition 206.
5.2.27
Prove Proposition 207.
5.2.28
Prove that R ⊆X2 is an equivalence relation iff R is reflexive and euclidean.
5.2.29∗Let E1 and E2 be equivalence relations on a set X. Then prove that:
(a)
E1 ∩E2 is an equivalence relation on X.
(b∗) The composition E1 ◦E2 is an equivalence relation if and only if the two
relations commute, that is, if and only if E1 ◦E2 = E2 ◦E1.
5.2.30
Prove that if (X, ≤) is a poset and Y ⊆X, then (Y, ≤|Y ) is also a poset.

Applications: Mathematical Proofs and Automated Reasoning
245
Exercises on ordered sets
5.2.31
Consider the poset (P(X), ⊆) where X is a non-empty set. Show that every
subset X ⊆P(X) has both a supremum and an infimum in (P(X), ⊆).
5.2.32
Let X be an infinite set. Prove that the poset (P(X), ⊆) is not well-founded.
5.2.33
Prove Proposition 208.
5.2.34
Prove Proposition 209
5.2.35
Prove Proposition 210.
5.2.36
Prove Proposition 211.
Friedrich Wilhelm Karl Ernst Schröder (25.11.1841–
16.06.1902) was a German mathematician mainly known for
his work on algebraic logic, regarded as one of the creators of
mathematical logic (a term he may have invented, according
to Wikipedia).
Schröder studied mathematics at Heidelberg, Königsberg,
and then Zürich. After teaching at school for a few years, he
had a position at the Technische Hochschule Darmstadt in
1874. He took up a Chair in Mathematics at the Polytechnische Schule in Karlsruhe
two years later, where he remained until the end of his life.
Schröder made important contributions to algebra, set theory, lattice theory,
ordered sets, and ordinal numbers. Together with Cantor, he discovered the now
called Cantor–Bernstein–Schröder Theorem, claiming that the cardinailities
of any two sets are comparable. However, he is best known for his monumental
three-volume Vorlesungen über die Algebra der Logik (Lectures on the Algebra
of Logic), published during 1890 –1905 at his own expense. In this collection, he
systematized and modernized the various systems of formal logic of his time, from
the works of Boole, Morgan, MacColl, and especially of Peirce (from whom he
took the idea of quantification). With this work he prepared to a great extent the
groundwork for the emergence of mathematical logic as a new mathematical disci-
pline in the late 19th–early 20th century. Schröder’s contribution in popularizing
Peirce’s work and his influence for the emergence of mathematical logic is regarded
as at least as strong as that of Frege and Peano.
Georg Cantor (19.02.1845–6.01.1918) was a German math-
ematician, famous for creating and developing the theory of
infinite sets.
Cantor was born in in St Petersburg in Russia in 1845
and, after some initial resistance from his father, went on to
study mathematics at the University of Zurich and then the
University of Berlin. He was taught by some of the leading
mathematicians of the time, including Kummer, Weierstrass,

246
Logic as a Tool
and Kronecker. Cantor’s first paper on infinite sets was published shortly before he
turned 30.
The notion of infinity has always been a tricky and elusive topic that very easily
leads to paradoxes such as those of Zeno of Elea (c. 490–430 BC). Because of that,
most mathematicians (including the genius Gauss) had previously preferred to work
only with “potential” infinities (e.g., for every natural number there is a greater one)
rather than actual “completed” infinities (e.g., the set of all natural numbers).
Cantor was the first to deal explicitly with actual infinite sets, perform operations
on them, and compare them; although regarded as a matter of course in mathe-
matics today, this was quite revolutionary then. It is therefore not surprising that
Cantor’s theory of infinite sets and transfinite numbers was originally regarded as
counter-intuitive, even unbelievable, such that it encountered strong resistance from
many authoritative mathematicians of the time such as Kronecker and Poincaré, and
later Weyl and Brouwer. For instance, Poincaré referred to his ideas as a “grave dis-
ease infecting the discipline of mathematics,” while Kronecker went even further
and personally attacked included Cantor, describing him as a “scientific charlatan”
and a “corrupter of youth.”
Some strong philosophical and theological objections against Cantor’s set theory
were also raised against it, by some influential theologians and philosophers. For
example, much later Wittgenstein still regarded it as “utter nonsense” and “laugh-
able” and complained about its poisonous effect on mathematics. Russell’s discovery
in 1901 of the paradoxical set of all sets that are not elements of themselves certainly
contributed to the strong suspicion and even plain rejection faced by Cantor’s set the-
ory for a long period. Possibly because of the harsh reaction of his contemporaries,
Cantor had recurring bouts of depression from 1884 until the end of his life in a
mental hospital.
The practical power of his theory was gradually recognized however, such that in
the early 1890s he was elected President of the German Mathematical Society. He
taught at the University of Halle in Germany from 1869 until his retirement in 1913.
In 1904, the British Royal Society awarded Cantor its Sylvester Medal, the highest
honor it confers for work in mathematics, while David Hilbert strongly defended
his theory by famously proclaiming: “No one shall expel us from the Paradise that
Cantor has created.”
5.3
Mathematical Induction and Peano Arithmetic
This section is devoted to two related important topics which are specific to the arithmetic
on natural numbers, where logical reasoning plays a crucial role.
The method of Mathematical Induction is a very important reasoning technique in
mathematics which cannot be extracted from the purely logical rules of Natural Deduction
for first-order logic. Even though it is not a rule of purely logical reasoning, Mathemati-
cal Induction is an indispensable reasoning tactic for proving universal statements about
natural numbers, so I present and explain it here in some detail.

Applications: Mathematical Proofs and Automated Reasoning
247
Probably the most popular formal mathematical theory is the first-order axiomatic
theory for addition and multiplication in the set of natural numbers, known as Peano
Arithmetic (PA) after the logician and mathematician Giuseppe Peano2 who was the
first to formalize and study it systematically. I present the axioms of PA, one of which is a
partial formalization of the principle of Mathematical Induction in the first-order language
of PA, list a number of basic arithmetical facts derivable in PA, and sketch semi-formal
proofs of a few of them (leaving the rest as exercises on axiom-based mathematical
reasoning).
5.3.1
Mathematical Induction
Mathematical Induction is a special case of the general Induction Principle for inductively
defined sets formulated in Section 1.4; I recommend that the reader (re)visits that section
to see the full generality and strength of the induction method. I present here its basic
version with a few variations and equivalent statements.
5.3.1.1
Mathematical Induction: basic version
Recall that we assume the set of natural numbers to be N = {0, 1, 2, . . . }. Here is the
most common version of the Principle of Mathematical Induction (PMI).
Theorem 212 (Principle of Mathematical Induction) Suppose that some property P
of natural numbers holds for 0, and whenever P holds for some natural number k then it
also holds for k + 1. Then P holds for every natural number.
We write P(n) to say that the property P holds for the number n. Using the PMI to
prove a statement of the type “P(n) holds for every natural number n” therefore requires
that the following two steps be proved:
1. Base step: P (0) holds; and
2. Induction step: for any k ∈N, if P(k) holds then P(k + 1) also holds.
We illustrate the use of the PMI with the following example.
Example 213 Prove that for every natural number n,
0 + 1 + . . . + n = n(n + 1)
2
.
Proof. Let us denote the above statement P(n). First, we have to verify the base step, that
is, show that P(0) is true:
0 = 0(0 + 1)
2
.
2 Read more on Peano in the biographic box at the end of this section.

248
Logic as a Tool
To verify the induction step, we then assume that P(k) is true for some k ≥0. This
assumption is called the Inductive Hypothesis (IH). We now have to show that P(k + 1)
is also true:
0 + 1 + . . . + k + (k + 1) = (0 + 1 + . . . + k) + (k + 1)
= k(k + 1)
2
+ (k + 1)
= k(k + 1) + 2(k + 1)
2
= (k + 1)((k + 1) + 1)
2
,
where we use the Inductive Hypothesis to justify the second equality.
By the Principle of Mathematical Induction, we therefore conclude that P(n) holds for
every natural number n. ■
Sometimes, we only need to prove that a property holds for all natural numbers greater
than some m ≥0. PMI works just as well by starting with a base step P(m) and applying
the induction step only to natural numbers k ≥m. Indeed, we can reduce this case to the
original by stating the property as an implication: n ≥m →P(n). As an exercise, show
that this reduction works as intended.
Example 214 Prove that n2 ≤2n for every natural number n ≥4.
Proof. Let P (n) be the statement n2 ≤2n.
Base step: Prove that P (4) is true: 42 = 16 ≤16 = 24.
Induction step: Assume that P(k) is true for some k ≥4, that is, k2 ≤2k. This is the
Inductive Hypothesis (IH).
We now have to prove that (k + 1)2 ≤2(k+1). We begin with:
(k + 1)2 = k2 + 2k + 1 = k

k + 2 + 1
k

.
Our goal is to rewrite this equation so that we can use the IH.
Note that 1
k < 1 since k ≥4. We therefore have:
k

k + 2 + 1
k

≤k(k + 2 + 1) = k(k + 3) ≤k(k + k) = 2k2.
We can now use the IH to obtain 2k2 ≤2.2k = 2(k+1).
We therefore have that (k + 1)2 ≤2(k+1), as required.
By the PMI, we conclude that n2 ≤2n for all natural numbers n ≥4. ■

Applications: Mathematical Proofs and Automated Reasoning
249
5.3.1.2
Equivalent versions of the principle of Mathematical Induction
Here I state two equivalent principles from which PMI can be deduced.
First, a fundamental property of the set of natural numbers, which we can take as an
axiom, is the following Principle of Well-Ordering (PWO):
Every non-empty set of natural numbers has a least element.
This is equivalent to the following Principle of Descending Chains (PDC):
There is no infinite strictly descending sequence of natural numbers.
Indeed, assume the PWO, then every strictly descending sequence of natural numbers,
considered as a set of its members, must have a least element; it therefore may not be
infinite. Conversely, assume the PDC and suppose PWO does not hold. Then take any
non-empty set X of natural numbers with no least element. Take any n0 ∈X. Since it is
not least in X, there must be n1 ∈X such that n1 < n0. Again, n1 is not least in X, so
there must be n2 ∈X such that n2 < n1, etc. We construct an infinite strictly descending
sequence of natural numbers, which contradicts the PDC.
Assuming any of the principles above, we can now prove the PMI.
Theorem 215 (Principle of Mathematical Induction) Suppose that some property P of
natural numbers holds for some natural number n0 ≥0, and whenever P holds for some
natural number k ≥n0 it also holds for k + 1. Then P holds for every natural number
greater than or equal to n0.
Proof. The PMI can be derived from the PWO by assumption of the contrary, as follows.
Suppose that there are natural numbers for which P does not hold. The collection of such
numbers has a least element m. It must be greater than n0 since P holds for n0. Therefore
m −1 is a natural number less than m, hence P must hold for m −1. But then P must
also hold for m, since m = (m −1) + 1. This is a contradiction, therefore our assumption
was wrong. ■
Here is a seemingly stronger but in fact equivalent form of the Principle of Mathematical
Induction.
Theorem 216 (Principle of Complete Mathematical Induction (PCMI)) Suppose a
property P of natural numbers holds for a natural number n0, and also suppose that, for
any natural number k ≥n0, if P holds for all natural numbers greater than or equal to
n0 and less than k, then P holds for k. Then P holds for every natural number greater
than or equal to n0.
Proof. Left as exercise (Exercise 2. in Section 5.3.3.1). ■
When using the PMCI we therefore need to prove the following.
1. P(n0) holds for some natural number n0.
2. For any k ≥n0, assuming that P(k) holds for all natural numbers greater than or equal
to n0 and less than k, then P(k) also holds.

250
Logic as a Tool
Note that in the statement of Theorem 216, the base step, that is, the assumption that
the property P holds for n0, can be omitted (see Exercise 3.).
5.3.2
Peano Arithmetic
The language of Peano Arithmetic is the already well-known first-order language LN
containing =, one constant symbol 0, one unary functional symbol s, and two binary
functional symbols, + and ×. Recall that numerals are the special ground terms of the
type n := s(· · · s
  
n times
(0) · · ·), respectively interpreted in the number n.
The axioms of Peano Arithmetic, where all free variables below are implicitly univer-
sally quantified, are:
(PA1) s(x) ̸= 0
(PA2) s(x) = sy →x = y
(PA3) x + 0 = x
(PA4) x + s(y) = s(x + y)
(PA5) x × 0 = 0
(PA6) x × s(y) = x × y + x
(PA7) Induction Scheme:
ϕ(0) ∧∀x(ϕ(x) →ϕ(s(x))) →∀xϕ(x)
for every formula ϕ(x) of LN.
This axiomatic system is denoted PA.
Note that the Induction Scheme formalizes the principle of Mathematical Induction,
but restricted only to properties (respectively, sets) of natural numbers definable in the
language LN . There are only countably many such definable properties/sets, since there
are only countably many formulae in the language LN. On the other hand, by Cantor’s
Theorem there are uncountably many sets of natural numbers, so all but countably
many of them are not definable in LN and the induction principle cannot be applied
to them.
Still, PA is a very expressive and powerful axiomatic system. In fact, by the famous
Gödel’s First Incompleteness Theorem, the set of its theorems is not axiomatizable “effec-
tively,” that is, by an algorithmically recognizable (recursive) set of axioms. Furthermore,
by Gödel’s Second Incompleteness Theorem, even though PA can express (in a sense)
its own consistency, it cannot prove it; this of course assumes it is consistent, otherwise
Ex Falso Quodlibet applies. These results can be regarded as a testimony of both the
expressive strength of PA and as an insurmountable limitation of axiomatic/deductive
reasoning.
Here are some definable relations and formulae in LN that will be used further:
1. x ≤y =def ∃z(x + z = y)
2. x < y =def x ≤y ∧x ̸= y
3. ∃yy≤zA(¯x, y) =def ∃y(y ≤z ∧A(¯x, y)), for any formula A(¯x, y)
4. ∀yy≤zA(¯x, y) =def ∀y(y ≤z →A(¯x, y)), for any formula A(¯x, y).

Applications: Mathematical Proofs and Automated Reasoning
251
We can also define the value of any ground term t by recursion on the inductive defi-
nition of ground terms:
1. v(0) = 0
2. v(s(t)) = v(t) + 1
3. v(t1 + t2) = v(t1) + v(t2), v(t1 × t2) = v(t1) × v(t2).
In the exercises I will list many simple but important claims about the arithmetic of N
which are derivable in PA.
References for further reading
Mathematical Induction is a very common topic, treated in just about any book on calculus
or discrete mathematics. For more detailed and logically enhanced treatment of induc-
tion see Sollow (1990), Nederpelt and Kamareddine (2004), Velleman (2006), Makinson
(2008), Conradie and Goranko (2015).
For in-depth treatment of Peano Arithmetic, see Shoenfield (1967), Mendelson (1997),
van Oosten (1999), Enderton (2001), Boolos et al. (2007), and Smith (2013).
Exercises
Exercises on Mathematical Induction
5.3.1
Using the basic Principle of Mathematical Induction (PMI) and applying (and
explicitly indicating) the reasoning tactics for the logical steps, prove that for every
natural number n the following holds.
1.1 n2 + n is even, for all natural numbers n.
1.2 If n ≥4 then 2n < n!.
1.3 If n ≥1 then the powerset of the set {1, 2, 3, . . . , n} has 2n elements.
1.4 20 + 21 + 22 + · · · + 2n = 2n+1 −1.
1.5 1 + 3 + 5 + · · · + (2n −1) = n2.
1.6 12 + 22 + · · · + n2 = n(n+1)(2n+1)
6
.
1.7 13 + 23 + · · · + n3 = n2(n+1)2
4
.
1.8 1 × 3 + 2 × 4 + · · · + n × (n + 2) = n(n+1)(2n+7)
6
.
1.9
1
1×3 +
1
2×4 + · · · +
1
n×(n+2) =
n(3n+5)
4(n+1)(n+2).
1.10
1
√
1 +
1
√
2 + · · · +
1
√n ≤2√n.
5.3.2
Prove the PMCI as stated in Theorem 216 using a variation of the proof of the PMI
in Theorem 215.
5.3.3
Show that the base step in the statement of Theorem 216, that is, the assumption
that the property P holds for n0, can be omitted.
5.3.4∗Show that the PMCI is equivalent to the PMI.

252
Logic as a Tool
5.3.5∗Prove that the PMCI is equivalent to the following form: “Let a property P of
natural numbers be such that, for any natural number k, if it holds for all natural
numbers less than k then it holds for k. Then P holds for all natural numbers.”
5.3.6
Some natural numbers seem more special, like 0, 1, 100, 333, 123456789, etc.
Let us call them interesting. Using the PWO, prove that all natural numbers are
interesting.
Exercises on Peano Arithmetic
5.3.7
Derive the following formulae of LN , all implicitly universally quantified, in the
axiomatic system of Peano Arithmetic (PA). You may write the proofs either as
formal derivations in (PA) or in a semi-formal mathematical style. Most of them
use the Induction Scheme formalizing the method of Mathematical Induction in N,
which is the main specific reasoning strategy here. When performing the proofs,
pay special attention to the separation of logical and arithmetical axioms used in
the proof steps.
7.1 x ̸= 0 →∃y(x = s(y))
7.2 0 + x = x
7.3 y + s(x) = s(y) + x
7.4 x + y = y + x
7.5 (x + y) + z = x + (y + z)
7.6 x × y = y × x
7.7 (x × y) × z = x × (y × z)
7.8 s(x) × y = x × y + y
7.9 x × (y + z) = x × y + x × z
7.10 x + s(y) ̸= x
7.11 x + y = x →y = 0
7.12 x ̸= 0 →x + y ̸= 0
7.13 (x ≤y ∧x ≤y) →x = y
7.14 (x ≤y ∧y ≤z) →x ≤z
7.15 x ≤y ∨y ≤x
7.16 x ≤s(y) ↔x ≤y ∨x = y
7.17 x ≤y ↔(x < y ∨x = y)
7.18 x < y ∨x = y ∨y < x
7.19 x ≤x + y
7.20 y ̸= 0 →x ≤x × y
7.21 x + y = 0 →(x = 0 ∧y = 0)
7.22 x × y = 0 →(x = 0 ∨y = 0)
7.23 x + z = y + z →x = y
7.24 (z ̸= 0 ∧x × z = y × z) →
x = y
7.25 x ≤n ↔(φ(0) ∧. . . ∧φ(n))
7.26 ∃xx≤nφ(x) ↔(φ(0) ∨. . . ∨
φ(n))
7.27 ∀xx≤nφ(x) ↔(φ(0) ∧. . . ∧
φ(n))
7.28 ∃xx<nφ(x) ↔(φ(0) ∨. . . ∨
φ(n–1))
7.29 ∀xx<nφ(x) ↔(φ(0) ∧. . . ∧
φ(n–1))
7.30 ∃xφ(x) →∃x(φ(x) ∧
∀yy<x¬φ(y)).
5.3.8
Recall that for any ground term t, v(t) denotes its value in N. Prove that the fol-
lowing hold.
(a) v(n) = n.
(b) If v(t) = n then PA ⊢t = n and if v(t) ̸= n then PA ⊢t ̸= n.
(c) If v(t1) = n and v(t2) = m then:
if n ≤m then PA ⊢t1 ≤t2 and if n ̸≤m then PA ⊢¬t1 ≤t2.
(d) Consequently, if v(t) = n then PA ⊢∃xx≤tφ(x) ↔(φ(0) ∨. . . ∨φ(n)).

Applications: Mathematical Proofs and Automated Reasoning
253
Giuseppe Peano (27.8.1858–20.4.1932) was an Italian mathe-
matician, one of the founders of mathematical logic, best known
for the axiom system, now called Peano Arithmetic, that he
proposed.
Peano was born and grew up in a farm in north Italy. He grad-
uated from the University of Turin in 1880 and then took up a
position and worked there for most of his career as Professor in
Mathematics.
Peano was strongly influenced by the logical system developed by Frege and,
along with Russell, did a lot to popularize and develop further his ideas. In 1889
Peano published his famous axioms, which defined the natural numbers in terms of
sets. Peano’s axioms, where “number” means “natural number,” are as follows.
1. Zero is a number.
2. The successor of any number is another number.
3. There are no two numbers with the same successor.
4. Zero is not the successor of a number.
5. (Axiom of Induction) If a set S of numbers contains zero and also the successor
of every number in S, then every number is in S.
Much later in 1930 Kurt Gödel proved his First Incompleteness Theorem, showing
not only that Peano’s system of axioms is incomplete but that it cannot be extended
to a complete system in any “reasonable” way, so that the axioms are effectively
(algorithmically) recognizable.
Another mathematical result for which Peano is known is his construction of
a space-filling curve – a continuous curve filling the entire unit square, an early
example of a fractal – as a counterexample to the claim that a continuous curve can
be enclosed in an arbitrarily small region.
Since 1892 Peano embarked on an extremely ambitious project called Formu-
lario Mathematico, intended as an encyclopedia of all mathematical formulae and
theorems expressed in a symbolic language. Many symbols that Peano used in the
Formulario are still in use today. His monumental work, completed in 1908, had a
strong influence due to its rigorous exposition and modern style, but Peano’s use of
it for teaching was not liked by his students and colleagues. In 1903 Peano started
developing an international language called Latino sine flexione, later also called
Interlingua, using Latin vocabulary but with simplified grammar to make it easier
to learn. He published the final edition of Formulario Mathematico in Latino sine
flexione, which was an added reason for its lack of popularity.
Eventually, Peano received wide recognition for his crucial contributions to mod-
ernizing mathematics and making mathematical notation and reasoning more rigor-
ous, based on mathematical logic.

254
Logic as a Tool
5.4
Applications: automated reasoning and logic programming
This section discusses briefly two of the most popular areas of logic-based applications to
computer science and artificial intelligence:
• automated reasoning and automated theorem proving; and
• logic programming and Prolog.
5.4.1
Automated reasoning and automated theorem proving
Automated reasoning is about the theory and development of mechanizable and com-
puter implementable methods and systems for logical deduction. The origin of this very
important area of applications dates back to the Middle Ages and is probably rooted in
Ramon Llull’s Ars Magna in the late 13th century. Automated reasoning in the modern
sense has been under active development since the mid 20th century. It has become ever
more intense and successful with the explosive development of computer hardware and
software and has found numerous important applications not only in logic and mathemat-
ics, but also in computer science, engineering, and in artificial intelligence, where it was
initially regarded as the most fundamental approach.
5.4.1.1
Automated and interactive theorem proving
A system of automated reasoning is based on a formal logical language for description
of the axioms, facts, assumptions, conjectures, and inference rules by means of formally
constructed expressions in that language plus an algorithmically implemented system for
deduction and formal reasoning. When such a system is used for proving (or disproving)
explicitly formulated conjectures, we talk about automated theorem proving. The proofs
produced by systems for automated reasoning usually describe how and why the con-
jecture follows from the axioms and hypotheses in a manner that can be understood and
agreed upon by everyone, and can be verified by means of a computer. The proof output
may not only be a convincing argument that the conjecture is a logical consequence of the
axioms and hypotheses, but it often also describes a process that may be implemented to
solve some open problems in the field.
Systems for automated theorem proving are powerful computer programs, potentially
capable of solving very difficult problems of high computational complexity and
practically impossible to solve manually. Furthermore, the underlying notion of logical
consequence – as in first-order logic – is often algorithmically undecidable, which means
that no completely automatic procedure can always guarantee successful derivation
of every logically valid consequence. For these reasons, the applications of systems
for automated theorem proving often need to be guided by an expert in the domain of
application, who has the intuition and insight to formulate intermediate conjectures

Applications: Mathematical Proofs and Automated Reasoning
255
(lemmas) and goals to optimize and direct the proof search in order to be completed
successfully in a reasonable amount of time and memory space. This approach is known
as interactive theorem proving.
A number of very successful systems for automated or interactive reasoning, such as
ACL2, Agda, Coq, HOL, Isabelle, LEGO, Mizar, NuPRL, PVS, TPS, and Twelf have been
designed and used to develop practically useful computer-aided proof assistants, while
many of them can also be run in a fully automatic mode. Some of these systems, such as
Isabelle, can be used as generic platforms for implementing other specific deductive sys-
tems for object logics, the axioms and rules of which can be formulated within Isabelle’s
metalogical language. The most commonly used formal logical language for automated
reasoning is first-order logic, but there are also several highly successful systems for
fully automated or interactive reasoning for non-classical or higher-order logics, based
on various deductive approaches such as lambda calculus, functional programming, term
rewriting, resolution, and tableaux.
Automated and interactive theorem-proving systems have been very useful in many
areas of mathematics. In particular, they have been instrumental for solving several open
problems in algebra, graph theory, logic, geometry, combinatorics, and calculus. Some
notable examples include the proofs of: the Four Color Theorem, stating that every map
in a plane can be colored using at most four colors in such a way that regions sharing
a common boundary do not share the same color, in 1976; the non-existence of a finite
projective plane of order 10, in 1989; Robbins conjecture that every “Robbins algebra”
(that is, algebra satisfying certain algebraic identities) is a Boolean algebra, in 1996; and
Kepler’s Conjecture (stating that the optimally dense sphere packing in a box is for cubic
or hexagonal arrangements), in 1998.
Most of the practically efficient systems for automated reasoning in first-order logic
have their inference engines based on various refinements and improvements of resolu-
tion. The currently most popular implemented systems for resolution-based automated
theorem proving include E-SETHEO, Prover9 (successor of Otter, one of the first imple-
mented systems for automated reasoning), SNARK, SPASS, Vampire. Some of these are
well supported and easy both to install and use online; some of the exercises ask you to
make use of them.
5.4.2
Logic programming and Prolog
Logic programming is a declarative programming style where the idea is to describe,
using a formal logic-based language, what the program is supposed to compute rather than
explicitly providing the instructions of how this should be executed, typical of the impera-
tive style of programming languages. A program written in a logic programming language
is therefore just a set of sentences in a formally specified logical language, expressing
facts, assumptions, and conjectures or queries about the problem domain.
The concept of logic programming can be traced back to the late 1960s–early 1970s,
just after the invention of the method of resolution with unification by J.A. Robinson,
when an intensive debate was underway on the topic of declarative versus procedural
knowledge representation and processing in the area of artificial intelligence. Currently,

256
Logic as a Tool
the most popular languages in logic programming include Prolog (which is short for for
“Programming in logic”), Datalog, and Answer Set Programming (ASP). I only briefly
discuss Prolog here. It is one of the first and most popular such languages originally devel-
oped in the early 1970s by Alain Colmerauer and Philippe Roussel in Marseille, France.
Prolog has been extensively used since then for automated theorem proving, expert sys-
tems, and other systems for intelligent control.
A typical program construction in a logic programming language is a clause-based
rule in the form:
A1 ∧. . . ∧An =⇒H
or, written in alternative notation:
H ←A1, . . . , An
or in Prolog:
H : −A1, . . . , An
where n ≥0. H is called the head of the rule and A1, . . . , An form the body of the rule.
The formulae A1, . . . , An in the body are called goals, for reasons that will become clear.
The rule above has the declarative meaning of the logical implication:
if A1 and . . . and An, then H
and can be formally identified with the clause {¬A1, . . . , ¬An, H}. All variables occur-
ring in a rule are assumed implicitly universally quantified over.
The body of the rule can be empty, when n = 0. Such a rule is written:
=⇒H
or in Prolog:
H : −
or just as H. It is called a fact, as it simply states the truth of H.
Further, when the head H of a rule is ⊥(the falsum), the rule is usually written:
A1 ∧. . . ∧An =⇒
or in Prolog:
: −A1, . . . , An
and is said to have an empty head. Such rule is called a query, explained further in the
following.
In most logic programming languages, such as the standard (pure) Prolog, the formulae
in the head and body of a rule can only be atomic formulae. Such rules represent definite
clauses, also called Horn clauses.An extension of Prolog, suitable for non-monotonic rea-
soning, also allows negations of atomic formulae in the body of a rule. A logic program
is just a finite set of Horn clauses in first-order language (without equality). A logic pro-
gram represents, in the same way as Resolution, a refutation-based deductive procedure
(the meaning of which is explained below). Derivations in a logic program are essentially
defined as for the system of first-order Resolution by successively applying Resolution

Applications: Mathematical Proofs and Automated Reasoning
257
with unification to derive new clauses (rules) that are added to the program. The purpose
is to eventually derive the empty clause : −(sometimes also denoted □) which has the
same meaning as in Resolution: a contradiction.
Due to the special shape of the clauses in a logic program, the application of Resolution
here is more intuitive: it consists of unifying the head of one rule with a goal in the body
of another, and then replacing that goal with the body of the latter rule after applying the
unifying substitution. To be precise, it works as follows. Given the rules
H : −A1, A2, . . . , An
and
C : −B1, . . . , Bm
such that σ is a most general unifier (MGU) of A1 and C (the fact that A1 is the first
goal in the body of the rule is not essential), Prolog resolves the rule above to produce the
new rule
σ(H : −B1, . . . , Bm, A2, . . . , An).
In particular, if applied to rules : −A1 and C : −it produces the empty clause : −.
Let me explain briefly how a logic program works.
First, a query : −A1, . . . , An represents the negated conjunction of the goals
A1, . . . , An and asks for answers or solutions: instantiations of the occurring variables
that satisfy the conjunction of the goals. The idea of a logic program P is that, given
a query G, the program P attempts to answer that query by deriving the empty clause
from its goals, possibly after suitable substitutions. The declarative meaning (semantics)
of a logic program P is given as follows. The program takes as an input a query G (or
its negation represented by the empty-headed rule with body G) and keeps applying
Resolution steps on its rules, as above. If it eventually derives the empty clause, the
output of the program is a solution of G in P , formally being the composite substitution
σ of (usually ground) terms for the variables occurring in G, applied in the derivation of
the empty clause. The purpose of that substitution is to suitably instantiate the variables
occurring in the query, in order to unify its goals with facts and heads of other rules in
the program, eventually computing an answer to the query.
In the case when no variables occur in the query, the program P must simply decide
whether the query follows logically from its clauses by trying to derive the empty clause
from it (i.e., from the rule representing its negation). In this case, the only possible solution
is the identity substitution when the empty clause is derivable, thus indicating an answer
“Yes.” Otherwise, the answer (if the program terminates at all) is “No,” which in fact
means “I don’t know” (see more on that further). In the general case, the solution is com-
puted by taking the composition of all unifying substitutions applied in the derivation of
the empty clause, and then restricting it to the variables occurring in the query G. In other
words, since the rule represents the negated query G, all variables in the original query
G are implicitly existentially quantified, so a solution to the query consists of explicit
witnesses for the existence of objects named by the terms in the query goals, satisfying
these goals.
Let us look at an example of a logic program written in Prolog. In what follows,
ancestor and parent are binary predicates and Elisabeth, Charles,

258
Logic as a Tool
Harry are constants/names (interpreted in the familiar domain of humans). The program
P consists of the following four facts and two rules:
(F1) parent(Elisabeth, Charles).
(F2) parent(Charles, William).
(F3) parent(Diana, William).
(F4) parent(William, George).
(R1) ancestor(X,Y) :- parent(X,Y).
(R2) ancestor(X,Y) :- parent(X,Z),ancestor(Z,Y).
1. First, consider the following query:
(Q0):
:- parent(Charles, William).
A derivation in the program P Q0 = P + (Q0) consist of a single step, resolving the
query (Q0) with the fact (F2) and producing the empty clause, thereby returning the
identity substitution, that is, the answer “Yes.”
2. Next, consider the following query:
(Q1):
:- ancestor(Charles, William).
Here is a derivation in the program P Q1 = P + (Q1): first, the substitution
[Charles/X, William/Y] unifies the head of (R1) with the body of (Q1), thus
resolving these two rules to produce the new query
(Q1′):
:- parent(Charles, William).
This, again, is immediately derivable, returning again the answer “Yes.”
3. On the other hand, running P on the query
(Q2):
:- parent(Elisabeth, William).
cannot produce the empty clause, simply because none of the rules in P can be resolved
with (Q2) since none of their heads can be unified with the only goal in the body of Q2.
P will therefore return an answer “No,” which actually means “I don’t know whether
parent(Elisabeth, William) holds or not.”
This feature of Prolog, known as Negation-by-failure, is discussed further below.
4. Now consider the query:
(Q3):
:- ancestor(Elisabeth, William).
A derivation in the program P Q3 = P + (Q3) is as follows.
(a) The substitution [Elisabeth/X, William/Y] unifies the head of (R2) with
the body of (Q3), thus resolving these two rules to produce the new query
(Q31):
:- parent(Elisabeth,Z), ancestor(Z,William).
(b) The substitution [Charles/Z] unifies the fact (F1) with the first goal in the body
of (Q31), thus resolving these rules to produce the new query
(Q32):
:- ancestor(Charles, William).

Applications: Mathematical Proofs and Automated Reasoning
259
(c) The substitution [Charles/X, William/Y] unifies the head of (R1) with the
body of (Q32), thus resolving these rules to produce the new query
(Q33):
:- parent(Charles, William).
(d) The fact (F2) is identical to the body of (Q33), thus resolving these rules to produce
the empty query/clause
(Q34):
:- .
The derivation of the empty clause in P Q3 means that the program P has answered
the original query (Q3) with “Yes.”
5. Let us now run P on the following query:
(Q4):
:- ancestor(Elisabeth, Y).
(a) The substitution [Elisabeth/X] unifies the head of (R1) with the body of (Q4),
thus resolving these two rules to produce the new query
(Q41):
:- parent(Elisabeth, Y).
(b) The substitution [Charles/Y] unifies the fact (F1) with the body of (Q41), thus
resolving these rules to produce the empty query
(Q42):
:- .
This completes the derivation, yielding a solution [Charles/Y].
If we are interested in only one solution, we can stop here. However, if we want
to look for other solutions we can re-run the program by instructing it to look for
another solution. This can be done by using a special feature of Prolog that cuts
the execution branch producing the previous solution and forces Prolog to look for
a new solution.
(c) In this case, re-running the program will cause Prolog to look for another way to
unify a rule with the body of (Q4). Such an alternative is provided by rule (R2) and
the substitution [Elisabeth/X, Wiliam/Y] will, as before, unify the head of
(R2) with the body of (Q4). This causes a further execution repeating the previous
one, which will also succeed, producing another solution: [Wiliam/Y].
(d) Repeating the re-running after disallowing the previous two solutions will likewise
produce yet another solution: [George/Y]. It is not difficult to see that no more
solutions can be obtained.
As expected, the program computes consecutively three answers – Charles,
William, George – to the query (Q4), which essentially asks “To whom is
Elisabeth an ancestor?”
How exactly does Prolog run? The derivation engine of Prolog is based on the ear-
lier mentioned SLD-resolution strategy, enhanced with a special mechanism for con-
trolled backtracking, which allows for a search of multiple solutions as above. As for
input resolution, SLD-resolution is not refutation-complete for first-order logic but it is
complete for the Horn logic of Prolog programs. However, because of the depth-first

260
Logic as a Tool
search strategy inbuilt to the SLD-resolution, pure Prolog does not always terminate even
when a derivation exists; it is therefore practically incomplete. To remedy that prob-
lem, Prolog employs some non-logical features such as cut and fail, implementing
“Negation-by-Failure” in a more aggressive way. This feature introduces an obvious prob-
lem: a Prolog program implicitly assumes that only what can be derived from its facts
and rules is true; everything else is declared to be false. Negation-by-failure therefore
yields a non-standard semantics of the negation: notA succeeds iff deriving A fails.
This semantics is non-monotonic: adding more facts or rules to the program may enable
more derivations and therefore falsify some negated statements which were previously
declared true.
Another problem with the semantics of Prolog programs arises from the control of
the execution of such programs. The execution, and sometimes the outcome, of a Prolog
program can be sensitive to the order in which the rules are listed within the program or
how the literals are ordered within a rule; the programmer must take this into account
when ordering these. Furthermore, the predicates cut and fail enable a mechanism
for preventing the deductive engine from revisiting certain paths in the search space (by
using cut) or for pruning them altogether (by using fail), which alters the purely logical
semantics of a Prolog program. However, skillful use of these features can produce very
elegant and efficient Prolog programs. Despite these strange effects, Prolog has remained
one of the top choices of logic programming language for over 40 years.
References for further reading
There is abundant literature on automated reasoning and theorem proving, including clas-
sical texts and references on the topic (some freely available on the internet) such as:
Robinson and Voronkov (2001), Ben-Ari (2012), Fitting (1996), Chang and Lee (1997),
Nerode and Shore (1993), and Gallier (1986). In addition, some specific useful references
include:
• https://en.wikipedia.org/wiki/Automated_reasoning;
• Geoff Sutcliffe’s Overview of Automated Theorem Proving
http://www.cs.miami.edu/tptp/OverviewOfATP.html; and
• The Thousand Problems for Theorem Provers (TPTP) library www.tptp.org.
For more on the history, theory, and art of logic programming and Prolog, see Nilsson
and Małuszy´nski (1995), Spivey (1995), Ebbinghaus et al. (1996), Fitting (1996), and
Ben-Ari (2012).
Exercises
5.4.1
Use the online version WebSPASS of the automated theorem prover SPASS
http://www.spass-prover.org, or any other implementation of an automated
theorem prover of your choice, to formalize and solve exercises 7, 8, 9, and 14
from Section 4.5.8, as well as the formalized exercises from Section 5.2.7.

Applications: Mathematical Proofs and Automated Reasoning
261
5.4.2
For many more exercises on automated theorem proving, using SPASS or any of
the other implemented tools mentioned in the text, visit the Thousand Problems
for Theorem Provers (TPTP) library www.tptp.org.
5.4.3
Consider the following Prolog program:
(F1) parent(Elisabeth, Charles).
(F2) parent(Charles, William).
(F3) parent(Diana, William).
(F3) parent(Diana, Harry).
(F4) parent(William, George).
(R1) ancestor(X,Y) :- parent(X,Y).
(R2) ancestor(X,Y) :- parent(X,Z),ancestor(Z,Y).
Run the program on the following input queries to produce all solutions (if any):
(a) (Q1) :- ancestor(Elisabeth, Harry).
(b) (Q2) :- ancestor(Diana, Y).
(c) (Q3) :- ancestor(X, Harry).
(d) (Q4) :- ancestor(X, George).
Alan
Mathison
Turing
(23.6.1912–7.6.1954)
was
a
British mathematician, logician, computer scientist, and
crypto-analyst, regarded as one of the founders of both
theoretical computer science, for his creation of the universal
model of computations, and artificial intelligence, for
proposing the idea of using a special test (Turing’s test)
for deciding if an artificial agent has intelligence. Turing
was instrumental in breaking the secret codes of the German
Enigma coding machine used during World War II.
In 1935 Turing studied Gödel’s incompleteness results
and Hilbert’s Entscheidungsproblem, which queries the
existence of an algorithm that takes as input a sentence of a first-order logic
and answers if it is true in every structure. The following year he wrote his
groundbreaking paper On Computable Numbers, with an Application to the
Entscheidungsproblem, where he introduced an abstract universal computing
device, now called a Turing machine, and argued that such a machine can
perform any conceivable mathematical computation which can be represented
as an algorithm. Remarkably, Turing machines described the concept of modern
programmable computers before they were actually invented. Moreover, Turing
described in his 1936 paper the construction of a universal Turing machine which
can simulate the work of any Turing machine. He then showed that the halting
problem for Turing machines is algorithmically undecidable in a sense that there
cannot exist an algorithm (represented by a Turing machine) that can take the code

262
Logic as a Tool
of any Turing machine and an input for it and determine as an output whether
that Turing machine will eventually halt for that input. Turing then concluded that
Hilbert’s Entscheidungsproblem has no solution, thus reproving this fundamental
result which had recently been published by Church.
Turing completed a doctorate in mathematical logic in Princeton University under
Church’s supervision in 1938. Church and Turing showed that the Church’s lambda
calculus and the Turing machine were equivalent as models of computation, and
argued that they capture precisely the intuitive notion of algorithm; this claim became
known as the Church–Turing thesis.
In 1938, Turing was invited to work for the British crypto-analytic department to
help break the secret codes of the German encrypting machines Enigma. Turing was
instrumental in developing methods and constructing special devices, called Bombe,
for decrypting messages sent by Enigma coders, saving many lives during the course
of the World War II. At the end of the war Turing was invited by the National Physical
Laboratory in London to design a real, physical computer and he submitted a report
proposing the Automatic Computing Engine (ACE) in 1946.
Turing was a homosexual and was arrested and convicted for violation of the
British anti-homosexuality law in 1952. To avoid going to prison, he agreed to chem-
ical castration. Turing died in 1954 of cyanide poisoning, widely believed to be
suicide following his prosecution and treatment. In 2009 the British Prime Minister
offered an official apology from the British Government for Turing’s prosecution,
and in 2013 he was granted a royal pardon by the Queen.
In 1966 the Association for Computing Machinery established the Turing Award
as the most prestigious award for scientific contributions in computer science,
regarded as the equivalent to the Nobel Prize.

6
Answers and Solutions to Selected
Exercises
Section 1.1
1.1.1
(a) Yes. (b) Yes. (c) No. (d) No. (e) Yes. (f) No. (Imperative sentence.) (g) No.
(“her” is not defined in this sentence, so truth value cannot be assigned to it.) (h)
Yes. (i) No. (Question.) (i) Yes. (k) Yes. (True.) (l) No. (Either truth value can be
assigned.)
1.1.2
(a) true. (c) true. (e) false. (g) true.
1.1.3
.(a) Antecedent: “John talks.” Consequent: “Everyone else listens.”
(c) Antecedent: “John talks.” Consequent: “Everyone else listens.”
(e) Antecedent: “The cube of the integer n is positive.”
Consequent: “An integer n is positive.”
(g) Antecedent: “A function is differentiable.”
Consequent: “The function is continuous.”
(i) Antecedent: “A function is continuous.”
Consequent: “The function is differentiable.”
1.1.4
(a) Neither. (c) Sufficient. (e) Necessary and sufficient.
(g) Necessary. (i) Necessary and sufficient.
1.1.5
Denote:
“The Earth rotates around itself” by A (true),
“The Moon rotates around the Earth” by B (true),
“The Sun rotates around the Moon” by C (false),
“The Sun rotates around the Earth” by D (false),
“The Earth rotates around the Moon” by E (false).
Then we have:
(a) A ∧(B →C): false. (c) (¬D ∧¬E) →¬B: false.
(e) A ↔(¬B ∨¬E): true.
1.1.6
.(a) By the truth definition of the implication, A must be true.
(c) ¬B is true so B is false. Then, for A ∨B to be true, A must be true.
Logic as a Tool: A Guide to Formal Logical Reasoning, First Edition. Valentin Goranko.
© 2016 John Wiley & Sons, Ltd. Published 2016 by John Wiley & Sons, Ltd.

264
Logic as a Tool
(e) Since ¬C ∧B is true then B is true and ¬C is true, hence C is false. Since
¬(A ∨C) →C is true and C is false, then ¬(A ∨C) is false, that is, A ∨C
is true. Now since C is false, A must be true.
1.1.7
.(b) Suppose R is false. Since P ∨R is true, then P is true. For (P ∨Q) →R to
be true, P ∨Q must be false. Hence P and Q is false. Contradiction. Hence
R is true.
(d) Suppose Q is true. Since ¬P →¬Q must be true, then ¬P must be false
and thus P is true. For P ∧¬R to be false, ¬R must be false. But this is a
contradiction with ¬R true. Hence Q is false.
(e) Suppose P is true. Since P →Q is true then ¬Q is false. Hence S ∧¬Q is
false. But R ∨(S ∧¬Q) is true so R must be true. This contradicts ¬R is
true, which was given, hence P must be false.
(i) Since P →R is false then P is true and R is false. But with P →Q true,
this means Q is true. Since Q →(R ∨S) is true then R ∨S must be true.
Since R is false we have that S is true.
1.1.8
.(g) Tautology:
p
q
(p
∨
¬q)
→
¬(q
∧
¬p)
F
F
F
T
T
T
T F
F
T
F
T
F
F
F
T
F T
T
T
T
F
T
T
T
T
T F
F
F
T
T
T
T
F
T
T T
F
F
↑
(i) Neither a tautology nor a contradictory formula:
p
q
r
¬(¬p
↔
q)
∧
(r
∨
¬q)
F
F
F
T T
F
F
T
F
T
T
F
F
T
T T
F
F
T
T
T
T
F
T
F
F T
T
T
F
F
F
F
F
T
T
F T
T
T
F
T
T
F
T
F
F
F F
T
F
F
F
T
T
T
F
T
F F
T
F
F
T
T
T
T
T
F
T F
F
T
F
F
F
F
T
T
T
T F
F
T
T
T
T
F
↑
(j) Contradictory formula:
p
q
r
¬
((p ∧¬q) →r)
↔(¬(q ∨r)
→
¬p)
F
F
F
F
F F T T F
F T F F F
T
T
F
F
T
F
F F T T T
F F F T T
T
T
F
T
F
F
F F F T F
F F T T F
T
T
F
T
T
F
F F F T T
F F T T T
T
T
T
F
F
T
T T T F F
F T F F F
F
F
T
F
T
F
T T T T T
F F F T T
T
F
T
T
F
F
T F F T F
F F T T F
T
F
T
T
T
F
T F F T T
F F F T T
T
F
↑

Answers and Solutions to Selected Exercises
265
1.1.9
.(d) If ((p →q) ∨(p →¬q)) →¬p is not a tautology, then:
((p →q) ∨(p →¬q)): T and ¬p: F
[p →q: T or p →¬q: T] and p: T
[p →q: T and p : T] or [p →¬q: T and p: T]
[q: T and p: T] or [q: F and p: T]
The formula is not a tautology. Either of the assignments p : T, q : T or p : T,
q : F will render the formula false.
(j) In order to falsify the formula ((p ∨q) →r) →((p →r) ∧(q →r)), we
must have (p ∨q) →r : T and (p →r) ∧(q →r) : F. For (p ∨q) →r to
be true, there are two possible cases:
Case 1: p ∨q : F. Then p : F and q : F. But then p →r : T and q →r : T so
that (p →r) ∧(q →r) : T, a contradiction with (p →r) ∧(q →r) : F.
Case 2: r : T. Then p →r : T and q →r : T so that (p →r) ∧(q →r) : T,
a contradiction with (p →r) ∧(q →r) : F.
All attempts to falsify the formula end in contradictions, so the formula is a
tautology.
(l) If ((p →r) ∨(q →r)) →((p ∨q) →r) is not a tautology then:
((p →r) ∨(q →r)) : T and (p ∨q) →r: F
[p →r: T or q →r: T] and [p ∨q: T and r: F]
[p →r: T and p ∨q: T and r: F] or [q →r: T and p ∨q: T and r: F]
[p: F and r: F and p ∨q: T] or [q: F and r: F and p ∨q: T]
[p: F and q: T and r: F] or [p: T and q: F and r: F]
The formula is not a tautology. For example, the assignment p : F, q : T, r :
F will render it false.
1.1.10
.(a) P cannot be a liar, otherwise he would be telling the truth. So, P is a truth-teller
and therefore Q must be a liar.
(b) If the answer was “yes,” the inhabitant he asked is a truth-teller and the other
inhabitant a liar. If the answer was “no,” then either both are truth-tellers or
the one who answered was a liar, so in this case the stranger would not be
able to determine who is a liar and who is not. The answer must therefore
have been “yes”.
(c) If A is a liar, then his claim must have been false, therefore B must be a liar
while A is not. So, A cannot be a liar. Therefore, B cannot be a liar either.
Therefore both A and B are truth-tellers.
(d) Note that no local can say that he is a liar, as this would result in the Liar’s
paradox; Y’s statement must therefore be false, making him a liar. But this
means that Z’s statement is true, so he is a truth-teller. Since X’s answer
invariably must be “No, I’m a truth-teller,” it is not possible to determine
whether X is a truth-teller or liar.

266
Logic as a Tool
(e) If D is a truth-teller, then E must be a liar. C’s answer is indeed “just one,”
so C must also be a liar; otherwise, there would be two of them and C’s
answer would have been false. There is therefore only one truth-teller; C
must therefore have said the truth – a contradiction. D must therefore have
been a liar, so E must have been a truth-teller and the stranger should go with
E since E has truthfully said that he was not a man-eater.
(f) The stranger could ask either inhabitant, “What would your answer be if I
asked you whether your road leads to the spaceship port?” Note that a liar
and a truth-teller would answer such a question in the same way. If the answer
is “yes” the stranger should take that road, otherwise, the stranger should take
the other road.
Section 1.2
1.2.1
(1) ⇒(3): Assume A1, A2, . . . , An |= B. Then, for all assignments of truth val-
ues to the variables occurring in A1, A2, . . . , An, B, if A1, A2, . . . , An are true,
then B is also true. Hence, for all such assignments, A1 ∧A2 ∧. . . ∧An →B
is true, so A1 ∧A2 ∧. . . ∧An →B is a tautology.
(3) ⇒(2): It is sufficient to show that |= A →B implies A |= B. Indeed, suppose
|= A →B. Then any assignment of truth values to the variables occurring in A, B
that make A true, also make – by the truth definition of the implication – B true.
Therefore, A |= B.
1.2.2
.(a) q →p is not a consequence of p →q since the third row contains premises
with truth values T but with a truth value F for the conclusion:
p
q
p →q
q →p
T
T
T T T
T T T
T
F
T F F
F T T
F
T
F T T
T F F
F
F
F T F
F T F
Hence, the rule is not sound.
(c) ¬q →¬p is a consequence of p →q since wherever the premises have truth
values T, the conclusion also has a truth value T:
p
q
p →q
¬q →¬p
T
T
T T T
F T F
T
F
T F F
T F F
F
T
F T T
F T T
F
F
F T F
T T T
Hence, the rule is sound.

Answers and Solutions to Selected Exercises
267
1.2.3
.(a) The inference rule is sound since wherever the premises have truth values T,
the conclusion also has a truth value T:
p
q
r
p →q
¬q ∨r
¬r
¬p
F
F
F
F T F
T T F
T
T
F
F
T
F T F
T T T
F
T
F
T
F
F T T
F F F
T
T
F
T
T
F T T
F T T
F
T
T
F
F
T F F
T T F
T
F
T
F
T
T F F
T T T
F
F
T
T
F
T T T
F F F
T
F
T
T
T
T T T
F T T
F
F
↑
↑
↑
(c) The inference rule is not sound since the third and seventh rows contain
premises with truth values T but with a truth value F for the conclusion:
p
q
r
p →q
p ∨¬r
¬r
¬q ∨r
F
F
F
F T F
F T T
T
T T F
F
F
T
F T F
F F F
F
T T T
F
T
F
F T T
F T T
T
F F F
F
T
T
F T T
F F F
F
F T T
T
F
F
T F F
T T T
T
T T F
T
F
T
T F F
T T F
F
T T T
T
T
F
T T T
T T T
T
F F F
T
T
T
T T T
T T F
F
F T T
↑
↑
↑
↑
1.2.4
.(b) Denote the propositions “Y is greater than −1” by p and “Y is greater than
−2” by q. Then the inference rule for the argument can be written as:
p →q, ¬q
¬p
.
This inference rule is sound:
p
q
p →q
¬q
¬p
F
F
F T F
T
T
F
T
F T T
F
T
T
F
T F F
T
F
T
T
T T T
F
F
↑
↑
(d) Denote “Victor is good at logic” by p, “Victor is clever” by q, and “Victor is
rich” by r. Then the inference rule for the argument can be written as:
p →q, q →r
p →r
.

268
Logic as a Tool
The truth table:
p
q
r
p →q
q →r
(p →r)
F
F
F
F T F
F T F
F T F
F
F
T
F T F
F T T
F T T
F
T
F
F T T
T F F
F T F
F
T
T
F T T
T T T
F T T
T
F
F
T F F
F T F
T F F
T
F
T
T F F
F T T
T T T
T
T
F
T T T
T F F
T F F
T
T
T
T T T
T T T
T T T
↑
↑
↑
This inference rule is sound.
1.2.5
.(a) Converse: If a is greater than −2, then a is greater than −1.
Inverse: If a is not greater than −1, then a is not greater than −2.
Contrapositive: If a is not greater than −2, then a not is greater than −1.
(c) Converse: The square of x is positive only if x is positive.
Inverse: x is not positive only if its square is not positive.
Contrapositive: The square of x is not positive only if x is not positive.
(e) Converse: For the function f to be differentiable it is sufficient that it is con-
tinuous.
Inverse: For the function f to be discontinuous it is sufficient that it is not
differentiable.
Contrapositive: For the function f to be not differentiable it is sufficient that
it is discontinuous.
(g) Converse: For the integer n to be indivisible by 10 it is necessary that it is
prime.
Inverse: For the integer n to be not prime it is necessary that it is divisible
by 10.
Contrapositive: For the integer n to be divisible by 10 it is necessary that it
is not prime.
Section 1.3
1.3.2
.(a) .
p
q
¬
(p ↔q)
(p ∧¬q)
∨
(q ∧¬p)
F
F
F
F T F
F F T
F
F F T
F
T
T
F F T
F F F
T
T T T
T
F
T
T F F
T T T
T
F F F
T
T
F
T T T
T F F
F
T F F
↑
↑

Answers and Solutions to Selected Exercises
269
(f) .
p
q
r
p
→
(q →r)
(p ∧q)
→
r
F
F
F
F
T
F T F
F F F
T
F
F
F
T
F
T
F T T
F F F
T
T
F
T
F
F
T
T F F
F F T
T
F
F
T
T
F
T
T T T
F F T
T
T
T
F
F
T
T
F T F
T F F
T
F
T
F
T
T
T
F T T
T F F
T
T
T
T
F
T
F
T F F
T T T
F
F
T
T
T
T
T
T T T
T T T
T
T
↑
↑
(h) .
p
q
r
(p →r)
∧
(q →r)
(p ∨q)
→
r
F
F
F
F T F
T
F T F
F F F
T
F
F
F
T
F T T
T
F T T
F F F
T
T
F
T
F
F T F
F
T F F
F T T
F
F
F
T
T
F T T
T
T T T
F T T
T
T
T
F
F
T F F
F
F T F
T T F
F
F
T
F
T
T T T
T
F T T
T T F
T
T
T
T
F
T F F
F
T F F
T T T
F
F
T
T
T
T T T
T
T T T
T T T
T
T
↑
↑
1.3.3
.(h) .
p
q
r
(p →r)
∨
(q →r)
(p ∨q)
→
r
F
F
F
F T F
T
F T F
F F F
T
F
F
F
T
F T T
T
F T T
F F F
T
T
F
T
F
F T F
T
T F F
F T T
F
F
F
T
T
F T F
T
T F F
F T T
T
T
T
F
F
T F F
T
F T F
T T F
F
F
T
F
T
T T T
T
F T T
T T F
T
T
T
T
F
T F F
F
T F F
T T T
F
F
T
T
T
T T T
T
T T T
T T T
T
T
↑
↑
The respective (indicated) columns of truth values are not the same, hence the
formulae are not equivalent.

270
Logic as a Tool
(k) .
p
q
r
p
↔
(q ↔r)
q
↔
(p ↔r)
F
F
F
F
F
F T F
F
F
F T F
F
F
T
F
T
F F T
F
T
F F T
F
T
F
F
T
T F F
T
T
F T F
F
T
T
F
F
T T T
T
F
F F T
T
F
F
T
T
F T F
F
T
T F F
T
F
T
T
F
F F T
F
F
T T T
T
T
F
T
F
T F F
T
F
T F F
T
T
T
T
T
T T T
T
T
T T T
↑
↑
The respective (indicated) columns of truth values are the same, hence the
formulae are equivalent.
1.3.4
.(a)
¬((p ∨¬q) ∧¬p)
≡¬(p ∨¬q) ∨¬¬p
≡(¬p ∧¬¬q) ∨p
≡(¬p ∧q) ∨p
(f)
¬(p →(¬q ↔r))
≡p ∧¬(¬q ↔r)
≡p ∧((¬¬q ∧r) ∨(¬q ∧¬r))
≡p ∧((q ∧r) ∨(¬q ∧¬r))
Section 1.4
No solutions are provided for these exercises, but the interested reader can find many of
these in the references provided at the end of the section.
Section 2.2
2.2.2
(c) 1. ⊢H (p ∧q) →q
by Axiom (∧1)
2. ⊢H (p ∧q) →p
by Axiom (∧2)
3. ⊢H ((p ∧q) →q) →(((p ∧q) →p) →((p ∧q) →(q ∧p))) by Axiom
(∧3)
4. ⊢H ((p ∧q) →p) →((p ∧q) →(q ∧p))
by 1, 3, and MP
5. ⊢H (p ∧q) →(q ∧p)
by 2, 4, and MP
(e) 1. ⊢H p →((p →p) →p)
by Axiom (→1)
2. ⊢H (p →((p →p) →p)) →((p →(p →p) →(p →p)))
by Axiom
(→2)
3. ⊢H ((p →(p →p) →(p →p)))
by 1, 2, and MP
4. ⊢H p →(p →p)
by Axiom (→1)
5. ⊢H p →p
by 3, 4, and MP
2.2.5
Let Γ, A ⊢H B and Γ, B ⊢H C. Then, Γ ⊢H B →C by the Deduction Theorem
(DT). Therefore, Γ, A ⊢H B →C, hence Γ, A ⊢H C by using Modus Ponens.

Answers and Solutions to Selected Exercises
271
2.2.6
Let A ⊢H B. Then B →C, A ⊢H B. Besides, B →C, A ⊢H B →C. There-
fore B →C, A ⊢H C by MP, hence B →C ⊢H A →C by DT.
2.2.9
(e) 1. P →(Q →R) ⊢H P →(Q →R)
2. P →(Q →R), P ⊢H (Q →R)
by 1 and DT
3. P →(Q →R), P, Q ⊢H R
by 2 and DT
4. P →(Q →R), Q ⊢H (P →R)
by 3 and DT
P →(Q →R) ⊢H Q →(P →R)
by 4 and DT.
(g) 1. P →(Q →R) ⊢H P →(Q →R)
2. P →(Q →R), P ⊢H (Q →R)
by 1 and DT
3. P →(Q →R), P ∧Q ⊢H P
by Axiom (∧1)
4. P →(Q →R), P ∧Q ⊢H (Q →R)
by 2, 3, and Exercise 5.
5. P →(Q →R), P ∧Q, Q ⊢H R
by 4 and DT
6. P →(Q →R), P ∧Q ⊢H Q
by Axiom (∧2)
7. P →(Q →R), P ∧Q ⊢H R
by 5, 6, and Exercise 5.
8. P →(Q →R) ⊢H (P ∧Q) →R
by 7 and DT.
(i) 1. ⊢H (¬P →¬Q) →((¬P →Q) →P)
by Axiom (→3)
2. ¬P →¬Q ⊢H (¬P →¬Q) →((¬P →Q) →P)
by 1
3. ¬P →¬Q ⊢H ¬P →¬Q
4. ¬P →¬Q ⊢H ((¬P →Q) →P)
by 2, 3, and MP.
5. Q ⊢H ¬P →Q
by Axiom (→1)
6. (¬P →Q) →P ⊢H Q →P
by 5 and Exercise 6
7. ¬P →¬Q ⊢H Q →P
by 4, 6, and Exercise 5.
(m)1. ⊢H (¬P →¬¬P) →((¬ →¬P ) →P)
by Axiom (→3)
2. ¬P →¬¬P ⊢H (¬P →¬P) →P
by 1 and DT
3. ⊢H ¬¬P →(¬P →¬¬P)
by Axiom (→1)
4. ¬¬P ⊢H ¬P →¬¬P
by 3 and DT
5. ¬¬P ⊢H (¬P →¬P ) →P
by 2, 4, and Exercise 5.
6. ¬P ⊢H ¬P
7. ⊢H ¬P →¬P
by 6 and DT
8. ¬¬P ⊢H P
by 5, 7, and MP.
(o) Sketch:
1. P ⊢H P ∨¬P
by Axiom (∨1)
2. ¬(P ∨¬P ) ⊢H ¬P
by 1 and Exercise 9k.
3. ¬P ⊢H P ∨¬P
by Axiom (∨1)
4. ¬(P ∨¬P ) ⊢H ¬¬P
by 3 and Exercise 9k.
5. ⊢H ¬(P ∨¬P) →¬¬P
by 4 and DP
6. ⊢H (¬(P ∨¬P) →¬¬P) →((¬(P ∨¬P ) →¬P) →(P ∨¬P)).
by Axiom (→3)
7. ⊢H (¬(P ∨¬P) →¬P) →(P ∨¬P)
by 5, 6, and MP
8. ⊢H ¬(P ∨¬P) →¬P
by 2 and DP
9. ⊢H P ∨¬P
by 7, 8, and MP.

272
Logic as a Tool
Section 2.3
2.3.1
.(a) The formula is not a tautology:
((p →q) →q) →q : F
(p →q) →q : T, q : F
p →q : F
p : T, q : F
⃝
q : T
×
(c) Tautology:
((p →q) ∧(p →¬q)) →¬p : F
(p →q) ∧(p →¬q) : T, ¬p : F
p : T
p →q : T, p →¬q : T
p : F
×
q : T
p : F
×
¬q : T
q : F
×

Answers and Solutions to Selected Exercises
273
(e) Tautology:
((p →q) →(p →r) →(p →(q ∧r)) : F
(p →q) →(p →r) : T, p →(q ∧r) : F
p →q : T, p →r : T
p : F
p : T, q ∧r : F
×
q : T
p : T, q ∧r : F
q : F
×
r : F
p : F
×
r : T
×
(g) Not a tautology:
((p →r) ∨(q →r) →((p ∨q) →r)) : F
(p →r) ∨(q →r) : T, (p ∨q) →r : F
p ∨q : T, r : F
p : T
p →r : T
p : F
×
r : T
×
q →r : T
q : F
⃝
r : T
×
q : T
p →r : T
p : F
⃝
r : T
×
q →r : T
q : F
×
r : T
×

274
Logic as a Tool
(i) Tautology:
p →((q →r) →((p →q) →r)) : F
p : T, (q →r) →((p →q) →r) : F
q →r : T, (p →q) →r : F
p →q : T, r : F
p : F
×
q : T
q : F
×
r : T
×
(k) Not a tautology.
2.3.2
.(b) The logical consequence is valid.
(¬p ∧q) →¬ r : T, r : T, p ∨¬ q : F
p : F, ¬q : F
q : T
¬p ∧q : F
¬p : F
p : T
×
q : F
×
¬r : T
r : F
×

Answers and Solutions to Selected Exercises
275
(d) The logical consequence is not valid.
(p ∧q) →r) : T, (p →r) ∧(q →r) : F
p →r : F
p : T, r : F
r : T
×
p ∧q : F
p : F
×
q : F
⃝
q →r : F
q : T, r : F
r : T
×
p ∧q : F
p : F
⃝
q : F
×
(g) The logical consequence is valid.
p →(q ∧r) : T, (p →q) ∧(p →r) : F
p →q : F
p : T, q : F
p : F
×
q ∧r : T
q : T, r : T
×
p →r : F
p : T, r : F
p : F
×
q ∧r : T
q : T, r : T
×
2.3.3
For each of these problems we construct the tableau with all premises assigned the
required truth values and the conclusion assigned the opposite truth value. If the
tableau closes, then such truth assignment is impossible; the stated consequence
therefore holds. If the tableau does not close, then the stated consequence does not
necessarily hold.
(a) The tableau closes, so the consequence holds:
P →Q : T, Q →R : T, P ∧S : T, R ∧S : F
P : T, S : T
P : F
×
Q : T
Q : F
×
R : T
R : F
×
S : F
×

276
Logic as a Tool
(c) The tableau closes, so the consequence holds:
Q →(R ∧S) : T, Q ∧S : F, R ∧Q : T
R : T, Q : T
Q : F
×
S : F
Q : F
×
R ∧S : T
R : T, S : T
×
(e) The tableau closes, so the consequence holds:
Q →(R ∧S) : T, Q ∧S : F, Q : T
Q : F
S : F
Q : F
×
R ∧S : T
R : T, S : T
×
2.3.4
.(a) The inference rule is sound:
(p ∨q) →r : T, p ∨r : T, r : F
p : T
r : T
×
p ∨q : F
p : F, q : F
r : T
×

Answers and Solutions to Selected Exercises
277
(c) The inference rule is not sound:
p →q : T, p ∨¬r : T, q →r : F
q : T, r : F
p : T
p : F
×
q : T
⃝
¬r : T
r : F
p : F
⃝
q : T
⃝
(e) The inference rule is not sound:
p →q : T, r ∨(s ∧¬ q) : T, q : T, ¬p : F
p : T
p : F
×
q : T
s ∧¬ q : T
s : T, ¬q : T
q : F
×
r : T
⃝
2.3.5
We first formalize each of the propositional arguments by identifying the atomic
propositions in it and replacing them with propositional variables. We then check
the soundness of the resulting inference rule.
(b) Denote “n is divisible by 2” by p, “n is divisible by 3” by q, and “n is divisible
by 6” by r. Then the inference rule on which the argument is based is:
(p ∧q) →r, r →p, ¬q
¬r
.

278
Logic as a Tool
The rule is not sound, hence the argument is not correct.
(p ∧q) →r : T, r →p : T, ¬q : T, ¬r : F
q : F
r : T
r : F
×
p : T
p ∧q : F
p : F
×
q : F
⃝
r : T
⃝
(d) Denote “Socrates is wise” by p, “Socrates is happy” by q, and “Socrates is a
philosopher” by r. Then the inference rule on which the argument is based is:
p ∨¬q, ¬p →¬r
r →¬q
.
The rule is not sound, hence the argument is not correct.
p ∨¬q : T, ¬p →¬r : T, r →¬q : F
r : T, ¬q : F
q : T
¬q : T
q : F
×
p : T
¬p : F
p : T
⃝
¬r : T
r : F

Answers and Solutions to Selected Exercises
279
(f) Denote
“ Victor studies hard” by p,
“ Victor is clever” by q,
“ Victor will go to a university” by r, and
“ Victor will get a good job” by s.
Then the inference rule on which the argument is based is:
(p ∧q) →r, ¬p →¬s
q →(r ∨¬s)
.
The rule is sound, hence the argument is correct.
(p ∧q) →r : T, ¬p →¬s : T, q →(r ∨¬s) : F
q : T, r ∨¬s : F
r : F, ¬s : F
s : T
¬p : F
p : T
p ∧q : F
p : F
×
q : F
×
r : T
×
¬s : T
s : F
×
(h) Denote
“ Johnnie learns logic” by p,
“ Johnnie skips lectures” by q, and
“ Johnnie passes the exam” by r.

280
Logic as a Tool
Then the inference rule on which the argument is based is:
q →¬p, p ∧¬q →r
q ∨¬r →¬p
.
The rule is sound, hence the argument is correct.
q →¬p : T, (p ∧¬q) →r : T, (q ∨¬r) →¬p : F
q ∨¬r : T, ¬p : F
p : T
q : F
q : T
×
¬r : T
r : F
p ∧¬q : F
p : F
×
¬q : F
q : T
×
r : T
×
¬p : T
p : F
×
(j) Denote
“Bonnie comes/will come to the party” by p,
“Clyde comes/will come to the party” by q, and
“Alice comes/will come to the party” by r.
Then the argument is based on the following inference rule:
¬p →(¬q ∨r), q →¬r
q →p
.

Answers and Solutions to Selected Exercises
281
¬p →(¬q ∨r) : T, q →¬r : T, q →p : F
q : T, p : F
q : F
×
¬r : T
r : F
¬p : F
p : T
×
¬q ∨r : T
¬q : T
q : F
×
r : T
×
The rule is sound, hence the argument is correct.
Section 2.4
2.4.2
.
Introduction rule
Elimination rules
(↔I)
[A] [B]
...
...
B
A
A ↔B
(↔E) A,
A ↔B
B
,
B,
A ↔B
A
2.4.3
.(c) .
[¬p]1
[¬¬p]2
¬p
⊥
p
1
¬¬p →p
2

282
Logic as a Tool
(d) .
[p]1
[¬(p ∨¬p)]3
p ∨¬p
⊥
¬p
1
[¬p]2
[¬(p ∨¬p)]3
p ∨¬p
⊥
¬¬p
2
⊥
p ∨¬p
3
(f) ⊢ND ((p →q) ∧(p →¬q)) →¬p:
[p]1
[(p →q) ∧(p →¬q)]2
p →q
q
[p]1
[(p →q) ∧(p →¬q)]2
p →¬q
¬q
⊥
¬p
1
((p →q) ∧(p →¬q)) →¬p
2
2.4.4
.(a) Suppose A ⊢ND B.
[A]1
...
¬B ,
B
⊥
¬A
1
Hence ¬B ⊢ND ¬A.
(b) Suppose ¬B ⊢ND ¬A.
[¬B]1
...
A ,
¬A
⊥
B
1
Hence A ⊢ND B.

Answers and Solutions to Selected Exercises
283
2.4.5
.(a) A →C, B →C
⊢ND
(A ∨B) →C:
[A]1, A →C
[B]1, B →C
[A ∨B]2
C
C
C
(A ∨B) →C
2
1
(c) (A ∨B) →C
⊢ND
(A →C) ∧(B →C):
[A]1
A ∨B
(A ∨B) →C
C
A →C
1
[B]2
A ∨B
(A ∨B) →C
C
B →C
2
(A →C) ∧(B →C)
(e) A →¬A
⊢ND
¬A:
[¬¬A]2, [¬A]1
⊥
A
1
A →¬A
¬A
[¬¬A]2
⊥
¬A
2
NB: there is a simpler derivation. Find it!
(g) ¬A →B, ¬A →¬B ⊢ND A:
[¬A]1, ¬A →B
B
[¬A]1, ¬A →¬B
¬B
⊥
A
1

284
Logic as a Tool
(i) (¬A ∧B) →¬C, C ⊢ND A ∨¬B:
C
[¬(A ∨¬B)]2
use 4(a) →
...
¬A
[¬(A ∨¬B)]2
... ←use 4(a)
¬¬B
[¬B]1
⊥
B
1
¬A ∧B
(¬A ∧B) →¬C
¬C
⊥
A ∨¬B
2
(k) ¬(A ∨B)
⊢ND
¬A ∧¬B:
¬(A ∨B)
¬(A ∨B)
...
←−using 2(a) −→
...
¬A
¬B
¬A ∧¬B
(m) ¬A ∧¬B ⊢ND ¬(A ∨B):
¬A ∧¬B
¬A ∧¬B
[A]1
¬A
[B]1
¬B
[A ∨B]2
⊥
⊥
⊥
¬(A ∨B)
2
1
(o) A ∧¬B ⊢ND ¬(A →B):
A ∧¬B
A
[A →B]1
B
A ∧¬B
¬B
⊥
¬(A →B)
1

Answers and Solutions to Selected Exercises
285
2.4.6
.(a) .
[¬p]1, [¬p →r]3
[¬r]2
r
⊥1
p
p →¬q
[p]2 p →¬q
¬r ∨p
¬q
¬q
¬q
(¬p →r) →¬q
3
2
2.4.7
We formalize each of the propositional arguments by identifying the atomic propo-
sitions in them and replacing them with propositional variables. For a selection of
them, we then will prove the soundness of the resulting inference rule by deriving
it in ND.
(b) Denote
“Nina will go to a party” by p, and
“Nina will go to office” by q.
Then the argument becomes:
p ∨¬q, ¬p ∨¬q
¬q
.
The rule is derivable in ND and therefore sound, so the argument is correct.
¬q ∨p
[¬q]2
¬q
¬q ∨¬p[¬q]1
¬q
[¬p]1, [p]2
⊥
¬q
¬q
1
¬q
2
(d) Denote
“Socrates is happy” by p,
“Socrates is stupid” by q, and
“Socrates is a philosopher” by r.
Then the inference rule on which the argument is based is:
p ∨¬q, p →¬r
r →¬q
.
The rule is derivable in ND and therefore sound, so the argument is correct.
p ∨¬q
[p]1, p →¬r
¬r,
r]2
⊥
¬q
[¬q]1
¬q
¬q
r →¬q2
1

286
Logic as a Tool
(f) Denote
“Olivia is sleeping” by p,
“Olivia is eating” by q, and
“Olivia is smiling” by r.
The underlying inference rule:
¬r →¬p, ¬q ∨r
(¬q →p) →r .
Here is a derivation in ND:
¬q ∨p
[¬q]3, [¬q →p]4
p
[¬r]1, ¬r →¬p
¬p
⊥
r 1
[p]3 [¬r]2, ¬r →¬p
¬p
⊥
r 2
r
(¬q →p) →r 4
3
(h) Denote
“Bill smokes regularly” by p,
“Bill is a heavy smoker” by q,
“Bill will quit smoking” by r, and
“Bill will get lung cancer” by s.
The underlying inference rule:
p →q, ¬(p →r), q →(r ∨s)
s
.
The rule is valid, hence the inference is correct.
(j) Denote
“Kristina is a good student” by p,
“Kristina is clever” by q, and
“Kristina is lazy” by r.
Then the argument can be written as:
¬p →(¬q ∨r), ¬q ∨¬r
q →p
.
Derivation in ND:
q]2,
[¬p]1, ¬p →(¬q ∨r)
¬q ∨r
¬q ∨¬r
¬q
(∗)
⊥
p
1
q →p 2
(*) Here we use the derivation in Exercise 7b.

Answers and Solutions to Selected Exercises
287
Section 2.5
2.5.3
Hint: given a truth table of a formula, consider the conjunction of elementary
disjunctions obtained as follows. For every row where the formula is false take
the disjunction of all variables assigned value F in that row and all negations of
variables assigned value T in that row.
2.5.4
.(a) First method:
¬(p ↔q)
≡¬((p →q) ∧(q →p))
≡¬((¬p ∨q) ∧(¬q ∨p)
≡(p ∧¬q) ∨(q ∧¬p) (DNF)
≡((p ∧¬q) ∨q) ∧((p ∧¬q) ∨¬p)
≡(p ∨q) ∧⊤∧⊤∧(¬q ∨¬p)
≡(p ∨q) ∧(¬q ∨¬p) (CNF)
Second method:
p
q
¬
(p ↔q)
F
F
F
F T F
F
T
T
F F T
T
F
T
T F F
T
T
F
T T T
DNF: (¬p ∧q) ∨(p ∧¬q)
CNF: (¬p ∨¬q) ∧(p ∨q)
(c) First method:
(p ↔¬q) ↔r
≡((p ↔¬q) →r) ∧(r →(p ↔¬q))
≡(¬(p ↔¬q) ∨r) ∧(¬r ∨(p ↔¬q))
≡(¬((p →¬q) ∧(¬q →p)) ∨r) ∧(¬r ∨((p →¬q) ∧(¬q →p)))
≡(¬((¬p ∨¬q) ∧(¬¬q ∨p)) ∨r) ∧(¬r ∨((¬p ∨¬q) ∧(¬¬q ∨p)))
≡((p ∧q) ∨(¬q ∧¬p) ∨r) ∧(¬r ∨(¬p ∧q) ∨(¬p ∧p) ∨(¬q ∧q) ∨(¬q ∧p))
≡(p ∧q ∧¬r) ∨(p ∧q ∧¬p ∧q) ∨(p ∧q ∧¬q ∧p) ∨(¬q ∧¬p ∧¬r)∨
(¬q ∧¬p∧¬p∧q) ∨(¬q ∧¬p∧¬q ∧p) ∨(r ∧¬r) ∨(r ∧¬p∧q) ∨(r ∧¬q ∧p)
≡(p ∧q ∧¬r) ∨(¬q ∧¬p ∧¬r) ∨(r ∧¬p ∧q) ∨(r ∧¬q ∧p)
(DNF)
≡(q ∧((p ∧¬r) ∨(r ∧¬p))) ∨(¬q ∧((¬p ∧¬r) ∨(r ∧p)))
≡(q ∧(p ∨r) ∧(p ∨¬p) ∧(¬r ∨r) ∧(¬r ∨¬p))∨
(¬q ∧(¬p ∨r) ∧(¬p ∨p) ∧(¬r ∨r) ∧(¬r ∨p))
≡(q ∧(p ∨r) ∧(¬r ∨¬p)) ∨(¬q ∧(¬p ∨r) ∧(¬r ∨p))
≡(q ∨¬q) ∧(q ∨¬p ∨r) ∧(q ∨¬r ∨p) ∧(p ∨r ∨¬q)∧
(p ∨r ∨¬p ∨r) ∧(p ∨r ∨¬r ∨p) ∧(¬r ∨¬p ∨¬q)∧
(¬r ∨¬p ∨¬p ∨r) ∧(¬r ∨¬p ∨¬r ∨p)
≡(q ∨¬p ∨r) ∧(q ∨¬r ∨p) ∧(p ∨r ∨¬q) ∧(¬r ∨¬p ∨¬q)
(CNF)

288
Logic as a Tool
Second method:
p
q
r
(p ↔¬q)
↔
r
F
F
F
F F T
T
F
F
F
T
F F T
F
T
F
T
F
F T F
F
F
F
T
T
F T F
T
T
T
F
F
T T T
F
F
T
F
T
T T T
T
T
T
T
F
T F F
T
F
T
T
T
T F F
F
T
DNF: (¬p ∧¬q ∧¬r) ∨(¬p ∧q ∧r) ∨(p ∧¬q ∧r) ∨(p ∧q ∧¬r)
CNF: (p ∨q ∨¬r) ∧(p ∨¬q ∨r) ∧(¬p ∨q ∨r) ∧(¬p ∨¬q ∨¬r)
(e) First method:
(¬p ∧(¬q ↔p)) →((q ∧¬p) ∨p)
≡¬(¬p ∧(¬q ↔p)) ∨(q ∧¬p) ∨p
≡p ∨¬((¬q →p) ∧(p →¬q)) ∨(q ∧¬p) ∨p
≡p ∨¬((q ∨p) ∧(¬p ∨¬q)) ∨(q ∧¬p)
≡p ∨(¬q ∧¬p) ∨(p ∧q) ∨(q ∧¬p) (DNF)
≡((p ∨¬q) ∧(p ∨¬p)) ∨((p ∨q) ∧(p ∨¬p) ∧(q ∨q) ∧(q ∨¬p))
≡(p ∨¬q) ∨((p ∨q) ∧q ∧(q ∨¬p))
≡(p ∨¬q ∨p ∨q) ∧(p ∨¬q ∨q) ∧(p ∨¬q ∨q ∨¬p)
≡¬q ∨q (CNF)
Second method:
p
q
(¬p
∧
(¬q ↔p))
→
((q ∧¬p) ∨p)
F
F
T
F
T
F
F
T
F
F
T
F
F
F
T
T
T
F
T
F
T
T
T
T
T
F
T
F
F
F
T
T
T
T
F
F
F
T
T
T
T
F
T
F
F
T
T
T
F
F
T
T
DNF: (¬p ∧¬q) ∨(¬p ∧q) ∨(p ∧¬q) ∨(p ∧q)
CNF: p ∨¬p
2.5.5
.(a) First, we transform ¬(((p →q) →q) →q) into a CNF:
¬(((p →q) →q) →q)
≡((p →q) →q) ∧¬q
≡(¬(p →q) ∨q) ∧¬q
≡((p ∧¬q) ∨q) ∧¬q
≡(p ∨q) ∧(¬q ∨q) ∧¬q
≡(p ∨q) ∧¬q.

Answers and Solutions to Selected Exercises
289
Now, transform the result to clausal form:
C1 = {p, q} and C2 = {¬q}.
Applying Resolution, we get C3 = Res(C1, C2) = {p}.
No more clauses are derivable. Therefore, the empty clause cannot be
derived, so the formula is not a tautology.
(c) Transform ¬(((p →q) ∧(p →¬q)) →¬p) into a CNF:
¬(((p →q) ∧(p →¬q)) →¬p)
≡((p →q) ∧(p →¬q)) ∧p
≡(¬p ∨q) ∧(¬p ∨¬q) ∧p.
Now, transform the result to clausal form:
C1 = {¬p, q}, C2 = {¬p, ¬q}, and C3 = {p}.
Finally, applying Resolution successively, we get
C4 = Res(C1, C2) = {¬p}
C5 = Res(C3, C4) = {}.
The empty clause has been derived, therefore the formula is a tautology.
(e) Transform ¬(((p →q) ∧(p →r)) →(p →(q ∧r))) into a CNF:
¬(((p →q) ∧(p →r)) →(p →(q ∧r)))
≡((p →q) ∧(p →r)) ∧¬(p →(q ∧r))
≡(¬p ∨q) ∧(¬p ∨r) ∧p ∧(¬q ∨¬r).
Now, transform the result to clausal form:
C1 = {¬p, q}, C2 = {¬p, r}, C3 = {p}, and C4 = {¬q, ¬r}.
Finally, applying Resolution successively, we get
C5 = Res(C2, C4) = {¬p, ¬q}
C6 = Res(C1, C5) = {¬p}
C7 = Res(C3, C6) = {}.
The empty clause has been derived, therefore the formula is a tautology.
(g) First, transform ¬(((p →r) ∨(q →r)) →((p ∨q) →r)) into a CNF:
¬(((p →r) ∨(q →r)) →((p ∨q) →r))
≡((p →r) ∨(q →r)) ∧¬((p ∨q) →r)
≡(¬p ∨r ∨¬q ∨r) ∧(p ∨q) ∧¬r
≡(¬p ∨r ∨¬q) ∧¬r ∧(p ∨q).

290
Logic as a Tool
Now, transform the result to clausal form:
C1 = {¬p, ¬q, r}, C2 = {p, q}, C3 = {¬r}.
Now, applying Resolution successively:
C4 = Res(C1, C2) (on the pair ¬p, p) = {¬q, q, r}.
C5 = Res(C1, C2) (on the pair ¬q, q) = {¬p, p, r}.
C6 = Res(C1, C3) = {¬p, ¬q}.
C7 = Res(C2, C6) (on the pair ¬p, p) = {¬q, q}.
C8 = Res(C2, C6) (on the pair ¬q, q) = {¬p, p}.
C9 = Res(C4, C2) = Res(C5, C2) = {p, q, r}.
At this stage, no new clauses can be obtained by applying Resolution again
and the empty clause has not been derived; the formula is therefore not a
tautology.
(i) First, transform ¬(p →((q →r) →((p →q) →r))) into a CNF:
¬(p →((q →r) →((p →q) →r)))
≡p ∧¬((q →r) →((p →q) →r))
≡p ∧(q →r) ∧¬((p →q) →r)
≡p ∧(¬q ∨r) ∧(p →q) ∧¬r
≡p ∧(¬q ∨r) ∧(¬p ∨q) ∧¬r.
Now, transform the result to clausal form:
C1 = {p}, C2 = {¬q, r}, C3 = {¬p, q}, and C4 = {¬r}.
Applying Resolution successively, we get
C5 = Res(C1, C3) = {q}
C6 = Res(C2, C4) = {¬q}
C7 = Res(C5, C6) = {}.
The empty clause has been derived, so the formula is a tautology.
2.5.6
.(a) Transform the formulae to clausal form:
C1 = {p, q}, C2 = {p, ¬q}, and C3 = {¬p}.
Applying Resolution successively, we get
C4 = Res(C1, C2) = {p}
C5 = Res(C3, C4) = {}.
The empty clause has been derived, so the consequence holds.

Answers and Solutions to Selected Exercises
291
(c) Transform the formulae to clausal form:
C1 = {¬p, r}, C2 = {¬q, r}, C3 = {p, q}, and C4 = {¬r}.
Applying Resolution successively, we get
C5 = Res(C2, C3) = {p, r}
C6 = Res(C1, C5) = {r}
C7 = Res(C4, C6) = {}.
The empty clause has been derived, so the consequence holds.
(e) Transform the formulae to clausal form:
C1 = {¬p, q}, C2 = {p, ¬r}, C3 = {¬r}, and C4 = {q}.
Finally, applying Resolution, we get
C5 = Res(C1, C2) = {q, ¬r}.
No more clauses are derivable. The empty clause cannot be derived, so the
consequence does not hold.
(g) Transform the formulae to clausal form:
C1 = {¬p, q}, C2 = {r, s}, C3 = {¬q, r}, C4 = {¬r}, and C5 = {p}.
Applying Resolution successively, we get
C6 = Res(C1, C5) = {q}
C7 = Res(C3, C4) = {¬q}
C8 = Res(C6, C7) = {}.
The empty clause has been derived, so the consequence holds.
2.5.7
.(b) Denote
“Nina will wear a red dress at the dinner” by p,
“Nina will wear high heels at the dinner” by q, and
“Nina will wear a silk scarf at the dinner” by r.
The argument is then formalized as
p ∨¬r, p →q, ¬p
q ∧¬r
.
We now transform the set {p ∨¬r, p →q, ¬p, ¬(q ∧¬r)} to clausal form:
C1 = {¬p, q}, C2 = {p, ¬r}, C3 = {¬p}, C4 = {¬q, r}.
Now, applying Resolution:
C5 = Res(C2, C3) = {¬r}
C6 = Res(C4, C5) = {¬q}

292
Logic as a Tool
C7 = Res(C1, C6) = {¬p}
C8 = Res(C1, C2) = {q, ¬r}
C9 = Res(C4, C8)(resolving on the pair{r, ¬r}) = {q, ¬q}
C10 = Res(C4, C8)(resolving on the pair{q, ¬q}) = {r, ¬r}.
At this stage, no new clauses can be obtained by applying Resolution and the
empty clause has not been derived; the argument is therefore not logically
correct.
(d) Denote
“The property prices increase” by p,
“The interest rates go down” by q, and
“The economy is doing well” by r.
The underlying inference rule:
q →p, q ∨¬r, ¬p
¬r
.
This inference rule is sound, so the argument is correct.
(f) Denote
p: Alice comes/will come to the party.
q: Bonnie comes/will come to the party.
r: Clyde comes/will come to the party.
The respective inference rule is:
q →¬p,
r →(p ∧¬q)
(q ∨¬r) →¬p
.
Transformation of the premises and the negated conclusion to clausal form:
q →¬p ≡¬q ∨¬p
C1 := {¬p, ¬q}
r →(p ∧¬q) ≡¬r ∨(p ∧¬q) ≡(¬r ∨p) ∧(¬r ∨¬q)
C2 := {p, ¬r}; C3 := {¬q, ¬r}
¬((q ∨¬r) →¬p) ≡(q ∨¬r) ∧p
C4 := {q, ¬r}; C5 := {p}.
Applying Resolution successively:
C6 := Res(C1, C5) = {¬q}
C7 := Res(C4, C6) = {¬r}.
No new clauses can be derived anymore. The empty clause is therefore not
derivable and the inference rule is not logically valid.
(h) Notation:
W: Hans does his job well.
C: Hans is clever.
P: Hans will be promoted.
F: Hans will be fired.
The argument in symbolic form is:
W ∧C →P,
¬W →F
C →P ∨F
.

Answers and Solutions to Selected Exercises
293
Transforming the premises and the negated conclusion to clausal form:
W ∧C →P ≡¬W ∨¬C¬F . C1 = {¬W, ¬C, F}
¬W →F ≡W ∨F C2 = {W, F};
¬(C →P ∨F) ≡C ∧¬P ∧¬F C3 := {C}, C4 := {¬P}, C5 := {¬F}.
Applying Resolution successively:
C6 = Res(C2, C5) = {W}
C7 = Res(C1, C6) = {¬C, P}
C8 = Res(C3, C7) = {P}
C9 = (C4, C8) = {}.
The empty clause has been derived; the inference rule is therefore sound and
the argument is logically correct.
Section 3.1
3.1.1
We can view any graph as the structure G = (V, R) in the language with one
binary relational symbol {E}. The domain of G is the set of vertices V , and E
is interpreted as the edge relation R.
We can view any group as the structure for the language LG with non-logical
symbols {·, ′, e}, where · is a binary functional symbol (multiplication), ′ is a
unary functional symbol (the inverse operation), and e is a constant symbol. The
domain of G is the set of group elements and the interpretations of ·, ′, e satisfy
the defining axioms of a group.
We can likewise view any ring as a structure for the language LR with non-logical
symbols {·, +, 0}, where · and + are both binary functional symbols (multipli-
cation and addition), and 0 is a constant symbol, such that their interpretations in
the set of ring elements satisfy the defining axioms of a ring.
A vector space over a field of scalars S can be formalized as a first-order structure
for the language LV with non-logical symbols {+, 0 } ∪{cs | s ∈S}, where +
is a binary functional symbol (vector addition), 0 is a constant symbol, and each
cs, where s ∈S is a unary functional symbol (multiplication by the scalar s).
The domain of a vector space is a set of vectors V and the interpretations of
+, {cs | s ∈S}, 0 in V satisfy the defining axioms of a vector space.
3.1.3
.(a) Yes; x × y.
(b) No: × is a binary functional symbol.
(c) No: parentheses and comma are missing.
(d) No: one right parenthesis is missing.
(e) Yes; ((x × (3 +y)) + ((3 +y) × x)).
(f) No, mismatch of arity: × is a binary functional symbol, while s is a unary
one.
(g) No: s is a name of a unary functional symbol, not a constant or variable.
(h) Yes; s((2 ×x) × (3 +y)).
(i) Yes; (s(2 ×x) × s(s(3) + y)).
(j) No, mismatch of arity: s is a unary functional symbol.

294
Logic as a Tool
3.1.4
.(a) Term: t = ×(x, y)
Parsing tree:
Subterms: {t, x, y}
×
x
y
(e)
Term:
Parsing tree:
t = +(×(x, +(3, y)), ×(+(3, y), x))
Subterms:
{x,
y,
0,
1,
2,
3,
+(3,y),
×(x, +(3,y)),
×(+(3,y), x),
+(×(x, +(3,y)), ×(+(3,y), x))}.
(h)
Term: s(×(×(2,x), +(3,y)))
Parsing tree:
Subterms:
{x,
y,
0,
1,
2,
3,
×(2,x),
+(3,y),
×(×(2,x),
+(3,y)),
s(×(×(2,x), +(3,y)))}.
3.1.6
.(b) No, a quantifier must be followed immediately by a variable x.
(d) No, = is an infix predicate symbol, so it may not be preceded by ¬.
(f) Yes.
(h) No, x is a variable, not a formula.
(j) Yes.
(l) Yes (though formally x < 0 should be in parentheses).
(n) No, there is no connective between y < y and ¬∀x(x < x).

Answers and Solutions to Selected Exercises
295
3.1.7
.(a)
Formula: A = ¬∀x(x = 0)
Parsing tree:
The set of subformulae:
{A,
∀x(x = 0),
x = 0 }
¬ (main connective)
∀x
x = 0
(f)
Formula: A = ¬∀x(¬x = 0).
The parsing tree:
The main connective is ¬.
The set of subformulae:
{x = 0,
¬(x = 0),
∀x(¬x = 0),
¬∀x(¬x = 0)}
x = 0
∀
¬
¬
(j)
Formula:
Parsing tree:
A = ∀x((x < 0) →∀x(x > 0))
Subformulae:
{A,
(x < 0) →∀x(x > 0),
∀x(x > 0),
x < 0,
x > 0 }
∀x (main connective)
→
x < 0
∀x
x > 0
(m)
Formula:
Parsing tree:
A = ∀y(x < x →∀x(x < x))
Subformulae:
{A,
x < x →∀x(x < x),
∀x(x < x), x < x}
∀y (main connective)
→
x < x
∀x
x < x

296
Logic as a Tool
3.1.10
.(a) A[¬(f(x) = y)/p, ∃x(x > y)/q)] =
(¬¬(f(x) = y) →(∃x(x > y) ∨¬(¬(f(x) = y) ∧∃x(x > y)))).
(b) A[¬(x = y)/p, ¬(x = y)/q)] =
(¬¬(x = y) →(¬(x = y) ∨¬(¬(x = y) ∧¬(x = y)))).
3.1.11
No, because (x = y) is substituted for the first occurrence of p and (y = x) is
substituted for the second occurrence of p. Note that (x = y) and (y = x) are
different formulae.
Section 3.2
3.2.1
.(a) 10 = 12: false.
(b) 4 > 4: false.
(c) The formula is x3 + 1 = (x + 1)(x2 −x + 1).
This is an algebraic identity, so it is true for every value of x.
3.2.4
Some answers and hints:
(a) z > y →x > y: false
(c) ∃y(z > y →x > y): true, take y = z
(e) ∀y(z > y →x > y): false, take y = x
(g) ∃x∀z(z > y →x > y): true, take x = y + 1
(i) ∃z∀x(z > y →x > y): true, take z = y
(k) ∃y∀z(z > y →x > y). yes, take y = 0
(m) ∀x∃z(z > y →x > y): true, take z = y
(o) ∀y∃z(z > y →x > y): true, take z = y
(q) ∀z∃y(z > y →x > y): true, take y = z
(s) ∃y∀x∀z(z > y →x > y): false
(u) ∀y∃x∀z(z > y →x > y): true
(w) ∀z∃x∀y(z > y →x > y): true
(y) ∀y∀z∃x(z > y →x > y): true, take x = y + 1.
3.2.5
.(a) Not every real number is different from 0. True.
(c) False, take x = 0.
(e) There exists a real number which, if that number equals its square, then it is
negative. True: take any x such that x ̸= x2. Then the antecedent is false so
the implication is true.
(g) True.
(i) For every pair of real numbers x and y, one of them is less than the other.
False: take x = y.
(k) True, for every x take for example y = 1.
(m) For every real x there is a real y such that if x is greater than y then it is also
greater than the square of y. True: given x take y = x.
(p) There is a real number x which can be added to any real number y to obtain
x again. False.
(q) There is a real number x which can be added to any real number y to obtain
y again. True: take x = 0.

Answers and Solutions to Selected Exercises
297
(r) There is a real number x such that, for every real number y, x is greater than
y or −x is greater than y. False.
(t) There is a real x such that every real y greater than x will have a square also
greater than x. True: take for example x = 1.
(w) For any real number x, there is some real number y so that xy will equal yz
for all real numbers z. True: given x, take y = 0.
(y) Between any two distinct real numbers there lies another real number. True:
density of R.
3.2.6
.(b) No natural number is greater than 6 or not greater than 5.
False: for example, 0 falsifies the claim.
(d) Every natural number that is not greater than 4 is not greater than 3.
False: take x = 4.
(f) Every natural number that is not 1 or 31 is not a divisor of 31.
True: 31 is a prime number.
(h) For every natural number x there is a greater number which is less
than or equal to any natural number greater than x.
True: every natural number has an immediate successor.
(k) For every natural number x there is a natural number z that is greater
than x and less than every natural number greater than x.
False: take any x. Then whatever z such that x < z is chosen, take y = z.
(l) If for every natural number x, the property P holds for x whenever
it holds for all natural numbers less than x, then P holds for every
natural number.
True: this is the principle of (complete) mathematical induction.
(n) Every non-empty set of natural numbers (the interpretation of the
predicate P) has a least element.
True. Suppose that for some interpretation of P in some set X ⊆N it is the
case that X has no least element. It can then be proven by induction that no
natural number belongs to X, and therefore X must be empty.
In fact, the property above is equivalent to the principle of mathematical
induction.
3.2.7
.(a) “Every father knows all of his children.” False.
(c) “There is no woman who does not know any of her children.”
False: there are women who have no children. They (vacuously) do not know
any of their children.
(e) “Every man is the father of every child of his.” True.
Section 3.3
3.3.1
.(b) ∀x(¬I(x) →x2 > 0)
(d) ∀x(I(x) →∃y(I(y) ∧(x = y + y ∨x = y + y + 1)))
(f) ∀x(I(x) →∃y(I(y) ∧x < y))

298
Logic as a Tool
(h) ¬∀x∃y(I(y) ∧x > y)
(j) ¬∃x∀y(I(y) →x > y)
(l) ∃x∀y(xy = y)
3.3.2
.(a) ∀xT(x) ∨∀xL(x)
(c) T(John) →∀xL(x)
(e) ∀x(T(x) →∀y(¬y = x →L(y)))
3.3.3
.(a) ¬∀x∃yT2(x, y)
(c) ∀x(∃yT2(x, y) →L2(John, x))
(e) ∀x(¬L2(x, John) →T2(John, x))
(g) ∀x∀y(¬L2(y, x) →¬L2(x, y))
(i) ∀x∃yT2(x, y) →¬∃x∃yL2(x, y)
3.3.4
.(a) ∃x(M(x) ∧∀y(W(y) →L(x, y)))
(c) ∀x(M(x) →∃y(W(y) ∧L(x, y) ∧¬L(y, x)))
(e) ∃x(M(x) ∧∀y(W(y) →(L(x, y) →¬L(y, x))))
≡∃x(M(x) ∧∀y((W(y) ∧L(x, y)) →¬L(y, x))).
(g) ∀x(W(x) →∀y(C(y, x) →L(x, y)))
(i) Two possible readings:
“Everyone loves any child”: ∀x∀y(∃zC(y, z) →L(x, y))
or
“Everyone loves some child”: ∀x∃y(∃zC(y, z) ∧L(x, y))
(k) ∀x(∃yC(x, y) →∃zL(z, x))
(m) ∀x(∃yC(x, y) →(L(x, m(x)) ∨L(x, f(x))))
(o) ¬∃x(∃yC(x, y) ∧∃z(∃u(z = m(u) ∧¬L(z, u)) ∧L(x, z)))
3.3.5
.(a) ∀x(∃yC(y, x) →∀z(∃u(m(u) = z) →R(z, x)))
(c) ∃x∀y(C(x, y) →¬K(x, y))
(e) ∃x∀y(C(x, y) →¬R(x, y))
or
∃x(∃yC(x, y) ∧∀y(C(x, y) →
¬R(x, y)))
(g) ¬∃x(W (x) ∧∃y(M(y) ∧L(x, y) ∧∃z(C(z, x) ∧¬K(y, z))))
(i) ¬∃x(M(x) ∧∃y(W (y) ∧R(x, y) ∧∃z(C(z, y) ∧¬K(y, z))))
(k) ∀x(∃yC(x, y) →∀z(∃u(m(u) = z) →L(z, x)))
(l) Either ¬∃x(K(x, Eve) ∧¬x = Adam),
which is equivalent to ∀x(K(x, Eve) →x = Adam),
or K(Adam,Eve) ∧∀x(K(x, Eve) →x = Adam)
(m) ∀x(∀y((K(x, y) ∧W(y)) →L(x, y)) →x = Adam)
(o) L(m(John), John) ∧∀x(L(x, John) →x = m(John))
(p) L(m(John), John) ∧∃x(L(x, John) ∧¬x = m(John))
3.3.6
.(a) ∀x((Q(x) ∧f(x) = 0) →0 < x)
(c) ¬∃x(x < 0 ∧∀y(f(y) = 0 →y < x))
(e) This sentence is possibly ambiguous. There are two non-equivalent transla-
tions:
∃x(x < 0 ∧f(x) = 0 ∧¬∀y((f(y) = 0 ∧¬Q(y)) →x < y)) and
∃x(x < 0 ∧f(x) = 0 ∧∀y((f(y) = 0 ∧¬Q(y)) →¬x < y)).

Answers and Solutions to Selected Exercises
299
3.3.7
.(a) x|y := ¬x = 0 ∧∃z(x × z = y)
(b) P(x) := 1 < x ∧∀y∀z(y × z = x →(y = 1 ∧z = x) ∨(y = x ∧z =
1))
(d) ∀x(2 < x ∧∃y(2 ×y = x) →∃z∃v(P(z) ∧P (v) ∧z + v = x))
(f) ∀x∃y(P(y) ∧¬(x < y × y) ∧∃u(u × y = x))
(h) z|x ∧z|y ∧∀u((u|x ∧u|y) →u ≤z)
3.3.10
.(a) The only free occurrence of a variable is the first occurrence of y.
The scopes of the respective occurrences of quantifiers:
∃x∀z(Q(z, y) ∨¬∀y(Q(y, z) →P(x))),
∀z(Q(z, y) ∨¬∀y(Q(y, z) →P(x))),
∀y(Q(y, z) →P(x)).
(d) The free occurrence of variables are: the first occurrence of y, the last occur-
rence of z, and the last occurrence of x.
The scopes of the respective occurrences of quantifiers:
∃x(∀zQ(z, y) ∨¬∀yQ(y, z)), ∀zQ(z, y),
∀yQ(y, z).
(g) The free occurrence of variables are: all occurrences of y.
The scopes of the respective occurrences of quantifiers:
∃x(∀xQ(x, y) ∨¬∀z(Q(y, z) →P (x))),
∀xQ(x, y),
∀z(Q(y, z) →P(x)).
3.3.11
.(a) ∃x∀z(Q(z, y) ∨¬∀w(Q(w, z) →P(x)))
(c) ∃x(∀zQ(z, y) ∨¬∀w(Q(y, w) →P(x)))
(e) ∃u∀z(Q(z, y) ∨¬∀wQ(u, z)) →P (x)
(g) ∃x(∀zQ(z, y) ∨¬(∀wQ(y, w) →P(x)))
(i) ∃u(∀y(Q(u, y) ∨¬∀vQ(v, z))) →P(x)
3.3.13
.(a) No.
(c) Yes; ∃x(∀zP(f(y)) ∨¬∀y(Q(y, z) →P(x))).
(f) Yes; ∀y(¬(∀x∃z(¬P(z) ∧∃yQ(z, x))) ∧(¬∀xQ(x, y) ∨P(f(x)))).
(g) No.
(h) Yes; (∀y∃z¬P(z)∧∀xQ(g(f(z), y), x))→(¬∃yQ(x, y)∨P (g(f(z), y))).
(i) No.
Section 3.4
3.4.1
.(a) ∀x(P(x) ∨¬P (x)): Yes.
(c) ∀xP(x) ∨¬∀xP(x): Yes.
(e) ∃xP(x) →∀xP(x): No.
(g) ∃x(P (x) →∀yP(y)): Yes.
Here is a short intuitive argument: either every element of the domain satisfies
P or not. In the latter case, the implication P(x) →∀yP(y) can be made
true for a suitable x which falsifies the antecedent. In the former case, the
implication P(x) →∀yP(y) is true for any x because the consequent is true.
Hence, the validity.

300
Logic as a Tool
Now, to prove this validity by formal semantic argument, take any structure
S for the first-order language introduced above and any variable assignment
v in S. (We know that the variable assignment is irrelevant here because the
formula is a sentence, but we need it in order to process the quantifiers.)
We have to show that:
S, v |= ∃x(P(x) →∀yP(y)).
Consider two cases:
(i) There is a variable assignment v′ obtained from v by redefining it on x
such that P(x) is false. Then:
(1) S, v′ ⊭P(x),
but by the definition of →it follows that:
(2) S, v′ |= P(x) →∀yP(y).
According to the truth definition of ∃, it follows from (2) that
(3) S, v |= ∃x(P(x) →∀yP(y)).
(ii) For every variable assignment v:
(7) v, S, v |= P(x).
Then, clearly also:
(8) S, v |= P(y) for every variable assignment v.
According to the truth definition of ∀, it follows from (8) that
(9)S, v |= ∀yP(y),
but by the definition of →it follows from (7) and (9) that:
(10) S, v |= P(x) →∀yP(y).
From (10) it follows from the truth definition of ∃that
(11) S, v |= ∃x(P(x) →∀yP(y)).
Thus, in either case S, v |= ∃x(P(x) →∀yP(y)), which concludes the
proof.
(i) Yes.
(k) Yes.
(m) No.
(o) Yes.
(q) No. Consider for example the set N where P(x, y) is interpreted as
|x −y| = 1.
(r) No. Consider for example the set N where P(x, y) is interpreted as x < y.
3.4.6
.(a) Suppose ∀xA(x) ⊭¬∃x¬A(x). Then there is a structure S and an assign-
ment v such that S, v |= ∀xA(x) but S, v ⊭¬∃x¬A(x). S, v |= ∀xA(x)
implies S, v[x := s] |= A(x) for all s ∈S. From S, v ⊭¬∃x¬A(x), we
have S, v |= ∃x¬A(x), which means that there is an s′ ∈S such that
S, v[x := s′] |= ¬A(x). Hence, S, v[x := s′] ⊭A(x), contradicting the
fact that S, v[x := s] |= A(x) for all s ∈S.
(b) Likewise.
(c) Suppose ∃xA(x) ⊭¬∀x¬A(x). Then there is a structure S and an
assignment v such that S, v |= ∃xA(x) but S, v ⊭¬∀x¬A(x). From
S, v |= ∃xA(x) it follows that there is s′ ∈S such that S, v[x := s′] |=
A(x). Besides, S, v ⊭¬∀x¬A(x) implies that S, v |= ∀x¬A(x). Hence,
S, v[x := s] |= ¬A(x) for all s ∈S, and so S, v[x := s] ⊭A(x) for all
s ∈S, contradicting the fact that S, v[x := s′] |= A(x).

Answers and Solutions to Selected Exercises
301
(d) Likewise.
(e) Suppose
∃x∃yA(x, y) ⊭
∃y∃xA(x, y).
Then
there
is
a
structure
S and an assignment v such that S, v |= ∃x∃yA(x, y) but S, v ⊭
∃y∃xA(x, y). S, v |= ∃x∃yA(x, y) implies that there are s′, s′′ ∈S
such that S, v[x := s′][y := s′′] |= A(x, y). From S, v ⊭∃y∃xA(x, y),
we have S, v[x := s][y := t] ⊭A(x, y) for all s, t ∈S. This means that
S, v[x := s′][y := s′′] ⊭A(x, y), which is a contradiction.
3.4.7
Hint: note the equivalences in Exercise 10 and also use the fact that if x does not
occur free in B, then B ≡∀xB ≡∃xB. Here we prove the case 7f.
(f) ∃xA(x) →B |= ∀x(A(x) →B)
To prove this logical consequence, consider any structure S and a variable
assignment v, such that
(1) S, v |= ∃xA(x) →B.
Now, suppose that
(2) S, v ⊭∀x(A(x) →B).
By the truth definition of ∀, there is a variable assignment v′ which possibly
only differs from v in the value for x, such that
(2) S, v′ ⊭A(x) →B.
Then, by the truth definition of →it follows that
(3) S, v′ |= A(x), and
(4) S, v′ ⊭B.
Since x does not occur free in B, it then follows that
(4) S, v ⊭B.
On the other hand, by (3) and the truth definition of ∃, it follows that
(5) S, v |= ∃xA(x).
From (4) and (5) it follows that
S, v ⊭∃xA(x) →B,
which contradicts (1).
Therefore, the assumption (2) must be wrong, hence
S, v |= ∀x(A(x) →B).
This proves the logical consequence ∃xA(x) →B |= ∀x(A(x) →B).
3.4.8
.(a) Yes. For the sake of a contradiction, suppose ∀xA(x), ∀xB(x) ⊭∀x(A(x) ∧
B(x)). Then there exist a structure S and an assignment v in S such that S, v
|= ∀xA(x) and S, v |= ∀xB(x) but S, v ⊭∀x(A(x) ∧B(x)). We therefore
have that S, v[x := s] |= A(x) and S, v[x := s] |= B(x) for all s ∈S, while
S, v[x := c] ⊭A(x) ∧B(x) for some c ∈S. Hence, S, v[x := c] ⊭A(x) or
S, v[x := c] ⊭B(x) for some c ∈S, which contradicts the fact that A(x)
and B(x) are true for all s in S.
(c) Yes. For the sake of a contradiction, suppose ∀xA(x) ∨∀xB(x) ⊭
∀x(A(x) ∨B(x)). Then there exist a structure S and an assignment v in
S such that S, v |= ∀xA(x) ∨∀xB(x) but S, v ⊭∀x(A(x) ∨B(x)). We
therefore have that S, v[x := s] |= A(x) or S, v[x := s] |= B(x) for all s
∈S, while S, v[x := c] ⊭A(x) and S, v[x := c] ⊭B(x) for some c ∈S,
contradicting the fact that A(x) or B(x) is true for all s in S.
(e) No. Consider for example the structure H with A and B taken respectively
as the predicates M and W.

302
Logic as a Tool
(g) Yes.
For
the
sake
of
a
contradiction,
suppose
∃x(A(x) ∧B(x))
⊭∃xA(x) ∧∃xB(x). Then there exist a structure S and an assignment v
in S such that S, v |= ∃x(A(x) ∧B(x)), while S, v ⊭∃xA(x) ∧∃xB(x).
Hence, S, v ⊭∃xA(x) or S, v ⊭∃xB(x). This means S, v[x := s] ⊭A(x)
for all s or S, v[x := s] ⊭B(x) for all s, while S, v[x := c] |= A(x) and
S, v[x := c] |= B(x) for some c ∈S, which is a contradiction.
(i) Yes.
For
the
sake
of
a
contradiction,
suppose
∃x(A(x) ∨B(x))
⊭∃xA(x) ∨∃xB(x). Then there exist a structure S and an assignment v
in S such that S, v |= ∃x(A(x) ∨B(x)), while S, v ⊭∃xA(x) ∨∃xB(x).
Hence, S, v ⊭∃xA(x) and S, v ⊭∃xB(x). This means that there is
some c ∈S such that S, v[x := c] |= A(x) or S, v[x := c] |= B(x), while
S, v[x := s] ⊭A(x) and S, v[x := s] ⊭B(x) for all s ∈S, which is a
contradiction.
(k) No. We will show that ∃x(A(x) →B(x)) ⊭∃xA(x) →∃xB(x). It is suf-
ficient to find any counter-model, that is, a structure S and a variable assign-
ment v such that:
S, v |= ∃x(A(x) →B(x)) and S, v ⊭∃xA(x) →∃xB(x).
Take for example the structure S with domain the set Z of all integers, with
an interpretation of the unary predicate A to be {0} and interpretation of the
unary predicate B to be ∅. (We know that the variable assignment is irrelevant
here because all formulae involved in the logical consequence are sentences,
but we will nevertheless need it in order to process the truth definitions of the
quantifiers.)
Now let v1 be a variable assignment obtained from v by redefining it on x as
follows: v′(x) := 1. Then:
(1) S, v1 ⊭A(x).
(2) S, v1 ⊭B(x).
According to the truth definition of →it follows from (1) and (2) that:
(3) S, v1 |= A(x) →B(x).
According to the truth definition of ∃it therefore follows from (3) that:
(4) S, v |= ∃x(A(x) →B(x)).
Now let v2 be a variable assignment obtained from v by redefining it on x as
follows: v2(x) := 0. Then:
(5) S, v2 |= A(x).
From (5) it follows according to the truth definition of ∃that:
(6) S, v |= ∃xA(x).
It follows directly from the definition of B and the truth definition of ∃that:
(7) S, v ⊭∃xB(x).
According to the truth definition of →it follows that:
(8) S, v ⊭∃xA(x) →∃xB(x).
S is therefore a counter-model, falsifying the logical consequence
∃x(A(x) →B(x)) |= ∃xA(x) →∃xB(x).
3.4.12
.(a) We show that each side logically implies the other.
First, to show ∀xA(x) |= ¬∃x¬A(x), suppose the contrary. Then there is
a structure S and an assignment v such that S, v |= ∀xA(x) but S, v ⊭

Answers and Solutions to Selected Exercises
303
¬∃x¬A(x). S, v |= ∀xA(x) implies S, v[x := s] |= A(x) for all s ∈S.
From S, v ⊭¬∃x¬A(x), we have S, v |= ∃x¬A(x), which means that there
is an s′ ∈S such that S, v[x := s′] |= ¬A(x). Hence, S, v[x := s′] ⊭A(x),
contradicting the fact that S, v[x := s] |= A(x) for all s ∈S.
Likewise, ¬∃x¬A(x) |= ∀xA(x).
(c) Again, we show that each side logically implies the other.
Suppose ∃xA(x) ⊭¬∀x¬A(x). Then there is a structure S
and
an assignment v such that S, v |= ∃xA(x) but S, v ⊭¬∀x¬A(x).
From
S, v |= ∃xA(x)
it
follows
that
there
is
s′ ∈S
such
that
S, v[x := s′] |= A(x).
Besides,
S, v ⊭
¬∀x¬A(x)
implies
that
S, v |= ∀x¬A(x). Hence, S, v[x := s] |= ¬A(x) for all s ∈S, and
so
S, v[x := s] ⊭A(x)
for
all
s ∈S,
contradicting
the
fact
that
S, v[x := s′] |= A(x).
Likewise, ¬∀x¬A(x) |= ∃xA(x).
(h) Again, we show that each side logically implies the other.
Suppose first that ∃x∃yA(x, y) ⊭∃y∃xA(x, y). Then there is a struc-
ture S and an assignment v such that S, v |= ∃x∃yA(x, y) but S, v ⊭
∃y∃xA(x, y). S, v |= ∃x∃yA(x, y) implies that there are s′, s′′ ∈S
such that S, v[x := s′][y := s′′] |= A(x, y). From S, v ⊭∃y∃xA(x, y),
we have S, v[x := s][y := t] ⊭A(x, y) for all s, t ∈S. This means that
S, v[x := s′][y := s′′] ⊭A(x, y), which is a contradiction.
Likewise, ∃y∃xA(x, y) |= ∃x∃yA(x, y).
3.4.13
All equivalences follow from the fact that if x does not occur free in Q, then
S, v |= Q iff S, v′ |= Q for every x-variant v′ of v iff S, v |= ∀xQ.
3.4.14
.(a) No. Consider for example the structure H with P and Q interpreted as the
predicates M and W, respectively.
(c) Yes.
(e) No. Consider for example the structure N with Q interpreted as the predicate
<.
(g) No. Consider for example the structure Z with Q(x, y) interpreted as “x ≤
y.”
(i) Yes. The two formulae are essentially contrapositives to each other.
3.4.15
.(b) .¬∀x((x = x2 ∧x > 1) →x2 < 1)
≡¬∀x(¬(x = x2 ∧x > 1) ∨x2 < 1)
≡∃x(x = x2 ∧x > 1 ∧¬x2 < 1).
(d) .¬∀x(x = 0 ∨∃y¬(xy = x))
≡∃x(¬x = 0 ∧¬∃y¬(xy = x))
≡∃x(¬x = 0 ∧∀y(xy = x)).
(f) .¬∃x∃y(x > y ∨−x > y)
≡∀x∀y(¬x > y ∧¬(−x > y)).

304
Logic as a Tool
(h) .¬(∀x(P(x) →Q(x)) →(∀xP(x) →∀xQ(x)))
≡∀x(P(x) →Q(x)) ∧¬(¬∀xP(x) ∨∀xQ(x))
≡∀x(P(x) →Q(x)) ∧(∀xP(x) ∧∃x¬Q(x)).
3.4.16
Using predicates L(x) for “x is a lawyer” and G(x) for “x is greedy,” we can
formalize A and B as follows:
A = ¬∀x(L(x) →G(x)),
B = ∃xL(x) ∧¬∀xG(x).
First, we show thatA|= B. Suppose ¬∀x(L(x)→G(x)) ⊭∃xL(x) ∧¬∀xG(x).
Then there is a structure S
and an assignment v
on S
such that
S, v |= ¬∀x(L(x) →G(x)),
while
S, v ⊭∃xL(x) ∧¬∀xG(x).
But
then
S, v ⊭∀x(L(x) →G(x)),
while
S, v ⊭∃xL(x)
or
S, v |= ∀xG(x).
S, v ⊭∀x(L(x) →G(x))
implies
there
is
some
c
in
S
such
that
S, v[x := c] ⊭L(x) →G(x).
This
means
that
S, v[x := c] |= L(x)
but
S, v[x := c] ⊭G(x). Hence, S, v |= ∃xL(x) and S, v ⊭∀xG(x), which is a
contradiction.
On the other hand, B ⊭A. To see this, consider for instance the structure N with
L(x) taken to mean “x is divisible by 6” and G(x) to mean “x is divisible by 3.”
3.4.17
Using predicates H(x) for “x is happy” and D(x) for “x is drunk,” we can
formalize A and B as follows: A = ¬∃x(H(x) →D(x)) and B = ∃xH(x) ∧
∃x¬D(x).
Claims: A |= B and B ⊭A.
3.4.18
.(a) Using predicates P(x) for “x is a philosopher,” H(x) for “x is human,” and
M(x) for “x is mortal,” we can formalize the argument in first-order logic
as follows:
∀x(H(x) →M(x)), ∀x(P(x) →H(x))
∀x(P(x) →M(x))
.
The argument is correct. To see this, suppose ∀x(P(x) →H(x)), ∀x(H(x)
→M(x)) ⊭∀x(P (x) →M(x)). Then there is a structure S and an assign-
ment v on S such that S, v |= ∀x(P(x) →H(x)) and S, v |= ∀x(H(x) →
M(x)) but S, v ⊭∀x(P (x) →M(x)). Now, S, v ⊭∀x(P(x) →M(x))
implies that S, v[x := c] ⊭P(x) →M(x) for some c in S, and so
S, v[x := c] |= P(x) while S, v[x := c] ⊭M(x). On the other hand, since
S, v |= ∀x(P(x) →H(x)), S, v[x := c] |= P(x) →H(x), which means
S, v[x := c] ⊭P(x) or S, v[x := c] |= H(x). If S, v[x := c] ⊭P(x), we
have a contradiction with S, v[x := c] |= P(x). If S, v[x := c] |= H(x),
note that S, v |= ∀x(H(x) →M(x)) implies S, v[x := c] ⊭H(x) or
S, v[x := c] |= M(x), again a contradiction.

Answers and Solutions to Selected Exercises
305
(c) Using predicates I(x) for “x is an integer,” Q(x) for “x is a rational,” and
N(x) for “x is negative,” we can formalize the argument as follows:
∃x(N(x) ∧Q(x)), ∀x(I(x) →Q(x))
∃x(I(x) ∧N(x))
.
This argument is not logically valid. To see this, consider for instance the
structure of integers Z and take I(x) to mean “x is positive,” Q(x) to mean
“x is an integer” (i.e., always true in Z), and N(x) to mean “x is negative.”
(e) Not valid.
(g) Using predicates P(x) for “x is a penguin,” B(x) for “x is a bird,” and W(x)
for “x is white,” we can formalize the argument as follows:
∀x(P(x) →B(x)), ∃x(P (x) ∧W (x))
∃x(B(x) ∧W(x) ∧P(x))
.
The argument is correct. To see this, suppose ∀x(P(x) →B(x)), ∃x(P(x) ∧
W (x)) ⊭∃x(B(x) ∧W(x) ∧P(x)).
Then
there
is
some
structure
S and an assignment on S such that S, v |= ∀x(P(x) →B(x)) and
S, v |= ∃x(P (x) ∧W (x)) but S, v ⊭∃x(B(x) ∧W(x) ∧P(x)). Now,
S, v |= ∃x(P (x) ∧W (x))
implies
that
S, v[x := c] |= P(x) ∧W(x)
for some c in S. Hence, S, v[x := c] |= P(x) and S, v[x := c] |= W(x).
But since S, v |= ∀x(P(x) →B(x)), S, v[x := c] ⊭P(x) or S, v[x :=
c] |= B(x). Furthermore, S, v ⊭∃x(B(x) ∧W(x) ∧P(x)) implies that
S, v[x := c] ⊭B(x) or S, v[x := c] ⊭W(x) or S, v[x := c] ⊭P(x). It is
not difficult to see that in each of these cases we get a contradiction.
(j) Using predicates P(x) for “x is a politician,” S(x) for “x is successful,” and
Q(x) for “x is poor,” we can formalize the argument as follows:
¬∃x(P (x) ∧S(x) ∧Q(x)), ∃x(Q(x) →P(x))
¬∃x(S(x) ∧Q(x))
.
The argument is not logically correct. For a counter-model consider for
instance the structure N and take P(x) to mean “x is negative,” S(x) to
mean “x is positive,” and Q(x) to mean “x is odd.”
(There are poor and successful philosophers, for instance.)
Section 3.5
3.5.2
.(a) The major term is “mortal,” the minor term is “philosopher,” and the middle
term is “human.” The first premise is the minor one and the second premise
is the major one. This syllogism is of type AAA-1.
(c) This syllogism is of type IAI-3.
(e) This syllogism is of type EIO-2.

306
Logic as a Tool
3.5.4
The nine syllogistic forms whose validity is conditional on the assumption of
existential import are:
Figure 1
Figure 2
Figure 3
Figure 4
(a) AAI-1 Barbari
(a) EAO-2 Cesaro
(a) AAI-3 Darapti
(a) AAI-4 Bramantip
(b) EAO-1 Celaront (b) AEO-2 Camestros (b) EAO-3 Felapton (b) EAO-4 Fesapo
(c) AEO-4 Camenop
Section 4.1
For this section I only provide a small selection of solutions. More can be found in some
of the references listed at the end of the section.
4.1.4
In the derivations below we will skip some purely propositional derivations. For
exercises on these, see Section 2.2.
(a) 1. ⊢H ∀xP(x) →P(y)
by (Ax∀2)
2. ⊢H ∀y(∀xP(x) →P(y))
by 1 and Generalization
3. ⊢H ∀y(∀xP(x) →P(y)) →(∀y∀xP(x) →∀yP(y))
by (Ax∀1)
4. ⊢H ∀y∀xP(x) →∀yP(y)
by 2, 3, and Modus Ponens
5. ⊢H ∀xP(x) →∀y∀xP(x)
by (Ax∀3)
6. ⊢H ∀xP(x) →∀yP(y) by 4, 5, Deduction Theorem, and Modus Ponens.
(c) 1. A(x) ⊢H ¬¬A(x)
by propositional derivation
(see Section 2.2.3, Exercise 9n)
2. ∀xA(x) ⊢H ∀x¬¬A(x)
by 1 and Generalization (Exercise 2)
3. ∀x¬¬A(x) ⊢H ¬¬∀x¬¬A(x)
by propositional derivation
(Section 2.2.3, Exercise 9n)
4. ∀xA(x) ⊢H ¬¬∀x¬¬A(x)
by 2, 3, and transitivity
(Section 2.2.3, Exercise 5)
that is, ∀xA(x) ⊢H ¬∃x¬A(x).
(e) 1. ∀yA(x, y) ⊢H A(x, y)
by (Ax∀2)
2. ∀x∀yA(x, y) ⊢H ∀xA(x, y)
by 1 and Generalization
(Section 4.1.4, Exercise 2)
3. ∀x∀yA(x, y) ⊢H ∀y∀xA(x, y)
by 2 and (Ax∀3).
4.1.5
(h) 1. ∀xP(x) ∧¬Q ⊢H ∀xP(x)
by Axiom (∧1)
2. ∀xP(x) ⊢H P(x)
by (Ax∀2)
3. ∀xP(x) ∧¬Q ⊢H P(x)
by 1, 2 , and transitivity
4. ∀xP(x) ∧¬Q ⊢H ¬Q
by Axiom (∧2)
5. ∀xP(x) ∧¬Q ⊢H P(x) ∧¬Q
by 3, 4, Axiom (∧2) and
Propositional Derivation
6. ∀xP(x) ∧¬Q ⊢H ∀x(P(x) ∧¬Q)
by 5 and (Ax∀3)
7. ¬∀x(P(x) ∧¬Q) ⊢H ¬(∀xP(x) ∧¬Q)
by 6 and contraposition
(Section 2.2.3, Exercise 9k)
8. ¬(∀xP(x) ∧¬Q) ⊢H ¬∀xP(x) ∨Q
by propositional derivation
(Section 2.2.3, Exercises 9u and 9m)

Answers and Solutions to Selected Exercises
307
9. ¬∀xP(x) ∨Q, ∀xP(x) ⊢H Q
by propositional derivation
(Section 2.2.3, Exercise 9q and Modus Ponens)
10. ¬∀x(P(x) ∧¬Q), ∀xP(x) ⊢H Q
by 7, 8, 9, and transitivity
11. P(x) ∧¬Q ⊢H ¬(P(x) →Q)
by propositional derivation
(using Section 2.2.3, Exercises 9r and 9t)
12. ∀x(P (x) ∧¬Q) ⊢H ∀x¬(P(x) →Q)
by 11 and Generalization
13. ¬∀x¬(P(x) →Q) ⊢H ¬∀x(P(x) ∧¬Q)
by 12 and contraposition
14. ¬∀x¬(P(x) →Q), ∀xP(x) ⊢H Q.
by 13, 10, and transitivity
that is, ∃x(P(x) →Q), ∀xP(x) ⊢H Q.
4.1.6
(p) 1. ⊢H ¬(P(x) →∀yP(y)) →(P(x) ∧¬∀yP(y))
propositional derivation
2. ⊢H ∀x(¬(P(x) →∀yP(y)) →(P(x) ∧¬∀yP(y)))
by 1 and
Generalization
3. ⊢H ∀x¬(P(x) →∀yP(y)) →∀x(P(x) ∧¬∀yP(y))
by 2, (Ax∀1), and
Modus Ponens
4. ⊢H (P(x) ∧¬∀yP(y)) →¬∀yP(y)
instance of a propositional axiom
5. ⊢H ∀x((P(x) ∧¬∀yP(y)) →¬∀yP(y))
by 4 and Generalization
6. ⊢H ∀x(P(x) ∧¬∀yP(y)) →∀x¬∀yP(y)
by 5, (Ax∀1), and
Modus Ponens
7. ⊢H ∀x(P(x) ∧¬∀yP(y)) →∀xP(x)
analogous to steps 4–6
8. ⊢H ∀x¬∀yP(y) →¬∀yP(y)
by (Ax∀2)
9. ⊢H ∀x(P(x) ∧¬∀yP(y)) →¬∀yP(y)
by 6, 8, and transitivity
10. ⊢H ∀xP(x) →∀yP(y)
by Exercise (1) above.
11. ⊢H ∀x(P(x) ∧¬∀yP(y)) →∀yP(y)
by 7, 10, and transitivity
12. ⊢H ¬∀x(P(x) ∧¬∀yP(y))
by 9, 11, and propositional derivation using
Section 2.2.3, Exercise 9q
13. ⊢H ¬∀x(P(x) ∧¬∀yP(y)) →¬∀x¬(P (x) →∀yP(y))
by
contraposition of 3
14. ⊢H ¬∀x¬(P(x) →∀yP(y))
by 12, 13, and Modus Ponens
that is, ⊢H ∃x(P (x) →∀yP(y)). Therefore, |= ∃x(P (x) →∀yP(y)).
4.1.9
(b) 1. x = f(x) ⊢H x = f(x)
2. ⊢H ∀x∀y(x = y →f(x) = f(y))
by (Axf)
3. ⊢H ∀y(x = y →f(x) = f(y))
by 2 and (Ax∀2)
4. ⊢H x = f(x) →f(x) = f(f(x))
by 3 and (Ax∀2)
5. x = f(x) ⊢H f(x) = f(f(x))
by 4 and Deduction Theorem
6. x = f(x) ⊢H x = f(f(x))
by 1,4, (Ax=3) and
Propositional Derivation
7. x = f(x) ⊢H f(f(x)) = x
by 6 and (Ax=2)
8. f(f(x)) = x ⊢H P(f(f(x))) = P(x)
by 7 and (Axr)
9. x = f(x) ⊢H P (f(f(x))) = P(x)
by 7, 8, and (Axr)
10. ⊢H x = f(x) →P (f(f(x))) = P(x)
by 9 and Deduction Theorem
11. ⊢H ∀x(x = f(x) →(P(f(f(x))) →P(x)))
by 10 and Generalization.

308
Logic as a Tool
Section 4.2
4.2.1
.(a) .
∀xP(x) →∀yP(y) : F
∀xP(x) : T1, ∀yP(y) : F
P(c) : F
P(c) : T
×
The tableau closes, hence |= ∀xP(x) →∀yP(y).
(d) .
¬∃x¬A(x) : T, ∀xA(x) : F
A(c) : F
∃x¬A(x) : F
¬A(c) : F
A(c) : T
×
The tableau closes, hence ¬∃x¬A(x) |= ∀xA(x).
(g) .
∃x∃yQ(x, y) : T, ∃y∃xQ(x, y) : F
∃yQ(c1, y) : T
Q(c1, c2) : T
∃xQ(x, c2) : F
Q(c1, c2) : F
×
The tableau closes, hence ∃x∃yQ(x, y) |= ∃y∃xQ(x, y).

Answers and Solutions to Selected Exercises
309
4.2.2
.(a) .
∀x(P(x) ∨Q) : T, ∀xP(x) ∨Q : F
∀xP(x) : F, Q : F
P(c) : F
P(c) ∨Q : T
P(c) : T
×
Q : T
×
The tableau closes, hence ∀x(P (x) ∨Q) |= ∀xP(x) ∨Q.
(d) .
∃xP(x) ∧Q : T, ∃x(P(x) ∧Q) : F
∃xP(x) : T, Q : T
P(c) : T
P(c) ∧Q : F
P(c) : F
×
Q : F
×
The tableau closes, hence ∃xP(x) ∧Q |= ∃x(P(x) ∧Q).
(f) .
∃xP(x) →Q : T, ∀x(P(x) →Q) : F
P(c) →Q : F
P(c) : T, Q : F
∃xP(x) : F
P(c) : F
×
Q : T
×
The tableau closes, hence ∃xP(x) →Q |= ∀x(P(x) →Q).

310
Logic as a Tool
(h) .
∃x(Q →P(x)) : T, Q : T, ∃xP(x) : F
Q →P(c) : T
P(c) : F
P(c) : T
×
Q : F
×
The tableau closes, hence ∃x(Q →P(x)), Q |= ∃xP(x).
4.2.3
.(a) .
∃x(P(x) →∀yP(y)) : F
P(d) →∀yP(y) : F
P(d) : T, ∀yP(y) : F
P(c) : F
P(c) →∀yP(y) : F
P(c) : T, ∀yP(y) : F
×
The tableau closes, hence |= ∃x(P(x) →∀yP(y)).

Answers and Solutions to Selected Exercises
311
(c) .
∀x∃yQ(x, y) →∀y∃xQ(x, y) : F
∀x∃yQ(x, y) : T, ∀y∃xQ(x, y) : F
∃xQ(x, c1) : F
Q(c1, c1) : F
∃yQ(c1, y) : T
Q(c1, c2) : T
Q(c2, c1) : F
∃yQ(c2, y) : T
Q(c2, c3) : T
...
⊭∀x∃yQ(x, y) →∀y∃xQ(x, y) since the tableau does not terminate.
Counter-model: consider the structure N with Q taken as the predicate <.
(e) ⊭∀x∃yQ(x, y) →∃y∀xQ(x, y) since the tableau does not terminate.
Counter-model: consider the structure N with Q taken as the predicate <.
(g) The tableau closes, hence the formula is valid.
(i) The tableau does not terminate, hence
⊭∀x∃yP(x, y) ∧∀x∀y(P(x, y) →P(y, x)) →∃xP(x, x).
4.2.4
.(b) .
∀x(A(x) ∧B(x)) : T, ∀xA(x) ∧∀xB(x) : F
∀xA(x) : F
A(c1) : F
A(c1) ∧B(c1) : T
A(c1) : T, B(c1) : T
×
∀xB(x) : F
B(c2) : F
A(c1) ∧B(c2) : T
A(c2) : T, B(c2) : T
×
The tableau closes, hence ∀x(A(x) ∧B(x)) |= ∀xA(x) ∧∀xB(x).

312
Logic as a Tool
(d) .
∀x(A(x) ∨B(x)) : T, ∀xA(x) ∨∀xB(x) : F
∀xA(x) : F, ∀xB(x) : F
A(c1) : F
B(c2) : F
A(c1) ∨B(c1) : T
A(c1) : T
×
B(c1) : T
A(c2) ∨B(c2) : T
A(c2) : T
⃝
B(c2) : T
×
The tableau does not close, hence ∀x(A(x) ∨B(x)) ⊭∀xA(x) ∨∀xB(x).
A counter-model extracted from the tableau: a structure with domain {a1, a2}
where c1 is interpreted as a1, c2 is interpreted as a2, A is interpreted as {a1},
and B is interpreted as {a2}.
(f) .
∀x(A(x) →B(x)) : T, ∀xA(x) →∀xB(x) : F
∀xA(x) : T, ∀xB(x) : F
B(c) : F
A(c) : T
A(c) →B(c) : T
A(c) : F
×
B(c) : T
×
The tableau closes, hence ∀x(A(x) →B(x)) |= ∀xA(x) →∀xB(x).

Answers and Solutions to Selected Exercises
313
(h) .
∃xA(x) : T, ∃xB(x) : T, ∃x(A(x) ∧B(x)) : F
A(c1) : T
B(c2) : T
A(c1) ∧B(c1) : F
A(c1) : F
×
×
⃝
B(c1) : F
A(c2) ∧B(c2) : F
A(c2) : F
B(c2) : F
The tableau does not close, hence ∃xA(x), ∃xB(x) ⊭∃x(A(x) ∧B(x)). A
counter-model extracted from the tableau: a structure with domain {a1, a2}
where c1 is interpreted as a1, c2 is interpreted as a2, A is interpreted as {a1},
and B is interpreted as {a2}.
(j) .
∃x(A(x) ∨B(x)) : T, ∃xA(x) ∨∃xB(x) : F
A(c) ∨B(c) : T
A(c) : T
∃xA(x) : F, ∃xB(x) : F
A(c) : F
×
B(c) : T
∃xA(x) : F, ∃xB(x) : F
B(c) : F
×
The tableau closes, hence ∃x(A(x) ∨B(x)) |= ∃xA(x) ∨∃xB(x).

314
Logic as a Tool
(l) .
∃x(A(x) →B(x)) : T, ∃xA(x) →∃xB(x) : F
∃xA(x) : T, ∃xB(x) : F
A(c1) : T
B(c1) : F
A(c2) →B(c2) : F
A(c2) : F
⃝
B(c2) : T
B(c2) : F
×
The tableau does not close, hence ∃x(A(x) →B(x)) ⊭∃xA(x) →∃xB(x).
A counter-model extracted from the tableau: a structure with domain {a1, a2}
where c1 is interpreted as a1, c2 is interpreted as a2, A is interpreted as {a1},
and B is interpreted as ∅.
4.2.5
.(a) Checking the validity of both consequences:
∀x(P(x) ∧Q(x)) |= ∀xP(x) ∧∀xQ(x):
∀x(P(x) ∧Q(x)) : T, ∀xP(x) ∧∀xQ(x) : F
∀xP(x) : F
P(c1) : F
P(c1) ∧Q(c1) : T
P(c1) : T, Q(c1) : T
×
∀xQ(x) : F
Q(c2) : F
P(c2) ∧Q(c2) : T
P(c2) : T, Q(c2) : T
×

Answers and Solutions to Selected Exercises
315
∀xP(x) ∧∀xQ(x) |= ∀x(P(x) ∧Q(x)):
∀xP(x) ∧∀xQ(x) : T, ∀x(P(x) ∧Q(x)) : F
P(c) ∧Q(c) : F
P(c) : F
∀xP(x) : T, ∀xQ(x) : T
P(c) : T
×
Q(c) : F
∀xP(x) : T, ∀xQ(x) : T
Q(c) : T
×
Hence, ∀x(P(x) ∧Q(x)) ≡∀xP(x) ∧∀xQ(x).
(c) ∀xP(x) →∀xQ(x) ̸≡∀x(P(x) →Q(x)) because
∀xP(x) →∀xQ(x) ⊭∀x(P(x) →Q(x)):
∀xP(x) →∀xQ(x) : T, ∀x(P(x) →Q(x)) : F
P(c1) →Q(c1) : F
P(c1) : T, Q(c1) : F
∀xP(x) : F
P(c2) : F
⃝
∀xQ(x) : T
Q(c1) : T
×
A counter-model extracted from the tableau: a structure with domain {a1, a2}
where c1 is interpreted as a1, c2 is interpreted as a2, P is interpreted as {a1}
and Q is interpreted as ∅.
(d) ∃xP(x) ∧∃xQ(x) ̸≡∃x(P(x) ∧Q(x))
because
∃xP(x) ∧∃xQ(x) ⊭
∃x(P(x) ∧Q(x)):

316
Logic as a Tool
∃xP(x) ∧∃xQ(x) : T, ∃x(P(x) ∧Q(x)) : F
∃xP(x) : T, ∃xQ(x) : T
P(c1) : T
Q(c2) : T
P(c1) ∧Q(c1) : F
P(c1) : F
×
Q(c1) : F
P(c2) ∧Q(c2) : F
P(c2) : F
⃝
Q(c2) : F
×
A counter-model extracted from the tableau: a structure with domain {a1, a2}
where c1 is interpreted as a1, c2 is interpreted as a2, P is interpreted as {a1}
and Q is interpreted as {a2}.
(f) Checking the validity of both consequences:
∃x(P(x) ∨Q(x)) : T, ∃xP(x) ∨∃xQ(x) : F
P(c) ∨Q(c) : T
P(c) : T
∃xP(x) : F, ∃xQ(x) : F
P(c) : F
×
Q(c) : T
∃xP(x) : F, ∃xQ(x) : F
Q(c) : F
×

Answers and Solutions to Selected Exercises
317
Therefore, ∃xP(x) ∨∃xQ(x) |= ∃x(P(x) ∨Q(x)):
∃xP(x) ∨∃xQ(x) : T, ∃x(P(x) ∨Q(x)) : F
∃xP(x) : T
P(c1) : T
P(c1) ∨Q(c1) : F
P(c1) : F, Q(c1) : F
×
∃xQ(x) : T
Q(c2) : T
P(c2) ∨Q(c2) : F
P(c2) : F, Q(c2) : F
×
Therefore, ∃x(P(x) ∨Q(x)) ≡∃xP(x) ∨∃xQ(x).
(h) ∃x∀yQ(x, y) ̸≡∃yQ(y, y) since ∃yQ(y, y) ⊭∃x∀yQ(x, y):
∃yQ(y, y) : T, ∃x∀yQ(x, y) : F
Q(c1, c1) : T
∀yQ(c1, y) : F
Q(c1, c2) : F
∃yQ(c2, y) : F
Q(c2, c3) : F
...
A counter-model: consider the structure N with Q taken as =.
(j) Checking the validity of ∀x∀yQ(x, y) ↔∀x∀yQ(y, x):

318
Logic as a Tool
∀x∀yQ(x, y) ↔∀x∀yQ(y, x) : F
(∀x∀yQ(x, y) →∀x∀yQ(y, x)) ∧(∀x∀yQ(y, x) →∀x∀yQ(x, y)) : F
∀x∀yQ(x, y) →∀x∀yQ(y, x) : F
∀x∀yQ(x, y) : T, ∀x∀yQ(y, x) : F
∀yQ(y, c1) : F
Q(c2, c1) : F
∀yQ(c2, y) : T
Q(c2, c1) : T
×
∀x∀yQ(y, x) →∀x∀yQ(x, y) : F
∀x∀yQ(y, x) : T, ∀x∀yQ(x, y) : F
∀yQ(c3, y) : F
Q(c3, c4) : F
∀yQ(y, c4) : T
Q(c3, c4) : T
×
The tableau closes, hence ∀x∀yQ(x, y) ≡∀x∀yQ(y, x).
4.2.6
.(a) Using predicates L(x) for “x is a lawyer” and G(x) for “x is greedy,” we can
formalize A and B as follows:
A : ¬∀x(L(x) →G(x)), and
B : ∃xL(x) ∧¬∀xG(x).
Checking the validity of both consequences:
The tableau for ¬∀x(L(x) →G(x)) |= ∃xL(x) ∧¬∀xG(x) closes, hence it
is valid.
However, ∃xL(x) ∧¬∀xG(x) ⊭¬∀x(L(x) →G(x)):

Answers and Solutions to Selected Exercises
319
∃xL(x) ∧¬∀xG(x) : T, ¬∀x(L(x) →G(x)) : F
∃xL(x) : T, ¬∀xG(x) : T
L(c1) : T
∀xG(x) : F
G(c2) : F
∀x(L(x) →G(x)) : T
L(c1) →G(c1) : T
L(c1) : F
×
G(c1) : T
L(c2) →G(c2) : T
L(c2) : F
⃝
G(c2) : T
×
A counter-model extracted from the tableau: a structure with domain {a1, a2}
where c1 is interpreted as a1, c2 is interpreted as a2, and both L and G are
interpreted as {a1}.
Therefore, A ̸≡B.
(c) Using predicates T(x) for “x is talking” and L(x) for “x is listening,” we can
formalize A and B as follows:
A : ∃xT(x) →∀xL(x), and
B : ∃x¬L(x) →¬∃xT (x).
Checking the validity of both logical consequences:

320
Logic as a Tool
∃xT(x) →∀xL(x) |= ∃x¬L(x) →¬∃xT(x):
∃xT(x) →∀xL(x) : T, ∃x¬L(x) →¬∃xT(x) : F
∃x¬L(x) : T, ¬∃xT(x) : F
¬L(c1) : T
L(c1) : F
∃xT(x) : T
T(c2) : T
∃xT(x) : F
T(c2) : F
×
∀xL(x) : T
L(c1) : T
×
∃x¬L(x) →¬∃xT(x) |= ∃xT (x) →∀xL(x):
∃x¬L(x) →¬∃xT(x) : T, ∃xT(x) →∀xL(x) : F
∃xT(x) : T, ∀xL(x) : F
T(c1) : T
L(c2) : F
∃x¬L(x) : F
¬L(c2) : F
L(c2) : T
×
¬∃xT(x) : T
∃xT(x) : F
T(c1) : F
×
Both tableaux close, hence A ≡B.

Answers and Solutions to Selected Exercises
321
4.2.7
.(b) Using predicates N(x) for “x is a natural number,” I(x) for “x is an integer,”
and O(x) for “x is odd,” we can formalize the argument as follows:
∀x(N(x) →I(x)), ∃x(I(x) ∧O(x))
∃x(N(x) ∧O(x))
.
The argument is not valid:
∀x(N(x) →I(x)) : T, ∃x(I(x) ∧O(x)) : T, ∃x(N(x) ∧O(x)) : F
(I(c) ∧O(c)) : T
I(c) : T, O(c) : T
N(c) ∧O(c) : F
O(c) : F
×
N(c) : F
N(c) →I(c)) : T
N(c) : F
⃝
I(c) : T
⃝
A counter-model extracted from the tableau: a structure with domain {a}
where c is interpreted as a, both I and O are interpreted as {a}, and N is
interpreted as the empty set. Another counter-model: keep the original inter-
pretation of N and I and change the interpretation of O to mean “x is nega-
tive.”
(d) Notation: S(x) means “x is selling,” B(x) means “x is buying,” and N(x)
means “x is negotiating.”
Formalization:
∀xS(x) →∃y(B(y) ∨N(y)), ¬∃x(S(x) ∧B(x)) |= ∃x(S(x) →N(x)).
The tableau closes, hence the argument is logically correct:

322
Logic as a Tool
∀xS(x) →∃y(B(y) ∨N(y)) : T, ¬∃x(S(x) ∧B(x)) : T, ∃(S(z) →N(z)) : F
∃x(S(x) ∧B(x)) : F
∀xS(x) : F
∃y(B(y) ∨N(y)) : T
S(c1) : F
S(c1) →N(c1) : F
S(c1) : T, N(c1) : F
×
B(c2) ∨N(c2) : T
S(c2) →N(c2) : F
S(c2) : T, N(c2) : F
S(c2) ∧B(c2) : F
×
B(c2) : F
B(c2) : T
N(c2) : T
×
×
(f) Using predicates P(x) for “x is a penguin,” B(x) for “x is a bird,” W(x) for
“x is white,” and E(x) for “x eats butterflies,” we can formalize the argument
as follows:
∀x(P(x) →B(x)), ∃x(P(x) ∧W(x)), ¬∃x(W(x) ∧E(x))
¬∀x(P(x) →E(x))
.

Answers and Solutions to Selected Exercises
323
The argument is valid:
∀x(P(x) →B(x)) : T, ∃x(P(x) ∧W(x)) : T
¬∃x(W(x) ∧E(x)) : T, ¬∀x(P(x) →E(x)) : F
P(c) ∧W(c) : T
P(c) : T, W(c) : T
P(c) →B(c) : T
P(c) : F
×
B(c) : T
∃x(W(x) ∧E(x)) : F
W(c) ∧E(c) : F
W(c) : F
×
E(c) : F
∀x(P(x) →E(x)) : T
P(c) →E(c) : T
P(c) : F
×
E(c) : T
×
(h) Using predicates B(x) for “x is a bachelor,” M(x) for “x is a man,” W(x)
for “x is a woman,” K(x, y) for “x knows y,” and L(x, y) for “x loves y,”
we can formalize the argument as follows:
∃x(B(x) ∧∀y(W(y) ∧K(x, y) →L(x, y)))
∀x(B(x) →M(x))
W (Eve) ∧∀x(M(x) →K(x, Eve))
∃x(B(x) ∧∃y(W(y) ∧L(x, y))).

324
Logic as a Tool
The argument is logically valid:
∃x(B(x) ∧∀y(W(y) ∧K(x, y) →L(x, y))) : T, ∀x(B(x) →M(x)) : T
W(Eve) ∧∀x(M(x) →K(x, Eve)) : T, ∃x(B(x) ∧∃y(W(y) ∧L(x, y))) : F
B(c) ∧∀y(W(y) ∧K(c, y) →L(c, y)) : T
B(c) : T, ∀y(W(y) ∧K(c, y) →L(c, y)) : T
W(Eve) : T, ∀x(M(x) →K(x, Eve)) : T
B(c) ∧∃y(W(y) ∧L(c, y)) : F
B(c) : F
×
∃y(W(y) ∧L(c, y)) : F
W(Eve) ∧L(c, Eve) : F
W(Eve)) : F
×
L(c, Eve) : F
W(Eve) ∧K(c, Eve) →L(c, Eve) : T
W(Eve) ∧K(c, Eve) : F
W(Eve) : F
×
K(c, Eve) : F
M(c) →K(c, Eve) : T
M(c) : F
B(c) →M(c) : T
B(c) : F
×
M(c) : T
×
K(c, Eve) : T
×
L(c, Eve) : T
×

Answers and Solutions to Selected Exercises
325
(i) In this case, the argument is formalized as follows and is not valid.
∃x(B(x) ∧∀y(W(y) ∧K(x, y) →L(x, y)))
∀x(B(x) →M(x))
W (Eve) ∧∀x(M(x) →K(x, Eve))
∀x(B(x) →∃y(W (y) ∧L(x, y)))
Section 4.3
4.3.1
.(b) . ⊢ND |= ∃xA(x) →∃yA(y):
[∃xA(x)]2
[A(c)]1
∃yA(y)
∃yA(y)
∃xA(x) →∃yA(y)
2
1
(c) ∀xA(x) ⊢ND
¬∃x¬A(x):
[∃x¬A(x)]2
[¬A(c)]1
∀xA(x)
A(c)
⊥
⊥
¬∃x¬A(x)
1
2
4.3.2
.(a) ∀x(P(x) ∨Q) ⊢ND
∀xP(x) ∨Q:
¬Q ∨Q (∗)
[¬Q]1
∀x(P(x) ∨Q)
P(c) ∨Q
¬Q →P(c)
(**)
P(c)
∀xP(x)
∀xP(x) ∨Q
[Q]1
∀xP(x) ∨Q
∀xP(x) ∨Q
1
(*) Here we use the derivation ⊢ND
¬Q ∨Q from Section 2.4.2.
(**) Here we use the derivation A ∨B ⊢ND ¬B →A from Section 2.4.2.

326
Logic as a Tool
(c) .
∃x(P(x) ∧Q)
[P(c) ∧Q]1
P(c)
∃xP(x)
[P(c) ∧Q]1
Q
∃xP(x) ∧Q
∃xP(x) ∧Q
1
(e) ∃xP(x) →Q ⊢ND
∀x(P(x) →Q):
[P(c)]1
∃xP(x)
∃xP(x) →Q
Q
P(c) →Q
1
∀x(P(x) →Q)
(h) ∃x(P (x) →Q), ∀xP(x) ⊢ND Q:
∀xP(x)
P(c)
[P(c) →Q]1
Q
∃x(P(x) →Q)
Q
1
(j) ∃x(P(x) ∨¬Q) ⊢ND Q →∃xP(x):
∃x(P(x) ∨¬Q)
[P(c) ∨¬Q]1 Q
P(c)
(*)
∃xP(x)
∃xP(x)
Q →∃xP(x)
2
1
(*) Here we use the propositional derivation P ∨¬Q, Q ⊢ND P, which is left
as an exercise.
4.3.3
.(b) ∀xA(x) ∨∀xB(x) ⊢ND
∀x(A(x) ∨B(x)):
[∀xA(x)]1
A(c)
A(c) ∨B(c)
[∀xB(x)]1
B(c)
A(c) ∨B(c)
∀xA(x) ∨∀xB(x)
∀x(A(x) ∨B(x))
∀x(A(x) ∨B(x))
∀x(A(x) ∨B(x))
1

Answers and Solutions to Selected Exercises
327
(d) No. Consider for example the structure H with A and B interpreted as M and
W, respectively.
(f) ∀x(A(x) →B(x)) ⊢ND
∀xA(x) →∀xB(x):
[∀xA(x)]1
A(x)
∀x(A(x) →B(x))
A(x) →B(x)
B(x)
∀xB(x)
∀xA(x) →∀xB(x)
1
(h) ∃x(A(x) ∧B(x)) ⊢ND
∃xA(x) ∧∃xB(x):
∃x(A(x) ∧B(x))
[A(c) ∧B(c)]1
A(c)
∃xA(x)
[A(c) ∧B(c)]1
B(c)
∃xB(x)
∃xA(x) ∧∃xB(x)
∃xA(x) ∧∃xB(x)
1
(j) ∃xA(x) ∨∃xB(x) ⊢ND
∃x(A(x) ∨B(x)):
∃xA(x) ∨∃xB(x)
[A(c)]1
A(c) ∨B(c)
[∃xA(x)]3 ∃x(A(x) ∨B(x))
∃x(A(x) ∨B(x))
1
[B(k)]2
A(k) ∨B(k)
[∃xB(x)]3 ∃x(A(x) ∨B(x))
∃x(A(x) ∨B(x))
2
∃x(A(x) ∨B(x))
3
(l) No. Consider for example the structure N with A(x) interpreted as “x is
prime” and B(x) to mean “x < x.”
(n) ∀y∃x(P(x) →Q(y)) ⊢ND
∀xP(x) →∀zQ(z):
∀y∃x(P(x) →Q(y))
∃x(P(x) →Q(d))
[∀xP(x)]2
P(d)
[P(d) →Q(c)]1
Q(c)
Q(c)
∀zQ(z)
∀xP(x) →∀zQ(z)
2
1
4.3.4
.(b) Using the predicates B(x) for “x is a bird,” P (x) for “x is a penguin,” F(x)
for “x can fly,” and W (x) for “x is white,” the argument is formalized as
follows.
∀x(P(x) →B(x)), ¬∃x(P(x) ∧F(x)), ∀x((B(x) ∧W(x)) →F(x))
¬∃x(P(x) ∧W(x))
.
This argument is logically correct because the conclusion is derivable from
the premises in ND.

328
Logic as a Tool
(c) Using the predicates above, but where F(x) means “x is a fish” plus E(x, y)
for “x eats y,” the argument is formalized as follows.
∀x(P(x) →B(x)), ∃x(P(x) ∧W(x)), ∀x(P(x) →∃y(E(x, y) ∧F(y)))
∃x(B(x) ∧W(x) ∧∃y(E(x, y) ∧F(y)))
.
This argument is logically correct because the conclusion is derivable from
the premises in ND.
Section 4.4
4.4.2
Transformation into a prenex DNF and a prenex CNF:
∃z(∃xQ(x, z) →(∃xP(x) ∨¬∃zP(z)))
≡∃z(¬∃xQ(x, z) ∨(∃xP(x) ∨∀z¬P(z)))
≡∃z(∀x¬Q(x, z) ∨∃xP(x) ∨∀z¬P (z))
≡∃z(∀x¬Q(x, z) ∨∃yP(y) ∨∀w¬P(w))
≡∃z∃y∀x∀w(¬Q(x, z) ∨P(y) ∨¬P(w)) (PCNF and PDNF)
Skolemization: ∀x∀w(¬Q(x, c) ∨P(d) ∨¬P(w))
Clausal form: {¬Q(x, c), P(d), ¬P(w)}
4.4.4
Transformation into a prenex DNF and a prenex CNF:
∀z(∃yP(y) ↔Q(z, y))
≡∀z((∃yP(y) →Q(z, y)) ∧(Q(z, y) →∃yP(y)))
≡∀z((¬∃yP(y) ∨Q(z, y)) ∧(¬Q(z, y) ∨∃yP(y)))
≡∀z((∀y¬P(y) ∨Q(z, y)) ∧(¬Q(z, y) ∨∃yP(y)))
≡∀z((∀u¬P(u) ∨Q(z, y)) ∧(¬Q(z, y) ∨∃vP(v)))
≡∀z∃v∀u((¬P(u) ∨Q(z, y)) ∧(¬Q(z, y) ∨P(v))) (PCNF)
≡∀z∃v∀u((¬P(u) ∧¬Q(z, y)) ∨(¬P(u) ∧P (v)) ∨(Q(z, y) ∧¬Q(z, y)) ∨
(Q(z, y) ∧P(v)))
≡∀z∃v∀u((¬P(u) ∧¬Q(z, y))∨(¬P(u)∧P(v))∨(Q(z, y)∧P(v))) (PDNF)
Skolemization: ∀z∀u((¬P(u) ∨Q(z, y)) ∧(¬Q(z, y) ∨P(f(z))))
Clausal form: {{¬P(u), Q(z, y)}, {¬Q(z, y), P(f(z))}}
4.4.6
Transformation into a prenex DNF and a prenex CNF:
∀x(¬∀yQ(x, y) ∧P(z)) →∃z(∀yQ(z, y) ∧¬P(x))
≡¬∀x(¬∀yQ(x, y) ∧P(z)) ∨∃z(∀yQ(z, y) ∧¬P(x))
≡∃x(∀yQ(x, y) ∨¬P(z)) ∨∃z(∀yQ(z, y) ∧¬P(x))
≡∃u(∀yQ(u, y) ∨¬P(z)) ∨∃w(∀vQ(w, v) ∧¬P(x))
≡∃u∃w∀y∀v((Q(u, y) ∨¬P(z)) ∨(Q(w, v) ∧¬P(x))) (PDNF)
≡∃u∃w∀y∀v((Q(u, y) ∨¬P(z) ∨Q(w, v)) ∧(Q(u, y) ∨¬P(z) ∨¬P(x)))
(PCNF)
Skolemization:
∀y∀u((Q(c1, y) ∨¬P(z) ∨Q(c2, v)) ∧(Q(c1, y) ∨¬P(z) ∨¬P(x)))
Clausal form: {{Q(c1, y), ¬P(z), Q(c2, v)}, {Q(c1, y), ¬P(z), ¬P(x)}}

Answers and Solutions to Selected Exercises
329
4.4.7
Transformation into a prenex CNF:
¬(∀y(∀zQ(y, z) →P(z)) →∃z(P(z) ∧∀x(Q(z, y) →Q(x, z))))
≡∀y(∀zQ(y, z) →P (z)) ∧¬∃z(P (z) ∧∀x(Q(z, y) →Q(x, z)))
≡∀y(¬∀zQ(y, z) ∨P(z)) ∧∀z(¬P(z) ∨¬∀x(¬Q(z, y) ∨Q(x, z)))
≡∀y(∃z¬Q(y, z) ∨P(z)) ∧∀z(¬P(z) ∨∃x(Q(z, y) ∧¬Q(x, z)))
≡∀y1(∃z1¬Q(y1, z1) ∨P(z)) ∧∀z2(¬P (z2) ∨∃x(Q(z2, y) ∧¬Q(x, z2)))
≡∀y1∃z1(¬Q(y1, z1) ∨P(z)) ∧∀z2∃x(¬P(z2) ∨(Q(z2, y) ∧¬Q(x, z2)))
≡∀y1∃z1∀z2∃x((¬Q(y1, z1) ∨P(z)) ∧(¬P(z2) ∨Q(z2, y)) ∧(¬P(z2) ∨
¬Q(x, z2)))
Skolemization:
∀y1∀z2((¬Q(y1, f(y1)) ∨P(z)) ∧(¬P(z2) ∨Q(z2, y)) ∧(¬P(z2) ∨
¬Q(g(y1, z2), z2)))
Clausification:
C1 = {¬Q(y1, f(y1)), P(z)},
C2 = {¬P(z2), Q(z2, y)},
C3 = {¬P(z2), ¬Q(g(y1, z2), z2)}.
Clausal form: {C1, C2, C3}.
4.4.9
Transformation into a prenex DNF and a prenex CNF:
¬(∀y(¬∃zQ(y, z) →P(z)) →∃z((P(z) →Q(z, y)) ∧¬∃xR(x, y, z)))
≡∀y(¬∃zQ(y, z) →P(z)) ∧∀z((P(z) ∧¬Q(z, y)) ∨∃xR(x, y, z))
≡∀y(∃zQ(y, z) ∨P (z)) ∧∀z((P (z) ∧¬Q(z, y)) ∨∃xR(x, y, z))
≡∀v(∃wQ(v, w) ∨P(z)) ∧∀u((P(u) ∧¬Q(u, y)) ∨∃xR(x, y, u))
≡∀v∃w∀u∃x((Q(v, w) ∨P(z)) ∧((P(u) ∧¬Q(u, y)) ∨R(x, y, u))) (⋆)
≡∀v∃w∀u∃x(((Q(v, w) ∨P(z)) ∧(P(u) ∧¬Q(u, y))) ∨((Q(v, w) ∨
P(z)) ∧R(x, y, u)))
≡∀v∃w∀u∃x((Q(v, w) ∧P(u) ∧¬Q(u, y)) ∨(P(z) ∧P(u) ∧¬Q(u, y)) ∨
(Q(v, w) ∧R(x, y, u)) ∨(P(z) ∧R(x, y, u))) (PDNF)
≡∀v∃w∀u∃x((Q(v, w) ∨P(z)) ∧(P(u) ∨R(x, y, u)) ∧(¬Q(u, y) ∨
R(x, y, u)))(PCNF from (⋆))
Skolemization:
∀v∀u((Q(v, f(v)) ∨P(z)) ∧(P(u) ∨R(g(v, u), y, u)) ∧(¬Q(u, y) ∨
R(g(v, u), y, u)))
Clausal form:
{{Q(v, f(v)), P(z)}, {P(u), R(g(v, u), y, u)}, {¬Q(u, y), R(g(v, u), y, u)}}
Section 4.5
4.5.2
.(a) Transformation of ¬(∀xP(x) →∀yP(y)) into a clausal form:
C1 = {P(x)} and C2 = {¬P(c)}
for some Skolem constant c.
Applying Resolution: unify P(c) and P(x) with MGU[c/x], and then
resolve C1 with C2 to obtain {}.

330
Logic as a Tool
(c) Transformation of ¬∃x(P(x) →∀yP(y)) into a clausal form:
C1 = {P(x)} and C2 = {¬P(f(y))}
for some Skolem function f.
Applying Resolution: unify P (x) and P (f(y)) with MGU[f(y)/x], and
then resolve C1 with C2 to obtain {}.
(e) Transformation of ¬∃x¬A(x) and ¬∀xA(x) into a clausal form:
C1 = {¬A(c)} and C2 = {A(x)}
for some Skolem constant c.
Applying Resolution: unify A(x) and A(c) with MGU[c/x], and then resolve
C1 with C2 to obtain {}.
(h) Transformation of ∃x∃yA(x, y) and ¬∃y∃xA(x, y) into a clausal form:
C1 = {A(c1, c2)} and C2 = {¬A(x, y)}
for Skolem constants c1 and c2.
Applying Resolution: unify A(c1, c2) and A(x, y) with MGU[c1/x, c2/y],
and then resolve C1 with C2 to obtain {}.
4.5.3
.(a) Transforming each of ∀x(P(x) ∨Q) and ¬(∀xP(x) ∨Q) to a clausal form:
C1 = {P(x), Q}, C2 = {¬P(c)}, and C3 = {¬Q}
for some Skolem constant c.
Now, applying the Resolution rule successively, we get
C4 = Res(C1, C2) = {Q}
MGU[c/x]
C5 = Res(C3, C4) = {}.
The empty clause is derived, hence the logical consequence holds.
(c) Transforming each of ∃x(P(x) ∧Q) and ¬(∃xP(x) ∧Q) to a clausal form:
C1 = {P(c)}, C2 = {Q}, and C3 = {¬P (x), ¬Q}
for some Skolem constant c.
Now, applying the Resolution rule successively:
C4 = Res(C1, C3) = {¬Q}
MGU[c/x]
C5 = Res(C2, C4) = {}.
The empty clause is derived, hence the logical consequence holds.
(e) Transforming each of ∀x(Q →P(x)), Q and ¬∀xP(x) to a clausal form:
C1 = {¬Q, P(x)}, C2 = {Q}, and C3 = {¬P(c)}
for some Skolem constant c.

Answers and Solutions to Selected Exercises
331
Now, applying the Resolution rule successively:
C4 = Res(C1, C3) = {¬Q}
MGU[c/x]
C5 = Res(C2, C4) = {}.
The empty clause is derived, hence the logical consequence holds.
(h) Transforming each of ∃x(Q →P(x)), Q and ¬∃xP(x) to a clausal form:
C1 = {¬Q, P (c)}, C2 = {Q}, and C3 = {¬P(x)}
for some Skolem constant c.
Now, applying the Resolution rule successively:
C4 = Res(C1, C3) = {¬Q}
MGU[c/x]
C5 = Res(C2, C4) = {}.
The empty clause is derived, hence the logical consequence holds.
4.5.4
.(a) First, we transform each of ∀xA(x), ∀xB(x) and ¬∀x(A(x) ∧B(x)) to
clausal form:
C1 = {A(x)}, C2 = {B(x)}, and C3 = {¬A(c), ¬B(c)}
for some Skolem constant c.
Now, applying the Resolution rule successively, we get
C4 = Res(C1, C2) = {¬B(c)}
MGU[c/x]
C5 = Res(C2, C4) = {}
MGU[c/x].
The empty clause is derived, hence the logical consequence holds.
(c) Transforming each of ∀x(A(x) ∨B(x)) and ¬(∀xA(x) ∨∀xB(x)) to
clausal form:
C1 = {A(x), B(x)}, C2 = {¬A(c1)}, and C3 = {¬B(c2)}
for Skolem constants c1 and c2. The only possible resolutions on this set
of clauses are as follows. Unify A(x) and A(c1) and resolve C1 and C2 to
obtain C4 = {B(c1)}. Likewise, unify B(x) and B(c2) and resolve C1 and
C3 to obtain C5 = {A(c2)}. No more clauses are derivable. The empty clause
therefore cannot be derived. Hence ∀x(A(x) ∨B(x)) ⊭∀xA(x) ∨∀xB(x).
A counter-model: for instance, consider the structure H with A and B taken
as the predicates M and W, respectively.
(e) The empty clause cannot be derived, hence the logical consequence does not
hold. A counter-model: consider for example the structure H with A and B
taken as the predicates M and W, respectively.
(g) The empty clause cannot be derived, hence the logical consequence does not
hold. A counter-model: for instance, consider the structure H with A and B
taken as the predicates M and W, respectively.

332
Logic as a Tool
(i) We transform each of ¬(∃xA(x) ∨∃xB(x)) and ∃x(A(x) ∨B(x)) to
clausal form:
C1 = {A(c), B(c)}, C2 = {¬A(x)}, and C3 = {¬B(y)}
for some Skolem constant c. Now, applying the Resolution rule successively,
we get
C4 = Res(C1, C2) = {B(c)}
MGU[c/x]
C5 = Res(C3, C4) = {}
MGU[c/y].
The empty clause is derived, hence the logical consequence holds.
(k) Transforming each of ∃xA(x) →∃xB(x) and ¬∃x(A(x) →B(x)) to
clausal form:
C1 = {¬A(x), B(f(x))}, C2 = {A(z)}, and C3 = {¬B(z)}
for some Skolem function f. Now, applying the Resolution rule successively,
we get
C4 = Res(C1, C2) = {B(f(x))}
MGU[x/z]
C5 = Res(C3, C4) = {}
MGU[f(x)/z].
The empty clause is derived, hence the logical consequence holds.
4.5.5
.(b) Using predicates N(x) for “x is a natural number,” I(x) for “x is an integer,”
and O(x) for “x is odd,” we can formalize the argument as follows:
∀x(N(x) →I(x)), ∃x(I(x) ∧O(x))
∃x(N(x) ∧O(x))
.
Then we transform the formulas ∀x(N(x) →I(x)), ∃x(I(x) ∧O(x)), and
¬∃x(N(x) ∧O(x)) to clausal form:
C1 = {¬N(x), I(x)}, C2 = {I(c)}, C3 = {O(c)}, and
C4 = {¬N(y), ¬O(y)},
for some Skolem constant c. Applying the Resolution rule we get
C5 = Res(C1, C2) = {¬N(c)}
MGU[c/x]
C6 = Res(C3, C4) = {¬N(c)}
MGU[c/y].
No more resolvents can be obtained, hence the empty clause cannot be
derived and the argument is not valid.
(d) Using predicates P(x) for “x is a penguin,” B(x) for “x is a bird,” W(x) for
“x is white,” and E(x) for “x eats some fish,” we can formalize the argument
as follows:
∀x(P(x) →B(x)), ∃x(P(x) ∧W(x)), ∀x(P (x) →E(x))
∃x(W(x) ∧B(x) ∧E(x))
.

Answers and Solutions to Selected Exercises
333
Now we transform the formulas ∀x(P(x) →B(x)), ∃x(P(x) ∧W(x)),
∀x(P(x) →E(x)), and ¬∃x(W(x) ∧B(x) ∧E(x)) to clausal form:
C1 = {¬P(x), B(x)}
C2 = {P(c)}
C3 = {W (c)}
C4 = {¬P(x), E(x)}
C5 = {¬W(x), ¬B(x), ¬E(x)}
for some Skolem constant c. Now, applying the Resolution rule successively,
we get
C6 = Res(C3, C5) = {¬B(c), ¬E(c)}
MGU[c/x]
C7 = Res(C4, C6) = {¬P(c), ¬B(c)}
MGU[c/x]
C8 = Res(C1, C7) = {¬P(c)}
MGU[c/x]
C9 = Res(C2, C8) = {}.
The empty clause is derived, hence the argument is valid.
(f) Using predicates Y (x) for “x is yellow,” P(x) for “x is a plonk,” and Q(x)
for “x is a qlink,” we can formalize the argument as follows:
¬∃x(Y (x) ∧P(x) ∧Q(x)), ∃x(P(x) ∨¬Q(x))
∃x¬(Y (x) ∧Q(x))
.
We next transform the formulas ¬∃x(Y (x) ∧P (x) ∧Q(x)),
∃x(P(x) ∨¬Q(x)), and ¬∃x¬(Y (x) ∧Q(x)) to clausal form:
C1 = {¬Y (x), ¬P(x), ¬Q(x)}
C2 = {P(c), ¬Q(c)}
C3 = {Y (x)}
C4 = {Q(x)}
for some Skolem constant c.
Now, applying the Resolution rule successively, we get
C5 = Res(C1, C2) = {¬Y (c), ¬Q(c)}
MGU[c/x]
C6 = Res(C3, C5) = {¬Q(c)}
MGU[c/x]
C7 = Res(C4, C6) = {}
MGU[c/x].
The empty clause is derived, hence the argument is valid.
4.5.6
We formalize A and B in the domain of all men, using P(x) for “x is happy”
and Q(x) for “x is drunk” as follows:
A := ¬∃x(P(x) →Q(x))
B := ∃yP(y) ∧¬∃zQ(z).
(a) First we check if A implies B.
Clausification of A and ¬B:
C1 := {P(x), ¬Q(x)}, C2 := {¬P(y)}, C3 := {Q(s1)},
where s1 is a new Skolem constant.

334
Logic as a Tool
Application of Resolution:
C4 := Res(C1, C2) = {¬Q(x)}, MGU : [x/y] C5 := Res(C3, C4) = {},
MGU : [s1/x].
The empty clause is derived and therefore the implication holds.
(b) We now check if B implies A.
Clausification of B and ¬A:
C1 := {¬P(s1), Q(s1)}, C2 := {P(s2)}, C3 := {¬Q(z)},
where s1, s2 are new Skolem constants.
Application of Resolution:
C4 := Res(C1, C3) = {Q(s2)}, MGU : [s3/z]
No new clauses can be obtained and therefore the consequence does not hold.
A counter-model: J = ⟨X, P J, QJ⟩,
where X = {a1, a2}, P J = {a1}, QJ = ∅.
Now, B is true if we choose y to be a1 as ¬∃Q(z) is always true in this
model. For A, we can choose x to be a2. Then P(x) is false and it follows
that ∃x(P(x) →Q(x)) is true and A is false. The consequence therefore
fails.
4.5.7
.(a) The argument can be written as follows:
∀x(F(x) →E(x))
∀x(H(x) →¬∃y(E(y) ∧L(x, y)))
∃x((G(x) ∨F (x)) ∧L(Kermit, x))
∀x(∃y(G(y) ∧L(x, y)) →¬∃z(T(z) ∧L(x, z)))
∃x(T(x) ∧L(Kermit, x)) →¬H(Kermit)
From all premises and the negated conclusion we obtain the following
clauses:
C1 = {¬F(x), E(x)}
C2 = {¬H(x), ¬E(x),
¬L(x, y)}
C3 = {G(c1), F(c1)}
C4 = {L(Kermit, c1)}
C5 = {¬G(y), ¬L(x, y), ¬T(z), ¬L(x, z)}
C6 = {T(c2)}
C7 = {L(Kermit, c2)}
C8 = {H(Kermit)}
for Skolem constants c1 and c2.
Now, applying Resolution successively, we get
C9 = Res(C5, C7) = {¬G(y), ¬L(Kermit, y), ¬T(c2)}
MGU[Kermit/x, c2/z]
C10 = Res(C6, C9) = {¬G(y), ¬L(Kermit, y)}
C11 = Res(C3, C10) = {F(c1), ¬L(Kermit, c1)}
MGU[c1/y]
C12 = Res(C1, C11) = {E(c1), ¬L(Kermit, c1)}
MGU[c1/x]
C13 = Res(C2, C12) = {¬H(x), ¬L(x, c1), ¬L(Kermit, c1)} MGU[c1, y]
C14 = Res(C8, C13) = {¬L(Kermit, c1)}
MGU[Kermit/x]
C15 = Res(C4, C14) = {}.
The empty clause is derived, hence the argument is valid.

Answers and Solutions to Selected Exercises
335
(c) We can formalize the argument as follows:
∀x(T(x) →((E(x) ∧¬G(x)) ∨(G(x) ∧¬E(x))))
∀y(T(y) ∧Fl(y) →E(y))
∀x(D(x) →Fl(x))
∀x(G(x) ∧U(x) →T(x))
∃x(G(x) ∧U(x))
∃x(U(x) ∧¬D(x))
From all premises and the negated conclusion we obtain the following
clauses:
C1 = {¬T(x), E(x), G(x)}
C2 = {¬T(x), ¬E(x), ¬G(x)}
C3 = {¬T(y), ¬Fl(y), E(y)}
C4 = {¬D(u), Fl(u)}
C5 = {¬G(v), ¬U(v), T(v)}
C6 = {G(c)}
C7 = {U(c)}
C8 = {¬U(w), D(w)}
for some Skolem constant c.
Now, applying the Resolution rule successively, we get
C9 = Res(C3, C4) = {¬T(y), E(y), ¬D(y)}
MGU[y/u]
C10 = Res(C8, C9) = {¬T(y), E(y), ¬U(y)}
MGU[y/w]
C11 = Res(C2, C10) = {¬T(y), ¬U(y), ¬G(y)}
MGU[y/x]
C12 = Res(C5, C11) = {¬G(y), ¬U(y)}
MGU[y/v]
C13 = Res(C6, C12) = {¬U(c)}
MGU[c/y]
C14 = Res(C7, C13) = {}.
The empty clause is derived, hence the argument is valid.
4.5.8
.(a) Using predicates B(x) for “x is a bachelor,” M(x) for “x is a man,” W(x)
for “x is a woman,” K(x, y) for “x knows y,” and L(x, y) for “x loves y,”
we can formalize the argument as follows:
∃x(B(x) ∧∀y(W(y) ∧K(x, y) →L(x, y)))
∀x(B(x) →M(x))
W(Eve) ∧∀x(M(x) →K(x, Eve))
∃x(B(x) ∧∃y(W(y) ∧L(x, y)))
We then obtain the following clausal form:
C1 = {B(c)}
C2 = {¬W(y), ¬K(c, y), L(c, y)}
C3 = {¬B(z), M(z)}
C4 = {W(Eve)}
C5 = {¬M(w), K(w, Eve)}
C6 = {¬B(u), ¬W(v), ¬L(u, v)}

336
Logic as a Tool
for some Skolem constant c. Now, applying Resolution successively, we get
C7 = Res(C2, C6) = {¬B(c), ¬W(y), ¬K(c, y)}
MGU[c/u, y/v]
C8 = Res(C5, C7) = {¬B(c), ¬M(c), ¬W(Eve)}
MGU[c/w, Eve/y]
C9 = Res(C3, C8) = {¬B(c), ¬W(Eve)}
MGU[c/z]
C10 = Res(C1, C9) = {¬W(Eve)}
C11 = Res(C4, C10) = {}.
The empty clause is derived, hence the argument is valid.
(c) We formalize the argument using the following non-logical symbols: J to
mean “Juan;” L to mean “Lara;” W(x) to mean “x is a woman;” M(x) to
mean “x is a man;” B(x) to mean “x is brave;” P(x) to mean “x is pretty;”
and A(x, y) to mean “x admires y.”
Here is the formalization of the argument:
“Every man admires some brave woman”:
P1 ≡∀x(M(x) →∃y(W(y) ∧B(y) ∧A(x, y))).
“Juan is a man who admires every pretty woman”:
P2 ≡M(J) ∧∀y(W(y) ∧P (y) →A(J, y)).
“Lara is a brave or pretty woman”: P3 ≡W(L) ∧(B(L) ∨P(L)).
“Some man admires Lara”: Q ≡∃x(M(x) ∧A(x, L)).
Clausification:
P1: C1 = {¬M(x), W(f(x))}, C2 = {¬M(x), B(f(x))},
C3 = {¬M(x), A(x, f(x))},
P2: C4 = {M(J)}, C5 = {¬W(y), ¬P(y), A(J, y)},
P3: C6 = {W(L)}, C7 = {B(L), P(L)},
¬Q: C8 = {¬M(z), ¬A(z, L)}.
Running Resolution:
C9 = (C4, C8) = {¬A(J, L)}
C10 = Res(C1, C4) = {W(f(J))},
C11 = Res(C2, C4) = {B(f(J))},
C12 = Res(C3, C4) = {A(J, f(J))},
C13 = (C5, C6) = {¬P(L), A(J, L)}
C14 = (C9, C13) = {¬P(L)}
C15 = (C7, C14) = {B(L)}
C16 = (C5, C9) = {¬W(L), ¬P (L)}
C17 = (C1, C5) = {¬M(x), ¬P(f(x)), A(J, f(x))},
C18 = (C4, C17) = {¬P(f(J)), A(J, f(J))}.
No more new clauses can be derived. The empty clause has not been derived.
Therefore, the argument is not logically correct.
A counter-model can be extracted from the execution above with universe
U = {Juan, Lara, X}, where:
M = {Juan}, W = B = {Lara, X}, P = ∅, f(u) = X for every u ∈U,
A = {(Juan,X)}.

Answers and Solutions to Selected Exercises
337
4.5.9
We use the predicates:
T(x) for “x can talk,”
W(x) for “x can walk,”
and C(x, y) for “x can construct y.”
(a) Formalizing the premises:
P1 = ∀x(T(x) →W(x))
P2 = ∃x(W(x) ∧∀y(T(y) →C(x, y)))
Formalizing the conclusion:
Q = ∃x(T(x) ∧∃z(W (z) ∧C(x, z)) ∧∀y(T(y) →C(x, y)))
Prenex normal form, Skolemization, and clausification (using different vari-
ables in the different clauses):
P1 ≡∀x(¬T(x) ∨W(x)). Clause: C1 = {¬T(x), W(x)}.
P2 ≡∃x∀y(W(x) ∧(¬T(y) ∨C(x, y))).
Skolemized: W(c) ∧(¬T(y) ∨C(c, y)).
Clauses: C2 = {W (c)}, C3 = {¬T(y), C(c, y)}.
Negated conclusion:
¬Q = ¬∃x(T(x) ∧∃z(W(z) ∧C(x, z)) ∧∀y(T(y) →C(x, y)))
≡∀x(¬T(x) ∨∀z(¬W(z) ∨¬C(x, z)) ∨∃y(T(y) ∧¬C(x, y)))
≡∀x∃y∀z(¬T(x) ∨¬W (z) ∨¬C(x, z) ∨(T(y) ∧¬C(x, y))).
Skolemization produces:
∀x∀z(¬T(x) ∨¬W(z) ∨¬C(x, z) ∨(T(f(x)) ∧¬C(x, f(x)))).
Clauses:
C4 = {¬T(x1), ¬W(z1), ¬C(x1, z1), T(f(x1))};
C5 = {¬T(x2), ¬W(z2), ¬C(x2, z2), ¬C(x2, f(x2))}.
Note that the clause C5 can be equivalently simplified to the subsuming one
C′
5 = {¬T(x2), ¬W(z2), ¬C(x2, f(x2))}.
Running Resolution (most unifications are obvious):
Res(C1, C4) (unifying W(x) with W(z1)) :=
C6 = {¬T(z1), ¬T(x1), ¬C(x1, z1), T(f(x1))};
Res(C1, C′
5) (unifying W(x) with W(z2)) :=
C7 = {¬T(z2), ¬T(x2), ¬C(x2, f(x2))};
Res(C2, C4) := C8 = {¬T(x1), ¬C(x1, c), T(f(x1))};
Res(C2, C′
5) := C9 = {¬T(x2), ¬C(x2, f(x2))};
(At this stage, the clause C2 is ‘used up’.)
Res(C1, C8) := C10 = {¬T(x1), ¬C(x1, c), W(f(x1))};
· · ·
It is easy to see that infinitely many clauses can be derived. However,
the empty clause cannot be derived, hence the argument above is not
logically correct. A counter-model: consider a universe with two robots
{R1 and R2} such that:
R1 can talk and walk, but cannot construct any robot;
R2 cannot talk, but can walk and can construct R1.

338
Logic as a Tool
(b) Using the predicates defined in the previous exercise, as well as the follow-
ing: R(x) for “x is a robot” and L(x) for “x can reason logically,” we
can formalize the argument as follows.
i. Some devices are robots: ∃xR(x).
ii. All robots can talk or walk or reason logically:
∀x(R(x) →(T(x) ∨W(x) ∨L(x))).
iii. No non-talking robot can reason logically:
¬∃x(R(x) ∧¬T(x) ∧L(x)).
iv. All talking robots can construct every logically reasoning device:
∀x(R(x) ∧T(x) →∀y(L(y) →C(x, y))).
v. A device can construct a robot only if it is itself a robot:
∀x(∃y(R(y) ∧C(x, y)) →R(x)).
vi. Some logically reasoning devices can construct every non-walking
robot:
∃x(L(x) ∧∀y((R(y) ∧¬W(y)) →C(x, y))).
vii. Some talking robot can construct itself:
∃x(R(x) ∧T(x) ∧C(x, x)).
The empty clause is not derivable, hence the argument is not logically correct.
(c) The modified last premise formalizes as
∀x(L(x) →∀y((R(y) ∧¬W(y)) →C(x, y))).
The empty clause is still not derivable, hence the resulting argument is not
logically correct.
(d) The added premise formalizes as
R(Creepy) ∧¬W(Creepy).
The empty clause is now derivable, hence the resulting argument is logically
correct.
4.5.10
Indeed, the unit clause σ(s) = σ(t) can be added as an instance of the reflexivity
axiom u = u and then, by unifying and resolving with ¬s = t ∨P(s), used to
derive σ(P(t)).
4.5.11
See algebraic derivations of all but the last exercises in many standard textbooks
on group theory. For the last derivation, see the appendix in Wos and Robinson
(1969).
Section 4.6
No solutions are provided for these exercises, but the interested reader can find detailed
proofs of completeness for each of the deductive systems studied here in the references
provided at the end of the section.

Answers and Solutions to Selected Exercises
339
Section 5.1
5.1.2
.(b) Here we combine direct reasoning with reasoning by contradiction.
Let a, b, x ∈R, and assume a < b and (x −a)(x −b) < 0.
From
(x −a)(x −b) < 0 we have that either x −a < 0 and x −b > 0 or
x −a > 0 and x −b < 0, hence either x < a and b < x or a < x and
x < b. The first case leads to a contradiction with a < b because x < a and
b < x imply b < a by transitivity of <, and b < a implies not a < b by
irreflexivity of <. The second case implies the claim a < x < b.
(d) We use a proof by contraposition. Let m, n ∈Z, and suppose mn is even, yet
it is not the case that m is even or n is even. So, both m and n are not even,
hence they are odd. Then there are integers a and b such that m = 2a + 1
and n = 2b + 1. Therefore,
mn = (2a + 1)(2b + 1) = 4ab + 2a + 2b + 1 = 2(2ab + a + b) + 1,
which means that mn is odd, hence not even: a contradiction.
(f) We use a direct proof here. Let a, b ∈Z, and assume ab is odd. This is only
possible if both a and b are odd. Hence, a = 2m + 1 and b = 2n + 1 for
some integers m and n. Then we obtain a + b = 2m + 1 + 2n + 1 = 2m +
2n + 2 = 2(m + n + 1); a + b is therefore even.
(h) We give a proof by contradiction. Suppose there are positive integers m and
n such that
1
m + 1
n > 2. But, since m and n are positive integers we have
that m ≥1 and n ≥1, therefore
1
m ≤1 and 1
n ≤1, hence
1
m + 1
n ≤2: a
contradiction.
Section 5.2
Exercises on sets and operations on sets
I only sketch a few sample proofs here, presented as semi-formal mathematical arguments.
I leave it to the interested reader to formalize them completely, in ND or another deductive
system.
5.2.4
. (a) We proceed with proof by contradiction: suppose A ∩∅is non-empty and
take any element of x of A ∩∅. Then, in particular, x ∈∅, which is impos-
sible. A ∩∅is therefore the empty set.
(g) Suppose X ⊆A and X ⊆B, we need to show that X ⊆A ∩B. To that
end, suppose that x ∈X. Since X ⊆A we have x ∈A, and since X ⊆B
we also have x ∈B. By the definition of intersection, we therefore have
x ∈A ∩B.
5.2.5
. (a) To prove that A ∪∅⊆A, suppose that x ∈A ∪∅. Hence x ∈A or x ∈∅.
But since it cannot be the case that x ∈∅, it must be the case that x ∈A.
For the inclusion A ⊆A ∪∅, suppose that x ∈A. Then x ∈A or x ∈∅,
so x ∈A ∪∅.

340
Logic as a Tool
5.2.6
. (a) Suppose that x ∈A −A. That means x ∈A and x /∈A, which is impos-
sible. There can therefore be no element x ∈A −A, that is, A −A = ∅.
(e) Suppose x ∈(A −B) ∩B. This means x ∈(A −B) and x ∈B. But x ∈
(A −B) means x ∈A and x /∈B. So it cannot be the case that x ∈B,
which is a contradiction. We conclude that (A −B) ∩B has no elements,
that is, (A −B) ∩B = ∅.
(k) We will prove both inclusions. To prove that A −(B ∪C) ⊆(A −B) ∩
(A −C), suppose that x ∈A −(B ∪C). Then, x ∈A and x /∈(B ∪C),
that is, x ∈A but x /∈B and x /∈C. Then x ∈A −B and x ∈A −C,
that is, x ∈(A −B) ∩(A −C).
For the inclusion (A −B) ∩(A −C) ⊆A −(B ∪C), suppose that x ∈
(A −B) ∩(A −C). This means that x ∈(A −B) and x ∈(A −C), that
is, x ∈A but x /∈B and x /∈C. Since x /∈B and x /∈C we have x /∈
(B ∪C). Combining these with x ∈A, we have x ∈A −(B ∪C).
5.2.7
. (a) The proof goes through a chain of “if and only if”s:
x ∈A ∩(B ∪C)
iff x ∈A and x ∈(B ∪C)
iff x ∈A, and x ∈B or x ∈C
iff x ∈A and x ∈B, or x ∈A and x ∈C
iff x ∈A ∩B or x ∈A ∩C
iff x ∈(A ∩B) ∪(A ∩C).
Note that for the third “iff” we made use of the propositional tautology p ∧
(q ∨r) ≡(p ∧q) ∨(p ∧r), which looks very much like the set-theoretic
property we are proving. This is not accidental, as there is a close relation-
ship between set theory and logic. Think about it!
5.2.8
. (g) In order to construct a subset A of X, for each element of X we must decide
whether it will be in A or not and these choices completely determine A.
For each element of X there are therefore two choices: “in” or “out.” The
number of ways to form the subset A is therefore 2 × 2 × × · · · × 2



ntimes
= 2n.
5.2.9
. (a) For the inclusion A × (B ∪C) ⊆(A × B) ∪(A × C), suppose that x ∈
A × (B ∪C). Then x = (y, z) for some y ∈A and z ∈(B ∪C). Since
z ∈(B ∪C), we have z ∈B or z ∈C. So (y, z) ∈A × B or (y, z) ∈
A × C, and hence x = (y, z) ∈(A × B) ∪(A × C).
For
the
inclusion
(A × B) ∪(A × C) ⊆A × (B ∪C),
suppose
that
x = (y, z) ∈(A × B) ∪(A × C).
Then
(y, z) ∈(A × B)
or
(y, z) ∈(A × C). Thus y ∈A, while z ∈B or z ∈C. We therefore have
y ∈A while z ∈B ∪C, and hence x = (y, z) ∈A × (B ∪C).

Answers and Solutions to Selected Exercises
341
Exercises on functions
5.2.10
Since f is surjective, we have rng(f) = B = dom(f −1). To prove that
f −1 is injective, suppose that b1, b2 ∈B and that f −1(b1) = f −1(b2). Since
rng(f) = B, there are a1, a2 ∈A such that f(a1) = b1 and f(a2) = b2. Thus
a1 = f −1(f(a1)) = f −1(b1) = f −1(b2) = f −1(f(a2)) = a2. Since we now
have a1 = a2, the injectivity of f allows us to conclude that f(a1) = f(a2) and
therefore that b1 = b2, as desired.
To show that f −1 is surjective, suppose that a ∈A. We must show that there
is a b ∈B such that f −1(b) = a. Take f(a) ∈B: we have that f −1(f(a)) = a.
5.2.12
.(a) Suppose
f
and
g
are
injective,
and
that
a1, a2 ∈A
such
that
gf(a1) = gf(a2). We must show that a1 = a2. Thus g(f(a1)) = g(f(a2)).
But since g is injective this means that f(a1) = f(a2), which in turn means
that a1 = a2, since f is injective.
(c) Follows immediately from Proposition 194, (a) and (b).
(e) Suppose that gf is surjective and that c ∈C. We need to find an b ∈B such
that g(b) = c. By the surjectivity of gf there is an a ∈A such that gf(a) =
c, that is, g(f(a)) = c. Then f(a) ∈B is the desired element of B.
5.2.13
By Proposition 194(3) gf is bijective and therefore we know that it has an inverse
(gf)−1. Clearly dom(f −1g−1) = C = dom((gf)−1) and rng(f −1g−1) = A =
rng((gf)−1). We need to show that f −1g−1(c) = (gf)−1(c) for all c ∈C. To
this end, let c ∈C arbitrarily. Then, because gf is surjective, there is an a ∈A
such that gf(a) = c. By definition of inverses (gf)−1(c) = (gf)−1(gf(a)) = a.
Now f −1g−1(c) = f −1g−1(gf(a)) = f −1g−1(g(f(a))) = f −1(g−1(g(f(a)))).
By the associativity of function composition (Proposition 193) this last expres-
sion is equal to f −1((g−1g)(f(a))) = f −1(f(a)) = a.
5.2.14
.(a) For the sake of definiteness, suppose that f : A →B. To prove the
left-to-right implication suppose that f is injective and take any mappings
g1 : C →A and g2 : C →A. Further suppose that g1f = g2f. We must
show that g1 = g2. So let c ∈C arbitrarily. Then fg1(c) = fg2(c), that is,
f(g1(c)) = f(g2(c)). Since f is injective, it follows that g1(c) = g2(c).
To prove the converse we will proceed by contraposition, so suppose that
f is not injective. Then there are a1, a2 ∈A such that a1 ̸= a2 but f(a1) =
f(a2). We need to show that the left cancellation rule does not hold. For this it
is sufficient to find two mappings g1 and g2 such that fg1 = fg2 but g1 ̸= g2.
Let g1 : A →A and g2 : A →A be maps such that g1 is the identity map on
A, that is, g1(a) = a for all a ∈A, and g2(a) = a for all a ∈A −{a1, a2}
but g(a1) = a2 and g(a2) = a1. Then it is easy to check that fg1 = fg2, but
g1 ̸= g2.

342
Logic as a Tool
Exercises on binary relations
5.2.15
.(a) For
the
inclusion
dom(R ∪S) ⊆dom(R) ∪dom(S),
suppose
that
a ∈dom(R ∪S), that is, there exists b ∈B such that (a, b) ∈R ∪S. But
then (a, b) ∈R or (a, b) ∈S, that is, a ∈dom(R) or a ∈dom(S), that is,
a ∈dom(R) ∪dom(S).
For the converse inclusion, suppose that a ∈dom(R) ∪dom(S), that is, a ∈
dom(R) or a ∈dom(S), that is, there is a b ∈B such that (a, b) ∈R or
(a, b) ∈S. But then (a, b) ∈R ∪S, and hence a ∈dom(R ∪S).
5.2.16
.(a) b ∈dom(R−1) iff ∃a ∈A such that (b, a) ∈R−1 iff ∃a ∈A such that
(a, b) ∈R iff b ∈rng(R).
(d) (b, a) ∈(R ∪S)−1 iff (a, b) ∈(R ∪S) iff (a, b) ∈R or (a, b) ∈S iff
(b, a) ∈R−1 or (b, a) ∈S−1 iff (b, a) ∈R−1 ∪S−1.
5.2.17
To be definite, let R ⊆A × B, S ⊆B × C, and T ⊆C × D. Then (a, d) ∈
(R ◦S) ◦T, iff there exists a c ∈C such that (a, c) ∈(R ◦S) and (c, d) ∈T,
iff there exists c ∈C and b ∈B such that (a, b) ∈R, (b, c) ∈S, and (c, d) ∈T,
iff there exists b ∈B such that (a, b) ∈R and (b, d) ∈S ◦T, iff (a, d) ∈R ◦
(S ◦T).
5.2.19
.(c) Suppose that R is asymmetric, that is, ∀x∀y(xRy →¬yRx). Now sup-
pose that (x, y) ∈R−1 ∩R, that is, (x, y) ∈R−1 and (x, y) ∈R, that is,
(y, x) ∈R and (x, y) ∈R, which is impossible by the asymmetry of R.
Hence, R−1 ∩R = ∅.
For the converse we argue contrapositively. Suppose that R is not asym-
metric, that is ¬∀x∀y(xRy →¬yRx), that is, there exists x, y ∈X such
that (x, y) ∈R and (y, x) ∈R, that is, (x, y) ∈R and (x, y) ∈R−1, that
is, (x, y) ∈R ∩R−1. So we conclude that R−1 ∩R ̸= ∅.
(e) Suppose R is connected. To show that R ∪R−1 ∪EX = X2 we only need
to show that X2 ⊆R ∪R−1 ∪EX since the converse containment holds
for any relation. To that end, take an arbitrary (x, y) ∈X2. If x = y, then
(x, y) = (x, x) ∈EX and we are done. If x ̸= y, then since R is connected
we know that xRy or yRx, that is, xRy or xR−1y, that is, (x, y) ∈R ∪R−1.
Conversely, suppose that X2 = R ∪R−1 ∪EX. For all x, y ∈X, if x ̸= y,
we therefore have (x, y) ∈R or (x, y) ∈R−1. That is, for all x, y ∈X, we
have (x, y) ∈R or (y, x) ∈R, which precisely means that R is connected.
5.2.21
Hint: consider the intersection of all reflexive/symmetric/transitive relations con-
taining the given relation.
5.2.22
.(c) Let X = {S | R ⊆S ⊆X2, where S is transitive}, that is, X is the set con-
sisting of all transitive binary relations on X which contain R. We must
show that Rtran =  X. We begin by noting that X ̸= ∅, since X2 is tran-
sitive and R ⊆X2. By definition of X we have that R ⊆S for all S ∈X,
so R ⊆ X by Proposition 186(7). Also,  X is transitive: suppose that
(x, y), (y, z) ∈ X. Then (x, y), (y, z) ∈S for all S ∈X and, since each

Answers and Solutions to Selected Exercises
343
S ∈X is transitive, (x, z) ∈S for all S ∈X. Hence (x, z) ∈ X. We have
therefore established that  X is a transitive binary relation containing R. All
that remains is to show that  X is the smallest such relation. To that end,
suppose that R′ ⊆X2 is transitive and R ⊆R′. Then, by definition of X we
have R′ ∈X, and hence  X ⊆R′.
5.2.24
The proofs of (1), (2), and (4) are left as an exercise. We prove (3) by proving the
contrapositive. Suppose that [x]R ∩[y]R ̸= ∅. Then there is at least one element
z ∈[x]R ∩[y]R, and so xRz and yRz. By symmetry we also have zRx, and
then by transitivity from yRz and zRx we get yRx, that is, x ∈[y]R.
5.2.25
In order to show that {[x]R | x ∈X} satisfies the conditions of the definition
of partition, we need to show that  {[x]R | x ∈X} = X and that for any two
different equivalence classes [x]R and [y]R it holds that [x]R ∩[y]R = ∅.
Since [x]R ⊆X for each x ∈X we immediately have that  {[x]R | x ∈
X} ⊆X. For the sake of the inclusion in the other direction, let y ∈X arbitrar-
ily. By Proposition 204(1), y ∈[y]R and therefore y ∈ {[x]R | x ∈X}. We
have therefore established that  {[x]R | x ∈X} = X.
Next suppose that [x]R ̸= [y]R. By Proposition 204(4) this implies that [x]R ∩
[y]R = ∅. This completes the proof.
5.2.27
.(b) To show that ˜R is well defined means, as usual, to show that the definition of
˜R does not depend on the particular choice of representatives of equivalence
classes. In particular, we need to show that if [x]∼= [x′]∼and [y]∼= [y′]∼,
then [x]∼˜R[y]∼iff [x′]∼˜R[y′]∼. To that purpose, suppose that [x]∼= [x′]∼
and [y]∼= [y′]∼and that [x]∼˜R[y]∼. We have to show that [x′]∼˜R[y′]∼.
Since [x]∼˜R[y]∼we have xRy. From the facts [x]∼= [x′]∼and [y]∼= [y′]∼
it follows by Proposition 204 that x ∼x′ and y ∼y′. Now applying the def-
inition of ∼to these last two facts we have xRx′, x′Rx, yRy′, and y′Ry.
Next applying the fact that R is transitive to the facts x′Rx and xRy yields
x′Ry, and then again applying the transitivity of R to the facts x′Ry and
yRy′ yields x′Ry′. But then, by the definition of ˜R, we have [x′]∼˜R[y′]∼as
we wanted. A symmetric argument proves that [x]∼˜R[y]∼if [x′]∼˜R[y′]∼.
Now that we have established that ˜R is well defined, we proceed to proving
that it is a partial order X/∼.
Reflexivity:
Since R is reflexive we have xRx for all x ∈X and hence that [x]∼˜R[x]∼
for all [x]∼∈X/∼.
Transitivity:
Suppose [x]∼˜R[y]∼and [y]∼˜R[z]∼. By the definition of ˜R we have xRy and
yRz and hence by the transitivity of R that xRz. Again, by applying the
definition of ˜R we get [x]∼˜R[z]∼.
Anti-symmetry:
Suppose that [x]∼˜R[y]∼and [y]∼˜R[x]∼. By the definition of ˜R we have xRy
and yRx and consequently, by the definition of ∼, that x ∼y. But then, by
Proposition 204, [x]∼= [y]∼.

344
Logic as a Tool
Exercises on ordered sets
5.2.33
.(a) We prove (1). Suppose that x and x′ are infima of Y . Then, since x is a lower
bound of Y we must have x ≤x′. Similarly, since x′ is a lower bound of Y
we must have x′ ≤x. Then x = x′, by the anti-symmetry of ≤.
5.2.34
It is sufficient to exhibit an infinite strictly descending chain of elements of P(X).
The key idea is that what remains after removing a single element from an infinite
set is again an infinite set.
5.2.35
.(a) (⇒) Suppose that (X, ≤) is well-founded but, for the sake of contradiction,
that there is a subset ∅̸= Y ⊆X such that Y has no minimal element. Take
an arbitrary element of Y , and call it y1. Since y1 cannot be a minimal element
of Y there must be another element, y2 say, such that y2 ∈Y and y1 > y2.
Again, since y2 cannot be a minimal element of Y , there must be another
element, y3 say, such that y3 ∈Y and y1 > y2 > y3. Continuing in this way
we can construct an infinite descending chain, contradicting our assumption
that (X, ≤) is well-founded. (This step of the proof requires a special axiom
of set theory know as the Axiom of Choice.)
(⇐)
We
prove
the
contrapositive.
Suppose
that
(X, ≤)
is
not
well-founded. If (X, ≤) is not well-founded it must contain an infinite
descending chain, say x1 > x2 > x3 > · · ·. Now let Y = {x1, x2, x3, . . . }.
Clearly, Y is a non-empty subset of X which does not have a least element.
5.2.36
Assume the contrary, that is, X \ P ̸= ∅. Then X \ P has a minimal element x.
Then all elements of X less than x belong to P, hence x must belong to P: a
contradiction.
Section 5.3
Exercises on Mathematical Induction
5.3.1
.1.1 For n = 0, 02 + 0 = 0, which is even. Our inductive hypothesis is that k2 +
k is even. Now, for n = k + 1,
(k + 1)2 + k + 1 = k2 + 2k + 1 + k + 1 = k2 + k + 2(k + 1).
We know that k2 + k is even from the inductive hypothesis. Besides, 2(k +
1) is also even, by definition. The sum of two even numbers is even, so k2 + k
is also even.
1.3 First, the set {1} has two subsets, namely {1} and ∅, so clearly, the
powerset of {1} has 21 elements. Our inductive hypothesis is that the
powerset of {1, 2, 3, . . . , k} has 2k elements for some k ≥1. Now, let
A = {1, 2, . . . , k, k + 1}. Choose an element a ∈A and set A′ = A −{a}.
Note that P(A) = {X ⊆A | a /∈X} ∪{X ⊆A | a ∈X}. It is clear that
these sets are disjoint, so to find the number of elements in P(A), we need

Answers and Solutions to Selected Exercises
345
only find the number of the elements in each of these sets and add them
together. First, clearly P(A′) = {X ⊆A | a /∈X} and so, since A′ has k
elements, {X ⊆A | a /∈X} has 2k elements by the inductive hypothesis.
Next, note that X = Y ∪{a} for all X ∈{X ⊆A | a ∈X}, where
Y ∈P(A′). Since there are k elements in the set A′, there are 2k such Y by
the inductive hypothesis. Hence, the set {X ⊆A | a ∈X} has 2k elements.
In total, P(A) therefore has 2k + 2k = 2 · 2k = 2k+1 elements.
1.5 For n = 1 both sides equal 1. The inductive hypothesis is that
1 + 3 + 5 + · · · + 2k −1 = k2
for some k ≥1. Then, for n = k + 1 we have
1 + 3 + 5 + · · · + 2k −1 + 2(k + 1) −1 = k2 + 2k + 2 −1
= k2 + 2k + 1
= (k + 1)2.
1.7 For n = 1 we have 13 = 1 and 12(1+1)2
4
= 4
4 = 1.
The inductive hypothesis is that
13 + 23 + 33 + · · · + k3 = k2(k + 1)2
4
for some k ≥1. Then, for n = k + 1 we have
13 + 23 + 33 + · · · + k3 + (k + 1)3 = k2(k + 1)2
4
+ (k + 1)3
= k2(k + 1)2 + 4(k + 1)3
4
= (k + 1)2(k2 + 4(k + 1))
4
= (k + 1)2(k2 + 4k + 4)
4
= (k + 1)2(k + 2)2
4
1.9 For n = 1 we have
1
1×3 = 1
3 and
3+5
4(1+1)(1+2) =
8
24 = 1
3.
The inductive hypothesis is
1
1 × 3 +
1
2 × 4 + · · · +
1
k × (k + 2) =
k(3k + 5)
4(k + 1)(k + 2)

346
Logic as a Tool
for some k ≥1. Then, for n = k + 1 we have
1
1 × 3 +
1
2 × 4 + · · · +
1
k × (k + 2) +
1
(k + 1)(k + 3)
=
k(3k + 5)
4(k + 1)(k + 2) +
1
(k + 1)(k + 3) = k(3k + 5)(k + 3) + 4(k + 2)
4(k + 1)(k + 2)(k + 3)
= 3k3 + 14k2 + 19k + 8
4(k + 1)(k + 2)(k + 3) = (k + 1)(3k2 + 11k + 8)
4(k + 1)(k + 2)(k + 3)
= (k + 1)(3k + 8)(k + 1)
4(k + 1)(k + 2)(k + 3) =
(k + 1)(3(k + 1) + 5)
4((k + 1) + 1)((k + 1) + 2).
5.3.2
Suppose there are natural numbers greater than or equal to n0, for which P does
not hold. The set of such numbers has a least element m. First, m must be greater
than n0, since P does hold for n0. Since m is the least natural number for which
P does not hold, P must hold for all natural numbers less than m. Hence, by
assumption, P must holds for m, which is a contradiction.
5.3.3
First, assume the PCMI holds. Now, suppose that some property P holds for
some natural number n0 ≥0, and whenever P holds for some natural number
m ≥n0, it holds for m + 1. Then if P holds for all natural numbers less that
m + 1 and greater than or equal n0, then it also holds for m + 1. By PMCI, P
therefore holds for every natural number greater than or equal to n0. We have
therefore proved the PMI.
Conversely, assume the PMI holds. We will prove the PCMI as follows. Sup-
pose that the property P holds for some natural number n0, and for any natural
number k ≥n0, whenever P holds for all natural numbers greater than or equal to
n0 and less than k, then P holds for k. We consider the property P ∗where P ∗(n)
means that P(n) holds for all natural numbers greater than or equal to n0 and
less than n. We will prove using the PMI that P ∗(n) holds for all natural num-
bers greater than or equal to n0 which, by the assumption above, will imply that
P(n) holds for all natural numbers greater than or equal to n0. Indeed, P ∗(n0)
holds vacuously, because there are no natural numbers greater than or equal to
n0 and less than n0. Now, suppose that P ∗(k) holds for some k ≥n0. To show
that P ∗(k + 1) also holds, it is sufficient to note first that P (k + 1) holds by the
starting assumption and the assumption that P ∗(k) holds, and second that the
natural numbers greater than or equal to n0 and less than k + 1 are precisely k
and all those that are greater than or equal to n0 and less than k. P therefore holds
for all of them, hence P ∗(k + 1) holds. This completes the proof.
5.3.4
Suppose not. Then there must be a least non-interesting natural number. But then,
this makes it quite interesting, no?

Answers and Solutions to Selected Exercises
347
Exercises on Peano Arithmetic
Here are just two sample proofs, using the Induction Principle formalized by the Induction
Scheme and previous exercises from the list.
5.3.7.17
Proof by induction on y.
When y = 0 the claim holds because:
(a) PA ⊢x ≤0 ↔x = 0, by definition of ≤, (11), and (12);
(b) PA ⊢¬(x < 0) because of the above.
Now, we are to prove PA ⊢(x ≤y ↔(x < y ∨x = y)) →(x ≤s(y) ↔
(x < s(y) ∨x = s(y))). The strategy will now be to prove the consequent
(without essential use of the antecedent). To simplify the argument we reason
partly semantically (skipping some propositional steps) by showing the
following.
(a) PA ⊢x ≤s(y) ↔(x ≤y ∨x = s(y)) (see Exercise 7.16).
(b) PA ⊢x < s(y) ↔(x ≤s(y) ∧x ̸= s(y)) by definition of <.
(c) Therefore PA ⊢(x < s(y) ∨x = s(y)) ↔((x ≤s(y)∧x ̸= s(y))∨x =
s(y)).
(d) PA ⊢((x ≤s(y) ∧x ̸= s(y)) ∨x = s(y)) ↔(x ≤y ∨x = s(y))
because ((A ∧¬B) ∨B) ↔(A ∨B) is a tautology.
This completes the induction.
5.3.7.20
Proof by induction on y.
PA ⊢0 ̸= 0 →x ≤x × 0 holds because of the false antecedent.
To prove PA ⊢(y ̸= 0 →x ≤x × y) →(s(y) ̸= 0 →x ≤x × s(y)) it
is
sufficient
to
prove
PA ⊢(y ̸= 0 →x ≤x × y) →(x ≤x × s(y)).
This
holds
because
PA ⊢x × s(y) = x × y + x
by
(PA6)
and
PA ⊢x ≤x × y + x by definition of ≤and commutativity of +.
This completes the induction.
Section 5.4
5.4.3
.(a) (Q1) :- ancestor(Elisabeth, Harry).
Answer: “No”.
(b) (Q2) :- ancestor(Diana, Y).
Answer: {William, Harry, George}.
(c) (Q3) :- ancestor(X, Harry).
Answer: {Diana}.
(d) (Q4) :- ancestor(X, George).
Answer: {William, Diana, Charles, Elisabeth}.

References
D’Agostino, M., Gabbay, D.M., Hähnle, R., and Posegga, J. (eds.) 1999. Handbook of Tableau Methods.
Kluwer Academic Publishing, Dordrecht.
Barwise, J. and Echemendy, J. 1999. Language, Proof and Logic. CSLI Publication, Stanford.
Ben-Ari, M. 2012. Mathematical Logic for Computer Science, third edition. Springer, New York.
van Benthem, J., van Ditmarsch, H., van Eijck, J., and Jaspars, J. 2014. Logic in Action. Open Course
Project, University of Amsterdam, http://www.logicinaction.org/docs/lia.pdf (accessed 7 April 2016).
Boole, G. 2005. An Investigation of the Laws of Thought. Project Gutenberg, eBook [#15114].
Boolos, G., Burgess, J., and Jeffrey, R. 2007. Computability and Logic, fifth edition. Cambridge Univer-
sity Press, Cambridge.
Bornat, R. 2005. Proof and Disproof in Formal Logic. Oxford Texts in Logic, Oxford University Press,
Oxford.
Carroll, L. 1886. The Game of Logic. MacMillan and Co., London. Available from the Project Gutenberg
as Ebook at http://www.gutenberg.org/ebooks/4763 (Accessed 6 April 2016).
Carroll, L. 1897. Symbolic Logic. MacMillan and Co., London. Available from the Project Gutenberg as
Ebook at http://www.gutenberg.org/files/28696/28696-h/28696-h.htm (accessed 6 April 2016).
Chang, C.-L. and Lee, R. C.-T. 1997. Symbolic Logic and Mechanical Theorem Proving. Academic Press,
Cambride, MA.
Chiswell, I. and Hodges, W. 2007. Mathematical Logic. Oxford University Press, Oxford.
Conradie, W. and Goranko, V. 2015. Logic and Discrete Mathematics: A Concise Introduction. John
Wiley & Sons, Chichester.
Copi, I.M., Cohen, C., and McMahon, K. 2010. Introduction to Logic, 14th edition. Pearson, New York.
Curry, H. 1977. Foundations of Mathematical Logic. Dover Publications, Mineola.
van Dalen, D. 1983. Logic and Structure, second edition. Universitext, Springer, New York.
Logic as a Tool: A Guide to Formal Logical Reasoning, First Edition. Valentin Goranko.
© 2016 John Wiley & Sons, Ltd. Published 2016 by John Wiley & Sons, Ltd.

References
349
Day, M. 2012. An Introduction to Proofs and the Mathematical Vernacular. Virginia Polytechnic Insti-
tute and State University, Blacksburg. Available at http://www.math.vt.edu/people/day/ProofsBook
(accessed 6 April 2016).
Devlin, K. 2004. Sets, Functions, and Logic: An Introduction to Abstract Mathematics, third edition.
Chapman Hall/CRC, Boca Raton.
Devlin, K. 2012. Introduction to Mathematical Thinking. Keith Devlin Publishing, Standford.
Ebbinghaus, H.-D., Flum, J., and Thomas, W. 1996. Mathematical Logic, second edition. Springer,
New York.
Enderton, H. 2001. A Mathematical Introduction to Logic, second edition. Academic Press, New York.
Fitting, M. 1996. First-Order Logic and Automated Theorem Proving, second edition. Springer, New
York.
Gallier, J. 1986. Logic for Computer Science: Foundations of Automatic Theorem Proving, revised 2003/
Dover Publications, Mineola.
Gamut, L. T. F. (a collective pseudonym for Johan van Benthem, Jeroen Groenendijk, Dick de Jongh,
Martin Stokhof, and Henk Verkuyl) 1991. Logic, Language and Meaning, Volume I: Introduction to
Logic. University of Chicago Press, Chicago.
Garnier, R. and Taylor, J. 1996. 100 % Mathematical Proof. John Wiley & Sons, New York.
Halbach, V. 2010. The Logic Manual. Oxford University Press, Oxford.
Hamilton, A.G. 1988. Logic for Mathematicians, revised edition. Cambridge University Press, Cam-
bridge.
Hedman, S. 2004. A First Course in Logic. An Introduction to Model Theory, Proof Theory, Computabil-
ity, and Complexity. Oxford University Press, Oxford.
Hodges, W. 2001. Elementary predicate logic. In: Handbook of Philosophical Logic (eds D. M. Gabbay
and F. Guenthner), second edition. Kluwer Academic Publishers, Dordrecht, pp. 1–129.
Hodges, W. 2005. Logic, second edition. Penguin, London.
Huth, M. and Ryan, M. 2004. Logic in Computer Science Modelling and Reasoning about Systems,
second edition. Cambridge University Press, Cambridge.
Jeffrey, R. 1994. Formal Logic: Its Scope and Limits, third edition. McGraw-Hill, New York.
Kalish, D. and Montague, R. 1980. Logic: Techniques of Formal Reasoning, second edition. Harcourt
Brace Jovanovich, San Diego.
Kleene, S. C. 2002. Mathematical Logic, Dover Publications, USA. (Originally published by
North-Holland in 1962.)
Makinson, D. 2008. Sets, Logic and Maths for Computing. Springer, New York.
Mendelson, E. 1997. Introduction to Mathematical Logic, fourth edition. CRC Press, New York.
Nederpelt, R. and Kamareddine, F. 2004. Logical Reasoning: A First Course. Kings College, London.
Nerode, A. and Shore, R. 1993. Logic for Applications. Springer, New York.

350
References
Nilsson, U. and Małuszy´nski, J. 1995. Logic, Programming and Prolog, second edition. John Wiley &
Sons Ltd. Also available at http://www.ida.liu.se/∼ulfni53/lpp/ (accessed 6 April 2016).
van Oosten, J. 1999. Introduction to Peano Arithmetic. Utrecht University. Also available at http://www
.staff.science.uu.nl/∼ooste110/syllabi/peanomoeder.pdf} (accessed 6 April 2016).
Prawitz, D. 2006. Natural Deduction: A Proof-Theoretical Study. Dover Publications, USA. (Originally
published at Stockholm University in 1965.)
Robinson, A. and Voronkov, A. (eds.) 2001. Handbook of Automated Reasoning, volumes 1 and 2. Else-
vier.
Russell, S. and Norvig, P. 2003. Artificial Intelligence: A Modern Approach. Prentice-Hall, New York.
Shoenfield, J. 1967. Mathematical Logic. Addison-Wesley, Boston.
Smith, P. 2003. An Introduction to Formal Logic. Cambridge University Press, Cambridge.
Smith, P. 2013. An Introduction to Gödel’s Theorems, second edition. Cambridge University Press, Cam-
bridge.
Smullyan, R. 1995. First-Order Logic. Dover Publications, USA.
Smullyan, R. 1998. The Riddle of Scheherazade and Other Amazing Puzzles. Mariner Books, Boston,
MA.
Smullyan, R. 2009a. The Lady or the Tiger? and Other Logic Puzzles. Dover Publications, USA.
Smullyan, R. 2009b. Satan, Cantor and Infinity: Mind-Boggling Puzzles. Dover Publications, USA.
Smullyan, R. 2011. What Is the Name of This Book? The Riddle of Dracula and Other Logical Puzzles.
Dover Publications, USA.
Smullyan, R. 2013. The Gödelian Puzzle Book: Puzzles, Paradoxes and Proofs. Publications, USA.
Smullyan, R. 2014. A Beginner’s Guide to Mathematical Logic. Dover Publications, USA, Books on
Mathematics.
Solow, D. 1990. How to Read and Do Proofs: An Introduction to Mathematical Thought Processes.
John Wiley & Sons, New York.
Spivey, M. 1995. An Introduction to Logic Programming Through Prolog. Prentice-Hall International,
New York. Also available at http://spivey.oriel.ox.ac.uk/wiki/files/lp/logic.pdf (accessed 10 April
2016).
Tarski, A. 1965. Introduction to Logic and to the Methodology of Deductive Sciences. Oxford University
Press, Oxford.
Velleman, D. 2006. How To Prove It. (A Structured Approach), second edition. Cambridge University
Press, Cambridge.
Wos, L. T. and Robinson, G. A. 1969. Paramodulation and theorem proving in first-order theories with
equality. Elsevier, New York, Machine Intelligence Vol 4, pp. 135–150.

Index
α-rule, 60, 64
β-rule, 60, 64
absorption, 30, 31
Alan Turing, 261
Alfred Tarski, 122
algorithm, 57
Alonzo Church, 149
Anatoly Maltsev, 193
antecedent, 4
argument
logically correct, 21
propositional, 21
Aristotle, 51
assignment
falsifying, 20
truth, 10
variable, 112
associativity, 30, 31
assumption, 19, 48
cancellation, 69
discharge, 69
atomic
formula, 102
Augustus De Morgan, 32
auxiliary symbols, 7
axiom, 48
Choice, 233
Empty set, 232
Logic as a Tool: A Guide to Formal Logical Reasoning, First Edition. Valentin Goranko.
© 2016 John Wiley & Sons, Ltd. Published 2016 by John Wiley & Sons, Ltd.
Extensionality, 231
Foundation, 233
Infinity, 233
Pair set, 232
Powerset, 232
Regularity, 233
Replacement, 233
Restricted Comprehension, 233
Separation, 233
Union set, 232
Axiom of Choice, 190, 344
Axiom schemes
for H, 53
axiomatic set theory
Zermelo-Fraenkel, 231
Bertrand Russell, 134
biconditional, 2
bijection, 236
Boolean algebra, 236
bound variable, 126
branching rule, 60, 64
C.I. Lewis, 230
canonical mapping, 240
canonical structure
of a theory, 214
capture of a variable, 129
Cartesian product, 234

352
Index
chain, 241
ascending, 241
strictly, 241
descending, 241
strictly, 241
Charles Peirce, 229
Chrysippus, 18
Church’s Undecidability
Theorem, 159
clausal form, 79
first-order, 190
clause, 79
empty, 79
first-order, 190
unit, 79
Horn, 256
closed formula, 127
closure of a relation
reflexive, 239
symmetric, 239
transitive, 239
cluster, 240
CNF
first-order, 187
co-domain
of a function, 236
Commutativity, 30, 31
complete mathematical induction,
principle of, 249
complexity class NP, 86
conclusion, 19
conditional, 2
congruence, 29
in a structure, 212
conjunction, 2
conjunctive normal form (CNF), 77
first-order, 187
prenex, 187
connective
main, 104
consequent, 4
constant, 98
Skolem, 189
construction sequence
of a formula, 8
construction tree, 8, 38
constructor operation, 37
contradictory pair of signed formulae, 60
counter-model, 138
Dag Prawitz, 186
deductive consequence
in D, 48
compact, 92
deductive system
adequate, 50, 51
complete, 50
complete1, 90
complete2, 90
correct, 50
sound, 50
sound1, 90
sound2, 90
definition
inductive, 7
recursive, 7
deletion
of repeating literals, 81
subsumption, 82
tautology, 82
derivation
from set of assumptions, 48
in H, 54
in ST, 61
resolution-based, 80, 198
Diodorus Cronus, 17
direct proof, 224
disbributivity, 31
disjunction, 2
disjunctive normal form (DNF), 77
first-order, 187
prenex, 187
Distributivity, 30
DNF
first-order, 187
domain
of a function, 236
of a relation, 237
domain of discourse, 97
element
maximal, 241
minimal, 241

Index
353
elementary conjunction, 77
first-order, 187
elementary disjunction, 77
first-order, 187
elimination rule, 69
Emil Post, 95
empty set, 232
Epimenides, 17
equality, 140, 238
in first-order logic, 140
equivalence class, 240
equivalence relation, 239, 240
equivalent replacement, 29
Ernst Schr¨oder, 245
Eubulides, 17
Euclid, 57
Euclid of Megara, 17
evaluation game, 115
configuration, 115
initial, 115
winning strategy, 116
Evert Beth, 67
Ex falso sequitur quodlibet, 69
Existential Import, 156
falsifiable, 12
Falsifier, 115
first-order
instance, 105
sentence, 127
structure, 97, 98
CNF, 187
conjunctive normal form, 187
disjunctive normal form, 187
DNF, 187
elementary conjunction, 187
elementary disjunction, 187
formula, 103
logically valid, 136
satisfiable, 136
language, 99
literal, 187
logic, 96
logical consequence, 137
quantifier, 100
term, 100
theory, 211
first-order formulae
logically equivalent, 142
formation rules, 7
formula
atomic, 102
clean, 128
closed, 127
compound, 103
contradictory, 12
derivable
in ST, 61
in H, 54
evaluation game, 115
first-order, 103
logically valid, 136
satisfiable, 136
ground, 103
logical, 108
open, 127
propositional, 6
satisfiable, 10
tautology, 11
valid, 11
scheme, 52
signed, 12
free variable, 126
function, 97, 236
bijective, 236
co-domain, 236
composition, 236
domain, 236
graph of, 239
image, 236
injective, 141, 236
inverse, 236
pre-image, 236
range, 236
Skolem, 189
surjective, 236
target set, 236
total, 97
unary,binary, n-ary, 97
value, 236
function:graph, 236
G¨odel’s completeness theorem, 159, 220
G¨odel’s incompleteness theorems, 220

354
Index
Georg Cantor, 245
George Boole, 26
Gerhard Gentzen, 75
Giuseppe Peano, 253
Gottlob Frege, 107
graph of a function, 239
greatest lower
bound, 241
Henkin
theory, 214
extension of a theory
maximal theory, 216
model, 215
Herbrand
interpretation, 212
structure, 212
universe, 194, 212
Hilary Putnam, 85
Hilbert-style
system, 52
Hugh MacColl, 33
Hyperresolution, 203
Idempotency, 30, 31
identity, 140, 238
implication, 2
contrapositive, 24
converse, 24
falacy, 24
derivative, 24
inverse, 24
falacy, 24
indirect proof, 224
induction principle, 40
inductive hypothesis,
248
inference
propositional, 21
inference rule
instance, 21
premise of, 21
propositional, 21
infimum, 241
initial elements, 37
injection, 236
instance
first-order, 105
interpretation, 108
standard, 111
intersection, 234
introduction rule, 69
intuitionistic logic, 70
Jaakko Hintikka, 68
Jacques Herbrand, 208
Jan Łukasiewicz, 94
John Alan Robinson, 209
John Venn, 158
Kurt G¨odel, 220
language
first-order, 99
propositional, 6
law
of excluded middle, 11
of non-contradiction, 11
De Morgan, 28
logical, 11
least upper bound, 241
Leon Henkin, 221
Leonid Levin, 88
Lewis Carroll, 158
linear order, 239
literal, 77
first-order, 187, 190
logic
classical, 96
predicate, 96
first-order, 96
logic program, 256
logic programming, 255
rule
body, 256
clause-based, 256
head, 256
logical connective
propositional, 2
logical consequence
first-order, 137
propositional, 18

Index
355
logically equivalent
first-order formulae, 142
propositional formulae, 28
lower bound, 241
greatest, 241
Ludwig Wittgenstein, 45
main connective, 7, 104
main subformula, 7
many-sorted structure, 105
mapping, 236
Martin Davis, 85
Mathematical Induction, 246
mathematical induction,
principle of, 249
matrix
of prenex form, 187
Megarian school, 17
Melvin Fitting, 179
model
of theory, 211
model checking game, 115
Modus ponens, 53
most general unifier, 196
name, 98
Natural Deduction, 69
negation, 2
negation normal form, 144
negation-by-failure, 258
non-branching rule, 60, 64
non-unifiable
terms, 196
normal form
negation, 144
NP-complete decision
problem, 86
numeral
for n, 101
open formula, 127
order
lexicographic, 242
linear, 239
strict, 239
partial, 239, 240
strict, 239
total, 239
strict, 239
ordered pair, 234
ordering
lexicographic, 40
well-founded, 40
well-ordering, 40
ordering strategies, 203
paradox
Berry, i, xxi
Grelling-Nelson, ii, xxii
Jourdain’s card, i, xxi
Liar, i, xxi
Russell, 231
paramodulant, 201
Paramodulation, 201
parsing tree, 9
partial order, 239, 240
induced, 241
partially ordered set, 240
partition, 240
Peano Arithmetic, 247
Philo of Megara, 17
polynomial time, 86
deterministic, 86
non-deterministic, 86
poset, 240, 242
well-founded, 242
powerset, 232
powerset algebra, 236
pre-order, 239, 240
predicate, 97
unary,binary, n-ary, 97
prefix
of prenex form, 187
premise, 48
prenex form, 187
principle
of descending chains,
249
of well-ordering, 249
Prolog, 255
program, 256
answer, 257
solution, 257
query, 256

356
Index
Prolog (continued)
rule
body, 256
fact, 256
head, 256
proof
by assumption of the contrary, 224
by induction, 36
non-constructive, 228
tactics, 227
proof by contradiction, 224
proof by contraposition, 226
proof strategy, 223
proposition, 1
atomic, 2
compound, 2
primitive, 2
propositional
argument, 21
inference, 21
theory, 89
propositional constants, 6
quantification
restricted, 124
quantifier, 99
existential, 100
first-order, 100
universal, 100
Quantifier rules
for Semantic Tableaux, 167
quasi-order, 239
quotient set, 240
quotient-element, 240
quotient-structure, 213
range
of a function, 236
of a relation, 237
rank of an element, 38
Raymond Smullyan, 178
recursion theory, 150
Reductio ad absurdum, 69
refutation-complete, 202
relation
antisymmetric, 239
asymmetric, 239
binary, 237
complementation, 238
composition, 238
connected, 239
diagonal, 238
domain, 237
empty, 238
equivalence, 29, 239
euclidean, 239
functional, 141, 239
image, 237
inverse, 238
inverse image, 237
irreflexive, 239
pre-order, 239
quasi-order, 239
range, 237
reflexive, 29, 239
restriction of, 238
serial, 239
symmetric, 29, 239
transitive, 29, 239
universal, 238
renaming of a variable, 128
Resolution
Propositional, 77
rule
Propositional, 77
Clausal, 80
derivation, 80
ground, 194
Hyperresolution, 203
Input, 203
linear, 203
Selective Linear Definite Clause, 203
Set-of-support, 203
SLD, 203
Unit, 203
with unification, 194
resolution-based derivation, 198
resolvent, 77, 80
rule
of propositional inference, 21
conclusion of, 21
Detachment, 21
elimination, 69
Factoring, 198

Index
357
for the equality
for Semantic Tableaux, 171, 180
formula decomposition, 59
Generalization, 160
inference, 21
introduction, 69
logically sound, 21
Modus Ponens, 21
of inference, 48
scope of quantifier, 126
Semantic Tableaux, 58
quantifier rules, 167
rules for the equality, 171, 180
signed version, 59
unsigned version, 64
semantics
of first-order logic, 108
sentence, 127
set
closed, 38
closed under operation, 38
inductive, 38
inductively defined, 39
ordered
well-founded, 40
successor, 233
set-builder notation, 231
Skolem
constant, 189
function, 189
sort, 105
Square of Opposition, 152
Stanisław Ja´skowski, 76
Stephen Cook, 88
Stephen Kleene, 150
Stoic school, 18
structure
of discourse, 108
first-order, 98
for a theory
canonical, 214
subformula, 104
propositional, 7
subset
proper, 234
substitution
domain of, 195
instance, 52
most general unifier, 196
of a term for a variable, 195
result, 128
unifier, 196
uniform, 9, 52
subterm, 102
supremum, 241
surjection, 236
syllogism, 151
categorical, 151
Syllogistic, 151
syllogistic form, 153
figure, 153
syllogistic proposition, 152
conclusion, 153
predicate, 152
premise, 153
major, 153
minor, 153
subject, 152
term, 152
major, 153
middle, 153
minor, 153
symbol
auxiliary, 99
constant, 99
functional, 99
logical, 99
non-logical, 99
predicate, 99
tableau, 59
branch
closed, 60, 64
open, 60, 64
saturated, 60
closed, 61
root, 59
tableaux
free variable, 174
tautology, 11
term
evaluation, 112
constant, 101

358
Index
term (continued)
first-order, 100
free for substitution for
a variable, 129
ground, 101
value for a variable assignment, 112
value in a structure, 111
theorem
of H, 54
of ST, 61
of D, 48
theorem proving
automated, 254
interactive, 255
theory
complete, 90
consistent, 89
first-order, 211
Henkin extension
maximal, 216
inconsistent, 89
maximal consistent, 89
model of, 211
propositional, 89
satisfiable, 211
unsatisfiable, 211
Thoralf Skolem, 192
total order, 239
tree
height, 38
truth
of formula in a structure under a
variable assignment, 109, 112
truth assignment
falsifying, 12
truth tables, 3
truth valuation, 41
truth value, 1
unifiable
literals, 196
terms, 196
unifier
substitution, 196
union, 232
unique generation, 43
unique readability property, 43,
105
universal closure, 136
universal set, 236
universe, 37
upper bound, 241
least, 241
value
of a term for a variable assignment,
112
of term in a structure, 111
variable
assignment, 112
bound, 126
existentially, 126
universally, 126
free, 126
individual, 99
propositional, 7
variant assignment, 113
Venn diagram, 153
Verifier, 115
well-ordering, 40, 242
principle, 249
Wilfrid Hodges, 123
William Jevons, 27
witness, 180
Zeno of Citium, 18

WILEY END USER LICENSE AGREEMENT
Go to www.wiley.com/go/eula to access Wiley’s ebook EULA.

