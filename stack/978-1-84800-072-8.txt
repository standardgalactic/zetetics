
Undergraduate Topics in Computer Science 
 
 
 
 
 
 
 

Undergraduate Topics in Computer Science' (UTiCS) delivers high-quality instructional content for 
undergraduates studying in all areas of computing and information science.  From core foundational 
and theoretical material to final-year topics and applications, UTiCS books take a fresh, concise, and 
modern approach and are ideal for self-study or for a one- or two-semester course.  The texts are all 
authored by established experts in their fields, reviewed by an international advisory board, and contain 
numerous examples and problems. Many include fully worked solutions. 
 
Also in this series 
 
Iain D. Craig 
Object-Oriented Programming Languages: Interpretation 
978-1-84628-773-2 
 
Max Bramer 
Principles of Data Mining 
978-1-84628-765-7 
 
Hanne Riis Nielson and Flemming Nielson 
Semantics with Applications: An Appetizer 
978-1-84628-691-9 
 
Michael Kifer and Scott A. Smolka 
Introduction to Operating System Design and Implementation: The OSP 2 Approcah 
978-1-84628-842-5 
 
Phil Brooke and Richard Paige 
Practical Distributed Processing 
978-1-84628-840-1 
 
Frank Klawonn 
Computer Graphics with Java 
978-1-84628-847-0 

David Salomon  
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
A Concise Introduction
to Data Compression

Professor David Salomon (emeritus) 
Computer Science Department 
California State University 
Northridge, CA 91330-8281, USA 
 
Series editor 
Ian Mackie, École Polytechnique, France and King's College London, UK 
 
Advisory board 
Samson Abramsky, University of Oxford, UK 
Chris Hankin, Imperial College London, UK 
Dexter Kozen, Cornell University, USA 
Andrew Pitts, University of Cambridge, UK 
Hanne Riis Nielson, Technical University of Denmark, Denmark 
Steven Skiena, Stony Brook University, USA 
Iain Stewart, University of Durham, UK 
David Zhang, The Hong Kong Polytechnic University, Hong Kong 
  
 
British Library Cataloguing in Publication Data 
A catalogue record for this book is available from the British Library 
 
 
Undergraduate Topics in Computer Science ISSN 1863-7310 
ISBN 978-1-84800-071-1 
e-ISBN 978-1-84800-072-8 
 
© Springer-Verlag London Limited 2008 
 
Apart from any fair dealing for the purposes of research or private study, or criticism or review, as permitted 
under the Copyright, Designs and Patents Act 1988, this publication may only be reproduced, stored or 
transmitted, in any form or by any means, with the prior permission in writing of the publishers, or in the 
case of reprographic reproduction in accordance with the terms of licences issued by the Copyright Licensing 
Agency.  Enquiries concerning reproduction outside those terms should be sent to the publishers. 
 
The use of registered names, trademarks, etc. in this publication does not imply, even in the absence of a 
specific statement, that such names are exempt from the relevant laws and regulations and therefore free for 
general use. 
 
The publisher makes no representation, express or implied, with regard to the accuracy of the information 
contained in this book and cannot accept any legal responsibility or liability for any errors or omissions that 
may be made.   
 
Printed on acid-free paper 
 
9  8  7  6  5  4  3  2  1 
 
springer.com 
Library of Congress Control Number:  2007939563 
email: david.salomon@csun.edu

This book is dedicated to you, the reader!
Nothing is more impossible than to write
a book that wins every reader’s approval.
—Miguel de Cervantes

Preface
It is virtually certain that a reader of this book is both a computer user and an Internet
user, and thus the owner of digital data.
More and more people all over the world
generate, use, own, and enjoy digital data. Digital data is created (by a word processor,
a digital camera, a scanner, an audio A/D converter, or other devices), it is edited
on a computer, stored (either temporarily, in memory, less temporarily, on a disk, or
permanently, on an optical medium), transmitted between computers (on the Internet
or in a local-area network), and output (printed, watched, or played, depending on its
type).
These steps often apply mathematical methods to modify the representation of the
original digital data, because of three factors, time/space limitations, reliability (data
robustness), and security (data privacy). These are discussed in some detail here:
The ﬁrst factor is time/space limitations. It takes time to transfer even a single
byte either inside the computer (between the processor and memory) or outside it over
a communications channel. It also takes space to store data, and digital images, video,
and audio ﬁles tend to be large. Time, as we know, is money. Space, either in memory
or on our disks, doesn’t come free either. More space, in terms of bigger disks and
memories, is becoming available all the time, but it remains ﬁnite. Thus, decreasing the
size of data ﬁles saves time, space, and money—three important resources. The process
of reducing the size of a data ﬁle is popularly referred to as data compression, although
its formal name is source coding (coding done at the source of the data, before it is
stored or transmitted).
In addition to being a useful concept, the idea of saving space and time by com-
pression is ingrained in us humans, as illustrated by (1) the rapid development of nan-
otechnology and (2) the quotation at the end of this Preface.
The second factor is reliability. We often experience noisy telephone conversations
(with both cell and landline telephones) because of electrical interference. In general,
any type of data, digital or analog, sent over any kind of communications channel may
become corrupted as a result of channel noise. When the bits of a data ﬁle are sent
over a computer bus, a telephone line, a dedicated communications line, or a satellite
connection, errors may creep in and corrupt bits. Watching a high-resolution color image
or a long video, we may not be able to tell when a few pixels have wrong colors, but other

viii
Preface
types of data require absolute reliability. Examples are an executable computer program,
a legal text document, a medical X-ray image, and genetic information. Change one bit
in the executable code of a program, and the program will not run, or worse, it may run
and do the wrong thing. Change or omit one word in a contract and it may reverse its
meaning. Reliability is therefore important and is achieved by means of error-control
codes. The formal name of this mathematical discipline is channel coding, because these
codes are employed when information is transmitted on a communications channel.
The third factor that aﬀects the storage and transmission of data is security. Gener-
ally, we do not want our data transmissions to be intercepted, copied, and read on their
way. Even data saved on a disk may be sensitive and should be hidden from prying eyes.
This is why digital data can be encrypted with modern, strong encryption algorithms
that depend on long, randomly-selected keys. Anyone who doesn’t possess the key and
wants access to the data may have to resort to a long, tedious process of either trying
to break the encryption (by analyzing patterns found in the encrypted ﬁle) or trying
every possible key. Encryption is especially important for diplomatic communications,
messages that deal with money, or data sent by members of secret organizations. A
close relative of data encryption is the ﬁeld of data hiding (steganography). A data ﬁle
A (a payload) that consists of bits may be hidden in a larger data ﬁle B (a cover) by
taking advantage of “holes” in B that are the result of redundancies in the way data is
represented in B.
Overview and goals
This book is devoted to the ﬁrst of these factors, namely data compression.
It
explains why data can be compressed, it outlines the principles of the various approaches
to compressing data, and it describes several compression algorithms, some of which are
general, while others are designed for a speciﬁc type of data.
The goal of the book is to introduce the reader to the chief approaches, methods,
and techniques that are currently employed to compress data. The main aim is to start
with a clear overview of the principles behind this ﬁeld, to complement this view with
several examples of important compression algorithms, and to present this material to
the reader in a coherent manner.
Organization and features
The book is organized in two parts, basic concepts and advanced techniques. The
ﬁrst part consists of the ﬁrst three chapters. They discuss the basic approaches to data
compression and describe a few popular techniques and methods that are commonly
used to compress data. Chapter 1 introduces the reader to the important concepts of
variable-length codes, preﬁx codes, statistical distributions, run-length encoding, dictio-
nary compression, transforms, and quantization. Chapter 2 is devoted to the important
Huﬀman algorithm and codes, and Chapter 3 describes some of the many dictionary-
based compression methods.
The second part of this book is concerned with advanced techniques. The original
and unusual technique of arithmetic coding is the topic of Chapter 4.
Chapter 5 is
devoted to image compression. It starts with the chief approaches to the compression of
images, explains orthogonal transforms, and discusses the JPEG algorithm, perhaps the
best example of the use of these transforms. The second part of this chapter is concerned

Preface
ix
with subband transforms and presents the WSQ method for ﬁngerprint compression as
an example of the application of these sophisticated transforms. Chapter 6 is devoted
to the compression of audio data and in particular to the technique of linear predic-
tion. Finally, other approaches to compression—such as the Burrows–Wheeler method,
symbol ranking, and SCSU and BOCU-1—are given their due in Chapter 7.
The many exercises sprinkled throughout the text serve two purposes, they illumi-
nate subtle points that may seem insigniﬁcant to readers and encourage readers to test
their knowledge by performing computations and obtaining numerical results.
Other aids to learning are a prelude at the beginning of each chapter and various
intermezzi where interesting topics, related to the main theme, are examined. In addi-
tion, a short summary and self-assessment exercises follow each chapter. The glossary
at the end of the book is comprehensive, and the index is detailed, to allow a reader to
easily locate all the points in the text where a given topic, subject, or term appear.
Other features that liven up the text are puzzles (indicated by
, with answers at
the end of the book) and various boxes with quotations or with biographical information
on relevant persons.
Target audience
This book was written with undergraduate students in mind as the chief readership.
In general, however, it is aimed at those who have a basic knowledge of computer science;
who know something about programming and data structures; who feel comfortable with
terms such as bit, mega, ASCII, ﬁle, I/O, and binary search; and who want to know how
data is compressed. The necessary mathematical background is minimal and is limited
to logarithms, matrices, polynomials, calculus, and the concept of probability.
This
book is not intended as a guide to software implementors and has few programs.
The book’s web site, with an errata list, BibTEX information, and auxiliary material,
is part of the author’s web site, located at http://www.ecs.csun.edu/~dsalomon/.
Any errors found, comments, and suggestions should be directed to dsalomon@csun.edu.
Acknowlegments
I would like to thank Giovanni Motta and John Motil for their help and encourage-
ment. Giovanni also contributed to the text and pointed out numerous errors.
In addition, my editors at Springer Verlag, Wayne Wheeler and Catherine Brett,
deserve much praise. They went over the manuscript, made numerous suggestions and
improvements, and contributed much to the ﬁnal appearance of the book.
Lakeside, California
David Salomon
August 2007
To see a world in a grain of sand
And a heaven in a wild ﬂower,
Hold inﬁnity in the palm of your hand
And eternity in an hour.
—William Blake, Auguries of Innocence

Contents
Preface
vii
Part I: Basic Concepts
1
Introduction
5
1
Approaches to Compression
21
1.1
Variable-Length Codes
25
1.2
Run-Length Encoding
41
Intermezzo: Space-Filling Curves
46
1.3
Dictionary-Based Methods
47
1.4
Transforms
50
1.5
Quantization
51
Chapter Summary
58
2
Huﬀman Coding
61
2.1
Huﬀman Encoding
63
2.2
Huﬀman Decoding
67
2.3
Adaptive Huﬀman Coding
76
Intermezzo: History of Fax
83
2.4
Facsimile Compression
85
Chapter Summary
90
3
Dictionary Methods
93
3.1
LZ78
95
Intermezzo: The LZW Trio
98
3.2
LZW
98
3.3
Deﬂate: Zip and Gzip
108
Chapter Summary
119

xii
Contents
Part II: Advanced Techniques
121
4
Arithmetic Coding
123
4.1
The Basic Idea
124
4.2
Implementation Details
130
4.3
Underﬂow
133
4.4
Final Remarks
134
Intermezzo: The Real Numbers
135
4.5
Adaptive Arithmetic Coding
137
4.6
Range Encoding
140
Chapter Summary
141
5
Image Compression
143
5.1
Introduction
144
5.2
Approaches to Image Compression
146
Intermezzo: History of Gray Codes
151
5.3
Image Transforms
152
5.4
Orthogonal Transforms
156
5.5
The Discrete Cosine Transform
160
Intermezzo: Statistical Distributions
178
5.6
JPEG
179
Intermezzo: Human Vision and Color
184
5.7
The Wavelet Transform
198
5.8
Filter Banks
216
5.9
WSQ, Fingerprint Compression
218
Chapter Summary
225
6
Audio Compression
227
6.1
Companding
230
6.2
The Human Auditory System
231
Intermezzo: Heinrich Georg Barkhausen
234
6.3
Linear Prediction
235
6.4
µ-Law and A-Law Companding
238
6.5
Shorten
244
Chapter Summary
245
7
Other Methods
247
7.1
The Burrows–Wheeler Method
248
Intermezzo: Fibonacci Codes
253
7.2
Symbol Ranking
254
7.3
SCSU: Unicode Compression
258
Chapter Summary
263
Bibliography
265
Glossary
271

Contents
xiii
Solutions to Puzzles
281
Answers to Exercises
283
Index
305
The content of most textbooks is perishable, but the
tools of self-directness serve one well over time.
—Albert Bandura

Part I:
Basic Concepts
Our everyday experience suggests that compression is an option that we naturally select
when faced with problems of high cost or restricted space. The following points illustrate
how such problems have been solved throughout history by resorting to (often intuitive)
compression:
In ancient Greece, manuscripts were written on papyrus, which was then very ex-
pensive. As a result, writers tried to squeeze more text in a given space by eliminating
punctuations and interword spaces (Figure 1).
In ancient Rome, people went around the high cost of tombstone engraving by
resorting to acronyms, the most common of which were S.T.L. (Sit Terra Levit, or let
the earth rest lightly upon her), D.M. (Dis Manibus, or to the ghosts of the underworld),
and B.M. (Bene Merenti, or to one deserving well).
Figure 1: Greek Papyrus and Ancient Coin.

2
Part I: Basic Concepts
In the middle ages, praise for the current ruler was often stamped onto coins in the
form of acronyms, because of the tight space available (Figure 1).
In a natural language, common words tend to be shorter than rarely-used words.
It is hard to imagine a language where the word for, say, yes would be as long as
encyclopedia or the word for establishment would be as short as me.
We are familiar with the term “ﬁne print.” This term is often used to hide unfriendly
clauses in a contract, or negative aspects of an item being advertised.
Sometimes,
however, small print is simply used to save space, as is common in newspapers.
The Arabic numerals that we use are based on weights assigned to positions in
the number. Thus, the digit 4 in 24,806 has a weight of 103 = 1000, so its value is
4,000. This numbering system has many advantages, not the least of which is that the
numbers are short.
They are shorter than Roman numerals and much shorter than
stone-age numerals (see the discussion of stone-age binary in Section 1.1.1).
The well-known Morse code (Section 1.1) assigns short codes to common letters,
such as E and T and long codes to rare letters, such as Z and Q. This is an early example
of intuitive text compression.
The six-shutter telecommunication system, used by the British admiralty in the
19th century, could transmit 64 diﬀerent symbols, more than enough for the letters and
digits. The extra symbols were assigned to common words. This system is described in
[Holzmann and Pehrson 95] and its application to compression is mentioned in [Bell et
al. 90].
A similar system is the well-known Braille code for the blind. Developed by Louis
Braille in the 1820s, this code consists of groups (or cells) of 3 × 2 dots each, embossed
on thick paper. Each of the six dots in a group may be ﬂat or raised, implying that the
information content of a group is equivalent to six bits, resulting in 64 possible groups.
Once appropriate codes are assigned to the letters, digits, and common punctuation
marks, several groups remain and may be used to code common words—such as and,
for, and of—and common strings of letters—such as ound, ation, and th.
Scientists often claim that the chief aim of science is to explain as many known
facts as possible by deduction from as few assumptions (or axioms) as possible. This is
an example of economy of expression.
Similarly, Occam’s razor (attributed to the 14th-century logician William of Ock-
ham) is a principle which states that the explanation of a phenomenon should make
as few assumptions as possible (entia non sunt multiplicanda praeter necessitatem, or
entities should not be multiplied beyond necessity).
The ﬁrst part of this book consists of the ﬁrst three chapters. They discuss the basic
approaches to data compression and describe a few popular techniques and methods
that are commonly used to compress data.
Chapter 1 introduces the reader to the
important concepts of variable-length codes, preﬁx codes, statistical distributions, run-
length encoding, dictionary compression, transforms, and quantization. Chapter 2 is

Part I: Basic Concepts
3
devoted to the important Huﬀman algorithm and codes, and Chapter 3 describes some
of the many dictionary-based compression methods.
There are four basic food groups: milk chocolate, dark
chocolate, white chocolate, and chocolate truﬄes.
—Anonymous

Introduction
The modern discipline of data compression is concerned with reducing the size of digital
binary data. A data compression algorithm inputs a bitstream (a disk ﬁle or bits read
from a network) and outputs a shorter bitstream. Most of the physical objects surround-
ing us are diﬃcult or impossible to shrink (or are damaged when forcibly compressed),
so shrinking the size of a data ﬁle may seem like magic (or perhaps like cheating). Thus,
before we try to explain how data is compressed, it is important to explain why it can
be compressed. The key to compressing data is the distinction between data and infor-
mation. Data is how information is represented; it is the physical embodiment of the
information. We know that it is possible to use diﬀerent amounts of data to convey the
same information. A good example is a story. A novel that originally occupies 300 pages
can be “digested” and compressed to just 30 pages without losing the main outlines of
the plot. The same story may be told by one person in 2000 words and by another in 200
words because the former employs unnecessary (or irrelevant) words, thus introducing
redundancy into his narrative, while the latter selects only those words that are strictly
needed.
In simple terms, data can be compressed because its original representation is not
the shortest possible. We use diﬀerent digital data structures to represent various types
of information in our computers, and we use these particular structures because they
make it easy to visualize the information and operate on it. Compression changes the
data representation to a shorter one (ideally, the shortest one), but it is diﬃcult or even
impossible to visualize and process the information in such a representation.
In technical terms we say that the original representation of data has redundancies
and compressing the data reduces or eliminates these redundancies. Random data is
just that, random; it has no structure. Any nonrandom data is nonrandom because it
has structure in the form of regular patterns, and it is this structure that introduces
redundancies into the data.
Data that has no redundancy to begin with cannot be
compressed. Thus, compression of data is not absolute. Given a data ﬁle, we cannot
tell whether it is small enough or too large.
We have to look for redundancies (in
terms of structures or patterns) in the data and compress the data by eliminating them.
Compression should always be measured by comparing the size of the compressed data
with the size of the original data.

6
Introduction
The interpretation of compression as the removal of redundancy also explains why
it is impossible to compress data that has already been compressed.
When data is
compressed, any redundancies in it, in the form of structures or patterns, is removed.
The compressed data features no structure and cannot be distinguished from random
data; in fact, it is random. Thus, any attempt to compress it again will fail. If it were
possible to compress data that is already compressed, then we could start with a data ﬁle
A, compress it to a smaller ﬁle B, compress B in turn to a smaller ﬁle C, and continue
in this way until a 1-byte (or even a 1-bit) ﬁle is reached. A 1-byte ﬁle cannot contain
all the data of ﬁle A (whose size is arbitrary), so recursive compression is impossible.
The following simple argument illustrates the essence of the statement “Data com-
pression is achieved by reducing or removing redundancy in the data.” The argument
shows that most data ﬁles cannot be compressed, no matter what compression method
is used. This seems strange at ﬁrst because we compress our data ﬁles all the time.
The point is that most ﬁles cannot be compressed because they are random or close
to random and therefore have no redundancy. The (relatively) few ﬁles that can be
compressed are the ones that we want to compress; they are the ﬁles we use all the time.
They have redundancy, are nonrandom and are therefore useful and interesting.
Here is the argument. Given two diﬀerent ﬁles A and B that are compressed to ﬁles
C and D, respectively, it is clear that C and D must be diﬀerent. If they were identical,
there would be no way to decompress them and get back ﬁle A or ﬁle B.
Suppose that a ﬁle of size n bits is given and we want to compress it eﬃciently.
Any compression method that can compress this ﬁle to, say, 10 bits would be welcome.
Even compressing it to 11 bits or 12 bits would be great.
We therefore (somewhat
arbitrarily) assume that compressing such a ﬁle to half its size or better is considered
good compression. There are 2n n-bit ﬁles and they would have to be compressed into
2n diﬀerent ﬁles of sizes less than or equal to n/2. However, the total number of these
ﬁles is
N = 1 + 2 + 4 + · · · + 2n/2 = 21+n/2 −1 ≈21+n/2,
so only N of the 2n original ﬁles have a chance of being compressed eﬃciently. The
problem is that N is much smaller than 2n. Here are two examples of the ratio between
these two numbers.
For n = 100 (ﬁles with just 100 bits), the total number of ﬁles is 2100 and the
number of ﬁles that can be compressed eﬃciently is 251. The ratio of these numbers is
the ridiculously small fraction 2−49 ≈1.78×10−15.
For n = 1000 (ﬁles with just 1000 bits, about 125 bytes), the total number of ﬁles
is 21000 and the number of ﬁles that can be compressed eﬃciently is 2501. The ratio of
these numbers is the incredibly small fraction 2−499 ≈9.82×10−91.
Most ﬁles of interest are at least some thousands of bytes long.
For such ﬁles,
the percentage of ﬁles that can be eﬃciently compressed is so small that it cannot be
computed with ﬂoating-point numbers even on a supercomputer (the result comes out
as zero).
The 50% ﬁgure used here is arbitrary, but even increasing it to 90% isn’t going to
make a signiﬁcant diﬀerence. Here is why. Assuming that a ﬁle of n bits is given and
that 0.9n is an integer, the number of ﬁles of sizes up to 0.9n is
20 + 21 + · · · + 20.9n = 21+0.9n −1 ≈21+0.9n.

Introduction
7
For n = 100, there are 2100 ﬁles and 21+90 = 291 can be compressed well. The ratio of
these numbers is 291/2100 = 2−9 ≈0.00195. For n = 1000, the corresponding fraction is
2901/21000 = 2−99 ≈1.578×10−30. These are still extremely small fractions.
It is therefore clear that no compression method can hope to compress all ﬁles or
even a signiﬁcant percentage of them. In order to compress a data ﬁle, the compression
algorithm has to examine the data, ﬁnd redundancies in it, and try to remove them.
The redundancies in data depend on the type of data (text, images, audio, etc.), which
is why a new compression method has to be developed for a speciﬁc type of data and
it performs best on this type.
There is no such thing as a universal, eﬃcient data
compression algorithm.
In spite of the arguments above, there are always those who claim to have developed
a “magic” compression method that can compress any data ﬁle to a small fraction of its
original size. Reference [incredible 07] lists quite a few such claims.
Multimedia digital data. The ﬁrst computers were conceived as fast, reliable
computing machines, but it did not take computer users long to realize that the computer
can also process nonnumeric data. The various compilers for programming languages
are one such example, as are also databases, computer games, and computer networks.
However, it was not until the 1990s that many multimedia applications were developed
and came into popular use. The term “multimedia” refers to the ability to digitize,
store, and manipulate in the computer all kinds of data, not just numbers.
Today
(2008), computer users commonly create, edit, store, view, and exchange text, still
images, video, and audio data easily and reliably.
Multimedia (noun, plural), the use of diﬀerent media to convey information; text
together with audio, graphics and animation, often packaged on CD-ROM with links
to the Internet.
—wiktionary.com
Each type of data is represented diﬀerently in the computer and features diﬀerent
structures and redundancies. This is why diﬀerent approaches and techniques are needed
to compress it. Following is a discussion of the representations and redundancies of the
main data types.
Text is represented in the computer as individual characters, each encoded in bi-
nary. The codes are all the same length, because ﬁxed-size codes are easy to store in
memory, move about, and operate on. For many years, the ASCII code, developed in
the 1960s, was the de facto standard. Each character of text was assigned a 7-bit code
(actually, a (7+1)-bit code, where the eighth bit serves as a parity, for added reliability).
Thus, there are 27 = 128 ASCII codes, for the letters, digits, some punctuation marks,
and various control functions.
In the 1970s and 1980s, inexpensive, high-resolution
printers and display monitors came into being, where virtually any character can be
displayed and printed. These developments were the motivation for the Unicode project
which started in the early 1990s. Unicode assigns 16-bit codes to text symbols, and can
therefore represent 216 = 65,536 symbols (there are provisions for even longer codes, so
the number of possible Unicode symbols is much greater).
The point is that certain letters appear in text more often than others, and the use
of ﬁxed-size codes introduces structure (and thus redundancy) into text. It has been
known for a long time that the letters E, T, and A are common in English texts, while

8
Introduction
J, Z, and Q are rare. Thus, English text can be compressed by assigning variable-length
codes to the various letters such that common letters are assigned short codes and rare
letters are assigned long codes. Chapter 1 discusses a few variable-length codes and their
applications.
Note that text compression must be lossless. It is hard to come up with examples
where text data can lose a certain percentage of the text while being compressed, and
still be useful after decompression. However, the other types of data discussed here
can lose much data while being compressed, and be decompressed later without any
noticeable degradation in quality. This is why lossy compression, which often features
excellent performance, is so popular.
Images. A digital image is a rectangular array of dots called pixels. A pixel has
one attribute, its color, and this attribute is stored in the computer as a ﬁxed-size code.
The use of ﬁxed-size codes again introduces redundancy, because adjacent pixels tend
to have similar codes (we say that the pixels are correlated). An image where adjacent
pixels always have wildly diﬀerent colors looks random, has no structure, features no
redundancy, and therefore cannot be compressed. Images that are of interest, however,
are far from random and exhibit structure in the form of pixel correlation. This type of
redundancy is termed interpixel redundancy.
Thus, an image can be compressed by, for example, subtracting the values of adja-
cent pixels. The pixels have similar colors, so their diﬀerences are small numbers, which
require fewer bits. More sophisticated approaches to image compression are discussed
in Chapter 5.
In addition to interpixel redundancy, images often have two more types of redun-
dancy, coding redundancy and psychovisual redundancy, which can be exploited for
compression.
Coding redundancy has to do with the distribution of colors in an image. Given a
digital image, it is easy to count the number of pixels that have color C. When this is
done for every color C, we normally ﬁnd that a few colors dominate the image. We say
that the color distribution (or histogram) is nonuniform. This redundancy suggests a
way to compress the image. Replace each pixel with a variable-length code and assign
the short codes to the dominant colors.
Psychovisual redundancy has to do with the properties of the human eye. The eye
is very sensitive to light and can often detect just a few photons. However, the eye is not
a precision device and its sensitivity varies with the type of light that falls on it. It has
been known for many years that the eye is very sensitive to variations in the luminance
(brightness) of light but is much less sensitive to variations in the chrominance (color)
component of the light. Thus, an image can be compressed if the color of each pixel
is represented in terms of luminance and chrominance and the latter components are
heavily quantized.
Video data consists of a string of images, much as a movie consists of many im-
ages (called frames) on a strip of celluloid. There are two sources of redundancy in a
video, intraframe redundancy (the correlation of pixels in each frame) and interframe
redundancy (the fact that adjacent frames tend to be similar). The former redundancy
can be reduced by the same methods employed in image compression, while the latter
redundancy is dealt with by methods that compare a frame with its predecessor and
encode the diﬀerences between them.

Introduction
9
Audio data also features redundancy in the form of correlation between consecutive
audio samples, but ﬁrst, a few words about audio and how it is digitized.
Sound is a wave. It can be viewed as a physical disturbance in the air (or some other
media) or as a pressure wave propagated by the movement of molecules. A microphone
is a device that senses sound and converts it to an electrical wave, i.e., a voltage that
varies continuously with time in the same way as the sound. To convert this voltage
into a format where it can be input into a computer, stored, edited, and played back,
the voltage is sampled many times each second. Each sample is a number whose value
is proportional to the voltage at the time of sampling. Figure Intro.1 shows a wave
sampled at three points and it is obvious that the ﬁrst sample is a small number and
the third sample is a large number, close to the maximum.
Amplitude
points
Sampling
Time
High frequency region
Maximum amplitude
Figure Intro.1: Sound Wave and Three Samples.
Thus, audio sampling (or digitized sound) is a simple concept, but its success in
practice depends on one important factor, the sampling rate. How many times should a
sound wave be sampled each second? Sampling too often creates too many audio sam-
ples, while a very low sampling rate results in low-quality played-back sound. It seems
intuitively that the sampling rate should depend on the frequency, but the frequency
of a sound wave varies all the time, while the sampling rate should remain constant (a
variable sampling rate makes it diﬃcult to edit and play back the digitized sound). The
solution was discovered back in the 1920s by H. Nyquist. It states that the optimum
sampling frequency should be slightly greater than twice the maximum frequency of the
sound. The sound wave of Figure Intro.1 has a region of high frequency at its center.
To obtain the optimum sampling rate for this particular wave, we should determine the
maximum frequency at this region, double it, and increase the result slightly.
Every sound wave has its own maximum frequency, but digitized sound used in
practice is based on the fact that the highest frequency that the human ear can perceive
is about 22,000 Hz. The optimum sampling rate that corresponds to this frequency is
44,100 Hz, and this rate is used when sound is digitized and recorded on a CD or DVD.
Now, back to audio compression. Digital audio is a string of audio samples, and
it can be compressed because adjacent audio samples tend to be similar; they are cor-
related, which introduces redundancy into the audio data. With 44,100 samples each
second, it is no wonder that adjacent samples are virtually always similar. Audio data
where many audio samples are very diﬀerent from their neighbors would sound harsh
and dissonant.

10
Introduction
Thus, audio can be compressed by subtracting each audio sample from its pre-
decessor and replacing the diﬀerences (which tend to be small integers) by suitable
variable-length codes. Practical methods often “predict” the current sample by com-
puting a weighted sum of several previously-input samples, and then subtracting the
current sample from the prediction.
Entropy and Redundancy
Understanding data compression and its codes must start with an understanding
of information, because the former is based on the latter. This short section introduces
the relevant concepts from information theory.
Information theory is the creation, in the late 1940s, of Claude
Shannon.
Shannon tried to develop means for measuring the
amount of information stored in a symbol without considering
the meaning of the information. He discovered the connection be-
tween the logarithm function and information, and showed that
the information content (in bits) of a symbol with probability p
is −log2 p. If the base of the logarithm is e, then the information
is measured in units called nats. If the base is 3, the information
units are trits, and if the base is 10, the units are referred to as
Hartleys.
Information theory is concerned with the transmission of information from a sender
(termed a source), through a communications channel, to a receiver. The sender and
receiver can be persons or machines and the receiver may, in turn, act as intermediary
and send the information it has received to another receiver. The information is sent
in units called symbols (normally bits, but in verbal communications the symbols are
spoken words) and the set of all possible data symbols is an alphabet.
The most important single factor aﬀecting communications is noise in the com-
munications channel. In verbal communications, this noise is, literally, noise. When
trying to talk in a noisy environment, we may lose part of the discussion. In electronic
communications, the channel noise is caused by imperfect hardware and by factors such
as sudden lightning, voltage ﬂuctuations—old, high-resistance wires—sudden surge in
temperature, and interference from machines that generate strong electromagnetic ﬁelds.
The presence of noise implies that special codes should be used to increase the reli-
ability of transmitted information. This is referred to as channel coding or, in everyday
language, error-control codes.
The second most important factor aﬀecting communications is sheer volume. Any
communications channel has a limited capacity. It can transmit only a limited number
of symbols per time unit. An obvious way to increase the amount of data transmitted is
to compress it before it is sent (in the source). Methods to compress data are therefore
known as source coding or, in everyday language, data compression. The feature that
makes it possible to compress data is the fact that individual symbols appear in our data
ﬁles with diﬀerent probabilities. Thus, data can be compressed by assigning variable-
length codes to the individual data symbols such that short codes are assigned to the
common symbols.

Introduction
11
Two concepts from information theory, namely entropy and redundancy, are needed
in order to fully understand the principles behind the various methods for and approaches
to data compression.
Roughly speaking, the term “entropy” as deﬁned by Shannon is a real number that
is proportional to the minimum number of yes/no questions needed to reach the answer
to some question. Another way of looking at entropy is as a quantity that describes how
much information is included in a signal or an event.
In order to understand the deﬁnition of entropy, we perform a thought experiment
where we measure the heights of 10,000 people. Suppose that we ﬁnd that 1,500 people
have a height of h. We can say that the probability of having height h in our sample
of 10,000 people is 1,500/10,000 = 0.15. Statisticians perform such experiments and
they talk about random variables. A random variable X is an entity that can have
certain values xi, each with probability Pi. In our experiment, the probability that our
random variable will have the value h is 0.15, and it can have other values with diﬀerent
probabilities.
Given a discrete random variable X that can have n values xi with probabilities Pi,
the entropy H(X) of X is deﬁned as
H(X) = −
n

i=1
Pi log2 Pi.
The surprising, unexpected part in this deﬁnition is the use of the logarithm.
The
following paragraphs explain why the familiar logarithm function constitutes such an
important part of information theory and plays such an important role in measuring
information.
Imagine a source that emits symbols ai with probabilities pi. We assume that the
source is memoryless, i.e., the probability of a symbol being emitted does not depend on
what has been emitted in the past (the parallel in our thought experiment is that the
height of a person being measured does not depend on the height of the previous person
measured). We want to deﬁne a function I(ai) that will measure the amount of informa-
tion gained when we discover that the source has emitted symbol ai. Function I will also
measure our uncertainty as to whether the next symbol will be ai. Alternatively, I(ai)
corresponds to our surprise in ﬁnding that the next symbol is ai. Clearly, our surprise at
seeing ai emitted is inversely proportional to the probability pi (we are surprised when
a low-probability symbol is emitted, but not when we notice a high-probability symbol).
Thus, it makes sense to require that function I satisﬁes the following conditions:
1. I(ai) is a decreasing function of pi, and returns 0 when the probability of a symbol
is 1. This reﬂects our feeling that high-probability events convey less information.
2. I(aiaj) = I(ai) + I(aj). This is a result of the source being memoryless and
the probabilities being independent. Discovering that ai was immediately followed by
aj, provided us with the same information as knowing that ai and aj were emitted
independently.
Even those with a minimal mathematical background may quickly realize that the
logarithm function satisﬁes the two conditions above. This is the ﬁrst example of the
relation between the logarithm function and the quantitative measure of information.
The next few paragraphs illustrate other connections between the two.

12
Introduction
Consider the case of person A selecting at random an integer N between 1 and 64
and person B having to guess N. What is the minimum number of yes/no questions that
are needed for B to guess N? Those familiar with the technique of binary search know
the answer. Using this technique, B should divide the interval 1–64 in two, and should
start by asking “is N between 1 and 32?” If the answer is no, then N is in the interval
33 to 64. This interval is then divided by two and B’s next question should be “is N
between 33 and 48?” This process continues until the interval selected by B shrinks to
a single number.
It does not take much to see that exactly six questions are necessary to determine
N. This is because 6 is the number of times 64 can be divided in half. Mathematically,
this is equivalent to writing 26 = 64 or 6 = log2 64, which is why the logarithm is the
mathematical function that quantiﬁes information.
What we call reality arises in the last analysis from the posing of yes/no questions. All
things physical are information-theoretic in origin, and this is a participatory universe.
—John Wheeler
Another approach to the same problem is to consider a nonnegative integer N and
ask how many digits does it take to express it. The answer, of course, depends on N. The
greater N, the more digits are needed. The ﬁrst 100 nonnegative integers (0 through 99)
can be expressed by two decimal digits. The ﬁrst 1,000 such integers can be expressed
by three digits. Again it does not take long to see the connection. The number of digits
required to represent N equals approximately log N. The base of the logarithm is the
same as the base of the digits. For decimal digits, use base 10; for binary digits (bits),
use base 2. If we agree that the number of digits it takes to express N is proportional
to the information content of N, then again the logarithm is the function that gives
us a measure of the information. As an aside, the precise length, in bits, of the binary
representation of a positive integer n is 1+⌊log2 n⌋, or alternatively, ⌈log2(n+1)⌉. When
n is represented in any other number base b, its length is given by the same formula,
but with the logarithm in base b instead of 2.
Here is another observation that illuminates the relation between the logarithm and
information. A 10-bit string can have 210 = 1, 024 values. We say that such a string
may contain one of 1,024 messages, or that the length of the string is the logarithm of
the number of possible messages the string can convey.
The following example sheds more light on the concept of entropy and will prepare us
for the deﬁnition of redundancy. Given a set of two symbols a1 and a2, with probabilities
P1 and P2, respectively, we compute the entropy of the set for various values of the
probabilities. Since P1+P2 = 1, the entropy of the set is −P1 log2 P1−(1−P1) log2(1−P1)
and the results are summarized in Table Intro.2.
When P1 = P2, at least one bit is required to encode each symbol, reﬂecting the
fact that the entropy is at its maximum, the redundancy is zero, and the data cannot be
compressed. However, when the probabilities are very diﬀerent, the minimum number
of bits required per symbol drops signiﬁcantly. We may not be able to conceive of a
compression method that expresses each symbol in just 0.08 bits, but we know that
when P1 = 99%, such compression is theoretically possible.
In general, the entropy of a set of n symbols depends on the individual probabilities
Pi of the symbols and is largest when all n probabilities are equal. Data representations

Introduction
13
P1
P2
Entropy
0.99
0.01
0.08
0.90
0.10
0.47
0.80
0.20
0.72
0.70
0.30
0.88
0.60
0.40
0.97
0.50
0.50
1.00
Table Intro.2: Probabilities and Entropies of Two Symbols.
often include redundancies and data can be compressed by reducing or eliminating these
redundancies. When the entropy is at its maximum, the data has maximum information
content and therefore cannot be further compressed. Thus, it makes sense to deﬁne
redundancy as a quantity that goes down to zero as the entropy reaches its maximum.
The fundamental problem of communication is that of reproducing at one point either
exactly or approximately a message selected at another point.
—Claude Shannon (1948)
To understand the deﬁnition of redundancy, we start with an alphabet of symbols
ai, where each symbol appears in the data with probability Pi. The data is compressed
by replacing each symbol with an li-bit-long code. The average code length is the sum
 Pili and the entropy (the smallest number of bits required to represent the symbols)
is [−Pi log2 Pi]. The redundancy R of the set of symbols is deﬁned as the average
code length minus the entropy. Thus,
R =

i
Pili −

i
[−Pi log2 Pi].
The redundancy is zero when the average code length equals the entropy, i.e., when the
codes are the shortest and compression has reached its maximum.
Given a set of symbols (an alphabet), we can assign binary codes to the individual
symbols. It is easy to assign long codes to symbols, but most practical applications
require the shortest possible codes.
Consider the four symbols a1, a2, a3, and a4. If they appear in our data strings
with equal probabilities (= 0.25), then the entropy of the data is −4(0.25 log2 0.25) = 2.
Two is the smallest number of bits needed, on average, to represent each symbol in this
case. We can simply assign our symbols the four 2-bit codes 00, 01, 10, and 11. Since
the probabilities are equal, the redundancy is zero and the data cannot be compressed
below two bits/symbol.
Next, consider the case where the four symbols occur with diﬀerent probabilities as
shown in Table Intro.3, where a1 appears in the data (on average) about half the time,
a2 and a3 have equal probabilities, and a4 is rare. In this case, the data has entropy
−(0.49 log2 0.49+0.25 log2 0.25+0.25 log2 0.25+0.01 log2 0.01) ≈−(−0.050−0.5−0.5−
0.066) = 1.57. The smallest number of bits needed, on average, to represent each symbol
has dropped to 1.57.
If we again assign our symbols the four 2-bit codes 00, 01, 10, and 11, the redundancy
would be R = −1.57 + log2 4 = 0.43. This suggests assigning variable-length codes to

14
Introduction
Symbol
Prob.
Code1
Code2
a1
0.49
1
1
a2
0.25
01
01
a3
0.25
010
000
a4
0.01
001
001
Table Intro.3: Variable-Length Codes.
the symbols. Code1 of Table Intro.3 is designed such that the most common symbol,
a1, is assigned the shortest code. When long data strings are transmitted using Code1,
the average size (the number of bits per symbol) is 1 × 0.49 + 2 × 0.25 + 3 × 0.25 +
3 × 0.01 = 1.77, which is very close to the minimum.
The redundancy in this case
is R = 1.77 −1.57 = 0.2 bits per symbol. An interesting example is the 20-symbol
string a1a3a2a1a3a3a4a2a1a1a2a2a1a1a3a1a1a2a3a1, where the four symbols occur with
approximately the right frequencies. Encoding this string with Code1 yields the 37 bits:
1|010|01|1|010|010|001|01|1|1|01|01|1|1|010|1|1|01|010|1
(without the vertical bars). Using 37 bits to encode 20 symbols yields an average size of
1.85 bits/symbol, not far from the calculated average size. (The reader should bear in
mind that our examples are short. To obtain results close to the best that’s theoretically
possible, an input stream with at least thousands of symbols is needed.)
However, the conscientious reader may have noticed that Code1 is bad because it
is not a preﬁx code. Code2, in contrast, is a preﬁx code and can be decoded uniquely.
Notice how Code2 was constructed. Once the single bit 1 was assigned as the code of a1,
no other codes could start with 1 (they all had to start with 0). Once 01 was assigned
as the code of a2, no other codes could start with 01. This is why the codes of a3 and
a4 had to start with 00. Naturally, they became 000 and 001.
Several important data compression terms are introduced next.
The compressor or encoder is the program that compresses raw data and generates
compressed (low-redundancy) output.
The decompressor or decoder converts in the
opposite direction. Note that the term encoding is very general and has several meanings,
but since this book discusses only data compression, it employs the term encoder for
compressor.
The term codec is used to describe both the encoder and the decoder.
Similarly, the term companding is short for “compressing/expanding.”
For the original, uncompressed data, we use the terms unencoded, raw, or original
data. The compressed data is also termed encoded. The term bitstream is often used in
the literature to indicate the compressed data.
A nonadaptive compression method is inﬂexible and does not modify its operations,
its parameters, or its tables in response to the particular data being compressed. In
contrast, an adaptive method examines the raw data and modiﬁes its operations and/or
its parameters accordingly. Some compression methods use a 2-pass algorithm, where
the ﬁrst pass reads the input to collect statistics on the data to be compressed, and the

Introduction
15
second pass does the actual compression using parameters or codes set by the ﬁrst pass.
Such a method may be called semiadaptive. A data compression method can also be
locally adaptive, meaning it adapts itself to local conditions in the input and varies this
adaptation as it moves from region to region in the input.
Lossy/lossless compression: Certain compression methods are lossy. They achieve
better compression by losing some information.
When the compressed data is later
decompressed, the result is diﬀerent from the original.
Such a method makes sense
especially in image, video, or audio compression. If the loss of data is small, the eye or
ear may not perceive any diﬀerence. In contrast, text ﬁles, especially ﬁles containing
computer programs, often become meaningless or worthless if even one bit is modiﬁed.
Such ﬁles should be compressed only by a lossless compression method.
Perceptive compression: A lossy encoder must take advantage of the special type
of data that is being compressed. It should delete only data whose absence would not
be detected by our senses. Such an encoder must therefore employ algorithms based
on our understanding of psychoacoustic and psychovisual perception, which is why it is
sometimes referred to as a perceptive encoder. Such an encoder can be made to operate
at a constant compression ratio, where for each x bits of raw data, it outputs y bits
of compressed data. This is convenient in cases where the compressed data has to be
transmitted at a constant rate. The trade-oﬀis a variable subjective quality. Parts of
the original data that are diﬃcult to compress may, after decompression, look (or sound)
bad. Such parts may require more than y bits of output for x bits of input.
Symmetric compression is the case where the decompressor is the reverse of the
compressor. Such a method makes sense for general work, where the same number of
ﬁles is compressed as is decompressed. In an asymmetric compression method, either the
compressor or the decompressor may have to work signiﬁcantly harder. Such methods
have their uses and are not necessarily bad. A compression method where the com-
pressor executes a slow, complex algorithm and the decompressor is simple is a natural
choice when ﬁles are compressed into an archive (a CDs and DVDs are good examples)
where they will be decompressed and used very often. The opposite case is useful in
environments where ﬁles are updated all the time and backups are made. There is only
a small chance that a backup ﬁle will be used, so the decompressor is rarely used and
can be slow.
When you look
kool uoy nehW
into a mirror
rorrim a otni
it is not
ton si ti
yourself you see
ees uoy ﬂesruoy
but a kind
dnik a tub
of apish error
rorre hsipa fo
posed in fearful
lufraef ni desop
symmetry
yrtemmys
John Updike, “Mirror,” in Telephone
Poles and Other Poems (1963)

16
Introduction
⋄Exercise Intro.1: Give an example of a compressed ﬁle where eﬃcient compression is
important but the speed of both compressor and decompressor isn’t important.
Many modern compression methods are asymmetric. Often, the formal speciﬁca-
tion of such a method consists of a description of the decoder and the format of the
compressed data, but does not discuss the operation of the encoder. Any encoder that
generates a correct compressed ﬁle is considered compliant, as is also any decoder that
can read and decode such a ﬁle. The advantage of such a description is that anyone
is free to develop and implement new, sophisticated algorithms for the encoder. The
implementor need not even publish the details of the encoder and may consider it pro-
prietary. If a compliant encoder is demonstrably better than competing encoders, it may
become a commercial success. In such a scheme, the encoder is considered algorithmic,
while the decoder, which is normally much simpler, is termed deterministic.
A data compression method is called universal if the compressor and decompressor
do not know the statistics of the input data and do not use it explicitly. A universal
method is optimal if the compressor can produce compression factors that asymptotically
approach the entropy of the input stream for long inputs.
The term ﬁle diﬀerencing refers to any method that locates and compresses the
diﬀerences between two ﬁles. Imagine a ﬁle A with two copies that are kept by two
users. When a copy is updated by one user, it should be sent to the other user, to keep
the two copies identical. Instead of sending a copy of A, which may be big, a much
smaller ﬁle containing just the diﬀerences, in compressed format, can be sent and used
at the receiving end to update the copy of A.
Most compression methods operate in the streaming mode, where the codec inputs a
byte or several bytes, processes them, and continues until an end-of-ﬁle is sensed. Some
methods, such as Burrows–Wheeler (Section 7.1), work in the block mode, where the
input is read block by block and each block is encoded separately. The block size in
this case should be a user-controlled parameter, because its size may greatly aﬀect the
performance of the method.
Compression performance: Several measures are commonly used to indicate the
performance of a compression method.
1. The compression ratio is deﬁned as
Compression ratio = size of the output stream
size of the input stream .
A value of 0.6 means that the data occupies 60% of its original size after compression.
Values greater than 1 imply expansion (negative compression). The compression ratio
can also be called bpb (bit per bit), since it equals the number of bits in the com-
pressed data that are needed, on average, to compress one bit in the input data. In
modern, eﬃcient text compression methods, it makes sense to talk about bpc (bits per
character)—the number of bits it takes, on average, to compress one character in the
input.
The term bitrate is a general name for bpb and bpc. Thus, the main goal of data
compression is to represent any given data at low bitrates.

Introduction
17
2. The inverse of the compression ratio is the compression factor:
Compression factor = size of the input stream
size of the output stream.
In this case, values greater than 1 indicate compression and values less than 1 imply
expansion. This measure seems natural to many people, since the bigger the factor, the
better the compression.
3. The expression 100 × (1 −compression ratio) is also a reasonable measure of com-
pression performance. A value of 60 means that the output occupies 40% of its original
size (or that the compression has resulted in savings of 60%).
4. In image compression, the quantity bpp (bits per pixel) is commonly used. It equals
the number of bits needed, on average, to compress one pixel of the image. This quantity
should always be compared with the bpp before compression.
5. The compression gain is deﬁned as
100 loge
reference size
compressed size,
where the reference size is either the size of the input or the size of the compressed
data produced by some standard lossless compression method. For small numbers x,
it is true that loge(1 + x) ≈x, so a small change in a small compression gain is very
similar to the same change in the compression ratio. Because of the use of the logarithm,
two compression gains can be compared simply by subtracting them. The unit of the
compression gain is called percent log ratio and is denoted by ◦–◦.
6. The speed of compression can be measured in cycles per byte (CPB). This is the aver-
age number of machine cycles it takes to compress one byte. This measure is important
when compression is done by special hardware.
7. Other quantities, such as mean square error (MSE) and peak signal-to-noise ratio
(PSNR), are used to measure the distortion caused by lossy compression of still images
and video.
The probability model. This concept is important in statistical data compression
methods. In such a method, a model for the data has to be constructed before com-
pression can begin. A typical model may be built by reading the entire input stream,
counting the number of times each symbol appears (its frequency of occurrence), and
computing the probability of occurrence of each symbol. The data is then input again,
symbol by symbol, and is compressed using the information in the probability model.
Reading the entire input twice is slow, which is why practical compression methods
use estimates, or adapt themselves to the data as it is being input and compressed. It
is easy to scan large quantities of, say, English text and compute the frequencies and
probabilities of every character. This information can later serve as an approximate
model for English text and can be used by text compression methods to compress any
English text. It is also possible to start by assigning equal probabilities to all the symbols
in an alphabet, then reading symbols and compressing them, and, while doing that, also
counting frequencies and changing the model as compression progresses. This is the
principle behind adaptive compression methods.

18
Introduction
The term “baud” is used in this book to mean bits per second, but see a more
general deﬁnition in http://en.wikipedia.org/wiki/Baud.
Data Compression Resources
A vast number of resources on data compression is available. Any Internet search
under “data compression,” “lossless data compression,” “image compression,” “audio
compression,” and similar topics returns at least tens of thousands of results.
The
following URLs have useful links and pointers to the many data compression resources
available on the Internet and elsewhere:
http://www.hn.is.uec.ac.jp/~arimura/compression_links.html
http://cise.edu.mie-u.ac.jp/~okumura/compression.html
http://compression.ca/ (mostly comparisons), and http://datacompression.info/
The latter URL has a wealth of information on data compression, including tutori-
als, links to workers in the ﬁeld, and lists of books. The site was maintained by Mark
Nelson but it currently belongs to Visicron Corp.
Traditional (hardcopy) resources range from general texts and texts on speciﬁc
aspects or particular methods, to survey articles in magazines, to technical reports and
research papers in scientiﬁc journals. Following is a short list of (mostly general) books,
sorted by date of publication.
James Storer, Proceedings of the IEEE Data Compression Conference, IEEE Press,
published annually since 1991.
Tinku Acharya and Ping-Sing Tsai, JPEG2000 Standard for Image Compression:
Concepts, Algorithms and VLSI Architectures, John Wiley and Sons (2005).
Ida Mengyi Pu, Fundamental Data Compression, Butterworth-Heinemann (2005).
Khalid Sayood, Introduction to Data Compression, Morgan Kaufmann, 3rd edition
(2005).
Darrel Hankerson, Introduction to Information Theory and Data Compression, Chap-
man & Hall (CRC), 2nd edition (2003).
Peter Symes, Digital Video Compression, McGraw-Hill/TAB Electronics (2003).
Charles Poynton, Digital Video and HDTV Algorithms and Interfaces, Morgan
Kaufmann (2003).
Iain E. G. Richardson, H.264 and MPEG-4 Video Compression: Video Coding for
Next Generation Multimedia, John Wiley and Sons (2003).
Marina Bosi and Richard E. Goldberg, Introduction to Digital Audio Coding and
Standards, Springer Verlag (2003).
Khalid Sayood, Lossless Compression Handbook, Academic Press (2002).
Touradj Ebrahimi and Fernando Pereira, The MPEG-4 Book, Prentice Hall (2002).
Adam Drozdek, Elements of Data Compression, Course Technology (2001).
Alistair Moﬀat and Andrew Turpin, Compression and Coding Algorithms, Springer
Verlag (2002).
David Taubman and Michael Marcellin (Editors), JPEG2000: Image Compression
Fundamentals, Standards and Practice, Springer Verlag (2001).
Kamisetty R. Rao, The Transform and Data Compression Handbook, CRC (2000).
Ian H. Witten, Alistair Moﬀat, and Timothy C. Bell, Managing Gigabytes: Com-
pressing and Indexing Documents and Images, Morgan Kaufmann, 2nd edition (1999).

Introduction
19
Peter Wayner, Compression Algorithms for Real Programmers, Morgan Kaufmann
(1999).
John Miano, Compressed Image File Formats: JPEG, PNG, GIF, XBM, BMP,
ACM Press and Addison-Wesley Professional (1999).
Jerry D. Gibson et al. Digital Compression for Multimedia: Principles & Standards,
Morgan Kaufmann (1998).
Nikil Jayant, Signal Compression: Coding of Speech, Audio, Text, Image and Video,
World Scientiﬁc (1997).
Weidong Kou, Digital Image Compression:
Algorithms and Standards, Kluwer
(1995).
Mark Nelson and Jean-Loup Gailly, The Data Compression Book, M&T Books, 2nd
edition (1995).
Rafail Krichevsky, Universal Compression and Retrieval, Kluwer Academic Pub-
lishers, 1994.
William B. Pennebaker and Joan L. Mitchell, JPEG: Still Image Data Compression
Standard, Springer Verlag (1992).
Timothy C. Bell, John G. Cleary, and Ian H. Witten, Text Compression, Prentice
Hall (1990).
James A. Storer, Data Compression: Methods and Theory, Computer Science Press
(1988).
John Woods, ed., Subband Coding, Kluwer Academic Press (1990).
The symbol “␣” is used to indicate a blank space in places where spaces may lead
to ambiguity.
Comments, suggestions, and corrections are always welcome and should be directed
to dsalomon@csun.edu.
History is a kind of introduction to more interesting
people than we can possibly meet in our restricted
lives; let us not neglect the opportunity.
—Dexter Perkins

1
Approaches
to Compression
How can a given a data ﬁle be compressed? Compression amounts to eliminating the
redundancy in the data, so the ﬁrst step is to ﬁnd the source of redundancy in each type
of data. Once we understand what causes redundancy in a given type of data, we can
propose an approach to eliminating the redundancy.
This chapter covers the basic approaches to the compression of diﬀerent types of
data. The chapter discusses the principles of variable-length codes, run-length encoding,
dictionary-based compression, transforms, and quantization. Later chapters illustrate
how these approaches are applied in practice.
Variable-length codes. Text is perhaps the simplest example of data with redun-
dancies. A text ﬁle consists of individual symbols (characters), each encoded in ASCII or
Unicode. These representations are redundant because they employ ﬁxed-length codes,
while characters of text appear with diﬀerent frequencies. Analyzing large quantities of
text indicates that certain characters tend to appear in texts more than other characters.
In English, for example, the most common letters are E, T, and A, while J, Q, and Z are
the rarest. Thus, redundancy can be reduced by the use of variable-length codes, where
short codes are assigned to the common symbols and long codes are assigned to the
rare symbols. Designing such a set of codes must take into consideration the following
points:
We have to know the probability (or, equivalently, the frequency of occurrence)
of each data symbol. The variable-length codes should be selected according to these

22
1.
Approaches to Compression
probabilities. For example, if a few data symbols are very common (i.e., appear with
large probabilities) while the rest are rare, then we should ideally have a set of variable-
length codes where a few codes are very short and the rest are long.
Once the original data symbols are replaced with variable-length codes, the result
(the compressed ﬁle) is a long string of bits with no separators between the codes of
consecutive symbols. The decoder (decompressor) should be able to read this string and
break it up unambiguously into individual codes. We say that such codes have to be
uniquely decodable or uniquely decipherable (UD).
Run-length encoding. A digital image is a rectangular array of dots called pix-
els. There are two sources of redundancy in an image, namely dominant colors and
correlation between pixels.
A pixel has a single attribute, its color. A pixel is stored in memory or on a ﬁle as
a color code. A pixel in a monochromatic image (black and white or bi-level) can be
either black or white, so a 1-bit code is suﬃcient to represent it. A pixel in a grayscale
image can be a certain shade of gray, so its code should be an integer. Similarly, the
code of a pixel in a color image must have three parts, describing the intensities of its
three color components. Imagine an image where each pixel is described by a 24-bit
code (eight bits for each of the three color components). The total number of colors in
such an image can be 224 ≈16.78 million, but any particular image may have only a
few hundred or a few thousand colors. Thus, one approach to image compression is to
replace the original pixel codes with variable-length codes.
We know from long experience that the individual pixels of an image tend to be
correlated. A pixel will often be identical, or very similar, to its near neighbors. This
can easily be veriﬁed by looking around. Imagine an outdoor scene with rocks, trees, the
sky, the sun, and clouds. As our eye moves across the sky, we see mostly blue. Adjacent
points may feature slightly diﬀerent shades of blue; they are not identical but neither
are they completely independent. We say that their colors are correlated. The same is
true when we look at points in a cloud. Most points will have a shade of white similar
to their near neighbors. At the intersection of a sky and a cloud, some blue points
will have immediate white neighbors, but such points constitute a small minority. Pixel
correlation is the main source of redundancy in images and most image compression
methods exploit this feature to obtain eﬃcient compression.
In a bi-level image, pixels can be only black or white. Thus, a pixel can either be
identical to its neighbors or diﬀerent from them, but not similar. Pixel correlation implies
that in such an image, a pixel will tend to be identical to its near neighbors. This suggests
another approach to image compression. Given a bi-level image to be compressed, scan
it row by row and count the lengths of runs of identical pixels. If a row in such an image
starts with 12 white pixels, followed by ﬁve black pixels, followed by 36 white pixels,
followed by six black pixels, and so on, then only the numbers 12, 5, 36, 6,. . . need to
be output. This is the principle of run-length encoding (RLE), a popular method that
is sometimes combined with other techniques to improve compression performance.
⋄Exercise 1.1: It seems that in addition to the sequence of run lengths, a practical RLE
compression method has to save the color (white or black) of the ﬁrst pixel of a row, or
at least the ﬁrst pixel of the image. Explain why this is not necessary.

Prelude
23
Dictionaries. Returning to text data, we observe that it has another source of
redundancy. Given a nonrandom text, we often ﬁnd that bits and pieces of it—such as
words, syllables, and phrases—tend to appear many times, while other pieces are rare
or nonexistent. A grammar book, for example, may contain many occurrences of the
words noun, pronoun, verb, and adverb in one chapter and many occurrences of con-
jugation, conjunction, subject, and subjunction in another chapter. The principle
of dictionary-based compression is to read the next data item D to be compressed, and
search the dictionary for D. If D is found in the dictionary, it is compressed by emitting a
pointer that points to it in the dictionary. If the pointer is shorter than D, compression
is achieved.
The dictionaries we commonly use consist of lists of words, each with its deﬁnition.
A dictionary used to compress data is diﬀerent. It is a list of bits and pieces of data that
have already been read from the input. When a data item is input for the ﬁrst time, it
is not found in the dictionary and therefore cannot be compressed. It is written on the
output in its original (raw) format, and is also added to the dictionary. When this piece
is read again from the data, it is found in the dictionary, and a pointer to it is written
on the output.
Many dictionary methods have been developed and implemented.
Their details
are diﬀerent, but the principle is the same. Chapter 3 and Section 1.3 describe a few
important examples of such methods.
Prediction. The fact that adjacent pixels in an image tend to be correlated implies
that the diﬀerence between a pixel and any of its near neighbors tends to be a small
integer (notice that it can also be negative).
The term “prediction” is used in the
technical literature to express this useful fact. Some pixels may turn out to be very
diﬀerent from their neighbors, which is why sophisticated prediction compares a pixel
to an average (sometimes a weighted average) of several of its nearest neighbors. Once a
pixel is predicted, the prediction is subtracted from the pixel to yield a diﬀerence. If the
pixels are correlated and the prediction is done properly, the diﬀerences tend to be small
(signed) integers. They are easy to compress by replacing them with variable-length
codes. Vast experience with many digital images suggests that the diﬀerences tend to be
distributed according to the Laplace distribution, a well-known statistical distribution,
and this fact helps in selecting the best variable-length codes for the diﬀerences.
The technique of prediction is also employed by several audio compression algo-
rithms, because audio samples also tend to be strongly correlated.
Transforms. Sometimes, a mathematical problem can be solved by transforming
its constituents (unknowns, coeﬃcients, numbers, vectors, or anything else) to a diﬀerent
format, where they may look familiar or have a simple form and thus make it possible
to solve the problem. After the problem is solved in this way, the solution has to be
transformed back to the original format. Roman numerals provide a convincing example.
The ancient Romans presumably knew how to operate on these numbers, but when we
are faced with a problem such as XCVI × XII, we may ﬁnd it natural to transform the
original numbers into modern (Arabic) notation, multiply them, and then transform the
result back into a Roman numeral. Here is the result:
XCVI × XII→96 × 12 = 1152 →MCLII.
Another example is the integer 65,536.
In its original, decimal representation, this
number doesn’t seem special or interesting, but when transformed to binary it becomes

24
1.
Approaches to Compression
the round number 10,000,000,000,000,0002 = 216.
Two types of transforms, orthogonal and subband, are employed by various com-
pression methods. They are described in some detail in Chapter 5. These transforms
do not by themselves compress the data and are used only as intermediate steps, trans-
forming the original data to a format where it is easy to compress. Given a list of N
correlated numbers, such as adjacent pixels in an image or adjacent audio samples, an
orthogonal transform converts them to N transform coeﬃcients, of which the ﬁrst is
large and dominant (it contains much of the information of the original data) and the
remaining ones are small and contain the details (i.e., the less important features) of
the original data. Compression is achieved in a subsequent step, either by replacing
the detail coeﬃcients by variable-length codes or by quantization, RLE, or arithmetic
coding. A subband transform (also known as a wavelet transform) also results in coarse
and ﬁne transform coeﬃcients, and when applied to an image, it separates the ver-
tical, horizontal, and diagonal constituents of the image, so each can be compressed
diﬀerently.
Quantization.
Text must be compressed without any loss of information, but
images, video, and audio can tolerate much loss of data when compressed and later
decompressed.
The loss, addition, or corruption of one character of text can cause
confusion, misunderstanding, or disagreements. Changing not to now, want to went
or under the edge to under the hedge may result in a sentence that is syntactically
correct but has a diﬀerent meaning.
⋄Exercise 1.2: Change one letter in each of the following phrases to create a syntactically
valid phrase with a completely diﬀerent meaning, “look what the cat dragged in,” “my
ears are burning,” “bad egg,” “a real looker,” “my brother’s keeper,” and “put all your
eggs in one basket”.
Quantization is a simple approach to lossy compression. The idea is to start with a
ﬁnite list of N symbols Si and to modify each of the original data symbols to the nearest
Si. For example, if the original data consists of real numbers in a certain interval, then
each can be rounded oﬀto the nearest integer. It takes fewer bits to express the integer,
so compression is achieved, but it is lossy because it is impossible to retrieve the original
real data from the integers. The well-known mp3 audio compression method is based on
quantization of the original audio samples.
The beauty of code is much more akin to the elegance, eﬃciency and clean lines of
a spiderweb. It is not the chaotic glory of a waterfall, or the pristine simplicity of a
ﬂower. It is an aesthetic of structure, design and order.
—Charles Gordon

1.1 Variable-Length Codes
25
1.1 Variable-Length Codes
Often, a ﬁle of data to be compressed consists of data symbols drawn from an alphabet.
At the time of writing (mid-2007) most text ﬁles consist of individual ASCII characters.
The alphabet in this case is the set of 128 ASCII characters. A grayscale image consists
of pixels, each coded as one number indicating a shade of gray. If the image is restricted
to 256 shades of gray, then each pixel is represented by eight bits and the alphabet is the
set of 256 byte values. Given a data ﬁle where the symbols are drawn from an alphabet,
it can be compressed by replacing each symbol with a variable-length codeword. The
obvious guiding principle is to assign short codewords to the common symbols and long
codewords to the rare symbols.
In data compression, the term code is often used for the entire set, while the indi-
vidual codes are referred to as codewords.
Variable-length codes (VLCs for short) are used in several real-life applications, not
just in data compression. The following is a short list of applications where such codes
play important roles.
The Morse code for telegraphy, originated in the 1830s by Samuel Morse and Alfred
Vail, employs the same idea. It assigns short codes to commonly-occurring letters (the
code of E is a dot and the code of T is a dash) and long codes to rare letters and
punctuation marks (--.- to Q, --.. to Z, and --..-- to the comma).
Processor design. Part of the architecture of any computer is an instruction set
and a processor that fetches instructions from memory and executes them. It is easy
to handle ﬁxed-length instructions, but modern computers normally have instructions
of diﬀerent sizes. It is possible to reduce the overall size of programs by designing the
instruction set such that commonly-used instructions are short. This also reduces the
processor’s power consumption and physical size and is especially important in embedded
processors, such as processors designed for digital signal processing (DSP).
Country calling codes. ITU-T recommendation E.164 is an international standard
that assigns variable-length calling codes to many countries such that countries with
many telephones are assigned short codes and countries with fewer telephones are as-
signed long codes. These codes also obey the preﬁx property (page 28) which means
that once a calling code C has been assigned, no other calling code will start with C.
The International Standard Book Number (ISBN) is a unique number assigned to a
book, to simplify inventory tracking by publishers and bookstores. The ISBN numbers
are assigned according to an international standard known as ISO 2108 (1970). One
component of an ISBN is a country code, that can be between one and ﬁve digits long.
This code also obeys the preﬁx property. Once C has been assigned as a country code,
no other country code will start with C.
VCR Plus+ (also known as G-Code, VideoPlus+, and ShowView) is a preﬁx,
variable-length code for programming video recorders. A unique number, a VCR Plus+,
is computed for each television program by a proprietary algorithm from the date, time,
and channel of the program. The number is published in television listings in newspa-
pers and on the Internet. To record a program on a VCR, the number is located in a
newspaper and is typed into the video recorder. This programs the recorder to record

26
1.
Approaches to Compression
the correct channel at the right time. This system was developed by Gemstar-TV Guide
International [Gemstar 07].
When we consider using VLCs to compress a data ﬁle, the ﬁrst step is to determine
which data symbols in this ﬁle are common and which ones are rare. More precisely,
we need to know the frequency of occurrence (or alternatively, the probability) of each
symbol of the alphabet. If, for example, we determine that symbol e appears 205 times
in a 1106-symbol data ﬁle, then the probability of e is 205/1106 ≈0.185 or about 19%.
If this is higher than the probabilities of most other alphabet symbols, then e is assigned
a short codeword. The list of probabilities (or frequencies of occurrence) is called the
statistical distribution of the data symbols. Figure 1.1 displays the distribution of the
256 byte values in a past edition of the book Data Compression: The Complete Reference
as a histogram. It is easy to see that the most-common symbol is the space, followed by
a cr (carriage return at the end of lines) and the lower-case e.
0.00
0
50
100
150
200
250
0.05
0.10
0.15
0.20
cr
space
Relative freq.
Byte value
uppercase letters
and digits
lowercase letters
Figure 1.1: A Histogram of Letter Distribution.
The problem of determining the distribution of data symbols in a given ﬁle is per-
haps the chief consideration in determining the assignment of variable-length codewords
to symbols and thus the performance of the compression algorithm. We discuss three
approaches to this problem as follows:
A two-pass compression job. The compressor (encoder) reads the entire data ﬁle
and counts the number of times each symbol appears.
At the end of this pass, the

1.1 Variable-Length Codes
27
probabilities of the symbols are computed and are used to determine the set of variable-
length codes that will be assigned to the symbols. This set is written on the compressed
ﬁle and the encoder starts the second pass. In this pass it again reads the entire input
ﬁle and compresses it by replacing each symbol with its codeword. This method provides
very good results because it uses the correct probabilities for each data ﬁle. The table
of codewords must be included in the output ﬁle, but this table is small (typically a few
hundred codewords written on the output consecutively, with no separators between
codes). The downside of this approach is its low speed. Currently, even the fastest
magnetic disks are considerably slower than memory and CPU operations, which is why
reading the input ﬁle twice normally results in unacceptably-slow execution. Notice that
the decoder is simple and fast because it does not need two passes. It starts by reading
the code table from the compressed ﬁle, following which it reads variable-length codes
and replaces each with its original symbol.
Use a set of training documents. The ﬁrst step in implementing fast software for
text compression may be to select texts that are judged “typical“ and employ them to
“train” the algorithm. Training consists of counting symbol frequencies in the training
documents, computing the distribution of symbols, and assigning them variable-length
codes. The code table is then built into both encoder and decoder and is later used to
compress and decompress various texts. An important example of the use of training
documents is facsimile compression (page 86). The success of such software depends on
how “typical” the training documents are.
It is unlikely that a set of documents will be typical for all kinds of text, but such a
set can perhaps be found for certain types of texts. A case in point is facsimile compres-
sion. Documents sent on telephone lines between fax machines have to be compressed in
order to cut the transmission times from 10–11 minutes per page to about one minute.
The compression method must be an international standard because fax machines are
made by many manufacturers, and such a standard has been developed (Section 2.4). It
is based on a set of eight training documents that have been selected by the developers
and include a typed business letter, a circuit diagram, a French technical article with
ﬁgures and equations, a dense document in Kanji, and a handwritten memo.
Another application of training documents is found in image compression.
Re-
searchers trying to develop methods for image compression have long noticed that pixel
diﬀerences in images tend to be distributed according to the well-known Laplace distri-
bution (by a pixel diﬀerence is meant the diﬀerence between a pixel and an average of
its nearest neighbors).
An adaptive algorithm. Such an algorithm does not assume anything about the
distribution of the symbols in the data ﬁle to be compressed. It starts “with a blank
slate” and adapts itself to the statistics of the input ﬁle as it reads and compresses
more and more symbols. The data symbols are replaced by variable-length codewords,
but these codewords are modiﬁed all the time as more is known about the input data.
The algorithm has to be designed such that the decoder would be able to modify the
codewords in precisely the same way as the encoder. We say that the decoder has to
work in lockstep with the encoder. The best known example of such a method is the
adaptive (or dynamic) Huﬀman algorithm (Section 2.3).

28
1.
Approaches to Compression
⋄Exercise 1.3: Compare the three diﬀerent approaches (two-passes, training, and adap-
tive compression algorithms) and list some of the pros and cons for each.
Several variable-length codes are listed and described later in this section, and the
discussion shows how the average code length can be used to determine the statistical
distribution to which the code is best suited.
The second consideration in the design of a variable-length code is unique decod-
ability (UD). We start with a simple example: the code a1 = 0, a2 = 10, a3 = 101,
and a4 = 111. Encoding the string a1a3a4 . . . with these codewords results in the bit-
string 0101111. . . . However, decoding is ambiguous. The same bitstring 0101111. . . can
be decoded either as a1a3a4 . . . or a1a2a4 . . .. This code is not uniquely decodable. In
contrast, the similar code a1 = 0, a2 = 10, a3 = 110, and a4 = 111 (where only the
codeword of a3 is diﬀerent) is UD. The string a1a3a4 . . . is easily encoded to 0110111. . . ,
and this bitstring can be decoded unambiguously. The ﬁrst 0 implies a1, because only
the codeword of a1 starts with 0. The next (second) bit, 1, can be the start of a2, a3,
or a4. The next (third) bit is also 1, which reduces the choice to a3 or a4. The fourth
bit is 0, so the decoder emits a3.
A little thinking clariﬁes the diﬀerence between the two codes. The ﬁrst code is
ambiguous because 10, the code of a2, is also the preﬁx of the code of a3. When the
decoder reads 10. . . , it often cannot tell whether this is the codeword of a2 or the start
of the codeword of a3. The second code is UD because the codeword of a2 is not the
preﬁx of any other codeword. In fact, none of the codewords of this code is the preﬁx
of any other codeword.
This observation suggests the following rule. To construct a UD code, the codewords
should satisfy the following preﬁx property. Once a codeword c is assigned to a symbol,
no other codeword should start with the bit pattern c. Preﬁx codes are also referred to
as preﬁx-free codes, preﬁx condition codes, or instantaneous codes. Observe, however,
that a UD code does not have to be a preﬁx code. It is possible, for example, to designate
the string 111 as a separator (a comma) to separate individual codewords of diﬀerent
lengths, provided that no codeword contains the string 111. There are other ways to
construct a set of non-preﬁx, variable-length codes.
A UD code is said to be instantaneous if it is possible to decode each codeword in
a compressed ﬁle without knowing the succeeding codewords. Preﬁx codes are instan-
taneous.
Constructing a UD code for given ﬁnite set of data symbols should start with the
probabilities of the symbols. If the probabilities are known (at least approximately),
then the best variable-length code for the symbols is obtained by the Huﬀman algo-
rithm (Chapter 2). There are, however, applications where the set of data symbols is
unbounded; its size is either extremely large or is not known in advance. Here are a few
practical examples of both cases:
Text. There are 128 ASCII codes, so the size of this set of symbols is reasonably
small. In contrast, the number of Unicodes is in the tens of thousands, which makes it
impractical to use variable-length codes to compress text in Unicode; a diﬀerent approach
is required.
A grayscale image. For 8-bit pixels, the number of shades of gray is 256, so a set of
256 codewords is required, large, but not too large.

1.1 Variable-Length Codes
29
Pixel prediction.
If a pixel is represented by 16 or 24 bits, it is impractical to
compute probabilities and prepare a huge set of codewords. A better approach is to
predict a pixel from several of its near neighbors, subtract the prediction from the
pixel value, and encode the resulting diﬀerence.
If the prediction is done properly,
most diﬀerences will be small (signed) integers, but some diﬀerences may be (positive or
negative) large, and a few may be as large as the pixel value itself (typically 16 or 24 bits).
In such a case, a code for the integers is the best choice. Each integer has a codeword
assigned that can be computed on the ﬂy. The codewords for the small integers should
be small, but the lengths should depend on the distribution of the diﬀerence values.
Audio compression. Audio samples are almost always correlated, which is why many
audio compression methods predict an audio sample from its predecessors and encode
the diﬀerence with a variable-length code for the integers.
Any variable-length code for integers should satisfy the following requirements:
1. Given an integer n, its code should be as short as possible and should be con-
structed from the magnitude, length, and bit pattern of n, without the need for any
table lookups or other mappings.
2. Given a bitstream of variable-length codes, it should be easy to decode the next
code and obtain an integer n even if n hasn’t been seen before.
Quite a few VLCs for integers are known. Many of them include part of the binary
representation of the integer, while the rest of the codeword consists of side information
indicating the length or precision of the encoded integer.
The following sections describe popular variable-length codes (the Intermezzo on
page 253 describes one more), but ﬁrst, a few words about notation. It is customary to
denote the standard binary representation of the integer n by β(n). This representation
can be considered a code (the beta code), but this code does not satisfy the preﬁx
property and also has a ﬁxed length. (It is easy to see that the beta code does not
satisfy the preﬁx property because, for example, 2 = 102 is the preﬁx of 4 = 1002.)
Given a set of integers between 0 and n, we can represent each in
1 + ⌊log2 n⌋= ⌈log2(n + 1)⌉
(1.1)
bits, a ﬁxed-length representation. When n is represented in any other number base b,
its length is given by the same expression, but with the logarithm in base b instead of 2.
A VLC that can code only positive integers can be extended to encode nonnegative
integers by incrementing the integer before it is encoded and decrementing the result
produced by decoding. A VLC for arbitrary integers can be obtained by a bijection, a
mapping of the form
0
−1
1
−2
2
−3
3
−4
4
−5
5
· · ·
1
2
3
4
5
6
7
8
9
10
11
· · ·
A function is bijective if it is one-to-one and onto.

30
1.
Approaches to Compression
1.1.1 Unary Code
Perhaps the simplest variable-length code for integers is the well-known unary code.
The unary code of the positive integer n is constructed from n −1 1’s followed by a
single 0, or alternatively as n −1 zeros followed by a single 1 (the three left columns of
Table 1.2). The length of the unary code for the integer n is therefore n bits. The two
rightmost columns of Table 1.2 show how the unary code can be extended to encode
the nonnegative integers (which makes the codes more useful but also one bit longer).
The unary code is simple to construct and is employed in many applications. Stone-age
people indicated the integer n by marking n adjacent vertical bars on a stone, which
is why the unary code is sometimes known as a stone-age binary and each of its n or
(n −1) 1’s [or n or (n −1) zeros] is termed a stone-age bit.
n
Code Reverse
Alt. code
Alt reverse
0
–
–
0
1
1
0
1
10
01
2
10
01
110
001
3
110
001
1110
0001
4
1110
0001
11110
00001
5
11110
00001
111110
000001
Table 1.2: Some Unary Codes.
It is easy to see that the unary code satisﬁes the preﬁx property. Since its length
L satisﬁes L = n, we get 2−L = 2−n, so it makes sense to use this code in cases were
the input data consists of integers n with exponential probabilities P(n) ≈2−n. Given
data that lends itself to the use of the unary code (i.e., a set of integers that satisfy
P(n) ≈2−n), we can assign unary codes to the integers and these codes will be as good
as the Huﬀman codes, with the advantage that the unary codes are trivial to encode
and decode. In general, the unary code is used as part of other, more sophisticated,
variable-length codes.
Example: Table 1.3 lists the integers 1 through 6 with probabilities P(n) = 2−n,
except that P(6) is artiﬁcially set to 2−5 ≈2−6 in order for the probabilities to add
up to unity. The table lists the unary codes and Huﬀman codes for the six integers
(see Chapter 2 for the Huﬀman codes), and it is obvious that these codes have the same
lengths (except the code of 6, because this symbol does not have the correct probability).
(From The Best Coin Problems, by Henry E. Dudeney, 1909). It is easy to place 16
pennies in a 4 × 4 square such that each row, each column, and each of the two main
diagonals will have the same number of pennies. Do the same with 20 pennies.
1.1.2 Elias Codes
In his pioneering work [Elias 75], Peter Elias described three useful preﬁx codes. The
main idea of these codes is to preﬁx the integer being encoded with an encoded repre-
sentation of its order of magnitude. For example, for any positive integer n there is an
integer M such that 2M ≤n < 2M+1. We can therefore write n = 2M + L where L is

1.1 Variable-Length Codes
31
n
Prob
Unary
Huﬀman
1
2−1
0
0
2
2−2
10
10
3
2−3
110
110
4
2−4
1110
1110
5
2−5
11110
11110
6
2−5
111110
11111
Table 1.3: Six Unary and Huﬀman Codes.
at most M bits long, and generate a code that consists of M and L. The problem is to
determine the length of M and this is solved in diﬀerent ways by the various Elias codes.
Elias denoted the unary code of n by α(n) and the standard binary representation of n,
from its most-signiﬁcant 1, by β(n). His ﬁrst code was therefore designated γ (gamma).
The Elias gamma code γ(n) is designed for positive integers n and is simple to
encode and decode.
Encoding. Given a positive integer n, perform the following steps:
1. Denote by M the length of the binary representation β(n) of n.
2. Prepend M −1 zeros to it (i.e., the α(n) code without its terminating 1).
Step 2 amounts to prepending the length of the code to the code, in order to ensure
unique decodability.
We now show that this code is ideal for applications where the probability of n is
1/(2n2). The length M of the integer n is, from Equation (1.1), 1 + ⌊log2 n⌋, so the
length of γ(n) is
2M −1 = 2⌊log2 n⌋+ 1.
(1.2)
In general, given a set of symbols ai, where each symbol occurs in the data with
probability Pi and the length of its code is li bits, the average code length is the sum
 Pili and the entropy (the smallest number of bits required to represent the symbols) is
[−Pi log2 Pi]. The diﬀerence between the average length and the entropy is 
i Pili −

i[−Pi log2 Pi] and we are looking for probabilities Pi that will minimize this diﬀerence.
For the gamma code, li = 1+2 log2 i. If we select symbol probabilities Pi = 1/(2i2)
(a power law distribution of probabilities, where the ﬁrst 10 values are 0.5, 0.125, 0.0556,
0.03125, 0.02 0.01389, 0.0102, 0.0078, 0.00617, and 0.005), both the average code length
and the entropy become the identical sums

i
1 + 2 log i
2i2
,
indicating that the gamma code is asymptotically optimal for this type of data.
A
power law distribution of values is dominated by just a few symbols and especially by
the ﬁrst. Such a distribution is very skewed and is therefore handled very well by the
gamma code which starts very short. In an exponential distribution, in contrast, the
small values have similar probabilities, which is why data with this type of statistical
distribution is compressed better by a Rice code (Section 1.1.3).

32
1.
Approaches to Compression
An alternative construction of the gamma code is as follows:
1. Find the largest integer N such that 2N ≤n < 2N+1 and write n = 2N + L.
Notice that L is at most an N-bit integer.
2. Encode N in unary either as N zeros followed by a 1 or N 1’s followed by a 0.
3. Append L as an N-bit number to this representation of N.
Peter Elias
1 = 20 + 0 = 1
10 = 23 + 2 = 0001010
2 = 21 + 0 = 010
11 = 23 + 3 = 0001011
3 = 21 + 1 = 011
12 = 23 + 4 = 0001100
4 = 22 + 0 = 00100
13 = 23 + 5 = 0001101
5 = 22 + 1 = 00101
14 = 23 + 6 = 0001110
6 = 22 + 2 = 00110
15 = 23 + 7 = 0001111
7 = 22 + 3 = 00111
16 = 24 + 0 = 000010000
8 = 23 + 0 = 0001000
17 = 24 + 1 = 000010001
9 = 23 + 1 = 0001001
18 = 24 + 2 = 000010010
Table 1.4: 18 Elias Gamma Codes.
Table 1.4 lists the ﬁrst 18 gamma codes, where the L part is in italics.
In his 1975 paper, Elias describes two versions of the gamma code. The ﬁrst version
(titled γ) is encoded as follows:
1. Generate the binary representation β(n) of n.
2. Denote the length |β(n)| of β(n) by M.
3. Generate the unary u(M) representation of M as M −1 zeros followed by a 1.
4. Follow each bit of β(n) by a bit of u(M).
5. Drop the leftmost bit (the leftmost bit of β(n) is always 1).
Thus, for n = 13 we prepare β(13) = ¯1¯1¯0¯1, so M = 4 and u(4) = 0001, resulting in
¯10¯10¯00¯11. The ﬁnal code is γ(13) = 0¯10¯00¯11.
The second version, dubbed γ′, moves the bits of u(M) to the left. Thus γ′(13) =
0001|¯1¯0¯1. The gamma codes of Table 1.4 are Elias’s γ′ codes. Both gamma versions are
universal.
Decoding is also simple and is done in two steps:
1. Read zeros from the code until a 1 is encountered. Denote the number of zeros
by N.
2. Read the next N bits as an integer L. Compute n = 2N + L.
It is easy to see that this code can be used to encode positive integers even in cases
where the largest integer is not known in advance. Also, this code grows slowly (see
Figure 1.5), which makes it a good candidate for compressing integer data where small
integers are common and large ones are rare.
Elias delta code. In his gamma code, Elias prepends the length of the code in
unary (α). In his next code, δ (delta), he prepends the length in binary (β). Thus, the
Elias delta code, also for the positive integers, is slightly more complex to construct.
Encoding a positive integer n, is done in the following steps:
1. Write n in binary. The leftmost (most-signiﬁcant) bit will be a 1.

1.1 Variable-Length Codes
33
Gamma
Length
Omega
Delta
Binary
5
10
15
20
10
100
1000
n
Figure 1.5: Lengths of Three Elias Codes.
(* Plot the lengths of four codes
1. staircase plots of binary representation *)
bin[i_] := 1 + Floor[Log[2, i]];
Table[{Log[10, n], bin[n]}, {n, 1, 1000, 5}];
g1 = ListPlot[%, AxesOrigin -> {0, 0},
PlotJoined -> True]
(* 2. staircase plot of Elias Omega code *)
omega[n_] := Module[{l, om},
l = Length[IntegerDigits[n, 2]];
om = l + 1;
While[l > 2,
l = Length[IntegerDigits[l - 1, 2]]; om = om + l;];
om]
Table[{Log[10, n], omega[n]}, {n, 1, 1000, 5}];
g2 = ListPlot[%, AxesOrigin -> {0, 0},
PlotJoined -> True,
PlotStyle -> { AbsoluteDashing[{5, 5}]}]
(* 3. staircase plot of gamma code length*)
gam[i_] := 1 + 2Floor[Log[2, i]];
Table[{Log[10, n], gam[n]}, {n, 1, 1000, 5}];
g3 = ListPlot[%, AxesOrigin -> {0, 0},
PlotJoined -> True,
PlotStyle -> { AbsoluteDashing[{2, 2}]}]
(* 4. staircase plot of delta code length *)
del[i_] := 1 + Floor[Log[2, i]] + 2Floor[Log[2, Log[2, i]]];
Table[{Log[10, n], del[n]}, {n, 2, 1000, 5}];
g4 = ListPlot[%, AxesOrigin -> {0, 0},
PlotJoined -> True,
PlotStyle -> { AbsoluteDashing[{6, 2}]}]
Show[g1, g2, g3, g4, PlotRange -> {{0, 3}, {0, 20}}]
Code for Figure 1.5
2. Count the bits, remove the leftmost bit of n, and prepend the count, in binary,
to what is left of n after its leftmost bit has been removed.
3. Subtract 1 from the count of step 2 and prepend that number of zeros to the
code.
When these steps are applied to the integer 17, the results are: 17 = 100012 (ﬁve
bits). Remove the leftmost 1 and prepend 5 = 1012 yields 101|0001. Three bits were
added, so we prepend two zeros to obtain the delta code 00|101|0001.
To determine the length of the delta code of n, we notice that step 1 generates [from
Equation (1.1)] M = 1 + ⌊log2 n⌋bits. For simplicity, we omit the ⌊and ⌋and observe
that
M = 1 + log2 n = log2 2 + log2 n = log2(2n).
The count of step 2 is M, whose length C is therefore C = 1+log2 M = 1+log2(log2(2n))

34
1.
Approaches to Compression
bits. Step 2 therefore prepends C bits and removes the leftmost bit of n. Step 3 prepends
C −1 = log2 M = log2(log2(2n)) zeros. The total length of the delta code is therefore
the 3-part sum
log2(2n) + [1 + log2 log2(2n)] −1 + log2 log2(2n) = 1 + ⌊log2 n⌋+ 2⌊log2 log2(2n)⌋.



step 1



step 2



step 3
(1.3)
Figure 1.5 illustrates the length graphically.
It is easy to show that this code is ideal for data where the integer n occurs with
probability 1/[2n(log2(2n))2]. The length of the delta code is li = 1+log i+2 log log(2i).
If we select symbol probabilities Pi = 1/[2i(log(2i))2] (where the ﬁrst ﬁve values are 0.5,
0.0625, 0.025, 0.0139, and 0.009), both the average code length and the entropy become
the identical sums

i
log 2 + log i + 2 log log(2i)
2i(log(2i))2
,
indicating that the redundancy is zero and the delta code is therefore asymptotically
optimal for this type of data.
An equivalent way to construct the delta code employs the gamma code:
1. Find the largest integer N such that 2N ≥n < 2N+1 and write n = 2N + L.
Notice that L is at most an N-bit integer.
2. Encode N + 1 with the Elias gamma code.
3. Append the binary value of L, as an N-bit integer, to the result of step 2.
When these steps are applied to n = 17, the results are: 17 = 2N +L = 24 +1. The
gamma code of N + 1 = 5 is 00101, and appending L = 0001 to this yields 00101|0001.
Table 1.6 lists the ﬁrst 18 delta codes, where the L part is in italics.
1 = 20 + 0 →|L| = 0 →1
10 = 23 + 2 →|L| = 3 →00100010
2 = 21 + 0 →|L| = 1 →0100
11 = 23 + 3 →|L| = 3 →00100011
3 = 21 + 1 →|L| = 1 →0101
12 = 23 + 4 →|L| = 3 →00100100
4 = 22 + 0 →|L| = 2 →01100
13 = 23 + 5 →|L| = 3 →00100101
5 = 22 + 1 →|L| = 2 →01101
14 = 23 + 6 →|L| = 3 →00100110
6 = 22 + 2 →|L| = 2 →01110
15 = 23 + 7 →|L| = 3 →00100111
7 = 22 + 3 →|L| = 2 →01111
16 = 24 + 0 →|L| = 4 →001010000
8 = 23 + 0 →|L| = 3 →00100000
17 = 24 + 1 →|L| = 4 →001010001
9 = 23 + 1 →|L| = 3 →00100001
18 = 24 + 2 →|L| = 4 →001010010
Table 1.6: 18 Elias Delta Codes.
Decoding is done in the following steps:
1. Read bits from the code until you can decode an Elias gamma code. Call the
decoded result M + 1. This is done in the following substeps:
1.1 Count the leading zeros of the code and denote the count by C.

1.1 Variable-Length Codes
35
1.2 Examine the leftmost 2C + 1 bits (C zeros, followed by a single 1, followed by
C more bits). This is the decoded gamma code M + 1.
2. Read the next M bits. Call this number L.
3. The decoded integer is 2M + L.
In the case of n = 17, the delta code is 001010001. We skip two zeros, so C = 2.
The value of the leftmost 2C + 1 = 5 bits is 00101 = 5, so M + 1 = 5. We read the next
M = 4 bits 0001, and end up with the decoded value 2M + L = 24 + 1 = 17.
Elias omega code. Unlike the previous Elias codes, the omega code uses itself
recursively to encode the preﬁx M, which is why it is sometimes referred to as a recursive
Elias code. The main idea is to prepend the length of n to n as a group of bits that
starts with a 1, then prepend the length of the length, as another group, to the result,
and continue prepending lengths until the last length is 2 or 3 (and therefore ﬁts in two
bits). In order to distinguish between a length group and the last, rightmost group (of
n itself), the latter is followed by a delimiter of 0, while each length group starts with a
1.
Encoding a positive integer n is done recursively in the following steps:
1. Initialize the code-so-far to 0.
2. If the number to be encoded is 1, stop; otherwise, prepend the binary represen-
tation of n to the code-so-far. Assume that we have prepended L bits.
3. Repeat step 2, with the binary representation of L −1 instead of n.
The integer 17 is therefore encoded by (1) a single 0, (2) prepended by the 5-bit
binary value 10001, (3) prepended by the 3-bit value of 5−1 = 1002, and (4) prepended
by the 2-bit value of 3 −1 = 102. The result is 10|100|10001|0.
Table 1.7 lists the ﬁrst 18 omega codes. Note that n = 1 is handled as a special
case.
1
0
10
11 1010 0
2
10 0
11
11 1011 0
3
11 0
12
11 1100 0
4
10 100 0
13
11 1101 0
5
10 101 0
14
11 1110 0
6
10 110 0
15
11 1111 0
7
10 111 0
16
10 100 10000 0
8
11 1000 0
17
10 100 10001 0
9
11 1001 0
18
10 100 10010 0
Table 1.7: 18 Elias Omega Codes.
Decoding is done in several nonrecursive steps where each step reads a group of
bits from the code. A group that starts with a zero signals the end of decoding.
1. Initialize n to 1.
2. Read the next bit. If it is 0, stop. Otherwise read n more bits, assign the group
of n + 1 bits to n, and repeat this step.

36
1.
Approaches to Compression
Some readers may ﬁnd it easier to understand these steps rephrased as follows.
1. Read the ﬁrst group, which will either be a single 0, or a 1 followed by n more
digits. If the group is a 0, the value of the integer is 1; if the group starts with a 1, then
n becomes the value of the group interpreted as a binary number.
2. Read each successive group; it will either be a single 0, or a 1 followed by n more
digits. If the group is a 0, the value of the integer is n; if it starts with a 1, then n
becomes the value of the group interpreted as a binary number.
Example. Decode 10|100|10001|0. The decoder initializes n = 1 and reads the ﬁrst
bit. It is a 1, so it reads n = 1 more bit (0) and assigns n = 102 = 2. It reads the next
bit. It is a 1, so it reads n = 2 more bits (00) and assigns the group 100 to n. It reads
the next bit. It is a 1, so it reads four more bits (0001) and assigns the group 10001 to
n. The next bit read is 0, indicating the end of decoding.
The omega code is constructed recursively, which is why its length |ω(n)| can also
be computed recursively. We deﬁne the quantity lk(n) recursively by l1(n) = ⌊log2 n⌋
and li+1(n) = l1(li(n)). Equation (1.1) tells us that |β(n)| = l1(n) + 1 (where β is the
standard binary representation), and this implies that the length of the omega code is
given by the sum
|ω(n)| =
k

i=1
β(lk−i(n)) + 1 = 1 +
k

i=1
(li(n) + 1),
where the sum stops at the k that satisﬁes lk(n) = 1. From this, Elias concludes that
the length satisﬁes |ω(n)| ≤1 + 5
2⌊log2 n⌋.
A glance at a table of these codes shows that their lengths ﬂuctuate. In general,
the length increases slowly as n increases, but when a new length group is added, which
happens when n = 22k for any positive integer k, the length of the code increases
suddenly by several bits. For k values of 1, 2, 3, and 4, this happens when n reaches 4, 16,
256, and 65,536. Because the groups of lengths are of the form “length,” “log(length),”
“log(log(length)),” and so on, the omega code is sometimes referred to as a logarithmic-
ramp code.
Table 1.8 compares the length of the gamma, delta, and omega codes. It shows that
the delta code is asymptotically best, but if the data consists mostly of small numbers
(less than 8) and there are only a few large integers, then the gamma code performs
better.
1.1.3 Rice Codes
The Rice code is named after its originator, Robert F. Rice ([Rice 79], [Rice 91], and
[Fenwick 96a]). This code is a special case of the Golomb code [Salomon 07], which is
why it is sometimes referred to as the Golomb–Rice code.
A Rice code depends on the choice of a base k and is computed in the following
steps: (1) Separate the sign bit from the rest of the number.
This is optional and
the bit becomes the most-signiﬁcant bit of the Rice code. (2) Separate the k LSBs.
They become the LSBs of the Rice code. (3) Code the remaining j = ⌊n/2k⌋bits as
either j zeros followed by a 1 or j 1’s followed by a 0 (similar to the unary code). This
becomes the middle part of the Rice code. Thus, this code is computed with a few logical

1.1 Variable-Length Codes
37
Values
Gamma
Delta
Omega
1
1
1
2
2
3
4
3
3
3
4
4
4
5
5
4
5–7
5
5
5
8–15
7
8
6–7
16–31
9
9
7–8
32–63
11
10
8–10
64–88
13
11
10
100
13
11
11
1000
19
16
16
104
27
20
20
105
33
25
25
105
39
28
30
Table 1.8: Lengths of Three Elias Codes.
operations, which makes it an ideal candidate for applications where speed is important.
Table 1.9 shows examples of this code for k = 2 (the column labeled “No. of ones” lists
the number of 1’s in the middle part of the code).
No. of
i Binary Sign LSB
ones
Code
i
Code
0
0
0
00
0
0|0|00
1
1
0
01
0
0|0|01
−1 1|0|01
2
10
0
10
0
0|0|10
−2 1|0|10
3
11
0
11
0
0|0|11
−3 1|0|11
4
100
0
00
1
0|10|00
−4 1|10|00
5
101
0
01
1
0|10|01
−5 1|10|01
6
110
0
10
1
0|10|10
−6 1|10|10
7
111
0
11
1
0|10|11
−7 1|10|11
8
1000
0
00
2
0|110|00
−8 1|110|00
11
1011
0
11
2
0|110|11
−11 1|110|11
12
1100
0
00
3
0|1110|00
−12 1|1110|00
15
1111
0
11
3
0|1110|11
−15 1|1110|11
Table 1.9: Various Positive and Negative Rice Codes.
The length of the (unsigned) Rice code of the integer n with parameter k is 1 + k +
⌊n/2k⌋bits, indicating that these codes are suitable for data where the integer n appears
with a probability P(n) that satisﬁes log2 P(n) = −(1 + k + n/2k) or P(n) ∝2−n, an
exponential distribution, such as the Laplace distribution. The Rice code is easy to
decode, once the decoder reads the sign bit and skips to the ﬁrst 0 from the left, it
knows how to generate the left and middle parts of the code. The next k bits should be
read and appended to that.

38
1.
Approaches to Compression
There remains the question of what base value n to select for the Rice codes. The
base determines how many low-order bits of a data symbol are included directly in the
Rice code, and this is linearly related to the variance of the data symbol. Tony Robinson,
the developer of the Shorten method for audio compression [Robinson 94], provides the
formula n = log2[log(2)E(|x|)], where E(|x|) is the expected value of the data symbols.
This value is the sum  |x|p(x) taken over all possible symbols x.
Figure 1.10 lists the lengths of various Rice codes and compares them to the length
of the standard binary (beta) code.
10
100
1000
10000
10
20
40
60
80
100
120
5
106
k=2
n
k=8
3
4
5
k=12
k=16
Binary
Code
Length
Figure 1.10: Lengths of Various Rice Codes.
(* Lengths of binary code and 7 Rice codes *)
bin[i_] := 1 + Floor[Log[2, i]];
Table[{Log[10, n], bin[n]}, {n, 1, 1000000, 500}];
gb = ListPlot[%, AxesOrigin -> {0, 0},
PlotJoined -> True,
PlotStyle -> { AbsoluteDashing[{6, 2}]}]
rice[k_, n_] := 1 + k + Floor[n/2^k];
k = 2; Table[{Log[10, n], rice[k, n]}, {n, 1, 10000, 10}];
g2 = ListPlot[%, AxesOrigin -> {0, 0},
PlotJoined -> True]
k = 3; Table[{Log[10, n], rice[k, n]}, {n, 1, 10000, 10}];
g3 = ListPlot[%, AxesOrigin -> {0, 0},
PlotJoined -> True]
k = 4; Table[{Log[10, n], rice[k, n]}, {n, 1, 10000, 10}];
g4 = ListPlot[%, AxesOrigin -> {0, 0},
PlotJoined -> True]
k = 5; Table[{Log[10, n], rice[k, n]}, {n, 1, 10000, 10}];
g5 = ListPlot[%, AxesOrigin -> {0, 0},
PlotJoined -> True]
k = 8; Table[{Log[10, n], rice[k, n]}, {n, 1, 100000, 50}];
g8 = ListPlot[%, AxesOrigin -> {0, 0},
PlotJoined -> True]
k = 12; Table[{Log[10, n], rice[k, n]}, {n, 1, 500000, 100}];
g12 = ListPlot[%, AxesOrigin -> {0, 0},
PlotJoined -> True]
k = 16; Table[{Log[10, n], rice[k, n]}, {n, 1, 1000000, 100}];
g16 = ListPlot[%, AxesOrigin -> {0, 0},
PlotJoined -> True]
Show[gb, g2, g3, g4, g5, g8, g12, g16, PlotRange -> {{0, 6}, {0, 120}}]
Code for Figure 1.10

1.1 Variable-Length Codes
39
1.1.4 The Kraft–McMillan Inequality
The Kraft–McMillan inequality is concerned with the existence of a uniquely decodable
(UD) code. It establishes the relation between such a code and the lengths Li of its
codewords.
One part of this inequality, due to [McMillan 56], states that given a UD variable-
length code, with n codewords of lengths Li, the lengths must satisfy the relation
n

i=1
2−Li ≤1.
(1.4)
The other part, due to [Kraft 49], states the opposite. Given a set of n positive integers
(L1, L2, . . . , Ln) that satisfy Equation (1.4), there exists an instantaneous variable-length
code such that the Li are the lengths of its individual codewords.
Together, both parts say that there is an instantaneous variable-length code with
codeword lengths Li if and only if there is a UD code with these codeword lengths. The
two parts do not say that a variable-length code is instantaneous or UD if and only if the
codeword lengths satisfy Equation (1.4). In fact, it is easy to check the three individual
code lengths of the code (0, 01, 011) and verify that 2−1 + 2−2 + 2−3 = 7/8. This code
satisﬁes the Kraft–McMillan inequality and yet it is not instantaneous, because it is not
a preﬁx code. Similarly, the code (0, 01, 001) also satisﬁes Equation (1.4), but is not
UD. A few more comments on this inequality are in order:
If a set of lengths Li satisﬁes Equation (1.4), then there exist instantaneous and
UD variable-length codes with these lengths. For example (0, 10, 110).
A UD code is not always instantaneous, but there exists an instantaneous code with
the same codeword lengths. For example, code (0, 01, 11) is UD but not instantaneous,
while code (0, 10, 11) is instantaneous and has the same lengths.
The sum of Equation (1.4) corresponds to the part of the complete code tree that
has been used for codeword selection. This is why the sum has to be less than or equal
to 1. This intuitive explanation of the Kraft–McMillan relation is explained in the next
paragraph.
We can gain a deeper understanding of this useful and important inequality by
constructing the following simple preﬁx code. Given ﬁve symbols ai, suppose that we
decide to assign 0 as the code of a1. Now all the other codes have to start with 1. We
therefore assign 10, 110, 1110, and 1111 as the codewords of the four remaining symbols.
The lengths of the ﬁve codewords are 1, 2, 3, 4, and 4, and it is easy to see that the sum
2−1 + 2−2 + 2−3 + 2−4 + 2−4 = 1
2 + 1
4 + 1
8 + 2
16 = 1
satisﬁes the Kraft–McMillan inequality. We now consider the possibility of constructing
a similar code with lengths 1, 2, 3, 3, and 4. The Kraft–McMillan inequality tells us
that this is impossible, because the sum
2−1 + 2−2 + 2−3 + 2−3 + 2−4 = 1
2 + 1
4 + 2
8 + 1
16

40
1.
Approaches to Compression
is greater than 1, and this is easy to understand when we consider the code tree. Starting
with a complete binary tree of height 4, it is obvious that once 0 was assigned as a
codeword, we have “used” one half of the tree and all future codes would have to be
selected from the other half of the tree. Once 10 was assigned, we were left with only
1/4 of the tree. Once 110 was assigned as a codeword, only 1/8 of the tree remained
available for the selection of future codes. Once 1110 has been assigned, only 1/16 of
the tree was left, and that was enough to select and assign code 1111. However, once
we select and assign codes of lengths 1, 2, 3, and 3, we have exhausted the entire tree
and there is nothing left to select the last (4-bit) code from.
The Kraft–McMillan inequality can be related to the entropy by observing that the
lengths Li can always be written as Li = −log2 Pi + Ei, where Ei is simply the amount
by which Li is greater than the entropy (the extra length of code i).
This implies that
2−Li = 2(log2 Pi−Ei) = 2log2 Pi/2Ei = Pi/2Ei.
In the special case where all the extra lengths are the same (Ei = E), the Kraft–McMillan
inequality says that
1 ≥
n

i=1
Pi/2E =
n
i=1 Pi
2E
= 1
2E =⇒2E ≥1 =⇒E ≥0.
An unambiguous code has nonnegative extra length, meaning its length is greater than
or equal to the length determined by its entropy.
Here is a simple example of the use of this inequality. Consider the simple case of
n equal-length binary codewords. The size of each codeword is Li = log2 n, and the
Kraft–McMillan sum is
n

1
2−Li =
n

1
2−log2 n =
n

1
1
n = 1.
The inequality is satisﬁed, so such a code is UD.
A more interesting example is the case of n symbols where the ﬁrst one is compressed
and the second one is expanded. We set L1 = log2 n −a, L2 = log2 n + e, and L3 =
L4 = · · · = Ln = log2 n, where a and e are positive. We show that e > a, which means
that compressing a symbol by a factor a requires expanding another symbol by a larger
factor. We can beneﬁt from this only if the probability of the compressed symbol is
greater than that of the expanded symbol.
n

1
2−Li = 2−L1 + 2−L2 +
n

3
2−log2 n
= 2−log2 n+a + 2−log2 n−e +
n

1
2−log2 n −2 × 2−log2 n
= 2a
n + 2−e
n
+ 1 −2
n.

1.2 Run-Length Encoding
41
The Kraft–McMillan inequality requires that
2a
n + 2−e
n
+ 1 −2
n ≤1,
or
2a
n + 2−e
n
−2
n ≤0,
or 2−e ≤2 −2a, implying −e ≤log2(2 −2a), or e ≥−log2(2 −2a).
The inequality above implies a ≤1 (otherwise, 2 −2a is negative) but a is also
positive (since we assumed compression of symbol 1). The possible range of values of
a is therefore (0, 1], and in this range e is greater than a, proving the statement above.
(It is easy to see that a = 1 →e ≥−log2 0 = ∞, and a = 0.1 →e ≥−log2(2 −20.1) ≈
0.10745.)
It can be shown that this is just a special case of a general result that says, given
an alphabet of n symbols, if we compress some of them by a certain factor, then the
others must be expanded by a greater factor.
1.2 Run-Length Encoding
The technique of run-length encoding (RLE) has been mentioned in the Prelude to this
chapter. The idea is that in certain types of data, such as images and audio, adjacent
symbols are often correlated, so there may be runs of identical symbols which may be
exploited to compress the data. The following are the main considerations that apply
to this technique:
Text in a natural language (as well as names) may have many doubles and a few
triples—as in AAA (an acronym), abbess, Emmanuelle, bookkeeper, arrowwood, freeer
(old usage), and hostessship (used by Shakespeare)—but longer runs are limited to
consecutive spaces and periods. Thus, RLE is not a good candidate for text compression.
In a bi-level image there are two types of symbols, namely black and white pixels, so
runs of pixels alternate between these two colors, which implies that RLE can compress
such an image by replacing each run with its length.
In general, the data to be compressed includes several types of symbols, so RLE
compresses a run by replacing it with a pair (length, symbol).
If the run is short, such a pair may be longer than the run of symbols, thereby
leading to expansion. A reasonable solution is to write such short runs on the output in
raw format, so at least they do not cause expansion. However, this raises the question
of distinguishing between pairs and raw items, because in the output ﬁle both types are
binary strings. A practical RLE program must therefore precede each pair and each raw
item with a 1-bit indicator. Thus, a pair becomes the triplet (0, length, symbol) and a
raw item becomes the pair (1, symbol).
Runs have diﬀerent lengths, which is why the pairs and triplets have diﬀerent
lengths. It therefore makes sense to replace each by a variable-length code and write the
codes on the output. Thus, RLE is normally just one step in a multistep compression
algorithm that may include a transform, variable-length codes, and perhaps also quan-
tization. The fax compression standard (Section 2.4) and the JPEG image compression

42
1.
Approaches to Compression
method (Section 5.6) employ specially-selected Huﬀman codes to write the run lengths
on the output.
The remainder of this section provides more information on the application of RLE
to the compression of bi-level and grayscale images.
The size of the compressed data depends on the complexity of the image. The more
detailed the image, the worse the compression. However, given an image with uniform
regions, it is easy to estimate the compression ratio of RLE. Figure 1.11 shows how scan
lines go through a uniform region. A line enters through one point on the perimeter of
the region and exits through another point, and these two points are not part of any
other scan lines. It is now clear that the number of scan lines traversing a uniform region
is roughly equal to half the length (measured in pixels) of its perimeter. Since the region
is uniform, each scan line contributes two runs to the output for each region it crosses.
The compression ratio of a uniform region therefore roughly equals the ratio
2×half the length of the perimeter
total number of pixels in the region = perimeter
area
.
Figure 1.11: Uniform Areas and Scan Lines.
⋄Exercise 1.4: What would be the compressed ﬁle in the case of the following 6 × 8
bi-level image?
RLE can also be used to compress grayscale images. Each run of pixels of the same
intensity (gray level) is encoded as a pair (run length, pixel value). The run length
is either emitted as one byte, allowing for runs of up to 255 pixels, or is encoded by
a variable-length code. The pixel value is encoded in a short ﬁxed-length code whose
length depends on the number of gray levels (typically between 4 and 8 bits).

1.2 Run-Length Encoding
43
Example: An 8-bit-deep grayscale bitmap that starts with
12, 12, 12, 12, 12, 12, 12, 12, 12, 35, 76, 112, 67, 87, 87, 87, 5, 5, 5, 5, 5, 5, 1, . . .
is compressed into the sequence of bytes 9,12,35,76,112,67,3,87,6,5,1,. . . , where the
boxed values indicate counts. The problem is to distinguish between a byte containing
a grayscale value (such as 12) and one containing a count (such as 9). Here are some
solutions (although not the only possible ones):
If the image is limited to just 128 grayscales, we can devote one bit in each byte to
indicate whether the byte contains a grayscale value or a count.
If the number of grayscales is 256, it can be reduced to 255 with one value reserved
as a ﬂag to precede every byte containing a count. If the ﬂag is, say, 255, then the
sequence above becomes 255,9,12,35,76,112,67,255,3,87,255,6,5,1,. . . .
Again, one bit is devoted to each byte to indicate whether the byte contains a
grayscale value or a count. This time, however, these extra bits are accumulated in
groups of 8, and each group is written on the output preceding (or following) the eight
bytes it corresponds to.
As an example, the sequence 9,12,35,76,112,67,3,87,6,5,1,. . . becomes
10000010,9,12,35,76,112,67,3,87,100.....,6,5,1,. . . .
The total size of the extra bytes is, of course, 1/8 the size of the output (they contain
one bit for each byte of the output), so they increase the size of the output by 12.5%.
A group of m pixels that are all diﬀerent is preceded by a byte with the negative
value −m. The sequence above is encoded by 9, 12, −4, 35, 76, 112, 67, 3, 87, 6, 5, ?, 1, . . .
(the value of the byte with ? is positive or negative depending on what follows the pixel
of 1). The worst case is a sequence of pixels (p1, p2, p2) repeated n times throughout the
bitmap. It is encoded as (−1, p1, 2, p2), four numbers instead of the original three! If
each pixel requires one byte, then the original three bytes are expanded into four bytes.
If each pixel requires three bytes, then the original three pixels (which constitute nine
bytes) are compressed into 1 + 3 + 1 + 3 = 8 bytes.
Three more points should be mentioned:
Since the run length cannot be 0, it makes sense to write the [run length minus one]
on the output. Thus the pair (3, 87) denotes a run of four pixels with intensity 87. This
way, a run can be up to 256 pixels long.
In color images it is common to have each pixel stored as three bytes, represent-
ing the intensities of the red, green, and blue components of the pixel.
In such a
case, runs of each color should be encoded separately. Thus, the pixels (171, 85, 34),
(172, 85, 35), (172, 85, 30), and (173, 85, 33) should be separated into the three vectors
(171, 172, 172, 173, . . .), (85, 85, 85, 85, . . .), and (34, 35, 30, 33, . . .). Each vector should be
run-length encoded separately. This means that any method for compressing grayscale
images can be applied to color images as well.
It is preferable to encode each row of the bitmap individually. Thus, if a row ends
with four pixels of intensity 87 and the following row starts with nine such pixels, it is

44
1.
Approaches to Compression
better to write . . . , 4, 87, 9, 87, . . . on the output rather than . . . , 13, 87, . . .. It is even
better to output the sequence . . . , 4, 87, eol, 9, 87, . . ., where “eol” is a special end-of-
line code. The reason is that sometimes the user may decide to accept or reject an
image just by examining a rough version of it, without any details.
If each line is
encoded individually, the decoding algorithm can start by decoding and displaying lines
1, 6, 11, . . ., follow with lines 2, 7, 12, . . ., and continue in the same way. The individual
rows of the image are interlaced, and the image is displayed on the screen gradually, in
steps. This way, it is possible to get an idea of what is in the image at an early stage,
when only a small fraction of it has been displayed. Figure 1.12c shows an example of
such a scan.
1
2
3
4
5
6
7
8
9
10
(a)
(b)
(c)
Figure 1.12: RLE Scanning.
Another advantage of individual encoding of rows is to make it possible to extract
just part of an encoded image (such as rows k through l). Yet another application is to
merge two compressed images without having to decompress them ﬁrst.
If this idea (encoding each bitmap row individually) is adopted, then the compressed
ﬁle must contain information on where each bitmap row starts in the ﬁle. This can be
done by writing, at the start of the ﬁle, a header with a group of four bytes (32 bits) for
each bitmap row. The kth group contains the oﬀset (in bytes) from the start of the ﬁle
to the start of the information for image row k. This increases the size of the compressed
ﬁle but may still oﬀer a good trade-oﬀbetween space (size of compressed ﬁle) and time
(time for the user to decide whether to accept or reject the image).
⋄Exercise 1.5: There is another, obvious, reason why each bitmap row should be coded
individually. What is it?
⋄Exercise 1.6: In the encoding of a run length, a special symbol has been used to signal
the end of a scan line. Is the insertion of the eol always necessary? If we decide to signal
the end of a scan line, is it really necessary to allocate a special symbol for it?

1.2 Run-Length Encoding
45
% Returns the run lengths of
% a matrix of 0s and 1s
function R=runlengths(M)
[c,r]=size(M);
for i=1:c;
x(r*(i-1)+1:r*i)=M(i,:);
end
N=r*c;
y=x(2:N);
u=x(1:N-1);
z=y+u;
j=find(z==1);
i1=[j N];
i2=[0 j];
R=i1-i2;
the test
M=[0 0 0 1; 1 1 1 0; 1 1 1 0]
runlengths(M)
produces
3
4
1
3
1
(a)
(b)
Figure 1.13: (a) Matlab Code To Compute Run Lengths. (b) A Bitmap.
Figure 1.13a lists Matlab code to compute run lengths for a bi-level image. The
code is very simple. It starts by ﬂattening the matrix into a one-dimensional vector, so
the run lengths continue from row to row.
Image RLE has its downside as well. When the image is modiﬁed, the run lengths
normally have to be completely redone.
The RLE output can sometimes be bigger
than pixel-by-pixel storage (i.e., an uncompressed image, a raw dump of the bitmap)
for complex pictures. Imagine a picture with many vertical lines. When it is scanned
horizontally, it produces very short runs, resulting in very bad compression, or even in
expansion. A good, practical RLE image compressor should be able to scan the bitmap
by rows, columns, or in a zigzag pattern (Figure 1.12a,b) and it may even try all three
ways on every bitmap it compresses to achieve the best compression.
⋄Exercise 1.7: Figure 1.12 shows three alternative scannings of an image. What is the
advantage of method (b) over (a) and (c)? Does method (b) have any disadvantage?
⋄Exercise 1.8: Given the 8 × 8 bitmap of Figure 1.13b, use RLE to compress it, ﬁrst
row by row, then column by column. Describe the results in detail.
Lossy RLE Image Compression. It is possible to achieve better compression if
short runs are ignored. Such a method loses information when compressing an image,
but this is sometimes acceptable. (Medical X-rays and images taken by large telescopes
are examples of data whose compression must be lossless.)
A lossy run-length encoding algorithm should start by asking the user for the longest
run that can be ignored.
If the user speciﬁes 3, then the program merges all runs

46
1.
Approaches to Compression
of 1, 2, or 3 identical pixels with their two immediate neighbors.
The run lengths
“6,8,1,2,4,3,11,2” would be saved, in this case, as “6,8,7,16” where 7 is the sum 1 +
2 + 4 (three runs merged) and 16 is the sum 3 + 11 + 2. This makes sense for large,
high-resolution images where the loss of some detail may be imperceptible, but may
signiﬁcantly reduce the size of the output ﬁle.
Space-Filling Curves. A space-ﬁlling curve is a parametric function P(t) that passes
through every mathematical point in a given two-dimensional region, normally the unit
square, when its parameter t varies in the interval [0, 1]. For any real t0 in this interval,
P(t0) is a point [x0, y0] in the unit square. Mathematically, such a curve is a mapping
from the interval [0, 1] to the two-dimensional interval [0, 1] × [0, 1].
To understand
how such a curve is constructed, it is best to think of it as the limit of an inﬁnite
sequence of recursively-constructed curves P1(t), P2(t), . . ., which are drawn inside the
unit square, where each curve is derived from its predecessor by a process of reﬁnement
which produces longer and longer curves. The details of the reﬁnement depend on the
speciﬁc curve. The most-well-known space-ﬁlling curves are the Peano curve, the Hilbert
curve, and the Sierpi´nski curve. Because the recursive sequence of curves is inﬁnite, it
is impossible to compute all its components. In practice, however, we are interested in a
curve that passes through every pixel in a ﬁnite bitmap, not through every mathematical
point in the unit square.
Space-ﬁlling curves are useful in data compression, speciﬁcally in image compres-
sion, because they provide another way of scanning a bitmap. Given an image that we
want to compress by RLE, we can scan it by rows, by columns, in a zigzag pattern, or
in the order provided by a space-ﬁlling curve.
The Hilbert Curve
This discussion is based on the approach taken by [Wirth 76]. The most familiar
of the space-ﬁlling curves is the Hilbert curve, described by the great mathematician
David Hilbert in 1891. The Hilbert curve [Hilbert 91] is the limit of a sequence H0, H1,
H2, . . . of curves, some of which are shown in Figure 1.14. Each curve Hi is constructed
recursively by making four copies of the preceding curve Hi−1, shrinking, rotating, and
connecting them. The resulting curve Hi ends up covering the same area as its prede-
cessor, but is longer. This is the reﬁnement process for the Hilbert curve.
The curve is deﬁned by the following steps:
0. H0 is a single point.
1. H1 consists of four copies of (the point) H0, connected with three straight seg-
ments of length h at right angles to each other. Four orientations of this curve, labeled
1, 2, 3, and 4, are shown in Figure 1.14a.
2. The next curve, H2, in the sequence is constructed by connecting four copies of
diﬀerent orientations of H1 with three straight segments of length h/2 (shown in bold
in Figure 1.14b). Again there are four possible orientations of H2, and the one shown is
#2. It is constructed of orientations 1223 of H1, connected by segments that go to the
right, up, and to the left. The construction of the four orientations of H2 is summarized
in Figure 1.14d.

1.3 Dictionary-Based Methods
47
Curve H3 is shown in Figure 1.14c. The particular curve shown is orientation 1223
of H2.
2
1
3
4
1
2
2
3
(a)
(b)
(c)
1: 2 ↑
1 →1 ↓
4
2: 1 →2 ↑
2 ←3
3: 4 ↓
3 ←3 ↑
2
4: 3 ←4 ↓
4 →1
(d)
Figure 1.14: Hilbert Curves of Orders 1, 2, and 3 and Construction Rules.
A chess board has 64 squares. Given a set of 32 cardboard rectangles, each covering
two adjacent squares, we can easily use them to cover the entire board. We now remove
two diagonally-opposite squares from the chess board, leaving 62 squares. Can they be
covered by 31 cardboard rectangles?
1.3 Dictionary-Based Methods
Dictionary-based compression methods are based on the fact that parts of data tend
to appear several times in a given data ﬁle.
Thus, a text ﬁle may contain several
occurrences of a word, a phrase, or a syllable. In an image ﬁle, the same string of pixels
may appear many times, and in an audio ﬁle, a string of audio samples may also appear
repeatedly. A dictionary-based method maintains a dictionary that contains bits and
pieces of the data. As a string of data symbols is read from the input, the algorithm
searches the dictionary for the longest match to the string. Once a match is found,
the string is compressed by replacing it with a pointer to the dictionary. Quite a few
dictionary-based methods are known and the diﬀerences between them are in the way
they organize and maintain the dictionary, in how they handle strings not found in the
dictionary, and in how they write their results (pointers, lengths, raw items, and perhaps
ﬂag bits) on the output.
The entire ﬁeld of dictionary-based compression is based on the pioneering work
of two researchers, Jacob Ziv and Abraham Lempel. In 1977 and 1978, they published
two papers that laid the foundation of this ﬁeld and on which later workers based their
algorithms. The basic methods developed by Ziv and Lempel have become known as
LZ77 and LZ78, and most other dictionary-based algorithms include the digram LZ in
their names. The remainder of this section describes LZ77, a simple, albeit not very
eﬃcient, dictionary-based method.
1.3.1 LZ77 (Sliding Window)
LZ77 (sometimes also referred to as LZ1) was originally proposed in [Ziv and Lempel 77].
The main idea is to use part of the previously-processed input as the dictionary. The

48
1.
Approaches to Compression
encoder maintains a window to the input data and shifts the input in that window from
right to left as strings of symbols are being read and encoded. Thus, the method is
based on a sliding window. The window shown here is divided into two parts. The part
on the left is the search buﬀer. This is the current dictionary, and it includes symbols
that have recently been input and encoded. The part on the right is the look-ahead
buﬀer, with text yet to be read and encoded. In practical implementations the search
buﬀer is some thousands of bytes long, while the look-ahead buﬀer is only tens of bytes
long. The vertical bar between the t and the e represents the dividing line between the
two buﬀers. It indicates that the text sir␣sid␣eastman␣easily␣t has already been
compressed, while the text eases␣sea␣sick␣seals still needs to be compressed.
←coded text. . . sir␣sid␣eastman␣easily␣t|eases␣sea␣sick␣seals. . . ←text to be read
The encoder scans the search buﬀer backwards (from right to left) looking for a
match for the ﬁrst symbol e in the look-ahead buﬀer. It ﬁnds one at the e of the word
easily. This e is at a distance (oﬀset) of 8 from the end of the search buﬀer. The
encoder then matches as many symbols following the two e’s as possible. Three symbols
eas match in this case, so the length of the match is 3. The encoder then continues the
backward scan, trying to ﬁnd longer matches. In our case, there is one more match, at
the word eastman, with oﬀset 16, and it has the same length. The encoder selects the
longest match or, if they are all the same length, the last one found, and prepares the
token (16, 3, e).
Selecting the last match, rather than the ﬁrst one, simpliﬁes the encoder, because it
has to keep track of only the last match found. It is interesting to note that selecting the
ﬁrst match, while making the program somewhat more complex, also has an advantage.
It selects the smallest oﬀset. It would seem that this is not an advantage, because a
token should have room enough for the largest possible oﬀset. However, a sophisticated,
multistep compression algorithm may employ LZ77 as a ﬁrst step, following which the
LZ77 tokens may be compressed further by replacing them with variable-length codes.
⋄Exercise 1.9: How does the decoder know whether the encoder selects the ﬁrst match
or the last match?
⋄Exercise 1.10: Assuming a very long search buﬀer, what can we say about the distri-
bution of matches? Would there be more matches in the older part (on the left), in the
newer part (on the right), or would the distribution of matches be more or less uniform?
In general, an LZ77 token has three parts: oﬀset, length, and next symbol in the
look-ahead buﬀer (which, in our case, is the second e of the word teases). This token
is written on the output, and the window is shifted to the right (or, alternatively, the
input is moved to the left) four positions: three positions for the matched string and
one position for the next symbol.
...sir␣sid␣eastman␣easily␣tease|s␣sea␣sick␣seals.......
If the backward search yields no match, an LZ77 token with zero oﬀset and length
and with the unmatched symbol is generated and emitted. This is also the reason a
token has a third component. Tokens with zero oﬀset and length are common at the

1.3 Dictionary-Based Methods
49
beginning of any compression job, when the search buﬀer is empty or almost empty.
The ﬁrst ﬁve steps in encoding our example are the following:
|sir␣sid␣eastman␣
⇒
(0,0,s)
s|ir␣sid␣eastman␣e
⇒
(0,0,i)
si|r␣sid␣eastman␣ea
⇒
(0,0,r)
sir|␣sid␣eastman␣eas
⇒
(0,0,␣)
sir␣|sid␣eastman␣easi
⇒
(4,2,d)
⋄Exercise 1.11: What are the next two steps?
Clearly, a token of the form (0, 0, . . .), which encodes a single symbol, provides lousy
compression and may also cause expansion. It is easy to estimate its length. The size
of the oﬀset is ⌈log2 S⌉, where S is the length of the search buﬀer. In practice, the
search buﬀer may be a few thousand bytes long, so the oﬀset size is typically 10–12 bits.
The size of the “length” ﬁeld is similarly ⌈log2(L −1)⌉, where L is the length of the
look-ahead buﬀer (see below for the −1). In practice, the look-ahead buﬀer is only a
few tens of bytes long, so the size of the “length” ﬁeld is just a few bits. The size of the
“symbol” ﬁeld is typically 8 bits, but in general, it is ⌈log2 A⌉, where A is the alphabet
size. The total size of the 1-symbol token (0, 0, . . .) may typically be 11 + 5 + 8 = 24
bits, much longer than the raw 8-bit size of the (single) symbol it encodes.
Here is an example showing why the “length” ﬁeld may be longer than the size of
the look-ahead buﬀer:
...Mr.␣alf␣eastman␣easily␣grows␣alf|alfa␣in␣his␣garden... .
The ﬁrst symbol a in the look-ahead buﬀer matches the ﬁve a’s in the search buﬀer. It
seems that the two extreme a’s match with a length of 3 and the encoder should select
the last (leftmost) of them and create the token (28,3,a). In fact, it creates the token
(3,4,␣). The four-symbol string alfa in the look-ahead buﬀer is matched with the last
three symbols alf in the search buﬀer and the ﬁrst symbol a in the look-ahead buﬀer.
The reason for this is that the decoder can handle such a token naturally, without any
modiﬁcations. It starts at position 3 of its search buﬀer and copies the next four symbols,
one by one, extending its buﬀer to the right. The ﬁrst three symbols are copies of the
old buﬀer contents, and the fourth one is a copy of the ﬁrst of those three. The next
example is even more convincing (and only somewhat contrived):
...alf␣eastman␣easily␣yells␣A|AAAAAAAAAAAAAAAH... .
The encoder creates the token (1,9,A), matching the ﬁrst nine copies of A in the look-
ahead buﬀer and including the tenth A. This is why, in principle, the length of a match
can be up to the size of the look-ahead buﬀer minus 1.
The decoder is much simpler than the encoder (LZ77 is therefore an asymmetric
compression method). It has to maintain a buﬀer, equal in size to the encoder’s window.
The decoder inputs a token, ﬁnds the match in its buﬀer, writes the match and the
third token ﬁeld on the output, and shifts the matched string and the third ﬁeld into
the buﬀer. This implies that LZ77, or any of its variants, is useful in cases where a ﬁle is
compressed once (or just a few times) and is decompressed often. A rarely-used archive
of compressed ﬁles is a good example.

50
1.
Approaches to Compression
At ﬁrst it seems that this method does not make any assumptions about the input
data. Speciﬁcally, it does not pay attention to any symbol frequencies. A little thinking,
however, shows that because of the nature of the sliding window, the LZ77 method
always compares the look-ahead buﬀer to the recently-input text in the search buﬀer
and never to text that was input long ago (which has therefore been ﬂushed out of the
search buﬀer). Thus, the method implicitly assumes that patterns in the input data
occur close together. Data that satisﬁes this assumption compresses well.
The basic LZ77 method was improved in several ways by researchers and program-
mers during the 1980s and 1990s. One way to improve it is to use variable-size “oﬀset”
and “length” ﬁelds in the tokens. Another option is to increase the sizes of both buﬀers.
Increasing the size of the search buﬀer makes it possible to ﬁnd better matches, but
the trade-oﬀis an increased search time. A large search buﬀer therefore requires a so-
phisticated data structure that allows for fast search. A third improvement has to do
with sliding the window. The simplest approach is to move all the text in the window
to the left after each match. A faster method is to replace the linear window with a
circular queue, where sliding the window is done by resetting two pointers. Yet another
improvement is adding an extra bit (a ﬂag) to each token, thereby eliminating the third
ﬁeld. Of special notice is the hash table employed by the Deﬂate algorithm [Salomon 07]
to search for matches.
1.4 Transforms
A transform is a mathematical operation that changes the appearance or representation
of the objects being transformed. A transform by itself does not compress data and
is only one step in a multistep compression algorithm. However, transforms play an
important role in data compression, especially in the compression of images. A digital
image can be compressed mainly because neighboring pixels tend to be similar; the
individual pixels are correlated. An image transform takes advantage of this feature and
converts correlated pixels to a representation where they are independent.
Two types of transforms are employed in image compression, namely orthogonal and
subband. They are described in detail in Chapter 5, while this section only illustrates
the power of a transform by an example. Consider the simple mathematical expression
(x∗, y∗) = (x, y)

cos 45◦
−sin 45◦
sin 45◦
cos 45◦

= (x, y) 1
√
2

1
−1
1
1

= (x, y)R.
(1.5)
When applied to a pair (x, y) of consecutive pixels, this expression yields a pair (x∗, y∗) of
transform coeﬃcients. As a simple experiment, we apply it to ﬁve pairs of correlated (i.e.,
similar) numbers to obtain (5, 5) →(7.071, 0), (6, 7) →(9.19, 0.7071), (12.1, 13.2) →
(17.9, 0.78), (23, 25) →(33.9, 1.41), and (32, 29) →(43.13, −2.12). A quick glance at
these numbers veriﬁes their signiﬁcance. The y∗transform coeﬃcient of each pair is a
small (signed) number, close to zero, while the x∗coeﬃcient is not appreciably diﬀerent
from the corresponding x value.

1.5 Quantization
51
⋄Exercise 1.12: Why does a 45◦rotation decorrelate pairs of consecutive pixels?
If we apply this simple transform to all the pixels of an image, two adjacent pixels
at a time, it reduces the sizes of half the pixels without signiﬁcantly changing the sizes of
the other half. In principle, some compression has already been obtained, but in practice
we need an algorithm that replaces the transform coeﬃcients with variable-length codes,
so they can be output and later input unambiguously. A sophisticated algorithm may
extend this transform so that it can be applied to triplets (or even larger n-tuples) of
pixels, not just pairs. This will result in two-thirds of the pixels being transformed into
small numbers, while the remaining third will not change much in size. In addition,
such an algorithm may quantize the small transform coeﬃcients, which results in lossy
compression, but also a better compression ratio.
In order to be practical, a transform must have an inverse. A simple check veriﬁes
that the inverse of our transform is the expression
(x, y) = (x∗, y∗)R−1 = (x∗, y∗)RT = (x∗, y∗) 1
√
2

1
1
−1
1

.
(1.6)
Chapter 5 discusses this transform and its interpretation in some detail. The matrix of
Equation (5.1) is a rotation matrix in two dimensions, and the matrix of Equation (5.2)
is its inverse.
⋄Exercise 1.13: It seems that this simple transform has produced something for nothing.
It has shrunk the sizes of half the numbers without a similar increase in the sizes of the
other half. What’s the explanation?
1.5 Quantization
The dictionary deﬁnition of the term “quantization” is “to restrict a variable quan-
tity to discrete values rather than to a continuous set of values.” In the ﬁeld of data
compression, quantization is employed in two contexts as follows:
If the data symbols are real numbers, quantization may round each to the nearest
integer. If the data symbols are large numbers, quantization may convert them to small
numbers.
Small numbers take less space than large ones, so quantization generates
compression.
On the other hand, small numbers convey less information than large
ones, which is why quantization produces lossy compression.
If the data to be compressed is analog (such as a voltage that varies with time),
quantization is employed to digitize it into numbers (normally integers). This is referred
to as analog-to-digital (A/D) conversion. If the integers generated by quantization are
8 bits each, then the entire range of the analog signal is divided into 256 intervals
and all the signal values within an interval are quantized to the same number. If 16-bit
integers are generated, then the range of the analog signal is divided into 65,536 intervals.
This relation illustrates the compromise between high resolution (a large number of
analog intervals) and high compression (small integers generated). This application of
quantization is used by several speech compression methods.

52
1.
Approaches to Compression
I would not have the courage to raise this possibility if Academician Arkhangelsky
had not come tentatively to the same conclusion.
He and I have disagreed about
the quantization of quasar red shifts, the explanation of superluminal light sources,
the rest mass of the neutrino, quark physics in neutron stars. . . . We have had many
disagreements.
—Carl Sagan, Contact (1986)
If the data symbols are numbers, then each is quantized to another number in a
process referred to as scalar quantization. Alternatively, if each data symbol is a vector,
then vector quantization converts a data symbol to another vector. Both aspects of
quantization are discussed here.
1.5.1 Scalar Quantization
We start with an example of naive discrete quantization.
Given input data of 8-bit
numbers, we can simply delete the least-signiﬁcant four bits of each data item. This
is one of those rare cases where the compression factor (= 2) is known in advance and
does not depend on the data. The input data consists of 256 diﬀerent symbols, while
the output data consists of just 16 diﬀerent symbols. This method is simple but not
very practical because too much information is lost in order to get the unimpressive
compression factor of 2.
The popular JPEG method for image compression (Section 5.6) is based on the
discrete cosine transform (DCT) that transforms a square n × n array of pixel values to
a list of n2 transform coeﬃcients, of which the ﬁrst is large and the rest are small. A
typical output for n = 4 may look like 1171, 34.6, 2, 0, 0, 0, −1, 3.8, 0, 1, 0, 0, 7.15,
2, 0, and 0. Scalar quantization may convert this list to 1171, 34, 2, 0, 0, 0, 0, 4, 0, 0,
0, 0, 7, 2, 0, and 0. The latter list can be highly compressed by replacing each nonzero
coeﬃcient and each run of zeros by variable-length codes.
A better approach to scalar quantization employs a spacing parameter. We assume
that the data consists of 8-bit unsigned integers and we select a spacing parameter s. We
compute the sequence of uniform quantized values 0, s, 2s, . . . , ks, such that ks ≤255
but (k + 1)s > 255. Each input symbol S is quantized by converting it to the nearest
value in this sequence. Selecting s = 3, for example, produces the uniform sequence 0,
3, 6, 9, 12, . . . , 252, 255. Selecting s = 4 produces 0, 4, 8, 12, . . . , 252, 255 (since the
next multiple of 4, after 252, is 256).
A similar approach is to select the quantized values in such a way that any integer
in the range [0, 255] will be no more than d units distant from one of the quantized
values. This is done by dividing the range [0, 255] into segments of size 2d + 1. If we
select d = 16, then the relation 256 = 7(2 × 16 + 1) + 25 implies that the range [0, 255]
should be partitioned into eight segments, seven of size 33 each and one of size 25. The
eight segments cover the subintervals 0–32, 33–65, 66–98, 99–131, 132–164, 165–197,
198–230, and 231–255. We select the middle of each segment as a quantized value and
end up with the eight values 16, 49, 82, 115, 148, 181, 214, and 243. Any integer in the
range [0, 255] is at most 16 units distant from any of these values.
The quantized values above make sense in cases where each symbol appears in the
input data with equal probability (cases where the source is i.i.d.). If the input data is
not uniformly distributed, the sequence of quantized values should be distributed in the

1.5 Quantization
53
same way as the data.
(A sequence or any collection of random variables is independent and identically
distributed (i.i.d.) if all have the same probability distribution and are mutually inde-
pendent. All other things being equal, a sequence of die rolls, a sequence of coin ﬂips,
and an unbiased random walk are i.i.d., because a step has no eﬀect on the following
step.)
bbbbbbbb
bbbbbbbb
1
1
.
10
2
.
11
3
.
100
4
100|000
32
101
5
101|000
40
110
6
110|000
48
111
7
111|000
56
100|0
8
100|0000
64
101|0
10
101|0000
80
110|0
12
110|0000
96
111|0
14
111|0000
112
100|00
16
100|00000
128
101|00
20
101|00000
160
110|00
24
110|00000
192
111|00
28
111|00000
224
Table 1.15: A Logarithmic Quantization Table.
Imagine, for example, input data of 8-bit unsigned integers of which most are zero
or close to zero and only a few are large. A good sequence of quantized values for such
data should have the same distribution, i.e., many small values and only a few large ones.
One way of computing such a sequence is to select a value for the length parameter l,
to construct a “window” of the form
1 b . . . bb
  
l
(where each b is a bit), and place it under each of the 8-bit positions of a data item. If
the window sticks out to the right, some of the l bits are truncated. As the window is
moved to the left, zero bits are appended to it. Table 1.15 illustrates this construction
with l = 2. It is easy to see how the resulting quantized values start with initial spacing
of one unit (i.e., the ﬁrst eight quantized values are 1 through 8), continue with spacing
of two units (the next four quantized values are 8, 10, 12, and 14) and four units, until
the last four values are spaced by 32 units (such as 192 and 224). The numbers 0 and
255 should be manually added to such a quasi-logarithmic sequence to make it more
general.
Figure 1.16 illustrates another aspect of scalar quantization, namely midtread versus
midrise quantization. The ﬁgure shows how continuous (real) x input values (also called

54
1.
Approaches to Compression
decision levels) are quantized to discrete y outputs (also called reconstruction levels).
The midtread quantization principle is to quantize to yi all the input values in the
subinterval ((xi−1 + xi)/2, (xi + xi+1)/2], which is centered on xi.
In contrast, the
philosophy behind midrise quantization is to quantize the subinterval (xi−1, xi] to yi.
Notice that these subintervals are open on the left and closed on the right, but the
reverse edge convention can also be used.
input
output
x1
y1
y0
y2
y3
y4
x2
x3 x4
midtread
input
output
x1
y1
−y2
−y1
y2
y3
y4
x2
−x2
−x2
x3 x4
midrise
Figure 1.16: Midtread and Midrise Quantization.
It is often convenient to use midrise quantization when the number of quantization
values (the yi) is even and employ midtread quantization when this number is odd. Also,
because of its construction, midrise quantization does not have 0 as a quantization value,
which is why applications where certain inputs should be quantized to 0 use midtread.
Notice that the input intervals (xi−1, xi] in Figure 1.16 are uniform (except for the
two extreme intervals) and the same is true for the output values. The ﬁgure illustrates
uniform quantization.
It makes sense to deﬁne the quantization error as the diﬀerence between an input
value and its quantized value, and Figure 1.17 illustrates the behavior of this error as
a function of the input. Part (a) of the ﬁgure shows a uniform midtread quantizer and
part (b) shows how the error behaves as a periodic sawtooth function, rising from −0.5
to 0.5, then dropping back to −0.5.
Nonuniform quantization has to be used in applications where the input is dis-
tributed nonuniformly. Table 1.18 (after [Lloyd 82], [Max 60], and [Paez and Glisson 72])
lists input and quantized values for optimal symmetric quantizers for uniform, Gaus-
sian, and Laplace distributions with zero mean and unit variance and for 2, 4, 8, and 16
subintervals of the input.
Scalar quantization produces lossy compression, but makes it is easy to control
the trade-oﬀbetween compression performance and the amount of data loss. However,
because it is so simple, applications of scalar quantization are limited to cases where
much loss can be tolerated.
Many image compression methods are lossy, but scalar
quantization is not suitable for image compression because it creates annoying artifacts
in the decompressed image. Imagine an image with an almost uniform area where all
pixels have values 127 or 128. If 127 is quantized to 111 and 128 is quantized to 144,
then the result, after decompression, may resemble a checkerboard where adjacent pixels

1.5 Quantization
55
input
(a)
(b)
output
0.5
−0.5
−1.5
1.5
1
−1
−2
2
3
4
2.5
3.5
0.5
0.5
−0.5
−0.5
−1.5
1.5
2.5
3.5
Figure 1.17: Midtread Quantization Error.
alternate between 111 and 144. This is why practical algorithms use vector quantization,
instead of scalar quantization, for lossy (and sometimes lossless) compression of images
and sound.
1.5.2 Vector Quantization
Vector quantization is based on the fact that adjacent data symbols in image and audio
ﬁles are correlated. The principle is simple and is stated in the following steps:
Select a parameter N that will be the size of the vectors and work with groups
(called vectors) of N adjacent data symbols (pixels or audio samples).
Prepare a set (referred to as a codebook) of vectors Vj. Determining the best vectors
for the codebook is the central problem of vector quantization.
Scan the input data vector by vector and compress each vector vi by ﬁnding the
codebook vector Vj that is “nearest” vi. The index j is then written on the output.
The left half of Figure 1.19 is a simple example.
The original data is a 4 × 12
image and the codebook consists of ﬁve vectors, each a smaller 4 × 4 image. The arrow
indicates that vector 3 is the best match for the center-bottom 4 × 4 part of the image.
Thus, that part is encoded as the single number 3 (in practice, a variable-length code
for 3 may be written on the output).
It is obvious that the decoder is very simple. It inputs numbers from the compressed
ﬁle and interprets each number as an index to the codebook. The corresponding vector
is read from the codebook and is appended to the data that’s being decompressed. The
encoder, on the other hand, is more complex (vector quantization is therefore a highly
asymmetric compression method). For each part of the original data, the encoder has
to search the entire codebook and determine the best match. There is also the question
of selecting the best vectors and constructing the codebook in the ﬁrst place.
We can gain a deeper understanding of vector quantization by thinking of it as a
partitioning of space. The right half of Figure 1.19 is a two-dimensional (approximate)
example of space partitioning. Given a number of points in space (nine points in the
ﬁgure), the space is partitioned into the same number of nonoverlapping regions with
one of the given points at the center of each region. The regions are selected such that

56
1.
Approaches to Compression
Uniform
Gaussian
Laplacian
−1.000
−0.500
−∞
−0.799
−∞
−0.707
2
0.000
0.500
0.000
0.799
0.000
0.707
1.000
∞
∞
−1.000
−0.750
−∞
−1.510
−∞
−1.834
−0.500
−0.250
−0.982
−0.453
−1.127
−0.420
4
0.000
0.250
0.000
0.453
0.000
0.420
0.500
0.750
0.982
1.510
1.127
1.834
1.000
∞
∞
−1.000
−0.875
−∞
−2.152
−∞
−3.087
−0.750
−0.625
−1.748
−1.344
−2.377
−1.673
−0.500
−0.375
−1.050
−0.756
−1.253
−0.833
−0.250
−0.125
−0.501
−0.245
−0.533
−0.233
8
0.000
0.125
0.000
0.245
0.000
0.233
0.250
0.375
0.501
0.756
0.533
0.833
0.500
0.625
1.050
1.344
1.253
1.673
0.750
0.875
1.748
2.152
2.377
3.087
1.000
∞
∞
−1.000
−0.938
−∞
−2.733
−∞
−4.316
−0.875
−0.813
−2.401
−2.069
−3.605
−2.895
−0.750
−0.688
−1.844
−1.618
−2.499
−2.103
−0.625
−0.563
−1.437
−1.256
−1.821
−1.540
−0.500
−0.438
−1.099
−0.942
−1.317
−1.095
−0.375
−0.313
−0.800
−0.657
−0.910
−0.726
−0.250
−0.188
−0.522
−0.388
−0.566
−0.407
−0.125
−0.063
−0.258
−0.128
−0.266
−0.126
16
0.000
0.063
0.000
0.128
0.000
0.126
0.125
0.188
0.258
0.388
0.266
0.407
0.250
0.313
0.522
0.657
0.566
0.726
0.375
0.438
0.800
0.942
0.910
1.095
0.500
0.563
1.099
1.256
1.317
1.540
0.625
0.688
1.437
1.618
1.821
2.103
0.750
0.813
1.844
2.069
2.499
2.895
0.875
0.938
2.401
2.733
3.605
4.316
1.000
∞
∞
Table 1.18: Uniform and Nonuniform Quantizations.

1.5 Quantization
57
Image
0
1
2
3
4
Codebook
Best match
Two-Dimensional Regions
Figure 1.19: Vector Quantization Example.
all the points in a region are closer to the region’s center point than to any other center
point. Every point in the region is then transformed or projected to the center point.
Voronoi Regions
Imagine a Petri dish ready for growing bacteria. Four bacteria of diﬀerent types
are simultaneously placed in it at diﬀerent points and immediately start multiplying.
We assume that their colonies grow at the same rate. Initially, each colony consists
of a growing circle around one of the starting points. After a while, the circles meet
and stop growing in the meeting area due to lack of food. The ﬁnal result is that the
entire dish is divided into four areas, one around each of the four starting points, such
that all the points within area i are closer to starting point i than to any other start
point. Such areas are called Voronoi regions or Dirichlet Tessellations.
In practice, each vector has N components, so vector quantization becomes the
problem of partitioning n-dimensional space into regions and determining a center point
for each region. Once this is done, quantization amounts to replacing all the points in a
region with the center point.
How is the codebook constructed? This problem is somewhat similar to the problem
of determining the distribution of data symbols in a given ﬁle (page 26) and can be
approached in three ways as follows:
A two-pass job, where the ﬁrst pass analyzes the data to be compressed and con-
structs the best codebook for that data and the second pass performs the actual com-
pression. In addition to being slow, this approach requires writing the entire codebook
on the compressed ﬁle.
A static codebook that is determined once and for all by means of a set of training
documents. The codebook is built into both encoder and decoder, so it doesn’t have to
be written on the compressed ﬁle. Obviously, the performance of this approach depends
on how much the data resembles the training documents.

58
1.
Approaches to Compression
An adaptive algorithm, where the codebook starts empty and is modiﬁed each time
new data is input and compressed. Such a method has to be designed carefully to make
sure that the decoder can modify the codebook in lockstep with the encoder.
The ﬁrst two approaches are similar. Both require an algorithm that constructs the
best codebook for a given data ﬁle (either the data to be compressed or the training doc-
uments). An example of such a method is the Linde, Buzo, and Gray (LBG) algorithm
[Linde et al. 80]. This algorithm, as well as a method for adaptive vector quantization,
are described in [Salomon 07].
Chapter Summary
This chapter introduces the important approaches to data compression. The Prelude
discusses variable-length codes, run-length encoding (RLE), the use of dictionaries, the
concept of a transform, and the techniques of scalar and vector quantization. Following
the Prelude, each approach is described in more detail in a separate section. Here is a
short summary of the main concepts involved.
Variable-length codes are a natural choice for simple, eﬀective compression of various
types of data. Most types of data (such as text characters, pixels, and audio samples)
are encoded with a ﬁxed-length code because this makes it convenient to input, process,
and save the individual data symbols. Replacing ﬁxed-length codes with variable-length
codes can lead to compression because of the following: (1) Often, certain symbols are
more common than others, so replacing them with short codes can greatly save on the
total number of bits needed. (2) Data symbols such as pixels and audio samples are
correlated. Subtracting adjacent symbols results in diﬀerences (or residuals), most of
which are small integers and thus can be replaced by short codes.
Run-length encoding is used to compress strings of identical symbols. In principle,
a string of n occurrences of symbol S can be replaced by a repetition factor n followed
by a single S. RLE is especially useful in combination with quantization, because the
latter may often quantize many symbols to zero and thus generate long strings of zeros.
Dictionary-based compression methods exploit the fact that a typical data ﬁle is
not random; it features patterns and repetitions. At any point during compression, the
input ﬁle is divided into two parts, data that has been compressed (this is kept in a
data structure called the dictionary) and data that still has to be input and compressed.
Assume that the latter part starts with the string of symbols abcd. . . . The encoder
searches the dictionary for this string, and locates the longest match.
The string is
then compressed by replacing it with a pointer to its match in the dictionary.
The
diﬀerence between the various dictionary methods is in how they organize and search
the dictionary and how they deal with strings not found in the dictionary.
A transform is a mathematical operation that changes the representation of a data
item. Thus, changing the decimal number 12,345 to the binary 11000000111001 is a
transform. Correlated data symbols such as the pixels of an image or the audio samples
of a sound ﬁle, can be transformed to representations where they require fewer bits.
This sounds like getting something for nothing, but in fact there is a price to pay. The
transformed items (transform coeﬃcients) are decorrelated. Such a transform already
achieves some degree of compression, but more can be obtained if lossy compression is

Chapter Summary
59
an option. The transform coeﬃcients can be quantized, a process that results in small
integers (which can be encoded with variable-length codes) and possibly also in runs of
zeros (which can be compressed with RLE).
Quantization is the operation of cutting up a number. A real number, for example,
can be quantized by converting it to the nearest integer (or the nearest smaller integer).
Heavier quantization may convert all the numbers in an interval [a, b] to the integer at
the center of the interval. For example, given an input ﬁle where the data symbols are
8-bit integers (bytes) we can compress each symbol to four bits as follows. A byte is
an integer in the interval [0, 255]. We divide this interval to 16 subintervals of width 16
each, and quantize each integer in a subinterval to the integer in the middle (or closest
to the middle) of the interval. This is referred to as scalar quantization and is very
ineﬃcient. The compression factor is only 2, and the quantization simply discards half
the original bits of each data symbol, which may be too much data to lose.
A more eﬃcient form of quantization is the so-called vector quantization, where
an array of data symbols is replaced by the index of another array. The replacement
arrays are the components of a codebook of arrays and are selected such that (1) for
each data array vi there is a replacement array Vj in the codebook such that vi and Vj
are suﬃciently close and (2) the codebook size (the number of replacement arrays) is
small enough, such that replacing vi by the index j result in signiﬁcant savings of bits.
Self-Assessment Questions
1. Most variable-length codes used in practical compression algorithms are preﬁx
codes, but there are other ways to design UD codes. Consider the following idea for
a “taboo” code. The user selects a positive integer n and decides on an n-bit taboo
pattern. Each codeword is constructed as a string of n-bit blocks where the last block
is the taboo pattern and no other block can have this pattern. Select n = 3 and a 3-bit
taboo pattern, and then write several taboo codewords with 3, 4, and 5 blocks. Find
out how the number of possible values of a b-block codeword depends on b.
2. Section 1.2 mentions mixing run lengths and raw items. Here is the relevant
paragraph:
“If the run is short, such a pair may be longer than the run of symbols, thereby
leading to expansion. A reasonable solution is to write such short runs on the output in
raw format, so at least they do not cause expansion. However, this raises the question
of distinguishing between pairs and raw items, because in the output ﬁle both types are
binary strings. A practical RLE program must therefore precede each pair and each raw
item with a 1-bit indicator. Thus, a pair becomes the triplet (0, length, symbol) and a
raw item becomes the pair (1, symbol).”
Prepare a data ﬁle with many run lengths and write a program that identiﬁes each
run and compresses it either as a triplet (0, length, symbol) or as a pair (1, symbol)
depending on its length (if the run is long enough to beneﬁt from RLE, it should be
converted into a triplet).
3. The last paragraph of Section 1.3.1 mentions several ways to improve the basic
LZ77 method. One such technique has to do with a circular queue. Study this interesting
data structure in books on data structures and implement a simple version of a circular
queue.

60
1.
Approaches to Compression
4. The matrix of Equation (5.1) is a rotation matrix in two dimensions. Use books
on geometric transformations to understand rotations in higher dimensions.
5. Prepare an example of vector quantization similar to that of Figure 1.19.
The best angle from which to approach any problem is the try-angle.
—Unknown

2
Huﬀman Coding
Huﬀman coding is a popular method for compressing data with variable-length codes.
Given a set of data symbols (an alphabet) and their frequencies of occurrence (or, equiv-
alently, their probabilities), the method constructs a set of variable-length codewords
with the shortest average length and assigns them to the symbols.
Huﬀman coding
serves as the basis for several applications implemented on popular platforms. Some
programs use just the Huﬀman method, while others use it as one step in a multistep
compression process. The Huﬀman method [Huﬀman 52] is somewhat similar to the
Shannon–Fano method, proposed independently by Claude Shannon and Robert Fano
in the late 1940s ([Shannon 48] and [Fano 49]). It generally produces better codes, and
like the Shannon–Fano method, it produces the best variable-length codes when the
probabilities of the symbols are negative powers of 2. The main diﬀerence between the
two methods is that Shannon–Fano constructs its codes from top to bottom (and the
bits of each codeword are constructed from left to right), while Huﬀman constructs a
code tree from the bottom up (and the bits of each codeword are constructed from right
to left).
Since its inception in 1952 by D. Huﬀman, the method has been the subject of
intensive research in data compression. The long discussion in [Gilbert and Moore 59]
proves that the Huﬀman code is a minimum-length code in the sense that no other
encoding has a shorter average length.
A much shorter proof of the same fact was
discovered by Huﬀman himself [Motil 07]. An algebraic approach to constructing the
Huﬀman code is introduced in [Karp 61]. In [Gallager 78], Robert Gallager shows that
the redundancy of Huﬀman coding is at most p1 + 0.086 where p1 is the probability of
the most-common symbol in the alphabet. The redundancy is the diﬀerence between
the average Huﬀman codeword length and the entropy. Given a large alphabet, such

62
2.
Huﬀman Coding
as the set of letters, digits and punctuation marks used by a natural language, the
largest symbol probability is typically around 15–20%, bringing the value of the quantity
p1 + 0.086 to around 0.1. This means that Huﬀman codes are at most 0.1 bit longer
(per symbol) than an ideal entropy encoder, such as arithmetic coding (Chapter 4).
This chapter describes the details of Huﬀman encoding and decoding and covers
related topics such as the height of a Huﬀman code tree, canonical Huﬀman codes, and
an adaptive Huﬀman algorithm. Following this, Section 2.4 illustrates an important
application of the Huﬀman method to facsimile compression.
David Huﬀman (1925–1999)
Being originally from Ohio, it is no wonder that Huﬀman went to Ohio State Uni-
versity for his BS (in electrical engineering). What is unusual was
his age (18) when he earned it in 1944. After serving in the United
States Navy, he went back to Ohio State for an MS degree (1949)
and then to MIT, for a PhD (1953, electrical engineering).
That same year, Huﬀman joined the faculty at MIT. In 1967,
he made his only career move when he went to the University of
California, Santa Cruz as the founding faculty member of the Com-
puter Science Department. During his long tenure at UCSC, Huﬀ-
man played a major role in the development of the department (he
served as chair from 1970 to 1973) and he is known for his motto
“my products are my students.” Even after his retirement, in 1994, he remained active
in the department, teaching information theory and signal analysis courses.
Huﬀman developed his celebrated algorithm as a term paper that he wrote in lieu
of taking a ﬁnal examination in an information theory class he took at MIT in 1951.
The professor, Robert Fano, proposed the problem of constructing the shortest variable-
length code for a set of symbols with known probabilities of occurrence.
It should be noted that in the late 1940s, Fano himself (and independently, also
Claude Shannon) had developed a similar, but suboptimal, algorithm known today as
the Shannon–Fano method ([Shannon 48] and [Fano 49]). The diﬀerence between the
two algorithms is that the Shannon–Fano code tree is built from the top down, while
the Huﬀman code tree is constructed from the bottom up.
Huﬀman made signiﬁcant contributions in several areas, mostly information theory
and coding, signal designs for radar and communications, and design procedures for
asynchronous logical circuits. Of special interest is the well-known Huﬀman algorithm
for constructing a set of optimal preﬁx codes for data with known frequencies of occur-
rence. At a certain point he became interested in the mathematical properties of “zero
curvature” surfaces, and developed this interest into techniques for folding paper into
unusual sculptured shapes (the so-called computational origami).

2.1 Huﬀman Encoding
63
2.1 Huﬀman Encoding
The Huﬀman encoding algorithm starts by constructing a list of all the alphabet symbols
in descending order of their probabilities. It then constructs, from the bottom up, a
binary tree with a symbol at every leaf. This is done in steps, where at each step two
symbols with the smallest probabilities are selected, added to the top of the partial tree,
deleted from the list, and replaced with an auxiliary symbol representing the two original
symbols. When the list is reduced to just one auxiliary symbol (representing the entire
alphabet), the tree is complete. The tree is then traversed to determine the codewords
of the symbols.
This process is best illustrated by an example. Given ﬁve symbols with probabilities
as shown in Figure 2.1a, they are paired in the following order:
1. a4 is combined with a5 and both are replaced by the combined symbol a45, whose
probability is 0.2.
2. There are now four symbols left, a1, with probability 0.4, and a2, a3, and a45, with
probabilities 0.2 each. We arbitrarily select a3 and a45 as the two symbols with smallest
probabilities, combine them, and replace them with the auxiliary symbol a345, whose
probability is 0.4.
3. Three symbols are now left, a1, a2, and a345, with probabilities 0.4, 0.2, and 0.4,
respectively. We arbitrarily select a2 and a345, combine them, and replace them with
the auxiliary symbol a2345, whose probability is 0.6.
4. Finally, we combine the two remaining symbols, a1 and a2345, and replace them with
a12345 with probability 1.
The tree is now complete. It is shown in Figure 2.1a “lying on its side” with its
root on the right and its ﬁve leaves on the left. To assign the codewords, we arbitrarily
assign a bit of 1 to the top edge, and a bit of 0 to the bottom edge, of every pair of
edges. This results in the codewords 0, 10, 111, 1101, and 1100. The assignments of bits
to the edges is arbitrary.
The average size of this code is 0.4 × 1 + 0.2 × 2 + 0.2 × 3 + 0.1 × 4 + 0.1 × 4 = 2.2
bits/symbol, but even more importantly, the Huﬀman code is not unique. Some of the
steps above were chosen arbitrarily, because there were more than two symbols with
smallest probabilities. Figure 2.1b shows how the same ﬁve symbols can be combined
diﬀerently to obtain a diﬀerent Huﬀman code (11, 01, 00, 101, and 100). The average
size of this code is 0.4 × 2 + 0.2 × 2 + 0.2 × 2 + 0.1 × 3 + 0.1 × 3 = 2.2 bits/symbol, the
same as the previous code.
⋄Exercise 2.1: Given the eight symbols A, B, C, D, E, F, G, and H with probabilities
1/30, 1/30, 1/30, 2/30, 3/30, 5/30, 5/30, and 12/30, draw three diﬀerent Huﬀman trees
with heights 5 and 6 for these symbols and compute the average code size for each tree.
⋄Exercise 2.2: Figure Ans.1d shows another Huﬀman tree, with height 4, for the eight
symbols introduced in Exercise 2.1. Explain why this tree is wrong.
It turns out that the arbitrary decisions made in constructing the Huﬀman tree
aﬀect the individual codes but not the average size of the code. Still, we have to answer
the obvious question, which of the diﬀerent Huﬀman codes for a given set of symbols
is best? The answer, while not obvious, is simple: The best code is the one with the

64
2.
Huﬀman Coding
0.4
0.1
0.2
0.2
0.1
0.4
0.1
0.2
0.2
0.1
(a)
(b)
a3
a345
a4
a45
a5
a2
a2345
a12345
a1
a3
a4
a5
a2
a23
a45
a1
a145
0.2
0.4
0
0
0
0
0
0
0
0
1
1
1
1
0.2
0.4
0.6
1
1
1
1
1.0
0.6
1.0
Figure 2.1: Huﬀman Codes.
smallest variance. The variance of a code measures how much the sizes of the individual
codewords deviate from the average size. The variance of the code of Figure 2.1a is
0.4(1 −2.2)2 + 0.2(2 −2.2)2 + 0.2(3 −2.2)2 + 0.1(4 −2.2)2 + 0.1(4 −2.2)2 = 1.36,
while the variance of code 2.1b is
0.4(2 −2.2)2 + 0.2(2 −2.2)2 + 0.2(2 −2.2)2 + 0.1(3 −2.2)2 + 0.1(3 −2.2)2 = 0.16.
Code 2.1b is therefore preferable (see below). A careful look at the two trees shows how
to select the one we want. In the tree of Figure 2.1a, symbol a45 is combined with a3,
whereas in the tree of 2.1b a45 is combined with a1. The rule is: When there are more
than two smallest-probability nodes, select the ones that are lowest and highest in the
tree and combine them. This will combine symbols of low probability with symbols of
high probability, thereby reducing the total variance of the code.
If the encoder simply writes the compressed data on a ﬁle, the variance of the code
makes no diﬀerence. A small-variance Huﬀman code is preferable only in cases where
the encoder transmits the compressed data, as it is being generated, over a network. In
such a case, a code with large variance causes the encoder to generate bits at a rate that
varies all the time. Since the bits have to be transmitted at a constant rate, the encoder
has to use a buﬀer. Bits of the compressed data are entered into the buﬀer as they are
being generated and are moved out of it at a constant rate, to be transmitted. It is easy
to see intuitively that a Huﬀman code with zero variance will enter bits into the buﬀer
at a constant rate, so only a short buﬀer will be needed. The larger the code variance,
the more variable is the rate at which bits enter the buﬀer, requiring the encoder to use
a larger buﬀer.
The following claim is sometimes found in the literature:
It can be shown that the size of the Huﬀman code of a symbol
ai with probability Pi is always less than or equal to ⌈−log2 Pi⌉.

2.1 Huﬀman Encoding
65
Even though it is correct in many cases, this claim is not true in general. It seems
to be a wrong corollary drawn by some authors from the Kraft–McMillan inequality,
Equation (1.4). The author is indebted to Guy Blelloch for pointing this out and also
for the example of Table 2.2.
Pi
Code
−log2 Pi
⌈−log2 Pi⌉
.01
000
6.644
7
*.30
001
1.737
2
.34
01
1.556
2
.35
1
1.515
2
Table 2.2: A Huﬀman Code Example.
⋄Exercise 2.3: Find an example where the size of the Huﬀman code of a symbol ai is
greater than ⌈−log2 Pi⌉.
⋄Exercise 2.4: It seems that the size of a code must also depend on the number n of
symbols (the size of the alphabet). A small alphabet requires just a few codes, so they
can all be short; a large alphabet requires many codes, so some must be long. This being
so, how can we say that the size of the code of ai depends just on the probability Pi?
Figure 2.3 shows a Huﬀman code for the 26 letters.
As a self-exercise, the reader may calculate the average size, entropy, and variance
of this code.
⋄Exercise 2.5: Discuss the Huﬀman codes for equal probabilities.
Exercise 2.5 shows that symbols with equal probabilities don’t compress under the
Huﬀman method. This is understandable, since strings of such symbols normally make
random text, and random text does not compress. There may be special cases where
strings of symbols with equal probabilities are not random and can be compressed. A
good example is the string a1a1 . . . a1a2a2 . . . a2a3a3 . . . in which each symbol appears
in a long run. This string can be compressed with RLE but not with Huﬀman codes.
Notice that the Huﬀman method cannot be applied to a two-symbol alphabet. In
such an alphabet, one symbol can be assigned the code 0 and the other code 1. The
Huﬀman method cannot assign to any symbol a code shorter than one bit, so it cannot
improve on this simple code.
If the original data (the source) consists of individual
bits, such as in the case of a bi-level (monochromatic) image, it is possible to combine
several bits (perhaps four or eight) into a new symbol and pretend that the alphabet
consists of these (16 or 256) symbols. The problem with this approach is that the original
binary data may have certain statistical correlations between the bits, and some of these
correlations would be lost when the bits are combined into symbols. When a typical
bi-level image (a painting or a diagram) is digitized by scan lines, a pixel is more likely to
be followed by an identical pixel than by the opposite one. We therefore have a ﬁle that
can start with either a 0 or a 1 (each has 0.5 probability of being the ﬁrst bit). A zero is
more likely to be followed by another 0 and a 1 by another 1. Figure 2.4 is a ﬁnite-state
machine illustrating this situation. If these bits are combined into, say, groups of eight,

66
2.
Huﬀman Coding
    000 E .1300
   0010 T .0900
   0011 A .0800
   0100 O .0800
0101 N .0700
0110 R .0650
0111 I .0650
10000 H .0600
10001 S .0600
10010 D .0400
10011 L .0350
10100 C .0300
10101 U .0300
10110 M .0300
10111 F .0200
11000 P .0200
11001 Y .0200
11010 B .0150
11011 W .0150
11100 G .0150
11101 V .0100
111100 J .0050
111101 K .0050
111110 X .0050
1111110 Q .0025
1111111 Z .0025
.005
.11
.010
.010
.020
.025
.045
.070
.115
.305
.420
.580
.30
.28
.195
1.0
1
1
0
0
1
0
1
0
0
1
0
1
Figure 2.3: A Huﬀman Code for the 26-Letter Alphabet.
the bits inside a group will still be correlated, but the groups themselves will not be
correlated by the original pixel probabilities. If the input data contains, e.g., the two
adjacent groups 00011100 and 00001110, they will be encoded independently, ignoring
the correlation between the last 0 of the ﬁrst group and the ﬁrst 0 of the next group.
Selecting larger groups improves this situation but increases the number of groups, which
implies more storage for the code table and longer time to calculate the table.
⋄Exercise 2.6: How does the number of groups increase when the group size increases
from s bits to s + n bits?
A more complex approach to image compression by Huﬀman coding is to create
several complete sets of Huﬀman codes. If the group size is, e.g., eight bits, then several
sets of 256 codes are generated. When a symbol S is to be encoded, one of the sets is
selected, and S is encoded using its code in that set. The choice of set depends on the
symbol preceding S.

2.2 Huﬀman Decoding
67
0
1
s
0,50%
1,50%
0,40%
1,60%
1,33%
0,67%
Start
Figure 2.4: A Finite-State Machine.
⋄Exercise 2.7: Imagine an image with 8-bit pixels where half the pixels have values 127
and the other half have values 128. Analyze the performance of RLE on the individual
bitplanes of such an image, and compare it with what can be achieved with Huﬀman
coding.
Which two integers come next in the inﬁnite sequence 38, 24, 62, 12, 74, . . . ?
2.2 Huﬀman Decoding
Before starting the compression of a data ﬁle, the compressor (encoder) has to determine
the codes. It does that based on the probabilities (or frequencies of occurrence) of the
symbols. The probabilities or frequencies have to be written, as side information, on
the output, so that any Huﬀman decompressor (decoder) will be able to decompress
the data. This is easy, because the frequencies are integers and the probabilities can
be written as scaled integers. It normally adds just a few hundred bytes to the output.
It is also possible to write the variable-length codes themselves on the output, but this
may be awkward, because the codes have diﬀerent sizes. It is also possible to write the
Huﬀman tree on the output, but this may require more space than just the frequencies.
In any case, the decoder must know what is at the start of the compressed ﬁle,
read it, and construct the Huﬀman tree for the alphabet. Only then can it read and
decode the rest of its input. The algorithm for decoding is simple. Start at the root
and read the ﬁrst bit oﬀthe input (the compressed ﬁle). If it is zero, follow the bottom
edge of the tree; if it is one, follow the top edge. Read the next bit and move another
edge toward the leaves of the tree. When the decoder arrives at a leaf, it ﬁnds there the
original, uncompressed symbol (normally its ASCII code), and that code is emitted by
the decoder. The process starts again at the root with the next bit.
This process is illustrated for the ﬁve-symbol alphabet of Figure 2.5. The four-
symbol input string a4a2a5a1 is encoded into 1001100111. The decoder starts at the
root, reads the ﬁrst bit 1, and goes up. The second bit 0 sends it down, as does the
third bit. This brings the decoder to leaf a4, which it emits. It again returns to the
root, reads 110, moves up, up, and down, to reach leaf a2, and so on.

68
2.
Huﬀman Coding
1
2
3
4
5
1
1
0
0
Figure 2.5: Huﬀman Codes for Equal Probabilities.
2.2.1 Fast Huﬀman Decoding
Decoding a Huﬀman-compressed ﬁle by sliding down the code tree for each symbol is
conceptually simple, but slow. The compressed ﬁle has to be read bit by bit and the
decoder has to advance a node in the code tree for each bit. The method of this section,
originally conceived by [Choueka et al. 85] but later reinvented by others, uses preset
partial-decoding tables. These tables depend on the particular Huﬀman code used, but
not on the data to be decoded. The compressed ﬁle is read in chunks of k bits each
(where k is normally 8 or 16 but can have other values) and the current chunk is used
as a pointer to a table. The table entry that is selected in this way can decode several
symbols and it also points the decoder to the table to be used for the next chunk.
As an example, consider the Huﬀman code of Figure 2.1a, where the ﬁve codewords
are 0, 10, 111, 1101, and 1100. The string of symbols a1a1a2a4a3a1a5 . . . is compressed
by this code to the string 0|0|10|1101|111|0|1100 . . .. We select k = 3 and read this string
in 3-bit chunks 001|011|011|110|110|0 . . .. Examining the ﬁrst chunk, it is easy to see
that it should be decoded into a1a1 followed by the single bit 1 which is the preﬁx of
another codeword. The ﬁrst chunk is 001 = 110, so we set entry 1 of the ﬁrst table (table
0) to the pair (a1a1, 1). When chunk 001 is used as a pointer to table 0, it points to entry
1, which immediately provides the decoder with the two decoded symbols a1a1 and also
directs it to use table 1 for the next chunk. Table 1 is used when a partially-decoded
chunk ends with the single-bit preﬁx 1. The next chunk is 011 = 310, so entry 3 of
table 1 corresponds to the encoded bits 1|011. Again, it is easy to see that these should
be decoded to a2 and there is the preﬁx 11 left over. Thus, entry 3 of table 1 should be
(a2, 2). It provides the decoder with the single symbol a2 and also directs it to use table 2
next (the table that corresponds to preﬁx 11). The next chunk is again 011 = 310, so
entry 3 of table 2 corresponds to the encoded bits 11|011. It is again obvious that these
should be decoded to a4 with a preﬁx of 1 left over. This process continues until the
end of the encoded input. Figure 2.6 is the simple decoding algorithm in pseudocode.
Table 2.7 lists the four tables required to decode this code. It is easy to see that
they correspond to the preﬁxes Λ (null), 1, 11, and 110. A quick glance at Figure 2.1a
shows that these correspond to the root and the four interior nodes of the Huﬀman code
tree. Thus, each partial-decoding table corresponds to one of the four preﬁxes of this
code. The number m of partial-decoding tables therefore equals the number of interior
nodes (plus the root) which is one less than the number N of symbols of the alphabet.

2.2 Huﬀman Decoding
69
i←0; output←null;
repeat
j←input next chunk;
(s,i)←Tablei[j];
append s to output;
until end-of-input
Figure 2.6: Fast Huﬀman Decoding.
T0 = Λ
T1 = 1
T2 = 11
T3 = 110
000 a1a1a1 0
1|000
a2a1a1 0
11|000
a5a1
0
110|000
a5a1a1 0
001 a1a1
1
1|001
a2a1
1
11|001
a5
1
110|001
a5a1
1
010 a1a2
0
1|010
a2a2
0
11|010
a4a1
0
110|010
a5a2
0
011 a1
2
1|011
a2
2
11|011
a4
1
110|011
a5
2
100 a2a1
0
1|100
a5
0
11|100
a3a1a1 0
110|100
a4a1a1 0
101 a2
1
1|101
a4
0
11|101
a3a1
1
110|101
a4a1
1
110 −
3
1|110
a3a1
0
11|110
a3a2
0
110|110
a4a2
0
111 a3
0
1|111
a3
1
11|111
a3
2
110|111
a4
2
Table 2.7: Partial-Decoding Tables for a Huﬀman Code.
Notice that some chunks (such as entry 110 of table 0) simply send the decoder
to another table and do not provide any decoded symbols. Also, there is a trade-oﬀ
between chunk size (and thus table size) and decoding speed. Large chunks speed up
decoding, but require large tables. A large alphabet (such as the 128 ASCII characters
or the 256 8-bit bytes) also requires a large set of tables. The problem with large tables
is that the decoder has to set up the tables after it has read the Huﬀman codes from the
compressed stream and before decoding can start, and this process may preempt any
gains in decoding speed provided by the tables.
To set up the ﬁrst table (table 0, which corresponds to the null preﬁx Λ), the
decoder generates the 2k bit patterns 0 through 2k −1 (the ﬁrst column of Table 2.7)
and employs the decoding method of Section 2.2 to decode each pattern. This yields
the second column of Table 2.7. Any remainders left are preﬁxes and are converted
by the decoder to table numbers. They become the third column of the table. If no
remainder is left, the third column is set to 0 (use table 0 for the next chunk). Each of
the other partial-decoding tables is set in a similar way. Once the decoder decides that
table 1 corresponds to preﬁx p, it generates the 2k patterns p|00 . . . 0 through p|11 . . . 1
that become the ﬁrst column of that table. It then decodes that column to generate the
remaining two columns.
This method was conceived in 1985, when storage costs were considerably higher
than today (early 2007). This prompted the developers of the method to ﬁnd ways to
cut down the number of partial-decoding tables, but these techniques are less important
today and are not described here.

70
2.
Huﬀman Coding
2.2.2 Average Code Size
Figure 2.8a shows a set of ﬁve symbols with their probabilities and a typical Huﬀman
tree. Symbol A appears 55% of the time and is assigned a 1-bit code, so it contributes
0.55·1 bits to the average code size. Symbol E appears only 2% of the time and is
assigned a 4-bit Huﬀman code, so it contributes 0.02·4 = 0.08 bits to the code size. The
average code size is therefore easily computed as
0.55 · 1 + 0.25 · 2 + 0.15 · 3 + 0.03 · 4 + 0.02 · 4 = 1.7 bits per symbol.
Surprisingly, the same result is obtained by adding the values of the four internal nodes
of the Huﬀman code tree 0.05 + 0.2 + 0.45 + 1 = 1.7. This provides a way to calculate
the average code size of a set of Huﬀman codes without any multiplications. Simply add
the values of all the internal nodes of the tree. Table 2.9 illustrates why this works.
A 0.55
B 0.25
C 0.15
D 0.03
E 0.02
0.05
0.2
0.45
1
0.02
0.03
0.05
1
d
(b)
(a)
ad−2
a1
Figure 2.8: Huﬀman Code Trees.
(Internal nodes are shown in italics in this table.) The left column consists of the values
of all the internal nodes. The right columns show how each internal node is the sum of

2.2 Huﬀman Decoding
71
.05 =
.02+ .03
.20 = .05+ .15 = .02+ .03+ .15
.45 = .20+ .25 = .02+ .03+ .15+ .25
1.0 = .45+ .55 = .02+ .03+ .15+ .25+ .55
Table 2.9: Composition of Nodes.
0.05 =
= 0.02 + 0.03 + · · ·
a1
= 0.05 + . . .= 0.02 + 0.03 + · · ·
a2
= a1
+ . . .= 0.02 + 0.03 + · · ·
...
=
ad−2 = ad−3 + . . .= 0.02 + 0.03 + · · ·
1.0
= ad−2 + . . .= 0.02 + 0.03 + · · ·
Table 2.10: Composition of Nodes.
some of the leaf nodes. Summing the values in the left column yields 1.7, and summing
the other columns shows that this 1.7 is the sum of the four values 0.02, the four values
0.03, the three values 0.15, the two values 0.25, and the single value 0.55.
This argument can be extended to the general case. It is easy to show that, in a
Huﬀman-like tree (a tree where each node is the sum of its children), the weighted sum
of the leaves, where the weights are the distances of the leaves from the root, equals
the sum of the internal nodes. (This property has been communicated to the author by
J. Motil.)
Figure 2.8b shows such a tree, where we assume that the two leaves 0.02 and 0.03
have d-bit Huﬀman codes. Inside the tree, these leaves become the children of internal
node 0.05, which, in turn, is connected to the root by means of the d −2 internal nodes
a1 through ad−2. Table 2.10 has d rows and shows that the two values 0.02 and 0.03
are included in the various internal nodes exactly d times. Adding the values of all the
internal nodes produces a sum that includes the contributions 0.02 · d + 0.03 · d from
the two leaves. Since these leaves are arbitrary, it is clear that this sum includes similar
contributions from all the other leaves, so this sum is the average code size. Since this
sum also equals the sum of the left column, which is the sum of the internal nodes, it is
clear that the sum of the internal nodes equals the average code size.
Notice that this proof does not assume that the tree is binary. The property illus-
trated here exists for any tree where a node contains the sum of its children.
2.2.3 Number of Codes
Since the Huﬀman code is not unique, the natural question is: How many diﬀerent codes
are there? Figure 2.11a shows a Huﬀman code tree for six symbols, from which we can
answer this question in two diﬀerent ways as follows:
Answer 1. The tree of 2.11a has ﬁve interior nodes, and in general, a Huﬀman code
tree for n symbols has n −1 interior nodes. Each interior node has two edges coming
out of it, labeled 0 and 1. Swapping the two labels produces a diﬀerent Huﬀman code
tree, so the total number of diﬀerent Huﬀman code trees is 2n−1 (in our example, 25 or
32). The tree of Figure 2.11b, for example, shows the result of swapping the labels of
the two edges of the root. Table 2.12a,b lists the codes generated by the two trees.
Answer 2. The six codes of Table 2.12a can be divided into the four classes 00x,
10y, 01, and 11, where x and y are 1-bit each. It is possible to create diﬀerent Huﬀman
codes by changing the ﬁrst two bits of each class. Since there are four classes, this is
the same as creating all the permutations of four objects, something that can be done
in 4! = 24 ways. In each of the 24 permutations it is also possible to change the values

72
2.
Huﬀman Coding
1
2
3
4
5
6
.11
.12
.13
.14
.24
.26
.11
.12
.13
.14
.24
.26
0
1
0
0
0
0
1
1
1
1
1
2
3
4
5
6
0
1
0
0
0
0
1
1
1
1
(a)
(b)
000 100 000
001 101 001
100 000 010
101 001 011
01
11
10
11
01
11
(a) (b)
(c)
Figure 2.11: Two Huﬀman Code Trees.
Table 2.12.
of x and y in four diﬀerent ways (since they are bits) so the total number of diﬀerent
Huﬀman codes in our six-symbol example is 24 × 4 = 96.
The two answers are diﬀerent because they count diﬀerent things. Answer 1 counts
the number of diﬀerent Huﬀman code trees, while answer 2 counts the number of diﬀerent
Huﬀman codes. It turns out that our example can generate 32 diﬀerent code trees but
only 94 diﬀerent codes instead of 96. This shows that there are Huﬀman codes that
cannot be generated by the Huﬀman method! Table 2.12c shows such an example. A
look at the trees of Figure 2.11 should convince the reader that the codes of symbols 5
and 6 must start with diﬀerent bits, but in the code of Table 2.12c they both start with
1. This code is therefore impossible to generate by any relabeling of the nodes of the
trees of Figure 2.11.
2.2.4 Ternary Huﬀman Codes
The Huﬀman code is not unique. Moreover, it does not have to be binary! The Huﬀman
method can easily be applied to codes based on other number systems. Figure 2.13a
shows a Huﬀman code tree for ﬁve symbols with probabilities 0.15, 0.15, 0.2, 0.25, and
0.25. The average code size is
2×0.25 + 3×0.15 + 3×0.15 + 2×0.20 + 2×0.25 = 2.3 bits/symbol.
Figure 2.13b shows a ternary Huﬀman code tree for the same ﬁve symbols. The tree
is constructed by selecting, at each step, three symbols with the smallest probabilities
and merging them into one parent symbol, with the combined probability. The average
code size of this tree is
2×0.15 + 2×0.15 + 2×0.20 + 1×0.25 + 1×0.25 = 1.5 trits/symbol.
Notice that the ternary codes use the digits 0, 1, and 2.
⋄Exercise 2.8: Given seven symbols with probabilities 0.02, 0.03, 0.04, 0.04, 0.12, 0.26,
and 0.49, construct binary and ternary Huﬀman code trees for them and calculate the
average code size in each case.

2.2 Huﬀman Decoding
73
(a)
.15
.15
.20
.15
.15
.20
.50
.25
.25
.25
.45
.30
.25
.55
1.0
1.0
(b)
(c)
(d)
.02
.03
.04
.02
.03
.04
.09
.04
.12
.26
.25
.49
.04
.08
.13
.12
.25
.26
.51
.49
1.0
1.0
.05
Figure 2.13: Binary and Ternary Huﬀman Code Trees.
2.2.5 Height of a Huﬀman Tree
The height of the code tree generated by the Huﬀman algorithm may sometimes be
important because the height is also the length of the longest code in the tree. The
Deﬂate method (Section 3.3), for example, limits the lengths of certain Huﬀman codes
to just three bits.
It is easy to see that the shortest Huﬀman tree is created when the symbols have
equal probabilities. If the symbols are denoted by A, B, C, and so on, then the algorithm
combines pairs of symbols, such A and B, C and D, in the lowest level, and the rest of the
tree consists of interior nodes as shown in Figure 2.14a. The tree is balanced or close
to balanced and its height is ⌈log2 n⌉. In the special case where the number of symbols
n is a power of 2, the height is exactly log2 n. In order to generate the tallest tree, we

74
2.
Huﬀman Coding
need to assign probabilities to the symbols such that each step in the Huﬀman method
will increase the height of the tree by 1. Recall that each step in the Huﬀman algorithm
combines two symbols. Thus, the tallest tree is obtained when the ﬁrst step combines
two of the n symbols and each subsequent step combines the result of its predecessor
with one of the remaining symbols (Figure 2.14b). The height of the complete tree is
therefore n −1, and it is referred to as a lopsided or unbalanced tree.
It is easy to see what symbol probabilities result in such a tree. Denote the two
smallest probabilities by a and b. They are combined in the ﬁrst step to form a node
whose probability is a + b. The second step will combine this node with an original
symbol if one of the symbols has probability a + b (or smaller) and all the remaining
symbols have greater probabilities. Thus, after the second step, the root of the tree
has probability a + b + (a + b) and the third step will combine this root with one of
the remaining symbols if its probability is a + b + (a + b) and the probabilities of the
remaining n −4 symbols are greater. It does not take much to realize that the symbols
have to have probabilities p1 = a, p2 = b, p3 = a+b = p1 +p2, p4 = b+(a+b) = p2 +p3,
p5 = (a + b) + (a + 2b) = p3 + p4, p6 = (a + 2b) + (2a + 3b) = p4 + p5, and so on
(Figure 2.14c). These probabilities form a Fibonacci sequence whose ﬁrst two elements
are a and b. As an example, we select a = 5 and b = 2 and generate the 5-number
Fibonacci sequence 5, 2, 7, 9, and 16. These ﬁve numbers add up to 39, so dividing
them by 39 produces the ﬁve probabilities 5/39, 2/39, 7/39, 9/39, and 15/39.
The
Huﬀman tree generated by them has a maximal height (which is 4).
000
001
010
011 100
101
110
111
(a)
(b)
(c)
a+b
a+2b
2a+3b
3a+5b
5a+8b
a
b
0
10
110
1110
11110 11111
Figure 2.14: Shortest and Tallest Huﬀman Trees.
In principle, symbols in a set can have any probabilities, but in practice, the proba-
bilities of symbols in an input ﬁle are computed by counting the number of occurrences
of each symbol. Imagine a text ﬁle where only the nine symbols A through I appear.
In order for such a ﬁle to produce the tallest Huﬀman tree, where the codes will have
lengths from 1 to 8 bits, the frequencies of occurrence of the nine symbols have to form a
Fibonacci sequence of probabilities. This happens when the frequencies of the symbols
are 1, 1, 2, 3, 5, 8, 13, 21, and 34 (or integer multiples of these). The sum of these
frequencies is 88, so our ﬁle has to be at least that long in order for a symbol to have
8-bit Huﬀman codes. Similarly, if we want to limit the sizes of the Huﬀman codes of a
set of n symbols to 16 bits, we need to count frequencies of at least 4,180 symbols. To
limit the code sizes to 32 bits, the minimum data size is 9,227,464 symbols.

2.2 Huﬀman Decoding
75
If a set of symbols happens to have the Fibonacci probabilities and therefore results
in a maximal-height Huﬀman tree with codes that are too long, the tree can be reshaped
(and the maximum code length shortened) by slightly modifying the symbol probabil-
ities, so they are not much diﬀerent from the original, but do not form a Fibonacci
sequence.
2.2.6 Canonical Huﬀman Codes
The code of Table 2.12c has a simple interpretation. It assigns the ﬁrst four symbols the
3-bit codes 0, 1, 2, and 3, and the last two symbols the 2-bit codes 2 and 3. This is an
example of a canonical Huﬀman code. The word “canonical” means that this particular
code has been selected from among the several (or even many) possible Huﬀman codes
because its properties make it easy and fast to use.
Canonical (adjective): Conforming to orthodox or well-established rules or patterns,
as of procedure.
Table 2.15 shows a slightly bigger example of a canonical Huﬀman code. Imagine
a set of 16 symbols (whose probabilities are irrelevant and are not shown) such that
four symbols are assigned 3-bit codes, ﬁve symbols are assigned 5-bit codes, and the
remaining seven symbols are assigned 6-bit codes. Table 2.15a shows a set of possible
Huﬀman codes, while Table 2.15b shows a set of canonical Huﬀman codes. It is easy to
see that the seven 6-bit canonical codes are simply the 6-bit integers 0 through 6. The
ﬁve codes are the 5-bit integers 4 through 8, and the four codes are the 3-bit integers 3
through 6. We ﬁrst show how these codes are generated and then how they are used.
1:
000
011
9:
10100
01000
2:
001
100
10: 101010 000000
3:
010
101
11: 101011 000001
4:
011
110
12: 101100 000010
5: 10000 00100
13: 101101 000011
6: 10001 00101
14: 101110 000100
7: 10010 00110
15: 101111 000101
8: 10011 00111
16: 110000 000110
(a)
(b)
(a)
(b)
length: 1 2 3 4 5 6
numl:
0 0 4 0 5 7
ﬁrst:
2 4 3 5 4 0
Table 2.15.
Table 2.16.
The top row (length) of Table 2.16 lists the possible code lengths, from 1 to 6 bits.
The second row (numl) lists the number of codes of each length, and the bottom row
(ﬁrst) lists the ﬁrst code in each group. This is why the three groups of codes start with
values 3, 4, and 0. To obtain the top two rows we need to compute the lengths of all
the Huﬀman codes for the given alphabet (see below). The third row is computed by
setting “first[6]:=0;” and iterating
for l:=5 downto 1 do first[l]:=⌈(first[l+1]+numl[l+1])/2⌉;
This guarantees that all the 3-bit preﬁxes of codes longer than three bits will be less
than first[3] (which is 3), all the 5-bit preﬁxes of codes longer than ﬁve bits will be
less than first[5] (which is 4), and so on.

76
2.
Huﬀman Coding
Now for the use of these unusual codes. Canonical Huﬀman codes are useful in
cases where the alphabet is large and where fast decoding is mandatory. Because of the
way the codes are constructed, it is easy for the decoder to identify the length of a code
by reading and examining input bits one by one. Once the length is known, the symbol
can be found in one step. The pseudocode listed here shows the rules for decoding:
l:=1; input v;
while v<first[l]
append next input bit to v; l:=l+1;
endwhile
As an example, suppose that the next code is 00110. As bits are input and appended
to v, it goes through the values 0, 00 = 0, 001 = 1, 0011 = 3, 00110 = 6, while l is
incremented from 1 to 5.
All steps except the last satisfy v<first[l], so the last
step determines the value of l (the code length) as 5. The symbol itself is found by
subtracting v −first[5] = 6 −4 = 2, so it is the third symbol (numbering starts at 0)
in group l = 5 (symbol 7 of the 16 symbols).
The last point to be discussed is the encoder. In order to construct the canoni-
cal Huﬀman code, the encoder needs to know the length of the Huﬀman code of every
symbol. The main problem is the large size of the alphabet, which may make it imprac-
tical or even impossible to build the entire Huﬀman code tree in memory. There is an
algorithm—described in [Hirschberg and Lelewer 90], [Sieminski 88], and [Salomon 07]—
that solves this problem. It calculates the code sizes for an alphabet of n symbols using
just one array of size 2n.
Considine’s Law. Whenever one word or letter can change the entire meaning of a
sentence, the probability of an error being made will be in direct proportion to the
embarrassment it will cause.
—Bob Considine
One morning I was on my way to the market and met a man with four wives
(perfectly legal where we come from). Each wife had four bags, containing four dogs
each, and each dog had four puppies. The question is (think carefully) how many were
going to the market?
2.3 Adaptive Huﬀman Coding
The Huﬀman method assumes that the frequencies of occurrence of all the symbols of
the alphabet are known to the compressor. In practice, the frequencies are seldom, if
ever, known in advance. One approach to this problem is for the compressor to read the
original data twice. The ﬁrst time, it only counts the frequencies; the second time, it
compresses the data. Between the two passes, the compressor constructs the Huﬀman
tree. Such a two-pass method is sometimes called semiadaptive and is normally too slow
to be practical. The method that is used in practice is called adaptive (or dynamic)
Huﬀman coding. This method is the basis of the UNIX compact program. The method

2.3 Adaptive Huﬀman Coding
77
was originally developed by [Faller 73] and [Gallager 78] with substantial improvements
by [Knuth 85].
The main idea is for the compressor and the decompressor to start with an empty
Huﬀman tree and to modify it as symbols are being read and processed (in the case of the
compressor, the word “processed” means compressed; in the case of the decompressor, it
means decompressed). The compressor and decompressor should modify the tree in the
same way, so at any point in the process they should use the same codes, although those
codes may change from step to step. We say that the compressor and decompressor
are synchronized or that they work in lockstep (although they don’t necessarily work
together; compression and decompression normally take place at diﬀerent times). The
term mirroring is perhaps a better choice. The decoder mirrors the operations of the
encoder.
Initially, the compressor starts with an empty Huﬀman tree. No symbols have been
assigned codes yet. The ﬁrst symbol being input is simply written on the output in its
uncompressed form. The symbol is then added to the tree and a code assigned to it.
The next time this symbol is encountered, its current code is written on the output, and
its frequency incremented by 1. Since this modiﬁes the tree, it (the tree) is examined to
see whether it is still a Huﬀman tree (best codes). If not, it is rearranged, an operation
that results in modiﬁed codes.
The decompressor mirrors the same steps. When it reads the uncompressed form
of a symbol, it adds it to the tree and assigns it a code. When it reads a compressed
(variable-length) code, it scans the current tree to determine what symbol the code
belongs to, and it increments the symbol’s frequency and rearranges the tree in the
same way as the compressor.
It is immediately clear that the decompressor needs to know whether the item
it has just input is an uncompressed symbol (normally, an 8-bit ASCII code, but see
Section 2.3.1) or a variable-length code. To remove any ambiguity, each uncompressed
symbol is preceded by a special, variable-size escape code. When the decompressor reads
this code, it knows that the next eight bits are the ASCII code of a symbol that appears
in the compressed ﬁle for the ﬁrst time.
Escape is not his plan. I must face him. Alone.
—David Prowse as Lord Darth Vader in Star Wars (1977)
The trouble is that the escape code should not be any of the variable-length codes
used for the symbols. These codes, however, are being modiﬁed every time the tree is
rearranged, which is why the escape code should also be modiﬁed. A natural way to do
this is to add an empty leaf to the tree, a leaf with a zero frequency of occurrence, that’s
always assigned to the 0-branch of the tree. Since the leaf is in the tree, it is assigned
a variable-length code.
This code is the escape code preceding every uncompressed
symbol. As the tree is being rearranged, the position of the empty leaf—and thus its
code—change, but this escape code is always used to identify uncompressed symbols in
the compressed ﬁle. Figure 2.17 shows how the escape code moves and changes as the
tree grows.

78
2.
Huﬀman Coding
1
0
0
1
0
1
0
0
1
1
0
1
0
1
0
1
0
1
0
0
000
Figure 2.17: The Escape Code.
2.3.1 Uncompressed Codes
If the symbols being compressed are ASCII characters, they may simply be assigned
their ASCII codes as uncompressed codes. In the general case where there may be any
symbols, uncompressed codes of two diﬀerent sizes can be assigned by a simple method.
Here is an example for the case n = 24. The ﬁrst 16 symbols can be assigned the numbers
0 through 15 as their codes. These numbers require only 4 bits, but we encode them in 5
bits. Symbols 17 through 24 can be assigned the numbers 17−16−1 = 0, 18−16−1 = 1
through 24−16−1 = 7 as 4-bit numbers. We end up with the sixteen 5-bit codes 00000,
00001, . . . , 01111, followed by the eight 4-bit codes 0000, 0001, . . . , 0111.
In general, we assume an alphabet that consists of the n symbols a1, a2, . . . , an. We
select integers m and r such that 2m ≤n < 2m+1 and r = n −2m. The ﬁrst 2m symbols
are encoded as the (m + 1)-bit numbers 0 through 2m −1. The remaining symbols are
encoded as m-bit numbers such that the code of ak is k −2m −1. This code is also
called a phased-in binary code (also a minimal binary code).
2.3.2 Modifying the Tree
The chief principle for modifying the tree is to check it each time a symbol is input. If
the tree is no longer a Huﬀman tree, it should be rearranged to become one. A glance
at Figure 2.18a shows what it means for a binary tree to be a Huﬀman tree. The tree in
the ﬁgure contains ﬁve symbols: A, B, C, D, and E. It is shown with the symbols and
their frequencies (in parentheses) after 16 symbols have been input and processed. The
property that makes it a Huﬀman tree is that if we scan it level by level, scanning each
level from left to right, and going from the bottom (the leaves) to the top (the root),
the frequencies will be in sorted, nondescending order. Thus, the bottom-left node (A)
has the lowest frequency, and the top-right node (the root) has the highest frequency.
This is called the sibling property.
⋄Exercise 2.9: Why is this the criterion for a tree to be a Huﬀman tree?
Here is a summary of the operations needed to update the tree. The loop starts
at the current node (the one corresponding to the symbol just input). This node is a
leaf that we denote by X, with frequency of occurrence F. Each iteration of the loop
involves three steps as follows:
1. Compare X to its successors in the tree (from left to right and bottom to top). If
the immediate successor has frequency F + 1 or greater, the nodes are still in sorted
order and there is no need to change anything. Otherwise, some successors of X have

2.3 Adaptive Huﬀman Coding
79
identical frequencies of F or smaller. In this case, X should be swapped with the last
node in this group (except that X should not be swapped with its parent).
2. Increment the frequency of X from F to F + 1. Increment the frequencies of all its
parents.
3. If X is the root, the loop stops; otherwise, it repeats with the parent of node X.
Figure 2.18b shows the tree after the frequency of node A has been incremented
from 1 to 2.
It is easy to follow the three rules above to see how incrementing the
frequency of A results in incrementing the frequencies of all its parents. No swaps are
needed in this simple case because the frequency of A hasn’t exceeded the frequency of
its immediate successor B. Figure 2.18c shows what happens when A’s frequency has
been incremented again, from 2 to 3. The three nodes following A, namely, B, C, and
D, have frequencies of 2, so A is swapped with the last of them, D. The frequencies
of the new parents of A are then incremented, and each is compared with its successor,
but no more swaps are needed.
Figure 2.18d shows the tree after the frequency of A has been incremented to 4.
Once we decide that A is the current node, its frequency (which is still 3) is compared to
that of its successor (4), and the decision is not to swap. A’s frequency is incremented,
followed by incrementing the frequencies of its parents.
In Figure 2.18e, A is again the current node. Its frequency (4) equals that of its
successor, so they should be swapped. This is shown in Figure 2.18f, where A’s frequency
is 5. The next loop iteration examines the parent of A, with frequency 10. It should
be swapped with its successor E (with frequency 9), which leads to the ﬁnal tree of
Figure 2.18g.
2.3.3 Counter Overﬂow
The frequency counts are accumulated in the Huﬀman tree in ﬁxed-size ﬁelds, and
such ﬁelds may overﬂow.
A 16-bit unsigned ﬁeld can accommodate counts of up to
216 −1 = 65,535. A simple solution to the counter overﬂow problem is to watch the
count ﬁeld of the root each time it is incremented, and when it reaches its maximum
value, to rescale all the frequency counts by dividing them by 2 (integer division). In
practice, this is done by dividing the count ﬁelds of the leaves, then updating the counts
of the interior nodes. Each interior node gets the sum of the counts of its children. The
problem is that the counts are integers, and integer division reduces precision. This may
change a Huﬀman tree to one that does not satisfy the sibling property.
A simple example is shown in Figure 2.18h. After the counts of the leaves are halved,
the three interior nodes are updated as shown in Figure 2.18i. The latter tree, however,
is no longer a Huﬀman tree, since the counts are no longer in sorted order. The solution
is to rebuild the tree each time the counts are rescaled, which does not happen very
often. A Huﬀman data compression program intended for general use should therefore
have large count ﬁelds that would not overﬂow very often. A 4-byte count ﬁeld overﬂows
at 232 −1 ≈4.3 × 109.
It should be noted that after rescaling the counts, the new symbols being read and
compressed have more eﬀect on the counts than the old symbols (those counted before
the rescaling). This turns out to be fortuitous since it is known from experience that
the probability of appearance of a symbol depends more on the symbols immediately
preceding it than on symbols that appeared in the distant past.

80
2.
Huﬀman Coding
A
B
C
D
E
(1)
(2)
(2)
(2)
(3)
(4)
(16)
(9)
(7)
A
B
C
D
E
(2)
(2)
(2)
(3)
(4)
(9)
A
B
C
D
E
(2)
(2)
(2)
(9)
A
B
C
D
E
(2)
(2)
(2)
(4)
(4)
(6)
(9)
(10)
A
B
C
D
E
(2)
(2)
(2)
(4)
(4)
(6)
(19)
(9)
(10)
A
B
C
D
E
(2)
(2)
(2)
(5)
(4)
(6)
(19)
(9)
(10)
A
B
C
D
E
(2)
(2)
(2)
(5)
(4)
(6)
(9)
(11)
(20)
155
155
310
310
310
620
930
77
77
155
155
154
310
464
(a)
(19)
(2)
(4)
(4)
(8)
(17)
(18)
(9)
(5)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
Figure 2.18: Updating the Huﬀman Tree.

2.3 Adaptive Huﬀman Coding
81
2.3.4 Code Overﬂow
An even more serious problem is code overﬂow. This may happen when many symbols
are added to the tree, and it becomes tall. The codes themselves are not stored in the
tree, since they change all the time, and the compressor has to ﬁgure out the code of a
symbol X each time X is input. Here are the details of this process:
1. The encoder has to locate symbol X in the tree. The tree has to be implemented as
an array of structures, each a node, and the array is searched linearly.
2. If X is not found, the escape code is emitted, followed by the uncompressed code of
X. X is then added to the tree.
3. If X is found, the compressor moves from node X back to the root, building the
code bit by bit as it goes along. Each time it goes from a left child to a parent, a “1”
is appended to the code. Going from a right child to a parent appends a “0” bit to the
code (or vice versa, but this should be consistent because it is mirrored by the decoder).
Those bits have to be accumulated someplace, since they have to be emitted in the
reverse order in which they are created. When the tree gets taller, the codes get longer.
If they are accumulated in a 16-bit integer, then codes longer than 16 bits would cause
a malfunction.
One solution to the code overﬂow problem is to accumulate the bits of a code in a
linked list, where new nodes can be created, limited in number only by the amount of
available memory. This is general but slow. Another solution is to accumulate the codes
in a large integer variable (perhaps 50 bits wide) and document a maximum code size
of 50 bits as one of the limitations of the program.
Fortunately, this problem does not aﬀect the decoding process. The decoder reads
the compressed code bit by bit and uses each bit to move one step left or right down
the tree until it reaches a leaf node. If the leaf is the escape code, the decoder reads the
uncompressed code of the symbol oﬀthe compressed data (and adds the symbol to the
tree). Otherwise, the uncompressed code is found in the leaf node.
⋄Exercise 2.10: Given the 11-symbol string sir␣sid␣is, apply the adaptive Huﬀman
method to it. For each symbol input, show the output, the tree after the symbol has
been added to it, the tree after being rearranged (if necessary), and the list of nodes
traversed left to right and bottom up.
2.3.5 A Variant
This variant of the adaptive Huﬀman method is simpler but less eﬃcient. The idea
is to calculate a set of n variable-length codes based on equal probabilities, to assign
those codes to the n symbols at random, and to change the assignments “on the ﬂy,” as
symbols are being read and compressed. The method is ineﬃcient because the codes are
not based on the actual probabilities of the symbols in the input. However, it is simpler
to implement and also faster than the adaptive method described earlier, because it has
to swap rows in a table, rather than update a tree, when updating the frequencies of
the symbols.
The main data structure is an n × 3 table where the three columns store the names
of the n symbols, their frequencies of occurrence so far, and their codes. The table is
always kept sorted by the second column. When the frequency counts in the second

82
2.
Huﬀman Coding
Name Count Code
a1
0
0
a2
0
10
a3
0
110
a4
0
111
(a)
Name Count Code
a2
1
0
a1
0
10
a3
0
110
a4
0
111
(b)
Name Count Code
a2
1
0
a4
1
10
a3
0
110
a1
0
111
(c)
Name Count Code
a4
2
0
a2
1
10
a3
0
110
a1
0
111
(d)
Table 2.19: Four Steps in a Huﬀman Variant.
column change, rows are swapped, but only columns 1 and 2 are moved. The codes in
column 3 never change. Table 2.19 shows an example of four symbols and the behavior
of the method when the string a2, a4, a4 is compressed.
Table 2.19a shows the initial state.
After the ﬁrst symbol a2 is read, its count
is incremented, and since it is now the largest count, rows 1 and 2 are swapped (Ta-
ble 2.19b). After the second symbol a4 is read, its count is incremented and rows 2 and
4 are swapped (Table 2.19c). Finally, after reading the last symbol a4, its count is the
largest, so rows 1 and 2 are swapped (Table 2.19d).
The only point that can cause a problem with this method is overﬂow of the count
ﬁelds. If such a ﬁeld is k bits wide, its maximum value is 2k −1, so it will overﬂow
when incremented for the 2kth time. This may happen if the size of the input ﬁle is not
known in advance, which is very common. Fortunately, we do not really need to know
the counts, we just need them in sorted order, which makes it easy to solve this problem.
One solution is to count the input symbols and, after 2k −1 symbols are input and
compressed, to (integer) divide all the count ﬁelds by 2 (or shift them one position to
the right, if this is easier).
Another, similar, solution is to check each count ﬁeld every time it is incremented,
and if it has reached its maximum value (if it consists of all ones), to integer divide all
the count ﬁelds by 2, as mentioned earlier. This approach requires fewer divisions but
more complex tests.
Naturally, whatever solution is adopted should be used by both the compressor and
decompressor.
2.3.6 Vitter’s Method
An improvement of the original algorithm, due to [Vitter 87], which also includes exten-
sive analysis is based on the following key ideas:
1. A diﬀerent scheme should be used to number the nodes in the dynamic Huﬀman
tree. It is called implicit numbering, and it numbers the nodes from the bottom up and
in each level from left to right.
2. The Huﬀman tree should be updated in such a way that the following will always
be satisﬁed. For each weight w, all leaves of weight w precede (in the sense of implicit
numbering) all the internal nodes of the same weight. This is an invariant.
These ideas result in the following beneﬁts:
1. In the original algorithm, it is possible that a rearrangement of the tree would
move a node down one level. In the improved version, this does not happen.

2.3 Adaptive Huﬀman Coding
83
2. Each time the Huﬀman tree is updated in the original algorithm, some nodes
may be moved up. In the improved version, at most one node has to be moved up.
3. The Huﬀman tree in the improved version minimizes the sum of distances from
the root to the leaves and also has the minimum height.
A special data structure, called a ﬂoating tree, is proposed to make it easy to
maintain the required invariant. It can be shown that this version performs much better
than the original algorithm. Speciﬁcally, if a two-pass Huﬀman method compresses an
input ﬁle of n symbols to S bits, then the original adaptive Huﬀman algorithm can
compress it to at most 2S + n bits, whereas the improved version can compress it down
to S + n bits—a signiﬁcant diﬀerence! Notice that these results do not depend on the
size of the alphabet, only on the size n of the data being compressed and on its nature
(which determines S).
“I think you’re begging the question,” said Haydock, “and I can see looming ahead
one of those terrible exercises in probability where six men have white hats and six
men have black hats and you have to work it out by mathematics how likely it is that
the hats will get mixed up and in what proportion. If you start thinking about things
like that, you would go round the bend. Let me assure you of that!”
—Agatha Christie, The Mirror Crack’d
History of Fax. Fax machines have been popular since the mid-1980s, so it is natural
to assume that this is new technology. In fact, the ﬁrst fax machine was invented in
1843, by Alexander Bain, a Scottish clock and instrument maker
and all-round inventor. Among his many other achievements, Bain
also invented the ﬁrst electrical clock (powered by an electromagnet
propelling a pendulum), developed chemical telegraph receivers and
punch-tapes for fast telegraph transmissions, and installed the ﬁrst
telegraph line between Edinburgh and Glasgow.
The patent for the fax machine (grandly titled “improvements
in producing and regulating electric currents and improvements in
timepieces and in electric printing and signal telegraphs”) was granted
to Bain on May 27, 1843, 33 years before a similar patent (for the
telephone) was given to Alexander Graham Bell.
Bain’s fax machine transmitter scanned a ﬂat, electrically conductive metal surface
with a stylus mounted on a pendulum that was powered by an electromagnet. The
stylus picked up writing from the surface and sent it through a wire to the stylus of
the receiver, where the image was reproduced on a similar electrically conductive metal
surface. Reference [hﬀax 07] has a ﬁgure of this apparatus.
Unfortunately, Bain’s invention was not very practical and did not catch on, which
is easily proved by the well-known fact that Queen Victoria never actually said “I’ll drop
you a fax.”
In 1850, Frederick Bakewell, a London inventor, made several improvements on
Bain’s design. He built a device that he called a copying telegraph, and received a patent

84
2.
Huﬀman Coding
on it. Bakewell demonstrated his machine at the 1851 Great Exhibition in London.
In 1862, Italian physicist Giovanni Caselli built a fax machine (the pantelegraph),
that was based on Bain’s invention and also included a synchronizing apparatus. It
was more successful than Bain’s device and was used by the French Post and Telegraph
agency between Paris and Lyon from 1856 to 1870. Even the Emperor of China heard
about the pantelegraph and sent oﬃcials to Paris to study the device. The Chinese
immediately recognized the advantages of facsimile for Chinese text, which was impos-
sible to handle by telegraph because of its thousands of ideograms. Unfortunately, the
negotiations between Peking and Caselli failed.
Elisha Gray, arguably the best example of the quintessential loser, invented the
telephone, but is virtually unknown today because he was beaten by Alexander Graham
Bell, who arrived at the patent oﬃce a few hours before Gray on the fateful day of March
7, 1876. Born in Barnesville, Ohio, Gray invented and patented many electrical devices,
including a facsimile apparatus. He also founded what later became the Western Electric
Company.
Ernest A. Hummel, a watchmaker from St.
Paul, Minnesota, invented, in 1895
a device he dubbed a copying telegraph, or telediagraph. This machine was based on
synchronized rotating drums, with a platinum stylus as an electrode in the transmitter.
It was used by the New York Herald to send pictures via telegraph lines. An improved
version (in 1899) was sold to several newspapers (the Chicago Times Herald, the St.
Louis Republic, the Boston Herald, and the Philadelphia Inquirer) and it, as well as
other, very similar machines, were in use to transmit newsworthy images until the 1970s.
A practical fax machine (perhaps the ﬁrst practical one) was invented in 1902 by
Arthur Korn in Germany. This was a photoelectric device and it was used to transmit
photographs in Germany from 1907.
In 1924, Richard H. Ranger, a designer for the Radio Corporation of America
(RCA), invented the wireless photoradiogram, or transoceanic radio facsimile.
This
machine can be considered the true forerunner of today’s fax machines. On November 29,
1924, a photograph of the American President Calvin Coolidge that was sent from New
York to London became the ﬁrst image reproduced by transoceanic wireless facsimile.
The next step was the belinograph, invented in 1925 by the French engineer Edouard
Belin. An image was placed on a drum and scanned with a powerful beam of light. The
reﬂection was converted to an analog voltage by a photoelectric cell. The voltage was sent
to a receiver, where it was converted into mechanical movement of a pen to reproduce
the image on a blank sheet of paper on an identical drum rotating at the same speed.
The fax machines we all use are still based on the principle of scanning a document with
light, but they are controlled by a microprocessor and have a small number of moving
parts.
In 1924, the American Telephone & Telegraph Company (AT&T) decided to im-
prove telephone fax technology. The result of this eﬀort was a telephotography machine
that was used to send newsworthy photographs long distance for quick newspaper pub-
lication.
By 1926, RCA invented the Radiophoto, a fax machine based on radio transmissions.
The Hellschreiber was invented in 1929 by Rudolf Hell, a pioneer in mechanical
image scanning and transmission. During WW2, it was sometimes used by the German
military in conjunction with the Enigma encryption machine.

2.4 Facsimile Compression
85
In 1947, Alexander Muirhead invented a very successful fax machine.
On March 4, 1955, the ﬁrst radio fax transmission was sent across the continent.
Fax machines based on optical scanning of a document were developed over the
years, but the spark that ignited the fax revolution was the development, in 1983, of the
Group 3 CCITT standard for sending faxes at rates of 9,600 bps.
More history and pictures of many early fax and telegraph machines can be found
at [hﬀax 07] and [technikum29 07].
2.4 Facsimile Compression
Data compression is especially important when images are transmitted over a communi-
cations line because a person is often waiting at the receiving end, eager to see something
quickly. Documents transferred between fax machines are sent as bitmaps, so a stan-
dard compression algorithm was needed when those machines became popular. Several
methods were developed and proposed by the ITU-T.
The ITU-T is one of four permanent parts of the International Telecommunications
Union (ITU), based in Geneva, Switzerland (http://www.itu.ch/). It issues recommen-
dations for standards applying to modems, packet switched interfaces, V.24 connectors,
and similar devices. Although it has no power of enforcement, the standards it recom-
mends are generally accepted and adopted by industry. Until March 1993, the ITU-T
was known as the Consultative Committee for International Telephone and Telegraph
(Comit´e Consultatif International T´el´egraphique et T´el´ephonique, or CCITT).
CCITT: Can’t Conceive Intelligent Thoughts Today
The ﬁrst data compression standards developed by the ITU-T were T2 (also known
as Group 1) and T3 (Group 2). These are now obsolete and have been replaced by T4
(Group 3) and T6 (Group 4). Group 3 is currently used by all fax machines designed to
operate with the Public Switched Telephone Network (PSTN). These are the machines
we have at home, and at the time of writing, they operate at maximum speeds of 9,600
baud. Group 4 is used by fax machines designed to operate on a digital network, such
as ISDN. They have typical speeds of 64K bits/sec (baud). Both methods can produce
compression factors of 10 or better, reducing the transmission time of a typical page to
about a minute with the former, and a few seconds with the latter.
One-dimensional coding. A fax machine scans a document line by line, con-
verting each scan line to many small black and white dots called pels (from Picture
ELement). The horizontal resolution is always 8.05 pels per millimeter (about 205 pels
per inch). An 8.5-inch-wide scan line is therefore converted to 1728 pels. The T4 stan-
dard, though, recommends to scan only about 8.2 inches, thereby producing 1664 pels
per scan line (these numbers, as well as those in the next paragraph, are all to within
±1% accuracy).
The word facsimile comes from the Latin facere (make) and similis (like).

86
2.
Huﬀman Coding
The vertical resolution is either 3.85 scan lines per millimeter (standard mode) or
7.7 lines/mm (ﬁne mode). Many fax machines have also a very-ﬁne mode, where they
scan 15.4 lines/mm.
Table 2.20 assumes a 10-inch-high page (254 mm), and shows
the total number of pels per page, and typical transmission times for the three modes
without compression. The times are long, illustrating the importance of compression in
fax transmissions.
Scan
Pels per
Pels per
Time
Time
lines
line
page
(sec.)
(min.)
978
1664
1.670M
170
2.82
1956
1664
3.255M
339
5.65
3912
1664
6.510M
678
11.3
Ten inches equal 254 mm. The number of pels
is in the millions, and the transmission times, at
9600 baud without compression, are between 3
and 11 minutes, depending on the mode. How-
ever, if the page is shorter than 10 inches, or if
most of it is white, the compression factor can
be 10 or better, resulting in transmission times
of between 17 and 68 seconds.
Table 2.20: Fax Transmission Times.
To derive the Group 3 code, the committee appointed by the ITU-T counted all the
run lengths of white and black pels in a set of eight “training” documents that they felt
represent typical text and images sent by fax, and then applied the Huﬀman algorithm
to construct a variable-length code and assign codewords to all the run length. (The
eight documents are described in Table 2.21 and can be found at [funet 07].) The most
common run lengths were found to be 2, 3, and 4 black pixels, so they were assigned
the shortest codes (Table 2.22). Next come run lengths of 2–7 white pixels, which were
assigned slightly longer codes. Most run lengths were rare and were assigned long, 12-bit
codes. Thus, Group 3 uses a combination of RLE and Huﬀman coding.
Image
Description
1
Typed business letter (English)
2
Circuit diagram (hand drawn)
3
Printed and typed invoice (French)
4
Densely typed report (French)
5
Printed technical article including ﬁgures and equations (French)
6
Graph with printed captions (French)
7
Dense document (Kanji)
8
Handwritten memo with very large white-on-black letters (English)
Table 2.21: The Eight CCITT Training Documents.

2.4 Facsimile Compression
87
⋄Exercise 2.11: A run length of 1,664 white pels was assigned the short code 011000.
Why is this length so common?
Since run lengths can be long, the Huﬀman algorithm was modiﬁed. Codes were
assigned to run lengths of 1 to 63 pels (they are the termination codes in Table 2.22a)
and to run lengths that are multiples of 64 pels (the make-up codes in Table 2.22b).
Group 3 is therefore a modiﬁed Huﬀman code (also called MH). The code of a run length
is either a single termination code (if the run length is short) or one or more make-up
codes, followed by one termination code (if it is long). Here are some examples:
1. A run length of 12 white pels is coded as 001000.
2. A run length of 76 white pels (= 64 + 12) is coded as 11011|001000.
3. A run length of 140 white pels (= 128 + 12) is coded as 10010|001000.
4. A run length of 64 black pels (= 64 + 0) is coded as 0000001111|0000110111.
5. A run length of 2,561 black pels (2560 + 1) is coded as 000000011111|010.
⋄Exercise 2.12: There are no runs of length zero. Why then were codes assigned to
runs of zero black and zero white pels?
⋄Exercise 2.13: An 8.5-inch-wide scan line results in 1,728 pels, so how can there be a
run of 2,561 consecutive pels?
Each scan line is coded separately, and its code is terminated by the special 12-bit
EOL code 000000000001. Each line also gets one white pel appended to it on the left
when it is scanned. This is done to remove any ambiguity when the line is decoded on
the receiving end. After reading the EOL for the previous line, the receiver assumes that
the new line starts with a run of white pels, and it ignores the ﬁrst of them. Examples:
1. The 14-pel line
is coded as the run lengths 1w 3b 2w
2b 7w EOL, which become 000111|10|0111|11|1111|000000000001. The decoder ignores
the single white pel at the start.
2. The line
is coded as the run lengths 3w 5b 5w 2b
EOL, which becomes the binary string 1000|0011|1100|11|000000000001.
⋄Exercise 2.14: The Group 3 code for a run length of ﬁve black pels (0011) is also the
preﬁx of the codes for run lengths of 61, 62, and 63 white pels. Explain this.
In computing, a newline (also known as a line break or end-of-line / EOL character)
is a special character or sequence of characters signifying the end of a line of text.
The name comes from the fact that the next character after the newline will appear
on a new line—that is, on the next line below the text immediately preceding the
newline. The actual codes representing a newline vary across hardware platforms and
operating systems, which can be a problem when exchanging data between systems
with diﬀerent representations.
—From http://en.wikipedia.org/wiki/End-of-line
The Group 3 code has no error correction, but many errors can be detected. Because
of the nature of the Huﬀman code, even one bad bit in the transmission can cause the
receiver to get out of synchronization, and to produce a string of wrong pels.
This
is why each scan line is encoded separately. If the receiver detects an error, it skips

88
2.
Huﬀman Coding
(a)
White
Black
White
Black
Run
code-
code-
Run
code-
code-
length
word
word
length
word
word
0 00110101 0000110111
32 00011011 000001101010
1 000111
010
33 00010010 000001101011
2 0111
11
34 00010011 000011010010
3 1000
10
35 00010100 000011010011
4 1011
011
36 00010101 000011010100
5 1100
0011
37 00010110 000011010101
6 1110
0010
38 00010111 000011010110
7 1111
00011
39 00101000 000011010111
8 10011
000101
40 00101001 000001101100
9 10100
000100
41 00101010 000001101101
10 00111
0000100
42 00101011 000011011010
11 01000
0000101
43 00101100 000011011011
12 001000
0000111
44 00101101 000001010100
13 000011
00000100
45 00000100 000001010101
14 110100
00000111
46 00000101 000001010110
15 110101
000011000
47 00001010 000001010111
16 101010
0000010111
48 00001011 000001100100
17 101011
0000011000
49 01010010 000001100101
18 0100111
0000001000
50 01010011 000001010010
19 0001100
00001100111
51 01010100 000001010011
20 0001000
00001101000
52 01010101 000000100100
21 0010111
00001101100
53 00100100 000000110111
22 0000011
00000110111
54 00100101 000000111000
23 0000100
00000101000
55 01011000 000000100111
24 0101000
00000010111
56 01011001 000000101000
25 0101011
00000011000
57 01011010 000001011000
26 0010011
000011001010
58 01011011 000001011001
27 0100100
000011001011
59 01001010 000000101011
28 0011000
000011001100
60 01001011 000000101100
29 00000010 000011001101
61 00110010 000001011010
30 00000011 000001101000
62 00110011 000001100110
31 00011010 000001101001
63 00110100 000001100111
(b)
White
Black
White
Black
Run
code-
code-
Run
code-
code-
length
word
word
length
word
word
64 11011
0000001111
1344 011011010
0000001010011
128 10010
000011001000
1408 011011011
0000001010100
192 010111
000011001001
1472 010011000
0000001010101
256 0110111
000001011011
1536 010011001
0000001011010
320 00110110
000000110011
1600 010011010
0000001011011
384 00110111
000000110100
1664 011000
0000001100100
448 01100100
000000110101
1728 010011011
0000001100101
512 01100101
0000001101100
1792 00000001000
same as
576 01101000
0000001101101
1856 00000001100
white
640 01100111
0000001001010
1920 00000001101
from this
704 011001100 0000001001011
1984 000000010010 point
768 011001101 0000001001100
2048 000000010011
832 011010010 0000001001101
2112 000000010100
896 011010011 0000001110010
2176 000000010101
960 011010100 0000001110011
2240 000000010110
1024 011010101 0000001110100
2304 000000010111
1088 011010110 0000001110101
2368 000000011100
1152 011010111 0000001110110
2432 000000011101
1216 011011000 0000001110111
2496 000000011110
1280 011011001 0000001010010
2560 000000011111
Table 2.22: Group 3 and 4 Fax Codes: (a) Termination Codes, (b) Make-Up Codes.

2.4 Facsimile Compression
89
bits, looking for an EOL. This way, one error can cause at most one scan line to be
received incorrectly. If the receiver does not see an EOL after a certain number of lines,
it assumes a high error rate, and it aborts the process, notifying the transmitter. Since
the codes are between two and 12 bits long, the receiver detects an error if it cannot
decode a valid code after reading 12 bits.
Each page of the coded document is preceded by one EOL and is followed by six EOL
codes. Because each line is coded separately, this method is a one-dimensional coding
scheme. The compression ratio depends on the image. Images with large contiguous
black or white areas (text or black and white images) can be highly compressed. Images
with many short runs can sometimes produce negative compression. This is especially
true in the case of images with shades of gray (such as scanned photographs). Such
shades are produced by halftoning, which covers areas with many alternating black and
white pels (runs of length 1).
⋄Exercise 2.15: What is the compression ratio for runs of length one (i.e., strictly
alternating pels)?
The T4 standard also allows for ﬁll bits to be inserted between the data bits and
the EOL. This is done in cases where a pause is necessary, or where the total number of
bits transmitted for a scan line must be a multiple of 8. The ﬁll bits are zeros.
Example: The binary string 000111|10|0111|11|1111|000000000001 becomes
000111|10|0111|11|1111|00|0000000001 after two zeros are added as ﬁll bits, bringing the
total length of the string to 32 bits (= 8 × 4). The decoder sees the two zeros of the ﬁll,
followed by the 11 zeros of the EOL, followed by the single 1, so it knows that it has
encountered a ﬁll followed by an EOL.
Two-dimensional coding. This variant was developed because one-dimensional
coding produces poor results for images with gray areas. Two-dimensional coding is
optional on fax machines that use Group 3 but is the only method used by machines
intended to work in a digital network. When a fax machine using Group 3 supports two-
dimensional coding as an option, each EOL is followed by one extra bit, to indicate the
compression method used for the next scan line. That bit is 1 if the next line is encoded
with one-dimensional coding, and 0 if it is encoded with two-dimensional coding.
The two-dimensional coding method is also called MMR, for modiﬁed modiﬁed
READ, where READ stands for relative element address designate. The term “mod-
iﬁed modiﬁed” is used because this is a modiﬁcation of one-dimensional coding, which
itself is a modiﬁcation of the original Huﬀman method. The two-dimensional coding
method is described in detail in [Salomon 07] and other references, but here are its main
principles. The method compares the current scan line (called the coding line) to its
predecessor (referred to as the reference line) and records the diﬀerences between them,
the assumption being that two consecutive lines in a document will normally diﬀer by
just a few pels. The method assumes that there is an all-white line above the page, which
is used as the reference line for the ﬁrst scan line of the page. After coding the ﬁrst line,
it becomes the reference line, and the second scan line is coded. As in one-dimensional
coding, each line is assumed to start with a white pel, which is ignored by the receiver.
The two-dimensional coding method is more prone to errors than one-dimensional
coding, because any error in decoding a line will cause errors in decoding all its successors
and will propagate throughout the entire document. This is why the T.4 (Group 3)

90
2.
Huﬀman Coding
standard includes a requirement that after a line is encoded with the one-dimensional
method, at most K −1 lines will be encoded with the two-dimensional coding method.
For standard resolution K = 2, and for ﬁne resolution K = 4.
The T.6 standard
(Group 4) does not have this requirement, and uses two-dimensional coding exclusively.
Chapter Summary
Huﬀman coding is one of the basic techniques of data compression.
It is also fast,
conceptually simple, and easy to implement. The Huﬀman encoding algorithm starts
with a set of symbols whose probabilities are known and constructs a code tree. Once
the tree is complete, it is used to determine the variable-length, preﬁx codewords for the
individual symbols. Each leaf of the tree corresponds to a data symbol and the preﬁx
code of a symbol S is determined by sliding down the tree from the root to the leaf
that corresponds to S. It can be shown that these codewords are the best possible in
the sense that they feature the shortest average length. However, the codewords are not
unique and there are several (perhaps even many) diﬀerent sets of codewords that have
the same average length. Huﬀman decoding starts by reading bits from the compressed
ﬁle and using them to slide down the tree from node to node until a leaf (and thus a
data symbol) is reached. Section 2.2.1 describes an interesting method for fast decoding.
The Huﬀman method requires knowledge of the symbols’ probabilities, but in prac-
tice, these are not always known in advance. This chapter lists the following methods
for handling this problem.
Use a set of training documents. The implementor of a Huﬀman codec (compres-
sor/decompressor) selects a set of documents that are judged typical or average. The
documents are analyzed once, counting the number of occurrences (and hence also the
probability) of each data symbol. Based on these probabilities, the implementor con-
structs a Huﬀman code (a set of codewords for all the symbols in the alphabet) and
hard-codes this code in both encoder and decoder. Such a code may not conform to the
symbols’ probabilities of any particular input ﬁle that’s being compressed, and so does
not produce the best compression, but this approach is simple and fast. The compression
method used by fax machines (Section 2.4) is based on this approach.
A two-pass compression job produces the ideal codewords for the input ﬁle, but is
slow. In this approach, the input ﬁle is read twice. In the ﬁrst pass, the encoder counts
symbol occurrences. Between the passes, the encoder uses this information to compute
symbol probabilities and constructs a set of Huﬀman codewords for the particular data
being compressed.
In the second pass the encoder actually compresses the data by
replacing each data symbol with its Huﬀman codeword.
An adaptive compression algorithm achieves the best of both worlds, being both
eﬀective and fast, but is more diﬃcult to implement. The principle is to start with
an empty Huﬀman code tree and to update the tree as input symbols are read and
processed. When a symbol is input, the tree is searched for it. If the symbol is in the
tree, its codeword is used; otherwise, it is added to the tree and a new codeword is
assigned to it. In either case, the tree is examined and may have to be rearranged to

Chapter Summary
91
keep it a Huﬀman code tree. This process has to be designed carefully, to make sure
that the decoder can perform it in the same way as the encoder (in lockstep). Such an
adaptive algorithm is discussed in Section 2.3.
The Huﬀman method is simple, fast, and produces excellent results, but is not as
eﬀective as arithmetic coding (Chapter 4). The conscientious reader may beneﬁt from
the discussion in [Bookstein and Klein 93], where the authors argue in favor of Huﬀman
coding.
Self-Assessment Questions
1. In a programming language of your choice, implement Huﬀman encoding and
test it on the ﬁve symbols of Figure 2.1.
2. Complete the decoding example in the second paragraph of Section 2.2.1.
3. The fax compression standard of Section 2.4 is based on eight training documents
selected by the CCITT (the predecessor of the ITU-T). Select your own set of eight
training documents (black and white images on paper) and scan them at 200 dpi to
determine the frequencies of occurrence of all the runs of black and white pels. Sort
the runs in ascending order and compare their probabilities to the lengths of the codes
of Table 2.22 (your most-common runs should correspond to the shortest codes of this
table).
The novelty of waking up to a fax machine next to your bed going oﬀ
in the middle of the night with an order from Japan wears oﬀ.
—Naomi Bradshaw

3
Dictionary Methods
The Huﬀman algorithm is based on the probabilities of the individual data symbols.
These probabilities become a statistical model of the data. As a result, the compres-
sion produced by this method depends on how good that model is. Dictionary-based
compression methods are diﬀerent. They do not use a statistical model of the data,
nor do they employ variable-length codes. Instead they select strings of symbols from
the input and employ a dictionary to encode each string as a token. The dictionary
holds strings of symbols, and it may be static or dynamic (adaptive). The former is
permanent, sometimes allowing the addition of strings but no deletions, whereas the
latter holds strings previously found in the input, allowing for additions and deletions
of strings as new input is being read.
Given a string of n symbols, a dictionary-based compressor can, in principle, com-
press it down to nH bits where H is the entropy of the string. Thus, dictionary-based
compressors are entropy encoders, but only if the input ﬁle is very large. For most ﬁles
in practical applications, dictionary-based compressors produce results that are good
enough to make this type of encoder very popular.
Such encoders are also general
purpose, performing on images and audio data as well as they perform on text.
The simplest example of a static dictionary is a dictionary of the English language
used to compress English text. Imagine a dictionary containing perhaps half a million
words (without their deﬁnitions). A word (a string of symbols terminated by a space or
a punctuation mark) is read from the input and the dictionary is searched. If a match is
found, an index to the dictionary is written on the output. Otherwise, the uncompressed
word itself is written.
As a result, the output contains indexes and raw words, and it is important to
distinguish between them. This can be done by reserving an extra bit in each item

94
3.
Dictionary Methods
written on the output. In principle, a 19-bit index is suﬃcient to specify an item in
a 219 = 524,288-word dictionary. Thus, when a match is found, we can write a 20-bit
token that consists of a ﬂag bit (perhaps a zero) followed by a 19-bit index. When no
match is found, a ﬂag of 1 is written on the output, followed by the size of the unmatched
word, followed by the word itself.
Example: Assuming that the word bet is found in dictionary entry 1025, it is
encoded as the 20-bit number 0|0000000010000000001. Assuming that the word xet
is not found, it is encoded as 1|0000011|01111000|01100101|01110100. This is a 4-byte
number where the 7-bit ﬁeld 0000011 indicates that three more bytes follow.
Assuming that the size is written as a 7-bit number, and that an average word size is
ﬁve characters, an uncompressed word occupies, on average, six bytes (= 48 bits) in the
output. Compressing 48 bits into 20 bits is excellent if it happens often enough. Thus,
we have to answer the question, how many matches are needed in order to have overall
compression? We denote the probability of a match (the case where the word is found
in the dictionary) by P. After reading and compressing N words, the size of the output
will be N[20P + 48(1 −P)] = N[48 −28P] bits. The size of the input is (assuming ﬁve
characters per word) 40N bits. Compression is achieved when N[48−28P] < 40N, which
implies P > 0.29. We need a matching rate of 29% or better to achieve compression.
⋄Exercise 3.1: (1) What compression factor do we get with P = 0.9? (2) What is the
maximum compression possible with this method?
As long as the input consists of English text, most words will be found in a 500,000-
word dictionary. Other types of data, however, may not do as well. A ﬁle with the source
code of a computer program may contain “words” such as cout, xor, and malloc that
may not be found in an English dictionary. A binary ﬁle normally contains gibberish
when viewed in ASCII (Figure 3.1), so very few matches may be found, resulting in
considerable expansion instead of compression.
Figure 3.1: An Image and Corresponding Text.
Thus, a static dictionary is not a good choice for a general-purpose compressor. It
may, however, be a good choice for a special-purpose dictionary-based encoder. Consider,
for example, a chain of hardware stores. Their ﬁles may contain words such as nut, bolt,
and paint many times, but words such as peanut, lightning, and painting will be
rare. Special-purpose compression software for such a company may beneﬁt from a small,
specialized dictionary containing, perhaps, just a few hundred words. The computers in
each branch would have a copy of the dictionary, making it easy to compress ﬁles and
send them between stores and oﬃces in the chain.
In general, an adaptive dictionary-based method is preferable. Such a method can
start with an empty dictionary or with a small, default dictionary, add words to it as

3.1 LZ78
95
they are found in the input, and delete old words because a big dictionary slows down
the search. Such a method consists of a loop where each iteration starts by reading
the input and breaking it up (parsing it) into words or phrases. It then should search
the dictionary for each word and, if a match is found, write a token on the output.
Otherwise, the uncompressed word is output and also added to the dictionary. The last
step in each iteration checks whether an old word should be deleted from the dictionary.
This may sound complicated, but it has two advantages:
1. It involves string search and match operations, rather than numerical computations.
Many programmers prefer that.
2. The decoder is simple (this is an asymmetric compression method). It reads the next
input item and determines whether it is a token or raw data. A token is used to obtain
data from the dictionary and write it on the output. Raw data is output as is. The
decoder does not have to parse the input in a complex way, nor does it have to search
the dictionary to ﬁnd matches. Many programmers like that, too.
I love the dictionary, Kenny, it’s the only book with the words in
the right place.
—Paul Reynolds as Colin Mathews in Press Gang (1989)
3.1 LZ78
The LZ78 method (sometimes also referred to as LZ2) [Ziv and Lempel 78] does not
employ any search buﬀer, look-ahead buﬀer, or sliding window.
Instead, there is a
dictionary of previously-encountered strings. This dictionary starts empty (or almost
empty), and its size is limited only by the amount of available memory. The encoder
outputs two-ﬁeld tokens. The ﬁrst ﬁeld is a pointer to the dictionary; the second is the
code of a symbol. Tokens do not contain the length of a string, because this is implied in
the dictionary. Each token corresponds to a string of input symbols, and that string is
added to the dictionary after the token is written on the compressed ﬁle. Nothing is ever
deleted from the dictionary, which is both an advantage over LZ77 (since future strings
can be compressed even by strings seen in the distant past) and a liability (because the
dictionary tends to grow rapidly and to ﬁll up the entire available memory).
The dictionary starts with the null string at position zero. As symbols are input
and encoded, strings are added to the dictionary at positions 1, 2, and so on. When
the next symbol x is read from the input, the dictionary is searched for an entry with
the one-symbol string x. If none is found, x is added to the next available position in
the dictionary, and the token (0, x) is output. This token indicates the string “null x”
(a concatenation of the null string and x). If an entry with x is found (at, say, position
37), the next symbol y is read, and the dictionary is searched for an entry containing
the two-symbol string xy. If none is found, then string xy is added to the next available
position in the dictionary, and the token (37, y) is output. This token indicates the
string xy, since 37 is the dictionary position of string x. The process continues until the
end of the input is reached.

96
3.
Dictionary Methods
In general, the current symbol is read and becomes a one-symbol string.
The
encoder then tries to ﬁnd it in the dictionary. If the symbol is found in the dictionary,
the next symbol is read and is concatenated with the ﬁrst to form a two-symbol string
that the encoder then tries to locate in the dictionary. As long as those strings are found
in the dictionary, more symbols are read and concatenated to the string. At a certain
point the string is not found in the dictionary, so the encoder adds it to the dictionary and
outputs a token with the last dictionary match as its ﬁrst ﬁeld, and the last symbol of the
string (the one that caused the search to fail) as its second ﬁeld. Table 3.2 lists the ﬁrst
14 steps in encoding the string sir␣sid␣eastman␣easily␣teases␣sea␣sick␣seals.
Dictionary
Token
Dictionary
Token
0
null
1
s
(0,s)
8
a
(0,a)
2
i
(0,i)
9
st
(1,t)
3
r
(0,r)
10
m
(0,m)
4
␣
(0,␣)
11
an
(8,n)
5
si
(1,i)
12
␣ea
(7,a)
6
d
(0,d)
13
sil
(5,l)
7
␣e
(4,e)
14
y
(0,y)
Table 3.2: First 14 Encoding Steps in LZ78.
⋄Exercise 3.2: Complete Table 3.2.
In each step, the string added to the dictionary is the one that is being encoded,
minus its last symbol. In a typical compression run, the dictionary starts with short
strings, but as more text is being input and processed, longer and longer strings are
added to it. The size of the dictionary can either be ﬁxed or may be determined by the
size of the available memory each time the LZ78 compression program is executed. A
large dictionary may contain more strings and thus allow for longer matches, but the
trade-oﬀis longer pointers (which implies longer tokens) and slower dictionary search.
A good data structure for the dictionary is a tree, but not a binary tree. The tree
starts with the null string as the root. All the strings that start with the null string
(strings for which the token pointer is zero) are added to the tree as children of the root.
In the example above those are s, i, r, ␣, d, a, m, y, e, c, and k. Each of them becomes
the root of a subtree as shown in Figure 3.3. For example, all the strings that start with
s (the four strings si, sil, st, and s(eof)) constitute the subtree of node s.
Assuming an alphabet of 8-bit symbols, there are 256 diﬀerent symbols, so in prin-
ciple, each node in the tree could have up to 256 children. The process of adding a child
to a tree node should therefore be dynamic. When the node is ﬁrst created, it has no
children and it should not reserve any memory space for them. As a child is added to
the node, memory space should be claimed for it. Recall that no nodes are ever deleted,
so there is no need to reclaim memory space, which simpliﬁes the memory management
task somewhat.
Such a tree makes it easy to search for a string and to add strings. To search for
sil, for example, the program looks for the child s of the root, then for the child i of

3.1 LZ78
97
8-a
25-l 11-n 17-s
22-c
16-e
20-a
18-s
6-d
23-k
2-i
10-m 3-r 1-s 14-y
26-eof
5-i
9-t
13-l
4-␣
19-s
7-e
15-t
12-a
24-e
21-i
null
Figure 3.3: An LZ78 Dictionary Tree.
s, and so on, going down the tree. Here are some examples:
1. When the s of sid is input in step 5, the encoder ﬁnds node “1-s” in the tree as a
child of “null”. It then inputs the next symbol i, but node s does not have a child i (in
fact, it has no children at all at this point), so the encoder adds node “5-i” as a child
of “1-s”, which eﬀectively adds the string si to the tree.
2. When the blank space between eastman and easily is input in step 12, a similar
situation occurs. The encoder ﬁnds node “4-␣”, inputs e, ﬁnds “7-e”, inputs a, but
“7-e” does not have “a” as a child, so the encoder adds node “12-a”, which eﬀectively
adds the string “␣ea” to the tree.
A tree of the type described here is called a trie (pronounced try). In general, a
trie is a tree in which the branching structure at any level is determined by just part
of a data item, not the entire item. In the case of LZ78, each string added to the tree
eﬀectively adds just one symbol, and does that by adding a branch.
Since the total size of the tree is limited, it may ﬁll up during compression. This, in
fact, happens all the time except when the input is unusually small. The original LZ78
method does not specify what to do in such a case, so we list a few possible solutions.
1. The simplest solution is to freeze the dictionary at that point. No new nodes should
be added, the tree becomes a static dictionary, but it can still be used to encode strings.
2. Delete the entire tree once it gets full and start with a new, empty tree. This solution
eﬀectively breaks the input into blocks, each with its own dictionary. If the content of
the input varies from block to block, this solution will produce good compression, since
it will eliminate a dictionary with strings that are unlikely to be used in the future. We
can say that this solution implicitly assumes that future symbols will beneﬁt more from
new data than from old (the same implicit assumption used by LZ77).
3. The UNIX compress utility uses a more complex solution.
4. When the dictionary is full, delete some of the least-recently-used entries, to make
room for new ones. Unfortunately, there is no simple, fast algorithm to decide which
entries to delete, and how many.
The LZ78 decoder works by building and maintaining the dictionary in the same
way as the encoder. It is therefore more complex than the LZ77 decoder.

98
3.
Dictionary Methods
The LZW Trio. Having one’s name attached to a scientiﬁc discovery, technique,
or phenomenon is considered a special honor in science. Having one’s name associated
with an entire ﬁeld of science is even more so. This is what happened to Jacob Ziv
and Abraham Lempel. In the 1970s these two researchers developed the ﬁrst methods,
LZ77 and LZ78, for dictionary-based compression. Their ideas have been a source of
inspiration to many researchers, who generalized, improved, and combined them with
RLE and statistical methods to form many popular lossless compression methods for
text, images, and audio. More than a dozen such methods are described in detail in
[Salomon 07].
Of special interest is the popular LZW algorithm, partly devised by
Terry Welch (Section 3.2), which has extended LZ78 and made it useful in practical
applications.
Abraham Lempel and Jacob Ziv.
Gamblers like the phrase “heads I win, tails I lose” (if you hear this, make sure you
did not hear “heads I win, tails you lose”). Mr G. Ambler, a veteran gambler, decided to
try a simple scheme, one where even he could easily ﬁgure out his winnings and losses.
The principle is to always gamble half the money he has in his pocket. Thus, if he starts
with an amount a and wins, he ends up with 1.5a. If next he loses, he pays out half that
and is left with 0.75a. Assuming that he plays g games and wins half the time, what is
his chance of making a net proﬁt?
3.2 LZW
LZW is a popular variant of LZ78, developed by Terry Welch in 1984 ([Welch 84] and
[Phillips 92]). Its main feature is eliminating the second ﬁeld of a token. An LZW token
consists of just a pointer to the dictionary. To best understand LZW, we will temporarily
forget that the dictionary is a tree, and will think of it as an array of variable-size strings.
The LZW method starts by initializing the dictionary to all the symbols in the alphabet.
In the common case of 8-bit symbols, the ﬁrst 256 entries of the dictionary (entries 0
through 255) are occupied before any data is input. Because the dictionary is initialized,
the next input character will always be found in the dictionary. This is why an LZW
token can consist of just a pointer and does not have to contain a character code as in
LZ77 and LZ78.

3.2 LZW
99
(LZW was patented and for many years its use required a license. This issue is
discussed in [Salomon 07] as well as in many places on the Internet.)
The principle of LZW is that the encoder inputs symbols one by one and accu-
mulates them in a string I. After each symbol is input and is concatenated to I, the
dictionary is searched for string I. As long as I is found in the dictionary, the process
continues. At a certain point, appending the next symbol x causes the search to fail;
string I is in the dictionary but string Ix (symbol x concatenated to I) is not. At this
point the encoder (1) outputs the dictionary pointer that points to string I, (2) saves
string Ix (which is now called a phrase) in the next available dictionary entry, and (3)
initializes string I to symbol x. To illustrate this process, we again use the text string
sir␣sid␣eastman␣easily␣teases␣sea␣sick␣seals. The steps are as follows:
0. Initialize entries 0–255 of the dictionary to all 256 8-bit bytes.
1. The ﬁrst symbol s is input and is found in the dictionary (in entry 115, since this is
the ASCII code of s). The next symbol i is input, but si is not found in the dictionary.
The encoder performs the following: (1) outputs 115, (2) saves string si in the next
available dictionary entry (entry 256), and (3) initializes I to the symbol i.
2. The r of sir is input, but string ir is not in the dictionary. The encoder (1) outputs
105 (the ASCII code of i), (2) saves string ir in the next available dictionary entry
(entry 257), and (3) initializes I to the symbol r.
Table 3.4 summarizes all the steps of this process. Table 3.5 shows some of the
original 256 entries in the LZW dictionary plus the entries added during encoding of the
string above. The complete output ﬁle is (only the numbers are output, not the strings
in parentheses) as follows:
115 (s), 105 (i), 114 (r), 32 (␣), 256 (si), 100 (d), 32 (␣), 101 (e), 97 (a), 115 (s), 116
(t), 109 (m), 97 (a), 110 (n), 262 (␣e), 264 (as), 105 (i), 108 (l), 121 (y), 32 (␣), 116
(t), 263 (ea), 115 (s), 101 (e), 115 (s), 259 (␣s), 263 (ea), 259 (␣s), 105 (i), 99 (c), 107
(k), 280 (␣se), 97 (a), 108 (l), 115 (s), eof.
Figure 3.6 is a pseudo-code listing of the algorithm. We denote by λ the empty
string, and by <<a,b>> the concatenation of strings a and b.
The line “append <<di,ch>> to the dictionary” is of special interest. It is clear
that in practice, the dictionary may ﬁll up. This line should therefore include a test for
a full dictionary, and certain actions for the case where it is full.
Since the ﬁrst 256 entries of the dictionary are occupied right from the start, pointers
to the dictionary have to be longer than 8 bits. A simple implementation would typically
use 16-bit pointers, which allow for a 64 K-entry dictionary (where 64 K = 216 = 65,536).
Naturally, such a dictionary will ﬁll up very quickly in all but the smallest compression
jobs. The same problem exists with LZ78, and any solutions used with LZ78 can also
be used with LZW. Another interesting fact about LZW is that strings in the dictionary
become only one character longer at a time. It therefore takes a long time to end up with
long strings in the dictionary, and thus with a chance to achieve really good compression.
We say that LZW adapts slowly to its input data.
⋄Exercise 3.3: Use LZW to encode the string alf␣eats␣alfalfa. Show the encoder
output and the new entries appended to the dictionary.

100
3.
Dictionary Methods
In
New
In
New
I
dict?
entry
Output
I
dict?
entry
Output
s
Y
y
Y
si
N
256-si
115 (s)
y
N
274-y
121 (y)
i
Y
␣
Y
ir
N
257-ir
105 (i)
␣t
N
275-␣t
32 (␣)
r
Y
t
Y
r
N
258-r
114 (r)
te
N
276-te
116 (t)
␣
Y
e
Y
␣s
N
259-␣s
32 (␣)
ea
Y
s
Y
eas
N
277-eas
263 (ea)
si
Y
s
Y
sid
N
260-sid
256 (si)
se
N
278-se
115 (s)
d
Y
e
Y
d
N
261-d
100 (d)
es
N
279-es
101 (e)
␣
Y
s
Y
␣e
N
262-␣e
32 (␣)
s
N
280-s
115 (s)
e
Y
␣
Y
ea
N
263-ea
101 (e)
␣s
Y
a
Y
␣se
N
281-␣se
259 (␣s)
as
N
264-as
97 (a)
e
Y
s
Y
ea
Y
st
N
265-st
115 (s)
ea
N
282-ea
263 (ea)
t
Y
␣
Y
tm
N
266-tm
116 (t)
␣s
Y
m
Y
␣si
N
283-␣si
259 (␣s)
ma
N
267-ma
109 (m)
i
Y
a
Y
ic
N
284-ic
105 (i)
an
N
268-an
97 (a)
c
Y
n
Y
ck
N
285-ck
99 (c)
n
N
269-n
110 (n)
k
Y
␣
Y
k
N
286-k
107 (k)
␣e
Y
␣
Y
␣ea
N
270-␣ea
262 (␣e)
␣s
Y
a
Y
␣se
Y
as
Y
␣sea
N
287-␣sea
281 (␣se)
asi
N
271-asi
264 (as)
a
Y
i
Y
al
N
288-al
97 (a)
il
N
272-il
105 (i)
l
Y
l
Y
ls
N
289-ls
108 (l)
ly
N
273-ly
108 (l)
s
Y
s,eof
N
115 (s)
Table 3.4: Encoding sir sid eastman easily teases sea sick seals.

3.2 LZW
101
0 NULL
110
n
262
␣e
276
te
1
SOH
. . .
263
ea
277
eas
. . .
115
s
264
as
278
se
32
SP
116
t
265
st
279
es
. . .
. . .
266
tm
280
s
97
a
121
y
267
ma
281
␣se
98
b
. . .
268
an
282
ea
99
c
255 255
269
n
283
␣si
100
d
256
si
270
␣ea
284
ic
101
e
257
ir
271
asi
285
ck
. . .
258
r
272
il
286
k
107
k
259
␣s
273
ly
287
␣sea
108
l
260
sid
274
y
288
al
109
m
261
d
275
␣t
289
ls
Table 3.5: An LZW Dictionary.
for i:=0 to 255 do
append i as a 1-symbol string to the dictionary;
append λ to the dictionary;
di:=dictionary index of λ;
repeat
read(ch);
if <<di,ch>> is in the dictionary then
di:=dictionary index of <<di,ch>>;
else
output(di);
append <<di,ch>> to the dictionary;
di:=dictionary index of ch;
endif;
until end-of-input;
Figure 3.6: The LZW Algorithm.

102
3.
Dictionary Methods
⋄Exercise 3.4: Analyze the LZW compression of the string aaaa....
A dirty icon (anagram of “dictionary”)
3.2.1 LZW Decoding
To understand how the LZW decoder works, we recall the three steps the encoder
performs each time it writes something on the output.
They are (1) it outputs the
dictionary pointer that points to string I, (2) it saves string Ix in the next available
entry of the dictionary, and (3) it initializes string I to symbol x.
The decoder starts with the ﬁrst entries of its dictionary initialized to all the symbols
of the alphabet (normally 256 symbols).
It then reads its input (which consists of
pointers to the dictionary) and uses each pointer to retrieve uncompressed symbols from
its dictionary and write them on its output. It also builds its dictionary in the same way
as the encoder (this fact is usually expressed by saying that the encoder and decoder
are synchronized, or that they work in lockstep).
In the ﬁrst decoding step, the decoder inputs the ﬁrst pointer and uses it to retrieve
a dictionary item I. This is a string of symbols, and it is written on the decoder’s output.
String Ix needs to be saved in the dictionary, but symbol x is still unknown; it will be
the ﬁrst symbol in the next string retrieved from the dictionary.
In each decoding step after the ﬁrst, the decoder inputs the next pointer, retrieves
the next string J from the dictionary, writes it on the output, isolates its ﬁrst symbol x,
and saves string Ix in the next available dictionary entry (after checking to make sure
string Ix is not already in the dictionary). The decoder then moves J to I and is ready
for the next step.
In our “sir sid...” example, the ﬁrst pointer that’s input by the decoder is 115.
This corresponds to the string s, which is retrieved from the dictionary, gets stored in
I, and becomes the ﬁrst item written on the decoder’s output. The next pointer is 105,
so string i is retrieved into J and is also written on the output. J’s ﬁrst symbol is
concatenated with I, to form string si, which does not exist in the dictionary, and is
therefore added to it as entry 256. Variable J is moved to I, so I is now the string i.
The next pointer is 114, so string r is retrieved from the dictionary into J and is also
written on the output. J’s ﬁrst symbol is concatenated with I, to form string ir, which
does not exist in the dictionary, and is added to it as entry 257. Variable J is moved to
I, so I is now the string r. The next step reads pointer 32, writes ␣on the output, and
saves string r␣.
⋄Exercise 3.5: Decode the string alf␣eats␣alfalfa by using the encoding results from
Exercise 3.3.
⋄Exercise 3.6: Assume a two-symbol alphabet with the symbols a and b. Show the ﬁrst
few steps for encoding and decoding the string “ababab...”.
3.2.2 LZW Dictionary Structure
Up until now, we have assumed that the LZW dictionary is an array of variable-size
strings. It turns out that a trie is an ideal data structure for a practical implementation
of such a dictionary. The ﬁrst step in understanding such an implementation is to recall

3.2 LZW
103
how the encoder works. It inputs symbols and concatenates them into a variable I as
long as the string in I is found in the dictionary. At a certain point the encoder inputs
the ﬁrst symbol x, which causes the search to fail (string Ix is not in the dictionary).
It then adds Ix to the dictionary. This means that each string added to the dictionary
eﬀectively adds just one new symbol, x.
(Phrased another way; for each dictionary
string of more than one symbol, there exists a “parent” string in the dictionary that’s
one symbol shorter.)
A tree similar to the one used by LZ78 is therefore a good data structure, because
adding string Ix to such a tree is done by adding one node with x. The main problem
is that each node in the LZW tree may have many children (this is a multiway tree, not
a binary tree). Imagine the node for the letter a in entry 97. Initially it has no children,
but if the string ab is added to the tree, node 97 receives one child. Later, when, say,
the string ae is added, node 97 receives a second child, and so on. The data structure
for the tree should therefore be designed such that a node could have any number of
children, but without having to reserve any memory for them in advance.
One way of designing such a data structure is to house the tree in an array of nodes,
each a structure with two ﬁelds: a symbol and a pointer to the parent node. A node
has no pointers to any child nodes. Moving down the tree, from a node to one of its
children, is done by a hashing process in which the pointer to the node and the symbol
of the child are hashed to create a new pointer.
Suppose that string abc has already been input, symbol by symbol, and has been
stored in the tree in the three nodes at locations 97, 266, and 284. Following that, the
encoder has just input the next symbol d. The encoder now searches for string abcd, or,
more speciﬁcally, for a node containing the symbol d whose parent is at location 284.
The encoder hashes the 284 (the pointer to string abc) and the 100 (ASCII code of d)
to create a pointer to some node, say, 299. The encoder then examines node 299. There
are three possibilities:
1. The node is unused. This means that abcd is not yet in the dictionary and should
be added to it. The encoder adds it to the tree by storing the parent pointer 284 and
ASCII code 100 in the node. The result is the following:
Node
Address
:
97
266
284
299
Contents
:
(-:a)
(97:b)
(266:c)
(284:d)
Represents:
a
ab
abc
abcd
2. The node contains a parent pointer of 284 and the ASCII code of d. This means
that string abcd is already in the tree. The encoder inputs the next symbol, say e, and
searches the dictionary tree for string abcde.
3. The node contains something else. This means that another hashing of a pointer
and an ASCII code has resulted in 299, and node 299 already contains information from
another string. This is called a collision, and it can be dealt with in several ways. The
simplest way to deal with a collision is to increment pointer 299 and examine nodes 300,
301, . . . until an unused node is found, or until a node with (284:d) is found.
In practice, we build nodes that are structures with three ﬁelds, a pointer to the
parent node, the pointer (or index) created by the hashing process, and the code (nor-

104
3.
Dictionary Methods
mally ASCII) of the symbol contained in the node. The second ﬁeld is necessary because
of collisions. A node can therefore be illustrated by
parent
index
symbol
We illustrate this data structure using string ababab...
of Exercise 3.6.
The
dictionary is an array dict where each entry is a structure with the three ﬁelds parent,
index, and symbol. We refer to a ﬁeld by, for example, dict[pointer].parent, where
pointer is an index to the array. The dictionary is initialized to the two entries a and
b. (To keep the example simple we use no ASCII codes. We assume that a has code 1
and b has code 2.) The ﬁrst few steps of the encoder are as follows:
Step 0: Mark all dictionary locations from 3 on as unused.
/
1
a
/
2
b
/
-
/
-
/
- . . .
Step 1: The ﬁrst symbol a is input into variable I. What is actually input is the code
of a, which in our example is 1, so I = 1. Since this is the ﬁrst symbol, the encoder
assumes that it is in the dictionary and so does not perform any search.
Step 2: The second symbol b is input into J, so J = 2. The encoder has to search
for string ab in the dictionary.
It executes pointer:=hash(I,J). Let’s assume that
the result is 5. Field dict[pointer].index contains “unused”, since location 5 is still
empty, so string ab is not in the dictionary. It is added by executing
dict[pointer].parent:=I;
dict[pointer].index:=pointer;
dict[pointer].symbol:=J;
with pointer = 5. J is moved into I, so I = 2.
/
1
a
/
2
b
/
-
/
-
1
5
b
. . .
Step 3: The third symbol a is input into J, so J = 1. The encoder has to search for string
ba in the dictionary. It executes pointer:=hash(I,J). Let’s assume that the result is
8. Field dict[pointer].index contains “unused”, so string ba is not in the dictionary.
It is added as before by executing
dict[pointer].parent:=I;
dict[pointer].index:=pointer;
dict[pointer].symbol:=J;
with pointer = 8. J is moved into I, so I = 1.
/
1
a
/
2
b
/
-
/
-
1
5
b
/
-
/
-
2
8
a
/
- . . .
Step 4: The fourth symbol b is input into J, so J=2. The encoder has to search for
string ab in the dictionary. It executes pointer:=hash(I,J). We know from step 2 that

3.2 LZW
105
the result is 5. Field dict[pointer].index contains 5, so string ab is in the dictionary.
The value of pointer is moved into I, so I = 5.
Step 5: The ﬁfth symbol a is input into J, so J = 1. The encoder has to search for string
aba in the dictionary. It executes as usual pointer:=hash(I,J). Let’s assume that the
result is 8 (a collision). Field dict[pointer].index contains 8, which looks good, but
ﬁeld dict[pointer].parent contains 2 instead of the expected 5, so the hash function
knows that this is a collision and string aba is not in dictionary entry 8. It increments
pointer as many times as necessary until it ﬁnds a dictionary entry with index = 8
and parent = 5 or until it ﬁnds an unused entry. In the former case, string aba is in the
dictionary, and pointer is moved to I. In the latter case aba is not in the dictionary,
and the encoder saves it in the entry pointed at by pointer, and moves J to I.
/
1
a
/
2
b
/
-
/
-
1
5
b
/
-
/
-
2
8
a
5
8
a
/
- . . .
Example: The 15 hashing steps for encoding the string alf eats alfalfa are
shown below. The encoding process itself is illustrated in detail in the answer to Exer-
cise 3.3. The results of the hashing are arbitrary; they are not the results produced by a
real hash function. The 12 trie nodes constructed for this string are shown in Figure 3.7.
1. Hash(l,97) →278. Array location 278 is set to (97, 278, l).
2. Hash(f,108) →266. Array location 266 is set to (108, 266, f).
3. Hash(␣,102) →269. Array location 269 is set to (102,269,␣).
4. Hash(e,32) →267. Array location 267 is set to (32, 267, e).
5. Hash(a,101) →265. Array location 265 is set to (101, 265, a).
6. Hash(t,97) →272. Array location 272 is set to (97, 272, t).
7. Hash(s,116) →265. A collision! Skip to the next available location, 268, and set it
to (116, 265, s). This is why the index needs to be stored.
8. Hash(␣,115) →270. Array location 270 is set to (115, 270, ␣).
9. Hash(a,32) →268. A collision! Skip to the next available location, 271, and set it to
(32, 268, a).
10. Hash(l,97) →278. Array location 278 already contains index 278 and symbol l
from step 1, so there is no need to store anything else or to add a new trie entry.
11. Hash(f,278) →276. Array location 276 is set to (278, 276, f).
12. Hash(a,102) →274. Array location 274 is set to (102, 274, a).
13. Hash(l,97) →278. Array location 278 already contains index 278 and symbol l
from step 1, so there is no need to do anything.
14. Hash(f,278) →276. Array location 276 already contains index 276 and symbol f
from step 11, so there is no need to do anything.
15. Hash(a,276) →274. A collision! Skip to the next available location, 275, and set it
to (276, 274, a).
Readers who have carefully followed the discussion up to this point will be happy to
learn that the LZW decoder’s use of the dictionary tree-array is simple and no hashing is
needed. The decoder starts, like the encoder, by initializing the ﬁrst 256 array locations.
It then reads pointers from its input and uses each to locate a symbol in the dictionary.
In the ﬁrst decoding step, the decoder inputs the ﬁrst pointer and uses it to retrieve
a dictionary item I. This is a symbol that is now written by the decoder on its output.

106
3.
Dictionary Methods
2
6
5
2
6
6
2
6
7
2
6
8
2
6
9
2
7
0
2
7
1
2
7
2
2
7
3
2
7
4
2
7
5
2
7
6
2
7
7
2
7
8
/
-
/
-
/
-
/
-
/
-
/
-
/
-
/
-
/
-
/
-
/
-
/
-
/
-
97
278
l
/
-
108
266
f
/
-
/
-
/
-
/
-
/
-
/
-
/
-
/
-
/
-
/
-
/
-
97
278
l
/
-
108
266
f
/
-
/
-
102
269
␣
/
-
/
-
/
-
/
-
/
-
/
-
/
-
/
-
97
278
l
/
-
108
266
f
32
267
e
/
-
102
269
␣
/
-
/
-
/
-
/
-
/
-
/
-
/
-
/
-
97
278
l
101
265
a
108
266
f
32
267
e
/
-
102
269
␣
/
-
/
-
/
-
/
-
/
-
/
-
/
-
/
-
97
278
l
101
265
a
108
266
f
32
267
e
/
-
102
269
␣
/
-
/
-
97
272
t
/
-
/
-
/
-
/
-
/
-
97
278
l
101
265
a
108
266
f
32
267
e
116
265
s
102
269
␣
/
-
/
-
97
272
t
/
-
/
-
/
-
/
-
/
-
97
278
l
101
265
a
108
266
f
32
267
e
116
265
s
102
269
␣
115
270
␣
/
-
97
272
t
/
-
/
-
/
-
/
-
/
-
97
278
l
101
265
a
108
266
f
32
267
e
116
265
s
102
269
␣
115
270
␣
32
268
a
97
272
t
/
-
/
-
/
-
/
-
/
-
97
278
l
101
265
a
108
266
f
32
267
e
116
265
s
102
269
␣
115
270
␣
32
268
a
97
272
t
/
-
/
-
/
-
278
276
f
/
-
97
278
l
101
265
a
108
266
f
32
267
e
116
265
s
102
269
␣
115
270
␣
32
268
a
97
272
t
/
-
102
274
a
/
-
278
276
f
/
-
97
278
l
101
265
a
108
266
f
32
267
e
116
265
s
102
269
␣
115
270
␣
32
268
a
97
272
t
/
-
102
274
a
276
274
a
278
276
f
/
-
97
278
l
Figure 3.7: Growing an LZW Trie for alf␣eats␣alfalfa.

3.2 LZW
107
String Ix needs to be saved in the dictionary, but symbol x is still unknown; it will be
the ﬁrst symbol in the next string retrieved from the dictionary.
In each decoding step after the ﬁrst, the decoder inputs the next pointer and uses
it to retrieve the next string J from the dictionary and write it on the output. If the
pointer is, say 8, the decoder examines ﬁeld dict[8].index. If this ﬁeld equals 8, then
this is the right node. Otherwise, the decoder examines consecutive array locations until
it ﬁnds the right one.
Once the right tree node is found, the parent ﬁeld is used to go back up the tree
and retrieve the individual symbols of the string in reverse order. The symbols are then
placed in J in the right order (see below), the decoder isolates the ﬁrst symbol x of J, and
saves string Ix in the next available array location. (String I was found in the previous
step, so only one node, with symbol x, needs be added.) The decoder then moves J to
I and is ready for the next step.
Retrieving a complete string from the LZW tree therefore involves following the
pointers in the parent ﬁelds. This is equivalent to moving up the tree, which is why the
hash function is no longer needed.
Example: The previous example describes the 15 hashing steps in the encoding
of string alf␣eats␣alfalfa. The last step sets array location 275 to (276,274,a) and
writes 275 (a pointer to location 275) on the compressed ﬁle. When this ﬁle is read
by the decoder, pointer 275 is the last item input and processed by the decoder. The
decoder ﬁnds symbol a in the symbol ﬁeld of location 275 (indicating that the string
stored at 275 ends with an a) and a pointer to location 276 in the parent ﬁeld. The
decoder then examines location 276 where it ﬁnds symbol f and parent pointer 278. In
location 278 the decoder ﬁnds symbol l and a pointer to 97. Finally, in location 97
the decoder ﬁnds symbol a and a null pointer. The (reversed) string is therefore afla.
There is no need for the decoder to do any hashing or to use the index ﬁelds.
The last point to discuss is string reversal. Two common approaches are outlined
here:
1. Use a stack. A stack is a common data structure in modern computers. It is an array
in memory that is accessed at one end only. At any time, the item that was last pushed
into the stack will be the ﬁrst one to be popped out (last-in-ﬁrst-out, or LIFO). Symbols
retrieved from the dictionary are pushed into the stack. When the last one has been
retrieved and pushed, the stack is popped, symbol by symbol, into variable J. When the
stack is empty, the entire string has been reversed. This is a common way to reverse a
string.
2. Retrieve symbols from the dictionary and concatenate them into J from right to left.
When done, the string will be stored in J in the right order. Variable J must be long
enough to accommodate the longest possible string, but then it has to be long enough
even when a stack is used.
⋄Exercise 3.7: What is the longest string that can be retrieved from the LZW dictionary
during decoding?
Big Amy lives in London and works in a store on Oxford Street. In order to justify
her name, she is married to two men, neither of whom knows about the other. Thus, she
has to juggle her marital life carefully. Every day after work, she walks to the Oxford

108
3.
Dictionary Methods
Circus underground station and takes either the Victoria line (cyan) south, to Brixton,
where one husband lives, or the Bakerloo line (brown) north, to Maida Vale, where
the other husband lives. Being a loving, impartial, and also a careful wife, she tries to
balance her visits so as not to prefer any husband over the other. To this end she gets
oﬀwork and arrives at the underground station at random times. In spite of this, she
ﬁnds herself at Maida Vale much more often than at Brixton. What could be a reason
for such imbalance?
3.3 Deﬂate: Zip and Gzip
Deﬂate is a popular compression method that was originally used in the well-known Zip
and Gzip software and has since been adopted by many applications, the most important
of which are (1) the HTTP protocol ([RFC1945 96] and [RFC2616 99]), (2) the PPP
compression control protocol ([RFC1962 96] and [RFC1979 96]), (3) the PNG (Portable
Network Graphics) and MNG (Multiple-Image Network Graphics) graphics ﬁle formats
([PNG 03] and [MNG 03]), and (4) Adobe’s PDF (Portable Document File) [PDF 01].
Deﬂate was developed by Philip Katz as a part of the Zip ﬁle format and imple-
mented in his PKZIP software [PKWare 03].
Both the ZIP format and the Deﬂate
method are in the public domain, which allowed implementations such as Info-ZIP’s
Zip and Unzip (essentially, PKZIP and PKUNZIP clones) to appear on a number of
platforms. Deﬂate is described in [RFC1951 96].
Phillip W. Katz was born in 1962. He received a bachelor’s
degree in computer science from the University of Wisconsin at
Madison. Always interested in writing software, he started working
in 1984 as a programmer for Allen-Bradley Co.
developing pro-
grammable logic controllers for the industrial automation industry.
He later worked for Graysoft, another software company, in Milwau-
kee, Wisconsin. At about that time he became interested in data
compression and founded PKWare in 1987 to develop, implement,
and market software products such as PKarc and PKzip.
For a
while, the company was very successful selling the programs as shareware.
Always a loner, Katz suﬀered from personal and legal problems, started drinking
heavily, and died on April 14, 2000 from complications related to chronic alcoholism.
He was 37 years old.
After his death, PKWare was sold, in March 2001, to a group of investors. They
changed its management and the focus of its business.
PKWare currently targets
the corporate market, and emphasizes compression combined with encryption. Their
product line runs on a wide variety of platforms.
The most notable implementation of Deﬂate is zlib, a portable and free compression
library ([zlib 03] and [RFC1950 96]) by Jean-Loup Gailly and Mark Adler who designed
and implemented it to be free of patents and licensing requirements. This library (the
source code is available at [Deﬂate 03]) implements the ZLIB and GZIP ﬁle formats
([RFC1950 96] and [RFC1952 96]), which are at the core of most Deﬂate applications,
including the popular Gzip software.

3.3 Deﬂate: Zip and Gzip
109
Deﬂate is based on a variation of LZ77 combined with Huﬀman codes. We start
with a simple overview based on [Feldspar 03] and follow with a full description based
on [RFC1951 96].
The original LZ77 method (Section 1.3.1) tries to match the text in the look-ahead
buﬀer to strings already in the search buﬀer. In the example
search buﬀer
look-ahead
...old␣..the␣a..then...there...|the␣new......more
the look-ahead buﬀer starts with the string the␣, which can be matched to one of three
strings in the search buﬀer. The longest match has a length of 4. LZ77 writes tokens
on the compressed ﬁle, where each token is a triplet (oﬀset, length, next symbol). The
third component is needed in cases where no string has been matched (imagine having
che instead of the in the look-ahead buﬀer) but it is part of every token, which reduces
the performance of LZ77. The LZ77 algorithm variation used in Deﬂate eliminates the
third component and writes a pair (oﬀset, length) on the compressed ﬁle. When no
match is found, the unmatched character is written on the compressed ﬁle instead of a
token. Thus, the compressed data consists of three types of entities: literals (unmatched
characters), oﬀsets (termed “distances” in the Deﬂate literature), and lengths. Deﬂate
actually writes Huﬀman codes on the compressed ﬁle for these entities, and it uses two
code tables—one for literals and lengths and the other for distances. This makes sense
because the literals are normally bytes and are therefore in the interval [0, 255], and the
lengths are limited by Deﬂate to 258. The distances, however, can be large numbers
because Deﬂate allows for a search buﬀer of up to 32 Kbytes.
⋄Exercise 3.8: When no match is found, Deﬂate writes the unmatched character (in
raw format) on the compressed ﬁle instead of a token. Suggest an alternative.
When a pair (length, distance) is determined, the encoder searches the table of
literal/length codes to ﬁnd the code for the length. This code (we later use the term
“edoc” for it) is then replaced by a Huﬀman code that’s written on the compressed ﬁle.
The encoder then searches the table of distance codes for the code of the distance and
writes that code (a special preﬁx code with a ﬁxed, 5-bit preﬁx) on the compressed ﬁle.
The decoder knows when to expect a distance code, because it always follows a length
code.
The LZ77 variant used by Deﬂate defers the selection of a match in the following
way. Suppose that the two buﬀers contain
search buﬀer
look-ahead
...old␣..she␣needs..then...there...|the␣new......more input
The longest match is 3. Before selecting this match, the encoder saves the t from
the look-ahead buﬀer and starts a secondary match where it tries to match he␣new...
with the search buﬀer. If it ﬁnds a longer match, it outputs t as a literal, followed
by the longer match. There is also a 3-valued parameter that controls this secondary
match attempt. In the “normal” mode of this parameter, if the primary match was long
enough (longer than a preset parameter), the secondary match is reduced (it is up to the
implementor to decide how to reduce it). In the “high-compression” mode, the encoder

110
3.
Dictionary Methods
always performs a full secondary match, thereby improving compression but spending
more time on selecting a match. In the “fast” mode, the secondary match is omitted.
Deﬂate compresses an input data ﬁle in blocks, where each block is compressed
separately. Blocks can have diﬀerent lengths and the length of a block is determined by
the encoder based on the sizes of the various preﬁx codes used (their lengths are limited
to 15 bits) and by the memory available to the encoder (except that blocks in mode 1
are limited to 65,535 bytes of uncompressed data). The Deﬂate decoder must be able
to decode blocks of any size. Deﬂate oﬀers three modes of compression, and each block
can be in any mode. The modes are as follows:
1.
No compression.
This mode makes sense for ﬁles or parts of ﬁles that are
incompressible (i.e., random) or already compressed, or for cases where the compression
software is asked to segment a ﬁle without compression. A typical case is a user who
wants to archive an 8 Gb ﬁle but has only a DVD “burner.” The user may want to
segment the ﬁle into two 4 Gb segments without compression. Commercial compression
software based on Deﬂate can use this mode of operation to segment the ﬁle. This mode
uses no code tables. A block written on the compressed ﬁle in this mode starts with a
special header indicating mode 1, followed by the length LEN of the data, followed by
LEN bytes of literal data. Notice that the maximum value of LEN is 65,535.
2. Compression with ﬁxed code tables. Two code tables are built into the Deﬂate
encoder and decoder and are always used. This speeds up both compression and de-
compression and has the added advantage that the code tables don’t have to be written
on the compressed ﬁle. The compression performance, however, may suﬀer if the data
being compressed is statistically diﬀerent from the data used to set up the code tables.
Literals and match lengths are located in the ﬁrst table and are replaced by a code
(called “edoc”) that is, in turn, replaced by a preﬁx code that’s output to the com-
pressed ﬁle. Distances are located in the second table and are replaced by special preﬁx
codes that are output to the compressed ﬁle. A block written on the compressed ﬁle in
this mode starts with a special header indicating mode 2, followed by the compressed
data in the form of preﬁx codes for the literals and lengths, and special preﬁx codes for
the distances. The block ends with a single preﬁx code for end-of-block.
3. Compression with individual code tables generated by the encoder for the partic-
ular data that’s being compressed. A sophisticated Deﬂate encoder may gather statistics
about the data as it compresses blocks, and may be able to construct improved code
tables as it proceeds from block to block. There are two code tables, for literals/lengths
and for distances. They again have to be written on the output, and they are written
in compressed format. A block output by the encoder in this mode starts with a special
header, followed by (1) a compressed Huﬀman code table and (2) the two code tables,
each compressed by the Huﬀman codes that preceded them. This is followed by the
compressed data in the form of preﬁx codes for the literals, lengths, and distances, and
ends with a single code for end-of-block.
What is the next integer in the sequence (12, 6), (6, 3), (10, ?)?
3.3.1 The Details
Each block starts with a 3-bit header where the ﬁrst bit is 1 for the last block in the ﬁle
and 0 for all other blocks. The remaining two bits are 00, 01, or 10, indicating modes

3.3 Deﬂate: Zip and Gzip
111
1, 2, or 3, respectively. Notice that a block of compressed data does not always end on
a byte boundary. The information in the block is suﬃcient for the decoder to read all
the bits of the compressed block and recognize the end of the block. The 3-bit header
of the next block immediately follows the current block and may therefore be located at
any position in a byte on the compressed ﬁle.
The format of a block in mode 1 is as follows:
1. The 3-bit header 000 or 100.
2. The rest of the current byte is skipped, and the next four bytes contain LEN and
the one’s complement of LEN (as unsigned 16-bit numbers), where LEN is the number of
data bytes in the block. This is why the block size in this mode is limited to 65,535
bytes.
3. LEN data bytes.
The format of a block in mode 2 is diﬀerent:
1. The 3-bit header 001 or 101.
2. This is immediately followed by the ﬁxed preﬁx codes for literals/lengths and
the special preﬁx codes of the distances.
3. Code 256 (rather, its preﬁx code) designating the end of the block.
Extra
Extra
Extra
Code
bits
Lengths
Code
bits
Lengths
Code
bits
Lengths
257
0
3
267
1
15,16
277
4
67–82
258
0
4
268
1
17,18
278
4
83–98
259
0
5
269
2
19–22
279
4
99–114
260
0
6
270
2
23–26
280
4
115–130
261
0
7
271
2
27–30
281
5
131–162
262
0
8
272
2
31–34
282
5
163–194
263
0
9
273
3
35–42
283
5
195–226
264
0
10
274
3
43–50
284
5
227–257
265
1
11,12
275
3
51–58
285
0
258
266
1
13,14
276
3
59–66
Table 3.8: Literal/Length Edocs for Mode 2.
Edoc
Bits
Preﬁx codes
0–143
8
00110000–10111111
144–255
9
110010000–111111111
256–279
7
0000000–0010111
280–287
8
11000000–11000111
Table 3.9: Huﬀman Codes for Edocs in Mode 2.
Mode 2 uses two code tables: one for literals and lengths and the other for distances.
The codes of the ﬁrst table are not what is actually written on the compressed ﬁle, so in

112
3.
Dictionary Methods
order to remove ambiguity, the term “edoc” is used here to refer to them. Each edoc is
converted to a preﬁx code that’s output. The ﬁrst table allocates edocs 0 through 255
to the literals, edoc 256 to end-of-block, and edocs 257–285 to lengths. The latter 29
edocs are not enough to represent the 256 match lengths of 3 through 258, so extra bits
are appended to some of those edocs. Table 3.8 lists the 29 edocs, the extra bits, and
the lengths that they represent. What is actually written on the output is preﬁx codes
of the edocs (Table 3.9). Notice that edocs 286 and 287 are never created, so their preﬁx
codes are never used. We show later that Table 3.9 can be represented by the sequence
of code lengths
8, 8, . . . , 8



144
, 9, 9, . . . , 9



112
, 7, 7, . . . , 7



24
, 8, 8, . . . , 8



8
,
(3.1)
but any Deﬂate encoder and decoder include the entire table instead of just the sequence
of code lengths. There are edocs for match lengths of up to 258, so the look-ahead buﬀer
of a Deﬂate encoder can have a maximum size of 258, but can also be smaller.
Examples.
If a string of 10 symbols has been matched by the LZ77 algorithm,
Deﬂate prepares a pair (length, distance) where the match length 10 becomes edoc 264,
which is written as the 7-bit preﬁx code 0001000. A length of 12 becomes edoc 265
followed by the single bit 1. This is written as the 7-bit preﬁx code 0001010 followed by
1. A length of 20 is converted to edoc 269 followed by the two bits 01. This is written
as the nine bits 0001101|01. A length of 256 becomes edoc 284 followed by the ﬁve bits
11110. This is written as 11000101|11110. A match length of 258 is indicated by edoc
285 whose 8-bit preﬁx code is 11000110. The end-of-block edoc of 256 is written as seven
zero bits.
The 30 distance codes are listed in Table 3.10. They are special preﬁx codes with
ﬁxed-size 5-bit preﬁxes that are followed by extra bits in order to represent distances
in the interval [1, 32768]. The maximum size of the search buﬀer is therefore 32,768,
but it can be smaller. The table shows that a distance of 6 is represented by 00100|1, a
distance of 21 becomes the code 01000|101, and a distance of 8195 corresponds to code
11010|000000000010.
Extra
Extra
Extra
Code
bits
Distance
Code
bits
Distance
Code
bits
Distance
0
0
1
10
4
33–48
20
9
1025–1536
1
0
2
11
4
49–64
21
9
1537–2048
2
0
3
12
5
65–96
22
10
2049–3072
3
0
4
13
5
97–128
23
10
3073–4096
4
1
5,6
14
6
129–192
24
11
4097–6144
5
1
7,8
15
6
193–256
25
11
6145–8192
6
2
9–12
16
7
257–384
26
12
8193–12288
7
2
13–16
17
7
385–512
27
12
12289–16384
8
3
17–24
18
8
513–768
28
13
16385–24576
9
3
25–32
19
8
769–1024
29
13
24577–32768
Table 3.10: Thirty Preﬁx Distance Codes in Mode 2.

3.3 Deﬂate: Zip and Gzip
113
3.3.2 Format of Mode-3 Blocks
In mode 3, the encoder generates two preﬁx code tables, one for the literals/lengths and
the other for the distances. It uses the tables to encode the data that constitutes the
block. The encoder can generate the tables in any way. The idea is that a sophisticated
Deﬂate encoder may collect statistics as it inputs the data and compresses blocks. The
statistics are used to construct better code tables for later blocks.
A naive encoder
may use code tables similar to the ones of mode 2 or may even not generate mode 3
blocks at all. The code tables have to be written on the output, and they are written
in a highly-compressed format. As a result, an important part of Deﬂate is the way it
compresses the code tables and outputs them. The main steps are (1) Each table starts
as a Huﬀman tree. (2) The tree is rearranged to bring it to a standard format where it
can be represented by a sequence of code lengths. (3) The sequence is compressed by
run-length encoding to a shorter sequence. (4) The Huﬀman algorithm is applied to the
elements of the shorter sequence to assign them Huﬀman codes. This creates a Huﬀman
tree that is again rearranged to bring it to the standard format. (5) This standard tree
is represented by a sequence of code lengths which are written, after being permuted
and possibly truncated, on the output. These steps are described in detail because of
the originality of this unusual method.
Recall that the Huﬀman code tree generated by the basic algorithm of Section 2.1
is not unique. The Deﬂate encoder applies this algorithm to generate a Huﬀman code
tree, then rearranges the tree and reassigns the codes to bring the tree to a standard
form where it can be expressed compactly by a sequence of code lengths. (The result is
reminiscent of the canonical Huﬀman codes of Section 2.2.6.) The new tree satisﬁes the
following two properties:
1. The shorter codes appear on the left, and the longer codes appear on the right
of the Huﬀman code tree.
2. When several symbols have codes of the same length, the (lexicographically)
smaller symbols are placed on the left.
The ﬁrst example employs a set of six symbols A–F with probabilities 0.11, 0.14,
0.12, 0.13, 0.24, and 0.26, respectively. Applying the Huﬀman algorithm results in a
tree similar to the one shown in Figure 3.11a. The Huﬀman codes of the six symbols
are 000, 101, 001, 100, 01, and 11. The tree is then rearranged and the codes reassigned
to comply with the two requirements above, resulting in the tree of Figure 3.11b. The
new codes of the symbols are 100, 101, 110, 111, 00, and 01. The latter tree has the
advantage that it can be fully expressed by the sequence 3, 3, 3, 3, 2, 2 of the lengths of
the codes of the six symbols. The task of the encoder in mode 3 is therefore to generate
this sequence, compress it, and write it on the output.
The code lengths are limited to at most four bits each. Thus, they are integers in
the interval [0, 15], which implies that a code can be at most 15 bits long (this is one
factor that aﬀects the Deﬂate encoder’s choice of block lengths in mode 3).
The sequence of code lengths representing a Huﬀman tree tends to have runs of
identical values and can have several runs of the same value. For example, if we assign
the probabilities 0.26, 0.11, 0.14, 0.12, 0.24, and 0.13 to the set of six symbols A–F, the
Huﬀman algorithm produces 2-bit codes for A and E and 3-bit codes for the remaining
four symbols. The sequence of these code lengths is 2, 3, 3, 3, 2, 3.

114
3.
Dictionary Methods
A
B
C
D
E
F
E
C
F
B
A
D
(a)
(b)
000
100
101
110
111
101
001
0
0
0
0
0
1
1
1
1
1
100
01
11
01
00
0
0
0
0
0
1
1
1
1
1
Figure 3.11: Two Huﬀman Trees.
The decoder reads a compressed sequence, decompresses it, and uses it to reproduce
the standard Huﬀman code tree for the symbols. We ﬁrst show how such a sequence is
used by the decoder to generate a code table, then how it is compressed by the encoder.
Given the sequence 3, 3, 3, 3, 2, 2, the Deﬂate decoder proceeds in three steps as
follows:
1. Count the number of codes for each code length in the sequence. In our example,
there are no codes of length 1, two codes of length 2, and four codes of length 3.
2. Assign a base value to each code length. There are no codes of length 1, so
they are assigned a base value of 0 and don’t require any bits. The two codes of length
2 therefore start with the same base value 0.
The codes of length 3 are assigned a
base value of 4 (twice the number of codes of length 2). The C code shown here (after
[RFC1951 96]) was written by Peter Deutsch. It assumes that step 1 leaves the number
of codes for each code length n in bl_count[n].
code = 0;
bl_count[0] = 0;
for (bits = 1; bits <= MAX_BITS; bits++)
{ code = (code + bl_count[bits-1]) << 1;
next code[bits] = code;
}
3. Use the base value of each length to assign consecutive numerical values to all
the codes of that length. The two codes of length 2 start at 0 and are therefore 00 and
01. They are assigned to the ﬁfth and sixth symbols E and F. The four codes of length
3 start at 4 and are therefore 100, 101, 110, and 111. They are assigned to the ﬁrst four
symbols A–D. The C code shown here (by Peter Deutsch) assumes that the code lengths
are in tree[I].Len and it generates the codes in tree[I].Codes.
for (n = 0; n <= max code; n++)
{ len = tree[n].Len;
if (len != 0)
{ tree[n].Code = next_code[len];
next_code[len]++;
}
}

3.3 Deﬂate: Zip and Gzip
115
In the next example, the sequence 3, 3, 3, 3, 3, 2, 4, 4 is given and is used to
generate a table of eight preﬁx codes. Step 1 ﬁnds that there are no codes of length 1,
one code of length 2, ﬁve codes of length 3, and two codes of length 4. The length-1
codes are assigned a base value of 0. There are zero such codes, so the next group is also
assigned the base value of 0 (more accurately, twice 0, twice the number of codes of the
previous group). This group contains one code, so the next group (length-3 codes) is
assigned base value 2 (twice the sum 0 + 1). This group contains ﬁve codes, so the last
group is assigned base value of 14 (twice the sum 2 + 5). Step 3 simply generates the
ﬁve 3-bit codes 010, 011, 100, 101, and 110 and assigns them to the ﬁrst ﬁve symbols.
It then generates the single 2-bit code 00 and assigns it to the sixth symbol. Finally,
the two 4-bit codes 1110 and 1111 are generated and assigned to the last two (seventh
and eighth) symbols.
Given the sequence of code lengths of Equation (3.1), we apply this method to
generate its standard Huﬀman code tree (listed in Table 3.9).
Step 1 ﬁnds that there are no codes of lengths 1 through 6, that there are 24 codes
of length 7, 152 codes of length 8, and 112 codes of length 9. The length-7 codes are
assigned a base value of 0. There are 24 such codes, so the next group is assigned the
base value of 2(0 + 24) = 48. This group contains 152 codes, so the last group (length-9
codes) is assigned base value 2(48 + 152) = 400. Step 3 simply generates the 24 7-bit
codes 0 through 23, the 152 8-bit codes 48 through 199, and the 112 9-bit codes 400
through 511. The binary values of these codes are listed in Table 3.9.
How many a dispute could have been deﬂated into a single paragraph if the disputants
had dared to deﬁne their terms.
—Aristotle
It is now clear that a Huﬀman code table can be represented by a short sequence
(termed SQ) of code lengths (herein called CLs). This sequence is special in that it
tends to have runs of identical elements, so it can be highly compressed by run-length
encoding. The Deﬂate encoder compresses this sequence in a three-step process where
the ﬁrst step employs run-length encoding; the second step computes Huﬀman codes for
the run lengths and generates another sequence of code lengths (to be called CCLs) for
those Huﬀman codes. The third step writes a permuted, possibly truncated sequence of
the CCLs on the output.
Step 1. When a CL repeats more than three times, the encoder considers it a run.
It appends the CL to a new sequence (termed SSQ), followed by the special ﬂag 16
and by a 2-bit repetition factor that indicates 3–6 repetitions. A ﬂag of 16 is therefore
preceded by a CL and followed by a factor that indicates how many times to copy the
CL. Thus, for example, if the sequence to be compressed contains six consecutive 7’s, it is
compressed to 7, 16, 102 (the repetition factor 102 indicates ﬁve consecutive occurrences
of the same code length). If the sequence contains 10 consecutive code lengths of 6, it
will be compressed to 6, 16, 112, 16, 002 (the repetition factors 112 and 002 indicate six
and three consecutive occurrences, respectively, of the same code length).
Experience indicates that CLs of zero are very common and tend to have long runs.
(Recall that the codes in question are codes of literals/lengths and distances. Any given
data ﬁle to be compressed may be missing many literals, lengths, and distances.) This is
why runs of zeros are assigned the two special ﬂags 17 and 18. A ﬂag of 17 is followed by

116
3.
Dictionary Methods
a 3-bit repetition factor that indicates 3–10 repetitions of CL 0. Flag 18 is followed by a
7-bit repetition factor that indicates 11–138 repetitions of CL 0. Thus, six consecutive
zeros in a sequence of CLs are compressed to 17, 112, and 12 consecutive zeros in an SQ
are compressed to 18, 012.
The sequence of CLs is compressed in this way to a shorter sequence (to be termed
SSQ) of integers in the interval [0, 18]. An example may be the sequence of 28 CLs
4, 4, 4, 4, 4, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2
that’s compressed to the 16-number SSQ
4, 16, 012, 3, 3, 3, 6, 16, 112, 16, 002, 17, 112, 2, 16, 002,
or, in decimal,
4, 16, 1, 3, 3, 3, 6, 16, 3, 16, 0, 17, 3, 2, 16, 0.
Step 2. Prepare Huﬀman codes for the SSQ in order to compress it further. Our
example SSQ contains the following numbers (with their frequencies in parentheses):
0(2), 1(1), 2(1), 3(5), 4(1), 6(1), 16(4), 17(1). Its initial and standard Huﬀman trees
are shown in Figure 3.12a,b. The standard tree can be represented by the SSQ of eight
lengths 4, 5, 5, 1, 5, 5, 2, and 4. These are the lengths of the Huﬀman codes assigned
to the eight numbers 0, 1, 2, 3, 4, 6, 16, and 17, respectively.
Step 3. This SSQ of eight lengths is now extended to 19 numbers by inserting zeros
in the positions that correspond to unused CCLs.
Position:
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
CCL:
4
5
5
1
5
0
5
0
0
0
0
0
0
0
0
0
2
4
0
Next, the 19 CCLs are permuted according to
Position:
16
17
18
0
8
7
9
6
10
5
11
4
12
3
13
2
14
1
15
CCL:
2
4
0
4
0
0
0
5
0
0
0
5
0
1
0
5
0
5
0
The reason for the permutation is to end up with a sequence of 19 CCLs that’s likely to
have trailing zeros. The SSQ of 19 CCLs minus its trailing zeros is written on the output,
preceded by its actual length, which can be between 4 and 19. Each CCL is written
as a 3-bit number. In our example, there is just one trailing zero, so the 18-number
sequence 2, 4, 0, 4, 0, 0, 0, 5, 0, 0, 0, 5, 0, 1, 0, 5, 0, 5 is written on the output as the
ﬁnal, compressed code of one preﬁx-code table. In mode 3, each block of compressed
data requires two preﬁx-code tables, so two such sequences are written on the output.
(a)
(b)
0000
1100
00010 00011
1
0
00100 00101
11111
11110
11101
11100
0011
1101
01
10
0
0
1
1
2
2
3
3
4
6
4
6
16
16
16
17
17
Figure 3.12: Two Huﬀman Trees for Code Lengths.
A reader ﬁnally reaching this point (sweating profusely with such deep concentration
on so many details) may respond with the single word “insane.” This scheme of Phil

3.3 Deﬂate: Zip and Gzip
117
Katz for compressing the two preﬁx-code tables per block is devilishly complex and hard
to follow, but it works!
The format of a block in mode 3 is as follows:
1. The 3-bit header 010 or 110.
2. A 5-bit parameter HLIT indicating the number of codes in the literal/length code
table. This table has codes 0–256 for the literals, code 256 for end-of-block, and the
30 codes 257–286 for the lengths. Some of the 30 length codes may be missing, so this
parameter indicates how many of the length codes actually exist in the table.
3. A 5-bit parameter HDIST indicating the size of the code table for distances. There
are 30 codes in this table, but some may be missing.
4. A 4-bit parameter HCLEN indicating the number of CCLs (there may be between
4 and 19 CCLs).
5. A sequence of HCLEN + 4 CCLs, each a 3-bit number.
6. A sequence SQ of HLIT + 257 CLs for the literal/length code table. This SQ is
compressed as explained earlier.
7.
A sequence SQ of HDIST + 1 CLs for the distance code table.
This SQ is
compressed as explained earlier.
8. The compressed data, encoded with the two preﬁx-code tables.
9. The end-of-block code (the preﬁx code of edoc 256).
Each CCL is written on the output as a 3-bit number, but the CCLs are Huﬀman
codes of up to 19 symbols.
When the Huﬀman algorithm is applied to a set of 19
symbols, the resulting codes may be up to 18 bits long. It is the responsibility of the
encoder to ensure that each CCL is a 3-bit number and none exceeds 7. The formal
deﬁnition [RFC1951 96] of Deﬂate does not specify how this restriction on the CCLs is
to be achieved.
3.3.3 The Hash Table
This short section discusses the problem of locating a match in the search buﬀer. The
buﬀer is 32 Kb long, so a linear search is too slow. Searching linearly for a match to
any string requires an examination of the entire search buﬀer. If Deﬂate is to be able to
compress large data ﬁles in reasonable time, it should use a sophisticated search method.
The method proposed by the Deﬂate standard is based on a hash table. This method is
strongly recommended by the standard, but is not required. An encoder using a diﬀerent
search method is still compliant and can call itself a Deﬂate encoder. Those unfamiliar
with hash tables should consult any text on data structures.
If it wasn’t for faith, there would be no living in this world; we couldn’t even eat hash
with any safety.
—Josh Billings
Instead of separate look-ahead and search buﬀers, the encoder should have a single,
32 Kb buﬀer. The buﬀer is ﬁlled up with input data and initially all of it is a look-ahead
buﬀer. In the original LZ77 method, once symbols have been examined, they are moved
into the search buﬀer. The Deﬂate encoder, in contrast, does not move the data in its
buﬀer and instead moves a pointer (or a separator) from left to right, to indicate the
boundary between the look-ahead and search buﬀers. Short, 3-symbol strings from the
look-ahead buﬀer are hashed and added to the hash table. After hashing a string, the

118
3.
Dictionary Methods
encoder examines the hash table for matches. Assuming that a symbol occupies n bits,
a string of three symbols can have values in the interval [0, 23n −1]. If 23n −1 isn’t
too large, the hash function can return values in this interval, which tends to minimize
the number of collisions. Otherwise, the hash function can return values in a smaller
interval, such as 32 Kb (the size of the Deﬂate buﬀer).
We demonstrate the principles of Deﬂate hashing with the 17-symbol string
abbaabbaabaabaaaa
1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7
Initially, the entire 17-location buﬀer is the look-ahead buﬀer and the hash table is
empty
0
1
2
3
4
5
6
7
8
0 0 0 0 0 0 0 0 ...
We assume that the ﬁrst triplet abb hashes to 7. The encoder outputs the raw
symbol a, moves this symbol to the search buﬀer (by moving the separator between the
two buﬀers to the right), and sets cell 7 of the hash table to 1.
a|bbaabbaabaabaaaa
1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7
0
1
2
3
4
5
6
7
8
0 0 0 0 0 0 0 1 ...
The next three steps hash the strings bba, baa, and aab to, say, 1, 5, and 0. The
encoder outputs the three raw symbols b, b, and a, moves the separator, and updates
the hash table as follows:
abba|abbaabaabaaaa
1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7
0
1
2
3
4
5
6
7
8
4 2 0 0 0 3 0 1 ...
Next, the triplet abb is hashed, and we already know that it hashes to 7. The
encoder ﬁnds 1 in cell 7 of the hash table, so it looks for a string that starts with abb at
position 1 of its buﬀer. It ﬁnds a match of size 6, so it outputs the pair (5 −1, 6). The
oﬀset (4) is the diﬀerence between the start of the current string (5) and the start of
the matching string (1). There are now two strings that start with abb, so cell 7 should
point to both. It therefore becomes the start of a linked list (or chain) whose data items
are 5 and 1. Notice that the 5 precedes the 1 in this chain, so that later searches of
the chain will ﬁnd the 5 ﬁrst and will therefore tend to ﬁnd matches with the smallest
oﬀset, because those have short Huﬀman codes.
abbaa|bbaabaabaaaa
1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7
0
1
2
3
4
5
6
7
8
4 2 0 0 0 3 0 ↓...
5 →1 0
Six symbols have been matched at position 5, so the next position to consider is
6 + 5 = 11. While moving to position 11, the encoder hashes the ﬁve 3-symbol strings it
ﬁnds along the way (those that start at positions 6 through 10). They are bba, baa, aab,
aba, and baa. They hash to 1, 5, 0, 3, and 5 (we arbitrarily assume that aba hashes to
3). Cell 3 of the hash table is set to 9, and cells 0, 1, and 5 become the starts of linked
chains.
abbaabbaab|aabaaaa
1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7
0
1
2
3
4
5
6
7
8
↓↓0 9 0 ↓0 ↓...
. . .
. . .
5 →1 0
Continuing from position 11, string aab hashes to 0. Following the chain from cell
0, we ﬁnd matches at positions 4 and 8. The latter match is longer and matches the
5-symbol string aabaa. The encoder outputs the pair (11 −8, 5) and moves to position

Chapter Summary
119
11 + 5 = 16. While doing so, it also hashes the 3-symbol strings that start at positions
12, 13, 14, and 15. Each hash value is added to the hash table. (End of example.)
It is clear that the chains can become very long. An example is an image ﬁle with
large uniform areas where many 3-symbol strings will be identical, will hash to the same
value, and will be added to the same cell in the hash table. Since a chain must be
searched linearly, a long chain defeats the purpose of a hash table. This is why Deﬂate
has a parameter that limits the size of a chain. If a chain exceeds this size, its oldest
elements should be truncated. The Deﬂate standard does not specify how this should
be done and leaves it to the discretion of the implementor. Limiting the size of a chain
reduces the compression quality but can reduce the compression time signiﬁcantly. In
situations where compression time is unimportant, the user can specify long chains.
Also, selecting the longest match may not always be the best strategy; the oﬀset
should also be taken into account. A 3-symbol match with a small oﬀset may eventually
use fewer bits (once the oﬀset is replaced with a variable-length code) than a 4-symbol
match with a large oﬀset.
⋄Exercise 3.9: Hashing 3-byte sequences prevents the encoder from ﬁnding matches of
length 1 and 2 bytes. Is this a serious limitation?
3.3.4 Conclusions
Deﬂate is a general-purpose lossless compression algorithm that has proved valuable over
the years as part of several popular compression programs. The method requires memory
for the look-ahead and search buﬀers and for the two preﬁx-code tables. However, the
memory size needed by the encoder and decoder is independent of the size of the data or
the blocks. The implementation is not trivial, but is simpler than that of some modern
methods such as JPEG 2000 or MPEG. Compression algorithms that are geared for
speciﬁc types of data, such as audio or video, may perform better than Deﬂate on such
data, but Deﬂate normally produces compression factors of 2.5 to 3 on text, slightly
smaller for executable ﬁles, and somewhat bigger for images. Most important, even in
the worst case, Deﬂate expands the data by only 5 bytes per 32 Kb block. Finally, free
implementations that avoid patents are available. Notice that the original method, as
designed by Phil Katz, has been patented (United States patent 5,051,745, September
24, 1991) and assigned to PKWARE.
Chapter Summary
The Huﬀman algorithm is based on the probabilities of the individual data symbols,
which is why it is considered a statistical compression method. Dictionary-based com-
pression methods are diﬀerent. They do not compute or estimate symbol probabilities
and they do not use a statistical model of the data. They are based on the fact that the
data ﬁles that are of interest to us, the ﬁles we want to compress and keep for later use,
are not random. A typical data ﬁle features redundancies in the form of patterns and
repetitions of data symbols.
A dictionary-based compression method selects strings of symbols from the input
and employs a dictionary to encode each string as a token. The dictionary consists of

120
3.
Dictionary Methods
strings of symbols, and it may be static or dynamic (adaptive). The former type is
permanent, sometimes allowing the addition of strings but no deletions, whereas the
latter type holds strings previously found in the input, thereby allowing for additions
and deletions of strings as new input is being read.
If the data features many repetitions, then many input strings will match strings
in the dictionary. A matched string is replaced by a token, and compression is achieved
if the token is shorter than the matched string. If the next input symbols is not found
in the dictionary, then it is output in raw form and is also added to the dictionary.
The following points are especially important: (1) Any dictionary-based method must
write the raw items and tokens on the output such that the decoder will be able to
distinguish them. (2) Also, the capacity of the dictionary is ﬁnite and any particular
algorithm must have explicit rules specifying what to do when the (adaptive) dictionary
ﬁlls up. Many dictionary-based methods have been developed over the years, and these
two points constitute the main diﬀerences between them.
This book describes the following dictionary-based compression methods. The LZ77
algorithm (Section 1.3.1) is simple but not very eﬃcient because its output tokens are
triplets and are therefore large. The LZ78 method (Section 3.1) generates tokens that
are pairs, and the LZW algorithm (Section 3.2) output single-item tokens. The Deﬂate
algorithm (Section 3.3), which lies at the heart of the various zip implementations, is
more sophisticated.
It employs several types of blocks and a hash table, for a very
eﬀective compression.
Self-Assessment Questions
1. Redo Exercise 3.1 for various values of P (the probability of a match).
2.
Study the topic of patents in data compression.
A good starting point is
[patents 07].
3. Test your knowledge of the LZW algorithm by manually encoding several short
strings, similar to Exercise 3.3.
Words—so innocent and powerless as they are, as standing in a
dictionary, how potent for good and evil they become
in the hands of one who knows how to combine them.
—Nathaniel Hawthorne

Part II:
Advanced Techniques
The second part of this book is concerned with advanced techniques. The original
and unusual technique of arithmetic coding is the topic of Chapter 4.
Chapter 5 is
devoted to image compression. It starts with the chief approaches to the compression
of images, explains orthogonal transforms, and discusses the JPEG algorithm, perhaps
the best example of the use of these transforms. The second part of this chapter intro-
duces the wavelet transform. It illustrates this transform and its advantages for image
compression. It explains the diﬀerences between orthogonal and subband transforms,
and it presents the WSQ method for ﬁngerprint compression as an example of the ap-
plication of a wavelet transform. Chapter 6 is devoted to the compression of audio data
and in particular to the technique of linear prediction. Finally, other approaches to
compression—such as the Burrows–Wheeler method, symbol ranking, and SCSU and
BOCU-1—are given their due in Chapter 7.
Great dancers aren’t great because of their
technique; they are great because of their passion.
—Unknown

4
Arithmetic Coding
The Huﬀman algorithm is simple, eﬃcient, and produces the best codes for the individual
data symbols. The discussion in Chapter 2 however, shows that the only case where
it produces ideal variable-length codes (codes whose average size equals the entropy) is
when the symbols have probabilities of occurrence that are negative powers of 2 (i.e.,
numbers such as 1/2, 1/4, or 1/8). This is because the Huﬀman method assigns a code
with an integral number of bits to each symbol in the alphabet. Information theory tells
us that a symbol with probability 0.4 should ideally be assigned a 1.32-bit code, because
−log2 0.4 ≈1.32. The Huﬀman method, however, normally assigns such a symbol a
code of one or two bits.
Arithmetic coding overcomes the problem of assigning integer codes to the individ-
ual symbols by assigning one (normally long) code to the entire input ﬁle. The method
starts with a certain interval, it reads the input ﬁle symbol by symbol, and employs
the probability of each symbol to narrow the interval. Specifying a narrower interval
requires more bits, as illustrated in the next paragraph.
Thus, the narrow intervals
constructed by the algorithm require longer and longer numbers to specify their bound-
aries. To achieve compression, the algorithm is designed such that a high-probability
symbol narrows the interval less than a low-probability symbol, with the result that
high-probability symbols contribute fewer bits to the output.
An interval can be speciﬁed by its lower and upper limits or by one limit and the
width. We use the latter method to illustrate how an interval’s speciﬁcation becomes
longer as the interval narrows.
The interval [0, 1] can be speciﬁed by the two 1-bit
numbers 0 and 1. The interval [0.1, 0.512] can be speciﬁed by the longer numbers 0.1
and 0.512. The very narrow interval [0.12575, 0.1257586] is speciﬁed by the long numbers
0.12575 and 0.0000086.

124
4.
Arithmetic Coding
The output of arithmetic coding is interpreted as a number in the range [0, 1). (The
notation [a, b) means the range of real numbers from a to b, including a but not including
b. The range is “closed” at a and “open” at b.) Thus, the code 9746509 is interpreted
as 0.9746509, although the 0. part is not included in the output ﬁle.
Before we plunge into the details, here is a bit of history. The principle of arithmetic
coding was ﬁrst proposed by Peter Elias in the early 1960s. Early practical implemen-
tations of this method were developed by several researchers in the 1970s. Of special
mention are [Moﬀat et al. 98] and [Witten et al. 87]. They discuss both the principles
and details of practical arithmetic coding and include examples.
4.1 The Basic Idea
The ﬁrst step is to compute, or at least to estimate, the frequencies of occurrence of
each input symbol. For best results, the precise frequencies are computed by reading the
entire input ﬁle in the ﬁrst pass of a two-pass compression job. However, if the program
can get good estimates of the frequencies from a diﬀerent source, the ﬁrst pass may be
omitted.
The ﬁrst example involves the three symbols a1, a2, and a3, with probabilities
P1 = 0.4, P2 = 0.5, and P3 = 0.1, respectively. The interval [0, 1) is divided among the
three symbols by assigning each a subinterval proportional in size to its probability. The
order of the subintervals is unimportant. In our example, the three symbols are assigned
the subintervals [0, 0.4), [0.4, 0.9), and [0.9, 1.0). To encode the string a2a2a2a3, we start
with the interval [0, 1). The ﬁrst symbol a2 reduces this interval to the subinterval from
its 40% point to its 90% point. The result is [0.4, 0.9). The second a2 reduces [0.4, 0.9) in
the same way (see note below) to [0.6, 0.85). The third a2 reduces this to [0.7, 0.825) and
the a3 reduces this to the stretch from the 90% point of [0.7, 0.825) to its 100% point,
producing [0.8125, 0.8250). The ﬁnal code our method produces can be any number in
this ﬁnal range.
Notice that the subinterval [0.6, 0.85) is obtained from the interval [0.4, 0.9) by
0.4 + (0.9 −0.4) × 0.4 = 0.6 and 0.4 + (0.9 −0.4) × 0.9 = 0.85.
With this example in mind, it should be easy to understand the following rules,
which summarize the main steps of arithmetic coding:
1. Start by deﬁning the current interval as [0, 1).
2. Repeat the following two steps for each symbol s in the input:
2.1. Divide the current interval into subintervals whose sizes are proportional to
the symbols’ probabilities.
2.2. Select the subinterval for s and deﬁne it as the new current interval.
3. When the entire input has been processed in this way, the output should be any
number that uniquely identiﬁes the current interval (i.e., any number inside the current
interval).
For each symbol processed, the current interval gets smaller, so it takes more bits to
express it, but the point is that the ﬁnal output is a single number and does not consist
of codes for the individual symbols. The average code size can be obtained by dividing
the size of the output (in bits) by the size of the input (in symbols). Notice also that

4.1 The Basic Idea
125
the probabilities used in step 2.1 may change all the time, since they may be supplied
by an adaptive probability model (Section 4.5).
A theory has only the alternative of being right or wrong. A model
has a third possibility: it may be right, but irrelevant.
—Eigen Manfred, The Physicist’s Conception of Nature
The next example is a bit more complex. We show the compression steps for the
short string SWISS␣MISS. Table 4.1 shows the information prepared in the ﬁrst step (the
statistical model of the data). The ﬁve symbols appearing in the input may be arranged
in any order. The number of occurrences of each symbol is counted and is divided by the
string size, 10, to determine the symbol’s probability. The range [0, 1) is then divided
among the symbols, in any order, with each symbol receiving a subinterval equal in size
to its probability. Thus, S receives the subinterval [0.5, 1.0) (of size 0.5), whereas the
subinterval of I is of size 0.2 [0.2, 0.4). The cumulative frequencies column is used by
the decoding algorithm on page 130.
Char
Freq
Prob.
Range
CumFreq
Total CumFreq=
10
S
5
5/10 = 0.5
[0.5, 1.0)
5
W
1
1/10 = 0.1
[0.4, 0.5)
4
I
2
2/10 = 0.2
[0.2, 0.4)
2
M
1
1/10 = 0.1
[0.1, 0.2)
1
␣
1
1/10 = 0.1
[0.0, 0.1)
0
Table 4.1: Frequencies and Probabilities of Five Symbols.
The symbols and frequencies in Table 4.1 are written on the output before any of
the bits of the compressed code. This table will be the ﬁrst thing input by the decoder.
The encoder starts by allocating two variables, Low and High, and setting them to
0 and 1, respectively. They deﬁne an interval [Low, High). As symbols are being input
and processed, the values of Low and High are moved closer together, to narrow the
interval.
After processing the ﬁrst symbol S, Low and High are updated to 0.5 and 1, re-
spectively. The resulting code for the entire input ﬁle will be a number in this range
(0.5 ≤Code < 1.0). The rest of the input will determine precisely where, in the interval
[0.5, 1), the ﬁnal code will lie. A good way to understand the process is to imagine that
the new interval [0.5, 1) is divided among the ﬁve symbols of our alphabet using the same
proportions as for the original interval [0, 1). The result is the ﬁve subintervals [0.5, 0.55),
[0.55, 0.60), [0.60, 0.70), [0.70, 0.75), and [0.75, 1.0). When the next symbol W is input,
the third of those subintervals is selected and is again divided into ﬁve subsubintervals.
As more symbols are being input and processed, Low and High are being updated
according to
NewHigh:=OldLow+Range*HighRange(X);
NewLow:=OldLow+Range*LowRange(X);

126
4.
Arithmetic Coding
where Range=OldHigh−OldLow and LowRange(X), HighRange(X) indicate the low and
high limits of the range of symbol X, respectively. In the example above, the second
input symbol is W, so we update Low := 0.5 + (1.0 −0.5) × 0.4 = 0.70, High := 0.5 +
(1.0 −0.5) × 0.5 = 0.75. The new interval [0.70, 0.75) covers the stretch [40%, 50%) of
the subrange of S. Table 4.2 shows all the steps of coding the string SWISS␣MISS (the
ﬁrst three steps are illustrated graphically in Figure 4.3). The ﬁnal code is the ﬁnal
value of Low, 0.71753375, of which only the eight digits 71753375 need be written on
the output (but see later for a modiﬁcation of this statement).
Char.
The computation of low and high
S
L
0.0 + (1.0 −0.0) × 0.5 = 0.5
H
0.0 + (1.0 −0.0) × 1.0 = 1.0
W
L
0.5 + (1.0 −0.5) × 0.4 = 0.70
H
0.5 + (1.0 −0.5) × 0.5 = 0.75
I
L
0.7 + (0.75 −0.70) × 0.2 = 0.71
H
0.7 + (0.75 −0.70) × 0.4 = 0.72
S
L
0.71 + (0.72 −0.71) × 0.5 = 0.715
H
0.71 + (0.72 −0.71) × 1.0 = 0.72
S
L
0.715 + (0.72 −0.715) × 0.5 = 0.7175
H
0.715 + (0.72 −0.715) × 1.0 = 0.72
␣
L
0.7175 + (0.72 −0.7175) × 0.0 = 0.7175
H
0.7175 + (0.72 −0.7175) × 0.1 = 0.71775
M
L
0.7175 + (0.71775 −0.7175) × 0.1 = 0.717525
H
0.7175 + (0.71775 −0.7175) × 0.2 = 0.717550
I
L
0.717525 + (0.71755 −0.717525) × 0.2 = 0.717530
H
0.717525 + (0.71755 −0.717525) × 0.4 = 0.717535
S
L
0.717530 + (0.717535 −0.717530) × 0.5 = 0.7175325
H
0.717530 + (0.717535 −0.717530) × 1.0 = 0.717535
S
L
0.7175325 + (0.717535 −0.7175325) × 0.5 = 0.71753375
H
0.7175325 + (0.717535 −0.7175325) × 1.0 = 0.717535
Table 4.2: The Process of Arithmetic Encoding.
0
1
1
0.5
0.5
0.7
0.7
0.71
0.71
0.715
0.72
0.72
0.75
0.75
Figure 4.3: Division of the Probability Interval.
The decoder operates in reverse. It starts by inputting the symbols and their ranges,
and reconstructing Table 4.1. It then inputs the rest of the code. The ﬁrst digit is 7,

4.1 The Basic Idea
127
so the decoder immediately knows that the entire code is a number of the form 0.7 . . ..
This number is inside the subrange [0.5, 1) of S, so the ﬁrst symbol is S. The decoder
then eliminates the eﬀect of symbol S from the code by subtracting the lower limit 0.5
of S and dividing by the width of the subrange of S (0.5). The result is 0.4350675, which
tells the decoder that the next symbol is W (since the subrange of W is [0.4, 0.5)).
To eliminate the eﬀect of symbol X from the code, the decoder performs the oper-
ation Code:=(Code-LowRange(X))/Range, where Range is the width of the subrange of
X. Table 4.4 summarizes the steps for decoding our example string (notice that it has
two rows per symbol).
The next example is of three symbols with probabilities listed in Table 4.5a. Notice
that the probabilities are very diﬀerent.
One is large (97.5%) and the others much
smaller. This is an example of skewed probabilities.
Encoding the string a2a2a1a3a3 produces the strange numbers (accurate to 16 dig-
its) in Table 4.6, where the two rows for each symbol correspond to the Low and High
values, respectively. Figure 4.7 lists the Mathematica code that computed the table.
At ﬁrst glance, it seems that the resulting code is longer than the original string,
but Section 4.4 shows how to ﬁgure out the true compression produced by arithmetic
coding.
The steps of decoding this string are listed in Table 4.8 and illustrate a special
problem.
After eliminating the eﬀect of a1, on line 3, the result is 0.
Earlier, we
implicitly assumed that this means the end of the decoding process, but now we know
that there are two more occurrences of a3 that should be decoded. These are shown on
lines 4 and 5 of the table. This problem always occurs when the last symbol in the input
is the one whose subrange starts at zero. In order to distinguish between such a symbol
and the end of the input, we need to deﬁne an additional symbol, the end-of-input (or
end-of-ﬁle, eof). This symbol should be included in the frequency table (with a very
small probability, see Table 4.5b) and it should be encoded once, at the end of the input.
Tables 4.9 and 4.10 show how the string a3a3a3a3eof is encoded into the number
0.0000002878086184764172, and then decoded properly.
Without the eof symbol, a
string of all a3s would have been encoded into a 0.
Notice how the low value is 0 until the eof is input and processed, and how the high
value quickly approaches 0. Now is the time to mention that the ﬁnal code does not
have to be the ﬁnal low value but can be any number between the ﬁnal low and high
values. In the example of a3a3a3a3eof, the ﬁnal code can be the much shorter number
0.0000002878086 (or 0.0000002878087 or even 0.0000002878088).
⋄Exercise 4.1: Encode the string a2a2a2a2 and summarize the results in a table similar
to Table 4.9. How do the results diﬀer from those of the string a3a3a3a3?
If the size of the input is known, it is possible to do without an eof symbol. The
encoder can start by writing this size (unencoded) on the output. The decoder reads the
size, starts decoding, and stops when the decoded ﬁle reaches this size. If the decoder
reads the compressed ﬁle byte by byte, the encoder may have to add some zeros at the
end, to make sure the compressed ﬁle can be read in groups of eight bits.

128
4.
Arithmetic Coding
Char.
Code −low
Range
S
0.71753375 −0.5 = 0.21753375/0.5 = 0.4350675
W
0.4350675 −0.4 = 0.0350675 /0.1 = 0.350675
I
0.350675 −0.2
= 0.150675
/0.2 = 0.753375
S
0.753375 −0.5
= 0.253375
/0.5 = 0.50675
S
0.50675 −0.5
= 0.00675
/0.5 = 0.0135
␣
0.0135 −0
= 0.0135
/0.1 = 0.135
M
0.135 −0.1
= 0.035
/0.1 = 0.35
I
0.35 −0.2
= 0.15
/0.2 = 0.75
S
0.75 −0.5
= 0.25
/0.5 = 0.5
S
0.5 −0.5
= 0
/0.5 = 0
Table 4.4: The Process of Arithmetic Decoding.
Char
Prob.
Range
a1
0.001838 [0.998162,
1.0)
a2
0.975
[0.023162, 0.998162)
a3
0.023162 [0.0,
0.023162)
(a)
Char
Prob.
Range
eof
0.000001
[0.999999,
1.0)
a1
0.001837
[0.998162, 0.999999)
a2
0.975
[0.023162, 0.998162)
a3
0.023162
[0.0,
0.023162)
(b)
Table 4.5: (Skewed) Probabilities of Three Symbols.
a2
0.0 + (1.0 −0.0) × 0.023162 = 0.023162
0.0 + (1.0 −0.0) × 0.998162 = 0.998162
a2
0.023162 + .975 × 0.023162 = 0.04574495
0.023162 + .975 × 0.998162 = 0.99636995
a1
0.04574495 + 0.950625 × 0.998162 = 0.99462270125
0.04574495 + 0.950625 × 1.0 = 0.99636995
a3
0.99462270125 + 0.00174724875 × 0.0 = 0.99462270125
0.99462270125 + 0.00174724875 × 0.023162 = 0.994663171025547
a3
0.99462270125 + 0.00004046977554749998 × 0.0 = 0.99462270125
0.99462270125 + 0.00004046977554749998 × 0.023162 = 0.994623638610941
Table 4.6: Encoding the String a2a2a1a3a3.

4.1 The Basic Idea
129
lowRange={0.998162,0.023162,0.};
highRange={1.,0.998162,0.023162};
low=0.; high=1.;
enc[i_]:=Module[{nlow,nhigh,range},
range=high-low;
nhigh=low+range highRange[[i]];
nlow=low+range lowRange[[i]];
low=nlow; high=nhigh;
Print["r=",N[range,25]," l=",N[low,17]," h=",N[high,17]]]
enc[2]
enc[2]
enc[1]
enc[3]
enc[3]
Figure 4.7: Mathematica Code for Table 4.6.
Char.
Code −low
Range
a2
0.99462270125 −0.023162 = 0.97146170125/0.975
= 0.99636995
a2
0.99636995 −0.023162
= 0.97320795
/0.975
= 0.998162
a1
0.998162 −0.998162
= 0.0
/0.00138 = 0.0
a3
0.0 −0.0
= 0.0
/0.023162 = 0.0
a3
0.0 −0.0
= 0.0
/0.023162 = 0.0
Table 4.8: Decoding the String a2a2a1a3a3.
a3
0.0 + (1.0 −0.0) × 0.0 = 0.0
0.0 + (1.0 −0.0) × 0.023162 = 0.023162
a3
0.0 + 0.023162 × 0.0 = 0.0
0.0 + 0.023162 × 0.023162 = 0.000536478244
a3
0.0 + 0.000536478244 × 0.0 = 0.0
0.0 + 0.000536478244 × 0.023162 = 0.000012425909087528
a3
0.0 + 0.000012425909087528 × 0.0 = 0.0
0.0 + 0.000012425909087528 × 0.023162 = 0.0000002878089062853235
eof
0.0 + 0.0000002878089062853235 × 0.999999 = 0.0000002878086184764172
0.0 + 0.0000002878089062853235 × 1.0 = 0.0000002878089062853235
Table 4.9: Encoding the String a3a3a3a3eof.
Char.
Code−low
Range
a3 0.0000002878086184764172-0 =0.0000002878086184764172 /0.023162=0.00001242589666161891247
a3 0.00001242589666161891247-0=0.00001242589666161891247/0.023162=0.000536477707521756
a3 0.000536477707521756-0
=0.000536477707521756
/0.023162=0.023161976838
a3 0.023161976838-0.0
=0.023161976838
/0.023162=0.999999
eof 0.999999-0.999999
=0.0
/0.000001=0.0
Table 4.10: Decoding the String a3a3a3a3eof.

130
4.
Arithmetic Coding
4.2 Implementation Details
The encoding process described earlier is not practical, because it requires that num-
bers of unlimited precision be stored in Low and High.
The decoding process de-
scribed on page 127 (“The decoder then eliminates the eﬀect of the S from the code
by subtracting. . . and dividing . . . ”) is simple in principle but also impractical. The
code, which is a single number, is normally long and may also be very long. A 1 Mbyte
ﬁle may be encoded into, say, a 500 Kbyte ﬁle that consists of a single number. Dividing
a 500 Kbyte number is complex and slow.
Any practical implementation of arithmetic coding should be based on integers, not
reals (because ﬂoating-point arithmetic is slow and precision is lost), and they should
not be very long (preferably just single precision). We describe such an implementation
here, using two integer variables Low and High. In our example they are four decimal
digits long, but in practice they might be 16 or 32 bits long. These variables hold the
low and high limits of the current subinterval, but we don’t let them grow too much. A
glance at Table 4.2 shows that once the leftmost digits of Low and High become identical,
they never change. We therefore shift such digits out of the two variables and write one
digit on the output. This way, the two variables don’t have to hold the entire code, just
the most-recent part of it. As digits are shifted out of the two variables, a zero is shifted
into the right end of Low and a 9 into the right end of High. A good way to understand
this is to think of each of the two variables as the left ends of two inﬁnitely-long numbers.
Low contains xxxx00 . . ., and High= yyyy99 . . . .
One problem is that High should be initialized to 1, but the contents of Low and
High should be interpreted as fractions less than 1. The solution is to initialize High to
9999. . . , to represent the inﬁnite fraction 0.999 . . ., because this fraction equals 1.
(This is easy to prove. If 0.999 . . . is less than 1, then the average a = (1+0.999 . . .)/2
would be a number between 0.999 . . . and 1, but there is no way to write a.
It is
impossible to give it more digits than to 0.999 . . ., because the latter already has an
inﬁnite number of digits. It is impossible to make the digits any bigger, since they are
already 9’s. This is why the inﬁnite fraction 0.999 . . . must equal 1.)
⋄Exercise 4.2: Write the number 0.5 in binary.
Table 4.11 describes the encoding process of the string SWISS␣MISS. Column 1 lists
the next input symbol. Column 2 shows the new values of Low and High. Column 3
shows these values as scaled integers, after High has been decremented by 1. Column
4 shows the next digit sent to the output. Column 5 shows the new values of Low and
High after being shifted to the left. Notice how the last step sends the four digits 3750
to the output. The ﬁnal output is 717533750.
Decoding is the opposite of encoding. We start with Low=0000, High=9999, and
Code=7175 (the ﬁrst four digits of the compressed ﬁle). These are updated at each step
of the decoding loop. Low and High approach each other (and both approach Code)
until their most signiﬁcant digits are the same. They are then shifted to the left, which
separates them again, and Code is also shifted at that time. An index is calculated at
each step and is used to search the cumulative frequencies column of Table 4.1 to ﬁgure
out the current symbol.
Each iteration of the loop consists of the following steps:

4.2 Implementation Details
131
1
2
3
4
5
S
L =
0+(1
−
0) × 0.5 = 0.5
5000
5000
H =
0+(1
−
0) × 1.0 = 1.0
9999
9999
W L = 0.5+(1
−
.5) × 0.4 = 0.7
7000
7
0000
H = 0.5+(1
−
.5) × 0.5 = 0.75
7499
7
4999
I
L =
0+(0.5
−
0) × 0.2 = 0.1
1000
1
0000
H =
0+(0.5
−
0) × 0.4 = 0.2
1999
1
9999
S
L =
0+(1
−
0) × 0.5 = 0.5
5000
5000
H =
0+(1
−
0) × 1.0 = 1.0
9999
9999
S
L = 0.5+(1
−
0.5) × 0.5 = 0.75
7500
7500
H = 0.5+(1
−
0.5) × 1.0 = 1.0
9999
9999
␣
L = 0.75+(1 −0.75) × 0.0 = 0.75
7500
7
5000
H = 0.75+(1 −0.75) × 0.1 = 0.775 7749
7
7499
M L = 0.5+(0.75 −0.5) × 0.1 = 0.525 5250
5
2500
H = 0.5+(0.75 −0.5) × 0.2 = 0.55
5499
5
4999
I
L = 0.25+(0.5 −0.25) × 0.2 = 0.3
3000
3
0000
H = 0.25+(0.5 −0.25) × 0.4 = 0.35
3499
3
4999
S
L =
0+(0.5
−
0) × 0.5 = .25
2500
2500
H =
0+(0.5
−
0) × 1.0 = 0.5
4999
4999
S
L = 0.25+(0.5 −0.25) × 0.5 = 0.375 3750 3750
H = 0.25+(0.5 −0.25) × 1.0 = 0.5
4999
4999
Table 4.11: Encoding SWISS␣MISS by Shifting.
1. Compute index:=((Code-Low+1)x10-1)/(High-Low+1) and truncate it to the near-
est integer. (The number 10 is the total cumulative frequency in our example.)
2. Use index to ﬁnd the next symbol by comparing it to the cumulative frequencies
column in Table 4.1. In the example below, the ﬁrst value of index is 7.1759, truncated
to 7. Seven is between the 5 and the 10 in the table, so it selects the S.
3. Update Low and High according to
Low:=Low+(High-Low+1)LowCumFreq[X]/10;
High:=Low+(High-Low+1)HighCumFreq[X]/10-1;
where LowCumFreq[X] and HighCumFreq[X] are the cumulative frequencies of symbol X
and of the symbol above it in Table 4.1.
4. If the leftmost digits of Low and High are identical, shift Low, High, and Code one
position to the left. Low gets a 0 entered on the right, High gets a 9, and Code gets the
next input digit from the compressed ﬁle.
Here are all the decoding steps for our example:
0. Initialize Low=0000, High=9999, and Code=7175.
1. index= [(7175 −0 + 1) × 10 −1]/(9999 −0 + 1) = 7.1759 →7. Symbol S is selected.
Low = 0 + (9999 −0 + 1) × 5/10 = 5000. High = 0 + (9999 −0 + 1) × 10/10 −1 = 9999.

132
4.
Arithmetic Coding
2. index= [(7175 −5000 + 1) × 10 −1]/(9999 −5000 + 1) = 4.3518 →4. Symbol W is
selected.
Low = 5000+(9999−5000+1)×4/10 = 7000. High = 5000+(9999−5000+1)×5/10−1 =
7499.
After the 7 is shifted out, we have Low=0000, High=4999, and Code=1753.
3. index= [(1753 −0 + 1) × 10 −1]/(4999 −0 + 1) = 3.5078 →3. Symbol I is selected.
Low = 0 + (4999 −0 + 1) × 2/10 = 1000. High = 0 + (4999 −0 + 1) × 4/10 −1 = 1999.
After the 1 is shifted out, we have Low=0000, High=9999, and Code=7533.
4. index= [(7533 −0 + 1) × 10 −1]/(9999 −0 + 1) = 7.5339 →7. Symbol S is selected.
Low = 0 + (9999 −0 + 1) × 5/10 = 5000. High = 0 + (9999 −0 + 1) × 10/10 −1 = 9999.
5. index= [(7533 −5000 + 1) × 10 −1]/(9999 −5000 + 1) = 5.0678 →5. Symbol S is
selected.
Low = 5000+(9999−5000+1)×5/10 = 7500. High = 5000+(9999−5000+1)×10/10−1 =
9999.
6. index= [(7533 −7500 + 1) × 10 −1]/(9999 −7500 + 1) = 0.1356 →0. Symbol ␣is
selected.
Low = 7500+(9999−7500+1)×0/10 = 7500. High = 7500+(9999−7500+1)×1/10−1 =
7749.
After the 7 is shifted out, we have Low=5000, High=7499, and Code=5337.
7. index= [(5337 −5000 + 1) × 10 −1]/(7499 −5000 + 1) = 1.3516 →1. Symbol M is
selected.
Low = 5000+(7499−5000+1)×1/10 = 5250. High = 5000+(7499−5000+1)×2/10−1 =
5499.
After the 5 is shifted out we have Low=2500, High=4999, and Code=3375.
8. index= [(3375 −2500 + 1) × 10 −1]/(4999 −2500 + 1) = 3.5036 →3. Symbol I is
selected.
Low = 2500+(4999−2500+1)×2/10 = 3000. High = 2500+(4999−2500+1)×4/10−1 =
3499.
After the 3 is shifted out we have Low=0000, High=4999, and Code=3750.
9. index= [(3750 −0 + 1) × 10 −1]/(4999 −0 + 1) = 7.5018 →7. Symbol S is selected.
Low = 0 + (4999 −0 + 1) × 5/10 = 2500. High = 0 + (4999 −0 + 1) × 10/10 −1 = 4999.
10. index= [(3750 −2500 + 1) × 10 −1]/(4999 −2500 + 1) = 5.0036 →5. Symbol S is
selected.
Low = 2500+(4999−2500+1)×5/10 = 3750. High = 2500+(4999−2500+1)×10/10−1 =
4999.
⋄Exercise 4.3: How does the decoder know to stop the loop at this point?
John’s sister (we won’t mention her name) wears socks of two diﬀerent colors, white
and gray. She keeps them in the same drawer, completely mixed up. In the drawer she
has 20 white socks and 20 gray socks. Assuming that it is dark and she has to ﬁnd two
matching socks. How many socks does she have to take out of the drawer to guarantee
that she has a matching pair?

4.3 Underﬂow
133
1
2
3
4
5
1 L=0+(1
−
0)×0.0
= 0.0
000000 0 000000
H=0+(1
−
0)×0.023162= 0.023162
023162 0 231629
2 L=0+(0.231629 −0)×0.0
= 0.0
000000 0 000000
H=0+(0.231629 −0)×0.023162= 0.00536478244 005364 0 053649
3 L=0+(0.053649 −0)×0.0
= 0.0
000000 0 000000
H=0+(0.053649 −0)×0.023162= 0.00124261813 001242 0 012429
4 L=0+(0.012429 −0)×0.0
= 0.0
000000 0 000000
H=0+(0.012429 −0)×0.023162= 0.00028788049 000287 0 002879
5 L=0+(0.002879 −0)×0.0
= 0.0
000000 0 000000
H=0+(0.002879 −0)×0.023162= 0.00006668339 000066 0 000669
Table 4.12: Encoding a3a3a3a3a3 by Shifting.
4.3 Underﬂow
Table 4.12 shows the steps in encoding the string a3a3a3a3a3 by shifting. This table is
similar to Table 4.11, and it illustrates the problem of underﬂow. Low and High approach
each other, and since Low is always 0 in this example, High loses its signiﬁcant digits as
it approaches Low.
Underﬂow may happen not just in this case but in any case where Low and High
need to converge very closely. Because of the ﬁnite size of the Low and High variables,
they may reach values of, say, 499996 and 500003, and from there, instead of reaching
values where their most signiﬁcant digits are identical, they reach the values 499999 and
500000. Since the most signiﬁcant digits are diﬀerent, the algorithm will not output
anything, there will not be any shifts, and the next iteration will only add digits beyond
the ﬁrst six ones. Those digits will be lost, and the ﬁrst six digits will not change. The
algorithm will iterate without generating any output until it reaches the eof.
The solution to this problem is to detect such a case early and rescale both variables.
In the example above, rescaling should be done when the two variables reach values of
49xxxx and 50yyyy. Rescaling should squeeze out the second most-signiﬁcant digits,
end up with 4xxxx0 and 5yyyy9, and increment a counter cntr. The algorithm may
have to rescale several times before the most-signiﬁcant digits become equal. At that
point, the most-signiﬁcant digit (which can be either 4 or 5) should be output, followed
by cntr zeros (if the two variables converged to 4) or nines (if they converged to 5).

134
4.
Arithmetic Coding
4.4 Final Remarks
All the examples so far have been in decimal, because the required computations are
easier to understand in this number base. It turns out that all the algorithms and rules
described above apply to the binary case as well and can be used with only one change:
every occurrence of 9 (the largest decimal digit) should be replaced with 1 (the largest
binary digit).
The examples above don’t seem to show any compression at all.
It seems that
the three example strings SWISS␣MISS, a2a2a1a3a3, and a3a3a3a3eof are encoded into
very long numbers. In fact, it seems that the length of the ﬁnal code depends on the
probabilities involved. The long probabilities of Table 4.5a generate long numbers in
the encoding process, whereas the shorter probabilities of Table 4.1 result in the more
reasonable Low and High values of Table 4.2. This behavior demands an explanation.
I am ashamed to tell you to how many ﬁgures I carried these computations, having
no other business at that time.
—Isaac Newton
To ﬁgure out the kind of compression achieved by arithmetic coding, we have to
consider two facts: (1) In practice, all the operations are performed on binary numbers,
so we have to translate the ﬁnal results to binary before we can estimate the eﬃciency
of the compression; (2) since the last symbol encoded is the eof, the ﬁnal code does not
have to be the ﬁnal value of Low; it can be any value between Low and High. This makes
it possible to select a shorter number as the ﬁnal code that’s being output.
Table 4.2 encodes string SWISS␣MISS into the ﬁnal low and high values 0.71753375
and 0.717535. The approximate binary values of these numbers are
0.10110111101100000100101010111 and 0.1011011110110000010111111011, so we can se-
lect the number 10110111101100000100 as our ﬁnal, compressed output. The ten-symbol
string has been encoded into a 20-bit number. Does this represent good compression?
The answer is yes. Using the probabilities of Table 4.1, it is easy to calculate the
probability of the string SWISS␣MISS. It is P = 0.55 ×0.1×0.22 ×0.1×0.1 = 1.25×10−6.
The entropy of this string is therefore −log2 P = 19.6096. Twenty bits are therefore the
minimum needed in practice to encode the string.
The symbols in Table 4.5a have probabilities 0.975, 0.001838, and 0.023162. These
numbers require quite a few decimal digits, and as a result, the ﬁnal low and high values
in Table 4.6 are the numbers 0.99462270125 and 0.994623638610941. Again it seems
that there is no compression, but an analysis similar to the above shows compression
that’s very close to the entropy.
The probability of the string a2a2a1a3a3 is 0.9752×0.001838×0.0231622 ≈9.37361×
10−7, and −log2 9.37361 × 10−7 ≈20.0249.
The binary representations of the ﬁnal values of low and high in Table 4.6 are
0.111111101001111110010111111001 and 0.111111101001111110100111101. We can se-
lect any number between these two, so we select 1111111010011111100, a 19-bit number.
(This should have been a 21-bit number, but the numbers in Table 4.6 have limited pre-
cision and are not exact.)

4.4 Final Remarks
135
⋄Exercise 4.4: Given the three symbols a1, a2, and eof, with probabilities P1 = 0.4,
P2 = 0.5, and Peof = 0.1, encode the string a2a2a2eof and show that the size of the
ﬁnal code equals the (practical) minimum.
The following argument shows why arithmetic coding can, in principle, be a very
eﬃcient compression method. We denote by s a sequence of symbols to be encoded, and
by b the number of bits required to encode it. As s gets longer, its probability P(s) gets
smaller and b becomes larger. Since the logarithm is the information function, it is easy
to see that b should grow at the same rate that log2 P(s) shrinks. Their product should
therefore be constant, or close to a constant. Information theory shows that b and P(s)
satisfy the double inequality
2 ≤2bP(s) < 4,
which implies
1 −log2 P(s) ≤b < 2 −log2 P(s).
(4.1)
As s gets longer, its probability P(s) shrinks, the quantity −log2 P(s) becomes a large
positive number, and the double inequality of Equation (4.1) shows that in the limit,
b approaches −log2 P(s). This is why arithmetic coding can, in principle, compress a
string of symbols to its theoretical limit.
For more information on this topic, see [Moﬀat et al. 98] and [Witten et al. 87].
The Real Numbers. We can think of arithmetic coding as a method that compresses
a given ﬁle by assigning it a real number in the interval [0, 1). Practical implementations
of arithmetic coding are based on integers, but in principle we can consider this method
as a mapping from the integers (because a data ﬁle can be considered a long integer) to
the reals. We feel that we understand integers intuitively (because we can count one cow,
two cows, etc.), but real numbers have unexpected properties and exhibit unintuitive
behavior, a glimpse of which is revealed in this short intermezzo.
The real numbers can be divided into the sets of rational and irrational. A rational
number can be represented as the ratio of two integers, whereas an irrational number
cannot be represented in this way. The ancient Greeks already knew that
√
2 is irrational.
The real numbers can also be divided into algebraic and transcendental numbers. The
former is the set of all the reals that are solutions of algebraic equations.
We know many integers (0, 1, 7, 10, and 10100 immediately come to mind). We are
also familiar with a few irrational numbers (
√
2, e, and π are common examples), so we
intuitively feel that most real numbers must be rational and the irrationals are a small
minority. Similarly, it is easy to believe that most reals are algebraic and transcendental
numbers are rare. However, set theory, the creation, in the 1870s, of Georg Cantor,
suggests that there are diﬀerent kinds of inﬁnities, that the reals constitute a greater
inﬁnity than the integers (the integers are said to be countable, while the reals are
not), that the rational numbers are countable, while the irrationals are uncountable,
and similarly, that the algebraic numbers are countable, while the transcendentals are
uncountable; completely counterintuitive notions.

136
4.
Arithmetic Coding
Today, we believe in the existence of atoms. If we start with a chunk of matter, cut it
into pieces, cut each piece into smaller pieces, and continue in this way, we will eventually
arrive at individual atoms or even their constituents. The real numbers, however, are
very diﬀerent. They can be represented as points along an inﬁnitely long number line,
but they are everywhere dense on this line. Thus, any segment on the number line, as
short as we can imagine, contains an (uncountable) inﬁnity of real numbers. We cannot
arrive at a segment containing just one number by repeatedly segmenting and producing
shorter and shorter segments.
We are also familiar with the concepts of successor and predecessor. An integer N
has both a successor N +1 and a predecessor N −1. Cantor has shown that the rational
numbers are countable; each can be associated with an integer. Thus, each rational
number can be said to have a successor and a predecessor. The real numbers, again, are
diﬀerent. Given a real number a, we cannot point to its successor. If we ﬁnd another real
number b that may be the successor of a, then there is always another number, namely
(a + b)/2, that is located between a and b and is thus closer to a than b is. We therefore
say that a real number does not have a successor or a predecessor; it does not have
any immediate neighbors. Yet the real numbers form a continuum, because every point
on the number line has a real number that corresponds to it. We cannot imagine any
collection of points, numbers, or any other objects that are everywhere (extremely) dense
but do not feature a predecessor/successor relation. The real numbers are therefore very
counterintuitive.
Pick up two real numbers x and y at random (but with a uniform distribution)
in the interval (0, 1), divide them to obtain the real number R = x/y, and examine
the integer I nearest R. We intuitively feel that I can be even or odd with the same
probability, but careful calculations [Weisstein-picking 07] show that the probability of
I being even is 0.46460. . . instead of the expected 0.5.
This book contains text, tables, mathematical expressions, and ﬁgures, and it can
be stored in the computer as a PDF ﬁle. Such a ﬁle, like any data ﬁle, can be considered
an integer or a long string B of digits (decimal, binary, or to any other base). A real
number is also a (ﬁnite or inﬁnite) string of digits. Thus, it is natural to ask, is there
a real number that includes B in its string of digits? The answer is yes. Even more,
there is a real number that includes in its inﬁnite expansion all the books ever written
and all those that will be written. Simply generate all the integers (we will use binary
notation) 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, 100, 101, 110, 111, 0000, 0001,. . . and
concatenate them to construct a real number R. From its construction, R includes every
possible bitstring and thus every past and future book. (Students pay attention. Both
the questions and answers of your next examination are also included in this number.
It’s just a question of ﬁnding this important part of R.)
A Lexicon is a real number that contains in its expansion inﬁnitely many times
anything imaginable and unimaginable, everything ever written, or that will ever be
written, and any descriptions of every object, process, and phenomenon, real or imagi-
nary. Contrary to any intuitive feelings that we may have, such monsters are not rare.
The surprising result, due to [Calude and Zamﬁrescu 98], is that almost every real num-
ber is a Lexicon! This may be easier to comprehend by means of a thought experiment.
If we put all the reals in a bag, and pick out one at random, it will almost certainly be
a Lexicon.

4.5 Adaptive Arithmetic Coding
137
Gregory Chaitin, the originator of algorithmic information theory, describes in The
Limits of Reason [Chaitin 07], a real number, denoted by Ω, that is well deﬁned and is
a speciﬁc number, but is impossible to compute in its entirety.
Unusual, unexpected, counterintuitive, weird!
4.5 Adaptive Arithmetic Coding
The method of arithmetic coding has two features that make it easy to extend:
1. One of the main encoding steps (page 125) updates NewLow and NewHigh. Similarly,
one of the main decoding steps (step 3 on page 131) updates Low and High according to
Low:=Low+(High-Low+1)LowCumFreq[X]/10;
High:=Low+(High-Low+1)HighCumFreq[X]/10-1;
This means that in order to encode symbol X, the encoder should be given the cumulative
frequencies of X and of the symbol immediately above it (see Table 4.1 for an example of
cumulative frequencies). This also implies that the frequency of X (or, equivalently, its
probability) could be modiﬁed each time it is encoded, provided that the encoder and
the decoder do this in the same way.
2. The order of the symbols in Table 4.1 is unimportant. They can even be swapped in
the table during the encoding process as long as the encoder and decoder do it in the
same way.
With this in mind, it is easy to understand how adaptive arithmetic coding works.
The encoding algorithm has two parts: the probability model and the arithmetic encoder.
The model reads the next symbol from the input and invokes the encoder, sending it
the symbol and the two required cumulative frequencies. The model then increments
the count of the symbol and updates the cumulative frequencies. The point is that the
symbol’s probability is determined by the model from its old count, and the count is
incremented only after the symbol has been encoded. This makes it possible for the
decoder to mirror the encoder’s operations. The encoder knows what the symbol is even
before it is encoded, but the decoder has to decode the symbol in order to ﬁnd out
what it is. The decoder can therefore use only the old counts when decoding a symbol.
Once the symbol has been decoded, the decoder increments its count and updates the
cumulative frequencies in exactly the same way as the encoder.
The model should keep the symbols, their counts (frequencies of occurrence), and
their cumulative frequencies in an array. This array should be maintained in sorted
order of the counts. Each time a symbol is read and its count is incremented, the model
updates the cumulative frequencies, then checks to see whether it is necessary to swap
the symbol with another one, to keep the counts in sorted order.
It turns out that there is a simple data structure that allows for both easy search
and update. This structure is a balanced binary tree housed in an array. (A balanced
binary tree is a complete binary tree where some of the bottom-right nodes may be
missing.) The tree should have a node for every symbol in the alphabet, and since it is
balanced, its height is ⌈log2 n⌉, where n is the size of the alphabet. For n = 256, the
height of the balanced binary tree is 8, so starting at the root and searching for a node

138
4.
Arithmetic Coding
takes at most eight steps. The tree is arranged such that the most probable symbols (the
ones with high counts) are located near the root, which speeds up searches. Table 4.13a
shows an example of a ten-symbol alphabet with counts. Table 4.13b shows the same
symbols sorted by count.
(a)
a1
a2
a3
a4
a5
a6
a7
a8
a9
a10
11
12
12
2
5
1
2
19
12
8
(b)
a8
a2
a3
a9
a1
a10
a5
a4
a7
a6
19
12
12
12
11
8
5
2
2
1
Table 4.13: A Ten-Symbol Alphabet With Counts.
The sorted array “houses” the balanced binary tree of Figure 4.15a.
This is a
simple, elegant way to construct a tree. A balanced binary tree can be housed in an
array without the use of any pointers. The rules are (1) the ﬁrst array location (with
index 1) houses the root, (2) the two children of the node at array location i are housed
at locations 2i and 2i + 1, and (3) the parent of the node at array location j is housed
at location ⌊j/2⌋. It is easy to see how sorting the array has placed the symbols with
largest counts at and near the root.
In addition to a symbol and its count, another value is now added to each tree node,
the total counts of its left subtree. This will be used to compute cumulative frequencies.
The corresponding array is shown in Table 4.14a.
Assume that the next symbol read from the input is a9. Its count is incremented
from 12 to 13. The model keeps the array in sorted order by searching for the farthest
array element to the left of a9 that has a count smaller than that of a9. This search can
be a straight linear search if the array is short enough, or a binary search if the array
is long. In our case, symbols a9 and a2 should be swapped (Table 4.14b). Figure 4.15b
shows the tree after the swap. Notice how the left-subtree counts have been updated.
a8
a2
a3
a9
a1
a10
a5
a4
a7
a6
(a)
19
12
12
12
11
8
5
2
2
1
40
16
8
2
1
0
0
0
0
0
a8
a9
a3
a2
a1
a10
a5
a4
a7
a6
(b)
19
13
12
12
11
8
5
2
2
1
41
16
8
2
1
0
0
0
0
0
Table 4.14: A Ten-Symbol Alphabet With Counts.
Finally, here is how the cumulative frequencies are computed from this tree. When
the cumulative frequency for a symbol X is needed, the model follows the tree branches
from the root to the node containing X while adding numbers into an integer af. Each
time a right branch is taken from an interior node N, af is incremented by the two
numbers (the count and the left-subtree count) found in that node. When a left branch
is taken, af is not modiﬁed. When the node containing X is reached, the left-subtree
count of X is added to af, and af then contains the quantity LowCumFreq[X].

4.5 Adaptive Arithmetic Coding
139
a4,2,0
a2,12,2
a9,13,16
a7,2,0
a6,1,0
a1,11,1
a10,8,0
a5,5,0
a3,12,8
a8,19,41
a4,2,0
a7,2,0
a6,1,0
a5,5,0
a10,8,0
a1,11,1
a9,12,2
a2,12,16
a8,19,40
a3,12,8
(a)
(b)
a4
2
0—1
a9
12
2—13
a7
2
14—15
a2
12
16—27
a6
1
28—28
a1
11
29—39
a8
19
40—58
a10
8
59—66
a3
12
67—78
a5
5
79—83
(c)
Figure 4.15: Adaptive Arithmetic Coding.

140
4.
Arithmetic Coding
As an example, we trace the tree of Figure 4.15a from the root to symbol a6, whose
cumulative frequency is 28. A right branch is taken at node a2, adding 12 and 16 to
af. A left branch is taken at node a1, adding nothing to af. When reaching a6, its
left-subtree count, 0, is added to af. The result in af is 12 + 16 = 28, as can be veriﬁed
from Figure 4.15c. The quantity HighCumFreq[X] is obtained by adding the count of a6
(which is 1) to LowCumFreq[X].
To trace the tree and ﬁnd the path from the root to a6, the algorithm performs the
following steps:
1. Find a6 in the array housing the tree by means of a binary search. In our example
the node with a6 is found at array location 10.
2. Integer-divide 10 by 2. The remainder is 0, which means that a6 is the left child of
its parent. The quotient is 5, which is the array location of the parent.
3. Location 5 of the array contains a1. Integer-divide 5 by 2. The remainder is 1, which
means that a1 is the right child of its parent. The quotient is 2, which is the array
location of a1’s parent.
4. Location 2 of the array contains a2. Integer-divide 2 by 2. The remainder is 0, which
means that a2 is the left child of its parent. The quotient is 1, the array location of the
root, so the process stops.
The PPM compression method, [Salomon 07], is a good example of a statistical
model that invokes an arithmetic encoder in the way described here.
The driver held out a letter. Boldwood seized it and opened it, expecting another
anonymous one—so greatly are people’s ideas of probability a mere sense that prece-
dent will repeat itself. “I don’t think it is for you, sir,” said the man, when he saw
Boldwood’s action. “Though there is no name I think it is for your shepherd.”
—Thomas Hardy, Far From The Madding Crowd
4.6 Range Encoding
The use of integers in arithmetic coding is a must in any practical implementation, but
it results in slow encoding because of the need for frequent renormalizations. The main
steps in any integer-based arithmetic coding implementation are (1) proportional range
reduction and (2) range expansion (renormalization).
Range encoding (or range coding) is an improvement of arithmetic coding that
reduces the number of renormalizations and thereby speeds up integer-based arithmetic
coding by factors of up to 2. The main references are [Schindler 98] and [Campos 06],
and the description here is based on the former.
The main idea is to treat the output not as a binary number, but as a number to
another base (256 is commonly used as a base, implying that each digit is a byte). This
requires fewer renormalizations and no bitwise operations. The following analysis may
shed light on this method.
At any point during arithmetic coding, the output consists of four parts as follows:
1. The part already written on the output. This part will not change.

Chapter Summary
141
2. One digit (bit, byte, or a digit to another base) that may be modiﬁed by at most
one carry when adding to the lower end of the interval. (There cannot be two carries
because when this digit was originally determined, the range was less than or equal to
one unit. Two carries require a range greater than one unit.)
3. A (possibly empty) block of digits that passes on a carry (1 in binary, 9 in decimal,
255 for base-256, etc.) and are represented by a counter counting their number.
4. The low variable of the encoder.
The following states can occur while data is encoded:
No renormalization is needed because the range is in the desired interval.
The low end plus the range (this is the upper end of the interval) will not produce
any carry. In this case the second and third parts can be output because they will never
change.
The digit produced will become part two, and part three will be empty. The low
end has already produced a carry. In this case, the (modiﬁed) second and third parts
can be output; there will not be another carry. Set the second and third part as before.
The digit produced will pass on a possible future carry, so it is added to the block
of digits of part three.
The diﬀerence between conventional integer-based arithmetic coding and range cod-
ing is that in the latter, part two, which may be modiﬁed by a carry, has to be stored
explicitly. With binary output this part is always 0 since the 1’s are always added to
the carry-passing-block. Implementing that is straightforward.
More information and code can be found in [Campos 06]. Range coding is used in
the LZMA dictionary-based method [Salomon 07].
Chapter Summary
An algorithm such as Huﬀman coding is simple, basic, and has many applications in data
compression. However, once we learn it, it does not surprise us. Students exposed to
this method tend to say “I could also come up with this algorithm if only I were 10 times
more intelligent.” Arithmetic coding, however, is diﬀerent. It is one of those ideas that
takes its student by surprise. We tend to say “I would never have thought of that.” The
main idea is to replace an entire ﬁle with a single, short number that can be considered
a real number in the interval [0, 1). The number is short because each data symbol input
from the ﬁle increases the length of the number by an amount inversely proportional to
the symbol’s probability (whereas in the original ﬁle, each symbol increases the length
of the ﬁle by the same amount).
Given a ﬁle with data symbols from an N-symbol alphabet, the principle of arith-
metic coding is to divide the interval [0, 1) into N segments, such that the width of the
segment for symbol S is proportional to the probability of S. If the ﬁrst symbol input
from the data ﬁle is, say, the letter P, then the segment for P is selected and is divided
into N segments of the same relative widths.

142
4.
Arithmetic Coding
It is easy to examine two extreme cases of this process. In the ﬁrst such case, we
assume that the input ﬁle consists of n copies of the most-common symbol of the alphabet
(such as the ﬁle EEE. . . E). The algorithm repeatedly selects the widest segment, divides
it into N segments, and again selects the widest of those. The ﬁnal result, after reading
n symbols, is a narrow segment W, but it is the widest possible segment obtainable
after n divisions. In the other extreme case, we imagine a ﬁle that consists of n copies
of the least-common symbol (such as the ﬁle QQQ. . . Q). After reading n such symbols,
always selecting the narrowest of N segments, and dividing again, we end up with the
narrowest segment R that is possible after n divisions.
Clearly, W is wider than R (although in certain rare cases they may have the same
width), and the point is that a narrow segment takes more digits to specify than a wide
segment. A segment [a, b) can be fully speciﬁed by its left boundary a and its width
b −a, or by its two boundaries a and b. In either case, the speciﬁcations require more
digits for a narrow segment. For example, the narrow segment (0.1234567, 0.1234568)
can be speciﬁed by the two long numbers 0.1234567 and 0.0000001, whereas the wider
segment [0.1, 0.2) can be speciﬁed by a = 0.1 and b −a = 0.1.
The Huﬀman method (Chapter 2) is simple, fast, and produces excellent results,
but is not as eﬀective as arithmetic coding. The conscientious reader may beneﬁt from
the discussion in [Bookstein and Klein 93], where the authors argue in favor of Huﬀman
coding.
Self-Assessment Questions
1. Arithmetic coding replaces a data ﬁle with a real number in the interval [0, 1).
The number of possible data ﬁles is, of course, inﬁnite, and so is the number of reals
in any interval. Discuss these inﬁnities and show that for any data ﬁle there is a real
number in the interval [0, 1).
2. In a computer, real numbers are represented as ﬂoating-point numbers. The
chapter mentions that ﬂoating-point arithmetic is slow and has limited precision. Search
the current literature to ﬁnd the precision of ﬂoating-point numbers on various comput-
ing platforms, especially supercomputers, which are designed for fast, high-precision
scientiﬁc computations.
3. Come up with an argument that shows why arithmetic coding can, in principle,
be a very eﬃcient compression method. This argument can be found somewhere in this
chapter.
That arithmetic is the basest of all mental activities is proved by the
fact that it is the only one that can be accomplished by a machine.
—Arthur Schopenhauer

5
Image Compression
A digital image is a rectangular array of dots, or picture elements, arranged in m rows
and n columns. The expression m × n is called the resolution of the image, and the dots
are called pixels (except in the cases of fax images and video compression, where they are
referred to as pels). The term “resolution” is often also used to indicate the number of
pixels per unit length of the image. Thus, dpi stands for dots per inch. For the purpose
of image compression it is useful to distinguish the following types of images:
1. A bi-level (or monochromatic) image. This is an image where the pixels can have one
of two values, normally referred to as black and white. Each pixel in such an image is
represented by one bit, making this the simplest type of image.
2. A grayscale image. A pixel in such an image consists of g bits, where g is normally
compatible with a byte size; i.e., it is 4, 8, 12, 16, 24, or some other convenient multiple
of 4 or of 8. The pixel’s value indicates one of 2g shades of gray (or shades of some
other color). The set of the most-signiﬁcant bits of all the pixels is the most-signiﬁcant
bitplane. Thus, a grayscale image has g bitplanes.
3.
A continuous-tone image.
This type of image can have many similar colors (or
grayscales). When adjacent pixels diﬀer by just one unit, it is hard or even impossible
for the eye to distinguish their colors. As a result, such an image may contain areas
with colors that seem to vary continuously as the eye moves along the area. A pixel
in such an image is represented by either a single large number (in the case of many
grayscales) or three components (in the case of a color image). A continuous-tone image
is normally a natural image (natural as opposed to artiﬁcial) and is obtained by taking a
photograph with a digital camera, or by scanning a photograph or a painting. Reference
[Carpentieri et al. 00] is a general survey of lossless compression of this type of image.
4. A discrete-tone image (also called a graphical image or a synthetic image). This is

144
5.
Image Compression
normally an artiﬁcial image. It may have a few colors or many colors, but it does not
have the noise and blurring of a natural image. Examples are an artiﬁcial object or
machine, a page of text, a chart, a cartoon, or the contents of a computer screen. (Not
every artiﬁcial image is discrete-tone. A computer-generated image that’s meant to look
natural is a continuous-tone image in spite of its being artiﬁcially generated.) Artiﬁcial
objects, text, and line drawings have sharp, well-deﬁned edges, and are therefore highly
contrasted from the rest of the image (the background). Adjacent pixels in a discrete-
tone image often are either identical or vary signiﬁcantly in value. Such an image does
not compress well with lossy methods, because the loss of just a few pixels may render
a letter illegible, or change a familiar pattern to an unrecognizable one. Compression
methods for continuous-tone images often do not handle sharp edges very well, so special
methods are needed for eﬃcient compression of these images. Notice that a discrete-tone
image may be highly redundant, since the same character or pattern may appear many
times in the image.
5. A cartoon-like image. This is a color image that consists of uniform areas. Each area
has a uniform color but adjacent areas may have very diﬀerent colors. This feature may
be exploited to obtain excellent compression.
Whether an image is treated as discrete or continuous is usually dictated by the depth
of the data. However, it is possible to force an image to be continuous even if it would
ﬁt in the discrete category. (From www.genaware.com)
It is intuitively clear that each type of image may feature redundancy, but they are
redundant in diﬀerent ways. This is why any given compression method may not perform
well for all images, and why diﬀerent methods are needed to compress the diﬀerent image
types. There are compression methods for bi-level images, for continuous-tone images,
and for discrete-tone images. There are also methods that try to break an image up into
continuous-tone and discrete-tone parts, and compress each separately.
5.1 Introduction
Modern computers employ graphics extensively. Window-based operating systems dis-
play the computer’s ﬁle directory graphically. The progress of many system operations,
such as downloading a ﬁle, may also be displayed graphically. Many applications pro-
vide a graphical user interface (GUI), which makes it easier to use the program and to
interpret displayed results. Computer graphics is used in many areas in everyday life
to convert many types of complex information to images. Thus, images are important,
but they tend to be big! Modern hardware can display many colors, which is why it is
common to have a pixel represented internally as a 24-bit number, where the percent-
ages of red, green, and blue occupy 8 bits each. Such a 24-bit pixel can specify one of
224 ≈16.78 million colors. As a result, an image at a resolution of 512×512 that consists
of such pixels occupies 786,432 bytes. At a resolution of 1024×1024 it becomes four
times as big, requiring 3,145,728 bytes. Videos are also commonly used in computers,
making for even bigger images. This is why image compression is so important. An im-
portant feature of image compression is that it can be lossy. An image, after all, exists
for people to look at, so, when it is compressed, it is acceptable to lose image features

5.1 Introduction
145
to which the eye is not sensitive. This is one of the main ideas behind the many lossy
image compression methods that have been developed in recent decades.
In general, information can be compressed if it is redundant. It has been mentioned
in the Introduction that data compression amounts to reducing or removing redundancies
that exist in the data. With lossy compression, however, we have a new concept, namely
compressing by removing irrelevancy. An image can be lossy-compressed by removing
irrelevant information, even if the original image does not have any redundancy.
⋄Exercise 5.1: It would seem that an image with no redundancy is always random (and
therefore uninteresting). It that so?
This chapter discusses methods for image compression.
The methods and ap-
proaches are all diﬀerent, but they remove redundancy from an image by using the
following principle:
The principle of natural image compression. If we select a pixel in the image
at random, there is a good chance that its neighbors will have the same color or very
similar colors.
Image compression is therefore based on the fact that neighboring pixels are highly
correlated. This correlation is also called spatial redundancy.
Here is a simple example that illustrates what can be done with correlated pixels.
The following sequence of values gives the intensities of 24 adjacent pixels in a row of a
continuous-tone image:
12, 17, 14, 19, 21, 26, 23, 29, 41, 38, 31, 44, 46, 57, 53, 50, 60, 58, 55, 54, 52, 51, 56, 60.
Only two of the 24 pixels are identical. Their average value is 40.3. Subtracting pairs
of adjacent pixels results in the sequence
12, 5, −3, 5, 2, 4, −3, 6, 11, −3, −7, 13, 4, 11, −4, −3, 10, −2, −3, 1, −2, −1, 5, 4.
The two sequences are illustrated graphically in Figure 5.1.
Figure 5.1: Values and Diﬀerences of 24 Adjacent Pixels.
The sequence of diﬀerence values has three properties that illustrate its compression
potential: (1) The diﬀerence values are smaller than the original pixel values. Their
average is 2.58. (2) They repeat. There are just 15 distinct diﬀerence values, so in
principle they can be coded by four bits each.
(3) They are decorrelated: adjacent

146
5.
Image Compression
diﬀerence values tend to be diﬀerent. This can be seen by subtracting them, which
results in the sequence of 24 second diﬀerences
12, −7, −8, 8, −3, 2, −7, 9, 5, −14, −4, 20, −11, 7, −15, 1, 13, −12, −1, 4, −3, 1, 6, 1,
which are larger than the diﬀerences themselves.
The principle of image compression has another aspect. We know from experience
that the brightness of neighboring pixels is also correlated. Two adjacent pixels may
have diﬀerent colors.
One may be mostly red, and the other may be mostly green.
Yet if the red component of the ﬁrst is bright, the green component of its neighbor
will, in most cases, also be bright. This property can be exploited by converting pixel
representations from RGB to three other components, one of which is the brightness,
and the other two represent color. One such representation (or color space) is YCbCr,
where Y (the “luminance” component) represents the brightness of a pixel, and Cb and
Cr specify its color. This format is discussed in Section 5.6.1 and its advantage is easy
to understand. The eye is sensitive to small variations in brightness but not to small
changes in color. Thus, losing information in the Cb and Cr components compresses the
image while introducing distortions to which the eye is not sensitive. Losing information
in the Y component, on the other hand, is very noticeable to the eye.
An extreme example of pixel correlation is the interesting 4096 × 4096 color image
found at [brucelindbloom 07]. Every pair of adjacent pixels in this image diﬀer by one
unit of RGB color and therefore they are highly correlated. The following is a quotation
from this reference:
“Although the image contains 16 million pixels (a 48 Mb uncompressed image), it
compresses very nicely, resulting in a surprisingly small download ﬁle. Click here for a
ZIP download (53K) or here for a SIT download (36K).”
5.2 Approaches to Image Compression
An image compression method is normally tailored for a speciﬁc type of image, and
this section lists various approaches to compressing images of diﬀerent types. Only the
general principles are discussed here; speciﬁc methods are described in the remainder of
this chapter.
Approach 1: This is appropriate for bi-level images.
A pixel in such an image is
represented by one bit. Applying the principle of image compression to a bi-level image
therefore means that the immediate neighbors of a pixel P tend to be identical to P.
Thus, it makes sense to use run-length encoding (RLE) to compress such an image. A
compression method for such an image may scan it in raster order (row by row) and
compute the lengths of runs of black and white pixels in each row. The lengths are
encoded by variable-length (preﬁx) codes and are written on the output. An example
of such a method is facsimile compression, Section 2.4.
It should be stressed that this is just an approach to bi-level image compression.
The details of speciﬁc methods vary. For instance, a method may scan the image column
by column or in zigzag (Figure 1.12b), it may convert the image to a quadtree, or it may
scan it region by region using a space-ﬁlling curve.

5.2 Approaches to Image Compression
147
Approach 2: Also for bi-level images. The principle of image compression tells us that
the neighbors of a pixel tend to be similar to the pixel. We can extend this principle
and conclude that if the current pixel P has color c (where c is either black or white),
then pixels of the same color seen in the past (and also those that will be found in the
future) tend to have the same immediate neighbors as P.
This approach looks at n of the near neighbors of the current pixel and considers
them as an n-bit number. This number is the context of the pixel. In principle there
can be 2n contexts, but because of image redundancy we expect them to be distributed
in a nonuniform way. Some contexts should be common, while others will be rare.
The encoder counts how many times each context has already been found for a pixel
of color c, and assigns probabilities to the contexts accordingly. If the current pixel has
color c and its context has probability p, the encoder can use adaptive arithmetic coding
to encode the pixel with that probability. This approach is used by the JBIG compression
standard [Salomon 07].
Next, we turn to grayscale images. A pixel in such an image is represented by n
bits and can have one of 2n values. Applying the principle of image compression to a
grayscale image implies that the immediate neighbors of a pixel P tend to be similar to
P, but are not necessarily identical. Thus, RLE should not be used to compress such
an image. Instead, two alternative approaches are discussed.
Approach 3: Separate the grayscale image into n bi-level images and compress each
with RLE and preﬁx codes. The principle of image compression seems to imply intu-
itively that two adjacent pixels that are similar in the grayscale image will be identical
in most of the n bi-level images. This, however, is not true, as the following example
makes clear. Imagine a grayscale image with n = 4 (i.e., 4-bit pixels, or 16 shades of
gray). The image can be separated into four bi-level images. If two adjacent pixels in
the original grayscale image have values 0000 and 0001, then they are similar. They
are also identical in three of the four bi-level images. However, two adjacent pixels with
values 0111 and 1000 are also similar in the grayscale image (their values are 7 and 8,
respectively) but diﬀer in all four bi-level images.
This problem occurs because the binary codes of adjacent integers may diﬀer by
several bits. The binary codes of 0 and 1 diﬀer by one bit, those of 1 and 2 diﬀer by two
bits, and those of 7 and 8 diﬀer by four bits. The solution is to design special binary
codes such that the codes of any consecutive integers i and i + 1 will diﬀer by one bit
only. An example of such a code is the reﬂected Gray codes of Section 5.2.1.
Approach 4: Use the context of a pixel to predict its value. The context of a pixel is
the values of some of its near neighbors. We can examine some neighbors of a pixel P,
compute an average A of their values, and predict that P will have the value A. The
principle of image compression tells us that our prediction will be correct in most cases,
almost correct in many cases, and terribly wrong in a few cases. We can say that the
predicted value of pixel P represents the redundant information in P. We now calculate
the diﬀerence
∆
def
= P −A,
and assign variable-length (preﬁx) codes to the diﬀerent values of ∆such that small
values (which we expect to be common) are assigned short codes and large values (which
are expected to be rare) are assigned long codes. If P can have the values 0 through

148
5.
Image Compression
m −1, then values of ∆are in the range [−(m −1), +(m −1)], and the number of codes
needed is 2(m −1) + 1 or 2m −1.
Experiments with a large number of images suggest that the values of ∆tend to
be distributed according to the well-known Laplace distribution (see the Intermezzo
on page 178). A compression method can, therefore, use this distribution to assign a
probability to each value of ∆, and use arithmetic coding to encode the ∆values very
eﬃciently. This is the principle of the MLP method [Salomon 07].
The context of a pixel may consist of just one or two of its immediate neighbors.
However, better results may be obtained when several neighbor pixels are included in the
context. The average A in such a case should be weighted, with near neighbors assigned
higher weights (see, for example, Figure 5.2a,b). Another important consideration is the
decoder. In order for it to decode the image, it should be able to compute the context of
every pixel in the same way as the encoder. This means that the context should employ
only pixels that have already been encoded. If the image is scanned in raster order, the
context should include only pixels located above the current pixel or on the same row
and to its left (Figure 5.2c).
1
−9
−9
−9
81
−9
1
81
81
1
−9
81
−9
−9
−9
1
0.0039
−0.0351
−0.0351
−0.0351
0.3164
−0.0351
0.0039
0.3164
0.3164
0.0039
−0.0351
0.3164
−0.0351
−0.0351
−0.0351
0.0039
O O O
O O O O O
O O P
O O O O
O O O O
O O P
Figure 5.2: (a) Sixteen Integer Weights. (b) Normalized. (c) Contexts.
Approach 5: Transform the values of the pixels and encode the transformed values.
The concept of a transform, as well as the most important transforms used in image
compression, are discussed in Section 5.3. Section 5.7 is devoted to subband transforms
(also referred to as wavelets). Recall that compression is achieved by reducing or re-
moving redundancy. The redundancy of an image is caused by the correlation between
pixels, so transforming the pixels to a representation where they are decorrelated elimi-
nates the redundancy. It is also possible to think of a transform in terms of the entropy
of the image. In a highly correlated image, the pixels tend to have equiprobable values,
which results in maximum entropy. If the transformed pixels are decorrelated, certain
pixel values become common, thereby having large probabilities, while others are rare.
This results in small entropy. Quantizing the transformed values can result in eﬃcient
lossy image compression. We want the transformed values to be independent because
coding independent values makes it simpler to construct a statistical model.
We now turn to color images.
A pixel in such an image consists of three color
components, such as red, green, and blue. Most color images are either continuous-tone
or discrete-tone.
Approach 6: The principle of this approach is to separate a continuous-tone color image
into three grayscale images and compress each of the three separately, using approaches
3, 4, or 5.

5.2 Approaches to Image Compression
149
For a continuous-tone image, the principle of image compression implies that adja-
cent pixels have similar, although perhaps not identical, colors. However, similar colors
do not imply similar pixel values. Consider, for example, 12-bit pixel values where each
color component is expressed in four bits. Thus, the 12 bits 1000|0100|0000 represent
a pixel whose color is a mixture of eight units of red (about 50%, since the maximum
is 15 units), four units of green (about 25%), and no blue. Now imagine two adjacent
pixels with values 0011|0101|0011 and 0010|0101|0011. They have similar colors, since
only their red components diﬀer, and only by one unit. However, when considered as
12-bit numbers, the two numbers 001101010011 and 001001010011 are very diﬀerent,
because they diﬀer in one of their most signiﬁcant bits.
An important feature of this approach is to use a luminance chrominance color rep-
resentation instead of the more common RGB. The concepts of luminance and chromi-
nance are discussed in Section 5.6.1 and in [Salomon 99]. The advantage of the luminance
chrominance color representation is that the eye is sensitive to small changes in lumi-
nance but not in chrominance. This allows the loss of considerable data in the chromi-
nance components, while making it possible to decode the image without a signiﬁcant
visible loss of quality.
Approach 7: A diﬀerent approach is needed for discrete-tone images. Recall that such
an image contains uniform regions, and a region may appear several times in the image.
A good example is a screen dump. Such an image consists of text and icons. Each
character of text and each icon is a region, and any region may appear several times in
the image. A possible way to compress such an image is to scan it, identify regions, and
ﬁnd repeating regions. If a region B is identical to an already-found region A, then B
can be compressed by writing a pointer to A on the output. The block decomposition
method (FABD [Salomon 07]) is an example of this approach.
Approach 8: Partition the image into parts (overlapping or not) and compress it by
processing the parts one by one. Suppose that the next unprocessed image part is part
number 15. Try to match it with parts 1–14 that have already been processed. If part
15 can be expressed, for example, as a combination of parts 5 (scaled) and 11 (rotated),
then only the few numbers that specify the combination need be saved, and part 15
can be discarded. If part 15 cannot be expressed as a combination of already-processed
parts, it is declared processed and is saved in raw format.
This approach is the basis of the various fractal methods for image compression.
It applies the principle of image compression to image parts instead of to individual
pixels. Applied this way, the principle tells us that “interesting” images (i.e., those that
are being compressed in practice) have a certain amount of self-similarity. Parts of the
image are identical or similar to the entire image or to other parts.
Image compression methods are not limited to these basic approaches. The many
image compression methods developed during the last several decades employ many dif-
ferent concepts and techniques such as context trees, Markov models, and wavelets. In
addition, the concept of progressive image compression [Salomon 07] should be men-
tioned, because it adds another dimension to the topic of image compression.
5.2.1 Gray Codes
An image compression method that has been developed speciﬁcally for a certain image
type can sometimes be used for other types. Any method for compressing bi-level images,

150
5.
Image Compression
for example, can be used to compress grayscale images by separating the bitplanes and
compressing each individually, as if it were a bi-level image. Imagine, for example, an
image with 16 grayscale values. Each pixel is deﬁned by four bits, so the image can be
separated into four bi-level images. The trouble with this approach is that it violates
the general principle of image compression.
Imagine two adjacent 4-bit pixels with
values 7 = 01112 and 8 = 10002. These pixels have similar values, but when separated
into four bitplanes, the resulting 1-bit pixels are diﬀerent in every bitplane! This is
because the binary representations of the consecutive integers 7 and 8 diﬀer in all four
bit positions. In order to apply any bi-level compression method to grayscale images,
a binary representation of the integers is needed where consecutive integers have codes
diﬀering by one bit only. Such a representation exists and is called reﬂected Gray code
(RGC). This code is easy to generate with the following recursive construction:
Start with the two 1-bit codes (0, 1). Construct two sets of 2-bit codes by duplicating
(0, 1) and appending, either on the left or on the right, ﬁrst a 0, then a 1, to the
original set. The result is (00, 01) and (10, 11). Now reverse (reﬂect) the second set and
concatenate the two. The result is the 2-bit RGC (00, 01, 11, 10); a binary code of the
integers 0 through 3 where consecutive codes diﬀer by exactly one bit. Applying the
rule again produces the two sets (000, 001, 011, 010) and (110, 111, 101, 100), which are
concatenated to form the 3-bit RGC. Note that the ﬁrst and last codes of any RGC also
diﬀer by one bit. Here are the ﬁrst three steps for computing the 4-bit RGC:
Add a zero (0000, 0001, 0011, 0010, 0110, 0111, 0101, 0100),
add a one (1000, 1001, 1011, 1010, 1110, 1111, 1101, 1100),
reﬂect (1100, 1101, 1111, 1110, 1010, 1011, 1001, 1000).
43210
Gray
43210
Gray
43210
Gray
43210
Gray
00000
00000
01000 10010
10000 00011
11000 10001
00001 00100
01001
10110
10001
00111
11001
10101
00010 01100
01010 11110
10010
01111
11010 11101
00011 01000
01011
11010
10011
01011
11011
11001
00100 11000
01100 01010
10100 11011
11100 01001
00101 11100
01101
01110
10101
11111
11101
01101
00110 10100
01110 00110
10110
10111
11110 00101
00111 10000
01111
00010
10111
10011
11111
00001
function b=rgc(a,i)
[r,c]=size(a);
b=[zeros(r,1),a; ones(r,1),flipud(a)];
if i>1, b=rgc(b,i-1); end;
Table 5.3: First 32 Binary and Reﬂected Gray Codes.
Table 5.3 shows how individual bits change when moving through the binary codes

5.2 Approaches to Image Compression
151
of the ﬁrst 32 integers. The 5-bit binary codes of these integers are listed in the odd-
numbered columns of the table, with the bits of integer i that diﬀer from those of i −1
shown in boldface. It is easy to see that the least-signiﬁcant bit (bit b0) changes all
the time, bit b1 changes for every other number, and, in general, bit bk changes every
k integers. The even-numbered columns list one of the several possible reﬂected Gray
codes for these integers. The table also lists a recursive Matlab function to compute
RGC.
⋄Exercise 5.2: It is also possible to generate the reﬂected Gray code of an integer n
with the following nonrecursive rule: Exclusive-OR n with a copy of itself that’s logically
shifted one position to the right. In the C programming language this is denoted by
n^(n>>1). Use this expression to construct a table similar to Table 5.3.
The conclusion is that the most-signiﬁcant bitplanes of an image obey the principle
of image compression more than the least-signiﬁcant ones. When adjacent pixels have
values that diﬀer by one unit (such as p and p+1), chances are that the least-signiﬁcant
bits are diﬀerent and the most-signiﬁcant ones are identical. Any image compression
method that compresses bitplanes individually should therefore treat the least-signiﬁcant
bitplanes diﬀerently from the most-signiﬁcant ones, or should use RGC instead of the
binary code to represent pixels.
History of Gray Codes. Gray codes are named after Frank Gray, who patented
their use for shaft encoders in 1953 [Gray 53]. However, the work was performed much
earlier, the patent being applied for in 1947. Gray was a researcher at Bell Telephone
Laboratories. During the 1930s and 1940s he was awarded numerous patents for work
related to television. According to [Heath 72] the code was ﬁrst, in fact, used by J. M.
E. Baudot for telegraphy in the 1870s, though it is only since the advent of computers
that the code has become widely known.
The Baudot code uses ﬁve bits per symbol. It can represent 32×2−2 = 62 characters
(each code can have two meanings, the meaning being indicated by the LS and FS codes).
It became popular and, by 1950, was designated the International Telegraph Code No. 1.
It was used by many ﬁrst- and second-generation computers.
The August 1972 issue of Scientiﬁc American contains two articles of interest, one
on the origin of binary codes [Heath 72], and another [Gardner 72] on some entertaining
aspects of the Gray codes.
The binary Gray code is fun,
For in it strange things can be done.
Fifteen, as you know,
Is one, oh, oh, oh,
And ten is one, one, one, one.
—Anonymous
What do the English words FAST, THROUGH, DOWN, AWAY, WATER and NECK have in
common?

152
5.
Image Compression
5.3 Image Transforms
The mathematical technique of a transform is a powerful tool that is employed in many
areas and can also serve as an approach to image compression. The prelude to Chapter 1
discusses this concept and describes a simple transform involving Roman numerals. An
image can be compressed by transforming its pixels (which are correlated) to a rep-
resentation where they are decorrelated.
Compression is achieved if the new values
are smaller, on average, than the original ones. Lossy compression can be achieved by
quantizing the transformed values. The decoder inputs the transformed values from
the compressed ﬁle and reconstructs the (precise or approximate) original data by ap-
plying the inverse transform. The transforms discussed in this section are orthogonal.
Section 5.7.1 discusses subband transforms.
The term “decorrelated” implies that the transformed values are independent of
one another. As a result, they can be encoded independently, which makes it simpler
to construct a statistical model. An image can be compressed if its representation has
redundancy. The redundancy in images stems from pixel correlation. If we transform
the image to a representation where the pixels are decorrelated, we have eliminated the
redundancy and the image has been fully compressed.
We start with a simple example of a transform. Given an image, it is scanned in
raster order and pairs of adjacent pixels are prepared. Because the pixels are correlated,
the two pixels (x, y) of a pair normally have similar values. We now consider each pair
of pixels as a point in two-dimensional space, and we plot the points. We know that all
the points of the form (x, x) are located on the 45◦line y = x, so we expect our points to
be concentrated around this line. Figure 5.4a shows the results of plotting the pixels of
a typical image—where a pixel has values in the interval [0, 255]—in such a way. Most
points form a cloud around this line, and only a few points are located away from it.
We now transform the image by rotating all the points 45◦clockwise about the origin,
such that the 45◦line now coincides with the x-axis (Figure 5.4b). This is done by the
simple transformation
(x∗, y∗) = (x, y)

cos 45◦
−sin 45◦
sin 45◦
cos 45◦

= (x, y) 1
√
2

1
−1
1
1

= (x, y)R,
(5.1)
where the rotation matrix R is orthonormal (i.e., the dot product of a row with itself is
1, the dot product of diﬀerent rows is 0, and the same is true for columns). The inverse
transformation is
(x, y) = (x∗, y∗)R−1 = (x∗, y∗)RT = (x∗, y∗) 1
√
2

1
1
−1
1

.
(5.2)
(The inverse of an orthonormal matrix is its transpose.)
It is obvious that most points end up with y coordinates that are zero or close to zero,
while the x coordinates don’t change much. Figure 5.5a,b shows the distributions of the x
and y coordinates (i.e., the odd-numbered and even-numbered pixels) of the 128×128×8
grayscale Lena image before the rotation. It is clear that the two distributions don’t
diﬀer by much. Figure 5.5c,d shows that the distribution of the x coordinates stays
about the same (with greater variance) but the y coordinates are concentrated around

5.3 Image Transforms
153
127
50
0
−128
−50
255
255
128
128
0
0
(a)
(b)
Figure 5.4: Rotating a Cloud of Points.

154
5.
Image Compression
0
50
100
150
200
250
300
0
10
20
30
40
50
60
70
80
90
100
0
50
100
150
200
250
300
0
10
20
30
40
50
60
70
80
90
100
0
50
100
150
200
250
300
350
0
10
20
30
40
50
60
70
80
90
0
50
100
150
200
250
300
0
100
200
300
400
500
600
700
800
900
1000
(a)
(b)
(d)
(c)
filename=’lena128’; dim=128;
xdist=zeros(256,1); ydist=zeros(256,1);
fid=fopen(filename,’r’);
img=fread(fid,[dim,dim])’;
for col=1:2:dim-1
for row=1:dim
x=img(row,col)+1; y=img(row,col+1)+1;
xdist(x)=xdist(x)+1; ydist(y)=ydist(y)+1;
end
end
figure(1), plot(xdist), colormap(gray) %dist of x&y values
figure(2), plot(ydist), colormap(gray) %before rotation
xdist=zeros(325,1); % clear arrays
ydist=zeros(256,1);
for col=1:2:dim-1
for row=1:dim
x=round((img(row,col)+img(row,col+1))*0.7071);
y=round((-img(row,col)+img(row,col+1))*0.7071)+101;
xdist(x)=xdist(x)+1; ydist(y)=ydist(y)+1;
end
end
figure(3), plot(xdist), colormap(gray) %dist of x&y values
figure(4), plot(ydist), colormap(gray) %after rotation
Figure 5.5: Distribution of Image Pixels Before and After Rotation.

5.3 Image Transforms
155
zero. The Matlab code that generated these results is also shown. (Figure 5.5d shows
that the y coordinates are concentrated around 100, but this is because a few were as
small as −101, so they had to be scaled by 101 to ﬁt in a Matlab array, which always
starts at index 1.)
Once the coordinates of points are known before and after the rotation, it is easy to
measure the reduction in correlation. A simple measure is the sum 
i xiyi, also called
the cross-correlation of points (xi, yi).
⋄Exercise 5.3: Given the ﬁve points (5, 5), (6, 7), (12.1, 13.2), (23, 25), and (32, 29),
rotate them 45◦clockwise and calculate their cross-correlations before and after the
rotation.
We can now compress the image by simply writing the transformed pixels on the
output. If lossy compression is acceptable, then all the pixels can be quantized (Sec-
tion 1.5), resulting in even smaller numbers. We can also write all the odd-numbered
pixels (those that make up the x coordinates of the pairs) on the output, followed by all
the even-numbered pixels. These two sequences are called the coeﬃcient vectors of the
transform. The latter sequence consists of small numbers and may, after quantization,
have runs of zeros, resulting in even better compression.
It can be shown that the total variance of the pixels does not change by the rotation,
because a rotation matrix is orthonormal. However, since the variance of the new y
coordinates is small, most of the variance is now concentrated in the x coordinates. The
variance is sometimes called the energy of the distribution of pixels, so we can say that
the rotation has concentrated (or compacted) the energy in the x coordinate and has
created compression in this way.
Concentrating the energy in one coordinate has another advantage. It makes it
possible to quantize that coordinate more ﬁnely than the other coordinates. This type
of quantization results in better (lossy) compression.
The following simple example illustrates the power of this basic transform. We start
with the point (4, 5), whose two coordinates are similar. Using Equation (5.1) the point
is transformed to (4, 5)R = (9, 1)/
√
2 ≈(6.36396, 0.7071). The energies of the point and
its transform are 42 + 52 = 41 = (92 + 12)/2. If we delete the smaller coordinate (4) of
the point, we end up with an error of 42/41 = 0.39. If, on the other hand, we delete the
smaller of the two transform coeﬃcients (0.7071), the resulting error is just 0.70712/41 =
0.012. Another way to obtain the same error is to consider the reconstructed point.
Passing
1
√
2(9, 1) through the inverse transform [Equation (5.2)] results in the original
point (4, 5). Doing the same with
1
√
2(9, 0) results in the approximate reconstructed
point (4.5, 4.5). The energy diﬀerence between the original and reconstructed points is
the same small quantity
	
(42 + 52) −(4.52 + 4.52)

42 + 52
= 41 −40.5
41
= 0.0012.
This simple transform can easily be extended to any number of dimensions. Instead
of selecting pairs of adjacent pixels we can select triplets. Each triplet becomes a point
in three-dimensional space, and these points form a cloud concentrated around the line
that forms equal (although not 45◦) angles with the three coordinate axes. When this

156
5.
Image Compression
line is rotated such that it coincides with the x-axis, the y and z coordinates of the
transformed points become small numbers. The transformation is done by multiplying
each point by a 3×3 rotation matrix, and such a matrix is, of course, orthonormal. The
transformed points are then separated into three coeﬃcient vectors, of which the last
two consist of small numbers. For maximum compression, each coeﬃcient vector should
be quantized separately.
This can be extended to more than three dimensions, with the only diﬀerence
being that we cannot visualize spaces of dimensions higher than three. However, the
mathematics can easily be extended. Some compression methods, such as JPEG, divide
an image into blocks of 8×8 pixels each, and rotate ﬁrst each row then each column of a
block by means of Equation (5.6), as shown in Section 5.5. This double rotation produces
a set of 64 transformed values, of which the ﬁrst—termed the “DC coeﬃcient”—is large,
and the other 63 (called the “AC coeﬃcients”) are normally small. Thus, this transform
concentrates the energy in the ﬁrst of 64 dimensions. The set of DC coeﬃcients and
each of the sets of 63 AC coeﬃcients should, in principle, be quantized separately (JPEG
does this a little diﬀerently, though; see Section 5.6.3).
5.4 Orthogonal Transforms
Image transforms are designed to have two properties: (1) to reduce image redundancy
by reducing the sizes of most pixels and (2) to identify the less important parts of the
image by isolating the various frequencies of the image. Thus, this section starts with a
short discussion of frequencies. We intuitively associate a frequency with a wave. Water
waves, sound waves, and electromagnetic waves have frequencies, but pixels in an image
can also feature frequencies. Figure 5.6 shows a small, 5×8 bi-level image that illustrates
this concept. The top row is uniform, so we can assign it zero frequency. The rows below
it have increasing pixel frequencies as measured by the number of color changes along a
row. The four waves on the right roughly correspond to the frequencies of the four top
rows of the image.
a
b
c
d
Figure 5.6: Image Frequencies.
An example of a high-frequency image is bright stars on a dark sky. An example of
a low-frequency image is a uniform wall, where all the points have identical or similar
colors.
Image frequencies are important because of the following basic fact: Low frequencies
correspond to the important image features, whereas high frequencies correspond to the
details of the image, which are less important. Thus, when a transform isolates the
various image frequencies, pixels that correspond to high frequencies can be quantized

5.4 Orthogonal Transforms
157
heavily, whereas pixels that correspond to low frequencies should be quantized lightly
or not at all. This is how a transform can compress an image very eﬀectively by losing
information, but only information that corresponds to unimportant image details.
Practical image transforms should be fast and preferably also simple to implement.
This suggests the use of linear transforms. In such a transform, each transformed value
(or transform coeﬃcient) ci is a weighted sum of the data items (the pixels) dj that are
being transformed, where each item is multiplied by a weight wij. Thus, ci = 
j djwij,
for i, j = 1, 2, . . . , n. For n = 4, this is expressed in matrix notation:
⎛
⎜
⎝
c1
c2
c3
c4
⎞
⎟
⎠=
⎛
⎜
⎝
w11
w12
w13
w14
w21
w22
w23
w24
w31
w32
w33
w34
w41
w42
w43
w44
⎞
⎟
⎠
⎛
⎜
⎝
d1
d2
d3
d4
⎞
⎟
⎠.
In general, we can write C = W·D. Each row of W is called a basis vector.
The only quantities that have to be determined are the weights wij. The guiding
principles are as follows:
1. Reducing redundancy. The ﬁrst transform coeﬃcient c1 can be large, but the
remaining values c2, c3, . . . should be small.
2. Isolating frequencies. The ﬁrst transform coeﬃcient c1 should correspond to zero
pixel frequency, and the remaining coeﬃcients should correspond to higher and higher
frequencies.
The key to determining the weights wij is the fact that our data items dj are not
arbitrary numbers but pixel values and therefore nonnegative and correlated.
The basic relation ci = 
j djwij suggests that the ﬁrst coeﬃcient c1 will be large
if all the weights of the form w1j are positive. To make the other coeﬃcients ci small,
it is enough to make half the weights wij positive and the other half negative. A simple
choice is to assign half the weights the value +1 and the other half the value −1. In the
extreme case where all the pixels dj are identical, this will result in ci = 0. When the
dj are similar, ci will be small (positive or negative).
This choice of wij satisﬁes the ﬁrst requirement: to reduce pixel redundancy by
means of a transform. In order to satisfy the second requirement, the weights wij of
row i should feature frequencies that get higher with i. Weights w1j should have zero
frequency; they should all be +1’s.
Weights w2j should have one sign change; i.e.,
they should be +1, +1, . . . , +1, −1, −1, . . . , −1. This argument is applied to the other
rows with more and more sign changes, until the last row of weights wnj receives the
highest frequency +1, −1, +1, −1, . . . , +1, −1.
The mathematical discipline of vector
spaces employs the term “basis vectors” for our rows of weights.
In addition to isolating the various frequencies of pixels dj, this choice results in
basis vectors that are orthogonal. The basis vectors are the rows of matrix W, which is
why this matrix and by implication, the entire transform, are also termed orthogonal.
These considerations are satisﬁed by the orthogonal matrix
W =
⎛
⎜
⎝
1
1
1
1
1
1
−1
−1
1
−1
−1
1
1
−1
1
−1
⎞
⎟
⎠.
(5.3)

158
5.
Image Compression
The ﬁrst basis vector (the top row of W) consists of all 1’s, so its frequency is zero. Each
of the subsequent vectors has two +1’s and two −1’s, so they produce small transformed
values, and their frequencies (measured as the number of sign changes along the basis
vector) become higher.
To illustrate how this matrix identiﬁes the frequencies in a data vector, we mul-
tiply it by the following test vectors (1, 0, 0, 1), (0, 0.33, −0.33, −1), (1, 0, 0, 0), and
(1, −0.8, 1, −0.8). The results are
W·
⎡
⎢⎣
1
0
0
1
⎤
⎥⎦=
⎡
⎢⎣
2
0
2
0
⎤
⎥⎦,
W·
⎡
⎢⎣
0
0.33
−0.33
−1
⎤
⎥⎦=
⎡
⎢⎣
0
2.66
0
1.33
⎤
⎥⎦,
W·
⎡
⎢⎣
1
0
0
0
⎤
⎥⎦=
⎡
⎢⎣
1
1
1
1
⎤
⎥⎦,
W·
⎡
⎢⎣
1
−0.8
1
−0.8
⎤
⎥⎦=
⎡
⎢⎣
0.4
0
0
3.6
⎤
⎥⎦.
The results make sense when we discover how the four test vectors were determined
(1, 0, 0, 1) = 0.5(1, 1, 1, 1) + 0.5(1, −1, −1, 1),
(1, 0.33, −0.33, −1) = 0.66(1, 1, −1, −1) + 0.33(1, −1, 1, −1),
(1, 0, 0, 0) = 0.25(1, 1, 1, 1) + 0.25(1, 1, −1, −1) + 0.25(1, −1, −1, 1) + 0.25(1, −1, 1, −1),
(1, −0.8, 1, −0.8) = 0.1(1, 1, 1, 1) + 0.9(1, −1, 1, −1).
The product of W and the ﬁrst vector is (2, 0, 2, 0), indicating how that vector con-
sists of equal amounts of the ﬁrst and the third frequencies. Similarly, the transform
(0.4, 0, 0, 3.6) shows that vector (1, −0.8, 1, −0.8) is a mixture of a small amount of the
ﬁrst frequency and nine times the fourth frequency.
It is also possible to modify this transform to conserve the energy of the data
vector.
All that’s needed is to multiply the transformation matrix W by the scale
factor 1/2. Thus, the product (W/2)×(a, b, c, d) has the same energy a2 + b2 + c2 + d2
as the data vector (a, b, c, d). An example is the product of W/2 and the correlated
vector (5, 6, 7, 8). It results in the transform coeﬃcients (13, −2, 0, −1), where the ﬁrst
coeﬃcient is large and the remaining ones are smaller than the original data items. The
energy of both (5, 6, 7, 8) and (13, −2, 0, −1) is 174, but whereas in the former vector the
ﬁrst component accounts for only 14% of the energy, in the transformed vector the ﬁrst
component accounts for 97% of the energy. This is how our simple orthogonal transform
compacts the energy of the data vector.
Another advantage of W is that it also performs the inverse transform. The product
(W/2)·(13, −2, 0, −1)T reconstructs the original data (5, 6, 7, 8).
We are now in a position to appreciate the compression potential of this transform.
We use matrix W/2 to transform the (not very correlated) data vector d = (4, 6, 5, 2).
The result is t = (8.5, 1.5, −2.5, 0.5). It’s easy to transform t back to d, but t itself
does not provide any compression. In order to achieve compression, we encode the AC
coeﬃcients of t by replacing them with variable-length codes, but the real power of an
orthogonal transform becomes apparent when we choose lossy compression. In this case,
we quantize the components of t before they are encoded, and the point is that even after
heavy quantization, it is still possible to get back a vector very similar to the original d.
We ﬁrst quantize t to the integers (9, 1, −3, 0) and perform the inverse transform
to get back (3.5, 6.5, 5.5, 2.5). In a similar experiment, we completely delete the two

5.4 Orthogonal Transforms
159
smallest elements and inverse-transform the coarsely-quantized vector (8.5, 0, −2.5, 0).
This produces the reconstructed data (3, 5.5, 5.5, 3), still very close to the original values
of d. The conclusion is that even this simple, intuitive transform is a powerful tool for
“squeezing out” the redundancy in data. More sophisticated transforms produce results
that can be quantized coarsely and still be used to reconstruct the original data to a
high degree.
5.4.1 Two-Dimensional Transforms
Given two-dimensional data such as the 4×4 matrix
D =
⎛
⎜
⎝
5
6
7
4
6
5
7
5
7
7
6
6
8
8
8
8
⎞
⎟
⎠,
where each of the four columns is highly correlated, we can apply our simple one-
dimensional transform to the columns of D. The result is
C′ = W·D =
⎛
⎜
⎝
1
1
1
1
1
1
−1
−1
1
−1
−1
1
1
−1
1
−1
⎞
⎟
⎠·D =
⎛
⎜
⎝
26
26
28
23
−4
−4
0
−5
0
2
2
1
−2
0
−2
−3
⎞
⎟
⎠.
Each column of C′ is the transform of a column of D. Notice how the top element of
each column of C′ is dominant, because the data in the corresponding column of D is
correlated. Notice also that the four components of each row of C′ are still correlated.
C′ is the ﬁrst stage in a two-stage process that produces the two-dimensional transform
of matrix D. The second stage should transform each row of C′, and this is done by
multiplying C′ by the transpose WT . Our particular W, however, is symmetric, so we
end up with C = C′·WT = W·D·WT = W·D·W or
C =
⎛
⎜
⎝
26
26
28
23
−4
−4
0
−5
0
2
2
1
−2
0
−2
−3
⎞
⎟
⎠
⎛
⎜
⎝
1
1
1
1
1
1
−1
−1
1
−1
−1
1
1
−1
1
−1
⎞
⎟
⎠=
⎛
⎜
⎝
103
1
−5
5
−13
−3
−5
5
5
−1
−3
−1
−7
3
−3
−1
⎞
⎟
⎠.
The elements of C are decorrelated. The top-left element is dominant (it is also the
sum of the 16 elements of D). It contains most of the total energy of the original D.
The elements in the top row and the leftmost column are somewhat large, while the
remaining elements are smaller than the original data items. The double-stage, two-
dimensional transformation has reduced the correlation in both the horizontal and ver-
tical dimensions. As in the one-dimensional case, excellent compression can be achieved
by quantizing the elements of C, especially those that correspond to higher frequencies
(i.e., located toward the bottom-right corner of C).
This is the essence of orthogonal transforms. The next section discusses the discrete
cosine transform (DCT), the most popular orthogonal transform, which is the heart of
several image and video compression algorithms, most notably JPEG and MPEG-1.

160
5.
Image Compression
5.5 The Discrete Cosine Transform
This important transform (DCT for short) was ﬁrst described by [Ahmed et al. 74] and
has been used and studied extensively since. Because of its importance for data com-
pression, the DCT is treated here in detail. Section 5.5.1 introduces the mathematical
expressions for the DCT in one dimension and two dimensions without any theoretical
background or justiﬁcation. The use of the transform and its advantages for data com-
pression are then demonstrated by several examples. Section 5.5.2 covers the theory of
the DCT and discusses its interpretation as a basis of a vector space. More information
on this important technique, as well as an alternative interpretation, can be found in
[Salomon 07].
Cosine, the opposite of “stop sign.”
5.5.1 Introduction
The DCT in one dimension is deﬁned by
Gf =

2
nCf
n−1

t=0
pt cos
(2t + 1)fπ
2n

,
for f = 0, 1, . . . , n −1,
(5.4)
where
Cf =

1
√
2,
f = 0,
1,
f > 0.
The input is a set of n data values pt (pixels, audio samples, or other data), and the
output is a set of n DCT transform coeﬃcients (or weights) Gf. The ﬁrst coeﬃcient
G0 is the DC coeﬃcient, and the rest are the AC coeﬃcients (these terms have been
inherited from electrical engineering, where they stand for “direct current” and “alter-
nating current”). Notice that the coeﬃcients are real numbers even if the input data
consists of integers. Similarly, the coeﬃcients may be positive or negative even if the
input data consists of nonnegative numbers only. This computation is straightforward
but slow (reference [Salomon 07] discusses faster versions). Equation (5.4) implies that
G0, the DC coeﬃcient, is given by
G0 =

1
n
n−1

t=0
pt = √n 1
n

pt,
and therefore equals

1
n times the average of the n data values.
The decoder inputs the DCT coeﬃcients in sets of n and uses the inverse DCT
(IDCT) to reconstruct the original data values (also in groups of n). The IDCT in one
dimension is given by
pt =

2
n
n−1

j=0
CjGj cos
(2t + 1)jπ
2n

,
for t = 0, 1, . . . , n −1.
(5.5)

5.5 The Discrete Cosine Transform
161
The important feature of the DCT, which makes it so useful in data compression,
is that it takes correlated input data and concentrates its energy in just the ﬁrst few
transform coeﬃcients. If the input data consists of correlated quantities, then most of
the n transform coeﬃcients produced by the DCT are zeros or small numbers, and only
a few are large (normally the ﬁrst ones). We will see that the early coeﬃcients contain
the important (low-frequency) image information and the later coeﬃcients contain the
less-important (high-frequency) image information.
Compressing a set of correlated
pixels with the DCT is therefore done by (1) computing the DCT coeﬃcients of the
pixels, (2) quantizing the coeﬃcients, and (3) encoding them with variable-length codes
or arithmetic coding.
The small coeﬃcients are quantized coarsely (possibly all the
way to zero), and the large ones can be quantized ﬁnely to the nearest integer. After
quantization, the coeﬃcients (or variable-length codes assigned to the coeﬃcients) are
written on the output. Decompression is done by performing the inverse DCT on the
quantized coeﬃcients. This results in data items that are not identical to the original
ones but are not much diﬀerent.
In practical applications, the data to be compressed is partitioned into sets of n
items each and each set is DCT-transformed and quantized individually. The value of
n is critical. Small values of n such as 3, 4, or 6 result in many small sets of data items.
Such a small set is transformed to a small set of coeﬃcients where the energy of the
original data is concentrated in a few coeﬃcients, but there are only a few coeﬃcients
in such a set! Thus, there are not enough small coeﬃcients to quantize. Large values of
n result in a few large sets of data. The problem in this case is that the individual data
items of a large set are normally not correlated and therefore result in a set of transform
coeﬃcients where all the coeﬃcients are large. Experience indicates that n = 8 is a good
value, and most data compression methods that employ the DCT use this value of n.
The following experiment illustrates the power of the DCT in one dimension. We
start with the set of eight correlated data items p = (12, 10, 8, 10, 12, 10, 8, 11), apply
the DCT in one dimension to them, and ﬁnd that it results in the eight coeﬃcients
28.6375, 0.571202, 0.46194, 1.757, 3.18198, −1.72956, 0.191342, −0.308709.
(Notice that the DC coeﬃcient equals
√
8 times the average 10.125 of the eight items.)
These can be fed to the IDCT and transformed by it to precisely reconstruct the original
data (except for small errors caused by limited machine precision). Our goal, however,
is to compress (with lossy compression) the data by quantizing the coeﬃcients. We ﬁrst
quantize them to 28.6, 0.6, 0.5, 1.8, 3.2, −1.8, 0.2, −0.3, and apply the IDCT to get back
12.0254, 10.0233, 7.96054, 9.93097, 12.0164, 9.99321, 7.94354, 10.9989.
We then quantize the coeﬃcients even more, to 28, 1, 1, 2, 3, −2, 0, 0, and apply the IDCT
to get back
12.1883, 10.2315, 7.74931, 9.20863, 11.7876, 9.54549, 7.82865, 10.6557.
Finally, we quantize the coeﬃcients to 28, 0, 0, 2, 3, −2, 0, 0, and still get back from the
IDCT the sequence
11.236, 9.62443, 7.66286, 9.57302, 12.3471, 10.0146, 8.05304, 10.6842,

162
5.
Image Compression
where the largest diﬀerence between an original value (12) and a reconstructed one
(11.236) is 0.764 (or 6.4% of 12). The code that does all that is listed in Figure 5.7.
n=8;
p={12.,10.,8.,10.,12.,10.,8.,11.};
c=Table[If[t==1, 0.7071, 1], {t,1,n}];
dct[i_]:=Sqrt[2/n]c[[i+1]]Sum[p[[t+1]]Cos[(2t+1)i Pi/16],{t,0,n-1}];
q=Table[dct[i],{i,0,n-1}] (* use precise DCT coefficients *)
q={28,0,0,2,3,-2,0,0}; (* or use quantized DCT coefficients *)
idct[t_]:=Sqrt[2/n]Sum[c[[j+1]]q[[j+1]]Cos[(2t+1)j Pi/16],{j,0,n-1}];
ip=Table[idct[t],{t,0,n-1}]
Figure 5.7: Experiments with the One-Dimensional DCT.
It seems magical that the eight original data items can be reconstructed to such
high precision from just four transform coeﬃcients. The explanation, however, relies on
the following arguments instead of on magic: (1) The IDCT is given all eight transform
coeﬃcients, so it knows the positions, not just the values, of the nonzero coeﬃcients.
(2) The ﬁrst few coeﬃcients (the large ones) contain the important information of the
original data items. The small coeﬃcients, the ones that are quantized heavily, contain
less important information (in the case of images, they contain the image details). (3)
The original data is correlated.
⋄Exercise 5.4: The quantization error between a vector X = x1, x2, . . . , xn and its
quantized representation ˆX = ˆx1, ˆx2, . . . , ˆxn is deﬁned as E = n
i=1 (xi −ˆx1)2.
Use the set of eight correlated data items p = (12, 10, 8, 10, 12, 10, 8, 11) and the
three quantizations of the previous experiment to compute (1) the error between the
original items and their quantized DCT coeﬃcients and (2) the error between the original
items and the quantized items obtained after the IDCT. Compare the results of (1) and
(2). Is there anything surprising about the results? Can this comparison be useful?
The following experiment illustrates the performance of the DCT when applied to
decorrelated data items. Given the eight decorrelated data items −12, 24, −181, 209,
57.8, 3, −184, and −250, their DCT produces
−117.803, 166.823, −240.83, 126.887, 121.198, 9.02198, −109.496, −185.206.
When these coeﬃcients are quantized to (−120, 170, −240, 125, 120, 9, −110, −185) and
fed into the IDCT, the result is
−12.1249, 25.4974, −179.852, 208.237, 55.5898, 0.364874, −185.42, −251.701,
where the maximum diﬀerence (between 3 and 0.364874) is 2.63513 or 88% of 3. Ob-
viously, even with such ﬁne quantization the reconstruction is not as good as with
correlated data.

5.5 The Discrete Cosine Transform
163
⋄Exercise 5.5: Compute the one-dimensional DCT [Equation (5.4)] of the eight corre-
lated values 11, 22, 33, 44, 55, 66, 77, and 88. Show how to quantize them, and compute
their IDCT from Equation (5.5).
The DCT in one dimension can be used to compress one-dimensional data, such
as a set of audio samples. This chapter, however, discusses image compression which is
based on the two-dimensional correlation of pixels (a pixel tends to resemble all its near
neighbors, not just those in its row). This is why practical image compression methods
use the DCT in two dimensions. This version of the DCT is applied to small parts (data
blocks) of the image. It is computed by applying the DCT in one dimension to each
row of a data block, then to each column of the result. Because of the special way the
DCT in two dimensions is computed, we say that it is separable in the two dimensions.
Because it is applied to blocks of an image, we term it a “blocked transform.” It is
deﬁned by
Gij =

2
m

2
nCiCj
n−1

x=0
m−1

y=0
pxy cos
(2y + 1)jπ
2m

cos
(2x + 1)iπ
2n

,
(5.6)
for 0 ≤i ≤n −1 and 0 ≤j ≤m −1 and for Ci and Cj deﬁned by Equation (5.4). The
ﬁrst coeﬃcient G00 is termed the DC coeﬃcient and is large. The remaining coeﬃcients,
which are much smaller, are called the AC coeﬃcients.
The image is broken up into blocks of n×m pixels pxy (with n = m = 8 typically),
and Equation (5.6) is used to produce a block of n×m DCT coeﬃcients Gij for each
block of pixels. The top-left coeﬃcient (the DC) is large, and the AC coeﬃcients become
smaller as we move from the top-left to the bottom-right corner. The top row and the
leftmost column contain the largest AC coeﬃcient, and the remaining coeﬃcients are
smaller. This behavior justiﬁes the zigzag sequence illustrated by Figure 1.12b.
The coeﬃcients are then quantized, which results in lossy but highly eﬃcient com-
pression. The decoder reconstructs a block of quantized data values by computing the
IDCT whose deﬁnition is
pxy =

2
m

2
n
n−1

i=0
m−1

j=0
CiCjGij cos
(2x + 1)iπ
2n

cos
(2y + 1)jπ
2m

,
(5.7)
where
Cf =

1
√
2,
f = 0
1 ,
f > 0,
for 0 ≤x ≤n −1 and 0 ≤y ≤m −1. We now show one way to compress an entire
image with the DCT in several steps as follows:
1. The image is divided into k blocks of 8×8 pixels each. The pixels are denoted
by pxy. If the number of image rows (columns) is not divisible by 8, the bottom row
(rightmost column) is duplicated as many times as needed.
2. The DCT in two dimensions [Equation (5.6)] is applied to each block Bi. The
result is a block (we’ll call it a vector) W (i) of 64 transform coeﬃcients w(i)
j
(where

164
5.
Image Compression
j = 0, 1, . . . , 63). The k vectors W (i) become the rows of matrix W
W =
⎡
⎢⎢⎢⎣
w(1)
0
w(1)
1
. . .
w(1)
63
w(2)
0
w(2)
1
. . .
w(2)
63
...
...
w(k)
0
w(k)
1
. . .
w(k)
63
⎤
⎥⎥⎥⎦.
3. The 64 columns of W are denoted by C(0), C(1), . . . , C(63). The k elements
of C(j) are

w(1)
j , w(2)
j , . . . , w(k)
j

. The ﬁrst coeﬃcient vector C(0) consists of the k DC
coeﬃcients.
4. Each vector C(j) is quantized separately to produce a vector Q(j) of quantized
coeﬃcients (JPEG does this diﬀerently; see Section 5.6.3). The elements of Q(j) are
then written on the output. In practice, variable-length codes are assigned to the ele-
ments, and the codes, rather than the elements themselves, are written on the output.
Sometimes, as in the case of JPEG, variable-length codes are assigned to runs of zero
coeﬃcients, to achieve better compression.
In practice, the DCT is used for lossy compression. For lossless compression (where
the DCT coeﬃcients are not quantized) the DCT is ineﬃcient but can still be used, at
least theoretically, because (1) most of the coeﬃcients are small numbers and (2) there
are often runs of zero coeﬃcients. However, the small coeﬃcients are real numbers, not
integers, so it is not clear how to write them in full precision on the output and still
achieve compression. Other image compression methods are better suited for lossless
image compression.
The decoder reads the 64 quantized coeﬃcient vectors Q(j) of k elements each, saves
them as the columns of a matrix, and considers the k rows of the matrix weight vectors
W (i) of 64 elements each (notice that these W (i) are not identical to the original W (i)
because of the quantization). It then applies the IDCT [Equation (5.7)] to each weight
vector, to reconstruct (approximately) the 64 pixels of block Bi. (Again, JPEG does
this diﬀerently.)
We illustrate the performance of the DCT in two dimensions by applying it to two
blocks of 8 × 8 values. The ﬁrst block (Table 5.8a) has highly correlated integer values
in the range [8, 12], and the second block has random values in the same range. The
ﬁrst block results in a large DC coeﬃcient, followed by small AC coeﬃcients (including
20 zeros, Table 5.8b, where negative numbers are underlined). When the coeﬃcients are
quantized (Table 5.8c), the result, shown in Table 5.8d, is very similar to the original
values. In contrast, the coeﬃcients for the second block (Table 5.9b) include just one
zero. When quantized (Table 5.9c) and transformed back, many of the 64 results are
very diﬀerent from the original values (Table 5.9d).
⋄Exercise 5.6: Explain why the 64 values of Table 5.8a are correlated.
The next example illustrates the diﬀerence in the performance of the DCT when
applied to a continuous-tone image and to a discrete-tone image. We start with the
highly correlated pattern of Table 5.10. This is an idealized example of a continuous-tone
image, since adjacent pixels diﬀer by a constant amount except the pixel (underlined) at
row 7, column 7. The 64 DCT coeﬃcients of this pattern are listed in Table 5.11. It is

5.5 The Discrete Cosine Transform
165
12 10
8 10 12 10
8 11
11 12 10
8 10 12 10
8
8 11 12 10
8 10 12 10
10
8 11 12 10
8 10 12
12 10
8 11 12 10
8 10
10 12 10
8 11 12 10
8
8 10 12 10
8 11 12 10
10
8 10 12 10
8 11 12
81
0
0
0
0
0
0
0
0 1.57 0.61 1.90 0.38 1.81 0.20 0.32
0 0.61 0.71 0.35
0 0.07
0 0.02
0 1.90 0.35 4.76 0.77 3.39 0.25 0.54
0 0.38
0 0.77 8.00 0.51
0 0.07
0 1.81 0.07 3.39 0.51 1.57 0.56 0.25
0 0.20
0 0.25
0 0.56 0.71 0.29
0 0.32 0.02 0.54 0.07 0.25 0.29 0.90
(a) Original data
(b) DCT coeﬃcients
81 0 0 0 0 0 0 0
0 2 1 2 0 2 0 0
0 1 1 0 0 0 0 0
0 2 0 5 1 3 0 1
0 0 0 1 8 1 0 0
0 2 0 3 1 2 1 0
0 0 0 0 0 1 1 0
0 0 0 1 0 0 0 1
12.29 10.26
7.92
9.93 11.51
9.94
8.18 10.97
10.90 12.06 10.07
7.68 10.30 11.64 10.17
8.18
7.83 11.39 12.19
9.62
8.28 10.10 11.64
9.94
10.15
7.74 11.16 11.96
9.90
8.28 10.30 11.51
12.21 10.08
8.15 11.38 11.96
9.62
7.68
9.93
10.09 12.10
9.30
8.15 11.16 12.19 10.07
7.92
7.87
9.50 12.10 10.08
7.74 11.39 12.06 10.26
9.66
7.87 10.09 12.21 10.15
7.83 10.90 12.29
(c) Quantized
(d) Reconstructed data (good)
Table 5.8: Two-Dimensional DCT of a Block of Correlated Values.
8 10
9 11 11
9
9 12
11
8 12
8 11 10 11 10
9 11
9 10 12
9
9
8
9 12 10
8
8
9
8
9
12
8
9
9 12 10
8 11
8 11 10 12
9 12 12 10
10 10 12 10 12 10 10 12
12
9 11 11
9
8
8 12
79.12 0.98 0.64 1.51 0.62 0.86 1.22 0.32
0.15 1.64 0.09 1.23 0.10 3.29 1.08 2.97
1.26 0.29 3.27 1.69 0.51 1.13 1.52 1.33
1.27 0.25 0.67 0.15 1.63 1.94 0.47 1.30
2.12 0.67 0.07 0.79 0.13 1.40 0.16 0.15
2.68 1.08 1.99 1.93 1.77 0.35
0 0.80
1.20 2.10 0.98 0.87 1.55 0.59 0.98 2.76
2.24 0.55 0.29 0.75 2.40 0.05 0.06 1.14
(a) Original data
(b) DCT coeﬃcients
79 1 1 2 1
1 1
0
0 2 0 1 0
3 1
3
1 0 3 2 0
1 2
1
1 0 1 0 2
2 0 10
20 1 0 1 0 10 0
0
3 1 2 2 2
0 0
1
1 2 1 1 2
1 1
3
2 1 0 1 2
0 0
1
7.59
9.23
8.33 11.88
7.12 12.47
6.98
8.56
12.09
7.97
9.3 11.52
9.28 11.62 10.98 12.39
11.02 10.06 13.81
6.5 10.82
8.28 13.02
7.54
8.46 10.22 11.16
9.57
8.45
7.77 10.28 11.89
9.71 11.93
8.04
9.59
8.04
9.7
8.59 12.14
10.27 13.58
9.21 11.83
9.99 10.66
7.84 11.27
8.34 10.32 10.53
9.9
8.31
9.34
7.47
8.93
10.61
9.04 13.66
6.04 13.47
7.65 10.97
8.89
(c) Quantized
(d) Reconstructed data (bad)
Table 5.9: Two-Dimensional DCT of a Block of Random Values.
clear that there are only a few dominant coeﬃcients. Table 5.12 lists the coeﬃcients after
they have been coarsely quantized, so that only four nonzero coeﬃcients remain! The
results of performing the IDCT on these quantized coeﬃcients are shown in Table 5.13.
It is obvious that the four nonzero coeﬃcients have reconstructed the original pattern to
a high degree. The only visible diﬀerence is in row 7, column 7, which has changed from
12 to 17.55 (marked in both ﬁgures). The Matlab code for this computation is listed in
Figure 5.18.
Tables 5.14 through 5.17 show the same process applied to a Y-shaped pattern,
typical of a discrete-tone image. The quantization, shown in Table 5.16, is light. The
coeﬃcients have only been truncated to the nearest integer. It is easy to see that the
reconstruction, shown in Table 5.17, isn’t as good as before. Quantities that should have
been 10 are between 8.96 and 10.11. Quantities that should have been zero are as big
as 0.86. The conclusion is that the DCT performs well on continuous-tone images but
is less eﬃcient when applied to a discrete-tone image.

166
5.
Image Compression
00
10
20
30
30
20
10
00
10
20
30
40
40
30
20
10
20
30
40
50
50
40
30
20
30
40
50
60
60
50
40
30
30
40
50
60
60
50
40
30
20
30
40
50
50
40
30
20
10
20
30
40
40
30
12
10
00
10
20
30
30
20
10
00
12
Table 5.10: A Continuous-Tone Pattern.
239
1.19
−89.76
−0.28
1.00
−1.39
−5.03
−0.79
1.18
−1.39
0.64
0.32
−1.18
1.63
−1.54
0.92
−89.76
0.64
−0.29
−0.15
0.54
−0.75
0.71
−0.43
−0.28
0.32
−0.15
−0.08
0.28
−0.38
0.36
−0.22
1.00
−1.18
0.54
0.28
−1.00
1.39
−1.31
0.79
−1.39
1.63
−0.75
−0.38
1.39
−1.92
1.81
−1.09
−5.03
−1.54
0.71
0.36
−1.31
1.81
−1.71
1.03
−0.79
0.92
−0.43
−0.22
0.79
−1.09
1.03
−0.62
Table 5.11: Its DCT Coeﬃcients.
239
1
-90
0
0
0
0
0
0
0
0
0
0
0
0
0
-90
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
Table 5.12: Quantized Heavily to Just Four Nonzero Coeﬃcients.
0.65
9.23 21.36 29.91 29.84 21.17
8.94
0.30
9.26 17.85 29.97 38.52 38.45 29.78 17.55
8.91
21.44 30.02 42.15 50.70 50.63 41.95 29.73 21.09
30.05 38.63 50.76 59.31 59.24 50.56 38.34 29.70
30.05 38.63 50.76 59.31 59.24 50.56 38.34 29.70
21.44 30.02 42.15 50.70 50.63 41.95 29.73 21.09
9.26 17.85 29.97 38.52 38.45 29.78 17.55
8.91
0.65
9.23 21.36 29.91 29.84 21.17
8.94
0.30
17
Table 5.13: Results of IDCT.

5.5 The Discrete Cosine Transform
167
00
10
00
00
00
00
00
10
00
00
10
00
00
00
10
00
00
00
00
10
00
10
00
00
00
00
00
00
10
00
00
00
00
00
00
00
10
00
00
00
00
00
00
00
10
00
00
00
00
00
00
00
10
00
00
00
00
00
00
00
10
00
00
00
Table 5.14: A Discrete-Tone Image (Y).
13.75
−3.11
−8.17
2.46
3.75
−6.86
−3.38
6.59
4.19
−0.29
6.86
−6.85
−7.13
4.48
1.69
−7.28
1.63
0.19
6.40
−4.81
−2.99
−1.11
−0.88
−0.94
−0.61
0.54
5.12
−2.31
1.30
−6.04
−2.78
3.05
−1.25
0.52
2.99
−0.20
3.75
−7.39
−2.59
1.16
−0.41
0.18
0.65
1.03
3.87
−5.19
−0.71
−4.76
0.68
−0.15
−0.88
1.28
2.59
−1.92
1.10
−9.05
0.83
−0.21
−0.99
0.82
1.13
−0.08
1.31
−7.21
Table 5.15: Its DCT Coeﬃcients.
13.75
−3
−8
2
3
−6
−3
6
4
−0
6
−6
−7
4
1
−7
1
0
6
−4
−2
−1
−0
−0
−0
0
5
−2
1
−6
−2
3
−1
0
2
−0
3
−7
−2
1
−0
0
0
1
3
−5
−0
−4
0
−0
−0
1
2
−1
1
−9
0
−0
−0
0
1
−0
1
−7
Table 5.16: Quantized Lightly by Truncating to Integer.
-0.13
8.96
0.55 -0.27
0.27
0.86
0.15
9.22
0.32
0.22
9.10
0.40
0.84 -0.11
9.36 -0.14
0.00
0.62 -0.20
9.71
-1.30
8.57
0.28 -0.33
-0.58
0.44
0.78
0.71 10.11
1.14
0.44 -0.49
-0.39
0.67
0.07
0.38
8.82
0.09
0.28
0.41
0.34
0.11
0.26
0.18
8.93
0.41
0.47
0.37
0.09 -0.32
0.78 -0.20
9.78
0.05 -0.09
0.49
0.16 -0.83
0.09
0.12
9.15 -0.11 -0.08
0.01
Table 5.17: The IDCT. Bad Results.

168
5.
Image Compression
% 8x8 correlated values
n=8;
p=[00,10,20,30,30,20,10,00; 10,20,30,40,40,30,20,10; 20,30,40,50,50,40,30,20; ...
30,40,50,60,60,50,40,30; 30,40,50,60,60,50,40,30; 20,30,40,50,50,40,30,20; ...
10,20,30,40,40,30,12,10; 00,10,20,30,30,20,10,00];
figure(1), imagesc(p), colormap(gray), axis square, axis off
dct=zeros(n,n);
for j=0:7
for i=0:7
for x=0:7
for y=0:7
dct(i+1,j+1)=dct(i+1,j+1)+p(x+1,y+1)*cos((2*y+1)*j*pi/16)*cos((2*x+1)*i*pi/16);
end;
end;
end;
end;
dct=dct/4; dct(1,:)=dct(1,:)*0.7071; dct(:,1)=dct(:,1)*0.7071;
dct
quant=[239,1,-90,0,0,0,0,0; 0,0,0,0,0,0,0,0; -90,0,0,0,0,0,0,0; 0,0,0,0,0,0,0,0; ...
0,0,0,0,0,0,0,0; 0,0,0,0,0,0,0,0; 0,0,0,0,0,0,0,0; 0,0,0,0,0,0,0,0];
idct=zeros(n,n);
for x=0:7
for y=0:7
for i=0:7
if i==0 ci=0.7071; else ci=1; end;
for j=0:7
if j==0 cj=0.7071; else cj=1; end;
idct(x+1,y+1)=idct(x+1,y+1)+ ...
ci*cj*quant(i+1,j+1)*cos((2*y+1)*j*pi/16)*cos((2*x+1)*i*pi/16);
end;
end;
end;
end;
idct=idct/4;
idct
figure(2), imagesc(idct), colormap(gray), axis square, axis off
Figure 5.18: Code for Highly Correlated Pattern.
5.5.2 The DCT as a Basis
The discussion so far has concentrated on how to use the DCT for compressing one-
dimensional and two-dimensional data.
The aim of this section is to show why the
DCT works the way it does and how Equations (5.4) and (5.6) were derived.
This
section interprets the DCT as a special basis of an n-dimensional vector space. We show
that transforming a given data vector p by the DCT is equivalent to representing it by
this special basis that isolates the various frequencies contained in the vector. Thus,
the DCT coeﬃcients resulting from the DCT transform of vector p indicate the various
frequencies in the vector. The lower frequencies contain the important visual information
in p, whereas the higher frequencies correspond to the details of the data in p and are
therefore less important. This is why they can be quantized coarsely. (What visual
information is important and what is unimportant is determined by the peculiarities of
the human visual system.) We illustrate this interpretation for n = 3, because this is the
largest number of dimensions where it is possible to visualize geometric transformations.
[Note. It is also possible to interpret the DCT as a rotation, as shown intuitively
for n = 2 (two-dimensional points) in Figure 5.4. This interpretation [Salomon 07] con-

5.5 The Discrete Cosine Transform
169
siders the DCT as a rotation matrix that rotates an n-dimensional point with identical
coordinates (x, x, . . . , x) from its original location to the x-axis, where its coordinates
become (α, ϵ2, . . . , ϵn) where the various ϵi are small numbers or zeros.]
For the special case n = 3, Equation (5.4) reduces to
Gf =

2
3Cf
2

t=0
pt cos
(2t + 1)fπ
6

,
for f = 0, 1, 2.
Temporarily ignoring the normalization factors

2/3 and Cf, this can be written in
matrix notation as
⎡
⎣
G0
G1
G2
⎤
⎦=
⎡
⎣
cos 0
cos 0
cos 0
cos π
6
cos 3π
6
cos 5π
6
cos 2 π
6
cos 2 3π
6
cos 2 5π
6
⎤
⎦
⎡
⎣
p0
p1
p2
⎤
⎦= D · p.
Thus, the DCT of the three data values p = (p0, p1, p2) is obtained as the product of
the DCT matrix D and the vector p. We can therefore think of the DCT as the product
of a DCT matrix and a data vector, where the matrix is constructed as follows: Select
the three angles π/6, 3π/6, and 5π/6 and compute the three basis vectors cos(fθ) for
f = 0, 1, and 2, and for the three angles. The results are listed in Table 5.19 for the
beneﬁt of the reader.
θ
0.5236
1.5708
2.618
cos 0θ
1
1
1
cos 1θ
0.866
0
−0.866
cos 2θ
0.5
−1
0.5
Table 5.19: The DCT Matrix for n = 3.
Because of the particular choice of the three angles, these vectors are orthogonal but
not orthonormal. Their magnitudes are
√
3,
√
1.5, and
√
1.5, respectively. Normalizing
them results in the three vectors v1 = (0.5774, 0.5774, 0.5774), v2 = (0.7071, 0, −0.7071),
and v3 = (0.4082, −0.8165, 0.4082). When stacked vertically, they produce the following
3×3 matrix
M =
⎡
⎣
0.5774
0.5774
0.5774
0.7071
0
−0.7071
0.4082
−0.8165
0.4082
⎤
⎦.
(5.8)
[Equation (5.4) tells us how to normalize these vectors: Multiply each by

2/3, and then
multiply the ﬁrst by 1/
√
2.] Notice that as a result of the normalization the columns of
M have also become orthonormal, so M is an orthonormal matrix (such matrices have
special properties).
The steps of computing the DCT matrix for an arbitrary n are as follows:
1. Select the n angles θj = (j +0.5)π/n for j = 0, . . . , n−1. If we divide the interval
[0, π] into n equal-size segments, these angles are the centerpoints of the segments.

170
5.
Image Compression
2. Compute the n vectors vk for k = 0, 1, 2, . . . , n −1, each with the n components
cos(kθj).
3. Normalize each of the n vectors and arrange them as the n rows of a matrix.
The angles selected for the DCT are θj = (j + 0.5)π/n, so the components of each
vector vk are cos[k(j + 0.5)π/n] or cos[k(2j + 1)π/(2n)]. Reference [Salomon 07] covers
three other ways to select such angles. This choice of angles has the following useful
properties (1) the resulting vectors are orthogonal, and (2) for increasing values of k,
the n vectors vk contain increasing frequencies (Figure 5.20). For n = 3, the top row
of M [Equation (5.8)] corresponds to zero frequency, the middle row (whose elements
become monotonically smaller) represents low frequency, and the bottom row (with
three elements that ﬁrst go down, then up) represents high frequency. Given a three-
dimensional vector v = (v1, v2, v3), the product M·v is a triplet whose components
indicate the magnitudes of the various frequencies included in v; they are frequency
coeﬃcients. [Strictly speaking, the product is M·vT , but we ignore the transpose in
cases where the meaning is clear.] The following three extreme examples illustrate the
meaning of this statement.
1
1
−0.5
−0.5
2
2
3
1.5
1.5
2.5
Figure 5.20: Increasing Frequencies.
The ﬁrst example is v = (v, v, v). The three components of v are identical, so they
correspond to zero frequency. The product M·v produces the frequency coeﬃcients
(1.7322v, 0, 0), indicating no high frequencies. The second example is v = (v, 0, −v).
The three components of v vary slowly from v to −v, so this vector contains a low
frequency. The product M·v produces the coeﬃcients (0, 1.4142v, 0), conﬁrming this
result.
The third example is v = (v, −v, v).
The three components of v vary from
v to −v to v, so this vector contains a high frequency. The product M·v produces
(0, 0, 1.6329v), again indicating the correct frequency.
These examples are not very realistic because the vectors being tested are short,
simple, and contain a single frequency each. Most vectors are more complex and contain
several frequencies, which makes this method useful. A simple example of a vector with
two frequencies is v = (1, 0.33, −0.34). The product M·v results in (0.572, 0.948, 0)
which indicates a large medium frequency, small zero frequency, and no high frequency.
This makes sense once we realize that the vector being tested is the sum 0.33(1, 1, 1) +
0.67(1, 0, −1). A similar example is the sum 0.9(−1, 1, −1)+0.1(1, 1, 1) = (−0.8, 1, −0.8),
which when multiplied by M produces (−0.346, 0, −1.469). On the other hand, a vector
with random components, such as (1, 0, 0.33), typically contains roughly equal amounts
of all three frequencies and produces three large frequency coeﬃcients. The product

5.5 The Discrete Cosine Transform
171
M·(1, 0, 0.33) produces (0.77, 0.47, 0.54) because (1, 0, 0.33) is the sum
0.33(1, 1, 1) + 0.33(1, 0, −1) + 0.33(1, −1, 1).
Notice that if M·v = c, then MT ·c = M−1 ·c = v. The original vector v can
therefore be reconstructed from its frequency coeﬃcients (up to small diﬀerences due to
the limited precision of machine arithmetic). The inverse M−1 of M is also its transpose
MT because M is orthonormal.
A three-dimensional vector can have only three frequencies, namely zero, medium,
and high. Similarly, an n-dimensional vector can have n diﬀerent frequencies, which this
method can identify. We concentrate on the case n = 8 and start with the DCT in one
dimension. Figure 5.21 shows eight cosine waves of the form cos(fθj), for 0 ≤θj ≤π,
with frequencies f = 0, 1, . . . , 7. Each wave is sampled at the eight points
θj = π
16,
3π
16 ,
5π
16 ,
7π
16 ,
9π
16 ,
11π
16 ,
13π
16 ,
15π
16
(5.9)
to form one basis vector vf, and the resulting eight vectors vf, f = 0, 1, . . . , 7 (a total
of 64 numbers) are shown in Table 5.22. They serve as the basis matrix of the DCT.
Notice the similarity between this table and matrix W of Equation (5.3).
Because of the particular choice of the eight sample points, the vi are orthogonal,
which is easy to check directly with appropriate mathematical software. After normal-
ization, the vi can be considered either as an 8×8 transformation matrix (speciﬁcally,
a rotation matrix, since it is orthonormal) or as a set of eight orthogonal vectors that
constitute the basis of a vector space. Any vector p in this space can be expressed as a
linear combination of the vi. As an example, we select the eight (correlated) numbers
p = (0.6, 0.5, 0.4, 0.5, 0.6, 0.5, 0.4, 0.55) as our test data and express p as a linear combi-
nation p =  wivi of the eight basis vectors vi. Solving this system of eight equations
yields the eight weights
w0 = 0.506,
w1 = 0.0143,
w2 = 0.0115,
w3 = 0.0439,
w4 = 0.0795,
w5 = −0.0432,
w6 = 0.00478,
w7 = −0.0077.
Weight w0 is not much diﬀerent from the elements of p, but the other seven weights
are much smaller. This is how the DCT (or any other orthogonal transform) can lead
to compression. The eight weights can be quantized and written on the output, where
they occupy less space than the eight components of p.
Figure 5.23 illustrates this linear combination graphically. Each of the eight vi is
shown as a row of eight small, gray rectangles (a basis image) where a value of +1 is
painted white and −1 is black. The eight elements of vector p are also displayed as a
row of eight grayscale pixels.
To summarize, we interpret the DCT in one dimension as a set of basis images
that have higher and higher frequencies. Given a data vector, the DCT separates the
frequencies in the data and represents the vector as a linear combination (or a weighted
sum) of the basis images. The weights are the DCT coeﬃcients. This interpretation can
be extended to the DCT in two dimensions. We apply Equation (5.6) to the case n = 8

172
5.
Image Compression
1.5
2
2.5
3
3
3
1
0.5
1.5
2
2.5
1
0.5
1.5
2
2.5
1
0.5
3
1.5
2
2.5
1
0.5
3
1.5
2
2.5
1
0.5
3
1.5
2
2.5
1
0.5
3
1.5
2
2.5
1
0.5
1.5
2
2.5
3
1
0.5
−1
−0.5
0.5
1
−1
−0.5
0.5
1
−1
−0.5
0.5
1
−1
−0.5
0.5
1
−1
−0.5
0.5
1
−1
−0.5
0.5
1
−1
−0.5
0.5
1
1
0.5
1.5
2
Figure 5.21: Angle and Cosine Values for an 8-Point DCT.

5.5 The Discrete Cosine Transform
173
dct[pw_]:=Plot[Cos[pw t], {t,0,Pi}, DisplayFunction->Identity,
AspectRatio->Automatic];
dcdot[pw_]:=ListPlot[Table[{t,Cos[pw t]},{t,Pi/16,15Pi/16,Pi/8}],
DisplayFunction->Identity]
Show[dct[0],dcdot[0], Prolog->AbsolutePointSize[4],
DisplayFunction->$DisplayFunction]
...
Show[dct[7],dcdot[7], Prolog->AbsolutePointSize[4],
DisplayFunction->$DisplayFunction]
Code for Figure 5.21.
θ
0.196
0.589
0.982
1.374
1.767
2.160
2.553
2.945
cos 0θ
1
1
1
1
1
1
1
1
cos 1θ
0.981
0.831
0.556
0.195
−0.195
−0.556
−0.831
−0.981
cos 2θ
0.924
0.383
−0.383
−0.924
−0.924
−0.383
0.383
0.924
cos 3θ
0.831
−0.195
−0.981
−0.556
0.556
0.981
0.195
−0.831
cos 4θ
0.707
−0.707
−0.707
0.707
0.707
−0.707
−0.707
0.707
cos 5θ
0.556
−0.981
0.195
0.831
−0.831
−0.195
0.981
−0.556
cos 6θ
0.383
−0.924
0.924
−0.383
−0.383
0.924
−0.924
0.383
cos 7θ
0.195
−0.556
0.831
−0.981
0.981
−0.831
0.556
−0.195
Table 5.22: The Unnormalized DCT Matrix in One Dimension for n = 8.
Table[N[t],{t,Pi/16,15Pi/16,Pi/8}]
dctp[pw_]:=Table[N[Cos[pw t]],{t,Pi/16,15Pi/16,Pi/8}]
dctp[0]
dctp[1]
...
dctp[7]
Code for Table 5.22.
0.506
0.0143
0.0115
0.0439
0.0795
−0.0432
0.00478
−0.0077
p
v0
v2
v4
v6
v7
wi
Figure 5.23: A Graphic Representation of the One-Dimensional DCT.

174
5.
Image Compression
to create 64 small basis images of 8 × 8 pixels each. The 64 images are then used as a
basis of a 64-dimensional vector space. Any image B of 8 × 8 pixels can be expressed as
a linear combination of the basis images, and the 64 weights of this linear combination
are the DCT coeﬃcients of B.
Figure 5.24 shows the graphic representation of the 64 basis images of the two-
dimensional DCT for n = 8. A general element (i, j) in this ﬁgure is the 8 × 8 image
obtained by calculating the product cos(i·s) cos(j ·t), where s and t are varied indepen-
dently over the values listed in Equation (5.9) and i and j vary from 0 to 7. This ﬁgure
can easily be generated by the Mathematica code shown with it. The alternative code
shown is a modiﬁcation of code in [Watson 94], and it requires the GraphicsImage.m
package, which is not widely available.
Using appropriate software, it is easy to perform DCT calculations and display the
results graphically. Figure 5.25a shows a random 8×8 data unit consisting of zeros and
ones. The same unit is shown in Figure 5.25b graphically, with 1 as white and 0 as
black. Figure 5.25c shows the weights by which each of the 64 DCT basis images has to
be multiplied in order to reproduce the original data unit. In this ﬁgure, zero is shown
in neutral gray, positive numbers are bright (notice how bright the DC weight is), and
negative numbers are shown as dark. Figure 5.25d shows the weights numerically. The
Mathematica code that does all that is also listed. Figure 5.26 is similar, but for a very
regular data unit.
⋄Exercise 5.7: Imagine an 8×8 block of values where all the odd-numbered rows consist
of 1’s and all the even-numbered rows contain zeros. What can we say about the DCT
weights of this block?
It must be an even-numbered day. I do so prefer the odd-numbered days when you’re
kissing my *** for a favor.
—From Veronica Mars (a television program)

5.5 The Discrete Cosine Transform
175
Figure 5.24: The 64 Basis Images of the Two-Dimensional DCT.
dctp[fs_,ft_]:=Table[SetAccuracy[N[(1.-Cos[fs s]Cos[ft t])/2],3],
{s,Pi/16,15Pi/16,Pi/8},{t,Pi/16,15Pi/16,Pi/8}]//TableForm
dctp[0,0]
dctp[0,1]
...
dctp[7,7]
Code for Figure 5.24.
Needs["GraphicsImage‘"] (* Draws 2D DCT Coefficients *)
DCTMatrix=Table[If[k==0,Sqrt[1/8],Sqrt[1/4]Cos[Pi(2j+1)k/16]],
{k,0,7}, {j,0,7}] //N;
DCTTensor=Array[Outer[Times, DCTMatrix[[#1]],DCTMatrix[[#2]]]&,
{8,8}];
Show[GraphicsArray[Map[GraphicsImage[#, {-.25,.25}]&, DCTTensor,{2}]]]
Alternative Code for Figure 5.24.

176
5.
Image Compression
10011101
11001011
01100100
00010010
01001011
11100110
11001011
01010010
(a)
4.000
−0.133
0.637
0.272
−0.250
−0.181
−1.076
0.026
0.081
−0.178
−0.300
0.230
0.694
−0.309
0.875
−0.127
0.462
0.125
0.095
0.291
0.868
−0.070
0.021
−0.280
0.837
−0.194
0.455
0.583
0.588
−0.281
0.448
0.383
−0.500
−0.635
−0.749
−0.346
0.750
0.557
−0.502
−0.540
−0.167
0
−0.366
0.146
0.393
0.448
0.577
−0.268
−0.191
0.648
−0.729
−0.008
−1.171
0.306
1.155
−0.744
0.122
−0.200
0.038
−0.118
0.138
−1.154
0.134
0.148
(d)
Figure 5.25: An Example of the DCT in Two Dimensions.
DCTMatrix=Table[If[k==0,Sqrt[1/8],Sqrt[1/4]Cos[Pi(2j+1)k/16]],
{k,0,7}, {j,0,7}] //N;
DCTTensor=Array[Outer[Times, DCTMatrix[[#1]],DCTMatrix[[#2]]]&,
{8,8}];
img={{1,0,0,1,1,1,0,1},{1,1,0,0,1,0,1,1},
{0,1,1,0,0,1,0,0},{0,0,0,1,0,0,1,0},
{0,1,0,0,1,0,1,1},{1,1,1,0,0,1,1,0},
{1,1,0,0,1,0,1,1},{0,1,0,1,0,0,1,0}};
ShowImage[Reverse[img]]
dctcoeff=Array[(Plus @@ Flatten[DCTTensor[[#1,#2]] img])&,{8,8}];
dctcoeff=SetAccuracy[dctcoeff,4];
dctcoeff=Chop[dctcoeff,.001];
MatrixForm[dctcoeff]
ShowImage[Reverse[dctcoeff]]
Code for Figure 5.25.

5.5 The Discrete Cosine Transform
177
4.000
−0.721
0
−0.850
0
−1.273
0
−3.625
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
(d)
Figure 5.26: An Example of the DCT in Two Dimensions.
Some painters transform the sun into a yellow spot; others trans-
form a yellow spot into the sun.
—Pablo Picasso
DCTMatrix=Table[If[k==0,Sqrt[1/8],Sqrt[1/4]Cos[Pi(2j+1)k/16]],
{k,0,7}, {j,0,7}] //N;
DCTTensor=Array[Outer[Times, DCTMatrix[[#1]],DCTMatrix[[#2]]]&,
{8,8}];
img={{0,1,0,1,0,1,0,1},{0,1,0,1,0,1,0,1},
{0,1,0,1,0,1,0,1},{0,1,0,1,0,1,0,1},{0,1,0,1,0,1,0,1},
{0,1,0,1,0,1,0,1},{0,1,0,1,0,1,0,1},{0,1,0,1,0,1,0,1}};
ShowImage[Reverse[img]]
dctcoeff=Array[(Plus @@ Flatten[DCTTensor[[#1,#2]] img])&,{8,8}];
dctcoeff=SetAccuracy[dctcoeff,4];
dctcoeff=Chop[dctcoeff,.001];
MatrixForm[dctcoeff]
ShowImage[Reverse[dctcoeff]]
Code for Figure 5.26.

178
5.
Image Compression
Statistical Distributions.
Most people are of medium height,
relatively few are tall or short,
and very few are giants or dwarves.
Imagine an experiment where we
measure the heights of thousands
of adults and want to summarize
the results graphically. One way
to do this is to go over the heights,
from the smallest to the largest
in steps of, say, 1 cm, and for
each height h determine the num-
ber ph of people who have this
height.
Now consider the pair
(h, ph) as a point, plot the points
for all the values of h, and con-
nect them with a smooth curve. The result will resemble the solid graph in the ﬁgure,
except that it will be centered on the average height, not on zero. Such a representation
of data is known as a statistical distribution.
<< Statistics‘ContinuousDistributions‘
g1=Plot[PDF[NormalDistribution[0,1], x], {x,-5,5}]
g2=Plot[PDF[LaplaceDistribution[0,1], x], {x,-5,5},
PlotStyle->{AbsoluteDashing[{5,5}]}]
Show[g1, g2]
The particular distribution of people’s heights is centered about the average height,
not about zero, and is called a Gaussian (after its originator, Carl F. Gauss) or a normal
distribution. The Gaussian distribution with mean m and standard deviation s is deﬁned
as
f(x) =
1
s
√
2π exp

−1
2
x −m
s
2 
.
This function has a maximum for x = m (i.e., at the mean), where its value is f(m) =
1/(s
√
2π). It is also symmetric about x = m, since it depends on x according to (x−m)2
and has a bell shape. The total area under the normal curve is one unit.
The normal distribution is encountered in many real-life situations and in science.
It’s easy to convince ourselves that people’s heights, weights, and income are distributed
in this way. Other examples are the following:
The speed of gas molecules. The molecules of a gas are in constant motion. They
move randomly, collide with each other and with objects around them, and change their
velocities all the time. However, most molecules in a given volume of gas move at about
the same speed, and only a few move much faster or much slower than this speed. This
speed is related to the temperature of the gas. The higher this average speed, the hotter
the gas feels to us. (This example is asymmetric, since the minimum speed is zero, but
the maximum speed can be very high.)
Chˆateau Chambord in the Loire valley of France has a magniﬁcent staircase, de-
signed by Leonardo da Vinci in the form of a double ramp spiral. Worn out by the
innumerable footsteps of generations of residents and tourists, the marble tread of this

5.6 JPEG
179
staircase now looks like an inverted normal distribution curve. It is worn mostly in
the middle, were the majority of people tend to step, and the wear tapers oﬀto either
side from the center. This staircase, and others like it, are physical embodiments of the
abstract mathematical concept of probability distribution.
Prime numbers are familiar to most people. They are attractive and important to
mathematicians because any positive integer can be expressed as a product of prime
numbers (its prime factors) in one way only. The prime numbers are thus the building
blocks from which all other integers can be constructed. It turns out that the number of
distinct prime factors is distributed normally. Few integers have just one or two distinct
prime factors, few integers have many distinct prime factors, while most integers have a
small number of distinct prime factors. This is known as the Erd˝os–Kac theorem.
The Laplace probability distribution is similar to the normal distribution, but is
narrower and sharply peaked. It is shown dashed in the ﬁgure. The general Laplace
distribution with variance V and mean m is given by
L(V, x) =
1
√
2V
exp
!
−

2
V |x −m|
"
.
The factor 1/
√
2V is included in the deﬁnition in order to scale the area under the curve
to 1.
Some people claim that Canada is a very boring country. There are no great com-
posers, poets, philosophers, scientists, artists, or writers whose names are inextricably
associated with Canada. Similarly, no Canadian plays, stories, or traditional legends
are as well-known as the Shakespeare plays, Grimm brothers’ stories, or Icelandic sagas.
However, I once heard that the following simple game may be considered Canada’s na-
tional game. Two players start with a set of 15 matches (they don’t have to be smokers)
and take turns. In each turn, a player removes between 1 and 4 matches. The player
removing the last match wins. Your task is to devise a winning strategy for this game
and publicize it throughout Canada. This winning strategy should not depend on any
of the players being Canadian.
5.6 JPEG
JPEG is a sophisticated lossy/lossless compression method for color or grayscale still
images (not videos). It does not handle bi-level (black and white) images very well. It
also works best on continuous-tone images, where adjacent pixels tend to have similar
colors. An important feature of JPEG is its use of many parameters, allowing the user to
adjust the amount of the data lost (and thus also the compression ratio) over a very wide
range. Often, the eye cannot see any image degradation even at compression factors of
10 or 20. There are two operating modes, lossy (also called baseline) and lossless (which
typically produces compression ratios of around 0.5). Most implementations support
just the lossy mode. This mode includes progressive and hierarchical coding. A few

180
5.
Image Compression
of the many references to JPEG are [Pennebaker and Mitchell 92], [Wallace 91], and
[Zhang 90].
JPEG is a compression method, not a complete standard for image representation.
This is why it does not specify image features such as pixel aspect ratio, color space, or
interleaving of bitmap rows.
JPEG has been designed as a compression method for continuous-tone images. The
main goals of JPEG compression are the following:
1. High compression ratios, especially in cases where image quality is judged as very
good to excellent.
2. The use of many parameters, allowing knowledgeable users to experiment and achieve
the desired compression/quality trade-oﬀ.
3. Obtaining good results with any kind of continuous-tone image, regardless of image
dimensions, color spaces, pixel aspect ratios, or other image features.
4. A sophisticated, but not too complex compression method, allowing software and
hardware implementations on many platforms.
5. Several modes of operation: (a) sequential mode where each image component (color)
is compressed in a single left-to-right, top-to-bottom scan; (b) progressive mode where
the image is compressed in multiple blocks (known as “scans”) to be viewed from coarse
to ﬁne detail; (c) lossless mode that is important in cases where the user decides that no
pixels should be lost (the trade-oﬀis low compression ratio compared to the lossy modes);
and (d) hierarchical mode where the image is compressed at multiple resolutions allowing
lower-resolution blocks to be viewed without ﬁrst having to decompress the following
higher-resolution blocks.
The name JPEG is an acronym that stands for Joint Photographic Experts Group.
This was a joint eﬀort by the CCITT and the ISO (the International Standards Or-
ganization) that started in June 1987 and produced the ﬁrst JPEG draft proposal in
1991. The JPEG standard has proved successful and has become widely used for image
compression, especially in Web pages.
The main JPEG compression steps are outlined here, and each step is then described
in detail in a later section.
1. Color images are transformed from RGB into a luminance/chrominance color space
(Section 5.6.1; this step is skipped for grayscale images). The eye is sensitive to small
changes in luminance but not in chrominance, so the chrominance part can later lose
much data, and thus be highly compressed, without visually impairing the overall image
quality much. This step is optional but important because the remainder of the algo-
rithm works on each color component separately. Without transforming the color space,
none of the three color components will tolerate much loss, leading to worse compression.
2. Color images are downsampled by creating low-resolution pixels from the original ones
(this step is used only when hierarchical compression is selected; it is always skipped
for grayscale images). The downsampling is not done for the luminance component.
Downsampling is done either at a ratio of 2:1 both horizontally and vertically (the so-
called 2h2v or 4:1:1 sampling) or at ratios of 2:1 horizontally and 1:1 vertically (2h1v
or 4:2:2 sampling).
Since this is done on two of the three color components, 2h2v
reduces the image to 1/3 + (2/3) × (1/4) = 1/2 its original size, while 2h1v reduces it
to 1/3 + (2/3) × (1/2) = 2/3 its original size. Since the luminance component is not

5.6 JPEG
181
touched, there is no noticeable loss of image quality. Grayscale images don’t go through
this step.
3. The pixels of each color component are organized in groups of 8×8 pixels called
data units, and each data unit is compressed separately. If the number of image rows or
columns is not a multiple of 8, the bottom row or the rightmost column are duplicated
as many times as necessary. In the noninterleaved mode, the encoder handles all the
data units of the ﬁrst image component, then the data units of the second component,
and ﬁnally those of the third component. In the interleaved mode, the encoder processes
the three top-left data units of the three image components, then the three data units
to their right, and so on. The fact that each data unit is compressed separately is one of
the downsides of JPEG. If the user asks for maximum compression, the decompressed
image may exhibit blocking artifacts due to diﬀerences between blocks. Figure 5.27 is
an extreme example of this eﬀect.
Figure 5.27: JPEG Blocking Artifacts.
4. The discrete cosine transform (DCT, Section 5.5) is then applied to each data unit
to create an 8×8 map of frequency components (Section 5.6.2). They represent the
average pixel value and successive higher-frequency changes within the group.
This
prepares the image data for the crucial step of losing information. Since DCT involves
the transcendental function cosine, it must involve some loss of information due to the
limited precision of computer arithmetic. This means that even without the main lossy
step (step 5 below), there will be some loss of image quality, but it is normally small.
5. Each of the 64 frequency components in a data unit is divided by a separate number
called its quantization coeﬃcient (QC), and then rounded to an integer (Section 5.6.3).
This is where information is irretrievably lost. Large QCs cause more loss, so the high-
frequency components typically have larger QCs. Each of the 64 QCs is a JPEG param-
eter and can, in principle, be speciﬁed by the user. In practice, most JPEG implemen-
tations use the QC tables recommended by the JPEG standard for the luminance and
chrominance image components (Table 5.30).
6. The 64 quantized frequency coeﬃcients (which are now integers) of each data unit are
encoded using a combination of RLE and Huﬀman coding (Section 5.6.4). An arithmetic
coding variant known as the QM coder can optionally be used instead of Huﬀman coding.
7.
The last step adds headers and all the required JPEG parameters, and outputs
the result.
The compressed ﬁle may be in one of three formats (1) the interchange

182
5.
Image Compression
format, in which the ﬁle contains the compressed image and all the tables needed by the
decoder (mostly quantization and Huﬀman codes tables); (2) the abbreviated format
for compressed image data, where the ﬁle contains the compressed image and either no
tables or just a few tables; and (3) the abbreviated format for table-speciﬁcation data,
where the ﬁle contains just tables, and no compressed image. The second format makes
sense in cases where the same encoder/decoder pair is used, and they have the same
tables built in. The third format is used where many images have been compressed by
the same encoder, using the same tables. When those images need to be decompressed,
they are sent to a decoder preceded by a ﬁle with table-speciﬁcation data.
The JPEG decoder performs the reverse steps (which shows that JPEG is a sym-
metric compression method).
The progressive mode is a JPEG option.
In this mode, higher-frequency DCT
coeﬃcients are written on the output in blocks called “scans.” Each scan that is read
and processed by the decoder results in a sharper image. The idea is to use the ﬁrst
few scans to quickly create a low-quality, blurred preview of the image, and then either
input the remaining scans or stop the process and reject the image. The trade-oﬀis
that the encoder has to save all the coeﬃcients of all the data units in a memory buﬀer
before they are sent in scans, and also go through all the steps for each scan, slowing
down the progressive mode.
Figure 5.28a shows an example of an image with resolution 1024×512. The image is
divided into 128×64 = 8192 data units, and each is transformed by the DCT, becoming
a set of 64 8-bit numbers. Figure 5.28b is a block whose depth corresponds to the 8,192
data units, whose height corresponds to the 64 DCT coeﬃcients (the DC coeﬃcient
is the top one, numbered 0), and whose width corresponds to the eight bits of each
coeﬃcient.
After preparing all the data units in a memory buﬀer, the encoder writes them on
the compressed ﬁle in one of two methods, spectral selection or successive approximation
(Figure 5.28c,d). The ﬁrst scan in either method is the set of DC coeﬃcients. If spectral
selection is used, each successive scan consists of several consecutive (a band of) AC
coeﬃcients. If successive approximation is used, the second scan consists of the four
most-signiﬁcant bits of all AC coeﬃcients, and each of the following four scans, numbers
3 through 6, adds one more signiﬁcant bit (bits 3 through 0, respectively).
In the hierarchical mode, the encoder stores the image several times in its output
ﬁle, at several resolutions. However, each high-resolution part uses information from the
low-resolution parts of the output ﬁle, so the total amount of information is less than
that required to store the diﬀerent resolutions separately. Each hierarchical part may
use the progressive mode.
The hierarchical mode is useful in cases where a high-resolution image needs to
be output in low resolution. Older dot-matrix printers may be a good example of a
low-resolution output device still in use.
The lossless mode of JPEG (Section 5.6.5) calculates a “predicted” value for each
pixel, generates the diﬀerence between the pixel and its predicted value, and encodes the
diﬀerence using the same method (i.e., Huﬀman or arithmetic coding) employed by step
5 above. The predicted value is calculated using values of pixels above and to the left
of the current pixel (pixels that have already been input and encoded). The following
sections discuss the steps in more detail.

5.6 JPEG
183
(c)
63
62
63
62
0
0
0
7 6
7
3
0
6 5 4
0
1
0
1
1
127
128
129
130
256
255
8192
8192
8191
8065
1
2
3
4
data units
1
2
0
1
2
2
(d)
(a)
(b)
63
62
0
1
2
63
62
0
1
2
63
62
0
1
2
7 6
0
1
1024=8×128
512=8×64
524,288 pixels
1st scan
1st scan
2nd scan
3rd scan
3rd scan
2nd scan
6th scan
kth scan
8,192 data units
Figure 5.28: Scans in the JPEG Progressive Mode.

184
5.
Image Compression
5.6.1 Luminance
The main international organization devoted to light and color is the International Com-
mittee on Illumination (Commission Internationale de l’´Eclairage), abbreviated CIE. It
is responsible for developing standards and deﬁnitions in this area. One of the early
achievements of the CIE was its chromaticity diagram [Salomon 99], developed in 1931.
It shows that no fewer than three parameters are required to deﬁne color. Expressing a
certain color by the triplet (x, y, z) is similar to denoting a point in three-dimensional
space, hence the term color space. The most common color space is RGB, where the
three parameters are the intensities of red, green, and blue in a color. When used in
computers, these parameters are normally in the range 0–255 (8 bits).
The CIE deﬁnes color as the perceptual result of light in the visible region of the
spectrum, having wavelengths in the region of 400 nm to 700 nm, incident upon the
retina (a nanometer, nm, equals 10−9 meter). Physical power (or radiance) is expressed
in a spectral power distribution (SPD), often in 31 components each representing a
10-nm band.
The CIE deﬁnes brightness as the attribute of a visual sensation according to which
an area appears to emit more or less light.
The brain’s perception of brightness is
impossible to deﬁne, so the CIE deﬁnes a more practical quantity called luminance. It is
deﬁned as radiant power weighted by a spectral sensitivity function that is characteristic
of vision (the eye is very sensitive to green, slightly less sensitive to red, and much less
sensitive to blue). The luminous eﬃciency of the Standard Observer is deﬁned by the CIE
as a positive function of the wavelength, which has a maximum at about 555 nm. When
a spectral power distribution is integrated using this function as a weighting function,
the result is CIE luminance, which is denoted by Y. Luminance is an important quantity
in the ﬁelds of digital image processing and compression.
Human Vision and Color. We see light that enters the eye and falls on the
retina, where there are two types of photosensitive cells. They contain pigments that
absorb visible light and hence give us the sense of vision.
One type of photosensitive cells is the rods, which are numerous, are spread all over
the retina, and respond only to light and dark. They are very sensitive and can respond
to a single photon of light. There are about 110,000,000 to 125,000,000 rods in the eye
[Osterberg 35]. The active substance in the rods is rhodopsin. A single photon can be
absorbed by a rhodopsin molecule which changes shape and chemically triggers a signal
that is transmitted to the optic nerve. Evolution, however, has protected us from too
much sensitivity to light and our brains require at least ﬁve to nine photons (arriving
within 100 ms) to create the sensation of light.
The other type is the cones, located in one small area of the retina (the fovea).
They number about 6,400,000, are sensitive to color, but require more intense light, on
the order of hundreds of photons. Incidentally, the cones are very sensitive to red, green,
and blue (Figure 5.29), which is one reason why these colors are often used as primaries.
In bright light, the cones become active, the rods are less so, and the iris is stopped
down. This is called photopic vision.

5.6 JPEG
185
We know that a dark environment improves our eyes’ sensitivity. When we enter a
dark place, the rods undergo chemical changes and after about 30 minutes they become
10,000 times more sensitive than the cones. This state is referred to as scotopic vision.
It increases our sensitivity to light, but drastically reduces our color vision.
The ﬁrst accurate experiments that measured human visual sensitivity were per-
formed in 1942 [Hecht et al. 42].
Each of the light sensors (rods and cones) in the eye sends a light sensation to
the brain that’s essentially a pixel, and the brain combines these pixels to a continuous
image. The human eye is therefore similar to a digital camera. Once we realize this, we
naturally want to compare the resolution of the eye to that of a modern digital camera.
Current digital cameras have from 500,000 sensors (for a cheap camera) to about ten
million sensors (for a high-quality one).
R
G
B
0
.02
.04
.06
.08
.10
.12
.14
.16
.18
.20
400 440 480 520 560 600 640 680
wavelength (nm)
Fraction of light absorbed
by each type of cone
Figure 5.29: Sensitivity of the Cones.
Thus, the eye features a much higher res-
olution, but its eﬀective resolution is even
higher if we consider that the eye can move
and refocus itself about three to four times
a second. This means that in a single sec-
ond, the eye can sense and send to the brain
about half a billion pixels. Assuming that
our camera takes a snapshot once a second,
the ratio of the resolutions is about 100.
Certain colors—such as red, orange,
and yellow—are psychologically associated
with heat. They are considered warm and
cause a picture to appear larger and closer
than it really is. Other colors—such as blue,
violet, and green—are associated with cool
things (air, sky, water, ice) and are therefore
called cool colors. They cause a picture to
look smaller and farther away.
Luminance is proportional to the power of the light source. It is similar to intensity,
but the spectral composition of luminance is related to the brightness sensitivity of
human vision.
The eye is very sensitive to small changes in luminance, which is why it is useful to
have color spaces that use Y as one of their three parameters. A simple way to do this
is to compute Y as a weighted sum of the R, G, and B color components with weights
determined by Figure 5.29, and then to subtract Y from the blue and red components
and have Y, B −Y, and R −Y as the three components of a new color space. The last
two components are called chroma. They represent color in terms of the presence or
absence of blue (Cb) and red (Cr) for a given luminance intensity.
Various number ranges are used in B −Y and R −Y for diﬀerent applications.
The YPbPr ranges are optimized for component analog video. The YCbCr ranges are
appropriate for component digital video such as studio video, JPEG, JPEG 2000, and
MPEG.

186
5.
Image Compression
The YCbCr color space was developed as part of Recommendation ITU-R BT.601
(formerly CCIR 601) during the development of a worldwide digital component video
standard. Y is deﬁned to have a range of 16 to 235; Cb and Cr are deﬁned to have a
range of 16 to 240, with 128 equal to zero. There are several YCbCr sampling formats,
such as 4:4:4, 4:2:2, 4:1:1, and 4:2:0, which are also described in the recommendation.
Conversions between RGB with a 16–235 range and YCbCr are linear and therefore
simple. Transforming RGB to YCbCr is done by (note the small weight of blue):
Y = (77/256)R + (150/256)G + (29/256)B,
Cb = −(44/256)R −(87/256)G + (131/256)B + 128,
Cr = (131/256)R −(110/256)G −(21/256)B + 128;
while the opposite transformation is
R = Y + 1.371(Cr −128),
G = Y −0.698(Cr −128) −0.336(Cb −128),
B = Y + 1.732(Cb −128).
When performing YCbCr to RGB conversion, the resulting RGB values have a
nominal range of 16–235, with possible occasional values in 0–15 and 236–255.
Other color spaces may be useful in special applications, but each space requires
three numbers to describes a color. This interesting and unexpected fact stems from the
way the cones in the retina respond to light. There are three types of cones, known as
S, L, and M. They are sensitive to wavelengths around 420, 564, and 534 nanometers
(corresponding to violet, yellowish-green, and green, respectively). When these cones
sense light of wavelength W, each produces a signal whose intensity depends on how close
W is to the “personal” wavelength of the cone. The three signals are sent, as a tristimulus,
to the brain where they are interpreted as color. Thus, most humans are trichromats
and it has been estimated that they can distinguish roughly 10 million diﬀerent colors.
This said, we should also mention that many color blind people can perceive only gray
scales (while others may only confuse red and green). Obviously, such a color blind
person needs only one number, the intensity of gray, to specify a color. Such a person is
therefore a monochromat. A hypothetical creature that can only distinguish black and
white (darkness or light) needs only one bit to specify a color, while some persons (or
animals or extraterrestrials) may be tetrachromats [Tetrachromat 07]. They may have
four types of cones in their eyes, and consequently need four numbers to specify a color.
We therefore conclude that color is only a sensation in our brain; it is not part of
the physical world. What actually exists in the world is light of diﬀerent wavelengths,
and we are fortunate that our eyes and brain can interpret mere wavelengths as the rich,
vibrant colors that so enrich our lives and that we so much take for granted.
Colors are only symbols. Reality is to be found in luminance alone.
—Pablo Picasso

5.6 JPEG
187
5.6.2 DCT
The general concept of a transform is discussed in Section 5.3.
The discrete cosine
transform is discussed in some detail in Section 5.5.
The JPEG committee elected to use the DCT because of its good performance,
because it does not assume anything about the structure of the data, and because there
are ways to speed it up (Section 4.6.5 in [Salomon 07]).
The JPEG standard calls for applying the DCT not to the entire image but to data
units (blocks) of 8×8 pixels. The reasons for this are: (1) Applying DCT to large blocks
involves many arithmetic operations and is therefore slow. Applying DCT to small data
units is faster.
(2) Experience shows that, in a continuous-tone image, correlations
between pixels are short range. A pixel in such an image has a value (color component
or shade of gray) that’s close to those of its near neighbors, but has nothing to do with
the values of far neighbors. The JPEG DCT is therefore executed by Equation (5.6),
duplicated here for n = 8
Gij =1
4CiCj
7

x=0
7

y=0
pxy cos
(2x + 1)iπ
16

cos
(2y + 1)jπ
16

,
where Cf =

1
√
2,
f = 0,
1,
f > 0,
and 0 ≤i, j ≤7.
(5.6)
The DCT is JPEG’s key to lossy compression. The unimportant image information
is reduced or removed by quantizing the 64 DCT coeﬃcients, especially the ones located
toward the lower-right. If the pixels of the image are correlated, quantization does not
degrade the image quality much. For best results, each of the 64 coeﬃcients is quantized
by dividing it by a diﬀerent quantization coeﬃcient (QC). All 64 QCs are parameters
that can be controlled, in principle, by the user (Section 5.6.3).
The JPEG decoder works by computing the inverse DCT (IDCT), Equation (5.7),
duplicated here for n = 8
pxy =1
4
7

i=0
7

j=0
CiCjGij cos
(2x + 1)iπ
16

cos
(2y + 1)jπ
16

,
where Cf =

1
√
2,
f = 0,
1,
f > 0.
(5.7)
It takes the 64 quantized DCT coeﬃcients and calculates 64 pixels pxy. If the QCs are the
right ones, the new 64 pixels will be very similar to the original ones. Mathematically,
the DCT is a one-to-one mapping of 64-point vectors from the image domain to the
frequency domain. The IDCT is the reverse mapping. If the DCT and IDCT could be
calculated with inﬁnite precision and if the DCT coeﬃcients were not quantized, the
original 64 pixels would be exactly reconstructed.
5.6.3 Quantization
After each 8×8 data unit of DCT coeﬃcients Gij is computed, it is quantized. This
is the step where information is lost (except for some unavoidable loss because of ﬁnite

188
5.
Image Compression
precision calculations in other steps). Each number in the DCT coeﬃcients matrix is
divided by the corresponding number from the particular “quantization table” used, and
the result is rounded to the nearest integer. As has already been mentioned, three such
tables are needed, for the three color components. The JPEG standard allows for up to
four tables, and the user can select any of the four for quantizing each color component.
The 64 numbers that constitute each quantization table are all JPEG parameters. In
principle, they can all be speciﬁed and ﬁne-tuned by the user for maximum compres-
sion. In practice, few users have the patience or expertise to experiment with so many
parameters, so JPEG software normally uses the following two approaches:
1. Default quantization tables. Two such tables, for the luminance and the chrominance
components, are the result of many experiments performed by the JPEG committee.
They are included in the JPEG standard and are reproduced here as Table 5.30. It is
easy to see how the QCs in the table generally grow as we move from the upper-left
corner to the bottom-right corner. This is how JPEG reduces the DCT coeﬃcients with
high spatial frequencies.
2. A simple quantization table Q is computed, based on one parameter R speciﬁed by
the user. A simple expression such as Qij = 1 + (i + j) × R guarantees that QCs start
small at the upper-left corner and get bigger toward the lower-right corner. Table 5.31
shows an example of such a table with R = 2.
16 11 10 16
24
40
51
61
12 12 14 19
26
58
60
55
14 13 16 24
40
57
69
56
14 17 22 29
51
87
80
62
18 22 37 56
68 109 103
77
24 35 55 64
81 104 113
92
49 64 78 87 103 121 120 101
72 92 95 98 112 100 103
99
17 18 24 47 99 99 99 99
18 21 26 66 99 99 99 99
24 26 56 99 99 99 99 99
47 66 99 99 99 99 99 99
99 99 99 99 99 99 99 99
99 99 99 99 99 99 99 99
99 99 99 99 99 99 99 99
99 99 99 99 99 99 99 99
Luminance
Chrominance
Table 5.30: Recommended Quantization Tables.
1
3
5
7
9 11 13 15
3
5
7
9 11 13 15 17
5
7
9 11 13 15 17 19
7
9 11 13 15 17 19 21
9 11 13 15 17 19 21 23
11 13 15 17 19 21 23 25
13 15 17 19 21 23 25 27
15 17 19 21 23 25 27 29
Table 5.31: The Quantization Table 1 + (i + j) × 2.
If the quantization is done correctly, very few nonzero numbers will be left in the
DCT coeﬃcients matrix, and they will typically be concentrated in the upper-left region.

5.6 JPEG
189
These numbers are the output of JPEG, but they are encoded before being written on
the output. In the JPEG literature this process is referred to as “entropy coding,” and
Section 5.6.4 illustrates it in detail. Three techniques are used by entropy coding to
compress the 8 × 8 matrix of integers:
1.
The 64 numbers are collected by scanning the matrix in zigzags (Figure 1.12b).
This produces a string of 64 numbers that starts with some nonzeros and typically ends
with many consecutive zeros. Only the nonzero numbers are output (encoded) and are
followed by a special end-of block (EOB) code. This way there is no need to output the
trailing zeros (we can say that the EOB is the run-length encoding of all the trailing
zeros). The interested reader should also consult [Salomon 07] for other methods to
compress binary strings with many consecutive zeros.
⋄Exercise 5.8: Propose a practical way to write a loop that traverses an 8 × 8 matrix
in zigzag.
2. The nonzero numbers are compressed using Huﬀman coding (Section 5.6.4).
3. The ﬁrst of those numbers (the DC coeﬃcient, page 156) is treated diﬀerently from
the others (the AC coeﬃcients).
She had just succeeded in curving it down into a graceful zigzag, and was going to
dive in among the leaves, which she found to be nothing but the tops of the trees
under which she had been wandering, when a sharp hiss made her draw back in a
hurry.
—Lewis Carroll, Alice in Wonderland (1865)
5.6.4 Encoding
We ﬁrst discuss point 3 above. Each 8×8 matrix of quantized DCT coeﬃcients contains
one DC coeﬃcient [at position (0, 0), the top left corner] and 63 AC coeﬃcients. The
DC coeﬃcient is a measure of the average value of the 64 original pixels, constituting
the data unit. Experience shows that in a continuous-tone image, adjacent data units
of pixels are normally correlated in the sense that the average values of the pixels in
adjacent data units are close. We already know that the DC coeﬃcient of a data unit
is a multiple of the average of the 64 pixels that constitute the unit. This implies that
the DC coeﬃcients of adjacent data units don’t diﬀer much. JPEG outputs the ﬁrst one
(encoded), followed by diﬀerences (also encoded) of the DC coeﬃcients of consecutive
data units.
Example: If the ﬁrst three 8×8 data units of an image have quantized DC coeﬃcients
of 1118, 1114, and 1119, then the JPEG output for the ﬁrst data unit is 1118 (Huﬀman
encoded, see below) followed by the 63 (encoded) AC coeﬃcients of that data unit. The
output for the second data unit will be 1114 −1118 = −4 (also Huﬀman encoded),
followed by the 63 (encoded) AC coeﬃcients of that data unit, and the output for the
third data unit will be 1119 −1114 = 5 (also Huﬀman encoded), again followed by the
63 (encoded) AC coeﬃcients of that data unit. This way of handling the DC coeﬃcients
is worth the extra trouble, because the diﬀerences are small.
Coding the DC diﬀerences is done with Table 5.32, so ﬁrst here are a few words
about this table. Each row has a row number (on the left), the unary code for the row
(on the right), and several columns in between. Each row contains greater numbers (and

190
5.
Image Compression
also more numbers) than its predecessor but not the numbers contained in previous rows.
Row i contains the range of integers [−(2i−1), +(2i−1)] but is missing the middle range
[−(2i−1 −1), +(2i−1 −1)]. Thus, the rows get very long, which means that a simple
two-dimensional array is not a good data structure for this table. In fact, there is no
need to store these integers in a data structure, since the program can ﬁgure out where
in the table any given integer x is supposed to reside by analyzing the bits of x.
The ﬁrst DC coeﬃcient to be encoded in our example is 1118. It resides in row
11 column 930 of the table (column numbering starts at zero), so it is encoded as
111111111110|01110100010 (the unary code for row 11, followed by the 11-bit binary
value of 930). The second DC diﬀerence is −4. It resides in row 3 column 3 of Table 5.32,
so it is encoded as 1110|011 (the unary code for row 3, followed by the 3-bit binary value
of 3).
⋄Exercise 5.9: How is the third DC diﬀerence, 5, encoded?
Point 2 above has to do with the precise way the 63 AC coeﬃcients of a data unit
are compressed. It uses a combination of RLE and either Huﬀman or arithmetic coding.
The idea is that the sequence of AC coeﬃcients normally contains just a few nonzero
numbers, with runs of zeros between them, and with a long run of trailing zeros. For each
nonzero number x, the encoder (1) ﬁnds the number Z of consecutive zeros preceding x;
(2) ﬁnds x in Table 5.32 and prepares its row and column numbers (R and C); (3) the
pair (R, Z) [that’s (R, Z), not (R, C)] is used as row and column numbers for Table 5.33;
and (4) the Huﬀman code found in that position in the table is concatenated to C (where
C is written as an R-bit number) and the result is (ﬁnally) the code emitted by the JPEG
encoder for the AC coeﬃcient x and all the consecutive zeros preceding it.
The Huﬀman codes in Table 5.33 are not the ones recommended by the JPEG
standard. The standard recommends the use of Tables 5.34 and 5.35 and says that up
to four Huﬀman code tables can be used by a JPEG codec, except that the baseline
mode can use only two such tables. The actual codes in Table 5.33 are thus arbitrary.
The reader should notice the EOB code at position (0, 0) and the ZRL code at position
(0, 15). The former indicates end-of-block, and the latter is the code emitted for 15
consecutive zeros when the number of consecutive zeros exceeds 15. These codes are
the ones recommended for the luminance AC coeﬃcients of Table 5.34. The EOB and
ZRL codes recommended for the chrominance AC coeﬃcients of Table 5.35 are 00 and
1111111010, respectively.
As an example consider the sequence
1118, 2, 0, −2, 0, . . . , 0
  
13
, −1, 0, . . . .
The ﬁrst AC coeﬃcient 2 has no zeros preceding it, so Z = 0. It is found in Table 5.32
in row 2, column 2, so R = 2 and C = 2. The Huﬀman code in position (R, Z) = (2, 0)
of Table 5.33 is 01, so the ﬁnal code emitted for 2 is 01|10. The next nonzero coeﬃcient,
−2, has one zero preceding it, so Z = 1. It is found in Table 5.32 in row 2, column 1, so
R = 2 and C = 1. The Huﬀman code in position (R, Z) = (2, 1) of Table 5.33 is 11011,
so the ﬁnal code emitted for 2 is 11011|01.

5.6 JPEG
191
0:
0
0
1:
-1
1
10
2:
-3
-2
2
3
110
3:
-7
-6
-5
-4
4
5
6
7
1110
4:
-15
-14
. . .
-9
-8
8
9
10
. . .
15
11110
5:
-31
-30
-29
. . .
-17
-16
16
17
. . .
31
111110
6:
-63
-62
-61
. . .
-33
-32
32
33
. . .
63
1111110
7:
-127
-126
-125
. . .
-65
-64
64
65
. . .
127
11111110
...
...
14:
-16383
-16382
-16381
. . .
-8193
-8192
8192
8193
. . .
16383
111111111111110
15:
-32767
-32766
-32765
. . .
-16385
-16384
16384
16385
. . .
32767
1111111111111110
16:
32768
1111111111111111
Table 5.32: Coding the Diﬀerences of DC Coeﬃcients.
R Z:
0
1
. . .
15
0:
1010
11111111001(ZRL)
1:
00
1100
. . .
1111111111110101
2:
01
11011
. . .
1111111111110110
3:
100
1111001
. . .
1111111111110111
4:
1011
111110110
. . .
1111111111111000
5:
11010 11111110110 . . .
1111111111111001
...
...
Table 5.33: Coding AC Coeﬃcients.
⋄Exercise 5.10: What code is emitted for the last nonzero AC coeﬃcient, −1?
Finally, the sequence of trailing zeros is encoded as 1010 (EOB), so the output for
the above sequence of AC coeﬃcients is 01101101110111010101010. We saw earlier that
the DC coeﬃcient is encoded as 111111111110|1110100010, so the ﬁnal output for the
entire 64-pixel data unit is the 46-bit number
1111111111100111010001001101101110111010101010.
These 46 bits encode one color component of the 64 pixels of a data unit. Let’s assume
that the other two color components are also encoded into 46-bit numbers.
If each
pixel originally consists of 24 bits, then this corresponds to a compression factor of
64 × 24/(46 × 3) ≈11.13; very impressive!
(Notice that the DC coeﬃcient of 1118 has contributed 23 of the 46 bits. Subsequent
data units encode the diﬀerences of their DC coeﬃcient, which may take fewer than 10
bits instead of 23. They may feature much higher compression factors as a result.)
The same tables (Tables 5.32 and 5.33) used by the encoder should, of course,
be used by the decoder.
The tables may be predeﬁned and used by a JPEG codec
as defaults, or they may be speciﬁcally calculated for a given image in a special pass
preceding the actual compression. The JPEG standard does not specify any code tables,
so any JPEG codec must use its own.
Some JPEG variants use a particular version of arithmetic coding, called the QM
coder, that is speciﬁed in the JPEG standard.
This version of arithmetic coding is
adaptive, so it does not need Tables 5.32 and 5.33. It adapts its behavior to the image

192
5.
Image Compression
R
Z
1
2
3
4
5
6
7
8
9
A
0
00
01
100
1011
11010
1111000
11111000
1111110110
1111111110000010
1111111110000011
1
1100
11011
11110001
111110110
11111110110
1111111110000100
1111111110000101
1111111110000110
1111111110000111
1111111110001000
2
11100
11111001
1111110111
111111110100
111111110001001
111111110001010
111111110001011
111111110001100
111111110001101
111111110001110
3
111010
111110111
111111110101
1111111110001111
1111111110010000
1111111110010001
1111111110010010
1111111110010011
1111111110010100
1111111110010101
4
111011
1111111000
1111111110010110
1111111110010111
1111111110011000
1111111110011001
1111111110011010
1111111110011011
1111111110011100
1111111110011101
5
1111010
11111110111
1111111110011110
1111111110011111
1111111110100000
1111111110100001
1111111110100010
1111111110100011
1111111110100100
1111111110100101
6
1111011
111111110110
1111111110100110
1111111110100111
1111111110101000
1111111110101001
1111111110101010
1111111110101011
1111111110101100
1111111110101101
7
11111010
111111110111
1111111110101110
1111111110101111
1111111110110000
1111111110110001
1111111110110010
1111111110110011
1111111110110100
1111111110110101
8
111111000
111111111000000
1111111110110110
1111111110110111
1111111110111000
1111111110111001
1111111110111010
1111111110111011
1111111110111100
1111111110111101
9
111111001
1111111110111110
1111111110111111
1111111111000000
1111111111000001
1111111111000010
1111111111000011
1111111111000100
1111111111000101
1111111111000110
A
111111010
1111111111000111
1111111111001000
1111111111001001
1111111111001010
1111111111001011
1111111111001100
1111111111001101
1111111111001110
1111111111001111
B
1111111001
1111111111010000
1111111111010001
1111111111010010
1111111111010011
1111111111010100
1111111111010101
1111111111010110
1111111111010111
1111111111011000
C
1111111010
1111111111011001
1111111111011010
1111111111011011
1111111111011100
1111111111011101
1111111111011110
1111111111011111
1111111111100000
1111111111100001
D
11111111000
1111111111100010
1111111111100011
1111111111100100
1111111111100101
1111111111100110
1111111111100111
1111111111101000
1111111111101001
1111111111101010
E
1111111111101011
1111111111101100
1111111111101101
1111111111101110
1111111111101111
1111111111110000
1111111111110001
1111111111110010
1111111111110011
1111111111110100
F
11111111001
1111111111110101
1111111111110110
1111111111110111
1111111111111000
1111111111111001
1111111111111010
1111111111111011
1111111111111101
1111111111111110
Table 5.34: Recommended Huﬀman Codes for Luminance AC Coeﬃcients.

5.6 JPEG
193
R
Z
1
2
3
4
5
6
7
8
9
A
0
01
100
1010
11000
11001
111000
1111000
111110100
1111110110
111111110100
1
1011
111001
11110110
111110101
11111110110
111111110101
111111110001000
111111110001001
111111110001010
111111110001011
2
11010
11110111
1111110111
111111110110
111111111000010
1111111110001100
1111111110001101
1111111110001110
1111111110001111
1111111110010000
3
11011
11111000
1111111000
111111110111
1111111110010001
1111111110010010
1111111110010011
1111111110010100
1111111110010101
1111111110010110
4
111010
111110110
1111111110010111
1111111110011000
1111111110011001
1111111110011010
1111111110011011
1111111110011100
1111111110011101
1111111110011110
5
111011
1111111001
1111111110011111
1111111110100000
1111111110100001
1111111110100010
1111111110100011
1111111110100100
1111111110100101
1111111110100110
6
1111001
11111110111
1111111110100111
1111111110101000
1111111110101001
1111111110101010
1111111110101011
1111111110101100
1111111110101101
1111111110101110
7
1111010
11111111000
1111111110101111
1111111110110000
1111111110110001
1111111110110010
1111111110110011
1111111110110100
1111111110110101
1111111110110110
8
11111001
1111111110110111
1111111110111000
1111111110111001
1111111110111010
1111111110111011
1111111110111100
1111111110111101
1111111110111110
1111111110111111
9
111110111
1111111111000000
1111111111000001
1111111111000010
1111111111000011
1111111111000100
1111111111000101
1111111111000110
1111111111000111
1111111111001000
A
111111000
1111111111001001
1111111111001010
1111111111001011
1111111111001100
1111111111001101
1111111111001110
1111111111001111
1111111111010000
1111111111010001
B
111111001
1111111111010010
1111111111010011
1111111111010100
1111111111010101
1111111111010110
1111111111010111
1111111111011000
1111111111011001
1111111111011010
C
111111010
1111111111011011
1111111111011100
1111111111011101
1111111111011110
1111111111011111
1111111111100000
1111111111100001
1111111111100010
1111111111100011
D
11111111001
1111111111100100
1111111111100101
1111111111100110
1111111111100111
1111111111101000
1111111111101001
1111111111101010
1111111111101011
1111111111101100
E
11111111100000
1111111111101101
1111111111101110
1111111111101111
1111111111110000
1111111111110001
1111111111110010
1111111111110011
1111111111110100
1111111111110101
F
111111111000011
111111111010110
1111111111110111
1111111111111000
1111111111111001
1111111111111010
1111111111111011
1111111111111100
1111111111111101
1111111111111110
Table 5.35: Recommended Huﬀman Codes for Chrominance AC Coeﬃcients.

194
5.
Image Compression
statistics as it goes along. Using arithmetic coding may produce 5–10% better compres-
sion than Huﬀman for a typical continuous-tone image. However, it is more complex to
implement than Huﬀman coding, so in practice it is rare to ﬁnd a JPEG codec that uses
it.
5.6.5 Lossless Mode
The lossless mode of JPEG uses diﬀerencing to reduce the values of pixels before they are
compressed. This particular form of diﬀerencing is called predicting. The values of some
near neighbors of a pixel are subtracted from the pixel to get a small number, which
is then compressed further using Huﬀman or arithmetic coding. Figure 5.36a shows a
pixel X and three neighbor pixels A, B, and C. Figure 5.36b shows eight possible ways
(predictions) to combine the values of the three neighbors. In the lossless mode, the
user can select one of these predictions, and the encoder then uses it to combine the
three neighbor pixels and subtract the combination from the value of X. The result is
normally a small number, which is then entropy-coded in a way very similar to that
described for the DC coeﬃcient in Section 5.6.4.
Predictor 0 is used only in the hierarchical mode of JPEG. Predictors 1, 2, and 3
are called one-dimensional. Predictors 4, 5, 6, and 7 are two-dimensional.
C
B
A
X
Selection value
Prediction
0
no prediction
1
A
2
B
3
C
4
A + B −C
5
A + ((B −C)/2)
6
B + ((A −C)/2)
7
(A + B)/2
(a)
(b)
Figure 5.36: Pixel Prediction in the Lossless Mode.
It should be noted that the lossless mode of JPEG has never been very successful.
It produces typical compression factors of 2, and is therefore inferior to other lossless
image compression methods.
Because of this, many JPEG implementations do not
even implement this mode. Even the lossy (baseline) mode of JPEG does not perform
well when asked to limit the amount of loss to a minimum. As a result, some JPEG
implementations do not allow parameter settings that result in minimum loss.
The
strength of JPEG is in its ability to generate highly compressed images that when
decompressed are indistinguishable from the original. Recognizing this, the ISO has
decided to come up with another standard for lossless compression of continuous-tone
images. This standard is now commonly known as JPEG-LS and is described, among
other places, in [Salomon 07].

5.6 JPEG
195
5.6.6 The Compressed File
A JPEG encoder outputs a compressed ﬁle that includes parameters, markers, and the
compressed data units. The parameters are either four bits (these always come in pairs),
one byte, or two bytes long. The markers serve to identify the various parts of the ﬁle.
Each is two bytes long, where the ﬁrst byte is X’FF’ and the second one is not 0 or
X’FF’. A marker may be preceded by a number of bytes with X’FF’. Table 5.38 lists all
the JPEG markers (the ﬁrst four groups are start-of-frame markers). The compressed
data units are combined into MCUs (minimal coded unit), where an MCU is either a
single data unit (in the noninterleaved mode) or three data units from the three image
components (in the interleaved mode).
Compressed
[Tables]
[Tables]
[DNL segment]
SOI
Frame
Frame
EOI
MCU MCU
MCU
MCU MCU
MCU
image
Frame header
Segment0
Segmentlast
Frame header
Scan1
Scan
ECS0 [RST0]
ECSlast
ECSlast-1 [RSTlast-1]
[Scan2]
[Scanlast]
Figure 5.37: JPEG File Format.
Figure 5.37 shows the main parts of the JPEG compressed ﬁle (parts in square
brackets are optional).
The ﬁle starts with the SOI marker and ends with the EOI
marker. In between these markers, the compressed image is organized in frames. In
the hierarchical mode there are several frames, and in all other modes there is only
one frame.
In each frame the image information is contained in one or more scans,
but the frame also contains a header and optional tables (which, in turn, may include
markers). The ﬁrst scan may be followed by an optional DNL segment (deﬁne number
of lines), which starts with the DNL marker and contains the number of lines in the
image that’s represented by the frame. A scan starts with optional tables, followed by
the scan header, followed by several entropy-coded segments (ECS), which are separated
by (optional) restart markers (RST). Each ECS contains one or more MCUs, where an
MCU is, as explained earlier, either a single data unit or three such units.

196
5.
Image Compression
Value
Name
Description
Nondiﬀerential, Huﬀman coding
FFC0
SOF0
Baseline DCT
FFC1
SOF1
Extended sequential DCT
FFC2
SOF2
Progressive DCT
FFC3
SOF3
Lossless (sequential)
Diﬀerential, Huﬀman coding
FFC5
SOF5
Diﬀerential sequential DCT
FFC6
SOF6
Diﬀerential progressive DCT
FFC7
SOF7
Diﬀerential lossless (sequential)
Nondiﬀerential, arithmetic coding
FFC8
JPG
Reserved for extensions
FFC9
SOF9
Extended sequential DCT
FFCA
SOF10
Progressive DCT
FFCB
SOF11
Lossless (sequential)
Diﬀerential, arithmetic coding
FFCD
SOF13
Diﬀerential sequential DCT
FFCE
SOF14
Diﬀerential progressive DCT
FFCF
SOF15
Diﬀerential lossless (sequential)
Huﬀman table speciﬁcation
FFC4
DHT
Deﬁne Huﬀman table
Arithmetic coding conditioning speciﬁcation
FFCC
DAC
Deﬁne arith coding conditioning(s)
Restart interval termination
FFD0–FFD7
RSTm
Restart with modulo 8 count m
Other markers
FFD8
SOI
Start of image
FFD9
EOI
End of image
FFDA
SOS
Start of scan
FFDB
DQT
Deﬁne quantization table(s)
FFDC
DNL
Deﬁne number of lines
FFDD
DRI
Deﬁne restart interval
FFDE
DHP
Deﬁne hierarchical progression
FFDF
EXP
Expand reference component(s)
FFE0–FFEF
APPn
Reserved for application segments
FFF0–FFFD
JPGn
Reserved for JPEG extensions
FFFE
COM
Comment
Reserved markers
FF01
TEM
For temporary private use
FF02–FFBF
RES
Reserved
Table 5.38: JPEG Markers.

5.6 JPEG
197
5.6.7 JFIF
It has been mentioned earlier that JPEG is a compression method, not a graphics ﬁle
format, which is why it does not specify image features such as pixel aspect ratio, color
space, or interleaving of bitmap rows. This is where JFIF comes in.
JFIF (JPEG File Interchange Format) is a graphics ﬁle format that makes it pos-
sible to exchange JPEG-compressed images between computers. The main features of
JFIF are the use of the YCbCr triple-component color space for color images (only one
component for grayscale images) and the use of a marker to specify features missing from
JPEG, such as image resolution, aspect ratio, and features that are application-speciﬁc.
The JFIF marker (called the APP0 marker) starts with the zero-terminated string
JFIF. Following this, there is pixel information and other speciﬁcations (see below).
Following this, there may be additional segments specifying JFIF extensions. A JFIF
extension contains more platform-speciﬁc information about the image.
Each extension starts with the zero-terminated string JFXX, followed by a 1-byte
code identifying the extension. An extension may contain application-speciﬁc informa-
tion, in which case it starts with a diﬀerent string, not JFIF or JFXX but something that
identiﬁes the speciﬁc application or its maker.
The format of the ﬁrst segment of an APP0 marker is as follows:
1. APP0 marker (4 bytes): FFD8FFE0.
2. Length (2 bytes): Total length of marker, including the 2 bytes of the “length” ﬁeld
but excluding the APP0 marker itself (ﬁeld 1).
3. Identiﬁer (5 bytes): 4A4649460016: This is the JFIF string that identiﬁes the APP0
marker.
4. Version (2 bytes): Example: 010216 speciﬁes version 1.02.
5. Units (1 byte): Units for the X and Y densities. 0 means no units; the Xdensity and
Ydensity ﬁelds specify the pixel aspect ratio. 1 means that Xdensity and Ydensity are
dots per inch, 2, that they are dots per cm.
6. Xdensity (2 bytes), Ydensity (2 bytes): Horizontal and vertical pixel densities (both
should be nonzero).
7. Xthumbnail (1 byte), Ythumbnail (1 byte): Thumbnail horizontal and vertical pixel
counts.
8.
(RGB)n (3n bytes): Packed (24-bit) RGB values for the thumbnail pixels.
n =
Xthumbnail×Ythumbnail.
The syntax of the JFIF extension APP0 marker segment is as follows:
1. APP0 marker.
2. Length (2 bytes): Total length of marker, including the 2 bytes of the “length” ﬁeld
but excluding the APP0 marker itself (ﬁeld 1).
3. Identiﬁer (5 bytes): 4A4658580016 This is the JFXX string identifying an extension.
4. Extension code (1 byte): 1016 = Thumbnail coded using JPEG. 1116 = Thumbnail
coded using 1 byte/pixel (monochromatic). 1316 = Thumbnail coded using 3 bytes/pixel
(eight colors).
5. Extension data (variable): This ﬁeld depends on the particular extension.

198
5.
Image Compression
JFIF is the technical name for the image format better (but inaccurately) known as
JPEG. This term is used only when the diﬀerence between the Image Format and the
Image Compression is crucial. Strictly speaking, however, JPEG does not deﬁne an
Image Format, and therefore in most cases it would be more precise to speak of JFIF
rather than JPEG. Another Image Format for JPEG is SPIFF deﬁned by the JPEG
standard itself, but JFIF is much more widespread than SPIFF.
—Erik Wilde, WWW Online Glossary
5.7 The Wavelet Transform
The transforms described in Section 5.3 are orthogonal. They transform the original
pixels into a few large numbers and many small numbers.
In contrast, the wavelet
transforms of this section decompose an image into bands (regions or subbands) that
correspond to diﬀerent pixel frequencies and also reﬂect diﬀerent geometrical artifacts
of the image. The ﬁnal (lossy) compression of the image can be very eﬃcient, because
each band can be independently quantized (by an amount that depends on the pixel
frequency it corresponds to) and then encoded.
Thus, a wavelet transform may be
the key to eﬃcient compression of images with a mixture of high-frequency and low-
frequency areas. In contrast, images with large uniform (or almost-uniform) areas may
respond better to other compression methods. Reference [Salomon 07] describes several
compression methods that are based on wavelet transforms.
Before we start, here are a few words about the origin of the term wavelet. In
the early 1800s, the French mathematician Joseph Fourier discovered that any periodic
function f can be expressed as a (possibly inﬁnite) sum of sines and cosines. These
functions are represented graphically as waves, which is why the Fourier expansion of a
function f is associated with waves and reveals the frequencies “hidden” in f. Fourier
expansion has many applications in engineering, mainly in the analysis of signals. It
can isolate the various frequencies that underlie a signal and thereby enable the user
to study the signal and also edit it by deleting or adding frequencies. The downside of
Fourier expansion is that it does not tell us when (at which point or points in time)
each frequency is active in a given signal. We therefore say that Fourier expansion oﬀers
frequency resolution but no time resolution.
Wavelet analysis (or the wavelet transform) is a successful approach to the problem
of analyzing a signal both in time and in frequency. Given a signal that varies with
time, we select a time interval, and use the wavelet transform to identify and isolate
the frequencies that constitute the signal in that interval. The interval can be wide,
in which case we say that the signal is analyzed on a large scale. As the time interval
gets narrower, the scale of analysis is said to become smaller and smaller. A large-scale
analysis illustrates the global behavior of the signal, while each small-scale analysis
illuminates the way the signal behaves in a short interval of time; it is like zooming in
the signal in time, instead of in space. Thus, the fundamental idea behind wavelets is
to analyze a function or a signal according to scale.
The continuous wavelet transform [Salomon 07] illustrates the connection between
(1) the time-frequency analysis of continuous functions and (2) waves that are concen-
trated in a small area. This analysis therefore justiﬁes the diminutive “wavelet” instead

5.7 The Wavelet Transform
199
of “wave.” In practical applications, the raw data is normally collected as sets of num-
bers, not as a continuous function, and is therefore discrete. Thus, the discrete, and not
the continuous, wavelet transform is the tool used in practice to analyze digital data
and compress it.
The wavelet transform is a tool that cuts up data or functions or operators into
diﬀerent frequency components, and then studies each component with a resolution
matched to its scale.
—Ingrid Daubechies (approximate pronunciation “Dobe-uh-shee”),
Ten Lectures on Wavelets (1992)
We start with the Haar transform, originated in work done by Alfred Haar in 1910–
1912 on systems of orthogonal functions [Haar 10]. The Haar transform is the simplest
wavelet transform, and we illustrate it in one dimension. Given an array of n pixels,
where n is a power of 2 (if it is not, we extend the array by appending copies of the
last pixels), we divide it into n/2 pairs of consecutive pixels, compute the average and
the diﬀerence of each pair, and end up with n/2 averages followed by n/2 diﬀerences.
We then repeat this process on the n/2 averages to obtain n/4 averages followed by
n/4 diﬀerences. This is repeated until we end up with one average followed by n −1
diﬀerences.
As an example, consider the eight correlated values 31, 32, 33.5, 33.5, 31.5, 34.5,
32, and 28. We compute the four averages (31 + 32)/2 = 31.5, (33.5 + 33.5)/2 = 33.5,
(31.5+34.5)/2 = 33, and (32+28)/2 = 30 and the four diﬀerences 31−32 = −1, 33.5−
33.5 = 0, 31.5−34.5 = −3, and 32−28 = 4. The diﬀerences are called detail coeﬃcients,
and in this section the terms “diﬀerence” and “detail” are used interchangeably. We can
think of the averages as a coarse resolution representation of the original values, and of
the details as the extra data needed to reconstruct the original image from this coarse
resolution. The four averages and four diﬀerences are suﬃcient to reconstruct the original
eight values, but because these values are correlated, the averages and diﬀerences feature
additional properties. The averages are a coarse representation of the original values and
the diﬀerences are small.
We repeat the process on the four averages and transform them into two averages
and two diﬀerences. The results are (31.5 + 33.5)/2 = 32.5, (33 + 30)/2 = 31.5, 31.5 +
33.5 = −2, and 33 −30 = 3. The last iteration of this process transforms the two new
averages into one average (32.5 + 31.5)/2 = 32 (the average of all eight components of
the original array) and one diﬀerence 32.5 −31.5 = 1. The eight numbers 32, 1, −2, 3,
−1, 0, −3, and 4 constitute the Haar wavelet transform of the original correlated values.
In general, the one-dimensional Haar transform converts n correlated values to n
transform coeﬃcients of which the ﬁrst is the average of the original values and the
remaining are small numbers. Compression is lossy and is achieved by quantizing the
diﬀerences (at least to the nearest integers and perhaps even coarser) and encoding them
with Huﬀman or other variable-length codes.

200
5.
Image Compression
Alfr´ed Haar (1885–1933)
Alfr´ed Haar was born in Budapest and received his higher mathematical training in
G¨ottingen, where he later became a privatdozent. In 1912, he re-
turned to Hungary and became a professor of mathematics ﬁrst in
Kolozsv´ar and then in Szeged, where he and his colleagues created
a major mathematical center.
Haar is best remembered for his work on analysis on groups.
In 1932 he introduced an invariant measure on locally compact
groups, now called the Haar measure, which allows an analog of
Lebesgue integrals to be deﬁned on locally compact topological
groups. Mathematical lore has it that John von Neumann tried to
discourage Haar in this work because he felt certain that no such
measure could exist. The following limerick celebrates Haar’s achievement.
Said a mathematician named Haar,
“Von Neumann can’t see very far.
He missed a great treasure—
They call it Haar measure—
Poor Johnny’s just not up to par.”
Before we continue, it is important (and also interesting) to estimate the computa-
tional complexity of this transform, i.e., the number of arithmetic operations needed to
transform N data values. In our example we needed 8+4+2 = 14 operations (additions
and subtractions, the divisions by 2 are ignored since they can be done with shifts), a
number that can also be expressed as 14 = 2(8−1). In the general case, assume that we
start with N = 2n data items. The ﬁrst iteration requires 2n operations, the second it-
eration requires 2n−1 operations, and so on, until the last iteration, where 2n−(n−1) = 21
operations are needed. The total number of operations is therefore
n

i=1
2i =
n

i=0
2i −1 = 1 −2n+1
1 −2
−1 = 2n+1 −2 = 2(2n −1) = 2(N −1).
The Haar transform of N values can therefore be performed with 2(N −1) operations,
so its complexity is O(N), an excellent result.
It is useful to associate with each iteration a quantity called resolution, which is
deﬁned as the number of remaining averages at the end of the iteration. The resolutions
after each of the three iterations above are 4(= 22), 2(= 21), and 1(= 20). Section 5.7.3
shows that each component of the wavelet transform should be normalized by dividing
it by the square root of the resolution. (This is the normalized Haar transform, also
discussed in Section 5.7.3.) Thus, our example wavelet transform becomes
 32
√
20 ,
1
√
20 , −2
√
21 ,
3
√
21 , −1
√
22 ,
0
√
22 , −3
√
22 ,
4
√
22

.
If the normalized wavelet transform is used, it can be shown that ignoring the smallest
diﬀerences is the best choice for lossy wavelet compression, since it causes the smallest
loss of image information.

5.7 The Wavelet Transform
201
The two procedures of Figure 5.39 illustrate how the normalized wavelet transform
of an array of n components (where n is a power of 2) can be computed. Reconstructing
the original array from the normalized wavelet transform is illustrated by the pair of
procedures of Figure 5.40.
These procedures seem at ﬁrst to be diﬀerent from the averages and diﬀerences
discussed earlier. They don’t compute averages, because they divide by
√
2 instead of
by 2; the ﬁrst procedure starts by dividing the entire array by √n, and the second one
ends by doing the reverse. The ﬁnal result, however, is the same.
We believe that the future of wavelet theory will continue to be marked by the synthe-
sis of traditionally distinct disciplines in mathematics, engineering, and the sciences.
This will require adjustments by all parties, including an increased willingness to
address applications on the part of mathematicians, and an increased attention to
mathematical rigor on the part of the engineering community. We hope that this
book will contribute toward this future.
—J. Benedetto and M. Frazier, Wavelets: Mathematics and Applications (1994)
5.7.1 Applying the Haar Transform
Once the concept of a wavelet transform is grasped, it’s easy to generalize it to a com-
plete two-dimensional image. This can be done in quite a few ways which are discussed
in [Salomon 07]. Here we describe only two such approaches, called the standard decom-
position and the pyramid decomposition.
The former (Figure 5.41) starts by computing the wavelet transform of every row of
the image. This results in a transformed image where the ﬁrst column contains averages
and all the other columns contain diﬀerences. The standard algorithm then computes
the wavelet transform of every column. This results in one average value at the top-left
corner, with the rest of the top row containing averages of diﬀerences, and with all other
pixel values transformed into diﬀerences.
The latter method computes the wavelet transform of the image by alternating
between rows and columns. The ﬁrst step is to calculate averages and diﬀerences for all
the rows (just one iteration, not the entire wavelet transform). This creates averages in
the left half of the image and diﬀerences in the right half. The second step is to calculate
averages and diﬀerences (just one iteration) for all the columns, which results in averages
in the top-left quadrant of the image and diﬀerences elsewhere. Steps 3 and 4 operate on
the rows and columns of that quadrant, resulting in averages concentrated in the top-left
subquadrant. Pairs of steps are executed repeatedly on smaller and smaller subsquares,
until only one average is left, at the top-left corner of the image, and all other pixel
values have been reduced to diﬀerences. This process is summarized in Figure 5.42.
Armed with these decompositions, it is easy to understand why the wavelet trans-
form is the key to eﬃcient lossy image compression. The transforms described in Sec-
tion 5.3 are orthogonal. They transform the original pixels into a few large numbers and
many small numbers. In contrast, wavelet transforms, such as the Haar transform, are
subband transforms. They partition the image into regions (also called subbands) such
that one region contains large numbers (averages in the case of the Haar transform) and
the other regions contain small numbers (diﬀerences). However, the subbands are more
than just sets of large and small numbers. They reveal diﬀerent geometrical artifacts

202
5.
Image Compression
procedure NWTcalc(a:array of real, n:int);
comment n is the array size (a power of 2)
a:=a/√n comment divide entire array
j:=n;
while j≥2 do
NWTstep(a, j);
j:=j/2;
endwhile;
end;
procedure NWTstep(a:array of real, j:int);
for i=1 to j/2 do
b[i]:=(a[2i-1]+a[2i])/
√
2;
b[j/2+i]:=(a[2i-1]-a[2i])/
√
2;
endfor;
a:=b; comment move entire array
end;
Figure 5.39: Computing the Normalized Wavelet Transform.
procedure NWTreconst(a:array of real, n:int);
j:=2;
while j≤n do
NWTRstep(a, j);
j:=2j;
endwhile
a:=a√n; comment multiply entire array
end;
procedure NWTRstep(a:array of real, j:int);
for i=1 to j/2 do
b[2i-1]:=(a[i]+a[j/2+i])/
√
2;
b[2i]:=(a[i]-a[j/2+i])/
√
2;
endfor;
a:=b; comment move entire array
end;
Figure 5.40: Restoring From a Normalized Wavelet Transform.

5.7 The Wavelet Transform
203
procedure StdCalc(a:array of real, n:int);
comment array size is nxn (n = power of 2)
for r=1 to n do NWTcalc(row r of a, n);
endfor;
for c=n to 1 do comment loop backwards
NWTcalc(col c of a, n);
endfor;
end;
procedure StdReconst(a:array of real, n:int);
for c=n to 1 do comment loop backwards
NWTreconst(col c of a, n);
endfor;
for r=1 to n do
NWTreconst(row r of a, n);
endfor;
end;
Original
image
L1
H1
H1
H1
L2 H2
H2
L3
H3
Figure 5.41: The Standard Image Wavelet Transform and Decomposition.

204
5.
Image Compression
procedure NStdCalc(a:array of real, n:int);
a:=a/√n comment divide entire array
j:=n;
while j≥2 do
for r=1 to j do NWTstep(row r of a, j);
endfor;
for c=j to 1 do comment loop backwards
NWTstep(col c of a, j);
endfor;
j:=j/2;
endwhile;
end;
procedure NStdReconst(a:array of real, n:int);
j:=2;
while j≤n do
for c=j to 1 do comment loop backwards
NWTRstep(col c of a, j);
endfor;
for r=1 to j do
NWTRstep(row r of a, j);
endfor;
j:=2j;
endwhile
a:=a√n; comment multiply entire array
end;
Original
image
L
H
HL
HH
LL
LH
HL
HH
LH
LLL
LLH
HL
HH
LH
Figure 5.42: The Pyramid Image Wavelet Transform.

5.7 The Wavelet Transform
205
of the image. To illustrate this important feature, we examine a small, mostly-uniform
image with one vertical line and one horizontal line. Figure 5.43a shows an 8 × 8 image
with pixel values of 12, except for a vertical line with pixel values of 14 and a horizontal
line with pixel values of 16.
12 12 12 12 14 12 12 12
12 12 12 12 14 12 12 12
12 12 12 12 14 12 12 12
12 12 12 12 14 12 12 12
12 12 12 12 14 12 12 12
16 16 16 16 14 16 16 16
12 12 12 12 14 12 12 12
12 12 12 12 14 12 12 12
12 12 13 12 0 0 2 0
12 12 13 12 0 0 2 0
12 12 13 12 0 0 2 0
12 12 13 12 0 0 2 0
12 12 13 12 0 0 2 0
16 16 15 16 0 0 2 0
12 12 13 12 0 0 2 0
12 12 13 12 0 0 2 0
12 12 13 12 0 0 2 0
12 12 13 12 0 0 2 0
14 14 14 14 0 0 0 0
12 12 13 12 0 0 2 0
0
0
0
0 0 0 0 0
0
0
0
0 0 0 0 0
4
4
2
4 0 0 4 0
0
0
0
0 0 0 0 0
(a)
(b)
(c)
Figure 5.43: An 8×8 Image and Its Subband Decomposition.
Figure 5.43b shows the results of applying the Haar transform once to the rows of
the image. The right half of this ﬁgure (the diﬀerences) is mostly zeros, reﬂecting the
uniform nature of the image. However, traces of the vertical line can easily be seen (the
notation 2 indicates a negative diﬀerence). Figure 5.43c shows the results of applying
the Haar transform once to the columns of Figure 5.43b. The upper-right subband now
features traces of the vertical line, whereas the lower-left subband shows traces of the
horizontal line. These subbands are denoted by HL and LH, respectively (Figures 5.42
and 5.44, although there is inconsistency in the use of this notation by various authors).
The lower-right subband, denoted by HH, reﬂects diagonal image artifacts (which our
example image lacks). Most interesting is the upper-left subband, denoted by LL, that
consists entirely of averages. This subband is a one-quarter version of the entire image,
containing traces of both the vertical and the horizontal lines.
⋄Exercise 5.11: Construct a diagram similar to Figure 5.43 to show how subband HH
reveals diagonal artifacts of the image.
(Artifact: A feature not naturally present, introduced during preparation or inves-
tigation.)
Figure 5.44 shows four levels of subbands, where level 1 contains the detailed fea-
tures of the image (also referred to as the high-frequency or ﬁne-resolution wavelet coef-
ﬁcients) and the top level, level 4, contains the coarse image features (low-frequency or
coarse-resolution coeﬃcients). It is clear that the lower levels can be quantized coarsely
without much loss of important image information, while the higher levels should be
quantized only ﬁnely. The subband structure is the basis of all the image compression
methods that employ the wavelet transform.
Figure 5.45 shows typical results of the pyramid wavelet transform. The original
image is shown in Figure 5.45a, and Figure 5.45c is a general pyramid decomposition.
In order to illustrate how the pyramid transform works, this image consists only of hor-
izontal, vertical, and slanted lines. The four quadrants of Figure 5.45b show smaller

206
5.
Image Compression
Frequencies
Energy
Coarse resolution
Fine resolution
LL4
LH1
LH2
LH3
HL1
HL2
HL3
HH1
HH2
HH3
Figure 5.44: Subbands and Levels in Wavelet Decomposition.
versions of the image. The top-left subband, containing the averages, is similar to the
entire image, while each of the other three subbands shows image details. Because of
the way the pyramid transform is constructed, the top-right subband contains vertical
details, the bottom-left subband contains horizontal details, and the bottom-right sub-
band contains the details of slanted lines. Figure 5.45c shows the results of repeatedly
applying this transform. The image is transformed into subbands of horizontal, vertical,
and diagonal details, while the top-left subsquare, containing the averages, is shrunk to
a single pixel. Figure 5.46 lists simple Matlab code to compute the pyramid transform
of a raw (uncompressed) image.
Either method, standard or uniform, results in a transformed, although not yet
compressed, image that has one average at the top-left corner and smaller numbers,
diﬀerences or averages of diﬀerences, everywhere else. These numbers are now com-
pressed (lossy compression) by quantizing them to integers which are then encoded by
replacing them with variable-length codes. Coarse quantization often results in runs
of zeros, which can be compressed by RLE (each run of zeros is replaced by a single
variable-length code).
Color Images. So far, we have assumed that each pixel is a single number (i.e.,
we have a single-component image, in which all pixels are shades of the same color, nor-
mally gray). Any compression method for single-component images can be extended to
color (three-component) images by separating the three components, then transforming
and compressing each individually. If the compression method is lossy, it makes sense

5.7 The Wavelet Transform
207
Figure 5.45: An Example of the Pyramid Image Wavelet Transform.

208
5.
Image Compression
clear; % main program, file pyramid.m
filename=’house128’; global dim; dim=128;
fid=fopen(filename,’r’);
if fid==-1 disp(’file not found’)
else img=fread(fid,[dim,dim])’; fclose(fid);
end
figure(1), image(img), colormap(gray), axis square
global timg; timg=zeros(dim,dim);
stp(img);
img=timg;
figure(2), image(img), colormap(gray), axis square
dim=64;
stp(img);
img=timg;
figure(3), image(img), colormap(gray), axis square
function x=stp(img) % file stp.m
global dim; global timg
for r = 1:dim
for c=1:2:dim-1;
timg(r,c)=(img(r,c)+img(r,c+1))/2;
timg(r,c+c)=img(r,c)-img(r,c+1);
end;
end
for c=1:dim;
for r=1:2:dim-1;
timg(r,c)=(img(r,c)+img(r+1,c))/2;
timg(r+r,c)=img(r,c)-img(r+1,c);
end;
end;
Figure 5.46: Matlab Code for a Pyramid Image Wavelet Transform.
to convert the three image components from their original color representation (often
RGB) to the YIQ color representation. The Y component of this representation is called
luminance, and the I and Q (the chrominance) components are responsible for the color
information [Salomon 99]. The advantage of this color representation is that the human
eye is most sensitive to Y and least sensitive to Q. A lossy compression method for color
images in YIQ should therefore: (1) apply a wavelet transform to each of the three com-
ponents separately; (2) quantize the transform coeﬃcients of the Y component lightly
(perhaps only to the nearest integer), perform medium quantization on the coeﬃcients
of the I component, and coarse quantization on the coeﬃcients of the Q component;
and ﬁnally (3) encode all the quantized transform coeﬃcients. Such a process results in
excellent compression, while losing only image data to which the eye is not sensitive.
Man’s life was spacious in the early world:
It paused, like some slow ship with sail unfurled
Waiting in seas by scarce a wavelet curled;
—George Eliot, Poems (1895)
It is interesting to note that United States color television transmission also takes
advantage of the YIQ representation. Signals are broadcast with bandwidths of 4 MHz
for Y, 1.5 MHz for I, and only 0.6 MHz for Q.

5.7 The Wavelet Transform
209
Make two squares and four right-angled triangles by drawing only seven straight
lines.
5.7.2 Properties of the Haar Transform
The examples in this section illustrate some properties of the Haar transform, and of
the discrete wavelet transform in general. Figure 5.47 shows a highly correlated 8×8
image and its Haar wavelet transform. Both the grayscale and numeric values of the
pixels and of the transform coeﬃcients are shown. Because the original pixels are highly
correlated, the wavelet coeﬃcients are small and there are many zeros.
⋄Exercise 5.12: A glance at Figure 5.47 suggests that the last sentence is wrong. The
wavelet transform coeﬃcients listed in the ﬁgure are very large compared with the pixel
values of the original image. In fact, we know that the top-left Haar transform coeﬃcient
should be the average of all the image pixels. Since the pixels of our image have values
that are (more or less) uniformly distributed in the interval [0, 255], this average should
be around 128, yet the top-left transform coeﬃcient is 1051. Explain this!
In a discrete wavelet transform, most of the wavelet coeﬃcients are details (or
diﬀerences). The details in the lower levels represent the ﬁne details of the image. As
we move higher in the subband level, we ﬁnd details that correspond to coarser image
features. Figure 5.48a illustrates this concept. It shows an image that is smooth on
the left and has “activity” (i.e., adjacent pixels that tend to be diﬀerent) on the right.
Part (b) shows the wavelet transform of the image. Low levels (corresponding to ﬁne
details) have transform coeﬃcients on the right, since this is where the image activity
is located. High levels (coarse details) look similar but also have coeﬃcients on the left
side, because the image is not completely blank on the left.
The Haar transform is the simplest wavelet transform, but even this simple method
illustrates the power of this type of transform. It has been mentioned earlier that the
low levels of the discrete wavelet transform contain the unimportant image features, so
quantizing these coeﬃcients (even all the way to zero) can lead to lossy compression that
is both eﬃcient and of high quality. Often, the image can be reconstructed from very
few transform coeﬃcients without any noticeable loss of quality. Figure 5.49a–c shows
three reconstructions of the simple 8×8 image of Figure 5.47. They were obtained from
only 32, 13, and 5 wavelet coeﬃcients, respectively.
Figure 5.50 is a similar example. It shows a bi-level image fully reconstructed from
just 4% of its transform coeﬃcients (653 coeﬃcients out of 128×128).
Experimenting is the key to understanding these concepts. Proper mathematical
software makes it easy to input images and experiment with various features of the
discrete wavelet transform.
In order to help the interested reader, Figure 5.51 lists
a Matlab program that inputs a grayscale image (in raw format, just rows of pixel
values), computes its Haar wavelet transform, discards a given percentage of the smallest
transform coeﬃcients, and then computes the inverse transform to reconstruct the image.
Lossy wavelet image compression involves the discarding of coeﬃcients (quantizing
them to zero), so the concept of sparseness ratio is deﬁned to measure the amount of
coeﬃcients discarded. Sparseness is deﬁned as the number of nonzero wavelet coeﬃ-
cients divided by the number of coeﬃcients left after some are discarded. The higher

210
5.
Image Compression
255 224 192 159 127
95
63
32
0
32
64 159 127 159 191 223
255 224 192 159 127
95
63
32
0
32
64 159 127 159 191 223
255 224 192 159 127
95
63
32
0
32
64 159 127 159 191 223
255 224 192 159 127
95
63
32
0
32
64 159 127 159 191 223
1051
34.0
-44.5
-0.7
-1.0 -62
0
-1.0
0
0.0
0.0
0.0
0.0
0
0
0.0
0
0.0
0.0
0.0
0.0
0
0
0.0
0
0.0
0.0
0.0
0.0
0
0
0.0
48 239.5 112.8 90.2 31.5
64 32 31.5
48 239.5 112.8 90.2 31.5
64 32 31.5
48 239.5 112.8 90.2 31.5
64 32 31.5
48 239.5 112.8 90.2 31.5
64 32 31.5
Figure 5.47: An 8×8 Image (top) and its Haar Transform (below).
Figure 5.48: (a) A 128×128 Image with Activity on the Right. (b) Its Transform.

5.7 The Wavelet Transform
211
Figure 5.49: Three Lossy Reconstructions of the Image of Figure 5.47.

212
5.
Image Compression
Figure 5.50: Reconstructing a 128×128 Simple Image from 4% of its Coeﬃcients.
the sparseness ratio, the fewer coeﬃcients are left. Higher sparseness ratios lead to bet-
ter compression but may result in poorly reconstructed images. The sparseness ratio
is distantly related to the compression factor, a compression measure deﬁned in the
Introduction.
The line “filename=’lena128’; dim=128;” contains the image ﬁle name and the
dimension of the image.
The image ﬁles used by the author were in raw form and
contained just the grayscale values, each as a single byte. There is no header, and not
even the image resolution (number of rows and columns) is included in the ﬁle. However,
Matlab can read other types of ﬁles. The image is assumed to be square, and parameter
“dim” should be a power of 2. The assignment “thresh=” speciﬁes the percentage of
transform coeﬃcients to be deleted. This provides an easy way to experiment with lossy
wavelet image compression.
File harmatt.m contains two functions that compute the Haar wavelet coeﬃcients
in a matrix form (Section 5.7.3).
(A technical note: A Matlab m ﬁle may include commands or a function but not
both. It may, however, contain more than one function, provided that only the top
function is invoked from outside the ﬁle. All the other functions must be called from
within the ﬁle. In our case, function harmatt(dim) calls function individ(n).)
⋄Exercise 5.13: Use the code of Figure 5.51 (or similar code) to compute the Haar
transform of the well-known Lena image and reconstruct it three times by discarding
more and more detail coeﬃcients.
5.7.3 A Matrix Approach
The principle of the Haar transform is to compute averages and diﬀerences. It turns
out that this can be done by means of matrix multiplication ([Mulcahy 96] and [Mulc-
ahy 97]). We use the top row of the 8×8 image of Figure 5.47 as an example. Anyone

5.7 The Wavelet Transform
213
clear; % main program
filename=’lena128’; dim=128;
fid=fopen(filename,’r’);
if fid==-1 disp(’file not found’)
else img=fread(fid,[dim,dim])’; fclose(fid);
end
thresh=0.0;
% percent of transform coefficients deleted
figure(1), imagesc(img), colormap(gray), axis off, axis square
w=harmatt(dim);
% compute the Haar dim x dim transform matrix
timg=w*img*w’;
% forward Haar transform
tsort=sort(abs(timg(:)));
tthresh=tsort(floor(max(thresh*dim*dim,1)));
cim=timg.*(abs(timg) > tthresh);
[i,j,s]=find(cim);
dimg=sparse(i,j,s,dim,dim);
% figure(2) displays the remaining transform coefficients
%figure(2), spy(dimg), colormap(gray), axis square
figure(2), image(dimg), colormap(gray), axis square
cimg=full(w’*sparse(dimg)*w);
% inverse Haar transform
density = nnz(dimg);
disp([num2str(100*thresh) ’% of smallest coefficients deleted.’])
disp([num2str(density) ’ coefficients remain out of ’ ...
num2str(dim) ’x’ num2str(dim) ’.’])
figure(3), imagesc(cimg), colormap(gray), axis off, axis square
File harmatt.m with two functions
function x = harmatt(dim)
num=log2(dim);
p = sparse(eye(dim)); q = p;
i=1;
while i<=dim/2;
q(1:2*i,1:2*i) = sparse(individ(2*i));
p=p*q; i=2*i;
end
x=sparse(p);
function f=individ(n)
x=[1, 1]/sqrt(2);
y=[1,-1]/sqrt(2);
while min(size(x)) < n/2
x=[x, zeros(min(size(x)),max(size(x)));...
zeros(min(size(x)),max(size(x))), x];
end
while min(size(y)) < n/2
y=[y, zeros(min(size(y)),max(size(y)));...
zeros(min(size(y)),max(size(y))), y];
end
f=[x;y];
Figure 5.51: Matlab Code for the Haar Transform of an Image.

214
5.
Image Compression
with a little experience with matrices can construct a matrix that when multiplied by
this vector results in a vector with four averages and four diﬀerences. Matrix A1 of
Equation (5.10) does that and, when multiplied by the top row of pixels of Figure 5.47,
generates (239.5, 175.5, 111.0, 47.5, 15.5, 16.5, 16.0, 15.5). Similarly, matrices A2 and A3
perform the second and third steps of the transform, respectively. The results are shown
in Equation (5.11):
A1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
2
1
2
0
0
0
0
0
0
0
0
1
2
1
2
0
0
0
0
0
0
0
0
1
2
1
2
0
0
0
0
0
0
0
0
1
2
1
2
1
2
−1
2
0
0
0
0
0
0
0
0
1
2
−1
2
0
0
0
0
0
0
0
0
1
2
−1
2
0
0
0
0
0
0
0
0
1
2
−1
2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
A1
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
255
224
192
159
127
95
63
32
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
239.5
175.5
111.0
47.5
15.5
16.5
16.0
15.5
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(5.10)
A2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
2
1
2
0
0
0
0
0
0
0
0
1
2
1
2
0
0
0
0
1
2
−1
2
0
0
0
0
0
0
0
0
1
2
−1
2
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
A3 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
2
1
2
0
0
0
0
0
0
1
2
−1
2
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
A2
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
239.5
175.5
111.0
47.5
15.5
16.5
16.0
15.5
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
207.5
79.25
32.0
31.75
15.5
16.5
16.0
15.5
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
A3
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
207.5
79.25
32.0
31.75
15.5
16.5
16.0
15.5
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
143.375
64.125
32.
31.75
15.5
16.5
16.
15.5
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(5.11)
Instead of calculating averages and diﬀerences, all we have to do is construct matri-
ces A1, A2, and A3, multiply them to get W = A1A2A3, and apply W to all the columns
of an image I by multiplying W ·I:
W
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
255
224
192
159
127
95
63
32
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
1
8
−1
8
−1
8
−1
8
−1
8
1
4
1
4
−1
4
−1
4
0
0
0
0
0
0
0
0
1
4
1
4
−1
4
−1
4
1
2
−1
2
0
0
0
0
0
0
0
0
1
2
−1
2
0
0
0
0
0
0
0
0
1
2
−1
2
0
0
0
0
0
0
0
0
1
2
−1
2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
255
224
192
159
127
95
63
32
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
143.375
64.125
32
31.75
15.5
16.5
16
15.5
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.

5.7 The Wavelet Transform
215
This, of course, is only half the job. In order to compute the complete transform, we
still have to apply W to the rows of the product W ·I, and we do this by applying it to
the columns of the transpose (W ·I)T , then transposing the result. Thus, the complete
transform is (see line timg=w*img*w’ in Figure 5.51)
Itr =
#
W(W ·I)T $T = W ·I·W T .
The inverse transform is performed by
W −1(W −1·IT
tr)T = W −1#
Itr·(W −1)T $
,
and this is where the normalized Haar transform (mentioned on page 200) becomes
important. Instead of calculating averages [quantities of the form (di + di+1)/2] and
diﬀerences [quantities of the form (di −di+1)], it is better to compute the quantities
(di+di+1)/
√
2 and (di−di+1)/
√
2. This results is an orthonormal matrix W, and it is well
known that the inverse of such a matrix is simply its transpose. Thus, we can write the
inverse transform in the simple form W T·Itr·W [see line cimg=full(w’*sparse(dimg)*w)
in Figure 5.51].
In between the forward and inverse transforms, some transform coeﬃcients may
be quantized or deleted. Alternatively, matrix Itr may be compressed by means of run
length encoding and/or Huﬀman codes.
Function individ(n) of Figure 5.51 starts with a 2×2 Haar transform matrix (notice
that it uses
√
2 instead of 2) and then uses it to construct as many individual matrices
Ai as necessary. Function harmatt(dim) combines those individual matrices to form
the ﬁnal Haar matrix for an image of dim rows and dim columns.
⋄Exercise 5.14: Perform the calculation W ·I·W T for the 8×8 image of Figure 5.47.
The past decade has witnessed the development of wavelet analysis, a new tool which
emerged from mathematics and was quickly adopted by diverse ﬁelds of science and
engineering. In the brief period since its creation in 1987–88, it has reached a certain
level of maturity as a well-deﬁned mathematical discipline, with its own conferences,
journals, research monographs, and textbooks proliferating at a rapid rate.
—Howard L. Resnikoﬀand Raymond O’Neil Wells,
Wavelet Analysis: The Scalable Structure of Information (1998)

216
5.
Image Compression
5.8 Filter Banks
So far, we have worked with the Haar transform, the simplest wavelet (and subband)
transform. We are now ready for the general subband transform. As a preparation for
the material in this section, we again examine the two main types of image transforms,
orthogonal and subband. An orthogonal linear transform is performed by computing the
inner product of the data (pixel values or audio samples) with a set of basis functions.
The result is a set of transform coeﬃcients that can later be quantized and encoded. In
contrast, a subband transform is performed by computing a convolution of the data with
a set of bandpass ﬁlters. Each of the resulting subbands encodes a particular portion of
the frequency content of the data.
Note. The discrete inner product of the two vectors fi and gi is deﬁned as the
following sum of products
⟨f, g⟩=

i
fi gi.
The discrete convolution h is denoted by f ⋆g and is deﬁned as
hi = f ⋆g =

j
fj gi−j.
(5.12)
(Each element hi of the discrete convolution h is the sum of products. It depends on i
in the special way shown in Equation (5.12).)
This section employs the matrix approach to the Haar transform to introduce the
reader to the idea of ﬁlter banks. We show how the Haar transform can be interpreted as
a bank of two ﬁlters, a lowpass and a highpass. We explain the terms “ﬁlter,” “lowpass,”
and “highpass” and show how the idea of ﬁlter banks leads naturally to the concept of
subband transform. The Haar transform, of course, is the simplest wavelet transform,
which is why it was used earlier to illustrate wavelet concepts. However, employing it
as a ﬁlter bank is not the most eﬃcient. Most practical applications of wavelet ﬁlters
employ more sophisticated sets of ﬁlter coeﬃcients, but they are all based on the concept
of ﬁlters and ﬁlter banks [Strang and Nguyen 96].
The simplest way to describe the discrete wavelet transform (DWT) is by means of
matrix multiplication, along the lines developed in Section 5.7.3. The Haar transform
depends on two ﬁlter coeﬃcients c0 and c1, both with a value of 1/
√
2 ≈0.7071. The
smallest transform matrix that can be constructed in this case is
#1
1
1 −1
$
/
√
2. It is a 2×2
matrix, and it generates two transform coeﬃcients, an average and a diﬀerence. (Notice
that these are not exactly an average and a diﬀerence, because
√
2 is used instead of 2.
Better names for them are coarse detail and ﬁne detail, respectively.) In general, the
DWT can use any set of wavelet ﬁlters, but it is computed in the same way regardless
of the particular ﬁlter used.
We start with one of the most popular wavelets, the Daubechies D4. As its name
implies, it is based on four ﬁlter coeﬃcients c0, c1, c2, and c3, whose values are listed in

5.8 Filter Banks
217
Equation (5.13). The transform matrix W is [compare with matrix A1, Equation (5.10)]
W =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
c0
c1
c2
c3
0
0
. . .
0
c3
−c2
c1
−c0
0
0
. . .
0
0
0
c0
c1
c2
c3
. . .
0
0
0
c3
−c2
c1
−c0
. . .
0
...
...
...
0
0
. . .
0
c0
c1
c2
c3
0
0
. . .
0
c3
−c2
c1
−c0
c2
c3
0
. . .
0
0
c0
c1
c1
−c0
0
. . .
0
0
c3
−c2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
When this matrix is applied to a column vector of data items (x1, x2, . . . , xn), its top
row generates the weighted sum s1 = c0x1 + c1x2 + c2x3 + c3x4, its third row generates
the weighted sum s2 = c0x3 + c1x4 + c2x5 + c3x6, and the other odd-numbered rows
generate similar weighted sums si. Such sums are convolutions of the data vector xi
with the four ﬁlter coeﬃcients. In the language of wavelets, each of them is called a
smooth coeﬃcient, and together they are termed an H smoothing ﬁlter.
In a similar way, the second row of the matrix generates the quantity d1 = c3x1 −
c2x2 + c1x3 −c0x4, and the other even-numbered rows generate similar convolutions.
Each di is called a detail coeﬃcient, and together they are referred to as a G ﬁlter. G
is not a smoothing ﬁlter. In fact, the ﬁlter coeﬃcients are chosen such that the G ﬁlter
generates small values when the data items xi are correlated. Together, H and G are
called quadrature mirror ﬁlters (QMF).
The discrete wavelet transform of an image can therefore be viewed as passing the
original image through a QMF that consists of a pair of lowpass (H) and highpass (G)
ﬁlters.
If W is an n×n matrix, it generates n/2 smooth coeﬃcients si and n/2 detail
coeﬃcients di. The transposed matrix is
W T =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
c0
c3
0
0
. . .
c2
c1
c1
−c2
0
0
. . .
c3
−c0
c2
c1
c0
c3
. . .
0
0
c3
−c0
c1
−c2
. . .
0
0
...
c2
c1
c0
c3
0
0
c3
−c0
c1
−c2
0
0
c2
c1
c0
c3
c3
−c0
c1
−c2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
It can be shown that in order for W to be orthonormal, the four coeﬃcients have to
satisfy the two relations c2
0 + c2
1 + c2
2 + c2
3 = 1 and c2c0 + c3c1 = 0. The other two
equations used to determine the four ﬁlter coeﬃcients are c3 −c2 + c1 −c0 = 0 and
0c3 −1c2 + 2c1 −3c0 = 0. They represent the vanishing of the ﬁrst two moments of the
sequence (c3, −c2, c1, −c0). The solutions are
c0 = (1 +
√
3)/(4
√
2) ≈0.48296,
c1 = (3 +
√
3)/(4
√
2) ≈0.8365,

218
5.
Image Compression
c2 = (3 −
√
3)/(4
√
2) ≈0.2241,
c3 = (1 −
√
3)/(4
√
2) ≈−0.1294.
(5.13)
Using a transform matrix W is conceptually simple, but not very practical, since W
should be of the same size as the image, which can be large. However, a look at W shows
that it is very regular, so there is really no need to construct the full matrix. It is enough
to have just the top row of W. In fact, it is enough to have just an array with the ﬁlter
coeﬃcients. Figure 5.52 lists Matlab code that performs this computation. Function
fwt1(dat,coarse,filter) takes a row vector dat of 2n data items, and another array,
filter, with ﬁlter coeﬃcients. It then calculates the ﬁrst coarse levels of the discrete
wavelet transform.
⋄Exercise 5.15: Write similar code for the inverse one-dimensional discrete wavelet
transform.
5.9 WSQ, Fingerprint Compression
This section presents WSQ, a wavelet-based image compression method that was speciﬁ-
cally developed to compress ﬁngerprint images. Other compression methods that employ
the wavelet transform can be found in [Salomon 07].
Most of us may not realize it, but ﬁngerprints are “big business.” The FBI started
collecting ﬁngerprints in the form of inked impressions on paper cards back in 1924,
and today they have about 200 million cards, occupying an acre of ﬁling cabinets in
the J. Edgar Hoover building in Washington, D.C. (The FBI, like many of us, never
throws anything away. They also have many “repeat customers,” which is why “only”
about 29 million out of the 200 million cards are distinct; these are the ones used for
running background checks.) What’s more, these cards keep accumulating at a rate of
30,000–50,000 new cards per day (this is per day, not per year)! There’s clearly a need
to digitize this collection, so it will occupy less space and will lend itself to automatic
search and classiﬁcation. The main problem is size (in bits). When a typical ﬁngerprint
card is scanned at 500 dpi, with eight bits/pixel, it results in about 10 Mb of data. Thus,
the total size of the digitized collection would be more than 2,000 terabytes (a terabyte
is 240 bytes); huge even by current (2008) standards.
⋄Exercise 5.16: Apply these numbers to estimate the size of a ﬁngerprint card.
Compression is therefore a must. At ﬁrst, it seems that ﬁngerprint compression
must be lossless because of the small but important details involved. However, lossless
image compression methods produce typical compression ratios of 0.5, whereas in order
to make a serious dent in the huge amount of data in this collection, compressions of
about 1 bpp or better are needed. What is needed is a lossy compression method that
results in graceful degradation of image details, and does not introduce any artifacts
into the reconstructed image. Most lossy image compression methods involve the loss
of small details and are therefore unacceptable, since small ﬁngerprint details, such as
sweat pores, are admissible points of identiﬁcation in court. This is where wavelets come
into the picture. Lossy wavelet compression, if carefully designed, can satisfy the criteria
above and result in eﬃcient compression where important small details are preserved or

5.9 WSQ, Fingerprint Compression
219
function wc1=fwt1(dat,coarse,filter)
%
The 1D Forward Wavelet Transform
%
dat must be a 1D row vector of size 2^n,
%
coarse is the coarsest level of the transform
%
(note that coarse should be <<n)
%
filter is an orthonormal quadrature mirror filter
%
whose length should be <2^(coarse+1)
n=length(dat); j=log2(n); wc1=zeros(1,n);
beta=dat;
for i=j-1:-1:coarse
alfa=HiPass(beta,filter);
wc1((2^(i)+1):(2^(i+1)))=alfa;
beta=LoPass(beta,filter) ;
end
wc1(1:(2^coarse))=beta;
function d=HiPass(dt,filter) % highpass downsampling
d=iconv(mirror(filter),lshift(dt));
% iconv is matlab convolution tool
n=length(d);
d=d(1:2:(n-1));
function d=LoPass(dt,filter) % lowpass downsampling
d=aconv(filter,dt);
% aconv is matlab convolution tool with time-
% reversal of filter
n=length(d);
d=d(1:2:(n-1));
function sgn=mirror(filt)
% return filter coefficients with alternating signs
sgn=-((-1).^(1:length(filt))).*filt;
A simple test of fwt1 is
n=16; t=(1:n)./n;
dat=sin(2*pi*t)
filt=[0.4830 0.8365 0.2241 -0.1294];
wc=fwt1(dat,1,filt)
which outputs
dat=
0.3827
0.7071
0.9239 1.0000 0.9239 0.7071 0.3827 0
-0.3827 -0.7071 -0.9239 -1.0000 -0.9239 -0.7071 -0.3827 0
wc=
1.1365 -1.1365 -1.5685 1.5685 -0.2271 -0.4239 0.2271 0.4239
-0.0281 -0.0818 -0.0876 -0.0421 0.0281 0.0818 0.0876 0.0421
Figure 5.52:
Code for the One-Dimensional Forward Discrete
Wavelet Transform.

220
5.
Image Compression
are at least identiﬁable. Figure 5.53a,b (obtained, with permission, from Christopher
M. Brislawn), shows two examples of ﬁngerprints and one detail, where ridges and sweat
pores can clearly be seen.
Figure 5.53: Examples of Scanned Fingerprints (courtesy Christopher Brislawn).
Compression is also necessary, because ﬁngerprint images are routinely sent between
law enforcement agencies. Overnight delivery of the actual card is too slow and risky
(there are no backup cards), and sending 10 Mb of data through a 9,600 baud modem
takes about three hours.
The method described here [Bradley et al. 93] has been adopted by the FBI as its
standard for ﬁngerprint compression [Federal Bureau of Investigations 93]. It involves
three steps: (1) a discrete wavelet transform, (2) adaptive scalar quantization of the
wavelet transform coeﬃcients, and (3) a two-pass Huﬀman coding of the quantization
indices.
This is the reason for the name wavelet/scalar quantization, or WSQ. The
method typically produces compression factors of about 20. Decoding is the reverse of
encoding, so WSQ is a symmetric compression method.
The ﬁrst step is a symmetric discrete wavelet transform (SWT) using the symmetric
ﬁlter coeﬃcients listed in Table 5.54 (where R indicates the real part of a complex
number). They are symmetric ﬁlters with seven and nine impulse response taps, and
they depend on the two numbers x1 (real) and x2 (complex). The ﬁnal standard adopted
by the FBI uses the values
x1 = A + B −1
6,
x2 = −(A + B)
2
−1
6 + i
√
3(A −B)
2
,
where
A =
!
−14
√
15 + 63
1080
√
15
"1/3
,
and B =
!
−14
√
15 −63
1080
√
15
"1/3
.
This wavelet image decomposition can be called symmetric. It is shown in Fig-
ure 5.55. The SWT is ﬁrst applied to the image rows and columns, resulting in 4×4 = 16

5.9 WSQ, Fingerprint Compression
221
Tap
Exact value
Approximate value
h0(0)
−5
√
2x1(48|x2|2 −16Rx2 + 3)/32
0.852698790094000
h0(±1)
−5
√
2x1(8|x2|2 −Rx2)/8
0.377402855612650
h0(±2)
−5
√
2x1(4|x2|2 + 4Rx2 −1)/16
−0.110624404418420
h0(±3)
−5
√
2x1(Rx2)/8
−0.023849465019380
h0(±4)
−5
√
2x1/64
0.037828455506995
h1(−1)
√
2(6x1 −1)/16x1
0.788485616405660
h1(−2, 0)
−
√
2(16x1 −1)/64x1
−0.418092273222210
h1(−3, 1)
√
2(2x1 + 1)/32x1
−0.040689417609558
h1(−4, 2)
−
√
2/64x1
0.064538882628938
Table 5.54: Symmetric Wavelet Filter Coeﬃcients for WSQ.
subbands. The SWT is then applied in the same manner to three of the 16 subbands,
decomposing each into 16 smaller subbands. The last step is to decompose the top-left
subband into four smaller ones.
0 1
2 3
10
16
18
40
42
48
50 51
54
55
57
56
60
61
59
58
62
63
52
53
8
21
27
29
19
22
28
30
20
25
31
33
23
26
32
34
24
9
15
17
39
41
47
49
7
6
12
14
36
38
44
46
5
11
13
35
37
43
45
4
Figure 5.55: Symmetric Image Wavelet Decomposition.
The larger subbands (51–63) contain the ﬁne-detail, high-frequency information of
the image. They can later be heavily quantized without loss of any important informa-
tion (i.e., information needed to classify and identify ﬁngerprints). In fact, subbands

222
5.
Image Compression
60–63 are completely discarded.
Subbands 7–18 are important.
They contain that
portion of the image frequencies that corresponds to the ridges in a ﬁngerprint. This
information is important and should be quantized lightly.
The transform coeﬃcients in the 64 subbands are ﬂoating-point numbers to be
denoted by a. They are quantized to a ﬁnite number of ﬂoating-point numbers that are
denoted by ˆa. The WSQ encoder maps a transform coeﬃcient a to a quantization index
p (an integer that is later mapped to a code that is itself Huﬀman encoded). The index
p can be considered a pointer to the quantization bin where a lies. The WSQ decoder
receives an index p and maps it to a value ˆa that is close, but not identical, to a. This
is how WSQ loses image information. The set of ˆa values is a discrete set of ﬂoating-
point numbers called the quantized wavelet coeﬃcients. The quantization depends on
parameters that may vary from subband to subband, since diﬀerent subbands have
diﬀerent quantization requirements.
Figure 5.56 shows the setup of quantization bins for subband k. Parameter Zk is
the width of the zero bin, and parameter Qk is the width of the other bins. Parameter
C is in the range [0, 1].
It determines the reconstructed value ˆa.
For C = 0.5, for
example, the reconstructed value for each quantization bin is the center of the bin.
Equation (5.14) shows how parameters Zk and Qk are used by the WSQ encoder to
quantize a transform coeﬃcient ak(m, n) (i.e., a coeﬃcient in position (m, n) in subband
k) to an index pk(m, n) (an integer), and how the WSQ decoder computes a quantized
coeﬃcient ˆak(m, n) from that index:
pk(m, n) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
)
ak(m,n)−Zk/2
Qk
*
+ 1,
ak(m, n) > Zk/2,
0,
−Zk/2 ≤ak(m, n) ≤Zk/2,
+
ak(m,n)+Zk/2
Qk
,
+ 1,
ak(m, n) < −Zk/2,
(5.14)
ˆak(m, n) =
⎧
⎪
⎨
⎪
⎩
#
pk(m, n) −C
$
Qk + Zk/2,
pk(m, n) > 0,
0,
pk(m, n) = 0,
#
pk(m, n) + C
$
Qk −Zk/2,
pk(m, n) < 0.
The ﬁnal standard adopted by the FBI uses the value C = 0.44 and determines the bin
widths Qk and Zk from the variances of the coeﬃcients in the diﬀerent subbands in the
following steps:
Step 1: Let the width and height of subband k be denoted by Xk and Yk, respectively.
We compute the six quantities
Wk =
-3Xk
4
.
,
Hk =
-7Yk
16
.
,
x0k =
-Xk
8
.
,
x1k = x0k + Wk −1,
y0k =
-9Yk
32
.
,
y1k = y0k + Hk −1.

5.9 WSQ, Fingerprint Compression
223
Step 2: Assuming that position (0, 0) is the top-left corner of the subband, we use the
subband region from position (x0k, y0k) to position (x1k, y1k) to estimate the variance
σ2
k of the subband by
σ2
k =
1
W ·H −1
x1k

n=x0k
y1k

m=y0k
#
ak(m, n) −µk
$2,
where µk denotes the mean of ak(m, n) in the region.
Step 3: Parameter Qk is computed by
q Qk =
⎧
⎪
⎨
⎪
⎩
1,
0 ≤k ≤3,
10
Ak loge(σ2
k),
4 ≤k ≤59, and σ2
k ≥1.01,
0,
60 ≤k ≤63, or σ2
k < 1.01,
where q is a proportionality constant that controls the bin widths Qk and thereby the
overall level of compression. The procedure for computing q is complex and will not be
described here. The values of the constants Ak are
Ak =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1.32,
k = 52, 56,
1.08,
k = 53, 58,
1.42,
k = 54, 57,
1.08,
k = 55, 59,
1,
otherwise.
Notice that the bin widths for subbands 60–63 are zero. As a result, these subbands,
containing the ﬁnest detail coeﬃcients, are simply discarded.
Step 4: The width of the zero bin is set to Zk = 1.2Qk.
Z/2+Q
Z/2
Z/2+Q(1−C)
Z/2+Q(2−C)
Z/2+Q(3−C)
Z/2+3Q
a
a
Figure 5.56: WSQ Scalar Quantization.
The WSQ encoder computes the quantization indices pk(m, n) as shown, then maps
them to the 254 codes shown in Table 5.57. These values are encoded with Huﬀman codes
(using a two-pass process), and the Huﬀman codes are then written on the compressed
ﬁle. A quantization index pk(m, n) can be any integer, but most are small and there
are many zeros. Thus, the codes of Table 5.57 are divided into three groups. The ﬁrst

224
5.
Image Compression
group consists of 100 codes (codes 1 through 100) for run lengths of 1 to 100 zero indices.
The second group is codes 107 through 254. They specify small indices, in the range
[−73, +74]. The third group consists of the six escape codes 101 through 106. They
indicate large indices or run lengths of more than 100 zero indices. Code 180 (which
corresponds to an index pk(m, n) = 0) is not used, because this case is really a run
length of a single zero. An escape code is followed by the (8-bit or 16-bit) raw value of
the index (or size of the run length). Here are some examples:
An index pk(m, n) = −71 is coded as 109. An index pk(m, n) = −1 is coded as 179.
An index pk(m, n) = 75 is coded as 101 (escape for positive 8-bit indices) followed by
75 (in eight bits). An index pk(m, n) = −259 is coded as 104 (escape for negative large
indices) followed by 259 (the absolute value of the index, in 16 bits). An isolated index
of zero is coded as 1, and a run length of 260 zeros is coded as 106 (escape for large run
lengths) followed by 260 (in 16 bits). Indices or run lengths that require more than 16
bits cannot be encoded, but the particular choices of the quantization parameters and
the wavelet transform virtually guarantee that large indices will never be generated.
Code
Index or run length
1
run length of 1 zeros
2
run length of 2 zeros
3
run length of 3 zeros
...
100
run length of 100 zeros
101
escape code for positive 8-bit index
102
escape code for negative 8-bit index
103
escape code for positive 16-bit index
104
escape code for negative 16-bit index
105
escape code for zero run, 8-bit
106
escape code for zero run, 16-bit
107
index value −73
108
index value −72
109
index value −71
...
179
index value −1
180
unused
181
index value 1
...
253
index value 73
254
index value 74
Table 5.57: WSQ Codes for Quantization Indices and Run Lengths.
The last step is to prepare the Huﬀman code tables. They depend on the image,
so they have to be written on the compressed ﬁle. The standard adopted by the FBI
speciﬁes that subbands be grouped into three blocks and all the subbands in a group
use the same Huﬀman code table. This facilitates progressive transmission of the image.

Chapter Summary
225
The ﬁrst block consists of the low- and mid-frequency subbands 0–18. The second and
third blocks contain the highpass detail subbands 19–51 and 52–59, respectively (recall
that subbands 60–63 are completely discarded). Two Huﬀman code tables are prepared,
one for the ﬁrst block and the other for the second and third blocks.
A Huﬀman code table for a block of subbands is prepared by counting the number
of times each of the 254 codes of Table 5.57 appears in the block. The counts are used
to determine the length of each code and to construct the Huﬀman code tree. This is a
two-pass job (one pass to determine the code tables and another pass to encode), and
it is done in a way similar to the use of the Huﬀman code by JPEG (Section 5.6.4).
O’Day ﬁgured that that was more than he’d had the right to expect under the cir-
cumstances. A ﬁngerprint identiﬁcation ordinarily required ten individual points—the
irregularities that constituted the art of ﬁngerprint identiﬁcation—but that number
had always been arbitrary. The inspector was certain that Cutter had handled this
computer disk, even if a jury might not be completely sure, if that time ever came.
—Tom Clancy, Clear and Present Danger
Chapter Summary
Images are an important type of digital multimedia data. Images are popular, they are
easy to create (by a digital camera, scanning a document, or by creating a drawing or
an illustration), and they feature several types of redundancies, which makes it easy to
come up with methods for compressing them. In addition, the human visual system
can perceive the general form and many details of an image, but it cannot register the
precise color of every pixel.
We therefore say that a typical image has much noise,
and this feature allows for much loss of original image information when the image is
compressed and then decompressed.
The chapter starts by discussing the various types of images, bi-level, grayscale,
continuous-tone, discrete-tone, and cartoon-like. It then states the main principle of im-
age compression, a principle that stems from the correlation of pixels. Eight approaches
to image compression are brieﬂy discussed, all of them based on the main principle.
The remainder of the chapter concentrates on image transforms (Section 5.3) and in
particular on orthogonal and wavelet transforms. The popular JPEG method is based
on the discrete cosine transform (DCT), one of the important orthogonal transforms,
and is explained in detail (Section 5.6).
The last part of the chapter, starting at Section 5.7, is devoted to the wavelet
transform. This type of transform is introduced by the Haar transform, which serves to
illustrate the concept of subbands and their importance. Finally, Section 5.9 discusses
WSQ, a sophisticated wavelet-based image compression method that was developed
speciﬁcally for the compression of ﬁngerprint images.
Self-Assessment Questions
1. Explain why this is the longest chapter in this book.

226
5.
Image Compression
2. The zigzag sequence employed by JPEG starts at the top-left corner of an 8 × 8
unit and works its way to the bottom-right corner in zigzag. This way, the sequence
proceeds from large to small transform coeﬃcients and may therefore contain runs of
zero coeﬃcients. Propose other (perhaps more sophisticated) ways to scan such a unit
from large coeﬃcients to small ones.
3. Section 5.7.1 discusses the standard and pyramid subband transforms. Check
the data compression literature for other ways to apply a two-dimensional subband
transform to the entire image.
4. Figure 5.27 illustrates the blocking artifacts caused by JPEG when it is asked
to quantize the DCT transform coeﬃcients too much. Locate a JPEG implementation
that allows the user to select the degree of compression (which it does by quantizing the
DCT coeﬃcients more or quantizing them less) and run it repeatedly, asking for better
and better compression, until the decompressed image clearly shows these artifacts.
5. Figure 5.52 lists Matlab code for performing a one-dimensional discrete wavelet
transform with the four ﬁlter coeﬃcients 0.4830, 0.8365, 0.2241, and −0.1294. Copy
this code from the book’s web site and run it with other sets of ﬁlter coeﬃcients (ref-
erence [Salomon 07] has examples of other sets). Even better, rewrite this code in a
programming language of your choice.
A picture of many colors proclaims images of many thoughts.
—Donna A. Favors

6
Audio Compression
In the Introduction, it is mentioned that the electronic digital computer was originally
conceived as a fast, reliable calculating machine. It did not take computer users long to
realize that a computer can also store and process nonnumeric data. The term “multi-
media,” which became popular in the 1990s, refers to the ability to digitize, store, and
manipulate in the computer all kinds of data, not just numbers. Previous chapters dis-
cussed digital images and methods for their compression, and this chapter concentrates
on audio data.
An important fact about audio compression is that decoding must be fast. Given
a compressed text ﬁle, we don’t mind waiting until it is fully decompressed before we
can read it. However, given a compressed audio ﬁle, we often want to listen to it while
it is decompressed (in fact, we decompress it only in order to listen to it). This is why
audio compression methods tend to be asymmetric. The encoder can be sophisticated,
complex, and slow, but the decoder must be fast.
First, a few words about audio and how it is digitized. The term audio refers to the
recording and reproduction of sound. Sound is a wave. It can be viewed as a physical
disturbance in the air (or some other medium) or as a pressure wave propagated by the
vibrations of molecules. A microphone is a device that senses sound and converts it to
an electrical wave, a voltage that varies continuously with time in the same way as the
sound. To convert this voltage into a format where it can be input into a computer,
stored, edited, and played back, the voltage is sampled many times each second. Each
audio sample is a number whose value is proportional to the voltage at the time of
sampling.
Figure Intro.1, duplicated here, shows a wave sampled at three points in
time. It is obvious that the ﬁrst sample is a small number and the third sample is a
large number, close to the maximum.

228
6.
Audio Compression
Amplitude
points
Sampling
Time
High frequency region
Maximum amplitude
Figure Intro.1. Sound Wave and Three Samples.
Thus, audio sampling (or digitized sound) is a simple concept, but its success in
practice depends on two important factors, the sampling rate and the sample size. How
many times should a sound wave be sampled each second and how large (how many
bits) should each sample be? Sampling too often creates too many audio samples, while
a very low sampling rate results in low-quality played-back sound. It seems intuitively
that the sampling rate should depend on the frequency, but the frequency of a sound
wave varies all the time, whereas the sampling rate should remain constant (a variable
sampling rate makes it diﬃcult to edit and play back the digitized sound). The solution
was discovered back in the 1920s by H. Nyquist. It states that the optimum sampling
frequency should be slightly greater than twice the maximum frequency of the sound.
The sound wave of Figure Intro.1 has a region of high frequency at its center. To obtain
the optimum sampling rate for this particular wave, we should determine the maximum
frequency at this region, double it, and increase the result slightly. The process of audio
sampling is also known as analog-to-digital conversion (ADC).
Every sound wave has its own maximum frequency, but the digitized sound used
in practical applications is based on the fact that the highest frequency that the human
ear can perceive is about 22,000 Hz. The optimum sampling rate that corresponds to
this frequency is 44,100 Hz, and this rate is used when sound is digitized and recorded
on a CD or DVD.
Modern computers are based on 8-bit storage units called bytes, which is why many
quantities, including audio samples, are stored in a computer in a byte or several bytes.
If each audio sample is a byte, there can be 256 sample sizes, so the digitized audio can
have up to 256 diﬀerent amplitudes. If the highest voltage produced by a microphone
is 1 volt, then 8-bit audio samples can distinguish voltages as low as 1/256 ≈0.004 volt
or 4 millivolts (mv). Any quiet sound that is converted by the microphone to a lower
voltage would result in audio samples of zero and played back as silence. This is why
most ADC converters create 16-bit audio samples. Such a sample can have 216 = 65,536
values, so it can distinguish sounds as low as 1/65,536 volt ≈15 microvolt (µv). Thus,
the sample size can be considered quantization of the original, analog, audio signal.
Eight-bit samples correspond to coarse quantization, while 16-bit samples lead to ﬁne
quantization and thus to better quality of the played-back sound.
Audio sampling (or ADC) is also known as pulse-code-modulation (PCM), a term
often found in the professional literature.

Prelude
229
Armed with this information, we can estimate the sizes of various audio ﬁles and
thereby show why audio compression is so important. A typical 3-minute song lasts
180 sec and results in 180 × 44,100 = 7,938,000 audio samples when it is digitized (for
stereo sound, the number of samples is double that). For 16-bit samples, this translates
to close to 16 Mb, bigger than most still images. A 30-minute symphony is ten times
longer, so it results in a 160 Mb ﬁle when digitized. Thus, audio ﬁles are much bigger
than text ﬁles and can easily be bigger than (raw) image ﬁles. Another point to consider
is that audio compression, similar to image compression, can be lossy and thus feature
large compression factors.
⋄Exercise 6.1: It is a handy rule of thumb that an average book occupies about a million
bytes. Explain why this makes sense.
Approaches to audio compression. The problem of compressing an audio ﬁle
can be approached from various directions, because audio data has several sources of
redundancy. The discussion that follows concentrates on three common approaches.
The main source of redundancy in digital audio is the fact that adjacent audio
samples tend to be similar; they are correlated. With 44,100 samples each second, it is
no wonder that adjacent samples are virtually always similar. Audio data where many
audio samples are very diﬀerent from their immediate neighbors would sound harsh and
dissonant.
Thus, a simple approach to audio compression is to subtract each audio sample from
its predecessor and encode the diﬀerences (which are termed errors or residuals and tend
to be small integers) with suitable variable-length codes. Experience suggests that the
Rice codes (Section 1.1.3) are a good choice for this task.
Practical methods often
“predict” the current sample by computing a weighted sum of several of its immediate
neighbors, and then subtracting the current sample from the prediction. When done
carefully, linear prediction (Section 6.3) produces very small residuals (smaller than those
produced by simple subtraction). Because the residuals are integers, smaller residuals
implies few residual values and therefore eﬃcient encoding (for example, if the residuals
are in the interval [−6, 5], then there are only 12 residual values and only 12 variable-
length codes are needed, making it possible to choose very short codes). The MLP and
FLAC lossless compression methods [Salomon 07] are examples of this approach.
⋄Exercise 6.2: The ﬁrst audio sample has no predecessor, so how can it be encoded?
Companding is an approach to lossy audio compression (companding is short for
compressing/expanding). It is based on the experimental fact that the human ear is more
sensitive to low sound amplitudes and less sensitive to high amplitudes. The idea is to
quantize each audio sample by a diﬀerent amount according to its size (recall that the size
of a sample is proportional to the sound amplitude). Large samples, which correspond
to high amplitudes, are quantized more than small samples. Thus, companding is based
on nonlinear quantization. Section 6.1 says more about companding. The µ-law and
A-law compression methods (Section 6.4) are examples of this approach.
Another source of redundancy in audio is the limitations of the ear–brain system.
The human ear is very sensitive to sound, but its sensitivity is not uniform (Section 6.2)
and it depends on the frequency. Also, a loud sound may severely aﬀect the sensitivity of

230
6.
Audio Compression
the ear for a short period of time. Scientiﬁc experiments conducted over many years have
taught us much about how the ear–brain system responds to sound in various situations,
and this knowledge can be exploited to achieve very eﬃcient lossy audio compression.
The idea is to analyze the raw audio second by second, to identify those parts of the
audio to which the ear is not sensitive, and to heavily quantize the audio samples in
those parts.
This approach is the principle of the popular lossy mp3 method [Brandenburg and
Stoll 94]. The mp3 encoder is complex, because (1) it has to identify the frequencies
contained in the input sound at each point in time and (2) it has to decide which parts
of the original audio will not be heard by the ear. Recall that the input to the encoder is
a set of audio samples, not a sound wave. Thus, the encoder has to prepare overlapping
subsets of the samples and apply a Fourier transform to each subset to determine the
frequencies of the sound contained in it. The encoder also has to include a psychoacoustic
model in order to decide which sounds will not be heard by the ear. The decoder, in
contrast, is very simple.
This chapter starts with a detailed discussion of companding (Section 6.1), the
human auditory system (Section 6.2), and linear prediction (Section 6.3). This material
is followed by descriptions of three audio compression algorithms, µ-law and A-law
companding (Section 6.4) and Shorten (Section 6.5).
6.1 Companding
Companding (short for “compressing/expanding”) is a simple nonlinear technique based
on the experimental fact that the ear requires more precise samples at low amplitudes
(soft sounds), but is more forgiving at higher amplitudes. The typical ADC found in
many personal computers converts voltages to numbers linearly. If an amplitude a is
converted to the number n, then amplitude 2a will be converted to the number 2n. A
compression method based on companding, however, is nonlinear. It examines every
audio sample in the sound ﬁle, and employs a nonlinear relation to reduce the number
of bits devoted to it. For 16-bit samples, for example, a companding encoder may use a
formula as simple as
mapped = 32,767

2
sample
65535 −1

(6.1)
to reduce each sample.
This formula maps the 16-bit samples nonlinearly to 15-bit
numbers (i.e., numbers in the range [0, 32,767]) such that small samples are less aﬀected
than large ones. Table 6.1 illustrates the nonlinearity of this mapping. It shows eight
pairs of samples, where the two samples in each pair diﬀer by 100. The two samples of
the ﬁrst pair are mapped to numbers that diﬀer by 34, whereas the two samples of the
last pair are mapped to numbers that diﬀer by 65. The mapped 15-bit numbers can be
decoded back into the original 16-bit samples by the inverse formula
Sample = 65,535 log2

1 +
mapped
32,767

.
(6.2)

6.2 The Human Auditory System
231
Sample
Mapped Diﬀ
Sample
Mapped Diﬀ
100 →
35
30,000 →
12,236
200 →
69
34
30,100 →
12,283
47
1,000 →
348
40,000 →
17,256
1,100 →
383
35
40,100 →
17,309
53
10,000 →
3,656
50,000 →
22,837
10,100 →
3,694
38
50,100 →
22,896
59
20,000 →
7,719
60,000 →
29,040
20,100 →
7,762
43
60,100 →
29,105
65
Table 6.1: 16-Bit Samples Mapped to 15-Bit Numbers.
Reducing 16-bit numbers to 15 bits doesn’t produce much compression.
Better
results can be achieved by substituting a smaller number for 32,767 in equations (6.1)
and (6.2). A value of 127, for example, would map each 16-bit sample into an 8-bit
integer, yielding a compression ratio of 0.5. However, decoding would be less accurate.
A 16-bit sample of 60,100, for example, would be mapped into the 8-bit number 113,
but this number would produce 60,172 when decoded by Equation (6.2). Even worse,
the small 16-bit sample 1,000 would be mapped into 1.35, which has to be rounded to 1.
When Equation (6.2) is used to decode a 1, it produces 742, signiﬁcantly diﬀerent from
the original sample. The amount of compression should therefore be a user-controlled
parameter, and this is an interesting example of a compression method where the com-
pression ratio is known in advance!
In practice, there is no need to go through Equations (6.1) and (6.2), since the
mapping of all the samples can be prepared in advance in a table. Both encoding and
decoding are therefore fast.
Companding is not limited to Equations (6.1) and (6.2). More sophisticated meth-
ods, such as µ-law and A-law, are commonly used and have been designated international
standards.
What do the integers 3, 7, 10, 11, 12 have in common?
6.2 The Human Auditory System
The frequency range of the human ear is from about 20 Hz to about 20,000 Hz, but the
ear’s sensitivity to sound is not uniform. It depends on the frequency, and experiments
indicate that in a quiet environment the ear’s sensitivity is maximal for frequencies in the
range 2 kHz to 4 kHz. Figure 6.2a shows the hearing threshold for a quiet environment.
Any sound whose amplitude is below the curve will not be heard by the ear.
The
threshold curve makes it clear that the ear’s sensitivity is minimal at very low and very
high frequencies and reaches a maximum for sound frequencies around 5 kHz.
⋄Exercise 6.3: Propose an appropriate way to conduct such experiments.
It should also be noted that the range of the human voice is much more limited. It
is only from about 500 Hz to about 2 kHz.

232
6.
Audio Compression
2
0
20
40
60
80
4
6
8
10
12
14
14
18
20
22
23
24
25
16
(c)
dB
KHz
Bark
2
0
10
20
30
40
4
6
8
10
12
14
16
(a)
dB
KHz
Frequency
2
0
10
20
30
40
4
6
8
10
12
14
16
(b)
dB
KHz
Frequency
x
Figure 6.2: Threshold and Masking of Sound.

6.2 The Human Auditory System
233
The existence of the hearing threshold suggests an approach to lossy audio compres-
sion. Apply a Fourier transform to determine the frequency of the sound at any point,
associate each audio sample with a frequency, and delete any sample whose correspond-
ing amplitude is below this threshold. Since the threshold depends on the frequency, the
encoder needs to know the frequency spectrum of the sound being compressed at any
time. The encoder therefore has to save several of the previously-input audio samples
at any time (n−1 samples, where n is either a constant or a user-controlled parameter).
When the current sample is input, the ﬁrst step is to compute the Fourier transform of
the most-recent n samples in order to reveal the frequencies contained in this part of
the audio. The result is a number m of values (called signals) that indicate the strength
of the sound at m diﬀerent frequencies. If a sample for frequency f is smaller than the
hearing threshold at f, it (the sample) should be deleted.
In addition, two more properties of the human hearing system are exploited for
eﬀective lossy audio compression. They are frequency masking and temporal masking.
Frequency masking (also known as auditory masking) occurs when a soft sound
that we can normally hear (because it is not too soft) is masked by another sound at a
nearby frequency. The thick arrow in Figure 6.2b represents a strong sound source at
800 kHz. This source temporarily raises the normal threshold in its vicinity (the dashed
curve), with the result that the nearby sound represented by the arrow at “x”, a sound
that would normally be audible because it is above the threshold, is now masked, and is
inaudible. A good lossy audio compression method should identify this case and delete
the audio samples that correspond to sound “x”, because it cannot be heard anyway.
This is a complex but very eﬀective technique for the lossy compression of sound.
The frequency masking (the width of the dashed curve of Figure 6.2b) depends on
the frequency. It varies from about 100 Hz for the lowest audible frequencies to more
than 4 kHz for the highest. The range of audible frequencies can therefore be partitioned
into a number of critical bands that indicate the declining sensitivity of the ear (more
accurately, its declining resolving power) for higher frequencies. We can think of the
critical bands as a measure similar to frequency. However, in contrast to frequency,
which is absolute and independent of human hearing, the critical bands are determined
according to the sound perception of the ear.
Thus, they constitute a perceptually
uniform measure of frequency. Table 6.3 lists 27 approximate critical bands.
Another way to describe critical bands is to say that because of the ear’s limited
perception of frequencies, the threshold at a frequency f is raised by a nearby sound
only if that sound is within the critical band of f. This also points the way to developing
a practical lossy compression algorithm. The audio signal should ﬁrst be transformed
into its frequency domain, and the resulting values (the frequency spectrum) should be
divided into subbands that resemble the critical bands as much as possible. Once this is
done, the signals in each subband should be quantized such that the quantization noise
(the diﬀerence between the original audio sample and its quantized value) should be
inaudible.
Yet another way to look at the concept of critical bands is to consider the human
auditory system as a ﬁlter that lets through only frequencies in the range (bandpass)
of 20 Hz to 20,000 Hz. We visualize the ear–brain system as a collection of ﬁlters, each
with a diﬀerent bandpass. The bandpasses are called critical bands. They overlap and
they have diﬀerent widths. They are narrow (about 100 Hz) at low frequencies and

234
6.
Audio Compression
band
range
band
range
band
range
0
0–50
9
800–940
18
3280–3840
1
50–95
10
940–1125
19
3840–4690
2
95–140
11
1125–1265
20
4690–5440
3
140–235
12
1265–1500
21
5440–6375
4
235–330
13
1500–1735
22
6375–7690
5
330–420
14
1735–1970
23
7690–9375
6
420–560
15
1970–2340
24
9375–11625
7
560–660
16
2340–2720
25
11625–15375
8
660–800
17
2720–3280
26
15375–20250
Table 6.3: Twenty-Seven Approximate Critical Bands.
become wider (to about 4–5 kHz) at high frequencies.
The width of a critical band is called its size. The widths of the critical bands
introduce a new unit, the Bark (after H. G. Barkhausen) such that one Bark is the
width (in Hz) of one critical band. The Bark is deﬁned as
1 Bark =

f
100,
for frequencies f < 500 Hz,
9 + 4 log2

f
1000

,
for frequencies f ≥500 Hz.
Figure 6.2c shows some critical bands, with Barks between 14 and 25, positioned above
the threshold.
Heinrich Georg Barkhausen was born on December 2, 1881,
in Bremen, Germany. He spent his entire career as a professor of elec-
trical engineering at the Technische Hochschule in Dresden, where
he concentrated on developing electron tubes. He also discovered
the so-called “Barkhausen eﬀect,” where acoustical waves are gener-
ated in a solid by the movement of domain walls when the material
is magnetized. He also coined the term “phon” as a unit of sound
loudness. The institute in Dresden was destroyed, as was most of
the city, in the famous ﬁre bombing in February 1945. After the
war, Barkhausen helped rebuild the institute. He died on February
20, 1956.
The dashed masking curve of Figure 6.2b is temporary and it disappears quickly.
It illustrates temporal masking. This type of masking may occur when a strong sound
A of frequency f is preceded or followed in time by a weaker sound B at a nearby (or
identical) frequency. If the time interval between A and B is short, sound B may not
be audible. Figure 6.4 illustrates an example of temporal masking. The threshold of
temporal masking due to a loud sound at time 0 goes down, ﬁrst sharply and then slowly.

6.3 Linear Prediction
235
20
30
40
60
20
−20
10
0
−10
50
−50
100
−100
200
−200
500
−500
dB
ms
Time
Figure 6.4: Threshold and Masking of Sound.
A weaker sound of 30 dB will not be audible if it occurs 10 ms before or after the loud
sound, but will be audible if the time interval between the sounds is 20 ms.
6.3 Linear Prediction
In a stream of correlated audio samples s(t), almost every sample s(t) is similar to its
predecessor s(t −1) and its successor s(t + 1). Thus, a simple subtraction s(t) −s(t −1)
normally produces a small diﬀerence. Sound, however, is a wave, and this is reﬂected
in the audio samples. Consecutive audio samples may become larger and larger and
be followed by smaller and smaller samples. It therefore makes sense to assume that
an audio sample is related in a simple way to several of its immediate predecessors
and several of its successors. This assumption is the basis of the technique of linear
prediction. A predicted value ˆs(t) for the current sample s(t) is computed from the p
immediately-preceding samples by a linear combination
ˆs(t) =
p

i=1
ais(t −i).
Notice that only the samples preceding s(t) can be used to predict it. The encoder
cannot use any of the successors of s(t) because the decoder must be able to compute
the same prediction as the encoder, but it (the decoder) is decoding s(t) so it hasn’t yet
decoded any of its successors.
If linear prediction is done properly, the resulting diﬀerences (also termed errors or
residuals) e(t) = s(t) −ˆs(t) will almost always be a small (positive or negative) integers.
The simplest type of wave is stationary.
In such a wave, a single set of coeﬃcients
ai always produces the best prediction. Naturally, most waves are not stationary and
should select a diﬀerent set of ai coeﬃcients to predict each sample. Such selection can
be done in diﬀerent ways, involving more and more neighbor samples, and this results
in predictors of diﬀerent orders. A few such predictors are described here.

236
6.
Audio Compression
A zeroth-order predictor simply predicts each sample s(t) as zero. A ﬁrst-order
predictor (Figure 6.5a) predicts each s(t) as equal to its predecessor s(t −1). Similarly,
a second-order predictor (Figure 6.5b) computes a straight segment (a linear function or
a degree-1 polynomial) from s(t−2) to s(t−1) and continues it to predict s(t). Extending
this idea to one more point, a third-order predictor (Figure 6.5c) computes a degree-2
polynomial (a conic section) that passes through the three points s(t −3), s(t −2), and
s(t −1) and extrapolates it to predict s(t). In general, an nth-order predictor computes
a degree-(n −1) polynomial that passes through the n points s(t −n) through s(t −1)
and extrapolates it to predict s(t). This section shows how to compute several such
predictors.
(a)
(b)
(c)
• • •
• • • • •
• • •
• • •
• • • • •
• • •
• • •
• • • • •
• • •
• • •
• • • • •
• • •
• • •
• • • • •
• • •
• • •
• • • • •
• • •
• • •
• • • • •
• • •
• • •
• • • • •
• • •
• • •
• • • • •
• • •
Figure 6.5: Predictors of Orders 1, 2, and 3.
Given the two points P2 = (t−2, s2) and P1 = (t−1, s1), we can write the parametric
equation of the straight segment connecting them as
L(u) = (1 −u)P2 + u P1 = (1 −u)(t −2, s2) + u(t −1, s1) = (u + t −2, (1 −u)s2 + u s1).
It’s easy to see that L(0) = P2 and L(1) = P1. Extrapolating to the next point, at
u = 2, yields L(2) = (t, 2s1 −s2). Using our notation, we conclude that the second-order
predictor predicts sample s(t) as the linear combination 2s(t −1) −s(t −2).
For the third-order predictor, we start with the three points P3 = (t −3, s3), P2 =
(t −2, s2), and P1 = (t −1, s1). The degree-2 polynomial that passes through those
points is given by the uniform quadratic Lagrange interpolation polynomial (see, for
example, [Salomon 06] p. 78, Equation 3.12).
L(u) = [u2, u, 1]
⎡
⎣
1
2
−1
1
2
−3
2
2
−1
2
1
0
0
⎤
⎦
⎡
⎣
P3
P2
P1
⎤
⎦
=
u2
2 −3u
2 + 1

P3 + (−u2 + 2u)P2 +
u2
2 −u
2

P1.
It is easy to verify that L(0) = P3, L(1) = P2, and L(2) = P1. Extrapolating to u = 3
yields L(3) = 3P1 −3P2 + P3. When this is translated to our samples, the result is

6.3 Linear Prediction
237
ˆs(t) = 3s(t −1) −3s(t −2) + s(t −3). The ﬁrst four predictors are summarized as
ˆs0(t) = 0,
ˆs1(t) = s(t −1),
ˆs2(t) = 2s(t −1) −s(t −2),
ˆs3(t) = 3s(t −1) −3s(t −2) + s(t −3).
(6.3)
These predictors can now be used to compute the error (or diﬀerence) values for
the ﬁrst four orders:
e0(t) = s(t) −ˆs0(t) = s(t),
e1(t) = s(t) −ˆs1(t) = s(t) −s(t −1) = e0(t) −e0(t −1),
e2(t) = s(t) −ˆs2(t) = s(t) −2s(t −1) + s(t −2)
= [s(t) −s(t −1)] −[s(t −1) −s(t −2)] = e1(t) −e1(t −1),
(6.4)
e3(t) = s(t) −ˆs3(t) = s(t) −3s(t −1) + 3s(t −2) −s(t −3)
= [s(t) −2s(t −1) + s(t −2)] −[s(t −1) −2s(t −2) + s(t −3)]
= e2(t) −e2(t −1).
This computation is recursive but it involves only three steps, it is arithmetically simple,
and does not require any multiplications (see also Equation (6.6)).
For even better compression, it is possible to compute all four predictors and their
errors and select the smallest error.
However, experience gained with practical im-
plementations of this technique indicates that even a zeroth-order predictor results in
typical compression of 48%, and going all the way to third-order prediction improves
this only to 58%. For most cases, there is therefore no need to use higher-order predic-
tors, and the precise predictor used should be determined by compression quality versus
run-time considerations. The Shorten method (Section 6.5) uses linear (second-order)
prediction as a default.
Extending these concepts to a fourth-order linear predictor is straightforward. We
start with the four points P4 = (t −4, s4), P3 = (t −3, s3), P2 = (t −2, s2), and
P1 = (t −1, s1) and construct a degree-3 polynomial that passes through those points
(the points are selected such that their x coordinates correspond to time and their
y coordinates are audio samples). A natural choice is the nonuniform cubic Lagrange
interpolation polynomial Q3nu(t) = 3
i=0 Pi+1L3
i (t) whose coeﬃcients are given by (see,
for example, [Salomon 99] p. 204, Equations 4.17 and 4.18)
L3
i (t) =
/3
j̸=i(t −tj)
/3
j̸=i(ti −tj)
,
for
0 ≤i ≤3.
The Mathematica code of Figure 6.6 performs the computations and produces
Q(t) = −1
6(t−1)(t−2)(t−3)P4+ 1
2t(t−2)(t−3)P3−1
2t(t−1)(t−3)P2+ 1
6t(t−1)(t−2)P1.

238
6.
Audio Compression
It is easy to verify that Q(0) = P4, Q(1) = P3, Q(2) = P2, and Q(3) = P1. Extrapolating
to t = 4 yields Q(4) = 4P1−6P2+4P3−P4, and when this is translated to audio samples
the result is
ˆs4(t) = 4s(t −1) −6s(t −2) + 4s(t −3) −s(t −4)
(6.5)
[compare with Equation (6.3)]. When this prediction is subtracted from the current
audio sample s(t), the residue is
e4(t) = s(t) −ˆs4(t) = s(t) −4s(t −1) + 6s(t −2) −4s(t −3) + s(t −4).
(6.6)
This is a simple arithmetic expression that involves only four additions and subtractions.
(* Uniform Cubic Lagrange polynomial for 4th-order prediction in FLAC *)
Clear[Q,t]; t0=0; t1=1; t2=2; t3=3;
Q[t_] := Plus @@ {
((t-t1)(t-t2)(t-t3))/((t0-t1)(t0-t2)(t0-t3))P4,
((t-t0)(t-t2)(t-t3))/((t1-t0)(t1-t2)(t1-t3))P3,
((t-t0)(t-t1)(t-t3))/((t2-t0)(t2-t1)(t2-t3))P2,
((t-t0)(t-t1)(t-t2))/((t3-t0)(t3-t1)(t3-t2))P1}
Figure 6.6: Code for a Lagrange Polynomial.
⋄Exercise 6.4: Check the performance of the fourth-order prediction developed here.
Select four correlated items and compare the prediction of Equation (6.5) to the actual
value of the next correlated item.
Given six matches, arrange them so as to end up with nothing.
6.4 µ-Law and A-Law Companding
These two international standards, formally known as recommendation G.711, are docu-
mented in [ITU-T 89]. They employ logarithm-based functions to encode audio samples
for ISDN (integrated services digital network) digital telephony services, by means of
nonlinear quantization (companding).
The ISDN hardware samples the voice signal
from the telephone 8,000 times per second, and generates 14-bit samples (13 for A-law).
The method of µ-law companding is used in North America and Japan, and A-law is
used elsewhere. The two methods are similar; they diﬀer mostly in their quantizations
(midtread vs. midriser).
Experiments (documented in [Shenoi 95]) indicate that low amplitudes of speech
signals contain more information than high amplitudes. This is why nonlinear quanti-
zation makes sense. Imagine an audio signal sent on a telephone line and digitized to
14-bit samples. The louder the conversation, the higher the amplitude, and the bigger
the value of the sample. Since high amplitudes are less important, they can be coarsely
quantized. If the largest sample, which is 214 −1 = 16,383, is quantized to 255 (the

6.4 µ-Law and A-Law Companding
239
largest 8-bit number), then the compression factor is 14/8 = 1.75. When decoded, a
code of 255 will become very diﬀerent from the original 16,383. We say that because
of the coarse quantization, large samples end up with high quantization noise. Smaller
samples should be ﬁnely quantized, so they end up with low quantization noise.
The µ-law encoder inputs 14-bit samples and outputs 8-bit codewords.
The A-
law inputs 13-bit samples and also outputs 8-bit codewords. The telephone signals are
sampled at 8 kHz (8,000 times per second), so the µ-law encoder receives 8,000×14 =
112,000 bits/sec. At a compression factor of 1.75, the encoder outputs 64,000 bits/sec.
The G.711 standard [G.711 72] also speciﬁes output rates of 48 Kbps and 56 Kbps.
The µ-law encoder receives a 14-bit signed input sample x. Thus, the input is in
the range [−8,192, +8,191]. The sample is normalized to the interval [−1, +1], and the
encoder uses the logarithmic expression
sgn(x)ln(1 + µ|x|)
ln(1 + µ) , where sgn(x) =
 +1,
x > 0,
0,
x = 0,
−1,
x < 0
(and µ is a positive integer), to compute and output an 8-bit code in the same interval
[−1, +1]. The output is then scaled to the range [−256, +255]. Figure 6.7 shows this
output as a function of the input for the three µ values 25, 255, and 2555. It is clear that
large values of µ cause coarser quantization for larger amplitudes. Such values allocate
more bits to the smaller, more important, amplitudes. The G.711 standard recommends
the use of µ = 255. The diagram shows only the nonnegative values of the input (i.e.,
from 0 to 8191). The negative side of the diagram has the same shape but with negative
inputs and outputs.
The A-law encoder uses the similar expression
⎧
⎪
⎨
⎪
⎩
sgn(x)
A|x|
1 + ln(A),
for 0 ≤|x| < 1
A,
sgn(x)1 + ln(A|x|)
1 + ln(A) ,
for 1
A ≤|x| < 1.
The G.711 standard recommends the use of A = 87.6.
The following simple examples illustrate the nonlinear nature of the µ-law. The two
(normalized) input samples 0.15 and 0.16 are transformed by µ-law to outputs 0.6618
and 0.6732. The diﬀerence between the outputs is 0.0114. On the other hand, the two
input samples 0.95 and 0.96 (bigger inputs but with the same diﬀerence) are transformed
to 0.9908 and 0.9927. The diﬀerence between these two outputs is 0.0019; much smaller.
Bigger samples are decoded with more noise, and smaller samples are decoded with
less noise. However, the signal-to-noise ratio (SNR) is constant because both it and the
µ-law are based on logarithmic expressions.
Logarithms are slow to compute, so the µ-law encoder performs much simpler cal-
culations that produce an approximation. The output speciﬁed by the G.711 standard
is an 8-bit codeword whose format is shown in Figure 6.8.
Bit P in Figure 6.8 is the sign bit of the output (same as the sign bit of the 14-bit
signed input sample). Bits S2, S1, and S0 are the segment code, and bits Q3 through
Q0 are the quantization code. The encoder determines the segment code by (1) adding

240
6.
Audio Compression
8000
6000
4000
2000
0
16
48
64
32
80
112
25
255
2555
128
96
Figure 6.7: The µ-Law for µ Values of 25, 255, and 2555.
dat=linspace(0,1,1000);
mu=255;
plot(dat*8159,128*log(1+mu*dat)/log(1+mu));
Matlab code for Figure 6.7. Notice how the input is normalized to the range
[0, 1] before the calculations, and how the output is scaled from the interval
[0, 1] to [0, 128].
P
S2
S1
S0
Q3
Q2
Q1
Q0
Figure 6.8: G.711 µ-Law Codeword.
Q3
Q2
Q1
Q0
0
0
0
1
0
1
0
1
1
0
0
0
1
12
11
10
9
8
7
6
5
4
3
2
1
0
Figure 6.9: Encoding Input Sample −656.

6.4 µ-Law and A-Law Companding
241
a bias of 33 to the absolute value of the input sample, (2) determining the bit position
of the most signiﬁcant 1-bit among bits 5 through 12 of the input, and (3) subtracting
5 from that position. The 4-bit quantization code is set to the four bits following the
bit position determined in step 2. The encoder ignores the remaining bits of the input
sample, and it inverts (1’s complements) the codeword before it is output.
We use the input sample −656 as an example. The sample is negative, so bit P
becomes 1. Adding 33 to the absolute value of the input yields 689 = 00010101100012
(Figure 6.9). The most signiﬁcant 1-bit in positions 5 through 12 is found at position
9. The segment code is thus 9 −5 = 4. The quantization code is the four bits 0101 at
positions 8–5, and the remaining ﬁve bits 10001 are ignored. The 8-bit codeword (which
is later inverted) becomes
P
S2
S1
S0
Q3
Q2
Q1
Q0
1
1
0
0
0
1
0
1
The µ-law decoder inputs an 8-bit codeword and inverts it. It then decodes it as
follows:
1. Multiply the quantization code by 2 and add 33 (the bias) to the result.
2. Multiply the result by 2 raised to the power of the segment code.
3. Decrement the result by the bias.
4. Use bit P to determine the sign of the result.
Applying these steps to our example produces
1. The quantization code is 1012 = 5, so 5 × 2 + 33 = 43.
2. The segment code is 1002 = 4, so 43 × 24 = 688.
3. Decrement by the bias 688 −33 = 655.
4. Bit P is 1, so the ﬁnal result is −655. Thus, the quantization error (the noise) is 1;
very small.
Figure 6.10a illustrates the nature of the µ-law midtread quantization. Zero is one
of the valid output values, and the quantization steps are centered at the input value of
0. The steps are organized in eight segments of 16 steps each. The steps within each
segment have the same width, but they double in width from one segment to the next.
If we denote the segment number by i (where i = 0, 1, . . . , 7) and the width of a segment
by k (where k = 1, 2, . . . , 16), then the middle of the tread of each step in Figure 6.10a
(i.e., the points labeled xj) is given by
x(16i + k) = T(i) + k×D(i),
(6.7)
where the constants T(i) and D(i) are the initial value and the step size for segment i,
respectively. They are given by
i
0
1
2
3
4
5
6
7
T(i)
1
35
103
239
511
1055
2143
4319
D(i)
2
4
8
16
32
64
128
256

242
6.
Audio Compression
(a)
x input
y output
x1
y1
y0
y2
y3
y4
x2
x3 x4
(b)
x input
y output
x1
y1
−y2
−y1
y2
y3
y4
x2
−x2
−x2
x3 x4
Figure 6.10: (a) µ-Law Midtread Quantization. (b) A-Law Midriser Quantization.
Segment 0
Segment 1
· · ·
Segment 7
Break
Output
Break
Output
Break
Output
points
values
points
values
points
values
y0 = 0
y16 = 33
· · ·
y112 = 4191
x1 = 1
x17 = 35
x113 = 4319
y1 = 2
y17 = 37
· · ·
y113 = 4447
x2 = 3
x18 = 39
x114 = 4575
y2 = 4
y18 = 41
· · ·
y114 = 4703
x3 = 5
x19 = 43
x115 = 4831
y3 = 6
y19 = 45
· · ·
y115 = 4959
x4 = 7
x20 = 47
x116 = 5087
· · ·
· · ·
· · ·
· · ·
x15 = 29
x31 = 91
x127 = 7903
y15 = 28
y31 = 93
· · ·
y127 = 8031
x16 = 31
x32 = 95
x128 = 8159
Table 6.11: Speciﬁcation of the µ-Law Quantizer.
Table 6.11 lists some values of the breakpoints (points xj) and outputs (points yj) shown
in Figure 6.10a.
The operation of the A-law encoder is similar, except that the quantization (Fig-
ure 6.10b) is of the midriser variety. The breakpoints xj are given by Equation (6.7),
but the initial value T(i) and the step size D(i) for segment i are diﬀerent from those
used by the µ-law encoder and are given by
i
0
1
2
3
4
5
6
7
T(i)
0
32
64
128
256
512
1024
2048
D(i)
2
2
4
8
16
32
64
128

6.4 µ-Law and A-Law Companding
243
Table 6.12 lists some values of the breakpoints (points xj) and outputs (points yj) shown
in Figure 6.10b.
Segment 0
Segment 1
· · ·
Segment 7
Break
Output
Break
Output
Break
Output
points
values
points
values
points
values
x0 = 0
x16 = 32
x112 = 2048
y1 = 1
y17 = 33
· · ·
y113 = 2112
x1 = 2
x17 = 34
x113 = 2176
y2 = 3
y18 = 35
· · ·
y114 = 2240
x2 = 4
x18 = 36
x114 = 2304
y3 = 5
y19 = 37
· · ·
y115 = 2368
x3 = 6
x19 = 38
x115 = 2432
y4 = 7
y20 = 39
· · ·
y116 = 2496
· · ·
· · ·
· · ·
· · ·
x15 = 30
x31 = 62
x128 = 4096
y16 = 31
y32 = 63
· · ·
y127 = 4032
Table 6.12: Speciﬁcation of the A-Law Quantizer.
The A-law encoder generates an 8-bit codeword with the same format as the µ-law
encoder. It sets the P bit to the sign of the input sample. It then determines the segment
code in the following steps:
1. Determine the bit position of the most signiﬁcant 1-bit among the seven most signif-
icant bits of the input.
2. If such a 1-bit is found, the segment code becomes that position minus 4. Otherwise,
the segment code becomes zero.
The 4-bit quantization code is set to the four bits following the bit position deter-
mined in step 1, or to half the input value if the segment code is zero. The encoder ignores
the remaining bits of the input sample, and it inverts bit P and the even-numbered bits
of the codeword before it is output.
The A-law decoder decodes an 8-bit codeword into a 13-bit audio sample as follows:
1. It inverts bit P and the even-numbered bits of the codeword.
2. If the segment code is nonzero, the decoder multiplies the quantization code by 2
and increments this by the bias (33). The result is then multiplied by 2 and raised to
the power of the (segment code minus 1). If the segment code is 0, the decoder outputs
twice the quantization code, plus 1.
3. Bit P is then used to determine the sign of the output.
Normally, the output codewords are generated by the encoder at the rate of 64 Kbps.
The G.711 standard [G.711 72] also provides for two other rates, as follows:
1. To achieve an output rate of 48 Kbps, the encoder masks out the two least-signiﬁcant
bits of each codeword. This works because 6/8 = 48/64.

244
6.
Audio Compression
2. To achieve an output rate of 56 Kpbs, the encoder masks out the least-signiﬁcant bit
of each codeword. This works because 7/8 = 56/64 = 0.875.
This applies to both the µ-law and the A-law. The decoder typically ﬁlls up the
masked bit positions with zeros before decoding a codeword.
6.5 Shorten
Shorten is a simple, special-purpose, lossless compressor for waveform ﬁles. Any ﬁle
whose data items (which are referred to as samples) go up and down as in a wave can
be eﬃciently compressed by this method.
Its developer [Robinson 94] had in mind
applications to speech compression (where audio ﬁles with speech are distributed on a
CD-ROM), but any other waveform ﬁles can be similarly compressed. The compression
performance of Shorten isn’t as good as that of mp3, but Shorten is lossless. Shorten
performs best on ﬁles with low-amplitude and low-frequency samples, where it yields
compression factors of 2 or better. It has been implemented on UNIX and on MS-DOS
and is freely available at [Softsound 07].
Shorten encodes the individual samples of the input ﬁle by partitioning the ﬁle into
blocks, predicting each sample from some of its predecessors, subtracting the prediction
from the sample, and encoding the diﬀerence with a special variable-length code. It also
has a lossy mode, where samples are quantized before being compressed. The algorithm
has been implemented by its developer and can input ﬁles in the audio formats ulaw,
s8, u8, s16 (this is the default input format), u16, s16x, u16x, s16hl, u16hl, s16lh, and
u16lh, where “s” and “u” stand for “signed” and “unsigned,” respectively, a trailing “x”
speciﬁes byte-mapped data, “hl” implies that the high-order byte of a sample is followed
in the ﬁle by the low-order byte, and “lh” signiﬁes the reverse.
An entire ﬁle is encoded in blocks, where the block size (typically 128 or 256 samples)
is speciﬁed by the user and has a default value of 256. (At a sampling rate of 16 kHz, this
is equivalent to 16 ms of sound.) The samples in the block are ﬁrst converted to integers
with an expected mean of 0. The idea is that the samples within each block have the
same spectral characteristic and can therefore be predicted accurately. Some audio ﬁles
consist of several interleaved channels, so Shorten starts by separating the channels in
each block. Thus, if the ﬁle has two channels and the samples are interleaved as L1, R1,
L2, R2, and so on up to Lb, Rb, the ﬁrst step creates the two blocks (L1, L2, . . . , Lb) and
(R1, R2, . . . , Rb) and each block is then compressed separately. In practice, blocks that
correspond to audio channels are often highly correlated, so sophisticated methods, such
as MLP [Salomon 07], try to remove interblock correlations before tackling the samples
within a block.
Once the channels have been separated, the audio samples in each channel are pre-
dicted and the resulting diﬀerences are losslessly encoded by Rice codes (Section 1.1.3).
These steps result in lossless coding of the audio samples. Sometimes, a certain loss in
the samples is acceptable if it results in signiﬁcantly better compression. Shorten oﬀers
two options of lossy compression by quantizing the original audio samples before they
are compressed. The quantization is done separately for each segment. One lossy option
encodes every segment at the same bitrate (the user speciﬁes the maximum bitrate),

Chapter Summary
245
and the other option maintains a user-speciﬁed signal-to-noise ratio in each segment
(the user speciﬁes the minimum acceptable signal-to-noise ratio in dB).
Tests of Shorten indicate that it generally performs better and faster than UNIX
compress and gzip, but that its lossy options are slow.
Chapter Summary
Audio data is one of the important members of the family of multimedia digital data.
It has become even more important and popular with the advent of popular mp3 play-
ers. Standards organizations as well as researchers have long felt the need for high-
performance audio compression algorithms that oﬀer fast, simple decompression. The
best-known example of such an algorithm, mp3, started its life in 1988, when a group of
audio experts ﬁrst met to discuss this topic, and became an approved international stan-
dard in 1992. Since then, many diﬀerent algorithms, both lossy and lossless, have been
proposed, developed, and implemented. Reference [Salomon 07] describes about a dozen
diﬀerent methods, based on diﬀerent principles, that have been speciﬁcally developed to
compress audio data.
This chapter starts with general discussion of companding (Section 6.1), the hu-
man auditory system (Section 6.2), and linear prediction (Section 6.3). It follows with
descriptions of two audio compression algorithms, µ-law and A-law companding (Sec-
tion 6.4) and Shorten (Section 6.5).
Companding (also termed nonlinear quantization) exploits the fact that the human
ear is sensitive to low sound amplitudes (quiet sound) and loses sensitivity as the sound
becomes louder. The principle of companding is to quantize each audio sample by a
diﬀerent amount according to its size (recall that the size of a sample is proportional to
the sound amplitude). Large samples, which correspond to high amplitudes, are quan-
tized more than small samples. This is the reason for the term nonlinear quantization.
Section 6.1 says more about companding. The µ-law and A-law compression methods
(Section 6.4) are examples of this approach.
The ear receives sound waves and converts them to signals that are sent to the
brain to create the sensation of sound. The ear–brain system is very sensitive and can
perceive very quiet sounds, but its sensitivity depends on the frequency of the sound.
Also, a loud sound drastically reduces the sensitivity of the ear over a short period.
These properties of the human auditory system can be exploited to achieve eﬀective
lossy audio compression. The principle is to develop a psychoacoustic model to identify
those parts of the input audio that will not be perceived by the ear and brain because of
the limitations of these organs. These parts can either be completely omitted or at least
coarsely quantized, thereby resulting in substantial savings and eﬀective compression.
This is how mp3 works.
The term “correlation” dominates this book. Images can be tightly compressed
because the pixels of an image tend to be correlated. Similarly, audio can be compressed
because its smallest units, the audio samples, are correlated. Thus, a simple approach to
audio compression may be to subtract each audio sample from its predecessor and encode
the resulting diﬀerences. The audio samples are unsigned integers, but the diﬀerences
(also termed errors or residuals) are signed. The point is that the residuals tend to be

246
6.
Audio Compression
small numbers and therefore can be encoded very eﬃciently. This simple approach can
be improved when we subtract from the current sample s(t) not just its predecessor
s(t −1) (which is usually similar to s(t) but may sometimes be diﬀerent) but a linear
combination of several past samples s(t −1), s(t −2), down to s(t −p) for some integer
p. This process of predicting the current sample from p of its predecessors, smoothes
out any unexpected diﬀerences between consecutive audio samples and results in smaller
residuals and thus better compression.
Self-Assessment Questions
1. The term “audio sample” is central to digital audio and its processing, compres-
sion, and playback. The audio samples are generated when a sound wave is sampled
many times per second. The question is, can audio samples be created solely by software,
or is there a need for a special piece of hardware to do the sampling?
2. The term “pulse modulation” refers to techniques for converting a continuous
wave to a stream of binary numbers (audio samples). The term pulse code modulation
(PCM) has been mentioned in this chapter. Search the professional literature and the
Internet for other modulation techniques, such as pulse amplitude modulation (PAM),
pulse position modulation (PPM), pulse width modulation (PWM), and pulse number
modulation (PNM).
3. Discuss the possible applications of RLE and dictionary-based methods for audio
compression. Does it make sense to adapt such approaches to the compression of audio
samples, and if so, can they be as eﬃcient as the methods discussed here?
4. Audio is a physical entity that varies in a very wide range. The interval from
the quietest sound (the threshold of hearing) to the loudest sound that we can hear
is spread over 12 orders of magnitude. As a result, sound intensity is measured by a
quantity called the decibel (dB) that is based on a logarithmic scale. Anyone interested
in sound, its measurement, and its applications should be familiar with decibels and
their properties.
The sweetest sound of all is praise.
—Xenophon

7
Other Methods
The discipline of data compression is vast. It is based on many approaches and tech-
niques and it borrows many tools, ideas, and concepts from diverse scientiﬁc, engineering,
and mathematical ﬁelds. The following are just a few examples:
Fourier transform, ﬁnite automata, Markov processes, the human visual and audi-
tory systems, statistical terms, distributions, and concepts, Unicode, XML, convolution,
space-ﬁlling curves, Voronoi diagrams, interpolating polynomials, Fibonacci numbers,
polygonal surfaces, data structures, error-correcting codes, fractals, ﬁngerprint identiﬁ-
cation, and analog and digital video. The Intermezzo on page 253 discusses one of these
topics.
This book is relatively short and concentrates on the most important approaches
and ideas used in data compression. It discusses sources of data redundancies, correlation
of data items, variable-length codes, the use of dictionaries, and the power of transforms.
About a dozen speciﬁc compression methods are described in order to illustrate these
concepts and approaches.
This chapter is an attempt to extend the scope of the book somewhat. It tries
to convince the reader that useful, practical compression algorithms can be based on
other techniques, and it illustrates this point by discussing the following: the Burrows–
Wheeler method (Section 7.1), symbol ranking (Section 7.2), and SCSU and BOCU-1,
two algorithms (Sections 7.3 and 7.3.1) for the compression of Unicode-based documents.

248
7.
Other Methods
7.1 The Burrows–Wheeler Method
Most compression methods operate in the streaming mode, where the codec inputs a
byte or several bytes, processes them, and continues until an end-of-ﬁle is sensed. The
Burrows–Wheeler (BW) method, described in this section [Burrows and Wheeler 94],
works in a block mode, where the input is read block by block and each block is encoded
separately as one string. The method is therefore referred to as block sorting. The BW
method is general purpose, it works well on images, sound, and text, and can achieve
very high compression ratios (1 bit per byte or even better).
The main idea of the BW method is to start with a string S of n symbols and to
scramble them into another string L that satisﬁes two conditions:
1. Any region of L will tend to have a concentration of just a few symbols. Another way
of saying this is, if a symbol s is found at a certain position in L, then other occurrences
of s are likely to be found nearby. This property means that L can easily and eﬃciently
be compressed with the move-to-front method [Salomon 07], perhaps in combination
with RLE. This also means that the BW method will work well only if n is large (at
least several thousand symbols per string).
2. It is possible to reconstruct the original string S from L (a little more data may be
needed for the reconstruction, in addition to L, but not much).
The mathematical term for scrambling symbols is permutation, and it is easy to
show that a string of n symbols has n! (pronounced “n factorial”) permutations. This is
a large number even for relatively small values of n, so the particular permutation used
by BW has to be carefully selected. The BW codec proceeds in the following steps:
1. String L is created, by the encoder, as a permutation of S. Some more information,
denoted by I, is also created, to be used later by the decoder in step 3.
2. The encoder compresses L and I and writes the results on the output. This step typi-
cally starts with RLE, continues with move-to-front coding, and ﬁnally applies Huﬀman
coding.
3. The decoder reads the output and decodes it by applying the same methods as in
step 2 but in reverse order. The result is string L and variable I.
4. Both L and I are used by the decoder to reconstruct the original string S.
I do hate sums. There is no greater mistake than to call arithmetic an exact science.
There are permutations and aberrations discernible to minds entirely noble like
mine; subtle variations which ordinary accountants fail to discover; hidden laws of
number which it requires a mind like mine to perceive. For instance, if you add a
sum from the bottom up, and then from the top down, the result is always diﬀerent.
—Mrs. La Touche, Mathematical Gazette, v. 12 (1924)
The ﬁrst step is to understand how string L is created from S, and what information
needs to be stored in I for later reconstruction. We use the familiar string swiss␣miss
to illustrate this process.

7.1 The Burrows–Wheeler Method
249
swiss␣miss
wiss␣misss
iss␣misssw
ss␣missswi
s␣missswis
␣missswiss
missswiss␣
issswiss␣m
ssswiss␣mi
sswiss␣mis
␣missswiss
iss␣misssw
issswiss␣m
missswiss␣
s␣missswis
ss␣missswi
ssswiss␣mi
sswiss␣mis
swiss␣miss
wiss␣misss
s
w
i
m
␣ 
i
s
s
s
s
s
w
i
m
␣ 
i
s
s
s
s
0
1
2
3
4
5
6
7
8
9
0:
1:
2:
3:
4:
5:
6:
7:
8:
9:
F
L
T
(a)
(b)
(c)
Figure 7.1: Principles of BW Compression.
Given an input string of n symbols, the encoder constructs an n × n matrix where
it stores string S in the top row, followed by n −1 copies of S, each cyclically shifted
(rotated) one symbol to the left (Figure 7.1a). The matrix is then sorted lexicograph-
ically by rows, producing the sorted matrix of Figure 7.1b. Notice that every row and
every column of each of the two matrices is a permutation of S and thus contains all
n symbols of S. The permutation L selected by the encoder is the last column of the
sorted matrix. In our example this is the string swm␣siisss. The only other information
needed to eventually reconstruct S from L is the row number of the original string in
the sorted matrix, which in our example is 8 (row and column numbering starts from
0). This number is stored in I.
It is easy to see why L contains concentrations of identical symbols. Assume that
the words bail, fail, hail, jail, mail, nail, pail, rail, sail, tail, and wail
appear somewhere in S. After sorting, all the permutations that start with il will
appear together. All of them contribute an a to L, so L will have a concentration of
a’s. Also, all the permutations starting with ail will end up together, contributing to a
concentration of the letters bfhjmnprstw in one region of L.
We can now characterize the BW method by saying that it uses sorting to group
together symbols based on their contexts. However, the method considers context on
only one side of each symbol.
⋄Exercise 7.1: The last column, L, of the sorted matrix contains concentrations of
identical characters, which is why L is easy to compress. However, the ﬁrst column, F, of
the same matrix is even easier to compress, since it contains runs, not just concentrations,
of identical characters. Why select column L and not column F?
Notice also that the encoder does not actually have to construct the two n × n
matrices (or even one of them) in memory. The practical details of the encoder are
discussed in Section 7.1.2, as well as the compression of L and I, but let’s ﬁrst see how
the decoder works.

250
7.
Other Methods
The decoder reads a compressed ﬁle, decompresses it using Huﬀman, move-to-front,
and perhaps also RLE (see [Salomon 07] for the move-to-front method), and then re-
constructs string S from the decompressed L in three steps:
1. The ﬁrst column of the sorted matrix (column F in Figure 7.1c) is constructed from
L. This is a straightforward process, since F and L contain the same symbols (both are
permutations of S) and F is sorted. The decoder simply sorts string L to obtain F.
2. While sorting L, the decoder prepares an auxiliary array T that shows the relations
between elements of L and F (Figure 7.1c). The ﬁrst element of T is 4, implying that
the ﬁrst symbol of L (the letter “s”) is located in position 4 of F. The second element
of T is 9, implying that the second symbol of L (the letter “w”) is located in position 9
of F, and so on. The contents of T in our example are (4, 9, 3, 0, 5, 1, 2, 6, 7, 8).
3. String F is no longer needed. The decoder uses L, I, and T to reconstruct S according
to
S[n −1 −i] ←L[Ti[I]],
for i = 0, 1, . . . , n −1,
where T0[j] = j, and Ti+1[j] = T[Ti[j]].
(7.1)
Here are the ﬁrst two steps in this reconstruction:
S[10-1-0]=L[T0[I]]=L[T0[8]]=L[8]=s,
S[10-1-1]=L[T1[I]]=L[T[T0[I]]]=L[T[8]]=L[7]=s.
⋄Exercise 7.2: Complete this reconstruction.
Before getting to the details of the compression, it may be interesting to understand
why Equation (7.1) reconstructs S from L. The following arguments explain why this
process works:
1. T is constructed such that F[T[i]] = L[i] for i = 0, . . . , n.
2. A look at the sorted matrix of Figure 7.1b shows that in each row i, symbol L[i]
precedes symbol F[i] in the original string S (the word precedes has to be understood
as precedes cyclically). Speciﬁcally, in row I (8 in our example), L[I] cyclically precedes
F[I], but F[I] is the ﬁrst symbol of S, so L[I] is the last symbol of S. The reconstruction
starts with L[I] and reconstructs S from right to left.
3. L[i] precedes F[i] in S for i = 0, . . . , n −1. Therefore L[T[i]] precedes F[T[i]], but
F[T[i]] = L[i]. The conclusion is that L[T[i]] precedes L[i] in S.
4. The reconstruction therefore starts with L[I] = L[8] = s (the last symbol of S) and
proceeds with L[T[I]] = L[T[8]] = L[7] = s (the next-to-last symbol of S). This is why
Equation (7.1) correctly describes the reconstruction.
7.1.1 Compressing L
Compressing L is based on its main attribute, namely, it contains concentrations (al-
though not necessarily runs) of identical symbols. Using RLE makes sense, but only as
a ﬁrst step in a multistep compression process. The main step in compressing L should
use the move-to-front method [Salomon 07]. This method is applied to our example
L=swm␣siisss as follows:
1. Initialize A to a list containing our alphabet A=(␣, i, m, s, w).
2. For i := 0, . . . , n −1, encode symbol Li as the number of symbols preceding it in A,
and then move symbol Li to the beginning of A.

7.1 The Burrows–Wheeler Method
251
3.
Combine the codes of step 2 in a list C, which will be further compressed with
Huﬀman or arithmetic coding.
The results are summarized in Figure 7.2a. The ﬁnal list of codes is the 10-element
array C = (3, 4, 4, 3, 3, 4, 0, 1, 0, 0), illustrating how any concentration of identical sym-
bols produces small codes. The ﬁrst occurrence of i is assigned code 4 but the second
occurrence is assigned code 0. The ﬁrst two occurrences of s get code 3, but the next
one gets code 1.
L
A
Code
C
A
L
s
␣imsw
3
3
␣imsw
s
w
s imw
4
4
s imw
w
m
ws im
4
4
ws im
m
␣
mws i
3
3
mws i
␣
s
␣mwsi
3
3
␣mwsi
s
i
s mwi
4
4
s mwi
i
i
is mw
0
0
is mw
i
s
is mw
1
1
is mw
s
s
si mw
0
0
si mw
s
s
si mw
0
0
si mw
s
(a)
(b)
Figure 7.2: Encoding/Decoding L by Move-to-Front.
It is interesting to compare the codes in C, which are integers in the range [0, n−1],
with the codes obtained without the extra step of “moving to front.” It is easy to encode
L using the three steps above but without moving symbol Li to the beginning of A. The
result is C′ = (3, 4, 2, 0, 3, 1, 1, 3, 3, 3), a list of integers in the same range [0, n −1]. This
is why applying move-to-front is not enough. Lists C and C′ contain elements in the
same range, but the elements of C are smaller on average. They should therefore be
further encoded using Huﬀman coding or some other statistical method. Huﬀman codes
for C can be assigned assuming that code 0 has the highest probability and code n −1,
the smallest probability.
In our example, a possible set of Huﬀman codes is 0—0, 1—10, 2—110, 3—1110, 4—
1111. Applying this set to C yields “1110|1111|1111|1110|1110|1111|0|10|0|0”; 29 bits.
(Applying it to C′ yields “1110|1111|110|0|1110|10|10|1110|1110|1110”; 32 bits.) Our
original 10-character string swiss miss has thus been coded using 2.9 bits/character,
a very good result.
It should be noted that the Burrows–Wheeler method can eas-
ily achieve better compression than that when applied to longer strings (thousands of
symbols).
⋄Exercise 7.3: Given the string S=sssssssssh, calculate string L and its move-to-front
compression.
Decoding C is done with the inverse of move-to-front.
We assume that the al-
phabet list A is available to the decoder (it is either the list of all possible bytes or it
is written by the encoder on the output). Figure 7.2b shows the details of decoding

252
7.
Other Methods
C = (3, 4, 4, 3, 3, 4, 0, 1, 0, 0). The ﬁrst code is 3, so the ﬁrst symbol in the newly con-
structed L is the fourth one in A, or “s”. This symbol is then moved to the front of A,
and the process continues.
7.1.2 Implementation Hints
Since the Burrows–Wheeler method is eﬃcient only for long strings (at least thousands of
symbols), any practical implementation should allow for large values of n. The maximum
value of n should be so large that two n×n matrices would not ﬁt in the available memory
(at least not comfortably), and all the encoder operations (preparing the permutations
and sorting them) should be done with one-dimensional arrays of size n. In principle,
it is enough to have just the original string S and the auxiliary array T in memory.
[Manber and Myers 93] and [McCreight 76] discuss the data structures used in this
implementation.
String S contains the original data, but surprisingly, it also contains all the necessary
permutations. Since the only permutations we need to generate are rotations, we can
generate permutation i of matrix 7.1a by scanning S from position i to the end, then
continuing cyclically from the start of S to position i −1. Permutation 5, for example,
can be generated by scanning substring (5, 9) of S (␣miss), followed by substring (0, 4)
of S (swiss). The result is ␣missswiss. The ﬁrst step in a practical implementation
would thus be to write a procedure that takes a parameter i and scans the corresponding
permutation.
Any method used to sort the permutations has to compare them. Comparing two
permutations can be done by scanning them in S, without having to move symbols or
create new arrays.
Once the sorting algorithm determines that permutation i should be in position j
in the sorted matrix (Figure 7.1b), it sets T[i] to j. In our example, the sort ends up
with T = (5, 2, 7, 6, 4, 3, 8, 9, 0, 1).
⋄Exercise 7.4: Show how how T is used to create the encoder’s main output, L and I.
Implementing the decoder is straightforward, because there is no need to create
n×n matrices. The decoder inputs bits that are Huﬀman codes. It uses them to create
the codes of C, decompressing each as it is created, with inverse move-to-front, into the
next symbol of L. When L is ready, the decoder sorts it into F, generating array T in
the process. Following that, it reconstructs S from L, T, and I. Thus, the decoder needs
at most three structures at any time, the two strings L and F (having typically one byte
per symbol), and the array T (with at least two bytes per pointer, to allow for large
values of n).
We describe a block-sorting, lossless data compression algorithm, and our
implementation of that algorithm. We compare the performance of our
implementation with widely available data compressors running on the same
hardware.
M. Burrows and D. J. Wheeler, May 10, 1994

7.1 The Burrows–Wheeler Method
253
What is the next integer in the sequence 5, 25, 61, 113, 181?
Fibonacci Codes. This short intermezzo explain how the well-known Fibonacci
numbers can be harnessed and employed to compress data. First,
a few words about this important sequence. The Fibonacci num-
bers, as their name implies, are named after Leonardo Fibonacci
and have many interesting and beautiful properties and many
useful applications. The ﬁrst two Fibonacci numbers are 1, and
any other Fibonacci number is simply the sum of its two immedi-
ate predecessors. Thus, the sequence starts with 1, 1, 3, 5, 8, 13,
21, . . . and can also can serve as a number system; any integer can
be expressed as a weighted sum of the Fibonacci numbers. Thus,
15 = 12+3 can be written as 1×12+0×8+0×5+1×3+0×1
or 10010.
A little thinking should convince the reader that the Fibonacci representation of an
integer cannot have two consecutive 1’s. The number 110, for example, equals 1 × 5 +
1 × 3 + 0 × 1 = 8, so it should be written 1000. This property was exploited in [Fraenkel
and Klein 96] to construct a variable-length Fibonacci code (in fact, several codes) for
the integers, a code that has a short average length and that is also easy to encode and
decode. Given an integer n, the idea is to construct its Fibonacci representation, reverse
the resulting string of bits, and append a 1 to it. Thus, the codeword of the integer 8
starts as 1000, reversed to 0001, to become 0001|1. Table 7.3 lists the ﬁrst 12 Fibonacci
codewords and it is clear that this code can be uniquely decoded because the only pair
of successive 1’s occurs at the end of a codeword.
1
11
7
01011
2
011
8
000011
3
0011
9
100011
4
1011
10
010011
5
00011
11
001011
6
10011
12
101011
Table 7.3: Twelve Fibonacci Codes.
To decode a given Fibonacci codeword, skip bits of the codeword until a pair 11 is
reached. Replace this 11 by 1. Multiply the skipped bits by the values . . . , 13, 8, 5,
3, 2, 1 (the Fibonacci numbers), and add the products. Obviously, it is not necessary
to actually multiply. Simply use the 1 bits to select the proper Fibonacci numbers and
add.

254
7.
Other Methods
7.2 Symbol Ranking
Like so many other ideas in the realm of information and data, the idea of text compres-
sion by symbol ranking is due to Claude Shannon, the creator of information theory. In
his classic paper on the information content of English text [Shannon 51] he describes
a method for experimentally determining the entropy of such texts. In a typical exper-
iment, a passage of text has to be predicted, character by character, by a person (the
examinee). In one version of the method the examinee predicts the next character and
is then told by the examiner whether the prediction was correct or, if it was not, what
the next character is. In another version, the examinee has to continue predicting until
he obtains the right answer. The examiner then uses the number of wrong answers to
estimate the entropy of the text.
As it turned out, in the latter version of the test, the human examinees were able to
predict the next character in one guess about 79% of the time and rarely needed more
than 3–4 guesses. Table 7.4 shows the distribution of guesses as published by Shannon.
# of guesses
1
2
3
4
5
> 5
Probability
79%
8%
3%
2%
2%
5%
Table 7.4: Probabilities of Guesses of English Text.
The fact that this probability is so skewed implies low entropy (Shannon’s conclusion
was that the entropy of English text is in the range of 0.6–1.3 bits per letter), which in
turn implies the possibility of very good compression.
The symbol ranking method of this section [Fenwick 96b] is based on the latter
version of the Shannon test. The method uses the context C of the current symbol
S (the N symbols preceding S) to prepare a list of symbols that are likely to follow
C. The list is arranged from most likely to least likely. The position of S in this list
(position numbering starts from 0) is then written by the encoder on the output after
being suitably encoded. If the program performs as well as a human examinee, we can
expect 79% of the symbols being encoded to result in 0 (ﬁrst position in the ranking
list), creating runs of zeros, which can easily be compressed by RLE.
The various context-based methods (most notably PPM) described in the data com-
pression literature use context to estimate symbol probabilities. They have to generate
and output escape symbols when switching contexts. In contrast, symbol ranking does
not estimate probabilities and does not use escape symbols. The absence of escapes
seems to be the main feature contributing to the excellent performance of the method.
Following is an outline of the main steps of the encoding algorithm.
Step 0: The ranking index (an integer counting the position of S in the ranked list) is
set to 0.
Step 1: An LZ77-type dictionary is used, with a search buﬀer containing text that
has already been input and encoded, and with a look-ahead buﬀer containing new,
unprocessed text. The most-recent text in the search buﬀer becomes the current context
C. The leftmost symbol, R, in the look-ahead buﬀer (immediately to the right of C) is
the current symbol. The search buﬀer is scanned from right to left (from recent to older
text) for strings matching C. The longest match is selected (if there are several longest

7.2 Symbol Ranking
255
matches, the most recent one is selected). The match length, N, becomes the current
order. The symbol P following the matched string (i.e., immediately to the right of it)
is examined. This is the symbol ranked ﬁrst by the algorithm. If P is identical to R, the
search is over and the algorithm outputs the ranking index (which is currently 0).
Step 2: If P is diﬀerent from R, the ranking index is incremented by 1, P is declared
excluded, and the other order-N matches, if any, are examined in the same way. Assume
that Q is the symbol following such a match. If Q is in the list of excluded symbols,
then it is pointless to examine it, and the search continues with the next match. If Q
has not been excluded, it is compared with R. If they are identical, the search is over,
and the encoding algorithm outputs the ranking index. Otherwise the ranking index is
incremented by 1, and Q is excluded.
Step 3: If none of the order-N matches is followed by a symbol identical to R, the order
of the match is decremented by 1, and the search buﬀer is again scanned from right to
left (from more recent text to older text) for strings of size N −1 that match C. For
each failure in this scan, the ranking index is incremented by 1, and Q is excluded.
Step 4: When the match order gets all the way down to 0, symbol R is compared with
symbols in a list containing the entire alphabet, again using exclusions and incrementing
the ranking index. If the algorithm gets to this step, it will ﬁnd R in this list, and will
output the current value of the ranking index (which will then normally be a large
number).
Some implementation details are discussed here.
1. Implementing exclusion. When a string S that matches C is found, the symbol P
immediately to the right of S is compared with R. If P and R are diﬀerent, P should be
declared excluded. This means that any future occurrences of P should be ignored. The
ﬁrst implementation of exclusion that comes to mind is a list to which excluded symbols
are appended. Searching such a list, however, is time consuming, and it is possible to
do much better.
The method described here uses an array excl indexed by the alphabet symbols. If
the alphabet consists, for example, of just the 26 letters, the array will have 26 locations
indexed a through z. Figure 7.5 shows a simple implementation that requires just one
step to determine whether a given symbol is excluded. Assume that the current context
C is the string “. . . abc”. We know that the c will remain in the context even if the
algorithm has to go down all the way to order-1. The algorithm therefore prepares a
pointer to c (to be called the context index). Assume that the scan ﬁnds another string
abc, followed by a y, and compares it to the current context. They match, but they
are followed by diﬀerent symbols. The decision is to exclude y, and this is done by
setting array element excl[y] to the context index (i.e., to point to c). As long as the
algorithm scans for matches to the same context C, the context index will stay the same.
If another matching string abc is later found, also followed by y, the algorithm compares
excl[y] to the context index, ﬁnds that they are equal, so it knows that y has already
been excluded. When switching to the next current context there is no need to initialize
or modify the pointers in array excl.
2. Recall that N (the order) is initially unknown. The algorithm has to scan the search
buﬀer and ﬁnd the longest match to the current context. Once this is done, the length
N of the match becomes the current order. The process therefore starts by hashing the
two rightmost symbols of the current context C and using them to locate a possible

256
7.
Other Methods
...abcy...
Search Buffer
Look-Ahead Buffer
...abcx...
...abcy...
abcd..........xyz
excl
Context index
Symbol R
Context C
Figure 7.5: Exclusion Mechanism.
match.
Figure 7.6 shows the current context “. . . amcde”. We assume that it has already
been matched to some string of length 3 (i.e., a string “...cde”), and we try to match
it to a longer string. The two symbols “de” are hashed and produce a pointer to string
“lmcde”. The problem is to compare the current context to “lmcde” and ﬁnd whether
and by how much they match. This is done by the following three rules.
...lmcdeygi..
Search Buffer
Look-Ahead Buffer
Index Table
..amcdeigh...
Hash Function
H
..ucde..
Figure 7.6: String Search and Comparison Method.
1. Compare the symbols preceding (i.e., to the left of) cde in the two strings. In
our example they are both m, so the match is now of size 4. Repeat this rule until it
fails. It determines the order N of the match. Once the order is known, the algorithm
may have to decrement it later and compare shorter strings. In such a case, this rule
has to be modiﬁed. Instead of comparing the symbols preceding the strings, it should
compare the leftmost symbols of the two strings.
2. (We are still not sure whether the two strings are identical.) Compare the middle
symbols of the two strings. In our case, since the strings have a length of 4, this would
be either the c or the d. If the comparison fails, the strings are diﬀerent. Otherwise,
Rule 3 is used.
3. Compare the strings symbol by symbol to ﬁnally determine whether they are
identical.
It seems unnecessarily cumbersome to go through three rules when only the third
one is really necessary. However, the ﬁrst two rules are simple, and they identify 90%

7.2 Symbol Ranking
257
of the cases where the two strings are diﬀerent. Rule 3, which is slow, has to be applied
only if the ﬁrst two rules have not identiﬁed the strings as diﬀerent.
3. If the encoding algorithm has to decrement the order all the way down to 1, it faces
a special problem. It can no longer hash two symbols. Searching for order-1 matching
strings (i.e., single symbols) therefore requires a diﬀerent method which is illustrated by
Figure 7.7. Two linked lists are shown, one linking occurrences of s and the other linking
occurrences of i. Notice how only certain occurrences of s are linked, while others are
skipped. The rule is to skip an occurrence of s which is followed by a symbol that has
already been seen. Thus, the ﬁrst occurrences of si, ss, s␣, and sw are linked, whereas
other occurrences of s are skipped.
The list linking these occurrences of s starts empty and is built gradually, as more
text is input and is moved into the search buﬀer. When a new context is created with s
as its rightmost symbol, the list is updated. This is done by ﬁnding the symbol to the
right of the new s, say a, scanning the list for a link sa, deleting it if found (not more
than one may exist), and linking the current s to the list.
s w i s s  m i s s  i s  m i s s i n g
Figure 7.7: Context Searching for Order-1.
This list makes it easy to search and ﬁnd all occurrences of the order-1 context s
that are followed by diﬀerent symbols (i.e., with exclusions).
Such a list should be constructed and updated for each symbol in the alphabet. If
the algorithm is implemented to handle 8-bit symbols, then 256 such lists are needed
and have to be updated.
The implementation details above show how complex this method is. It is slow, but
it produces excellent compression.
Doublets (also known as word ladders or word chains) is a popular word game
invented by Lewis Carroll. A source word is transformed into a target word of the same
length in steps, by changing a single letter in each step, so that each link in the chain
is a valid word. For example, the ladder LEAD, LOAD, GOAD, GOLD can make you
rich if performed literally. Your task is to transform FLOUR into BREAD.

258
7.
Other Methods
7.3 SCSU: Unicode Compression
The ASCII code is old, having been designed in the early 1960s. With the advent of
inexpensive laser and inkjet printers and high-resolution displays, it has become possible
to display and print characters of any size and shape. As a result, the ASCII code, with
its 128 characters, no longer satisﬁes the needs of modern computing. Starting in 1991,
the Unicode consortium (whose members include major computer corporations, software
producers, database vendors, research institutions, international agencies, various user
groups, and interested individuals) has proposed and designed a new character coding
scheme that satisﬁes the demands of current hardware and software. Information about
Unicode is available at [Unicode 07].
The Unicode Standard assigns a number, called a code point, to each character
(code element). A code point is listed in hexadecimal with a “U+” preceding it. Thus,
the code point U+0041 is the number 004116 = 6510. It represents the character “A” in
the Unicode Standard.
The Unicode Standard also assigns a unique name to each character. Code ele-
ment U+0041, for example, is assigned the name “LATIN CAPITAL LETTER A” and
U+0A1B is assigned the name “GURMUKHI LETTER CHA.”
An important feature of the Unicode Standard is the way it groups related codes.
A group of related characters is referred to as a script, and such characters are assigned
consecutive codes; they become a contiguous area or a region of Unicode. If the char-
acters are ordered in the original script (such as A–Z and α through ω), then their
Unicodes reﬂect that order. Region sizes vary greatly, depending on the script.
Most Unicode code points are 16-bit (2-byte) numbers. There are 64K (or 65,536)
such codes, but Unicode reserves 2,048 of the 16-bit codes to extend this set to 32-bit
codes (thereby adding about 1.4 million surrogate code pairs). Most of the characters
in common use ﬁt into the ﬁrst 64K code points, a region of the codespace that’s called
the basic multilingual plane (BMP). There are about 6,700 unassigned code points for
future expansion in the BMP, plus over 870,000 unused supplementary code points in
the other regions of the codespace. More characters are under consideration for inclusion
in future versions of the standard.
Unicode starts with the set of 128 ASCII codes U+0000 through U+007F and con-
tinues with Greek, Cyrillic, Hebrew, Arabic, Indic, and other scripts. These are followed
by symbols and punctuation marks, diacritics, mathematical symbols, technical sym-
bols, arrows, dingbats, and so forth. The codespace continues with Hiragana, Katakana,
and Bopomofo. The uniﬁed Han ideographs are followed by the complete set of modern
Hangul.
Toward the end of the BMP is a range of code points reserved for private
use, followed by a range of compatibility characters. The compatibility characters are
character variants that are encoded only to enable transcoding to earlier standards and
old implementations that happen to use them.
The Unicode Standard also reserves code points for private use. Anyone can assign
these codes privately for their own characters and symbols or use them with special-
ized fonts. There are 6,400 private-use code points on the BMP and another 131,068
supplementary private-use code points elsewhere in the codespace.
Version 3.2 of Unicode speciﬁes codes for 95,221 characters from the world’s alpha-
bets, ideograph sets, and symbol collections. The current (late 2007) version is 5.0.0.

7.3 SCSU: Unicode Compression
259
The method described in this section is due to [Wolf et al. 00]. It is a standard
compression scheme for Unicode, abbreviated SCSU. It compresses strings of code points.
Like any compression method, it works by removing redundancy from the original data.
The redundancy in Unicode stems from the fact that typical text in Unicode tends to
have characters located in the same region in the Unicode codespace. Thus, a text using
the basic Latin character set consists mostly of code points of the form U+00xx. These
can be compressed to one byte each. A text in Arabic tends to use just Arabic characters,
which start at U+0600. Such text can be compressed by specifying a start address and
then converting each code point to its distance (or oﬀset) from that address.
This
introduces the concept of a window. The distance should be just one byte because a 2-
byte distance replacing a 2-byte code results in no compression. This kind of compression
is called the single-byte mode. A 1-byte oﬀset suggests a window size of 256, but we’ll see
why the method uses windows of half that size. In practice, there may be complications,
as the following three examples demonstrate:
1. A string of text in a certain script may have punctuation marks embedded in
it, and these have code points in a diﬀerent region. A single punctuation mark inside a
string can be written in raw form on the compressed ﬁle and also requires a special tag
to indicate a raw code. The result is a slight expansion.
2. The script may include hundreds or even thousands of characters. At a certain
point, the next character to be compressed may be too far from the start address, so a
new start address (a new window) has to be speciﬁed just for the next character. This
is done by a nonlocking-shift tag.
3. Similarly, at a certain point, the characters being compressed may all be in a
diﬀerent window, so a locking-shift tag has to be inserted, to indicate the start address
of the new window.
As a result, the method employs tags, implying that a tag has to be at least a few
bits, which raises the question of how the decoder distinguishes tags from compressed
characters. The solution is to limit the window size to 128. There are 8 static and
8 dynamic windows (Tables 7.8 and 7.9, respectively, where CJK stands for Chinese,
Japanese, and Korean). The start positions of the latter can be changed by tags.
SCSU employs the following conventions:
1. Each tag is a byte in the interval [0x00,0x1F], except that the ASCII codes for CR
(0x0D), LF (0x0A), and TAB (or HT 0x09) are not used for tags. There can therefore
be 32 −3 = 29 tags (but we’ll see that more values are reserved for tags in the Unicode
mode). The tags are used to indicate a switch to another window, a repositioning of a
window, or an escape to an uncompressed (raw) mode called the Unicode mode.
2. A code in the range U+0020 through U+007F is compressed by eliminating its
most-signiﬁcant eight zeros. It becomes a single byte.
3. Any other codes are compressed to a byte in the range 0x80 through 0xFF. These
indicate oﬀsets in the range 0x00 through 0xEF (0 through 127) in the current window.
Example: The string 041C, 043E, 0441, 002D, 043A, 0562, and 000D is compressed
to the bytes 12, 9C, BE, C1, 2D, BA, 1A, 02, E2, and 0D. The tag byte 12 indicates
the window from 0x0400 to 0x047F. Code 041C is at oﬀset 1C from the start of that
window, so it is compressed to the byte 1C + 80 = 9C. Code 043E is at oﬀset 3E, so it
is compressed to 3E + 80 = BE. Code 0441 is similarly compressed to C1. Code 002D

260
7.
Other Methods
n
Start
Major area
0
0000
Quoting tags in single-byte mode
1
0080
Latin1 supplement
2
0100
Latin Extended-A
3
0300
Combining diacritical marks
4
2000
General punctuation marks
5
2080
Currency symbols
6
2100
Letterlike symbols and number forms
7
3000
CJK symbols and punctuation
Table 7.8: Static Windows.
n
Start
Major area
0
0080
Latin1 supplement
1
00C0
Latin1 supp. + Latin Extended-A
2
0400
Cyrillic
3
0600
Arabic
4
0900
Devanagari
5
3040
Hiragana
6
30A0
Katakana
7
FF00
Fullwidth ASCII
Table 7.9: Dynamic Windows (Default Positions).
(the ASCII code of a hyphen) is compressed (without any tags) to its least-signiﬁcant 8
bits 2D. (This is less than 0x80, so the decoder does not get confused.) Code 043A is
compressed in the current window to BA, but compressing code 0562 must be done in
window [0x0500,0x057F] and must therefore be preceded by tag 1A (followed by index
02) which selects this window. The oﬀset of code 0562 in the new window is 62, so
it is compressed to byte E2. Finally, code 000D (CR) is compressed to its eight least-
signiﬁcant bits 0D without an additional tag.
Tag 12 is called SC2. It indicates a locking shift to dynamic window 2, which starts
at 0x0400 by default. Tag 1A is called SD2 and indicates a repositioning of window 2.
The byte 02 that follows 1A is an index to Table 7.10 and changes the window’s start
position by 2×8016 = 10016, so the window moves from the original 0x0400 to 0x0500.
We start with the details of the single-byte mode. This mode is the one in eﬀect
when the SCSU encoder starts. Each 16-bit code is compressed in this mode to a single
byte. Tags are needed from time to time and may be followed by up to two arguments,
each a byte. This mode continues until one of the following is encountered: (1) end-of-
input, (2) an SCU tag, or (3) an SQU tag. Six types of tags (for a total of 27 diﬀerent
tags) are used in this mode as follows.
1. SQU (0E). This tag (termed quote Unicode) is a temporary (nonlocking) shift
to Unicode mode. This tag is followed by the two bytes of a raw code.
2. SCU (0F). This tag (change to Unicode) is a permanent (locking) shift to Unicode
mode. This is used for a string of consecutive characters that belong to diﬀerent scripts
and are therefore in diﬀerent windows.

7.3 SCSU: Unicode Compression
261
X
Oﬀset[X]
Comments
00
reserved
for internal use
01–67
X×80
half-blocks from U+0080 to U+3380
68–A7
X×80 + AC00
half-blocks from U+E000 to U+FF80
A8–F8
reserved for future use
F9
00C0
Latin1 characters + half of Extended-A
FA
0250
IPA extensions
FB
0370
Greek
FC
0530
Armenian
FD
3040
Hiragana
FE
30A0
Katakana
FF
FF60
Halfwidth Katakana
Table 7.10: Window Oﬀsets.
3. SQn (01–08). This tag (quote from window n) is a nonlocking shift to window
n. It quotes (i.e., writes in raw format) the next code, so there is no compression. The
value of n (between 0 and 7) is determined by the tag (between 1 and 8). This tag must
be followed by a byte used as an oﬀset to the selected window. If the byte is in the
interval 00 to 7F, static window n should be selected. If it is in the range 80 to FF,
dynamic window n should be selected. This tag quotes one code, then switches back to
the single-byte mode. For example, SQ3 followed by 14 selects oﬀset 14 in static window
3, so it quotes code 0300 + 14 = 0314. Another example is SQ4 followed by 8A. This
selects oﬀset 8A −80 = 0A in dynamic window 4 (which normally starts at 0900, but
could be repositioned), so it quotes code 0900 + 0A = 090A.
4. SCn (10–17). This tag (change to window n) is a locking shift to window n.
5. SDn (18–1F). This tag (deﬁne window n) repositions window n and makes it the
current window. The tag is followed by a one-byte index to Table 7.10 that indicates
the new start address of window n.
6. SDX (0B). This is the “deﬁne extended” tag. It is followed by two bytes denoted
by H and L. The three most-signiﬁcant bits of H determine the static window to be
selected, and the remaining 13 bits of H and L become one integer N that determines
the start address of the window as 10000 + 80×N (hexadecimal).
Tag SQ0 is important. It is used to ﬂag code points whose most-signiﬁcant byte
may be confused with a tag (i.e., it is in the range 00 through 1F). When encountering
such a byte, the decoder should be able to tell whether it is a tag or the start of a raw
code. As a result, when the encoder inputs a code that starts with such a byte, it writes
it on the output in raw format (quoted), preceded by an SQ0 tag.
Next comes the Unicode mode. Each character is written in this mode in raw form,
so there is no compression (there is even slight expansion due to the tags required).
Once this mode is selected by an SCU tag, it stays in eﬀect until the end of the input
or until a tag that selects an active window is encountered. Four types of tags are used
in this mode as follows:
1. UQU (F0). This tag quotes a Unicode character. The two bytes following the
tag are written on the output in raw format.

262
7.
Other Methods
2. UCn (E0–E7). This tag is a locking shift to single-byte mode and it also selects
window n.
3. UDn (E8–EF). Deﬁne window n. This tag is followed by a single-byte index. It
selects window n and repositions it according to the start positions of Table 7.10.
4. UDX (F1). Deﬁne extended window. This tag (similar to SDX) is followed
by two bytes denoted by H and L. The three most-signiﬁcant bits of H determine the
dynamic window to be selected, and the remaining 13 bits of H and L become an integer
N that determines the start address of the window by 10000 + 80×N (hexadecimal).
The four types of tags require 18 tag values, but almost all the possible 29 tag values
are used by the single-byte mode. As a result, the Unicode mode uses tag values that
are valid code points. Byte E0, for example, is tag UC0, but is also the most-signiﬁcant
half of a valid code point (in fact, it is the most-signiﬁcant half of 256 valid code points).
The encoder therefore reserves these 18 values (plus a few more for future use) for tags.
When the encoder encounters any character whose code starts with one of those values,
the character is written in raw format (preceded by a UQU tag). Such cases are not
common because the reserved values are taken from the private-use area of Unicode,
and this area is rarely used.
SCSU also speciﬁes ways to compress Unicode surrogates. With 16-bit codes, there
can be 65,536 codes.
However, 80016 = 204810 16-bit codes have been reserved for
an extension of Unicode to 32-bit codes. The 40016 codes U+D800 through U+DBFF
are reserved as high surrogates, and the 40016 codes U+DC00 through U+DFFF are
reserved as low surrogates. This allows for an additional 400×400 = 100, 00016 32-bit
codes. A 32-bit code is known as a surrogate pair and can be encoded in SCSU in one
of several ways, three of which are shown here:
1. In Unicode mode, in raw format (four bytes).
2. In single-byte mode, with each half quoted. Thus, SQU, H1, L1, SQU, H2, L2.
3. Also in single-byte mode, as a single byte, by setting a dynamic window to the
appropriate position with an SDX or UDX tag.
The 2-code sequence U+FEFF (or its reversed counterpart U+FFFE) occurs very
rarely in text ﬁles, so it serves as a signature, to identify text ﬁles in Unicode. This
sequence is known as a byte order mark or BOM. SCSU recommends several ways of
compressing this signature, and an encoder can select any of those.
7.3.1 BOCU-1: Unicode Compression
The acronym BOCU stands for binary-ordered compression for Unicode. BOCU is a
simple compression method for Unicode-based ﬁles [BOCU 01].
Its main feature is
preserving the binary sort order of the code points being compressed. Thus, if two code
points x and y are compressed to a and b and if x < y, then a < b.
The basic BOCU method is based on diﬀerencing.
The previous code point is
subtracted from the current code point to yield a diﬀerence value. Consecutive code
points in a document are normally similar, so most diﬀerences are small and ﬁt in a
single byte. However, because a code point in Unicode 2.0 (published in 1996) is in the
range U+000000 to U+10FFFF (21-bit codes), the diﬀerences can, in principle, be any
numbers in the interval [−10FFFF, 10FFFF] and may require up to three bytes each.
This basic method is enhanced in two ways.

Chapter Summary
263
The ﬁrst enhancement improves compression in small alphabets. In Unicode, most
small alphabets start on a 128-byte boundary, although the alphabet size may be more
than 128 symbols. This suggests that a diﬀerence be computed not between the current
and previous code values but between the current code value and the value in the
middle of the 128-byte segment where the previous code value is located. Speciﬁcally,
the diﬀerence is computed by subtracting a base value from the current code point. The
base value is obtained from the previous code point as follows. If the previous code value
is in the interval xxxx00 to xxxx7F (i.e., its seven LSBs are 0 to 127), the base value
is set to xxxx40 (the seven LSBs are 64), and if the previous code point is in the range
xxxx80 to xxxxFF (i.e., its seven least-signiﬁcant bits are 128 to 255), the base value is
set to xxxxC0 (the seven LSBs are 192). This way, if the current code point is within
128 positions of the base value, the diﬀerence is in the range [−128, +127] which makes
it ﬁt in one byte.
The second enhancement has to do with remote symbols. A document in a non-
Latin alphabet (where the code points are very diﬀerent from the ASCII codes) may use
spaces between words. The code point for a space is the ASCII code 2016, so any pair of
code points that includes a space results in a large diﬀerence. BOCU therefore computes
a diﬀerence by ﬁrst computing the base values of the three previous code points, and
then subtracting the smallest base value from the current code point.
BOCU-1 is the version of BOCU that’s commonly used in practice [BOCU-1 02]. It
diﬀers from the original BOCU method by using a diﬀerent set of byte value ranges and
by encoding the ASCII control characters U+0000 through U+0020 with byte values 0
through 2016, respectively. These features make BOCU-1 suitable for compressing input
ﬁles that are MIME (text) media types.
Il faut avoir beaucoup ´etudi´e pour savoir peu (it is necessary to study much in order
to know little).
—Montesquieu (Charles de Secondat), Pens´ees diverses
Chapter Summary
This chapter is devoted to data compression methods and techniques that are not based
on the approaches discussed elsewhere in this book. The following algorithms illustrate
some of these original techniques:
The Burrows–Wheeler method (Section 7.1) starts with a string S of n symbols and
scrambles (i.e., permutes) them into another string L that satisﬁes two conditions: (1)
Any area of L will tend to have a concentration of just a few symbols. (2) It is possible
to reconstruct the original string S from L. Since its inception in the early 1990s, this
unexpected method has been the subject of much research.
The technique of symbol ranking (Section 7.2) uses context, rather than probabili-
ties, to rank symbols.
Sections 7.3 and 7.3.1 describe two algorithms, SCSU and BOCU-1, for the com-
pression of Unicode-based documents.

264
7.
Other Methods
Chapter 8 of [Salomon 07] discusses other methods, techniques, and approaches to
data compression.
Self-Assessment Questions
1. The term “fractals” appears early in this chapter. One of the applications of
fractals is to compress images, and it is the purpose of this note to encourage the reader
to search for material on fractal compression and study it.
2. The Burrows–Wheeler method has been the subject of much research and at-
tempts to speed up its decoding and improve it. Using the paper at [JuergenAbel 07]
as your starting point, try to gain a deeper understanding of this interesting method.
3. The term “lexicographic order” appears in Section 7.1. This is an important
term in computer science in general, and the conscientious reader should make sure this
term is fully understood.
4. Most Unicodes are 16 bits long, but this standard has provisions for longer codes.
Use [Unicode 07] as a starting point to learn more about Unicode and how codes longer
than 16 bits are structured.
In comedy, as a matter of fact, a greater variety of
methods were discovered and employed than in tragedy.
—T. S. Eliot, The Sacred Wood (1920)

Bibliography
Ahmed, N., T. Natarajan, and R. K. Rao (1974) “Discrete Cosine Transform,” IEEE
Transactions on Computers, C-23:90–93.
Bell, Timothy C., John G. Cleary, and Ian H. Witten (1990) Text Compression, Engle-
wood Cliﬀs, Prentice Hall.
BOCU (2001) is http://oss.software.ibm.com/icu/docs/papers/
binary_ordered_compression_for_unicode.html.
BOCU-1 (2002) is http://www.unicode.org/notes/tn6/.
Bookstein, Abraham and S. T. Klein (1993) “Is Huﬀman Coding Dead?” Proceedings of
the 16th Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval, pp. 80–87. Also published in Computing, 50(4):279–296, 1993,
and in Proceedings of the Data Compression Conference, 1993, Snowbird, UT. p. 464.
Bradley, Jonathan N., Christopher M. Brislawn, and Tom Hopper (1993) “The FBI
Wavelet/Scalar Quantization Standard for Grayscale Fingerprint Image Compression,”
Proceedings of Visual Information Processing II, Orlando, FL, SPIE vol. 1961, pp. 293–
304, April.
Brandenburg, Karlheinz, and Gerhard Stoll (1994) “ISO-MPEG-1 Audio: A Generic
Standard for Coding of High-Quality Digital Audio,” Journal of the Audio Engineering
Society, 42(10):780–792, October.
brucelindbloom (2007) is http://www.brucelindbloom.com/ (click on “info”).
Burrows, Michael, and D. J. Wheeler (1994) A Block-Sorting Lossless Data Compression
Algorithm, Digital Systems Research Center Report 124, Palo Alto, CA, May 10.
Calude, Cristian and Tudor Zamﬁrescu (1998) “The Typical Number is a Lexicon,” New
Zealand Journal of Mathematics, 27:7–13.
Campos, Arturo San Emeterio (2006) Range coder, in
http://www.arturocampos.com/ac_range.html.

266
Bibliography
Carpentieri, B., M. J. Weinberger, and G. Seroussi (2000) “Lossless Compression of
Continuous-Tone Images,” Proceedings of the IEEE, 88(11):1797–1809, November.
Chaitin (2007) is http://www.cs.auckland.ac.nz/CDMTCS/chaitin/sciamer3.html.
Choueka Y., Shmuel T. Klein, and Y. Perl (1985) “Eﬃcient Variants of Huﬀman Codes
in High Level Languages,” Proceedings of the 8th ACM-SIGIR Conference, Montreal,
pp. 122–130.
Deﬂate (2003) is http://www.gzip.org/zlib/.
Elias, P. (1975) “Universal Codeword Sets and Representations of the Integers,” IEEE
Transactions on Information Theory, 21(2):194–203, March.
Faller N. (1973) “An Adaptive System for Data Compression,” in Record of the 7th
Asilomar Conference on Circuits, Systems, and Computers, pp. 593–597.
Fano, R. M. (1949) “The Transmission of Information,” Research Laboratory for Elec-
tronics, MIT, Tech Rep. No. 65.
Federal Bureau of Investigation (1993) WSQ Grayscale Fingerprint Image Compression
Speciﬁcation, ver. 2.0, Document #IAFIS-IC-0110v2, Criminal Justice Information Ser-
vices, February.
Feldspar (2003) is http://www.zlib.org/feldspar.html.
Fenwick, Peter (1996a) “Punctured Elias Codes for Variable-Length Coding of the Inte-
gers,” Technical Report 137, Department of Computer Science, University of Auckland,
December. This is also available online.
Fenwick, P. (1996b) Symbol Ranking Text Compression, Tech. Rep. 132, Dept. of Com-
puter Science, University of Auckland, New Zealand, June.
Fraenkel, Aviezri S. and Shmuel T. Klein (1996) “Robust Universal Complete Codes for
Transmission and Compression,” Discrete Applied Mathematics, 64(1):31–55, January.
funet (2007) is ftp://nic.funet.fi/pub/graphics/misc/test-images/.
G.711 (1972) is http://en.wikipedia.org/wiki/G.711.
Gallager, Robert G. (1978) “Variations on a Theme by Huﬀman,” IEEE Transactions
on Information Theory, 24(6):668–674, November.
Gardner, Martin (1972) “Mathematical Games,” Scientiﬁc American, 227(2):106, Au-
gust.
Gemstar (2007) is http://www.gemstartvguide.com.
Gilbert, E. N. and E. F. Moore (1959) “Variable Length Binary Encodings,” Bell System
Technical Journal, 38:933–967.
Gray, Frank (1953) “Pulse Code Communication,” United States Patent 2,632,058,
March 17.
Haar, A. (1910) “Zur Theorie der Orthogonalen Funktionensysteme,” Mathematische
Annalen ﬁrst part 69:331–371, second part 71:38–53, 1912.

Bibliography
267
Heath, F. G. (1972) “Origins of the Binary Code,” Scientiﬁc American, 227(2):76,
August.
Hecht, S., S. Schlaer, and M. H. Pirenne (1942) “Energy, Quanta and Vision,” Journal
of the Optical Society of America, 38:196–208.
hﬀax (2007) is http://www.hffax.de/html/hauptteil_faxhistory.htm.
Hilbert, D. (1891) “Ueber stetige Abbildung einer Linie auf ein Fl¨achenst¨uck,” Math.
Annalen, 38:459–460.
Hirschberg, D., and D. Lelewer (1990) “Eﬃcient Decoding of Preﬁx Codes,” Communi-
cations of the ACM, 33(4):449–459.
Holzmann, Gerard J. and Bj¨orn Pehrson (1995) The Early History of Data Networks,
Los Alamitos, CA, IEEE Computer Society Press. This is available online at
http://labit501.upct.es/ips/libros/TEHODN/ch-2-5.3.html.
Huﬀman, David (1952) “A Method for the Construction of Minimum Redundancy
Codes,” Proceedings of the IRE, 40(9):1098–1101.
incredible (2007) is http://datacompression.info/IncredibleClaims.shtml.
ITU-T (1989) CCITT Recommendation G.711: “Pulse Code Modulation (PCM) of
Voice Frequencies.”
JuergenAbel (2007) is ﬁle Preprint_After_BWT_Stages.pdf in
http://www.data-compression.info/JuergenAbel/Preprints/.
Karp, R. S. (1961) “Minimum-Redundancy Coding for the Discrete Noiseless Channel,”
Transactions of the IRE, 7:27–38.
Knuth, Donald E. (1985) “Dynamic Huﬀman Coding,” Journal of Algorithms, 6:163–
180.
Kraft, L. G. (1949) A Device for Quantizing, Grouping, and Coding Amplitude Modulated
Pulses, Master’s Thesis, Department of Electrical Engineering, MIT, Cambridge, MA.
Linde, Y., A. Buzo, and R. M. Gray (1980) “An Algorithm for Vector Quantization
Design,” IEEE Transactions on Communications, COM-28:84–95, January.
Lloyd, S. P. (1982) “Least Squares Quantization in PCM,” IEEE Transactions on In-
formation Theory, IT-28:129–137, March.
Manber, U., and E. W. Myers (1993) “Suﬃx Arrays: A New Method for On-Line String
Searches,” SIAM Journal on Computing, 22(5):935–948, October.
Max, Joel (1960) “Quantizing for minimum distortion,” IRE Transactions on Informa-
tion Theory, IT-6:7–12, March.
McCreight, E. M (1976) “A Space Economical Suﬃx Tree Construction Algorithm,”
Journal of the ACM, 32(2):262–272, April.
McMillan, Brockway (1956) “Two Inequalities Implied by Unique Decipherability,”
IEEE Transactions on Information Theory, 2(4):115–116, December.

268
Bibliography
MNG (2003) is http://www.libpng.org/pub/mng/spec/.
Moﬀat, Alistair, Radford Neal, and Ian H. Witten (1998) “Arithmetic Coding Revis-
ited,” ACM Transactions on Information Systems, 16(3):256–294, July.
Motil, John (2007) Private communication.
Mulcahy, Colm (1996) “Plotting and Scheming with Wavelets,” Mathematics Magazine,
69(5):323–343, December. See also http://www.spelman.edu/~colm/csam.ps.
Mulcahy, Colm (1997) “Image Compression Using the Haar Wavelet Transform,” Spel-
man College Science and Mathematics Journal, 1(1):22–31, April.
Also available at
URL http://www.spelman.edu/~colm/wav.ps. (It has been claimed that any smart
15-year-old could follow this introduction to wavelets.)
Osterberg, G. (1935) “Topography of the Layer of Rods and Cones in the Human
Retina,” Acta Ophthalmologica, (suppl. 6):1–103.
Paez, M. D. and T. H. Glisson (1972) “Minimum Mean Squared Error Quantization in
Speech PCM and DPCM Systems,” IEEE Transactions on Communications, COM-
20(2):225–230,
patents (2007) is http://www.datacompression.info/patents.shtml.
PDF (2001) Adobe Portable Document Format Version 1.4, 3rd ed., Reading, MA,
Addison-Wesley, December.
Pennebaker, William B., and Joan L. Mitchell (1992) JPEG Still Image Data Compres-
sion Standard, New York, Van Nostrand Reinhold.
Phillips, Dwayne (1992) “LZW Data Compression,” The Computer Application Journal
Circuit Cellar Inc., 27:36–48, June/July.
PKWare (2003) is http://www.pkware.com.
PNG (2003) is http://www.libpng.org/pub/png/.
RFC1945 (1996) Hypertext Transfer Protocol—HTTP/1.0, available at URL
http://www.faqs.org/rfcs/rfc1945.html.
RFC1950 (1996) ZLIB Compressed Data Format Speciﬁcation version 3.3, is
http://www.ietf.org/rfc/rfc1950.
RFC1951 (1996) DEFLATE Compressed Data Format Speciﬁcation version 1.3, is
http://www.ietf.org/rfc/rfc1951.
RFC1952 (1996) GZIP File Format Speciﬁcation Version 4.3. Available in PDF format
at URL http://www.gzip.org/zlib/rfc-gzip.html.
RFC1962 (1996) The PPP Compression Control Protocol (CCP), available from many
sources.
RFC1979 (1996) PPP Deﬂate Protocol, is http://www.faqs.org/rfcs/rfc1979.html.
RFC2616 (1999) Hypertext Transfer Protocol – HTTP/1.1. Available in PDF format at
URL http://www.faqs.org/rfcs/rfc2616.html.

Bibliography
269
Rice, Robert F. (1979) “Some Practical Universal Noiseless Coding Techniques,” Jet
Propulsion Laboratory, JPL Publication 79-22, Pasadena, CA, March.
Rice, Robert F. (1991) “Some Practical Universal Noiseless Coding Techniques—Part
III. Module PSI14.K,” Jet Propulsion Laboratory, JPL Publication 91-3, Pasadena, CA,
November.
Robinson, Tony (1994) “Simple Lossless and Near-Lossless Waveform Compression,”
Technical Report CUED/F-INFENG/TR.156, Cambridge University, December. Avail-
able at http://citeseer.nj.nec.com/robinson94shorten.html.
Salomon, David (1999) Computer Graphics and Geometric Modeling, New York, Springer.
Salomon, David (2006) Curves and Surfaces for Computer Graphics, New York, Springer.
Salomon, D. (2007) Data Compression: The Complete Reference, London, Springer
Verlag.
Schindler, Michael (1998) “A Fast Renormalisation for Arithmetic Coding,” a poster in
the Data Compression Conference, 1998, available at URL
http://www.compressconsult.com/rangecoder/.
Shannon, Claude E. (1948), “A Mathematical Theory of Communication,” Bell System
Technical Journal, 27:379–423 and 623–656, July and October,
Shannon, Claude (1951) “Prediction and Entropy of Printed English,” Bell System Tech-
nical Journal, 30(1):50–64, January.
Shenoi, Kishan (1995) Digital Signal Processing in Telecommunications, Upper Saddle
River, NJ, Prentice Hall.
Sieminski, A. (1988) “Fast Decoding of the Huﬀman Codes,” Information Processing
Letters, 26(5):237–241.
Softsound (2007) is http://mi.eng.cam.ac.uk/reports/ajr/TR156/tr156.html.
Strang, Gilbert, and Truong Nguyen (1996) Wavelets and Filter Banks, Wellesley, MA,
Wellesley-Cambridge Press.
technikum29 (2007) is http://www.technikum29.de/en/communication/fax.shtm.
Tetrachromat (2007) is http://en.wikipedia.org/wiki/Tetrachromat.
Unicode (2007) is http://unicode.org/.
Vitter, Jeﬀrey S. (1987) “Design and Analysis of Dynamic Huﬀman Codes,” Journal of
the ACM, 34(4):825–845, October.
Wallace, Gregory K. (1991) “The JPEG Still Image Compression Standard,” Commu-
nications of the ACM, 34(4):30–44, April.
Watson, Andrew (1994) “Image Compression Using the Discrete Cosine Transform,”
Mathematica Journal, 4(1):81–88.
Weisstein-pickin (2007) is Weisstein, Eric W. “Real Number Picking.” From MathWorld,
A Wolfram web resource. http://mathworld.wolfram.com/RealNumberPicking.html.

270
Bibliography
Welch, T. A. (1984) “A Technique for High-Performance Data Compression,” IEEE
Computer, 17(6):8–19, June.
Wirth, N. (1976) Algorithms + Data Structures = Programs, 2nd ed., Englewood Cliﬀs,
NJ, Prentice-Hall.
Witten, Ian H., Radford M. Neal, and John G. Cleary (1987) “Arithmetic Coding for
Data Compression,” Communications of the ACM, 30(6):520–540.
Wolf, Misha et al. (2000) “A Standard Compression Scheme for Unicode,” Unicode Tech-
nical Report #6, available at http://unicode.org/unicode/reports/tr6/index.html.
Zhang, Manyun (1990) The JPEG and Image Data Compression Algorithms (disserta-
tion).
Ziv, Jacob, and A. Lempel (1977) “A Universal Algorithm for Sequential Data Com-
pression,” IEEE Transactions on Information Theory, IT-23(3):337–343.
Ziv, Jacob and A. Lempel (1978) “Compression of Individual Sequences via Variable-
Rate Coding,” IEEE Transactions on Information Theory, IT-24(5):530–536.
zlib (2003) is http://www.zlib.org/zlib_tech.html.
A literary critic is a person who ﬁnds meaning in
literature that the author didn’t know was there.
—Anonymous

Glossary
A glossary is a list of terms in a particular domain of knowledge with the deﬁnitions
for those terms. Traditionally, a glossary appears at the end of a book and includes
terms within that book which are either newly introduced or at least uncommon.
In a more general sense, a glossary contains explanations of concepts relevant to a
certain ﬁeld of study or action. In this sense, the term is contemporaneously related
to ontology.
—From Wikipedia.com
Adaptive Compression.
A compression method that modiﬁes its operations and/or its
parameters in response to new data read from the input. Examples are the adaptive
Huﬀman method of Section 2.3 and the dictionary-based methods of Chapter 3. (See
also Semiadaptive Compression.)
Alphabet. The set of all possible symbols in the input. In text compression, the alphabet
is normally the set of 128 ASCII codes. In image compression, it is the set of values a
pixel can take (2, 16, 256, or anything else). (See also Symbol.)
Arithmetic Coding. A statistical compression method (Chapter 4) that assigns one
(normally long) code to the entire input ﬁle, instead of assigning codes to the individual
symbols.
The method reads the input symbol by symbol and appends more bits to
the code each time a symbol is input and processed. Arithmetic coding is slow, but it
compresses at or close to the entropy, even when the symbol probabilities are skewed.
(See also Model of Compression, Statistical Methods.)
ASCII Code. The standard character code on all modern computers (although Unicode
is fast becoming a serious competitor). ASCII stands for American Standard Code for
Information Interchange. It is a (1+7)-bit code, with one parity bit and seven data bits
per symbol. As a result, 128 symbols can be coded. They include the uppercase and
lowercase letters, the ten digits, some punctuation marks, and control characters. (See
also Unicode.)

272
Glossary
Bark. Unit of critical band rate. Named after Heinrich Georg Barkhausen and used in
audio applications. The Bark scale is a nonlinear mapping of the frequency scale over
the audio range, a mapping that matches the frequency selectivity of the human ear.
Bi-level Image. An image whose pixels have two diﬀerent colors. The colors are nor-
mally referred to as black and white, “foreground” and “background,” or 1 and 0. (See
also Bitplane.)
Bitplane. Each pixel in a digital image is represented by several bits. The set of all the
kth bits of all the pixels in the image is the kth bitplane of the image. A bi-level image,
for example, consists of one bitplane. (See also Bi-level Image.)
Bitrate. In general, the term “bitrate” refers to both bpb and bpc. However, in audio
compression, this term is used to indicate the rate at which the compressed ﬁle is read
by the decoder. This rate depends on where the ﬁle comes from (such as disk, communi-
cations channel, memory). If the bitrate of an MPEG audio ﬁle is, e.g., 128 Kbps, then
the encoder will convert each second of audio into 128 K bits of compressed data, and
the decoder will convert each group of 128 K bits of compressed data into one second
of sound. Lower bitrates mean smaller ﬁle sizes. However, as the bitrate decreases, the
encoder must compress more audio data into fewer bits, eventually resulting in a no-
ticeable loss of audio quality. For CD-quality audio, experience indicates that the best
bitrates are in the range of 112 Kbps to 160 Kbps. (See also Bits/Char.)
Bits/Char. Bits per character (bpc). A measure of the performance in text compression.
Also a measure of entropy. (See also Bitrate, Entropy.)
Bits/Symbol. Bits per symbol. A general measure of compression performance.
Block Coding. A general term for image compression methods that work by breaking the
image into small blocks of pixels, and encoding each block separately. JPEG (Section 5.6)
is a good example, because it processes blocks of 8×8 pixels.
Burrows–Wheeler Method. This method (Section 7.1) prepares a string of data for
later compression. The compression itself is done with the move-to-front method (see
item in this glossary), perhaps in combination with RLE. The BW method converts a
string S to another string L that satisﬁes two conditions:
1. Any region of L will tend to have a concentration of just a few symbols.
2. It is possible to reconstruct the original string S from L (a little more data may be
needed for the reconstruction, in addition to L, but not much).
CCITT. The International Telegraph and Telephone Consultative Committee (Comit´e
Consultatif International T´el´egraphique et T´el´ephonique), the old name of the ITU, the
International Telecommunications Union. The ITU is a United Nations organization
responsible for developing and recommending standards for data communications (not
just compression). (See also ITU.)
CIE. CIE is an abbreviation for Commission Internationale de l’´Eclairage (International
Committee on Illumination). This is the main international organization devoted to light
and color. It is responsible for developing standards and deﬁnitions in this area. (See
also Luminance.)

Glossary
273
Circular Queue. A basic data structure (see the last paragraph of Section 1.3.1) that
moves data along an array in circular fashion, updating two pointers to point to the
start and end of the data in the array.
Codec. A term that refers to both encoder and decoder.
Codes. A code is a symbol that stands for another symbol. In computer and telecom-
munications applications, codes are virtually always binary numbers. The ASCII code
is the defacto standard, although the new Unicode is used on several new computers and
the older EBCDIC is still used on some old IBM computers. In addition to these ﬁxed-
size codes there are many variable-length codes used in data compression and there are
the all-important error-control codes for added robustness (See also ASCII, Unicode.)
Compression Factor. The inverse of compression ratio. It is deﬁned as
compression factor = size of the input ﬁle
size of the output ﬁle.
Values greater than 1 indicate compression, and values less than 1 imply expansion. (See
also Compression Ratio.)
Compression Gain. This measure is deﬁned as
100 loge
reference size
compressed size,
where the reference size is either the size of the input ﬁle or the size of the compressed
ﬁle produced by some standard lossless compression method.
Compression Ratio. One of several measures that are commonly used to express the
eﬃciency of a compression method. It is the ratio
compression ratio = size of the output ﬁle
size of the input ﬁle .
A value of 0.6 indicates that the data occupies 60% of its original size after compres-
sion. Values greater than 1 mean an output ﬁle bigger than the input ﬁle (negative
compression).
Sometimes the quantity 100 × (1 −compression ratio) is used to express the quality
of compression. A value of 60 means that the output ﬁle occupies 40% of its original
size (or that the compression has resulted in a savings of 60%). (See also Compression
Factor.)
Continuous-Tone Image. A digital image with a large number of colors, such that
adjacent image areas with colors that diﬀer by just one unit appear to the eye as having
continuously varying colors. An example is an image with 256 grayscale values. When
adjacent pixels in such an image have consecutive gray levels, they appear to the eye as
a continuous variation of the gray level. (See also Bi-level image, Discrete-Tone Image,
Grayscale Image.)

274
Glossary
Decoder. A program, an algorithm, or a piece of hardware for decompressing data.
Deﬂate. A popular lossless compression algorithm (Section 3.3) used by Zip and gzip.
Deﬂate employs a variant of LZ77 combined with static Huﬀman coding. It uses a 32-
Kb-long sliding dictionary and a look-ahead buﬀer of 258 bytes. When a string is not
found in the dictionary, its ﬁrst symbol is emitted as a literal byte. (See also Zip.)
Dictionary-Based Compression. Compression methods (Chapter 3) that save pieces of
the data in a “dictionary” data structure. If a string of new data is identical to a piece
that is already saved in the dictionary, a pointer to that piece is output to the compressed
ﬁle. (See also LZ Methods.)
Discrete Cosine Transform.
A variant of the discrete Fourier transform (DFT) that
produces just real numbers.
The DCT (Sections 5.5 and 5.6.2) transforms a set of
numbers by combining n numbers to become an n-dimensional point and rotating it in
n dimensions such that the ﬁrst coordinate becomes dominant. The DCT and its inverse,
the IDCT, are used in JPEG (Section 5.6) to compress an image with acceptable loss, by
isolating the high-frequency components of an image, so that they can later be quantized.
(See also Fourier Transform, Transform.)
Discrete-Tone Image. A discrete-tone image may be bi-level, grayscale, or color. Such
images are (with some exceptions) artiﬁcial, having been obtained by scanning a docu-
ment, or capturing a computer screen. The pixel colors of such an image do not vary
continuously or smoothly, but have a small set of values, such that adjacent pixels may
diﬀer much in intensity or color. (See also Continuous-Tone Image.)
Discrete Wavelet Transform. The discrete version of the continuous wavelet transform.
A wavelet is represented by means of several ﬁlter coeﬃcients, and the transform is car-
ried out by matrix multiplication (or a simpler version thereof) instead of by calculating
an integral.
Encoder. A program, algorithm, or hardware circuit for compressing data.
Entropy. The entropy of a single symbol ai is deﬁned as −Pi log2 Pi, where Pi is the
probability of occurrence of ai in the data. The entropy of ai is the smallest number
of bits needed, on average, to represent symbol ai.
Claude Shannon, the creator of
information theory, coined the term entropy in 1948, because this term is used in ther-
modynamics to indicate the amount of disorder in a physical system. (See also Entropy
Encoding, Information Theory.)
Entropy Encoding.
A lossless compression method where data can be compressed such
that the average number of bits/symbol approaches the entropy of the input symbols.
(See also Entropy.)
Facsimile Compression. Transferring a typical page between two fax machines can take
up to 10–11 minutes without compression. This is why the ITU has developed several
standards for compression of facsimile data. The current standards (Section 2.4) are T4
and T6, also called Group 3 and Group 4, respectively. (See also ITU.)

Glossary
275
Fourier Transform. A mathematical transformation that produces the frequency com-
ponents of a function. The Fourier transform represents a periodic function as the sum
of sines and cosines, thereby indicating explicitly the frequencies “hidden” in the original
representation of the function. (See also Discrete Cosine Transform, Transform.)
Gaussian Distribution. (See Normal Distribution.)
Golomb Codes. The Golomb codes consist of an inﬁnite set of parametrized preﬁx
codes. They are the best variable-length codes for the compression of data items that
are distributed geometrically. (See also Unary Code.)
Gray Codes. Gray codes are binary codes for the integers, where the codes of consecutive
integers diﬀer by one bit only. Such codes are used when a grayscale image is separated
into bitplanes, each a bi-level image. (See also Grayscale Image,)
Grayscale Image. A continuous-tone image with shades of a single color.
(See also
Continuous-Tone Image.)
Huﬀman Coding.
A popular method for data compression (Chapter 2). It assigns
a set of “best” variable-length codes to a set of symbols based on their probabilities.
It serves as the basis for several popular programs used on personal computers. Some
of them use just the Huﬀman method, while others use it as one step in a multistep
compression process. The Huﬀman method is somewhat similar to the Shannon–Fano
method.
It generally produces better codes, and like the Shannon–Fano method, it
produces best code when the probabilities of the symbols are negative powers of 2. The
main diﬀerence between the two methods is that Shannon–Fano constructs its codes top
to bottom (from the leftmost to the rightmost bits), while Huﬀman constructs a code
tree from the bottom up (builds the codes from right to left). (See also Shannon–Fano
Coding, Statistical Methods.)
Information Theory. A mathematical theory that quantiﬁes information. It shows how
to measure information, so that one can answer the question, how much information is
included in a given piece of data? with a precise number! Information theory is the
creation, in 1948, of Claude Shannon of Bell Labs. (See also Entropy.)
ITU. The International Telecommunications Union, the new name of the CCITT, is a
United Nations organization responsible for developing and recommending standards for
data communications (not just compression). (See also CCITT.)
JFIF. The full name of this method (Section 5.6.7) is JPEG File Interchange Format.
It is a graphics ﬁle format that makes it possible to exchange JPEG-compressed images
between diﬀerent computers. The main features of JFIF are the use of the YCbCr triple-
component color space for color images (only one component for grayscale images) and
the use of a marker to specify features missing from JPEG, such as image resolution,
aspect ratio, and features that are application-speciﬁc.
JPEG. A sophisticated lossy compression method (Section 5.6) for color or grayscale
still images (not video). It works best on continuous-tone images, where adjacent pixels
have similar colors. One advantage of JPEG is the use of many parameters, allowing
the user to adjust the amount of data loss (and thereby also the compression ratio) over

276
Glossary
a very wide range. There are two main modes, lossy (also called baseline) and lossless
(which typically yields a 2:1 compression ratio). Most implementations support just the
lossy mode. This mode includes progressive and hierarchical coding.
The main idea behind JPEG is that an image exists for people to look at, so when the
image is compressed, it is acceptable to lose image features to which the human eye is
not sensitive.
The name JPEG is an acronym that stands for Joint Photographic Experts Group. This
was a joint eﬀort by the CCITT and the ISO that started in June 1987. The JPEG
standard has proved successful and has become widely used for image presentation,
especially in web pages.
Kraft–McMillan Inequality. A relation that says something about unambiguous variable-
length codes. Its ﬁrst part states: Given an unambiguous variable-size code, with n codes
of sizes Li, then
n

i=1
2−Li ≤1.
[This is Equation (1.4).] The second part states the opposite, namely, given a set of n
positive integers (L1, L2, . . . , Ln) that satisfy Equation (1.4), there exists an unambigu-
ous variable-size code such that Li are the sizes of its individual codes. Together, both
parts state that a code is unambiguous if and only if it satisﬁes relation (1.4).
Laplace Distribution. A probability distribution similar to the normal (Gaussian) dis-
tribution, but narrower and sharply peaked.
The general Laplace distribution with
variance V and mean m is given by
L(V, x) =
1
√
2V
exp
!
−

2
V |x −m|
"
.
Experience seems to suggest that the values of the residues computed by many im-
age compression algorithms are Laplace distributed, which is why this distribution is
employed by those compression methods, most notably MLP. (See also Normal Distri-
bution.)
Lossless Compression. A compression method where the output of the decoder is iden-
tical to the original data compressed by the encoder. (See also Lossy Compression.)
Lossy Compression. A compression method where the output of the decoder is diﬀerent
from the original data compressed by the encoder, but is nevertheless acceptable to a
user.
Such methods are common in image and audio compression, but not in text
compression, where the loss of even one character may result in wrong, ambiguous, or
incomprehensible text. (See also Lossless Compression.)
Luminance.
This quantity is deﬁned by the CIE (Section 5.6.1) as radiant power
weighted by a spectral sensitivity function that is characteristic of vision.
(See also
CIE.)

Glossary
277
LZ Methods.
All dictionary-based compression methods are based on the work of
J. Ziv and A. Lempel, published in 1977 and 1978. Today, these are called LZ77 and
LZ78 methods, respectively.
Their ideas have been a source of inspiration to many
researchers, who generalized, improved, and combined them with RLE and statistical
methods to form many commonly used adaptive compression methods, for text, images,
and audio. (See also Dictionary-Based Compression, Sliding-Window Compression.)
LZW. This is a popular variant (Section 3.2) of LZ78, originated by Terry Welch in
1984. Its main feature is eliminating the second ﬁeld of a token. An LZW token consists
of just a pointer to the dictionary. As a result, such a token always encodes a string of
more than one symbol. (See also Patents.)
Model of Compression. A model is a method to “predict” (to assign probabilities to)
the data to be compressed. This concept is important in statistical data compression.
When a statistical method is used, a model for the data has to be constructed before
compression can begin. A simple model can be built by reading the entire input, counting
the number of times each symbol appears (its frequency of occurrence), and computing
the probability of occurrence of each symbol. The data is then input again, symbol by
symbol, and is compressed using the information in the probability model. (See also
Statistical Methods.)
One feature of arithmetic coding is that it is easy to separate the statistical model (the
table with frequencies and probabilities) from the encoding and decoding operations. It
is easy to encode, for example, the ﬁrst half of a data using one model, and the second
half using another model.
Move-to-Front Coding. The basic idea behind this method is to maintain the alphabet
A of symbols as a list where frequently occurring symbols are located near the front. A
symbol s is encoded as the number of symbols that precede it in this list. After symbol
s is encoded, it is moved to the front of list A.
Normal Distribution. A probability distribution with the well-known bell shape.
It
occurs often in theoretical models and real-life situations. The normal distribution with
mean m and standard deviation s is deﬁned by
f(x) =
1
s
√
2π exp

−1
2
x −m
s
2 
.
Patents. A mathematical algorithm can be patented if it is intimately associated with
software or ﬁrmware implementing it. Several compression methods, most notably LZW,
have been patented, creating diﬃculties for software developers who work with GIF,
UNIX compress, or any other system that uses LZW. (See also LZW.)
Pel.
The smallest unit of a facsimile image; a dot. (See also Pixel.)
Pixel. The smallest unit of a digital image; a dot. (See also Pel.)
PKZip. A compression program developed and implemented by Phil Katz for the old
MS/DOS operating system. Katz then founded the PKWare company which also mar-
kets the PKunzip, PKlite, and PKArc software (http://www.pkware.com).

278
Glossary
Prediction. Assigning probabilities to symbols.
Preﬁx Property. One of the principles of variable-length codes. It states that once a
certain bit pattern has been assigned as the code of a symbol, no other codes should start
with that pattern (the pattern cannot be the preﬁx of any other code). Once the string
1, for example, is assigned as the code of a1, no other codes should start with 1 (i.e.,
they all have to start with 0). Once 01, for example, is assigned as the code of a2, no
other codes can start with 01 (they all should start with 00). (See also Variable-Length
Codes, Statistical Methods.)
Psychoacoustic Model. A mathematical model of the sound-masking properties of the
human auditory (ear–brain) system.
Rice Codes. A special case of the Golomb code. (See also Golomb Codes.)
RLE. A general name for methods that compress data by replacing a run of identical
symbols with one code, or token, containing the symbol and the length of the run. RLE
sometimes serves as one step in a multistep statistical or dictionary-based method.
Scalar Quantization. The dictionary deﬁnition of the term “quantization” is “to restrict
a variable quantity to discrete values rather than to a continuous set of values.”
If
the data to be compressed is in the form of large numbers, quantization is used to
convert them to small numbers.
This results in (lossy) compression.
If the data to
be compressed is analog (e.g., a voltage that varies with time), quantization is used
to digitize it into small numbers. This aspect of quantization is used by several audio
compression methods. (See also Vector Quantization.)
Semiadaptive Compression.
A compression method that uses a two-pass algorithm,
where the ﬁrst pass reads the input to collect statistics on the data to be compressed, and
the second pass performs the actual compression. The statistics (model) are included in
the compressed ﬁle. (See also Adaptive Compression.)
Shannon–Fano Coding. An early algorithm for ﬁnding a minimum-length variable-size
code given the probabilities of all the symbols in the data.
This method was later
superseded by the Huﬀman method. (See also Statistical Methods, Huﬀman Coding.)
Shorten. A simple compression algorithm for waveform data in general and for speech
in particular (Section 6.5). Shorten employs linear prediction to compute residues (of
audio samples) which it encodes by means of Rice codes. (See also Rice Codes.)
Sliding-Window Compression. The LZ77 method (Section 1.3.1) uses part of the already-
seen input as the dictionary. The encoder maintains a window to the input ﬁle, and
shifts the input in that window from right to left as strings of symbols are being encoded.
The method is therefore based on a sliding window. (See also LZ Methods.)
Space-Filling Curves.
A space-ﬁlling curve is a function P(t) that goes through every
point in a given two-dimensional region, normally the unit square, as t varies from 0 to
1. Such curves are deﬁned recursively and are used in image compression.

Glossary
279
Statistical Methods. Statistical data compression methods work by assigning variable-
length codes to symbols in the data, with the shorter codes assigned to symbols or
groups of symbols that appear more often in the data (have a higher probability of
occurrence). (See also Variable-Length Codes, Preﬁx Property, Shannon–Fano Coding,
Huﬀman Coding, and Arithmetic Coding.)
Symbol.
The smallest unit of the data to be compressed. A symbol is often a byte but
may also be a bit, a trit {0, 1, 2}, or anything else. (See also Alphabet.)
Token. A unit of data written on the compressed ﬁle by some compression algorithms.
A token consists of several ﬁelds that may have either ﬁxed or variable sizes.
Transform. An image can be compressed by transforming its pixels (which are corre-
lated) to a representation where they are decorrelated. Compression is achieved if the
new values are smaller, on average, than the original ones. Lossy compression can be
achieved by quantizing the transformed values. The decoder inputs the transformed
values from the compressed ﬁle and reconstructs the (precise or approximate) original
data by applying the opposite transform. (See also Discrete Cosine Transform, Discrete
Wavelet Transform, Fourier Transform.)
Unary Code.
A simple variable-size code for the integers that can be constructed in
one step. The unary code of the nonnegative integer n is deﬁned (Section 1.1.1) as n−1
1’s followed by a single 0 (Table 1.2). There is also a general unary code. (See also
Golomb Code.)
Unicode.
A new international standard code, the Unicode, has been proposed, and is
being developed by the international Unicode organization (www.unicode.org). Uni-
code speciﬁes 16-bit codes for its characters, so it provides for 216 = 64K = 65,536
codes. (Notice that doubling the size of a code much more than doubles the number of
possible codes. In fact, it squares the number of codes.) Unicode includes all the ASCII
codes in addition to codes for characters in foreign languages (including complete sets of
Korean, Japanese, and Chinese characters) and many mathematical and other symbols.
Currently, about 39,000 out of the 65,536 possible codes have been assigned, so there is
room for adding more symbols in the future. (See also ASCII, Codes.)
Variable-Length Codes. These codes are employed by statistical methods. Most variable-
length codes are preﬁx codes (page 28) and should be assigned to symbols based on their
probabilities. (See also Preﬁx Property, Statistical Methods.)
Vector Quantization. Vector quantization is a generalization of the scalar quantization
method. It is used in both image and audio compression. In practice, vector quantization
is commonly used to compress data that has been digitized from an analog source, such
as sampled sound and scanned images (drawings or photographs). Such data is called
digitally sampled analog data (DSAD). (See also Scalar Quantization.)
Voronoi Diagrams. Imagine a petri dish ready for growing bacteria. Four bacteria of
diﬀerent types are simultaneously placed in it at diﬀerent points and immediately start
multiplying. We assume that their colonies grow at the same rate. Initially, each colony
consists of a growing circle around one of the starting points. After a while some of
them meet and stop growing in the meeting area due to lack of food. The ﬁnal result

280
Glossary
is that the entire dish gets divided into four areas, one around each of the four starting
points, such that all the points within area i are closer to starting point i than to any
other start point. Such areas are called Voronoi regions or Dirichlet tessellations.
Wavelets.
(See Discrete-Wavelet Transform.)
Zip. Popular software that implements the Deﬂate algorithm (Section 3.3) that uses
a variant of LZ77 combined with static Huﬀman coding. It uses a 32-Kb-long sliding
dictionary and a look-ahead buﬀer of 258 bytes. When a string is not found in the
dictionary, its ﬁrst symbol is emitted as a literal byte. (See also Deﬂate.)
Glossary (noun). An alphabetical list of technical
terms in some specialized ﬁeld of knowledge; usually
published as an appendix to a text on that ﬁeld.
—A typical dictionary deﬁnition

Solutions to Puzzles
Page 30. The diagram shows how easy it is. ⃝
⃝⃝⃝⃝
⃝⃝⃝⃝
⃝
⃝⃝
⃝⃝⃝
⃝⃝⃝
⃝⃝
Page 47. No, because each rectangle on the chess board covers one white and one black
square, but the two squares that we have removed have the same color.
Page 67. The next two integers are 28 and 102. The rule is simple but elusive. Start
with (almost) any positive 2-digit integer (we somewhat arbitrarily selected 38). Multi-
ply the two digits to obtain 3 × 8 = 24, then add 38 + 24 to generate the third integer
62. Now multiply 6 × 2 = 12 and add 62 + 12 = 74. Similar multiplication and addition
produce the next two integers 28 and 102.
Page 76. Just me. All the others were going in the opposite direction.
Page 98. Each win increases the mount in Mr Ambler’s pocket from m to 1.5m and
each loss reduces it from n to 0.5n. In order for him to win half the time, g, the number
of games, has to be even and we denote i = g/2. We start with the simple case g = 2,
where there are two possible game sequences, namely WL and LW. In the ﬁrst sequence,
the amount in Mr Ambler’s pocket varies from a to 3
2a to 1
2
3
2a and in the second sequence
it varies from a to 1
2a to 3
2
1
2a. It is now obvious that the amount left in his pocket after
i wins and i losses does not depend on the precise sequence of winnings and losses and
is always
# 1
2
$i # 3
2
$i a =
# 3
4
$i a. This amount is less than a, so Ambler’s chance of net
winning is zero.
Page 107.
The schedule of the London underground is such that a Brixton-bound
train always arrives at Oxford circus a minute or two after a train to Maida Vale has
departed. Anyone planning to complain to London Regional Transport, should do so at
http://www.tfl.gov.uk/home.aspx.
Page 110. The next integer is 3. The ﬁrst integer of each pair is random, and the
second one is the number of letters in the English name of the integer.
Page 132. Three socks.

282
Solutions To Puzzles
Page 151. When each is preceded by BREAK it has a diﬀerent meaning.
Page 179.
Consider the state of the game when there are ﬁve matches left.
The
player whose turn it is will lose, because regardless of the number of matches he removes
(between 1 and 4), his opponent will be able to remove the last match.
A similar
situation exists when there are 10 matches left because the player whose turn it is can
remove ﬁve and leave ﬁve matches. The same argument applies to the point in the
game when 15 matches are left. Thus, he who plays the ﬁrst move has a simple winning
strategy; remove two matches.
Page 209. This is easy. Draw two squares with a common side (ﬁve lines), and then
draw a diagonal of each square.
Page 231. When fully spelled in English, the only vowel they contain is E.
Page 238. The letters NIL can be made with six matches.
Page 253.
It is 265.
Each integer is the sum of two consecutive squares.
Thus,
12 + 22 = 5, 32 + 42 = 25, and so on.
Page 257. FLOUR, FLOOR, FLOOD, BLOOD, BROOD, BROAD, BREAD.
Who in the world am I? Ah, that’s the great puzzle.
—Lewis Carroll

Answers to Exercises
A bird does not sing because he has an answer, he sings because he has a song.
—Chinese Proverb
Intro.1.
When a software maker has a popular product it tends to come up with new
versions. Typically, a user downloads the update oﬀthe maker’s site anonymously. Over
time, the updates become bigger and may take too long to download. This is why such
updates should be supplied in compressed format. This is an example of data that is
compressed once, at the source, and is decompressed once by each user downloading it.
Obviously, the speed of encoder and decoder is uncritical, but eﬃcient compression is
important.
1.1.
We can assume that each image row starts with a white pixel. If a row starts with,
say, seven black pixels, we can prepare the run lengths 0, 7, . . . and the compressor will
simply ignore the run of zero white pixels.
1.2.
“Cook what the cat dragged in,” “my ears are turning,” “sad egg,” “a real hooker,”
“my brother’s beeper,” and “put all your eggs in one casket.”
1.3.
The following list summarizes the advantages and downsides of each approach:
Two-passes: slow, not online, may require memory to store result of the ﬁrst pass,
on the other hand it achieves best possible compression.
Training: sensitive to the choice of training documents (which may or may not
reﬂect the characteristics of the data being compressed).
Fast, because all the data
symbols have been assigned variable-length codes even before the encoder starts. General
purpose (this has been proved by the fax compression standard). Good performance if
the training data is statistically relevant.
Adaptive: adapts while reading and compressing the data, which is why it performs
poorly on small input ﬁles. Online, generally faster than the other two methods.

284
Answers to Exercises
1.4.
6,8,0,1,3,1,4,1,3,1,4,1,3,1,4,1,3,1,2,2,2,2,6,1,1. The ﬁrst two are the bitmap resolu-
tion (6×8). The original image occupies either 48 bits, so compression will be achieved
if the resulting 25 runs can be encoded in fewer than 48 bits. Like most compression
methods, RLE does not work well for small data ﬁles.
1.5.
RLE of images is based on the idea that adjacent pixels tend to be identical. The
last pixel of a row, however, has no reason to be identical to the ﬁrst pixel of the next
row.
1.6.
It is not necessary if the width of a row is known a priori. For example, if image
dimensions are contained in the header of the compressed image. If the eol is sent for the
ﬁrst row, there is no need to signal the end of a line again, since the decoder can infer
the line width after decoding the ﬁrst line and split the runs into lines by counting the
pixels. It is possible to signal the end of a scan line without using any special symbol.
The end of a line could be signalled by inserting two consecutive zeros. Since it is not
possible to have two consecutive runs of zero length, the decoder may interpret two
consecutive zeros as the end of a scan line.
1.7.
Method (b) has two advantages as follows:
1. The paragraph following Equation (5.6) shows that a block of DCT coeﬃcients
(Section 5.5) has one large element (the DC) at its top-left corner and smaller elements
(AC) elsewhere. The AC coeﬃcients generally become smaller as we move from the
top-left to the bottom-right corner and are largest on the top row and the leftmost
column. Thus, scanning a block of DCT coeﬃcients in zigzag order collects the large
elements ﬁrst. Notice that this order collects all the coeﬃcients of the top row and
leftmost column before it even starts collecting the elements in the bottom-right half of
the block. Thus, this scan order, which is used in JPEG, transforms a two-dimensional
block of DCT coeﬃcients into a one-dimensional sequence where numbers, especially
after quantization, become smaller and smaller, and include runs of zeros.
Such a
sequence is easy to compress with variable-length codes and RLE, which is why a zigzag
scan is a key to eﬀective lossy compression of images.
2. In a zigzag scan, every pair of pixels are near neighbors. When scanning by rows,
the last element of a row is not correlated with the ﬁrst element of the next row. Thus,
we have to make sure that each row has an even number of elements (otherwise, the
last column must be duplicated and temporarily appended to the image being scanned).
The same problem exists when scanning by columns or as shown in Figure 1.12c.
Method (b) also has its own downside. It is easy to see, from Figure 1.12 or any
similar grid, that the distance between diagonally-adjacent pixels is slightly larger than
the distance between near neighbors on the same row or on the same column. Thus, if the
distance between the centers of pixels on the same row is 1, then the distance between
the centers of neighbors on a diagonal is
√
2, about 41% greater. If we assume that
correlation is inversely proportional to distance, then a 41% greater distance translates
to 41% weaker correlation and therefore worse compression.
1.8.
Each of the ﬁrst four rows yields the eight runs 1,1,1,2,1,1,1,eol. Rows 6 and 8
yield the four runs 0,7,1,eol each. Rows 5 and 7 yield the two runs 8,eol each. The total
number of runs (including the eol’s) is therefore 44.

Answers to Exercises
285
When compressing by columns, columns 1, 3, and 6 yield the ﬁve runs 5,1,1,1,eol
each. Columns 2, 4, 5, and 7 yield the six runs 0,5,1,1,1,eol each. Column 8 gives 4,4,eol,
for a total of 42 runs. We conclude that this image is “balanced” with respect to rows
and columns.
1.9.
The straightforward answer is that the decoder doesn’t know but it does not need
to know. The decoder simply reads tokens and uses each oﬀset to locate a string of text
without having to know whether the string was a ﬁrst or a last match.
1.10.
Imagine a history book divided into chapters.
One chapter may commonly
employ words such as “Greek,” “Athens,” and “Troy,” while the following chapter may
have a preponderance of “Roman,” “empire,” and “legion.” In such a document, we
can expect to ﬁnd more matches in the newer (recent) part of the search buﬀer. Now
imagine a very long poem or hymn, organized in long stanzas, each followed by the
same chorus. When trying to match pieces of the current chorus, the best matches may
be found in the previous chorus, which may be located in the older part of the search
buﬀer. Thus, the distribution of matches depends heavily on the type of data that is
being compressed.
1.11.
The next step matches the space and encodes the string ␣e.
sir␣sid|␣eastman␣easily␣
⇒
(4,1,e)
sir␣sid␣e|astman␣easily␣te
⇒
(0,0,a)
and the next one matches nothing and encodes the a.
1.12.
Any two correlated pixels a and b are similar (this is what being correlated
means). Thus, if the pair (a, b) is considered a point, it will be located in the xy plane
on or near the 45◦line x = y. After the rotation, the point will end up on or near the x
axis, where it will have a small y coordinate, while its x coordinate will not change much
(Figure 5.4). We say that the rotation squeezes the range of one of the two dimensions
and slightly increases the range of the other.
1.13.
This transform, like anything else in life, does not come “free” and involves a
subtle cost. The original numbers were correlated, but the transform coeﬃcients are
not. The statistical cross-correlation of a set of pairs (xi, yi) is the sum 
i xiyi, and it
is easy to see that the cross-correlation of our ﬁve pairs has dropped from 1729.72 before
the transform to −23.0846 after it. A signiﬁcant reduction!
2.1.
Figure Ans.1a,b,c shows the three trees. The codes sizes for the trees are
(5 + 5 + 5 + 5·2 + 3·3 + 3·5 + 3·5 + 12)/30 = 76/30,
(5 + 5 + 4 + 4·2 + 4·3 + 3·5 + 3·5 + 12)/30 = 76/30,
(6 + 6 + 5 + 4·2 + 3·3 + 3·5 + 3·5 + 12)/30 = 76/30.

286
Answers to Exercises
(a)
A B
2
5
8
(b)
(c)
(d)
A B
2
A B
2
C
D
3
C
D
3
C
D
3
D
8
8
F
E
5
5
E
5
E
8
G
20
H
10
F
E
A B
2
3
C
G
10
F
G
10
F
G
10
H
30
30
18
H
30
18
H
30
18
Figure Ans.1: Three Huﬀman Trees for Eight Symbols.
2.2.
After adding symbols A, B, C, D, E, F, and G to the tree, we were left with
the three symbols ABEF (with probability 10/30), CDG (with probability 8/30), and
H (with probability 12/30). The two symbols with lowest probabilities were ABEF and
CDG, so they had to be merged. Instead, symbols CDG and H were merged, creating a
non-Huﬀman tree.
2.3.
The second row of Table 2.2 (due to Guy Blelloch) shows a symbol whose Huﬀman
code is three bits long, but for which ⌈−log2 0.3⌉= ⌈1.737⌉= 2.
2.4.
The explanation is simple. Imagine a large alphabet where all the symbols have
(about) the same probability. Since the alphabet is large, that probability will be small,
resulting in long codes. Imagine the other extreme case, where certain symbols have
high probabilities (and, therefore, short codes). Since the probabilities have to add up
to 1, the rest of the symbols will have low probabilities (and, therefore, long codes). We
therefore see that the size of a code depends on the probability, but is indirectly aﬀected
by the size of the alphabet.
2.5.
Figure Ans.2 shows Huﬀman codes for 5, 6, 7, and 8 symbols with equal proba-
bilities. In the case where n is a power of 2, the codes are simply the ﬁxed-sized ones.
In other cases the codes are very close to ﬁxed-size. This shows that symbols with equal
probabilities do not beneﬁt from variable-length codes. (This is another way of saying
that random text cannot be compressed.) Table Ans.3 shows the codes, their average
sizes and variances.
2.6.
It increases exponentially from 2s to 2s+n = 2s × 2n.
2.7.
The binary value of 127 is 01111111 and that of 128 is 10000000. Half the pixels in
each bitplane will therefore be 0 and the other half, 1. In the worst case, each bitplane
will be a checkerboard, i.e., will have many runs of size one. In such a case, each run
requires a 1-bit code, leading to one codebit per pixel per bitplane, or eight codebits per

Answers to Exercises
287
1
2
3
4
5
6
7
8
1
1
1
0
0
0
1
2
3
4
5
6
1
1
0
0
1
2
3
4
5
6
7
1
1
1
0
0
0
1
2
3
4
5
1
1
0
0
Figure Ans.2: Huﬀman Codes for Equal Probabilities.
Avg.
n
p
a1
a2
a3
a4
a5
a6
a7
a8
size
Var.
5
0.200
111
110
101
100
0
2.6
0.64
6
0.167
111
110
101
100
01
00
2.672
0.2227
7
0.143
111
110
101
100
011
010
00
2.86
0.1226
8
0.125
111
110
101
100
011
010
001
000
3
0
Table Ans.3: Huﬀman Codes for 5–8 Symbols.

288
Answers to Exercises
pixel for the entire image, resulting in no compression at all. In comparison, a Huﬀman
code for such an image requires just two codes (since there are just two pixel values) and
they can be one bit each. This leads to one codebit per pixel, or a compression factor
of eight.
2.8.
The two trees are shown in Figure 2.13c,d. The average code size for the binary
Huﬀman tree is
1×0.49 + 2×0.25 + 5×0.02 + 5×0.03 + 5×.04 + 5×0.04 + 3×0.12 = 2 bits/symbol,
and that of the ternary tree is
1×0.26 + 3×0.02 + 3×0.03 + 3×0.04 + 2×0.04 + 2×0.12 + 1×0.49 = 1.34 trits/symbol.
2.9.
A symbol with high frequency of occurrence should be assigned a shorter code.
Therefore, it has to appear high in the tree. The requirement that at each level the
frequencies be sorted from left to right is artiﬁcial. In principle, it is not necessary, but
it simpliﬁes the process of updating the tree.
2.10.
Figure Ans.4 shows the initial tree and how it is updated in the 11 steps (a)
through (k). Notice how the esc symbol gets assigned diﬀerent codes all the time, and
how the diﬀerent symbols move about in the tree and change their codes. Code 10, e.g.,
is the code of symbol “i” in steps (f) and (i), but is the code of “s” in steps (e) and (j).
The code of a blank space is 011 in step (h), but 00 in step (k).
The ﬁnal output is: “s0i00r100␣1010000d011101000”. A total of 5×8 + 22 = 62
bits. The compression ratio is thus 62/88 ≈0.7.
2.11.
Because a typical fax machine scans lines that are about 8.2 inches wide (≈
208 mm), so a blank scan line produces 1,664 consecutive white pels.
2.12.
These codes are needed for cases such as example 4, where the run length is 64,
128, or any length for which a make-up code has been assigned.
2.13.
There may be fax machines (now or in the future) built for wider paper, so the
Group 3 code was designed to accommodate them.
2.14.
Each scan line starts with a white pel, so when the decoder inputs the next code
it knows whether it is for a run of white or black pels. This is why the codes of Table 2.22
have to satisfy the preﬁx property in each column but not between the columns.
2.15.
Imagine a scan line where all runs have length one (strictly alternating pels).
It’s easy to see that this case results in expansion. The code of a run length of one white
pel is 000111, and that of one black pel is 010. Two consecutive pels of diﬀerent colors
are thus coded into nine bits. Since the uncoded data requires just two bits (01 or 10),
the compression ratio is 9/2 = 4.5 (the compressed data is 4.5 times bigger than the
original data; a signiﬁcant expansion).

Answers to Exercises
289
Initial tree
(a). Input: s. Output: ‘s’.
esc s1
(b). Input: i. Output: 0‘i’.
esc i1 1 s1
esc
0
1
s1
esc
0
s1
0
i1
esc
0
1
1
1
(c). Input: r. Output: 00‘r’.
esc r1 1 i1 2 s1 →
esc r1 1 i1 s1 2
s1
0
i1
0
1
1
2
1
r1
esc
0
1
s1
0
i1
0
1
1
2
1
r1
esc
0
1
(d). Input: ␣. Output: 100‘␣’.
esc ␣1 1 r1 2 i1 s1 3 →
esc ␣1 1 r1 s1 i1 2 2
␣1
s1
0
i1
0
1
1
3
1
r1
0
2
esc
0
1
1
␣1
s1
0
i1
0
1
1
2
1
r1
0
2
esc
0
1
1
Figure Ans.4: Exercise 2.10. Part I.

290
Answers to Exercises
␣1
s2
0
i1
0
1
1
3
1
r1
0
2
esc
0
1
1
s2
␣1
0
i1
0
1
1
3
1
r1
0
2
esc
0
1
1
(e). Input: s. Output: 10.
esc ␣1 1 r1 s2 i1 2 3 →
esc ␣1 1 r1 i1 s2 2 3
s2
␣1
0
i2
0
1
1
4
1
r1
0
2
esc
0
1
1
(f). Input: i. Output: 10.
esc ␣1 1 r1 i2 s2 2 4
s2
␣1
0
i2
0
1
1
4
1
r1
0
3
0
2
1
d1
esc
0
1
s2
␣1
0
i2
0
1
1
4
1
r1
0
3
0
2
1
d1
esc
0
1
(g). Input: d. Output: 000‘d’.
esc d1 1 ␣1 2 r1 i2 s2 3 4 →
esc d1 1 ␣1 r1 2 i2 s2 3 4
Figure Ans.4: Exercise 2.10. Part II.

Answers to Exercises
291
s2
␣2
0
i2
0
1
1
4
1
r1
0
4
0
2
1
d1
esc
0
1
1
s2
␣2
0
i2
0
1
1
4
1
r1
0
4
0
3
1
d1
esc
0
1
1
(h). Input: ␣. Output: 011.
esc d1 1 ␣2 r1 3 i2 s2 4 4 →
esc d1 1 r1 ␣2 2 i2 s2 4 4
s2
␣2
0
i3
0
1
1
5
1
r1
0
4
0
2
1
d1
esc
0
1
1
s2
␣2
0
i3
0
1
1
5
1
r1
0
4
0
2
1
d1
esc
0
1
1
(i). Input: i. Output: 10.
esc d1 1 r1 ␣2 2 i3 s2 4 5 →
esc d1 1 r1 ␣2 2 s2 i3 4 5
Figure Ans.4: Exercise 2.10. Part III.

292
Answers to Exercises
s3
␣2
0
i3
0
1
1
6
1
r1
0
4
0
2
1
d1
esc
0
1
1
(j). Input: s. Output: 10.
esc d1 1 r1 ␣2 2 s3 i3 4 6
s3
␣3
0
i3
0
1
1
6
1
r1
0
5
0
2
1
d1
esc
0
1
1
s3
␣3
0
i3
0
1
1
6
1
r1
0
5
0
2
1
d1
esc
0
1
1
(k). Input: ␣. Output: 00.
esc d1 1 r1 ␣3 2 s3 i3 5 6 →
esc d1 1 r1 2 ␣3 s3 i3 5 6
Figure Ans.4: Exercise 2.10. Part IV.

Answers to Exercises
293
3.1.
(1) The size of the output is N[48 −28P] = N[48 −25.2] = 22.8N. The size
of the input is, as before, 40N. The compression factor is therefore 40/22.8 ≈1.75.
(2) Maximum compression is obtained when the output size N[48 −28P], is minimum,
which occurs when P = 1. The compression factor in this case is 40N/N[48 −28P] = 2.
3.2.
This is straightforward. The remaining steps are listed in Table Ans.5
Dictionary
Token
Dictionary
Token
15
␣t
(4, t)
21
␣si
(19,i)
16
e
(0, e)
22
c
(0, c)
17
as
(8, s)
23
k
(0, k)
18
es
(16,s)
24
␣se
(19,e)
19
␣s
(4, s)
25
al
(8, l)
20
ea
(4, a)
26
s(eof)
(1, (eof))
Table Ans.5: Next 12 Encoding Steps in the LZ78 Example.
3.3.
Table Ans.6 summarizes the steps. The output emitted by the encoder is
97 (a), 108 (l), 102 (f), 32 (␣), 101 (e), 97 (a), 116 (t), 115 (s), 32 (␣), 256 (al), 102
(f), 265 (alf), 97 (a),
and the following new entries are added to the dictionary
(256: al), (257: lf), (258: f ), (259: ␣e), (260: ea), (261: at), (262: ts),
(263: s ), (264: ␣a), (265: alf), (266: fa), (267: alfa).
In
New
In
New
I
dict?
entry
Output
I
dict?
entry
Output
a
Y
s
N
263-s
115 (s)
al
N
256-al
97 (a)
␣
Y
l
Y
␣a
N
264-␣a
32 (␣)
lf
N
257-lf
108 (l)
a
Y
f
Y
al
Y
f
N
258-f
102 (f)
alf
N
265-alf
256 (al)
␣
Y
f
Y
␣e
N
259-␣e
32 (w)
fa
N
266-fa
102 (f)
e
Y
a
Y
ea
N
260-ea
101 (e)
al
Y
a
Y
alf
Y
at
N
261-at
97 (a)
alfa
N
267-alfa
265 (alf)
t
Y
a
Y
ts
N
262-ts
116 (t)
a,eof
N
97 (a)
s
Y
Table Ans.6: LZW Encoding of alf eats alfalfa.

294
Answers to Exercises
3.4.
The encoder inputs the ﬁrst a into I, searches and ﬁnds a in the dictionary. It
inputs the next a but ﬁnds that Ix, which is now aa, is not in the dictionary. The
encoder thus adds string aa to the dictionary as entry 256 and outputs the token 97 (a).
Variable I is initialized to the second a. The third a is input, so Ix is the string aa, which
is now in the dictionary. I becomes this string, and the fourth a is input. Ix is now aaa
which is not in the dictionary. The encoder thus adds string aaa to the dictionary as
entry 257 and outputs 256 (aa). I is initialized to the fourth a. Continuing this process
is straightforward.
The result is that strings aa, aaa, aaaa, . . . are added to the dictionary as entries
256, 257, 258, . . . , and the output is
97 (a), 256 (aa), 257 (aaa), 258 (aaaa), . . .
The output consists of pointers pointing to longer and longer strings of a’s. The ﬁrst k
pointers thus point at strings whose total length is 1 + 2 + · · · + k = (k + k2)/2.
Assuming input data that consists of one million a’s, we can ﬁnd the size of the
compressed output stream by solving the quadratic equation (k + k2)/2 = 1,000,000
for the unknown k. The solution is k ≈1,414. The original, 8-million bit input is thus
compressed into 1,414 pointers, each at least 9-bit (and in practice, probably 16-bit) long.
The compression factor is thus either 8M/(1414×9) ≈628.6 or 8M/(1414×16) ≈353.6.
This is an impressive result but such input ﬁles are rare (notice that this particular
input can best be compressed by generating an output ﬁle containing just “1,000,000
a”, and without using LZW).
3.5.
We simply follow the decoding steps described in the text. The results are:
1. Input 97. This is in the dictionary so set I=a and output a. String ax needs to be
saved in the dictionary but x is still unknown.
2. Input 108. This is in the dictionary so set J=l and output l. Save al in entry 256.
Set I=l.
3. Input 102. This is in the dictionary so set J=f and output f. Save lf in entry 257.
Set I=f.
4. Input 32. This is in the dictionary so set J=␣and output ␣. Save f
in entry 258.
Set I=␣.
5. Input 101. This is in the dictionary so set J=e and output e. Save ␣e in entry 259.
Set I=e.
6. Input 97. This is in the dictionary so set J=a and output a. Save ea in entry 260.
Set I=a.
7. Input 116. This is in the dictionary so set J=t and output t. Save at in entry 261.
Set I=t.
8. Input 115. This is in the dictionary so set J=s and output s. Save ts in entry 262.
Set I=t.
9. Input 32. This is in the dictionary so set J=␣and output ␣. Save s
in entry 263.
Set I=␣.
10. Input 256. This is in the dictionary so set J=al and output al. Save ␣a in entry
264. Set I=al.
11. Input 102. This is in the dictionary so set J=f and output f. Save alf in entry
265. Set I=f.

Answers to Exercises
295
12. Input 265. This has just been saved in the dictionary so set J=alf and output alf.
Save fa in dictionary entry 266. Set I=alf.
13. Input 97. This is in the dictionary so set J=a and output a. Save alfa in entry 267
(even though it will never be used). Set I=a.
14. Read eof. Stop.
3.6.
Assume that the dictionary is initialized to just the two entries (1: a) and (2: b).
The encoder outputs
1 (a), 2 (b), 3 (ab), 5 (aba), 4 (ba), 7 (bab), 6 (abab), 9 (ababa), 8 (baba), . . .
and adds the new entries (3: ab), (4: ba), (5: aba), (6: abab), (7: bab), (8: baba), (9:
ababa), (10: ababab), (11: babab), . . . to the dictionary. This regular behavior can be
analyzed and the kth output pointer and dictionary entry predicted, but the eﬀort is
probably not worth it.
3.7.
The answer to Exercise 3.4 shows the relation between the size of the compressed
ﬁle and the size of the largest dictionary string for the “worst case” situation (input that
creates the longest strings). For a 1 Mbyte input ﬁle, there will be 1,414 strings in the
dictionary, the largest of which is 1,414 symbols long.
3.8.
A simple alternative it to store the 256 8-bit values in the ﬁrst (rightmost) 256
bytes of the sliding window and keep these locations ﬁxed whenever the window is
shifted.
3.9.
No. Besides the slowdown caused by evaluating a much larger number of matching
candidates, sending the oﬀset of a match is expensive and matches of length shorter than
3 would probably increase the size of the compressed ﬁle.
4.1.
Table Ans.7 shows the steps of encoding the string a2a2a2a2. Because of the high
probability of a2 the low and high variables start at very diﬀerent values and approach
each other slowly.
a2
0.0 + (1.0 −0.0) × 0.023162=0.023162
0.0 + (1.0 −0.0) × 0.998162=0.998162
a2
0.023162 + 0.975 × 0.023162=0.04574495
0.023162 + 0.975 × 0.998162=0.99636995
a2
0.04574495 + 0.950625 × 0.023162=0.06776322625
0.04574495 + 0.950625 × 0.998162=0.99462270125
a2
0.06776322625 + 0.926859375 × 0.023162=0.08923124309375
0.06776322625 + 0.926859375 × 0.998162=0.99291913371875
Table Ans.7: Encoding the String a2a2a2a2.
4.2.
It can be written either as 0.1000. . . or 0.0111. . . .
4.3.
In practice, the eof symbol has to be included in the original table of frequencies
and probabilities. This symbol is the last to be encoded, and the decoder stops when it
detects an eof.

296
Answers to Exercises
4.4.
The encoding steps are simple (see ﬁrst example on page 124). We start with the
interval [0, 1). The ﬁrst symbol a2 reduces the interval to [0.4, 0.9). The second one,
to [0.6, 0.85), the third one to [0.7, 0.825) and the eof symbol to [0.8125, 0.8250). The
approximate binary values of the last interval are 0.1101000000 and 0.1101001100, so
we select the 7-bit number 1101000 as our code.
The probability of a2a2a2eof is (0.5)3×0.1 = 0.0125, but since −log2 0.0125 ≈6.322
it follows that the practical minimum code size is 7 bits.
5.1.
An image with no redundancy is not always random. The deﬁnition of redun-
dancy tells us that an image where each color appears with the same frequency has no
redundancy (statistically) yet it is not necessarily random and may even be interesting
and/or useful.
5.2.
The results are shown in Table Ans.8 together with the Matlab code used to
calculate it.
43210
Gray
43210
Gray
43210
Gray
43210
Gray
00000
00000
01000 01100
10000 11000
11000 10100
00001 00001
01001
01101
10001
11001
11001
10101
00010 00011
01010 01111
10010
11011
11010 10111
00011 00010
01011
01110
10011
11010
11011
10110
00100 00110
01100 01010
10100 11110
11100 10010
00101 00111
01101
01011
10101
11111
11101
10011
00110 00101
01110 01001
10110
11101
11110 10001
00111 00100
01111
01000
10111
11100
11111
10000
Table Ans.8: First 32 Binary and Gray Codes.
a=linspace(0,31,32); b=bitshift(a,-1);
b=bitxor(a,b); dec2bin(b)
Code for Table Ans.8.
5.3.
The code of Figure Ans.9 yields the coordinates of the rotated points
(7.071, 0), (9.19, 0.7071), (17.9, 0.78), (33.9, 1.41), (43.13, −2.12)
(notice how all the y coordinates are small numbers) and shows that the cross-correlation
drops from 1729.72 before the rotation to −23.0846 after it. A signiﬁcant reduction!
5.4.
Numerical precision aside, the resulting quantization errors (Table Ans.10) com-
puted in (1) and (2) should be the same (approximately 0.011 for the ﬁnest quantization,
1.1778 for the second best, and 1.244 for the coarsest). This feature is a useful conse-
quence of the linearity of the IDCT and it can be used to estimate the quantization error
in the quantized data without having to perform an IDCT.

Answers to Exercises
297
p={{5,5},{6, 7},{12.1,13.2},{23,25},{32,29}};
rot={{0.7071,-0.7071},{0.7071,0.7071}};
Sum[p[[i,1]]p[[i,2]], {i,5}]
q=p.rot
Sum[q[[i,1]]q[[i,2]], {i,5}]
Figure Ans.9: Code for Rotating Five Points.
Items
12.000000 10.000000
8.000000 10.000000 12.000000 10.000000
8.000000 11.000000
DCT
28.637500
0.571202
0.461940
1.757000
3.181960
-1.729560
0.191342
-0.308709
Q1
28.600000
0.600000
0.500000
1.800000
3.200000
-1.800000
0.200000
-0.300000
Q2
28.000000
1.000000
1.000000
2.000000
3.000000
-2.000000
0.000000
0.000000
Q3
28.000000
0.000000
0.000000
2.000000
3.000000
-2.000000
0.000000
0.000000
E1
0.0014062 0.0008293 0.0014486 0.0018490 0.0003254 0.0049618 0.0000750 0.0000758 0.0109712
E2
0.4064062 0.1838677 0.2895086 0.0590490 0.0331094 0.0731378 0.0366118 0.0953012 1.1769918
E3
0.4064062 0.3262717 0.2133886 0.0590490 0.0331094 0.0731378 0.0366118 0.0953012 1.2432758
IDCT1 12.025400 10.023300
7.960540
9.930970 12.016400
9.993210
7.943540 10.998900
IDCT2 12.188300 10.231500
7.749310
9.208630 11.787600
9.545490
7.828650 10.655700
IDCT3 11.236000
9.624430
7.662860
9.573020 12.347100 10.014600
8.053040 10.684200
E1
0.0006452 0.0005429 0.0015571 0.0047651 0.0002690 0.0000461 0.0031877 0.0000012 0.0110143
E2
0.0354569 0.0535923 0.0628455 0.6262665 0.0451138 0.2065793 0.0293608 0.1185425 1.1777575
E3
0.5836960 0.1410528 0.1136634 0.1823119 0.1204784 0.0002132 0.0028132 0.0997296 1.2439586
Table Ans.10: Results of a DCT Quantization Error Experiment.
5.5.
The Mathematica code of Figure 5.7 produces the eight coeﬃcients 140, −71, 0,
−7, 0, −2, 0, and 0. We now quantize this set coarsely by clearing the last two nonzero
weights −7 and −2, When the IDCT is applied to the sequence 140, −71, 0, 0, 0, 0, 0,
0, it produces 15, 20, 30, 43, 56, 69, 79, and 84. These are not identical to the original
values, but the maximum diﬀerence is only 4; an excellent result considering that only
two of the eight DCT coeﬃcients are nonzero.
5.6.
The eight values in the top row are very similar (the diﬀerences between them
are either 2 or 3). Each of the other rows is obtained as a right-circular shift of the
preceding row.
5.7.
It is obvious that such a block can be represented as a linear combination of the
patterns in the leftmost column of Figure 5.24. The actual calculation yields the eight
weights 4, 0.72, 0, 0.85, 0, 1.27, 0, and 3.62 for the patterns of this column. The other
56 weights are zero or very close to zero.
5.8.
First ﬁgure out the zigzag path manually, then record it in an array zz of struc-
tures, where each structure contains a pair of coordinates for the path as shown, e.g., in
Figure Ans.11.
If the two components of a structure are zz.r and zz.c, then the zigzag traversal
can be done by a loop of the form:
for (i=0; i<64; i++){

298
Answers to Exercises
(0,0) (0,1) (1,0) (2,0) (1,1) (0,2) (0,3) (1,2)
(2,1) (3,0) (4,0) (3,1) (2,2) (1,3) (0,4) (0,5)
(1,4) (2,3) (3,2) (4,1) (5,0) (6,0) (5,1) (4,2)
(3,3) (2,4) (1,5) (0,6) (0,7) (1,6) (2,5) (3,4)
(4,3) (5,2) (6,1) (7,0) (7,1) (6,2) (5,3) (4,4)
(3,5) (2,6) (1,7) (2,7) (3,6) (4,5) (5,4) (6,3)
(7,2) (7,3) (6,4) (5,5) (4,6) (3,7) (4,7) (5,6)
(6,5) (7,4) (7,5) (6,6) (5,7) (6,7) (7,6) (7,7)
Figure Ans.11: Coordinates for the Zigzag Path.
row:=zz[i].r; col:=zz[i].c
...data_unit[row][col]...}
5.9.
The third DC diﬀerence, 5, is located in row 3 column 5, so it is encoded as
1110|101.
5.10.
Thirteen consecutive zeros precede this coeﬃcient, so Z = 13. The coeﬃcient
itself is found in Table 5.32 in row 1, column 0, so R = 1 and C = 0. Assuming that
the Huﬀman code in position (R, Z) = (1, 13) of Table 5.33 is 1110101, the ﬁnal code
emitted for 1 is 1110101|0.
5.11.
Figure Ans.12a shows a simple, 8×8 image with one diagonal line above the
main diagonal. Figure Ans.12b,c shows the ﬁrst two steps in its pyramid decomposition.
It is obvious that the transform coeﬃcients in the bottom-right subband (HH) indicate
a diagonal artifact located above the main diagonal. It is also easy to see that subband
LL is a low-resolution version of the original image.
12 16 12 12 12 12 12 12
12 12 16 12 12 12 12 12
12 12 12 16 12 12 12 12
12 12 12 12 16 12 12 12
12 12 12 12 12 16 12 12
12 12 12 12 12 12 16 12
12 12 12 12 12 12 12 16
12 12 12 12 12 12 12 12
14 12 12 12 4 0 0 0
12 14 12 12 0 4 0 0
12 14 12 12 0 4 0 0
12 12 14 12 0 0 4 0
12 12 14 12 0 0 4 0
12 12 12 14 0 0 0 4
12 12 12 14 0 0 0 4
12 12 12 12 0 0 0 0
13 13 12 12 2 2 0 0
12 13 13 12 0 2 2 0
12 12 13 13 0 0 2 2
12 12 12 13 0 0 0 2
2
2
0
0 4 4 0 0
0
2
2
0 0 4 4 0
0
0
2
2 0 0 4 4
0
0
0
2 0 0 0 4
(a)
(b)
(c)
Figure Ans.12: The Subband Decomposition of a Diagonal Line.
5.12.
The average can easily be calculated.
It turns out to be 131.375, which is
exactly 1/8 of 1051. The reason the top-left transform coeﬃcient is eight times the
average is that the Matlab code that did the calculations uses
√
2 instead of 2 (see
function individ(n) in Figure 5.51).

Answers to Exercises
299
5.13.
Figure Ans.13a–c shows the results of reconstruction from 3277, 1639, and 820
coeﬃcients, respectively. Despite the heavy loss of wavelet coeﬃcients, only a very small
loss of image quality is noticeable. The number of wavelet coeﬃcients is, of course, the
same as the image resolution 128×128 = 16,384. Using 820 out of 16,384 coeﬃcients
corresponds to discarding 95% of the smallest of the transform coeﬃcients (notice, how-
ever, that some of the coeﬃcients were originally zero, so the actual loss may amount
to less than 95%).
5.14.
The Matlab code of Figure Ans.14 calculates W as the product of the three
matrices A1, A2, and A3 and computes the 8×8 matrix of transform coeﬃcients. Notice
that the top-left value 131.375 is the average of all the 64 image pixels.
clear
a1=[1/2 1/2 0 0 0 0 0 0; 0 0 1/2 1/2 0 0 0 0;
0 0 0 0 1/2 1/2 0 0; 0 0 0 0 0 0 1/2 1/2;
1/2 -1/2 0 0 0 0 0 0; 0 0 1/2 -1/2 0 0 0 0;
0 0 0 0 1/2 -1/2 0 0; 0 0 0 0 0 0 1/2 -1/2];
% a1*[255; 224; 192; 159; 127; 95; 63; 32];
a2=[1/2 1/2 0 0 0 0 0 0; 0 0 1/2 1/2 0 0 0 0;
1/2 -1/2 0 0 0 0 0 0; 0 0 1/2 -1/2 0 0 0 0;
0 0 0 0 1 0 0 0; 0 0 0 0 0 1 0 0;
0 0 0 0 0 0 1 0; 0 0 0 0 0 0 0 1];
a3=[1/2 1/2 0 0 0 0 0 0; 1/2 -1/2 0 0 0 0 0 0;
0 0 1 0 0 0 0 0; 0 0 0 1 0 0 0 0;
0 0 0 0 1 0 0 0; 0 0 0 0 0 1 0 0;
0 0 0 0 0 0 1 0; 0 0 0 0 0 0 0 1];
w=a3*a2*a1;
dim=8; fid=fopen(’8x8’,’r’);
img=fread(fid,[dim,dim])’; fclose(fid);
w*img*w’ % Result of the transform
131.375
4.250
−7.875
−0.125
−0.25
−15.5
0
−0.25
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
12.000
59.875
39.875
31.875
15.75
32.0
16
15.75
12.000
59.875
39.875
31.875
15.75
32.0
16
15.75
12.000
59.875
39.875
31.875
15.75
32.0
16
15.75
12.000
59.875
39.875
31.875
15.75
32.0
16
15.75
Figure Ans.14: Code and Results for the Calculation of Matrix W and Transform W ·I·W T .
5.15.
Figure Ans.15 lists the Matlab code of the inverse wavelet transform function
iwt1(wc,coarse,filter) and a test.
5.16.
The simple equation 10×220×8 = (500x)×(500x)×8 is solved to yield x2 = 40
square inches. If the card is square, it is approximately 6.32 inches on a side. Such

300
Answers to Exercises
Figure Ans.13: Three Lossy Reconstructions of the 128×128 Lena Image.

Answers to Exercises
301
function dat=iwt1(wc,coarse,filter)
% Inverse Discrete Wavelet Transform
dat=wc(1:2^coarse);
n=length(wc); j=log2(n);
for i=coarse:j-1
dat=ILoPass(dat,filter)+ ...
IHiPass(wc((2^(i)+1):(2^(i+1))),filter);
end
function f=ILoPass(dt,filter)
f=iconv(filter,AltrntZro(dt));
function f=IHiPass(dt,filter)
f=aconv(mirror(filter),rshift(AltrntZro(dt)));
function sgn=mirror(filt)
% return filter coefficients with alternating signs
sgn=-((-1).^(1:length(filt))).*filt;
function f=AltrntZro(dt)
% returns a vector of length 2*n with zeros
% placed between consecutive values
n =length(dt)*2; f =zeros(1,n);
f(1:2:(n-1))=dt;
A simple test of iwt1 is
n=16; t=(1:n)./n;
dat=sin(2*pi*t)
filt=[0.4830 0.8365 0.2241 -0.1294];
wc=fwt1(dat,1,filt)
rec=iwt1(wc,1,filt)
Figure Ans.15: Code for the 1D Inverse Discrete Wavelet Transform.
a card has 10 rolled impressions (about 1.5 × 1.5 each), two plain impressions of the
thumbs (about 0.875×1.875 each), and simultaneous impressions of both hands (about
3.125×1.875 each). All the dimensions are in inches.
6.1.
An average book may have 60 characters per line, 45 lines per page, and 400
pages. This comes to 60×45×400 = 1,080,000 characters, requiring one byte of storage
each.
6.2.
Audio samples are normally 16-bit unsigned integers, so they are in the interval
[1, 216 −1]. The residuals tend to be small numbers, but can also be large. In principle,
a residual can be as large as 216 and as small as −216. Thus, enough variable-length
codes are needed to encode all the residuals. The ﬁrst audio sample of an input ﬁle is
not subtracted from anything but is encoded with the same variable-length code used
to encode the residuals and such codes exist even for the largest samples because they
are needed for the residuals.

302
Answers to Exercises
6.3.
Such an experiment should be repeated with several persons, preferably of diﬀerent
ages. The person should be placed in a sound insulated chamber, and a pure tone of
frequency f should be played. The amplitude of the tone should be gradually increased
from zero until the person can just barely hear it. If this happens at a decibel value
d, point (d, f) should be plotted. This should be repeated for many frequencies until a
graph similar to Figure 6.2a is obtained.
6.4.
We ﬁrst select identical items. If all s(t −i) equal s, Equation (6.5) yields the
same s. Next, we select values on a straight line. Given the four values a, a + 2, a + 4,
and a + 6, Equation (6.5) yields a + 8, the next linear value. Finally, we select values
roughly equally-spaced on a circle. The y coordinates of points on the ﬁrst quadrant of a
circle can be computed by y =
√
r2 −x2. We select the four points with x coordinates 0,
0.08r, 0.16r, and 0.24r, compute their y coordinates for r = 10, and substitute them in
Equation (6.5). The result is 9.96926, compared to the actual y coordinate for x = 0.32r
which is

r2 −(0.32r)2 = 9.47418, a diﬀerence of about 5%. The code that did the
computations is shown in Figure Ans.16.
(* Points on a circle. Used in exercise to check
4th-order prediction in FLAC *)
r = 10;
ci[x_] := Sqrt[100 - x^2];
ci[0.32r]
4ci[0] - 6ci[0.08r] + 4ci[0.16r] - ci[0.24r]
Figure Ans.16: Code for Checking Fourth-Order Prediction.
7.1.
Because the original string S can be reconstructed from L but not from F.
7.2.
A direct application of Equation (7.1) eight more times produces:
S[10-1-2]=L[T2[I]]=L[T[T1[I]]]=L[T[7]]=L[6]=i;
S[10-1-3]=L[T3[I]]=L[T[T2[I]]]=L[T[6]]=L[2]=m;
S[10-1-4]=L[T4[I]]=L[T[T3[I]]]=L[T[2]]=L[3]=␣;
S[10-1-5]=L[T5[I]]=L[T[T4[I]]]=L[T[3]]=L[0]=s;
S[10-1-6]=L[T6[I]]=L[T[T5[I]]]=L[T[0]]=L[4]=s;
S[10-1-7]=L[T7[I]]=L[T[T6[I]]]=L[T[4]]=L[5]=i;
S[10-1-8]=L[T8[I]]=L[T[T7[I]]]=L[T[5]]=L[1]=w;
S[10-1-9]=L[T9[I]]=L[T[T8[I]]]=L[T[1]]=L[9]=s;
The original string swiss␣miss is indeed reproduced in S from right to left.
7.3.
Figure Ans.17 shows the rotations of S and the sorted matrix. The last column, L
of Ans.17b happens to be identical to S, so S=L=sssssssssh. Since A=(s,h), a move-
to-front compression of L yields C = (1, 0, 0, 0, 0, 0, 0, 0, 0, 1). Since C contains just the
two values 0 and 1, they can serve as their own Huﬀman codes, so the ﬁnal result is
1000000001, 1 bit per character!

Answers to Exercises
303
sssssssssh
sssssssshs
ssssssshss
sssssshsss
ssssshssss
sssshsssss
ssshssssss
sshsssssss
shssssssss
hsssssssss
hsssssssss
shssssssss
sshsssssss
ssshssssss
sssshsssss
ssssshssss
sssssshsss
ssssssshss
sssssssshs
sssssssssh
(a)
(b)
Figure Ans.17: Permutations of “sssssssssh”.
7.4.
The encoder starts at T[0], which contains 5. The ﬁrst element of L is thus the
last symbol of permutation 5. This permutation starts at position 5 of S, so its last
element is in position 4. The encoder thus has to go through symbols S[T[i −1]] for
i = 0, . . . , n−1, where the notation i−1 should be interpreted cyclically (i.e., 0−1 should
be n−1). As each symbol S[T[i−1]] is found, it is compressed using move-to-front. The
value of I is the position where T contains 0. In our example, T[8]=0, so I=8.
I don’t have any solution, but I certainly admire the problem.
—Ashleigh Brilliant

Index
The index caters to those who have already read the book and want to locate a familiar
item, as well as to those new to the book who are looking for a particular topic. I have
included any terms that may occur to a reader interested in any of the topics discussed
in the book (even topics that are just mentioned in passing). As a result, even a quick
glance over the index gives the reader an idea of the terms and topics included in the
book. Notice that the index items “data compression” and “image compression” are
very general.
I have attempted to make the index items as complete as possible, including middle
names and dates.
Any errors and omissions brought to my attention are welcome.
They will be added to the errata list and will be included in any future editions.
2-pass compression, 14, 26, 57, 76, 90, 278
A-law companding, 238–244
adaptive arithmetic coding, 137–140
adaptive compression, 14, 27, 58
adaptive Huﬀman coding, 76–83, 271
Adler, Mark (1959–), 108
algorithmic encoder, 16
alphabet (deﬁnition of), 10, 271
analog data, 278
Aristotle (384–322) b.c., 115
arithmetic coding, 123–141, 271
adaptive, 137–140
in JPEG, 181, 191
QM coder, 181
ASCII, 271, 273
asymmetric compression, 15, 16, 49, 55
audio compression, 15, 227–245
µ-law, 238–244
A-law, 238–244
frequency masking, 233–234
temporal masking, 233–235
background pixel (white), 272
Bain, Alexander (1811–1877), 83
Bakewell, Frederick Collier, 83
Bandura, Albert (1925–), xiii
Bark (unit of critical band rate), 234, 272
Barkhausen, Heinrich Georg (1881–1956),
234, 272
and critical bands, 234
Baudot code, 151
Baudot, Jean Maurice Emile (1845–1903),
151
Belin, Edouard (1876–1963), 84
Bell, Alexander Graham (1847–1922), 83, 84
Benedetto, John J., 201
beta code, 29, 31
bi-level image, 65, 143, 272
bijection, 29

306
Index
Billings, Josh (1818–1885), 117
bitplane, 272
bitplane (deﬁnition of), 143
bitrate (deﬁnition of), 16, 272
bits/char (bpc), 16, 272
bits/symbol, 272
bitstream (deﬁnition of), 14
Blake, William (1757–1827), ix
Blelloch, Guy, 65
block coding, 272
block mode, 16
BOCU-1 (Unicode compression), ix, 121,
247, 262–263
bpb (bit per bit), 16
bpc (bits per character), 16, 272
bpp (bits per pixel), 17
Bradshaw, Naomi, 91
Braille code (as example of compression), 2
Braille, Louis (1809–1852), 2
Brett, Catherine, ix
Brislawn, Christopher M., 220
Burrows–Wheeler method, ix, 16, 121,
247–252, 263, 264, 272
canonical Huﬀman codes, 75–76, 113
Cantor, Georg Ferdinand Ludwig Philipp
(1845–1918), 135
Carroll, Lewis (1832–1898), 189, 257, 282
cartoon-like image, 144
CCITT, 85, 272
Cervantes, Miguel Saavedra de (1547–1616),
v
Chaitin, Gregory John (1947–), 136
Chambord (Chˆateau, 1526), 178
channel coding, viii, 10
Christie Mallowan, Dame Agatha Mary
Clarissa (Miller 1890–1976), 83
chromaticity diagram, 184
CIE, 184, 272
color diagram, 184
circular queue, 273
Clancy, Thomas Leo (1947–), 225
codec, 14, 273
codes
ASCII, 273
deﬁnition of, 273
EBCDIC, 273
phased-in binary, 78
Rice, 278
Unicode, 273
variable-length
unambiguous, 39, 276
codes for the integers, 28–38
Fibonacci, 253
color
cool, 185
warm, 185
color images (and grayscale compression),
color space, 184
companding, 14
companding (audio compression), 229–231
compression factor, 17, 273
compression gain, 17, 273
compression performance measures, 16–17
compression ratio, 16, 273
compressor (deﬁnition of), 14
continuous-tone image, 143, 144, 273
cool colors, 185
correlation between pixels, 8, 22, 65
da Vinci, Leonardo (1452–1519), 178
data compression (approaches to), 21–58
data hiding (steganography), viii
data structures, ix, 81, 273
Daubechies, Ingrid (1954–), 199
decoder, 274
deﬁnition of, 14
deterministic, 16
decompressor (deﬁnition of), 14
decorrelated pixels, 152
deﬁnition (deﬁnition of), 18
Deﬂate, 50, 108–119, 274
delta code (Elias), 32–35
deterministic decoder, 16
Deutsch, Peter, 114
dictionary-based compression, 47–50, 93–119
dictionary-based methods, 274
discrete cosine transform, 159–174, 187, 274
discrete wavelet transform (DWT), 274
discrete-tone image, 143, 274
distributions
Gaussian, 275
Laplace, 276
normal, 277
downsampling, 180
Dudeney, Henry Ernest (1857–1930), 30
43

Index
307
EBCDIC, 273
Elias codes, 30–36
Elias, Peter (1923–2001), 30, 32, 124
Eliot, George (1819–1880), 208
Eliot, Thomas Stearns (1888–1965), 264
encoder, 274
algorithmic, 16
deﬁnition of, 14
entropy, 10–14, 40, 65
deﬁnition of, 274
Erd˝os–Kac theorem, 179
escape code in adaptive Huﬀman, 77
eye (and brightness sensitivity), 146
facsimile compression, 85–90, 146, 274
1D, 85–89
2D, 89–90
factor of compression, 17, 273
Fano, Robert Mario (1917–), 61, 62
Favors, Donna A. (1955–), 226
Fibonacci code, 253
Fibonacci numbers (and height of Huﬀman
trees), 74
Fibonacci, Leonardo Pisano (1170–1250),
253
ﬁle diﬀerencing, 16
ﬁlter banks, 216–218
ﬁngerprint compression, ix, 121, 218–225
ﬁnite-state machines, 65
foreground pixel (black), 272
Fourier transform, 275
Fourier, Jean Baptiste Joseph (1768–1830),
198, 275
Frazier, Michael W., 201
frequencies of symbols, 78, 79
frequency domain, 233
frequency masking, 233–234
Gailly, Jean-Loup, 108
gain of compression, 17, 273
Gallager, Robert Gray (1931–), 61
gamma code (Elias), 31–32
gas molecules (and normal distribution), 178
Gauss, Carl Friedrich (1777–1855), 178
Gaussian distribution, 178–179, 275
Golomb code, 275
Gordon, Charles, 24
Gray codes, 149–151
Gray, Elisha (1835–1901), 84
Gray, Frank, 151
grayscale image, 143, 275
grayscale image compression (extended to
color images), 43
group 3 fax compression, 85, 274
group 4 fax compression, 85, 274
Gzip, 274
Haar transform, 199–215
Haar, Alfred (1885–1933), 199, 200
Hardy, Thomas (1840–1928), 140
Hartley (information unit), 10
Hawthorne, Nathaniel (1804–1864), 120
hearing (properties of), 231–235
Hell, Rudolf (1901–2002), 84
hierarchical image compression, 182
Hilbert curve, 46–47
Hilbert, David (1862–1943), 46
Huﬀman coding, 61–83, 89, 275
adaptive, 76–83, 271
canonical, 75–76
code size, 70–71
decoding, 67–69
in JPEG, 189
not unique, 63
number of codes, 71–72
ternary, 72
2-symbol alphabet, 65
variance, 64
Huﬀman, David Albert (1925–1999), 61, 62
human hearing, 231–235
human voice (range of), 231
Hummel, Ernest A., 84
i.i.d., see also memoryless
i.i.d. (independent and identically
distributed), 53
image
bi-level, 65, 272
bitplane, 272
continuous-tone, 273
discrete-tone, 274
grayscale, 275
interlaced, 44
resolution of, 143
types of, 143–144
image compression, 143–226
bi-level (extended to grayscale), 150

308
Index
principle of, 146
image transforms, 152–174, 279
information theory, 10–14, 275
instantaneous codes, 28
interlacing scan lines, 44
ITU, 272, 274, 275
ITU-T, 85
recommendation T.4, 85
recommendation T.6, 85
JFIF, 197, 275
JPEG, 179–195, 272, 275
blocking artifacts, 181, 226
lossless mode, 194
JPEG-LS, 194
Katz, Philip Walter (1962–2000), 108, 117,
119, 277
Knuth, Donald Ervin (1938–), 310,
(Colophon)
Korn, Arthur (1870–1945), 84
Kraft–McMillan inequality, 39–41, 276
and Huﬀman codes, 65
Laplace distribution, 37, 148, 179, 276
Lempel, Abraham (1936–), 47, 98, 277
linear prediction (in audio compression),
235–238
in shorten, 244
lockstep, 77
logarithm (as the information function), 12
lossless compression, 15, 276
lossy compression, 15, 276
luminance component of color, 180, 181,
184–186, 208
LZ77 method, 48–50, 278
LZ78, 95–97
LZW, 98–107, 277
decoding, 102
Manfred, Eigen (1927–), 125
mean square error (MSE), 17
memoryless, see i.i.d., 11
midriser quantization, 242
midtread quantization, 241
minimal binary codes, see phased-in binary
codes
MLP, 276
MMR coding, 89
model (in data compression), 17, 277
modem, 85
Montesquieu, (Charles de Secondat,
1689–1755), 263
Morse code, 25
as compression, 2
Motil, John Michael (1938–), ix, 71
Motta, Giovanni (1965–), ix
move-to-front method, 272, 277
µ-law companding, 238–244
Muirhead, Alexander, 85
nat (information unit), 10
Nelson, Mark, 18
Newton, Isaac (1643–1727), 134
Nin, Anais (1903–1977), (Colophon)
nonadaptive compression, 14
normal distribution, 178–179, 277
Nyquist, Harry (1889–1976), 9, 228
Occam’s razor, 2
omega code (Elias), 35–36
optimal compression method, 16
patents of algorithms, 277
peak signal-to-noise ratio (PSNR), 17
Peano curve, 46
pel (in fax compression), 85
perceptive compression, 15
phased-in binary codes, 78
Picasso, Pablo Ruiz (1881–1973), 177, 186
pixels
background, 272
decorrelated, 152
deﬁnition of, 143, 277
foreground, 272
PKArc, 277
PKlite, 277
PKunzip, 277
PKWare, 277
PKzip, 277
power law distribution of probabilities, 31
prediction, 278
preﬁx property, 28, 278
prime factors (and normal distribution), 179
probability model, 17
Perkins, Dexter, 19
Morse, Samuel Finley Breese (1791–1872), 25

Index
309
progressive image compression, 149
Prowse, David (Darth Vader, 1935–), 77
psychoacoustic model, 278
psychoacoustics, 231–235
pyramid (wavelet image decomposition), 201
QM coder, 181, 191
quadrature mirror ﬁlters, 217
quantization, 24
deﬁnition of, 51
image transform, 152
in JPEG, 187–189
midriser, 242
midtread, 241
scalar, 51–55, 278
vector, 55–58, 279
queue (data structure), 273
random data, 65
range encoding, 140–141
Ranger, Richard H., 84
ratio of compression, 273
redundancy, 10–14
deﬁnition of, 12–13
reﬂected Gray codes, 149–151, 275
Resnikoﬀ, Howard L., 215
Reynolds, Paul, 95
RGB (reasons for using), 184
Rice codes, 36–38, 278
for audio compression, 229
Rice, Robert F. (Rice codes developer), 36
RLE, 278
Robinson, Tony (Shorten developer), 38
run-length encoding (RLE), 22, 41–46
and BW method, 248, 250
Sagan, Carl Edward (1934–1996), 52
scalar quantization, 51–55, 278
Schopenhauer, Arthur (1788–1860), 142
SCSU (Unicode compression), ix, 121, 247,
258–263
semiadaptive compression, 15
Shannon, Claude Elwood (1916–2001), 10,
11, 13, 61, 62, 254, 274, 275
Shannon–Fano method, 61–62, 275, 278
Shorten (audio compression), 38, 244–245,
278
sibling property, 78, 79
Sierpi´nski curve, 46
sliding window compression, 48–50, 278
source (of data), 10
source coding, vii, 10
space-ﬁlling curves, 46–47, 278
sparseness ratio, 209
standard (wavelet image decomposition),
201
statistical methods, 279
stone-age binary (unary code), 2, 30
streaming mode, 16
subband transform, 152, 201–218
symbol ranking, ix, 121, 247, 254–257, 263
symmetric (wavelet image decomposition),
220
symmetric compression, 15
taboo code, 59
taps (wavelet ﬁlter coeﬃcients), 220
temporal masking, 233–235
text
ﬁles, 15
random, 65
token (deﬁnition of), 279
training (in data compression), 86
training documents (used in compression),
27
transforms, 23–24, 50–51
discrete cosine, 274
Haar, 199–215
images, 279
subband, 201–218
wavelet, 198–218
tree
adaptive Huﬀman, 77–79
Huﬀman, 63, 77–79
height of, 73–75
trie (deﬁnition of), 97
trit (ternary digit), 10, 72, 279
two-pass compression, 14, 26, 57, 76, 90, 278
unary code, 30, 279
general, 279
Unicode, 273, 279
Unicode compression, 258–263
uniquely decodable (UD) codes, 22
not preﬁx codes, 28
universal compression method, 16
Updike, John Hoyer (1932–), 15

310
Index
Vail, Alfred (1807–1859), 25
variable-length codes, 21–22, 25–38, 253, 279
unambiguous, 39, 276
variance of Huﬀman codes, 64
vector quantization, 55–58, 279
Voronoi diagrams, 279
Voronoi regions, 57
warm colors, 185
wavelet image decomposition
pyramid, 201
standard, 201
symmetric, 220
wavelet transform, 198–218
wavelets
discrete transform, 274
ﬁlter banks, 216–218
quadrature mirror ﬁlters, 217
wavelets (Daubechies D4), 216
wavelets scalar quantization (WSQ),
218–225
Welch, Terry A., 98, 277
Wells, Raymond O’Neil, 215
Wheeler, John Archibald (1911–), 12
Wheeler, Wayne, ix
Wilde, Erik, 198
William of Ockham, 2
Xenophon (431–350) b.c., 246
YCbCr color space, 146, 185, 186, 197
YIQ color model, 208
zigzag sequence
in JPEG, 189
in RLE, 45
Zip (compression software), 108, 274, 280
Ziv, Jacob (1931–), 47, 98, 277
Any inaccuracies in this index may be explained by the
fact that it has been sorted with the help of a computer.
—D. Knuth

Colophon
This book was written during the short period February through September 2007. The
book was designed by the author and was typeset by him with the TEX typesetting
system developed by Donald Knuth. The text and the tables were done with Textures, a
TEX implementation for the Macintosh. The ﬁgures were generated in Adobe Illustrator,
also on the Macintosh. Figures with calculations were computed ﬁrst in Mathematica
or Matlab, and then “polished” in Illustrator.
The CM set of fonts consists of 75 fonts that are described in Knuth’s Computer
Modern Typefaces (Volume E of the Computers and Typesetting series) as well as the
“line,” “circle,” and symbol fonts associated with LaTeX.
Knuth started his research in computerized typesetting in 1975 and developed the
fonts as part of the huge TEX project. The fonts were initially called AM (American
Modern) and were later improved, to become the familiar CM fonts, released in 1985.
The Type 1 versions of the Computer Modern fonts (1990) and AMSFonts (1992)
were produced by Blue Sky Research and Y&Y, who published the fonts as part of their
implementations of TEX.
Character outlines for the CM fonts were derived from high-resolution METAFONT-
generated character bitmaps by ScanLab from Projective Solutions, applied and cor-
rected by Blue Sky Research. The outlines for the AMS Euler fonts were derived al-
gorithmically from METAFONT code using tools developed by Y&Y. Character- and
font-level hints were programmed using software from Y&Y, with extensive manual labor
by programmers from both companies.
In 1988, Blue Sky Research released the PostScript Type 3 versions of the CM fonts.
The CMMI* fonts were revised in 1996 to conform to Knuth’s changes to the Greek
delta and arrow characters.
In 1997, the type-1 CM fonts became public domain.
Life shrinks or expands in proportion to one’s courage.
—Anais Nin

