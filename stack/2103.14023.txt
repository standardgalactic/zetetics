AgentFormer: Agent-Aware Transformers for
Socio-Temporal Multi-Agent Forecasting
Ye Yuan1
Xinshuo Weng1
Yanglan Ou2
Kris Kitani1
1Carnegie Mellon University
2Penn State University
https://www.ye-yuan.com/agentformer
Abstract
Predicting accurate future trajectories of multiple agents
is essential for autonomous systems but is challenging due
to the complex interaction between agents and the uncer-
tainty in each agent’s future behavior. Forecasting multi-
agent trajectories requires modeling two key dimensions:
(1) time dimension, where we model the inﬂuence of past
agent states over future states; (2) social dimension, where
we model how the state of each agent affects others. Most
prior methods model these two dimensions separately, e.g.,
ﬁrst using a temporal model to summarize features over
time for each agent independently and then modeling the
interaction of the summarized features with a social model.
This approach is suboptimal since independent feature en-
coding over either the time or social dimension can result
in a loss of information. Instead, we would prefer a method
that allows an agent’s state at one time to directly affect
another agent’s state at a future time.
To this end, we
propose a new Transformer, termed AgentFormer, that si-
multaneously models the time and social dimensions. The
model leverages a sequence representation of multi-agent
trajectories by ﬂattening trajectory features across time and
agents. Since standard attention operations disregard the
agent identity of each element in the sequence, AgentFormer
uses a novel agent-aware attention mechanism that pre-
serves agent identities by attending to elements of the same
agent differently than elements of other agents. Based on
AgentFormer, we propose a stochastic multi-agent trajec-
tory prediction model that can attend to features of any
agent at any previous timestep when inferring an agent’s
future position. The latent intent of all agents is also jointly
modeled, allowing the stochasticity in one agent’s behavior
to affect other agents. Extensive experiments show that our
method substantially improves the state of the art on well-
established pedestrian and autonomous driving datasets.
1. Introduction
The safe planning of autonomous systems such as self-
driving vehicles requires forecasting accurate future trajec-
Time
Social
Temporal Models
Temporal Models
Temporal Models
Social
Models
(Joint Social & Temporal Modeling +
Preserve Time & Agent Information)
Agent-Aware Transformer
+
Standard Multi-Agent Trajectory Models
Our Multi-Agent Trajectory Model
Multi-Agent Trajectories
Trajectory Features
Trajectory Features in 2D
t = 1
t = 2
t = 3
t = 4
t = 5
t = 1
t = 2
t = 3
t = 4
t = 5
Agent 3
Agent 2
Agent 1
Social
Models
Figure 1. Different from standard approaches that model multi-
agent trajectories in the time and social dimensions separately, our
AgentFormer allows for joint modeling of the time and social di-
mensions while preserving time and agent information.
tories of surrounding agents (e.g., pedestrians, vehicles).
However, multi-agent trajectory forecasting is challenging
since the social interaction between agents, i.e., behavioral
inﬂuence of an agent on others, is a complex process. The
problem is further complicated by the uncertainty of each
agent’s future behavior, i.e., each agent has its latent intent
unobserved by the system (e.g., turning left or right) that
governs its future trajectory and in turn affects other agents.
Therefore, a good multi-agent trajectory forecasting method
should effectively model (1) the complex social interaction
between agents and (2) the latent intent of each agent’s fu-
ture behavior and its social inﬂuence on other agents.
Multi-agent social interaction modeling involves two key
dimensions as illustrated in Fig. 1 (Top): (1) time dimen-
sion, where we model how past agent states (positions and
velocities) inﬂuence future agent states; (2) social dimen-
sion, where we model how each agent’s state affects the
arXiv:2103.14023v3  [cs.AI]  7 Oct 2021

state of other agents.
Most prior multi-agent trajectory
forecasting methods model these two dimensions separately
(see Fig. 1 (Middle)). Approaches like [25, 1, 15] ﬁrst use
temporal models (e.g., LSTMs [17] or Transformers [47]) to
summarize trajectory features over time for each agent inde-
pendently and then input the summarized temporal features
to social models (e.g., graph neural networks [23]) to cap-
ture social interaction between agents. Alternatively, meth-
ods like [45, 18] ﬁrst use social models to produce social
features for each agent at each independent timestep and
then apply temporal models over the social features. In this
work, we argue that modeling the time and social dimen-
sions separately can be suboptimal since the independent
feature encoding over either the time or social dimension
is not informed by features across the other dimension, and
the encoded features may not contain the necessary infor-
mation for modeling the other dimension.
To tackle this problem, we propose a new Transformer
model, termed AgentFormer, that simultaneously learns
representations from both the time and social dimensions.
AgentFormer allows an agent’s state at one time to af-
fect another agent’s state at a future time directly instead
of through intermediate features encoded over one dimen-
sion.
As Transformers require sequences as input, we
leverage a sequence representation of multi-agent trajecto-
ries by ﬂattening trajectory features across time and agents
(see Fig. 1 (Bottom)). However, directly applying standard
Transformers to these multi-agent sequences will result in
a loss of time and agent information since standard atten-
tion operations discard the timestep and agent identity as-
sociated with each element in the sequence. We solve the
loss of time information using a time encoder that appends
a timestamp feature to each element. However, the loss of
agent identity is a more complicated problem: unlike time,
there is no innate ordering between agents, and assigning an
agent index-based encoding will break the required permu-
tation invariance of agents and create artiﬁcial dependencies
on agent indices in the model. Instead, we propose a novel
agent-aware attention mechanism to preserve agent infor-
mation. Speciﬁcally, agent-aware attention generates two
sets of keys and queries via different linear transformations;
one set of keys and queries is used to compute inter-agent
attention (agent to agent) while the other set is designated
for intra-agent attention (agent to itself). This design al-
lows agent-aware attention to attend to elements of the same
agent differently than elements of other agents, thus keep-
ing the notion of agent identity. Agent-aware attention can
be implemented efﬁciently via masked operations. Further-
more, AgentFormer can also encode rule-based connectiv-
ity between agents (e.g., based on distance) by masking out
the attention weights between unconnected agents.
Based on AgentFormer, which allows us to model social
interaction effectively, we propose a multi-agent trajectory
prediction framework that also models the social inﬂuence
of each agent’s future trajectory on other agents. The prob-
abilistic formulation of the model follows the conditional
variational autoencoder (CVAE [21]) where we model the
generative future trajectory distribution conditioned on con-
text (e.g., past trajectories, semantic maps). We introduce a
latent code for each agent to represent its latent intent. To
model the social inﬂuence of each agent’s future behavior
(governed by latent intent) on other agents, the latent codes
of all agents are jointly inferred from the future trajectories
of all agents during training, and they are also jointly used
by a trajectory decoder to output socially-aware multi-agent
future trajectories. Thanks to AgentFormer, the trajectory
decoder can attend to features of any agent at any previ-
ous timestep when inferring an agent’s future position. To
improve the diversity of sampled trajectories and avoid sim-
ilar samples caused by random sampling, we further adopt
a multi-agent trajectory sampler that can generate diverse
and plausible multi-agent trajectories by mapping context
to various conﬁgurations of all agents’ latent codes.
We evaluate our method on well-established pedestrian
datasets, ETH [38] and UCY [28], and an autonomous driv-
ing dataset, nuScenes [3]. On ETH/UCY and nuScenes,
we outperform state-of-the-art multi-agent prediction meth-
ods with substantial performance improvement. We further
conduct extensive ablation studies to show the superiority of
AgentFormer over various combinations of social and tem-
poral models. We also demonstrate the efﬁcacy of agent-
aware attention against agent encoding.
To summarize, the main contributions of this paper are:
(1) We propose a new Transformer that simultaneously
models the time and social dimensions of multi-agent tra-
jectories with a sequence representation. (2) We propose a
novel agent-aware attention mechanism that preserves the
agent identity of each element in the multi-agent trajectory
sequence. (3) We present a multi-agent forecasting frame-
work that models the latent intent of all agents jointly to
produce socially-plausible future trajectories. (4) Our ap-
proach substantially improves the state of the art on well-
established pedestrian and autonomous driving datasets.
2. Related Work
Sequence Modeling. Sequences are an important represen-
tation of data such as video, audio, price, etc. Historically,
RNNs (e.g., LSTMs [17], GRUs [7]) have achieved re-
markable success in sequence modeling, with applications
to speech recognition [52, 35], image captioning [53], ma-
chine translation [32], human pose estimation [56, 24], etc.
In particular, RNNs have been the preferred temporal mod-
els for trajectory and motion forecasting. Many RNN-based
methods model the trajectory pattern of pedestrians to pre-
dict their 2D future locations [1, 19, 61]. Prior work has also
used RNNs to model the temporal dynamics of 3D human

pose [11, 58, 60]. With the invention of Transformers and
positional encoding [47], many works start to adopt Trans-
formers for sequence modeling due to their strong ability to
capture long-range dependencies. Transformers have ﬁrst
dominated the natural language processing (NLP) domain
across various tasks [9, 26, 54]. Beyond NLP, numerous
visual Transformers have been proposed to tackle vision
tasks, such as image classiﬁcation [10], object detection [4],
and instance segmentation [50].
Recently, Transformers
have also been used for trajectory forecasting. Transformer-
TF [12] applies the standard Transformer to predict the fu-
ture trajectories of each agent independently. STAR [55]
uses separate temporal and spatial Transformers to forecast
multi-agent trajectories. Interaction Transformer [30] com-
bines RNNs and Transformers for multi-agent trajectory
modeling.
Different from prior work, Our AgentFormer
leverages a sequence representation of multi-agent trajec-
tories and a novel agent-aware attention mechanism to pre-
serve time and agent information in the sequence.
Trajectory Prediction. Early work on trajectory prediction
adopts a deterministic approach using models such as social
forces [16], Gaussian process (GP) [49], and RNNs [1, 36,
48]. A thorough review of these deterministic methods is
provided in [43]. As the future trajectory of an agent is un-
certain and often multi-modal, recent trajectory prediction
methods start to model the trajectory distribution with deep
generative models [21, 13, 40] such as conditional varia-
tional autoencoders (CVAEs) [27, 57, 19, 46, 51, 45], gener-
ative adversarial networks (GANs) [15, 44, 25, 62], and nor-
malizing ﬂows (NFs) [41, 42, 14]. Most of these methods
follow a seq2seq structure [2, 6] and predict future trajecto-
ries using intermediate features of past trajectories. In con-
trast, our AgentFormer-based trajectory prediction frame-
work can directly attend to features of any agent at any
previous timestep when inferring an agent’s future position.
Moreover, our approach models the future trajectories of all
agents jointly to predict socially-aware trajectories.
Social Interaction Modeling.
Methods for social inter-
action modeling can be categorized based on how they
model the time and social dimensions. While RNNs [17, 7]
and Transformers [47] are the prefered temporal mod-
els [18, 1, 55], graph neural networks (GNNs) [23, 31] are
often employed as the social models for interaction model-
ing [22, 29, 25]. One popular type of methods [25, 1, 15]
ﬁrst uses temporal models to summarize trajectory features
over time for each agent independently and then feeds the
temporal features to social models to obtain socially-aware
agent features. Alternatively, approaches like [45, 18] ﬁrst
use social models to produce social features of each agent at
each independent timestep and then apply temporal models
to summarize the social features over time for each agent.
One common characteristic of these prior works is that they
model the time and social dimensions on separate levels.
This can be suboptimal since it prevents an agent’s feature
at one time from directly interacting with another agent’s
feature at a different time, thus limiting the model’s ability
to capture long-range dependencies. Instead, our method
models both the time and social dimensions simultaneously,
allowing direct feature interaction across time and agents.
3. Approach
We formulate multi-agent trajectory prediction as mod-
eling the generative future trajectory distribution of N (vari-
able) agents conditioned on their past trajectories. For ob-
served timesteps t ≤0, we represent the joint state of
all N agents at time t as Xt = (xt
1, xt
2, . . . , xt
N), where
xt
n ∈Rds is the state of agent n at time t, which in-
cludes the position, velocity and (optional) heading an-
gle of the agent. We denote the history of all agents as
X =
 X−H, X−H+1, . . . , X0
which includes the joint
agent state at all H + 1 observed timesteps. Similarly, the
joint state of all N agents at future time t (t > 0) is de-
noted as Yt = (yt
1, yt
2, . . . , yt
N), where yt
n ∈Rdp is the
future position of agent n at time t.
We denote the fu-
ture trajectories of all N agents over T future timesteps as
Y =
 Y1, Y2, . . . , YT 
. Depending on the data, optional
contextual information I may also be given, such as a se-
mantic map around the agents (annotations of sidewalks,
road boundaries, etc.). Our goal is to learn a generative
model pθ(Y|X, I) where θ are the model parameters.
In the following, we ﬁrst introduce the proposed agent-
aware Transformer, AgentFormer, for joint modeling of
socio-temporal relations.
We then present a stochastic
multi-agent trajectory prediction framework that jointly
models the latent intent of all agents.
3.1. AgentFormer: Agent-Aware Transformers
Our agent-aware Transformer, AgentFormer, is a model
that learns representations from multi-agent trajectories
over both time and social dimensions simultaneously, in
contrast to standard approaches that model the two dimen-
sions in separate stages.
AgentFormer has two types of
modules – encoders and decoders, which follow the en-
coder and decoder design of the original Transformer [47]
but with two major differences: (1) it replaces positional en-
coding with a time encoder; (2) it uses a novel agent-aware
attention mechanism instead of the scaled dot-product at-
tention. As we will discuss below, these two modiﬁcations
are motivated by a sequence representation of multi-agent
trajectories that is suitable for Transformers.
Multi-Agent Trajectories as a Sequence. The past multi-
agent trajectories X can be denoted as a sequence X =
 x−H
1
, . . . , x−H
N , x−H+1
1
, . . . , x−H+1
N
, . . . , x0
1, . . . , x0
N

of
length Lp = N × (H + 1). Similarly, the future multi-
agent trajectories can also be represented as a sequence

Y =
 y1
1, . . . , y1
N, y2
1, . . . , y2
N, . . . , yT
1 , . . . , yT
N

of length
Lf = N × T. We adopt this sequence representation to be
compatible with Transformers. At ﬁrst glance, it may seem
that we can directly apply standard Transformers to these
sequences to model temporal and social relations. How-
ever, there are two problems with this approach: (1) loss of
time information, as Transformers have no notion of time
when computing attention for each element (e.g., xt
n) w.r.t.
other elements in the sequence; for instance, xt
n does not
know xt
m is a feature of the same timestep while xt+1
n
is
a feature of the next timestep; (2) loss of agent informa-
tion, since Transformers do not consider agent identities
when applying attention to each element, and elements of
the same agent are not distinguished from elements of other
agents; for example, when computing attention for xt
n, both
xt+1
n
and xt+1
m
are treated the same, disregarding the fact
that xt+1
n
is from the same agent while xt+1
m
is from a dif-
ferent agent. Below, we present the solutions to these two
problems – (1) time encoder and (2) agent-aware attention.
Time Encoder. To inform AgentFormer about the timestep
associated with each element in the trajectory sequence, we
employ a time encoder similar to the positional encoding
in the original Transformer. Instead of encoding the posi-
tion of each element based on its index in the sequence, we
compute a timestamp feature based on the timestep t of the
element. The timestamp uses the same sinusoidal design as
the positional encoding. Let us take the past trajectory se-
quence X as an example. For each element xt
n, the times-
tamp feature τ t
n ∈Rdτ is deﬁned as
τ t
n(k) =
(
sin((t + H)/10000k/dτ ),
k is even
cos((t + H)/10000(k−1)/dτ ),
k is odd
where τ t
n(k) denotes the k-th feature of τ t
n and dτ is the
feature dimension of the timestamp. The time encoder out-
puts a timestamped sequence ¯X and each element ¯xt
n ∈
Rdτ in ¯X is computed as ¯xt
n = W2(W1xt
n ⊕τ t
n) where
W1 ∈Rdτ ×ds and W2 ∈Rdτ ×2dτ are weight matrices
and ⊕denotes concatenation.
Agent-Aware Attention. To preserve agent information in
the trajectory sequence, it may be tempting to employ a sim-
ilar strategy to the time encoder, such as an agent encoder
that assigns an agent index-based encoding to each element
in the sequence. However, using such agent encoding is not
effective as we will show in the experiments. The reason is
that, different from time which is naturally ordered, there is
no innate ordering between agents, and assigning encodings
based on agent indices will break the required permutation
invariance of agents and create artiﬁcial dependencies on
agent indices in the model.
We tackle the loss of agent information from a differ-
ent angle by proposing a novel agent-aware attention mech-
anism. The agent-aware attention takes as input keys K,
Number of Agents N = 3 (for illustration)
Agent 1
Agent 2
Agent 3
Attention Weight Matrix
Mask
Figure 2. Illustration of agent-aware attention. The mask M
allows the attention weights in A to be computed differently based
on whether the i-th query and j-th key belong to the same agent.
queries Q and values V, each of which uses the sequence
representation of multi-agent trajectories. As an example,
let the keys K and values V be the past trajectory sequence
X ∈RLp×ds, and let the queries Q be the future trajec-
tory sequence Y ∈RLf ×dp. Recall that X is of length
Lp = N×(H+1) as X contains the trajectory features of N
agents of H +1 past timesteps; Y is of length Lf = N ×T
containing trajectory features of T future timesteps. The
output of agent-aware attention is computed as
AgentAwareAttention(Q, K, V) = softmax
 A
√dk

V
(1)
A = M ⊙(QselfKT
self) + (1 −M) ⊙(QotherKT
other)
(2)
Qself = QWQ
self,
Kself = KWK
self
(3)
Qother = QWQ
other,
Kother = KWK
other
(4)
where ⊙denotes element-wise product and we use two sets
of projections {WQ
self, WK
self} and {WQ
other, WK
other} to
generate projected keys Kself, Kother
∈RLp×dk and
queries Qself, Qother ∈RLf ×dk with key (query) dimen-
sion dk. Each element Aij in the attention weight matrix
A represents the attention weight between the i-th query qi
and j-th key kj. As illustrated in Fig. 2, when computing
the attention weight matrix A ∈RLf ×Lp, we also use a
mask M ∈RLf ×Lp which is deﬁned as
Mij = 1(i mod N = j mod N)
(5)
where Mij denotes each element inside the mask M and
1(·) denotes the indicator function. As · mod N computes
the agent index of a query/key, Mij equals to one if the i-th
query qi and j-th key kj belongs to the same agent, and
Mij equals to zero otherwise, as shown in Fig. 2. Using the
mask M, Eq. (2) computes each element Aij of the atten-
tion weight matrix A differently based on the agreement of
agent identity: If qi and kj have the same agent identity,
Aij is computed using the projected queries Qself and keys
Kself designated for intra-agent attention (agent to itself);
If qi and kj have different agent identities, Aij is computed
using the projected queries Qother and keys Kother desig-
nated for inter-agent attention (agent to other agents). In this

AgentFormer Encoder
CVAE Past Encoder
AgentFormer Decoder
CVAE Future Encoder
Value
CVAE Future Decoder (Autoregressive)
AgentFormer
Decoder
MLP
(Element-wise)
AgentFormer
Decoder
MLP
(Element-wise)
AgentFormer
Decoder
MLP
(Element-wise)
Agent-wise Pooling
MLP
MLP
MLP
Key
Value
Key
Query
Key
Query
Value
Query
Query
Query
Agent-wise Pooling
MLP
MLP
MLP
Agent-wise Pooling
MLP
MLP
MLP
CVAE Prior
Trajectory Sampler
Time Encoder
Add Context     (Optional)
Time Encoder
Add Context     (Optional)
Time
Encoder
Time
Encoder
Time
Encoder
(Optional)
Add Context    
(Optional)
Add Context    
(Optional)
Add Context    
Number of Agents N = 3 (for illustration)
Agent 1
Agent 2
Agent 3
: Past Trajectories
: Latent Code (Agent n)
 : GT Future Trajectories
 : Pred Future Trajectories
Figure 3. Overview of our AgentFormer-based multi-agent trajectory prediction framework.
way, the agent-aware attention learns to attend to elements
of the same agent in the sequence differently than elements
of other agents, thus preserving the notion of agent iden-
tity. Note that AgentFormer only uses agent-aware attention
to replace the scaled dot-product attention in the original
Transformer and still allows multi-head attention to learn
distributed representations.
Encoding Agent Connectivity. AgentFormer can also en-
code rule-based agent connectivity information by mask-
ing out the attention weights between unconnected agents.
Speciﬁcally, we deﬁne that two agents n and m are con-
nected if their distance Dnm at the current time (t = 0) is
smaller than a threshold η. If agents n and m are not con-
nected, we set the attention weight Aij = −∞between any
query qi of agent n and any key kj of agent m.
3.2. Multi-Agent Prediction with AgentFormer
Having introduced AgentFormer for modeling tempo-
ral and social relations, we are now ready to apply it in
our multi-agent trajectory prediction framework based on
CVAEs. As discussed at the start of Sec. 3, the goal of
multi-agent trajectory prediction is to model the future tra-
jectory distribution pθ(Y|X, I) conditioned on past trajec-
tories X and contextual information I.
To account for
stochasticity and multi-modality in each agent’s future be-
havior, we introduce latent variables Z = {z1, . . . , zN}
where zn ∈Rdz represents the latent intent of agent n. We
can then rewrite the future trajectory distribution as
pθ(Y|X, I) =
Z
pθ(Y|Z, X, I)pθ(Z|X, I)dZ ,
(6)
where pθ(Z|X, I) = QN
n=1 pθ(zn|X, I) is a conditional
Gaussian prior factorized over agents and pθ(Y|Z, X, I) is
a conditional likelihood model. To tackle the intractable in-
tegral in Eq. (6), we use the negative evidence lower bound
(ELBO) Lelbo in the CVAE as our loss function:
Lelbo = −Eqφ(Z|Y,X,I)[log pθ(Y|Z, X, I)]
+ KL(qφ(Z|Y, X, I)∥pθ(Z|X, I)) ,
(7)
where qφ(Z|Y, X, I) = QN
n=1 qφ(zn|Y, X, I) is an ap-
proximate posterior distribution factorized over agents and
parametrized by φ. In our probabilistic formulation, the la-
tent codes Z of all agents in the posterior qφ(Z|Y, X, I) are
jointly inferred from the future trajectories Y of all agents;
similarly, the future trajectories Y in the conditional likeli-
hood pθ(Y|Z, X, I) are modeled using the latent codes Z of
all agents. This design allows each agent’s latent intent rep-
resented by zn to affect not just its own future trajectory but
also the future trajectories of other agents, which enables us
to generate socially-aware multi-agent trajectories. Having
described the probabilistic formulation, we now introduce
the detailed model architecture as outlined in Fig. 3.
Encoding Context (Semantic Map). As aforementioned,
our model can optionally take as input contextual infor-
mation I if provided by the data. Here, we assume I ∈
RH0×W0×C is a semantic map around the agents at the cur-
rent timestep (t = 0) with annotated semantic information
(e.g., sidewalks, crosswalks, and road boundaries). For each
agent n, we rotate I to align with the agent’s heading an-
gle and crop an image patch In ∈RH×W ×C around the
agent. We use a hand-designed convolutional neural net-
work (CNN) to extract visual features vn from In, which
will later be used by other modules in the model.
CVAE Past Encoder.
The past encoder starts with the
multi-agent past trajectory sequence X.
If the semantic
map I is provided, the past encoder concatenates each el-
ement xt
n ∈X with the corresponding visual feature vn

of agent n. The new sequence is then fed into the time
encoder to obtain a timestamped sequence, which is then
input to the AgentFormer encoder as keys, queries, and val-
ues. The output of the encoder is a past feature sequence
C =
 c−H
1
, . . . c−H
N , c−H+1
1
, . . . c−H+1
N
, . . . , c0
1, . . . , c0
N

that summarizes the past agent trajectories X and context I.
CVAE Prior. The prior module ﬁrst performs an agent-wise
pooling that computes a mean agent feature Cn from the
past features across timesteps: Cn = mean(c−H
n
, . . . , c0
n).
We then use a multilayer perceptron (MLP) to map Cn to
the Gaussian parameters (µp
n, σp
n) of the prior distribution
pθ(zn|X, I) = N(µp
n, Diag(σp
n)2).
CVAE Future Encoder. Given the multi-agent future tra-
jectory sequence Y, similar to the past encoder, the future
encoder appends visual features from the semantic map I
to Y and feeds the resulting sequence to the time encoder
to produce a timestamped sequence. The timestamped se-
quence is then input as queries to the AgentFormer decoder
along with the past feature sequence C which serves as both
keys and values. We use the AgentFormer decoder here be-
cause it allows the feature extraction of Y to condition on
X through C, thus effectively modeling the X-conditioning
in the posterior qφ(Z|Y, X, I). We then perform an agent-
wise mean pooling across timesteps on the output sequence
of the AgentFormer decoder to extract a feature for each
agent. Each agent feature is then input to an MLP to obtain
the Gaussian parameters (µq
n, σq
n) of the approximate pos-
terior distribution qφ(zn|Y, X, I) = N(µq
n, Diag(σq
n)2).
CVAE Future Decoder. Unlike the original Transformer
decoder, our future trajectory decoder is autoregressive,
which means it outputs trajectories one step at a time
and feeds the currently generated trajectories back into the
model to produce the trajectories of the next timestep. This
design mitigates compounding errors during test time at the
expense of training speed. Starting from an initial sequence
(ˆy0
1, . . . , ˆy0
N) where ˆy0
n = ˜x0
n (˜x0
n is the position feature in-
side x0
n), the future decoder module maps an input sequence
(ˆy0
1, . . . , ˆy0
N, . . . , ˆyt′
1 , . . . , ˆyt′
N)
to
an
output
sequence
(ˆy1
1, . . . , ˆy1
N, . . . , ˆyt′+1
1
, . . . , ˆyt′+1
N
) and grows the input se-
quence into (ˆy0
1, . . . , ˆy0
N, . . . , ˆyt′+1
1
, . . . , ˆyt′+1
N
).
By au-
toregressively applying the decoder T times, we obtain the
output sequence ˆY = (ˆy1
1, . . . , ˆy1
N, . . . , ˆyT
1 , . . . , ˆyT
N). In-
side the future decoder module (Fig. 3 (Right)), we ﬁrst
form a feature sequence F = (f 0
1 , . . . , f 0
N, . . . , f t′
1 , . . . , f t′
N)
where f t
n = ˆyt
n ⊕zn, thus concatenating the currently gen-
erated trajectories with the corresponding latent codes. The
latent codes are sampled from the approximate posterior
during training but from the trajectory sampler (as discussed
below) at test time. The feature sequence F is then con-
catenated with the semantic map features and timestamped
before being input as queries to the AgentFormer decoder
alongside the past feature sequence C which serves as keys
and values. The AgentFormer decoder enables the future
trajectories to directly attend to features of any agent at any
previous timestep (e.g., c−H
3
or ˆy1
2), allowing the model to
effectively infer future trajectories based on the whole agent
history. We use proper masking inside the AgentFormer de-
coder to enforce causality of the decoder output sequence.
Each element of the output sequence is then passed through
an MLP to generate the decoded future agent position ˆyt
n.
As we use a Gaussian to model the conditional likelihood
pθ(Y|Z, X, I) = N( ˆY, I/β), where I is the identity ma-
trix and β is a weighting factor, the ﬁrst term in Eq. (7)
equals the mean squred error (MSE): Lmse =
1
2β ∥Y−ˆY∥2.
Trajectory Sampler. We adapt a diversity sampling tech-
nique, DLow [59], to our multi-agent trajectory prediction
setting and employ a trajectory sampler to produce diverse
and plausible trajectories once our CVAE model is trained.
The trajectory sampler generates K sets of latent codes
{Z(1), . . . , Z(K)} where each set Z(k) = {z(k)
1 , . . . , z(k)
N }
contains the latent codes of all agents and can be decoded
by the CVAE decoder into a multi-agent future trajectory
sample ˆY(k). Each latent code z(k)
n
∈Z(k) is generated by
a linear transformation of a Gaussian noise ϵn ∈Rdz:
z(k)
n
= A(k)
n ϵn + b(k)
n ,
ϵn ∼N(0, I),
(8)
where A(k)
n
∈Rdz×dz is a non-singular matrix and b(k)
n
∈
Rdz is a vector. Eq. (8) induces a Gaussian sampling dis-
tribution rθ(z(k)
n |X, I) over z(k)
n . The distribution is condi-
tioned on X and I because its inner parameters {A(k)
n , b(k)
n }
are generated by the trajectory sampler module (Fig. 3)
through agent-wise pooling of the past feature sequence C
and an MLP. The trajectory sampler loss is deﬁned as
Lsamp = min
k
∥ˆY(k) −Y∥2
+
N
X
n=1
KL(rθ(z(k)
n |X, I)∥pθ(zn|X, I))
+
1
K(K −1)
K
X
k1=1
K
X
k1̸=k2
exp
 
−∥ˆY(k1) −ˆY(k2)∥2
σd
!
,
(9)
where σd is a scaling factor. The ﬁrst term encourages the
future trajectory samples ˆY(k) to cover the ground truth Y.
The second KL term encourages each latent code z(k)
n
to
follow the prior and be plausible; the KL can be computed
analytically as both distributions inside are Gaussians. The
third term encourages diversity among the future trajectory
samples ˆY(k) by penalizing small pairwise distance. When
training the trajectory sampler with Eq. (9), we freeze the
weights of the CVAE modules. At test time, we sample
latent codes {Z(1), . . . , Z(K)} using the trajectory sampler
instead of sampling from the CVAE prior and decode the
latent codes into trajectory samples { ˆY(1), . . . , ˆY(K)}.

4. Experiments
Datasets. We evaluate our method on well-established pub-
lic datasets: the ETH [38], UCY [28], and nuScenes [3]
datasets. The ETH/UCY datasets are the major benchmark
for pedestrian trajectory prediction. There are ﬁve datasets
in ETH/UCY, each of which contains pedestrian trajectories
captured at 2.5Hz in multi-agent social scenarios with rich
interaction. nuScenes is a recent large-scale autonomous
driving dataset, which consists of 1000 driving scenes with
each scene annotated at 2Hz. nuScenes also provides HD
semantic maps with 11 semantic classes.
Metrics. We report the minimum average displacement er-
ror ADEK and ﬁnal displacement error FDEK of K trajec-
tory samples of each agent compared to the ground truth:
ADEK =
1
T minK
k=1
PT
t=1 ∥ˆyt,(k)
n
−yt
n∥2,
FDEK =
minK
k=1 ∥ˆyT,(k)
n
−yT
n ∥2, where ˆyt,(k)
n
denotes the future po-
sition of agent n at time t in the k-th sample and yT
n is the
corresponding ground truth. ADEK and FDEK are the stan-
dard metrics for trajectory prediction [15, 44, 45, 39, 5].
Evaluation Protocol.
For the ETH/UCY datasets, we
adopt a leave-one-out strategy for evaluation, following
prior work [15, 44, 45, 34, 55].
We forecast 2D fu-
ture trajectories of 12 timesteps (4.8s) based on observed
trajectories of 8 timesteps (3.2s).
Similar to most prior
works, we do not use any semantic/visual information for
ETH/UCY for fair comparisons. All metrics are computed
with K = 20 samples. For the nuScenes dataset, following
prior work [39, 5, 8, 33], we use the vehicle-only train-val-
test split provided by the nuScenes prediction challenge and
predict 2D future trajectories of 12 timesteps (6s) based on
observed trajectories of 4 timesteps (2s). We report results
with metrics computed using K = 1, 5 and 10 samples.
Implementation Details. For all datasets, we represent tra-
jectories in a scene-centered coordinate where the origin is
the mean position of all agents at t = 0. The future decoder
in Fig. 3 outputs the offset to the agent’s current position ˜x0
n,
so ˜x0
n is added to obtain ˆyt
n for each element in the output
sequence. Following prior work [45, 55], random rotation
of the scene is adopted for data augment. Our multi-agent
prediction model (Fig. 3) uses two stacks (deﬁned in [47])
of identical layers in each AgentFormer encoder/decoder
with 0.1 dropout rate. The dimensions dk, dv, dτ of keys,
queries, and timestamps in AgentFormer are all set to 256,
and the hidden dimension of feedforward layers is 512. The
number of heads for multi-head agent-aware attention is 8.
All MLPs in the model have hidden dimensions (512, 256).
For the CVAE, the latent code dimension dz is 32, the co-
efﬁcient β of the MSE loss equals 1, and we clip the max-
imum value of the KL term in Lelbo (Eq. (7)) down to 2.
We also use the variety loss in SGAN [15] in addition to
Lelbo. The agent connectivity threshold η is set to 100. We
Method
ADE20/FDE20 ↓(m), K = 20 Samples
ETH
Hotel
Univ
Zara1
Zara2
Average
SGAN [15]
0.81/1.52 0.72/1.61 0.60/1.26 0.34/0.69 0.42/0.84 0.58/1.18
SoPhie [44]
0.70/1.43 0.76/1.67 0.54/1.24 0.30/0.63 0.38/0.78 0.54/1.15
Transformer-TF [12] 0.61/1.12 0.18/0.30 0.35/0.65 0.22/0.38 0.17/0.32 0.31/0.55
STAR [55]
0.36/0.65 0.17/0.36 0.31/0.62 0.26/0.55 0.22/0.46 0.26/0.53
PECNet [34]
0.54/0.87 0.18/0.24 0.35/0.60 0.22/0.39 0.17/0.30 0.29/0.48
Trajectron++ [45]
0.39/0.83 0.12/0.21 0.20/0.44 0.15/0.33 0.11/0.25 0.19/0.41
Ours (AgentFormer)
0.45/0.75 0.14/0.22 0.25/0.45 0.18/0.30 0.14/0.24 0.23/0.39
Table 1. Baseline comparisons on the ETH/UCY datasets.
Method
K = 5 Samples
K = 10 Samples
ADE5 ↓
FDE5 ↓
ADE10 ↓
FDE10 ↓
MTP [8]
2.93
-
2.93
-
MultiPath [5]
2.32
-
1.96
-
CoverNet [39]
1.96
-
1.48
-
DSF-AF [33]
2.06
4.67
1.66
3.71
DLow-AF [59]
2.11
4.70
1.78
3.58
Trajectron++ [45]
1.88
-
1.51
-
Ours (AgentFormer)
1.86
3.89
1.45
2.86
Table 2. Baseline comparisons on the nuScenes dataset.
train the CVAE model using the Adam optimizer [20] for
100 epochs on ETH/UCY and nuScenes. We use an ini-
tial learning rate of 10−4 and halve the learning rate every
10 epochs. More details including the CNN for encoding
semantic maps and the training procedure of the trajectory
sampler can be found in Appendix B.
4.1. Results
Baseline Comparisons. On the ETH/UCY datasets, we
compare our approach with current state-of-the-art meth-
ods – Trajectron++ [45], PECNet [34], STAR [55], and
Transformer-TF [12] – as well as common baselines –
SGAN [15] and Sophie [44]. The performance of all meth-
ods is summarized in Table 1, where we use ofﬁcially-
reported results for the baselines.
We can observe that
our AgentFormer achieves very competitive performance
and attains the best FDE. Particularly, our approach sig-
niﬁcantly outperforms prior Transformer-based methods,
Transformer-TF [12] and STAR [55]. As FDE measures the
ﬁnal displacement error of predicted trajectories, it places
more emphasis on a method’s ability to predict distant fu-
tures than ADE. We believe the strong performance of our
method in FDE can be attributed to the design of Agent-
Former, which can model long-range trajectory dependen-
cies effectively by directly attending to features of any agent
at any previous timestep when inferring an agent’s future
position.
Compared to ETH/UCY, the trajectories in nuScenes are
much longer as we evaluate with a longer time horizon
(6s) and vehicles are much faster than pedestrians. Thus,
nuScenes presents a different challenge for multi-agent pre-
diction methods. On the nuScenes dataset, we evaluate our
approach against state-of-the-art vehicle prediction meth-
ods – Trajectron++ [45], MTP [8], MultiPath [5], Cover-

Model
ADE20/FDE20 ↓(m), K = 20 Samples
Social
Temporal
ETH
Hotel
Univ
Zara1
Zara2
Average
GCN
LSTM
0.57/0.90 0.20/0.34 0.29/0.52 0.24/0.44 0.23/0.42 0.31/0.52
GCN
TF
0.56/0.93 0.15/0.28 0.28/0.51 0.24/0.45 0.19/0.35 0.28/0.50
TF
LSTM
0.55/0.91 0.18/0.31 0.28/0.50 0.24/0.44 0.21/0.39 0.29/0.51
TF
TF
0.50/0.82 0.15/0.27 0.28/0.52 0.22/0.42 0.16/0.31 0.26/0.47
Joint Socio-Temporal
ETH
Hotel
Univ
Zara1
Zara2
Average
Ours w/o joint latent
0.49/0.77 0.15/0.25 0.29/0.52 0.22/0.41 0.18/0.33 0.27/0.46
Ours w/o AA attention
0.49/0.80 0.15/0.25 0.31/0.54 0.23/0.41 0.19/0.34 0.27/0.47
Ours w/ agent encoding 0.48/0.78 0.14/0.23 0.32/0.55 0.22/0.40 0.19/0.34 0.27/0.46
Ours (AgentFormer)
0.45/0.75 0.14/0.22 0.25/0.45 0.18/0.30 0.14/0.24 0.23/0.39
Table 3. Ablation studies on the ETH/UCY datasets. “TF” means
Transformer and “AA Attention” denotes agent-aware attention.
Model
K = 5 Samples
K = 10 Samples
Social
Temporal
ADE5 ↓
FDE5 ↓
ADE10 ↓
FDE10 ↓
GCN
LSTM
2.17
4.42
1.57
3.09
GCN
TF
2.03
4.36
1.52
2.95
TF
LSTM
2.12
4.48
1.69
3.31
TF
TF
1.99
4.12
1.54
3.07
Joint Socio-Temporal
ADE5 ↓
FDE5 ↓
ADE10 ↓
FDE10 ↓
Ours w/o semantic map
1.97
4.21
1.58
3.14
Ours w/o joint latent
1.95
3.98
1.50
2.92
Ours w/o AA attention
2.02
4.29
1.55
2.91
Ours w/ agent encoding
2.01
4.28
1.63
3.11
Ours (AgentFormer)
1.86
3.89
1.45
2.86
Table 4. Ablation studies on the nuScenes dataset. “TF” means
Transformer and “AA Attention” denotes agent-aware attention.
Net [39], DSF-AF [33], and DLow-AF [59]. We report the
performance of all methods in Table 2, where the results of
Trajectron++ are taken from the nuScenes prediction chal-
lenge leaderboard, the performance of DLow-AF is from
[33], and we also use the ofﬁcially-reported results for the
other baselines. The FDE of some baselines is not available
since the number has not been reported. We can see that
our approach, AgentFormer, outperforms the baselines, es-
pecially the strong model Trajectron++ [45], consistently in
ADE and FDE for both 5 and 10 sample settings.
Ablation Studies. We further perform extensive ablation
studies on ETH/UCY and nuScenes to investigate the con-
tribution of key technical components in our method. The
ﬁrst ablation study explores variants of our method that
use separate social and temporal models to replace our
joint socio-temporal model, AgentFormer, in our multi-
agent prediction framework. We choose GCN [23] or Trans-
former (TF) as the social model, and LSTM or Transformer
as the temporal model. In total, there are 4 (2×2) combina-
tions of social and temporal models. The ablation results are
summarized in the ﬁrst group of Table 3 and 4. It is evident
that all combinations of separate social and temporal mod-
els lead to inferior performance compared to our method
which models the social and temporal dimensions jointly.
The second ablation study investigates the role of (1)
joint latent intent modeling, (2) agent-aware attention, and
(3) semantic maps, and we denote the corresponding vari-
0.0
0.2
0.0
0.2
Target (Being Predicted)
Attention to Past
Attention to Future
Past Trajectory
Predicted Future Trajectory
GT Future Trajectory
(a) Sample 1
(b) Sample 1 (Attention)
(c) Sample 2
(d) Sample 3
A1
A2
A3
A4
A5
A1
A2
A3
A4
A5
A1
A2
A3
A4
A5
A1
A2
A3
A4
A5
Figure 4. (a,c,d) Three samples of forecasted multi-agent futures
(green) via our method, which exhibit social behaviors like follow-
ing (A3 & A4) and collision avoidance (A1 & A2 in (a), A2 & A3
in (c)). (b) Attention visualization for sample 1. When predicting
the target (red), the model pays more attention (darker color) to
key timesteps (turning point) of adjacent agents and spreads out
attention to the target’s past timesteps to reason about dynamics.
ants as “w/o joint latent”, “w/o AA attention”, and “w/o se-
mantic map”. We further test a variant “w/ agent encoding”
where we replace agent-aware attention with agent encod-
ing. The results are reported in the second group of Table 3
and 4. We can see that all variants lead to considerably
worse performance compared to our full method. In par-
ticular, the variants “w/o AA attention” and “w/ agent en-
coding” result in pronounced performance drop, which in-
dicates that agent-aware attention is essential in our method
and alternatives like agent encoding are not effective.
Trajectory Visualization. Fig. 4 (a,c,d) shows three sam-
ples of forecasted multi-agent futures of the same scene
via our method. We can see that the samples correspond
to different modes of socially-aware and non-colliding tra-
jectories, and exhibit behaviors like following (A3 & A4)
and collision avoidance (A1 & A2 in (a), A2 & A3 in (c)).
Fig. 4 (b) visualizes the attention of sample 1 and shows
that, when predicting the target (red), the model pays more
attention to key timesteps (turning point) of adjacent agents
and also spreads out attention to the target’s past timesteps
to reason about the dynamics and curvature of its trajectory.
More attention visualization can be found in Appendix C.
5. Conclusion
In this paper, we proposed a new Transformer, Agent-
Former, that can simultaneously model the time and social
dimensions of multi-agent trajectories using a sequence rep-
resentation. To preserve agent identities in the sequence,
we proposed a novel agent-aware attention mechanism that
can attend to features of the same agent differently than
features of other agents. Based on AgentFormer, we pre-
sented a stochastic multi-agent trajectory prediction frame-
work that jointly models the latent intent of all agents to pro-
duce diverse and socially-aware multi-agent future trajecto-
ries. Experiments demonstrated that our method substan-

tially improved state-of-the-art performance on challenging
pedestrian and autonomous driving datasets.
Acknowledgments. This work is supported by the Qual-
comm Innovation Fellowship.
References
[1] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan,
Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. So-
cial lstm: Human trajectory prediction in crowded spaces. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 961–971, 2016. 2, 3
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. arXiv preprint arXiv:1409.0473, 2014. 3
[3] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom.
nuscenes: A multi-
modal dataset for autonomous driving. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 11621–11631, 2020. 2, 7
[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In European Confer-
ence on Computer Vision, pages 213–229. Springer, 2020.
3
[5] Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir
Anguelov. Multipath: Multiple probabilistic anchor trajec-
tory hypotheses for behavior prediction. In Conference on
Robot Learning, pages 86–99. PMLR, 2020. 7
[6] Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using rnn
encoder-decoder for statistical machine translation.
arXiv
preprint arXiv:1406.1078, 2014. 3
[7] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and
Yoshua Bengio.
Empirical evaluation of gated recurrent
neural networks on sequence modeling.
arXiv preprint
arXiv:1412.3555, 2014. 2, 3
[8] Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou,
Tsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schnei-
der, and Nemanja Djuric. Multimodal trajectory predictions
for autonomous driving using deep convolutional networks.
In 2019 International Conference on Robotics and Automa-
tion (ICRA), pages 2090–2096. IEEE, 2019. 7
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova.
Bert:
Pre-training of deep bidirectional
transformers for language understanding.
arXiv preprint
arXiv:1810.04805, 2018. 3
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale.
arXiv preprint
arXiv:2010.11929, 2020. 3
[11] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Ji-
tendra Malik. Recurrent network models for human dynam-
ics. In Proceedings of the IEEE International Conference on
Computer Vision, pages 4346–4354, 2015. 3
[12] Francesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio
Galasso.
Transformer networks for trajectory forecasting.
ICPR, 2020. 3, 7
[13] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio.
Generative adversarial networks.
arXiv
preprint arXiv:1406.2661, 2014. 3
[14] Jiaqi Guan, Ye Yuan, Kris M Kitani, and Nicholas Rhine-
hart. Generative hybrid representations for activity forecast-
ing with no-regret learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 173–182, 2020. 3
[15] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese,
and Alexandre Alahi. Social gan: Socially acceptable tra-
jectories with generative adversarial networks. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 2255–2264, 2018. 2, 3, 7
[16] Dirk Helbing and Peter Molnar.
Social force model for
pedestrian dynamics. Physical review E, 51(5):4282, 1995.
3
[17] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term
memory. Neural computation, 9(8):1735–1780, 1997. 2, 3
[18] Yingfan Huang, Huikun Bi, Zhaoxin Li, Tianlu Mao, and
Zhaoqi Wang.
Stgat: Modeling spatial-temporal interac-
tions for human trajectory prediction. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 6272–6281, 2019. 2, 3
[19] Boris Ivanovic and Marco Pavone. The trajectron: Proba-
bilistic multi-agent trajectory modeling with dynamic spa-
tiotemporal graphs. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 2375–2384,
2019. 2, 3
[20] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 7, 12
[21] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 3
[22] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max
Welling, and Richard Zemel. Neural relational inference for
interacting systems. In International Conference on Machine
Learning, pages 2688–2697. PMLR, 2018. 3
[23] Thomas N Kipf and Max Welling. Semi-supervised classi-
ﬁcation with graph convolutional networks. arXiv preprint
arXiv:1609.02907, 2016. 2, 3, 8, 12
[24] Muhammed Kocabas, Nikos Athanasiou, and Michael J
Black.
Vibe: Video inference for human body pose and
shape estimation.
In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
5253–5263, 2020. 2
[25] Vineet Kosaraju, Amir Sadeghian, Roberto Mart´ın-Mart´ın,
Ian D Reid, Hamid Rezatoﬁghi, and Silvio Savarese. Social-
bigat: multimodal trajectory forecasting using bicycle-gan
and graph attention networks. In Advances in Neural Infor-
mation Processing Systems 2019. Neural Information Pro-
cessing Systems (NIPS), 2019. 2, 3

[26] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin
Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite
bert for self-supervised learning of language representations.
arXiv preprint arXiv:1909.11942, 2019. 3
[27] Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B
Choy, Philip HS Torr, and Manmohan Chandraker. Desire:
Distant future prediction in dynamic scenes with interacting
agents. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 336–345, 2017. 3
[28] Alon Lerner, Yiorgos Chrysanthou, and Dani Lischinski.
Crowds by example.
In Computer graphics forum, vol-
ume 26, pages 655–664. Wiley Online Library, 2007. 2, 7
[29] Jiachen Li, Fan Yang, Masayoshi Tomizuka, and Chiho
Choi. Evolvegraph: Multi-agent trajectory prediction with
dynamic relational reasoning. Advances in Neural Informa-
tion Processing Systems, 33, 2020. 3
[30] Lingyun Luke Li, Bin Yang, Ming Liang, Wenyuan Zeng,
Mengye Ren, Sean Segal, and Raquel Urtasun. End-to-end
contextual perception and prediction with interaction trans-
former. arXiv preprint arXiv:2008.05927, 2020. 3
[31] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard
Zemel.
Gated graph sequence neural networks.
arXiv
preprint arXiv:1511.05493, 2015. 3
[32] Minh-Thang Luong, Hieu Pham, and Christopher D Man-
ning. Effective approaches to attention-based neural machine
translation. arXiv preprint arXiv:1508.04025, 2015. 2
[33] Yecheng Jason Ma, Jeevana Priya Inala, Dinesh Jayara-
man, and Osbert Bastani.
Diverse sampling for normal-
izing ﬂow based trajectory forecasting.
arXiv preprint
arXiv:2011.15084, 2020. 7, 8
[34] Karttikeya Mangalam, Harshayu Girase, Shreyas Agarwal,
Kuan-Hui Lee, Ehsan Adeli, Jitendra Malik, and Adrien
Gaidon. It is not the journey but the destination: Endpoint
conditioned trajectory prediction. In European Conference
on Computer Vision, pages 759–776. Springer, 2020. 7
[35] Yajie Miao, Mohammad Gowayyed, and Florian Metze.
Eesen: End-to-end speech recognition using deep rnn mod-
els and wfst-based decoding. In 2015 IEEE Workshop on
Automatic Speech Recognition and Understanding (ASRU),
pages 167–174. IEEE, 2015. 2
[36] Jeremy Morton, Tim A Wheeler, and Mykel J Kochenderfer.
Analysis of recurrent neural networks for probabilistic mod-
eling of driver behavior. IEEE Transactions on Intelligent
Transportation Systems, 18(5):1289–1298, 2016. 3
[37] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. arXiv
preprint arXiv:1912.01703, 2019. 13
[38] Stefano Pellegrini, Andreas Ess, Konrad Schindler, and Luc
Van Gool.
You’ll never walk alone: Modeling social be-
havior for multi-target tracking. In 2009 IEEE 12th Inter-
national Conference on Computer Vision, pages 261–268.
IEEE, 2009. 2, 7
[39] Tung Phan-Minh, Elena Corina Grigore, Freddy A Boulton,
Oscar Beijbom, and Eric M Wolff. Covernet: Multimodal
behavior prediction using trajectory sets. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 14074–14083, 2020. 7, 8
[40] Danilo Rezende and Shakir Mohamed. Variational inference
with normalizing ﬂows. In International Conference on Ma-
chine Learning, pages 1530–1538. PMLR, 2015. 3
[41] Nicholas Rhinehart, Kris M Kitani, and Paul Vernaza. R2p2:
A reparameterized pushforward policy for diverse, precise
generative path forecasting. In Proceedings of the European
Conference on Computer Vision (ECCV), pages 772–788,
2018. 3
[42] Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and
Sergey Levine. Precog: Prediction conditioned on goals in
visual multi-agent settings. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 2821–
2830, 2019. 3
[43] Andrey Rudenko, Luigi Palmieri, Michael Herman, Kris M
Kitani, Dariu M Gavrila, and Kai O Arras. Human motion
trajectory prediction: A survey. The International Journal of
Robotics Research, 39(8):895–935, 2020. 3
[44] Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki
Hirose, Hamid Rezatoﬁghi, and Silvio Savarese.
Sophie:
An attentive gan for predicting paths compliant to social and
physical constraints. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
1349–1358, 2019. 3, 7
[45] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and
Marco Pavone. Trajectron++: Dynamically-feasible trajec-
tory forecasting with heterogeneous data.
arXiv preprint
arXiv:2001.03093, 2020. 2, 3, 7, 8
[46] Yichuan Charlie Tang and Ruslan Salakhutdinov. Multiple
futures prediction. arXiv preprint arXiv:1911.00997, 2019.
3
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Proceedings of the
31st International Conference on Neural Information Pro-
cessing Systems, pages 6000–6010, 2017. 2, 3, 7, 12, 13
[48] Anirudh Vemula, Katharina Muelling, and Jean Oh. Social
attention: Modeling attention in human crowds.
In 2018
IEEE international Conference on Robotics and Automation
(ICRA), pages 4601–4607. IEEE, 2018. 3
[49] Jack M Wang, David J Fleet, and Aaron Hertzmann. Gaus-
sian process dynamical models for human motion.
IEEE
transactions on pattern analysis and machine intelligence,
30(2):283–298, 2007. 3
[50] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,
Baoshan Cheng, Hao Shen, and Huaxia Xia.
End-to-
end video instance segmentation with transformers. arXiv
preprint arXiv:2011.14503, 2020. 3
[51] Xinshuo Weng, Ye Yuan, and Kris Kitani. Joint 3d track-
ing and forecasting with graph neural network and diversity
sampling. arXiv preprint arXiv:2003.07847, 2020. 3
[52] Wayne Xiong, Lingfeng Wu, Fil Alleva, Jasha Droppo, Xue-
dong Huang, and Andreas Stolcke. The microsoft 2017 con-
versational speech recognition system. In 2018 IEEE inter-
national conference on acoustics, speech and signal process-
ing (ICASSP), pages 5934–5938. IEEE, 2018. 2

[53] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron
Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua
Bengio. Show, attend and tell: Neural image caption gen-
eration with visual attention. In International conference on
machine learning, pages 2048–2057. PMLR, 2015. 2
[54] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,
Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized
autoregressive pretraining for language understanding. arXiv
preprint arXiv:1906.08237, 2019. 3
[55] Cunjun Yu, Xiao Ma, Jiawei Ren, Haiyu Zhao, and Shuai Yi.
Spatio-temporal graph transformer networks for pedestrian
trajectory prediction. In European Conference on Computer
Vision, pages 507–523. Springer, 2020. 3, 7
[56] Ye Yuan and Kris Kitani. 3d ego-pose estimation via imita-
tion learning. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 735–750, 2018. 2
[57] Ye Yuan and Kris Kitani.
Diverse trajectory forecast-
ing with determinantal point processes.
arXiv preprint
arXiv:1907.04967, 2019. 3
[58] Ye Yuan and Kris Kitani. Ego-pose estimation and forecast-
ing as real-time pd control. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 10082–
10092, 2019. 3
[59] Ye Yuan and Kris Kitani. Dlow: Diversifying latent ﬂows for
diverse human motion prediction. In European Conference
on Computer Vision, pages 346–364. Springer, 2020. 6, 7, 8
[60] Ye Yuan and Kris Kitani. Residual force control for agile
human behavior imitation and extended motion synthesis. In
Advances in Neural Information Processing Systems, 2020.
3
[61] Pu Zhang, Wanli Ouyang, Pengfei Zhang, Jianru Xue, and
Nanning Zheng.
Sr-lstm: State reﬁnement for lstm to-
wards pedestrian trajectory prediction.
In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 12085–12094, 2019. 2
[62] Tianyang Zhao, Yifei Xu, Mathew Monfort, Wongun Choi,
Chris Baker, Yibiao Zhao, Yizhou Wang, and Ying Nian Wu.
Multi-agent tensor fusion for contextual trajectory predic-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 12126–12134,
2019. 3

A. Handling a Time-Varying Number of Agents
For clarity and ease of exposition, we assume the number of agents remains the same across timesteps in the main paper.
However, this assumption is not necessary, and our method can easily generalize to use cases where the number of agents
changes over time due to agents going out of the scene or being missed by detection. We illustrate how to apply our method
to such cases in Fig. 5. Owning to the ﬂexible sequence representation we employ for multi-agent trajectories, we can simply
remove the features of missing agents at each timestep from the sequence. The reason why we do not need to ﬁll the missing
features is that our method uses time encoding to preserve time information, unlike RNNs which have to use recurrence to
encode timesteps and thus necessitate the features of all timesteps. As the number of agents is no longer N for all timesteps,
the computation of the mask M in agent-aware attention needs to be changed accordingly:
Mij = 1(Agent(i) = Agent(j))
(10)
where Agent(·) extracts the agent index of a query/key and 1(·) denotes the indicator function. An example of mask M is
shown in Fig. 5 (Right).
Time
Social
Agent-Aware Transformer
Multi-Agent Trajectories
Trajectory Features
Trajectory Features in 2D
t = 1
t = 2
t = 3
t = 4
t = 5
t = 1
t = 2
t = 3
t = 4
t = 5
Agent 3
Agent 2
Agent 1
Mask
Missing Feature
Missing Trajectory
Figure 5. Our method can naturally handle a time-varying number of agents because of the ﬂexible sequence representation of multi-agent
trajectories. We can simply remove the trajectory features of missing agents at each timestep from the sequence. The mask M of the
example sequence (when applying self-attention) is computed based on the agreement of agent identity between each query and key.
B. Additional Implementation Details
Encoding Semantic Maps. The semantic map In ∈RH×W ×C for each agent n has spatial dimensions (100, 100) with 3
meters between adjacent pixels. It has C = 3 channels annotating drivable areas, road dividers, and lane dividers obtained
using the ofﬁcial nuScenes software development kit. Since the semantic map is relatively easy to parse, we use a simple
hand-designed CNN to extract visual features vn from it. In particular, the CNN has four convolutional layers with channels
(32, 32, 32, 1), kernel size (5, 5, 5, 3), and strides (2, 2, 1, 1). A ﬁnal linear layer is used to obtain a 32-dimensional feature.
Training Trajectory Sampler. The scaling factor σd in the trajectory sampler loss Lsamp (Eq. (9) in the main paper) is set
to 5 for ETH/UCY and 20 for nuScenes. We clip the maximum value of the KL term in Lsamp down to 2. We train the
trajectory sampler using the Adam optimizer [20] for 50 epochs on ETH/UCY and nuScenes. We use an initial learning rate
of 10−4 and halve the learning rate every 5 epochs.
Ablation Study Details. We ﬁrst provide details for the ablation study of separate social and temporal models (ﬁrst group
of Table 3 and 4 in the main paper). We ﬁrst use a temporal model (LSTM or Transformer) to extract the temporal feature of
each agent and then apply a social model (GCN [23] or Transformer) over the temporal features to obtain social features for
each agent; ﬁnal trajectories are decoded from the social features using either an LSTM or Transformer. For the GCN, we use
two graph convolutional layers with channels (256, 256) and residual connections within each layer. The hidden dimensions
of the LSTMs are set to 256. The Transformers have two layers with key/query dimensions 256 and 8 heads; the feedforward
layer has 512 hidden units, and the dropout ratio is 0.1. We use the positional encoding [47] for the temporal Transformer
but not for the social Transformer as agents are permutation-invariant.
Next, we provide details for the ablation study of each key technical component (second group of Table 3 and 4 in the
main paper). For the variant without joint latent modeling (“w/o joint latent”), we append the latent codes to the trajectory

sequence after the AgentFormer decoder instead of before the decoder. In this way, the latent code of one agent will not affect
the future trajectory of another agent. For the variant without the agent-aware attention (“w/o AA attention”), we replace our
agent-aware attention with standard scaled dot-product attention used in the original transformer [47]. For the variant with
agent encoding (“w/ agent encoding”), in addition to removing the agent aware attention, we also append an agent encoding
to each element in the trajectory sequence. The agent encoding is computed similarly as the positional encoding [47] but
uses the agent index instead of the position index. For the variant without semantic maps (“w/o semantic map”), we simply
do not append any visual features extracted from the semantic maps to the trajectory sequence.
Other Details. Our models are implemented using PyTorch [37] and are trained with a single NVIDIA RTX 2080 Ti and
standard CPUs. The training time is approximately one day for each dataset in ETH/UCY and three days for nuScenes.
C. Additional Attention Visualization
As discussed in the main paper, our method can attend to any agent at any previous timestep when predicting the future
position of an agent. Here, we provide more visualization of the attention in Fig. 6 to understand the behavior of our model.
Across all the examples, it is evident that when predicting the target future position of an agent, the model pays more attention
to the agent’s own trajectories and recent timesteps, and it also attends more to nearby agents than distant agents.
0.0
0.2
Past Attention Range:
0.0
0.2
Future Attention Range:
Target Position (Being Predicted)
Attention to Past Feature
Attention to Future Feature
Past Trajectory
Future Trajectory
Figure 6. Attention Visualization on ETH/UCY. We plot the attention to past (blue) and future (green) trajectory features of all agents
when inferring a target position (red). Darker color means higher attention. When predicting the target future position of an agent, the
model pays more attention to the agent’s own trajectories and recent timesteps, and it also attends more to nearby agents than distant agents.

D. Trajectory Sample Visualization
To demonstrate the importance of agent-aware attention, we also provide qualitative comparisons of our method against
the variant without agent-aware attention (w/o AA attention) on the nuScenes dataset in Fig. 7. We can observe that the
future trajectory samples produced by our method using agent-aware attention cover the ground truth (GT) future trajectories
signiﬁcantly better. Our method also produces much fewer implausible trajectories such as those going out of the road.
Ours
Ours
w/o AA attention
w/o AA attention
Past Trajectory
Future Trajectory Samples
GT Future Trajectory
Road
Walkway
Undrivable Area
Figure 7. Trajectory Sample Visualization on nuScenes. We compare our method against the variant without agent-aware attention (w/o
AA attention). The future trajectory samples produced by our method using agent-aware attention cover the ground truth (GT) future
trajectories signiﬁcantly better. Our method also produces much fewer implausible trajectories such as those going out of the road.

