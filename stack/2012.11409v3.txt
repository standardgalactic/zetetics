3D Object Detection with Pointformer
Xuran Pan1* Zhuofan Xia1* Shiji Song1 Li Erran Li2† Gao Huang1‡
1Department of Automation, Tsinghua University, Beijing, China
Beijing National Research Center for Information Science and Technology (BNRist),
2Alexa AI, Amazon / Columbia University
{pxr18, xzf20}@mails.tsinghua.edu.cn,
erranlli@gmail.com,
{shijis, gaohuang}@tsinghua.edu.cn
Abstract
Feature learning for 3D object detection from point
clouds is very challenging due to the irregularity of 3D
point cloud data. In this paper, we propose Pointformer,
a Transformer backbone designed for 3D point clouds to
learn features effectively. Speciﬁcally, a Local Transformer
module is employed to model interactions among points in
a local region, which learns context-dependent region fea-
tures at an object level. A Global Transformer is designed
to learn context-aware representations at the scene level.
To further capture the dependencies among multi-scale rep-
resentations, we propose Local-Global Transformer to in-
tegrate local features with global features from higher res-
olution. In addition, we introduce an efﬁcient coordinate
reﬁnement module to shift down-sampled points closer to
object centroids, which improves object proposal genera-
tion. We use Pointformer as the backbone for state-of-the-
art object detection models and demonstrate signiﬁcant im-
provements over original models on both indoor and out-
door datasets. Code and pre-trained models are available
at https://github.com/Vladimir2506/Pointformer.
1. Introduction
3D object detection in point clouds is essential for many
real-world applications such as autonomous driving [10]
and augmented reality [20]. Compared to images, 3D point
clouds can provide detailed geometry and capture 3D struc-
ture of the scene. On the other hand, point clouds are irreg-
ular, which can not be processed by powerful deep learn-
ing models, such as convolutional neural networks directly.
This poses a big challenge for effective feature learning.
The common feature processing methods in 3D detec-
*Equal contribution.
†Work done prior to Amazon.
‡Corresponding author.
Image of the scene
Ground truth
Top-50 attention
Top-100 attention
Top-200 attention
Figure 1. Attention maps directly from Pointformer block,
darker blue indicates stronger attention.
For the key point
(star), Pointformer ﬁrst focuses on the local region of the same
object (the back of the chair), then spreads the attention to other
regions (the legs), ﬁnally attends to points from other objects glob-
ally (other chairs), leveraging both local and global dependencies.
tion can be roughly categorized into three types, based on
the form of point cloud representations. Voxel-based ap-
proaches [30, 12, 45] gridify the irregular point clouds into
regular voxels and are followed by sparse 3D convolutions
to learn high dimensional features. Though effective, voxel-
based approaches face the dilemma between efﬁciency and
accuracy. Speciﬁcally, using smaller voxels gains more pre-
cision, but suffers from higher computational cost. Con-
versely, using larger voxels misses potential local details in
the crowded voxels.
Alternatively, point-based approaches [27], inspired by
the success of PointNet [23] and its variants, consume raw
points directly to learn 3D representations, which mitigates
the drawback of converting point clouds to some regular
structures. Leveraging learning techniques for point sets,
point-based approaches avoid voxelization-induced infor-
mation loss and take advantage of the sparsity in point
clouds by only computing on valid data points. Neverthe-
less, due to the irregularity of point cloud data, point-based
learning operations have to be permutation-invariant and
1
arXiv:2012.11409v3  [cs.CV]  22 Jun 2021

Scene Point Cloud
3D Boxes
Local
Transformer
Local-Global
Transformer
Global
Transformer
Keypoints Abstraction
Multiscale Cross-Attention
Output from Previous Block
Context Aggregation
Pointformers
Upsampling
Feature Learning Module
Detection
Heads
Learned Representations
A Pointformer Block
 Pointformer Block
 Upsample Block
Figure 2. The Pointformer backbone for 3D object detection in point clouds. A basic feature learning block consists of three parts: a Local
Transformer to model interactions in the local region; a Local-Global Transformer to integrate local features with global information; a
Global Transformer to capture context-aware representations at the scene level.
adaptive to the input size. To achieve this, it learns sim-
ple symmetric functions (e.g. using point-wise feedforward
networks with pooling functions) which highly restricts its
representation power.
Hybrid approaches [44, 16, 42, 26] attempt to combine
both voxel-based and point-based representations. [44, 16]
leverages PointNet features at the voxel level and a column
of voxels (pillar) level respectively. [42, 26] deeply inte-
grate voxel features and PointNet features at the scene level.
However, the fundamental difference between the two rep-
resentations could pose a limit on the effectiveness of these
approaches for 3D point-cloud feature learning.
To address the above limitations, we resort to the Trans-
former [33] models, which have achieved great success
in the ﬁeld of natural language processing. Transformer
models [8] are very effective at learning context-dependent
representations and capturing long range dependencies in
the input sequence.
Transformer and the associate self-
attention mechanism not only meet the demand of permu-
tation invariance, but also are proved to be highly expres-
sive. Speciﬁcally, [6] proves that self-attention is at least
as expressive as convolution. Currently, self-attention has
been successfully applied to classiﬁcation [25] and 2D ob-
ject detection [2] in computer vision. However, the straight-
forward application of Transformer to 3D point clouds is
prohibitively expensive because computation cost grows
quadratically with the input size.
To this end, we propose Pointformer, a backbone for 3D
point clouds to learn features more effectively by leveraging
the superiority of the Transformer models on set-structured
data. As shown in Figure 2, Pointformer is a U-Net structure
with multi-scale Pointformer blocks. A Pointformer block
consists of Transformer-based modules that are both expres-
sive and friendly to the 3D object detection task. First, a
Local Transformer (LT) module is employed to model in-
teractions among points in the local region, which learns
context-dependent region features at an object level. Sec-
ond, a coordinate reﬁnement module is proposed to ad-
just centroids sampled from Furthest Point Sampling (FPS)
which improves the quality of generated object proposals.
Third, we propose Local-Global Transformer (LGT) to inte-
grate local features with global features from higher resolu-
tion. Finally, Global Transformer (GT) module is designed
to learn context-aware representations at the scene level. As
illustrated in Figure 1, Pointformer can capture both local
and global dependencies, thus boosting the performance of
feature learning for scenes with multiple cluttered objects.
Extensive experiments have been conducted on several
detection benchmarks to verify the effectiveness of our ap-
proach. We use the proposed Pointformer as the backbone
for three object detection models, CBGS [45], VoteNet [21],
and PointRCNN [27], and conduct experiments on three in-
door and outdoor datasets, SUN-RGBD [29], KITTI [10],
and nuScenes [1] respectively. We observe signiﬁcant im-
provements over the original models on all experiment set-
tings, which demonstrates the effectiveness of our method.
In summary, we make the following contributions:
• We propose a pure transformer model, Pointformer,
which serves as a highly effective feature learning
backbone for 3D point clouds. Pointformer is permu-
tation invariant, local and global context-aware.
2

Positional Encoding
Furthest Point 
Sampling
Grouping
Features
Self-attention
Coordinate Refinement
Max-pooling
FFN
Attention maps
Transformer Block  x2
FFN
Figure 3. Illustration of the Local Transformer. Input points are ﬁrst down-sampled by FPS and generate local regions by ball query.
Transformer block takes point features and coordinates as input and generate aggregated features for the local region. To further adjust the
centroid points, attention maps from the last Transformer layer are adopted for coordinate reﬁnement. As a result, points are pushed closer
to the object centers instead of surfaces.
• We show that Pointformer can be easily applied as the
drop-in replacement backbone for state-of-the-art 3D
object detectors for the point cloud.
• We perform extensive experiments using Pointformer
as the backbone for three state-of-the-art 3D object
detectors, and show signiﬁcant performance gains on
several benchmarks including both indoor and out-
door datasets. This demonstrates that the versatility
of Pointformer as 3D object detectors are typically de-
signed and optimized for either indoor or outdoor only.
2. Related Work
Feature learning for 3D point clouds. Prior work includes
feature learning on voxelized grids, direct feature learning
on point clouds and the hybrid of the two. 3D sparse con-
volution [11] is very effective on voxel grids. For direct
feature learning, PointNet [23] and PointNet++ [24] learn
point-wise features and region features using feed-forward
networks and simple symmetric functions (e.g. max) re-
spectively. PCCN [36] generalizes convolution to non-grid
structured data by exploiting parameterized kernel func-
tions that span the full continuous vector space. EdgeConv
[37] exchanges local neighborhood information and acts on
graphs dynamically computed in each layer of the network.
Hybrid methods combine both types of features at the local
level [44, 16] or at the network level [42, 26].
Transformers in computer vision. Image GPT [3] is the
ﬁrst to adopt the Transformers in 2D image classiﬁcation
task for unsupervised pretraining.
Further, ViT [9] ex-
tends this scheme to large scale supervised learning on im-
ages. For high level vision tasks, DETR [2] and Deformable
DETR [46] leverage the advantages of Transformers in 2D
object detection. Set Transformer [17] uses attention mech-
anisms to model interactions among elements in the input
set. In the ﬁeld of 3D vision, PAT [41] designs novel group
shufﬂe attentions to capture long range dependencies in
point clouds. To the best of our knowledge, we are the ﬁrst
to propose a pure Transformer model for 3D points clouds
feature learning with carefully designed Transformer blocks
and a positional encoding module to capture geometric and
rich context information.
3D object detection in point clouds. Detectors are de-
signed either with point clouds as the only input [45, 44, 16,
42, 26, 27, 21, 28, 38, 43] or fusing multiple sensor modal-
ities such as LiDAR and camera [22, 18, 34]. Their back-
bones are designed with the aforementioned feature learn-
ing approaches. We focus on point cloud only object de-
tection. In this category, VoxelNet [44] divides the point
cloud into voxels, followed by 3D convolutions to extract
features. VoteNet [21] devises a novel 3D proposal mecha-
nism using deep Hough voting, before H3DNet [43] makes
further investigations on geometric primitives. In addition,
MLCVNet [38] focuses more on contextual information ag-
gregation based on VoteNet, and PointGNN [28] exploits
graph learning methods in point cloud detection. We show
that our novel Transformer based model, Pointformer, can
be used as a drop-in replacement for voxel-based detec-
tor, CBGS [45] and point-based detectors, VoteNet [21] and
PointRCNN [27].
3. Pointformer
Feature learning for 3D point clouds needs to confront
its irregular and unordered nature as well as its varying size.
Prior work utilizes simple symmetric functions, e.g., point-
wise feedforward networks with pooling functions [23, 24],
or resorts to the techniques in graph neural networks by
aggregating information from the local neighborhood [37].
However, the former is not effective in incorporating lo-
cal context-dependent features beyond the capability of the
simple symmetric functions; the latter focuses on the mes-
sage passing between the center point and its neighbors
while neglecting the feature correlations among the neigh-
bor points. Additionally, global representations are also in-
formative but rarely used in 3D object detection tasks.
In this paper, we design Transformer-based modules for
3

point set operations which not only increase the expres-
siveness of extracting local features, but incorporate global
information into point representations as well. As shown
in Figure 2, a Pointformer block mainly consists of three
parts: Local Transformer (LT), Local-Global Transformer
(LGT) and Global Transformer (GT). For each block, LT
ﬁrst receives the output from its previous block (high res-
olution) and extracts features for a new set with fewer el-
ements (low resolution). Then, LGT uses the multi-scale
cross-attention mechanism to integrate features from both
resolutions. Lastly, GT is adopted to capture context-aware
representations. As for the up-sampling block, we follow
PointNet++ and adopt the feature propagation module for
its simplicity.
3.1. Background
We ﬁrst revisit the general formulation of the Trans-
former model. Let F = {fi} and X = {xi} denote a set
of input features and their positions, where fi and xi repre-
sent the feature and position of token i, respectively. Then, a
Transformer block comprises of a multi-head self-attention
module and feedforward network:
q(m)
i
= fiW (m)
q
, k(m)
i
= fiW (m)
k
, v(m)
i
= fiW (m)
v
,
(1)
y(m)
i
=
X
j
σ(q(m)
i
k(m)
j
/
√
d + PE(xi, xj))v(m)
j
,
(2)
yi = fi + Concat(y(0)
i
, y(1)
i
, . . . , y(M−1)
i
),
(3)
oi = yi + FFN(yi),
(4)
where Wq, Wk, Wv are projections for query, key and value.
m is the index of M attention heads and d is the feature di-
mension. PE(·) is the positional encoding function for in-
put positions, and FFN(·) represents a position-wise feed-
forward network. σ(·) is a normalization function and Soft-
Max is mostly adopted.
In the following sections, for simplicity, we use
O = Transblock(F, PE(X)),
(5)
to represent the basic Transformer block (Eq.(1)∼Eq.(4)).
Readers can refer to [33] for further details.
3.2. Local Transformer
In order to build a hierarchical representation for a point
cloud scene, we follow the high level methodology to build
feature learning blocks on different resolutions [24]. Given
an input point cloud P = {x1, x2, . . . , xN}, we ﬁrst use
furthest point sampling (FPS) to choose a subset of points
{xc1, xc2, . . . , xcN′ } as a set of centroids. For each cen-
troid, ball query is applied to generate K points in the local
region within a given radius. Then we group these features
around the centroids, and feed them as a point sequence to
a Transformer layer, as shown in Figure 3. Let {xi, fi}t de-
note the local region for tth centroid, where xi ∈R3 and
fi ∈RC represent the coordinates and features of the i-th
points in the group, respectively. Subsequently, a shared L-
layer Transformer block is applied to all local regions which
receives the input of {xi, fi}t as follows:
f (0)
i
= FFN(fi), ∀i ∈N(xct),
(6)
F (l+1) =Transblock(F (l), PE(X)), l=0, .., L −1,
(7)
where F = {fi|i ∈N(xct)} and X = {xi|i ∈N(xct)}
denote the set of features and coordinates in the local region
with centroid xct.
Compared to the existing local feature extraction mod-
ules in [39, 40, 32], the proposed Local Transformer has
several advantages. First, the dense self-attention operation
in the Transformer block greatly enhances its expressive-
ness. Several graph learning based approaches can be ap-
proximated as special cases of the LT module with learned
parameter space carefully designed. For instance, a gener-
alized graph feature learning function can be formulated as:
eij = FFN(FFN(xi ⊕xj) + FFN(fi ⊕fj)),
(8)
f ′
i = A(σ(eij) × FFN(fj), ∀j ∈N(xi)),
(9)
where most of the models utilize summation as the ag-
gregation function A and the operation ⊕is chosen
from {Concatenation, Plus, Inner-product}.
Therefore,
the edge function eij is at most a quadratic function of
{xi, xj, fi, fj}.
For a one-layer Transformer block, the
learning module can be formulated with the inner-product
self-attention mechanism as follows:
eij = fiWqW T
k f T
j
√
d
+ PE(xi, xj),
(10)
f ′
i = A(σ(eij × FFN(fj), ∀j ∈N(xi)),
(11)
where d is the feature dimension of fi and fj. We can ob-
serve that the edge function is also a quadratic function of
{xi, xj, fi, fj}. With sufﬁcient number of layers in FFNs,
the graph-based feature learning module has the same ex-
pressive power as a one-layer Transformer encoder. When
it comes to Pointformer, as we stack more Transformer lay-
ers in the block, the expressiveness of our module is further
increased and can extract better representations.
Moreover, feature correlations among the neighbor
points are also considered, which are commonly omitted
in other models.
Under some circumstances, neighbor
points can be even more informative than the centroid point.
Therefore, by leveraging message passing among all points,
features in the local region are equally considered, which
makes the local feature extraction module more effective.
4

3.3. Coordinate Reﬁnement
Furthest point sampling (FPS) is widely used in many
point cloud frameworks, as it can generate a relatively
uniform sampled points while keeping the original shape,
which ensures that a large fraction of the points can be cov-
ered with limited centroids. However, there are two main
issues in FPS: (1) It is notoriously sensitive to the outlier
points, leading to highly instability especially when deal-
ing with real-world point clouds. (2) Sampled points from
FPS are a subset of original point clouds, which makes it
challenging to infer the original geometric information in
the cases that objects are partially occluded or not enough
points of an object are captured. Considering that points are
mostly captured on the surface of objects, the second issue
may become more critical as the proposals are generated
from sampled points, resulting in a natural gap between the
proposal and ground truth.
To overcome the aforementioned drawbacks, we propose
a point coordinate reﬁnement module with the help of the
self-attention maps. As shown in Figure 3, we ﬁrst take out
the self-attention map of the last layer of the Transformer
block for each attention head. Then, we compute the aver-
age of the attention maps and utilize the particular row for
the centroid point as a weight vector:
W = 1
M
M
X
m=1
A(m)
0,: ,
(12)
where M represents the number of attention heads and
A(m) is the attention map for the mth head. Lastly, the
reﬁned centroid coordinates are computed as weighted av-
erage of all points in the local region:
x′
ct =
K
X
k=1
wkxk,
(13)
where wk is the kth entry of W. With the proposed co-
ordinate reﬁnement module, centroid points are adaptively
moving closer to object centers. Moreover, by utilizing the
self-attention map, our module introduces little computa-
tional cost and no additional learning parameters, making
the reﬁnement process more efﬁcient.
3.4. Global Transformer
Global information representing scene contexts and fea-
ture correlations between different objects is also valuable
in the detection tasks. Prior work using PointNet++ [24]
or sparse 3D convolution to extract high level features for
3D point clouds enlarges the receptive ﬁeld as the depth of
their networks increases. However, this has limitations on
modeling long-range interactions.
As a remedy, we leverage the power of Transformer
modules on modeling non-local relations and propose a
Global Transformer to achieve message passing through the
whole point cloud. Speciﬁcally, all points are gathered to a
single group P and serves as input to a Transformer module.
The formulation for GT is summarized as follows:
f (0)
i
= FFN(fi), ∀i ∈P,
(14)
F (l+1) =Transblock(F (l),PE(X)), l=0, .., L −1. (15)
By leveraging the Transformer on the scene level, we
can capture the context-aware representations and promote
message passing among different objects. Moreover, global
representations can be particularly helpful for detecting ob-
jects with very few points.
PE(xi, xj) = FFN(xi −xj).
(16)
3.5. Local-Global Transformer
Local-Global Transformer is also a key module to com-
bine the local and global features extracted by the LT and
GT modules. As shown in Figure 2, the LGT adopts a multi-
scale cross-attention module and generates relations be-
tween low resolution centroids and high resolution points.
Formally, we apply cross attention similar to the encoder-
decoder attention used in Transformer. The output of LT
serves as query and the output of GT from the higher resolu-
tion is used as key and value. With the L-layer Transformer
block, the module is formulated as:
f (0) = FFN(fi), ∀i ∈Pl,
(17)
f ′
j = FFN(fj), ∀j ∈Ph,
(18)
F (l+1) =Transblock(F (l), F ′
j, PE(X)),l=0,..,L−1, (19)
where Pl (keypoints, the output of LT in Figure 2) and Ph
(the input of a Pointformer block in Figure 2) represent sub-
samples of point cloud P from low and high resolution re-
spectively. Through the Local-Global Transformer module,
we utilize whole centroid points to integrate global informa-
tion via an attention mechanism, which makes the feature
learning of both more effective.
3.6. Positional Encoding
Positional encoding is an integral part of Transformer
models as it is the only mechanism that encodes position
information for each token in the input sequence. When
adapting Transformers for 3D point cloud data, positional
encoding plays a more critical role as the coordinates of
point clouds are valuable features indicating the local struc-
tures. Compared to the techniques used in natural language
processing, we propose a simple and yet efﬁcient approach.
For all Transformer modules, coordinates of each input
point are ﬁrstly mapped to the feature dimension. Then,
we subtract the coordinates of the query and key points and
use relative positions for encoding. The encoding function
is formalized as:
5

Method
Modality
Car(IoU=0.7)
Pedestrian (IoU=0.5)
Cyclist (IOU=0.5)
Easy
Moderate
Hard
Easy
Moderate
Hard
Easy
Moderate
Hard
PointRCNN [27]
LiDAR
85.94
75.76
68.32
49.43
41.78
38.63
73.93
59.60
53.59
+ Pointformer
LiDAR
87.13
77.06
69.25
50.67
42.43
39.60
75.01
59.80
53.99
Table 1. Performance comparison of PointRCNN with and without Pointformer on KITTI test split by submitting to ofﬁcial test server.
The evaluation metric is Average Precision(AP) with IoU threshold 0.7 for car and 0.5 for pedestrian/cyclist.
Method
Modality
Car
Ped
Bus
Barrier
TC
Truck
Trailer
Moto
Cons. Veh.
Bicycle
mAP
CBGS [45]
LiDAR
81.1
80.1
54.9
65.7
70.9
48.5
42.9
51.5
10.5
22.3
52.8
+ Pointformer
LiDAR
82.3
81.8
55.6
66.0
72.2
48.1
43.4
55.0
8.6
22.7
53.6
Table 2. Performance comparison of PointRCNN with and without Pointformer on the nuScenes benchmark.
Method
Car(IoU=0.7)
Easy
Moderate
Hard
PointRCNN
88.88
78.63
77.38
+ Pointformer
90.05
79.65
78.89
Table 3. Performance comparison of PointRCNN with and without
Pointformer on the car class of KITTI val split set.
RoIs
Recall(IoU=0.5)
Recall(IoU=0.7)
PointRCNN +Pointformer PointRCNN +Pointformer
10
86.66
87.51
29.87
35.46
50
96.01
96.52
40.28
42.45
100
96.79
96.91
74.81
75.82
200
98.03
97.99
76.29
76.51
Table 4. Recall of proposal generation network with different num-
ber of RoIs and 3D IoU thresholds for the car class on the val split
at moderate difﬁculty.
3.7. Computational Cost Reduction
Since Pointformer is a pure attention model based on
Transformer blocks, it suffers from extremely heavy com-
putational overhead. Applying a conventional Transformer
to a point cloud with n points consumes O(n2) time and
memory, leading to much more training cost.
Some recent advances in efﬁcient Transformers have
mitigated this issue [15, 13, 35, 4, 38], among which Lin-
former [35] reduces the complexity to O(n) by low-rank
factorization of the original attention. Under the hypothesis
that the self attention mechanism is low rank, i.e. the rank
of the n × n attention matrix
A = softmax
 
QK⊤
√dk
!
,
(20)
is much smaller than n, Linformer projects the n-dimension
keys and values to the ones with lower dimension k ≪n,
and k is closer to the rank of A. Therefore, the i-th head in
the projected multi-head self-attention is
headi = softmax
 
Q(EiK)⊤
√dk
!
FiV,
(21)
where Ei, Fi ∈Rk×n are the projection matrices, which
reduces the complexity from O(n2) to O(kn).
Compared with the Taylor expansion approximation
technique used in MLCVNet [38], Linformer is easier to
implement in out method. We thus adopt it to replace the
Transformer layers in the vanilla Pointformer. Practically,
we map the number of points n to k = n
r , where r is a fac-
tor controlling the number of projected dimensions. We ap-
ply this mapping in Local Transformer, Global Transformer
and Local-Global Transformer blocks. By setting an appro-
priate factor r for each block, there would be a signiﬁcant
boost in both time and space consumption with little perfor-
mance decay.
4. Experimental Results
In this section, we use Pointformer as the backbone for
state-of-the-art object detection models and conduct ex-
periments on several indoor and outdoor benchmarks. In
Sec. 4.1, we introduce the implementation details of the ex-
periments. In Sec. 4.2 and Sec. 4.3, we show the compari-
son results on indoor and outdoor datasets respectively. In
Sec. 4.4, we conduct extensive ablation studies to analyze
our proposed Pointformer model. Finally, we show qualita-
tive results in Sec. 4.5. More analysis and visualizations are
provided in the appendix.
4.1. Experimental Setup
Datasets. We adopt SUN RGB-D [29] and ScanNet V2 [7]
for indoor 3D detection benchmark. SUN RGB-D has 5K
6

Method
bathtub
bed
bookshelf
chair
desk
dresser
nightstand
sofa
table
toilet
mAP
VoteNet [21]
74.4
83.0
28.8
75.3
22.0
29.8
62.2
64.0
47.3
90.1
57.7
VoteNet*
75.5
85.6
32.0
77.4
24.8
27.9
58.6
67.4
51.1
90.5
59.1
+ Pointformer
80.1
84.3
32.0
76.2
27.0
37.4
64.0
64.9
51.5
92.2
61.1
Table 5. Perfomance comparison of VoteNet with and without Pointformer on SUN RGB-D validation dataset. The evaluation metric is
Average Precision with 0.25 IoU threshold.* denotes the model implemented in MMDetection3D [5].
Method
cab bed chair sofa table door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP
VoteNet [21]
36.3 87.9 88.7 89.6 58.8 47.3 38.1 44.6
7.8 56.1 71.7 47.2 45.4
57.1 94.9 54.7 92.1 37.2 58.6
VoteNet*
47.7 88.7 89.5 89.3 62.1 54.1 40.8 54.3 12.0 63.9 69.4 52.0 52.5
73.3 95.9 52.0 95.1 42.4 62.9
+ Pointformer 46.7 88.4 90.5 88.7 65.7 55.0 47.7 55.8 18.0 63.8 69.1 55.4 48.5
66.2 98.9 61.5 86.7 47.4 64.1
Table 6. Performance comparison of VoteNet with and without Pointformer on ScanNetV2 validation dataset. The evaluation metric is
Average Precision with 0.25 IoU threshold.* denotes the model implemented in MMDetection3D [5].
training images annotated with oriented 3D bounding boxes
for 37 object categories and ScanNet V2 has 1513 labeled
scenes with 40 semantic classes. We follow the same setting
in VoteNet [21] and report performance on the 10 classes on
SUN RGB-D and 18 classes on ScanNet V2. For outdoor
datasets, we choose KITTI [10] and nuScenes [1] for eval-
uation. KITTI contains 7,481 training samples and 7,518
test samples for autonomous driving. NuScenes contains
1k different scenes with 40K key frames, which has 23 cat-
egories and 8 attributes. We follow the evaluation protocol
proposed along with the datasets.
Experimental setups. We use the Pointformer as the back-
bone for three 3D detection models, including VoteNet [21],
PointRCNN [27] and CBGS [45]. VoteNet is a point-based
approach for indoor datasets, while PointRCNN and CBGS
are adopted for outdoor datasets. PointRCNN is a classic
approach for autonomous driving detection and CBGS is
the champion of nuScenes 3D detection Challenge held in
CVPR 2019. For a fair comparison, we adopt the same de-
tection head, number of points for each resolution, hyper-
parameters and training conﬁgurations as baseline models.
4.2. Outdoor Datasets
KITTI. We ﬁrst evaluate our method comparing with
PointRCNN on KITTI’s 3D detection benchmark. PointR-
CNN uses PointNet++ as its backbone with four set abstrac-
tion layers. Similarly, we adopt the same architecture, while
switching the set abstraction layer in PointNet++ with the
proposed Transformer block. The comparison results on the
KITTI test server are shown in Table 1.
For the car category, we also report the performance of
3D detection results on the val split as shown in Table 3.
As we can observe, by adopting Pointformer, our model
achieves consistent improvements comparing to the original
PointRCNN. Especially in the hard difﬁculty, our method
shows the most promising result with 1.5% AP improve-
ment. We believe the better performance on hard objects is
attributed to the higher expressiveness of local Transformer
module. For hard objects which are often small or occluded,
GT captures context-dependent region features, which con-
tributes to the bounding box regression and classiﬁcation.
Additionally, we evaluate the performance of proposal
generation network by calculating the recall of 3D bounding
box with various number of proposals and 3D IoU thresh-
old. As shown in Table 4, our backbone module signif-
icantly enhances the performance of proposal generation
network under almost all the settings. Analyzing the ﬁg-
ures vertically, we observe that our backbone shows bet-
ter performance when the number of RoIs are relatively
small. As stated in Sec.3, the GT and LGT help to cap-
ture context-aware representations and models the relations
among different objects (proposals). This provides addi-
tional references for locating and reasoning the bounding
boxes. Therefore, despite the lack of RoIs, we can still im-
prove the performance of the proposal generation module
and achieve higher recall.
NuScenes.
We also validate the effectiveness of Point-
former on the nuScenes dataset, which greatly extends
KITTI in dataset size, number of object categories and
number of annotated objects. Furthermore, nuScenes suf-
fers from severe class imbalance issues, making the detec-
tion task more difﬁcult and challenging. In this part, we
adopt CBGS, the champion of nuScenes 3D detection Chal-
lenge held in CVPR 2019, as the baseline model and show
the comparison results when replacing the backbone with
Pointformer.
We summarize the results in Table 2.
As
we can observe, by utilizing Pointformer as the backbone,
our model achieves 0.8 higher mAP than baseline. For 8
of 10 classes, our model shows better performance, which
demonstrates the effectiveness of Pointformer on larger and
7

LT
GT
LGT
CoRe
Car (IoU=0.7)
Easy
Moderate
Hard
1
-
-
-
-
88.88
78.63
77.38
2
✓
-
-
-
89.46
78.91
77.65
3
✓
-
-
✓
89.76
79.24
78.43
4
✓
✓
-
-
89.68
79.22
78.52
5
✓
✓
✓
-
89.82
79.34
78.62
6
✓
✓
✓
✓
90.05
79.65
78.89
Table 7. Effects of each component on the val split of KITTI. CoRe
represents the coordinates reﬁnement module.
Positional Encoding
Car (IoU=0.7)
Easy
Moderate
Hard
1
-
85.42
75.67
72.34
2
✓
90.05
79.65
78.89
Table 8. Effects of positional encoding on the val split of KITTI.
more challenging datasets.
4.3. Indoor Datasets
We evaluate our Pointformer accompanied by VoteNet [21]
on SUN RGB-D and ScanNet V2. We follow the same hy-
perparameters on the backbone structure as VoteNet. Fol-
lowed by the Pointformer blocks, two feature propaga-
tion(FP) modules proposed in PointNet++ [24] serve as up-
samplers to increase the resolution for the subsequent de-
tection heads.
SUN RGB-D. We report the average precision(AP) over
10 common classes in SUN RGB-D, as shown in Table 5.
Compared with the PointNet++ [24] in VoteNet [21], our
Pointformer provides a signiﬁcant boost with 2% mAP over
the implementation in MMDetection3D [5]. On some cate-
gories with large and complex objects like dresser or bath-
tub, Pointformer shows its splendid capability on extract-
ing non-local information by a sharp increase over 5% AP,
which we attribute to the GT module in Pointformer.
ScanNet V2. We report the average precision(AP) over 18
classes in ScanNet V2, as shown in Table 6. Compared
with VoteNet, Pointformer outperforms its original version
by 1.2% mAP with MMDetection3D.
4.4. Ablation Study
In this section, we conduct extensive ablation experi-
ments to analyze the effectiveness of different components
of Pointformer. All experiments are trained on the train split
with PointRCNN detection head and evaluated on the val
split with the car class.
Effects of each component. We validate the effectiveness
of each Transformer component and the coordinate reﬁne-
ment module, and summarized the results in Table 7. The
ﬁrst row corresponds to the PointRCNN baseline and the
last row is the full Pointformer model. By comparing the
ﬁrst row and second row, we can observe that easy objects
beneﬁt more from the local Transformer with 0.6 AP im-
provement. By comparing the second row and fourth row,
we can see that global Transformer is more suitable for hard
objects with 0.9 AP improvement. This observation is con-
sistent with our analysis in Sec. 4.2. As for Local-Global
Transformer and coordinate reﬁnement, the improvement is
similar under three difﬁculty settings.
Positional Encoding. Playing a critical role in Transformer,
position encoding can have huge impact on the learned rep-
resentation. As we have shown in Table 8, we compare the
performance of Pointformer without positional encoding
and with two approaches to position encoding (adding or
concatenating positional encoding with the attention map).
We can observe that Pointformer without positional encod-
ing suffers from a huge performance drop, as the coordi-
nates of points can capture the local geometric information.
4.5. Qualitative Results and Discussion
Qualitative results on SUN RGB-D. Figure 4 shows rep-
resentative examples of detection results on SUN RGB-D
with VoteNet + Pointformer. As we can observe, our model
achieves robust results despite the challenges of clutter and
scanning artifacts. Additionally, our model can even recog-
nize the missing objects in the ground truth. For instance,
the dresser in the left scene is only partially observed by the
sensor. However, our model can still generate precise pro-
posals for the object with proper bounding box sizes. Sim-
ilar results are shown in the right scene, where the table in
the front suffers from clutter because of the books on it.
Inspecting Pointformer with attention maps. To validate
how modules in Pointformer affect learned point features,
we visualize the attention maps from the GT module of the
second last Pointformer block. We show the attention of
the particular points in Figure 5. The second row shows the
50, 100, 200 points with highest attention values towards
the points marked with star. We can observe that Point-
former ﬁrst focuses on the local region of the same object,
then spread the attention to other regions, and ﬁnally attends
points from other objects globally. The overall attention
map shows the average attention weights of all the points
in the scene, indicating that our model mostly focuses on
points on the objects. These visualization results show that
Pointformer can capture local and global dependencies, and
enhance message passing on both object and scene levels.
5. Conclusion
This paper introduces Pointformer, a highly effective
feature learning backbone for 3D point clouds that is per-
mutation invariant to points in the input and learns local
8

Image of the scene
Predictions
Ground truth
Image of the scene
Predictions
Ground truth
Figure 4. Qualitative results of 3D object detection on SUN RGB-D. From left to right: Originial scene image, our model’s prediction,
and annotated ground truth boxes.
Image of the scene
Ground truth
Overall attention
Top-50 attention
Top-100 attention
Top-200 attention
Figure 5. Visualization results of the attention maps. In top-k
attention, darker color indicates larger attention weight, in overall
attention red indicates large value.
and global context-aware representations. We apply Point-
former as the drop-in replacement backbone for state-of-
the-art 3D object detectors and show signiﬁcant perfor-
mance improvements on several benchmarks including both
indoor and outdoor datasets.
Comparing to classiﬁcation and segmentation tasks in-
cluding part-segmentation and semantic segmentation in
prior work, 3D object detection typically involves more
points (4× - 16×) in a scene, which makes it harder for
Transformer-based models. For future work, we would like
to explore extensions to these two tasks and other 3D tasks
such as shape completion, normal estimation, etc.
Acknowledgments
This work is supported in part by the National Science
and Technology Major Project of the Ministry of Science
and Technology of China under Grants 2018AAA0100701,
the National Natural Science Foundation of China un-
der Grants 61906106 and 62022048, the Institute for Guo
Qiang of Tsinghua University and Beijing Academy of Ar-
tiﬁcial Intelligence.
References
[1] H. Caesar,
Varun Bankiti,
A. Lang,
Sourabh Vora,
Venice Erin Liong, Q. Xu, A. Krishnan, Yu Pan, Giancarlo
Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset
for autonomous driving. CVPR, pages 11618–11628, 2020.
[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-
to-End object detection with transformers. In ECCV, May
2020.
[3] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Hee-
woo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever.
Generative pretraining from pixels. ICML, 2020.
[4] Krzysztof Choromanski, Valerii Likhosherstov, David Do-
han, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter
Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,
David Belanger, Lucy Colwell, and Adrian Weller. Rethink-
ing attention with performers, 2020.
[5] MMDetection3D Contributors.
MMDetection3D: Open-
MMLab next-generation platform for general 3d object
detection.
https://github.com/open-mmlab/
mmdetection3d, 2020.
[6] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin
Jaggi. On the relationship between self-attention and con-
volutional layers. arXiv preprint arXiv:1911.03584, 2019.
[7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes.
In
CVPR, pages 5828–5839, 2017.
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional trans-
formers for language understanding. In NAACL-HLT, pages
4171–4186, Minneapolis, Minnesota, June 2019. Associa-
tion for Computational Linguistics.
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale, 2020.
[10] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we
ready for autonomous driving? the kitti vision benchmark
suite. In CVPR, 2012.
[11] Benjamin Graham, Martin Engelcke, and Laurens van der
Maaten. 3d semantic segmentation with submanifold sparse
convolutional networks. In CVPR, June 2018.
[12] Ji Hou, Angela Dai, and Matthias Niessner. 3d-sis: 3d se-
mantic instance segmentation of rgb-d scans. In CVPR, June
2019.
[13] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas,
and Franccois Fleuret.
Transformers are rnns: Fast au-
toregressive transformers with linear attention.
ArXiv,
abs/2006.16236, 2020.
9

[14] Diederik P. Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In Yoshua Bengio and Yann LeCun,
editors, ICLR, 2015.
[15] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-
former: The efﬁcient transformer. ICLR, 2020.
[16] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,
Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders
for object detection from point clouds. In CVPR, 2019.
[17] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se-
ungjin Choi, and Yee Whye Teh. Set transformer: A frame-
work for attention-based permutation-invariant neural net-
works. In Kamalika Chaudhuri and Ruslan Salakhutdinov,
editors, ICML, volume 97 of Proceedings of Machine Learn-
ing Research, pages 3744–3753, Long Beach, California,
USA, 09–15 Jun 2019. PMLR.
[18] Ming Liang, Bin Yang, Yun Chen, Rui Hu, and Raquel Urta-
sun. Multi-task multi-sensor fusion for 3d object detection.
In CVPR, June 2019.
[19] Ilya Loshchilov and Frank Hutter. Fixing weight decay reg-
ularization in adam. CoRR, abs/1711.05101, 2017.
[20] Youngmin Park, Vincent Lepetit, and W. Woo. Multiple 3d
object tracking for augmented reality. 2008 7th IEEE/ACM
International Symposium on Mixed and Augmented Reality,
pages 117–120, 2008.
[21] Charles R Qi, Or Litany, Kaiming He, and Leonidas J
Guibas. Deep hough voting for 3d object detection in point
clouds. In CVPR, pages 9277–9286, 2019.
[22] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J
Guibas. Frustum pointnets for 3d object detection from rgb-d
data. In CVPR, 2018.
[23] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classiﬁcation
and segmentation. arXiv preprint arXiv:1612.00593, 2016.
[24] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. In NeurIPS, pages 5099–5108,
2017.
[25] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, I. Bello,
Anselm Levskaya, and Jonathon Shlens. Stand-alone self-
attention in vision models. In NeurIPS, 2019.
[26] Shaoshuai Shi, Chaoxu Guo, L. Jiang, Zhe Wang, Jianping
Shi, X. Wang, and Hongsheng Li. Pv-rcnn: Point-voxel fea-
ture set abstraction for 3d object detection.
CVPR, pages
10526–10535, 2020.
[27] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr-
cnn: 3d object proposal generation and detection from point
cloud. In CVPR, June 2019.
[28] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural net-
work for 3d object detection in a point cloud. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition, pages 1711–1719, 2020.
[29] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.
Sun rgb-d: A rgb-d scene understanding benchmark suite. In
CVPR, pages 567–576, 2015.
[30] Shuran Song and Jianxiong Xiao. Deep sliding shapes for
amodal 3d object detection in rgb-d images. In CVPR, June
2016.
[31] OpenPCDet Development Team.
Openpcdet: An open-
source toolbox for 3d object detection from point clouds.
https://github.com/open-mmlab/OpenPCDet,
2020.
[32] H. Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, B.
Marcotegui, F. Goulette, and L. Guibas. Kpconv: Flexible
and deformable convolution for point clouds. ICCV, pages
6410–6419, 2019.
[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS, pages
5998–6008, 2017.
[34] Sourabh Vora, Alex H. Lang, Bassam Helou, and Oscar Bei-
jbom. Pointpainting: Sequential fusion for 3d object detec-
tion. In CVPR, June 2020.
[35] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and
Hao Ma. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768, 2020.
[36] Shenlong Wang,
Simon Suo,
Wei-Chiu Ma,
Andrei
Pokrovsky, and Raquel Urtasun. Deep parametric continu-
ous convolutional neural networks. In CVPR, June 2018.
[37] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,
Michael M Bronstein, and Justin M Solomon.
Dynamic
graph cnn for learning on point clouds. Acm Transactions
On Graphics (tog), 38(5):1–12, 2019.
[38] Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming
Zhang, Kai Xu, and Jun Wang. Mlcvnet: Multi-level con-
text votenet for 3d object detection.
In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 10447–10456, 2020.
[39] Qiangeng Xu, Xudong Sun, Cho-Ying Wu, Panqu Wang,
and U. Neumann. Grid-gcn for fast and scalable point cloud
learning. CVPR, pages 5660–5669, 2020.
[40] Xu Yan, C. Zheng, Zhuguo Li, S. Wang, and Shuguang Cui.
Pointasnl: Robust point clouds processing using nonlocal
neural networks with adaptive sampling. CVPR, pages 5588–
5597, 2020.
[41] Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li,
Jinxian Liu, Mengdie Zhou, and Qi Tian. Modeling point
clouds with self-attention and gumbel subset sampling. In
CVPR, June 2019.
[42] M. Ye, Shuangjie Xu, and Tongyi Cao. Hvnet: Hybrid voxel
network for lidar based 3d object detection. CVPR, pages
1628–1637, 2020.
[43] Zaiwei Zhang, Bo Sun, Haitao Yang, and Qixing Huang.
H3dnet: 3d object detection using hybrid geometric primi-
tives. In European Conference on Computer Vision, pages
311–329. Springer, 2020.
[44] Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning
for point cloud based 3d object detection. In CVPR, June
2018.
[45] Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li,
and Gang Yu. Class-balanced Grouping and Sampling for
Point Cloud 3D Object Detection.
arXiv e-prints, page
arXiv:1908.09492, Aug 2019.
[46] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
and Jifeng Dai. Deformable detr: Deformable transformers
for end-to-end object detection, 2020.
10

Appendix
A. Architectures and Implementation Details
In this section, we discuss each component in our Point-
former models for indoor and outdoor settings in detail.
Indoor Datasets. First, the Local Transformer(LT) block
is composed of a sequence of sampling and grouping oper-
ations, followed by a shared positional encoding layer and
two self-attention transformer layers, with a linear shared
Feed-Forward Network(FFN) in the end. As shown in Table
9, we use the same sampling and grouping parameters, and
feature dimensions as those of PointNet++ [24], the back-
bone in VoteNet [21] and H3DNet [43].
Second, the Local-Global Transformer(LGT) and the
Global Transformer(LT) have fewer hyper-parameters than
LT, where we adopt two self-attention layers in GT and one
cross-attention layer in LGT for each Pointformer block.
Since their massive attention computation may lead to over-
ﬁtting, we apply dropout with the dropping probability 0.4
on SUN RGB-D [29] and 0.2 on ScanNetV2 [7]. As for the
number of heads in multi-head attention, we set it to 8 on
ScanNetV2 and 4 on SUN RGB-D. In our experiments, we
found that noisy backgrounds in indoor datasets affect the
LGT performance, by reducing 1∼2%mAP. So we report
the results on indoor datasets without the LGT module.
#block
Nin
Nout radius samples Cin Cmed Cout
1
Npoint 2048
0.2
64
64
64
128
2
2048 1024
0.4
32
128
128
256
3
1024
512
0.8
16
256
256
512
4
512
256
1.2
16
512
512
512
Table 9. Model Architecture details on indoor datasets. Nin de-
notes the number of input points to this Pointformer block, and
Nout is the number of sampled output points of the block. Radius
and samples are hyper-parameters of ball query operation to gather
points in a neighborhood in LT. Cin, Cmed and Cout denote the di-
mensions of features in LT, LGT and GT respectively. Npoint is
the scale of original point clouds in the dataset, 20,000 for SUN
RGB-D and 40,000 for ScannNetV2.
Finally, we implement our indoor models on the top of
MMDetection3D, an open source toolbox 3D object detec-
tion. We follow the same hyper-parameters and data aug-
mentation techniques as those of VoteNet. To train a Point-
former on SUN RGB-D, we use the AdamW [14, 19] opti-
mizer with an initial learning rate of 3e-4 and weight decay
factor of 0.05, and decay the learning rate by 0.3 at epoch
24 and 32 during the training of a total of 36 epochs. And
for ScanNet, we use AdamW optimizer with 0.002 learning
rate and set 0.1 weight decay. We decay the learning rate by
0.3 at epoch 32 and 40 during the training of 48 epochs.
Outdoor Datasets. We adopt the same structure of Trans-
former blocks as that for indoor datasets. Two self-attention
layers with FFN are adopted in LT and GT, while only one
cross-attention layer is utilized in LGT block. The number
of heads are set to 8 for both KITTI [10] and nuScenes [1].
#block
Nin
Nout radius samples Cin Cmed Cout
1
16384 4096
0.1
64
64
64
128
2
4096
1024
0.5
32
128
128
256
3
1024
256
1.0
16
256
256
512
4
256
64
2.0
16
512
512
512
Table 10. Model Architecture details on KITTI datasets.
We implement our outdoor models on top of OpenPCDet
[31], an open source toolbox for LiDAR-based 3D object
detection. We follow the same hyper-parameters as that of
PointRCNN, including data augmentation, post-processing,
etc. To train a Pointformer on KITTI, we use the Adam
optimizer with an initial learning rate of 5e-3 and weight
decay of 0.01.
B. More Quantitative Results
In this section, we provide more results and analysis on
SUN RGB-D and ScanNetV2 as shown in Table 11&12.
With 0.5 IoU threshold, our proposed Pointformer achieves
consistent improvements on both dataset. In Table 13, we
use the one tower version H3DNet [43] as baseline, showing
our method can work well with the recent advanced model.
C. More Ablation Studies
Parameter Efﬁciency.
To further validate the effective-
ness of Pointformer, we conduct experiments and compare
the backbones with similar model parameters. We reduce
the Transformer layers adopted in each block and refer the
model as Pointformer(small). Similarly, we increase the
FFN layers in PointNet++ and refer the model as Point-
Net++(large). As we have shown in Table 14, Pointformer
achieves better results under both parameter budgets. Al-
though our model suffers from a performance reduction
when using fewer Transformer layers, we are still 0.5% to
1% AP higher for all difﬁculty levels. Additionally, Point-
Net++ shows little improvement with larger feature dimen-
sions.
By comparison, Pointformer can adapt to deeper
models and use learning parameters more efﬁciently.
Computational Cost Reduction. As stated in Section 3.7,
Transformer-based modules suffer from heavy computa-
tional cost and memory consumption. Therefore, we adopt
the Linformer technique to improve model efﬁciency. The
results are shown in Table 15 and we can observe that infer-
ence latency is decreased with little drop in performance.
11

Method
cab bed chair sofa table door wind bkshf pic cntr desk curt fridg showr toil sink bath ofurn mAP
VoteNet [21]
8.1 76.1 67.2 68.8 42.4 15.3
6.4
28.0 1.3 9.5 37.5 11.6 27.8
10.0 86.5 16.8 78.9 11.7 33.5
VoteNet*
14.6 77.9 73.1 80.5 46.5 25.1 16.0 41.8 2.5 22.3 33.3 25.0 31.0
17.6 87.8 23.0 81.6 18.7 39.9
+Pointformer 19.0 80.0 75.3 69.0 50.5 24.3 15.0 41.9 1.5 26.9 45.1 30.3 41.9
25.3 75.9 35.5 82.9 26.0 42.6
Table 11. Performance comparison of VoteNet with and without Pointformer on ScanNetV2 validation dataset. The evaluation metric is
Average Precision with 0.5 IoU threshold.* denotes the model implemented in MMDetection3D [5].
Method
bathtub
bed
bookshelf
chair
desk
dresser
nightstand
sofa
table
toilet
mAP
VoteNet [21]
49.9
47.3
4.6
54.1
5.2
13.6
35.0
41.4
19.7
58.6
32.9
VoteNet*
43.5
55.9
7.2
56.5
5.7
12.6
39.7
50.1
20.7
66.3
35.8
+Pointformer
42.5
59.0
6.3
54.2
5.4
20.5
43.3
51.0
22.4
61.2
36.6
Table 12. Performance comparison of VoteNet with and without Pointformer on SUN RGB-D validation dataset. The evaluation metric is
Average Precision with 0.5 IoU threshold.* denotes the model implemented in MMDetection3D [5].
Method
mAP@0.25 mAP@0.5
H3DNet* - 1 tower
64.1
44.2
+Pointformer
64.4
44.4
Table 13. Performance comparison of H3DNet [43] with and with-
out Pointformer on ScanNet V2 validation dataset. For fair com-
parison we use single backbone instead of multiple backbones. *
denotes the model implemented in MMDetection3D [5].
Method
Params
Car (IoU=0.7)
(PointRCNN+)
Easy
Moderate
Hard
PointNet++(default)
4.04M
88.88
78.63
77.38
Pointformer(small)
4.12M
89.35
79.01
78.34
PointNet++(large)
6.24M
89.01
78.82
77.67
Pointformer(default)
6.06M
90.05
79.65
78.89
Table 14. Comparison of PointNet++ and Pointformer with similar
parameters on the val split of KITTI.
Method
Latency
Car (IoU=0.7)
Easy Moderate Hard
Poinftormer+Linformer
0.22
89.94
79.63
78.85
Pointformer
0.25
90.05
79.65
78.89
Table 15. Performance of Pointformer with and withour the Lin-
former technique on the val split of KITTI.
D. More Qualitative Results
We provide additional visualization results in this sec-
tion. Figure 6 shows more visualized attention maps on
SUN RGB-D dataset. Figure 7 and Figure 8 present qualita-
tive results of detection models with Pointformer on Scan-
NetV2 and KITTI dataset, respectively.
12

Image of the scene
Ground truth
Top-50 attention
Top-100 attention
Top-200 attention
Overall attention
Figure 6. More attention maps visualizations on SUN RGB-D. From left to right: Original scene image, ground truth annotations, top-50,
100, 200 attention maps of points to a query point, and the overall attention map for the entire scene. In top-k attention, the star in orange
indicates the query point and darker color indicates larger attention weight, in overall attention red indicates large value. Different object
categories are presented with different colors.
Scene
Predictions
Ground truth
Figure 7. Qualitative results of 3D object detection on ScanNetV2. From left to right: Original scene image, our model’s prediction,
and annotated ground truth boxes. Different object categories are presented with different colors.
13

Figure 8. Qualitative results of 3D object detection on KITTI val split. We show detection results in four scenes. In each scene, the left
is bird eye view detection results, the upper right is the scene image, and the lower right is the front view detection results. Our detection
results are consistent with the ground truth labels (not shown).
14

