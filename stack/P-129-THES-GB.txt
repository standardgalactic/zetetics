 
 
 
Beware, your imagination leaves digital traces 
Bruno Latour, Sciences-Po, Paris 
A piece for the Times Higher Literary Supplement 6th of April 2007 
 
’Who would know how to love without having read novels?” This saying seems 
to take on a new meaning with the multiplication of virtual worlds, even though 
the adjective “virtual” may be greatly misleading. It would be very odd to say, 
when thinking of the young hero of Marcel Proust’s A la recherche du temps perdu, 
who spends whole days utterly absorbed in the fictional landscapes painted by his 
favourite novelists, that he resided in a “real” world, while a youngster of today 
who buys rather expensive equipment to play with buddies on the other side of the 
planet through wireless and satellite connections would be said to be living in a 
“virtual” landscape. 
 
It would be much more reasonable to argue that it was Proust’s narrator 
who lived his adventures “virtually” while his 21st-century counterparts 
have to embed their imagination in so much hardware and software 
paraphernalia that they clearly end up in a more real, more connected, 
more technical world. Or rather we might agree to say that the capacity of 
young children to absorb remains the same but that the technology of the 
printed book has been partially replaced by a vastly more complicated and 
concentrated entertainment industry. 
Imagination no longer comes as cheaply as it did in the past. The 
slightest move in the virtual landscape has to be paid for in lines of code. If 
you want your avatar to wear a new golden helmet or jump in the air, gangs 
of underpaid software engineers somewhere in Bangalore have to get out of 
bed to work on your demands. The fancies of our brains have shifted so 
little from the real to the virtual that tens of thousands of children in China 
are earning a living by causing avatars to graduate to higher levels in 
various digital games before reselling them for a good prize to boys in 
America who like to play those games but have not the time nor patience to 
earn enough “points” for their aliases. When Segolène Royal, the French 
presidential candidate, bought a piece of real estate on Second Life to start 
a campaign headquarters there she paid for it in hard cash. 
If it is rather useless to try to decide whether we are ready to upload our 
former selves into these virtual worlds or not, it is more rewarding to notice 

another much more interesting difference between the two industries and 
technologies of imagination. Apart from the number of copies sold and the 
number and length of reviews published, a book in the past left few traces. 
Once in the hands of their owners, what happened to the characters 
remained a private affair. If readers swapped impressions and stories about 
them, no one else knew about it. 
The situation is entirely different with the digitalisation of the 
entertainment industry: characters leave behind a range of data. In other 
words, the scale to draw is not one going from the virtual to the real, but a 
scale of increasing traceability. The stunning innovation is that every click of 
every move of every avatar in every game may be gathered in a data bank 
and submitted to a second-degree data-mining operation. 
I am sure that this accumulation of traces has enormous effects for the 
entertainment industry, for specialists in marketing, advertising, intelligence, 
police and so on, but another consequence is worth pointing out. The 
precise forces that mould our subjectivities and the precise characters that 
furnish our imaginations are all open to inquiries by the social sciences. It is 
as if the inner workings of private worlds have been pried open because 
their inputs and outputs have become thoroughly traceable. 
Before digitalisation, social psychologists used very vague words such as 
“rumours”, “influences”, “fads”, “fashions” or even “contexts” to describe 
the complex ecology of our minds. But today it just happens that a 
character from a game can be followed through the IP numbers of the 
computers from which they are clicked or from the stream of news in which 
they are commented upon, all the way from the designers who draw them 
to the blogs where their adventures are exchanged. 
The ancient divide between the social on the one hand and the 
psychological on the other was largely an artefact of an asymmetry between 
the traceability of various types of carriers: what Proust’s narrator was doing 
with his heroes, no one could say, thus it was said to be private and left to 
psychology; what Proust earned from his book was calculable, and thus was 
made part of the social or the economic sphere. But today the data bank of 
Amazon.com has simultaneous access to my most subtle preferences as well 
as to my Visa card. As soon as I purchase on the web, I erase the difference 
between the social, the economic and the psychological, just because of the 
range of traces I leave behind. 
Dozens of tools and crawlers can now absorb this vast amount of data 
and represent it again through maps of various shapes and colours so that a 
“rumour” or a “fad” becomes almost as precisely described as a “piece of 
news”, “information”, or even a “scientific fact”. It’s not by accident that 
the founders of Google have one reference in their original patent, and it is 
to a chapter of Robert K. Merton, the American sociologist, about citation 
patterns in science. 

Owen Gingerich, the great historian of astronomy, spent a life-time 
retrieving all the annotations of all the copies of Copernicus’s first edition. 
He could thus give a precise meaning to the rather empty notion of 
“Copernican revolution” and could show which parts of the book everyone 
had read and misinterpreted. Nowadays, any scientist can do the same for 
each portion of each article he or she has published so long as the local 
library has bought a good package of digital data banks. But what is more 
extraordinary is that any journalist can do so as well for the latest Madonna 
video or the dirtiest rumour about Prince Harry’s love affairs. 
In other words, the former distinction between the circulation of facts 
and the dissemination of opinions has been erased in such a way that they 
are both graduating to the same type of visibility — not a small advantage if 
we wish to disentangle the mixture of facts and opinions that has become 
our usual diet of information. Subjectivities used to be the inner sanctum 
where social sciences had to stop and dismount in order to shift to other, less 
reliable vehicles. It is now possible to follow how the characters of a “reality 
show” or the finalists of Star Academy have so modified the ways and 
means with which their viewers speak and think about the world that the 
social has become, so to speak, continuous with the psychological. 
French kids arrested by the police and brought before a judge raise their 
hands with an “Objection, your Honour!” that has no meaning in the 
French legal system but is ubiquitous on American TV series. The 
consequences for the social sciences will be enormous: they can finally have 
access to masses of data that are of the same order of magnitude as that of 
their older sisters, the natural sciences. But my view is that “social” has 
probably become as obsolete as “natural”: what is common to both is a sort 
of new epidemiology that was anticipated, a century ago, by the sociologist 
Gabriel Tarde and that has now, at last, the empirical means of its scientific 
ambition. 
 
Bruno Latour is professor of sociology at the Institut d’Etudes Politiques 
(Sciences-Po) in Paris. 
 

