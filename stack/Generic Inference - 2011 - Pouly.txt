GENERIC INFERENCE 

GENERIC INFERENCE 
A Unifying Theory for Automated Reasoning 
Marc Pouly 
Cork Constraint Computation Centre 
University College Cork, Ireland 
Interdisciplinary Centre for Security, Reliability and Trust 
University of Luxembourg 
Jürg Kohlas 
Department of Informatics 
University of Fribourg, Switzerland 
WILEY 
A JOHN WILEY & SONS, INC., PUBLICATION 

Copyright © 2011 by John Wiley & Sons, Inc. All rights reserved 
Published by John Wiley & Sons, Inc., Hoboken, New Jersey 
Published simultaneously in Canada 
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form 
or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as 
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior 
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to 
the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax 
(978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission should 
be addressed to the Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 
07030, (201) 748-6011, fax (201) 748-6008, or online at http://www.wiley.com/go/permission. 
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best efforts in 
preparing this book, they make no representations or warranties with respect to the accuracy or 
completeness of the contents of this book and specifically disclaim any implied warranties of 
merchantability or fitness for a particular purpose. No warranty may be created or extended by sales 
representatives or written sales materials. The advice and strategies contained herein may not be 
suitable for your situation. You should consult with a professional where appropriate. Neither the 
publisher nor author shall be liable for any loss of profit or any other commercial damages, including 
but not limited to special, incidental, consequential, or other damages. 
For general information on our other products and services or for technical support, please contact our 
Customer Care Department within the United States at (800) 762-2974, outside the United States at 
(317) 572-3993 or fax (317) 572-4002. 
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may 
not be available in electronic formats. For more information about Wiley products, visit our web site at 
www.wiley.com. 
Library of Congress Cataloging-in-Publication Data: 
Pouly, Marc, 1980-, author. 
Generic Inference : A Unifying Theory for Automated Reasoning / Marc Pouly, Jürg Kohlas. 
p. cm 
Includes bibliographical references and index. 
ISBN 978-0-470-52701-6 (hardback) 
1. Valuation theory. 2. Algorithms. 3. Algebra, Abstract. I. Kohlas, Jürg, 1939-, 
author. II. Title. 
QA162.P65 2011 
519.54—dc22 
2010042336 
Printed in Singapore. 
oBook ISBN: 9781118010877 
ePDF ISBN: 9781118010846 
ePub ISBN: 9781118010860 
10 9 8 7 6 5 4 3 2 1 

To Marita and Maria 

CONTENTS 
List of Instances and Applications 
xiii 
List of Figures and Tables 
xvii 
Acknowledgments 
xxi 
Introduction 
xxiii 
PART I LOCAL COMPUTATION 
1 
Valuation Algebras 
3 
1.1 
Operations and Axioms 
4 
1.2 
First Examples 
6 
1.3 
Conclusion 
17 
Appendix: Generalizations of the Valuation Algebra Framework 
17 
A.l 
Ordered Sets and Lattices 
17 
A. 1.1 
Partitions and Partition Lattices 
19 
A.2 
Valuation Algebras on General Lattices 
20 
A.3 
Valuation Algebras with Partial Projection 
24 
Problem Sets and Exercises 
26 
VII 

viii 
CONTENTS 
2 
Inference Problems 
29 
2.1 
Graphs, Trees and Hypergraphs 
30 
2.2 
Knowledgebases and their Representation 
32 
2.3 
The Inference Problem 
33 
2.4 
Conclusion 
44 
Problem Sets and Exercises 
44 
3 
Computing Single Queries 
47 
3.1 
Valuation Algebras with Variable Elimination 
49 
3.2 
Fusion and Bucket Elimination 
50 
3.2.1 
The Fusion Algorithm 
50 
3.2.2 
Join Trees 
55 
3.2.3 
The Bucket Elimination Algorithm 
57 
3.2.4 
First Complexity Considerations 
61 
3.2.5 
Some Generalizing Complexity Comments 
66 
3.2.6 
Limitations of Fusion and Bucket Elimination 
68 
3.3 
Valuation Algebras with Neutral Elements 
69 
3.3.1 
Stable Valuation Algebras 
70 
3.4 
Valuation Algebras with Null Elements 
73 
3.5 
Local Computation as Message-Passing Scheme 
75 
3.5.1 
The Complexity of Fusion as Message-Passing Scheme 
77 
3.6 
Covering Join Trees 
78 
3.7 
Join Tree Construction 
82 
3.7.1 
Join Tree Construction by Triangulation 
83 
3.8 
The Collect Algorithm 
87 
3.8.1 
The Complexity of the Collect Algorithm 
90 
3.8.2 
Limitations of the Collect Algorithm 
91 
3.9 
Adjoining an Identity Element 
91 
3.10 
The Generalized Collect Algorithm 
92 
3.10.1 
Discussion of the Generalized Collect Algorithm 
96 
3.10.2 
The Complexity of the Generalized Collect Algorithm 
97 
3.11 
An Application: The Fast Fourier Transform 
98 
3.12 
Conclusion 
100 
Appendix : Proof of the Generalized Collect Algorithm 
101 
Problem Sets and Exercises 
103 
4 
Computing Multiple Queries 
109 

CONTENTS 
IX 
4.1 
The Shenoy-Shafer Architecture 
111 
4.1.1 
Collect & Distribute Phase 
114 
4.1.2 
The Binary Shenoy-Shafer Architecture 
116 
4.1.3 
Performance Gains due to the Identity Element 
117 
4.1.4 
Complexity of the Shenoy-Shafer Architecture 
118 
4.1.5 
Discussion of the Shenoy-Shafer Architecture 
120 
4.1.6 
The Super-Cluster Architecture 
120 
4.2 
Valuation Algebras with Inverse Elements 
121 
4.2.1 
Idempotent Valuation Algebras 
122 
4.3 
The Lauritzen-Spiegelhalter Architecture 
123 
4.3.1 
Complexity of the Lauritzen-Spiegelhalter Architecture 127 
4.4 
The HUGIN Architecture 
128 
4.4.1 
Complexity of the HUGIN Architecture 
131 
4.5 
The Idempotent Architecture 
131 
4.5.1 
Complexity of the Idempotent Architecture 
134 
4.6 
Answering Uncovered Queries 
134 
4.6.1 
The Complexity of Answering Uncovered Queries 
141 
4.7 
Scaling and Normalization 
143 
4.8 
Local Computation with Scaling 
147 
4.8.1 
The Scaled Shenoy-Shafer Architecture 
147 
4.8.2 
The Scaled Lauritzen-Spiegelhalter Architecture 
150 
4.8.3 
The Scaled HUGIN Architecture 
151 
4.9 
Conclusion 
152 
Appendix: Valuation Algebras with Division 
153 
D. 1 
Properties for the Introduction of Division 
154 
D.l.l 
Separative Valuation Algebras 
155 
D.I.2 
Regular Valuation Algebras 
164 
D.1.3 
Idempotent Valuation Algebras 
167 
D.2 
Proofs of Division-Based Architectures 
167 
D.2.1 
Proof of the Lauritzen-Spiegelhalter Architecture 
167 
D.2.2 
Proof of the HUGIN Architecture 
168 
D.3 
Proof for Scaling in Valuation Algebras 
169 
Problem Sets and Exercises 
173 
PART II GENERIC CONSTRUCTIONS 
5 
Semiring Valuation Algebras 
181 
5.1 
Semirings 
182 

X 
CONTENTS 
5.1.1 
Multidimensional Semirings 
185 
5.1.2 
Semiring Matrices 
185 
5.2 
Semirings and Order 
186 
5.3 
Semiring Valuation Algebras 
190 
5.4 
Examples of Semiring Valuation Algebras 
192 
5.5 
Properties of Semiring Valuation Algebras 
195 
5.5.1 
Semiring Valuation Algebras with Neutral Elements 
196 
5.5.2 
Stable Semiring Valuation Algebras 
196 
5.5.3 
Semiring Valuation Algebras with Null Elements 
197 
5.6 
Some Computational Aspects 
198 
5.7 
Set-Based Semiring Valuation Algebras 
200 
5.8 
Properties of Set-Based Semiring Valuation Algebras 
203 
5.8.1 
Neutral and Stable Set-Based Semiring Valuations 
203 
5.8.2 
Null Set-Based Semiring Valuations 
204 
5.9 
Conclusion 
204 
Appendix: Semiring Valuation Algebras with Division 
205 
E. 1 
Separative Semiring Valuation Algebras 
205 
E.2 
Regular Semiring Valuation Algebras 
209 
E.3 
Cancellative Semiring Valuation Algebras 
212 
E.4 
Idempotent Semiring Valuation Algebras 
213 
E.5 
Scalable Semiring Valuation Algebras 
215 
Problem Sets and Exercises 
217 
6 
Valuation Algebras for Path Problems 
219 
6.1 
Some Path Problem Examples 
220 
6.2 
The Algebraic Path Problem 
223 
6.3 
Quasi-Regular Semirings 
232 
6.4 
Quasi-Regular Valuation Algebras 
237 
6.5 
Properties of Quasi-Regular Valuation Algebras 
245 
6.6 
Kleene Algebras 
246 
6.6.1 
Matrices over Kleene Algebras 
249 
6.7 
Kleene Valuation Algebras 
253 
6.8 
Properties of Kleene Valuation Algebras 
256 
6.9 
Further Path Problems 
256 
6.10 
Conclusion 
262 
Problem Sets and Exercises 
262 

CONTENTS 
XI 
7 
Language and Information 
265 
7.1 
Propositional Logic 
266 
7.1.1 
Language and Semantics 
266 
7.1.2 
Propositional Information 
269 
7.1.3 
Some Computational Aspects 
271 
7.2 
Linear Equations 
274 
7.2.1 
Equations and Solution Spaces 
275 
7.2.2 
Algebra of Affine Spaces 
279 
7.3 
Information in Context 
280 
7.3.1 
Contexts: Models and Language 
281 
7.3.2 
Tuple Model Structures 
286 
7.4 
Conclusion 
289 
Problem Sets and Exercises 
290 
PART III APPLICATIONS 
8 
Dynamic Programming 
293 
8.1 
Solutions and Solution Extensions 
294 
8.2 
Computing Solutions 
297 
8.2.1 
Computing all Solutions with Distribute 
297 
8.2.2 
Computing some Solutions without Distribute 
299 
8.2.3 
Computing all Solutions without Distribute 
303 
8.3 
Optimization and Constraint Problems 
304 
8.3.1 
Totally Ordered Semirings 
304 
8.3.2 
Optimization Problems 
307 
8.4 
Computing Solutions of Optimization Problems 
311 
8.5 
Conclusion 
317 
Problem Sets and Exercises 
318 
9 
Sparse Matrix Techniques 
321 
9.1 
Systems of Linear Equations 
322 
9.1.1 
Gaussian Variable Elimination 
323 
9.1.2 
Fill-ins and Local Computation 
325 
9.1.3 
Regular Systems 
328 
9.1.4 
LDR-Decomposition 
335 
9.2 
Symmetric, Positive Definite Matrices 
338 
9.2.1 
Valuation Algebra of Symmetric Systems 
339 
9.2.2 
Solving Symmetric Systems 
343 

XII 
CONTENTS 
9.2.3 
Symmetrie Gaussian Elimination 
348 
9.2.4 
Symmetrie Deeompositions and Elimination Sequenees 354 
9.2.5 
An Application: The Least Squares Method 
357 
9.3 
Semiring Fixpoint Equation Systems 
364 
9.3.1 
Computing Quasi-Inverse Matrices 
365 
9.3.2 
Local Computation with Quasi-Regular Valuations 
366 
9.3.3 
Local Computation with Kleene Valuations 
371 
9.4 
Conclusion 
382 
Problem Sets and Exercises 
382 
10 
Gaussian Information 
385 
10.1 
Gaussian Systems and Potentials 
386 
10.2 
Generalized Gaussian Potentials 
395 
10.3 
Gaussian Information and Gaussian Potentials 
400 
10.3.1 
Assumption-Based Reasoning 
400 
10.3.2 
General Gaussian Information 
403 
10.3.3 
Combination of Gaussian Information 
407 
10.3.4 
Projection of Gaussian Information 
408 
10.4 
Valuation Algebra of Gaussian Potentials 
414 
10.5 
An Application: Gaussian Dynamic Systems 
417 
10.6 
An Application: Gaussian Bayesian Networks 
423 
10.7 
Conclusion 
428 
Appendix: 
428 
J.l 
Valuation Algebra Properties of Hints 
428 
J.2 
Gaussian Densities 
433 
Problem Sets and Exercises 
435 
References 
437 
Index 
447 

List of Instances and Applications 
1.1 Indicator Functions, Boolean Functions and Crisp Constraints 
7 
1.2 Relational Algebra 
9 
1.3 Arithmetic Potentials 
10 
1.4 Set Potentials 
12 
1.5 Density Functions 
13 
1.6 Gaussian Densities and Gaussian Potentials 
15 
A.7 Set Potentials on Partition Lattices 
24 
A.8 Quotients of Density Functions 
26 
2.1 Вayesian Networks 
33 
2.2 Query Answering in Relational Databases 
35 
2.3 Dempster-Shafer Theory of Evidence 
36 
2.4 Satisfiability of Constraints and Propositional Logic 
38 
2.5 Hadamard Transform 
41 
xiii 

XIV 
LIST OF INSTANCES AND APPLICATIONS 
2.6 Discrete Fourier and Cosine Transforms 
B.4 Filtering, Prediction and Smoothing in hidden Markov chains 
4.4 Probability Potentials 
4.5 Probability Density Functions 
D.9 Scaling of Belief Functions 
5.1 Weighted Constraints, Spohn Potentials and GAI Preferences 
5.2 Possibility Potentials, Probabilistic Constraints and Fuzzy Sets 
5.3 Set-based Constraints and Assumption-based Reasoning 
5.3 Possibility Measures 
5.3 Disbelief Functions 
E.15 Quasi-Spohn Potentials 
E. 18 Bottleneck Constraints 
6.1 Connectivity Path Problem 
6.2 Shortest and Longest Distance Problem 
6.3 Maximum Capacity Problem 
6.4 Maximum Reliability Problem 
6.5 Regular Languages 
6.6 Path Counting Problem 
6.7 Markov Chains 
6.8 Numeric Partial Differentiation 
6.9 Matrix Multiplication 
F. 1 Path Listing Problems 
F.2 Testing whether Graphs are Bipartite 
F.3 Identifying Cut Nodes in Graphs 
F.6 Network Compilation 
F.6 Symbolic Partial Differentiation 
7.1 Satisfiability in Prepositional Logic 
7.2 Theorem Proving in Prepositional Logic 
42 
45 
146 
146 
170 
192 
193 
194 
202 
202 
212 
215 
220 
221 
221 
222 
223 
257 
257 
259 
261 
263 
263 
263 
264 
264 
272 
272 

LIST OF INSTANCES AND APPLICATIONS 
XV 
7.3 Propositional Logic 
283 
7.4 Linear Equations Systems and Affine Spaces 
283 
7.5 Linear Inequality Systems and Convex Polyhedra 
284 
7.6 Predicate Logic 
284 
G. 1 Consequence Finding in Propositional Logic 
290 
8.1 Classical Optimization 
307 
8.2 Satisfiability of Constraints and Propositional Logic 
308 
8.3 Maximum Satisfiability of Constraints and Propositional Logic 
308 
8.4 Most and Least Probable Explanations 
308 
8.5 Bayesian and Maximum Likelihood Decoding 
309 
8.6 Linear Decoding and Gallager-Tanner-Wiberg Algorithm 
310 
9.1 Least Squares Method 
357 
9.2 Smoothing and Filtering in Linear Dynamic System 
361 
10.1 Gaussian Time-Discrete Dynamic Systems 
395 
10.2 Kaiman Filter: Filtering Problem 
418 
10.3 Gaussian Bayesian Networks 
423 

List of Figures 
I.l 
A.l 
2.1 
2.2 
2.3 
2.4 
2.5 
2.6 
2.7 
2.8 
2.9 
2.10 
2.11 
The graph associated with the inference problem of Exercise 1.1. 
A tree of diagnoses for stubborn cars. 
Undirected graphs. 
Directed graphs. 
A labeled graph. 
Hypergraph, primal graph and dual graph. 
Possibilities of knowledgebase representation. 
Bayesian network of a medical example. 
A digital circuit of a binary adder. 
The result table of a full adder circuit. 
Connecting two 1-bit-adders produces a 2-bit-adder. 
Input and output of a discrete Fourier transform. 
Function board of a discrete Fourier transform. 
xxvi 
21 
30 
31 
31 
32 
33 
35 
39 
39 
40 
42 
44 
xvii 

XVÜi 
LIST OF FIGURES 
2.12 
3.1 
3.2 
3.3 
3.4 
3.5 
3.6 
3.7 
3.8 
3.9 
3.10 
3.11 
3.12 
3.13 
3.14 
3.15 
3.16 
4.1 
4.2 
4.3 
4.4 
4.5 
4.6 
4.7 
4.8 
4.9 
4.10 
4.11 
A hidden Markov chain. 
Finalization of the graphical fusion process. 
Extending a labeled tree to a join tree. 
The bucket-tree of Example 3.5. 
The join tree obtained from the bucket-tree of Figure 3.3. 
The join tree for Example 1.2. 
Different elimination sequences produce different join trees. 
A primal graph and its induced graph. 
A covering join tree of an inference problem. 
The fusion algorithm as message-passing scheme. 
A covering join tree of Instance 2.1. 
A covering join tree of Instance 2.1. 
A graph with two possible triangulations. 
The triangulated primal graph of Instance 2.1. 
Join graph and derived join tree. 
A complete run of the collect algorithm. 
Join tree of a discrete Fourier transform. 
Changing the root of a join tree. 
Mailboxes in the Shenoy-Shafer architecture. 
Join tree to illustrate the Shenoy-Shafer architecture. 
Using non-binary join trees creates redundant combinations. 
Larges treewidth due to non-binary join trees. 
The performance gain due to the identity element 1. 
The performance gain due to the identity element 2. 
Illustration of the super-cluster approach. 
Separator in the HUGIN architecture. 
The path between node 1 and node 6 in the join tree of Figure 4.3. 
A graphical summary of local computation architectures. 
46 
56 
56 
59 
60 
61 
63 
64 
76 
77 
79 
81 
83 
86 
86 
89 
99 
110 
111 
113 
117 
117 
118 
119 
121 
129 
140 
154 

LIST OF FIGURES 
XIX 
D. 1 
Embedding a separative valuation algebra into a union of groups. 
157 
D.2 
Decomposing a regular valuation algebra into a union of groups. 
165 
5.1 
Semiring valuation algebras with neutral and null elements. 
199 
E. 1 
Embedding a separative semigroup into a union of groups. 
207 
E.2 
Decomposing a regular semigroup into a union of groups. 
210 
E.3 
Embedding a cancellative semigroup into a group. 
213 
E.4 
Semiring valuation algebras and division-related properties. 
216 
6.1 
The connectivity path problem. 
221 
6.2 
The shortest distance problem. 
222 
6.3 
The maximum capacity problem. 
222 
6.4 
The maximum reliability problem. 
223 
6.5 
Determining the language of an automaton. 
224 
6.6 
Path sets in cycled graphs may be infinite. 
225 
6.7 
The adjacency matrix of a directed, weighted graph. 
226 
6.8 
An interpretation of equation (6.32). 
250 
6.9 
The path counting problem. 
257 
6.10 
Interpreting computations in Markov chains as path problems. 
259 
6.11 
The computational graph of a function. 
260 
8.1 
A binary, unreliable, memory less communication channel. 
310 
8.2 
The join tree that belongs to the node factors of Example 8.4. 
314 
8.3 
A serial connection of unreliable, memoryless channels. 
318 
8.4 
A parallel connection of unreliable, memoryless channels. 
319 
9.1 
Zero-pattern of a linear equation system. 
325 
9.2 
The join tree for Figure 9.1. 
326 
9.3 
The join tree induced by the variable elimination in Example 9.1. 
334 
9.4 
Recursive buildup of lower triangular matrices. 
350 
9.5 
The join tree of Example 9.3. 
353 
9.6 
The graph representing the non-zero pattern of Example 9.4. 
355 

XX 
LIST OF FIGURES 
9.7 
9.8 
9.9 
10.1 
10.2 
10.3 
10.4 
10.5 
The join tree of the triangulated graph in Figure 9.6. 
The join tree covering the equations of the linear dynamic system. 
A path problem with values from a partially ordered semiring. 
A causal model for the wholesale price of a car. 
The graph of the time-discrete dynamic system. 
A covering join tree for the system (10.6). 
Influence among variables in the Kaiman filter model. 
A covering join tree for the Kaiman filter model. 
355 
362 
379 
388 
396 
397 
419 
419 

Acknowledgments 
The authors would like to thank our collaborators from the Department of Informatics 
at the University of Fribourg (Switzerland), the Cork Constraint Computation Centre 
at the University College Cork (Ireland) and the Interdisciplinary Centre for Security, 
Reliability and Trust at the University of Luxembourg for the many interesting 
discussions that helped to improve the content and presentation of this book. In 
particular, we are grateful for the indispensable support of Christian Eichenberger 
during our journey through the Gaussian world and to Radu Marinescu and Nie Wilson 
for providing expert knowledge regarding inference, search methods and semiring 
systems. Jutta Langel and Jacek Jonczy both corrected parts of this book and provided 
valuable feedback. Special thank also goes to Cassie Craig from John Wiley & Sons, 
Inc. for the editing support and to the Swiss National Science Foundation, which 
financed the research stay of Marc Pouly at the Cork Constraint Computation Centre. 
Marc Pouly & Jiirg Kohlas 
XXI 

Introduction 
Generic Algorithms 
Abstract mathematical structures usually have a large number of models. Algorithms 
based on operations and laws of such structures therefore have an identical form: 
they are generic. This means that for each particular instance of the structure, only 
the basic operations have to be adapted, whereas the overall algorithm remains the 
same. The simplest and typical problem is the one of sorting. It applies to totally 
ordered structures with an order relation <. Any sorting algorithm can be formulated 
in terms of the order relation < and it can be adapted to any particular model of an 
order relation by specifying this relation; for example for integers, real numbers or for 
a lexicographical order relation of alphanumeric strings. In addition, many program-
ming languages allow for generic programming, i.e. in their syntax they provide the 
means to formulate generic algorithms and to specialize them to particular instances. 
In this book, an abstract algebraic structure called valuation algebra is proposed. 
It provides the foundation to formulate an abstract inference problem, to construct 
a graphical data structure and a generic inference algorithm to solve the inference 
problem. Usually, one arrives at such general structures from concrete problems and 
algorithms by lifting them to their most abstract form. This also holds for the present 
case. In 1988, Lauritzen and Spiegelhalter (Lauritzen & Spiegelhalter, 1988) pro-
posed an algorithm for solving the inference problem for Bayesian networks based 
XXIII 

XXÏV 
INTRODUCTION 
on a paradigm called local computation. Soon after, Shenoy and Shafer (Shafer & 
Shenoy, 1988; Shenoy & Shafer, 1990) noted that the same algorithm could also be 
applied to solve inference problems with belief functions. They proposed a small 
but sufficient system of axioms for an algebraic framework that makes possible the 
application of the generic inference algorithm. The algebraic theory of valuation 
algebras was developed in (Kohlas, 2003), and it was shown that they permit, under 
certain varying additional conditions, four different generic inference architectures. 
In (Pouly, 2008) a computer implementation of these generic architectures together 
with a number of concrete instances is described, see also (Pouly, 2010). 
In the meantime, many different models of valuation algebras from very dis-
tant fields of Mathematics and Computer Science were identified. They range from 
probabilistic and statistical systems, different uncertainty formalisms to constraint 
systems, passing by various systems of logic, relational databases and systems of 
linear equations over fields and semirings. Many of these instances are presented 
in this book. Their computational interest can always be expressed in terms of the 
general inference problem, which can be solved by the same generic algorithm or 
inference architectures. This is first and foremost of great practical interest. Instead 
of developing and programming inference algorithms for each instance separately, 
it is possible to use a single generic system. Of course, in the past, individual so-
lution algorithms and computer programs have been proposed and written mostly 
independent from each other, using the same basic concepts but often naming them 
quite differently. This adds to a certain Babylonian confusion, especially for students 
who should get an overall view of Computer Science. Further, it also represents a 
dissipation of valuable human resources in research and problem solving. 
Second, the process of abstraction is of great theoretical importance. It provides a 
unifying view of seemingly different fields of Computer Science and allows to elab-
orate both similarities and differences between problem classes. In fact, valuation 
algebras allow for a very appealing general view of information processing: infor-
mation comes in pieces, usually from different sources. Each piece of information 
concerns a particular domain and answers, possibly only partially, specific questions. 
Pieces of information can be aggregated or combined, which is one of the basic oper-
ations of a valuation algebra. So, the general inference problem essentially consists 
of combining all available information. But usually one is only interested in some 
particular facet or aspect of the total information. Therefore, the parts of the total 
information, which are relevant to precise questions, must be extracted. Extraction 
or projection of information is the second basic operation of a valuation algebra. 
In relational databases for example, combination corresponds to the join operation, 
whereas information extraction corresponds to the usual operation of projection in the 
relational algebra. So, valuation algebras show that this scheme of relational algebra 
is much more general. The form and nature of information elements may be very 
different from model to model. 

INTRODUCTION 
XXV 
Complexity Considerations 
Combining pieces of information to solve the inference problem leads to what can 
be called a race of dimensionality. It was said above that each piece of information is 
associated with a certain domain. Combining information results in information on 
larger domains. So, when many pieces of information are combined, the domains may 
become very large. This often leads to serious problems of computational complexity, 
as we are next going to illustrate with two simple examples (Aji & McEliece, 2000): 
Example 1.1 Let f(Xi,X2), 
д(Х2,Хз) 
and h(X2,X4,X5) 
be three real-valued 
functions, where X\ to X$ are variables taking values from a finite set of n elements. 
Such discrete functions can always be represented in tabular form. This leads to 
tables with n2 entries for f and g, and a table with n3 entries for the function h. If 
we identify combination with component-wise multiplication, and projection with the 
summation of unrequested variables, we may define the following inference problem: 
Σ 
f(Xi,X2)-g(X2,X3)-h(X2,X4,Xb). 
(1.1) 
- ^ 3 i-^4 
t ^ 5 
Note that we combine all available information and project the result afterwards to 
the domain of interest, represented by the variable set {X\,X2}- 
This set is also 
called the query of the inference problem, whereas the set of factors f, g and h is 
called knowledgebase. Although each knowledgebase factor can be represented by 
a small table of at most n3 entries, the total product already needs a table of n5 
entries. In our small example, this may still be tractable, but the reader will recognize 
that with growing numbers of factors and variables, computing this product will fast 
become intractable. In fact, if s denotes the total number of variables, the complete 
table over all variables has ns entries. We can see that this exhibits an exponential 
growth in the total number of variables. Thus, although the size of each factor in the 
knowledgebase may be small, computing their combination may be intractable. The 
next example points out a more realistic situation. 
Example 1.2 For 1 < i < 100 we consider 100 real-valued functions, where each 
function fi is defined over exactly three binary variables Xi to Xi+2- Again, each 
of these functions is representable by a table of 23 = 8 entries. Similarly to the 
foregoing example, we then choose the variable set {X\o\,X\o2} 
US query and 
obtain the following inference problem: 
/ „ 
fl(Xl,X2,X3) 
' ■■■ · /ΐΟθ(-ΧΊθΟι-ΧΊθ1,-ΧΊθ2)· 
(1-2) 
Xι ,...,λ"κ)ο 
Here, each factor is represented by a small table of 23 entries. The total product on 
the other hand needs a table t>/2102 entries that requires 1018 terabytes of memory. 
There is no computer with such a large memory capacity. If we further assume that 
a computer performs one multiplication per nanosecond, then these computations 
require at least 1012 years, which is 100 times longer than the estimated age of 

XXVI 
INTRODUCTION 
the universe. We therefore conclude that the above sum cannot be computed by 
first computing the total product and then summing out the hundred variables. This 
example is quite representative for the inference problems considered in this book. 
So, what is then the escape from this race of dimensionality? We sketch the basic 
idea using the above examples. The secret lies in a clever use of the distributive law 
of arithmetics, which can often be used to arrange sequences of summations and 
multiplications such that all intermediate results stay manageable. This allows us to 
write equation (1.1) as 
!{ХъХ2)-\У^д{Х2,Х3)\ 
■ ( J2 h(X2,X4,X5)Y 
We immediately observe that the largest intermediate result produced during the 
computation of this formula contains 2 variables and thus requires a tables of only 
n2 entries. This is a considerable increase of efficiency which becomes even more 
apparent by applying the same idea to equation (1.2). We obtain 
/ , · · · I /_^[ ^ / 1 ( ^ 1 , ^ 2 , - ^ 3 ) I ' / 2 ( ^ 2 , ^ 3 , ^ 4 ) I · · -/100(^100,^101,^102) 
Xioo 
\ X2 
\ Xi 
/ 
/ 
(1.3) 
Here, the largest intermediate result contains only 3 variables that requires a table of 
8 entries. Thus, instead of this huge amount of memory and computational time, the 
required resources remain proportional to the input size. Note also that besides the 
distributive law, we used the associative laws of addition and multiplication. 
We may change the operation that is associated with projection in the above 
examples. Instead of addition, we now take the maximum value in Example 1.1 that 
then leads to the following computational task: 
max 
f(XuX2) 
■ д(Х2,Хг) 
■ h(X2,X4,X5). 
(1.4) 
Both expressions (1.1) and (1.4) model inference problems over different valuation 
algebras, whereas the second problem now takes the shape of an optimization task. 
Obviously, the same complexity considerations with respect to the size of intermediate 
tables apply in both cases, and because the distributive law still holds between 
multiplication and maximization, we may also perform the same improvement: 
f(XuX2) 
· max 9(X 2,X 3) 
- m a x Ь(Х2,Х4,Х5) 
. 
V Хз 
I 
\ Χί,Χ$ 
I 
We thus have two different problems with different semantics and perhaps different 
application fields, but from the algebraic perspective they are both specializations of 
the same generic problem and can be solved by the same generic algorithm. This 
is exactly the idea of generic inference, and the secret of efficiency consists of a 

INTRODUCTION 
XXVII 
clever arrangement of the computations (combinations and projections) based on a 
generalization of the distributive law. Essentially the same technique was applied in 
the past for the processing of every specific inference formalism, which attests that 
the developers of these algorithms did not only solve the same generic problem, but 
they were also confronted with the same complexity concerns and finally hit upon 
the same solution. This is once more a convincing argument for the necessity of a 
generic inference theory, as it will be developed in the first part of this book. 
The essence of valuation algebras consists in capturing these associative and 
distributive laws with respect to abstract operations of combination and projection. 
This will then be sufficient to rearrange the computations in the inference problem 
such that they remain feasible at each stage. In fact, this technique tends to keep 
storage proportional to the domain size of the input factors of the inference problem. 
One speaks of local computation, i.e. local on the domains of individual factors. Local 
computation is closely related to a well-known technique called tree-decomposition 
of graphs. Suppose a graphical representation of an inference problem where each 
node represents a variable. In addition, two nodes are linked by an edge, if their 
variables occur in the domain of a common factor or in the query. Figure 1.1 shows 
the graph associated with the inference problem of Exercise 1.1. If we again assume 
that there are s variables, each with a domain of n values, then the total information 
represented by the graph needs a table of ns entries. 
Figure 1.1 The graph associated with the inference problem of Exercise 1.1. 
The key parameter, called treewidth ω of the graph, represents an important 
structural property. For the time being, we can imagine the treewidth as some measure 
related to the sparsity of the graph. Then, instead of the intractable complexity of 
ns associated with the total graph, the operations of the inference problem can be 
rearranged in such a way that the complexity is reduced to g(s)-nw, where g is a linear 
function. Observe that the complexity is still exponential, but now in the treewidth 
ω instead of the total number of variables s. Since in many cases ω is much smaller 
than s, it represents a big gain and often turns a intractable problem into a tractable 
problem. This is a case of what has recently been called parametric or parameterized 
complexity; see for example (Downey & Fellows, 1999). However, we should also 
emphasize that the complexity of inference problems is not always exponential. There 
are important cases where it may be polynomial, for instance s3 with s still being 
the number of variables. But we then still have the change in complexity from s3 to 
g(s) ■ ω3. It may thus be said that the polynomial or exponential complexity of the 

XXVIII 
INTRODUCTION 
inference problem is a dimension somehow orthogonal to the framework of valuation 
algebras; but it always allows to reduce the complexity to a product of a linear function 
of problem size s and a polynomial or exponential function of treewidth ω according 
to the nature of the problem. That is essentially what valuation algebras achieve in 
terms of complexity. 
Generic Constructions 
From a practical point of view, the relevance of a generic theory can best be measured 
by the number of instances and applications covered by the theory. Here, a formalism 
is called an instance of a valuation algebra, if it satisfies the properties or axioms of the 
algebra. Besides the many concrete, individual instances presented in this book, we 
also describe generic approaches to construct whole classes of valuation algebras. We 
call this generic constructions. Essentially, these methods derive valuation algebras in 
a constructive manner from other algebraic structures such as fields, vector spaces or 
semirings. For example, we will see that matrices over particular semirings, systems 
of linear equation over arbitrary fields or mappings from configurations to semiring 
values always form a valuation algebra. Another generic construction identifies al-
gebras derived from structures of languages with models, as in logic and many other 
fields. This adds to picture elements of valuation algebras as pieces of information, 
since information is often expressed in terms of a language; i.e. a formal language 
in our case. Generic constructions further allow to verify axioms and properties for 
whole classes of formalisms on a more abstract level and thereby simplify the theory. 
Finally, they also identify new formalisms that can be processed efficiently by the 
generic inference algorithms but which are yet unknown to the community. 
The Content of this Book 
This book is divided into three parts. The first part introduces the algebraic system 
of a valuation algebra and defines generic inference algorithms for their processing. 
The second part is entirely dedicated to generic constructions and the identification 
of new valuation algebra instances. Finally, the third part studies some selected 
applications of local computation. Some of these applications even go beyond the pure 
computation of inference, but they are nevertheless based on the same computational 
techniques. Typical examples are the construction of solutions for constraint and 
equation systems and sparse matrix techniques. Let us consider the organisation of 
the three parts in more detail: 
Part I: Local Computation 
Chapter 1 introduces the valuation algebra framework upon which all later chapters 
are based. For simplicity, we do not stress the most general axiomatic system that 
is based on arbitrary lattices with partial projection. Instead, we restrict ourselves to 
valuation algebras with full projection over variable systems that cover most practical 

INTRODUCTION 
XXIX 
applications. The two generalizations will be discussed in the appendix of this chapter. 
Also, we give first examples of well-known formalisms that satisfy the structure of a 
valuation algebra, including crisp constraints, relations, probability mass functions, 
belief functions or density functions. 
Chapter 2 is dedicated to the definition of the generic inference problem that reflects 
the fundamental computational interest in valuation algebras. It will be distinguished 
between single-query and multi-query inference problems. We also introduce sev-
eral possibilities to represent the structure of knowledgebases and give important 
applications that require the solution of inference problems with knowledgebases 
from different valuation algebras. This includes reasoning in Bayesian networks and 
Dempster-Shafer theory, satisfiability in logic and constraint systems, or discrete 
Fourier and Hadamard transforms. 
Chapter 3 provides generic algorithms for the solution of single-query inference 
problems. In particular, it introduces the fusion algorithm, the bucket-elimination 
scheme and two variations of the collect algorithm. We also provide a detailed com-
plexity analysis of these first local computation methods. 
Chapter 4 extends the collect algorithm to take multiple queries into consideration, 
leading to the Shenoy-Shafer architecture. We will see that its complexity can be 
improved if the valuation algebra provides some concept of division. Based on this 
additional operator, three further local computation architectures for the solution of 
multi-query inference problems called Lauritzen-Spiegelhalter, Hugin and idempo-
tent architecture are derived. The study of the algebraic requirements of a valuation 
algebra to provide a division operator is a more ambitious topic. Its comprehensive 
discussion is therefore postponed to the appendix of this chapter. 
Part II: Generic Constructions 
Chapter 5 presents the first generic construction that identifies valuation algebras by 
means of a simple mapping from configurations to values of a commutative semiring. 
This family of valuation algebras for example includes probability potentials, crisp 
constraints, weighted constraints, probabilistic constraints, possibilistic constraints 
and assumption-based constraints. A second family of valuation algebras is obtained 
from mapping sets of configurations to semiring values and includes in particular the 
formalism of belief functions from Dempster-Shafer theory. 
Chapter 6 introduces path problems and shows that their computation amounts to 
the solution of a semiring fixpoint equation system. Based on this observation, two 
generic constructions related to matrices with semiring values are identified. Typical 
members and application fields of these families of valuation algebras are shortest 
path problems, maximum capacity problems, connectivity problems, path reliability, 
path counting, Markov chains or partial differentiation. Moreover, this chapter pro-

XXX 
INTRODUCTION 
vides the preliminary work for the discussion of sparse matrix techniques in Chapter 9. 
Chapter 7 deals with the duality between information and its representation in terms 
of a language. Typical examples of this generic construction are different kinds of 
logic and systems of equations and inequalities. 
Part ill: Applications 
Chapter 8 points out that many valuation algebra have an associated notion of a 
solution. This is typically the case for systems of equations and inequalities, but it 
also holds in logic and constraint systems. The identification of solutions in arbitrary 
valuation algebras is the main topic in this chapter. It will be shown that based on a 
previous execution of a local computation architecture, solutions can be found without 
increasing the complexity of the local computation scheme. This results in two further 
generic algorithms to compute either a single solution or all solutions of a valuation 
given as factorization. In the second part of Chapter 8, this theory is applied to semi-
ring constraint systems for the computation of solutions in optimization or constraints 
problems. Further specializations for equation systems will be discussed in Chapter 9. 
Chapter 9 deals with sparse matrix techniques. The first part of the chapter focuses 
on sparse, linear systems over fields. It introduces Gaussian elimination and different 
decomposition approaches and establishes the connection to the valuation algebra 
of affine spaces introduced in Chapter 7. The famous least squares method provides 
an interesting case study. In the second part, we consider path problems that induce 
sparse fixpoint equation systems over semirings. This is based on the valuation alge-
bras from Chapter 6. We will further show how local computation is used to solve 
the single-source, multiple-pairs or all-pairs version of different path problems. 
Chapter 10 looks at linear systems with stochastic disturbances. In many important 
applications, these disturbances may be assumed to have a Gaussian distribution. 
Together with observations, such systems provide Gaussian information. It will be 
shown that several valuation algebras including Gaussian potentials and Gaussian 
hints hide behind Gaussian information. Also, this chapter investigates inference in 
Gaussian systems with local computation and provides an in-depth study of statisti-
cal, assumption-based reasoning. 
All chapters in this book are completed by a collection of exercises and open 
problems. We distinguish three degrees of difficulty. One-star exercises are either 
simple finger exercises to actively digest the theory of the chapter, straightforward 
applications of the theory to concrete problems or omitted proofs with a reference to 
another textbook or journal article that contains the complete proof. Two-star exercises 
are small research projects that extend the theory of the chapter and establish links to 
related research topics. Finally, three-star exercises label comprehensive and mainly 
open research questions that could be included in a research program on valuation 
algebras and local computation. 

INTRODUCTION 
XXXI 
Beyond the Content of this Book 
This book is about generic local computation or tree-decomposition methods that 
are derived from the general valuation algebra framework. Additionally, we discuss 
several specializations and extensions of these algorithms for particular families of 
valuation algebras in the third part. However, it should also be mentioned that other 
techniques for the processing of inference problems exist. Depending on the concrete 
application, these methods may be less or more efficient than the class of algorithms 
presented in this book. But we also point out that it is in general not possible to 
apply these techniques to arbitrary valuation algebras, because they usually require 
more information about the concrete buildup of formalisms. A popular class of such 
algorithms are search methods, see for example (Nilsson, 1982; Pearl, 1984; Kanal 
& Kumar, 1988; Dechter, 2003). Search methods work through a search space that 
is obtained from a knowledgebase by assigning different values to variables. It is 
therefore clear that search methods are suitable only for formalisms that consist of 
mappings from finite variable assignments to some values. In particular, this covers 
the large family of semiring valuation algebras introduced in Chapter 5, but we 
will also meet numerous formalisms in this book that do not provide this structure. 
Search methods and local computation have also been combined to hybrid methods 
(Larrosa & Dechter, 2003; Dechter, 2006). A closely related aspect concerns the 
representation of valuations. This is again specific to each family of valuation algebras 
and goes along with tailored inference methods. Important techniques are AND-OR 
search (Dechter & Mateescu, 2007; Marinescu & Dechter, 2009a; Marinescu & 
Dechter, 2009b), OR search with caching (Bacchus et al, 2003), methods based on 
automata (Vempaty, 1992; Fargier & Vilarem, 2004), decision diagrams (Wilson, 
2005; Nicholson et al, 2006) and various other techniques related to knowledge 
compilation (Darwiche, 2001; Darwiche & Marquis, 2001; Darwiche & Marquis, 
2002; Wächter & Haenni, 2006; Fargier & Marquis, 2007; Wächter et al, 2007; 
Wächter, 2008). We further restrict ourselves to exact inference in this book. There 
are several methods to perform approximated inference with local computation as for 
example the mini-bucket scheme (Dechter & Rish, 2003). We further refer to (Haenni, 
2004) for approximate inference with ordered valuation algebras and to (Kohlas & 
Wilson, 2008) for semiring valuation algebras. It should also be mentioned that other 
algebraic frameworks for generic inference exists, e.g. (Pralet et al, 2007), but these 
systems generally include the valuation algebra framework. Finally, it has already 
been mentioned in the first publications about local computation that these algorithms 
qualify for an implementation on distributed and parallel computing environments. In 
particular, local computation is successfully applied for inference in sensor networks 
(Paskin et al, 2005). This raises many interesting questions that go beyond the scope 
of this book. Examples are the management of computing resources, the minimization 
of communication costs (Pouly, 2008) or the increase of parallelism. These questions 
are discussed for probabilistic reasoning in (Kozlov & Singh, 1994; Kozlov & Singh, 
1996; Namasivayam & Prasanna, 2006; Yinglong & Prasanna, 2008), and we also 
refer to the comprehensive literature on parallel and distributed databases. 

PARTI 
LOCAL COMPUTATION 
Generic Inference: A Unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc.

CHAPTER 1 
VALUATION ALGEBRAS 
The valuation algebra framework provides the algebraic foundation for the application 
of all generic inference mechanisms introduced in this book and therefore marks the 
beginning of our studies. Comparable to the total order that is required for the 
application of sorting procedures, all formalisms must satisfy the structure of a 
valuation algebra in order to be processed by these generic inference tools. Further, 
this framework mirrors all the essential properties we naturally associate with the 
rather imprecise notions of knowledge and information. Let us engross ourselves 
in this thought and put our daily perception of information into words: information 
exists in pieces and comes from different sources. A piece of information refers to 
some specific questions that we later call the domain of an information piece. Also, 
there are two principal operations to manipulate information: we may combine or 
aggregate pieces of information to a new information piece and we may project a 
piece of information to some specific question which corresponds to information 
extraction. Depending on the operation, we either get a broader or more focused 
information. In the following section, we give a formal definition of the valuation 
algebra framework consisting of its operations and axioms. Along the way, we further 
bear on our idea of valuations as pieces of knowledge or information to clarify the 
rather abstract and formal structures. 
Generic Inference: A Unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc. 
3 

4 
VALUATION ALGEBRAS 
1.1 OPERATIONS AND AXIOMS 
The basic elements of a valuation algebra are so-called valuations that we subse-
quently denote by lower-case Greek letters φ,ψ,... 
Intuitively, a valuation can be 
regarded as a representation of information about the possible values of a finite set 
of variables. We use Roman capitals Χ,Υ,... 
to refer to variables and lower-case 
letters s, t,... for sets of variables. Thus, we assume that each valuation φ refers to a 
finite set of variables ά(φ), called its domain. For an arbitrary, finite set of variables 
s, Φ3 denotes the set of all valuations φ with ά{φ) — s. With this notation, the set of 
all possible valuations for a countable set of variables r can be defined as 
Φ = υ φ -
Let D = V(r) be the powerset (the set of all subsets) of r and Φ the set of valuations 
with their domains in D. We assume the following operations defined in (Φ, D): 
1. Labeling: Φ -> D; ф н-» ά(φ); 
2. Combination: Φ χ Φ - ) Φ ; ( ψ , ψ ) ι - > ψ ® ψ ; 
3. Projection: Φ x D -»· Φ; (φ, χ) н-> φίχ for χ С ά(φ). 
These are the three basic operations of a valuation algebra. If we readopt our idea of 
valuations as pieces of information, the labeling operation tells us to which questions 
(variables) such a piece refers. Combination can be understood as aggregation of 
information and projection as the extraction of the part we are interested in. Sometimes 
this operation is also calledfocusing or marginalization. We now impose the following 
set of axioms on Φ and D: 
(Al) Commutative Semigroup: Φ is associative and commutative under (g>. 
(A2) Labeling: For φ,ψ G Φ, 
ά{φ®ψ) 
= 
ά(φ)ΐ)ά(ψ). 
(1.1) 
(A3) Projection: For φ £ Φ, x G D and x ç ά(φ), 
ά{φΧχ) 
= 
χ. 
(1.2) 
(A4) Transitivity: For φ G Φ and x Ç y С ά(φ), 
(φΙν^Ι* 
= 
фХ*ш 
(1.3) 

OPERATIONS AND AXIOMS 
5 
(A5) Combination: For φ, ψ G Φ with ά(φ) = χ, ά(φ) = y and ζ € D such that 
x Ç z Ç xLiy, 
(φ®ψγζ 
= 
ф®ф^гПу. 
(1.4) 
(A6) Domain: For ψ ξ Φ with d(0) = x, 
φίχ 
= 
φ. 
(1.5) 
These axioms require natural properties regarding knowledge or information. 
The first axiom indicates that Φ is a commutative semigroup under combination. If 
information comes in pieces, the sequence of their aggregation does not influence 
the overall result. The labeling axiom tells us that the combination of valuations 
yields knowledge about the union of the involved domains. Variables do not vanish, 
nor do new ones appear. The projection axiom expresses the natural functioning of 
focusing. Transitivity says that projection can be performed in several steps. For the 
combination axiom, let us assume that we have some information about a domain 
in order to answer a certain question. Then, the combination axiom states how the 
answer is affected if a new information piece arrives. We can either combine the new 
piece to the given information and project afterwards to the specified domain, or first 
remove the uninteresting parts of the new information and combine it afterwards. 
Both approaches lead to the same result. In fact, we are going to see in Section 5.3 
that this axiom correlates with some generalized distributive law. Finally, the domain 
axiom ensures that information is not influenced by projection to its own domain, 
which expresses some kind of stability with respect to trivial projection. 
Definition 1.1 A system (Φ, D) together with the operations of labeling, projection 
and combination satisfying these axioms is called a valuation algebra 
In the first appearance of the valuation algebra axioms (Shafer & Shenoy, 1988; 
Shenoy & Shafer, 1990) only Axioms (Al), (A4) and a simpler version of (A5) were 
listed. Axiom (A2) was simply assumed in the definition of combination. (Shafer, 
1991) mentioned Property (A3) for the first time and also remarked that Axiom (A6) 
cannot be derived from the others. (Kohlas, 2003) then assembled these early results 
to a complete and sufficient axiomatic system for generic inference and local compu-
tation. But in addition to the above system, all former approaches contained so-called 
neutral valuations that express vacuous information with respect to a certain domain. 
Here, we will introduce valuation algebras with neutral elements in Section 3.3 as a 
special case of the definition given here. 
Before we turn towards the first concrete examples of valuation algebras, we list 
a few elementary properties that are derived directly from the above set of axioms. 

6 
VALUATION ALGEBRAS 
Lemma 1.1 
1. If φ, φ G Φ with d(4>) — x and d(ip) = y, then 
{ф®ф)ХхПу 
= 
<f>ixny®ipixnv. 
(1.6) 
2. Ι/φ,ψξΦ 
with ά(φ) = x, d(iß) = у and z Ç x, then 
{φ®ψγζ 
= 
(φ®ψΙ*ην)±ζ. 
(1.7) 
Proof: 
1. By the transitivity and combination axiom: 
{φ®φγχΓλυ 
= ((ф®ф)1х)1хПу 
= (ф®ф^хПу)±хПу 
= ф1хГ,у ® ф1хПу. 
2. By the transitivity and combination axiom: 
(φ®ψ)!>ζ = ((φ®ψ)1χ)ίζ 
= 
(ф®ф1хПу)^. 
1.2 
FIRST EXAMPLES 
Our first encounter with the valuation algebra framework took place on a very abstract 
level. To reward the reader for this formal effort, we now consider a first catalogue 
of concrete formalisms that satisfy the valuation algebra structure. Such formalisms 
are subsequently called valuation algebra instances. For the moment, we content 
ourselves with the observation of how these formalisms fit in the valuation algebra 
framework. Later sections then inform about their exact purpose and semantics. 
However, these instances are all based on the important concepts of configurations, 
tuples or vectors that shall first be introduced separately. 
Frames, Tuples, Configurations and Vectors: 
Consider a countable set r of variables where each variable Xi G ris referenced by its 
index г G N. Conversely, every index г G N refers to a unique variable X, G r. This 
one-to-one correspondence between variables and indexes allows us subsequently to 
identify the two concepts. Further, we assume for every variable X G r a set Ω,χ of 
possible values, called its frame. Such variable frames are sometimes assumed to be 
finite or countable, but this is not a general requirement. If a frame contains exactly 
two elements, the corresponding variable is said to be binary. Moreover, if the two 
elements represent the states true and false, the variable is called propositional or 
Boolean. A tuple or configuration with finite domain s С г is a function 
x : s —> fts = 
I f Ωχ 
XGs 

FIRST EXAMPLES 
7 
that associates a value x(X) 
G Ωχ with each variable X € s. By convention, 
the single tuple with empty domain is identified by the diamond symbol o, and we 
use bold-face, lower-case letters x, y,... to refer to tuples. Subsequently, we write 
x e Os to mark x as a tuple with domain s, for which we also use the shorthand term 
s-tuple or s-configuration. 
It is often convenient to decompose tuples according to some variable partition. 
This operation is also called projection, although it is not directly related to valuation 
algebras. Given an s-tuple x and t Ç s, the projection of x to t is defined by a t-tuple 
y such that y(X) = x( X) for all X et. We subsequently write x+É for the projection 
of x to t. Note that о denotes the projection of any tuple to the empty set. Further, 
this notation allows us to write x — (х+*, χ-Ι·5-*) for the decomposition of x with 
respect to t and s — t. Observe also that (x, o) = (o, x) = x. Similar to frames of 
single variables, the set Os represents all possible s-tuples and is therefore called the 
frame of the variable or index set s. In particular, Ω@ = {о}. Then, a tuple set with 
domain s is simply a subset S Ç Ω8. If a tuple set consists of only one element, then 
it is also called a singleton. 
Example 1.1 To describe the attributes color, speed and prize of a car, we assume 
the set of variables r — {C, S, P} with variable frames: Qc = {red, blue, black}, 
Ω§ = {slow, fast} and Ωρ = N. Two possible r-tuples are x = (blue, fast, 30Ό00) 
and y = (black, slow, 10Ό00). The tuple x can for example be decomposed into 
X4.{C,P} = (blue5 30Ό00) and x^s> = (fast). Further, the variable set {C, S} pos-
sesses the frame Sl{c,s} = {(red, slow), (blue, slow),..., (black, fast)}. Observe 
also that the frame ΩΓ contains infinitely many tuples. 
If X e r is a variable that takes real numbers, we have Ωχ = R, and for a set 
s Ç r of real variables the linear space Rs corresponds to the frame of all s-tuples. 
In this case, s-tuples are also called s-vectors. It is possible to introduce an extension 
operator that lifts an s-vector x to some larger domain t Ç s by assigning zeros to 
all variables in t - s. We thus have χΐ'(Χ) = x(X) if X e s and xt4(X) = 0 
otherwise. Clearly, the introduction of this operation is not only possible for real 
number but for all algebraic structures that contain a zero element. These definitions 
of frames, tuples and tuple sets allow for a uniform notation in the following tour 
through a first collection of valuation algebra instances. 
■ 1.1 Indicator Functions - Boolean Functions - Crisp Constraints 
An indicator function with domain s С r identifies a subset S ç Ω8 by 
specifying for each tuple x e f i , whether x belongs to S or not. If we adopt 
the usual interpretation of 0 for x not being an element of S and 1 for x being 
in 5, an indicator function i is defined by 
., 
/ 0 
i f x ^ S , 
г(х) 
= 
< 
\l 
i f x e S . 

8 
VALUATION ALGEBRAS 
Thus, an indicator г with domain d(i) = s is a function that maps every 
tuple x € Ω6 onto a value i(x) G {0,1}, i.e. г : Ω8 —» {0,1}. These are 
the valuations in the valuation algebra of indicator functions. As introduced 
in Section 1.1, we usually write Φ3 for the set of all indicator functions with 
domain s and Φ for the set of all possible indicator functions over subsets of r. 
Combination of indicator functions is defined by multiplication. If i\ and г2 are 
indicator functions with domain s and ί respectively, we define for x € ils\jt 
ii<g>i2(x) 
= 
n(x i s) •î2(xit)· 
Alternatively, we may also define combination in terms of minimization: 
ii®i2(x) 
= 
min{n(x i s),i 2(x i t)}. 
Projection, on the other hand, corresponds to maximization. For an indicator г 
with domain s and i Ç s w e define for all x € i ! ( 
г^(х) 
= 
max г(х, у). 
If we consider only variables with finite frames, an indicator function with 
domain s can be represented by an \s\-dimensional table with |Ω5| zero-one 
entries. Below, we simply use a relational or tabular representation, but em-
phasize that indicator functions are not limited to this inefficient representation. 
In order to prove that indicator functions form a valuation algebra, the reader 
may verify that all axioms are satisfied. However, in Section 5.3 this proof 
will be made for a whole family of formalisms that also includes indicator 
functions. Otherwise, a direct proof can be found in (Kohlas, 2003). Depend-
ing on its application field, the formalism of indicator functions has different 
names. In the context of constraint reasoning, they are usually called crisp 
constraints. Otherwise, indicator functions for propositional variables are also 
called Boolean functions. 
For a concrete example of computing with indicator functions, consider a 
set of variables r = {A,B,C} 
with frames Ω^ = {α,α}, Ωβ = {b,b} and 
Çlc — {c>c}- Then, assume two indicator functions i\ and г2 with domains 
d(ii) = {A,B} andd(i2) = {B,C}: 
В 
b 
b 
b 
b 
с 
с 
с 
с 
с 
1 
0 
1 
1 
А 
а 
а 
а 
а 
В 
b 
ь 
b 
ь 
0 
1 
0 
0 

FIRST EXAMPLES 
9 
We combine i\ and i2 and project the result to {A}: 
A 
a 
a 
a 
a 
ä 
ö 
a 
a 
В 
6 
b 
b 
b 
b 
b 
b 
b 
с 
с 
с 
с 
с 
с 
с 
с 
с 
0 
0 
1 
1 
0 
0 
0 
0 
А 
a 
ä 
1 
0 
■ 1.2 Relational Algebra 
The second instance we are going to study is closely related to the first one. It is 
a subset of the relational algebra (Maier, 1983; Ullman, 1988), which tradition-
ally belongs to the most fundamental formalisms for representing knowledge 
and information. In its usual extent, at least six operations are provided to ma-
nipulate knowledge represented as sets of tuples. Respecting the language of 
relational database theory, variables are called attributes and sets of tuples are 
called relations. A relation over s Ç r is therefore simply a tuple set R С Ç}s. 
It is important to note that variable frames do not need to be finite and conse-
quently, relations can also be infinite sets. Combination is defined by natural 
join: If Ri and R2 are relations with domain s and t respectively, we define 
Л1СХЯ2 
- 
{xensUt:x.is 
eRux11 
eR2}- 
(1.8) 
Projection is defined for a relation R with domain s and i Ç s a s 
Rit 
= 
rxit . χ e 
щ 
Alternatively, the notation nt(R) = R^ is also used in the context of relational 
algebras. If we consider only finite variables in r, then the relational algebra 
over r is isomorphic to the valuation algebra of indicator functions over r. 
In fact, relations are just another representation of indicators. We obtain the 
relation Д4, associated to the indicator function ф with domain ά(φ) = s by 
Rj, 
= 
{ x e ! î s : ^ ( x ) = l } . 
(1.9) 
Conversely, we derive the indicator function φ associated to a relation R with 
domain s Ç r b y φ(χ) = 1 if x e R, and 0(x) = 0 otherwise, for all x € Ω,3. 
Hence, we directly conclude that relations form a valuation algebra in the case 
of finite variables. This statement also holds for the general case as is will be 
shown in Chapter 7.3. Let us write the combination axiom for z = x in the 
notation of relational algebra: For R\, R2 G Φ with d{ R\ ) = x and d(R2) = y, 
nx(R1 M R2) 
= 
R\ ix 
nxny(R2). 

10 
VALUATION ALGEBRAS 
The right-hand side is called semi-join in database theory (Maier, 1983). 
To give an example of computing in the relational algebra, we consider 
two relations R\ and R? with domain d(R\) = {continent,country} 
and 
d(i?2) = {country, city}: 
continent 
Africa 
Asia 
Asia 
Europe 
Europe 
country 
Kenya 
China 
Japan 
Germany 
France 
Ri 
Combining i?i and R^ gives 
R3 = -Ri txj R2 
Ri 
country 
France 
France 
Germany 
Italy 
Kenya 
city 
Paris 
Lyon 
Berlin 
Rome 
Nairobi 
continent 
Africa 
Europe 
Europe 
Europe 
country 
Kenya 
Germany 
France 
France 
city 
Nairobi 
Berlin 
Paris 
Lyon 
and we obtain for the projection of R3 to {continent}: 
/ jy \ 
j-*\. 
{continent} 
π{continent} 
{К-З) — Λ3 
continent 
Africa 
Europe 
1.3 Arithmetic Potentials - Probability Potentials 
Probability potentials are perhaps the most cited example of a valuation algebra, 
and their common algebraic structure with belief functions was originally the 
guiding theme for the abstraction process that lead to the valuation algebra 
framework (Shafer & Shenoy, 1988). However, at this point we yet ignore 
their usual interpretation as discrete probability mass functions and refer to this 
formalisms as arithmetic potentials. These are simple mappings that associate 
a non-negative real value with each tuple, i.e. p : fts —>· K>o· Also, we consider 
from the outset only variables with finite frames and identify d(p) = s ç r to 
be the domain of the potential p. The tabular representation can again be used 
for arithmetic potentials. Combination of two potentials p\ and p^ with domain 
s and t is defined by 
Pi >P2(x) 
= 
Ρ ι ( χ ^ ) · ρ 2 ( χ ^ ) 
(1.10) 
for x G Ω8υί. Projection consists in summing up all variables to be eliminated. 
If arithmetic potentials are used to express discrete probability mass functions, 

FIRST EXAMPLES 
11 
then this corresponds to the operation of marginalization in probability theory. 
For a potential p with domain s,tÇs 
and x € ΩΕ, we define 
p*(x) 
Σ 
Р(Х>: 
(1.11) 
ye(is-t 
Arithmetic potentials belong to the same family of instances as indicator func-
tions. Section 5.3 provides a generic proof that all formalisms of this family 
satisfy the valuation algebra axioms. Moreover, we will also conclude from 
this result that other sets (such as integers, rational or complex numbers) may 
be used instead of real numbers to define arithmetic potentials. 
We pointed out in the introduction of this book that the distributive law 
from arithmetics is used to make the computational process of inference more 
efficient. Also, we said that this property is contained in the valuation algebra 
axioms. This becomes evident by writing the combination axiom in the notation 
of arithmetic potentials: Forpi,p2 € Ф with d(pi) = s, ά(ρ2) = t andx € Çls 
Σ 
Pl(x)-P2(^Snt,y) 
= 
P 1(X). £ 
P2(x^ n Î,y). 
yeQt-s 
y6i2 t_ s 
For an example of how to compute with arithmetic potentials, consider a set 
r = {A, B, C} of three variables with finite frames Ω^ = {a, S}, Ωβ = {b,b} 
and üc = {с, с}. We define two arithmetic potentials p\ and p? with domain 
d(Pl) = {A,B}and<Hp2) 
= {B,C}: 
Pi 
A 
а 
а 
а 
а 
В 
b 
b 
b 
b 
0.6 
0.4 
0.3 
0.7 
P2 = 
В 
b 
b 
b 
b 
с 
с 
с 
с 
с 
0.2 
0.8 
0.9 
0.1 
We combine p\ and p2 and project the result to {A, C}: 
Рз 
Pi ®P2 
A 
а 
а 
а 
а 
а 
а 
а 
а 
В 
b 
b 
b 
b 
b 
b 
b 
b 
с 
с 
с 
с 
с 
с 
с 
с 
с 
0.12 
0.48 
0.36 
0.04 
0.06 
0.24 
0.63 
0.07 
1{А,С} 
Рз 
А 
а 
а 
а 
а 
С 
с 
с 
с 
с 
0.48 
0.52 
0.69 
0.31 

12 
VALUATION ALGEBRAS 
1.4 Set Potentials - Belief Functions 
As we already mentioned, be lief functions (Shafer, 1976; Kohlas & Monney, 
1995) rank among the few instances that originally initiated the abstraction 
process culminating in the valuation algebra framework (Shafer & Shenoy, 
1988). Belief functions are special cases of set potentials. Therefore, we first 
focus on this slightly more general formalism and refer to Instance D.9 and 
D.6 for a discussion of belief functions and its alternative representations. 
The theory is again restricted to variables with finite frames. A set potential 
m : V(i}s) —> R>o with domain d(m) = s Ç r is a function that assigns non-
negative real numbers to all subsets of the variable set frame Ω8. Combination 
of two set potentials rti\ and т2 with domains s and t is defined for all tuple 
sets A Ç üsUt by 
mi 
<gim2(A) 
Σ 
m1(B)-m2(C). 
(1.12) 
BcxlC=A,BÇa3,CÇnt 
This is a simplified version of Dempster's rule of combination (Dempster, 1968; 
Shafer, 1991). Projection of a set potential m with domain s to t Ç s is given 
for A Ç flt by 
at (•4) 
Σ 
7Tt(B)=A,BÇns 
m(B). 
(1.13) 
Observe that these definitions use the language from the relational algebra to 
deal with tuple sets. This particular notation will later be helpful. Since re-
lations satisfy the valuation algebra axioms, we may call in its properties to 
derive important results about set potentials. Note also that the formalism of set 
potentials essentially differs from the other instances introduced beforehand 
that map single tuples to some value. Indeed, it belongs to a second family of 
valuation algebras that will be introduced in Section 5.7 where we also provide 
a generic proof that all formalisms of this family satisfy the valuation algebra 
axioms. Before presenting a concrete example, it is important to remark that 
although all variable frames are finite, a simple enumeration of all assignments 
as proposed for indicator functions or arithmetic potentials is beyond question 
since set potentials assign values to all elements of the powerset of Qs. Instead, 
only those entries are listed whose values differ from zero. These tuple sets are 
usually called focal sets. 
Consider a set of two variables r = {A, B} with finite frames CIA = {«, a} 
and ΩΒ = {b, b}. We define two set potentials mi and m^ with domains 
d(mi) = {A, B} and d{m2) = {A}: 
mi = 
{(a,6)} 
{(3,6), ( M ) } 
{(a, 6), (a,6)} 
0.7 
0.1 
0.2 
m2 = 
{(«)} 
{(«)} 
0.6 
0.4 

FIRST EXAMPLES 
13 
The task of computing the combination of mi and mi is simplified by construct-
ing the following table as an intermediate step. The first column contains mi and 
the top row 7П2, both extended to the union domain d(mi) U d(m,2) = {A, B}. 
Then, every internal cell contains the intersection between the two correspond-
ing tuple sets and the product of the corresponding values. This corresponds to 
the natural join in equation (1.12). 
{(a,ft)}, 0.7 
{(S, ft), (a, ft)}, 0.1 
{(a,ft), (a,b)}, 0.2 
{(a,ft), (a,ft)}, 0.6 
0, 0.42 
{(ä, ft)}, 0.06 
{(a, ft)}, 0.12 
{(a, ft), (a, ft)}, 0.4 
{(a, ft)}, 0.28 
{(a, ft)}, 0.04 
{(a, ft)}, 0.08 
To complete the combination, it is then sufficient to add the values of all internal 
cells with equal tuple set. The result of the combination is projected afterwards 
to {A} using equation (1.13) 
m3 
mi ® πΐ2 
0 
{(a,ft)} 
{(a,ft)} 
{(a,ft)} 
{(3,6)} 
0.42 
0.36 
0.04 
0.06 
0.12 
MA} 
0 
{(«)} 
{(«)} 
0.42 
0.18 
0.40 
1.5 Density Functions 
All instances discussed so far are based on variables with finite (or at least 
countable) frames. Although such a setting is typical for a large number of 
formalisms used in computer science, it is in no way mandatory. Thus, we next 
introduce the valuation algebra of density functions which are defined over 
variables taking values from the set of real numbers. For a set of variables 
s Ç r, a density function / : Rs —> M>o with domain s is a continuous, 
non-negative valued function with finite Riemann integral, 
J —с 
/(x)dx 
< 
oo. 
(1.14) 
Note that x denotes a vector in the linear space Ω„ = Rs. The combination 
/ <g> g of two density functions / and g with domain s and t is defined for 
x G 
by simple multiplication 
/®«/(x) 
= 
Д х ^ ) - о ( х ^ ) · 
(1.15) 
The projection /·'-' of a density / with d(f) = s to t Ç s is given by the integral 
/
oo 
-oo 
(1.16) 

14 
VALUATION ALGEBRAS 
where x ξ I 1 and y G R s _ t. Density functions together with the above 
definitions of combination and projection form a valuation algebra. The proof 
of this statement even for the more general case of Lebesque measurable density 
functions can be found in (Kohlas, 2003). The perhaps most interesting example 
of a density function in the context of valuation algebras is the Gaussian 
density. In fact, Instance 1.6 below shows that Gaussian densities are closed 
under combination and projection and therefore establish a subalgebra of the 
valuation algebra of density functions. 
Labeled Matrices 
These first examples were all based on tuples and tuple sets. Next, we are going 
to introduce two further valuation algebra instances based on labeled matrices. For 
finite sets s , f C r a labeled, real-valued matrix is a mapping M : s x M M such 
that for X G s and У G t we have M(X, Y) G Ш. We write М(Ш, s х t) for the set 
of matrices of this kind. The sum of Mi, Μ2 G M (R, s x t) is defined by 
(M1+M2)(X,Y) 
= M1(X,Y)+M2{X,Y) 
(1.17) 
for X G s and У G ί. If Mi G М{Ш, sxt) and M 2 G M(R, t x u) for s,t,uÇ r, 
X G s and У G M we define the product Mi · M2 by 
(Mi-M2)(X,y) = 
^2M1(X,Z)-M2(Z,Y). 
(1.18) 
Z€t 
A special element of the set ΛΊ (M, s x t) is the zero matrix defined for X G s and 
У G t by 0(X, У) = 0. The projection of a labeled matrix M G M(R, s x ί) to 
и С s and υ С t is defined by Miu<v(X,Y) 
= M(X,Y) 
if X G и and У G υ, 
which simply corresponds to dropping the unrequested rows and columns. Similarly 
to the extension of vectors, we extend a labeled matrices M G ΛΊ(ΙΚ, s x t) to и D s 
and v ~Э t with X £ и and У G v by 
Mt-(x,y) = lM^Y^ *xe'"*y*b 
(U9) 
10, 
otherwise. 
Of special interest are often square matrices M G Λ4(Κ, s x s). We then say that 
the domain of M is d(M) = s and use the abbreviation M(M., s) for the set of all 
matrices with domain s. Also, the projection of a matrix M with domain s to t Ç s 
is abbreviated by M^s and accordingly, we write M1"" for its extension to the larger 
domain и D s. Finally, we define the identity matrix for the domain s by 1(X, Y) = 1 
if X = Y and I(X, У ) = 0 otherwise. In later chapters, we will also consider labeled 
matrices with values from other algebraic structures than real numbers. But for the 
introduction of the following instances, the limitation to real numbers is sufficient. 

FIRST EXAMPLES 
15 
■ 1.6 Gaussian Densities - Gaussian Potentials 
An important family of density functions is provided by Gaussian distributions. 
For a set of real variables s Ç r and x e l ' a Gaussian density is defined by 
where μ G Rs denotes the mean vector and К : s x s —>■ R the symmetric 
and positive definite concentration matrix. The matrix K _ 1 is the variance-
covariance matrix of the Gaussian density. Looking at this definition, we remark 
that a Gaussian density is completely determined by its mean vector and con-
centration matrix. Such pairs (μ, К) with μ e Ms and К е М(Ш,з) being 
symmetric and positive definite are called Gaussian potentials. The square root 
in front of the exponential function in equation ( 1.20) is merely a normalization 
factor guaranteeing that the integral over the density function equals one. 
Let / and g be two Gaussian densities with domain d(f) = s and d(g) = t, 
represented by the Gaussian potentials (μι, Ki) and (μ2, K2). When combin-
ing these two densities by equation (1.15) we can neglect the normalization 
constants and simply multiply the exponential functions. This in turn means 
adding their exponents. Doing so, we may also neglect additive terms without 
variables, since these terms simply go into the normalization factor. In the 
following, equality must always be taken up to such terms. We then get for the 
exponent of the combined density function and x € WsUt 
(x i s - μ 1) ΤΚ 1(χ^ - μι) + (χ^ - μ 2) ΤΚ 2(χ^ - μ2) 
-
(χ - ß[sUt)TK\sUt(* - ß\sUt) + (x - μ^ υ ί) τΚ^ υ ί(χ - μΙ°υί) 
= 
x T K t * u t x _ xT Ktsu t / xt Sut _ μ Γ τ . υ * κ ΐ . υ ί χ 
+ 
x TK^ u tx - x TK^ uVl s U t - μ Γ ^ Κ ^ ' χ 
= 
xT(KÎsUÎ + K^ U É)X - xr(KÎsUVîsUt + K|iUt/4eUt) 
-
(μΡ ϋ ίΚΪ 8 υ ί+μΓ* υ ίΚί υ ί)χ. 
Define now 
К 
= K^sUt + K^sU< 
Note that К is still symmetric and positive definite. Then, we rewrite the 
last expression using K, add a constant, additive term to the last expression 

16 
VALUATION ALGEBRAS 
completing it to a quadratic form: 
xTKx - xT(KÎ5UVîsUÎ + l4sUt/4sUt) 
-
(KÎsUVîsUt + K^sUVIsUt)Tx + 
(KÎSUVÎSUÎ + K^UÎ/xIsUt)TK-1(KÎsUVîsUt + Klsu^lsut) 
= 
(x - κ-^κΐ^'μΐ™ 
+ K^sUi4sUi))TK 
(x - Κ-^Κΐ'^μΙ™ 
+ KÎeUVÎ*Ut)) 
This finally means that the multiplication of two Gaussian densities results 
again in a Gaussian density with concentration matrix 
К 
= 
KÎ s U t + K| e U t. 
(1.21) 
and mean vector 
μ = K-^KÎ^VÎ^* + K|sUt/4sUt)· 
(1.22) 
Motivated by this result, we may define the combination of two Gaussian 
potentials (μχ,Κι) and (μ2,Κ2) with ίί(μι,Κι) = s andd^ 2,K 2) = t by 
(μ 1,Κ 1)®(μ 2,Κ 2) 
= 
(μ,Κ), 
where μ is defined by (1.22) and К by (1.21). We found this combination rule 
by a purely formal derivation. Its reason and meaning will be given in Section 
10.1. The projection of a Gaussian density / with domain d(f) = s, mean 
vector μ and concentration matrix К to a subset t С s is again a Gaussian 
density, as it will be shown in Appendix J.2. Its mean vector is simply μ^* and 
((K - 1)^*) - 1 is its concentration matrix. Consequently, Gaussian densities are 
closed under combination and projection and form an important subalgebra of 
the valuation algebra of density functions. The operations of combination and 
projection can both be expressed in terms of the Gaussian potentials associated 
with the densities. Gaussian densities and Gaussian potentials will be studied in 
detail in Chapter 10, which will also clarify the interest in this valuation algebra. 
Many further valuation algebra instances will be introduced throughout the subse-
quent chapters. At this point we interrupt the study of formalisms for the time being 
and focus on the computational interest in valuation algebras. The following chapter 
phrases an abstract computational task called inference problem using the language 
of valuation algebras and also gives a first impression of the different semantics of 
this problem under different valuation algebra instances. But first, we propose a more 
far-reaching discussion of the valuation algebra axiomatics in the appendix of this 
chapter. The valuation algebra framework presented in Section 1.1 is more general 
than most of its antecessors. Nevertheless, there is still potential for further gener-
alizations. Two such systems will be presented in the following appendix: the first 

CONCLUSION 
17 
introduces valuation algebras that are no more necessarily based on variable systems. 
Instead, valuations take their domains from a more general lattice structure. The sec-
ond framework focuses on a generalization of the projection operator. Here, it is no 
longer the case that projections on arbitrary subsets are possible, but there is a fourth 
operator that tells us to which domain a valuation is allowed to be projected. For the 
understanding of the subsequent chapters, these systems are of minor importance. 
But it is nevertheless interesting to see that further instances can be covered by a 
more general and abstract definition of the valuation algebra framework, which still 
provides enough structure for the application of local computation. 
1.3 
CONCLUSION 
This first chapter introduced the valuation algebra framework upon which all fol-
lowing chapters are based. Valuations can be imagined as pieces of information that 
refer to a question called the domain of the valuation. This domain is returned by the 
labeling operation. Two further operations called combination and projection are used 
to manipulate valuations. Combination corresponds to aggregation, and projection to 
focussing or extraction of knowledge. In addition, the valuation algebra framework 
consists of a set of six axioms that determine the behaviour of the three operations. 
Formalisms that satisfy the structure of a valuation algebra are called instances and 
occur numerously in very different fields of mathematics and computer science. This 
chapter gave a first selection of instances including crisp constraints, arithmetic and 
probability potentials, Dempster-Shafer belief functions, density functions and the 
important family of Gaussian distributions. 
Appendix: Generalizations of the Valuation Algebra Framework 
We first give a formal definition of lattices with their main variations and refer to 
(Davey & Priestley, 1990) for an extensive discussion of these an related concepts. 
A.1 ORDERED SETS AND LATTICES 
Definition A.2 A preorder is a binary relation < over a set P which is reflexive and 
transitive. We have for a,b,c G P: 
• a < a (reflexivity); 
• a <b and b < с implies that a < с (transitivity). 
A preorder is called partial order if it is antisymmetric, i.e. if 
• a <b and b < a implies that a — b (antisymmetry). 
A set with a partial order is also called a partially ordered set or simply an ordered 
set. Chapter 5 starts with an introduction to semiring theory that naturally offers a 

18 
VALUATION ALGEBRAS 
large number of examples for preorders and partial orders. We therefore refer to the 
numerous examples of semirings in Chapter 5 for concrete sets providing preorders 
and partial orders. 
Definition A.3 Let P be an ordered set. 
• P has a bottom element if there exists an element l e P such that _L < xfor 
all x e P. 
• P has a top element if there exists an element T G P such that x < T for all 
xe 
P. 
Definition A.4 Let P be a ordered set and S С P. An element и € P is called 
supremum, least upper bound or join of S if 
• x < и for all x G S; 
• for any v £ P such that x < vfor all x G S it holds that и < v. 
Likewise, an element и G P is called infimum, greatest lower bound or meet of S if 
• и < xfor all x G S; 
• for any v G P such that v < xfor all x G S it holds that v < u. 
If join or meet of a subset S Ç P exist, then they are always unique, and we write 
V S for join and Д S for meet. Moreover, if S — {x, y} consists of two elements, 
we generally write xWyor sup{x, y] for join and xAyor 
inf {x, y} for meet. 
Definition A.5 Let P be a non-empty, ordered set. 
• Ifx V у and x Ay exist for all x,y G P, then P is called a lattice. 
• If\/S and Д S exist for all subsets S С P, then P is called a complete lattice. 
Lattices may satisfy further identities: 
Definition A.6 Let L be a lattice. 
• L is said to be bounded, if it has a bottom and a top element. 
• L is said to be distributive, if for all a,b,c G L 
aA(bVc) 
= 
(а Л b) V (а Л с); 
aV(bAc) 
= 
(aVb)A(aVc). 
This definition lists both statements of distributivity, although they are in fact equiv-
alent. The proof is given in (Davey & Priestley, 1990). Note also that every complete 
lattice is bounded. 

ORDERED SETS AND LATTICES 
19 
Example A.2 (Powerset or Subset Lattice) For any set A we consider the set of all 
subsets called its powerset or subset lattice V{A). This set is partially ordered via set 
inclusion and forms a lattice with set intersection as meet a Ab = аПЬ and set union 
as join a V b = a U b. The powerset lattice is distributive, complete and bounded by 
A itself and the empty set 0. 
Example A.3 (Division Lattice) The set ofN U {0} also forms a distributive lattice 
with the least common multiple as join lcm(a, b) = a V b and the greatest common 
divisor as meet gcd(a, b) = a Ab. The order is defined by divisibility, i.e. a < b if a 
divides b. This lattice is also bounded with 1 as bottom and 0 as top element. 
An important source of non-distributive lattices are partitions: 
A.1.1 
Partitions and Partition Lattices 
Partition lattices take a universal position among all lattices. (Grätzer, 1978) shows 
that every lattice is isomorph to some partition lattice. 
Definition A.7 A partition π = {В, : 1 < г < n} of a set U called universe consists 
of a collection of subsets Bi Ç U called blocks such that 
• Вг ф 0; 
• Bi П Bj: = 0 for г ф j and 1 < i.j < n; 
•\S=1Bi 
= u. 
Let Part(U) denote the set of all possible partitions of a universe U. It is then 
possible to introduce a partial order between its elements. For π\,π2 G Part(U) we 
write 71"! < π2 if every block of π\ is contained in some block of τ2. This is the 
case if, and only if, every block of π2 is a union of blocks from щ. We then also say 
that the partition πχ is finer than π2, or conversely that π2 is coarser than πχ. The 
blocks of the infimum or meet of an arbitrary collection of partitions P Ç Part(U) 
corresponds to the non-empty intersections of all blocks contained in the partitions 
of P. The definition of the supremum or join especially for the case of universes 
with an infinite number of elements is more involved. We refer to (Grätzer, 1978) for 
a discussion of this aspect. Here, we directly conclude that the set Part(U) of all 
partitions of the universe U forms a complete lattice. The partition {{и} : и € U} 
consisting of all one-element subsets of U is the lower bound, and the partition {£/} 
is the upper bound. A lattice whose elements are partitions of a universe U is called 
partition lattice. They are sublattices of Part(U) and generally not distributive. The 
above refinement relation is the natural way of introducing an order relation between 
partitions. However, in the context of algebraic information theory (Kohlas, 2003) it 
is often useful to consider the inverse of the natural order, i.e. π2 <c τΐΊ if- and only 
if, iti < π2 or equivalently, if π2 is coarser than π\. The motivation is that a finer 
partition expresses more information than a coarser partition. 

20 
VALUATION ALGEBRAS 
Example A.4 (Interval Partitions) Let U = [a,b) be a semi-closed interval of real 
numbers. A sequence XQ < X\ < ... < xn with XQ = a and xn = b establishes a 
partition π = {[xi,Xi+i) 
: 0 < i < n — 1}. If an interval partition πχ contains all 
elements of the sequence defining the partition ΤΤ2, then every bloc of-K\ is contained 
in some bloc of 1^2- In other words, τχ\ is a refinement of Ж2- The meet of two interval 
partitions is obtained from the union sequence, and the join from the intersection. 
If for example U = [0,1) the partition for 0 < 0.2 < 0.4 < 0.6 < 0.8 < 1 is a 
refinement of the partition for 0 < 0.4 < 0.8 < 1. If furthermore πχ is given by the 
sequence 0 < 0.5 < 0.8 < 1 and π2 fry 0 < 0.2 < 0.8 < 1, their meet is obtained 
from the sequence 0 < 0.2 < 0.5 < 0.8 < 1 and their join from 0 < 0.8 < 1. 
A.2 VALUATION ALGEBRAS ON GENERAL LATTICES 
The definition of a valuation algebra given at the beginning of this chapter is based 
on a particular lattice, namely on the powerset lattice of variables (see Example A.2). 
This lattice is distributive. However, it turns out that the valuation algebra framework 
can even be generalized to arbitrary lattices (Shafer, 1991; Kohlas, 2003). Thus, let 
D be a lattice with a partial order < and the two operations meet Л and join V. We 
denote by Ф the set of valuations with domains in D and suppose the following three 
operations defined on Ф and D. 
1. Labeling: Ф —► D; φ ι-»· ά{φ), 
2. Combination: Φ x Φ -> Φ; (φ, -φ) ι-> φ (g> ψ, 
3. Projection: Φ x D -t Ф; (ф, х) ь-> ф^х for χ < ά(φ). 
We impose the following set of axioms on Φ and D: 
(ΑΙ') Commutative Semigroup: Φ is associative and commutative under (g>. 
(A2') Labeling: For φ, ψ e Φ, 
ά{φ®φ) = ά{φ)νά(ψ). 
(A.l) 
(A3') Projection: For ф е Ф, x e D and x < ά(φ), 
ά{φίχ) 
= 
x. 
(A.2) 
(A4') Transitivity: For φ e Φ and x < у < ά(φ), 
(φ^γχ 
= ф±х. 
(А.З) 

VALUATION ALGEBRAS ON GENERAL LATTICES 
21 
(A5') Combination: For φ, ψ € Φ with ά{φ) = x, ά(ψ) = y and z € D such that 
x < z < x V y, 
(ф® ip)iz 
= 
φ®ψ IzAy 
(A.4) 
(A6') Domain: For 0 e Φ with ά(φ) = x, 
Ux 
(A.5) 
Clearly, this definition of a valuation algebra is more general than the framework 
introduced beforehand. It is indeed surprising that no further properties of the lattice 
are required to enable the application of local computation (Kohlas & Monney, 1995). 
We must, however, accept that not all properties of the more restricted concept of 
a valuation algebra are maintained. Although the subsequent chapters of this book 
are entirely based on variable systems, we nevertheless present an example from 
diagnostics that is based on a valuation algebra with domains from a more general 
lattice, i.e. from a lattice of partitions (Kohlas & Monney, 1995; Shafer et al., 1987). 
Figure A.l A tree of diagnoses for stubborn cars. 
Example A.5 Suppose that your car does not start anymore and you want to deter-
mine the cause of its failure. The tree of diagnoses shown in Figure A. 1 provides a 
structural approach for the identification of the cause of failure. At the beginning, 
the list of possible diagnoses is fairly coarse and only contains the failure possi-
bilities Ωι = {a, b, c, d}. Note, however, that these options are mutually exclusive 
and collectively exhaustive. More precise statements are obtained by partitioning 

22 
VALUATION ALGEBRAS 
coarse diagnoses. Doing so, we replace diagnosis a by {e, /} and diagnosis с by 
{l,g, h} which leads to the new set of failure possibilities Ω2 = {e, /, b, I, g, h, d}. 
Afier a third specialization step we finally obtain Ω3 = {e, /, b, i, j , k, g, h, d}. These 
step-wise refined sets of diagnoses are called frames of discernment. When a frame 
Ωί is replaced by a finer frame £ii+\ the substitution of a diagnosis by a set of more 
precise diagnoses is described by a mapping 
г : Qi 
V{üi +1) 
(A.6) 
called refinement mapping or simply refinement (Shafer, 1976). Refinement mappings 
must satisfy the following requirements: 
1. τ(6>)/0/οΓα//6»€Ω<,· 
2. τ(θι) Π τ(θ2) = 0 whenever θι φ θ2; 
3. υ{τ{θ) :веПг} 
= Ωι+1. 
For example, the passage from Ω,χ to Ω2 is expressed by the mapping 
T(X) 
= 
{x} 
ifx£{b,d}, 
{e,f} 
ifx = a, 
Jl,9,h} 
ifx = c. 
From an alternative but equivalent point of view, this family of related frames can be 
seen as a collection of partitions of the universe Ω3. These partitions are: 
• ito = 
{{e,f,b,i,i,k,g,h,d}}; 
• 7Ti = 
{{e,f},{b},{i,j,k,g,h},{d}}; 
• *2 = {{e}, {/}, {b}, {i,j, h}, {g}, {h}, {d}}; 
• ^з = {{e}, {/}, {b}, {i}, {j}, {k}, {g}, {h}, {d}}. 
Note that π,+ ι < π,, i.e. every block of Vj+i is contained in some block of V;. Because 
each frame Ω^ corresponds to a partition 7Tj, we may replace the frames in equation 
(A.6) by their corresponding partitions. The mapping 
δ : π; 
V(ni+1) 
assigns to each block В G 7Tj the set of blocks in TÏÎ+\ whose union is B. The passage 
from 7Γι to π2 is thus expressed by the mapping 
6{x) 
{{x}} 
ifx = {b,d}, 
{{e},{/}} 
ifx = 
{e,f], 
.{{«.J.^I.M.i' 1}} 
'fx = 
{i,j,k,g,h}. 
This mapping is referred to as decomposition mapping. 

VALUATION ALGEBRAS ON GENERAL LATTICES 
23 
Although many valuation algebras with domains from variable lattices could be 
adapted to partition and other lattices, the modified valuation algebra of set potentials 
from Dempster-Shafer theory is probably the most important example for practical 
applications. In fact, the derivation of partitions from a tree of diagnoses takes center 
stage in the applications of medical diagnostics in (Gordon & Shortliffe, 1985). Other 
authors studied Dempster-Shafer theory on unspecified lattices (Grabisch, 2009; 
Shafer, 1991). Here, we follow the algebraic approach from (Kohlas & Monney, 
1995) and first extract the necessary requirements from the above example that later 
enables the definition of set potentials with domains from partition lattices. 
Definition A.8 A collection offrantes T together with a collection of refinements V, 
forms a family of compatible frames if the following conditions hold: 
1. For each pair Ωι, Ω2 G T of frames, there is at most one refinement r : Ωχ —> 
Ρ(Ω2) in П. 
2. There exists a set U and a mapping φ : T —> Part(U) such that for all frames 
Ωι,Ω2 G J- we have φ(Ωι) ф </?(^2) whenever Ωι ф Ω2, and such that 
φ{Τ) — {φ(Ώ,) : Ω G J7} forms a bounded sublattice of Part(U). 
3. For each frame Ω G T there is a bijective mapping b : Ω —> φ(Ω). This 
mapping identifies for each frame element the corresponding block in the 
partition that contains this element. 
4. There is a refinement τ : Ωι —> 'Ρ(Ω2) in TZ exactly ί/<^(Ω2) < ¥>(Ωι). 
Condition 2 means that every frame corresponds to a partition and that the cor-
responding mapping is an embedding of T into Part(U). Since the least partition 
of U is in φ{Τ), there is exactly one frame Ω whose partition φ(Ώ.) corresponds 
to the least partition. By the mapping of Condition 3, the elements of the frame Ω 
are in one-to-one correspondence with the blocks of the least partition that are the 
singletons of the universe U. From a mathematical point of view, the two sets U and 
Ω are thus isomorphic. Finally, as a consequence of Condition 4, we can introduce a 
decomposition mapping for each refinement. Indeed, if τ : Ωι —> ν(Ωϊ), we have 
</?№) < ¥>(Ωι) which means that every block in φ{Ω2) is contained in some block 
of <p(Sl\). We may therefore assume a decomposition mapping 
δ : ν>(Ωι) 
-> 
?{<РШ) 
(АЛ) 
that assigns to each block В G </>(Ωι) the set of blocks in ιρ(Ω2) whose union is B. 
This mapping can further be extended to sets of blocks δ : Т(щ) —»■ 'Ρ(πί+ι) by 
δ(Α) = \J{S(B):BeA}. 
(A.8) 
for А С 7г». Such families of compatible frames provide sufficient structure for the 
definition of set potentials over partition lattices (Kohlas & Monney, 1995). 

24 
VALUATION ALGEBRAS 
■ A.7 Set Potentials on Partition Lattices 
Let (T, 1Z) be a family of compatible frames with universe U and partition 
lattice Part(U). A set potential m : V{ir) —)· R>0 with domain d(m) — 
π G φ(T) ç Part(U) is defined by a mapping that assigns non-negative real 
numbers to all subsets of the partition π. Let us again consider the inverse 
natural order between partitions. According to the labeling axiom, the domain 
of the combination of m\ and m2 with domains πι and π2 must be πχ V π2, 
which corresponds to the coarsest partition that is finer than πι and π2. We 
then conclude from Condition 2 that frames Ωι,Ω2 and Ω exist such that 
7Ti = ψ(Ω,ι), π2 = ψ{^2) and πι V f2 = ψ(Ω)- Since πι,π2 < i i V f2 in 
the natural order, it follows from Condition 4 that the two refinement mappings 
TI : Ω -» Τ^Ωχ) and r2 : Ω -»■ Ρ(Ω2) exist in 7?.. We thus obtain the 
corresponding decomposition functions δι : V(w\ V π2) —» Ρ(πι) and <52 : 
■ρ(πι V π2) —>■ 73(π2). Altogether, this allows us to define the combination 
rule: For А С πι V π2 we define 
т ! ® т 2 ( Л ) 
= 
^ 
m1(<51(ß))-m2((52(C)) 
(A.9) 
л=впс 
For the projection operator, we assume a set potential m with domain ж\ 
and 7Г2 < πι. By the same justification we find the decomposition mapping 
δ : V(ITI) —> V{IÎ2) 
and define for A Ç π2 the projection rule as follows: 
т ^ ^ Л ) 
= 
^ 
m(ß). 
(A.10) 
ВС2т!1:АПО(В)ф$ 
The proof that set potentials over partition lattices satisfy the axioms of a 
valuation algebra on general lattices can be found in (Kohlas & Monney, 1995). 
A.3 VALUATION ALGEBRAS WITH PARTIAL PROJECTION 
The valuation algebra definition given at the beginning of this chapter allows every 
valuation ф e Φ to be projected to any subset of ά(φ). Hence, we may say that φ 
can be projected to all domains in the marginal set Л4(ф) = Т(а(ф)). Valuation 
algebras with partial projection are more general in the sense that not all projections 
are necessarily defined. In this view, Λ4(φ) may be a strict subset of ν(ά(φ)). It is 
therefore sensible that a fourth operation is needed which produces Μ(ά(φ)) for all 
φ G Φ. Additionally, all axioms that bear somehow on projection must be generalized 
to take the corresponding marginal sets into account. Thus, let Φ be a set of valuations 
over domains s Ç r and D = V{r). We assume the following operations in (Ф, D): 
1. Labeling: Ф -»· D; φ ι-> ά(φ), 
2. Combination: ΦχΦ->Φ;(ι|ι,ψ)ι->^ψ, 
3. Domain: Φ ->■ V(D); φ ■->■ Μ(φ), 

VALUATION ALGEBRAS WITH PARTIAL PROJECTION 
25 
4. Partial Projection: Φ x D —>■ Ф; (ф, x) н-)· <^"x denned for x G -M(</>). 
The set Μ{φ) contains therefore all domains x G D such that the marginal of ф 
relative to x is denned. We impose now the following set of axioms on Ф and D, 
pointing out that the two Axioms (Al") and (A2") remain identical to the traditional 
definition of a valuation algebra. 
(Al") Commutative Semigroup: Ф is associative and commutative under ®. 
(A2") Labeling: For ф, ф G Ф, 
ά{φ®φ) 
= 
ά(φ)υά(ψ). 
(A.ll) 
(A3") Projection: For φ G Φ and x G Μ(φ), 
ά(φ1χ) 
= 
x. 
(A. 12) 
(A4") Transitivity: If φ G Φ and i Ç j C ά(φ), then 
ж е 7W(0) = я е M ^ ) , ? / G Λί(0) and ( ^ ) ^ = ф1х. 
(А.13) 
(Α5") Combination: If φ,ψ G Φ with tf (<£) = x, ά(ψ) = y and z £ D such that 
ж Ç z С a; U у, then 
г П у G Λ4(νθ =!>2£ Μ(φ <2> φ) and (0 ® V)iz = 0 ® V4'202'· 
(A. 14) 
(Α6") Domain: For 0 G Φ with d(0) = ж, we have a; G ΛΊ(</>) and 
4>ix 
= 
ф. 
(А. 15) 
Definition A.9 A system (Φ, £)) together with the operations of labeling, combi-
nation, partial projection and domain satisfying these axioms is called a valuation 
algebra with partial projection. 
It is easy to see that this system is indeed a generalization of the traditional 
valuation algebra, because if Μ{φ) = Τ(ά(φ)) holds for all φ G Φ, the axioms 
reduce to the system given at the beginning of this chapter. Therefore, the latter is 
also called valuation algebra with full projection. 

2 6 
VALUATION ALGEBRAS 
■ A.8 Quotients of Density Functions 
Let us reconsider the valuation algebra of density functions introduced in 
Instance 1.5 and assume that / is a positive density function over variables in 
s С r, i.e. for x £ Г we always have /(x) > 0. We consider the quotient 
with t С s. For any t Ç s these quotients represent the family of condi-
tional distributions of x^ s _ t given x.^ associated with the density /. However, 
projection is only partially defined for such quotients since 
This is a constant function with an infinite integral and therefore not a density 
anymore. Nevertheless, conditional density functions are part of a valuation 
algebra with partial projection. A formal verification of the corresponding 
axiomatic system can be found in (Kohlas, 2003). 
PROBLEM SETS AND EXERCISES 
A.l * Verify the valuation algebra axioms for the relational algebra of Instance 1.2 
without restriction to variables with finite frames. 
A.2 * Reconsider the valuation algebra of arithmetic potentials from Instance 1.3. 
This time, however, we restrict ourselves to the unit interval and replace the operation 
of addition in the definition of projection in equation (1.11) by maximization. This 
leads to the formalism of possibility potentials or probabilistic constraints that will 
later be discussed in Instance 5.2. Prove that the valuation algebra axioms still hold 
in this new formalism. 
A.3 * Again, reconsider the valuation algebra of arithmetic potentials from Instance 
1.3. This time, we replace the operation of multiplication in the definition of combi-
nation in equation ( 1.10) by addition and the operation of addition in the definition of 
projection in equation (1.11) by minimization. This leads to the formalism of Spohn 
potentials or weighted constraints which will later be discussed in Instance 5.1. Prove 
that the valuation algebra axioms still hold in this new formalism. Alternatively, we 
could also take maximization for the projection rule. 
A.4 * Cancellativity is an important property in semigroup theory and will be used 
frequently in later parts of this book. A valuation algebra (Ф, D) is called cancellative, 

EXERCISES 
2 7 
if its semigroup under combination is cancellative, i.e. if for all φ G Φ 
φ®ψ 
= 
φ<8> Φ' 
implies that ψ = ψ'. Prove that cancellativity holds in the valuation algebras of 
arithmetic potentials from Instance 1.3, Gaussian potentials from Instance 1.6 and 
Spohn potentials from Exercise A.3. 
A.5 * The domain axiom (A6) expresses that valuations are not affected by trivial 
projection. Prove that this axiom is not a consequence of the remaining axioms (Al) 
to (A5), i.e. construct a formalism that satisfies the axioms (Al) to (A5) but not the 
domain axiom (A6). The basic idea is given in (Shafer, 1991): take any valuation 
algebra and double the number of elements by distinguishing two versions of each 
element, one marked and one unmarked. We further define that the result of a pro-
jection is always marked and that a combination produces a marked element if, and 
only if, one of its factors is marked. Prove that all valuation algebra axioms except 
the domain axiom (A6) still hold in this algebra. 
A.6 * * Study the approximation of probability density functions by discrete prob-
ability distributions and provide suitable definitions of combination and projection. 
A.7 * * * 
it was shown in Instance 1.6 that Gaussian potentials form a subalgebra 
of the valuation algebra of density functions. Look for other parametric classes 
of densities that establish subalgebras, for example uniform distributions, or more 
general classes like densities v/ith finite or infinite support. The support of a density 
corresponds to the part of its range where it adopts a non-zero value. 

CHAPTER 2 
INFERENCE PROBLEMS 
With the valuation algebra framework introduced in the first chapter, we dispose 
of a system to express the structure of information independently of any concrete 
formalism. This system will now be used to describe the main computational problem 
of knowledge representation systems: the task of computing inference. Let us go 
back to our initial idea of valuations as pieces of knowledge or information. Given 
a collection of information pieces called knowledgebase and some query of interest, 
inference consists in aggregating all elements of the knowledgebase, followed by 
projecting the result to a specified domain representing the question of interest. This 
computational problem called inference or projection problem will take center stage 
in many of the following chapters. Here, we start with a short presentation of some 
important graphical structures used to represent knowledgebases and to uncover 
their hidden structure. Then, we give a formal definition of inference problems in 
Section 2.3, followed by a selection of examples that arise from the valuation algebra 
instances of Chapter 1. Most interesting are the different meanings that the inference 
problem adopts under different valuation algebras. We therefore not only say that 
formalisms instantiate valuation algebras, but the computational problems based on 
these formalisms also instantiate the generic inference problem. Algorithms for the 
efficient solution of inference problems will be presented in Chapter 3 and 4. 
Generic Inference: A Unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc. 
29 

30 
INFERENCE PROBLEMS 
2.1 GRAPHS, TREES AND HYPERGRAPHS 
An undirected graph G = (V, E) is specified by a set of vertices or nodes V and a 
set of edges E. Edges are sets of either one or two nodes, i.e. E Ç V\ U V2 where V\ 
denotes the set of all one-element subsets and V2 the set of all two-element subsets 
of V. Without loss of generality, we subsequently identify nodes by natural numbers 
V С N. The neighbors of node г G V are all other nodes with an edge to node i, 
ne(i) = {j e V : {i,j} E. E}, and the degree of a graph G is the maximum number 
of neighbors over all its nodes: 
deg(G) 
= 
max|ne(i)|. 
(2.1) 
A sequence of nodes (io, *i,... ,г„) with {ik-i,ik} 
€ E for к = 1,..., n is called 
a ραί/г of length n from the source node г'о to the terminal node in. A graph is said 
to be connected if there is a path between any pair of nodes. A cycle is a path with 
го = in- A iree is a connected graph without cycles and V\ — 0. Trees satisfy the 
property that removing one edge always leads to a non-connected graph. A subset 
of nodes W С V is called a clique, if all elements are pairwise connected, i.e. if 
{i,j} € £7 for all г, j € W with г ф j . A clique is maximal if the set obtained by 
adding any node from V — W is no more a clique. 
Example 2.1 Figure 2.1 displays two graphs: The left-hand graph G\ = ( Vi, E\ ) is 
specified by Vx = {1,2,3,4} with EY = {{1,2}, {2,3}, {3,1}, {4}}. 77гй grap/г Й 
ηοί connected, the neighbors of node 2 are ne(2) = {1,3}, anuf ί/ге node sei {1, 2, 3} 
forms a maximal clique. On the right, G2 = (V2, £2) is given by V2 = {1, 2, 3,4, 5} 
and E2 = {{5,1}, {1, 2}, {1,3}, {3,4}}. This graph is connected and since there 
are no cycles, it is a tree. The neighbors of node 1 are ne(l) = {2,3,5}, and the 
degree of G 2 is deg{G2) = 3. 
Figure 2.1 Undirected graphs. 
In a directed graph G = (V, E) edges are ordered pairs of nodes E Ç V x V. 
We thus refer to (i,j) € E as the directed edge from node г to node j . A sequence 
of nodes (г'о, ii,..., 
in) with (ik-i,ik) 
€ E {or к = 1,..., n is called a /?αίη of 
length n from the source node io to the terminal node in. If г0 = г„ the path is again 
called a cycle. A directed graph is connected if for every pair of nodes i and j there 
exists either a path from г to j or from j to г. For every directed graph we find an 

GRAPHS, TREES AND HYPERGRAPHS 
31 
associated undirected graph by ignoring all directions of the edges. We then say that 
a directed graph is a tree if its associated undirected graph is a tree. Moreover, if a 
directed graph is a tree and all edges are directed towards a selected node, then the 
graph is called a directed tree with the selected node as root node. 
Example 2.2 Consider the two graphs in Figure 2.2. The left-hand graph G\ = 
(Vi.Bi) is specified by νλ = {1,2,3,4} with Ег = {(1,3), (2,1), (2,3), (4,4)}. 
This graph is not connected. On the right, Gi = (ν^,-Ε^) is given by V2 = 
{1,2,3,4,5} and E2 = {(5,1), (2,1), (3,1), (4,3)}. This graph is a directed tree 
with root node 1. 
M 
Figure 2.2 Directed graphs. 
Next, we consider directed or undirected graphs where each node possesses a label 
out of the powerset V(r) of a set of variables r. Thus, a labeled, undirected graph 
(V, E, A, V{r)) is an undirected graph with a labeling function X : V —> V(r) that 
assigns a domain to each node. Such a graph is shown in Example 2.3. 
Example 2.3 Figure2.3 shows a labeled graph (V,E,X,V(r)) 
whereV = {1,2,3,4, 
5}, E = {{5,1}, {1,2}, {1,3}, {2,4}, {4,3}} am/r = {A,B,C,D}. 
The labeling 
function X is given by: λ(1) = λ(4) = {А, С, D}, A(2) = {A, D}, A(3) = {D} and 
A(5) = {A, B}. Observe that node labels are not necessarily unique and therefore 
cannot replace the node numbering. 
Figure 2.3 A labeled graph. 
A hypergraph H = (V, S) is specified by a set of vertices or nodes V and a set of 
hyperedges S = {S\,..., 
Sn} which are subsets of V, i.e. Si Ç V for г = 1,..., п. 

32 
INFERENCE PROBLEMS 
The primal graph of a hypergraph is an undirected graph (V, Ep) such that there is an 
edge {u, v} e Ep for any two vertices u,v eV that appear in the same hyperedge, 
Ep 
= 
{{u, v} : u, v e Si for some 1 < г < n). 
The dual graph of a hypergraph is an undirected graph (S,Ed) that has a vertex for 
each hyperedge, and there is an edge {Si, Sj} € Ed if the corresponding hyperedges 
share a common vertex, i.e. if Si П Sj Ф 0. 
Example 2.4 Consider a hypergraph (V,S) with V = {1,2,3,4,5,6} and S = 
{{1,2,3}, {3,4,5}, {1,5,6}}. Figure 2.4 shows a graphical representation of this 
hypergraph together with its associated primal and dual graphs. 
Figure 2.4 
The hypergraph and its associated primal and dual graphs of Example 2.4. 
2.2 KNOWLEDGEBASES AND THEIR REPRESENTATION 
Consider a valuation algebra (Ф, V{r)) for a set of variables r and a set of valua-
tions {φι,..., 
фп} С Ф. This valuation set stands for the available knowledge about 
some topic which constitutes the starting point of any deduction process. Accord-
ingly, we refer to such a set as knowledgebase. It is often very helpful to represent 
knowledgebases graphically, since this uncovers a lot of hidden structure. There are 
numerous possibilities for this task, but most of them are somehow related to a par-
ticular instance. A general representation of knowledgebases are valuation networks: 
A valuation network (Shenoy, 1992b) is a graphical structure where variables are 
represented by circular nodes and valuations by rectangular nodes. Further, each 
valuation is connected with all variables of its domain. Valuation networks highlight 
the structure of a factorization and are therefore also called factor graphs (Kschis-
chang et al, 2001) in the literature. Another general representation is given by the 
hypergraph H = (r, S) with S — {ά(φι),..., 
ά(φ„)} and its associated primal and 
dual graphs. These possibilities are also illustrated in Example 2.5. 
Example 2.5 Figure 2.5 shows a valuation network, a hypergraph and its associated 
primal graph for the knowledgebase {ф1,ф2,Фз} with ά(φ\) = {А,С}, d(<fo) = 
{B, C, D} and ά(φ3) = {В, D}. 

THE INFERENCE PROBLEM 
3 3 
Figure 2.5 Representations for the knowledgebase in Example 2.5. 
2.3 THE INFERENCE PROBLEM 
The knowledgebase is the basic component of an inference problem. It represents all 
the available information that we may want to combine and project on the domains 
of interest. This is the statement of the following definition. 
Definition 2.1 The task of computing 
<piXi = 
{Φι ® · · · ® ΦηΫΧί 
(2.2) 
for a given knowledgebase {φι,... 
,φη} Q Φ and domains x = {x\,..., 
xs } where 
Xi С ά(φι ® ...®φ„) 
is called an inference problem or a projection problem. The 
domains x» are called queries. 
The valuation φ = φχ <S> ■ ■. <S> φη resulting from the combination of all knowl-
edgebase factors is often called joint valuation or objective function (Shenoy, 1996). 
Further, we refer to inference problems with |x| = 1 as single-query inference prob-
lems and we speak about multi-query inference problems if \x\ > 1. For a better 
understanding of how different instances give rise to inference problems, we first 
touch upon a selection of typical and well-known fields of application that are based 
on the valuation algebra instances introduced in Chapter 1. Efficient algorithms for 
the solution of inference problems are discussed in Chapter 3 and 4. Here, we only 
focus on the identification of inference problems behind different applications. 
■ 2.1 Bayesian Networks 
A Bayesian network, as for instance in (Pearl, 1988), is a graphical representa-
tion of a. joint probability distribut ion over a set of variables r = {X\,..., 
Xn}· 
The network itself is a directed, acyclic graph that reflects the conditional inde-
pendencies among variables, which are associated with a node of the network. 
Each node contains a conditional probability table that quantifies the influence 
between variables. These tables can be considered as probability potentials, 
introduced as Instance 1.3. The joint probability distribution p of a Bayesian 

3 4 
INFERENCE PROBLEMS 
network is given by 
Р №, . . . Д П ) 
ï[p(Xi\rtXi)) 
(2.3) 
where pa(Xi) denotes the parents of Xi in the network. If the set pa(Xt) is 
empty, the conditional probability becomes an ordinary probability distribution. 
Consider next a set q Ç r called query variables and a set e С г called evidence 
variables. The evaluation of a Bayesian network then consists in computing the 
posterior probability distribution for the query variables, given some observed 
values of the evidence variables. Such an observation corresponds to a tuple 
e e Ω6. Thus, if q € ilq denotes a query event, we want to compute the 
posterior probability p(q|e) defined as 
p(q|e) 
p(q>e) 
p(e) 
(2.4) 
To clarify how Bayesian networks give rise to inference problems, we consider 
an example from medical diagnostics (Lauritzen & Spiegelhalter, 1988). 
Shortness-of-breath (dyspnoea) may be due to tuberculosis, lung cancer or bron-
chitis, or none of them, or more than one of them. A recent visit to Asia increases 
the chances of tuberculosis, while smoking is known to be a risk factor for both 
lung cancer and bronchitis. The results of a single chest X-ray do not discriminate 
between lung cancer and tuberculosis, as neither does the presence or absence 
of dyspnoea. A patient presents himself at a chest clinic with dyspnoea and has 
recently visited Asia. Smoking history and chest X-ray are not yet available. The 
doctor wants to know the chance that this patient suffers from bronchitis. 
Here, the query and evidence variables are easily identified: the query set q = 
{B} contains the variable standing for bronchitis, and the evidence variables 
e = {A, D} are dyspnoea and the recent visit to Asia. Figure 2.6 shows the 
Bayesian network of this example that reflects the dependencies among the 
involved variables r = {A,B,D,E,L,S,T,X}. 
All variables are assumed 
to be binary, e.g. the frame values Ω$ = {s, s} refer to a smoking or non-
smoking patient respectively. Also, the Bayesian network shows the conditional 
probability tables that are associated with each node. These are the valuations of 
our example and can be modelled using the language of probability potentials. 
For example, we have 
p(T\A) 
Thus, we get a knowledgebase that consists of eight valuations (one probability 
potential for each node of the Bayesian network) and the joint valuation (joint 
T 
t 
t 
t 
t 
A 
a 
a 
a 
U 
0.05 
0.01 
0.95 
0.99 

THE INFERENCE PROBLEM 
35 
probability mass function) is given by 
p(A,B,D,E,L,S,T,X) 
= 
p(A)®p(T\A) 
)P(D\E,B). 
(2.5) 
p(t|a) = 0.05 
p(t|a) = 0.01 
J_P(a) ■ 
to Л 
a 
J 
-- 0.01 
p(l|s) = 0.1 
p(l|5) = 0.01, 
p(x|e) = 0.98 
p(xjs) = 0.05 
p(s) = 0.4 
p(b|s) = 0.6 
p(b|5) = 0.3 
p(d|e,b) = 0.9 
p(d|e,b) = 0.7 
p(d|e,b) = 0.8 
p(d|B,b) = 0.1 
Figure 2.6 The Bayesian network of the medical example. The bold-faced, underlined letters 
refer to the variable associated with the corresponding node. 
To answer the query p(B\ A, D) we therefore need to compute 
p(A, B, D) 
p{A, B, D, E, L, S, T, 
X)1{A,B,D} 
p{B\A,D) 
p(A,D) 
p{A,B,D,E,L,S,T,X)^,D] 
and obtain our final answer by a last table lookup in p{B\ A, D) to extract the 
entry that corresponds to the query and evidence event. To sum it up, evaluating 
the Bayesian network of the medical example requires to compute an inference 
problem with knowledgebase {p(A),p(T\A),... 
,p(D\E, B)} and query set 
{{A, B, D}, {A, D}}. In fact, it is sufficient to compute the query {A, B, D} 
because p(A, D) can easily be obtained from this result by one last projection 
p(A,D) 
= 
p(A,B,D)^A'D 
(2.6) 
2.2 Query Answering in Relational Databases 
The procedure of query answering in a relational database is another example 
of an inference problem. The knowledgebase is a set of database tables that 
are elements of the valuation algebra of relations introduced as Instance 1.2. 
The inference problem then computes the natural join of all relations in the 
knowledgebase and projects the result on the attributes of interest. However, it 
is important to note that the selection operator, which is integral part of every 

3 6 
INFERENCE PROBLEMS 
relational database system, does not directly have a counterpart in the valuation 
algebra framework. Nevertheless, there are techniques that allow embedding 
selections. A particular simple but limited possibility is to transform selections 
into relations themselves that are then added to the knowledgebase. The ad-
vantage of this method is that we stay on a generic level and do not need to 
care about selections during the computations. On the other hand, transforming 
selections may create relations of infinite cardinality if special queries over 
attributes with infinite frames are involved. In such cases, more complex tech-
niques are required that extend the definition of inference problems and treat 
selections explicitly. We refer to (Schneuwly, 2007) for a detailed discussion 
of selection enabled local computation. In the following (self-explanatory) 
example we focus on the first method and convert selections into relations. 
SID 
1 
2 
3 
Student 
Ann 
John 
Laura 
SID 
1 
2 
2 
Grade 
С 
A 
В 
LID 
901 
901 
903 
LID 
901 
902 
903 
Lecture 
Chemistry 
Mathematics 
Physics 
Semester 
S1898 
S1801 
A1905 
PID 
1 
2 
3 
LID 
903 
902 
901 
PID 
1 
2 
3 
Professor 
Einstein 
Gauss 
Curie 
Thus, we get a knowledgebase of five relations {ri,...,r^}. 
Let us further 
assume the following database query: 
Find all students with grade A or В in a lecture of professor Einstein. 
This gives us the single query for the inference problem { Student}. In addition, 
it contains two selections which correspond to the following new relations: 
Sl 
Grade 
A 
В 
S2 
= 
Professor 
Einstein 
Thus, we finally derive the inference problem 
П <E> r 2 <8> r3 <B> r4 <S> r 5 (g> s i >s2 
^{Students} 
2.3 Reasoning with Mass Functions 
Mass functions correspond to the formalism of set potentials introduced as 
Instance 1.4 with an additional condition of normalization. They are used to 

THE INFERENCE PROBLEM 
3 7 
represent knowledge in the Dempster-Shafer theory of evidence (Shafer, 1976) 
and provide similar reasoning facilities as Bayesian networks, but with different 
semantics as pointed out in (Smets & Kennes, 1994; Kohlas & Monney, 1995). 
It will be shown in Section 4.7 that normalization in the context of valuation 
algebras can be treated on a generic level. We therefore neglect normalization 
and directly use set potentials for reasoning with mass functions in the subse-
quent example. Normalized set potentials will later be introduced in Instance 
D.9. The following imaginary example has been proposed by (Shafer, 1982): 
Imagine a disorder called ploxoma which comprises two distinct diseases: θι 
called virulent ploxoma, which is invariably fatal, and 02 called ordinary plox-
oma, which varies in severity and can be treated. Virulent ploxoma can be 
identified unequivocally at the time of a victim's death, but the only way to 
distinguish between the two diseases in their early stages is a blood test with 
three possible outcomes Xi, X2 and X3. The following evidence is available: 
1. Blood tests of a large number of patients dying of virulent ploxoma showed 
the outcomes Xi, X2 and X3 occurring 20, 20 and 60 per cent of the time, 
respectively. 
2. A study of patients whose ploxoma had continued so long as to be almost 
certainly ordinary ploxoma showed outcome Xi to occur 85 per cent of 
the time and outcomes X2 and X3 to occur 15 per cent of the time. (The 
study was made before methods for distinguishing between X2 and X3 
were perfected.) There is some question whether the patients in the study 
represent a fair sample of the population of ordinary ploxoma victims, but 
experts feel fairly confident (say 75 per cent) that the criteria by which 
patients were selected for the study should not affect the distribution of 
test outcomes. 
3. It seems that most people who seek medical help for ploxoma are suffering 
from ordinary ploxoma. There have been no careful statistical studies, but 
physicians are convinced that only 5-15 per cent of ploxoma patients suffer 
from virulent ploxoma. 
This story introduces the variables D for the disease with the possible values 
{#i,#2} and T for the test result taking values from {Χχ,Х2,Хз}. 
(Shafer, 
1982) comes up with a systematic disquisition on how to translate this evidence 
into mass functions in a meaningful manner. Here, we do not want to enter into 
this discussion and directly give the corresponding knowledgebase factors. In-
stead, we focus on the fact that reasoning with mass functions again amounts 
to the solution of an inference problem. 
The mass function тгц derived from the first statement is: 
πΐ\ 
For example, our belief that the blood test returns ΧΆ for a patient with virulent 
ploxoma θ\ is 0.6. This mass function does not give any evidence about the 
{(θ1,Χΐ)Λθ2,Χ1),(θ2,Χ2),(θ2,Χ3)} 
{(θΐ,Χ2)Λθ2,Χΐ)Λθ2,Χ2),(θ2,Χ3)} 
{(0 1,Хз),(о 2,Х 1),(о 2,Х 2),(о 2,Хз)} 
0.2 
0.2 
0.6 

38 
INFERENCE PROBLEMS 
test outcome with disease #2- The second factor таг is: 
TO2 
= 
{{θ2,Χΐ)Λθΐ,Χΐ)Λθΐ,Χ2),{01,Χ3)} 
{(θ2,Χ2),(θ2,Χ3)Λθΐ,Χΐ)Λθΐ,Χ2),(θΐ,Χ3)} 
{(θ2,Χΐ),(θ2,Χ2)Λθ2,Χ3),(θΐ,Χΐ)Λθΐ,Χ2)Λθΐ,Χ3)} 
0.85 · 0.75 
0.15-0.75 
0.25 
{(Öi)} 
{№)} 
{№).№)} 
0.05 
0.85 
0.10 
For example, in case of ordinary ploxoma #2 the test outcome is X\ in 85 per 
cent of all cases. This value is scaled with the expert's confidence of 0.75. There 
are 25 per cent of cases where the test tells us nothing whatsoever. Finally, the 
third mass function таз is: 
m3 
For example, the belief that a patient, who seeks medical help for ploxoma, 
suffers from #2 is 0.85. Here, the value 0.1 signifies that it is not always possible 
to distinguish properly between the two diseases. 
Let us now assume that we are interested in the reliability of the test result. 
In other words, we want to compute the belief held in the presence of either θ\ 
or 02 given that the test result is for example X\. Similar to Bayesian networks, 
this is achieved by adding the observation m0 to the knowledgebase which 
expresses that the test result is X\ : 
mn 
{(*i)} 
1.0 
Thus, we obtain the belief for the presence of either virulent or ordinary 
ploxoma by solving the following computational task: 
mi <g> m2 ® та3 )ГП0\ 
. 
This is clearly an inference problem with knowledgebase {та1,та2,таз,та0} 
and query {T}. Although this example with only 4 knowledgebase factors, one 
query and two variables seems easily manageable, it is already a respectable 
effort to solve it by hand. We therefore refer to (Blau et ai, 1994) where these 
computations are executed and commented. 
2.4 Satisfiability 
Let us consider a digital circuit build from AND, OR and XOR gates which are 
connected as shown in Figure 2.7. Each gate generates an output value from 
two input values according to the following tables: 
and 
0 
1 
0 
1 
0 
0 
0 
1 
or 
0 
1 
0 
1 
0 
1 
1 
1 
xor 
0 
1 
0 
1 
0 
1 
1 
0 

THE INFERENCE PROBLEM 
39 
This particular circuit is caWed fiill adder since it computes the binary addition 
of the three input values as shown in Figure 2.8. The value Outi corresponds 
to the carry-over bit. 
Figure 2.7 A digital circuit of a binary adder. 
Ini 
0 
0 
0 
0 
1 
1 
1 
1 
In.2 
0 
0 
1 
1 
0 
0 
1 
1 
1пз 
0 
1 
0 
1 
0 
1 
0 
1 
Outi 
0 
1 
1 
0 
1 
0 
0 
1 
Out2 
0 
0 
0 
1 
0 
1 
1 
1 
Figure 2.8 The result table of a full adder circuit. 
From the adder circuit in Figure 2.7, we extract the following five Boolean 
functions that constitute our knowledgebase {φι,..., 
φ5}: 
Vi = xor(Ini,In2) 
Outi = xor(Vi,In3) 
Out2 = or(V2, V3) 
V2 = and(Vi,In3) 
V3 = and(Ini, 
In2) 
Now, imagine the following situation: we insert the values Ιηχ — 0,Iri2 = 0 
and/пз = 0 into the above adder circuit and observe Outi = landOwÎ2 = 1· 
A quick glance on Figure 2.8 confirms that this result can only be produced by 
a faulty adder component. Fortunately, we are supplied with a voltage meter to 
query the actual values of each line V\ to V3, which can be used to locate the 
fault. However, this requires that the correct line values for the particular input 
are known which cannot be deduced from the above tables. We therefore add 
the three Boolean function φ% : ln\ = 0, ф7 : Ιη2 = 0 and ф$ : In3 = 0 to our 

4 0 
INFERENCE PROBLEMS 
knowledgebase and compute the inference problem with query {Vi, V2, V3}: 
i{Vi,V 2,V 3} 
)1W<P2 
>Φί 
Here, we compute in the valuation algebra of Boolean functions introduced 
in Instance 1.1. Computing this query gives V\ — 1, V2 = 0 and V3 = 0 
as the correct line values which now makes the physical monitoring process 
possible. Alternatively, we may also deduce the correct input or output values 
from partial observations: If we for example know that φ% : 1п$ = 0 and 
Out-2 = 0, solving the inference problem 
'10 
ф\®ф2®Фг®Ф4®Фь[ 
?10 
i{In\ 
,Iri2,Outi} 
gives In\ = 1, In.2 = Out\ = 0. Clearly, it would have been possible to 
extract this result from Figure 2.8 directly, but building this table becomes 
quickly intractable even for an only slightly larger circuit. How such larger 
adder circuits are constructed from the one-bid-adder is shown in Figure 2.9. 
^=ЙВ> 
Figure 2.9 Connecting two 1-bit-adders produces a 2-bit-adder. 
This example of an inference problem is a very classical application of 
constraint programming, and many similar applications can be found in the 
corresponding literature. For example, we refer to (Apt, 2003) for a collection 
of constraint satisfaction problems. However, let us point out that the various 
constraint formalisms indeed represent an important class of valuation algebras, 
but there are many further instances beyond classical constraints. We will 
discuss different constraint formalism in Chapter 5 and also show that they all 
instantiate the valuation algebra framework. 

THE INFERENCE PROBLEM 
41 
2.5 Hadamard Transforms 
The Hadamard transform (Beauchamp, 1984; Gonzalez & Woods, 2006) is 
an essential part of many applications that incorporate error correcting codes, 
random number generation or image and video compression. The unnormalized 
variant of a Hadamard transform uses a 2 m x 2 m Hadamard matrix Hm that is 
defined recursively by the so-called Sylvester construction: 
Hi 
1 
1 
1 
- 1 
H„ 
Hm-1 
Hm-i 
Hm-\ 
—H-m-i 
For instance, we obtain for m — 3, 
/ 
Я , 
V 
1 
1 
1 
1 
- 1 
1 
1 
1 
- 1 
1 
- 1 
- 1 
1 
- 1 
- 1 
1 
1 
- 1 
1 
- 1 
1 
1 
1 
1 
1 \ 
- 1 
- 1 / 
Hadamard matrices have many interesting properties: they are symmetric, or-
thogonal, self-inverting up to a constant and any two rows differ in exactly 
2 m _ 1 positions. The columns of a Hadamard matrix are called Walsh func-
tions. Thus, if we multiply Hadamard matrix with a real-valued vector v, we 
obtain a transformation of this vector into a superposition of Walsh functions 
which constitutes the Hadamard transform. For illustration, assume three bi-
nary variables X\ to X3 and a function v : {Xi, X2, X3} - > 1 . This function 
is completely determined by its values w, for г = 0,..., 7. We then obtain for 
its Hadamard transform, 
Я , 
/ V0 \ 
I V0 + Vi + V2 + V3 + Vi + V5 + Ve + V7 \ 
vi 
v0 - vi + v2 - v3 + Vi - v5 + v6 - v7 
V2 
V0 + Vi - V2 - V3 + Vi + V5 - V6 - V7 
V3 
_ 
V0 - Vi - V2 + V3 + Vi - 1>5 - V6 + V7 
Vi 
V0 + Vi + V2 + V3 — Vi — V5 - V6 — V7 
V5 
V0 - Vi + V2 - V3 - Vi + V5 - V6 + V7 
V6 
V0 + Vi - V2 - V3 - V4 - V5 + V6 + V7 
\ V 7 ) 
\ V0 - Vi - V2 + V3 - Vi + V5 + V6 - V7 / 
An alternative way to compute this Hadamard transform bears on a duplica-
tion of variables (Aji & McEliece, 2000). We introduce the binary variables Υχ 
to Уз which give a binary representation of the line number in the Hadamard 
matrix. Then, the same computational task is expressed by the formula 
Σ 
f(Xi,X2,X3) 
■ (-i)XiYi 
■ (-l)x^ 
■ (-1) 
■X3Y3 
^ 1 >-^2>-^3 

4 2 
INFERENCE PROBLEMS 
The reader may easily convince himself of the equivalence between the two 
computational tasks. In this alternative representation, we discover a sum over 
a multiplication of four discrete functions taking real values or, in other words, 
an inference problem over the valuation algebra of arithmetic potentials, where 
the knowledgebase consists of the four discrete functions, and the single query 
is {Yi, Y2, Y3}. Thus, we could alternatively write for the Hadamard transform: 
/ 
\ 4.(^1,У2,Уз} 
( / ® (-l) X l Y l ® {-\)X*Y* 
® {-\)X*Y* 
j 
(2.7) 
which is reduced to the solution of a singe-query inference problem. 
■ 2.6 Discrete Fourier and Cosine Transforms 
Our last example of an inference problem comes from signal processing. For a 
positive integer iV e N we consider a function / : {0,..., N — 1} —> С that 
represents a sampled signal, i.e. the value f(x) e С corresponds to the signal 
value at time 0 < x < N — 1. Thus, the input signal is usually said to be in 
the time domain although any kind of sampled data may serve as input. The 
complex, discrete Fourier transform then changes the N point input signal into 
two N point output signals which contain the amplitudes of the component 
sine and cosine waves. It is given by 
Л Г - 1 
F(y) 
= ] T / 0 r ) e - ^ . 
(2.8) 
for 0 < у < N — 1. Accordingly, F is usually said to be in frequency domain. 
The discrete Fourier transform has many important applications as for example 
in audio and image processing or data compression. We refer to (Smith, 1999) 
for a survey of different applications and for a more detailed mathematical 
background. Figure 2.10 gives an illustration of the discrete Fourier transform. 
Re F(x) 
1 1 1 1 1 1 1 1 π ^ 
0 
cosine wave amplitudes 
N-1 
Im F(x) 
I I M I I I I ΙΤΤΊ 
0 
sine wave amplitudes 
Ν Ί 
Figure 2.10 Input and output of a complex, discrete Fourier transform. 
f(x) 
The following procedure of transforming a discrete Fourier transform into 
an inference problem was proposed by (Aji, 1999) and (Aji & McEliece, 2000). 

THE INFERENCE PROBLEM 
4 3 
Let us take N = 2m for some m € N and write x and y in binary representation: 
771— 1 
771—1 
x = J2 xJ2J and y = Σ У/2' 
j=0 
(=0 
with Xj, Yj G {0,1}. This corresponds to an encoding of x and y into binary 
configurations x — (X0,..., 
Xm-i) and у = (Υό, · ■ ■, Ym-i)· Observe that 
we may consider the components Xi and Υί as new variables of this system 
(therefore the notation as capital letters). The product xy then gives 
xy = 
Σ 
Xi***1· 
0<j,l<m-l 
We insert this encoding into equation (2.8) and obtain 
Σ 
ηΝο,...,Νη-J-e 
-i-T» 
/V0,...,JVm-i 
Σ 
/(ΛΓο,...,^.!) 
Π 
e 
^ 
ЛГ0,...,ЛГт-1 
0 < j , i < m - l 
ЛГ0,...,ЛГт-1 
0 < i , ; < m - l 
No,...,Nm-i 
0<j+l<m-l 
The last equality follows since the exponential factors become unity when 
j + I > m. Introducing the short-hand notation 
we obtain for the equation (2.8): 
F(K0,...,Km-i) 
= 
Σ 
f(N0,...,Nm-i) 
Ц 
ehl. 
N0,...,Nm-i 
0<j+l<m-l 
This is an inference problem over the valuation algebra of (complex) arithmetic 
potentials. All variables r = {No,..., 
Nm-i,Ko,..., 
Km-i} 
take values 
from {0,1} and the discrete functions / and ejti map binary configurations to 
complex values. The knowledgebase is therefore given by 
{f}U{ejtÎ:0<j 
+ 
l<m-l} 
and the query of the inference problem is {Ко,..., 
Km-\}. 
Thus, we could 
alternatively express the computational task of a discrete Fourier transform as 
, 
α ( ί ί ο , . , κ ^ ι ) 
I / ® e0,o ® · · · ® em_ii0 ® eifl <8> · · · <g> ei j m_ 2 <8> · · · I 
. (2.9) 

4 4 
INFERENCE PROBLEMS 
Observe that the inference problem behind the discrete Fourier transform has 
been uncovered by the artificial introduction of variables that manifests itself 
in the e^i knowledgebase factors. This knowledgebase has a very particular 
structure as shown by the board in Figure 2.11. For m = 8, each cell of this 
board corresponds to one factor e^ with j identifying the row and I the column. 
Observe, however, that only the shaded cells refer to functions contained in the 
knowledgebase. As it will be shown in Section 3.11, this sparse structure of 
the knowledgebase enables the efficient solution of this inference problems by 
the local computation procedures introduced in the next chapter. 
Figure 2.11 
Illustration of the e^i functions in the inference problem of the discrete Fourier 
transform with m = 8. Index j identifies the row, and index I the column. The shaded cells 
refer to the functions contained in the knowledgebase. 
The same technique of introducing binary variables can be applied to other discrete, 
linear transforms such as the discrete cosine transform which for example plays an 
important role in the JPEG image compression scheme. See Exercise B.l. 
2.4 CONCLUSION 
Sets of valuations are called knowledgebases and represent the available knowledge 
at the beginning of an inference process. There are several possibilities for visualizing 
knowledgebases and their internal structure. Most important are valuation networks, 
hypergraphs and their associated primal and dual graphs. The second ingredient of 
an inference task is the query set. According to the number of queries, we distin-
guish between single-query inference problems and multi-query inference problems. 
Inference problems state the main computational interest in valuation algebras. They 
require combining all available information from the knowledgebase and projecting 
the result afterwards on the elements of the query set. Depending on the underlying 
valuation algebra inference problems adopt very different meanings. Examples of 
applications that attribute to the solution of inference problems are reasoning tasks 
in Bayesian and belief networks, query answering in relational databases, constraint 
satisfaction and discrete Fourier or Hadamard transforms. 

EXERCISES 
45 
PROBLEM SETS AND EXERCISES 
B.l * The discrete cosine transform of a signal / is defined by 
лг-1 
T(y) 
= 
2^2f(x)-cos 
x=0 
Exploit the binary representation of x and y and the identity 
οοβ(φ) 
= 
i (βίψ + ε~ίφ 
to transform this formula into two inference problems over the valuation algebra of 
arithmetic potentials. A similar technique has been applied for the discrete Fourier 
transform in Instance 2.6. Compare the knowledgebase factors, their domains and 
the query sets of the two inference problems and also with the inference problem of 
the discrete Fourier transform. 
B.2 * Assume that we want to determine the exact weight of an object. It is clear 
that the result of a weighting process always includes an unknown error term. Using 
n G N different balances, we thus obtain n different observations Xi = m + ω» for 
i = 1,..., n, where m refers to the object weight and ω, to the error term. We further 
assume that the errors of the different devices are mutually independent and normally 
distributed with mean value 0 and variance of. Rewriting the above equation as 
m = Xi — ω;, we obtain a Gaussian potential фг = (xi,-^) 
for each measurement. 
For simplicity, assume m = 2 and determine the combined potential ф = ф\ ® ф? 
by the combination rule in Instance 1.6. Then, by choosing σ\ = σ-ι (balances of 
the same type), show that the object weight m corresponds to the arithmetic mean of 
the observed values. These calculations mirror the solution of a (trivial) projection 
problem where the domain of the objective function ф consists of only one variable. 
A more comprehensive example will be given in Section 10.1. 
B.3 * We proved in Exercise A.2 that possibility potentials form a valuation alge-
bra. In fact, this corresponds to the valuation algebra of probability potentials with 
addition replaced by maximization. Apply this algebra to the Bayesian network of 
Instance 2.1 and show that the projection problem with empty domain identifies the 
value of the most probable configuration in the objective function. 
B.4 * 
Figure 2.12 shows a representation of a stochastic process caifed hidden 
Markov chain. The random variables Xk refer to the state of the process at time 
к — 1,..., n and are subject to a known transition probability p{Xk+i \Xk)· Further, 
we assume that a prior probability p(Xo) for the first variable Xo is given. However, 
the states of the hidden Markov chain cannot be observed from the outside. Instead, a 
7Г - у 
N 
(2.10) 

46 
INFERENCE PROBLEMS 
second random process with variables Yk generates an observable state at each point 
in time that only depends on the real but hidden state Xk with a known probability 
p(Yk\Xk)· For a fixed к > 0, a hidden Markov chain is a particular Bayesian network. 
There are three computational problems related to hidden Markov chains: 
• Filtering: Given an observed sequence of states y\,... 
yk, determine the prob-
ability of the hidden state Xk by computing p(Xk \Yi = У\, ■ ■ ■, Yk = У к ) ■ 
• Prediction: Given an observed sequence of states yi,-. .yk and n > 1, de-
termine the probability of the hidden state at some point к + n in the future 
by computing p(Xk+n\Y\ = Vi, ■ ■ ■ ,Yk — Ук)· This corresponds to repeated 
filtering without new observations. 
• Smoothing: Given an observed sequence of states у\,...ук 
and 1 < n < к, 
determine the probability of the hidden state at some point к — n in the past by 
computing p(Xk-n\Yi 
=yi,---,Yk 
= Ук)-
Show that the computational tasks of filtering, prediction and smoothing in hidden 
Markov chains can be expressed by projection problems with different query sets 
in the valuation algebra of probability potentials. How does the meanings of these 
projection problems change when the valuation algebra of possibility potentials from 
Exercise A.2 is used? Hidden Markov chains and other stochastic processes will be 
reconsidered in Chapter 10. 
Xk-1 ) 
И 
Xk 
И Xk+1 
И Χι»2 
f 
Yk-1 
* 
Yk 
Yk+1 ) 
( Yk+2 
Figure 2.12 A hidden Markov chain. 

CHAPTER 3 
COMPUTING SINGLE QUERIES 
The foregoing chapter introduced the major computational task when dealing with 
valuation algebras, the inference or projection problem. Also, we gained a first insight 
into the different semantics that inference problems can adopt under specific valuation 
algebra instances. The logical next step consists now in the development of algorithms 
to solve general inference problems. In order to be generic, these algorithms must 
only be based on the valuation algebra operations without any further assumptions 
about the concrete, underlying formalism. But before this step, we should also raise 
the question of whether such algorithms are necessary at all, or, in other words, why 
a straightforward computation of inference problems is inadequate in most cases. To 
this end, remember that inference problems are defined by a knowledgebase whose 
factors combine to the objective function which has to be projected on the actual 
queries of interest. Naturally, this description can directly be understood as a trivial 
procedure for the computation of inference problems. However, the reader may have 
guessed that the complexity of this simple procedure puts us a spoke in our wheel. 
If the knowledgebase consists of n € N valuations, then, as a consequence of the 
labeling axiom, computing the objective function φ = φι Çg> ... ® φη results in a 
valuation of domain s = ά(φι)υ· ■ ·υά(φη).1η many valuation algebra instances the 
size of valuations grows at least exponentially with the size of their domain. And even 
Generic Inference: A Unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc. 
47 

4 8 
COMPUTING SINGLE QUERIES 
if the domains grow only polynomially, this growth may be prohibitive. Let us be 
more concrete and analyse the amount of memory that is used to store a valuation. In 
Definition 3.3 this measure will be called weight function. Since indicator functions, 
arithmetic potentials and all their connatural formalisms (see Chapter 5) can be repre-
sented in tabular form, their weight is bounded by the number of table entries which 
corresponds to the number of configurations of the domain. Clearly, this is exponen-
tial in the number of variables. Even worse is the situation for set potentials and all 
instances of Section 5.7 whose weights behave super-exponentially in the worst case. 
If we further assume that the time complexity of the valuation algebra operations is 
related to the weight of their factors, we may come to the following conclusion: it is 
in most cases intractable to compute the objective function φ explicitly. Second, the 
domain acts as the crucial point for complexity when dealing with valuation algebras. 
Consequently, the solution of inference problems is intractable unless algorithms are 
used which confine the domain size of all intermediate results. This essentially is the 
promise of local computation that organizes the computations in such a way that do-
mains do not grow significantly. Before we start the discussion of local computation 
methods, we would like to point out that not all valuation algebras are subject to the 
above complexity concerns. There are indeed instances that have a pure polynomial 
behaviour, and this also seems to be the reason why these formalisms have rarely 
been considered in the context of local computation. However, we will introduce a 
family of such formalisms in Chapter 6 and also show that there are good reasons to 
apply local computation to polynomial formalisms. 
This first chapter about local computation techniques focuses on the solution of 
single-query inference problems. We start with the simplest and most basic local 
computation scheme called fusion or bucket elimination algorithm. Then, a graph-
ical representation of the fusion process is introduced that brings the fundamental 
ingredient of local computation to light: the important concept of a join tree. On the 
one hand, join trees allow giving more detailed information about the complexity 
gain of local computation compared to the trivial approach discussed above. On the 
other hand, they represent the required structure for the introduction of a second and 
more general local computation scheme for the computation of single queries called 
collect algorithms. Finally, we are going to reconsider the inference problem of the 
discrete Fourier transform from Section 2.3 and show in a case study that applying 
these schemes reveals the complexity of the fast Fourier transform. This shows that 
applying local computation may also be worthwhile for polynomial problems. 
The fusion or bucket elimination algorithm is a simple local computation scheme 
that is based on variable elimination instead of projection. Therefore, we first investi-
gate how the operation of projection in a valuation algebra can (almost) equivalently 
be replaced by the operation of variable elimination. 

VALUATION ALGEBRAS WITH VARIABLE ELIMINATION 
4 9 
3.1 VALUATION ALGEBRAS WITH VARIABLE ELIMINATION 
The axiomatic system of a valuation algebra given in Chapter 1 is based on a universe 
of variables r whose finite subsets form the set of domains D. This allows us to 
replace the operation of projection in the definition of a valuation algebra by another 
primitive operation called variable elimination which is sometimes more convenient 
as for the introduction of the fusion algorithm. Thus, let (Φ, D) be a valuation algebra 
with D = V(r) and Φ being the set of all possible valuations with domains in D. For 
a valuation φ G Φ and a variable X G ά(φ), we define 
φ-χ 
= 
фЩФ)-{х}, 
( З Л ) 
Some important properties of variable elimination, that follow from this definition 
and the valuation algebra axioms, are pooled in the following lemma: 
Lemma 3.1 
1. For ф G Ф and X € ά(φ) we have 
ά(φ-χ) 
= 
ά{φ)-{Χ). 
(3.2) 
2. For φ G Φ and X, Y G ά{φ) we have 
(ф-Х)-У 
= 
{ф-*)-х. 
(3.3) 
3. For φ, ψ G Φ, χ — ά(φ), y = ά(ψ), Y φ χ and Y £ y we have 
{Φ®·ψ)~Υ 
= 
φ®ψ~γ. 
(3.4) 
Proof: 
1. By the labeling axiom 
ά(φ-χ) 
= d ( ^ d W - W ) = ά{φ) - 
{X}. 
2. By Property 1 and the transitivity axiom 
(ф-Х)-У 
= 
(фЩФ)-{х})№Ф)-{х})-{У} 
= 
φΙά(φ)-{Χ,Υ} 
= 
(φΙά(φ)-{Υ}}1(ά(φ)-{Υ})-{Χ} 
= (Φ-ΥΓΧ-
3. Since Y $. x and У G y we have x Ç (x U y) — {Y} С x U у. Hence, we 
obtain by application of the combination axiom 
(φ®ψ)~γ 
= 
(ф^гр)ихиу)-{¥} 
= 
φ®ψ±((χυυ)-{Υ})ηυ 
= 
ф®ф1у-{у} 
= 
φ®ψ~γ. 

50 
COMPUTING SINGLE QUERIES 
According to equation (3.3) variables can be eliminated in any order with the same 
result. We may therefore define the consecutive elimination of a non-empty set of 
variables s = {X\,..., 
Xn} Ç ά(φ) unambiguously by 
φ-s 
= 
(((^-Xi)-*.)...)-x». 
( 3 . 5 ) 
This, on the other hand, permits expressing any projection by the elimination of all 
unrequested variables. For x с ά(φ), we have 
ψΐχ 
= 
φ-(ά(φ)-χ)_ 
( 3 ι 6 ) 
To sum it up, equation (3.1) and (3.6) allow switching between projection and variable 
elimination ad libitum. Moreover, we may, as an alternative to definitions in Chapter 
1, define the valuation algebra framework based on variable elimination instead of 
projection (Kohlas, 2003). Then, the properties in the above lemma replace their 
counterparts based on projection in the new axiomatic system, and projection can 
later be derived using equation (3.6) and the additional domain axiom (Shafer, 1991). 
3.2 
FUSION AND BUCKET ELIMINATION 
The fusion (Shenoy, 1992b) and bucket elimination (Dechter, 1999) algorithms are 
the two simplest local computation schemes for the solution of single-query inference 
problems. They are both based on successive variable elimination and correspond 
essentially to traditional dynamic programming (Bertele & Brioschi, 1972). Applied 
to the valuation algebra of crisp constraints, bucket elimination is also known as 
adaptive consistency (Dechter & Pearl, 1987). We will see in this section that fusion 
and bucket elimination perform exactly the same computations and are therefore 
equivalent. On the other hand, their description is based on two different but strongly 
connected graphical structures that bring the two important perspectives of local 
computation to the front: the structural buildup of inference problems that makes 
local computation possible and the concept that determines its complexity. This 
section introduces both algorithms separately and proves their equivalence such that 
later considerations about complexity apply to both schemes at once. In doing so, we 
provide a slightly more general introduction which is not limited to queries consisting 
of single variables, as it is often the case for variable elimination schemes. 
3.2.1 The Fusion Algorithm 
Let (Φ, D) be a valuation algebra with the operation of projection replaced by variable 
elimination. We further assume an inference problem given by its knowledgebase 
{φι,..., 
фт} С Ф and a single query x Ç ά{φ) = ά{φ\) U ... U а(фт) where ф 
denotes the joint valuation ф — ф\ ® ... <g> фт. The domain ά(φ) must correspond 
to a set of variables {Xi,..., 
Xn} with n — \ά{φ)\. The fusion algorithm (Shenoy, 
1992b; Kohlas, 2003) is based on the important property that eliminating a variable 
X € ά(φ) only affects the knowledgebase factors whose domains contain X. This is 
the statement of the following theorem: 

FUSION AND BUCKET ELIMINATION 
51 
Theorem 3.1 Consider a valuation algebra (Φ, D) and a factorization φ = φ\ 
... ® фт with φι G Φ and 1 < i < т. For X G ά{φ) we have 
0 
Фг\. 
(3.7) 
Proof: This theorem is a simple consequence of Lemma 3.1, Property 3. Since 
combination is associative and commutative, we may always write 
We therefore conclude that eliminating variable X only requires combining the 
factors whose domains contain X. According to the labeling axiom, this creates a 
new factor of domain 
sx = 
U 
dfa) 
(3.8) 
иХеа(фг) 
that generally is much smaller than the total domain ά(φ) of the objective function. The 
same considerations now apply successively to all variables that do not belong to the 
query x С GÎ(<JÎ>). This procedure is called fusion algorithm (Shenoy, 1992b) and can 
formally be defined as follows: let Fusy({</>i,..., фт}) denote the set of valuations 
after fusing {φχ,..., фт} with respect to variable Y G ά(φι) U ... U а{фт), 
Fusy({0 b--.,<M) 
= 
{ψ-γ}ο{φί:Υ^ά(φί)} 
(3.9) 
where 
φ = 
ί:ΧΕά(ψι) 
Using this notation, the result of Theorem 3.1 can equivalently be expressed as: 
Ф~Х 
= 
®Ъ1*х{{Фи---,Фт})- 
(ЗЛО) 
By repeated application and the commutativity of variable elimination we finally ob-
tain the fusion algorithm for the computation of a query x Ç ά(φ). If {Yi,..., Yk } = 
ά(φ) — x are the variables to be eliminated, we have 
φ1* 
= 
0-{tt,..,n> 
= 
(g)FusY k(...(Fus Y l({0 b..., фт}))...). 
(З.П) 

52 
COMPUTING SINGLE QUERIES 
This is a generic algorithm for solving the single-query inference problem over 
an arbitrary valuation algebra as introduced in Chapter 1. Algorithm 3.1 provides 
a summary of the complete fusion process which stands out by its simplicity and 
compactness. Also, Example 3.1 expands the complete computation of fusion for the 
inference problem of Instance 2.1. 
Algorithm 3.1 The Fusion Algorithm 
input: 
{φι,.-.,φτη}, 
xÇd(<p) 
output: 
(φ\ ® ... ® фт)1х 
begin 
Ф := 
{ф!,...,фт}; 
for each Y e ά(φ) — x do 
Г := {фг 6 * : Y G ά(φ{)}; 
Ψ 
■■= <8>Γ; 
φ := (Φ 
-Γ)υ{ψ-γ}; 
end; 
return ® Φ ; 
end 
Example 3.1 To exemplify the fusion algorithm, we reconsider the inference prob-
lem of the Bayesian network from Instance 2.1. The knowledgebase consists of eight 
valuations with domains: ά(φι) — {Ä},d(<f)2) = {Α,Τ}, а(фз) = {L,S},d^4) 
= 
{B,S},d(4>5) = {E,L,T},d(4>6) 
= {Ε,Χ},ά(φ7) 
= {Β,Ό,Ε} 
and ά(φ8) = 
{S}. We choose {А, В, D} as query and eliminate all variables from {E, L, 5, T, X} 
in the elimination sequence (X, S, L, T, E): 
Elimination of variable X: 
φ-W 
= фвХ ® ®{Ф1,Ф2,Фз,Ф4,Ф5,Ф7,Ф&} 
Elimination of Variable S: 
φ-{χ·δ} 
= фёХ ®(фз®Ф4®ф8)~3 
® 
(^{Φΐ,Φ2,Φ5,Φΐ} 
Elimination of Variable L: 
<P-{X,S,L} = 0-Х®((03®^4®^8)-5®^5)-^ ® ®{φι,φ2,Φτ} 
Elimination of Variable T: 
φ-{ΧΜ,τ} 
= ф-х ъ^фз^ф^ф^-э^ф^-г-^ф^-т 
0 
(^){ф1,ф7} 
Elimination of Variable E: 
^-{X,S,L,T,E} 
= 
(ф-Х 
®(((ф3®ф4®ф8)-Э®ф5)-Ь®ф2)-Т®ф7)-Е®ф1 
We next present a graphical representation of the fusion process proposed by 
(Shafer, 1996). For this purpose, we maintain a set of domains that is initialized with 

FUSION AND BUCKET ELIMINATION 
53 
the domains of all knowledgebase factors I = {ά(φι),..., 
а{фт)}. In parallel, we 
build up a labeled graph ( V, E, λ, D) which is assumed to be empty at the beginning, 
i.e. V — 0 and E = 0. When variable Xi is eliminated during the fusion process, we 
remove all domains from the set / that contain variable Xi and compute their union 
Si 
= 
U 
s. 
(3.12) 
sel-.XiEs 
We then add the new domain s^ — {Xi} to the set / which altogether updates to 
(I - {s e I : Xi e a}) U {Si - 
{Xi}}. 
This corresponds to the г-th step of the fusion algorithm where variable Xi G ά(φ) — x 
is eliminated. Then, a new node i is constructed with label X(i) = s,. This node is 
tagged with a color and added to the graph. We then go through all other colored 
graph nodes: if the label of such a node v GV contains the variable X,, then its color 
is removed and a new edge {г, v} is added to the graph. This whole process is repeated 
for all variables ά{φ) - x to be eliminated. Finally, one last finalization step has to be 
performed, which corresponds to the combination in equation (3.11): at the end of the 
variable elimination process, the set I will not be empty. It consists either of the query 
variables that are never eliminated, or, if the query is empty, it contains an empty set. 
We therefore add a last, colored node to the graph whose domain corresponds to the 
union of all remaining elements in the domain set. Then, we link all nodes that are still 
tagged with a color to this new node and remove all node colors. Algorithm 3.2 gives a 
summery of the whole construction process and returns a labeled graph (V, E, X, D). 
Since all node labels created during the graph construction process consist of the 
variables from ά(φ), we may always set D = Ρ(ά(φ)). The yet mysterious name of 
this algorithm will be explained subsequently. 
Algorithm 3.2 Join Tree Construction 
input: 
{d(<M,...,ciW>m)}, x Q α{φ) output: 
{V,E,X) 
begin 
V := 0; E := 0; 
I := 
{ά(φι),...,а(фт)}; 
foreach Xi £ ά(φ) — x do 
/ / S t a r t graph c o n s t r u c t i o n 
p r o c e s s . 
Si := U ( s € i :Xi 6 s } ; 
I := 
(l-{sel:Xies})u{si-{Xi}}; 
г := new v e r t e x ; X(i) := SJ; color(i) 
:= 
true; 
foreach j< e V do 
if Xi £ \(j) 
and color(j) — true 
do 
E := 
EL>{{i,j}}; 
color(j) 
:= 
false; 
end; 
end; 
V 
:= 
VU{i}; 
end; 

5 4 
COMPUTING SINGLE QUERIES 
i := new vertex; λ(χ) := |Ji; 
// Finalization step. 
for each j g V do 
if color (j) = true 
do 
E := 
EU{{i,j}}; 
color(j) := false; 
end; 
end; 
У := VU {г}; 
return ( ν , Β , λ ) ; 
end 
Example 3.2 We give an example of the graphical representation of the fusion algo-
rithm based on the factor domains of the Bayesian network example from Instance 2.1. 
At the beginning, we have I = 
{{A},{A,T},{L,S},{B,S},{E,L,T},{E,X}, 
{B, D, E}, {S}}. The query of this inference problem is {A, B, D} and the vari-
ables {E,L, S,T,X} 
have to be eliminated. We choose the elimination sequence 
(X, S, L, T, E) as in Example 3.1 and proceed according to the above description. 
Colored nodes are represented by a dashed border. 
• Elimination of variable X: 
I : {A}, {A, T}, {L, S}, {B, S}, {E, L, T}, {E, X}, {B, D, E}, {S} 
; 
1 '. 
• Elimination of variable S: 
I : {A}, {A,T}, {L, S}, {B, S}, {E, L, T}, {E}, {B, D, E}, {S} 
2 
> 
1 
{B,L,S} ,· 
\ {E,X} 
• Elimination of variable L: 
I : {A}, {A, T}, {B, L}, {E, L, T}, {E}, {B, D, E] 
1 
{E,X> . 
{B,E,L,T> ,· 
• Elimination of variable T: 
I : 
{A},{A,T},{B,T,E},{E},{B,D,E} 

FUSION AND BUCKET ELIMINATION 
55 
V {E.X> /' 
V {Α,Β.Ε,Τ} ,' 
Elimination of variable E: 
1:{A},{A,B,E},{E},{B,D,E} 
{A,B,D,E} / 
• End of variable elimination: after the elimination of variable E, only the last 
added node {A, B, E, D} is colored. This, however, is due to the particular 
structure of our knowledgebase, and it is quite possible that multiple colored 
nodes still exist after the elimination of the last variable. We next enter the 
finalization step and add a last node whose label corresponds to the union of 
the remaining elements in the domain list, i.e. {A} and {A, B, D}. This node 
is then connected to the colored node and all colors are removed. The result of 
the construction process is shown in Figure 3.1. 
3.2.2 Join Trees 
Let us investigate the graphical structure that results from the fusion process in 
more detail. We first remark that each node can be given the number of the eliminated 
variable. For example, in Figure 3.1 the node labeled with {B, E, L, T} has number 3 
because it was introduced during the elimination of the third variable that corresponds 
to L in the elimination sequence. This holds for all nodes except the one added in the 
finalization step. We therefore say that variable Xi has been eliminated in node г for 
1 < г < \ά(φ) — x\. If we further assign number \ά{φ) — x\ + 1 to the finalization 
node, we see that if г < j implies that node j has been introduced after node i. 

56 
COMPUTING SINGLE QUERIES 
Figure 3.1 Finalization of the graphical fusion process. 
Lemma 3.2 At the end of the fusion algorithm the graph G is a tree. 
Proof: During the graph construction process, only colored nodes are connected, 
which always causes one of them to lose its color. It is therefore impossible to create 
cycles. But it may well be that different (unconnected) colored nodes exist after the 
variable elimination process. Each of them is part of an independent tree, and each 
tree contains exactly one colored node. In the finalization step, we add a further node 
and connect it with all the remaining colored nodes. The total graph G must therefore 
be a tree to which all nodes are connected. 
■ 
This tree further satisfies the running intersection property which is sometimes 
also called Markov property or simply join tree property. Accordingly, such trees are 
named join trees, junction trees or Markov trees. 
Definition 3.1 A labeled tree (V, E, A, D) satisfies the running intersection property 
if for two nodes i,j G V and X G X(i) Π \(j), X € \(k) for all nodes к on the path 
between г and j . 
Example 3.3 Figure 3.2 reconsiders the tree G2 from Example 2.1 equipped with 
a labeling function A for r = {A, B,C, D}. The labels are: A(l) = 
{A,C,D}, 
A(2) = {A,D}, A(3) = {£>}, A(4) = {C} and A(5) = {A,B}. This labeled tree is 
not a join tree since Ce λ(1) Π A(4) but С ^ A(3). However, adding variable С to 
node label A(3) will result in a join tree. 
Figure 3.2 
This labeled tree is not a join tree since variable С is missing in node 3. 

FUSION AND BUCKET ELIMINATION 
5 7 
Lemma 3.3 At the end of the fusion algorithm the graph G is a join tree. 
Proof: We already know that G is a tree. Select two nodes s' and s" and X 6 s'Cis". 
We distinguish two cases: If X does not belong to the query, then X is eliminated 
in some node. We go down from s' along to later nodes, until we arrive at the node 
where X is eliminated. Let k! be the number of this node. Similarly, we go down from 
s" to later nodes until we arrive at the elimination node of X. Assume the number of 
this node to be k". But X is eliminated in exactly one node. So к' = к" and the path 
from s' to s" goes from s' to k' = k" and from there to s". X belongs to all nodes of 
the paths from s' to k' = k" and s" to к' = к", hence to all nodes of the path from s' 
to s". The second possibility is that X belongs to the query. Then, X has never been 
eliminated but belongs to the label of the node added in the finalization step. Thus, 
the two paths meet in this node and the same argument applies. 
■ 
Example 3.4 In Figure 3.1, variable E is eliminated in node 5 but appears already 
in node 3, therefore it must also belong to the label of node 4. On the other hand, 
variable В belongs to the query and is therefore part of the node label introduced in 
the finalization step. This is node number 6. But variable В also appears in node 2 
and therefore does so in the nodes 3 to 5. 
3.2.3 The Bucket Elimination Algorithm 
We reconsider the same setting for the introduction of bucket elimination: let (Ф, D) 
be a valuation algebra with the operation of projection replaced by variable elim-
ination. The knowledgebase of the inference problem is {φι,... 
,фт} С Ф and 
х С ά(φ) = {Χι,..., 
Xn} denotes the single query. Similarly to the fusion algo-
rithm, the bucket elimination scheme (Dechter, 1999) is based on the important ob-
servation of Theorem 3.1 that eliminating a variable only affects the valuations whose 
domain contain this variable. We again choose an elimination sequence (ΥΊ,..., Yk) 
for the variables in ά(φ) — x and imagine a bucket for each of these variables. These 
buckets are ordered with respect to the variable elimination sequence. We then par-
tition the valuations in the knowledgebase as follows: all valuations whose domain 
contains the first variable Y\ are placed in the first bucket, which we subsequently 
denote by bucket\. This process is repeated for all other buckets, such that each 
valuation is placed in the first bucket that mentions one of its domain variables. It 
is not possible that a valuation is placed in more than one bucket, but there may 
be valuations whose domain is a subset of the query and therefore do not fit in any 
bucket. Bucket elimination then proceeds as follows: at step г = l,...,k— 1, bucket^ 
is processed by combining all valuations in this bucket, eliminating variable Y, from 
the result and placing the obtained valuation in the first of the remaining buckets 
that mentions one of its domain variables. At then end, we combine all valuations 
contained in bucket к with the remaining valuations of the knowledgebase for which 
no bucket was found in the initialization process. 

5 8 
COMPUTING SINGLE QUERIES 
In the first step of the bucket elimination process, bucketi computes 
( Θ <ч 
\i:Yi€d(<t>i) 
j 
and moves the result into the next bucket that contains one of its domain variables. 
This corresponds exactly to equation (3.9) whereas 
fc 
\фг : Yx <£ а{фг)} 
= {J bucketi 
и^:а(ф{)Сху 
In other words, the first step of the bucket elimination process corresponds to the first 
step of the fusion algorithm, and the same holds also naturally for the remaining steps. 
Bucket elimination therefore computes equation (3.11) that implies its correctness 
and the equivalence to the fusion algorithm. 
Theorem 3.2 Fusion and bucket elimination with identical elimination sequences 
perform exactly the same computations. 
Example 3.5 We repeat the computations of Example 3.1 and 3.2 from the perspec-
tive of bucket elimination. The knowledgebase consists of eight valuations with do-
mains: d{<h) = {Α},ά(φ2) = {Α,Τ},ά(φ3) 
= {L,S},d(04) = {ß,S},d(<fe) = 
{Е,Ь,Т},а{ф6) 
= {Ε,Χ},ά{φ7) 
= {B,D,E} 
and ά(φ8) = {S}. The query is 
{A, B, D}. We choose the same elimination sequence (X, S, L, T, E), construct a 
bucket for each variable in the elimination sequence and partition the valuations 
accordingly. Since ά{φ\) Ç x, φλ is not contained in any bucket. 
Initialization: 
bucket x : φ§ 
buckets ■ Фз, ФА, ФВ 
bucket^ : ф5 
bucketx '■ φι 
buckets '■ φι 
Elimination of bucket X: 
buckets : Фз, Ф4, Ф& 
bucket^ : ф$ 
bucketr '■ 02 
bucket E '■ ΦΊ, Фё 

FUSION AND BUCKET ELIMINATION 
5 9 
Elimination of bucket S: 
bucketL : 05, (03 ig> 04 ® 08)~'S 
bucketx ■ 02 
buckets '■ ΦΊ, ФёХ 
Elimination of bucket L: 
bucketT : Ф2, (05 <8> (0з ® ФА ® 08)~S)~L 
bucketE : 07, Φ^ 
Elimination of bucket T: 
buckets : 07, φ^χ, (ф2 ® (Фь ® (Фз ® 04 <8> 
Ф»)~3)~Ь)'Т 
Elimination of bucket E: 
(Фг ® ФбХ ® (02 ® (</>5 ® (03 О 04 ® 08)" S)" L)" T)"' B 
Combining this with valuation ф\ confirms the result of Example 3.2. 
Similar to the fusion algorithm, we give a graphical representation of the bucket 
elimination process. We first create a graph node for each bucket and label it with the 
corresponding variable. If during the elimination of bucketχ a valuation is added to 
buckety, then an edge {X, Y} is added to the graph. Since the elimination of each 
bucket generates exactly one new valuation that is added to the bucket of a variable 
which follows later in the elimination sequence, the resulting graph will always be a 
tree called bucket-tree. The bucket-tree of Example 3.5 is shown in Figure 3.3. 
Figure 3.3 The bucket-tree of Example 3.5. 
Based on the bucket-tree, we can derive the join tree that underlies the bucket 
elimination process. We revisit each bucket at the time of its elimination and compute 
the union domain of all valuations contained in this bucket. This domain is assigned 
as label to the corresponding node in the bucket tree. Finally, we add a new node with 
the query of the inference problem as label to the modified bucket-tree and connect 
this node with the node that belongs to the last variable in the elimination sequence. 
It follows from Theorem 3.2 and Lemma 3.3 that the resulting graph is a join tree, 
i.e. we obtain the same join tree as for the fusion algorithm. 

60 
COMPUTING SINGLE QUERIES 
Example 3.6 At the time of elimination, the buckets of Example 3.5 contain valua-
tions over the following variables: 
bucketx : {E,X} 
buckets '■ {B, L, S} 
bucketL : {B, E, L, T} 
bucketT : {A, B, E, T} 
bucketE : {A, B, D, E) 
We next assign these domains as labels to the corresponding nodes in the bucket-tree. 
Finally, we create a new node with the query domain {A, B, D} and connect it to the 
node that refers to the last variable in the elimination sequence. The resulting graph 
is shown in Figure 3.4. If we further replace the variables from the bucket-tree by 
their position in the elimination sequence and assign number к + 1 to the node with 
the query domain, we obtain the join tree of Figure 3.1. 
Figure 3.4 The join tree obtained from the bucket-tree of Figure 3.3. 
Our presentation of the bucket elimination algorithm differs in two points from 
the usual way that it is introduced. First, we consider arbitrary query sets with 
possibly more than one variable. Consequently, there may be valuations that cannot 
be assigned to any bucket as shown in Example 3.5, and this makes it necessary to 
add the additional query node when deriving the join tree from the bucket-tree. The 
second difference concerns the elimination sequence. In the literature, buckets are 
normally processed in the inverse order to the elimination sequence. This however 
is only a question of terminology. Before we turn towards a complexity analysis of 
the fusion or bucket elimination algorithm, we reconsider the computational task 
of Example 1.2 used to illustrate the potential benefit of local computation in the 
introduction of this book. 
Example 3.7 The formalism used in Example 1.2 in the introduction consists of 
mappings from configurations to real numbers. Combination refers to point-wise 

FUSION AND BUCKET ELIMINATION 
61 
multiplication and projection to summing up the values of the eliminated variables. 
In other words, this formalism corresponds to the valuation algebra of arithmetic 
potentials. The knowledgebase is given by the set of function {/i,... ,/юо} апа" 
the query is {Xioi, -^102}· We recall that an explicit computation of the objective 
function is impossible. Instead, we apply the fusion or bucket elimination algorithm 
to this inference problem with the elimination sequence (Xi,..., 
X100)· This leads 
to the join tree shown in Figure 3.5, and the executed computations correspond to 
equation (1.3). Thus, the domains of all intermediate factors that occur during the 
computations contain at most three variables. 
Figure 3.5 The join tree for Example 1.2. 
3.2.4 
First Complexity Considerations 
Making a statement about the complexity of local computation for arbitrary valu-
ation algebras is impossible since we do not have information about the form of 
valuations. Remember, the valuation algebra framework describes valuations only 
by their operational behaviour. We therefore restrict ourselves first to a particular 
class of valuations and show later how the corresponding complexity considerations 
can be generalized. More concretely, we focus on valuations that are representable 
in tabular form such as indicator functions from Instance 1.1 or arithmetic poten-
tials from Instance 1.3. In fact, we shall study in Chapter 5 the family of semiring 
valuation algebras which all share this common structure of mapping configurations 
to specific values. Thus, given the universe of variables with finite frames, we de-
note by d G N the size of the largest variable frame. Then, the space (subsequently 
called weight) of a valuation ф with domain ά(φ) = s is bounded by ö(d^ ) and sim-
ilar statements can be made about the time complexity of combination and projection. 
When during the fusion process variable Xi € ά(φ)—χ is eliminated, all valuations 
are combined whose domain contains this variable according to equation (3.9). Then, 

62 
COMPUTING SINGLE QUERIES 
variable Xi is eliminated. Thus, if rrii valuations contain variable Xi before iteration 
г of the fusion algorithm, its elimination requires гщ — 1 combinations and one 
variable elimination (projection). The domain Sj of the valuation resulting from the 
combination sequence is given in equation (3.12). We may therefore say that during 
the elimination of variable Xi, the time complexity of each operation is bounded by 
0{SSi\) 
which gives us the following total time complexity for the fusion or bucket 
elimination process: 
\ ί€ά(φ)-χ 
/ 
In the graphical representation of the fusion algorithm, the elimination of variable 
Xi introduces a new join tree node with label A(i) = Sj. The size of these labels 
are bounded by the largest node label ω = max^v |λ(ί)|. A related measure called 
treewidth will be introduced in Definition 3.2 below. Further, we observe that the fu-
sion process reinserts a valuation into its current knowledgebase after the elimination 
of each variable. This valuation was called ψ in equation (3.9). We may therefore say 
that Σ rrii < m + \ά(φ) — x\ < m + \ά(φ)\ where m denotes the size of the original 
knowledgebase of the inference problem and x the query. Putting things together, we 
obtain the following simplified expression for the time complexity: 
θ({πι+\ά{φ)\)-<ΐΑ. 
(3.13) 
Slightly better is the space complexity. During the elimination of variable Xi 
a sequence of combinations is computed that creates an intermediate result with 
domain Sj, from which the variable Xi is eliminated. In case of valuations with a 
tabular representation, the intermediate result does not need to be computed explicitly. 
This will be explained in more detail in Section 5.6. Here, we content ourselves with 
the rather intuitive idea that each value (or table entry) of this intermediate valuation 
can be computed separately and directly projected. This omits the computation of the 
complete intermediate valuation and directly gives the result after eliminating variable 
Xi from the combination. The domain size of this valuation is si — {Xi} < ω — 1, 
and its size is therefore bounded by ο(άω~1). 
Further, we have just seen that the 
number of eliminations is at most \d(<j>)\ which gives us a total space complexity of 
θ(\ά(φ)\-άω-Λ. 
(3.14) 
It is important to remark that both complexities depend on the shape of the join 
tree, respectively on its largest node label called treewidth. This measure varies under 
different elimination sequences as illustrated in the following example. 
Example 3.8 Figure 3.6 shows two different join trees for the domain list of Example 
3.2 obtained by varying the elimination sequence. In order to get comparable results, 
we take the empty set as query and eliminate all variables using Algorithm 3.2. The 
left-hand join tree is obtained from the elimination sequence (T, S, E, L, B, A, X, D) 

FUSION AND BUCKET ELIMINATION 
6 3 
Figure 3.6 
Different elimination sequences produce different join trees and therefore 
influence the size of the largest node label. 
and has ω = 6. The right-hand join tree has ω = 3 due to the clever choice of the 
elimination sequence (A, T, S, L, B, E, X, D). After a certain number of variable 
elimination steps only a single valuation remains in both examples from which one 
variable after the other is eliminated. This uninteresting part is omitted in the join 
trees of Figure 3.6. A third join tree with ω = 4 can be obtained by eliminating the 
remaining variables in Example 3.2. This corresponds to the elimination sequences 
that start with (X, S, L, T, E) 
The complexity of solving an inference problem with the fusion algorithm there-
fore depends on the variable elimination sequence that produces a join tree whose 
largest node label should be as small as possible. This follows from equation (3.13) 
and (3.14). In order to understand how elimination sequences and the treewidth mea-
sure are related, we consider the primal graph of the inference problem introduced in 
Section 2.1. Since a primal graph contains a node for every variable, an elimination 
sequence can be represented by a particular node numbering. Here, we number the 
nodes with respect to the inverse elimination sequence as shown on the left-hand side 
of Figure 3.7. Such graphs are called ordered graphs, and we define the width of a 
node in an ordered graph as the number of its earlier neighbors. Likewise, we define 
the width of an ordered graph as the maximum node width. Next, we construct the 
induced graph of the ordered primal graph by the following procedure: process in the 
reversed order of the node numbering (i.e. according to the elimination sequence) and 
add edges to ensure for each node that its earlier neighbors are directly connected. We 
define the induced width of an elimination sequence (or ordered graph) as the width 
of its induced graph and refer to the induced width of a graph ω* as the minimum 
width over all possible elimination sequences (orderings) (Dechter & Pearl, 1987). 
Since a primal graph is associated with each inference problem, we may also speak 
about the induced width of an inference problem. 

6 4 
COMPUTING SINGLE QUERIES 
Elimination 
Sequence I 
Numbering Î 
Figure 3.7 A primal graph and its induced graph. 
Example 3.9 The left-hand part of Figure 3.7 shows the primal graph of Example 
3.2 ordered with respect to the elimination sequence (T, S, E, L, B, A, X, D). Node 
E has width 4 since four neighbors posses a smaller node number. Then, the right-
hand part of Figure 3.7 shows its induced graph. Here, node E has 5 neighbors 
{A,B,D,L,X} 
with a smaller number which also determines the width of this 
elimination sequence. Observe that this corresponds to the largest node label of the 
join tree on the left-hand side of Figure 3.6, if we add the variable E that has been 
eliminated in this node. 
The following theorem has been proved by (Arnborg, 1985): 
Theorem 3.3 The largest node label in a join tree equals the induced width of the 
elimination sequence plus one. 
This relationship allows us subsequently to focus only on join trees when dealing 
with complexity issues. It is then common to use a second but equivalent terminol-
ogy called treewidth, which has been introduced by (Robertson & Seymour, 1983; 
Robertson & Seymour, 1986): 
Definition 3.2 The treewidth of a join tree (V, E, A, D) is given by 
max|A(i)| - 1, 
(3.15) 
and we refer to the minimum treewidth over all join trees created from all possible 
variable elimination sequences as the treewidth of the inference problem. 

FUSION AND BUCKET ELIMINATION 
65 
Since treewidth and induced width are equal according to the above theorem, we 
reuse the notation ω* for the treewidth of an inference problem. It is very important 
to distinguish properly between the two notions treewidth of a join tree and treewidth 
of an inference problem. The first refers to a concrete join tree whereas the second 
captures the best join tree over all possible elimination sequences of a given inference 
problem. The decrement in equation (3.15) ensures that inference problems whose 
primal graphs are trees have treewidth one. Besides induced width and treewidth, 
there are other equivalent characterization in the literature. Perhaps most common 
among them is the notion of a partial k-tree (van Leeuwen, 1990). 
Example 3.10 The value ufrom Example 3.8 refers to the incremented treewidth of 
the join tree. We therefore have in this example three join trees of treewidth 2, 3 and 
5 and conclude that the treewidth of the medical inference problem with empty query 
is at most 2. in fact, it is equal to 2 because one of the knowledgebase factors has a 
domain of three variables, which makes it impossible to get a lower treewidth. 
Returning to the task of identifying the complexity of the fusion and bucket 
elimination algorithm, we obtain a complexity bound for the solution of a single-
query inference problem by including the treewidth ω* of the latter into equation 
(3.13) and (3.14): 
θ((πι + \ά{φ)\)·άω"+Λ 
and ОШ(ф)\ ■ άω*J. 
(3.16) 
This is called a parameterized complexity (Downey & Fellows, 1999) with the 
treewidth of the inference problem as parameter. Naturally, the question arises how 
the minimum treewidth over all possible variable elimination sequences can be found, 
the more so as it determines the complexity of applying the fusion and bucket elimi-
nation algorithm. Concerning this important question we only have bad news. It has 
been shown by (Arnborg et al, 1987) that finding the smallest treewidth of a graph is 
NP-complete. In fact, the task of deciding if the primal graph of the inference prob-
lem has a treewidth below a certain constant к can be performed in 0(\ά(φ)\ ■ д{к)) 
where д(к) is a very bad exponential function in к (Bodlaender, 1998; Bodlaender, 
1993). Fortunately, there are good heuristics for this task that deliver join trees with 
reasonable treewidths as we will see in Section 3.7. It is thus important to distinguish 
the two ways of specifying the complexity of local computation schemes. Given a 
concrete join tree or an elimination sequence with treewidth ω, we insert this value 
into equation (3.16) and obtain the achieved complexity of a concrete run of the 
fusion algorithm. On the other hand, using the treewidth ω* of the inference problem 
in these formulae denotes the best complexity over all possible join trees or variable 
elimination sequences that could be achieved for the solution of this inference prob-
lem by the fusion or bucket elimination algorithm. But since we are generally not 
able to exactly determine this value, this often remains an unsatisfied wish. However, 
it is nevertheless common to specify the complexity of local computation schemes 
by the treewidth of the inference problem and we will also go along with this. 

6 6 
COMPUTING SINGLE QUERIES 
3.2.5 
Some Generalizing Complexity Comments 
At the beginning of our complexity considerations we limited ourselves to a family 
of valuation algebras whose elements are representable in tabular form. These for-
malisms share the property that the size of valuations grows exponentially with the 
size of their domains. Thus, the space of a valuation φ with domain s = ά(φ) is 
bounded by 0(rf's') where d denotes the size of the largest variable frame. Similar 
statements were made about the time complexity of combination and projection. This 
view is too restrictive, since there are many other valuation algebra instances that do 
not keep with this estimation. On the other hand, there are also examples where it is 
unreasonable to measure time and space complexity by the same function as it has 
been done in equation (3.16). Examples of such formalisms will be given in Chapter 
6. However, a general assumption we are allowed to make is that the space of a val-
uation and the execution time for the operations shrink under projection. Remember, 
the idea of local computation is to confine the domain size of intermediate factors 
during the computations. If this assumption could not be made, local computation 
would hardly be a reasonable approach. 
Definition 3.3 Let (Φ, D) be a valuation algebra. A function ω : Φ —> N0 is called 
weight function if for all φ £ Φ and x Ç ά(φ) we have ω(φ) > ω(φ^χ). 
Weight functions are still too general to be used for complexity considerations. In 
many cases however, we can give a predictor for the weight function which is only 
based on the valuation's domain. Such weight predictable valuation algebras share 
the property that the weight of a valuation can be estimated from its domain only. 
Definition 3.4 A weight function ω ofa valuation algebra (Φ, D) is called weight 
predictor if there exists a function f : D —> No such that for all φ € Φ 
ω(φ)€θ(/(ά{φ))). 
We give some examples of weight functions and weight predictors for the valuation 
algebras introduced in Chapter 1 : 
Example 3.11 
• A weight predictor for the valuation algebra of arithmetic potentials that also 
applies to indicator functions and all formalisms of Section 5.3 has already 
been used in equation (3.16): 
ω(φ) = 
Π |Ωχ| G 0 ( > l ) 
(3.17) 
X£s 
where s = ά(φ) denotes the domain of valuation φ and 
max ΙΩχΙ. 
е<1(Ф) 

FUSION AND BUCKET ELIMINATION 
6 7 
the largest variable frame. Observe that in this particular case, we could also 
use the weight function directly as weight predictor since it depends only on 
the valuation's domain. 
• A reasonable and often used weight function for relations is ω{φ) = \ά(φ)\ ■ 
сага(ф), where сага(ф) stands for the number of tuples in ф. Regrettably, this 
is not a weight predictor itself since the number of tuples cannot be deduced 
from the relation's domain. 
• Finally, a weight predictor for the valuation algebra of set potentials and all 
formalisms of Section 5.7 is 
ω(φ) e θ ( 2 2 Ν ) . 
(3.18) 
Observe that we presuppose binary variables in this formula. 
Subsequently, we will use weight predictors for the analysis of the time and space 
complexity of local computation. This allows us to make more general complexity 
statements that can then be specialized to a specific formalism by choosing an ap-
propriate weight predictor. Further, we use two different weight predictors / and д 
for time and space complexity. This accounts for the observation that time and space 
complexity cannot necessarily be estimated by the same function, but both satisfy the 
fundamental property that they are non-increasing under projection. We obtain for 
the general time complexity of the fusion and bucket elimination algorithm: 
θ((τη+\ά(φ)\)-/(ω* + 1)\ 
(3.19) 
This follows directly from equation (3.16). Remember that the assumption of valu-
ations with a tabular representation allowed us to derive a lower space complexity 
in equation (3.16). This optimization cannot be performed for arbitrary valuation 
algebras. For the elimination of variable A» we therefore need to compute the combi-
nations first, which results in an intermediate factor with domain size |X(i) \ < ω* +1. 
Then, the variable X; can be eliminated and the result is added to the knowledgebase. 
Altogether, this gives us the following general space complexity of the fusion and 
bucket elimination algorithm: 
о(д(ш* + 1) + \а(ф)\-д{Ш*)\ 
(3.20) 
Clearly, for the case of applying the fusion algorithm to arithmetic potentials, indicator 
functions and all other formalism of Section 5.3, we may choose the same weight 
predictor of equation (3.17) for both time and space complexity. Moreover, the space 
complexity can then be improved to the one given in equation (3.16). This shows the 
difficulties of making general complexity statements for local computation applied 
to unspecified valuation algebras. 

6 8 
COMPUTING SINGLE QUERIES 
3.2.6 
Limitations of Fusion and Bucket Elimination 
The fusion or bucket elimination algorithm is a generic local computation scheme to 
solve single-query inference problems in a simple and natural manner. Nevertheless, 
it has some drawbacks compared to other local computation procedures that will be 
introduced below. Here, we address two important points of criticism: 
• The fusion or bucket elimination algorithm can be applied to each formalism 
that satisfies the valuation algebra axioms from Section 1.1. But we have also 
seen in Appendix A.2 of Chapter 1 that there exists a more general definition of 
the axiomatic framework which is based on general lattices instead of variable 
systems. Fusion and bucket elimination require variable elimination and can 
therefore not be generalized to valuation algebras on arbitrary lattices. 
• Time and space complexity of the fusion algorithm are bounded by the treewidth 
of the inference problem, which makes the application difficult when join 
trees have large labels. Experiments from constraint programming (Larrosa & 
Morancho, 2003) show that the space requirement is particularly sensitive, for 
which reason inference is often combined with search methods that only require 
linear space. However, search methods need more structure than offered by the 
valuation framework. We refer to (Rossi et al, 2006) for a survey of such 
trading or hybrid methods in constraint programming. Looking at equation 
(3.20), we observe that the function arguments of the two terms distinguish in 
a constant number. We will see later that this is a consequence of the particular 
join trees that emerge from the fusion and bucket elimination process. Further, 
we know that the first term vanishes when dealing with particular valuation 
algebras. It will be shown in Section 5.3 that this also includes constraint sys-
tems. To make local computation more space efficient for such formalisms, we 
therefore aim at reducing the argument of the second term in equation (3.20). 
In the subsequent sections, we present an alternative picture of local computation 
that finally leads to a second algorithm for the solution of single-query inference 
problems that does not depend on variables anymore and which may also provide a 
better space performance. The essential difference with respect to the fusion algorithm 
is that the join tree construction is decoupled from the actual inference process. 
Instead, a suitable join tree for the current inference problem is initially constructed, 
and local computation is then described as a message-passing process where nodes 
act as virtual processors and collaborate by exchanging messages. The key advantage 
of this approach is that we no longer depend on the very particular join trees that are 
obtained from the fusion algorithm. To study the requirements for a join tree to serve 
as local computation base, we first introduce a specialized class of valuation algebras 
that provide neutral information pieces. It will later be shown in Section 3.10 that 
the existence of such neutral elements is not mandatory for the description of local 
computation as message-passing scheme. But if they exist, the discussion simplifies 
considerably. Moreover, neutral elements are also interesting in other contexts that 
will be discussed in later parts of this book. 

VALUATION ALGEBRAS WITH NEUTRAL ELEMENTS 
6 9 
3.3 VALUATION ALGEBRAS WITH NEUTRAL ELEMENTS 
Following the view of a valuation algebra as a generic representation of knowledge 
or information, some instances may exist that contain pieces that express vacuous 
information. In this case, such a neutral element must exist for every set of questions 
s G D and must therefore be contained in every subsemigroup Φ8 of Φ. If we then 
combine those pieces with already existing knowledge of the same domain, we do not 
gain new information. Hence, there is an element es G Φβ for every subsemigroup 
Φ8 such that φ ® es = es <8> φ = φ for all φ G Φ3. We further add a neutrality axiom 
to the valuation algebra system which states that a combination of neutral elements 
leads to a neutral element with respect to the union domain. 
Definition 3.5 A valuation algebra (Φ, D) has neutral elements, if for every domain 
s G D there exists an element es G Φ8 such that φ <S> es = es <g> φ = c/>/or all φ G Φ8. 
These elements must satisfy the following property: 
(A7) Neutrality: For x,y G D, 
ex<g>ey = 
exUy. 
(3.21) 
If neutral elements exist, they are unique within the corresponding subsemigroup. 
In fact, suppose the existence of another element e's G Фя with the identical property 
that ф® e's = e's ® ф = ф for all ф G Ф5. Then, since es and e's behave neutrally 
among each other, es = es <g> e's = e^. Furthermore, valuation algebras with neutral 
elements allow for a simplified version of the combination axiom that was already 
proposed in (Shenoy & Shafer, 1990). 
Lemma 3.4 In a valuation algebra with neutral elements, the combination axiom is 
equivalently expressed as: 
(A5) Combination: For φ, ψ G Φ with ά(φ) = χ, ά(φ) = y, 
(φ®ψ)1χ 
= 
ф®ф1хПу. 
(3.22) 
Proof: Equation (3.22) follows directly from the combination axiom with z = x. 
On the other hand, let x С z Ç x U y. Since z(l(xUy) 
= z we derive from equation 
(3.22) and Axiom (Al) that 
ez (g> (ф <g> ф)и 
(ег®ф® 
ф)и 
εζ®φ® 
ф1уПг 
ф®ф^уПг. 
The last equality holds because 
ά(φ®φ1υηζ) 
= xU{yHz) 
= (xUy)Ci(xUz) 
= z = 
d(ez). 
(φ 0 V)iz 
-

70 
COMPUTING SINGLE QUERIES 
Note that this result presupposes the distributivity of the lattice D. 
m 
The following lemma shows that neutral elements also behave neutral with respect 
to valuations on larger domains: 
Lemma 3.5 For φ ε Φ with ά(φ) = x and у С x it holds that 
ф®еу 
= 
ф. 
(3.23) 
Proof: From the associativity of combination and the neutrality axiom follows 
ф <8> ey = (ф <g> ex) ® ey — ф ® (ex <8> ey) = ф®ех 
= ф. 
The presence of neutral elements allows furthermore to extend valuations to larger 
domains. This operation is called vacuous extension and may be seen as a dual 
operation to projection. For ф е Ф and ά{φ) Cj/we define 
фЪ 
= 
ф®еу. 
(3.24) 
Before we consider some concrete examples of valuation algebras with and without 
neutral elements, we introduce another property called stability that is closely related 
to neutral elements. In fact, stability can either be seen as the algebraic property 
that enables undoing the operation of vacuous extension, or as a strengthening of the 
domain axiom (A6). 
3.3.1 
Stable Valuation Algebras 
The neutrality axiom (A7) determines the behaviour of neutral elements under the 
operation of combination. It states that a combination of neutral elements will always 
result in a neutral element again. However, a similar property can also be requested 
for the operation of projection. In a stable valuation algebra, neutral elements always 
project to neutral elements again (Shafer, 1991). This property seems natural but 
there are indeed important examples which do not fulfill stability. 
Definition 3.6 A valuation algebra with neutral elements (Ф, D) is called stable if 
the following property is satisfied for all x,y G D with x С у, 
(A8) Stability: 
ey
x 
= 
ex. 
(3.25) 
Stability is a very strong condition which also implies other valuation algebra 
properties: together with the combination axiom, it can, for example, be seen as a 
strengthening of the domain axiom. Also, it is possible to revoke vacuous extension 
under stability. These are the statements of the following lemma: 

VALUATION ALGEBRAS WITH NEUTRAL ELEMENTS 
71 
Lemma 3.6 
1. The property of stability with the combination axiom implies the domain axiom. 
2. If stability holds, we have for φ G Φ with ά(φ) = i Ç j , 
{φ^γχ 
= φ. 
(3.26) 
Proof: 
1. For φ G Φ with ά(φ) = x we have 
φ1χ = {ф®ех)1х 
= ф <8> ex
x
x = ф® ех = ф. 
2. For ф G Ф with ά(φ) = x Ç y we derive from the combination axiom 
(фЪ>)1* = {ф®еу)1х 
= φ®ή,χ 
= ф®ех 
= ф. 
m 
■ 3.1 Neutral Elements and Indicator Functions 
In the valuation algebras of indicator functions, the neutral element for the 
domain s G Dis given by es(x) = 1 for all x e i ] s . We then have for ф G Фа 
0®e s(x) = </>(x) · es(x) = </>(x) ■ 1 = 
φ(χ). 
Also, the neutrality axiom holds, since for s,t G D with x G Ω8υί we have 
e t®e s(x) = e t(x i t)-e s(x i s) = 1-1 = 1 = esUi(x). 
Finally, the valuation algebra of indicator functions is also stable. For s,t G D 
with s Çt and x G ils it holds that 
ef*(x) = 
max et(x,y) = 1 = es(x). 
y€f2 t- s 
■ 3.2 Neutral Elements and Arithmetic Potentials 
Arithmetic potentials and indicator functions share the same definition of com-
bination. We may therefore conclude that arithmetic potentials also provide 
neutral elements. However, in contrast to indicator functions, arithmetic poten-
tials are not stable. We have for s,t G D with s e t and x G fls 
e\s(x) = ^ 
е*(х>У) = ΙΩ*-«Ι > L 
yeü t_ s 

7 2 
COMPUTING SINGLE QUERIES 
■ 3.3 Neutral Elements and Set Potentials 
In the valuation algebra of set potentials, the neutral element es for the domain 
s £ D is given by es(A) = 0 for all proper subsets А с fts and es(Qs) — 1. 
Indeed, it holds for ф £ Ф with ά(φ) = s that 
ф®е8{А) = 
Σ 
Ф(В)-е3(С) = ф(А) · β(Ωβ) = <А(Л). 
BtxiC=A 
The second equality follows since for all other values of С we have es (C) = 0. 
Also, the neutrality axiom holds which will be proved explicitly in Section 
5.8.1. Further, set potentials are stable. For s,t £ D and s С t it holds that 
ACnt:ns(A)=ns 
The second equality holds because et(nt) = 1 is the only non-zero summand. 
■ 3.4 Neutral Elements and Relations 
For s £ D the neutral element in the valuation algebra of relations is given 
by es = Ω8. Similar to indicator functions these neutral elements are also 
stable. However, there is an important issue regarding neutral elements in the 
relational algebra. Since variable frames are often very large or even infinite 
(i.e. all possible strings), the neutral elements can sometimes not be expressed 
explicitly. 
■ 3.5 Neutral Elements and Density Functions 
Equation (1.15) introduced the combination of density functions as simple 
multiplication. Therefore, the definition es(x) = 1 for x £ R'sl would clearly 
meet the requirements for a neutral element. However, this function es is not a 
density since its integral will not be finite 
/
CO 
1 dx. = 
со. 
-oo 
Hence, densities form a valuation algebra without neutral elements. This holds 
in particular also for Gaussian potentials in Instance 1.6. 
Before we continue with the interpretation of the fusion algorithm as a message-
passing scheme in Section 3.5, we introduce a second family of elements that may 
be contained in a valuation algebra. As neutral elements express neutral knowledge, 
null elements express contradictory knowledge with respect to their domain. 

VALUATION ALGEBRAS WITH NULL ELEMENTS 
73 
3.4 VALUATION ALGEBRAS WITH NULL ELEMENTS 
Some valuation algebras contain elements that express incompatible, inconsistent 
or contradictory knowledge according to questions s G D. Such valuations behave 
absorbingly for combination and are therefore called absorbing elements or null 
elements. Hence, in a valuation algebra with null elements there is an element zs e Фя 
such that ζ8®φ = φ®ζ3 = zs for all φ € Φ s. We call a valuation ф е Φ8 consistent, 
if and only if φ φ zs. It is furthermore a very natural claim that a projection of 
some consistent valuation produces again a consistent valuation. This requirement is 
captured by the following additional axiom. 
(A9) Nullity: For x, у € D, x С у and ф € Фу, 
Ф1х 
= 
ζχ 
if, and only if, φ = zy. 
In other words, if some valuation projects to a null element, then it must itself 
be a null element. Thus, contradictions can only be derived from contradictions. We 
further observe that according to the above definition, null elements absorb only 
valuations of the same domain. In case of stable valuation algebras, however, this 
property also holds for any other valuation. 
Lemma 3.7 In a stable valuation algebra with null elements we have for all φ 6 Φ 
with ά{φ) = x and y £ D 
ф®гу 
= 
zxUy. 
(3.27) 
Proof: We remark first that from equation (3.26) it follows 
Hence, we conclude from the nullity axiom that z^xUy = zxUy and derive 
ф®гу 
= 
(ф ® exUy) ® (zy ® exUy) 
= фииу 
® z\x^y 
= 
Ф 
® ZX\Jy = 
ZX\Jy. 
и 
Let us search for null elements in the valuation algebra instances of Chapter 1. 
■ 3.6 Null Elements and Indicator Functions 
In the valuation algebras of indicator functions the null element for the domain 
s G D is given by zs(x) = 0 for all X E O S . We then have 
φ®ζ3(χ) 
= φ(χ) ■ zs(x) 
— ф{х)-0 
= 
zs(x). 
Since projection refers to maximization, it is easy to see that null elements, and 
only null elements project to null elements. 

74 
COMPUTING SINGLE QUERIES 
■ 3.7 Null Elements and Arithmetic Potentials 
Again, since their combination rules are equal, arithmetic potentials and in-
dicator functions share the same definition of null elements: for s G D we 
define zs(x) = 0 for all x G Ω8. The absorption property for combination 
follows directly from Instance 3.6. Further, projection is defined as summation 
for arithmetic potentials and since the values are non-negative, it is again only 
the null element that projects to a null element. 
■ 3.8 Null Elements and Relations 
Since we have already identified the null elements in the valuation algebra of 
indicator functions, it is simple to give way to relations. Due to equation (1.9) 
null elements are simply empty relations with respect to their domain. 
■ 3.9 Null Elements and Set Potentials 
In the valuation algebra of set potentials, the null element zs for the domain 
s С r is defined by zs(A) = 0 for all A Ç Qs. These elements behave 
absorbingly for combination, and are the only elements that project to null 
elements, which follows again from their non-negative values. Although this is 
rather easy to see, we will give a formal proof in Section 5.8.2. 
■ 3.10 Null Elements and Density Functions 
In the valuation algebra of density functions, the null element for the domain 
s is given by zs(x) = 0 for all x G Ω8. The properties of null elements follow 
again from the fact that density functions are non-negative. 
■ 3.11 Null Elements and Gaussian Densities 
In the introduction of density functions we also mentioned that the family of 
Gaussian densities forms a subalgebra of the valuation algebra of density func-
tions. Although we are going to study this formalism extensively in Chapter 
10, we already point out that Gaussian densities do not possess null elements 
due to the requirement for a positive definite concentration matrix. 
Null elements play an important role in the semantics of inference problems. 
Imagine that we solve an inference problem with the empty set as single query. After 
the execution of local computation, we find a null element as answer to this query: 
(</>i <8> · · · <8><PnJ 
= 
4-
We therefore conclude from Axiom (A9) that the objective function φ must itself be 
a null element, or, in other words, that the knowledgebase is inconsistent. This is an 
important issue in constraint programming since it allows to check the satisfiability of 

LOCAL COMPUTATION AS MESSAGE-PASSING SCHEME 
75 
a constraint system by local computation. A similar argumentation has, for example, 
been used in Instance 2.4. 
This closes our intermezzo of special elements in a valuation algebra. We now 
return to the solution of single-query inference problems by local computation tech-
niques and give an alternative picture of the fusion algorithm in terms of a message-
passing scheme. This presupposes a valuation algebra with neutral elements, although 
it will later be shown in Section 3.10 that this assumption can be avoided. 
3.5 
LOCAL COMPUTATION AS MESSAGE-PASSING SCHEME 
Let us reconsider the join tree resulting from the graphical representation of the fusion 
algorithm. We then observe that for all knowledgebase factor domains ά{φ{), there 
exists a node v G V in the join tree which covers d(4>i), i.e. d(4>i) С \(v). This is 
a simple consequence of equation (3.12), which defines the label of the new nodes 
added during the elimination of a variable. More precisely, if Xj e ά(φΐ) is the 
first variable in the elimination sequence, then ά{φί) Ç \(j). This is a consequence 
of the particular node numbering defined in the fusion process. We may therefore 
assign factor φi to the join tree node j G V. This process is repeated for all factors 
in the knowledgebase of the inference problem, which is illustrated in Example 3.12. 
Remember also that the query x С ά(φ) of the inference problem always corresponds 
to the label of the root node since the latter contains all variables that have not been 
eliminated. A join tree that allows such a factor distribution will later be called 
a covering join tree in Definition 3.8. The process of distributing knowledgebase 
factors over join tree nodes may assign multiple factors to one node, whereas other 
nodes go away without an assigned valuation. In Example 3.12, only the root node 
does not contain a knowledgebase factor, but if we had eliminated more variables, 
then inner nodes also would exist that do not posses a knowledgebase factor. If we 
further assume a valuation algebra with neutral elements, we may assign the neutral 
element βχ^ to all nodes г € V which do not hold a knowledgebase factor. On the 
other hand, if a node contains multiple knowledgebase factors, they are combined. 
The result of this process is a join tree where every node г € V contains exactly 
one valuation ф, with ά{φι) С А(г). This valuation either corresponds to a single 
knowledgebase factor, to a combination of multiple knowledgebase factors or to a 
neutral element, as shown in Example 3.12. 
Example 3.12 The knowledgebase factor domains in the medical inference problem 
of Instance 2.1 are: ά(φλ) = {A}, ά{φ2) = {Α,Τ}, ά(φ3) = {L,S}, ά(φΑ) = 
{В,S}, ά(φ5) = {E,L,T}, 
ά(φ6) = {Ε,Χ}, 
ά(φ7) = {Β,Ό,Ε} 
and ά(φ8) = 
{S}. The single query to compute is x = {A, B, D}. These factors are distributed 
over the nodes of the covering join tree in Figure 3.8 that results from the graphical 
fusion process (see Figure 3.1). The node factors of this join tree become: φ\ = φρ, 
ф2 = Фъ®Ф4<& Ф&, Фз = 05. ФА — Φΐ ® Ф2, Фъ - Ф7, Фб = Ё{л,в,о}· 

7 6 
COMPUTING SINGLE QUERIES 
Figure 3.8 A covering join tree for the medical example of Instance 1.3. 
A common characteristic of all local computation algorithms is their interpretation 
as message-passing schemes (Shenoy & Shafer, 1990). In this paradigm, nodes act 
as virtual processors that communicate by the exchange of messages. Nodes process 
incoming messages, compute new messages and send them to neighboring nodes of 
the join tree. Based on the new factorization constructed above, we now introduce 
the fusion algorithm from the viewpoint of message-passing: at the beginning, node 
1 contains the valuation ψχ. According to the fusion process, it eliminates the first 
variable X\ of the elimination sequence from its node content and sends the result to 
node 5, which then combines the message to its own node content. Node 5 computes 
■Фь ® V»rXl 
and replaces its node content with this result. More generally, if node i contains at 
step i of the fusion algorithm the valuation щ (which consists of its initial factor 
ipi combined with all messages sent to node г in the foregoing г — 1 steps), then 
it computes v~Xi and sends the result to its unique neighbor with a higher node 
number. In Definition 3.11 this node is called child node and denoted by ch(i). The 
child node then computes 
Vch(i) ® 
V~Xi 
and sets its node content to the new value. This process is repeated for i = 1,..., | V | — 
1. Then, at the end of the message-passing, the root node | V| contains the result of 
the inference problem 
φ^ 
= 
(0i®---®<Am)ix. 
(3.28) 
On the one hand, we may draw this conclusion directly since the message-passing 
procedure is just another view of the fusion algorithm. On the other hand, we deliver 
an explicit proof for the correctness of message-passing in a more general context in 
Theorem 3.6 and 3.7. 

LOCAL COMPUTATION AS MESSAGE-PASSING SCHEME 
77 
In the message-passing concept, nodes act as virtual processors with a private 
valuation storage that execute combinations and projections independently and com-
municate via the exchange of messages along the edges of the tree. Since all local 
computation methods we subsequently introduce adopt this interpretation, we may 
classify them as (virtually) distributed algorithms. 
Example 3.13 Figure 3.9 illustrates the message-passing view of the fusion algo-
rithm. Ifßi-+j denotes the message sent from node г to node j at step г of the fusion 
algorithm, we have: 
• μι-+5 = 
Фё 
• μ2->3 = 
(<!>3®<t>4®<t>8yS 
• μ3_>4 = (05®M2-»3)"L 
• M4->5 = (01 ®02 <8>μ3-+4)~Τ 
• μ5->6 = (07®μι->5®μ4-+5)~'Ε 
Thus, we obtain at the end of the message-passing in node 6: 
e{A,B,D] 
® (ФбХ 
® Фг ® (01 <8> 02 ® (05 ® (03 ® 04 ® 
0 8 r
S r
L r
T r
B 
which, by Lemma 3.5, corresponds to Example 3.1. 
Figure 3.9 
The fusion algorithm as message-passing scheme where arrows indicate the 
direction along which messages are sent. 
3.5.1 The Complexity of Fusion as Message-Passing Scheme 
The message-passing implementation of the fusion algorithm adopts the same time 
complexity but requires more space than actually necessary. When a variable Xi is 
eliminated in the fusion algorithm, all valuations that contain this variable in their 
domain are combined which leads to a result of domain s,. In the message-passing 

7 8 
COMPUTING SINGLE QUERIES 
scheme, this valuation is stored in node i, whereas in the fusion algorithm, variable 
Xi is first eliminated before the result is stored. Consequently, the message-passing 
implementation of the fusion algorithm has a space complexity of: 
θ(\ά{φ)\-9{ω* + 1)\ 
(3.29) 
However, it is important to understand that this is just a consequence of its imple-
mentation as message-passing scheme and does not modify the statement about the 
general complexity of the fusion algorithm given in equation (3.20). 
The reason why the space complexity of the fusion algorithm becomes worse when 
implemented as message-passing scheme is the very particular join tree that has been 
used. Remember, so far we obtained the join tree from a graphical representation of 
the fusion algorithm, and this procedure only creates very particular join trees. The 
fusion algorithm eliminates exactly one variable in each step and therefore, exactly 
one variable vanishes from the knowledgebase between node г and i + 1. This is 
mirrored in the general space complexity of the fusion algorithm given in equation 
(3.20) where the two terms differ not surprisingly in one exponent. If we aim at 
an improvement of the space complexity, we must therefore get rid of this limiting 
property. In the following section, we develop a generalization of the message-passing 
conception by decoupling it from the actual join tree construction process. At the 
same time, we replace variable elimination again by projection such that the resulting 
algorithm can be applied to valuation algebras on arbitrary lattices. 
3.6 COVERING JOIN TREES 
The foregoing discussion identified the join tree as the major ingredient of local 
computation since it reflects the tree decomposition structure of an inference problem 
that bounds the domain of intermediate factors during the computations. If we aim 
at the decoupling of local computation from the join tree construction process, we 
have to ensure that the join tree taken for the computation is suitable for the current 
inference problem. This requirement is captured by the concept of a covering join 
tree that will be introduced below. Here, we directly give a general definition with 
regard to the processing of multiple queries in Chapter 4. In Figure 3.8 we assigned 
a knowledgebase factor to a join tree node if the domain of the factor was a subset 
of the node label. If such a node can be found for each knowledgebase factor, one 
speaks about a covering join tree for the factorization. 
Definition 3.7 Let T = (V, E, λ, D) be a join tree. 
• T is said to cover the domains s\,..., 
sm G D if there is for every Sj a node 
j e V with Si С X(j). 
• T is called covering join tree for the factorization ф\ <g> · · · ® фт if it covers 
the domains Si = ά(φί) £ D for 1 < г < т. 

COVERING JOIN TREES 
7 9 
We furthermore say that a join tree covers an inference problem, if it covers all 
knowledgebase factors and queries. Its formal definition demands in particular that 
no free variables exist in the join tree (condition 3 below) . This means that each 
variable in the node labels must also occur somewhere in the knowledgebase. 
Definition 3.8 A join tree T is called covering join tree for the inference problem 
(01 ® ■ · ■ ® 
фт)Ы 
with Xi G {x\,..., 
xs}, if the following conditions are satisfied: 
1. T is a covering join tree for the factorization ф\ <g> ■ ■ ■ ® фт. 
2. T covers the queries {x\,..., 
xs}. 
3. D corresponds to the power set Τ{ά{φ\ <S> ■ ■ ■ <8> фт))-
A covering join tree corresponds to the concept of Ά tree-decomposition (Robertson 
& Seymour, 1984) that is widely used in the literature - also in many other contexts 
that are not directly related to valuation algebras. 
Example 3.14 In Section 3.5 we have already exploited the fact that the join tree of 
Figure 3.1 is a covering join tree for the medical inference problem of Instance 1.3. 
Another covering join tree for the same inference problem is shown in Figure 3.10. 
(л) 
Figure 3.10 A further covering join tree for the medical example of Instance 2.1. 
Covering join trees provide the required data structure for the introduction of 
more general and economical local computation schemes. We now follow the same 
procedure as in Section 3.5 and assign the knowledgebase factors to covering nodes. 
This defines an assignment mapping according to the following definition. 
Definition 3.9 Let (V, E, λ, D) be a covering join tree for the factorization 
φ 
= 
φι ® ... <g> фт. 
A function 
a : {1,.. .,m} ->· V 
is called an assignment mapping for ф regarding V if for every г € { 1 , . . . , m} we 
have ά(φί) С Х(а(г)). 

8 0 
COMPUTING SINGLE QUERIES 
Example 3.15 The assignment mapping of Figure 3.8 is a(l) = 4, a(2) = 4, 
o(3) = 2, o(4) = 2, a(5) = 3, a(6) = 1, a(7) = 5, a(8) = 2. Observe also that 
there are in general several possible assignment mappings. 
It was already pointed out that some nodes may go away empty-handed from this 
factor distribution process. Also, it may be that the domain of a factor assigned to a 
certain node does not fill its node label completely. We therefore initialize every join 
tree node with the neutral element of its label. 
Definition 3.10 Let a be an assignment mapping for a factorization φ regarding the 
nodes V of a covering join tree (V, E, A, D). The factor assigned to node г £ V is 
Фг = 
ex{i) ® (g) φ,. 
(3.30) 
j:a(j)=i 
This assures that for every node factor rpi we have ά{φΐ) = \(i). However, it will 
later be shown in Section 3.10 that we may in fact dispose of this artificial filling. 
Example 3.16 The join tree of Figure 3.11 is a covering join tree for the factorization 
of Instance 2.1 (but not for the inference problem since the query is not covered). 
The corresponding factor domains are given in Example 3.12. A possible factor 
assignment is: a(l) = a(2) = 1, a(3) = o(4) = o(8) = 3, a(5) = 2, a(6) = 7 and 
a(7) = 4. No factor is assigned to the nodes 5 and 6. This assignment creates the 
node factors: 
• Vl = e{A,T) ®Φ\®Φΐ 
= Φ\®Φΐ 
• φ2 = e{E,L,T] ® Фь = Фь 
Фъ = ΦΖ®ΦΑ® 
Ф& 
• 
• 
• 
• 
• 
4>з 
ψ4 
Фъ 
Фб 
ΦΊ 
— 
= 
= 
= 
= 
e{B,L,S] 
e{B,D,E) 
C{B,E,L] 
HEM 
e{E,X} & 
® φ3 (8) 04 ® 
®ф7 
)φ6 
= 
— ΦΊ 
Φβ 
The factors φί are usually called join tree factors and correspond either to (the 
vacuous extension of) a single factor of the original knowledgebase, a combination 
of multiple factors, or to a neutral element, if the factor set of the combination in 
equation (3.30) is empty. The following lemma shows that the join tree factors provide 
an alternative factorization to the knowledgebase of the inference problem: 
Lemma 3.8 It holds that 
φ = φ1®·-·®φ„ί 
= ( g ) ^ . 
(3.31) 
iev 

COVERING JOIN TREES 
81 
Figure 3.11 An covering join tree for the factorization of Instance 2.1. 
Proof: The assignment mapping assigns every knowledgebase factor to exactly one 
join tree node. It therefore follows from the commutativity of combination and the 
neutrality axiom that 
0 ^ i 
= 
^ 1 ® . . . ® ^ ® ( ^ ) е а д 
iev 
iev 
= 
ф\ <g>... <g> фт ® еа(ф) = ф <8> еа(ф) = ф. 
The second equality follows from the third requirement of Definition 3.8. 
■ 
When a covering join tree is obtained from the graphical fusion process, its 
node numbering is determined by the variable elimination sequence, i.e. a node gets 
number г G N if is was created during the elimination of the г-th variable in the 
fusion process. This particular numbering shaped up as very useful to identify the 
sending and receiving node during the message-passing process. However, if local 
computation is defined on arbitrary covering join trees, such a node numbering has 
to be introduced artificially. To do so, we first fix a node which covers the query as 
root node and assign the number |V| to it. Then, by directing all edges towards the 
root node m, it is possible to determine a numbering in such a way that if j is a node 
on the path from node г to m, then j > i. Formally, let (V, E, λ, D) be a join tree. 
We determine a permutation π of the elements in V such that 
• n(k) = | V|, if к is the root node; 
• πϋ) 
> π(*) f°r every node j eV lying on the path between г and | V|. 
The result is a renumbered join tree (V, Ε', λ, D) with edges E' = {{π(ΐ), n(j)) : 
(i,j) G E} that is usually called a directed join tree towards the root node |V|. Note 
also that such a numbering is not unique. It is furthermore convenient to introduce 
the notions of parents, child, and leaves with respect to this node numbering: 
Definition 3.11 Let (V, E, X, D) be a directed join tree towards root node \V\. 
• The parents pa(i) of a node г are defined by the set 
pa(i) 
= 
{j :j <i and (i,j) G E}. 

8 2 
COMPUTING SINGLE QUERIES 
• Nodes without parents are called leaves. 
• Thechi\dch(i)ofanodei 
< \V\ is the unique node j with j > iand(i,j) 
G E. 
In this numbering parents always have a smaller number than their child. 
Definition 3.12 Let (V, E, λ, D) be a directed join tree towards root node \V\. 
• The separator sep(i) of a node i < \V\ is defined by sep(i) — λ(ί) П X(ch(i)). 
• The eliminator elim(i) of anode г < \V\ is defined by elim(i) = X(i) — sep(i). 
Finally, the definition of a tree implies that whenever one of its edges (г, ch(i)) is 
removed, the tree splits into two separated trees where the one that contains the node г 
is called subtree rooted to node i, abbreviated by %. Clearly, the running intersection 
property remains satisfied if join trees are cut in two. 
Example 3.17 Consider the join tree of Figure 3.15 which corresponds to the join 
tree of Figure 3.11 directed towards node {B,D,E} 
and renumbered according 
to the above scheme. The parents of node 5 are pa(5) — {2,4} and its child is 
ch(5) = 6. Further, we have sep(5) = {E, L} and elim(5) = {B}. Also, if we for 
example cut the edge between the nodes 5 and 6, we obtain two trees that both satisfy 
the running intersection property. 
3.7 JOIN TREE CONSTRUCTION 
Before we actually describe how local computation can be performed on arbitrary 
covering join trees, we should first say a few words about how to build covering join 
trees for an inference problem. In fact, this topic is a broad research field by itself, 
which makes it impossible at this point to give all the important references. However, 
from our complexity studies of the fusion algorithm, we know the principal quality 
criterion of ajoin tree. The treewidth of the join tree determines the complexity of local 
computation and therefore, we focus on finding covering join trees whose treewidth 
is as small as possible. We further know that the treewidth of the inference problem 
provides a lower bound. Finding such an optimum join tree is NP-hard. Thus, if we 
want to achieve reasonable local computation complexity, we are forced to fall back on 
heuristics. There are a many different heuristics to find a suitable variable elimination 
sequence as for example (Rose, 1970; Bertele & Brioschi, 1972; Yannakakis, 1981; 
Kong, 1986; Almond & Kong, 1991; Haenni & Lehmann, 1999; Allen & Darwiche, 
2002; Hopkins & Darwiche, 2002). An overview of recommended heuristics can be 
found in (Lehmann, 2001; Dechter, 2003) and a comparison is drawn by (Cano & 
Moral, 1995). These methods deliver a variable elimination sequence from which 
the covering join tree is constructed using some possibly improved version of the 
graphical fusion process of Section 3.2.1. The justification of this approach is the fact 
that no better join tree can be found if we do without variable elimination (Arnborg, 

JOIN TREE CONSTRUCTION 
8 3 
1985; Dechter & Pearl, 1989). However, building a join tree from a previously 
fixed elimination sequence requires a lot of knowledgebase rearrangement operations 
that in turn can lead to severe performance problems. A particularly efficient way 
has been proposed by (Lehmann, 2001) and uses a data-structure called variable-
valuation-linked-list. It directly returns a covering join tree including an assignment 
mapping that assigns at most one knowledgebase factor to each join tree node. This 
is often a very advantageous property since it avoids that combinations are executed 
before the actual local computation algorithm is started. On the other hand, such join 
trees generally contain more nodes and therefore also require to store more factors 
and messages during local computation. To sum it up, covering join trees can be 
found by the graphical fusion process and some variable elimination sequence. As 
a complement, we skim over an alternative but equivalent method to construct join 
trees starting from the primal graph of the knowledgebase. 
3.7.1 Join Tree Construction by Triangulation 
Imagine the primal graph representation of a knowledgebase as introduced in Section 
2.2. Here, the variables of the knowledgebase valuations correspond to the graph 
nodes, and two nodes are linked if the corresponding variables occur in the domain of 
the same valuation. Its associated dual graph has a node for each factor domain, and 
the nodes are connected if they share a common variable. Therefore, each valuation 
is naturally covered by some node of the dual graph. Finding a covering join tree 
therefore consists in removing edges from the dual graph until it is a tree that satisfies 
the running intersection property. The following theorem (Beeri et al, 1983) states 
under which condition this is possible: 
Theorem 3.4 A graph is triangulated if, and only if, its dual graph has a join tree. 
A graph is called triangulated if each of its cycles of four or more nodes has an 
edge joining two nodes that are not adjacent in the cycle. Elsewhere, triangulated 
graphs are also called chorda! graphs where the chord refers to the introduces edges 
during the triangulation process. Figure 3.12 shows a cycle of six nodes together with 
two possible triangulations. 
Figure 3.12 A graph with two possible triangulations. 
Obviously, there are different possibilities to triangulate a graph. Various algo-
rithmic aspects of this task are discussed in (Tarjan & Yannakakis, 1984). Here, we 
give a simple procedure from (Cano & Moral, 1995) that is based on a node élimina-

8 4 
COMPUTING SINGLE QUERIES 
tion sequence (X\,..., 
Xn). Applied to primal graphs, this clearly corresponds to a 
variable elimination sequence. 
• For г = 1,... ,n: 
- Connect all pairs of nodes that are neighbors to Xt. Let Li denote the set 
of added edges. 
- Remove node Xi and all edges connected to this node from the graph. 
• To obtain the triangulation of the original graph, add all edges in (J"=i -^»· 
In fact, this algorithm corresponds exactly to the procedure of constructing an induced 
graph from a primal graph described in Section 3.2.4. We thus conclude that the 
induced graph of a primal graph ordered along a given variable elimination sequence 
is equal to the triangulated graph obtained from the same elimination sequence. This 
supports the above statement that constructing join trees via graph triangulation is 
just another interpretation of the methods introduce before. 
Example 3.18 Consider the knowledgebase of the medical example from Instance 
2.1, whose domains are listed in Example 3.1, and draw up its primal graph: 
If we want to ensure that the join tree covers some specified query, then the query 
must also be added to the primal graph. This mirrors the insertion of a correspond-
ing neutral element into the knowledgebase. We choose the elimination sequence 
(X, S, L, T, E, A, B, D) and execute the above algorithm: 
• Elimination of variable X: 
• Elimination of variable S: 

JOIN TREE CONSTRUCTION 
85 
• Elimination of variable L: 
Elimination of variable T: 
Elimination of variable E: 
At this point, the triangulation process can be stopped because the remaining nodes 
form a clique. Figure 3.13 shows the completed triangulation of the primal graph 
which is in fact equal to the induced graph that we obtain from the same elimination 
sequence by the procedure of Section 3.2.4, see Example 3.8. We further observe that 
the triangulated graph has the maximal cliques {B, L, S}, {B, E, L, T}, {A, E, T}, 
{A, B, E, T}, {A, B, D, E} and {E, X}. 

8 6 
COMPUTING SINGLE QUERIES 
Figure 3.13 
The triangulated primal graph of the medical example from Instance 2.1. 
We now have a triangulated graph from which we can derive the join tree that was 
promised in Theorem 3.4. For this purpose, we identify all maximal cliques in the 
triangulated graph and build a new graph whose nodes are the maximal cliques. Each 
node is connected to all other nodes and each edge is labeled with the intersection of 
the corresponding cliques. Such a graph is called a join graph. Further, we refer to 
the weight of an edge in the join graph as the number of variables in the edge label. 
The following theorem is proved in (Jensen, 1988). 
Theorem 3.5 A spanning tree of a join graph is a join tree if and only if its sum of 
edge weights is maximal. 
An important advantage of this characterization is that standard enumeration 
algorithms for spanning trees (Broder & Mayr, 1997) can be used. Note also that 
several join trees with the same weight may exist, as it can easily be seen in the 
following example. These join trees clearly have the same nodes and thus the same 
treewidth, but they may distinguish in the separator widths. 
Figure 3.14 
The join graph of the triangulation from Example 3.18 and a possible join tree 
derived as a spanning tree with maximal weight. 
Example 3.19 The left-hand side of Figure 3.14 shows the join graph obtained 
from the cliques identified in Example 3.18. There are 6 nodes which implies that a 
spanning tree must have 5 edges. A spanning tree with maximal weight can be found 
by including the three edges with weight 3 and choosing the other two edges with 

THE COLLECT ALGORITHM 
8 7 
maximal weight among the remaining candidates. There are several join trees with 
maximal weight, one being shown on the right-hand side of Figure 3.14. 
We now abandon the widespread field of join tree construction and focus in the 
following sections on the solution of single-query inference problems based on a 
previously built covering join tree. 
3.8 THE COLLECT ALGORITHM 
The collect algorithm is the principal local computation scheme for the solution of 
single-query inference problems. It presupposes a covering join tree for the current 
inference problem that is directed and numbered according to the scheme at the end 
of Section 3.6 and that keeps a factor ipj on every node j which is defined according 
to equation (3.30). We therefore say that ipj represents the initial content of node j . 
The collect algorithm can then be described by three simple rules: 
Rl: Each node sends a message to its child when it has received all messages from 
its parents. This implies that leaves (i.e. nodes without parents) can send their 
messages right away. 
R2: When a node is ready to send, it computes the message by projecting its current 
content to the separator and sends the message to its child. 
R3: When a node receives a message, it updates its current content by combining 
it with the incoming message. 
This procedure is repeated up to the root node. It follows from this first sketch that 
the content of the nodes may change during the algorithm's run. To incorporate this 
dynamic behavior, the following notation is introduced. 
• ψ\ 
= ipj is the initial content of node j . 
(i) 
• xjr- is the content of node j before step г of the collect algorithm. 
The particular way of numbering the nodes of the directed join tree implies that at 
step i, node i can send a message to its child. This allows the following specification 
of the collect algorithm. 
• At step i, node i computes the message 
/^Cft(0 
= 
4l)isep{l). 
(3.32) 
This message is sent to the child node ch(i) with node label X(ch(i)). 
• The receiving node ch(i) combines the message with its node content: 
Uch(i) 
= 
^cfc(i)®^->ch(i)· 
< 3 3 3 ) 

8 8 
COMPUTING SINGLE QUERIES 
The content of all other nodes does not change at step i, i.e. for all j φ ch(i) 
^ + 1 > 
= 
ψ&. 
(3.34) 
The justification of the collect algorithm is formulated by the following theorem. 
Remember that the root node has been chosen in such a way that it covers the query 
of the inference problem. 
Theorem 3.6 At the end of the collect algorithm, the root node r =\V\ contains the 
marginal of φ relative to X(r), 
ф(г) 
= 
^|λ(Γ)_ 
( 3 _ 3 5 ) 
The proof of this theorem can be found in the appendix of this chapter. The 
marginal φ^χ^ 
can now be used to solve the single-query inference problem by one 
last projection to the query x, because the latter is covered by the root node. 
φΐχ 
= Ui4r)yx _ 
(336) 
A summary of the collect algorithm is given in Algorithm 3.3. The input to this 
algorithm is a covering join tree (V, E, X, D) where each node г e V contains a 
factor ψί G Φ. Also, the query x is given as input for the final projection of equation 
(3.36) performed in the statement. 
Algorithm 3.3 The Collect Algorithm 
input: 
(V, E, X, D), 
x С ά(φ) 
output: 
(φι ® · · · ® <j>m)ix 
begin 
for г = 1... |V| - 1 do 
._ ,i,lHi)n\(ch(i)) 
. 
^i-fch(i) 
■- V>i 
i>ch(i) 
■= 4>ch(i) ® ßi-+ch(i)'< 
end; 
return Vr »' 
end 
The medical inference problem of Instance 2.1 is based on variables which allows 
us to express projection by variable elimination. Thus, if we illustrate the collect 
algorithm on the same join tree as the fusion algorithm in Example 3.13, we repeat 
exactly the same computations. We therefore consider a new query for the illustration 
of the collect algorithm in Example 3.20. 
Example 3.20 We reconsider the knowledgebase of the medical inference problem 
with the new query x = {B,E} and use the join tree of Figure 3.11 for the com-
putation. It has already been shown in Example 3.16 that this join tree covers our 
knowledgebase. Since node 4 of Figure 3.11 covers the new query, we directly have a 
covering join tree for the new inference problem. However, the node numbering has 

THE COLLECT ALGORITHM 
8 9 
to be modified such that node 4 becomes the new root node. A possible 
renumbering 
is shown in Figure 3.15 and the new node factors are: φ\ = φχ <8> Ф2, Φ2 = Фь> 
Фз = ФЗ®ФА®ФЪ, 
ФА = Фб, Фб = 4{E,L}: 
Фб = e{B,E,L}· ΦΊ = Фт- The messages 
sent during the collect algorithm are: 
• μι_* = < ( 1 ) η λ ( 2 ) = (*i®fc)«r> 
• μ2^5 = ( ^ 2 Θ μ ι ^ 2 Ρ ( 2 ) η λ ( 5 ) = 
&5®Pl^2)i{E'L] 
. 
Мз^6 = ^ λ ( 3 ) Π λ ( 6 ) = 
(Φ3®Φ4®Φ*)1{Β'ί} 
• μ^5 = ^ ( 4 ) η λ ( 6 ) = ΦΪ{Ε) 
• 
μ 5-> 6 = ( ^ 5 ® М 2 ^ 5 ® М 4 ^ 5 ) и ( 5 ) П Л ( 6 ) = 
(e { J S,L}®AÎ2^5®M4^5) i { E' L } 
• 
μ6^7 
= (^6®Мз^бОМ5^б)' 1- А ( 6 ) П Л ( 7 ) = 
(e{ß,£;,L}®^3^6<S)/X5^6)'1-{ß'£;} 
Ai the end of the message-passing, the content of node 7 is: 
φ7 ® μ6->7 
= 
^ 6 ® M6-+7 
and, according to Theorem 3.6, the query {В, E} is obtained by one last projection: 
Figure 3.15 A complete run of the collect algorithm. 

9 0 
COMPUTING SINGLE QUERIES 
3.8.1 The Complexity of the Collect Algorithm 
In the complexity study of the fusion algorithm, we brought forward the argument 
that all computations take place within the nodes of the join tree whose labels bound 
the domains of all intermediate results. Therefore, the treewidth of the inference 
problem, which reflects the minimum size of the largest node label over all possible 
covering join trees, became the decisive complexity factor. This argumentation was 
based on the interpretation of the fusion algorithm as message-passing scheme on a 
(particular) covering join tree. Exactly the same statement can also be made for the 
time complexity of the collect algorithm. However, the decoupling of the join tree 
construction process from the actual local computation gives us considerably more 
liberty in choosing an appropriate covering join tree for a given inference problem. 
One could therefore think that perhaps other methods exist that lead to better join 
trees than variable elimination can produce. The answer to this question has already 
been given in Section 3.7 since for a join tree of fixed treewidth, there always exists 
a variable elimination sequence producing a join tree of equal treewidth (Arnborg, 
1985; Dechter & Pearl, 1989). The only difference with respect to the time complexity 
of fusion given in equation (3.19) is that the number of join tree nodes is no more 
proportional to the number of variables in the inference problem plus the number 
of knowledgebase factors. This results from the decoupling of join tree construction 
from local computation. Thus, recall that each node combines all incoming messages 
to its content. There are \E\ = | V\ — 1 edges, hence | V\ — 1 combinations to execute. 
Additionally, each new message is obtained by projecting the current node content to 
the separator. This adds another | V\ — 1 projections which altogether gives 2(| V\ — 1) 
operations. We therefore obtain for the time complexity of the collect algorithm 
θ(|νΊ·/(ω* + 1)Υ 
(3.37) 
In the derivation of the time complexity for the fusion algorithm, the number of 
operations was bounded by m + \ά(φ) | with m being the number of knowledgebase 
factors. It is fairly common for complexity considerations to assume a factor assign-
ment mapping of the collect algorithm that assigns at most one knowledgebase factor 
to each join tree node. This ensures according to equation (3.30) that no combinations 
of knowledgebase factors are executed before the collect algorithm is started, since 
this would clearly falsify the complexity analysis of the latter. Therefore, similar 
assumptions will also be made for the complexity analysis of all subsequent local 
computation schemes. We then have m < \V\ and since \V\ « \ά(φ)\ in the fusion 
algorithm, we conclude that both algorithms have the same time complexity. 
Also, the space complexity is still similar to the message-passing implementation 
of the fusion algorithm given in equation (3.29), although the messages are generally 
much smaller. The reason is that each node combines incoming messages to its node 
content. Then, the message can be discarded that makes its smaller size irrelevant. 
Instead, we still keep one factor per node in memory whose size is bounded by the 

ADJOINING AN IDENTITY ELEMENT 
91 
node label. Thus, we end with the following space complexity: 
o(\V\-g{w* + l)\ 
(3.38) 
3.8.2 Limitations of the Collect Algorithm 
In Section 3.2.6 we addressed two important limitations of the fusion algorithm which 
should now be discussed from the viewpoint of collect. Collect is entirely based on 
projection and can therefore be applied to valuations taking domains from an arbi-
trary lattice. However, in such cases we need more general prerequisites since the 
definition of ajoin tree is still based on variables. The concepts generalizing join trees 
are Markov trees where the notion of conditional independence between elements 
of a general lattice replaces the running intersection property (Kohlas & Monney, 
1995). We therefore conclude that the collect algorithm is, from this point of view, 
more generally applicable than the fusion algorithm. 
The attentive reader may have noticed that we expressed the statement about the 
generality of the collect algorithm with great care. In fact, the version of the collect 
algorithm given here can be applied to valuation algebras on general lattices but, 
on the other hand, requires neutral elements for the initialization of join tree nodes. 
This in turn is not required for the fusion algorithm, and it is indeed an extremely 
limiting requirement. We have for example seen in Instance 3.4 that neutral elements 
in the valuation algebra of relations often correspond to infinite tables which can 
hardly be used to initialize join tree nodes. Even worse, the valuation algebra of 
density functions from Instance 3.5 does not provide neutral elements at all. This 
dependency of the collect algorithm on neutral elements was tacitly accepted for 
many years. It can, however, be avoided. A general solution that further increases 
the efficiency of the collect algorithm has been proposed by (Schneuwly et al. ,2004). 
We know from Lemma 3.5 that neutral elements also behave neutral with respect 
to valuations of larger domains. Thus, the neutral element eg behaves neutral with 
respect to all valuations φ G Φ and could therefore be used to initialize join tree nodes. 
Instead of a neutral element per domain, we only need a single neutral element for 
the empty domain. Moreover, we will see in the following section that an element 
with this property can always be adjoined to any valuation algebra, if the latter does 
not provide neutral elements. This element is called identity element and its use 
finally leads to a general collect algorithm that can be applied to every valuation 
algebra without any restriction. A particular implementation of this algorithm will 
also exempt the message-passing conception from its inefficient space complexity. 
3.9 ADJOINING AN IDENTITY ELEMENT 
We are going to show in this section that a single identity element can be adjoined 
to any valuation algebra. This identity element then replaces neutral elements in the 
generalized version of the collect algorithm presented in Section 3.10. Let (Φ, D) 

9 2 
COMPUTING SINGLE QUERIES 
be a valuation algebra according to Definition 1.1. We add a new valuation e to Φ 
and denote the resulting system by (Φ', D). Labeling, combination and projection 
are extended from Φ to Φ' in the following way: 
1. Labeling: Φ' -> D; ф н-> ά'(φ) 
• ά'(φ) = 
ά(φ),ΐΐφΕΦ, 
• d'(e) = 0; 
2. Combination: Φ' χ Φ' -> Φ'; (0, ψ) ^ φ®' ψ 
• φ ®' ψ = φ ® ψ \ϊ φ,ψ € Φ, 
• 0 ®' e = e (g)' 0 = φ if 0 G Φ, 
• e (g)' e = e. 
3. Projection: Φ' x fl 4 Φ'; (0,я) ^ ^ ' ^ forx ç d(0) 
• ^ 
= ф±* if 0 G Φ, 
• e+'0 = e. 
If another element e' with the identical property that e' ® φ = φ® e' = φϊοτ 
all φ G Φ' already exists in Φ', there is no need to perform the extension. This 
is in particular the case if the valuation algebra provides a neutral element for the 
empty domain. We will next see that the proposed extension of (Φ,-D) conserves 
the properties of a valuation algebra. The simple proof of this lemma is given in 
(Schneuwly et al.,2004). 
Lemma 3.9 (Φ', D) with extended operations d', <g>' and ,|/ is a valuation algebra. 
If there is no danger of confusion, we usually identify the operations in (Φ', D) 
with their counterparts in (Φ, D). Doing so, we subsequently write d for d', cg> for <g>' 
and 4 for 4-'· 
3.10 THE GENERALIZED COLLECT ALGORITHM 
Finally, in this section, we derive the most general local computation scheme for the 
solution of single-query inference problems. It essentially corresponds to the collect 
algorithm of Section 3.8 but avoids the use of neutral elements. Instead, we assume 
a valuation algebra (Φ, D) with an identity element e G Φ. The first occurrence of 
neutral elements in the collect algorithm is in equation (3.30) that defines the join 
tree factorization or, in other words, the initial node content for the message-passing. 
This equation can easily be rewritten using the identity element: 
Definition 3.13 Let a be an assignment mapping for a factorization φ regarding the 
nodes V of a covering join tree (V, E, X, D). The node factor i G V assigned by a is 
Ψί = 
e® 
(g) фу 
(3.39) 
j:a(j)=i 

THE GENERALIZED COLLECT ALGORITHM 
9 3 
Also, the important statement of Lemma 3.8 still holds under this new setting: 
($$Ψί= ^ ® - ® i ® 0 e 
= 0i ® ... <g> 0m <g> e = ф®е = φ. 
iev 
iev 
In the subsequent sections, we always consider this modified factorization as join 
tree factorization. Each factor Vi corresponds either to a single factor of the original 
knowledgebase, a combination of some of them, or to the identity element if the factor 
set of the combination in equation (3.39) is empty. However, there is one essential 
difference between the two factorizations of equation (3.30) and (3.39). When neutral 
elements are used, it is always guaranteed that the domain of the factors covers the 
join tree node label entirely, i.e. we have ά{ψΐ) = X(i). This is a consequence of the 
neutral element used in equation (3.30). On the other hand, if the identity element is 
used to initialize join tree nodes, we only have d{xpi) Ç X(i). It is therefore important 
to distinguish between the node label X(i) and the node domain d(ipi): the node 
domain refers to the domain of the factor that is actually kept by the current node 
and this value may grow when incoming messages are combined to the node content. 
The label, on the other hand, represents the largest possible domain of a factor that 
would fit into this node and always remains constant. 
Example 3.21 The factors of Example 3.16 change only marginally under this mod-
ified definition. We now have τ/>5 = Ψβ — e-
The generalized collect algorithm for the solution of single-query inference prob-
lems without neutral elements can now be described by the following rules: 
Rl: Each node sends a message to its child when it has received all messages from 
• its parents. This implies that leaves can send their messages right away. 
R2: When a node is ready to send, it computes the message by projecting its current 
content to the intersection of its domain and its child's node label. 
R3: When a node receives a message, it updates its current content by combining 
it with the incoming message. 
One has to look closely to detect the difference to the collect algorithm given in 
Section 3.8. In fact, only Rule 2 changed in a small but important way. Remember, 
the node domain may now be smaller than the node label due to the use of identity 
elements. If node г would project its node factor to the separator Χ(ΐ) Π X(ch(i)), 
then this could lead to an undefined operation since Х(г) П X(ch(i)) Ç ά{φ{) does 
not necessarily hold anymore. Instead, the factor is projected to ά(ψί) Π X(ch(i)). In 
Section 3.8 we already introduced a notation to incorporate the dynamic behavior of 
the node factors during the message-passing. This notation must now be extended to 
take the growing node domains into consideration: 
• ψ\ ' = ipj is the initial content of node j . 
(i) 
• ip) is m e content of node j before step г of the collect algorithm. 

94 
COMPUTING SINGLE QUERIES 
Uch(i) 
A similar notation is adopted to refer to the domain of a node: 
• or- = bjj = d(ipj) is the initial domain of node j . 
• or- = ά{φ*' ) is the domain of node j before step i of the collect algorithm. 
As before, the particular way of numbering the nodes of the directed join tree implies 
that at step i, node i can send a message to its child. This allows the following 
specification of the generalized collect algorithm. 
• At step i, node i computes the message 
ßi^ch(i) 
- 
Wi 
· 
(3.4U) 
This message is sent to the child node ch(i) with node label X(ch(i)). 
• The receiving node ch(i) combines the message with its node content: 
Its node domain changes to: 
*(lO 
= "& 
u («ί° π К*®)) ■ 
(3.42) 
The content of all other nodes does not change at step i, 
for all j φ ch(i), and the same holds for the node domains: 
ω?+1) 
= 
ωψ. 
(3.44) 
The justification of the collect algorithm is formulated in the following theorem. 
Remember that in case of single-query inference problems, the root node is always 
chosen in such a way that it covers the query. 
Theorem 3.7 At the end of the generalized collect algorithm, the root node r = \V\ 
contains the marginal of φ relative to X(r), 
ф(г) 
= 
фЩг)_ 
(3_45) 
The proof of this theorem is given in the appendix of this chapter. We should also 
direct our attention to the implicit statement of this theorem that the node domain 
of the root node d(ipr ) always corresponds to its node label A(r) at the end of the 
collect algorithm. This is again a consequence of the third requirement in Definition 
3.8 as shown in the proof of the collect algorithm. We then obtain the solution of the 
single-query inference problem by one last projection: 
I 
/ 
, \ i ч \ 
Iх 
r )J 
. 
(3.46) 

THE GENERALIZED COLLECT ALGORITHM 
95 
Algorithm 3.4 The Generalized Collect Algorithm 
input: 
(V,E,X,D), 
x Ç ά(φ) output: 
{φλ ® ... ® фт)1х 
begin 
for г = l...|V| - 1 do 
._ ,,,«*,)n»WO). 
i>ch(i) 
:= i>ch(i) ®Mi->cft(i); 
end; 
lx 
return ψ; ; 
end 
Example 3.22 Example 3.21 gives a modified join tree factorization based on the 
use of identity elements instead of neutral elements. Since this concerns only the 
two factors 4>$ and ψβ, only the messages μ5_>6 and μβ->7 change with respect to 
Example 3.20. 
5: μ5^6 = (ψ6®μ2->δ®μ4-45)Χω**)ηΗβ) 
= {e® μ^® 
μΑ^)^Ε>^ 
6: /i 6^ 7 = (ψ6®μ3^6®μ5^)1ω°6)ηλ{7) 
= (е®йчб®Д5-.б) | < В' Е } 
The collect algorithm based on identity elements instead of neutral elements is the 
most general and fundamental local computation scheme for the solution of single-
query inference problems. Moreover, this algorithm will play an important part in the 
following chapter in which local computation procedures for multi-query inference 
problems are studied. It turns out that many algorithmic insights concerning the 
computation of multiple queries can be derived from the correctness of the collect 
algorithm. We therefore list some further properties for later use, starting with a 
partial result of the generalized collect theorem for each subtree: 
Lemma 3.10 At the end of the generalized collect algorithm, node г contains 
( 
\
^ 
4l) 
= l(8>lM 
(3-47) 
The proof is given in the appendix of this chapter. Comparing this result with 
Theorem 3.7 indicates that only the label of the root node is guaranteed to be filled 
after the generalized collect algorithm, because it is not sure that all the variables in 
X(i) are covered by some factors in the current subtree. However, as the following 
lemma states, the node labels can be scaled down to coincide with the node domain 
and the resulting tree will again be a join tree. 
Lemma 3.11 At the end of the collect algorithm executed on a join tree T = 
(V, E, A, D), the labeled tree T* = (V, E, X*,D) with 
У (г) = ω « = 
ω" 
for г — 1,..., r is still a covering join tree for the projection problem. 

9 6 
COMPUTING SINGLE QUERIES 
Proof: It will be shown that the running intersection property is still satisfied 
between the nodes of the labeled tree T*. Let г and j be two nodes whose reduced 
labels contain X, i.e. X G X*{i) Π X*(j). There exists a node к with X G X(k) and 
M < к, since X G А(г) Π A(j). By the same token, X e A(c/i(i)). Then, from 
X*(ch(i)) 
= 
ω« ( Ου(λ*(ί)ηλ(<Λ(0)) 
follows that X € A*(c/i(i)) and by induction along the path from i to k, X G A*(fc). 
The same argument applies to the nodes on the path from j to к and therefore the 
running intersection property holds for T*. The fact that T* covers the original 
(r) 
knowledgebase factors follows directly from и; С ω| '. 
■ 
The last property highlights the relationship between node domains and labels: 
Lemma 3.12 It holds that 
Ч ( Г ) П ^ , ) 
= 
4 Γ ) Π λ ( # ) ) . 
(3.48) 
Proof: The left-hand part of this equation is clearly contained in the right-hand part, 
because ω^λ^ Ç X(ch(i)). Remembering that ω]1' = u>\r' for г = 1,..., r and that 
(i) 
ω\ are non-decreasing in j = 1,2... the reversed inclusion is derived as follows: 
(г) r~. (r) 
-> 
(r) r-, 
(i+l) 
= 
^ п ( ^ ( г ) и ( Ч
( г ) П А ( с М г ) ) ) ) 
- 
(^n^{i))u^nX(ch(i))) 
= 
w|r)nA(c/i(i)). 
The last equality follows from ω^λ^ Ç ω ^ | - С А(с/г(г)). 
■ 
3.10.1 
Discussion of the Generalized Collect Algorithm 
The generalized collect algorithm is the most general and fundamental local compu-
tation scheme for the solution of single-query inference problems. It depends neither 
on a particular lattice such as the fusion algorithm, nor on the existence of neutral 
elements. Consequently, this algorithm can be applied to any valuation algebra and 
it is therefore also qualified to serve as a starting point for the development of further 
local computation procedures in the following chapters. Besides its generality, there 
is a second argument for the use of the generalized collect algorithm. Using neu-
tral elements to initialized join tree nodes blows the node content up until its domain 
agrees with the node label. All operations performed during the message-passing then 
take place on this maximum domain. On the other hand, using the identity element 
keeps the node domains as small as possible with the node label as upper bound. 
This makes the generalized collect algorithm more efficient, although its general 

THE GENERALIZED COLLECT ALGORITHM 
9 7 
complexity is not affected by this optimization. However, the real benefit of using 
the generalized collect algorithm becomes apparent when dealing with multi-query 
inference problems. We therefore refer to Section 4.1.3 in which the discussion about 
this performance gain is reconsidered and illustrated. 
3.10.2 The Complexity of the Generalized Collect Algorithm 
We have just mentioned that the use of the identity element instead of neutral elements 
for the initialization of join tree nodes does not change the overall complexity of 
the collect algorithm. Computations still take place on the factor domains that are 
bounded by the largest node label, and incoming messages are still combined to the 
node content. This repeats the two main arguments that lead to the complexity bounds 
given in equation (3.37) and (3.38). However, there is a technique to reduce the space 
complexity of the collect algorithm coming from the Shenoy-Shafer architecture 
that will be introduced in the following chapter. We first point out that the space 
requirement of the identity element can be neglected since it has been adjoined to 
the valuation algebra (Pouly & Kohlas, 2005). If we furthermore stress a factor 
distribution that assigns at most one knowledgebase factor to each join tree node, the 
node initialization process does not require any additional memory. This is again in 
contrast to the use of neutral elements for initialization. We next assume that each 
node has a separate mailbox for all its parents. Then, instead of combining a message 
directly to the node content, it is simply stored in the corresponding mailbox of the 
child node. Once all mailboxes are full, a node computes the message for its child node 
by combining its still original node content with all the messages of its mailboxes. 
Clearly, this is equal to the node content just before computing the message ßi^,ch(i) 
in the above scheme. Finally, we project this valuation as proposed in equation (3.40) 
and send the result to the mailbox of the child node. It is clear that the statement of 
Theorem 3.7 (and Lemma 3.10) must be adapted under this optimization. At the end 
of the message-passing, the root node does not contain the projection of the objective 
function to its node label directly, but this result must first be computed by combining 
its still original node content with all the messages of its mailboxes. We pass on a 
formal proof for the correctness of this particular implementation of the generalized 
collect algorithm at this point, since it will be given in Section 4.1.1. Instead, we 
now analyze its impact on the space complexity: this procedure does not combine 
messages to the content of the receiving nodes but stores them in mailboxes and 
computes their combination only as an intermediate result of the message creation 
process, or to return the final query answer. Then, as soon as the message has been 
sent, this intermediate result can be discarded. Thus, instead of keeping \V\ node 
factors, we only store \E\ = \V\ — 1 messages. According to equation (3.40) the 
size of a message sent from node г to node ch(i) is always bounded by the size of 
the corresponding separator sep(i) = X(i) П X(ch(i)). Consequently, the space of 
all messages in a complete run of the collect algorithm are bounded by the largest 
separator size in the join tree called its separator width. Similarly to the treewidth, we 
define the separator width of an inference problem by the smallest separator width 
over all possible covering join trees and write sep* for this measure. This leads to the 

9 8 
COMPUTING SINGLE QUERIES 
following bound for the space complexity of this implementation: 
o(g(u* + l) + \V\-g(sep*)\ 
(3.49) 
We conclude from the definition of the separator that sep* < ω* or, in other words, 
that the separator width of an inference problem is not larger than its treewidth. 
Moreover, in practical applications the separator width is often much smaller than the 
treewidth. This generalizes the space complexity derived from the fusion algorithm in 
equation (3.20) to arbitrary covering join trees. It will be shown in Section 5.6 that for 
certain valuation algebras, we can omit the explicit computation of the node content 
and directly derive the message from the original node factor and the messages in 
the mailboxes. Then, the first term in the equation (3.49) vanishes, and the mailbox 
implementation of the generalized collect algorithm indeed provides a noteworthy 
improvement regarding space complexity. 
It corresponds to the relevant literature (Pouly, 2008; Schneuwly, 2007; Schneuwly 
et a/. ,2004) to introduce the generalized collect algorithm in the above manner, 
although the improvement based on mailboxes is clearly preferable for computational 
purposes. On the other hand, combining messages directly to the node content often 
simplifies the algebraic reasoning about the collect algorithm and makes it possible 
to apply its correctness proof to other local computation schemes. In the following 
chapter, we will introduce the Shenoy-Shafer architecture as a local computation 
technique to solve multi-query inference problems. This architecture is entirely based 
on the mailbox scheme, and if we only consider some part of its message-passing, 
we naturally obtain the mailbox implementation of the generalized collect algorithm. 
This allows us to refer to equation (3.49) when talking about the space complexity 
of the generalized collect algorithm. Finally, since the collect algorithm with neutral 
elements does not have any advantage over its generalized version, we subsequently 
refer to the second approach as the collect algorithm. 
3.11 
AN APPLICATION: THE FAST FOURIER TRANSFORM 
In this last section, we focus on the application of local computation to the single-
query inference problem derived from the discrete Fourier transform in Instance 2.6. 
There are several reasons why this exact problem has been chosen for a case study: 
from a historic perspective, local computation methods were introduced to deal with 
problems of exponential (or even higher) complexity. But a direct computation of the 
discrete Fourier transform given in equation (2.8) only requires 0(N2) 
operations, 
if TV denotes the number of signal samplings. Moreover, a clever arrangement of 
the operations even leads to a 0(N · log TV) complexity known as the fast Fourier 
transform. In fact, there is a whole family of algorithms that share this improved 
complexity, such that the notion of a fast Fourier transform does not refer to a single 
algorithm but rather to all algorithms that solve the discrete Fourier transform with a 

AN APPLICATION: THE FAST FOURIER TRANSFORM 
9 9 
time complexity of 0(N ■ log N). Our case study thus shows the effect of applying 
local computation to a polynomial problem. 
■ 3.12 The Fast Fourier Transform 
Instance 2.6 transformed the discrete Fourier transform into a single-query 
inference problem by the introduction of variables. The next step therefore 
consists in finding a covering join tree for the inference problem of equation 
(2.9). This however is not a difficult task due to the very structured factor 
domains in the inference problem. This structure is illustrated in Figure 2.11 
on page 44. Figure 3.16 shows a possible join tree for the discrete Fourier 
transform. There are m + 1 nodes, and we quickly remark that the running 
intersection property is satisfied. It is furthermore a covering join tree: the root 
node corresponds to the query {KQ, ..., Km_i} 
and the knowledgebase fac-
tors can be distributed as shown in Figure 3.16. Thus, executing the generalized 
collect algorithm on this join tree computes a discrete Fourier transform. 
Figure 3.16 A possible covering join tree for the discrete Fourier transform. 
Let us now analyse the time complexity of these computations. It is known 
from Section 3.10.2 that the time complexity of the generalized collect algo-
rithm is bounded by 
o(\V\-f{iS 
+ \) 

100 
COMPUTING SINGLE QUERIES 
We used the valuation algebra of (complex) arithmetic potentials in the deriva-
tion of the inference problem for the discrete Fourier transform. All variables 
are binary and we obtain a possible weight predictor from equation (3.17). Also, 
we have |V| — m + 1 nodes, and the largest node label has m + 1 variables. 
This specializes the complexity bound to: 
o(m-2m+1\ 
In Instance 2.6 we chose N = 2m, thus m = log N which finally results in 
OUV-logJVJ. 
This analysis shows that we obtain the time complexity of the fast Fourier transform 
by application of local computation. It is important to note that the local computation 
approach does not correspond to the classical scheme of J. Cooley and J. Tukey (Coo-
ley & Tukey, 1965) that is based on the divide-and-conquer paradigm. Nevertheless, 
local computation reproduces the same complexity which makes it join the family 
of fast Fourier transforms. From a more general perspective, this result suggests that 
there may be good reasons to apply local computation techniques also to polynomial 
problems. This opens a new field of application for the theory in this book that has 
not yet been caught completely by the research community. Further applications of 
local computation to polynomial problems will be studied in Chapter 6 and 9 which 
both deal with formalism to model path problems in graphs. 
3.12 CONCLUSION 
This chapter was focused on the solution of single-query inference problems. We 
pointed out that the memory requirement of valuations as well as the time complexity 
of their operations generally increase with the size of the involved domains. This 
increase may be polynomial, but it is in many cases exponential or even worse which 
makes a direct computation of inference problems impossible. Instead, it is essential 
that algorithms confine in some way the size of all intermediate results that occur 
during the computations. This is the exact promise of local computation. The simplest 
local computation scheme is the fusion or bucket elimination algorithm. It is based on 
variable elimination instead of projection and is therefore only suitable for valuation 
algebras based on variable systems. The analysis of its time and space complexity 
identified the treewidth of the inference problem as the crucial complexity measure 
for local computation. It also turned out that especially the memory consumption 
of the fusion algorithm is often too high for practical purposes. A very promising 
approach to tackle these problems is to decouple join tree construction from the 
actual local computation process. We may then take an arbitrary covering join tree 
for the solution of a given inference problem and describe local computation as a 
message-passing scheme where nodes act as virtual processors that independently 

APPENDIX 
101 
compute and transmit messages. Doing so, all local computation schemes become 
distributed algorithms. The decoupling process leads to the collect algorithm that is 
based on projection and therefore also applicable for valuations with domains from an 
arbitrary lattice. In its first occurrence, however, the collect algorithm presupposed the 
existence of neutral elements to initialize join tree nodes. In addition, it suffered from 
an even worse space complexity than the fusion algorithm. An algebraic solution for 
the dependence on neutral elements adjoins a unique identity element to the valuation 
algebra that is henceforth used for node initialization. Now, the collect algorithm can 
be applied to all formalisms that satisfy the valuation algebra axioms without any kind 
of restriction. Furthermore, computations generally take place on smaller domains and 
no unnecessary combinations with neutral elements are executed. The last addressed 
issue was the still unsatisfactory space complexity of the collect algorithm that comes 
from the algebraic description where each node combines incoming messages to its 
content. Alternatively, we proposed an implementation where messages are stored in 
mailboxes and only combined for the creation of the child message. Immediately after, 
the combination is again discarded. Consequently, the collect algorithm only stores 
messages that generally are much smaller than their combination as node content. 
The general space complexity reduces slightly under this setting, but for the valuation 
algebras we are going to study in Chapter 5, this reduction may be considerable. A 
similar caching policy based on mailboxes will take center stage in the following 
chapter where multi-query inference problems are treated. The last section of this 
chapter applied the collect algorithm to the inference problem of a discrete Fourier 
transform. Doing so, the treewidth complexity of local computation turns into the low 
polynomial complexity of the fast Fourier transform. This is a strong argument for 
our claim that local computation may also be useful to solve problems of polynomial 
nature. 
Appendix: Proof of the Generalized Collect Algorithm 
We prove Theorem 3.7 which is a generalization of Theorem 3.6. The following 
lemma will be useful for this undertaking: 
Lemma C.13 Define 
r 
Vi = 
\Ju>f 
t = l,...,r. 
(C.l) 
Then, for г = 1,..., r — 1, 
(
\ 
4-2/1+1 

102 
COMPUTING SINGLE QUERIES 
Proof: First, it has to be ensured that yi+i С yi holds in order to guarantee that the 
projection in equation (C.2) is well defined: 
к = ^ υ ^ , υ 
Ù "f 
r 
Уг+l 
- 
Uch{i) U 
( J 
"j 
■ 
j=i+l,j^ch(i) 
From equation (3.42) it follows that 
«So = J&i)u K(<) n ^ W ) ) Ç «$,) и « « 
(C.3) 
and since ω^ 
= ω; for all j ^ ch(i), yi+i Q yi holds. 
Next, the following property will be proved: 
Lüll)nyi+1 = 
uVn\(ch(i)). 
(C.4) 
Assume first that X G uif' Π \(ch(i)). Then, equation (C.3) implies that X G ш^щ\ 
(г) 
and hence, by the definition of yi+i, we have X G Уг+ι- Thus, X G ω^ Π î/i+i. 
(г) 
On the other hand, assume that X G ω\ Π 2/i+i. Then, by the running intersection 
property and the definition of yi+i, X G X(ch(i)) and therefore X G ω·1'Πλ(ώ(ί)). 
An immediate consequence of equation (3.42) and (3.43) is that 
Vi+i 
- 
Uch(i) 
U 
j = 
3 
^l
)
( i)
u 
и 
=г+1,,7^с/г(г) 
и 
( * ■ 
1 
(i) 
j=i+l,jjtch(i) 
We therefore obtain by application of the combination axiom and Property (C.4): 
)
4l/i+l 
/ 
/ 
\ \ iVi + l 
V 
\ 
j=i+l,jjich(i) 
j=i+l,j'^c/i(i) 
j=i+lj^ch(i) 
- ^ ® é 
€+ι) = ê *ί+ι) 
j=i+l,jyich(i) 
j=i+l 

EXERCISES 
103 
This proves the first equality of (C.2). The second is shown by induction over i. For 
i = 1, the equation is satisfied since 
4î/2 
Let us assume that the equation holds for i, 
Then, by transitivity of marginalization, 
(
\ iVi+i 
(g^f) 
= (^')
i ! / i + 1 = <t>
iyi + 1 
which proves (C.2) for all i. 
m 
Theorem 3.7 can now be verified: 
Proof of Theorem 3.7 
Proof: By application of equation (C.2) for г = r — 1, it follows that 
Vr = 
<4r>. 
(C.5) 
It remains to prove that wf = A(r). For this purpose, it is sufficient to show that if 
X € X(r) then X e u>r since Wr Ç A(r). Let X G A(r). Then, according to the 
definition of the covering join tree for an inference problem, there exists a factor xpj 
with X e d(ipj). ipj is assigned to node p — a(j) and therefore X G ωρ. equation 
(3.41) implies that X £ 
^cfi(p) ant^ ^ ^ ω^ ^ follows by repeating this argument up 
to the root node r. 
m 
Proof of Lemma 3.10 
Proof: Node г is the root of the subtree 7Î and contains the marginal to ω\1' of the 
factors associated with 7i due to equation (C.5). 
■ 
PROBLEM SETS AND EXERCISES 
C.l * Propose a weight predictor for the valuation algebra of Gaussian potentials. 
C.2 * Exercise B. 1 in Chapter 2 asked to analyse the two inference problems from 
the discrete cosine transform and the inference problems from the discrete Fourier 

104 
COMPUTING SINGLE QUERIES 
transform. Continue this analysis by showing that all three problems can be solved 
with the same covering join tree from Instance 3.12. 
C.3 * Apply the collect algorithm to the inference problem of the Hadamard trans-
form derived in Instance 2.5 and show that a clever choice of the covering join tree 
leads to the time complexity of the fast Hadamard transform. Indications are given 
in (Aji & McEliece, 2000; Gonzalez & Woods, 2006). 
C.4 * If neutral elements are present in a valuation algebra (Φ, D), we may introduce 
an operation of vacuous extension as shown in equation (3.24). Prove the following 
properties of vacuous extension: 
1. if x С у then ely = ey; 
2. if ά{φ 
3. ϋά{φ 
4. ifd{(j> 
5. if d(0 
6. ίΐά(φ 
7. ιίά(φ 
8. ϋά(φ 
= x then for all y G D we have ф®еу = ф^хиу; 
= x then φΐχ = φ; 
= χ and у С χ, then ф®еу = ф; 
= x and x Ç у С z, then (φ^ν)^ζ — φ^ζ; 
= χ, ά{φ) = y, and х,у С z, then (ф <g> φ)^ζ = φ^ζ (g> ijAz; 
= x, d{4>) = у, then ф <g> ψ = ф*хиу ® гр^хиУ; 
= χ then (ф*хиУ)±У = 
(ф±хПУуУ. 
The solution to this exercise is given in (Kohlas, 2003), Lemma 3.1. 
C.5 * In a valuation algebra with neutral element (Ф, D) we may define a transport 
operation as follows: For ф s Ф with ά(φ) = x and y £ D, 
ф-^У 
= 
(ф1хиУ)±У. 
Due to Property 8 in Exercise C.4 we also have 
φ-+ν = (ф±хПУ)ТУ. 
Prove the following properties of the transport operation: 
1. ίίά(φ) = χΰιεηφ~+χ = φ; 
2. if ά(φ) = x and x, у, С z then ф~>у = 
(фи)1у; 
3. if ά(φ) = xandx<Zy 
then ф->* = ф^у; 
4. if ά(φ) = xandxDy 
then ф^у = φ^; 
(C.6) 
(C.7) 

EXERCISES 
105 
5. if y Ç z then φ^ 
= 
{ф^г)^у; 
6. For each ф and all x, у we have (ф~*х)~*у 
7. if ά(φ) — x and ά(φ) = у then (φ ® ψ)~*: 
The solution to this exercise is given in (Kohlas, 2003), Lemma 3.2. 
C.6 * Let (Φ, D) be a stable valuation algebra according to Definition 3.6. Based 
on the transport operation of equation (C.6), we further define the following relation: 
For φ, φ G Φ with ά(φ) — x and ά(ψ) = y we have 
φ = φ 
if, and only if, 
ф^у = ф and ф~+х = ф. 
(С.8) 
a) Prove that this relation is an equivalence relation satisfying the properties 
ф = ф (reflexivity), ф = ф implies ф = ф (symmetry) and ф = ф and 
ф = η implies that φ = η (transitivity) for φ,φ,η G Φ. The solution to this 
exercise is given in (Kohlas, 2003), Lemma 3.3. 
b) Prove that this relation is further a congruence relation with respect to the 
operations of combination and transport, i.e. verify the following properties. 
The solution to this exercise is given in (Kohlas, 2003), Theorem 3.4. 
1. ΐίφι=φι 
and φι = φ2 then φγ ® φ? = φ\ <8> φι\ 
2. \ίφ = φ then it holds for all z € D that φ^ζ = 
φ^ζ. 
c) Let r be a set of variables and D = V(r) its powerset. Prove that for 
ф, ф Ç. Ф we have ф = ф if, and only if, 
φ^ 
= 
VTr· 
(C.9) 
C.7 * * 
Let (Φ, D) be a stable valuation algebra with the congruence relation of 
equation (C.8). We next consider the induced equivalence classes defined for φ € Φ 
with ά{φ) = x by [φ] = {φ G Φ : φ~*χ = φ}. Intuitively, this groups valuations that 
express the same information, even if they have different domains. As a consequence 
of equation (C.9) all valuations of the same equivalence class are equal, if they are 
vacuously extended to the complete variable universe. The elements [φ] are therefore 
called domain-free valuations. The set of all equivalence classes forms an algebra 
(Φ, D), called quotient algebra, with 
Φ = им. 
Since the relation is a congruence, we may define the following operations in (Φ, D) : 
1. Combination: \φ] (g) [φ] = [φ®φ]\ 
2. Transport: [φ]^χ = [φ^χ]. 
= φ®φ->χ. 

106 
COMPUTING SINGLE QUERIES 
These operations are well-defined since they do not depend on the chosen represen-
tative for the equivalence classes. Verify the following properties in (Φ, D): 
1. for all x,y G D we have {[φ]^χ)^ν 
= [ф]^хПу; 
2. for all x G D we have (\ф]^х <g> [ф\)^х = [ф]^х ® [ф}^х; 
3. for all x G D we have [e^]"** = [e3]; 
4. for all [ф] G Ф we have [ф)^г = [</>]. 
The solution to this exercise is given in (Kohlas, 2003), Theorem 3.5. Structures like 
(Ф, D) are called domain-free valuation algebras and posses a rich theory which 
takes center stage in algebraic information theory. 
C.8** We have seen in Exercise C.7 that a domain-free valuation algebra can be 
obtained for every stable valuation algebra. Here, we propose the converse direction 
to derive a valuation algebra starting from a domain-free valuation algebra. Let r 
be a set of variables and D = V{r) its powerset lattice. We assume an arbitrary 
domain-free valuation algebra (Ф, D) defined by two operations 
1. Combination: Ф х Ф -» Φ; (φ,ψ) t-> ф <g> ф; 
2. Focusing: Φ x D ->· Φ; (ψ,χ) ^ 
φ^χ, 
satisfying the following axioms: 
(Dl) Semigroup: Φ is associative and commutative under combination. There is a 
neutral element e € Φ such that e <g> φ = φ ® e = φ for all φ G Φ. 
(D2) Transitivity: For φ G Φ andx7y £ D, 
(ф=*х)^у 
= 
ф^хПу. 
(D3) Combination: For φ,φ £ ty and i € f l 
(V^x ® 0 ) ^ 
= 
Φ^Χ®Φ^Χ. 
(D4) Neutrality: For ж G D, 
(D5) Support: For t/; G Ф, 
V>^r 
= 
V· 
If for г/' € Ф and x e D w e have i/^* = ^ then a; is called a support of i/>. Due to 
Axiom (D5), r is a support of every valuation algebra and, according to (D4), every 
domain is a support of the neutral element. We next consider the set of pairs 
Ф* = 
{(φ,χ) : фе Ф and ф=*х = ф} . 
(CAO) 

EXERCISES 
107 
These pairs can be considered as valuations labeled by their supports. Therefore, we 
define the following operations on (Φ*, D): 
1. Labeling: For (φ,χ) € Φ* we define 
d(ip,x) 
= 
x. 
(CM) 
2. Combination: For (φ, χ), (ψ, y) G Φ* we define 
(ф,х)®(ф,у) 
= 
(ф®ф,хиу) 
(С.12) 
3. Projection: For (ф, х) £ Ф* and у С х we define 
(φ,χ)^ 
= 
(ф^\у). 
(С.13) 
Prove that (Φ*,ΰ) together with these operations satisfy the axioms of a stable 
valuation algebra. Indications can be found in (Kohlas, 2003), Section 3.2. Putting 
these things together, we may start from a stable valuation algebra and derive a 
domain-free valuation algebra, from which we may again derive a stable valuation 
algebra. This last algebra is isomorph to the valuation algebra at the beginning of the 
process as shown in (Kohlas, 2003). Conversely, we may start from a domain-free 
valuation algebra, derive a stable valuation algebra and then again a domain-free 
valuation algebra. Again, the algebras at the beginning and end of this process are 
isomorph. 

CHAPTER 4 
COMPUTING MULTIPLE QUERIES 
The foregoing chapter introduced three local computation algorithms for the solution 
of single-query inference problems, among which the generalized collect algorithm 
was shown to be the most universal and efficient scheme. We therefore take this 
algorithm as the starting point for our study of the second class of local computation 
procedures that solve multi-query inference problems. Clearly, the most simple ap-
proach to compute multiple queries is to execute the collect algorithm repeatedly for 
each query. Since we already gave a general definition of a covering join tree that 
takes an arbitrary number of queries into account, it is not necessary to take a new 
join tree for the answering of a second query on the same knowledgebase. Instead, we 
redirect and renumber the join tree in such a way that its root node covers the current 
query and execute the collect algorithm. This procedure is repeated for each query. 
In Figure 3.15, arrows indicate the direction of the message-passing for a particular 
root node and therefore reflect the restrictions imposed on the node numbering. It is 
easy to see that changing the root node only affects the arrows between the old and 
the new root node. This is shown in Figure 4.1 where we also observe that a valid 
node numbering for the redirected join tree is obtained by inverting the numbering 
on the path between the old and the new root node. Hence, it is possible to solve a 
multi-query inference problem by repeated execution of the collect algorithm on the 
Generic Inference: A Unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc. 
109 

110 
COMPUTING MULTIPLE QUERIES 
same join tree with only a small preparation between two consecutive runs. Neither 
the join tree nor its factorization have to be modified. 
Figure 4.1 The left-hand join tree is rooted towards the node with label {B, D, E}. If then 
node {E, L, T} is elected as new root node, only the dashed arrows between the old and the 
new root are affected. Their direction is inverted in the right-hand figure and the new node 
numbering is obtained by inverting the numbering between the old and the new root. 
During a single run of the collect algorithm on a covering join tree (V, E, A, D), 
exactly \E\ messages are computed and transmitted. Consequently, if a multi-query 
inference problem with n G N queries is solved by repeated application of collect on 
the same join tree, n\E\ messages are computed and exchanged. It will be illustrated 
in a short while that most of these messages are identical and their computation 
can therefore be avoided by an appropriate caching policy. The arising algorithm is 
called Shenoy-Shafer architecture and will be presented in Section 4.1. It reduces the 
number of computed messages to 2\E\ for an arbitrary number of queries. However, 
the drawback of this approach is that all messages have to be stored until the end of 
the total message-passing. This was the main reason for the development of more 
sophisticated algorithms that aim at a shorter lifetime of messages. These algorithms 
are called Lauritzen-Spiegelhalter architecture, HUGIN architecture and idempotent 
architecture and will be introduced in Section 4.3 to Section 4.5. But these alternative 
architectures require more structure in the underlying valuation algebra and are there-
fore less generally applicable than the Shenoy-Shafer architecture. More concretely, 
they presuppose some concept of division that will be defined in Section 4.2. Essen-
tially, there are three requirements for introducing a division operator, namely either 
separativity, regularity or idempotency. These considerations require more profound 
algebraic insights that are not necessary for the understanding of division-based local 
computation algorithms. We therefore delay their discussion to the appendix of this 
chapter. A particular application of division in the context of valuation algebras is 
scaling or normalization. If, for example, we want to interpret arithmetic potentials 
from Instance 1.3 as discrete probability distributions, or set potentials from Instance 
1.4 as Dempster-Shafer belief functions, normalization becomes an important issue. 
Based on the division operator, scaling or normalization can be introduced on a 
generic, algebraic level as explored in Section 4.7. Moreover, it is then also required 
that the solution of inference problems gives scaled results. We therefore focus in 
Section 4.8 on how the local computation architectures of this chapter can be adapted 
to deliver scaled results directly. 

THE SHENOY-SHAFER ARCHITECTURE 
111 
4.1 THE SHENOY-SHAFER ARCHITECTURE 
The initial situation for the Shenoy-Shafer architecture and all other local computation 
schemes of this chapter is a covering join tree for the multi-query inference problem 
according to Definition 3.8. Join tree nodes are initialized using the identity element, 
and the knowledgebase factors are combined to covering nodes as specified by 
equation (3.39). Further, we shall see that local computation algorithms for the 
solution of multi-query inference problems exchange messages in both directions of 
the edges. It is therefore no longer necessary to direct join trees. Let us now reconsider 
the above idea of executing the collect algorithm repeatedly for each query on the 
same join tree. Since only the path from the old to the new root node changes 
between two consecutive runs, only the messages of these region are affected. All 
other messages do not change but are nevertheless recomputed. 
Example 4.1 Assume that the collect algorithm has been executed on the left-hand 
join tree of Figure 4.1. Then, the root node is changed and collect is restarted for 
the right-hand join tree. It is easy to see that the messages μι_>7, μ^β 
and μ3->5 
already existed in the first run of the collect algorithm. Only the node numbering 
changed but not their content. 
A technique to benefit from already computed messages is to store them for 
later reuse. The Shenoy-Shafer architecture (Shenoy & Shafer, 1990) organises this 
caching by installing mailboxes between neighboring nodes which store the ex-
changed messages. In fact, this is similar to the technique applied in Section 3.10.2 
to improve the space complexity of the collect algorithm. This time however, we 
assume two mailboxes between every pair of neighboring nodes since messages are 
sent in both directions. Figure 4.2 schematically illustrates this concept. 
v 
ßi-yj 
1 
* 
Mailbox of j 
Mailbox of i „ 
/ 
ßj^-i 
' 
Figure 4.2 
The Shenoy-Shafer architecture assumes mailboxes between neighboring nodes 
to store the exchanged messages for later reuse. 
Then, the Shenoy-Shafer algorithm can be described by the following rules: 
Rl: Node i sends a message to its neighbor j as soon as it has received all messages 
from its other neighbors. Leaves can send their messages right away. 
R2: When node г is ready to send a message to neighbor j , it combines its initial 
node content with all messages from all other neighbors. The message is 
computed by projecting this result to the intersection of the result's domain 
and the receiving neighbor's node label. 
The algorithm stops when every node has received all messages from its neighbors. In 
order to specify this algorithm formally, a new notation is introduced that determines 

112 
COMPUTING MULTIPLE QUERIES 
the domain of the valuation described in Rule 2. If г and j are neighbors, the domain 
of node г at the time where it sends a message to j is given by 
Wi-yj 
= 
<^U 
( J 
d(ßk^i), 
(4.1) 
where d(ipi) = ω^ is the domain of the node content of node i. 
• The message sent from node г to node j is 
,Ι-ω^ηλϋ) 
μ ^ 
= 
| V» ® 
Q9 
Mfc^i ) 
· 
(4.2) 
Theorem 4.1 At the end of the message-passing in the Shenoy-Shafer architecture, 
node г can compute 
Φ14ΐ) 
= Фг® (g) ßj^i- 
(4.3) 
j€ne(i) 
Proof: The important point is that the messages μ*;_^ do not depend on the actual 
schedule used to compute them. Due to this fact, any node i &V can be selected as root 
node. Then, the edges are directed towards this root and the nodes are renumbered. 
The message-passing towards the root corresponds to the collect algorithm and the 
proposition for node i follows from Theorem 3.7. 
■ 
Answering the queries of the inference problem from this last result demands one 
additional projection per query. Since each query Xi is covered by some node j € V, 
we obtain the answer for this query by computing: 
ЫЧз)\ 
Хг 
(44) 
Example 4.2 We execute the Shenoy-Shafer architecture on the join tree of Figure 
4.3. There are 6 edges, thus 12 messages to be sent and stored in mailboxes. These 
messages are: 
• μι-,.2 = Щ 
• M3->6 
• 
M4-S-5 
• M7->6 
• М2->1 
= *f™ 
= ψΐω^ 
= φ^β 
= (φ2 ® μ5-+2)4'ω2 
• Α*6->3 = 
(Фб ® M5->6 ® M7->e)4'U'6^3 
• μ5-»4 - 
(Фь ® μ2->-5 ® 
μβ-+5)1ω5~*4 

THE SHENOY-SHAFER ARCHITECTURE 
113 
• μ6_>7 = (Ψβ ® Мз-νβ <8> 
μ5^6)1ω6^7 
• / i 2 ^ 5 - 
{ф2 ® Mi-^)4"2-*5 
• μ 5^ 2 
= (^5 ® M4^5 ® Me^s)4·"5-*2 
• M5->6 = 
(V>5 ® М4-У5 <8> /Хг^.б)4'"5^6 
• Мб->5 = 
(^6 8) M7->6 ® /^З-^б)4-"6^5 
Ai iAen enJ o/iAe message-passing, the nodes compute: 
• ф^Е'Ь'Т^ 
= tp2 ® μι->2 О μ5^2 
• ^ B - L - s > = V>3 О Ме^з 
• 0i{ß-^} = 
ψ4®μ5^4 
• (/r^'·^ = ψ5 (g) μ2^5 <8> μ4->5 ® Мб-»-5 
Figure 4.3 
Since messages are sent in all directions in the Shenoy-Shafer architecture, join 
trees do not need to be rooted anymore. 
Since the Shenoy-Shafer architecture accepts arbitrary valuation algebras, we may 
in particular apply it to formalisms based on variable elimination. This specialization 
is sometimes called cluster-tree elimination. In this context, a cluster refers to a join 
tree node together with the knowledgebase factors that are assigned to it. Further, 
we could use the join tree from the graphical representation of the fusion algorithm 
if we are, for example, only interested in queries that consist of single variables. 
Under this even more restrictive setting, the Shenoy-Shafer architecture is also called 
bucket-tree elimination (Kask et al, 2001). Answering more general queries with the 
bucket-tree elimination algorithm is again possible if neutral elements are present. 
However, we continue to focus our studies on the more general and efficient version 
of the Shenoy-Shafer architecture introduced above. 

114 
COMPUTING MULTIPLE QUERIES 
4.1.1 Collect & Distribute Phase 
For a previously fixed node numbering, Theorem 4.1 implies that 
^ λ « = ^ Θ 
(g) ßj-nQßchw^i 
= il>\r) ® ßch(i)-*i- 
(4-5) 
j€pa(i) 
This shows that the collect algorithm is extended by a single message coming from 
the child node in order to obtain the Shenoy-Shafer architecture. In fact, it is always 
possible to schedule a first part of the messages in such a way that their sequence 
corresponds to the execution of the collect algorithm. The root node r eV will then 
be the first node which has received the messages of all its neighbors. It suggests itself 
to name this first phase of the Shenoy-Shafer architecture collect phase or inward 
phase since the messages are propagated from the leaves towards the root node. It 
furthermore holds that 
(i) 
Ui-+Ch(i) 
— 
4 
for this particular scheduling. This finally delivers the proof of correctness for the 
particular implementation proposed in Section 3.10.2 to improve the space complexity 
of the collect algorithm. In fact, the collect phase of the Shenoy-Shafer architecture 
behaves exactly in this way. Messages are no more combined to the node content but 
stored in mailboxes, and the combination is only computed to obtain the message for 
the child node. Then, the result is again discarded, which guarantees the lower space 
complexity of equation (3.49). The messages that remain to be sent after the collect 
phase of the Shenoy-Shafer architecture constitute the distribute phase. Clearly, the 
root node will be the only node that may initiate this phase since it possesses all 
necessary messages after the collect phase. Next, the parents of the root node will 
be able to send their messages and this propagation continues until the leaves are 
reached. In fact, nodes can send their messages in the inverse order of their numbering. 
Therefore, the distribute phase is also called outward phase. 
Lemma 4.1 It holds that 
\{i) 
= 
i^r)U(A(i)nA(c/i(i))). 
(4.6) 
Proof: We have 
X(i) 
= 
A(t)U(A(î)nA(c/i(i))) 
= 
o4r) U ucMi)_>i U (A(i) П X(ch(i))) 
= 
ujlr)U(\{i)n\(ch(i))). 
The second equality follows from equation (4.5). 
■ 
The next lemma states that the eliminator elim(i) = \(i) — (λ(ζ) ΓΊ X(ch(i))) (see 
Definition 3.12) of each node i G V will be filled after the collect phase: 

THE SHENOY-SHAFER ARCHITECTURE 
115 
Lemma 4.2 It holds that 
4
( r ) - (ω|Γ) П A(cft(i))) 
= 
A(i)-(A(i)nA(cA(i))). 
(4.7) 
Proof: Assume X contained in the right-hand part of the equation. Thus, X G А(г) 
but X <£ \(ι) П X(ch(i)). From equation (4.6) can be deduced that X G ω> . Since 
ш|г)ПА(<Ж(г)) 
С А(г) П А(сЛ(г)), 
X ф ω^ Π А(сЛ(г)). This proves that 
Lülr) - {ω\Γ) П \{ch(i))) 
D λ(ι)-(λ(ί)ηλ(ίΛ(ι))). 
Assume that X is contained in the left-hand part. So, X G ω^ 
С А(г) but X ^ 
шг 
П А(с/г(г)) which in turn implies that X ф \(ch(i)) and consequently X ^ 
А(г) П А(сЛ(г)). Thus, X € А(г) - (А(г) П A(c/i(i))) and equality must hold. 
■ 
The particular scheduling into a collect and distribute phase allows us to describe 
the intrinsically distributed Shenoy-Shafer architecture by a sequential algorithm. 
First, Algorithm 4.1 performs the complete message-passing. It does not return any 
value but ensures that all Дг->с/цг) ar,d Мс^(г)->г же stored in the corresponding 
mailboxes. To answer a query x С ά(φ) we then call Algorithm 4.2 which presup-
poses that the query is covered by some node in the join tree. The message-passing 
procedure can therefore be seen as a preparation step to the actual query answering. 
Algorithm 4.1 Shenoy-Shafer Architecture: Message-Passing 
input: 
(V, E, A, D) output: 
-
begin 
for г = l . . . | V | - 1 do 
/ / Collect Phase: 
ω := <*(^г)иЦ,-ера(0<*(/^->г); 
ft^A(i) 
:= (^i®®jepa(i)Mj-«)-l"nA(c'l(i)); 
/ / in mailbox 
end; 
for г = \V\ - 1... 1 do 
// Distribute Phase: 
ω := * ) U U j 6 M ( c K i ) ) A 3 ^ i d ( ' i H 0 ; 
ßch(i)-n 
■= (V'c/l(i)®<8'jene(cft(i))A^i^^i)4-a'nA(i); 
// in mailbox 
end; 
end 
Algorithm 4.2 Shenoy-Shafer Architecture: Query Answering 
input: 
(V,E,\,D), 
x Ç ά(φ) output: 
φίχ 
begin 
for i = l...\V\ 
do 
if x С А(г) do 
return (Vi <g> <8>j6„e(i) H-n)ix 
; 
end; 
end 

116 
COMPUTING MULTIPLE QUERIES 
The implementation of Algorithm 4.2 corresponds to Theorem 4.1, but can easily 
be changed using equation (4.5). The combination of the node content with all parent 
messages has already been computed for the creation of the child message in the 
collect phase. If this partial result is still available, it is sufficient to combine it with 
the message obtained from the child node. Doing so, Algorithm 4.2 executes at most 
one combination. Clearly, under this modification, the collect phase becomes again 
equal to the original description of the collect algorithm in Section 3.10 since we 
again store the combination of the parent messages with the original node content for 
each node. This saves a lot of redundant computation time in the query-answering 
procedure since the combination of the original node factor with all arrived messages 
is already available, for the prize of an additional factor per node whose space is 
again bounded by the node label. Hence, we generally refer to such a situation as a 
space-for-time trade. As memory seems to be the more sensitive resource in many 
practical applications, it always depends on the context if a space-for-time trade is 
also a good deal. In the following, we discuss some further optimization issues. 
4.1.2 The Binary Shenoy-Shafer Architecture 
In a binary join tree, each node has at most three neighbors or, if we reconsider 
directed join trees, at most two parents. (Shenoy, 1997) remarked that binary join trees 
generally allow better performance for the Shenoy-Shafer architecture. The reason is 
that nodes with more than three neighbors compute a lot of redundant combinations. 
Figure 4.4 shows a non-binary join tree that covers the same knowledgebase factors 
and queries as the join tree of Figure 4.3. Further, this join tree has one node less such 
that only 10 messages instead of 12 are sent during the Shenoy-Shafer architecture. 
We list the messages sent by node 5: 
M5->2 
= 
(V>5 ® A«4->5 ® Мб->5 ® A«3->5)i"5"+2 
μ5^3 
= 
{Фъ ® μ2->·5 ® M4->5 <8> /i 6^5) i W 5" 3 
μ 5-π 
= 
{Фъ ® M2->5 ® μβ->5 ® μ3->5)|ω5^4 
μ5-^6 
= 
{Фъ ® μ2->5 ® μ4->5 ® 
μ3->5)1ω^6 
It is easy to see that some combinations of messages must be computed more than 
once. For comparison, let us also list the messages sent by node 5 in Figure 4.3 that 
only has three neighbors. Here, no combination is computed more than once: 
μ5->·2 
= 
μ5->4 
= 
μ5->6 
= 
= {Фъ ® μ4->5 ® μ&^$Ϋω5 
= 
{Фъ ® μ2->5 ® μβ-^δ)4'"5 
= 
{Фъ ® μ2->5 ® μ4-»5)4'ω5 
A second reason for dismissing non-binary join trees is that some computations 
may take place on larger domains than actually necessary. Comparing the two join 
trees in Figure 4.5, we observe that the treewidth of the binary join tree is smaller 
which naturally leads to a better time complexity. On the other hand, the binary join 
tree contains more nodes that again increases the space requirement. But this increase 

THE SHENOY-SHAFER ARCHITECTURE 
117 
Figure 4.4 Using non-binary join trees creates redundant combinations. 
is only linear since the node separators will not grow. It is therefore generally a good 
idea to use binary join trees for the Shenoy-Shafer architecture. Finally, we state that 
any join tree can be transformed into a binary join tree by adding a sufficient number 
of new nodes. A corresponding join tree binarization algorithm is given in (Lehmann, 
2001). Further ways to improve the performance of local computation architectures 
by aiming to cut down on messages during the propagation are proposed by (Schmidt 
& Shenoy, 1998) and (Haenni, 2004). 
Figure 4.5 
The treewidth of non-binary join trees is sometimes larger than necessary. 
4.1.3 Performance Gains due to the Identity Element 
It was foreshadowed multiple times in the context of the generalized collect algo-
rithm that initializing join tree nodes with the identity element instead of neutral 
elements also increases the efficiency of local computation. In fact, the improvement 
is absolute and concerns both time and space resources. This effect also occurs when 
dealing with multiple queries. To solve a multi-query inference problem, we first en-
sure that each query is covered by some node in the join tree. In most cases, however, 
queries are very different from the knowledgebase factor domains. Therefore, their 
covering nodes often still contain the identity element after the knowledgebase fac-
tor distribution. In such cases, the performance gain caused by the identity element 
becomes important. Let us reconsider the knowledgebase of the medical example 
from Instance 2.1 and assume the query set {{A, B, S, Γ}, {D, S, X}}. We solve 

118 
COMPUTING MULTIPLE QUERIES 
this multi-query inference problem using the Shenoy-Shafer architecture and first 
build a join tree that covers the eight knowledgebase factors and the two queries. 
Further, we want to ensure that all computations take place during the run of the 
Shenoy-Shafer architecture, which requires that each join tree node holds at most 
one knowledgebase factor. Otherwise, a non-trivial combination would be executed 
prior to the Shenoy-Shafer architecture as a result of equation (3.39). A binary join 
tree that fulfills all these requirements is shown in Figure 4.6. Each colored node 
holds one of the eight knowledgebase factors, and the other (white) nodes store the 
identity element. We observe that the domain of each factor directly corresponds to 
the corresponding node label in this particular example. 
Figure 4.6 
A binary join tree for the medical inference problem with queries {A, B, S, T} 
and {D, S, X}. The colored nodes indicate the residence of knowledgebase factors. 
We now execute the collect phase of the Shenoy-Shafer architecture and send 
messages towards node 14. In doing so, we are not interested in the actual value of 
the messages but only in the domain size of the total combination in equation (4.2) 
before the projection is computed. Since domains are bounded by the node labels, we 
color each node where the total label has been reached. The result of this process is 
shown in Figure 4.7. Interestingly, only two further nodes compute on their maximum 
domain size. All others process valuations with smaller domains. For example, the 
message sent by node 11 consists of its node content (i.e. the identity element), the 
message from node 1 (i.e. a valuation with domain {A}) and the message from node 2 
(i.e. a valuation of domain {A, T}). Combining these three factors leads to a valuation 
of domain {A, T} С {А, В, S, T). Therefore, this node remains uncolored in Figure 
4.7. If alternatively neutral elements were used for initialization, each node content 
would be blown up to the label size, or, in other words, all nodes would be colored. 
4.1.4 Complexity of the Shenoy-Shafer Architecture 
The particular message scheduling that separates the Shenoy-Shafer architecture into 
a collect and a distribute phase suggests that executing the message-passing in the 
Shenoy-Shafer architecture doubles the effort of the (improved) collect algorithm. 
We may therefore conclude that the Shenoy-Shafer architecture adopts a similar 

THE SHENOY-SHAFER ARCHITECTURE 
119 
Figure 4.7 
The behavior of node domains during the collect phase. Only the colored nodes 
deal with valuations of maximum domain size. 
time and space complexity as the collect algorithm in Section 3.10.2. There are 
2\E\ = 2(|V| — 1) messages exchanged in the Shenoy-Shafer architecture. Each 
message requires one projection and consists of the combination of the original 
node content with all messages obtained from all other neighbors. This sums up 
to |ne(i)| — 1 combinations for each message. Clearly, the number of neighbors is 
always bounded by the degree of the join tree, i.e. we have |пе(г)| — 1 < deg — 1. In 
total, the number of operations executed in the message-passing of the Shenoy-Shafer 
architecture is therefore bounded by 
2 ( | V | - l ) ( l + ( d e S - l ) ) 
< 
2\V\deg. 
Thus, we obtain the following time complexity bound for the message-passing: 
o(\V\-deg-f(LJ* + l)\ 
(4.8) 
It is clear that we additionally require |пе(г)| — 1 < deg combinations and one pro-
jection to obtain the answer for a given query after the message-passing. But since 
every query must be covered by some node in the join tree, it is reasonable to bound 
the number of queries by the number of join tree nodes. The additional effort for 
query answering does therefore not change the above time complexity. Note also that 
the factor deg is a constant when dealing with binary join trees and disappears from 
equation (4.8), possibly at the expense of a higher number of nodes. 
Regarding space complexity, we recall that the number of messages sent in the 
Shenoy-Shafer architecture equals two times the number of messages in the improved 
collect algorithm. Therefore, both algorithms have the same space complexity 
o(g(w* + l) + \V\-g{sep')). 
(4.9) 

120 
COMPUTING MULTIPLE QUERIES 
4.1.5 
Discussion of the Shenoy-Shafer Architecture 
Regarding time complexity, the improvement brought by the Shenoy-Shafer archi-
tecture compared to the repeated application of the collect algorithm as proposed 
in the introduction of this chapter is obvious. If we compute projections to all node 
labels in the join tree, we need \V\ executions of the collect algorithm, which result 
in an overall complexity of 
Since the degree of a join tree is generally much smaller than its number of nodes, 
the Shenoy-Shafer architecture provides a considerable speedup. Both approaches 
further share the same asymptotic space complexity. 
The Shenoy-Shafer architecture is the most general local computation scheme for 
the solution of multi-query inference problems since it can be applied to arbitrary 
valuation algebras without restrictions of any kind. Further, it provides the best pos-
sible asymptotic space complexity one can expect for a local computation procedure. 
Concerning the time complexity, further architectures for the solution of multi-query 
inference problems have been proposed that eliminate the degree from equation (4.8), 
even if the join tree is not binary. However, it was shown in (Lepar & Shenoy, 1998) 
that the runtime gain of these architectures compared to the binary Shenoy-Shafer 
architecture is often insignificant. Moreover, these alternative architectures generally 
have a worse space complexity, which recommends the application of the Shenoy-
Shafer architecture whenever memory is the more problematic resource. On the other 
hand, the computation of messages becomes much easier in these architectures, and 
they also provide more efficient methods for query answering. 
4.1.6 The Super-Cluster Architecture 
A possible improvement that applies to the Shenoy-Shafer architecture consists in a 
time-for-space trade called super-cluster scheme or, if applied to variable systems, 
super-bucket scheme (Dechter, 2006; Kask et al., 2001). If we recall that the space 
complexity of the Shenoy-Shafer architecture is determined by the largest separator 
in the join tree, we may simply merge those neighboring nodes that have a large 
separator in-between. Doing so, the node labels grow but large separators disappear 
as shown in Example 4.3. Clearly, it is again subject to the concrete complexity 
predictors / and g if this time-for-space trade is also a good deal. 
Example 4.3 The join tree shown on the left-hand side of Figure 4.8 has treewidth 
5 and separator width 3. The separators are shown as edge labels. The largest 
separator {B,C,D} 
is between node 2 and 4. Merging the two nodes leads to the 
join tree on the right-hand side of this of Figure 4.8. Now, the treewidth became 6 but 
the separator width is only 2. 
The subsequent studies of alternative local computation schemes are essentially 
based on the assumption that time is the more sensitive resource. These additional 

VALUATION ALGEBRAS WITH INVERSE ELEMENTS 
121 
Figure 4.8 
Merging neighboring nodes with a large separator in-between leads to better 
space complexity at the expense of time complexity. 
architectures then simplify the message-passing by exploiting a division operator in 
the underlying valuation algebra which therefore requires more algebraic structure. 
The background for the existence of a division operation will next be examined. 
4.2 VALUATION ALGEBRAS WITH INVERSE ELEMENTS 
A particular important class of valuation algebras are those that contain an inverse 
element for every valuation ф е Ф. Hence, these valuation algebras posses the notion 
of division, which will later be exploited for a more efficient message caching policy 
during local computation. Let us first give a formal definition of inverse elements in 
a valuation algebra (Φ, D), based on the corresponding notion in semigroup theory. 
Definition 4.1 Valuations φ, ψ S Φ are called inverses, if 
φ®ψ®φ = φ, and φ®φ®φ = ψ. 
(4.10) 
We directly conclude from this definition that inverses necessarily have the same 
domain since the first condition implies that ά(ψ) С ά(φ) and from the second we 
obtain ά(φ) Ç ά(ψ). Therefore, ά(φ) = d(ip) must hold. It is furthermore suggested 
that inverse elements are not necessarily unique. 
In Appendix D.l of this chapter, we are going to study under which conditions 
valuation algebras provide inverse elements. In fact, the pure existence of inverse 
elements according to the above definition is not yet sufficient for the introduction 
of division-based local computation architectures since it only imposes a condition 
of combination but not on projection. For this purpose, (Kohlas, 2003) distinguishes 
three stronger algebraic properties that all imply the existence of inverses. Separa-
tivity is the weakest requirement among them. It allows the valuation algebra to be 
embedded into a union of groups that naturally include inverse elements. However, 
the price we pay is that full projection gets lost in the embedding algebra (see Ap-
pendix A.3 of Chapter 1 for valuation algebras with partial projection). Alternatively, 
regularity is a stronger condition than separativity and allows the valuation algebra to 

122 
COMPUTING MULTIPLE QUERIES 
be decomposed into groups directly such that full projection is conserved. Finally, if 
the third and strongest requirement called idempotency holds, every valuation turns 
out to be the inverse of itself. The algebraic study of division demands notions from 
semigroup theory, although the understanding of these semigroup constructions is 
not absolutely necessary, neither for computational purposes nor to retrace concrete 
examples of valuation algebras with inverses. We continue with a first example of 
a (regular) valuation algebra that provides inverse elements. Many further instances 
with division can be found in the appendix of this chapter. 
■ 4.1 Inverse Arithmetic Potentials 
The valuation algebra of arithmetic potentials provides inverse elements. For 
p £ Φ with d(p) = s and x G il, we define 
P-(«) = Й
 
ifP(X)>° 
10 
otherwise. 
It is easy to see that p and ρ~λ satisfy equation (4.10) and therefore are inverses. 
In fact, we learn in the appendix of this chapter that arithmetic potentials form 
a regular valuation algebra. 
Idempotency is a very strong and important algebraic condition under which each 
valuation becomes the inverse of itself. Thus, division becomes trivial in the case of 
an idempotent valuation algebra. Idempotency can be seen as a particular form of 
regularity, but it also has an interest in itself. There is a special local computation ar-
chitecture discussed in Section 4.5 that applies only to idempotent valuation algebras 
but not to formalisms with a weaker notion of division. Finally, idempotent valuation 
algebras have many interesting properties that become important in later sections. 
4.2.1 
Idempotent Valuation Algebras 
The property of idempotency is defined as follows: 
Definition 4.2 A valuation algebra (Φ, D) is called idempotent if for all φ € Φ and 
t С ά(φ), it holds that 
ψ®φ^ 
= φ. 
(4.11) 
In particular, it follows from this definition that φ ® φ = φ. Thus, choosing 
■φ = φ satisfies the requirement for inverses in equation (4.10) which means that each 
element becomes the inverse of itself. Comparing the two definitions 4.1 and 4.2, 
we remark that the latter also includes a statement about projection, which makes it 
considerably stronger. A similar condition will occur in the definition of separativity 
and regularity given in the appendix of this chapter that finally enables division-based 
local computation. 

THE LAURITZEN-SPIEGELHALTER ARCHITECTURE 
123 
Lemma 4.3 The neutrality axiom (A7) always holds in a stable and idempotent 
valuation algebra. 
Proof: From stability and idempotency we derive 
ex 09 eX{jy = exuy 09 cx\jy = exuy· 
Then, by the labeling axiom and the definition of neutral elements, 
Cx 09 Cy = 
€x 09 Cy 09 6xUy = ^ w Cxuy = 
&xUyi 
which proves the neutrality axiom. 
■ 
■ 4.2 Indicator Functions and Idempotency 
The valuation algebra of indicator functions is idempotent. Indeed, for i e Φ 
with d{i) = s, i Ç s and x € ΩΑ we have 
(г®г;Е)(х) = г(х)-^'(x4-') = г(х) · max г(х4',у) = г(х). 
This holds because г(х) < maxyef2s_t i(xil, y). 
■ 4.3 Relations and Idempotency 
Is is not surprising that also the valuation algebra of relations is idempotent. 
For R € Ф with d(R) = s and f Ç s w e have 
Rt&R^ 
= Rwnt(R) 
= i ? M { x j l : x G i ? } = -R. 
Besides their computational importance addressed in this book, idempotent valu-
ation algebras are fundamental in algebraic information theory. An idempotent val-
uation algebra with neutral and null elements is called information algebra (Kohlas, 
2003) and allows the introduction of a partial order between valuations to express 
that some knowledge pieces are more informative than others. This establishes the 
basis of an algebraic theory of information that is treated exhaustively in (Kohlas, 
2003). See also Exercise D.4 at the end of this chapter. 
Definition 4.3 An idempotent valuation algebra with neutral and null elements is 
called information algebra. 
We now focus on local computation schemes which exploit division to improve 
the message scheduling and time complexity of the Shenoy-Shafer architecture. 
4.3 THE LAURITZEN-SPIEGELHALTER ARCHITECTURE 
The Shenoy-Shafer architecture answers multi-query inference problems on any val-
uation algebra and therefore represents the most general local computation scheme. 

124 
COMPUTING MULTIPLE QUERIES 
Alternatively, research in the field of Bayesian networks has led to another archi-
tecture called the Lauritzen-Spiegelhalter architecture (Lauritzen & Spiegelhalter, 
1988), which can be applied if the valuation algebra provides a division operator. 
Thus, the starting point of this section is a multi-query inference problem with fac-
tors taken from a separative valuation algebra. As explained above, separativity is 
the weakest algebraic condition that allows the introduction of a division operator 
in the valuation algebra. Essentially, a separative valuation algebra (Φ, D) can be 
embedded into a union of groups (Φ*, D) where each element of Φ* has a well-
defined inverse. On the other hand, it is shown in Appendix D.l.l that projection 
is only partially defined in this embedding. We therefore need to impose a further 
condition on the inference problem to avoid non-defined projections during the lo-
cal computation process. The knowledgebase factors {φχ,..., 
</>„} may be elements 
of Φ* but the objective function φ = φχ <g> · ■ · ® φη must be element of Φ which 
guarantees that φ can be projected to any possible query. The same additional con-
straint is also assumed for the HUGIN architecture (Jensen et al., 1990) of Section 4.4. 
Remember, the inward phase of the Shenoy-Shafer architecture is almost equal 
to the collect algorithm with the only difference that incoming messages are not 
combined with the node content but kept in mailboxes. This is indispensable for the 
correctness of the Shenoy-Shafer architecture since otherwise, node г would get back 
its own message sent during the inward phase as part of the message obtained in the 
outward phase. In other words, some knowledge would be considered twice in every 
node, and this would falsify the final result. In case of a division operation, we can 
divide this doubly treated message out, and this idea is exploited by the Lauritzen-
Spiegelhalter architecture and by the HUGIN architecture. 
The Lauritzen-Spiegelhalter architecture starts executing the collect algorithm 
towards root node m. We define by ψ the content of node г just before sending its 
message to the child node. According to Section 3.10 we have 
jepa(i) 
Then, the message for the child node ch(i) is computed as 
This is the point where division comes into play. As soon as node г has sent its 
message, it divides the message out of its own content: 
Ф1 <8> /vU(i)· 
(4.13) 
This is repeated up to the root node. Thereupon, the outward propagation proceeds 
in a similar way. A node sends a message to all its parents when it has received a 
message from its child. In contrast, however, no division is performed during the 
outward phase. So, if node j is ready to send a message towards г and its current 

THE LAURITZEN-SPIEGELHALTER ARCHITECTURE 
125 
content is Φ'Α, the message is 
μ^ 
= 
( ^ λ ω η λ ( ί ) 
which is combined directly with the content of the receiving node i. 
Theorem 4.2 At the end of the Launtzen-Spiegelhalter architecture, node г con-
tains ф^г> provided that all messages during an execution of the Shenoy-Shafer 
architecture can be computed. 
The proof of this theorem is given in Appendix D.2.1. It is important to note 
that the correctness of the Lauritzen-Spiegelhalter architecture is conditioned on the 
existence of the messages in the Shenoy-Shafer architecture, since the computations 
take place in the separative embedding Ф* where projection is only partially defined. 
However, (Schneuwly, 2007) weakens this requirement by proving that the existence 
of the inward messages is in fact sufficient to make the Shenoy-Shafer architecture 
and therefore also the Lauritzen-Spiegelhalter architecture work. Furthermore, in 
case of a regular valuation algebra, all factors are elements of Ф and consequently all 
projections exist. Theorem 4.2 can therefore be simplified by dropping this assump-
tion. A further interesting issue with respect to this theorem concerns the messages 
sent in the distribute phase. Namely, we may directly conclude that 
^ch(i)-
il\(i)n\(ch(i)) 
_ 
(4.14) 
We again summarize this local computation scheme in the shape of a pseudo-code 
algorithm. As in the Shenoy-Shafer architecture, we separate the message-passing 
procedure from the actual query answering process. 
Algorithm 4.3 Lauritzen-Spiegelhalter Architecture: Message-Passing 
input: 
(V,E,X,D) 
output: 
-
begin 
for i = 1 . . . \V\ - 1 do 
/ / C o l l e c t 
Phase: 
._ ,,,ld(i>i)nX(ch(i)) 
i>ch(i) ■= ФсЫ(г)® ßz->ch(i)! 
Фг ■■= Ψί®μΓ-Ιο/φ); 
end; 
for i = |V| — 1 . . . 1 do 
/ / D i s t r i b u t e 
Phase: 
._ ,/,4.A(i)nA(ch(<)) . 
Mch(t)->-i · 
fch(i) 
' 
фг := 
φί®μαΗ(ί)-*ί; 
end; 
end 

126 
COMPUTING MULTIPLE QUERIES 
Algorithm 4.4 Lauritzen-Spiegelhalter Architecture: Query Answering 
input: 
(V, E,\,D), 
xÇ ά(φ) 
output: 
φ1χ 
begin 
for г = 1 . . . \V\ do 
if x С А(г) do 
return 4>fx ; 
end; 
end 
We next illustrate the Lauritzen-Spiegelhalter architecture using the medical 
knowledgebase. In Instance 2.1 we expressed the conditional probability tables of the 
Bayesian network that underlies this example in the formalism of arithmetic poten-
tials. It is known from Instance 4.1 that arithmetic potentials form a regular valuation 
algebra. We are therefore not only allowed to apply this architecture, but we do not 
even need to care about the existence of messages as explained before. 
Example 4.4 Reconsider the covering join tree for the medical knowledgebase of 
Figure 3.15. During the collect phase, the Lauritzen-Spiegelhalter sends exactly the 
same messages as the generalized collect algorithm. These messages are given in the 
Examples 3.20 and 3.22. Whenever a node has sent a message, it divides the latter 
out of its node content. Thus, the join tree nodes hold the following factors at the end 
of the inward phase: 
Ф'х 
ф'2 
Ф'з 
Ф\ 
Ф'ь 
Ф'6 
Ф'т 
= Ψι ® μϊ\2 
= ^2®μΐ->2 
= ^з®МзДе 
= ·04®μίΛ5 
= фь ® М4^5 
= Фб ® μ5-+6 
= 
Ф7®Рв^1 
The root node does not perform a division since there is no message to divide out. As 
a consequence of the collect algorithm, it holds that 
φ'7 = ψ7®μ6-+7 = φ±(Β·°·Ε1 
We then enter the distribute phase and send messages from the root down to the 
leaves. These messages are always combined to the node content of the receiver: 
• Node 6 obtains the message 
μ7^ 
= (ψ7®μ6^Β^ 
=фЯВ,Е} 
®μ2Λ5 
<g> μ2^5 ® μ^"Λ6 
®μ^7 

THE LAURITZEN-SPIEGELHALTER ARCHITECTURE 
127 
and updates its node content to 
φ'6 ® μ7_>6 = 
φ6 ® μ5^6 ® / v U ® (^7 ® Мб-^)4"^"^ 
- 
(г/>6 ® А*5^б ® А*е^7 ®Ч>7® μβ^7) 
= {φ6®μ^6®Φ,γ{Β^} 
=фХв'Е·». 
The second equality follows from the combination axiom. For the remaining 
nodes, we only give the messages that are obtained in the distribute phase. 
• Node 5 obtains the message 
μ6_>5 = (φ6 ® μ3_>6 ® /i5->6 <g> μ ^ 7 <g> μ7_>6)·|·ίΒ·Ι'> = ^ E > L > 
• M?<fe 4 obtains the message 
М5-и1 = (V>5 ® A*2^5 ® μ4->5 8» МбДб ® Мб^б)4'{В} = 0 4 { ß } 
• Node 3 obtains the message 
μ6-+3 = (Φβ ® Мз^е «i М5^б ® мёЛ7 ® M7^6)4{B,L> = ^ { S ' L } 
• Afoife 2 obtains the message 
μ5->2 = (^5 ® A*2->5 ® M4^5 ® /хаДе <8> μ6^5)ι{Ε'1} 
= 
4>i[E'L} 
• Node 1 obtains the message 
M2^1 = W>2 ® A*1^2 <E> ßiXb ® /Х5^2)4'{Т} = </>4_{T} 
Ai //ie end σ/ί/ге message-passing, each node directly contains the projection of the 
objective function ф to its own node label. This example is well suited to once more 
highlight the idea of division. Consider node 4 of Figure 3.15: During the collect 
phase, it sends a message to node 5 that consists of its initial node content. Later, in 
the distribute phase, it receives a message that consists of the relevant information 
from all nodes in the join tree, including its own content sent during the collect 
phase. It is therefore necessary that this particular piece of information has been 
divided out since otherwise, it would be considered twice in the final result. Dealing 
with arithmetic potentials, the reader may easily convince himself that combining a 
potential with itself leads to a different result. 
4.3.1 Complexity of the Lauritzen-Spiegelhalter Architecture 
The collect phase of the Lauritzen-Spiegelhalter architecture is almost identical to the 
unimproved collect algorithm. In a nutshell, there are | V \ — 1 transmitted messages and 
each message is combined to the content of the receiving node. Further, one projection 

128 
COMPUTING MULTIPLE QUERIES 
is required to obtain a message that altogether gives 2( | V| — 1) operations. In addition 
to the collect algorithm, the collect phase of the Lauritzen-Spiegelhalter architecture 
computes one division in each node. We therefore obtain 3(| V| — 1) operations that all 
take place on the node labels. To compute a message in the distribute phase, we again 
need one projection and, since messages are again combined to the node content, 
we end with an overall number of 5(| V| — 1) operations for the message-passing in 
the Lauritzen-Spiegelhalter architecture. Finally, query answering is simple and just 
requires a single projection per query. Thus, the time complexity of the Lauritzen-
Spiegelhalter architecture is: 
θ(\ν\·/(ω· 
+ 1)\ 
(4.15) 
In contrast to the Shenoy-Shafer architecture, messages do not need to be stored 
until the end of the complete message-passing, but can be discarded once they have 
been sent and divided out of the node content. But we again keep one factor in each 
node whose size is bounded by the node label. This gives us the following space 
complexity that is equal to the unimproved version of the collect algorithm: 
o(\V\-g(u>* + l)\ 
(4.16) 
The Lauritzen-Spiegelhalter architecture performs division during its collect phase. 
One can easily imagine that a similar effect can be achieved by delaying the execution 
of division to the distribute phase. This essentially is the idea of a further multi-query 
local computation scheme called HUGIN architecture. It is then possible to perform 
division on the separators between the join tree nodes that increases the performance 
of this operation. The drawback is that we again need to remember the collect 
messages until the end of the algorithm. 
4.4 THE HUGIN ARCHITECTURE 
The HUGIN architecture (Jensen et al., 1990) is a modification of Lauritzen-Spiegel-
halter to make the divisions less costly. It postpones division to the distribute phase 
such that the collect phase again corresponds to the collect algorithm with the only 
difference that every message μ»-^· is stored in the separator between the neighbor-
ing nodes г and j . These separators have the label sep(i) = X(i) П X(j) according to 
Definition 3.12. They can be seen as components between join tree nodes, comparable 
to the mailboxes in the Shenoy-Shafer architecture. A graphical illustration is shown 
in Figure 4.9. Subsequently, we denote by S the set of all separators. In the follow-
ing distribute phase, the messages are computed as in the Lauritzen-Spiegelhalter 
architecture, but have to pass through the separator lying between the sending and 
receiving nodes. The separator becomes activated by the crossing message and holds 
it back in order to divide out its current content. Finally, the modified message is 
delivered to the destination, and the original message is stored in the separator. 

THE HUGIN ARCHITECTURE 
129 
sep(i) > ~ 
-Ц ch(i) 
Figure 4.9 Separator in the HUGIN architecture. 
Formally, the message sent from node г to ch(i) during the collect phase is 
iwin\(ch(i)) 
Vi-^chii) 
= 
( Φί ® QQ Hj-+ch(i) 
J'€pa(t) 
This message is stored in the separator sep(i). In the distribute phase, node ch(i) 
then sends the message 
(
4 
isep(i) 
^chii)® (g) μ^ο4Δ 
= 
<t>
lsep{i) 
k£ne(i) 
J 
towards i. This message arrives at the separator where it is altered to 
The message pch(i)^i is ser,t to node г and combined with the node content. This 
formula also shows the advantage of the HUGIN architecture over the Lauritzen-
Spiegelhalter architecture. Divisions are performed in the separators exclusively and 
these have in general smaller labels than the join tree nodes. In equation (4.14) we 
have seen that the distribute messages in the Lauritzen-Spiegelhalter architecture 
correspond already to the projection of the objective function to the separator. This 
is also the case for the messages that are sent in the distribute phase of the HUGIN 
architecture and that are stored in the separators. However, the message emitted by 
the separator does not have this property anymore since division has been performed. 
Theorem 4.3 At the end of the HUGIN architecture, each node г G V stores φ^λ^ 
and every separator between i and j the projection φ^χΜηΧ^> provided that all 
messages during an execution of the Shenoy-Shafer architecture can be computed. 
The HUGIN algorithm imposes the identical restrictions on the valuation algebra 
as the Lauritzen-Spiegelhalter architecture and has furthermore been identified as an 
improvement of the latter that makes the division operation less costly. It is there-
fore not surprising that the above theorem underlies the same existential condition 
regarding the messages of the Shenoy-Shafer architecture as Theorem 4.2. The proof 
of this theorem can be found in Appendix D.2.2. 

130 
COMPUTING MULTIPLE QUERIES 
Algorithm 4.5 HUGIN Architecture: Message-Passing 
input: 
(V,E,X,D) 
output: 
-
begin 
for i = 1 . . . |V| - 1 do 
/ / C o l l e c t 
Phase: 
. _ 
,i,ld(U>i)nX(ch(i)) 
Pi->ch(i) 
■- Wi 
^5ер(г) 
: = Мг—ych{i) ' 
i>ch(i) 
■= i>ch(i) ® 
ßi^ch(i): 
end; 
for г = |V| — 1 . . . 1 do 
/ / D i s t r i b u t e 
Phase: 
</>; 4.A(i)nA(ch(i)) . 
ch(i) 
'Фвер(г) 
'■= Mt->c/i(i)' 
î/>i := Vi ®Mch(t)->i; 
end; 
end 
We again summarize this local computation scheme by a pseudo-code algorithm. 
Here, the factor фзер^) refers to the content of the separator between node i and ch(i). 
However, since both theorems 4.2 and 4.3 make equal statements about the result at 
the end of the message-passing, we can reuse Algorithm 4.4 for the query answering 
in the HUGIN architecture and just redefine the message-passing procedure: 
Example 4.5 We illustrate the HUGIN architecture in the same setting as Lauritzen-
Spiegelhalter architecture in Example 4.4. However, since the collect phase of the 
HUGIN architecture is identical to the generalized collect algorithm, we directly take 
the messages from Examples 3.20 and 3.22. The node contents after the collect phase 
corresponds to Example 4.4 but without the inverses of the messages. Instead, all 
messages sent during the collect phase are assumed to be stored in the separators. In 
the distribute phase, messages are sent from the root node down to the leaves. When a 
message traverses a separator that stores the upward message from the collect phase, 
the latter is divided out of the traversing message and the result is forwarded to the 
receiving node. Let us consider these forwarded messages in detail: 
• Node 6 obtains the message 
μ7->6 = 
(ψ7®μ6^7)ΗΒ'Ε}®μβ\7-
Note that the division was performed while traversing the separator node. 
Then, node 6 updates its content to 
/ , 
, 
- 1 
\i{B,E,L} 
= 
(ψ6 ® μ 5 ^ 6 ® ψ7 ® μβ->7 ® μ 6->7) 
The second equality follows from the combination axiom. For the remaining 
nodes, we only give the messages that are obtained in the distribute phase. 

THE IDEMPOTENT ARCHITECTURE 
131 
• Node 5 obtains the message 
μ6->5 = (^6 ® Мз->б ® M5-+6 ® /i 7^6) i { E' L } ® А%Лб 
• Node 4 obtains the message 
μ5_*4 — {Фь ® M2->5 ® А*4->5 ® M6->5)i{E} ® М4Д5 
• AWe 3 obtains the message 
μ6_>3 = (ψ6 <8> Мз^е ® M5^6 ® M7^6)i{S,i,} ® МзЛ6 
• AWe 2 obtains the message 
M5-»2 = (^5 ® M2->5 ® M4->-5 ® ße^5)i{E'L} 
® μ^Λδ 
• Noofe 1 obtains the message 
/Li2-H = (^2 ® Ml->2 ® ß5^2)i{T} 
® μΓ^2 
Ttoese messages consist of the messages from Example 4.4 from which the collect 
message is divided out. We therefore see that the division only takes place on the 
separator size and not on the node label itself. 
4.4.1 Complexity of the HUGIN Architecture 
Division is more efficient in the HUGIN architecture since it takes place on the 
separators which are generally smaller than the node labels. On the other hand, we 
additionally have to keep the inward messages in the memory that is not necessary 
in the Launtzen-Spiegelhalter architecture. Both changes do not affect the overall 
complexity such that HUGIN is subject to the same time and space complexity 
bounds as derived for the Lauritzen-Spiegelhalter architecture in Section 4.3.1. From 
this point of view, the HUGIN architecture is generally preferred over Lauritzen-
Spiegelhalter. An analysis for the time-space trade-off between the HUGIN and 
Shenoy-Shafer architecture based on Bayesian networks has been drawn by (Allen 
& Darwiche, 2003), and we again refer to (Lepar & Shenoy, 1998) where all three 
multi-query architectures are juxtaposed. We are now going to discuss one last local 
computation scheme for the solution of multi-query inference problems that applies to 
idempotent valuation algebras. In some sense, and this already foreshadows its proof, 
this scheme is identical to the Lauritzen-Spiegelhalter architecture where division is 
simply ignored because it has no effect in an idempotent valuation algebra. 
4.5 THE IDEMPOTENT ARCHITECTURE 
Section 4.2.1 introduces idempotency as a strong algebraic property under which 
division becomes trivial. Thus, the Lauritzen-Spiegelhalter as well as the HUGIN 

132 
COMPUTING MULTIPLE QUERIES 
architecture can both be applied to inference problems with factors from an idem-
potent valuation algebra. Clearly, if division in these architectures does not provoke 
any effect, we can ignore its execution altogether. This leads to the most simple local 
computation scheme called idempotent architecture (Kohlas, 2003). 
In the Lauritzen-Spiegelhalter architecture, the message sent from i to ch{i) during 
the collect phase is divided out of the node content. Since idempotent valuations are 
their own inverses, this reduces to a simple combination. Hence, instead of dividing 
out the emitted message, it is simply combined to the content of the sending node. 
By idempotency, this combination has no effect. Thus, consider the collect message 
J.WjnA(c/i(i)) 
μ»-*Λ(ί) = 
I Ψί ® (££) ßj-
\ 
jepo(i) 
This message is divided out of the node of i: 
Φί® (^) Mj->i ® AV-Lh(j) = Ψί® (S) ßj~>i®ßi^ch{l) 
jepa(i) 
jepa(i) 
luiiC[\(ch(i)) 
Фг ® QS) ßj^i 
® \фг ® Q£) ßj-
j'epa(i) 
/ 
\ 
j£pa(i) 
= 
Фг ® (££) ßj^i-
jepa(i) 
The last equality follows from idempotency. Consequently, the idempotent architec-
ture consists in the execution of the collect algorithm towards a previously fixed root 
node m followed by a simple distribute phase where incoming messages are com-
bined to the node content without any further action. In particular, no division needs to 
be performed. This derives the idempotent architecture from Lauritzen-Spiegelhalter, 
but it also follows in a similar way from the HUGIN architecture (Pouly, 2008). To 
sum it up, the architectures of Lauritzen-Spiegelhalter and HUGIN both simplify to 
the same scheme where no division needs to be done. 
Theorem 4.4 At the end of the idempotent architecture, node i contains φ^ι\ 
This theorem follows from the correctness of the Lauritzen-Spiegelhalter architec-
ture and the above considerations. Further, there is no need to condition the statement 
on the existence of the Shenoy-Shafer messages since idempotent valuation algebras 
are always regular. This is shown in Appendix D.1.3. For the pseudo-code algorithm, 
we may again restrict ourselves to the message-passing part and refer to Algorithm 
4.4 for the query answering procedure. 

THE IDEMPOTENT ARCHITECTURE 
133 
Algorithm 4.6 Idempotent Architecture: Message-Passing 
input: 
(V, E,\, D) 
output: 
-
begin 
for i = 1 ...\V\ - 1 do 
/ / C o l l e c t 
Phase: 
._ 
,hid(*i)n\(ch(i)) 
Фск(г) 
■ = Фск(г) ® ßi^ch(i) 
'" 
end; 
for г = \V\ — 1... 1 do 
/ / D i s t r i b u t e 
Phase: 
„ .,., 
. ·= ^,U(i)nA(ch(0). 
РсН(г)-и 
■ 
rch.(,i) 
' 
ti 
:= Tpi ®Mcft(i)-+<; 
end; 
end 
Example 4.6 If we again call in the same setting as for the Lauritzen-Spiegelhalter 
and HUGIN architecture in Example 4.4 and 4.5 for the illustration of the idempotent 
architecture, things become particularly simple (as it is often the case with the idem-
potent architecture). The collect phase executed the generalized collect algorithm 
whose messages can be found in Example 3.20 and 3.22. Then, the distribute phase 
just continues in exactly the same way: 
• Node 6 obtains the message 
μ7^6 = (ψΊ ® 
μ^7Ϋ{Β>Ε} 
and updates its node content to 
ψ6 ® μ5->.6 ® M7->6 
= 
"Ψβ ® μ5->6 ® (ψ7 ® ß6-i7) 
' 
= 
{Ψβ ® М5-и> ® ^ 7 <8> μ 6 ^ 7 ) 4 
= 
(</, 6® /z 5- + 6W { B' E' L } = ^ B- B' L>. 
77ге second equality follows from the combination axiom. Since μβ->7 corre-
sponds to a projection oftpe <g) μ5_>6 it follows from idempotency that 
V>6 ® μ5->6 ® μβ-)·7 
= 
^6 ® μ5^·6· 
77ι« explains the third equality. For the remaining nodes, we only give the 
messages that are obtained in the distribute phase. 
• Node 5 obtains the message 
μ6->5 — (V>6 ® μ3->6 ® μ5->6 ® μ7->6)4'{β"£'} 
• Afrwfe 4 obtains the message 
μ5^4 = (-05 ® μ2^5 ® μ4->5 ® μ6->δ)'1'{£} 

134 
COMPUTING MULTIPLE QUERIES 
• Node 3 obtains the message 
Мб->з = {Фб <8> Мз^б ® M5^6 ® 
ß7^e)^B'L) 
• Node 2 obtains the message 
• Node 1 obtains the message 
μ2->ι = (V>2 ® Mi->2 ® M5->2)i{T} 
Observe that no division has been performed anywhere. 
4.5.1 Complexity of the Idempotent Architecture 
The idempotent architecture is absolutely symmetric concerning the number of exe-
cuted operations. There are 2(| V| — 1) exchanged messages, thus 2(|V| — 1) projec-
tions and 2(| V| — 1) combinations to perform. No division is performed anywhere 
in this architecture. Further, each message can directly be discarded once it has 
been combined to the node content of its receiver. Thus, although we obtain the 
same bounds for the time and space complexity as in the Launtzen-Spiegelhalter 
and HUGIN architecture, the idempotent architecture can be considered as getting 
the best of both worlds. Since no division is performed, the idempotent architecture 
even outperforms the HUGIN scheme and does not need to remember any messages. 
Thus, if time is the more sensitive resource and idempotency holds, the idempo-
tent architecture becomes the best choice to solve multi-query inference problems 
by local computation. Moreover, if we are dealing with a valuation algebra where 
both complexities are polynomial, the idempotent architecture provides a specialized 
query answering procedure for queries that are not covered by the join tree. This 
procedure is presented in the following section. 
4.6 ANSWERING UNCOVERED QUERIES 
The concept of a covering join tree ensures that all queries from the original inference 
problem can be answered after the completed distributed phase. But if later a new 
query arrives that is not covered by any of the join tree nodes, it is generally necessary 
to construct a new join tree and to execute the local computation architecture from 
scratch. Alternatively, there are updating methods described in (Schneuwly, 2007) 
that modify the join tree to cover the new query. Then, only a part of the complete 
message-passing has to be re-processed. However, a third option that is especially 
suited for formalisms with a polynomial complexity exists in the idempotent case. 
This method is a consequence of an interesting property of the idempotent architecture 
that is concerned with a particular factorization of the objective function. Initially, 

ANSWERING UNCOVERED QUERIES 
135 
the original factorization of the objective function is given in the definition of the 
inference problem. Then, a second factorization is obtained from Lemma 3.8 with the 
property that the domain of each factor is covered by some specific node of the join 
tree. In case of idempotency, we obtain a third factorization of the objective function 
from the node contents after local computation: 
Lemma 4.4 In an idempotent valuation algebra we have 
r 
Φ = (g)^W(i)· 
С4·18) 
i=l 
Proof: The proof of this lemma is based on the correctness of the Lauritzen-
Spiegelhalter architecture, which can always be executed when idempotency holds. 
At the beginning, we clearly have 
r 
Φ = ®Vi-
During the collect phase, each node computes the message for its child node and 
divides it out of its node content. Then, the message is sent to the child node where 
it is combined to the node content. Thus, each message that is divided out of some 
node content is later combined to the content of another node. We therefore conclude 
that the above equation still holds at the end of the collect phase in the Lauritzen-
Spiegelhalter architecture. Formally, equation (4.13) defines the node content at the 
end of the collect phase in the Lauritzen-Spiegelhalter architecture and we thus have 
r - l 
Φ = 0 u ( r )®®$®0rÎ c h ( i ). 
t = l 
By idempotency it further holds that 
r - l 
Φ = 0®00 U ( i ) n^ C' l ( i ) ) 
i=l 
and therefore 
φ = 
Φ1λ(τ)®Τ(£)Φ^ηΧ^®®Ψΐ®μ^{ι) 
г = 1 
г = 1 
= Ф1Х{Г) ® 0 ti ® μ-^(ί) ® ^МОПА(СОД) . 
г = 1 
But the marginal of ф with respect to А(г) П X(ch(i)) corresponds exactly to the 
message sent from node ch(i) to node г during the distribute phase of the Lauritzen-
Spiegelhalter architecture, see equation (4.14). We therefore have by the correctness 

136 
COMPUTING MULTIPLE QUERIES 
of the Lauritzen-Spiegelhalter architecture 
r - l 
Φ = 
^ λ ( Γ )®®$®μΓ-ί<Λ(ο®^ λ ( 
г = 1 
As a consequence of this result, we may consider local computation as a trans-
formation of the join tree factorization of Lemma 3.8 into a new factorization where 
each factor corresponds to the projection of the objective function to some node label. 
The following theorem provides a further generalization of this statement. 
Theorem4.5 Forг — 
\,...,тand 
r 
Vi = | j A ( i ) 
(4-19) 
j=i 
it holds for an idempotent valuation algebra that 
r 
фЫ 
= 0^0). 
(4.20) 
j=i 
Proof: We proceed by induction: For г = 1 the statement follows from Lemma 4.4 
and from the domain axiom. For г + 1 we observe that yi+\ Ç yt and obtain using 
the induction hypothesis and the combination axiom 
(
r 
\ iy.+ i 
r 
<^λ(ί)® (g) φΙΧυ)) 
= ^ « ^ 1 ® (g φΙΧυ). 
From the running intersection property we further derive Л(г)ПЛ(с/г(г)) = Л(г)Пг/г+1 
and obtain by idempotency 
r 
фЫ+1 
_ 
0iA(i)nA(cA(i)) ^ ^(ch(i)) g, 
/ 0 \ 
ф±х(Л 
= ^(«л«))® 
(g) 
0i*O) = (g) ^ λ ω · 
j=i+l,j^ch(i) 
j=i+l 

ANSWERING UNCOVERED QUERIES 1 3 7 
We next observe that combining the node content of two neighboring nodes always 
results in the objective function projected to their common domain. 
Lemma 4.5 For 1 <i,j < r and j G ne(i) we have 
φΐχ(ί)υχα) 
= φΐχ(ί) g, ф1\и) _ 
Proof: We remark that if i and j are neighbors, then either i = ch(j) or j = ch(i). 
It is therefore sufficient to prove the following statement: 
ilX{i)UX{ch(i)) 
_ 
ф\.\(г) ф ф1\(сЫ{г)) _ 
(4.21
) 
It follows from Theorem 4.5, idempotency and the combination axiom that 
(
r 
\ J.A(t)UA(c/i(t)) 
I 
r 
ч 4A(i)UA(ch(i)) 
= 
I Φ
ίΗί) ® ^(
cfcW) ® (g) ^·
λω J 
(
r 
\ iyi+iD(X(i)UX{ch(i))) 
(g) <t>
iX{j) I 
j=i+l 
/ 
IW1 
i w / , r ^ 
/ I 
\4î/i+in(A(t)UA(cfc(«))) 
We further conclude from equation (4.19) that 
î/i+i П (λ(») U λ(<Λ(»))) = 
(№+ιηλ(ΐ))υ(ΐϋ+ιηλ((Α(ΐ))) 
= 
(λ(ι) П A(cft(i))) U Л(сЛ(»)) = λ(<Λ(»)) 
and finally obtain 
^A(»)UA(ch(i)) 
= 
ф1Х(г) ^ ф1Х{ск(г)) ^ Л ^ у . + Л 4 " ^ ^ ^ 
= 
0iMO 0 ^4.A(cfc(0) g, 0iA(cfc(i)) 
= 
^(Ο β^(Λ(ή). 
This lemma states that if a new query x Ç d(0) arrives that is not covered by the 
join tree, it can nevertheless be answered if we find two neighboring nodes whose 
union label covers the query, i.e. x Ç λ(ΐ) U X(j). We then compute 
φΐ* = (φΐχωυχωγ* = (φί*α)®φιχωγχ 
Thus, we neither need a new join tree nor a new propagation to answer this query. 
However, it is admittedly a rare case that two neighboring nodes can be found that 

138 
COMPUTING MULTIPLE QUERIES 
cover some new query. The following lemma therefore generalizes this procedure to 
paths connecting two arbitrary nodes in the join tree. 
Lemma 4.6 Let (pi,... ,pk) be a path from node p\ to node pk with pi G V for 
1 < г < к. We then have for an idempotent valuation algebra 
к 
ф1Х(Р1)и...и\(Рк) 
= 
(g)0U(Pi)_ 
( 4 2 2 ) 
i=\ 
Proof: We proceed by induction over the length of the path: For г = 2 we have 
Pi G пе(рг) and the statement follows from Lemma 4.5. For a path (pi,... 
,pk+i) 
we merge the nodes {pi,...,pk} 
to a single node called z to which all neighbors of 
Pi to pk are connected. The label of node z is \(z) — \(pi) U ... U \{pk)- Clearly, 
the running intersection property is still satisfied in the new tree. Since pk+i & ne{z) 
we may again apply Lemma 4.5 and obtain, 
фЩг)и\(рк 
+ 1 ) 
= 
φΙΧ(ζ) ф ф1\(рк+1) 
= 
(φίΚΡι) 
® ._ _ ^ ф^(Рк)) 
g, 
φΙΗΡκ+ι) 
The last equality follows from the induction hypothesis. 
■ 
It then follows immediately from this lemma and the transitivity of projection that 
/ к 
\ U(pi)UA(PAi) 
ф1Х(Р1)иХ(Рк) 
= 
j ( g ) 0 ^ ( P i ) j 
(4.23) 
This allows us to express the following yet rather inefficient procedure to answer a 
query x С ά(φ) that is covered by the union label of two arbitrary join tree nodes: 
1. Find two nodes i,j&V 
such that x Ç \(i) U X(j). 
2. Combine all node contents on the unique path between г and j . 
3. Project the result to x. 
We here assume for simplicity that the query is split over only two nodes. If more 
than two nodes are necessary to cover the query, it is easily possible to generalize 
Lemma 4.6 to arbitrary subtrees of the join tree instead of only paths. Clearly, this 
simple procedure is very inefficient and almost amounts to a naive computation of the 
objective function itself, due to the combination of all node factors on the path between 
i and j in the second step. However, there is a more intelligent way to perform these 
computations that only requires combining the content of two neighboring nodes. 
This is the statement of the following lemma: 
Lemma 4.7 In an idempotent valuation algebra, it holds that 
ф1Х(Р1)иХ(Рк) 
_ 
ф1Х{Рк) φ0±λ( Ρ ι)υ(λ(ρ*_ι)ηλ(ρ*))_ 

ANSWERING UNCOVERED QUERIES 
139 
Proof: It follows from the transitivity axiom, Lemma 4.6, the combination axiom 
and the running intersection property that 
ф1\{Р1)и\(Рк) 
_ 
Λ|λ(Ρι)υ...υλ(Ρί;Λ l\(Pl)u\(Pk) 
4.À(pi)UA(pfc) 
_ 
ίφΙΗρι*) φ04·λ(ρι)υ...υλ(ρ*_ιΛ 
= 
ф^Рь) 0 0-1-(л(Р1)и-иЛ(Р*-1))п(л(Р1)иА(рк)) 
= 
ф±Х(Рь) (g) ^4-A(pi)U(A(pfc_i)nA(pfc))_ 
We now give an algorithm similar to the above procedure that exploits Lemma 4.7 
to avoid the combination of all node factors on the path between node p\ and p^. 
Algorithm 4.7 Specialized Query Answering 
input: 
(V,E,X,D), 
pi,Pk&V, 
x Ç λ(ρι) U X(pk) 
output: 
ήλχ 
begin 
n := φίΗνι) ■ 
find a path 
(pi,...,Pfc); 
for г = 1... к — 1 do 
j ; 
: = <£4.A(pi + i) ®^4-А(Р1)и(А(р4)пА(Р; + 1 ) ) . 
end; 
return rr^x ; 
end 
Theorem 4.6 Algorithm 4.7outputs η^χ = ф^хргх 
С λ(ρι) U λ(ρ^). 
Proof: Let us first show that the projection of η in the loop is well-defined. After 
repetition г of the loop we have: 
ά(η) = Х(Рг+1) U \{Pl) U (\{pi) П X(pi+1)) = λ(ρι)υλ(ρ 4 +ι). 
(4.24) 
In loop г + 1, ту is projected to A(pi) U (λ(ρί+ι) D A(pj+2)) ç λ(ρι) U λ(ρί+1). 
Hence, this projection is feasible. We next prove that 
V 
j4-A(pi)uA(pfc) 
( 4 . 2 5 ) 
holds at the end of the loop. The statement of the theorem then follows by the 
transitivity of projection. For к — 2 it follows from Lemma 4.6 that 
^lA(Pl)UA(p2) 
= 
φΙΗΡι) g, φΙΗΡι) 
= 
φιχ<·Ρ2) 
® η = φ^Ρ^ 
(^7?-1·λ(Ρι)υ(λ(Ρι)ηλ(Ρ2)). 
The second equality follows by initialization and the third from the domain axiom. 
This proves the correctness of the algorithm for paths of length 2. We now assume 

140 
COMPUTING MULTIPLE QUERIES 
that equation (4.25) holds for paths of length k. For a path of length к + 1, the last 
repetition of the loop computes 
, , , „ 
ч 
/ 
,w„ s, ,w„ x\U(pi)U(A(pfc)nA(Pfc + l)) 
= 
фЩРк+ι) (g, 04-A(pi)U(A(pfc)nA(pfc+i)) 
_ 
ф1НР1)иЦрк+1) 
The first equality follows from the induction hypothesis, the second from transitivity 
and the third from lemma 4.7. This proves the correctness of equation (4.25 and the 
statement of the theorem follows by the transitivity of projection. 
■ 
Example 4.7 We want to answer the new query x = {A, B} in the join tree of Figure 
4.3 after a complete run of the idempotent architecture. We immediately see that this 
query is not covered by the join tree, but it is for example covered by the union label 
of the two nodes 1 and6, i.e. x Ç λ(1) U A(6) = {A, B, E, L, T}. The path between 
these two nodes has already been shown in Figure 4.10. The following three steps 
are executed by Algorithm 4.7: 
Step 1: 
0+λωυλ(2) = ^λ(2) Θ ^|λ(ΐ) 
Step 2: 
^λ(ΐ)υλ(5) 
= ^λ(5) ^ ^ΐλ(ΐ)υ(λ(2)ηλ(5)) 
Step 3: 
04·λ(ΐ)υλ(6) = ^ ( 6 ) ^ ^4·λ(ΐ)υ(λ(5)ηλ(6)) 
Finally, the algorithm returns 
/ 0±λ(1)υλ(6) j 
= 
1ф1{А,В,Е,Ь,Т}\ 
= фНА,В}_ 
Figure 4.10 The path between node 1 and node 6 in the join tree of Figure 4.3. 
A closer look on Algorithm 4.7 or more precisely on equation (4.24) shows that 
we may extract a lot more information from the intermediate results 
77 = 
0-Ι·λ(ρι)υλ(Ρί) 
for 1 < г < к. In fact, it is not only possible to compute the final query x С 
A(pi) U X(pk) as stated in Theorem 4.6, but we actually obtain all possible queries 

ANSWERING UNCOVERED QUERIES 
141 
that are subsets of A(pi ) U X{pi) by just one additional projection per query. This is 
the statement of the following complement to Theorem 4.6. 
Lemma 4.8 Let (pi,... ,Pk) be a path from node p\ to node pk- Algorithm 4.7 can 
be adapted to compute ήλχ for all x С A (pi ) U λ (pi ) for i = 2,..., к. 
Example 4.8 From the intermediate results of Example 4.7 we may compute all 
possible queries that are subsets of {A, T, E, L, T] and {A, T, B, E, L} by just 
one additional projection per query. This includes in particular the binary queries 
{A, E}, {A, L} and {T, B} which are not covered by the join tree in Figure 4.3. 
A further extension of Algorithm 4.7 provides for a previously fixed node i G V 
a pre-compilation of the join tree such that all possible queries, that are subsets of 
А(г) U X(j) for all nodes j € V, can be computed by just one additional projection 
per query. To do so, we start with the selected node г and compute for each neighbor 
к £ ne(i) the projection of ф to А(г) U A(fc). This intermediate result is stored in the 
neighbor node k. Then, the process continues for all neighbors of node к that have not 
yet been considered. If l € ne(k) is such a neighbor of k, we compute the projection 
to А(г) U X(l) by reusing the intermediate result stored in k. This is possible due to 
Lemma 4.7: 
φΙ\(ί)υ\(1) 
= 
(^4.A(i)(g)^4A(t)U(A(fc)nA(0)_ 
At the end of this process, each node j e V contains ^ л( г) и Ли) which allows us 
to answer all queries that are subsets of А(г) U A(j) by just one additional projec-
tion. These projections setup another factorization of the objective function ф as a 
consequence of Lemma 4.6. 
Algorithm 4.8 Compilation for Specialized Query Answering 
input: 
(V,E,X,D), 
ieV 
output: 
-
begin 
m := φ±χΜ; 
for each к б пе(г) do 
v i s i t ( i , 
j ) ; 
end; 
end 
function v i s i t (k, 1 ) 
begin 
m 
:= ^MO e i JfWu(A(fc)nA(0). 
for each j € ne(fc) — {1} do 
v i s i t (1,j) ; 
end; 
end 
4.6.1 The Complexity of Answering Uncovered Queries 
We first point out that finding the path between two nodes in a directed tree is 
particularly easy. We simply compute the intersection of the two paths that connect 

142 
COMPUTING MULTIPLE QUERIES 
both nodes with the root node. Due to equation (4.24) the loop statement of Algorithm 
4.7 computes a factor of domain \(pi) U \(pi). Its number of repetitions is bounded 
by the longest possible path in the join tree, which has at most | V| nodes. We therefore 
conclude that Algorithm 4.7 adopts a time complexity of 
o(\V\-f(2{w* + lj)\ 
(4.26) 
Since only one combination of two node contents is kept in memory at every time, 
we obtain for the space complexity 
o(g(2(w' + ljj\. 
(4.27) 
Algorithm 4.8 combines the node content of each node with the content of one of its 
neighbors. This clearly results in the same time complexity. But in contrast, we store 
one such factor in each node, which gives us a space complexity of 
θ(\ν\-9(2(ω* + 1))λ 
(4.28) 
for Algorithm 4.8. At first glance, the complexities of these algorithms seem very bad 
because they are controlled by the double of the treewidth. Especially for formalism 
with exponential time or space behaviour, it could therefore be more efficient to 
construct a new join tree that also covers the new queries and execute the complete 
local computation process from scratch. But whenever we add new queries to the 
inference problem, we risk obtaining a join tree with a larger treewidth. This makes 
it difficult to directly compare the two approaches. The definition of a covering join 
tree ensures that every query is covered by the label of a join tree node. Here, we 
only require that every query is covered by the union of two arbitrary node labels. 
We will see in Chapter 9 that important applications exist where this is always sat-
isfied. Imagine for example that our query set consists of all possible queries of two 
variables {X, Y} with X, Y e ά(φ) and X φ Y. Building a covering join tree for 
such an inference problem always leads a join tree whose largest node label contains 
all variables. We then compute the objective function directly and the effect of local 
computation disappears. On the other hand, it is always guaranteed that such queries 
are covered by the union label of two join tree nodes. We can therefore ignore the 
query set in the join tree construction process and obtain the best possible join tree 
that can be found for the current knowledgebase. Later, queries are answered by 
the above procedures which clearly is more efficient than computing the objective 
function directly. Chapter 9 shows that such particular query sets frequently occur 
when local computation is used for the solution of path problems in sparse networks. 
Then, the query {X, Y} for example represents the shortest distance between two 
network hosts X and Y, and the query set of all possible pairs of variables models 
the so-called all-pairs shortest path problem. 

SCALING AND NORMALIZATION 
143 
We have now seen several important concepts that are all related to a division 
operator in the valuation algebra: namely, two local computation architectures that 
directly exploit division and the particularly simple idempotent architecture, including 
a specialized procedure to answer uncovered queries. In addition, the presence of 
inverse elements allows us to introduce the notion of scaling or normalization on an 
algebraic level and also to derive variations of local computation architectures that 
directly compute scaled results. This is the subject of the next section. 
4.7 SCALING AND NORMALIZATION 
Scaling or normalization is an important notion in a couple of valuation algebra 
instances. If, for example, we want to use arithmetic potentials to represent discrete 
probability distributions, it is semantically important that all values sum up to one. 
Only in this case are we allowed to talk about probabilities. Similarly, normalized set 
potentials from Instance 1.4 adopt the semantics of belie/functions (Shafer, 1976). As 
mentioned above, the algebraic background for the introduction of a generic scaling 
operator is a separative valuation algebra. It is shown in this section how scaling 
can be introduced potentially in every valuation algebra that fulfills this mathemati-
cal property. Nevertheless, we emphasize that there must also be a semantical reason 
that demands a scaling operator, and this cannot be treated on a purely algebraic level. 
Following (Kohlas, 2003), we define the scaling or normalization of φ € Φ by 
φ± = ф® (У 0) 
. 
(4.29) 
It is important to note that because separativity is assumed, scaled valuations do not 
necessarily belong to Ф but only to the separative embedding Ф*. For simplicity and 
because this holds for all instances presented in this book, we subsequently assume 
that φ^ G Φ for all φ G Φ. This ensures that all projections of φ^ to t Ç ά{φ) are 
defined, although we are dealing with a valuation algebra with partial projection. We 
refer to (Kohlas, 2003) for a more general study of scaling without this additional 
assumption. However, to gain further insights into scalable valuation algebras, we 
require some results derived for separative valuation algebras in Appendix D.l.l. 
This is necessary to obtain the following equation and for the proof of Lemma 4.9. 
Nevertheless, it is possible to understand these statements without knowledge of 
separative valuation algebras. 
Combining both sides of equation (4.29) with φ^ leads to 
φ 
= φχ®φ^ 
(4.30) 
according to the theory of separative valuation algebras. A valuation and its scale 
are contained in the same group. The following lemma lists some further elementary 
properties of scaling. It is important to note that these properties only apply if the 

144 
COMPUTING MULTIPLE QUERIES 
scale φ^ of a valuation ф is again contained in the original algebra Ф. This is required 
since scaling is only denned for elements in Ф. 
Lemma 4.9 
1. For ф G Ф with φ^ G Φ we have 
(Φ1Ϋ 
= Φ1. 
(4.31) 
2. For φ, φ G Φ with φ^, φ^ G Φ we have 
{Φ®ΨΫ 
= (φι®-ψ1)1. 
(4.32) 
3. For φ G Φ with ^ ε Φ and t Ç ά(φ) we have 
№)U 
= (φ»γ. 
(4.33) 
The first property of Lemma 4.9 states that scaling an already scaled valuation 
has no effect. Second, the combination of scaled valuations does not generally lead 
to a scaled valuation anymore. But computing a scaled combination does not require 
scaling the factors of the combination. Finally, the third statement states that projec-
tion and scaling are interchangeable. The proof can be found an Appendix D.3 
In a separative valuation algebra with null elements, every null element zs forms 
itself a group that is a consequence of cancellativity. Therefore, null elements are 
inverse to themselves and consequently 
4 = zs®(zf) 
= zs®z^1 
= zs<g>Z0 = zs. 
(4.34) 
If the separative valuation algebra has neutral elements according to Section 3.3, 
their scale is generally not a neutral element anymore. This property however is 
guaranteed if the valuation algebra is stable. It follows from (4.30) that 
es = ej®ef 
= e£ <g> e0 = el
s. 
(4.35) 
Let Φ^ be the set of all scaled valuations, i.e. 
Φ4- = 
{φι :φ£ Φ}. 
(4.36) 
The operation of labeling is well-defined in Φ^. If we assume furthermore that all 
scales are element of Φ, i.e. Φ^ С Ф, we know from Lemma 4.9 that Φ·*· is also closed 
under projection. However, since a combination of scaled valuations does generally 
not yield a scaled valuation anymore, we define 
^ φ ^ 
= 
{Φ1®Ψ1)1- 
(4.37) 
Note that Lemma 4.9 also implies 
ф±®ф1 
= 
(φ®ψ)1. 
(4.38) 

SCALING AND NORMALIZATION 
145 
The requirement that all scales are elements of Φ holds in particular if the valuation 
algebra is regular. If even idempotency holds, every valuation is the scale of itself, 
φ± = φ®(φ^ 
=φ®(φιί>} 
= φ. 
(4.39) 
Consequently, we have Φ^ = Φ in case of idempotency. 
Theorem 4.7 Assume Φ^ С Ф. Then, (Φ^,Ό) with the operations of labeling d, 
combination Θ, and projection 4. is a valuation algebra. 
Proof: We verify the axioms from Section 1.1 on (Φ+, D): 
(Al) Commutative Semigroup: Commutativity of θ follows directly from (4.37). 
Associativity is derived as follows: 
{φι®ψι)®χι 
= (φ®φ)ιφχι 
= {{φ ® ψ) ® χ)4-
= 
(Φ®{Φ®χ))1 = 
ΦιΦ{Φ®χ)1 
= 
φι®{ψι@χι). 
(Α2) Labeling: Since a valuation and its scale both have the same domain, 
ά{φί@ψχ) 
= а{(ф1®ф^) 
= а(ф±®ф1) = 
ά(φ+)οά(ψι). 
(A3) Projection: For φ^ G Φ^ with ά(φ^) = x we have 
ά({φ^χ) 
= ά{{φ^χγ) 
= ά{φ1χ) = χ. 
(A4) Transitivity: For у С х С ά(φ^) it follows that 
((Φ±)ίχγν = ((φίχ)ιγυ 
= ({Φ1χ)ίυγ 
= (Ф1у)1 = {Ф1)Ху. 
(А5) Combination: For φ^, ψ^ € Φ^ with ά(φ^) = χ, ά{φ^) = у and ζ G D such 
that x Ç z Ç x и у, we have 
{φ±®ψψζ 
= {{ф®ф)1)и 
= 
((Φ®Φ)1ζ)1 
= 
(ф®ф1уПг)± 
= φί®{ψ^ηζγ 
= φ^ (В 
{φι)1νΠζ. 
(Α6) Domain: For φ^ £ Φ^ and ά(φ^) = χ, 
(φ^χ = [φ^γ = φί. 
m 
Accepting the requirement that Φ^ С Ф, the mapping ф н-> φ^ is a homomorphism 
from (Φ, D) to (Φ^, D). The latter is called scaled valuation algebra associated with 
(Φ, D). Finally, in a valuation algebra (Φ', D) with adjoined identity element e, this 
element is not affected from scaling. From e = e _ 1 follows that 
e^ — e® i e i 0 J 
= e ® e~l = 
e®e 
= e. 
(4.40) 
Let us now consider two examples of separative valuation algebras with a seman-
tical requirement for scaling. A third example can be found in Appendix D.3 of this 
chapter and in Appendix E.5 of Chapter 5. 

146 
COMPUTING MULTIPLE QUERIES 
4.4 Probability Potentials 
Normalized arithmetic potentials often take a preferred position because they 
adopt the interpretation of discrete probability distributions. An arithmetic 
potential p with domain s is normalized if its values sum up to one, i.e. if 
$>w 
1. 
xef2a 
This is achieved by applying the generic scaling operation of Definition 4.29: 
p(x) 
/ ( x ) 
= ρ ( χ ) · ( ^ 0 ) _ 1 ( ο ) 
= 
Σ ν 6Ω.ρ(γ)' 
(4.41) 
We illustrate this process by applying the scaling operation to the arithmetic 
potential рз from Instance 1.3. Since p\ (o) = 2 we obtain 
Рз 
A 
a 
a 
a 
a 
ä 
a 
ö 
ö 
В 
b 
b 
b 
b 
b 
b 
b 
b 
с 
с 
с 
с 
с 
с 
с 
с 
с 
0.12 
0.48 
0.36 
0.04 
0.06 
0.24 
0.63 
0.07 
1 
Рз 
А 
а 
а 
а 
а 
о 
а 
а 
а 
В 
b 
b 
b 
b 
b 
b 
b 
b 
с 
с 
с 
с 
с 
с 
с 
с 
с 
0.060 
0.240 
0.180 
0.020 
0.030 
0.120 
0.315 
0.035 
4.5 Probability Density Functions 
Continuous probability distributions are a particularly important class of den-
sity functions and motivate our interest in normalized densities. According to 
equation (4.29), the scale of a density function / € Ф with d(f) = s is 
for x e Qs. Consequently, we have for a scaled density 
/ ^ χ ^ χ 
(4.42) 
/ · 
1. 
Obtaining scaled results from the computation of an inference problem is an 
essential requirement for many applications. We may deduce from the second property 
of Lemma 4.9 that the result of computing a query of an inference problem will not 
necessarily lead to a scaled valuation again, even though the knowledgebase of the 

LOCAL COMPUTATION WITH SCALING 
147 
inference problem consists of only scaled valuations. This problem will be analyzed 
in the following section. To do so, we should also be aware of the computational 
effort of executing a scaling operation on some valuation φ € Φ*. We know from 
equation (4.29) that the scale of φ is obtained by combining φ with the inverse of its 
projection to the empty domain. Scaling therefore requires executing one projection 
and one combination. The inverse is computed on the empty domain for all valuations 
and can therefore be neglected, i.e. this effort is constant for all valuations. 
4.8 LOCAL COMPUTATION WITH SCALING 
Let us now focus on the task of computing scaled queries {χχ,... ,xs} from a 
knowledgebase {φι,..., 
φη} where Xi Ç ά{φ) and φ — φχ ® ... ® φη. We derive 
from Lemma 4.9 that 
(Ф^У 
= (ф1УХг = 
((0l®···®<^>m)i)iж, 
= 
((Φ\®---®Φΐ)ι)ίχ' 
= 
(Φί®---®Φΐ)ΙΧ\ 
This derivation provides different possibilities. The first expression is the straightfor-
ward approach: we compute each query using some local computation architecture 
and perform a scaling operation on the result. This requires \x\ scaling operations, 
thus |ж| combinations but only one projection since the inverse of the objective func-
tion projected to the empty domain is identical for all queries. The second expression 
performs only a single scaling operation on the total objective function. However, we 
directly refuse this approach since it requires explicitly building the objective func-
tion ф. A third approach follows from the last expression if we additionally suppose 
that all scales are contained in Ф, i.e. that Ф+ С Ф. We then know from Theorem 
4.7 that we may directly compute in the valuation algebra (Φ^, D). However, this is 
again worse than the straightforward approach since it executes one scaling operation 
per combination during the local computation process as a result of equation (4.38). 
A far more promising approach arises if we integrate scaling directly into the 
local computation architectures. Since the underlying valuation algebra is assumed 
to be separative, we can potentially use the Shenoy-Shafer, Lauritzen-Spiegelhalter 
or HUGIN architecture. Furthermore, if the valuation algebra is idempotent, we do 
not need to care about scaling at all because idempotent valuations are always scaled. 
Thus, we are next going to reconsider the three architectures above and show that 
only a single execution of scaling in the root node of the join tree is sufficient in each 
of them to scale all query answers. This is well-known in the context of Bayesian 
networks, and for general valuation algebras it was shown by (Kohlas, 2003). 
4.8.1 The Scaled Shenoy-Shafer Architecture 
The scaled Shenoy-Shafer architecture first executes the collect phase so that the 
projection of the objective function φ to the root node label A(r) can be computed in 

148 
COMPUTING MULTIPLE QUERIES 
the root node r € V. Then, the root content ψτ is replaced by 
^ « ( V 0 ) " 1 = Vv®((0-u(r))ie)~1. 
(4.43) 
When the distribute phase starts, the outgoing messages from the root node are 
computed from this modified node content. The result of this process is stated in the 
following theorem. 
Theorem 4.8 At the end of the message-passing in the scaled Shenoy-Shafer archi-
tecture, we obtain at node i 
(<f>Wi)Y = H U W 
=lfc® ® 
μ ^ , 
(4.44) 
jene{i) 
where μί·_^ are the modified messages from the scaled architecture. 
Proof: The messages of the collect phase do not change, i.e. ßi^ch(i) = М*_»сш)· 
For the distribute messages, we show that 
* 4 ( 0 - i 
= 
Vch(i)^i® 
( V 0 ) 
· 
(4.45) 
The distribute message sent by the root node to parent г is 
±ωΓ-,ί!~ιλ(ί) 
ß'r^i 
= 
| ^ r ® ( ^ i f l ) 
' ® 
(g) 
ßj-
y 
jene(r),jjii 
и-. 
j€ne(r),j^i 
This follows from the combination axiom. Hence, equation (4.45) holds for all 
messages that are emitted by the root node. We proceed by induction and assume that 
the proposition holds for μ0Η(ί)-*ί· Then, by application of the combination axiom, 

LOCAL COMPUTATION WITH SCALING 
149 
we have for all j e pa(i) 
μ Î->J 
iwi-tjCWij) 
ß'k-
kEne(i),k^j 
4.ωί^-ηλθ) 
Фг ® Vch(i)^i ® 
0 
^-
k€pa(i),kjij 
Фг ® Mch(i)-»i ® (^40) 
® 
^ 9 
μ*_ 
кС.ра(г),кф] 
.Цл-^ПАС?) 
μk 
k£ne(i) 
,k^j 
Ι ω ^ Π λ Ο ' ) 
V>i ® 
(££) Mfe 
By Theorem 4.1 we finally conclude that 
O
i0)~ 
Mi->j 
j6ne(i) 
j6pa(i) 
w 
jepa(t) 
= Vi ® 0 
μ,--* ® (V0) 
jGrae(i) 
To sum it up, the execution of one single scaling operation in the root node 
scales all projections of the objective function in the Shenoy-Shafer architecture. 
We summarize this modified Shenoy-Shafer architecture in terms of a pseudo-code 
algorithm. Since the modification only concerns the message-passing, only Algorithm 
4.1 must be adapted. 
Algorithm 4.9 Scaled Shenoy-Shafer Architecture: Message-Passing 
input: 
(V,E,\,D) 
output: 
-
begin 
for i = l . . . | V | - 1 do 
ω := 
d(il>i)U\Jjepa(i)d(ßj^i); 
II 
C o l l e c t 
Phase: 

150 
COMPUTING MULTIPLE QUERIES 
Hi-tchii) := (Фг®®^^^!^)1""40^; 
II in mailbox 
end; 
φ^ := Wr8® J (: p„( r)MH r) Wi 
/ / Scaling: 
■фг := t/>r ® W)-1 
; 
for г = \V\ — 1... 1 do 
/ / Distribute Phase: 
ω := (iWOuUjenefcMiDAjïé.^i^)'· 
ßch(i)^i ■= (i>ch(i)®®je„e(ch(i))Aj^iN^i)l'J'nX(Î)! 
Il i n mailbox 
end; 
end 
4.8.2 The Scaled Lauritzen-Spiegelhalter Architecture 
The Lauritzen-Spiegelhalter architecture first executes the collect algorithm with the 
extension that emitted messages are divided out of the node content. At the end 
of this collect phase, the root node directly contains the projection of the objective 
function φ to the root node label X(r). We pursue a similar strategy as in the scaled 
Shenoy-Shafer architecture and replace the root content by its scale 
(^i)U(r) 
= 
фШг) ® ίφΙ*Υ\ 
(4.46) 
Then, the distribute phase proceeds as usually. 
Theorem 4.9 At the end of the scaled Lauritzen-Spiegelhalter architecture, node i 
contains 
(<f>i4i))l = (φ^(ί) 
provided that all messages during an execution of the 
Shenoy-Shafer architecture can be computed. 
Proof: By equation (4.46), the theorem is satisfied for the root node. We proceed by 
induction and assume that the proposition holds for the node ch{i) which therefore 
contains (ф^х(сН(1)). 
The content of node г in turn is given by equation (4.13). 
Applying Theorem 4.2 gives 
= iï ® /vU(o ® ΦιΗί)ηΧ{οΗ{ί)) ® (Ф^у1 
Again, a single execution of scaling in the root node scales all results in the 
Lauritzen-Spiegelhalter architecture. We again give the required modification of Al-
gorithm 4.3 explicitly: 

LOCAL COMPUTATION WITH SCALING 
151 
Algorithm 4.10 Scaled Lauritzen-Spiegelhalter Architecture: Message-Passing 
input: 
(V,E,\,D) 
output: 
-
begin 
for г = l . . . | V | - 1 do 
/ / Collect Phase: 
._ 
,ld(tl>i)n\(ch(i)) 
ßi-,ch(i) 
■- 4>i 
Фск(г) 
■■= ^ch(i)® 
ßi^ch(i)> 
Фг := ^®μ,~4 ε Μ 0; 
end; 
фг := фг ® (ι/>^0)-1; 
// Scaling: 
for i = \V\ - 1... 1 do 
// Distribute Phase: 
._ ,,,lA(i)nÀ(ch(i)). 
Фг ·■= 
Φί®μεΗ(ί)^ϊ; 
end; 
end 
4.8.3 The Scaled HUGIN Architecture 
Scaling in the HUGIN architecture is equal to Lauritzen-Spiegelhalter. We again 
execute the collect phase and modify the root content according to equation (4.46). 
Then, the outward phase starts in the usual way. 
Theorem 4.10 At the end of the computations in the HUGIN architecture, each node 
г G V stores (φ^χ^Υ 
= (φ^)^χ^ 
and every separator between i and j the projection 
(ф1Чг)пЧз)^1 = (φΙ~·)14ί)<~ιλϋ) provided that all messages during an execution of 
the Shenoy-Shafer architecture can be computed. 
Proof: By equation (4.46), the theorem is satisfied for the root node. We proceed by 
induction and assume that the proposition holds for node ch(i). Then, the separator 
lying between i and ch(i) will update its stored content to 
Node i contains φ^ when receiving the forwarded separator content and computes 
ψ 0 04д(опА(с/ко) Θ (>0) - 1 g, μ-^Ηι) 
= 
Once more, a single execution of scaling in the root node scales all results in the 
HUGIN architecture. It follows the corresponding adaption of Algorithm 4.5. 

152 
COMPUTING MULTIPLE QUERIES 
Algorithm 4.11 Scaled HUGIN Architecture: Message-Passing 
input: 
(V,E,\,D) 
output: 
-
begin 
for i = l...\V\ 
- 1 do 
. _ 
Л<Щ,г)П\(сЬ.(г)) 
H'i—ychii) 
' 
Ψχ 
4>sep(i) 
'■= 
ßi-tchl^i)! 
Фс)г(г) 
■ = i>ch(i) ® Mi->ch(i) ; 
end; 
i>r 
:= 
Ψ Γ ® ( ^ 0 ) _ 1 ; 
for г = |V| - 1...1 
do 
„' 
._ ,/,4.>(<)nA(ch(i)) . 
ßch(i)^i 
■= Mch(i)->i ® ^sep(i) ; 
Vsep(i) 
'■= 
^i^ch(i)' 
■фг := 4>i <8^с/г(г)-И'' 
end; 
end 
// Collect Phase: 
// Scaling: 
/ / Distribute Phase: 
These adaptions for scaling close the discussion of local computation. Further local 
computation schemes but for an extended computational task will be introduced in 
Chapters 9 and 8. 
4.9 
CONCLUSION 
The topic of this chapter was the solution of inference problems where multiple 
queries have to be computed. We quickly remarked that a repeated application of the 
generalized collect algorithm for each query leads to a lot of redundant computations. 
On the other hand, a more detailed analysis has shown that two such runs differ only 
in the messages sent between the two root nodes. This fact is exploited by the Shenoy-
Shafer architecture that solves a multi-query inference problem by only the double 
number of messages used for the computation of a single query. In addition, since 
the Shenoy-Shafer architecture stores messages in mailboxes instead of combining 
them to the node content, we obtain the best possible space complexity one can 
expect for a general local computation scheme. However, the prize we have to pay 
is that a lot of redundant operations are still performed, unless particular join trees 
such as binary join trees are used. This was the key motivation for the introduction 
of further local computation procedure with a more sophisticated caching policy. 
These architectures benefit from a division operation that is in general not present 
in a valuation algebra. We therefore discussed three different, algebraic properties in 
the appendix of this chapter that allow the introduction of division: if the valuation 
algebra is separative, it can be embedded into a union of groups which naturally 
provide inverse elements. This is the most general property and leads to an extension 
of the original valuation algebra where division is possible but where projection is 
only partially defined. Alternatively, if the valuation algebra is regular, it decomposes 

APPENDIX 
153 
directly into a union of groups and full projection is conserved. The third and strongest 
property is idempotency. In this case, every element is the inverse of itself and 
division becomes trivial. Back to local computation, we developed two specialized 
architectures that apply to separative (and hence regular) valuation algebras. The 
Lauritzen-Spiegelhalter architecture executes the collect algorithm in its inward phase 
but always divides the message for the child node out of each node content. This 
ensures that no information is combined a second time when the distribute message 
arrives. Here, the division takes place on the node content and all messages can be 
discarded as soon as they have been combined to some node content. On the other 
hand, we need to store one factor per node whose size is bounded by the node label. 
Thus, we avoid the redundant computations of the Shenoy-Shafer architecture to 
the expense of a considerably higher space requirement. The second scheme called 
HUGIN architecture delays division to the distribute phase where it only takes place 
on the separators. This makes division less costly but requires to store the collect 
messages until the end of the message-passing. A fourth and last local computation 
scheme called idempotent architecture can be applied if idempotency holds in the 
valuation algebra. Essentially, it is equal to the Lauritzen-Spiegelhalter architecture 
but passes on the execution of trivial division. Thus, no additional effort is spent in 
division and no messages must be cached. The very strong property of idempotency 
further enables a specialized procedure to answer queries which are not covered by 
the join tree. This procedure becomes especially worthwhile when a large number of 
queries has to be computed. Finally, we pointed out that scaling or normalization is an 
important issue for many applications. From the algebraic point of view, we propose 
a generic definition of scaling when division is present, i.e. if the valuation algebra is 
at least separative. However, there must also be a semantical motivation for a scaling 
operation in a valuation algebra which cannot be justified on a purely algebraic level. 
If this requirement exists, we are naturally interested in computing scaled queries 
and it turned out that a slight modification of local computation architectures allows 
computing an arbitrary number of scaled queries with only one single execution of 
the scaling operation in the root node of the join tree. Figure 4.11 summarizes the 
local computation procedures and their algebraic requirements. 
Appendix: Valuation Algebras with Division 
In the first part of the appendix, we will study under which conditions valuation 
algebras provide a division operation. It has already been mentioned that the pure 
existence of inverse elements according to Definition 4.1 is not sufficient since this 
only refers to combination. But in order to apply division in local computation archi-
tectures, we also need a property for the projection operation. However, the definition 
of inverse elements is nevertheless a good starting point for the understanding of di-
vision in the context of valuation algebras, and it turns out that all properties studied 
below imply their existence. (Kohlas, 2003) distinguishes three different cases: Sep-
arativity is the weakest requirement. It allows the valuation algebra to be embedded 
into a union of groups that naturally include inverse elements. However, the price we 

154 
COMPUTING MULTIPLE QUERIES 
Fusion 
Bucket Elimination 
l 
Collect 
Variable 
Systems 
t L 
Single-Query 
Inference Problem 
LauritzerbSpiegeJhalter 
ArchKecture 
Shenoy-Shafer 
Architecture 
j . 
Hugin 
Architecture 
i 
Idem potent 
Architecture 
ι > 
Division 
! . . 
. 
! 
i Idempotency . 
, i 
1 L 
Multi-Query 
Inference Problem 
Valuation Algebra 
Figure 4.11 A graphical summary of local computation architectures. 
pay is that full projection gets lost and we therefore recommend reading Appendix 
A.3 before entering the study of separative valuation algebras. Alternatively, regu-
larity is a stronger condition. In this case, the valuation algebra itself is decomposed 
into groups, such that full projection is conserved. Finally, if the third and strongest 
requirement called idempotency holds, every valuation turns out to be the inverse of 
itself. This has already been shown in Section 4.2.1 but here we are going to treat 
idempotency as a special case of regularity and thus separativity. 
D.1 PROPERTIES FOR THE INTRODUCTION OF DIVISION 
Equivalence relations play an important role in all three cases since they will be used 
for the decomposition of Φ. An equivalence relation 7 is areflexive, symmetric and 
transitive relation. Thus, we have for φ, φ, η € Φ 
1. Reflexivity: φ = φ (mod 7)» 
2. Symmetry: φ = φ (mod 7) implies ф = ф (mod 7), 
3. Transitivity: ф = ф (mod 7) and ψ = η (mod 7) imply φ = η (mod 7). 
Since we want to partition a valuation algebra, equivalence relations that are compat-
ible with the operations in Φ are of particular importance. Such relations are called 
congruences and satisfy the following properties: 
1. φ Ξ φ (mod 7) implies ά(φ) = ά{·ψ). 
2. φ = φ (mod 7) implies φ^χ = φ^χ (mod 7) if x Q ά(φ) = ά(φ). 
3. φι = φ\ (mod 7) and02 = Φι (mod 7) imply φ\®φι = φι®φ% (mod 7)· 

PROPERTIES FOR THE INTRODUCTION OF DIVISION 
155 
An intuitive motivation for the introduction of congruences in valuation algebras is 
that different valuations sometimes represent the same knowledge. Thus, they are 
pooled in equivalence classes that are induced by such a congruence relation 7. 
D.1.1 
Separative Valuation Algebras 
The mathematical notion of a group involves a set with a binary operation that is 
associative, has a unique identity and each element has an inverse. Thus, we may 
obtain inverse elements in a valuation algebra if the latter can be embedded into 
a union of groups. It is known from semigroup theory (Clifford & Preston, 1967) 
that the property of cancellativity is sufficient to embed a semigroup into a union 
of groups, and it is therefore reasonable to apply this technique also for valuation 
algebras which, according to Axiom (Al), form a semigroup under combination 
(Lauritzen & Jensen, 1997). See also Exercise A.4 in Chapter 1. However, (Kohlas, 
2003) remarked that a more restrictive requirement is needed for valuation algebras 
that also accounts for the operation of projection. These prerequisites are given in the 
following definition of separativity where we assume a congruence 7 that divides Φ 
into disjoint equivalence classes [φ]Ί given by 
[φ}Ί 
= 
{феФ:ф 
= ф (mod 7)}. 
(D.l) 
Definition D.4 A valuation algebra (Ф, D) is called separative, if there is a congru-
ence 7 such that 
• for all ψ, ψ' G [φ\Ί with φ®ψ = φ®ψ',\νε 
have ψ — φ', 
• for αΙΙφ e Φ and t Ç ά(φ); 
фи®ф 
= 
ф (mod 7)· 
(D.2) 
From the properties of a congruence, it follows that all elements of an equiva-
lence class have the same domain. Further, the equivalence classes are closed under 
combination. For φ,ψ e [φ]Ί and thus φ = φ (mod 7), we conclude from (D.2) 
φ®ψ 
= φ®φ 
= φ 
(mod 7). 
Additionally, since all [φ]Ί are subsets of Φ, combination within an equivalence class 
is both associative and commutative. Hence, Φ decomposes into a family of disjoint, 
commutative semigroups 
Φ = (J[#r 
феФ 
Semigroups obeying the first property in the definition of separativity are called 
cancellative (Clifford & Preston, 1967) and it follows from semigroup theory that 
every cancellative and commutative semigroup [φ)Ί can be embedded into a group. 
These groups are denoted by η(φ) and contain pairs (φ, ψ) of elements from [φ\Ί 

156 
COMPUTING MULTIPLE QUERIES 
(Croisot, 1953; Tamura & Kimura, 1954). This is similar to the introduction of 
positive, rational numbers as pairs (numerator and denominator) of natural numbers. 
Two group elements (φ, φ) and (φ', ψ') are identified if φ ig> ψ' = φ' ® ψ. Further, 
combination within η{φ) is defined by 
{φ,ψ) ® {φ',ψ') = (φ®φ',ψ®ψ'). 
(D.3) 
Let Φ* be the union of those groups η(φ), i.e. 
феФ 
We define the combination ®* of elements (</>, ψ), (φ', φ') G Φ* by 
(φ,ψ)®*(φ',ψ') = {φ®φ',ψ®ψ'). 
(D.4) 
It can easily be shown that this combination is well-defined, associative and commu-
tative (Kohlas, 2003). Φ* is therefore a semigroup under <g>*. Moreover, the mapping 
from Φ to Φ* defined by 
ф н-> {ф®ф,ф) 
is a semigroup homomorphism which is furthermore one-to-one due to the cancella-
tivity of the semigroup [φ\Ί. In this way, Φ is embedded as a semigroup into Φ*. 
Subsequently, we identify φ G Φ with its counterpart (φ ® φ, φ) in Φ*. Since *у(ф) 
are groups, they contain both inverses and an identity element. We therefore write 
φ-χ 
= {φ,φ®φ) 
for the inverse of group element φ, and denote the identity element within η{φ) as 
/7(Φ). Note that neither inverses nor identity elements necessarily belong to Φ but 
only to Φ*. The embedding of Φ into a union of groups Φ* via its decomposition into 
commutative semigroups is illustrated in Figure D.I. 
We next introduce a relation between the semigroups [φ] 7. Since 7 is a congruence, 
we may define the combination between congruence classes as 
Wi ® \Φ\Ί = [Φ® Ψ\η 
(D-5) 
and say that 
\Φ]Ί<[Φ]Ί 
if №® # γ = [0]7. 
(D.6) 
This relation is a partial order, i.e. reflexive, transitive and antisymmetric according 
to Definition A.2, as shown in (Kohlas, 2003). It can further be carried over to η{φ) 
by defining 
Ί{Φ)<Ί{Φ) if 
M 7 < [ # r 
The following properties are proved in (Kohlas, 2003): 

PROPERTIES FOR THE INTRODUCTION OF DIVISION 
157 
φ^ 
(ψ®φ,φ) 
φ* = {]Ί(φ) 
Figure D.l 
In a separative valuation algebra Φ is decomposed into disjoint equivalence 
classes, which are cancellative semigroups, by the congruence 7 and finally embedded into a 
union of groups Φ*. 
Lemma D.10 
1. Ifli-Φ) < Ί{Φ), then φ' <8> /7(ψ) = φ'for all φ' G η{φ). 
2. Ί{<(№) < Ί{φ) forallt<Z ά(φ). 
3. For all φ, ψ G Φ it holds that η(φ) < η{φ <g> ψ). 
4. {φ®ψ)~ι 
=φ~ι 
®V - 1· 
It is now possible to extend (partially) the operations of labeling, projection and 
combination from (Φ, D) to (Φ*, D): 
• Labeling d* for elements in Φ* is denned for η G Φ* by d* (η) = ά(ψ) for 
some ψ € Φ, if η e Ύ(Ψ). Since 7 is a congruence, d* does not depend on the 
representative φ of the group 7(V0· Therefore, d* is well-defined. Further, the 
label for an element φ € Φ is d* (φ) = ά(φ). In other words, d* is an extension 
of d, and since η and η~λ are contained in the same group, we directly get 
• We have already seen in equation (D.4) how the combination is carried out 
among elements in Φ*. This operator also extends (g>, since 
φ®* -φ = {φ®φ,φ)®* {ψ®-ψ,ψ) = (φ®ψ®φ®ψ,φ®'φ) = φ®ψ 
for elements φ,φ € Φ, which are identified by {φ®φ,φ) G Φ* and (ψ®"ψ, φ) G 
Φ* respectively. With this extension of combination, we further remark that φ 
and φ^1 are indeed inverses according to Definition 4.1, since 
( . - 1 
and 
Ф = Φ ®* ΪΊ(Φ) = Φ 
(D.7) 
Φ'1 = Φ'1 ®* ΪΊ(Φ) = Φ~Χ· 
(D.8) 

158 
COMPUTING MULTIPLE QUERIES 
We therefore conclude that separativity is a sufficient property to guarantee the 
existence of inverse elements. 
• Finally, we partially extend the operator of projection. Given an element η € 
Φ*, the projection I* of 77 to a domain s is defined by 
T^*S = φι"®*φ-\ 
(D-9) 
if there are ф, ф е Ф with ά(ψ) Ç s Ç ά{φ) and η{φ) < η{φ) such that 
η = 
φ®*φ~ι. 
This definition will be justified using the following lemma. 
Lemma D.ll Let 77 = (φ, φ) be the pair representation of an element η € Φ* with 
φ, φ e Φ and ά(φ) = ά(φ). Then η can be written as 
η = 
φ®*φ~ι. 
Proof: Because 7 is a congruence, we have d* (77) = ά(φ) — ά(φ) and further 
(φ,ψ) = 
(φ®φ®φ,φ®φ®φ) 
= {φ®φ,φ)Θ* (Φ,Φ®Φ) 
= 
φ®*φ~ι. 
Note again the similarity to positive, rational numbers represented as pairs of 
naturals. So, any element 77 € Φ* can at least be projected to its own domain. It will 
next be shown that \* is well-defined. Take two representations of the same element 
Vi,V2 e Φ* with 771 = (φ,φ),η2 = (Φ',Φ') алаф®ф' = φ®φ' such that щ = η2. 
Assume that both can be projected to a domain s, i.e. there are φι,φ2,ΨΐιΨ2 
& Φ» 
Ί{Ψι) < Ί(Φι), 7(<Ы < 7(<Ы and ά{φλ) ÇsÇ ά(φι), ά(φ2) ÇsÇ ά(φ2) such 
that 771 = φ\ ®* ψϊ1 and η2 = φ2 <8>* φ2
ι ■ We show that 
η\*8 = 4* s. 
(D.10) 
First, we conclude from 771 = 772 that 
Φΐ®*Φγ1 
= 
Φ2®*Φ2
λ-
We multiply both sides with ψι ®* ψ2 and obtain 
Φι <8>* Φΐ1 <g>* ψι ®* Ψ2 = φ2®*Φ2
λ 
®*ф2®*Ф\, 
thus 
Φΐ ®* /7(ψι) ®* ^2 = 
Ф2®* ίΊ(φ2) ®*Φΐ-

PROPERTIES FOR THE INTRODUCTION OF DIVISION 
159 
Since 7(^1) < 7(<£i) and 7(^2) < 7(0г)> Lemma D. 10 implies 
Φ1&Ψ2 
= 
Φ2®* Ψι, 
and because <g>* extends the combination in Φ, 0i (8> τ/>2 = 02 ® Φι must hold. By 
application of the combination axiom in (Φ, D) 
φ\°®ψ2 
= ( 0 1 ® ^ ) ^ = (Φ2®Ψΐ)1° 
= Φ\3®ΦΧ- 
(D.ll) 
Next, it will be shown that 
7(^1 ) < Ί{ΦΪ8) 
and 
Ί(φ2) 
< 7(0f)· 
(D.12) 
Since 7(^1) = 7 ( ^ f 1 ) < 7(^1 ), equation (D.6) implies that 7(^1 ) = 7(0i®Vi) = 
7(0i ® Vf1) an(i therefore 
7(0i) = 7(01 ®*^1-1) = 7(02 ®* Т/Ъ1) = 7(02). 
(D.13) 
On the other hand, the combination axiom in (Ф, D) yields 
7(0f) - Ί((Φι®Ψι)1Ί 
= 7(0iS®V>i)· 
Therefore 7(^1) < 7(0i ) and using equation (D.13) we obtain 7(^1) < 7(02 )· 
This proves the first relation of (D.12) and the second is obtained by symmetry. 
Finally, we deduce from multiplying both sides of (D.l 1) with ip^1 ®* ψ^1 
φ\8 ®* ip2 ®* -0Γ1 ®* Φ21 = ΦΪ3 ®* Ψι ®* Ψϊ1 ®* ^г-1' 
hence 
0 f ®* Vf' ®* /7(ψ2) = 02S ®* Ψ21 ®* /7(lh) 
and due to (D.12) and Lemma D.10 we finally obtain 
η?' = 0f ®* ^ 
= 0^ ®* ψ-1 = 4*s. 
This proves that projection in (Φ*, D) is well-defined. 
It remains to be verified that ],* is really an extension of |. Let 0 G Φ. In the first 
step we have to guarantee that 0^*s is defined for all s Ç ά{φ). The valuation 0 can 
be written as 
φ = 
(φ®φ^)®*(φ^)-\ 
since 
0^®*(0+0)-1 = 
V 

160 
COMPUTING MULTIPLE QUERIES 
and η{φ) > Ί{Φ^)· 
According to Lemma D.10 we have 7((^0) < η(φ ® φ^) 
and ά{φ^) = 0 Ç ά(φ <8> φ^). Therefore, the projection \.* of φ to any domain 
s С ά(φ <g> 0i0) = d(</>) is defined. Now, using the combination axiom in (Φ, D), 
ф±*° = 
(ф^ф^^Цф^у1 
= 
{фи ® ф&) ®* ( ^ ) - 1 
= 
^ s . 
Note that the extended domain operator M.* is directly induced by the definition 
of the new projection operator, i.e. for η G Φ* 
ΛΊ*(τ7) 
= 
^ Э ^ е Ф в и с Ь т а Ы ^ ) Ç s C d ( 0 ) , 
(D.14) 
7(г/>) < 7(<Д) and η = φ®* φ~1}. 
Theorem D.11 (Φ*, D) with the operations of labeling d*, combination (8)*, domain 
M* and projection \.* is a valuation algebra with partial projection. 
Proof: We verify the axioms on (Φ*, D) given in Appendix A.3: 
(Al") Commutative Semigroup: We have already seen that Φ* is a commutative 
semigroup under ®*. 
(A2") Labeling: Consider two elements щ, щ G Ф* with щ G ■у(ф) and r/2 G ^{ф) 
for ф, ψ G Φ. It follows from the definition of the labeling operator that 
d*(»?i) = ά(φ) and ά*(η2) = ά(φ). We conclude from equation (D.4) that 
^l ®* r?2 € Ί{Φ ® Φ)· Indeed, щ and щ can be written as pairs of elements 
of \φ\Ί and [φ]Ί respectively and therefore щ ®* т/г as pairs of elements of 
[<A]7 ® fr/>]7 = [^ ® Φ]Ί· Then, it follows that 
ά*(ηι®*η2) 
= ά(φ®φ) 
= ά{φ)υά{φ) 
= d*(щ) Ud*(η2). 
(A3") Projection: If η G Φ* can be projected to s, we have 
rf*' 
= φ^ ®* г/Г1 
with ά(φ) С s. It follows from the labeling axiom 
d*(^*s) = а*(ф1з®*ф~1) 
= d ^ U d ' f o r 1 ) 
= ^(0 i s) = e-
(A4") Transitivity: Let r? G Ф*, t Ç s Ç d(r?) such that ί G Λ4*(τ/). We have 
η 
= 
φ®*φ~ι 
with φ,φ € Φ, ά(φ) С ί and 7(^) < 7(0). Since ά(φ) Ç i C s C d(</>) it 
follows s G Μ*{η). The projection of η to the domain s yields 

PROPERTIES FOR THE INTRODUCTION OF DIVISION 
161 
From Lemma D.10 and Ί(Ψ) < 7(0) we derive 7(0) = 7(0 ® ψ) and obtain 
by application of the combination axiom in (Φ, D): 
Ί{φ±η = Ί{{φ®ψγ°) = 7(^5®^). 
Therefore 7 ^ ) < η(φ^8) holds. Since ά(ψ) Ç t we have shown that t e 
Μ*{η^*β). By the transitivity axiom in (Φ, D) we finally get 
(,,·!·«)-U = (ψ±η^ ®* ψ-1 = ф^®*ф~1 
= ту4·*'. 
(A5") Combination: Let щ,щ G Ф* with s = а(щ), t = ά(η2) and s Ç z Ç 
silt. 
We claim that z ΓΊ t £ M*fa) 
implies z e Μ*(ηι ®* rfë)· Assume that 
zCit 6 M*(i|2), i.e. 
m 
= 
Φι <э* V'J1 
with 02, ^2 € Ф, CÎ(T/>2) Ç ζ Π ί and 7(^2) < 7(02)· By Lemma D.l 1 there 
are 0i, ·0ι S Ф with 7(0ι) = 7(^1) and ά(φι) = ά(ψι) = s such that 
т/1 = 
φι&Φϊ1-
We then get 
m®* m 
= {Φι ® Φ2) ®* {Ψι ® ΨΊ)'1 
with <i(V>i <8) V2) = rf(^i) U d(^2) С z. Further 
7(01 (g) 02) 
= 
7(01 <8> -01 ® 02 ® ^2) 
and it follows 7(^1 ® т/>2) < 7(0i <8> 0г)· So z G М*(щ <8>* %). We finally 
get by the combination axiom in (Ф, D) 
Tfc®*^™ 
= 
(01 ( ^ ™) ® * ^ - 1 ® * ^ 1 
= 
(0i®0 2) i 2®* (V^i®^)" 1 
= 
(m®V2)l*z-
This holds because d(0i ® 02) = s U i and 7(^1 ® Ψ2) < 7(0ι ® 0г)· 
(A6") Domain: Let 77 e Ф* with ί = d*(r?). By Lemma D.ll, ί e -М*(ту), that is 
η = φ ®* ψ~ι with d(V') Ç ί and 7(V0 < 7(0)· Then, as a consequence of 
the domain axiom in (Φ, D), we get 
η^ 
= φ^&ψ-1 
= ф®*-ф~1 = η. 
ш 
This shows that the property of separativity is sufficient to provide a valuation 
algebra with a division operation. Again, we point out that we loose the property of 
full projection by this construction. Therefore, we end with a valuation algebra with 

162 
COMPUTING MULTIPLE QUERIES 
partial projection as stated by the above theorem. Before listing some instances of 
separative valuation algebras, we remark that cases exist where we do not need to 
worry about the existence of the projections in the combination axiom. 
Lemma D.12 Let η E Φ* and ψ e Φ with s = ά*(η) and t = d(ip). For every 
domain z G D such that s<Zz<ZsUtwe 
have z € Μ.{η <8> Φ)· 
Proof: Since ψ can be projected to any domain contained in ά(ψ), especially to 
z П t, it follows from the combination axiom that z G Μ(η <8> φ). 
■ 
It is shown in (Pouly, 2008) that an identity element e can also be adjoined to 
a separative valuation algebra without affecting separativity. It always holds that 
e = e _ 1 that allows us to derive division-based local computation architectures from 
the Shenoy-Shafer architecture of Section 4.1 that uses this particular element. 
■ D.6 Set Potentials and Separativity 
In order to prove that set potentials are separative, we first introduce some 
alternative representations. We define for a set potential m with domain d(m) — 
s and ACÜS 
bm(A) = Σ 
m(B) 
^ 
Irn(A) = ^ 
m(B). 
(D.15) 
BÇA 
BDA 
Clearly, both functions bm and qm are themselves set potentials. It is further-
more shown in (Shafer, 1976) that the two transformation rules are one-to-one 
with the following inverse transformations 
m(A) = Σ(-ΐ)|Α"β|^(β) 
(EU6) 
BÇA 
and 
m(A) = Υ^{-\)^-Α\ 
qm{B). 
(D.17) 
BDA 
Consequently, the operations of combination and projection can be carried over 
from m-functions to 6-functions and g-functions: 
O-mi 
*У 07712 
^771107712 5 
^777 
^771^*5 
Чт-ί ® Qm2 — Чт1®гП21 
Чт = 
Чт±1 ■ 
To sum it up, m-functions, ύ-functions and (/-functions describe the same sys-
tem, and since m-functions build a valuation algebra, the same holds for the 
two other representations. Additionally, we also deduce that all properties that 
hold for one of these representations naturally hold for the two others. 

PROPERTIES FOR THE INTRODUCTION OF DIVISION 
163 
It turns out that the operations of combination and projection simplify con-
siderably if they are expressed either in the system of g-functions or 6-functions. 
More concretely, combination reduces to simple multiplication in the system of 
(/-functions and projection does not need any computation at all in the system 
of b-functions. This is the statement of the following theorem which is for 
example proved in (Kohlas, 2003). 
Theorem D.12 
1. For qmi with domain s and qm2 with domain t we have for all A Ç QsUt 
(A) 
= 
qmi(Ai-s).qm2(Alt)- 
(D.18) 
2. For bm with domain s and t Ç s we have for all A ÇÇlt 
b£(A) 
= 
bm(A^). 
(D.19) 
Now, we are able to show that set potentials are separative. For this purpose, 
we change into the system of g-functions. Then, a congruence 7 is needed that 
divides Ф into cancellative equivalence classes. Let us therefore introduce the 
support of a g-function q e Ф with d(q) — s by 
supp{q) 
= 
{А С Çîs : q(A) > 0}. 
qi and q2 with equal domain are equivalent if they have the same support, i.e. 
q!=q2 
(modi) 
if d(qi) = d(q2) and supp(qi) = supp(q2). 
In particular, we have q = q® q^1 (mod 7) since 
supp(q ® q*) 
= 
{A Ç Ω« : q(A) ■ Ч^(А^) 
> 0} 
= 
{A Ç Çls : q{A) > 0} 
= 
supp(q). 
The second equality holds because m-functions are non-negative, projection of 
m-functions is defined by a sum, and q-functions are defined by a sum over the 
values of an m-function. Altogether, we have qit(Ait) 
= 0 implies q(A) = 0 
because the non-negative terms of a sum giving zero must themselves be zero. 
Hence, Ф is divided into disjoint equivalence classes of ç-functions with equal 
domain and support. These classes are cancellative since 
q(A).Ql(A) 
= 
q(A)-q2(A) 
implies that qi(A) = q2{A) if q,qi,q2 all have the same domain and support. 
This fulfills all requirements for separativity given in Definition D.4. 
It should be mentioned that not every δ-function or g-function transforms 
into a set potential m using either equation (D.16) or (D.17). This is because 

164 
COMPUTING MULTIPLE QUERIES 
both transformations can create negative values which shows that Φ* is really 
an extension of the system of g-functions. 
■ D.7 Density Functions and Separativity 
In order to show that density functions are separative, we proceed similarly to 
the foregoing example and define the support of a density / G Φ8 by 
supp(f) 
= 
{ x £ i î s : /(x) > 0}. 
Then, we again say that two densities / and g with equal domain are equivalent 
if they have the same support. We conclude / = / ® /■'■' (mod 7) since 
supp{f®fU) 
= 
{ x 6 ß , : / ( x ) . / V ) > 0 } 
= 
{x G Üs : /(x) > 0} 
= 
supp{f). 
The second equality holds because densities are non-negative and therefore 
/-μ(χ.μ) _ Q j mpij e s д х ) — о. Hence, Ф divides into disjoint equivalence 
classes of densities with equal domain and support. These classes are cancella-
tive which fulfills all requirements for separativity given in Definition D.4. 
D.1.2 
Regular Valuation Algebras 
We have just seen that a separative valuation algebra decomposes into disjoint semi-
groups which in turn can be embedded into groups. Thus, we obtain an extended 
valuation algebra where every element has an inverse. However, (Kohlas, 2003) 
remarked that in some cases, valuation algebras can directly be decomposed into 
groups instead of only semigroups. This makes life much easier since we can avoid 
the rather complicated embedding and, moreover, full projection is conserved. A 
sufficient condition for this simplification is the property of regularity stated in the 
following definition. Note also that it again extends the notion of regularity from 
semigroup theory to incorporate the operation of projection. 
Definition D.5 
• An element ф G Ф is called regular, if there exists for all t Ç ά(φ) an element 
X G Φ with d(x) = t, such that 
φ 
= 
φ^®χ®φ. 
(D.20) 
• A valuation algebra (Φ, D) is called regular, if all its elements are regular. 
In contrast to the more general case of separativity, we are now looking for a 
congruence that decomposes Φ directly into a union of groups. For this purpose, we 
introduce the Green relation (Green, 1951) between valuations: 
φ = φ 
(mod 7) 
if 
φ®Φ = ψ®Φ. 
(D.21) 

PROPERTIES FOR THE INTRODUCTION OF DIVISION 
165 
φ Cg> Φ denotes the set of valuations {φ ® η : η G Φ}, i.e. the principal ideal 
generated by φ. (Kohlas, 2003) proves that the Green relation is actually a congruence 
in a regular valuation algebra and that the corresponding equivalence classes [φ]Ί are 
directly groups which therefore provide inverse elements. 
Lemma D.13 Ι/φ is regular with φ = φ®χ®φ, then φ and χ®φ®χ are inverses. 
Proof: From Definition 4.1 we obtain 
φ®{χ®φ®χ)®φ 
= 
φ®χ®{φ®χ®φ) 
= 
φ®χ®φ 
= φ 
and 
(χ <8> 0 <8> χ) <8> 0 ® (χ ®φ® χ) 
= 
χ<8>(<Α<8>χ<8<Α)®χ®0®χ 
= 
χ®{φ®χ®φ)®χ 
= 
Χ®Φ®Χ-
и 
The equivalence classes [φ\Ί are clearly cancellative since they are groups and 
therefore contain inverse elements. Further, the Green relation clearly satisfies equa-
tion (D.2), which makes a regular valuation algebra also separative. From this point of 
view, regular valuation algebras are special cases of separative valuation algebras and 
since Φ does not need to be extended, full projection is preserved. This construction 
is illustrated in Figure D.2. 
Figure D.2 
A regular valuation algebra Φ decomposes directly into a union of groups Φ* 
by the Green relation. 
Idempotent elements play an important role in regular valuation algebras. These 
are elements / € Φ such that /<8>/ = /. According to Definition 4.1 we may therefore 
say that idempotent elements are inverse to themselves. Essentially, idempotents are 
obtained by combining inverses. Thus, if φ,ψ £ Φ are inverses, / = φ (g) ψ is 
idempotent since 
/ O / = (φ (g, φ) (g> (φ <g, ψ) = {φ®-φ®φ)®ψ 
= φ®ψ = f. 

166 
COMPUTING MULTIPLE QUERIES 
These idempotents behave neutral with respect to φ and ψ. We have 
Ϊ®Φ 
= {φ®ψ)®φ 
= φ, 
and the same holds equally for ψ. The following important lemma states that every 
principal ideal φ ® Φ is generated by a unique idempotent valuation. 
Lemma D.14 In a regular valuation algebra there exists for all φ € Φ a unique 
idempotent / G Φ with φ <g> Φ = / ® Φ. 
Proof: Since φ is regular there is a χ such that φ = φ®χ®φ 
and we know from 
Lemma D. 13 that φ and χ <g> φ ® χ are inverses. Thus, f = φ®{χ®φ®χ) 
= φ®χ 
is an idempotent such that f ®φ = φ. Therefore, φ®φ = $®{φ®'ψ) 
G / <S> Φ and 
/(git/; = φ®{χ®ψ) G 0<8>Φ. Consequently, 0®Φ = /(8>Φ must hold, and it remains 
to prove that / is unique. Suppose that /i ® Ф = / 2 ® Ф. Then, there exists χ G Ф 
such that / 2 = /i ® χ. This implies that /i ® /2 = /1 ® (/1 <8> x) = /1 ® X = /2-
Similarly, we derive /1 ® /2 = /2 and therefore it follows that /1 = /2· 
■ 
We again point out that we may also adjoin an identity element to a regular 
valuation algebra without loosing this property. An explicit proof can be found in 
(Pouly, 2008). Here, we now focus on an instance of a regular valuation algebra. 
More examples will be given in Appendix E.2 of Chapter 5. 
■ D.8 Arithmetic Potentials and Regularity 
In Instance 4.1 we used arithmetic potentials to give a simple example of inverse 
elements in a valuation algebra. There, we also mentioned that arithmetic 
potentials are in fact regular which induces these inverse elements. Now, we 
are able to verify this claim. Let p be an arithmetic potential with domain s and 
t С s. Then, the definition of regularity may be written as: 
p(x) = pil <g> x <g> p(x) = 
plt{xu)-x(xlt)-p(x). 
So, defining 
xW 
if/'(x) > 0, 
J pi'(x) 
1 arbitrary 
otherwise 
leads naturally to a solution of this equation. Hence, the inverse of a potential 
p with domain s and x G Ω8 is given by 
p-i ( x ) 
= 
( ^ 
Μ*)>°> 
0 
otherwise. 

PROOFS OF DIVISION-BASED ARCHITECTURES 
167 
D.1.3 
Idempotent Valuation Algebras 
The property of regularity allows the decomposition of a valuation algebra into groups 
such that inverses exist within Φ directly. We also learned that every such group is 
generated from a unique idempotent element. Therefore, the last simplifying condi-
tion for the introduction of division identifies valuation algebras where every element 
is idempotent. This has been proposed in (Kohlas, 2003) and leads to a decomposition 
where every valuation forms its own group. The property of idempotency has already 
been defined in Definition 4.2. By choosing χ = φ in Definition D.5, we directly 
remark that every idempotent valuation algebra is regular too. Then, since principal 
ideals are spanned by a unique idempotent, each element of an idempotent valuation 
algebra generates its own principal ideal. Consequently, all groups [φ\Ί consist of the 
single element φ which is therefore also the inverse of itself. 
It is not surprising that also the property of idempotency remains conserved if an 
identity element is adjoined to an idempotent valuation algebra. We again refer to 
(Pouly, 2008) for the proof of this statement. Some examples of idempotent valuation 
algebras have already been given in Section 4.2.1, others follow in Appendix E.4. 
D.2 PROOFS OF DIVISION-BASED ARCHITECTURES 
The proofs of the two division-based local computation schemes called Lauritzen-
Spiegelhalter architecture and HUGIN architectures were retained in Chapter 4 since 
they are based on properties derived in Appendix D. 1. 
D.2.1 Proof of the Lauritzen-Spiegelhalter Architecture 
The proof of the Lauritzen-Spiegelhalter architecture is based on the messages used in 
the Shenoy-Shafer architecture. If they exist, Lauritzen-Spiegelhalter gives the correct 
results because the inward messages are identical in both architectures. Further, the 
Shenoy-Shafer messages are needed in equation (D.23) below for the application of 
the combination axiom. However, (Schneuwly, 2007) weakens this requirement by 
proving that the existence of the inward messages is in fact sufficient to make the 
Shenoy-Shafer architecture and therefore also Lauritzen-Spiegelhalter work. 
Proof of Theorem 4.2 
Proof: Let μ' denote the messages sent during an execution of the Shenoy-Shafer 
architecture. We know that 
during the collect propagation and the theorem holds for the root node as a result of 
the collect algorithm. We prove that it is correct for all nodes by induction over the 
outward propagation phase using the correctness of Shenoy-Shafer. For this purpose, 

168 
COMPUTING MULTIPLE QUERIES 
we schedule the distribute phase by taking the reverse node numbering. When a node 
j is ready to send a message towards i, node г stores 
Φί ® (μΐ-ν,·)"1 ® 
0 
ώ-"· 
(Ε>·22) 
k€ne(i),fc^.j 
By the induction hypothesis, the incoming message at step г is 
μ ^ 
= ф+ЧЛпЧг) 
iX(j)n\(i) 
k£ne(j) 
Φί® CR) /4 
*i-vj 
The third equality follows from the combination axiom, and the assumption is needed 
to ensure that μ'^ί exists. So we obtain at node i, when the incoming message μ3^ι 
is combined with the actual content and using Theorem 4.1, 
& ® 
(g) 
M f e - n ® ^ , - ) " 1 ® ^ - * ® / ^ , - 
= 
^ ( i ) e / 7 W 4 j ) . 
А:етге(г),/с7^.7 
It follows from Lemma D.10 that 
l(ßUj) 
< 7 ( ^ ® μ ί - ^ - ) = 7 ( ^ λ ϋ ) η λ ( 0 ) < 7(^ λ ( ί )), 
hence 
D.2.2 
Proof of the HUGIN Architecture 
Proof of Theorem 4.3 
Proof: The proof is based on the correctness of the Lauritzen-Spiegelhalter architec-
ture. First we consider the separators in the join tree (V, E, A, D) as real nodes. Thus, 
let (V', Ε', λ', D) be such a modified join tree. We then adapt the assignment map-
ping a making it again surjective. We simply assign to every separator node in V — V 
the identity element e and name the extended assignment mapping a'. Now, the ex-
ecution of the Lauritzen-Spiegelhalter architecture is started using 
(V',E',X',D,a'). 

PROOF FOR SCALING IN VALUATION ALGEBRAS 
169 
Take a node г € V which is not a separator in the original tree. It sends a message 
/Xi->j in the collect phase of Lauritzen-Spiegelhalter and divides it out of its current 
content which we abbreviate with 77^. By the construction of (V',E',X',D), 
the 
receiving node j = ch(i) is a separator in (V, E, X, D), that is j G (V — V). Node г 
contains r\i (g> (/ii->j)_1 and j stores e <g> ßi-+j — ßi-+j after this step. Then node j is 
ready to send a message towards node к — ch(j). But we clearly have /Xi_>j = ßj-+k· 
Since every emitted message is divided out of the store, the content of node j becomes 
Vi-yj ® (Vj^k) 
= 
/7(Mj-
> f c)· 
We continue with Lauritzen-Spiegelhalter and assume that node к is ready to send 
the message for node j during the distribute phase. This message equals (f>^Wn\(j) 
due to Theorem 4.2 and also becomes the new content of j according to equation 
(D.23). The message sent from j towards г is finally </»-1-λ0')ηλ(*) = 0-U(k)nA(») so 
that we get there 
ί&®(Αϋ->,·)"1®^Ι'λ(0Πλ('0 
= <t>iHi)-
This follows again from the correctness of Lauritzen-Spiegelhalter. But 
is also the message from к towards г in the HUGIN architecture using (V, 
E,\,D,a), 
which has passed already through the separator j . 
m 
The messages used throughout the proof correspond to the Lauritzen-Spiegelhalter 
messages. Therefore, we may conclude that the existence of the collect messages 
is also a sufficient condition for Theorem 4.3. If, on the other hand, the valuation 
algebra is regular, this condition can again be dropped. 
D.3 
PROOF FOR SCALING IN VALUATION ALGEBRAS 
Proof of Lemma 4.9 
Proof: 
1. By application of the combination axiom 
(
_ i \ 10 
_i 
Hence 
since η{ήλ) = Ί(Φ) > 
η(φ^). 

170 
COMPUTING MULTIPLE QUERIES 
2. We first remark that φ = φ^ ® φ^ since η{φ) > ^(φ^) 
and using the 
combination axiom and equation (4.30) we have 
φ®φ 
= 
(φ±® φ^) ® (ψ^ ® </>i0) 
= 
(Φ1 ® V1) ® (</>i0 ® V*40) 
= 
{Φ1 ® г / ^ ® ( ^ ® V 4) i 0 ® (</>i0 ® ^ i e ) 
= 
(с^®^) г®((<А 4 0®^ 4 0)®>(<^®^)) 1 0 
= 
(<M ® V4-)4· ® ( ( ^ ® 4>i0) ® (<M ® ^ i 0 ) ) 4 0 
= 
( ^ ® ^ ) 4 " ® ^ ® ^ ) 1 0 · 
From this we conclude that 
(Φ®ψγ 
= (Φ ® ψ) ® ({Φ ® ψγΛ 
= 
( ^ - ® ^ о 4 - ® (<£ ® ^ ) 4 0 ® ((</> ® VO40) 
= 
(φ1 ® ψψ ® 
ί^φ^ΙΦ) 
= 
(Φι ® v4")4" 
because again "/{(φ^ ® ΐ/'4')4') > "){{φ ® VO4"0)· 
3. From equation (4.30) we conclude on the one hand 
φ* = (φ^γ ® (φ^γ® = (φ^γ®φ^, 
and on the other hand 
ф* 
= 
(ф±®ф^)Ы 
= 
(ф±)и®фМ. 
Hence 
( ^ 4 ) 4 · ® ^ 0 
= 
( « ^ ) % < ^ 0 . 
Since 7(<M0) < 7((^) i t), 7((04'i)4·) it follows that 
(Φ*γ = (ф^ъф^ъ^у1 
= (ф±)и. 
D.9 Scaling of Belief Functions 
In Instance D.6 we derive the two alternative representations of set potentials as 
6-functions and ç-functions. These systems are isomorphic, which allows us to 
conclude that they all form valuation algebras and satisfy the same additional 
properties. Since combination corresponds to simple multiplication in the case 

PROOF FOR SCALING IN VALUATION ALGEBRAS 
1 7 1 
of (/-functions, it is often convenient to study related algebraic properties in this 
system. Thus, we obtain by applying the definition of scaling to a g-function: 
^ 
= <7< ( « " ) " · 
It is important to note that q^ consists of two values, q^ (0) and q^ ( {o} ). Since 
the empty set can never be obtained from projecting a non-empty configuration 
set, the formula can be written for the case where А ф 0 and q^{{o}) φ 0 as 
MA\ 
- 
^A) 
q{A) = шкг 
(D-24) 
We then obtain for the denominator 
9 i 0(W) = Y^rniA) 
= m^({o}). 
Since 
ci0(0) = 
Σ 
ml0(A) 
= m+0({o}) + mi0(0) 
ACQm 
= ^m(A)+m(0) = J2m(A) = 9(0), 
A^0 
A 
we have for the empty set 
a(0) = _#L = m = ! 
(D25) 
9 W 
9^(0) 
g(0) 
l 
^ 
So, the scale of a g-function is obtained by equation (D.24) and (D.25), and it is 
common to refer to non-negative, scaled ç-functions as commonality functions. 
We next translate the scaling operation for g-functions to the system of 
m-functions using equation (D.17) and obtain 
т1(А) 
= 
^ ( - 1 ) | в " л | ql{B) 
(D.26) 
BDA 
Σ Β Ρ Λ ( - 1 ) | Β - Λ | 4{B) 
= 
m(A) 
= 
m(A) 
Ев^гп(В) 
ΣΒ^ΜΒ) 
m^({o})' 

172 
COMPUTING MULTIPLE QUERIES 
if А ф 0 and m4-0({o}) φ 0. Finally, we obtain for то4-(0) 
mi(0) = Σ(-ΐ)Μ 
ql(A) 
A 
Ел#0 т( л) 
Ел(-1) | д |дИ)- 9(0) | t 
ΣΑ^ 
m(A) 
m(0) 
m(0) + £A^0m(A) | 1 = 
Q 
ΣΑ#0 m(^) 
ΣΛ^0 m(A) 
Thus, the scale of a set potential is given by equation (D.26) if А ф 0 and 
m4-0({o}) φ o5 and m-|'(0) = 0 otherwise. This satisfies the two properties: 
m-<-(0) = 0 
and 
J2 ml(A) 
= 1. 
(D.27) 
In the theory of evidence, such potentials are called basic probability assign-
ments or mass functions. Intuitively, m{ A ) represents the part of our belief that 
the actual world (i.e. some configuration) belongs to A - without supporting 
any more specific subset, by lack of adequate information (Smets, 2000; Smets 
& Kennes, 1994). Under this interpretation, the two normalization conditions 
imply that no belief is held in the empty configuration set (which corresponds 
to a closed world assumption) and that the total belief has measure 1. In a 
similar way, the operation of scaling can also be translated to the system of 
b-functions using equation (D.15). It is then common to refer to non-negative, 
scaled 6-functions as belief functions. A comprehensive theory of mass, belief 
and commonality functions is contained in (Shafer, 1976). We also pointed 
out in Lemma 4.9 that the combination of two normalized set potentials (mass 
functions) does generally not lead to a mass function again. However, this can 
be achieved using equation (4.32). Assume two mass functions m\ and m2 
with domains s and t. We have for 0 с Л С iîsUt 
m1®m2(A) 
= 
(mi 
®m2)x{A) 
_ 
m\ 
®m2{A) 
(тщ ®m2)^0({<>}) 
where 
К 
= 
( m 1 ® m 2 ) i e ( M ) 
= 
^2 mi ®m2{B) 
= 
^2 
τηι(Αι) ■ m2(A2). 
ВфЧ 
AlsUtnAlsutï<i 
к 
m\ 
®m2(Ä), 

EXERCISES 
173 
We further define ray Θ m2(0) = 0 and, if К = 0, we set m\ Θ m2{A) = 0. 
This operation is called Dempster's rule of combination (Dempster, 1968). 
We complete the study of scaling for set potentials and our excursion to 
Dempster-Shafer theory by a small example of normalizing a set potential. 
Let r = {A, B} be a set of variables with finite frames Ω^ = {α,α} and 
Ωβ = {b, b}, and m a set potentials with domain d(m) — {A, B} defined as: 
m 
0 
{(o,b)} 
{(a,b),(o,b)} 
{(a,b),(a,b)} 
0.6 
0.1 
0.1 
0.2 
We then compute 
m 10 
0 
{о} 
0.6 
0.4 
and obtain for its associated mass function 
0 
{(a,b)} 
{(a,b),(a,b)} 
{(a,b),(a,b)} 
0 
0.25 
0.25 
0.50 
PROBLEM SETS AND EXERCISES 
D.l * Exercise B.4 in Chapter 2 asked to transform the fundamental computational 
problems of filtering, prediction and smoothing in hidden Markov chains into an in-
ference problem. Show that the filtering and smoothing problem can simultaneously 
be solved by a complete run of the Shenoy-Shafer, Lauritzen-Spiegelhalter or HUGIN 
architecture, i.e. that filtering corresponds to the collect phase and smoothing to the 
distribute phase in the three local computation architectures. 
D.2 * Section 4.5 derives the idempotent architecture from the correctness of the 
Lauritzen-Spiegelhalter architecture. 
a) Provide an alternative proof by starting from the HUGIN architecture. 
Indication: consider the separators in the HUGIN architecture as ordinary 
join tree nodes and observe that HUGIN becomes identical to Lauritzen-
Spiegelhalter. 
b) Derive the idempotent architecture directly from the generalized collect 
algorithm in Section 3.10 without reference to another division-based ar-
chitecture. This is similar to the proof of the Shenoy-Shafer architecture. 

174 
COMPUTING MULTIPLE QUERIES 
D.3 * * Consider a join tree (V, E, X, D) with | V\ = r that is fully propagated by 
the Shenoy-Shafer architecture, i.e. for each node г £ V we have 
0^(0 
= ( ^ ® . . . ® ^ r ) ± * W = ^ ® (g) /zj-^i. 
j€ne(t) 
as stated in Theorem 4.1. Assume now that a new valuation η £ Φ is combined to 
some covering node к & V with ^(ту) = λ(&). This is called a consistent update. If 
the Shenoy-Shafer architecture is repeated from scratch, we obtain at the end of the 
message-passing for each node г £V 
{ф ® η)^ 
= f>i ® · ■ · ® Vr ® ί?)^λ(ί) = $ ® ® 
Mj--+i 
jgne(i) 
where ^ = Vi f°r i ф к and ψ'ι,. = Фк ® η. However, instead of recomputing all 
messages μ,-^-, it is more appropriate to reuse the messages that do not change by 
this updating process. Prove that during the collect phase, only the messages between 
the node к and the root node change. In the distribute phase, all messages must be 
recomputed. Perform the same analysis for the Lauritzen-Spiegelhalter, HUGIN and 
idempotent architecture. The solution can be found in Chapter 6 of (Schneuwly, 2007). 
D.4 * Let (Ф, D) be an idempotent valuation algebra. For φ, ψ £ Φ we define 
φ>ψ 
if, and only if, φ®ψ = φ. 
(D.28) 
This expresses that the information piece φ is more informative than φ. 
a) Prove that this relation is a partial order, i.e. verify the axioms of Definition 
A.2 in the appendix of Chapter 1. 
b) We further assume that (Φ, D) is an information algebra, i.e. that neutral 
and null elements are present, see Definition 4.3. We then also have the 
operation of vacuous extension given in equation (3.24) of Chapter 3. Prove 
the following properties of relation (D.28) for φ,ψ £ Φ and x,y £ D: 
1. if φ < ψ then ά(φ) С ά(ψ); 
2. if x С у then ex < ey and zx < zy; 
3. if x Ç ά(φ) then ex < φ and, if ά(φ) С х, then ф < zx; 
4. φ,ψ <φ®φ; 
5. φ®ψ = sup{0,-0}; 
6. ifx Ç ά{φ) then φίχ < φ; 
7. ifd((/>) С у then ф < ф^У; 
8. if х С у = ά{φ) then (ф±х)Ъ < ф; 

EXERCISES 
175 
9. φι < φι and ψ\ < ψ2 imply φχ <8>φι <φ2® Φϊ, 
10. if χ Ç ά(φ) (Ί ά(φ) then ^ χ ® Vi:E < (φ ® V0ix; 
11. if χ С d(<£) then φ < φ implies 0"1-* < ^ 1 ; 
12. if d(0) С у then ф < ф implies ф^у < ф^у; 
13. if ά(φ) Ç i C d(t/>) then φ < φ implies φ < φ^χ. 
The partial order together with Property 5 implies that Φ is a semilattice. 
Indications for this exercise can be found in Lemma 6.3 of (Kohlas, 2003). 
D.5 * * 
Let (Φ, D) be an information algebra and suppose that the information 
content of an element φ G Φ has been asserted. Then, all information pieces which 
are less informative than φ should also be true. The sets 
Ι(φ) 
= 
{φ e φ : φ > φ}. 
(D.29) 
are called principal ideals. Observe that principal ideals are closed under combination, 
i.e. if φ, v G Ι(φ) then φ ® v G 1{φ). We further define 
/Ф 
= 
{1(ф):феФ} 
and introduce the following operations in (Ιφ, D): 
1. Labeling: For Ι(φ) G 7φ we define ά(Ι(φ)) = ά(φ). 
2. Combination: For 1(ф\), /(</>г) € /ф we define 
/i ® h 
= 
{v G Ф : φι (g) 02 > v} 
3. Projection: For /(0) G /φ and x Ç ά(Ι(φ)) we define 
/J-* 
= 
{φ £φ : ф±х > ф]. 
Prove that the set of principal ideals /ф is closed under combination and projection, 
and show that (Ιφ, D) satisfies the axioms of an information algebra. Indications can 
be found in (Kohlas, 2003), Section 6.2. 
D.6 * * 
Let s,t,u 
G D be disjoint sets of variables and assume that we want to 
define a valuation φ G Φ with ά(φ) = s U t U и by specifying its projections ф\ and 
ф% relative t o s U « and t U u. Of course, the projections must be consistent such 
that ф\и = -02 holds. The problem of finding such a valuation ф = ф\ % φ^ is 
called marginal problem. Intuitively, this is similar to the specification of a building 
by its floor, body and sheer plan. Prove that if (Φ, D) is a regular valuation algebra 
according to Definition D.5, then there exists φ G Φ with ά{φ) = sötöu 
such that 
^ s U " = φλ 
and 
фиии 
= ф2. 

176 
COMPUTING MULTIPLE QUERIES 
The solution to this exercise can be found in (Kohlas, 2003), Theorem 5.15. Marginal 
problems are closely related to the notion of conditional independence in valuation 
algebras that is treated in Exercise D.7. 
D.7 * * Let (Φ, D) be a valuation algebra and φ G Φ. If s, t, u are disjoint subsets 
of ά{φ), we say that s is conditionally independent of t given и with respect to ф, if 
there exist tpi,ip2 € Ф such that ά{φ\ ) = sou, d{%l>2) = tUu and 
ф1зиЮи 
= 
^ ι ® ^ . 
(D.30) 
If s is conditionally independent of t given u, we write s_L^i |tt. 
a) Show that conditional independence in valuation algebras coincides with 
stochastic conditional independence in probability theory when applied 
to the instances of probability potentials, density functions and Gaussian 
potentials presented in Chapter 1. 
b) Prove the following properties of conditional independence. Let ф G Ф and 
assume s, t,u,v С ά(φ) to be disjoint sets of variables. Then 
1. Symmetry: s_L</,i|u implies tJ_0s|u. 
(Gl) 
2. Decomposition: sJ-φί U v\u implies s_!_0Î|ii. 
(G2) 
If furthermore the valuation algebra has neutral elements, we have 
3. Weak Union: S-L^i U v\u implies s^t\u 
U v. 
(G3) 
The solution to these exercises can be found in Section 5.1 of (Kohlas, 2003). 
D.8 ** Consider the definition of conditional independence in Exercise D.7. For 
и = 0 we use the short notation sJ_^i and say that s is independent of t with respect 
to ф G Ф. Let (Ф, D) be a regular valuation algebra, ф G Ф and s,t G D disjoint 
subsets of ά(φ). Then, the valuation 
4>,\t - 
r y r 1 ® ^ * 
(D.31) 
is called the conditional of ф for s given t. 
a) Prove that the factors ψι = фя^ and ψ2 = Φ^ satisfy equation (D.30) and 
identify φ8^ in the valuation algebra or probability potentials. 
b) Conditionals in regular valuation algebras have many properties that are 
well-known from conditional probabilities. Prove the following identities 
and rewrite them in the formalism of probability potentials: 
1. If φ G Φ and s,t С ά(φ) are disjoint, then 
0S|t 
= 
А(^')· 
2. If ф G Ф and s, i, и С ά(φ) are disjoint, then 
<AsUt|u 
= 
</»s|tUu ® 0 t | u · 

EXERCISES 
177 
3. If φ G Φ, s, t C ά(φ) are disjoint and и С s, then 
jltUU 
/ 
<Ps\t 
= 
<Pu\t-
4. If ф £ Ф and s,i,uÇ ά(φ) are disjoint, then 
(0„|.Ut ® 0S|i)itU" 
= 
&»|f 
5. If ^>, г/> е Ф, s, i Ç d(^) are disjoint, and ά(ψ) = t, then 
(0 i e U t ® V).|t 
= 
0β|ί®/7(ψ). 
c) Prove that if the valuation algebra is regular, Property (G3) holds even if 
no neutral elements are present. 
d) Prove that conditional independence in regular valuation algebras satisfies: 
4. Contraction: s±<f,t\u and S-L^i U и imply s-L^f U v\u. 
(G4) 
The properties (Gl) to (G4) may be considered as a system of axioms for 
an abstract calculus of conditional independence. 
The solution to these exercises can be found in Section 5.1 of (Kohlas, 2003). 
D.9 * * Exercise D.8 can be generalized to separative valuation algebras, see Ap-
pendix D. 1.1. The same definition of conditionals applies in this case, but in contrast, 
the conditionals do generally not belong to Ф but only to the separative embedding 
Ф*. Also, the properties listed in Exercise D.8.b still hold in the separative case, 
although we must pay attention to partial projection in the proofs. Identify the condi-
tional 0e|t in the valuation algebras of density functions, Gaussian potentials and set 
potentials. The solution to this exercises can be found in Section 5.3 of (Kohlas, 2003). 

PART II 
GENERIC CONSTRUCTIONS 
Generic Inference: A Unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc.

CHAPTER 5 
SEMIRING VALUATION ALGEBRAS 
The mathematical framework of valuation algebras introduced in Chapter 1 provides 
sufficient structure for the application of generic local computation methods. In the 
process, the numerous advantages of such a general framework became clear, and 
we have seen for ourselves that different formalisms are unified under this common 
perspective. As a fundamental requirement for this generality, we did not assume any 
further knowledge about the structure of valuations. They have only been considered 
as mathematical objects that possess a domain and that can further be combined and 
projected according to certain axioms. For certain applications, however, it is useful 
to have additional knowledge about the structure of valuations. This amounts to the 
identification of families of valuation algebra instances which share some common 
structure. A first example of such a family of valuation algebras grows out of the 
obvious similar structure of indicator functions and arithmetic potentials introduced 
as Instances 1.1 and 1.3. On the one hand, both instances are obtained by assign-
ing values to tuples out of a finite set of tuples, but on the other hand, they differ 
in their definitions of combination and projection. Yet, there is a common ground 
between them. We will learn in this chapter that both examples belong to a family 
of instances, called semiring valuation algebras. A commutative semiring is by itself 
a fundamental algebraic structure that comprises a set of values and two operations 
Generic Inference: A Unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc. 
181 

182 
SEMIRING VALUATION ALGEBRAS 
called addition and multiplication. Then, the values assigned to tuples are those of the 
semiring, and the operations of combination and projection can both be expressed us-
ing the two semiring operations. In other words, every commutative semiring induces 
a new valuation algebra, and the two examples of indicator functions and arithmetic 
potentials are both members of this large family. The reasons why we are interested 
in semiring valuation algebras are manifold. First and foremost, semiring instances 
are very large in number and, because each commutative semiring gives rise to a 
valuation algebra, we naturally obtain as many new valuation algebras as commuta-
tive semirings exist. Moreover, if we interpret valuation algebras as formalisms for 
knowledge representation, we are not even able to explain for some of these instances 
what kind of knowledge they model. Nevertheless, we have efficient algorithms for 
their processing thanks to the local computation framework. Second, we will see that 
a single verification proof of the valuation algebra axioms covers all formalisms that 
adopt this common structure, and we are also relieved from searching each formalism 
separately for neutral elements, null elements and the presence of a division operator. 
Finally, semiring valuations also stimulate new applications of local computation 
techniques that go beyond the pure computation of inference. Solution construction 
in constraint systems is an example of such an application that will be discussed in 
Chapter 8. Besides semiring valuation algebras, there are other families of formalisms 
derived in a similar way from other algebraic structures. This powerful technique is 
called generic construction and takes center stage in the second part of this book. 
This chapter starts with a general introduction to semiring theory, accompanied 
by a large catalogue of instances to convince the reader of the richness of semiring 
examples. Section 5.2 then studies different classes of semirings that arise from the 
properties of a canonical order relation. Following (Kohlas & Wilson, 2008), we 
then show in Section 5.3 how semirings produce valuation algebras and we give an 
extensive member list of this family of valuation algebras in Section 5.4. Section 5.5 
deals with the algebraic properties of semiring valuations and show which properties 
of the semiring guarantee the existence of neutral and null elements. The study of 
division is again postponed to the chapter appendix. A second family of valuation 
algebras, which is closely related to semiring valuations, is introduced in Section 5.7, 
and its algebraic properties are analyzed in Section 5.8. 
5.1 
SEMIRINGS 
A semiring is an algebraic structure consisting of a set provided with two operations, 
called addition and multiplication, which satisfy the distributive law. In recent years, 
the importance of semirings for computational purposes has grown significantly and 
the distributive law is in fact the reason for this increasing popularity. From the simple 
formula a x (b + c) =axb 
+ axc, 
we can directly observe that the left-hand 
side is computationally more efficient since we perform only one multiplication. The 
distributive law is the cause of efficiency of local computation, since it induces the 
combination axiom. However, let us start with a short introduction to semiring theory. 

SEMIRINGS 
183 
Definition5.1 A tuple {A,+, x,0,1) with binary operations + and x is called 
semiring if the following properties hold: 
• + and x are both associative; 
(SI) 
• + is commutative; 
(S2) 
• fora,b,с 
G A: a x (b + c) — a x b + a x c; 
(S3) 
• for a,b,c G A: (a + b) x с = а х с + b x c; 
(S4) 
• + has a neutral element 0, i.e. a + 0 = a for all a G A; 
(S5) 
• x has a neutral element 1, i.e. axl 
= lxa 
= a for all a G A; 
(S6) 
• α χ Ο = Ο χ α = Ofor all a G A. 
(S7) 
There are different definitions of semirings in the literature. This definition is 
taken from (Golan, 1999) and assumes the existence of two neutral elements. The 
neutral element 0 G A with respect to addition is sometimes called zero element. It 
is a simple consequence of (S5) that a zero element is always unique. Moreover, if 
an algebraic structure provides all properties of a semiring except the presence of a 
zero element then the latter can always be adjoined artificially. The neutral element 
1 G Л with respect to multiplication is called unit element. It is also unique but in 
contrast to the zero element, it can only be adjoined if the operation of addition is 
idempotent (see Definition 5.2 below). The corresponding constructions are shown 
in (Kohlas & Wilson, 2008). 
Definition 5.2 Let (A, +, x, 0,1) be a semiring: 
• it is called commutative ifaxb 
= bx a for alla,b G A; 
• it is called idempotent ifa + a = a for all a G A; 
• it is called positive if a + b = 0 implies that a = b = Ofor all a, b G A; 
Let us consider some examples of semirings: 
Example 5.1 (Arithmetic Semirings) Consider the set of non-negative real num-
bers R>o with + and x designating the usual operations of addition and multipli-
cation. This is clearly a positive, commutative semiring with the number 0 as zero 
element and the number 1 as unit element. In the same way, we could also take 
the fields of complex, real or rational numbers, or alternatively only non-negative 
integers or non-negative rationals. In the former three cases, the semiring would not 
be positive anymore, whereas ordinary addition and multiplication on non-negative 
integers and rationals yield again a positive semiring. 
Example 5.2 (Boolean Semiring) Take the set В = {0,1} with the intention that 
0 designates the truth value "false " and 1 "true ". Addition is defined as a + b = 

184 
SEMIRING VALUATION ALGEBRAS 
max{a, 6} and represents the logical disjunction. Similarly, multiplication is defined 
asaxb = min{a, 6} which stands for the logical conjunction. This is a commutative, 
positive and idempotent semiring with zero element 0 and unit element 1. 
Example 5.3 (Bottleneck Semiring) A generalization of the Boolean semiring is 
obtained if we take a + b = max{a, b} and a x b = min{a, b} over the set of real 
numbers Ш. U {+oo, —oo}. Then, —oo is the zero element, +oo the unity, and the 
semiring remains commutative, positive and idempotent. 
Example 5.4 (Tropical Semiring) An important semiring is defined over the set of 
non-negative integers N U {0, oo} with a + b = min{a, b} and the usual integer addi-
tion +nfor x with the convention that a+^oo = oo. This semiring is commutative, 
positive and idempotent, oo is the zero element and the integer 0 is the unit element. 
Example 5.5 (Arctic Semiring) The arctic semiring takes max for addition over 
the set of real numbers R U {—oo}. Multiplication is +κ with a +R (—oo) = —oo. 
77«J semiring is commutative, positive and idempotent, has — oo as zero element and 
0 as unit element. 
Example 5.6 (Truncation Semiring) An interesting variation of the tropical semi-
ring is obtained if we take A = {0,..., k}for some integer k. Addition corresponds 
again to minimization but this time, we take the truncated integer addition for x, i.e. 
a x b = min{a +N b, k}. This is a commutative, positive and idempotent semiring 
with к being the zero element and 0 the unit. 
Example 5.7 (Semiring of Formal Languages) A string is a finite sequence of sym-
bols from a countable alphabet Σ and a set of strings is called language. In partic-
ular, we refer to the language of all possible strings over the alphabet Σ as Σ*. For 
А, В CE* we define A + В = Л и В and A x В = {ab \a € A and b e B}. This 
semiring of formal languages is idempotent with zero element 0 and unit element {e}. 
It is also positive but not commutative. 
Example 5.8 (Triangular Norm Semiring) Triangular norms (t-norms) were origi-
nally introduced in the context ofprobabilistic metric spaces (Menger, 1942; Schweizer 
& Sklar, 1960). They represent binary operations on the unit interval [0,1] which are 
commutative, associative, nondecreasing in both arguments, and have the number 1 
as unit and 0 as zero element: 
1. Va, b, с € [0,1] we have T(a, b) = T(b, a) and T{a, T(b, c)) = T{T{a, b), c), 
2. a<a' and b < b' imply T{a, b) < T(a', b'), 
3. Va G [0,1] we have T(a, 1) = T(l, a) = a and T(a, 0) = T(0, a) = 0. 
T-norms are used in fuzzy set theory and possibility theory. In order to obtain a 
semiring, we define the operation x on the unit interval by a t-norm and + as max. 
This is a commutative, positive and idempotent semiring with the number 0 as zero 
element and 1 as unit. Here are some typical t-norms: 

SEMIRINGS 
185 
• Minimum T-Norm: T(a, b) = min{a, b}. 
• Product T-Norm: T(a, b) =a-b. 
• Lukasiewicz T-Norm: T(a, b) = max{a + b — 1,0}. 
• Drastic Product: T{a, 1) = T(l, a) = a and T(a, b) = 0 in all other cases. 
The semiring induced by the product t-norm is also called probabilistic semiring. 
Instead of maximization, we may also take minimization for addition, which only 
reverses the definition of zero and unit element. 
In addition to these examples, we may also define structures to derive semirings 
from other semirings. Vectors and matrices with semiring values are typical examples 
that form themselves a semiring. 
5.1.1 
Multidimensional Semirings 
Take n possibly different semirings {Ai, +», Xj, Oj, lj) for г = 1,..., n and define 
A 
= 
Ai x ·■■ x An 
with corresponding, component-wise operations 
(ai,...,an) 
+ (bi,...,bn) 
= 
(ai +i 6 b ... ,an +n bn) 
(αι,...,α η) x (&i,...,&n) 
= 
(ai Xi 6i,...,o n x„6„). 
These operations inherit associativity, commutativity and distributivity from their 
components. Hence, {A, +, x,0,1) becomes itself a semiring with zero element 
0 = (Oi,..., 0n) and unit 1 = ( l i , . . . , 1„). If all Ai are commutative, positive or 
idempotent, then so is A. 
5.1.2 
Semiring Matrices 
Given a semiring {A, +, x, 0,1) and n € N, we consider the set M{A, 
n j o f n x n 
matrices with elements in A. Addition and multiplication of semiring matrices is 
defined in the usual way: 
( M i + M 2 ) ( i , j ) 
= 
Mi(i,j) + M 2(t,i) 
(5.1) 
and 
n 
(Mi x M 2 ) ( M ) 
= Σ 
Mi(i,fc) x M2{k,j) 
(5.2) 
fe=l 
for 1 < i, j < n. It is easy to see that square matrices over a semiring themselves form 
a semiring. Also, we obtain the zero element for semiring matrices by 0{i,j) 
= 0 
for 1 < i, j <n and likewise, we obtain the unit element for the algebra of semiring 
matrices by 1(г, j) — 1 if г = j and 1(г, j) = 0 otherwise. Finally, if {A, +, x, 0,1) 
is positive or idempotent, then so are matrices over this semiring, but this does not 
hold for commutativity. Note in particular that ordinary real-valued, square matrices 
of the same order form a semiring. 

186 
SEMIRING VALUATION ALGEBRAS 
5.2 SEMIRINGS AND ORDER 
The substructure ( A, +) is a commutative monoid with neutral element, which makes 
it always possible (Gondran & Minoux, 2008) to introduce a canonical preorder (see 
Définition A.2 in the appendix of Chapter 1). We define for a,b £ A: 
a < b if, and only if 3c £ A : a + с = b. 
(5.3) 
Lemma 5.1 The Relation (5.3) is a preorder, i.e. reflexive and transitive. 
Proof: Reflexivity follows from the existence of a neutral element. Since a + 0 = a 
we have a < a for all a £ A. To prove transitivity, let us assume that a < b and 
b < d. Consequently, there exist c,d £ A such that a + с = b and b + c' = d. We 
have a + c + c' = d and therefore a < d. 
m 
The following lemma ensures that the canonical preorder is compatible with both 
semiring operations. 
Lemma 5.2 The canonical preorder of a semiring (A, +, x, 0,1} satisfies: 
1. a <b implies that a + с < b + с for all a,b,c £ A; 
(SP1) 
2. a < b implies that a x с < b x с and e x a < e x bfor all a,b,c G A; (SP2) 
3. 0<a,b<a 
+ bfor all a,b € A. 
(SP3) 
Proof: 
1. Assume a < b, i.e. there exists x £ A such that a + x = b. Then, (a + c)+x = 
b + с and therefore a + с < b + c. 
2. Assume a < b, i.e. there exists x £ A such that a + a; = b. Then ( a | i ) x c = 
(a x c) + (x x c) = b x с and therefore ax с <bxc. 
The proof of the second 
statement is symmetric. 
3. Since 0 + b = b we clearly have 0 < b. Using Property (SP1) we further derive 
a + 0 < a + b. Hence, a < a + b and a similar argument shows b < a + b. 
m 
The canonical preorder of a semiring is in general not antisymmetric. Take for 
example the arithmetic semiring of integers (Z, +, ·,0,1) and observe that a < b 
and b < a =£> a = b. This is a consequence of the existence of additive inverses. 
Antisymmetry is therefore compatible with the existence of additive inverses or, in 
other words, with the ring structure of (A, +). This is also indicated by Property (SP3) 
of the above lemma. However, if the canonical preorder is antisymmetric, it is called 
a partial order (see Definition A.2 in the appendix of Chapter 1) and the semiring 
becomes a dioid (Gondran & Minoux, 2008). A sufficient condition is idempotency. 

SEMIRINGS AND ORDER 
187 
Lemma 5.3 If (A, +, x, 0,1) is an idempotent semiring, we may characterize the 
canonical preorder (5.3) as 
a < b if and only if a + b — b. 
(5.4) 
Proof: It is sufficient to show that a < b according to (5.3) implies a + b — b. 
Suppose a < b, i.e. there exists с G A such that a + с = b. We then have by 
idempotency a + b = a + a + c = a + c = b. 
m 
Lemma 5.4 The Relation (5.4) is a partial order. 
Proof: Since the two relations (5.3) and (5.4) are equivalent in case of an idem-
potent semiring, we obtain reflexivity and transitivity from Lemma 5.1. To prove 
antisymmetry, we assume that a < b and b < a, i.e. a + b = b and b + a = a. 
Consequently, a = a + b — b and therefore a = b. 
m 
An idempotent semiring is therefore always a dioid and we refer to Relation (5.4) 
as its canonical partial order. 
Example 5.9 We already pointed out that the canonical preorder in the arithmetic 
semiring (Z, +, ·, 0,1} is not antisymmetric and therefore not a partial order. Restrict-
ing this semiring to non-negative integers (N U {0}, +, ·,0,1) turns the canonical 
preorder into a partial order. This is an example of a dioid that is not idempotent 
and shows that idempotency is indeed only a sufficient condition for a canonical 
partial order. The examples 5.2 to 5.8 are all idempotent and thus dioids. However, 
it is important to remark that the canonical semiring order does not necessarily 
agree with the natural order between number. Take for example the tropical semiring 
(N U {0, oo}, min, +, со, 0) where the relation (5.4) becomes a < b if, and only if, 
min{a, b} = b. Here, the canonical semiring order corresponds to the inverse of the 
natural order between natural numbers. 
Two further properties of idempotent semirings are listed in the following lemma. 
In particular, we learn from (SP5) why all idempotent semirings in the example 
catalogue of Section 5.1 are positive. 
Lemma 5.5 Let (A, +, x, 0,1) be an idempotent semiring. 
I. We have a + b = sup{a, 6} with respect to the canonical order. 
(SP4) 
2. The semiring is positive. 
(SP5) 

188 
SEMIRING VALUATION ALGEBRAS 
Proof: 
1. According to Property (SP3) a,b < a + b. Let с е A be another upper bound 
for a and 6, i.e. a < с and i> < c. We conclude from a + c = с and b + c= с that 
(a + c) + {b + c) = c + c and by idempotency (a + b) + с = с. Consequently, 
a + b < с which implies that о + b is the least upper bound of a and b. 
2. Suppose that a+b = 0. Applying Property (SP3) we obtain 0 < a < a+b = 0 
and by transitivity and antisymmetry we conclude that a = 0. Similarly, we 
prove 6 = 0. 
■ 
Definition 5.3 If a semiring (A, +, x, 0,1) satisfies a + 1 = lfor all a e A, then 
it is called bounded semiring. If also commutativity holds, the semiring is called 
c-semiring (constraint semiring). 
This definition comes from (Mohri, 2002; Bistarelli et al, 2002) and characterizes 
bounded semirings by the fact that the unit element is absorbing with respect to 
addition. Also, bounded semirings are close to simple semirings in (Lehmann, 1976). 
We first remark that bounded semirings are idempotent, since 1 + 1 = 1 implies that 
о + a = a for all a e A. This follows from distributivity: a + a = lx{a 
+ a) = 
(1 x a) + (1 x a) = (1 + 1) x a = 1 x a = a. Consequently, the relation < is a 
partial order which furthermore satisfies the following properties. 
Lemma 5.6 Let {A, +, x, 0,1) be a bounded semiring. 
1. For all a,b € Awe have 0<axb<a,b<a 
+ b<l 
2.1fx 
is idempotent, a x 6 = inf {a, b}. 
Proof: 
1. From Definition 5.3 follows that a < 1 for all a e A. Because Property (SP3) 
still holds, it remains to be proved that a x b < a for all a,b Ç. A. This claim 
results from the distributive law since a + (a x b) = (a x 1) + (a x b) = 
a x (1 + b) = a x 1 = a. 
2. By Property (SP6) we have a x b < a,b. Let с be another lower bound of a 
and b, i.e. с < a and с < b. Then, by (SP2) ex b < ax b. Similarly, we derive 
from с < b that с — с х с < с x b and therefore by transitivity с < a x b. 
Thus, a x bis the greatest lower bound. 
■ 
With supremum and infimum according to (SP4) and (SP7), c-semirings with 
idempotent multiplication adopt the structure of a lattice according to Definition A.5 
in the appendix of Chapter 1. Moreover, the following theorem states that it is even 
distributive. This has been remarked by (Bistarelli et ai, 1997), but in contrast to this 
reference it is not necessarily complete since we do not assume infinite summation 
in the definition of a c-semiring. 
(SP6) 
(SP7) 

SEMIRINGS AND ORDER 
189 
Theorem 5.1 A c-semiring with idempotent multiplication is a bounded, distributive 
lattice with a + b = sup{a, b} = a V b and a x b = inf {a, b} = a A b. 
Proof: It remains to prove that the two operations + and x distribute over each 
other in the case of an idempotent c-semiring. By definition, x distributes over +. 
On the other hand, we have for a,b,c £ A 
(a + b) x (a + c) 
= 
a x (a + b) + e x (a + b) 
— (a x a) + (a x b) + (a x c) + (6 x c) 
= 
a + ax (b + c) + (b x c) 
= 
a x (l + (b + c)) + (bx c) 
= 
(a x 1) + (b x c) = a + (b x c). 
We therefore have a distributive lattice. Since a +1 = 1 and a x 0 = 0 for all a G A 
the lattice is bounded with bottom element 0 and top element 1. 
■ 
Example 5.10 The Boolean semiring of Example 5.2, the bottleneck semiring of 
Example 5.3, the tropical semiring of Example 5.4, the truncation semiring of Exam-
ple 5.6 and all t-norm semirings of Example 5.8 are c-semirings and therefore also 
bounded. Among them, multiplication is idempotent in the Boolean semiring and in 
the bottleneck semiring which therefore become bounded, distributive lattices. Note 
also that if all semirings are c-semirings, then so is the induced multidimensional 
semiring. A similar statement does not hold for the semiring of matrices. 
Example 5.11 (Semiring of Boolean Functions) Consider asetofreN 
preposi-
tional variables. Then, the set of all Boolean functions f : {0, l}r —> {0,1} forms a 
semiring with addition f + д = max{/, g} and multiplication f x g — min{/, g}, 
both being evaluated point-wise. If /o denotes the constant mapping to 0 and Д the 
constant mapping to 1, then /o is the zero element and Д the unit element of the 
semiring. The semiring is a c-semiring and since multiplication is also idempotent, 
this semiring is a distributive lattice. In particular, the semiring of Boolean functions 
with r = 0 corresponds to the Boolean semiring of Example 5.2 where the two and 
only elements /o and Д are identified with their values 0 and 1. 
Conversely to Theorem 5.1, we note that every bounded, distributive lattice (see 
Definition A.6 in the appendix of Chapter 1) is an idempotent semiring with join for 
+ and meet for x. The bottom element J_ of the lattice becomes the zero element and 
the top element T becomes the unit element. Thus, this semiring is even a c-semiring 
with idempotent multiplication. We therefore have an equivalence between the two 
structures and conclude that the powerset lattice of Example A.2 and the division 
lattice of Example A.3 are both c-semirings with idempotent multiplication. There 
are naturally many other semiring examples that have not appeared in this chapter. 
For example, we mention that every (unit) ring is a semiring with the additional 
property that inverse additive elements exist. In the same breath, fields are rings with 
multiplicative inverses. These remarks lead to further semiring examples such as the 
ring of polynomials or the Galois field. We also refer to (Davey & Priestley, 1990) for 

190 
SEMIRING VALUATION ALGEBRAS 
a broad listing of further examples of distributive lattices and to the comprehensive 
literature about semirings (Golan, 1999; Golan, 2003; Gondran & Minoux, 2008). 
5.3 SEMIRING VALUATION ALGEBRAS 
Equipped with this catalogue of semiring examples, we will now come to the main 
part of this chapter and show how semirings induce valuation algebras by a mapping 
from tuples to semiring values. This theory was developed in (Kohlas, 2004; Kohlas 
& Wilson, 2006; Kohlas & Wilson, 2008), who also substantiated for the first time the 
relationship between semiring properties and the attributes of their induced valuation 
algebras. In particular, we will discover that formalisms for constraint modelling 
are an important subgroup of semiring valuation algebras. For this reason, we sub-
sequently prefer the term configuration instead of tuple which is more common in 
constraint literature. In this context, a similar framework to abstract constraint satis-
faction problems was introduced by (Bistarelli et a/., 1997; Bistarelli et ai, 2002). 
The importance of such formalisms outside the field of constraint satisfaction was 
furthermore explored by (Aji, 1999; Aji & McEliece, 2000), who also proved the 
applicability of the Shenoy-Shafer architecture. However, this is only one of many 
conclusions to which we come by showing that semiring-based formalisms satisfy 
the valuation algebra axioms. 
To start with, consider a commutative semiring (A, +, x,0,1} and a set r of 
variables with finite frames. A semiring valuation φ with domain s С г is defined to 
be a function that associates a value from A with each configuration x e i ! s , 
ф : Ω5 -¥ A. 
Remember that Ω$ = {о} such that a semiring valuation on the empty domain is 
ф(о) G A. We subsequently denote the set of all semiring valuations with domain s 
by Ф8 and use Ф for all semiring valuations whose domains belong to the powerset 
lattice D = V(r). Next, the following operations in (Ф, D) are introduced: 
1. Labeling: Ф -> D: ά(φ) = s if φ G Φ8. 
2. Combination: Φ χ Φ —> Φ: for φ, ψ G Φ and χ G Ωά(φ)υά(·ψ) w e define 
(φ®ψ)(χ) 
= 
ф{х.Ы(ф)) χ V(xMV,)). 
(5.5) 
3. Projection: Ф x D -»■ Ф: for ф Е Ф, t С ά(φ) and x G Ω4 we define 
^'(x) = 
Σ 
ф(х,у). 
(5.6) 
Note that the definition of projection is well defined due to the associativity and 
commutativity of semiring addition and the finiteness of the domains. We now arrive 

SEMIRING VALUATION ALGEBRAS 
191 
at the central theorem of this chapter which states that we obtain a valuation algebra 
from every commutative semiring through the above generic construction: 
Theorem 5.2 A system of semiring valuations (Φ, D) with respect to a commutative 
semiring (A, +, x, 0,1) with labeling, combination and projection as defined above, 
satisfies the axioms of a valuation algebra. 
Proof: We verify the Axioms (Al) to (A6) of a valuation algebra given in Section 
1.1. Observe that the labeling (A2), projection (A3) and domain (A6) properties are 
immediate consequences of the above definitions. 
(Al) Commutative Semigroup: The commutativity of combination follows directly 
from the commutativity of the x operation in the semiring A and the definition 
of combination. To prove associativity, assume that φ, ψ and η are valuations 
with domains ά(φ) — s, ά(ψ) = t and ά(η) = и, then for x € f2sUtUu 
(Ф ® (V ® J?))(X) 
= 
0(x i s) x (Ф <8> ry)(xitUu) 
= Ф(*и) x 0M(x
4tUu)
4t) x ??((x
iiUu)
iu)) 
= φ{χ
ιη x (ν>(χ
4*) χ rç(x
iu)) 
= 
ф(хи) х -ф(х.и) х »?(х4")· 
The same result is obtained in exactly the same way for {{ф <8> гр) <g> rç)(x) 
which proves associativity. 
(A4) Transitivity: Transitivity of projection means simply that we can sum out 
variables in two steps. That is, if t Ç s Ç ά(φ) = и, then, for all x S Ω<, 
(<^(x) = Σ ^'(x»y) = Σ 
Σ Ф(*,у,*) 
(y,z)en„_t 
(A5) Combination: Suppose that φ has domain t and ψ domain и and x G ΩΑ, 
where t Ç s С t U в. Using the distributivity of semiring multiplication over 
addition, we obtain for x € i l s , 
(</>®V)is(x) - 
Σ 
(^®^)(Х>У) 
y€fi ( t U u )_ s 
yenu_s 
= φ(^)χ 
Σ 
iP(xisnu,y) 
yen„_s 
= 
0(xiÉ) x visn"(xisn") = (<A®Visnu)(x)· 

192 
SEMIRING VALUATION ALGEBRAS 
To sum it up, a simple mapping from configurations to the values of a commutative 
semiring provides sufficient structure to give rise to a valuation algebra. The listing 
of semirings given in the two foregoing sections served to exemplify the semiring 
concepts introduced beforehand. We are next going to reconsider these semirings in 
order to show which valuation algebras they concretely induce. We will meet familiar 
instances such as arithmetic potentials or indicator functions, but also many new 
instances that considerably extend the valuation algebra catalogue of Chapter 1. 
5.4 
EXAMPLES OF SEMIRING VALUATION ALGEBRAS 
If we consider the arithmetic semiring (R>n, +, ·, 0,1) of non-negative real num-
bers presented in Section 5.1, we come across the valuation algebra of arithmetic 
potentials introduced as Instance 1.3. It is easy to see that both operations defined 
for semiring valuations correspond exactly to the operations for arithmetic potentials 
in equation (1.10) and (1.11). Therefore, the proof of Theorem 5.2 also delivers the 
promised argument that arithmetic potentials indeed satisfy the valuation algebra ax-
ioms. Moreover, since the arithmetic semiring can be built on complex numbers, we 
also come to the conclusion that the formalism used for the discrete Fourier transform 
in Instance 2.6 forms a valuation algebra. Similar statements hold for the valuation 
algebras of indicator functions, Boolean functions, crisp constraints or the relational 
algebra that are induced by the Boolean semiring ({0,1}, max, min, 0,1). Again, if 
we replace semiring addition and multiplication in equation (5.5) and (5.6) by the 
corresponding operations max and min, we obtain the combination and projection 
rules given in Instance 1.1. 
■ 5.1 Weighted Constraints - Spohn Potentials - GAI Preferences 
Besides crisp constraints, alternative constraint systems may be derived by ex-
amining other semirings: the tropical semiring (N U {0, oo}, min, +, oo, 0) of 
Example 5.4 induces the valuation algebra of weighted constraints (Bistarelli 
et al., 1997; Bistarelli et ai, 1999). This formalism also corresponds to Spohn 
potentials (Spohn, 1988) which have been proposed as a dynamic theory of 
graded belief states based on ordinary numbers. (Kohlas, 2003) delivers the 
explicit proof that weighted constraints satisfy the valuation algebra axioms. 
But in our context of semiring valuation algebras, this insight follows naturally 
from Theorem 5.2. No further proof is necessary. We again refer to later sec-
tions for concrete applications based on weighted constraints that in turn give 
birth to further inference problems. Here, we content ourselves with a small 
example on how to compute with weighted constraints. 
Letr = {A, ß , С} be a set of three variables with finite frames Ω^ = {α,α}, 
ΩΒ = {b, b} and Ωο = {с, с}. We define two weighted constraints c\ and c<i 
with domain d(c{) = {A, B} and d(c2) = {В, С}: 

EXAMPLES OF SEMIRING VALUATION ALGEBRAS 
193 
ci = 
A 
a 
a 
ä 
ä 
В 
b 
b 
b 
b 
4 
8 
0 
2 
Cl 
= 
В 
b 
b 
b 
b 
с 
с 
с 
с 
с 
оо 
1 
1 
7 
We combine c\ and c<i and project the result to {A, C} 
C3 
Cl ®C2 
A 
a 
a 
a 
a 
ä 
a 
U 
a 
В 
b 
b 
b 
b 
b 
b 
b 
b 
с 
с 
с 
с 
с 
с 
с 
с 
с 
оо 
5 
9 
15 
оо 
1 
3 
9 
Л{А,С} __ 
сз 
— 
А 
о 
а 
а 
а 
С 
с 
с 
с 
с 
оо 
5 
3 
1 
Alternatively to the tropical semiring, we may also take the arctic semiring 
(E U {—оо}, max, +, -оо, 0} of Example 5.5 to build weighted constraints, 
which then corresponds to the formalism of generalized additive independent 
preferences (GAIpreferences) (Fishburn, 1974; Bacchus & Adam, 1995). The 
modification of the above example to weighted constraints induced by the arctic 
semiring is left to the reader. 
5.2 Possibility Potentials - Probabilistic Constraints - Fuzzy Sets 
The very popular valuation algebra ofpossibility potentials is induced by the tri-
angular norm semirings ([0,1], max, t-norm, 0,1) of Example 5.8. Historically, 
possibility theory was proposed by (Zadeh, 1978) as an alternative approach to 
probability theory, and (Shenoy, 1992a) furnished the explicit proof that this 
formalism indeed satisfies the valuation algebra axioms. This was limited to 
some specific t-norms and (Kohlas, 2003) generalized the proof to arbitrary 
t-norms. However, thanks to the generic construction of semiring valuations, 
the general statement that possibility potentials form a valuation algebra under 
any t-norm follows immediately from Theorem 5.2. We also refer to (Schiex, 
1992) which unified this and the foregoing instance to possibilistic constraints. 
To give an example of how to compute with possibility potentials, we settle 
for the use of the Lukasiewicz t-norm defined as a x b = max{o + b — 1,0} for 
all о € [0,1]. Again, let r = {А, В, С} be a set of three variables with finite 
frames QA = {a, ä}, Ωβ = {b, b} and Q,c = {с, с}. We define two possibility 

194 
SEMIRING VALUATION ALGEBRAS 
potentials p\ andp2 with domain d(pi) = {A, B} and d(p2) = {B, C}: 
Pi 
A 
a 
a 
U 
ä 
В 
b 
b 
b 
b 
0.6 
0.4 
0.3 
0.7 
P2 
= 
В 
b 
b 
b 
b 
с 
с 
с 
с 
с 
0.2 
0.8 
0.9 
0.1 
We combine pi and p2 and project the result to {A, C} 
i>3 = Pi ® Pi 
= 
A 
a 
a 
a 
a 
ä 
ä 
ä 
a 
В 
b 
b 
b 
b 
b 
b 
b 
b 
с 
с 
с 
с 
с 
с 
с 
с 
с 
0 
0.2 
0 
0.2 
0 
0.1 
0.6 
0 
п1{А,С} 
_ 
Рз 
— 
А 
а 
а 
а 
а 
С 
с 
с 
с 
с 
0 
0.2 
0.6 
0.1 
Particularly important among these t-norms is ordinary multiplication. The val-
uation algebra induced by the corresponding probabilistic semiring of Example 
5.8 is known as the formalism of probabilistic constraints (Bistarelli & Rossi, 
2008) от fuzzy subsets (Zadeh, 1978). An example can easily be obtained from 
the arithmetic potentials presented as Instance 1.3. Combination is identical for 
both instances such that only projection, which now consists of maximization 
instead of summation, has to be recomputed. 
5.3 Set-based Constraints - Assumption-based Constraints 
The valuation algebra induced by the powerset lattice of Example A.2 in the 
appendix of Chapter 1 corresponds to the formalism of set-based constraints 
(Bistarelli et ai, 1997). Imagine two propositional variables A\ and A2. We then 
build the powerset lattice from their configuration set ^({(0,0),..., (1,1)}) 
and obtain a complete, distributive lattice according to Example A.2. To in-
troduce valuations that assign values from this semiring, let r = {А, В, С} 
be a set of three variables with finite frames Ω^ = {a,ä}, Ωβ = {b,b} 
and Clc = {c,c}. We define two set-based constraint c\ and c2 with domain 
d(ci) = {A, B} and d(c2) = {B, C}\ 
c\ 
А 
В 
a 
b 
a 
b 
ä 
b 
ä 
b 
{(0,0), (1,0)} 
0 
{(1Д)} 
{(1,0), (0,1)} 
c2 = 
В 
С 
6 
с 
b 
с 
b 
с 
b 
с 
{(0,0), (0,1), (1,0)} 
{(0,1)} 
0 
{(0,0), (0,1)} 

PROPERTIES OF SEMIRING VALUATION ALGEBRAS 
195 
We combine c\ and C2 and project the result to {A, C} 
A 
a 
a 
a 
a 
ä 
ä 
ä 
ä 
В 
b 
b 
b 
b 
b 
b 
b 
b 
с 
с 
с 
с 
с 
с 
с 
с 
с 
{(0,0), (1,0)} 
0 
0 
0 
0 
0 
0 
{(0,1)} 
А 
а 
а 
а 
а 
С 
с 
с 
с 
с 
{(0,0), (1,0)} 
0 
0 
{(0,1)} 
Let us give a particular interpretation to this example: The variables A and В 
are considered as assumptions. Now, observing that (1,0) e ci (a, 6), we may 
say that the configuration (a, fr) is possible under the assumption that A holds 
but not B. Since (0,0) G ci(a, fe) too, the configuration (α, 6) is still possible 
if neither A nor В holds. The constraint c^ specifies that (b, c) is possible if 
at most one of the assumptions is true. Combining the two constraints c\ and 
C2 gives {(0,0), (1,0)} for the configuration (a, b, c). The assignment (0,1) is 
missing since ci does not hold under this assumption. So, a set-based constraint 
can also be understood as an assumption-based constraint which relates this 
formalism to assumption-based reasoning (de Kleer et al, 1986). 
The number of valuation algebras obtained via the construction of semiring val-
uations is enormous. In fact, we obtain a different valuation algebra from every 
commutative semiring and the instances presented just above are only the tip of the 
iceberg. Generic constructions also produce formalisms for which we not even have a 
slight idea of possible application fields. Nevertheless, we understand their algebraic 
properties and also possess efficient algorithms for their processing thanks to the local 
computation framework. One such example could be the valuation algebra induced 
by the division lattice from Example A3. 
5.5 
PROPERTIES OF SEMIRING VALUATION ALGEBRAS 
Throughout the two foregoing chapters about local computation, we introduced ad-
ditional properties of valuation algebras which are interesting for computational and 
semantical purposes. The potential presence of these properties had to be verified 
for each formalism separately. A further gain of generic constructions is that such 
properties can now be verified for whole families of valuation algebras instances. Fol-
lowing this guideline, we are now going to investigate which mathematical attributes 
are needed in the semiring to guarantee the presence of neutral and null elements in 
the induced valuation algebra. Again, the more technical discussion of division for 
semiring valuations is postponed to the appendix of this chapter. 

196 
SEMIRING VALUATION ALGEBRAS 
5.5.1 
Semiring Valuation Algebras with Neutral Elements 
Section 3.3 introduced neutral elements as a (rather inefficient) possibility to initialize 
join tree nodes or, in other words, to derive a join tree factorization from a given 
knowledgebase. On the other hand, neutral elements represent an important seman-
tical aspect in a valuation algebra by expressing neutral knowledge with respect to 
some domain. Since we presuppose the existence of a unit element in the underlying 
semiring, we always get a neutral elements in the induced valuation algebra by the 
definition es(x) = 1 for all x e i l , and s e D. This is the neutral valuation in the 
semigroup Φ3 with respect to combination, i.e. for all φ € Φ3 we have 
es <8> Φ = es(x) x </>(x) = l x 0(x) = φ. 
These neutral elements satisfy Property (A7) of Section 3.3: 
(A7) Neutrality: We have by definition for all x e OsUt: 
(ea ® et)(x) = e s(x i s) x е((х^) = 1 x 1 = 1. 
(5.7) 
5.5.2 
Stable Semiring Valuation Algebras 
Although all semiring valuation algebras provide neutral elements, they generally 
are not stable, i.e. neutral semiring valuations do not necessarily project to neutral 
elements again. This has already been remarked in Instance 3.2 in Section 3.3.1 where 
it is shown that the valuation algebra induced by the arithmetic semiring of Example 
5.1 is not stable. However, it turns out that idempotent addition is a sufficient semiring 
property to induce a stable valuation algebra. 
(A8) Stability: If the semiring is idempotent, we have e\l = et for t Ç s and X G S 1 S 
ei'(x) = 
Σ 
е*(х'У) = 
Σ 
* = *· 
(5·8) 
Let us summarize the insights of this section: 
Theorem 5.3 All semiring valuation algebras provide neutral elements. If further-
more the semiring is idempotent, then the induced valuation algebra is stable. 
In particular, all elements induced by c-semirings (or bounded, distributive lat-
tices) are stable since these properties always imply idempotency. From this new 
perspective, we easily confirm the properties related to neutral elements of indicator 
functions and arithmetic potentials that have been derived in Instance 3.1 and 3.2. 
Let us consider some more instances: 
■ 5.4 Weighted Constraints and Neutral Elements 
The valuation algebra of weighted constraints from Instance 5.1 is induced by 
the tropical semiring of Example 5.4 which has the number 0 as unit element. 

PROPERTIES OF SEMIRING VALUATION ALGEBRAS 
197 
The neutral weighted constraint es for the domain s £ D and x e i!s is 
therefore given by es(x) = 0. Further, this semiring is idempotent, which 
directly implies stability in the valuation algebra of weighted constraints. 
■ 5.5 Probabilistic Constraints and Neutral Elements 
The valuation algebra of probabilistic constraints from Instance 5.2 is induced 
by the multiplicative t-norm semiring of Example 5.8 which has the number 1 
as unit element. The neutral probabilistic constraint es for the domain s £ D 
and x 6 Qs is therefore given by es(x) = 1. Further, this semiring is idempo-
tent, which again implies stability in this valuation algebra. 
This gives a first indication of how easily the algebraic properties of a formalism 
can be analysed through the perspective of generic construction. A second interest-
ing valuation algebra property that we are now going to study from the semiring 
perspective is the existence of null elements. 
5.5.3 
Semiring Valuation Algebras with Null Elements 
Null elements have been introduced in Section 3.4 for pure semantical purposes. They 
represent contradictory information with respect to a given domain and are significant 
to interpret the possible results of inference problems. Since every semiring contains 
a zero element, we can directly advise the candidate for a null semiring valuation in 
Φθ. This is zs{x) = 0 for all x € ils, and it clearly holds for φ € Φ3 that 
4>®za(x) = φ(χ) x zs(x) — 0 
and therefore φ <8> zs = zs. But this is not yet sufficient because the nullity axiom (A9) 
must also be satisfied, and this comprises two requirements. First, remark that null 
elements always project to null elements. More involved is the second requirement 
which claims that only null elements project to null elements. Positivity of the 
semiring is a sufficient condition to fulfill this axiom. 
(A9) Nullity: In a positive semiring φ^ = zt always implies that φ = za. Indeed, 
let Χ Ε Ω ( with t Ç s = ά(φ). Then, 
zt(x) = 0 = <^(x) = 
^ 
0(x,y) 
(5.9) 
implies that φ(χ, y) = 0 for all у € fls-t and consequently, ф = zs. 
To summarize: 
Theorem 5.4 Semiring valuation algebras induced by commutative, positive semir-
ings provide null elements. 
Remember that due to Property (SP5), idempotent semirings are always positive 
and therefore induce valuation algebras with null elements. This again confirms what 

198 
SEMIRING VALUATION ALGEBRAS 
we have found out in Section 3.4: indicator functions are induced by the Boolean 
semiring of Example 5.2 and therefore provide null elements. The valuation algebra 
of arithmetic potentials is induced by the arithmetic semirings of Example 5.1 that 
are, in certain cases, positive and, in others, not. In the latter case, no null elements 
will be present. Let us add some further examples of valuation algebras with null 
elements: 
■ 5.6 Weighted Constraints and Null Elements 
The valuation algebra of weighted constraints from Instance 5.1 is induced 
by the tropical semiring of Example 5.4 which has oo as zero element. This 
semiring is idempotent, thus positive and therefore induces the null element 
zs(x.) = oo for the domain s 6 D and x S Cls. 
■ 5.7 Possibility Potentials and Null Elements 
Independently of the chosen t-norm, all t-norm semirings of Example 5.8 are 
idempotent and therefore positive. Consequently, they all provide the same null 
element zs(x) = 0 for the domain s G D and x e S î s . 
■ 5.8 Set-based Constraints and Null Elements 
The valuation algebra of set-based constraints from Instance 5.3 is induced by 
the powerset lattice of Example A.2. This semiring is again positive with the 
empty set as zero element. It therefore induces the null element zs (x) = 0 for 
the domain s G D and x e i ! s . 
In the appendix of this chapter, we provide a detailed analysis of division and 
identify the semiring properties that either lead to separative, regular or idempotent 
valuation algebras. Also, we revisit normalization or scaling which is an important 
application of division in valuation algebras. Figure 5.1 summarizes the properties 
related to neutral and null elements for each semiring valuation algebra studied in this 
chapter. The properties related to division are shown in Figure E.4 in the appendix. 
5.6 SOME COMPUTATIONAL ASPECTS 
Examples of inference problems based on semiring valuations have already been con-
sidered in Instance 2.1 and Instance 2.4. Further examples from constraint reasoning 
will be listed in Chapter 8. Given a multi-query inference problem with a knowledge-
base of semiring valuations, we may always apply the Shenoy-Shafer architecture 
for the computation of the queries. The applicability of the other architectures from 
Chapter 4 naturally depends on the presence of a division operator in the semiring 
valuation algebra. Let us consider the complexity of the Shenoy-Shafer architecture 
for semiring valuations in more detail. A possible weight predictor for semiring val-

SOME COMPUTATIONAL ASPECTS 
199 
1.1 
1.3 
1.3 
5.1 
5.2 
5.3 
Instance 
Indicator Functions 
Arithmetic Potentials on R>o 
Arithmetic Potentials on R, С 
Weighted Constraints 
Possibility Potentials 
Set-based Constraints 
Neutral Elements 
V 
v/ 
V 
V 
V 
yj 
Stability 
V 
о 
о 
у/ 
V 
у/ 
Null Elements 
У 
у/ 
о 
V 
V 
V 
Figure 5.1 Semiring valuation algebras with neutral and null elements. 
uations was already given in equation (3.17). Inserted into equation (4.8), we obtain 
for the time complexity of the Shenoy-Shafer architecture 
0[\V\-deg-d 
ιω* + 1 
(5.10) 
where d denotes the size of the largest variable frame. Concerning the space com-
plexity, there is an important issue with respect to the general bound given in equation 
(4.9). In the Shenoy-Shafer architecture, the message sent from node г to neighbor 
j G ne(i) is obtained by first combining the node content of г with all messages 
received from all other neighbors of i except j . Then, the result of this combination 
is projected to the intersection of its domain and the node label of neighbor j . For 
arbitrary valuation algebras, we must assume that the complete combination has to 
be computed before the projection can be evaluated. This creates an intermediate 
factor whose domain is bounded by the node label X(i). In the general space com-
plexity of equation (4.9) this corresponds to the first term. However, when dealing 
with semiring valuations, the computation of the complete combination can be omit-
ted. For illustration, assume a set of semiring valuations {φχ,... ,фп} С Ф with 
s = ά(φχ) U ... U ά(φη), t Ç s and x e Qs, then the projection 
φ(χ^) = (φ, 
*n)U(xU) 
Σ(* 
..®φη)(χ^,ζ) 
ζ€Ωβ_ 
can be computed as follows: We initialize V(y) = 0 for all y € ΩΕ and compute for 
each configuration x G Ω5 
φ(χ) 
L(X ΙΊ(Φι)\ 
X . . . X 
»(χ-Ы(фп) ). 
Then, the semiring value <Д(х) is added to ^(х^а). Clearly, this procedure determines 
ψ completely. Since only one value ф(х) exists at a time, the space complexity is 

200 
SEMIRING VALUATION ALGEBRAS 
bounded by the domain t = d(ip). Applying this technique in the Shenoy-Shafer 
architecture therefore reduces the space complexity to the domain of the messages, 
which in turn are bounded by the separator width sep*. Altogether, we obtain for the 
space complexity of the Shenoy-Shafer architecture applied to semiring valuations 
o{\V\-dsep'y 
(5.11) 
Note also that the time complexity is not affected by this procedure. 
This brings the study of semiring valuation algebras to a first end. Semiring 
valuations will again take center stage in Chapter 8 where it is shown that the inference 
problem turns into an optimization task when dealing with valuation algebras induced 
by idempotent semirings. In the following section, we focus on a second generic 
construction that is closely related to semiring valuation algebras. But instead of 
mapping configurations to semiring values, we consider sets of configurations that 
are mapped to semiring values. An already known member of this new family of 
valuation algebras is the formalism of set potentials from Instance 1.4 in Chapter 1. 
5.7 SET-BASED SEMIRING VALUATION ALGEBRAS 
The family of semiring valuation algebras is certainly extensive, but there are nev-
ertheless important formalisms that do not admit this particular structure. Some of 
them have already been mentioned in Section 1 ; for example, densities or set poten-
tials. The last formalism is of particular interest in this context. Set potentials map 
configuration sets on non-negative real numbers, whereas both valuation algebra op-
erations reduce to addition and multiplication. This is remarkably close to the buildup 
of semiring valuation algebras, if we envisage a generalization from configurations 
to configuration sets. In doing so, we hit upon a second family of valuation algebra 
instances that covers such important formalisms as set potentials or possibility mea-
sures. This theory again produces a multiplicity of new valuation algebra instances 
and also puts belief functions into a more general context. Here, we confine ourselves 
to a short treatment of set-based semiring valuations, leaving out an inspection of the 
more complex topic of division as performed for semiring valuations in the appendix 
of this chapter. 
Let us again consider a commutative semiring (E, +, x, 0,1) and a countable set 
r of variables with finite frames. A set-based semiring valuation φ with finite domain 
s Ç r is defined to be a function that associates a semiring value from E with each 
configuration subset of Ω3, 
φ : V{QS) -> E. 
The set of all set-based semiring valuations with domain s will subsequently be de-
noted by Φβ and we use Φ for all possible set-based semiring valuations whose domain 
belongs to the lattice D = V(r). Next, the following operations are introduced: 
1. Labeling: Φ -> D: d(<j>) = s if ф е Фа. 

SET-BASED SEMIRING VALUATION ALGEBRAS 
201 
2. Combination: Φ x Φ -> Φ: for φ, ψ G Φ and A Ç Ω^,^υ^ψ) we define 
(0®V)(^) = 
Σ 
Ф(В)*Ф(С)· 
(5.12) 
ВмС=Л 
3. Projection: Ф x D —> Ф: for ф G Ф, t Ç <ί(ψ) and Л С Ω4 we define 
^ ' ( Л ) 
= 
Σ 
φ{Β). 
(5.13) 
7Г,(В) = Л 
Similar to Instance 1.4 we again use the operations from the relational algebra 
(i.e. projection and natural join) to deal with configuration sets. The advantages 
become apparent in the proof of the following theorem. Since relations are known to 
form a valuation algebra, we may benefit from their algebraic properties to verify the 
valuation algebra axioms for set-based semiring valuations. This makes the following 
proof particularly elegant. 
Theorem 5.5 A system of set-based semiring valuations (Φ,-D) with respect to a 
commutative semiring (E, +, x, 0,1) with labeling, combination and projection as 
defined above, satisfies the axioms of a valuation algebra. 
Proof: We verify the valuation algebra axioms given in Section 1.1. The labeling 
(A2) and projection axioms (A3) are direct consequences of the above definitions. 
(A 1 ) Commutative Semigroup: Commutativity of combination follows directly from 
the commutativity of semiring multiplication and natural join. To prove asso-
ciativity we assume that ф € Φβ, ψ G Φί( ν е Фи and A Ç OsUtUu 
(φ ® (ψ ® v))(A) 
= 
Σ 
φ(Β) x (il>®v)(E) 
B M E = A 
= 
Σ 
φ(Β)χ 
Σ 
^(C)xv{D) 
BtxiE=A 
CtxD=E 
BÎXE=A 
Cc<D=E 
Σ 
Ф(В) х ф(С) х !/(£>). 
BtxCtxD=A 
The same result is obtained in exactly the same way for {{ф ® ф) (g) и){А) 
which proves associativity of combination. 
(A4) Transitivity: For ф G Ф with s Ç t ç ά(φ) we have 
(Ф^ЧА) = Σ ^(β) = Σ 
Σ κ°) 
ivs(B)=A 
K,{B)=ATXt(C) = B 
Σ 
№) = Σ 
Φ&) = ^и)· 
7Γ„(π,(σ))=Λ 
*,(C)=A 

202 
SEMIRING VALUATION ALGEBRAS 
Observe that we used the transitivity of projection for relations. 
(A5) Combination: Suppose that φ e Φ.,,ί/' € ΦίαηάΑ Ç Ωζ, where s ç z С sut. 
Then, using the combination property of the relational algebra we obtain 
{Φ®ΨΫ*{Α) = 
Σ 
Φ®Ψ(Β) = 
Σ 
Σ 
<KC)XI>(D) 
πζ(Β)=Α 
πζ(Β)=Α 
CtxlD=B 
Σ 
ф(С) х φ{Ό) = 
Σ 
Φ(°) χ №) 
nz(Cc<D)=A 
C\xs-ntnz(D)=A 
= Σ №)* 
Σ 
^(β) 
Σ 
^(c) x^inz(£") = Φ®ψ1ίηζ(Α). 
C\x\E=A 
(Α6) Domain: For 0 € Φ and x = d(0) we have 
Φ*(Α) = Σ №) = Φ(Α)-
В=А 
m 
Giving a first summary, commutative semirings possess enough structure to afford 
this second family of valuation algebras. The best known example of such an algebra 
are set potentials from Instance 1.4 (including their normalized variant called mass 
functions and belief functions), and it is indeed interesting to see them as a member 
of a more comprehensive family of formalisms. We list some further examples: 
■ 5.9 Possibility Measures 
(Zadeh, 1979) originally introduced possibility measures compatible to our 
framework of set-based semiring valuations over the product t-norm semiring 
of Example 5.8. But it turned out that possibility functions are completely 
specified by their values assigned to singleton configuration sets. Based on this 
insight, (Shenoy, 1992a) derived the valuation algebra of possibility potentials 
discussed as Instance 5.2. Working with possibility functions is therefore a bit 
unusual but we can deduce from the above theorem that they nevertheless form 
a valuation algebra themselves. 
■ 5.10 Disbelief Functions 
A disbelief function according to (Spohn, 1988; Spohn, 1990) is again com-
patible to our framework of set-based semiring valuations over the tropical 
semiring of Example 5.4. But similar to the foregoing instance, it was shown 
by (Shenoy, 1992b) that disbelief functions are completely specified by their 

PROPERTIES OF SET-BASED SEMIRING VALUATION ALGEBRAS 
2 0 3 
values assigned to singleton configuration sets. This corresponds to the valu-
ation algebra of weighted constraints discussed in Instance 5.1. It is therefore 
again not usual to work with disbelief functions in practice, although they also 
form a valuation algebra. 
There are further attempts to connect belief functions with fuzzy set theory that 
results in further instances of set-based semiring valuations over different t-norm 
semirings. See for example (Biacino, 2007; Pichon & Denoeux, 2008). 
5.8 
PROPERTIES OF SET-BASED SEMIRING VALUATION ALGEBRAS 
We are next going to investigate the necessary requirements for the underlying semi-
ring to guarantee neutral and null elements in the induced valuation algebra. 
5.8.1 Neutral and Stable Set-Based Semiring Valuations 
Derived from Instance 3.3 we identify the neutral element es for the domain s £ D: 
"■<"> = in "* "·"*· 
<5J4) 
I0, 
otherwise. 
Indeed, it holds for ф б Ф with d(<j>) = s that 
ф®е3{А) = 
J2 
Ф(В)хеа(С) 
BcxC=A 
= 
ф(А) x β(Ωβ) = φ{Α) χ 1 = φ{Α). 
The second equality follows since for all other values of С we have es(C) = 0. 
These elements also satisfy property (A7): 
(A7) Neutrality: On the one hand we have 
es®et(üsUt) 
= 
Σ 
es{A)xet{B) 
= es{üs) x et(Qt) = 1. 
AtxiB=Si sut 
On the other hand, if A ix В С ΩΑϋί, then either А с ils or В С ΩΕ. So, at 
least one factor corresponds to the zero element of the semiring and therefore 
es <g) et(C) = 0 for all С С Ωβυί. 
Set-based semiring valuation algebras are always stable. 
(A8) Stability: On the one hand we have 
7Tt(A)=nt 

204 
SEMIRING VALUATION ALGEBRAS 
The second equality holds because es(Qs) = 1 is the only non-zero term 
within this sum. On the other hand, we have ejf(A) = 0 for all Л С il( 
because es(Cls) does not occur in the sum of the projection. 
These result are summarized in the following theorem: 
Theorem 5.6 Set-based semiring valuation algebras provide neutral elements and 
are always stable. 
5.8.2 
Null Set-Based Semiring Valuations 
Because all semirings possess a zero element, we have for every domain s e D a 
valuation zs such that ф® zs — zs ® ф — zs. This element is defined as zs(A) — 0 
for all A ÇÇls. Indeed, we have for ф G Ф3, 
φ®ζ8{Α) = 
Σ 
Φ(Β) χ zs(C) = 0. 
BtxlC=A 
These candidates zs must additionally satisfy the nullity axiom that requires two 
properties. The first condition that null elements project to null elements is clearly 
satisfied. More involved is the second condition that only null elements project to 
null elements. Positivity is again a sufficient condition. 
(A9) Nullity: For φ € Φ5 and t Ç s, we have 
0 = φ*{Α) = 
Σ 
φ(Β) 
* t(B)=A 
implies that ф(В) = 0 for all В with nt(B) = A. Hence, ф — zs. 
Theorem 5.7 Set-based semiring valuation algebras induced by commutative, pos-
itive semirings provide null elements. 
A question unanswered up to now concerns the relationship between traditional 
semiring valuations and set-based semiring valuations. On the one hand, semiring 
valuations might be seen as special cases of set-based semiring valuations where 
only singleton configuration sets are allowed to have non-zero values. However, we 
nevertheless desist from saying that set-based semiring valuations include the family 
of traditional semiring valuations. The main reason for this is the inconsistency of 
the definition of neutral elements in both formalisms. This is also underlined by the 
fact that a semiring valuation algebra requires an idempotent semiring for stability, 
whereas all set-based semiring valuation algebras with neutral elements are naturally 
stable. We thus prefer to consider the two semiring related formalisms studied in this 
chapter as disjoint subfamilies of valuation algebras. 
5.9 
CONCLUSION 
A generic construction identifies a family of valuation algebra instances that share 
a common structure. On the one hand, this relieves us from verifying the axiomatic 

APPENDIX 
2 0 5 
system and algebraic properties individually for each member of such a family. On the 
other hand, it also helps to search for new valuation algebra instances. This chapter 
introduced two generic constructions related to semirings. Semiring valuations are 
mappings from configurations to values of a commutative semiring. This allows 
directly to assimilate the many important formalisms used in soft constraint reasoning 
into the valuation algebra framework and to immediately conclude that they all 
qualify for the application of local computation. Typical examples of such formalisms 
are crisp constraints, weighted constraints, probabilistic constraints, possibilistic 
constraint or assumption-based constraint, but it also includes other formalisms that 
are not related to constraint systems such as probability potentials. A closer inspection 
of semiring valuation algebras in general identified the sufficient properties of a 
semiring to induce valuation algebras with neutral and null elements or with a division 
and scaling operator (see appendix). This again discharges us from analysing each 
formalism individually. The second family of valuation algebras studies in this chapter 
are set-based semiring valuations, obtained from mapping sets of configurations to 
semiring values. Its best known member is the formalism of set potentials. 
Appendix: Semiring Valuation Algebras with Division 
The presence of inverse valuations is of particular interest because they allow the 
application of specialized local computation architectures. In Appendix D. 1 we iden-
tified three different conditions for the existence of inverse valuations in general. 
Namely, these conditions are separativity, regularity and idempotency. Following 
(Kohlas & Wilson, 2006), we renew these considerations and investigate the re-
quirements for a semiring to induce a valuation algebra with inverse elements. More 
precisely, it is effectual to identify the semiring properties that either induce separa-
tive, regular or idempotent valuation algebras. Then, the theory developed in Section 
4.2 can be applied to identify the inverse valuations. We start again with the most 
general requirement called separativity. 
E.1 SEPARATIVE SEMIRING VALUATION ALGEBRAS 
According to (Hewitt & Zuckerman, 1956), a commutative semigroup A with opera-
tion x can be embedded into a semigroup which is a union of disjoint groups if, and 
only if, it is separative. This means that for all a, b € A, 
a x b = axa 
= b x b 
(E.l) 
implies a = b. Thus, let {Ga : a e Y} be such a family of disjoint groups with 
index set Y, whose union 
G = [JGa 

206 
SEMIRING VALUATION ALGEBRAS 
is a semigroup into which the commutative semigroup A is embedded. Hence, there 
exists an injective mapping h : A —> G such that 
h(a x b) = 
h(a) x h(b). 
Note that the left-hand multiplication refers to the semigroup operation in A whereas 
the right-hand operation stands for the semigroup operation in G. If we identify every 
semigroup element a £ A with its image h(a) £ G, we may assume without loss of 
generality that A Ç G. 
Every group Ga contains a unit element, which we denote by fa. These units are 
idempotent since fax fa = fa. Let / £ G be an arbitrary idempotent element. Then, 
/ must belong to some group Ga. Consequently, / x / = / x /Q, which implies 
that / = fa due to equation (E.l). Thus, the group units are the only idempotent 
elements in G. 
Next, it is clear that fa x fß is also an idempotent element and consequently the 
unit of some group G7, i.e. fa x fß — / 7. We define a < ß if 
fa X fß 
= 
fa-
This relation is clearly reflexive, antisymmetric and transitive, i.e. a partial order 
between the elements of Y. Now, if faxfß 
= Л* then it follows that 7 < a, ß. Let 
δ £ Y be another lower bound of a and /?. We have fa x fs = fs and fßxfs 
= fs-
Then, / 7 x fs = fa x fß x fs = fa x fs = fs. So δ < η and 7 is therefore the 
greatest lower bound of a and β. We write 7 = a A β and hence 
fa X fß 
= 
faAß-
To sum it up, Y forms a semilattice, a partially ordered set where the infimum exists 
between any pair of elements. 
Subsequently, we denote the inverse of a group element a £ Ga by a - 1. Suppose 
a, a"1 e Ga and b, b'1 £ Gß, then (a x b) x (a - 1 x 6"1) = fa x fß = faAß, 
and 
it follows that 
( a x e ) 4 
= 
a^xb'1. 
Suppose now ax b £ G7. Thus (a x b)~l £ G7 and (o x b) x (a x fr)"1 = / 7. But 
as we have just seen / 7 = /αΛ/8, hence 7 = α Λ β and αχίι,α" 1 χίι" 1 € GaAß-
We next introduce an equivalence relation between semigroup elements of A and 
say that о = b if a and b belong to the same group Ga. This is a congruence relation 
with respect to x, since a = a' and b = b' imply that a x b = a' x b', which in turn 
implies that the congruence classes are semigroups. Consequently, A decomposes 
into a family of disjoint semigroups, 
A = U [a]. 

SEPARATIVE SEMIRING VALUATION ALGEBRAS 
2 0 7 
Also, the partial order of Y carries over to equivalence classes by defining [a] < [b] 
if, and only if, [a x b] = [a]. Further, we have [o x b] = [а] Л [b] for all a, b e A 
which shows that the semigroups [a] form a semilattice, isomorph to Y. We call 
the equivalence class [a] the support of a. Observe also that [0] = {0}. This holds 
because if a e [0] = G7 then a = 0 and / 7 = 0. Hence, a = a x /Ί = 0 x 0 = 0. 
The result of the support decomposition of A is summarized in Figure E. 1. The 
following definition given in (Kohlas & Wilson, 2006) is crucial since it summarizes 
all requirements for a semiring to give rise to a separative valuation algebra. Together 
with the requirement of a separative semigroup, it demands a decomposition that is 
monotonie under addition. This is needed to make allowance for the projection in 
equation (D.2), as shown beneath. 
G = \jGa 
A = \J[a] 
Figure E.l 
A separative semigroup A is embedded into a semigroup G that consists of 
disjoint groups Ga. The support decomposition of A is then derived by defining two semigroup 
elements as equivalent if they belong to the same group Ga ■ 
Definition E.4 A semiring {A, +, x, 0,1) is called separative, if 
• iis multiplicative semigroup is separative, 
• there is an embedding into a union of groups such that for all a,b G A 
[a] 
< 
[а + Ь]. 
(Е.2) 
The second condition expresses a kind of strengthening of positivity. This is the 
statement of the following lemma. 
Lemma E.7 A separative semiring is positive. 
Proof: From equation (E.2) we conclude [0] < [a] for all a G A. Then, assume 
a + b = 0. Hence, [0] < [a], [b] < [a + b] = [0] and consequently, a = b = 0. 
■ 
The following theorem states that a separative semiring is sufficient to guarantee 
that the induced valuation algebra is separative in the sense of Definition D.4. 
Theorem E.8 Let (Ф, D) be a valuation algebra induced by a separative semiring. 
Then (Ф, D) is separative. 

2 0 8 
SEMIRING VALUATION ALGEBRAS 
Proof: Since the semiring is separative, the same holds for the combination semi-
group of Φ, i.e. φ®φ = φ®φ = φ®φ implies φ = φ. Consequently, this semigroup 
can also be embedded into a semigroup that is a union of disjoint groups. In fact, 
the decomposition which is interesting for our purposes is the one induced by the 
decomposition of the underlying semiring. We say that φ = φ,'ύ 
• ά(φ) = ά(φ) — s, and 
• φ(χ) = ψ(χ) for all x e i l j . 
This is clearly an equivalence relation in Φ. Let φ, ψ, η £ Φ and φ = η with ά(φ) — s 
and ά(φ) = ά(η) = t. Then, it follows that ά(φ <S> φ) = ά(φ <g> η) = s U t and for all 
x £ flsut we have 
φ(χιη 
x ф(х^) 
ΞΞ 0(x i s) х гу(х^). 
We therefore have a combination congruence in Ф. It then follows that the equiva-
lence classes [ф] are subsemigroups of the combination semigroup of Ф. 
Next, we define for a valuation ф £ Фа the mapping врдо : Qs —> У by 
зрдо(х) =a 
if 
φ(χ) £ Ga, 
where Y is the semilattice of the group decomposition of the separative semiring. This 
mapping is well-defined since зрщ = зрщ if [ф] = [ф]. We define for a valuation ф 
with ά(φ) = s 
β{Φ] = 
{5 : Ω8-*■ G : Vx S Ω5, g(x) e G s p w ( x ) } . 
It follows that G до is a group, if we define g x / by g x /(x) = g(x) x /(x), 
and the semigroup [φ] is embedded in it. The unit element /до of G до is given by 
/до(х) = /βΡ[ψ](χ) and the inverse of φ is defined by </>_1(x) = (<J6(x))_1. This 
induces again a partial order [0] < [φ] if /до(х) < /[ψ](χ) f°r aU x € Ω8, or if 
[φ®ψ\ = [φ]. It is even a semilattice with /[ψ0ψ] = /до Л / и . 
The union of these groups 
G* = 
U GI*1 
is a commutative semigroup because, if gi £ G^ and 32 € G^j, then 51 (8) g2 is 
defined for x £ OsUi, d(</>) = s and ά(φ) = ί by 
9i®92(x) 
= 9\(xis) 
x 92(x4"') 
and belongs to G^®^] and is commutative as well as associative. 

REGULAR SEMIRING VALUATION ALGEBRAS 
2 0 9 
We have the equivalence φ <8> φ = φ because \φ] is closed under combination. 
From equation (E.2) it follows that for t Ç ά(φ) and all x G Ω„, 
№(x)] 
< 
[ф»(х»)]. 
This means that [ф] < [ф^ь] or also 
фи®ф 
= ф. 
We thus derived the second requirement for a separative valuation algebra given in 
Definition D.4 for the congruence induced by the separative semiring. It remains 
to show that the cancellativity property holds in every equivalence class [ф]. Thus, 
assume ψ, ψ' G [φ], ά(φ) = s and for all x G Ω8 
φ(χ) x ψ(χ.) 
= 
φ(χ) χ ·0'(χ). 
Since all elements φ(χ.),ψ(χ) and ψ'(χ) are contained in the same group, it follows 
that ф(х) = ψ'{χ) by multiplication with </>(x)_1. Therefore, ψ = ψ' which proves 
cancellativity of [φ]. 
■ 
Let us consider an example of a separative semiring and its induced valuation 
algebra. Subsequently, we then show that although separativity is again the weakest 
condition to allow for a division operation in the induced valuation algebra, there are 
still formalisms that do not even possess this structure. 
■ E.11 Arithmetic Potentials and Separativity 
The particular arithmetic semiring of non-negative integers Nu{0} is separative 
and decomposes into the semigroups {0} and N. The first is already a trivial 
group, whereas N is embedded into the group of positive rational numbers. 
Note also that [0 x a] = [0], hence [0] < [a] and [0] < [0 + a] < [a]. So, 
equation (E.2) holds. Thus, it induces a particular subalgebra of the valuation 
algebra of arithmetic potentials that is separative. 
■ E.12 Possibility Potentials and Separativity 
The Lukasiewicz and drastic product t-norm semirings of Example 5.8 are not 
separative. Consequently, we cannot deduce anything about a possible division 
operation in their induced valuation algebras. This, however, does not hold for 
all t-norm semirings as shown in the following section. 
E.2 
REGULAR SEMIRING VALUATION ALGEBRAS 
In the previous case, we exploited the fact that the multiplicative semigroup of a 
separative semiring can be embedded into a semigroup consisting of a union of 
disjoint groups. This allows introduction of a particular equivalence relation between 

210 
SEMIRING VALUATION ALGEBRAS 
semiring elements which in turn leads to a decomposition of the induced valuation 
algebra into cancellative semigroups with the corresponding congruence relation 
satisfying the requirement of a separative valuation algebra. In this section, we start 
with a semiring whose multiplicative semigroup decomposes directly into a union of 
groups. The mathematical requirement is captured by the following definition. 
Definition E.5 A semigroup A with an operation x is called regular if for all a G A 
there is an element b G A such that 
a x b x a — a. 
Appendix D. 1.2 introduced the Green relation in the context of a regular valuation 
algebra and we learned that the corresponding congruence classes are directly groups. 
This technique can be generalized to regular semigroups. We define 
a = b if, and only if, 
a x A = b x A, 
and obtain a decomposition of A into disjoint congruence classes, 
A = 
U [a]. 
a£A 
These classes are commutative groups under x, where every element a has a unique 
inverse denoted by a~ г. Further, we write /[„j for the identity element in the group [a] 
and take up the partial order denned by /|a] < /щ if, and only if, /ja] x /до = /[aj. 
This is again a semilattice as we know from Section E.l. Finally, we obtain an 
induced partial order between the decomposition groups by [a] < [b] if, and only if, 
f[a] < /[6]· This is summarized in Figure E.2. 
Definition E.6 A semiring (A, +, x, 0,1) is called regular if 
• its multiplicative semigroup is regular, 
• for all a,b e A, [a] <[a + b]. arm 
A - \J[a] 
Figure E.2 
A regular semigroup A decomposes directly into a union of disjoint groups 
using the Green relation. 
Theorem E.9 Let (Ф, D) be a valuation algebra induced by a regular semiring. 
Then (Ф, D) is regular. 

REGULAR SEMIRING VALUATION ALGEBRAS 
211 
Proof: Suppose φ G Φ with t С s = ά(φ) and y G Ω{. We define 
x(y) 
= 
( ^ ( y ) ) " 1 . 
Then, it follows that for any x G fls 
(ψ®φ1ί®χ)(χ) 
= 
<A(x) x ^ ( χ ^ ) x χ(χ^) 
= 
<£(x) x фи (x4t ) x {φχι (xiÉ ) ) - l 
= 
«AM x ϊφ^ ■ 
Here, the abbreviations /φ and / ^ t are used for /[ψ(χ)] and /[ψ4.«(χ±*)] respectively. 
Since the semiring is regular, we have [</>(x)] < [^'(x·''4)] and /ф < /фи. Thus 
<Α(χ) χ / ^ t 
= 
(4>(x) x /0) x / ^ ' 
= 
0(x) x (/φ * ίφΐ') 
= 
φ(χ) x ίΦ = (/)(χ). 
This proves the requirement of Definition D.5. 
■ 
We remark that regular semirings are also separative. Regularity of the multi-
plicative semigroup implies that every element in A has an inverse and we derive 
from axa 
— a x b = b x b that a and b are contained in the same group of the 
semiring decomposition, i.e. [a] = [b]. Multiplying with the inverse of a gives then 
a = /[a] x b = b. This proves that the multiplicative semigroup of a regular semiring 
is separative. Requirement (E.2) follows from the strengthening of positivity in Def-
inition E.6. 
Let us again consider some concrete examples of regular semirings and their 
induced regular valuation algebra. As a confirmation of what we found out in Instance 
D.8, we first show that the arithmetic semiring is regular. 
■ E.13 Arithmetic Potentials and Regularity 
The arithmetic semiring of non-negative real numbers is regular. It decomposes 
into a union of two disjoint groups K>o and {0}, and we clearly have [0] < [a] 
for all semiring elements a. Applying Theorem E.9 we conclude that its induced 
valuation algebra called arithmetic potentials is regular. However, we have also 
seen in Instance E. 11 that this property changes if we define the arithmetic 
semiring over different sets of numbers. On this note, the semiring view affords a 
whole family of valuation algebra instances that are closely related to arithmetic 
potentials but differ in their properties. 
■ E.14 Probabilistic Constraints and Regularity 
The t-norm semiring with usual multiplication as t-norm is regular too as 
we may deduce from the foregoing example. Therefore, its induced valuation 

212 
SEMIRING VALUATION ALGEBRAS 
algebra called probabilistic constraints is also regular. In fact, it is the only 
example of the t-norms listed in Example 5.8 that leads to a valuation algebra 
with a non-trivial division. The two t-norm discussed in Instance E. 12 do not 
allow division at all, and the minimum t-norm induces an idempotent valuation 
algebra as shown in Instance E.19 below. 
■ E.15 Spohn Potentials and Regularity 
The tropical semiring (N U {0, oo}, min, +, oo, 0) is not regular because the 
solution b = — a of the regularity equation in Definition E.5 is not contained 
in the semiring. But if we extend the tropical semiring to all integers, then 
semiring is regular. The induced regular valuations of this extended semiring 
are called quasi-Spohn potentials. 
E.3 CANCELLATIVE SEMIRING VALUATION ALGEBRAS 
In Appendix E.l, we started from a separative semigroup, which can be embedded 
into a semigroup consisting of a union of disjoint groups. Here, we discuss a special 
case in which the semigroup is embedded into a single group. The required property 
for the application of this technique is cancellativity. A semigroup A with operation 
x is called cancellative, if for a,b,c G A, 
ax b = 
axe 
implies b = с Such a semigroup can be embedded into a group G by application 
of essentially the same technique as in Appendix D.1.1. We consider pairs (o, b) of 
elements a, b G A and define equality: 
(a, b) = (c, d) 
if 
a x d = b x с 
Multiplication between pairs of semigroup elements is defined component-wise, 
(a,b) x (c,d) 
= 
(axe,bxd). 
This operation is well-defined, since (a, b) — {a',b') and (c, d) = (c',a") implies 
(a, b) x (c, d) — (α', b') x (c', d'). It is furthermore associative, commutative and the 
multiplicative unit e is given by the pairs (a, a) for all a G A. Note that all these pairs 
are equal with respect to the above definition. We further have 
(a,b) x (b,a) 
= 
(axb,axb), 
which shows that (a, b) and (b, a) are inverses. Consequently, the set G of pairs (a, b) 
is a group into which A is embedded by the mapping a >->· (a x a, a). If A itself has 
a unit, then 1 >->· (1,1) = e. Without loss of generality, we may therefore consider A 
as a subset of G. This is summarized in Figure E.3. 

IDEMPOTENT SEMIRING VALUATION ALGEBRAS 
2 1 3 
G={(a,b) 
:a,b£ A} 
Figure E.3 
A cancellative semigroup A is embedded into a group G consisting of pairs of 
elements from A. 
Let us return to semirings and show how cancellative semirings induce valuation 
algebras with inverse elements. 
Definition E.7 A semiring (A, +, x, 0,1) is called cancellative if its multiplicative 
semigroup is cancellative. 
A cancellative semiring is also separative since from axa 
= axb it follows that 
a = b. We know that cancellative semirings can be embedded into a single group 
consisting of pairs of semiring elements, and since we have only one group, (E.2) 
is trivially fulfilled. Thus, cancellative semirings represent a particularly simple case 
of separative semirings and consequently induce separative valuation algebras due 
to Appendix E. 1. We also point out that no direct relationship between cancellative 
and regular semirings exist. Cancellative semirings are embedded into a single group 
whereas regular semirings decompose into a union of groups. These considerations 
prove the following theorem: 
Theorem ЕЛО Cancellative semirings induce separative valuation algebras. 
We now examine an important example of a separative valuation algebra that is 
induced by a cancellative semiring. 
■ E.16 Weighted Constraints and Canceliativity 
The tropical semiring of Example 5.4 is cancellative. Since the semiring values 
are non-negative and multiplication corresponds to addition, we always have 
that a x b = a x с implies b = с To any pair of numbers a, b e A we assign 
the difference a — b, which is not necessarily in the semiring anymore. In other 
words, the tropical semiring is embedded into the additive group of all integers. 
E.4 
IDEMPOTENT SEMIRING VALUATION ALGEBRAS 
Section 4.2.1 and Appendix D. 1.3 introduced idempotency as a very strong additional 
condition. If this property is present in a valuation algebra, then every valuation 
becomes the inverse of itself, which allows us to simplify inference algorithms 

214 
SEMIRING VALUATION ALGEBRAS 
considerably. This lead to the very simple and time-efficient idempotent architecture 
of Section 4.5. Thus, we next go about studying which semiring properties give rise 
to idempotent valuation algebras. 
Theorem E.ll c-semirings with idempotent multiplication induce idempotent valu-
ation algebras. 
Proof: From Lemma 5.6, (SP6) we conclude that 
</>(x) x <^'(x;t) 
< 
ф(х). 
On the other hand, we have by Lemma 5.5, (SP4) and idempotency 
φ(κ) x < ^ V ) = φ(κ) x Σ 
ф{х*,у) > ф(х) x <Kx) = #х). 
This holds because ф(х) is a term of the sum. We therefore have φ(χ) χ φ^ι(χ.^) = 
φ(χ) which proves idempotency. 
■ 
Idempotency of a semiring (A, +, x, 0,1) directly implies the regularity of its 
multiplicative semigroup. Additionally, all groups [a] consist of a unique element as 
we learned in Appendix D.1.3. Since for all a, b € A we have 
a x (a + b) = 
(a x a) + (a x b) = a + (a x b) 
— a x (1 + 6) = a x 1 = a. 
This implies [a] < [a + b], from which we conclude that idempotent semirings 
are regular. Moreover, Lemma 5.5, (SP5) states that idempotent semirings are also 
positive which ensures the existence of neutral elements, null elements and stability 
in the induced valuation algebra as shown in Section 5.5.1 and 5.5.3. Altogether, this 
proves the following lemma: 
Lemma E.8 c-semirings with idempotent x induce information algebras. 
Theorem E. 11 states that a c-semiring with idempotent multiplication is a sufficient 
condition for an idempotent semiring valuation algebra. It will next be shown that 
it is also necessary, if two additional conditions hold. In order to make a statement 
about all semiring elements starting from a semiring valuation algebra, all semiring 
elements must occur in the valuation algebra. In other words, Φ must be the set of all 
possible valuations taking values from a given semiring, whereas in other contexts it 
is often sufficient that Φ is only closed under combination and projection. The second 
condition excludes trivial lattices where the operation of projection has no effect. 
Theorem E.12 Let (A, +, x, 0,1) be a semiring, r a set of variables and Φ the set 
of all possible semiring valuations defined over A and г. //"(Ф, V(r)) is idempotent, 
then the semiring has idempotent multiplication. Moreover, ifr contains at least one 
variable X 6 r with |Ωχ| > 1, then the semiring is also a c-semiring. 

SCALABLE SEMIRING VALUATION ALGEBRAS 
2 1 5 
Proof: Since Φ is the set of all possible valuations, there exists a for all a G Л а 
valuation ф G Ф with ά(φ) = 0 and φ(ο) — a. Idempotency implies that 
φ(ο) = φ® φ(ο) = φ{ο) χ φ(ο). 
We therefore have a x a — a for all a G A with proves idempotency of semiring 
multiplication. Next, assume that there exists X G r with ίϊχ = {χι, x2, · · ·, xn}-
For all a G Л, we then have a valuation ф £ Ф with d(0) = {X} and 0(#i) = l· 
0(^2) = a and φ(χί) = 0 for all г > 2. Idempotency implies that 
1 = </>(χι) = φ(χ1)χφ^(ο) 
= φ(χ1)χ{φ(χ1) 
+ φ(χ2)) = 1χ(1+α) = 1+α. 
This shows that the semiring is a c-semiring. 
■ 
Let us again consider some examples of c-semirings with idempotent multipli-
cation and their induced information algebras. First, we apply this analysis to the 
Boolean semiring to confirm what we have found out in Instance 4.2. 
■ E.17 Indicator Functions and Idempotency 
It has been shown in Example 5.10 that the Boolean semiring is a c-semiring. 
Since multiplication corresponds to minimization, this semiring is also idem-
potent. We therefore conclude from Lemma E.8, the induced valuation algebra 
of indicator functions must be an information algebra. 
■ E.18 Bottleneck Constraints and Idempotency 
The bottleneck semiring of Example 5.3 is clearly a c-semiring with idempotent 
multiplication. Its induced valuation algebra is therefore another example of an 
information algebra. 
■ E.19 Possibility Potentials and Idempotency 
We already pointed out in Example 5.10 that all t-norm semirings are c-
semirings. However, only the minimum t-norm among them is idempotent 
which again turns its induced valuation algebra into an information algebra. 
This example is very similar to the Bottleneck constraints above. 
A summary of the division-related properties of different semiring valuation alge-
bras is shown in Figure E.4. 
E.5 SCALABLE SEMIRING VALUATION ALGEBRAS 
As a last complement on division for semiring valuation algebras, we recall from 
Section 4.7 that scaling or normalization may be an important semantical issue 

216 
SEMIRING VALUATION ALGEBRAS 
1.1 
1.3 
1.3 
5.1 
5.2 
5.2 
5.3 
E.15 
Instance 
Indicator Functions 
Arithmetic Potentials on K>o 
Arithmetic Potentials on N>0 
Weighted Constraints 
Probabilistic Constraints 
Lukasiewicz Constraints 
Set-based Constraints 
Quasi-Spohn Potentials 
Separative 
V 
V 
V 
V 
V 
о 
V 
V 
Cancellative 
V 
0 
о 
V 
о 
о 
V 
о 
Regular 
V 
V 
о 
о 
V 
о 
V 
V 
Idempotent 
V 
о 
о 
о 
О 
о 
V 
О 
Figure E.4 Semiring valuation algebras and division-related properties. 
for some valuation algebras. The algebraic requirement that makes scaling possible 
is separativity. Thus, we may conclude that separative semirings induce valuation 
algebra with scaling if there exists a semantical need for this operation. Among the 
instances discussed in this appendix, we have seen that the arithmetic semiring is 
regular and allows scaling in its induced valuation algebra. This leads again to the 
probability potentials introduced in Instance 4.4. Here are two further examples of 
scalable valuation algebras: 
■ E.20 Normalized Weighted Constraints 
Weighted constraints are induced by the cancellative tropical semiring and are 
therefore separative. According to equation (4.29), the scale of a weighted 
constraint с G Фя is given by 
c^x) = c(x) + (c10)'1 
(o) = c(x) - min c(y). 
This operation ensures that the minimum weight is always zero. 
■ E.21 Normalized Probabilistic Constraints 
The product t-norm semiring induces probabilistic constraints which naturally 
explains the interest in scaling. This semiring is regular and therefore also 
fulfills the algebraic requirement. According to equation (4.29), the scale of 

EXERCISES 
2 1 7 
the probabilistic potential p e Φ3 is given by 
p(x) 
PHX) = P(x) · ( > ) ' (o) 
maxyens p(y) ' 
PROBLEM SETS AND EXERCISES 
E.l * Section 5.6 shows that the space complexity of the Shenoy-Shafer architec-
ture of equation (4.9) can be improved when dealing with semiring valuations. Think 
about a similar improvement for set-based semiring valuation algebras. 
E.2 * Consider mappings from singleton configuration sets to the values of a com-
mutative semiring and prove that they form a subalgebra of the valuation algebra 
of set-based semiring valuations. Identify the requirements for neutral and null ele-
ments and observe that neutral elements are different in the two algebras. Compare 
the results with usual semiring valuations. 
E.3 * Exercise D.4 in Chapter 4 introduced a partial order between valuations in an 
information algebra. We~know from Lemma E.8 that c-semirings with idempotent x 
induce information algebras. How is the semiring order of equation (5.4) related to 
the partial order between valuations in the induced information algebra? 
E.4 * We claim in Example 5.8 and 5.10 that ([0,1], max, x, 0,1) always forms a 
c-semiring when an arbitrary t-norm is taken for multiplication. Prove this statement. 
The definition of a t-norm requires that the number 1 is the unit element. Without this 
condition the structure it is called a uninorm. Prove that we still obtain a semiring, 
albeit not a c-semiring, when a uninorm is taken instead of a t-norm. Finally, show 
that we may also replace maximization for semiring addition by an arbitrary uninorm. 
E.5 * * 
Exercise D.8 in Chapter 4 introduced conditionals <j>s\t for disjoint sets 
s, t Ç ά(φ) in regular valuation algebras (Φ, D). 
a) Identify the conditionals in the valuation algebras of probabilistic con-
straints from Instance 5.2 and quasi-Spohn potentials of Instance E.15. 
b) Prove that if φ £ Φ and s, t, и С ά(φ) are disjoint sets we have 
фииюи 
= 
0 s | u 0 0 t | u 0 0 i * . 
(E.3) 
c) Convince yourself that the arctic semiring is regular and therefore also the 
induced valuation algebra of GAI preferences from Instance 5.1. Show that 
equation (E.3) corresponds to Bellman's principle of optimality (Bellman, 
1957) when it is expressed in the valuation algebra of GAI preferences. 

218 
SEMIRING VALUATION ALGEBRAS 
The solution to these exercises is given in Section 5.2 of (Kohlas, 2003). 
E.6 * * Identify the algebraic properties (i.e. division, neutral and null elements) for 
the valuation algebras induced by the t-norm semirings with the following t-norms: 
1. Nil-potent maximum: For a, b € [0,1] 
T(a,b) = W « . * } 
a + b > 1' 
10 
otherwise. 
2. Hamacher product: For a, b £ [0,1] 
T(ab) = i° 
a = b = 0, 
I Ab
 
h 
otherwise. 
^ a-\-o~ab 
E.7 * * * 
Exercise E.4 defines a uninorm as a generalization of a t-norm where 
the unit element does not necessarily correspond to the number 1. If we have a 
uninorm with the number 0 as unit element, then it is called a t-conorm. We directly 
conclude from Exercise E.4 that we obtain a semiring when we take a t-conorm for 
multiplication and maximization for addition. Give a t-norm T and a, b £ [0,1], we 
define its associated t-conorm by 
C{a,b) 
= 
Ι - Γ ( Ι - α , Ι - δ ) . 
(Ε.4) 
Is it possible to conclude the form of division in a valuation algebra induced by a 
t-conorm semiring from the form of division that is present in the valuation algebra 
induced by the corresponding t-norm semiring? 
E.8 * * * Instance D.6 shows that the valuation algebra of set potentials is separative. 
Generalize these considerations to set-based semiring valuations and identify the 
algebraic requirements for a semiring to induce separative, regular and idempotent 
set-based semiring valuation algebras. 

CHAPTER 6 
VALUATION ALGEBRAS FOR PATH 
PROBLEMS 
Together with semiring constraint systems introduced in Chapter 5, the solution of 
path problems in graphs surely ranks among the most popular and important appli-
cation fields of semiring theory in computer science. In fact, the history of research 
in path problems strongly resembles that of valuation algebras and local computation 
in its search for genericity and generality. In the beginning, people studied specific 
path problems which quickly brought out a large number of seemingly different algo-
rithms. Based on this research, it was then observed that path problems often provide 
a common algebraic structure. This gave birth of an abstract framework called al-
gebraic path problem which unifies the formerly isolated tasks. Typical instances of 
this framework include the computation of shortest distances, maximum capacities 
or reliabilities, but also other problems that are not directly related to graphs such 
as partial differentiation or the determination of the language accepted by a finite 
automaton. This common algebraic viewpoint initiated the development of generic 
procedures for the solution of the algebraic path problem, similar to our strategy of 
defining generic local computation architectures for the solution of inference prob-
lems. Since the algebraic path problem is not limited to graph related applications, 
its definition is based on a matrix of semiring values instead. We will show in this 
chapter that depending on the semiring properties, such matrices induce valuation 
Generic Inference: A Unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc. 
219 

220 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
algebras. This has several important consequences: on the one hand, we then know 
that matrices over special families of semirings provide further generic constructions 
that deliver many new valuation algebra instances. These instances are very different 
from the semiring valuation algebras of Chapter 5, because they are subject to a pure 
polynomial time and space complexity. On the other hand, we will see in Chapter 
9 that inference problems over such valuation algebras model path problems that 
directly enables their solution by local computation. This comes along with many 
existing approaches that focus on solving path problems by sparse matrix techniques. 
The first section introduces the algebraic path problem from a topological perspec-
tive and later points out an alternative description as a particular solution to a fixpoint 
equation, which generally is more adequate for an algebraic manipulation. Section 
6.3 then discusses quasi-regular semirings that always guarantee the existence of at 
least one solution to this equation. Moreover, it will be shown that a particular quasi-
inverse matrix can be computed from the quasi-inverses of the underlying semiring. 
This leads in Section 6.4 to the first generic construction related to the solution of 
path problems. Since quasi-regular semirings are very general and only possess little 
algebraic structure, it will be remarked in Section 6.5 that the same holds for their 
induced valuation algebras. We therefore present in Section 6.6 a special family of 
quasi-regular semirings called Kleene algebras that uncover a second generic con-
struction related to path problems in Section 6.7. This approach always leads to 
idempotent valuation algebras and also distinguishes itself in other important aspects 
from quasi-regular valuation algebras, as pointed out in Section 6.8. 
6.1 SOME PATH PROBLEM EXAMPLES 
The algebraic path problem establishes the connection between semiring theory and 
path problems. To motivate a closer investigation of this combination, let us first look 
at some typical examples of path problems. 
■ 6.1 The Connectivity Problem 
The graph of Figure 6.1 represents a traffic network where nodes model junc-
tions and arrows one-way streets. Every street is labeled with a Boolean value 
where 1 stands for an accessible street and 0 for a blocked street due to con-
struction works. The connectivity problem now asks the question whether node 
T can still be reached from node S. This is computed by taking the maximum 
value over all possible paths leading from S to T, where the value of a path 
corresponds to the minimum of all its edge values. We compute for the three 
possible paths from S to T: 
min{0,1} 
= 
0 
min{l,l,l} 
= 
1 
min{l, 1,0,1} = 
0 

SOME PATH PROBLEM EXAMPLES 
221 
and then 
max{0,1,0} 
= 
1. 
Thus, node Г can still be reached from node S. 
Figure 6.1 The connectivity path problem. 
■ 6.2 The Shortest and Longest Distance Problem 
A similar traffic network is shown in Figure 6.2. This time, however, edges 
are labeled with natural numbers to express the distance between neighboring 
nodes. The shortest distance problem then asks for the minimum distance 
between node S and node T, which corresponds to the minimum value of all 
possible paths leading from S to T, where the value of a path consists of the 
sum of all its edge values. We compute 
9 + 4 = 
13 
1 + 6 + 5 = 
12 
1 + 2 + 3 + 5 = 
11 
and then 
min{13,12,ll} 
= 
11. 
Thus, the shortest distance from node S to node T is 11. We could also replace 
minimization by maximization to determine the longest or most critical path. 
■ 6.3 The Maximum Capacity Problem 
Let us next interpret Figure 6.3 as a communication network where edges 
represent communication channels between network hosts. Every channel is 
labeled with its capacity. The maximum capacity problem requires to compute 
the maximum capacity of the communication path between node S and node 
T, where the capacity of a communication path is determined by the minimum 

222 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
Figure 6.2 The shortest distance problem. 
capacity over all channels in this path. Similar to Instance 6.1 we compute 
min{3.4,4.5} 
= 
3.4 
min{3.6,5.5,5.1} 
= 
3.6 
min{3.6,4.2,3.5,5.1} 
= 
3.5 
and then 
max{3.4,3.6,3.5} 
= 
3.6. 
Thus, the maximum capacity over all channels connecting S and T is 3.6. 
Figure 6.3 The maximum capacity problem. 
■ 6.4 The Maximum Reliability Problem 
We again consider Figure 6.4 as a communication network where edges repre-
sent communication channels. Such channels may fail and are therefore labeled 
with a probability value to express their probability of availability. Thus, we 
are interested in the most reliable communication path between node S and 
node T, where the reliability of a path is obtained by multiplying all its channel 
probabilities. We compute 
0.4 · 0.8 
= 
0.32 
0.9-0.2-0.7 
= 
0.126 
0.9-0.9- 1.0 -0.7 
= 
0.567 

THE ALGEBRAIC PATH PROBLEM 
223 
and then 
max{0.32,0.126,0.567} 
= 
0.567. 
Thus, the most reliable communication path has a reliability of 0.567. 
Figure 6.4 The maximum reliability problem. 
■ 6.5 The Language of an Automaton 
Finally, we consider the automaton of Figure 6.5 where nodes represent states, 
and the edge values input words that bring the automaton into the neighbor state. 
Here, the task consists in determining the language that brings the automaton 
from state S to T. This is achieved by collecting the strings of each path from 
S to T, which are obtained by concatenating their edge values. We compute 
{a} ■ {c} 
= 
{ac} 
{a} ■ {b} ■ {a} 
= 
{aba} 
{a} ■ {c} · {&} · {a} 
— {acba} 
and then 
ac}, {aba}, {acba}} 
= 
{ac, aba, acba}. 
Thus, the language that brings the automaton from state S to Г is {ac, aba, acba}. 
Further examples of path problems will be given at the end of this chapter. 
6.2 THE ALGEBRAIC PATH PROBLEM 
We next introduce a common formal background of these well-known path problems: 
Consider a weighted, directed graph (V, E, A, w) where (V, E) is a directed graph 
and w a weight function that assigns a value from an arbitrary semiring (A, +, x, 0,1) 
to each edge of the graph, w : E —> A. If p = (V0,..., Vn) denotes a path of length 
U« 

2 2 4 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
Figure 6.5 Determining the language of an automaton. 
n between a source node S — Vo and a target node T = Vn, the path weight w(p) is 
defined as the product of all semiring values that are assigned to its edges, 
w{p) 
= 
w(V0,V1)xw(V1,V2)x---xw(Vn-1,Vn). 
(6.1) 
Since we later identify graph nodes with variables, we here use capital letters to refer 
to graph nodes. It is important to note that we assumed an arbitrary semiring where 
multiplication is not necessarily commutative, which forces us to mind the order of 
multiplication. Also, many paths between S and T may exist. We write Ρς,τ for the 
set of all possible paths connecting a source S with a terminal T. Solving the path 
problem for S φ Τ then consists in computing 
O(S,T) 
= 
0 
w(p). 
(6.2) 
p£Ps,T 
If S = T we simply define D(5, Γ) = 1 as the weight of the empty path. This 
is the unit element of the semiring with respect to multiplication and ensures that 
path weights are not affected by cycling an arbitrary number of times within an 
intermediate node. If no path exists between the two nodes S and T, equation (6.2) 
sums over the empty set which, by definition, returns the zero element of the semiring. 
Alternatively, we may define Pg T as the set of all paths from node S to node T of 
length r > 0. We obtain 
PS,T 
= 
| J 
PS,T 
r>0 
and consequently for S ф Т 
O(S,T) = 0 0 
w(p). 
(6.3) 
r>0 
ρ€Ρξτ 
Example 6.1 The connectivity problem of Instance 6.1 is based on the Boolean se-
miring of Example 5.2. Finding shortest distances according to Instance 6.2 amounts 
to computing in the tropical semiring of Example 5.4 where semiring multiplication 
corresponds to addition and semiring addition to minimization. The maximum capac-
ity problem of Instance 6.3 is induced by the bottleneck semiring of Example 5.3, and 

THE ALGEBRAIC PATH PROBLEM 
2 2 5 
the product t-norm semiring of Example 5.8 allows to compute the most reliable path 
in Instance 6.4. Finally, the language of the automaton in Instance 6.5 is computed 
by the semiring of regular languages of Example 5.7. Observe that this semiring is 
not commutative. 
There are several critical points regarding the equations (6.2) and (6.3): First, 
these formulas is entirely based on a graph related conception due to the explicit use 
of a path set. This may be sufficient for the examples given above, but we already 
pointed out in the introduction of this chapter that other applications without this 
interpretation exist. Second, it may well be that an infinite number of paths between 
two nodes exists, as it is for example the case in graphs with cycles. We then obtain 
an infinite sum in both equations that is not necessarily defined. These are the two 
major topics for the remaining part of this section. To start with, we develop a more 
general definition of path problems based on semiring matrices instead of path sets. 
Example 6.2 The graph in Figure 6.6 contains a cycle between the nodes 1 and 6. 
The path set Ρ\$ therefore contains an infinite number of paths. Depending on the 
semiring, the sum in equation (6.2) is not defined. 
Figure 6.6 Path sets in cycled graphs may be infinite. 
From now on, we again identify graph nodes with natural numbers {1,... , n} 
with n = \V\. The advantage of this convention is that we may represent a weighted 
graph in terms of its adjacency matrix M e M (A, n) defined as follows: 
M(X,Y) = HX^ 
i
W
^ 
m) 
10, 
otherwise. 
Due to the particular node numbering, the matrix entries directly identify the edge 
weights in the graph. It is sometimes common to set the diagonal elements of this 
semiring matrix to the unit element 1, if no other weight is specified in the graph. 
This, however, is not necessary for our purposes. Note that an adjacency matrix 
describes a full graph where any two nodes are linked, although some edges have 
weight zero. As a consequence, any sequence of nodes describes a path, but only the 
paths of weight different from zero describe paths in the underlying graph. Also, the 
definition of an adjacency matrix excludes multigraphs where multiple edges with 
identical directions but different weights may exist between two nodes. But this can 

226 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
be taken into consideration by assigning the sum of all weights of edges leading from 
nodeXtoYtoM(X,Y). 
Example 6.3 Figure 6.7 shows a directed, weighted graph defined over the tropical 
semiring (N U {0, oo}, min, +, oo, 0) and its associated adjacency matrix. Unknown 
distances are set to the zero element of the semiring, which here corresponds to oo. 
oo 
9 
8 
oo 
oo 
oo 
6 
oo 
oo 
oo oo 
7 
5 
oo со 
со 
Figure 6.7 The adjacency matrix of a directed, weighted graph. 
The solution of path problems based on adjacency matrices uses semiring matrix 
addition and multiplication as introduced in Section 5.1.2. Remember that matrices 
over semirings form themselves a semiring with the identity matrix as unit and 
the zero matrix as zero element. Let us next consider the successive powers of the 
adjacency matrix: 
M 2(5,T) 
= 
Σ 
Μ ( 5 , Κ ι ) χ Μ ( Α Ί , Γ ) 
Ki€{l,...,n] 
M 3(5,T) 
= 
Σ 
M(S,K1)xM(K1,K2)xM{K2,T) 
Kl,K2e{i,...,n} 
and more generally 
Mr(S,T) 
= 
Σ 
M(S,Jfi) хМ(Кг,К2) 
х 
...xM(Kr-i,T). 
ΚΊ,...,κν_ιε{ι,...,η} 
We immediately observe that М г(5, Т) corresponds to the sum of all path weights 
of length r € N between the selected source and target node. It therefore holds that 
Mr(S,T) = 
0 
w(p) 
P£ps,T 
and we obtain the sum of all path weights of length at most r > 0 by 
M ( r ) 
= 
I + M + M 2 + ... + M r. 
(6.5) 
The identity matrix handles paths of length 0 by denning I(X, Y ) = М ( 0 )(Х,У) = 1 
if X = Y and I{X,Y) 
= M^(X,Y) 
= 0 otherwise. Its importance will be 
M 

THE ALGEBRAIC PATH PROBLEM 
2 2 7 
illustrated in Example 6.4 below. For r > 0, this sum of matrix powers can also be 
seen as the sum of the weights of all paths between S and T that visit at most r — 1 
intermediate nodes. We are tempted to express the matrix D of equation (6.3) as the 
infinite sum 
D = 
0 M r = I + M + M2 + ... 
(6.6) 
This corresponds to the sum of all paths between S and T visiting an arbitrary 
and unrestricted number of nodes. Contrary to equation (6.3) we have now derived 
a description of path problems that is independent of path sets and therefore not 
restricted to graph based applications anymore. On the other hand, we still face the 
problem of infinite sums of semiring values. Subsequently, we therefore introduce 
a topology on partially ordered semirings that allows us to define and study such 
formal sequences. 
Example 6.4 We reconsider the adjacency matrix from Example 6.3 and compute 
its successive powers for 1 < r < 4 based on the tropical semiring with addition for 
semiring multiplication and minimization for semiring addition. 
oo oo 
15 15 
oo oo oo 
13 
12 oo oo oo 
oo 
14 13 oo 
20 oo 
oo 22 
18 oo oo oo 
oo 21 20 oo 
oo oo 20 20 
27 29 28 oo 
oo 27 26 oo 
oo oo 27 27 
25 oo oo 27 
M 4(l, 3) = 28 for example means that the distance of the shortest path from node 1 
to node 3 containing exactly 4 edges is 28. Using equation (6.5) we compute M^4^ 
that contains all shortest paths of at most 4 edges: 
M 1 
M2 
= 
Md 
M4 
oo 
oo 
oo 
5 
00 
oo 
oo 
5 
oo 
oo 
oo 
5 
oo 
oo 
oo 
5 
9 
oo 
oo 
oo 
9 
oo 
oo 
oo 
9 
oo 
oo 
oo 
9 
oo 
oo 
oo 
8 
6 
oo 
oo 
8 
6 
oo 
oo 
8 
6 
oo 
oo 
8 
6 
oo 
oo 
oo 
oo 
7 
oo 
00 
oo 
7 
oo 
00 
oo 
7 
oo 
oo 
oo 
7 
oo 
00 
oo 
00 
5 
00 
00 
12 
oo 
20 
18 
oo 
oo 
9 
oo 
00 
00 
00 
oo 
oo 
14 
oo 
oo 
21 
oo 
8 
6 
00 
00 
15 
00 
oo 
13 
oo 
oo 
20 
20 
oo 
oo 
7 
oo 
15 
13 
oo 
oo 
22 
oo 
oo 
20 

2 2 8 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
M ( 4 ) 
= 
I + M + M 2 + M 3 + M 4 
o o o o o o 
o o 9 8 o o 
oo oo 
15 15 
O o o o o 
о о о о б о о 
o o o o o o l 3 
o o O o o 
oo oo oo 
7 
12 oo oo oo 
o o o o O 
б о о о о о о 
oo 
14 13 oo 
0 
9 
8 
15 
18 
0 
6 
13 
12 21 
0 
7 
5 
14 
13 
0 
Неге, М^4) ( 1,3) = 8 means that the distance of the shortest path from node 1 to node 
3 containing at most 4 edges is 8. In this very special case, Мл4' already contains 
all shortest paths in the graph of Figure 6.7 which is not generally the case. Also, 
we observe the importance of the identity matrix to ensure that each node can reach 
itself with a shortest distance ofO. 
We next generalize the power sequence of semiring matrices to arbitrary semirings 
A and define for r > 0 and a G A: 
a(r) 
= 
l + a + a2 + a3 + ... + ar. 
(6.7) 
The following recursions are direct consequences of this definition: 
a<r+1> = a x a ( r ) + l 
= a<r> + a r + 1. 
(6.8) 
From Lemma 5.2, (SP3) follows that the power sequence is non-decreasing, 
a(r) 
< 
fl(r+l)_ 
( 6 9 ) 
This refers to the canonical preorder of semirings introduced in equation (5.3). We 
also remember that if this relation is a partial order, the semiring is called a dioid. 
Idempotency of addition is always a sufficient condition to turn the preorder into a par-
tial order which then is equivalently expressed by equation (5.4). Following (Bétréma, 
1982) dioids can be equipped with a particular topology called sup-topology, where 
a sequence of dioid values is convergent, if it is non-decreasing, bounded from above 
and if it has a least upper bound. The limit of a convergent sequence then corresponds 
to its least upper bound. However, the simple presence of the sup-topology is not yet 
sufficient for our purposes. Instead, two additional properties must hold which are 
summarized in the definition of a topological dioid (Gondran & Minoux, 2008). 
Definition 6.1 A dioid endowed with the sup-topology with respect to its canonical 
partial order is called topological dioid if the following properties hold: 
0 
oo 
00 
00 
+ 
20 oo oo 22 
18 oo oo oo 
oo 21 20 oo 
oo oo 20 20 
+ 
27 29 28 oo 
oo 27 26 oo 
oo oo 27 27 
25 oo oo 27 
1. Every non-decreasing sequence bounded from above has a least upper bound. 

THE ALGEBRAIC PATH PROBLEM 
2 2 9 
2. Taking the limit is compatible with semiring addition and multiplication, i.e. 
lim (a(r) + 6(r)) 
= 
lim a(r) + lim b(r) 
Г—»СЮ 
Г—УОО 
Г—ЮО 
lim (a(r) x ^ r)) 
= 
lim a(r) x lim b{r\ 
г—юо 
r—>oo 
r—>oo 
Thus, if the semiring is a topological dioid and if the power sequence is bounded 
from above, it follows from Definition 6.1 that the sequence has a least upper bound. 
We then know that the sequence is convergent with respect to the sup-topology and 
that its limit corresponds to the least upper bound. We subsequently denote the limit 
of the power sequence by 
a* = sup{o(r)} = 
lim a(r). 
(6.10) 
r>0 
r^°° 
Example 6.5 Bounded semirings of Definition 5.3 satisfy a + 1 = lfor all a G A. 
They are idempotent since 1 + 1 = 1 always implies that a + a = a. Thus, bounded 
semirings are dioids, and since a < 1 we also have ar < 1 and therefore a^ 
= 
1 + a + ... + ar < 1. This shows that power sequences over bounded semirings are 
always non-decreasing and bounded from above. If we further have a topological 
dioid, then the limit of this sequence exists for each a € A. Some instances of bounded 
semirings were given in Example 5.10. 
The following theorem links the limit of a power sequence in a topological dioid to 
the solution of a linear fixpoint equation. Semiring elements that satisfy this equation 
are generally referred to as quasi-inverses. Note that we subsequently write ab for 
a x b whenever it is contextually clear that we refer to semiring multiplication. Also, 
we keep our convention to refer to variables by capital letters. 
Theorem 6.1 If a* exists in a topological dioid, it is always the least solution to the 
fixpoint equations 
X = aX + l 
and X = Xa + l. 
(6.11) 
Let us first prove the following intermediate result: 
Lemma 6.1 Let A be an arbitrary semiring and y € A a solution to (6.11). We have 
y = 
ar+ïy + a(r) 
(6.12) 
and the solution y is an upper bound for the sequence a^r\ 
Proof: We proceed by induction: For r = 0 we have y = ay + 1. This holds 
because y is a solution to equation (6.11). Assume now that equation (6.12) holds for 
r. For r + 1 it then follows from equation (6.8) and the induction hypothesis that 
ar+2y + a<r+1> 
= 
a r + 2 y W r ) + l 
= 
a(a r + 1y + a(r)) + l = ay + 1 = y. 

230 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
Using Lemma 5.2, (SP3) we finally conclude that a^ < y. 
m 
Using this result we next prove Theorem 6.1 : 
Proof: Taking the limit in a topological dioid is compatible with semiring addition 
and multiplication. We thus conclude that the limit of aa^ is aa* and that aa^ + 1 
is convergent with limit aa* + 1. It follows from equation (6.8) that a* — aa* + 1 
and similarly that a* = a*a + 1. This shows that a* is a solution to equation (6.11). 
From Lemma 6.1 follows that for all solutions y we have a^ < y. Due to equation 
(6.10) the particular solution a* is the least upper bound of the power sequence and 
therefore also the least solution to both fixpoint equations. 
■ 
There may be several solutions to these fixpoint equations and thus several quasi-
inverses for a semiring element as shown in Example 6.6 below. But if we have 
a topological dioid where the power sequence converges, then its limit is the least 
element among them. Further, we show in Lemma 6.2 below that the limit also 
determines the least solution to more general fixpoint equations. 
Example 6.6 Consider the tropical semiring (R U {oo}, min, +, сю, 0) of Example 
5.4. We then obtain for equation (6.11) x = min{a + x, 0}. For a > 0 this equation 
always has a unique solution x = 0. If a < 0 there is no solution and ifa = 0, then 
every x < 0 is a solution to this fixpoint equation. 
Lemma 6.2 
1. Ifx is a solution to the fixpoint equation X = aX +1 in an arbitrary semiring 
A, then xb is a solution to X = aX + b with b € A. Likewise, ifx is a solution 
to X = Xa + 1, then bx is a solution to X = Xa + b. 
2. If the limit a* exists in a topological dioid, then a*b is always the least solution 
to X = aX + b. Likewise, ba* is always the least solution to X = Xa + b. 
Proof: 
1. If x satisfies x = ax + 1 we have 
axb + b = (ax + l)b = xb. 
Thus, xb is a solution to X — aX + b. The second statement follows similarly. 
2. If o* exists, it is a solution to X — aX + 1 by Theorem 6.1. By the first 
statement, a*b is a solution to X = aX + b. In order to prove that it is the least 
solution, assume у e A to be another solution to X = aX + b. An induction 
proof similar to Lemma 6.1 shows that a^b < у for all к € N U {0}. The 
solution у is therefore an upper bound for the sequence {a^b} and since this 
sequence is non-decreasing and bounded from above, it converges towards the 
least upper bound a* b of the sequence. We therefore have a*b < y. 

THE ALGEBRAIC PATH PROBLEM 
231 
Let us return to the solution of path problems and the power sequence of semiring 
matrices in equation (6.6). We first recall that matrices with values from a semiring 
themselves form a semiring with a component-wise definition of the canonical pre-
order. Thus, if this relation is a partial order, then the corresponding semiring of 
matrices is also a dioid. Moreover, if the semiring is a topological dioid, then so is 
the semiring of matrices (Gondran & Minoux, 2008). We conclude from equation 
(6.10) that if the power sequence of semiring matrices with values from a topological 
dioid converges, then solving a path problem is equivalent to computing the least 
quasi-inverse matrix. This computational task is stated by the following definition: 
Definition 6.2 Let M 6 M.(A, n) be a square matrix over a topological dioid A. If 
the limit exists, the algebraic path problem consists in computing 
M* 
= 
lim M ( r ). 
(6.13) 
Г—ЮО 
We now have a formally correct and general description of a path problem that is 
not restricted to graph related applications anymore. However, it is based on the limit 
of an infinite sequence and therefore rather inconvenient for computational purposes. 
An alternative way to express the algebraic path problem by means of the least 
solution to a linear fixpoint equation is indicated by Theorem 6.1. For that purpose, 
we first derive a converse result to this theorem: 
Lemma 6.3 If in a topological dioid y is the least solution to the fixpoint equations 
X = aX + 1 and X = Xa + 1, 
(6.14) 
then it corresponds to the quasi-inverse ofaEA 
satisfying 
y = 
lim 
œr\ 
r-+oo 
Proof: We recall from equation (6.9) that the sequence a^ 
is non-decreasing. 
Further, because we have assumed the existence of a least solution, the sequence 
is also bounded by Lemma 6.1. Definition 6.1 then assures that the sequence has a 
least upper bound which corresponds to the limit of the power sequence. But then, 
this limit is the least solution to the fixpoint equation according to Theorem 6.1 and 
corresponds to y since the least element is uniquely determined. 
■ 
We obtain from Theorem 6.1 and Lemma 6.3 the following alternative but equiv-
alent definition of the algebraic path problem: 
Definition 6.3 Let M € Ai(A, n) be a square matrix over a topological dioid A. 
The algebraic path problem consists in computing the least solution to 
X = MX + I = X M + I, 
(6.15) 
if such a solution exists. 
Here, the variable X refers to an unknown matrix in M(A, n). It is important to 
note that this definition is still conditioned on the existence of a solution, respectively 

232 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
on the convergence of the power sequence. Imagine for example that we solve a 
shortest distance problem on the cyclic graph of Figure 6.6 with negative values 
assigned to the edges of the cycle. If we then consider the path weights between the 
source and target node, we obtain a shorter total distance for each additional rotation 
in the cycle. In this case, the limit of equation (6.13) does not exist or, in other words, 
the fixpoint equations have no solution. This is comparable to the case a < 0 in 
Example 6.6. Investigating the requirements for the existence of quasi-inverses is 
an ongoing research topic. Many approaches impose restrictions on graphs which 
for example forbid cycles of negative path weights in case of the shortest distance 
problem. But this naturally only captures graph related applications. Alternatively, 
we may ensure the existence of quasi-inverses axiomatically which also covers other 
applications. Then, the semiring of Example 6.6, where no quasi-inverses exist for 
negative elements, would not be part of the framework anymore. However, we can 
easily imagine that shortest distances can still be found in graphs with negative 
values, as long as there are no cycles with negative path weights. In many cases we 
can adjoin an additional element to the semiring that acts as an artificial limit for 
those sequences that are divergent in the original semiring. Reconsider the shortest 
distance problem with edge weights from the semiring (K U {oo}, min, +, со, 0). 
Here, it is reasonable to define M.*(S,T) = —oo, if the path from node S to T 
contains a cycle of negative weight. This amounts to computing in the extended 
semiring (M U {—oo, oo}, min, +, oo, 0) with —oo + oo = oo. Then, the problem of 
non-existence in Definition 6.3 is avoided and the algebraic path problem reduces to 
finding a particular solution to a semiring fixpoint equation. 
6.3 QUASI-REGULAR SEMIRINGS 
We now focus on semirings where each element has at least one quasi-inverse. Then, 
it is always possible to introduce a third semiring operation called star operation 
that delivers a particular quasi-inverse for each semiring element. Such semirings 
are called closed semirings in (Lehmann, 1976) but to avoid confusions with other 
semiring structures, we prefer to call them quasi-regular semirings. 
Definition 6.4 A quasi-regular semiring {A, +, x, *, 0,1) is an algebraic structure 
where (A, +, x, 0,1) is a semiring and * an unary operation such that for a £ A 
a* = aa* + 1 = a* a + 1. 
Restricting Example 6.6 to non-negative integers, we see that quasi-inverse ele-
ments are not necessarily unique in a semiring. Consequently, there may be different 
ways of defining the star operation which each time leads to a different quasi-regular 
semiring. Here, we list some possible extensions to quasi-regular semirings for the 
examples that marked the beginning of this chapter: 
Example 6.7 The tropical semiring (M>o U {oo}, min, +, oo, 0) of non-negative real 
numbers (or integers) introduced in Example 5.4 is quasi-regular with a* = Ofor all 

QUASI-REGULAR SEMIRINGS 
2 3 3 
a G R>o U {oo}. This can be extended to the tropical semiring of all real numbers 
(K U {—oo, oo},min, +,oo, 0) by a* = 0 for a > 0 and a* = —oo for a < 0. 
Similarly, the arctic semiring from Example 5.5 (R>o U {—oo, oo}, max, +, —oo, 0) 
is quasi-regular with a* = oo for a > 0 and a* = Ofor a € {—oo, 0}. The Boolean 
semiring ({0,1}, max, min, 0,1) of Example 5.2 is quasi-regular with 0* = 1* = 1. 
This also holds for the bottleneck semiring (MU {—oo, oo}, max, min, — oo, oo) from 
Example 5.3 where a* = oo for all elements. The probabilistic or product t-norm 
semiring ([0,1], max, ·, 0,1) of Example 5.8 is extended to a quasi-regular semiring 
by defining a* = 1 for all a G [0,1]. Finally, the semiring of formal languages 
(■Ρ(Σ*), U, ·, 0, {e}) presented in Example 5.7 is quasi-regular with 
U 
г>1 
a = aUaaU aaa U . 
(6.16) 
Example 6.1 showed that these semirings were used in the introductory path problems 
of this chapter which are thus based on quasi-regular semirings. 
If we consider matrices with values from a quasi-regular semiring, then it is 
possible to derive quasi-inverse matrices from the star operation in the underlying 
semiring through the following induction: 
• For n = 1 we define [a]* = [a*]. 
• For n > 1 we decompose the matrix M into submatrices В, С, D, E such that 
В and E are square and define 
M* 
where F 
E 
В С 
D E 
+ DB*C 
* 
В* +B*CF*DB* 
F*DB* 
B*CF* 
F* 
(6.17) 
It is important to note that equation (6.17) does not depend on the way how the 
matrix M is decomposed. This can easily be proved by applying equation (6.17) to 
a matrix with nine submatrices, decomposed in two different ways: 
В 
E 
H 
С 
D 
F 
G 
J 
К 
and 
В 
С 
E 
F 
H 
J 
D " 
G 
К 
and by verifying nine identities using the semiring properties from Definition 5.1. A 
graph based justification of equation (6.17) will later be given in Example 6.12. The 
following theorem has been proposed by (Lehmann, 1976): 
Theorem 6.2 Let M G Л4(А,п) be a matrix with values from a quasi-regular 
semiring A. Then, the matrix M* obtained from equation (6.17) satisfies 
M* = MM* + 1 = M*M + 1 . 
(6.18) 

2 3 4 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
Proof: Because the two equalities in (6.18) are symmetric we only prove the first 
one. Let us proceed by induction: For n = 1 the equality holds by Definition 6.4. 
The induction hypothesis is that equation (6.18) holds for matrices of size less than 
n. For n > 1 suppose the decomposition 
M 
В 
С 
D 
E 
with В G Л4(А, к) for some n > к > 0. By equation (6.17) we have 
M* 
= 
B*+B*CF*DB* 
B*CF* 
F*DB* 
F* 
and therefore 
MM* 
= 
BB*+BB*CF*DB* + CF*DB* 
BB*CF* + CF* 
DB* + DB*CF*DB* + EF*DB* 
DB*CF* + EF* 
By the induction hypothesis we have 
B* = Ifc + BB* 
and 
F* = I n_ f c+FF*. 
This together with F = E + DB*C gives us the following four equalities: 
CF*DB* + BB*CF*DB* 
CF*+BB*CF* 
DB* + DB*CF*DB* + EF*DB* 
DB*CF* +EF* 
(Ifc + BB*)CF*DB* = B*CF*DB* 
(Ifc + BB*)CF* = B*CF* 
DB* + FF*DB* = 
(In_fe + FF*)DB* = F*DB* 
FF* 
We finally obtain 
I n + MM* 
I f c+BB*+B*CF*DB* 
B*CF* 
F*DB* 
I„_fc + FF 
B*+B*CF*DB* 
B*CF* 
F*DB* 
F* 
M*. 
Thus, given a quasi-regular semiring, we can find a quasi-inverse for each matrix 
defined over this semiring. In other words, each quasi-regular semiring induces a 
particular quasi-regular semiring of matrices through the above construction. Algo-
rithms to compute such quasi-inverse matrices will be presented in Chapter 9. Here, 
we benefit from this important result to reformulate the algebraic path problem of 
Definition 6.3 for matrices over quasi-regular semirings. 
Definition 6.5 For a matrix M € Л4(А, п) over a quasi-regular semiring A, the 
algebraic path problem consists in computing M* defined by equation (6.17). 

QUASI-REGULAR SEMIRINGS 
2 3 5 
It is important to note that the two Definitions 6.3 and 6.5 of the algebraic path 
problem are not equivalent and in fact difficult to compare. We mentioned several 
times that quasi-inverses do not need to be unique. Definition 6.3 presupposes a 
partial order and focusses on the computation of the least quasi-inverse with respect 
to this order. Quasi-regular semirings do not necessarily provide a partial order such 
that we obtain an arbitrary quasi-inverse from solving the algebraic path problem in 
Definition 6.5. But even if we assume a partial order in the quasi-regular semiring 
of Definition 6.5 and also that all sequences in Definition 6.3 converge (Aho et ai, 
1974), we do not necessarily have equality between the two quasi-inverses (Kozen, 
1990). We can therefore only observe that a solution to the algebraic path problem 
with respect to Definition 6.5 always satisfies the fixpoint equation of Theorem 6.1, 
although it is not necessarily the least element of the solution set. In Section 6.6 we 
will introduce a further semiring structure that extends quasi-regular semirings by 
idempotency and also adds a monotonicity property with respect to the star operation. 
In this setting, the quasi-inverse matrix obtained from equation (6.17) will again be the 
least solution to the fixpoint equation. We will now give an example of a quasi-inverse 
matrix which is obtained from the above construction: 
Example 6.8 We have seen in Example 6.7 that the tropical semiring of non-negative 
integers (N U {0, oo}, min, +, oo, 0) is quasi-regular with a* = 0 for all a G N U 
{0, oo}. Let us assume the following matrix defined over this semiring and compute 
its quasi-inverse by equation (6.17). The following decomposition is chosen: 
M 
00 
4 
00 
7 
00 
2 
1 
00 
00 
— 
В 
D 
С 
E 
With semiring addition as minimization, semiring multiplication as integer addition 
and oo* = 0 we first compute 
E + DB*C 
oo 
2 
oo 
2 
00 
00 
oo 
00 
+ 
+ 
4 
oo 
" 11 
oo 
[oo]* [ 7 
5 
oo 
= 
И 
' 11 
2 
5 
oo 
Applying equation (6.17) we obtain 
F* 
= 
11 
5 
2 
oo 
0 5 
2 0 
Since B* = [0] we directly conclude 
B*+B*CF*DB* 
= 
[0] 

236 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
and compute for the two remaining submatrices: 
B*CF* 
= 
CF* 
= [ 7 1 ] 
F*DB* 
F*D 
0 
5 
2 0 
0 
5 
2 
0 
4 
oo 
[3 1] 
0 
4 
6 
3 
0 
2 
1 
5 
0 
Putting these results together, we obtain the quasi-inverse matrix M*: 
M* 
If we interpret M ai a distance matrix, we immediately observe that M* contains 
all shortest distances. An explanation why the above construction for quasi-inverse 
matrices always leads to shortest distances in case of the tropical semiring over 
non-negative numbers will be given in Section 6.6. 
To conclude this section, Example 6.9 alludes to an alternative way of deriving 
the fixpoint equation of Definition 6.5 which is not motivated by the semiring power 
sequence, but instead uses Bellmann's principle of optimality (Bellman, 1957). 
Example 6.9 Let M be the adjacency matrix of a directed, weighted graph with 
edge weights from the tropical semiring (N U {0, oo}, min, +, oo, 0} of non-negative 
integers. We may then write equation (6.18) for X φ Y such that 1(X, Y) = oo as 
M*(X,Y) 
= 
mxa(M(X,Z) 
+ 
M*(Z,Y)Y 
Similarly, we obtain for X = Y 
M*(X,Y) 
min 
miniM(X,Z) 
M *(Z,Y)), 
0. 
The first equation states that the shortest path from node X to node Y corresponds 
to the minimum sum of the direct distance to an intermediate node Z and the shortest 
path from Z to the terminal node Y. This is Bellmann's principle of optimality. The 
second equality states that each node can reach itself with distance zero. 
The complete matrix M* of Example 6.9 contains the shortest distances between 
all pairs of graph nodes. It is therefore common to refer to the generalized com-
putational task of Definition 6.3 as the all-pairs algebraic path problem. If we are 
only interested in the shortest distances between a selected source node S and all 
possible target nodes, it is sufficient to compute the row of the matrix M* that cor-
responds to this source node, i.e. we compute bM* where b is a row vector with 
b(S) = 1 and b(Y) = 0 for all Y ^ S. It then follows from M* = MM* + I that 
bM* = bMM* + b. This shows that bM* is a solution to 
X = XM + b, 
(6.19) 

QUASI-REGULAR VALUATION ALGEBRAS 
2 3 7 
where X is a variable row vector. Hence, we refer to the task of computing bM* 
defined by equation (6.17) as the single-source algebraic path problem. In a similar 
manner, the column vector M*b is a solution to 
X 
= 
MX + b 
(6.20) 
and refers in the above instantiation to the shortest distances between all possible 
source nodes and the selected target node. The task of computing equation M*b 
defined by equation (6.17) is therefore called single-target algebraic path problem. 
Clearly, instead of solving the all-pairs problem directly, we can either solve the 
single-source or single-target problem for each possible vector b, i.e. for each possible 
source or target node. We will see in the following section that matrices over quasi-
regular semirings induce a family of valuation algebras called quasi-regular valuation 
algebras. It will be shown in Chapter 9 that the single-source algebraic path problem 
can be solved by local computation with quasi-regular valuation algebras. 
6.4 QUASI-REGULAR VALUATION ALGEBRAS 
In this section, we are going to derive a generic construction that leads to a new 
valuation algebra for each quasi-regular semiring. These formalisms, called quasi-
regular valuation algebras, are related to the solution of the single-target algebraic 
path problem or, in other words, to the computation of a single column of the quasi-
inverse matrix that results from equation (6.17). Until now, we have dealt in this 
chapter with ordinary semiring matrices. But for the study of generic constructions 
and valuation algebras, we will reconsider labeled matrices as introduced in Chapter 
1 for real numbers. This time, however, we assume labeled matrices with values 
from a quasi-regular semiring A. Thus, let r be a set of variables, s Ç r and 
M(A, s) the set of labeled matrices M : s x s —> A. Inspired by (Lehmann, 1976; 
Radhakrishnan et al., 1992; Backhouse & Carré, 1975), we observe that the pair 
(M, b) for M € M{A, s) and an s-vector b : s ->· A sets up the fixpoint equation 
X 
= 
MX + b 
(6.21) 
where X denotes a variable column vector with domain s. This equation can be 
regarded as a labeled version of equation (6.20) generalized to vectors b of arbitrary 
values from a quasi-regular semiring. We further define the set 
Φ = 
{(M,b) | M e M(A,s) andb : s -> Afors С г} 
(6.22) 
and also write d(M, b) = s if M e M{A, s). 
In order to introduce a projection operator for the elements in Ф, we first decompose 
the above system with respect to t С s and obtain 
X4. s-t 
X l t 
TV/fls — t,s — t 
TV/rJ^s — t,t 
xis-
b-U-

238 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
from which we derive the two equations 
v-Ls — t 
A|J.s-i,s-iy4.s-i _j_ i i f j s - ί , ί γ ΐ ί 
_j_ i_ls — É 
and 
χ-Ι-4 
= 
]у(4-4>«-'х-1·«-' _i_ ]γ^4-*.ίχ-1-ί I folt 
Lemma 6.2 points out that (MU-t^-ty^6^ 
with 
provides a solution to the first equation. Note, however, that b ^ - * is not a semiring 
value, so that our discussion so far is only informal. Nevertheless, let us proceed with 
this expression and insert it into the second equation. After regrouping of variables 
we obtain 
_l_ 
fyjl-t + 
fy[i-t,s~t(Mi's~t's~t)*bi's~t\ 
For (M, b) e Ф with ei(M, b) = s and t Ç s this indicates that we could define 
(M,b)^ 
= 
(M t,b t) 
(6.23) 
with 
M t 
= 
M i t' t + M i t' s- i(M i s- i , s- t)*M J- s- t' t 
(6.24) 
and 
b t 
= 
ь 4' + М ^ ' 8 - Е ( М ^ - ' ' 8 - ' ) * Ь ^ - ' 
(6.25) 
such that 
X4·* 
= 
M j X ^ + b t . 
The following theorem justifies this informal argument. It states that if a particular 
solution to equation (6.21) is given, then the restriction of this solution to the subdo-
main also solves the projected equation system. Conversely, it is always possible to 
reconstruct the solution to the original system from the projected solution. 
Theorem 6.3 For a quasi-regular semiring A with M € Л4(А, s), b : s —> A and 
t Ç s, the following statements hold: 
1. Ifx = M*b is a solution to X = MX + b then 
x w 
= 
(Mt)*bt 
(6.26) 

QUASI-REGULAR VALUATION ALGEBRAS 
2 3 9 
w a solution to Xй = MtXlt 
+ b t. 
2. //x = (Mt)*bt is a solution to X4·' = MtX4"* + b t and 
y 
= 
( M ^ - ' - ^ - ^ ^ M ^ - ' - ' x + b·1·3-4) 
(6.27) 
then (y, x) = M*b is a solution to X = MX + b. 
Proof: 
1. We first consider the submatrices of M* with respect to s — t and t. From 
M* 
j^jis-t,s-t 
Jyjis-ί,ί 
(M*)is-t,s-t 
(M*)-^-*'* 
(M;*)4-*.s—t 
( M * )-!■*,* 
we derive the following identity using equation (6.17) and (6.24): 
F = м.^'1 + M}t's~tCM.^s~t's~t)*M.^s~t't 
= Mj 
and then further: 
(M*)iS~f",~t
 
= 
fJ^is-t,S~ty 
+ 
( M i s " ' ' s ~ , ) * M i s _ l , t ( M i ) , M i t , s " 1 ( M l s " t , s " i ) * 
( M»)| s-t,i 
= 
( м ^ - ' ' " - ' ) * М ^ - ' ' ' ( М 4 ) * 
(M*)4·'·«-' 
_ 
(M i)*M i i' s~ t(Mi s~ t' s~ <)* 
(M*)4·'·' 
= 
(M t)' 
(6.28) 
If x — M*b is a solution to X — MX + b we have 
M*b 
= 
M M ' b + b. 
Decomposing the system with respect to s — t and t gives 
(M*)4- S _ t' s _ i 
(]y|*Us-t>t 
bis-t 
b i t 
bis-* 
b i t 
]yrii,s-i 
Mit.t 
(M*U«-t,«-t 
(M*)4-"-''* 
(M*)44·8"' 
(M*)4-«·' 
bis-t 
b i t 
We identify the following subsystem for t: 
Cjyplit.S-ÎbiS-t _|_ fM*)4"4'*^' 
= 
Mit.S-t/JyJiUs-t.S-tbis-t 
+ 
M^'^M*)4·*'8-^4·8"* 
+ 
M4-t'e-t(M*)4'e-t'tbt 
+ 
МШ{М*)ШЪ1Ь 
+ b 4'. 

240 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
If we insert the above identities for the left-hand side of this equation, we obtain 
(Μ*) 4-*' 8 -'^ 5 -' + (M*)4-*''^' 
= 
(Mi)*M4'i's~t(M4's~t's~i)*bJ's~t 
= 
(Mt)* ÎM.irt's~t(Mi-s~t's~tybi-s~t 
+ b i É) 
= 
(Mt)*bt. 
This follows from equation (6.25). Similarly, we obtain for the right-hand part: 
M-U.s-tQyp^-Ls-t.tb-U _). j^iJ-t't(M*)-|'i'tb·'·' + b 4 t 
= 
■jyjit^-t/l^jis-t.s-t'jtj^ls-t 
, 
■Mi't's~t(Mi's~t's~t)*Mi's~t't(Mt)*Mit's~t(M.is~t's~t)*bi's 
]Vi-l-t'i(Mt)*M-''t's_i(MJ's_t,s_t)*bJ's 
M-LM"~t(Mis~t,s-i)*M-l's~i't(Mt)*b4't H 
М 4 М(М()'Ь 4 1 + b i É. 
Reordering the terms of this expression gives: 
(ΊνΓ1"''4 + MJ-t's_t(M-1's"i's-t)*M-1-s~t't) (Mt)* 
(b^ + M^^-^M^-'·3-')*^5-') = мг(м*)*ь<. 
The last equality follows from equations (6.24) and (6.25). 
(Mt)*bt 
= 
M t(M t)*b t 
which by Lemma 6.2 proves that (M()*bt is a solution to 
Xй 
= 
M t X ^ + b « . 
This proves the first statement of the theorem. 
2. It follows from equations (6.25) and (6.28) that 
(Mt)*bt 
= 
(M t)*('b i i+M 4- t' s- i(M 4- s- i' s- t)*b i s- i) 
= 
(Mt)*biÉ + (Mt)*M4-t's_t(M4-s-t's-t)*b-l-s-i 
= 
( M * ) ^ ' ' ^ ' + (M*)J-t's~ibJ-a_t. 

QUASI-REGULAR VALUATION ALGEBRAS 
2 4 1 
Similarly, we derive 
y = (М 4- в _ 4 , а-')*(м 4- 8 _ мх + Ь4·8-*) = 
^MU-t,s-ty 
(M^-^iMtYbt 
+ bis-*) = 
(Mis-t,s-i)*M4S-t,t(Mt)*bt + ^Mu-t,s-tybu-t 
= 
(M4'8~t's~t)*M4-8~t't(Mi)* (b4-* + M4-t's_t(M4'8~t'8-i)*b4's_t') + 
(TvT'"s~t's-i')*b4's-t _ 
(Mie-t-"-t)*M4-e_ilt(Mt),b·1·* + 
('(Μ4-8~ί'8~ί)*Μ4-8~ί'ί(Μ()*Μ4-ί'8~ί(Μ4-8_ί'8~ί)* + 
(^/[U-t^-ty^u-t 
= 
(M*) 4· 8 -'''^* + (M*) 4 - 8 -'' 8 -'^ 8 -*. 
We therefore obtain: 
(y,x) = ((M*)4-8-'·'^* + (Μ*)^-*·'-^ 4·"-', 
(M*)4-5-*'8-' 
(M*)4-8-'-' 
bu-t 
b4t 
= M*b. 
(M*)4-*'8-' 
(M*)4-*'* 
But according to Lemma 6.2 M*b is always a solution to the fixpoint equation 
X = MX + b. 
This proves the second statement of the theorem. 
Subsequently, we use to say that x is a solution to (M, b) e Φ if x is a solution 
to the fixpoint equation X = MX + b. The following lemma highlights a direct 
consequence of the above theorem: 
Lemma 6.4 If x = M*b is a solution to (M, b) and i Ç s = d(M, b) then 
x = ((M t)*b t, (M e_ t)*b,_ t). 
Proof: If M*b is a solution to (M,b), then (Mt)*bt and (Ms_()*bs_t are 
solutions to (Mt, b t) and (M s_ t, b s_ t) by the first statement of Theorem 6.3. On 
the other hand, if (Mt)*bt is a solution to (Mt, b t) and 
yi = (м4'в-''в-'У(м4-"-*'*(М()*1^ + Ь·1-'-'), 

242 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
then ((M t)*b t,yi) = M*b, which follows from the second statement of Theorem 
6.3. Likewise, if (M s_ t)*b s_ t is a solution to (M s_ t,b s_i), there exists an extension 
y2 such that (y2, (Ms_t)*bs_t) — M*b. We conclude from 
x = 
((M t)*b t, У 1) 
= 
(y2, (M s_ t)*b s_ t) 
= 
M*b 
that yi = (M s_ t)*b s_ t and y2 = (Mt)*bt. 
■ 
We next prove that the projection operator is transitive. 
Lemma 6.5 For (M, b ) g $ with d(M, b) = s and tÇuÇswe 
have 
it 
(M,b) it 
((M,b)-^) 
(6.29) 
Proof: Since projection is defined component-wise in equation (6.23), we can prove 
the statement for M and b separately. Thus, consider the following decomposition: 
M 
A/[4-s — u,s—и 
~\/ris~u,u — t 
A4|s~ti,i 
'bjliu — t,s—и 
A/fJ-u~ t,u—t 
Λ/[4Μ —t,t 
]yj4.i,s-u 
-^/lit,u-t 
fy[4-M 
Applying equation (6.24) gives 
M 
= 
M^u'u + M"'-u's-ti(M'''s~u's_u)*M'''s-u'" 
which corresponds to 
Д/rju- t,s — и 
TV/f-l-w — t,u — t 
'\jiiu~t,t 
+ 
(~M^S~U'S~U)* \ ]VI-I-S_"'u_t 
M-l-s~u 
j^jj.i,s-u 
Executing the second projection to t Ç и gives 
(мЛ 
= (MiM
 + ми'"-и(м1з~и'з~иум1з~иЛ 
+ 
(Л/\№,и — t _ι_ ly/f-UjS — u('\/\is~u,s 
— U\*JL/TIS — U,U—t\ 
f Tur\.u — t,u—t I A/T-l-u — t,s — и C\/i-ls~u,s — u\*JUi^-s~u,u 
— t λ 
f-^ßlu—t,t 
i -^lu — 
t.s—uf^is-u,s-u\*-^is-u,t\ 
We define 
тр* 
/ д/г4-и —£,u —ί _|_ дл-4-u —ί,s — и(Л/\is —u,s — u\*iyr-|,s —u,u —ί ^ 

QUASI-REGULAR VALUATION ALGEBRAS 2 4 3 
and obtain by rearranging the above expression 
( M U ) 
= м + м + 
Mu's-u((Mis-u's-u)* 
_ι 
■^jritiu—tT^*-\jr],u 
— t,s — u/-\r\,s 
— u,s—u\*'pur\.s 
— u,t 
Using equation (6.17) we further conclude that 
( M U ) 
= M i M 
/ 
\ -Is — U,S — U 
/ 
\ \.U — t,S — t 
_|_ 
j^4t,u-t/^is-t,s-t^*\ 
j^Ls-u.t 
/ 
\ \.u— t,u — t 
and because s — t — (s — u) U (u — t) we finally obtain 
( M U ) = M i M + [ M^>S~U 
м^'и~1 
] (м4-*-'·5-')* 
= 
M4*'* + MJ-i's~t(M-'pa_t's_i)*M-''s_i't = Mi. 
A similar derivation proves the statement for the vector component. 
■ 
Complementary to the operation of projection for the elements Ф, we next intro-
duce an operation of vacuous extension. For (M, b) G Ф with d(M, b) = s and 
t D s, this operation corresponds for both components to the operation of matrix and 
tuple extension introduced in Chapter 1. However, instead of assigning the number 
0 G К to the variables in t — s, we take the zero element 0 G A of the quasi-regular 
semiring. We therefore have 
(M,b) t É = (M T M,b t ê). 
This operation is used for the introduction of combination: Assume (Mi,bi) G Ф 
and (M 2,b 2) G Ф withd(Mi,bi) = sandd(M 2,b 2) = t. We define 
( M b b i ) ® ( M 2 , b 2 ) = ( М ь Ь , ) ^ + (M 2,b 2) t s U i 
(6.30) 
= 
(MÎsUt'sUi + M j s U M U \ bî s U t + b^ s u i). 
Ml«-«,* 

244 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
The operation of combination just extends both matrices and vectors to their union 
domain and performs a component-wise addition. We show next that the combination 
axiom is satisfied by this definition. 
Lemma6.6 For ( M b b i ) , (M2,b2) G Φ wifAd(Mi,bi) = s, d(M 2,b 2) = tand 
sCzCsUtit 
holds that 
iznt 
( ( M i , b i ) ® ( M 2 , b 2 ) ) ' U 
= 
(Mi,bi)®(M 2,b2)-
Proof: We again consider the statement for both components separately. Consider 
the following decomposition with respect to z and (s U i) — z = t — z and zC\t. 
Here, О G Л4(А, s Lit) denotes the zero matrix for the domain sUt. 
■»«■tsUt,sUt 
, 
»«taUt.sUt 
Applying equation (6.24) gives 
(MÎ 
tsUi,SU< 
, 
·»/|■ÎSUt,SUÉ', 
M\z'z + (M^nt'2nt)T2,'z 
О 4-t —2,2 —ί 
M: 
it — z,z(~\t 
О 
M; 
M 
iz — t,t — z 
4,2Πί,ί —2 
it — z,t — z 
Μ\ζ·ζ 
+ (M^nt.zntjt*,* 
r\iz—t,t 
— z 
M: iz<~lt,t-z 
(Mlt-z,t-zy 
|- Qit-z,z-t 
Mit-z,znt 
j 
\z,z 
= 
M\z'z + ((M^ n t' 2 n t) 
+ 
(Ml*rit,t-z(Mu-z,t-zyMlt-z,znt ltz'z) 
M tz,z 
+ M: 
((»4 
г(М2 
ζ Π ί , ζ Π ί 
4·ΖΠί,ί — Ζ f-^/fit 
— Z,t — Z\*-K*lt 
— Ζ , Ζ Π ί λ 
' 
) 
= 
MJZ'Z + ( М 2 г г и ) ^ г . 
ф,2 
This proves that the combination axiom holds for the matrix components and a similar 
derivation applies to the vector components. 
■ 
All these properties imply the following theorem if D denotes the lattice of finite 
subsets of the countable set r of variables: 
Theorem 6.4 The system (Φ, D) with labeling, projection (6.23) and combination 
(6.30) satisfies the axioms of a valuation algebra. 
Proof: Since combination reduces to semiring addition, we conclude that Φ is 
a commutative semigroup under combination. Further, the axioms (A2), (A3) and 
(A6) follow directly from the definition of projection. Transitivity (A4) is proved in 
Lemma 6.5 and the combination axiom (A5) corresponds to Lemma 6.6. 
■ 

PROPERTIES OF QUASI-REGULAR VALUATION ALGEBRAS 
2 4 5 
How quasi-regular valuation algebras are used to solve path problems will be 
discussed in Section 9.3.2. There, we also compare this approach with the traditional 
way of solving path problems and with another generic construction that will be 
presented in Section 6.7 of this chapter. Knowing that labeled fixpoint equations 
over quasi-regular semirings satisfy the valuation algebra axioms allows us to apply 
the local computation architectures of Chapter 3 and 4 for their processing. But 
some architectures require additional properties such as, for example, the presence 
of neutral elements or idempotency of combination. Therefore, we next look for such 
properties in quasi-regular valuation algebras. 
6.5 
PROPERTIES OF QUASI-REGULAR VALUATION ALGEBRAS 
Since combination simply corresponds to matrix addition, we directly identify the 
neutral element for the domain s Ç r by the pair (О, о) of null matrix О G 
M(A,s) 
and null vector о : s -» A such that o(X) = 0 for all X e s. It is clear that 
this element behaves neutrally with respect to all other valuations of domain s and 
also that the combination of two neutral elements again results in a neutral element. 
Moreover, it follows directly from equation (6.23) that neural elements project to 
neutral elements. Together, this proves the following lemma. 
Lemma 6.7 Quasi-regular valuation algebras have neutral elements and are stable. 
In contrast to neutral elements, quasi-regular valuation algebras do not provide 
null elements, although the supplementary semiring property a + 1 = 1 for all a e A 
would in fact be sufficient to obtain an absorbing element for each domain. Such 
semirings were called bounded or simple semirings in Section 5.2. The corresponding 
absorbing elements also project to absorbing elements as a consequence of the sum 
in the definition of projection. But this sum also makes it possible that non-absorbing 
elements may project to absorbing elements which contradicts the nullity axiom of 
Section 3.4. Second, we also point out that quasi-regular valuation algebras are not 
idempotent, not even when semiring addition is idempotent due to the quasi-inverse 
in the definition of projection. These considerations show that quasi-regular valuation 
algebras do not provide a lot of interesting properties and are therefore rather poor 
in structure. First and foremost, it is the property of idempotency that would be 
desirable, since it enables the simple local computation architecture of Section 4.5 
which is especially suited for valuation algebras with polynomial time and space 
complexity. But we have just seen that imposing additional semiring properties such 
as idempotent addition is not sufficient to obtain an idempotent valuation algebra. 
In Section 6.7, we therefore introduce another family of valuation algebras for the 
solution of path problems which, besides tackling the all-pairs algebraic path problem 
directly, are always idempotent. This is achieved by moving the computational effort 
from the projection to the combination operation. However, the prize we pay is that 
more algebraic structure is needed in the underlying semiring as it is the case in a 
Kleene algebra presented in the following section. We will therefore dispose of two 
different families of valuation algebras for the solution of path problems. How path 

246 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
problems are solved with local computation, and how the two approaches differ from 
each other will be discussed in Chapter 9. 
6.6 
KLEENE ALGEBRAS 
This section introduces a special family of quasi-regular semirings called Kleene 
algebras (Conway, 1971 ; Kozen, 1990) which have many interesting properties. First, 
Kleene algebras are idempotent semirings and therefore partially ordered. Based on 
this order, they further satisfy a monotonicity law with respect to the star operation 
to guarantee that it always returns the least solution to the fixpoint equation (6.11). 
This provides the yet missing interpretation of quasi-inverse elements as a solution 
of path problems. Another important consequence of the monotonicity property is 
that the star operation in a Kleene algebra satisfies the axioms of a closure operator 
(Davey & Priestley, 1990). This important insight enables to derive a new generic 
construction based on matrix closures which always produces idempotent valuation 
algebras. However, let us first give a formal introduction to Kleene algebras and 
derive their most important properties: 
Definition 6.6 A tuple (A, +, x, *, 0,1) is called Kleene algebra if: 
• (A, +, x, 0,1) is an idempotent semiring; 
• 1 + aa* < a* for a G A; 
(Kl) 
• 1 + a* a < a* for a G A; 
(K2) 
• ax < x implies that a*x < xfor a,x G A; 
(КЗ) 
• xa < x implies that xa* < xfor a,x G A. 
(K4) 
Here, the relation < refers to the canonical partial order of an idempotent semiring 
defined in equation (5.4). The following properties are immediate consequences of 
the above axioms: 
Lemma 6.8 In a Kleene algebra (A, +, x, *, 0,1) we have: 
1. 1 < a* for all a G A; 
(KP1) 
2. a<a* for all a G A; 
(KP2) 
3. 0* = 1 == 1*; 
(KP3) 
4. a* a* = a* for all a G A; 
(KP4) 
5. (a*)* = a* for all a G A; 
(KP5) 
6. a < b implies that a* < b* for all a,b G A. 
(KP6) 
Proof: 

KLEENE ALGEBRAS 
2 4 7 
1. From (SP3) of Lemma 5.2 and Axiom (Kl) we conclude 1 < 1 + aa* < a*. 
2. By (SP2), (SP3) and Axiom (Kl), 1 < a* implies a < aa* < aa* + 1 < a*. 
3. 1 < 0* follows from (KP1) and 0* < 1 from Axiom (КЗ) taking a = 0 and 
x = 1. So, 0* = 1. Further, we conclude from (KP2), 1 < 1* and 1* < 1 
follows again from Axiom (КЗ) taking a — x — 1, hence 1* = 1. 
4. Using (SP3) and (Kl) we derive aa* < 1 + aa* < a*. From Axiom (КЗ) it 
follows that aa* < a* implies a*a* < a*. Conversely, we have 1 < a* by 
(KP1) and thus a* < a* a* by (SP2). 
5. From (KP2) follows that a* < (a*)*. Further, we derive from (KP4) and 
Axiom (КЗ) that a*a* < a* implies (a*)*a* < a*. We finally conclude from 
(KP1) and (SP2) that 1 < a* implies (a*)* < (a*)*a* < a*. Hence, it follows 
that (a*)* =a*. 
6. Let a < b and thus a < b* due to (KP2). Using (SP2) and (KP4) we then have 
ab* < b*b* = b* which implies a*b* < b* by (КЗ). Finally, we conclude from 
(KP1) and (SP2) that a* = a*l < a*b* < b*. 
We will next see that the star operation in a Kleene algebra is uniquely determined. 
Lemma 6.9 For each element a € A in a Kleene algebra (A, +, x, *, 0,1), there is 
a unique element a* G A that satisfies (Kl) to (K4). 
Proof: Let r G A be another element for a that satisfies (Kl) to (K4). From (Kl) 
and (SP3) we conclude that ar < r which in turn implies a*r < r by Axiom (КЗ). 
Since 1 < r we obtain using (SP2) that a* < a*r < r and therefore a* < r. On the 
other hand, we also have aa* < a* from (Kl) and (SP3) which implies that ra* < a* 
by (КЗ). Since 1 < a* we conclude using (SP2) that r < ra* < a* and therefore 
r < a*. Together, the two arguments prove r = a*. 
m 
Lemma 6.10 In a Kleene algebra it always holds that 
(a + b)* = 
a*(ba*)*. 
Proof: From (SP3) and (KP6) we derive 
a*(ba*)* < (a + b)* ({a + b) (a + b)* )* 
< 
(a + b)* ((a + b)*)* = (a + b)*(a + b)* = (a + b)*. 
The second inequality follows from (Kl) and (SP3), and the two equalities from 
(KP5) and (KP4). On the other hand, (Kl) and (SP3) imply that aa* < a*, hence 
aa*(ba*)* < a*(ba*)* by (SP2). aa* < a* also implies ba*(ba*)* < (ba*)* which 
together with (KP1) gives ba*(ba*)* < a*(ba*)*. Adding the two results gives: 
aa*(ba*)* + ba*(ba*)* < a*(ba*)* + a*(ba*)* 

2 4 8 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
thus by distributivity and idempotency 
(a + b)a*(ba*)* < a*(ba*)* 
which due to (КЗ) implies 
(a + b)*a*(ba*)* < 
a*(ba*)*. 
Finally, we conclude from 1 < a*(ba*)* that 
(a + b)* < {a + b)*a*{ba*)* < a*(ba*)*. 
The statement of Lemma 6.10 follows now by antisymmetry. 
■ 
Lemma 6.11 In a Kleene algebra it always holds that 
(a + b)* = (a* + b)* = (a + b*)* = 
(a*+b*)*. 
Proof: Using Lemma 6.10 and Property (KP5) we derive 
(a + b)* = a*(ba*y = a**(ba**)* = 
(a*+b)*. 
The other equalities follow directly from the commutativity of addition. 
■ 
The following lemma states what we have already mentioned in the introduction 
of this section. Namely, that Kleene algebras are quasi-regular semirings. 
Lemma 6.12 In a Kleene algebra we have for alla e A 
a* = aa* + 1 = a*a + 1. 
Proof: Using (KP1), (SP1) and (SP2) we have 1 + aa* < a* implies that a(l + 
aa*) < aa* and then 1 + o(l + aa*) < (1 + aa*). We conclude from (SP3) that 
a(l + aa*) < (1 + aa*). Applying (КЗ) we obtain a*(l + aa*) < (1 + aa*) and 
finally derive from 1 < 1 + aa* that a* < a* (1 + aa*) < (1 + aa*). It follows that 
a* < 1 + aa* which together with (Kl) implies equality. The second statement is 
proved in a similar manner. 
■ 
Thus, the star operation always provides a solution to the fixpoint equation (6.11) 
which allows us to consider Kleene algebras as special cases of quasi-regular semir-
ings. We again point out that multiple solutions to this equation may exist in a Kleene 
algebra, but due to Lemma 6.9, a* is the only solution among them that satisfies the 
properties (КЗ) and (K4). As shown in the following lemma, this implies that a* is 
the least element of the solution set. 
Lemma 6.13 The element a* in a Kleene algebra is the least solution to 
X = aX + 1 and X = Xa + 1. 
Proof: We know from Lemma 6.12 that a* is a solution to the two fixpoint equations. 
Let r e Abe another solution such that r — ar + 1 . From (SP3) we conclude ar <r 
which implies a*r < r by (КЗ). Since 1 < r we obtain o* < a*r < r by (SP2). ■ 

KLEENE ALGEBRAS 
2 4 9 
As a consequence of idempotency, all Kleene algebras are dioids. If we further 
assume that a particular Kleene algebra is a topological dioid according to Definition 
6.1 where the power sequence of equation (6.7) converges, then its limit is equal to the 
result of the star operation. The converse direction of this statement is not true: (Kozen, 
1990) gives a very artificial example of a Kleene algebra whose star operation does 
not correspond to the limit of the corresponding power sequence. However, besides 
being quasi-regular, all semirings of Example 6.7 are idempotent, and it can be shown 
that they all are topological dioids. Further, Example 6.10 identifies them as Kleene 
algebras under the particular definition of the star operation. This finally explains 
why the algebraic path problem of Definition 6.3 applied to these semirings results 
in minimum distances, maximum capacities, maximum reliabilities, etc. 
Example 6.10 All quasi-regular semirings of Example 6.7 are idempotent and their 
star operation satisfies (Kl) and (K2). We therefore only focus on the two axiom (КЗ) 
and (K4). Let us look at the tropical semiring in more detail: Taking non-negative 
numbers (M>o U {сю}, min, +, *, oo, 0) with a* = 0property (КЗ) is always satisfied 
since a* + x < x trivially holds. If we take all real numbers (R U {—οο,οο}, 
min, +,*, oo, 0) with a* = 0 for a > 0 and a* = —oo for a < 0 the property 
does still hold: For a < 0 we always have a* + x = —oo + x < x, for a = 0 
the statement holds as above and for a > 0, the assumption a + x < x cannot be 
satisfied. These tropical semirings are therefore Kleene algebras and we can show in 
the same manner that all other semirings from Example 6.7 are Kleene algebras too. 
Example 6.11 A important example of a quasi-regular semiring that is not a Kleene 
algebra is given by the arithmetic semiring (NU{0, oo},+, -,*,0,1) of non-negative 
integers. To make sure that 0 is indeed the zero element of this semiring, we define 
a x oo = oo for all a > 0 and 0 x oo = 0. Then, the star operation is defined 
as a* = oo for all a > 0 and 0* = 1. It can easily be shown that this satisfies 
the definition of a quasi-regular semiring. Alternatively, we may take the arithmetic 
semiring (K U {oo}, +,·,*, 0,1) of real numbers with the definition 
1 
a 
= 
Ϊ 
for а ф 1 and a* = oo for a = 1. This again fulfills the requirements for a quasi-
regular semiring and also explains why a* is called a quasi-inverse. We observe that 
both semirings are not idempotent and can therefore not be Kleene algebras. 
6.6.1 
Matrices over Kleene Algebras 
Let us now consider matrices with values from a Kleene algebra. We already observed 
in Section 5.1.2 that matrices over idempotent semirings again form an idempotent 
semiring. Further, all Kleene algebras are quasi-regular, which implies by Theorem 
6.2 that the result of the construction (6.17) applied to matrices with values from a 
Kleene algebra satisfies axiom (Kl) and (K2). It has further been shown in (Conway, 
1971) that they also respect the two monotonicity properties (КЗ) and (K4) which 
altogether proves the following theorem: 

250 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
Theorem6.5 Let (Л,+,х,*,0,1) be a Kleene algebra. Then, the semiring of 
Л4(А, п) with the star operation of equation (6.17) also forms a Kleene algebra. 
Due to the many algebraic properties of Kleene algebras, there are several defi-
nitions of the star operation for matrices which are all equivalent to equation (6.17). 
Here, we give one such alternative definition proposed by (Kozen, 1994) that adopts 
the nice graphical interpretation of Example 6.12 below. 
• For n = 1 we define [a]* = [a*]. 
• For n > 1 we decompose the matrix M into submatrices В, С, D, E such that 
В and E are square. We define 
M* 
where F 
В 
С 
D 
E 
В + CE*D. 
F* 
F*CE* 
E*DF* 
E*+E*DF*CE* 
(6.31) 
Lemma 6.14 In case of a Kleene algebra, the two definitions o/M* in the equations 
(6.17) and (6.31) are identical. 
We propose the proof of this lemma as Exercise F.4 to the reader. 
Example 6.12 Let us first write equation (6.31) for the case n = 2: 
M* = 
b с 
d e 
r 
e*df* 
Pee* 
f e*df*ce* 
(6.32) 
with f = b + ce*d. Then, consider the automaton of Figure 6.8. M*(l, 1) = /* 
represents all possible strings that bring the automaton from state 1 back into state 
1, i.e. we may either go directly to 1 using b, or change to state 2 with symbol c, then 
repeat symbol e an arbitrary number of times, which is expressed by e*, and finally 
return to state 1 with symbol d. This whole procedure can also be repeated indefinitely 
which altogether gives (b + ce*d)* = /* = M*(l, 1). The other components o/M* 
are interpreted in a similar manner. 
Figure 6.8 An interpretation of equation (6.32). 
The following property of matrices over Kleene algebras will later be useful: 

KLEENE ALGEBRAS 
251 
Lemma 6.15 Assume 
M* = [ B 
C 
[ D 
E 
" 
Then, it holds that В = В + CE*D or equivalently that CE*D < B. 
Proof: From (KP5) follows that M* = M** and therefore В = (В + CE*D)* 
by equation (6.31). Using (SP3) and (KP2) we then have В < В + CE*D < 
(В + CE*D)* = В and therefore В = В + CE*D. 
■ 
The proof of this lemma is based on a particular property of Kleene algebras 
that henceforth becomes important. Namely, that the star operation satisfies the three 
axioms of a closure operator (Davey & Priestley, 1990). It is therefore common to 
refer to a* as the closure of a G A. In terms of Kleene algebras, the properties of a 
closure operator are: 
1. a < a*, 
2. a < b implies that a* <b*, 
3. a* = a**. 
They follow directly from (KP2), (KP5) and (KP6). Subsequently, we are going to 
derive a new generic construction where each valuation corresponds to the closure of 
a matrix with values from a Kleene algebra. This generates a whole family of new 
valuation algebras called Kleene valuation algebras. The principal idea behind this 
construction is motivated from graph related applications as for example from the 
shortest distance problem. Here, it is obvious that the adjacency matrices of multiple 
graphs may share the same matrix closure, i.e. different graphs defined over the same 
set of vertices may have the same shortest distances. Let us explore this observation 
in more detail by reconsidering square, labeled matrices. Thus, let r be a set of 
variables, {A, +, x, *, 0,1) a Kleene algebra, Mi G M(A, s) and M 2 G M(A, t) 
with s,t Ç r. It is then easy to show that the following relation is an equivalence 
relation between labeled matrices: 
Mi = M 2 
if, and only if, 
d(Mi) = d(M2) and М*г = Щ. 
(6.33) 
This equivalence relation decomposes the set of labeled matrices into disjoint equiv-
alence classes of matrices with equal domains and closures. Moreover, since each 
closure belongs itself to a different equivalence class, they can be considered as rep-
resentatives of their associated classes. Thus, defining closures as valuations allows 
us to compute with a unique representation of graph related knowledge or information. 
Let us next introduce some operations to manipulate labeled matrices: For a Kleene 
algebra (A, +, x, *, 0,1) the direct sum of Mi G M(A, s) and M 2 G M(A, t) with 

252 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
s Π t = 0 and X, Y G s U t is defined as 
(Mi0M 2)(A-,y) 
= 
{ 
MipC.y) 
if X, Y e s , 
M2(Jf,Y) 
if X, 
Yet, 
0 
otherwise. 
(6.34) 
This operation satisfies the distributive law with respect to the closure operation: 
Lemma 6.16 It holds that 
(Μ!θΜ2)* = м^ем;. 
Proof: Let Oi e M. (A, sxt) and 0 2 G M. (A, t x s) be two zero matrices for the 
domains sxt and t x s. Applying equation (6.31) gives 
( Μ ! θ Μ 2 ) * = 
Mi 
Oi 
o2 м2 
o2 м; 
м^ем;. 
Next, we reconsider the usual projection operator for labeled matrices, but since 
we only deal with square matrices, we introduce the following shorthand notation: 
For M G M{A, s) and i Ç s w e define 
M4·* 
Mlt,i_ 
(6.35) 
However, in the context of Kleene valuation algebras, we need to redefine the opera-
tion of vacuous extension for labeled matrices. Instead of assigning the zero element 
of the semiring to all the new components, we assign the unit element to the diago-
nal elements and the zero element to all other components. Using the direct sum of 
matrices, this operation can be defined as follows: For M G M(A, s) and s Ç t 
M * 
Μ φ Ι . 
(6.36) 
where I G M(A,t — s) is the unit matrix for the domain t — s. The following 
lemma states that the application of the closure operation and vacuous extension are 
interchangeable. 
Lemma 6.17 For M G M(A, s) and s Çtwe have 
(мпУ 
= (M*V*. 
Proof: Due to Lemma 6.16 and (KP3), which implies I = I*, we have 
(м^у 
= (ΜΘΙ)* = (Μ*ΘΓ) = (Μ*ΦΙ) = (м*) . 

KLEENE VALUATION ALGEBRAS 
2 5 3 
6.7 KLEENE VALUATION ALGEBRAS 
We are going to show in this section that closures of labeled matrices over а Юеепе 
algebra (A,+,x,*,0,l) 
satisfy the axioms of a valuation algebra. For a countable 
set of variables r and its lattice of finite subsets D we first define the set of matrix 
closures as 
Ф = 
{M* | M G M(A, s) and s G D}. 
(6.37) 
Under the instantiation of closure matrices as shortest distances in a graph, the 
operation of projection simply corresponds to dropping distances. On the other hand, 
vacuous extension corresponds to adding new graph nodes which cannot be accessed 
from other nodes. It is therefore clear that both operations do not affect other distances. 
This observation is generalized by the following two lemmas. 
Lemma 6.18 Ф is closed under projection. 
Proof: We show that for M* G Ф and s Ç d(M*) = t it holds that 
(M*)ls = 
((M*)+SV. 
First, observe that it is always possible to decompose M* such that 
M* 
= 
В 
С 
D 
E 
whereB e M(A,t-s)andE 
G M (A, s). We then have (M*)4·8 = E and therefore 
((M*)-18)* 
= E*. 
We conclude from (KP2) that E < E*. But using (KP5) and (6.31) we also obtain 
E = (M*)is 
= (M**)ls 
= E*+E*DF*CE* 
which implies E* < E according to (SP3) of Lemma 5.2. This proves that E = E*. ■ 
Lemma 6.19 Ф is closed under vacuous extension. 
Proof: This follows from Lemma 6.17 and (KP5): 
((M*)tÉ)* = ((M*)*) 
= (M*) 
We next introduce a very intuitive combination rule for elements in Ф. Imagine, 
for example, that we have two closure matrices which express the shortest distances 
between two possibly overlapping regions of a large graph. Then, the shortest distance 
matrix for the unified region can be found by vacuously extending the two matrices 

254 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
to their union domain, taking the component-wise minimum which corresponds to 
semiring addition and computing the new shortest distances. Thus, for Ml, MJ £ Φ 
with d(M{) = s and d(M2) = i w e define 
MI®M*2 
= ({MiysUt + (M*2)tsUtY. 
(6.38) 
We directly conclude from this definition that Φ is also closed under combination. 
Moreover, Φ becomes a commutative semigroup as shown by the following lemma. 
Lemma 6.20 Combination in Φ is commutative and associative. 
Proof: 
Commutativity of combination follows directly from the commutativity 
of addition in a semiring. To prove associativity, assume MJ, Mj, MJ € Φ with 
d{Ml) = s, d(M%) = t and d(M*z) = u. Using Lemma 6.17 we obtain: 
( M Î ® M ^ ) ® M ^ 
tsUiUu 
tsUtUu 
((M«.) t s U i
 + ( M*)fsuA 
((MÎ) t s U t + (M;) t s U f) 
i(M*)tsUtUu 
+ (M2) t s U t U u) 
+ (M;) t s U t U u 
+ (M;) t s U t U u 
+ (M;) t s U i U u) 
((MÏ) t s U Î U u + (M^)TsUtu" + (M^) t s U i U u)*. 
The last equality follows from Lemma 6.11 and the associativity of addition. Exactly 
the same expression can be derived in a similar way for MJ (g> (Mj <8> MJ) which 
proves associativity. 
■ 
Next, we verify the combination axiom: 
Lemma 6.21 If Ml, M^ e Φ, d(Ml) = s, dfMj) = t and s Ç z С s U t we have 
{Μ\®Μ*2γζ 
= 
M* ® (M;) i z n t. 
(6.39) 
Proof: From the definition of vacuous extension, it follows directly that vacuous 
extension can be performed step-wise. We therefore obtain 
(Mî)tsUf = ΠΜΙΫΛ 
= (Mi)tz< Ί , 
where I is the unit matrix with domain t — z. On the other hand, we also observe that 
since ( s U f ) - z = t - 2 
(M£)tsUt 
= 
((M*2)unt) 
( ( M ^ s U i ) 
\,z,t — z 

KLEENE VALUATION ALGEBRAS 
2 5 5 
Since Φ is closed under vacuous extension, we may apply Lemma 6.15 and obtain 
- 
((M2)lznt) 
+ 
4-t — 2,2 
( ( M ^ 2 ™) ' * 
= 
((M^) i 2 n i) 
+ 
\.z,t — z 
((M;) t s U t) *' 
z (Μΐ)1ί~ζ 
((M^)TsUt) 
We next compute 
ΜΪ ® M£ 
= 
( (Mt) t s u t + (M*) t s U i 
\,z, t — z 
(Mi)** + ((MS)·1·*™) * 
((M^)t s U t) 
((M*2ysUty~z'z 
I + (M*2)^-Z 
(Mtf* + ((M^zntYz 
((м*2узиЛ 
((M^) t s U t) *"г'г 
( м ^ ' - г 
iz,t—z 
This follows from (KPl) and because Ф is closed under projection. We next determine 
the closure of this matrix using (6.31) and project the result to the domain z: 
(MÏ®M*) i 2 
= 
+ 
(M^) t z + ({M*2)iznt) 
z 
((M^) T s U t) i 2 , t~ 2 (M*)4-'-2 ((M;) t s U t) 
\.t — z,z 
(Ml)tz 
+ ((M*) i 2 n i) t« 
M Î ® (M5) 
*\4-znt 
All these properties imply the following theorem if D denotes the lattice of finite 
subsets of the countable set r of variables: 
Theorem 6.6 The system (Φ, D) with labeling, projection (6.35) and combination 
(6.38) as defined above satisfies the axioms of a valuation algebra. 
Proof: Axioms (A2) to (A4) and (A6) follow directly from the above definitions. 
Axiom (A1 ) is proved in Lemma 6.20 and Axiom (A5) in Lemma 6.21. 
■ 
Closures of labeled matrices with values from a Kleene algebra therefore provide 
another example of a generic construction that leads to as many new valuation 
algebra instances as Kleene algebras exist. How exactly Kleene valuation algebras 
are used to solve path problems will be discussed in Section 9.3.3 where we also 
compare this approach with the solution of factorized path problems using the quasi-
regular valuation algebras from Section 6.4. Remember, one reason that motivated the 

256 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
introduction of Kleene valuation algebras was the promise that they always produce 
idempotent valuation algebras. This and other properties will next be studied. 
6.8 
PROPERTIES OF KLEENE VALUATION ALGEBRAS 
Kleene algebras have many interesting properties as we have seen in Section 6.6. It is 
therefore not astonishing that Kleene valuation algebras are also very rich in structure. 
In fact, the following results show that Kleene valuation algebras are idempotent and 
always provide neutral elements. 
Lemma 6.22 Kleene valuation algebras have neutral elements and are stable. 
Proof: We first show that the neutral element for the domain s £ Dis given by the 
identity matrix I € M(A, s). Due to Property (KP3) we have I* = I and therefore 
I g Φ. Further, we obtain for M* € Φ with d(M*) = s 
M*® I = (M*+I)* = (M*)* = M*, 
due to (KPl) and (KP5). It is furthermore clear that projecting a neutral element 
always results in a neutral element for the subdomain which proves stability. 
■ 
Lemma 6.23 Kleene valuation algebras are idempotent. 
Proof: For M* e Φ with s Ç t = d(M*) we have 
м*®(м*;г = M* + ((M*)-1-5) 
pyLsyl-t 
M** = M*. 
This follows from the idempotency of addition, (KPl) and (KP5). 
■ 
However, Kleene valuation algebras generally do not have null elements. Even 
if we assume an element for each domain that behaves absorbingly with respect to 
combination, the nullity axiom of Section 3.4 will not be satisfied. Since projection 
only drops matrix rows and columns, there may always exist other valuations that 
project to null elements which contradicts the nullity axiom. 
6.9 FURTHER PATH PROBLEMS 
This chapter presented two different families of formalisms to model path problems 
that both satisfy the valuation algebra axioms. Namely, these are quasi-regular valua-
tion algebras and Kleene valuation algebras. Also, it was shown that Kleene valuation 
algebras provide more algebraic structure as for example the property of idempotency 
that is not fulfilled in quasi-regular valuation algebras. Looking at the proof of Lemma 
6.23, we immediately see that idempotency of combination is a consequence of idem-
potent addition and the monotonicity laws in a Kleene algebra. Both requirements 
are not present in a quasi-regular semiring. On the other hand, the absence of these 

FURTHER PATH PROBLEMS 
2 5 7 
properties makes quasi-regular semirings more general than Юеепе algebras, and the 
same holds for their induced valuation algebras. Since all path problems shown at 
the beginning of this chapter are based on Kleene algebras, we end with a few more 
examples based on only quasi-regular semirings. These applications can therefore not 
be solved in the formalism of Kleene valuation algebras. The examples are based on 
the arithmetic semirings of non-negative integers and of real numbers from Example 
6.11 that are both quasi-regular but not Юеепе algebras. The first instance is a typical 
graph-related application, whereas the subsequent problems are not directly related 
to graphs anymore. This keeps the promise articulated in the introduction that the 
algebraic path problem also covers such rather unexpected applications. 
■ 6.6 The Path Counting Problem 
Figure 6.9 shows a network similar to the connectivity problem of Instance 6.1. 
This time however, we ask for the number of paths that connect the source node 
S with the target node T. We therefore compute the sum of the weights of all 
possible paths leading from S to T, where the weight of a path corresponds to 
the product of its edge weights. We compute for the three possible paths: 
0-1 
= 
0 
1 1 1 
= 
1 
1 - 1 - 1 1 
= 
1 
and then 
0 + 1 + 1 = 
2. 
Clearly, this description corresponds to equation (6.6) based on the arithmetic 
semiring of non-negative integers. 
Figure 6.9 The path counting problem. 
■ 6.7 Markov Chains 
A time-homogeneous, first order Markov chain is specified by a countable 
sequence of random variables Χι,Χι,... 
taking values from a finite state 

258 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
space Ω and satisfying the property: 
Ρ(Χη+ι 
=χ\χ\ 
=χι,-··,Χη-χη) 
P\Xn+\ 
— χ\Χη — %n) 
for all n G N and x,Xi G Ω. Thus, the probability to arrive in state x at some 
time depends only on the last state but not on previous states. Such a Markov 
chain is determined by its transition matrix M : Ω χ Ω —> [0,1] satisfying 
ΣΜ(Χ,Υ) 
= 1 
Yen 
for all X G Ω, where the value M(X, Y) spécifies the probability to arrive in 
state Y from the previous state X. A Markov chain can always be represented 
by a weighted, directed graph where the nodes correspond to the states and 
the edge weights to the transition probabilities. Let us for example assume a 
Markov chain with state space Ω = {1,2,3} and transition matrix 
M 
0 
0.6 
0 
0.1 
0 
1 
0.4 
0.9 
0 
Its graph is shown in Figure 6.10. For two selected states S and T, the value 
of the matrix power Mn(S', T) corresponds to the weight of the path between 
S and T that contains exactly n edges. If the computations are executed in 
the arithmetic semiring of real numbers (R U {oo}, +,·,*, 0,1), then this path 
weight corresponds to the probability of reaching state T in n steps from 
node S. Accordingly, the values of the quasi-inverse matrix M*(5, T) refer 
to the sum of all probabilities of reaching state T from state S in an arbitrary 
number of steps. It is clear that this will not be a probability anymore. If we 
for example obtain M*(5, T) = oo, we say that the state Г has^mie hitting 
time from state S. Moreover, if a state has finite hitting time from itself, then 
it is called recurrent. Such states return to themselves infinitely often with 
probability 1. On the other hand, if we obtain a value different from oo, then 
the state is called transient and leads back to itself only a finite number of times. 
Identifying recurrent and transient states in Markov chains is an important task 
that can thus be seen as an instance of the algebraic path problem. For the above 
matrix M we obtain by equation (6.17): 
1 
oo 
oo 
0 
0 0 
0 0 
0 
oo 
oo 
M* 
The diagonal values tell us that state 1 is transient and all other states are re-
current. Alternatively, we may repeat these computations with the probabilistic 
semiring ([0,1], max, ·, *, 0,1) where a* = 1 for all a G [0,1]. This is a Kleene 
algebra as shown in Example 6.10 and M*(5, T) corresponds to the value of 

FURTHER PATH PROBLEMS 
2 5 9 
the most likely path from state S to T. We obtain for the above example: 
M* 
= 
1 
0.6 
0.54 
0 
1 
0.9 
0 
1 
1 
The reader may convince himself that 0.6 · 0.9 — 0.54 is indeed the highest 
probability among all sequences of transitions that lead from state 1 to state 3. 
We refer to (Norris, 1998; Meyn & Tweedie, 1993; Kemeny et ai, 1960) for a 
systematic discussion of Markov chains. 
Figure 6.10 A graphical representation of a Markov chain. 
6.8 Numeric Partial Differentiation 
The numerical computation of the partial derivations of a function can be 
reduced to an algebraic path problem as shown in (Rote, 1990), from where we 
also borrow this example. Assume the following function over three variables: 
f{Zx,Z2,Z3) 
= 
Z2Z3 + y/(Z2Z3)2 + 4Z2Z2 
2Z\ 
This function is step-wise computed by the following program: 
Yi 
:= 
Z2Z3 + ^/(Z2Z3)2 
+ 4Z2Z2 
Yi 
2 \ \ 2 
(Yi I (2Zi)) 
This is a rather simple example but we could as well imagine some compli-
cated program involving loops and conditional statements. However, we next 
decompose this program into elementary operations: 
A 
В 
С 
D 
— z2 ■ z3 
= 
A2 
= zl 
= zl 
E 
G 
H 
I 
= 
CD 
= 
A-E 
= 
B + G 
= VH 
YI 
J 
К 
Y2 
= 
A + I 
= 
2-C 
= 
Y/J 
= 
K2 
This decomposition allows us to construct the computational graph of Figure 
6.11 by the following procedure: First, a node for is created for each variable 

260 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
Z\ to Z3. Then, we add a node for each elementary operation and connect it 
with a directed edge to its arguments. If the expression contains a constant, 
then we first add an additional node for this value. 
Figure 6.11 The computational graph of a function. 
Now, let us look at some node w with two outgoing edges leading to и 
and v. This subtree represents a composition function w(u,v), whose partial 
derivatives dw/dZi can be determined by the chain rule: 
dw 
dw 
du 
dw 
dv 
Note that dw/ди and dw/dv can easily be calculated, since only elementary 
operations are involved. Let us for example take w = u/v. We then have 
dw/ди = 1/v and dw/dv = —u/v2 = —w/v. It is therefore possible to 
assign the weight Μ(ιυ, υ) = dw/dv to each edge (w, v) of the computational 
graph. We then obtain for the above application of the chain rule: 
Xw,zt 
= M(w,u)Xu,Zi 
+ 
M(w,v)Xv,Zi 
which is a recursive expression in the unknowns 
Xu,Zi = du/dZi 
and Xv,Zi = dv/dZ^. 
For the root node r we have Xr,Zi = df/dZi. 
By developing this expression 
recursively, we obtain the derivative of / with respect to Zi by the sum of 
the weights of all paths leading from / to Zi, whereas the weight of a path 

FURTHER PATH PROBLEMS 
261 
corresponds to the product of its edge weights. So far, we considered the edge 
weights as symbolic expressions. But if we assign values from the arithmetic 
semiring of real numbers from Example 6.11 to the variables Z\ to Z3, then 
all edge weights also correspond to values in this semiring. Then, the above 
scheme for calculating the partial derivations of / amounts to the solution 
of a path problem in the quasi-regular semiring of real numbers. Moreover, 
computing the Jacobi matrix (dfj/dZi) 
then corresponds to a submatrix of 
the quasi-inverse matrix that is obtained from solving the all-pairs problem on 
this setting. We refer to (Rote, 1990) for a more comprehensive analysis of this 
approach and for related references. 
6.9 Matrix Multiplication 
Consider a quasi-regular semiring (A, +, x, *, 0,1) and two square matrices 
Mi, M2 : {1,..., n} x {1,..., ra} —> A defined over the same index set. 
The following idea of reducing the task of multiplying Mi and M2 to the 
computation of a quasi-inverse matrix was proposed by (Aho et ai, 1974). We 
first construct a new matrix M : { 1 , . . . , 3n} x { 1 , . . . , 3n} —»■ A defined as: 
M 
О 
Mi 
О 
О 
О 
M 2 
О 
О 
О 
We assume the following decomposition of M and compute the quasi-inverse 
M* by equation (6.17): 
M* 
О 
Mi 
О 
О 
О 
О 
о 
м2 
о 
в с 
D 
Е 
Since D and E are zero matrices, we directly obtain F = E + DB*C 
and thus F* = I as a consequence of equation (6.18). We also compute 
О 
В* 
О 
Mi 
О 
О 
I 
Mi 
О 
I 
and finally obtain for the three remaining submatrices: 
B*CF* 
= 
B*C = 
F*DB* 
= 
DB* = 
I 
Mi 
О 
I 
О 
M 2 
M i M 2 
M 2 
О О ] 
and В* + B*CF*DB* = В*. This determines the quasi-inverse matrix as 
M* 
I 
Mi 
MjM 2 
О 
I 
M 2 
O O 
I 

262 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
Two square matrices Mi and M2 with equal index sets and values from a 
quasi-regular semiring can thus be multiplied by computing the quasi-inverse 
matrix of M and extracting the corresponding submatrix. 
6.10 
CONCLUSION 
This chapter started with an introduction to the algebraic path problem which requires 
to solve a particular fixpoint equation of matrices taking values from a semiring. In 
the general case, such equations do not necessarily have a solution, but if they exist, 
solutions are called quasi-inverses. In order to avoid the problem of non-existence, we 
limited ourselves to so-called quasi-regular semirings where at least one quasi-inverse 
exists for each semiring element. Based on the construction of (Lehmann, 1976) it is 
then possible to compute a particular quasi-inverse of a matrix from the quasi-inverses 
of the underlying semiring. This leads to a first generic construction called quasi-
regular valuation algebra where valuations correspond to labeled pairs of matrices 
and vectors over a quasi-regular semiring. Such valuation algebras can be used for 
the solution of the single-target algebraic path problem which will be discussed in 
Section 9.3.2. A second approach focussing directly on the solution of the all-pairs 
algebraic path problem is based on a special family of quasi-regular semirings called 
Kleene algebras. Under this setting, we always obtain the least quasi-inverse for each 
semiring element and the corresponding operation further satisfies the axioms of a 
closure operator. These two properties lead to another family of valuation algebras 
where valuations are closures of labeled matrices over a Kleene algebra. Projection 
corresponds to simple matrix restriction and combination to the computation of the 
closure after taking the sum of the two factor matrices. In contrast to quasi-regular 
valuation algebras, the second approach therefore moves the computational effort 
from the projection operation to the combination which manifests itself in the fact 
that Kleene valuation algebras are always idempotent. This combination essentially 
consists in the computation of a quasi-inverse matrix which, according to (Lehmann, 
1976), is possible in polynomial time. Algorithms for this task will be presented in 
Chapter 9 where we also describe the solution of factorized path problems using quasi-
regular and Kleene valuation algebras. Further, we observe that both approaches only 
store matrices and vectors which also implies that space complexity is polynomial. 
We therefore have two generic constructions that both induce valuation algebras with 
a pure polynomial behaviour. 
PROBLEM SETS AND EXERCISES 
F.l * Instance 6.5 describes the task of determining the language that brings an 
automaton from state S to state Г as a path problem over the semiring of formal 
languages (P(T,*),U,-,®,{e}) 
from Example 5.7. Alternatively, this can also be 
interpreted as the problem of listing all paths between node S and node T in a graph, 
provided that the edge weights express the identifier of the corresponding connection. 

EXERCISES 
263 
However, the set of all possible paths between two nodes is often infinite. Thus, we 
are rather interested in listing only simple paths that contain every edge at most 
once. Identify the corresponding subsemiring of the semiring of formal languages 
and determine whether it is still a Kleene algebra. 
F.2 * Testing whether a graph is bipartite: An undirected graph is bipartite, if it 
contains no cycle with an odd number of edges. Since cycles are special paths, we can 
formulate this as a path problem. Consider the set of symbols A = {0, E, O, EO}. 
The weight of a path is either E, if it contains an even number of edges, or O, if it 
contains an odd number of edges. Accordingly, the weight of a set of paths is either 0, 
E, О or EO, depending on whether the set is empty, contains only even paths, only 
odd paths or both types of paths. Continue the development of this path problem by 
specifying how the edge weights are initialized and how the semiring operations are 
defined. Prove that the semiring is quasi-regular and test if it is also a Kleene algebra. 
The solution can be found in Section 6.7.1 of (Rote, 1990). 
F.3 * Identifying cut nodes of a graph: A cut node in a connected graph is a node 
whose removal causes the graph to become disconnected. Formulate the problem 
of finding cut nodes as a path problem and analyze the underlying semiring. The 
solution can be found in Section 6.7.2 of (Rote, 1990). 
F.4 * Prove the following property of Kleene algebras: For a,b e Awe have 
(ab)*a = 
a(ba)*. 
(6.41) 
The solution to this exercise is given in Corollary 5 of (Kozen, 1994). Then, prove 
Lemma 6.14 by applying this identity and the properties of Kleene algebras listed in 
Section 6.6. The solution to this exercise is given in Chapter 3 of (Conway, 1971). 
F.5 * We have seen in Section 3.1 that in case of valuation algebras defined over 
variable systems, the operation of projection can be replaced by variable elimina-
tion. We may then use the simpler versions of the valuation algebra axioms given in 
Lemma 3.1. Develop the quasi-regular and Kleene valuation algebra from Section 
6.4 with variable elimination instead of projection. 
F.6 * * 
It is sometimes necessary to solve many path problems with different Kleene 
algebras over the same graph. Instead of repeating the computations each time from 
scratch, we propose a two-stage compilation process: We first determine some formal 
expression for the paths between S and T which can later be evaluated for different 
Kleene algebras without recomputing the paths. To define this formal language, we 
assume a symbol νΰχ^γ, 
if an edge between node X and Y exists. In addition, we 
identify the two semiring operations + and x with the symbols 0 and 0. Then, the 
formal expression for the path between node S and T in Figure 6.1 is 

264 
VALUATION ALGEBRAS FOR PATH PROBLEMS 
WS^T 
— WS->A © VJA^T ® 
WS^-B 
© Wfl-tD Θ WD-^T ® 
WS^B © WB^C © 
Wc^DWD^T-
This string can now be evaluated by replacing the symbols with concrete edge values 
and operations from а Юеепе algebra. 
a) Specify this formal language completely such that it forms а Юеепе algebra. 
The solution to this exercise can be found in Section 3.2.2 of (Jonczy, 2009). 
There, it is also shown how such path expressions can be represented and 
evaluated efficiently, and that many further queries can be computed that 
go far beyond the computation of path weights. 
b) Instance 6.8 considers the numeric computation of partial differentiations 
as a path problem. Apply the compilation technique to this particular path 
problem for the aim of symbolic differentiation. 

CHAPTER 7 
LANGUAGE AND INFORMATION 
Information often concerns the values of variables or the truth of propositions sat-
isfying equations or logical formulae. It is in many cases stated through linguistic 
constructs with a determined interpretation. This general situation is a source of many 
valuation algebras that all have the interesting property of idempotent combination. 
Moreover, they always provide neutral and null elements and therefore adopt the 
structure of an information algebra according to Section 4.2.1. In such cases, the 
computations are usually not carried out on the level of the information or valua-
tions, but rather in the corresponding linguistic system using syntactic procedures. In 
this chapter, we illustrate the general concept of representing information by formal 
languages with a determined interpretation. We first give two important examples 
of such formalisms and then derive a more general concept that serves as a generic 
construction to produce the two previous examples and many other instances. 
In the first section, we show how propositional logic serves to state the truth about 
propositions or logical variables. The interpretation of propositional formulae repre-
sents the information expressed by the formulae. We shall show that such pieces of 
information form a valuation algebra and more particularly an information algebra. 
The second example presented in Section 7.2 concerns systems of linear equations 
Generic Inference: A Unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc. 
265 

266 
LANGUAGE AND INFORMATION 
whose solutions provide information about the values of a set of variables. Their 
solution spaces are again shown to form an information algebra, and the exploitation 
of this algebraic structure leads to sparse matrix techniques for the solution of sparse 
linear systems, see Chapter 9. In this chapter, we only focus on the algebraic proper-
ties of linear equation systems. Although we consider fields instead of semirings, this 
nevertheless is closely related to the path problems of the foregoing chapter that also 
require the solution of particular equation systems. We therefore move the discussion 
of the computational aspects of all these applications to Chapter 9. Finally, the con-
cluding section of this chapter presents the abstract generic framework generalizing 
both linguistic systems. There, we also allude to some further important instances 
that emerge from this new generic construction. 
7.1 
PROPOSITIONAL LOGIC 
Prepositional logic or sentential calculus is concerned with the truth of elementary 
propositions or with the values of Boolean variables. With regard to information, 
the question addressed by this formalism is which propositions or variables are 
true and which are false. Such truth values are usually expressed by Ί ' for true 
and Ό' for false, but the available information regarding these values may also be 
expressed in the propositional language. In most cases, these expressions are much 
simpler and shorter than the explicit enumeration of all possible truth values for 
the variables under consideration. Subsequently, we define this formal language 
and equip it with an interpretation to show how propositional information (i.e. the 
information about the truth values of Boolean variables) is expressed. We then point 
out in the following section that a dual pair of valuation algebras arises from the 
language and its interpretation. 
7.1.1 Language and Semantics 
The language of propositional logic Cp is constructed over a countable set of proposi-
tional symbols or propositional variables p = {P\, P2,...}. It consists of well-formed 
formulae (wff) defined inductively as follows: 
1. Each element P £ pas well as T and _L are wffs (called atomic formulae). 
2. If / is a wff, then ->/ is a wff. 
3. If / and д are wffs, then / Л д is a wff. 
4. If / is a wff and P € p then (ЭР)/ is a wff. 
All wffs are generated from atomic formulae by finitely many applications of the 
rules 2, 3 and 4. The construction 4 is usually not part of the propositional language, 
but it suits our purposes. The symbol 3 is called existential quantifier. We conclude 
from these considerations that any wff has finite length and contains only a finite 
number of propositional symbols. For simplification, it is also common to extend this 

PROPOSITIONAL LOGIC 
267 
language by adding the following constructions: 
1. 
/ V f l 
:= 
-.(-/Λ-.0); 
2- 
f-*g 
■= 
-./V f f; 
3. 
/ « J 
:= 
(/-► s) Л (g-►/). 
The intended meaning of these constructions is given by the interpretation of the 
formulae defined below. But before doing this, let us remark that we often consider 
propositional languages over subsets q Ç p. In particular, we often limit ourselves to 
finite subsets q of propositional symbols. So, Cq is defined as above by replacing p 
in Rule 1 by q. The subsets of p form a lattice under inclusion with intersection as 
meet and union as join, see Example A.2 in the appendix of Chapter 1. This lattice 
structure is also reflected in the family of languages Cq as follows: 
1. If pi С p2 we also have £ P l С СР2; 
*·· *^PlDp2 
= 
J-Pl ' '•^-P2 
= 
^-7>1 ΛΖ·,ρ 2, 
J · ^-piUp 2 
*-"Pl V ^P2 
= 
I Н ^ д ' ^~Pl ^ ^ P 2 — ^ 9 J · 
In particular, we have CQ = {_L, T} for the language without propositional symbols. 
We now give meaning to this language by interpreting its wffs. This is achieved 
by mappings v : p —> {0,1} which are called valuations. Here, valuations do not 
correspond to elements of a valuation algebra but to the traditional name of an 
assignment of truth values to propositions which is equivalent to the notion of tuple, 
configuration or vector of Chapter 1. However, we accept this double sense for a short 
while and always explicitly refer to the valuation algebra when the context changes. 
Valuations are extended to propositional formulae v : Cp —» {0,1} by: 
1. v ( P ! ) = v ( P i ) f o r a l l P i e p ; 
2. v(T) = l a n d v ( l ) = 0; 
3. v(-./) = 1 if v(/) = 0 and v(^/) = 0 otherwise; 
4. v ( / Л д) = 1 if v(/) = 1 and v(g) — 1, v ( / Л g) = 0 otherwise; 
5. v((3P)/)=v(/[P/T]V/[P/±]). 
Here, f[P/T] denotes the formula obtained from / by replacing all occurrences of 
the proposition P 6 p by T. Similarly, f[P/l_] refers to the formula obtained from 
/ by replacing all occurrences of P by _L 
If we read Ί ' as true and Ό' as false, we then see that the connector -i represents 
the negation of a propositional formula, i.e. the truth value v(-<f) is the opposite of 
v(f). The connector Л expresses the logical and or logical conjunction: f А д is true 
if, and only if, both formulae / and д are true. The existential quantifier ЭР means 
that the quantified formula (3P)f evaluates to true, if / evaluates to true when P is 

268 
LANGUAGE AND INFORMATION 
either replaced by T or ± in /. In other words, / is true if it is true with P either 
interpreted as true or false. The evaluation of the formulae of the extended language 
follow from these definitions: 
1· v ( / V g) = 1 if v(/) = 1 or \{g) = 1, v ( / Л g) = 0 otherwise; 
2. v ( / —» g) = 1 if either v(/) = 0 or v(g) = 1, v ( / —» g) = 0 otherwise; 
3. v ( / +>g) = l, if v(/) = v(g), v ( / o j ) = 0 otherwise. 
/ V g is interpreted as the logical or or the logical disjunction. The expression / —> д 
is called implication and means that if / is true, then so is д. Finally, / 0 5 expresses 
the equivalence between / and g, i.e. / and g are either both true or both false. 
Example 7.1 We describe the full adder circuit of Figure 2.7from Instance 2.4 in the 
language of propositional logic over p = {Ini,In2, 
Ιη^, Outi,Out2}. 
The XOR 
gates in Figure 2.7 output 1 if, and only if both inputs have different values. This 
can be modeled by the propositional formula f)Lg := (/V ->g) Л (->/ V g). We then 
have v ( / У g) = 1 ifv(f) 
ф v(g). Otherwise, we have v ( / Y g) = 0. Using this 
construction, the full adder circuit is completely described by the following formulae: 
Outi ^ (Im У In2) У In3 
(7.1) 
and 
Out2 о ((/ni У In2) Л In3) V (/ni Л In2). 
(7.2) 
A valuation v satisfies a propositional formula / if v(/) = 1. Then, v is called a 
model of /, and we write v |= /. The set of all valuations satisfying / is denoted by 
Hf) 
= { v : v h / } . 
More generally, r(S) denotes the set of valuations which satisfy all formulae / G S, 
r(S) 
= 
{ v : v H / , V / e S } . 
Conversely, f(v) denotes the set of all wffs which are satisfied by a valuation v, 
r(v) 
= 
{ / : v ( = / } . 
Thus, v is a model of each element of f(v). This notation is again extended to sets of 
valuations: If M is a set of valuations, then f(M) is the set of all sentences satisfied 
by all valuations in M, 
f(M) 
= 
{ / : v h / , V v e M } . 
Let Mp denote the set of all valuations v : p —»· {0,1}. A tuple (CP,MP, \=) con-
sisting of a language £p, a set of models Mp and a satisfying relation |=C £p x M.p 
is called a context. This concept will be generalized in Section 7.3.1. 

PROPOSITIONAL LOGIC 
2 6 9 
A formula / is called satisfiable, if there is at least one valuation v that satisfies /, 
i.e. if f (/) φ 0. Checking the satisfiability of a formula is the fundamental problem 
of propositional logic (see Instance 2.4). Two formulae / and g are called (logically) 
equivalent if they have the same models, i.e. if f (/) = f(g). Note that (ЭР)/ is 
always equivalent to a formula where the proposition P 6 p does not occur anymore. 
So, existential quantification serves to eliminate variables from a formula. Similarly, 
two sets of formulae S\ and S2 are equivalent, if r(Si) — r(S2). We denote by 
(3Ph ...PiJS 
the set of formulae {3Ph ) . . . (3Pin ) / for all / e S. It is equivalent 
to a set of formulae without the variables P^ to P^n. 
Example 7.2 The valuation v(/ni) = v(Outi) = 1 and v(Iri2) = v(/ri3) = 0 
satisfies the propositional formula (7.1) and is therefore called a model of this formula. 
On the other hand, the assignment v(Ouii) = landv(Ini) 
— \{1п2) — v(In^) = 
0 does not satisfy the formula. If S consists of the two formulae (7.1) and (7.2), the 
set of valuations r(S) satisfying both formulae is given in Table 2.8 of Instance 2.4. 
In the next section, we describe how these concepts are used to capture proposi-
tional information and how two dual information algebras are related to propositional 
languages and models. 
7.1.2 
Propositional Information 
The available information concerning propositional variables is usually expressed 
by sets of propositional formulae. More precisely, a set of propositional formulae S 
determines the information f(S) Ç M., and the formulae S say that the unknown 
model, sometimes called a possible world, is a member of r(S). With respect to the 
above adder example, the equations (7.1) and (7.2) specify the possible configura-
tions (Table 2.8) of the circuit variables, if the components of the adder work correctly. 
In order to move towards a valuation or even an information algebra, we consider 
the lattice D of finite subsets of the countable set of propositional symbols p. Any 
subsets q Ç p represents a question, namely the question about the truth values of the 
propositions Pi £ q. The projection v^q of a valuation v : p —» {0,1} is called an 
interpretation of the language Cq. Let Mq be the set of all possible interpretations 
of Cq. This is a finite set of 2^ elements. As mentioned before, the elements of Mq 
can also be considered as Boolean ç-tuples or configurations m : q -» {0,1}. If / is 
a wff from Cq, it contains only propositional symbols from q. For m £ M.q we write 
m \=q f if m = v^q and v |= /. In other words, m is a model of / with respect to 
the variables in q. The values of v on variables outside q clearly do not influence the 
relation \=q. We thus have for all q € D a context cq = (Cq, Л4д, \=q) and we next 
extend the notions of f(/) and f(v) to interpretations or models in Mq and formulae 
in Cq. If S ç Cq is a set of formulae and M Ç M.q a set of models, we have 
fq(S) 
= 
{ m e A V m h , / , V / e S } 
and 
fq(M) 
= 
{ / E £ , : m h / , V m e M } . 

270 
LANGUAGE AND INFORMATION 
fq(S) is called thepropositional information of S with respect to q, and fq{M) is the 
theory of M in q. The information sets, i.e. the subsets of Mq for all q G D, form 
an information algebra. In fact, this formalism corresponds to the relational algebra 
of Instance 1.2 where all relations M Ç Mq are Boolean. Given an information set 
M, we define its label by d(M) = q if M С Λ4ς. Combination is defined by natural 
join. If Mi is labeled with q and M2 with и we have 
MX®M2 
= Mx ix M2 = {me Mquu ■ miq e Mi and miu e M2}. (7.3) 
Finally, projection of an information set M to some domain q Ç d(M) is defined by 
Mu 
= 
{miq : m e M}. 
(7.4) 
The neutral element for the domain q Ç p is Mq, and the null element corresponds 
to the empty subsets of A4q. 
Example 7.3 Let Si and S2 denote the two singleton sets containing the adder for-
mula (7.1) and (7.2) respectively. We identify their model sets Mi = fq(Si) and M2 = 
fq (S2 ) which are information sets with domain d(M\ ) = {Ini, In2, In?,, Outi} and 
d(M2) = 
{Irn,In2,In3,Out2}: 
Ini 
0 
0 
0 
0 
1 
1 
1 
1 
In,2 
0 
0 
1 
1 
0 
0 
1 
1 
In3 
0 
1 
0 
1 
0 
1 
0 
1 
Outi 
0 
1 
1 
0 
1 
0 
0 
1 
Ini 
0 
0 
0 
0 
1 
1 
1 
1 
1П2 
0 
0 
1 
1 
0 
0 
1 
1 
In3 
0 
1 
0 
1 
0 
1 
0 
1 
Out2 
0 
0 
0 
1 
0 
1 
1 
1 
The combination Mi <g> M2 again corresponds to the information in Table 2.8. 
To this algebra of information sets, we may associate a corresponding algebra 
of sentences. For that purpose, we show how the above rules for combination and 
projection can be expressed by sentences. Consider an information Mi — rq(Si) 
described by a set Si of wffs in Cq and an information M2 = fu(S2) 
described by 
S2 С Cu. We observe that 5Ί and £2 may also be seen as formulae of the language 
5Ί U ^2 ç CqUu. Further, let 
M t 9 U u 
= 
{m e Mquu ■■ m ^ e M} 
be the cylindric or vacuous extension of M Ç M.q toq\Ju. Then, if M = fq(S), we 
also have M t ? U u = fqUu(S) 
and therefore 
Mi®M2 
= M}qUunM^Uu= 
fqUu(Si)nrqUu(S2) 
= r, U u(5i U S2). 

PROPOSITIONAL LOGIC 
271 
The last identity follows immediately from the definition of f(S). Thus, combining 
pieces of information means to take the union of their defining sentences. Further, if 
M С Mu is a set of models with M = fu(S) for some set of sentences S ç £u, it 
holds for q С и that 
M^ 
= {m^ : m e fu(S)} 
= f,((3P n ... 
Pln)S), 
where u — q = {Pil... 
Piri}. This shows that projection or extraction of information 
corresponds to the existential quantification of the sentences describing the original 
information. In this way, an algebra of formulae is associated to the algebra of 
models. In fact, to each model set M = fq(S) ofMq its theory fq(M) = 
fq(rq(S)) 
is associated. The operator Cq(S) = fq(fq(S)) 
satisfies the axioms of a closure 
operator (Davey & Priestley, 1990). This will be proved in the general context in 
Lemma 7.2 of Section 7.3 below. Subsets 5 Ç £q with 5 = Cq(S) are therefore 
called closed. Clearly, M = rq(S) implies that M = rq(Cq(S)) 
and therefore 
Cq(S) = Cq(Cq(S)). Thus, theories Cq(S) are always closed sets, and the algebra 
of formulae is in fact an algebra of closed sets of formulae. Combination between 
closed sets Si Ç Cq and S2 Ç Си is defined by 
SX®S2 
= 
C,Uu(Si U S2), 
(7-5) 
and projection of a closed set S Ç Cu to some subset q С и by 
Si4 
= 
Cq((3Pn...Pln)S) 
(7.6) 
where u — q = {P,, ... Pin }. The neutral elements in this algebra are the tautologies 
(7,(0), whereas the null elements equal the whole language Cq. Both are clearly 
closed sets. This algebra of closed sets is closely related to the well-known Linden-
baum algebra of propositional logic (Davey & Priestley, 1990) which is a Boolean 
algebra that covers combination but not projection. Our algebra is a reduct of the Lin-
denbaum algebra as a Boolean algebra, but extended by the operation of projection. 
To conclude this section, we formalize the relations between contexts cq for 
different q e D. If и Ç q, then any wff s e Cu is also a wff in Cq. Formally, we 
define this embedding /U>Q : Cu —» Cq simply by the identity mapping fu,q(s) = s. 
On the other hand, models m ε Mq can be projected to M.u. We therefore define 
the projection mapping gq^u : Mq —» Mu by gq^u(m) = m^u. Then, the pair of 
contravariant mappings /Ui9 and gq<u clearly satisfies the following property: 
m Ης fu,q(s) 
<=> 
59>u(m) \=u s. 
Such a pair of contravariant mappings is called an infomorphism between the contexts 
cq and cu (Barwise & Seligman, 1997). They are considered in a more general and 
abstract setting in Section 7.3 below. 
7.1.3 
Some Computational Aspects 
The formalism of propositional logic induces a dual pair of valuation algebras that 
arises from the language and its interpretation. In the first case, valuations are closed 

272 
LANGUAGE AND INFORMATION 
sets of formulae and their rules of combination and projection are given in equations 
(7.3) and (7.4). In the second case, valuations are sets of models and their operations 
of combination and projection are given in equations (7.5) and (7.6). The proof that the 
valuation algebra axioms are satisfied in both systems will be given in a more general 
context in Section 7.3.2 below. Hence, inference problems with knowledgebases 
from propositional logic can be computed by the local computation architectures of 
Chapters 3 and 4. We now give two important examples of such inference problems. 
■ 7.1 Satisfiability in Propositional Logic 
Let {</>i,..., фп} С Ф be a set of propositional information pieces, either 
represented in the valuation algebra of closed sets of formulae or in the valuation 
algebra of model sets. In any case, this set is interpreted conjunctively, i.e. the 
objective function φ = φγ ® ... ® φη is satisfiable if, and only if, each factor 
φί with i = 1,..., n is satisfiable. A satisfiablity test therefore reduces to a 
single-query inference problem with the empty set as query: 
φΐ* 
= 
( 0 ! ® . . . ® &,)*«. 
Assume that we work on the language level. If we obtain the tautology 
ф1<Ь _ |-|-j t n e n t n e knowledgebase is satisfiable. Otherwise, if φ^ = {_L} 
then the knowledgebase is contradictory. If we work on model level, then 
φ^ = {о} indicates a satisfiable knowledgebase and φ^ = 0 a contradictory 
knowledgebase. 
■ 7.2 Theorem Proving in Propositional Logic 
Let {φι,..., 
φη } С Ф be a set of propositional information pieces and ψ/,ΕΦ 
a hypothesis. Testing whether the hypothesis is true under the given knowledge-
base is equivalent to verifying whether {фг,..., 
φη} U {ф^н} is contradictory. 
This induces an inference problem according to the foregoing instance. Here, 
ф^ь. denotes the negation of the hypothesis. Depending on the knowledgebase, 
this is either expressed in the valuation algebra on language or on model level. 
If some propositional information over the variables q С p is expressed by a set of 
formulae S Ç Cq, then the corresponding valuation on language level is the closed 
set of formulae Cq(S). Closed sets of a set of formulae correspond to their theory 
and are infinite constructs. For practical applications, it is therefore essential to work 
with some finite representation of Cq(S), most appropriately with some normal form 
of S. It is then important that the valuation algebra operations can be expressed with 
respect to the chosen normal form. This requires that executing a combination or 
projections again leads to a corresponding normal form. There are many different 
normal forms that could be used (Darwiche & Marquis, 2001). As an example, we 
use the conjunctive normal form . 

PROPOSITIONAL LOGIC 
2 7 3 
A positive literal P G p is a propositional variable and its negation ->P is called 
a negative literal. A clause φ is a disjunction of either positive or negative literals 
/i for г = 1,..., m, i.e. φ = l\ V ... V lm. Such clauses are called proper if every 
propositional variable appears at most once. A formula is in conjunctive normal form 
(CNF) if it is written as a conjunction of proper clauses, i.e. / = ψ\ Л ... Л ψη 
where ψι are proper clauses for г = 1,..., п. It is known that every formula can 
be transformed into an equivalent CNF (Cfyang & Lee, 1973) and because sets of 
formulae are interpreted conjunctively, we may also transform sets of formulae into 
a CNF. It is common to represent CNFs as clause sets Σ = {ψ\,..., 
φη}, which 
henceforth are used as normal form. Following (Kohlas et ai, 1999) we define the 
combination of two clause sets Σι and Σ2 by 
Σι<8>Σ2 
= 
μ(ΣχυΣ 2). 
(7.7) 
Here, μ denotes the subsumption operator that eliminates non-minimal clauses. This 
means that if all literals of some clause are contained in another clause, then the 
second is a logical consequence of the first and can be removed. We observe that if 
Cq(Ei) = Cq(Si) and (7„(Σ2) = Cu(52), the result of this combination is again a 
clause set whose closure satisfies 
σ βυ„(μ(Σ 1υΣ 2)) 
- 
<7gu«(SiUS2). 
(7.8) 
For the definition of projection it is more suitable to switch to variable elimination. 
We first remark that every clause set Σ, obtained from a set of formulae S Ç Cu, can 
be decomposed into disjoint subsets with respect to a proposition X G и: 
Σχ 
= 
{φ G Σ : φ contains X as positive literal}, 
Σγ 
= 
{ψ G Σ : φ contains X as negative literal}, 
Σχ 
= 
{ψ 6 Σ : φ does not contain X at all}. 
This is possible because Σ contains only proper clauses. Then, the elimination of a 
variable X G и is defined by 
Σ~χ 
= 
μ ( Σ ^ υ β χ ( Σ ) ) , 
(7.9) 
where 
ϋχ(Σ) 
= 
{ΐ?ι V ϋ2 : X V г?! G Σχ and - X V ϋ2 G ΣΎ} 
(7.10) 
is the set of resolvants between clauses in Σχ and Σγ. We again observe that 
the result of this operation is a clause set. Moreover, equation (7.9) corresponds to 
existential quantification with respect to the proposition X G и, such that we have 
0^{Χ}(Σ~Χ) 
- 
C U_ W((3X)S), 
(7.11) 
see (Kohlas et al., 1999). Clause sets are closed under combination (7.7) and variable 
elimination (7.9). Because further the equations (7.8) and (7.11) hold, we conclude 

274 
LANGUAGE AND INFORMATION 
that clause sets form a valuation algebra. A direct proof of this statement can also 
be found in (Haenni et al., 2000). The following example illustrates the computation 
with propositional clause sets. 
Example 7.4 Consider the propositional variables p— {U, V, W, X, Y, Z} and two 
clause sets Σ,χ and Σ2 defined as 
Σι 
= 
{XVY,XV^Y\/Z,ZV-,W} 
Σ2 
= 
{UV-AV\J 
Z,^XVW,X\JV]. 
Combining Σχ and Σ2 gives 
Σι ® Σ2 
= 
μ{Χ V Y, X V -Y V Z, Z V -^W, U V -W V Z, ^X V W, X V V} 
= 
{XVY,XV-YV 
Z,ZV-iW,-iXV\V,XW}. 
Observe that the variable U £ pis contained in Σ 2 but disappears in the combination. 
This seems to contradict the labeling axiom of valuation algebras. However, it does 
not because p — {U} С p implies that £p_{[/} С Ср. This allows us to consider 
Σι (g> Σ2 as a valuation with domain ά{Σ\ (8> Σ2) = p. To eliminate variable X G p 
we partition the clause set as follows: 
( Σ ι ® Σ 2 ) χ 
= 
{XVY,XV^YVZ,XW}, 
(Σ!®Σ 2) Χ 
= 
b l V l f } , 
(Σ!®Σ2)^ 
= 
{ZV-iW}. 
We then obtain 
(Σ!®Σ 2)- χ 
= 
p({Z\J^W}U{YVW,^YVZvW,WW}) 
= 
{ZV^W,YVW,^YVZVW,WW}. 
For practical purposes, the use of the CNF normal form is often unsuitable since 
clause sets may be very large. We therefore prefer a more compact representation as 
for example prime implicates. A proper clause φ is called implicate of a sentence 
7 G С, if 7 |= φ. An implicate ψ of 7 is then a prime implicate if no proper subclause 
of φ is also an implicate of 7. In other words, prime implicates of some sentence 
7 are the logically strongest consequences of 7. The set of all prime implicates of 
7 defines a conjunctive normal form denoted by "£(7). It is therefore possible to 
apply the above rules of combination and projection to sets of prime implicates, if 
we additionally change the μ operator in such a way that the result is again a set of 
prime implicates. We refer to (Haenni et al, 2000) for a discussion of computing 
with prime implicates and other normal forms of propositional logic. 
7.2 
LINEAR EQUATIONS 
The second formalism we are going to consider are systems of linear equations 
over sets of variables. We examine the solution spaces of such systems and introduce 

LINEAR EQUATIONS 
2 7 5 
operations between solution spaces that are also mirrored by corresponding operations 
on the systems of linear equations. This program needs some notational conventions 
and we also remind some basic elements of linear algebra. 
7.2.1 
Equations and Solution Spaces 
We consider an index set r of a set of variables which, in the interest of simplicity, 
is assumed to be finite, although the following discussion could be extended to a 
countable set without much difficulty. For a subset s С r and то € N we define a 
linear system of m equations 
J^atjXj 
= 
bt 
(7.12) 
jes 
for г = 1,..., то. The number of equations m can be smaller, equal or larger than 
the number of variables \s\. Note that such equations are purely syntactic constructs. 
They obtain a meaning, if we specify the coefficients ctij and bi to be real numbers. 
Then, we are looking for real values for the variables Xj that satisfy the above 
equations, when the arithmetic sum and multiplication are used on the left-hand side. 
These assignments again correspond to real s-vectors x : s —> Ш such that 
/ 
j ai,jxj 
= 
"i 
for г = 1,..., то. Any s-vector that satisfies this relation is called a solution to the 
linear system. There may be no solution, exactly one solution or infinitely many 
solutions. We consider the set of solutions of the system (7.12) as the information 
expressed with respect to the variables involved, and refer to it as the solution space 
of system (7.12). 
In order to discuss solution spaces of linear systems in general, we need to remind 
some basic facts of linear algebra. The s-vectors form a linear space where addition 
is defined component-wise, i.e. x + у has components Xj + y4 for г £ s, and scalar 
multiplication с · x of x with a real value с has components с · x*. The linear space of s-
vectors is denoted by Ks and its dimension equals \s\. Similarly, for m € N, we define 
то-vectors by mappings b : {1,..., m} —> К which also form a linear space with 
component-wise addition and scalar multiplication. This linear space of dimension 
то is denoted by Rm. Finally, the m x s matrix A of the linear system (7.12) is a 
mapping {1,..., то} x s - t M with components α^ for г € {1,..., то} and j G s. 
It defines a linear mapping A : Rs —> Mm determined by the matrix-vector product 
x !-»■ Ax e Rm for x 6 Ms. The mapping is linear since A(x + y) = Ax + Ay 
and A(c · x) = с · Ax. Its range is the set of all vectors in IRm which are images of 
s-vectors in Ms, 
ЩА) 
= 
{ z e K r a : 3 x e R s such that Ax = z}. 
The range 7£(A) is a linear subspace of Rm and therefore has a determined dimension 
dim(TZ(A)) called the rank of A. We subsequently write rank(A) for the rank of the 

276 
LANGUAGE AND INFORMATION 
linear mapping A which corresponds to the maximal number of linearly independent 
rows or columns of A. The null space of the mapping A is the set of s-vectors that 
map to the zero vector 0 G Km, 
ЩА) 
= 
{x € Ks : Ax = 0}. 
This is again a linear space, a subspace of Rs with a determined dimension 
dim(N(A)). 
A well-known theorem of linear algebra states that 
\s\ 
= 
rank{A)+dim{N{A)). 
(7.13) 
Thus, a linear mapping is associated to every system of linear equations determined 
by the matrix of the system. The solution space of the linear system can be described 
in terms of this mapping: If y is a solution of (7.12), i.e. Ay = b, then clearly y + xo 
is a solution of (7.12) for any xo G Л/"(Л), and any solution can be written in this 
form. The solution space of (7.12) is therefore 
5(A,b) = {xGffi s:x = y + x0, х 0еЛГ(А)} = y+JV(A). 
The solutions to (7.12) are thus determined by a particular solution у to the linear 
system and the set of solutions to the corresponding homogeneous system AX = 0. 
A set у + £, where у is an s-vector and £ is a linear subspace of W, is called an 
affine space in Rs. Its dimension equals the dimension of С So, the solution spaces 
of linear equations over variables in s are affine spaces in Ks. The following example 
illustrates these concepts by a linearly dependent, under-determined system: 
Example 7.5 For a set of variables {Xi,X2, Хз] consider the linear system: 
Xi 
3Xi 
4ΧΊ 
+ 
+ 
2X2 
5X2 
3X2 
+ 2X3 
3X3 
Хз 
We have \s\ = m — 3 and the associated matrix determining the linear mapping is: 
This equation system is linearly dependent because the third equation corresponds 
to the sum of the two others. We have rank(A) 
= dim(1Z(A)) = 2 and according 
to (7.13) dim(Af(A)) 
= 1. Transforming A into a lower triangular matrix by 
subtracting the triple of the first row from the second row, and by subtracting the sum 
of the first two rows from the third, gives 
A 
= 
1 
- 2 
2 
0 
11 
- 9 
0 
0 
0 

LINEAR EQUATIONS 
2 7 7 
Thus, we may describe the null space of the linear mapping represented by A as 
4 
9 
Λί(Α) 
= 
{{ΧΙ,Χ2,ΧΆ) 
G M3 : χχ = ~Т7хз, 
^ι = ТТ^з}-
A particular solution to the original system is: X\ = 1, X2 = 1 and X3 = 0. We can 
therefore describe the solution space by 
5(A,b) 
= 
+ N{A). 
There are four possible cases to be distinguished at this stage: 
1. rank(A) = m < \s\; 
2. rank(A) < m < \s\; 
3. rank(A) = \s\ < m; 
4. rank(A) < \s\ < m. 
In the first case, the system (7.12) has a solution for any m-vector b, since 7?.(A) = 
Rm, and it follows from (7.13) that dim{M{A)) = \s\ - m > 0. Consequently, the 
solution space has dimension dim(N'(A)). In the second case, the system (7.12) has 
solutions only if b £ 1Z(A). Then, the solution space is an affine space of dimension 
dim(Af(A)) 
> 0. In the third case, we see that dim(Äf(A)) 
= 0 according to (7.13). 
So, if the system (7.12) has a solution, it is unique. The solution space is a point, 
and it has a solution if b € 7£(A). If \s\ — m, then this is automatically the case. 
Finally, in the fourth case, we again have dim(N(A)) 
> 0 and solutions exist, if and 
only if, b € 1Z(A). The solution space again has dimension dim(J\f(A)). Example 
7.5 presents a system in this case. So, solution spaces of systems (7.12) are either 
empty or affine spaces in Rs. It is convenient to consider the empty set also as an 
affine space in Ms. Then, solution spaces are always affine spaces without exception. 
Conversely, we remark that any affine space у + С in Rs is the solution space of 
some linear system over variables in s. This is another well-known result from linear 
algebra, and it clarifies the relation between affine spaces in Ms and systems of linear 
equations over a set s of variables. 
We now look at systems of linear equations and solutions in a way similar to the 
prepositional logic case in the previous section. Consider syntactic constructions like 
:= 
y^ajXj 
with cti and b being real numbers. Such single equations are the sentences or formulae 
of a language £ s defined over the variables Xi with indices from a set s Ç r. If an 
s-vector x is a solution of such an equation e, we write x \=s e which defines a binary 

278 
LANGUAGE AND INFORMATION 
relation |=s in Cs x W. We now consider the structure (Cs, Rs, f=s) and reformulate 
the above discussion about systems of linear equations in terms of this structure: For 
an equation e G £ s let r(e) be the set of all solutions to this equation, 
f(e) 
= 
{x G Ms : x |=s e}. 
If E Ç £ s is a set of equations, then r(E) denotes the set of all solutions to the 
equations in E, 
r(E) 
= 
{ x e R s : x h e , V e e Ê } . 
If E is a finite set, then f(E) is an affine space. Conversely, we may look for all 
equations having a given s-vector as solution, 
r(x) 
= 
{ e € £ s : x K e } · 
Similarly, for a set M С Rs of s-vectors, 
f(M) 
= 
{ e e £ s : x | = s e , V x e M } 
is the set of equations for which all elements of M are solutions. We observe that 
E1ÇE2 
=> f(Ei) D f(E2) 
and 
Mi С M2 =>· f(Mi) D f(M2). 
These concepts allow us to derive and express some well-known facts from linear 
algebra: first, two systems E and E' of linear equations are called equivalent, if they 
have the same solution space, i.e. E =s E' if, and only if, r(E) — r(E'). Then, 
E' Ç E is called a minimal system, if E =s E' and there is no subset E" С Е' 
such that E =s E". For any set of equations E, and thus also for any affine space 
у + £, there clearly exist such minimal systems E'. They are characterized by the 
property that their matrix A' has full rank m, where m = \E'\ < \s\ is the number 
of equations in E'. In fact, if rank(A) 
< m < \s\, then the equations are linearly 
dependent, and we may eliminate m — rank(A) of them to get an equivalent system 
with matrix A'. We then have rank(A') 
= m = \s\ — dim(C) if \s\ < m. We may 
likewise eliminate m — r equations if the system has rank r. Further, if M is a subset 
of an affine space y + С then f{M) 2 f{y + £), hence f(f(M)) 
ç f (y + £). If 
equality holds, then M is said to span the affine space y + £. In any case, 
f(f(M)) 
is itself an affine space spanned by M. It is also clear that y + £ spans itself, so that 
^(^(y + £)) = У + £ m u s t hold. For any affine space y + £ there are minimal sets 
M which span the affine space, and the cardinality of these minimal sets equals the 
dimension of the affine space. 
Structures (Cs, IRS, j=s) of systems of linear equations over sets of variables s С г 
may of course be considered for any non-empty subset s С г. The relations between 
these structures for different subsets will next be considered. Assume t ç s. Then, 
any equation e G £ t with 
e 
:= 
^aiXi 
= b 
iet 

LINEAR EQUATIONS 
279 
may also be seen as an equation in Cs via the embedding 
/t,s(e) 
:= 
Y^diXi 
= b, 
where Oj = 0 if i G s — t. Also, s-vectors x may be projected to the subspace R*. 
This projection is denoted by gs.t and denned component-wise for i G t by 
5t,i(x)(i) 
= 
Xi-
We observe that if the s-vector x is a solution of the equation /t;S (e), then the t-vector 
<?s,t(x) is a solution of the equation e and vice-versa. It therefore holds that 
x К ftAe) 
*£=> fls,t(x) ht e. 
Subsequently, we again write x^' for the projection of an s-vector to t Ç s. Hence, 
the contravariant pair of mappings ft,s : £ s —> t and gStt : Ks -> R* have the same 
property as the corresponding mappings introduced for propositional logic in Section 
7.1. They form again an informorphism. This hints at some common structure behind 
the two systems of propositional logic and linear equations which will be discussed 
in Section 7.3. Here, we continue by showing that affine spaces form an information 
algebra. This yields a foundation for discussing local computation techniques for the 
solution of linear systems in Chapter 9. 
7.2.2 
Algebra of Affine Spaces 
Let r be a finite index set and D its lattice of subsets. We denote by Φ8 the family 
of affine spaces in the linear space Rs where for s = 0, Φ@ = {0} is the only affine 
space. We further define 
Φ = U$» 
sCr 
and introduce within this family of affine spaces the operations of labeling, combi-
nation and projection. Note that the affine spaces in Rs represented by y + £' und 
y' + С are identical, if С = С and у — у' е С 
The labeling operation is simply defined by ά(φ) = s if φ = y + С is an affine 
space in Rs, i.e. £ is a linear subspace of Rs. For the combination of two affine spaces 
in Rs and R', we first extend them to RsUt and compute their intersection. So, let 
us define the extension of an affine space: If ф is an affine space with ά(φ) = s and 
s Ç i , then 
φ^ 
= 
{x G R* : x i s e φ}. 
We show that ф^ь is again an affine space: From ф = у + С and χ·1-8 € φ we conclude 
that χ^ = у + xo for some xo G £. This implies that φ^ — y^1 + Ûb with 
^ 
- 
( o ) 

2 8 0 
LANGUAGE AND INFORMATION 
where 0 denotes the zero vector for the domain t — s and 
£ t ( 
= 
{x0 6 R* : x ^ e £}· 
It is easy to verify that £Tt is a linear space since (xo + хо)^ = xjj + *o 
anc^ 
(c · xo)4·* = с · χ^. Therefore, ^ ' is an affine space in R' which finally allows us to 
define the combination between an affine space ф G Φβ and an affine space φ € Φ4: 
This is again identical to the operation of natural join in the relational algebra: 
φ®φ 
= ф>зф = {x € RsUi : x4·8 € φ and x4"' £ г/;}. 
It remains to verify that ф <g> ф is still an affine space which follows immediately from 
the following argument: Let ф be represented by a system of linear equation E over 
variables in s, and ф by a system i£' over variables in t. Then, the solution space of 
the union EUE' of these two systems is clearly φιχφ 
= </»<Е>^, hence an affine space. 
For the definition of projection let ф = у + С be an affine space in Rs and t C s. 
The projection φ^ is defined as in the relational algebra by 
φϋ 
= 
| x4.t . x e 0}_ 
Since x G 0 implies that x = y + xo for some xo € £, we obtain x 4' = y4·' + XQ 
and therefore 
Clearly, £4-i is a linear subspace of R* and therefore φ^ an affine space in R*. 
Altogether, the algebraic structure (Φ, D) with the operations of labeling, com-
bination and projection is well-defined. Combination has Rs as a neutral element in 
domain s and the empty set as null element. Thus, we see that affine spaces form a 
subalgebra of the relational algebra of s-vectors and therefore inherit the properties 
of an information algebra. In other words, affine spaces provide another instance of 
an information algebra and are therefore amenable to the application of local compu-
tation. However, we generally cannot compute with affine spaces directly, since they 
are infinite sets. Instead, we revert to finite representations of them through systems 
of linear equations. We refer to Chapter 9 for such computational aspects. Finally we 
note that the above discussion also applies to linear equations over arbitrary fields. 
This identifies another generic construction that produces a new information algebra 
for each field. Particularly important among them are Galois fields that have many 
important applications in coding theory. 
7.3 INFORMATION IN CONTEXT 
In the previous two sections, we described the two rather different formalisms of 
propositional logic and systems of linear equations. Different as they are, we have 

INFORMATION IN CONTEXT 
281 
nevertheless worked out some common features. Essentially, information as set of 
models or set of vectors is represented or described by sentences of some formal 
language, and sets of such sentences determine the information described by them. 
In this section, we examine the abstract structure behind both formalisms that leads 
to a further generic construction for obtaining information algebras. 
7.3.1 Contexts: Models and Language 
We introduce an abstract concept called context that represents the connection be-
tween language and information. Here, language refers to a very simple form of a 
formal language such as the language of propositional logic or linear equations. For 
our purposes, it is sufficient to represent a language £ by the set of its sentences 
without regard to the syntactic structure of these sentences. In general terms, infor-
mation concerns a question of interest which formally is represented by the set of 
its possible answers denoted by M. In the example of propositional logic M. is the 
set of interpretations of propositions. In the case of linear equations over variables of 
an index set r, M. is the set of r-vectors. We refer to the elements of Л4 as models 
and further assume a binary relation |=C £ x M.. For a pair (s,m) £ 
£ x M 
we write m |= s if (s, m) G(=. In propositional logic this means that model m sat-
isfies formula s. For linear equations it means that vector m is a solution to equation s. 
A triple (£, Ai, \=) is called a context and serves to express information by 
sentences: If s € £ is a sentence, then 
f(s) 
= 
{m G M : m (= s} 
is the set of models satisfying s, which is thought of being the information described 
by stating s. If S Ç £, 
r(S) 
= 
{ m e M : m | = s , V s e S } 
is the set of models satisfying all sentences of S or the information expressed by S. 
Similarly, if m G M is a model, then 
r(m) 
= 
{s G £ : m (= s} 
is the set of sentences satisfied by m. And if M Ç M. is a set of models, then 
r(M) 
= 
{s G £ : m \= s, Vm G M} 
is the set of sentences satisfied by all elements of M. It can be considered as the 
theory of M, i.e. the set of all sentences which express M. 
It is important to note the formal duality between the concepts derived from £ 
and M.. This is emphasized by the following lemma which collects some elementary 
properties of these mappings between the power sets of M. and £: 

282 
LANGUAGE AND INFORMATION 
Lemma 7.1 Let f and f be the above operators for a context (£, Л4, \=). Then if 
S,Sj С £ and M, Mj Ç A4, the following dual pairs of properties hold: 
1. 
SÇ f(f(S)) 
M Ç f(f(M)) 
2. 
S1ÇS2^f(S1)Df(S2) 
Mi CM2=>f(Mi) 
Df(M2) 
3. 
f(S) = f(f(f(S))) 
f(M) = 
f(r(f(M))) 
4. 
m3 
S^ = П, r(Sj) 
гГД- Ма) = П, HMi). 
Proof: We prove the properties for the language part. The corresponding results for 
the model part follow by duality. 
1. Let s G S. Then, m G f(S) means that m \= s for all s e S. Consequently, s 
belongs to f(m) for all m G r(S), which means that s G f(f(S)). 
2. If m G г(5г) we have m |= s for all s G 5г, and therefore m |= s for all 
s G Si Ç ί>2. This implies that m G r(Si). 
3. From Property 1 and 2 it follows that f(5) Э f(f(f(5))). But on the other 
hand, we conclude from the model part of Property 1 that f(S) Ç 
f(f(f(S))). 
4. It follows from S, Ç (Jj 5 j a n d Property 2 that f(Sj) 
2 r((Jj Sj), hence 
C\j r(Sj) 2 ^(Uj Sj)- Conversely, m G f]j r(Sj) means that m f= s for all 
s G Sj and all j . Therefore m G f((Jj Sj). 
m 
We next introduce the operators 
C h(5) = r(f(5)) 
and 
С И(М) = 
f(f(M)). 
We remark that s G С^(5) means that m |= s for all m which are models of S. In 
logic, such a relation is called a logical consequence, and it is said that s is a logical 
consequence of S, written as S \= s. So, C|= (S) is the set of all logical consequences 
of S. We therefore call (7μ a consequence operator. Similar relations hold for 
the operator C^ on the model side. The following dual properties of consequence 
operators follow immediately from Lemma 7.1: 
Lemma 7.2 For the consequence operators (7μ and C^ in (С, Л4, \=), and S Ç C 
and M С A4 the following properties hold: 
1. 
SCC N(S) 
MCM^(M) 
2. 
Si Ç S 2 ^ C N ( S i ) CC*N(S2) 
Mi Ç M2 =)■ CN(Mi) С C^(M2) 
3. 
CN(CH(5)) = CN(5) 
CN(Ch(M)) = C^(M). 
Operators satisfying these three conditions are called closure operators and play an 
important role in the foundations of logic and in topology (Wojcicki, 1988; Norman 
& Pollard, 1996). We already came across closure operators in the context of Kleene 
valuation algebras in Chapter 6. Sets S Ç £ and M Ç A4 such that C^(S) 
= S 

INFORMATION IN CONTEXT 
283 
and C^(M) 
— M are called closed. Closed sets of sentences and models can be 
understood in the following sense: if a set S of sentences is stated, then f(S) is 
by Property 3 of Lemma 7.1 a closed set of models representing the information 
expressed by S. We therefore call a closed sets of models an information set or 
shortly, an information. In particular, f(S) is the information expressed by S. The 
closure C|=(S) of S is the set of all logical consequences of S called its theory. 
Similarly, if M is an arbitrary set of models, f(M) is a closed set of sentences called 
the theory of M. Any set M of models determines an information set C^(M). 
Note 
that f is a map r : V(C) —> V(M) and f is a contravariant map f : V(M) —> V{C) 
to f. This pair of contravariant mappings satisfies the property that 
M Ç f(S) 
<=ï 
S Ç f{M). 
(7.14) 
This follows from Property 1 and 2 of Lemma 7.1. A contravariant pair of mappings 
satisfying (7.14) is an instance of a Galois connection. We mention that such struc-
tures are also used informal concept analysis (Davey & Priestley, 1990). 
It is time to illustrate and to link these abstract concepts with the examples of the 
previous two sections, as well as with some additional examples. In Section 7.3.2 it 
will then be shown that these formalisms satisfy the axioms of an information algebra 
under some additional assumptions. They are therefore new instances of our generic 
framework. 
■ 7.3 Propositional Logic 
Section 7.1 introduced the language and interpretation of propositional logic. 
Models are valuations v of propositional symbols and serve to interpret propo-
sitional formulae. More precisely, v \= s holds if the formula s evaluates to true 
under the valuation v, i.e. if v(s) = 1. If we restrict ourselves to finite sets q of 
propositional symbols, then all sets of models are closed. The closure operators 
Cq correspond to the consequence operators C|= in the abstract setting above. 
■ 7.4 Linear Equation Systems 
Another example is provided by systems of linear equations discussed in Sec-
tion 7.2. The language is formed by linear equations e over variables from an 
index set s with coefficients in a field, for example in the field of real or rational 
numbers. Models x are s-tuples with values in the field. The relation x |= e 
means that the s-tuple x is a solution to the equation e. In case of real numbers, 
the closed sets of models are exactly the affine spaces in Es. Note that an 
affine space of dimension к is spanned by к linearly independent vectors. The 
affine space spanned by a set of vectors M is the closure of M. The theories 
C^ (S) are formed by the totality of equations sharing the same affine space as 
solutions with the set S. 

284 
LANGUAGE AND INFORMATION 
■ 7.5 Linear Inequality Systems 
Instead of linear equations, we may also consider linear inequalities 
e := 22 aiXi ^ ao 
over variables with indices from a set s and real-valued coefficients сц and ao. 
Such formulae are the elements of the language, and models are again «-vectors 
x. The relation x |= e holds if x satisfies the inequality e, i.e. if 
^ α , Χ ί 
< 
O0. 
Closed sets of models are convex polyhedra in the vector space Rs and, as 
we will see below, form an information valuation algebra. Systems of linear 
inequalities are especially used in linear programming (Chvâtal, 1983; Winston 
& Venkataramanan, 2002). 
■ 7.6 Predicate Logic 
A further example in the domain of logic is provided by predicate logic. The 
vocabulary of predicate logic consists of a countable set of variables X\, X-i, · · · 
and a countable set of predicate symbols Ρχ, P2,... 
and further includes the 
logical constants T and -L and the connectors Λ, -■, 3. Each predicate symbol 
Pi has a definite rank pi, and a predicate with rank p is referred to as a p-place 
predicate. Formulae of predicate logic are built according to the following rules: 
1. PiX^ ... Xip, where p is the rank of Pi, _L and T are (atomic) formulae. 
2. If / is a formula, then ->/ and (3Xi)f are formulae. 
3. If / and д are formulae, then / Л д is a formula. 
The predicate language С consists of all formulae which are obtained by ap-
plying these rules a finite number of times. We consider also the predicate 
language Cs where only variables from a subset s of variables are allowed. 
Often, s is restricted to be a finite set. 
In order to define an interpretation for formulae of predicate logic, we choose 
a relational structure TZ = (U, Ri, R2, ■ ■ ·) where U is a nonempty set called 
universe, and the Ri are relations among elements of U with arity pi equal to 
the rank of predicate Pj. In other words, Ri are subsets of UPi. A valuation 
is a mapping v : {1,2,...} -» U which assigns to each variable Xi a value 
v(i) 6 U. We write С/ш for the set of all possible valuations. Given a valuation 
v and an index i, we define the set of all possible valuations that agree with v 
on all values except v(i), 
v=" 
= 
{ u e t T : u ( j ) = v ( j ) for 
jjii}. 

INFORMATION IN CONTEXT 
2 8 5 
Valuations v are used to assign a truth value v(/) e {0,1} to formulae / € С 
This assignment is defined inductively as follows: 
1. v(T) = landv(_L) = 0; 
2. ЦРгХ^ . ..Xip) 
= 1, if (v(ii),..., v(v)) € Ri and 
v ^ X i , ... Xip) = 0 otherwise; 
3. v ( i / ) = 1 if v(/) = 0 and v(-./) = 0 otherwise; 
4. v((3Xj)/) = 1 if there is a valuation u e ν^* such that û(/) = 1 and 
v((BXi)f) 
= 0 otherwise; 
5. v ( / Л g) = 1 if v(/) = v(g) = 1 and v ( / Л g) = 0 otherwise. 
A valuation v is called a model of a formula / in the structure Έ, if v(/) = 1. 
We then write v |= /. This defines the relation (= between formulae of С and 
valuations in ΙΙω. Further, the relation can be restricted to languages Cs and 
s-tuples m in Us which are simply projections of valuations v to subsets s. In 
this case, we write m \=s f. Information sets rs(S) then are subsets of Us, i.e. 
relations in s. They form the relational algebra over subsets s which exhibits 
the close and well-known relation of predicate logic to relational algebra and 
hence to relational databases. We refer to (Langel, 2010) for more details about 
predicate logic and information. 
Besides propositional and predicate logic, many other logic systems possess this 
context structure. We refer to (Wilson & Mengin, 2001) for further instances from 
the field of logic. 
Two sets of sentences Si and S2 are called equivalent, if they have the same 
models, f(S\) = г(5г). This means that S\ and S2 express the same information 
and it is equivalent to saying that they have the same theories C^(Si) = С ^ ^ г ) . 
Any set S of sentences is in particular equivalent to its theory C|=(5). In the same 
way, two sets of models Mi and M^ are equivalent, if they have the same theory 
r(Mi) = f (M2). They span the same information (see for example Instance 7.4). 
In a first step towards a valuation algebra, we may already discuss a preliminary 
concept of combination of information. In fact, suppose two sources which both 
send a piece of information by stating sets S\ and S2 of sentences. They express 
the two information sets Μχ = f(Si) and Mi = r(S?). On the level of sentences, 
combining these two pieces of information means clearly putting the two sets of 
sentences together as Si U S?.. Think for example of two systems of linear equations. 
On the model level, the combined information is then given by the models of Si U S2. 
Hence, we may tentatively define the following combination operation between two 
information sets: 
M i ® M 2 
= 
f(SiUS 2). 

286 
LANGUAGE AND INFORMATION 
Using Property 4 of Lemma 7.1 we may also write this operation as: 
MX®M2 
= f(5i) Π r(S2) = М1ПМ2. 
Combining two information sets simply corresponds to set intersection. It thus follows 
that the intersection of two closed sets is again a closed set. In fact, since Property 
4 of Lemma 7.1 holds for any family of sets Sj, we conclude that the intersection 
of any family of closed sets is again a closed set. From this, it follows by a standard 
results of lattice theory that closed sets form a complete lattice (see Definition A.5) 
(Davey & Priestley, 1990). This is a fundamental result of context analysis. We are 
pursuing however a different direction. In the following section we shall construct an 
information algebra out of context systems. 
7.3.2 Tuple Model Structures 
In all examples we have seen so far, we dealt with subsets s of variables, and the 
models were valuations of these variables in some sets, for example real numbers 
as in linear systems of equations or Boolean values as in propositional logic. For 
every subset of variables we have a context c5 linking the sentences or formulae 
with the corresponding values, for example as solutions to sets of linear equations or 
interpretations satisfying propositional formulae. These contexts are linked together 
by projection of models and embeddings of sentences. In this section, we subsume 
this situation into an abstract frame which induces a generic construction producing 
these examples as particular instances. In a first step, we abstract the structure of 
s-vectors and interpretations (Boolean vectors) into a general abstract frame called 
tuple system. In fact s-vectors and s-interpretations for subsets s of a set r of variables 
are instances of a tuple system defined as follows: 
Definition 7.1 Л tuple system over the lattice D of subsets s Ç r is a set T together 
with two operations d : T —» D and J.: T x D —> T defined for x Ç d(f) which 
satisfies the following axioms: For f,g G T and x,y € D, 
1. IfxCd{f)thend(fix) 
= x. 
2. IfxÇyÇ 
d(f) then {fiy)lx 
= 
fix. 
3. Ifd(f) = x then f±x = /. 
4. For d(f) = x, d(g) = y and /±ХПУ = gi-xr,y there exists h 6 T such that 
d{h) =xUy,hix 
= f and hiy = g. 
5. For d(f) — x and x Ç y there exists g G Г such that d{g) = у and g^x = f. 
The operation J. of course corresponds to the projection of s-vectors, and the label 
d indicates to which group of variables a vector or tuple belongs. Vectors and ordinary 
tuples as occurring in relational databases are evidently instances of abstract tuples 
according to the definition above. 

INFORMATION IN CONTEXT 
2 8 7 
Example 7.6 The relational algebra of Instance 1.2 can be generalized to abstract 
tuple systems. Then, a relation R over s G D is a set of tuples f G T which all have 
the domain s. This also defines the label of R as d(R) = s. Combination of two 
relations R\ and i?2 with domains s and t respectively is defined by natural join, 
R1[xiR2 
= 
{feT:d(f) 
= 
sUtandflseRiandfueR2}, 
and projection of relation R with domain s to t Ç s is defined as 
RU 
= 
yit 
. j e 
щ 
Verifying that this generalized relational algebra also satisfies the valuation algebra 
axioms is similar to the ordinary relational algebra. 
The algebra of models in Section 7.1 corresponds to such a generalized relational 
algebra over Boolean tuples. The algebra of affine spaces in Section 7.2 is a subalgebra 
of the relational algebra over s-vectors, and the same holds for the algebra of convex 
polyhedra in Instance 7.5. Here follows a more unusual example. 
Example 7.7 Let (Ф, D) be an idempotent valuation algebra, i.e. for ф G Ф and 
x Ç ά(φ) it holds that 
φ®φ±χ = φ. 
We further assume a neutral element ex for each domain x G D, and that the 
valuation algebra is stable, i.e. that e^y = ey for y Ç x. Then Φ is a tuple system: 
The properties 1, 2 and 3 of a tuple system simply correspond to the Axioms (A3), (A4) 
and (A6) of the valuation algebra. To verify Property 4, assume ά(φ) = x, ά{φ) = y 
and ф^хПУ = ф±хПУ. Defining χ = φ <g> ψ, we obtain from the combination axiom 
(A5) and idempotency that 
χί* = (φ®ψγχ 
= ф®ф±хПУ = ф®ф±хПУ = ф. 
In a similar way, we derive χ^υ = φ. For Property 5, let ά(φ) = x and χ = φ <S> ey. 
It follows that d(\) = у and by the combination axiom and the property assumed for 
neutral elements that 
x±x = (ф®еу)1х 
= φ®β^χ 
= ф®ех 
= ф. 
This shows that each idempotent valuation algebra with neutral elements adopts itself 
the structure of a tuple system. 
As in the above examples, we now link tuples systems with formal languages 
to form a family of contexts. For that purpose, remember that the contexts cx = 
(Cx, Mx, \=x) for all x ç r in the examples are linked together by embedding and 
projection mappings. Thus, let M be a tuple system over the subsets of a set r. We 
define the set of all tuples with domain x by 
Mx 
= 
{m G M : d(m) = x}. 

288 
LANGUAGE AND INFORMATION 
Further, we assume the existence of language Cx associated with Mx to express 
information in Mx. More precisely, we suppose a context cx = (Cx, Mx, \=x) for 
each subset x Ç r, and for y Ç χ an embedding fy<x : Cy —¥ Cx such that it 
forms together with the projection gx<y : Mx —> M.y defined by gXty{m) = m^y an 
infomorphism. We thus have 
m \=x fy,x(s) 
<=> 9x,y(m) t=y s-
For a set of sentences S of the language Cx we have the information set M = rx(S) 
in Mx which is a |=x-closed set M = C^ 1 (M) = fx(fx{M)). 
Let Φχ be the set of 
all |=x-closed sets in Mx and 
We examine whether Φ adopts the structure of an information algebra relative to the 
lattice of subsets of r. In fact, since the elements of Φ are generalized relations, it is 
sufficient to show that it is a subalgebra of the generalized relational algebra associ-
ated with the tuple system M. (see Example 7.6), and this requires that Φ is closed 
under join and projection. Unfortunately, it follows from the formal duality between 
languages and models that Φ is not necessarily closed under projection. This can, for 
example, be seen by the example of propositional logic: the embedding fy,x(S) 
of a 
theory in the context cy is not yet closed in the context cx. Dually, we may generally 
not expect that the projection дХгУ(М) of a closed set in the context cx is still closed 
in cy. However, this is the case in many examples and in particular in all examples we 
have seen so far. Therefore we tacitly assume or require that дхл(М) 
is ^-closed 
when M is )=x-closed. 
The situation is different for combination: Let M\ e Фх, M2 G Фу and recall that 
Μλ^Μ2 
= M\xUynMlxUy, 
(7.15) 
where M^y for M С M.x and x Ç y is defined as: 
Mtj/ 
= {теМу: mix e M}. 
We claim that m^y is |=y-closed if M is f=x-closed. In fact, assume that M = rx{S) 
for some set of sentences S С £х. We then have the following equivalences due to 
the infomorphism property: 
m e Miy 
& mix S M = fx(S) 
&■ 9y,x(m) \=χ s,ys e S 
Ф> m \=y /x>y(s),Vs G S 
<£> m& 
ry{fx,y(S)). 
This shows that M?y = ry(fXiy(S)) 
which is a f=j,-closed set. Therefore, both 
Mj Uy and M\XUy 
in equation (7.15) are f=xUy-closed sets, and since the intersec-
tion of closed sets is still closed, we have M\ ixi M2 G Ф. 

CONCLUSION 
289 
To sum it up, Φ is a subset of the generalized relational algebra from which we 
already know that it satisfies the information algebra axioms. It is furthermore closed 
under combination and projection which turns the structure (Φ, D) into a subalgebra 
of the generalized relational algebra. Therefore, (Φ, D) also forms an information 
algebra. This holds for all families of contexts where the corresponding operations 
of embedding and projection are linked by an infomorphism, given the additional 
condition that the projection of closed model sets always leads to a closed model set. 
The algebra contains neutral elements Mx for all contexts x, corresponding to the 
tautologies (7μχ (0) in context x. This generic construction called context valuation 
algebra produces among other formalisms the information algebras of propositional 
and predicate logic, and of linear equation and inequality systems. This is shown by 
the following examples: 
Example 7.8 In the case of propositional logic (see Section 7.1 and Instance 7.3) the 
tuple system M. consists of all Boolean s-vectors m : s —l· {0,1}, and Φ is equal to 
the relational algebra over this tuple system, ifr is finite. This is because any subset 
of Λ4Χ is \=x-closed. The same situation is found in the predicate logic of Instance 
7.6 where the models also form a relational algebra. 
Example 7.9 In the context of linear equations (see Section 7.2 and Instance 7.4) 
the tuple system ΛΊ consists of all s-vectors m : s —> К. Here, Ф is the subalgebra 
of affine spaces of the relational algebra of s-vectors. As we have seen, affine spaces 
project to affine spaces again. 
Example 7.10 Similar as in systems of linear equations, the tuple system M. of linear 
inequalities (see Instance 7.5) consists of all s-vectors m : s —>· E. But here, Ф is 
the subalgebra of convex polyhedra in the linear space M.s. Again, convex polyhedra 
always project to convex polyhedra. 
7.4 
CONCLUSION 
We started into this chapter by considering the two formalisms of propositional 
logic and linear equations. Both examples provide a formal language to describe 
their information sets. In the first case, propositional formulae describe sets of truth 
value assignments to propositional symbols under which the corresponding formula 
evaluates to true, and in the second case, linear equations describe their associated sets 
of solutions. This interaction of sentences and model sets uncovers two information 
algebras for each formalism. On the model level, both formalism correspond to (a 
subalgebra of) the relational algebra of Instance 1.2 and therefore adopt the structure 
of a information algebra. These properties mirror the context structure. Closed sets of 
sentences therefore also form an information algebra. Although our considerations for 
linear equations were limited to real numbers, we noted its applicability for arbitrary 
fields. This identified a first generic construction which actually produces two new 
information algebras for each field. However, the final section of this chapter showed 
that propositional logic and linear equations over arbitrary fields are instances of 

290 
LANGUAGE AND INFORMATION 
a far more general generic construction called context valuation algebra. Based on 
so-called tuple systems, this further induces the information algebra behind linear 
inequalities, predicate logic, and many other formalisms where this duality between 
language and models exists. 
PROBLEM SETS AND EXERCISES 
G.l * Express consequence finding in propositional logic as an inference problem. 
This solution to this exercise can be found in (Inoue, 1992). 
G.2 * Exercise D.4 in Chapter 4 introduced a partial order between the elements 
of an idempotent valuation algebra. Apply this order to propositional logic, linear 
equation systems and to contexts in general and explore its semantics. 
G.3 * * In this chapter, we discussed propositional and predicate logic as instances 
of context valuation algebras. Other non-classical logics are studied in (Wilson & 
Mengin, 2001). Identify the context structure in these logics. 
G.4 * * * Context valuations are infinite constructs but with a finite representation. 
For propositional logic such representations could be so-called normal forms. Ex-
emplarily, we studied the conjunctive normal form and the normal form of prime 
implicates in Section 7.1.3. But there are many other normal forms listed in (Dar-
wiche & Marquis, 2002; Wächter & Haenni, 2006) that could be used as well. Analyse 
further normal forms and provide suitable definitions for combination and projection. 

PART III 
APPLICATIONS 
Generic Inference: A Unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc.

CHAPTER 8 
DYNAMIC PROGRAMMING 
Many valuation algebras have an associated notion of a solution, best or preferred 
value with respect to some criteria. Most typical are solutions to linear equations or 
inequalities. In logic, solutions may correspond to the truth value assignments under 
which a sentence evaluates to true. Likewise, the solutions of a constraint system are 
the variable assignments that satisfy the constraints and in optimization problems, 
solutions lead to either maximum or minimum values. Given a set r or variables, we 
again write Ωχ for the frame of each variable X £r and Ω3 for the set of all possible 
tuples or configurations of a subset s Çr, see Section 1.2. If (Φ, D) denotes a valua-
tion algebra defined over the subset lattice D = V(r), a solution for some valuation 
φ G Φ with domain d(<j>) = s always corresponds to some element xGÎÎ s. We know 
from Instance 1.2 that such tuple systems form a relational algebra. It is therefore rea-
sonable to consider the solutions of φ as a specific relation Сф С Qs. In this chapter, 
we focus on the computation of the solutions for valuations ф that are given as factor-
izations ф = фу®.. .®φη. This must again be done without the explicit computation 
of φ. Moreover, it is a general observation that determining a solution to an equation 
system becomes easier the less variables are involved. In standard Gaussian elimina-
tion for regular systems, we therefore eliminate one variable after the other until a 
single equation with only one variable remains. The solution to this simple equation 
Generic Inference: A unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc. 
293 

294 
DYNAMIC PROGRAMMING 
can then be determined and the complete solution to the total system is obtained 
from a backward substitution process. The more general procedure presented in this 
chapter follows exactly this course of action. We execute an arbitrary local computa-
tion architecture to obtain the marginal φ^ for some t Ç ά(φ). This corresponds to 
the elimination of the variables in s — t. We then observe that every solution to this 
marginal is also a partial solution to φ with respect to the variables in t. In a second 
step, we extend this partial solution to a complete solution for the valuation φ using 
only the intermediate results obtained from the local computation process. This en-
sures that solution construction adopts the complexity of a local computation scheme. 
The first section of this chapter gives a more rigorous definition of solutions in 
the context of valuation algebras and derives some basic properties. We then present 
different algorithms for the construction of single solutions or complete solution sets 
in Section 8.2. These methods are again generic and can be applied to all valuation 
algebras with a suitable notion of solution. A particular important field of application 
is constraint reasoning or the solution of optimization problems. It will be shown in 
Section 8.3.2 that these problems emerge from a particular subclass of the family of 
semiring valuation algebras from Chapter 5. In fact, executing the fusion or bucket 
elimination algorithm for the computation of a marginal of an optimization problem 
and applying the generic solution construction procedure presented in this chapter 
coincides with a well-known programming paradigm called dynamic programming 
(Bertele & Brioschi, 1972). This close relationship between valuation algebras and 
dynamic programming was established by (Shenoy, 1996), who further proved that 
the axioms for discrete dynamic programming given in (Mitten, 1964) entail those 
of the valuation algebra. Hence, the axiomatic system of a valuation algebra that 
enables local computation also constitutes the mathematical foundation to dynamic 
programming. Essentially the same idea for the solution of constraint systems based 
on bucket elimination was also found by (Dechter, 1999). This chapter can therefore 
be seen as a generalization of both approaches from constraint systems to arbitrary 
valuation algebras with solutions. Further applications of solution construction to 
symmetric, positive definite equation systems and quasi-regular semiring fixpoint 
equation systems will be discussed in Chapter 9. 
8.1 SOLUTIONS AND SOLUTION EXTENSIONS 
Let г be a set of variables and (Ф, D) a valuation algebra defined over the subset 
lattice D — V(r). Intuitively, solutions in valuation algebras are defined by the 
fundamental property that a partial solution x G Ω( to φ with respect to some 
variables in t Ç s = ά(φ) can always be extended to a solution (x, y) G Ω8 to 
φ. Moreover, this property of extensibility must hold for all configurations, i.e. if 
x G Ωί? it must be possible to find an extension y G Qs-t such that (x, y) G Ω5 
leads to the "preferred" value of φ among all configurations z G Ω8 with z^4 = x. 
It is required that this configuration extension y can either be computed directly by 

SOLUTIONS AND SOLUTION EXTENSIONS 
2 9 5 
extending x to the domain s, or step-wise by first extending x to и and then to s for 
t С и С s. This is the basic idea of the following definition: 
Definition 8.1 Let ф е Ф with t Ç s = ά(φ) and x e ΩΕ. A set 
Wfr) 
ç Vis_t 
is called configuration extension set of φ from t to s, given x, if the following property 
holds for all и £ D such that t С и Ç s: 
\¥г
ф(х) 
= 
{ Z e f i s _ É : z ^ - t G ^ „ ( x ) a n J Z
i s - u e ^ ( x , z J ' u - t ) } . 
Intuitively, the set W|(x) is assumed to contain all extensions of the configuration 
x £ fit to the domain of ф and is characterized by the fundamental property that con-
figuration extensions can be computed step-wise. Instead of searching an extension 
of a configuration x e Ω( to some preferred value in φ, we can first extend it to ф^и 
and then to ф. Observe that configuration extension sets may also be empty. Further, 
we will see in Section 8.4 that we may define solution extension sets in different ways 
for the same valuation algebra. They are therefore not unique. In the introduction 
we presented the conception of solutions as the set of configurations that lead to the 
preferred value in ф. This can now be expressed as the configuration extension set of 
the empty configuration to ф. 
Definition 8.2 A solution set Сф of a valuation ф £ Ф is defined as 
сФ = 
Wl(o). 
An important relationship between solution sets and configuration extension sets 
follows directly by applying Definition 8.1 to t = 0 and x — o: 
сФ = Wj(o) = 
{ z e O s : z ^ € H ^ , > ) a n d z i s - u e W 7 ( z ^ ) } 
= 
{z € Us : ziu € сф^ and zls~u 
£ W${ziu)}. 
(8.1) 
This shows how solution sets are computed step-wise. The following lemma states 
an important property of solution sets, saying that every solution to a projection of ф 
is also a projection of some solution to ф and vice versa. 
Lemma 8.1 For ф € Ф and t Ç ά(φ) it holds that 
_ 
и 
Сфи 
—. сф . 
Proof: Let ά(φ) = s. It then follows from equation (8.1) that 
it 
[Wj(o) 
= 
{ z £ i ! s : zil e сфи and z^~l £ W^z4·')} ' = сфи 

296 
DYNAMIC PROGRAMMING 
Subsequently, we refer to a projection x^' of a solution x g c ^ a s a partial solution 
of φ with respect to t Ç ά(φ). If s and ί are two arbitrary subsets of the domain of 
a valuation ф G Φ, the partial solutions of </> with respect t o s U i may be obtained 
by extending the partial solutions of φ with respect to s from silt tot. This is the 
statement of the following theorem: 
Theorem 8.1 For s,t С d(0) we have 
4sUt 
= 
j z E Ωβυί : z+s G 4 s am/ z ^ " s G W # V ' n t ) } . 
Proof: We first show that if z G с? then 
W£. u t(z) 
С W # V e n ' ) · 
(8.2) 
Applying equation (8.1) and Lemma 8.1 gives 
cj e u t 
= 
{z G nsUt : z ^ G с1/ and z ^ " s G W^.u«^*)} 
(8.3) 
and 
c£ 
= 
{z G Ω, : z+sni G # η ί and ζ^" β G W # V ' n t ) } . 
If z G 4 s and y G W'i.u, (z) we have (y, z) G cJeUt and therefore (y, z^ n t) G c%. 
Since z G c^ implies that z i s n t G c^sni we conclude that y G И ^ * ^ ™) . This 
proves (8.2). Next, we obtain by applying two times equation (8.1) and Lemma 8.1: 
clsUt 
= 
j z E OsUt : z+snt G 4 s n t and z^~s G W # V n t ) and 
z-'-'-'eW^.u.iz4·*)}. 
(8.4) 
Similarly, we also have 
4 s u t 
= 
{z G Ω,υΕ : z ^ n t G # n t and z ^ " ' G W ^ . V ™) and 
z^GH^ u t(z^)}. 
It follows from these two expressions that 
4 s u t 
= 
{z G nsut : z+snt G c\snt and z*-« G W^(z^nt) 
and 
z4t-e e w»iJUt(z4.») and z 4 · - ' G W^utiz4-*)}. 
Since z G ст"-" implies that z^s G ci" we can add this as a further condition to the 
last expression. But zis G cv* in turn implies that z^snt G c^nt. Hence, the added 

COMPUTING SOLUTIONS 
2 9 7 
condition zis G cv subsumes z^snt G cfnt and we obtain 
c\sut 
= 
j z € Ωβυί : z+s G 4 s and z ^ " ' G W*™(z^nt) and 
z i i - s e w-aijut(z4.-) a n d zis-t 
e 
w ^ , ^ ) } . 
It then follows from property (8.2) that 
cl;ut 
= 
{z G nsut 
: z i s G c^s and z^" 5 G Η ^ υ ( ( ζ ^ ) and 
z4.-t e W^ u t( zli)J. 
(8-5) 
Finally, we conclude by comparing (8.4) with (8.5) that if z^s G c^s that 
w;uut(zis) = w#ivni) 
must hold. Replacing the expression in (8.3) finally gives: 
4 s U t 
= 
{z G OsUt : z+s G с1; and z ^ s G ^ ™( z ^ n t ) } . 
■ 
8.2 COMPUTING SOLUTIONS 
We now focus on the efficient computation of solutions for some valuation φ G Φ 
that is given as a factorization φ = φ\ (g> ... ® φη. It was pointed out in many 
places that every reasonable approach for this task must dispense with the explicit 
computation of φ. Since local computation is a suitable way to avoid building the 
objective function, it is sensible to embark on a similar strategy. This section presents 
a class of methods that assemble solutions for φ using partial solution extension sets 
with respect to the previously computed marginals in a join tree. This mirrors the 
fundamental assumption that computing solutions becomes easier, if less variables 
are involved. It is important to note that we do not say in this section how solutions 
or configuration extension sets are computed for concrete valuation algebras, but 
only how they are assembled from smaller sets. Concrete case studies for different 
valuation algebras will later be given in Section 8.3.2 and Chapter 9. 
8.2.1 Computing all Solutions with Distribute 
The most obvious approach to identify all solution configurations of φ follows from 
the application of local computation for the solution of multi-query inference prob-
lems. Depending on the architecture, each node i G V in the covering join tree 
(V, E, X, D) can compute or already contains the marginal of φ relative to its node 
label X(i) after the complete message-passing. We remind that join tree nodes are 

2 9 8 
DYNAMIC PROGRAMMING 
numbered in such a way that if j G V is a node on the path from node г to the root 
node r = \V\, then j > i. From the computed marginals, we can build the set of 
all solutions by the following procedure: due to Lemma 8.1 and Definition 8.2, we 
obtain for the root node: 
4λ(Γ) 
= 
И^л(г,(о). 
(8.6) 
Since X(r) is generally much smaller than ά(φ), we here assume that c, 
can be 
computed efficiently. Then, the following lemma shows how this partial solution set 
can be extended to the complete solution set Сф. 
Lemma 8.2 For г = r — 1,..., 1 and s = X(r) U ... U X(i + 1) we have 
cj-uA(i) 
= 
{zGnsu4.):zlseclsand 
zi\(i)-s 
g W4i)n\(ch(i)) 
,iX(i)nX(ch(i))}\ 
Proof: It follows from Theorem 8.1 that 
с^иЛ(г) 
= 
{z G Ω8υλ(<) : ζ^ G 4 s and ζ ^ > - G ^ ? W ( ^ n A ( i ) ) } · 
From the running intersection property we conclude 
sr\X{i) 
= (x(r)U...UX(i 
+ l)\nX(i) 
= 
X(i)nX(ch(i)). 
Then, the statement of the lemma follows directly. 
■ 
To sum it up, we compute Сф by the following procedure. 
1. Execute a multi-query local computation procedure on 
{φι,...,</>„}. 
2. Identify c\ v 4n the root node. 
3. For г = r — 1,..., 1 
(a) compute W}l^{i)
 
(c 
" in node г GV; 
(b) build Сф ( r ) u 
u (г) by application of Lemma 8.2. 
4. Return сф = 
с1
ф
х{г)и-ит. 
The pseudo-code of this procedure is given in Algorithm 8.1. It is important to 
note that the domains of the configuration extension sets computed in step 3 are 
always bounded by the domain X(i) of the corresponding join tree node. Hence, 
although the construction of Сф naturally depends on the actual number of solutions, 
the computations of the configuration extension sets adopt the complexity of a local 
computation scheme. Moreover, the successive buildup of Сф can be seen as a top-
down message-passing in the join tree. The messages 
ßch(i)^i 
— 
сф 
(О./) 

COMPUTING SOLUTIONS 
2 9 9 
are tuple sets and thus elements of the relational algebra. As an alternative to building 
up Сф directly with Theorem 8.1, we may content ourselves with computing the 
marginal of Сф relative to the node label А(г) in each node г £V. This could be seen 
as a pre-compilation of the solution set Сф and requires to compute 
4 λ ( ί ) 
= 
j z E Ω λ ( 0 : ζ4λ(<)ηλ(ο/ι(ί)) e С1А(0ПА(<Л«) a n d 
Zl\(i)-X(ch(i)) 
e 
wHi)n\(ch(i)),zi.\(i)n\(ch(i)))\ 
= 
{z e Ωλ(<) : z4-M0nA(c/,(i)) e м.^ с Ц г ) a n d 
ζ4.λ(ί)-λ(ε/ί(ί)) e ^A(i)nA(cii(i))(.z4.A(i)nA(c/i(i))0 _ 
in each node. Observe that this formula is closely related to the operation of natural 
join, which points out that solution construction corresponds to local computation 
in the relational algebra. The detailed elaboration of this perspective is proposed as 
Exercise H. 1 to the reader. However, an important disadvantage of this algorithm 
is that it requires a complete run of a multi-query local computation architecture to 
obtain the marginals of the objective function relative to all node labels used in Lemma 
8.2. We shall therefore study in the following subsection, under what restrictions one 
can omit the execution of the outward phase and compute the configuration extensions 
only from the result of a single-query local computation architecture. 
Algorithm 8.1 Computing all Solutions I 
input: 
0Ί-λΜ for 1 < i < r output: Сф 
begin 
for i = r — 1... 1 do 
s : = A(r)U...U А(г + 1); 
end; 
return c; 
end 
8.2.2 
Computing some Solutions without Distribute 
We now start from a complete run of the generalized collect algorithm from Sec-
tion 3.10 or any other single-query local computation procedure. At the end of the 
message-passing, the root node r contains the marginal <^λ(Γ'. This ensures that the 
initialization step of Algorithm 8.1 can still be performed, even if we omit the exe-
cution of the outward phase. More challenging is the loop statement that constructs 
the solution set Сф using Lemma 8.2. Here, assume the existence of configuration 
extension sets with respect to φ^λ^ 
for г = 1,..., r — 1, but these marginals are 
not available at the end of a single-query local computation procedure. We therefore 
need some way to build solution sets from configuration extension sets with respect 

3 0 0 
DYNAMIC PROGRAMMING 
to the node contents 
V»ir) 
= 
^t® (g) 
ßj-* 
jSpo(i) 
at the end of the collect algorithm. For simplicity, we subsequently write 
Щ = d(^r)) 
= d(rl>i)U ( J 
ά(μ^) 
С X(i). 
j€pa(i) 
As a first important result, we show that the node domain at the end of the collect 
phase already contain all required information: 
Lemma 8.3 For x G c\ 
it holds that 
jp-A(i)nA(c/i(i))/4 
_ 
^ j n A ( c / i ( i ) ) 
,^ir\\(ch{i))\ 
Proof: It follows from Lxmma 4.2 that А(г) - X(ch(i)) — ил — X(ch(i)). Since 
ru*i 
_ 
L u w l 4 0 " 
we have 
$*< 
= 
{z G Ωωί : ζ ^ ' η λ ^ » G c^*™*0""» and 
zJ.Wi-A(c/i(i)) £ ту^ПЛ(с?1(г))^4.ш,пА(С^(г))0 
= 
{z G Ωλ(0 : ζ^«™^» 6 cU(i)nA(cfc(i)) a n d 
zlX(i)-\(ch(i)) 
g ^А(г)ЛА(с/1(г))^4Д(г)пА(сЛ(*)))\ 
= 
{z 6 Ωλ(ΐ) : ζ + λ « η λ ^ » G c$*(i)nA(<A(<)) and 
ζ4.ωί-λ(ο/ι(«)) g 
WrA(i)nA(c/l(i))^z4.A(i)nA(cfc(f))^-|'U'i 
i^i 
{ z ^ ' : z G Плт and ζ^>ηλ(«Λ(0) e cU(i)nA(cM0) a n d 
z 
A(i) 
4.<jj—A(ch(i)) e ида^'»^0™^")}· (8.8) 
Now, assume x G С^<0ПА(«Л(0) a n d y £ ^ΑΜηλ(οΜ*))(χ)_ χ ^ 
( x > y ) 6 cJA(i) 
which implies (х^пА(с/>(г)))у) e c£* andhencey G π ς ^ λ ( ί : Μ ί ) ) ( χ + ω ί Π λ ^ » ) . 
On the other hand, assume that z = (x, y) G Ω ^ ) with x G c\ 
and y G H^;lf ( c / i ( i )V" i n M c' l ( i ) ))· Since χ+ ω· η λ<^» G c j * " * ^ » 
We have 
ζ1ω, 
= ^хяпл(сА(0)]У) e 
C 1 " , 
ft 
m e n f o U o w s f r o m equation (8.8) that y G 
W,^(i) 
(x) must hold. Hence, the equality claimed holds. 
■ 

COMPUTING SOLUTIONS 
3 0 1 
Applying this result, we can replace the node labels А(г) in Lemma 8.2 by the 
corresponding node domains ω» just after collect. 
We mentioned in the foregoing section that the process of solution construction 
can be interpreted as local computation in the relational algebra of solution sets. It is 
clear that for φ G Φ with ά(φ) — swe have а(сф) = s. Note that the last expression 
refers to the labeling operator in the relational algebra. In addition, Lemma 8.1 shows 
how the operation of projection in the valuation algebra (Ф, D) is related to projection 
in the relational algebra of solution sets. The following lemma presupposes a similar 
property for combination and shows that if this property holds, the marginals of ф in 
Lemma 8.2 can be replaced by the node contents at the end of a single-query local 
computation procedure. 
Lemma 8.4 If the configuration extension sets in a valuation algebra (Ф, D) satisfy 
the property that for all φι,ψ2 G Φ with а{ф\ ) = s, й(^г) =t, sÇuÇsLlt 
and 
x G Ω„ we have 
W&nVunt) 
С Η^ιβψ2(χ), 
(8.9) 
then it holds for г = r — 1,..., 1 and s = X(r) U ... U А (г + 1) that 
cU(r)u...uA(i) 
3 
{zensUHl):z^e$and 
(8.10) 
z4-A(i)-3 g W"in\(ch(i)) 
^lUin\(ch(i))j} 
Proof: From the distribute phase, transitivity and the combination axiom follows: 
Ф* = (^«Г= (*ίρ)®**«-*Γ= ^ « с : в д ) -
We next apply property (8.9) with s = и = иц Л X(ch(i)) and t = ω» and obtain for 
all x G Ω„ 
u /w inA(c/i(i))/ 4,ω4ηλ(<Α(0)\ 
_ 
т1/"«ПЛ(с/г(г)) 
/ 4.ωίηλ(ο/ι(ί))\ 
r c h . ( i ) - n 
*°"^ г 
D 
^^ПЛ(сМг))/х4о>;ПЛ(с/г(г))^ 
(8.11) 
Applying Lemma 8.3, it then follows for the statement of Lemma 8.2: 
cUuXd) 
= 
{ z G Î 7 s U À ( i ) : z 4 S e c ^ a n d 
zi\(i)-s 
g 1уА(г)ПА(сЛ(г)),4.А(г)ПА(сМ*)))\ 
= 
{z € Ω8υλ(ί) : zis G 4 s and 
z4.A(t)-s g ^^ПА(сЛ(«))/ 
^ n A i c h « ) ) " ! 
D {z G Ω8υλ(ί) : z i s G сф
я and 
z4.A(»)-s g р^-^ПЛ(сЛ(г)), 4Ш;ПЛ(сЛ(г))ч\ 
(8.12) 

302 
DYNAMIC PROGRAMMING 
Theorem 8.2 If property (8.9) holds and if configuration extension sets are always 
non-empty, then at least one solution of φ — φ\ (g> ... ® φη can be found, using the 
node contents at the end of any single-query local computation architecture. 
This follows from Lemma 8.4 applying the following procedure: 
1. Execute a single-query local computation procedure on {φι,,.., 
φη}-
2. Compute с, 
in the root node. 
3. For г = r — 1,..., 1 
(a) compute №ш'{г)
 (с (г' in node г eV; 
(b) build a subset of c, 
'"u 
by application of equation (8.10). 
4. Return the subset of Сф = c\ 
'"u 
. 
Hence, if configuration extension sets are always non-empty and if property (8.9) 
is satisfied, we obtain at least one solution from the above procedure that is entirely 
based on the node contents at the end of a single-query local computation procedure. 
The two conditions imposed on the valuation algebra and on the solution extension 
sets therefore reflect the prize we pay to avoid the distribute phase. Here is the 
corresponding algorithm for the procedure above: 
Algorithm 8.2 Computing some Solutions 
(r) 
input: 
V; 
f ° r 1 < г < r output: 
с С Сф 
begin 
с := W]> (о); 
for i = r — 1... 1 do 
s := A(r)U...UA(î + 1); 
s е w 
с := {ζ : ζ-^ e c A z W ' ) - s e 
w",£4ch(i))^ШгГ[Х(ск^щ. 
end; 
return с 
end 
In particular, if both conditions are satisfied, we can always find one solution by 
the following non-deterministic algorithm: 

COMPUTING SOLUTIONS 
3 0 3 
Algorithm 8.3 Computing one Solution 
(r) 
input: -ф\ f o r 1 < г < г output: x £ Сф 
begin 
choose x. eW^.Jo); 
for г = r — 1 . . . 1 do 
hoose y e v ^ M c h ( i ) V a ; i n M c ' ' ( i ) ) ) ; 
(x,y); 
end; 
return x; 
end 
8.2.3 
Computing all Solutions without Distribute 
We have seen so far that all solutions can be found, if we agree on a complete run 
of the outward propagation. Without this computational effort, at least a subset of 
solutions can be identified, if condition (8.9) is satisfied. This section will show that if 
we impose a more limiting condition, all solutions can be found even with the second 
method. In fact, it is sufficient to ensure equality in Lemma 8.4: 
Theorem 8.3 If equality holds in equation (8.9), then all solutions can be computed 
based on the node contents at the end of a single-query local computation architecture. 
Proof: Since equality holds in property (8.9), we also have equality in the equations 
(8.11) and (8.12). This gives us the following reformulation of Lemma 8.2: For 
г = r — 1,..., 1 and s = \{r) U ... U λ(ζ + 1) we have 
4.sUA(i) 
Сл. 
{z G Ωβυλ(ί) : z+s G 4 s and 
(8.13) 
zi\(i)-s e №ω , η λ ( ώ ( . ) ) , 4.WinA(cft(i))0 
We can then compute Сф by the following procedure: 
1. Execute a single-query local computation procedure on {φχ,..., 
φη}. 
2. Identify с, w in the root node. 
3. For г = r — 1,..., 1 
(a) compute W"lr) 
in node i G V; 
тр. 
(b) build Сф 
'"и 
by application of equation (8.13). 
4. Return Сф = 
с1
ф
Х{г)и-иХ{1). 

304 
DYNAMIC PROGRAMMING 
This is expressed in the following algorithm: 
Algorithm 8.4 Computing all Solutions II 
input: 
<^AW for 1 < i < r output: Сф 
begin 
с - 
W£A(r)(o); 
for г = r — 1... 1 do 
s := A(r) U ... U λ(ΐ + 1); 
с := { z : z i ) 6 c A Z W ' b ' e r ! " A W > ] ( x ^ n A W , l > ) } ; 
Ф. 
end; 
return с; 
end 
This completes our general study on local computation based solution construction 
in valuation algebras. In the following section, we provide a detailed analysis of 
a particularly important field of application. This is constraint reasoning or more 
generally, the solution of optimization problems. In addition, this section will also 
show that important examples exist, where only strict inclusion holds in (8.9). An 
application to solving linear systems of equations will be presented in Chapter 9. 
8.3 OPTIMIZATION AND CONSTRAINT PROBLEMS 
Chapter 5 introduced the extensive family of semiring valuation algebras that emerge 
from a simple mapping from configurations to the values of a commutative semiring. 
Particularly important among these valuation algebras are constraint formalisms, 
which are obtained from the subclass of c-semirings in Definition 5.3 (Bistarelli 
et ai, 1997; Bistarelli et al, 1999). C-semirings are characterized by an absorbing 
unit element, which further implies the idempotency of semiring addition. Moreover, 
it follows from Lemma 5.3 that all idempotent semirings are partially ordered. We 
will see in this section that inference problems over valuation algebras induced by 
idempotent semirings represent optimization problems with respect to the partial 
semiring order. If in addition this order is total, we may ask for one or more con-
figurations that lead to the optimum value. This is a very typical field of application 
for the theory of solution construction developed in the first part of this chapter. In 
particular, since c-semirings are idempotent too, it covers the solution of constraint 
systems as a special case. 
8.3.1 Totally Ordered Semirings 
In the general case, a semiring (A, +, x, 0,1) only provides the canonical preorder 
introduced in equation (5.3). This is a very fundamental remark since it leads to a 
split of algebra into semirings with additive inverses (i.e. the monoid (A, +) forms 
a group) and semirings with a canonical partial order called dioids (Gondran & 
Minoux, 2008). Take for example the arithmetic semiring of integers (Z, +, ·, 0,1) 

OPTIMIZATION AND CONSTRAINT PROBLEMS 
305 
and observe that a < b and b < a does not imply that a = b. Antisymmetry therefore 
contradicts the existence of additive inverses or, in other words, the group structure of 
(A, +). As mentioned above, idempotency of addition is a sufficient condition to turn 
the semiring preorder into a partial order. If this relation is furthermore a total order 
as it is in many of the examples in Section 5.1, we obtain a class of semirings that are 
sometimes called addition-is-max semirings (Wilson, 2005). This name comes from 
the following lemma. 
Lemma 8.5 In an idempotent semiring we have a + b = max{a, b} if, and only if, 
its canonical order is total. 
Proof: If the canonical order is total, we either have о < b or b < о for all a, 6 G A. 
Assume that a < b. Then, a + b = b and therefore a + b = max{a, b}. The converse 
claim holds trivially. 
■ 
Alternatively, we may also see this lemma as a strengthening of Property (SP4) 
from Lemma 5.5, and according to (SP1) and (SP2) in Lemma 5.2 the total order 
behaves monotonie with respect to both semiring operations. For later applications 
however, we often require a more restrictive version of monotonicity whose definition 
is based on the following notation: 
a < b <Ф a <b and а ф b. 
(8.14) 
Definition 8.3 An idempotent semiring (A,+,x,0,1) 
is called strict monotonie, if 
for a,b,c G A and с ф 0, a < b implies that a x с < b x с 
Let us see which of the semirings from Section 5.1 are strict monotonie. 
Example 8.1 The Boolean semiring ({0,1}, max, min, 0,1) of Example 5.2 is strict 
monotonie, since 0 < 1 implies that min(0,1) < min(l, 1). Its generalization on 
the other hand, the Bottleneck semiring (Ш U {—oo,+oo},max, min, — oo, +oo) 
of Example 5.3, is not strict monotonie since we cannot conclude from a < b 
that min(a, c) < min(6, c). A bit more challenging is the tropical semiring (N U 
{0, +oo}, min, +, oo, 0) from Example 5.4. Here, the canonical semiring order cor-
responds in fact to the inverse order of natural numbers. Nevertheless, this semiring 
is strict monotonie, min(a, b) = b and а ф b implies that min(a + c,b + c) — b + с 
and a + с ф b + с, if с ф oo. Also, the arctic semiring (M U {—oo}, max, +, —oo, 0} 
is strict monotonie which follows from the same argument replacing minimization 
by maximization. Further, we directly see that strict monotonicity cannot hold in the 
truncation semiring of Example 5.6, but it does so in the semiring of formal languages 
from Example 5.7. Here, the strict order from equation (8.3) corresponds to strict set 
inclusion which is maintained under concatenation. Finally, in the triangular norm 
semirings of Example 5.8, strict monotonicity has to be verified for each choice of the 
triangular norm separately since they are only non-decreasing in their arguments. 
An important consequence of a total order flows out of Definition 8.3: 

306 
DYNAMIC PROGRAMMING 
Lemma 8.6 In a totally ordered, strict monotonie, idempotent semiring we have for 
a φθ that axb = axcif, 
and only if,b = c. 
Proof: Clearly, b = с implies that a x b = a x с On the other hand, assume 
a x b = a x с Since the semiring is totally ordered, either b < с, с < b or b — с 
must hold. Suppose b < с Then, because x behaves strict monotonie, we have 
a x b < o x с which contradicts the assumption. A similar contradiction is obtained 
if we assume с < b which proves b = с. 
ш 
So far, we called an idempotent semiring totally ordered, if its canonical order is 
total. In fact, the following lemma shows that if other total and monotonie orders are 
present in an idempotent semiring, they are always equivalent to either the canonical 
order or the inverse canonical order. The exact case can be found out by comparing 
the zero and unit element of the semiring. Thus, we may simply speak about totally 
ordered, idempotent semirings without specifying the exact order relation. On the 
other hand, we can limit our considerations to the total canonical order and the results 
then also apply to other total orders. 
Lemma 8.7 Let (A, +, x, 0,1) be an idempotent semiring with a total canonical 
order <i and an arbitrary total, monotonie order <2 defined over A. We then have 
for all a,b G A: 
1. 0 <2 1 => [a<2b О a<i b]. 
2. 1 <2 0 => [a <2 b <!=> b <i a]. 
Proof: We first remark that 0 < 2 1 implies 0 = 
flx6<2lx/i=i) 
and therefore 
a = a + 0 <2 a + b by monotonicity of the assumed total order. Hence, a,b <2 a + b 
for all α,ίιΕΑ Similarly, we derive from 1 <2 0 that a + b <2 a, b. 
1. Assume that a <2 b and by monotonicity and idempotency a + b < 2 b + b = b. 
Since 0 <2 1 we have b <2 a + b and therefore b <2 a + b <2 b. Thus, we 
conclude by antisymmetry that a + b = b, i.e. a <i b. On the other hand, if 
a <i b, we obtain from 0 <2 1 that a <% a + b = b and therefore a <2 b. 
2. Assume that a < 2 b and by idempotency and monotonicity a = a + a < 2 a + b. 
Since 1 <2 0 we have a + b <2 a and therefore a <2 a + b <2 a. Thus, we 
conclude by antisymmetry that a + b = a, i.e. b <i a. On the other hand, if 
b <i a, we obtain from 1 <2 0 that a = a + b <2 b and therefore a <2 b. 
m 
Note that for semirings where 0 = 1, both consequences of Lemma 8.7 are 
trivially satisfied because these semirings consist of only one element. 
Example 8.2 Consider the tropical semiring (N U {0, +00}, min, +, 00,0) which is 
idempotent and provides the natural order between non-negative integers. This order 
is monotonie with respect to + and min. Since 0 = 00 and 1 = 0 we have 1 < 0. We 
therefore conclude from Lemma 8.7 that the natural order of the tropical semiring 
corresponds to the inverse canonical order. 

OPTIMIZATION AND CONSTRAINT PROBLEMS 
3 0 7 
8.3.2 
Optimization Problems 
We learned in Section 2.3 that inference problems capture the computational interest 
in valuation algebras, and their efficient solution was the aim of the development of 
local computation algorithms. Next, it will be shown that inference problems acquire 
a very particular meaning when dealing with valuation algebras that are induced by 
semirings with idempotent addition. Due to Lemma 5.5, Property (SP4) we then have 
for an inference problem with query t Ç ά(φ) = s and x 6 fit, 
Фи{х) 
= 
Σ 
0(х,У) = sup{0(x,y), y e Q s _ t } , 
(8.15) 
if φ = φι (g>... ® фп denotes the objective function. This corresponds to the com-
putation of the lowest upper bound of all semiring values that are assigned to those 
configurations of φ that project to x. In particular, we obtain for the empty query 
φ^(ο) 
= 
sup{0(x), x € ns}, 
which amounts to the computation of the lowest upper bound of all values of φ. If 
we furthermore assume that the underlying semiring is totally ordered, we obtain 
according to Lemma 8.5 
<^(x).= 
J2 
<^х'У) = max{4>(x,y), y e i l . 4 
(8.16) 
and 
<^0(o) 
= 
max{0(x), x € Os}. 
(8.17) 
In other words, inference problems on valuation algebras induced by a totally or-
dered, idempotent semiring amount to the computation of maximum (or minimum) 
values. They therefore also called optimization problems or, if they are induced by 
c-semirings, constraint problems. Let us survey some typical examples: 
■ 8.1 Classical Optimization 
The most classical optimization problems are obtained from the tropical or arc-
tic semirings of Example 5.4, where x corresponds to the usual addition and 
4- to either minimization or maximization. Let us consider the tropical semi-
ring (Ш U {oo}, min, +, oo, 0) and a knowledgebase {φχ,..., 
φη} of induced 
valuations. We then obtain for the inference problem with empty query 
0i«(o) 
= 
^ 8 . . . ® ^ / 
= 
min { ^ ( x ^ * ) ) + ... + &,(x M* n )), x G Ω8} 
where s = ά(φ). Recall from Example 8.2 that the natural order of a tropical 
semiring corresponds to its inverse canonical order. This explains the difference 

308 
DYNAMIC PROGRAMMING 
to equation (8.17). Solving optimization problems induced by tropical or arctic 
semirings is the typical task of reasoning with weighted constraints. A concrete 
application from coding will be presented in Instance 8.5 below. 
■ 8.2 Satisfiability 
Satisfiability with crisp constraints corresponds to the task of solving an op-
timization problem induced by the Boolean semiring ({0,1}, max, min, 0,1). 
Given a knowledgebase of valuations induced by the Boolean semiring, the 
crisp constraint φ = φ\ ®... ® φη is satisfiable if Яф = {x e fls : ф(х) = 1} 
contains at least one configuration, and this is the case if ф^(о) = 1. Other-
wise, if ф^(о) = 0 = 20(о), the set of constraints is contradictory due to the 
nullity axiom of Section 3.4. This also includes the satisfiability problem of 
propositional logic from Section 7.1. If {/i,... ,/„} denotes a set of formulae, 
we may express their possible interpretations by a set of crisp constraints or 
indicator functions {φχ,... ,φη}. The objective function φ then reflects the 
possible interpretations of the conjunction /i Л ... Л / n, and Яф corresponds 
to its set of models. Thus, if |Д^| > 0, the conjunction is satisfiable. 
■ 8.3 Maximum Satisfiability 
Maximum satisfiability (Garey & Johnson, 1990) can be regarded as an ap-
proximation task for the satisfiability problem. For a set {ф\,..., 
φη} of crisp 
constraints or formulae, we do not require that all elements are satisfied as in 
the foregoing instance, but we content ourselves with satisfying a maximum 
number of elements. Thus, if ф\ to фп still map configurations to Boolean 
values, we may compute 
ф^{о) 
= 
т а х { ^ ( ^ < « ) + ... + 
^ * ) ) , х е ^ } · 
This identifies the maximum number of constraints or formulae that can be 
satisfied by any configuration. The semiring used in this application is the 
arctic semiring (Z U {—oo, oo}, max, +, —oo, 0) of Example 5.5. 
■ 8.4 Most & Least Probable Values 
We have seen in Section 5.4 that the arithmetic semiring induces the valuation 
algebra of probability potentials. However, instead of a probability value, we 
are sometimes interested in identifying either a minimum or maximum value of 
all probabilities in the objective function ф. The maximum can for example be 
determined using the product t-norm semiring. Here, addition is maximization 
and ф^(о) corresponds to the probability of the most probable configuration 
or explanation of a given situation. This is especially important in diagnostic 
problems. Similarly, if we replace maximization by minimization, the marginal 

OPTIMIZATION AND CONSTRAINT PROBLEMS 
3 0 9 
identifies the value of the least probable configuration or explanation. 
When dealing with optimization or constraint problems, we are in general less 
interested in the optimum value of the objective function, but more interested in the 
actual configurations that adopt this value. Following (Shenoy, 1996) we refer to such 
configurations as solution configurations. The most and least probable configurations 
of Instance 8.4 are typical examples of such solution configurations. 
Definition 8.4 Let (Φ, D) be a valuation algebra induced by a totally ordered, idem-
potent semiring. For φ € Φ with ά(φ) = s, we call x e Ω,8 a solution configuration, 
i/0(x) = φ^{ο). 
We shall see in Section 8.4 that solution configurations give rise to solution exten-
sion sets according to Definition 8.1. Let us consider some typical applications, where 
knowing the solution configuration is more important than computing their assigned 
semiring value. These examples all require to find a single solution configuration for 
a valuation φ = φ\ ® ... ® φη with ά(φ) = s. Depending on whether the canonical 
semiring order corresponds to minimization or maximization, we write 
arg max φ or arg min φ, 
for the task of finding an arbitrary solution configuration. 
■ 8.5 Bayesian and Maximum Likelihood Decoding 
Consider an unreliable, memoryless communication channel to transmit sym-
bols out of a finite coding alphabet Λ. If we assign the random variables X 
and Y to the input and output of the channel, then the latter is fully specified 
by its transmission probabilities p(Y = yi\X = Xj) for yi,Xj € A. This is the 
probability that input symbol Xj is changed into output symbol yi by sending 
it through the channel. Figure 8.1 illustrates such a channel for a binary coding 
alphabet A = {0,1}. Instead of single symbols, we now transmit code words 
that consist of n G N consecutive symbols. Thus, an unknown input word 
x = (xi,... ,xn) is transmitted over the channel and on the receiver side, 
an output word y = (yi,..., yn) is observed. The decoding process asks to 
deduce the input from the received output and this can for example be done 
by choosing the input word x = (χχ,... ,xn) that leads most probably to the 
observed output y. In other words, we choose x such thatp(y|x) is maximum. 
Since p(y|x) = p(y, x)/p(y) and maximization only concerns the input code 
words, it is sufficient to compute 
arg max I JJ p(Yi = у^Х{ = хг) ■ р(Хг = х ь ..., Хп = хп) I . 
Here, p(x) = р{Х\ = х\,..., 
Хп = х„) is the prior distribution of the input 
word. The semiring valuation algebra used in this example is induced by the 
t-norm semiring ([0,1], max, ·, 0,1} of Example 5.8. 

310 
DYNAMIC PROGRAMMING 
Input 
о — 
Output 
о 
Figure 8.1 A binary, unreliable, memoryless communication channel. 
The above decoding approach is generally called Bayes decoding since it is 
based on a prior distribution p(x) of the input code words. Alternatively, this 
can be simplified by assuming a uniform prior distribution that can be ignored 
in the maximization process. We then obtain 
- 8 m» П*«-«.!*-*> 
called maximum likelihood decoding. It is convenient for computational pur-
poses to replace the maximization of probabilities by the minimization of their 
negated logarithms. We obtain for the case of maximum likelihood decoding 
arg max I Д p{Yi = Уг\Хг = х{) 
arg min - ^ 
logp{yi\xi) 
(8.18) 
i=\ 
This is an optimization problem induced by the tropical semiring. 
8.6 Linear Decoding 
Linear codes go a step further and take into consideration that not all ar-
rangements of binary input symbols may be valid code words. Such systems 
therefore provide a so-called (low density) parity check matrix H which assures 
H · x T = 0 if, and only if, x is a valid code word. For illustration purposes, 
assume the following example matrix (Wiberg, 1996): 
Я 
1 1 0 
1 0 
0 0 
0 0 1 1 0 
1 0 
0 0 0 1 1 0 
1 

COMPUTING SOLUTIONS OF OPTIMIZATION PROBLEMS 
311 
Then, x = (x\,..., 
χγ) is a valid code word if 
(Xl+X2+Xi,X3 
+ Xi + X6,X4+X5+ 
ΧΊ) 
= 
(0,0,0). 
These additional constraints can be inserted into the maximum likelihood de-
coding scheme using the following auxiliary functions: 
χ ι ( ζ ι , £2,2:4) 
Χ2{ΧΆ,Χ4,Χβ) 
= 
0, 
if x\ + X2 + X4 = 0 
00, 
otherwise, 
0, 
if X3 + X4 + xe = 0 
00, 
otherwise, 
, 
. 
I 0, 
if x4 + хь + χγ = 0 
Хз(а:4,ж5,а:7) = < 
. 
100, 
otherwise. 
We then obtain for equation (8.18) and n = 7, 
7 
arg min( ^ 
- logp^lxj) + XI(XI,X2,XA) 
+ 
i = l 
Xi(x3, X4, хв) + Хз(ж4, хь, ΧΊ)) ■ 
It remains an optimization problem with knowledgebase factors induced by 
the tropical semiring. Furthermore, applying the fusion algorithm from Section 
3.2.1 to this setting yields the Gallager-Tanner-Wiberg algorithm (Gallager, 
1963) as it was observed by (Aji & McEliece, 2000). A survey of these and 
related decoding schemes is given in (MacKay, 2003) where the author presents 
the corresponding algorithms as message-passing schemes. This suggests that 
even more sophisticated and state of the art decoding systems such as convo-
lutional or turbo codes are subsumed by optimization problems over semiring 
valuation algebras. 
More examples can be found in constraint literature, e.g. (Apt, 2003). 
8.4 COMPUTING SOLUTIONS OF OPTIMIZATION PROBLEMS 
Given a factorized valuation φ = φ\ ® ... ® φη with ά(φ) = s, induced by a totally 
ordered, idempotent semiring, we obtain the optimum value φ^(ο) by applying an 
arbitrary single-query local computation architecture from Chapter 3. But we have 
seen in the foregoing section that in such cases, one is often interested in finding one or 
more configurations x e Q s that adopt the optimum value, i.e. with φ{χ) = φ^(ο). 

3 1 2 
DYNAMIC PROGRAMMING 
Such configurations were called solution configuration in Definition 8.4, and it follows 
directly from equation (8.16) that at least one solution configuration always exists. 
Moreover, given an arbitrary configuration y e Ω( for ί Ç s, it is always possible 
to find an extension x € Qs-t such that ^ ' ( x ) = 0(x, y). Subsequently, we refer 
to y as a configuration extension of x with respect to ф. The set of all configuration 
extensions of x with respect to ф is then given by 
W^(x) = 
( у е И и : ф , у ) = ^ ( х ) } . 
(8.19) 
Theorem 8.4 Solution extension sets in optimization problems satisfy the property 
of Definition 8.1, i.e. for all ф G Ф with t С и Ç s = ά(φ) and x G ilt we have 
Wj(x) - 
{z G ns_t : z4·"-' G И^„(х) andzls-u 
G ^ ( χ , ζ ^ ) } . 
Proof: It follows from equation (8.19) that 
{z G iîs_t : z^-É G W|iu(x) andz^"" G W^x.z*"-')} 
{z G Qs_t : <^(x) = #"(x,z^- É) a n d ^ x ^ " - ' ) = 0(x,z)} 
( г е О и : Л х ) = ф , г ) } 
Configuration extension sets in optimization problems therefore instantiate the 
general definition of configuration extension sets given in Section 8.1. So, we may also 
specialize the general definition of solution sets given in Definition 8.2 to optimization 
problems. We obtain 
4 = W*{o) = {yelîs: 0(x) = φι\ο))- 
(8.20) 
Observe that this indeed corresponds to the notion of solutions in constraint or 
optimization problems given in Definition 8.4. Moreover, we also see that several 
possibilities to define configuration extension sets may exist in a valuation algebra. 
Instead of the configurations that map to the optimum value, we could also consider 
all other configurations that do not satisfy this property as solutions and define the 
configuration extension sets accordingly. In terms of constraint reasoning, we then 
search for counter-models. However, we continue with the above definition and 
show in the following example that computing solution extensions or solution sets in 
optimization problems essentially amounts to a table lookup. 
Example 8.3 Consider a set {A, B, C} of variables with finite frames Ω,Α = {a, S}, 
Ωβ = {b, b} and Ü.C = {c,c}, and a valuation φ induced by the tropical semi-
ring (N U {0, oo}, min, +, —oo, 0). We perform a step-wise projection οίφ until all 
variables are eliminated: 
= Wftx). 

COMPUTING SOLUTIONS OF OPTIMIZATION PROBLEMS 
313 
A 
a 
a 
a 
a 
a 
a 
â 
В 
b 
b 
b 
b 
b 
b 
b 
b 
с 
с 
с 
с 
с 
с 
с 
с 
с 
2 
4 
9 
4 
5 
8 
2 
5 
фНА,С} = 
А 
а 
а 
а 
а 
С 
с 
с 
с 
с 
2 
4 
2 
5 
^ с > = 
С 
с 
с 
2 
4 
We finally obtain φ^ (о) = 2. By consulting the tabular representation of ф, we 
determine the solution configuration set 
Сф 
{(a,b,c),(a,b,c)}. 
JW 
For the partial solution (a) G et 
we find the solution extension set 
rW 
W^(a) 
= 
{(b,c)} 
,I{C} 
again by consulting the above table, and for (c) e ct 
we find 
Wlc\c) 
{(a,b),(a,b)}. 
Since configuration extensions in optimization problems satisfy Definition 8.1, it 
follows from Section 8.2.1 that the complete solution set Сф of a factorized valuation 
Ф = φι ® · · ·®φη can be determined from the results of a previous run of a multi-query 
local computation architecture. The corresponding procedure is given in Algorithm 
8.1. However, many practical applications of constraint reasoning just require to find 
a single solution, and this should, if possible, be done without the additional effort of 
a downward propagation in the local computation scheme. Indeed, we have seen in 
Section 8.2.2 that this is possible if configuration extension sets are always non-empty 
and condition 8.9 of Lemma 8.4 holds. The first requirement directly follows from 
equation (8.19), i.e. configuration extension sets and therefore also solution sets in 
optimization problems are always non-empty. The second requirement is guaranteed 
by the following lemma: 
Lemma 8.8 In a valuation algebra induced by a totally ordered, idempotent semiring 
we have for all φι,ψ2 € Φ with ά{φ\) = s, с2(ч/>2) = t, s Ç и С s Ut andx € Ω„ 
W«nt(x4.unt) 
ς 
WCWx). 
Proof: Assume x € Ω„ and y e W^nt(-K^tnu). By equation (8.19) we have 
ф2(^иП\у) 
- 
^ U n V n t ) , 

314 
DYNAMIC PROGRAMMING 
hence also 
^ i ( x i s ) < ^ 2 ( x i u n t , y ) 
= 
< / > i ( x i s ) ® ^ " n V u n t ) , 
By application of the definition of combination in semiring valuation algebras and 
the combination axiom we obtain: 
(?/>i®V>2)(x,y) 
V>i(xis)®V^unt(x4unt) 
( ^ l ® ^ n f ) W 
= W>1^2) i U(x). 
We conclude from (ψχ ® ^2)(x, y) = {φλ <g> V2)4"(x) that y e Π^ ι β ψ 2(χ). 
■ 
Both conditions of Theorem 8.2 are therefore satisfied in optimization problems, 
which allows us directly to apply Algorithm 8.2 or the computation of a non-empty 
subset of solutions, or Algorithm 8.3 for the identification of a single solution. The 
following example illustrates a complete run of Algorithm 8.3. 
Example 8.4 We consider binary variables A,B,C 
with frames ÇIA = {а, о} to 
Q.C = {c,~c}. Let "ψΐιΨζ and -фз be three join tree factors defined over the bottle-
neck semiring (R U {—oo, oo}, max, min, — со, ос) with domains d(ipi) — {A, B}, 
d^ï) 
= {В,С}, фз = {В} and the following values: 
Φι 
A 
a 
a 
ä 
ä 
В 
b 
b 
b 
b 
2 
4 
3 
2 
V>2 = 
В 
b 
b 
b 
b 
с 
с 
с 
с 
с 
5 
2 
3 
3 
Фз = 
В 
ъ 
ь 
1 
6 
The join tree in Figure 8.2 corresponds to this factorization and numbering. 
{A,B} 
- * 
{B} 
* -
{B,C} 
Figure 8.2 The join tree that belongs to the node factors of Example 8.4. 
Let us first compute ф — ψι <g> Ψ2 <8> "фз directly to verify later results: 

COMPUTING SOLUTIONS OF OPTIMIZATION PROBLEMS 
315 
A 
a 
a 
a 
a 
a 
U 
ä 
ä 
В 
b 
ь 
b 
b 
b 
b 
b 
b 
с 
с 
с 
с 
с 
с 
с 
с 
с 
1 
1 
3 
3 
1 
1 
2 
2 
We observe that Сф = {(а, Ь, с), (а, Ь, с)} are the configurations with the maximum 
value. This will now be compared it with the result of Algorithm 8.2 that first requires 
to execute a complete run of the collect algorithm: 
μι^3 = в 
b 
b 
3 
4 42) = в 
6 
b 
1 
4 
β2^3 
= 
В 
b 
5 
b 
3 
ф^ = 
В 
b 
b 
1 
3 
Thus, the maximum value ofc\> is indeed ^ (о) </4
3)i0(o) 
3. Next we compute 
MB) 
< i ( B » (o) = Wj(3)(o) = {(6)}. 
Hfe can onfy choose x^B> = (b) and proceed for i — 2: 
W {B} (b) = 
{(с), (с)}. 
We choose (c) ana1 obtain x.^iB^c} = (i,^ c^ Finally, for i — lwe compute: 
= 
{(«)} 
and obtain the solution configuration x = (a, fe, c) G Сф with ф(а, b, с) = 3. 
w$Cb) 
If we repeat the computations in this example with Algorithm 8.2, we find the 
complete solution set Сф = {(a, b, с), (а, b, с)}. But this is generally not the case. The 
following example shows that sometimes strict inclusion holds in Lemma 8.8, which 
makes the computation of all solutions using Algorithm 8.2 impossible. Moreover, we 
cannot even know without computing ф explicitly, whether all solution configurations 
have been found or not. 
Example 8.5 We take the bottleneck semiring (Ku{—сю, сю}, max, min, —сю, сю) of 
Example 5.3 and assume two semiring valuations ф and ψ with domains ά(φ) = {A} 
and ά{φ) = {A, B}. The variable frames are ΩΑ = {a, ô} and Ωβ = {b, b}. 
A 
a 
â 
1 
1 
φ = 
A 
a 
a 
ä 
ä 
В 
b 
b 
b 
b 
6 
7 
8 
9 
Φ® ψ = 
A 
а 
a 
a 
a 
В 
b 
b 
b 
b 
1 
1 
1 
1 

316 
DYNAMIC PROGRAMMING 
For и = {А} = и Dt, we have 
ф1иШ 
_ 
Finally, we remark that 
A 
a 
ä 
7 
9 
[φ®ΨΫη = 
A 
a 
ä 
1 
1 
Wln\a) 
= {(b)} С \¥^ф(а) 
= W^Jä) 
= {(&),(&)}· 
wi 
All solution construction algorithms in Section 8.1 start from the marginal of φ 
with respect to the root node label \(r), which is obtained from the previous local 
computation process. From this marginal, we may further compute 
40 
^β(ο) = ( У * « ) (■ 
by one additional projection. The following lemma states that if this value corresponds 
to the zero element in the semiring, then all configurations are solutions of φ. If this 
is the case, we do not need to run the solution construction process. 
Lemma 8.9 Ι/ά(φ) = s, then ф^®(о) = 0 implies that Сф — Ω8. 
Proof: Property (SP3) in Lemma 5.2 implies that 0 < φ(χ) for all x G fls. Hence, 
if the maximum value is equal to zero, then all configurations are solutions. 
■ 
Constraints with zero as maximum element are sometimes called contradictory. 
Hence, it follows from the definition of solution extension sets in constraint systems 
that all configurations are solutions to a contradictory constraint. If, at the end of 
the collect phase, we obtain φ^ (о) = 0 in the root node, we know from the above 
lemma that all configurations are solutions. It is therefore not necessary to build the 
solution configuration set and we can stop the algorithm. Subsequently, we exclude 
this special case in the root node, before starting the solution construction procedure. 
Then, equality in Lemma 8.8 can be guaranteed, if the semiring is strict monotonie 
according to Definition 8.3. 
Lemma 8.10 In a valuation algebra induced by a totally ordered, idempotent and 
strict monotonie semiring we have for all 'φχ,'φι G Φ with ά(ψι) = s, ά(ψ2) = t, 
s Çu Ç sUt and x £ Ω„ 
w72
nV"nt) = wiwA*)-
Proof: It remains to prove that 
Wunt{xiunt) 
2 
W » i e ^ ( X ) . 
Assume x € Ω„ with ψχ (х+я) ф 0 and y g W ^ ® ^ (x)· By definition, 
(Ψι®ψ2)1χι(χ) 
= 
Wi®^2)(x,y) 
= 
^ ( x ^ ) x V 2 ( x i u n t , y ) . 

CONCLUSION 
317 
Similarly, we deduce from the combination axiom and the definition of combination 
(Ψι®ψ2)ι"(χ) 
= ( Λ β ^ " η ' ) ( χ ) = ^ i ( x i s ) x V 4 u n V u n i ) · 
Therefore, 
</>i(x+s) x V2(x i u n t,y) = ΦΙ(Χ18) 
x ?/4unt(xiuni). 
We conclude from Lemma 8.6 that 
V>2(xiunt,y) 
= 
^ u n V " n i ) 
and consequently that y e \¥^™(х^иПЬ). 
ш 
The additional property of strict monotonicity therefore allows us to apply Algo-
rithm 8.4 for the identification of all solution configurations based on the results of 
a single-query local computation procedure only. But the exclusion of the zero ele-
ment as optimum value in the root node is crucial. We therefore execute an arbitrary 
single-query local computation procedure on the factorization ф = ф\ ® ■ ■ ■ ® фп 
and determine the optimum value ф^ (о) in the rot node. If ф^ (о) = 0, we know 
from Lemma 8.9 that all configurations of ф are solutions. Hence, there is no need to 
start the solution construction process. On the other hand, ф^ (о) ф 0 implies that 
</>U(i)(x) Ф 0 for all x e 4,λ(ί) and X(i) Ç ά{φ). Then, 
О ф ф^(о) = ^ λ « ( χ ) 
= 
(^ r )®M c f c ( i H i)(x) 
= 
^ ) ( x i u , ) x M c M î ) ^ ( ^ ( , ) n A ( c M l ) ) ) · 
This shows that φ^ (о) ф 0 and x € ci l implies 
^ ( r ) (x^· ) ф 0 
and 
μ<Λ(0_* (xU«)nA(cfc(i)) ) φ 0_ 
The requirement of strict monotonicity is therefore always satisfied when Lemma 
8.10 is applied in the proof of Theorem 8.3, i.e. in each step of the solution con-
struction algorithm. To sum it up, it is always possible to compute a single solution 
to an optimization problem based on the results of a single-query local computation 
scheme. If all solutions are required, we either need the property of strict mono-
tonicity in the underlying valuation algebra, or we must execute a complete run of a 
multi-query local computation architecture. 
8.5 
CONCLUSION 
Many important valuation algebras have some notion of a solution, characterized by 
the property that every solution to a projected valuation is also a partial solution to the 
original valuation. This property allows us to extend partial solutions step-wise to a 
complete solution. The first part of this chapter presents three approaches to compute 

318 
DYNAMIC PROGRAMMING 
solution sets of a factorized valuation. First, we presuppose a completed run of a multi-
query local computation procedure, determine the solution set with respect to the root 
node label and proceed downwards the join tree to buildup the complete solution set. 
It was shown that these computations take place in the relational algebra of solution 
sets, such that the solution construction algorithm shapes up as a local computation 
scheme. This first approach works for all valuation algebras with a suitable notion 
of solutions, but it has the drawback to depend on a multi-query local computation 
scheme. Alternatively, if solution sets are non-empty and if they satisfy an additional 
condition with respect to combination, then it is possible to find a non-empty subset 
of solutions based on the join tree node contents at the end of a single-query local 
computation scheme. Finally, if a stronger condition with respect to combination of 
solution sets holds, then it is possible to find all solutions based on a single-query 
architecture. In the second part of the chapter we focus on the important application 
field of constraint reasoning or solving optimization problems. It was shown that such 
problems arise from totally ordered, idempotent semirings and they always satisfy 
the weaker condition for solution construction. The stronger condition, that makes 
the computation of all solutions based on a single-query architecture possible, holds 
when the semiring is strict monotonie. Further applications of solution construction 
for solving systems of linear equations will be studied in Chapter 9. 
PROBLEM SETS AND EXERCISES 
H.l * Describe the solution construction process from Section 8.2.1 in terms of 
local computation in the relational algebra of solution sets. The messages are given 
in Equation 8.7. It therefore remains to express Theorem 8.1 in terms of natural join. 
H.2 * Explore the t-norm semirings of Example 5.8 and Exercise E.6 in Chapter 5 
for strict monotonicity. 
H.3 * Instance 8.5 shows that Bayesian and maximum likelihood decoding establish 
optimization problems. This application only considers one channel. Identify the 
optimization problems that occur when multiple channels are connected in series as 
shown in Figure 8.3 and in parallel as shown in Figure 8.4. 
Gb 
P(YilX) 
Channel 1 
Yi 
*-
P<Y2IY1> 
Channel 2 
p(YnlYn-i) 
Channel n 
Figure 8.3 A serial connection of unreliable, memoryless channels. 
H.4 ** Context valuation algebras from Chapter 7 are based on the duality between 
the sentences of a language and their models. All instances of this family satisfy 

EXERCISES 
3 1 9 
ρ(Υ,ΙΧ) 
■θ 
о 
■о 
€) 
Figure 8.4 A parallel connection of unreliable, memoryless channels. 
idempotency and can therefore be processed by the idempotent architecture of Section 
4.5. In the introduction to this chapter, we mentioned several examples of context 
valuations as typical formalisms with some notion of solutions, i.e. solutions in linear 
equations, inequalities or models in different logics. Show that the set of models 
associated with a context valuation always satisfies the requirements for a solution set. 
Moreover, show that the interpretation of solution construction as local computation in 
the relational algebra of solution sets given in Section 8.2.1 and recessed in Exercise 
H.l then complies with the usual distribute phase in the idempotent architecture. 
Finally, investigate property (8.9) for context valuation algebras. 

CHAPTER 9 
SPARSE MATRIX TECHNIQUES 
We learned in Section 7.2 that the solution spaces of linear equation systems form 
a valuation algebra and even an information algebra. This hints at an application of 
local computation for solving systems of linear equations. However, in contrast to 
many other inference formalisms studied in this book, the valuations here are infi-
nite structures and therefore not directly suitable for computational purposes. On the 
other hand, the equations themselves are finite constructs and provide a linguistic 
description of the solution spaces. It is thus reasonable for linear systems to perform 
the computations not on the valuation algebra directly but on the associated language 
level. This is the topic of the present chapter. Whereas solving systems of linear 
equations is an old and classical subject of numerical mathematics, looking at it from 
the viewpoint of local computation is new and leads to simple and clear insights 
especially for computing with sparse matrices. It is often the case that the matrix of a 
system of linear equations contains many zeros. Such matrices are called sparse, and 
this sparsity can be exploited to economize memory and computing time. However, 
it is well-known that zero entries may get lost during the solution process if care 
is not taken. Matrix entries that change from a zero to a non-zero value are called 
fill-ins and they destroy the advantages of sparsity. Therefore, much effort has been 
spent on developing method that maintain sparsity. We claim that local computation 
Generic Inference: A Unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc. 
321 

3 2 2 
SPARSE MATRIX TECHNIQUES 
offers a very simple and easy to understand method for controlling fill-ins and thus 
for maintaining sparsity. This is not only true for ordinary, real-valued linear equa-
tions, or linear equations over a field, but also for linear equations over semirings. 
We have seen in Chapter 6 that path problems induce semiring fixpoint equation 
systems, where the sparsity often comes from an underlying graph. Independently, 
local computation maintains the sparsity in all these applications. 
An interesting aspect that arises from studying equation systems in the context 
of valuation algebras is the necessity to distinguish between factorizations and de-
compositions. The generic local computation methods from Chapter 3 and 4 solve 
inference problems that consist of a knowledgebase of valuations which factor the 
objective function. Where do these valuations come from? A key note behind the the-
ory of valuation algebras is that information exists in pieces and comes from different 
sources, which indicates that factorizations occur naturally. This is for example the 
case in the semiring valuation systems of Chapter 5, where factorizations are often 
the only mean to express the objective function, due to the exponential complexity 
behind these formalisms. In contrast, linear systems are polynomial, and it is of-
ten more realistic that a total linear system exists, from which the knowledgebase 
must be artificially fabricated. We then speak about a decomposition rather than a 
factorization. We will continue the discussion of factorizations and decompositions 
throughout this chapter and show that both perspectives are equally important when 
dealing with linear systems. 
In the beginning two sections of this chapter, we treat ordinary systems of linear 
equations by examining arbitrary systems in Section 9.1 and the important case of 
systems with symmetric, positive definite matrices in Section 9.2. As an application, 
the method of least squares is shown to fit into the local computation framework. 
Formally, this is very similar to the problems that we are going to treat in Chapter 10; 
only the semantics of the two systems are different. This uncovers a first application 
of the valuation algebra from Instance 1.6 in Chapter 1. The remaining sections are 
devoted to the solution of linear fixpoint equation systems over semirings. Section 
9.3 focuses on the local computation based solution of arbitrary fixpoint equation 
systems with values from quasi-regular semirings. The application of this theory to 
path problems over Kleene algebras is compiled in Section 9.3.3. 
9.1 SYSTEMS OF LINEAR EQUATIONS 
We first give a short review of Gaussian variable elimination. 

SYSTEMS OF LINEAR EQUATIONS 
3 2 3 
9.1.1 
Gaussian Variable Elimination 
Consider a system of linear equations with real-valued coefficients, 
Οΐ,ΐΧΐ 
+ 
Ol,2^2 
+ 
·"' 
+ 
Gl.n-Xn 
= 
Ь\, 
Û2,l-^1 
+ 
12,2^2 
+ 
· · ■ + 
a2nXn 
= 
b2, 
. 
(9-1) 
Xi 
+ 
·■· 
We make no assumptions about the number m of equations or the number n of 
unknowns and neither about the rank of the matrix of the system. So this system 
of linear equations may have no solution, exactly one solution or infinitely many 
solutions. In any case, the solutions form an affine space as explained in Section 7.2. 
The computational task is to decide whether the system has a solution or not. If it 
has exactly one solution, then this solution must be determined. If it has infinitely 
many solutions, then the solution space must be determined. The usual way to solve 
systems of linear equations is by Gaussian variable elimination. Assuming that the 
element αι,ι is different from zero, an elimination step based on aiti proceeds by 
first solving the first equation for Χχ in terms of the remaining variables, 
Xl 
= ±L-^lx2 
^Xn. 
(9.2) 
Ol,l 
Ol,l 
Й1Д 
Then, the variable X\ is replaced by the right-hand expression in all other equations. 
After rearranging terms this results in the new system of linear equations 
(02,2 --57^—)л 2 
+ 
··· 
+ 
(a2,„ 
^ — )Xn 
= 
b2--^j-, 
{am,2 
S7^iA2 
+ 
··■ 
+ 
(am,n 
^—)Xn 
- 
bm 
^ - . 
This is called a pivoting step with αιι as pivot element. Variable X\ is called the pivot 
variable and the first equation, where the pivot element is selected from, is called the 
pivot equation. It eliminates variable X\ and reduces the system of equations to a new 
system with one less variable and one less equation. The new system is equivalent to 
the old one in the following sense: 
1. If (xi,a;2,... ,xn) is a solution to the original system, then (x2, ■. ■ :xn) is a 
solution of the new system. 
2. If (x2,..., 
x„) is a solution of the new system, then (χχ, x2, ■ ■ ■, xn) with 
Ol,l 
a l , l 
01,1 
is a solution to the original system. 
This procedure is called Gaussian variable elimination and can be repeated on the 
new system. In order to execute a pivot step on a variable Xi there must be at least 

324 
SPARSE MATRIX TECHNIQUES 
one equation with a non-zero coefficient of Xi. If there is no such equation, Xi can 
be eliminated from the system without further actions. In fact, in this case Xi has 
zero coefficients in all equations which is tantamount to say that Xi does not appear 
in the system. So, Gaussian elimination permits to reduce the system stepwise by 
eliminating one variable after the other until only one variable remains. Several cases 
may arise during the elimination of variables: assume that the variables X\ to Xp 
have been successfully eliminated. Then, the following three cases may arise: 
1. All coefficients on the left-hand side of the new system vanish, but on the 
right-hand side there is at least one element different from zero. In this case 
the system has no solution. 
2. All coefficients on both sides of an equation vanish. Then, this equation can be 
eliminated. If no equations remain after elimination, then the variables Xp+i 
to Xn can take arbitrary values. 
3. There remains a non-vanishing system of linear equations with at least one 
equation less than the system before pivoting on Xp. Then, a next pivot step 
can be executed. 
The first case identifies contradictions in the system. For this, we do not necessarily 
need to continue the pivoting process until all coefficients on the left-hand side of the 
system vanish. Instead, we may stop the process when the first equation arises, where 
all coefficient on the left-hand side vanish, but the right-hand side value is different 
from zero. These three cases solve the linear system in the following sense: 
1. If case 1 occurs, the system has no solution. 
2. In case 2 a certain number of variables X%,..., 
Xp can be expressed linearly 
by the remaining variables Xp+i,..., 
Xn. This determines the solution space. 
3. All variables up to and including X„_i can be eliminated. Then Xn gets a 
fixed value in the last system, and a unique solution exists for the system. It 
can be found by backward substitution of the values of Xn, 
Xn~i,... 
Note that which of the above cases arises does not depend on the order in which 
variables are eliminated. 
The equations in the system after the first pivot step clearly show that original 
zero coefficients of certain variables may easily become non-zero in a pivot step. 
This is called a fill-in. The number of fill-ins depends on the choice of the pivot 
element or, in other words, there may be many fill-ins if the pivot element is not 
carefully chosen. Selection of the pivot element is partially a question of selecting 
elimination sequences of variables, but also of selecting the pivot equation. In the next 
section, we show how local computation allows to fix an elimination sequence such 
that fill-ins can be controlled and bounded. It should, however, be emphasized that 
limiting fill-ins is not the only consideration in selecting pivot elements. Numerical 
stability and the control of numerical errors are other important aspects that may be 

SYSTEMS OF LINEAR EQUATIONS 
325 
in contradiction to the minimization of fill-ins. The reader should keep this aspect in 
mind, although we here only focus on the limitation of fill-ins. 
9.1.2 Fill-ins and Local Computation 
To discuss a concrete scenario we assume that in our system of linear equations 
(9.1) we have a^i = 0 for all j = 1,..., r and i = h:... ,n and also for j — 
r + 1,..., m and i = 1,..., к for some r < m and к < h < п. This corresponds 
to a decomposition of the system into a first subsystem of r equations, where only 
the variables X\ to Xh occur with non-zero coefficients and a second subsystem of 
m — r equations where only the variables Xk+i to Xn occur. The corresponding 
matrix decomposition is illustrated in Figure 9.1. We denote by A the r x h matrix 
of the first subsystem for the variables X\ to Xh and by В the (m — r) x (n — к) 
matrix of the second subsystem for the variables Xk+i to Xn. It is already clear that 
if we select the pivot elements for the variables X\ to Xk in the first subsystem, then 
pivoting does not change the second subsystem, and in the first subsystem only the 
coefficients of the variables Xi to Xh change. So the zero-elements outside the two 
subsystems are maintained. This is how fill-ins are controlled by a decomposition. 
к 
h 
m 
A 
: 
0 
0 
В 
Figure 9.1 
A first decomposition of a system of linear equations into two subsystems and 
the associated zero-pattern of the coefficients. 
If we want to eliminate the variables X\ to Xk, we have to distinguish two cases: 
1. First, assume that r < к, i.e. there are less equations than variables to be 
eliminated. Then, we may eliminate at most r variables. Let us eliminate the 
variables in the order of their numbering Х\,...,ХГ. 
Then, it either turns out 
that the first subsystem has no solution, which implies that the whole system 
has no solution, or Χχ,..., 
Xr are at the end of the elimination process linearly 
expressed by the remaining variables Xr+\,..., 
Xh of the subsystem. We have 
for i = 1,... ,r, 
Xi 
— 
ci + Citr+iXr+i 
+ 
h Ci^kXk + Ci,fc+lA"fc+l + · · · CithXh-

326 
SPARSE MATRIX TECHNIQUES 
Here, the first variables Xr+i 
to Xk can freely be chosen because they do not 
appear in the remaining second part of the system, whereas the last variables 
Xk+\ to Xh are determined by the second part of the system which can be 
solved independently of the first part. 
2. In the second case, if к < r, the elimination of the variables Χι, Χ2, ■ ■ ■ may 
already show that the system has no solution. Otherwise, the variables X\ to 
Xk are eliminated from at least к equations of the first subsystem and there 
remain at most r — к equations containing only the variables Xk+i to Xh- This 
system is added to the second subsystem which still contains only the variables 
Xk+i to Xn. We thus obtain a new linear system for the variables Xk+i to Xn. 
The eliminated variables X\,..., 
Xk are linearly expressed by the variables 
Xk+i, · · ·, Xh- Once the second subsystem is solved for Xk+i, ■ ■ ■, Xn we 
can backward substitute the solution into the expressions for X\,..., 
Xk-
This describes a simple local computation scheme: the decomposition of the 
system can be represented by the two-node join tree of Figure 9.2. The variables of 
the first subsystem are covered by the label of the left-hand node and the variables of 
the second subsystem by the label of the right-hand node. The message sent from the 
left to the right node is obtained from eliminating all variables outside the intersection 
of the node labels. These are the variables Xi to Xk. The message is either empty or 
consists of the remaining system of the first subsystem after eliminating the variables 
X\ to Xk- If the elimination of the variables in the first subsystem already shows that 
the total system has no solution, then no message needs to be sent. Otherwise, the 
arriving message is simply added to the subsystem of the receiving node. Then, the 
variable elimination is continued in the new system until either a solution is found in 
the sense of the previous subsection, or it is seen that no solution exists. In the first 
case, assume that the variables Xk+i to Xi are expressed by the remaining variables 
Xi+\,..., 
Xn. Note that since we did not make any assumption about the rank of the 
matrix, we cannot be sure that all this holds for all variables Xk+i to Xh, but only 
for some / < h. Then, the expression of the variables Xk+\ to Xi may be backward 
substituted into the expressions of the variables Χχ to Xr if r < к, or to Xk obtained 
in the process of variable elimination in the first subsystem. 
Figure 9.2 The join tree corresponding to the decomposition of Figure 9.1. 
Let us point out more explicitly how the above scheme is connected to the local 
computation framework. We know from Section 7.2 that solution spaces of linear 
systems form a valuation algebra. Sets of equations provide a formal description of 
solution spaces and can therefore be considered as a representation of the latter. The 
domain of a set of equations consists of all variables that have at least one non-zero 

SYSTEMS OF LINEAR EQUATIONS 
3 2 7 
coefficient in this system. In the above description, we combined sets of equations 
by simple union. The original system therefore corresponds to the objective function 
and the knowledgebase factors are subsets of equations, whose union builds up the 
total system. This corresponds to the decomposition view presented in the introduc-
tion of this chapter. When producing such a decomposition from an existing system, 
we should keep the factor domains as small as possible. Otherwise, we will need 
large join tree nodes to cover these factors, which results in a bad complexity. In the 
case at hand, we can provide a factorization with minimum granularity where each 
factor contains exactly one equation. Then, the above procedure can be generalized 
as follows: we assume a covering join tree (V, E, A, D) for the knowledgebase where 
each node г G V contains a subsystem of linear equations whose variables are cov-
ered by the node label А(г). This join tree decomposition reflects a certain pattern 
of zeros in the original total system. Note also that these systems may be empty on 
certain nodes, which mirrors the assignment of identity elements in Section 3.6. If 
we then execute the collect algorithm, the messages correspond to the description 
above. At the end of the message-passing, the root node contains the total system 
from which all variables outside the root label have been eliminated. We determine 
the remaining variables from this system and, if it exists, build the total solution by 
backward substitution. The depicted variable elimination process clearly maintains 
the pattern of zeros represented by the join tree decomposition. Consequently, join 
tree decompositions control fill-ins in general sparse systems of linear equations. 
This process will be formulated more precisely in Section 9.1.3 for the important 
case of regular systems that always provide a unique solution. There, we also point 
out that backward substitution in fact corresponds to the construction of solution 
configurations according to Section 8.2.1, see also Exercise H.l. 
To identify the complexity of this approach, we consider a system that gives the 
most of work, i.e. a regular system of n variables and n equations. Eliminating 
one variable from this system takes a time complexity of ö(n2) and eliminating 
all variables is thus possible in C(n3). Backward substitution takes linear time for 
one variable and is repeated n times which result in 0{n2). Altogether, the time 
complexity of Gaussian elimination without taking care of fill-ins is ö(n 3). If local 
computation is applied, each join tree node contains a system of at most ω* + 1 
variables, where ω* denotes the treewidth of the inference problem derived from the 
total system. The effort of eliminating one variable is ϋ((ω* + l)2), and a complete 
run of the collect algorithm that eliminates all variables takes 
6>(η·(ω* + 1)2). 
(9.3) 
Backward substitution is performed n times with the equations that result from the 
variable elimination process and which contain at most ω* variables. We therefore 
obtain a time complexity of ο(ηω*) for the complete backward substitution process. 
Because each node stores a matrix whose domain is bounded by the treewidth, we 
obtain the same bound as (9.3) for the space complexity. If the treewidth is small with 
respect to n, then big savings may be expected. 

328 
SPARSE MATRIX TECHNIQUES 
9.1.3 Regular Systems 
In this section we examine more closely and more precisely the local computation 
scheme and the related structure of fill-ins for regular systems AX = b, where A is 
a regular nx n matrix. This assumption means that there is a unique solution to this 
system. In particular, we look at the fusion algorithm of Section 3.2.1 for computing 
the solution to this system. We choose an arbitrary elimination sequence for the n 
variables. By renumbering the variables and applying corresponding permutations 
of the columns of the matrix A, we may without loss of generality assume that the 
elimination sequence is (Xi, X2, · · · > Xn)· Further, by a well-known theorem of lin-
ear numerical analysis, it is also always possibleto permute the rows of the matrix A 
such that in the sequence of the elimination of the variables X\,..., 
Xn all diagonal 
elements are different form zero (Schwarz, 1997). Again, without loss of generality, 
we may assume that the rows of A are already in the required order. This means that 
the following process of variable elimination works fine. 
In the first step variable X\ is eliminated. Since by assumption 01Д ψ 0 we may 
solve the first equation for X\ 
X\ 
= ci - 
yji,iXi, 
i=2 
where 
ci = 
and 
nti 
= —^ 
(9.4) 
αι,ι 
αι,ι 
for г = 2,..., п. Using this linear expression, we replace the variable X\ in the 
remaining equations to obtain 
7 _ (Oj,i — lj,lül,i) Xi 
— bj — lj,lbi 
г=2 
for j = 2,..., n with 
Let us define 
1 
_ 
ai+ 
чл 
— 
— ■ 
αι,ι 
(9.5) 
ajJ 
= «j,i - ij.ioi.i 
and 
bj 
— 
bj-lj^ibi 
for j , г = 2,..., п. Then, the new system can be written as 
5>#* = ь 
(i) 
3 ' 

SYSTEMS OF LINEAR EQUATIONS 
329 
By assumption α ^ is different from zero, variable X2 can be eliminated by solving 
the first equation of the new system and so forth. In the fc-th step of this elimination 
process, к = 1,..., n, we similarly have 
Xk 
Ck 
7 
J 
Tk,iXi-i 
i=k+l 
where 
Cfc 
J_k 
Jfc-i) 
лк,к 
and 
rkti 
for г = к + 1,..., п. Further, with 
Лк) _ 
ifc-1) ~ ЧМак i 
and 
b (к) 
Ak-i) 
lk,i 
lk,k 
^fc-D 
for j , г = fc + 1,..., n, where 
4,k 
(fc-1) 
3fc,fc 
we get the system 
lj,kbk 
(fe-l) 
(9.6) 
(9.7) 
(9.8) 
(9.9) 
Σ$* 
i=fc+l 
In this process we define a (0) 
'hi 
a,jti. For к 
a(n-1]X 
Uk). 
n — 1 it remains a simple system 
6 ( » - i ) 
with one unknown that can easily be solved. By backward substitution for к = 
n — 2,..., 1, using equation (9.6), we obtain the solution of the regular system. 
In order to study the control of fill-ins by local computation, we consider the join 
tree induced by the above execution of the fusion algorithm as described in Section 
3.2.2. To each eliminated variable Xi corresponds a node г € У in the join tree 
(V, E, X, D) with a certain label А(г) G D = V({1,..., 
n}). Let us now interpret 
the fusion algorithm as a message-passing scheme in this join tree according to 
Section 3.5. To start with, consider node 1 where the variable X\ is eliminated. Let 
c(l) 
= 
{ j : flj.i ^ 0} 
define the index set of all equations which contain the variable Xx. Only these 
equations are changed when variable X\ is eliminated. Further, let 
λ(1) 
{г : а,·,* ф 0 for some j G e(l)} 

330 
SPARSE MATRIX TECHNIQUES 
be the set of variables that occur in the subset of equations with indices in e(l). 
Note that the first equation is in e(l), since by assumption <цд ψ 0. Hence, also 
the variable X\ is in the label A(l) of node 1. When we now look at the elimination 
process of variable X\ according to the equations (9.4) and (9.5) above, then we 
remark that riyi = 0 for г £ A(l) and ^д = 0 for j ^ e(l) and also 
a $ = а™ш1Ь?) 
= fcf forj^e(l). 
This shows that zeros in the first row and the first column of A are maintained when 
eliminating variable X\. Further, it exhibits the locality of the process by showing 
that only equations which contain variable X\ will change. The message sent to the 
child node ch{\) of node 1 in the join tree is determined by the coefficients 
„(1) _ 
„(0) 
; 
„(0) 
. 
,(1) _ ,(0) 
, ,(0) 
aj,i 
- 
aj,i 
_ / i . i a i , i 
a n d 
bj 
- 
bj 
~ЧЛЬ1 
for j e e(l) — {1} and г & X(l) — {1}. These coefficients specify the new system of 
equations after elimination of variable X\. This process is repeated for variables Xk 
and nodes к for к — 1,..., n — 1. For node к of the join tree, where the variable Xk 
is eliminated, we define as above 
<k) = 
{j-.α^φθ} 
for the equations containing variable Xk after к — 1 elimination steps and 
\{k) 
= {i : af~x) ф 0 for some j e e(k)} 
for the variables contained in these equations. Again, by assumption, arkk / 0 . 
Hence, the fc-th equation belongs to e{k) and the variable Xk belongs to X(k). So, 
we may eliminate variable Xk from the fc-th equation. Similar to the first step, we 
remark that Гк,% = 0 for г ^ X(k) and ljyk = 0 for j ^ e(k) and also 
a-J = a,j~ and Щ =Щ~ for j £ e(k) and i = 
k,...,n. 
The message sent from node к to its child ch(k) is given by the coefficients 
(fe) 
(fc-l) 
, 
(fe-l) 
, 
,(k) 
.(fe-1) 
7 
Jfe-l) 
a)J = a),i - h.k4,i 
a n d b) =b) 
h,kbk 
for j G e(k) — {к} and г G X(k) — {к}. The process stops at node n. This discussion 
permits to clarify how the zero-pattern of the matrix A is controlled and maintained 
by this fusion algorithm. 
Lemma 9.1 The k-th equation of the system AX = b is covered by some node г < к 
of the join tree (V, E, A, D). More precisely, there exists an index г < к such that 
{h:ak,h^0} 
Ç X(i). 
Proof: Let i be the least index of the variables occurring in the fc-the equation. 
Since the k-th equation contains no variables with indices h < г, we conclude that 

SYSTEMS OF LINEAR EQUATIONS 
331 
к $_ e(h) for h < г and hence ak ; — ak,i for I = 1,..., n and ak t = йк,г ф 0. So, 
the fc-th equation belongs to e(i) and is therefore covered by X(i). 
m 
This result allows us to assign each equation к to some node i < к of the join 
tree, and we have ak,h = 0 for h ф Х(г). This zero-pattern is maintained through the 
variable elimination process due to the remarks of the analysis above, i.e. ak h — 0 
for h £ \{i) and / < i. 
To complete the description of the fusion algorithm in the case of a regular system 
of linear equations, we summarize it as a message-passing scheme: The system of 
equations assigned to the node к of the join tree is denoted by ψ\ for к = 1,..., п. 
Some of these systems may be empty. A system фк determines the affine space фк of 
its solutions. If the system is empty, the affine space consists of all possible vectors. 
In other words, фк is then the neutral valuation in the information algebra of affine 
spaces. The problem of finding the solution to the system AX = b can then be 
written for к = 1,..., n as the inference problem 
фЦХк) 
= 
( ^ ® . . . ® ^ ) · « * * } . 
Given the uniqueness of the solution, each such projection is just a number. 
Now, the fusion algorithm is described in terms of the information elements -фк as 
follows: at step 1, node 1 sends the message 
μι-кла) = V>rXl 
to its child node ch(l). This is done by transforming the systems of equations ψχ 
according to the above procedure, i.e. by solving the first equation with respect to 
X\ and replacing the variable Xi in the remaining equations of ψι. The resulting 
system is denoted by fii^ch(i) ar>d represents the message /x1_K:/l(i). The message 
is combined to the content of node c/i(l) to get 
^cft(l) 
= 
^С/»(1)®М1->сМ1)· 
This new system is simply represented by 
$fc(l) 
= 
^ch(l) U£l-«A(l)· 
More generally, let ψ\ 
be the information element on node г before step к of the 
fusion algorithm. We therefore have ψ\ ' — φί. The associated system of equations 
is denoted by ·ψ\ '. At step k, node к sends the message 
to node ch(k) by solving the first equation of the system щ 
with respect to Xk 
and replacing the variable Xk in the remaining equations, which gives the system 

332 
SPARSE MATRIX TECHNIQUES 
Afc->ch(fc) representing the message. This message is added to the system in the child 
node ch(k) to yield 
7(fe+i) 
r(fe) , , -
All other nodes remain unchanged, i.e. щ + = щ 
for г ф ch(k). This process 
is repeated for к = 1,..., n — 1. At the end, the root node n contains the valuation 
φΗχη] which is represented by the system 
a^Xn 
= b^\ 
(9.10) 
This is the end of the fusion algorithm interpreted as message-passing scheme, or 
equivalently, of the collect algorithm as described in Section 3.8, executed on the 
particular join tree that is induced by the fusion algorithm. 
Since we are dealing with an idempotent valuation algebra, we can use the cor-
responding architecture from Section 4.5 for the distribute phase to compute the 
complete solution φ^χ*} for all г = 1,..., n. We first transform equation (9.10) on 
the node n into the equivalent form 
Xn 
= —
, 
(9.11) 
from which we determine the solution value 
hi»-1) 
( n - l ) · 
This in fact defines the single-point affine space 0-И·*"} = {xn}- Observe now that if 
n —1 is a neighbor to node n such that c/i(n— 1) = n, it follows by the construction of 
the join tree from the fusion algorithm that λ(η — 1) Π λ(η) = λ(η - 1 ) - {Χη-ι} = 
{Xn}- So, the message sent from node n to node n — 1 in the distribute phase is 
φΙλ(η-1)ηλ(η) 
= 
ф\{Хп}ш 
This means that equation (9.11 ) is added to the system ψ^-ι 
containing the equation 
-^η-ι 
= cn-i - rn-itnXn, 
(9.12) 
besides possibly a second system that represents the message from node n — 1 to n in 
the collect phase. This second system is redundant with the equation (9.11) and can 
be eliminated. Finally, the solution xn from equation (9.11) can be introduced into 
(9.12) to yield the solution value for Xn-i determining the affine space 
ф^^·1-1^. 
In the general step of the distribute phase, node к receives a message 
ф1\(к)п\(сНк)) 
= 
ф1Х(к)-{к} 

SYSTEMS OF LINEAR EQUATIONS 
3 3 3 
from its child node ch(k). By the induction assumption this message is a single-point 
affine space given by the solution values Xi for i e X(k) — {k}. The system of 
equations on node к contains the equation 
Xk = Cfc - 
2 J 
Tk,iXi. 
ie\(k)-{k} 
The solution values contained in the message are introduced into this equation to get 
the solution value for Xk, 
Xk 
C-k 
/ 
J 
Tk,i%i' 
ie\(k)-{k} 
So, by induction, distribute corresponds to the construction of the solution values of 
the variables Xk on the nodes к of the join tree by backward substitution. This is 
in fact a consequence of the theory of solution construction developed in Chapter 8 
applied to context valuation algebras. In Exercise H.4 we proposed to define solution 
sets in context valuation algebras by their associated set of models. The computa-
tions in the relational algebra of solution sets performed by the solution construction 
algorithms of Section 8.2 therefore correspond to computations in the context valu-
ation algebra itself. Hence, the above execution of the idempotent distribute phase 
coincides with the application of the generic solution construction procedure from 
Section 8.2.3. We continue this discussion in Section 9.2 and Section 9.3.2, where 
other valuation algebra are studied that do not belong to the class of context valuation 
algebras. There, the solution construction procedures will be applied explicitly. 
This completes the picture of the solution process as a message-passing procedure 
on the join tree induced by the fusion algorithm. In particular, the discussion shows 
that locality as represented by the join tree is also maintained during the distribute 
phase. It is important to understand that a similar process can be executed on an 
arbitrary join tree, where Х(к) П X(ch(k)) = X(k) — {Xk} does not necessarily 
hold. Here, we have chosen the very particular join tree of the fusion algorithm for 
illustration purposes, i.e. exactly one variable is eliminated from the equation system 
between two neighboring join tree nodes. It follows a numerical example: 
Example 9.1 Consider a regular 4 x 4 matrix A and a corresponding vector b, 
2 
3 
4 
6 
1 
2 
0 
0 
1 
1 
2 
0 
0 
1 
1 
2 
0 
0 
1 
1 
and b 
We solve the linear system AX = b by elimination of the variables X\, Xi and 
Хз in this order. Eliminating Xi yields the following new matrix A^ and the new 
right-hand side vector b' 1': 
A(D 
- 1 1 0 
2 
1 1 
0 
2 1 
and b ( 1 ) 
4 
6 

334 
SPARSE MATRIX TECHNIQUES 
We remember also the non-zero r- and l-elements 7*1,2 = 1 and /2,1 = 2. Next, we 
eliminate X2 and obtain 
A ( 2 ) 
3 1 
2 1 
and b ( 2 ) 
In addition we have Г2,з — — 1 and /3,2 = —2. /i remains to eliminate X3 giving 
A^ 
= [ 1/3 ] 
and b<2) = 
[ 14/3 ] 
with /43 = I and Г34 = |. We can now solve the equation 
:XA 
14 
T 
for X4 and obtain the solution value X4 = 14. 
Figure 9.3 shows the join tree induced by the fusion algorithm described above. 
We may assign the first two equations to the left-most node and the last two equations 
to the second node from the left. Eliminating the variable X\ generates the equation 
-Xo + X3 
-1 
as the message sent to the next node. There, this equation is added to the two equations 
already hold by this node. This yields the linear system defined by the matrix A ^ 
and the vector Ъ^\ Next, eliminating the variable X2 generates the system described 
by the matrix A^ 
and the vector h^ 
as the message to be sent from the second 
to the third node. Since the system contained in the third node is empty, the node 
content becomes equal to the received message. Finally, eliminating the variable X3 
generates the equation (l/3)Xi = 14/3 as the message sent to the fourth node. 
Again, the content of node 4 is an empty system such that the received message 
directly becomes the new node content. 
Figure 9.3 The join tree induced by the variable elimination in Example 9.1. 
Now, the distribute phase starts by solving the equation of node 4, which gives 
us the solution value x\ = 14 as remarked above. Node 4 sends this solution value 
back to node 3, where it can substitute the variable X4. Using the first equation of 
the system defined by A'2) and Ъ^ on this node, we can solve for X3 to obtain 
xz = 2/3 — (l/3)x4 = —4. This is the value sent back to node 2, where it can be 
used to determine the solution value of Xi using the first equation of the system on 
this node, £2 = 1 + X3 — — 3. Finally, this values is sent back to node 1, where we 
get from the first equation x\ =2 — xi = 5. This constitutes the totality of the unique 

SYSTEMS OF LINEAR EQUATIONS 
3 3 5 
solution of the system AX = b. Note that in the whole process, at each step only 
the variables belonging to the label of the current join tree node are involved. This 
shows the locality of the solution process. 
In the next section we study a variant of this process, which has advantages if the 
system of equations AX = b must be solved for different vectors b. By memorizing 
certain intermediate factors derived from the matrix A in the first run of the above 
algorithm, we obtain a factorization of A into an upper and a lower triangular matrix. 
If later another equation system with the same matrix must be solved, only the 
computations with respect to the new vector must be repeated. This considerably 
simplifies the computations of the involved local computation messages. 
9.1.4 
LDR-Decomposition 
Let us review the process of Gaussian variable elimination described in Section 9.1.3. 
From the recursive definition in equation (9.8) we derive 
7(fc-i) 
lk,i 
(0) 
,(!) 
a-k,i - h,ia\J 
- Zfc,2<i2,i 
ik,k-\&k-\ 
(fc-2) 
= 
Ofc.i — h,lVl,i 
— ik,2^2,i — · · · — 
lk,k-lVk-l,i 
(fc-i) 
for 1 < к < i, if we define Vk,i = a,k i 
. This implies that 
fc-l 
ak,i 
= 
/Jk,jVj,i 
+ «fc,i-
If we set Ik,к = 1> we may also write 
®к,г — 
/ , 
k,j^j,i· 
3 = 1 
This can also be expressed by the matrix product 
A 
= 
LV, 
where L is a lower triangular und R an upper triangular matrix, 
and 
1 
hi 
/3,1 
ln,l 
«1,1 
0 
0 
0 
0 
1 
h,2 
ln,2 
«1,2 
«2,2 
0 
0 
0 
0 
. 
1 
'n,3 
· 
«1,3 
·· 
«2,3 
· · 
«3,3 
· · 
0 
.. 
.. 
0 
.. 
0 
.. 
0 
.. 
1 
• 
«1,71 
• 
«2,71 
• 
«3,71 
^n.n 

336 
SPARSE MATRIX TECHNIQUES 
This representation is called LV-decomposition of the matrix A. We remark that the 
Vk,i used here are closely related to the elements Гк,% in equation (9.7). We have 
Гк,; 
Jfe-i) 
lk,i 
Jfe-i) 
lk,k 
Vk,i 
Vk,k' 
Therefore, if we define the diagonal matrix D with elements dk,k = l/ufc,fc o n the 
diagonal and zeros otherwise, and further the upper triangular matrix 
R 
1 
0 
0 
0 
r l , 2 
1 
0 
0 
r l , 3 
· 
^2,3 
■ 
1 
0 
. 
• 
rhn 
■ 
Г2,п 
■ 
7"3i„ 
1 
then we have the representation 
LDR. 
This is called LOR-decomposition of the matrix A. 
We may neglect the right-hand side of the system AX = b in the fusion algorithm 
described in the previous Section 9.1.3 and only compute the elements lk,i, rk,i and 
(k — l) 
ak k ' that depend on the matrix A. If we relate these elements to the join tree 
induced by the fusion algorithm, we remind that r,^ = 0 for i £ X(j) and lk,j = 0 
for j $L e(j). Thus, the two matrices L and R maintain the zero-pattern described by 
the join tree. The computation of the LDR-decomposition in the fusion algorithm 
can be interpreted as a compilation of the matrix A, which can then be used to solve 
AX = b for different vectors b. This will be shown next. 
Given some vector b and the LV- or LDR-decomposition of the matrix A, we 
solve the system AX = b by first solving LY = b, yielding the solution y, and then 
VX = DRX = y. This solution process is simple, since both matrices L and V 
are triangular: We first solve the system LY = b by executing the collect algorithm 
on the join tree. In fact, starting with node 1 we directly obtain the solution value 
2/i = 6i from the first equation Y\ = b\. We introduce this value into the equations 
j G e(l) — {1} and obtain the new system 
/ 
lj,iXi 
— 
t>j—ljt\b\. 
i€A(l) 
(,(!) 
Defining bj ' =bj — lj,ibi, the message sent to the child node ch(l) is the set 
{b^:jee(l)-{l}}. 

SYSTEMS OF LINEAR EQUATIONS 
3 3 7 
This defines the new lower triangular system after elimination of variable Υχ. In the 
(k — l) 
fc-th step, the equation for Yk on the node fc is Yjt = 6^ 
. The solution value 
Ук = bk~ 
is introduced into the equations j € е(к) — {к} to obtain 
V 
l Y 
- 
b(k~1] - I ■ Л(к~1} 
ieA(fc) 
So, the message sent to the child node ch(k) is the set 
{b$fc):jG£(fc)-{*}} 
with bj ' = bj 
' — lj,kbk 
.In this way, the solution yk for к — 1,..., n is 
obtained. It is important to remark that in the general case, the collect algorithm only 
computes the solution for the variable Yn in the root node, and the values for the 
remaining variables are found by backward substitution in the distribute phase. In the 
case at hand, we are dealing with triangular systems and therefore obtain all solutions 
у of the system LY = b already from the collect algorithm. 
In a second step, we execute the distribute algorithm to solve the system VX = 
DRX = y, respectively RX = D _ 1y. The root node n contains the equation 
-Xn = Vn/dn,n from which we determine the solution value xn = yn/dn,n- 
This 
value is sent to node n — 1 and replaces there the variable Xn in the equations from 
RX = D~*y that are contained in the node n — 1. More precisely, we compute for 
Vj 
for j € λ(η — 1), according to equation (9.12). In the general case, node к receives 
a message from node ch(k), which consist of 
Уз 
Xj 
= 
-\ 
rjtCh(k)Xch(k-). 
for j € X(k). Altogether, this permits to solve the triangular system of equations on 
the node к for к = n,..., 
1. 
We again point out that the solution of AX = b for an arbitrary, regular matrix 
A required in the previous section a complete run of the collect and distribute algo-
rithm. But if we dispose of an LDR-decomposition of A, we may solve the system 
LY = b with a single run of the collect algorithm, because the involved matrix L 
is lower triangular. Likewise, we directly obtain the solution to the system D R = y 
by a single run of the distribute algorithm, because the matrix D R is upper trian-
gular. This symmetry between the application of collect and distribute is worth noting. 
We apply this compilation approach to Example 9.1 above: 

338 
SPARSE MATRIX TECHNIQUES 
Example 9.2 In Example 9.1 we determined the elements of the matrices L and R, 
L = 
1 
2 
0 
0 
0 
1 
- 2 
0 
0 
0 
1 
2/3 
0 
0 
0 
1 
and 
R 
1 
0 
0 
0 
1 
1 
0 
0 
0 
- 1 
1 
0 
0 
0 
1/3 
1 
We remark how the zero-pattern controlled by the join tree of Figure 9.3 is still 
reflected, both in the matrix L as well as in the matrix R. With the diagonal matrix 
below we obtain the LiDTl-decomposition A = LDR: 
D 
1 0 
0 
0 
0 - 1 0 0 
0 
0 
3 
0 
0 
0 
0 
1/3 
In the first step, we solve the system LY = b, where b is given in Example 9.3. 
We again assign the first two equations of this system to the first join tree node and 
the last two equations to the second node to control fill-ins. In the first node, we 
obtain j / i = 2 and introduce this value into the second equation, whose right-hand 
side becomes b. (i) 
the solution value г/2 = Щ, 
(i) 
(2) 
message b\ 
h,iyi = —1· This value is sent to node 2, where now 
-1 is obtained. Also, node 2 computes the message 
2 and sends it to node 3. Node 3 computes г/з = 2 and sends the 
' ^4,з2/з = 14/3 to node 4, where we finally obtain г/4 = 14/3. 
The solution o / R X = D _ 1 y by the distribute algorithm, using the values у 
computed in the collect phase, is initiated by the root node 4. We obtain on node 4 
the value X4 = 2/4/(^4,4 = 14. This value is sent to node 3, where we obtain X3 = 
2/з/^з,з ~~ 7*3,4^4 = —4 and send this message to node 2. Continuing this process, 
we obtain x^ = yil^fi 
— ^"2,3^3 = —3 in node 2 and x\ = yi/d\^ — r1:2^2 = 5 
in node 1. This terminates the distribute phase. We note that on all nodes j only 
variables from the node label \(j) are involved. 
This ends our first discussion of regular systems. An important special case of 
regular systems is provided by symmetric, positive definite systems. They allow to 
refine the present approach as shown in the following section. 
9.2 SYMMETRIC, POSITIVE DEFINITE MATRICES 
The method of LDR-decomposition presented in the previous section becomes 
especially interesting in the case of linear systems with symmetric, positive definite 
matrices. Such systems arise frequently in electrical network analysis, analysis of 
structural systems and hydraulic problems. Further, they arise in the classical method 
of least squares, which will be discussed in Section 9.2.5 as an application of the 
results developed here. In all these cases, the exploitation of sparsity is important. 

SYMMETRIC, POSITIVE DEFINITE MATRICES 
3 3 9 
However, it turns out that in the case of systems of linear equations with symmetric, 
positive definite matrices, the underlying algebra of affine solution spaces is of less 
interest and can be replaced by another valuation algebra. We first introduce this 
algebra, which interestingly is closely related to the valuation algebra of Gaussian 
potentials introduced in Instance 1.6. The reason for this is elucidated in Section 
9.2.5, where systems of linear equations with symmetric, positive definite matrices 
are related to statistical problems. 
9.2.1 Valuation Algebra of Symmetric Systems 
To put the discussion into the general framework of valuation algebras, we consider 
variables X\,..., 
Xn together with the associated index setr = { 1 , . . . , n}. Vectors 
of variables X, as well as matrices A and vectors b then refer to certain subsets 
s Ç r. For instance, a system AX = b is said to be an s-system, if X is the variable 
vector whose components Xi have indices in s Ç r, A is a symmetric, positive 
definite s x s matrix and b is an s-vector. Such systems are fully determined by the 
pair (A, b) where A is an s x s matrix, b an s-vector for some s Ç r. These are 
the elements of the valuation algebra to be identified. Let Ф8 be the set of all pairs 
(A, b) relative to some subset s Ç r and define 
Ф = и ф -
sCr 
By convention, we define for the empty set Φβ = {(о, о)}. The label of a pair (A, b) 
is defined as d(A, b) = s, if (A, b) € Φ,. 
In order to define the operations of combination and projection for elements in Φ, 
we first consider variable elimination in the system AX = b. Suppose (A, b) e Φ5, 
i.e. the system is over variables in s, and assume t С s. We want to eliminate the 
variables in the index set s — t. For this purpose, we decompose A as follows: 
Д4-5—£,s — t 
Ajs-ΐ,ί 
A i t , i - t 
Alt,* 
The system AX = b can then be written as 
A | s - i , i - f v | s - i 
_i 
д 4-s — ί , ί ν ^ ί 
— 
V|4-s — t 
Α ΐ ί , ί - Ι Υ - Ι ί - ί 
_ι 
A-U.tY-l-t 
— 
Vj-U 
Solving the first part for X-i-s-t and substituting this in the second part leads to the 
reduced system for X^* 
(A-LM _ j^lt,s-t/j^is-t,s-t\-lj^ls-t,t\-^it 
_ 
folt_£it,s-t/j^ls-t,s-t\-ly)ls-t 
Here we use the fact that any diagonal submatrix of a symmetric, positive definite 
matrix is regular. We define 
A^' 
= 
A"*-''* — A^i's_tiA~''s_i's_i)_1A"''s_i'< 
ЪЫ ^ 
ъа _Alt,s-t(Ais-t,s-tj-ibU-t 
( 9 Л З ) 

340 
SPARSE MATRIX TECHNIQUES 
and remark that A*' is still symmetric, positive definite. 
Theorem 9.1 /fx is the unique solution of AX. = b, then x^* is the unique solution 
to A^X^ 
= b^K Conversely, if y is the unique solution o/A^'Y = b^' and 
χ 
_ 
a l s - i . s - t ^ - l j j i s - t _ 
/j^ls-t,s-t\-lj^ls-t,t 
then (x, y) is the unique solution of AX = b. 
Proof: The proof is straightforward, using the definitions of A^' and b^'. Since x 
is a solution of AX = b, it holds that 
д \.s-~ i,s—t__4-s—t 
i 
A | S - t,t„4-i 
— b^s—* 
^4.t,s-ixis-t 
_|_ 
j^it,t^it 
_ 
jjli 
hence 
l\it,t 
_ j^lt,s-tij^is-t,s-t\-l 
j^is-t,t\ 
J,t _ i-it_ 
\lt,s-tr 
\ls-t,s-t\-ly^ls-t 
This proves the first part of the theorem. The second part follows from the first 
subsystem above, if x^ s _ i is replaced by x and x^' by y and if it is multiplied on 
both sides by (A^-'·"-')- 1. 
■ 
This theorem justifies to define the operation of projection for elements in Φ by 
(A^) 4·' 
= 
(A*',b^), 
(9.14) 
for t Ç d(A, b). It is well-known in matrix algebra that A^f may also be written as 
A*' 
= 
((A- 1)^·«)" 1. 
A proof can for example be found in (Harville, 1997). 
Next, consider two systems AiXi = bi and A2X2 = b 2. What is a sensible way 
to combine these two systems into a new system? Clearly, taking simply the union 
of both systems as we did so far with systems of linear equations makes no sense, 
because the matrix of the combined system will no more be symmetric. Moreover, 
the combined system will most of the time have no solution anymore. We propose 
alternatively to add the matrices and the right-hand vectors component-wise. More 
precisely, we define for d(Ai, bi) = s and d(A2, b2) = ί, 
(A 1,b 1)^(A 2,b 2) 
= 
(AÎsUÎ + A^ u t,bî s U Î + b£sUt). 
(9.15) 
At this point, the proposed definition is rather ad hoc. It will be justified by the 
success of local computation and even more strongly by a semantical interpretation 
given in the following Section 9.2.5. For the time being, the question is whether 
(Φ, D) together with the operations of labeling, projection and combination satisfies 
the axioms of a valuation algebra. Instead of verifying the axioms directly, we 

SYMMETRIC, POSITIVE DEFINITE MATRICES 
341 
consider the mapping (A, b) ь-»· (A, u), where u = A _ 1b. Let Ф be the set of 
all pairs (A,u), where A is a symmetric, positive definite s x s matrix and u 
an s-vector. This mapping is a surjection between Ф and Ф. We next look at how 
combination and projection in Ф is mapped to Ф. Let (Αχ, bi) and (A2, b2) belong 
to Ф with labels s and t, respectively. Then, by the definition of combination above, 
( A b b i ) ® (A2,b2) = (A,b) with 
A 
= 
AÎ s U i + A^sUi. 
Hence, (Ai,bi) <8> (A 2,b 2) maps to (A,u) with 
u 
= 
A- Xb = 
A - ^ b ^ + b ^ ) 
= 
A - 1 ( ( A l U l ) ^ u t + (A 2u 2) t 5 U t) = A" 1 ( A Î s U X s U t + A| e U tu| a U t), 
whereux = Aj~1biandui = A2~1b2. Consequently, a combination of two elements 
in Ф is mapped as follows to an element in Ф 
(Ai,bi)<8)(A2,b2) 
м· 
(A,u) 
with 
A 
= 
Aî s u i + A| s u i 
and 
u 
= 
A - ^ A Î ^ u î ^ + A ^ u ^ ' ) . 
Turning to projection, let (A, b) be an element in Ф with domain s, which is 
mapped to (A,u) with u = A _ 1 b . If t Ç s, then (А,Ь)+* represents the system 
Α^'Χ^* = b^' with the unique solution u^ as a consequence of Theorem 9.1. This 
means that (А, Ъ)^г maps to 
(Α^,(Α^)~ν<) = (A^V) 
where 
A** 
= 
((A- 1)*·*)" 1. 
We thus have the following projection rule in Ф: 
(A,u)^ 
= 
( ( ( A - 1 ) ^ · ' ) " 1 , ^ ' ) . 
We observe that combination and projection in Ф correspond exactly to the operations 
of Gaussian potentials introduced in Instance 1.6 and we therefore know that (Ф, D) 
forms a valuation algebra. Moreover, due to the introduced mapping, (Ф, D) and 

342 
SPARSE MATRIX TECHNIQUES 
(Φ, D) are isomorphic which implies that (Φ, D) is a valuation algebra. We proved 
the following theorem: 
Theorem 9.2 The algebra of linear systems with symmetric, positive definite matri-
ces is isomorphic to the valuation algebra of Gaussian potentials. 
Let us have a closer look at the combination of (Ai,bi) and (A2,b2) with 
cî(Ai,bi) = s and d(A2,b2) = t. If X denotes an (s U t) variable vector, these 
valuations represent the systems ΑχΧ-1-5 = bi and А2Х·^ = b 2 with symmetric, 
positive definite matrices. We consider the system AX = b which is represented by 
(Ai,bi) ® (A2,b2) suchthat 
Ai 
taut i 
*tsUi 
and 
bt sut + btsut_ 
In order to compute the solution to the system AX = b, we proceed in two steps by 
first projecting to t, i.e. eliminating the variables X ^ - ' , and then solving the system 
A^'X^' = b^'. This is justified by Theorem 9.1. We decompose the matrices and 
vectors according to the sets s — t, s Π t and t — s and write the system as 
д is — t,s — t 
д 
lsilt,S-t 
0 
* 
is—tysnt 
A isnt,snt , «4.snt,«nt 
A it — S,sC\t 
Ά2 
0 
A \.Snt,t — S 
Ά2 
A \-t — S,t — S 
Ά2 
J 
= 
bfs b 
Hi 
b 
v | s - t 
v i s n t 
v i t —s 
is-t 
+ b^ s n t 
it-s 
2 
Eliminating the variables X^s ' means to solve the first subsystem for the variables 
X^ s _ i and to substitute the solution into the other equations, which gives the system 
■ ΑψβΠί,βΠί 
, д isDi.snt 
A 
it—s,snt 
Λ2 
A isnt,t 
— S ' 
Ά2 
A it,t — S 
Ά2 
X i s n t 
X J . t - s 
bj 
+щ 
Ь2 
J 
Note that this system is represented by either 
((A 1,b 1)®(A 2,b 2)) i t 
or 
( А ь Ь О ^ ' 
(A 2,b 2 
(9.16) 
This illustrates the combination axiom and shows how local computation can be 
applied to symmetric, positive definite systems. If the second representation is used, 
then the variables Х+а~г in the first system are eliminated and the result is combined 
to the second system. It would be reasonable to apply the local computation methods 
of Section 9.1.2 and 9.1.3 for the variable elimination process. This however is 
not possible because idempotency does not hold anymore in this algebra. Symmetric, 
positive definite systems only form a valuation algebra but not an information algebra. 
For local computation, this means that we must apply the Shenoy-Shafer architecture. 
However, Theorem 9.1 offers an alternative possibility based on solution construction 
which comes near to idempotency. This is exploited in the following section, and 
variable elimination and LDR-decomposition with symmetric systems is examined 
in Section 9.2.3. 

SYMMETRIC, POSITIVE DEFINITE MATRICES 
343 
9.2.2 Solving Symmetric Systems 
We now return to local computation as a means to control fill-ins. Note that zero-
patterns in a symmetric matrix are symmetric too. Consequently, symmetric decom-
positions of the matrix A as shown in Figure 9.1 are not possible. This implies that 
the underlying valuation algebra of affine spaces introduced in Section 7.2 and used 
so far is no longer of interest. In other words, combination as intersection or join of 
affine spaces does not really reflect the natural decomposition of symmetric systems. 
Instead, this is achieved by adding symmetric, positive definite matrices as showed in 
the foregoing section. The corresponding algebra will now be used to control fill-ins. 
Consider a join tree (V, E, X, D) with the labeling function λ : V -> D = V(r) 
for r — {1,..., n}. We number the nodes from 1 to m = \V\ and assume that any 
node г 6 V in this join tree covers a pair <fo = (Ai, hi) with ωι = d(4>i). They 
consist of a symmetric, positive definite ω* χ ω* matrix Aj and an Wj-vector bj. We 
further assume that ω* С А(г) and ω\ U... Uu>m = r. In other words, фг are elements 
of the valuation algebra introduced in Section 9.2.1 and (V, E, A, D) is a covering 
join tree for the factorization 
ф = (A,b) = ф1®---®фт 
(9.17) 
according to Definition 3.8. As usual, the nodes of the join tree are numbered such 
that if node j is on the path form node г to node m, then i < j . We will see in Section 
9.2.4 how such decompositions can be produced, and it will be shown in 9.2.5 that 
factorizations of symmetric, positive definite systems may also occur naturally in 
certain applications. The objective function ф represents the symmetric, positive 
definite system AX = b, where 
A = AÎr + ... + A£ 
and 
ь = ьГ + ... + ь£. 
The join tree exhibits the sparsity of the matrix A. We have α ^ = 0, if j and h do 
not belong both to some ω*. Its zero-pattern is denoted by the set 
Z 
= 
{{зЛ)'■ there is no г = 1,... ,m, such that j , h € LOi}. 
(9.18) 
Given a factorized system ф = (A, b) = φγ ® ... ® фт and some covering join 
tree, we next focus on the solution process using local computation. As mentioned 
above, this algebra is fundamentally different from the algebra of affine spaces and 
does not directly yield solution sets. Instead, we aim at first executing a local com-
putation scheme and later build the solution x = A _ 1 b using a generic solution 
construction algorithm from Chapter 8. Hence, we start by introducing a suitable 
notion of configuration extension sets for symmetric, positive define systems, moti-
vated by Theorem 9.1: For a t-vector of real numbers x, ф = (A, b) and t Ç ά(φ) 

344 
SPARSE MATRIX TECHNIQUES 
we define 
W£(x) 
= 
|(A^-*> s- t)- 1b- |- e- t-(A- |-''- t ,' ,- t)- 1A- |- e- t l tx}. (9.19) 
For later reference we prove the following property: 
Lemma 9.2 Assume ф\ = (A, bi) and фг = (А, Ьг) with ά(φι) = ά(φ2) = s and 
tÇuÇs.If 
b is — t 
i_Xs — t 
1 
= 
b 2 
then it holds for all t-vectors x that 
/Voo/V By equation (9.19) and ф\и = ( A^u, bfu ) we have for г = 1,2 
W*fu(x) 
= 
| ( ( А ^ ) ^ - * ' " - * ) _ 1 ( Ь ^ ) ^ - Е -
f(A^u)"''u_t'u_t) 
1(A^ u)^ u _ i' txl. 
The statement of the lemma then follows directly from: 
\.u — t 
/ 
. 
\ \.u — t 
= ( bf )iU_t - (Vu's-u(A^-u's-u)-1bfs-u) 
Based on this lemma, we show that the definition of equation (9.19) complies with 
the general definition of configuration extension sets in Section 8.1. 
Theorem 9.3 Solution extension sets in valuation algebras of symmetric, positive 
definite systems satisfy the property of Definition 8.1, i.e. for all ф = (A, b) G Ф 
with t С и С s = ά(φ) and all t-vectors x we have 
Wj(x) 
= 
{(y,z):yeW^(x)andzeW%(y,x)}. 
Proof: If x is the partial solution to the system φ = (A, b), then the above statement 
follows directly from Theorem 9.1. In the following proof, we consider an arbitrary 
ί-vector x and determine a new system φ\ = (A, bi) to which x is the partial 
solution. The statement then holds for x with respect to this new system, and if φι 
is chosen in such a way that Lemma 9.2 applies, then the statement also holds for 

SYMMETRIC, POSITIVE DEFINITE MATRICES 
3 4 5 
x with respect to the original system φ. Hence, let x be an arbitrary i-vector. We 
consider the new system φ\ = (A, bi) with 
bls-t 
= 
hU-t 
and determine b f such that x is the solution to φ\ι = (A^*, bf*), i.e. 
Α ψ'χ 
= 
b f 
= 
b f 
- 
A ^ - ^ A ^
4 ' - ' ) -
1 ^ -
1 
and therefore 
b f 
= A ^ x + A ^ ' - ' t A ^ - 1 ' 5 - 1 ) 4 ^ 8 - ' . 
Since x is the solution to (A, bi)-^, it follows from Theorem 9.1 that (y, x) with 
is the solution to the system (A, bi). We thus have 
yew^(x) - Wj(x) 
as a consequence of Lemma 9.2. Then, since (y,x) is the solution to (A, bi), it 
follows from Theorem 9.1 that ^ " " ' , χ ) is the solution to (A, bi)■'-". Hence, 
у 1 и " ' ^ ; ; „ ( х ) 
= 
и ^ „ ( х ) 
by Lemma 9.2. Finally, since {у^и~г, х) is the solution to (A, bi)^" it again follows 
from Theorem 9.1 that (y^u_t, y^s~u, 
x) is the solution to (A, bi). We have 
y i s - „ e W „ i ( y 4 „ - t ) X ) 
= 
w „ ( y | „ - É j X ) _ 
Altogether, this show that for an arbitrary ί-vector x we have 
Wj(x) 
= 
{ ( y , z ) : y e W ^ „ ( x ) à n d z € W ^ ( y , x ) } . 
Next, we specialize general solution sets from Definition 8.2 to symmetric, positive 
definite systems. For ф = (A, b) G Ф and t = 0, it follows from equation (9.19) that 
4 = < ( o ) = {А- гЬ}. 
(9.20) 
This shows that Сф indeed corresponds to the singleton set of the unique solution 
x = A _ 1 b to the symmetric, positive definite system AX = b. We therefore 
conclude that the generic solution construction procedure of Section 8.2.1 could be 
applied to build Сф based on the results of a multi-query local computation scheme. 
However, the same is also possible using the results of a single-query architecture 
only. This follows by verifying the property of Theorem 8.3: 

346 
SPARSE MATRIX TECHNIQUES 
Lemma 9.3 Symmetric, positive definite systems satisfy the property that for all 
ψ ι , ψ ΐ ξ φ with d(4>i) = s, d(ip2) — t, s С и С sUt and u-vector x we have 
w&nVnt) = 
WI^2(K). 
Proof: We observe the following identities for ψ\ = (Ai, bi) and Φ2 = (A2, Ьг): 
/ » t i U t 
. л t s U i \ 4 . ( s U t ) - u , ( s U t ) - u 
_ 
д 4 ( s U t ) - u , ( s U t ) - u 
__ 
· 4 . t - u , t - u 
and similarly 
(bîsUt + b^ut)^Ui)-u = bf-u 
It then follows that 
W^(x) = {([AÎsUt + A^']^^*)-"·^«)—)-1^* + b^sUt]«sUi)-« 
- 
( ί Α Ϊ 6 ϋ ί + J^sUt]UsOt)-u,(sUt)~u\-l^txUt 
^tsUt|4.(sUt)—u,u x"l 
— //л-l·*-"'*-""i-iu-l-'-11 _ ( д-I-'-ii,t—«л —1 дit—и.« 4«ntl 
= 
W^ n t(x i u n i). 
■ 
Hence, given the results of a single-query local computation architecture, we may 
apply Algorithm 8.4 to build the unique solution x = A _ 1 b of a factorized system 
φ = (A, b) = 0i <8>... <8> </>m with <fo = (Ai; b») for г — 1,..., m. At this point, we 
should remember that the valuation algebra of symmetric, positive definite systems 
is isomorphic to the valuation algebra of Gaussian potentials and does not provide 
neutral elements; see Instance 3.5. Hence, the domain ω^ = ά(φί) of node i £ V in 
the join tree does not necessarily correspond to the node label X(i). We only have 
uji С А(г). Consequently, we must choose the generalized collect algorithm from 
Section 3.10 to explain the message-passing view of this local computation based 
solution. The run of this algorithm followed by the solution construction process is 
delineated next, using the notation of equation (3.42) from Section 3.10. 
At step г = 1,..., m — 1, node г sends the message 
/A(i) 
(i)\^i°nA(cfc(i)) 
to its child node ch(i), where the message is combined with the current node content 
\^-ch(i) 
' ^ch(i) ) 
= 
М»->с/>(») ® \^ch(i)^"ch(i)J 
' 
The domain of the node content updates to: 

SYMMETRIC, POSITIVE DEFINITE MATRICES 
3 4 7 
For all other nodes j φ ch(i) we set 
(АГ\ЬГ>) = ( Α > « ) and 
(i+1) 
(i) 
ω 
= ω· 
with 
(AW.bW) = 
( А , , Ь < ) 
and 
Ч
( 1 ) = ωζ 
for all г = 1,..., m. We now add an additional step that is not part of the general 
collect algorithm. When node г computes the message for its child node, it has to 
eliminate the variables 
uf] - \{ch(i)) 
= 
А(г) - X(ch(i)) 
(9.21) 
from its node content. This equality is due to Lemma 4.2. The eliminated variables 
satisfy the system 
xU(i)-A(c/i(i)) 
_ 
/"A(«)4-^(i)-A(c/i(i)),A(i)-À(c/i(i))\"1
b(i)4,A(i)-À(c/i(i)) 
_ 
iA(i) 
4.A(i)-A(c/i(i)),A(i)-A(ch(i))\ _ 1
 
( 9 
22) 
A(i)ia;< i )nÀ(c/ l(i)),a;< i )nÀ(c? l(i)) x i u ;( i) n À ( c f t ( i ) ) 
For later use in the solution construction phase, we store in node i the matrices on 
the right-hand side of the equation. This is the only modification with respect to the 
generalized collect algorithm. If we repeat this process up to г = m, we obtain on 
node m the pair 
as a consequence of Theorem 3.7. The content of node m at the end of the collect 
phase represents the symmetric system 
д (m)y4.A(m) 
_ 
. (m) 
Solving this system gives us the partial solution set c\ 
as a consequence of 
Lemma 9.20 and Lemma 8.1. We now follow the solution construction algorithm of 
Section 8.2.3 starting in the root node m. At step г = m — 1,..., 1 node г G V 
receives the partial solution set 
с 
rl\(i)n\(ch(i)) 
_ 
i 
i\(i)nX(ch(i)) 
|х4А(г)ПА(с/г(г))| 
from its child node ch(i). Using the matrices stored in (9.22), node г computes 
fx4.A(i)-A(cft(i))l 
= 
pyWinA(ch(i))^x4.A(»)nA(ch(i))j 
( 9 _ 2 3 ) 
(Ai ,Ь^ ) 

348 
SPARSE MATRIX TECHNIQUES 
to obtain the partial solution x-^W = (х4.А(г)пл(сЛ(г))х4.л(г)-л(сл.(г))) w i t h r e s p e c t 
to its proper node label. At the end of the solution construction process, each node 
г € X(i) contains the solution x to the system AX = b projected to its node label, and 
the complete solution x can simply be obtained by assembling these partial solutions. 
This shows how factorized, symmetric, positive definite systems are solved using 
local computation. The sparsity reflected by the join tree is maintained all the time. 
A very similar scheme for the computation of solutions in quasi-regular semiring 
systems will be presented in Section 9.3.2. Here, we next focus on the adaption of 
LDR-decomposition to symmetric, positive definite systems. 
9.2.3 
Symmetric Gaussian Elimination 
Consider again a system AX = b, where A is a symmetric, positive definite n x n 
matrix, and X and b are vectors with corresponding dimensions. Since positive 
definite systems are always regular, we could apply the local computation scheme 
based on the LDR-decomposition from Section 9.1.3, if we have to solve this system 
multiple times for different vectors b. However, here we are dealing with a different 
valuation algebra such that parts of this discussion must be revisited. Moreover, the 
approach presented in this section does not only exploit the sparsity of the system, 
but also benefits from the symmetry in the matrix A. Because this matrix is regular, 
we can use any variable elimination sequence in the Gaussian method to solve the 
system. As in Section 9.1.3, we renumber the variables such that they are eliminated 
in the order X\,..., 
Xn. We only need to permute the columns and the rows of the 
matrix A accordingly. By the symmetry of the matrix, the same permutation must 
be used for both columns and rows. In addition, it follows by symmetry that the 
LDR-decomposition satisfies R = L T such that 
A 
= 
LDL T. 
For positive definite matrices, D has positive diagonal entries. It also holds that 
A 
= 
GG T, 
where G = LD 1/ 2. This decomposition is due to Choleski (Forsythe & Moler, 1967). 
The symmetry of the matrix A can be exploited for the elimination of variables, 
where it is now sufficient to store the lower (or upper) half of the matrix A and all 
derived matrices. Then, equation (9.9) still applies: For j , г — к + 1,..., n we have 
a(fe_1) 
ak,k 
and according to equation (9.8) also 
(fc) 
(fc-l) 
, 
(fc-1) 
j 
,(fc) 
.(fc-1) 
i 
.(fc-1) 
ai,i 
= ai,i 
1з,ьЧ,г 
and 
b)> = b) 
'-Itfbi 
>. 

SYMMETRIC, POSITIVE DEFINITE MATRICES 
3 4 9 
Only the elements ar-J for к < j < г must be computed due to the symmetry in the 
matrix A. We again remark that in a symmetric, positive definite matrix the diagonal 
elements dk,k are positive in each step к — 1,..., п. They can thus be used as pivot 
elements (Schwarz, 1997). Once the decomposition A = LDLT is found, the solu-
tion to the system is obtained by solving the triangular systems LY = b, DZ = Y 
and L TX = Z, or alternatively by solving LY = b and L TX = D _ 1 Y . 
We now return to local computation to avoid fill-ins in the set of zero elements Z 
given in equation (9.18). Since we are still dealing with a valuation algebra without 
neutral elements, we again assume a covering join tree for the factorization of equation 
(9.17) and further that the join tree edges are directed towards the root node та. As 
usual, the nodes are numbered such that if node j is on the path form node г to node 
та, then i < j . Each node i £ V contains a factor φι with ω^ = ά(φ{) Ç A(i). If we 
execute the collect algorithm as in Section 9.2.2, the computation of each message 
ßi^ch(i) requires to eliminate the variables in 
wf] - X(ch(i)) 
= \(i) - 
\(ch(i)). 
See equation 9.21. This allows us to determine a corresponding variable elimination 
sequence. Because the matrix A is regular, we may without loss of generality as-
sume that the variables are numbered such that X\,..., 
Xn is a variable elimination 
sequence that corresponds to the sequence of messages. Note that a possible renum-
bering of variables matches a permutation of the rows and columns of the matrix A 
and the vector b. 
We claim that the factors aS I remain zero for all elimination steps k, if (j, h) £ Z. 
Theorem 9.4 For all (j, h) £ Z and к = 0,...,n — I we have a W = 0 and lj^ — 0. 
Proof: We prove the theorem by induction: For к = 0 the proposition holds by 
the definition of Z. We note also that ljt\ = 0 for all (j, 1) G Z. This follows from 
equation (9.24). So, let us assume that the proposition holds for к — 1 and also that 
ljik = 0 for all (j, k) £ Z. From equation (9.8) follows that af^ = 0, if (j, h) G Z. 
Then, by equation (9.24) it also follows that l^k+i = 0 for (j, к + 1) G Z. 
m 
The theorem states that the non-zero pattern defined by the join tree decomposition 
of the matrix A is maintained in the LDL T factorization of A. 
We next observe that all diagonal, square submatrices of a lower triangular matrix 
are lower triangular too. So, for s Ç r = {X\,..., X„} the submatrix L^s's is 
lower triangular as well as L^ t - S' t - S and L,^snt 'snt with sUt — r. This situation is 
schematically represented in Figure 9.4. 
( k) 
Subsequently, we assume that the factors <r ^ or the Z-elements are still available 
from an earlier run of the collect algorithm, see equation (9.24). This means that each 
node i £ V contains the matrix L ^ 
,ω* which is lower triangular as remarked 
above. Using these matrices, we now discuss how the system AX = b can be solved 

350 
SPARSE MATRIX TECHNIQUES 
Figure 9.4 
Square submatrices of a lower triangular matrix are lower triangular too. 
by LDLT-decomposition and local computation. In a first step, we solve the system 
LY = b by a similar collect phase as in the foregoing Section 9.2.2. Thus, we directly 
explain the computations performed by node i s F i n the step г = 1,...,m — 1. At 
step г of the generalized collect algorithm, node i computes the message to its child 
node by eliminating the variables 
у ^ | ° - А ( с Л ( г ) ) 
_ 
γ|λ(ί)-λ(ο/ι(ί)) 
(see equation (9.21)) from the system 
From the decomposition 
-^4.А(г)-Л(с/г(г)),А(г)-А(с/г(г))у4А(г)-А(с/г(г)) 
_ 
^(i) 4A(i)-A(c/i(i)) 
£4ш^)ПЛ(с/г(г)),А(г)-А(с^(г))у4.А(г)-А(с^(г)) 
л_ 
^4шг
(0ПА(с/г(г)),шг!£)ПА(с/г(г))у4.ш<°ПА(с/г(г)) 
_ 
^(г) Ιω^ 
nX(ch(i)) 
we obtain 
ylX(i)-X(ch(i)) 
= 
(LlX(i)-X(ch(i)),X(i)-X(ch(i))ylb(i) 
4A(i)-A(c/l(i))_ 
Note that due to the triangularity of the involved matrix, it is a simple stepwise 
process to compute y-I^M-McM*)). This solution is introduced into the second part 
of the system which gives after rearrangement 
■^1а)<*)ПА(с/1(г)),^<)ПА(с/г(г))у4ш<*)ПА(с/г(г)) 
_ 
b(i) 4шг
<;)ПА(с/1(г)) _ j^lw^nX(ch(i)),X(i)-X(ch(i)) 
iX(i)-X(ch(i)) 
We define the vector 
. , W n W , i , i ; « 
T ι,.,(') 
h(ch(i)) 
tj(t) 4шг
1"пА(с/1(г)) _ ^4ш>"пА(сЛ(г)),А(^)-А(с/г(г)) 
4.А(г)-А(с/г(г)) 
b(0 lu~ .,<<> 

SYMMETRIC, POSITIVE DEFINITE MATRICES 
3 5 1 
where и is the union of all X(k) for к = г,..., т. So, node i sends the message 
ßi->\(ch(i)) 
_ 
_jj^\i)D\(ch(i)),\{i)-X(ch(i)) 
i\(i)~X(ch(i)) 
to its child node. The child node combines the message with its content b w 
Λ<·) : 
m I,/*) 
/ 
<i\ |,,<4> 
^ш^Мш^ПЦс^г))) 
/L^r>nA(cMi))^(0-A(cMi))y|A(i)-A(cMi))\ÎW-<->U^i,nÀ(C'i(i)))_ 
We thus obtain for the domain of the updated node content: 
The content of all other nodes does not change at step i. At the end of the collect 
algorithm, the root node m contains the system 
y l\(m),\(m)-yl\(m) 
_ 
K(m) 4-A(m) 
from which we obtain 
J.A(m) 
ίτ i,\(m),\(m)\ 
~i_Jm) 4.Л(т) 
/L4A(m),A(mA ^ · 
This clearly shows how the collect algorithm operates locally on the join tree of the 
decomposition (9.17). At the end of the collect algorithm, each node г = 1,..., m— 1 
contains the partial solution у4-л(г)-л(с/г(г))5 an(j the root node contains y+A(m). Re-
mark also that no solution extension phase is necessary, because the involved matrices 
are all lower triangular. The total solution у could be obtained from Lemma 8.4, al-
though this is not necessary for the second part of the process. 
As in Section 9.2.2 we now solve the system D L r X = Y by solution extension. 
The root node m solves the system 
and finds the partial solution 
x|A(m) 
^ 
Л с 1 Т \ Д ( " ' ) , А ( т ) Г 1
у а ( т ) 1 
( 9 2 5 ) 
These computations are simple because DL T is upper triangular. Assume now that 
node г = m — 1,..., 1 obtains the partial solution X-I"4*)n-4C>*W) from its child node. 
Then, node г computes equation (9.22) with A replaced by DL T and b by y: 
//р^Т\-Р>(г)-А(<Л(г)),А(г)-А(с/1(г)П-1 
I А(г)-Л(с/г(г)) 
_ 
(9 26) 
//DLT\U(i)-A(cft(i)),A(i)-A(c/1(i))\-1 
((OLTYJ'Г>пМсМ0),^т)пл(сВД)\х4.ш< 
m)n\(ch(i)) 
_ 
xiX(i)-X(ch(i)) 

352 
SPARSE MATRIX TECHNIQUES 
and obtains 
,U(i) 
fxl\(i)-\(ch(i)) 
^xi\(i)nx(ch(i))\ 
However, there is actually no need to compute the inverse matrices that occur in this 
formula explicitly. Instead, χ4-λ(ι)-λ(<:Λ(0) corresponds to the solution to the system 
/"/j-jjjT\4-A(i)->'(c'ï(i)),A(i)-A(c/i(i))\ ^·4.λ(ί)-λ(οΛ(»)) 
_ 
4A(i)-A(ch(i)) 
_ 
//DLT\K(m)nA(C/i(i)),a,l
(m,nA(C/l(i))\x4.u,('™)nA(c/1(i)) 
and solving this system is simple because only triangular matrices are involved. This 
process is repeated for г = m — 1,..., 1. At the end of the solution construction 
process, each node г G V contains the partial solution χΊ-λΜ which can then be 
aggregated to the total solution x of AX = b. 
We given an example of this process: 
Example 9.3 Consider the join tree of Figure 9.5 where the nodes are labeled with 
index sets. This is suitable because we are going to solve two systems with differ-
ent variable vectors X on the same join tree. The variables are already numbered 
according to an elimination sequence that corresponds to this join tree, if node 3 is 
taken as root. The symmetric, positive definite matnces Ai, Аг and A3 are then 
written as 
Αχ 
OÎ.1 
«2,1 
/,1 
a4,l 
aî,2 
α2,2 
«1 
α4,2 
«1,4 ' 
α2,4 
„ 1 
°4,4 
, Α2 = 
Г η2 
α3,3 
η2 
α4,3 
α1,4 ' 
η2 
α4,4 
, Α3 = 
α4,4 
α4.5 
α4,6 
„3 
„3 
„3 
χ5,4 
73 
χ6,4 s: 
^■5,6 
Ζ6,6 
Similarly, we define the vectors bi, Ьг and Ьз associated with the three matrices. 
This defines three valuation φί = (Aj, hi) for г = 1,2,3, which combine to the total 
system ф = ф\ Cg> <f>2 <8> φι representing the valuation φ = (A, b) with 
A 
= 
ΑΪ{1'2'3,4'5' 6} 
^{1,2,3,4,5,6} 
t{l,2,3,4,5,6} 
and 
, 
_ 
, t{l,2,3,4,5,6} 
, t{l,2,3,4,5,6} 
, t{l,2,3,4,5,6} 
Note also that all join tree nodes are filled in this simple example, i.e. ац = Л(г). We 
further observe that the matrix A has the following non-zero structure 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 
X 

SYMMETRIC, POSITIVE DEFINITE MATRICES 
3 5 3 
Figure 9.5 The join tree of Example 9.3. 
where x denotes a non-zero element. 
If we execute the collect algorithm on the join tree of Figure 9.5, we first eliminate 
the variables Χχ and X2 from the system in node 1 to obtain the message sent from 
node 1 to node 3. Then, variable X3 is eliminated for the message of node 2 to node 
3. Finally, on node 3 the variables X4 and X5 are eliminated. This gives the lower 
triangular matrix L with the following non-zero structure: 
" 1 
x 
1 
L 
= 
x 
x 
x 
1 
x 
1 
x 
x 
1 
We see that this matrix maintains the zero-pattern captured by the join tree and 
exhibited in the matrix A. Note also that the submatrices like L''·'1'2'4'''1'2'4' and 
^4.(3,4},{3,4} are stm iower triangular. 
Let us now consider the computations of the collect phase for solving LY = b 
in more detail. In the first step, we eliminate the variables Υχ and Y2 to obtain the 
message that is sent from node 1 to node 3. We thus have 
LJ.{1,2},{1,2}Y4.{1,2} 
= 
bl{l,2} 
Solving this system is very simple because the matrix L-^x '2Ь t * '2^ is lower triangular. 
In fact, from 
Yi = 61 and htiYx+Y2 
= b2 
we obtain 
yx = Ьх and y2 — 
b2-h,xbx. 
This solution is introduced into the fourth equation on node 3 to obtain 
^3,4*3 + Yi 
= 
b4- 
l4tlyx 
- 
l4,2y2-
The message of node 2 to node 3 determines the value of Y3, which then allows to 
compute the solution values of variables Y4, Y5 and Yç. This builds up the entire 
solution y of the system LY = b and concludes the collect phase. 

354 
SPARSE MATRIX TECHNIQUES 
The solution to the system AX = b is then obtained by solving DL TX = y by 
solution extension. The root node obtains from equation (9.25) the partial solution 
х1{х4,*5,*б} αηα· sends the mesSage χ-Η-*·»} to the nodes 1 and 2. The two receiving 
nodes compute the partial solutions with respect to their proper node label by equation 
(9.26). For example, node 1 solves the system 
/' D LT|{X 1,X 2},{X 1,X 2}W|{X 1,X 2} = yl{Xi,JV 2} _ ίΌίΤ 
l{X4},{X4}\l{X4} 
Finally, the total solution x is build from χ-Η^,ΧδΛ^ х1№,л:2} a n j х4{Хз} 
At the beginning of Section 9.2.2 we assumed in equation (9.17) a join tree decom-
position of the valuation (A, b) that represents a symmetric, positive definite system. 
There are important applications where such factorizations occur naturally, as for 
example in the least squares method that will be discussed in Section 9.2.5 below. 
If, however, no natural prior factorization of the total system is given, then a corre-
sponding decomposition must be found in order to benefit from local computation. 
How such decompositions are produced is the topic of the following section. 
9.2.4 
Symmetric Decompositions and Elimination Sequences 
The problem of producing a good decomposition of a symmetric, positive definite 
system ф = (A, b) is closely related to the problem of choosing a good elimination 
sequence for the construction of join trees that has already been addressed in Section 
3.7. Here, we revisit parts of this discussion by following the graphical theory for 
symmetric, positive definite matrices proposed by (Rose, 1972). Interestingly, this 
can be done to a large extend without reference to an underlying valuation algebra, 
but the algebra becomes important for the discussion of applications like the least 
squares method in Section 9.2.5. 
The non-zero pattern in an n x n matrix A of the symmetric, positive definite 
system ф can be represented by an undirected graph: The vertices of the graph 
G = (V, E) correspond to the rows or, equivalently, to the columns of matrix A. 
The vertex associated with row i is called vi for г = 1,..., п. We further introduce 
an edge {vi,Vj} G E for any non-zero element a^j of the matrix A. This graphical 
structure reflects the non-zero structure of the matrix A. The example below assumes 
a 7 x 7 matrix whose graph is shown Figure 9.6. We now apply the theory of Section 
3.7.1 to this graph by first constructing a triangulation. In the triangulated graph, we 
then search for all maximum cliques and form a join tree whose nodes correspond to 
the identified cliques. Finally, we fix a node in the join tree as root, direct all edges 
towards it and renumber the nodes of the graph G by n(i), such that the number of 
rows in a join tree node before another one on the path to the root is smaller. More 
precisely, if s and t are neighbor nodes with s before t on the path to the root, then 
n(i) < n(j) for any г G s — t and j G t. 

SYMMETRIC, POSITIVE DEFINITE MATRICES 
3 5 5 
Example 9.4 The following table shows the non-zero pattern ofa7x 
7 symmetric 
matrix, where non-zero elements are represented by a symbol x, 
Vl 
V2 
V3 
VA 
V5 
ve 
V7 
Vi 
X 
X 
X 
Vl 
X 
X 
X 
X 
X 
V-i 
X 
X 
VA 
X 
X 
X 
V5 
X 
X 
X 
v6 
X 
X 
v7 
X 
X 
The associated graph shown in Figure 9.6 has seven nodes v\ to νγ. We observe that 
it is already triangulated with maximal cliques {vi,v$, νγ}, {v2,Vs}, {^2,^3,^4} 
and {г>2, г>4, ve}. These cliques can be arranged in a join tree as shown in Figure 9.7. 
The node {г>2, гц, г>б} 's chosen as root node. 
Figure 9.6 The graph representing the non-zero pattern of Example 9.4. 
Figure 9.7 The join tree of the triangulated graph in Figure 9.6. 
From the left to the right in the join tree of Figure 9.7, the variables are elimi-
nated in the following sequence: first {^1,^7}, then {v$} and {г>з} until finally the 
variables {v2, V4,ve} in the root node remain. Hence, we may choose an elimi-
nation sequence from the set {νι,νγ} 
x {115} x {г>з} x {t>2,î>4,f6}> for example 
(vi,vj,V5,V3,V2,V4,ve). 
This elimination sequence induces the following renum-
bering that satisfies the above requirement: 
V\ 
V2 
V3 
V4 
t>5 
1>6 
1>7 
1 5 
4 
6 
3 
7 
2 

3 5 6 
SPARSE MATRIX TECHNIQUES 
Rearranging the rows and columns of the matrix according to this numbering gives 
Vl 
v7 
V5 
V3 
V2 
VA 
v& 
Vl 
X 
X 
X 
Vl 
X 
X 
Vb 
X 
X 
X 
V3 
X 
X 
V2 
X 
X 
X 
X 
X 
VA 
X 
X 
X 
V6 
X 
X 
This identifies the block structure exploited in local computation. The zeros outside 
the blocks will not befilled-in, if the variables are eliminated in the sequence defined 
by the renumbering. At the end of the collect algorithm applied to the above join tree, 
we therefore have the following non-zero pattern of the triangular matrix: 
Vl 
V7 
v$ 
V3 
V2 
VA 
νβ 
Vl 
X 
X 
X 
V7 
X 
X 
Vb 
X 
X 
V3 
X 
X 
X 
V2 
X 
X 
X 
VA 
X 
X 
V6 
X 
All unmarked matrix entries remain zero during the complete local computation 
process. Alternatively, we could also select the node {2,5} as root and choose the 
elimination sequence (υι,υγ, ve,V3,V4,v$, v^). This induces the renumbering: 
Vl 
V2 
V3 
VA 
Vb 
Ve 
Vl 
~1 
7 
4 
5 
6 
3 
2~ 
Rearranging the rows and columns of the matrix according to this numbering gives 
Vl 
VT 
V6 
v3 
VA 
Vs 
V2 
Vl 
X 
X 
X 
Vl 
X 
X 
V6 
X 
X 
V3 
X 
X 
VA 
X 
X 
X 
V5 
X 
X 
X 
V2 
X 
X 
X 
X 
X 
In this case, variables X\ and Χγ are eliminated first and expressed by X$ in the 
original equations 1 and 7. Then, in the original equations 2, 4, and 6, the variable 
XQ is eliminated, i.e. it is expressed by X2 and X4. This process continues for the 
Xz and X4 until two equations for the variables X2 and X$ remain in the root node. 

SYMMETRIC, POSITIVE DEFINITE MATRICES 
3 5 7 
We obtain the following non-zero pattern of the lower triangular matrix: 
Vi 
v7 
V6 
v3 
Vi 
v& 
V2 
Vl 
X 
X 
X 
v7 
X 
X 
ve 
X 
X 
X 
V3 
X 
X 
X 
t>4 
X 
X 
«5 
X 
X 
V2 
X 
These examples show that different numberings lead to different LDL T decomposi-
tions, but the non-zero pattern represented by the join tree is always respected. 
We next present an application where factorizations of symmetric, positive definite 
systems according to this valuation algebra occur in a natural manner. 
9.2.5 An Application: The Least Squares Method 
It is often the case in empirical studies that unknown parameters in a functional rela-
tionship have to be estimated from observations. Here, we assume a linear relationship 
between the unknown parameters. In general, there are much more observations than 
parameters which allows us to account for inevitable errors in the observations. This 
leads to overdetermined systems which cannot be solved exactly. Statistical methods 
are therefore needed to obtain estimates of the unknown parameters as for example 
the method of least squares proposed by Gauss. In this method the parameters are 
determined by minimizing the sum of squared residues in the linear equations. The 
minimization process leads to linear equations, the so-called normal equations, in 
the unknown parameters with a symmetric, positive definite matrix. So, the theory 
developed above applies. Moreover, we shall see that a decomposition of the orig-
inal overdetermined systems leads to combining normal equations according to the 
combination rule of the valuation algebra introduced in the previous Section 9.2.1. 
In other words, we face here an important application of the theory developed so far. 
■ 9.1 Least Squares Method 
Consider a system of linear equations 
αι,ι-ΧΊ 
+ 
α1ι2Χ2 
+ 
··· 
+ 
αι,ηΧ„ 
+ 
Di 
— 
Ь ь 
θ2,ι-ΧΊ 
+ 
α2,2^2 
+ 
· · · 
+ 
α2,ηΧη 
+ 
D2 
= 
b2, 
^τη.η^η 
i 
-Urn 
— 
®m ? 
(9.27) 
where m > n, i.e. there are more equations than unknowns Xi to Xn. Also, 
we have introduced the residues or differences Di in order to balance the right 

358 
SPARSE MATRIX TECHNIQUES 
and left-hand side of the equations. In matrix form, this system is written as 
AX + D = b, where A is an m x n matrix and D and b are m-vectors. We 
assume that the matrix A has maximal rank n. The principal idea of the least 
squares method is to determine the unknowns X\ to Xn such that the sum of 
the squares of the residues becomes minimal. This sum can be written as 
D T D = ( b - A X f ( b - A X ) 
= X T A T A X - 2(A Tb) TX + b Tb. 
So, the function to be minimized is a quadratic function of the unknowns X. 
For simplification we introduce the short-hand notations 
С = A TA 
and 
с = A Tb, 
where С is an n x n symmetric, positive definite matrix and с an n-vector. The 
sum of squares can then be written as 
F(X) = D T D = X T C X - 2c TX + b T b = min! 
(9.28) 
The necessary condition that F(X) takes a minimum is the vanishing of the 
gradient VF(X), whose г-th component can be determined from equation 
(9.28) as 
^ 
= 
2±cidX3-2c%. 
We divide by 2 and obtain the system of equations CX — с = 0. These are 
the normal equations of the overdetermined system (9.27). They can also be 
written as 
A T A X 
= 
A Tb. 
Since the matrix A TA is positive definite, the unknowns are uniquely deter-
mined by 
x = 
{KTA)-lATh. 
This is the so-called least squares estimate of the unknown parameters in the 
system (9.27). If we compute the second order derivatives of F(%) we obtain 
2C. Since С is positive definite, we conclude that the solutions of the nor-
mal equations indeed minimize the function F(X). Statistical theory justifies 
this way of determining the parameters, in particular, if certain probabilistic 
assumptions are made about the residues. We reconsider this in Chapter 10 
and derive similar results by a very different method based on an isomorphic 
valuation algebra. 
In order to solve the normal equations we may proceed as described in Section 
9.2.3 and use local computation to exploit the sparsity of the normal equations. 

SYMMETRIC, POSITIVE DEFINITE MATRICES 
3 5 9 
In this respect, we prove a remarkable result relating the original overdetermined 
system to its normal equations. In fact, already the original system may be very 
sparse. More precisely, we assume that we may capture this sparsity by a covering 
join tree for the original system. Thus, let r = { 1 , . . . , n} be the index set of the 
variables of the system and (V, E, A, D) a join tree with D = V{r). We now assume 
that this join tree covers the system (9.27). If V = { 1 , . . . , I}, then this means that 
after permuting the rows of the system of equations, its matrix A decomposes into 
submatrices A* for i = 1,..., I, where A; is an m; x s, matrix such that s, Ç А(г) 
and mi + · ■ · + пц = m. Therefore, every pair (Aj, bj) is covered by some node of 
the join tree and thus also every equation of the system (9.27). It is not excluded that 
some of these systems are empty, i.e. m; = 0. The following theorem claims that the 
corresponding normal system is decomposed into normal systems of the subsystems 
of the decomposition above and covered by the same join tree. Here, <g> denotes the 
combination operation of the valuation algebra of Section 9.2.1. 
Theorem 9.5 Let A X + D = b be an overdetermined system and A an m x n matrix 
of full column rank n. Let further (V, E, X, V(r)) be a join tree with V = { 1 , . . . , /}. 
If after permutation of the equations of the system AX + D = b, the matrix A 
decomposes into mi x Si matrices Aj and b correspondingly into rrii-vectors bifor 
г = 1,...,/, such that 
1. Si С \{i); 
2. mi + ■ ■ ■ + mi = m; 
3. all matrices Ai have column rank \si\; 
then for С = A T A and с = A T b 
i 
(C,c) 
= 
(gNCi.Ci), 
i=l 
where Ci = Aj Ai and c, = Aj bi. 
Proof: The proof goes by induction over the nodes of the join tree. We select node 
I 6 V as a root and direct all edges towards this root. Further, we number the nodes 
such that i < j if node j is on the path from г to the root I. Then, node 1 is necessarily 
a leaf. Let si = s to simplify notation and 
i 
t 
= 
[J Si, 
i=2 
If the system of equations covered by this node is empty, then we may eliminate this 
node directly and proceed to the next node in the numbering. Otherwise, the first 
mi equations contain only variables from s. We then decompose the columns of the 
matrix Ai of the first πΐχ equations according to s — t and silt, 
Ai 
= 
[ AiiS_t 
Ai)Snt ] · 

360 
SPARSE MATRIX TECHNIQUES 
The matrix В of the remaining m — m\ equations contains only variables from the 
set t. We decompose В according to s П t and t into 
В 
= 
[ 0 
Bsnt 
B t _ s J . 
Then the total matrix A of the system may be decomposed into 
see that 
A T A 
= 
= 
A 
= 
A , 
\ Al°-t 
° 1 
A7^ 
B r 
" A T 
A, 
A7^ 
A, 
0 
- t 
-t 
,s 
0 
-t 
Ai ) g nt 
0 
B snt 
B t _ s 
A 
Ai ) S_t 
Ai)Snt 
0 
B s n t 
Ai ) S_tAi ) Snt 
T 
A 
_u RT 
l,snt Al>snt "Г £> s n t 
B t - s B t n s 
0 
B t _ s 
В 
0 
snt 
В я П ЕВ е_ 5 
B t _ s B t _ s 
= (Af A,YsUt + (BTB)tsUf. 
The vector b is decomposed in the same way with respect to the first πΐχ and the 
remaining m — m\ equations: 
b = [ bl ~ 
с 
Then, we see in the same way that 
A T b 
- 
(Ajb1)^ut 
+ 
(BTc)^sUt 
and finally conclude that 
( A T A , A T b ) 
= 
( A f A 1 , A f b 1 ) ® ( B T B , B r c ) . 
If node 1 in the join tree is eliminated, the remaining tree is still a join tree and 
node ch{\) is now a leaf. The remaining system of m — mi equations with matrix В 
and right-hand side с can then be treated as the system above and this process may be 
repeated until the remaining matrix is A; and the right-hand side is b;. If t = t\ and 
ti for г = 2,... / — 1 denote the variables in the remaining systems obtained during 
this process respectively, such that i;_i = si, we obtain in this way 
ATA = 
(AjA1YaiUtl+((A^A2Y"ut' 
-L 
(. . . + (A T_A..)t si-l U sf)tsi-2U«;_i . _ . -)ts2Ut2-\tsiUti 
= 
{AjAx)* 
+ (A?A 2)f + · · · + (Af A ;) t r, 
since si U ii = r. Similarly, we also obtain 
A T b 
= 
(A?'b 1)^ + (Ai ,b 2)f + " - + (A?,bI)tr. 

SYMMETRIC, POSITIVE DEFINITE MATRICES 
361 
But this means that 
i 
(ATA,ATb) 
= 
(gNAfAi.Afbi). 
t = l 
which proves the theorem. 
■ 
Local computation in the valuation algebra of Section 9.2.1 is thus applicable to 
the least squares method for linear equations, where a factorization of the normal 
equation is induced by a join tree decomposition of the original overdetermined 
system. We shall illustrate this more concretely in the following application. Note 
also that if some factor in the decomposition of the original system is empty, then the 
induced normal system corresponds to the identity element, see Section 3.9. Finally, 
we refer to Chapter 10 where closely related problems are studied from a different 
perspective. In particular, the assumption of Theorem 9.5 that the matrix of the system 
has full column rank will be dropped. 
■ 9.2 Smoothing and Filtering in Linear Dynamic System 
Let XQ, Xi, X2 and X3 be n-dimensional variable vectors that satisfy 
Xl = 
X2 = 
Хз = 
= A0Xo 
= ΑιΧχ 
= A 2X 2 
+ D0, 
+ Di, 
+ D2. 
The vectors Xj may be imagined as unknown state vectors of some system, and 
the equations describe how these states change form time г — 1 to i, here for 
г е {1,2,3}. The residues D^ represent unknown disturbances which influence 
the development of the state over time. The matrices A» are all regular n x n 
matrices. State vectors may not be observed directly, but only through some 
measurement devices which are described by the equations 
H0Xo 
+ 
Eo 
= 
уо, 
HiXi 
+ 
Ei 
= 
yi, 
H2X2 
+ 
E2 
— У2-
The matrices Hi are m x n matrices and the vectors y*, representing observed 
values, give information about the unknown states. The residues Ej again 
represent unknown measurement errors. The task is to estimate the values of 
the state vectors from these equations and the observations. We first bring the 
equations into the form of system (9.27): 
A0Xo -
HQXQ 
XI 
AiXx -
HxXi 
x2 
A 2X 2 -
H2X2 
+ 
+ 
+ 
+ 
- Хз + 
+ 
Do 
Eo 
Di 
Ei 
D2 
hj2 
= 0, 
= Уо, 
= о, 
= Уь 
= о, 
= 
У2· 

362 
SPARSE MATRIX TECHNIQUES 
We observe that the system can be decomposed into three subsystems, each 
consisting of two consecutive equations that are covered by the three nodes of 
the simple join tree in Figure 9.8. In order to apply Theorem 9.5, we assume that 
the subsystems have full column rank, i.e. that m > n, and that Hi have full 
column rank too. If we switch to the valuation algebra of symmetric, positive 
definite systems, then by Theorem 9.5 the normal equations of the total system 
can be decomposed into 
where 
Ci = 
and 
(Co,co)®(Ci,ci)®(C 2 )c 2), 
Ai 
Hi 
Ai 
Hi 
AfAi 
+ UfHi 
-Ai 
-Aï 
I 
Ai 
- I 
H, 
0 
0 
Уг 
0 
for г € {0,1,2}. This allows us to apply local computation for the solution of 
the normal equations using the collect algorithm as described in Section 9.2.2 
or the fusion algorithm. If we select the rightmost node in Figure 9.8 as the 
root, we eliminate first Xo and then Xi, i.e. we express Xo in terms of Xi and 
then Xi in terms of X2. The variables X2 are eliminated in the last node to 
obtain X3. Determining the last state, given the past and present measurements 
is called a filter solution. Then, by backward substituting, the least squares esti-
mates of the other states can be computed. This is called the smoothing solution. 
Xo, Xi 
X1,X2 
Хг, Хз 
Figure 9.8 The join tree covering the equations of the linear dynamic system. 
We examine this procedure a bit more closely. In terms of message-passing, the 
message from the leftmost node Xo, Xi in the join tree of Figure 9.8 corresponds to 
the elimination of Xo. The message is the projection of (Co, Co) to the variables Xi 
which, according to (9.13) and (9.14), is 
(ΣΟ,Ι,^Ο,Ι) 
= 
( A o E o ^ J ' + I . A o S ö ^ y o ) , 
where 
Σο 
= 
A 0 A0 + H 0 H 0. 

SYMMETRIC, POSITIVE DEFINITE MATRICES 
3 6 3 
This matrix is the sum of two positive definite matrices and is thus positive definite 
itself, i.e. it has an inverse. The elimination process for Xo in the first subsystem 
leads to the equation 
Xo 
= 
Σ 0 Α 0 Χ ι + Σ 0 
H 0yo, 
which can later be used for backward substitution, once Xi is determined. Passing 
the message to the next node of the join tree and combining it to the node content 
is equivalent to substituting this expression for Xo in the second subsystem. More 
generally, let us define for г = 0,1,2,... 
Σέ 
= 
Σί-ι,ί + At Ai + Н4 Hj, 
and 
Vi = 
i>i-i,i + H f y i ; 
where Σ_ι,ο = 0 and i/-i,o = yo· Since combination (9.15) corresponds to matrix 
addition, the system on the node Xi, Xj+i at step г of the collect algorithm is: 
Xi 
Хг+1 
= 
0 
Due to (9.13) and (9.14), the message from node Xi_i, Xj to node Xj, Xj+i com-
puted by eliminating Xj_i is 
for г = 1,2,... and the eliminated variables in this step can be expressed as 
Хг-i 
= 
S ^ A j ^ X j - i + Σ ^ Η ^ γ ^ ι . 
If the sequence stops with X 3 as in our example, then eliminating X2 in the subsystem 
on the middle node in join tree of Figure 9.8 gives 
(-Α 2Σ 2- 1Α^ + Ι)Χ3 
= 
Α 2Σ 2-ν 2, 
hence 
X 3 
= 
(-Α2Σ2-1Α2" + Ι)- 1Α 2Σ 2-ν 2. 
This is also called a one-step predictor for X3 and can be used to start backward 
substitution: first for X2, then for Χχ and finally for Xo. Such linear dynamic systems 
will be reconsidered from a more general point of view in Section 10.5. In particular, 
it will be shown that the assumption that Hj has full column rank can be dropped. 
This closes our discussion of ordinary linear systems and we next focus on linear 
fixpoint equation systems. A third class of linear systems called linear systems with 
Gaussian disturbances will be studied in Section 10.1. 

364 
SPARSE MATRIX TECHNIQUES 
9.3 SEMIRING FIXPOINT EQUATION SYSTEMS 
A linear fixpoint equation system takes the form M X + b = X, where M is an n x n 
matrix, X an n-vector of variables and b an n-vector. If the matrix M and the vector 
b take values from a field, we may express the fixpoint system as (I — M)X = b 
and by defining A = I — M we obtain an ordinary linear system AX = b. This 
allows us to apply the theory of the previous sections and shows that fixpoint equation 
systems over fields do not need to be considered separately. However, we pointed 
out in Section 6.2 that the computation of path problems amounts to the solution of 
fixpoint equation systems over semirings. Moreover, we identified a particular class 
of semirings called quasi-regular semirings, where all fixpoint equations provide 
a solution determined by the star operation. If (A, +, x, *, 0,1) denotes a quasi-
regular semiring, then the set A4(A, n) of n x n matrices with values from A also 
forms a quasi-regular semiring with the inductive definition of the star operation 
given in equation (6.17), see Theorem 6.2. Hence, a solution to the fixpoint equation 
MX + b = X in a quasi-regular semiring is always given by the element M*b. 
How is this related to fixpoint equations over real numbers and thus to ordinary 
equation systems? It was shown in Example 6.11 that real numbers M U {сю} form 
a quasi-regular semiring with the definition 
for а ф 1 and a* = oo for a — 1. If we now consider its induced semiring of matrices 
and further assume that I — M is regular, then the fixpoint system MX + b = X 
has a unique solution that must be equal to the result of the star operation, 
M*b 
= 
( I - M ) _ 1 b . 
By applying the star operation to I — M we obtain 
( I - M ) * b = ( I - ( I - M ) ) _ 1 b = M _ 1 b , 
which solves the regular system MX = b. On the other hand, if we directly com-
pute (I — M)* without caring about the regularity of I — M, then the result cor-
responds to the inverse matrix of M, if (I — M)* does not contain the adjoined 
semiring element oo. This follows from Theorem 6.18 and the uniqueness of the 
inverse matrix (Lehmann, 1976). Concluding, we see that fixpoint equation systems 
over quasi-regular semirings indeed provide an important generalization of ordinary 
linear systems. Semiring fixpoint equations induce two different types of valuation 
algebras which will both be used in the remaining parts of this chapter to obtain a local 
computation based solution to such equations. First, we focus on quasi-regular valu-
ation algebras from Section 6.4 which leads to a solution approach that is similar to 
local computation with symmetric, positive definite systems studied in Section 9.2.2. 
The second approach is based on Kleene valuation algebras from Section 6.7 and 
leads to an interesting compilation approach which is based on the query answering 
technique for idempotent valuation algebras in Section 4.6. However, computation 

SEMIRING FIXPOINT EQUATION SYSTEMS 
365 
in both valuation algebras requires evaluating the star operation for matrices over 
quasi-regular semirings. The following section therefore presents first a well-known 
algorithm for this task. 
9.3.1 Computing Quasi-Inverse Matrices 
For a finite index set s, the set of labeled matrices M{A, s) with values from a 
quasi-regular semiring {A, +, x, *, 0,1) forms itself a quasi-regular semiring with 
the inductive definition of the star operation given in equation (6.17). This operation 
is performed as part of the projection rule in quasi-regular valuation algebras, respec-
tively as part of the combination rule in Kleene valuation algebras. In the latter case, 
we often use the alternative induction of equation (6.31), but it has also been shown 
that the two definitions are equivalent for Kleene algebras. It is therefore sufficient 
for both algebras to dispose of an algorithm that computes a quasi-inverse matrix M* 
for M e M{A, s) that satisfies the fixpoint equation 
M* = MM* + 1 = M*M + 1 . 
(9.29) 
The well-known Warshall-Floyd-Kleene algorithm proposed by (Roy, 1959; War-
shall, 1962; Floyd, 1962; Kleene, 1956) computes quasi-inverses iteratively by the 
following formula: For s = { 1 , . . . , n} and к = 0,..., n we define 
M(fc) 
= 
M(fc-1) + (M(fc-1))4s'{fc}(M(fc-1)(fc,fc))*(M(fe-1))i{'cbs 
(9.30) 
with М(°) = M. Note that the operation of matrix projection is used to refer to the 
k-th column and row of the matrix M. Also, we again omit the explicit writing of 
the semiring multiplication symbol. It is proved in (Lehmann, 1976) that M*n) + I 
indeed corresponds to the quasi-inverse of equation (6.17), which implies that 
M* 
= 
M ( n ) + 1 
satisfies equation (9.29) as a consequence of Theorem 6.2. Formula (9.30) therefore 
provides a general algorithm for the computation of the star operation in Ai{A, s) 
using only the semiring operations in A. The space complexity of this algorithm 
is C(|s|2) because it only needs to store two matrices. Given two indices i, j £ s, 
equation (9.30) is equivalently expressed as 
M^{i,j) 
= M{k-1\i,k) 
+ 
M(k-1\i,k)(Mi-k-V(k,k)yM<-k-1')(k,j). 
This shows that an implementation required three nested loops for the indices i, j and 
к, which leads to a time complexity of C?(|s|3). It is important to note that the theory 
in the subsequent sections is independent on the actual algorithm used to compute the 
star operation of matrices over quasi-regular semirings and Kleene algebras. Here, we 
presented the Warshall-Floyd-Kleene algorithm but there are other algorithms as for 
example the Gauss-Jordan method. However, we will use the Warshall-Floyd-Kleene 
algorithm to reason about the complexity of local computation based approaches to 

366 
SPARSE MATRIX TECHNIQUES 
the solution of semiring fixpoint equation systems, but if another algorithm is present 
for some particular quasi-regular semirings that outperforms the Warshall-Floyd-
Kleene algorithm or its implementation, then this algorithm can be used and its gain 
in efficiency also carries over to the local computation scheme. 
Algorithm 9.1 The Warshall-Floyd-Kleene Algorithm 
input: 
MeM(A,s) 
output: M* 
begin 
foreach X,Y £ s do 
// 
I n i t i a l i z a t i o n . 
Мо(Х,У) := M(X,Y) 
end; 
foreach Z = 1... |s| do 
for each X,Y e s do 
с := 
mz_1{X,Z){mz^{Z,Z))*-M.Z-i(Z,Y); 
MZ{X,Y) 
:= M z_ 1(X,y) + c; 
end; 
end; 
foreach X € s do 
/ / Addition of unit matrix. 
MZ(X,X) 
:= MZ(X,X) 
+ 1; 
end; 
return Mz; 
9.3.2 
Local Computation with Quasi-Regular Valuations 
We focus in this section on a local computation based solution of fixpoint equation 
systems X = MX + b denned over a quasi-regular semiring (A,+, x, *, 0,1). For 
variables Χχ,..., 
Xn and the associated index set r = { 1 , . . . , n}, we assume for 
s Ç r that M € M(A,s) 
is an s x s matrix of semiring values, X an s-vector 
of variables and b an s-vector of semiring values. It was shown in Section 6.4 that 
such systems correspond to valuations φ — (A, b) with domain ά{φ) = s in the 
quasi-regular valuation algebra (Φ, D) induced by the semiring A, where D = V{r). 
We observe the similar structure with the valuation algebra of symmetric, positive 
definite systems in Section 9.2.1: in both algebras, valuations are pairs of square 
matrices and vectors, the operations of combination defined in the equations (6.30) 
and (9.15) are identical, and also the operations of projection given in the equations 
(6.23) and (9.14) look very similar. 
As usual for local computation, we subsequently assume that φ = (M, b) is given 
as a factorization or decomposition φ = φ\ <g>... <8> фт of quasi-regular valuations 
φί = (Mi, bj) 6 Φ with г = 1,..., m. It is important to note that the following local 
computation based solution solves any such factorized system. But since we motivated 
semiring fixpoint equations from the perspective of the algebraic path problem in 
Chapter 6, we subsequently go into this important application field. Moreover, when 
the system (M, b) models a path problem in a graph, such factorizations often come 
from the underlying graph. Imagine, for example, that we want to compute shortest 

SEMIRING FIXPOINT EQUATION SYSTEMS 
3 6 7 
distances between cities of different European countries from road maps that contain 
only direct distances between neighboring cities, see Instance 6.2. Hence, let s denote 
the set of all European cities. Instead of a single road map M that contains all cities 
of Europe, it is more natural to consider local maps M* for each country with some 
overlapping region to neighboring countries. Since matrix addition corresponds to 
minimization in the tropical semiring, we clearly have 
M 
= 
М|" + ... + м £ . 
(9.31) 
Further, we explained at the very end of Section 6.3 that the vector b serves to specify 
the distances to be computed. If we are interested in the shortest distances between 
all possible cities of Europe and a selected target city T € s, we define the vector b 
such that b(T) = 1 and b(Y) = 0 for all Y e s - {T}. Solving the fixpoint equation 
X = MX + b then corresponds to the single-target shortest distance problem and its 
transposed system represents the single-source problem. The vectors b^ are therefore 
specified with respect to query and must satisfy 
b 
= bîs + ... + b£. 
This gives us a natural factorization 
(M,b) 
= 
(M bbi)®---(g)(M m,b m) 
for the shortest distance problem. If x = M*b is a solution to the system (M, b), 
it contains the shortest distances between all European cities and the selected target 
city T 6 s. For an arbitrary source city S € s, it follows from Theorem 6.3 that 
XHS,T] 
= 
( M { s, T })*b { s > T }. 
Hence, the single-pair problem requires to compute 
(M,bJ 
= (M{s,r},b{s,r}J 
and we therefore obtain its solution by solving the single-query inference problem 
/ 
a{s,T} 
/ 
\1{S,T} 
(M,bJ 
= ^(Mi)bi)®---®(MTO,bm)J 
This can either be done by the fusion or collect algorithm. As for symmetric, positive 
definite systems in Section 9.2.2, we finally compute the complete solution x by a 
step-wise extension of the single-pair solution in a solution construction process. To 
do so, we show in this section that solutions to quasi-regular fixpoint equations satisfy 
the requirements for general solutions in valuation algebras imposed in Section 8.1. 
This allows us to directly apply one of the generic solution construction procedures. 
Alternatively, if no natural factorization of the sparse system φ = (M, b) exists, 
we have to produce a decomposition φ = φ\ ®... ® фт. There are many applications 

368 
SPARSE MATRIX TECHNIQUES 
of path problems where the adjacency matrix is symmetric. Imagine for example the 
modelling of air-line distances, which are always symmetric. We then obtain a de-
composition by applying the graphical theory of Section 9.2.4. This follows from 
the similar structure between the two valuation algebras mentioned above. On the 
other hand, if the matrix is not symmetric, then the construction of the graph for 
the triangulation process must be modified. We propose this as Exercise 1.1 to the 
reader. By application of local computation, we exploit the sparsity in the system 
φ = (M, b) captured by the factorization or decomposition. As shown below, this is 
again similar to local computation for symmetric, positive definite systems studied 
in Section 9.2.2 and also follows the same principle of locality. 
We now propose a suitable definition of configuration extension sets for quasi-
regular valuation algebras: for φ = (M, b) e Φ with ά{φ) = s and i C s w e define 
the configuration extension set for an arbitrary ί-vector x to φ as 
Wl ,(x) = {(М^-^-')*(М+8-*'*х + Ь^)}. 
(9.32) 
Again, observe the similarity to the definition of configuration extension sets for 
symmetric, positive definite systems in equation (9.19). The following theorem shows 
that this definition also satisfies the requirements for general solution extension sets 
in valuation algebras imposed in Section 8.1. 
Theorem 9.6 Solution extension sets in quasi-regular valuation algebras satisfy the 
property of Definition 8.1, i.e. for all ф € Ф with t С и Ç s = ά(φ) and all t-vectors 
x we have 
Wj(x) 
= 
{ ( y , z ) : y € V ^ l u ( x ) û i M / z e W 7 ( x , y ) } . 
Proof: For an arbitrary ί-vector x let 
y 
= 
( M i s - t ' s - t ) ' ( M l i - t ' t x + b i s - | ) e ^ ( x ) . 
It then follows from Lemma 6.2 and Definition 6.4 that y is the solution to the system 
Y 
= (М^-''«-')У + (М^-'' 4х + Ь ^ - ' ) . 
We therefore have 
.-Is —и 
jy|4.U-t,li- t 
-\/r^u—t,s—и 
X + 
fols-u 
ylU-t 
yls-u 
+ 
and obtain the two systems 
yiu-t 
_ 
j^j|«-t,u-i |u-f , -jyjiu-i.s-t 4.S--U _i_ |yj4«-i,i x I Yylu-t 
ls-u 
_ 
j^jis-u,»-t in-t _j_ J^J|S-U,S-U J.S-U i Iyl4 s-".t x _i_ b " ^ - " 

SEMIRING FIXPOINT EQUATION SYSTEMS 
3 6 9 
From the second equation follows that y^s " is the solution to 
We must therefore have 
yis-u 
_ 
r-^ßls-u,s-u\*/-^ls-u,u-tylu-t 
. |^j4.s-u,t x , ^4,s-u\ 
(9 33) 
We insert this expression into the first equation and obtain after rearranging terms: 
\fy£lu-t,s-u/-£ßls-u,s-uy£ßls-u,tx 
i 
riu-t 
j^jiu-ijS-u/i^jJ.s-u.s-UNi^ls-u 
i ]yj4-"-i,t x 
i j-jiu—ί 
(9.34) 
Next, we observe the following identities: 
. \.u—t,u — t 
4-u—t,u—t 
Λ/ΐ-lu—i,u 
— t _j_ TV/f4-u—t,s — U,(-KS\,S—u,s 
— 
u\*MKjr\.s—u,u~t 
(ъЛ 
u 
= 
^b4-u + M i u' s" u(M i s" u' s" u) ! , ,b i s _ u) 
u 
y^\.u — t _(_ TV/r4-u — t,s—и/"V/T-l-S — u,s — u\*V|4- s — u 
This follows from the definition of projection in the equations (6.24) and (6.25). 
Likewise, we also have 
( м и ) 
' 
— м ^ и _ м + M^u_M_u(M-1's~u's~u)*M-l-s~u'i. 
We insert these identities in equation (9.34) and obtain 
lu—t 
(Mu)lu-t,u-tylu-t 
+ 
((bu)4.«-t 
+ 
( Μ„)^-*·*χ). 
We observe that y^-" 
É is the solution to the system 
Yi-«-t 
= 
(Mu}iu-t,u-tYiu-t 
+ (bu)J-«-* + ( M , ) ^ - ' ^ 
and it must again hold that 
This shows that y^ u - i g W\iu (x). Finally, we conclude from equation (9.33) 
Ли-t 
ij^j4.s-ti,s-uu /jyjis-u,« 
+ b 4 s- u]. 

370 
SPARSE MATRIX TECHNIQUES 
This implies that y^8"" G W^{y^u~\ x). 
■ 
Configuration extension sets therefore fulfill the requirements for the generic 
solution construction procedure of Section 8.2.1, that builds the solution set 
сф = W% = {M*b}. 
(9.35) 
based on the results of a multi-query local computation procedure. However, remem-
ber that the combination rule for quasi-regular valuation algebras and symmetric, 
positive definite systems are identical, see equations (6.30) and (9.15). For symmet-
ric, positive definite systems we know from Lemma 9.3 that they also satisfy the 
additional condition of Theorem 8.3 for solution construction based on the results of 
a single-query architecture. This property is only based on the combination operator 
and therefore also holds for quasi-regular valuation algebras. 
Lemma 9.4 Quasi-regular valuation algebras satisfy the property that for all ipi,ip2 G 
Ф with d(ipi ) = s, d{4>2) = t, sÇuÇsUt 
and u-vector x we have 
^;2
nvuni) = 
Щ^ФМ)-
Hence, we can conclude from Section 8.2.3 that Algorithm 8.4 returns the solu-
tion to a factorized, quasi-regular fixpoint equation system based on the results of 
a single-query local computation architecture. The actual process is very similar to 
the case of symmetric, positive definite systems, which we considered in detail in 
Section 9.2.2. 
To determine the complexity of solving fixpoint equation systems by local com-
putation, we first remind that projection is the more time consuming operation than 
combination. For ф = (M, b) G Ф with ά(φ) = s and i Ç s , the projection rule for 
quasi-regular valuations, given in equation (6.23), is 
(M,b)"l'i 
= 
(M*-*'1 + 
Mi't's~t(Mi-s~t's~t)*Mi's~t't, 
Ы-* + M~''i's_i(M"''s_i's_t)*b'''s_iN) 
includes the evaluation of the star operation for matrices and this has a cubic time 
complexity in the general case, as shown in Section 9.3.1. However, let us now rewrite 
this operation with variable elimination instead of projection. If t = s — {Z} for some 
Z G s, the matrix component of this operation becomes 
which requires computing 
M(X, Y) + M{X, Z)M(Z, Z)*M(Z, Y) 
for all X, Y G s — {Z}. This expression does not apply the star operation to matrices 
anymore. Since the same holds for the vector component, the operation of variable 

SEMIRING FIXPOINT EQUATION SYSTEMS 
371 
elimination adopts a quadratic time complexity. This motivates the application of the 
fusion algorithm instead of collect. Moreover, the size of the valuation stored in a 
join tree node is bounded by the treewidth ω* which then also gives an upper bound 
for the time complexity of the operations of combination and variable elimination. 
To sum it up, the time and space complexity of applying the fusion algorithm to 
quasi-regular valuations is 
The single-target path problem with target node T e s is represented by the fixpoint 
equation X = MX + b T with b(T) = 1 and Ь(У) = 0 for all Y e s - {Г}. Its 
solution x = M*bx corresponds to the column of the matrix M* that corresponds to 
the variable T. If we solve this problem for all possible target nodes, we completely 
determine the matrix M* and thus solve the all-pairs path problem. This is possible 
with | s | repetitions of the above local computation procedure using the same matrix M 
but different vectors. In other words, we have a prototype application for a compilation 
approach such as the LDR-decomposition for regular systems in Section 9.1.4 or the 
LDLT-decomposition for symmetric, positive definite systems in Section 9.2.3. In 
fact, it was shown by (Backhouse & Carré, 1975) that LDR-decomposition can also 
be applied to regular algebras, which are quasi-regular semirings with idempotent 
addition. This approach was later generalized by (Radhakrishnan et al., 1992) to 
quasi-regular semirings. We propose the development of LDR-decomposition for 
matrices with values from a quasi-regular semiring as Exercise 1.2 to the reader. In 
the following section, we focus on an alternative approach based on Kleene valuation 
algebras that computes some selected values of the quasi-inverse matrix M* directly. 
This corresponds to the multiple-pairs algebraic path problem which does not assume 
specific source or target nodes. 
9.3.3 
Local Computation with Kleene Valuations 
For an index set r = {1,..., n} and s Ç r, the solution to the all-pairs algebraic 
path problem M* defined by the matrix M £ M{A, s) is a solution to the fixpoint 
system X = MX + 1 . Here, X is an s x s matrix of variables, M an s x s matrix of 
semiring values and I the unit matrix in M(A, s). We further assume in this section 
that the semiring {A, +, x, *, 0,1) is a Kleene algebra according to Definition 6.6. It 
then follows from Theorem 6.5 that the set of labeled matrices M(A, s) also forms 
a Kleene algebra, and we obtain its induced Kleene valuation algebra (Ф, D) by 
defining D — V{r) and 
Ф = 
{ N * | N e M ( A , s ) a n d s e D } 
as shown in Theorem 6.6. Consequently, we may consider the solution M* to the 
all-pairs path problem over a Kleene algebra as an element of a valuation algebra. 
With regard to inference problems, we declare this element as the objective function 
and assume a knowledgebase {M*,..., M£} С Ф such that 
M * = MI ® · · · ® M ; 

372 
SPARSE MATRIX TECHNIQUES 
This corresponds to the situation of having an existing factorization for the path 
problem solution. Alternatively, we could take a factorization of the original matrix 
M and start from a set of matrices {Mi,..., M m } with M, € M(A, s,) and Si Ç s 
for г = 1,..., m such that 
M = М\' + ... + М%. 
(9.36) 
This second factorization represents in the example of the previous section the local 
road maps for each country, whereas the elements of the first factorization correspond 
to tables of shortest distances that already exist for each country. Although probably 
more realistic, the second factorization cannot be considered as a knowledgebase 
directly, because the factors Mi are generally not contained in Ф. But as this example 
foreshadows, it proves sufficient to compute the closure of each factor in order to 
obtain a knowledgebase that factors M* as a consequence of Lemma 6.11 and 6.17: 
M* = 
(ΜΪ* + ··· + Μ ^ ) * 
— 
( 7 ( M Î S l U S 2 + M T S l U S 2)T si U s2US3 _|_ 
\f»iU...Ui m _j_ jyjtsiU...Us mV 
= 
( 7 ( M * ^ S l U S 2 + M*^ S l U S 2)* T S l U S 2 U s 3 + 
\*T«iU...Usm , jyj*TSiU...Uam 
= 
MÏ (Э ■ ■ · ® M ^ . 
The last equality follows from the definition of combination in Kleene valuation 
algebras given in equation (6.38). Thus, we may either start from a factorization of the 
path problem solution, which directly gives a knowledgebase of Kleene valuations, 
or produce the latter from a factorization of the path problem input matrix. Note 
that the second case includes the type of factorization that was considered for quasi-
regular valuation algebras in equation (9.31). Alternatively, if no natural factorization 
of either the closure or the input matrix exists, we may exploit the idempotency 
of semiring addition to produce a decomposition of the matrix M € Λί(Α, s) that 
satisfies equation (9.36): For X, Y € s and X 
φΥ, 
• if M(X, Y) φ 0 or M{Y, X) φ 0, add M^ x- y> to the factorization; 
• if M(X, X) φ 0, add M + W to the factorization. 
This creates a factorization of minimal granularity where the domain of each factor 
contains at most two variables. Note also that the same diagonal value M(X, X) 
possibly occurs in multiple factors. This, however, does not matter as a consequence of 
idempotent semiring addition in a Kleene algebra. Moreover, if the Kleene algebra is a 
bounded semiring with the property a +1 = 1 for all a € A, it follows from equation 
(9.29) that M*(X, X) = 1 for allX G s and independently of the original values 
of M(X, X). In this case, we can ignore the diagonal elements in the decomposition 
process. Also, the semantics of path problems often yield input matrices where most of 
the diagonal elements are M(X, X) = 1. Then, the decomposition process also leads 
to factors that share this property, and the following lemma shows that these factors 

SEMIRING FIXPOINT EQUATION SYSTEMS 
3 7 3 
are equal to their closures. This simplifies the process of building a knowledgebase 
from the decomposition process considerably, as illustrated in Example 9.5. 
Lemma 9.5 In a bounded Kleene algebra we have 
(9.37) 
Proof: We apply equation (6.32) and remark that the property of boundedness 
implies f = b + ce*d = 1. It follows from Property (KP3) in Lemma 6.8 that 
/ = /* = 1, and the statement then follows directly. 
■ 
Example 9.5 Consider the variable set s = {Berlin, Paris, Rome, Berne} and 
the following sparse matrix defined over the tropical semiring of non-negative integers 
(NU {0, oo}, min, +, oo, 0) expressing the path lengths between some selected cities: 
M 
This table might be seen as the direct distances in a road map, where the symbol oo 
fags unknown distances or the absence of a direct connection in the road map. The 
matrix M then factorizes as 
Berlin 
Paris 
Rome 
Berne 
Berlin 
0 
1111 
1181 
oo 
Paris 
1111 
0 
oo 
436 
Rome 
1181 
oo 
0 
oo 
Berne 
oo 
436 
oo 
0 
M 
with factors 
ίjuri{Berlin,Paris}\ 
, 
/ -\jr\,{Berne,Paris} 
\ 
, 
f -bjc\.{Rome,Berlin} 
\ 
■ьж\.{ Berlin, 
Paris} 
ту/т4-{ ßerne, Paris} 
■л/г \.{ Rome, В er lin} 
Berlin 
Paris 
Berlin 
0 
1111 
Paris 
1111 
0 
1 
Berne 
Paris 
Berne 
0 
436 
Paris 
436 
0 
Rome 
Berlin 
Rome 
0 
1181 
Berlin 
1181 
0 
We further recall from Example 5.10 that the tropical semiring is bounded with the 
integer 0 as unit element. By Lemma 9.5 the above factors are equal to their closures 

3 7 4 
SPARSE MATRIX TECHNIQUES 
and therefore directly form a knowledgebase, i.e. a factorization of the yet unknown 
solution M*. We have 
Λ/Γ* 
TL*-l{Berlin,Paris} ^ тиг^.{Вете,Paris} ^> -\я^,{Н.оте,ВегИп} 
The second component of an inference problem is the query set. When solving 
the multiple-pairs path problem, we are interested in the semiring values M* (X, Y) 
for certain pairs of variables (X, Y) from the domain s = d(M* ) of the objective 
function. To extract these values, it is in fact not necessary to compute the objective 
function M* completely, but only the marginals Μ*^χ,γ} 
must be available. The 
queries of the inference problem are therefore given by sets of variables {X, Y} 
for non-diagonal entries and {X} for the diagonal entries of the objective function. 
Depending on the structure of the query set, we retrieve the classical types of path 
problems: when the query set consists of a single element {X, Y}, the inference 
problem corresponds to a single-pair algebraic path problem. If the query set if 
formed by all possible pairs of variables {X, Y}, we solve the all-pairs algebraic 
path problem. If for a selected variable X E s , the query set contains the pairs {X, Y} 
for all nodes Y £ s with X ψ Y, we find the single-source algebraic path problem. 
Note that since we consider sets of variables, this is equivalent to the single-target 
algebraic path problem. Finally, if the non-empty query set has no such structure, we 
refer to the inference problem as the multiple-pairs algebraic path problem. It will 
later be shown in this section that we can completely pass on the explicit definition 
of query sets when dealing with Kleene valuation algebras. This, however, requires 
some complexity considerations with respect to the local computation process. Until 
then, we assume such an explicit query set as part of the inference problem and next 
focus on the local computation based solution process. 
Given an inference problem with a knowledgebase of Kleene valuations and a 
query set, its solution can be delegated to one of the generic local computation 
methods introduced in Chapter 4. Since Kleene valuation algebras are idempotent, 
we go for the particularly simple idempotent architecture of Section 4.5. We first 
construct a join tree (V, E, \, D) that covers the inference problem according to 
Definition 3.8 and execute the chosen architecture. At then end of the message-
passing, each join tree node i G V contains M*-^^) as a consequence of Theorem 
4.4. The result of each query {X, Y} is finally obtained by finding a covering node 
i e V with {X, Y} Ç А(г) and performing one last projection per query 
, 
Λ1{Χ,Υ} 
, 
i l w n N - i - W > 
ÎM*J 
= ÎM*iÀWJ 
(9.38) 
This shows how local computation solves multiple-pairs path problems represented 
as inference problems over Kleene valuation algebras. Let us next consider the 
complexity of this approach in more detail: since Kleene valuations are labeled 
matrices, the space requirement to store a valuation M* with domain d(M*) = s is 
C(|s|2). Concerning the time complexity of the valuation algebra operations, we first 
remark that combination is the more expensive operation. Projection only drops rows 

SEMIRING FIXPOINT EQUATION SYSTEMS 
3 7 5 
and columns, whereas the combination of two Kleene valuations M* and Mj with 
domains d(MJ) = s and ^(M^) = t performs a component-wise addition of the 
two matrices and computes the closure of the result. Applying the Warshall-Floyd-
Kleene algorithm of Section 9.3.1 therefore gives a time-complexity of ö(\s U i|3) 
for the operation of combination. Based on the time and space complexity of Kleene 
valuations and the generic complexity analysis for the idempotent architecture in 
Section 4.5.1, we conclude that the message-passing process adopts a total time and 
space complexity of 
ö(|V|-(w* + l) 3) 
and 
o(\V\ ■ (ω* + l) 2), 
(9.39) 
where ω* denotes the treewidth of the inference problem and | V\ the number of join 
tree nodes. This may be compared with a direct computation of M* that adopts a time 
and space complexity of ö(|s| 3) and C(|s|2) respectively. Since it is always possible 
to apply the fusion and bucket elimination algorithms to knowledgebases of Kleene 
valuations, we may assume that |V| « |s| when comparing the two approaches. 
Depending on the treewidth and thus on the sparsity of the input matrix, the difference 
to the local computation complexity of equation (9.39) may be considerable, as shown 
in the following example. 
Example 9.6 Assume that we are charged with the task of computing the travelling 
distances of packets for a European express delivery service. Two different rules apply 
for national and international shipping: In order to guarantee fast delivery, packets 
are transported from the source to the target city directly, if both cities are in the 
same country. Otherwise, packets are first transported to an international distribution 
center. For simplicity, we assume that only one distribution center exists per country. 
If we for example send a packet from a Portuguese city to a French city, the packet is 
first transported to the Portuguese distribution center, then to the Spanish distribution 
center, then to the French distribution center and finally to the French address. It is 
furthermore assumed that the delivery service always chooses the shortest travelling 
distance and that packets are transported by car or train. The data of this example 
consists of a local map for each country including its distribution center and the 99 
largest cities and villages. Hence, if we include 43 European countries with a common 
land frontier to another European country, we have \s\ — 4300. In addition, the local 
road maps contain only the direct distances between neighboring cities and villages 
in the corresponding country and the direct distances between its own distribution 
center and the distribution centers of all neighboring countries with a common land 
frontier. If we model these maps as valuations over the tropical semiring of non-
negative integers, we obtain a knowledgebase of 43 valuations. The 9 neighboring 
countries of Germany are never exceeded in our example, which implies that the 
German map contains exactly 9 international distances. The largest domain of all 
valuations in the knowledgebase is therefore 109. Let us compare different approaches 
for the computation of the required distances. First, we could unify all local road maps 
to a single European road map and compute its closure. The complexity ofO( \s\3)for 
computing closures by the Warshall-Floyd-Kleene algorithm gives us an estimation 

3 7 6 
SPARSE MATRIX TECHNIQUES 
of 43003 = 79'507'0Ό0'0ΰΌ operations for this approach. A more intelligent way 
of solving this problem exploits the observation that it is in fact sufficient to know 
all shortest distances between the distribution centers. An international distance 
between two cities can then be computed by adding the shortest distances to their 
corresponding distribution centers and the distance between the two distribution 
centers. Hence, we create a European road map that contains only the distribution 
centers and compute its closure together with the closures of all local road maps for 
the national distances. Since the largest map has 109 cities, the computational effort 
of this approach is bounded by (43 + 1) · 1093 = 56'981'276 operations. This is a 
very rough estimation such that we can ignore the two additions needed to obtain 
an international distance. We observe that the second approach is more efficient 
because it exploits the problem structure. Finally, we could solve this problem by local 
computation. The OSLA-SC algorithm of Section 3.7 implemented in the NENOK 
framework (Pouly, 2010) returns a join tree with a treewidth ofu* + 1 — 109. This 
shows that the join tree identifies the same problem structure and local computation 
essentially mirrors the computations of the second approach. 
The complexity of solving a path problem by local computation depends on the 
treewidth of the inference problem, which also includes the query set. Query an-
swering according to equation (9.38) presupposes that for each query {X, Y}, it is 
possible to find some join tree node i € V that covers this query. This important as-
pect was ignored in Example 9.6. The join tree must therefore ensure that each query 
is covered by some node, and this blows up the node labels. In other words, the larger 
the query set is, the larger becomes the treewidth of the inference problem, and the 
less efficient is local computation. The worst-case scenario is the all-pairs algebraic 
path problem where the query set is built from all possible pairs of variables. Every 
covering join tree for such an inference problem contains at least one node whose 
label equals the domain of the objective function. We thus have ω* + 1 = \s\ and the 
message-passing process essentially mirrors the direct computation of the objective 
function. Conversely, if the query set is empty (or if each query is covered by some 
knowledgebase factor), the treewidth depends only on the sparsity of the matrix, 
which results in the best possible performance of local computation. From this point 
of view, the query set may destroy the gain of the sparsity in the knowledgebase. 
However, there are two properties of Kleene valuation algebras that allow us to ignore 
the query set and benefit from a treewidth that only depends on the knowledgebase. 
These are the polynomial complexity of Kleene valuations and the property of idem-
potency. Section 4.6 and in particular Algorithm 4.7 propose a general procedure for 
the computation of uncovered queries based on the idempotent architecture. For a 
given inference problem, it is thus sufficient to find a covering join tree for the knowl-
edgebase and execute a complete run of the idempotent architecture. This results in 
a join tree compilation of the path problem, from which the unconsidered queries 
can be computed by Algorithm 4.7: For each query {X, Y}, this algorithm searches 
two join tree nodes whose labels contain the variables X and Y respectively. Then, it 
computes the query by successively combining and projecting the neighboring node 
contents on the path between the two nodes. We observed in Section 4.6.1 that the 

SEMIRING FIXPOINT EQUATION SYSTEMS 
3 7 7 
complexity of this algorithm doubles the treewidth. Its time complexity is 
θ(\ν\·(2(ω* 
+ 1))3) = c(|V|-8(w* + l) 3) = o(\V\ 
· (ω* + l) 3). 
Similarly, we obtain for the space complexity 
ο((2(ω* + 1))2) = ο(4(ω* + 1)2) = ο((ω* + 1)2) 
since only one such factor has to be kept in memory. Repeating this procedure for 
each query provides the better option for two reasons: first, we keep the smallest pos-
sible treewidth that only depends on the knowledgebase and second, we can answer 
new incoming queries dynamically without reconstructing the join tree. In addition, 
this procedure does not presume any structure in the query set which is contrary to 
many other approaches for the solution of path problems. On the other hand, it is 
clear that solving the all-pairs algebraic path problem with | V|2/2 different queries 
still exceeds the effort of computing the objective function directly. Nevertheless, it 
is an efficient method for the solution of sparse multi-pairs algebraic path problems 
over Kleene algebras with a moderate number of queries. 
A further query answering strategy is pointed out by the compilation algorithm 4.8 
for uncovered queries in Section 4.6. Assume, for example, that we want to solve the 
single-source problem for a knowledgebase of Kleene valuations. We first execute a 
complete run of the idempotent architecture to obtain the join tree compilation. Then, 
if S € s denotes the selected source variable, we search a node i GV with S £ X(i). 
Algorithm 4.8 then computes a new factorization such that each node j € V contains 
the projection of M* relative to \(i) U \(j). This corresponds to adding the node 
label X(i) to each join tree node and is sometimes referred to as distance tree. For an 
arbitrary variable T 6 À(j'), we thus obtain the two values M*(S, T) and M*(T, S) 
from the extended domain of node j 6 V. In other words, this algorithm solves 
the single-source and single-target path problem simultaneously with the same time 
complexity of answering a single query with Algorithm 4.7. In fact, applied to the 
shortest distance problem, this procedure corresponds to the classical construction of 
a shortest path tree rooted at variable S and confirms the common knowledge that 
solving a single-source path problem using Dijkstra's algorithm (Dijkstra, 1959) or 
the Bellman-Ford algorithm (Ford, 1956; Bellman, 1958) does not take more effort 
than computing a single-pair path problem. A similar way of deriving a distance 
tree from a join tree factorization but limited to shortest distances is proposed in 
(Chaudhuri & Zaroliagis, 1997). Finally, we could tackle the all-pairs algebraic path 
problem by solving the single-source problem for each variable. This corresponds to 
|s| и |V| repetitions of Algorithm 4.8 on the same propagated join tree that results 
in a time complexity of 
ο(\ν\2-(ω* + ΐ)ή. 
For extremely sparse matrices, i.e. if the number of non-zero values is 0({/|VJ), 
this might be more efficient than the direct computation of the objective function. 

378 
SPARSE MATRIX TECHNIQUES 
On the other hand, it is improbable that a better time complexity can be reached for 
the all-pairs problem based on Kleene valuation algebras. We need to compute \V\2 
different values and the combination rule requires the execution of a closure in each 
join tree node. However, we have seen in Section 9.3.2 that quasi-regular valuation 
algebras provide a more efficient way to solve the all-pairs problem. There are still 
\V\2 values to be computed, but since the closure is part of the projection rale, the 
corresponding effort can further be reduced. 
Finding Paths in Kleene Valuation Algebras 
Chapter 6 presented several applications of path problems that are not related to 
graphs. Here, we return to the very first conception of path problems in weighted, 
directed graphs represented by an adjacency matrix M with values from a Kleene 
algebra (A, +, x, *, 0,1). For a pair (X, Y) of variables the value M*(X, Y) e A 
corresponds to the weight of the optimum path between X and Y, where the exact 
criterion of optimality depends on the Kleene algebra. However, Kleene algebras 
are idempotent semirings with the canonical partial order of equation (5.4). Property 
(SP4) in Lemma 5.5 then states that a + b = sup{o, b} for all a, b € A. If furthermore 
the Kleene algebra is totally ordered, we conclude that a + b = max{a, b}. A formal 
proof of this statement is given in Lemma 8.5. Computing the sum of all path weights 
between X and Y therefore amounts to choosing the maximum value among them 
with respect to the canonical semiring order. In other words, the algorithms for the 
computation of a matrix closure can be interpreted as a search algorithm under this 
setting. In addition, we also conclude that if M* (X, Y) is the weight of the optimum 
path evaluated by comparing path weights, we can always find a path between X 
and Y with this total weight. Example 9.7 shows that this does not hold in partially 
ordered semirings. 
Example 9.7 We know from Section 5.1.1 that multi-dimensional semirings with a 
component-wise definition of addition and multiplication again form a semiring. We 
thus consider the two-dimensional semiring with values from the tropical semiring 
(NU {0, oo}, min, +, oo, 0) of non-negative integers. An example of a directed graph 
with values from this semiring is shown in Figure 9.9. There are two possible paths 
connecting the source node S with the terminal T. Evaluating the total path weights 
by multiplying their edge weights gives: (1, 2) x (6, 2) = (7,4) and (3,4) x (1, 5) = 
(4,9). Finally, we solve the path problem by adding the path weights and obtain 
(7,4) + (4,9) = (4,4). There is no path between S and T whose weight equals the 
computed optimum path weight, because the multi-dimensional, tropical semiring is 
only partially ordered. 
For totally ordered Kleene algebras, the Warshall-Floyd-Kleene algorithm of Sec-
tion 9.3.1 can be extended to build a so-called predecessor matrix simultaneously to 
the computation of the matrix closure (Aho et al, 1974). For a pair of variables (S, T) 
the predecessor matrix P determines the variable X = P(S, T) that is the immediate 
predecessor of variable T on the optimum path from S to T. The corresponding 
extension of Algorithm 9.1 is shown in Algorithm 9.2: 

SEMIRING FIXPOINT EQUATION SYSTEMS 
3 7 9 
Figure 9.9 A path problem with values from a partially ordered semiring. 
Algorithm 9.2 The Warshall-Floyd-Kleene Algorithm with Predecessor Matrix 
input: MeM(A,s) 
output: M*, P 
begin 
foreach X,Y € s do 
/ / 
I n i t i a l i z a t i o n . 
Мо(Х,У) := M(X,Y) 
P(X,Y) 
:= X; 
end; 
for each Z = 1... |s| do 
for each X,Y £ s do 
с := Mz_i(A-,Z)(Mz_i(Z,Z))*Mz_i(Z,y); 
MZ(X,Y) 
: = М2_!(Л:,У) + с; 
if MZ(X,Y) 
φΜ.Ζ-ι{Χ,Υ) 
do 
Р(Х,У) := 
P(Z,Y); 
end; 
end; 
end; 
foreach X e s do 
// Addition of unit matrix. 
if 1 > M Z(X,X) do 
M Z(X,X) := 1; 
P(X,X) 
:= X; 
end; 
end; 
return MZ/ P; 
end 
This algorithm still performs the same computations for the closure matrix. Be-
cause addition corresponds to maximization with respect to the canonical semiring 
order, the value M.z(X,Y) 
in the middle block is set to the larger value between 
M z _ i ( I , Y) and с In the second case, it means that a better path exists from vari-
able X to variable Y through the intermediate variable Z. We therefore update the 
predecessor of variable Y according to this new path. Finally, the addition of the 
matrix M.z with the unit matrix I s in the last block affects the values M^(X, X) 
only if 1 > Mz(X,X). 
It is easy to see that both variations of the Warshall-
Floyd-Kleene algorithm adopt equal time and space complexity. The optimum path 

380 
SPARSE MATRIX TECHNIQUES 
can then be constructed from the predecessor matrix by recursively enumerating all 
predecessors. This is illustrated in Example 9.8. 
Example 9.8 We take the adjacency matrix of Example 6.2 defined over the tropical 
semiring of non-negative integers. Here, we identify the graph nodes with letters 
instead of numbers to prevent confusions with the semiring values. We obtain 
A 
В 
С 
D 
A 
0 
18 
12 
5 
В 
9 
0 
21 
14 
С 
8 
6 
0 
13 
D 
15 
13 
7 
0 
А 
В 
С 
D 
А 
оо 
оо 
оо 
5 
В 
9 
оо 
оо 
оо 
С 
8 
6 
оо 
оо 
D 
оо 
оо 
7 
оо 
The predecessor matrix obtained from Algorithm 9.2 is 
A 
В 
С 
D 
A 
A 
D 
D 
D 
В 
A 
В 
A 
A 
С 
A 
В 
С 
A 
D 
С 
С 
С 
D 
Let us now look for the shortest path from С to В which, according to the closure 
matrix, has shortest distance 21. We obtain from the predecessor matrix P(C, B) = 
A. This means that the last visited node just before reaching В on the path from С 
to В is A. We proceed recursively and obtain P(C, A) = D for the predecessor of 
A. Finally, P(C, D) = С means that the direct path between С and D also has the 
shortest distance. Putting things together, the shortest path with distance 21 is 
p = 
(C,D)(D,A)(A,B). 
Given a single matrix with values from a totally ordered Kleene algebra, the 
extended version of the Warshall-Floyd-Kleene algorithm computes the closure and 
its associated predecessor matrix with the same computational effort. Based on the 
predecessor matrix, it is then possible to recursively enumerate each path (X, Y) 
in linear time to the total number of variables. However, the advantage of using 
Kleene valuations and local computation for the solution of path problems is the 
abdication of the explicitly computation of the total matrix and its closure. When we 
are interested in paths, we must apply a similar strategy to deduce total paths only from 
the predecessor matrices of smaller subgraphs. For that purpose, we subsequently 
assume that Kleene valuations consist of pairs of closure and predecessor matrices and 
startfromaknowledgebase{(/>i,... ,фп} withφι = (M*, P;) wherePj corresponds 
to the predecessor matrix of M* obtained from Algorithm 9.2. We define the domain 
of фг by ά{φί) = d(M*). The projection rule still corresponds to matrix restriction 
and is applied to both components separately. Here, it is important to note that the 
values of the projected predecessor matrix may refer to variables that have been 
eliminated from the domain of the valuation. The combination is executed in the 

SEMIRING FIXPOINT EQUATION SYSTEMS 
3 8 1 
usual way for the closure component. It consists of a component-wise addition of the 
two extended matrices, followed by the execution of a closure operation. Since we are 
dealing with a totally ordered Kleene algebra, the component-wise addition of two 
matrices corresponds to the component-wise maximum, which allows us to initialize 
the predecessor matrix of the combination. For φ = (MJ, Pi) and φ = (Μ^, Рг) 
with ά(φ) = s, ά(φ) = t and X, Y e ά(φ) U ά(φ), we compute 
Μ(Χ,Υ) 
M?sUt(x,Y) + м?зиг (Χ,Υ) 
and 
P(X,Y) 
Ρι(Χ,Υ) 
ίΐΧ,Υ esniandM^(X,y) > Щ(Х,У), 
P2(X,Y) 
ifX,Y €sniandMÏ(X,y) < Щ(Х,У), 
Ρι(Χ,Υ) 
ifX,Yes-t, 
P2(X,Y) 
if 
X,Y€t-s, 
X 
otherwise. 
The combined valuation ф <g> ф is then obtained by computing the closure of M with 
P replacing the initialization of the predecessor matrix in Algorithm 9.2. We now 
observe that this produces a correct predecessor matrix: if computing the closure 
shows no effect, i.e. if M* {X, Y) = M(X, Y), then P(X, Y) either corresponds to 
Pi(X, Y), P2(X, Y) or X. In the first two cases, the correctness follows from the 
assumption that ф and ф are pairs of closures and their corresponding predecessor 
matrices. The third case is equal to the initialization in Algorithm 9.2. If on the other 
hand M.*(X, Y) ф M.(X, Y), then some better path was found that goes through 
some variable Z € sUt. The algorithm then sets P(X,Y) 
= P(Z,Y) 
and the 
correctness follows by induction. 
Let us now summarize the complete process of computing an optimum path from S 
to T by local computation. We start from a factorization of Kleene valuations extended 
by their predecessor matrices, build a covering join tree for this knowledgebase and 
execute a complete run of the idempotent architecture. We then answer the single-
source problem for the variable S by the compilation approach presented above. If ф 
denotes the objective function, we answer the query 
φϋβ,τ} 
= 
(M*^S>T\ 
Р-И5^}4) 
from which we obtain the optimum path weight M*(5, Г) and the predecessor 
Z = P(S,T) of T. We proceed recursively by computing the query ф^*2} and 
finally obtain the complete path by at most |V| repetitions of this process. It is 
important to note that once the compilation for the single-source problem is available, 
no more computations in the valuation algebra are necessary. We thus obtain the path 
from S to T with a complexity equal to just computing the path weight. 

382 
SPARSE MATRIX TECHNIQUES 
9.4 
CONCLUSION 
The first part of this chapter deals with sparse, linear systems AX = b with values 
from a field. Exploiting the sparsity of the matrix A means to limit the number of 
zero values that change to a non-zero value during the computations. These values 
are called fill-ins. If no additional structure of the matrix A is present, then ordinary 
Gaussian elimination with pivoting is performed. This corresponds to computing in 
the valuation algebra of affine spaces, where the equations are considered as finite 
representations of the latter. The equations are distributed over the nodes of a join tree, 
such that the non-zero values in the matrix A are covered by the join tree nodes. Since 
only these values are involved in the computations, fill-ins are controlled by the join 
tree. Similar considerations for regular systems lead to LDR-decomposition based 
on local computation. A second valuation algebra, which is isomorphic to Gaussian 
potentials, was derived for the case of symmetric, positive definite matrices. Here, the 
valuations are no more ordinary equations, but they are symmetric, positive definite 
subsystems themselves. It was shown that such factorizations can either be produced 
from the matrix, or they occur naturally from specific applications as for example for 
the normal equations in the least squares method. Local computation with symmetric, 
positive definite systems required replacing the ordinary distribute phase by a solution 
construction process. The second part of this chapter focused on fixpoint equation 
systems X = AX + b with values from a quasi-regular semiring. Such equations 
frequently occur in path problems and their local computation based solution uses 
quasi-regular valuation algebras, whose structure is very similar to symmetric, posi-
tive definite systems. Consequently, we have a similar solution construction process 
that follows the usual collect phase of local computation. The second approach was 
based on Kleene valuation algebras and can be applied to fixpoint systems with values 
from a Kleene algebra. The application of the idempotent architecture to such knowl-
edgebases lead to a compilation of the path problem into a join tree that qualifies for 
online query answering. 
PROBLEM SETS AND EXERCISES 
1.1 * * We observed in the introduction of Section 9.3.2 that the valuation algebra 
of linear systems with symmetric, positive definite matrices and the quasi-regular 
valuation algebra have a very similar structure. Given a symmetric, positive definite 
system that represents the objective function φ, it was shown in Section 9.2.4 how 
a possible decomposition φ = φι ® ... <8> фт can be found. Repeat these consid-
erations for quasi-regular valuation algebras, where the matrices are not necessarily 
symmetric anymore. 
1.2 * * Section 9.3.2 discusses local computation with quasi-regular valuation al-
gebras. This process is very similar to local computation with symmetric, positive 
definite systems in Section 9.2.2, from which we derived LDLT-decomposition in 

EXERCISES 
383 
Section 9.2.3. Show that LDR-decomposition based on local computation is also 
possible for semiring fixpoint equation systems and thus for quasi-regular valuations. 
Indication: It is shown in (Backhouse & Carré, 1975) that LDR-decomposition is 
possible for quasi-regular semirings with idempotent addition. This approach is gen-
eralized to quasi-regular semirings in (Radhakrishnan et al, 1992). 
1.3 * * In graph related path problems we are often not only interested in the so-
lution of the path problem (e.g. the shortest distance) but also in finding a path that 
adopts this optimum value (e.g. the shortest paths). At the end of Section 9.3.3 this 
problem was addressed for Kleene valuation algebras. Provide a similar method that 
computes paths for quasi-regular valuation algebras. 
1.4 * * In the least squares method of Section 9.2.5 the residues are often weighed. 
a) Derive the normal equations, if residue Dj has weight σ^ such that 
is to be minimized. 
b) More generally, derive the normal equations if D T E D is to be minimized. 
The weight matrix Σ is a positive definite, symmetric matrix. 
c) Consider a join tree decomposition of the equations. What conditions must 
Σ satisfy in order that the normal equations can be solved by local compu-
tation on the join tree? 
1.5 * * Reconsider the linear dynamic system from Instance 9.2: 
a) Add the equation H3X3 = уз to the equations in Instance 9.2. How does 
the estimate of X3 change? This gives the filter solution for X3. 
b) Treat this system in the general case for time г = 1,..., n and derive the 
one-step prediction and the filter solution for X^. 
c) Assume in the dynamic equations and the measurement equations that 
the residues D^ and E^ are weighed with weight matrices Qi and Ri (all 
positive definite and symmetric). Derive the filter and smoothing solutions 
in this case. 
d) Note that the solution derived so far works if all Ai are regular. No assump-
tion about the rank of Hj is needed. Can you extend the valuation algebra 
of Section 9.2.1 for non-negative definite, symmetric matrices? Indication: 
Take (A, Ab) as valuations instead of (A, b). Similar algebras will be 
studied in Chapter 10. 
1.6 * * 
The equations of the linear dynamic system in Instance 9.2 may also be 
differently assigned to a join tree: add a node XQ to the left of the join tree in Figure 

3 8 4 
SPARSE MATRIX TECHNIQUES 
9.8 and put the equation H0Xo on this node. On the next node Χο,Χι put the 
equations AoXo — Xi = 0 and HiXiyi and so forth. 
a) Derive the filter solution with this set-up. 
b) Derive the smoothing solutions with this set-up. 
1.7 * * * The exact algebraic relationship between the quasi-regular valuation alge-
bra of Section 6.4 and the symmetric, positive definite valuation algebra of Section 
9.2 is yet unknown. The valuations both have a very similar structure, share the same 
combination rule and also have a related projection rule. On the other hand, there are 
also important differences, e.g. the quasi-regular valuation algebra provides neutral 
elements which is not the case in the other algebra. Try to find a common theory for 
the two algebras. 

CHAPTER 10 
GAUSSIAN INFORMATION 
In this chapter we look at linear systems with stochastic disturbances. Imagine, for 
example, that we want to determine a physical state of a certain object by repeated 
measurement. Then, the observed measurement results are always composed of the 
real state and some unknown measurement errors. In many important applications, 
however, these stochastic disturbances may be assumed to have a normal or Gaussian 
distribution. Together with accompanying observations, such a system forms what 
we call a Gaussian information. We are going to discuss in this chapter how inference 
from Gaussian information can be carried out. This leads to a compact representation 
of Gaussian information in the form of Gaussian potentials, and it will be shown that 
these potentials form a valuation algebra. We may therefore apply local computation 
for the inference process which exploits the structure of the Gaussian information. 
This chapter is largely based on (Eichenberger, 2009). 
Section 10.1 defines an important form of Gaussian information that is used for 
assumption-based reasoning. This results in a particular stochastic structure, called 
precise Gaussian hint, which is closely related to the Gaussian potentials of Instance 
1.6. In fact it provides meaning to these potentials and shows that combination and 
projection of Gaussian potentials reflect natural operations on the original Gaussian 
Generic Inference. A Unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc. 
385 

3 8 6 
GAUSSIAN INFORMATION 
information. This gives sense to the valuation algebra of Gaussian potentials that 
was introduced in Instance 1.6 on a purely algebraic level. Section 10.2 starts with an 
illustrative example to motivate a necessary generalization of the concept of Gaussian 
information and also indicates how inference can be adapted to this more general form. 
Then, Section 10.3 rigorously carries through the sketched approach, which finally 
results in an important extension of the valuation algebra of Gaussian potentials. 
Section 10.2 will also be developed in another direction in Section 10.6. Here, we 
show that the extended valuation algebra of Gaussian potentials serves to compute 
Gaussian networks. Finally, Section 10.5 applies the results of this chapter to the 
important problems of filtering, smoothing and prediction in Gaussian time-discrete 
dynamic systems. 
10.1 GAUSSIAN SYSTEMS AND POTENTIALS 
In this section, we consider models that explain how certain parameters generate 
certain output values or observations. In a simple but still very important case, we may 
assume that they do this in a linear manner, which means that the output values are 
linear combinations of the parameters. Often, there are additional disturbance terms 
that also influence the output. Let us call the parameters xi,..., 
xn, the observed 
output values z\,...,zm 
and let the term which influences observation Zi be Wi. The 
following model describes how the parameters generate the output values 
n 
i=l 
where j = 1,..., m. This is often called a linear functional model. Then, the prob-
lem considered in this section is the following: Given this functional model and 
m observations ζχ,... ,zm, try to determine the unknown values of the parame-
ters xi,..., 
xm. Since the parameters are unknown, we replace them by variables 
Xi,..., 
Xm and obtain for j = 1,..., m the system of linear equations 
n 
"Y^dj^Xi+uij = Zj. 
(10.1) 
i=l 
This system rarely has a unique solution for the unknown variables, and it is often 
assumed that the system is overdetermined, i.e. the number of equations exceeds the 
number of unknowns, m > n. The method of least squares, presented in Section 
9.2.5, provides a popular approach to this problem. It determines the unknowns 
x i , . . . , xn by minimizing the sum of squares 
m 
n 
Σ(^·-Σα^Χί)2· 
j=l 
i=l 
We propose a different approach from least squares that yields more information 
about the unknowns, but at the price of an additional assumption: we assume that 

GAUSSIAN SYSTEMS AND POTENTIALS 
3 8 7 
the disturbances Uj have a Gaussian distribution with known parameters. For that 
purpose, it is convenient to change into matrix notation. Let A be the matrix with 
elements ajyi for г = 1,..., n and j = 1,..., m. X is the vector with components Xi 
and ω and z are the vectors with components ojj and Zj respectively. System (10.1) 
can then be written as 
ΑΧ + ω 
= 
z. 
(10.2) 
The vector of disturbances ω is assumed to have a Gaussian distribution as in equation 
(1.20) with the zero vector 0 as mean and an m x m concentration matrix K. We 
further assume an overdetermined system where m > n and also that matrix A has 
full column rank n. This setting is called a linear Gaussian system whereas more 
general systems are considered in Section 10.3. The following example of a linear 
Gaussian system is taken from (Pearl, 1988), see also (Kohlas & Monney, 2008; 
Eichenberger, 2009). 
Example 10.1 This example considers the causal model of Figure 10.1 for estimating 
the wholesale price of a car. There are observations of quantities that influence the 
wholesale price (i.e. production costs and marketing costs) and quantities that are 
influenced by the wholesale price (i.e. the selling prices of dealers). Besides, each 
observation has an associated Gaussian random term that simulates the variation of 
estimations and profits. We are interested in determining the wholesale price of a car 
given the final selling prices of dealers and the estimated costs for production and 
marketing. Let W denote the wholesale price of the car, U\ the production costs and 
U2 the marketing costs. The manufacturer's profit i/3 is influenced by a certain profit 
variation ω3. We thus have 
W 
= 
UI+U2 
+ 
U3+LU3. 
The selling prices Y\ and Y2 of two dealers are composed of the wholesale price and 
the mean profits Z\ and Z2 of each dealer. They are subject to variations ω\ and ω2: 
Υχ 
= 
W + Zi+U)!, 
Y2 
= 
W + 
Z2+ÜJ2. 
The manufacturer's production costs U\ are estimated by two independent experts. 
Both estimates I\ and I2 are affected by estimation errors V\ and v2: 
h 
= 
Ui+uu 
h 
= 
Ui + v2. 
The marketing costs U2 are also estimated by two independent marketing experts. 
Again, both estimates J\ and J2 are affected by errors Ai and λ2: 
Jj 
= 
U2 + \ u 
J2 
= 
U2 + X2. 

388 
GAUSSIAN INFORMATION 
We bring this system into the standard form: 
w 
w 
w -
f/i 
tfl 
- 
tfi 
-
u2 
u2 
- u2 
+ 
+ 
+ 
+ 
+ 
+ 
-
ωι 
ω2 
v\ 
v2 
λι 
A2 
ω3 
= 
= 
= 
= 
= 
= 
= 
Yi-
Y2-
h, 
h, 
л, 
</2, 
и3. 
-ζλ 
-ζ2 
This is an overdetermined system with m 
A has full column rank. 
7 and n = 3. The corresponding matrix 
Ui ; production costs 
■ 
■ 
li 
1 
' 
\2 . . 
— U2 : marketing costs 
W : wholesale price 
Us : industry profit 
ш : profit variation 
1 
■ 
Ys : selling price of 
dealer 2 
Z2 : mean profit 
ou : profit variation 
Figure 10.1 A causal model for the wholesale price of a car. 
The idea of assumption-based reasoning is the following-: although ω is unknown 
in equation (10.2), we may tentatively assume a value for it and see what can be 
inferred about the unknown parameter x under this assumption. First we note this 
system only has a solution if the vector z — ω is in the linear space spanned by the 
column vectors of A. The crucial observation of assumption-based reasoning is that 
all ω which do not satisfy this condition are excluded by the observation of z, since 
z has been generated by some parameter vector x according to the functional model 
(10.2). Thus, admissible disturbances ω are only those vectors for which (10.2) has 
a solution. In other words, the observation z defines an event 
{ω e 
: Эх such that ω 
Ax} 

GAUSSIAN SYSTEMS AND POTENTIALS 
3 8 9 
in the space of disturbances and we have to condition the distribution of ω on this 
event. The set vx is clearly an affine subspace of M.m. In order to compute this 
conditional distribution, we apply a linear transformation to the original system 
(10.2) which does not change its solutions. Since the rank of matrix A equals n, there 
must be n linearly independent rows in A. If necessary, we renumber the equations 
and can therefore assume that the first n rows are linearly independent. They form a 
submatrix Ai of A, and A2 denotes the submatrix formed by the remaining m — n 
rows. We then define the m x m matrix 
В 
0 
where Im_„ is the,(m — n) x (m — n) identity matrix. Clearly, В has full rank m 
and has therefore the inverse 
T = 
B - 1 
-А2АГ 
Let us now consider the transformed system TAX + Τω = Tz. Since the matrix 
T is regular this system has the same solutions as the original. This system can be 
written as 
X + ώι 
= 
Ax
 
lz\ 
ώ% = 
- Α 2 Α 7 1 ζ ι + ζ 2 
(10.3) 
where 
ωι 
ώ2 
= 
A j V , 
= 
- A 2 A j χωι + ω2 
and zi, ω\ and z2, ω2 are the subvectors of the first n and the remaining m — n 
components of z and ω respectively. The advantage of the transformed system (10.3) 
is that the solution is given explicitly as a function of ώ\, whereas ώ2 has a constant 
value. In this situation, the conditional density of ώι given ώ2 = — Α ^ ^ ω ι + ω2 
can be determined by classical results of Gaussian distributions (see appendix to this 
chapter). First, we note that the transformed disturbance vector ώ with the components 
ώχ and <D2 has still a Gaussian density with mean vector 0 and concentration matrix 
К 
= 
B J K B 
0 
Ά2 
Κι,ι 
K2.1 
Ki>2 
K2i2 
Ai 
A2 
Aj Κι,ι Ai + Aj Ki, 2A 2 + A j K 2 i l A ! 
Af K1)2 + AjK 2 ; 2 
, τ ^ 
A 
+ A 2 K 2 , 2 A 2 
K 2 iiAj + K2)2A2 
A T K A 
AfKi i 2 + А^Кг^ 
K 2 jiAi + K2>2A2 
K2,2 
К 2,2 

390 
GAUSSIAN INFORMATION 
The correctness of the first equality will be proved in Appendix J.2. Then, the 
conditional distribution of ώ\ given ώ2 = —A2A^:zi + z2 is still Gaussian and has 
concentration matrix A TKA (see Appendix J.2) and mean vector 
( A ^ A ) - 1 (AfKi, 2 + A^K2,2) ( A a A f V - z2) . 
Note now that according to (10.3) we have 
X 
= 
Aj"1zi — ώ\. 
Since the stochastic term ώ\ has a Gaussian distribution, the same holds for X. In 
fact, it has the same concentration matrix A TK A as ώ\. Its mean vector is 
A ^ Z l - ( A ^ A ) " 1 (AfK l i 2 + A^K2,2) (AaA^zi - z2) 
A ^ z i - (A TKA)- 1A TK 
A f V - (A TKA)" 1A TK 
0 
- 1 
A2AX zi - z2 
AiA x
 1zl - z i 
АгА^гх - z2 
A ^ z i - (A TKA)- 1(A TKA)A^ 1zi + (A TKA) _ 1A rKz 
= 
(A TKA)- 1A TKz. 
We thus obtain for the unknown parameter in equation (10.2) a Gaussian density or 
a Gaussian potential 
((A TKA)" LA JKz, A J K A ) · 
(10.4) 
Assumption-based reasoning therefore infers from an overdetermined linear Gaussian 
systems a Gaussian potential. How is this result to be interpreted? The unknown 
parameter vector x is finally not really a random variable. Assumption-based analysis 
simply infers that for a feasible assumption ω about the random distributions, the 
unknown parameter vector would take the value A^ xzi — ώ. But this expression 
has a Gaussian density with parameters as specified in the potential above. So, a 
hypothesis about x, like x < 0 for instance, is satisfied for all feasible assumptions 
ω satisfying Aj^zi — ώ < 0, and the probability that the assumptions satisfy this 
condition can be obtained from the Gaussian density above. This is the probability 
that the hypothesis can be derived from the model. The Gaussian density determined 
by the Gaussian potential is also called a fiducial distribution. The concept of fiducial 
probability goes back to the statistician Fisher (Fisher, 1950; Fisher, 1935). For its 
present interpretation in relation to assumption-based reasoning we refer to (Kohlas 
& Monney, 2008; Kohlas & Monney, 1995). 
Example 10.2 We first execute a diagnostic estimation/or the linear Gaussian system 
of Example 10.1. The term diagnostic means that observations are given with respect 
to the variables that are influenced by the wholesale price. These are the selling prices 

GAUSSIAN SYSTEMS AND POTENTIALS 
391 
and the mean profits of both dealers, and inference is performed on the disturbance 
vector ω— [ ω\ 
ω2 ]■ We thus consider the first two equations of the system: 
W + ωι 
W + ω2 
Yi-Zu 
Y2 — Z2. 
Ifo^i denotes the variance of the disturbance ω, associated to Yi we have 
о 
4-
and 
К = 
With respect to equation (10.4) we compute 
A T K A 
= 
and 
A TKz 
oil + σ12 
σ1^
σ12 
Υχ-Ζχ 
Y2 
Finally, we obtain the following Gaussian potential for the wholesale price W given 
Y\, Y2, Z\ and Z2 in the above system: 
G W\YuY2,Zi,Z2 
2 
1 
Yi-Zj 
Y2 
Example 10.3 Let us next execute a predictive estimation for the Gaussian system 
of Example 10.1. The term predictive means that observations are given with respect 
to the variables that influence the wholesale price. These are the production costs U\ 
and the marketing costs U2 which are estimated by two experts, and the industry profit 
Uz for which we have one estimation. We thus consider the remaining subsystem: 
Ui 
Ui 
Ui 
-
u2 
u2 
- u2 
+ 
+ 
+ 
+ 
-
V\ 
v2 
λι 
λ2 
ω3 
= h, 
= h, 
- Ji, 
= J2, 
= u3. 
W 
Observe that the last equation W = U\ + U2 + Uz + ω$ defines the wholesale price 
as a linear combination of the production costs, marketing costs and the industry 
profit. We may therefore conclude that the same holds for the Gaussian potential of 
the wholesale price given the estimations for the production costs, marteting costs 
and the industry profit. We have 
G W\h,I2,Ji,J2,U3 
- 
( l1w\h,i2,j-i,j2,u3, 
K-w\[. 
Ii,I2,Ji,J2,U3) 

392 
GAUSSIAN INFORMATION 
where 
and 
fiW\Ii,l2,Ji,J2,U3 
— 
Mt/l|/l,/2 + fXU2\Jl,J2 
+ 
^ 3 
K-W\Ii,I2,Ji,J2,U3 
— 
^-Ui\h,I2 
+ ^-U2\Ji,J2 
+ 
au. 
A similar analysis as in Example 10.2 gives: 
and 
\ 
λι 
λ2 
wii/г 
τ2 
, ^ 2 
< + σ Α 2 
2 
о 
К
_ 
σί/1 "Г °Ί/2 
„„J 
\Г 
_ 
' A l 
' - Λ2 
ί/ι|/ι,/2 - 
„2 „2 
and 
*^U2\Ji,J2 
- 
~2T~Ji 
· 
σνλ
συ2 
" \ ι \ 2 
Next, we are going to relate the operations of combination and projection in the 
valuation algebra of Gaussian potentials from Instance 1.6 to linear Gaussian systems. 
For this purpose, we fix a set r of variables with subsets s and t. So, let X denote an 
(s U i)-tuple of variables. We consider the system of linear equations with Gaussian 
disturbances composed of two subsystems, 
AiX4·* + ωι 
АгХ1' + ω2 
Zl, 
z2-
Here, we assume that Ai is an mi x s matrix, A2 an m2 x t matrix and zj, ω\ and 
Ζ2,ω2 are mi and m2-vectors respectively. Both matrices are assumed to have full 
column rank | s | and 111 respectively. Further, we assume that ω\ has a Gaussian density 
with mean 0 and concentration matrix Ki, whereas ω2 has a Gaussian density with 
mean 0 and concentration matrix K2. Both vectors are stochastically independent. 
Now, we can either determine the Gaussian potentials of the two systems: 
Gi 
= 
( ( A f K i A i ^ A i Z i , Α^ΚχΑχ) 
and 
G2 
= 
((Α^Κ 2Α 2)- 1Α 2ζ 2, Α^Κ 2Α 2), 
or we can determine the Gaussian potential of the combined system 
if"' 
0 
At 
isflt 
isflt 
At 
0 
x + 
ω2 
Zl 
z2 

GAUSSIAN SYSTEMS AND POTENTIALS 
3 9 3 
Here, the disturbance vector has the concentration matrix 
Κχ 
0 
0 
K 2 
Remark that the matrix of this combined system has still full column rank | s U t \. The 
Gaussian potential of the combined system is (μ, Κ) with 
К 
Afs 
Al 
-t т 
|snt T 
О 
О 
A4.snt т 
Ai'" -s T 
Ki 
0 
0 
K2 
i-M-t 
Af _ I 
Α | 
istlt 
istlt 
0 
+ 
Afe-tTKiAf-' 
AfsniTKiA|s-tT 
0 
■ Ats-tTKiAf-É 
AisntTKiAis-tT 
0 
" 0 
0 
Ats-tTK!Afsni 
AUntTKiAuntT 
+AÏ°ntTK2Al
2
sntT 
A ^ T K 2 A ^ n i T 
At' -' ΓΚιAfent 
0 
AlsntTKiAisntT 
0 
0 
0 
0 
0 AisntTK2AisntT 
A±sntTK2AisntT 
. 0 Af- s TK 2A^ n t T 
A^- s TK 2A^- s 
.isiit 
T K 2A it-a 
T 
. i t s T K 2A | Î -
= 
( A f K i A O T ^ + (A^KiA 2)î* u t. 
Note that this is exactly the concentration matrix of the combined potential G\®G2 
given in equation (1.21). For the mean vector μ of the combined system we have: 
К 
К 
A | 
A | 
is-t 
T 
isdt T 
0 
isdt T 
it-s 
T 
Ki 
0 
0 
K 2 
Zl 
Z2 
At 
At s _ t T K l Z i 
^ n t T K i z 1 + A ^ n t T 
. i t s T K 2z 2 
= 
К 
.is-t 
T 
A f - ^ K i Z j 
^ S n t T K i Z ! 
Ll 
0 
+ 
K 2z 2 
0 
.isflt 
T 
»■2 
. i t s T 
K 2z 2 
K 2z 2 
Let now μ\ = (AjKiAi)_1AiKiZi 
and μ2 = ( А ^ К г А г ^ А г К г г г be the 
mean vectors of the two potentials G\ and G2. We then see that we can rewrite the 
last expression above as 
К ^ ( ( A f K i A i ^ V r * + (A^K 2A 2)t^VÎ s U Î)· 

3 9 4 
GAUSSIAN INFORMATION 
But this is the mean vector of the combined potential G\ <g> G<i as shown in equation 
(1.22). Thus, we have shown that 
(μ,Κ) 
= 
Gl®G2. 
Therefore, combining two linear Gaussian systems results in the Gaussian potential, 
which is the combination of the Gaussian potentials of the individual systems. This 
justifies the combination operation which itself is derived from multiplying the as-
sociated Gaussian densities. The combination of Gaussian potentials as defined in 
Instance 1.6 therefore makes much sense. 
Example 10.4 Let us now assume that all values in the system of Example 10.1 have 
been observed. Then, inference can either be made for the complete model, or by 
combining the two potentials derived from the submodels in Example 10.2. We have 
Gw\Yi,Y2,ZuZ2JiJ2,Ji,J2,U3
 
= 
(μ>Κ) = Gw[Yi,Y2,Zi,Z2 
®Gw\h,I2,Ji,J2,U3 
The two components of the combined potential are: 
К 
= 
KW\YliY2tz1,Z2+^-W\IllI2,J1,J2,U3 
and 
μ = К 
y^W\Y^,Y2,Zi,Z2ßW\Yi,Y2,Zi,Z2+^W\h,l2,Ji,J2,U3l''W\h,I2,J1,J2,U3y 
Note that since we are dealing with potentials over the same single variable, there 
is no need for the extensions in the definition of combination. This is an example 
of solving a rather large system by decomposition and local computation. However, 
we remark that the theory is not yet general enough since the subsystems for the 
decompositions do not always have full column rank. 
Now, suppose that X is a vector of variables from a subset s Ç r which represents 
unknown parameters in a linear Gaussian system (10.1). Inference leads to a Gaussian 
potential (μ, К) = ((Α ΓΚ ωΑ) _ 1 Α τ Κ ω ζ , (Α τΚ ωΑ)). If Eisa subset of s, and we 
are only interested in the parameters in X·1-', it seems intuitively reasonable that for 
this vector the marginal distribution of the associated Gaussian density is the correct 
fiducial distribution. This marginal is still a Gaussian density with the corresponding 
potential (μίι, ((ΑτΚΑ)~1)+ί)~1). It corresponds to the projection of the Gaussian 
potential (μ, К) to t as defined in Instance 1.6: 
(μ,Κ)* 
= 
( > ' , ((Α τΚ ωΑ)" 1)^)- 1). 
(10.5) 
In fact, this can be justified by the elimination of the variables X+ 5 - t in the system 
(10.1), but it is convenient to consider for this purpose the equivalent system (10.3). 
The second part of this system does not contain the variables at all, and in the first 
part we simply have to extract the variables in the set t to get the system 
XiJ = (Aizip - 
uf. 

GENERALIZED GAUSSIAN POTENTIALS 
3 9 5 
The distribution of ώ\ has not changed by this operation. This therefore shows that 
the fiducial density of X^' is indeed the marginal of the fiducial density of X (for the 
marginal of a Gaussian density see Appendix J.2). 
By these considerations, Gaussian potentials are strongly linked to certain Gaus-
sian systems, and the operations of combination and projection in the valuation 
algebra of Gaussian potentials are explained by corresponding operations on linear 
Gaussian systems. The valuation algebra of Gaussian potentials therefore obtains a 
clear significance through this connection. In particular, Gaussian potentials can be 
used to solve inference problems for large, sparse systems (10.1), since they can 
often be decomposed into many much smaller systems. This will be illustrated in 
the subsequent parts of this chapter. However, it will also become evident that an 
extension of the valuation algebra of Gaussian potentials will be useful and even 
necessary for some important problems. We finally refer to Section 9.2.5 where a 
different interpretation of the same algebra based on the least squares method is given. 
10.2 GENERALIZED GAUSSIAN POTENTIALS 
The purpose of this section is to show that the valuation algebra of Gaussian potentials 
is not always sufficient for the solution of linear, functional models by local computa-
tion. To do so, we present an important instance of such models named time-discrete, 
dynamic system with Gaussian disturbances and error terms. 
■ 10.1 Gaussian Time-Discrete Dynamic Systems 
Let X\, Χ2, ■ ■ ■ be state variables where the index refers to the time. The state 
at time г + 1 is proportional to the state at time i plus a Gaussian disturbance: 
X2 
= 
aX\ + u)\, 
X3 
— 
αΧ2+ω2, 
The state cannot be observed exactly but only with some measurement error. 
We denote the observation of the state at time г by zi such that 
Ζγ 
= 
Χχ + V\, 
z2 
= 
X2 + V2, 
We further assume the disturbance terms Ui and the measurement errors Vi to 
be mutually independent. The ωί are all assumed to have a Gaussian density 
with mean 0 and variance <xj, whereas the observation errors Vi are assumed 
to have a Gaussian density with mean 0 and variance σ^. Such a system has 

3 9 6 
GAUSSIAN INFORMATION 
an illustrative graphical representation which indicates that state Xi+i is influ-
enced by state Xi and observation Ζχ by state X;. This is shown in Figure 10.2. 
It resembles a Bayesian network like the one in Instance 2.1 with the difference 
that the involved variables are not discrete but continuous. The computational 
problem is to infer about the states of the system at time г = 1,2,... given the 
equations above and the measurements Ζχ of the states at different points in time. 
X 1 
D 
— * ■ 
X 
z 
2 
2 
*· 
Хз 
И 
Figure 10.2 The graph of the time-discrete dynamic system. 
Let us now consider a manageable size of this instance where Zi for г = 1,2,3 is 
given and the states X\, X2 and X3 are to be determined. Then, the equations above 
can be rewritten in a format corresponding to the linear Gaussian systems introduced 
in the previous section: 
Xi 
aXi 
-
Xi 
- x2 
aX2 
-
X2 
- 
Хъ 
Хз 
+ 
+ 
+ 
+ 
+ 
+ 
ω0 
ωι 
ω2 
V\ 
Ь>2 
"3 
= 
m, 
= 
0, 
= 
0, 
= 
Zi, 
= 
z2, 
= 
Z3-
(10.6) 
The first equation has been added to indicate an initial condition on state Xi. If we 
consider each individual equation of this system as a piece of information, or as a 
valuation, then we obtain a covering join tree for the system as shown in Figure 10.3. 
Each equation of the system is on one of the nodes of the join tree. This constitutes 
a nice decomposition of the total system. If we could derive a Gaussian potential 
for each node, we could solve the problem specified above by local computation. 
However, although each equation represents a linear Gaussian system, it does not 
satisfy the requirement of full column rank imposed in the previous section. In fact, 
the single equations do not represent overdetermined systems, but rather underde-
termined systems. So, unfortunately, we cannot directly apply the results from the 
previous section and work with Gaussian potentials. This indicates the need of an 
extension of the theory of inference in linear Gaussian systems. 

GENERALIZED GAUSSIAN POTENTIALS 
3 9 7 
{X1.X2} 
{X2, Хз} 
{Хз, X4} 
Г <Χι> J 
( m j 
( (хз) j 
Figure 10.3 A covering join tree for the system (10.6). 
In order to introduce the new concepts needed to treat such systems, let us have a 
closer look at the first two equations of system (10.6): 
X\ 
+ ω0 
= 
m, 
αΧχ 
- 
X2 
+ ωι 
= 
0 
(10.7) 
Its matrix and vector are 
A = 
1 
0 
a 
- 1 
and 
z = 
m 
0 
The disturbance vector ω = [ ωχ u>2 ] has the concentration matrix 
^ 
0 
Κω — 
0 
Finally, if X denotes the variable vector with components X\ and X%, the system 
(10.7) can be represented in the standard form (10.2) of Section 10.1: 
AX + ω = 
z. 
This system has full column rank 2 and thus fulfills the requirement of assumption-
based reasoning imposed in Section 10.1. We derive for X a Gaussian potential 
Gx 
= 
( μ χ , Κ χ ) 
with 
Κ χ 
Α ' Κ ω Α 
-1 
0 
0 
- 1 
1+<П 
The mean vector of X can be obtained from system (10.7) directly, 
μχ 
= 
m 
am 
(10.8) 

3 9 8 
GAUSSIAN INFORMATION 
or from the results of Section 10.1. The fact that A has an inverse simplifies this way. 
We now look at this result from another perspective. If we consider the second 
equation of system (10.7) in the form X2 = aX\ + ω\, we can read from it the 
conditional Gaussian distribution of X2 given Xi as a Gaussian density fx2\xl 
with 
mean ax\ and variance σ^. This is the same conditional density as we would get 
from the Gaussian density of X above by conditioning on ΛΊ. Hence, this conditional 
density of X2 given Χχ can also be obtained as the quotient of the density / χ of 
X divided by the marginal density /χα of ΛΊ, which is Gaussian with mean m and 
variance σ£. If we compute this quotient, we divide two exponential functions, and, 
neglecting normalization factors, it is sufficient to look at the exponent, which is the 
difference of the exponent of / χ and /χ,, 
x\ — m 
X2 — am 
Xl 
X2 -
- m 
am 
(xi - m) —(xi - T O ) . 
The second term of this expression can be written as 
Xl — TO X2 
К t{l,2} 
Xi 
Xl 
Xl -
- TO 
am 
whereKx1 = [1/σ£] is the one-element concentration matrix of Λ"ι. If we introduce 
this in the exponent of the conditional Gaussian density above, we obtain 
Xl — TO X2 — ОТО 
Xl — TO 
X2 — am 
This looks almost like a Gaussian density, except that the concentration matrix 
T^-x2\Xi ш this exponent is only non-negative definite and no more positive definite, 
since it has only rank 1. We remark that this concentration matrix is essentially the 
difference of the concentration matrices of / χ and fx1, 
К Χτ\Χι 
К , 
К Î{1,2} 
Χι 
■ 
This observation leads us to tentatively introduce a generalized Gaussian potential 
(μ, Κχ2 \Χι ) where μ is still given by (10.8). It is thought that this potential represents 
the conditional Gaussian distribution of X2 given X\. This will be tested in a moment. 
Remark that from the first equation of (10.7), it follows that X\ has a Gaussian 
density /xj with mean то and variance σ£. This is the marginal obtained from the 
density/χ and it is represented by the ordinary Gaussian potential (то, 1/σ£). The 
vector X then has the Gaussian density / χ — fx2\x1 /хг ■ Let us try whether we get 
this also by formally applying the combination rule to the Gaussian potential for ΑΊ 
and the generalized potential of X2 given X\. So, let 
(μχ, K x j = 
^μχ2|χ1( Kx2|XlJ <g> (TO, l/<x£j. 

GENERALIZED GAUSSIAN POTENTIALS 
399 
According to the combination rule for potentials defined in Instance 1.6 we have 
Κ χ 
К XilAa 
К Î{1,2} 
Xi 
+ 
1 
Ï5" 
0 
0 ' 
0 
_ 
l+a 2 
"l 
a 
a 
?» 
1 
This is indeed the concentration matrix of /χ. Let us see whether this also works for 
the mean vector. We first determine 
к; 
9 
1 
<*t 
ασω 
ασ2
ω 
(1 + α2)σ2
ω 
According to the combination rule for potentials defined in Instance 1.6 we have 
Mx 
Κχ 1 
Κχ 1 
1+αΔ 
m 
am 
m 
- am 
+ \ 
о Ί 
0 
0 
m 
am 
m 
0 
This is indeed the mean vector of /χ. It seems that the combination operation for 
ordinary potentials works formally also with generalized Gaussian potentials. 
We shall see later that it is even more convenient to define the generalized Gaussian 
potentials as (Κμ, Κ). Indeed, we remark that the generalized Gaussian potential of 
Xi given X\ can also be determined directly from the second equation of system 
(10.8) which defines the conditional density. Let A = [ a 
— 1 ] be the matrix of 
this equation and Κω = [Ι/σ^] its concentration matrix. We consider the exponent 
of the exponential function defining the conditional density of X2 given X\ : 
[x% - αχι)Κω(χ2 
- ax\) 
= 
(—a(xi — m) + (x2 — am)) Κ ω (—a(x\ — m) + (x2 — am)) 
Α Γ Κ ω Α 
= 
[ χι — m 
X2 — am 
From this we see that indeed 
K-X2\Xi = A ΚωΑ. 
Further, we obtain 
Κ-χ2\χιμχ2\χ1 
= 
A 
ΚωΑμχ2\Χι 
i 
Κωτη = 
ATKu!m. 
x\—m 
X2 — am 
a 
- 1 
Κω [ a 
- 1 ] 
m 
am — m 

4 0 0 
GAUSSIAN INFORMATION 
So, indeed, the generalized Gaussian potential is completely determined by the 
elements of the defining equation only. This approach can be developed in general, 
and it will be done so in Section 10.3. For the time being, we claim that the same 
procedure can be applied to the equations associated with the nodes of the join tree 
in Figure 10.3. We claim, and show later, that the extended Gaussian potentials form 
again a valuation algebra, extending the algebra of ordinary Gaussian potentials. The 
Gaussian potential of the total system (10.6) can therefore be obtained by combining 
the generalized Gaussian potentials. The fiducial densities of the state variables is 
then obtained by projecting this combination of potentials to each variable. Thus, 
the inference problem stated at the beginning of this section can be solved by local 
computation with very simple data rather than by treating system (10.6) as a whole. 
This example is a simple instance of a general filtering, smoothing and projection 
problem associated with time-discrete Gaussian dynamic systems. It will be examined 
in its full generality in Section 10.5. Moreover, can also be seen as an instance of a 
Gaussian Bayesian network. This perspective will be discussed in Section 10.6. 
10.3 GAUSSIAN INFORMATION AND GAUSSIAN POTENTIALS 
In this section, we take up the approach sketched in the previous Section in a more sys-
tematic and general manner by considering arbitrary underdetermined linear systems 
with Gaussian disturbances. 
10.3.1 Assumption-Based Reasoning 
Let us assume a linear system with Gaussian disturbances 
ΑΧ + ω = 
z. 
(10.9) 
which, in contrast to Section 10.1, is underdetermined. Here, X is an n-vector, A an 
mxn 
matrix with m <n and z an m- vector. The stochastic m- vector ω has a Gaus-
sian density with mean vector 0 and concentration matrix Κω. Further, we assume for 
the time being that A has full row rank m. More general cases will be considered later. 
We first give a geometric picture of this situation based on the approach of 
assumption-based reasoning. Note that, under the above assumptions, system (10.9) 
has a solution for any vector ω € Km. So, in contrast to the situation in Section 10.1 
with overdetermined systems, no ω has to be excluded, and for any ω e Mm the 
solution space is an affine space С + у(ш) in Rn. Here, С is the kernel (see Section 
7.2) of the system and у{ш) any particular solution of the system with a fixed ω. 
It is evident that the different assumptions lead to disjoint, parallel, affine spaces. 
Therefore, we sometimes call Γ(ω) = С + ν{ω) the focal space associated with ω. 
It is also clear that if ω varies over Rm, then their focal spaces cover the whole space 
R™. The Gaussian probability distribution of ω induces also an associated Gaussian 
probability of the focal spaces. Such a structure is also called a hint: Each assumption 
ω out of a set of possible assumptions determines a set Γ(ω) of possible values for 

GAUSSIAN INFORMATION AND GAUSSIAN POTENTIALS 
401 
the unknown parameter x, and the assumptions are not all equally likely but have a 
specified probability distribution. We refer to (Kohlas & Monney, 1995) for a general 
theory of hints and to (Kohlas & Monney, 2008) for the connection of hints with 
statistical inference. If В is a regular m x m matrix, then the transformed system 
BAX + ώ = z with ώ = Bu; and z = Bz has the same family of parallel focal 
spaces Γ(ώ) = Γ(Βω) as the original system and also the same Gaussian density 
over the focal spaces. Such hints, which differ from each other only by a regular 
transformation, are therefore called equivalent. lfm — n, we have an important spe-
cial case: the focal spaces Γ(ω) then reduce to single points, i.e. the unique solutions 
у{ш) = A - 1 (z — ω) of the system for a fixed assumption ω. We refer to the system 
(10.7) and its analysis in the previous Section 10.2 as an example for such a situation. 
It induces an ordinary Gaussian density with mean vector A _ 1z and concentration 
matrix А ГК ША. This case is also covered in Section 10.1, and the Gaussian density 
corresponds to the Gaussian potential (A _ 1z, Α τ Κ ω Α ) associated with the system. 
This can indeed be verified from (10.4) in Section 10.1: for the concentration matrix 
we have the same expression. The mean vector in (10.4) can be developed in this 
particular case as follows: 
( Α τ Κ ω Α ) " 1 Α τ Κ ω ζ = A ^ K ^ A 7 " - χ Α τ Κ ω ζ = A ^ z . 
Hints with one-point focal spaces result from assumption-based reasoning according 
to Section 10.1 from any overdetermined system. 
In the following, we usually assume m strictly smaller than n. We show that 
in this case Gaussian conditional densities can be associated with linear Gaussian 
systems (10.9) and thus also with generalized Gaussian potentials as exemplified in 
the previous section. Since A has full row rank m, there is at least one mxm 
regular 
submatrix of A. By renumbering columns we may always assume that the first m 
columns of A are linearly independent. We decompose A into the regular submatrix 
Ai composed of the first m columns and the remaining m x (n — m) submatrix A2, 
that is A = [Ai, Α2]. Let us further decompose the vector of variables X accordingly 
into the m-vector Χχ and the (n — m)-vector X2. Then, the system (10.9) can be 
written as 
A1X1 + A2X2 +ω 
= 
z. 
Applying the regular transformation В = Af1 yields the equivalent system 
Xi + А5"гА2Х2 +ώ 
= 
z, 
where ώ — Α ^ ω and z = A]~1z. The stochastic vector ώ still has a Gaussian 
density with mean 0 and concentration matrix K.Q = AjΚωΑι 
as shown in the 
appendix of this chapter. Using the transformed system, we can express Xi by X2, 
Xi 
= 
- А ^ А г Х г + (z — ω). 
If we fix X2 to some value X2, we can directly read the Gaussian conditional density of 
Xi given X2 = X2 as a Gaussian density with mean-vector μχ1 |χ2 and concentration 

4 0 2 
GAUSSIAN INFORMATION 
matrix Κ Χ ι|χ 2 obtained as follows: 
Mx!|x2(x2) 
= 
A f 1 z - A f 1 A 2 x 2 
К Xi|X 2 
A f K u A i . 
This conditional Gaussian density of Xi given X2 has the exponent 
(xi + A ^ A 2 x 2 - A^zfAjK^A^xj 
+ A ^ A ^ - A ^ z ) . 
(10.10) 
Next, we are going to associate a generalized Gaussian density to this conditional 
Gaussian density in a way similar to that of the previous Section 10.2: To do so, we 
introduce two undetermined vectors μι and μ2 which are linked by the relation 
This also means that 
μι 
= 
- A j :Α2μ2 + Aj :z. 
Αιμι + Α2μ2 
= 
z. 
(10.11) 
The vectors μι and μ2 are only used temporarily and will disappear in the end. We 
introduce this expression into the exponent of equation (10.10): 
(xi + Af xA 2x 2 - А7 гг) тА^К шА1(х1 + AJ~xA2x2 - A ^ z ) 
= 
((x2 - μι) + Aj-1A2(x2 - μ2))ΤΑ^Ku,A1((xi - μι) + A ^ A z f o - μ2)) 
[ xi - μι 
χ2 - μ2 
-*-m 
, Τ Α - Ι Τ 
AfK wAi 
Α2 Αχ 
[ I m 
Α ^ Α ^ ] [ χ ι - μ ι 
χ 2 - μ 2 ] . 
This looks like the exponent of a Gaussian density with concentration matrix 
К 
Т А - I T 
A 2 A 1 
Α τ Κ ω Α ! 
Α^Κ ωΑ 2 
ΑΐΊί,,Αι 
Α τ Κ ω Α 2 
AfK„Ai [ Im 
Α ^ Α ^ } 
= Α τ Κ ω Α . 
Since this matrix is only non-negative definite, we call it a pseudo-concentration 
matrix. The conditional Gaussian density above has an undetermined mean vector μ 
with components μι and μ2, but this mean vector disappears if we consider 
Κμ = Α τ Κ ω Α μ = Α τ Κ ω [ Ai 
A2 
μι 
ß2 
Α τΚ ω(Α χμι + Α2μ2) 
A1 Κ ωζ. 
The last equality follows from ( 10.11 ). To sum it up, we have associated a generalized 
Gaussian potential 
{A1 Κ ωζ, Α1 Κ ωΑ) 
(10.12) 

GAUSSIAN INFORMATION AND GAUSSIAN POTENTIALS 
4 0 3 
with the original linear Gaussian system (10.9). We have done this via a conditional 
density extracted from the system (10.11). Note that there may be several different 
conditional Gaussian densities that can be extracted from (10.11), depending on 
which set of m variables we choose to solve for. But the final generalized conditional 
Gaussian potential is independent of this choice. It only depends on the Gaussian 
information expressed by the system (10.9). Let us verify this assertion by considering 
an m x m regular matrix В and the corresponding transformed system BAX + 
ώ = z, where K.Q — Β Γ _ 1 Κ ω Β _ 1 . The generalized potential associated with this 
transformed system has concentration matrix 
К = (ΒΑ) τΚ ώΒΑ = Α τ Β τ Β τ 1 Κ ω Β 
ΧΒΑ = Α τ Κ ω Α . 
Similarly, we obtain for the first part of the generalized potential 
v = (ΒΑ) τΚ ώΒζ = Α τ Β τ Β τ 1 Κ ω Β 1 Β ζ 
= Α τ Κ ω ζ . 
So, the transformed equivalent system has the same potential as the original system. 
In other words: equivalent systems have identical generalized Gaussian potentials. 
10.3.2 General Gaussian Information 
Linear Gaussian information can be represented in a compact form by generalized 
Gaussian potentials. How are natural operations with linear Gaussian information 
reflected by Gaussian potentials? Here, we look at the operations of combination. 
For subsets s and t of some set of indices of variables let X be an (s U i)-vector 
and consider two pieces of linear Gaussian information, one referring to the group of 
variables in s 
Α χ Χ ^ + ω ι 
= 
zi 
(10.13) 
where Ai is an mi x s matrix and ωχ and zj are mi-vectors, ωχ has a Gaussian 
density with mean vector 0 and concentration matrix Κ ω ι. The second piece of linear 
Gaussian information refers to the set t of variables, 
Α 2 Χ ^ + ω 2 
= 
z2, 
(10.14) 
where A2 is an m2 x t matrix and ω2 and z2 are m2-vectors. ω2 has a Gaussian 
density with mean vector 0 and concentration matrix КШ2. Together, the two systems 
provide linear Gaussian information for the total vector X, 
ΑΧ + ω 
= 
z. 
(10.15) 
The (mi +m 2 )-vector ω has a Gaussian density with mean vector 0. Since we assume 
that the vectors ωχ and ω2 are stochastically independent, its concentration matrix is 
Κω — 
κωι 
о 

4 0 4 
GAUSSIAN INFORMATION 
The (mi + m2)-vector z is composed of the two vectors zi and Z2. Finally, the 
(mi + m2) x (äUt) matrix A is composed of the matrices Ai and A2 as follows: 
A 
= 
Aj*-' 
0 
M Unt 
\Unt 
A2 
0 
(10.16) 
Now, even if the two matrices Ai and A2 have full row rank, this is no more 
guaranteed for the matrix A. Therefore, we need to extend our assumption-based 
analysis to the case of a general linear Gaussian system 
ΑΧ + ω 
z, 
(10.17) 
before we can return to the combined system above. Here, we assume A to be an 
mxn 
matrix of rank к <т,п. The number of equations m may be less than, equal 
to or greater than the number of variables n. So, the following discussion generalizes 
both particular cases studied so far. The vectors ω and z have both dimension m, and 
ω has a Gaussian density with mean 0 and Κω as concentration matrix. As usual, we 
consider the system to be derived from a functional model where the linear, stochastic 
function Ax + ω determines the outcome or the observation z. Then, by the usual 
approach of assumption-based reasoning, we first remark that, given the observation 
z, only assumptions ω in the set 
{ω 6 
: ω = z - Ax, x e M71} 
may have generated the observation z. The set vz is an affine space in the linear space 
Rm and in general different from Rm. These are the admissible assumptions. Then, 
each admissible assumption ω Ε « , determines the set of possible parameters x by 
I » 
{ x £ l n : Αχ = ζ - ω } . 
As before, these are parallel affine subspaces of K" which cover Rn completely. It is 
also possible that the affine spaces are single points. Different admissible assumptions 
ω have disjoint focal spaces Γ(ω). 
Next, we need to determine the conditional density of ω, given that ω must be 
in the affine space vx. As in Section 10.1, we transform the system (10.17) in a 
convenient way which allows us to easily derive the conditional density. For this 
purpose, we select an m x к matrix Bi whose columns form a basis of the column 
space of matrix A. For example, we could take к linearly independent columns of 
A, which is always possible since A has rank k. Consequently, the matrix Bi has 
rank k. Then, there is а к x m matrix Л such that A = BiA. We complete Bi with 
an m x (m — к) matrix B2 such that В = [Bi B2] is a regular m x m matrix and 
define T = B _ 1 . We decompose 
т 
= [тГ 
T 2 

GAUSSIAN INFORMATION AND GAUSSIAN POTENTIALS 
4 0 5 
such that Ύι is а к x m and T 2 an (m — к) х m matrix. Then, we see that 
TA 
TXA 
T 2A 
^ A 
T 2BiA 
TXA 
0 
since Т В = I implies that T 2Bi = 0. Now, the matrix ТА has rank к because T 
is regular. This implies that the A; x m matrix TjA has full row rank k. The focal 
spaces for ω can now be described by 
Γ(ω) 
= 
{χ: Τ χΑχ + Τιω = Τιζ} 
and the admissible assumptions by 
vz 
= 
{ω : Τ2ω = T 2z}. 
This corresponds to the transformed system 
TjAX + ώι 
= 
zi, 
ώ2 
= 
z2, 
(10.18) 
where ώ\ = Τχω and z\ = Tiz. The transformed vector ώ = Τω is composed of 
the components ώ\ and ώ2. It is still Gaussian with mean 0 and concentration matrix 
Κώ = В КШВ 
Bj Κ ωΒι 
Bi Κ ωΒ 2 
B 2 Κ ωΒι 
В 2 Κ ωΒ 2 
(10.19) 
From the system (10.18) it is now easy to determine the conditional Gaussian density 
of ώι given ώ2 = z2. According to equation (10.19) it has concentration matrix 
Bf ΚωΒχ and mean vector (see Appendix J.2) 
-(Β^Κ ωΒ 1)- 1(Β^Κ ωΒ 2)ζ. 
Finally, we obtain from system (10.18) the following linear Gaussian system 
TiAX + ώ 
- 
Τ 1ζ + (Β^Κ ωΒ 1)- 1(Β^Κ ωΒ 2)ζ, 
(10.20) 
where 
ώ 
- 
ώ 1 - ( Β ^ Κ ω Β 1 ) - 1 ( Β ^ Κ ω Β 2 ) ζ . 
(10.21) 
This is now a linear Gaussian system with full row rank k. Therefore, we can apply 
the results from Section 10.3.1 to derive its generalized potential. 
We now argue that this is also the generalized Gaussian potential associated with 
the original system. In fact, the selection of the basis Bi was arbitrary, and the 
resulting system depends on this choice, but the associated generalized Gaussian 
potential does not, as we show next. A linear Gaussian system like (10.17) is fully 

4 0 6 
GAUSSIAN INFORMATION 
determined by the three elements A, z and Κω. Therefore, we represent it by the 
triplet (A, z, Κω) and define the mapping 
e(A,z, Κω) 
( Α τ Κ ω ζ , Α Γ Κ ω Α ) . 
The image of this mapping looks like a generalized Gaussian potential. In fact, it is 
one, but it remains to verify that the matrix Α τ Κ ω Α is non-negative definite. This 
however is a consequence of the following theorem: 
Theorem 10.1 Let I — (A, z, Κ ω) describe a linear Gaussian system (10.17) with 
an m x n matrix A of rank k, and 
h : TjX + ώ = 
Τ 1ζ + (Β^Κ ωΒ 1)- 1(Β[Κ ωΒ 2)ζ 
a system with full row rank derived from it as in (10.20) and (10.21). Then, e(l) = 
e(h). 
Proof: We define 
A 
z 
K, 
TiA, 
τ ι Ζ + ( B Î X B O - ^ B Ï X B S K 
B J Κ ωΒι. 
It follows that e(h) = ( Α τ Κ ώ ζ , Α τ Κ ώ Α ) . But then, if В is the matrix used in 
transforming the system (A, z, Κω) and T = B _ 1 
А 7К ША 
Α 7 Τ ' Β ' Κ ω Β Τ Α = (TA) JB JK WB(TA) 
(TiA) T 
0„,m_fc 
B]^ Κ ωΒι 
Bj Κ ωΒ 2 
B 2 Κ ωΒι 
B 2 Κ ωΒ 2 
(Τ 1Α) τΒ^Κ ωΒ 1Τ 1Α = Α ΓΚ ώΑ. 
TXA 
Um — k,n 
Further, 
ATIC 
T l Z 
T 2z 
= 
( Τ Α ) τ Β τ Κ ω Β Τ ζ 
_ 
ί / τ Α\τ n 
, 1 Г BfK^Bi 
Β^Κ ωΒ 2 
- 
[ ( i j A j 
u„,m_fe j ^ Β τ Κ Β ι 
Β ^ Κ ω Β 2 
= 
(TXA)T (Bf Κ ω Β ! Τ ι Ζ + 
BjΚωΒ2Τ2ζ) 
= 
(Τ 1Α) τ(Β[Κ ωΒ 1) ( Τ ι Ζ + ( B Ï X B O ^ B Ï X B a T a z ) 
= 
Α τ Κ ώ ζ . 
So, indeed the generalized Gaussian potential of h equals e(Z). 
■ 
Since e(h) is indeed a generalized Gaussian potential, it follows that e(l) is one 
too. Thus, by the mapping e(Z), we associate a uniquely determined generalized 

GAUSSIAN INFORMATION AND GAUSSIAN POTENTIALS 
4 0 7 
Gaussian potential to each linear Gaussian information of the form (10.17) without 
any restriction on the rank of the system. We may say that two linear Gaussian systems 
I and /' are equivalent if they induce equivalent systems h and h' with full row rank. 
Since we have seen that equivalent systems h and h! generate the same generalized 
Gaussian potential, this also holds for equivalent Gaussian systems. 
10.3.3 Combination of Gaussian Information 
We are now able to compute the generalized Gaussian potential (г/, К) of the com-
bined system (10.15) with the combined matrix (10.16). Note first that the generalized 
Gaussian potential (v\, Ki) corresponding to the first subsystem (10.13) is given by 
Ki — Aj Κ ω ιΑι 
is-t 
Tjv- 
д | . 
Af 
AlsDi T-rr 
»4-
1 
χ ν ω ι Λ ι 
t 
is-t 
Α | 8 - * τ Κ ω ι Α 
AJ s n t TK W lAi 
Isnt 
1 
I s n t 
(10.22) 
and 
vx = A ^ K ^ z i 
A{ 
Κ ω ιζι 
(10.23) 
For the second subsystem (10.14), we have similar results, essentially replacing Αχ 
by Аг, Κωι by Κω2 and z\ by Z2. Now, the second part of the potential (г/, К) of 
the combined system (10.15) is given by 
К 
= Α τ Κ ω Α 
is-t 
T 
A f 
A | 
isDt 
T 
0 
, isnt 
T 
.it-sT 
K, 
0 
к, 
, is—t 
д ф в - Г 
д ф 
isnt 
istlt 
lit-S 
Ά2 
» 4 . s - t T x ^ 
д ф 
Αί 
Li 
isnt 
T **·ωι A j 
is — t 
1 
is — t 
0 
4-5 
K-uJi-A-i 
Aisflt T-ws- 
д isnt 
1 
*V«;i-"-j 
0 
A i s - ' V 
A4· 
A isnt 
T-rr 
A 
1 
n w i A l 
+ A i s n i T K ( 
A 
ω2·ΛΛ-2 
Д 4 ' - * Т К 
А 
Λ 2 
χ νω2-"-2 
|snt 
1 
J,sni 
isnt 
2 
|t—s 
4 . S - Î T 
L l 
, |snt T 
Κ
Α Ψ 
ω ι Λ ι 
A f ' ^ K ^ A t 
Isnt 
isnt 
■К-шг А-2 
| i -
|snt 
,4t-s T 
0 
0 
0 
■"-012-^-2 
l t -
0 
0 
0 
0 
• Isnt Tjv- 
« Isnt 
A 2 
*Vj2-™-2 
. | t - s T , 
» 4 . t - s J т^ 
д . 
Λ 2 
χνω2-ίΛ-2 
L | s n t - | 
lins 
A + > n t l f 
A | t —s 
.it-s 
T 
" k ) 2 ^ 2 
| t - s 
K{ tsUt . x^tsUt 
+ Л-9 
The last equality can be concluded from equation (10.22). 

4 0 8 
GAUSSIAN INFORMATION 
Next, we compute the first part of the potential (v, K) of the combined system 
Α τ Κ ω ζ 
Ai 
is-t 
T 
isilt T 
0 
, isflt T 
L2 
.it-s 
T 
K, 
0 
к. 
. 4.«nt T 
A | is-t 
T K. 
Κ ω ιζι + A; isnt T Κω2Ζ2 
L-u-
Λ. ω 2Ζ 2 
A
l s —t -ΐ τ ^ 
i 
Κ ω ι ζ ι 
ΑΓ ί ΤΚ ω ι Ζ ι 
+ 
Zl 
ζ 2 
0 
Aisnt T-wr 
A ^ - s τΚ ω 2ζ 2 
2^2 
fsUi 
TsUi 
This shows how generalized Gaussian potentials can be combined in order to reflect 
the combination of the linear Gaussian information. Namely, if the potential (ι/χ, Ki ) 
has domain s and (г/2, К 2) has domain t, 
(i/i.KOate.Ka) = (^sUt + ^ tsut 
2 
' К tsut + K^sUt). 
10.3.4 
Projection of Gaussian Information 
After the operation of combination, we examine the operation of information extrac-
tion or projection. We start with a linear Gaussian system 
I : ΑΧ + ω 
(10.24) 
where X is an s-vector of variables, A an m x s matrix, representing m equations 
over the variables in s, z an m-vector and ω a stochastic m-vector having Gaussian 
density with mean vector 0 and concentration matrix Кш. We want to extract from 
this Gaussian information the part of information relating to the variables X^' for 
some subset t Ç s. To this end, we decompose the linear system according to the 
variables X-^-* and X·1-4, 
ΑιΧ-μ-* + Α 2Χ-μ+ω 
(10.25) 
Here, Ai is the m x (s — t) matrix containing the columns of A which correspond 
to the variables in s — t, and A2 t h e m x i matrix containing the columns of A 
which correspond to the variables in t. Extracting the information bearing on the 
variables in t means to eliminate the variables ins — t from the linear system above. 
This operation has already been examined in Chapter 9. But here, we additionally 
look at the stochastic component ω in the system. To simplify the discussion, we 
first assume that Ai has full column rank \s — t\ = k. This implies that there are k 
linearly independent rows in the matrix Ai. By renumbering rows, we may assume 
that the first k rows are linearly independent such that the matrix Ai decomposes 

GAUSSIAN INFORMATION AND GAUSSIAN POTENTIALS 
4 0 9 
into a regular kxk submatrix Αχ,ι composed of the first к rows and the (m — k) xk 
submatrix А2д containing the remaining rows. If we decompose system (10.25) 
accordingly into the system of the first к equations and the m — к equations, we 
obtain 
Ai.iX-'-'-' + Ai.aX^+wi 
= 
z b 
Α2,ιΧ1*-1 + А2,2Хи 
+ ω2 
= 
z2. 
Here, A l j 2 is a fc x (silt) matrix and A2,2 an (m — k) x (s Л t) matrix. The vectors 
LOI and zi have dimensions fc, whereas ω2 and z2 have dimension m — к. Since Αι,ι 
is regular, we can solve the first part for X-^ -' and replace these variables in the 
second part of the system, 
X4-'-* + А-}А 1 ) 2Х+< + A ^ > ! 
(A2,2 - Aa.iAfJAi.aJX4·* + ( - А 2 Д А ^ и л 
+ω2) 
This is a new linear Gaussian system 
ΑΧ + ώ 
= 
z 
A 1 , Î Z L 
-A 2,iA^jzi + z 2 . 
(10.26) 
with 
and 
Ife 
Om-k,k 
ΑΓ,1Α12 
A2,2 - A 2 )iA^iAi > 2 
(10.27) 
ω 
= 
Lui 
ώ2 
Z l 
= 
= 
Aj }ωι 
1,1 
Afjzi 
—АгдА^}^! +ω2 
- А г д А ^ г ! + ζ2 
We note that in the second part of the linear system (10.26) the variables in s — t 
are eliminated. Therefore, this part of the system represents the part of the original 
Gaussian information (10.24) bearing on the variables in t, that is 
Iй : A 2 2X 4 t + ■ω2 
Z2-
(10.28) 
Here, the matrix A2,2 = A2^2 — АгдА! }Ai,2 corresponds to the right lower 
element of the matrix A. This linear Gaussian information can be represented by the 
generalized Gaussian potential (ü, C), where 
v = Α^ 2Κ ώ 2ζ 2 
and 
С = А ^ К ^ А г ^ . 
(10.29) 
In order to fully determine this potential, it only remains to compute Κώ2. For this 
purpose, note that ώ = Τω with the matrix 
ΑΓ] 
0 
-Α2,ιΑΓ,ί 
I J 

4 1 0 
GAUSSIAN INFORMATION 
The inverse of this matrix is 
1 - 1 
Αι,ι 
A 2,i 
0 
I 
From this we obtain the concentration matrix of ω = Τω, 
К , 
-1 T Κ ω Τ " 
Αι,ι 
Ά 
О 
2,1 
I 
Κι,ι 
Κι,2 
К2,2 
Αι,ι 
А-2,1 
О 
I 
(10.30) 
Α^Κ ωΑ! 
Α^Κ^ 
К2А1 
К2,2 
Here, Ai is the matrix of Χ^ θ _ ί in the system (10.25), whereas 
K2 
— [ Кгд Кг,2 J 
denotes the lower row in the decomposition of Κω. We finally obtain the concentration 
matrix of the marginal density of ώ, and it is shown in Appendix J.2 that this also 
corresponds to the concentration matrix of ώ2, 
К , 
= 
Ka.a-KaAitAÏXA!) - 1 AfK£. 
(10.31) 
We have completely determined the information for X^, once as extracted from the 
original system and then as the associated generalized Gaussian potential. It is very 
important to remark that these results do not depend on the particular choice of the 
regular submatrix Αχ,χ of A b which is used in the variable elimination, since only 
the matrix Αχ appears in the generalized Gaussian potential. 
We want to see whether and how this potential relative to the extracted information 
can directly be obtained from the generalized Gaussian potential (v, C) of the original 
system (10.24). As we have shown, equivalent systems have identical potentials. So, 
instead of computing the potential of system (10.24), we can as well compute the 
potential of the transformed system (10.26) with the matrix Ä as defined in (10.27). 
For the pseudo-concentration matrix we have 
Α ' Κ ω Α = A J K ^ A 
ÄfKaÄi 
ÄfKaÄa 
A^K^Aj 
Ά.2 К ^ А г 
The matrix Äi denotes the left-hand column of the matrix A in (10.27), whereas Ä2 
denotes the right-hand column. Here, С is decomposed with respect to s — t and t. 
The blocks of С are as follows, if the internal structure of matrix Ä in (10.27) is 
used: 
I 
0 
C M = ÄfKaÄi = [ I 0 ] Κώ 
The last term equals the left upper element of K.a in (10.30). We thus obtain 
Ci.i = 
A ? X A i . 

GAUSSIAN INFORMATION AND GAUSSIAN POTENTIALS 
411 
If А2Д and Аг,2 denote the submatrices of A2 corresponding to the decomposition 
with respect to s — t and t, we obtain for the lower right element of С using (10.30) 
'2,2 
= Α^κωΑ2 
Ä£2 
AfK^Ai 
AfKj 
К2А2Д 
K2,2 
= 
[4l 
= Al^Aj К^АОАгд + Ä^AfK^Ä2,2 
+ 
А22К2А2ДА2Д + Α 2 2Κ2,2Α2 ι2· 
Next, we compute the lower left element of C: 
С2д 
= 
A2 K ö A i 
А2д 
A2,2 
- 
[4i Ц,2 ] KÛ 
= Â^iAf ΚωΑ0 + Ä2
r
i2K2A1. 
By the symmetry of the matrix С it finally follows that 
Ci,2 = C ^ = 
(AfK wA 1)Ä 2, 1+AfK 2
rÄ 2, 2. 
These elements are sufficient to establish the relation between the concentration 
matrix С in the potential of the original system (10.24) and the concentration matrix 
С in the potential of the reduced system (10.28). In fact, we conjecture that the same 
projection rule applies as for ordinary Gaussian potentials (10.5) in its transformed 
form (Ομ,Ο), i.e. 
CU 
= ((C-i)4.t)-i = C a . a - C a . i C ^ i d . a , 
see Appendix J.2. This can easily be verified by using the above results for the 
submatrices Cij to obtain 
((C 
) ) 
— ^2,2 — C2,lC 1 ) 1Ci )2 
= Ца (K2i2 - KaAxiAÎXAO^AfK^) Ä2,2 
= 
Α2ι2ΚώΑ2,2 = С, 
see equation (10.29). Similarly, we examine the first part и of the potential, 
v 
= 
Α τ Κ ω ζ = A TK^z 
I 
0 
A 2 , 2 
Α^Κ ωΑ! 
A f K j 
K 2Ai 
K2,2 
Zl 
z2 
A7 
We find for its two components 
v\ 
= 
Α^Κ ωΑιΖι + А^К^гг, 
u2 
= 
Ä ^ ^ A f Κ ωΑι)ζι + À^K^AiZi + Ä ^ A f K f z ^ + Äf^K^z^. 

4 1 2 
GAUSSIAN INFORMATION 
We next want to apply the projection rule for ordinary Gaussian potentials from 
equation (10.5) to the first part of the potential. This requires first transforming the 
rule to the new form of the potential. From v = Ομ we derive 
V\ 
= 
C M / i i + Ci,2M2, 
i/2 
= 
C2,l/il + C2,2M2· 
From this we obtain 
^ 2 - C 2 , i C r > i = (C 2, 2-C 2,iC^jC 1, 2) M2 = С + У . 
So, the conjecture is 
vu 
= 
z / 2 - C 2 , i C ^ i . 
In fact, 
^ - C 2 i l C ï > i 
= 
Äl2(K2t2-K2A1(AlKulA1)-1AlKl)z2 
= 
Α^)2Κώζ2 = î>, 
see equation (10.29). Putting things together, we conclude that generalized and ordi-
nary Gaussian potentials essentially share the same projection rule 
(v,C)il 
= (vu,Cu) 
= 
Lit 
_clt,s-t(Cu-t,s-tyiuu-t^ 
s~i],t,s—t{(-i±s — t,s—t\ — l(-i±s — t,t\ 
This, however, does not yet cover the general case, since we assumed for the extraction 
of information that the matrix Ai has full column rank, i.e. that all columns of Ai 
are linearly independent, which is not always the case. Fortunately, it is now no more 
difficult to treat the general case. So, we return to the original system (10.24) together 
with its decomposition (10.25) according to the variables in s — t and i. But this time 
we allow that the rank fc of Ai may be less than the number of columns, к < \s —1\. 
Then, there are fc linearly independent columns in Ai and we again assume that these 
are the fc first columns. Denote the matrix formed by these fc linearly independent 
columns by A'j and the matrix formed by the remaining \s — t\ — к columns by A". 
The columns in the second matrix can be expressed as linear combinations of the first 
fc columns. This means that there is a fc x (m — fc) matrix Л such that 
A'/ = 
A;A. 
Within the matrix A[ of these first fc columns there is a regular fc x fc submatrix. 
Now, if we decompose the matrix A of the system (10.24) according to k, \s —1\ — к 
and |i| columns and the rows according to the first fc and the remaining m — к rows, 
we have 
Αι,ι 
Α Μ Λ 
Αι)2 
A2,i 
А 2дА 
A 2 2 

GAUSSIAN INFORMATION AND GAUSSIAN POTENTIALS 
4 1 3 
The system (10.24) then decomposes accordingly, if x denotes the set of variables 
associated with the first к columns of Ai and у the set of variables associated with 
its remaining columns such that x U y = s — t, we have 
А 1 , 1 Х 4 х + А 1 Д Л Х ^ + А 1 1 2 Х ^ + и л 
= zi, 
А 2 д Х ^ + А 2 д А Х ^ + А 2, 2Х^+и; 2 = z2. 
As before, the first part of this system can be solved for X.^x + Λ Χ ^ which gives 
χ 1 * + Λ Χ 4 ! / + Α-ιΑΐι2χ|ί + Α-ιωι 
= 
Al\zu 
(A2,2 - A2,iA^"jAi,2)X4-t + (-A 2,iAjJ^i + ω2) = - А 2 д A^jzi + z2. 
We observe that ω is transformed into ώ = Τω just as before and consequently, ω still 
has the concentration matrix Κ ώ as computed in (10.30). It follows from equation 
(10.31) that the same holds for Κώ2. As above we define the matrix 
Ifc 
Λ 
Aj"jAii2 
Om-fc,fc 
Om-k,\s-t\-k 
A 2 j 2 -A^ArJA^J 
where the submatrices Ai (left-hand column), Ä2)i and Ä2>2 (right upper and lower 
submatrices) are still the same. As a consequence, the generalized Gaussian potential 
of the projected system 
llt : A , 2 X it 
■ω 2 
z2 
does not change and equation (10.29) still holds. In contrast, the potential of the 
original system changes somewhat. If we define 
B1 
= [ л т om_ 
we obtain 
С = Α τ Κ ω Α = Α τ Κ ώ Α = 
m — k,\s—t\ — k 
ÄfKoÄi 
Ä f K ^ B 
ÄfKöÄa 
Β τ Κ ώ Α ! 
Β τΚα,Β 
Β τ Κ ώ Α 2 
Ä^KaÄ! 
Α^Κ ώΒ 
А^К^Аг 
Writing Cjj for the submatrices above and i,j = 1,2,3, we remark that С^д is the 
old Ci,i computed above, Сз,з is the old C2,2, Сзд and Ci,3 are the old С2д and 
Ci)2. It only remains to compute the new C2,2, С2д, Сз,2 and the symmetric Cij2 
and C2>3. From the matrix above one easily obtains 
C2,2 = 
Λ τ(Α^Κ ωΑ χ)Λ, 
C2,i 
= 
Λ τ(Α^Κ ωΑ!), 
Сз,2 = Ä2"4(AfKaJA1)A + Ä|;2K2A1A. 

4 1 4 
GAUSSIAN INFORMATION 
The remaining matrices Cj.,2 and С2,з are the transposes of Сгд and C3j2· As before, 
it follows from these remarks that 
Сз,з — СздС-^С^з 
^з - C3iiCj"}i/i 
С, 
Ό. 
More generally, it is now easy to verify that 
С 1 г [ Ci,2 
Ci,3 J = 
C<2,2 
Сз,2 
С2,з 
Сз,2 
Сгд 
Сзд 
О О 
о с 
and also that 
"2 
Сг,1 
Сзд 
C f > i 
= 
о 
This clarifies the general situation concerning extraction of information or projection. 
The following theorem summarizes this important result: 
Theorem 10.2 Consider the linear Gaussian system I : AX + ω = zfor the set s of 
variables and the system № for t Ç s obtained by eliminating the variables in s — t. 
If (у, С) = e(Z) is the generalized Gaussian potential for the system I, then there is 
a subset x Ç y = s — t such that Cix is regular of rank гапк(С^х) 
= 
rank(C^y). 
For each such subset x, 
<ilt) 
ί Ί Xt 
r~iit,x /сл\.х\ — \ \.x çï\t 
г~л\.1,х /г~л^х\— \filx,t\ 
There may be several subsets x С s — t for which the statement of the theo-
rem holds, and for each such subset the generalized potential e(l^1) is the same. In 
practice, this projected potential will be computed from the original potential {v, C) 
by successive variable elimination. Gaussian potentials summarize the information 
contained in a linear Gaussian system. In the following section, we show that these 
potentials form a valuation algebra that later enables the application of local com-
putation for the efficient solution of large, linear Gaussian systems. Illustrations are 
given in Section 10.5 and 10.6. 
10.4 VALUATION ALGEBRA OF GAUSSIAN POTENTIALS 
In the previous section, we defined combination of generalized Gaussian potentials, 
representing union of the underlying linear Gaussian systems, and projection of 
generalized Gaussian potentials, representing extraction of information in the cor-
responding linear Gaussian system. In this section, we show that the generalized 
Gaussian potentials form a valuation algebra under these operations and that the 
valuation algebra of ordinary Gaussian potentials introduced in Instance 1.6 essen-
tially is a subalgebra of this valuation algebra. For this purpose, we show that hints 

VALUATION ALGEBRA OF GAUSSIAN POTENTIALS 
4 1 5 
form a valuation algebra isomorphic to the algebra of generalized Gaussian potentials. 
To start, we formally define the algebra under consideration. Let r be a set of 
variables and D the subset lattice of r. For any non-empty subset s Ç r w e consider 
pairs (v, C) where С is a symmetric, non-negative definite s x s matrix and v an 
s-vector in the column space C(C) of C. Denote by Ф5 the set of all such pairs. For 
the empty set we define Фд = {(о, о)}. Finally, let 
Ф = иф* 
sÇr 
to be the set of all pairs (u, C) with domain subset of r. The elements of Ф are called 
generalized Gaussian potentials, or shortly, potentials. Note that ordinary Gaussian 
potentials in the form (Κμ, К) also belong to Ф. Within Ф we define the operations 
of a valuation algebra as follows: 
1. Labeling: d{v, C) = s if (и, С) е Φβ. 
2. Combination: If d(yi, Ci) — s and dfa, C2) = t then 
KCi)®(i*,C2) = (vlsUt + 4sUt,ClsUt + ClsUt). 
3. Projection: If d(v, C) = s and i Ç s then 
(г/, С)4-' 
= 
(vil - c^x(Cix)~lvix, 
Clt - 
clt,x(cix)-lCixA, 
where x C s — t is selected such that C^* is regular with rank rank(C^x) — 
гапк(С^~(). 
According to Theorem 10.2 such a subset always exists. 
Instead of directly verifying the valuation algebra axioms for these operations, it 
is easier to examine a corresponding algebra of hints. Indeed, we know that potentials 
faithfully represent the associated operations with linear Gaussian systems, in partic-
ular with Gaussian hints. It turns out to be easier to verify the axioms of a valuation 
algebra with hints rather than with potentials. We remember however that operations 
with hints are not uniquely defined. For example, depending on how the variables in 
s — t are eliminated in a hint h : AX + ω = ζοη variables s, the reduced systems 
h^ are different, but equivalent. Therefore, rather than considering individual hints 
h, we must look at classes of equivalent hints [h]. So, as with potentials, we define 
Фя to be the family of all classes [h] where h are hints relating to the subset s Ç r o f 
variables and 
Ф = u*»· 
sÇr 
As above, we represent hints h by triplets (A, z, K) where A is an m x s matrix 
with full row rank, hence m < \s\, z is an то-vector and К a symmetric, positive 

416 
GAUSSIAN INFORMATION 
definite s x s matrix. In order to define the basic operations of labeling, combination 
and projection with hints we denote the operation of the union of two linear systems 
l\ = (Ai, zi, Ki) and /2 = (A2, Z2, K2) on variables s and t respectively by 
A t«Ut 
L A 2 
J 
5 
Zl 
z2 
5 
Ki 
0 
0 
K2 
Similarly, for a system I = (A\, zi, Ki) on variables in s and t С s, let 
I* 
= 
(Ä2,2,Z2,Ku) 
denote a system like (10.28) were the variables in s — t have been eliminated. Finally, 
if / is a linear Gaussian system, then let h(l) denote the equivalence class of hints 
derived from I. We are now able to define the basic operations in Ф: 
1. Labeling: d([h]) = s if [h] € Ф5. 
2. Combination: For hi 6 [/ii] and /i2 G [/12] 
[/ii]® [/12] = 
h(hi®h2). 
3. Projection: If d([h]) = s and t Ç s then for /1 e [h] 
[/ip 
= 
/i([/i4*]). 
These operation are all well-defined, since equivalent systems have equivalent unions 
and projections. The very technical proof of the following theorem can be found in 
Appendix J. 1. 
Theorem 10.3 The algebra of hints (Ф, D) with labeling, combination and projec-
tion satisfies the axioms of a valuation algebra. 
We show that the algebra of generalized Gaussian potentials (Ф, D), which consists 
of the potentials e(h) € Ф associated to the hints h € Ф, also forms a valuation 
algebra. We first recall that the mapping e : Ф -> Ф, defined by e([h]) = e(h), 
respects labeling, combination and projection 
d(e([h])) = d([h\), 
е([ЛЬ®[Л]а) = e([/i]i)®e([/i]2), 
e([h]*) - е([Л])*. 
This has been shown in Section 10.3. Further, we claim that the mapping is onto. In 
fact, let (v, C) be an element of Ф. С is therefore a symmetric, non-negative definite 
s x s matrix and v an s-vector in the column space C(C) of C. For any symmetric 
non-negative definite s x s matrix С of rank к < \s\, there exists а к x s matrix A 
of rank к such that С = A TA. This is a result from (Harville, 1997). The columns 
of С span the same linear space as A T and therefore, there is a /c-vector z such that 
v 
= 
A Tz. 

AN APPLICATION: GAUSSIAN DYNAMIC SYSTEMS 
4 1 7 
Consequently, h = (A, z, Ifc) is a hint on s such that 
e(h) = e([h}) = (ATIfcz,AIfcAT) = (A T,A TA) = 
(V,C). 
The fact that (Φ, D) is a valuation algebra then implies that (Φ, D) is a valuation 
algebra, too, and that both algebras are isomorphic. 
We have seen in Section 10.1 that Gaussian hints h = (A, z, K), where A has 
full column rank, are represented by ordinary Gaussian potentials 
ί ( Α Γ Κ Α ) - 1 Α Γ Κ ζ , Α Τ Κ Α ) , 
that in turn determine Gaussian densities. Such a potential can also be one-to-one 
transformed into a generalized Gaussian potential (A TKz, A TKA), and combina-
tion as well as projection of ordinary Gaussian potentials correspond to the operations 
on generalized Gaussian potentials. Thus, the original valuation algebra of ordinary 
Gaussian potentials can be embedded into the valuation algebra of generalized Gaus-
sian potentials and can therefore be considered as a subalgebra of the latter. These 
insights are summarized in the following theorem: 
Theorem 10.4 Both algebras of hints (Φ, D) and generalized Gaussian potentials 
(Φ, D) are valuation algebras and e : Φ —>· Φ is an isomorphism between them. The 
valuation algebra of ordinary Gaussian potentials is embedded into (Φ, D). 
Consequently, we may perform all computations with hints also with generalized 
Gaussian potentials. This will be exploited in the remaining sections of this chapter. 
10.5 AN APPLICATION: GAUSSIAN DYNAMIC SYSTEMS 
Linear dynamic systems with Gaussian disturbances are used in many fields, for 
instance in control theory (Kaiman, 1960) or in coding theory (MacKay, 2003). The 
basic scenario is the following: the state of a system changes in time in a linear 
way and is also influenced by Gaussian disturbances. However, the state cannot 
be observed directly, but only through some linear functions, again disturbed by 
Gaussian noise. The problem is to reconstruct the unknown present, past or future 
states from the available temporal series of measurements. This is a very well-known 
and much studied problem, usually from the perspective of least squares estimation 
or of maximum likelihood estimation. Here, we shall look at this problem from the 
point of view of local computation in the valuation algebra of generalized Gaussian 
potentials. Therefore, this section will be an illustration and application of ideas, 
concepts and results introduced in previous parts of this chapter. Although the main 
results will not be new, the approach is dramatically different from the usual ways 
of treating the problem. This results in new insights into the well-known problem 
which is a value in itself. We should, however, credit (Dempster, 1990a; Dempster, 
1990b) and also refer to (Kohlas, 1991; Monney, 2003; Kohlas & Monney, 2008) 

418 
GAUSSIAN INFORMATION 
for applying the theory of Gaussian hints to this problem. But the use of generalized 
Gaussian potentials, as proposed here, is different and more straightforward. We also 
refer to Instance 9.2 for a simplified version of a similar problem. 
■ 10.2 Kaiman Filter: Filtering Problem 
The basic model of a time-discrete, linear dynamic system with additive Gaus-
sian noise is as follows: 
Xfc+i 
= 
AfcXfc+Wfc, 
(10.32) 
Yfc 
= 
HfcXfc + ^fc, 
(10.33) 
Yfc 
= 
yfc, 
(10.34) 
Xo 
= 
ωο, 
(10.35) 
for к = 1,2,... Here, Xfc are n-dimensional vectors, the matrices Afc are 
n x n real-valued matrices, the vectors Yfc are m-vectors, yfc G Шт and Hfc 
denote mxn real-valued matrices. The n- vector disturbances ω^ and то-vector 
disturbances vk are distributed normally with mean vectors 0 and variance-
covariance-matrices Qfc and Rfc respectively. Equation (10.32) defines a first 
order Markov state evolution process. Further, equation (10.33) defines the 
measurement process for state variables, namely how the measurement Yfc 
will be determined from the state Xfc. In equation (10.34) the actually observed 
value yfc is introduced into the system. Finally, equation (10.35) fixes the 
initial condition of the process. Although this is usually introduced as part of 
the system, this information is not really needed in our approach. We come 
back to this point later in the development. Such a linear Gaussian system is 
called a Kaiman filter model after the inventor of the filter solution associated 
with the system. We can easily transform this system into the standard form of 
a linear Gaussian system, 
Xfc 
Xfc+1 
+ u>fc = 
0, 
(10.36) 
HfcXfc + ^fc = 
yk, 
(10.37) 
- Χ 0 + ω 0 
= 
0. 
(10.38) 
Note that we substituted the observation equation (10.34) directly into (10.33) 
as suggested in the previous Section 10.6. This has the effect that the variables 
Yfc disappear from the model. In fact, the Kaiman filter model is also an in-
stance of a Gaussian Bayesian network as discussed in the following Section 
10.6. Figure 10.4 represents the system graphically. Alternatively, if conditional 
Gaussian densities instead of the linear Gaussian systems are given in the graph 
of Figure 10.4, we speak of a hidden Markov chain. It will be shown in Section 
10.6 that we may reduce this model to a Kaiman filter model. 

AN APPLICATION: GAUSSIAN DYNAMIC SYSTEMS 
4 1 9 
/ Xk J 
»/хк-и J 
»/хк*2 J 
JL 
JL 
" 
( yk j 
f yk*1 J 
f yk*2 J 
Figure 10.4 
The direct influence among variable groups in the Kaiman filter model. 
If we consider the standard form of the Kaiman filter model given above, but 
with the variables Yfc eliminated, we see that we may cover this system by the join 
tree of Figure 10.5. Each subsystem (10.36) is assigned to the nodes labeled with 
Χ^, Xfc+i- We point out that node labels in join trees usually correspond to either 
sets of variables or indices. Here, it is more convenient to refer to the node labels by 
the vectors directly. So, the subsystems (10.37) are affected to the nodes with label 
Xfc, and the initial condition (10.38) is on the node labeled with Xo. To each of these 
linear Gaussian subsystems belongs a corresponding generalized Gaussian potential. 
Let фк,к+1 for fc = 1,2,... denote the potential on node Xfc, Xfc+i, фо the potential 
on node Xo and φ^ are the potentials on the nodes Xfc. According to equation (10.12) 
these potentials are 
(o, Qo1), 
»fc,fc + l 
0, 
A^Q^Afc 
-A^Q,- 1 
and 
■фк 
-Qfc'Afc 
< V 
(10.39) 
(10.40) 
Figure 10.5 A covering join tree for the Kaiman filter model. 
Assume now that measurements yfc are given for к — 1,..., h. The information 
about the whole system (10.32), (10.33), (10.34) and (10.35) for к = 1,..., h is then 
represented by the potential 
Xh 
= 
fc,fc+l 
i-h 
Computing χ£ means to infer about the state X^ given the Kaiman filter model and 
state measurements for к — 1 up to к = h. This is called the filter problem for time 
h. It can easily be solved by orienting the join tree of the Kaiman filter model in 

420 
GAUSSIAN INFORMATION 
Figure 10.5 towards node X/j and applying the collect algorithm from Section 3.8. In 
fact, due to the linear structure of the join tree in Figure 10.5, after the messages are 
passed up to node X^-i, this node contains the valuation χ^_~[ . Then, the message 
Xh-i 
^ sent from node Χ/,-ι to node Χ^-ι,Χ^ where it is combined with the 
node content 4>h-i,h- Finally, the result is projected to h and sent to the root node 
X/j where it is again combined to the node content. The root node then contains 
( 
ih-l 
(10.41) 
This is a recursive solution of the filtering problem. The recursion starts with the 
initial condition χζ = φο and is applied iteratively for each time step h — 1,2,... 
incorporating the new state measurements at time h. The message 
ß(h-\,h)-*h 
= 
[XhL.11 ® <i>h-i,h) lh 
(10.42) 
sent from node X^-i, X/г to node X/j, is also called the one-step forward prediction. 
It corresponds to the inference about the state variable X^ before the measurement 
y h of this state becomes available. Note that these results do not yet depend on the 
actual valuation algebra instance, but they only reflect the linear form of the join tree. 
The recursive computation of the potentials (10.41) can be translated into corre-
sponding matrix operations for the elements involved in the potentials, if we use the 
definitions of combination and projection of the algebra of generalized Gaussian po-
tentials given by (10.39) and (10.40). The recursion (10.41) can now be transformed 
step-wise into the corresponding operations on the associated generalized Gaussian 
potentials. To start with let 
xtk = K.Cfc), 
where for к = 0 we have the initial condition 
i>o = (bO.Qö1)· 
4.0 
Xo 
We now apply the valuation algebra operations for generalized Gaussian potentials 
defined in Section 10.4. If follows from the rule of combination that 
Ik 
Xk ' 
bk,k+i — 
(vk,Ck) 
o, 
-AiQ,- 1 
QH1 
0 
Tn-i 
AiQ 
A* 
1 
Cfc 
- A £ Q ^ 
- Q ^ A / c 
Qfc1 
From this we compute the projection or one-step forward prediction (10.42), 
M(fe,fc+i)^fc+i 
= 
(Q^AfctAjQ^Afc + Cfe)"1^, 
(10.43) 
Q^ 1 - Qj^1 AfciA^Q^1 Afc + С*)"1 A£Qfc x) (10.44) 

AN APPLICATION: GAUSSIAN DYNAMIC SYSTEMS 
421 
Here, we tacitly assume that the matrix A^Q^ 1 Afe + Cfc is regular. This is surely the 
case if either Afc or Cfc is regular, since then either A^Q^Afc or Cfc is symmetric, 
positive definite. If this is not the case, then Theorem 10.2 must be applied to compute 
the projection. We further define 
M(fc,fc+i)-»fc+i 
= 
(^fe,fc+i, Cfc^fc+ij, 
with 
^fc,fc+i 
= 
Qfc^AfctA^Q^Afc + CfcrVfc 
(10.45) 
Cfc,fc+1 
= 
Q ^ - Q ^ A f c t A ^ Q ^ A f c + C f c r ^ Q f c " 1 
(10.46) 
This determines the one-step forward prediction. Finally, we obtain the filter potential 
(10.41) for Xfc+i, given the observation y i , . . . , Ук+ь again by the combination rule 
for generalized Gaussian potentials: 
Xfc+l1 
= 
M(fc,fc+l)->k+l ® "Фк+1 
= 
(^fc.k+i+Hfe+1Rfc+1yfc+i, 
Cfc;fc+i+Hfe+1Rfc+1Hfc+1J. 
So, we have 
ί/fc+i 
= 
i/fe,fe+i+Hfc
r
+1R^1yfc+i, 
(10.47) 
Cfc+i 
= 
Ck,k+i + Hfc+1Rfc+1Hfc+i. 
(10.48) 
The equations (10.45), (10.46), (10.47) and (10.48) define a general recursion scheme 
for the filtering solution of a linear dynamic system with Gaussian disturbances. Note 
that in this general solution no assumptions about the initial condition of Xo are 
needed. In fact, it is not necessary that фо is an ordinary Gaussian potential as as-
sumed above. Further, no regularity assumptions about the matrices Afc are needed; 
provided that for the computation of the one-step forward projection the general pro-
jection rule of Theorem 10.2 is used. However, in the usual Kaiman filter applications 
as discussed in the literature, see for instance (Roweis & Ghahramani, 1999), the 
initial condition фо in the form of an ordinary Gaussian potential is assumed and 
the matrices Afc are assumed to be regular. Consequently, all the filter potentials 
XÏ. are ordinary Gaussian potentials, which of course simplifies the interpretation of 
the filter solution as well as the computations. That the potentials \ k are ordinary 
Gaussian potentials in this case follows by induction over k. Indeed, χ^ = фо is an 
ordinary Gaussian potential by assumption. Assume then that this holds also for χ\. 
which means that in the generalized Gaussian potential xj. = (i/k, Cfc) the matrix 
Cfc is positive definite, hence regular. But then, as remarked above, the same holds 
for Cfctfc+i and by the same argument it then also holds for Cfc defined in (10.48). 
We propose in this case to compute directly with the mean vector and the variance-
covariance matrix of the different potentials. If χ\ 
= (ffc,Cfc) is an ordinary 

422 
GAUSSIAN INFORMATION 
Gaussian potential, i.e. the matrix C^ is regular and Ck
 λ = Σ/t is the variance-
covariance matrix of the filter solution at time k, then 
xik ® Фк,к+1 
is an ordinary Gaussian potential too. It represents a two-component vector, where 
the first component is the filter solution at time к and the second component the 
one-step forward prediction to time к + 1. It has the variance-covariance matrix 
A№A f c 
- Q ^ A * 
-A£QiT 
SfcAT 
AfcEfc 
AfcEfcA^. 
LfcSfc 
Qfc 
(10.49) 
This can easily be verified. Then, since v^ = Ckßk if μ^ is the mean value of the 
ordinary potential representing the filter solution \ k at time k, we conclude that 
Mfc 
ßk,k+l 
S t 
AfeSfc 
Σ * Α £ 
1 
AfcSfcA^ + Qfc _ 
Cfc/ifc 
0 
Mk 
Afc/ifc 
where ßk,k+i = Ck k+lVk,k+i is the mean vector of the one-step forward prediction. 
It follows that 
ßk,k+l 
Afc/ifc. 
This, by the way, is a result which could also be derived from elementary probability 
considerations. For the variance-covariance matrix of the one-step forward prediction 
we read directly from equation (10.49) that 
Sfc,fc+i 
= 
AfcEfcAfe + Qfc. 
Next, we go for the filter хк_^1 
= (uk+i,Ck+i). According to equation (10.48), the 
inverse of C^+i is the variance-covariance matrix of the filter solution at time к + 1, 
Sfc+i 
— (Cfc^+i +H f c + 1R f c + 1H f c +i) 
= 
Sfc,fc+i — Efcife+iHfc+1(Hfc+iEfcifc+1Hfc+1 + Rfc+i) 
Hk+i^k,k+i-
Here, we use a well-known and general identity from matrix algebra (Harville, 1997): 
T / - . - 1 T J \ - 1 
(А + В'С - 1В) 
A"1 - A" 1B r(BA- 1B T + C)^BA" 

AN APPLICATION: GAUSSIAN BAYESIAN NETWORKS 
4 2 3 
and Efc,fc+i = Ck
 l
k+l. This result together with equation (10.47) can be used to 
compute the mean value ßk+i of the filter solution, 
Mfc+i 
= 
C f c + 1 i ^ + i 
= 
ΣΑ:+1(ΣΑ:Λ+1//ί;Λ+ι +Hfc+1R^"+1yfc+i) 
= 
(Sfe,fe+i — Efcjfc+iHfc+1(Hfc+iEfcifc+iHfc+1 + Rfc+i) 
Hfc+iSfc^+i) 
= 
ßk,k+i — Sfc)fc+iHfc+1(Hfc+iEfcife+iHfc+1 
+Rfc+i) 
[—(Hfe+iSfeifc+iHfc+1 + Rfc+i)Rfc+1yfc+1 + H^+^fc^+i + 
Hfc+iEfcjfc+iHfe+1Rfc+1yfc+i] 
= 
Mfc,fc+i ~~ Sfeife+iHfc+1(Hfc+iEfcifc+iHfc+1 + Rfe+i) 
[H.k+ißk,k+i - yfc+i]. 
This determines the classical Kaiman filter for linear dynamic systems with Gaus-
sian noise. Often, the gain matrix 
Kfc 
= 
Efcifc+1Hfc+1(Hfc+iEfci)t+iH/t.+1 + Rfc+i) 
is introduced and the filter equations become 
ßk+l 
= 
ßk,k+l — J^k(ük + lßk,k+l ~ Ук+l) 
(10.50) 
Sfc+i 
= 
S/^fc+i - KfcHfc+iSfc^+i 
(10.51) 
We finally remark that these recursive equations are usually derived by the least 
squares or by maximum likelihood methods as estimators for the unknown state 
variables. In contrast, they result from assumption-based reasoning in our approach 
and the potentials represent fiducial probability distributions. Although the results 
are formally identical, they differ in their derivation and interpretation. 
10.6 AN APPLICATION: GAUSSIAN BAYESIAN NETWORKS 
Graphical models such as the Bayesian networks of Instance 2.1 are popular tools for 
probabilistic modeling, in particular using discrete probability distributions. These 
models serve to generate a factorization of a probability distribution into a product 
of conditional distributions, which is then amenable to local computation. A similar 
scheme applies also to Gaussian densities. 
■ 10.3 Gaussian Bayesian Networks 
We recall that a Bayesian network is a directed, acyclic graph G = (V, E) 
with a set V of nodes representing variables and a set E of directed edges. We 
number the nodes from 1 to m = I V\ and denote the variable associated with 

4 2 4 
GAUSSIAN INFORMATION 
node iby Xi. The parent set pa(i) of node i contains all the nodes j with a 
directed edge from j to i, i.e. pa(i) = {j G V : (j, i) G E}. Nodes without 
parents are called roots. Let Xpo(j) denote the vector of variables associated to 
the parents of node i. Then, a conditional Gaussian density function / χ ^ χ 
(i) 
is associated with every node i G V. If i is a root, this is simply an ordinary 
Gaussian density fXi. The graph gives rise to a function / of m variables which 
is the product of the conditional Gaussian densities associated with the nodes, 
f{xi,...,Xm) 
= 
fJ/xi(Xf,a(i)(Xi|Xpa(i))· 
iev 
Theorem 10.5 The function f{x\,..., 
xm) is a Gaussian density. 
Proof: It is always possible to number the nodes in the graph G in such a way that 
any parent j of a node г has a smaller number than i, i.e. j < г if j G pa(i). Such an 
order of the nodes is called a construction order (Shafer, 1996). Let us now assume 
that the nodes of G are numbered in a construction order. We then have 
J \X\, 
. . . 
,Χγη} 
/xi(^i)/x1|x2(:Eila;2)/x3|xpa(3)(a;3|xpa(3)) · · •/xm|xpa(m)(a;™|xpa(m))· 
Clearly, each subproduct from j = 1 up to some i < misa Gaussian density, hence 
this holds for the whole product. 
■ 
The factorization of a Gaussian density f(xi,..., 
xm) into a product of condi-
tional Gaussian densities as induced by a Bayesian network is very particular. We 
may have much more general factorizations of Gaussian densities, 
f(xi,...,xm) 
= 
Д 
/x„|xt(x/i|xt) 
(10.52) 
(h,t)ej 
where J is a family of pairs (h, t) of subsets h, t of the index set { 1 , . . . , m} of 
the variables Χχ,..., 
Xm. The set h is called the head and the set t the tail of the 
conditional density /x h|x t (x/Jxt). The symbols X^ and X t represent the vectors of 
variables over the index sets h and t respectively. Similarly, x/j and xt represent the 
corresponding real-valued vectors of the variable vectors. Of course, the collection 
of conditional Gaussian densities /xh|xt(x/i|xt) must satisfy some conditions to 
turn f(xi,..., 
xm) into a Gaussian density. In fact, it is necessary and sufficient that 
the conditionals can be brought in such an order (hj,tj) for j = 1,..., n that the 
following conditions are satisfied: 
1. ii is empty; 
2. U С d\ U... U di^i and hi is disjoint from d\ U... U di-i where di = hiUti. 
Theorem 10.6 If these conditions above are satisfied, the product 
г 
Π/χ^ιχ<,(χ^Ιχί3) 

AN APPLICATION: GAUSSIAN BAYESIAN NETWORKS 
4 2 5 
is a Gaussian density for all г = 1,..., п. 
Proof: We proceed by induction: The theorem holds for i — 1 since 
/x M|x t l(x/nl xti) 
= 
/x h l(x/n) 
is a Gaussian density. Assume that the theorem holds for г — 1, i.e. 
i - l 
f(xu...,xk) 
= П^1 хь( Х^1 Х'^' 
j = l 
where {1,..., к} = d\ U · · · U dj-i is a Gaussian density. Then 
/ ( X l , . . . , Xk)fyLh. 
+ l |Xt. + 1 (Xhi + 1 |Xt i + 1 ) 
is a Gaussian density too, because i;+i is a subset of the variables 1 to к of the prior 
/(xi,. ■ ■, Xfc) and hi represents variables outside the prior (see Appendix J.2). 
■ 
A sequence of pairs (hj,tj) satisfying these conditions is called a construction 
sequence (Shafer, 1996) for the Gaussian density of equation (10.52). 
Example 10.5 Consider the linear dynamic Gaussian system discussed in the pre-
vious section. The dynamic equations (10.32) induce conditional Gaussian densities 
with head X$ and tail X,-i for г = 1,2,... The measurement equations (10.33) 
can be represented by conditional Gaussian densities with head Xj and tail Yj. 
If we further assume ordinary Gaussian densities for Xo and Yi (interpreted as 
"conditional" densities without tail) then a possible construction sequence is 
(X0,0), (Y l t 0), ( X b Yx), ( X b Xo), (Y2,0), (X2, Y2), (X2, Xi), · · · 
There are other ones. Construction sequences are generally not unique. 
The decomposition of a Gaussian density into a product of conditional densities 
is a problem of modeling. Bayesian networks may help. Subsequently, we assume 
a given factorization and focus on computational aspects. For local computation of 
marginals of such a factorized Gaussian density (10.52) we need a join tree that covers 
the domains d = h U t of the head-tail pairs (h, t) G J as explained in Chapter 3. 
Let (V, E, λ, D) be such a join tree, where D is the lattice of subsets of the index 
set {1,..., m) of the variables of the Gaussian density f(x\,..., 
xm). So we have 
d Ç A(j) for every conditional (h, t) € J for some node j € V. Assume that we 
want to compute the marginals of f{x\,..., 
xm) for all variables X\ to Xm. For that 
purpose, it suffices to compute the marginals of /(χχ,..., xm) for all node labels 
X(j) in the join tree. The one-variable marginals follow then by one last projection 
per variable. This can be achieved by the Shenoy-Shafer architecture of Section 4.1. 
As a preparation, we transform all the conditionals /x h|x t(x^|xt) into generalized 
Gaussian potentials (fh,t,Ch,t)- To do so, we represent the conditional Gaussian 

426 
GAUSSIAN INFORMATION 
density /xh|xt(x/i|xt) with mean vector Zh,t + A/ijtxt and concentration matrix 
K/,)t by a linear Gaussian system 
z/!,t + A/, tX t + ω Λ ί 
(10.53) 
with concentration matrix Κωι,, = К/г.t. This transforms into the standard form of 
a linear Gaussian system 
*-wh,t 
[ I - A M ] 
Xfc 
X* 
W/!,t 
Z/i,t 
and then into the generalized Gaussian potential 
K-h,tzh,t 
■A M K / i , t 
-Κ^,ίΑ^,ί 
Α^ίΚ/,,ίΖ/,,ί 
(10.54) 
(10.55) 
according to equation (10.12) in Section 10.3. We then select an arbitrary node r e V 
of the join tree as root node and execute the collect phase of the Shenoy-Shafer archi-
tecture with the generalized Gaussian potentials on the nodes of the join tree. At the 
end of the message-passing, the Gaussian potential of the marginal of f(xi,..., 
xm) 
to the root node label \{r) can be obtained from the root node. This corresponds 
to an ordinary Gaussian potential since this marginal is a Gaussian density. After 
the distribute phase, the marginal of f(x\,..., 
xm) to each node label \{i) can be 
obtained from the corresponding join tree node г € V, and these marginals are all 
ordinary Gaussian potentials. 
As long as we remain within the valuation algebra of the generalized Gaussian po-
tentials, we have to use the Shenoy-Shafer architecture due to the absence of a division 
operator in this algebra. However, we may embed this valuation algebra into the larger 
valuation algebra of potentials (v, C), where С is only assumed to be symmetric, but 
no more non-negative definite. In this algebra, division is defined and the application 
of local computation architectures with division such as the Lauritzen-Spiegelhalter 
or the HUGIN architecture become possible for factorizations of Gaussian densities 
into conditional Gaussian densities. For details see (Eichenberger, 2009). 
An essential aspect missing so far concerns observations. Assume that we have 
observed the values of some variables and want to condition the Gaussian density 
/(xi, ■ ■ ■, xm) on these observations and again compute the marginals of the con-
ditional density to all remaining variables. This amounts to adding simple equations 
like Xi = Xi to the system. More generally, we may need to add general linear sys-
tems without disturbances. This gives a combination of linear Gaussian systems with 
ordinary linear systems of equations. We shall not treat this general case and refer 
to (Eichenberger, 2009) where an extension of the valuation algebra of generalized 
Gaussian potentials to a valuation algebra covering both ordinary linear equations 
and linear Gaussian systems is developed. Here, we treat only the simpler case of 
observations of single variables. Consider a Gaussian density f(x\,..., 
xm) defined 
by a factorization (10.52) and assume that the values of a subset of variables Xi 

AN APPLICATION: GAUSSIAN BAYESIAN NETWORKS 
4 2 7 
with г G о has been observed such that the equations Xi = Xi for г € о hold. We 
decompose the index set { 1 , . . . , m} into о and its complement oc and let X 0 and 
X0c denote the corresponding variable vectors. Then we want to compute marginals 
of the conditional density 
/ х о с | Х 0 = х 0 ( Х И х о ) 
/ ( x 0 , X 0 c ) 
/ i 0 ( X o ) 
' 
where /^°(x0) is the marginal of / with respect to o. This is clearly again a Gaus-
sian density. In fact, it is simply the nominator /(x 0,x 0c) renormalized to one. In 
view of the representation of Gaussian and conditional Gaussian densities by linear 
Gaussian systems and their associated generalized Gaussian potentials, the density 
f{xi, ■ ■ ■ ,xm) is represented by the totality of the linear Gaussian systems associated 
with the conditional Gaussian densities into which f(x\,..., 
xm) is factorized. So, 
we may simply add the observation equations X0 = x0 to this system. Or, we may 
simply replace all variables Xi with г £ о by their values x; in all these equations. 
This is exactly what we did in the linear dynamic Gaussian system discussed in the 
previous Section 10.5. 
To be a little more explicit, consider a conditional /x h|x t(xh|xt) of the factor-
ization (10.52) and its associated linear Gaussian system (10.54). Define hi = h П о, 
the set of observed head variables, /12 = h — h\ and similarly t\ = t По, the set of 
observed tail variables, i2 = t — t\. Decompose the matrix Α^,,ί according to theses 
subsets of h and t into 
k-M 
= 
Ai.i 
A2,i 
Ai,2 
A2,2 
where A^i is a hi x £1 matrix, Αχι2 is hi x i2> Агд is /12 x h and A2,2 is a 
/12 x ti matrix. Decompose also the variable vector X accordingly, whereas ω and z 
are decomposed according to the head variables hi and /г2 of the conditional density. 
Then the system (10.54) reads as 
I 
0 
0 
I 
- A M 
-Ai,2 
-A21 
—A2.2 
X/11 
Xfi 2 
X t 2 
+ 
= 
If we introduce the observed values X ^ = x^i and X t l = x^ into these equations 
and rearrange the equations to the standard form, we get the system 
0 
-Ai, 2 
1 
-A2,2 
X/l 2 
Xt 2 
+ 
= 
Z / l 2 
-
0 
+ 
' Αι,ι ' 
. A 2,i . xtl-
This permits to compute the associated generalized Gaussian potential and then to 
carry out local computation as above to get the marginals of the conditional Gaussian 
density f(xi,..., 
xm), given the observation Xi = ж» for г e o. 

428 
GAUSSIAN INFORMATION 
To conclude this section, we emphasize again that there is an alternative approach, 
which considers ordinary linear equations as information in the same way as con-
ditional Gaussian densities or the associated linear Gaussian systems and which is 
based on an extension of both the valuation algebra of generalized Gaussian potentials 
and the information algebra of linear systems (Eichenberger, 2009). This approach 
has the advantage that more general linear system than simple observations can be 
considered. Further, observation can be added as they arrive by using update versions 
of local computation (Schneuwly, 2007). This approach is also more in line with 
least squares estimation. We further mention that mixed discrete Gaussian systems 
have been considered in connection to local computation (Cowell et ai, 1999). These 
systems however do not fit anymore into the algebraic structure of valuation algebras. 
10.7 CONCLUSION 
The valuation algebra of Gaussian potentials was introduced in Chapter 1 in a pure 
algebraic context. In the first section of this chapter we gave meaning to this algebra. 
By dint of a technique called assumption-based reasoning, we showed that a Gaus-
sian potential can be associated with an overdetermined, linear system with Gaussian 
disturbances, whose matrix has full column rank. Combination and projection for 
Gaussian potentials then mirror the operation of union of equations and variable 
elimination in the associated linear system. However, we then observed by studying 
certain time-discrete, dynamic systems that the requirement of an overdetermined 
system with full column rank is too strong for many interesting applications. This 
inspired a generalization of Gaussian potentials that correspond to conditional Gaus-
sian distributions and that can be associated with arbitrary linear, Gaussian systems. 
This finally allows us to solve arbitrary linear systems with Gaussian disturbances by 
local computation. Two typical models of such systems and their local computation 
based solution were studied in the final part of this chapter. These models are Kaiman 
filters and Gaussian Bayesian networks. 
Appendix: 
J.1 VALUATION ALGEBRA PROPERTIES OF HINTS 
We are going to verify the axioms of a valuation algebra for the algebra (Φ, D) of 
hints following (Kohlas & Monney, 2008). This supplies the missing poof of Theorem 
10.3. We directly observe that the labeling (A2), projection (A3) and domain (A6) 
axioms follow directly from the definitions of labeling and projection. The remaining 
axioms are verified as follows: 
(Al) Commutative Semigroup: Commutativity of combination follows from the 
equivalence of the linear systems h\ φ h2 and h2 Θ hi. Therefore, 
[h] <g> [h2] = h(h1®h2) 
= h(h2®hi) 
= [h2] <g> [hi]. 

VALUATION ALGEBRA PROPERTIES OF HINTS 
4 2 9 
For the associative law of combination, we note that 
([hi] ® [h2]) ® [h3] = 
h(h'®h3), 
where h! G h(hi ® h2). But then Ы is equivalent to h\ ® h2 and further h' ® h3 
is equivalent to ((hi Θ /ι2) θ h3 = (/ii Θ (/12 Θ /13))· This in turn is equivalent 
to/ii®/i"fora/i" e h(h2®h3). So, finally /ι'φ/ι3 and Λ,χθ/ι" are equivalent. 
This implies that 
([hi] ® [h2]) ® [h3] = h(h'®h3) 
= h(hi ® h") = [h{\® ([h2]®[h3]). 
This proves associativity of combination. 
(A4) Transitivity: This axiom needs a somewhat longer reflection and is the most 
difficult part of the proof. We first generalize the approach to variable elimi-
nation, which is used to define projection. If h = (A, z, K) is a hint on s and 
t Ç s, we decompose the linear systems into 
ΑιΧ^-* 
+ A2Xlt 
+ ω = 
z. 
Now, if Ai is an m x \s — t\ matrix with rank к <m,\s — t\, then there is an 
m x к matrix Bi which spans the column space C(Ai) such that Ai = ΒχΛ. 
Further, there is an m x (m - k) matrix B 2 such that В = [Bi B2] is a regular 
m x m matrix. Let then T = B _ 1 and 
. T 2 
where Τχ is а к x m matrix and T2 an (то — к) x m matrix. The original 
system can be transformed into the equivalent system 
h' : TAX + Τω 
Tz. 
Here, ω = Τω has concentration matrix B T K B or variance-covariance matrix 
T K T - 1 . In a decomposed form, the matrix TA is written as 
TA = 
Ti 
T2 
[Ax A 2 ] 
Τχ 
T2 
BiA A2 
TiBiA TjA2 
T2BiA T2A2 
T1A1 TiA2 
Om-fc,fc T2A2 
and the transformed system becomes 
T i A i X ^ + TiAsX^ + Tju; = 
Τ 2 Α 2 Χ ^ + Τ 2 ω 
= 
In the second part of this system, 
ТгАзХ4-* +ώ2 
= T2z, 
Τιζ, 
T2z. 
(J.l) 

430 
GAUSSIAN INFORMATION 
the variables in s — t have been eliminated. The stochastic vectored = Τ2ω has 
a Gaussian density with concentration matrix (T 2K~ 1T^)~ 1 as the marginal 
with respect to t of the Gaussian vector ώ = Τω. Since the matrix T is regular 
and A has full row rank, it follows that TA has full row rank and therefore, 
the matrix T 2A 2 has full row rank too. Hence, 
( τ 2 Α 2 , Τ 2 Ζ , ( Τ , Κ " 1 ! ^ ) - 1 ) 
is a hint. It represents [h]^1 because the solution space of (J.l) is a projection 
of the solution space of the original system for every disturbance ω. This is a 
generalization of former approaches to compute a projection of a linear Gaus-
sian system, see Section 10.3.4. 
An (m — k) x m matrix like T 2 which has the property 
1. of full row rank m — к and 
2. T 2Ai = Om-k,k 
is called a projection matrix for A to t. A matrix A may have different projec-
tion matrices to i, but they all lead to equivalent systems. In fact, if T and T 
are projection matrices for A to t, then ΤΑχ = TAi = 0, then also AjTT 
= 
AjTT 
= 0, which shows that the rows of both matrices T and T are bases of 
the null space of Aj. This implies that there is a regular (m — k) x (m — k) 
matrix С such that T T = T TC. Hence, the system ТАгХ·1·' + Τω = Tz is 
the same as the system CTTA2X'|-t + CTTLÜ = C TTz, which is equivalent 
to the system ΤΑ 2Χ^ + Τω = Tz. 
Further, elimination of the same variables in equivalent hints results in equiva-
lent hints. Indeed, if hi = (A, z, K) and /i2 = (Ä, z, K) are equivalent hints, 
then both A and Ä are m x s matrices for some m and s and there is a regular 
m x m matrix T such that 
Ä = TA, 
z = Tz, 
K = ( T K - 1 T T ) - 1 . 
For t С s, decompose the matrix of the first system for the elimination of the 
variables in s — t into 
A 
- 
[ A ! 
AiA 
A2 ] , 
where Aχ is an m x к matrix of rank к < \s — t\ spanning the column space 
of the submatrix Α^~ι. 
The matrix A2 has dimension m x t. We decompose 
T 
= 
Ti 
T 2 

VALUATION ALGEBRA PROPERTIES OF HINTS 
431 
such that Ti is а к x m and T2 an (m — к) х m matrix and obtain 
TA 
T1A1 Т1А1Л TiA2 
T2A2 T2AXA T2A2 
The matrix ΤχΑι has rank к and is regular, because T is regular, hence Τχ 
has rank к as well as A\. It follows that the matrix 
P 
= 
[ - T a A i i T i A O " 1 
Im_fc ] , 
is a projection matrix for À to t. Indeed, we have 
PÄJ 
= 
PTAi = 
[ -TaAiOTiAi)- 1 
Im_fc 
= 
- T a A i O T i A O ^ T ï A i + T a A ! - 0. 
Ti 
T 2 
Ai 
The system reduced with projection matrix P is P A X + Ρώ = Pz or also 
РТАХ + ΡΤω = PTz. But the last system is the reduced system of AX + 
ω — z with projection matrix PT. This proves the equivalence of the reduced 
systems, hence of projected hints h\ and /i2, stated in the following lemma. 
We remark for later reference in the proof of the combination axiom that these 
considerations can be generalized to arbitrary linear Gaussian systems. We 
have proved the following lemma: 
Lemma J.l Projections of equivalent systems are equivalent. 
These results about projection of hints with the aid of projection matrices 
finally permit to prove the transitivity axiom of projection. Let h — (A, z, K) 
be a hint with label x and s C i C i . Select a projection matrix P\ for A to t. 
Decompose the matrix A into A = [ A ^ - 4 A^ t _ s A^s]. We then obtain for 
the hint h projected to t, 
[h] 
It 
([PiA4·*-8 Pi A4-8], Piz, ( P i K ^ P f ) - 1 ) 
According to the discussion above, this unambiguously defines the projection 
of hint [h]. Further, let P2 be a projection matrix for Pi A to s, such that 
([ΛΓ) 
4-t\4-s 
( P 2 P I A 4 S , P 2 P l Z , ( P . P i K ^ P f Ρ ^ Γ 1 ) 
(J.2) 
The matrix P i P 2 is indeed a projection matrix, since PiA 4^ * = 0 and 
PaPiA4-4-8 = 0 , hence 
P 2 P ! [ A4-*"* 
A4·*"8 ] = P 2 [ 0 
РзА4-«"8 ] 
0. 
Both matrices Pi and P 2 have full row rank which implies that P 2Pi has full 
row rank too. More precisely, Pi is an (m — rank(A^x~b)) 
x m matrix and 

432 
GAUSSIAN INFORMATION 
P2 an (m — rank(A^x 
') — rank(P iA^1 
s)) x (m — гапк(А^х 
')) matrix. 
It holds that 
rank{Aix-s) 
= 
rank{Aix~t)+rank{P 
ιΑ1ί~3). 
Thus, ranfc(P2) — ranA;(P2Pi). But this implies that P2P1 is a projection 
matrix for A to s. Therefore, P2P1 is also the projection of [h] to s, hence 
1 it Л Is 
_ 
Г 
= [h] is 
and transitivity holds. 
(A5) Combination: The results about projection matrices also help to verify the 
combination axiom. Let h\ = (Αι,ζι,Κι) and /г2 = (A2,Z2,K2) be two 
hints with Ai an mi x x matrix and A2 an m2 x у matrix. The vectors ζχ and 
z2 have dimension mi and т г respectively, whereas Ki and K2 are mi x mi 
and 77i2 x ?П2 matrices. Accordingly, we have d([/ii]) = x and d([/i2]) = У 
and assume x Ç. z Ç x(jy. Select a projection matrix P for A2 to у П z such 
that Р А ^ " г = 0 with rank rank(P) = m2 - rank(A^y~z). 
We then have 
[fcpn* = [(рА^Пг, Pz2, (PK2-1PT)-1)" . 
The combined linear system hi ф h^nz 
of the two hints hi and /i 2
y n 2 is 
О 
(ΡΑ2)ίχΓ,ν 
Ki 
О 
0 
(PKï1PT)-1 
_ 
On the other hand, we define the matrix 
0 
(PA 2)^-* 
Zl 
Pz 2 
P 
and it follows that 
= 
Im 
0 " 
0 
P 
Ρ ( Α ι θ Α 2 ) 
= 
P 
A^x~y 
А}ХПЪ 
0 
Af n î 
" д±х-у 
д1хпу 
0 
p » IxCiy 
1 
0 
\iz-x 
Ά2 
0 
Ρ Α ^ - χ 
0 
A±y-z 
Ä 2 
J 
0 
0 
We note that for the rank of P we have 
rank(P) 
= 
mi + ranfc(P) = mi + {πι2 — ranki^A^ ~2)) 
and also rank(hi Θ /i2
 n z) = ranfc(A2 ~z). Therefore, P is a projection 
matrix for hi θ h2 to z. Finally, this shows that 
[(Λι Θ h2)u] 
= [hi Θ h\ynz) 
= [/ίι] ® [h2] iynz 

GAUSSIAN DENSITIES 
4 3 3 
but it also holds by definition of combination and projection that 
[(Λι θ /ι 2)Ρ = [(Λι Θ M i z ] = ( M ® [/i2])iz = 
M ® N i v n * . 
This proves the combination axiom. 
J.2 GAUSSIAN DENSITIES 
Let x e R" be an n-vector, μ an n-vector and К a symmetric, positive definite 
nxn 
matrix, then the function 
|det(K)| 
J[ ' ~~ у (2π)" 
is the Gaussian density with parameters μ and K. The matrix К is called the con-
centration matrix of the density and the square root of the fraction in front of the 
exponential is the normalization factor, which ensures that the integral over the Gaus-
sian density equals 1. The vector μ is the expected value or mean of the density 
and Σ = K _ 1 is the variance-covariance matrix of a random n-vector X with the 
Gaussian density above. The matrix Σ is also symmetric and positive definite. 
Let Y = с + BX be an affine transformation of the random vector X having a 
Gaussian density with expected value μχ and variance-covariance matrix Σχ, where 
с is an m-vector and B a n m x « matrix. Then, Y still has a Gaussian distribution 
with expected value c + Βμ and variance-covariance matrix Β Σ Β Τ . In the particular 
case that m = n and the matrix В is regular with inverse T the Gaussian density of 
the random vector Y has the expected value 
μ γ 
= 
с + Β μ χ 
and concentration matrix 
K Y 
= 
( Β Σ Χ Β Γ ) _ 1 = T T K X T . 
Let s be a subset of {1,..., n}. The marginal of the Gaussian density with respect to 
s is still a Gaussian density with expected value μ^-3 and variance-covariance matrix 
£is,s j t s concentration matrix is 
((κ-1)ί'Α~1. 
if К = Σ ~l. If К is decomposed according to the sets s and ί = { 1 , . . . , n} — s, 
_ 
Г K±s's 
К4·8·* 
K 
- 
KiM к4-'·* 
the concentration matrix of the marginal can also be written as 

4 3 4 
GAUSSIAN INFORMATION 
The conditional distribution of a random vector X with Gaussian distribution is 
still a Gaussian distribution. Let again s be a subset of { 1 , . . . , n}, t = {1,..., n} — s 
and the concentration matrix К be decomposed according to s and t as above. Then, 
the conditional Gaussian density of the random vector X^s given X^' = x^' has 
expected value 
and concentration matrix 
If / denotes a Gaussian density over variables X g l " , f^b the marginal density 
with respect to the subset t С { 1 , . . . , n} and /x-u|x.u the conditional density of 
X i s given X·1-', then 
/(x) = 
/ x ^ x ^ X + V ) / ^ ) · 
More generally, if / is a Gaussian density over a set of variables s and g a conditional 
Gaussian density over variables in a set h given variables in a set t, such that t Ç s 
and h П s = 0, we claim that for x E l s , 
/(xMxJ-V) 
is a Gaussian density. To verify this, consider the generalized Gaussian potentials 
(i/i,Ki) and (ι^,Κ^) of / and g. Then, the product above is associated with the 
generalized Gaussian potential (v\, Ki) <8> (^, Κ2) = (г/, К) and in particular 
К 
-vîsUh 
K l 
К tsU/ι 
Now, assume the conditional Gaussian density g is associated with a system of linear 
equations like (10.53) having the concentration matrix K^ t. Then, the associated 
concentration matrix is given according to (10.55) by 
К 2 
— 
~ A M K M 
AZjKhitAh,t 
From this we deduce that 
К 
к м 
A£,tKM 
0 
-K h i fA M 
Kf ' + A£tKMAfc,t 
K l 
0 
■Lri-t S-t 
K l 
J 
This matrix is positive definite, since Ki and K^ t are both positive definite. Hence, 
the generalized potential (1/, K) associated with the product f(x.)g(x^h \x^1) is indeed 
an ordinary Gaussian potential and therefore this product is a Gaussian density. 

EXERCISES 
4 3 5 
PROBLEM SETS AND EXERCISES 
J.l * Develop the prediction for к + 1, к + 2,... in the Kaiman filter model, given 
observations up to time k, by continuing the collect algorithm beyond the node for 
time к in the join tree. 
J.2 * * Develop a general smoothing algorithm for nodes i = 1,..., к — 1 in the 
Kaiman filter model given observations up to time k. Take node г as the root of the 
join tree for the collect algorithm and show that the smoothing solution at time г 
is given as the combination of potentials for the filtering solution at time i and a 
backward filtering from time к backwards to i. 
J.3 * * Develop the backward filtering found in the preceding exercise in terms of 
generalized Gaussian potentials. 
J.4 * * 
Develop the backward filtering in terms of mean vectors and variance-
covariance matrices assuming that the potential фу. at time fc is ordinary Gaussian 
and that the matrices Afc are regular. Solve the smoothing problem in this case. 
J.5 * * Study the updating of the smoothing solution if a new measurement at time 
fc + 1 is added. 
J.6 * * Solve the filtering, prediction and smoothing problem for the discrete state 
hidden Markov chain model. 
J.7 * * * Solve the filtering, prediction and smoothing problem for a general semi-
ring valuation algebra. 
J.8 * * * In this chapter, we studied linear systems with Gaussian disturbances and 
showed that the corresponding operations of combination and variable elimination 
are mirrored by the operations for Gaussian potentials in Instance 1.6. Parts of this 
theory for linear systems with disturbances can be developed without the assumption 
that the disturbances are Gaussian (Kohlas & Monney, 2008). Abstract the theory of 
this chapter in such a way that other distributions can be assumed for the disturbances. 
Does this induce valuation algebras based on other classes of densities? This question 
is closely related to Exercise A.7 of Chapter 1. 

References 
Aho, A., Hopcroft, J.E., & Ullman, J. 1974. The Design and Analysis of Computer 
Algorithms. Addison-Wesley Publishing Company. 
Aji, S.M. 1999. Graphical Models and Iterative Decoding. Ph.D. thesis, California 
Institute of Technology. 
Aji, S.M., & McEliece, R.J. 2000. The Generalized Distributive Law. IEEE Trans, on 
Information Theory, 46(2), 325-343. 
Allen, D., & Darwiche, A. 2002. On the Optimality of the min-fill Heuristic. Tech. rept. 
University of California, Los Angeles. 
Allen, D., & Darwiche, A. 2003. Optimal Time-Space Tradeoff in Probabilistic Inference. 
Pages 969-975 of: IJCAl'03: Proc. of the 18th Int. Joint Conference on Artif. Intell. 
Morgan Kaufmann Publishers, Inc. 
Almond, R., & Kong, A. 1991. Optimality Issues in Constructing a Markov Tree from 
Graphical Models. Research Report A-3. Department of Statistics, Harvard University. 
Apt, K.R. 2003. Principles of Constraint Programming. Cambridge University Press. 
Arnborg, S. 1985. Efficient Algorithms for Combinatorial Problems with Bounded De-
composability - A Survey. BIT, 25(1), 2-23. 
Arnborg, S., Cornell, D., & Proskurowski, A. 1987. Complexity of Finding Embeddings 
in a k-Tree. SIAM J. of Algebraic and Discrete Methods, 8, 277-284. 
Bacchus, F., & Adam, G. 1995. Graphical Models for Preference and Utility. Pages 
3-10 of: UAI'95: Proc. of the 11 th Conference on Uncertainty in Artif. Intell. Morgan 
Kaufmann Publishers, Inc. 
Generic Inference: A Unifying Theory for Automated Reasoning 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc. 
437 

438 
REFERENCES 
Bacchus, F., Dalmao, S., & Pitassi, T. 2003. Value Elimination: Bayesian Interence 
via Backtracking Search. Pages 20-28 of: UAI'03: Proc. of the 19th Conference on 
Uncertainty in Artif. Intell. 
Backhouse, R.C., & Carré, B.A. 1975. Regular Algebra Applied to Path-Finding Prob-
lems. J. of the Institute for Mathematics and its Applications, 15, 161-186. 
Barwise, J., & Seligman, J. 1997. The Logic of Distributed Systems. Cambridge Univer-
sity Press. 
Beauchamp, K.G. 1984. Applications of Walsh and related Functions. Academic Press, 
Inc. 
Been, C, Fagin, R., Maier, D., & Yannakakis, M. 1983. On the Desirability of Acyclic 
Database Schemes. J. oftheACM, 30(3), 479-513. 
Bellman, R. 1958. On a Routing Problem. Quarterly of Applied Mathematics, 16(1), 
87-90. 
Bellman, R.E. 1957. Dynamic Programming. Princeton University Press. 
Bertele, U., & Brioschi, F. 1972. Nonserial Dynamic Programming. Academic Press, 
Inc. 
Bétréma, J. 1982. Topologies sur des Espaces Ordonnés. Informatique Théorique et 
Applications, 16(2),165-182. 
Biacino, L. 2007. Fuzzy Subsethood and Belief Functions of Fuzzy Events. Fuzzy Sets 
and Systems, 158(1), 38—49. 
Bistarelli, S., & Rossi, F. 2008. Semiring-Based Soft Constraints. Pages 155-173 of: 
Degano, P., Nicola, R. De, & Meseguer, J. (eds), Concurrency, Graphs and Models: 
Essays Dedicated to Ugo Montanari on the Occasion of His 65th Birthday. Springer-
Verlag. 
Bistarelli, S., Montanari, U., & Rossi, F. 1997. Semiring-Based Constraint Satisfaction 
and Optimization. J. oftheACM, 44(2), 201-236. 
Bistarelli, S., Montanari, U., Rossi, F., Verfaillie, G., & Fargier, H. 1999. Semiring-based 
CSPs and Valued CSPs: Frameworks, Properties and Comparison. Constraints, 4(3). 
Bistarelli, S., Friihwirth, T., Marte, M., & Rossi, F. 2002. Soft Constraint Propagation 
and Solving in Constraint Handling Rules. Pages 1-5 of: Proc. of the ACM Symposium 
on Applied Computing. 
Blau, H., Kyburg, J.R., & Henry, E. 1994. Ploxoma: Testbed for Uncertain Inference. 
Tech. rept. University of Rochester. 
Bodlaender, H. 1993. A Linear Time Algorithm for Finding Tree-Decompositions of 
Small Treewidth. Pages 226-234 of: STOC'93: Proc. of the 25th Annual ACM Sympo-
sium on Theory of Computing. ACM Press. 
Bodlaender, H. 1998. Treewidth: Algorithmic Techniques and Results. Pages 19-36 of: 
Mathematical Foundations of Computer Science. Springer-Verlag. 
Broder, A.Z., & Mayr, E.W. 1997. Counting Minimum Weight Spanning Trees. J. of 
Algorithms, 24(1), 171-176. 
Cano, A., & Moral, S. 1995. Heuristic Algorithms for the Triangulation of Graphs. Pages 
166-171 of: Bouchon-Meunier, R.R. Yager, & Zadeh, L.A. (eds), Proc. of the 5th IPMU 
Conference. Springer-Verlag. 

REFERENCES 
4 3 9 
Chang, CL., & Lee, R.C.T. 1973. Symbolic Logic and Mechanical Theorem Proving. 
Academic Press, Inc. 
Chaudhuri, S., & Zaroliagis, CD. 1997. Shortest Paths in Digraphs of Small Treewidth. 
Part I: Sequential Algorithms. Algorithmica, 27, 212-226. 
Chvâtal, V. 1983. Linear Programming. W.H. Freeman. 
Clifford, A.H., & Preston, G.B. 1967. Algebraic Theory of Semigroups. American 
Mathematical Society. 
Conway, J.H. 1971. Regular Algebra and Finite Machines. Chapman and Hall Mathe-
matics Series. Chapman and Hall. 
Cooley, J.W., & Tukey, J.W. 1965. An Algorithm for the Machine Calculation of Complex 
Fourier Series. Mathematics of Computation, 19(90), 297-301. 
Cowell, R.G., Dawid, A.P., Lauritzen, S.L., & Spiegelhalter, D.J. 1999. Probabilistic 
Networks and Expert Systems. Information Sei. and Stats. Springer-Verlag. 
Croisot, R. 1953. Demi-Groupes Inversifs et Demi-Groupes Réunions de Demi-Groupes 
Simples. Ann. Sei. Ecole norm. Sup., 79(3), 361-379. 
Darwiche, A. 2001. Decomposable Negation Normal Form. J. of the ACM, 48(4), 
608-647. 
Darwiche, A., & Marquis, P. 2001. A Perspective on Knowledge Compilation. Pages 
175-182 of: IJCAI'Ol: Proc. of the 17th Int. Joint Conference on Artif. Intell. San 
Francisco, CA, USA: Morgan Kaufmann Publishers, Inc. 
Darwiche, A., & Marquis, P. 2002. A Knowledge Compilation Map. J. of Artif. Intell. 
Research, 17, 229-264. 
Davey, B.A., & Priestley, H.A. 1990. Introduction to Lattices and Order. Cambridge 
University Press. 
de Kleer, J., Brown, J., & Seely, J. 1986. Theories of Causal Ordering. Artif. Intell., 
29(1),33-61. 
Dechter, R. 1999. Bucket Elimination: A Unifying Framework for Reasoning. Artif. 
Intell., 113, 41-85. 
Dechter, R. 2003. Constraint Processing. Morgan Kaufmann Publishers, Inc. 
Dechter, R. 2006. Tractable Structures for Constraint Satisfaction Problems. In: Rossi, 
F., van Beek, P., & Walsh, T. (eds), Handbook of Constraint Programming. Elsevier. 
Dechter, R., & Mateescu, R. 2007. AND/OR Search Spaces for Graphical Models. Artif. 
Intell, 171(2-3), 73-106. 
Dechter, R., & Pearl, J. 1987. Network-Based Heuristics for Constraint-Satisfaction 
Problems. Artif. Intell, 34(1), 1-38. 
Dechter, R., & Pearl, J. 1989. Tree Clustering for Constraint Networks. Artif. Intell, 
38(3), 353-366. 
Dechter, R., & Rish, R. 2003. Mini-Buckets: A General Scheme for Bounded Inference. 
J. of the ACM, 50(2), 107-153. 
Dempster, A.P. 1968. A Generalization of Bayesian Inference. J. Royal Stat. Soc. B, 30, 
205-247. 

440 
REFERENCES 
Dempster, A.P. 1990a. Construction and Local Computation Aspects of Network Belief 
Functions. In: Smith, J.Q., & Oliver, R.M. (eds), Influence Diagrams, Belief Nets and 
Decision Analysis. John Wiley & Sons, Inc. 
Dempster, A.P. 1990b. Normal Belief Functions and the Kaiman Filter. Tech. rept. 
Harvard University, USA. 
Dijkstra, E.W. 1959. A Note on Two Problems in Connexion with Graphs. Numerische 
Mathematik, 1,269-271. 
Downey, R., & Fellows, M. 1999. Parameterized Complexity. Springer-Verlag. 
Eichenberger, Ch. 2009. Algebras of Gaussian Linear Information. Ph.D. thesis, Dept. 
of Computer Science, University of Fribourg. 
Fargier, H., & Marquis, P. 2007. On Valued Negation Normal Form Formulas. Pages 
360-365 of: IJCAI'07: Proc. of the 20th Int. Joint Conference on Artif lntell. 
Fargier, H., & Vilarem, M. 2004. Compiling CSPs into Tree-Driven Automata for 
Interactive Solving. Constraints, 9(4), 263-287. 
Fishburn, P.C. 1974. Lexicographic Orders, Utilities and Decision Rules: A Survey. 
Management Science, 20, 1442-1471. 
Fisher, R.A. 1935. The Fiducial Argument in Statistical Inference. Ann. Eugen., 9, 
391-398. 
Fisher, R.A. 1950. Contributions to Mathematical Statistics. John Wiley & Sons, Inc. 
Floyd, R.W. 1962. Algorithm 97: Shortest Path. Comm. ACM, 5, 345. 
Ford, L.R. 1956. Network Flow Theory. Tech. rept. The RAND Corperation, Santa 
Moncia, California. 
Forsythe, G.,&Moler, C.B. 1967. Computer Solution ofLinear Algebra Systems. Prentice 
Hall. 
Gallager, R.C. 1963. Low Density Parity Check Codes. MIT Press. 
Garey, M.R., & Johnson, D.S. 1990. Computers and Intractability; A Guide to the Theory 
of NP-Completeness. W.H. Freeman. 
Golan, J. S. 1999. Semirings and their Applications. Kluwer, Dordrecht. 
Golan, J.S. 2003. Semirings and Affine Equations over them: Theory and Applications. 
Kluwer, Dordrecht. 
Gondran, M., & Minoux, M. 2008. Graphs, Dioids and Semirings: New Models and 
Algorithms. Operations Research Computer Science Interfaces Series. Springer-Verlag. 
Gonzalez, R.C., & Woods, R.E. 2006. Digital Image Processing. 3 edn. Prentice Hall. 
Gordon, J., & Shortliffe, E.H. 1985. A Method for Managing Evidential Reasoning in a 
Hierarchical Hypothesis Space. Artif. Inteli, 26(3), 323-357. 
Grabisch, M. 2009. Belief Functions on Lattices. Int. J. lntell. Syst., 24(1), 76-95. 
Grätzer, G. 1978. General Lattice Theory. Pure and Applied Mathematics, vol. 75. 
Academic Press, Inc. 
Green, J.A. 1951. On the Structure of Semigroups. Ann. of Math., 54, 163-172. 
Haenni, R. 2004. Ordered Valuation Algebras: A Generic Framework for Approximating 
Inference. Int. J.Approx. Reasoning, 37(1), 1-41. 

REFERENCES 
441 
Haenni, R., & Lehmann, N. 1999. Efficient Hypertree Construction. Tech. rept. 99-3. 
Department of Informatics, University of Fribourg. 
Haenni, R., Kohlas, J., & Lehmann, N. 2000. Probabilistic Argumentation Systems. 
Pages 221-287 of: Kohlas, J., & Moral, S. (eds), Handbook of Defeasible Reasoning and 
Uncertainty Management Systems, Volume 5: Algorithms for Uncertainty and Defeasible 
Reasoning. Kluwer, Dordrecht. 
Harville, D.A. 1997. Matrix Algebra from a Statistician's Perspective. Springer-Verlag. 
Hewitt, E., & Zuckerman, H.S. 1956. The ίχ-Algebra of a Commutative Semigroup. 
Amer. Math. Soc, 83, 70-97. 
Hopkins, M., & Darwiche, A. 2002. A Practical Relaxation of Constant-Factor Treewidth 
Approximation Algorithms. In: Proc. of the 1st European Workshop on Probabilistic 
Graphical Models. 
Inoue, K. 1992. Linear Resolution for Consequence Finding. Artif. Intell., 56(2-3), 
301-353. 
Jensen, F.V. 1988. Junction Trees and Decomposable Hypergraphs. Tech. rept. Aalborg 
University, Denmark. 
Jensen, F.V., Lauritzen, S.L., & Olesen, K.G. 1990. Bayesian Updating in Causal Proba-
bilistic Networks by Local Computation. Computational Statistics Quarterly, 4,269-282. 
Jonczy, J. 2009. Generic Frameworks for the Analysis of Dependable Systems: Alge-
braic Path Problems, Reliability, and Diagnostics. Ph.D. thesis, University of Berne, 
Switzerland. 
Kaiman, R.E. 1960. A new Approach to Linear Filtering and Predictive Problems. Trans. 
of the ASME - J. of Basic Engineering, 82, 34—45. 
Kanal, L., & Kumar, V. (eds). 1988. Search in Artificial Intelligence. Springer-Verlag. 
Kask, K., Dechter, R., Larrosa, J., & Fabio, G. 2001. Bucket-Tree Elimination for 
Automated Reasoning. Artif. Intell., 125, 91-131. 
Kemeny, J.G., Mirkil, H., Snell, J.L., & Thompson, G.L. 1960. Finite Mathematical 
Structures. Prentice Hall. 
Kleene, S. 1956. Representation of Events in Nerve Nets and Finite Automata. Princeton 
University Press. 
Kohlas, J. 1991. Describing Uncertainty in Dynamical Systems by Uncertain Restrictions. 
Pages 210-223 of: Masi, G.B. Di, Gombani, A., & Kurzhansky, A.B. (eds), Modeling, 
Estimation and Control of Systems with Uncertainty. Birkhäuser, Boston. 
Kohlas, J. 2003. Information Algebras: Generic Structures for Inference. Springer-
Verlag. 
Kohlas, J. 2004. Valuation Algebras Induced by Semirings. Tech. rept. 04-03. Depart-
ment of Informatics, University of Fribourg. 
Kohlas, J., & Monney, P.-A. 1995. A Mathematical Theory of Hints. An Approach to 
the Dempster-Shafer Theory of Evidence. Lecture Notes in Economics and Mathematical 
Systems, vol. 425. Springer-Verlag. 
Kohlas, J., & Monney, P.-A. 2008. Statistical Information. Assumption-Based Statistical 
Inference. Sigma Series in Stochastics, vol. 3. Heldermann. 

442 
REFERENCES 
Kohlas, J., & Wilson, N. 2006. Exact and Approximate Local Computation in Semiring 
Induced Valuation Algebras. Tech. rept. 06-06. Department of Informatics, University 
of Fribourg. 
Kohlas, J., & Wilson, N. 2008. Semiring induced Valuation Algebras: Exact and Ap-
proximate Local Computation Algorithms. Artif. Intell, 172(11), 1360-1399. 
Kohlas, J., Haenni, R., & Moral, S. 1999. Prepositional Information Systems. J. of Logic 
and Computation, 9(5), 651-681. 
Kong, A. 1986. Multivariate Belief Functions and Graphical Models. Ph.D. thesis, 
Department of Statistics, Harvard University. 
Kozen, D. 1990. On Kleene Algebras and Closed Semirings. Pages 26—47 of: Lecture 
Notes in Computer Science. Springer-Verlag. 
Kozen, D. 1994. A Completeness Theorem for Kleene Algebras and the Algebra of 
regular Events. Information and Computing, 110(2), 366-390. 
Kozlov, A.V., & Singh, J.P. 1994. A Parallel Lauritzen-Spiegelhalter Algorithm for 
Probabilistic Inference. Pages 320-329 of: Supercomputing '94: Proc. of the 1994 
ACM/IEEE Conference on Supercomputing. ACM Press. 
Kozlov, A.V., & Singh, J.P. 1996. Parallel Implementations of Probabilistic Inference. 
Computer, 29(12), 33^0. 
Kschischang, F.R., Frey, B.J., & Loeliger, H.A. 2001. Factor Graphs and the Sum-Product 
Algorithm. IEEE Trans, on Information Theory, 47(2), 498-519. 
Langel, J. 2010. Logic and Information: A Unifying Approach to Semantic Information 
Theory. Ph.D. thesis, Department of Informatics, University of Fribourg. 
Larrosa, J., & Dechter, R. 2003. Boosting Search with Variable Elimination in Constraint 
Optimization and Constraint Satisfaction Problems. Constraints, 8(3), 303-326. 
Larrosa, J., & Morancho, E. 2003. Solving 'Still Life' with Soft Constraints and Bucket 
Elimination. Pages 466-479 of: CP'03: Proc. of the 9th Int. Conference on Principles 
and Practice of Constraint Programming. 
Lauritzen, S.L., & Jensen, F.V. 1997. Local Computation with Valuations from a Com-
mutative Semigroup. Ann. Math. Artif. Intell, 21(1), 51-69. 
Lauritzen, S.L., & Spiegelhalter, D.J. 1988. Local Computations with Probabilities on 
Graphical Structures and their Application to Expert Systems. J. Royal Stat. Soc. B, 50, 
157-224. 
Lehmann, D.J. 1976. Algebraic Structures for Transitive Closure. Tech. rept. Department 
of Computer Science, University of Warwick. 
Lehmann, N. 2001. Argumentation System and Belief Functions. Ph.D. thesis, Depart-
ment of Informatics, University of Fribourg. 
Lepar, V., & Shenoy, P.P. 1998. A Comparison of Lauritzen-Spiegelhalter, Hugin 
and Shenoy-Shafer Architectures for Computing Marginals of Probability Distributions. 
Pages 328-337 of: Cooper, G., & Moral, S. (eds), UAI'98: Proc. of the 14th Conference 
on Uncertainty in Artif. Intell. Morgan Kaufmann Publishers, Inc. 
MacKay, D.J.C. 2003. Information Theory, Inference, and Learning Algorithms. Cam-
bridge University Press. 
Maier, D. 1983. The Theory of Relational Databases. Pitman. 

REFERENCES 
4 4 3 
Marinescu, R., & Dechter, R. 2009a. AND/OR Branch-and-Bound Search for Combina-
torial Optimization in Graphical Models. Artif. Inteli., 173(16-17), 1457-1491. 
Marinescu, R., & Dechter, R. 2009b. Memory Intensive AND/OR Search for Combina-
torial Optimization in Graphical Models. Artif. Inteli, 173(16-17), 1492-1524. 
Menger, K. 1942. Statistical Metrics. Proc. of the National Academy of Sciences of the 
United States of America, 28, 535-537. 
Meyn, S.P., & Tweedie, R.L. 1993. Markov Chains and Stochastic Stability. Springer-
Verlag. 
Mitten, L.G. 1964. Composition Principles for the Synthesis of Optimal Multi-Stage 
Processes. Operations Research, 12. 
Mohri, M. 2002. Semiring Frameworks and Algorithms for shortest-distance Problems. 
J. Autom. Lang. Comb., 7(3), 321-350. 
Monney, P.-A. 2003. A Mathematical Theory of Arguments for Statistical Evidence. 
Contributions to Statistics. Physica-Verlag. 
Namasivayam, V.N., & Prasanna, V.K. 2006. Scalable Parallel Implementation of Exact 
Inference in Bayesian Networks. Pages 143-150 of: ICPADS '06: Proc. of the 12th Int. 
Conference on Parallel and Distributed Systems, vol. 1. Washington, DC, USA: IEEE 
Computer Society. 
Nicholson, R., Bridge, D., & Wilson, N. 2006. Decision Diagrams: Fast and Flexible 
Support for Case Retrieval and Recommendation. Pages 136-150 of: Proc. of the 8th 
European Conference on Case-Based Reasoning. LNAI 4106. Springer-Verlag. 
Nilsson, N.J. 1982. Principles of Artificial Intelligence. Springer-Verlag. 
Norman, N.M., & Pollard, S. 1996. Closure Spaces and Logic. Springer-Verlag. 
Norris, J.R. 1998. Markov Chains (Cambridge Series in Statistical and Probabilistic 
Mathematics). Cambridge University Press. 
Paskin, K., Guestrin, C, & McFadden, J. 2005. A Robust Architecture for Distributed 
Inference in Sensor Networks. In: IPSN'05: Proc. of the 4th Int. Symposium on 
Information Processing in Sensor Networks. 
Pearl, J. 1984. Heuristics: Intelligent Search Strategies for Computer Problem Solving. 
Addison-Wesley Publishing Company. 
Pearl, J. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible 
Inference. Morgan Kaufmann Publishers, Inc. 
Pichon, F, & Denoeux, T. 2008. T-Norm and Uninorm-Based Combination of Belief 
Functions. In: NAFIPS'08: Annual Meeting of the North American Fuzzy Information 
Processing Society. 
Pouly, M. 2008. A Generic Framework for Local Computation. Ph.D. thesis, Department 
of Informatics, University of Fribourg. 
Pouly, M. 2010. NENOK - A Software Architecture for Generic Inference. Int. J. on 
Artif. Intel. Tools, 19. 
Pouly, M., & Kohlas, J. 2005. Minimizing Communication Costs of Distributed Local 
Computation. Tech. rept. 05-20. Department of Informatics, University of Fribourg. 
Pralet, C, Verfaillie, G., & Schiex, T. 2007. An Algebraic Graphical Model for Decision 
with Uncertainties, Feasibilities and Utilities. J. of Artif. Inteli. Research, 29, 421^189. 

444 
REFERENCES 
Radhakrishnan, V., Hunt, H.B., & Steams, R.E. 1992. Efficient Algorithms for Solving 
Systems of Linear Equations and Path Problems. Pages 109-119 of: STACS'92: Proc. of 
the 9th Annual Symposium on Theoretical Aspects of Computer Science. Springer-Verlag. 
Robertson, N., & Seymour, P.D. 1983. Graph Minors I: Excluding a Forest. J. Comb. 
Theory, Ser. B, 35(1), 39-61. 
Robertson, N., & Seymour, P.D. 1984. Graph Minors III: Planar Tree-Width. J. Comb. 
Theory, Ser. B, 36(1), 49-64. 
Robertson, N., & Seymour, P.D. 1986. Graph Minors II: Algorithmic Aspects of Tree-
Width. J. of Algorithms, 7(3), 309-322. 
Rose, D. 1972. A Graph-Theoretic Study of the Numerical Solution of Sparse Positive 
Definite Systems of Linear Equations. In: Read, R. (ed), Graph Theory and Computing. 
Academic Press, Inc. 
Rose, D.J. 1970. Triangulated Graphs and the Elimination Process. J. of Math. Analysis 
and Applications, 32, 597-609. 
Rossi, F., van Beek, P., & Walsh, T. 2006. Handbook of Constraint Programming. 
Foundations of Artif. Intell. Elsevier Science, Inc. 
Rote, G. 1990. Path Problems in Graphs. Computing Suppl, 7, 155-198. 
Roweis, S.T., & Ghahramani, Z. 1999. A Unifying Review of Linear Gaussian Models. 
Neural Computation, 11(2), 305-345. 
Roy, B. 1959. Transitivité et connexité. С. R. Acad. Sei. Paris, 249, 216-218. 
Schiex, Th. 1992. Possibilistic Constraint Satisfaction Problems or "How to handle Soft 
Constraints?". Pages 268-275 of: UAI'92: Proc. of the 8th Conference on Uncertainty 
in Artif. Intell. Morgan Kaufmann Publishers, Inc. 
Schmidt, T., & Shenoy, P. 1998. Some Improvements to the Shenoy-Shafer and Hugin 
Architectures for Computing Marginals. Artif. Intell., 102, 323-333. 
Schneuwly, С 2007. Computing in Valuation Algebras. Ph.D. thesis, Department of 
Informatics, University of Fribourg. 
Schneuwly, C, Pouly, M., & Kohlas, J. 2004. Local Computation in Covering Join Trees. 
Tech. rept. 04-16. Department of Informatics, University of Fribourg. 
Schwarz, H.R. 1997. Numerische Mathematik. 4edn. Teubner. 
Schweizer, В., & Sklar, A. 1960. Statistical Metric Spaces. Pacific J. Math., 10, 313-334. 
Shafer, G. 1976. A Mathematical Theory of Evidence. Princeton University Press. 
Shafer, G. 1982. Belief Functions and Parametric Models. J. Royal Stat. Soc. B, 44(3), 
322-352. 
Shafer, G. 1991. An Axiomatic Study of Computation in Hypertrees. Working Paper 232. 
School of Business, University of Kansas. 
Shafer, G. 1996. Probabilistic Expert Systems. Society for Industrial and Applied 
Mathematics. 
Shafer, G., & Shenoy, P. 1988. Local Computation in Hypertrees. Tech. rept. School of 
Business, University of Kansas. 
Shafer, G., Shenoy, P.P., & Mellouli, K. 1987. Propagating Belief Functions in qualitative 
Markov trees. Int. J. Approx. Reasoning, 1(4), 349-400. 

REFERENCES 
4 4 5 
Shenoy, P.P. 1992a. Using Possibility Theory in Expert Systems. Fuzzy Sets and Systems, 
51(2), 129-142. 
Shenoy, P.P. 1992b. Valuation-Based Systems: A Framework for Managing Uncertainty 
in Expert Systems. Pages 83-104 of: Zadeh, L.A., & Kacprzyk, J. (eds), Fuzzy Logic for 
the Management of Uncertainty. John Wiley & Sons, Inc. 
Shenoy, P.P. 1996. Axioms for Dynamic Programming. Pages 259-275 of: Gammerman, 
A. (ed), Computational Learning and Probabilistic Reasoning. John Wiley & Sons, Inc. 
Shenoy, P.P. 1997. Binary Join Trees for Computing Marginals in the Shenoy-Shafer 
Architecture. Int. J. Approx. Reasoning, 17, 239-263. 
Shenoy, P.P., & Shafer, G. 1990. Axioms for Probability and Belief-Function Propagation. 
Pages 575-610 of: Shafer, G., & Pearl, J. (eds), Readings in Uncertain Reasoning. 
Morgan Kaufmann Publishers, Inc. 
Smets, Ph. 2000. Belief Functions and the Transferable Belief Model. 
Smets, Ph., & Kennes, R. 1994. The Transferable Belief Model. Artif. Intell, 66(2), 
191-234. 
Smith, S.W. 1999. The Scientist and Engineer's Guide to Digital Signal Processing. 2 
edn. California Technical Publishing. 
Spohn, W. 1988. Ordinal Conditional Functions: A Dynamic Theory of Epistemic States. 
Pages 105-134 of: Harper, W.L., & Skyrms, B. (eds), Causation in Decision, Belief 
Change, and Statistics, vol. 2. Dordrecht, Netherlands. 
Spohn, W. 1990. A General Non-Probabilistic Theory of Inductive Reasoning. Pages 
149-158 of: UAl'88: Proc. of the 4th Conference on Uncertainty in Artif. Intell. 
Amsterdam, The Netherlands: North-Holland Publishing Co. 
Tamura, T., & Kimura, N. 1954. On Decompositions of a Commutative Semigroup. 
KodaiMath. Sem. Rep., 109-112. 
Tarjan, R.E., & Yannakakis, M. 1984. Simple Linear-Time Algorithms to Test Chordality 
of Graphs, Test Acyclicity of Hypergraphs, and Selectively Reduce Acyclic Hypergraphs. 
SIAMJ. Comput., 13(3), 566-579. 
Ullman, J.D. 1988. Principles of Database and Knowledge-Base Systems Volume I. 
Computer Science Press. 
van Leeuwen, J. 1990. Graph Algorithms. Pages 525-631 of: van Leeuwen, J. (ed), 
Handbook of Theoretical Computer Science: Algorithms and Complexity, vol. A. MIT 
Press. 
Vempaty, N.R. 1992. Solving Constraint Satisfaction Problems using Finite State Au-
tomata. Pages 453-458 of: AAAI'92: Proc. of the 10th National Conference on Artif. 
Intell. AAAI Press. 
Wächter, M. 2008. Knowledge Compilation Map - Theory and Application. Ph.D. thesis, 
Philosophisch-Naturwissenschaftlichen Fakultät, University of Berne. 
Wächter, M., & Haenni, R. 2006. Propositional DAGs: A New Graph-Based Language 
for Representing Boolean Functions. Pages 277-285 of: Doherty, P., Mylopoulos, J., & 
Welty, C. (eds), KR'06: 10th Int. Conference on Principles of Knowledge Representation 
and Reasoning. AAAI Press. 

446 
REFERENCES 
Wächter, M., Haenni, R., & Pouly, M. 2007. Optimizing Inference in Bayesian Networks 
and Semiring Valuation Algebras. Pages 236-247 of : Gelbukh, A., & Kuri Morales, A. F. 
(eds), М1САГ07: 6th Mexican Int. Conference on Artif Intell. LNAI 4827. 
Warshall, S. 1962. A Theorem on Boolean Matrices. J. oftheACM, 9(1), 11-12. 
Wiberg, N. 1996. Codes and Decoding on General Graphs. Ph.D. thesis, Linkoping 
University, Sweden. 
Wilson, N. 2005. Decision Diagrams for the Computation of Semiring Valuations. Pages 
331-336 of: IJCAI'05: Proc. of the 19th Int. Joint Conference on Artif. Intell. Morgan 
Kaufmann Publishers, Inc. 
Wilson, N., & Mengin, J. 2001. Embedding Logics in the Local Computation Framework. 
J. of Applied Non-Classical Logics, 11(3-4), 239-261. 
Winston, W.L., & Venkataramanan, M. 2002. Introduction to Mathematical Program-
ming: Applications and Algorithms. Vol. 1. Duxbury Press. 
Wojcicki, R. 1988. Theory of Logical Calculi: Basic Theory of Consequence Operations. 
Kluwer, Dordrecht. 
Yannakakis, M. 1981. Computing the Minimum Fill-in is NP-Complete. SIAM J. of 
Algebraic and Discrete Methods, 2( 1 ), 77-79. 
Yinglong, X., & Prasanna, V.K. 2008. Junction Tree Decomposition for Parallel Exact 
Inference. Pages 1-12 of: IPDPS'08: IEEE Int. Symposium on Parallel and Distributed 
Processing. 
Zadeh, L.A. 1978. Fuzzy Sets as a Basis for a Theory of Possibility. Fuzzy Sets and 
Systems, 1, 3-28. 
Zadeh, L.A. 1979. A Theory of Approximate Reasoning. Pages 149-194 of: Ayes, 
J.E., Michie, D., & Mikulich, L.I. (eds), Machine Intelligence, vol. 9. Ellis Horwood, 
Chichester, UK. 

INDEX 
adaptive consistency, see bucket elimination 
addition-is-max semiring, 305 
adjacency matrix, 225 
affine space, 276 
algebraic information theory, 123 
algebraic path problem, 219, 231 
all-pairs problem, 142, 236, 374 
multiple-pairs problem, 371, 374 
simple path, 263 
single-pair problem, 367, 374 
single-source problem, 237, 374 
single-target problem, 237, 374 
AND-OR search, xxxi 
arithmetic potential, 10, 192 
assumption-based constraint, 195 
assumption-based reasoning 
in constraint systems, 195 
in Gaussian systems, 388, 400 
attribute, see variable 
backward substitution, 294, 324, 329, 337 
basic probability assignment, see mass func-
tion 
Bayesian decoding, 310 
Bayesian network, 33, 423 
construction order, 424 
Generic Inference: A Unifying Theory for Auto 
First Edition. By Marc Pouly and Jiirg Kohlas 
Copyright © 2011 John Wiley & Sons, Inc. 
posterior probability, 34 
query and evidence variable, 34 
query event, 34 
belief function, 12, 143 
Bellman-Ford algorithm, 377 
Bellmann's principle of optimality, 236 
Boolean function, see indicator function 
bucket elimination, 50, 58, 294 
bucket-tree, 59 
bucket-tree elimination, 113 
Choleski decomposition, 348 
chordal graph, see triangulated graph 
clique, 30 
maximal, 30 
closed set, 283 
closure operator, 246, 251, 271, 282 
cluster tree elimination, see Shenoy-Shafer ar-
chitecture 
collect algorithm, 87, 93 
combination, 4 
combination axiom, 5, 21, 25, 69 
commonality function, 171 
communication channel, 309 
computational graph, 259 
concentration matrix, 433 
Reasoning 
447 

4 4 8 
INDEX 
configuration, see tuple 
configuration extension set, 295 
for optimization problem, 312 
for quasi-regular valuation algebra, 368 
for symmetric, positive definite system, 
344 
congruence relation, 105, 154 
connectivity problem, 220 
consequence finding, 290 
consequence operator, 282 
constraint formalism, 304 
constraint problem, 307 
constraint semiring, see c-semiring 
context, 268, 281 
context valuation algebra, 289, 318 
control of fill-ins, 329, 343 
convex polyhedra, 284 
convolutional code, 311 
counter-model, 312 
covering join tree, 75, 78 
for factorization, 78 
for inference problem, 79 
free variable, 79 
valuation assignment, 79 
crisp constraint, see indicator function 
Dempster rule, 173 
Dempster-Shafer theory of evidence, 37, 173 
density function, 13, 164 
conditional, 26 
support, 27, 164 
Dijkstra algorithm, 377 
dioid, 186, 228, 304 
topological, 228 
disbelief function, 202 
discrete cosine transform, 44, 45 
discrete Fourier transform, 42, 98 
amplitude, 42 
time and frequency domain, 42 
distance tree, 377 
distributive law, xxvi, 11 
domain, 4 
domain axiom, 5, 21, 25, 27 
domain-free valuation algebra, 105 
support, 106 
dual graph, 83 
dynamic programming, 50, 294 
eliminator, 82 
equivalence relation, 105, 154 
existential quantifier, 266 
factor graph, see valuation network 
factorization vs. decomposition, 322 
family of compatible frames, 23 
fast Fourier transform, 99 
fast Hadamard transform, 104 
fiducial distribution, 390 
fill-in, 321, 324 
focal set, 12 
focal space, 400 
focusing, see projection 
frame 
variable, 6 
variable set, 7 
frame of discernment, 22 
fusion algorithm, 50, 51 
graphical representation, 52 
fuzzy set, see probabilistic constraint 
Gallager-Tanner-Wiberg algorithm, 311 
Galois connection, 283 
Galois field, 189, 280 
Gauss-Jordan method, 365 
Gaussian Bayesian network, 418, 423 
construction sequence, 425 
observation, 426 
Gaussian density, 14, 15, 433 
concentration matrix, 15 
conditional, 424, 434 
head and tail, 424 
expected value, 433 
mean vector, 15 
variance-covariance matrix, 15, 433 
Gaussian diagnostic estimation, 390 
Gaussian distribution, see Gaussian density 
Gaussian dynamic system, 387, 395 
filtering, smoothing and projection prob-
lem, 400 
Gaussian hint, 400 
precise, see Gaussian hint 
Gaussian information, 385, 403 
Gaussian potential, 15, 45, 385 
Gaussian predictive estimation, 391 
Gaussian variable elimination, 293, 323 
pivoting, 323 
generalized collect algorithm, 93 
generalized Gaussian potential, 402 
generalized relational algebra, 287 
generic, xxiii 
generic construction, xxviii, 182, 255 
generic inference, xxvi 
graph 
bipartite, 263 
connected, 30 
cut node, 263 
cycle, 30 
degree, 30 
directed, 30 

INDEX 
449 
directed, weighted, 223 
edge and node, 30 
labeled, 31 
labeling function, 31 
neighboring node, 30 
ordered, see ordered graph 
path, 30 
length, 30 
source and terminal node, 30 
triangulated, 83, 354 
undirected, 30 
greatest lower bound, see infimum 
Green relation, 164 
group, 155 
Hadamard transform, 41, 104 
Hamacher product, 218 
hidden Markov chain, 45, 418 
filtering, prediction, smoothing, 46, 173 
hint, see Gaussian hint 
HUGIN architecture, 128 
hybrid inference methods, xxxi 
hypergraph, 31 
hyperedge, 31 
hypothesis, 272 
idempotent architecture, 132, 319 
implicate, 274 
prime, 274 
indicator function, 7, 192 
inference 
exact and approximated, xxxi 
inference problem, 33 
induced width, 63 
multi-query, 33, 109 
separator width, 97 
single-query, 33, 48 
treewidth, see treewidth 
infimum, 18 
infomorphism, 271, 288 
information algebra, 123, 265 
information set, 283, 288 
theory, 283 
Jacobi matrix, 261 
join graph, 86 
join tree, 56, 79 
binary, 116 
directed, 81 
parent, child and leaf, 82 
node domain, 93 
node factor, 80, 92 
join tree factorization, 80, 93 
join tree property, see running intersection 
property 
joint probability distribution, 33 
joint valuation, see objective function 
junction tree, see join tree 
Kaiman filter, 418 
filter problem, 419 
gain matrix, 423 
one-step forward prediction, 420 
Kleene algebra, 246, 246 
Kleene valuation algebra, 251, 255 
knowledge compilation, xxxi 
knowledgebase, xxv, 32 
inconsistent, 74 
labeled matrix, 14, 237, 251 
direct sum, 251 
domain, 14 
extension, 14 
identity, 14 
projection, 14, 252 
zero element, 14 
labeling, 4 
labeling axiom, 4, 20, 25 
language of an automaton, 223 
lattice, 18 
bottom and top element, 18 
bounded, 18, 189 
complete, 18, 286 
conditional independence, 91, 176 
distributive, 18, 189 
independence, 176 
Lauritzen-Spiegelhalter architecture, 124 
LDR-decomposition, 336 
least probable configuration, 309 
least squares method, 322, 338, 357, 386 
least squares estimate, 358 
normal equation, 357, 358 
least upper bound, see supremum 
Lindenbaum algebra, 271 
linear codes, 310 
linear dynamic system, 418 
filter solution, 362 
one-step predictor, 363 
smoothing solution, 362 
with Gaussian disturbances, see Gaus-
sian dynamic system 
linear equation system, 275 
equivalence, 278 
minimal system, 278 
solution space, 275 
linear functional model, 386 
linear inequality, 284 
linear mapping, 275 
null space, 276 

450 
INDEX 
range, 275 
rank, 275 
linear programming, 284 
linear space, 7, 275 
dimension, 275 
local computation, xxiv, xxvii, 48 
LV-decomposition, 336 
mailbox, 97, 111 
marginal problem, 175 
marginal set, 24 
marginalization, see projection 
Markov chain, 257 
transient and recurrent state, 258 
transition matrix, 258 
Markov property, see running intersection prop-
erty 
Markov state evolution model, 418 
Markov tree, see join tree 
mass function, 36, 172 
matrix compilation, 336 
matrix power sequence, 226 
maximum capacity problem, 221 
maximum likelihood decoding, 310 
maximum reliability problem, 222 
maximum satisfiability problem, 308 
monoid, 186 
most probable configuration, 45, 308 
multigraph, 225 
natural join, 9, 270, 299 
network compilation, 263 
neutrality axiom, 69, 123 
nil-potent maximum t-norm, 218 
nullity axiom, 73 
numeric partial differentiation, 259 
objective function, 33 
optimization problem, 307 
configuration extension, 312 
solution configuration, 309 
OR search with caching, xxxi 
order of information, 174 
order relation, xxiii 
ordered graph, 63 
induced graph, 63 
induced width, 63 
node width, 63 
width, 63 
parameterized complexity, xxvii, 65 
parity check matrix, 310 
partial k-tree, see treewidth 
partial order, 17 
partition, 19 
block, 19 
partition lattice, 19 
decomposition mapping, 22 
path counting problem, 257 
possibilistic constraint, 193 
possibility measure, 202 
possibility potential, 26, 193, 202 
powerset, 4, 19 
predecessor matrix, 378 
predicate logic, 284 
rank of predicate, 284 
preorder, 17, 186 
primal graph, 32, 83 
principal ideal, 165, 175 
probabilistic constraint, 194 
probabilistic semiring, 185 
probability potential, 10, 33 
projection, 4 
projection axiom, 4, 20, 25 
projection problem, see inference problem 
proposition 
valuation, 267 
propositional formula, 266 
atomic, 266 
equivalence, 268, 269 
interpretation, 269 
model, 268 
satisfiable, 269 
valuation, 267 
propositional information, 270 
propositional logic, 266 
clause, 273 
conjunctive normal form, 272 
context, 268 
literal, 273 
normal forms, 272 
resolution, 273 
résolvant, 273 
subsumption, 273 
propositional theory, 270 
pseudo-concentration matrix, 402 
quasi-inverse, 229 
quasi-regular valuation algebra, 237, 244 
query, xxv, 33 
quotient algebra, 105 
refinement mapping, 22 
regular algebra, 371 
relation, 9 
relational algebra, 9, 35, 192, 287, 293 
selection operator, 35 
relational representation, 8 
resolution algorithm, 273 

INDEX 
451 
ring, 189 
of polynomials, 189 
running intersection property, 56, 91 
satisfiability 
in constraint system, 308 
in propositional logic, 269, 272 
satisfiability problem, see satisfiability 
scaled HUGIN architecture, 151 
scaled Lauritzen-Spiegelhalter architecture, 150 
scaled Shenoy-Shafer architecture, 147 
scaled valuation algebra, 145 
search methods, xxxi 
semi-join, 10 
semigroup 
cancellative, 26, 155, 212 
commutative, 4, 155 
regular, 210 
separative, 205 
semigroup axiom, 4, 20, 25 
semilattice, 206 
semiring, 183 
bounded, 188, 245, 372 
c-semiring, 188 
cancellative, 213 
commutative, 183 
idempotent, 183 
multidimensional, 185 
of matrices, 185 
positive, 183 
quasi-regular, 232 
regular, 210 
separative, 207 
unit and zero element, 183 
semiring fixpoint equation, 229, 237 
semiring matrix 
power sequence, 228 
semiring valuation algebra, 191 
sensor network, xxxi 
separator, 82, 97, 128 
width, 97 
set potential, 12, 36 
support, 163 
set-based constraint, 194 
set-based semiring valuation algebra, 201 
Shenoy-Shafer architecture, 111 
collect or inward phase, 114 
distribute or outward phase, 114 
shortest distance problem, 221, 367 
shortest distance tree, 377 
simple semiring, see bounded semiring 
singleton, 7 
solution configuration, 312 
solution set, 295 
in optimization problem, 312 
in quasi-regular valuation algebra, 370 
in symmetric, positive define system, 
345 
space-for-time trade, 116 
sparse matrix, 321 
sparse matrix techniques, 220, 266, 321 
Spohn potential, see weighted constraint 
stability axiom, 70 
subset lattice, see powerset 
sup-topology, 228 
convergence, 228 
limit, 228 
super-bucket scheme, 120 
super-cluster scheme, see super-bucket scheme 
support, 207 
supremum, 18 
Sylvester construction, 41 
symmetric, positive definite system, 338 
non-zero structure, 354 
t-conorm, 218 
t-norm, see triangular norm 
tabular representation, see relational represen-
tation 
time-discrete, linear dynamic system, see lin-
ear dynamic system 
time-for-space trade, 117, 120 
transitivity axiom, 4, 20, 25 
tree, 30 
directed, 31 
root node, 31 
undirected, see tree 
tree of diagnoses, 21 
tree-decomposition, xxvii, see covering join 
tree 
treewidth, xxvii, 62, 64 
triangular norm, 184, 218 
triangulation, 83 
tuple, 6, 293 
domain, 6 
extension and projection, 7 
tuple set, see relation 
tuple system, 286 
turbo code, 311 
uninorm, 217 
updating methods, 134 
consistent update, 174 
vacuous extension, 70, 104 
valuation algebra, xxiii, 5 
cancellative, 26 
conditional, 176 
consistent, 73 

452 
INDEX 
division, 110, 121 
idempotent, 122, 167 
identity element, 91 
instance, xxviii, 6 
inverse element, 121 
neutral element, 69 
null element, 73 
ordered, xxxi 
partial projection, 25 
regular, 121, 164 
scaled or normalized, 143 
separative, 121, 155 
stable, 70, 105 
transport, 104 
weight predictable, 66 
valuation network, 32 
variable 
binary, 6 
Boolean, see propositional variable 
propositional, 6 
variable elimination operator, 49 
variable elimination sequence, 51, 57 
induced width, 63 
variable-valuation-linked-list, 83 
vector, see tuple 
Walsh function, 41 
Warshall-Floyd-Kleene algorithm, 365, 378 
weight function, 48, 66 
weight predictor, 66 
weighted constraint, 26, 192, 308 
zero-pattern, 325, 327, 331, 336, 343 

