Probabilistic Methods for
Uncertainty Quantiﬁcation in
Computational Models
Khachik Sargsyan
Livermore, CA
Schlumberger IOU Webinar
April 19, 2016
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
1 / 57
SAND2016-3646PE

Acknowledgements
H. Najm, B. Debusschere, C. Safta, X. Huan — Sandia National
Laboratories, CA
R. Ghanem — USC
O. Knio — Duke
O. Le Maˆıtre — LIMSI-CNRS, Paris
Y. Marzouk — MIT
D. Ricciuto, P. Thornton – Oak Ridge National Lab
This work was supported by:
DOE Advanced Scientiﬁc Computing Research (ASCR), Scientiﬁc Discovery through
Advanced Computing (SciDAC)
DOE, Biological and Environmental Research (BER)
DOD, DARPA Enabling Quantiﬁcation of Uncertainty in Physical Systems (EQUiPS)
program
Sandia National Laboratories is a multi-program laboratory operated by Sandia Corporation, a wholly owned subsidiary of
Lockheed Martin Corporation, for the U.S. Department of Energy’s National Nuclear Security Administration under contract
DE-AC04-94AL85000.
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
2 / 57

Outline
1
Introduction
2
Forward UQ – Polynomial Chaos
3
Inverse UQ – Bayesian Inference
4
Advanced Topics
High Dimensional PC Surrogate Construction
Account for Model Error in Bayesian Inference
5
Closure
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
2 / 57

Background
Ph.D. from U. of Michigan, Applied Math,, 2007
2007-present: working in UQ at Sandia National Labs
US Department of Energy, Ofﬁce of Science, Advanced Scientiﬁc
Computing Research (ASCR)
QUEST Institute (Quantiﬁcation of Uncertainty in Extreme Scale
CompuTations)
PI: Habib Najm
www.quest-scidac.org
Advanced UQ methods development
Reach out to application community
SNL-CA: 5-10 staff members, ∼5 postdocs
Main research code: UQTk (www.sandia.gov/UQToolkit)
Lightweight C++/Python codebase
UQTk v3.0 to be posted soon
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
3 / 57

UQ Software Packages under QUEST
The SciDAC Institute on Quantiﬁcation of Uncertainty in Extreme
Scale Computations (QUEST) is developing and maintaining a
number of packages
http://www.quest-scidac.org/
DAKOTA: http://dakota.sandia.gov/
UQTk: UQ Toolkit http://www.sandia.gov/UQToolkit/
GPMSA: Gaussian Process Modeling and Sensitivity Analysis
QUESO: Bayesian inference
https://github.com/libqueso/queso/releases
MUQ: MIT Uncertainty Quantiﬁcation library
https://bitbucket.org/mituq/muq
Many packages are becoming available outside QUEST (UQLab,
OpenTurns, SmartUQ, ChaosPy,...)
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
4 / 57

Uncertainty Quantiﬁcation Toolkit (UQTk)
A library of C++ and Python functions for propagation of uncertainty
through computational models
Mainly relies on Polynomial Chaos (PC) expansions for representing
random variables and stochastic processes
Target usage:
Rapid prototyping
Algorithmic research
Tutorials / educational
Version 2.1 released under the GNU Lesser General Public License
C++ Tools for intrusive and non-intrusive UQ
Polynomial Chaos
Bayesian inference tools (various MCMC types)
Regression (polynomial, RBF, GP) tools
(Sparse) quadrature integration
Rosenblatt transformation
Python postprocessing and analysis tools
Version 3.0 to be released very soon
Available at http://www.sandia.gov/UQToolkit
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
5 / 57

Uncertainty Quantiﬁcation and Computational Science
!"#$%&'(")'*+,"-.*+
y = f(x) 
/)$%&0+
1%&$%&+
23.-45(")0+
2'3'#.&.30+
!"#$%&'(")'*+,"-.*+
y = f(x)
x+
y+
Forward problem
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
6 / 57

Uncertainty Quantiﬁcation and Computational Science
!"#$%&'(")'*+,"-.*+
y = f(x) 
/)$%&0+
1%&$%&+
23.-45(")0+
2'3'#.&.30+
,.'0%3.#.)&+,"-.*+
z = g(x) 
+
,.'0%3.#.)&+,"-.*
z = g(x)
!"#$%&'(")'*+,"-.*+
y = f(x)
x+
y+
6'&'+
Inverse & Forward problems
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
6 / 57

Uncertainty Quantiﬁcation and Computational Science
!"#$%&'(")'*+,"-.*+
y = f(x) 
/)$%&0+
1%&$%&+
23.-45(")0+
2'3'#.&.30+
,.'0%3.#.)&+,"-.*+
z = g(x) 
+
,.'0%3.#.)&+,"-.*
z = g(x)
!"#$%&'(")'*+,"-.*+
y = f(x)
x+
y+
6'&'+
zd+
Inverse & Forward UQ
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
6 / 57

Uncertainty Quantiﬁcation and Computational Science
!"#$%&'(")'*+,"-.*+
y = f(x) 
/)$%&0+
1%&$%&+
23.-45(")0+
2'3'#.&.30+
,.'0%3.#.)&+,"-.*+
z = g(x) 
+
,.'0%3.#.)&+,"-.*
z = g(x)
!"#$%&'(")'*+,"-.*+
y = f(x)
x+
y+
6'&'+
zd+
yd+
y ={f1(x), f2(x)7+87+fM(x)}+
Inverse & Forward UQ
Model validation & comparison, Hypothesis testing
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
6 / 57

The Case for Uncertainty Quantiﬁcation
UQ needed for...
Model predictions
Model validation and comparison
Conﬁdence assessment
Reliability analysis
Dimensionality reduction
Optimal design
Decision support
(Noisy) data assimilation
Uncertainty Sources
Model parameters
Initial/boundary conditions
Model geometry/structure
Lack of knowledge
Data noise
Intrinsic stochasticity
Numerical errors, too
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
7 / 57

Outline
1
Introduction
2
Forward UQ – Polynomial Chaos
3
Inverse UQ – Bayesian Inference
4
Advanced Topics
High Dimensional PC Surrogate Construction
Account for Model Error in Bayesian Inference
5
Closure
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
7 / 57

Polynomial Chaos – functional representation for RVs
U ≃
p
X
k=0
ukψk(ξ)
First introduced by Wiener, 1938
Revitalized by Ghanem and Spanos, 1991
Convergent series if U has ﬁnite variance
Selection of order p is a modeling choice
Describes a r.v. U with a vector of PC modes (u0, u1, . . . , up)
Standard r.v. ξ, standard orthogonal polynomials ψk(ξ), i.e.
R
ψi(ξ)ψj(ξ)πξ(ξ)dξ = δij||ψi||2
PC Type
Domain
Density πξ(ξ)
Polynomial
Free parameters
Gauss-Hermite
(−∞, +∞)
1
√
2π e−ξ2
2
Hermite
none
Legendre-Uniform
[−1, 1]
1
2
Legendre
none
Gamma-Laguerre
[0, +∞)
ξαe−ξ
Γ(α+1)
Laguerre
α > −1
Beta-Jacobi
[−1, 1]
(1+ξ)α(1−ξ)β
2α+β+1B(α+1,β+1)
Jacobi
α > −1, β > −1
[Wiener, 1938; Ghanem & Spanos, 1991; Xiu & Karniadakis, 2002; Le Maˆıtre & Knio, 2010]
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
8 / 57

Construction of 1D PC
U ≃Pp
k=0ukψk(ξ)
Orthogonal projection:
uk =
1
||ψk||2 ⟨Uψk⟩
Need to compute integral
⟨Uψk⟩=
R
U(?)ψk(ξ)πξ(ξ)dξ
Need a map U ↔ξ
If lucky, there is an explicit formula, e.g. lognormal U = eξ
0
1
2
3
4
5
6
0.0
0.2
0.4
0.6
0.8
1.0
HG PC Order 1
Exact Lognormal
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
9 / 57

Construction of 1D PC
U ≃Pp
k=0ukψk(ξ)
Orthogonal projection:
uk =
1
||ψk||2 ⟨Uψk⟩
Need to compute integral
⟨Uψk⟩=
R
U(?)ψk(ξ)πξ(ξ)dξ
Need a map U ↔ξ
If lucky, there is an explicit formula, e.g. lognormal U = eξ
0
1
2
3
4
5
6
0.0
0.2
0.4
0.6
0.8
1.0
HG PC Order 3
Exact Lognormal
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
9 / 57

Construction of 1D PC
U ≃Pp
k=0ukψk(ξ)
Orthogonal projection:
uk =
1
||ψk||2 ⟨Uψk⟩
Need to compute integral
⟨Uψk⟩=
R
U(?)ψk(ξ)πξ(ξ)dξ
Need a map U ↔ξ
If lucky, there is an explicit formula, e.g. lognormal U = eξ
0
1
2
3
4
5
6
0.0
0.2
0.4
0.6
0.8
1.0
HG PC Order 5
Exact Lognormal
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
9 / 57

Construction of 1D PC
U ≃Pp
k=0ukψk(ξ)
Orthogonal projection:
uk =
1
||ψk||2 ⟨Uψk⟩
Need to compute integral
⟨Uψk⟩=
R
U(?)ψk(ξ)πξ(ξ)dξ
Need a map U ↔ξ
If lucky, there is an explicit formula, e.g. lognormal U = eξ
0
1
2
3
4
5
6
0.0
0.2
0.4
0.6
0.8
1.0
HG PC Order 7
Exact Lognormal
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
9 / 57

Construction of 1D PC
U ≃Pp
k=0ukψk(ξ)
Orthogonal projection:
uk =
1
||ψk||2 ⟨Uψk⟩
Need to compute integral
⟨Uψk⟩=
R
U(?)ψk(ξ)πξ(ξ)dξ
Need a map U ↔ξ
CDF transform helps:
U = F −1
U ( ξ+1
2 ) if ξ is Uniform, Legendre-Uniform PC
U = F −1
U (Φ(ξ)) if ξ is Normal, Gauss-Hermite PC
where FU(·) is the Cumulative Distribution Function (CDF) of U.
[and Φ(·) is CDF for standard normal]
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
9 / 57

Multivariate Polynomial Chaos































U1 =
K1
X
k=0
u1kΨk(ξ1, . . . , ξn)
U2 =
K2
X
k=0
u2kΨk(ξ1, . . . , ξn)
...
...
Ud =
Kd
X
k=0
udkΨk(ξ1, . . . , ξn)
Multivariate polynomial
Ψk(ξ) = ψα1(ξ1) · · · ψαn(ξn)
Usually d = n
Construction non-trivial: e.g., capture
the PDF of U
select moments of U
some QoI h(U)
Multivariate normal is a special case
Multiindex (α1, . . . , αn) selection,
Truncation; see later
Rosenblatt map
(multivariate CDF transform)
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
10 / 57

Multivariate Polynomial Chaos































U1 =
K1
X
k=0
u1kΨk(ξ1, . . . , ξn)
U2 =
K2
X
k=0
u2kΨk(ξ1, . . . , ξn)
...
...
Ud =
Kd
X
k=0
udkΨk(ξ1, . . . , ξn)
Multivariate polynomial
Ψk(ξ) = ψα1(ξ1) · · · ψαn(ξn)
Usually d = n
Construction non-trivial: e.g., capture
the PDF of U
select moments of U
some QoI h(U)
Multivariate normal is a special case
Multiindex (α1, . . . , αn) selection,
Truncation; see later
Rosenblatt map
(multivariate CDF transform)
Fun example: X = ξ2
1 + ξ2
2 is exponential r.v. if ξ’s are i.i.d. gaussians.
However, no ﬁnite order 1D PC exists.
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
10 / 57

Essential Use of PC in UQ
U ≃PK
k=0 ukΨk(ξ)
Strategy:
Represent model parameters/solution as random variables
Construct PC for uncertain parameters
Evaluate PC for model outputs
Advantages:
Computational efﬁciency
Utility
Moments: E[u] = u0, V[u] = PK
k=1 u2
k||Ψk||2, . . .
Global Sensitivities – fractional variances, Sobol’ indices
Uncertainty propagation
Surrogate for forward model
Requirements:
Finite variances (not a handicap in practice)
Smooth forward functions
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
11 / 57

PC features: moment extraction
U ≃
K
X
k=0
ukΨk(ξ)
Expectation: ⟨u⟩= u0
Variance σ2
σ2
=

(u −⟨u⟩)2
=
*
(
K
X
k=1
ukΨk(ξ))2
+
=
* K
X
k=1
K
X
j=1
ujukΨj(ξ)Ψk(ξ)
+
=
K
X
k=1
K
X
j=1
ujuk ⟨Ψj(ξ)Ψk(ξ)⟩=
K
X
k=1
u2
k||Ψk||2
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
12 / 57

PC features: Global Sensitivity Analysis U(ξ) ≃
K
X
k=0
ukΨk(ξ)
Main effect sensitivity indices
Si = V ar[E(U(ξ|ξi)]
V ar[U(ξ)]
=
P
k∈Ii u2
k||Ψk||2
P
k>0 u2
k||Ψk||2
Ii is the set of bases with only ξi involved
Si is the uncertainty contribution that is due to i-th parameter only
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
13 / 57

PC features: Global Sensitivity Analysis U(ξ) ≃
K
X
k=0
ukΨk(ξ)
Main effect sensitivity indices
Si = V ar[E(U(ξ|ξi)]
V ar[U(ξ)]
=
P
k∈Ii u2
k||Ψk||2
P
k>0 u2
k||Ψk||2
Ii is the set of bases with only ξi involved
Si is the uncertainty contribution that is due to i-th parameter only
Total effect sensitivity indices
Ti = 1 −V ar[E(U(ξ|ξ−i)]
V ar[U(ξ)]
=
P
k∈IT
i u2
k||Ψk||2
P
k>0 u2
k||Ψk||2
IT
i is the set of bases with ξi involved, including all its interactions.
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
13 / 57

PC features: Global Sensitivity Analysis U(ξ) ≃
K
X
k=0
ukΨk(ξ)
Main effect sensitivity indices
Si = V ar[E(U(ξ|ξi)]
V ar[U(ξ)]
=
P
k∈Ii u2
k||Ψk||2
P
k>0 u2
k||Ψk||2
Ii is the set of bases with only ξi involved
Si is the uncertainty contribution that is due to i-th parameter only
Joint sensitivity indices
Sij = V ar[E(U(ξ|ξi, ξj)]
V ar[U(ξ)]
−Si −Sj =
P
k∈Iij u2
k||Ψk||2
P
k>0 u2
k||Ψk||2
Iij is the set of bases with only ξi and ξj involved
Sij is the uncertainty contribution that is due to (i, j) parameter pair
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
13 / 57

PC features: uncertainty propagation
U ≃
K
X
k=0
ukΨk(ξ)
f(U) ≃
K
X
k=0
fkΨk(ξ)
Basic task: given PC for inputs, ﬁnd PC for outputs.
Input-output map can also be deﬁned implicitly, via governing
equations G(f, U) = 0.
Two approaches
Intrusive: project governing equations
Results in set of equations for the PC modes
Requires redesign of computer code
PCEs for all uncertain variables in system
Non-intrusive: project outputs of interest
Sampling to evaluate projection operator
Can use existing code as black box
Only computes PCEs for quantities of interest
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
14 / 57

Non-intrusive Spectral Projection (NISP) PC UQ
U ≃
K
X
k=0
ukΨk(ξ)
f(U) ≃
K
X
k=0
fkΨk(ξ)
For any model output of interest f(X):
fk = ⟨fΨk⟩
⟨Ψ2
k⟩=
1
||Ψk||2
Z
f(X(ξ)) Ψk(ξ)πξ(ξ)dξ
Evaluate projection integral numerically
Relies on black-box utilization of the computational model
Integral can be evaluated using
– A variety of (Quasi) Monte Carlo methods
Slow convergence; ∼indep. of dimensionality
– Quadrature/Sparse-Quadrature methods
Fast convergence; depends on dimensionality
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
15 / 57

PC surrogate construction
Build/presume PC for input parameter U
U(ξ) =
K
X
k=0
ukΨk(ξ)
with respect to multivariate standard polynomials.
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
16 / 57

PC surrogate construction
Build/presume PC for input parameter U
U(ξ) =
K
X
k=0
ukΨk(ξ)
with respect to multivariate standard polynomials.
E.g., uniform on an interval, or gaussian with known moments,
U = U0 + U T
1 ξ
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
16 / 57

PC surrogate construction
Build/presume PC for input parameter U
U(ξ) =
K
X
k=0
ukΨk(ξ)
with respect to multivariate standard polynomials.
If input parameters are uniform Ui ∼Uniform[ai, bi], then
Ui = ai + bi
2
+ bi −ai
2
ξi.
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
16 / 57

PC surrogate construction
Build/presume PC for input parameter U
U(ξ) =
K
X
k=0
ukΨk(ξ)
with respect to multivariate standard polynomials.
Input parameters are represented via their cumulative distribution
function (CDF) F(·), such that, with ξi ∼Uniform[−1, 1]
Ui = F −1
Ui
ξi + 1
2

,
for i = 1, 2, . . . , d.
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
16 / 57

PC surrogate construction
Build/presume PC for input parameter U
U(ξ) =
K
X
k=0
ukΨk(ξ)
with respect to multivariate standard polynomials.
Input parameters are represented via their cumulative distribution
function (CDF) F(·), such that, with ξi ∼Uniform[−1, 1]
Ui = F −1
Ui
ξi + 1
2

,
for i = 1, 2, . . . , d.
Forward function f(·), output Z
Z = f(U(ξ))
Z =
K
X
k=0
fkΨk(ξ) ≡fs(ξ)
Global sensitivity information for free
- Sobol indices, variance-based decomposition.
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
16 / 57

Outline
1
Introduction
2
Forward UQ – Polynomial Chaos
3
Inverse UQ – Bayesian Inference
4
Advanced Topics
High Dimensional PC Surrogate Construction
Account for Model Error in Bayesian Inference
5
Closure
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
16 / 57

Inverse UQ – Estimation of Uncertain Parameters
Probabilistic setting
Require joint PDF on input space
Statistical inference – an inverse problem
Bayesian setting
Given Constraints: PDF on uncertain inputs can be estimated
using the Maximum Entropy principle
– MaxEnt Methods
Given Data: PDF on uncertain inputs can be estimated using
Bayes formula
– Bayesian Inference
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
17 / 57

Bayes formula for Parameter Inference
Collected data:
{(xi, yi)}N
i=1
Data model:
yi = f(xi; λ) + ϵi
Bayes formula:
p(λ|y)
Posterior =
Likelihood
p(y|λ)
Prior
p(λ)
p(y)
Evidence
Prior: knowledge of λ prior to data
Likelihood: forward model and measurement noise
Posterior: combines information from prior and data
Evidence: normalizing constant for present context
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
18 / 57

The Prior
Prior p(λ) comes from
Physical constraints
Prior data/knowledge
Types of uninformative priors
Improper prior
Objective prior
Maxent prior
Reference prior
Jeffreys prior
It can be chosen to impose regularization
Unknown aspects of the prior can be added to the rest of the
parameters as hyperparameters
The choice of prior can be crucial if data is not informative
When there is sufﬁcient information in the data, the data can
overrule the prior
p(λ|y)
Posterior =
Likelihood
p(y|λ)
Prior
p(λ)
p(y)
Evidence
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
19 / 57

Construction of the Likelihood p(y|λ)
Requires a presumed error model
Data model:
yi = f(xi; λ) + ϵi
Model this error as a random variable, e.g.
Error is due to instrument measurement noise
Instrument has Gaussian errors, with no bias
Measurements are independent
ϵ ∼N(0, σ2)
For any given λ, this implies
yi|λ, σ ∼N(f(xi; λ), σ2)
or
p(y|λ, σ) =
N
Y
i=1
1
√
2π σ exp

−(yi −f(xi; λ))2
2σ2

p(λ|y)
Posterior =
Likelihood
p(y|λ)
Prior
p(λ)
p(y)
Evidence
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
20 / 57

Exploring the Posterior
Given any sample λ, the un-normalized posterior probability can
be easily computed
Posterior
p(λ|y)
∝
Likelihood
p(y|λ)
Prior
p(λ)
Explore posterior w/ Markov Chain Monte Carlo (MCMC)
– Metropolis-Hastings algorithm:
Random walk with proposal PDF & rejection rules
– Computationally intensive, O(105) samples
– Each sample: evaluation of the forward model
Surrogate models
Evaluate moments/marginals from the MCMC statistics
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
21 / 57

Forward and Inverse UQ in a nutshell
Forward UQ
Inverse UQ
f(λ)
Model
fc(λ)
Surrogate
Dim.
Red.
Likelihood
D = {yi}
Data
Posterior p(λ|D)
Prior p(λ)
h(λ)
Any model
Prediction p(h(λ)|D)
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
22 / 57

Bayesian inference: Noise↑⇒Posterior uncertainty↑
True model y = tanh(3x −0.9)
Increasing data noise level
Calibrating f(x; λ) = λ1eλ0x −2
Larger data noise ⇒larger posterior uncertainty
−1.0
−0.5
0.0
0.5
1.0
x
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
y
Data, Noise = 0.1
Predictive mean
True function
0.5
0.6
0.7
0.8
0.9
1.0
λ1
1.3
1.4
1.5
1.6
1.7
1.8
λ2
Noise = 0.1
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
23 / 57

Bayesian inference: Noise↑⇒Posterior uncertainty↑
True model y = tanh(3x −0.9)
Increasing data noise level
Calibrating f(x; λ) = λ1eλ0x −2
Larger data noise ⇒larger posterior uncertainty
−1.0
−0.5
0.0
0.5
1.0
x
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
y
Data, Noise = 0.05
Predictive mean
True function
0.5
0.6
0.7
0.8
0.9
1.0
λ1
1.3
1.4
1.5
1.6
1.7
1.8
λ2
Noise = 0.05
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
23 / 57

Bayesian inference: Noise↑⇒Posterior uncertainty↑
True model y = tanh(3x −0.9)
Increasing data noise level
Calibrating f(x; λ) = λ1eλ0x −2
Larger data noise ⇒larger posterior uncertainty
−1.0
−0.5
0.0
0.5
1.0
x
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
y
Data, Noise = 0.02
Predictive mean
True function
0.5
0.6
0.7
0.8
0.9
1.0
λ1
1.3
1.4
1.5
1.6
1.7
1.8
λ2
Noise = 0.02
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
23 / 57

Bayesian inference: Noise↑⇒Posterior uncertainty↑
True model y = tanh(3x −0.9)
Increasing data noise level
Calibrating f(x; λ) = λ1eλ0x −2
Larger data noise ⇒larger posterior uncertainty
−1.0
−0.5
0.0
0.5
1.0
x
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
y
Data, Noise = 0.1
Predictive mean
True function
0.5
0.6
0.7
0.8
0.9
1.0
λ1
1.3
1.4
1.5
1.6
1.7
1.8
λ2
Noise = 0.02
Noise = 0.05
Noise = 0.1
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
23 / 57

Bayesian inference: Data range ⇒Correlation
True model y = tanh(3x −0.9)
Collecting data at different locations
Calibrating f(x; λ) = λ1eλ0x −2
Correlation structure can change drastically
−1.0
−0.5
0.0
0.5
1.0
x
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
y
−0.5
0.0
0.5
1.0
1.5
λ1
0.5
1.0
1.5
2.0
2.5
λ2
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
24 / 57

Outline
1
Introduction
2
Forward UQ – Polynomial Chaos
3
Inverse UQ – Bayesian Inference
4
Advanced Topics
High Dimensional PC Surrogate Construction
Account for Model Error in Bayesian Inference
5
Closure
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
24 / 57

Laundry List of Challenges/Issues (Incomplete)
High-Dimensionality
Expensive Models
Non-Linear Models, Discontinuities, Bimodalities
Scarce Data
Intrinsic Stochasticity
Model Errors
Input Correlations
Time Dynamics
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
25 / 57

Laundry List of Challenges/Issues (Incomplete)
High-Dimensionality
• Large number of input parameters
• Dense spatial/temporal grid
• PC truncation is a challenge
• Low-rank (tensor) representations
• Sparse learning, (Bayesian) compressive sensing
Expensive Models
Non-Linear Models, Discontinuities, Bimodalities
Scarce Data
Intrinsic Stochasticity
Model Errors
Input Correlations
Time Dynamics
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
25 / 57

Laundry List of Challenges/Issues (Incomplete)
High-Dimensionality
Expensive Models
• UQ studies seriously hindered
• Need surrogates with few model simulations
Non-Linear Models, Discontinuities, Bimodalities
Scarce Data
Intrinsic Stochasticity
Model Errors
Input Correlations
Time Dynamics
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
25 / 57

Laundry List of Challenges/Issues (Incomplete)
High-Dimensionality
Expensive Models
Non-Linear Models, Discontinuities, Bimodalities
• Polynomial representation not good enough
• Quadrature integration fails
• Stochastic domain decomposition
• Data clustering/classiﬁcation
Scarce Data
Intrinsic Stochasticity
Model Errors
Input Correlations
Time Dynamics
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
25 / 57

Laundry List of Challenges/Issues (Incomplete)
High-Dimensionality
Expensive Models
Non-Linear Models, Discontinuities, Bimodalities
Scarce Data
• Bayesian inference is prior-dominated
• Lack of parameter identiﬁability
• Bayesian methods do quantify lack-of-data uncertainty
Intrinsic Stochasticity
Model Errors
Input Correlations
Time Dynamics
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
25 / 57

Laundry List of Challenges/Issues (Incomplete)
High-Dimensionality
Expensive Models
Non-Linear Models, Discontinuities, Bimodalities
Scarce Data
Intrinsic Stochasticity
• Quadrature and sparse quadrature methods fail
• Averaged quantities
• Bayesian regression
Model Errors
Input Correlations
Time Dynamics
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
25 / 57

Laundry List of Challenges/Issues (Incomplete)
High-Dimensionality
Expensive Models
Non-Linear Models, Discontinuities, Bimodalities
Scarce Data
Intrinsic Stochasticity
Model Errors
• Models are not perfect
• Can not be ignored during parameter estimation
• Additive model error as a Gaussian Process
• Embedded model error
Input Correlations
Time Dynamics
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
25 / 57

Laundry List of Challenges/Issues (Incomplete)
High-Dimensionality
Expensive Models
Non-Linear Models, Discontinuities, Bimodalities
Scarce Data
Intrinsic Stochasticity
Model Errors
Input Correlations
• Hard to sample from
• Hard to interpret sensitivities
• Rosenblatt transformation
Time Dynamics
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
25 / 57

Laundry List of Challenges/Issues (Incomplete)
High-Dimensionality
Expensive Models
Non-Linear Models, Discontinuities, Bimodalities
Scarce Data
Intrinsic Stochasticity
Model Errors
Input Correlations
Low-Probability (Tail) Events
• PC inaccurate in capturing regions of low probability
• Use targeted PC germs ξ with fat tails
Time Dynamics
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
25 / 57

Laundry List of Challenges/Issues (Incomplete)
High-Dimensionality
Expensive Models
Non-Linear Models, Discontinuities, Bimodalities
Scarce Data
Intrinsic Stochasticity
Model Errors
Input Correlations
Time Dynamics
• Large ampliﬁcation of phase errors over long time horizon
• Chaotic dynamics
• Increase order with time to retain accuracy
• Ad-hoc corrections
• Look at averaged quantities
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
25 / 57

Outline
1
Introduction
2
Forward UQ – Polynomial Chaos
3
Inverse UQ – Bayesian Inference
4
Advanced Topics
High Dimensional PC Surrogate Construction
Account for Model Error in Bayesian Inference
5
Closure
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
25 / 57

Surrogate construction: scope and challenges
Construct surrogate for a complex model
f(λ)
to enable
• Global sensitivity analysis
• Optimization
• Forward uncertainty propagation
• Input parameter calibration
• · · ·
• Computationally expensive model simulations, data sparsity
• Need to build accurate surrogates with as few
training runs as possible
• High-dimensional input space
• Too many samples needed to cover the space
• Too many terms in the polynomial expansion
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
26 / 57

Surrogate construction: scope and challenges
Construct surrogate for a complex model
f(λ)
to enable
• Global sensitivity analysis
• Optimization
• Forward uncertainty propagation
• Input parameter calibration
• · · ·
• Computationally expensive model simulations, data sparsity
• Need to build accurate surrogates with as few
training runs as possible
• High-dimensional input space
• Too many samples needed to cover the space
• Too many terms in the polynomial expansion
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
26 / 57

Surrogate construction: scope and challenges
Construct surrogate for a complex model
f(λ)
to enable
• Global sensitivity analysis
• Optimization
• Forward uncertainty propagation
• Input parameter calibration
• · · ·
• Computationally expensive model simulations, data sparsity
• Need to build accurate surrogates with as few
training runs as possible
• High-dimensional input space
• Too many samples needed to cover the space
• Too many terms in the polynomial expansion
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
26 / 57

Alternative methods to obtain PC coefﬁcients
Z = f(U(ξ)) ≃
K
X
k=0
fkΨk(ξ)
• Projection
fk = ⟨f(ξ)Ψk(ξ)⟩
||Ψk||2
The integral ⟨f(ξ)Ψk(ξ)⟩=
R
f(ξ)Ψk(ξ)πξ(ξ)dξ is estimated by...
• Monte-Carlo
1
N
N
X
j=1
f(ξj)Ψk(ξj)
many(!) random samples
• Quadrature
Q
X
j=1
f(ξj)Ψk(ξj)wj
samples at quadrature
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
27 / 57

Alternative methods to obtain PC coefﬁcients
Z = f(U(ξ)) ≃
K
X
k=0
fkΨk(ξ)
• Projection
fk = ⟨f(ξ)Ψk(ξ)⟩
||Ψk||2
The integral ⟨f(ξ)Ψk(ξ)⟩=
R
f(ξ)Ψk(ξ)πξ(ξ)dξ is estimated by...
• Monte-Carlo
1
N
N
X
j=1
f(ξj)Ψk(ξj)
many(!) random samples
• Quadrature
Q
X
j=1
f(ξj)Ψk(ξj)wj
samples at quadrature
• Bayesian regression
P(fk|f(ξj)) ∝P(f(ξj)|fk)P(fk)
any (number of) samples
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
27 / 57

Alternative methods to obtain PC coefﬁcients
Z = f(U(ξ)) ≃
K
X
k=0
fkΨk(ξ)
• Projection
fk = ⟨f(ξ)Ψk(ξ)⟩
||Ψk||2
The integral ⟨f(ξ)Ψk(ξ)⟩=
R
f(ξ)Ψk(ξ)πξ(ξ)dξ is estimated by...
• Monte-Carlo
1
N
N
X
j=1
f(ξj)Ψk(ξj)
many(!) random samples
• Quadrature
Q
X
j=1
f(ξj)Ψk(ξj)wj
samples at quadrature
• Bayesian regression
P(f|D)
| {z }
Posterior
∝P(D|f)
| {z }
Likelihood
P(f)
| {z }
Prior
any (number of) samples
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
27 / 57

Bayesian inference of PC surrogate
Z = f(ξ) ≃PK
k=0 fkΨk(ξ) ≡fs(ξ)
Posterior
z }| {
P(f|D) ∝
Likelihood
z }| {
P(D|f)
Prior
z }| {
P(f)
• Data consists of training runs
D ≡{(ξi, Zi)}N
i=1
• Likelihood with a gaussian noise model with σ2 ﬁxed or inferred,
L(f) = P(D|f) =

1
σ
√
2π
N
N
Y
i=1
exp

−(fi −fs(ξi))2
2σ2

• Prior on f is chosen to be conjugate, uniform or gaussian.
• Posterior is a multivariate normal
f
∈
MVN(µ, Σ)
• The (uncertain) surrogate is a gaussian process
fs(ξ) =
K
X
k=0
fkΨk(ξ) = Ψ(ξ)T f
∈
GP(Ψ(ξ)T µ, Ψ(ξ)ΣΨ(ξ′)T )
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
28 / 57

Bayesian inference of PC surrogate: high-d, low-data regime
Z = f(ξ) ≈PK
k=0 fkΨk(ξ)
Ψk(ξ1, ξ2, ..., ξd) = ψk1(ξ1)ψk2(ξ2) · · · ψkd(ξd)
• Issues:
• how to properly choose
the basis set?
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
Dim 1
Dim 2
• need to work in underdetermined regime
N < K: fewer data than bases (d.o.f.)
• Discover the underlying low-d structure in the model
• get help from the machine learning community
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
29 / 57

Bayesian inference of PC surrogate: high-d, low-data regime
Z = f(ξ) ≈PK
k=0 fkΨk(ξ)
Ψk(ξ1, ξ2, ..., ξd) = ψk1(ξ1)ψk2(ξ2) · · · ψkd(ξd)
• Issues:
• how to properly choose
the basis set?
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
Dim 1
Dim 2
• need to work in underdetermined regime
N < K: fewer data than bases (d.o.f.)
• Discover the underlying low-d structure in the model
• get help from the machine learning community
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
29 / 57

Bayesian inference of PC surrogate: high-d, low-data regime
Z = f(ξ) ≈PK
k=0 fkΨk(ξ)
Ψk(ξ1, ξ2, ..., ξd) = ψk1(ξ1)ψk2(ξ2) · · · ψkd(ξd)
• Issues:
• how to properly choose
the basis set?
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
Dim 1
Dim 2
• need to work in underdetermined regime
N < K: fewer data than bases (d.o.f.)
• Discover the underlying low-d structure in the model
• get help from the machine learning community
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
29 / 57

Bayesian inference of PC surrogate: high-d, low-data regime
Z = f(ξ) ≈PK
k=0 fkΨk(ξ)
Ψk(ξ1, ξ2, ..., ξd) = ψk1(ξ1)ψk2(ξ2) · · · ψkd(ξd)
• Issues:
• how to properly choose
the basis set?
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
Dim 1
Dim 2
• need to work in underdetermined regime
N < K: fewer data than bases (d.o.f.)
• Discover the underlying low-d structure in the model
• get help from the machine learning community
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
29 / 57

Bayesian inference of PC surrogate: high-d, low-data regime
Z = f(ξ) ≈PK
k=0 fkΨk(ξ)
Ψk(ξ1, ξ2, ..., ξd) = ψk1(ξ1)ψk2(ξ2) · · · ψkd(ξd)
• Issues:
• how to properly choose
the basis set?
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
Dim 1
Dim 2
• need to work in underdetermined regime
N < K: fewer data than bases (d.o.f.)
• Discover the underlying low-d structure in the model
• get help from the machine learning community
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
29 / 57

In a different language....
• N training data points (ξi, Zi) and K + 1 basis terms Ψk(·)
• Projection matrix P N×(K+1) with P ik = Ψk(xi)
• Find regression weights f = (f0, . . . , fK) so that
Z ≈P f
or
Zi ≈PK
k=0fkΨk(ξi)
• The number of polynomial basis terms grows fast; a p-th order,
d-dimensional basis has a total of K + 1 = (p + d)!/(p!d!) terms.
• For limited data and large basis set (N ≤K) this is a sparse signal
recovery problem ⇒need some regularization/constraints.
• Least-squares
argminc {||Z −P f||2}
• The ‘sparsest’
argminc {||Z −P f||2 + α||f||0}
• Compressive sensing
argminc {||Z −P f||2 + α||f||1}
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
30 / 57

In a different language....
• N training data points (ξi, Zi) and K + 1 basis terms Ψk(·)
• Projection matrix P N×(K+1) with P ik = Ψk(xi)
• Find regression weights f = (f0, . . . , fK) so that
Z ≈P f
or
Zi ≈PK
k=0fkΨk(ξi)
• The number of polynomial basis terms grows fast; a p-th order,
d-dimensional basis has a total of K + 1 = (p + d)!/(p!d!) terms.
• For limited data and large basis set (N ≤K) this is a sparse signal
recovery problem ⇒need some regularization/constraints.
• Least-squares
argminc {||Z −P f||2}
• The ‘sparsest’
argminc {||Z −P f||2 + α||f||0}
• Compressive sensing
argminc {||Z −P f||2 + α||f||1}
Bayesian
Likelihood
Prior
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
30 / 57

Bayesian Compressive Sensing (BCS), or
Relevance Vector Machine (RVM)
• Dimensionality reduction by using hierarchical priors
p(fk|σ2
k) =
1
√
2πσk
e
−
f2
k
2σ2
k
p(σ2
k|α) = α
2 e−
ασ2
k
2
• Effectively, one obtains Laplace sparsity prior
p(f|α) =
Z
K−1
Y
k=0
p(fk|σ2
k)p(σ2
k|α)dσ2
k =
K−1
Y
k=0
√α
2 e−√α|fk|
• The parameter α can be further modeled hierarchically, or ﬁxed.
• Evidence maximization dictates values for σ2
k, α, σ2 and allows exact
Bayesian solution
c ∼MVN(µ, Σ)
with
µ = σ−2ΣP T u
Σ = σ2(P T P + diag(σ2/σ2
k))−1
[Tipping, 2001, Ji et al., 2008; Babacan et al., 2010]
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
31 / 57

Bayesian Compressive Sensing (BCS), or
Relevance Vector Machine (RVM)
• Dimensionality reduction by using hierarchical priors
p(fk|σ2
k) =
1
√
2πσk
e
−
f2
k
2σ2
k
p(σ2
k|α) = α
2 e−
ασ2
k
2
• Effectively, one obtains Laplace sparsity prior
p(f|α) =
Z
K−1
Y
k=0
p(fk|σ2
k)p(σ2
k|α)dσ2
k =
K−1
Y
k=0
√α
2 e−√α|fk|
• The parameter α can be further modeled hierarchically, or ﬁxed.
• Evidence maximization dictates values for σ2
k, α, σ2 and allows exact
Bayesian solution
c ∼MVN(µ, Σ)
with
µ = σ−2ΣP T u
Σ = σ2(P T P + diag(σ2/σ2
k))−1
[Tipping, 2001, Ji et al., 2008; Babacan et al., 2010]
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
31 / 57

Bayesian Compressive Sensing (BCS), or
Relevance Vector Machine (RVM)
• Dimensionality reduction by using hierarchical priors
p(fk|σ2
k) =
1
√
2πσk
e
−
f2
k
2σ2
k
p(σ2
k|α) = α
2 e−
ασ2
k
2
• Effectively, one obtains Laplace sparsity prior
p(f|α) =
Z
K−1
Y
k=0
p(fk|σ2
k)p(σ2
k|α)dσ2
k =
K−1
Y
k=0
√α
2 e−√α|fk|
• The parameter α can be further modeled hierarchically, or ﬁxed.
• Evidence maximization dictates values for σ2
k, α, σ2 and allows exact
Bayesian solution
c ∼MVN(µ, Σ)
with
µ = σ−2ΣP T u
Σ = σ2(P T P + diag(σ2/σ2
k))−1
[Tipping, 2001, Ji et al., 2008; Babacan et al., 2010]
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
31 / 57

Bayesian Compressive Sensing (BCS), or
Relevance Vector Machine (RVM)
• Dimensionality reduction by using hierarchical priors
p(fk|σ2
k) =
1
√
2πσk
e
−
f2
k
2σ2
k
p(σ2
k|α) = α
2 e−
ασ2
k
2
• Effectively, one obtains Laplace sparsity prior
p(f|α) =
Z
K−1
Y
k=0
p(fk|σ2
k)p(σ2
k|α)dσ2
k =
K−1
Y
k=0
√α
2 e−√α|fk|
• The parameter α can be further modeled hierarchically, or ﬁxed.
• Evidence maximization dictates values for σ2
k, α, σ2 and allows exact
Bayesian solution
c ∼MVN(µ, Σ)
with
µ = σ−2ΣP T u
Σ = σ2(P T P + diag(σ2/σ2
k))−1
• KEY: Some σ2
k →0, hence the corresponding basis terms are dropped.
[Tipping, 2001, Ji et al., 2008; Babacan et al., 2010]
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
31 / 57

BCS removes unnecessary basis terms
f(x, y) = cos(x + 4y)
f(x, y) = cos(x2 + 4y)
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
Order (dim 2)
Order (dim 1)
 
 
−18
−16
−14
−12
−10
−8
−6
−4
−2
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
Order (dim 2)
Order (dim 1)
 
 
−18
−16
−14
−12
−10
−8
−6
−4
−2
The square (i, j) represents the (log) spectral coefﬁcient
for the basis term ψi(x)ψj(y).
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
32 / 57

BCS recovers true PC coefﬁcients with increased
number of measurements
0
20
40
60
80
100
Coef Id, k
10-9
10-8
10-7
10-6
10-5
10-4
10-3
10-2
10-1
100
Coef magnitude, |ck |
Truth
BCS w/ N=30
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
33 / 57

BCS recovers true PC coefﬁcients with increased
number of measurements
0
20
40
60
80
100
Coef Id, k
10-9
10-8
10-7
10-6
10-5
10-4
10-3
10-2
10-1
100
Coef magnitude, |ck |
Truth
BCS w/ N=50
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
33 / 57

BCS recovers true PC coefﬁcients with increased
number of measurements
0
20
40
60
80
100
Coef Id, k
10-9
10-8
10-7
10-6
10-5
10-4
10-3
10-2
10-1
100
Coef magnitude, |ck |
Truth
BCS w/ N=100
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
33 / 57

Bayesian Compressive Sensing
• Dimensionality reduction by using hierarchical priors
p(fk|σ2
k) =
1
√
2πσk
e
−
f2
k
2σ2
k
p(σ2
k|α) = α
2 e−
ασ2
k
2
• Effectively, one obtains Laplace sparsity prior
p(c|α) =
Z
K−1
Y
k=0
p(fk|σ2
k)p(σ2
k|α)dσ2
k =
K−1
Y
k=0
√α
2 e−√α|fk|
• The parameter α can be further modeled hierarchically, or ﬁxed.
• Evidence maximization dictates values for σ2
k, α, σ2 and allows exact
Bayesian solution
f ∼MVN(µ, Σ)
with
µ = σ−2ΣP T u
Σ = σ2(P T P + diag(σ2/σ2
k))−1
• KEY: Some σ2
k →0, hence the corresponding basis terms are dropped.
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
34 / 57

Weighted Bayesian Compressive Sensing
• Dimensionality reduction by using hierarchical priors
p(fk|σ2
k) =
1
√
2πσk
e
−
f2
k
2σ2
k
p(σ2
k|αk) = αk
2 e−
αkσ2
k
2
• Effectively, one obtains Laplace sparsity prior
p(c|α) =
Z
K−1
Y
k=0
p(fk|σ2
k)p(σ2
k|αk)dσ2
k =
K−1
Y
k=0
√αk
2
e−√αk|fk|
• The parameter αk can be further modeled hierarchically, or ﬁxed.
• Evidence maximization dictates values for σ2
k, αk, σ2 and allows exact
Bayesian solution
f ∼MVN(µ, Σ)
with
µ = σ−2ΣP T u
Σ = σ2(P T P + diag(σ2/σ2
k))−1
• KEY: Some σ2
k →0, hence the corresponding basis terms are dropped.
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
34 / 57

Iteratively reweighting Compressive Sensing
[Candes et al., 2007]
Sparsest solution:
min||f||0 such that Z ≈P f
Compressive sensing:
min||f||1 such that Z ≈P f
Weighted compressive sensing:
min||W f||1 such that Z ≈P f
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
35 / 57

Iteratively reweighting Compressive Sensing
[Candes et al., 2007]
Sparsest solution:
min||f||0 such that Z ≈P f
Compressive sensing:
min||f||1 such that Z ≈P f
Weighted compressive sensing:
min||W f||1 such that Z ≈P f
For sparse signals, Z = P f s, with ||f s||0 = S < K, ideal weights are
W = diag
 1
|fs
k|

[i.e., Wkk = +∞if fs
k = 0]
In practice, the true signal coefﬁcients are not known, so...
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
35 / 57

Iteratively reweighting Compressive Sensing
[Candes et al., 2007]
Sparsest solution:
min||f||0 such that Z ≈P f
Compressive sensing:
min||f||1 such that Z ≈P f
Weighted compressive sensing:
min||W f||1 such that Z ≈P f
For sparse signals, Z = P f s, with ||f s||0 = S < K, ideal weights are
W = diag
 1
|fs
k|

[i.e., Wkk = +∞if fs
k = 0]
In practice, the true signal coefﬁcients are not known, so...
Iterative re-weighting
W (i+1) = diag
 
1
|f(i)
k | + ϵ
!
[ϵ ≪1 for stability]
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
35 / 57

Weighted Iterative BCS
Iterative BCS: We implement an iterative procedure that allows
increasing the order for the relevant basis terms while maintaining the
dimensionality reduction [Sargsyan et al. 2014], [Jakeman et al. 2015].
Combine basis growth and reweighting!
Initial Basis
Iterations
Weighted
BCS
Model data
Sparse Basis
Final Basis
Basis
Growth
Reweighting
New Basis
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
36 / 57

Basis set growth: simple anisotropic function
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
37 / 57
0
1
2
3
4
5
6
7
Dim 1
0
1
2
3
4
5
6
7
Dim 2

Basis set growth: ... added outlier term
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
38 / 57
0
1
2
3
4
5
6
7
Dim 1
0
1
2
3
4
5
6
7
Dim 2

The UQ Challenge for ACME Land Model
http://www.cesm.ucar.edu/models/clm/
A single-site, 1000-yr simulation takes ∼10 hrs on 1 CPU
Involves ∼70 input parameters; some dependent
Non-smooth input-output relationship
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
39 / 57

Sparse PC surrogate and uncertainty decomposition
for the ACME Land Model
Main effect sensitivities : rank input parameters
Joint sensitivities : most inﬂuential input couplings
About 200 polynomial basis terms in the 68-dimensional space
Sparse PC will further be used for
• sampling in a reduced space
• parameter calibration against experimental data
12
12 - flnr
61
61 - crit_onset_swi
32
32 - q10_mr
60
60 - crit_onset_fdd
19
19 - froot_leaf
16
16 - frootcn
Site # 51
GPP
12
12 - flnr
59
59 - fstor2tran
16
16 - frootcn
19
19 - froot_leaf
32
32 - q10_mr
61
61 - crit_onset_swi
Site # 51
TOTVEGC
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
40 / 57

Sparse PC surrogate and uncertainty decomposition
for the ACME Land Model
Main effect sensitivities : rank input parameters
Joint sensitivities : most inﬂuential input couplings
About 200 polynomial basis terms in the 68-dimensional space
Sparse PC will further be used for
• sampling in a reduced space
• parameter calibration against experimental data
SLATOP
LEAFCN
FLNR
FROOTCN
FROOT_LEAF
BDNR
BR_MR
Q10_MR
CN_S1
CN_S3
RF_L2S2
R_MORT
FSTOR2TRAN
RIT_ONSET_FDD
CRIT_ONSET_SWI
TLAI
GPP
TOTVEGC
EFLX_LH_TOT
TOTSOMC
10−3
10−2
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
40 / 57

Sparse PC surrogate and uncertainty decomposition
for the ACME Land Model
Main effect sensitivities : rank input parameters
Joint sensitivities : most inﬂuential input couplings
About 200 polynomial basis terms in the 68-dimensional space
Sparse PC will further be used for
• sampling in a reduced space
• parameter calibration against experimental data
GPP
gross primary
productivity
-1
-1
-1
-1
-1
-1
-1
-1
-1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
4
4
4
4
4
5
5
5
5
5
6
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
8
10
10
11
12
12
12
12
13
13
13
13
13
13
13
13
13
13
13
13
13
13
13
0.0
0.2
0.4
0.6
0.8
1.0
Sensitivity
soilpsi_off
mp
bp
slatop
crit_onset_swi
leafcn
flnr
frootcn
froot_leaf
fstor2tran
bdnr
br_mr
q10_mr
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
40 / 57

Outline
1
Introduction
2
Forward UQ – Polynomial Chaos
3
Inverse UQ – Bayesian Inference
4
Advanced Topics
High Dimensional PC Surrogate Construction
Account for Model Error in Bayesian Inference
5
Closure
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
40 / 57

Main target: quantiﬁcation of model error
Model error = deviation from ‘truth’, or from a higher-ﬁdelity model
• Represent and estimate the error associated with
• Simplifying assumptions, parameterizations
• Mathematical formulation, theoretical framework
• Numerical discretization
• ...will be useful for
• Model validation
• Model comparison
• Scientiﬁc discovery and model improvement
• Reliable computational predictions
• Inverse modeling context
• Given experimental or higher-ﬁdelity model data,
estimate the model error
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
41 / 57

Motivation: can’t afford ignoring model error
−1.0
−0.5
0.0
0.5
1.0
x
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
y
Data, N = 5
Model-data ﬁt
Given noisy data – Gaussian noise
y = gtrue(x) + ϵ
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
42 / 57

Motivation: can’t afford ignoring model error
−1.0
−0.5
0.0
0.5
1.0
x
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
y
Data, N = 5
Predictive mean
Predictive stdev
0.5
0.6
0.7
0.8
0.9
1.0
1.1
λ1
1.3
1.4
1.5
1.6
1.7
1.8
λ2
N = 5
Model-data ﬁt
Posterior on parameters
Employ Bayesian inference to ﬁt an exponential model: ym = f(x; λ)
Discrepancy between data and prediction presumed exclusively due to
i.i.d. Gaussian data noise: y = f(x; λ) + ϵd
Plotted:
Posterior density on the parameters
Preditive mean and standard deviation
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
42 / 57

Motivation: can’t afford ignoring model error
−1.0
−0.5
0.0
0.5
1.0
x
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
y
Data, N = 5
Predictive mean
Predictive stdev
True function
0.5
0.6
0.7
0.8
0.9
1.0
1.1
λ1
1.3
1.4
1.5
1.6
1.7
1.8
λ2
N = 5
Model-data ﬁt
Posterior on parameters
Employ Bayesian inference to ﬁt an exponential model: ym = f(x; λ)
Discrepancy between data and prediction presumed exclusively due to
i.i.d. Gaussian data noise: y = f(x; λ) + ϵd
True model g(x) – dashed-red – differs from ﬁt model f(x, λ)
Actual discrepancy includes both data and model errors
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
42 / 57

Motivation: can’t afford ignoring model error
−1.0
−0.5
0.0
0.5
1.0
x
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
y
Data, N = 20
Predictive mean
Predictive stdev
True function
0.5
0.6
0.7
0.8
0.9
1.0
1.1
λ1
1.3
1.4
1.5
1.6
1.7
1.8
λ2
N = 20
Model-data ﬁt
Posterior on parameters
Increasing number of data points decreases posterior and predictive
uncertainty
We are increasingly sure about predictions based on the wrong model
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
42 / 57

Motivation: can’t afford ignoring model error
−1.0
−0.5
0.0
0.5
1.0
x
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
y
Data, N = 50
Predictive mean
Predictive stdev
True function
0.5
0.6
0.7
0.8
0.9
1.0
1.1
λ1
1.3
1.4
1.5
1.6
1.7
1.8
λ2
N = 50
Model-data ﬁt
Posterior on parameters
Increasing number of data points decreases posterior and predictive
uncertainty
We are increasingly sure about predictions based on the wrong model
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
42 / 57

Motivation: can’t afford ignoring model error
−1.0
−0.5
0.0
0.5
1.0
x
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
y
Data, N = 50
Predictive mean
Predictive stdev
True function
−1.0
−0.5
0.0
0.5
1.0
x
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
y
Data, N = 50
Predictive mean
Predictive stdev
True function
Model-data ﬁt
What we want
If the model has structural uncertainty, more data leads to biased and
overconﬁdent results
We want to quantify model-vs-truth discrepancy in a rigorous and
systematic way
Cannot ignore model error
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
42 / 57

Model Error – Challenges with current methods
Total error budget
yi = f(xi; λ) + δ(xi)
|
{z
}
Truth g(xi)
+ϵi
−1.0
−0.5
0.0
0.5
1.0
x
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
y
Data, N = 50
Predictive mean
Predictive stdev
True function
Ignoring model error δ(x) leads to incorrect predictive errors
Conventional statistical modeling (Kennedy and O’Hagan, 2001)
makes it difﬁcult to disambiguate model/data errors
may violate physical constraints
not meaningful for prediction of other QoIs
Issue is highlighted in model-to-model calibration (ϵi = 0)
no a priori knowledge of the statistical structure of the discrepancy
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
43 / 57

Model Error – Key idea: probabilistic embedding
Cast input parameters λ as a random variable Λ
yi = f(xi; λ) + δ(xi) + ϵi −−−−−−−−−→
a
yi = f(xi; Λ) + ϵi
Embed model error in speciﬁc submodel phenomenology
a modiﬁed transport or constitutive law
a modiﬁed formulation for a material property
turbulent model constants
Allows placement of model error term in locations where key
modeling assumptions and approximations are made
as a correction or high-order term
as a possible alternate phenomenology
Naturally preserves model structure and physical constraints
Disambiguates model/data errors
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
44 / 57

Model Error – Bayesian density estimation
yi = f(xi; Λ) + ϵi
Parametrise embedded random variable Λ:
PDF form πΛ(·; α)
Polynomial Chaos (PC): Λ = P
k αkΨk(ξ)
Multivariate Normal (MVN):











Λ1 = α10 + α11ξ1
Λ2 = α20 + α21ξ1 + α22ξ2
...
Λd = αd0 + αd1ξ1 + αd2ξ2 + · · · + αddξd
Inverse modeling context
Parameter estimation of λ ⇒PDF estimation of Λ ⇒
⇒parameter estimation of α
Bayesian formulation
p(α|y)
| {z }
Posterior
∝Ly(α)
| {z }
Likelihood
p(α)
|{z}
Prior
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
45 / 57

Model Error – Likelihood options
K. Sargsyan, H. Najm, and R. Ghanem, “On the Statistical Calibration of Physical Models”.
International Journal for Chemical Kinetics, 47(4): pp 246–276, 2015.
α
Prior p(α)
Prob. model for Λ
Λ
πΛ(·; α)
f(xi; Λ)
Model
Likelihood
p(y|α)
yi = f(xi; Λ) + ϵi
Data
Posterior p(α|y)
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
46 / 57

Model Error – Likelihood options
K. Sargsyan, H. Najm, and R. Ghanem, “On the Statistical Calibration of Physical Models”.
International Journal for Chemical Kinetics, 47(4): pp 246–276, 2015.
α
Prior p(α)
Prob. model for Λ
Λ
πΛ(·; α)
f(xi; Λ)
Model
Likelihood
p(y|α)
yi = f(xi; Λ) + ϵi
Data
Posterior p(α|y)
Full Likelihood: L(α) = p(y|α) = p(y1, . . . , yN|α) = π(y)
Degenerate if no data noise
Requires multivariate KDE or high-d integration
Gaussian approximation:
L(α) ∝exp
 −1
2(y −µ(α))T Σ−1(α)(y −µ(α))

Non-intrusive spectral projection with Polynomial Chaos relieves
the expense and provides easy access to mean µ(α) and
covariance Σ(α)
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
46 / 57

Model Error – Likelihood options
K. Sargsyan, H. Najm, and R. Ghanem, “On the Statistical Calibration of Physical Models”.
International Journal for Chemical Kinetics, 47(4): pp 246–276, 2015.
α
Prior p(α)
Prob. model for Λ
Λ
πΛ(·; α)
f(xi; Λ)
Model
Likelihood
p(y|α)
yi = f(xi; Λ) + ϵi
Data
Posterior p(α|y)
Marginalized Likelihood:
L(α) = p(y|α) ≈QN
i=1 p(yi|α) = QN
i=1 π(yi)
Requires univariate KDE
Neglects built-in correlations
Gaussian approximation:
L(α) ∝exp

−1
2
PN
i=1 Σ−1
ii (α)(yi −µi(α))2
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
46 / 57

Model Error – Likelihood options
K. Sargsyan, H. Najm, and R. Ghanem, “On the Statistical Calibration of Physical Models”.
International Journal for Chemical Kinetics, 47(4): pp 246–276, 2015.
α
Prior p(α)
Prob. model for Λ
Λ
πΛ(·; α)
f(xi; Λ)
Model
Likelihood
p(y|α)
yi = f(xi; Λ) + ϵi
Data
Posterior p(α|y)
Approximate Bayesian Computation (ABC):
L(α) = 1
ϵK

ρ(SM,SD)
ϵ

Mean of f(xi; Λ) is “centered” on the data
The width of the distribution of f(xi; Λ) is consistent with the spread of the
data around the nominal model prediction
L(α) ∝exp
 
−1
2ϵ2
N
X
i=1
h
(µi(α) −yi)2 + (
p
Σii(α) −γ|µi(α) −yi|)2i!
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
46 / 57

Model Error – Predictions
f(x; Λ) = f (x; P
k αkΨk(ξ)) = P
k fk(x; α)Ψk(ξ)
Non-intrusive spectral projection (NISP) will be employed for
Likelihood computation
Posterior/pushed-forward predictions
Easy access to ﬁrst two moments:
µ(x; α) = f0(x; α),
σ2(x; α) =
X
k>0
f 2
k(x; α)||Ψk||2
Predictive mean
E[y(x) = Eα[µ(x; α)]
Decomposition of predictive variance
V[y(x)] = Eα[σ2(x; α)]
|
{z
}
Model error
+ Vα[µ(x; α)] + σ2
d
|
{z
}
Poserior/Data error
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
47 / 57

Predictions account for model error
Calibrating single-exponential models
with data from a double exponential model g(x) = e−0.5x + e−2x
Linear-exponential f(x, λ) = eλ1+λ2x
0
1
2
3
4
5
x
10-1
100
Data from truth
Predictive mean
Predictive stdev
Additive Gaussian error
0
1
2
3
4
5
x
10-1
100
Data from truth
Predictive mean
Predictive stdev
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
48 / 57

Predictions account for model error
Calibrating single-exponential models
with data from a double exponential model g(x) = e−0.5x + e−2x
Linear-exponential f(x, λ) = eλ1+λ2x
0
1
2
3
4
5
x
10-1
100
Data from truth
Predictive mean
Predictive stdev
Quadratic-exponential f2(x, λ) = eλ1+λ2x+λ3x2
0
1
2
3
4
5
x
10-1
100
Data from truth
Predictive mean
Predictive stdev
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
48 / 57

More data leads to ‘leftover’ model error
Calibrating a quadratic f(x) = λ0 + λ1x + λ2x2
w.r.t. ‘truth’ g(x) = 6 + x2 −0.5(x + 1)3.5 measured with noise σ = 0.1.
N = 20
N = 50
N = 1000
−1.0
−0.5
0.0
0.5
1.0
x
1
2
3
4
5
6
7
8
y
Data from truth
Truth function
Predictive mean
Predictive stdev: data noise
Predictive stdev: model error
−1.0
−0.5
0.0
0.5
1.0
x
1
2
3
4
5
6
7
8
y
Data from truth
Truth function
Predictive mean
Predictive stdev: data noise
Predictive stdev: model error
−1.0
−0.5
0.0
0.5
1.0
x
1
2
3
4
5
6
7
8
y
Data from truth
Truth function
Predictive mean
Predictive stdev: data noise
Predictive stdev: model error
Summary of features:
Well-deﬁned model-to-model calibration
Model-driven discrepancy correlations
Respects physical constraints
Disambiguates model and data errors
Calibrated predictions of multiple QoIs
101
102
103
104
105
106
Number of Samples
10-4
10-3
10-2
10-1
100
101
Variance
line
quad
cube
Model error
Data noise
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
49 / 57

Chemistry problem – ABC
Homogeneous ignition, methane-air mixture
Single-step global reaction model calibrated against a detailed
chemical kinetic model
Data: ignition time; range of
initial T & equivalence ratio
Single-step model:
CH4 + 2O2 →CO2 + 2H2O
R
=
[CH4][O2]kf
kf
=
A exp(−E/RoT)
(ln A, E) = P
k αkΨk(ξ)
Temp., T0
1000
1050
1100
1150
1200
1250
1300
Eq. ratio, Φ
0.6
0.8
1.0
1.2
1.4
1.6
Log (Ignition time), lnτ
−4
−3
−2
−1
0
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
50 / 57

Quality of Uncertain Calibrated Model Predictions
Calibrated uncertain ﬁt model
is consistent with the
detailed-model data.
Over the range of (T 0, Φ):
MAP predictive mean
ignition-time is centered
on the data
MAP predictive stdv
is consistent with the
scatter of the data
K. Sargsyan, H.N. Najm, and R. Ghanem
”On the Statistical Calibration of Physical Models”
Int. J. Chem. Kin., 47(4): 246-276, 2015
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
51 / 57

TransCom3 Experiment of CO2 Flux Inversion
[Gurney et al., Tellus B, 2003]
• Observations d at N = 77 sites around the world
• Inverse problem: ﬁnd ﬂuxes s at M = 22 locations
• Linearized ‘response’ model R, such that d ≈Rs
d = Rs + ϵd
• Model R is never perfect thus contaminating the inversion
• The inferred values of s compensate for model deﬁciencies
• ϵd is meant to capture data errors, but is ‘entangled’ with
model errors
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
52 / 57

Consider 14 different response models R
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
CSU.gurney
1.6
2.4
3.2
4.0
4.8
5.6
6.4
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
GCTM.baker
1.6
2.4
3.2
4.0
4.8
5.6
6.4
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
GISS.fung
1.6
2.4
3.2
4.0
4.8
5.6
6.4
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
GISS.prather
1.6
2.4
3.2
4.0
4.8
5.6
6.4
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
GISS.prather2
1.6
2.4
3.2
4.0
4.8
5.6
6.4
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
GISS.prather3
1.6
2.4
3.2
4.0
4.8
5.6
6.4
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
JMA-CDTM.maki
1.6
2.4
3.2
4.0
4.8
5.6
6.4
Infer ﬂuxes s, given measurements d to satisfy d ≈Rs
• Conventional additive Gaussian error (least-squares):
d = Rs + ξ
• Embed probabilistic model for ﬂuxes s:
d = R(µs + Csξ)
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
53 / 57

Consider 14 different response models R
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
MATCH.bruhwiler
1.6
2.4
3.2
4.0
4.8
5.6
6.4
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
MATCH.chen
1.6
2.4
3.2
4.0
4.8
5.6
6.4
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
MATCH.law
1.6
2.4
3.2
4.0
4.8
5.6
6.4
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
NIES.maksyutov
1.6
2.4
3.2
4.0
4.8
5.6
6.4
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
NIRE.taguchi
1.6
2.4
3.2
4.0
4.8
5.6
6.4
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
RPN.yuen
1.6
2.4
3.2
4.0
4.8
5.6
6.4
0
5 10 15 20
Sources
0
10
20
30
40
50
60
70
Measurements
SKYHI.fan
1.6
2.4
3.2
4.0
4.8
5.6
6.4
Infer ﬂuxes s, given measurements d to satisfy d ≈Rs
• Conventional additive Gaussian error (least-squares):
d = Rs + ξ
• Embed probabilistic model for ﬂuxes s:
d = R(µs + Csξ)
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
53 / 57

Inferred ﬂuxes show less variability across models
Prior
CSU.gurney
GCTM.baker
GISS.fung
GISS.prather
GISS.prather2
GISS.prather3
JMA-CDTM.maki
MATCH.bruhwiler
MATCH.chen
MATCH.law
NIES.maksyutov
NIRE.taguchi
RPN.yuen
SKYHI.fan
−1.0
10.5
0.0
0.5
1.0
1.5
Flux
Region lnd01
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
54 / 57

Inferred ﬂuxes show less variability across models
Prior
CSU.gurney
GCTM.baker
GISS.fung
GISS.prather
GISS.prather2
GISS.prather3
JMA-CDTM.maki
MATCH.bruhwiler
MATCH.chen
MATCH.law
NIES.maksyutov
NIRE.taguchi
RPN.yuen
SKYHI.fan
−6
−4
−2
0
2
4
6
Flux
Region lnd09
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
54 / 57

Inferred ﬂuxes show less variability across models
Prior
CSU.gurney
GCTM.baker
GISS.fung
GISS.prather
GISS.prather2
GISS.prather3
JMA-CDTM.maki
MATCH.bruhwiler
MATCH.chen
MATCH.law
NIES.maksyutov
NIRE.taguchi
RPN.yuen
SKYHI.fan
−2.0
−1.5
11.0
10.5
0.0
0.5
1.0
1.5
2.0
Flux
Region lnd11
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
54 / 57

Inferred ﬂuxes show less variability across models
Prior
CSU.gurney
GCTM.baker
GISS.fung
GISS.prather
GISS.prather2
GISS.prather3
JMA-CDTM.maki
MATCH.bruhwiler
MATCH.chen
MATCH.law
NIES.maksyutov
NIRE.taguchi
RPN.yuen
SKYHI.fan
−1.0
00.5
0.0
0.5
1.0
Flu.
Region ocn01
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
54 / 57

Inferred ﬂuxes show less variability across models
Prior
CSU.gurney
GCTM.baker
GISS.fung
GISS.prather
GISS.prather2
GISS.prather3
JMA-CDTM.maki
MATCH.bruhwiler
MATCH.chen
MATCH.law
NIES.maksyutov
NIRE.taguchi
RPN.yuen
SKYHI.fan
−2.5
−2.0
−1.5
11.0
10.5
0.0
0.5
1.0
1.5
Flux
Region ocn04
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
54 / 57

Inferred ﬂuxes show less variability across models
Prior
CSU.gurney
GCTM.baker
GISS.fung
GISS.prather
GISS.prather2
GISS.prather3
JMA-CDTM.maki
MATCH.bruhwiler
MATCH.chen
MATCH.law
NIES.maksyutov
NIRE.taguchi
RPN.yuen
SKYHI.fan
−1.5
11.0
10.5
0.0
0.5
1.0
1.5
2.0
Flux
Region ocn09
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
54 / 57

Inferred ﬂuxes show less variability across models
Prior
CSU.gurney
GCTM.baker
GISS.fung
GISS.prather
GISS.prather2
GISS.prather3
JMA-CDTM.maki
MATCH.bruhwiler
MATCH.chen
MATCH.law
NIES.maksyutov
NIRE.taguchi
RPN.yuen
SKYHI.fan
−1.0
00.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Flu.
Region ocn10
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
54 / 57

Model error in LES: embed model err in Smagorinsky coefﬁcient
Calibrate with TKE data, predict both TKE and Pressure
Pushed forward posterior
TKE
−5
−4
−3
−2
−1
0
y-position
0.000
0.001
0.002
0.003
0.004
0.005
0.006
TKE
Data from high-fid model
Pred. mean of lo -fid model
Pred. st.dev. due to posterior
No model error
−5
−4
−3
−2
−1
0
y-position
0.000
0.001
0.002
0.003
0.004
0.005
0.006
0.007
TKE
Data from high-fid model
Pred. mean of low-fid model
Pred. st.de . due to model error
Pred. st.de . due to posterior
With model error
Pressure
−5
−4
−3
−2
−1
0
y-position
−0.038
−0.036
−0.034
−0.032
−0.030
−0.028
−0.026
−0.024
Pressure
Data from high-fid model
Pred. mean of low-fid model
Pred. st.dev. due to posterior
No model error
−5
−4
−3
−2
−1
0
y-position
−0.038
−0.036
−0.034
−0.032
−0.030
−0.028
−0.026
−0.024
−0.022
Pressure
Data from high-fid model
Pred. mean of low-fid model
Pred. st.dev. due to model error
Pred. st.dev. due to posterior
With model error
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
55 / 57

Summary
Forward UQ: Polynomial Chaos representation of RVs
• Non-intrusive spectral projection
• Surrogate construction, Bayesian regression
• High-D challenge: sparse PC via Bayesian
compressive sensing
• Intrusive spectral projection
• Time/space-resolved processes (Karhunen-Loeve
expansions)
• Non-polynomial regression (Radial Basis Functions,
Gaussian Processes)
• Rosenblatt transform, Kernel Density Estimation
• Domain decomposition, multiwavelets
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
56 / 57

Summary
Inverse UQ: Bayesian inference for parameter estimation
• Bayesian parameter estimation
• Model error quantiﬁcation: embedded model error
approach
• Markov chain Monte Carlo (MCMC) details
• Model plausibility theory: evidence, model selection,
Bayes factors
• MaxEnt methods, data-free inference
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
56 / 57

Literature
Thank you!
General PC
N. Wiener, “Homogeneous Chaos”, American Journal of Mathematics, 60(4), 897-936, (1938).
Ghanem, R., Spanos, P., “Stochastic Finite Elements: A Spectral Approach”, Springer Verlag, (1991).
Xiu, D., Karniadakis, G., “The Wiener-Askey Polynomial Chaos for Stocahstic Differential Equations”, SIAM J. Sci. Comp., 24(2),
619-644, (2002).
Le Maˆıtre, O., Knio, O., “Spectral Methods for Uncertainty Quantiﬁcation: With Applications to Computational Fluid Dynamics”,
Springer-Verlag, (2010).
Najm, H., “Uncertainty Quantiﬁcation and Polynomial Chaos Techniques in Computational Fluid Dynamics”, Ann. Rev. Fluid
Mech., 41(1):35-52, (2009).
Xiu, D., “Numerical Methods for Stochastic Computations: A Spectral Method Approach”, Princeton U. Press (2010).
Debusschere, B., Najm, H., P´ebay, P., Knio, O., Ghanem, R., Le Maˆıtre, O., “Numerical Challenges in the Use of Polynomial
Chaos Representations for Stochastic Processes”, SIAM J. Sci. Comp., 26(2):698-719, (2004).
Marzouk, Y., Najm, H., “Dimensionality Reduction and Polynomial Chaos Acceleration of Bayesian Inference in Inverse
Problems”, J. Comp. Phys., 228(6):1862-1902, (2009).
Bayesian compressive sensing
S. Ji, Y. Xue and L. Carin, “Bayesian Compressive Sensing”, IEEE Trans. Signal Proc., 56(6), (2008).
K. Sargsyan, C. Safta, H. Najm, B. Debusschere, D. Ricciuto, P. Thornton, “Dimensionality reduction for complex models via
Bayesian compressive sensing”, Int. J. Uncertainty Quantiﬁcation, 4(1), 63-93, (2014).
Model error
M. Kennedy, M. and A. O’Hagan, “Bayesian calibration of computer models”, Journal of the Royal Statistical Society, Series B. 63,
425-464, (2001).
K. Sargsyan, H. N. Najm, R. Ghanem, “On the Statistical Calibration of Physical Models”, Int. J. Chem. Kinetics, 47(4), 246-276,
(2015).
K. Sargsyan (ksargsy@sandia.gov)
SLB Webinar
April 19, 2016
57 / 57

Additional Material
(Core Dump)

Probabilistic Forward UQ & Polynomial Chaos
Representation of Random Variables
With y = f(x), x a random variable, estimate the RV y
Can describe a RV in terms of its
density, moments, characteristic function, or
as a function on a probability space
Constraining the analysis to RVs with ﬁnite variance
⇒Represent RV as a spectral expansion in terms of orthogonal
functions of standard RVs
– Polynomial Chaos Expansion
Enables the use of available functional analysis methods for
forward UQ

Sensitivity indices are directly computable from PC
g(ξ) =
P
X
k=0
ckΨk(ξ)
Consider dimensionality d = 3, total order p = 2,
number of PC terms P + 1 = (d + p)!/(d!p!) = 10.
g(ξ1, ξ2, ξ3) = c0 + c1ψ1(ξ1) + c2ψ1(ξ2) + c3ψ1(ξ3) +
+ c4ψ2(ξ1) + c5ψ1(ξ1)ψ1(ξ2) + c6ψ1(ξ1)ψ1(ξ3) + c7ψ2(ξ2) + c8ψ1(ξ2)ψ1(ξ3) + c9ψ2(ξ3)
Variance contributions
V ar(g) = 0 + c2
1⟨ψ2
1⟩+ c2
2⟨ψ2
1⟩+ c2
3⟨ψ2
1⟩+
+ c2
4⟨ψ2
2⟩+ c2
5⟨ψ2
1⟩⟨ψ2
1⟩+ c2
6⟨ψ2
1⟩⟨ψ2
1⟩+ c2
7⟨ψ2
2⟩+ c2
8⟨ψ2
1⟩⟨ψ2
1⟩+ c2
9⟨ψ2
2⟩

Sensitivity indices are directly computable from PC
g(ξ) =
P
X
k=0
ckΨk(ξ)
Consider dimensionality d = 3, total order p = 2,
number of PC terms P + 1 = (d + p)!/(d!p!) = 10.
g(ξ1, ξ2, ξ3) = c0 + c1ψ1(ξ1) + c2ψ1(ξ2) + c3ψ1(ξ3) +
+ c4ψ2(ξ1) + c5ψ1(ξ1)ψ1(ξ2) + c6ψ1(ξ1)ψ1(ξ3) + c7ψ2(ξ2) + c8ψ1(ξ2)ψ1(ξ3) + c9ψ2(ξ3)
Variance contributions
V ar(g) = 0 + c2
1⟨ψ2
1⟩+ c2
2⟨ψ2
1⟩+ c2
3⟨ψ2
1⟩+
+ c2
4⟨ψ2
2⟩+ c2
5⟨ψ2
1⟩⟨ψ2
1⟩+ c2
6⟨ψ2
1⟩⟨ψ2
1⟩+ c2
7⟨ψ2
2⟩+ c2
8⟨ψ2
1⟩⟨ψ2
1⟩+ c2
9⟨ψ2
2⟩
Main effect sensitivities ξ1
ξ2
ξ3

Sensitivity indices are directly computable from PC
g(ξ) =
P
X
k=0
ckΨk(ξ)
Consider dimensionality d = 3, total order p = 2,
number of PC terms P + 1 = (d + p)!/(d!p!) = 10.
g(ξ1, ξ2, ξ3) = c0 + c1ψ1(ξ1) + c2ψ1(ξ2) + c3ψ1(ξ3) +
+ c4ψ2(ξ1) + c5ψ1(ξ1)ψ1(ξ2) + c6ψ1(ξ1)ψ1(ξ3) + c7ψ2(ξ2) + c8ψ1(ξ2)ψ1(ξ3) + c9ψ2(ξ3)
Variance contributions
V ar(g) = 0 + c2
1⟨ψ2
1⟩+ c2
2⟨ψ2
1⟩+ c2
3⟨ψ2
1⟩+
+ c2
4⟨ψ2
2⟩+ c2
5⟨ψ2
1⟩⟨ψ2
1⟩+ c2
6⟨ψ2
1⟩⟨ψ2
1⟩+ c2
7⟨ψ2
2⟩+ c2
8⟨ψ2
1⟩⟨ψ2
1⟩+ c2
9⟨ψ2
2⟩
Main effect sensitivities ξ1
ξ2
ξ3

Sensitivity indices are directly computable from PC
g(ξ) =
P
X
k=0
ckΨk(ξ)
Consider dimensionality d = 3, total order p = 2,
number of PC terms P + 1 = (d + p)!/(d!p!) = 10.
g(ξ1, ξ2, ξ3) = c0 + c1ψ1(ξ1) + c2ψ1(ξ2) + c3ψ1(ξ3) +
+ c4ψ2(ξ1) + c5ψ1(ξ1)ψ1(ξ2) + c6ψ1(ξ1)ψ1(ξ3) + c7ψ2(ξ2) + c8ψ1(ξ2)ψ1(ξ3) + c9ψ2(ξ3)
Variance contributions
V ar(g) = 0 + c2
1⟨ψ2
1⟩+ c2
2⟨ψ2
1⟩+ c2
3⟨ψ2
1⟩+
+ c2
4⟨ψ2
2⟩+ c2
5⟨ψ2
1⟩⟨ψ2
1⟩+ c2
6⟨ψ2
1⟩⟨ψ2
1⟩+ c2
7⟨ψ2
2⟩+ c2
8⟨ψ2
1⟩⟨ψ2
1⟩+ c2
9⟨ψ2
2⟩
Main effect sensitivities ξ1
ξ2
ξ3

Sensitivity indices are directly computable from PC
g(ξ) =
P
X
k=0
ckΨk(ξ)
Consider dimensionality d = 3, total order p = 2,
number of PC terms P + 1 = (d + p)!/(d!p!) = 10.
g(ξ1, ξ2, ξ3) = c0 + c1ψ1(ξ1) + c2ψ1(ξ2) + c3ψ1(ξ3) +
+ c4ψ2(ξ1) + c5ψ1(ξ1)ψ1(ξ2) + c6ψ1(ξ1)ψ1(ξ3) + c7ψ2(ξ2) + c8ψ1(ξ2)ψ1(ξ3) + c9ψ2(ξ3)
Variance contributions
V ar(g) = 0 + c2
1⟨ψ2
1⟩+ c2
2⟨ψ2
1⟩+ c2
3⟨ψ2
1⟩+
+ c2
4⟨ψ2
2⟩+ c2
5⟨ψ2
1⟩⟨ψ2
1⟩+ c2
6⟨ψ2
1⟩⟨ψ2
1⟩+ c2
7⟨ψ2
2⟩+ c2
8⟨ψ2
1⟩⟨ψ2
1⟩+ c2
9⟨ψ2
2⟩
Total sensitivities ξ1
ξ2
ξ3

Sensitivity indices are directly computable from PC
g(ξ) =
P
X
k=0
ckΨk(ξ)
Consider dimensionality d = 3, total order p = 2,
number of PC terms P + 1 = (d + p)!/(d!p!) = 10.
g(ξ1, ξ2, ξ3) = c0 + c1ψ1(ξ1) + c2ψ1(ξ2) + c3ψ1(ξ3) +
+ c4ψ2(ξ1) + c5ψ1(ξ1)ψ1(ξ2) + c6ψ1(ξ1)ψ1(ξ3) + c7ψ2(ξ2) + c8ψ1(ξ2)ψ1(ξ3) + c9ψ2(ξ3)
Variance contributions
V ar(g) = 0 + c2
1⟨ψ2
1⟩+ c2
2⟨ψ2
1⟩+ c2
3⟨ψ2
1⟩+
+ c2
4⟨ψ2
2⟩+ c2
5⟨ψ2
1⟩⟨ψ2
1⟩+ c2
6⟨ψ2
1⟩⟨ψ2
1⟩+ c2
7⟨ψ2
2⟩+ c2
8⟨ψ2
1⟩⟨ψ2
1⟩+ c2
9⟨ψ2
2⟩
Total sensitivities ξ1
ξ2
ξ3

Sensitivity indices are directly computable from PC
g(ξ) =
P
X
k=0
ckΨk(ξ)
Consider dimensionality d = 3, total order p = 2,
number of PC terms P + 1 = (d + p)!/(d!p!) = 10.
g(ξ1, ξ2, ξ3) = c0 + c1ψ1(ξ1) + c2ψ1(ξ2) + c3ψ1(ξ3) +
+ c4ψ2(ξ1) + c5ψ1(ξ1)ψ1(ξ2) + c6ψ1(ξ1)ψ1(ξ3) + c7ψ2(ξ2) + c8ψ1(ξ2)ψ1(ξ3) + c9ψ2(ξ3)
Variance contributions
V ar(g) = 0 + c2
1⟨ψ2
1⟩+ c2
2⟨ψ2
1⟩+ c2
3⟨ψ2
1⟩+
+ c2
4⟨ψ2
2⟩+ c2
5⟨ψ2
1⟩⟨ψ2
1⟩+ c2
6⟨ψ2
1⟩⟨ψ2
1⟩+ c2
7⟨ψ2
2⟩+ c2
8⟨ψ2
1⟩⟨ψ2
1⟩+ c2
9⟨ψ2
2⟩
Total sensitivities ξ1
ξ2
ξ3

Sensitivity indices are directly computable from PC
g(ξ) =
P
X
k=0
ckΨk(ξ)
Consider dimensionality d = 3, total order p = 2,
number of PC terms P + 1 = (d + p)!/(d!p!) = 10.
g(ξ1, ξ2, ξ3) = c0 + c1ψ1(ξ1) + c2ψ1(ξ2) + c3ψ1(ξ3) +
+ c4ψ2(ξ1) + c5ψ1(ξ1)ψ1(ξ2) + c6ψ1(ξ1)ψ1(ξ3) + c7ψ2(ξ2) + c8ψ1(ξ2)ψ1(ξ3) + c9ψ2(ξ3)
Variance contributions
V ar(g) = 0 + c2
1⟨ψ2
1⟩+ c2
2⟨ψ2
1⟩+ c2
3⟨ψ2
1⟩+
+ c2
4⟨ψ2
2⟩+ c2
5⟨ψ2
1⟩⟨ψ2
1⟩+ c2
6⟨ψ2
1⟩⟨ψ2
1⟩+ c2
7⟨ψ2
2⟩+ c2
8⟨ψ2
1⟩⟨ψ2
1⟩+ c2
9⟨ψ2
2⟩
Joint sensitivities (ξ1, ξ2)
(ξ1, ξ3)
(ξ2, ξ3)

Sensitivity indices are directly computable from PC
g(ξ) =
P
X
k=0
ckΨk(ξ)
Consider dimensionality d = 3, total order p = 2,
number of PC terms P + 1 = (d + p)!/(d!p!) = 10.
g(ξ1, ξ2, ξ3) = c0 + c1ψ1(ξ1) + c2ψ1(ξ2) + c3ψ1(ξ3) +
+ c4ψ2(ξ1) + c5ψ1(ξ1)ψ1(ξ2) + c6ψ1(ξ1)ψ1(ξ3) + c7ψ2(ξ2) + c8ψ1(ξ2)ψ1(ξ3) + c9ψ2(ξ3)
Variance contributions
V ar(g) = 0 + c2
1⟨ψ2
1⟩+ c2
2⟨ψ2
1⟩+ c2
3⟨ψ2
1⟩+
+ c2
4⟨ψ2
2⟩+ c2
5⟨ψ2
1⟩⟨ψ2
1⟩+ c2
6⟨ψ2
1⟩⟨ψ2
1⟩+ c2
7⟨ψ2
2⟩+ c2
8⟨ψ2
1⟩⟨ψ2
1⟩+ c2
9⟨ψ2
2⟩
Joint sensitivities (ξ1, ξ2)
(ξ1, ξ3)
(ξ2, ξ3)

Sensitivity indices are directly computable from PC
g(ξ) =
P
X
k=0
ckΨk(ξ)
Consider dimensionality d = 3, total order p = 2,
number of PC terms P + 1 = (d + p)!/(d!p!) = 10.
g(ξ1, ξ2, ξ3) = c0 + c1ψ1(ξ1) + c2ψ1(ξ2) + c3ψ1(ξ3) +
+ c4ψ2(ξ1) + c5ψ1(ξ1)ψ1(ξ2) + c6ψ1(ξ1)ψ1(ξ3) + c7ψ2(ξ2) + c8ψ1(ξ2)ψ1(ξ3) + c9ψ2(ξ3)
Variance contributions
V ar(g) = 0 + c2
1⟨ψ2
1⟩+ c2
2⟨ψ2
1⟩+ c2
3⟨ψ2
1⟩+
+ c2
4⟨ψ2
2⟩+ c2
5⟨ψ2
1⟩⟨ψ2
1⟩+ c2
6⟨ψ2
1⟩⟨ψ2
1⟩+ c2
7⟨ψ2
2⟩+ c2
8⟨ψ2
1⟩⟨ψ2
1⟩+ c2
9⟨ψ2
2⟩
Joint sensitivities (ξ1, ξ2)
(ξ1, ξ3)
(ξ2, ξ3)

Other non-intrusive methods (stochastic collocation)
Interpolation: Fit interpolant to samples
Oscillation concern in multi-D
Regression: Estimate best-ﬁt response surface
Least-squares
Sparsity via ℓ1 constraints; compressive sensing
Bayesian inference
Sparsity via Laplace priors; Bayesian compressive sensing
Useful when quadrature methods are infeasible, e.g.:
– Samples given a priori
– Can’t choose sample locations
– Can’t take enough samples
– Forward model is noisy

PCE Construction for Noisy Functions
Quadrature formulae presume a degree of smoothness
– No convergence for a noisy function
uk =
1

Ψ2
k

Z
u(λ(ξ)) Ψk(ξ)pξ(ξ)dξ,
k = 0, . . . , P
Sparse-Quadrature formulae are ill-conditioned and
highly-sensitive to noise
– No convergence with order
– Error grows with increased dimensionality
Options in the presence of noise:
RMS ﬁtting for PC coefﬁcients
Bayesian inference of PC coefﬁcients

PC and High-Dimensionality
Dimensionality n of the PC basis: ξ = {ξ1, . . . , ξn}
n ≈number of uncertain parameters
P + 1 = (n + p)!/n!p!
grows fast with n
Impacts:
Size of intrusive PC system
Hi-D projection integrals ⇒large # non-intrusive samples
Sparse quadrature methods
Clenshaw-Curtis sparse grid, Level = 3
Clenshaw-Curtis sparse grid, Level = 5

PC coefﬁcients via sparse regression
PCE:
y = f(x) ≃
K−1
X
k=0
ckΨk(x)
with x ∈Rn, Ψk max order p, and K = (p + n)!/p!/n!
N samples (x1, y1), . . . , (xN, yN)
Estimate K terms c0, . . . , cK−1, s.t.
min ||y −Ac||2
2
where y ∈RN, c ∈RK, Aik = Ψk(xi), A ∈RN×K
With N << K ⇒under-determined
Need some form of regularization

Regularization – Compressive Sensing (CS)
ℓ2-norm — Tikhonov regularization; Ridge regression:
min {∥y −Ac∥2
2 + ∥c∥2
2}
ℓ1-norm — Compressive Sensing; LASSO; basis pursuit
min {∥y −Ac∥2
2 + ∥c∥1}
min {∥y −Ac∥2
2}
subject to ∥c∥1 ≤ϵ
min {∥c∥1}
subject to ∥y −Ac∥2
2 ≤ϵ
⇒discovery of sparse signals

Bayesian Regression
Bayes formula
p(c|D) ∝p(D|c)π(c)
Bayesian regression: prior as a regularizer, e.g.
Log Likelihood ⇔∥y −Ac∥2
2
Log Prior ⇔∥c∥p
p
Laplace sparsity priors π(ck|α) =
1
2αe−|ck|/α
LASSO (Tibshirani 1996) ... formally:
min {∥y −Ac∥2
2 + λ∥c∥1}
Solution ∼the posterior mode of c in the Bayesian model
y ∼N(Ac, IN),
ck ∼1
2αe−|ck|/α
Bayesian LASSO (Park & Casella 2008)

Bayesian Compressive Sensing (BCS)
BCS (Ji 2008; Babacan 2010)— hierarchical priors:
Gaussian priors N(0, σ2
k) on the ck
Gamma priors on the σ2
k
⇒Laplace sparsity priors on the ck
Evidence maximization establishes ML estimates of the σk
many of which are found ≈0
⇒
ck ≈0
iteratively include terms that lead to the largest increase in the
evidence
iterative BCS (iBCS) (Sargsyan 2012):
adaptive iterative order growth
BCS on order-p Legendre-Uniform PC
repeat with order-p + 1 terms added to surviving p-th order terms

Random Fields
A random variable is a function on an event space Ω
No dependence on other coordinates –e.g. space or time
A random ﬁeld is a function on a product space Ω× D
e.g. sea surface temperature TSS(z, ω), z ≡(x, t)
It is a more complex object than a random variable
A combination of an inﬁnite number of random variables
In many physical systems, uncertain ﬁeld quantities, described by
random ﬁelds:
are smooth, i.e.
they have an underlying low dimensional structure
due to large correlation length-scales

Random Fields – KLE
Smooth random ﬁelds can be represented with a small no. of
stochastic degrees of freedom
A random ﬁeld M(x, ω) with
– a mean function: µ(x)
– a continuous covariance function:
C(x1, x2) = ⟨[M(x1, ω) −µ(x1)][M(x2, ω) −µ(x2)]⟩
can be represented with the Karhunen-Loeve Expansion (KLE)
M(x, ω) = µ(x) +
∞
X
i=1
p
λiηi(ω)φi(x)
where
λi and φi(x) are the eigenvalues and eigenfunctions of the
covariance function C(·, ·)
ηi are uncorrelated zero-mean unit-variance RVs
KLE ⇒representation of random ﬁelds using PC

Intrusive PC UQ: A direct non-sampling method
Given model equations:
M(u(x, t); λ) = 0
Express uncertain parameters/variables using PCEs
u =
P
X
k=0
ukΨk;
λ =
P
X
k=0
λkΨk
Substitute in model equations; apply Galerkin projection
New set of equations:
G(U(x, t), Λ) = 0
– with U = [u0, . . . , uP ]T , Λ = [λ0, . . . , λP ]T
Solving this deterministic system once provides the full
speciﬁcation of uncertain model ouputs

Intrusive Galerkin PC ODE System
du
dt = f(u; λ)
λ =
P
X
i=0
λiΨi
u(t) =
P
X
i=0
ui(t)Ψi
dui
dt = ⟨f(u; λ)Ψi⟩

Ψ2
i

i = 0, . . . , P
Say f(u; λ) = λu, then
dui
dt
=
P
X
p=0
P
X
q=0
λpuqCpqi,
i = 0, · · · , P
where the tensor Cpqi = ⟨ΨpΨqΨi⟩/⟨Ψ2
i ⟩is readily evaluated

Intrusive PC UQ Pros/Cons
Cons:
Reformulation of governing equations
New discretizations
New numerical solution method
– Consistency, Convergence, Stability
– Global vs. multi-element local PC constructions
New solvers and model codes
– Opportunities for automated code transformation
New preconditioners
Pros:
Tailored solvers can deliver superior performance

Model Evidence and Complexity
Let M = {M1, M2, . . .} be a set of models of interest
Parameter estimation from data is conditioned on the model
p(θ|D, Mk) = p(D|θ, Mk)π(θ|Mk)
p(D|Mk)
Evidence (marginal likelihood) for Mk:
p(D|Mk) =
Z
p(D|θ, Mk)π(θ|Mk)dθ
Model evidence is useful for model selection
Choose model with maximum evidence
Compromise between ﬁtting data and model complexity
Optimal complexity – Occam’s razor principle
Avoid overﬁtting

Too much model complexity leads to overﬁtting
Data model:
i = 1, . . . , N
yi
=
x3
i + x2
i −6 + ϵi
ϵi
∼
N(0, s)
Bayesian regression with Legendre
PCE ﬁt models, order 1-10
ym =
P
X
k=0
ckψk(x)
Uniform priors π(ck), k = 0, . . . , P
Order = 1
−1.0
−0.5
0.0
0.5
1.0
−6.5
−6.0
−5.5
−5.0
−4.5
−4.0
−3.5
Fitted model
Noisy data
True function
Fitted model pushed-forward
posterior versus the data

Too much model complexity leads to overﬁtting
Data model:
i = 1, . . . , N
yi
=
x3
i + x2
i −6 + ϵi
ϵi
∼
N(0, s)
Bayesian regression with Legendre
PCE ﬁt models, order 1-10
ym =
P
X
k=0
ckψk(x)
Uniform priors π(ck), k = 0, . . . , P
Order = 2
−1.0
−0.5
0.0
0.5
1.0
−6.5
−6.0
−5.5
−5.0
−4.5
−4.0
−3.5
Fitted model
Noisy data
True function
Fitted model pushed-forward
posterior versus the data

Too much model complexity leads to overﬁtting
Data model:
i = 1, . . . , N
yi
=
x3
i + x2
i −6 + ϵi
ϵi
∼
N(0, s)
Bayesian regression with Legendre
PCE ﬁt models, order 1-10
ym =
P
X
k=0
ckψk(x)
Uniform priors π(ck), k = 0, . . . , P
Order = 3
−1.0
−0.5
0.0
0.5
1.0
−6.5
−6.0
−5.5
−5.0
−4.5
−4.0
−3.5
Fitted model
Noisy data
True function
Fitted model pushed-forward
posterior versus the data

Too much model complexity leads to overﬁtting
Data model:
i = 1, . . . , N
yi
=
x3
i + x2
i −6 + ϵi
ϵi
∼
N(0, s)
Bayesian regression with Legendre
PCE ﬁt models, order 1-10
ym =
P
X
k=0
ckψk(x)
Uniform priors π(ck), k = 0, . . . , P
Order = 4
−1.0
−0.5
0.0
0.5
1.0
−6.5
−6.0
−5.5
−5.0
−4.5
−4.0
−3.5
Fitted model
Noisy data
True function
Fitted model pushed-forward
posterior versus the data

Too much model complexity leads to overﬁtting
Data model:
i = 1, . . . , N
yi
=
x3
i + x2
i −6 + ϵi
ϵi
∼
N(0, s)
Bayesian regression with Legendre
PCE ﬁt models, order 1-10
ym =
P
X
k=0
ckψk(x)
Uniform priors π(ck), k = 0, . . . , P
Order = 5
−1.0
−0.5
0.0
0.5
1.0
−6.5
−6.0
−5.5
−5.0
−4.5
−4.0
−3.5
Fitted model
Noisy data
True function
Fitted model pushed-forward
posterior versus the data

Too much model complexity leads to overﬁtting
Data model:
i = 1, . . . , N
yi
=
x3
i + x2
i −6 + ϵi
ϵi
∼
N(0, s)
Bayesian regression with Legendre
PCE ﬁt models, order 1-10
ym =
P
X
k=0
ckψk(x)
Uniform priors π(ck), k = 0, . . . , P
Order = 6
−1.0
−0.5
0.0
0.5
1.0
−6.5
−6.0
−5.5
−5.0
−4.5
−4.0
−3.5
Fitted model
Noisy data
True function
Fitted model pushed-forward
posterior versus the data

Too much model complexity leads to overﬁtting
Data model:
i = 1, . . . , N
yi
=
x3
i + x2
i −6 + ϵi
ϵi
∼
N(0, s)
Bayesian regression with Legendre
PCE ﬁt models, order 1-10
ym =
P
X
k=0
ckψk(x)
Uniform priors π(ck), k = 0, . . . , P
Order = 7
−1.0
−0.5
0.0
0.5
1.0
−6.5
−6.0
−5.5
−5.0
−4.5
−4.0
−3.5
Fitted model
Noisy data
True function
Fitted model pushed-forward
posterior versus the data

Too much model complexity leads to overﬁtting
Data model:
i = 1, . . . , N
yi
=
x3
i + x2
i −6 + ϵi
ϵi
∼
N(0, s)
Bayesian regression with Legendre
PCE ﬁt models, order 1-10
ym =
P
X
k=0
ckψk(x)
Uniform priors π(ck), k = 0, . . . , P
Order = 8
−1.0
−0.5
0.0
0.5
1.0
−6.5
−6.0
−5.5
−5.0
−4.5
−4.0
−3.5
Fitted model
Noisy data
True function
Fitted model pushed-forward
posterior versus the data

Too much model complexity leads to overﬁtting
Data model:
i = 1, . . . , N
yi
=
x3
i + x2
i −6 + ϵi
ϵi
∼
N(0, s)
Bayesian regression with Legendre
PCE ﬁt models, order 1-10
ym =
P
X
k=0
ckψk(x)
Uniform priors π(ck), k = 0, . . . , P
Order = 9
−1.0
−0.5
0.0
0.5
1.0
−6.5
−6.0
−5.5
−5.0
−4.5
−4.0
−3.5
Fitted model
Noisy data
True function
Fitted model pushed-forward
posterior versus the data

Too much model complexity leads to overﬁtting
Data model:
i = 1, . . . , N
yi
=
x3
i + x2
i −6 + ϵi
ϵi
∼
N(0, s)
Bayesian regression with Legendre
PCE ﬁt models, order 1-10
ym =
P
X
k=0
ckψk(x)
Uniform priors π(ck), k = 0, . . . , P
Order = 10
−1.0
−0.5
0.0
0.5
1.0
−6.5
−6.0
−5.5
−5.0
−4.5
−4.0
−3.5
Fitted model
Noisy data
True function
Fitted model pushed-forward
posterior versus the data

Evidence and Cross-Validation Error
Model evidence peaks at the
true polynomial order of 3
Cross validation error is
equally minimal at order 3
Models with optimal
complexity are robust to cross
validation
1
2
3
4
5
6
7
8
9
10
Order
−100
−80
−60
−40
−20
0
20
Log (Score)
Fit
Comple ity
Evidence
Log evidence: sum of two
scores, balances complexity &
ﬁt

Evidence and Cross-Validation Error
Model evidence peaks at the
true polynomial order of 3
Cross validation error is
equally minimal at order 3
Models with optimal
complexity are robust to cross
validation
1
2
3
4
5
6
7
8
9
10
Order
−90
−80
−70
−60
−50
−40
−30
−20
−10
Log (Evidence)
−5.0
−4.5
−4.0
−3.5
−3.0
−2.5
Log (Validation error)
Cross validation error and
model evidence versus order

Challenges in PC UQ – High-Dimensionality
Dimensionality n of the PC basis: ξ = {ξ1, . . . , ξn}
– number of degrees of freedom
– P + 1 = (n + p)!/n!p! grows fast with n
Impacts:
– Size of intrusive system
– # non-intrusive (sparse) quadrature samples
Generally n ≈number of uncertain parameters
Reduction of n:
– Sensitivity analysis
– Dependencies/correlations among parameters
– Dominant eigenmodes of random ﬁelds
– Manifold learning: Isomap, Diffusion maps
– Sparsiﬁcation: Compressed Sensing, LASSO

High dimensionality challenge – Forward UQ
Consider a forward model
y = f(x)
Let x ∈Rn be uncertain, represented as a random vector,
x ∼p(x)
Estimate moments of y
Mq =
Z
[f(x)]qp(x)dx
Forward UQ is an integration problem.

Integration in High Dimensions
Monte Carlo (MC) methods
well suited for high-D integrals – convergence rate independent of
dimensionality
nonetheless they require large numbers of samples for good
accuracy
Quadrature
Tensor product quadrature is useless in hi-D
– Say m points in each of n dimensions: mn points
Adaptive sparse quadrature
– Much more feasible
– Can beat MC – dep. on smoothness of integrand
Greedy algorithms
Dimensionality reduction
Low rank and sparse representations
Global sensitivity analysis

High dimensionality challenge – Inverse UQ
Bayesian inference in a computational setting relies on Markov
Chain Monte Carlo (MCMC) methods
MCMC: A random walk algorithm for generation of samples from
the posterior density on model inputs
Moments are evaluated from the random samples
Need many random sample evaluations of forward model
– Employ model surrogates built via forward UQ
– Adaptive local surrogates
High dimensionality can lead to poor performance
– local maxima
– many directions uninformed by data
– choice of proposal density
– Dimension-Adaptive Likelihood-Informed MCMC

Bayesian inference – High Dimensionality Challenge
Judgement on local/global posterior peaks is difﬁcult
Multiple chains; Tempering
Choosing a good starting point is very important
An initial optimization strategy is useful, albeit not trivial
Choosing good MCMC proposals, and attaining good mixing
Likelihood-informed
– Markov jump in those dimensions informed by data
– Sample from prior in complement of dimensions
– Adaptive proposal learning from MCMC samples
– Log-Posterior Hessian ⇒local Gaussian approx.
– Adaptive, Geometric, Langevin MCMC
Dimension independent
– Proposal design: good MCMC performance in hiD
Literature: A. Stuart, M. Girolami, K. Law, T. Cui, Y. Marzouk
(Law 2014; Cui et al., 2014,2015; Cotter et al., 2013)

Curse of Dimensionality
• (Dim-adaptive) Sparse quadrature integration [Gerstner, 2003]
• High Dimensional Model Representation [Rabitz & Alis, 1999]
• would not handle strong nonlinearities
• tried cut-HDMR in a chemical kinetics context: fails!
• Proper Generalized Decomposition [Nuoy, 2010]
• Turn it into the blessing of dimensionality [Donoho, 2000]
• Compressive sensing in spectral methods [Doostan et al., 2009]
• Bayesian compressive sensing [Ji et al., 2008]

Curse of Dimensionality
• (Dim-adaptive) Sparse quadrature integration [Gerstner, 2003]
• High Dimensional Model Representation [Rabitz & Alis, 1999]
• would not handle strong nonlinearities
• tried cut-HDMR in a chemical kinetics context: fails!
• Proper Generalized Decomposition [Nuoy, 2010]
• Turn it into the blessing of dimensionality [Donoho, 2000]
• Compressive sensing in spectral methods [Doostan et al., 2009]
• Bayesian compressive sensing [Ji et al., 2008]
short answer: no free lunch

Challenges in PC UQ – Non-Linearity
Bifurcative response at critical parameter values
Rayleigh-B´enard convection
Transition to turbulence
Chemical ignition
Discontinuous u(λ(ξ))
Failure of global PCEs in terms of smooth Ψk()
⇔failure of Fourier series in representing a step function
Local PC methods
Subdivide support of λ(ξ) into regions of smooth u ◦λ(ξ)
Employ PC with compact support basis on each region
A spectral-element vs. spectral construction
Domain mapping

Discontinuities/Nonlinearities/Bifurcations
• Stochastic domain decomposition
• Wiener-Haar expansions,
Multiblock expansions,
Multiwavelets, [Le Maˆıtre et al, 2004,2007]
• also known as Multielement PC [Wan & Karniadakis, 2009]
• Data domain decomposition [Sargsyan et al, 2009,2010]
• Data clustering, classiﬁcation
• Mixture PC expansions
• Adaptive setting helps
• Does not scale with dimensionality
• For expensive models, can not split much
• Need a ‘smart’ domain decomposition

Challenges in PC UQ – Time Dynamics
Systems with limit-cycle or chaotic dynamics
Large ampliﬁcation of phase errors over long time horizon
PC order needs to be increased in time to retain accuracy
Time shifting/scaling remedies
Futile to attempt representation of detailed turbulent velocity ﬁeld
v(x, t; λ(ξ)) as a PCE
– Fast loss of correlation due to energy cascade
– Problem studied in 60’s and 70’s
Focus on ﬂow statistics, e.g. Mean/RMS quantities
Well behaved
Argues for non-intrusive methods with DNS/LES of turbulent ﬂow

Model Complexity challenge
If a single model run is a challenge then UQ is infeasible
Most physical model output quantities of interest depend on only a
“small” number of parameters, however:
Global sensitivity analysis itself requires many samples
Even after reduction of dimensionality to, say, 5 parameters, O(100)
samples may be necessary
Large number of independent samples
– ideally suited for HPC
Multiﬁdelity UQ methods are useful – forward UQ
Use combinations of many low-resolution/low-ﬁdelity runs with a
few high-resolution/high-ﬁdelity runs
Parallel MCMC methods – inverse UQ

Data Scarcity Challenge
Even in a “big-Data” context, it’s common to ﬁnd no information in
the data on many big-model parameters
Situation is typical in statistical inversion for ﬁeld quantities
Bayesian inference of optimal random ﬁeld constructions
Use adaptive MCMC methods that focus on data-informed
parameters
Usually, raw data is not published
Published “data” is essentially processed data products, being
statistics on
– the data, or functions of ﬁtted model parameters
Use Maximum-Entropy and Approximate Bayesian Computation
(ABC) methods – DFI
– Discover posterior density on model parameters
consistent with published statistics

Input correlations: Rosenblatt transformation
• Rosenblatt transformation maps any (not necessarily independent) set of
random variables ξ = (ξ1, . . . , ξn) to uniform i.i.d.’s {ηi}n
i=1 [Rosenblatt,
1952].
η1
=
F1(ξ1)
η2
=
F2|1(ξ2|ξ1)
η3
=
F3|2,1(ξ3|ξ2, ξ1)
...
ηn
=
Fn|n−1,...,1(ξn|ξn−1, . . . , ξ1)
0.18
0.2
0.22
0.24
0.4
0.45
0.5
0.3
0.32
0.34
0.36
Labile
Cellulose
Lignin
• Inverse Rosenblatt transformation ξ = R−1(η) ensures a well-deﬁned
quadrature integration to build PC [Sargsyan et al., 2010]
ck = ⟨ξΨk(η)⟩=
Z
R−1(η)Ψk(η)dη
• Caveat: if only samples of ξ are available, the conditional distributions are
hard to evaluate accurately.

