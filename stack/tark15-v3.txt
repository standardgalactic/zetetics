Human-Agent Decision-making: Combining Theory and
Practice
[Extended Abstract]
Sarit Kraus
Dept. of Computer Science
Bar-Ilan University
Ramat-Gan, Israel
sarit@cs.biu.ac.il
ABSTRACT
Extensive work has been conducted both in game theory and
logic to model strategic interaction. An important question
is whether we can use these theories to design agents for
interacting with people?
On the one hand, they provide
a formal design speciﬁcation for agent strategies.
On the
other hand, people do not necessarily adhere to playing in
accordance with these strategies, and their behavior is af-
fected by a multitude of social and psychological factors. In
this paper we will consider the question of whether strate-
gies implied by theories of strategic behavior can be used by
automated agents that interact proﬁciently with people. We
will focus on automated agents that we built that need to
interact with people in two negotiation settings: bargaining
and deliberation. For bargaining we will study game-theory
based equilibrium agents and for argumentation we will dis-
cuss logic-based argumentation theory. We will also consider
security games and persuasion games and will discuss the
beneﬁts of using equilibrium based agents.
Categories and Subject Descriptors
I.2 [ARTIFICIAL INTELLIGENCE]: Miscellaneous
Keywords
Intelligent Agents
1.
INTRODUCTION
Agents that interact proﬁciently with people may be use-
ful for training [27], supporting [23, 12, 11, 1] and even re-
placing people in many applications [10, 22].
We are considering the agent-human interactions as being
a strategic activity [14]. That is, we assume that when the
automated agent engages in the interaction, it should act
as best it can to realize its preferences. Game theory is the
mathematical theory of strategic decision-making [28] and
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
TARK15 2015 CMU USA
Copyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.
thus it seems that game theory might be an appropriate an-
alytical tool for understanding how a strategic agent can and
should act, and might also be useful in both the design of au-
tomated agents and protocols for the interactions. However,
game theory assumes that all players will act as best they
can to realize their preferences. Unfortunately, humans tend
to make mistakes, and they are aﬀected by cognitive, social
and cultural factors [8, 25, 4]. In particular, people’s ob-
served behavior does not correspond to game theory-based
equilibrium strategies [13, 29].
Another approach for the development of automated agents
is the (non-classical)-logic approach.
The agent is given
a logical representation of its environment and its desired
goals, and it reasons logically in order to generate its activ-
ities. When interacting with people, the environment con-
sists also of the human model. Yet, modeling people’s be-
havior is a big challenge. We have incomplete information
about the person’s preferences, and we have to cope with the
uncertainties inherent in human decision-making and behav-
ior. Human behavior is diverse, and cannot be satisfactorily
captured by a simple abstract model. In particular, human
decision-making tends to be very noisy: a person may make
diﬀerent strategic decisions in similar situations.
In this paper we survey brieﬂy a few of the agents that we
built over the years that interact proﬁciently with people. In
most of the cases, deploying only a game-theory approach
or logical-based approach was not beneﬁcial. Heuristics and
machine-learning techniques were augmented into the for-
mal models to lead to agents that interact proﬁciently with
people. We will discuss three negotiation settings: multi-
issue negotiations, games where the players interleave nego-
tiations with resource exchange while attempting to satisfy
their goals and argumentation settings. Finally, we will dis-
cuss security games.
2.
MULTI-ISSUE NEGOTIATIONS
Over the years we designed and implemented several au-
tomated agents for multi-issue negotiations. In multi-issue
negotiations the players need to reach an agreement on sev-
eral issues. Each issue is associated with a set of possible
values and the players need to agree on a speciﬁc value for
each issue. The negotiations can end with the negotiators
signing an agreement or with one of the sides opting out
of the negotiations. In addition, if the crisis does not end
within a pre-speciﬁed deadline then the status quo is imple-
mented. Each outcome of the negotiations is associated with

Table 1: Multi-issue negotiations
Settings
Agent
Agent Properties
Scenarios
Signiﬁcance
Name
vs people
Bilateral, single-issue, full information,
EQH
SPE with manually
ﬁshing
One role
complex actions, agreements not enforceable
designed heuristics
dispute
Bilateral, uncertainty, multi-issue
QO-agent
Qualitative decision-making
job interview
One role
Non-deterministic behavior
tobacco
KBAgent
Machine learning,
job interview
Both roles
qualitative decision-making
tobacco
non-deterministic behavior
NegoChat
KBagent algorithms, AAT
job interview
Both roles
Anchoring, NLP module
a utility score for both players. A summary of our agents is
presented in Table 1.
2.1
EQH agent
The ﬁrst agent, EQH, that we developed was for crisis sce-
narios and the setting was quite complex [24]. In addition to
the message exchange in a semi-structured language, players
could take actions during the negotiations and agreements
were not enforceable. In particular, opting out in a crisis is
a stochastic action and thus the agents are uncertain about
the result. In addition to the main issue of the negotiation or
opting out, there are various other parameters of an agent’s
action. These parameters inﬂuence the utility of the nego-
tiators from the crisis. Time plays an important role in the
crisis [42]. The speciﬁc scenario we used for the experimen-
tal study was a ﬁshing dispute between Canada and Spain.
We formalized the crisis scenario as a game and identiﬁed
a subgame-perfect equilibrium. We ran preliminary experi-
ments when the automated agent followed its subgame per-
fect equilibrium strategy. However, the human negotiators
who negotiated with it became frustrated and the negoti-
ation often ended with no agreement.
The frustration of
the human negotiators was mainly due the lack of ﬂexibility
of the agent. Since the proposed and accepted agreements
of the subgame perfect negotiation did not change over the
negotiation time, the agent did not compromise.
To address this limitation of the equilibrium-based agent,
we incorporated several heuristics to the EQH agent. We
allowed the owner of the agent to determine the way the
agent will deviate from the equilibrium strategies by deter-
mining parameters that inﬂuence the agent’s behavior which
are instantiated before the beginning of negotiations. In or-
der to provide the agent with some ﬂexibility when playing
against people, we allowed the agent to consent to agree-
ments that have a lower utility than it would have obtained
according to the relevant subgame perfect equilibrium strat-
egy agreement. Therefore we added the margin parameter
that determines the largest number of points lower than the
desired utility value to which the agent will agree.
An additional parameter is the number of negotiation
units by which the agent will increase or decrease its ﬁrst
oﬀer from the agreement speciﬁed in its equilibrium strat-
egy. Human negotiators usually begin negotiations with an
oﬀer higher (or lower, depending on the negotiator’s role)
than the value they would eventually like to reach at the
end of negotiations. This leaves bargaining space and our
agent uses this type of strategy. Another parameter indi-
cates whether the agent will send the ﬁrst message in the ne-
gotiation or will wait for its opponent to make the ﬁrst oﬀer.
The default value of this parameter, following some litera-
ture recommendations [16], was that the agent will send the
ﬁrst oﬀer, since we wanted a trigger to initiate negotiations
with the other agent.
Another heuristic concerns opting out. Given our assump-
tions, while rational agents will not opt out, people may opt
out. If the agent’s expected utility from opting out is higher
than its expected utility from its opponent opting out, it will
try to predict whether its opponent is going to opt out. If
so, it will opt out ﬁrst. The heuristic for the prediction of
whether an opponent will opt out is based on the messages
sent by the opponent.
For example, when a threatening
message is received, or when a comment message indicating
that the negotiations are heading in a dangerous direction
is received, the estimation that the opponent may opt out
increases.
We ran extensive experiments for evaluating the equilib-
rium agent with the heuristics (EQH agent) [24]. We com-
pared the results of the humans to those of the agents and
concluded that the EQH agent received a higher utility score
playing both roles, but the results were only statistically sig-
niﬁcant when the agent played just one of the roles. Fur-
thermore, when an agent participates in a negotiation, the
sum of the utilities are signiﬁcantly higher than when two
humans play since the agent always proposes Pareto-optimal
oﬀers while people reach agreements that are not.
While the EQH agent was based on the subgame perfect
equilibrium strategies, it required the introduction of many
heuristics, and its success compared with people was only in
one role. The main open question is whether it is possible
to provide formal methodology that will lead to an agent
that is similar to the EQH without the need to manually
design the EQH heuristics. Furthermore, we are aiming for
an agent that can achieve a signiﬁcantly higher utility score
than people in both roles. Toward this challenges, we next
tried to use a qualtative approach, to introduce incomplete
information into the environment and to improve the agent’s
results in both roles.
2.2
QOagent and KBagent
The QOagent was designed to interact with people in
environments of bilateral negotiations with incomplete in-
formation when the agreements consist of multiple issues
[26].
With respect to incomplete information, each nego-
tiator keeps his preferences private, though the preferences

might be inferred from the actions of each side (e.g., oﬀers
made or responses to oﬀers proposed). Incomplete informa-
tion is expressed as uncertainty regarding the utility pref-
erences of the opponent, and it is assumed that there is a
ﬁnite set of diﬀerent negotiator types. These types are asso-
ciated with diﬀerent additive utility functions (e.g., one type
might have a long term orientation regarding the ﬁnal agree-
ment, while the other type might have a more constrained
orientation). Lastly, the negotiation is conducted once with
each opponent. The experiments were run on two distinct
domains. In the ﬁrst domain, England and Zimbabwe ne-
gotiate in order to reach an agreement evolving from the
World Health Organization’s Framework Convention on To-
bacco Control, the world’s ﬁrst public health treaty. In the
second domain a negotiation takes place after a successful
job interview between an employer and a job candidate.
We ﬁrst formalized the scenario as a Bayesian game and
computed the Bayesian Nash equilibrium. Though we did
not run simulations of the Bayesian Nash equilibrium agent
against human negotiators, we ran two humans negotiations.
We found out that the opponent’s utility score from the of-
fers suggested by the equilibrium agent are much lower than
the ﬁnal utility values of the human negotiations. By also
analyzing the simulation process of the human negotiations,
we deduced that without incorporating any heuristics into
the equilibrium agent, the human players would not have
accepted the oﬀers proposed by it which will lead to low
utility scores for the equilibrium agent, similar to the low
score of the equilibrium agent in the ﬁshing dispute.
Therefore, we developed the QOAgent. For the decision-
making process, the approach used by the QOAgent tries
to take the utility of both sides into consideration. While the
QOAgent’s model applies utility functions, it is based on a
non-classical decision-making method, rather than focusing
on maximizing the expected utility: the maximin function
and a qualitative valuation of oﬀers. Using these methods,
the QOAgent generates oﬀers and decides whether to ac-
cept or reject proposals it has received. As for incomplete
information, the QOAgent tackles this problem using a
simple Bayesian update mechanism. After each action, this
mechanism tries to infer which negotiator type best suits the
opponent.
The eﬀectiveness of this method was demonstrated through
extensive empirical experiments by [26].
The results of the experiments showed that the automated
agent achieved higher utility scores than the human coun-
terpart. This can be explained by the nature of our agent
both in reference to accepting oﬀers and generating oﬀers.
Using the decision-making mechanism we allow the agent to
propose agreements that are good for it, but also reasonable
for its opponent. In addition, the automated agent makes
straightforward calculations. It evaluates the oﬀer based on
its attributes, and not based on its content. In addition, it
also places more weight on the fact that it loses or gains as
time advances. This is not the case, however, when analyz-
ing the logs of the people. It seems that people put more
weight on the content of the oﬀer than on its value. This was
more evident in the Job Candidate domain with which the
human subjects could more easily identify. Yet, this does
not explain why, in both domains, similar to the EQH agent
experiments, these results are signiﬁcant only for one of the
sides. In the England-Zimbabwe domain, the results are sig-
niﬁcant when the agent played the role of England, while in
Figure 1: The negotiation system’s interface for NegoChat.
the Job Candidate domain these results are signiﬁcant when
it played the role of the job candidate.
In order to improve the QOAgent, we extended it by
using a generic opponent modeling mechanism, which allows
the agent to model its counterpart’s population and adapt its
behavior to that population [32]. The extended agent, called
KBAgent, is an automated negotiator that negotiates with
each person only once, and uses past negotiation sessions of
others as a knowledge base for generic opponent modeling.
The database containing the a relatively small number of
past negotiation sessions is used to extract the likelihood of
acceptance of proposals and which proposals may be oﬀered
by the opposite side.
The performance of KBAgent in
terms of its counter-oﬀer generation and generic opponent
modeling was tested against people in the Tobacco and the
Job interview domains.
The results of these tests indicate that the KBAgent ne-
gotiates proﬁciently with people and even achieves higher
utility score values than the QOAgent.
Moreover, the
KBAgent achieves signiﬁcantly better agreements, in terms
of utility score, than the human counterparts in both roles.
These results indicate that integrating general opponent mod-
eling into qualtative decision-making is beneﬁcial for auto-
mated negotiations.
2.3
NegoChat Agent
All the agents we discussed so far negotiated with the hu-

man counterpart either using a structured language or using
a menu-driven interaction. They lack the natural language
processing support required to enable real world types of
interactions.
To address this challenge we ﬁrst developed
an NLP module that translates the free text of the hu-
man player to the agent’s formal language.
We modiﬁed
the KBagent by adding this module without changing the
KBagent strategy and ran an experiment in which the mod-
iﬁed KBagent played with people in a chat-like environment
(see Figure 1 for the negotiation system’s interface for chat-
based negotiations). We found that simply modifying the
KBagent to include an NLP module is insuﬃcient to cre-
ate a good agent for such settings and the revised agent
achieved relatively low utility scores.
The main observa-
tion was that people in chat-based negotiations make and
accept partial agreements and follow issue-by-issue negotia-
tions while the KBagent proposes full oﬀers and has diﬃcul-
ties reaching partial agreements. To address this limitation,
we developed NegoChat, which extended the KBagent fo-
cusing on strategies that allow for partial agreements and
issue-by-issue interactions. NegoChat’s algorithm is based
on bounded rationality, speciﬁcally anchoring and Aspira-
tion Adaptation Theory (AAT). The AAT was used for de-
ciding on the order in which the issues will be discussed.
The agent begins each negotiation interaction by proposing
a full oﬀer based on the KBagent’s strategy, which serves as
its anchor. Assuming this oﬀer is not accepted, NegoChat
then proceeds to negotiate via partial agreements, propos-
ing the next issue for negotiation based on people’s typical
urgency (according to AAT).
We evaluated the NegoChat agent in extensive experi-
ments negotiating with people in the job interview domain.
We compared its performance to the performance of the
KBAgent that also negotiated with (diﬀerent) people us-
ing the same NLP module. The NegoChat agent achieved
signiﬁcantly better agreements (i.e., higher utility score) in
less time.
However, people playing against KBAgent, on
average, did better. This implies that some of NegoChat’s
success is evidently at the cost of the person’s score and
consequently the social welfare score of this agent is not sig-
niﬁcantly better than that of KBAgent. As our goal is to
maximize the agent’s utility score this should not be seen
as a fault. However, future generations of automated agents
may decide to implement diﬀerent strategies to maximize
social welfare.
3.
NEGOTIATIONS AND ACTIONS INTER-
LEAVING
In most situations, negotiation is not done in isolation but
is associated with agreement implementation and other ac-
tivities. We developed agents that can interact with people
in such settings. These studies were carried out in a con-
ﬁgurable system called Colored Trails (CT)1. It is a game
played by two or more participants on a board of colored
squares. CT is an abstract, conceptually simple but highly
versatile game in which players negotiate and exchange re-
sources to enable them to achieve their individual or group
goals. It provides a realistic analogue to multi-agent task do-
mains, while not requiring extensive domain modeling [15,
18]. A summary of the agents we developed are speciﬁed in
Table 2.
1See http://www.eecs.harvard.edu/ai/ct.
3.1
Revelation games
We considered negotiation settings in which participants
lack information about each other’s preferences, often hin-
dering their ability to reach beneﬁcial agreements [34]. Specif-
ically, we studied a particular class of such settings we call
“revelation games”, in which two players are given the choice
to truthfully reveal private information before commencing
two rounds of alternating negotiation.
Revealing this in-
formation narrows the search space of possible agreements
and may lead to agreement more quickly, but may also cause
players to be exploited by others (see examples of such games
in Figure 2). Revelation games combine two types of inter-
actions that have been studied in the past in the economics
literature:
Signaling games [39], in which players choose
whether to convey private information to each other, and
bargaining [31], in which players engage in multiple negoti-
ation rounds.
We were hopeful that, for revelation games, equilibrium-
based agents will interact well with people since behavioral
economics work has shown that people often follow equilib-
rium strategies [7] when deciding whether to reveal private
information to others. The question is whether this obser-
vation will be stronger than our previous observations re-
ported above that people’s behavior in bargaining settings
does not adhere to equilibrium strategies. We formalized the
setting as a Bayesian game and computed two types of per-
fect Bayesian equilibrium: a separating equilibrium where
both players reveal their type, and a pooling equilibrium
where none of the players reveal their types.
We compared the equilibrium agents with people playing
with other people and with the Sigmoid Acceptance Learn-
ing Agent (SIGAL) that we developed [34].
The SIGAL
agent used classical machine learning techniques to predict
how people make and respond to oﬀers during negotiation,
how they reveal information and their response to potential
revelation actions by the agent. This model is integrated
into the agent’s decision tree. We conducted an extensive
empirical study spanning hundreds of human subjects.
Results show that the SIGAL agent was able to outper-
form people and the equilibrium agents. Furthermore, peo-
ple outperformed the equilibrium agents. It turned out that
the negotiation part of the game was more important (with
respect to the utility score) than the revelation part. The
equilibrium agent made very selﬁsh oﬀers in the last round
of the negotiations. Most of these oﬀers were rejected. In
the ﬁrst round, it made oﬀers that were highly beneﬁcial to
people and most of these oﬀers were accepted, but the small
beneﬁt it incurred in these proposals did not aid its perfor-
mance. The SIGAL agent, on the other hand, (i) learned
to make oﬀers that were beneﬁcial to people while not com-
promising its own beneﬁt; and (ii) incrementally revealed
information to people in a way that increased its expected
performance. We were able to adjust SIGAL to new, similar
settings that varied rules and situational parameters of the
game without the need to accumulate new data. However,
moving to a completely new setting requires a lot of work
collecting data and adjusting the machine learning module
to the new setting.
3.2
Non-binding agreements
We also studied CT settings of two players in which both
participants needed to complete their individual tasks by
reaching agreements and exchanging resources, the number

(a) Symmetric Board Game
(b) Asymmetric Board
Game
(c) A possible proposal
Figure 2: Two CT revelation games
Table 2: CT games
Settings
Agent Name
Agent Properties
Signiﬁcance vs people
Bilateral, Uncertainty, Revelation Game
PBE agent
Bayesian perfect equilibrium
No roles
two-phases: revelation, bargaining
SIGAL
decision theory, machine learning
Both roles
Bilateral, full information, agreements
PAL
decision theory: Inﬂuence Diagram
Both roles
not enforceable, multiple rounds, three phases:
machine learning
bargaining, resource exchange, movement
Contract game, three players
SPE agent
subgame-perfect equilibrium
CS role
two-phases: bargaining, movement
SP-RAP
subgame-perfect equilibrium
bounded rational model of opponent
SP role
risk averse
Figure 3: An example of a CT Board for multiple negotiation
games with unenforceable agreements.
of negotiation rounds were not ﬁxed in advance, and the ne-
gotiation protocol was an alternating oﬀers protocol that al-
lowed parties to choose the extent to which they kept each of
their agreements during the negotiation [19]. That is, there
are three phases in each round of the game: negotiation,
transfer and movement. The negotiation phase consisted of
two rounds of alternating oﬀers in which the players could
reach an agreement on resource exchange. After each phase
of negotiations, the game moved to the “transfer phase” in
which both players could transfer resources to each other.
The transfer action was done simultaneously, such that nei-
ther player could see what the other player transferred until
the end of the phase. A player could choose to transfer more
resources than it agreed to, or any subset of the resources it
agreed to, including not transferring any resources at all. In
the “movement phase” both players could move their icons
on the board one step towards the goal square, provided
they had the necessary resources. Then, the game moved
to the next round, beginning again with negotiation phase.
The game ends when one of the players reaches his goal or
does not move for two rounds (see an example of one such
game in Figure 3).
The most important decision of a player in such settings
is whether or not to keep the agreements. Another impor-
tant decision is whether to accept an oﬀer given by the other
player. In subgame perfect equilibrium, the players should
not keep the agreements. Diﬀerent equilibria may specify
various strategies for the acceptance decision. We ran pre-
liminary experiments and observed that such strategies are
not beneﬁcial when the equilibrium agent interacts with peo-
ple. Most of the time the agent was not able to reach its
goal, yielding a low utility score.

Galit et al. [19] present the Personality Adaptive Learning
(PAL) agent for negotiating with people from diﬀerent cul-
tures for the CT game where agreements are not enforceable.
The methodology was similar to that of SIGAL (Section 3.1),
combining a decision-theoretic model using a decision tree
with classical machine learning techniques to predict how
people respond to oﬀers, and the extent to which they fulﬁll
agreements.
PAL was evaluated empirically in the Colored Trails (CT)
environment by playing with people in three countries: Lebanon,
the U.S., and Israel, in which people are known to vary
widely in their negotiation behavior. The agent was able to
outperform people in all three countries.
3.3
Contract Game
We studied commitment strategies in a three-player CT
game.
The game is called Contract Game and is analo-
gous to a market setting in which participants need to reach
agreements over contracts and commit to or renege from
contracts over time in order to succeed [20]. The game com-
prises three players, two service providers and one customer.
The service providers compete to make repeated contract of-
fers to the customer consisting of resource exchanges in the
game (see an example of one such game in Figure 4). We
formally analyzed the game to compute subgame perfect
equilibrium strategies for the customer and service provider
in the game that are based on making contracts containing
commitment oﬀers. To evaluate agents that use the equi-
librium strategies, we conducted extensive empirical studies
in three diﬀerent countries, the U.S., Israel and China. We
ran several conﬁgurations in which two human participants
played a single agent participant in various role conﬁgura-
tions in the game. Our results showed that the computer
agent using subgame Nash equilibrium strategies for the cus-
tomer role was able to outperform people playing the same
role in all three countries and obtained statistically signiﬁ-
cant, higher utility scores than the humans. This was very
surprising since it was the ﬁrst EQ agent after trying many
equilibrium agents that was able to achieve such results.
In particular, the customer agent made signiﬁcantly more
commitment type proposals than people did, and requested
signiﬁcantly more resources from service providers than did
people. It was quite surprising that people playing with it
accepted these oﬀers; in other settings (such as the revela-
tion games) such unfair oﬀers were rejected by people. We
hypothesize that the competition between the two service
providers made such oﬀers more acceptable.
In addition,
while in the revelation games the EQ agent had only one
opportunity to make an oﬀer, in the contract game it could
make oﬀers several times (oﬀthe equilibrium path) which
we believe also increased the acceptance rate. Also, the cus-
tomer agent reached one of the goals in all its games and
was able to reach the goal signiﬁcantly more often than peo-
ple. This is again quite surprising since at the beginning of
the game the customer has enough resources to reach both
goals. Since reaching one of the goals is very beneﬁcial to
the customer it is diﬃcult to understand why human players
hadn’t always reached the goal.
While the customer EQ agent outperformed people, peo-
ple outperformed the EQ agent when it played the role of
one of the service providers. We believe that this is mainly
due to people playing the customer role not reaching the
goal even when they have all the needed resources to do so.
Figure 4: An example of a CT Board for the Contract game.
To face this problem we then developed an agent termed
SP-RAP which extended the EQ agent in the following two
ways to handle the uncertainty that characterizes human
play in negotiation: First, it employed a risk averse strategy
using a convex utility function. Second, it reasoned about a
possibly bounded rational customer (CS) player by assign-
ing a positive probability p > 0 for the customer player not
reaching the goal. We assigned a separate value for p for
each country by dividing the number of times the CS player
reached the goal by the total number of games played. Con-
sequently, SP-RAP outperformed people playing the SP role
in all three countries.
4.
ARGUMENTATION AGENT
An automated agent can help a human when engaging in
an argumentative dialog by utilizing its knowledge and com-
putational advantage to provide arguments to him. Argu-
mentation was studied extensively using the well-established
Argumentation Theory (see [41] for a summary). Therefore,
in the ﬁrst step in the development of an automated agent
that advised people in such settings, we considered the abili-
ties of Argumentation Theory to predict people’s arguments.
In [38] we presented extensive studies in three experimen-
tal settings, varying in complexity, which show the lack of
predictive power of the existing Argumentation Theory. Sec-
ond, we used Machine Learning (ML) techniques to provide
a probability distribution over all known arguments given a
partial deliberation. That is, our ML techniques provided
the probability of each argument to be used next in a given
dialog. Our model achieves 76% accuracy when predicting
people’s top three argument choices given a partial delib-
eration.
Last, using the prediction model and the newly
introduced heuristics of relevance, we designed and eval-
uated the Predictive and Relevance based Heuristic agent
(PRH). Through an extensive human study, we showed that
the PRH agent outperforms other agents that propose ar-
guments based on Argumentation Theory, predicted argu-
ments without heuristics or only the heuristics on both axes
we examined: people’s satisfaction from agents and people’s
use of the suggested arguments.
5.
SECURITY GAMES
The last several years have witnessed the successful appli-
cation of Bayesian Stackelberg games in allocating limited
resources to protect critical infrastructures.
These inter-
esting eﬀorts have been led by Prof.
Milind Tambe from
USC. The ﬁrst application is the ARMOR system (Assistant
for Randomized Monitoring over Routes) that has been de-
ployed at the Los Angeles International Airport (LAX) since

2007 to randomize checkpoints on the roadways entering the
airport and canine patrol routes within the airport terminals
[33, 35]. Other applications include IRIS, a game-theoretic
scheduler for randomized deployment of the US Federal Air
Marshal Service (FAMS) requiring signiﬁcant scale-up in un-
derlying algorithms, which has been in use since 2009 [40];
and PROTECT, which requires further scale up, is deployed
for generating randomized patrol schedules for the US Coast
Guard in Boston, New York, Los Angeles and other ports
around the US [2, 3]. Furthermore, TRUSTS has been eval-
uated for deterring fare evasion, suppressing urban crime
and counter-terrorism within the Los Angeles Metro Sys-
tem [44, 21, 9] and GUARDS was earlier tested by the US
Transportation Security Administration (TSA) for security
inside the airport [37].
The evaluation of these systems could be quite limited.
The only system that was truly evaluated in the ﬁeld is
TRUSTS. We were able to conduct controlled experiments
of our game theoretic resource allocation algorithms. Be-
fore this project, the actual evaluation of the deployed se-
curity games applications in the ﬁeld was a major open
challenge.
The reasons were twofold.
First, previous ap-
plications focused on counter-terrorism, therefore controlled
experiments against real adversaries in the ﬁeld were not
feasible. Second, the number of practical constraints related
to real-world deployments limited the ability of researchers
to conduct head-to-head comparisons
In TRUSTS we were able to address this challenge and run
the largest scale evaluation of security games in the ﬁeld in
terms of duration and number of security oﬃcials deployed.
We evaluated each component of the system (Fare Evasion,
Counter Terrorism and Crime algorithms) by designing and
running ﬁeld experiments. In the context of fare evasion, we
ran an extensive experiment, where we compared schedules
generated using game theory against competing schedules
comprised of a random scheduler, augmented with oﬃcers
providing real-time knowledge of the current situation. Our
results showed that our schedules led to statistically signif-
icant improvements over the competing schedules, despite
the fact that the latter were improved with real-time knowl-
edge.
In addition, extensive human experiments in the lab were
conducted [36, 30]. These experiments showed that incor-
porating bounded rational models of the adversaries to the
Stackelberg games improves the performance of the defend-
ers. These results were observed both when the role of ad-
versaries was played by novices and when it was played by
security experts.
6.
PERSUASION GAMES
A persuasion game involves two players: a sender who at-
tempts to persuade another agent (the receiver) to take a
certain action [17].
Persuasion games are similar to both
negotiation games and security games [43]. They are simi-
lar to negotiation and argumentation games since one player
tries to convince another player to do something, as in ne-
gotiations. They are also similar to security games in the
asymmetry between the players: the sender and the defender
are trying to inﬂuence the activities of the receiver and the
attacker, respectively. So, it is interesting to check if equilib-
rium strategies will be beneﬁcial in persuasion games. Fur-
thermore, the incorporation of a bounded rational model
of the receiver will be beneﬁcial to the sender as the in-
corporation of bounded rational models of the attacker was
beneﬁcial to the defender in security games.
We focused on information disclosure games with two-
sided uncertainty [5, 6]. This is a special type of persua-
sion game in which an agent tries to lead a person to take
an action that is beneﬁcial to the agent by providing him
with truthful, but possibly partial, information relevant to
the action selection. We ﬁrst computed the subgame perfect
Bayesian Nash equilibrium of the game assuming the human
receiver is fully rational. We developed a sender agent that
follows the equilibrium strategy (GTBA agent).
We also developed a machine learning-based model that
eﬀectively predicts people’s behavior in these games and we
called it Linear weighted-Utility Quantal response (LUQ).
The model we provide assumes that people use a subjective
utility function which is a linear combination for all given
attributes. The model also assumes that while people use
this function as a guideline, they do not always choose the
action with the greatest utility value, however, the higher an
action’s utility value is, the more likely they are to choose
that action. We integrated this model into our persuasion
model and built the LUQA agent.
We ran an extensive empirical study with people in two
diﬀerent games.
In a multi-attribute road selection game
with two-sided uncertainty, the LUQA agent obtained sig-
niﬁcantly higher utility points than the GTBA agent. How-
ever, in the second game, the Sandwich game, there was no
signiﬁcant advantage to the machine learning-based model,
and using the game theory-based agent, GTBA, which as-
sumes that people maximize their expected monetary values
is beneﬁcial. We hypothesize that these diﬀerent results are
due to the nature of the domains. The monetary result plays
an important role in the sandwich game. This is because
the game is played in an environment where a person’s goal
is to make a proﬁt.
However, in the road selection game
the utility scores are associated with time. Thus, it seems
that maximizing expected monetary utility is easier for peo-
ple than maximizing utility scores that are associated with
time.
7.
DISCUSSIONS
The state-of-the-art agent, NegoChat, for multi-issue ne-
gotiations integrates methods from several disciplines: quali-
tative decision-making, machine learning and heuristics based
on psychological theories. None of the equilibrium agents
that were developed were successful when interacting with
people. The reliance on heuristic and machine learning makes
the transfer of NegoChat from one scenario to the other and
from one culture to the other problematic. This was evident
recently when we tried to run experiments with NegoChat,
which was developed based on data collected in Israel and
Egypt. We had to spend a lot of time and eﬀort until this
transfer was possible.
Similarly, in most of the cases, the equilibrium agent was
not successful in the CT game settings. The only exception
is the contract game.
We believe that the success of the
equilibrium agent in the contract game has to do with the
speciﬁcs of the game: the competition between the two SPs.
In the contract game, it was extremely diﬃcult to predict
people’s behavior, thus the success of the equilibrium agent
is even more signiﬁcant.
The same observations were seen in argumentation – the
argumentation theory-based agent was not very successful.

Therefore, in all these cases the development of new negoti-
ation agents to new settings requires the collection of data
and the adjustment of the agent to the new settings. There-
fore, we strongly believe that the development of theoretical
models for the design and implementation of agents that ne-
gotiate in multi-issue negotiation settings can be very useful.
However, this is still an open question.
On the other hand, it seems that in security games the
deployment of Stackelberg equilibrium is beneﬁcial (possi-
bly with the incorporation of a bounded rational model of
the attacker) and similarly in persuasion games where using
subgame perfect Baysian equilibrium is beneﬁcial (possibly
with the incorporation of a bounded rational model of the
receiver).
We hypothesize that this is the case since in security
games and persuasion games the interactions between the
agent and the human is quite limited.
The attacker or
the receiver needs to choose one action compared to many
decision-making activities that are required from a human
negotiator. Nevertheless, even in security games and to some
extent in persuasion games it was shown that taking the lim-
itations of the other player into consideration is beneﬁcial.
8.
ACKNOWLEDGMENT
This work was supported in part by ERC grant #267523.
9.
REFERENCES
[1] O. Amir, B. Grosz, E. Law, and R. Stern.
Collaborative health care plan support. In Proceedings
of the Eleenth International Joint Conference on
Autonomous Agents and Multiagent Systems
(AAMAS-2013), pages 793–796, St Paul, MN, 2013.
[2] B. An, M. Jain, M. Tambe, and C. Kiekintveld.
Mixed-initiative optimization in security games: A
preliminary report. In AAAI Spring Symposium: Help
me help you: Bridging the gaps in human-agent
collaboration, 2011.
[3] B. An, F. Ord´o˜nez, M. Tambe, E. Shieh, R. Yang,
C. Baldwin, J. DiRenzo III, K. Moretti, B. Maule, and
G. Meyer. A deployed quantal response-based patrol
planning system for the us coast guard. Interfaces,
43(5):400–420, 2013.
[4] D. Ariely. Predictably Irrational. Harper Collins, 2008.
[5] A. Azaria, Z. Rabinovich, C. V. Goldman, and
S. Kraus. Strategic information disclosure to people
with multiple alternatives. ACM Transactions on
Intelligent Systems and Technology (TIST), 5(4):64,
2014.
[6] A. Azaria, Z. Rabinovich, S. Kraus, and C. V.
Goldman. Strategic information disclosure to people
with multiple alternatives. In Proceedings of the
Twenty-Fifth AAAI Conference on Artiﬁcial
Intelligence, AAAI 2011, San Francisco, California,
USA, August 7-11, 2011, 2011.
[7] J. Banks, C. F. Camerer, and D. Porter. Experimental
tests of Nash reﬁnements in signaling games. Games
and Economic Behavior, 6:1–31, 1994.
[8] M. H. Bazerman and M. A. Neale. Negotiator
rationality and negotiator cognition: The interactive
roles of prescriptive and descriptive research. In H. P.
Young, editor, Negotiation Analysis, pages 109–130.
The University of Michigan Press, 1992.
[9] F. M. Delle Fave, A. X. Jiang, Z. Yin, C. Zhang,
M. Tambe, S. Kraus, and J. P. Sullivan.
Game-theoretic patrolling with dynamic execution
uncertainty and a case study on a real transit system.
Journal of Artiﬁcial Intelligence Research, pages
321–367, 2014.
[10] E. Durenard. Professional Automated Trading: Theory
and Practice. John Wiley & Sons, 2013.
[11] A. Elmalech, D. Sarne, and B. J. Grosz. Problem
restructuring for better decision making in recurring
decision situations. Autonomous Agents and
Multi-Agent Systems, 29(1):1–39, 2015.
[12] A. Elmalech, D. Sarne, A. Rosenfeld, and E. S. Erez.
When suboptimal rules. In Proceedings of the
Twenty-Ninth AAAI Conference on Artiﬁcial
Intelligence, pages 1313–1319, 2015.
[13] I. Erev and A. E. Roth. Predicting how people play
games: Reinforcement learning in experimental games
with unique, mixed strategy equilibria. American
Economic Review, 88(4):848–881, 1998.
[14] S. Fatima, S. Kraus, and M. Wooldridge. Principles of
Automated Negotiation. Cambridge University Press,
2014.
[15] Y. Gal, B. Grosz, S. Kraus, A. Pfeﬀer, and S. Shieber.
Agent decision-making in open mixed networks.
Artiﬁcial Intelligence, 174(18):1460–1480, 2010.
[16] A. D. Galinsky and T. Mussweiler. First oﬀers as
anchors: the role of perspective-taking and negotiator
focus. Journal of personality and social psychology,
81(4):657, 2001.
[17] J. Glazer and A. Rubinstein. A study in the
pragmatics of persuasion: a game theoretical
approach. Theoretical Economics, 1(4):395–410, 2006.
[18] B. J. Grosz, S. Kraus, S. Talman, B. Stossel, and
M. Havlin. The inﬂuence of social dependencies on
decision-making: Initial investigations with a new
game. In Proceedings of the Third International Joint
Conference on Autonomous Agents and Multiagent
Systems-Volume 2, pages 782–789. IEEE Computer
Society, 2004.
[19] G. Haim, Y. Gal, M. Gelfand, and S. Kraus. A
cultural sensitive agent for human-computer
negotiation. In Proceedings of the Eleventh
International Joint Conference on Autonomous Agents
and Multiagent Systems (AAMAS-2012), pages
451–458, Valencia, Spain, 2012.
[20] G. Haim, Y. K. Gal, S. Kraus, and B. An.
Human-computer negotiation in three-player market
settings. In Proc. of ECAI14, pages 417–422, 2014.
[21] A. X. Jiang, Z. Yin, C. Zhang, M. Tambe, and
S. Kraus. Game-theoretic randomization for security
patrolling with dynamic execution uncertainty. In
Proceedings of the 2013 international conference on
Autonomous agents and multi-agent systems, pages
207–214. International Foundation for Autonomous
Agents and Multiagent Systems, 2013.
[22] K. Kauppi, A. Brandon-Jones, S. Ronchi, and E. van
Raaij. Tools without skills: Exploring the moderating
eﬀect of absorptive capacity on the relationship
between e-purchasing tools and category performance.
International Journal of Operations & Production
Management, 33(7):828–857, 2013.

[23] G. Kersten and H. Lai. Negotiation support and
e-negotiation systems: an overview. Group Decision
and Negotiation, 16(6):553–586, 2007.
[24] S. Kraus, P. Hoz-Weiss, J. Wilkenfeld, D. R.
Andersen, and A. Pate. Resolving crises through
automated bilateral negotiations. Artiﬁcial
Intelligence, 172(1):1–18, 2008.
[25] D. A. Lax and J. K. Sebenius. Thinking coalitionally:
party arithmetic, process opportunism, and strategic
sequencing. In H. P. Young, editor, Negotiation
Analysis, pages 153–193. The University of Michigan
Press, 1992.
[26] R. Lin, S. Kraus, J. Wilkenfeld, and J. Barry.
Negotiating with bounded rational agents in
environments with incomplete information using an
automated agent. Artiﬁcial Intelligence,
172(6):823–851, 2008.
[27] R. Lin, Y. Oshrat, and S. Kraus. Investigating the
beneﬁts of automated negotiations in enhancing
people’s negotiation skills. In Proceedings of the Eighth
International Joint Conference on Autonomous Agents
and Multiagent Systems (AAMAS-2009), pages
345–352, Budapest, Hungary, 2009.
[28] M. Maschler, E. Solan, and S. Zamir. Game Theory.
Cambridge University Press: Cambridge, England,
2013.
[29] R. D. McKelvey and T. R. Palfrey. An experimental
study of the centipede game. Econometrica,
60(4):803–836, 1992.
[30] T. H. Nguyen, R. Yang, A. Azaria, S. Kraus, and
M. Tambe. Analyzing the eﬀectiveness of adversary
modeling in security games. In Proc of AAAI 2013,
2013.
[31] M. J. Osborne and A. Rubinstein. A course in game
theory. MIT press, 1994.
[32] Y. Oshrat, R. Lin, and S. Kraus. Facing the challenge
of human-agent negotiations via eﬀective general
opponent modeling. In Proceedings of the Eighth
International Joint Conference on Autonomous Agents
and Multiagent Systems (AAMAS-2009), pages
377–384, Budapest, Hungary, 2009.
[33] P. Paruchuri, J. P. Pearce, J. Marecki, M. Tambe,
F. Ordonez, and S. Kraus. Playing games for security:
an eﬃcient exact algorithm for solving bayesian
stackelberg games. In Proceedings of the 7th
international joint conference on Autonomous agents
and multiagent systems-Volume 2, pages 895–902.
International Foundation for Autonomous Agents and
Multiagent Systems, 2008.
[34] N. Peled, K. Gal, and S. Kraus. A study of
computational and human strategies in revelation
games. Autonomous Agents and Multi-Agent Systems,
29(1):73–97, 2015.
[35] J. Pita, M. Jain, J. Marecki, F. Ord´o˜nez, C. Portway,
M. Tambe, C. Western, P. Paruchuri, and S. Kraus.
Deployed armor protection: the application of a game
theoretic model for security at the los angeles
international airport. In Proceedings of the 7th
international joint conference on Autonomous agents
and multiagent systems: industrial track, pages
125–132. International Foundation for Autonomous
Agents and Multiagent Systems, 2008.
[36] J. Pita, M. Jain, M. Tambe, F. Ord´o˜nez, and
S. Kraus. Robust solutions to stackelberg games:
Addressing bounded rationality and limited
observations in human cognition. Artiﬁcial
Intelligence, 174(15):1142–1171, 2010.
[37] J. Pita, M. Tambe, C. Kiekintveld, S. Cullen, and
E. Steigerwald. Guards - innovative application of
game theory for national airport security. In IJCAI
Proceedings-International Joint Conference on
Artiﬁcial Intelligence, page 2710, 2011.
[38] A. Rosenfeld and S. Kraus. Providing arguments in
discussions based on the prediction of human
argumentative behavior. In Proc. of AAAI, 2015.
[39] A. M. Spence. Market signaling: Informational
transfer in hiring and related screening processes,
volume 143. Harvard Univ Pr, 1974.
[40] J. Tsai, C. Kiekintveld, F. Ordonez, M. Tambe, and
S. Rathi. Iris-a tool for strategic security allocation in
transportation networks. In Proc. of AAMAS09, 2009.
[41] D. Walton. Argumentation theory: A very short
introduction. In Argumentation in artiﬁcial
intelligence, pages 1–22. Springer, 2009.
[42] J. Wilkenfeld, S. Kraus, K. M. Holley, and M. A.
Harris. Genie: A decision support system for crisis
negotiations. Decision Support Systems,
14(4):369–391, 1995.
[43] H. Xu, Z. Rabinovich, S. Dughmi, and M. Tambe.
Exploring information asymmetry in two-stage
security games. In Proc of AAAI, 2015.
[44] Z. Yin, A. X. Jiang, M. Tambe, C. Kiekintveld,
K. Leyton-Brown, T. Sandholm, and J. P. Sullivan.
Trusts: Scheduling randomized patrols for fare
inspection in transit systems using game theory. AI
Magazine, 33(4):59, 2012.

