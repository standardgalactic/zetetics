DRAFT Academic Questions, in press 
 
SCIENCE FICTIONS: How fraud, bias, negligence, and hype undermine 
the search for truth.  
A review of the book by Stuart Ritchie (New York: Metropolitan Books, 2020) 
by 
John Staddon 
Stuart Ritchie is a cognitive psychologist, a Lecturer at the University of London’s King’s 
College.  A few years ago he had an experience that seems to have been the impetus for this 
lively and important book.   
In 2011 Daryl Bem, a well-known social psychologist at Cornell University, published a series of 
experiments in a mainstream peer-review journal1. Bem claimed to have demonstrated 
precognition. He used a very simple procedure: A hundred subjects had to guess (36 trials each) 
which of two curtains (on a computer screen) concealed a randomly assigned picture. A third of 
the pictures were erotic, two-thirds were not. Subjects failed to guess correctly when the pictures 
were neutral but did better than chance when they were erotic. Since the guesses occurred before 
the pictures were presented, this counts as precognition. The effects were relatively small, but 
‘statistically significant’; 53.1% of choices correctly anticipated the erotic pictures, but only 
49.8% anticipated the non-erotic ones.  Nine similar experiments followed, eight of which 
showed significant results. This ‘breakthrough research’ created quite a stir.  
The experiments were simple and then-graduate-student Ritchie and a couple of collaborators 
each tried to replicate the first of them. They failed to find any evidence for precognition2.  
If true, Bem’s results would be an astonishing challenge to everything from physics to 
psychology. A failed replication should therefore be of the greatest interest — like the famous 
Michelson-Morley experiment which failed to find an expected change in the velocity of light 
and eventually led to Einstein’s special relativity theory. Sometimes a null result can have huge 
implications.  
 
1 Daryl J. Bem, Feeling the Future: Experimental Evidence for Anomalous Retroactive Influences on Cognition and 
Affect, Journal of Personality and Social Psychology 100, no. 3 (2011): pp. 407– 25; https:// doi.org/ 10.1037/ 
a0021524 
2 A battle ensued. Ritchie et al’s paper was eventually published. Bem and collaborators mustered considerable 
support, but overall the consensus is “Not Proven”. 

DRAFT Academic Questions, in press 
 
But not, apparently, for the editor of the Journal of Personality and Social Psychology, who 
rejected Ritchie et al.’s paper without review. In other words, standard policy for a prestigious 
psychology journal at that time was “no replications, please, we’re scientists”! 
In 2005, medical statistician John Ioannidis, in a paper provocatively entitled “Why most 
published research findings are false” had already highlighted what is now known as the 
replication crisis: the fact that a majority of research findings in biomedicine could not be 
replicated (not that very many attempts had been made!). Replication turned out to be a problem 
not just in biomedicine but also in psychology and many other areas. The issue has been 
reviewed at length by the National Association of Scholars3 and is now a general concern. But 
still, six years after Ioannides revelation, replication had apparently failed to catch the attention 
of the editor at JPSP.  
Science Fictions covers replication and many other ways that science can fail and is failing. 
Ritchie’s point is that the failures are to a large extent systemic. No one ever accused Bem of 
faking his data. The problem was partly the methods he used but chiefly the incentives to find 
something spectacular and the impediments to adequate criticism of the work.   
Ritchie begins pessimistically: 
[this book] reveals a deep corruption within science itself: a corruption that affects the very 
culture in which research is practised and published. Science, the discipline in which we 
should find the harshest scepticism, the most pin-sharp rationality and the hardest-headed 
empiricism, has become home to a dizzying array of incompetence, delusion, lies and self-
deception. In the process, the central purpose of science – to find our way ever closer to 
the truth – is being undermined. 
At the end of the book he recounts an admonition just like one I also received from a reviewer of 
my own book on Scientific Method: “Isn’t it irresponsible to write something like that? Won’t 
you encourage a free-for-all, where people use your arguments to justify their disbelief in 
evolution, or in the safety of vaccines, or in man-made global warming?” Well, no; as Ritchie 
points out, science is, or should be, all about criticism: “How do they know” should always be 
the response to any new claim.  Questions, criticism, should always be welcomed. Now they 
rarely are. 
 
3 https://www.nas.org/reports/the-irreproducibility-crisis-of-modern-science, see also 
https://en.wikipedia.org/wiki/Replication_crisis  

DRAFT Academic Questions, in press 
 
How bad is the problem? 
I thought that I was reasonably familiar with the science’s problems, but Ritchie’s book 
convinced me that I had greatly under-estimated them. Here are just a few examples: 
Nature, one of the two leading general-science journals, found that 52% of 1500 researchers who 
filled out a 2016 website survey thought there was a “significant crisis” of replicability.  
Obstacles to replication:  An attempt to replicate fifty-one important preclinical cancer studies 
was foiled when, “In every single one of the original papers, for every single one of the 
experiments reported, there wasn’t enough information provided for researchers to know how to 
re-run the experiment.” A later study of 268 biomedical papers again found only one that 
adequately reported what had been done.  
Some medical failures to replicate: Caesarean section safer for childbirth: Believed for years, but 
a big 2013 randomized trial found no difference. Peanut allergy: Guidelines for at-risk babies: 
avoid peanuts; 2015 randomized trial found opposite was best.  Heart attack: Cooling patient can 
help; 2014 study found the opposite. Stroke: Keep ‘em moving afterwards; er, no, 2016 study 
found the opposite: rest is better.  The vacillations of dietary diktats are now so frequent that 
even the woman-in-the-street is aware of them4.  
Fraud  A surprising number of ‘scientific’ results are simply faked. This can be dangerous in 
areas like biomedicine. The champ in this respect seems to be Italian surgeon Paulo Macchiarini, 
who claimed in many published papers to have developed a treatment that allowed him to 
successfully transplant artificial human tracheas. After painful patient deaths in several countries, 
Macchiarini was revealed as a data faker and a liar, claiming his research had succeeded and his 
patients had all done well, none of it true. Less lethal but prodigious in his publication of made-
up data, was Dutch social psychologist Diederik Stapel, who rode fashionable bandwagons (such 
as finding covert racism in novel contexts) in dozens of fake studies.  
Biomedical fraud is disturbingly widespread. Yoshihiro Sato in Japan “had fabricated data for 
dozens of clinical trials published in international journals.”5 Two instances of fraud at Duke 
University Medical Center have led to disciplinary action from government agencies and fines in 
the hundreds of millions of dollars. In 2018, Harvard Medical School and Brigham and Women’s 
Hospital in 2018 reported on 31 publications with “falsified and/or fabricated data”. Wikipedia 
 
4 https://quillette.com/2019/09/18/diet-reporting-the-real-fake-news/  
5 https://www.sciencemag.org/news/2018/08/researcher-center-epic-fraud-remains-enigma-those-who-exposed-him 

DRAFT Academic Questions, in press 
 
has an entry that lists scientific frauds in many areas and in many countries; the list for 
biomedicine is much the longest.  
Yes, science is in trouble. Error, some accidental, some via fraud, most of it a tacit response to 
perverse incentives which I will get to in a moment.   
The NHST Method 
Stuart Ritchie is a psychologist and is therefore most familiar with the Null Hypothesis Statistical 
Test method, which is favored in that area and in most social and biomedical science research. 
The method is now dominant but was not always so.  
The NHST method was invented when the single-subject method6 used in earlier experimental 
sciences like physics and chemistry seemed to be impractical. The single-subject method is still 
used in much of experimental psychology (as opposed to social, personality, clinical and even 
cognitive psychology).  For example, as a graduate student, I measured visual acuity under two 
conditions: white on black letters or the reverse, to see which gave the better acuity.  I needed 
only a handful of subjects, as the two conditions could be repeated and compared indefinitely 
within each subject.  Neither averaging nor statistics was necessary.  
It is worth noting that Bem’s ESP experiment used group-averaging, even though the 
phenomenon he was after is in fact better suited to the single-subject method. ESP, if it exists, is 
a faculty possessed by individual human beings, just like visual acuity. Averaging “ESP test” 
results across a group makes in fact no sense whatever.  The best way to demonstrate ESP would 
be to select a handful of people — perhaps people who already claim to have the ability — and 
then test them individually over many trials.   
Why didn’t Bem use this much more straightforward method? There are three possibilities: It 
didn’t fit the payment method for undergraduate subjects at Cornell; group-averaging had 
become a de-facto standard; or Bem tried his test first with one or two subjects, but they failed to 
show ESP.  
The single-subject method runs in to difficulties when the experimental treatment itself has an 
irreversible effect on the subject.  If you want to compare two methods of teaching kids to read, 
for example, you can’t teach using method A and have the kid unlearn so you can try method B.  
If you are very clever and persistent, like memory-research pioneer Herman Ebbinghaus, who 
 
6 Staddon, J. (2019)  The Object of Inquiry: Psychology’s Other (Non-replication) Problem  Acad. Quest. 32, 246-
256. 

DRAFT Academic Questions, in press 
 
discovered important laws of memory while using only himself as a subject, these history effects 
can sometimes be overcome.  But it is much easier just to compare two randomly selected (that 
bit is important!) groups of kids, one of which learns under A, the other under B.  
This is the NHST method: compare the average scores of two (or more) groups to see which 
treatment is best. But what if the scores overlap: A on average is better, but some B kids do 
better than some A kids? Is A really better, or could this degree of difference come about just by 
chance? This is where the problem gets tricky, because in order to answer the question you need 
to know just what by chance means in this context. This is not the place to go into the thorny 
thickets of statistical models7.  But I have said enough to allow me to illustrate some other 
problems of the NHST method. 
Given a suitable statistical model it is possible to use the variability of the results in the two 
groups to be compared and decide how likely it is that the mean difference obtained could have 
happened by chance.  R. A. Fisher, who pioneered the method, proposed that if the chance that 
the observed mean difference could have occurred even if the groups were the same, the p-value, 
is less than 5%, it is reasonable to reject the null hypothesis (that the two groups are from the 
same population) in favor of the hypothesis that they are in fact different. If p < .05, the 
experiment worked: you can assume that one treatment is really better than the other.   
But what is the correct p-value? 5% is completely arbitrary, after all (nevertheless it has become 
the standard8).  Fisher was working at an agricultural station at the time. He was dealing with 
practical comparisons between, say, two fertilizers. The farmer must make a decision: which one 
should he choose? If the two fertilizers do not differ in cost, a 5% probability of error seems 
reasonable. If, on the other hand, one is very much more costly than the other, then 5% might be 
too generous.  A full analysis would need to take the differential cost into account as well as how 
much more effective (the effect size) is the better fertilizer. If the cost differential is high in 
relation to the differential effectiveness, a much smaller p-value would be appropriate.  
Fisher’s method has been applied with little thought to much of social and biomedical science.  
But in a typical scientific experiment, unlike the practical decisions Fisher’s method was 
designed to resolve, in fact no decision needs to be made.  You have compared drug X against 
placebo Y and the result is insignificant. So you know that X is probably ineffective; but you do 
not know what is effective. An inconclusive result in the fertilizer experiment just means your 
 
7 See Staddon, John (2017) Scientific Method: How science works, fails to work or pretends to work.  Taylor and 
Francis, esp, Chapter 3, for a relatively simple account.  
8 p = .05 is popular possibly because it allows a sufficient number of accidental positives. With enough effort anyone 
can eventually get a publishable result even if every experiment is really null.  

DRAFT Academic Questions, in press 
 
choice is a toss-up; but you can at least make a decision. In the drug experiment, all you know is 
“try again”.  
There is a measurable cost to error in the fertilizer experiment.  In experiments like Bem’s ESP 
study or the very many social psychology experiments looking at hypothesized effects such as 
“implicit bias” or social attitudes the cost of finding an effect when there is none seems to be 
negligible. But the cost is in fact very high, for reasons that Ritchie points out. Subsequent 
studies will be based on the  error and will propagate it. The result is often a cross-cited body of 
organized misinformation which, if it happens to agree with existing prejudices (and it very often 
will), proliferates and corrupts the body of science. The cost of scientific error — borne by the 
field, if not by the perpetrator — can be very high indeed.  In response to this, one group of 
scientists has argued that the statistical significance should never be used as a deciding factor9; 
others, more generous, simply propose setting a much higher standard, say p < .001 vs. p < .05.  
The NHST method is prone to other errors.  Suppose, for example, that an epidemiologist is 
interested in the causes of depression. He thinks that, say “screen time” on mobile devices is the 
problem: too much screen time causes depression. He can’t do an experiment; he can’t hire a 
bunch of people and force them to spend X or Y amount of time on their phones. But he can 
measure (however inadequately) the amount of time people spend on their phones and correlate 
that with the amount of depressions they report. Being a conscientious fellow, he will also record 
their age sex, income and any other personal characteristics he can get a hold of.  
After all this work, suppose he finds that in fact depression is only weakly, insignificantly, 
correlated with screen time: should he give up? Not if he is like Brian Wansink, a discredited 
Cornell University food researcher10. Confronted with a similar situation, a study that failed to 
confirm his hypothesis, Wansink famously commented to a graduate student: “There’s got to be 
something here we can salvage.” Salvaging, for our epidemiologist, takes the form of looking for 
correlations other than the one with which he began. Suppose he finds that although screen time 
doesn’t work, income is significantly correlated with depression. What he should do, if he 
believes the (significant) correlation is not accidental, is use the correlation not as an excuse to 
publish, but as a new hypothesis to be independently tested. But in fact what Wansink, and many 
others like him in this situation, do is to write up the study as if the income-depression link was 
the hypothesis with which they began, a dishonest practice called p-hacking. If he is willing to do 
that, our duplicitous epidemiologist will also likely slip in the observation — to journalists if not 
 
9Nature COMMENT  20 MARCH 2019 Scientists rise up against statistical significance. Valentin Amrhein, Sander 
Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of 
possibly crucial effects. 
10 See also Scientific Method, op. cit. and Peer Review: the Publication Game and “the Natural Selection of Bad 
Science” 

DRAFT Academic Questions, in press 
 
journal editors — that low income causes depression, a completely unwarranted conclusion 
when he has done no experiment but only measured correlations.   
What should be done to correct these malpractices? Here I differ slightly with author Ritchie, 
who suggests, among other things, that scientists should abandon “small studies”, which are 
likely by chance to show spuriously large effects, in favor of large ones, with many subjects, 
simply because effects found in a large sample are more likely to be replicable. There are 
problems with this solution, however. True, a large sample has more ‘power’ hence allows for a 
more stringent p-criterion: .01 instead of .05, say. But in fact, researchers trying to publish large-
sample studies are very happy with the 5% criterion when a more stringent criterion might make 
the work unpublishable.  
A large sample and a lax p value has a bad feature: it allows weak effects (a drug that improves 
patient outcomes by 10% rather than 80%, say) to achieve significance and be published — and 
possibly approved by the FDA. In this way drug companies may get to market a drug that is little 
more effective than the one it supersedes but is a lot more costly for the patient.  
So, don’t publish a ‘p-hacked’ result and do worry about effect size as well as statistical 
significance. What then to do about a failed study? Ritchie points out that null results matter too, 
so why not publish them? Publication was certainly warranted in the case of the failed Bem 
replications, but is it always? Science is an evolutionary, trial-and-error process. There are many 
more ways to be wrong than right. I suspect that there are simply too many failed studies to 
publish. There are alternatives to the standard hard-copy publication route, which in any case has 
its own problems, many of which Ritchie discusses11.  But in general the function of a failed 
study is to guide the researcher’s future work — at least that’s how it has worked in the past. 
Scientists from Humphrey Davy and Marie Curie to B. F. Skinner tried out many ideas and 
techniques before coming up with their breakthroughs: new elements in the case of Davy and 
Curie, reinforcement schedules in the case of Skinner. None of these early missteps were 
separately published. The proper course after a failed study or even a ‘significant” study which 
nevertheless has exceptions is to keep trying until you find the reasons for essentially all the 
exceptions12. So, my guess is that for basic (as opposed to applied) science, a strategy of small 
but focused studies is likely in the long run to be more fruitful than the approach of costly and 
infrequent large studies, especially because mega-projects are likely to be cautious if not 
unimaginative.  
 
11 See also How Is Science Judged? How Useful Is Peer Review?   
12 For more on this see The Object of Inquiry, op. cit. 

DRAFT Academic Questions, in press 
 
I end this section on a note of profundity: there is no ‘science algorithm’, no gold standard 
scientific method.   
Incentives, the real problem 
A solution to the p-hacking problem that has become quite popular also shows the problem at the 
heart of bad science: “From 2005, the International Committee of Medical Journal Editors, 
recognising the massive problem of publication bias13, ruled that all human clinical trials should 
be publicly registered before they take place – otherwise they wouldn’t be allowed to be 
published in most top medical journals. [my emphasis]”  The idea is that the hypothesis to be 
tested should be made public before study is begun, so it can be compared with the hypothesis 
tested in the final manuscript — to see that no p-hacking has occurred.  
This sounds like a good solution and registration has been widely adopted14. But think for a 
moment about what it implies about the motivations of many scientists: Why would any 
competent scientist who knows about the p-hacking problem, knows that it leads not to truth but 
to falsehood hence knows why it should be avoided, why should he or she need to be regulated 
into honesty? It is almost as if scientists were to be legally enjoined to do their math correctly 
and describe their procedures accurately. External regulation of the details of scientific practice 
is a potential death blow to the spontaneous creativity that is the essence of great science. That 
regulation is now thought necessary points to a rot at its heart.  
The rot has to do with why people become scientists and the conditions under which they work. 
In past times, science was often a vocation, done for love of the subject not to make a living. Not 
that ambition played no part: Isaac Newton fought bitterly for priority with rivals like Leibniz 
and Hooke.  Even cautious and retiring Charles Darwin was devastated at the thought that Alfred 
Russel Wallace’s paper on natural selection might scoop him. But now science has become a 
career for most scientists. The ways they are rewarded and their research is supported have 
promoted much of the bad behavior that Stuart Ritchie so ably documents.  
Science has become a career rather than a vocation. Scientists paid a salary must be evaluated for 
raises and promotion. Evaluation is very difficult and has become more so as scientific 
disciplines have grown ever more specialized and the number of scientific journals has swelled 
into the tens of thousands. The mid-19th century era of the natural philosopher, with a broad 
understanding of the whole of science, has become essentially impossible. Grant proposals, 
 
13 The reference here is to the so-called “file-drawer problem”, where failed experiments are never reported, giving 
published positives unwarranted credibility.  
14 Chris Chambers The seven deadly sins of psychology: A manifesto for reforming the culture of scientific practice. 
Princeton University Press, 2017. 

DRAFT Academic Questions, in press 
 
academic promotions and new hires are generally scrutinized by committees. Only rarely does 
such a committee contain individuals from outside the candidate’s sub-specialty.  Routine 
decisions about salary must rely on proxies, measurable features of the person’s CV such as 
number and citation-rate of his publications and the reputation of the journals involved.  
Proxies for research excellence have numerous problems.  Citations are an obvious example: A 
paper that makes an error may gain more citations than the paper that corrects it15. An excellent  
paper in a small field will usually get fewer citations than a mediocre paper in a large one.  
Ritchie points out a fatal problem with all evaluation-by-proxy: proxies can be “gamed”. If 
number of publications is important, career-driven scientists will turn to the “LPU strategy”16, 
splitting their product into the largest number of separately publishable packages.  If shared 
authorship counts, the number of multi-author papers will increase.  Ritchie describes the travails 
of famous psychologist Robert Sternberg, onetime president of the American Psychological 
Association, who had to step down from a prestigious editorship after being criticized for self-
citation, self-plagiarism and other practices aimed at increasing his publication and citation 
counts.  Sternberg’s reputation seems to have suffered little from what are now regarded as 
minor missteps.  
In addition to incentives created by the system there are also self-generated incentives. Excessive 
faith in a favorite theory has led many able scientists to “spin” their data or experimental design  
in ways that can end up being dishonest. Indeed, the only two frauds known to me personally 
have involved bending data to fit a favored theory: both were senior professors at respected 
institutions. It was fame rather than job security they were after.  
Finally there is the number of scientists. Science as a profession has grown exponentially over 
the past 100 years or so; 90% of all the scientists that have ever lived are alive today. The 
question is: are there enough solvable scientific problems available to keep them all usefully 
occupied?  This problem is rarely mentioned17, perhaps because at the crest of the most recent 
burst of scientific discovery in 1945, top science administrator Vannevar Bush came out with his 
influential report Science, the Endless Frontier. Since then the possibilities of discovery have 
been assumed to be unlimited.  But are they? Believers will point out that at the end of the 19th 
century physicist Lord Kelvin is reported to have said (though there is some dispute) “There is 
nothing new to be discovered in physics now. All that remains is more and more precise 
 
15 Diederik Stapel has 10,412 Google Scholar citations; Stuart Ritchie has 4617 (August 5, 2020). 
16 “Least Publishable Unit”, by faux-science analogy with the ”British Thermal Unit” etc.   
17 But see Science and Its Discontents: Too Few Jobs—or Too Many Scientists? 

DRAFT Academic Questions, in press 
 
measurement.” He was completely wrong, of course: quantum mechanics and relativity followed 
a few years after.  
But will Kelvin always be wrong, especially about the social sciences: just how many solvable 
questions are available at any time?  The number in physics is limited to some extent by existing 
theory — it took a social scientist to try and demonstrate ESP even though it challenges physical 
laws. A weak study like Bem’s could never make it in physics. The “softer” social sciences lack 
a firm theoretical structure, so that a bright researcher can always come up with a new term and 
purport to test for it, even though it may not exist at all.  Current examples are things like the 
self-system, white fragility, implicit bias, color-blind racism, stereotype threat and the like.  The 
point is that as the proper motivation for science — curiosity and the desire to understand nature 
—is overshadowed by ideology and the careerist incentives Ritchie describes, there is little to 
stop the steady decay of the sciences into tools of activism18.  
* * * 
Stuart Ritchie has written a thoughtful, well-researched and surprisingly readable book on a 
difficult but hugely important topic. Science, and the freedom of inquiry on which it depends, is 
at the heart of Western civilization. It is perhaps no coincidence that the very words “Western 
civilization” are now taboo on many campuses. Science Fictions can help us understand how 
corrupt science has become. To cure this corruption we must also understand the social forces 
that have brought it about. Perhaps that will be a topic for Ritchie’s next book.  
 
18 See, for example, https://quillette.com/2018/10/07/the-devolution-of-social-science/ and 
https://www.theamericanconservative.com/dreher/wokeness-endarkenment-lysenkoism-cult-of-social-justice-
science/ 
,  

