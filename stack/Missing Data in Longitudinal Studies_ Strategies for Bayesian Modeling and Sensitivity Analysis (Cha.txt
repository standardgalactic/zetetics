
Missing Data in
Longitudinal Studies
Strategies for Bayesian Modeling
and Sensitivity Analysis

MONOGRAPHS ON STATISTICS AND APPLIED PROBABILITY
General Editors
J. Fan, V. Isham, N. Keiding, T. Louis, R. L. Smith, and H. Tong
1 Stochastic Population Models in Ecology and Epidemiology M.S. Barlett (1960)
2 Queues D.R. Cox and W.L. Smith (1961)
3 Monte Carlo Methods J.M. Hammersley and D.C. Handscomb (1964)
4 The Statistical Analysis of Series of Events D.R. Cox and P.A.W. Lewis (1966)
5 Population Genetics W.J. Ewens (1969)
6 Probability, Statistics and Time M.S. Barlett (1975)
7 Statistical Inference S.D. Silvey (1975)
8 The Analysis of Contingency Tables B.S. Everitt (1977)
9 Multivariate Analysis in Behavioural Research A.E. Maxwell (1977)
10 Stochastic Abundance Models S. Engen (1978)
11 Some Basic Theory for Statistical Inference E.J.G. Pitman (1979)
12 Point Processes D.R. Cox and V. Isham (1980)
13 Identification of Outliers D.M. Hawkins (1980)
14 Optimal Design S.D. Silvey (1980)
15 Finite Mixture Distributions B.S. Everitt and D.J. Hand (1981)
16 Classification A.D. Gordon (1981)
17 Distribution-Free Statistical Methods, 2nd edition J.S. Maritz (1995)
18 Residuals and Influence in Regression R.D. Cook and S. Weisberg (1982)
19 Applications of Queueing Theory, 2nd edition G.F. Newell (1982)
20 Risk Theory, 3rd edition R.E. Beard, T. Pentikäinen and E. Pesonen (1984)
21 Analysis of Survival Data D.R. Cox and D. Oakes (1984)
22 An Introduction to Latent Variable Models B.S. Everitt (1984)
23 Bandit Problems D.A. Berry and B. Fristedt (1985)
24 Stochastic Modelling and Control M.H.A. Davis and R. Vinter (1985)
25 The Statistical Analysis of Composition Data J. Aitchison (1986)
26 Density Estimation for Statistics and Data Analysis B.W. Silverman (1986)
27 Regression Analysis with Applications G.B. Wetherill (1986)
28 Sequential Methods in Statistics, 3rd edition
G.B. Wetherill and K.D. Glazebrook (1986)
29 Tensor Methods in Statistics P. McCullagh (1987)
30 Transformation and Weighting in Regression
R.J. Carroll and D. Ruppert (1988)
31 Asymptotic Techniques for Use in Statistics
O.E. Bandorff-Nielsen and D.R. Cox (1989)
32 Analysis of Binary Data, 2nd edition D.R. Cox and E.J. Snell (1989)
33 Analysis of Infectious Disease Data N.G. Becker (1989)
 34 Design and Analysis of Cross-Over Trials B. Jones and M.G. Kenward (1989)
35 Empirical Bayes Methods, 2nd edition J.S. Maritz and T. Lwin (1989)
36 Symmetric Multivariate and Related Distributions
K.T. Fang, S. Kotz and K.W. Ng (1990)
37 Generalized Linear Models, 2nd edition P. McCullagh and J.A. Nelder (1989)
38 Cyclic and Computer Generated Designs, 2nd edition
J.A. John and E.R. Williams (1995)
39 Analog Estimation Methods in Econometrics C.F. Manski (1988)
40 Subset Selection in Regression  A.J. Miller (1990)
41 Analysis of Repeated Measures M.J. Crowder and D.J. Hand (1990)
42  Statistical Reasoning with Imprecise Probabilities P. Walley (1991)
43 Generalized Additive Models T.J. Hastie and R.J. Tibshirani (1990)

44 Inspection Errors for Attributes in Quality Control
N.L. Johnson, S. Kotz and X. Wu (1991)
45 The Analysis of Contingency Tables, 2nd edition B.S. Everitt (1992)
46 The Analysis of Quantal Response Data B.J.T. Morgan (1992)
47 Longitudinal Data with Serial Correlation—A State-Space Approach
R.H. Jones (1993)
48 Differential Geometry and Statistics M.K. Murray and J.W. Rice (1993)
49 Markov Models and Optimization M.H.A. Davis (1993)
50 Networks and Chaos—Statistical and Probabilistic Aspects
O.E. Barndorff-Nielsen, J.L. Jensen and W.S. Kendall (1993)
51 Number-Theoretic Methods in Statistics K.-T. Fang and Y. Wang (1994)
52 Inference and Asymptotics O.E. Barndorff-Nielsen and D.R. Cox (1994)
53 Practical Risk Theory for Actuaries
C.D. Daykin, T. Pentikäinen and M. Pesonen (1994)
54 Biplots J.C. Gower and D.J. Hand (1996)
55 Predictive Inference—An Introduction  S. Geisser (1993)
56 Model-Free Curve Estimation M.E. Tarter and M.D. Lock (1993)
57 An Introduction to the Bootstrap B. Efron and R.J. Tibshirani (1993)
58 Nonparametric Regression and Generalized Linear Models
P.J. Green and B.W. Silverman (1994)
59 Multidimensional Scaling T.F. Cox and M.A.A. Cox (1994)
60 Kernel Smoothing M.P. Wand and M.C. Jones (1995)
61 Statistics for Long Memory Processes J. Beran (1995)
62 Nonlinear Models for Repeated Measurement Data
M. Davidian and D.M. Giltinan (1995)
63 Measurement Error in Nonlinear Models
R.J. Carroll, D. Rupert and L.A. Stefanski (1995)
64 Analyzing and Modeling Rank Data J.J. Marden (1995)
65 Time Series Models—In Econometrics, Finance and Other Fields
D.R. Cox, D.V. Hinkley and O.E. Barndorff-Nielsen (1996)
66 Local Polynomial Modeling and its Applications J. Fan and I. Gijbels (1996)
67 Multivariate Dependencies—Models, Analysis and Interpretation
D.R. Cox and N. Wermuth (1996)
68 Statistical Inference—Based on the Likelihood  A. Azzalini (1996)
69 Bayes and Empirical Bayes Methods for Data Analysis
B.P. Carlin and T.A Louis (1996)
70 Hidden Markov and Other Models for Discrete-Valued Time Series
I.L. Macdonald and W. Zucchini (1997)
71 Statistical Evidence—A Likelihood Paradigm R. Royall (1997)
72 Analysis of Incomplete Multivariate Data J.L. Schafer (1997)
73 Multivariate Models and Dependence Concepts H. Joe (1997)
74 Theory of Sample Surveys M.E. Thompson (1997)
75 Retrial Queues G. Falin and J.G.C. Templeton (1997)
76 Theory of Dispersion Models B. Jørgensen (1997)
77 Mixed Poisson Processes J. Grandell (1997)
78 Variance Components Estimation—Mixed Models, Methodologies and Applications
P.S.R.S. Rao (1997)
79 Bayesian Methods for Finite Population Sampling
G. Meeden and M. Ghosh (1997)
80 Stochastic Geometry—Likelihood and computation
O.E. Barndorff-Nielsen, W.S. Kendall and M.N.M. van Lieshout (1998)
81 Computer-Assisted Analysis of Mixtures and Applications—
Meta-analysis, Disease Mapping and Others D. Böhning (1999)
82 Classification, 2nd edition A.D. Gordon (1999)

83 Semimartingales and their Statistical Inference B.L.S. Prakasa Rao (1999)
84 Statistical Aspects of BSE and vCJD—Models for Epidemics
C.A. Donnelly and N.M. Ferguson (1999)
85 Set-Indexed Martingales G. Ivanoff and E. Merzbach (2000)
86 The Theory of the Design of Experiments D.R. Cox and N. Reid (2000)
87 Complex Stochastic Systems
O.E. Barndorff-Nielsen, D.R. Cox and C. Klüppelberg (2001)
88 Multidimensional Scaling, 2nd edition T.F. Cox and M.A.A. Cox (2001)
89 Algebraic Statistics—Computational Commutative Algebra in Statistics
G. Pistone, E. Riccomagno and H.P. Wynn  (2001)
90 Analysis of Time Series Structure—SSA and Related Techniques
N. Golyandina, V. Nekrutkin and A.A. Zhigljavsky  (2001)
91 Subjective Probability Models for Lifetimes
Fabio Spizzichino  (2001)
92 Empirical Likelihood Art B. Owen  (2001)
93 Statistics in the 21st Century
Adrian E. Raftery, Martin A. Tanner, and Martin T. Wells  (2001)
94 Accelerated Life Models: Modeling and Statistical Analysis
Vilijandas Bagdonavicius and Mikhail Nikulin  (2001)
95 Subset Selection in Regression, Second Edition Alan Miller  (2002)
96 Topics in Modelling of Clustered Data
Marc Aerts, Helena Geys, Geert Molenberghs, and Louise M. Ryan  (2002)
97 Components of Variance D.R. Cox and P.J. Solomon (2002)
98 Design and Analysis of Cross-Over Trials, 2nd Edition
Byron Jones and Michael G. Kenward (2003)
99 Extreme Values in Finance, Telecommunications, and the Environment
Bärbel Finkenstädt and Holger Rootzén (2003)
100 Statistical Inference and Simulation for Spatial Point Processes
Jesper Møller and Rasmus Plenge Waagepetersen (2004)
101 Hierarchical Modeling and Analysis for Spatial Data
Sudipto Banerjee, Bradley P. Carlin, and Alan E. Gelfand (2004)
102 Diagnostic Checks in Time Series Wai Keung Li (2004)
103 Stereology for Statisticians Adrian Baddeley and Eva B. Vedel Jensen (2004)
104 Gaussian Markov Random Fields: Theory and Applications
Havard Rue and Leonhard Held (2005)
105 Measurement Error in Nonlinear Models: A Modern Perspective, Second Edition
Raymond J. Carroll, David Ruppert, Leonard A. Stefanski,
and Ciprian M. Crainiceanu (2006)
106 Generalized Linear Models with Random Effects: Unified Analysis via H-likelihood
Youngjo Lee, John A. Nelder, and Yudi Pawitan (2006)
107 Statistical Methods for Spatio-Temporal Systems
Bärbel Finkenstädt, Leonhard Held, and Valerie Isham (2007)
108 Nonlinear Time Series: Semiparametric and Nonparametric Methods
Jiti Gao (2007)
109 Missing Data in Longitudinal Studies: Strategies for Bayesian Modeling and Sensitivity Analysis
Michael J. Daniels and Joseph W. Hogan (2008)

Michael J. Daniels
University of Florida
Gainesville, U.S.A.
Missing Data in
Longitudinal Studies
Strategies for Bayesian Modeling
and Sensitivity Analysis
Joseph W. Hogan
Brown University
Providence, Rhode Island, U.S.A.
Monographs on Statistics and Applied Probability 109
Boca Raton   London   New York
Chapman & Hall/CRC is an imprint of the
Taylor & Francis Group, an informa business

Chapman & Hall/CRC
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2008 by Taylor & Francis Group, LLC 
Chapman & Hall/CRC is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Printed in the United States of America on acid-free paper
10 9 8 7 6 5 4 3 2 1
International Standard Book Number-13: 978-1-58488-609-9 (Hardcover)
This book contains information obtained from authentic and highly regarded sources  Reason-
able efforts have been made to publish reliable data and information, but the author and publisher 
cannot assume responsibility for the validity of all materials or the consequences of their use. The 
Authors and Publishers have attempted to trace the copyright holders of all material reproduced 
in this publication and apologize to copyright holders if permission to publish in this form has not 
been obtained. If any copyright material has not been acknowledged please write and let us know so 
we may rectify in any future reprint 
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, 
transmitted, or utilized in any form by any electronic, mechanical, or other means, now known or 
hereafter invented, including photocopying, microfilming, and recording, or in any information 
storage or retrieval system, without written permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.
copyright.com (http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC) 
222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that 
provides licenses and registration for a variety of users. For organizations that have been granted a 
photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and 
are used only for identification and explanation without intent to infringe.
Library of Congress Cataloging-in-Publication Data
Daniels, M. J.
Missing data in longitudinal studies : strategies for Bayesian modeling and 
sensitivity analysis / Michael J. Daniels and Joseph W. Hogan.
p. cm. -- (Monographs on statistics and applied probability ; 109)
Includes bibliographical references and index.
ISBN 978-1-58488-609-9 (alk. paper)
1. Missing observations (Statistics) 2. Longitudinal method. 3. Sensitivity 
theory (Mathematics) 4. Bayesian statistical decision theory. I. Hogan, Joseph W. 
II. Title. III. Series.
QA276.D3146 2007
519.5--dc22 
2007040408
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com

Missing Data
in Longitudinal Studies:
Strategies for Bayesian Modeling and
Sensitivity Analysis
Michael J. Daniels
University of Florida
Joseph W. Hogan
Brown University

To Marie, Mary and Mia (M.J.D.)
To Dawn, Jack, Luke and Patrick (J.W.H.)

Contents
Preface
xvii
1
Description of Motivating Examples
1
1.1
Overview
1
1.2
Dose-ﬁnding trial of an experimental treatment for schizophre-
nia
2
1.2.1
Study and data
2
1.2.2
Questions of interest
2
1.2.3
Missing data
2
1.2.4
Data analyses
2
1.3
Clinical trial of recombinant human growth hormone (rhGH)
for increasing muscle strength in the elderly
4
1.3.1
Study and data
4
1.3.2
Questions of interest
4
1.3.3
Missing data
4
1.3.4
Data analyses
5
1.4
Clinical trials of exercise as an aid to smoking cessation in
women: the Commit to Quit studies
6
1.4.1
Studies and data
6
1.4.2
Questions of interest
6
1.4.3
Missing data
7
1.4.4
Data analyses
8
1.5
Natural history of HIV infection in women: HIV Epidemiology
Research Study (HERS) cohort
9
1.5.1
Study and data
9
1.5.2
Questions of interest
9
1.5.3
Missing data
9
1.5.4
Data analyses
10
1.6
Clinical trial of smoking cessation among substance abusers:
OASIS study
11
1.6.1
Study and data
11
1.6.2
Questions of interest
11
1.6.3
Missing data
12
1.6.4
Data analyses
12
ix

x
CONTENTS
1.7
Equivalence trial of competing doses of AZT in HIV-infected
children: Protocol 128 of the AIDS Clinical Trials Group
13
1.7.1
Study and data
13
1.7.2
Questions of interest
14
1.7.3
Missing data
14
1.7.4
Data analyses
14
2
Regression Models
15
2.1
Overview
15
2.2
Preliminaries
15
2.2.1
Longitudinal data
15
2.2.2
Regression models
17
2.2.3
Full vs. observed data
18
2.2.4
Additional notation
19
2.3
Generalized linear models
19
2.4
Conditionally speciﬁed models
20
2.4.1
Random eﬀects models based on GLMs
21
2.4.2
Random eﬀects models for continuous response
22
2.4.3
Random eﬀects models for discrete responses
23
2.5
Directly speciﬁed (marginal) models
25
2.5.1
Multivariate normal and Gaussian process models
26
2.5.2
Directly speciﬁed models for discrete longitudinal
responses
28
2.6
Semiparametric regression
31
2.6.1
Generalized additive models based on regression splines
32
2.6.2
Varying coeﬃcient models
34
2.7
Interpreting covariate eﬀects
34
2.7.1
Assumptions regarding time-varying covariates
35
2.7.2
Longitudinal vs. cross-sectional eﬀects
36
2.7.3
Marginal vs. conditional eﬀects
37
2.8
Further reading
38
3
Methods of Bayesian Inference
39
3.1
Overview
39
3.2
Likelihood and posterior distribution
39
3.2.1
Likelihood
39
3.2.2
Score function and information matrix
41
3.2.3
The posterior distribution
42
3.3
Prior Distributions
43
3.3.1
Conjugate priors
43
3.3.2
Noninformative priors
46
3.3.3
Informative priors
49
3.3.4
Identiﬁability and incomplete data
50

CONTENTS
xi
3.4
Computation of the posterior distribution
51
3.4.1
The Gibbs sampler
52
3.4.2
The Metropolis-Hastings algorithm
54
3.4.3
Data augmentation
55
3.4.4
Inference using the posterior sample
58
3.5
Model comparisons and assessing model ﬁt
62
3.5.1
Deviance Information Criterion (DIC)
63
3.5.2
Posterior predictive loss
65
3.5.3
Posterior predictive checks
67
3.6
Nonparametric Bayes
68
3.7
Further reading
69
4
Worked Examples using Complete Data
72
4.1
Overview
72
4.2
Multivariate normal model: Growth Hormone study
72
4.2.1
Models
72
4.2.2
Priors
73
4.2.3
MCMC details
73
4.2.4
Model selection and ﬁt
73
4.2.5
Results
74
4.2.6
Conclusions
75
4.3
Normal random eﬀects model: Schizophrenia trial
75
4.3.1
Models
76
4.3.2
Priors
77
4.3.3
MCMC details
77
4.3.4
Results
77
4.3.5
Conclusions
78
4.4
Models for longitudinal binary data: CTQ I Study
79
4.4.1
Models
80
4.4.2
Priors
81
4.4.3
MCMC details
81
4.4.4
Model selection
81
4.4.5
Results
82
4.4.6
Conclusions
83
4.5
Summary
84
5
Missing Data Mechanisms and Longitudinal Data
85
5.1
Introduction
85
5.2
Full vs. observed data
86
5.2.1
Overview
86
5.2.2
Data structures
87
5.2.3
Dropout and other processes leading to missing re-
sponses
87

xii
CONTENTS
5.3
Full-data models and missing data mechanisms
89
5.3.1
Targets of inference
89
5.3.2
Missing data mechanisms
90
5.4
Assumptions about missing data mechanism
91
5.4.1
Missing completely at random (MCAR)
91
5.4.2
Missing at random (MAR)
93
5.4.3
Missing not at random (MNAR)
93
5.4.4
Auxiliary variables
94
5.5
Missing at random applied to dropout processes
96
5.6
Observed data posterior of full-data parameters
98
5.7
The ignorability assumption
99
5.7.1
Likelihood and posterior under ignorability
99
5.7.2
Factored likelihood with monotone ignorable missing-
ness
101
5.7.3
The practical meaning of ‘ignorability’
102
5.8
Examples of full-data models under MAR
103
5.9
Full-data models under MNAR
106
5.9.1
Selection models
107
5.9.2
Mixture models
109
5.9.3
Shared parameter models
112
5.10 Summary
114
5.11 Further reading
114
6
Inference about Full-Data Parameters under Ignorability
115
6.1
Overview
115
6.2
General issues in model speciﬁcation
116
6.2.1
Mis-speciﬁcation of dependence
116
6.2.2
Orthogonal parameters
118
6.3
Posterior sampling using data augmentation
121
6.4
Covariance structures for univariate longitudinal processes
124
6.4.1
Serial correlation models
124
6.4.2
Covariance matrices induced by random eﬀects
128
6.4.3
Covariance functions for misaligned data
129
6.5
Covariate-dependent covariance structures
130
6.5.1
Covariance/correlation matrices
130
6.5.2
Dependence in longitudinal binary models
134
6.6
Joint models for multivariate processes
134
6.6.1
Continuous response and continuous auxiliary covariate
135
6.6.2
Binary response and binary auxiliary covariate
137
6.6.3
Binary response and continuous auxiliary covariate
138
6.7
Model selection and model ﬁt under ignorability
138
6.7.1
Deviance information criterion (DIC)
139
6.7.2
Posterior predictive checks
141

CONTENTS
xiii
6.8
Further reading
143
7
Case Studies: Ignorable Missingness
145
7.1
Overview
145
7.2
Structured covariance matrices: Growth Hormone study
145
7.2.1
Models
145
7.2.2
Priors
146
7.2.3
MCMC details
146
7.2.4
Model selection and ﬁt
147
7.2.5
Results and comparison with completers-only analysis
147
7.2.6
Conclusions
149
7.3
Normal random eﬀects model: Schizophrenia trial
149
7.3.1
Models and priors
149
7.3.2
MCMC details
150
7.3.3
Model selection
150
7.3.4
Results and comparison with completers-only analysis
150
7.3.5
Conclusions
151
7.4
Marginalized transition model: CTQ I trial
151
7.4.1
Models
152
7.4.2
MCMC details
153
7.4.3
Model selection
153
7.4.4
Results
154
7.4.5
Conclusions
154
7.5
Joint modeling with auxiliary variables: CTQ II trial
155
7.5.1
Models
156
7.5.2
Priors
157
7.5.3
Posterior sampling
157
7.5.4
Model selection and ﬁt
157
7.5.5
Results
158
7.5.6
Conclusions
159
7.6
Bayesian p-spline model: HERS CD4 data
159
7.6.1
Models
160
7.6.2
Priors
161
7.6.3
MCMC details
161
7.6.4
Model selection
161
7.6.5
Results
161
7.7
Summary
162
8
Models for Handling Nonignorable Missingness
165
8.1
Overview
165
8.2
Extrapolation factorization and sensitivity parameters
166
8.3
Selection models
167
8.3.1
Background and history
167

xiv
CONTENTS
8.3.2
Absence of sensitivity parameters in the missing data
mechanism
168
8.3.3
Heckman selection model for a bivariate response
171
8.3.4
Speciﬁcation of the missing data mechanism for longi-
tudinal data
173
8.3.5
Parametric selection models for longitudinal data
174
8.3.6
Feasibility of sensitivity analysis for parametric selection
models
175
8.3.7
Semiparametric selection models
176
8.3.8
Posterior sampling strategies
180
8.3.9
Summary of pros and cons of selection models
181
8.4
Mixture models
181
8.4.1
Background, speciﬁcation, and identiﬁcation
181
8.4.2
Identiﬁcation strategies for mixture models
183
8.4.3
Mixture models with discrete-time dropout
188
8.4.4
Mixture models with continuous-time dropout
198
8.4.5
Combinations of MAR and MNAR dropout
201
8.4.6
Mixture models or selection models?
202
8.4.7
Covariate eﬀects in mixture models
203
8.5
Shared parameter models
206
8.5.1
General structure
206
8.5.2
Pros and cons of shared parameter models
207
8.6
Model selection and model ﬁt in nonignorable models
209
8.6.1
Deviance information criterion (DIC)
209
8.6.2
Posterior predictive checks
213
8.7
Further reading
215
9
Informative Priors and Sensitivity Analysis
216
9.1
Overview
216
9.1.1
General approach
216
9.1.2
Global vs. local sensitivity analysis
217
9.2
Some principles
219
9.3
Parameterizing the full-data model
220
9.4
Specifying priors
222
9.5
Pattern mixture models
224
9.5.1
General parameterization
224
9.5.2
Using model constraints to reduce dimensionality of
sensitivity parameters
225
9.6
Selection models
226
9.7
Further reading
231
10 Case Studies: Nonignorable Missingness
233
10.1 Overview
233

CONTENTS
xv
10.2 Growth Hormone study: Pattern mixture models and sensitiv-
ity analysis
234
10.2.1 Overview
234
10.2.2 Multivariate normal model under ignorability
234
10.2.3 Pattern mixture model speciﬁcation
235
10.2.4 MAR constraints for pattern mixture model
235
10.2.5 Parameterizing departures from MAR
236
10.2.6 Constructing priors
238
10.2.7 Analysis using point mass MAR prior
238
10.2.8 Analyses using MNAR priors
239
10.2.9 Summary of pattern mixture analysis
246
10.3 OASIS Study: Selection models, mixture models, and elicited
priors
248
10.3.1 Overview
248
10.3.2 Selection model speciﬁcation
249
10.3.3 Selection model analyses under MAR and MNAR
251
10.3.4 Pattern mixture model speciﬁcation
252
10.3.5 MAR and MNAR parameterizations
252
10.3.6 Pattern mixture analysis under MAR
255
10.3.7 Pattern mixture analysis under MNAR using elicited
priors
255
10.3.8 Summary: selection vs. pattern mixture approaches
259
10.4 Pediatric AIDS trial: Mixture of varying coeﬃcient models for
continuous dropout
261
10.4.1 Overview
261
10.4.2 Model speciﬁcation: CD4 counts
263
10.4.3 Model speciﬁcation: dropout times
265
10.4.4 Summary of analyses under MAR and MNAR
265
10.4.5 Summary
266
Distributions
268
Bibliography
271
Author Index
292
Index
298


Preface
Motivations
A considerable amount of research in public health, medicine, and the life and
social sciences is driven by longitudinal studies that collect repeated obser-
vations over time. An almost inevitable complication in drawing inferences is
that data will be missing. For studies with long follow up periods, the pro-
portion of individuals with missing data can be substantial.
Despite a vast and growing literature, and recent advances in statistical
software, the appropriate handling of missing data in longitudinal studies re-
mains a vexing statistical problem, and has enough complexity to make it a
worthy topic of study in its own right; more than just the standard set of
longitudinal modeling and missing data tools are needed.
With repeated measures, the best information about missing responses fre-
quently comes from observed responses on the same variable; hence methods
for covariance modeling take on heightened importance. Missingness in longi-
tudinal studies is frequently a result of dropout, mortality and other processes
that are well characterized as event times; hence methods for joint modeling
of repeated measures and event times play a central role.
And as with any analysis of incomplete data, parameters or models of inter-
est cannot be inferred without resorting to assumptions that employ various
degrees of subjective judgment. All too often, the assumptions used for data
analysis are motivated more by convenience than by substance; this seems
especially true for longitudinal data, where the models or data structures are
complex enough on their own without the added complication of missing data.
Hence a principled approach is needed for formalizing subjective assumptions
in contextually interpretable ways.
The Bayesian approach
When data are incomplete, inferences about parameters of interest cannot be
carried out without the beneﬁt of subjective assumptions about the distribu-
tion of missing responses. They are subjective because data cannot be used
to critique them. Some of these assumptions are used with such regularity
that we forget they are being made; for example, when commercial software
such as SAS or Stata is used to analyze incomplete longitudinal data using
a random eﬀects model, the missing at random (MAR) assumption is being
used; when the Kaplan-Meier estimator is used to summarize a survival curve
xvii

xviii
PREFACE
from censored event times, non-informative censoring is being assumed. Nei-
ther assumption can be formally checked, so the validity of inferences relies
on subjective judgment.
For this reason, it is our view that analysis of incomplete data cries out
for the Bayesian approach in order to formalize the subjective component of
inference. Speciﬁcally, we argue that subjective assumptions should be formu-
lated in terms of prior distributions for parameters that index the conditional
distribution of missing data given observed data. In this framework, default
assumptions such as missing at random can be encoded using point mass pri-
ors, or assumptions can be elaborated by centering priors away from MAR,
introducing uncertainty about the assumptions, or both.
Ideally, the role of the statistician is to write down a model that enables
critical thinking about the missing data assumptions, based on questions like
these: Can missing data be predicted from observables using an extrapolation?
Are dropouts systematically diﬀerent from completers? If so, how? And what
is the degree of uncertainty about the assumptions?
This is not a new idea (see Rubin, 1977), but in our reading of the lit-
erature it is surprisingly underutilized. One possible reason is the diﬃculty
associated with parameterizing complex models so that missing data assump-
tions are coherently represented with priors. Another potential reason is lack
of knowledge about how to use appropriate software. With this in mind, we
have included 11 worked examples and case studies. These illustrate con-
cepts ranging from basic modeling of repeated measures to structured mod-
els for covariance to use of informative priors under nonignorable missing-
ness. Several of these examples have been carried out using WinBUGS soft-
ware, which is available at http://www.mrc-bsu.cam.ac.uk/bugs/. Code
used for analyses in the book, and datasets where available, are posted at
http://www.stat.brown.edu/~jhogan.
Audience
The book is intended for statisticians, data analysts and other scientists in-
volved in the collection and analysis of longitudinal data. Our emphasis is
on clinical and public health research, but many of the ideas will be appli-
cable to other ﬁelds. We assume familiarity with statistical inference at the
level of Casella and Berger (2001), and with regression at the level of Kutner
et al. (2003). Familiarity with regression for longitudinal data and principles
of Bayesian inference is helpful, but these ideas are reviewed in Chapters 2
and 3, where numerous references to books and key papers are given.
Readers of this book are likely to have at least a passing familiarity with
basic methods for handling missing data, but the literature is vast and there
are distinct points of view on how to approach inference. Several outstanding
texts can be consulted to gain an appreciation, including Little and Rubin
(2002); Schafer (1997); van der Laan and Robins (2003); Tsiatis (2006); and
Molenberghs and Kenward (2007).

PREFACE
xix
Although this is primarily a research monograph, we expect that it will
be a suitable supplemental text for graduate courses on longitudinal data,
missing data, or Bayesian inference. We have used material from the book for
our own graduate courses at University of Florida and Brown University.
Content
The book is composed of ten chapters, roughly divided into three main parts.
Chapters 1 through 4 make up the ﬁrst part, which covers needed background
material. Chapter 1 provides detailed descriptions of motivating examples
that are used throughout the book; Chapters 2 and 3 provide background on
longitudinal data models and Bayesian inference, respectively; and Chapter 4
illustrates key concepts with worked data analysis examples.
The second part includes Chapters 5 through 7; it introduces missing data
mechanisms for longitudinal data settings, and gives in-depth treatment of in-
ference under the ignorability assumption. Chapter 5 gives detailed coverage
of Rubin’s missing data taxonomy (Rubin, 1976) applied to longitudinal data,
including MAR, ignorability, and implications for posterior inference. The no-
tion of a full-data model is introduced, and the chapter primarily describes
assumptions that are needed to infer features or parameters of the full-data
model from incomplete data. Chapter 6 discusses inference under the ignora-
bility assumption, with emphasis on the importance of covariance modeling.
In Chapter 7, several case studies are used to illustrate.
Chapters 8 through 10 make up the third part, where the focus shifts to
nonignorable missingness. In Chapters 8 and 9, we reiterate the central idea
that inference about a full-data distribution from incomplete data requires
untestable assumptions. We divide the full-data distribution into an observed-
data model and an extrapolation model, and introduce sensitivity parameters
that govern the untestable assumptions about the extrapolation. Chapter 8
reviews common approaches such as selection models, mixture models, and
shared parameter models, and focuses on parameterization of missing data
assumptions in each model.
In Chapter 8, the primary focus is on the likelihood (data model), whereas
Chapter 9 is concerned with formulation of priors that reﬂect assumptions
about missing data. In Chapter 9, we show how to construct priors that reﬂect
speciﬁc missing data mechanisms (MAR, MNAR), how to center the models
at MAR, and give suggestions about calibrating the priors. Both mixture and
selection modeling approaches are addressed. Finally, Chapter 10 provides
three detailed case studies that illustrate analyses under MNAR.
Acknowledgments
We began this project in 2005; prior to that and since then, we have beneﬁted
from friendship, cooperation and support of family, friends and colleagues.
Our research in this area has been encouraged and positively inﬂuenced
by interactions with many colleagues in statistics and biostatistics; Alicia
Carriquiry, Peter Diggle, Constantine Gatsonis, Rob Kass, Nan Laird, Tony

xx
PREFACE
Lancaster, Xihong Lin, Mohsen Pourahmadi, Jason Roy, Dan Scharfstein and
Scott Zeger deserve special mention for mentorship, support and/or collabo-
ration. Ian Abramson and the Department of Mathematics at UC San Diego
provided generous hospitality during a sabbatical for J.H. The National Insti-
tutes of Health have funded our joint and individual research on missing data
and Bayesian inference over the past several years.
Our subject-matter collaborators have generously allowed us to use their
data for this book. Thanks are due to Charles Carpenter and Ken Mayer at
Brown University and Lytt Gardner at the CDC (HERS data); Dave MacLean
at Pﬁzer (Growth Hormone Study); Bess Marcus at Brown University (both
CTQ studies); and David Abrams at NIH and Brian Hitsman at Brown Uni-
versity (OASIS data).
Graduate students Li Su and Joo Yeon Lee at Brown University, and Keun-
baik Lee and Xuefeng Liu at University of Florida provided expert program-
ming and assistance with preparation of data examples. Li Su served as our
expert consultant on WinBUGS and developed code for many of the examples.
Several colleagues took time to read the text and oﬀer invaluable sugges-
tions. Rob Henderson carefully read the entire book and provided insightful
comments that led to major improvements in the ﬁnal version. Brad Carlin,
Allison DeLong, and Tao Liu each provided detailed and timely feedback on
one or more chapters. And through what turned out to be a much longer pro-
cess than we ever imagined, Rob Calver at Chapman & Hall never failed to
keep the wind in our sails with his encouragement, patience and good humor.
And ﬁnally, it is diﬃcult to overstate the support and patience provided by
our friends and – most of all – our families, at just the right times during this
project. Without Marie, Mary and Mia; and Dawn, Jack, Luke and Patrick,
it simply could not have been completed.
Gainesville
M. J. Daniels
Providence
J. W. Hogan
December, 2007

CHAPTER 1
Description of Motivating Examples
1.1 Overview
This chapter describes, in detail, several datasets that will be used throughout
the book to motivate the material and to illustrate data analysis methods.
For each dataset, we describe the associated study, the primary research goals
to be met by the analyses, and the key complications presented by missing
data. Empirical summaries are given here; for detailed analyses we provide
references to subsequent chapters.
These datasets derive primarily from our own collaborations, introducing
the potential for selection bias in the coverage of topics; however, we have
selected these because they cover a range of study designs and missing data
issues. Although the data are primarily from clinical trials, we also have data
from a large observational study. The datasets have both continuous and dis-
crete longitudinal endpoints. Both continuous-time and discrete-time dropout
processes are covered here. In one of the studies, an intermediate goal involves
capturing the joint distribution of two processes evolving simultaneously.
It is our hope that readers can ﬁnd within these examples connections to
their own work or data-analytic problem of interest. The literature on missing
data continues to grow, and it is impossible to address every situation with
representative examples, so we have included at the end of each chapter a
list of supplementary reading to assist readers in their navigation of current
research.
In the descriptions of studies and datasets, we use the terms dropout, miss-
ing at random, and missing not at random under the assumption that the
reader has at least a casual acquaintance with their meaning. Precise deﬁni-
tions and implications for analyses are treated in considerable detail in up-
coming chapters; readers also may want to consult Little and Rubin (2002),
Tsiatis (2006), and Molenberghs and Kenward (2007). In short, missing data
are said to be missing at random (MAR) if the probability of missingness
depends only on observable data; when probability of missingness depends
on the missing observations themselves, even after conditioning on observed
information, missing data are said to be missing not at random (MNAR).
1

2
MOTIVATING EXAMPLES
1.2 Dose-ﬁnding trial of an experimental treatment for
schizophrenia
1.2.1 Study and data
These data were collected as part of a randomized, double-blind clinical trial
of a new pharmacologic treatment of schizophrenia (Lapierre et al., 1990).
Other published analyses of these data can be found in Hogan and Laird
(1997a) and Cnaan et al. (1997).
The trial compares three doses of the new treatment (low, medium, high)
to the standard dose of halperidol, an eﬀective antipsychotic that has known
side eﬀects. At the time of the trial, the experimental therapy was thought to
have similar antipsychotic eﬀectiveness with fewer side eﬀects; the trial was
designed to ﬁnd the appropriate dosing level. The study enrolled 245 patients
at 13 diﬀerent centers, and randomized them to one of the four treatment
arms. The intended length of follow-up was 6 weeks, with measures taken
weekly except for week 5. Schizophrenia severity was assessed using the Brief
Psychiatric Rating Scale, or BPRS, a sum of scores of 18 items that reﬂect
behaviors, mood, and feelings (Overall and Gorham, 1988). The scores ranged
from 0 to 108 with higher scores indicating higher severity. A minimum score
of 20 was required for entry into the study.
1.2.2 Questions of interest
The primary objective is to compare mean change from baseline to week 6
between the four treatment groups.
1.2.3 Missing data
Dropout in this study was substantial; only 139 of the 245 participants had
a measurement at week 6. The mean BPRS on each treatment arm showed
diﬀerences between dropouts and completers (see Figure 1.1). Reasons for
dropout included adverse events (e.g., side eﬀects), lack of treatment eﬀect,
and withdrawal for unspeciﬁed reasons; a summary appears in Table 1.1.
Reasons such as lack of observed treatment eﬀect are clearly related to the
primary eﬃcacy outcome, but others such as adverse events and participant
withdrawal may not be.
1.2.4 Data analyses
In Section 4.3 we use data on those with complete follow-up to illustrate
posterior inference for a standard random eﬀects model. In Section 7.3 the
model is re-ﬁtted using all the data under an MAR assumption.
For some individuals, dropout occurs for reasons that clearly are related

SCHIZOPHRENIA TRIAL
3
C
C
C
C
C
C
0
1
2
3
4
5
6
0
10
20
30
40
50
New Trt − Low Dose
Week
BPRS
D
D
D
D
D
D
C
C
C
C
C
C
0
1
2
3
4
5
6
0
10
20
30
40
50
New Trt − Medium Dose
Week
BPRS
D
D
D
D
D
D
C
C
C
C
C
C
0
1
2
3
4
5
6
0
10
20
30
40
50
New Trt − High Dose
Week
BPRS
D
D
D
D
D
D
C
C
C
C
C
C
0
1
2
3
4
5
6
0
10
20
30
40
50
Standard Trt
Week
BPRS
D
D
D
D
D
D
Figure 1.1 Schizophrenia trial: mean BPRS scores, stratiﬁed on dropout for lack of
treatment eﬀect (plotting symbol D for dropouts and C for non-dropouts). Among
those experiencing lack of eﬀect, 10 had BPRS measured at week 6; hence BPRS
scores appear at week 6 among ‘dropouts’. Measurements were not taken at week 5.
to outcome, and for others the connection between dropout and outcome is
less clear (e.g., dropout due to adverse side eﬀects); hence MAR may not be
entirely appropriate. Using this trial as a motivating example, model formu-
lation for settings with both MAR and MNAR missingness is discussed in
Section 8.4.5. We use a pattern mixture approach, treating MAR dropouts as
censoring times for the MNAR dropout process.

4
MOTIVATING EXAMPLES
Table 1.1 Schizophrenia trial: dropout by reason, stratiﬁed by treatment arm.
Reason for Dropout
Adverse
Lack of
Treatment Arm
Completed
Event
Eﬀect
Other
New Trt – Low Dose
27
2
21
11
New Trt – Medium Dose
40
1
7
13
New Trt – High Dose
33
2
13
12
Standard Trt
34
12
11
6
1.3 Clinical trial of recombinant human growth hormone (rhGH)
for increasing muscle strength in the elderly
1.3.1 Study and data
The data come from a randomized clinical trial conducted to examine the
eﬀects of recombinant human growth hormone (rhGH) therapy for building
and maintaining muscle strength in the elderly (Kiel et al., 1998). The study
enrolled 161 participants and randomized them to one of four treatment arms:
placebo (P), growth hormone only (G), exercise plus placebo (EP), and ex-
ercise plus growth hormone (EG). Various muscle strength measures were
recorded at baseline, 6 months, and 12 months. Here, we focus on mean quadri-
ceps strength (QS), measured as the maximum foot-pounds of torque that can
be exerted against resistance provided by a mechanical device.
1.3.2 Questions of interest
The primary objective of our analyses is to compare mean QS at month 12
in the four treatment arms among all those randomized (i.e., draw inference
about the intention to treat eﬀect).
1.3.3 Missing data
Roughly 75% of randomized individuals completed all 12 months of follow-
up, and most of the dropout was thought to be related to the unobserved
responses at the dropout times. Table 1.2 summarizes mean and standard
deviation of QS for available follow-up data, both aggregated and stratiﬁed
by dropout time.

GROWTH HORMONE TRIAL
5
Table 1.2 Growth hormone trial: sample means (standard deviations) stratiﬁed by
treatment group and dropout pattern s. Patterns deﬁned by number of observations
and ns is the number in pattern s.
Month
Treatment
s
ns
0
6
12
EG
1
12
58 (26)
2
4
57 (15)
68 (26)
3
22
78 (24)
90 (32)
88 (32)
All
38
69 (25)
87 (32)
88 (32)
G
1
6
68 (17)
2
5
77 (33)
81 (42)
3
30
67 (22)
64 (21)
63 (20)
All
41
69 (23)
66 (25)
63 (20)
EP
1
7
65 (32)
2
2
87 (52)
86 (51)
3
31
65 (24)
81 (25)
73 (21)
All
40
66 (26)
82 (26)
73 (21)
P
1
8
66 (29)
2
5
53 (19)
62 (31)
3
28
67 (23)
62 (20)
63 (19)
All
41
65 (24)
62 (22)
63 (19)
1.3.4 Data analyses
These data are used several times throughout the book to illustrate various
models. In Section 4.2 we analyze data from individuals with complete follow-
up to illustrate multivariate normal regression and model selection procedures
for choosing an appropriate variance-covariance structure; Example 7.2 illus-
trates the use of multivariate normal models under an MAR constraint, with
emphasis on how the choice of variance-covariance structure aﬀects inferences
about the mean. In Section 10.2 we use more general pattern mixture models
that permit MNAR to illustrate strategies for sensitivity analysis, incorpora-
tion of informative priors, and model ﬁt.

6
MOTIVATING EXAMPLES
1.4 Clinical trials of exercise as an aid to smoking cessation in
women: the Commit to Quit studies
1.4.1 Studies and data
The Commit to Quit studies were randomized clinical trials to examine the
impact of exercise on the ability to quit smoking among women. The women
in each study were aged 18 to 65, smoked ﬁve or more cigarettes per day for
at least 1 year, and participated in moderate or vigorous intensity activity for
less than 90 minutes per week.
The ﬁrst trial (hereafter CTQ I; Marcus et al., 1999) enrolled 281 women
and tested the eﬀect on smoking cessation of supervised vigorous exercise
vs. equivalent staﬀcontact time to discuss health care (henceforth called the
‘wellness’ arm); the second trial (hereafter CTQ II; Marcus et al., 2005) en-
rolled 217 female smokers and was designed to examine the eﬀect of moderate
partially supervised exercise. Other analyses of these data can be found in
Hogan et al. (2004b), who illustrate weighted regression using inverse propen-
sity scores; Roy and Hogan (2007), who use principal stratiﬁcation methods
to infer the causal eﬀect of compliance with vigorous exercise; and Liu and
Daniels (2007), who formulate and apply new models for the joint distribution
of smoking cessation and weight change.
In each study, smoking cessation was assessed weekly using self-report, with
conﬁrmation via laboratory testing of saliva and exhaled carbon monoxide.
As is typical in short-term intervention trials for smoking cessation, the target
date for quitting smoking followed an initial ‘run-in’ period during which the
intervention was administered but the participants were not asked to quit
smoking. In CTQ I, measurements on smoking status were taken weekly for
12 weeks; women were asked to quit at week 5. In CTQ II, total follow-up
lasted 8 weeks, and women were asked to quit in week 3.
1.4.2 Questions of interest
In each study, the question of interest was whether the intervention under
study reduced the rate of smoking. This can be answered in terms of the
eﬀect of randomization to treatment vs. control, or in terms of the eﬀect of
complying with treatment vs. not complying (or vs. complying with control).
The former can be answered using an intention to treat analysis, contrasting
outcomes based on treatment arm assignment. The latter poses additional
challenges but can be addressed using methods for inferring causal eﬀects; we
refer the reader to Roy and Hogan (2007) for details.
In our analyses, we frame the treatment eﬀect in terms of either (a) time-
averaged weekly cessation rate following the target quit date or (b) cessation
rate at the ﬁnal week of follow-up. More details about analyses are given
below.

SMOKING CESSATION TRIALS
7
Wellness
Week
Proportion quit
1
5
12
0.0
0.1
0.2
0.3
0.4
0.5
Exercise
Week
Proportion quit
1
5
12
0.0
0.1
0.2
0.3
0.4
0.5
Wellness
Week
Proportion in follow−up 
1
5
12
0.5
0.6
0.7
0.8
0.9
1.0
Exercise
Week
Proportion in follow−up
1
5
12
0.5
0.6
0.7
0.8
0.9
1.0
Figure 1.2 CTQ-I: graph of proportion quit by week (top panels) and proportion
remaining in the study (bottom panels), stratiﬁed by treatment group. Proportion
quit at each week is calculated among those remaining in follow-up. Week 5 is the
target ‘quit week’.
1.4.3 Missing data
Each of the studies had substantial dropout: in CTQ I, 31% dropped out on
the exercise arm and 35% on the control arm; for CTQ II, 46% dropped out
on the exercise arm and 41% on the control arm.
Figure 1.2 shows, for CTQ I, weekly cessation rates based on available data
at each time point, coupled with proportion remaining in the study; Figure 1.3
shows the same data stratiﬁed by dropout status (yes/no), making clear that
dropout is related at least to observed smoking status during the study. There
also exists some empirical support for the notion that dropout is related to

8
MOTIVATING EXAMPLES
missing outcomes in smoking cessation studies, in the sense that dropouts
are more likely to be smoking once they have dropped out (Lichtenstein and
Glasgow, 1992).
1.4.4 Data analyses
Analyses of CTQ I using standard regression models under MAR
In Section 4.4 we illustrate and compare random eﬀects logistic regression and
marginalized transition models for estimating conditional (subject-speciﬁc)
and marginal (population-averaged) treatment eﬀects using data from individ-
uals with complete follow-up. Model selection procedures also are illustrated.
In Example 7.4 the data are reanalyzed under MAR using all available data.
Analysis of CTQ II using auxiliary variables
Weight change is generally associated with smoking cessation. In Section 7.5
we illustrate the use of auxiliary information on longitudinal weight changes
to inform the distribution of smoking cessation outcomes in making treatment
comparisons in CTQ II. The weight change data are incorporated through a
joint model for longitudinal smoking cessation and weight, and the marginal
distribution of smoking cessation is used for treatment comparisons.
Wellness
Week
Proportion quit
C C C
C
C C C C C C C
D D D
D
D
D
D
D D D D
1
5
12
0.0
0.1
0.2
0.3
0.4
0.5
C
Exercise
 
 
C C C
C
C C
C C
C C C
D D D
D
D D
D
D
D D D
1
5
12
0.0
0.1
0.2
0.3
0.4
0.5
C
Figure 1.3 CTQ-I: proportion quit by week, separately by treatment group, stratiﬁed
by dropout status (C = completer, D = dropout).

HERS: HIV NATURAL HISTORY STUDY
9
1.5 Natural history of HIV infection in women: HIV Epidemiology
Research Study (HERS) cohort
1.5.1 Study and data
The HIV Epidemiology Research Study (HERS) was a longitudinal cohort
study of the natural history of HIV in women (Smith et al., 1997). Between
1993 and 1996, the HERS enrolled 1310 women who were either HIV-positive
or at high risk for infection; 871 were HIV-positive at study entry. Every 6
months for up to 5 years, several outcomes were recorded for each participant,
including standard measures of immunologic function and viral burden, plus
a comprehensive set of measures characterizing health status and behavioral
patterns (e.g., body mass index, depression status, drug use behavior). Our
analyses of HERS data will focus on modeling CD4 progression relative to
timing of highly active antiviral therapy (HAART) initiation, using data where
some individuals have incomplete follow-up.
1.5.2 Questions of interest
The HERS is a multi-site study with several published investigations on dif-
ferent aspects of HIV epidemiology and associated statistical methodology.
Relative to the full scope of HERS, our objectives for illustrating data analy-
ses are necessarily simpliﬁed. Our interest is in characterizing the trajectory
of CD4 relative to the initiation of HAART. Figure 1.4 shows an aggregated
scatterplot of 4327 CD4 counts vs. time to HAART initiation from a sample
of 393 individuals who were observed to initiate HAART during the study
period. A small number of individual trajectories are highlighted. The data
exhibit considerable within- and between-individual variability, and nonlinear
trajectories relative to initiation of treatment. The goal of our analysis is to
infer the population CD4 curve relative to initiation of HAART.
1.5.3 Missing data
All individuals enrolled in HERS were scheduled for 5 year follow-up (12
visits); in our subsample of 393 women, only 195 completed follow-up. The
remainder had incomplete data due to missed visits, dropout, and death (36).
Missingness due to death necessitates careful deﬁnition of the target quantities
for inference. Our treatment of the HERS data does not explicitly address this
issue; readers are referred to Zheng and Heagerty (2005) and Rubin (2006)
for recent treatment.

10
MOTIVATING EXAMPLES
−2000
−1000
0
1000
0
500
1000
1500
2000
time since HAART initiation
CD4
Figure 1.4 HER Study: Plot of CD4 vs. time for 393 HIV infected women who
received HAART during their enrollment in HERS. Horizontal axis is centered at
time of the ﬁrst visit where receipt of HAART is reported.
1.5.4 Data analyses
An analysis of these data is given in Chapter 6. The nonlinearity of the time
trend motivates a ﬂexible approach to modeling; we use penalized splines
under MAR to characterize the population CD4 curve, with emphasis on
the importance of choosing an appropriate variance-covariance model and its
eﬀect on the ﬁtted curve.

OASIS STUDY
11
1.6 Clinical trial of smoking cessation among substance abusers:
OASIS study
1.6.1 Study and data
The OASIS Trial is an NIH-funded study designed to compare standard (ST)
vs. enhanced (ET) counseling interventions for smoking cessation among al-
coholics. The trial enrolled 298 individuals, randomized to standard vs. more
intensive counseling. Assessment of smoking status was made at 1, 3, 6, and
12 months following randomization. Previous analyses of these data appear
in Lee et al. (2007).
1.6.2 Questions of interest
The primary goal of our analysis is comparison of smoking cessation rates at
12 months by treatment randomization (i.e., intention to treat eﬀect).
Table 1.3 OASIS Trial: number (proportion) quit, smoking and missing at each
month. S/(Q + S) denotes empirical smoking rate among those still in follow-up;
(S + M)/(Q + S + M) denotes empirical smoking rate after counting missing values
as smokers. ET = enhanced intervention, ST = standard intervention.
Month
Treatment
1
3
6
12
ET
Quit
26 (.18)
13 (.09)
17 (.11)
16 (.11)
(n = 149)
Smoking
123 (.83)
70 (.47)
63 (.42)
51 (.34)
Missing
—
66 (.44)
69 (.46)
82 (.55)
S
Q + S
.83
.84
.79
.76
S + M
Q + S + M
.83
.91
.89
.89
ST
Quit
23 (.15)
14 (.09)
15 (.10)
11 (.07)
(n = 149)
Smoking
126 (.85)
80 (.54)
78 (.52)
78 (.52)
Missing
—
55 (.37)
56 (.38)
60 (.40)
S
Q + S
.85
.85
.84
.88
S + M
Q + S + M
.85
.91
.90
.93

12
MOTIVATING EXAMPLES
1.6.3 Missing data
Dropout rate was relatively high in the OASIS study (40% on ST, 55% on ET).
Table 1.3 summarizes proportion quit, smoking, and missing at each month,
stratiﬁed by treatment arm, and includes two commonly used estimates of
overall smoking rate. The ﬁrst is derived using only those still in follow-up, and
the second assumes those with missing observations are smokers.∗In Table 1.4
the association between status at times tj−1 and tj (for j = 2, 3, 4) indicates
that smoking status at tj−1 is predictive of dropout at tj, and motivates the
use of models that assume MAR at a minimum.
Table 1.4 OASIS Trial: number and proportion of transitions from status at mea-
surement times tj−1 to tj (with tj corresponding to months 1, 3, 6, and 12 for
j = 1, 2, 3, 4), stratiﬁed by treatment. Figures in parentheses are row proportions.
ET = enhanced intervention, ST = standard intervention.
Status
Status at tj (j = 2, 3, 4)
Treatment
at tj−1
Quit
Smoking
Missing
Total
Quit
26 (.46)
21 (.38)
9 (.16)
56
ET
Smoking
18 (.07)
152 (.59)
86 (.34)
256
Missing
2 (.01)
11 (.08)
122 (.90)
135
Quit
24 (.46)
17 (.33)
11 (.21)
52
ST
Smoking
12 (.04)
204 (.72)
68 (.24)
284
Missing
4 (.04)
15 (.14)
92 (.83)
111
1.6.4 Data analyses
These data are analyzed in detail in Section 10.3, using both MAR and MNAR
models. The ﬁrst analysis uses a pattern mixture model where, conditional on
dropout time, the longitudinal smoking outcomes follow a Markov transition
model. The model is ﬁt under MAR assumptions, then elaborated to allow
for MNAR mechanisms. Sensitivity analyses and the use of informative priors
elicited from experts are illustrated.
We also use a selection model approach, allowing for MNAR dropout. The
two models are compared in terms of inference about treatment eﬀect and
assumptions about missing data.
∗In our analyses of these data, we make a simplifying assumption that those with missing
outcome at month 1 are smokers.

PEDIATRIC AIDS TRIAL
13
1.7 Equivalence trial of competing doses of AZT in HIV-infected
children: Protocol 128 of the AIDS Clinical Trials Group
1.7.1 Study and data
ACTG 128 is a randomized equivalence trial of high vs. low dose of AZT for
the treatment of HIV in children (Brady et al., 1998). The trial enrolled 426
children and randomized them to a regimen of either 180mg or 90mg AZT six
times daily. Because AZT is associated with potentially harmful side eﬀects
and drug toxicity, the trial was designed to provide information on whether the
lower dose provided eﬃcacy comparable to the high dose while reducing the
rate of side eﬀects. The key clinical outcomes are CD4 cell count, scheduled
for measurement every 3 months, and neurocognitive test results, scheduled
for measurement every 6 months. Our focus here is on CD4 cell counts; both
the CD4 and neurocognitive outcomes have been been analyzed elsewhere,
with attention to handling dropout and noncompliance; see Hogan and Laird
(1996); Hogan and Daniels (2002), and Hogan et al. (2004a).
0
50
100
150
200
0
2
4
6
8
High dose
Weeks from baseline
log CD4
0
50
100
150
200
0
2
4
6
8
Low Dose
Weeks from baseline
log CD4
Figure 1.5 Pediatric AIDS Trial: CD4 count vs. time for all individuals, stratiﬁed
by treatment, with a subsample of individual trajectories highlighted.

14
MOTIVATING EXAMPLES
1.7.2 Questions of interest
The primary objective of our analyses is comparison of change in CD4 count
from baseline to the end of the study, using the intention to treat principle;
that is, we seek to compare change in CD4 among all participants randomized
to high or low dose AZT.
1.7.3 Missing data
Dropout occurs for various reasons, including lack of treatment eﬀect and drug
toxicity. A key feature of this study is that dropout time is measured on a
continuum, which requires some added ﬂexibility in the modeling process. For
purposes of keeping the example straightforward, we do not use information
about reason for dropout in our analyses, but see Hogan and Laird (1996) and
Hogan and Daniels (2002) for analyses that do. Figure 1.5 shows CD4 counts
aggregated over all individuals, with a subsample of individual trajectories
highlighted. The plot suggests that dropouts (those with shorter trajectories)
tend to have a lower CD4 count at baseline and a more pronounced negative
slope over time.
1.7.4 Data analyses
These data are analyzed in Section 10.4 using a mixture of varying coeﬃcient
models (Hogan et al., 2004a). The modeling approach allows regression coef-
ﬁcients such as slope over time to vary as smooth functions of dropout times.
The conditional distribution of CD4 given dropout is then averaged over the
dropout distribution, which can be modeled nonparametrically. The results of
our analysis are compared to the standard random eﬀects approach.

CHAPTER 2
Regression Models
for Longitudinal Data
2.1 Overview
This chapter reviews some modern approaches to formulation and interpre-
tation of regression models for longitudinal data. Section 2.2 outlines nota-
tion for longitudinal data and describes basic regression approaches. In Sec-
tion 2.3 we describe the generalized linear model (GLM) for univariate data,
which forms the basis of many regression models for longitudinal data. Sec-
tions 2.4 and 2.5 describe diﬀerent approaches to regression modeling based
on whether the mean is speciﬁed directly, in terms of marginal means; or con-
ditionally, in terms of latent variables, random eﬀects, or response history.
Many conditionally speciﬁed models are multilevel models that partition the
variance-covariance structure in a natural way, leading to low dimensional
parameterizations.
In Section 2.6 we focus on semiparametric models highlighting those that
permit ﬂexibility in modeling time trends and allow covariate eﬀects to vary
smoothly with time (e.g., varying coeﬃcient models). Finally, Section 2.7 re-
views key issues in the interpretation of longitudinal regression models, high-
lighting longitudinal vs. cross-sectional eﬀects and key assumptions about
time-varying covariates.
The literature on regression models for longitudinal data is vast, and we
make no attempt to be comprehensive here. Our review is designed to highlight
predominant approaches to regression modeling, emphasizing those models
used in later chapters. For recent accounts, readers are referred to Davidian
and Giltinan (1998); Verbeke and Molenberghs (2000); Diggle et al. (2002);
Fitzmaurice et al. (2004); Laird (2004); Weiss (2005); Hedeker and Gibbons
(2006) and Molenberghs and Verbeke (2006).
2.2 Preliminaries
2.2.1 Longitudinal data
Appealing to ﬁrst principles, one can think of longitudinal data as arising
from the joint evolution of response and covariates,
{Yi(t), xi(t) : t ≥0}.
15

16
REGRESSION MODELS
If the process is observed at a discrete set of time points T = {t1, . . . , tJ}
that is common to all individuals, the resulting response data can be written
as a J × 1 vector
Y i
=
{Yi(t) : t ∈T }
=
(Yi1, . . . , YiJ)T.
The covariate process {xi(t) : t ≥0} is 1 × p. At time tj, the observed
covariates are collected in the 1 × p vector
xij
=
xi(tj)
=
(xi1(tj), . . . , xip(tj))
=
(xij1, . . . , xijp).
Hence the full collection of observed covariates is contained in the J ×p matrix
Xi
=


xi1
xi2
...
xiJ


.
When the set of observation times is common to all individuals, we say the
responses are balanced or temporally aligned. It is sometimes the case that
observation times are unbalanced, or temporally misaligned, in that they vary
by subject. In this case the set T is indexed by i, such that
Ti = {ti1, . . . , tiJi},
and the dimensions of Y i and Xi are Ji × 1 and Ji × p, respectively.
In regression, we are interested in characterizing the eﬀect of covariates X
on a longitudinal dependent variable Y . Formally, we wish to draw inference
about the joint distribution of the vector Y i conditionally on Xi. Likelihood-
based regression models for longitudinal data require a speciﬁcation of this
joint distribution using a model p(y | x, θ). The parameter θ typically is a
ﬁnite-dimensional vector of parameters indexing the model; it might include
regression coeﬃcients, variance components, and parameters indexing serial
correlation.
The joint distribution of responses can speciﬁed directly or conditionally.
In this chapter we diﬀerentiate models on how the mean is speciﬁed. A model
with a directly speciﬁed mean characterizes E(Y | x) without resorting to
latent structures such as random eﬀects. Models with conditionally speciﬁed
means include Markov models and various types of multilevel models, where
the mean is given conditionally on previous responses or on random eﬀects
that reﬂect aspects of the joint response distribution. Mostly, we deal with
conditionally speciﬁed models that use a multilevel format, for example involv-
ing subject-speciﬁc random eﬀects or latent variables b to partition within-

PRELIMINARIES
17
and between-subject variation. The usual strategy is to specify a model for
the joint distribution of responses and random eﬀects, factored as
p(y, b | x)
=
p(y | b, x) p(b | x).
The distribution of interest, p(y | x), is obtained by integrating over b. For
example, if the conditional mean is given by a function like
E(Y | x, b) = g(xβ + b),
then
E(Y | x) =

g(xβ + b) p(b | x) db,
which may not always take a closed form.
2.2.2 Regression models
As we review several diﬀerent regression models, the intent is to give the
reader a sense of the rich variety of models that can be used to characterize
longitudinal data, and to demonstrate how these ﬁt coherently into a single
framework. As a result, missing data strategies described in later chapters
can be applied very generally. Methods described here will be familiar to
those with experience analyzing longitudinal data (e.g., multivariate normal
regression model, random eﬀects models), but others represent fairly new
developments. Examples include marginalized transition models (Heagerty,
2002), varying coeﬃcient models (Zhang, 2004), and regression splines (Eilers
and Marx, 1996; Lin and Zhang, 1999; Ruppert et al., 2003). Here we focus on
speciﬁcation and interpretation; Chapter 3 covers various aspects of inference.
Because many regression models for longitudinal data have their founda-
tion in the generalized linear model (GLM) for cross-sectional data (McCul-
lagh and Nelder, 1989), our review begins with a concise description of GLMs.
Coverage of models for longitudinal data begins with random eﬀects models;
these build directly on the GLM structure by introducing individual-level ran-
dom eﬀects to capture between-subject variation. Conditional on the random
eﬀects, within-level variation can be described by a simpler model, such as a
GLM. Random eﬀects models are very attractive in that they naturally parti-
tion variation in the dependent variable into its between- and within-subject
components, and they can be used to model both balanced and unbalanced
data. At the same time, there is sometimes the disadvantage that the implied
marginal distributions of responses can be opaque.
Directly speciﬁed models have a natural construction when the error dis-
tribution is multivariate normal; for binary, count, and other discrete data,
the choice of an appropriate joint distribution is less obvious. Our review
touches on some recent developments for discrete longitudinal responses, such
as the marginalized transition model (Heagerty, 2002). For a detailed review

18
REGRESSION MODELS
of likelihood-based models of multivariate discrete response, see Chapter 11 of
Diggle et al. (2002), Chapter 7 of Laird (2004), and Chapter 11 of Fitzmaurice
et al. (2004).
For all models covered in the ﬁrst part of this chapter, E(Y | x) takes a
known functional form, usually linear in some transformed scale. Section 2.6
describes models in which the regression function is unknown but can be at
least partially speciﬁed in terms of unspeciﬁed smooth functions. The latter
type of model is typically called semiparametric, because one or more com-
ponents of the regression function are left unspeciﬁed, while distributional
assumptions are made about the error structure. Nonlinear and semipara-
metric models have a close connection to the GLM structure; we emphasize
that connection and illustrate that regression models as a whole can be very
generally characterized (Hastie and Tibshirani, 1990; Ruppert et al., 2003).
The ﬁnal element of our review concerns interpretation of covariate ef-
fects in longitudinal models. Because the response and covariates change with
time, models of longitudinal data aﬀord the opportunity to infer both within-
and between-subject covariate eﬀects; however the importance of underlying
assumptions to the interpretation of covariate eﬀects should not be underesti-
mated. Section 2.7 discusses three key aspects of interpretation and speciﬁca-
tion for longitudinal models: cross-sectional vs. longitudinal eﬀects of a time-
varying covariate, marginal (population-averaged) vs. conditional (subject-
speciﬁc) covariate eﬀects, and assumptions governing the use of time-varying
covariates.
2.2.3 Full vs. observed data
The distinction between full and observed data is particularly important when
drawing inference from incomplete longitudinal data. Throughout Chapters 2
and 3, the models refer to a full-data distribution.
We deﬁne the full data as those observations intended to be collected on a
pre-speciﬁed interval, such as [0, T ]. For example, if intended collection times
t1, . . . , tJ are common to all individuals, then the full response and covariate
data are (Yi1, Xi1), . . . , (YiJ, XiJ), where Yij = Yi(tj) and Xij = Xi(tj). In
Chapter 5 we expand the deﬁnition of full data to include random variables
such as dropout time that characterize the missing data process.
In most applications, interest lies in the eﬀect of covariates on the mean
structure. When data are fully observed, the variance and covariance models
can frequently be treated as nuisance parameters. Correct speciﬁcation of
variance and covariance allows more eﬃcient use of the data, but it is not
always necessary for obtaining proper inferences about mean parameters.
When data are not fully observed, variance-covariance speciﬁcation takes
on heightened importance because missing data will eﬀectively be imputed or
extrapolated from observed data, based on modeling assumptions. For longi-

GENERALIZED LINEAR MODELS
19
tudinal data, unobserved responses will be imputed from observed responses
for the same individual; the assumed correlation structure will usually dictate
(at least in part) the functional form of the imputation. This theme recurs
throughout the book, and therefore our review pays particular attention to
aspects of variance-covariance speciﬁcation.
2.2.4 Additional notation
Random variables and their realizations are denoted by Roman letters (e.g.,
X, x), and parameters are represented by Greek letters (e.g., α, θ). Vector-
and matrix-valued random variables and parameters are represented using
boldface (e.g., x, Y , β, Σ). For any matrix or vector A, we use AT to denote
transpose. If A is invertible, then A−1 is its inverse, |A| is its determinant,
and L = A1/2 is the lower triangular matrix square root (Cholesky factor)
such that LLT = A. A q-dimensional identity matrix is denoted Iq and a
diagonal matrix by diag(a), where a is the vector of diagonal elements. The
parameterizations of speciﬁc probability distributions used in the text can be
found in the Appendix.
2.3 Generalized linear models for cross-sectional data
The generalized linear model (GLM) forms the foundation for many ap-
proaches to regression with multivariate responses, such as longitudinal or
clustered data. Models such as random eﬀects or mixed eﬀects models, latent
variable and latent class models, and regression splines, all highly ﬂexible and
general, are based on the GLM framework. Moment-based methods such as
generalized estimating equations (GEE) also follow directly from the GLM
for cross-sectional data (Liang and Zeger, 1986).
The GLM is a regression model for a dependent variable Y arising from
the exponential family of distributions
p(y | θ, ψ) = exp {(yθ −b(θ)) /a(ψ) + c(y, ψ)} ,
where a, b, and c are known functions, θ is the canonical parameter, and ψ
is a scale parameter. The exponential family includes several commonly used
distributions, such as normal, Poisson, binomial, and gamma. It can be readily
shown that
E(Y )
=
b′(θ)
var(Y )
=
a(ψ)b′′(θ),
where b′(θ) and b′′(θ) are ﬁrst and second derivatives of b(θ) with respect to
θ (see McCullagh and Nelder, 1989, Section 2.2.2 for details).
The eﬀect of covariates xi = (xi1, . . . , xip) can be modeled by introducing

20
REGRESSION MODELS
the linear predictor
ηi = η(xi, β) = xiβ,
where β = (β1, . . . , βp)T is a vector of regression coeﬃcients. Deﬁne µi =
µ(xi, β) = E(Y | xi, β). A smooth, monotone function g links the mean µi
to the linear predictor ηi via
g(µi) = ηi = xiβ.
(2.1)
In many
exponential family distributions, it is possible to identify a link
function g such that XTY is the suﬃcient statistic for β (here, X is the
n × p design matrix and Y = (Y1, . . . , Yn)T is the n × 1 vector of responses).
In this case, the canonical parameter is θ = η. Examples are well-known and
widespread: for the Poisson distribution, the canonical parameter is log(µ);
for binomial distribution, it is the log odds (logit), log{µ/(1 −µ)}.
Although canonical links are sometimes convenient, their use is not neces-
sary to form a GLM. In general, only the speciﬁcation of a mean and variance
function, conditionally on covariates, is required. The mean follows (2.1), and
the variance is given by
v(µi, φ) = φh(µi),
where h(·) is some function of the mean and φ > 0 is a scale factor. Certain
choices of g and h will yield likelihood score equations for common parametric
regression models based on exponential family distributions. For example,
setting g(µ) = log{µ/(1 −µ)}, h(µ) = µ(1 −µ) and φ = 1 yields logistic
regression under a Bernoulli distribution. Similarly, Poisson regression can be
speciﬁed by setting g(µ) = log(µ), h(µ) = µ, and φ = 1.
2.4 Conditionally speciﬁed models
This section focuses on models that specify the mean of Y given X con-
ditionally on random eﬀects; these models also are known by a variety of
names, including ‘mixed eﬀects models’, ‘multilevel models’, ‘random eﬀects
models’, and ‘random coeﬃcient models’. Throughout the text, we use the
terms ‘random eﬀects’ and ‘multilevel’ models. Readers are referred to Bres-
low and Clayton (1993) and Daniels and Gatsonis (1999) for a more complete
accounting and list of references. This class of models also includes regres-
sion models with factor-analytic and latent class structures (see Bartholomew
and Knott, 1999 for a full account) and Markov models (where the mean is
speciﬁed conditional on a subset of past responses).
Conditionally speciﬁed models with multiple levels, using random eﬀects
or latent variables, provide a highly ﬂexible class of models for handling lon-
gitudinal data. The models can be applied either to balanced or unbalanced
response proﬁles and can be used to capture key features of both between-
and within-subject variation using relatively few parameters.

CONDITIONALLY SPECIFIED MODELS
21
The most common random eﬀects models for longitudinal data specify the
joint distribution p(y, b | x, θ) as
p(y | b, x, θ1) p(b | x, θ2).
The parameter θ1 captures the conditional eﬀect of X on Y and the parameter
θ2 captures features of the distribution of random eﬀects b. The distribution
unconditional on random eﬀects is obtained by integrating b out of the joint
distribution
p(y | x, θ1, θ2) =

p(y | x, b, θ1) p(b | x, θ2) db;
(2.2)
notice the unconditional distribution is indexed by the full set of parameters
θ = (θ1, θ2).
2.4.1 Random eﬀects models based on GLMs
By including random eﬀects, generalized linear models can be used to model
longitudinal and clustered data. For common distributions such as Bernoulli
and Poisson, the GLM with random eﬀects can be written in terms of the
conditional mean and variance. The model for the conditional mean µb
ij =
E(Yij | xij, wij, bi) takes the form
g{E(Yij | xij, wij, bi)} = g(µb
ij) = xijβ + wijbi,
where g(·) is a link function and wij is a design matrix for the subject-speciﬁc
random eﬀects. This representation of the conditional mean motivates the
term ‘mixed-eﬀects model’ because the coeﬃcients quantify both population-
level (β) and individual-level (bi) eﬀects.
The conditional variance is given by
V b
ij = var(Yij | xij, wij, bi) = φh(µb
ij)
for φ > 0 and a suitably chosen h(·). Finally, within-subject correlation is
speciﬁed through a covariance function
Cb
ijk(γ) = cov(Yij, Yik | xij, xik, bi, γ).
In many cases it is assumed that Cb
ijk = 0; i.e., that the random eﬀects capture
relevant within-subject correlation (after averaging over their distribution),
but this assumption may not always be appropriate for longitudinal responses.
At the second level, the random eﬀects bi follow some distribution such
as multivariate normal. The model for the marginal joint distribution of
(Yi1, . . . , YiJ | Xi) is obtained by integrating over bi as in (2.2).
The relationship between marginal and conditional models is important to
understand, particularly as it relates to interpreting covariate eﬀects. In what
follows we give several examples to illustrate.

22
REGRESSION MODELS
2.4.2 Random eﬀects models for continuous response
A natural choice for modeling continuous or measured responses is the normal
distribution. In random eﬀects models, allowing both within- and between-
subject variation to follow a normal distribution, or more generally a Gaussian
process, aﬀords considerable modeling ﬂexibility while retaining interpretabil-
ity.
Example 2.1. Normal random eﬀects model for continuous responses.
A common model for continuous longitudinal responses is the normal random
eﬀects model. This model illustrates well the concept of a conditionally speci-
ﬁed joint distribution because the variance-covariance structure in p(y | x, θ)
is a by-product of the assumed random eﬀects distribution.
Like many random eﬀects models, it is easiest to describe in two stages. At
the ﬁrst stage, the vector of responses Y i, measured at times {ti1, . . . , ti,Ji},
are normal conditionally on a q × 1 vector of random eﬀects bi,
Y i | xi, bi ∼N(µb
i, Σb
i),
where the superscript b denotes that the mean and covariance are conditional
on bi. To incorporate covariate eﬀects, let
µb
i = xiβ + wibi,
where wi is the design matrix for random eﬀects. The variance matrix Σb
i =
Σb
i(φ) captures within-subject variation and is parameterized by the r × 1
vector φ of nonredundant parameters. Hence θ1 = (β, φ).
When wi ⊆xi, as is usually the case, the bi can be thought of as error
terms for one or more of the regression coeﬃcients, which gives rise to the
term ‘random coeﬃcient model’. For example, if xi = wi, we obtain
µb
i = xiβi = xi(β + bi),
(2.3)
where the random eﬀects bi can be interpreted as individual-speciﬁc deviations
from β.
The within-subject variance Σb
i(φ) usually has a simpliﬁed structure, pa-
rameterized through a covariance function Cb
ijk(φ). For example, an exponen-
tial structure takes the form
Cb
ijk(φ)
=
σ2ρ|tij−tik|,
where φ = (σ2, ρ) and 0 ≤ρ ≤1.
At the second level, the q-dimensional vector of random eﬀects is assigned
a distribution that can depend on covariates. The (multivariate) normal is a
common choice,
bi | xi ∼N(0, Ω),
where Ω= Ω(η) is a q × q covariance matrix indexed by η (hence θ2 = η).

CONDITIONALLY SPECIFIED MODELS
23
It also is possible to allow η to depend on individual-level covariates through
appropriate speciﬁcations (Daniels and Zhao, 2003).
Upon integrating over b, the marginal distribution of Y i follows the mul-
tivariate normal distribution
Y i | xi, wi ∼N(xiβ, wiΩwT
i + Σb
i).
(2.4)
The marginal variance var(Y i | xi) depends on parameters from both p(y |
x, b) and p(b | x). Moreover, we see by comparing (2.3) and (2.4) that β can
be interpreted both as a marginal and a conditional eﬀect of X on Y . We
discuss this further in Section 2.7. A version of this model is used to analyze
the schizophrenia trial in Sections 4.3 and 7.3.
2
2.4.3 Random eﬀects models for discrete responses
Random eﬀects speciﬁcations can be very useful for modeling longitudinal
discrete responses, where the joint distribution rarely takes an obvious form
and principles from generalized linear models are not as easily applied. In the
case of longitudinal binary data, for example, it is straightforward to show that
the joint distribution of a J-dimensional response variable can be represented
by a multinomial distribution with 2J categories. When J is appreciably large,
however, parameter constraints must be imposed to make modeling practical.
See Laird (2004), Chapter 7, for a more detailed discussion.
Compared to direct speciﬁcation of the joint distribution, random eﬀects
models oﬀer the advantage of being parsimonious, providing a natural decom-
position of multiple sources of variation, and applying equally well to balanced
and unbalanced response proﬁles. The regression parameters represent covari-
ate eﬀects in the conditional rather than marginal joint distribution of Y ,
however, and because the link functions are nonlinear transformations of the
mean (e.g., log, logit), these do not generally coincide. Therefore care must be
taken when interpreting regression eﬀects. The logistic regression with normal
random eﬀects illustrates several of these points rather well.
Example 2.2. Logistic regression with random eﬀects.
As in Example 2.1, a logistic random eﬀects model is speciﬁed in terms of the
joint distribution
p(y, b | x, θ)
=
p(y | x, b, θ1) p(b | x, θ2),
where θ = (θ1, θ2). Conditionally on bi, the distribution of each component
Yij of Y i follows the Bernoulli model
Yij | xij, bi ∼Ber(µb
ij),
where
g(µb
ij) = xijβ + wijbi
(2.5)

24
REGRESSION MODELS
(hence θ1 = β). The random eﬀects distribution follows
bi | xi ∼N(0, Ω),
so θ2 corresponds to the nonredundant components of Ω.
The parameter β characterizes the conditional, or subject-speciﬁc eﬀect of
X on Y . By contrast, the marginal — or population-averaged — distribution
p(y | x, θ) must be obtained by integrating over b. When wi is a subset of
xi, the marginal mean µij(β, Ω) = E(Yij | xij, β, Ω) is
µij(β, Ω)
=

µb
ij(β) p(b | Ω) db
=

exp(xijβ + wijb)
1 + exp(xijβ + wijb) p(b | Ω) db.
where p(b | Ω) is the multivariate normal density with mean 0 and variance
Ω. The marginal eﬀect of X diﬀers from the conditional eﬀect in that it is a
function of both β and Ω, and on the logit scale, it is no longer linear.
Zeger and Liang (1992) show that in some cases, the marginal eﬀect in
the logit-normal model is approximately linear on the logit scale, and diﬀers
from the conditional eﬀect by a scale factor that depends on Ω. To illustrate,
consider the simple case of a logistic regression with a single covariate xi and
random intercept; i.e.,
logit(µb
ij)
=
β0 + bi + β1xi,
where bi ∼N(0, ν2). Here, β1 is the conditional eﬀect of x. It can be shown
that the marginal eﬀect of x, denoted by βm
1 , can be approximately represented
by the logistic model
logit(µij)
=
βm
0 + βm
1 xi,
where βm
1 = (c2ν2 + 1)−1/2β, and c ≈.346. Hence the marginal or population
averaged eﬀect of x is attenuated relative to the conditional or subject-speciﬁc
eﬀect, with degree of attenuation governed by the magnitude of the random
eﬀects variance ν2. Interpreting the marginal and conditional eﬀects is con-
sidered further in Section 2.7.
In Section 4.4 we use this model to characterize the eﬀect of a behavioral
intervention on weekly smoking cessation status using longitudinal binary data
from the Commit to Quit I study.
2
Examples 2.1 and 2.2 assume the random eﬀects bi follow a normal dis-
tribution; this is not necessary and in many cases it may be inappropriate
or incorrect. Zhang and Davidian (2001) describe models where the random
eﬀects distribution belongs to a ﬂexible class of densities that includes the
normal as a special case. Verbeke and Lesaﬀre (1996) describe random ef-
fects distributions that follow discrete mixtures of normal distributions. For
simple models, it is sometimes possible to use exploratory analysis in order

DIRECTLY SPECIFIED (MARGINAL) MODELS
25
to ascertain whether a normal or other symmetric distribution is suitable for
describing the random eﬀects. In other cases, more formal methods of model
choice may be needed.
2.5 Directly speciﬁed (marginal) models
This section reviews the family of models in which the joint distribution of Y
given X is directly speciﬁed by a model p(y | x, θ). Usually the most challeng-
ing aspect of model speciﬁcation is ﬁnding a suitable parameterization for the
correlation and/or covariance, particularly when observations are unbalanced
in time or when the number of observations per subject is large relative to
sample size. In these cases, sensible decisions about dimension reduction must
be made.
For continuous data that can be characterized using a normal distribution
or Gaussian process, model speciﬁcation (though not necessarily selection)
can be reasonably straightforward, owing to the natural separation of mean
and variance parameters in the normal distribution. The analyst can focus
eﬀorts separately on models for mean and covariance structure.
Other types of data pose more signiﬁcant challenges to the process of direct
speciﬁcation due to a lack of obvious choices for joint distribution models. Un-
like the normal distribution, which generalizes naturally to the multivariate
and even the stochastic process setting, common distributions like binomial
and Poisson do not have obvious multivariate analogues. One problem is that
the mean and covariance models share the same parameters, even for sim-
ple speciﬁcations. Another potential problem is that unlike with the normal
model, higher-order associations do not necessarily follow from pairwise as-
sociations, and need to be speciﬁed or explicitly constrained (Fitzmaurice
and Laird, 1993). The joint distribution of J binary responses, for example,
has 2J −J parameters governing the association structure. With count data,
appropriate speciﬁcation of even a simple correlation structure is not imme-
diately obvious.
This section describes various approaches to direct model speciﬁcation,
illustrated with examples from the normal and binomial distributions. The
ﬁrst examples use the normal distribution. For longitudinal binary data, we
describe an extension of the log-linear model that allows transparent inter-
pretation of both the mean and serial correlation. Another useful approach to
modeling association in binary data is the multivariate probit model, which
exploits properties of the normal distribution by assuming the binary ran-
dom variables are manifestations of an underlying normally distributed latent
process.

26
REGRESSION MODELS
2.5.1 Multivariate normal and Gaussian process models
The multivariate normal distribution provides a highly ﬂexible starting point
for modeling continuous response data, both temporally aligned and mis-
aligned. It also is useful for handling situations where the number of observa-
tion times is large relative to the number of units being followed. The most
straightforward situation is where data are temporally aligned and n ≫J,
allowing both the mean and variance to be unstructured. When responses are
temporally misaligned, or when J is large relative to n, structure must be
imposed.
A key characteristic of the normal distribution allowing for ﬂexible mod-
eling across a wide variety of settings is that the mean and variance have
separate parameters. The next two examples illustrate a variety of model
speciﬁcations using the normal distribution.
Example 2.3. Multivariate normal regression for temporally aligned obser-
vations.
Assume that observations on the primary response variable are taken at a
ﬁxed schedule of times t1, . . . , tJ. For a response vector Y i = (Yi1, . . . , YiJ)T
with associated J ×p covariate matrix Xi, the multivariate normal regression
is written as
Y i | xi ∼N(µi, Σi),
where µi is J × 1 and Σi is J × J. The mean µi = E(Y i | Xi = xi) follows
a regression model
µi = xiβ,
where xi is the observed J × p covariate matrix and β is a p × 1 vector of
regression coeﬃcients.
The covariance matrix is parameterized with a vector of non-redundant
parameters φ. To emphasize that the covariance matrix may depend on xi
through φ, we sometimes write
Σi(φ) = Σ(xi, φ)
(Daniels and Pourahmadi, 2002). If Σi is assumed constant across individuals,
it has J(J + 1)/2 unique parameters, but structure can be imposed to reduce
this number (Jennrich and Schluchter, 1986). As an alternative to leaving Σi
fully parameterized, common structures for longitudinal data include banded
or Toeplitz (with common parameter along each oﬀ-diagonal), and autore-
gressive correlations of pre-speciﬁed order (N´u˜nez Ant´on and Zimmermann,
2000; Pourahmadi, 2000; Pourahmadi and Daniels, 2002).
The matrix Xi can include information about measurement time, baseline
covariates, and the like. If we set xij = (1, tj) and β = (β0, β1)T, then β1
corresponds to the average slope over time, where the average is taken over

DIRECTLY SPECIFIED (MARGINAL) MODELS
27
the population from which the sample of individuals is drawn. When J is small
enough, xij can include a vector of time indicators, allowing the mean to be
unstructured in time. In Sections 4.2 and 7.2, this model is used to analyze
data from the Growth Hormone Study.
2
In the previous example, it is sometimes possible to allow both the mean
and variance to remain unstructured in time when there are relatively few time
points and covariate levels. When time points are temporally misaligned, or
when the number of observation times is large relative to the sample size,
information at the unique measurement times will be sparse and additional
structure needs to be imposed. Our focus in the next example is on covariance
parameterization in terms of a covariance function. Further details can be
found in Diggle et al. (2002), Chapter 4.
Example 2.4. Multivariate normal regression model for temporally misaligned
observations.
The main diﬀerence in model speciﬁcation when observations are temporally
misaligned has mainly to do with the covariance parameterization. As with
Example 2.3, a normal distribution may be assumed, but with covariance Σi
whose dimension and structure depend on the number and timing of observa-
tions for individual i. The joint distribution follows
Y i | xi ∼N(µi, Σi).
Also as in Example 2.3, µi = xiβ. The covariance Σi(φ) has dimension Ji,
and structure is imposed by specifying a model C(tik, til, φ) for the elements
σikl = cov(Yik, Yil). For example, the model
C(tik, til; φ1, φ2)
=
φ1 exp(−φ|tik−til|
2
)
(2.6)
requires only φ1 and φ2 to fully parameterize Σi(φ) (with the constraints
φ1 > 0 and φ2 ≥0). This covariance model implies
(i) var(Yij | xi, φ) = φ1 (setting tik = til)
(ii) For any two observations Yik, Yil separated by lag |tik −til|,
corr(Yik, Yil | xi, φ) = exp(−φ|tik−til|
2
).
The correlation function above is stationary because for any given lag, the
correlation is a constant function of (tik, til).
As with Example 2.3, the variance and correlation parameters may depend
on covariates through an appropriately speciﬁed model. If we modify (2.6)
such that
σikl = φ1i exp(−φ|tik−til|
2
)
(i.e., φ1 now depends on i), then we may model φ1i as a function of covariates
via
log(φ1i) = xiα.

28
REGRESSION MODELS
In Section 7.6, a semiparametric version of this model will be used to char-
acterize the response of longitudinal CD4 counts to initiation of highly active
antiretroviral therapy (HAART) in the HER Study.
2
2.5.2 Directly speciﬁed models for discrete longitudinal responses
Although the multivariate normal distribution is a natural choice for charac-
terizing the joint distribution of continuous longitudinal responses, approaches
to dealing with discrete observations such as binary, categorical, or count
data are less obvious. This section describes two models for longitudinal bi-
nary data, both of which can be generalized to handle ordinal or multinomial
response.
A challenge in formulating models for the joint distribution of discrete
responses is having a sensible parameterization of the correlation while main-
taining an interpretable regression structure for the mean. This diﬃculty arises
because the correlation is a function of the mean. One approach is to fully
parameterize higher-order interactions and then constrain some of them to
be zero (Fitzmaurice and Laird, 1993; Fitzmaurice et al., 1994, 1996). An-
other is to formulate models that deal explicitly with serial correlation. Our
examples here include the marginalized transition model (MTM) (Heagerty,
2002), which is a constrained version of a log-linear model, and a multivari-
ate probit model, which uses a latent multivariate normal structure to induce
correlation (Chib and Greenberg, 1998). The probit model is used widely in
econometric modeling but less so for biostatistical applications. Model coeﬃ-
cients lack the simplicity of odds ratio interpretation, but the assumption of
an underlying latent normal distribution provides computational tractability
and ﬂexible modeling of serial covariance structures.
Marginalized transition models
We illustrate the formulation of MTMs using an example with ﬁrst-order
dependence.
Example 2.5. Marginalized transition model for temporally aligned binary
data.
Let Y i = (Yi1, . . . , YiJ)T denote a vector of binary responses, and let Xi
denote the design matrix. The marginal mean given covariates xij is
µij = E(Yij | xij) = P(Yij = 1 | xij).
To describe the model, some additional notation is needed. For any time-
dependent variable Z, let Zj = {Z1, . . . , Zj} denote its history up to and
including time j. The MTM likelihood is a transition model where the dis-
tribution of Yij given (Y i,j−1, xij) follows a Bernoulli distribution, but is

DIRECTLY SPECIFIED (MARGINAL) MODELS
29
constrained to allow g(µij) to be linear in covariates.∗Hence the marginal
mean retains its form as a regression, but the distributional assumptions are
made in the serial correlation model.
We use the logistic regression formulation for illustration. The underlying
joint distribution of responses is factored as
p(y1, . . . , yJ | x) = p(y1 | x1) p(y2 | y1, x2) · · · p(yJ | yJ−1, xJ).
Each component of the joint distribution is assumed to follow a Bernoulli
distribution
Yij | yi,j−1, xij ∼Ber(φij),
where φij = E(Yij | yi,j−1, xij). For the ﬁrst-order dependence model, de-
noted MTM(1), we have
p(yj | yj−1, xj) = p(yj | yj−1, xij).
The model is speciﬁed in terms of two simultaneous equations. The ﬁrst allows
logit of the marginal mean µij to depend linearly on covariates xij; the second
characterizes the dependence structure described by the conditional mean φij,
logit(µij)
=
xijβ
logit(φij)
=
∆ij + yi,j−1γj,
(2.7)
where ∆ij = ∆(xij) is determined by β and γj. If serial correlation γj depends
on individual-level covariates wij ⊆xij, then we replace γj by γij, where
γij = wijα.
These constraints imply that for a given value of xij, ∆ij is a deterministic
function of γj, β, and y (Heagerty, 2002). To see this clearly, ﬁrst write
φij = φ(∆ij, γj, y) = E(Yij | Yi,j−1 = y)
to emphasize its dependence on Yi,j−1. Then
µij(β)
=
P(Yij = 1 | xij)
=
P(Yij = 1 | Yi,j−1 = 0, xij, γ, ∆ij) P(Yi,j−1 = 0 | xi,j−1, β)
+ P(Yij = 1 | Yi,j−1 = 1, xij, γ, ∆ij) P(Yi,j−1 = 1 | xi,j−1, β)
=
φij(γ, ∆ij, 0){1 −µi,j−1(β)} + φij(γ, ∆ij, 1)µi,j−1(β).
(2.8)
In Section 4.4 we use this model to analyze data from the CTQ I study
(Section 1.4) and compare the results to those obtained with the logistic-
normal random eﬀects model described in Example 2.2. In Section 7.4, we
also use this model to analyze data from the same study under MAR.
2
∗In many cases, we implicitly assume xij includes relevant covariate history up to and
including time j, obviating the need for overbar notation.

30
REGRESSION MODELS
In practice, the MTM works well for ﬁrst-order Markov dependence, but
higher-order dependence structures can make computation of ∆ij more diﬃ-
cult. Because the models are derived from log-linear speciﬁcations, they can be
extended to handle ordinal or multinomial response (Lee and Daniels, 2007).
MTMs are well-suited to temporally aligned data, but can be expanded to
handle temporally misaligned observations through properly speciﬁed design
matrices in both the mean and serial correlation. Su and Hogan (2007) develop
a ﬂexible approach using penalized splines.
When interest focuses only on the transition probabilities φij and not the
marginal mean, then the conditional part (2.7) of the MTM can be used for
inference (it is simply a ﬁrst-order Markov model). The intercept ∆ij in (2.7)
is replaced by xijψ, and the parameters of interest are either the γj or ψ.
Multivariate probit models
Another approach to handling longitudinal discrete data, the multivariate
probit model, exploits the separation of mean and variance in the normal dis-
tribution by assuming the existence of a latent multivariate normal structure
that gives rise to observed data. Probit models are widely used in psychomet-
rics and econometrics, where justiﬁcation for and interpretation of the latent
scales can frequently be made on subject matter grounds (see Lancaster, 2004,
Section 5.2, for example).
In epidemiology and public health applications, justiﬁcation for the use of
latent scales is less obvious. In any application, the data oﬀer limited justiﬁ-
cation for the assumption that the latent variable follows a normal distribu-
tion, which has invited criticism of these models. Nevertheless, probit models
aﬀord considerable opportunity to parameterize covariance structures for dis-
crete data and covariances between discrete and continuous data, a property
that is diﬃcult to overlook. Moreover, the assumed latent structure makes
inference via posterior sampling rather easy to implement (see Chapter 3).
The probit link is functionally quite similar to the logit link, so the ﬁt is typ-
ically very similar between the two models. This example gives some details
about speciﬁcation. More details on the multivariate probit model, including
methods for inference, are found in Chapters 3 and 6.
Example 2.6. Multivariate probit model.
The probit model based on an underlying multivariate normal model can be
used to directly model the joint distribution of longitudinal binary data. The
multivariate probit model assumes the existence of a vector of latent variables
Zi = (Zi1, . . . , ZiJ)T that follow a multivariate normal distribution
Zi | xi ∼N(µi, Ψi),
where µi = xiβ, Ψi = Ψi(φ) is a J × J correlation matrix with elements
{ψkl(φ)}, and θ = (β, φ). The diagonal elements ψkk are set to one for identi-
ﬁability (Chib and Greenberg, 1998). Interpretation of regression parameters

SEMIPARAMETRIC REGRESSION
31
for probit models is most easily done on the latent scale: the coeﬃcient βp is
the average diﬀerence in standard deviations of Zij corresponding to one-unit
diﬀerences in xijp.
The binary observations Yij are viewed as manifestations of the underlying
latent variables such that Yij = I{Zij > 0}. Marginally,
Yij | xij ∼Ber(µij),
with µij = Φ(xijβ), where Φ(·) is the cdf of a standard normal distribution.
Within-subject dependence structure is modeled through Ψ(φ). The underly-
ing multivariate normal distribution of Zi induces the correlation and leads to
use of the normal cdf for the transformation of xijβ to the probability scale.
The multivariate distribution p(y | x, θ), and in particular its serial corre-
lation structure, is obtained by integrating p(z | x, θ) over a subspace of RJ
deﬁned by the observed binary outcomes Y i; i.e.,
p(y1, . . . , yJ | x, θ) =

Q(y)
p(z | x, θ) dz,
where in this case p(z | x, θ) is a J-variate normal density function having
mean and correlation parameters in θ, and Q(y) ⊂RJ deﬁnes the limits of
integration over z such that the jth integral is taken over (0, ∞) when yj = 1
and over (−∞, 0] if yj = 0. A probit model is used in Section 7.5 to illustrate
auxiliary covariates.
2
2.6 Semiparametric and nonlinear regression
Our focus thus far has been on models where the regression function is speci-
ﬁed as a linear function of covariates. Although it is possible, by transforming
covariates, to specify models that are implicitly nonlinear, in practice lin-
ear models can be limiting in some settings. Models where the regression
function is allowed to be explicitly nonlinear, either through speciﬁcation of
some known function or by allowing a ﬂexible, unspeciﬁed function, oﬀer a
wide array of alternatives and signiﬁcantly expand the capability of regres-
sion modeling. Our focus in this section is on models that allow nonlinearity
in time trends, although in principle the models we discuss can be used to
characterize nonlinear eﬀects of any covariate.
In many models with nonlinear time trends, an appropriate functional form
can be speciﬁed directly; the canonical reference for this line of work is the
book by Davidian and Giltinan (1998) and a recent update by the same au-
thors (Davidian and Giltinan, 2003). These models are especially useful in
settings like pharmacokinetics, viral dynamics, and growth curve modeling,
where processes like drug uptake, viral replication, and growth rate can be
described by known but potentially complex functions of time (see Wakeﬁeld,
1996 and Wu and Wu, 2002, for examples).

32
REGRESSION MODELS
The main focus of this section is on semiparametric models, particularly
those in which covariate eﬀects involving time can be captured with unspec-
iﬁed smooth functions. Here we divide the model types into two categories:
generalized additive models (GAM) and varying coeﬃcient models (VCM). In
a GAM, the main eﬀect of time is characterized by a smooth function f(t);
for longitudinal or repeated measures data, f(t) can be inferred using random
eﬀects models where the smoothness parameter emerges as a variance com-
ponent (Zhang et al., 1998; Ruppert et al., 2003); Example 2.7 below is used
to illustrate. In the VCM, eﬀects of a covariate (say x) are allowed to vary
with time via β(t)x, where the regression coeﬃcient β(t) is a smooth function
of time (Hoover et al., 1998; Chiang et al., 2001; Zhang, 2004). Example 2.8
below gives a formulation for continuous responses and a scalar covariate.
The review here is conﬁned to likelihood-based methods, but readers should
be aware of the sizable literature on moment-based methods for ﬁtting semi-
parametric regression (see for example Lin and Carroll, 2001; Ruppert et al.,
2003 and Wang et al., 2005). The emerging literature connecting functional
data analysis with longitudinal repeated measures also is highly relevant; see
for example Guo (2004), Rice (2004), and Zhao et al. (2004).
2.6.1 Generalized additive models based on regression splines
Consider the cross-sectional data model with scalar covariate
Yi | xi ∼N(µi, σ2),
with µi = f(xi) for an unknown smooth function f(·). A natural cubic smooth-
ing spline estimator of f is the value f that minimizes a penalized sum of
squares
n
	
i=1
{Yi −f(Xi)}2 + λ

{f ′′(x)}2 dx,
(2.9)
where f ′′ denotes the second derivative of f. For a suitably chosen n ×
n matrix K, the penalty term can be rewritten as λf TKf, where f =
(f(x1), . . . , f(xn))T; this term penalizes overﬁtting, and in particular the pa-
rameter λ governs smoothness of the ﬁtted curve f.
Regression splines for longitudinal data can be similarly constructed; when
using splines to model time trends, longitudinal data aﬀords the advantage of
replicated curves across individuals. This structure can be exploited to permit
automated smoothing by formulating models where λ is a variance component
(Zhang et al., 1998; Ruppert et al., 2003), and permits ﬁtting both GAM and
VCM. The basic principle of formulating a semiparametric model as a linear
regression model is illustrated in the context of penalized regression splines
(called p-splines).

SEMIPARAMETRIC REGRESSION
33
Example 2.7. A regression spline for continuous longitudinal data.
This example is designed to show that an unspeciﬁed time trend f(t) can be
estimated within a linear model formulation. Assume Y i = (Yi1, . . . , Yi,Ji)T
is measured at times ti = (ti1, . . . , ti,Ji)T. The model is
Y i | ti ∼N(µi, Σi),
with
µij
=
f(tij)
(2.10)
σijk(φ)
=

σ2
j = l
σ2 exp(−φ|tij −til|)
j ̸= l
.
(2.11)
A p-spline model represents f(t) in terms of an R-dimensional basis B(t) =
{B1(t), . . . , BR(t)}T, where R is less than or equal to the number of unique
observation times. A common choice is based on pth-degree polynomial splines
with pre-selected knot points s1, . . . , sk along the time axis (Berry et al., 2002),
B(t) = (1, t, t2, . . . , tq, (t −s1)q
+, (t −s2)q
+, . . . , (t −sk)q
+)T.
(2.12)
Here, a+ = aI{a > 0}. This basis has dimension R = q + k + 1; the knots
can be chosen in a variety of ways; Berry et al. (2002) recommend using
percentiles of the unique observation times. In practice, a linear (q = 1) or
quadratic (q = 2) basis is often suﬃcient.
The linear model formulation replaces f(tij) in (2.10) with B(tij)Tβ, where
β is R × 1. Inference is then based on a likelihood having a penalty term
λβTDβ, where D is a ﬁxed R×R penalty matrix. The choice of basis function
will motivate the form of D. If (2.12) is used, coeﬃcients of the terms involving
knot points correspond to jumps in the qth derivative of f(t) at each knot
point. To ensure smoothness, one could penalize the sum of squared jumps
at the knot points via a simple form of D that places 1’s along the diagonal
corresponding to βq+2, . . . , βR,
D =


0(q+1)×(q+1)
0(q+1×k)
0k×(q+1)
τ 2Ik


(Ruppert and Carroll, 2000). Given this form of D, we partition the regression
parameters as
β = (α0, α1, . . . , αq, a1, . . . , ak)T = (αT, aT)T.
The penalty can then be formulated as a (random eﬀects) distribution on
a = (a1, . . . , ak)T,
a | τ ∼N(0, τ2Ik),
(2.13)
and ﬁt using mixed models software. The original penalty parameter λ is
related to the variance component σ2 in (2.11) and τ2 in (2.13) via λ = σ2/τ 2.

34
REGRESSION MODELS
In Section 7.6, we use this model to analyze longitudinal CD4 data from the
HER Study.
2
This approach can be used to estimate f(t) via natural cubic smoothing
splines (Zhang et al., 1998), and applies to discrete data in a GLM framework;
see Lin and Zhang (1999) for a comprehensive account.
2.6.2 Varying coeﬃcient models
Another type of semiparametric model for longitudinal data, similar to the
GAM, is the varying coeﬃcient model. The model allows covariate eﬀects to
change with time by allowing the coeﬃcient β(t) to be an unspeciﬁed smooth
function of time, a particularly useful feature in settings where covariate eﬀects
are not time-constant and at the same time do not have an obvious functional
form. Although VCMs have a diﬀerent speciﬁcation than GAMs, the method
of estimation is similar. Recent work by Zhang (2004) shows that β(t) can be
estimated by smoothing splines using a modiﬁed linear model similar to the
one described in Example 2.7.
Example 2.8. Varying coeﬃcient model for continuous responses.
Let xi represent a scalar baseline covariate, and let t denote time elapsed from
baseline. The following VCM allows the eﬀect of x to vary over time through
an unspeciﬁed function for the regression coeﬃcient. At the ﬁrst level, we
assume
Yij | xi, tij ∼N(µij, σ2(tij)),
where the correlation function from Example 2.7 applies. One can specify the
following VCM that allows an overall linear time trend, but an eﬀect of x that
varies with time:
µij = β0 + β1tij + β2(tij)xi.
Here, β0 and β1 are unknown parameters, and β2(t) is a smooth function of t.
Hence
E(Yij | xi, t = t∗) = β0 + β1t∗+ β2(t∗)x
is the mean at some ﬁxed time t = t∗. The model can be extended to allow
the intercept to depend on time via β0(t) (making the VCM a generalization
of the GAMs described above) and to models for discrete data (Zhang, 2004).
In Section 10.4, we use these models to handle continuous time dropout in
the Pediatric AIDS trial.
2
2.7 Interpreting covariate eﬀects
Proper interpretation of covariate eﬀects in models for longitudinal data re-
quires careful attention to model speciﬁcation and assumptions about covari-

INTERPRETING COVARIATE EFFECTS
35
ate processes. The use of time-varying covariates in particular allows inves-
tigators to study both within- and between-individual covariate eﬀects, but
incorporation of these covariates and interpretation of their eﬀects must be
done with care.
This section focuses on several aspects of model speciﬁcation and interpre-
tation, including assumptions about covariates, diﬀerentiation between lon-
gitudinal and cross-sectional eﬀects of a time-varying covariate, and diﬀer-
entiation between marginal and conditional eﬀects in random eﬀects models.
These topics are covered in considerable detail elsewhere; readers are referred
to Chapters 1 and 12 of Diggle et al. (2002), Chapter 15 of Fitzmaurice et al.
(2004), and Chapter 1 of Laird (2004).
2.7.1 Assumptions regarding time-varying covariates
Throughout the book, we assume covariates are exogenous and measured with-
out error. Exogeneity is a standard assumption for nearly all regression mod-
els, and in its simplest form asserts independence between errors and covari-
ates. In longitudinal models, exogeneity must be met for both time-constant
and time-varying covariates. Time-varying covariates can be either determin-
istic or stochastic functions of time. Deterministic functions of time include
calendar date or participant age; if they are exogenous at baseline, then their
time-varying counterparts also will be.
Stochastic covariate processes can be more problematic and must be treated
with caution. A stochastic time-varying covariate will usually be exogenous if
it is completely ‘external’ to the measurement process (e.g., daily air pollution
levels measured at the city level in a study where response is daily number of
asthma events measured at the individual level). However, stochastic covari-
ates measured at the same level as the response will tend to be endogenous
because they may be systematically predicted by prior responses. In cohort
studies of HIV infection, suppose that for measurement occasion j, Xj is ex-
posure to antiviral treatment and Yj is CD4 cell count. It is highly probable
that Yj is predictive of Xj+1 because variations in CD4 count are clinically
important to initiation or adjustment of treatment.
Consider another example where the objective is to characterize the eﬀect
of physical functioning (Xj) on depression (Yj). It is easy to imagine that
changes in physical functioning will be associated with subsequent changes in
depression (i.e., Xj predicts Yj+1), but the converse also may be true: changes
in depression may also lead to changes in physical functioning (i.e., Yj or some
function of Y1, . . . , Yj predicts Xj+1).
Standard regression models are not usually suitable for characterizing the
eﬀects of time-varying stochastic covariates unless those covariates are ex-
ogenous. Eﬀects of endogenous covariates are best summarized in terms of
structural models. These require the use of instrumental variables or propen-

36
REGRESSION MODELS
sity score methods (Angrist et al., 1996; Hern´an et al., 2001; Wooldridge,
2001; van der Laan and Robins, 2003). In other circumstances, it is more ap-
propriate to treat {Yj, Xj} as a joint process and use a model to characterize
its joint evolution and association structure. See Jones et al. (2003) and Liang
et al. (2003) for applications in HIV.
Measurement error is another important concern and can introduce appre-
ciable bias. The literature on this topic is deep and wide-ranging; see Fuller
(1987) and Carroll et al. (1995) for a full treatment.
2.7.2 Longitudinal vs. cross-sectional eﬀects
Longitudinal data aﬀords researchers the opportunity to study both within-
and between-subject eﬀects for exogenous time-varying covariates (Ware, 1985;
Diggle et al., 2002). In aging research, for example, there is interest in the
average eﬀect of age on variables like physical functioning and depression.
Suppose that Yij represents level of depression and Xij is age at assessment
j (age is exogenous). The cross-sectional eﬀect is a between-subject eﬀect,
and represents the diﬀerence in mean depression score between cohorts of in-
dividuals that diﬀer by one unit in age. In the simplest formulation of the
model, this diﬀerence is assumed to be time-constant. The longitudinal eﬀect
is a within-subject measure of association, and represents the average change
in depression score per unit change in age, within an individual.
Clearly the longitudinal and cross-sectional eﬀects measure diﬀerent types
of association, and frequently take diﬀerent values. The linear model with
identity link provides a convenient illustration The ideas here apply more
generally to other models where the covariate is exogenous (see Laird, 2004,
Section 1.4, for a general treatment). For the model
E(Yij | xij) = β0 + β1xi1 + β2(xij −xi1),
β1 measures cross-sectional (between-subject) association and β2 captures lon-
gitudinal (within-subject) association. The choice of centering on xi1 is some-
what arbitrary; in this case, the cross-sectional eﬀect is
β1 = E(Yi1 | xi1 = x + 1) −E(Yi1 | xi1 = x),
and the longitudinal eﬀect is
β2 = E(Yij −Yij′ | xij −xij′ = 1).
More generally, we can write E(Yij −Yij′ | xij, xij′) = β2(xij −xij′). Besides
the importance to interpretation of covariate eﬀects, this decomposition is
central to design of longitudinal studies (Diggle et al., 2002).

INTERPRETING COVARIATE EFFECTS
37
2.7.3 Marginal vs. conditional eﬀects
As indicated earlier in this chapter, the regression coeﬃcients have distinctly
diﬀerent interpretations in marginal and conditional models. The marginal
mean is deﬁned as the mean response, averaged over the population from
which the sample is drawn. Regression coeﬃcients in directly speciﬁed marginal
models capture the eﬀects of covariates between groups of individuals deﬁned
by diﬀerences in covariate values; these eﬀects are sometimes referred to as
‘population-averaged’ eﬀects (Zeger and Liang, 1992).
By contrast, regression coeﬃcients in conditionally speciﬁed random ef-
fects models capture the eﬀect of a covariate at the level of aggregation in the
random eﬀect. For example, in model (2.5), random eﬀects capture individual-
level variation; the regression coeﬃcients β therefore represent covariate ef-
fects at the individual level. When the level of aggregation represented by
random eﬀects is an individual or subject, the conditional regression param-
eters are sometimes referred to as ‘subject-speciﬁc’ eﬀects.
An important consideration in the interpretation of regression parameters
from models of repeated measures is whether the covariate in question varies
only between individuals, or whether it also varies within individuals (Fitz-
maurice et al., 2004). Two examples serve to illustrate the point. Consider
ﬁrst the longitudinal studies of smoking cessation described in Section 1.4,
where the binary response is cessation status Yij at measurement occasions
j = 1, . . . , J. Further suppose that each individual is enrolled in one of two
treatment programs, denoted by Xi ∈{0, 1}. A random eﬀects logistic re-
gression can be used to characterize a conditional or subject-speciﬁc eﬀect
of X,
logit{E(Yij | xi, bi, β)} = bi + β0 + β1xi,
where (say) bi ∼N(0, σ2). The random eﬀect bi hypothetically captures all
observable variation in the mean of Yij that is not explained by x, and in
that sense two individuals with the same value of bi can be thought of as
perfectly matched with respect to unobserved individual-level covariate ef-
fects. Hence the coeﬃcient β1 can be interpreted as the eﬀect of treatment
on smoking cessation for two individuals with the same unobserved covariate
proﬁle (i.e., the same value of bi), or for two individuals whose predictors of
smoking cessation, other than treatment, are exactly the same. Here, as in
all random eﬀects models, it is being assumed that unobserved characteristics
can be captured with a scalar value bi that follows a normal distribution. Both
are strong assumptions, and the ‘matching’ interpretation hinges critically on
correct speciﬁcation of the model.
When X is a covariate that could potentially be manipulated (such as
treatment or environmental exposure), and if the study design is appropriate,
then β1 sometimes can be interpreted as the causal eﬀect of X on Y because
it is the conditional eﬀect of X, given a covariate measure that is technically

38
REGRESSION MODELS
unique to each individual and captures information from omitted individual-
speciﬁc covariates. If X cannot be manipulated, for example as with gender
or race at the individual level, then causal interpretations are not appropriate
because they are not well-deﬁned and information on within-subject variation
is not available. See Chapter 13 in Fitzmaurice et al. (2004) for a discussion
focused on appropriate use of random eﬀects models for causal inference.
2.8 Further reading
Marginally speciﬁed multilevel models
Miglioretti and Heagerty (2004) specify marginalized models with Markov de-
pendence and random eﬀects in the setting of breast cancer screening. Schild-
crout and Heagerty (2007) also propose marginalized models with Markov
dependence and random eﬀects but in settings of long series of binary re-
sponses. Ilk and Daniels (2007) specify a three level marginalized model to
address serial correlation and multivariate dependence for multivariate longi-
tudinal binary data.
Functional data analysis
Functional data analysis has important applications in longitudinal settings.
For recent developments, see Zhao et al. (2004), Yao et al. (2005a) and Yao
et al. (2005b). For a recent Bayesian treatment, see Rodriguez et al. (2007).

CHAPTER 3
Methods of Bayesian Inference
3.1 Overview
The Bayesian approach to modeling provides a natural framework for mak-
ing inferences from incomplete data. Bayesian inference is characterized by
specifying a model, then specifying prior distributions for the parameters of
the models, and then lastly updating the prior information on the param-
eters using the model and the data to obtain the posterior distribution of
the parameters. Analyses with missing data involve assumptions that cannot
be veriﬁed; assumptions about the missing data (and our uncertainty about
them) can be made explicit through prior distributions.
This chapter reviews some key ideas in Bayesian statistics for complete
data, including specifying prior distributions, computation of the posterior
distribution, and strategies for assessing model ﬁt. Chapters 6, 8, and 9 will
discuss extensions and modiﬁcations of these concepts in speciﬁc missing data
settings. Many procedures discussed in this chapter can be implemented in
WinBUGS software. We indicate those settings where special programming
may be needed.
Many of our examples are based on models described in Chapter 2. In the
following, we will mostly suppress conditioning on the model covariates x.
3.2 Likelihood and posterior distribution
3.2.1 Likelihood
Let Y i = (Yi1, . . . , Yi,Ji)T denote observed response data for individual i,
where i = 1, . . . , n. Independence is assumed across subjects. For a parameter
θ, the likelihood L(θ | y) is any function proportional in θ to the joint density
evaluated at the observed sample,
L(θ | y)
∝
p(y1, . . . , yn | θ)
=
n

i=1
p(yi | θ).
The function p(yi | θ) denotes a probability density (or mass) function for the
Ji observations on subject i. The likelihood summarizes the information the
data y has about the parameters θ. We will see in subsequent chapters that in
39

40
BAYESIAN INFERENCE
incomplete data settings, the likelihood does not always provide information
about all the components of θ. Likelihood for some of the models introduced
in Chapter 2 are given next.
Example 3.1. Likelihood for multivariate normal model for temporally aligned
observations (continuation of Example 2.3).
For the model with temporally aligned observations Ji = J, the likelihood
takes the form
L(θ | y) =
n

i=1
|Σ(φ)|−1/2 exp{−1
2ei(β)TΣ(φ)−1ei(β)T},
(3.1)
where ei(β) = yi −xiβ and θ = (β, φ).
2
Example 3.2. Likelihood for multivariate normal model for temporally mis-
aligned observations (continuation of Example 2.4).
Suppose subject i has Ji observations at times {ti1, . . . , ti,Ji}. In Example 3.1
each individual was assumed to have J observation times. Let C(·, ·; φ) be a
covariance function as deﬁned in Example 2.4.
The likelihood contribution for subject i has the same form as (3.1), but
with Σ(φ) replaced with Σi having the (k, l) element parameterized via
C(tik, til; φ). Here, we are assuming the covariance for observations at times
tk and tl is the same across subjects.
2
Example 3.3. Likelihood for normal random eﬀects model (continuation of
Example 2.1).
The marginal likelihood (after integrating out the random eﬀects bi) is typi-
cally used for inference. It is given by
L(θ | y)
∝
n

i=1

|Σ|−1/2 exp

−1
2ei(β, bi)T(Σb
i)−1ei(β, bi)

p(bi | Ω)dbi
=
n

i=1
|Ai|−1/2 exp

−1
2ei(β)TA−1
i ei(β)

,
where
ei(β, bi)
=
yi −xiβ −wibi,
ei(β)
=
yi −xiβ,
Ai
=
Σb
i + wiΩwT
i ,
p(b | Ω) is a normal density with mean 0 and covariance matrix Ω, and
θ = (β, Σ, Ω).
2
Example 3.4. Likelihood for marginalized transition model of order 1 (con-
tinuation of Example 2.5).

THE POSTERIOR DISTRIBUTION
41
The likelihood takes the form
L(θ | y)
∝
n

i=1


µyi1
i1 (1 −µi1)1−yi1
J

j=2
φyij
ij (1 −φij)1−yij


,
where µi1 = P(Yi1 = 1 | xi1; β) and φij = P(Yij = 1|yi,j−1, xij; β, α) for
j > 1. The ﬁrst term, µyi1
i1 (1−µi1)1−yi1, corresponds to the contribution from
each subject at the initial time; the second, φyij
ij (1 −φij)1−yij, each subject’s
contribution over the subsequent times. The forms of these probabilities are
given in Example 2.5.
The likelihood for non-marginalized transition models takes the same form,
but the conditional probabilities are not constrained by the model for the
marginal mean (Diggle et al., 2002).
2
Example 3.5. Likelihood for multivariate probit model (continuation of Ex-
ample 2.6).
The likelihood takes the form
L(θ | y) ∝
n

i=1

Q(yi)
|Ψ|−1/2 exp

−1
2(zi −xiβ)TΨ−1(zi −xiβ)

dzi,
where Q(yi) determines the limits of integration of zi, and θ = (β, Ψ).
2
3.2.2 Score function and information matrix
Two important quantities related to the likelihood are the vector valued score
function and the information matrix. Deﬁne ℓ(θ | y) = log L(θ | y). The score
function, S(θ | y), is deﬁned as
S(θ | y) = ∂ℓ(θ | y)
∂θ
.
The equations given by S(θ | y) = 0 are called the score equations. The root
θ of the score equations is the maximum likelihood estimator (mle) for θ, and
is typically used as the point estimator in frequentist inference.
A second quantity of interest is the information matrix I(θ), deﬁned as
I(θ) = −EY
∂2ℓ(θ | Y )
∂θdθT

.
This matrix quantiﬁes the rate of curvature of the log likelihood at θ. In
a frequentist setting, standard errors for θ can be approximated using the
diagonal elements of I(θ)−1; i.e., the information evaluated at the mle. When
I(θ) does not take a closed form, as is often the case, the observed information
I(θ | y) = −∂2ℓ(θ | y)
∂θdθT
= −∂S(θ | y)
∂θ
can be used.

42
BAYESIAN INFERENCE
3.2.3 The posterior distribution
Bayesian inference involves updating the prior distribution p(θ) using the
information in the data as quantiﬁed by the likelihood L(θ | y).
Deﬁnition 3.1. Posterior distribution.
Given a prior p(θ) and a likelihood L(θ | y), the posterior distribution of θ is
p(θ | y) =
L(θ | y)p(θ)

L(θ | y)p(θ)dθ.
(3.2)
The denominator is a normalizing constant, so as a function of θ, we have
p(θ | y) ∝L(θ | y)p(θ).
2
If p(θ) is proportional to a constant, then the posterior mode of θ will be
equivalent to the maximum likelihood estimator. However the posterior mode
is not invariant to a re-parameterization of θ because the prior distribution is
not invariant to re-parameterizations.
In most cases, expressing the posterior distribution in closed form is com-
plicated because the normalizing constant for the posterior is frequently an
intractable high-dimensional integral (more on this later). The most common
approach to inference using the posterior is to obtain a sample from the pos-
terior distribution using techniques that do not require explicit evaluation
of the denominator. Alternatively, the posterior distribution of θ can be ap-
proximated with analogs of the score and information based on the likelihood
and the prior (Tierney and Kadane, 1986); however, such approximations will
prove more useful in implementing some of the computational approaches to
sample from the posterior distribution (see Section 3.4.2). The posterior can
also be viewed in a frequentist context, as we see below.
Deﬁnition 3.2. Posterior consistency.
We call the posterior distribution consistent if it approaches a point mass at
the true value of the parameters as the sample size goes to inﬁnity.
2
Posterior Summaries
The posterior is usually summarized with a few carefully chosen quantities.
For point estimates, the posterior mean and median are common choices.
Uncertainty is represented either by the standard deviation of the posterior
(counterpart to the standard error in frequentist analysis) or by forming a
credible interval based on the percentiles of the posterior distribution. For
example, 95% credible intervals of a scalar parameter often are constructed
using the 2.5th and 97.5th percentiles of the posterior distribution.
The choice of summary measures for the posterior distribution can be de-
rived formally under a decision theoretic framework by minimizing a loss

PRIOR DISTRIBUTIONS
43
function, L (θ, a), which is a function of the parameter θ and its estimator a.
From a Bayesian decision-theoretic perspective, a is chosen to minimize the
expectation of this loss with respect to the posterior distribution p(θ | y), and
is called a Bayes estimator. For example, under L (θ, a) = (θ −a)2 (squared
error loss), the Bayes estimator is the posterior mean. We refer the reader to
Berger (1985) or Carlin and Louis (2000) for other loss functions and addi-
tional details.
Hypothesis testing can be conducted formally using Bayes Factors (Kass
and Raftery, 1995) or more informally by examining relevant features of the
posterior distribution. As an example of the latter, a point null hypothesis
could be tested by examining whether the credible interval covers the null
value. In addition, for a one-sided test, we could quantify the strength of
evidence by computing, e.g., the posterior probability that the parameter is
greater than the null value. We will illustrate these approaches in the data
examples in Chapter 4. Hypothesis testing as it relates to comparing the ﬁt
of diﬀerent models is discussed further in Section 3.5.
3.3 Prior Distributions
Prior distributions quantify a priori knowledge about θ. Prior information
can be represented through a probability distribution, usually called an infor-
mative prior. In the absence of ‘prior’ information, vague prior beliefs can be
reﬂected using diﬀuse probability distributions (often called default or non-
informative priors).
We begin our review of priors by introducing conjugate and related pri-
ors, which are the most commonly used priors in Bayesian modeling. After
reviewing common priors, we give recommendations on choosing priors for
parameters in longitudinal models.
3.3.1 Conjugate priors
Conjugate priors are constructed such that the prior and posterior distribution
are from the same family of distributions (e.g., both normal distributions). As
a result, the posterior distribution can be expressed in closed form.
Example 3.6. Conjugate priors for a normal linear regression model.
Consider a normal linear regression model
Yi | xi, β, σ2 ∼N(xiβ, σ2)
with σ2 known. A conjugate prior on the regression coeﬃcients β is
β | β0, V β ∼N(β0, V β),

44
BAYESIAN INFERENCE
where β0 and V β are ﬁxed. Parameters indexing the prior are called hyper-
parameters. The posterior distribution for β is N(β, V ), where
β
=

ixixT
i /σ2 + V −1
β
−1 
V −1
β β +

ixixT
i /σ2
β0

,
(3.3)
V
=

ixixT
i /σ2 + V −1
β
−1
,
(3.4)
and β is the ordinary least squares estimator of β.
2
When conjugate priors cannot be found for the model of interest, priors that
are conditionally conjugate are often speciﬁed to facilitate posterior sampling
(see Section 3.4) using full conditional distributions.
Deﬁnition 3.3. Full conditional distribution.
The full conditional distribution for a parameter θj ∈θ is the distribution of
θj, given ({θk : k ̸= j}, y).
2
Deﬁnition 3.4. Conditionally conjugate prior.
A prior for a parameter (or set of parameters) is called conditionally conju-
gate when it is from the same family of distributions as its full conditional
distribution.
2
Here are some examples of conditionally conjugate priors; for a summary of
(conditionally) conjugate priors, see Carlin and Louis (2000).
Example 3.7. Conditionally conjugate priors for a normal linear regression
model (continuation of Example 3.6).
Consider again the normal linear regression model from Example 3.6, now
with σ2 unknown. A normal prior on β is a conjugate prior, conditional on σ2,
i.e., p(β | y, σ2) is normal with mean and variance given in (3.3) and (3.4),
respectively. However, p(β | y) is not a normal distribution. If we place a
Gamma(a, b) prior on 1/σ2, the full conditional distribution of 1/σ2 is
Gamma

n/2 + a,

1/b + 
i(yi −xiβ)2/2
−1
.
The marginal posterior p(1/σ2 | y), however, is not a gamma distribution.
These two full conditional distributions can be generated from to obtain a
sample from the joint posterior distribution of (β, σ2). Details follow in Sec-
tion 3.4.
2
Example 3.8. Conditionally conjugate priors for a multivariate normal model
with temporally aligned observations (continuation of Example 2.4).
Recall the multivariate normal regression model
Y i | β, Σ ∼N(xiβ, Σ).
In this model, the conditionally conjugate priors for β and Σ−1 are multivari-
ate normal and Wishart, respectively (see Appendix). When Σ−1 is assumed

PRIOR DISTRIBUTIONS
45
to have a particular structure, conjugate priors of known form are often not
available except in special cases.
2
Example 3.9. Conditionally conjugate priors for normal random eﬀects model
(continuation of Example 2.1).
Recall the normal random eﬀects model,
Y i | bi, β, σ2
∼
N(xiβ + wibi, Σb
i)
bi | Ω
∼
N(0, Ω).
Here, we assume Σb
i = σ2I. The conditionally conjugate priors for this model
are the same as Example 3.8 supplemented by a Wishart prior on Ω−1.
2
Example 3.10. Conditionally conjugate priors for Bayesian penalized splines
(continuation of Example 2.7).
Recall that the model is
Y i | ti ∼N(µi, Σi),
(3.5)
with
µij
=
f(tij)
(3.6)
σijk(φ)
=

σ2
j = k
σ2 exp(−φ|tij −tik|)
j ̸= k
.
The function f(t) is represented in terms of the spline basis given in (2.12),
B(t) = (1, t, . . . , tq, (t −s1)q
+, . . . , (t −sk)q
+)T
having dimension R = (q + 1) + k with a+ = aI{a > 0}. The penalty has the
form λβTDβ, where D is a diagonal matrix having the ﬁrst q + 1 diagonal
elements equal to zero and the last k equal to one (Berry, Carroll, and Ruppert,
2002).
This penalty can be reformulated as a (conditionally conjugate) prior dis-
tribution on β. To do this, we again partition β into (α, a), where α is (q+1)-
dimensional coeﬃcient vector corresponding to the ﬁrst (q + 1) components
of the basis and a is k-dimensional coeﬃcient vector corresponding to the
last k components. The penalty described above corresponds to conditionally
conjugate prior on α (a ﬂat prior on Rp+1) and a normal prior on a,
a ∼N(0, τ2Ik).
The smoothing parameter λ is the ratio of the two variances λ = σ2/τ 2.
Conditionally conjugate priors for the inverse of the variance components
(σ2, τ2) are Gamma priors (Craineceau, Ruppert, and Wand, 2005; Berry et
al., 2002).
2

46
BAYESIAN INFERENCE
3.3.2 Noninformative priors
In many models, some components of θ are not of primary interest (nui-
sance parameters) and/or may not have prior information available. Various
approaches are available to express ‘ignorance’ via an appropriately speci-
ﬁed prior distribution. These ‘default’ or noninformative priors are often con-
structed to satisfy some reasonable criterion such as invariance, admissibility,
minimaxity, and/or favorable frequentist properties (Berger and Bernardo,
1992; Mukerjee and Ghosh, 1997). For example, the Jeﬀreys’ prior p(θ) ∝
|I(θ)|1/2 is invariant to the parameterization of θ (Jeﬀreys, 1961).
Example 3.11. Jeﬀreys’ prior for a normal linear regression model (contin-
uation of Example 3.6).
Again, consider the regression model in Example 3.6 with σ2 known. Jeﬀreys’
prior for the regression coeﬃcients β is
p(β) ∝1.
The posterior for β is thus proportional to the likelihood; it is a normal
distribution N(β, V ), where
β
=
β
V
=
σ2 
ixixT
i
−1
and β is the ordinary least squares estimator of β.
2
Unfortunately, Jeﬀreys’ prior tends to have suboptimal properties in higher
dimensions (Berger and Bernardo, 1992). Since Jeﬀreys’ work, there have
been considerable developments in this area. The ‘reference’ priors of Berger
and Bernardo (1992) overcome some of the ﬂaws in Jeﬀreys’ prior in multi-
dimensions.
In one-dimensional problems with a continuous parameter, where Jeﬀreys’
prior typically works well, the reference prior is the same as Jeﬀreys’ prior
(Bernardo, 2006). However, in multi-dimensional settings, the reference prior
is superior (in terms of frequentist properties). Unfortunately, it is sometimes
unclear how to ‘order’ the parameters to construct their joint distribution
(diﬀerent priors often result from diﬀerent ordering of the parameters) and
they can be diﬃcult to derive (Bernardo, 2006). A variety of other default
priors have been proposed in recent years, including the unit information
priors of Kass and Wasserman (1995), probability matching priors (Datta
and Ghosh, 1995; Mukerjee and Ghosh, 1997), and intrinsic priors (Berger
and Pericchi, 1996). For a good review of non-informative priors, see Kass
and Wasserman (1996) and Bernardo (2006).

PRIOR DISTRIBUTIONS
47
Improper priors
Many vague or noninformative priors are not proper density functions; that
is,

p(θ)dθ = ∞. In Jeﬀreys’ prior from Example 3.11,

dβ = ∞. When
using improper priors, there is no guarantee that the posterior distribution
will be proper in the sense that

L(θ | y) p(θ) dθ < ∞.
The data analyst needs either to show that the posterior is proper or replace
the improper priors with proper or ‘just proper’ priors.
To illustrate a ‘just proper’ prior, again consider the example of the prior on
the regression coeﬃcients in the normal linear regression model. Jeﬀreys’ prior
(and the reference prior) is p(β) ∝1; this can be replaced by p(β) = N(0, vI)
where v ≫0 and ﬁxed. This prior is proper but as v →∞, it approaches
the (improper) Jeﬀreys’ prior. It also provides intuition behind the conjugacy
of Jeﬀreys’ prior seen in Example 3.11. In general, for means and regression
coeﬃcients, just proper normal priors are a convenient choice when little prior
information is available. For WinBUGS, improper priors are not permitted,
so just proper alternatives must be used.
Default priors for variances
Priors for variance (and covariances) have increased importance for incomplete
longitudinal data. With complete data, variances and covariances are often
important primarily in terms of eﬃciency, but with incomplete data, they
have wider impact and when speciﬁed without care, can result in inconsistent
estimation of mean parameters (see Chapter 6).
Common default choices for variance parameters in a normal model include
p(σ2) ∝1/σ2 (Jeﬀreys’), p(σ2) ∝I{σ2 > 0} (ﬂat), and p(σ2) ∝1/(c + σ2)2
(uniform shrinkage), where c is ﬁxed (and corresponds to the prior median).
The uniform shrinkage prior is often used when the variance σ2 is at the 2nd
level of a two-level model (see, e.g., the random eﬀects variance); c can be
chosen as a prior guess at the value of the ﬁrst level variance. It is sensible
when there is prior belief that the variance component might actually be zero,
but little other prior information is available (Christensen and Morris, 1997;
Daniels, 1999).
Jeﬀreys’ prior tends to pull the variance toward zero and the ﬂat prior pulls
the variance away from zero. For a more detailed explanation and exploration
of all these priors, including their frequentist properties, and further discussion
of the choice of the constant c for the uniform shrinkage prior, see Daniels
(1999).
The ‘just proper’ prior most frequently used in the literature is a Gamma
prior (on 1/σ2) with parameters (103, 10−3). Gelman (2006) recommends

48
BAYESIAN INFERENCE
against using this prior and instead recommends bounded uniform or folded
non-central t (or normal) priors on σ (not σ2). All the proper priors for vari-
ances described above can be used in WinBUGS.
Priors for a covariance/correlation matrix
For the p–dimensional covariance matrix for a multivariate normal model
(Example 2.3), the two most common default choices are Jeﬀreys’ prior
p(Σ) ∝|Σ|−(p+1)/2
and the ‘just proper’ Wishart distribution on Σ−1, which has degrees of free-
dom ν equal to p. The latter choice avoids concerns about the propriety of
the posterior that arise when using improper priors (and can be used in Win-
BUGS), but requires providing a value for the scale matrix Σ0. This choice
is diﬃcult and, as a default, is often chosen to be a diagonal matrix with
marginal variances chosen to be roughly consistent with the variation in the
data or chosen based on be the maximum likelihood estimator of Σ. Unfor-
tunately, this prior can only be made noninformative to a limited extent; the
minimum value for ν is p, which can be thought of as a prior sample size. So,
if p is large and the sample size is small, this prior may be more informative
than desired (Daniels and Kass, 1999). Other default priors proposed in the
literature include the reference prior of Yang and Berger (1994) which, though
it has good frequentist properties, is improper and lacks the conditional con-
jugacy property of the Wishart.
For two-level models with a normal distribution on the random eﬀects (for
example, the models in Examples 2.1 and 2.2), common choices are the just
proper Wishart prior (again) and the ﬂat prior, p(Σ) ∝1 (Everson and Morris,
2000). Other priors proposed recently include a modiﬁcation to the reference
prior of Yang and Berger (Berger, Strawderman, and Tang, 2005) and the
approximate uniform shrinkage prior (Natarajan and Kass, 2000).
Advantages of Jeﬀreys’ and the ﬂat prior as default choices are that they
are both conditionally conjugate and do not require the choice of a scale
matrix, Σ0. However, they are both improper and not available in WinBUGS.
Priors for the parameters in a structured covariance matrix, e.g., a ﬁrst-
order autoregressive covariance structure, have been examinined in Monahan
(1983) and Ghosh and Heo (2003); specifying structure can be viewed as a very
strong prior restricting Σ to be within a certain class of covariance models
(Daniels, 2005); see Further Reading for more details.
Default priors for p–dimensional correlation matrices, which are needed for
multivariate probit models (Example 2.6), have been proposed by Barnard et
al. (2000), who consider either a joint prior that induces marginal uniform
priors on the individual correlations in the correlation matrix or a uniform
prior on the space of correlation matrices. Neither is (conditionally) conjugate,
but both are proper. The latter is a uniform prior on the compact subspace

PRIOR DISTRIBUTIONS
49
of the p(p −1)/2 dimensional cube [−1, 1]p(p−1)/2 such that the correlation
matrix is positive deﬁnite.
Recommendations for using default priors in longitudinal models
For regression coeﬃcients β and unconstrained means µ, ﬂat improper priors
are a good choice if the posterior can be conﬁrmed to be proper. Otherwise,
the just proper normal priors should be used. In addition, if the WinBUGS
software is being used, the just proper normal priors would be the only choice.
For variances, following Gelman’s recommendations (Gelman, 2006), we
suggest (truncated) uniform priors or folded normal priors on σ. For the 2nd-
level variances (e.g., the variance of the random eﬀects), uniform shrinkage
priors can be considered (Daniels, 1999).
For covariance matrices, either the ﬂat prior or the reference priors of Berger
and colleagues would be recommended; unfortunately, both are improper and
cannot be used in WinBUGS. As a result, the just proper Wishart is the most
common choice. For covariance matrices with structure, it is hard to make a
general recommendation as it will vary with how the structure is imposed. For
a speciﬁc family of structured covariance matrices and recommended priors,
see Section 6.4.1.
Most of the analyses in later chapters (Chapters 4, 7, 10) are done in
WinBUGS, which will necessarily restrict the choice of priors.
3.3.3 Informative priors
Informative priors can be used in situations where prior information exists,
whether in the form of historical information, expert opinion, or pilot data.
With complete data, using strong priors has the eﬀect of combining informa-
tion from the prior, together with information from the data and model, to
form the posterior. This can be illustrated very clearly in the normal linear
regression model (Example 3.6). The posterior mean of β,
E(β | y) =

ixixT
i /σ2 + V −1
β
−1 
V −1
β β +

ixixT
i /σ2
β0

,
is a weighted average of the ordinary least squares estimator β (formed from
the data and the model) and the prior mean β0, with weights depending on
the data and the prior variance V β. The strength of the prior is (partially)
determined by the magnitude of V β. The informativeness of this prior can be
altered by appropriately modifying V β.
With incomplete data, the role of the prior takes on considerably added
importance. When data are missing, information about some components of θ
often derives only from the prior. This places a premium on methods for
obtaining and ‘modeling’ prior information.
Constructing informative priors that formalize expert opinion is diﬃcult.

50
BAYESIAN INFERENCE
A common approach is to specify a particular family of distributions (often
conditionally conjugate) and then to elicit information from the investigator
to ﬁll in values for the hyperparameters. A key component is determining an
easy to comprehend scale to ‘accurately’ elicit information from an expert
on the hyperparameters (or functions of them) (Chaloner, 1996). As an ex-
ample, in linear regression models, Kadane et al. (1980), and later Laud and
Ibrahim (1996), proposed constructing priors on regression coeﬃcients based
on elicited information about future data (Y ’s), and then deriving the in-
duced hyperparameters on the regression parameters of interest; they viewed
data predictions as a more understandable metric for the experts than the
regression coeﬃcients themselves.
The power priors of Ibrahim and Chen (2000) are a class of informative
priors that provide a way to introduce prior information based on historical
data. A prior is speciﬁed that is proportional to the likelihood of the historical
data with a subjective choice of the power on this likelihood, i.e., how much
prior ‘weight’ should be placed on the historical data.
We revisit informative priors in the context of sensitivity analysis in Chap-
ter 9 and illustrate an elicited prior in Section 10.3.
3.3.4 Identiﬁability and incomplete data
In incomplete data settings, some components of θ are well-identiﬁed by the
data and others are not.
Deﬁnition 3.5. Nonidentiﬁed parameter.
A parameter θ1 is called nonidentiﬁed when
p(θ1 | y) ≡p(θ1)
for any choice of prior p(θ1).
2
Note that for the component θ1 of θ to be nonidentiﬁed (as given by the above
deﬁnition), the prior for θ typically must be able to be factored as
p(θ) = p(θ1)p(θ−1),
where θ−1 is θ with θ1 removed. For a weakly identiﬁed parameter and for
dependent priors, this equivalence will hold approximately; for a very careful
discussion of identiﬁability, see Dawid (1979). We give a simple example next.
Example 3.12. Nonidentiﬁability in a mixture of bivariate normals.
Consider the following normal mixture model,
(Y1, Y2)T | R = 1
∼
N(µ(1), Σ)
(Y1, Y2)T | R = 0
∼
N(µ(0), Σ)
R | φ
∼
Ber(φ)
where in the ﬁrst component of the mixture (R = 1), we observe (Y1, Y2)T; and

COMPUTATION OF THE POSTERIOR DISTRIBUTION
51
for the second component (R = 0), we only observe Y1. Suppose we specify
independent priors on µ(r), Σ, and φ. Let Σ = {σjk}, and for r = 0, 1, let
µ(r) = (µ(r)
1 , µ(r)
2 )T. Then one can show
p(µ(0)
2
| y, r)
∝
p(µ(1)
1 ) p(µ(1)
2 ) p(µ(0)
1 ) p(µ(0)
2 ) p(Σ)
×
n

i=1
p(yi1, yi2 | µ(1)
1 , µ(1)
2 , Σ) p(yi1 | µ(0)
1 , σ11)
∝
σ−n0/2
11
exp


−
	
{i:ri=0}
(yi1 −µ(0)
1 )2/2σ11


p(µ(0)
2 )
∝
p(µ(0)
2 ),
where n0 is the number of subjects with Yi2 missing (Ri = 0). Since the
likelihood does not contain µ(0)
2 , the posterior for µ(0)
2
is proportional to its
prior.
2
In Chapters 5 and 8, we discuss various restrictions (implicitly, priors) that
can be are used to identify µ(0)
2
in mixture models of this type.
In missing data problems, the Bayesian approach allows assumptions about
nonidentiﬁed parameters, and our uncertainty about those assumptions, to be
formalized through prior distributions. By contrast, in frequentist inference,
nonidentiﬁed parameters are typically ﬁxed at some value or constrained via
untestable assumptions. We discuss this issue in more detail in Chapters 6, 8,
and 9.
3.4 Computation of the posterior distribution
The 1990 paper by Gelfand and Smith (1990) marked the start of the revo-
lution in Bayesian computations within the statistical community and pop-
ularized an approach to obtain exact (up to Monte Carlo error) inferences
for complex Bayesian hierarchical models. The late 1990s saw the advent of
software to do these computations, including WinBUGS. The seminal idea
underlying all these approaches was to obtain a sample from the posterior
distribution without the need for explicit evaluation of normalizing constant
of the posterior by constructing a Markov chain having the posterior distri-
bution of interest as its stationary distribution. Marginal posteriors and the
posterior of functions of the parameters are easily obtained by doing Monte
Carlo integration using the sample from the Markov chain. The approaches
are often referred to as Markov chain Monte Carlo (MCMC) algorithms.

52
BAYESIAN INFERENCE
3.4.1 The Gibbs sampler
The Gibbs sampler (Geman and Geman, 1984) is the most common MCMC
approach used in statistics. It is an algorithm that involves sampling a Markov
chain whose kernel is the product of the sequentially updated full conditional
distributions of the parameters and whose stationary distribution is the pos-
terior.
To be more explicit, consider a q-dimensional parameter vector θ with
elements (θ1, θ2, . . . , θq) and let θ(k)
j
correspond to the sample of the jth
component of θ at iteration k. We sample from the following distributions
sequentially,
1.
θ(k)
1
∼p(θ(k)
1
| θ(k−1)
2
, θ(k−1)
3
, . . . , θ(k−1)
q
, y)
2.
θ(k)
2
∼p(θ(k)
2
| θ(k)
1 , θ(k−1)
3
, . . . , θ(k−1)
q
, y)
...
q.
θ(k)
q
∼p(θ(k)
q
| θ(k)
1 , θ(k)
2 , . . . , θ(k)
q−1, y).
A common extension is block sampling (Roberts and Sahu, 1997), whereby
subsets of θ are sampled together. For example, we might sample from the
joint distribution p(θ(k)
1 , . . . , θ(k)
l
| θ(k−1)
l+1
, . . . , θ(k−1)
q
, y), instead of the com-
ponentwise distributions as above. We provide an example next to illustrate
block sampling.
Example 3.13. Block Gibbs sampling for normal random eﬀects model (con-
tinuation of Example 3.9).
Recall the normal random eﬀects model
Y i | xi, bi ∼N(µb
i, σ2I),
where
µb
i = xiβ + wibi
and
bi | xi ∼N(0, Ω).
Assume conditionally conjugate prior distributions for the parameters
β
∼
N(β0, V β)
1/σ2
∼
Gamma(a, b)
Ω−1
∼
Wishart(p, Ω0),
where p = dim(Ω). Here we sample the regression coeﬃcients β, the ran-
dom eﬀects bi, and the components of Ωin blocks. The corresponding full

COMPUTATION OF THE POSTERIOR DISTRIBUTION
53
conditional distributions are
β | b, σ2, Ω, y, x
∼
N

A−1 
ixT
i (yi −wibi)/σ2 + V −1
β β0

, A−1
bi | β, σ2, Ω, y, x
∼
N

B−1 
iwT
i (yi −xiβ)/σ2 + Ω−1
, B−1
σ−2 | β, b, Ω, y, x
∼
Gamma

n/2 + a,

1/b + 
ieT
i ei/2
−1
Ω−1 | β, b, σ2, y, x
∼
Wishart

n + p,

Ω−1
0
+ 
ibibT
i
−1 
,
where ei = ei(β, bi) = yi −xiβ −wibi and
A
=
n
	
i=1
xixT
i /σ2 + V −1
β
B
=
n
	
i=1
wiwT
i /σ2 + Ω−1.
Each is available in closed form, demonstrating the computational advantage
of using conditionally conjugate priors and the ease with which a posterior
sample can be drawn.
2
Having all the full conditional distributions available in closed form is atypical.
We go through a more representative example next.
Example 3.14. Gibbs sampling for logistic random eﬀects model (continua-
tion of Example 2.2).
We specify priors for β and Ω−1 as in Example 3.13. Recall the model is
logit(µb
ij) = xijβ + wijbi,
where bi ∼N(0, Ω). The full conditional distributions here are
β | b, Σ, Ω, y, x
∝
p(β)
n

i=1
J

j=1
(µb
ij)yij(1 −µb
ij)1−yij
bi | β, Σ, Ω, y, x
∝
exp

−1
2bT
i Ω−1bi

J

j=1
(µb
ij)yij(1 −µb
ij)1−yij.
The full conditionals of β and bi are not available in closed form. However,
the full conditional distribution for Ω−1 is a Wishart as in Example 3.13; i.e.,
Ω−1 | β, b, y, x
∼
Wishart(n + p, {Ω−1
0
+ 
ibibT
i }−1).
2
As stated earlier, the inability to obtain all the full conditional distributions
in closed form is not an uncommon occurrence in most models, in some cases
due to the use of non-conjugate priors. For example, the binary data models
in Examples 2.4 and 2.5 have at least one full conditional with an unknown

54
BAYESIAN INFERENCE
form. Clearly, approaches are needed to sample from the full conditional dis-
tributions for such cases, and there are many techniques available to do this.
We review one such approach in the next section.
3.4.2 The Metropolis-Hastings algorithm
The most commonly used approach for sampling from non-standard full con-
ditional distributions is the Metropolis-Hastings algorithm (Hastings, 1970).
We describe its implementation within a Gibbs sampling algorithm; this in-
volves constructing another Markov chain (within the Gibbs sampler) that
has as its stationary distribution the full conditional distribution of interest.
For simplicity, denote the full conditional distribution of interest as
p(θ(k)
j
| {θ(k)
l
: l < j}, {θ(k−1)
l
: l > j}, y).
In the following, for clarity, we will remove the conditioning and write this
full conditional as p(θ(k)
j
).
At the kth iteration, θ(k)
j
is sampled from some candidate distribution,
q(θ(k)
j
| θ(k−1)
j
), and the sampled value is accepted with probability α⋆given by
α⋆= min

1,
p(θ(k)
j
)q(θ(k−1)
j
| θ(k)
j
)
p(θ(k−1)
j
)q(θ(k)
j
| θ(k−1)
j
)
!
.
This choice of acceptance probability ensures the stationary distribution of
the Markov chain is the full conditional distribution of interest.
An important practical issue is the choice of candidate distribution q. Set-
ting q(θ(k)
j
| θ(k−1)
j
) = N(θ(k−1)
j
, v), where v is a ﬁxed variance, yields the
random walk Metropolis-Hastings algorithm. For this choice, q(θ(k)
j
| θ(k−1)
j
) =
q(θ(k−1)
j
| θ(k)
j
), and α⋆reduces to the ratio of the full conditionals evaluated
at the proposal value and the previous value; i.e.,
α⋆= min

1,
p(θ(k)
j
)
p(θ(k−1)
j
)
!
.
(3.7)
Optimal acceptance for this choice should be around 20 −30% (Brooks and
Gelman, 1998). Higher acceptance rates will result in θj moving very slowly
around the posterior distribution and a highly correlated sample of θj, creating
an ineﬃcient algorithm; these issues are discussed in detail in Section 3.4.4.
A common initial choice for v is some value proportional to the squared
standard error (based on the information matrix), usually with proportionality
constant less than one; this initial value can be adjusted by trial and error to
attain an appropriate acceptance rate.
Another common choice for q is an approximation to the full conditional

COMPUTATION OF THE POSTERIOR DISTRIBUTION
55
distribution p(·), independent of the previous value, θ(k−1)
j
. For example, one
might construct a normal approximation (Laplace approximation) to the full
conditional distribution based on the modiﬁed score vector and information
matrix as discussion in Section 3.2; this usually involves implementing an
optimization algorithm (e.g., Newton-Raphson) at each iteration. In this case,
(3.7) can be rewritten as the ratio of importance weights,
α⋆= min

1,
p(θ(k)
j
)/q(θ(k)
j
)
p(θ(k−1)
j
)/q(θ(k−1)
j
)
!
,
where q(·) is the approximation to the full conditional distribution. For this
approach, the optimal acceptance rate is as close to 100% as possible. Poor
choices of q will result in low acceptance often due to long runs of not ac-
cepting the candidate value. Heavier tailed candidates will often improve the
eﬃciency (Chib and Greenberg, 1998; Daniels and Gatsonis, 1999). Choosing
the candidate distribution based on approximating the full conditional distri-
bution has been implemented with considerable success for sampling the ﬁxed
eﬀects and random eﬀects, β and bi respectively, in generalized linear mixed
models (Example 2.2). For some examples, see Daniels and Gatsonis (1999).
Recommendations
The random walk Metropolis-Hastings algorithm typically works best for sam-
pling one parameter at a time and is computationally inexpensive (no need to
do a maximization at each iteration). However, it does not usually generalize
well to sampling blocks because of the diﬃculty in specifying an appropri-
ate covariance matrix for the candidate distribution. In addition, sampling
the parameter vector θ componentwise can result in poor performance of the
MCMC algorithm (more details in Section 3.4.4).
Using an approximation to the full conditional distribution for the can-
didate distribution in the Metropolis-Hastings algorithm often works well
when it is not computationally expensive to do a maximization at each it-
eration (e.g., compute derivatives and do Newton-Raphson steps), and is a
good approach when trying to sample more than one parameter simultane-
ously. Heavy-tailed approximations, e.g., a t-distribution instead of a normal
distribution, will often increase eﬃciency (Chib and Greenberg, 1998). Other
approaches to sample non-standard full conditionals are mentioned in Further
Reading.
3.4.3 Data augmentation
As discussed previously, it is often the case that some of the full conditional
distributions of p(θ | y) are diﬃcult to sample. An alternative to Metropolis-
Hastings is to introduce latent data Z such that it is easy to sample p(θ |

56
BAYESIAN INFERENCE
z, y) and p(z | θ, y) and often improves the mixing properties of the sampler
(Tanner and Wong, 1987; van dyk and Meng, 2001). This was coined data
augmentation by Tanner and Wong. The approach proceeds in two steps:
1. Sample p(z | θ, y),
2. Sample p(θ | z, y) using Gibbs sampling.
This approach is well-suited to two (related) situations. First, complete data
problems that inherently have latent structure based on known distributions,
e.g., probit or random eﬀects models; we illustrate this in Examples 3.15 and
3.16. Second, data augmentation is also a natural way to deal with incomplete
data. For incomplete data problems, we often specify the model for the full
data. As such, it is often easier to work with the full data posterior as opposed
to the posterior based only on the observed data. In this case, Z would corre-
spond to the missing data. Sampling p(θ | z, y) corresponds to sampling from
the full data posterior; p(z | θ, y) to sampling from the posterior predictive
distribution of the missing data. Chapters 6 and 8 provide details on data
augmentation in this setting.
In the following, we provide details on data augmentation for some of the
longitudinal models introduced in Chapter 2. Both correspond to complete
data problems with latent structure, where data augmentation can make sam-
pling from the posterior considerably easier.
Example 3.15. Data augmentation for the probit model (continuation of Ex-
ample 2.6).
This is the multivariate probit formulation but with J = 1. The posterior for
this model is
p(β | y) ∝

 n

i=1
Φ(xiβ)yiΦ(−xiβ)1−yi
!
p(β),
(3.8)
where Φ(·) is the standard normal cdf and p(β) is typically chosen to be a
(multivariate) normal prior. Clearly, the posterior distribution of β is not
available in closed form. However, we can use data augmentation here to
simplify sampling from the posterior. We exploit the existence of the latent
variables Zi such that
L(β | y) ∝

zi∈Q(yi)
p(zi | β)dzi,
where Q(yi) = (0, ∞) if yi = 1 and Q(yi) = (−∞, 0) if yi = 0, and p(zi | β)
is the pdf of a normal distribution with mean xiβ and variance 1. With the
introduction of the latent zi, sampling from the posterior distribution of β
can proceed (quite easily) in two steps:
1. Sample β from p(β | z, y), a normal distribution (assuming the prior
p(β) is a normal distribution).
2. Sample z from p(z | β, y), a truncated normal distribution.

COMPUTATION OF THE POSTERIOR DISTRIBUTION
57
Extensions to the multivariate model are conceptually straightforward in that
they use the underlying latent structure as well.
2
In some cases, t-distributions are used instead of normal distributions to
obtain more robust inferences; the heavier tails of the t-distribution make it
more robust to outliers (Lange, Little, and Taylor, 1989). In addition, they can
be used to build multivariate logistic models for longitudinal binary data (Ex-
ample 6.6.2). Unfortunately, regardless of the prior speciﬁcation, the full con-
ditional distributions of the location and scale parameters of the t-distribution
are not available in closed form. However, we can use data augmentation to
facilitate sampling from the location and scale parameters.
Example 3.16. Data augmentation for the multivariate t-distribution (con-
tinuation of Example 2.4).
To implement data augmentation for the t-distribution, we take advantage
of the following relationship between the multivariate t-distribution with ν
degrees of freedom and the multivariate normal distribution. If Y i | µ, Σ ∼
Tν(µ, Σ), then its density can be written as a gamma mixture of normals,

τ J/2
i
(2π)J/2|Σ|1/2 exp

−1
2(yi −µ)T(Σ/τi)−1(yi −µ)

p(τi | ν)dτi,
(3.9)
where p(τi | ν) is a Gamma density with parameters (ν/2, 2/ν). Using this re-
sult, data augmentation approaches can be used to greatly simplify sampling.
This result was ﬁrst used in the context of Gibbs sampling by Chib and Albert
(1993). By using the expanded parameter space with the latent variables τi
instead of integrating them out as in (3.9), we can sample from the posterior
of (µ, Σ) using the following steps, assuming a multivariate normal prior on
µ and a Wishart prior on Σ−1:
1. Sample τi from p(τi|µ, Σ, y), independent gamma distributions.
2. Sample (µ, Σ−1) from p(µ, Σ | τ, y) using Gibbs sampling:
(a) Sample µ from p(µ|Σ, τ, y), a multivariate normal distribution.
(b) Sample Σ−1 from p(Σ−1|µ, τ, y), a Wishart distribution.
2
As a ﬁnal example, we show that the Gibbs sampler for the normal random
eﬀects model in Example 3.13 implicitly uses data augmentation.
Example 3.17. Data augmentation for the normal random eﬀects model (con-
tinuation of Example 3.13).
In the normal random eﬀects model, the random eﬀects bi can be integrated
out in closed form. Thus, the posterior distribution can be sampled by just
using the full conditional distributions of β, Ω, and σ2 based on the inte-
grated likelihood (3.2) and priors. However, the full conditional distributions
of Ωand σ2 will not have known forms and in particular, the full conditional
for Ωwill be very hard to sample (Daniels, 1998). On the other hand, if the

58
BAYESIAN INFERENCE
random eﬀects are not integrated out and are sampled from their full con-
ditional distributions as well, the full conditional distribution of Ω−1 will be
a Wishart distribution (cf. Example 3.13) and 1/σ2 will be a Gamma distri-
bution. Hence, from a computational perspective, we can view the random
eﬀects bi as latent variables augmenting the model
Y i ∼N(xiβ, wiΩwT
i + σ2I)
and the Gibbs sampler described in Example 3.13 can be viewed as a Gibbs
sampler with data augmentation.
2
Now that we have reviewed the tools to obtain a sample from the posterior,
we are ready to discuss how to use this sample for posterior inference.
3.4.4 Inference using the posterior sample
The sample generated by Gibbs sampling or other MCMC algorithms must be
used carefully. Although WinBUGS will typically do the sampling described
in Sections 3.4.1–3.4.3 automatically, we need to be sure we have obtained
an accurate and correct sample using these approaches. As such, at least two
issues need to be considered:
1. When do we consider the Markov chain to have reached the stationary
distribution, i.e., the posterior distribution? (The time before reaching
this is often referred to as the burn-in.)
2. How many additional samples are needed after the burn-in in order to
make ‘accurate’ inferences? This can be tricky since MCMC approaches
generate a dependent sample from the posterior distribution of the pa-
rameters.
Related to issue (1), it is often advocated to discard the ﬁrst K samples as
burn-in, giving the sampler time to ﬁnd the stationary distribution, i.e., the
posterior. To make sure the chain has actually converged to the correct place,
it is recommended to run multiple chains with diﬀerent starting values and
make sure they all converge to the same region of the parameter space (Gel-
man and Rubin, 1992); Gelman and Rubin proposed a statistic to monitor
convergence based on the variability within and between the multiple chains.
We refer the reader to Cowles and Carlin (1996) for more details and other
approaches to assess convergence. Figure 3.1 illustrates the concept; the sam-
pler has reached the stationary distribution rather quickly after two or three
iterations (i.e., when the four chains come together).
Once the sampler has converged, the issue then becomes how eﬃciently
the sampler takes draws from the posterior. In most practical settings, the
sequential MCMC draws are autocorrelated (dependent). Figure 3.2 is an au-
tocorrelation plot showing lag-k correlations in a chain. It appears by lag 10
that the autocorrelation is negligible. If the chain moves slowly around the pa-
rameter space (high autocorrelation), more draws are needed for inference. For

COMPUTATION OF THE POSTERIOR DISTRIBUTION
59
0
10
20
30
40
50
−4
−2
0
2
4
iteration
beta
Figure 3.1 Plot of multiple chains illustrating the concept of burn-in.
example, the information in 10, 000 autocorrelated draws might be equivalent
to only 500 independent draws; see Tierney (1994) for details.
Figure 3.3 shows one sampler that is mixing well and another that is mixing
poorly. In the bottom plot, it is clear that the chain is moving slowly around
the parameter space. Block Gibbs sampling or non-random walk Metropolis-
Hastings approaches are often used to speed mixing by reducing autocorrela-
tion.
Computation of functions of the posterior such as means, medians, and
percentiles (credible intervals) can be done by just calculating the appropri-
ate summary using the MCMC sample after throwing away the ‘burn-in’.
However, to compute other functions, such as the posterior standard devia-
tion, care must be taken to account for the autocorrelation in the chain. One
approach is ‘thinning’, whereby every kth MCMC draw is retained, with k
chosen large enough so the lag-k correlation is small. Thinning yields an ap-
proximately independent sample. For example, based on Figure 3.2, we might
choose k = 10.
Thinning can be ineﬃcient because (k −1)/k% of the sample is discarded.
Batching is usually a more eﬃcient approach. It involves breaking the output

60
BAYESIAN INFERENCE
0
10
20
30
40
50
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
Figure 3.2 Autocorrelation plot. Lag-k correlation is plotted vs. k.
of the chain into m batches of length m⋆, such that m⋆is large enough for
the correlation between batch means to be negligible. To calculate the pos-
terior variance, we use a suitably normalized corrected sum of squares of the
batch means (see pp. 194–195 in Carlin and Louis, 2000). Other approaches,
including some based on time series methodology, can be found in Chapter 5
of Carlin and Louis (2000).
Marginal posterior distributions
The MCMC sample obtained provides a (dependent) sample from the joint
posterior distribution of interest. However, we are often interested in marginal
posterior distributions; for example, in the multivariate normal model in Ex-
ample 2.3, we may be interested in a speciﬁc regression coeﬃcient, say βj. A
sample from the marginal posterior distribution of βj (or, in general, any func-
tion of the parameters) is obtained by using only the sampled values of that
parameter. To obtain the marginal posterior distribution for a function of the
parameters, we can evaluate that function at each iteration, given the current
values of the parameters, to obtain a sample from the marginal posterior of
that function. See Section 4.4 for an illustration.

COMPUTATION OF THE POSTERIOR DISTRIBUTION
61
0
500
1000
1500
2000
2500
3000
−10
0
10
20
30
40
50
iteration
beta
0
500
1000
1500
2000
2500
3000
25
30
35
40
iteration
beta
Figure 3.3 Plot illustrating good mixing (top) and poor mixing (bottom).
Reweighting
It will sometimes be the case that we have a sample from a distribution
(e.g., an MCMC sample from the posterior) and we would like to use this
sample to make inference based on a similar posterior (maybe with a slightly
diﬀerent likelihood and/or priors). We can avoid re-running a Gibbs sampler
on this new model by appropriately reweighting the sample already obtained.
In general, suppose we have a sample from some distribution, p(θ) and we
want to make inference based on some diﬀerent distribution, p⋆(θ). Then, we

62
BAYESIAN INFERENCE
can reweight the sample using weights of the form
w = p(θ)
p⋆(θ).
The reliability of this approach depends on the weights, w being stable (Pe-
ruggia, 1997). Reweighting will be useful for computing intractable likelihoods
(e.g., the multivariate probit model in Example 2.6) that are sometimes needed
for model selection criterion (see Section 3.5) and for computing several model
selection criteria in the presence of incomplete data (see Chapters 6 and 8).
A note on improper priors and Gibbs sampling
We end this section on posterior sampling with a cautionary remark on using
improper priors in Gibbs sampling (Hobert and Casella, 1996), illustrated
by an example. Consider the normal random eﬀects model in Example 2.1.
Suppose we specify the following (improper) prior on Ω,
p(Ω)
∝
|Ω|−(p+1)/2,
where p = dim(Ω). Clearly,

p(Ω) dΩ= ∞.
The full conditional distribution of Ω−1 will be a proper Wishart distri-
bution. However, the posterior distribution of Ω−1 will be improper. This
phenomenon (when using improper priors) of all the full conditionals being
proper distributions, but the posterior being improper, was ﬁrst noticed in
Hobert and Casella (1996). An even more problematic aspect from a practical
perspective is that the sample from the improper posterior may not indicate
any problems! So, when using improper priors, the propriety of the poste-
rior distribution needs to be veriﬁed analytically. Otherwise, improper priors
should not be used. If WinBUGS is being used, this is not a concern as it does
not allow improper priors; however, for investigators writing their own code
and using improper priors, this is an important issue.
3.5 Model comparisons and assessing model ﬁt
When we ﬁt a parametric model to a dataset, we should examine how well
the model ﬁts the observed data. A related issue is how to select among sev-
eral plausible models (model selection), which tells us only about the ﬁt of
models relative to the others under consideration. We address model selec-
tion ﬁrst. Two common criteria are the deviance information criterion (DIC)
(Spiegelhalter et al., 2002) and posterior predictive loss (PPL) (Gelfand and
Ghosh, 1998). Both take into account goodness of ﬁt while penalizing models
for overﬁtting (a complexity penalty).

MODEL FIT
63
3.5.1 Deviance Information Criterion (DIC)
The DIC is a model-based criterion composed of a goodness of ﬁt term and
a penalty term. The ﬁt is measured by the deviance, a linear function of the
log likelihood, given by
Dev(θ) = −2 log L(θ | y).
Larger values of the deviance indicate poorer ﬁt.
The penalty term measures model complexity and is given by
pD = E{Dev(θ) | y} −Dev{E(θ | y)}.
(3.10)
The variable pD is called the eﬀective number of parameters. As the variability
in the posterior of θ decreases, pD →0. How overﬁtting is penalized can best
be understood by introducing the concept of the residual information in data
y conditional on parameters θ, deﬁned as −2 log{p(y | θ)} (Kullback and
Liebler, 1951; Burham and Anderson, 1998). Recall that L(θ | y) ∝p(y | θ).
Deﬁne θ to be an estimator of θ and θ⋆to be the true parameter value. The
diﬀerence between the residual information at the true parameter value and
at the estimated parameter value is
−2 log p(y | θ⋆) + 2 log p(y | θ).
(3.11)
This can be interpreted as the degree of overﬁtting due to the inﬂuence of y
on the estimator θ. In a Bayesian analysis, θ is random and we can replace
(3.11) with its posterior expectation, the eﬀective number of parameters pD
given in (3.10).
The DIC itself is deﬁned as
DIC = Dev{E(θ | y)} + 2pD.
(3.12)
The ﬁrst term measures goodness of ﬁt and the second term is the complexity
penalty. The form is very similar to the Akaike Information Criterion (AIC)
(Akaike, 1973). Equivalently, the DIC can be written explicitly as a function
of the log likelihood,
DIC = −4E{log L(θ | y) | y} + 2 log L{E(θ | y) | y},
(3.13)
which will be a more convenient form for its development in the setting of
incomplete data and for describing its computation in Chapters 6 and 8.
The DIC is easy to compute from a posterior sample; it requires calculat-
ing two quantities, E{Dev(θ) | y} and Dev{E(θ | y)}, using the output from
MCMC approaches; WinBUGS will often calculate it automatically. Ease of
implementation has contributed to its widespread use. In Section 4.2, we use
the DIC to compare several multivariate normal models for the Growth Hor-
mone data (described in Section 1.3)
An advantage of the DIC over approaches like AIC, where the user speciﬁes
the number of parameters, is that the (eﬀective) number of parameters is

64
BAYESIAN INFERENCE
counted automatically (see (3.10)). This is particularly helpful in multilevel
models where the number of parameters is sometimes diﬃcult to quantify. As
an example, consider the normal random eﬀects models in Example 2.1 with
likelihood given by
L(θ, bi | y) ∝
n

i=1
|Σ|−1/2 exp

−1
2ei(β, bi)TΣ−1ei(β, bi)

,
(3.14)
where ei(β, bi) = yi −xiβ −wibi and θ = (β, Σ). The random eﬀects have
not been integrated out and are now treated as parameters along with θ. On
the surface, if we count the number of random eﬀects (assume for simplicity
they are one-dimensional), there are n. However, the eﬀective number can be
quite smaller because the random eﬀects distribution p(bi | θ) shrinks the
random eﬀects to zero. As the variance of the random eﬀects distribution
goes to zero, there are fewer parameters; in fact, if the variance is zero, all the
random eﬀects are identically zero so there are in fact no parameters. On the
other hand, as the variance increases, the number of parameters approaches n.
Despite its computational simplicity, the DIC does have drawbacks. The
best model as determined by the DIC can change depending on the choice of
‘likelihood’ (see Trevisani and Gelfand, 2003); for example, again revisiting
the normal random eﬀects model (Example 2.1), the likelihood can take one
of two forms: the integrated likelihood given in (3.2), or the likelihood without
the random eﬀects integrated out, given in (3.14).
In addition, the DIC is not invariant to the parameterization of θ. This oc-
curs because the ﬁt term Dev{E(θ | y)} in (3.12) involves a plug-in estimator
for θ based on the posterior, E(θ | y); and in general, E{h(θ) | y} ̸= h{E(θ |
y)}. For the multivariate normal model in Example 2.3, θ could be deﬁned
as (β, Σ−1) or (β, Σ). Using Σ vs. Σ−1 will result in diﬀerent values for the
DIC; see Section 4.2 for an illustration on the Growth Hormone data. For
covariance matrices, Spiegelhalter et al. (2002) recommend using the inverse
because its posterior mean is more stable.
Another limitation, common to all likelihood based criteria, is that for some
models, the likelihood is not available in closed form (e.g., the multivariate
probit model in Example 2.6). For many models, to evaluate the likelihood,
it is possible to use Monte Carlo integration and reweighting. For example, in
the multivariate probit model, we need to compute
Ez


J

j=1
I{zij > 0}yijI{zij < 0}1−yij


given (β, Σ), where zi follows a multivariate normal distribution with mean
xiβ and covariance matrix Σ. We can sample from the distribution of zi and
compute the expectation by averaging the term in brackets over the samples.

MODEL FIT
65
However, it would be computationally prohibitive to do this for every sampled
value of θ = (β, Σ) that is needed to compute E{Dev(θ)} in the DIC.
A more practical approach is reweighting (as discussed in Section 3.4.4). To
implement it here, we can take a likely value, say θ⋆= E(θ | y), and sample
L values, z(l) : l = 1, . . . , L, from Zi ∼N(xiβ⋆, Σ⋆), where θ⋆= (β⋆, Σ⋆).
Then, to compute the likelihood for other values of θ, we evaluate

l h(z(l))wl

l wl
,
where h(z) = &
j I(zij > 0)yijI(zij < 0)1−yij. The weights are given by
wl = p(z(l) | θ)
p(z(l) | θ⋆),
where p(· | θ) is a multivariate normal distribution with parameters θ =
(β, Σ) (Liu and Daniels, 2007). We illustrate this in Section 7.5. For further
recommendations and discussion on the choice of likelihood and parameteri-
zation, we refer the reader to Spiegelhalter et al. (2002).
3.5.2 Posterior predictive loss
Posterior predictive loss (PPL) (Gelfand and Ghosh, 1998) is another model
selection criterion. Before providing details, we ﬁrst need to deﬁne the poste-
rior predictive distribution.
Deﬁnition 3.6. Posterior predictive distribution.
The posterior predictive distribution is
p(yrep | y) =

p(yrep | θ, y)p(θ | y)dθ,
(3.15)
where p(yrep | θ, y) = p(yrep | θ). Samples from the posterior predictive
distribution are replicates of the observed data generated by the model.
2
PPL quantiﬁes the ﬁt of the model by comparing features of the (model-
based) posterior predictive distribution to equivalent features of the observed
data. The comparison is based on a user-chosen loss function. L (yrep, a; y),
where a is chosen to minimize the expectation of the loss with respect to
the posterior predictive distribution E{L (yrep, a; y) | y}, i.e., the posterior
predictive loss. For some choices of L , the minimization has a closed form.
Gelfand and Ghosh consider loss functions of the form
Lk(yrep, a; y) = L (yrep, a) + kL (y, a), k ≥0.
(3.16)

66
BAYESIAN INFERENCE
For univariate y, if L is chosen as squared error loss, it can be shown that
min
a E

Lk(yrep, a; y) | y

=
n
	
i=1
σ2
i +
k
k + 1
n
	
i=1
(µi −yi)2
=
P +
k
k + 1G,
(3.17)
where
µi = E(Yi,rep | y) =

yi,rep p(yi,rep | θ) p(θ | y) dθ dyi,rep
is the posterior predictive mean and σ2
i = var(Yi,rep | y) is the posterior
predictive variance.
The ﬁrst term in (3.17), P = n
i=1 σ2
i , is a penalty term. Overﬁtting the
model will result in large predictive variances σ2
i and a large value for P. The
second term, G = n
i=1(µi−yi)2, is a goodness of ﬁt term, which will decrease
with model complexity. This statistic is easy to compute using samples from
the posterior predictive distribution.
For other (smooth) choices of L (·), the criterion can also be approximated
in a similar form with a goodness of ﬁt and a complexity term. Like the DIC,
this criterion contains an ‘automatic’ penalty P. The choice of k determines
how much weight is placed on the goodness of ﬁt term relative to the penalty
term. As k →∞, k/(k + 1) →1. Unlike the DIC, which uses a non-invariant
plug-in estimator for θ, PPL is based on the posterior predictive distribution
and is invariant to the model parameterization.
The downsides of PPL are that it requires the choice of an appropriate loss
function (which we do not specify in the course of most Bayesian analyses)
and possibly nontrivial analytical calculations to obtain the criterion. Another
issue is applying it to multivariate observations (e.g., longitudinal data), where
we have to account for correlation when computing both the penalty and the
ﬁt terms. However, approaches such as using the log likelihood loss can account
for correlation (Gelfand and Ghosh, 1998). Finally, extensions of this criterion
to incomplete data are an area that needs further study.
A simple way to extend this approach to longitudinal (correlated) data,
without using a loss function based on the likelihood, is to summarize each
multivariate observation with a univariate measure Ti = h(Y i), which is a
function of the response vector for subject i, and then apply univariate meth-
ods (Hogan and Wang, 2001). The univariate summary Ti might be speciﬁed
as a weighted average of the longitudinal responses, Ti =  wjYij for some
ﬁxed set of weights. To emphasize ﬁt based on the last observation time, we
can set
wl =

0
l = 1, . . . , J −1
1
l = J.

MODEL FIT
67
When emphasis is on a change from baseline, set
wl =





0
l = 2, . . . , J −1
1
l = 1
−1
l = J.
Given the choice of Ti and using the Gelfand and Ghosh loss function (3.16)
with squared error loss, the PPL criterion becomes
PPL
=
n
	
i=1
σ2
i(T ) +
k
k + 1
n
	
i=1
(µi(T ) −Ti)2
(3.18)
=
P +
k
k + 1G,
where µi(T ) = E(Ti,rep | y) and σ2
i(T ) = var(Ti,rep | y), with Ti,rep = h(Y i,rep).
In Section 4.4, we illustrate this approach on the CTQ I smoking cessation
data (described in Section 1.4).
3.5.3 Posterior predictive checks
To determine how well the model ﬁts the data in an absolute sense, posterior
predictive checks can be used. They are a simple but versatile approach to
determine whether particular aspects of the data are captured adequately by
the model. They require sampling from the posterior predictive distribution
given in (3.15). The draws from p(yrep | y) can be made using the MCMC
output. In particular, for each draw of θ from the MCMC sample, we sample
a set of replicated data from p(yrep | θ). This is easy to do in WinBUGS.
Model critique requires choosing an appropriate data summary T , which
may be a function of the parameters θ, and comparing its value based on
the observed data, T (yobs; θ), to its values based on the replicated data,
T (yrep; θ). We provide some examples relevant for longitudinal data next.
Gelman, Meng, and Stern (1996) proposed Pearson’s χ2 statistics as an
overall measure of model ﬁt (designed for independent data). For (temporally
aligned) longitudinal data, we might use a multivariate version
T (y; θ) =
n
	
i=1
Qi(θ),
(3.19)
where
Qi(θ) = {yi −E(yi | θ)}TΣ(θ)−1{yi −E(yi | θ)}
(3.20)
and Σ(θ) = var(Y i | θ). Another global measure is the empirical distribution,
F of Qi(θ),
T (y; θ) = { F }.
(3.21)

68
BAYESIAN INFERENCE
The distribution of residuals for each time point,
eit(θ) = yit −E(Yit | θ)
var(Yit | θ)1/2 ,
might also be considered. Numerous other summaries can be chosen based on
the application.
Posterior predictive probabilities are a means of quantifying the relationship
between the statistics computed based on the observed data and the statistics
computed based on the replicated data, and can be used to assess model ﬁt.
Deﬁnition 3.7. Posterior predictive probability.
The posterior predictive probability based on data summary T (·; θ) is deﬁned
as

I

h(T (yobs; θ), T (yrep; θ)) > c

p(yrep | θ, y) p(θ | y) dθ dyrep
for some function h(·) and constant c.
2
For the multivariate version of Pearson’s χ2 (3.19), we might set c = 0 and
h{T (yobs; θ), T (yrep; θ)} = T (yobs; θ) −T (yrep; θ).
For the empirical cdf (3.21), we might again set c = 0 and
h{T(yobs; θ), T (yrep; θ)} = sign × arg max
x
|{ Fobs(x) −Frep(x)}|,
(3.22)
where Fobs(x) is the empirical cdf of Qi(θ) based on yobs, Frep(x) is the
empirical cdf of Qi(θ) based on yrep, and sign is the sign of this maximum
deviation. Extreme probabilities, either close to 0 or close to 1, suggest lack
of ﬁt with respect to T (·; θ).
We illustrate these checks in our analyses of the Growth Hormone data in
Section 4.2. We discuss modiﬁcations to these checks for incomplete data in
Chapters 6 and 8.
3.6 Nonparametric Bayes
Nonparametric and semiparametric Bayesian approaches that weaken model
assumptions have become much more common in the literature in recent years
due to breakthroughs in computations. In the context of semiparametric re-
gression, we have discussed spline approaches to model a trajectory over time
nonparametrically in Examples 2.7 and Example 3.10. There are also a variety
of approaches (see Further Reading) to specify distributions on the responses
or random eﬀects nonparametrically. Here we focus on mixtures of Dirichlet
process models (Escobar, 1994; MacEachern, 1994) as a way to do this. This
approach can often be implemented in WinBUGS (see Section 10.4 where it
is used to specify the dropout distribution).
Consider univariate responses yi with distribution p(yi | θi). Assume the

FURTHER READING
69
parameters θi follow some distribution G(θi). We specify a Dirichlet process
prior on the distribution G as G ∼DP(G0, α) with base measure αG0 and
mass parameter α. The distribution G0 is often chosen as a simple parametric
form (often what would be chosen for a fully parametric model). The mass
parameter α provides a measure of how similar the nonparametric distribution
G is to the parametric speciﬁcation G0. As α →∞, G →G0.
As a simple nonparametric speciﬁcation for a continuous response yi, we
can assume
yi
∼
N(µi, σ2)
µi
∼
G
G
∼
DP(G0, α),
with G0 a uniform (a, b) distribution.
These models can also be used for the random eﬀects distribution. See
for example Kleinman and Ibrahim (1998a, 1998b). For details on eﬃcient
computation for these models, see MacEachern and Muller (1998).
3.7 Further reading
Priors
In several recent papers, Daniels and others (Daniels and Kass, 1999, 2001;
Daniels and Pourahmadi, 2002) have proposed approaches to shrink toward
a parametric structure for the covariance matrix in longitudinal data; this
oﬀers both robustness to mis-speciﬁcation of the structure and parsimony via
the structure. This is well-suited to covariance matrices in longitudinal data
models where parsimonious parametric models are often used to describe the
covariance structure (see the multivariate normal models in Example 2.3 and
Chapter 6), particularly Section 6.4.1. Gelman (2006) discusses the folded
non-central t-distribution for a standard deviation parameter σ and shows it
is conditionally conjugate for a normal model.
Some results on the propriety of the posterior for various longitudinal
data models when using improper priors on regression coeﬃcients, variances,
and/or covariance matrices can be found in Natarajan and McCulloch (1995),
Hobert and Casella (1996), Natarajan (2001), Berger, Strawderman, and Tang
(2005), and Daniels (2006).
Prior elicitation and informative priors
For formulation and elicitation of informative priors, an alternative to conju-
gate priors is to use some sort of nonparametric prior, either constructed by
just specifying quantiles (Berger and O’Hagan, 1988) or more formally using
a modern Bayesian nonparametric approach (Oakley and O’Hagan, 2007). If
multiple experts are to be used for elicitation, a major issue becomes how

70
BAYESIAN INFERENCE
best to combine their opinions. Strategies can be found in Press (2003) and
Garthwaite, Kadane, and O’Hagan (2005), with additional references therein.
For elicitation and informative priors for covariance matrices, we refer the
reader to Brown, Le, and Zidek (1994), Garthwaite and Al-Awadhi (2001),
and Daniels and Pourahmadi (2002); for correlation matrices, some recent
work can be found in Zhang et al. (2006).
Other recent work on elicitation can be found in Kadane and Wolfson
(1998), O’Hagan (1998), and Chen et al. (2003). Excellent reviews can be
found in Chaloner (1996), Press (2003), and Garthwaite et al. (2005).
Computing
Slice sampling is an another approach to sample from unknown full condi-
tional distributions (Damien, Wakeﬁeld, and Walker, 1999; Neal, 2003). The
approach involves augmenting the parameter space with non-negative latent
variables in a speciﬁc way. It is a special case of data augmentation. WinBUGS
uses this approach for parameters with bounded domains.
Another approach that has been used to sample from unknown full condi-
tionals is hybrid MC (Gustafson, 1997; Neal, 1996), which uses information
from the ﬁrst derivative of the log full conditional and has been implemented
successfully in longitudinal models in Ilk and Daniels (2007) in the context
of some extensions of MTM’s for multivariate longitudinal binary data. For
other candidate distributions for the Metropolis-Hastings algorithm, we refer
the reader to Gustafson et al. (2004) who propose and review approaches that
attempt to avoid the random walk behavior of the certain Metropolis-Hastings
algorithm without necessarily doing an expensive numerical maximization at
each iteration. This includes hybrid MC as a special case.
For settings where the logarithm of the full conditional distribution is log
concave, Gilks and Wild (1992) have proposed easy-to-implement adaptive
rejection sampling algorithms.
Parameter expansion algorithms to sample correlation matrices have re-
cently been proposed (Liu, 2001; Liu and Daniels, 2006) that greatly simplify
sampling by providing a conditionally conjugate structure by sampling a co-
variance matrix from the appropriate distribution and then transforming it
back to a correlation matrix.
The eﬃciency of posterior summaries can be increased by using a tech-
nique called Rao-Blackwellization (Gelfand and Smith, 1990; Liu, Wong, and
Kong, 1994). See Eberly and Casella (2003) for Rao-Blackwellization applied
to credible intervals.
Additional extensions of data augmentation, termed marginal, conditional,
and joint, that can further improve the eﬃciency and convergence of the
MCMC algorithm can be found in van Dyk and Meng (2001).

FURTHER READING
71
Model comparison and model ﬁt
For other diagnostics for assessing model ﬁt, see Hodges (1998) and Gelfand,
Dey, and Chang (1992).
Several authors have expressed concern that the posterior predictive prob-
abilities, often called posterior predictive p-values, do not have a uniform dis-
tribution under the true model (Robins, Ventura, and van der Vaart, 2000).
Given this concern, Hjort, Dahl, and Steinbakk (2006) recently proposed a way
to appropriately calibrate these probabilities and more correctly call them p-
values.
Semiparametric and nonparametric Bayes
For further discussion of priors in p-splines, see Berry et al. (2002) and
Craineceau et al. (2005). Also, see Crainecau, Ruppert, and Carroll (2007)
for recent developments in the longitudinal setting.
Most Bayesian approaches to regression splines with unknown number and
location of knots use reversible jump MCMC methods (Green, 1995). See
Denison, Mallick and Smith (1998) and DiMateo, Kass, and Genovese (2001)
for methodology for a single longitudinal trajectory and Botts and Daniels
(2007) for methodology for multiple trajectories (the typical longitudinal set-
ting).
To model distributions nonparametrically, Dirichlet process priors (see Fer-
guson, 1982, and Sethuraman, 1994) and polya tree priors (Lavine, 1992, 1994)
can be speciﬁed directly for the distribution of responses or for random eﬀects.

CHAPTER 4
Worked Examples
using Complete Data
4.1 Overview
In this chapter we illustrate many of the ideas discussed in Chapters 2 and 3
by analyzing several of the datasets described in Chapter 1. For simplicity, we
focus only on subjects with complete data. Missing data and dropout in these
examples are addressed in a more deﬁnitive way in the analyses in Chapters 7
and 10. The sole purpose here is to illustrate using real data models from
Chapter 2 and inferential methods described in Chapter 3.
4.2 Multivariate normal model: Growth Hormone study
We will illustrate aspects of model selection and inference for the multivariate
normal model described in Example 2.3 using the data from the growth hor-
mone trial described in Section 1.3. The primary outcome of interest is mean
quadriceps strength (QS) at baseline, month 6, and month 12; the vector of
outcomes for subject i is Y i = (Yi1, Yi2, Yi3)T. Subjects were randomized to
one of four treatment groups: growth hormone plus exercise (EG), growth hor-
mone (G), placebo plus exercise (EP), or placebo (P); we denote treatment
group as Zi, which takes values {1, 2, 3, 4} for the four treatment groups, re-
spectively. The main inferential objective for this data is to compare mean
QS at 12 months in the four treatments.
4.2.1 Models
The general model is
Y i | Zi = k ∼N(µk, Σk),
(4.1)
with µk = (µ1k, µ2k, µ3k)T and Σk = Σ(φk); φk contains the six nonredun-
dant parameters in the covariance matrix for treatment k. We compare models
with φk distinct for each treatment with reduced versions, including φk = φ.
72

GROWTH HORMONE STUDY
73
Table 4.1 Growth hormone trial: sample covariance matrices for each treatment.
Elements below the diagonal are the pairwise correlations.
EG
G



563
516
589
.68
1015
894
.77
.87
1031






490
390
366
.85
429
364
.83
.89
397



EP
P



567
511
422
.85
631
482
.84
.91
442






545
380
292
.81
403
312
.65
.81
367



4.2.2 Priors
Conditionally conjugate priors are speciﬁed as in Example 3.8 using diﬀuse
choices for the hyperparameters,
µk
∼
N(0, 106I)
Σ−1
∼
Wishart(ν, A−1/ν),
where ν = 3 and A = diag{(600, 600, 600)}. The scale matrix A is set so
that the diagonal elements are roughly equal to those of the sample variances
(Table 4.1).
4.2.3 MCMC details
For all the models, we ran four chains, each with 10, 010 iterations with a
burn-in of 10 iterations (they all converged very quickly). The chains mixed
well with minimal autocorrelation.
4.2.4 Model selection and ﬁt
Based on an examination of sample covariance matrices in Table 4.1, we con-
sidered three models for the treatment speciﬁc covariance parameters:
(1) {φk : k = 1, . . . , 4}
(2) {φk = φ : k = 1, . . . , 4},
(3) {φ1, φk = φ : k = 2, 3, 4}.
We ﬁrst used the DIC with θ = {β, Σ(φk)−1}. Results appear in Ta-
ble 4.2. The DIC results clearly favored covariance model (3). When we re-

74
WORKED EXAMPLES USING COMPLETE DATA
Table 4.2 Growth hormone trial: DIC for the three covariance models.
θ = {β, Σ(φk)−1}
θ = {β, Σ(φk)}
Model
DIC
Dev(θ)
PD
DIC
Dev(θ)
PD
(1)
2209
2139
35
2194
2153
20
(2)
2189
2152
18
2187
2154
17
(3)
2174
2127
24
2169
2132
19
parameterized the DIC using θ = (β, Σ(φk)), the diﬀerence between the DICs
for models (1) and (2) decreased by over 60%, illustrating the sensitivity of
the DIC to the parameterization of θ as discussed in Section 3.5.1.
We computed the multivariate version of Pearson’s χ2 statistic
T (y; θ) =
n
	
i=1
(yi −µk)TΣ−1
k (yi −µk)
(4.2)
as an overall measure of model ﬁt based on the posterior predictive distribu-
tion of the residuals. The posterior predictive probability .11, which did not
indicate a substantial departure of the model from the observed data.
4.2.5 Results
Based on the model selection results, we base inference on covariance model (3).
Posterior means and 95% credible intervals for the mean parameters are given
in Table 4.3. The posterior estimate of the baseline mean on the EG treat-
ment diﬀers substantially from the other arms, but our analysis is conﬁned to
completers. We can see the mean baseline quadriceps strength is very diﬀerent
from those who dropped out (cf. Table 1.2).
Histograms of the posterior distributions of the pairwise diﬀerences be-
tween the four treatment means at month 12 are given in Figure 4.1. We
observe that for all the pairwise comparisons (except for G vs. P), most of
the mass was either to the left (or right) of zero. We can further quantify this
graphical determination by computing posterior probabilities that the month
12 mean was higher for pairs of treatments. As an example, we can quantify
the evidence for the eﬀect of growth hormone plus exercise over placebo plus
exercise (EG vs. EP) using the posterior probability
P(µ13 > µ33 | y) =

I{(µ13 −µ33) > 0} p(µ | y) dµ.
This is an intractable integral, but it can be computed easily from the posterior

SCHIZOPHRENIA TRIAL
75
Table 4.3 Growth hormone trial: posterior means and 95% credible intervals of the
mean parameters for each treatment under covariance model (3).
.
Month
Treatment
0
6
12
EG
78 (67, 89)
90 (76, 105)
88 (74, 103)
G
67 (59, 76)
64 (56, 72)
63 (56, 71)
EP
65 (57, 73)
81 (73, 89)
73 (65, 80)
P
67 (58, 76)
62 (54, 70)
63 (55, 71)
sample using
1
K
K
	
k=1
I{µ(k)
13 −µ(k)
33 > 0},
where k indexes draws from the MCMC sample of size K (after burn-in). For
this comparison, the posterior probability was .99, indicating strong evidence
that the 12-month mean on treatment EG is higher than treatment EP.
4.2.6 Conclusions
The analysis here suggests that the EG arm improves quadriceps strength
more than the other arms. The DIC was used to choose the best ﬁtting co-
variance model and posterior predictive checks suggested that this model ﬁt
adequately.
Our analysis has ignored dropouts, which can induce considerable bias in
estimation of mean parameters. We saw this in particular by comparing the
baseline mean on the EG treatment for the completers only vs. the full data
given in Table 1.2. We discuss such issues, revisit this example, and do a more
deﬁnitive analysis of this data in Section 7.2 under MAR and Section 10.2
using pattern mixture models that allow MNAR and sensitivity analyses.
4.3 Normal random eﬀects model: Schizophrenia trial
We illustrate aspects of inference for the normal random eﬀects model de-
scribed in Example 2.1 using the data from the Schizophrenia Trial described
in Section 1.2. The goal of this 6-week trial is to compare the mean change in
schizophrenia severity (as measured by BPRS scores) from baseline to week 6
between four treatment groups. The treatment groups consist of three doses
of a new treatment (low (L), medium (M), high(H)) and a standard dose (S)
of an established treatment, halperidol.

76
WORKED EXAMPLES USING COMPLETE DATA
EG vs. G
mean difference
Frequency
−20
0
20
40
60
0
1000
EG vs. EP
mean difference
Frequency
−20
0
20
40
60
0
1000
EG vs. P
mean difference
Frequency
−20
0
20
40
60
0
1000
G vs. EP
mean difference
Frequency
−20
0
20
40
60
0
1500
G vs. P
mean difference
Frequency
−20
0
20
40
60
0
1500
EP vs. P
mean difference
Frequency
−20
0
20
40
60
0
1500
Figure 4.1 Posterior distribution of pairwise diﬀerences of month 12 means for co-
variance model (3) for the growth hormone trial. Reading across, the corresponding
posterior probabilities that the pairwise diﬀerence in means are greater than zero are
1.00, .97, 1.00, .04, .52, .96.
The vector of outcomes for subject i is Y i = (Yi1, . . . , Yi6)T. We denote
treatment group as Zi, which takes values {1, 2, 3, 4} for the four treatment
groups, respectively.
4.3.1 Models
We use the normal random eﬀects model described in Example 2.1,
Y i | bi, Zi = k
∼
N(Xi(βk + bi), σ2I)
bi
∼
N(0, Ω),
where the jth row of Xi, xij, is an orthogonal quadratic polynomial. Fig-
ure 1.1 provides justiﬁcation for the quadratic trend. Using a random eﬀects

SCHIZOPHRENIA TRIAL
77
model here provides a parsimonious way to estimate var(Y i | Zi), which has
21 parameters, by reducing it to 7 parameters (6 in Ωand the variance com-
ponent σ2).
4.3.2 Priors
Conditionally conjugate diﬀuse priors were speciﬁed as in Example 3.9 for βk
and Ω−1,
βk
∼
N(0, 106I3),
Ω−1
∼
Wishart(ν, A−1/ν),
where k = 1, 2, 3, 4 indexes treatment group. For the Wishart prior, ν = 3
and A = diag{(120, 2, 2)}. Diagonal elements of A are chosen to be consistent
with the observed variability (across subjects) of the orthogonal polynomial
coeﬃcients from ﬁtting these to each subject individually. For the within-
subject standard deviation σ, we use the bounded uniform prior
σ ∼Unif(0, 100).
4.3.3 MCMC details
For all the models, we ran four chains, each with 10, 010 iterations, with
a burn-in of 10 iterations (similar to the Growth Hormone example, they all
converged very quickly). The autocorrelation in the chains was higher than the
growth hormone trial analysis in Section 4.2, but was negligible by iteration 25.
4.3.4 Results
Table 4.4 contains posterior means and 95% credible intervals for the treat-
ment speciﬁc regression coeﬃcients βk. Clearly the quadratic trend is neces-
sary as the 95% credible interval for the quadratic coeﬃcient for each treat-
ment β2k excluded zero.
Figure 4.2 plots the posterior means of the trajectories for each treatment
over the 6 weeks of the trial; completers on the high dose appeared to do best,
completers on the low dose did worst.
The last column of Table 4.4 gives the estimated change from baseline
for all four treatments. All of the changes were negative, with credible inter-
vals that excluded zero, indicating that all treatments reduced symptoms of
schizophrenia severity. The smallest improvement was seen in the low dose
arm, with a posterior mean of −12 and a 95% credible interval (−19, −5).
Table 4.5 shows the estimated diﬀerences in the change from baseline be-
tween treatments. None of the changes from baseline were diﬀerent between
the four treatments as the credible intervals for all the diﬀerences covered
zero.

78
WORKED EXAMPLES USING COMPLETE DATA
Figure 4.2 Schizophrenia trial: posterior mean trajectories for each of the four treat-
ments.
Adequacy of the random eﬀects covariance structure
Table 4.6 shows the sample covariance matrix and the posterior mean of
the marginal covariance matrix under the random eﬀects model Σ = σ2I +
wiΩwT
i . The random eﬀects structure, with only seven parameters, appears
to provide a good ﬁt, capturing the form of the sample covariance matrix (un-
structured). We do more formal covariance model selection for the schizophre-
nia data in Section 7.3.
4.3.5 Conclusions
Our analysis showed improvement in schizophrenia severity in all four treat-
ment arms, but did not show signiﬁcant diﬀerences in the change from baseline
between the treatments. However, similar to the previous example with the
growth hormone data, dropouts were ignored. Figure 1.1 suggests very diﬀer-

CTQ I STUDY
79
Table 4.4 Schizophrenia trial: posterior means and 95% credible intervals for the
regression parameters and changes from baseline to week 6 for each treatment.
Treatment Group
Parameter
Low
Medium
High
Standard
β0
27
25
22
25
(22, 32)
(22, 29)
(18, 26)
(21, 39)
β1
–2.0
–2.6
–2.7
–2.8
(–2.9, –1.1)
(–3.4, –1.9)
(–3.5, –2.0)
(–3.6, –2.1)
β2
.8
.7
.8
.8
(.5, 1.2)
(.4, 1.0)
(.5, 1.2)
(.5, 1.1)
Change
–12
–16
–16
–17
(–17, –7)
(–20, –11)
(–21, –12)
(–21, –13)
Table 4.5 Schizophrenia trial: posterior means and 95% credible intervals for the
pairwise diﬀerences of the changes from baseline among the four treatments.
L vs. M
L vs. H
L vs. S
M vs. H
M vs. S
H vs. S
4
4
5
0
1
1
(–3, 11)
(–3, 11)
(–2, 12)
(–6, 7)
(–5, 8)
(–6, 7)
ent BPRS scores between dropouts and completers. This example is revisited
in Section 7.3 under an assumption of MAR using all the data.
4.4 Models for longitudinal binary data: CTQ I Study
We illustrate aspects of inference and model selection for longitudinal binary
data models using data from the CTQ I smoking cessation trial, described in
Section 1.4. The outcomes in this study are binary weekly quit status from
weeks 1 to 12. The protocol called for women to quit at week 5. As such,
the objective is to compare the time-averaged treatment eﬀect from weeks 5
through 12. We ﬁt both a logistic-normal random eﬀects model (Example 2.2)
and a marginalized transition model with ﬁrst-order dependence (MTM(1),
Example 2.5).
The vector of outcomes for woman i is Y i = (Yi1, . . . , Yi,12)T, binary indi-
cators of weekly quit status for weeks 1 through 12. Treatment is denoted by
Xi = 1 (exercise) or Xi = 0 (wellness).

80
WORKED EXAMPLES USING COMPLETE DATA
Table 4.6 Schizophrenia trial: sample covariance matrix and posterior mean of the
marginal covariance matrix under the random eﬀects structure.
Sample Covariance
126
104
81
85
75
74
.71
169
132
126
106
111
.58
.82
154
147
131
134
.57
.72
.89
179
149
151
.51
.62
.81
.85
171
162
.46
.59
.75
.78
.86
209
Posterior Mean Covariance
153
111
99
90
86
86
.75
145
123
124
122
109
.62
.79
166
149
150
130
.53
.75
.84
189
169
150
.49
.71
.82
.86
204
168
.46
.60
.67
.73
.78
226
4.4.1 Models
(1) Logit normal random eﬀects model
Our random eﬀects speciﬁcation uses a single random eﬀect bi, and assumes
separate cessation rates before and after week 5, the designated quit week.
Let Zj = I(j ≥5). The treatment eﬀect is the log odds ratio comparing
time-averaged quit rate from week 5 to week 12 (i.e., over the period where
Zj = 1).
The model is given in two levels. At the ﬁrst level, weekly quit indicators
within individual are independent Bernoulli outcomes conditionally on bi,
Yij | Xi, bi, ∼Ber(µb
ij),
with
logit(µb
ij) = (1 −Zj)α + Zj(bi + β0 + β1Xi).
Here, β = (β0, β1)T, and β1 is the subject-speciﬁc log odds ratio capturing
the treatment eﬀect. At the second level, we assume
bi | τ2 ∼N(0, τ2).

CTQ I STUDY
81
The marginal, or population-averaged, treatment eﬀect is approximated by
βm
1 ≈β1K(τ 2), where K(τ 2) = (1 + 0.346 × τ2)−1/2 (Zeger and Liang, 1992).
(2) Marginalized transition model
We use a similar formulation to specify the MTM(1). Serial correlation is
modeled only for the weeks following the quit date. The marginal mean follows
logit(µij) = (1 −Zj)αm + Zj(βm
0 + βm
1 Xi),
where superscript m denotes parameters from the marginal (population aver-
aged) rather than conditional (subject-speciﬁc) distribution. For j = 5, . . . , 12,
serial correlation follows
logit(φij) = ∆ij + γiYi,j−1,
where φij = E(Yij | Yi,j−1). The correlation is allowed to diﬀer by treatment
group by setting
γi = φ0 + φ1Xi.
4.4.2 Priors
Regression parameters in both models and serial correlation parameters in the
MTM(1) are assumed independent with ‘just proper’ normal priors N(0, 106).
In the random eﬀects model, the standard deviation τ of the random eﬀects
is given a Unif(0, 100) prior.
4.4.3 MCMC details
For each model, inference is based on a posterior sample of size 30,000, gen-
erated by running three chains each with a burn-in comprising 1000 samples.
The autocorrelation for some parameters in the logistic random eﬀects model
was quite large, not dying down until lag 30. For the MTM, the chains mixed
very well with minimal autocorrelation.
4.4.4 Model selection
To select between the two models, we use posterior predictive loss (PPL).
We specify the loss function given in (3.16). As a univariate summary of the
multivariate response for each subject, we chose Ti = 12
j=5 Yij, the sum of
the responses for weeks 5 to 12, which is proportional to the average quit rate

82
WORKED EXAMPLES USING COMPLETE DATA
for weeks 5 to 12. For these choices, the criterion takes the form
PPL
=
n
	
i=1
σ2
i(T ) +
k
k + 1
n
	
i=1
(µi(T ) −Ti)2
=
P +
k
k + 1G,
where
µi(T )
=
E(Ti,rep | y) = E


12
	
j=5
Yi,rep


σ2
i(T )
=
var(Ti,rep | y) = var


12
	
j=5
Yi,rep

.
The choice of PPL here avoids the need to compute the integrated (over the
random eﬀects) likelihood necessary for the DIC. The PPL results appear in
Table 4.7. The PPL for the MTM is considerably smaller than the logistic
random eﬀects model. The ﬁt term G is almost ten times smaller; in fact,
the ﬁt term for the random eﬀects models here is about the same as ﬁtting
an independence model (results not shown). In addition, the complexity term
P is also much smaller under the MTM; the large complexity term for the
random eﬀects model is directly related to the large random eﬀects variance
(given in Table 4.8). Clearly, inference should be based on the MTM.
Table 4.7 CTQ I: posterior predictive loss (PPL) for the marginalized transition
model (MTM) and logistic random eﬀects (LRE) models.
Model
PPL
G
P
LRE
4346
2168
2178
MTM
303
166
137
4.4.5 Results
Table 4.8 summarizes the key parameters. Turning ﬁrst to the MTM(1), we
can compute the posterior mean and credible intervals of the smoking rates
in the control and treatment conditions using the MCMC draws of βm
0 and
βm
1 . The rates are given by
h1(βm
0 )
=
exp(βm
0 )/{1 + exp(βm
0 )}
h2(βm
0 , βm
1 )
=
exp(βm
0 + βm
1 )/{1 + exp(βm
0 + βm
1 )}.

CTQ I STUDY
83
We can compute their posterior means and credible intervals by evaluating
these functions at each iteration of the MCMC sample to obtain a posterior
sample of these parameters, giving posterior means (credible intervals) of .30
(.24, .37) and .40 (.32, .48), respectively.
In the MTM, the posterior mean of the population-averaged treatment
log odds ratio is βm
1
= .43 (odds ratio roughly 1.5). Its posterior credible
interval (−.04, .88) suggests those on exercise have higher cessation rates.
Within-subject correlation in the control group is very high, as indicated by
the posterior distribution of φ0; the posterior of φ1 does not suggest any
diﬀerence in correlation between treatment groups (credible interval covers
zero).
In the random eﬀects model, the between subject variance τ2 is very large,
leading to a substantial diﬀerence between the subject-speciﬁc eﬀect β1 (poste-
rior mean 1.8) and its model-derived population-averaged eﬀect βm
1 (posterior
mean .50). The mean of the population-averaged eﬀect is similar between the
two models.
Table 4.8 CTQ I: posterior estimates of key model parameters from logistic-normal
random eﬀects (LRE) model and ﬁrst-order marginalized transition model (MTM).
Model
Parameter
Posterior Mean
95% Credible Interval
LRE
β1
1.8
(–.12, 3.7)
τ2
6.0
(4.7, 7.5)
βm
1
.50
(–.03, 1.0)
MTM
βm
0
–.85
(–1.2, –.53)
βm
1
.43
(–.04, .88)
φ0
4.4
(3.9, 4.9)
φ1
.79
(.00, 1.6)
4.4.6 Conclusions
Both models suggest a positive eﬀect of exercise on quit rates. The population
averaged treatment eﬀect was very diﬀerent than the subject speciﬁc eﬀect
in the random eﬀects model due to the large variability of the random eﬀect
in the population. PPL strongly indicated that the MTM ﬁt the data better
than the logistic random eﬀects model.
Figure 1.3 shows large diﬀerence in the observed quit rates between com-

84
WORKED EXAMPLES USING COMPLETE DATA
pleters and dropouts. In Section 7.4, we do a more appropriate analysis under
an MAR assumption using all the observed data.
4.5 Summary
We have used several of the data examples introduced in Chapter 1 to illus-
trate some features of the regression models introduced in Chapter 2 using the
inferential methods described in Chapter 3. To maintain focus on key ideas,
we only used subjects with complete data. Thus, the conclusions should not be
viewed as valid given that a sensible approach to handle missing data was not
undertaken. These analyses only serve to illustrate the models and inferential
methods on real data.
In the rest of the book, we discuss ways to more appropriately handle
missing data, starting in the next chapter with a careful deﬁnition of diﬀerent
types of missing data and dropout. All the approaches proposed to handle
dropout in the remainder of the book will use all the observed data (not just
completers).

CHAPTER 5
Missing Data Mechanisms and
Longitudinal Data
5.1 Introduction
This chapter lays out the deﬁnitions and assumptions that are commonly used
to make inference about a full-data distribution from incompletely observed
data. Some speciﬁc examples help to motivate the discussion.
Example 5.1. Dropout in a longitudinal clinical trial.
In the Growth Hormone study described in Section 1.3, individuals were sched-
uled for measurement at baseline, month 6, and month 12. The target of in-
ference is mean diﬀerence in quadriceps strength between the two treatment
groups, at month 12, among everyone who began follow up. This is a pa-
rameter indexing the distribution of full data; i.e., the data that would have
been observed had everyone completed the study. However several individuals
dropped out of the study before completing follow up.
2
Example 5.2. Dropout and mortality in a cohort study.
The HER Study (Section 1.5) followed 871 HIV-infected women for up to 6
years, recording clinical and behavioral outcomes every 6 months. All women
were scheduled for 12 follow-up measurements. About 10 percent died from
AIDS and other causes, and about another 20 percent withdrew or were lost
to follow-up. For a speciﬁc outcome of interest, like CD4 count, the full data
could be deﬁned in several ways. One deﬁnition is all CD4 counts that were
scheduled to be observed (12 for each woman). Because of the diﬃculty in
conceptualizing CD4 count for someone who has died, an alternate deﬁnition
is CD4 counts taken while still alive.
2
A key theme of this chapter is the distinction, for purposes of model spec-
iﬁcation and inference, between full data and observed data. Inferences about
full data that are based on incomplete observations must rely on assumptions
about the distribution of missing responses. We demonstrate here that these
assumptions are always encoded in a full data model by some combination of
modeling assumptions and constraints on the parameter space. This is even
true — actually it is especially true — for commonly used assumptions such
as MAR and ignorability.
Section 5.2 provides deﬁnitions for full data and characterizes some common
missing data processes such as dropout and monotone missingness. Section 5.3
85

86
MISSING DATA MECHANISMS
gives a general conceptualization of the full-data model, and Section 5.4 de-
scribes missing data mechanisms such as MAR in the context of a full-data
model. In Section 5.5 we show how the MAR assumption applies to dropout in
longitudinal studies. The remainder of the chapter is devoted to model spec-
iﬁcation and interpretation under various assumptions for the missing data
mechanism. A number of issues are highlighted for further reading.
5.2 Full vs. observed data
5.2.1 Overview
When drawing inference from incomplete data, it is necessary to expand the
context of the modeling problem to characterizing some set of ‘full’ — but
incompletely observed — data. In this section we diﬀerentiate full data, ob-
served data, and full data response.
The full data refers to all elements of
the data that are observed or are intended to be observed, usually including
some response or dependent variable, covariates of direct interest, auxiliary
variables, and missing data indicators. Observed data comprise the observed
subset of the full data. Finally, the full-data response refers to the dependent
variable of primary interest. While missing data indicators are part of the full
data, their distribution or relation to covariates typically is not of primary
interest to the modeler.
As an example, consider the schizophrenia trial (Section 1.2), which com-
pares standard treatment to one of several doses of a new therapy. The design
calls for symptom severity to be recorded weekly for 6 weeks, but many pa-
tients drop out of the study before their follow-up is complete. Here, the full
data consists of
– Symptom severity scores for all weeks, regardless of whether they actually
were recorded;
– A binary random variable for each week, indicating whether the symptom
severity was recorded (sometimes called missing data indicators);
– The covariate of interest, namely treatment group;
– Other baseline covariates that may have been recorded.
The full response data comprise only the ﬁrst item. The observed data comprise
the observed elements of symptom score, the missing data indicators, and the
covariates.
The remainder of this section lays out notation and deﬁnitions; in Sec-
tion 5.3 we deﬁne parameters that can be used to characterize various aspects
of the full-data distribution.

FULL VS. OBSERVED DATA
87
5.2.2 Data structures
Consider ﬁrst the case of bivariate response, where the ﬁrst response is always
observed but the second one may be missing. The full data for individual i
consists of a response vector Y i = (Yi1, Yi2)T, a 2 × p matrix Xi of model
covariates, a 2 × q matrix V i of auxiliary covariates, and the missing data
indicator Ri. Throughout the book, we assume the process that causes re-
sponse data to be missing is stochastic (as opposed to being part of a study
design); hence Ri (and its generalizations below) are always treated as random
variables.
The observed data for individual i are Oi = (Y i,obs, Xi, V i, Ri), where
Y i,obs =

(Yi1, Yi2)T
if Ri = 1
Yi1
if Ri = 0 .
For more general patterns of longitudinal data, similar structures apply. Con-
sider ﬁrst the case of temporally aligned observations. The full data response
vector is Y i = (Yi1, . . . , YiJ)T. The vector Ri = (Ri1, . . . , RiJ)T indicates
which components are observed, with Rij = 1 if Yij is observed, and Rij = 0
otherwise. Let Ji = 
j Rij denote the number of full-data components of
Y i that are observed. A useful way to represent the observed and missing
components of the full data is the (possibly temporally unordered) partition
(Y T
i,obs, Y T
i,mis)T. The subvector Y T
i,obs is Ji×1, with elements {Yij : Rij = 1};
similarly, Y i,mis is (J −Ji) × 1, with elements {Yij : Rij = 0}.
For temporally misaligned data, the situation is somewhat diﬀerent; the
deﬁnition of full data is not always obvious because the timing of measure-
ments is itself a random variable. For example, the full response data could
be all values of a stochastic process {Yi(t)} over a ﬁxed range of t, and the
observed data is the subset {Yi(ti1), . . . , Yi(tiJi)}. Here the observation times
ti1, . . . , tiJi might arise from a marked counting process Ni(t), with obser-
vations of Yi(t) taken at points where dNi(t) = 1; i.e., where the counting
process jumps (see Yao et al., 1998; Lin and Ying, 2001; and Lin et al., 2004b
for examples). While our focus is primarily on full data with ﬁxed (as opposed
to random) observation times, many of the topics we discuss can be applied to
settings where observation times are random. For instance, a key considera-
tion in modeling longitudinal data is whether the missing data process deﬁned
by R is independent of the responses Y ; for continuous-time processes, the
consideration is whether N(t) is in some sense independent of Y (t). For a
detailed treatment, see Lin and Ying (2001) and Tsiatis and Davidian (2004).
5.2.3 Dropout and other processes leading to missing responses
Any number of events can lead to missing data. Commonly encountered ex-
amples include

88
MISSING DATA MECHANISMS
– Missed visits, either at random or for reasons related to response, such
as when a patient in a study of depression fails to show up when he is
experiencing symptoms;
– Withdrawal from a study, decided either by the participant or by the
investigator conducting the study; common examples in pharmacologic
trials are withdrawal due to side eﬀects, toxicity, or lack of eﬃcacy;
– Loss to follow-up, distinguished from withdrawal because the reasons are
not reported;
– Death or disabling event, possibly related to the outcome but sometimes
not; for example, in a longitudinal study of HIV, accidental death is not
outcome related;
– Missingness by design, as in a longitudinal survey where only a subset of
individuals is selected for follow-up.
This certainly is not exhaustive but covers many reasons for missing data
in longitudinal studies. Adding to the complexity of handling missing data
is that missingness may have diﬀerent causes, and in most cases should be
treated diﬀerently. Withdrawal for lack of eﬃcacy is a diﬀerent process than
withdrawal for toxicity; outcome-related mortality must be treated diﬀerently
than death by other causes.
Our focus throughout the book is primarily on dropout, and for simplicity
we begin with the assumption that dropout — and its relation to the response
process — can be captured using a single random variable. When there are
distinct types of dropout such that they are related to outcome in diﬀerent
ways, then it is straightforward to introduce a multinomial version of the
missing data indicator, and many of the same ideas discussed here will apply
directly (see Rotnitzky et al., 2001, for example). Also, see Section 8.4.5.
Before moving forward to describing models for incomplete data, it is nec-
essary to deﬁne formally two important terms: dropout and monotone missing
data pattern. For many of the models we discuss in this and subsequent chap-
ters, monotone missingness is a key requirement.
Deﬁnition 5.1. Dropout process.
For full-data responses Y1, . . . , YJ scheduled to be recorded at times t1, . . . , tJ,
let R1, . . . , RJ denote the missing data indicators, with Rj = 1 if Yj is observed
and Rj = 0 if missing. A missing data process is a dropout process if for some
j such that 1 < j < J, Rj = 0 ⇒Rj+k = 0 for all 1 < k ≤J −j; that is,
there exists a measurement occasion j such that a missing response at time j
implies all subsequent observations are missing.
2
Missingness that does not lead to dropout usually is called intermittent
missingness because rather than truncating the longitudinal process, it creates
gaps. Dropout that occurs in the absence of intermittent missingness leads to
a monotone pattern for the responses

FULL-DATA MODELS AND MISSING DATA MECHANISMS
89
Deﬁnition 5.2. Monotone missing data pattern.
A missing data pattern is monotone if, for each individual, there exists a
measurement occasion j such that R1 = · · · = Rj−1 = 1 and Rj = Rj+1 =
· · · = RJ = 0; that is, all responses are observed through time j −1, and no
responses are observed thereafter.
2
Throughout the book, we will use time-to-event variables S and U to char-
acterize follow-up time and dropout time, respectively.
Deﬁnition 5.3. Follow-up time.
Assuming discrete time measurement at times times t1, . . . , tJ, the follow-up
time is the time of the last observed measurement. It is denoted by tS, where
S = max{j : Rj = 1}. When missingness is monotone, S = 
j Rj.
2
Deﬁnition 5.4. Dropout time.
Again assuming discrete-time measurement at times t1, . . . , tJ, the dropout
time is the ﬁrst scheduled measurement time after the follow-up time (i.e.,
after the last observed response). It is denoted by tU, where U = 1+S and by
convention, U = 1 + J for those with complete follow-up. When missingness
is monotone, U = 1 + 
j Rj.
2
Clearly S and U carry equivalent information. The distinction is made because
in some situations it is more convenient to deﬁne the missing data process in
terms of one or the other.
5.3 Full-data models and missing data mechanisms
5.3.1 Targets of inference
In almost all practical settings, the analyst is interested in drawing inference
about a function of the parameter θ that indexes the full data response model
p(y | x, θ) = p(y1, y2, . . . , yJ | x, θ).
Examples include the full-data mean µ(θ) = E(Y | θ) and coeﬃcients β =
β(θ) in a regression model for E(Y | X, β).
When data are completely observed, p(y | x, θ) can be speciﬁed directly. If
responses are not fully observed, θ is a function of the parameter ω indexing
a larger model p(y, r | x, ω) of the full data (Y , R, X); hence θ = θ(ω). The
form of the full-data response model p(y | x, θ(ω)) will therefore depend on
speciﬁcations of and assumptions about the full-data model p(y, r | x, ω).
When information on auxiliary covariates V is used, the full-data model is
p(y, r, v | x, ω). We defer discussion of auxiliary covariates to Section 5.4.4.
Deﬁnition 5.5. Full-data model.
Let Y denote the full-data response vector for an individual, let R denote the
associated vector of missingness indicators, and let X represent covariates of
interest. The full-data model p(y, r | x, ω) describes the joint distribution of Y

90
MISSING DATA MECHANISMS
and R, conditionally on covariates X, and is indexed by a ﬁnite-dimensional
parameter ω.
2
Deﬁnition 5.6. Full-data response model.
The full-data response model characterizes the distribution of Y conditionally
on covariates X. It is indexed by a parameter θ = θ(ω) that is a subset or
function of the full-data parameter ω, and is related to the full-data model
via
p(y | x, θ(ω)) =
	
r∈R
p(y, r | x, ω),
where R is the sample space of R.
2
A given full-data model will give rise to only one observed-data model, but
the converse of course is not true. Our motivation for starting with a full-data
model is to force the analyst to specify, through appropriate subject-matter
motivations, a model for the data generating process and its relationship to
the missingness indicators.
Inference about θ will depend crucially on choices made in specifying the
full-data model p(y, r | x, ω); however, observed data oﬀer no information to
validate these choices as they relate to the distribution of missing responses
Y mis. Assumptions made by the data analyst will generally exert consider-
able inﬂuence over ﬁnal inferences, even for very large samples of observed
data. This point can sometimes be lost when using popular models based
on assumptions like missing at random (MAR) and implemented in standard
software packages. In many cases, this approach yields valid inference about
a particular full-data model under speciﬁc assumptions about the joint distri-
bution of Y and R and/or constraints on the parameter ω.
As an example, many software packages now ﬁt random eﬀects models to
incomplete data using maximum likelihood, under the assumption that both
the random eﬀects and the residual error distributions are normal (e.g. PROC
MIXED in SAS or xtreg in Stata). When the data are incomplete, the analyst
is implicitly assuming that the model speciﬁed in the software routine applies
to the full-data response, and that missingness is MAR (or more speciﬁcally,
ignorable; see Deﬁnition 5.12).
5.3.2 Missing data mechanisms
In general terms, the missing data mechanism is the stochastic mechanism
leading to missingness among elements of Y . Formally, we will use the term
missing data mechanism to refer to the conditional distribution of missing
data indicators R given the full-data response Y and covariates X, denoted
by p(r | y, x, ω). Generally speaking, speciﬁcation of p(y, r | x, ω) will imply a
missing data mechanism; in practice, the modeler often begins with a working

ASSUMPTIONS ABOUT MISSING DATA MECHANISM
91
assumption about the missing data mechanism and uses it to specify the full-
data model.
Deﬁnition 5.7. Missing data mechanism.
The missing data mechanism is the model for the joint distribution of miss-
ing data indicators R as a function of Y and X; it is indexed by a ﬁnite-
dimensional parameter ψ = ψ(ω) and written as p(r | y, x, ψ(ω)).
2
Any full-data model can therefore be factored as the product of a full-data
response model and the associated missing data mechanism,
p(y, r | x, ω) = p(y | x, θ(ω)) p(r | y, x, ψ(ω)).
(5.1)
Being able to characterize the missing data mechanism does not depend on
whether the full-data model has been speciﬁed according to (5.1), which is
commonly referred to as a selection model factorization. The implied missing
data mechanism can, in principle, be derived from any speciﬁcation of the
full-data model, but it may not always take a closed form.
5.4 Common assumptions about the missing data mechanism
Restrictions on the missing data mechanism can be classiﬁed as missing com-
pletely at random (MCAR), missing at random (MAR), or missing not at
random (MNAR); these progressively weaker assumptions delineate the de-
pendence of the missing data indicators R on the observed and missing parts
of the full-data response vector Y (conditionally on X and possibly also on
V ). The taxonomy and its terminology were developed by Rubin (1976), and
generalized using the ‘coarsening’ framework by Heitjan and Rubin (1991).
Robins et al. (1995), Jacobsen and Keiding (1995), Gill and Robins (1997),
and Gill and Robins (2001) develop formalizations for stochastic process and
longitudinal data settings. Recent surveys on the characterization of missing
data mechanisms in longitudinal studies, from a variety of diﬀerent perspec-
tives, can be found in Little (1995), Little and Rubin (2002), van der Laan
and Robins (2003), Hogan et al. (2004b), Tsiatis (2006), and Molenberghs
and Kenward (2007).
To deﬁne missing data assumptions, it is useful to rewrite the missing data
mechanism as
p(r | y, x, ψ) = p(r | yobs, ymis, x, ψ)
to emphasize dependence of R on the observed and missing components of Y .
Although we write ψ, it is assumed that ψ = ψ(ω) unless stated otherwise.
5.4.1 Missing completely at random (MCAR)
Missingness completely at random occurs when data are missing for reasons
wholly unrelated to either the observed or missing parts of Y , conditionally

92
MISSING DATA MECHANISMS
on X. With repeated measurements, the data can be used to provide evidence
against the MCAR assumption through the association between R and Y obs;
e.g., by regressing Rj on one or more observed components of (Y1, . . . , Yj−1)T
(see also Chen and Little, 1999). In general, however, analyses under the
weaker MAR assumption will provide valid inference when MCAR holds. We
include a description here for the purposes of completeness and to introduce
MAR and MNAR.
Deﬁnition 5.8. Missing completely at random.
Missing responses are missing completely at random (MCAR) if, for all x
and ψ,
p(r | y, x, ψ) = p(r | x, ψ);
i.e., if p(r | y, x, ψ) is a constant function of y.
2
One implication of MCAR is that missingness can be fully explained by
covariates X that are included in the full-data model. Another is that the
full-data distribution can be factored as
p(y, r | x, ω) = p(y | x, ω) p(r | x, ω),
meaning that the observed and missing response data have the same distribu-
tion, conditionally on X. Under MCAR, valid inference about the full-data
response distribution can be based solely on those with complete response
data. Even though this is a valid approach, it discards data on those without
complete response and therefore does not make optimal use of the available
data; the posterior variance of θ may be higher than necessary.
Example 5.3. Missing completely at random without covariates.
A longitudinal cohort study of school performance will record test scores each
year for 2 years of secondary school; these are Y1 and Y2. In the ﬁrst year,
1000 students are sampled and their test scores are recorded. In the second
year, budget constraints force the investigators to reduce the sample size to
800. Data on Y2 are collected for a random subsample of the original 1000
students. Because the subsample is randomly drawn, missing responses on Y2
are missing completely at random.
2
Example 5.4. Missing completely at random with covariates.
In the same study, suppose the investigators are interested in comparing test
scores between boys and girls, and let X denote gender. The distribution of
interest is p(y1, y2 | x, θ). Suppose further that the random subsample at time
2 oversamples girls, so that girls are more likely to have Y2 observed. Because
gender is a model covariate, the missing data on Y2 are missing completely at
random.
2
The MCAR assumption usually is not realistic for longitudinal studies be-
cause unplanned missingness is so common. Consider the ﬁrst CTQ study of

ASSUMPTIONS ABOUT MISSING DATA MECHANISM
93
smoking cessation described in Section 1.4, where Y = (Y1, . . . , Y12)T are the
cessation indicators measured weekly over the course of the study, X is the
binary indicator of treatment, and R = (R1, . . . , R12)T are the binary miss-
ingness indicators. The full-data model of interest is p(y | x, θ), which charac-
terizes the joint distribution of smoking outcomes conditionally on treatment
group X. The MCAR assumption states that p(r | yobs, ymis, x, ψ) = p(r |
x, ψ); in this context, it means that probability of nonresponse can depend on
treatment group, but within treatment group, nonresponse is completely in-
dependent of cessation outcomes. Under the plausible (and testable) scenario
that within treatment group, participants observed to be heavier smokers are
more likely to drop out, the MCAR assumption would not hold.
5.4.2 Missing at random (MAR)
A more realistic condition for many longitudinal studies is missing at random
(MAR), which requires that missingness is independent of missing responses
Y mis, conditionally on observed responses Y obs and model covariates X.
Deﬁnition 5.9. Missing at random.
Missing responses are missing at random (MAR) if, for all yobs, x and ψ,
p(r | yobs, ymis, x, ψ) = p(r | yobs, x, ψ);
i.e., if p(r | yobs, ymis, x, ψ) is a constant function of ymis.
2
The MAR assumption is one component of the ignorability condition, which
allows valid inference about θ to be based on the likelihood function for Y obs.
Ignorability is discussed in detail in Section 5.7.
Example 5.5. A missing at random mechanism.
Continuing with Examples 5.3 and 5.4, if those with lower values of the ﬁrst
test score Y1 are less likely to sit for the second test, then missingness in Y2
depends on Y1. If missingness further depends on X, but does not depend on
any other variable including Y1, then the missing data mechanism is MAR. 2
Application of the MAR condition for dropout mechanisms requires some
more development and is discussed in more detail in Section 5.5.
5.4.3 Missing not at random (MNAR)
Although the MAR assumption is fairly general in that it allows the missing
data mechanism (and sometimes the missing data itself) to be explained by
observables, there are cases where MAR may fail to hold. MAR will not be
valid, for example, when the probability of missingness depends on the value of
the missing response — or on other unobservables — even after conditioning
on observed data. We refer to mechanisms of this type as missing not at
random (MNAR).

94
MISSING DATA MECHANISMS
Deﬁnition 5.10. Missing not at random.
Missing responses are missing not at random (MNAR) if, for some ymis ̸=
y′
mis,
p(r | yobs, ymis, x, ψ) ̸= p(r | yobs, y′
mis, x, ψ);
i.e., if R depends on some part of Y mis, even after conditioning on Y obs
and X.
2
Example 5.6. Missing not at random mechanism: dropout in a smoking ces-
sation study.
Returning to the smoking cessation trial, dropout is an inevitable complica-
tion. Consider a simpliﬁed version where the full data consist of cessation
outcomes (Y1, Y2), treatment indicator X, and missing data indicator R. Un-
der MAR,
P(R = 1 | X, Y1, Y2 = 1) = P(R = 1 | X, Y1, Y2 = 0),
which implies that, conditional on treatment group and time-1 smoking sta-
tus Y1, the dropout probability is the same for both smokers and nonsmokers
at time 2. In smoking cessation studies, this assumption will not hold if the
dropout probability diﬀers by Y2 (or equivalently, if smoking status Y2 diﬀers
between dropouts and non-dropouts having the same (X, Y1) proﬁle). Licht-
enstein and Glasgow (1992) cite empirical evidence that in smoking cessation
studies, participants followed up after dropout are more likely than not to
have experienced relapse, raising the possibility that MAR is not plausible
here.
2
5.4.4 Auxiliary variables
Recall that the full-data response model is p(y | x, θ(ω)), and primary interest
is in parameters governing this distribution. We have implicitly assumed up
to this point that (Y , R, X) constitutes the only data that potentially are
available.
In many cases, however, investigators may have access to a set of auxil-
iary variables, denoted by V , that may be correlated with one or more of
(Y , R, X). Examples of auxiliary variables include covariates that are ob-
served but not of direct interest, or other longitudinal responses that are
measured concurrently with Y . In a clinical trial, the model covariates X
would include only treatment indicators and possibly stratiﬁcation variables;
however, other covariates might be available. In an HIV cohort study where
longitudinal CD4 counts are the response of interest, the investigator may also
have access to viral load measurements taken at the same time points.
If the primary responses are completely observed, the modeler generally
has little or nothing to gain by using these auxiliary variables to estimate

ASSUMPTIONS ABOUT MISSING DATA MECHANISM
95
covariate eﬀects or other parameters; however, when data are incomplete,
auxiliary variables can be helpful to the extent that they explain variation in
the joint distribution of Y and R. Missingness mechanisms can be deﬁned
conditionally on auxiliary variables as well.
Deﬁnition 5.11. Auxiliary variable MAR (A-MAR).
Missing responses are A-MAR if MAR holds conditionally on auxiliary vari-
ables V ; i.e., if, for all yobs, x, z, and ψ,
p(r | yobs, ymis, x, v, ψ) = p(r | yobs, x, v, ψ).
2
Clearly, A-MAR does not imply MAR. Consequently, when primary interest
is in the full-data response model p(y | x, θ) — which does not involve v
— it is necessary to specify a full-data model conditional on v and integrate
it out to obtain the unconditional model. Speciﬁcally, the full-data model of
(Y , R | X) is
p(y, r | x, ω) =

p(y, r, v | x, ω) dv.
(5.2)
Returning to the longitudinal HIV example, suppose Y is longitudinal CD4
count and V is longitudinal viral load, and that there is appreciable miss-
ingness in CD4. The MAR condition requires the analyst to assume that
conditional on observed CD4 history, missingness is unrelated to the CD4
count that would have been measured; this may be unrealistic. Further sup-
pose that the investigator can conﬁdently specify a joint model for CD4 count
and viral load (e.g., based on knowledge of disease progression dynamics), and
that viral load is thought to explain suﬃcient variability in CD4 count that
one could use it to predict missing responses. In other words, the analyst is
willing to assume A-MAR, or MAR conditional on viral load.
This approach requires specifying the joint distribution of (Y , R, V | X)
under the integral sign in (5.2). One possibility is to write
p(y, r, v | x, ω) = p(y, v | x, ω) p(r | y, v, x, ω),
where the ﬁrst factor is the joint model for CD4 and viral load, the second
is the missing data mechanism. As we will see in Chapter 7, incorporating
auxiliary covariates can sometimes be simpler than it would appear. Under
A-MAR (and assuming ignorability, see Section 5.7), the missing data mech-
anism can be left unspeciﬁed, although the joint model for Y and V does
have to be speciﬁed. Section 6.6 describes some speciﬁc models that can be
used for this purpose; in Section 7.5, we illustrate by analyzing data from the
CTQ II trial, where the responses are smoking cessation outcomes measured
weekly for 8 weeks, and the auxiliary process is individual weight, recorded
concurrently.

96
MISSING DATA MECHANISMS
5.5 Missing at random applied to dropout processes
The deﬁnitions of MCAR, MAR, and MNAR given in the previous section
were general. In this section we describe the implications of the MAR as-
sumption for handling monotone missing data patterns caused by dropout.
Although the MAR condition is rather simple to state and interpret for cross-
sectional data, it is somewhat less transparent when applied to longitudinal
data, particularly if missingness patterns are non-monotone. When missing-
ness follows a monotone pattern, as when dropout is the sole cause of missing
data, it turns out that MAR has an intuitive representation in terms of the
hazard of dropout, with obvious analogies to the cross-sectional setting.
Our discussion assumes a discrete set of measurement times within indi-
vidual, and assumes dropout happens at one of the scheduled times, t1, . . . , tJ.
Recall that the missingness indicator at time tj is Rj, with Rj = I(Yj observed).
Hence dropout time, denoted by U, is U = 1+
j Rj (see also Deﬁnition 5.4).
Further deﬁne, for each individual,
Y j = {Y1, . . . , Yj}
to be the subset of the full-data vector Y that would be observed up to and
including measurement time j (i.e., the response history up to and includ-
ing tj).
Under monotone dropout, the MAR condition states that
P(U = j | Y J) = P(U = j | Y j−1),
or that the marginal probability of dropping out at tj cannot depend on
measurements at or beyond tj (Molenberghs et al., 1998). In practical settings,
this version of MAR can be somewhat diﬃcult to interpret because it makes
reference to the marginal probability of dropping out at tj, P(U = j), rather
than the more intuitive hazard function, P(U = j | U ≥j). The marginal
dropout rate is the probability of dropping out at tj among all those who
begin follow-up, while the hazard rate is the probability of dropping out at tj
among those still eligible to be measured at tj.
At time tj and for any given response history Y k = {Y1, . . . , Yk}, let
h(tj | Y k) = P(U = j | U ≥j, Y k)
denote the hazard of dropout at tj conditional on Y k. It turns out that mono-
tone missingness caused by dropout is MAR if and only if the hazard of
dropout at tj depends only on observed response history Y j−1, and condition-
ally on Y j−1 is independent of present and future outcomes {Yj, Yj+1, . . . , YJ}.
Proposition 1. Under monotone dropout, missingness is MAR if and only
if for all j, the hazard of dropout at tj is independent of current and future
responses Yj, . . . , YJ conditionally on past responses Y1, . . . , Yj−1.
Proof: Recall that under MAR, P(U = j | Y J) = P(U = j | Y j−1). By deﬁni-

MAR AND DROPOUT
97
tion, the hazard can be written in terms of the marginal dropout probabilities
h(tj | Y k) =
P(U = j | Y k)
1 −j−1
l=1 P(U = l | Y k)
,
for any (j, k) such that 1 ≤j ≤J and 1 ≤k ≤J. Hence, under MAR, we
have
h(tj | Y J)
=
P(U = j | Y J)
1 −j−1
l=1 P(U = l | Y J)
mar
=
P(U = j | Y j−1)
1 −j−1
l=1 P(U = l | Y l−1)
=
function of Y1, . . . , Yj−1 only.
Conversely, suppose the hazard of dropout at tj given Y J depends only on
Y j−1; that is,
P(Rj = 0 | R1 = · · · = Rj−1 = 1, Y J)
= P(Rj = 0 | R1 = · · · = Rj−1 = 1, Y j−1).
To show this implies MAR, we need to demonstrate that P(U = j | Y J)
is a constant with respect to Yj, . . . , YJ, or equivalently, is a function of
Y1, . . . , Yj−1 only. Writing it in terms of the hazard, we have
P(U = j | Y J)
=
P(R1 = · · · = Rj−1 = 1, Rj = 0 | Y J)
=
P(Rj = 0 | R1 = · · · = Rj−1 = 1, Y J)
× P(Rj−1 = 1 | Rj−2 = · · · = R1 = 1, Y J)
× · · · × P(R1 = 1 | Y J)
=
h(tj | Y J)
j−1

k=1
{1 −h(tk | Y J)}.
(5.3)
But if h(tj | Y J) = h(tj | Y j−1), then the probability of dropout at j, given
by (5.3), reduces to
P(U = j | Y J)
=
h(tj | Y j−1)
j−1

k=1
{1 −h(tk | Y k−1)}
=
function of Y1, . . . , Yj−1 only.
2
This result is useful for making the MAR assumption more transparent;
dropout is really an event process, and it is more intuitive to think about
imposing conditions on hazard of dropout rather than on its marginal prob-
ability. It also is helpful in writing down the observed data likelihood under
MAR, which we discuss in Section 5.7

98
MISSING DATA MECHANISMS
In the foregoing, we have not discussed the role of covariates, and in par-
ticular time-varying covariates. These results will be expected to hold when
exogenous covariates are included in the full-data model; i.e., covariates meet-
ing the following conditions (see also Section 2.7):
1. Time-invariant covariates observed at baseline;
2. Time-varying covariates that are a deterministic function of baseline
value (e.g., age);
3. Stochastic time-varying covariates that are external to the response pro-
cess, and can be expected to be observed throughout the measurement
period (e.g., when air pollution level measured from a monitoring station
is a covariate, and health status of an individual is the response process;
the individual may drop out of the study, but air pollution will continue
to be measured.).
5.6 Observed data posterior of full-data parameters
Thus far, we have described processes that cause data to be missing, distin-
guished full from observed data, described the Rubin taxonomy for missing
data mechanisms, and showed how it applies to monotone missingness in-
duced by dropout. In particular we showed that in monotone patterns, the
MAR condition is equivalent to assuming the hazard of dropout at some time
tj depends only on observed response data up to and including time tj−1.
We are now ready to describe formal methods for drawing posterior inference
about full-data parameters from observed (incomplete) data.
When using incomplete data to draw posterior inference about aspects
of the full data, constraints or model speciﬁcations must be imposed on a
full-data model and on priors for parameters indexing that model. The full-
data model p(y, r | ω), including its assumptions and constraints, will encode
both the full-data response model p(y | θ(ω)) and the missing data mecha-
nism p(r | y, ψ(ω)). Because all components of ω may not be identiﬁed, the
prior distribution p(ω) will directly inform certain aspects of the posterior
distribution p(ω | yobs, r). We do not conﬁne ourselves in this discussion to
a particular factorization or speciﬁcation of the full-data model; the setup
described here applies to any full-data model. Posterior inference under a va-
riety of full-data model speciﬁcations is described and discussed throughout
the remainder of this chapter.
By deﬁnition, the posterior distribution of the full-data parameter ω, given
observed data (yobs, r), is
p(ω | yobs, r)
=
p(yobs, r | ω) p(ω)

p(yobs, r | ω) p(ω)dω
=
p(yobs, r | ω) p(ω)
p(yobs, r)
.
(5.4)

IGNORABILITY ASSUMPTION
99
(For clarity we will drop covariate dependence, but it is implied throughout.)
The observed-data likelihood, proportional in ω to p(yobs, r | ω), is obtained
by averaging the full-data model over all possible realizations of the missing
data,
L(ω | yobs, r)
∝
p(yobs, r | ω)
=

p(yobs, ymis, r | ω) dymis.
Because the marginal distribution p(yobs, r) in the denominator of (5.4) is a
constant with respect to ω, the observed-data posterior is proportional in ω
to the product of the full data prior p(ω) and the observed-data likelihood,
p(ω | yobs, r)
∝
p(ω) L(ω | yobs, r).
In the following sections we describe speciﬁc strategies for observed-data pos-
terior inference about full-data parameters. In general the missing data distri-
bution is not identiﬁable, and assumptions are needed for the full-data model
and informative priors need to be speciﬁed. When missingness is MAR, the
ignorability assumption provides considerable simpliﬁcation of the observed-
data posterior. We deﬁne this assumption in the next section.
5.7 The ignorability assumption
5.7.1 Likelihood and posterior under ignorability
The ignorability condition, ﬁrst described by Rubin (1976), can be used to
facilitate posterior inference without having to specify the missing data mech-
anism. An ignorable missing data mechanism meets three conditions, given
here in its deﬁnition.
Deﬁnition 5.12. Ignorable missing data mechanism (Little and Rubin, 2002).
A missing data mechanism is said to be ignorable for the purposes of posterior
inference if
1. The missing data mechanism is MAR.
2. The full data parameter ω can be decomposed as ω = (θ, ψ), where θ
indexes the full-data response model p(y | θ) and ψ indexes the missing
data mechanism p(r | y, ψ).
3. The parameters θ and ψ are a priori independent; i.e.,
p(θ, ψ) = p(θ)p(ψ).
2
An important consequence of ignorability is that inference about θ — typ-
ically of direct interest — can be based on a likelihood function that is pro-
portional in θ to the observed-data response model,
p(yobs | θ) =

p(yobs, ymis | θ) dymis.

100
MISSING DATA MECHANISMS
This can be shown as follows. Recall that the observed data posterior for ω
is given by
p(ω | yobs, r)
∝
p(ω) L(ω | yobs, r).
(5.5)
By condition (2) of the ignorability assumption, we can write
L(ω | yobs, r) = L(θ, ψ | yobs, r).
Conditions (1) and (3) — MAR and prior independence between θ and ψ —
lead to the further simpliﬁcation
L(θ, ψ | yobs, r)
∝
p(r, yobs | θ, ψ)
=

p(r | yobs, ymis, ψ) p(yobs, ymis | θ) dymis
MAR
=
p(r | yobs, ψ)

p(yobs, ymis | θ) dymis
=
p(r | yobs, ψ) p(yobs | θ)
=
L1(ψ | r, yobs) L2(θ | yobs).
(5.6)
Conditions (2) and (3) from the deﬁnition of ignorability imply that p(ω) =
p(θ)p(ψ). Substituting the simpliﬁed likelihood (5.6) into the observed-data
posterior (5.5) and using the factored prior, we have
p(ω | yobs, r)
∝
p(ω) L(ω | yobs, r)
=
{p(ψ)L1(ψ | r, yobs)} {p(θ)L2(θ | yobs)}
(Little and Rubin, 2002). Because the posterior factors over ψ and θ, the
ignorability condition implies
p(θ | yobs) ∝p(θ) L2(θ | yobs),
(5.7)
which does not involve the part of the likelihood corresponding to p(r | y, ψ);
hence the missing data mechanism is ‘ignored’ for the purposes of drawing
posterior inference about θ. This should not be interpreted to mean that
observations with missing data can be ignored or discarded; what is meant by
‘ignoring’ the missing data mechanism is that p(r | y, ψ) does not have to be
speciﬁed (Laird, 1988).
We illustrate with a derivation for the case where J = 2. For those with
missing Y2, the observed data are (Y1, R = 0), and the contribution to the
likelihood is proportional to p(y1, r | θ, ψ), evaluated at r = 0. The expression
for this term must be derived by integrating over the distribution of missing
responses — in this case y2 — for the assumed full-data model,
p(y1, r | θ, ψ)
=

p(y1, y2 | θ) p(r | y1, y2, ψ) dy2.
(5.8)
If missingness in Y2 is MAR, then p(r | y1, y2, ψ) = p(r | y1, ψ) and (5.8)

IGNORABILITY ASSUMPTION
101
reduces to
p(y1, r | θ, ψ)
=

p(y1, y2 | θ) p(r | y1, ψ) dy2
=
p(r | y1, ψ)

p(y1, y2 | θ) dy2
=
p(r | y1, ψ)p(y1 | θ).
Hence the observed-data likelihood contribution for individual i factors over
ψ and θ as
L(θ, ψ | yi,obs, ri)
∝
{p(yi1, yi2 | θ)}ri {p(yi1 | θ)}(1−ri)
× p(ri | yi1, ψ)
=
L1(θ | yi,obs) L2(ψ | ri, yi1)
(5.9)
An additional implication of ignorability is that missing responses Y mis can
be imputed or extrapolated from observed data Y obs using only the full-data
response model (ignoring the missing data mechanism); this can be seen as
follows:
p(ymis | yobs, r, ω)
=
p(ymis, yobs, r | ω)
p(yobs, r | ω)
=
p(r | yobs, ymis, ψ) p(yobs, ymis | θ)
p(r | yobs, ψ) p(yobs | θ)
(5.10)
=
p(yobs, ymis | θ)
p(yobs | θ)
(5.11)
=
p(ymis | yobs, θ),
where (5.10) follows from condition (2) of ignorability, and (5.11) is a direct
consequence of MAR (condition (1) of ignorability). Applied to the simple
case of bivariate response data where Y1 is always observed but Y2 may be
missing, we see that
p(y2 | y1, r = 0, θ) = p(y2 | y1, r = 1, θ);
hence missing values of Y2 can be imputed from a model of p(y2 | y1, r = 1, θ)
(e.g., from a regression of Y2 on Y1 among those with complete data). This
consequence of the ignorability assumption ﬁgures prominently when using
data augmentation techniques for posterior inference under MAR (Chapter 6).
5.7.2 Factored likelihood with monotone ignorable missingness
When the missing data pattern is monotone — as in the previous example —
the method of factored likelihood can be used to simplify computations for

102
MISSING DATA MECHANISMS
posterior sampling based on observed-data likelihoods for longitudinal data
(Little and Rubin, 2002). The joint distribution of observed data in (5.9) can
be rewritten as
p(yi,obs, ri | θ, ψ)
=
p(yi2 | yi1, θ)ri p(yi1 | θ) p(ri | yi1, ψ).
If there exists an invertible mapping g(θ) = φ = (φ2|1, φ1) such that
p(y2, y1 | θ)
=
p(y2 | y1, φ2|1) p(y1 | φ1),
then (5.9) can be factored over the parameters φ2|1 and φ1 indexing ‘complete-
data’ likelihood terms,
L(φ, ψ | yi,obs, ri)
∝
p(yi2 | yi1, φ2|1)ri p(yi1 | φ1) p(ri | yi1, ψ)
=
L1(φ2|1 | yi,obs) L2(φ1 | yi,obs) L3(ψ | ri, yi1).
For distributions where this factorization and transformation of the parameter
space yields simple models for the conditional distributions (e.g., of Y2 given
Y1) and for priors speciﬁed directly on φ, posterior sampling is simpliﬁed
because each term of the likelihood behaves like a ‘complete-data’ model,
using observable data.
The general form of a factored likelihood for observed data under ignorable
monotone missingness is
L(φ | yi,obs) = p(yi1 | φ1)
J

j=2
p(yij | yi,j−1, . . . , yi1, φj|{1,...,j−1})rij,
(5.12)
where φj|{1,...,j−1} denotes parameters indexing the conditional distribution
of Y j given {Y1, . . . , Yj−1}.
For some models used in applied settings, generating a factored likelihood
is sometimes possible and sometimes not. The multivariate normal distribu-
tion without covariates can be factored easily, and is further illustrated in
Example 5.7. For other models, such as the marginalized transition model
in Example 5.8, or most models having time-constant covariate eﬀects, the
likelihood may not factor easily over separable components of φ.
5.7.3 The practical meaning of ‘ignorability’
Calling the missing data mechanism ‘ignorable’ can be deceptive; if we view
the process of inference as starting from a full-data model and moving forward
with a series of assumptions, then plainly we have not ‘ignored’ the missing
data mechanism at all; indeed, we have made explicit constraints on it by
placing the MAR assumption on p(r | y, ψ), by partitioning the parameter
space of ω into (θ, ψ), and by assuming a priori independence between θ
and ψ. It is possible to form qualitative justiﬁcations of these assumptions,
but they cannot be empirically veriﬁed, and therefore should not be viewed

FULL-DATA MODELS UNDER MAR
103
as benign. The term ‘ignorable’ means that once these assumptions and pa-
rameter constraints have been imposed, and assuming the full-data model has
been correctly speciﬁed, the functional form of the missing data mechanism
p(r | y, ψ) can be ignored when making inference about the full-data response
model parameter θ = θ(ω) from observed data.
5.8 Examples of full-data models under MAR
We are now ready to illustrate applications of MAR and ignorability in con-
structing models from some speciﬁc distributions. Here we focus on construc-
tion of the observed data likelihood, identiﬁcation of full-data parameters, and
the predictive distribution of Y mis given Y obs.
Example 5.7. Bivariate normal with missing Y2.
Consider a study, such as a pretest-posttest design, where measurements Y1
are taken at baseline and Y2 at follow-up. All individuals have their outcome
recorded at baseline, but some are missing at time 2. Further assume that
primary interest is in E(Y2), the mean of Y2.
The full data for each individual is (Y1, Y2, R), where R = 1 if Y2 is observed
and R = 0 if not. In general terms, the full-data model is p(y1, y2, r | ω).
If we wish to characterize the full-data response distribution p(y1, y2 | θ(ω))
using a bivariate normal model under ignorability, we must make the following
assumptions:
1. The full-data parameter ω can be decomposed as
p(y1, y2, r | ω)
=
p(y1, y2 | θ) p(r | y1, y2, ψ),
where ω = (θ, ψ);
2. The full-data response model p(y1, y2 | θ) is the N(µ, Σ) density func-
tion, where Σ = [{σij}] and θ = (µ1, µ2, σ11, σ12, σ22);
3. The missing data mechanism is MAR, which implies
p(r | y1, y2, ψ)
=
p(r | y1, ψ);
4. The prior distribution factors as p(θ, ψ) = p(θ) p(ψ).
Let σ = (σ11, σ12, σ22), so that θ = (µ, σ). From our bivariate example
(5.9), the observed-data likelihood contribution for individual i is
L(θ | yobs,i)
∝
p(yi1, yi2 | µ, σ)ri

p(yi1, y2 | µ, σ) dy2
(1−ri)
=
p(yi1, yi2 | µ, σ)ri p(yi1 | µ1, σ11)(1−ri).
(5.13)
A more convenient parameterization for posterior inference directly on the
observed data posterior uses a factored likelihood. If (Y1, Y2)T ∼N(µ, Σ),

104
MISSING DATA MECHANISMS
then
Y2 | Y1
∼
N(β0 + β1Y1, τ2)
Y1
∼
N(µ1, σ2
11).
Using standard results from the normal distribution, there exists an invertible
mapping g(θ) = φ such that φ = (β0, β1, τ, µ1, σ11) (see also Little and Rubin,
2002). The parameter of interest, µ2 = E(Y2), is simply a function of φ,
E(Y2) = EY1{E(Y2 | Y1)} = β0 + β1µ1.
The observed-data likelihood (5.13) is equivalent to
L(φ | yi,obs)
∝
p(yi2 | yi1, β0, β1, τ)ri p(yi1 | µ1, σ11).
This likelihood factors over separable components of φ; hence posterior com-
putations can be based directly on the observed data-posterior. Moreover, the
model is more naturally parameterized for sensitivity analysis, a topic that is
taken up in considerable detail in Chapters 9 and 10. In Section 10.2, we use
a trivariate normal model with similar parameterization to analyze data from
the Growth Hormone Study.
2
Posterior computations can also be based on the full-data model directly
using data augmentation (see Section 3.4.3), which is described in the setting
of ignorable missingness in Section 6.3. This approach is more convenient
when we cannot use the factored likelihood or priors are speciﬁed directly
on the full-data response model parameters. The following example shows
an instance where the missing data mechanism is ignorable but the factored
likelihood approach cannot be used because of speciﬁc parameter constraints.
Here, the eﬀect of covariates on the marginal mean is assumed constant over
time.
Example 5.8. Marginalized transition model under ignorability.
For longitudinal responses Y1, . . . , YJ and covariates Xj, the full-data MTM(1)
model assumes the marginal regression
g(πj) = xjβ,
where πj = E(Yj | xj). The full-data distribution is speciﬁed as
Y1 | x1
∼
Ber(π1)
Yj | Yj−1, xj
∼
Ber(φj)
j = 2, . . . , J,
where
h(φj)
=
∆ij + yj−1γ.
Hence θ = (β, γ). Given xj, β, and γ, the parameter ∆j = ∆(xj) is deter-
mined by the marginal regression constraints given in (2.8).
Under ignorability and monotone missingness, there is no need to specify

FULL-DATA MODELS UNDER MAR
105
p(r | y). To write the observed-data likelihood, let πi1(β) = g−1(xi1β), ∆ij =
∆(xij), and φij(∆ij, γ) = h−1(∆ij + yi,j−1γ). Then
L(β, γ | yi,obs)
=
p(yi1 | xi1, β)
J

j=2
p(yij | yi,j−1, xij, ∆ij, γ), (5.14)
where
p(yi1 | xi1, β)
=
{πi1(β)}yi1{1 −πi1(β)}1−yi1
p(yij | yi,j−1, xij, ∆ij, γ)
=
{φij(∆ij, γ)}yij {1 −φij(∆ij, γ)}1−yij.
Comparing (5.14) to the general version of the factored likelihood (5.12), we
see that the observed-data likelihood for this model does not factor over (β, γ)
because γ and β are common to each term under the product sign in (5.14)
through ∆. In Section 7.4, this model is used to analyze data from the ﬁrst
CTQ trial, using an ignorability assumption.
2
Although MAR is needed for ignorability, not all MAR mechanisms lead
to a model where the missing data is ignorable. This is particularly true for
mixture speciﬁcations, evident from this example.
Example 5.9. Nonignorability under MAR: mixture of bivariate normals.
The two-component mixture of bivariate normal distributions having common
variance is given by
(Y1, Y2)T | R = 1
∼
N(µ(1), Σ)
(Y1, Y2)T | R = 0
∼
N(µ(0), Σ)
R
∼
Ber(φ),
where Σ has elements σ = (σ11, σ12, σ22); hence ω = (µ(0), µ(1), σ, φ). In this
model, the components are deﬁned by whether Y2 is observed (R = 1). The
missing data mechanism is the discriminant function (e.g., Anderson, 1984,
Chapter 6)
log
P(R = 1 | y, ω)
P(R = 0 | y, ω)

= ψ0 + ψ1y1 + ψ2y2,
where
ψ0
=
log
φ
1 −φ + (µ(0))TΣ−1µ(0) −(µ(1))TΣ−1µ(1)
ψ1
=
∆1σ22 −∆2σ12
σ11σ22 −σ2
12
(5.15)
ψ2
=
∆2σ11 −∆1σ12
σ11σ22 −σ2
12
,
(5.16)
with ∆j = µ(1)
j
−µ(0)
j
denoting between-pattern diﬀerence in means at mea-
surement time j. It can be shown that when the pattern-speciﬁc variances are

106
MISSING DATA MECHANISMS
equal, MAR holds if and only if
E(Y2 | Y1, R = 0, ω) = E(Y2 | Y1, R = 1, ω)
for all possible realizations of Y1; therefore, in the mixture of normals model,
MAR holds if and only if
µ(0)
2
−σ12
σ22
µ(0)
1
+ σ12
σ22
Y1
=
µ(1)
2
−σ12
σ22
µ(1)
1
+ σ12
σ22
Y1
for all Y1. Hence MAR holds if and only if
∆2 = σ12
σ11
∆1.
From (5.15) and (5.16) we see that MAR corresponds to ψ1 = ∆1/σ11 and
ψ2 = 0; however, ∆1 and σ11 index both the missing data mechanism (through
ψ1) and the full-data response model
p(y | ω)
=
φ p(y | r = 1, µ(1), σ) + (1 −φ) p(y | r = 0, µ(0), σ),
violating condition (2) in the deﬁnition of ignorability (Deﬁnition 5.12). Hence
the missing data mechanism is MAR but not ignorable.
2
5.9 Full-data models under MNAR
The steps involved in drawing inference from incomplete data are speciﬁcation
of a full data model, speciﬁcation of the priors, and then sampling from the
posterior distribution of full-data parameters, given the observed data Y obs,
R and X. In Section 5.7 we showed the form of the posterior distribution
for the full-data response parameter θ when the missing data mechanism is
ignorable.
As we saw in the previous section, identiﬁcation of a full-data response
model — particularly the part involving Y mis — requires making unveriﬁable
assumptions about the full-data model f(y, r | x, ω). In the previous section,
we relied on the ignorability assumption to identify the full-data response
model f(y | x, θ).
When ignorability is believed not to be a suitable assumption, one can use
a more general class of models that allows missing data indicators to depend
on missing responses themselves. Essentially these models allow one to param-
eterize the conditional dependence between R and Y mis, given Y obs and X.
Without the beneﬁt of untestable assumptions, this association structure can-
not be identiﬁed from observed data for the simple reason that Y mis cannot
be observed. Inference therefore depends on some combination of (a) unver-
iﬁable parametric assumptions and (b) informative prior distributions. This
can be viewed either as a strength or a weakness. The goal of this section is to
illustrate, using some simple examples, the construction, identiﬁcation, and
practical utility of some commonly used ‘nonignorable’ models.

FULL-DATA MODELS UNDER MNAR
107
5.9.1 Selection models
The selection model (SM) approach factors the full-data distribution as
p(y, r | x, ω) = p(y | x, ω)p(r | y, x, ω),
so that the full-data response model p(y | x, ω) and the missing data mech-
anism p(r | y, x, ω) must be speciﬁed by the analyst. We have written the
model so that both factors are indexed by a common parameter ω; it is com-
mon but not necessary to assume that ω can be decomposed as (θ, ψ), sepa-
rate parameters for each factor.
Selection models can be attractive for several reasons. First, the analyst is
usually interested in the full-data response distribution p(y | x), which in the
SM is speciﬁed directly. Second, the SM factorization appeals to the missing
data taxonomy described in Section 5.3, enabling easy characterization of the
missing data mechanism. Third, when the missing data pattern is monotone,
the missing data mechanism can be formulated as a hazard function, where
hazard of dropout at some time t can depend on parts of the full-data vec-
tor Y . Under monotone missingness, assumptions about the hazard function
translate directly into the MCAR-MAR-MNAR taxonomy.
Two potential downsides to selection models are their sensitivity to model
speciﬁcation and the sometimes opaque nature of identiﬁability conditions.
Several features of selection models can be illustrated using a simple example
involving the bivariate normal distribution.
Example 5.10. Selection model for bivariate normal data.
Consider a situation where Y1 is always observed but Y2 may be missing, and
as usual deﬁne R = I(Y2 observed). For simplicity we assume no covariates.
A selection model factors the full-data distribution as
p(y1, y2, r | ω) = p(y1, y2 | θ) p(r | y1, y2, ψ),
where we assume ω = (θ, ψ). Suppose we specify p(y1, y2 | θ) as a bivari-
ate normal density N(µ, Σ). The distribution p(r | y1, y2, ψ) is a Bernoulli
distribution with some regression structure for the mean; e.g.,
Ri | yi1, yi2 ∼Ber{πi(ψ)},
where
g(πi) = ψ0 + ψ1yi1 + ψ2yi2.
(5.17)
Setting g(·) equal to the inverse normal cdf Φ−1(·) yields the Heckman pro-
bit selection model (Heckman, 1976). For further details on this model, see
Section 8.3.3.
2
The model generalizes easily to longitudinal data by using a multivariate
normal distribution for Y i = (Yi1, . . . , YiJ)T and replacing πi in (5.17) with

108
MISSING DATA MECHANISMS
a discrete-time hazard function for dropout
h(tj | Y ij)
=
P(Rij = 0 | Ri,j−1 = 1, Yi1, . . . , Yij).
Diggle and Kenward (1994) use the logit function to model the discrete-time
hazard in terms of observed response history Y j−1 and the current but pos-
sibly missing Yij. Referring back to (5.17), if ψ2 = 0, we have MAR. If, in
addition, p(µ, Σ, ψ) = p(µ, Σ) p(ψ), then we have ignorability.
Even though the parameter ψ2 in (5.17) characterizes the association be-
tween R and the incompletely observed Y2, the parametric assumptions being
made in this example will identify ψ2, even in the absence of prior information;
that is, the observed-data likelihood is a function of ψ2.
The contribution of individual i to the observed-data likelihood for the
selection model in Example 5.10 can be written as
L(µ, Σ, ψ | yi,obs, ri)
∝
{p(yi1, yi2 | µ, Σ) π(yi1, yi2, ψ)}ri
×
*
p(yi1, y2 | µ, Σ) {1 −π(yi1, y2, ψ)} dy2
+(1−ri)
,
where π(y1, y2, ψ) = pr(R = 1 | y1, y2, ψ) = g−1(ψ0 + ψ1y1 + ψ2y2), For
common choices of g(·), such as probit and logit, the observed data likelihood
generally will be a function of the parameter ψ2.
Moreover, although the parameter ψ2 does not index the full-data response
model p(y1, y2 | µ, Σ), it does index the joint distribution of observables Y obs
and R, and in general is identiﬁed by observed data. This can be seen by
writing
p(yobs, r)
=

p(yobs, ymis, µ, Σ) p(r | yobs, ymis, ψ) dymis
=
function of (µ, Σ, ψ)
(see Heckman, 1976 for a complete treatment). Although there is nothing
counterintuitive about this — the distribution of the selected sample is a
function of the selection parameter — this property of parametric selection
models make them ill-suited to assessing sensitivity to assumptions about the
missing data mechanism. This point is taken up in more detail in Chapters 8
and 9.
To summarize, selection models represent a natural generalization from
ignorability to nonignorability by expanding p(r | yobs, ymis) to depend on
ymis. The structure of selection models appeals directly to the MAR-MNAR
taxonomy developed by Rubin (1976), and in some cases the models can even
be ﬁt using standard software. On the other hand, identiﬁcation of the missing
data distribution is accomplished primarily through parametric assumptions
about the full-data response model p(y | θ) and the explicit form of the
missing data mechanism (e.g., linear in y). For the simple case where the
full-data response model is bivariate normal and the selection model is linear

FULL-DATA MODELS UNDER MNAR
109
in y, the parameter ψ2 indexing association between R and the partially
observed Y2 is identiﬁable from observed data. This feature of the model
places considerable importance on assumptions that cannot be veriﬁed, and
has the potential to make sensitivity analysis problematic.
5.9.2 Mixture models
Mixture models represent another way to factor the full-data model so that
R depends on Y mis. In contrast to selection models, the MM approach uses
the factorization
p(y, r | x, ω) = p(y | r, x, ω) p(r | x, ω).
(5.18)
As with the selection model, it is sometimes useful to partition the parameter
space as ω = (α, φ), where α and φ, respectively, index the two factors in
(5.18). (Recall that the decomposition ω = (θ, φ) required for ignorability
implicitly refers to a selection model factorization of the full-data model, so
the partition ω = (α, φ) is not equivalent.) The full-data response model is a
mixture
p(y | x, α, φ) =
	
r∈R
p(y | r, x, α) p(r | x, φ).
The missing data mechanism can be derived using Bayes’ rule,
p(r | y, x, α, φ) = p(y | r, x, α) p(r | x, φ)
p(y | x, α, φ)
.
(5.19)
Despite its seeming intractability, in some circumstances the missing data
mechanism implied by a mixture model actually takes a closed form, as in the
mixture of normals model in Example 5.9.
One criticism of mixture models, not readily apparent when studying the
case of bivariate data with missingness in Y2 only, is that the hazard of miss-
ingness at time tj can depend not only on the potentially missing observation
Yj, but also on future measurements Yj+1, . . . , YJ. In one sense this is a reason-
able criticism because in a stochastic process it does not seem sensible for the
future to predict the past. However, the construction of mixture models asks
for speciﬁcation of the full-data distribution conditional on diﬀerent dropout
times or patterns. It seems entirely sensible, in considering the association
between dropout and response, to specify whether and how the distribution
of (say) the vector (Y1, Y2, Y3)T diﬀers between those who drop out after one
measurement and those who record complete follow-up. If desired, mixture
models can be constrained such that, conditionally on past responses, hazard
of dropout at time tj depends only on the past and current (but possibly
missing) response, and conditionally on those, does not depend on future re-
sponses (Kenward et al., 2003). Further discussion of this point is provided in
Section 8.4.2, and an illustrative comparison of mixture models where hazard

110
MISSING DATA MECHANISMS
of dropout does and does not depend on future observations is given in our
analysis of the Growth Hormone trial in Section 10.2.
From the point of view of modeling, mixture models treat dropout or miss-
ingness as a source of variation in the full data distribution. Specifying a
diﬀerent distribution for each dropout time or missing data pattern may seem
cumbersome, but it frequently has the advantage of making explicit the pa-
rameters that cannot be identiﬁed by observed data. This can be seen as
follows (again leaving aside covariates). Recall the mixture model speciﬁca-
tion is
p(yobs, ymis, r | ω)
=
p(ymis, yobs | r, α) p(r | φ).
Further decompose the ﬁrst term on the right-hand side as
p(yobs, ymis | r, α)
=
p(ymis | yobs, r, αE) p(yobs | r, αO),
(5.20)
where αE = λ1(α), αO = λ2(α) are functions of the mixture component
parameter α. The parameters αE and αO may overlap. The subscript E in-
dicates that αE indexes an extrapolation distribution; i.e., the distribution of
missing responses given the observables under the assumed full-data model.
The subscript O indicates that αO indexes a model for observables. In general,
αE cannot be fully identiﬁed from data alone.
Now suppose there exists a partition (αE:I, αE:NI) of αE such that the
observed-data likelihood is a function of αE:I but not of αE:NI (i.e., αE:I
is identiﬁed from data but αE:NI is not). A parameter αE:NI satisfying this
condition makes a suitable basis for formulating sensitivity analysis or for
encoding the conditional distribution of missing responses using informative
prior distributions; in general, mixture models lend themselves well to pa-
rameterizations having these properties. To illustrate, we revisit Example 5.9.
Example 5.11. Mixture of bivariate normals with missingness in Y2.
We generalize the speciﬁcation of Example 5.9 to allow separate variance
matrices by pattern of missingness; speciﬁcally let Y = (Y1, Y2)T, with
Y | R = 1
∼
N(µ(1), Σ(1))
Y | R = 0
∼
N(µ(0), Σ(0))
R
∼
Ber(φ),
where µ(r) = (µ(r)
1 , µ(r)
2 )T and Σ(r) has elements σ(r) = (σ(r)
11 , σ(r)
12 , σ(r)
22 ). For
pattern r, deﬁne β(r)
0 , β(r)
1 , and σ(r)
2|1 to be, respectively, the intercept, slope,
and residual variance for the regression of Y2 on Y1. Under this reparameter-
ization, the full-data model parameters are
α =

µ(r)
1 , σ(r)
11 , β(r)
0 , β(r)
1 , σ(r)
2|1 : r = 0, 1

.
The nonidentiﬁed parameters are seen to be those indexing the conditional

FULL-DATA MODELS UNDER MNAR
111
distribution of Y2 given Y1 among those with R = 0. Hence the distribution
p(yobs, ymis | r, α) can be parameterized as
p(yobs, ymis | r, α) = p(ymis | yobs, r, αE) p(yobs | r, αO).
(5.21)
To simplify notation, deﬁne pr(y) = p(y | r). Then the two factors of (5.21)
can be written as
pr(ymis | yobs, αE)
=

p0(y2 | y1, β(0), σ(0)
2|1)
1−r
pr(yobs | αO)
=

p0(y1 | µ(0)
1 , σ(0)
11 )
1−r
×

p1(y2 | y1, β(1), σ(1)
2|1) p1(y1 | µ(1)
1 , σ(1)
11 )
r
.
Hence αE can be partitioned as (αE:I, αE:NI), with
αE:I
=
∅
αE:NI
=
(β(0), σ(0)
2|1).
Furthermore,
αO = (µ(1), β(1), σ(1)
11 , µ(0)
0 , σ(0)
11 ).
It is a simple matter to show that the observed data likelihood contribution
for individual i does not depend on αE:NI,
L(αO, φ | yi,obs, ri)
∝

(1 −φ) p0(yi1 | µ(0)
1 , σ(0)
11 )
1−ri
×

φ p1(yi2 | yi1, β(1), σ(1)
2|1) p1(yi1 | µ(1)
1 , σ(1)
11 )
ri
.
In this model, it is possible to show that setting β(0) = β(1) and σ(0)
2|1 = σ(1)
2|1
yields MAR; hence, in general, a function that maps identiﬁed parameters and
sensitivity parameters to the space of nonidentiﬁed parameters can be used
to quantify departures from MAR.
For example, if we constrain αE:NI such that β(0)
1
= β(1)
1
and σ(0)
2|1 = σ(1)
2|1,
but let
β(0)
0
=
β(1)
0
+ ∆,
then assigning a point mass prior at ∆= 0 implies MAR. Fixing ∆at a
nonzero value implies MNAR. Formal priors on ∆also can be used. More
detail on this approach, including a general framework for model parameter-
ization and prior speciﬁcation, is given in Chapter 9. Illustrations using the
Growth Hormone trial and the OASIS study are provided in Chapter 10.
2
By contrast with the selection model for bivariate response in Example 5.10,
this observed-data likelihood is completely free of parameters indexing the
conditional distribution of Y mis given Y obs. For the purposes of posterior

112
MISSING DATA MECHANISMS
inference, information about this distribution will derive at least in part from
parameters that are informed solely by prior distributions.
To summarize, mixture models have the advantage that in many cases, one
can ﬁnd full-data parameters indexing the distribution of missing responses
that are not identiﬁed by observed data. In one sense, this makes inferences
about the full-data more transparent in that the source of information about
the posterior is clear. A potential downside of the models relates to practical
implementation: in the previous example, we conﬁne attention to a bivari-
ate distribution with no covariates, and even in this simple case there are
three unidentiﬁed parameters. As the dimension of Y increases, so will the
dimension of the unidentiﬁed parameter. In our case studies in Chapter 10,
we address this concern by illustrating sensible ways to reduce the dimension
of nonidentiﬁed parameters indexing the extrapolation distribution.
5.9.3 Shared parameter models
A third approach to specifying the full data distribution is to use an explicitly
multilevel formulation, frequently called a shared parameter model, where ran-
dom eﬀects b are modeled jointly with Y and R. In some cases the random
eﬀects can be used to explain the association or to capture multiple sources
of variation. Key papers tracing the development of these models include Wu
and Carroll (1988); Follmann and Wu (1995); Pulkstenis et al. (1998), and
Henderson et al. (2000). De Gruttola and Tu (1994), Wulfsohn and Tsiatis
(1997), and Faucett and Thomas (1996) use similar formulations but with the
objective of modeling a survival process as a function of stochastic longitudinal
covariates.
The general form of the full-data model using a shared parameter ap-
proach is
p(y, r | x, ω) =

p(y, r, b | x, ω) db.
(5.22)
Speciﬁc shared parameter models are formulated by making assumptions
about the joint distribution under the integral sign. Notice in (5.22) that
the full-data parameter ω also includes parameters indexing the distribution
of the random eﬀects.
Advantages to the SPM model include simpliﬁed speciﬁcation for the re-
sponse and missingness components. When Y is measured with error, SPMs
provide a useful way for missingness to depend on the error-free version of
Y , represented in terms of the random eﬀects (this is a type of ‘random-
coeﬃcient-dependent missingness’; see Wu and Carroll, 1988, and Little, 1995).
Another potential advantage is that through the use of random eﬀects, SPMs
can be used to handle high-dimensional or multilevel response data. A disad-
vantage is that except in simple settings, the underlying missing data mecha-

FULL-DATA MODELS UNDER MNAR
113
nism can be diﬃcult to understand and may not even have a closed form: the
representation of p(r | y, ω) requires integration over the random eﬀects.
Example 5.12. Random coeﬃcients selection model.
Wu and Carroll (1988) are concerned with estimating the population slope of
longitudinal lung function measurements using data from a study where the
dropout proportion was appreciable. They assume the lung function measures
follow a linear random eﬀects model
Y i | xi, bi ∼N(xiβ + wibi, Σi(φ)),
where wi ⊆xi are the random eﬀects covariates, with rows wij = (1, tij);
hence each individual has a random slope and intercept. The random eﬀects
bi = (bi1, bi2)T are assumed to follow a bivariate normal distribution
bi ∼N(0, Ω).
The hazard of dropout is Bernoulli, with
Rij | Ri,j−1 = 1, bi
∼
Ber(πij),
where hazard of dropout depends on the random eﬀects governing Y i via
g(πij) = ψ0 + ψ1bi1 + ψ2bi2.
The model is seen as a special case of (5.22) where the joint distribution
under the integral is factored as
p(y, r, b | x, ω)
=
p(r | y, b, x, ψ) p(y | b, x, β, φ) p(b | x, Ω)
=
p(r | b, ψ) p(y | b, x, β, φ) p(b | Ω),
with ω = (β, φ, Ω, ψ). The ﬁrst equality is just a factorization of the joint
distribution, and the second equality follows from the key assumption that
dropout R is independent of both Y obs and Y mis, conditionally on random
eﬀects. However, integrating over the random eﬀects induces dependence be-
tween R and Y mis conditionally on Y obs; hence the model characterizes an
MNAR mechanism.
2
The ‘conditional linear model’ (Wu and Bailey, 1989; Hogan and Laird,
1997a) also can be viewed as a version of an SPM, though its more standard
representation is in terms of a mixture model. The conditional linear model
specializes (5.22) to
p(y, r, b | x)
=
p(y | r, b, x) p(b | r, x) p(r | x).
As an example, Hogan and Laird (1997a) assume that mixture components
p(y | r, b, x), r ∈R in the ﬁrst factor follow random eﬀects models where
the regression coeﬃcients β depend on r, and the random eﬀects have a
distribution that does not depend on r; i.e., p(b | r, x) = p(b | x). Hogan
et al. (2004a) generalize the model to allow continuous dropout time, using

114
MISSING DATA MECHANISMS
a mixture of varying coeﬃcient models. For the case where p(y | r, b, x)
and p(b | r, x) follow normal distributions and E(Y | r, b, x) is linear in b,
integrating the random eﬀects yields a standard mixture-of-normals model for
the full-data distribution (as in Example 5.11).
5.10 Summary
In this chapter, we have provided a formal framework for conceptualizing infer-
ence from incomplete data, based on specifying a full-data model and drawing
inference from an observed-data likelihood (and associated priors). In order to
make clear the limits of drawing inference from incomplete data, we provided
formal deﬁnitions for missing data mechanisms and applied them to settings
with dropout. We then showed how to apply the notions of model speciﬁ-
cations and missing data assumptions in terms of selection models, mixture
models, and shared parameter models.
In the next chapter, we turn our focus to drawing inference about full-
data models for longitudinal responses under the ignorability assumption. For
longitudinal data, the key issue lies in specifying the dependence structure,
because the extrapolation model under ignorability, p(ymis | yobs), depends
critically on the dependence model.
5.11 Further reading
Models that accommodate dropout and intermittent missingness
Several papers have discussed approaches to handle dropout and intermittent
missingness simultaneously. Albert (2000) and Albert et al. (2002) constructed
models with a multinomial random variable speciﬁed at each time indicating
still in study, intermittent missing, or dropout. Lin, McCulloch, and Rosen-
heck (2004) deal with intermittent missingness by grouping the missing data
patterns using a latent pattern mixture approach. Troxel et al. (1998) use a
pseudo-likelihood approach that models the missingness indicator at time t
conditional on only the potentially missing response at time t.
Multiple cause dropout and dropout due to death
For a general discussion of multiple cause dropout, see Rotnitzky et al. (2001).
There have been several important papers recently that address the issues
involved with dropouts due to death. Frangakis and Rubin (2002) illustrate
the use of principal stratiﬁcation to estimate causal eﬀects of treatment in
the presence of death. Kurland and Heagerty (2005) address dropout due to
death in observational studies by directly modeling the mean conditional on
surviving past a speciﬁed time.

CHAPTER 6
Inference about Full-Data Parameters
under Ignorability
6.1 Overview
In this chapter, we discuss inference about the full-data distribution under
the ignorability assumption. Under the full-data model factorization
p(y, r, | x, ω)
=
p(y | x, θ(ω)) p(r | y, x, ψ(ω)),
(6.1)
missing at random implies
p(r | y, x, ψ(ω)) = p(r | yobs, x, ψ(ω)).
(6.2)
Ignorability arises under the additional restriction of a priori independence
between the parameters of the full-data response model θ and the parameters
of the missing data mechanism ψ. Under ignorability, we only need to specify
the full-data response model, not the missing data mechanism. In the following
we suppress the dependence on x to maintain clarity.
Recall that the full-data posterior of θ is p(θ | y) = p(θ | yobs, ymis). Under
ignorability, inference is based on the observed-data posterior,
p(θ | yobs)
=

p(θ | y)p(ymis | yobs) dymis
∝

p(yobs, ymis | θ)p(θ)dymis.
(6.3)
This integral, and hence p(θ | yobs), depends on the assumed full data response
model p(y | θ), where y = (yobs, ymis). Thus, valid inference depends on its
correct speciﬁcation. We must keep in mind that neither the full-data response
model nor the ignorability assumption itself is veriﬁable from the observed
data. In longitudinal data, one of the most important aspects in specifying a
full-data response model is the association (dependence) structure.
The main focus of this chapter is methods for posterior inference. Sampling
from the observed data posterior can often be simpliﬁed by supplementing the
Gibbs sampler with a data augmentation step. In particular, we can sample
from
1. p(ymis | yobs, θ) using a data augmentation step, and
2. p(θ | y) using Gibbs sampling.
115

116
INFERENCE UNDER MAR
The second step of this algorithm, which samples the full-data response model
parameters, is the same as if there were no missing data (Section 3.4). This
often simpliﬁes computations considerably. We provide more details on this
in Section 6.2.
The remainder of the chapter is organized as follows. First, issues of model
speciﬁcation are discussed, emphasizing the importance of correctly specifying
the dependence structure in p(y | θ). Second, the use of data augmentation to
simplify posterior sampling will be described in the setting of missing data and
its implementation in speciﬁc models shown. Third, we will illustrate modeling
of dependence by giving examples of convenient parsimonious structures and
computationally tractable ways to allow the dependence to be a function of
model covariates. Fourth, we will illustrate the use of joint modeling of several
processes as a way to incorporate information on auxiliary stochastic time-
varying covariates (recall Section 5.4.4 from Chapter 5 and Deﬁnition 5.11).
The chapter closes with some suggested approaches for assessing model ﬁt and
model checking. Throughout the chapter, we often refer the full-data response
model and posterior as the full-data model and posterior without any loss of
clarity.
6.2 General issues in model speciﬁcation
Inference under ignorability relies on correct speciﬁcation of the full-data re-
sponse model p(y | θ). It is convenient to view the the full-data response model
in terms of assumptions about the mean, variability and (serial) dependence,
and distribution. The following development focuses on the consequences of
mis-speciﬁcation of the dependence.
6.2.1 Mis-speciﬁcation of dependence
In longitudinal settings, when primary interest is in the mean, the dependence
is often viewed as a nuisance, with correct speciﬁcation only leading to gains
in eﬃciency. However, when dealing with incomplete data, correctly specifying
the dependence takes on added importance.
To illustrate the consequences of mis-speciﬁcation of dependence under
ignorability, we return to the bivariate normal model (Example 5.7) where
Yi1 is observed and Yi2 is possibly missing. Recall, Y i ∼N(µ, Σ), with the
j, k element of Σ denoted as σjk. To ﬁll in missing values of Yi2 within the
sampling algorithm, we sample from the (conditional) normal distribution
p(yi2 | yi1, µ, Σ), where θ = (µ, Σ). It has mean µ2 + φ21(yi1 −µ1) and is
a function of the covariance parameter through φ21 = σ21/σ11. By contrast,
with complete data, the data augmentation step is not necessary and the
posterior of µ2 is not a function of φ21.
The simplest mis-speciﬁcation would be to assume φ21 = 0 when, in truth,

ISSUES IN MODEL SPECIFICATION
117
φ21 ̸= 0. A less obvious, but important mis-speciﬁcation would be to assume
φ21 is constant when, in truth, φ21 is a function of model covariates via φ21 =
h(x) for some function h.
Of course, when J > 2, there are more dependence parameters and covari-
ance model simpliﬁcation is often necessary. For example, in a multivariate
normal model, when Σ is large, a common strategy is to assume a simpler,
parsimonious form for Σ; we might (incorrectly) assume an AR-1 form for Σ.
Or we might introduce structure via random eﬀects, but assume the wrong
model for the random eﬀects covariance matrix (Daniels and Zhao, 2003).
Several of these cases are explored in detail below. We start by examining
the consistency of the posterior distribution of the mean when the dependence
is mis-speciﬁed.
Example 6.1. Posterior distribution of the mean for a bivariate normal un-
der monotone missingness that is ignorable (continuation of Example 5.7).
Consider a bivariate response from a randomized controlled trial with binary
treatment and response vector Y = (Y1, Y2)T. Y1 is always observed, but Y2
may be missing. The missingness indicator R takes value 0 if Y2 is missing.
Conditional on a single scalar covariate x, we assume the full-data response
model is a bivariate normal distribution
Y | x, µ, Σ
∼
N(µ(x), Σ(x)),
with Σ(x) = {σjk(x)} and φ21(x) = σ21(x)/σ11(x). For j = 1, 2, deﬁne
µj(x)
=
E(Yj | x)
µC
j (x)
=
E(Yj | x, R = 1)
µD
j (x)
=
E(Yj | x, R = 0).
It can be shown that under ignorability,
µC
2 (x) −φ21(x) µC
1 (x)
=
µD
2 (x) −φ21(x) µD
1 (x)
=
µ2(x) −φ21(x) µ1(x).
(6.4)
We now examine the posterior distribution of µ2(x) under mis-speciﬁcation
of the dependence φ21(x); in particular, we incorrectly assume φ21(x) = φ21.
Deﬁne β = (β0, φ21)T to be the ordinary least squares estimates for regressing
Y2 on Y1 for those with R = 1 and X = x where φ21(x) is the slope. Assume
there are nx individuals with X = x.
Consider the conditional posterior mean of µ2(x) given {yobs, µ1(x)}; we
do not need to condition on µ1(x) here, but it will make the arguments more
clear. We examine the behavior of the posterior mean of µ2(x) as nx →∞(and
the dropout proportion remains constant); in particular, we will determine
whether the posterior distribution of µ2(x) is consistent (recall Deﬁnition 3.2).
Under ignorability and a correctly speciﬁed covariance, condition (6.4) can

118
INFERENCE UNDER MAR
be used to show that
E{µ2(x) | yobs, µ1(x)}
=
β0 + φ21µ1(x)
→
µC
2 (x) −φ21(x)µC
1 (x) + φ21(x)µ1(x)
=
µ2(x).
(6.5)
However, under the mis-speciﬁcation φ21(x) = 0,
E{µ2(x) | yobs, µ1(x)}
=
¯yC
2 (x)
→
µC
2 (x)
=
µ2(x) −φ21(x){µ1(x) −µC
1 (x)},
where
yC
2 (x)
=
1
mx
	
{i:xi=x,ri=1}
yi2
and mx is the number of subjects with {xi = x, ri = 1}. So, under mis-
speciﬁcation of the dependence structure, the posterior distribution of µ2(x)
will be inconsistent with bias
−φ21(x){µ1(x) −µC
1 (x)}.
Clearly, the bias increases with the magnitude of the dependence parame-
ter. The bias is also a function of the diﬀerence in the means between the
completers and dropouts at time 1.
Using similar arguments, bias in treatment eﬀect at time 2, µ2(1) −µ2(0),
is equal to
−φ21(1){µ1(1) −µC
1 (1)} + φ21(0){µ1(0) −µC
1 (0)}.
2
This example illustrates that even in the simplest case, a bivariate normal with
ignorable monotone missingness, mis-speciﬁcation of the dependence leads to
an inconsistent posterior for the mean parameters. Similar biases occur when
the mis-speciﬁcation is φ21(x) = φ21 ̸= 0. These results readily generalize
to higher-dimensional multivariate normals and other models discussed in
Chapter 2.
6.2.2 Orthogonal parameters
A more general way to understand the importance of correctly modeling the
dependence is by exploring the diﬀerence in the form of the information matrix
for the mean and dependence parameters for complete data vs. incomplete
data (under ignorability). Before going into the details, we ﬁrst introduce the
concept of orthogonal parameters. Let β represent the mean parameters, α
the dependence parameters, and let ℓ= log L.

ISSUES IN MODEL SPECIFICATION
119
Deﬁnition 6.1. Orthogonal parameters.
For parameters β and α, let β0 and α0 be their true values, respectively.
Then β and α are orthogonal (Cox and Reid, 1987) if, for any component βj
of β and any component αk of α,
Iβj,αk(β0, α0) = −E
∂2ℓ(β, α)
∂βj∂αk
,,,,β0,α0
= 0.
(6.6)
2
The block diagonality of the information matrix implies that the maximum
likelihood estimates of the orthogonal parameters are asymptotically indepen-
dent (Cox and Reid, 1987).
In the Bayesian paradigm, in addition to the full-data likelihood, we also
have a prior. Typically, the prior will be Op(1), whereas the likelihood is
Op(n). We now state a similar deﬁnition to Deﬁnition 6.1 for the Bayesian
setting.
Deﬁnition 6.2. Orthogonal parameters in Bayesian inference.
Consider parameters β and α with prior p(β, α), and let β0 and α0 be their
true values. Then β and α are orthogonal if, for any component of βj of β
and any component αk of α,
I⋆
βj,αk(β0, α0) = −E
*∂2{ℓ(β, α) + log(p(β, α))}
∂βj∂αk
+,,,,β0,α0
= 0.
(6.7)
2
For identiﬁed parameters, if condition (6.6) holds, then condition (6.7) holds
exactly under independent priors on β and α. If p(β, α) ̸= p(β)p(α), but
satisﬁes p(β, α) = Op(1), then condition (6.7) holds asymptotically.
When condition (6.7) holds, the joint posterior distribution of (β, α) is
equal to the product of the marginal posteriors of β and α (asymptotically).
For orthogonality involving non- or weakly identiﬁed parameters, the a priori
independence of β and α is necessary for orthogonality.
Orthogonality of the mean and dependence parameters is only a necessary
condition for the posterior distribution of the mean parameters to be consis-
tent if the dependence is mis-speciﬁed. A suﬃcient condition for the poste-
rior of the mean parameters to be consistent is given next. This condition is
stronger than orthogonality conditions given in Deﬁnitions 6.1 and 6.2.
Theorem 6.1. Consistency of the posterior for mean parameters.
Let β0 be the true value for the mean parameters and α⋆be any value for
the dependence parameters. If
Iβ,α(β0, α⋆) = −E
∂2ℓ(β, α)
∂β∂α
,,,,β0,α⋆= 0,
(6.8)
then the mle’s of the mean parameters are consistent even if the dependence
is mis-speciﬁed (Firth, 1987). The analogous condition for the consistency of

120
INFERENCE UNDER MAR
the posterior for the mean parameters in Bayesian inference replaces ℓ(β, α)
in (6.8) with ℓ(β, α) + log p(β, α).
2
We illustrate using a multivariate normal model.
Example 6.2. Information matrix based on observed data log likelihood un-
der ignorability with a multivariate normal model.
Assume Y i follows a multivariate normal distribution with mean Xiβ and
covariance matrix Σ(α). Let θ = (β, α) and assume that p(β, α) = p(β)p(α)
(a common assumption). For the case of complete data, it is easy to show
that the oﬀ-diagonal block of the information matrix, Iβ,α, is equal to zero
for all values of α, thereby satisfying condition (6.8). For Bayesian inference,
the posterior for β will be consistent even under mis-speciﬁcation of Σ(α).
However, under ignorability, the submatrix of the information matrix, now
based on the observed data log likelihood ℓobs (or observed data posterior)
and given by
Iobs
β,α(β, α) = −E
∂2ℓobs(β, α)
∂β∂αT

,
is no longer equal to zero even at the true value for Σ(α) (Little and Rubin,
2002). Hence the weaker parameter orthogonality condition given in Deﬁnition
6.2 does not even hold. As a result, in order for the posterior distribution of the
mean parameters to be consistent, the dependence structure must be correctly
speciﬁed.
This lack of orthogonality can be seen in the setting of a bivariate normal
linear regression, by making a simple analogy to univariate simple linear re-
gression. This will also provide some additional intuition into how inferences
change under missingness.
Suppose E(Y ) = µ, Ri = 1 for i = 1, . . . , n1 (yi2 observed), and Ri = 0 for
i = n1+1, . . ., n (yi2 missing). As in Chapter 5, we factor the joint distribution
of p(y1, y2) as p(y1)p(y2 | y1). For complete data, the conditional distribution
of Y2 given Y1 (ignoring priors for the time being) as a function of µ2 and
φ21 = σ12/σ11 is proportional to
exp
-
−
n
	
i=1
{yi2 −µ2 −φ21(yi1 −µ1)}2/2σ2|1
.
,
(6.9)
where σ2|1 = σ22 −σ21σ−1
11 σ12. Note that φ21 and µ2 do not appear in p(y1).
The orthogonality of µ2 and φ21 is apparent by recognizing (6.9) as the
same form as the log likelihood for a simple linear regression having a centered
covariate yi1 −µ1 with intercept µ2 and slope φ21. It can be shown from this
form that the element of the (expected) information matrix corresponding to
µ2 and φ21 is zero for all values of φ21. However, with missing data (under

DATA AUGMENTATION
121
MAR), the analogue of (6.9) is
exp
-
−
n1
	
i=1
{yi2 −µC
2 −φ21(yi1 −µC
1 )}2/2σ2|1
.
.
The sum is now only over the terms that correspond to Ri = 1.
Recall the mean of the completers at time j is µC
j = E(Yij | R = 1). Again
using the analogy to simple linear regression, µC
2 and φ21 are orthogonal,
but µ2 and φ21 are not. This is clear from the following, which holds under
ignorability:
µ2
=
µC
2 π + µD
2 (1 −π)
=
µC
2 −φ21(1 −π)(µC
1 −µD
1 ),
where π = P(Ri = 1). Thus, µ2 is a function of φ21. With no missing data,
π = 1 (so µ2 = µC
2 ). Under MCAR, (µC
1 −µD
1 ) = 0 and the second term
involving φ21 disappears.
2
Examples 6.1 and 6.2 demonstrate the importance of correctly specifying
Σ even when primary interest is in µ. Thus, if we model the covariance matrix
parsimoniously, we must be sure to consider whether Σ depends on covariates.
Of course, such modeling decisions are only veriﬁable from the data under the
ignorability assumption.
Similar results hold for directly speciﬁed models for binary data. It can
be shown that the information matrix for the mean parameters β and the
dependence parameters α in an MTM(1) (Example 2.5) satisﬁes condition
(6.8) under no missing data or MCAR (Heagerty, 2002) , but not under MAR.
There are also situations where misspeciﬁcation of dependence with com-
plete data can lead to biased estimates of the mean parameters. For example,
in marginalized transition model of order p (where p ≥2), β and α are not
orthogonal (Heagerty, 2002). Hence, the dependence structure must be cor-
rectly speciﬁed. Similar speciﬁcation issues with complete data are also seen
in conditionally speciﬁed models for binary data.
Of course, it is not possible to verify whether the dependence structure is
correct when data are incomplete. As a practical matter, when ignorability
is being assumed, it is recommended that the dependence model be selected
based on the model that is most suitable for the observed data.
6.3 Posterior sampling using data augmentation
Data augmentation is an important tool for full data inference in the pres-
ence of missing data; it is related to the EM algorithm (Dempster, Laird, and
Rubin, 1977) and its variations (van Dyk and Meng, 2001). As we illustrated
in Chapter 5 the general strategy is to specify a model and priors for the full
data and then to base posterior inference on the induced observed data pos-

122
INFERENCE UNDER MAR
terior, p(θ | yobs). However, the full-data posterior p(θ | y) is often easier to
sample than the observed-data posterior p(θ | yobs). Speciﬁcally, full condi-
tional distributions of the full-data posterior used in Gibbs sampling typically
have simpler forms than the full conditionals derived using the observed-data
posterior. This motivates augmenting the observed-data posterior with the
missing data ymis. We point out that if the model is speciﬁed directly for the
observed data, i.e, specify the observed data response model instead of the
full-data response model, and priors are put directly on the parameters of the
observed-data response model (see Example 5.7), then data augmentation is
not needed.
For data augmentation, at each iteration k of the sampling algorithm, we
sample (y(k)
mis, θ(k)) via
1. y(k)
mis ∼p(ymis | yobs, θ(k−1))
2. θ(k) ∼p(θ | yobs, y(k)
mis).
Thus, we can sample θ using the tools we described in Chapter 3 for the full-
data posterior, as if we had complete data. Implicitly, via Monte Carlo integra-
tion within the MCMC algorithm, we obtain a sample from the observed-data
posterior p(θ | yobs) given in (6.3).
Because data augmentation depends on sampling from p(ymis | yobs, θ), the
augmentation depends heavily on the within-subject dependence structure.
We illustrate by giving some examples of the data augmentation step for
several models from Chapter 2 under ignorable dropout.
Example 6.3. Data augmentation under ignorability with a multivariate nor-
mal model (continuation of Example 2.3).
Without loss of generality, deﬁne Y obs,i = (Yi1, . . . , YiJ⋆)T and Y mis,i =
(Yi,J⋆+1, . . . , YiJ)T with corresponding partitions of Xi and Σ given by
xi
=
/
xobs,i
xmis,i
0
Σ
=
/
Σobs
Σobs,mis
ΣT
obs,mis
Σmis
0
.
Within the sampling algorithm, the distribution of p(ymis,i | yobs,i, θ) takes
the form
Y mis,i | Y obs,i, θ
∼
N(µ⋆, Σ⋆),
where
µ⋆
=
xmis,iβ + B(Σ) (yobs,i −xobs,iβ)
Σ⋆
=
Σmis −ΣT
obs,mis Σ−1
obs Σobs,mis,
and B(Σ) = ΣT
obs,misΣ−1
obs. The dependence of Y mis,i on Y obs,i is governed

DATA AUGMENTATION
123
by B(Σ), the matrix of autoregressive coeﬃcients from regressing Y mis,i on
Y obs,i. Clearly, B(Σ) is a function of the full-data covariance matrix Σ.
2
Example 6.4. Data augmentation under ignorability with random eﬀects lo-
gistic regression (continuation of Example 2.2).
Again, we deﬁne Y mis,i and Y obs,i as in Example 6.3. The distribution p(ymis,i |
yobs,i, bi, θ) is a product of independent Bernoullis with probabilities
P(Ymis,ij = 1 | yobs,i, bi, θ) =
exp(xijβ + wijbi)
1 + exp(xijβ + wijbi), j ≥J⋆+ 1.
(6.10)
The dependence of these imputed values on the random eﬀects covariance
matrix Ωis evidenced by the presence of bi in (6.10). By integrating out bi
(as in Example 2.2), we have
p(ymis,i | yobs,i, θ)
=

p(ymis,i | yobs,i, bi, θ) p(bi | θ, yobs,i) dbi
=

p(ymis,i | bi, β) p(bi | β, Ω, yobs,i) dbi. (6.11)
The integral (6.11) is not available in closed form; however, recalling that the
population-averaged distribution can be approximated by
P(Ymis,ij = 1 | yobs,i, θ) ≈
exp(xijβ⋆)
1 + exp(xijβ⋆),
where β⋆≈βK(Ω) and K(Ω) is a constant that depends on Ω, the depen-
dence on the random eﬀects covariance matrix is clear.
2
Data augmentation and multiple imputation
Certain types of multiple imputation (Rubin, 1987) can be viewed as ap-
proximations to data augmentation. Bayesianly proper multiple imputation
(Schafer, 1997) provides an approximation to the fully Bayesian data augmen-
tation procedure in (6.3), which is based on the full-data response model; for
nonignorable missingness (Chapter 8), it would be based on the entire full-
data model. This approximation is computed by sampling just a few values,
say M, from p(ymis | yobs) (as opposed to full Monte Carlo integration). The
M sets of ymis are then used to create M full datasets that are analyzed using
full-data response log likelihoods, ℓ(θ | y) or full-data response model poste-
riors p(θ | y). Inferences are then appropriately adjusted for the uncertainty
in the missing values (Schafer, 1997).
Bayesianly ‘improper’ multiple imputation would sample M values from
some distribution, say p⋆(ymis | yobs), where
p⋆(ymis | yobs) ̸= p(ymis | yobs).
This might be implemented when the imputation model is speciﬁed and ﬁt

124
INFERENCE UNDER MAR
separately from the full-data response model (Rubin, 1987) or when auxiliary
covariates V are being used under an MAR assumption.
6.4 Covariance structures for univariate longitudinal processes
In Examples 6.1 and 6.2, we showed the importance of covariance speciﬁca-
tion in incomplete data. We now describe a number of speciﬁc approaches to
accomplish this. For multivariate normal models where the dimension of Y i
is large relative to the sample size, it is common to assume a parsimonious
structure for Σ to avoid having to estimate a large number of parameters.
We discuss two classes of models that are computationally convenient to do
this. For the ﬁrst class, we directly specify the covariance structure. For the
second, we specify the covariance structure indirectly via random eﬀects.
6.4.1 Serial correlation models
A natural parameterization on which to introduce structure for Σ in multivari-
ate normal models is via the parameters in the modiﬁed Cholesky decomposi-
tion (Pourahmadi, 1999). The parameters of this decomposition correspond to
the means and variances of the conditional distributions p(yj | y1, . . . , yj−1) :
j = 1, . . . , J,
E(Yj | y1, . . . , yj−1)
=
µj +
j−1
	
k=1
φjk(yk −µk),
(6.12)
var(Yj | y1, . . . , yj−1)
=
σ2
j .
(6.13)
The autoregressive coeﬃcients in (6.12),
{φjk : k = 1, . . . , j −1; j = 2, . . . , J},
are called generalized autoregressive parameters (GARP) and characterize the
dependence structure. The variance parameters in (6.13),
{σ2
j : j = 1, . . . , J},
are called the innovation variances (IV). A major advantage of these param-
eters is that the GARP are unconstrained regression coeﬃcients and the logs
of the innovation variances are also unconstrained, unlike the variance and
covariances {σjk} of Σ.
The GARP/IV parameters are also natural for characterizing missingness
due to dropout and for characterizing identifying restrictions given their con-
nections to the conditional distributions p(yj | y1, . . . , yj−1). This was brieﬂy
discussed in Example 5.11 and will be discussed in detail in Section 8.4.2 in
Chapter 8.
Before discussing a particular class of models based on the GARP and IV

COVARIANCE STRUCTURES
125
parameters, we review some approaches to explore the feasibility of diﬀerent
parsimonious structures based on these parameters.
Exploratory analysis of GARP/IV parameters
Exploratory model selection can be conducted by examining an unstructured
estimate of Σ and by examining regressograms (Pourahmadi, 1999), which
plot the GARP and IV parameters vs. both lag and time. We illustrate both
these approaches on the schizophrenia data (Section 1.2). To simplify this
demonstration, we ignore the fact that the lag between the last two measure-
ments was 2 weeks, not 1 week.
Table 6.1 Schizophrenia trial: GARP parameters from ﬁtting a multivariate normal
model. The elements in the matrix are φj,j+k.
Week (j)
Lag (k)
1
2
3
4
5
1
.81
.89
.85
.68
.80
2
–.07
.03
.30
.14
3
.17
–.10
.06
4
–.02
.03
5
–.04
Table 6.1 shows the estimated GARP parameters from ﬁtting a multivari-
ate normal model to the schizophrenia data. The lag-1 parameters (ﬁrst row)
are the largest and appear to characterize most of the dependence. We can also
view the GARP graphically using regressograms. Figure 6.1 shows a regresso-
gram of the lag-1 GARP as a function of week (corresponding to the ﬁrst row
of Table 6.1); there is little structure to exploit here other than potentially
assuming the lag-1 GARP are constant over week φj,j+1 = φ1.
Figure 6.2 is another regressogram showing the GARP as a function of lag
(for example, the ﬁrst column in the ﬁgure has a dot for each of the ﬁve lag-1
GARP given in Table 6.1). The GARP for lags greater than 2 seem small,
suggesting these parameters can be ﬁxed at zero. An alternative approach,
as suggested in Pourahmadi (1999) would be to model the GARP using a
polynomial in lag. Based on Figure 6.2, a quadratic might be adequate for
this data and would reduce the number of GARP parameters from ﬁfteen to
three. Note that this model implicitly assumes that for a given lag, the GARP
are constant.
Figure 6.3 shows the log of the IV as a function of weeks. Clearly, the

126
INFERENCE UNDER MAR
Figure 6.1 Schizophrenia trial: posterior means of lag-1 GARP with 95% credible
intervals as a function of weeks.
innovation variances are decreasing over time and a simple linear trend in
weeks would likely be adequate to model the IV, reducing the number of IV
parameters from six to two.
Structured GARP/IV models for Σ
Structured GARP/IV models (Pourahmadi and Daniels, 2002) specify linear
and log-linear models for the GARP and IV parameters, respectively, via
φjk =
vjkγ,
k = 1, . . . , j −1; j = 2, . . . , J
log(σ2
j ) =
djλ,
j = 1, . . . , J.
(6.14)
The design vector vjk can be speciﬁed as a smooth function in lag (j −k) (cf.
Figure 6.2) and/or a smooth function in j for a ﬁxed lag j −k (cf. Figure 6.1).
The design vector dj can be speciﬁed as a smooth function of j (cf. Figure
6.3). These smooth functions are typically chosen as low-order polynomials

COVARIANCE STRUCTURES
127
Figure 6.2 Schizophrenia trial: posterior means of GARP as a function of lag.
(or splines). Special cases of these models include setting φjk = φ⋆
|j−k| for all
j and k, i.e., constant within lag (stationary) GARP. A ﬁrst-order structure
would set φ⋆
|j−k| = 0 for |j −k| > 1.
Based on our exploratory analysis of the schizophrenia data, we might
specify a ﬁrst order lag structure for the GARP. As such, the design vector
for the GARP would be
vjk =

1
|j −k| = 1
0
otherwise
with γ representing the lag-1 regression coeﬃcient. We might specify a linear
model for the log IV, setting dj = (1, j)T.
A practical advantage of structured GARP/IV models is that they allow
simple computations since the GARP regression parameters γ have full con-
ditional distributions that are normal when the prior on γ is normal (see
Pourahmadi and Daniels, 2002). In general, ﬁtting these models in WinBUGS

128
INFERENCE UNDER MAR
0
1
2
3
4
5
6
3.8
4.0
4.2
4.4
4.6
4.8
5.0
Weeks
Log(IV)
Figure 6.3 Schizophrenia trial: posterior means of log of the IV as a function of
weeks.
can be diﬃcult (slow mixing) because WinBUGS does not recognize that
the full conditional distributions of the mean regression coeﬃcients and the
GARP regression parameters are both multivariate normal (see Pourahmadi
and Daniels, 2002). However, for some simple cases, these can be ﬁt eﬃciently
in WinBUGS. In Chapter 7, we explore models based on these parameters
further (including the computations) for the Growth Hormone trial (Section
1.3).
6.4.2 Covariance matrices induced by random eﬀects
Random eﬀects are another way to parsimoniously model the dependence
structure. Whereas the GARP/IV models parameterize the covariance matrix
directly, random eﬀects models induce structure on the covariance matrix
indirectly.

COVARIANCE STRUCTURES
129
Recall the normal random eﬀects model (Example 2.1),
Y i | xi, bi
∼
N(µb
i, Σb)
bi | Ω
∼
N(0, Ω),
with
µb
i = xiβ + wibi.
Here, we set Σb = σ2I. The marginal covariance structure for Y i, after inte-
grating out the random eﬀects, is
Σ = σ2I + wiΩwT
i
with j, k element
σjk = σ2I(j = k) + wijΩwT
ik,
where wij is the jth row of wi and dim(Ω) = q. The vector of covariance
parameters has dimension q(q + 1)/2 + 1 whereas the vector of parameters of
an unstructured Σ has dimension J(J + 1)/2. Typically, q ≪J.
For the schizophrenia clinical trial analysis in Chapter 7, we specify wij
using an orthogonal quadratic polynomial (q = 3). As such, we reduce the
number of covariance parameters from 21 to 7.
The random eﬀects structure is an alternative to the GARP/IV struc-
ture and can also be used to introduce structured dependence in general-
ized linear mixed models (cf. Example 2.2). It is possible to combine struc-
tured GARP/IV models with random eﬀects by decomposing Σ as Σ = Σb +
wiΩwT
i and modeling Σb using a parsimonious GARP/IV model (Pourah-
madi and Daniels, 2002).
6.4.3 Covariance functions for misaligned data
As discussed in Example 2.4, a structured covariance is typically required
to estimate the covariance function for misaligned temporal data. We review
some examples next.
For temporally misaligned longitudinal data, the covariance structure is
typically summarized via a covariance function,
cov(Yij, Yik|xi; φ) = C(tij, tik; xi, φ).
To draw inference about this function, simplifying assumptions (about the
process) or structure (on the function itself) are necessary because it is often
the case that no (or few) replications are available for observations at certain
times or pairs of times. In the remaining development, we drop the dependence
on x in the covariance function for clarity.
A common assumption is (weak) stationarity, where the covariance func-
tion C(tij, tik; φ) is only a function of the diﬀerence between times, i.e.,

130
INFERENCE UNDER MAR
C(tij, tik; φ) = g(|tij −tik|; φ). An example is the exponential covariance
function (cf. Example 2.4) given by
C(tij, tik; σ2, φ) = σ2 exp{−φ|tij −tik|}.
The corresponding correlation function is exp(−φ|tij−tik|), which for φ > 0
decays exponentially with the lag between times. We use this covariance func-
tion to model the residual autocorrelation of CD4 trajectories in the HERS
data (described in Section 1.5) in Section 7.6.
An example of a nonstationary covariance function is the one based on an
integrated Ornstein-Uhlenbeck process (Taylor, Cumberland, and Sy, 1994),
which takes the form
C(tij, tik; σ2, α)
=
σ2
2α3 {2α min(tij, tik) + exp(−αtij)
+ exp(−αtik) −1 −exp(−α|tij −tik|)} .
This is a structured nonstationary covariance function because the covariance
depends on the lag |tij −tik| between the observation times, and the times
themselves. These covariance functions have been used for longitudinal CD4
counts (Taylor, Cumberland, and Sy, 1994).
6.5 Covariate-dependent covariance structures
The covariance structure can also depend on covariates. Recent work by Hea-
gerty and Kurland (2001) and Kurland and Heagerty (2004) has shown that
even for complete data, there can be bias in the mean regression coeﬃcients in
generalized linear mixed models if the random eﬀects variance depends on a
between-subject covariate and this dependence is not modeled. The problems
for incomplete data have already been documented.
6.5.1 Covariance/correlation matrices
A complication in allowing components of a covariance matrix to depend
on covariates is to ensure the resulting covariance matrices are positive def-
inite. Several parameterizations have been proposed that provide a new set
of parameters that are unconstrained, giving a natural parameterization on
which to introduce covariates. As we did to introduce structure, we use the
GARP/IV parameters of the modiﬁed Cholesky decomposition as a way to
introduce covariates. Other approaches are mentioned in Further Reading.

COVARIATE-DEPENDENT STRUCTURES
131
Modeling Σ as a function of covariates using the GARP/IV parameters
Recall that the GARP, {φijk}, are unconstrained, as are the log of the inno-
vation variances {log(σ2
ij)}. Covariates can therefore be introduced as
φijk
=
vijkγ,
k = 1, . . . , j −1, j = 2, . . . , J,
log(σ2
ij)
=
dijλ,
j = 1, . . . , J,
where vijk and dij are design matrices for the GARP and log innovation
variances, respectively. These design vectors contain covariates of interest.
The form of these models is the same as the structured GARP/IV models,
but now the design vectors are also indexed by i and include covariates.
We again illustrate this approach using the data from the schizophrenia
clinical trial (described in Section 1.2). The main covariate of interest in this
data was treatment, so we examine the GARP and IV parameters by treat-
ment. Figure 6.4 shows the posterior means of the log innovation variances
for each treatment. Within each treatment, the innovation variances do not
show much structure as a function of time. However, there appear to be some
large diﬀerences across treatments. For example, the innovation variances for
the high dose at weeks 4 and 6 are considerably higher than for the other
three treatments. These plots suggest that the innovation variances at weeks
4 and 6 could be modeled as a function of treatment.
Figure 6.5 shows the lag-1 GARP for each treatment, again plotted as a
function of time. Here, the lag-1 GARP at week 6 for the medium dose is
much smaller than for the other three treatments (with its credible interval
not overlapping with the standard dose treatment) and we could allow this
GARP to diﬀer by treatment.
This exploratory analysis suggests the covariance matrix for the schizophre-
nia data does depend on treatment, and the GARP/IV parameterization pro-
vides a parsimonious way to model this as the individual parameters can de-
pend (or not) on (a subset of) treatment groups and the resulting covariance
matrices will be guaranteed to be positive deﬁnite.
These GARP/IV models will be explored more formally for the Growth
Hormone data in Chapter 7. For a detailed application of these models, see
Pourahmadi and Daniels (2002).
The modiﬁed Cholesky parameterization has also been used to introduce
covariates into the random eﬀects covariance matrix in the normal random
eﬀects model (Example 2.1) (Daniels and Zhao, 2003) and could also be used
for the random eﬀects covariance matrix in generalized linear mixed models
(Example 2.2); however, there should be some implicit or explicit ordering of
the random eﬀects for this parameterization to be fully justiﬁed because the
parameterization is not invariant to the ordering of the components of bi. If the
components of bi were the coeﬃcients of orthogonal polynomials or regression
splines, there is an obvious ordering. For a detailed example of introducing

132
INFERENCE UNDER MAR
0
1
2
3
4
5
6
3.5
4.5
5.5
(a)
Weeks
Log(IV)
0
1
2
3
4
5
6
3.5
4.5
5.5
(b)
Weeks
Log(IV)
0
1
2
3
4
5
6
3.5
4.5
5.5
(c)
Weeks
Log(IV)
0
1
2
3
4
5
6
3.5
4.5
5.5
(d)
Weeks
Log(IV)
Figure 6.4 Schizophrenia trial: posterior means of log of the innovation variances
(as a function of week) with 95% credible intervals for each treatment: (a) low dose,
(b) medium dose, (c) high dose, (d) standard treatment.
covariates into the random eﬀects matrix, we refer the reader to Daniels and
Zhao (2003). In Section 7.3, we allow the random eﬀects covariance matrix in
the schizophrenia trial to diﬀer by treatment.
There are other less computationally friendly ways to introduce covari-
ates into a covariance/correlation matrix. In the setting of the multivari-
ate normal model, there has been work on modeling the logarithm of the
marginal variances while keeping the correlation matrix constant across co-
variates (Manly and Rayner, 1987; Barnard, McCulloch, and Meng, 2000;
Pourahmadi, Daniels, and Park, 2007).

COVARIATE-DEPENDENT STRUCTURES
133
0
1
2
3
4
5
6
0.0
0.6
1.2
(a)
Weeks
GARP
0
1
2
3
4
5
6
0.0
0.6
1.2
(b)
Weeks
GARP
0
1
2
3
4
5
6
0.0
0.6
1.2
(c)
Weeks
GARP
0
1
2
3
4
5
6
0.0
0.6
1.2
(d)
Weeks
GARP
Figure 6.5 Schizophrenia trial: posterior means of lag-1 GARP (as a function of
week) with 95% credible intervals for each treatment: (a) low dose, (b) medium dose,
(c) high dose, (d) standard treatment.
Modeling the correlation matrix as a function of covariates in the
multivariate probit model
In the setting of multivariate probit models, we can model the individual
correlations as a function of covariates. The typical approach is to individually
transform the correlations to R using Fisher’s z-transform
z(ρijk) = 1
2 log (1 −ρijk)
(1 + ρijk) = vijkγ
(Czado, 2000). This transformation of the individual correlations does not
guarantee the resulting correlation matrices are positive deﬁnite. Hence, the
vector of regression coeﬃcients for the correlations γ will be constrained;
within a Gibbs sampling algorithm, values of γ sampled corresponding to a

134
INFERENCE UNDER MAR
non-positive deﬁnite correlation matrix will have prior probability zero and
will be rejected within a Metropolis-Hastings algorithm.
Modeling the covariance function in terms of covariates
For temporally misaligned data, similar approaches can be applied to co-
variance/correlation functions. Consider the exponential covariance function
introduced in Example 2.4,
C(tij, tik; σ2, φ) = σ2 exp(−φ|tij −tik|).
A natural way to introduce covariates here is through log(σ2) and log(φ)
(assuming φ > 0) as these transformations provide a set of unconstrained
parameters to maintain positive deﬁniteness of C(·, ·; σ2, φ).
6.5.2 Dependence in longitudinal binary models
For longitudinal binary data models based on underlying normal latent vari-
ables (Example 2.6) or random eﬀects (Example 2.2), covariates can be intro-
duced into their corresponding correlation/covariance matrix as discussed in
the previous section. For (marginalized) transition models, covariates can be
introduced through the (unconstrained) Markov dependence parameters.
Recall a marginalized transition model of order one (MTM(1)) has (Markov)
dependence parameters γij, j = 2, . . . , J. The γij are (unconstrained) log odds
ratios that can be modeled as a function of covariates via
γij = vijα,
with few additional computations compared to models without covariates. For
higher-order MTMs, the dependence parameters corresponding to lags larger
than one also are unconstrained and can be similarly modeled.
These models are ﬁt to data from the CTQ I trial in Section 7.4.
6.6 Joint models for multivariate processes
Multiple longitudinal processes are typically modeled separately, despite po-
tential gains in eﬃciency from modeling them jointly, unless there is inter-
est speciﬁcally in the relationship between the multiple processes (Liu and
Daniels, 2007). In the case of incomplete data, there are additional reasons
to build and base inference on joint models. Joint modeling allows the use of
all available data, on both the process of interest and the other processes, to
‘impute’ the missing values on the process of interest. This can be especially
advantageous when (1) the processes are not all observed at the same times
and observed responses for other processes are ‘closer’ temporally than ob-
served responses from the same process and (2) the correlation between the

JOINT MODELS FOR MULTIVARIATE PROCESSES
135
processes is strong. By modeling the processes separately, we do not use the
information from the other processes to ﬁll in the missing values.
Our primary interest here in developing joint models will be to incorporate
auxiliary variables under the auxiliary variable MAR (A-MAR) assumption
(Deﬁnition 5.11), where missingness in Y is MAR after conditioning on the
observed responses yobs, model covariates X, and auxiliary covariates V . Let
V obs be the observed values of the auxiliary covariate process. Then A-MAR
corresponds to the following form of the missing data mechanism,
p(r | y, x, vobs) = p(r | yobs, x, vobs).
In Bayesian inference, to allow for A-MAR mechanisms, we need to specify
the joint distribution p(y, v | x) and then integrate out v
p(y | x) =

p(y, v | x)dv
(6.15)
to obtain the full data response model of interest; from a practical perspective,
joint models for which this integral can be obtained in closed form would be
preferred. When the auxiliary covariate(s) are time-varying, joint longitudinal
models can be constructed for the auxiliary process V and the process of
interest Y .
For the CTQ II example (Section 1.4), the A-MAR assumption implies
that probability of missingness of cessation status at time t is independent of
the unobserved cessation status at time t conditional on the previous weeks’
smoking responses and the previous weeks’ weight change responses (as op-
posed to conditional on just the previous weeks’ smoking responses). In a data
augmentation step, we draw Y mis from p(ymis | yobs, vobs), not p(ymis | yobs).
Hence, the A-MAR assumption is weaker than MAR to the extent that we can
correctly specify the distribution p(y, v | x). In the following, we review and
introduce some models for multivariate processes that are convenient when
one of the processes is of primary interest and the other process is an aux-
iliary covariate process (Carey and Rosner, 2001; Gueorgeiva and Agresti,
2001; O’Brien and Dunson, 2004; Liu and Daniels, 2007). In particular, the
models presented here lend themselves to the setting of auxiliary time-varying
covariates in that the full data response model of interest can be obtained in
closed form (cf. (6.15)). We ﬁt one of these models to the CTQ II smoking
cessation data in Section 7.5.
In the following examples, we denote Yij as the primary response of interest
and Vij as the auxiliary covariate process at time tij.
6.6.1 Continuous response and continuous auxiliary covariate
As a starting point, we assume both processes are (potentially) measured at
the same set of observation times (tij = tj). For continuous data, the multi-
variate normal distribution is an obvious choice for the joint distribution of the

136
INFERENCE UNDER MAR
longitudinal processes. We model the joint distribution of the two processes,
(Y i, V i), as
/
Y i
V i
0,,,,, xi ∼N(xiβ, Σ),
where
xi
=
/
xiy
xiv
0
Σ
=
/
Σyy
Σyv
ΣT
yv
Σvv
0
.
The extension of this model to more than one auxiliary covariate process is
obvious. The dimension of Σ can be quite large, so structure is often imposed
on Σ using latent variables (Roy and Lin, 2000) or by inducing structure
directly on Σ (Carey and Rosner, 2001). To illustrate, consider models for
CD4 where the auxiliary covariate is viral load.
Consider the following (serial correlation) structure for Σ (Carey and Ros-
ner, 2001),
cov(Yij, Yij′)
=
σ2
yγ
|tj−tj′ |θy
y
cov(Vij, Vij′)
=
σ2
vγ
|tj−tj′ |θv
v
cov(Yij, Vij′)
=
σyσvγ
|tj−tj′+1|θyv
yv
.
The ﬁrst two covariances correspond to within-process covariances for the
primary response (CD4) and the auxiliary covariate (viral load), and the third
corresponds to the between-process covariance. For each, it is assumed that
the correlation decreases for responses farther apart in time by restricting γy,
γv, and γyv to be in [0, 1). The parameter γyv determines the relationship
between CD4 and viral load; if γyv = 0, then they are independent processes.
Missing data is imputed in the data augmentation step by constructing
the appropriate conditional distributions from this multivariate normal model
with a structured covariance matrix. Clearly, this form of Σ implies that
observed responses closer in time to the missing responses will carry more
weight in the imputation and the magnitude of γyv will determine how much
information is used from the auxiliary covariates process to ﬁll in values for
the primary process.
Multivariate normal models are natural for handling time-varying continu-
ous (auxiliary) covariates as the conditional distribution of one process given
the other can be written down in closed form (as a linear regression) as well
as the marginal distribution of the primary process of interest using standard
multivariate normal results.

JOINT MODELS FOR MULTIVARIATE PROCESSES
137
The covariance structures described here also can be used for misaligned
measurement times by inducing a structured covariance function within a
Gaussian process.
6.6.2 Binary response and binary auxiliary covariate
Models for a binary response and binary auxiliary covariate can be speciﬁed
similarly to those described in Section 6.6.1. Let Yij = I{ZY
ij > 0} and Vij =
I{ZV
ij > 0} where ZY
i = (ZY
i1, . . . , ZY
iJ)T and ZV
i = (ZV
i1, . . . , ZV
iJ)T are latent
variables modeled as
/
ZY
i
ZV
i
0,,,,, xi ∼N(xiβ, Σ)
with
xi
=
/
xiy
xiv
0
Σ
=
/
Σyy
Σyv
ΣT
yv
Σvv
0
.
The covariance matrix Σ is a correlation matrix for identiﬁability (cf. the
multivariate probit model in Example 2.6). Models using latent normal for-
mulations are convenient for speciﬁcation of dependence and computations.
Recent work has also considered allowing the latent variables to follow a
multivariate t-distribution (O’Brien and Dunson, 2004). One reason for using
a t-distribution is that when the scale matrix and degrees of freedom are appro-
priately speciﬁed, the multivariate t-distribution approximates a multivariate
logistic model; that is, the marginal distribution of ZY
ij follows (approximately)
a logistic distribution, which in turn implies P(Yijk = 1 | xi) follows a logistic
regression model. Computations are as easy as the probit model given that
the multivariate t-distribution can be re-expressed as a gamma mixture of
multivariate normals (see Example 3.16).
Similar to the model described in Section 6.6.1, this class of models is well-
suited for handling time-varying auxiliary covariates as the conditional and
marginal distributions of the process of interest take a simple form due to the
underlying normal (or t-) latent structure.
We ﬁnish our discussion of joint modeling in the next section by introducing
a model for a binary primary process of interest and a continuous auxiliary
covariate (or vice versa). This is motivated by the CTQ II smoking cessation
data (Section 1.4) where the primary process of interest was smoking cessation
(binary) and the auxiliary covariate was weight change (continuous).

138
INFERENCE UNDER MAR
6.6.3 Binary response and continuous auxiliary covariate
A straightforward way to induce correlation between a continuous and a bi-
nary longitudinal process is to specify a joint multivariate normal distribution
for the continuous responses and the latent variables underlying the binary
process, with the restriction that the marginal covariance matrix correspond-
ing to the binary responses is a correlation matrix. This model has appeared
in various forms in the literature (Catalano and Ryan, 1992; Gueorgeiva and
Agresti, 2001; Liu and Daniels, 2007).
As in Section 6.6.2, Yij = I{Zij > 0} where Z is the vector of latent
variables underlying the vector of longitudinal binary responses Y , and (Z, V )
are jointly multivariate normal as follows:
/
Zi
V i
0,,,,, xi ∼N(xiβ, Σ),
where, as above,
xi
=
/
xiy
xiv
0
Σ
=
/
Σyy
Σyv
ΣT
yv
Σvv
0
,
with Σyy a correlation matrix. Notice that the marginal distribution of Zi is
multivariate normal and the marginal distribution of Y i follows a multivari-
ate probit model. The submatrix Σyv characterizes the dependence between
the continuous and binary longitudinal processes. If Σyv = 0, then the two
longitudinal processes are independent.
This model is used to address an auxiliary covariate (process) in the CTQ II
smoking cessation data in Section 7.5.
6.7 Model selection and model ﬁt under ignorability
We now propose some modiﬁcations of the techniques for model comparison
and for assessing model ﬁt, ﬁrst introduced in Chapter 3, that can be extended
to incomplete data settings (under ignorability). Before we discuss these mod-
iﬁcations, we remind the reader that we can only assess the ﬁt of the full-data
model to the observed data; the adequacy of the model for the missing data
cannot be ascertained. Diﬀerent full-data models that provide the same ﬁt to
the observed data should be indistinguishable when using sensible model se-
lection criteria even though they may make very diﬀerent assumptions about
the missing data.
Because the focus of this chapter has been on ignorable missingness, we

MODEL SELECTION AND MODEL FIT
139
assess the ﬁt of the model using only the full-data response model p(y | θ),
and not the full data model p(y, r | ω) because it was unnecessary to specify
p(r | yobs, ψ). For nonignorable mechanisms, we do need to use p(y, r | ω);
this will be presented in detail in Chapter 8.
6.7.1 Deviance information criterion (DIC)
With complete data, the DIC is constructed using the likelihood based on the
full-data response model. The complication in using the DIC with incomplete
data under ignorability is that we do not observe the full data. The literature
is sparse on recommendations for constructing the DIC from incomplete data.
We consider two constructions based on those used in Pourahmadi and Daniels
(2002), Ilk and Daniels (2007) and Celeux et al. (2007). The recommendations
in Celeux et al., however, may not generalize well to our setting as they deal
with missing data that are latent (e.g., in the context of mixture and random
eﬀects models). Latent data are of course fundamentally diﬀerent than missing
response data. First, we can never observe the latent data, but we could have
observed the missing response data. Second, the observed vector of missingness
indicators has no analog in random eﬀects and mixture models.
DIC based on the observed data likelihood
A ﬁrst, and perhaps most obvious construction based on the development
in Chapter 5 and Section 6.1 would be to construct the DIC based on the
observed data likelihood L(θ | yobs),
DICO = DICobs(yobs) = −4E{ℓ(θ | yobs)} + 2ℓ(θ | yobs),
where θ = E(θ | yobs). This approach has been applied by Pourahmadi and
Daniels (2002) and Ilk and Daniels (2007) and typically can be computed
directly in WinBUGS.
DIC based on the full-data likelihood
An alternative approach would be to construct the DIC based on the full-data
response model likelihood,
DICfull(yobs, ymis)
=
−4Eθ{ℓ(θ | yobs, ymis)} + 2ℓ{θ(ymis) | yobs, ymis},
where Eθ(·) is the expectation with respect to p(θ | y) and θ(ymis) =
E(θ | yobs, ymis). Since ymis is not observed, we take the expectation of
DICfull(yobs, ymis) with respect to the posterior predictive distribution p(ymis |

140
INFERENCE UNDER MAR
yobs) to obtain
DICF
=
DICfull(yobs)
=
Eymis{DICfull(yobs, ymis)}
=
−4Eymis
1
Eθ {ℓ(θ | yobs, ymis)}
2
+ 2Eymis
1
ℓ(θ(ymis) | yobs, ymis)
2
,
(6.16)
where Eymis(·) is the expectation with respect to p(ymis | yobs). Note that
we took a similar expectation with respect to p(ymis | yobs) to derive the
observed data posterior from the full data posterior in (6.3).
DICF cannot be computed directly in WinBUGS. To compute (6.16), we
use the samples from the data augmented Gibbs sampling algorithm; de-
note this sample as {y(m)
mis , θ(m) : m = 1, . . . , M}. The ﬁrst term in (6.16)
is computed by averaging ℓ(θ | yobs, ymis) over the sample {y(m)
mis , θ(m) : m =
1, . . . , M}. For the second term, we need to compute
θ(y(m)
mis ) = E(θ | yobs, y(m)
mis )
at each iteration m, and then average the entire term, ℓ{θ(ymis) | yobs, ymis},
over the sample {y(m)
mis : m = 1, . . . , M}.
A computational problem with this approach is that θ(y(m)
mis ) will typically
not be available in closed form. A straightforward, but computationally im-
practical approach would be to run a new Gibbs sampler for every value of
ymis sampled and compute θ(ymis) from these draws.
We recommend two alternative approaches that are more practical compu-
tationally. First, recall we have a sample {y(m)
mis , θ(m) : m = 1, . . . , M} from
p(ymis, θ | yobs). The ﬁrst approach is to reweight this sample to estimate
E(θ | yobs, y(m)
mis ) for all values y(m)
mis in the sample. In particular, we can use
the weighted average∗
θ(y(m)
mis ) = E(θ | yobs, y(m)
mis ) ≈
M
a=1 w(m)
a
θ(a)
M
a=1 w(m)
a
,
(6.17)
with weights w(m)
a
given by
w(m)
a
= p(θ(a), y(m)
mis , yobs)/p(y(m)
mis | yobs)
p(θ(a), y(a)
mis, yobs)
,
and p(θ(l), y(l)
mis, yobs) = p(yobs, y(l)
mis | θ)p(θ). The term p(y(m)
mis | yobs) can be
∗The approximate equalities in this section, signiﬁed by ≈, are due only to Monte Carlo
error.

MODEL SELECTION AND MODEL FIT
141
estimated by
p(y(m)
mis | yobs) ≈1
M
M
	
j=1
p(y(m)
mis | yobs, θ(j)),
where {θ(j) : j = 1, . . . , M} is the sample from p(θ | yobs). This approach does
not require re-running the sampling algorithm and we expect these weights
to be fairly stable.
A second computational approach would be to run one additional Gibbs
sampler for a ﬁxed value of ymis, say ymis = E(ymis | yobs); denote this sample
as {θ(l) : l = 1, . . . , L}. We can then use the simpler weights
w(m)
l
= p(θ(l), y(m)
mis, yobs)
p(θ(l), ymis, yobs)
,
in (6.17), which are available in closed form. The sum is taken over the second
sample, {θ(l) : l = 1, . . . , L}
Parts of DICF can be computed in WinBUGS while other parts need to be
computed in other software (like R).
Summary
Both DIC constructions need further exploration and comparison to deter-
mine their behavior in the setting of ignorable missingness. DICO removes
the missing data by averaging over the predictive distribution conditional
on θ,
p(yobs | θ) =

p(yobs, ymis | θ)dymis,
to obtain the observed data likelihood with which the DIC is then constructed.
Using the observed-data log likelihood in DICO is very similar to the AIC, a
frequentist model selection criterion.
On the other hand, DICF removes the missing data by averaging the DIC
based on the full-data model, DICfull(yobs, ymis) with respect to the posterior
predictive distribution of ymis, p(ymis | yobs). So, essentially, this form of
the DIC is a weighted average of the DIC based on the complete data with
weights equal to how likely these ‘completed’ datasets are under the model
and the observed data. That the full-data response model and its parameters
are typically of primary interest provides some support for this approach. For
the data examples in Chapter 7, we use DICO.
6.7.2 Posterior predictive checks
In Section 3.5.3, we discussed posterior predictive checks based on complete
data. Assessing model ﬁt for incomplete data using posterior predictive checks

142
INFERENCE UNDER MAR
can be based on statistics computed from replications of the observed data
or replications of completed datasets. A recent paper by Gelman et al. (2005)
advocates doing checks using completed (full-) data. The idea is to use the
value of ymis at each iteration of the data-augmented Gibbs sampler to create
an ‘observed complete’ dataset to then compare to a ‘replicated complete’
dataset. We provide more details below. In using this approach, we must keep
in mind that the ﬁt of the model is still only assessed to the observed data
since the missing data components of the observed datasets are ﬁlled in using
data augmentation (conditional on the model).
Basing the checks on completed datasets oﬀers several advantages. In prin-
ciple, as discussed throughout this chapter, interest most often is on the full-
data response model p(y | θ). Therefore, diagnostics on this model (as opposed
to the implied observed data model) will typically be of primary interest. As
such, the choices of test statistics discussed in the complete data setting in
Chapter 3 would be appropriate here. Second, model building under an as-
sumption of ignorability does not require explicit speciﬁcation of the missing
data mechanism, p(r | yobs, x, ψ). By using diagnostics based on completed
datasets, the replicated realizations of the full data vector y do not need to
diﬀerentiate the missing and observed components as speciﬁed by the response
indicator vector r; hence there is no need to specify or check p(r | yobs, x, ψ).
For these reasons, we conduct posterior predictive checks on the completed
datasets.
To assess the ﬁt of certain features of the model to the observed data,
appropriate test statistics need to be constructed. These test statistics have
the full data as their argument. For each MCMC sample, the test statistics
evaluated at the ‘observed’ data will have as their argument the observed
data yobs combined with the current realization of the posterior predictive
distribution of the missing data sampled in the data augmentation step. For
example, at iteration l, the full data would be (yobs, y(l)
mis), where within the
data augmented Gibbs sampler, we sample y(l)
mis from p(y(l)
mis | yobs, θ(l)). The
test statistics for the replicated data y(l)
rep will be sampled from the full-data
response model p(y(l)
rep | θ(l)) given the current value of the parameter at iter-
ation l, θ(l) (cf. posterior predictive distribution in (3.15)). This approach has
been implemented recently in several papers, including Ilk and Daniels (2007).
Choice of test statistics would be similar to those suggested in Chapter 3.
To be more speciﬁc, for some data summary T (·; θ) of interest, we compute
T (yobs, y(l)
mis; θ) and T (y(l)
rep; θ) at iteration l and compare the two realizations
at each iteration. The corresponding posterior predictive probability is de-
ﬁned as
p =

H(yobs, ymis, yrep, c; θ) p(yrep, ymis, θ | yobs) dθ dymis dyrep,
(6.18)

FURTHER READING
143
where
p(yrep, ymis, θ | yobs) = p(ymis | yobs, θ)p(yrep | θ)p(θ | yobs)
and
H(yobs, ymis, yrep, c; θ) = I

h(T (yobs, ymis; θ), T (yrep; θ)) > c

.
In Sections 7.2 and 7.5, we conduct checks for the growth hormone data and
CTQ II data, respectively.
For nonignorable missing data mechanisms, p(r | y, ω) is speciﬁed and is
therefore part of the full-data model. We discuss posterior predictive checks
for these models in Chapter 8.
6.8 Further reading
Model speciﬁcation: dependence
Daniels and Pourahmadi (2002) propose prior distributions to shrink toward
structured GARP/IV models. Structured antedependence models (SAD) are
an alternative class of models that can be used to introduce structure into
a covariance matrix (N´u˜nez-Ant´on and Zimmerman, 2000) and are similar
to structured GARP/IV models. The log matrix parameterization is an al-
ternative unconstrained reparameterization of a covariance matrix that can
be used to introduce covariates (Leonard and Hsu, 1992; Chiu, Leonard, and
Tsui, 1996). For correlation matrices, Daniels and Pourahmadi (2007) have
been exploring alternative parameterizations for both structure and covari-
ates.
Another line of research on parsimoniously modeling covariance matrices
lets the data itself ﬁnd a parsimonious structure. Wong et al. (2003) model de-
pendence parsimoniously (within the multivariate normal models in Example
2.3) by using Bayesian model averaging with priors that can ‘zero-out’ partial
correlations. Liu and Daniels (2007) use a similar approach in the context of
the joint model in Section 6.6.3. Liechty et al. (2004) develop Bayesian mod-
els to parsimoniously estimate a correlation matrix through the construction
of appropriate priors. Chen and Dunson (2003) re-parametrize the random
eﬀects covariance matrix in order to ‘zero-out’ random eﬀects variances. This
parametrization is related to the modiﬁed Cholesky decomposition (Pourah-
madi, 2007).
For misaligned measurement times in binary data, Su and Hogan (2007)
have developed continuous-time MTMs for longitudinal binary data that allow
the dependence to depend on covariates.

144
INFERENCE UNDER MAR
Joint models
For the covariance structure for multivariate continuous longitudinal data
with misaligned times, additional structures can be found in Sy et al. (1997),
Sammel et al. (1999), Henderson et al. (2000), and Ferrer and McArdle (2003).
Directly speciﬁed models for binary data that do not use an underlying
multivariate normal or multivariate t latent structure, but provide marginal
logistic regressions for each component of Y , Yijk, have been proposed. Fitz-
maurice and Laird (1993) introduce a very general model for multivariate
(longitudinal) data based on the canonical log-linear parameterization. How-
ever, given the parameterization of this model, it is diﬃcult to parsimoniously
exploit the longitudinal correlation using serial correlation structures. Ilk and
Daniels (2007) constructed a model speciﬁcally for multivariate longitudinal
data, building on earlier work by Heagerty (1999, 2002) mixing both transition
(Markov) and random eﬀects structures; computations are complex though an
eﬃcient MCMC algorithm is proposed and implemented.
Indirectly speciﬁed conditional models (via random eﬀects or latent classes)
are another approach for modeling (multivariate) longitudinal binary data.
Relevant work includes Bandeen-Roche et al. (1997), Ribaudo and Thompson
(2002), Dunson (2003), and Miglioretti (2003).
There are many other approaches for general mixed longitudinal data that
have been proposed in the literature. Models for a set of mixed longitudinal
outcomes (more than just one continuous and binary longitudinal process)
have been developed without using an underlying normal latent structure, at
the cost of more complex computations. Some of these models use latent vari-
ables or latent classes to connect the processes, e.g., see Sammel et al. (1997),
Dunson (2003), and Miglioretti (2003). Instead of using latent variables, Lam-
bert et al. (2002) developed models for mixed outcomes using copulas (Nelson,
1999). Other authors have used the general location model for mixtures of
categorical and continuous outcomes (Fitzmaurice and Laird, 1997; Liu and
Rubin, 1998), but not speciﬁcally in the setting of multivariate longitudinal
data.
We did not discuss joint modeling of longitudinal and time to event data.
However, there is an extensive literature on this topic, both in the context of
surrogate markers and for missing data where dropout is modeled as a time
to event process. See DeGruttola and Tu (1994), Faucett and Thomas (1996),
Wulfsohn and Tsiatis (1997), Wang and Taylor (2001), and Xu and Zeger
(2001), among others. For an early review of these methods, see Hogan and
Laird (1997b), and more recently, Tsiatis and Davidian (2004).

CHAPTER 7
Case Studies: Ignorable Missingness
7.1 Overview
In this chapter, we re-analyze several examples from Chapter 4 using all avail-
able data, under an ignorability assumption for the missing data. We also
analyze two additional examples. The ﬁrst uses the CTQ II smoking cessation
data (Section 1.4) to illustrate joint modeling and implementation of the aux-
iliary variable MAR assumption. The second uses the CD4 data from HERS
(Section 1.5) to illustrate modeling irregularly spaced longitudinal data with
missingness. In all these analyses we focus on the importance of correctly
modeling/specifying the dependence under MAR.
7.2 Structured covariance matrices: Growth Hormone study
7.2.1 Models
The notation and model used here are the same as in Section 4.2. The distri-
bution of quadriceps strength (QS), Y i = (Yi1, Yi2, Yi3)T, for treatment group
Zi = k is
Y i | Zi = k ∼N(µk, Σk),
(7.1)
with µk = (µ1k, µ2k, µ3k)T and Σk = Σ(ηk), with ηk containing the six
nonredundant parameters in the covariance matrix.
A convenient parameterization of the covariance matrix is based on the
GARP and IV parameters. To ﬁt these models in WinBUGS, we re-parameterize
the multivariate normal model as
Yi1 | Zi = k
∼
N(µ1k, σ2
1k)
Yi2 | yi1, Zi = k
∼
N(β0k + φ21,kyi1, σ2
2k)
Yi3 | yi1, yi2, Zi = k
∼
N(β1k + φ31,kyi1 + φ32,kyi2, σ2
3k),
where, for k = 1, . . . , 4,
φk = (φ21,k, φ31,k, φ32,k)
is the set of GARP for each treatment and σ2
k = (σ2
1k, σ2
2k, σ2
3k) are the IV
for each treatment. We set ηk = (φk, σ2
k). Using this re-parameterized model,
145

146
CASE STUDIES: IGNORABLE MISSINGNESS
the marginal means µk can be computed recursively as
µjk = βj−2,k +
j−1
	
l=1
φjl,kµjk, j = 2, 3.
7.2.2 Priors
Conditionally conjugate priors as described in Daniels and Pourahmadi (2002)
were used for the GARP,
φk
∼
N(0, 106I), k = 1, . . . , 4.
We used diﬀuse normal priors on the intercepts (β0k, β1k) and truncated uni-
forms on the square root of the IV parameters,
βjk
∼
N(0, 106I3),
j = 0, 1; k = 1, . . . , 4,
σjk
∼
U(0, 100),
j = 1, . . . , 3; k = 1, . . . , 4.
7.2.3 MCMC details
For all the models, we ran three chains, each with 55, 000 iterations with a
burn-in of 5000 iterations. The chains mixed very well with minimal autocor-
relation.
Table 7.1 Growth hormone trial: posterior mean of the GARP/IV parameters of the
covariance matrices by treatment. The entries on the main diagonal are the innova-
tion variances (IV) and below the main diagonal are the generalized autoregressive
parameters (GARP).
EG
G





697
.98
563
.45
.65
241










552
.90
176
.26
.61
91





EP
P





741
.89
199
.21
.59
82










622
.74
203
−.01
.78
154






GROWTH HORMONE STUDY
147
7.2.4 Model selection and ﬁt
We ﬁrst ﬁt a model with Σk distinct and unstructured for each treatment (k),
which we will call covariance model (1). The posterior means of the GARP/IV
parameters ηk for covariance model (1) are given in Table 7.1. Examination of
this table suggests most of the GARP/IV parameters do not vary substantially
across the treatments. As a result, we ﬁt two more parsimonious models:
covariance model (2) {ηk = η : k = 1, . . . , 4} (common Σ across treatments)
and covariance model (3),
(σ2
1k, φ31,k, φ32,k)
=
(σ2
1, φ31, φ32) k = 1, 2, 3, 4
(σ2
2k, σ2
3k)
=
(σ2
2, σ2
3)
k = 2, 3, 4
φ21,k
=
φ21
k = 1, 2, 3,
which allowed individual GARP/IV parameters to vary across (subsets) of
the treatments. Covariance model (3) was chosen based on our examination
of the estimates from covariance model (1) in Table 7.1. We use DICO with
θ = (µ1, β0, β1, φ21, φ31, φ32, {1/σ2
jk : j = 1, . . . , 3; k = 1, . . . , 4})
to compare the ﬁt of the models. These results, which appear in Table 7.2,
support covariance model (3).
We also conducted two posterior predictive checks based on the residuals
to assess the ﬁt of covariance model (3). The posterior predictive probability
based on the multivariate version of Pearson’s χ2 given in (3.19) was .65. The
one based on largest deviation in the empirical cdf’s, given in (3.22), was .40.
Both measures suggest the model ﬁts well.
Table 7.2 Growth hormone trial: DICO for the three covariance models.
Model
DIC
Dev(¯θ)
pD
(1)
2674
2599
37
(2)
2667
2631
18
(3)
2652
2609
21
7.2.5 Results and comparison with completers-only analysis
Posterior means and 95% credible intervals for the mean parameters in co-
variance model (3) are given in Table 7.3; the posterior means and credible
intervals were similar under the three covariance models (not shown). In com-
paring these results to the complete case results in Table 4.3, we point out

148
CASE STUDIES: IGNORABLE MISSINGNESS
Table 7.3 Growth hormone trial: posterior means and 95% credible intervals of the
mean parameters for each treatment assuming covariance model (3).
Month
Treatment
0
6
12
EG
69 (62, 77)
82 (71, 94)
81 (70, 92)
G
68 (61, 76)
66 (58, 74)
65 (58, 73)
EP
66 (58, 74)
81 (73, 90)
73 (65, 80)
P
65 (58, 73)
62 (55, 70)
63 (56, 70)
Table 7.4 Growth hormone trial: posterior probabilities that each of the pairwise
diﬀerences at month 12 is greater than zero under covariance model (3) and the
independence model.
Treatment Group Contrast
Model
EG-G
EG-EP
EG-P
G-EP
G-P
EP-P
(3)
.99
.90
1.00
.08
.68
.97
Indep.
1.00
.98
1.00
.07
.52
.94
a few interesting diﬀerences. For example, the credible intervals are narrower
under ignorability; this is expected as we are now using all the data. The diﬀer-
ence in the posterior mean of the 12-month mean in treatment EG illustrates
the potentially strong bias from the completers-only analysis; the posterior
mean was 88 (74, 103) in the completers-only analysis vs. 81 (70, 92) in the
MAR analysis). This is related to the fact that at baseline, there were large
diﬀerences between the mean of completers and the mean of all the subjects
(cf. Tables 4.3 and 7.3).
Posterior probabilities that each of the pairwise diﬀerences of the month 12
across the treatments are greater than zero are given in Table 7.4. The poste-
rior probabilities under ignorability showed diﬀerences from the completers-
only analysis (cf. the results in the caption of Figure 4.1). For example, the pos-
terior probabilities for the diﬀerence between treatment EG and EP changed
from .97 for the completers-only analysis to .90 for the MAR analysis. There
were also important diﬀerences between the three covariance models. For ex-

SCHIZOPHRENIA TRIAL
149
ample, in comparing covariance models (1) and (2) under ignorability, the
posterior probability for EG vs. EP was .90 and .80, respectively.
To further illustrate the importance of modeling dependence under MAR,
we ﬁt an independence model, Σ(ηk) = σ2
kI3. The DIC for this model was
2956 and the eﬀective number of parameters was 16. Based on the DIC, this
model ﬁt considerably worse than the dependence models (cf. Table 7.2).
The largest diﬀerence from the dependence models was seen in the month 12
mean for treatment EG. Under the independence model, this mean was 7 to 9
units higher than under the dependence models. This large diﬀerence was due
to the data augmentation step under the independence model not using the
observed values at month 0 and month 6 to ‘ﬁll-in’ the month 12 values for the
dropouts. This diﬀerence also led to more extreme (but incorrect) posterior
probabilities for the diﬀerences between month 12 means for treatment EG
and the other three treatments (see Table 7.4). It is also interesting to note
that the posterior distribution of the month 12 means under the independence
model are essentially equivalent to the posterior distribution under models
that allow dependence based on the completers only data.
7.2.6 Conclusions
The covariance model that allowed the individual GARP/IV parameters to
vary by treatment, covariance model (3), was the preferred model for this
analysis (as measured by the DIC) and also seemed to provide a good ﬁt
itself as measured by the posterior predictive checks. Mean QS at month 12
was signiﬁcantly higher on EG vs. the other three treatments, with posterior
probabilities ranging from .90 to 1.00. EP was signiﬁcantly higher than P
with a diﬀerence of 10 (ft.-lbs. of torque) and a posterior probability of .97.
An analysis of this data considering MNAR and sensitivity analysis can be
found in Chapter 10.
7.3 Normal random eﬀects model: Schizophrenia trial
7.3.1 Models and priors
The notation, models, and priors used here are the same as in Section 4.3.
Recall the goal of this 6-week trial is to compare the mean change in BPRS
scores (measure of schizophrenia severity) from baseline to week 6 between
the four treatment groups. For the full data Y i = (Yi1, . . . , Yi6)T, we assume
Y i | bi, Zi = k
∼
N(xi(βk + bi), σ2I)
bi | Ω(φk), Zi = k
∼
N(0, Ω(φk)),
where the jth row of xi, xij is an orthogonal quadratic polynomial and Zi ∈
{1, 2, 3, 4} is treatment group.

150
CASE STUDIES: IGNORABLE MISSINGNESS
7.3.2 MCMC details
For all the models, we ran four chains, each with 10, 000 iterations with a
burn-in of 100 iterations. The autocorrelation was negligible by lag 30.
Table 7.5 Schizophrenia trial: DICO for the two random eﬀects models under MAR.
Model
DIC
Dev(¯θ)
pD
MAR(1)
6407
6369
19
MAR(2)
6434
6365
34
7.3.3 Model selection
We ﬁt two separate covariance models. For model (1), we assumed the random
eﬀects covariance matrix was constant across the four treatments {φk = φ :
k = 1, . . . , 4}. For model (2), we allowed this matrix to be diﬀerent across
treatments.
We compared the ﬁt using DICO (based on the observed-data likelihood)
with the random eﬀects, bi integrated out. Based on the DIC (see Table 7.5),
the more parsimonious model with a common random eﬀects covariance ma-
trix provided the best ﬁt.
7.3.4 Results and comparison with completers-only analysis
Figure 7.1 plots the posterior means of the trajectories for each treatment over
the 6 weeks of the trial. Mean BPRS for the medium and high dose individuals
started to go back up by week 6, unlike with the completers-only analysis (cf.
Figure 4.2). This is because those dropping out were more likely to have been
doing poorly under their respective treatments, as evidenced in Figure 1.1.
Tables 7.6 and 7.7 are of primary interest for inference as they contain sum-
mary information from the posterior on the change from baseline to 6 weeks
for all four treatments and the diﬀerences in the change from baseline among
the treatments, respectively. All the changes from baseline were apparently
non-zero (credible intervals exclude zero) except for the low dose treatment.
The low dose arm under the best-ﬁtting MAR model exhibited the small-
est improvement with a posterior mean of −5 and a 95% credible interval of
(−10, 1).
Table 7.7 shows the diﬀerences between treatments in the change from
baseline with credible intervals. In the completers only analysis, the credible

CTQ I TRIAL
151
intervals for all these diﬀerences covered zero. However, under the MAR anal-
yses with the common random eﬀects covariance matrix, the credible intervals
for the diﬀerences between the low dose arm and the other three treatments
all had credible intervals that excluded zero.
Table 7.6 Schizophrenia trial: posterior mean changes from baseline to week 6 for all
four treatments. Results for random eﬀects model with Ωconstant across treatments
(model 1) and diﬀerent across treatments (model 2) under MAR using all available
data, and for model (1) using data on completers only.
Treatment Group
Model
Low
Medium
High
Standard
MAR (1)
–5
–14
–12
–12
(–10, 1)
(–19, –10)
(–16, –7)
(–16, –7)
MAR (2)
–4
–15
–12
–13
(–10, 4)
(–19, –10)
(–17, –6)
(–17, –8)
Comp (1)
–12
–16
–16
–17
(–17, –7)
(–20, –11)
(–21, –12)
(–21, –13)
7.3.5 Conclusions
There were non-zero diﬀerences in the change from baseline between the treat-
ments, unlike in the completers-only analysis. In particular, subjects on the
low dose arm did signiﬁcantly worse than the two other dose arms of the new
drug and the standard dose of halperidol. In addition, the point estimates and
corresponding estimates of uncertainty were very diﬀerent between the MAR
and completers-only analysis as expected. There were also small diﬀerences
between the two covariance models under MAR, e.g., for the low vs. high
dose comparison, the credible interval for model (2) covered zero, but not the
interval for model (1). We based inference on the random eﬀects model that
provided the best ﬁt to the observed data as measured by DICO.
7.4 Marginalized transition model: CTQ I trial
We revisit the CTQ I data (Section 1.4) using all available data under an
ignorability assumption. Recall that the outcomes in this study are binary
weekly quit status for weeks 1 to 12, and the objective is to compare time-

152
CASE STUDIES: IGNORABLE MISSINGNESS
Table 7.7 Schizophrenia trial: pairwise diﬀerences of the changes among the four
treatments under random eﬀects models (1) and (2) under MAR and random eﬀects
model (1) for completers only.
Treatment Group Contrast
Model
L vs. M
L vs. H
L vs. S
M vs. H
M vs. S
H vs. S
MAR (1)
10
7
7
–2
–2
1
(2 ,16)
(0, 14)
(1, 14)
(–9, 4)
(–9, 4)
(–6, 7)
MAR (2)
11
8
9
–3
–2
1
(3, 19)
(–1, 17)
(1, 17)
(–10, 4)
(–8, 4)
(–6, 8)
Comp (1)
4
4
5
1
1
1
(–3, 11)
(–3, 11)
(–2, 12)
(–6 ,7)
(–5, 8)
(–6, 7)
averaged treatment eﬀect from weeks 5 through 12 (where week 5 was the
quit week). Let Yij : j = 1, . . . , 12 denote quit status of woman i at week j.
Treatment is denoted by Xi = 1 (exercise) and Xi = 0 (wellness).
7.4.1 Models
We focus on marginalized transition models here and consider two models for
the marginal mean structure and two models for the serial dependence. Serial
correlation is modeled only for the weeks following the quit date (i.e., weeks
5 through 12). Deﬁne the corresponding indicator as Zj = I{j ≥5}.
For the marginal mean structure, we consider the following two formula-
tions. Mean model (1) assumes an unstructured mean from week 5 to 12, with
a constant treatment eﬀect β1:
logit(µij) = (1 −Zj)α + Zj(β0j + β1Xi).
Mean model (2) assumes a constant mean with a constant treatment eﬀect:
logit(µij) = (1 −Zj)α + Zj(β0 + β1Xi).
The serial correlation follows one of two models. Serial correlation model (1)
allows the serial dependence to diﬀer between week 5 and weeks 6–12 with
both parameters depending on treatment; i.e.,
logit(φij) = ∆ij + γi1I{j = 5}Yi,j−1 + γi2I{j > 5}Yi,j−1,
where γik = φ0k + φ1kxi, k = 1, 2. Serial correlation model (2) assumes the

CTQ I TRIAL
153
0
1
2
3
4
5
6
20
25
30
35
Weeks
BPRS Score
Standard
Low
Medium
High
Figure 7.1 Schizophrenia trial: posterior means of the trajectories for each of the
four treatments for the random eﬀects model (1) under MAR.
serial dependence is constant for weeks 5–12 and diﬀers by treatment,
logit(φij) = ∆ij + γiYi,j−1,
where γi = φ0 + φ1xi. For comparison, we also ﬁt the two marginal means
models under independence (φ = 0). The priors for (α, φ, β) are speciﬁed as
diﬀuse normal priors as in the completers-only analysis (Section 4.4).
7.4.2 MCMC details
We ran three chains each with 55,000 iterations with a burn-in of 5000. There
was minimal autocorrelation in the chains.
7.4.3 Model selection
We use DICO to compare the ﬁt of all the models. The results appear in
Table 7.8. The serial dependence models ﬁt considerably better than the two
independence models with the DIC decreasing by 50%. The best ﬁtting model
has the combination of the simpler mean model, mean model (2) (constant

154
CASE STUDIES: IGNORABLE MISSINGNESS
mean after week 4), and the more complex dependence model, serial model (1)
(serial dependence diﬀers between week 5 and weeks 6–12). We base inference
on this model.
Table 7.8 CTQ I: model selection summaries for the four MTM models.
Model
Mean
Dependence
DICO
Dev(θ)
pD
(1)
(1)
1288
1260
14
(1)
(2)
1296
1272
12
(2)
(1)
1285
1271
7
(2)
(2)
1293
1284
5
(1)
indep
2404
2384
10
(2)
indep
2394
2388
3
7.4.4 Results
The posterior means and credible intervals for the treatment eﬀect parameters
β1 appear in Table 7.9. The posterior mean for the best-ﬁtting model was .27
with a 95% credible interval of (−.18, .73).
Posterior estimates of the dependence parameters are given in Table 7.10.
The dependence parameter for weeks 6–12 was almost three times as large
as the dependence parameter for week 5 (compare φ01 vs. φ02). Treatment
did not appear to aﬀect the dependence parameters; both φ11 and φ12 had
credible intervals that covered zero.
Inference under the poorly ﬁtting independence models (cf. Table 7.8) had
treatment eﬀects 50% larger with credible intervals that exclude zero. Clearly,
ignoring the dependence here results in an incorrect determination of a treat-
ment eﬀect.
7.4.5 Conclusions
The eﬀect of exercise on smoking cessation was to lower the quit rate, but the
eﬀect (over wellness) was not signiﬁcant in our analysis under an ignorability
assumption. DICO indicated the best-ﬁtting model had a constant mean after
the quit date and a non-constant serial dependence structure after the quit

CTQ II TRIAL
155
Table 7.9 CTQ I: posterior mean and 95% credible interval for treatment eﬀect β1
across models.
Model
Mean
Dependence
Posterior Mean
95% C.I.
(1)
(1)
.31
(–.14, .76)
(1)
(2)
.34
(–.10, .78)
(2)
(1)
.27
(–.18, .73)
(2)
(2)
.31
(–.14, .74)
(1)
indep
.48
(.28, .68)
(2)
indep
.47
(.27, .68)
Table 7.10 CTQ I: posterior means and credible intervals for the dependence param-
eters φ from the best-ﬁtting model.
Parameter
Posterior Mean
95% C.I.
φ01
1.7
(.29, 3.4)
φ11
1.8
(–.85, 5.3)
φ02
4.7
(4.1, 5.2)
φ12
.69
(–.13, 1.5)
date. The importance of modeling the dependence was clear from comparing
the results to models ﬁt under independence.
7.5 Joint modeling with auxiliary variables: CTQ II trial
The objective of the following analysis is to illustrate joint modeling as an
approach to accommodate the auxiliary variable MAR assumption with a
stochastic time-varying auxiliary covariate. We illustrate this using the CTQ II
data (described in Section 1.4) where the primary response of interest is smok-
ing status and the auxiliary process is weight change. Weight change is gen-

156
CASE STUDIES: IGNORABLE MISSINGNESS
erally associated with smoking cessation and we use it here to implement the
auxiliary MAR (A-MAR) assumption (and to help ﬁll in the missing cessation
outcomes). The A-MAR assumption here says that the probability of dropout
at week j is independent of quit status at week j only after conditioning on
quit status at weeks 1 through j −1 and weight change at weeks 1 through
j −1.
We focus on outcomes after the quit date (week 3), so there are 6 weeks
of data (weeks 3–8). Dropout was high on both treatment arms, over 40%.
Let Y i = (Yi1, . . . , Yi6)T be the vector of cessation outcomes and V i =
(Vi1, . . . , Vi6)T be the vector of weight change outcomes.
7.5.1 Models
We use the joint model for binary and continuous longitudinal data introduced
in Example 6.6.3. Let Zi = (Zi1, . . . , Zi6)T be the vector of latent variables
underlying the longitudinal vector of binary quit status responses, Y i, with
components deﬁned such that Yij = I{Zij > 0}. The model is given by

Zi
V i


,,,,,,
xi
∼
N(xiβ, Σ)
where
Xi
=

Xiy
Xiv

,
Σ
=

Σyy
Σyv
ΣT
yv
Σvv

,
and Σyy is a correlation matrix. The design matrix Xy is speciﬁed as a sepa-
rate quadratic polynomial in time for each treatment and Xv is an unstruc-
tured mean over time for each treatment.
Following the approach in Liu and Daniels (2007), we re-parameterize Σ
in terms of (Σyy, Byv, Σ⋆
vv), where
Byv
=
ΣT
yvΣ−1
yy
Σ⋆
vv
=
Σvv −ByvΣyv.
The matrices Byv and Σ⋆
vv can be recognized as the regression coeﬃcients and
residual covariance matrix, respectively, from the regression of the auxiliary
variables V on the latent response variables Z; in fact,
V | Z ∼N(Xvβ + Byz(Z −Xyβ), Σ⋆
vv).
We ﬁt models assuming dependence between the two processes, Byv ̸= 0

CTQ II TRIAL
157
(or equivalently, Σyv ̸= 0), independence between the processes (Byv = 0),
and independence within and between processes (diagonal Σ).
7.5.2 Priors
The vector of regression coeﬃcients β is given an improper uniform prior.
Parameterizing Σ as (Σyy, Byv, Σ⋆
vv), we placed a (proper) uniform prior on
the correlation matrix Σyy, an (improper) uniform prior on the components
of the coeﬃcient matrix Bzy, and a ﬂat prior on Σ⋆
yy. See Liu and Daniels
(2007) for veriﬁcation that this speciﬁcation gives a proper posterior. However
this prior speciﬁcation is not allowed in WinBUGS.
7.5.3 Posterior sampling
The key to posterior sampling in this model is moving between two forms of
the likelihood in deriving the full conditional distributions. Given the standard
multivariate normal likelihood and the priors speciﬁed in the previous section,
the full conditional distribution for β can easily be derived as a multivariate
normal distribution. To derive the full conditional distributions of Byv and
(Σ⋆
vv)−1, which will be multivariate normal and Wishart, respectively, the
multivariate normal likelihood can be factored into p(v | z)p(z). For full
details on the forms and derivations of the full conditional distributions for
these models, see Liu and Daniels (2007).
7.5.4 Model selection and ﬁt
To compare the ﬁt of the models with diﬀerent assumptions on the covariance
structure, we use DICO. To compute the observed-data likelihood, which is
not available in closed form (cf. Example 3.5), we use Monte Carlo integra-
tion and re-weighting as discussed in Chapter 3. The joint model that allows
dependence between the smoking and weight gain process provides the best
ﬁt, with a DIC value of 2746; the model that assumed Byv = 0 had a DIC of
2843 and the model with Σ = I had a DIC of 3776.
We also conducted posterior predictive checks using the techniques outlined
in Section 6.6. In particular, at each iteration of the sampling algorithm, we
computed test statistics for each week (j) as the diﬀerences in quit rates
and weight changes, respectively, for the replicated full data at each iteration
and the completed dataset at each iteration using the observed data and the
missing data sampled during the data augmentation step. For example, at

158
CASE STUDIES: IGNORABLE MISSINGNESS
week j, for the cessation outcomes, we compute
h{T (yobs,j, ymis,j), T (yrep,j)}
=
	
{i:rij=1}
yobs,ij +
	
{i:rij=0}
ymis,ij
−
n
	
i=1
yrep,ij.
(7.2)
For weight change, replace y with v in (7.2).
These statistics were chosen to examine consistent over- or underestima-
tion of the mean for each process at each measurement time. The posterior
predictive probabilities were deﬁned as the probabilities that these diﬀerences
were bigger than zero. All the probabilities were between .2 and .6, providing
no evidence of lack of ﬁt.
7.5.5 Results
Table 7.11 shows the posterior means of the quit rates under the full joint
model (Byv ̸= 0), the model with Byv = 0 (independence between weight
change and smoking), and the one assuming Σ = I (complete independence).
As discussed earlier, we base inference on the model with Byv ̸= 0.
The reason for the small diﬀerences between the model that assumes Byv =
0 and the full joint model is the fact that the dependence here is much stronger
within each process than between the processes; for example, the correlations
between the quit process over time are all greater than .5, while they are
all less than .2 for the correlations between the quit and weight processes.
Clearly, the A-MAR assumption here has little impact on inference over the
MAR assumption (equivalent to model with Byv = 0).
In general, if the missingness is A-MAR, but
p(y, v | x) ≈p(y | x)p(v | x),
then joint modeling is probably not needed. See Liu and Daniels (2007) for
a more detailed exploration of the dependence structure and a discussion
addressing inference on the relationship between smoking and weight change
between the two treatments, which we do not address here.
Table 7.12 contains the treatment diﬀerence at the ﬁnal week with 95%
credible intervals. The wellness arm had a higher quit rate than the exercise
arm (with a 95% credible interval that had a lower bound of zero). Clearly,
the independence model underestimates the treatment diﬀerence. This is due
to the fact that in the dependence models, the values ﬁlled in for the missing
responses using data augmentation used the previous quit responses to ﬁll in
smokers more often than quitters vs. the independence models, which did not
use the previous responses.

BAYESIAN P-SPLINE MODEL: HERS CD4 DATA
159
Table 7.11 CTQ II: posterior means and 95% credible intervals for weekly cessation
rates on the exercise (E) and wellness W) treatment arms, computed for each of the
three models. Observed-data sample means included for comparison.
Model
Treatment
A-MAR
MAR
Indep
Observed
W
.39 (.34,.45)
.39 (.34, .45)
.38 (.31, .46)
.39
.50 (.44,.55)
.50 (.44, .55)
.50 (.43, .56)
.49
.55 (.49, .60)
.55 (.49, .60)
.56 (.48, .64)
.57
.53 (.47, .59)
.53 (.47, .59)
.56 (.48, .64)
.56
.53 (.47, .59)
.53 (.47, .59)
.53 (.45, .62)
.53
.46 (.40, .52)
.46 (.40, .52)
.46 (.40, .52)
.46
E
.45 (.39, .51)
.45 (.39, .51)
.47 (.40,.54)
.44
.37 (.32, .43)
.37 (.32, .43)
.38 (.30, .47)
.38
.44 (.38, .49)
.44 (.38, .49)
.45 (.39, .51)
.44
.42 (.37, .48)
.42 (.37, .48)
.47 (.40, .55)
.46
.46 (.40, .52)
.46 (.40, .52)
.50 (.43, .57)
.49
.38 (.32, .43)
.38 (.32, .43)
.43 (.36, .51)
.42
7.5.6 Conclusions
The analysis provided evidence of a higher quit rate on the wellness arm.
Posterior predictive checks suggested the best model (as chosen by DICO)
ﬁt the observed data well. The inclusion of the auxiliary longitudinal process
weight change had little eﬀect on the conclusions in this example because it
was not strongly correlated with smoking cessation.
7.6 Bayesian p-spline model: HERS CD4 data
In HIV epidemiology, the eﬀect of HAART on disease dynamics remains an im-
portant question. Our analysis here examines CD4 trajectory relative to initia-
tion of HAART in the HER study (described in Section 1.5). Because HAART
use was incident in HERS, these data aﬀord an opportunity to capture the

160
CASE STUDIES: IGNORABLE MISSINGNESS
Table 7.12 CTQ II: posterior means of diﬀerence in quit rates with 95% credible
intervals at week 8 for each of the three models considered. Observed-data sample
means included for comparison.
Model
Posterior Mean
95% C.I.
A-MAR
.08
(.00, .16)
MAR
.08
(.00, .16)
Independence
.03
(–.06, .10)
Observed
.04
population-level variation in CD4 before and after initiation of HAART. Here
we present a comparative analysis based on p-splines.
7.6.1 Models
Deﬁne Y i to be the Ji-dimensional vector of CD4 counts measured at times
(ti1, . . . , ti,Ji). We model the CD4 trajectories using p-splines (see Examples
2.7 and 3.10) with linear (q = 1) and quadratic (q = 2) bases using a multi-
variate normal model with an exponential covariance function
Y i | Xi ∼N(Xiβ, Σi),
where the elements σikl(φ) of Σi take the form
σikl = σ2 exp(−φ|tik−til|).
(7.3)
The matrices xi are speciﬁed using a truncated power basis with 18 equally
spaced knots placed at the sample quantiles of the observation times. So, the
jth row is given by
xij = {1, tij, t2
ij, . . . , tq
ij, (tij −s1)q
+, (tij −s2)q
+, . . . , (tij −s18)q
+}T,
where {s1, . . . , s18} are the knots and β is partitioned as
β = (α0, α1, . . . , αq, a1, . . . , a18)T = (αT, aT)T.
In this study, not all the data were observed and the observed data were
irregularly spaced. The analysis here implicitly assumes that the unobserved
data are MAR.

HERS CD4 DATA
161
7.6.2 Priors
Bounded uniform priors are speciﬁed for the residual standard deviation σ
and the correlation parameter φ for the exponential covariance function in
(7.3), with lower bounds of 0 and upper bounds of 100 and 10, respectively.
The components of the (q+1)-dimensional vector of coeﬃcients α are given
‘just proper’ independent normal priors with mean 0 and variance 104; the
coeﬃcients corresponding to the knots a are given normal priors with mean
zero and variance τ2. The standard deviation τ is then given a Unif(0, 10)
prior.
7.6.3 MCMC details
Three chains were run each for 15, 000 iterations with a burn-in of 5000. The
autocorrelation in the chains was minimal.
7.6.4 Model selection
In addition to the model with an exponential covariance function for the
residuals, we also ﬁt a model that assumes independent residuals (φ = 0) for
comparison. To assess ﬁt among the models, we computed the DIC with θ =
(β, φ, 1/σ2, 1/τ2); see Table 7.13. The best-ﬁtting model was the quadratic
p-spline with the exponential covariance function. The greatly improved ﬁt
of both the linear and quadratic p-splines with residual autocorrelation over
independence of the residuals is clear from the large diﬀerences in the DIC.
Table 7.13 HERS CD4 data: DIC under q = 1, q = 2 (linear and quadratic spline
bases), under independence and exponential covariance functions.
Model
DIC
Dev(θ)
pD
Independence, q = 1
19417
19400
8
Independence, q = 2
19417
19402
7
Exponential, q = 1
18008
17985
11
Exponential, q = 2
17999
17980
9
7.6.5 Results
Posterior means and 95% credible intervals for the covariance parameters
in the best-ﬁtting model (as measured by the DIC) with q = 2 appear in

162
CASE STUDIES: IGNORABLE MISSINGNESS
Table 7.14. The correlation parameter φ was signiﬁcantly diﬀerent from zero,
with the posterior mean corresponding to a lag 1 (week) correlation of .52.
Posterior means of the curves for q = 1 and q = 2 appear in Figures 7.2 and
7.3, respectively.
The posterior mean of the spline curves were relatively similar for q = 1 and
q = 2 and captured more local variation than the independence models; time
zero on the plot represents self-reported initiation of HAART. An increase
can be seen near self-reported initiation (the eﬀect of HAART), followed by a
decrease later on, possibly attributable to noncompliance and development of
resistant viral strains. The results also suggest that the self-reports of HAART
initiation are subject to a delay relative to the actual initiation of HAART
because the decrease in CD4 levels occurs prior to self-reported initiation time
(t = 0).
Table 7.14 Posterior means and 95% credible intervals for exponential covariance
function parameters and smoothing standard deviation under q = 2 for the HERS
CD4 data.
Parameter
Posterior Mean
95% C.I.
φ
.65
(.60, .70)
σ
3.9
(3.8, 4.0)
τ
1.9
(.81, 3.6)
7.7 Summary
The ﬁve examples in this chapter have demonstrated a variety of fundamental
concepts for properly modeling incomplete data under an assumption of ignor-
ability. The importance of correctly specifying the dependence structure was
emphasized in all the examples and approaches for choosing the best-ﬁtting
model and assessing the ﬁt of such models under ignorability using DICO and
posterior predictive checks were demonstrated.
In addition, we illustrated the use of joint modeling to properly address
auxiliary stochastic time-varying covariates when an assumption of auxiliary
variable MAR (A-MAR) is thought plausible.
Under ignorability, model speciﬁcation only involved the full data response
model. In the next three chapters we discuss model-based approaches for
nonignorable missing data that require speciﬁcation of the full-data model,
i.e., the full-data response model and the missing data mechanism (explicitly).
In Chapter 10 we present three detailed case studies.

SUMMARY
163
−2000
−1000
0
1000
200
400
600
800
1000
Time since HAART initiation
CD4 count
exponential covariance
naive independence
Figure 7.2 HERS CD4 data: plot of observed data with ﬁtted p-spline curve with
q = 1 under independence and exponential covariance function. The observed data
(dots) are averages over 100 day windows. The tick marks on the top of the plot
denote percentiles.

164
CASE STUDIES: IGNORABLE MISSINGNESS
−2000
−1000
0
1000
200
400
600
800
1000
Time since HAART initiation
CD4 count
exponential covariance
naive independence
Figure 7.3 HERS CD4 data: plot of observed data with ﬁtted p-spline curve with
q = 2 under independence and exponential covariance function. The observed data
(dots) are averages over 100 day windows. The tick marks on the top of the plot
denote percentiles.

CHAPTER 8
Models for Handling Nonignorable
Missingness
8.1 Overview
This chapter covers many of the central concepts and modeling approaches
for dealing with nonignorable missingness and dropout. A prevailing theme
is the factorization of the full-data model into the observed data distribution
and the extrapolation distribution, where the latter characterizes assumptions
about the conditional distribution of missing data given observed information
(observed responses, missing data indicators, and covariates). The extrapo-
lation factorization (as we refer to it) factors the full-data distribution into
its identiﬁed and nonidentiﬁed components. We argue that models for non-
ignorable dropout should be parameterized in such a way that one or more
parameters for the extrapolation distribution is completely nonidentiﬁed by
data. These also should have transparent interpretation so as to facilitate sen-
sitivity analysis or incorporation of prior information about the missing data
distribution or missing data mechanism.
In Section 8.2, we describe the extrapolation distribution and state con-
ditions for parameters to be sensitivity parameters. Section 8.3 reviews se-
lection models, with particular emphasis on how the models are identiﬁed.
We show that parametric selection models cannot be factored into identi-
ﬁed and nonidentiﬁed parts, resulting in an absence of sensitivity parameters.
Semiparametric selection models are introduced as a potentially useful alter-
native. Section 8.4 covers mixture models for both discrete and continuous
responses, and for discrete and continuous dropout times. We show that in
many cases, mixture models are easily factored using the extrapolation factor-
ization, enabling interpretable sensitivity analyses and formulation of missing
data assumptions. Computation of covariate eﬀects for mixture models also is
discussed. Section 8.5 provides an overview of shared parameters models. In
Section 8.6, we provide a detailed discussion of possible approaches to model
selection and model ﬁt. This is still an open area of research, and our coverage
highlights several of the key conceptual and technical issues. Model ﬁt and
model selection under nonignorable missingness diﬀer from the ignorable case
because with nonignorability, the full-data distribution includes a speciﬁca-
tion for f(r | y) that must be checked against observed r. Suggestions for
further reading are given in Section 8.7.
165

166
NONIGNORABLE MISSINGNESS
8.2 Extrapolation factorization and sensitivity parameters
In Chapter 5, we introduced a classiﬁcation of dropout mechanisms that, with
respect to posterior inference, can be classiﬁed as ignorable or nonignorable.
In Chapter 6, we reviewed and discussed concepts and models for ignorable
dropout, and illustrated using case studies in Chapter 7. In this chapter, we
describe several diﬀerent models for nonignorable dropout that are character-
ized via speciﬁcation of the full data distribution f(y, r | ω).
Recall that the full data distribution can be factored into an extrapolation
model and an observed data model,
p(y, r | ω)
=
p(ymis | yobs, r, ωE) p(yobs, r | ωO).
Here, ωE and ωO denote parameters indexing the extrapolation and observed
data models, respectively. They are potentially, but not necessarily, overlap-
ping functions of ω. The observed data distribution p(yobs, r | ωO), is identi-
ﬁed and can (in principle) be estimated nonparametrically. The extrapolation
distribution p(ymis | yobs, r, ωE) cannot be identiﬁed without modeling as-
sumptions or constraints on the parameter space.
In general we advocate using parameterizations in which one or more pa-
rameters indexing the extrapolation cannot be identiﬁed by observed data.
To formalize this notion, we deﬁne a class of parameters for full-data models
that can be used for sensitivity analysis or incorporation of informative prior
information. Generally they are not identiﬁable from observed data, but when
their values are ﬁxed, the remainder of the full-data model is identiﬁed. For-
mally we call them sensitivity parameters and in general, we use the term to
mean parameters that satisfy this deﬁnition.
Deﬁnition 8.1. Sensitivity parameter.
Let p(y, r | ω) denote a full-data model. Its extrapolation factorization is
given by
p(y, r | ω)
=
p(ymis | yobs, r, ωE) p(yobs, r | ωO).
If there exists a reparameterization ξ(ω) = (ξS, ξM) such that:
1. ξS is a nonconstant function of ωE,
2. the observed-data likelihood
L(ξS, ξM | yobs, r)
is constant as a function of ξS, and
3. at a ﬁxed value of ξS, the observed data likelihood is a nonconstant
function of ξM,
then ξS is a sensitivity parameter.
2
The ﬁrst condition requires that ξS be a nonconstant function of the param-
eters of the extrapolation distribution. The second condition implies that the

SELECTION MODELS
167
ﬁt of the model to the observed data is not aﬀected by the sensitivity parame-
ters. In any particular full-data model, there may be several sets of parameters
that satisfy the ﬁrst two conditions. However, there will often be only one set
that satisﬁes the third condition as well, which states when the sensitivity
parameters are ﬁxed, the full data model is identiﬁed. These conditions will
be helpful in determining sensitivity parameters for semiparametric selection
models in Section 8.3.7. For mixture models, ﬁnding sensitivity parameters is
usually very easy (see Section 8.4.3).
The framework we use is very similar to the one based on nonparametric
identiﬁed (NPI) full-data models proposed by Robins (1997) and discussed in
some detail by Vansteelandt et al. (2006). A class M (γ) of full-data models is
nonparametric identiﬁed if, for an observed data distribution p(yobs, r), there
exists a unique full-data model p(yobs, ymis, r | γ) ∈M (γ) that marginalizes
to p(yobs, r); i.e.

p(yobs, ymis, r | γ) dymis
=
p(yobs, r).
The observed data do not contain information about γ, but knowing γ points
to a speciﬁc p(yobs, ymis, r | γ) in the class M (γ) that coincides with p(yobs, r).
Vansteelandt et al. (2006) call γ a sensitivity parameter if it indexes a NPI
class of full-data models M (γ).
Deﬁnition 8.1 is designed for the likelihood framework, but essentially pos-
sesses the same attributes. In short, sensitivity parameters index the missing
data extrapolation and cannot be identiﬁed by observed data. They provide
the framework for assessing sensitivity of model-based inferences to assump-
tions about the missing data mechanism, or incorporating those assumptions
formally in terms of informative prior distributions.
As it turns out, not all models are amenable to this type of parametriza-
tion. Parametric selection models in particular cannot generally be factored
to permit sensitivity parameterizations. Mixture models tend to be easier to
work with. For models described below, we provide examples of parameteri-
zations that satisfy Deﬁnition 8.1, or indicate when they are diﬃcult or even
impossible to ﬁnd.
8.3 Selection models
In most of this section, we assume a common set, {t1, . . . , tJ}, of observation
times and without loss of generality tj = j.
8.3.1 Background and history
Recall from Chapter 5 that the selection model factors the full-data distribu-
tion as
p(y, r | ω) = p(r | y, ψ(ω)) p(y | θ(ω));
(8.1)

168
NONIGNORABLE MISSINGNESS
this factorization underlies the missing data taxonomy described in Section 5.4.
The typical strategy to model the two components is to specify a parametric
model for each and to assume that ψ and θ are distinct. In the following, we
will assume ψ and θ are distinct and a priori independent unless stated oth-
erwise. Note that this does not correspond to the (ωE, ωO) partition in (8.1).
An early example of selection modeling for multivariate data can be found
in Heckman (1979), where the joint distribution of a bivariate response Y
with missing Y2 was speciﬁed using a multivariate normal distribution (im-
plicitly, a probit model for the binary indicators of missingness that is linear in
(Y1, Y2)T). Diggle and Kenward (1994) expanded the Heckman model to the
case of dropout in longitudinal studies using a logistic model for the hazard
of dropout. Many subsequent articles adapted and extended their approach
within a likelihood framework (see, e.g., Fitzmaurice, Molenberghs, and Lip-
sitz, 1995; Baker, 1995; Molenberghs, Kenward, and Lesaﬀre, 1997; Liu, Wa-
ternaux, and Petkova, 1999; Albert, 2000; Heagerty and Kurland 2004). A
framework for semiparametric inference can be found in Robins et al. (1995)
and Scharfstein et al. (1999).
We begin our review of parametric selection models by demonstrating via
several examples the diﬃculty in ﬁnding sensitivity analysis parameteriza-
tions.
8.3.2 Absence of sensitivity parameters in the missing data mechanism
In many parametric selection models, all the parameters are identiﬁed. Iden-
tiﬁcation is driven by parametric assumptions on both the full-data response
model and the missing data mechanism (MDM).
To illustrate using a simple example, consider a cross-sectional setting
where Y may or may not be missing. Suppose the histogram of the observed
y’s looks like that given in Figure 8.1 and that we specify the following model
for the missing data mechanism:
logit{P(R = 1 | y)} = ψ0 + ψ1y,
where R = 1 corresponds to observing Y . With no further assumptions about
the distribution of Y , we cannot identify ψ1 because when R = 0, we do
not observe Y . It can also be shown (Scharfstein et al., 2003) that ψ1 is a
sensitivity parameter.
However, suppose we further assume that the full-data response model is
a normal distribution, N(µ, σ2). By looking at the histogram of the observed
y’s in Figure 8.1, it is clear that for this histogram to be consistent with
normally distributed full-data y’s, we need to ﬁll in the right tail; this implies
ψ1 < 0. On the other hand, if we assumed a parametric model for the full-data
response model that was consistent with the histogram for the observed y’s
(e.g., a skew-normal), it would suggest that ψ1 = 0. Thus, inference about

SELECTION MODELS
169
ψ1 depends heavily on the distributional assumptions about p(y), and ψ1 is
in fact identiﬁed by observed data. Hence it does not meet the criteria in
Deﬁnition 8.1, and therefore is not a sensitivity parameter.
y
Frequency
−3
−2
−1
0
1
0
50
100
150
Figure 8.1 Histogram of observed responses.
This univariate example extends readily to the longitudinal setting. For
example, consider a bivariate normal full-data response having missingness in
Y2, with missing data mechanism
logit{P(R = 1 | y)} = ψ0 + ψ1y1 + ψ2y2,
(8.2)
where R = 1 corresponds to observing Y2. Under a bivariate normal distribu-
tion for the full-data response model, the same argument used above would
be applicable in terms of the conditional distribution, p(y2 | y1) and the iden-
tiﬁcation of ψ2.
Sensitivity of MDM parameters to assumptions on the full-data response

170
NONIGNORABLE MISSINGNESS
model was studied in a simple but very informative empirical example by Ken-
ward (1998). He considered a bivariate full-data response with Y2 potentially
missing and missing data mechanism given in (8.2), and assessed the sensi-
tivity of inference about ψ to distributional assumptions about p(y2 | y1). If
this distribution was assumed to be normal, the estimates and standard errors
of ψ implied MNAR. However, if this distribution was assumed to follow a
t-distribution with only a few degrees of freedom, the estimates and standard
errors of ψ implied MAR. So by just changing the tails of the distribution
of the full-data response, inference concerning the missing data mechanism
changed signiﬁcantly. This and the previous hypothetical examples illustrate
the prominent role of modeling assumptions in parametric selection models:
widely diﬀering conclusions can be drawn based on unveriﬁable modeling as-
sumptions about the full-data response.
The preceding discussion has focused entirely on the full-data response
model in identifying potential sensitivity parameters in the missing data mech-
anism. However, we have focused on the simple (and common) situation where
the response y is entered into the MDM linearly. This is a strong assumption
and is not always appropriate. For example, in the schizophrenia clinical trial
(Section 1.2), it is conceivable that participants may be more likely to drop out
when they are doing much better or much worse, suggesting that the missing
data mechanism should be quadratic in y. Revisiting the previous example
with cross-sectional data, assume p(yobs) follows the histogram in Figure 8.1,
p(y) is a normal distribution, and the MDM follows
logit{P(R = 1 | y)} = ψ0 + ψ1y + ψ2y2.
(8.3)
This scenario is consistent with an MNAR mechanism having ψ1 < 0 and
ψ2 = 0, because the right tail needs to be ﬁlled in for the full-data responses
to be normally distributed (the left tail is already consistent with a normal
distribution). Alternatively, suppose that p(yobs) resembled the histogram
in Figure 8.2. The quadratic MDM (8.3) is now consistent with ψ2 > 0,
which is MNAR. On the other hand, with the same observed data response
distribution, an MDM that is linear in y (ψ2 = 0) is consistent with ψ1 = 0,
which is MAR.
These simple examples demonstrate that for a fully speciﬁed parametric
selection model, all parameters are identiﬁed. As a consequence, there are no
obvious sensitivity parameters. By contrast, in an ideal sensitivity analysis, the
distribution of Y mis given Y obs and R is governed by parameters that aﬀect
the full-data distribution but not the observed-data distribution, so that per-
turbations of these parameter values do not aﬀect ﬁt of the full-data model
to observables. By this criterion, parametric selection models are not well-
suited to sensitivity analysis or to incorporation of prior information about
p(ymis | yobs, r). A detailed discussion of this point follows in Sections 8.3.3
through 8.3.6; in Section 8.3.7 we introduce semiparametric selection mod-

SELECTION MODELS
171
y
Frequency
−2
−1
0
1
2
0
20
40
60
80
Figure 8.2 Histogram of observed responses.
els, which oﬀer a more viable selection-model-based framework for sensitivity
analysis.
8.3.3 Heckman selection model for a bivariate response
We start out our review of parametric selection models by providing details
on the Heckman model (Heckman, 1979). Let Y = (Y1, Y2)T denote a bivari-
ate outcome, and let R be an indicator of whether Y2 is observed (R = 1
corresponds to Y2 being observed). Heckman proposed to jointly model the

172
NONIGNORABLE MISSINGNESS
outcome and missingness using a trivariate normal distribution as follows:





Y1
Y2
Z




∼N










µ1
µ2
µz




,





σ11
σ21
σ22
σ31
σ32
σ33









.
(8.4)
In this setup, Z is a continuous latent variable underlying the missingness
indicator, where R = I{Z > 0}. This model implies Y ∼N(µY , ΣY ), where
ΣY is the upper left 2 × 2 submatrix of the covariance matrix in (8.4). The
Heckman model requires some restrictions on the the σjk, which we detail
below.
Let Γ = Σ−1
Y , and denote the unique elements of Γ by {γ11, γ12, γ22}.
The selection model above implies that the missing data mechanism follows
a probit regression that is linear in Y1 and Y2,
P(R = 1 | y) = P(Z > 0 | y) = Φ(ψ0 + ψ1y1 + ψ2y2).
(8.5)
Referring to (8.4),
ψ0
=
µz −ψ1µ1 −ψ2µ2
ψ1
=
σ31γ11 + σ32γ12
ψ2
=
σ31γ12 + σ32γ22.
Note that the regression parameters in the missing data mechanism are func-
tions of the mean and covariance parameters of the full-data response model
p(y | θ), which is bivariate normal with mean µY = (µ1, µ2)T and covariance
matrix Γ−1. If the missing data mechanism was thought to be nonlinear in
y, the trivariate normal model given in (8.4) would not be appropriate as it
induces the linear missing data mechanism in (8.5).
For identiﬁability, the model in (8.4) is typically parameterized such that
the conditional variance of the latent variable Z is ﬁxed, i.e.,
var(Z | y1, y2) = σ33 −σ3ΓσT
3 = 1,
where σ3 = (σ31, σ32). When the conditional variance is set to 1, the missing
data mechanism coeﬃcients ψ in (8.5) correspond to a standard deviation
change in the latent variable Z.
Individual contributions to the observed data likelihood take the form
 0
−∞
p(y1, z | θ, ψ)dz
1−r  ∞
0
p(y1, y2, z | θ, ψ)dz
r
,
where the ﬁrst integrand is a bivariate normal derived from (8.4) and the
second integrand is the trivariate normal given in (8.4).
The Heckman selection model can be extended in several ways. First, there
are obvious extensions to settings where J > 2 by increasing the dimension

SELECTION MODELS
173
of the model. Second, binary longitudinal responses can be accommodated
by making the multivariate normal in (8.4) into a multivariate probit model.
Third, the means µY can be modiﬁed to include covariates. Because the model
relies on a normal speciﬁcation, integrating over Y mis is relatively straight-
forward. But the normality assumption also identiﬁes all model parameters,
including coeﬃcients of incompletely observed Y ’s in the missing data mech-
anism. We continue our review of parametric selection models by introducing
more general models for longer series of longitudinal responses.
8.3.4 Speciﬁcation of the missing data mechanism for longitudinal data
Recall that selection models for longitudinal data require speciﬁcation of two
separate models: (1) a model p(r | y, ψ) for the missing data mechanism; and
(2) a model p(y | θ) for the full-data response. General issues in specifying
p(y | θ) were discussed in detail in Chapter 6 and still are applicable here.
The assumption of nonignorable dropout requires further speciﬁcation of p(r |
y, ψ).
As discussed in Chapter 5, it is convenient to specify the MDM for longi-
tudinal data with dropout using the hazard of dropout. As in Chapter 5, let
Y j = {Y1, . . . , Yj} denote the history of responses for an individual up to and
including time tj. For the case of J ﬁxed measurement times, the hazard of
dropout at U = tj is a function of the full data Y via
h(tj | yJ, ψ) = P(Rj = 0 | R1 = · · · = Rj−1 = 1, yJ, ψ).
(8.6)
A transformation of h(tj | yJ, ψ) is often speciﬁed as additive in the compo-
nents of Y J. For example, we might assume
h(tj | yJ, ψ) = g−1(ψT
j yi),
where g is an appropriate link function and ψj = (ψj1, . . . , ψjJ)T. Under
MNAR, the parameters
{ψjj, ψj,j+1, . . . , ψjJ}
are potential sensitivity parameters.
The general model in (8.6), even under additivity in the components of Y j,
has a lot of potential sensitivity parameters. Diﬀerent simpliﬁcations can be
made to constrain the parameter space fully or partially identify the model;
choices include restricting the dependence on elements of Y J to one or two
informative variables (e.g., Yj and Yj −Yj−1); assuming constant baseline
hazard; or imposing restrictions on the missing data mechanism.
Related to the third choice, a parsimonious class of models for the missing
data mechanism, referred to as missing non-future dependence (Kenward et
al., 2003) is characterized by
h(tj | yJ; ψ) = h(tj | yj; ψ),
(8.7)

174
NONIGNORABLE MISSINGNESS
where the hazard of dropout at time j depends on past responses Y j−1, on
the potentially missing response Yj, but not on future responses. The non-
future dependent missing data mechanisms also can be applied in mixture
models (see Section 8.4.2). A simple version of the hazard of dropout under
non-future dependent missingness is
h(tj | yj, ψ) = g−1(ψ0 + ψ1yj−1 + ψ2yj),
(8.8)
where the hazard of dropout at tj depends only on the current response yj
and the most recent past response yj−1. In this speciﬁcation, there is only
one potential sensitivity parameter, ψ2, which considerably simpliﬁes sen-
sitivity analyses in certain classes of semiparametric selection models (see
Section 8.3.7 and Chapter 9). In addition, in this simpliﬁed form, a test for
MNAR vs. MAR reduces to a test of ψ2 = 0. However, this test is valid only
under the assumption that the full-data model is speciﬁed correctly.
8.3.5 Parametric selection models for longitudinal data
In this section, we present some details on parametric selection models for
continuous longitudinal responses and binary longitudinal responses, respec-
tively.
Example 8.1. A parametric selection model for continuous responses.
Diggle and Kenward (1994) proposed a parametric selection model for a J-
dimensional vector of longitudinal responses Y with monotone dropout. We
describe this model in the context of the schizophrenia clinical trial (Section
1.2). In this trial, measurements of schizophrenia symptomatology (BPRS
scores) were intended to be collected for 6 weeks but there were dropouts at
each week after baseline. The full-data response model (for the six weekly
BPRS measurements) might be speciﬁed as
Y i | xi
∼
N(xiβ, Σ),
where θ = (β, Σ). The hazard of dropout can be speciﬁed using a simple form
of the hazard with a logistic link,
logit{h(tj | yij, ψ)}
=
ψ0 + ψ1yi,j−1 + ψ2yij.
Thus, dropout at time j is allowed to depend on the immediate previous
response yi,j−1 and the potentially unobserved current response yij. Equiva-
lently, the model could be speciﬁed using yij and the diﬀerence yij −yi,j−1.
The observed data likelihood for this model takes a complex form. In par-
ticular, with monotone missingness caused by dropout, the contribution to
the observed data likelihood for an individual who drops out just prior to

SELECTION MODELS
175
observing response k is
L(θ, φ | yobs,i, ri)
∝


j<k
{1 −h(tj | yij, ψ)} p(yij | yi,j−1, θ)


×

h(tk | yi,k−1, yk, ψ) p(yk | yi,k−1, θ) dyk.
(8.9)
The complexity of this likelihood is due to the integral with respect to the
missing response yik. Bayesian MCMC approaches avoid direct evaluation of
this integral through data augmentation. Details are provided in Section 8.3.8.
The inability in parametric selection models to partition the full-data model
parameters vector into identiﬁed and nonidentiﬁed parameters is evident from
(8.9), where the observed data likelihood is a function of all the model pa-
rameters, including the coeﬃcient ψ2 of the potentially missing response data;
hence, the parameters of the extrapolation distribution cannot be disentangled
from the parameters of the observed data distribution.
2
There is a considerable literature on parametric selection models for longi-
tudinal binary responses (Baker, 1995; Fitzmaurice, Molenberghs, and Lipsitz,
1995; Albert, 2000; and Kurland and Heagerty, 2004) which mostly diﬀers in
how the the full-data response model is speciﬁed. For example, Kurland and
Heagerty (2004) proposed a parametric selection model for binary responses
that assumes the same form for the missing data mechanism as in Example 8.1
and then replaces the multivariate normal likelihood for the full-data response
model with a ﬁrst-order marginalized transition model, MTM(1) (see Exam-
ple 2.5). The observed-data log likelihood takes the same form as in (8.9), with
the terms involving the full-data response model now being derived based on
an MTM and integrals replaced by sums.
We ﬁt parametric selection model to longitudinal binary responses in Chap-
ter 10 using data from the OASIS study (Section 1.6) to examine the eﬀect
of two treatments on smoking cessation among alcoholic smokers.
8.3.6 Feasibility of sensitivity analysis for parametric selection models
In the setting of parametric selection models, several authors have suggested
ﬁxing parameters like ψ2 in (8.2) at reasonable values (via expert opinion),
and then maximizing the observed data log likelihood over the remaining pa-
rameters (see Little and Rubin, 1999; Kurland and Heagerty, 2004). This is
similar in spirit to the sensitivity analysis approaches of Robins and colleagues
(e.g., Rotnitzky et al., 1998; Scharfstein et al., 1999), but there is an important
diﬀerence. In the work of Robins and colleagues, the observed data response
model was speciﬁed nonparametrically. Thus, ﬁxing the potential sensitivity
parameters in the MDM did not impact the ﬁt of the model to the observed
responses. In a parametric selection model, diﬀerent choices of ψ2 yield dif-

176
NONIGNORABLE MISSINGNESS
ferent models for the observable data because ψ2 does not satisfy condition 2
of Deﬁnition 8.1. In a frequentist analysis, there is a best ﬁt to the observed
data within a given parametric speciﬁcation (at the mle of ψ).
In the next section, we discuss a more ﬂexible class of selection models.
These (Bayesian) semiparametric selection models allow sensitivity analysis
to be done without aﬀecting ﬁt to the observed data, and provide a sensitivity
analysis framework similar to those in the work of Robins and colleagues.
8.3.7 Semiparametric selection models
Semiparametric selection models typically use a parametric model for the
missing data mechanism and a semi- or nonparametric model for the observed-
data response distribution (or the full-data response distribution). In a frame-
work that is likelihood-based, it is sometimes the case that one can ﬁnd pa-
rameters in the missing data mechanism that very nearly satisfy Deﬁnition 8.1
in that they are only weakly identiﬁed by observed data. As such, we can then
specify an informative prior on these parameters in the missing data mecha-
nism — e.g., ψ2 in (8.8) — and not aﬀect the ﬁt of the model to the observed
data. Fully parametric selection models typically do not allow this ﬂexibility.
For a (univariate) continuous response Y , a semi-parametric selection model
was proposed in Scharfstein et al. (1999) and extended to a fully Bayesian
model by Scharfstein et al. (2003). Unfortunately, the multivariate nature
in the longitudinal case makes direct extensions diﬃcult due to the curse of
dimensionality (see, e.g., Robins and Ritov, 1997). However, in the case of
binary (categorical) longitudinal data, such extensions are more feasible.
Binary longitudinal responses
We begin with the simplest longitudinal setting of a bivariate binary response
with missingness only in the second component (Y2) and no covariates. This
case provides a useful platform for understanding fully nonparametric model
speciﬁcation for selection models with longitudinal binary data and will pro-
vide a starting point for a semiparametric speciﬁcation for the full-data model.
Let Y
= (Y1, Y2)T denote the full-data response, with R = 1 if Y2 is
observed and R = 0 if it is missing. The entire full-data distribution p(y, r | ω)
can be enumerated using a multinomial distribution with probabilities
ω(r)
y1,y2
=
P(Y1 = y1, Y2 = y2, R = r),
shown in Table 8.1. The multinomial model has seven distinct parameters
(noting that 
r,y1,y2 ω(r)
y1,y2 = 1). The parameters ω(1)
00 , . . . , ω(1)
11 corresponding
to p(y1, y2, r = 1) all are identiﬁable from the observed data. Among those
with R = 0, we can identify only P(Y1 = 0, R = 0) = ω(0)
00 + ω(0)
01
and

SELECTION MODELS
177
P(Y1 = 1, R = 0) = ω(0)
10 + ω(0)
11 . We denote the identiﬁed parameters as
ωI
=
(ω(1)
00 , ω(1)
01 , ω(1)
10 , ω(1)
11 , ω(0)
0+, ω(0)
1+),
(8.10)
where
ω(0)
0+
=
ω(0)
00 + ω(0)
01
ω(0)
1+
=
ω(0)
10 + ω(0)
11 .
Table 8.1 Multinomial parameterization of full-data distribution for bivariate binary
data with possibly missing Y2.
R
Y1
Y2
p(y1, y2, r | ω)
0
0
0
ω(0)
00
0
0
1
ω(0)
01
0
1
0
ω(0)
10
0
1
1
ω(0)
11
1
0
0
ω(1)
00
1
0
1
ω(1)
01
1
1
0
ω(1)
10
1
1
1
ω(1)
11
In this setting, the selection model can be viewed as a reparameterization
of the joint distribution in Table 8.1 using the factorization
p(y1, y2, r)
=
p(r | y1, y2) p(y2 | y1) p(y1),
where
Y1
∼
Ber(θ1)
Y2 | Y1
∼
Ber(θ2|1)
R | Y1, Y2
∼
Ber(π),
and
logit(θ1)
=
α
logit(θ2|1)
=
β0 + β1y1
logit(π)
=
ψ0 + ψ1y1 + ψ2y2 + ψ3y1y2.

178
NONIGNORABLE MISSINGNESS
This model has seven unique parameters, so it is saturated in time and
missingness pattern. It is therefore nonparametric. Clearly, with the selec-
tion model parameterization, α is directly identiﬁed from the observed data.
However, none of the other parameters can be identiﬁed without imposing
untestable constraints.
To identify the remaining parameters without aﬀecting the ﬁt of the model
to the observed data, we have two degrees of freedom with which to work.
Equivalently, we have two sensitivity parameters. In the logit parameteriza-
tion, the missing data mechanism is conveniently represented by the sensi-
tivity parameters (ψ2, ψ3) that satisfy the three conditions in Deﬁnition 8.1.
Here, ψ2 and ψ3 are zero under MAR and correspond to log odds ratios of
missingness in Y2 given Y1.
Fixing ψ2 and ψ3 identiﬁes all remaining model parameters. To see this,
we ﬁrst express the parameters in the missing data mechanism as functions
of the original multinomial probabilities ω:
ψ0
=
logit
ω(1)
00
ω(1)
00 + ω(0)
00
(8.11)
ψ0 + ψ1
=
logit
ω(1)
10
ω(1)
10 + ω(0)
10
(8.12)
ψ0 + ψ2
=
logit
ω(1)
01
ω(1)
01 + ω(0)
01
(8.13)
ψ0 + ψ1 + ψ2 + ψ3
=
logit
ω(1)
11
ω(1)
11 + ω(0)
11
.
(8.14)
By subtracting (8.12) from (8.14), we obtain ψ2 +ψ3. After some algebra, it is
possible to obtain a closed-form (but not simple) expression for ω(0)
10 /ω(0)
11 . By
combining this with the identiﬁed sum ω(0)
10 + ω(0)
11 , we can obtain closed-form
expressions for ω(0)
10 and ω(0)
11 individually. By taking a similar approach with
(8.11) and (8.13), we can identify ω(0)
00 and ω(0)
01 .
It turns out that if we specify an informative prior on (ψ2, ψ3) that is
independent of the identiﬁed parameters ωI given in (8.10), the posterior for
(ψ2, ψ3) will be equal to the prior
p(ψ2, ψ3 | yobs, r) = p(ψ2, ψ3).
(8.15)
In Chapter 9 we actually recommend priors such that (ψ2, ψ3) are a priori
dependent on ωI. In this case, the equality in (8.15) will not hold exactly. We
defer details on this to Chapter 9.
Unfortunately, when J ≫2, a fully nonparametric selection model is less
practical. For monotone dropout (and no dropouts at the ﬁrst observation
time), there are (2J−1)+(J−1)2J (= J2J−1) parameters, of which only (2J−

SELECTION MODELS
179
1) + J−1
j=1 2j are identiﬁed; this leaves J−1
j=1 (2J −2j) sensitivity parameters.
For example, for J = 4, there are 34 sensitivity parameters — and this does
not even include covariates! However, a semiparametric selection model can
be a viable alternative. By semiparametric, we mean a parametric model for
the missing data mechanism and a nonparametric model for the full-data
response. We provide an example for a trivariate binary response next.
Example 8.2. Semiparametric selection model for longitudinal binary data
with J = 3.
Let Y = (Y1, Y2, Y3)T be the full-data response and R = (R1, R2, R3)T be
the observed data indicators (assuming monotone dropout), where Rj = 0
corresponds to Yj being missing. We assume R1 ≡1. The nonparametric
selection model can be written as
Y1
∼
Ber(θ1)
Y2 | Y1
∼
Ber(θ2|1)
Y3 | Y1, Y2
∼
Ber(θ3|21)
R2 | Y1, Y2, Y3
∼
Ber(π2)
R3 | R2 = 1, Y1, Y2, Y3
∼
Ber(π3)
with
logit(θ1)
=
α
logit(θ2|1)
=
β0 + β1y1
logit(θ3|21)
=
β2 + β3y1 + β4y2 + β5y1y2
logit(π2)
=
ψ0 + ψ1y1 + ψ2y2 + ψ3y3 + ψ4y1y2
+ ψ5y1y3 + ψ6y2y3 + ψ7y1y2y3.
logit(π3)
=
ψ8 + ψ9y1 + ψ10y2 + ψ11y3 + ψ12y1y2
+ ψ13y1y3 + ψ14y2y3 + ψ15y1y2y3.
There are 23 parameters, including 10 sensitivity parameters. Clearly, some
model simpliﬁcation is needed. We can reduce the number of sensitivity pa-
rameters used in a sensitivity analysis by ﬁxing some of these at zero. For
example, we could consider the following simpliﬁed MDM,
logit(π2)
=
ψ0 + ψ1y1 + ψ2y2
logit(π3)
=
ψ3 + ψ4y1 + ψ5y2 + ψ6y1y2 + ψ7y3.
This model now has only two sensitivity parameters, ψ2 and ψ7. They are not
identiﬁed by data, but ﬁxing their values identiﬁes the full-data model.
Even if we reduce the number of sensitivity parameters, there are still 13
parameters that need to be identiﬁed by the observed data. To reduce the
number of identiﬁed parameters, we can consider a semiparametric speciﬁca-
tion with a nonparametric full-data response model and a simpler, parametric

180
NONIGNORABLE MISSINGNESS
missing data mechanism. For example, the hazard of dropout can be simpliﬁed
to allow dependence only on the most current and most recent outcome,
logit(π2)
=
ψ0 + ψ1y1 + ψ2y2
logit(π3)
=
ψ0 + ψ1y2 + ψ2y3.
The simplifying assumptions in this missing data mechanism are: (1) non-
future dependence; (2) the hazard of dropout is constant over time; (3) dropout
at time j only depends on the response at time j and the response at the previ-
ous time j−1; (4) the eﬀect of the current and past response on dropout is the
same at each dropout time (i.e., the coeﬃcients of yj and yj−1 do not depend
on j). We have further reduced the number of sensitivity parameters via the
fourth simplifying assumption that equates the sensitivity parameter in the
regression for π2 and for π3. This model has one sensitivity parameter, ψ2 and
only nine parametrs for the data to identify. This model is no longer nonpara-
metric as we have assumed a (simpler) parametric form for the MDM. Given
that we now have a parametric model for the MDM, the sensitivity param-
eter may be weakly identiﬁed by data. Speciﬁcation of such semiparametric
models, particularly in higher dimensions, and exploration of their properties,
including the degree of identiﬁcation of the potential sensitivity parameters,
is an area of ongoing work.
2
We close out our discussion of selection models by providing some guidance
on posterior sampling.
8.3.8 Posterior sampling strategies
We provide some strategies for posterior sampling for selection models. First,
we point out that by using data augmentation to ﬁll in ymis, posterior sampling
of ψ and θ proceeds using complete data techniques for the missing data
mechanism and the full-data response model, respectively, i.e., sampling p(ψ |
y, r) and p(θ | y) ‘separately’. As a result, intractable integrals, such as those
found in (8.9), do not need to be evaluated directly. The main new computing
issue to discuss here — in the sense that it is speciﬁc to selection models —
is sampling the missing data, ymis from p(ymis | yobs, r, θ, ψ).
For binary response data, individual components Yij in the distribution
p(ymis | yobs, r, θ, ψ) will be sampled from a Bernoulli distribution with prob-
ability
P(Yij = 1 | y−ij, r, θ, ψ) =
p(r | y−ij, yij = 1, ψ)p(y−ij, yij = 1 | θ)
1
y=0 p(r | y−ij, yij = y, ψ)p(y−ij, yij = y | θ)
,
where y−ij corresponds to the vector of responses with yij removed. Consid-
erable simpliﬁcation of this expression for speciﬁc models is typical.
For continuous responses, a simple Metropolis-Hastings algorithm can be
used to sample from p(ymis | yobs, r, θ, ψ). At iteration k, sample a candidate

MIXTURE MODELS
181
y(k)
mis from p(y(k)
mis | yobs, θ), i.e., the conditional distribution of ymis based
on only the full-data response model. This conditional distribution typically
takes a known form (see Chapter 6). Accept this candidate with probability
p⋆= min

1, p(r | y(k)
mis, yobs, ψ)
p(r | y(k−1)
mis
, yobs, ψ)
!
.
If this candidate distribution results in low acceptance, a Laplace approxima-
tion to p(ymis | yobs, r, θ, ψ) can be used instead.
For certain models with continuous responses, draws from the predictive
distribution (or a suitably data augmented version of this distribution) can
be taken directly (e.g., the Heckman selection model described earlier).
8.3.9 Summary of pros and cons of selection models
The main advantages of SMs are (1) the ability to directly specify both the full
data response model and the missing data mechanism using familiar models;
and (2) the correspondence between the speciﬁed missing data mechanism and
the MAR-MNAR hierarchy. As a result, the primary parameters of interest are
explicit in the model, and the nature of the dependence between missingness
and response has a transparent representation.
The main disadvantage in parametric selection models — for most practical
situations — is the inability to partition the full-data parameter vector ω into
identiﬁed and unidentiﬁed components as discussed in Section 8.2. In partic-
ular, the extrapolation distribution p(ymis | yobs, r, ωE) is generally identiﬁed
for parametric selection models, and the distribution of observables is indexed
by parameters that govern the missing data mechanism. As a consequence,
any strategy for sensitivity analysis based on reﬁtting the model under diﬀer-
ent missing data assumptions will have the unfortunate side eﬀect of changing
the ﬁt to observed data as well. Semiparametric selection models oﬀer a more
ﬂexible alternative, and are revisited in Chapter 9.
8.4 Mixture models
8.4.1 Background, speciﬁcation, and identiﬁcation
The development of mixture models for handling missing data can be traced
at least to Rubin (1977), who described methods for using informative pri-
ors in surveys to capture and use subjective information about nonresponse.
Since then a number of key papers have expanded mixture models for han-
dling informative dropout in longitudinal data. Little (1993, 1994) developed a
general theory for ﬁnite mixtures of multivariate distributions in discrete-time
settings. For longer follow-up times, mixtures of random eﬀects models proved
useful (Wu and Bailey, 1988, 1989; Mori et al., 1992; Hogan and Laird, 1997a).

182
NONIGNORABLE MISSINGNESS
Fitzmaurice and Laird (2000) developed moment-based approaches based on
mixtures of generalized linear models. Roy (2003) and Roy and Daniels (2007)
addressed the issue of having a large number of dropout categories by using
mixtures over latent classes. Hogan et al. (2004a) developed approaches for
continuous dropout times based on mixtures of varying coeﬃcient models.
Reviews of model-based approaches can be found in Little (1995), Hogan and
Laird (1997b), Kenward and Molenberghs (1999), Fitzmaurice (2003), and
Hogan et al. (2004b).
Another key thread of research concerns model identiﬁcation. Here, Molen-
berghs and colleagues have developed an important body of work for the case
of discrete-time dropout (cf. Molenberghs et al., 1998; Kenward et al., 2003),
much of which is described in this section.
The mixture model approach factors the full-data model as
p(y, r | x, ω) = p(y | r, x, ω) p(r | x, ω).
(8.16)
The full-data response distribution is obtained by averaging (8.16) over the
distribution of r,
p(y | x, ω)
=
	
r∈R
p(y | r, x, ω) p(r | x, ω),
where R is the sample space of R (see also Section 5.9.2). In this section,
we describe some speciﬁc formulations that give a broad representation of
the settings where the models can be used, and describe formal methods for
identifying them. In general, when R is discrete, the component distributions
comprise the set {p(y | r, x) : r ∈R}. When missingness is caused by dropout
at some time U, then the component distributions p(y | u, x) may also be a
discrete set of distributions, or may be speciﬁed in terms of a continuous u.
To maintain focus on key ideas related to speciﬁcation and identiﬁcation, we
defer discussion of covariates to Section 8.4.7.
Mixture models for either discrete or continuous dropout time are under-
identiﬁed and require speciﬁc constraints for ﬁtting to data. There are various
approaches to identifying the models; in this section, we focus on strategies
that divide the full-data distribution into identiﬁed and nonidentiﬁed compo-
nents (see the extrapolation factorization (8.1)).
In the case of discrete-time dropout, the MAR assumption is used as a
basis for identifying the full-data distribution and for building a larger class of
models that accommodate MNAR. For continuous-time dropout (or discrete-
time dropout with large number of support points), models can be identiﬁed
by making assumptions about the mean of the full-data response as a function
of time; e.g., assuming E{Y (t) | U = u} is linear in t, with intercept and
slope depending on U. Usually one assumes that given dropout at U = u,
mean response prior to and after U — i.e., E{Y (t) | U = u, t < u} and
E{Y (t) | U = u, t ≥u} — are either equivalent or related through some

MIXTURE MODELS
183
known function; for example,
E{Y (t) | U = u, t > u} = q(u, t)E{Y (t) | U = u, t ≤u}
for some known q(u, t). In this example, q(u, t) cannot be identiﬁed, but ﬁxing
its value, or placing a prior on it, identiﬁes the full-data model.
8.4.2 Identiﬁcation strategies for mixture models
In this section we describe several approaches to model identiﬁcation, empha-
sizing the case where follow-up times are discrete.
Identiﬁcation via MAR constraints
Although the usual goal of ﬁtting a mixture model is to represent a MNAR
mechanism, for the purposes of discussing model identiﬁcation it helps to
begin by showing how to impose the MAR condition in a mixture model
having discrete measurement times and monotone dropout. In Section 5.5,
we saw that for the case of monotone dropout, MAR can be represented in
terms of the hazard of dropout. For pattern mixture models, Molenberghs
et al. (1998) show that MAR is equivalent to the available case missing value
(ACMV) constraint (Little, 1994).
To simplify notation in our discussion of discrete-time mixture models, we
write the conditional distribution of Y given follow-up time S = k as
pk(y)
=
p(y | S = k).
Similarly, we write p≥k(y) = p(y | S ≥k). In the case of discrete-time follow-
up with monotone dropout, MAR has a speciﬁc characterization given by the
following theorem.
Theorem 8.1. MAR for discrete-time pattern mixture models under mono-
tone dropout.
Let Y1, . . . , YJ denote the full-data responses, with measurements scheduled at
times t1, . . . , tJ. Without loss of generality, assume tj = j. Let S ∈{1, 2, . . ., J}
denote follow-up time, with S = J for those with complete follow-up. MAR
holds if and only if, for each j ≥2 and k < j,
pk(yj | y1, . . . , yj−1)
=
p≥j(yj | y1, . . . , yj−1).
(8.17)
The proof can be found in Molenberghs et al. (1998).
2
The theorem states that the conditional distribution of Yj given past re-
sponses for those whose follow-up terminates at some time prior to j – i.e.,
pk(yj | y1, . . . , yj−1) for some k < j – is equivalent to the corresponding dis-
tribution for those who have observed data at or beyond j – i.e., p≥j(yj |
y1, . . . , yj−1). Those still in follow-up at time j are an aggregation of those
having S ∈{j, j+1, j+2, . . ., J}. Under monotone dropout, the RHS of (8.17)
is identiﬁable from observed data, while the LHS is not.

184
NONIGNORABLE MISSINGNESS
Tables 8.2 and 8.3 illustrate Theorem 8.1 using a schematic for the case
J = 4. Table 8.2 shows the identiﬁed observed data distributions in a general
pattern mixture model. Nonidentiﬁed components of the full-data distribution
are denoted by question marks (‘?’). In Table 8.3, they are ﬁlled in using the
MAR constraint given in Theorem 8.1. The format of this table is used to
illustrate several models and identiﬁcation strategies discussed in this section.
Table 8.2 Identiﬁable components of pattern-mixture model with J = 4 under mono-
tone dropout. Nonidentiﬁed distributions labeled using ‘?’.
j = 2
j = 3
j = 4
S = 1
?
?
?
S = 2
p2(y2|y1)
?
?
S = 3
p3(y2|y1)
p3(y3|y1, y2)
?
S = 4
p4(y2|y1)
p4(y3|y1, y2)
p4(y4|y1, y2, y3)
Table 8.3 Schematic representation of Theorem 8.1, illustrating identiﬁcation of
pattern-mixture model with J = 4 and monotone dropout using MAR constraints.
Distributions above the dividing line are identiﬁed via MAR, using an aggregation
of distributions in the same column appearing below the dividing line.
j = 2
j = 3
j = 4
S = 1
p≥2(y2|y1)
p≥3(y3|y1, y2)
p4(y4|y1, y2, y3)
S = 2
p2(y2|y1)
p≥3(y3|y1, y2)
p4(y4|y1, y2, y3)
S = 3
p3(y2|y1)
p3(y3|y1, y2)
p4(y4|y1, y2, y3)
S = 4
p4(y2|y1)
p4(y3|y1, y2)
p4(y4|y1, y2, y3)
To impose the MAR condition, one approach is to assume a paramet-
ric model for observables, and then use the MAR assumption to constrain
the distribution of the missing data. Although this chapter is primarily con-
cerned with MNAR models, we advocate embedding an MAR model within a
broader class of MNAR speciﬁcations, so as to make clear the assumptions or
parameter constraints that diﬀerentiate MNAR from MAR. Theorem 8.1 mo-
tivates this approach in the pattern mixture context and is discussed further
in Chapter 9. The examples later in this chapter illustrate the application of
the theorem to speciﬁc mixture models.

MIXTURE MODELS
185
Interior family constraints
Consider the case of discrete-time measurement at times j = 1, . . . , J, with
individual-speciﬁc follow-up time denoted by S = 
j Rj. We can identify the
nonidentiﬁed distributions pk(yj | y1, . . . , yk−1) for j ≥2 and k < j using the
constraints
pk(yj | y1, . . . , yj−1) =
J
	
s=j
∆jks ps(yj | y1, . . . , yj−1),
where, for any k < j, the ∆jks are ﬁxed weights satisfying J
s=j ∆jks = 1.
Constraints with this form (linear combinations of the corresponding iden-
tiﬁed distributions) are called ‘interior family constraints’ (Kenward et al.,
2003). They can be used to represent commonly used mixture model identi-
ﬁcation strategies such as ‘complete case’, ‘nearest-neighbor’, and ‘available
case’ constraints, which we detail below. In the absence of these types of con-
straints, the ∆jks parameters satisfy the conditions of Deﬁnition 8.1 and can
be used as sensitivity parameters.
With complete case missing value (CCMV) constraints, the distributions
{pk(yj | y1, . . . , yj−1) : j ≥2, k < j}
(8.18)
for those who drop out are equated to the distribution of the completers,
pJ(yj | y1, . . . , yj−1). For example, if J = 4, then the unindentiﬁed distribution
p1(y2 | y1) is identiﬁed by equating it to p4(y2 | y1). This corresponds to
∆jks =

0
s < 4
1
s = 4
for k = 1, 2, 3 and j > k.
Nearest-neighbor or adjacent-pattern constraints equate each of the uniden-
tiﬁed distributions in (8.18) to the ‘nearest-neighbor’ — in terms of dropout
pattern — having an identiﬁed distribution for (Yj | Y1, . . . , Yj−1). Continuing
with the example above, p1(y2 | y1) is identiﬁed under nearest-neighbor con-
straints by equating it to p2(y2 | y1), because S = 2 is the ‘nearest neighbor’
to S = 1. Thus,
∆jks =

0
s ̸= j + 1
1
s = j + 1 .
Rather than rely on nearest-neighbor or complete-case patterns, the avail-
able case missing value (ACMV) constraints use all available patterns where
pk(yj | y1, . . . , yj−1) is identiﬁed by data. Molenberghs et al. (1998) show
that for discrete-time monotone dropout, MAR as stated in Theorem 8.1 is
equivalent to ACMV.
Table 8.4 gives the general interior family constraint identiﬁcation strategy
for J = 4. A more detailed illustration of interior family constraints, including

186
NONIGNORABLE MISSINGNESS
expressions for ∆jks under MAR, is given in Example 8.4 and the discussion
following the example.
Table 8.4 Identiﬁcation for discrete-time pattern mixture model with J = 4 and
monotone dropout using interior family constraints. Distributions identiﬁed by ob-
served data appear below the dividing line. Nonidentiﬁed distributions appear above
the dividing line and are equated to weighted averages of identiﬁed distributions in
the same column.
j = 2
j = 3
j = 4
S = 1
4
	
s=2
∆21s ps(y2|y1)
4
	
s=3
∆31s ps(y3|y1, y2)
p4(y4|y1, y2, y3)
S = 2
p2(y2|y1)
4
	
s=3
∆32s ps(y3|y1, y2)
p4(y4|y1, y2, y3)
S = 3
p3(y2|y1)
p3(y3|y1, y2)
p4(y4|y1, y2, y3)
S = 4
p4(y2|y1)
p4(y3|y1, y2)
p4(y4|y1, y2, y3)
Non-future dependence missing value restrictions
Non-future dependence missing value restrictions (Kenward et al., 2003) al-
low the probability of dropout at j to depend on the current (but possibly
unobserved) response Yj, but not on the future values Yj+1, . . . , YJ (see also
Section 8.3.4). The constraint is more intuitive when given in terms of dropout
time U = 1 + 
j Rj (= 1 + S). It identiﬁes all the unidentiﬁed conditional
distributions in each pattern except for the one corresponding to current but
unobserved value of the response, p(yj | y1, . . . , yj−1, u = j) (here, ‘current’ is
relative to dropout time). Beyond that, for each j ≥2 and k < j,
p(yj | y1, . . . , yj−1, u = k)
=
p(yj | y1, . . . , yj−1, u ≥j)
(8.19)
(recall that completers have U = J + 1). In terms of the follow-up time
S = 
j Rj, the non-future dependence restriction leaves
pj−1(yj | y1, . . . , yj−1)
unidentiﬁed for 2 ≤j ≤J, and imposes the constraint
pk(yj | y1, . . . , yj−1)
=
p≥j−1(yj | y1 . . . , yj−1)
(8.20)
for each j > 2 and k < j −1. This constraint looks very similar to the MAR
restriction in Theorem 8.1, except that this condition holds for j > 2 (not
j ≥2) and k < j −1 (not k < j). Hence the MAR restriction is a special case.
The non-future dependence constraints are illustrated in Table 8.5.

MIXTURE MODELS
187
Table 8.5 Schematic representation of non-future dependence missing value con-
straints for the pattern-mixture model with J = 4 and monotone dropout. Distribu-
tions below the dividing line are identiﬁed by observed data.
j = 2
j = 3
j = 4
S = 1
?
p≥2(y3|y1, y2)
p≥3(y4|y1, y2, y3)
S = 2
p2(y2|y1)
?
p≥3(y4|y1, y2, y3)
S = 3
p3(y2|y1)
p3(y3|y1, y2)
?
S = 4
p4(y2|y1)
p4(y3|y1, y2)
p4(y4|y1, y2, y3)
A primary motivation for using this set of constraints is that in a sensitivity
analysis, only one univariate distribution in each pattern is left unidentiﬁed;
the MAR restriction provides a starting point for conducting sensitivity anal-
ysis within this class of restrictions. These restrictions are equivalent to the
non-future dependence missing data mechanism introduced in the context of
selection models in Section 8.3.4.
Identiﬁcation via extrapolation
Unless the number of distinct dropout times is discrete and small (say 3 or
4), the number of constraints and/or sensitivity parameters potentially can
become unmanageable, especially for saturated or nonparametric models, and
structural constraints may be needed.
One approach is to assume that the mean response as a function of time
follows a known function; this is the approach taken by several authors such as
Wu and Bailey (1989), Hogan and Laird (1997a), and Fitzmaurice and Laird
(2000). As an example, suppose the number of measurement occasions is large
but common across subjects. Assume (Y1, . . . , YJ | U = u) ∼N(µ(u), Σ(u)).
One possible set of constraints is to assume the mean follows
µ(u)
j
= f(tj; β(u)),
where f(tj; β(u)) has a known form such as β(u)
0
+ tjβ(u)
1 , and the variance
has some simpliﬁed form such as Σ(u) = Σ. Because the variance matrix is
assumed common across pattern, it will be identiﬁed. If the mean is linear over
time within pattern, it will be fully identiﬁed for patterns with observations
at two or more time points.
These models are of course restrictive, and there is no information in the
data to verify the structural constraints. As with the discrete-time models,
we can separate parameters ωE and ωO indexing the distributions p(ymis |

188
NONIGNORABLE MISSINGNESS
yobs, r, ωE) and p(yobs, r | ωO). For example, instead of assuming that mean
response within pattern follows a single line before and after dropout, we
might assume that
f(t, β(u)) = β(u)
0
+ β(u)
1
t + ∆(t −u)+,
where a+ = aI{a > 0} is the positive part of a. This larger model assumes
that, conditionally on U = u, slope is β(u)
1
for t ≤u (prior to dropout),
and β(u)
1
+ ∆for t > u (following dropout). Unlike the models described for
the discrete case, setting ∆= 0 does not imply MAR. But this model does
admit a reparameterization ξ(ω) = (ξS, ξM), where ξS = ∆is a sensitivity
parameter. In Chapter 10, we illustrate this approach in detail using data
from the pediatric AIDS trial.
8.4.3 Mixture models with discrete-time dropout
Continuous responses
To understand the PMM for continuous data in discrete time, and its connec-
tion to selection models, it is useful to revisit the model for the bivariate case
(Little, 1994). We rely on speciﬁcations using mixtures of normal distribu-
tions, which tend to be mathematically tractable. For continuous responses,
there is no reason to be conﬁned to mixtures of normal distributions, although
this area is largely open to further investigation.
In order to see the connection between mixture models and selection mod-
els, and the key diﬀerences between mixture models that assume MAR vs.
those that allow MNAR, we ﬁrst revisit Example 5.9. In the bivariate case, it
is straightforward to impose the MAR constraint in a way that keeps the com-
ponent distributions normal. However, for responses with dimension greater
than two, pattern mixture models are more easily constructed by assuming
normality within pattern for observed data distributions, but not necessarily
for the full-data distributions.
Example 8.3. Pattern mixture model for bivariate response with missing Y2
(Example 5.9 revisited).
Consider a full-data model for the joint distribution of (Y1, Y2, R), such that
(Y1, Y2)T is the bivariate full-data response, and R is a binary indicator of
whether Y2 is observed. Recall from Chapter 5 that the full-data PMM for
bivariate normal data follows
(Y1, Y2)T | R = r
∼
N(µ(r), Σ(r))
R
∼
Ber(φ),
(8.21)
where, for pattern r ∈{0, 1}, the parameters are {µ(r)
1 , µ(r)
2 , σ(r)
11 , σ(r)
22 , σ(r)
12 }.
Recall also that for pattern r, we can reparameterize the model in terms of

MIXTURE MODELS
189
the marginal distribution of Y1 and the conditional distributions of Y2 given
Y1, such that
Y1 | R = r
∼
N(µ(r)
1 , σ(r)
11 )
Y2 | Y1, R = r
∼
N(β(r)
0
+ β(r)
1 Y1, σ(r)
2|1),
where
φ(r)
=
(µ(r)
1 , σ(r)
11 , β(r)
0 , β(r)
1 , σ(r)
2|1)
=
g(µ(r)
1 , σ(r)
11 , µ(r)
2 , σ(r)
22 , σ(r)
12 ).
Clearly the parameters {β(0)
0 , β(0)
1 , σ(0)
2|1} cannot be identiﬁed from the ob-
servables, but they can be identiﬁed with parameter constraints. For simplic-
ity, let us further suppose that Σ(1) = Σ(0) = Σ, so that variance compo-
nents are equal across pattern. Because β(r)
1
= σ(r)
21 /σ(r)
11 and σ(r)
2|1 = σ(r)
22 −
(σ(r)
21 )2/σ(r)
11 , we have β(0)
1
= β(1)
1
= β1 and σ(0)
2|1 = σ(1)
2|1 = σ2|1. This still leaves
β(0)
0
(equivalently, µ(0)
2 ) unidentiﬁed.
From Theorem 8.1, MAR is satisﬁed when p(y2 | y1, r = 1) = p(y2 | y1, r =
0). Equality of the conditional distributions implies equality of conditional
means E(Y2 | Y1 = y1, R = r) for r = 0, 1 and for all y1. Multivariate
normality within pattern gives
E(Y2 | Y1 = y1, R = r)
=
µ(r)
2
+ σ21
σ11
(y1 −µ(r)
1 )
=

µ(r)
2
−σ21
σ11
µ(r)
1

+ σ21
σ11
y1.
(8.22)
Hence equality of conditional means is satisﬁed when the intercept term in
(8.22) is equal for r = 0, 1, i.e., when
µ(0)
2
−β1µ(0)
1
=
µ(1)
2
−β1µ(1)
1 .
A more intuitive representation is
µ(0)
2
=
µ(1)
2
+ β1(µ(0)
1
−µ(1)
1 )
(8.23)
=
E(Y2 | Y1 = µ(0)
1 , R = 1),
which is the predicted value, at Y1 = µ(0)
1 , of the missing Y2 based on the
regression of Y2 on Y1 among those with (Y1, Y2)T observed. Under MAR,
µ(0)
2
is entirely a function of identiﬁed parameters.
In Example 5.9, we saw that the selection model implied by this mixture
of normals is
logit{P(R = 1 | y1, y2)}
=
ψ0 + ψ1y1 + ψ2y2,

190
NONIGNORABLE MISSINGNESS
where, relevant to this example,
ψ2
=
1
σ2
2|1

µ(0)
2
−µ(1)
2
−β1(µ(0)
1
−µ(1)
1 )

.
By substituting the expression for µ(0)
2
from (8.23) into the expression for
ψ2 above, we get ψ2 = 0. Hence MAR implies ψ2 = 0. Not surprisingly, the
converse also is true. Furthermore, it is straightforward to expand this model
to allow MNAR while maintaining normality within pattern.
Referring to (8.23), let µ(0)
2:MAR = µ(1)
2 +β1(µ(0)
1 −µ(1)
1 ) denote the identiﬁed
value of µ(0)
2
under MAR. To embed this model in a larger one that allows
MNAR, we can introduce a parameter ∆such that
µ(0)
2
=
µ(0)
2:MAR + ∆.
Because this amounts to a shift in the marginal mean of Y2 in pattern R = 0,
bivariate normality of (Y1, Y2)T given R = 0 is preserved.
2
The mixture of normals model for J = 2 is instructive for understanding
the connections between mixture models and their implied selection models;
however, as we see in the examples that follow, mixture models constructed
from multivariate normal component distributions with J = 3 cannot typically
be constrained in a way that is consistent with an MAR assumption.
A better approach is to assume a normal distribution for the observed data
within pattern, and then to impose MAR constraints that will identify the
extrapolation distribution p(ymis | yobs, r), which generally will not be nor-
mally distributed within pattern under MAR. The examples below illustrate
two approaches to model construction. In Example 8.4, we assume observed
responses within pattern are multivariate normal; in Example 8.5, we use a
diﬀerent parameterization to preserve conditional normality in the full-data
distributions. The two parameterizations suggest diﬀerent approaches to sen-
sitivity analysis and MNAR formulations.
Example 8.4. Pattern mixture model identiﬁed using interior family con-
straints.
Here we use the normal distribution for observed data, and use the MAR
assumption to identify the distribution of missing data. The case J = 3 is
used to illustrate. Let (Y1, Y2, Y3)T denote the full data. We assume mono-
tone dropout, and we parameterize the model in terms of the number of
observations, S = 
j Rj (refer to Deﬁnition 5.3). The full-data distribution
is factored as
p(y1, y2, y3, s) = ps(y1, y2, y3) p(s),
where S follows a multinomial distribution with probabilities φs = P(S = s).
The observed data response distributions within each pattern are speciﬁed

MIXTURE MODELS
191
using normal distributions,
Y1 | S = 1
∼
N(µ(1), σ(1)),
(Y1, Y2)T | S = 2
∼
N(µ(2), Σ(2)),
(Y1, Y2, Y3)T | S = 3
∼
N(µ(3), Σ(3)).
Now consider identifying ps(ymis | yobs) under MAR. Start with S = 1. We
need to identify p1(y2, y3 | y1), which can be factored as
p1(y2, y3 | y1)
=
p1(y2 | y1) p1(y3 | y2, y1).
The MAR constraint (8.17) implies
p1(y2 | y1)
=
p≥2(y2 | y1),
p1(y3 | y2, y1)
=
p3(y3 | y2, y1).
The distribution p≥2(y2 | y1) is a mixture of the normal distributions from
patterns S = 2 and S = 3,
p≥2(y2 | y1)
=
φ2 p2(y2 | y1) p2(y1) + φ3 p3(y2 | y1) p3(y1)
φ2 p2(y1) + φ3 p3(y1)
.
(8.24)
Referring to Table 8.4, this corresponds to identiﬁcation of p1(y2 | y1) as
3
s=2 ∆21s ps(y2 | y1), where
∆21s
=
φsps(y1)
φ2p2(y1) + φ3p3(y1).
(8.25)
Hence the distribution p1(y2 | y1) is in general a mixture of normals, even
though p2(y2 | y1) and p3(y2 | y1) are normal (by speciﬁcation). If we impose
the additional constraint that p2(y2 | y1) = p3(y2 | y1), with both being
normal, then p≥2(y2 | y1) clearly will be normal as well.
2
To see how the identiﬁcation strategy for the model in Example 8.4 uses
the interior family constraints, we focus again on those with S = 1, the dis-
tribution p1(y2 | y1) can be identiﬁed in general by the mixture
p1(y2 | y1)
=
∆p2(y2 | y1) + (1 −∆) p3(y2 | y1).
(8.26)
The parameter ∆is not identiﬁable from data and is a sensitivity param-
eter. Setting ∆= 1 is a ‘nearest-neighbor’ identiﬁcation scheme, equating
the unidentiﬁed p1(y2 | y1) to the corresponding distribution from those with
S = 2. Setting ∆= 0 identiﬁes p1(y2 | y1) by equating it to the corresponding
distribution among those with complete follow-up; i.e., p3(y2 | y1). This is
known as the complete case missing value (CCMV) restriction. Under MAR,
as we see from (8.24) and (8.25),
∆
=
φ2p2(y1)
φ2p2(y1) + φ3p3(y1).

192
NONIGNORABLE MISSINGNESS
Apart from constraints like these, a sensitivity analysis can be based on vary-
ing ∆over [0, 1], or a prior distribution can be assigned to ∆. Although most
values of ∆technically represent MNAR models (the MAR constraint above
being the exception), the full-data model space under this parameterization
is indexed by a single parameter, and extrapolation distributions are conﬁned
to functions of the observed data distributions.
In the next example, we show how to parameterize the PMM to allow
more global departures from MAR under monotone dropout. The observed
data distribution is given as follows. At j = 1 we assign separate normal
distributions to Y1 | S = s for s = 1, . . . , J. Then at each time point j ≥
2, we assume the conditional distribution p≥j(yj | y1, . . . , yj−1) — i.e., the
distribution of Yj | Y1, . . . , Yj−1 among those with S ≥j — follows a normal
distribution with mean and variance given by
E(Yj | Y1, . . . , Yj−1)
=
α(≥j)
0
+
j−1
	
l=1
α(≥j)
l
Yl
var(Yj | Y1, . . . , Yj−1)
=
τ (≥j)
j
and that
pk(yj | y1, . . . yj−1) = p≥j(yj | y1, . . . yj−1), k ≥j.
Consequently, for any j, the joint distribution of (Y1, . . . , Yj)T | S = j is
multivariate normal. For example,
p3(y1, y2, y3)
=
p3(y3 | y1, y2) p3(y2 | y1) p3(y1)
=
p≥3(y3 | y1, y2) p≥2(y2 | y1) p3(y1),
where the ﬁrst line is just a factorization of the joint distribution given S = 3,
and the second line replaces each factor with its speciﬁed model.
The missing response data distributions also are speciﬁed using normal dis-
tributions. For each j ≥2, and for each s < j, we assume ps(yj | y1, . . . , yj−1)
is normal. It follows that the joint distribution ps(y1, . . . , yj), for any s and j,
is multivariate normal. Table 8.6 shows the identiﬁcation strategy.
Imposing a normal distribution on the missing data is a stronger assump-
tion than in Example 8.4; however, it does lend some consistency to the model
speciﬁcation by allowing each full-data distribution within pattern to be nor-
mal, and it expands the range of sensitivity analyses. Both attributes are
illustrated in the following example.
Example 8.5. Pattern mixture model based on normal distributions within
pattern, with departure from MAR.
Consider again the case J = 3 with no covariates. Following Table 8.6, identi-
ﬁable observed data distributions are p1(y1), p2(y1), p3(y1), p≥2(y2 | y1), and
p≥3(y3 | y1, y2) (which is equal to p3(y3 | y1, y3)). We specify these directly,

MIXTURE MODELS
193
Table 8.6 Schematic representation of mixture model having multivariate normal
distribution within pattern, for J = 4. All densities in the table are normal with
parameters depending on the subscript of p(·). Identiﬁed observed data distributions
fall below the dividing line, and nonidentiﬁed missing distributions are above.
j = 1
j = 2
j = 3
j = 4
S = 1
p1(y1)
p1(y2|y1)
p1(y3|y1, y2)
p1(y4|y1, y2, y3)
S = 2
p2(y1)
p≥2(y2|y1)
p2(y3|y1, y2)
p2(y4|y1, y2, y3)
S = 3
p3(y1)
p≥2(y2|y1)
p≥3(y3|y1, y2)
p3(y4|y1, y2, y3)
S = 4
p4(y1)
p≥2(y2|y1)
p≥3(y3|y1, y2)
p4(y4|y1, y2, y3)
but make the further assumption that certain unidentiﬁed distributions are
normal. The distribution of (Y obs | S) is given by
Y1 | S = 1
∼
N(µ(1), σ(1))
Y1 | S = 2
∼
N(µ(2), σ(2))
Y1 | S = 3
∼
N(µ(3), σ(3))
Y2 | Y1, S = 2
Y2 | Y1, S = 1

∼
N(α(≥2)
0
+ α(≥2)
1
Y1, τ(≥2)
2
)
Y3 | Y1, Y2, S = 3
∼
N(β(3)
0
+ β(3)
1 Y1 + β(3)
2 Y2, τ(3)
3 ).
To round out the full-data response distribution, we specify (Y mis | S) as
follows:
Y2 | Y1, S = 1
∼
N(α(1)
0
+ α(1)
1 Y1, τ(1)
2 )
Y3 | Y1, Y2, S = s
∼
N(β(s)
0
+ β(s)
1 Y1 + β(s)
2 Y2, τ(s)
3 )
(s = 1, 2).
The general MAR constraints require
p1(y2 | y1)
=
p≥2(y2 | y1),
(8.27)
p1(y3 | y1, y2)
=
p3(y3 | y1, y2),
(8.28)
p2(y3 | y1, y2)
=
p3(y3 | y1, y2).
(8.29)
Condition (8.27) can be satisﬁed by equating means and variances of p1(y2 |
y1) and p≥2(y2 | y1),
α(1)
0
+ α(1)
1 y1
=
α(≥2)
0
+ α(≥2)
1
y1
(∀y1 ∈R),
τ(1)
2
=
τ (≥2)
2
.
The ﬁrst equality above holds only when α(1)
0
= α(≥2)
0
and α(1)
1
= α(≥2)
1
.

194
NONIGNORABLE MISSINGNESS
Similar constraints can be imposed on the parameters indexing p1(y3 | y1, y2)
and p2(y3 | y1, y2) to fully satisfy MAR.
Writing the model in this way makes it fairly simple to embed the MAR
speciﬁcation in a large class of MNAR models indexed by parameters ∆0, ∆1,
and ∆2 that measure departures from MAR. For example, to characterize
MNAR in terms of departures from (8.27), write
α(1)
0
=
α(≥2)
0
+ ∆0
α(1)
1
=
α(≥2)
1
+ ∆1
log τ(1)
2
=
log τ(≥2)
2
+ ∆2





(8.30)
Assuming constraints (8.28) and (8.29) hold, dropout is MAR when ∆0 =
∆1 = ∆2 = 0. None of the ∆parameters appears in the observed data like-
lihood. In general, a separate ∆is needed for each model constraint, but in
practice it is necessary to limit the dimensionality of these. Our analysis of
the Growth Hormone study in Section 10.2 provides methods for doing so. 2
In Chapter 9, we formalize this structure for fully Bayesian model speciﬁ-
cations by writing (8.30) as a function
ξS
=
h(ξM, ∆),
where ξS are the (nonidentiﬁed) sensitivity parameters in the full-data model,
ξM are (identiﬁed) parameters indexing the implied observed data model, and
∆captures departures from MAR. In many cases, the h function represents
the missing data mechanism, and makes explicit how assumptions or priors
are being used to infer the full-data model (also see Rubin, 1977).
Examples 8.4 and 8.5 diﬀer not only in how the observed data distribution is
speciﬁed, but also how the missing data are extrapolated. In Example 8.4, the
missing data distribution is identiﬁed using a mixture of normals constructed
using a weighted average of observed data distributions; see (8.26). The range
of possibilities for extrapolating missing data is conﬁned to this structure;
though it may be fairly limited in scope, it is simple for practical settings
because of the small number of unidentiﬁed parameters (for J = 3, there is
only one).
In Example 8.5, parametric distributions are assumed for the extrapola-
tions. This imposes untestable distributional assumptions, but allows for con-
siderable ﬂexibility in extrapolating the missing data either by ﬁxing values
or assigning priors to parameters like (∆0, ∆1, ∆2) in (8.30).
A key consideration for model speciﬁcation, including speciﬁcation of priors
or ranges for sensitivity parameters, is understanding the physical meaning of
sensitivity parameters in context. This is discussed further in Chapter 9. In
addition, in Section 10.2, we address speciﬁc related issues (model speciﬁca-
tion, dimensionality of sensitivity parameters, formulation and interpretation
of priors, and calibration of sensitivity analyses) in a detailed analysis of data
from the growth hormone study.

MIXTURE MODELS
195
Binary responses with discrete-time dropout
Pattern mixture models can be used for binary data as well. Some key ref-
erences here include Baker and Laird (1988), Ekholm and Skinner (1998),
Birmingham and Fitzmaurice (2002), and Little and Rubin (2002). In this
section we describe pattern mixture models for the bivariate case J = 2 and
for higher-dimensional settings (J > 2). For each model, we show how to
impose MAR constraints using Theorem 8.1, and introduce parameters that
index a larger class of MNAR models. Many of the key ideas that apply to
continuous-data models also apply here.
Example 8.6. Pattern mixture model for bivariate binary data.
The bivariate case with missingness in Y2 provides a useful platform for under-
standing model speciﬁcation with binary responses. As with the continuous
case, let Y = (Y1, Y2)T denote the full-data response, and let S denote the
number of observed measurements (i.e., S = 2 if Y2 is observed and S = 1 if
it is missing). The entire full-data distribution p(y, s | ω) can be enumerated
as in Table 8.1, and viewed as a multinomial distribution with seven distinct
parameters.
The pattern mixture model factors the joint distribution as
p(y1, y2 | s, α) p(s | φ),
where, referring again to Table 8.1, (α, φ) = g(ω) is just a reparameterization
of ω. A simple PM speciﬁcation is
Y2 | Y1, S = s
∼
Ber(µ(s)
2|1)
Y1 | S = s
∼
Ber(µ(s)
1 )
(8.31)
for s ∈{1, 2}, with P(S = s) = φs. Regression models can be used to structure
the parameters for the responses,
logit(µ(s)
2|1)
=
α(s)
0
+ α(s)
1 y1,
logit(µ(s)
1 )
=
β(s),
for s ∈{1, 2}. The seven unique parameters in this PMM comprise three
regression parameters each for k = 1, 2, and the Bernoulli probability P(S =
2) = φ2 = 1 −φ1.
Without further assumptions, α(1)
0
and α(1)
1
cannot be identiﬁed; indeed,
α(1)
0
and α(1)
1
are sensitivity parameters. In this simple model, MAR holds
when p1(y2 | y1) = p2(y2 | y1), and can be imposed by setting (α(1)
0 , α(1)
1 ) =
(α(2)
0 , α(2)
1 ), yielding µ(1)
2|1 = µ(2)
2|1.
For handling MNAR mechanisms, the regression formulation is favored
over the the multinomial parameterization because it facilitates simple rep-
resentations of departures from MAR in terms of diﬀerences between the α

196
NONIGNORABLE MISSINGNESS
parameters in the component distributions pk(y2 | y1); speciﬁcally, if we let
α(1)
0
=
α(2)
0
+ ∆0,
α(1)
1
=
α(2)
1
+ (∆1 −∆0),
(8.32)
then (∆0, ∆1) = (0, 0) represents MAR, and departures from MAR can be
represented by non-zero values of ∆0, ∆1, or both.
For y = 0, 1, the parameter ∆y is a log odds ratio comparing the odds that
Y2 = 1 between completers and dropouts, conditionally on Y1 = y,
∆y
=
log
*E(Y2 | S = 1, Y1 = y)/{1 −E(Y2 | S = 1, Y1 = y)}
E(Y2 | S = 2, Y1 = y)/{1 −E(Y2 | S = 2, Y1 = y)}
+
=
log
odds(Y2 | S = 1, Y1 = y)
odds(Y2 | S = 2, Y1 = y)

.
2
Another advantage to using the regression formulation is that its represen-
tation of MNAR extends readily to settings where J > 2. To illustrate, con-
sider a binary response vector Y = (Y1, . . . , YJ)T. As above, denote follow-up
time as as the number of observed measurements, S = 
j Rj. The full-data
model can be speciﬁed in terms of the dropout-speciﬁc component distribu-
tions ps(y1, . . . , yJ) and the dropout model P(S = s). Assume the component
distributions, for s = 1, . . . , J, follow
Y1 | S = s
∼
Ber(µ(s)
1 ),
Yj | Y1, . . . , Yj−1, S = s
∼
Ber(η(s)
j )
j = 2, . . . , J.
A general regression formulation for η(s)
j
would clearly have too many param-
eters if it were to include main eﬀects, two-way and higher-order interactions
involving Y1, . . . , Yj−1. For purposes of illustration, one can consider a simpli-
ﬁed model where serial dependence has order 1. Using a logistic regression,
logit(η(s)
j ) = γ(s)
j
+ θ(s)
j yj−1.
(8.33)
Even using this simpliﬁed structure, the full-data parameters (γ(s)
j , θ(s)
j ) can-
not in general be identiﬁed for j > s; it turns out that they qualify as sensi-
tivity parameters.
In the next example, we illustrate how to specify the PMM for J = 3 using
ﬁrst-order Markov models as the component distributions. In Section 10.3, we
use a similar model to analyze data from the OASIS trial; there, we provide
details on model speciﬁcation under both MAR and MNAR, and guidance
about speciﬁcation of informative priors that capture assumptions about de-
partures from MAR.
Example 8.7. Pattern mixture model for binary response with J = 3.
Here we consider a model for the joint distribution p(y1, y2, y3, s), with S =

MIXTURE MODELS
197
3
j=1 Rj again denoting number of observed responses. For s ∈{1, 2, 3}, we
decompose the full-data distribution as a mixture
p(y1, y2, y3)
=
3
	
s=1
φs ps(y1, y2, y3)
over the distribution of S, with
ps(y1, y2, y3)
=
ps(y1) ps(y2 | y1) ps(y3 | y2, y1)
and φs = P(S = s). We further assume ﬁrst-order serial dependence, so that
ps(y3 | y2, y1) = ps(y3 | y2). Within pattern s,
Y1 | S = s
∼
Ber(µ(s)),
Y2 | Y1, S = s
∼
Ber(η(s)
2 ),
Y3 | Y2, S = s
∼
Ber(η(s)
3 ),
with
logit(µ(s))
=
β(s),
logit(η(s)
j )
=
γ(s)
j
+ θ(s)
j Yj−1
(j = 2, 3).
(8.34)
The MAR constraint (8.17) is satisﬁed when these equalities hold:
p1(y2 | y1) = p≥2(y2 | y1)
p1(y3 | y2) = p2(y3 | y2) = p3(y3 | y2).
(8.35)
The ﬁrst part of the MAR constraint can be imposed by assuming
Y2 | Y1, S ≥2
∼
Ber(η(≥2)
j
),
with
logit(η(≥2)
j
)
=
γ(≥2)
j
+ θ(≥2)
j
Yj−1,
(j = 2, 3)
(8.36)
and then equating regression coeﬃcients across patterns by setting
γ(1)
2
=
γ(≥2)
2
θ(1)
2
=
θ(≥2)
2
.
(8.37)
In the absence of covariates, models (8.34) and (8.36) are compatible because
with binary data, the mixture distribution p≥2(y2 | y1) obtained by averag-
ing the component-speciﬁc logistic models for p2(y2 | y1) and p3(y2 | y1) is
equivalent to the single logistic model for data aggregated over patterns 2
and 3.
The second part of the MAR constraint (8.35) that requires p1(y3 | y2) =
p2(y3 | y2) = p3(y3 | y2) can be satisﬁed by setting
γ(s)
3
=
γ(3)
3
θ(s)
3
=
θ(3)
3

198
NONIGNORABLE MISSINGNESS
for s = 1, 2.
As with the continuous data model in Example 8.5, the MAR model can
easily be embedded in a larger class of models that allows MNAR. For exam-
ple, referring to (8.37), we can write
γ(1)
2
=
γ(≥2)
2
+ ∆0
θ(1)
2
=
θ(≥2)
2
+ (∆1 −∆0).
(8.38)
The parameters ∆0 and ∆1 correspond to log odds ratios that compare odds
of Y2 = 1 between those who have only one observed response (S = 1) and
those who have two or more (S ≥2). Speciﬁcally, notice that
γ(1)
2
=
logit{P(Y2 = 1 | Y1 = 0, S = 1)},
γ(≥2)
2
=
logit{P(Y2 = 1 | Y1 = 0, S ≥2)},
so that
∆0 = log
odds(Y2 = 1 | Y1 = 0, S = 1)
odds(Y2 = 1 | Y1 = 0, S ≥2)

.
Similarly, under the parameterization in (8.38), ∆1 is the same log odds ratio,
but conditioned on Y1 = 1,
∆1 = log
odds(Y2 = 1 | Y1 = 1, S = 1)
odds(Y2 = 1 | Y1 = 1, S ≥2)

.
More ∆parameters can be introduced along these lines, and the model con-
ﬁgured such that MAR is implied by setting each ∆to zero.
2
In Chapter 9, we provide a more complete development of principles for
ﬁnding sensitivity parameters, parameterizing untestable assumptions in terms
of unidentiﬁed full-data model parameters. incorporating informative prior in-
formation, and conducting sensitivity analyses. Further consideration of this
model and a detailed illustration using data from the OASIS Study is provided
in Section 10.3.
8.4.4 Mixture models with continuous-time dropout
Our illustrations of mixture models to this point have focused on settings
where dropout occurs in discrete time and for small numbers of measurement
times. It is possible to apply these models in continuous-time settings as well.
Here, we change notation for the event time from S to U, where U > 0 is
a continuous positive-valued random variable representing time to dropout.
The full-data model is still factored as
p(y, u | ω) = p(y | u, ω) p(u | ω),

MIXTURE MODELS
199
and the full-data response model is obtained by averaging over the dropout
distribution
p(y | ω) =

p(y | u, ω) p(u | ω) du.
For continuous data, a general formulation of this joint distribution uses
a varying coeﬃcient model, illustrated below, for the conditional distribution
p(y | u, x, ω) (Hogan et al., 2004a). In the longitudinal data setting, the
standard pattern mixture model for discrete-time dropout (Little, 1993, 1994)
and the conditional linear model (Wu and Bailey, 1988) can be viewed as
special cases of the varying coeﬃcient model approach. Moreover, although
the VCM approach is developed for continuous responses and is based on a
mixture of normal distributions (used below to illustrate), in principle the
approach can be used for other types of distributions (Su and Hogan, 2007).
In the mixture of normals case, we assume that the full-data response
vector Y
= (Y1, . . . , YJ)T arises from a process {Y (t) : t ≥0} observed
at times t1, . . . , tJ. The set of measurement times is assumed to vary by
individual. Conditionally on dropout at time U = u, we assume Y (t) has
a mean function denoted by µ(t|u) and covariance given by C(s, t|u); e.g.,
C(s, t|u) = σ(s|u)σ(t|u)ρ(s, t|u) for s ̸= t. We further assume that the process
is conditionally normal (i.e., a Gaussian process), such that
Y (t) | U = u
∼
N( µ(t | u), C(t, t | u) ).
To obtain E{Y (t)}, we integrate the conditional mean function over the dis-
tribution of dropout time,
E{Y (t)}
=

µ(t | u) p(u) du.
A question arises as to the functional form of µ(t|u); in the VCM approach,
it is assumed that µ(t|u) is a known function of t for any given u, but as a
function of u it can be any smooth function. To illustrate, we use a simple
formulation where µ(t|u) is linear in t, and the goal is to estimate the overall
mean of {Y (t)}.
Example 8.8. Mixture of varying coeﬃcient models for estimating the inter-
cept and slope under continuous-time dropout.
Consider a full-data response Y i that is generated by potentially observing
the process {Yi(t)} at time points ti1, . . . , tiJi, i.e.,
Y i = (Yi1, . . . , YiJi)T = (Yi(ti1), . . . , Yi(tiJi))T.
Dropout for individual i occurs at Ui. Let xi(t) = {xi1(t), . . . , xip(t)} represent
a p-dimensional covariate process, and let
Xi
=
( xi(ti1)T, . . . , xi(tiJi)T )T
=
(xT
i1, . . . , xT
iJi)T

200
NONIGNORABLE MISSINGNESS
denote a Ji × p covariate matrix.
A varying coeﬃcient model for those who drop out at u is
Y i | Xi, Ui = u ∼N( Xiβ(u), Σi(u) ),
where β(u) is a p × 1 parameter vector comprised of functions of u; i.e.,
β(u) = (β1(u), . . . , βp(u))T and the (j, k)th element of Σi(u) is C(tij, tik | u).
The functions can be speciﬁed using penalized splines (Ruppert et al., 2003;
Crainiceanu et al., 2005).
For this example we assume Σi(u) = Σi{φ(u)} = Σi(φ); that is, we as-
sume the conditional covariance matrix is parameterized by the vector φ(u),
but that φ(u) = φ does not depend on dropout time. This assumption may
be relaxed by modeling covariance parameters as smooth functions of u; see
Chapter 6.
This model implies that conditional on dropout time,
E{Yi(t) | xi(t), U = u} = µi(t|u) = xi(t)β(u),
and therefore
E(Yij | Xi, Ui = u)
=
xi(tij)β(u)
=
xijβ(u).
If xij = (1, tij), then E(Yij | tij, Ui = u) follows a straight line as a function
of t, but the intercept and slope parameters are functions of the dropout time
u. For this model, the mean of Yi(t) itself is a straight line because
E(Yij)
=

{β1(u) + β2(u)tij} p(u) du
(8.39)
=
β1 + β2tij,
where βq =

βq(u) p(u) du for q = 1, 2.
2
An important feature of this model is that the distribution of dropout times,
p(u), can be left unspeciﬁed. For inference, mixtures of Dirichlet process mod-
els (cf. Section 3.6) can be used to ensure ﬂexibility. Parametric distributions
for p(u) also can be used where appropriate, but in general leaving the distri-
bution unspeciﬁed does not introduce signiﬁcant additional complications. A
detailed analysis of the Pediatric AIDS trial, including details on model and
prior speciﬁcation using a VCM, is given in Section 10.4.
For normally distributed data, the VCM approach reduces to the standard
multivariate normal repeated measures regression — and consequently the
missing data mechanism reduces to MAR — when β(u) = β and Σ(u) = Σ.
When the regression coeﬃcients are a nonconstant function of u, the work-
ing assumption for the VCM is that conditional on U = u, the regression
coeﬃcient remains the same both before and after u. This assumption is par-
ticularly important for time-varying covariates, the most common being time
itself. In the simple model described in Example 8.8, it is assumed that among

MIXTURE MODELS
201
those dropping out at U = u, the slope β1(u) applies both prior to and after
dropout. Hence the full-data mean of Y (t) at a ﬁxed time t = t∗will be based
on extrapolations for those who have dropped out prior to t∗.
It is possible to relax the assumption about constant slope (or covariate
eﬀect) both before and after dropout by adding one or more sensitivity pa-
rameters (cf. to the discussion of identiﬁcation via extrapolation in Section
8.4.2); this is addressed in Su and Hogan (2007) and discussed further in our
analysis in Section 10.4.
In principle, the VCM can be applied to settings where dropout time is
continuous and responses are discrete. In that case, calculation of full-data
functionals (such as the mean) requires integrating a nonlinear link function
over the dropout distribution. This is in contrast to (8.39), where it is only
necessary to integrate the functions β(u) over p(u).
8.4.5 Combinations of MAR and MNAR dropout
In many trials, subjects drop out for a variety of reasons, leading to situations
where a single study may have both MNAR and MAR mechanisms. For ex-
ample, in the schizophrenia clinical trial (Section 1.2), subjects dropped out
for a variety of reasons, including adverse events, lack of treatment eﬀect, and
other reasons — including improvement in schizophrenia symptoms (see Ta-
ble 1.1). Subjects who dropped out due to lack of treatment eﬀect might be
assumed to potentially be MNAR dropouts, while those who dropped out for
other reasons might be assumed MAR dropouts (see Hogan and Laird, 1997a
for further details).
Using mixture models, we can deﬁne patterns based on the follow-up time
corresponding to MNAR dropout. Recall that for the schizophrenia data, the
measurement times are T = {1, 2, 3, 4, 6}; let S denote time followed up until
an individual leaves the study for a reason that would be classiﬁed as MNAR.
Here, S ∈T . If an individual leaves the study for a reason that is assumed
MAR, then S is right-censored. For example, if an individual discontinues
follow up after week 2 due to an adverse event that is unrelated to treatment
eﬃcacy, we observe S > 2 or equivalently S ∈{3, 4, 6}.
Consider a pattern mixture model given by
Y | S = s
∼
N(xβ(s), Σ)
S
∼
Mult(φ1, . . . , φ6),
where x is a design matrix reﬂecting treatment group and time trend (e.g.,
quadratic trend over time), and the multinomial parameters for S are con-
strained such that φ5 ≡0 (to reﬂect that measurements are not taken at
week 5) and 
s φs = 1. Let α = {β(1), . . . , β(6), Σ}, and φ = (φ1, . . . , φ6)T.
Implementation of posterior sampling is best done using data augmenta-
tion, as opposed to working directly with the observed data likelihood. For

202
NONIGNORABLE MISSINGNESS
individuals that drop out for reasons deemed MNAR, S is observed. For each
individual, the data augmentation step draws y∗
i,mis from the distribution
p(ymis | yi,obs, si, α), which is simply a conditional normal distribution in
pattern S = si for the pattern mixture model given above.
For individuals that discontinue follow up at time k for reasons deemed
MAR, data augmentation proceeds in two steps. First, draw s∗
i from a multi-
nomial distribution having probabilities (φ∗
i1, . . . , φ∗
i6), where
φ∗
ij
=
p(S = j | yi,obs, S > k, α, φ)
=
φj p(yi,obs | S = j, α) I(j > k)
6
j=k+1 φj p(yi,obs | S = j, α)
.
Note that φ∗
ij = 0 for j ≤k. Next, we draw a new value of y∗
i,mis as above,
this time conditioning on s∗
i and using the distribution p(ymis | yi,obs, s∗
i , α).
For a complete analysis of the schizophrenia data, see Hogan and Laird
(1997a); in that paper, an EM algorithm is used, where the E step is very
similar to the data augmentation step described here.
8.4.6 Mixture models or selection models?
With binary (or categorical) data, when the measurement occasions are dis-
crete and dropout is the sole cause of missingness, the duality between mix-
ture and selection models holds for any J, but of course the dimensionality
increases exponentially: for J measurement occasions with J possible dropout
times, the number of unique parameters is J2J −1, and any realistic analysis
must rely on simplifying assumptions to reduce the dimension of the param-
eter space. This raises an obvious question of which factorization to use for
the full-data model p(y, r | ω).
With a selection model, the simpliﬁcations must be made in terms of the
full-data response distribution p(y) and the selection mechanism p(r | y).
Possible strategies include limiting the association structure in the full-data
response distribution (e.g., to include only two-way interactions), or limiting
the selection mechanism such that dropout at tj depends only on a small
part of the observed history (say Yj and Yj−1). For purposes of sensitivity
analysis, however, this can become problematic because unless p(y) is speciﬁed
nonparametrically, the full-data model parameter ω can be identiﬁed from
the observed data, and the choice of an appropriate sensitivity parameter is
often not possible. An alternative is a semiparametric formulation for the full-
data response model Scharfstein et al. (2003), but this approach can present
nontrivial technical complications.
By contrast, model simpliﬁcations in mixture models may be more feasi-
ble, despite the proliferation of nonidentiﬁed parameters. Sensible simplifying
assumptions can be imposed on the observed data, while keeping the distribu-
tion of the missing data indexed by one or more nonidentiﬁed parameters. Our

MIXTURE MODELS
203
example using ﬁrst-order dependence within pattern on mixtures of longitu-
dinal binary data distributions is representative. There is a clear delineation
between identiﬁed and nonidentiﬁed parameters and, importantly, simplifying
assumptions that are used to constrain the observed data distribution can be
empirically critiqued.
8.4.7 Covariate eﬀects in mixture models
Although most of our examples do not involve drawing inference about more
than one or two covariates, it is important to understand how covariate ef-
fects are computed and interpreted in mixture models. Because the full-data
model is a mixture over component distributions corresponding to missing
data pattern or dropout time, covariate eﬀects must be interpreted in terms
of the mixture distribution. For the PMM with identity link, covariate eﬀects
for the full data can sometimes have a simple representation as a weighted
average over pattern-speciﬁc covariate eﬀects, and the scale of the covariate
eﬀects is preserved; see Examples 8.9 and 8.10. This is generally not true with
nonlinear link functions, as we illustrate in Example 8.11.
We focus on covariates X that are exogenous, either time-invariant (e.g.,
baseline characteristics, gender) or ﬁxed functions of time (e.g., age), and
whose eﬀects are time invariant (see Roy and Lin (2002) for settings with
stochastic time-varying covariates subject to missingness from dropout).
PMM with identity link function within pattern
Here we illustrate the computation of covariate eﬀects using the identity link.
In the ﬁrst example, missingness does not depend on covariates, and in the
second it does.
Example 8.9. Mixture of regressions with identity link for bivariate response,
where dropout does not depend on covariates.
Consider the PMM for bivariate data where interest is in estimating the eﬀect
of covariates X, represented in a 2 × p matrix, on the bivariate outcome
Y = (Y1, Y2)T. Recall that the full-data model is factored as
p(y1, y2, r | x, ω) = p(y1, y2 | r, x, ω) p(r | x, ω).
In the case where missingness does not depend directly on covariates, we
have p(r | x, ω) = p(r | ω), which implies that the eﬀect of X on Y can
be fully characterized by its within-pattern eﬀects. We further assume that
the covariate eﬀect is constant over time and can be captured in a time-
independent parameter β.
Regardless of the parametric distribution assigned to p(y1, y2 | r, x, ω), an
identity link within pattern implies the mean is linear in covariates, via
E(Y | X = x, R = r)
=
xβ(r),

204
NONIGNORABLE MISSINGNESS
where β(r) is a p×1 vector of regression parameters. With the two-component
mixture, R ∼Ber(φ). Hence
E(Y | x)
=
ER{E(Y | x, R)}
=
φxβ(1) + (1 −φ)xβ(0)
=
x{φβ(1) + (1 −φ)β(0)},
and the covariate eﬀect is weighted average of pattern-speciﬁc coeﬃcients, i.e.,
β
=
φβ(1) + (1 −φ)β(0).
These can be identiﬁed because the covariate is assumed to have a constant
eﬀect over time.
2
More generally, dropout may depend on covariates, in which case
p(y | x) =
	
r∈R
p(y | r, x) p(r | x).
Usually we are interested in µ(x) = E(Y | X = x), computed via
µ(x) =
	
r∈R
µ(r)(x) p(r | x).
The next example considers this more general setting using a discrete mixture
over distinct dropout times.
Example 8.10. Discrete-time mixture of regressions with identity link, where
dropout depends on covariates.
Denote the full-data response by Y = (Y1, . . . , YJ)T, with missing data indi-
cators R = (R1, . . . , RJ)T. For this example we consider only baseline covari-
ates, collected in a p−dimensional row vector X. Dropout is characterized
using the follow-up time S = 
j Rj, with S ∈{1, . . ., J}, and the model is a
mixture over p(s | x).
The within-pattern regression model follows
µ(s)
j (x) = E(Yj | S = s, X = x) = xβ(s),
and follow-up time depends on covariates via
S | X = x
∼
Mult(φ1(x), . . . , φJ(x)),
where φs(x) = P(S = s | x). In practice the φs(x) could be represented using
a saturated model if components of X are discrete and low dimensional (e.g.,
treatment group in a randomized trial); otherwise they would be speciﬁed
using a model for multinomial distribution, such as relative risk regression.
The mean response as a function of covariates is
µj(x) = E(Yj | X = x) =
	
s
φs(x) xβ(s).

MIXTURE MODELS
205
The covariate eﬀect is seen to depend on x,
∂µj(x)
∂x
=
	
s

x∂φs(x)
∂x
+ φs(x)

β(s).
(8.40)
For capturing the eﬀect of ﬁxed diﬀerences x −x′, we have
µj(x) −µj(x′) =
	
s
{xφs(x) −x′φs(x′)}β(s).
(8.41)
If missingness does not depend on covariates, then p(s | x) = p(s), which
implies ∂φs(x)/∂x = 0 and φ(x) −φ(x′) = 0 (for the discrete case). Hence
(8.40) simpliﬁes to 
s φsβ(s) and (8.41) simpliﬁes to (x−x′) 
s φsβ(s); each
is a weighted average of pattern-speciﬁc regression coeﬃcients.
2
PMM with nonlinear link functions within pattern
For PMM that are speciﬁed as mixtures of regression models, evaluating co-
variate eﬀects is somewhat more complicated if nonlinear link functions are
used (e.g., logistic or log). As with the previous examples, we specify regres-
sions within patterns deﬁned by follow-up time S = 
j Rj. Let µ(s)
j (x) =
E(Yj | Xj = x, S = s) denote the within-pattern mean, and assume
g{µ(s)
j (x)} = xβ(s),
where g : R →R is a smooth monotone link function.
In general the eﬀect of X on the full-data mean of Yj is
∂µj(x)
∂x
=
∂
∂x
	
s

µ(s)
j (x) φs(x)

=
	
s
* ∂
∂xg−1(xβ(s))

φs(x) + g−1(xβ(s))
 ∂
∂xφs(x)
+
.
If dropout does not depend on covariates, then φs(x) = φs and
∂µj(x)
∂x
=
	
s
 ∂
∂xg−1(xβ(s))

φs(x).
(8.42)
Example 8.11. Covariate eﬀects in mixture of loglinear regression models.
Following the setup from above, if a log link is used within pattern, we have
g−1(xβ(s))
=
exp(xβ(s))
and
∂
∂xg−1(xβ(s))
=
β(s) exp(xβ(s)).
Now assume φs(x) = φs, i.e., dropout does not depend on covariates. For the

206
NONIGNORABLE MISSINGNESS
mixture of loglinear models, (8.42) becomes
∂µj(x)
∂x
=
	
s
β(s) exp(xβ(s)) φs.
Hence, even if dropout is independent of x, the covariate eﬀect still depends
on x when the link function within pattern is nonlinear.
In fact, the eﬀect of x on the full-data mean of Yj is a weighted average
of within-pattern regression coeﬃcients β(s), with weights that depend on x.
Inspection of (8.42) shows this will be true in general for nonlinear g.
2
In summary, important considerations in the computation of covariate ef-
fects from mixture models include (a) whether the mean is linear in covariates;
(b) whether missingness depends on covariates, and (c) whether the covariate
eﬀects are time varying. Our focus here has been on (a) and (b), illustrat-
ing computations for settings where the covariate eﬀects are time constant.
When the link function is nonlinear, it can be diﬃcult to capture the covariate
eﬀect succintly. To improve interpretability of covariate eﬀects, Wilkins and
Fitzmaurice (2006) and Roy and Daniels (2007) have introduced marginalized
models, imposing constraints on the marginal mean similar to those used for
marginalized transition models.
8.5 Shared parameter models
8.5.1 General structure
Shared parameter models were introduced in Section 5.9.3. In the most general
case, a SPM takes the form
p(y, r, ω) =

p(y, r | b, ω) p(b | ω) db,
where b are subject-speciﬁc random eﬀects. The chief characteristic of these
models is that a single set of ‘shared parameters’ — usually random eﬀects —
applies to the joint distribution of Y and R. In many cases it is assumed that
Y and R are independent conditionally on b, though this is not a require-
ment. Theoretically, all SPM can be represented either as a mixture model
or selection model, but the functional form of the component distributions is
typically not tractable.
In this example, we describe an SPM that can be used for a longitudinal
response process with continuous time dropout. It uses a standard random
eﬀects model formulation for the full-data response distribution and a pro-
portional hazards model for the dropout time.
Example 8.12. Shared parameter model for normally distributed full-data
response with continuous time dropout.
Henderson et al. (2000) describe a general structure for the SPM whereby the

SHARED PARAMETER MODELS
207
full-data response is characterized by a Gaussian process having between- and
within-subject variation. Missingness is induced by dropout, which depends
through a second model on individual-speciﬁc random eﬀects characterizing
between-subject variation of the responses.
The Henderson et al. speciﬁcation assumes a continuous-time process Y (t)
that can be right-censored by dropout at time U; to make the notation agree
with our conventions, let Yj = Y (tj). Then their model is written as
Yj | xj, Zj
∼
N(xjβ + Zj, σ2)
hU(tj | Zj)
=
h0(tj) exp(Zjγ),
(8.43)
where Zj is a realization at tj of a latent Gaussian process Z(t) that character-
izes between-subject variation, and within-subject variation in the response
model is characterized by a stationary Gaussian process having constant vari-
ance σ2. The function hU is the hazard of dropout, and h0 is a baseline hazard.
In this formulation, Zj is viewed as the ‘shared parameter’. The scalar pa-
rameter γ links the longitudinal process Y (t) to the hazard of dropout. When
γ = 0 we have MAR, and otherwise we have MNAR.
Contextually, this model can be motivated by the need to deal with re-
sponses measured with error, whereby xjβ+Zj is the error-free version of Yj,
with measurement error captured by a residual process.
Connections to the SPM formulations given in Section 5.9.3 can be seen
by considering the simple case where xj = (1, tj) and Zj = xjb, where b =
(b0, b1)T ∼N(0, Ω) are subject speciﬁc random eﬀects for intercept and slope.
Here the underlying error-free process follows a straight line over time. Then
the model (8.43) is written as follows, using subject indices i to emphasize
sources of variation:
Yij | xij, bi
∼
N(xijβ + xijbi, σ2)
hU(tij | bi)
=
h0(tij) exp {(xijbi)γ} .
The full-data likelihood for this model is proportional to
p(y, u | x, β, Ω, σ, γ) =

p(y | x, b, β, Ω, σ) p(u | x, b, γ) p(b | Ω) db,
demonstrating that it is a shared parameter model with Y ⊥⊥U | (b, X).
2
8.5.2 Pros and cons of shared parameter models
Shared parameter models are very eﬀective for decomposing the variance for
multivariate processes, and work in this area has been very eﬀective at facili-
tating joint modeling of repeated measures and event times. Most commonly,
shared parameter models are used either to (a) make adjustments for selec-
tion bias, when the main objective is drawing inference about the full-data

208
NONIGNORABLE MISSINGNESS
distribution of repeated measures, or (b) model the hazard of an event time
as a function of stochastic time-varying covariates.
With respect to handling dropout, these models can be eﬀective for complex
data structures (e.g., multivariate longitudinal responses, situations where
observations are taken very frequently across time) or when the main outcome
of interest can be conceptualized as a latent variable (for example severity of
disease as measured by several indicators). In the latter case, the mechanism
relating the full-data distribution of interest (the latent variable) is explicit.
A disadvantage of shared parameter models is that the functional depen-
dence between full-data responses Y and dropout time U is not usually ex-
plicit; to obtain p(u | y) or p(y | u), the latent variables must be integrated
out of the full-data model. Consequently, the missing data mechanism is not
always transparent.
Another by-product of assuming a common latent structure for both the
response and dropout distributions is that the hazard of dropout in a shared
parameter model will generally depend on future observations, even after con-
ditioning on past and current observations. To illustrate, consider the simple
shared parameter model where the full-data response is Y = (Y1, Y2, Y3)T,
Y1 is always observed, and hazard of dropout at times 2 and 3 depends on a
common random eﬀect. This model can be speciﬁed as
Yj | b
∼
N(b, σ2)
(j = 1, 2, 3)
b
∼
N(0, τ2)
Rj | Rj−1 = 1, b
∼
Ber( Φ(γb) )
(j = 2, 3),
where Φ(·) is the cdf of a standard normal distribution and γ is a scalar
parameter. The hazard of dropout at time 2 as a function of y is
P(R2 = 1 | y)
=

P(R2 = 1 | b) p(y | b) p(b) db
p(y)
=

P(R2 = 1 | b) p(b | y) p(y) db
p(y)
=

Φ(γb) p(b | y) db
=
Eb|Y {Φ(γb) | y}.
(8.44)
Because 
j yj is a suﬃcient statistic for b in p(b | y), the expectation (8.44)
depends on 
j yj and therefore on y3. The exception is when γ = 0 (MCAR),
in which case Φ(γb) = Φ(0) = 1
2 is a constant.
The latent variable structure also can make it diﬃcult to separate pa-
rameters indexing p(ymis | yobs, u) from those indexing p(yobs, u), and it is
therefore hard to embed an MAR speciﬁcation in a larger class of models
for assessing sensitivity to departures from MAR. Finally, shared parameter
models frequently rely for identiﬁcation on distributional assumptions about

MODEL SELECTION AND MODEL FIT
209
the latent variable b, which governs both observed and missing data. These
assumptions are frequently motivated by convenience rather than by context.
To summarize, shared parameter models are very useful for characterizing
joint distributions of repeated measures and event times, and can be particu-
larly useful as a method of data reduction when the dimension of Y is high.
Nonetheless, their application to the problem of making full-data inference
from incomplete longitudinal data should be made with caution and with an
eye toward justifying the required assumptions. Sensitivity analysis is an open
area of research for these models.
8.6 Model selection and model ﬁt in nonignorable models
Unlike with the ignorable models described in Chapter 6, model compari-
son and assessment of ﬁt for nonignorable models must consider the missing
data mechanism p(r | y, ψ(ω)) itself. Model checking and model selection are
therefore based on the full-data model p(y, r | ω), and not just the full-data
response model p(y | θ).
Likelihood-based criteria, like the DIC, will now be based on the ﬁt of the
full-data model to the observed data, (yobs, r). Poor ﬁt indicates the full-data
modeling assumptions are not consistent with the observed data.
None of the metrics and checks for comparing and assessing ﬁt of mod-
els provide information about the feasibility of the implicit or explicit as-
sumptions about the missing data. They only provide information about how
modeling assumptions, priors, and speciﬁc missing data assumptions as an en-
semble ﬁt the observed data (yobs, r). Thus, the following criteria and checks
can only be used to assess speciﬁc missing data assumptions within the con-
text of a speciﬁc fully parametric model for the full data. We can therefore
use these criteria to compare the ﬁt of diﬀerent parametric models within
the same class (e.g., selection models) or between classes (e.g., a parametric
selection model vs. a pattern mixture model).
8.6.1 Deviance information criterion (DIC)
For nonignorable dropout, the full-data likelihood is proportional to p(y, r |
ω). In Chapter 6, we discussed two forms of the DIC: one based on the ob-
served data response likelihood, DICO, and one based on the posterior pre-
dictive expectation of the full-data response likelihood, DICF . We develop
corresponding criteria here based on the observed data likelihood and the
full-data likelihood, respectively. DICO takes the form
DICO
=
−4Eω{ℓ(ω | yobs, r)} + 2ℓ(ω | yobs, r),

210
NONIGNORABLE MISSINGNESS
where ℓ= log L and ω = E(ω | yobs, r). The observed data likelihood L(ω |
yobs, r) satisﬁes
L(ω | yobs, r) ∝

p(y, r | ω)dymis,
(8.45)
and is typically not available in closed form for selection models (SM) and
shared parameter models (SPM).
When the observed data likelihood is available in closed form, the DIC can
often be computed entirely in WinBUGS. We provide examples of this on the
book webpage.
DICF is based on the full-data likelihood. We ﬁrst construct the DIC based
on the full-data model (assuming complete response data),
DICfull(yobs, ymis, r)
=
−4Eω {ℓ(ω | yobs, ymis, r)}
+ 2ℓ{ω(ymis) | yobs, ymis, r},
where ω(ymis) = E(ω | yobs, ymis, r). To obtain DICF , we take the expecta-
tion of DICfull(yobs, ymis, r) with respect to the posterior predictive distribu-
tion p(ymis | yobs, r),
DICF
=
Eymis {DICfull(yobs, ymis, r)}
=
−4Eymis [Eω{ℓ(ω | yobs, ymis, r)}]
+ 2Eymis {ℓ(ω(ymis) | yobs, ymis, r)} .
(8.46)
Computing DICF requires the evaluation of ω(ymis) = E(ω | yobs, ymis, r)
(in the second term of (8.46)) at each value of ymis sampled during the data
augmentation step of the sampling algorithm. We provide suggestions for com-
puting ω(ymis) in the setting of diﬀerent nonignorable models, similar to the
approaches proposed in Chapter 6 in the next section.
Once the expectations, ω(ymis) are computed, DICF can be computed with
little diﬃculty. The ﬁrst term in (8.46) is the expectation of the full-data log
likelihood with respect to p(ymis, ω | yobs, r). Except for SPMs, for which the
full-data likelihood can often not be written in closed form, the ﬁrst term can
be estimated as
1
M
M
	
m=1
ℓ(ω(m) | yobs, y(m)
mis , r),
where (y(m)
mis , ω(m)) are the draws from the data-augmented Gibbs sampling
algorithm that was used to sample from the posterior distribution of ω; we
can compute this term in WinBUGS. The second term, given ω(ymis), can be
computed using the same draws as the ﬁrst term via
1
M
M
	
m=1
ℓ(ω(y(m)
mis ) | yobs, y(m)
mis , r).
Both DICF and DICO can be used to compare the ﬁt of the full-data models

MODEL FIT UNDER NONIGNORABILITY
211
to the observed data, (yobs, r). Next, we present more details on implementa-
tion and computation of both forms of the DIC for selection models, pattern
mixture models, and shared parameter models.
Computation for selection models
Computation of DICO in selection models is complicated by the fact that the
observed data log likelihood will typically not be available in closed form (see,
e.g., Diggle and Kenward model in Example 8.1). However, it can, in general,
be computed using a reweighting approach. Recall the form of the observed
data likelihood given in (8.45) and assume distinct (and a priori indepen-
dent) parameters for the missing data mechanism and the full-data response
model. It can be shown that the observed data likelihood for a selection model
factorization is
L(ω | yobs, r)
∝

p(r | y; ψ) p(y | θ) dymis
=
p(yobs | θ) Eymis|yobs,θ {p(r | ymis, yobs, ψ)}.
(8.47)
The expectation in (8.47) can be computed using Monte Carlo integration by
averaging draws from p(ymis | yobs, θ), the predictive distribution, conditional
on θ, based on only the full-data response model. However, a separate Monte
Carlo integration would need to be done for every value of θ from the Gibbs
sampler output of p(θ | yobs, r). As a more computationally practical alter-
native, we recommend the following approach. First, obtain a Monte Carlo
sample of ymis for a likely value of θ, say θ⋆= E(θ | yobs, r); denote this
sample as {y(l)
mis : l = 1, . . . , L}. Then, reweight (cf. Chapter 6) this sample to
estimate the expectation of interest as a function of θ,
Eymis|yobs,θ {p(r | ymis, yobs, ψ)}
=
L
l=1 wlp(r | y(l)
mis, yobs, ψ)
L
l=1 wl
,
with weights wl = wl(θ) given by
wl = p(y(l)
mis | yobs, θ)
p(y(l)
mis | yobs, θ⋆)
.
For DICF , we do not need to compute the observed data likelihood. The
computational complexity for DICF , as discussed earlier, is the computation
of ω(ymis) = E(ω | yobs, ymis, r) for every value of ymis in the MCMC sam-
ple. We can do this using a similar reweighting strategy to that proposed in
Chapter 6. First, run one additional Gibbs sampler with ymis ﬁxed at a likely
value, say y⋆
mis = E(ymis | yobs, r); denote this sample as {ω(l) : l = 1, . . . , L}.
Then, compute ω(y(m)
mis ) = E(ω | yobs, y(m)
mis , r) for each sampled value y(m)
mis

212
NONIGNORABLE MISSINGNESS
from the original MCMC sample using
ω(y(m)
mis ) =
L
l=1 wlω(l)
L
l=1 wl
(8.48)
with weights wl given by
wl = p(θ(l), ψ(l), y(m)
mis , yobs, r)
p(θ(l), ψ(l), y⋆
mis, yobs, r)
.
(8.49)
Expressions of the form p(θ(l), ψ(l), ymis, yobs, r) in wl can be replaced by
p(y | θ(l)) p(r | y, ψ(l)) because p(θ(l), ψ(l)) cancel out. Recall here that m
indexes the original MCMC sample.
Computation for pattern mixture models
In a pattern mixture formulation, the full-data likelihood is factored as p(y |
r, α)p(r | φ). So the observed data likelihood, which needs to be computed
for DICO, is proportional to
p(r | φ)

p(y | r, α)dymis,
(8.50)
which can be computed in closed form here as long as the pattern-speciﬁc
distributions p(y | r, α) oﬀer a closed form for  p(y | r, α)dymis. The mixture
models proposed in Examples 8.4, 8.5, and 8.8 all oﬀer a closed form for (8.50).
In such cases, DICO can be computed in WinBUGS.
For DICF , we need to compute
ω(ymis)
=
E(α, φ | ymis, yobs, r)
for all values of ymis in the data-augmented Gibbs sampler output for the
posterior, {ω(m), y(m)
mis : m = 1, . . . , M}, where ω(m) = (α(m), φ(m)). One
approach, similar to that suggested for selection models, is the following: First,
run one additional Gibbs sampler with ymis ﬁxed at (a likely value), say
y⋆
mis = E(ymis | yobs, r); denote this sample as {ω(l) : l = 1, . . . , L}. Note that
this sample is drawn from the full-data posterior where we have ﬁlled in ymis
with y⋆
mis. Then, to compute ω(y(m)
mis ) for the values {y(m)
mis : m = 1, . . . , M}
from the original data-augmented Gibbs sampler run, we can use (8.48) with
weights
wl = p(α(l), φ(l), y(m)
mis , yobs, r)
p(α(l), φ(l), y⋆
mis, yobs, r)
.
(8.51)
As with (8.49), we can make the substitution
p(α(l), φ(l), ymis, yobs, r) = p(y | α(l), r) p(r | φ(l)).

MODEL FIT UNDER NONIGNORABILITY
213
Computation for shared parameter models
Shared parameter models pose an additional problem for computation of both
forms of the DIC. The joint distribution p(y, r | ω),
p(y, r | ω) =

p(y, r | b, ω) p(b | ω) db,
typically cannot be computed in closed form (i.e., cannot analytically integrate
out the shared parameters). This joint distribution is required to compute
both forms of the DIC. To evaluate p(y, r | ω), a Monte Carlo integration
could be done at each iteration to compute the integrated (over b) likelihood.
However, this would be extremely ineﬃcient. A more computationally feasible
approach might be to use Laplace approximations (Tierney and Kadane, 1986)
to approximate the likelihood integrated over b.
Strategies for computing DICO and DICF are similar to those for selection
models and pattern mixture models but with the additional complication that
the full-data model itself is typically not available in closed form. Neither of
these criteria can be computed in WinBUGS.
However, to compare the ﬁt among diﬀerent shared parameter models, we
can avoid the integration over the shared parameters b by using the likelihood
conditional on the shared parameters, which is proportional to p(y, r | b, ω)
(cf. discussion of DIC with random eﬀects models in Chapter 3). The DICO
based on this likelihood can be computed in WinBUGS.
Summary
As discussed in Chapter 6, the relative merits and operating characteristics of
these two forms of the DIC with incomplete data need careful study, including
the computational eﬃciency of the reweighting-based approaches. For some
models, the DIC can be computed completely in WinBUGS. For others, parts
of the DIC need to computed using simple user-written functions in a package
like R. Comparison of nonignorable models (which require speciﬁcation of the
missing data mechanism) to ignorable models (which do not) is an area that
needs further study.
8.6.2 Posterior predictive checks
In Chapters 3 and 6 we recommended posterior predictive checks to assess
the ﬁt of the speciﬁc aspects of a parametric model to yobs. In the setting of
nonignorable missingness, we typically want to assess the ﬁt of the full-data
model to features of the observed data (yobs, r). We therefore sample from
the posterior predictive distribution
p(yrep, rrep | yobs, r).
Given that we now model the missing data mechanism, we can compute

214
NONIGNORABLE MISSINGNESS
data summaries based on replicates of observed data, not just replicates of
complete data. For individual i, we deﬁne replicates of the observed data as
yi,obs,rep
=
{yij,rep : rij,rep = 1} ,
i.e., the components yij,rep of the replicated complete datasets for which the
corresponding missing data indicator rij,rep is equal to 1. These replicated
‘observed’ datasets do not have a one-to-one correspondence with the observed
data. We further clarify with a simple example.
Consider complete data (Y1, Y2, R), where R = 0 corresponds to Y2 being
missing. When we sample from the posterior predictive distribution, rrep ̸= r.
As a result, we will have components rij,rep of rrep that take a value of one
when the corresponding component rij of r is zero. So, even though we have
an ‘observed’ yij,rep, there is not necessarily a corresponding yij,obs with which
to compare it.
There are two solutions to this problem. The ﬁrst would be to do the
predictive checks based on replicates of the complete data, as in Chapter 6.
The second solution would be to construct statistics that attach indicators
I{rij = rij,rep = 1} to the pairs yij and yij,rep; that is, at each iteration, com-
pute the checks using only those components of yobs and yrep where rij,rep = 1.
The power of these two approaches to detect model departures needs fur-
ther study. Both approaches will result in less power than if we actually had
complete data. The ﬁrst approach loses power by ‘ﬁlling in’ the missing data
based on the model, thus biasing the checks in favor of the model. The sec-
ond approach loses power by not using all the observed data. Under ignorable
dropout, we advocated the ﬁrst approach based on completed datasets. One
of the reasons for this was that under ignorability, the missing data mecha-
nism p(r | y, ψ) is not speciﬁed. However, in nonignorable models, we specify
the missing data mechanism so this reason is no longer valid and the second
approach can be implemented.
Checks of the full-data response model will be similar to those discussed
in Chapter 6, where we assessed the ﬁt of aspects of the full-data response
model under ignorable dropout. Although our primary interest is typically
the ﬁt of the full-data model to the observed responses, yobs, we should also
check the ﬁt of the missing data mechanism. To assess the ﬁt, we can compute
statistics that measure the agreement between the observed missing data indi-
cators r and the replicates rrep. Because our focus is on monotone missingness
(dropout), we might consider a statistic such as
h{T (ri), T (ri,rep)} =
	
j
rij −
	
j
rij,rep.
If the missing data mechanism ﬁts well, we would expect this distribution to
have a median around zero.

FURTHER READING
215
8.7 Further reading
Marginalized mixture models for marginal covariate eﬀects
Wilkins and Fitzmaurice (2006) developed what they termed a ‘hybrid model’
for nonignorable longitudinal binary data. Speciﬁcally, they model the joint
distribution of (y, r) using a canonical log-linear model (Fitzmaurice and
Laird, 1993) that allows for direct speciﬁcation of the marginal means for
the longitudinal binary data and for the dropout indicators. However, within
this setup, it can be hard to specify sensible parsimonious longitudinal depen-
dence. Roy and Daniels (2007) extended earlier work by Roy (2003) on group-
ing dropout times into patterns using a latent class approach in the frame-
work of mixture models. Using ideas from Heagerty (1999), they construct
the model to allow direct speciﬁcation of the marginal means (marginalized
over the random eﬀects and the dropout distribution).
Semiparametric selection models
For continuous longitudinal responses, nonparametric speciﬁcations for the
full-data response model are typically not practical. To oﬀer some ﬂexibil-
ity in the context of random eﬀects models, the random eﬀects distributions
can be speciﬁed nonparametrically using mixtures of Dirichlet process models
(Kleinman and Ibrahim, 1998b) as described in Chapter 3.
Flexible modeling of the missing data mechanism
Work on more ﬂexible estimation of the ‘form’ of the missing data mechanism
has been undertaken by several authors. Lee and Berger (2001) have proposed
Bayesian nonparametric estimation of the missing data mechanism in the sim-
ple setting of a univariate y and no covariates using Dirichlet process priors.
Scharfstein and Irizarry (2003) entered baseline covariates x into the missing
data mechanism nonparametrically (using generalized additive models). How-
ever, full extension of these ideas to the longitudinal setting with covariates
is nontrivial.
Model Checking
An alternative approach to posterior predictive checks for model checking
is to examine residuals for the observed data standardized with means and
variances conditional on being observed. See Dobson and Henderson (2003)
for a discussion.

CHAPTER 9
Informative Priors
and Sensitivity Analysis
9.1 Overview
Exploring sensitivity to unveriﬁable missing data assumptions, characterizing
the uncertainty about these assumptions, and incorporating subjective beliefs
about the distribution of missing responses, are key elements in any analy-
sis of incomplete data. The Bayesian paradigm provides a natural venue for
accomplishing these aims through the construction and integration of prior
information and beliefs into the analysis.
9.1.1 General approach
Recall that the extrapolation factorization of the full-data model is
p(ymis, yobs, r | ω)
=
p(ymis | yobs, r, ωE) p(yobs, r | ωO),
(9.1)
where ωE indexes the conditional distribution of missing responses, given ob-
served data (the extrapolation distribution), and ωO indexes the distribution
of observables. Our use of the term sensitivity analysis refers to assessment
of sensitivity of model-based inferences to assumptions that cannot be veriﬁed
or checked with data. In the missing data setting, inference about the full-
data distribution requires unveriﬁable assumptions about the extrapolation
distribution p(ymis | yobs, r, ωE).
Without assumptions such as a parametric model for the full-data response,
or constraints such as MAR for the missing data mechanism, the observed data
provide no information about the extrapolation distribution.
The general strategy espoused here is to work with the subset of sensitiv-
ity parameters ξS that satisfy the conditions laid out in Deﬁnition 8.1. The
sensitivity parameters are then used to encode prior beliefs about the missing
data mechanisms, either by ﬁxing their values at some constant, examining
inferences across a range of constants, or by assigning an appropriate prior
distribution.
The key idea is to separate what is identiﬁed from what is not, and to
use prior distributions — possibly including point mass priors — to inform
unidentiﬁed parts of the full-data model. Hence the priors are viewed as an
216

OVERVIEW
217
integral part of the full-data model. In that sense we are responding to the
observations and recommendations of Diggle (1999) that
... scientists are inveterate optimists. They ask questions of their data that their
data cannot answer. Statisticians can either refuse to answer such questions,
or they can explain what is needed over and above the data to yield an answer
and be properly cautious in reporting their results.
An important objective therefore is to make the priors understandable and
transparent to consumers of the model-based inferences, and to make them
amenable to incorporation of external information (e.g., from historical data
or elicited from experts). Indeed, Scharfstein et al. (1999) observe that
... the biggest challenge in conducting sensitivity analysis is the choice of one
or more sensibly parameterized functions whose interpretation can be commu-
nicated to subject matter experts with suﬃcient clarity ... .
This chapter outlines the strategy for model parameterization and prior
speciﬁcation for both mixture and selection models. Generally speaking, pa-
rameterizations for sensitivity analysis tend to be more straightforward with
mixture models; however, semiparametric selection models can be used for this
purpose as well. We illustrate using several examples of each type of model.
9.1.2 Global vs. local sensitivity analysis
Although we are discussing ideas that are more general than sensitivity anal-
ysis per se, it is important to distinguish between local and global sensitivity
analysis. Our concern is with the latter, but it is helpful to distinguish the
two with a brief comparison.
One type of local sensitivity analysis calls for parameters indexing depar-
tures from MAR to be varied in a neighborhood of MAR. In some circum-
stances, as discussed in Section 8.3, the parameters being varied are actually
identiﬁed by the observed data. This is especially true for models where the
full-data response p(y | ω) is assumed to follow a parametric model, and the
selection mechanism p(r | y, ω) has an assumed functional form.
To illustrate concretely, consider again the bivariate setting where Y2 may
be missing, and suppose
(Y1, Y2)T
∼
N(µ, Σ)
R | Y1, Y2
∼
Ber(π),
with
logit(π)
=
ψ0 + ψ1y1 + ψ2y2.
(9.2)
MAR holds when ψ2 = 0; however, as discussed in Chapter 8, the parameter
ψ2 is identiﬁed, even though Y2 is missing for some individuals.
For models that can be identiﬁed under MNAR by virtue of parametric
assumptions, imposing the MAR constraint (e.g., setting ψ2 = 0 above) may

218
INFORMATIVE PRIORS AND SENSITIVITY ANALYSIS
yield substantially diﬀerent ﬁt to observed data when compared to the ﬁt
under MNAR. Equivalently, the observed data likelihood diﬀers between the
MAR and MNAR speciﬁcations. Empirical illustrations can be found in Dig-
gle and Kenward (1994) and Baker et al. (2003). This phenomenon motivates
a local sensitivity approach for assessing sensitivity of inferences in a neigh-
borhood of MAR (Troxel et al., 2004; Zhang and Heitjan, 2006). Referring
to (9.2), the approach is to examine changes in inference about a full-data
parameter such as µ as a function of ψ2 in a neighborhood of ψ2 = 0. Nan-
dram and Choi (2002a,b) take a similar approach, using priors on identiﬁed
selection parameters such as ψ2.
Another type of local sensitivity examines the inﬂuence of individual data
points on model-based inference (Verbeke et al., 2001), using methods similar
to inﬂuence analysis in regression.
By contrast, global sensitivity analysis enables the analyst to examine infer-
ences about the full data over a class of full-data models that (a) are indexed
by one or more nonidentiﬁable parameters and (b) have identical or very sim-
ilar observed-data likelihoods.
Two approaches can be identiﬁed. The ﬁrst is to begin with a model for
the observed data, and expand it to admit one or more missing data mecha-
nisms that are consistent with the observed data distribution. A very general
approach, based on model expansion in terms of the missing data mechanism,
is developed in Robins (1997) and comprehensively described in van der Laan
and Robins (2003). Application to categorical data models can be found in
Vansteelandt et al. (2006), and application to longitudinal responses appears
in Scharfstein et al. (1999). For more general discussion of model expansion
and model uncertainty from a Bayesian viewpoint, see Draper (1995) and
Gustafson (2006).
A second but closely related approach to global sensitivity analysis begins
with speciﬁcation of a full-data distribution, followed by examination of in-
ferences across a range of values for one or more unidentiﬁed parameters. In
the Bayesian setup, priors would be placed on the unidentiﬁed parameters.
This approach can be traced at least to Rubin (1977), who assumes a full-
data distribution that is a mixture of normal distributions over respondents
and nonrespondents, and uses covariate information to partially inform the
distribution of nonrespondents. Assumptions about the diﬀerences between
respondents and nonrespondents are expressed as prior distributions.
Other approaches based on speciﬁcation of a full-data model with non-
identiﬁed parameters embedded have been developed mainly for categorical
data settings, as in Forster and Smith (1998). The work by Nandram and
Choi (2002a,b) is similar in spirit, but the ‘sensitivity parameters’ are iden-
tiﬁable. Copas and Eguchi (2005) give a general treatment that extends to
problems such as unmeasured confounding, but does not restrict attention to
using nonidentiﬁed parameters for sensitivity analysis.

SOME PRINCIPLES
219
Our development in this chapter relies mainly the second approach de-
scribed above. Inference is based on a speciﬁc model for the full-data distri-
bution, where one or more of its parameters are sensitivity parameters in the
sense of Deﬁnition 8.1. The sensitivity parameters are informed by (possibly
point mass) prior distributions that characterize assumptions about the miss-
ing data mechanism or the missing data itself. Our focus is on how to specify
and parameterize models so that are amenable either to (a) global sensitivity
analysis or (b) incorporation of informative prior information that, essentially,
fully characterizes the posterior distribution of the sensitivity parameters.
For mixture models, the material described here builds on Rubin (1977)
whose work examined informative priors for full-data models of survey data.
For selection models, we build on the ideas in Scharfstein et al. (1999) and
Scharfstein et al. (2003), who develop methods for semiparametric inference
about full-data parameters using incomplete (longitudinal) data. For categor-
ical responses, there is little qualitative diﬀerence between taking a selection
model or mixture model approach other than the interpretation of the sensi-
tivity parameters that they admit. For continuous responses, selection models
present a number of technical challenges for applied settings.
9.2 Some principles
In what follows, we articulate several principles for parameterizing models
for sensitivity analysis and incorporation of informative prior distributions.
These principles will be illustrated for both mixture and selection models in
the examples below.
1. Parameterize the model in terms of a sensitivity parameter ξS that sat-
isﬁes Deﬁnition 8.1. Consequently,
(a) assumptions about the missing data mechanism are fully encoded by
(possibly point mass) prior distributions;
(b) one can move smoothly through the space of full-data models with
MNAR missingness by varying the value of the sensitivity parameters;
(c) changing the value of the sensitivity parameter does not aﬀect the
observed data likelihood, and hence does not aﬀect ﬁt to the observed
data
2. Use an appropriately constructed prior distribution on the sensitivity
parameters to reﬂect certainty or uncertainty about the missing data
assumptions and other nonidentiﬁable aspects of the model.
3. Anchor or ‘center’ full-data models at MAR, such that the MAR as-
sumption coincides with a ﬁxed point on the range of the sensitivity
parameters ξS (e.g., ξS = 0). The eﬀect of the missing data assumptions
can then be viewed in terms of departures from MAR.

220
INFORMATIVE PRIORS AND SENSITIVITY ANALYSIS
The prior for ξS may be informed by a credible source of external infor-
mation, such as expert opinion, historical data, or some combination of the
two. The Bayesian setup is ideal for this because we can, at least in principle,
quantify uncertainty about assumptions through these priors.
A common practice for assessing sensitivity to missing data assumptions
is to compare inferences about a parameter of interest under several diﬀerent
full-data models that are not compatible in their ﬁt to the observed data. For
example, one might compare inferences under a selection model, a pattern
mixture model, and a shared parameter model. It is also common to com-
pare nested models that have diﬀerent assumptions about the missing data
mechanism, but also have diﬀerent observed data likelihoods (for example by
ﬁtting model (9.2) and comparing it to the nested model with ψ2 = 0). By
contrast, principles 1.(b) and 1.(c) emphasize that model parameterizations
should allow for exploration of sensitivity to missing data assumptions along
a continuum of model speciﬁcations that have similar or identical ﬁts to the
observed data.
9.3 Parameterizing the full-data model
Recall that any full-data model has an associated extrapolation factorization
(9.1). The component p(ymis | yobs, r, ωE) is the extrapolation distribution,
or the distribution of the missing data given the observed data; without para-
metric assumptions, it cannot be identiﬁed by observed data. The component
p(yobs, r | ωO) is the observed data distribution and is proportional in ω to
the observed data likelihood.
We seek a parameterization ξ(ω) = (ξS, ξM) such that ξM is identiﬁed
by observed data and ξS is a sensitivity parameter. Sensitivity analysis and
elicitation of informative priors is then be based on functions of ξS alone
(although this may not be possible in all full-data models).
To make inferences under MNAR, we construct a prior for ξS and draw
inference about the full-data distribution under this prior (Rubin, 1977). In
practice we will frequently introduce a redundant parameter (or vector of
parameters) ∆that captures explicitly the departures from MAR. The prior
on ξS can then be expressed in terms of ∆and ξM. The following example
provides a concrete illustration.
Example 9.1. Mixture model parameterization in terms of (ξS, ξM).
Consider a mixture model for univariate full-data response Y such that
Y | R = 1
∼
N(µ(1), σ2)
Y | R = 0
∼
N(µ(0), σ2)
R
∼
Ber(φ),
where, as usual, R = 1 indicates that Y is observed and R = 0 indicates

PARAMETERIZING THE FULL-DATA MODEL
221
missingness. Suppose the target of inference is θ = E(Y ); in terms of model
parameters,
θ = φµ(1) + (1 −φ)µ(0).
This model assumes observed and missing responses diﬀer in their mean
but share a common variance. The constraint on the variance is a modeling
assumption that cannot be veriﬁed by observed data. However, we can still
parameterize the model in terms of a sensitivity parameters ξS that satisﬁes
Deﬁnition 8.1.
The vector of full-data parameters is
ω = (µ(0), µ(1), σ, φ).
Deﬁne ξ(ω) = (ξS, ξM) such that
ξS
=
µ(0)
ξM
=
(µ(1), σ, φ).
The sensitivity parameter ξS does not appear in the observed data likelihood;
hence, the model has equivalent ﬁt to the observed data across all values of
ξS. It is easy to show that MAR holds when µ(0) = µ(1); or, in terms of the
sensitivity parameter, when ξS = µ(1). To ‘center’ the model at MAR, we
introduce a function h such that
ξS
=
h(ξM, ∆)
=
µ(1) + ∆.
(9.3)
MAR holds when ∆= 0. Departures from MAR are represented by varying
∆away from 0. Because ∆does not appear in the observed-data likelihood,
varying the missing data mechanism by moving smoothly through values of
∆in (9.3) has no eﬀect on ﬁt to the observed data. The eﬀect of missing data
assumptions on inferences about θ = E(Y ) can be seen by expressing θ as a
function of ∆; i.e.,
θ(∆)
=
φµ(1) + (1 −φ)µ(0)
=
φµ(1) + (1 −φ)(µ(1) + ∆)
=
µ(1) + (1 −φ)∆.
2
The ‘sensitivity’ to departures from MAR — or indeed from any missing
data mechanism characterized by ∆= ∆∗— can be measured in terms of
the derivative ∂θ(∆)/∂∆evaluated near ∆= ∆∗. In Example 9.1, θ(∆) is
linear and its derivative is the proportion of missing data, 1 −φ; however, the
same principle applies in more complex models (see also Troxel et al., 2004
and Zhang and Heitjan, 2006).
In the frequentist setting, a typical sensitivity analysis (see, e.g., Scharf-
stein et al., 1999) ﬁts the full-data model at ﬁxed values over a range of ∆,

222
INFORMATIVE PRIORS AND SENSITIVITY ANALYSIS
and examines how the inference of interest varies with ∆. For example, if the
target parameter is θ, the sensitivity analysis might comprise point estimates
and conﬁdence intervals for θ(∆) over a range for ∆. Viewed in the Bayesian
context, this corresponds to summarizing conditional posteriors, where con-
ditioning is done on ﬁxed values of ∆. It can also be viewed as examining
sensitivity to the prior, where all the priors under consideration are point
masses.
More generally, one can use priors for ∆that are centered at a speciﬁc
value and convey uncertainty about the missing data assumption by assigning
nonzero prior variance. This approach is used by Nandram and Choi (2002b)
in selection models with binary data, and by Rubin (1977) in mixture models
for univariate response. In the next two sections, we provide details on how
to specify priors in both selection and mixture models, using the principles
articulated above.
9.4 Specifying priors
Let h(ξM, ∆) be a one-to-one function such that
ξS
=
h(ξM, ∆),
where ∆captures the information about the missing data mechanism. To
anchor the model at MAR, there must exist some ∆0 such that h(ξS, ∆0)
implies MAR. Typically we can parameterize the model such that ∆0 = 0;
examples can be found in Chapter 8 (Examples 8.4, 8.5, and 8.6); see also
Example 9.1.
It is frequently useful to specify the joint prior p(ξS, ξM) as
p(ξS, ξM)
=
p(ξS | ξM) p(ξM),
(9.4)
where p(ξS | ξM) captures assumptions about the missing data mechanism.
In terms of the sensitivity parameter ∆, we can re-express p(ξS | ξM) as
p(ξS | ξM)
=
p(ξS | ξM, ∆) p(∆| ξM),
(9.5)
where
p(ξS | ξM, ∆)
=
I{ξS = h(ξM, ∆)}
is a point mass prior; that is, ξS is a deterministic function of (ξM, ∆) as
above.
The ﬁrst part of the prior in (9.5) simply reparameterizes ξS in terms of
departures from MAR; the second part, p(∆| ξM), encodes the missing data
mechanism and the uncertainty about it. A point mass prior
p(∆| ξM)
=
I{∆= ∆∗}
for a ﬁxed value ∆∗encodes a particular missing data mechanism such as

SPECIFYING PRIORS
223
MAR with absolute certainty. For example, if ∆= 0 implies MAR, then
speciﬁc MNAR mechanisms can be assumed by setting p(∆| ξM) = I{∆=
∆∗} for some ∆∗̸= 0.
Alternatively, priors that convey uncertainty about the missing data mech-
anism can be used. A non-degenerate prior such as
∆| ξM ∼N(d, D),
where d and D are hyperparameters giving the prior mean and variance for
∆, can be used to center the missing data mechanism at ∆= d, with prior
uncertainty quantiﬁed through a variance matrix D.
More generally, when ∆= ∆0 implies MAR, we can use the prior mean
and variance as follows (using the case of scalar ∆):
MAR with no uncertainty
E(∆| ξM) = ∆0
var(∆| ξM) = 0
MAR with uncertainty
E(∆| ξM) = ∆0
var(∆| ξM) > 0
MNAR with no uncertainty
E(∆| ξM) = ∆∗̸= ∆0
var(∆| ξM) = 0
MNAR with uncertainty
E(∆| ξM) = ∆∗̸= ∆0
var(∆| ξM) > 0
To illustrate, we revisit Example 9.1.
Example 9.2. Prior speciﬁcations for univariate mixture of normals (con-
tinuation of Example 9.1).
Recall that the full-data model follows the mixture distribution
Y | R = 1
∼
N(µ(1), σ2)
Y | R = 0
∼
N(µ(0), σ2)
R
∼
Ber(φ),
and that the full-data parameters are partitioned as
ξM
=
(µ(1), σ, φ)
ξS
=
µ(0).
Let
ξS = h(ξM, ∆) = µ(1) + ∆
(9.6)
so that ∆= 0 coincides with MAR.
Following (9.4) and (9.5), we can factor the prior as
p(ξM, ξS, ∆)
=
p(ξS | ξM, ∆) p(∆| ξM) p(ξM)
=
p(µ(0) | µ(1), σ, φ, ∆) p(∆| µ(1), σ, φ) p(µ(1), σ, φ).
To center the model at MAR with uncertainty about MAR governed by σ,
we can use the priors
µ(0) | µ(1), σ, φ, ∆
∼
I{µ(0) = µ(1) + ∆}
∆| µ(1), σ, φ
∼
N(0, σ2)
(9.7)

224
INFORMATIVE PRIORS AND SENSITIVITY ANALYSIS
and let p(µ(1), σ, φ) = p(µ(1))p(σ)p(φ), with vague priors for each.
To assume MAR with absolute certainty, we replace the N(0, σ2) prior in
(9.7) with the point mass I{∆= 0}. We see from (9.6) that departures from
MAR are measured in terms of µ(1) −µ(0); i.e., diﬀerences in mean response
between observed and missing Y . MNAR mechanisms can be represented in
(9.7) by using a distribution or point mass centered away from zero. For
example, setting
∆| µ(1), σ, φ
∼
I{∆= σ}
states that µ(0) diﬀers from µ(1) by one standard deviation. Fixed constants
also can be used, as can prior distributions with nonzero variance.
2
In the next two sections, we discuss how to apply this framework in mixture
models and in selection models.
9.5 Pattern mixture models
9.5.1 General parameterization
The factorization that deﬁnes pattern mixture models makes them a natural
class of models for partitioning ω into its identiﬁed and nonidentiﬁed compo-
nents. To see this, recall that the PMM factorization is
p(y, r | ω) = p(y | r, α(ω)) p(r | φ(ω)).
(9.8)
From here, we assume ω = (α, φ) and drop dependence of α and φ on ω.
Then,
p(y, r | α, φ)
=
p(y | r, α) p(r | φ)
=
p(ymis | yobs, r, α) p(yobs | r, α) p(r | φ).
(9.9)
Notice that the extrapolation model depends only on parameters α indexing
the mixture components p(y | r, α). This property makes the reparameteri-
zation of ω = (α, φ) as ξ(ω) = (ξS, ξM) relatively straightforward in many
practical settings. To illustrate in a simple case, we use the mixture of bivariate
normals from Examples 5.9 and 5.11.
Example 9.3. Sensitivity parameterizations for mixture of bivariate normals.
From Example 5.11, recall that the full data are Y = (Y1, Y2)T and R =
I{Y2 observed}. Assume
Y | R = r
∼
N(µ(r), Σ(r)),
R
∼
Ber(φ).
Within pattern r, we can reparameterize the distribution of Y as
Y2 | Y1, R = r
∼
N(β(r)
0
+ β(r)
1 Y1, σ(r)
2|1),
Y1 | R = r
∼
N(µ(r)
1 , σ(r)
11 ).

PATTERN MIXTURE MODELS
225
Hence, in terms of (9.9),
α
=

β(r)
0 , β(r)
1 , σ(r)
2|1, µ(r)
1 , σ(r)
11 : r = 0, 1

.
Now deﬁne ξ(ω) = ξ(α, φ) = (ξS, ξM) such that
ξS
=
(β(0)
0 , β(0)
1 , σ(0)
2|1),
ξM
=
(β(1)
0 , β(1)
1 , σ(1)
2|1, µ(0)
1 , µ(1)
1 , σ(1)
11 , σ(0)
11 , φ).
Notice that ξS is exclusively a function of α.
The model can be centered at MAR as follows. Let ∆= (∆0, ∆1, ∆2)T
with ∆2 > 0, and deﬁne the function h(ξM, ∆) as
h1(ξM, ∆)
=
β(1)
0
+ ∆0
h2(ξM, ∆)
=
β(1)
1
+ ∆1
h3(ξM, ∆)
=
∆2σ(1)
2|1.
By setting ξS = h(ξM, ∆), we have
β(0)
0
=
β(1)
0
+ ∆0
β(0)
1
=
β(1)
1
+ ∆1
σ(0)
2|1
=
∆2σ(1)
2|1.
Setting (∆0, ∆1, ∆2) = (0, 0, 1) yields MAR. Priors on ∆can be used to
encode alternate missing data assumptions.
2
9.5.2 Using model constraints to reduce dimensionality of sensitivity
parameters
In the previous example, a relatively simple model yields 3 sensitivity param-
eters. As the dimension increases, the number of sensitivity parameters will
grow quickly. In the next example, we show how to use a simple modeling
constraint to reduce the dimension of ∆to 1. Our analysis of the Growth
Hormone data in Chapter 10 takes a similar approach for a trivariate normal
distribution.
Example 9.4. Parameterization and prior speciﬁcations for mixture of bi-
variate normals having common variance structure.
Continuing with Example 9.3, if we constrain Σ(r) = Σ, part of the full-data
is identiﬁed by parameter constraints. It is easy to show that
β(1)
1
=
β(0)
1
=
β1
σ(1)
2|1
=
σ(0)
2|1
=
σ2|1
σ(1)
11
=
σ(0)
11
=
σ11.

226
INFORMATIVE PRIORS AND SENSITIVITY ANALYSIS
Hence,
ξS
=
β(0)
0
ξM
=
(β(1)
0 , β1, σ2|1, σ11, φ).
To center the model at MAR, let ξS = β(0)
0
= h(ξM, ∆), where
h(ξM, ∆)
=
β(1)
0
+ ∆.
The prior speciﬁcation can follow (9.4) and (9.5); i.e.,
p(ξS, ξM, ∆)
=
p(ξS | ξM, ∆) p(∆| ξM) p(ξM).
For the ﬁrst factor, we have
β(0)
0
| ∆, ξM
∼
I{β(0)
0
= β(1)
0
+ ∆}.
It may be useful (but not necessary) to calibrate the prior for ∆given ξM
using identiﬁed model parameters; for example,
∆| ξM
∼
N(d, σ2|1).
If d = 0, the prior is centered at MAR. The conditional variance σ2|1 =
var(Y2 | Y1) is chosen because the sensitivity parameter ξS = β(0)
0
characterizes
the conditional mean E(Y2 | Y1, R = 0). Finally, priors for components of ξM
can be speciﬁed in the usual ways (e.g., by assuming independence between
components and assigning vague or ﬂat priors).
2
The general strategy illustrated by Example 9.4 — to specify a constrained
full-data model in conjunction with a parameterization that admits sensitivity
parameters — is used for analysis of the Growth Hormone study and the
OASIS study in Chapter 10.
9.6 Selection models
The extrapolation factorization (9.1) is not generally easy to derive in closed
form for parametric selection models. In most parametric selection models
(Section 8.3), all full-data parameters are identiﬁed, including the parameters
ωE of the extrapolation distribution. The parameters ωE are complex func-
tions of the missing data mechanism parameters ψ and the full-data response
model parameters θ, and it is not generally possible to ﬁnd a reparameteri-
zation ξ(ω) = ξ(θ, ψ) = (ξS, ξM) that satisﬁes the criteria of Deﬁnition 8.1.
An alternative is to use semiparametric selection models. By semiparamet-
ric we mean that the full-data distribution p(y | θ) is left unspeciﬁed, but
the missing data mechanism p(r | y, ψ) may have a parametric component.
If the distribution p(y | θ) has discrete support (as when Y is categorical),
then semiparametric models will usually admit sensitivity parameters. When

SELECTION MODELS
227
Y is continuous, the situation is somewhat more complicated, but candidate
sensitivity parameters can often still be found.
In either case, model parameterization has in important bearing on prior
speciﬁcation and posterior inference. We use three examples to illustrate. The
ﬁrst two illustrate the importance of model parameterization with binary re-
sponse. The third shows how to parameterize a selection model for a contin-
uous univariate response.
Example 9.5. Prior speciﬁcation in nonparametric and semiparametric se-
lection models for longitudinal binary data.
In this example we revisit the nonparametric selection model for bivariate
binary data with missingness in the second observation; this model was intro-
duced in Section 8.3.7. Let Y = (Y1, Y2)T denote the full-data response, with
R = 1 if Y2 is observed and R = 0 if missing. The entire full-data distribution
p(y, r | ω) can be enumerated as in Table 9.1. The parameters ω(1)
00 , . . . , ω(1)
11 ,
corresponding to R = 1, all are identiﬁable from the observed data. The sums
ω(0)
1+
=
ω(0)
00 + ω(0)
01
ω(0)
0+
=
ω(0)
10 + ω(0)
11 ,
corresponding to R = 0, also are identiﬁable. Let
ωI
=
(ω(1)
00 , ω(1)
01 , ω(1)
10 , ω(1)
11 , ω(0)
0+, ω(0)
1+),
denote the set of identiﬁed parameters in this multinomial distribution. Before
moving to the discussion of the selection model, note that this model admits
a set of sensitivity parameters. Referring to Deﬁnition 8.1, if we set ξM = ωI,
then either ξS = (ω(0)
00 , ω(0)
10 ) or ξS = (ω(0)
01 , ω(0)
11 ) meets the criteria for a
Table 9.1 Multinomial parameterization of full-data distribution for bivariate binary
data with possibly missing Y2.
R
Y1
Y2
p(y1, y2, r | ω)
0
0
0
ω(0)
00
0
0
1
ω(0)
01
0
1
0
ω(0)
10
0
1
1
ω(0)
11
1
0
0
ω(1)
00
1
0
1
ω(1)
01
1
1
0
ω(1)
10
1
1
1
ω(1)
11

228
INFORMATIVE PRIORS AND SENSITIVITY ANALYSIS
sensitivity parameter. Essentially, the full-data distribution has two degrees
of freedom unaccounted for by observed data.
The corresponding selection model can be written as
Y1
∼
Ber(θ1)
Y2 | Y1
∼
Ber(θ2|1)
R | Y1, Y2
∼
Ber(π),
with
logit(θ1)
=
α
logit(θ2|1)
=
β0 + β1y1
logit(π)
=
ψ0 + ψ1y1 + ψ2y2 + ψ3y1y2.
Priors are usually speciﬁed as
p(ψ, θ)
=
p(ψ)p(θ),
(9.10)
where here, θ = (α, β) (Scharfstein et al., 2003). It is often further assumed
that
p(ψ) = p(ψ0, ψ1)p(ψ2, ψ3),
(9.11)
i.e., the sensitivity parameters in the MDM are a priori independent of the
other MDM parameters. These independent priors do not (in general) im-
ply that p(ψ2, ψ3, ωI) — the corresponding joint prior on the sensitivity pa-
rameters (ψ2, ψ3) and the identiﬁed parameters ωI — can be factored as
p(ψ2, ψ3)p(ωI). The prior dependence between (ψ2, ψ3) and ωI has implica-
tions for posterior inference. In particular, the prior and posterior for the
sensitivity parameters, (ψ2, ψ3) are not (in general) equivalent unlike in mix-
ture models. Before exploring the a priori dependence and the implications for
posterior inference, we note that this a priori dependence is desirable. Scharf-
stein et al. (2003) (p. 501) argue that it makes little sense to construct (or
elicit) independent priors on ωI and (ψ2, ψ3). But, by specifying independent
priors on θ and ψ, the dependence between ωI and (ψ2, ψ3) is induced.
To see how ωI and (ψ2, ψ3) are a priori dependent, we explore the reparam-
eterization of the model from (ωI, ψ2, ψ3) to (θ, ψ0, ψ1, ψ2, ψ3). The function
to move between these parameterizations is indexed by (ψ2, ψ3),
ωI = hψ2,ψ3(θ, ψ0, ψ1).
We derive this function for the simple cross-sectional setting of a binary re-
sponse. Consider the selection model
Y
∼
Ber(θ)
logit{P(R = 1 | y)}
=
ψ0 + ψ1y,
where R = 1 means Y is observed. In this model, ψ1 is the sensitivity param-

SELECTION MODELS
229
eter. We denote the identiﬁed parameters as ωI = (θ1, α), where
θ1
=
P(Y = 1 | R = 1)
α
=
P(R = 1).
With a little algebra, it can be shown that

θ
ψ0
 
= hψ1(θ1, α) =


αθ1 +
1 −α
1 + θ1 exp(−ψ1)/(1 −θ1)
logit(α) + log{(1 −θ1) + exp(−ψ1)θ1}

.
The a priori dependence can be seen explicitly by deriving the Jacobian using
these results. For a related discussion, see Gustafson (2006).
As a result, a priori independence between the full data response model
parameters and the missing data mechanism parameters given in (9.10) and
between the sensitivity parameters and the other parameters in the missing
data mechanism given in (9.11) does not imply a priori independence between
ωI and (ψ2, ψ3) (the sensitivity parameters). This lack of a priori indepen-
dence between ωI and (ψ2, ψ3) implies that the posterior for the sensitivity
parameters is not equal to the prior; i.e.,
p(ψ2, ψ3 | yobs, r)
̸=
p(ψ2, ψ3).
However, it is often still the case that
p(ψ2, ψ3 | y, r)
≈
p(ψ2, ψ3).
(9.12)
To see why the equality between prior and posterior does not hold here, we
write
p(ψ2, ψ3 | yobs, r)
∝

p(yobs, r | ψ2, ψ3, ωI)p(ωI | ψ2, ψ3)p(ψ2, ψ3)dωI
=

p(yobs, r | ωI)p(ωI | ψ2, ψ3)p(ψ2, ψ3)dωI
=
p(yobs, r | ψ2, ψ3)p(ψ2, ψ3).
The ﬁrst equality holds given that ωI parameterizes the observed data distri-
bution. When we integrate out ωI, we are left with an observed data distribu-
tion that depends on (ψ2, ψ3). However, in a nonparametric selection model,
it will typically be the case that
p(yobs, r | ψ2, ψ3)
p(yobs, r)
≈
1.
That is, p(yobs, r | ψ2, ψ3) is almost constant with respect to (ψ2, ψ3). How
well (9.12) holds for diﬀerent semiparametric speciﬁcations is an area of on-
going work.
2
We illustrate the approximate equality given in (9.12) using the OASIS
data (described in Section 1.6) in the next example.

230
INFORMATIVE PRIORS AND SENSITIVITY ANALYSIS
Example 9.6. Prior dependence in nonparametric selection models using data
from the OASIS study.
For simplicity, we use the ﬁrst and second observations in the OASIS smoking
cessation trial. We ﬁt the nonparametric selection model from Example 9.5,
Y1
∼
Ber(θ1)
Y2 | Y1
∼
Ber(θ2|1)
R | Y1, Y2
∼
Ber(π),
with
logit(θ1)
=
α
logit(θ2|1)
=
β0 + β1y1
logit(π)
=
ψ0 + ψ1y1 + ψ2y2 + ψ3y1y2,
separately for each treatment. Diﬀuse normal priors on (θ, ψ0, ψ1) and inde-
pendent N(0, 1) priors on ψ2 and ψ3 are used. The normal priors on ψ2 and
ψ3 correspond to an MAR missing data mechanism with uncertainty. Neither
ψ2 nor ψ3 appear in the observed data likelihood, but they are not a priori
independent of the identiﬁed parameters (see Example 9.5). However, for non-
parametric selection models, we expect the approximate equivalence to hold,
as in (9.12).
Table 9.2 shows posterior summaries of ψ2 and ψ3 from using the N(0, 1)
priors described above. Posterior means and variances are very close to 0
and 1, respectively, supporting the approximate equivalence given in (9.12).
The posterior mean and credible interval of the odds ratio for treatment eﬀect
are 1.3 (.3, 3.3). If instead of the normal priors, we use MAR point mass priors
(no uncertainty), the posterior mean and credible interval of the odds ratio
are 1.3 (.5, 2.9). As expected, the posterior credible intervals are wider under
the N(0, 1) priors.
2
The ﬁnal example of semiparametric selection models illustrates their con-
struction for a continuous univariate response.
Table 9.2 Posterior means and standard deviations for the sensitivity parameters in
the nonparametric selection model.
Treatment
Parameter
Posterior Mean
Posterior SD
ET
ψ2
.01
1.00
ψ3
.06
1.00
ST
ψ2
.03
.99
ψ3
–.01
.99

FURTHER READING
231
Example 9.7. Sensitivity analysis with semiparametric selection models for
cross-sectional continuous response.
Denote the response as Y and the missing indicator as R. Suppose we specify
the following semiparametric selection model using a mixture of Dirichlet
processes model (cf. Section 3.6):
logit{P(Ri = 1 | yi, ψ)}
=
ψ0 + ψ1yi
Yi | µi, σ2
∼
N(µi, σ2)
µi
∼
G
G
∼
DP(G0, α),
(9.13)
where θ = (G, σ2) and (G0, α) are ﬁxed hyperparameters. In Scharfstein et al.
(2003), a similar model was speciﬁed with a prior of the form (9.10). Clearly,
if no distributional assumptions are made about the full-data response y, the
data will provide no information on the missing data mechanism parameter
ψ1, and the MDP on the distribution of the full-data response allows ψ1 to be
essentially unidentiﬁed; by contrast, it is identiﬁed when using a parametric
model for the full-data response. Here, ψ1 is a sensitivity parameter.
2
Semiparametric selection models can provide a viable alternative to mixture
models for sensitivity analyses. The underlying factorization into the full-data
response model and the missing data mechanism facilitates elicitation of priors
for parameters indexing the missing data mechanism (e.g., ψ1 in (9.13)). See
Scharfstein et al. (2003) and Scharfstein et al. (2006) for examples.
The main stumbling block to implementation of these models in general
is the feasibility of their speciﬁcation for longitudinal data with many time
points, especially if the responses are continuous (cf. Chapter 8). The com-
putational challenges also are nontrivial. For complex longitudinal settings,
the full-data response model can be speciﬁed semiparametrically (Daniels and
Scharfstein, 2007). Construction of sensitivity analysis and informative priors
will still be valid as long as (9.12) holds.
9.7 Further reading
Model uncertainty and incomplete data
Copas and Eguchi (2005) also emphasize the importance of characterizing
uncertainty in incomplete data problems, but more from the perspective of
mis-specifying the full-data model. To account for model uncertainty, they
suggest rules for adjusting standard errors and conﬁdence intervals for pa-
rameters of interest. Work by Forster and Smith (1998) is closely related, and
deals with categorical data. Recent work by Gustafson (2006) describes model
expansion and model contraction for handling full-data models that are only
partially identiﬁed by observed data. Illustrations related to measurement er-

232
INFORMATIVE PRIORS AND SENSITIVITY ANALYSIS
ror and unmeasured confounding are provided, both of which can be viewed
as missing data problems.

CHAPTER 10
Case Studies:
Nonignorable Missingness
10.1 Overview
This chapter provides three detailed case studies that illustrate analyses under
the missing not at random assumption. For each of our three examples, we
give details on specifying both the full-data model and the appropriate prior
distributions. The analyses shown here were implemented using WinBUGS
software; the code is available from the book Web site.
In Section 10.2, pattern mixture models are used to analyze data from two
arms of the Growth Hormone Study described in Section 1.3. We ﬁt the model
using both a multivariate normal model (assuming ignorability) and a pattern
mixture model (assuming nonignorable MAR). Departures from MAR in the
mixture model are incorporated by introducing sensitivity parameters and
constructing appropriate prior distributions for them.
In Section 10.3, both pattern mixture models and selection models are used
to analyze data from the OASIS Study, described in Section 1.6. We examine
the role of model speciﬁcation by comparing inferences using both selection
and pattern mixture models. Analyses under MNAR are illustrated using
a pattern mixture model with informative priors elicited from experts, and
using a parametric selection model that identiﬁes the parameters governing
departures from MAR. The latter analysis highlights some limitations of using
parametric selection models with informative priors and sensitivity analysis.
The third case study, in Section 10.4, uses a mixture of varying coeﬃcient
models to analyze longitudinal CD4 counts from the Pediatric AIDS trial,
described in Section 1.7. The model used here permits continuous dropout
times, and assumes the CD4 intercepts and slopes depend on dropout time via
smooth but unspeciﬁed functions. The mixture of VCM analysis is compared
to the standard random eﬀects model under MAR.
233

234
CASE STUDIES: NONIGNORABLE MISSINGNESS
10.2 Growth Hormone study: Pattern mixture models and
sensitivity analysis
10.2.1 Overview
The Growth Hormone Study is described in detail in Section 1.3. In short,
the trial examines eﬀect of various combinations of growth hormone and ex-
ercise on 1-year changes in quadriceps strength in a cohort of 160 individuals.
Measurements are taken at baseline, 6 months, and 12 months. Of the 160 ran-
domized, only 111 (69%) had complete follow-up. The observed data appear
in Table 1.2.
To illustrate various types of models, we conﬁne attention to the arms using
exercise plus placebo (EP; n = 40) and exercise plus growth hormone (EG;
n = 38). On arm EP, 7 individuals (18%) had only one measurement, 2 (5%)
had two measurements, and 31 (78%) had three. Missingness is more prevalent
on the EG arm, with 12 (32%) having only one measurement, 4 (11%) having
two, and 22 (58%) having three. Missingness is caused by dropout and follows
a monotone pattern (Table 1.2).
The objective is to compare mean quadriceps strength at 12 months be-
tween the two groups. We use a pattern mixture approach. The pattern mix-
ture model is ﬁrst speciﬁed under MAR; we then elaborate the speciﬁcation to
allow MNAR using two additional nonidentiﬁed parameters. We illustrate how
to conduct and interpret a sensitivity analysis using posteriors over a range
of point mass priors for the sensitivity parameters. In addition, we show how
to construct and use informative priors to obtain a single summary about the
treatment eﬀect.
The relevant variables for the full-data model are:
(Y1, Y2, Y3)T
=
quad strength measures at months 0, 6, 12,
(R1, R2, R3)T
=
response indicators (1 = observed, 0 = missing).
Z
=
treatment group indicator (1 = EG, 0 = EP),
We deﬁne patterns based on observed follow-up time S = 
j Rj. The objec-
tive is to compare mean quad strength between the two arms at month 12;
the treatment eﬀect is denoted by
θ
=
E(Y3 | Z = 1) −E(Y3 | Z = 0).
10.2.2 Multivariate normal model under ignorability
For reference, we compare the mixture model results to an ignorable model
where we assume the full-data distribution is multivariate normal within treat-
ment arm. This analysis was done in Section 7.2.

GROWTH HORMONE STUDY
235
10.2.3 Pattern mixture model speciﬁcation
In this section we provide the full-data model speciﬁcation, show how to con-
strain the model under MAR, and show how to parameterize the model in
terms of sensitivity parameters that capture departures from MAR.
Our analysis is based on the pattern mixture model speciﬁed in Exam-
ple 8.5. To avoid too many subscripts, we suppress treatment subscripts z
until they are needed; hence the model below applies separately to each treat-
ment arm.
The full-data distribution is a mixture over follow-up times, with com-
ponents (patterns) deﬁned by S ∈{1, 2, 3}. Nonidentiﬁed components are
marked by ⋆; parameters for these do not appear in the observed data likeli-
hood. The complete model speciﬁcation is as follows:
Y1 | S = 1
∼
N(µ(1), σ(1))
Y1 | S = 2
∼
N(µ(2), σ(2))
Y1 | S = 3
∼
N(µ(3), σ(3))
⋆
Y2 | Y1, S = 1
∼
N(α(1)
0
+ α(1)
1 Y1, τ(1)
2 )
Y2 | Y1, S = 2
Y2 | Y1, S = 3
!
∼
N(α(≥2)
0
+ α(≥2)
1
Y1, τ(≥2)
2
)
⋆
Y3 | Y1, Y2, S = 1
∼
N(β(1)
0
+ β(1)
1 Y1 + β(1)
2 Y2, τ(1)
3 )
⋆
Y3 | Y1, Y2, S = 2
∼
N(β(2)
0
+ β(2)
1 Y1 + β(2)
2 Y2, τ(2)
3 )
Y3 | Y1, Y2, S = 3
∼
N(β(3)
0
+ β(3)
1 Y1 + β(3)
2 Y2, τ(3)
3 )
S
∼
Mult(φ).
(10.1)
The multinomial parameter is φ = (φ1, φ2, φ3) with φs = P(S = s) for
s ∈{1, 2, 3} and 
s φs = 1.
Although the model speciﬁcation looks cumbersome, it follows the struc-
ture p(ymis | yobs, s) p(yobs | s) p(s), where the components marked with ⋆
comprise p(ymis | yobs, s), and those not marked comprise p(yobs | s) and p(s).
10.2.4 MAR constraints for pattern mixture model
Under MAR, the distribution of missing data is identiﬁed using the constraints
listed in Theorem 8.1. Let ps(y) denote p(y | S = s). For the missing data in
pattern S = 1, the MAR constraints imply
p1(y2 | y1)
=
p(≥2)(y2 | y1),
p1(y3 | y1, y2)
=
p3(y3 | y1, y2);

236
CASE STUDIES: NONIGNORABLE MISSINGNESS
and for pattern S = 2, we have
p2(y3 | y1, y2)
=
p3(y3 | y1, y2).
By assuming the unidentiﬁed components of our model are normally dis-
tributed, MAR is satisﬁed by equating means and variances. For example, to
identify p1(y2 | y1), we set
α(1)
0
+ α(1)
1 y1
=
α(≥2)
0
+ α(≥2)
1
y1,
τ(1)
2
=
τ(≥2)
2
.
In order for the ﬁrst constraint to hold for all possible values of y1, we require
α(1)
0
=
α(≥2)
0
,
α(1)
1
=
α(≥2)
1
.
(10.2)
Similarly, to identify p1(y3 | y1, y2) and p2(y3 | y1, y2), we equate regression
parameters and variance components as follows, for s = 1, 2:
β(s)
0
=
β(3)
0 ,
β(s)
1
=
β(3)
1 ,
β(s)
2
=
β(3)
2 ,
τ (s)
3
=
τ(3)
3 .
(10.3)
All told, 22 constraints are needed to impose the MAR assumption for this
model; for each treatment arm, there are eight constraints on regression pa-
rameters and three on variance components.
10.2.5 Parameterizing departures from MAR
We can reparameterize the model in terms of sensitivity parameters to em-
bed the MAR constraint in a larger class of MNAR models for the full-data
distribution. Note ﬁrst that the full-data model can easily be reparameterized
as ξ(ω) = (ξS, ξM), where elements of ξS are parameters from distributions
in (10.1) marked with ⋆, and elements of ξM are the remaining parameters.
Speciﬁcally,
ξS
=

α(1)
0 , α(1)
1 , τ(1)
2
β(s)
0 , β(s)
1 , β(s)
2 , τ(s)
3
s = 1, 2
ξM
=









µ(s), σ(s)
s = 1, 2, 3
α(≥2)
0
, α(≥2)
1
, τ(≥2)
2
β(3)
0 , β(3)
1 , β(3)
2 , τ(3)
3
φ1, φ2, φ3.

GROWTH HORMONE STUDY
237
Moreover, ξS meets the criteria for a sensitivity parameter given in Deﬁni-
tion 8.1.
Following the strategy laid out in Section 9.3, departures from MAR can
be captured by reparameterizing ξS in terms of ξM and a set of parameters
∆via ξS = h(ξM, ∆). Components of h are most easily described in several
parts. First, expanding on regression parameter constraints in (10.2), we have
α(1)
0
=
h1(ξM, ∆)
=
α(≥2)
0
+ ∆(1:2)
α0
,
α(1)
1
=
h2(ξM, ∆)
=
α(≥2)
1
+ ∆(1:2)
α1
,
(10.4)
where the superscript (1:2) on ∆denotes a relation between parameters from
patterns 1 and (≥2) (using a slight abuse of notation).
The next six components h3, . . . , h8 of h reparameterize the β coeﬃcients
in ξS as
β(s)
0
=
β(3)
0
+ ∆(s:3)
β0
β(s)
1
=
β(3)
1
+ ∆(s:3)
β1
β(s)
2
=
β(3)
2
+ ∆(s:3)
β2
(10.5)
for s = 1, 2. The ∆parameters here are interpreted similarly to (10.4). For
example, ∆(1:3)
β0
is the diﬀerence between β(1)
0 , which is not identiﬁed, and
β(3)
0 , which is.
Finally, h9, h10, h11 deal with variance components:
τ(1)
2
=
∆(1:2)
τ2
τ (≥2)
2
τ (1)
3
=
∆(1:3)
τ3
τ (3)
3
τ (2)
3
=
∆(2:3)
τ3
τ (3)
3 .
(10.6)
The full collection of ∆parameters is denoted
∆
=
(∆α, ∆β, ∆τ),
where
∆α
=
(∆(1:2)
α0
, ∆(1:2)
α1
),
∆β
=
(∆(1:3)
β0
, ∆(1:3)
β1
, ∆(1:3)
β2
, ∆(2:3)
β0
, ∆(2:3)
β1
, ∆(2:3)
β2
),
∆τ
=
(∆(1:2)
τ2
, ∆(1:3)
τ3
, ∆(2:3)
τ3
).
The MNAR model includes MAR as a special case: setting ∆α = 0, ∆β = 0,
and ∆τ = 1 yields MAR.

238
CASE STUDIES: NONIGNORABLE MISSINGNESS
10.2.6 Constructing priors
Following (9.4) and (9.5), we factor the prior on (ξS, ξM, ∆) as
p(ξS, ξM, ∆)
=
p(ξS | ξM, ∆) p(∆| ξM) p(ξM),
where
p(ξS | ξM, ∆)
=
I{ξS = h(ξM, ∆)}
is a point mass, and p(∆| ξM) reﬂects prior beliefs about the departures from
MAR. For example, the prior
p(∆α, ∆β, ∆τ | ξM)
=
I{∆α = 0, ∆β = 0, ∆τ = 1}
(10.7)
assumes MAR with absolute certainty.
To move beyond the MAR assumption, we can make alternate choices for
p(∆| ξM). A sensitivity analysis can be conducted by examining posterior in-
ferences about treatment eﬀect (or any parameter of interest) over a bounded
set of values (say D) for the ∆parameters. Concretely, this involves summa-
rizing posterior treatment eﬀects based on the set
{p(∆| ξM) = I{∆= ∆∗} : ∆∗∈D}
of point mass priors. A practical question arises as to the choice of D because
its dimension can be high (in this case, there are 22 sensitivity parameters).
In the analysis that follows, we ﬁrst draw inferences using the MAR prior,
and then illustrate the implementation of a sensitivity analysis where D is
calibrated using the posterior of ξM under MAR.
10.2.7 Analysis using point mass MAR prior
In this analysis we use the prior (10.7) for ∆and place diﬀuse priors on ξM as
follows (for simplicity we do not include superscripts and subscripts — e.g.,
all µ parameters have the same prior; refer to (10.1) for speciﬁcs):
µ
∼
N(0, 106),
α, β
∼
N(0, 104),
σ2
∼
N(0, 202) I{0 < σ2 < 104},
τ2
∼
N(0, 202) I{0 < τ2 < 104},
φ
∼
Dirichlet(1, 1, 1).
Posterior inference was implemented using two parallel Markov chains with
25,000 iterations each. We discarded results from the ﬁrst 5000 per chain
(burn-in) and based posterior inference on the remaining 40,000 draws.
Inference for the mean for each treatment group at each time point is
summarized in Table 10.1. For comparison we also include inferences from a
standard multivariate normal speciﬁcation, described and ﬁt in Section 7.2. As

GROWTH HORMONE STUDY
239
expected, the posterior means are very similar; posterior variability is slightly
higher in the mixture model, which can be attributed to the larger number of
parameters.
Table 10.1 Growth hormone trial: posterior mean (s.d.) for quadriceps strength at
each time point, stratiﬁed by treatment group. MVN = multivariate normal dis-
tribution assumed for joint distribution of responses; MAR = missing at random
constraints; MNAR-1 = ∆assumed common across treatment groups; MNAR-2 =
∆assumed diﬀerent by treatment group. Both MNAR analyses use uniform priors
for ∆bounded away from zero. See Section 10.2.8 for details.
Pattern Mixture Models
Treatment
Month
MVN
MAR
MNAR-1
MNAR-2
EP
0
65 (4.2)
66 (4.6)
66 (4.6)
66 (4.6)
6
81 (4.4)
82 (4.7)
80 (4.9)
80 (4.9)
12
73 (3.7)
73 (4.0)
70 (4.3)
69 (4.6)
EG
0
69 (4.2)
69 (4.4)
69 (4.4)
69 (4.4)
6
81 (6.0)
81 (6.4)
78 (6.8)
78 (6.8)
12
78 (6.3)
78 (6.7)
73 (7.4)
72 (8.1)
Diﬀerence at 12 mos.
5.7 (7.3)
5.4 (7.8)
3.1 (8.2)
2.6 (9.3)
10.2.8 Analyses using MNAR priors
The pattern mixture model can be expanded to allow for MNAR through
appropriate speciﬁcation of the prior p(∆| ξM). This prior will convey infor-
mation about the degree to which the distribution of missing responses diﬀers
from that of observed responses. In general, priors for components of ∆α and
∆β will shift the posterior means away from their MAR values, with ∆> 0
indicating that the mean response is higher for missing observations than for
observed ones, relative to MAR.
Under this parameterization of the PMM, the conditional variance param-
eters τ from nonidentiﬁed distributions do not appear in the posterior dis-
tribution of the marginal mean parameters; hence priors for components of
∆τ will not aﬀect posterior inference about marginal means. This reduces the
number of sensitivity parameters from 22 to 16.

240
CASE STUDIES: NONIGNORABLE MISSINGNESS
Speciﬁcation and calibration of MNAR priors
There are two obstacles that must be overcome in specifying MNAR priors:
the high dimension of the parameter space for ∆, and the need to make a
speciﬁc choice of prior distribution. Even if a series of point mass priors will
be used to conduct a sensitivity analysis, we must specify the range of values
to include.
We focus on departures from MAR in terms of the intercept sensitivity
parameters (∆(1:2)
α0
, ∆(1:3)
β0
, ∆(2:3)
β0
), reducing the number of sensitivity param-
eters to six (three for each treatment group). The justiﬁcation for this choice
is based mainly on interpretability; each represents a diﬀerence in the condi-
tional mean of Yj given previous Y ’s between patterns. For example,
∆(1:2)
α0
=
E(Y2 | Y1, S = 1) −E(Y2 | Y1, S ≥2)
represents the diﬀerence in E(Y2 | Y1) between those with missing and ob-
served Y2. Similarly,
∆(1:3)
β0
=
E(Y3 | Y2, Y1, S = 1) −E(Y3 | Y2, Y1, S = 3),
∆(2:3)
β0
=
E(Y3 | Y2, Y1, S = 2) −E(Y3 | Y2, Y1, S = 3).
As a ﬁnal constraint, we assume ∆(1:3)
β0
= ∆(2:3)
β0
, and denote the common
parameter by ∆(•:3)
β0
. The implication of our ﬁnal constraint is that, relative
to MAR, the diﬀerence in E(Y3 | Y1, Y2) between those with observed and
missing Y3 is common across missing data patterns S = 1 and S = 2. Hence
the reduced set of ∆parameters for sensitivity analysis and incorporation of
informative priors is denoted by
∆= (∆(1:2)
α0
, ∆(•:3)
β0
).
(10.8)
There are four sensitivity parameters in all: one set for each treatment group.
Recall that the treatment eﬀect of interest is
θ
=
E(Y3 | Z = 1) −E(Y3 | Z = 0),
with θ > 0 if quad strength on EG is greater than on EP. With respect to
summarizing posterior inference about θ under MNAR, each of our analyses
below has two components: First, we examine sensitivity to departures from
MAR in terms of posterior mean and posterior probabilities for treatment
eﬀect over a set of values ∆∈D. A sensitivity analysis summarizing the
range of posterior inferences about mean treatment eﬀect over a range of
missing data mechanisms is carried out by summarizing the posterior mean
E(θ | yobs, z, ∆) and posterior probability P(θ > 0 | yobs, z, ∆) over all ∆∈
D. In Analysis 1 we give ranges for these quantities over D, and in Analysis 2
we use contour plots to identify subsets of D where inference about treatment
eﬀect might diﬀer substantially from MAR. Second, we use informative priors

GROWTH HORMONE STUDY
241
for ∆to generate an overall inference about θ. In our case, we assume ∆is
uniformly distributed over a domain D.
In general, components of ∆can range over the entire real line. It is there-
fore necessary to calibrate or otherwise bound its domain D. The direction of
departure from MAR can typically be informed by context. In our analyses be-
low, we assume dropouts have lower mean quad strength than non-dropouts.
In terms of the model, our assumption is that for j = 2, 3, the conditional
mean of quad strength Yj given the past is lower for those with missing Yj
compared to those with observed Yj.
The scale of departure from MAR can be determined in any number of
ways. Our approach is to use the variability in the observed data. Because
the components of ∆in (10.8) correspond to intercepts in the conditional
distributions p≥2(y2 | y1) and p3(y3 | y1, y2), a natural metric for scaling D
is the set of residual variances for identiﬁed conditional distributions in the
pattern mixture model (10.1). For example, by setting D = D(τ), where
D(τ)
=
*
−
3
τ (≥2)
2
, 0
+
×
*
−
3
τ (3)
3 , 0
+
,
departures from MAR are assumed to lie within one standard deviation for
the conditional distributions p≥2(y2 | y1) and p3(y3 | y1, y2).
Priors for ∆can be calibrated in the same fashion. Following with this
example, we can assume p(∆| ξM) = p(∆| τ), and set
∆| τ(≥2)
2
, τ(3)
3
∼
Unif{D(τ)}.
In our analyses, we approximate this approach by plugging in posterior means
for relevant components of τ to determine D. Details are provided below.
Analysis 1: Assume common ∆between treatments
Sensitivity analyses and informative priors used in the ﬁrst analysis will be
based on the two-dimensional parameter ∆= (∆(1:2)
α0
, ∆(•:3)
β0
) ∈D1. We set the
maximum range of departures from MAR roughly equal to the highest residual
standard deviation between the treatment-speciﬁc regressions for p(y2 | y1)
and p(y3 | y1, y2) based on the observed data. Using the results in Table 10.2,
where the residual standard deviation ranges from 14 to 23 for the regression
(Y2 | Y1) and from 8.9 to 15 for (Y3 | Y1, Y2), we set D1 = [−20, 0] × [−15, 0].
To help with interpretation, it is useful to understand the eﬀect of depar-
tures from MAR on the marginal means µ2 = E(Y2) and µ3 = E(Y3). For
clarity we again suppress treatment indices. Consider ﬁrst µ2; in the pattern
mixture model,
µ2
=
E(Y2)
=
3
	
s=1
φsE(Y2 | S = s).
(10.9)

242
CASE STUDIES: NONIGNORABLE MISSINGNESS
Table 10.2 Growth hormone trial: posterior mean (posterior SD) of regression pa-
rameters for pattern mixture model (10.1) ﬁt under MAR constraints.
Treatment Group
Model
Covariate
Parameter
EP(z = 0)
EG (z = 1)
Y2 | Y1
Int.
α(≥2)
0z
23 (6.9)
14 (15)
Y1
α(≥2)
1z
.89 (.10)
.97 (.19)
τ(≥2)
2z
14
23
Y3 | Y1, Y2
Int.
β(3)
0z
11 (5.5)
–5.3 (12)
Y1
β(3)
1z
.21 (.13)
.45 (.19)
Y2
β(3)
2z
.59 (.12)
.65 (.14)
τ (3)
3z
8.9
15
In this summation, E(Y2 | S = 1) is not identiﬁed and is therefore a function
of sensitivity parameters; speciﬁcally,
E(Y2 | S = 1)
=
EY1|S=1{E(Y2 | Y1, S = 1)}
=
E(α(1)
0
+ α(1)
1 Y1 | S = 1)
=
α(1)
0
+ α(1)
1 µ(1)
=
(α(≥2)
0
+ ∆(1:2)
α0
) + (α(≥2)
1
+ ∆(1:2)
α1
)µ(1).
Recall that we have constrained ∆(1:2)
α1
= 0, so that
E(Y2 | S = 1) = α(≥2)
0
+ ∆(1:2)
α0
+ α(≥2)
1
µ(1).
Referring back to (10.9), write E(Y2) = µ2 = µ2(∆(1:2)
α0
) to emphasize its
dependence on the sensitivity parameter. The eﬀect of departures from MAR
on µ2 is captured by the diﬀerence
µ2(∆(1:2)
α0
) −µ2(0)
=
φ1∆(1:2)
α0
.
(10.10)
Hence the contribution of the sensitivity parameter to the shift in µ2 is pro-
portional to the fraction dropping out at S = 1.
As with E(Y2), the marginal mean E(Y3) is the weighted average
µ3
=
E(Y3)
=
3
	
s=1
φsE(Y3 | S = s).

GROWTH HORMONE STUDY
243
Because neither E(Y3 | S = 1) nor E(Y3 | S = 2) is identiﬁed, µ3 depends on
both sensitivity parameters; hence we write µ3 = µ3(∆(1:2)
α0
, ∆(•:3)
β0
).
Calculations similar to those above can be used to show that
µ3(∆(1:2)
α0
, ∆(•:3)
β0
) −µ3(0, 0) = (φ1 + φ2)∆(•:3)
β0
+ φ1β(3)
2 ∆(1:2)
α0
.
(10.11)
Expressions (10.10) and (10.11) can be used in conjunction with output from
Table 10.2 to understand the maximum eﬀect of departures from MAR, based
on the set D1. At the boundary value (∆(1:2
α0 , ∆(•:3)
β0
) = (−20, −15), we see that
for z = 0 (EP),
µ2(−20) −µ2(0)
=
7
40(−20)
=
−3.5,
µ3(−20, −15) −µ3(0, 0)
=
( 7
40 + 2
40)(−15) + 7
40(.59)(−20)
=
−6.6,
(10.12)
and for z = 1 (EG),
µ2(−20) −µ2(0)
=
12
38(−20)
=
−6.3,
µ3(−20, −15) −µ3(0, 0)
=
( 12
38 + 4
38)(−15) + 12
38(.65)(−20)
=
−10.4.
(10.13)
Hence departures from MAR may lead to mean quad strength that is up to
6.6 units lower for EP and 10.4 units lower for EG, relative to MAR.
When the sensitivity parameters are equivalent by treatment group, as they
are here, the dropout proportion inﬂuences which treatment group will have
greater sensitivity to departures from MAR. Because dropout rate is higher
on the EG arm, the gradient for µ31 away from MAR is steeper than for µ30;
the posterior mean of µ31 ranges from 78 to 68; and for µ30, the range is 73
to 68. Posterior treatment eﬀect over D1 ranges from 5 to 1, with associated
posteriors for P(θ > 0) taking values from .74 to .56. None of the posteriors
over the set D1 leads to the conclusion that mean quad strength on EG is
greater than on EP.
If prior belief about the values for ∆is conﬁned to the fact that it falls
within D, and if each ∆∈D1 is assumed to have equal a priori probability,
then we can assign a uniform prior over D1 and summarize treatment eﬀects
using a single posterior distribution. Results from this approach are summa-
rized in Table 10.1 under MNAR-1. As expected, posterior marginal means
have shifted downward. Uncertainty about ∆is reﬂected in the increased
posterior SD for parameters that rely on ∆(means at months 6, 12), but
importantly, not for fully identiﬁed parameters (means at month 0). Posterior
mean treatment diﬀerence is 3.1 (PSD = 8.2), which is closer to zero but
does not change the qualitative conclusion under MAR that quad strength on

244
CASE STUDIES: NONIGNORABLE MISSINGNESS
the EG arm is similar to EP. Given the sensitivity analysis above, this is not
surprising.
Analysis 2: Assume ∆is treatment speciﬁc
In the previous analysis, we assumed a common ∆across the treatment arms.
However, it may be more reasonable to assume that the degree of departure
from MAR diﬀers on the two arms. Recall that our previous analysis was
based on the sensitivity parameters
∆(1:2)
α0
=
E(Y2 | Y1, S = 1) −E(Y2 | S ≥2)
∆(•:3)
β0
=
E(Y3 | Y1, Y2, S = 2) −E(Y3 | Y2, Y1, S = 3)
=
E(Y3 | Y1, Y2, S = 1) −E(Y3 | Y2, Y1, S = 3).
In this analysis, we allow these parameters to vary by treatment, but to keep
the dimension of ∆equal to 2, we combine them by assuming
∆(1:2)
α0(z) = ∆(•:3)
β0(z) = ∆z,
for z = 0, 1.
It is again helpful to quantify the eﬀect of ∆z on the shift in marginal
means; arguing as before, it is straightforward to show that the maximum
eﬀects of departures from MAR on the marginal means µ2z = E(Y2 | Z = z)
and µ3z = E(Y3 | Z = z) are
µ2z(∆z) −µ2z(0)
=
∆zφ1z
µ3z(∆z) −µ3z(0)
=
∆z{φ2z + φ1z(1 + β(3)
2 )}.
We set D2 = [0, 20] × [0, 20], based on the largest residual standard deviation
over both treatments in Table 10.2.
Using calculations similar to (10.12) and (10.13), the following maximum
shifts in µ2z and µ3z relative to MAR are
µ20(−20) −µ20(0)
=
−3.5
µ30(−20) −µ30(0)
=
−6.3
!
z = 0 (EP)
µ21(−20) −µ21(0)
=
−6.6
µ31(−20) −µ31(0)
=
−12.5
!
z = 1 (EG).
Figure 10.1 shows contours of the posterior mean of θ and of P(θ > 0),
conditional on ﬁxed values of (∆0, ∆1) ∈D2. The contour plot of P(θ > 0)
indicates the speciﬁc departures from MAR that would change the qualitative
conclusions about treatment eﬀect. In particular, the part of the posterior
distribution having P(θ > 0) > .9 is in the region where ∆1 is near zero and
∆0 is between −20 and −15; i.e., where dropouts on EG are roughly MAR,

GROWTH HORMONE STUDY
245
but dropouts on EP have quad strength much lower than expected relative
to MAR.
As with Analysis 1, we can use an informative prior on ∆. For illustration
we use a uniform prior on D2 = [−20, 0]×[−20, 0]; the posterior is summarized
in Table 10.1. The results largely agree with Analysis 1, but posterior variances
are higher because of the wider range for ∆.
Posterior predictive model checking
Although the distributions p(ymis | yobs, s) and p(yobs, s) are fully separated,
we have made a number of parametric and distributional assumptions about
p(yobs, s); primarily, we have assumed multivariate normality and linearity
of associations. Unlike assumptions about p(ymis | yobs, s), those applied to
p(yobs, s) can be critiqued. We use Pearson’s χ2 statistic (3.19) to assess ﬁt of
the identiﬁed distributions distributions in (10.1) to the observed data using
the posterior predictive approach described in Chapter 8.
Speciﬁcally, to assess the ﬁt of p(y1), we compute
T1(y1; ω) = 1
n
n
	
i=1
Qi1(ω),
where
Qi1(ω) = {yi1 −E(Yi1 | ω)}2
var(Yi1 | ω)
with
E(Yi1 | ω)
=
3
	
s=1
φs µ(s),
var(Yi1 | ω)
=
E{var(Y1 | S, ω)} + var{E(Y1 | S, ω)}
=
3
	
s=1
φs [σ(s) + {µ(s) −E(Yi1 | ω)}2].
To assess the ﬁt of p(y2 | y1, S ≥2) and p(y3 | y1, y2, S = 3), we use T2(y; ω)
and T3(y; ω), respectively, where, for j = 2, 3,
Tj(y; ω) =
1
n≥j
n≥j
	
i=1
Qij(ω).
As above,
Qij(ω)
=
{yij −E(Yij | yi1, . . . , yi,j−1, S ≥j, ω)}2
Vj(ω)
,

246
CASE STUDIES: NONIGNORABLE MISSINGNESS
where
E(Yij | yi1, . . . , yi,j−1, S ≥j, ω)
=
3
	
s=j
φs E(Yij | yi1, . . . , yi,j−1, S = s)
=
3
	
s=j
φs
/
β(j)
0
+
j−1
	
l=1
β(j)
l
Yil
0
and
Vj(ω) = var(Yj | y1, . . . , yj−1, S ≥j, ω) = τ (≥j)
j
.
We normalize the data summary Tj(y; ω) using n≥j, the total number of
subjects in patterns greater than or equal to j. The reason for doing this is
that the total number in these patterns in the replicated dataset will vary with
each sample from the posterior predictive distribution and will diﬀer from the
observed number of subjects in those patterns in the original dataset.
Posterior predictive probabilities based on Tj(·; ω) were computed as de-
scribed in Chapter 8. The probabilities for these three checks were .71, .56,
and .56, indicating no evidence of wide departures from the observed data
model.
10.2.9 Summary of pattern mixture analysis
Our strategy here has been to construct the full-data distribution as a PMM.
The general model allows missingness to be MNAR, with MAR as a special
case. The degree of departure from MAR is quantiﬁed by a set of parame-
ters ∆measuring departures from MAR. The ∆parameters are completely
nonidentiﬁed, so the ﬁt of the model to observed data is identical across the
assumed missing data mechanisms. We assess the ﬁt of the observed data
model using posterior predictive checks.
Several key issues must be addressed when implementing a PMM analysis.
First, the general model has a large number of sensitivity parameters. In
practical settings, the dimension must be reduced; we have given suggestions
for doing this.
Second, analyses based on examining conditional posteriors at ﬁxed values
of ∆over a predetermined range D can be useful, but speciﬁcation of D is
subjective. Our approach has been to calibrate D using relevant posterior dis-
tributions under the MAR constraint. Other approaches are certainly possible
and will depend on context. It is important to ensure that choices for D do
not extrapolate the missing data outside of a ‘reasonable’ range (e.g., negative
values of quadriceps strength).
Third, it must be kept in mind that the contour plots generated for Anal-
yses 1 and 2 represent conditional posteriors (or posteriors over an inﬁnite
number of point mass priors), and that ideally, a ﬁnal inference should be

GROWTH HORMONE STUDY
247
E(θ|∆EG, ∆EP)
∆EP
∆EG
−20
−15
−10
−5
0
−20
−15
−10
−5
0
MAR
P(θ > 0|∆EG, ∆EP)
∆EP
∆EG
−20
−15
−10
−5
0
−20
−15
−10
−5
0
MAR
Figure 10.1 Growth hormone trial: contours of posterior mean treatment eﬀect θ
(diﬀerence in quad strength) and of posterior probability P(θ > 0) as function of ∆
from Analysis 2.

248
CASE STUDIES: NONIGNORABLE MISSINGNESS
based on a well-motivated prior distribution for the nonidentiﬁed sensitivity
parameters. The priors for ∆| ξM used in Analyses 1 and 2 are informed solely
by the choice of D, and all possible values in D are weighted equally in our
example here. In practice, however, it is likely that more information is known
about departures from MAR, whether from previous studies or from expert
opinion. In the case study presentation given in Section 10.3, we illustrate the
use of informative priors based on elicited expert opinion.
10.3 OASIS Study: Selection models, mixture models, and elicited
priors
10.3.1 Overview
The OASIS trial is described in detail in Section 1.6. OASIS was a two-arm
randomized trial designed to reduce smoking rates in alcoholics. Smoking sta-
tus was assessed at 1, 3, 6, and 12 months following randomization. For each in-
dividual, the full data comprise the 4×1 response vector of smoking outcomes
Y = (Y1, Y2, Y3, Y4)T, obtained at months 1, 3, 6, and 12 following baseline;
the corresponding vector of response indicators R = (R1, R2, R3, R4)T; and
treatment group Z (1 if randomized to enhanced intervention, 0 if standard
intervention).
The objective is to compare smoking rates at month 12 between those
randomized to Z = 1 vs. Z = 0. Inference is made in terms of the odds ratio
ϕ
=
odds(Y4 = 1 | Z = 1)
odds(Y4 = 1 | Z = 0).
This trial has intermittent missingness; follow-up time S = S(R) is deﬁned
as the last time point at which data are observed (all individuals are observed
at the ﬁrst time point):
S
=









1
if R = (1, 0, 0, 0)
2
if R = (1, 1, 0, 0)
3
if R ∈{(1, 1, 1, 0), (1, 0, 1, 0)}
4
if R ∈{(1, 1, 1, 1), (1, 0, 1, 1), (1, 1, 0, 1), (1, 0, 0, 1)}.
To handle intermittent missing values, we assume missingness is MAR condi-
tionally on S; that is, we assume
p(ymis | yobs, s, z, r) = f(ymis | yobs, s, z),
or that Y mis is independent of R given (Y obs, S, Z).
Figure 10.2 displays the smoking rate by pattern from observed data. For
both treatment arms, those who complete the study (S = 4) show lower smok-
ing rates than dropouts. Furthermore, for S ∈{2, 3}, smoking rate appears to
increase preceding dropout in ET arm. Later, for the pattern mixture analy-

OASIS STUDY
249
sis, we combine the relatively few subjects with S = 2 or S = 3 into a single
pattern that, using a slight abuse of notation, is labeled as S = (2, 3).
We conduct several analyses using (parametric) selection models and using
mixture models. For the latter, we describe the construction of an informative
prior that will be used for inference under MNAR.
Measurement time j
 
1
2
3
4
0.6
0.8
1.0
S=1 (54)
S=2 (12)
S=3 (16)
S=4 (67)
All
Enhanced Intervention
Measurement time j
 
1
2
3
4
0.6
0.8
1.0
S=1 (41)
S=2 (9)
S=3 (10)
S=4 (89)
All
Standard Intervention
Figure 10.2 OASIS study: proportion smoking by dropout pattern S at each month,
stratiﬁed by treatment arm (number of subjects per pattern in parentheses).
10.3.2 Selection model speciﬁcation
A selection model factorization of the full-data model follows p(y | ω) p(r |
y, ω). We use a parametric selection model to infer treatment eﬀect under
both MAR and MNAR. The selection model being used here is parametric
because, we impose constraints on the full-data distribution, and on the miss-
ing data mechanism (similar to the constraints used in Example 8.2). We focus

250
CASE STUDIES: NONIGNORABLE MISSINGNESS
attention here on the eﬀect of priors on the parameters governing departures
from MAR, and the degree to which they may be informed by observed data.
Full-data model
For the ﬁrst factor, we assume the full-data response follows a ﬁrst-order
Markov model, separately by treatment,
Y1
∼
Ber(θ1)
Yj | Yj−1
∼
Ber(θj|j−1),
with
logit(θj|j−1) = β0j + β1yj−1.
The missing data mechanism is speciﬁed in terms of the hazard of dropout,
hS(tj | y)
=
P(S = j | S ≥j, y),
j = 1, 2, 3, 4
(recall that all participants have at least one measurement, so S ≥1). In
general, hS(tj | Y ) can depend on any function of the full-data vector Y .
Here we allow dependence on Yj−1 and Yj via
logit{hS(tj | y)} = ψ0 + ψ1yj−1 + ψ2yj + ψ3yjyj−1.
(10.14)
Conditional on Yj−1, the log odds of dropout at tj is ψ0+ψ2yj when Yj−1 = 0,
and is (ψ0+ψ1)+(ψ2+ψ3)yj when Yj−1 = 1. Dropout is MAR (and ignorable)
when ψ2 = ψ3 = 0.
Priors
Our analysis uses various priors on (ψ2, ψ3). First, we use the point mass MAR
prior
p(ψ2, ψ3)
=
I{ψ2 = ψ3 = 0}.
Next, we use a series of informative priors that permit departures from MAR.
Rather than use point mass priors, we employ normal priors centered away
from zero and having unit variance. Speciﬁcally,
(ψ2, ψ3)T | (d2, d3)T
∼
N
/ /
d2
d3
0
,
/
1
0
0
1
0 0
.
(10.15)
The mean parameters are varied over the set
(d2, d3)
∈
{−0.5, 0, 0.5, 1.0}⊗2.
Priors for (β, ψ0, ψ) are speciﬁed as independent diﬀuse normal priors with
mean zero and variance 100.

OASIS STUDY
251
Table 10.3 OASIS trial: posterior means (SD) for hazard function parameters,
treatment-speciﬁc smoking rates, and treatment eﬀects from selection model analysis
using diﬀerent priors for (ψ2, ψ3). Summaries provided here for point mass MAR
prior and for normal priors having mean (d2, d3) and variance I; see (10.15).
Prior means (d2, d3)
Trt
Parameter
MAR
(–1,–1)
(0,0)
(1,1)
ET
ψ0
–2.4 (.50)
–2.5 (.64)
–2.8 (.69)
–3.2 (.80)
ψ1
1.6 (.50)
1.8 (1.0)
.88 (.99)
–.02 (1.0)
ψ2
—
–.15 (.96)
.50 (.86)
1.2 (.90)
ψ3
—
–.16 (.99)
.56 (.91)
1.2 (.91)
ST
ψ0
–2.0 (.41)
–1.7 (.44)
–1.8 (.54)
–2.6 (.76)
ψ1
.47 (.43)
2.6 (.60)
1.7 (1.2)
–.71 (1.3)
ψ2
—
–2.0 (.86)
–1.1 (1.2)
.86 (1.1)
ψ3
—
–1.9 (.87)
–.95 (1.1)
.87 (1.1)
E(Y4 | ET)
.78 (.05)
.76 (.07)
.83 (.04)
.82 (.04)
E(Y4 | ST)
.88 (.03)
.77 (.06)
.82 (.07)
.89 (.04)
Trt eﬀect ϕ
.51
1.09
.97
.61
95% interval
(.19, 1.06)
(.32, 2.53)
(.26, 2.38)
(.22, 1.38)
10.3.3 Selection model analyses under MAR and MNAR
Results are shown in Table 10.3. Under MAR (ψ2 = ψ3 = 0), the treatment
odds ratio is .5 with a credible interval of (.2, 1.1), indicating a marginally
positive eﬀect of the ET treatment in lowering the rate of smoking.
For the ET arm, subjects smoking the previous week were more likely to
drop out at the current week than those not smoking, with an odds ratio of
about ﬁve. A similar relation was seen in the ST arm, but with a smaller odds
ratio (1.6) and a credible interval covering one.
Under MNAR, the posteriors for ψ2 and ψ3 were relatively diﬀuse and
informative priors were needed to stabilize the posterior. In fact, the priors
and posteriors for (ψ2, ψ3) are very diﬀerent for all three informative priors,
indicating that observed data are contributing information.
We therefore do not recommend the MNAR analyses with informative pri-
ors here because the prior and the posterior for (ψ2, ψ3) are very diﬀerent and
therefore do not satisfy the condition (9.12) as recommended in Chapter 9.

252
CASE STUDIES: NONIGNORABLE MISSINGNESS
Semiparametric selection models oﬀer a possible alternative; this is an area of
ongoing work.
10.3.4 Pattern mixture model speciﬁcation
For the mixture model speciﬁcation, the full-data response distribution is a
mixture over the follow-up times, factored as
p(ymis, yobs, s | z) = p(ymis | yobs, s, z) p(yobs | s, z) p(s | z).
We group the patterns S = 2 and S = 3 into a single pattern and label it
S = (2, 3). Hence for purposes of the pattern mixture model, the realizations
of S are s ∈{1, (2, 3), 4}. The component distributions follow
Y1 | S = s, Z = z
∼
Ber(µ(s)
1z ),
Yj | Yj−1, S = s, Z = z
∼
Ber(φ(s)
jz ),
j = 2, 3, 4,
(10.16)
where
logit(φ(s)
jz ) = γ(s)
jz + θ(k)
jz Yj−1.
(10.17)
For the dropout distribution, we assume
S | Z = z
∼
Mult(ψ1z, ψ(2,3)z, ψ4z).
This model assumes that within dropout pattern, the joint distribution of
Y can be captured using a ﬁrst-order serial dependence structure. Table 10.4
shows the structure of the model in the logit scale; empty cells represent
quantities that cannot be identiﬁed by observed data.
10.3.5 MAR and MNAR parameterizations
Identifying the model under MAR
As with the Growth Hormone example, model identiﬁcation under MAR is
based on condition (8.17) in Theorem 8.1. To satisfy MAR under the ﬁrst-
order dependence structure given by (10.17), we require
ps(yj | yj−1, z) = p≥j(yj | yj−1, z)
for each s < j and for j = 2, 3, 4. ∗Again, this implies that the distribution
(Yj | Yj−1, Z) is equivalent for those dropping out prior to j (i.e., s < j) and
those still in follow-up at j (i.e., s ≥j).
For those still in follow-up at j, deﬁne
φ(≥j)
jz
=
E(Yj | Yj−1, Z = z, S ≥j),
∗With the collapsed patterns, if s = (2, 3), then s < j only when j = 4.

OASIS STUDY
253
Table 10.4 OASIS trial: representation, on the logit scale, of full-data component
distributions model given by (10.17). Cell entries are logits of φ(s)
j (y) = E(Yj |
Yj−1 = y, S = s). Empty cells cannot be identiﬁed by observed data. Treatment
subscripts suppressed for clarity.
y
φ(s)
2 (y)
φ(s)
3 (y)
φ(s)
4 (y)
S = 1
0
1
S = (2, 3)
0
γ(2,3)
2
γ(2,3)
3
1
γ(2,3)
2
+ θ(2,3)
2
γ(2,3)
3
+ θ(2,3)
3
S = 4
0
γ(4)
2
γ(4)
3
γ(4)
4
1
γ(4)
2
+ θ(4)
2
γ(4)
3
+ θ(4)
3
γ(4)
4
+ θ(4)
4
and let
logit(φ(≥j)
jz
) = γ(≥j)
jz
+ θ(≥j)
jz
Yj−1.
(10.18)
It can be shown that, in this setting, (10.18) is compatible with the full-data
model (10.17); in fact, the regression parameters in (10.18) are determined by
those in (10.17).
From Theorem 8.1, the MAR constraint is satisﬁed when, for each j > s,
γ(s)
jz
=
γ(≥j)
jz
θ(s)
jz
=
θ(≥j)
jz
.
The MAR identiﬁcation scheme is shown in Table 10.5.
Parameterizing the full-data model under MNAR
The full-data model can be reparameterized in terms of sensitivity parameters.
Let ξ(ω) = (ξS, ξM), where
ξS
=

θ(1)
j , γ(1)
j
j = 2, 3, 4
θ(2,3)
4
, γ(2,3)
4
ξM
=









θ(2,3)
j
, γ(2,3)
j
j = 2, 3
θ(4)
j , γ(4)
j
j = 2, 3, 4
µ(1)
1 , µ(2,3)
1
, µ(4)
1
ψ(1), ψ(2,3), ψ(4)

254
CASE STUDIES: NONIGNORABLE MISSINGNESS
Table 10.5 OASIS trial: identiﬁcation of full-data model (10.17) under MAR, on the
logit scale. Cell entries are (logits of) φ(s)
j (y) = E(Yj | Yj−1 = y, S = s). Treatment
subscripts suppressed for clarity.
y
φ(s)
2 (y)
φ(s)
3 (y)
φ(s)
4 (y)
S = 1
0
γ(≥2)
2
γ(≥3)
3
γ(4)
4
1
γ(≥2)
2
+ θ(≥2)
2
γ(≥3)
3
+ θ(≥3)
3
γ(4)
4
+ θ(4)
4
S = (2, 3)
0
γ(2,3)
2
γ(2,3)
3
γ(4)
4
1
γ(2,3)
2
+ θ(2,3)
2
γ(2,3)
3
+ θ(2,3)
3
γ(4)
4
+ θ(4)
4
S = 4
0
γ(4)
2
γ(4)
3
γ(4)
4
1
γ(4)
2
+ θ(4)
2
γ(4)
3
+ θ(4)
3
γ(4)
4
+ θ(4)
4
(see Table 10.4), where we have suppressed the dependence of these param-
eters on treatment. We also can characterize departures from MAR via the
mapping ξS = h(ξM, ∆), deﬁned as follows.
Among those in patterns s = 1 and s = (2, 3), deﬁne, for each j > s and
for z = 0, 1, a pair of parameters (∆(s)
0jz, ∆(s)
1jz) that satisfy
γ(s)
jz
=
h1(ξM) = γ(≥j)
jz
+ ∆(s)
0jz
θ(s)
jz
=
h2(ξM) = θ(≥j)
jz
+ (∆(s)
1jz −∆(s)
0jz).
(10.19)
Each ∆parameter represents a log odds ratio comparing odds of smoking at
time j, conditional on Yj−1, between those with s < j (dropped out) and
those with s ≥j (still in follow-up). Speciﬁcally,
∆(s)
yjz = log
odds(Yj = 1 | Yj−1 = y, Z = z, S = s)
odds(Yj = 1 | Yj−1 = y, Z = z, S ≥j)

.
Setting each ∆(s)
yjz = 0 implies MAR.
The dimension of ∆is 16. To reduce the parameter space, we make the sim-
plifying assumption that ∆(s)
yjz = ∆y, leaving the two-dimensional parameter
∆= (∆0, ∆1). The simpliﬁcation implies that departures from MAR at time
j may diﬀer depending on Yj−1, but conditionally on Yj−1 are constant across
measurement time, treatment group, and dropout time. The MNAR model
is shown in Table 10.6. Naturally, other simpliﬁcations can be considered,
depending on the application.
Prior distributions for parameters can still be speciﬁed as
p(ξS, ξM, ∆)
=
p(ξS | ξM, ∆) p(∆| ξM) p(ξM),

OASIS STUDY
255
Table 10.6 OASIS trial: full-data model (10.17) under MNAR, with parameters
(∆0, ∆1) capturing departures from MAR on the logit scale. Cell entries are logits
of φ(s)
j (y) = E(Yj | Yj−1 = y, S = s). Treatment subscripts suppressed for clarity.
y
φ(s)
2 (y)
φ(s)
3 (y)
φ(s)
4 (y)
S = 1
0
γ(≥2)
2
+ ∆0
γ(≥3)
3
+ ∆0
γ(4)
4
+ ∆0
1
γ(≥2)
2
+ θ(≥2)
2
+ ∆1
γ(≥3)
3
+ θ(≥3)
3
+ ∆1
γ(4)
4
+ θ(4)
4
+ ∆1
S = (2, 3)
0
γ(2,3)
2
γ(2,3)
3
γ(4)
4
+ ∆0
1
γ(2,3)
2
+ θ(2,3)
2
γ(2,3)
3
+ θ(2,3)
3
γ(4)
4
+ θ(4)
4
+ ∆1
S = 4
0
γ(4)
2
γ(4)
3
γ(4)
4
1
γ(4)
2
+ θ(4)
2
γ(4)
3
+ θ(4)
3
γ(4)
4
+ θ(4)
4
where, as usual,
p(ξS | ξM, ∆)
=
I{ξS = h(ξM, ∆)}.
10.3.6 Pattern mixture analysis under MAR
As a starting point, we present analysis under MAR assumption. First we
assign point mass at zero to (∆0, ∆1) separately by both treatment arms via
p(∆0, ∆1 | ξM)
=
I{(∆0, ∆1) = (0, 0)}.
This prior assumes MAR with 100% certainty. Priors for components of ξM
are diﬀuse but proper. Results are shown in Table 10.7, compared side by
side with summaries using available data separately at each time point. The
table suggests that the MAR analysis varies very little from simply computing
smoking rates from available data.
10.3.7 Pattern mixture analysis under MNAR using elicited priors
Under MAR, the probability of smoking at time j, conditional on observed
smoking history up to but not including j, is equivalent between dropouts
and those who continue the study. Although research on smoking cessation
has suggested that dropouts are more likely to be smokers after leaving a study
(Lichtenstein and Glasgow, 1992), the data oﬀer no evidence about this.
The mixture model factorization allows assumptions about the missing data
mechanism to be completely encoded in the prior p(∆| ξM). In this section,

256
CASE STUDIES: NONIGNORABLE MISSINGNESS
Table 10.7 OASIS trial: inference about time-speciﬁc smoking rates using available
data at each time point and using a pattern mixture model with an MAR point mass
prior.
Available
MAR
Treatment
Month
Data
Prior
1
.83
.83
ET
3
.84
.87
6
.79
.82
12
.76
.78
1
.85
.85
ST
3
.85
.85
6
.84
.84
12
.88
.88
Trt eﬀect ϕ
.43
.51
95% interval
(.19, 1.06)
we summarize the use of priors elicited from a panel of experts. Complete
details appear in Lee et al. (2007) and Lee (2006); the process and analyses
are brieﬂy summarized here.
Eliciting information and constructing prior distributions
Lee et al. (2007) convened a panel of four faculty investigators who are ex-
perts in smoking cessation in order to elicit information about ∆= (∆0, ∆1).
Elicitation was conducted using a written questionnaire that provided detailed
information about the study and its target population. The sensitivity param-
eters ∆0 and ∆1 are measured on the odds ratio scale, but prior information
was elicited on the probability scale for the sake of transparency.
The main part of the questionnaire asked each expert to provide a best
guess and 90% interval for the probability of smoking at tj among those who
dropped out at tj−1. To help calibrate the response, experts were asked to
provide their answer conditionally on hypothetical rates for those who did
not drop out. Formally, each expert was provided with a range of values for
E(Yj | Yj−1 = y, S ≥j),
y = 0, 1
and asked to provide a best guess and 90% interval for
E(Yj | Yj−1 = y, S = j −1),
y = 0, 1.

OASIS STUDY
257
Next, each expert’s best guess and lower and upper bounds of their 90%
interval for smoking rates among dropouts at each time j were converted to
the log odds scale, based on the observed smoking rates at each time among
non-dropouts; i.e., based on observed values of E(Yj | Yj−1 = y, S ≥j). In
that sense we are eliciting priors for ∆| ξM because E(Yj | Yj−1, S ≥j) is a
function of ξM.
The log odds ratios corresponding to the best guess and interval boundaries
were then averaged over time, weighting by sample size, to obtain a summary
guess and summary interval for each expert. This yields, for each expert, a
90% interval and a modal value for both ∆0 and ∆1.
Not surprisingly, the expert-speciﬁc intervals for the ∆’s were asymmet-
ric. We therefore assumed a skew-normal prior for each expert (Azzalini and
Valle, 1996). The skew-normal distribution p( · | µ, η, ν) has three parameters:
location µ, scale η, and shape (or skewness) ν. Two percentiles and a mode
are suﬃcient to uniquely determine the parameters. Speciﬁcally, using sum-
maries of best guess (mode), 5th percentile, and 95th percentile as inputs,
the following system of equations was solved for (µ, η, ν), separately for each
expert and each of ∆0, ∆1:
arg max
∆p(∆| µ, η, ν)
=
mode,
 L
−∞
p(∆| µ, η, ν) d∆
=
0.05,
 U
−∞
p(∆| µ, η, ν) d∆
=
0.95.
Here, L and U are, respectively, the lower and upper bounds of the 90%
interval for ∆, elicited from the experts.
Finally, a four-component mixture (one for each expert) of skew normal
distributions was used for each ∆parameter, with each expert’s prior con-
tributing equally to the mixture.
Summary of analysis
Figure 10.3 shows the eﬀect of the informative priors relative to MAR for the
ET arm by examining the conditional smoking probabilities E(Yj | Yj−1 =
0, S = 1), for j = 2, 3, 4. In each case, the expert-speciﬁc prior shifts the
smoking probabilities toward one. This ﬁgure shows that even for participants
who were not smoking at tj−1, those who drop out after time 1 (S = 1) are
believed to have substantially higher smoking probability compared to those
who continue (S ≥2). Under MAR, the probabilities would be equal between
dropouts and non-dropouts.
Figure 10.4 shows the distribution of E(Y4) for each treatment arm; it
separates out posteriors derived by using individual expert-speciﬁc priors, the

258
CASE STUDIES: NONIGNORABLE MISSINGNESS
mixture (of experts) prior, and the MAR prior. Again, relative to MAR, we
see E(Y4) shifted toward one.
The full summary of time-speciﬁc smoking rates and the treatment odds
ratio is given in Table 10.8, and the posterior distribution of the treatment
odds ratio is given in Figure 10.5. Using the informative prior, the odds ratio
is attenuated toward 1 relative to MAR, and shows more variability (posterior
mean .5 with 95% interval (.2, 1.1) under MAR; posterior mean .7 with 95%
interval (.2, 1.4) using informative priors). The additional variability can also
be seen in Figure 10.5 as the right tail under MNAR is heavier than under
0
0.5
1
0
5
Expert1
E(Y2  Y1=0)
0
0.5
1
0
5
E(Y3  Y2=0)
0
0.5
1
0
5
E(Y4  Y3=0) 
0
0.5
1
0
5
Expert2
0
0.5
1
0
5
0
0.5
1
0
5
0
0.5
1
0
5
Expert3
0
0.5
1
0
5
0
0.5
1
0
5
0
0.5
1
0
5
Expert4
0
0.5
1
0
5
0
0.5
1
0
5
0
0.5
1
0
5
Mixture
0
0.5
1
0
5
0
0.5
1
0
5
Figure 10.3 OASIS trial: for the ET arm, comparison of conditional means E(Yj |
Yj−1 = 0, S = 1) for j = 2, 3, 4 under MNAR using expert-speciﬁc and mixture-of-
experts priors (solid lines), and under MAR (dashed line).

OASIS STUDY
259
MAR. Whereas the MAR analysis suggests that ET is moderately superior,
the MNAR analysis incorporating expert opinion — along with uncertainty
stemming from within-expert and between-expert variation — is more equiv-
ocal. Because dropout rate is greater on the ET arm, the relative adjustment
in smoking rates under MNAR also is greater.
10.3.8 Summary: selection vs. pattern mixture approaches
We ﬁt both parametric selection models and mixture models under MAR and
by using informative priors. This analysis has brought out several key issues.
First, the identiﬁability of all parameters in parametric selection models makes
them ill-suited for sensitivity analysis and constructing informative priors.
Semiparametric selection models would be a possible alternative. On the other
0.6
0.7
0.8
0.9
1.0
0
5
10
15
 
 
Enhancement arm
MAR
Mixture
Each expert
0.6
0.7
0.8
0.9
1.0
0
5
10
15
Standard arm
 
 
Figure 10.4 OASIS trial: posterior distribution of E(Y4), smoking rate at month
12, by treatment. Posteriors shown correspond to the following priors for ∆: point
mass at zero (MAR), expert-speciﬁc priors, and the mixture of expert-speciﬁc priors
(MNAR).

260
CASE STUDIES: NONIGNORABLE MISSINGNESS
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
2.5
 
 
MAR
Mixture
Each expert
Figure 10.5 OASIS trial: posterior distribution of the treatment odds ratio ϕ under
priors for ∆corresponding to MAR, expert-speciﬁc opinion, and a mixture of expert
opinion. The MAR prior places point mass at (∆0, ∆1) = (0, 0).
hand, mixture models are well suited because the parameter space is easier
to separate. The various pattern mixture models ﬁt here all have the same
observed-data likelihood.
Second, eliciting informative priors can be complex. A key component of
the elicitation is to develop tools for which expert opinion can be accurately
acquired. Here, experts were asked to condition their opinion about smoking
rate at time j among dropouts using hypothetical smoking rates for non-
dropouts, conditional on smoking status Yj at time j −1. This helped the
experts calibrate their priors.
Finally, we saw that the (ignorable) selection model MAR analysis and
the MAR mixture model analysis were identical. This is not a general phe-

PEDIATRIC AIDS TRIAL
261
Table 10.8 OASIS trial: posterior inferences about time-speciﬁc smoking probabil-
ity, derived using informative priors elicited from experts. The MAR inferences are
shown for comparison.
Expert-Speciﬁc Posteriors
Treatment
Month
MAR
MNAR
1
2
3
4
1
.83
.83
.83
.83
.83
.83
ET
3
.87
.88
.87
.89
.89
.88
6
.82
.85
.83
.87
.87
.85
12
.78
.84
.83
.87
.87
.83
1
.85
.85
.85
.85
.85
.85
ST
3
.85
.87
.86
.87
.87
.86
6
.84
.86
.86
.87
.86
.86
12
.88
.90
.90
.91
.90
.90
Trt eﬀect ϕ
.51
.65
.65
.62
.71
.68
(.2, 1.1)
(.2, 1.4)
nomenon. The reason for their equivalence here is that with longitudinal bi-
nary data, a mixture of ﬁrst-order serial dependence models can also be repre-
sented as a single ﬁrst-order serial dependence model. Hence inferences about
the full data will be the same under MAR.
10.4 Pediatric AIDS trial: Mixture of varying coeﬃcient models
for continuous dropout
10.4.1 Overview
In this example we use a mixture of varying coeﬃcient models to analyze
data from the pediatric clinical trial described in Section 1.7. Individuals
were scheduled for follow-up every 3 months, but the actual measurement
times varied considerably from individual to individual. Dropout time was
ﬁxed at the time of the last observed measurement. The observed CD4 data,
in the log scale, are shown in Figure 1.5, with several individual trajectories
highlighted. From the small random sample of individuals, we see that those
with longer follow-up tend to have higher overall CD4 counts and less steep
declines over time.
Our objective is to characterize the average CD4 count over time in both
treatment arms, and to draw inference about the diﬀerence in mean CD4 at

262
CASE STUDIES: NONIGNORABLE MISSINGNESS
the end of the study period. We assume that the average (log) CD4 is linear
in time with slope and intercept that depend on dropout time. The condi-
tional distributions (given dropout time) are averaged across the distribution
of dropout times separately by treatment, using a mixture of varying coeﬃ-
cient models (Section 8.4.4). The intercept and slope of CD4 will be given by
smooth but unknown functions β0(u) and β1(u) of dropout time.
Figure 10.6 is an exploratory plot that can be used to motivate the mix-
ture model. It shows individual-speciﬁc OLS slope estimates plotted against
dropout time, and suggests a possibly nonlinear association between slope and
dropout time. However, it should be kept in mind that the slope estimates
for earlier dropouts are based on fewer observations and tend to have higher
variance.
Inference from the mixture of VCM will be compared to inference under a
standard regression model that assumes MAR. Hogan et al. (2004a) analyze
these data using a similar model; the Bayesian formulation given here derives
mainly from Su and Hogan (2007), who provide a more extensive treatment,
including detailed sensitivity analysis.
0
50
100
150
200
−15
−10
−5
0
5
0
50
100
150
200
−15
−10
−5
0
5
Least Squares Slopes
Least Squares Slopes
follow-up time (weeks)
follow-up time (weeks)
Figure 10.6 Pediatric AIDS study: OLS slopes vs. follow-up time for each treatment,
with unweighted lowess smoother ﬁt to each set of points.

PEDIATRIC AIDS TRIAL
263
10.4.2 Model speciﬁcation: CD4 counts
Centering and rescaling the time axis leads to a more interpretable treatment
eﬀect and more stable implementation of posterior sampling. Let t0 and U 0,
respectively, denote measurement time and dropout time in the original scale.
We use the shifted and rescaled time variables t = (t0 −C)/L and U =
(U 0 −C)/L, where C = U
0 = 139 and L = maxi U 0
i . The length of the
rescaled time axis is equal to one and it is centered at the (sample) average
follow-up time.
The model used here is described more generally in Section 8.4.4; the spe-
ciﬁc formulation is similar to Example 8.8. Treatment assignment is denoted
by Z (1 = high dose AZT, 0 = low dose AZT). Let t denote (rescaled) time
elapsed from beginning of follow-up, let Y (t) = log{CD4(t) + 1}, and let U
denote (rescaled) length of follow-up (i.e., dropout time). Our objective is to
characterize the distribution of Y (t) over the time interval.
Figure 1.5 shows, in the original time axis, a scatterplot of log CD4 counts
vs. time, with individual proﬁles highlighted. Even in a small sample of indi-
viduals, we see that longer follow-up time tends to be associated with greater
intercept and slope.
As with Example 8.8, we assume the process Yi(t) is potentially observed
at a set of points Ti = {ti1, . . . , ti,ni}, giving rise to the full-data vector
Y i = (Yi(ti1), . . . , Yi(ti,ni))T = (Yi1, . . . , Yi,ni)T.
Conditional on dropout, we assume the log CD4 trajectory is linear in time.
Let xij = (1, tij), so that Xi = (xT
i1, . . . , xT
i,ni)T is the ni × 2 design matrix.
The full-data distribution p(y, u | z) is factored as p(y | u, z) p(u | z); the
ﬁrst factor follows
Y i | Ui = u, Zi = z
∼
N(µiz(u), Σiz).
(10.20)
The conditional mean µiz(u) is an ni × 1 vector having elements
µijz(u) = E(Yij | u, z) = β0z(u) + β1z(u)tij,
(10.21)
where β0z(u) and β1z(u) are smooth but unspeciﬁed functions of u. When
using the rescaled time axis, the intercept term β0z(u) is the full-data mean
of log CD4 count at time t = U
0 = 139 among those who drop out at time
u; the slope β1z(u) is the full-data mean change in log CD4 from baseline to
end of the study among those dropping out at u. When both β0(u) and β1(u)
are constant in u, we have MAR.
The variance Σiz is parameterized using a random eﬀects structure
Σiz = Σiz(Ω, σ) = xiΩxT
i + σ2Ini,
where Ωis a 2×2 positive deﬁnite matrix and Ini is an identity matrix having
dimension ni.

264
CASE STUDIES: NONIGNORABLE MISSINGNESS
The smooth functions β0z(u) and β1z(u) are represented in terms of pe-
nalized splines. We use a low-rank thin-plate spline basis (Crainiceanu et al.,
2005). Suppressing individual-level subscripts for now, we have
β(u; α, ψ) = α0 + α1u +
k
	
l=1
ψl|u −sl|3,
(10.22)
where sl are ﬁxed knot points, α = (α0, α1)T, and ψ = (ψ1, . . . , ψk)T. In
the full-data loglikelihood, we introduce a penalty term that shrinks the co-
eﬃcients of (10.22) to induce smoothness; it takes the form λφTDφ, where
λ > 0 is a smoothing parameter, φT = (αT, ψT), and D is a symmetric
matrix with dimension 2 + k. Following Crainiceanu et al. (2005), we only
penalize higher-order coeﬃcients ψ, so that
D =
-
02×2
02×k
0k×2
Qk×k
.
,
where the (l, l′) entry of Q is |sl −sl′|3. Now write v(u) = (1, u) and w(u) =
(|u −s1|3, . . . , |u −sk|3), and let ψ = Q
1
2 ψ and w(u) = w(u)Q−1
2 .
The penalized splines have a linear model representation as
β(u, φ) = v(u)α + w(u)ψ,
where the ψ ∼N(0, τ2Ik) are viewed as random eﬀects with variance τ2 =
1/(2λ). Applying this separately to each of the four functions (intercept and
slope function for each treatment group), we have, for l = 0, 1 and z = 0, 1,
βlz(u, φ) = β(u, φlz) = v(u)αlz + w(u)ψlz,
with ψlz ∼N(0, τ2
lzIk).
The conditional distribution of CD4 counts conditional on dropout time
(10.20) can be rewritten as
(Y i | u, z)
∼
N( Xiβ(u; φz, λz), Σ(Ωz, σz) ),
where φT
z = (αT
0z, αT
1z, ψ
T
0z, ψ
T
1z) and τ 2
z = (τ 2
0z, τ2
1z)T. Priors for this part of
the model are
αlz
∼
N(0, 104 I2),
ψlz | τlz
∼
N(0, τ2
lzIk),
τ −2
lz
∼
Gamma(.01, 1/.01),
Ω−1
z
∼
Wishart(3, 3I),
σ−2
z
∼
Gamma(.01, 1/.01),
for l = 0, 1 and z = 0, 1.

PEDIATRIC AIDS TRIAL
265
10.4.3 Model speciﬁcation: dropout times
For treatment groups z = 0, 1, the distributions p(u | z) are left essentially
unspeciﬁed, and we use Bayesian nonparametric approaches to characterize
them. In particular, we use a Dirichlet mixture of normal distributions (Sec-
tion 3.6), where
Ui | z
∼
N(αiz, νz),
αiz
∼
Gz,
Gz
∼
DP(F0, a),
with F0 a normal distribution. Technical details on sampling can be found in
MacEachern and M¨uller (1998).
10.4.4 Summary of analyses under MAR and MNAR
The primary objective is to draw inference about the diﬀerence in mean change
in log CD4 cell count at the end of follow-up; denote this by θ. In the mixture
model, this corresponds to the diﬀerence in integrated slope functions β1z(u)
between treatment groups,
θ =

β11(u) p(u | z = 1) du −

β10(u) p(u | z = 0) du.
Treatment eﬀect is summarized using the posterior mean and standard devi-
ation for θ and the posterior probability that θ > 0.
In addition to the mixture model, we ﬁt a standard random eﬀects model
that assumes MAR; the latter corresponds to a special case of the mixture
model where the βlz(u) are constant functions of u.
Figure 10.7 shows posterior means for the smooth functions βlz(u) (for
l = 0, 1; z = 0, 1), together with posterior estimates of individual-speciﬁc in-
tercepts and slopes. The plots of intercept vs. dropout time indicate that mean
CD4 is higher among those with longer follow-up, with a more pronounced
eﬀect in the low dose arm; likewise, CD4 slopes are higher among those with
longer follow-up times.
Inferences about treatment-speciﬁc intercepts and slopes, and treatment
eﬀect, are summarized in Table 10.9. In general, inferences under the mix-
ture model indicate that overall mean (intercept term) and slope over time
are lower under the MNAR assumption. The eﬀect of the MNAR assump-
tion (relative to MAR) is most pronounced in the low dose arm, where the
association between CD4 count and dropout time is stronger (Figure 10.7).
In fact, the shift in intercepts and slopes from MAR to MNAR exceeds two
posterior standard deviations (PSD) for all parameters except the intercept
on high dose. The PSD is generally greater for the mixture model.
The MNAR assumption also inﬂuences the treatment eﬀect; under MAR,
the posterior mean of θ is −.37 (PSD = .21) with P(θ > 0) = .96, suggesting

266
CASE STUDIES: NONIGNORABLE MISSINGNESS
superiority of high dose; under MNAR we have posterior mean .01 (PSD =
.25) and P(θ > 0) = .49, which is far more equivocal.
10.4.5 Summary
Studies like the Pediatric AIDS trial require methods that can handle con-
tinuous time dropout. The mixture model given here induces an MNAR
dropout process by assuming the full-data response distribution, conditional
on dropout time u, has mean that is linear in time; the intercept and slope
parameters are smooth but unspeciﬁed functions of u.
−0.6
−0.4
−0.2
0.0
0.2
−4
−2
0
2
4
6
8
−0.6
−0.4
−0.2
0.0
0.2
−15
−10
−5
0
5
−0.6
−0.4
−0.2
0.0
0.2
−4
−2
0
2
4
6
8
−0.6
−0.4
−0.2
0.0
0.2
−15
−10
−5
0
5
intercept (low dose)
intercept (high dose)
slope (low dose)
slope (high dose)
follow-up time (standardized)
follow-up time (standardized)
follow-up time (standardized)
follow-up time (standardized)
Figure 10.7 Pediatric AIDS study: posterior (pointwise) mean of intercept function
β0z(u) and slope function β1z(u), separately by treatment (z = 0, 1). Points are
individual-speciﬁc posterior mean intercepts and slopes. Tick marks on top edge of
each plot are deciles of the dropout distribution.

PEDIATRIC AIDS TRIAL
267
Table 10.9 Pediatric AIDS trial: posterior means and standard deviations for key pa-
rameters from random eﬀects model (REM) ﬁt under MAR, and mixture of varying
coeﬃcient model (VCM), which assumes MNAR.
Model
Dose
Parameter
REM (MAR)
VCM (MNAR)
Low
Intercept
5.6 (.13)
5.2 (.11)
Slope
–1.6 (.14)
–2.1 (.13)
High
Intercept
5.4 (.14)
5.3 (.14)
Slope
–1.9 (.16)
–2.1 (.19)
Diﬀerence (θ)
–.37 (.21)
.01 (.25)
P(θ > 0)
.96
.49
An advantage of the model is that the functional form of the missingness
mechanism does not have to be known; it is characterized by functions that can
be left unspeciﬁed. However, it is necessary to specify the form of E{Y (t) | u}
as a function of t; here we assume it is linear with intercept β0(u) and intercept
β1(u). A key consequence is that conditionally on U = u, the mean of Y (t)
has the same functional form before and after dropout; for the linear case, the
implication is that the slope is the same prior to and after u.
This property can be relaxed by expanding the model to accommodate
sensitivity parameters that allow (for example) a diﬀerent slope following
dropout. For example, we can expand (10.21) by introducing a sensitivity
parameter ξ = (ξ0, ξ1), so that
E(Yij | u, z) = β0z(u) + β1z(u)tij + ξz(tij −u)+,
where a+ = aI{a > 0} is the positive part of a. In this formulation, β1z(u)
is the slope prior to dropout time u and β1z(u) + ξz is the slope after u.
More generally, ξz can depend on u. Another generalization of the model
allows the variance-covariance structure to vary smoothly with u; this can
be accomplished using an appropriate parameterization of Σ. Details can be
found in Su and Hogan (2007).

Distributions
Discrete Distributions
Binomial
Y | n, θ ∼Bin(n, θ)
The binomial model characterizes the number of successes in n independent
trials, where success probability on each trial is θ. It has mass function
p(y | θ, n)
=
n
y
 
θy(1 −θ)n−y,
where y ∈{0, 1, . . ., n} and 0 < θ < 1.
Multinomial with k categories
Y | n, θ ∼Mult(n, θ)
The multinomial model characterizes the distribution of n events across k
mutually exclusive categories labeled j = 1, . . . , k, where Yj is the count and
θj is the probability for category j. The multinomial random variable has
mass function
p(y | n, θ)
=

n
y1 y2 · · · yk
 
θy1
1 θy2
2 · · · θyk
k ,
where y = (y1, . . . , yk)T, yj ≥0, k
j=1 yj = n, 0 < θj < 1, and k
j=1 θj = 1.
Continuous distributions
Dirichlet with k categories
θ ∼Dir(β)
The Dirichlet distribution characterizes the joint distribution of a vector of
random variables θ = (θ1, . . . , θk) having θj > 0 and 
j θj = 1. It is fre-
quently used as a prior for multinomial probabilities. It density function is
p(θ | β) = Γ(β1 + · · · + βk)
&k
j=1 Γ(βj)
θβ1−1
1
· · · θβk−1
k
,
where βj > 0 and Γ(·) is the gamma function.
268

DISTRIBUTIONS
269
Gamma
Y ∼Gamma(α, β)
A random variable Y having the gamma distribution has density function
p(y | α, β) =
1
Γ(α)βα exp(−y/β)yα−1,
with y > 0, α > 0, and β > 0. Its mean is E(Y ) = αβ.
Normal
Y ∼N(µ, σ2)
A random variable Y having the normal (Gaussian) distribution with mean
µ and variance σ2 has density function
p(y | µ, σ) = (2π)−1/2σ−1 exp

−(x −µ)2/2σ2
with −∞< y < ∞, −∞< µ < ∞, and σ > 0.
Multivariate normal (q-dimensional)
Y ∼N(µ, Σ)
A q-dimensional normal random variable Y = (Y1, . . . , Yq)T with mean µ =
(µ1, . . . , µq)T and q × q variance matrix Σ has density function
p(y | µ, Σ) = (2π)−q/2|Σ|−1/2 exp

−1
2(y −µ)TΣ−1(y −µ)

,
where −∞< yj < ∞, −∞< µj < ∞, and Σ is positive deﬁnite.
t distribution
Y ∼Tν(µ, σ2)
A random variable Y having the t distribution with mean µ, scale σ2 and ν
degrees of freedom has density
p(y | µ, σ2, ν) =
Γ((ν + 1)/2)
Γ(ν/2)(νσπ)1/2

1 + (y −µ)2/νσ2−(ν+1)/2 ,
where −∞< y < ∞, −∞< µ < ∞, σ2 > 0, and ν > 0.
Multivariate t (q-dimensional)
Y ∼Tν(µ, Σ)
A q dimensional vector Y having the multivariate t distribution has density
function
p(y | µ, Σ, ν) = Γ((ν + p)/2)
Γ(ν/2)(νπ)p/2 |Σ|−1/2 
1 + (y −µ)T Σ−1(y −µ)/ν
−(ν+p)/2
with −∞< Yj < ∞, −∞< µj < ∞, ν > 0, and Σ positive deﬁnite.
Wishart (q-dimensional covariance matrix)
S ∼Wishart(ν, A)
A q dimensional covariance matrix from the Wishart distribution with scale
matrix A and degrees of freedom ν has density function
p(S | A, ν) = c(ν, q) |A|−ν/2 |S|(ν−q−1)/2 exp

−tr(A−1S)

,

270
DISTRIBUTIONS
where
c(ν, q) =

2νq/2 πq(q−1)/4
q

i=1
Γ
q + 1 −i
2
 !−1
,
S and A are positive deﬁnite, and ν ≥q. The Wishart distribution is fre-
quently used as a prior for an inverse covariance matrix. It has mean E(S) =
νA.
Uniform
Y ∼Unif(a, b)
The uniform distribution has density
p(y | a, b) =
1
b −a
where a ≤y ≤b and −∞< a < b < ∞.

Bibliography
Akaike, H. (1973). Information theory and an extension of the maximum
likelihood principle, in Proceedings of 2nd International Symposium on In-
formation theory. Petrov, B. & Csaki, F., eds. Akademiai Kiado [Bu-
dapest], 267–281.
Albert, J. H. & Chib, S. (1993). Bayesian analysis of binary and poly-
chotomous response data. Journal of the American Statistical Association
88 669–679.
Albert, P. S. (2000).
A transitional model for longitudinal binary data
subject to nonignorable missing data. Biometrics 56 602–608.
Albert, P. S., Follmann, D. A., Wang, S. A. & Suh, E. B. (2002). A
latent autoregressive model for longitudinal binary data subject to infor-
mative missingness. Biometrics 58 631–642.
Anderson, T. (1984). An Introduction to Multivariate Statistical Analysis,
2nd Edition. Wiley.
Angrist, J. D., Imbens, G. W. & Rubin, D. B. (1996). Identiﬁcation of
causal eﬀects using instrumental variables (with discussion). Journal of the
American Statistical Association 91 444–472.
Azzalini, A. & Valle, A. D. (1996). The multivariate skew-normal distri-
bution. Biometrika 83 715–726.
Baker, S. G. (1995).
Marginal regression for repeated binary data with
outcome subject to non-ignorable non-response. Biometrics 51 1042–1052.
Baker, S. G., Ko, C. W. & Graubard, B. I. (2003). A sensitivity analysis
for nonrandomly missing categorical data arising from a national health
disability survey. Biostatistics 4 41–56.
Baker, S. G. & Laird, N. M. (1988). Regression analysis for categorical
variables with outcome subject to nonignorable nonresponse (Corr: V83
p1232). Journal of the American Statistical Association 83 62–69.
Bandeen-Roche, K., Miglioretti, D. L., Zeger, S. L. & Rathouz,
P. J. (1997).
Latent variable regression for multiple discrete outcomes.
Journal of the American Statistical Association 92 1375–1386.
Barnard, J., McCulloch, R. & Meng, X.-L. (2000). Modeling covariance
matrices in terms of standard deviations and correlations, with application
to shrinkage. Statistica Sinica 10 1281–1311.
271

272
BIBLIOGRAPHY
Bartholomew, D. J. & Knott, M. (1999). Latent Variable Models and
Factor Analysis. Edward Arnold Publishers Ltd.
Berger, J. O. (1985). Statistical Decision Theory and Bayesian Analysis
(Second Edition). Springer-Verlag Inc.
Berger, J. O. & Bernardo, J. M. (1992). On the development of reference
priors (Disc: P49-60), in Bayesian Statistics 4. Proceedings of the Fourth Va-
lencia International Meeting. Bernardo, J. M., Berger, J. O., Dawid,
A. P. & Smith, A. F. M., eds. Clarendon Press [Oxford University Press],
35–49.
Berger, J. O. & O’Hagan, A. (1988).
Ranges of posterior probabili-
ties for unimodal priors with speciﬁed quantiles, in Bayesian Statistics 3.
Bernardo, J. M., DeGroot, M. H., Lindley, D. V. & Smith, A.
F. M., eds. Clarendon Press [Oxford University Press], 45–65.
Berger, J. O. & Pericchi, L. R. (1996). The intrinsic Bayes factor for
linear models, in Bayesian Statistics 5 – Proceedings of the Fifth Valencia
International Meeting. Bernardo, J., Berger, J., Dawid, A. & Smith,
A., eds. 25–44.
Berger, J. O., Strawderman, W. & Tang, D. (2005). Posterior propriety
and admissibility of hyperpriors in normal hierarchical models. The Annals
of Statistics 33 606–646.
Bernardo, J. M. (2006). Reference analysis, in Handbook of Statistics. Dey,
D. & Rao, C., eds. Elsevier, 17–90.
Berry, S. M., Carroll, R. J. & Ruppert, D. (2002). Bayesian smooth-
ing and regression splines for measurement error problems. Journal of the
American Statistical Association 97 160–169.
Birmingham, J. & Fitzmaurice, G. M. (2002). A pattern-mixture model
for longitudinal binary responses with nonignorable nonresponse. Biomet-
rics 58 989–996.
Botts, C. & Daniels, M. J. (2007). A ﬂexible approach to Bayesian multi-
ple curve ﬁtting. Tech. rep., Department of Statistics, University of Florida.
Brady, M., McGrath, N., Brouwers, P., Gelber, R., Fowler, M. G.,
Yogev, R., Hutton, N., Bryson, Y. J., Mitchell, C. D., Fikrig,
S., Borkowsky, W., Jimenez, E., McSherry, G., Rubinstein, A.,
Wilfert, C. M., McIntosh, K., Elkins, M. M. & Weintrub, P. S.
(1996). Randomized study of the tolerance and eﬃcacy of high- versus low-
dose zidovudine in human immunodeﬁciency virus-infected children with
mild to moderate symptoms (AIDS Clinical Trials Group 128). Journal of
Infectious Diseases 173 1097–1106.
Breslow, N. E. & Clayton, D. G. (1993). Approximate inference in gener-
alized linear mixed models. Journal of the American Statistical Association
88 9–25.

BIBLIOGRAPHY
273
Brooks, S. P. & Gelman, A. (1998). General methods for monitoring con-
vergence of iterative simulations. Journal of Computational and Graphical
Statistics 7 434–455.
Brown, P. J., Le, N. D. & Zidek, J. V. (1994). Inference for a covariance
matrix, in Aspects of Uncertainty. A Tribute to D. V. Lindley. Freeman,
P. R. & Smith, A. F. M., eds. John Wiley & Sons, 77–92.
Burnham, K. P. & Anderson, D. R. (1998). Model Selection and Inference:
A Practical Information-theoretic Approach. Springer-Verlag Inc.
Carey, V. J. & Rosner, B. A. (2001). Analysis of longitudinally observed
irregularly timed multivariate outcomes: regression with focus on cross-
component correlation. Statistics in Medicine 20 21–31.
Carlin, B. P. & Louis, T. A. (2000). Bayes and Empirical Bayes Methods
for Data Analysis. Chapman & Hall Ltd.
Carroll, R. J., Ruppert, D. & Stefanski, L. A. (1995). Measurement
Error in Nonlinear Models. Chapman and Hall.
Casella, G. & Berger, R. L. (2001). Statistical Inference. Duxbury Press.
Catalano, P. J. & Ryan, L. M. (1992). Bivariate latent variable models
for clustered discrete and continuous outcomes. Journal of the American
Statistical Association 87 651–658.
Celeux, G., Forbes, F., Robert, C. & Titterington, D. (2006). De-
viance information criteria for missing data models. Bayesian Analysis 1
651–674.
Chaloner, K. (1996). Elicitation of prior distributions, in Bayesian Bio-
statistics.
Berry, D. A. & Stangl, D. K., eds. Marcel Dekker Inc.,
141–156.
Chen, H. Y. & Little, R. J. A. (1999). A test of missing completely at
random for generalised estimating equations with missing data. Biometrika
86 1–13.
Chen, M.-H., Ibrahim, J. G., Shao, Q.-M. & Weiss, R. E. (2003). Prior
elicitation for model selection and estimation in generalized linear mixed
models. Journal of Statistical Planning and Inference 111 57–76.
Chen, Z. & Dunson, D. B. (2003). Random eﬀects selection in linear mixed
models. Biometrics 59 762–769.
Chiang, C.-T., Rice, J. A. & Wu, C. O. (2001). Smoothing spline esti-
mation for varying coeﬃcient models with repeatedly measured dependent
variables. Journal of the American Statistical Association 96 605–619.
Chib, S. & Greenberg, E. (1998). Analysis of multivariate probit models.
Biometrika 85 347–361.

274
BIBLIOGRAPHY
Chiu, T. Y. M., Leonard, T. & Tsui, K.-W. (1996).
The matrix-
logarithmic covariance model. Journal of the American Statistical Asso-
ciation 91 198–210.
Christiansen, C. L. & Morris, C. N. (1997). Hierarchical Poisson regres-
sion modeling. Journal of the American Statistical Association 92 618–632.
Cnaan, A., Laird, N. M. & Slasor, P. (1997). Using the general linear
mixed model to analyse unbalanced repeated measures and longitudinal
data. Statistics in Medicine 16 2349–2380.
Copas, J. & Eguchi, S. (2005). Local model uncertainty and incomplete-
data bias (with discussion). Journal of the Royal Statistical Society, Series
B: Statistical Methodology 67 459–513.
Cowles, M. K. & Carlin, B. P. (1996). Markov chain Monte Carlo conver-
gence diagnostics: a comparative review. Journal of the American Statistical
Association 91 883–904.
Cox, D. R. & Reid, N. (1987). Parameter orthogonality and approximate
conditional inference (C/R: P18-39). Journal of the Royal Statistical Soci-
ety, Series B: Methodological 49 1–18.
Crainiceanu, C. M., Ruppert, D. & Carroll, R. J. (2007). Spatially
adaptive penalized splines with heteroscedastic errors. Journal of Compu-
tational and Graphical Statistics (in press) .
Crainiceanu, C. M., Ruppert, D. & Wand, M. P. (2005). Bayesian anal-
ysis for penalized spline regression using WinBUGS. Journal of Statistical
Software 14.
Czado, C. (2000). Multivariate regression analysis of panel data with binary
outcomes applied to unemployment data. Statistical Papers 41 281–304.
Damien, P., Wakefield, J. & Walker, S. W. (1999). Gibbs sampling
for Bayesian non-conjugate and hierarchical models by using auxiliary vari-
ables. Journal of the Royal Statistical Society, Series B: Statistical Method-
ology 61 331–344.
Daniels, M. J. (1998). Computing posterior distributions for covariance ma-
trices, in Computing Science and Statistics. Dimension Reduction, Compu-
tational Complexity and Information. Proceedings of the 30th Symposium
on the Interface. Weisberg, S., ed. Interface Foundation of North Amer-
ica, 192–196.
Daniels, M. J. (1999). A prior for the variance in hierarchical models. The
Canadian Journal of Statistics / La Revue Canadienne de Statistique 27
567–578.
Daniels, M. J. (2005). Shrinkage priors for the dependence structure in
longitudinal data. Journal of Statistical Planning and Inference 127 119–
130.

BIBLIOGRAPHY
275
Daniels, M. J. (2006). Bayesian modelling of several covariance matrices
and some results on the propriety of the posterior for linear regression with
correlated and/or heterogeneous errors. Journal of Multivariate Analysis
97 1185–1207.
Daniels, M. J. & Gatsonis, C. (1999).
Hierarchical generalized linear
models in the analysis of variations in health care utilization. Journal of
the American Statistical Association 94 29–42.
Daniels, M. J. & Kass, R. E. (1999). Nonconjugate Bayesian estimation
of covariance matrices and its use in hierarchical models. Journal of the
American Statistical Association 94 1254–1263.
Daniels, M. J. & Kass, R. E. (2001). Shrinkage estimators for covariance
matrices. Biometrics 57 1173–1184.
Daniels, M. J. & Pourahmadi, M. (2002). Bayesian analysis of covariance
matrices and dynamic models for longitudinal data. Biometrika 89 553–566.
Daniels, M. J. & Pourahmadi, M. (2007). Modeling correlation matri-
ces via ordered partial correlations. Tech. rep., Department of Statistics,
University of Florida.
Daniels, M. J. & Scharfstein, D. O. (2007).
A Bayesian analysis of
informative missing data with competing causes of dropout. Tech. rep.,
Department of Statistics, University of Florida.
Daniels, M. J. & Zhao, Y. (2003). Modelling the random eﬀects covariance
matrix in longitudinal data. Statistics in Medicine 22 1631–1647.
Datta, G. S. & Ghosh, J. K. (1995).
On priors providing frequentist
validity for Bayesian inference. Biometrika 82 37–45.
Davidian, M. & Giltinan, D. M. (1998). Nonlinear Models for Repeated
Measurement Data. Chapman & Hall Ltd.
Davidian, M. & Giltinan, D. M. (2003). Nonlinear models for repeated
measurement data: an overview and update. Journal of Agricultural, Bio-
logical, and Environmental Statistics 8 387–419.
Dawid, A. P. (1979). Conditional independence in statistical theory. Journal
of the Royal Statistical Society, Series B 41 1–15.
De Gruttola, V. & Tu, X. M. (1994). Modelling progression of CD4-
lymphocyte count and its relationship to survival time. Biometrics 50 1003–
1014.
Dempster, A. P., Laird, N. M. & Rubin, D. B. (1977). Maximum likeli-
hood from incomplete data via the EM algorithm (C/R: P22-37). Journal
of the Royal Statistical Society, Series B: Methodological 39 1–22.
Denison, D. G. T., Mallick, B. K. & Smith, A. F. M. (1998). Automatic
Bayesian curve ﬁtting. Journal of the Royal Statistical Society, Series B:
Statistical Methodology 60 333–350.

276
BIBLIOGRAPHY
Diggle, P. J. (1999). Comment on “Adjusting for nonignorable drop-out
using semiparametric nonresponse models”. Journal of the American Sta-
tistical Association 94 1128–1129.
Diggle, P. J., Heagerty, P. J., Liang, K.-Y. & Zeger, S. L. (2002).
Analysis of Longitudinal Data, Second Edition. Oxford University Press.
Diggle, P. J. & Kenward, M. G. (1994). Informative drop-out in longi-
tudinal data analysis (Disc: P73-93). Applied Statistics 43 49–73.
DiMateo, I., Kass, R. E. & Genovese, C. (2001). Bayesian curve ﬁtting
with free-knot splines. Biometrika 88 1055–1071.
Dobson, A. & Henderson, R. (2003). Diagnostics for joint longitudinal
and dropout time modeling. Biometrics 59 741–751.
Draper, D. (1995). Assessment and propagation of model uncertainty. Jour-
nal of the Royal Statistical Society. Series B (Methodological) 57 45–97.
Dunson, D. B. (2003). Dynamic latent trait models for multidimensional
longitudinal data. Journal of the American Statistical Association 98 555–
563.
Eberly, L. E. & Casella, G. (2003). Estimating Bayesian credible inter-
vals. Journal of Statistical Planning and Inference 112 115–132.
Eilers, P. H. C. & Marx, B. D. (1996). Flexible smoothing with B-splines
and penalties (Disc: P102-121). Statistical Science 11 89–102.
Ekholm, A. & Skinner, C. (1998). The Muscatine children’s obesity data
reanalysed using pattern mixture models. Journal of the Royal Statistical
Society, Series C: Applied Statistics 47 251–263.
Escobar, M. D. (1994). Estimating normal means with a Dirichlet process
prior. Journal of the American Statistical Association 89 268–277.
Escobar, M. D. (1995). Nonparametric Bayesian methods in hierarchical
models. Journal of Statistical Planning and Inference 43 97–106.
Faucett, C. L. & Thomas, D. C. (1996). Simultaneously modelling cen-
sored survival data and repeatedly measured covariates: a Gibbs sampling
approach. Statistics in Medicine 15 1663–1685.
Ferguson, T. S. (1982). Sequential estimation with Dirichlet process pri-
ors, in Statistical Decision Theory and Related Topics III, in two volumes,
Volume 1. Gupta, S. S. & Berger, J. O., eds. Academic Press, 385–401.
Ferrer, E. & McArdle, J. J. (2003). Alternative structural models for
multivariate longitudinal data analysis. Structural Equation Modeling 10
493–524.
Firth, D. (1987). Discussion of parameter orthogonality and approximate
conditional inference. Journal of the Royal Statistical Society, Series B:
Methodological 49 22–23.

BIBLIOGRAPHY
277
Fitzmaurice, G. M. (2003). Methods for handling dropouts in longitudinal
clinical trials. Statistica Neerlandica 57 75–99.
Fitzmaurice, G. M. & Laird, N. M. (1993). A likelihood-based method
for analysing longitudinal binary responses. Biometrika 80 141–151.
Fitzmaurice, G. M. & Laird, N. M. (1997). Regression models for mixed
discrete and continuous responses with potentially missing values. Biomet-
rics 53 110–122.
Fitzmaurice, G. M. & Laird, N. M. (2000). Generalized linear mixture
models for handling nonignorable dropouts in longitudinal studies. Bio-
statistics 1 141–156.
Fitzmaurice, G. M., Laird, N. M. & Lipsitz, S. R. (1994). Analysing
incomplete longitudinal binary responses: a likelihood-based approach. Bio-
metrics 50 601–612.
Fitzmaurice, G. M., Laird, N. M. & Ware, J. H. (2004). Applied Lon-
gitudinal Analysis. Wiley Interscience.
Fitzmaurice, G. M., Laird, N. M. & Zahner, G. E. P. (1996). Mul-
tivariate logistic models for incomplete binary responses. Journal of the
American Statistical Association 91 99–108.
Fitzmaurice, G. M., Molenberghs, G. & Lipsitz, S. R. (1995). Regres-
sion models for longitudinal binary responses with informative drop-outs.
Journal of the Royal Statistical Society, Series B: Methodological 57 691–
704.
Follmann, D. & Wu, M. C. (1995). An approximate generalized linear
model with random eﬀects for informative missing data (Corr: 97V53 p384).
Biometrics 51 151–168.
Forster, J. J. & Smith, P. W. F. (1998). Model-based inference for cat-
egorical survey data subject to non-ignorable non-response. Journal of the
Royal Statistical Society: Series B (Statistical Methodology) 60 57–70.
Frangakis, C. E. & Rubin, D. B. (2002). Principal stratiﬁcation in causal
inference. Biometrics 58 21–29.
Fuller, W. A. (1987). Measurement Error Models. Wiley.
Garthwaite, P. H. & Al-Awadhi, S. A. (2001).
Non-conjugate prior
distribution assessment for multivariate normal sampling. Journal of the
Royal Statistical Society, Series B: Statistical Methodology 63 95–110.
Garthwaite, P. H., Kadane, J. B. & O’Hagan, A. (2005). Statistical
methods for eliciting probability distributions.
Journal of the American
Statistical Association 100 680–700.
Gelfand, A. E., Dey, D. K. & Chang, H. (1992).
Model determina-
tion using predictive distributions, with implementation via sampling-based
methods (Disc: P160–167), in Bayesian Statistics 4. Proceedings of the

278
BIBLIOGRAPHY
Fourth Valencia International Meeting. Bernardo, J. M., Berger, J. O.,
Dawid, A. P. & Smith, A. F. M., eds. Clarendon Press [Oxford University
Press], 147–159.
Gelfand, A. E. & Ghosh, S. K. (1998). Model choice: a minimum posterior
predictive loss approach. Biometrika 85 1–11.
Gelfand, A. E. & Smith, A. F. M. (1990). Sampling-based approaches to
calculating marginal densities. Journal of the American Statistical Associ-
ation 85 398–409.
Gelman, A. (2006). Prior distributions for variance parameters in hierarchi-
cal models. Bayesian Analysis 1 515–534.
Gelman, A., Meng, X.-L. & Stern, H. (1996). Posterior predictive assess-
ment of model ﬁtness via realized discrepancies (Disc: P760–807). Statistica
Sinica 6 733–760.
Gelman, A. & Rubin, D. B. (1992). Inference from iterative simulation
using multiple sequences (Disc: P483–501, 503–511). Statistical Science 7
457–472.
Gelman, A., Van Mechelen, I., Verbeke, G., Heitjan, D. F. & Meul-
ders, M. (2005). Multiple imputation for model checking: completed-data
plots with missing latent data. Biometrics 61 74–85.
Geman, S. & Geman, D. (1984). Stochastic relaxation, Gibbs distributions,
and the Bayesian restoration of images.
IEEE Transactions on Pattern
Analysis and Machine Intelligence 6 721–741.
Ghosh, M. & Heo, J. (2003). Default Bayesian priors for regression models
with ﬁrst-order autoregressive residuals. Journal of Time Series Analysis
24 269–282.
Gilks, W. R. & Wild, P. (1992). Adaptive rejection sampling for Gibbs
sampling. Applied Statistics 41 337–348.
Gill, R. D. & Robins, J. M. (1997). Sequential models for coarsening and
inference, in Proceedings of the First Seattle Symposium in Biostatistics:
Survival Analysis. Lin, D. & Fleming, T. R., eds. Springer-Verlag, 295–
305.
Gill, R. D. & Robins, J. M. (2001). Causal inference for complex longitu-
dinal data: the continuous case. The Annals of Statistics 29 1785–1811.
Green, P. J. (1995). Reversible jump Markov chain Monte Carlo computa-
tion and Bayesian model determination. Biometrika 82 711–732.
Gueorguieva, R. V. & Agresti, A. (2001). A correlated probit model for
joint modeling of clustered binary and continuous responses. Journal of the
American Statistical Association 96 1102–1112.
Guo, W. (2004).
Functional data analysis in longitudinal settings using
smoothing splines. Statistical Methods in Medical Research 13 49–62.

BIBLIOGRAPHY
279
Gustafson, P. (1997). Large hierarchical Bayesian analysis of multivariate
survival data. Biometrics 53 230–242.
Gustafson, P. (2006). On model expansion, model contraction, identiﬁabil-
ity, and prior information: two illustrative scenarios involving mismeasured
variables. Statistical Science 20 111–140.
Gustafson, P., MacNab, Y. & Wen, S. (2004). On the value of derivative
evaluations and random walk suppression in Markov chain Monte Carlo
algorithms. Statistics and Computing 14 23–38.
Hastie, T. J. & Tibshirani, R. J. (1990). Generalized Additive Models.
Chapman & Hall/CRC.
Hastings, W. K. (1970).
Monte Carlo sampling methods using Markov
chains and their applications. Biometrika 57 97–109.
Heagerty, P. J. (1999).
Marginally speciﬁed logistic-normal models for
longitudinal binary data. Biometrics 55 688–698.
Heagerty, P. J. (2002). Marginalized transition models and likelihood in-
ference for longitudinal categorical data. Biometrics 58 342–351.
Heagerty, P. J. & Kurland, B. F. (2001). Misspeciﬁed maximum likeli-
hood estimates and generalised linear mixed models. Biometrika 88 973–
985.
Heckman, J. J. (1976). The common structure of statistical models of trun-
cation, sample selection and limited dependent variables and a simple es-
timator for such models. Annals of Economic and Social Measurement 5
475–492.
Heckman, J. J. (1979). Sample selection bias as a speciﬁcation error. Econo-
metrica 47 153–161.
Hedeker, D. & Gibbons, R. D. (2006). Longitudinal Data Analysis. John
Wiley & Sons.
Heitjan, D. F. & Rubin, D. B. (1991). Ignorability and coarse data. The
Annals of Statistics 19 2244–2253.
Henderson, R., Diggle, P. J. & Dobson, A. (2000). Joint modelling of
longitudinal measurements and event time data. Biostatistics (Oxford) 1
465–480.
Hern´an, M. A., Brumback, B. & Robins, J. M. (2001). Marginal struc-
tural models to estimate the joint causal eﬀect of nonrandomized treat-
ments. Journal of the American Statistical Association 96 440–448.
Hjort, N. L., Dahl, F. A. & Steinbakk, G. H. (2006). Post-processing
posterior predictive P values. Journal of the American Statistical Associa-
tion 101 1157–1174.

280
BIBLIOGRAPHY
Hobert, J. P. & Casella, G. (1996). The eﬀect of improper priors on Gibbs
sampling in hierarchical linear mixed models.
Journal of the American
Statistical Association 91 1461–1473.
Hodges, J. S. (1998). Some algebra and geometry for hierarchical models,
applied to diagnostics (Disc: P521–536). Journal of the Royal Statistical
Society, Series B: Statistical Methodology 60 497–521.
Hogan, J. W. & Daniels, M. J. (2002). A hierarchical modelling approach
to analysing longitudinal data with drop-out and non-compliance, with ap-
plication to an equivalence trial in paediatric acquired immune deﬁciency
syndrome. Journal of the Royal Statistical Society, Series C: Applied Statis-
tics 51 1–21.
Hogan, J. W. & Laird, N. M. (1996). Intention-to-treat analyses for in-
complete repeated measures data. Biometrics 52 1002–1017.
Hogan, J. W. & Laird, N. M. (1997a).
Mixture models for the joint
distribution of repeated measures and event times. Statistics in Medicine
16 239–257.
Hogan, J. W. & Laird, N. M. (1997b).
Model-based approaches to
analysing incomplete longitudinal and failure time data.
Statistics in
Medicine 16 259–272.
Hogan, J. W., Lin, X. & Herman, B. (2004a). Mixtures of varying coeﬃ-
cient models for longitudinal data with discrete or continuous nonignorable
dropout. Biometrics 60 854–864.
Hogan, J. W., Roy, J. & Korkontzelou, C. (2004b). Handling dropout
in longitudinal studies. Statistics in Medicine 23 1455–1497.
Hogan, J. W. & Wang, F. (2001). Model selection for longitudinal repeated
measures subject to informative dropout. Tech. rep., Center for Statistical
Science, Brown University.
Hoover, D. R., Rice, J. A., Wu, C. O. & Yang, L.-P. (1998). Nonpara-
metric smoothing estimates of time-varying coeﬃcient models with longi-
tudinal data. Biometrika 85 809–822.
Ibrahim, J. G. & Chen, M.-H. (2000). Power prior distributions for regres-
sion models. Statistical Science 15 46–60.
Ilk, O. & Daniels, M. J. (2007). Marginalized transition random eﬀects
models for multivariate longitudinal binary data.
Canadian Journal of
Statistics 35 105–123.
Jacobsen, M. & Keiding, N. (1995).
Coarsening at random in general
sample spaces and random censoring in continuous time. The Annals of
Statistics 23 774–786.
Jeffreys, H. (1961). Theory of Probability, 3rd Edition. Oxford University
Press.

BIBLIOGRAPHY
281
Jennrich, R. I. & Schluchter, M. D. (1986).
Unbalanced repeated-
measures models with structured covariance matrices. Biometrics 42 805–
820.
Jones, C. Y., Hogan, J. W., Snyder, B., Klein, R. S., Rompalo, A.,
Schuman, P. & Carpenter, C. C. J. (2003). Overweight and human
immunodeﬁciency virus (HIV) in women: associations between HIV progres-
sion and changes in body mass index in women in the HIV Epidemiology
Research Study cohort. Cinical Infectious Diseases 37 S69–S80.
Kadane, J. B., Dickey, J. M., Winkler, R. L., Smith, W. S. & Peters,
S. C. (1980). Interactive elicitation of opinion for a normal linear model.
Journal of the American Statistical Association 75 845–854.
Kadane, J. B. & Wolfson, L. J. (1998). Experiences in elicitation (Disc:
P55–68). Journal of the Royal Statistical Society, Series D: The Statistician
47 3–19.
Kass, R. E. & Raftery, A. E. (1995).
Bayes factors.
Journal of the
American Statistical Association 90 773–795.
Kass, R. E. & Wasserman, L. (1995). A reference Bayesian test for nested
hypotheses and its relationship to the Schwarz criterion. Journal of the
American Statistical Association 90 928–934.
Kass, R. E. & Wasserman, L. (1996). The selection of prior distributions
by formal rules (Corr: 1998V93 p412). Journal of the American Statistical
Association 91 1343–1370.
Kenward, M. G. (1998). Selection models for repeated measurements with
non-random dropout: an illustration of sensitivity. Statistics in Medicine
17 2723–2732.
Kenward, M. G. & Molenberghs, G. (1999).
Parametric models for
incomplete continuous and categorical longitudinal data. Statistical Methods
in Medical Research 8 51–83.
Kenward, M. G., Molenberghs, G. & Thijs, H. (2003). Pattern-mixture
models with proper time dependence. Biometrika 90 53–71.
Kiel, D., Puhl, J., Rosen, C., Berg, K., Murphy, J. & MacLean,
D. (1998).
Lack of association between insulin-like growth factor I and
body composition, muscle strength, physical performance or self reported
mobility among older persons with functional limitations. Journal of the
American Geriatrics Society 46 822–828.
Kleinman, K. P. & Ibrahim, J. G. (1998a). A semi-parametric Bayesian
approach to generalized linear mixed models.
Statistics in Medicine 17
2579–2596.
Kleinman, K. P. & Ibrahim, J. G. (1998b). A semiparametric Bayesian
approach to the random eﬀects model. Biometrics 54 921–938.

282
BIBLIOGRAPHY
Kullback, S. & Leibler, R. (1951). On information and suﬃciency. Annals
of Mathematical Statistics 22 79–86.
Kurland, B. F. & Heagerty, P. J. (2004). Marginalized transition models
for longitudinal binary data with ignorable and non-ignorable drop-out.
Statistics in Medicine 23 2673–2695.
Kurland, B. F. & Heagerty, P. J. (2005). Directly parameterized regres-
sion conditioning on being alive: analysis of longitudinal data truncated by
deaths. Biostatistics 6 241–258.
Kutner, M. H., Nachtsheim, C. J., Wasserman, W. & Neter, J. (2003).
Applied linear regression models. McGraw-Hill/Irwin.
Laird, N. M. (1988).
Missing data in longitudinal studies.
Statistics in
Medicine 7 305–315.
Laird, N. M. (2004). Analysis of Longitudinal and Cluster-Correlated Data.
Institute of Mathematical Statistics.
Lambert, P. & Vandenhende, F. (2002). A copula-based model for mul-
tivariate non-normal longitudinal data: analysis of a dose titration safety
study on a new antidepressant. Statistics in Medicine 21 3197–3217.
Lancaster, T. (2004). An Introduction to Modern Bayesian Econometrics.
Blackwell.
Lange, K. L., Little, R. J. A. & Taylor, J. M. G. (1989). Robust statis-
tical modeling using the t-distribution. Journal of the American Statistical
Association 84 881–896.
Lapierre, Y. D., Nai, N. V., Chauinard, G., Awad, A. G., Saxena,
B., James, B., McClure, D. J., Bakish, D., Max, P., Manchanda,
R., Beaudry, P., Bloom, D., Rotstein, E., Ancill, R., Sandor, P.,
Sladen-Dew, N., Durand, C., Chandrasena, R., Horn, E., Elliot,
D., Das, M., Ravindra, A. & Matsos, G. (1990). A controlled dose-
ranging study of remoxipride and haloperidol in schizophrenia: a Canadian
multicentre trial. Acta Psychiatric Scandinavica 82 72–76.
Laud, P. W. & Ibrahim, J. G. (1996). Predictive speciﬁcation of prior
model probabilities in variable selection. Biometrika 83 267–274.
Lavine, M. (1992). Some aspects of Polya tree distributions for statistical
modelling. The Annals of Statistics 20 1222–1235.
Lavine, M. (1994). More aspects of Polya tree distributions for statistical
modelling. The Annals of Statistics 22 1161–1176.
Lee, J. (2006). Sensitivity analysis for longitudinal binary data with dropout.
Ph.D. thesis, Brown University.
Lee, J. & Berger, J. O. (2001). Semiparametric Bayesian analysis of selec-
tion models. Journal of the American Statistical Association 96 1397–1409.

BIBLIOGRAPHY
283
Lee, J. Y., Hogan, J. W. & Hitsman, B. (2007).
Expert opinion, in-
formative priors and sensitivity analysis for longitudinal binary data with
informative dropout. Biostatistics (in revision) .
Lee, K. & Daniels, M. J. (2007). A class of Markov models for longitudinal
ordinal data. Biometrics 63 1060–1067.
Leonard, T. & Hsu, J. S. J. (1992). Bayesian inference for a covariance
matrix. The Annals of Statistics 20 1669–1696.
Liang, H., Wu, H. & Carroll, R. J. (2003). The relationship between
virologic and immunologic responses in aids clinical research using mixed-
eﬀects varying-coeﬃcient models with measurement error. Biostatistics 4
297–312.
Liang, K.-Y. & Zeger, S. L. (1986).
Longitudinal data analysis using
generalized linear models. Biometrika 73 13–22.
Lichtenstein, E. & Glasgow, R. E. (1992).
Smoking cessation: what
have we learned over the past decade? Journal of Consulting and Clinical
Psychology 60 518–527.
Liechty, J. C., Liechty, M. W. & Muller, P. (2004). Bayesian correla-
tion estimation. Biometrika 91 1–14.
Lin, D. Y. & Ying, Z. (2001). Semiparametric and nonparametric regression
analysis of longitudinal data. Journal of the American Statistical Associa-
tion 96 103–126.
Lin, H., McCulloch, C. E. & Rosenheck, R. A. (2004a). Latent pattern
mixture models for informative intermittent missing data in longitudinal
studies. Biometrics 60 295–305.
Lin, H., Scharfstein, D. O. & Rosenheck, R. A. (2004b). Analysis of
longitudinal data with irregular, outcome-dependent follow-up. Journal of
the Royal Statistical Society, Series B: Statistical Methodology 66 791–813.
Lin, X. & Carroll, R. J. (2001). Semiparametric regression for clustered
data. Biometrika 88 1179–1185.
Lin, X. & Zhang, D. (1999). Inference in generalized additive mixed models
by using smoothing splines. Journal of the Royal Statistical Society, Series
B: Statistical Methodology 61 381–400.
Little, R. J. A. (1993). Pattern-mixture models for multivariate incomplete
data. Journal of the American Statistical Association 88 125–134.
Little, R. J. A. (1994).
A class of pattern-mixture models for normal
incomplete data. Biometrika 81 471–483.
Little, R. J. A. (1995). Modeling the drop-out mechanism in repeated-
measures studies. Journal of the American Statistical Association 90 1112–
1121.

284
BIBLIOGRAPHY
Little, R. J. A. & Rubin, D. B. (1999).
Comment on “Adjusting for
nonignorable drop-out using semiparametric nonresponse models”. Journal
of the American Statistical Association 94 1130–1132.
Little, R. J. A. & Rubin, D. B. (2002). Statistical Analysis with Missing
Data. John Wiley & Sons.
Liu, C. (2001). Comment on “The art of data augmentation” (Pkg: P1–111).
Journal of Computational and Graphical Statistics 10 75–81.
Liu, C. & Rubin, D. B. (1998). Ellipsoidally symmetric extensions of the gen-
eral location model for mixed categorical and continuous data. Biometrika
85 673–688.
Liu, J. S., Wong, W. H. & Kong, A. (1994).
Covariance structure of
the Gibbs sampler with applications to the comparisons of estimators and
augmentation schemes. Biometrika 81 27–40.
Liu, X. & Daniels, M. J. (2006). A new algorithm for simulating a correla-
tion matrix based on parameter expansion and re-parameterization. Journal
of Computational and Graphical Statistics 15 897–914.
Liu, X. & Daniels, M. J. (2007). Joint models for the association of a longi-
tudinal binary and continuous process. Tech. rep., Department of Statistics,
University of Florida.
Liu, X., Waternaux, C. & Petkova, E. (1999). Inﬂuence of human im-
munodeﬁciency virus infection on neurological impairment: an analysis of
longitudinal binary data with informative drop-out. Journal of the Royal
Statistical Society, Series C: Applied Statistics 48 103–115.
MacEachern, S. N. (1994). Estimating normal means with a conjugate
style Dirichlet process prior. Communications in Statistics: Simulation and
Computation 23 727–741.
MacEachern, S. N. & M¨uller, P. (1998). Estimating mixture of Dirichlet
process models. Journal of Computational and Graphical Statistics 7 223–
238.
Manly, B. F. J. & Rayner, J. C. W. (1987). The comparison of sample
covariance matrices using likelihood ratio tests. Biometrika 74 841–847.
Marcus, B. H., Albrecht, A. E., King, T. K., Parisi, A. F., Pinto,
B. M., Roberts, M., Niaura, R. S. & Abrams, D. B. (1999). The
eﬃcacy of exercise as an aid for smoking cessation in women: a randomized
controlled trial. Archives of Internal Medicine 159 1169–1171.
Marcus, B. H., Lewis, B., Hogan, J. W., King, T. K., Albrecht,
A. E., Bock, B. & Parisi, A. F. (2005). The eﬃcacy of moderate-intensity
exercise as an aid for smoking cessation in women: a randomized controlled
trial. Nicotine and Tobacco Research 7 397–404.

BIBLIOGRAPHY
285
McCullagh, P. & Nelder, J. A. (1989). Generalized Linear Models. Chap-
man & Hall Ltd.
Miglioretti, D. L. (2003). Latent transition regression for mixed outcomes.
Biometrics 59 710–720.
Miglioretti, D. L. & Heagerty, P. J. (2004).
Marginal modeling of
multilevel binary data with time-varying covariates. Biostatistics 5 381–
398.
Molenberghs, G. & Kenward, M. G. (2007). Missing Data in Clinical
Studies. John Wiley & Sons.
Molenberghs, G., Kenward, M. G. & Lesaffre, E. (1997). The analysis
of longitudinal ordinal data with nonrandom drop-out. Biometrika 84 33–
44.
Molenberghs, G., Michiels, B., Kenward, M. G. & Diggle, P. J.
(1998).
Monotone missing data and pattern-mixture models.
Statistica
Neerlandica 52 153–161.
Molenberghs, G. & Verbeke, G. (2006). Models for Discrete Longitudinal
Data. Springer-Verlag.
Monahan, J. F. (1983). Fully Bayesian analysis of ARMA time series models.
Journal of Econometrics 21 307–331.
Mori, M., Woodworth, G. & Woolson, R. F. (1992). Application of
empirical bayes methodology to estimation of changes in the presence of
informative right censoring. Statistics in Medicine 11 621–631.
Mukerjee, R. & Ghosh, M. (1997). Second-order probability matching
priors. Biometrika 84 970–975.
Nandram, B. & Choi, J. W. (2002a). A Bayesian analysis of a proportion
under non-ignorable non-response. Statistics in Medicine 21 1189–1212.
Nandram, B. & Choi, J. W. (2002b). Hierarchical Bayesian nonresponse
models for binary data from small areas with uncertainty about ignorability.
Journal of the American Statistical Association 97 381–388.
Natarajan, R. (2001). On the propriety of a modiﬁed Jeﬀreys’s prior for
variance components in binary random eﬀects models. Statistics & Proba-
bility Letters 51 409–414.
Natarajan, R. & Kass, R. E. (2000).
Reference Bayesian methods for
generalized linear mixed models. Journal of the American Statistical Asso-
ciation 95 227–237.
Natarajan, R. & McCulloch, C. E. (1995). A note on the existence of the
posterior distribution for a class of mixed models for binomial responses.
Biometrika 82 639–643.
Neal, R. M. (1996). Bayesian Neural Networks. Springer-Verlag Inc.

286
BIBLIOGRAPHY
Neal, R. M. (2003). Slice sampling. The Annals of Statistics 31 705–767.
Nelsen, R. B. (1999). An Introduction to Copulas. Springer-Verlag Inc.
N´u˜nez Ant´on, V. & Zimmermann, D. L. (2000). Modeling nonstationary
longitudinal data. Biometrics 56 699–705.
Oakley, J. & O’Hagan, A. (2007).
Uncertainty in prior elicitations: a
nonparametric approach. Biometrika 94 427–441.
O’Brien, S. M. & Dunson, D. B. (2004). Bayesian multivariate logistic
regression. Biometrics 60 739–746.
O’Hagan, A. (1998). Eliciting expert beliefs in substantial practical appli-
cations (Disc: P55–68). Journal of the Royal Statistical Society, Series D:
The Statistician 47 21–35.
Overall, J. E. & Gorham, D. R. (1988). The Brief Psychiatric Rating
Scale (BPRS): recent developments in ascertainment and scaling.
Psy-
chopharmacology Bulletin 22 97–99.
Peruggia, M. (1997). On the variability of case-deletion importance sam-
pling weights in the Bayesian linear model. Journal of the American Sta-
tistical Association 92 199–207.
Pourahmadi, M. (1999). Joint mean-covariance models with applications to
longitudinal data: unconstrained parameterisation. Biometrika 86 677–690.
Pourahmadi, M. (2000). Maximum likelihood estimation of generalised lin-
ear models for multivariate normal covariance matrix. Biometrika 87 425–
435.
Pourahmadi, M. (2007). Cholesky decompositions and estimation of a co-
variance matrix: orthogonality of variance-correlation parameters.
Tech.
rep., Division of Statistics, Northern Illinois University.
Pourahmadi, M. & Daniels, M. J. (2002). Dynamic conditionally linear
mixed models for longitudinal data. Biometrics 58 225–231.
Pourahmadi, M., Daniels, M. J. & Park, T. (2007). Simultaneous mod-
elling of several covariance matrices using the modiﬁed choleski decompo-
sition with applications. Journal of Multivariate Analysis 98 568–587.
Press, S. J. (2003). Subjective and Objective Bayesian Statistics: Principles,
Models, and Applications. John Wiley & Sons.
Pulkstenis, E. P., Ten Have, T. R. & Landis, J. R. (1998). Model for
the analysis of binary longitudinal pain data subject to informative dropout
through remedication. Journal of the American Statistical Association 93
438–450.
Ribaudo, H. J. & Thompson, S. G. (2002). The analysis of repeated multi-
variate binary quality of life data: a hierarchical model approach. Statistical
Methods in Medical Research 11 69–83.

BIBLIOGRAPHY
287
Rice, J. A. (2004). Functional and longitudinal data analysis: perspectives
on smoothing. Statistica Sinica 14 631–647.
Roberts, G. O. & Sahu, S. K. (1997).
Updating schemes, correlation
structure, blocking and parameterization for the Gibbs sampler. Journal of
the Royal Statistical Society, Series B: Methodological 59 291–317.
Robins, J. M. (1997). Non-response models for the analysis of non-monotone
non-ignorable missing data. Statistics in Medicine 16 21–37.
Robins, J. M. & Ritov, Y. (1997). Toward a curse of dimensionality appro-
priate (CODA) asymptotic theory for semi-parametric models. Statistics
in Medicine 16 285–319.
Robins, J. M., Rotnitzky, A. & Zhao, L. P. (1995). Analysis of semipara-
metric regression models for repeated outcomes in the presence of missing
data. Journal of the American Statistical Association 90 106–121.
Robins, J. M., Ventura, V. & van der Vaart, A. (2000). Asymptotic
distribution of P values in composite null models (Pkg: P1127–1171). Jour-
nal of the American Statistical Association 95 1143–1156.
Rodriguez, A., Dunson, D. B. & Gelfand, A. E. (2007). Nonparametric
functional data analysis through Bayesian density estimation. Tech. rep.,
Duke University.
Rotnitzky, A., Robins, J. M. & Scharfstein, D. O. (1998). Semipara-
metric regression for repeated outcomes with nonignorable nonresponse.
Journal of the American Statistical Association 93 1321–1339.
Rotnitzky, A., Scharfstein, D. O., Su, T.-L. & Robins, J. M. (2001).
Methods for conducting sensitivity analysis of trials with potentially non-
ignorable competing causes of censoring. Biometrics 57 103–113.
Roy, J. (2003). Modeling longitudinal data with nonignorable dropouts using
a latent dropout class model. Biometrics 59 829–836.
Roy, J. & Daniels, M. J. (2007). A general class of pattern mixture models
for nonignorable dropout with many possible dropout times. Biometrics
(in press) .
Roy, J. & Hogan, J. W. (2007). Causal contrasts in randomized trials of
two active treatments with noncompliance. Tech. rep., Center for Statistical
Sciences, Brown University.
Roy, J. & Lin, X. (2000). Latent variable models for longitudinal data with
multiple continuous outcomes. Biometrics 56 1047–1054.
Roy, J. & Lin, X. (2002). The analysis of multivariate longitudinal outcomes
with nonignorable dropouts and missing covariates: changes in methadone
treatment practices.
Journal of the American Statistical Association 97
40–52.
Rubin, D. B. (1976). Inference and missing data. Biometrika 63 581–590.

288
BIBLIOGRAPHY
Rubin, D. B. (1977).
Formalizing subjective notions about the eﬀect of
nonrespondents in sample surveys. Journal of the American Statistical As-
sociation 72 538–543.
Rubin, D. B. (1987). Multiple Imputation for Nonresponse in Surveys. John
Wiley & Sons.
Rubin, D. B. (2006). Causal inference through potential outcomes and prin-
cipal stratiﬁcation: Application to studies with censoring due to death (with
discussion). Statistical Science 21 299–321.
Ruppert, D. & Carroll, R. J. (2000). Spatially-adaptive penalties for
spline ﬁtting. Australian & New Zealand Journal of Statistics 42 205–223.
Ruppert, D., Wand, M. P. & Carroll, R. J. (2003). Semiparametric
Regression. Cambridge University Press.
Sammel, M. D., Lin, X. & Ryan, L. M. (1999). Multivariate linear mixed
models for multiple outcomes. Statistics in Medicine 18 2479–2492.
Sammel, M. D., Ryan, L. M. & Legler, J. M. (1997). Latent variable
models for mixed discrete and continuous outcomes. Journal of the Royal
Statistical Society, Series B: Methodological 59 667–678.
Schafer, J. L. (1997). Analysis of Incomplete Multivariate Data. Chapman
& Hall Ltd.
Scharfstein, D. O., Daniels, M. J. & Robins, J. M. (2003). Incorporat-
ing prior beliefs about selection bias into the analysis of randomized trials
with missing outcomes. Biostatistics (Oxford) 4 495–512.
Scharfstein, D. O., Halloran, M. E., Chu, H. & Daniels, M. J. (2006).
On estimation of vaccine eﬃcacy using validation samples with selection
bias. Biostatistics 7 615–629.
Scharfstein, D. O. & Irizarry, R. A. (2003). Generalized additive selec-
tion models for the analysis of studies with potentially nonignorable missing
outcome data. Biometrics 59 601–613.
Scharfstein, D. O., Rotnitzky, A. & Robins, J. M. (1999). Adjusting
for nonignorable drop-out using semiparametric nonresponse models (C/R:
P1121–1146).
Journal of the American Statistical Association 94 1096–
1120.
Schildcrout, J. & Heagerty, P. J. (2007). Marginalized models for long
series of binary responses. Biometrics 63 322–331.
Sethuraman, J. (1994). A constructive deﬁnition of Dirichlet priors. Sta-
tistica Sinica 4 639–650.
Smith, D., Warren, D., Vlahov, D., Schuman, P., Stein, M., Green-
berg, B. & Holmberg, S. (1997). Design and baseline participant char-
acteristics of the Human Immunodeﬁciency Virus Epidemiology Research
Study (HERS). American Journal of Epidemiology 146 459–469.

BIBLIOGRAPHY
289
Spiegelhalter, D. J., Best, N. G., Carlin, B. P. & van der Linde, A.
(2002). Bayesian measures of model complexity and ﬁt (Pkg: P583–639).
Journal of the Royal Statistical Society, Series B: Statistical Methodology
64 583–616.
Su, L. & Hogan, J. W. (2007). Likelihood-based semiparametric marginal
regression for continuously measured longitudinal binary outcomes. Tech.
rep., Center for Statistical Sciences, Brown University.
Sy, J. P., Taylor, J. M. G. & Cumberland, W. G. (1997). A stochastic
model for the analysis of bivariate longitudinal AIDS data. Biometrics 53
542–555.
Tanner, M. A. & Wong, W. H. (1987). The calculation of posterior distri-
butions by data augmentation (C/R: P541-550). Journal of the American
Statistical Association 82 528–540.
Taylor, J. M. G., Cumberland, W. G. & Sy, J. P. (1994). A stochastic
model for analysis of longitudinal AIDS data.
Journal of the American
Statistical Association 89 727–736.
Tierney, L. (1994).
Markov chains for exploring posterior distributions
(Disc: P1728–1762). The Annals of Statistics 22 1701–1728.
Tierney, L. & Kadane, J. B. (1986). Accurate approximations for poste-
rior moments and marginal densities. Journal of the American Statistical
Association 81 82–86.
Trevisani, M. & Gelfand, A. E. (2003). Inequalities between expected
marginal log-likelihoods, with implications for likelihood-based model com-
plexity and comparison measures. Canadian Journal of Statistics 31 239–
250.
Troxel, A. B., Lipsitz, S. R. & Harrington, D. P. (1998). Marginal
models for the analysis of longitudinal measurements with nonignorable
non-monotone missing data. Biometrika 85 661–672.
Troxel, A. B., Ma, G. & Heitjan, D. F. (2004). An index of local sensi-
tivity to nonignorability. Statistica Sinica 14 1221–1237.
Tsiatis, A. A. (2006). Semiparametric Theory and Missing Data. Springer.
Tsiatis, A. A. & Davidian, M. (2004). Joint modeling of longitudinal and
time-to-even data: an overview. Statistica Sinica 14 809–834.
van der Laan, M. J. & Robins, J. M. (2003). Uniﬁed Methods for Censored
Longitudinal Data and Causality. Springer.
van Dyk, D. A. & Meng, X.-L. (2001). The art of data augmentation (Pkg:
P1–111). Journal of Computational and Graphical Statistics 10 1–50.
Vansteelandt, S., Goetghebeur, E., Kenward, M. G. & Molen-
berghs, G. (2006). Ignorance and uncertainty regions as inferential tools
in a sensitivity analysis. Statistica Sinica 16 953–979.

290
BIBLIOGRAPHY
Verbeke, G. & Lesaffre, E. (1996). A linear mixed-eﬀects model with
heterogeneity in the random-eﬀects population. Journal of the American
Statistical Association 91 217–221.
Verbeke, G. & Molenberghs, G. (2000). Linear Mixed Models for Lon-
gitudinal Data. Springer-Verlag Inc.
Verbeke, G., Molenberghs, G., Thijs, H., Lesaffre, E. & Kenward,
M. G. (2001). Sensitivity analysis for nonrandom dropout: a local inﬂuence
approach. Biometrics 57 7–14.
Wakefield, J. (1996). The Bayesian analysis of population pharmacokinetic
models. Journal of the American Statistical Association 91 62–75.
Wang, N., Carroll, R. J. & Lin, X. (2005).
Eﬃcient semiparametric
marginal estimation for longitudinal/clustered data. Journal of the Amer-
ican Statistical Association 100 147–157.
Wang, Y. & Taylor, J. M. G. (2001). Jointly modeling longitudinal and
event time data with application to acquired immunodeﬁciency syndrome.
Journal of the American Statistical Association 96 895–905.
Ware, J. H. (1985). Linear models for the analysis of longitudinal studies.
The American Statistician 39 95–101.
Weiss, R. E. (2005). Modeling Longitudinal Data. Springer.
Wilkins, K. J. & Fitzmaurice, G. M. (2006). A hybrid model for nonig-
norable dropout in longitudinal binary responses. Biometrics 62 168–176.
Wong, F., Carter, C. K. & Kohn, R. (2003).
Eﬃcient estimation of
covariance selection models. Biometrika 90 809–830.
Wooldridge, J. M. (2001).
Econometric Analysis of Cross Section and
Panel Data. The MIT Press.
Wu, H. & Wu, L. (2002). Identiﬁcation of signiﬁcant host factors for HIV dy-
namics modelled by non-linear mixed-eﬀects models. Statistics in Medicine
21 753–771.
Wu, M. C. & Bailey, K. R. (1988). Analysing changes in the presence of
informative right censoring caused by death and withdrawal. Statistics in
Medicine 7 337–346.
Wu, M. C. & Bailey, K. R. (1989). Estimation and comparison of changes
in the presence of informative right censoring: conditional linear model
(Corr: V46 p889). Biometrics 45 939–955.
Wu, M. C. & Carroll, R. J. (1988). Estimation and comparison of changes
in the presence of informative right censoring by modeling the censoring
process (Corr: V45 p1347; V47 p357). Biometrics 44 175–188.
Wulfsohn, M. S. & Tsiatis, A. A. (1997). A joint model for survival and
longitudinal data measured with error. Biometrics 53 330–339.

BIBLIOGRAPHY
291
Xu, J. & Zeger, S. L. (2001). Joint analysis of longitudinal data comprising
repeated measures and times to events. Journal of the Royal Statistical
Society, Series C: Applied Statistics 50 375–387.
Yang, R. & Berger, J. O. (1994). Estimation of a covariance matrix using
the reference prior. The Annals of Statistics 22 1195–1211.
Yao, F., M¨uller, H.-G. & Wang, J.-L. (2005a). Functional data analysis
for sparse longitudinal data. Journal of the American Statistical Association
100 577–590.
Yao, F., M¨uller, H.-G. & Wang, J.-L. (2005b). Functional linear regres-
sion analysis for longitudinal data. The Annals of Statistics 33 2873–2903.
Yao, Q., Wei, L. J. & Hogan, J. W. (1998). Analysis of incomplete re-
peated measurements with dependent censoring times. Biometrika 85 139–
149.
Zeger, S. L. & Liang, K.-Y. (1992).
An overview of methods for the
analysis of longitudinal data. Statistics in Medicine 11 1825–1839.
Zhang, D. (2004). Generalized linear mixed models with varying coeﬃcients
for longitudinal data. Biometrics 60 8–15.
Zhang, D. & Davidian, M. (2001). Linear mixed models with ﬂexible dis-
tributions of random eﬀects for longitudinal data. Biometrics 57 795–802.
Zhang, D., Lin, X., Raz, J. & Sowers, M. (1998). Semiparametric stochas-
tic mixed models for longitudinal data. Journal of the American Statistical
Association 93 710–719.
Zhang, J. & Heitjan, D. F. (2006). A simple local sensitivity analysis tool
for nonignorable coarsening: application to dependent censoring. Biometrics
62 1260–1268.
Zhang, X., Boscardin, W. J. & Belin, T. R. (2006). Sampling correlation
matrices in Bayesian models with correlated latent variables. Journal of
Computational and Graphical Statistics 15 880–896.
Zhao, X., Marron, J. S. & Wells, M. T. (2004). The functional data
analysis view of longitudinal data. Statistica Sinica 14 789–808.
Zheng, Y. & Heagerty, P. J. (2005). Partly conditional survival models
for longitudinal data. Biometrics 61 379–391.

Author Index
Abrams, D. B. 6
Agresti, A. 135, 138
Akaike, H. 63
Al-Awadhi, S. A. 70
Albert, J. H. 57
Albert, P. S. 168, 175
Albrecht, A. E. 6
Ancill, R. 2
Anderson, D. R. 63
Anderson, T. 105
Awad, A. G. 2
Azzalini, A. 257
Bailey, K. R. 113, 181, 187, 199
Baker, S. G. 168, 175, 195, 218
Bakish, D. 2
Bandeen-Roche, K. 144
Barnard, J. 48, 132
Bartholomew, D. J. 20
Beaudry, P. 2
Belin, T. R. 70
Berg, K. 4
Berger, J. O. 43, 46, 48, 69, 215
Berger, R. L. xviii
Bernardo, J. M. 46
Berry, S. M. 33, 45, 71
Best, N. G. 62, 64, 65
Birmingham, J. 195
Bloom, D. 2
Bock, B. 6
Boscardin, W. J. 70
Botts, C. 71
Breslow, N. E. 20
Brooks, S. P. 54
Brown, P. J. 70
Burnham, K. P. 63
Carey, V. J. 135, 136
Carlin, B. P. 43, 44, 58, 60, 62, 64, 65
Carpenter, C. C. J. 36
Carroll, R. J. 17, 18, 32, 33, 36, 45,
71, 112, 113, 200
Carter, C. K. 143
Casella, G. xviii, 62, 69, 70
Catalano, P. J. 138
Celeux, G. 139
Chaloner, K. 50, 70
Chandrasena, R. 2
Chang, H. 71
Chauinard, G. 2
Chen, H. Y. 92
Chen, M.-H. 50, 70
Chen, Z. 143
Chiang, C.-T. 32
Chib, S. 28, 30, 55, 57
Chiu, T. Y. M. 143
Choi, J. W. 218, 222
Christiansen, C. L. 47
Chu, H. 231
Clayton, D. G. 20
Cnaan, A. 2
Copas, J. 218, 231
Cowles, M. K. 58
Cox, D. R. 119
Crainiceanu, C. M. 45, 71, 200, 264
Cumberland, W. G. 130, 144
Czado, C. 133
Dahl, F. A. 71
Damien, P. 70
292

AUTHOR INDEX
293
Daniels, M. J. 6, 13, 14, 20, 23, 26,
30, 38, 47–49, 55, 57, 65, 69–71,
117, 126, 127, 129, 131, 132, 134,
135, 138, 139, 142–144, 146,
156–158, 168, 176, 182, 202, 206,
215, 219, 228, 231
Das, M. 2
Datta, G. S. 46
Davidian, M. 15, 24, 31, 87, 144
Dawid, A. P. 50
De Gruttola, V. 112, 144
Dempster, A. P. 121
Denison, D. G. T. 71
Dey, D. K. 71
Dickey, J. M. 50
Diggle, P. J. 15, 18, 27, 35, 36, 41,
96, 108, 112, 144, 168, 174, 182,
183, 185, 206, 217, 218
DiMateo, I. 71
Dobson, A. 112, 144, 206, 215
Draper, D. 218
Dunson, D. B. 38, 135, 137, 143, 144
Durand, C. 2
Eberly, L. E. 70
Eguchi, S. 218, 231
Eilers, P. H. C. 17
Ekholm, A. 195
Elliot, D. 2
Escobar, M. D. 68, 71
Faucett, C. L. 112, 144
Ferguson, T. S. 71
Ferrer, E. 144
Firth, D. 119
Fitzmaurice, G. M. 15, 18, 25, 28,
35, 37, 38, 144, 168, 175, 182, 187,
195, 206, 215
Follmann, D. 112
Forbes, F. 139
Forster, J. J. 218, 231
Frangakis, C. E. 114
Fuller, W. A. 36
Garthwaite, P. H. 70
Gatsonis, C. 20, 55
Gelfand, A. E. 38, 51, 62, 64–66, 70,
71
Gelman, A. 47, 49, 54, 58, 67, 69,
142
Geman, D. 52
Geman, S. 52
Genovese, C. 71
Ghosh, J. K. 46
Ghosh, M. 46, 48
Ghosh, S. K. 62, 65, 66
Gibbons, R. D. 15
Gilks, W. R. 70
Gill, R. D. 91
Giltinan, D. M. 15, 31
Glasgow, R. E. 8, 94, 255
Goetghebeur, E. 167, 218
Gorham, D. R. 2
Graubard, B. I. 218
Green, P. J. 71
Greenberg, B. 9
Greenberg, E. 28, 30, 55
Gueorguieva, R. V. 135, 138
Guo, W. 32
Gustafson, P. 70, 218, 229, 231
Halloran, M. E. 231
Hastie, T. J. 18
Hastings, W. K. 54
Heagerty, P. J. 9, 15, 17, 18, 27–29,
35, 36, 38, 41, 114, 121, 130, 144,
168, 175, 215
Heckman, J. J. 107, 108, 168, 171
Hedeker, D. 15
Heitjan, D. F. 91, 142, 218, 221
Henderson, R. 112, 144, 206, 215
Heo, J. 48
Herman, B. 13, 14, 113, 182, 199, 262
Hitsman, B. 11, 256
Hjort, N. L. 71
Hobert, J. P. 62, 69
Hodges, J. S. 71

294
AUTHOR INDEX
Hogan, J. W. 2, 6, 11, 13, 14, 30, 36,
66, 87, 91, 113, 143, 144, 181, 182,
187, 199, 201, 202, 256, 262, 267
Holmberg, S. 9
Hoover, D. R. 32
Horn, E. 2
Hsu, J. S. J. 143
Ibrahim, J. G. 50, 69, 70, 215
Ilk, O. 38, 70, 139, 142, 144
Irizarry, R. A. 215
Jacobsen, M. 91
James, B. 2
Jeﬀreys, H. 46
Jennrich, R. I. 26
Jones, C. Y. 36
Kadane, J. B. 42, 50, 70, 213
Kass, R. E. 43, 46, 48, 69, 71
Keiding, N. 91
Kenward, M. G. xviii, 1, 91, 96, 108,
109, 167, 168, 170, 173, 174, 182,
183, 185, 186, 218
Kiel, D. 4
King, T. K. 6
Klein, R. S. 36
Kleinman, K. P. 69, 215
Knott, M. 20
Ko, C. W. 218
Kohn, R. 143
Kong, A. 70
Korkontzelou, C. 6, 91, 182
Kullback, S. 63
Kurland, B. F. 114, 130, 168, 175
Kutner, M. H. xviii
Laird, N. M. 2, 13–15, 18, 23, 25, 28,
35–38, 100, 113, 121, 144, 181,
182, 187, 195, 201, 202, 215
Lambert, P. 144
Lancaster, T. 30
Landis, J. R. 112
Lange, K. L. 57
Lapierre, Y. D. 2
Laud, P. W. 50
Lavine, M. 71
Le, N. D. 70
Lee, J. 215, 256
Lee, J. Y. 11, 256
Lee, K. 30
Legler, J. M. 144
Leibler, R. 63
Leonard, T. 143
Lesaﬀre, E. 24, 168, 218
Lewis, B. 6
Liang, H. 36
Liang, K.-Y. 15, 18, 19, 24, 27,
35–37, 41
Lichtenstein, E. 8, 94, 255
Liechty, J. C. 143
Liechty, M. W. 143
Lin, D. Y. 87
Lin, H. 87
Lin, X. 13, 14, 17, 32, 34, 113, 136,
144, 182, 199, 203, 262
Lipsitz, S. R. 28, 168, 175
Little, R. J. A. xviii, 1, 57, 91, 92,
99, 100, 102, 104, 112, 120, 175,
181–183, 188, 195, 199
Liu, C. 70, 144
Liu, J. S. 70
Liu, X. 6, 65, 70, 134, 135, 138, 143,
156–158, 168
Louis, T. A. 43, 44, 60
Ma, G. 218, 221
MacEachern, S. N. 68, 69, 265
MacLean, D. 4
MacNab, Y. 70
Mallick, B. K. 71
Manchanda, R. 2
Manly, B. F. J. 132
Marcus, B. H. 6
Marron, J. S. 32, 38

AUTHOR INDEX
295
Marx, B. D. 17
Matsos, G. 2
Max, P. 2
McArdle, J. J. 144
McClure, D. J. 2
McCullagh, P. 17, 19
McCulloch, C. E. 69
McCulloch, R. 48, 132
Meng, X.-L. 48, 56, 67, 70, 121, 132
Meulders, M. 142
Michiels, B. 96, 182, 183, 185
Miglioretti, D. L. 38, 144
Molenberghs, G. xviii, 1, 15, 91, 96,
109, 167, 168, 173, 175, 182, 183,
185, 186, 218
Monahan, J. F. 48
Mori, M. 181
Morris, C. N. 47
Mukerjee, R. 46
M¨uller, H.-G. 38
Muller, P. 143
Murphy, J. 4
Nachtsheim, C. J. xviii
Nai, N. V. 2
Nandram, B. 218, 222
Natarajan, R. 48, 69
Neal, R. M. 70
Nelder, J. A. 17, 19
Nelsen, R. B. 144
Neter, J. xviii
Niaura, R. S. 6
N´u˜nez Ant´on, V. 26, 143
Oakley, J. 69
O’Brien, S. M. 135, 137
O’Hagan, A. 69, 70
Overall, J. E. 2
Parisi, A. F. 6
Park, T. 132
Pericchi, L. R. 46
Peruggia, M. 62
Peters, S. C. 50
Petkova, E. 168
Pinto, B. M. 6
Pourahmadi, M. 26, 69, 70, 124–127,
129, 131, 132, 139, 143, 146
Press, S. J. 70
Puhl, J. 4
Pulkstenis, E. P. 112
Raftery, A. E. 43
Rathouz, P. J. 144
Ravindra, A. 2
Rayner, J. C. W. 132
Raz, J. 32, 34
Reid, N. 119
Ribaudo, H. J. 144
Rice, J. A. 32
Ritov, Y. 176
Robert, C. 139
Roberts, G. O. 52
Roberts, M. 6
Robins, J. M. xviii, 71, 88, 91, 114,
167, 168, 175, 176, 202, 217–219,
221, 228, 231
Rodriguez, A. 38
Rompalo, A. 36
Rosen, C. 4
Rosenheck, R. A. 87
Rosner, B. A. 135, 136
Rotnitzky, A. 88, 91, 114, 168, 175,
176, 217–219, 221
Rotstein, E. 2
Roy, J. 6, 91, 136, 182, 203, 206, 215
Rubin, D. B. xviii, xix, 1, 9, 58, 91,
99, 100, 102, 104, 108, 114, 120,
121, 123, 124, 144, 175, 181, 195,
218–220, 222
Ruppert, D. 17, 18, 32, 33, 36, 45,
71, 200, 264
Ryan, L. M. 138, 144
Sahu, S. K. 52

296
AUTHOR INDEX
Sammel, M. D. 144
Sandor, P. 2
Saxena, B. 2
Schafer, J. L. xviii, 123
Scharfstein, D. O. 87, 88, 114, 168,
175, 176, 202, 215, 217–219, 221,
228, 231
Schildcrout, J. 38
Schluchter, M. D. 26
Schuman, P. 9, 36
Sethuraman, J. 71
Shao, Q.-M. 70
Skinner, C. 195
Sladen-Dew, N. 2
Slasor, P. 2
Smith, A. F. M. 51, 70, 71
Smith, D. 9
Smith, P. W. F. 218, 231
Smith, W. S. 50
Snyder, B. 36
Sowers, M. 32, 34
Spiegelhalter, D. J. 62, 64, 65
Stefanski, L. A. 36
Stein, M. 9
Steinbakk, G. H. 71
Stern, H. 67
Strawderman, W. 48, 69
Su, L. 30, 143, 199, 201, 262, 267
Su, T.-L. 88, 114
Sy, J. P. 130, 144
Tang, D. 48, 69
Tanner, M. A. 56
Taylor, J. M. G. 57, 130, 144
Ten Have, T. R. 112
Thijs, H. 109, 173, 182, 185, 186, 218
Thomas, D. C. 112, 144
Thompson, S. G. 144
Tibshirani, R. J. 18
Tierney, L. 42, 59, 213
Titterington, D. 139
Trevisani, M. 64
Troxel, A. B. 218, 221
Tsiatis, A. A. xviii, 1, 87, 91, 112,
144
Tsui, K.-W. 143
Tu, X. M. 112, 144
Valle, A. D. 257
van der Laan, M. J. xviii, 91, 218
van der Linde, A. 62, 64, 65
van der Vaart, A. 71
van Dyk, D. A. 56, 70, 121
Van Mechelen, I. 142
Vandenhende, F. 144
Vansteelandt, S. 167, 218
Ventura, V. 71
Verbeke, G. 15, 24, 142, 218
Vlahov, D. 9
Wakeﬁeld, J. 31, 70
Walker, S. W. 70
Wand, M. P. 17, 18, 32, 45, 71, 200,
264
Wang, F. 66
Wang, J.-L. 38
Wang, N. 32
Wang, Y. 144
Ware, J. H. 15, 18, 35–38
Warren, D. 9
Wasserman, L. 46
Wasserman, W. xviii
Waternaux, C. 168
Wei, L. J. 87
Weiss, R. E. 15, 70
Wells, M. T. 32, 38
Wen, S. 70
Wild, P. 70
Wilkins, K. J. 206, 215
Winkler, R. L. 50
Wolfson, L. J. 70
Wong, F. 143
Wong, W. H. 56, 70
Woodworth, G. 181
Woolson, R. F. 181

AUTHOR INDEX
297
Wu, C. O. 32
Wu, H. 31, 36
Wu, L. 31
Wu, M. C. 112, 113, 181, 187, 199
Wulfsohn, M. S. 112, 144
Xu, J. 144
Yang, L.-P. 32
Yang, R. 48
Yao, F. 38
Yao, Q. 87
Ying, Z. 87
Zahner, G. E. P. 28
Zeger, S. L. 15, 18, 19, 24, 27, 35–37,
41, 144
Zhang, D. 17, 24, 32, 34
Zhang, J. 218, 221
Zhang, X. 70
Zhao, L. P. 91, 168
Zhao, X. 32, 38
Zhao, Y. 23, 117, 131, 132
Zheng, Y. 9
Zidek, J. V. 70
Zimmermann, D. L. 26, 143

Index
auxiliary variables, 8, 87, 94, 124
joint models, 134–138
MAR, 94–95, 134–138
applied to CTQ II, 155–159
available case missing value (ACMV)
constraints, see mixture model
basis function, see penalized splines
canonical parameter, 20
Commit to Quit I (CTQ I)
analyses
completers only, 79–84
ignorable, 151–155
description, 5–8
Commit to Quit II (CTQ II)
analyses, 155–159
description, 5–8
complete case missing value constraints,
see mixture model
conditional linear model, 113, 199
conditionally speciﬁed models, 20–25
covariance models
applied to growth hormone trial,
145–149
covariates, 130–134
mis-speciﬁcation, eﬀects of, 116–121
misaligned data, 129–130
random eﬀects, 128–129
structured GARP/IV, 124–128
covariate eﬀects
longitudinal vs. cross-sectional, 36
multivariate probit model, 30
population-averaged, 37
subject-speciﬁc, 37
subject-speciﬁc vs.
population-averaged, 24, 36–38
covariates
endogenous, 35
exogenous, 35
measurement error in, 36
time-varying, 35
assumptions regarding, 35–36
stochastic, 35
data analyses
Commit to Quit I (CTQ I), 79–84,
151–155
Commit to Quit II (CTQ II),
155–159
growth hormone trial, 72–75,
145–149, 234–248
HIV Epidemiology Research Study
(HERS), 159–162
OASIS study, 248–261
Pediatric AIDS Trial (ACTG 128),
261–267
schizophrenia trial, 75–79, 149–151
data augmentation, 55–58, 121–124
in selection models, 180–181
DIC, see model selection
directly speciﬁed models, 25–31
continuous responses, 25–28
discrete responses, 28–31
dropout, 85, 88
and mortality, 85
deﬁnition, 88
description of, 87–89
hazard of, 96, 173
multiple cause, 201–202
endogeneity, see covariates
Examples
conditionally conjugate priors, 44

Index
299
multivariate normal model, 44–45
normal random eﬀects model, 45
penalized splines, 45
conjugate priors, 43–44
data augmentation
logistic random eﬀects model, 123
multivariate normal model,
122–123
multivariate probit model, 56–57
multivariate t-distribution, 57
normal random eﬀects model,
57–58
dropout and mortality in a cohort
study, 85
dropout in a longitudinal trial, 85
Gibbs sampling
logistic random eﬀects model, 53
normal random eﬀects model,
52–53
ignorability
with marginalized transition
model, 104–105
Jeﬀreys’ prior, 46
logistic random eﬀects model, 23–24
MAR
nonignorability, 105–106
with bivariate normal model,
103–104
with bivariate response, 93
marginalized transition model, 28–30
likelihood, 40–41
MCAR
with covariates, 92
without covariates, 92
mis-speciﬁcation of dependence,
117–118
orthogonality, 120–121
mixture model
bivariate binary data, 195–196
bivariate response, 188–190
covariate eﬀects, 203–206
identiﬁcation via interior family
constraints, 190–191
mixture of normals, 110–111
multivariate normal within
pattern, 192–194
priors for sensitivity parameters,
223–224
sensitivity analysis
parameterization, 220–221
sensitivity parameterization for
bivariate normals, 224–226
trivariate binary data, 196–198
varying coeﬃcient model, 199–200
MNAR
with bivariate response, 94
multivariate normal likelihood
temporally aligned, 40
temporally misaligned, 40
multivariate normal model
temporally aligned, 26–27
temporally misaligned, 27–28
multivariate probit model, 30–31
likelihood, 41
nonidentiﬁability, 50–51
normal random eﬀects model, 22–23
likelihood, 40
penalized splines, 32–34
selection model
bivariate normal, 107
Diggle and Kenward, 174–175
prior dependence, 229–230
prior speciﬁcation, 227–229
semiparametric, 179–180, 230–231
shared parameter model, 113,
206–207
varying coeﬃcient model, 34
exogeneity, see covariates
exponential family, 20
extrapolation factorization, 166, 216,
220
factored likelihood, see ignorability
follow-up time, 89
full data, 85, 86
deﬁnition, 18
in schizophrenia trial, 86
vs. full-data response, 86
vs. observed data, 86
full-data
model, 89
posterior, 98

300
Index
prior, 99
response, 86
for temporally aligned data, 87
for temporally misaligned data, 87
response model, 90
Gaussian process
applied to HERS, 159–162
generalized additive models, 32–34
generalized linear models
cross-sectional data, 19–20
growth hormone trial
analyses
completers only, 72–75
ignorable, 145–149
nonignorable, 234–248
description, 3–5
HIV Epidemiology Research Study
(HERS)
analyses, 159–162
description, 8–10
hypothesis testing, 43
ignorability, 93, 99–103
and data augmentation, 121–124
and factored likelihood, 101–103
covariance mis-speciﬁcation, 116–121
deﬁnition, 99
extrapolation of missing data under,
101
illustration using bivariate response,
100–101
in bivariate normal model, 103
in marginalized transition model, 104
interpretation of, 100, 102–103
intention to treat, 6
interior family constraints, see mixture
model
joint models, 144
applied to CTQ II, 155–159
for using auxiliary variables, 134–138
likelihood, 39–42
information matrix, 41, 118–121
score function, 41
local inﬂuence, 218
logistic random eﬀects model, see
random eﬀects model
loss function, 42, 65–67
MAR, see missing at random
marginal models, see directly speciﬁed
models
marginalized transition model, 28–30
applied to CTQ I
completers only, 79–84
ignorable, 151–155
under ignorability, 104
Markov chain Monte Carlo, 51–62
convergence, 58–60
data augmentation, 55–58, 121–124
Gibbs sampler, 52–54
problems with improper priors, 62
Metropolis-Hastings, 54–55
mixing, 58–60
parameter expansion, 70
Rao-Blackwellization, 70
slice sampling, 70
MCAR, see missing completely at
random
MCMC, see Markov chain Monte Carlo
missing at random, 1, 91
and nonignorability, 105
applied to dropout, 95–98
with covariates, 98
as point mass prior, 111, 223
auxiliary variables, 95, 124, 134–138
illustration, 95
deﬁnition, 93
example using bivariate normal
model, 103
illustration with bivariate response,
93
in mixture of normals, 105
under monotone dropout, 96–97
missing completely at random, 91
deﬁnition, 92
with covariates, 92
missing data mechanism, 90

Index
301
deﬁnition, 91
in mixture model, 109
in selection model, 107
in shared parameter model, 112
under non-future dependence, 173
missing not at random, 1, 91
deﬁnition, 94
depending on random coeﬃcients,
112
illustration with bivariate response,
94
in mixture models, 181–206
in selection models, 167–181
mixture of bivariate normals with,
110
mixture model
applied to growth hormone trial,
234–248
applied to OASIS study, 248–261
covariate eﬀects in, 203–206
for binary data, 194–198
hazard of dropout in, 109, 189
identiﬁcation, 182–188
via extrapolation, 187–188
via interior family constraints, 185,
190–192
via MAR constraints, 183–185
via non-future dependence
restrictions, 109, 186
incorporation of informative priors,
110, 222–224
applied to growth hormone trial,
234–248
applied to OASIS study, 248–261
marginalized, 215
pros and cons, 109, 112
sensitivity analysis for, 110, 190,
224–226
binary response, 196, 198
continuous response, 190, 192, 194
vs. selection models, 111, 202–203
with continuous time dropout
applied to Pediatric AIDS Trial,
261–267
with continuous-time dropout,
198–201
with discrete-time dropout, 188–198
MNAR, see missing not at random
model ﬁt, 71, 215
posterior predictive checks, 67–68, 71
applied to CTQ II, 157–158
applied to growth hormone trial,
73–74, 146–147
under ignorability, 141–143
under nonignorability, 213–214
model selection
DIC, 63–65
applied to CTQ I, 153–154
applied to CTQ II, 157–158
applied to growth hormone trial,
73–74, 146–147
applied to HERS, 161
applied to schizophrenia trial, 150
under ignorability, 138–141
under nonignorability, 209–213
posterior predictive loss, 65–67
applied to CTQ I, 81–82
monotone missing data pattern, 89
multiple imputation, 123–124
multivariate normal model
applied to CTQ II, 155–159
applied to growth hormone trial
completers only, 72–75
ignorable, 145–149
nonignorable, 234–248
temporally aligned response, 26–27
temporally misaligned response,
27–28
multivariate probit model, 30–31, 56, 64
applied to CTQ II, 155–159
covariates in correlation matrix,
133–134
in joint models, 137–138
interpretation of parameters, 30
non-future dependence restrictions, see
mixture model
nonignorability, see missing not at
random
nonignorability index, 217
nonparametric Bayes, 68–69
applied to Pediatric AIDS Trial,
261–267

302
Index
normal random eﬀects model, see
random eﬀects model
OASIS study
analyses, 248–261
description, 10–12
observed data, 86
likelihood, 99
under ignorability, 100
posterior, 98
prior under ignorability, 100
orthogonal parameters
and model mis-speciﬁcation, 119–121
deﬁnition of, 118
parameter identiﬁcation, 50–51, 166,
219–222
in mixture models, 110–111, 224–226
in selection models, 168–171, 226–231
pattern mixture model, see mixture
model
Pediatric AIDS Trial (ACTG 128)
analyses, 261–267
description, 12–14
penalized splines, 32, 71
applied to HERS, 159–162
applied to Pediatric AIDS Trial,
261–267
basis function for, 33
conjugate prior, 45
speciﬁcation of, 71
posterior distribution, 42–43
deﬁnition of, 42
large sample behavior, 117–121
posterior predictive checks, see model
ﬁt
posterior predictive loss, see model
selection
prior distribution, 43–50
conditionally conjugate, 69
conjugate, 43–45
Dirichlet process prior, 71
elicitation of, 69–70
applied to OASIS study, 248–261
identiﬁability, 50–51
improper, 47, 62, 69
informative, 49–50, 69–70
applied to OASIS study, 248–261
Jeﬀreys’, 46
noninformative, 46, 49
Polya tree prior, 71
shrinkage, 69
speciﬁcation under MNAR, 222–224,
227–230
probit model, see multivariate probit
model
random coeﬃcients selection model, 113
random eﬀects models
as random coeﬃcient model, 22
basic speciﬁcation, 21
continuous responses, 21–23
discrete responses, 23–25
logistic-normal, 23
applied to CTQ I, 79–84
normal-normal, 22
applied to schizophrenia trial,
75–79, 149–151
regression splines, see penalized splines
reweighting, 61
to compute DIC
under ignorability, 140–141
under nonignorability, 209–213
schizophrenia trial
analyses
completers only, 75–79
ignorable, 149–151
description, 1–3
selection model
advantages of, 107
applied to OASIS study, 248–261
disadvantages of, 107
example using bivariate normal
distribution, 107
formulation of sensitivity analysis,
226–231
informative priors, 226–231
observed data likelihood, 108
parametric, 167–181
binary responses, 175
continuous responses, 171–175

Index
303
Heckman selection model, 107,
171–173
posterior sampling, 180–181
problems with sensitivity analysis,
175–176
pros and cons, 181
semiparametric, 176–180
prior speciﬁcation in, 227–230
sensitivity analysis, 230–231
speciﬁcation of missing data
mechanism, 173–174
under MNAR, 107
sensitivity analysis
and the extrapolation distribution,
166, 216
in growth hormone trial, 234–248
in mixture model
binary response, 248–261
continuous response, 234–248
in OASIS study, 248–261
local vs. global, 217–218
principles of, 219
problems with parametric selection
models, 175–176
semiparametric selection models,
176–180, 230–231
shared parameter model, 206–209
hazard of dropout in, 208
pros and cons, 112–113, 207–209
time-varying covariates, see covariates
varying coeﬃcient model, 14, 32, 34,
199
applied to Pediatric AIDS Trial,
261–267
example, 199–200

