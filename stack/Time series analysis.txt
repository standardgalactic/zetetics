

TIME SERIES ANALYSIS

WILEY SERIES IN PROBABILITY AND STATISTICS
Established by WALTER A. SHEWHART and SAMUEL S. WILKS
Editors: David J. Balding, Noel A. C. Cressie, Garrett M. Fitzmaurice,
Geof H. Givens, Harvey Goldstein, Geert Molenberghs, David W. Scott, 
Adrian F. M. Smith, Ruey S. Tsay, Sanford Weisberg
Editors Emeriti: J. Stuart Hunter, Iain M. Johnstone, Joseph B. Kadane, 
Jozef L. Teugels
A complete list of the titles in this series appears at the end of this volume.

TIME SERIES ANALYSIS
Wilfredo Palma
Pontiﬁcia Universidad Cat´olica de Chile

Copyright c⃝2016 by John Wiley & Sons, Inc. All rights reserved
Published by John Wiley & Sons, Inc., Hoboken, New Jersey
Published simultaneously in Canada
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any
form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise,
except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without
either the prior written permission of the Publisher, or authorization through payment of the
appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers,
MA 01923, (978) 750-8400, fax (978) 750-4470, or on the web at www.copyright.com. Requests to
the Publisher for permission should be addressed to the Permissions Department, John Wiley &
Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, or online at
http://www.wiley.com/go/permission.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best
eﬀorts in preparing this book, they make no representations or warranties with respect to the
accuracy or completeness of the contents of this book and speciﬁcally disclaim any implied
warranties of merchantability or ﬁtness for a particular purpose. No warranty may be created or
extended by sales representatives or written sales materials. The advice and strategies contained
herein may not be suitable for your situation. You should consult with a professional where
appropriate. Neither the publisher nor author shall be liable for any loss of proﬁt or any other
commercial damages, including but not limited to special, incidental, consequential, or other
damages.
For general information on our other products and services or for technical support, please contact
our Customer Care Department within the United States at (800) 762-2974, outside the United
States at (317) 572-3993 or fax (317) 572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print
may not be available in electronic formats. For more information about Wiley products, visit our
web site at www.wiley.com.
Library of Congress Cataloging-in-Publication Data:
Palma, Wilfredo, 1963-
Time series analysis / Wilfredo Palma, Ponticia Universidad Cat´olica de Chile.
pages cm
Includes bibliographical references and indexes.
ISBN 978-1-118-63432-5 (cloth)
1. Time-series analysis. I. Title.
QA280.P354 2016
519.5′5–dc23
2015024282
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

CONTENTS
Preface
xiii
Acknowledgments
xvii
Acronyms
xix
1
Introduction
1
1.1
Time Series Data
2
1.1.1
Financial Data
2
1.1.2
Economic Data
6
1.1.3
Hydrological Data
6
1.1.4
Air Pollution
7
1.1.5
Transportation Data
9
1.1.6
Biomedical Time Series
9
1.1.7
Sociological Data
10
1.1.8
Energy Data
11
1.1.9
Climatic Data
12
1.2
Random Variables and Statistical Modeling
16
1.3
Discrete-Time Models
22
v
About the Companion Website
xxv

vi
CONTENTS
1.4
Serial Dependence
22
1.5
Nonstationarity
25
1.6
Whiteness Testing
32
1.7
Parametric and Nonparametric Modeling
36
1.8
Forecasting
38
1.9
Time Series Modeling
38
1.10 Bibliographic Notes
39
Problems
40
2
Linear Processes
43
2.1
Deﬁnition
44
2.2
Stationarity
44
2.3
Invertibility
46
2.4
Causality
46
2.5
Representations of Linear Processes
46
2.5.1
Wold Decomposition
47
2.5.2
Autoregressive Representation
48
2.5.3
State Space Systems
48
2.6
Weak and Strong Dependence
49
2.7
ARMA Models
51
2.7.1
Invertibility of ARMA Processes
52
2.7.2
Simulated ARMA Processes
52
2.8
Autocovariance Function
56
2.9
ACF and Partial ACF Functions
57
2.9.1
Sample ACF
60
2.9.2
Partial ACF
63
2.10 ARFIMA Processes
64
2.10.1 Long-Memory Processes
64
2.10.2 Linear Representations
65
2.10.3 Autocovariance Function
66
2.10.4 Sample Mean
67
2.10.5 Partial Autocorrelations
67
2.10.6 Illustrations
68
2.11 Fractional Gaussian Noise
71
2.11.1 Sample Mean
72
2.12 Bibliographic Notes
72
Problems
72

CONTENTS
vii
3
State Space Models
89
3.1
Introduction
90
3.2
Linear Dynamical Systems
92
3.2.1
Stability
92
3.2.2
Hankel Matrix
93
3.2.3
Observability
94
3.2.4
Controllability
94
3.2.5
Minimality
95
3.3
State space Modeling of Linear Processes
96
3.3.1
State Space Form to Wold Decomposition
96
3.3.2
Wold Decomposition to State Space Form
96
3.3.3
Hankel Matrix to State Space Form
96
3.4
State Estimation
97
3.4.1
State Predictor
98
3.4.2
State Filter
98
3.4.3
State Smoother
99
3.4.4
Missing Values
99
3.4.5
Additive Noise
105
3.4.6
Structural Models
110
3.4.7
Estimation of Future States
111
3.5
Exogenous Variables
113
3.6
Bibliographic Notes
114
Problems
114
4
Spectral Analysis
121
4.1
Time and Frequency Domains
122
4.2
Linear Filters
122
4.3
Spectral Density
123
4.4
Periodogram
125
4.5
Smoothed Periodogram
128
4.6
Examples
130
4.7
Wavelets
136
4.8
Spectral Representation
138
4.9
Time-Varying Spectrum
140
4.10 Bibliographic Notes
145
Problems
145

viii
CONTENTS
5
Estimation Methods
151
5.1
Model Building
152
5.2
Parsimony
152
5.3
Akaike and Schwartz Information Criteria
153
5.4
Estimation of the Mean
153
5.5
Estimation of Autocovariances
154
5.6
Moment Estimation
155
5.7
Maximum-Likelihood Estimation
156
5.7.1
Cholesky Decomposition Method
156
5.7.2
Durbin-Levinson Algorithm
157
5.8
Whittle Estimation
157
5.9
State Space Estimation
160
5.10 Estimation of Long-Memory Processes
161
5.10.1 Autoregressive Approximations
162
5.10.2 Haslett-Raftery Method
162
5.10.3 A State Space Method
164
5.10.4 Moving-Average Approximations
165
5.10.5 Semiparametric Approach
168
5.10.6 Periodogram Regression
169
5.10.7 Rescaled Range Method
170
5.10.8 Variance Plots
171
5.10.9 Detrended Fluctuation Analysis
171
5.10.10A Wavelet-Based Method
174
5.10.11Computation of Autocovariances
177
5.11 Numerical Experiments
178
5.12 Bayesian Estimation
180
5.12.1 Markov Chain Monte Carlo Methods
181
5.12.2 Metropolis-Hastings Algorithm
181
5.12.3 Gibbs Sampler
182
5.13 Statistical Inference
184
5.14 Illustrations
189
5.15 Bibliographic Notes
193
Problems
194
6
Nonlinear Time Series
209
6.1
Introduction
210
6.2
Testing for Linearity
211
6.3
Heteroskedastic Data
212

CONTENTS
ix
6.4
ARCH Models
213
6.5
GARCH Models
216
6.6
ARFIMA-GARCH Models
218
6.7
ARCH(∞) Models
220
6.8
APARCH Models
222
6.9
Stochastic Volatility
222
6.10 Numerical Experiments
223
6.11 Data Applications
225
6.11.1 SP500 Data
225
6.11.2 Gold Data
226
6.11.3 Copper Data
231
6.12 Value at Risk
236
6.13 Autocorrelation of Squares
241
6.13.1 Squares of Gaussian Processes
241
6.13.2 Autocorrelation of Squares
242
6.13.3 Illustrations
243
6.14 Threshold autoregressive models
247
6.15 Bibliographic Notes
252
Problems
253
7
Prediction
267
7.1
Optimal Prediction
268
7.2
One-Step Ahead Predictors
268
7.2.1
Inﬁnite Past
268
7.2.2
Finite Past
269
7.2.3
Innovations Algorithm
269
7.2.4
An Approximate Predictor
273
7.3
Multistep Ahead Predictors
275
7.3.1
Inﬁnite Past
275
7.3.2
Finite Past
275
7.4
Heteroskedastic Models
276
7.4.1
Prediction of Returns
278
7.4.2
Prediction of Volatility
278
7.5
Prediction Bands
281
7.6
Data Application
287
7.7
Bibliographic Notes
289
Problems
289

x
CONTENTS
8
Nonstationary Processes
295
8.1
Introduction
296
8.1.1
Deterministic Trends
296
8.1.2
Stochastic Trends
296
8.2
Unit Root Testing
297
8.3
ARIMA Processes
298
8.4
Locally Stationary Processes
301
8.4.1
State-Space Representations
308
8.4.2
Whittle Estimation
310
8.4.3
State Space Estimation
311
8.4.4
Asymptotic Variance
316
8.4.5
Monte Carlo Experiments
319
8.4.6
Data Application
320
8.5
Structural Breaks
326
8.6
Bibliographic Notes
331
Problems
332
9
Seasonality
337
9.1
SARIMA Models
338
9.1.1
Spectral Density
341
9.1.2
Several Seasonal Components
343
9.1.3
Estimation
343
9.1.4
Estimator Performance
343
9.1.5
Heating Degree Day Data Application
345
9.2
SARFIMA Models
351
9.3
GARMA Models
353
9.4
Calculation of the Asymptotic Variance
355
9.5
Autocovariance Function
355
9.6
Monte Carlo Studies
359
9.7
Illustration
362
9.8
Bibliographic Notes
364
Problems
365
10 Time Series Regression
369
10.1 Motivation
370
10.2 Deﬁnitions
373
10.2.1 Grenander Conditions
373

CONTENTS
xi
10.3 Properties of the LSE
375
10.3.1 Asymptotic Variance
375
10.3.2 Asymptotic Normality
376
10.4 Properties of the BLUE
376
10.4.1 Eﬃciency of the LSE Relative to the BLUE
376
10.5 Estimation of the Mean
378
10.5.1 Asymptotic Variance
379
10.5.2 Relative Eﬃciency
381
10.6 Polynomial Trend
382
10.6.1 Consistency
384
10.6.2 Asymptotic Variance
384
10.6.3 Relative Eﬃciency
385
10.7 Harmonic Regression
385
10.7.1 Consistency
386
10.7.2 Asymptotic Variance
387
10.7.3 Eﬃciency
387
10.8 Illustration: Air Pollution Data
388
10.9 Bibliographic Notes
392
Problems
392
11 Missing Values and Outliers
399
11.1 Introduction
400
11.2 Likelihood Function with Missing Values
401
11.2.1 Integration
402
11.2.2 Maximization
402
11.2.3 Calculation of the Likelihood Function
404
11.2.4 Kalman Filter with Missing Observations
404
11.3 Eﬀects of Missing Values on ML Estimates
405
11.3.1 Monte Carlo Experiments
407
11.4 Eﬀects of Missing Values on Prediction
407
11.5 Interpolation of Missing Data
410
11.6 Spectral Estimation with Missing Values
418
11.7 Outliers and Intervention Analysis
421
11.7.1 Methodology
424
11.7.2 Known time of the event
424
11.7.3 Unknown time of the event
426
11.8 Bibliographic Notes
434
Problems
435

xii
CONTENTS
12 Non-Gaussian Time Series
441
12.1 Data Driven Models
442
12.1.1 INAR Processes
442
12.1.2 Conditional Distribution Models
446
12.2 Parameter Driven Models
452
12.3 Estimation
453
12.3.1 Monte Carlo Experiments
461
12.3.2 Diagnostics
465
12.3.3
Prediction
465
12.4 Data Illustrations
466
12.4.1 IBM Trading Volume
466
12.4.2 Glacial Varves
469
12.4.3 Voting Intentions
473
12.5 Zero-Inﬂated Models
477
12.6 Bibliographic Notes
483
Problems
483
Appendix A: Complements
487
A.1
Projection Theorem
488
A.2
Wold Decomposition
490
A.2.1
Singularity and Regularity
490
A.2.2
Wold Decomposition
490
A.2.3
Causality
492
A.2.4
Invertibility
493
A.2.5
Best Linear Predictor
493
A.2.6
Szeg¨o-Kolmogorov Formula
494
A.2.7
Ergodicity
494
A.2.8
Fractional Brownian Motion
496
A.3
Bibliographic Notes
497
Appendix B: Solutions to Selected Problems
499
Appendix C: Data and Codes
557

CONTENTS
xiii
References
559
Topic Index
573
Author Index
577


PREFACE
This book aims to provide an overview of time series analysis to a wide au-
dience of students, practitioners, and scientists from diﬀerent ﬁelds.
It is
intended as an introductory text on the vast time series subject.
Conse-
quently, it focuses on methodologies and techniques rather than theoretical
results. This book strives to provide a working knowledge of the practical
applications of time series methods. However, it does not attempt to cover
all of the relevant topics in this ﬁeld.
Consistent with this objective, the ﬁrst chapter reviews the main features
of a number of real-life time series arising in diﬀerent ﬁelds, including ﬁnance,
hydrology, meteorology, sociology, and politics, among others. At the same
time, this chapter refreshes some basic knowledge on statistical distributions.
Furthermore, Chapter 1 provides an overview of the time series modeling
fundamentals by taking a ﬁrst look to concepts such as stationarity, non-
stationarity, parametric and nonparametric approaches, and whiteness tests.
Further readings are suggested in a bibliographical notes section. This chapter
ends with a number of proposed exercises.
Chapter 2 addresses linear processes, one of the fundamental concepts of
time series analysis. It reviews the diﬀerent representations of linear time se-
ries and discusses essential topics such as stationarity, invertibility, and causal-
xv

xvi
PREFACE
ity. One interesting feature of this chapter is that it covers both short and
long-memory linear processes.
State space models are discussed in Chapter 3. Apart from being another
representation of linear processes, state space systems provide several practical
tools for estimating, smoothing, and predicting time series models. Moreover,
they are useful for handling nonstationarity and missing data problems. In
particular, we discuss applications of state space techniques to parameter
estimation in Chapter 4, to nonstationary processes in Chapter 8, to seasonal
models in Chapter 9, and to missing values in Chapter 11.
As discussed in Chapter 4, time series analysis can be carried out from
a time-domain or from a spectral domain. However, in practice one usually
combine both approaches. For example, spectral analysis is fundamental for
modeling time series exhibiting seasonal patterns. On the other hand, Chap-
ter 5 provides an overview of the realm of methodologies for estimating time
series models. It begins with some essential notions about specifying appro-
priate statistical models, including the concepts of parsimony and information
criteria. Afterwards, it proceeds to discuss several estimation techniques such
as maximum likelihood, Whittle approach, Bayesian estimation, along with
an extensive list of speciﬁc methods developed for long-memory models. This
chapter also reviews techniques for carrying out statistical inferences about
the ﬁtted models. A number of simulations and practical applications com-
plete this chapter.
Nonlinear time series are addressed in Chapter 6. This is an important
subject that provides tools for modeling time series data which do not ﬁt into
the linear processes category discussed in Chapter 2. For example, most of
ﬁnancial time series are better described by heteroskedastic nonlinear models.
This chapter also discusses techniques for assessing ﬁnancial risk and modeling
time series with complex structure. For instance, these data can be modeled
via threshold processes that allow for the treatment of time series undergoing
regime changes.
The fundamental topic of forecasting time series is discussed in Chapter 7.
Optimal prediction with ﬁnite and inﬁnite past is reviewed in the context of
linear and nonlinear processes. Calculating one-step and multistep predictors
and procedures for establishing prediction bands are described.
Chapter 8 examines the subject of nonstationary time series models. Since
many real-life time series do not exhibit stationary behavior, this chapter
discusses methods for handling nonstationarity, including autoregressive in-
tegrated moving-average models and locally stationary processes. While the
former assume that the series results from the integration of an stationary
process, the later assume that the parameters of the model change smoothly
over time. In order to account for abrupt changes, this chapter also discusses
methods for treating structural breaks.
Seasonal patterns are present in time series data as diverse as Internet
traﬃc, sales revenues, and transportation. Methods for analyzing these time

PREFACE
xvii
series are described in Chapter 10, including models based on seasonal dif-
ferentiation. This chapter illustrates the ﬁnite sample performance of these
techniques via Monte Carlo simulations and a real-life data application.
Time series regression methods are reviewed in Chapter 9. These tech-
niques allow for the modeling of time series data aﬀected by some underlying
trend or exogenous variables. For example, these trends can be described by a
polynomial structure. Additionally, harmonic regression can be a useful tool
for handling seasonality.
Data gaps and outliers are frequent problems in time series analysis. These
topics are reviewed in Chapter 11. The eﬀects of incomplete data on parameter
estimates and predictors is studied in this chapter. The problem of deﬁning an
appropriate likelihood function in the context of incomplete data is discussed
along with state space techniques for obtaining estimates. On the other hand,
methods to account for outliers are also addressed in this chapter.
Most time series models assume that the observations are normally dis-
tributed. In practice, however, many real-life time series do not ﬁt this as-
sumption.
For example, the time series may correspond to count data or
positive observations. Consequently, Chapter 12 provides an overview of sev-
eral methods for estimating and predicting non-normal time series.
Some basic knowledge of calculus is required for understanding most meth-
ods discussed in this book.
Apart from this, the text intends to be self-
contained in terms of other more advanced concepts.
In particular, Ap-
pendix A provides some speciﬁc technical details about fundamental concepts
in time series analysis. On the other hand, Appendix B contains solutions
to some of the proposed problems. Finally, Appendix C provides informa-
tion about the data and the computing codes used in this book. It is worth
noting that similarly to Chapter 1, every chapter of this book ends with a
bibliographical notes section and a list of proposed problems. Supplementary
material for this book can be also found on the Book Companion Site through
the books page on wiley.com.
W. PALMA
Santiago, Chile
January, 2015


ACKNOWLEDGMENTS
I wish to express my gratitude to Steve Quigley for encouraging me to write
this book and for many valuable comments on its contents. I would like to
thank Jon Gurstelle and Sari Friedman, and the editorial staﬀat John Wiley
& Sons for their continuous support and for making the publication of this
book possible. I am also indebted to many coauthors and colleagues, some
of the results described in this text reﬂect part of that fruitful collaboration.
I am also grateful of the support from the Department of Statistics and the
Faculty of Mathematics at the Pontiﬁcia Universidad Cat´olica de Chile. Sev-
eral chapters of this book evolved from lecture notes for undergraduate and
graduate courses on time series analysis. I would like to thank many students
for constructive remarks on the text and for trying out the proposed exer-
cises. The ﬁnancial support by the Ministry of Economy, Development, and
Tourism’s Millennium Science Initiative through grant IC120009, awarded to
The Millennium Institute of Astrophysics, MAS, and from Fondecyt Grant
1120758 are gratefully acknowledged.
W.P.
xix


ACRONYMS
ACF
autocorrelation function
AIC
Akaike’s information criterion
ANOVA
analysis of variance
APARCH
asymmetric power autoregressive conditionally
heteroskedastic
AO
additive outlier
AR
autoregressive
ARCH
autoregressive conditionally heteroskedastic
ARFIMA
autoregressive fractionally integrated moving-average
ARMA
autoregressive moving-average
BLUE
best linear unbiased estimator
DFA
detrended ﬂuctuation analysis
DWT
discrete wavelet transform
EGARCH
exponential generalized autoregressive conditionally
heteroskedastic
fBm
fractional Brownian motion
xxi

xxii
Acronyms
FFT
fast Fourier transform
fGn
fractional Gaussian noise
FI
fractionally integrated
FIGARCH
fractionally integrated generalized autoregressive
conditionally heteroskedastic
FIEGARCH
fractionally integrated exponential generalized
autoregressive conditionally heteroskedastic
FN
fractional noise
GARCH
generalized autoregressive conditionally heteroskedastic
GARMA
Gegenbauer autoregressive moving-average
HDD
heating degree day
HTTP
hyper text transfer protocol
IG
inverse gamma distribution
i.i.d.
independent identically distributed
IM
intermediate memory
IO
innovative outlier
LM
long memory
LMGARCH
long-memory generalized autoregressive conditionally
heteroskedastic
LMSV
long-memory stochastic volatility
LS
level shift
LSARFIMA
locally stationary autoregressive fractionally integrated
moving-average
LSE
least squares estimator
MA
moving-average
MCMC
Markov chain Monte Carlo algorithm
ML
maximum likelihood
MLE
maximum-likelihood estimator
MSE
mean-squared error
MSPE
mean-squared prediction error
PACF
partial autocorrelation function
PM
particulate matterl
QMLE
quasi-maximum-likelihood estimator
R/S
rescaled range statistics
SARFIMA
seasonal autoregressive fractionally integrated
moving-average

Acronyms
xxiii
SD
standard deviation
SETAR
self-exciting threshold autoregressive
SM
short memory
SV
stochastic volatility
TAR
threshold autoregressive
TC
temporary change
VaR
value at risk
WN
white noise
ZIM
zero-inﬂated model
ZINB
zero-inﬂated Negative Binomial model


ABOUT THE COMPANION WEBSITE
This book is accompanied by both Instructor and Student companion websites,
which are available via the book’s page on www.wiley.com.
The Instructor and Student websites includes:
•
Data Sets
•
R script
v
xx


CHAPTER 1
INTRODUCTION
A time series is a collection of observations taken sequentially in time. The
nature of these observations can be as diverse as numbers, labels, colors,
and many others. On the other hand, the times at which the observations
were taken can be regularly or irregularly spaced.
Moreover, time can be
continuous or discrete. In this text, we focus primarily on describing methods
for handling numeric time series observed at regular intervals of time. Note,
however, that many nonnumeric data can be readily transformed to numeric.
For instance, data concerning an election between candidate A and candidate
B can be described by a numeric variable taking the value 0 for candidate A
and 1 for candidate B. However, data observed at irregular time intervals are
more diﬃcult to handle. In this case, one may approximate the actual time to
the closest integer value and still use the methodologies for handling regularly
spaced series. If this approach does not provide adequate results, there are a
number of more advanced techniques to treat those types of data. Another
common problem in time series analysis is missing observations. In this case,
the collected data display irregularly spaced observation times.
There are
special techniques for handling this problem and some of them are discussed
in Chapter 11.
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
1

2
INTRODUCTION
This introductory chapter presents a number of real-life time series data
examples as well as provides a general overview of some essential concepts
of the statistical analysis of time series, such as random variable, stochastic
process, probability distribution and autocorrelation, among others. These
notions are fundamental for the statistical modeling of serially dependent
data.
Since this text attempts to reach a large audience interested in time series
analysis, many of the more technical concepts are explained in a rigorous
but simple manner. Readers interested in extending their knowledge of some
particular concept in time series analysis will ﬁnd an extensive list of references
and a selected bibliographical discussion at the end of each chapter.
1.1
TIME SERIES DATA
Let us denote by {yt} a time series where t denotes the time at which the
observation was taken. Usually, t ∈Z, where Z = {. . . , −2, −1, 0, 1, 2, . . . } is
the set of positive and negative integer values. In practice, however, only a
ﬁnite stretch of data is available. In such situations, we can write the time
series as {y1, y2, . . . , yn}. A time series {yt} corresponds to a stochastic process
which in turn is composed of random variables observed across time. Both
concepts are explained in detail later in this chapter.
Several examples of real-life time series data are presented in the follow-
ing subsections.
These data illustrations come from ﬁelds as diverse as,
ﬁnance, economic, sociology, energy, medicine, climatology, and transport,
among many others. Apart from exhibiting the time series, we describe their
main features and some basic data transformations that help uncovering these
characteristics.
1.1.1
Financial Data
Finance is a ﬁeld where time series arises naturally from the evolution of
indexes and prices.
In what follows, we present two basic examples, the
evolution of a well-known stock index and its volume of stock transactions.
Standard & Poor’s Stock Index. Figure 1.1 shows the logarithm of the S&P500
daily stock index for the period from January 1950 to January 2014. Note
that this index seems to increase with time, but there are some downward
periods commonly denoted as bear markets. In order to study these indices,
it is customary in ﬁnance to consider the logarithm return, which is deﬁned
as
rt = log
Pt
Pt−1
= log Pt −log Pt−1,
where Pt denotes the price or the index value at time t. These returns are
displayed in Figure 1.2. Observe the great drop in returns experienced on
October 1987 and the abrupt changes or great volatility during 2009.

TIME SERIES DATA
3
Time
Log index
1950
1960
1970
1980
1990
2000
2010
3
4
5
6
7
Figure 1.1
S&P500 daily stock log index, January 1950 to January 2014.
Time
Log returns
1950
1960
1970
1980
1990
2000
2010
-0.20
-0.15
-0.10
-0.05
0.00
0.05
0.10
Figure 1.2
S&P500 daily log returns, January 1950 to January 2014.

4
INTRODUCTION
Time
Squared log returns
1950
1960
1970
1980
1990
2000
2010
0.00
0.01
0.02
0.03
0.04
0.05
Figure 1.3
S&P500 daily square log returns, January 1950 to January 2014.
Another look at the volatility is shown in Figure 1.3 where the squared
returns, r2
t , are plotted. From this graph, the high volatility of this stock
index is evident during these periods.
Financial time series possess speciﬁc features, such as those indicated above.
Consequently, Chapter 6 describes methodologies for handling this type of
data. These time series can be analyzed by means of the so-called condition-
ally heteroskedastic processes or stochastic volatility models, among others.
Volume of Transactions. As another example of ﬁnancial data, the daily vol-
ume of transactions of the S&P500 stocks is displayed in Figure 1.4. Observe
that this series exhibits an upward trend up to 2009.
On the other hand, Figure 1.5 depicts a logarithm transformation of the
above time series. Note that the variance of the data across time is now more
stabilized, emerging a seemingly overall upward trend, excepting the values
after 2009 and some other periods.
These transaction volume data can be considered as an example of a non-
Gaussian time series. In particular, these observations are positive counts.
Speciﬁc methods for modeling and predicting non-Gaussian data are described
in Chapter 12.

TIME SERIES DATA
5
Time
Volume
1950
1960
1970
1980
1990
2000
2010
0e+00
2e+09
4e+09
6e+09
8e+09
1e+10
Figure 1.4
S&P500 daily volume of transactions, January 1950 to January 2014.
Time
Log volume
1950
1960
1970
1980
1990
2000
2010
14
16
18
20
22
Figure 1.5
S&P500 daily log volume of transactions, January 1950 to January
2014.

6
INTRODUCTION
1.1.2
Economic Data
Figure 1.6(a) exhibits the monthly US employment in the arts, entertainment
and recreation section for the January 1990 to December 2012, measured in
thousands of persons. On the other hand, Figure 1.6(b) shows the logarithm
transformation of these data. Notice that this data transformation seems to
stabilize the variance of the series across time. On both panels, however, a
seasonal pattern and an upper trend are evident.
(a)
Time
Thousands of persons
1990
1995
2000
2005
2010
1000
1400
1800
2200
(b)
Time
Log thousands of persons
1990
1995
2000
2005
2010
7.0
7.2
7.4
7.6
Figure 1.6
US employment arts, entertainment, and recreation, January 1990 to
December 2012.
1.1.3
Hydrological Data
In hydrology, time series data is usually related to the collection of river ﬂows
observations though the years. For example, the yearly minimum water levels
of the Nile river measured at the Roda gauge is a well-known time series
exhibiting high levels of serial dependency. These measurements, available
from Statlib, www.stat.cmu.edu, are displayed in Figure 1.7 spanning a time
period from 622 A.D. to 1921 A.D.
Notice that there are several blocks of seemingly repeated values. That
is, consecutive years having exactly the same minimum water level. Since
the observations are speciﬁed by four digits, these repetitions are probably
the result of a lack of new information. The analysis of this time series data

TIME SERIES DATA
7
Year
Nile river level
600
800
1000
1200
1400
1600
1800
9
10
11
12
13
14
15
Figure 1.7
Nile river yearly minimum level at the Roda gauge, from 622 A.D. to
1921 A.D.
also indicates that the ﬁrst 100 observations seem to have a diﬀerent serial
dependence structure, suﬀering a structural break phenomenon.
A detailed analysis of this historically important hydrological time series is
proved in Chapter 8, where the changes in the serial dependence structure of
these observations are modeled. The analysis of these hydrological data was
crucial in the formal study of the so-called long-memory processes reviewed
in Chapter 2.
1.1.4
Air Pollution
Figure 1.8 exhibits a daily index that measures the particulate matter of di-
ameter less than 2.5 µ in Santiago, Chile, for the period 1989–1999, commonly
referred to as PM2.5. A log-transformed data is shown in Figure 1.9.
These measurements indicate the level of air pollution in certain city or
region. Note the seasonal behavior of this series, due to the eﬀects of climate
conditions across the year. In winter, the PM2.5 level increases dramatically.
On the other hand, it appears that there is downward trend in the series,
indicating an improvement of the air quality during that period. In order
to stabilize the variance exhibited by this data, a logarithmic transformation
has been made and the resulting series is shown in Figure 1.9. A possible
downward trend is now more clear in the transformed data.

8
INTRODUCTION
Time
PM2.5 daily level
1990
1992
1994
1996
1998
2000
0
100
200
300
400
Figure 1.8
Air pollution data: daily PM2.5 measurements at Santiago, Chile,
1989 - 1999.
Time
Log PM2.5 daily level
1990
1992
1994
1996
1998
2000
2
3
4
5
6
Figure 1.9
Air pollution data: log daily PM2.5 measurements at Santiago, Chile,
1989 - 1999.

TIME SERIES DATA
9
1.1.5
Transportation Data
Figure 1.10 shows the number of monthly passenger enplanements for the
period from January 2004 to December 2013. These observations correspond
to the number of passenger boarding an airplane in the United States in a
given month. Note the seasonal behavior of this series derived from the annual
cycle of winter and summer seasons. Besides, it seems that there was a drop
on passenger enplanements around 2009, revealing a plausible eﬀect of the
ﬁnancial crisis of that year. In this situation, it is possible that the process
was aﬀected by a structural break or structural change. Methodologies for
handling these situations will be discussed in Chapter 8.
Time
Passenger enplanements
2004
2006
2008
2010
2012
50000
55000
60000
65000
70000
Figure 1.10
Passenger enplanements from January 2004 to January 2013.
1.1.6
Biomedical Time Series
The annual global number of dengue cases during the period 1955 - 2008 is
depicted in Figure 1.11. In order to emphasize that these data correspond
to counts, the values are shown as bars. Note the increase of cases by the
end of this period, reaching very high values around the year 2000 and then
a sharp decay by the end of that decade. Apart from the analysis of the evo-
lution of diseases, there are several others applications of time series methods
to biomedical studies. Techniques for modeling count data is described in
Chapter 12.

10
INTRODUCTION
1960
1970
1980
1990
2000
2010
0e+00
2e+05
4e+05
6e+05
8e+05
1e+06
Time
Number of cases
Figure 1.11
Annual global number of dengue cases during the period 1955 - 2008.
1.1.7
Sociological Data
The results from a series of monthly United Kingdom voting intention sur-
veys for the period June 1984 to March 2012 are shown in Figure 1.12. For
simplicity, we have only plotted the vote intentions for the Conservative Party
and the Labor Party.
The heavy line indicates the Labor Party voting intention, while the dotted
line corresponds to the voting intention of the Conservative Party. Observe
the seemingly mirror eﬀect in the behavior of these two series given that
these two political parties historically concentrate a large percentage of vote
intentions.
Furthermore, for the period from 1993 to 2005, there is large distance
between the voting intentions of the two parties.
During this period, the
Labor Party shows a higher level of voting intention than the Conservative
Party. The opposite is true for the following period, from 2006 to 2010.
These time series are additional examples of cases where the data is not
necessarily Gaussian and speciﬁc methods must be developed for handling
them. Some of these technique are reviewed in Chapter 12, including, for
instance, the conditional distribution models.

TIME SERIES DATA
11
Time
Voting intentions
1985
1990
1995
2000
2005
2010
30
40
50
60
Figure 1.12
Monthly United Kingdom voting intentions, June 1984 to March
2012. Heavy line, Labor Party; dotted line: Conservative Party.
1.1.8
Energy Data
A time series consisting of monthly heating degree day (HDD) measurements
is shown in Figure 1.13. HDD values are indicative of the amount of energy
required to heat a building, and they result from from measurements of out-
side air temperature. The heating requirements for a particular structure at
a speciﬁc location are usually considered to be directly proportional to the
number of HDD at that location.
These series are important in the analysis of energy demand, and they can
be measured at diﬀerent speciﬁc locations such as cities or at a county level.
In this case, the data correspond to the global measurements for 27 Eu-
ropean Union countries from January 1980 to May 2010. As in other series
previously presented, this time series displays a clear seasonal behavior, ex-
pected from the diﬀerent heating requirements within a year.
This series displays a clear seasonal behavior and no upper or downward
trends are evident, as compared to the previous example of employment data.
In Chapter 9, we discuss methodologies for modeling this type of seasonal
time series and apply the techniques to the monthly HDD measurements.

12
INTRODUCTION
Time
Heating degree days
1980
1985
1990
1995
2000
2005
2010
0
100
200
300
400
500
600
700
Figure 1.13
Monthly heating degree days (HDDs) measurements for 27 countries
of the European Union, for the the period January 1980 to May 2010.
1.1.9
Climatic Data
Time series play an important role in the study of climate, allowing to model
actually observed values in the historical records or reconstructing unobserved
data. For example, paleoclimatic studies usually rely on the reconstruction
of climatic conditions by means of tree rings, mineral sediments, and other
related data.
Temperature Reconstruction. Figure 1.14 exhibits a reconstruction of an-
nual temperatures in north-central China from 1470 A.D. to 2002 A.D., based
on drought/ﬂood index and tree-ring records. Observe the apparent nonsta-
tionary behavior of the data, exhibiting heteroskedasticity and periods with
upward and downward trends. However, detailed studies must be conducted
on these data to distinguish whether there are deterministic trends or they
are just random or stochastic. On the other hand, reconstructions of pre-
cipitation conditions for the same region and period of time are displayed in
Figure 1.15. Note the increasing variability of the series.
Mineral Deposits. Figure 1.16 displays a 2,650-year centered time series
of speleothem cave deposit data. This series is composed by stalagmite layer
thickness observations taken at Shihua Cave, Beijing, China, from 665 B.C.
to 1985 A.D., see Appendix C for details about these data.
A logarithm
transformation of these data is exhibited in Figure 1.17.

TIME SERIES DATA
13
Year A.D.
Temperature index
1500
1600
1700
1800
1900
2000
-1.0
-0.5
0.0
0.5
1.0
Figure 1.14
North-Central China temperature, 1470 A.D. to 2002 A.D.
Year A.D.
Precipitation index
1500
1600
1700
1800
1900
2000
-1.0
-0.5
0.0
0.5
1.0
Figure 1.15
North-Central China precipitation, 1470 A.D. to 2002 A.D.

14
INTRODUCTION
Time
Mineral deposits
-500
0
500
1000
1500
2000
0
50
100
150
200
Figure 1.16
Speleothem cave deposit data.
Time
Log  mineral deposits
-500
0
500
1000
1500
2000
2
3
4
5
Figure 1.17
Log speleothem cave deposit data.

TIME SERIES DATA
15
Year
Varves series
-11800
-11700
-11600
-11500
-11400
-11300
-11200
0
50
100
150
Figure 1.18
Glacial varves time series data.
Glacial Varves Data. A varve is an annual layer of sediment or sedimen-
tary rock. This word derives from the Swedish word varv which means layers
or circles. Figure 1.18 exhibits the thicknesses of the yearly varves at one
location in Massachusetts for the period 11,833 B.C. to 11,200 B.C., see Ap-
pendix C for further details about these data.
Tree Ring Data. Figure 1.19 displays a time series consisting of annual
Pinus longaeva tree ring width measurements at Mammoth Creek, Utah, from
0 A.D. to 1989 A.D. An analysis of these data is described in Section 8.4.6.
As discussed above, real-life time series may exhibit several features such
as trends, seasonality, and heteroskedasticity, among others. These aspects
can be considered as nonstationary behavior, where stationarity is associated
to, for example, constant mean and variance across time.
Given that most time series methodologies are mainly focussed on station-
ary data, a number of techniques have been developed to transform a real-life
time series into a stationary one. However, before reviewing some of these
procedures, it is necessary to introduce some fundamental probabilistic and
statistical concepts such as random variable, probability distribution, and au-
tocorrelation, among others.

16
INTRODUCTION
Year
Tree ring series
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
2.0
2.5
Figure 1.19
Tree ring data. Mammoth Creek, Utah, from 0 A.D. to 1989 A.D.
1.2
RANDOM VARIABLES AND STATISTICAL MODELING
Time series are stochastic processes which, in turn, correspond to a sequence of
random variables. Loosely speaking, a random variable is a function between
a sample space Ωcontaining all possible outcomes and the set of real numbers
denoted by R. A number of examples of real-valued random variables are
presented next. In some cases, the random variables take values on a discrete
set such as {0, 1}, and in other cases, the random variable is continuous, taking
values on R or R+, the set of positive real numbers.
EXAMPLE 1.1
A very simple illustration of random variable is the tossing of a coin.
In this case, the two possible outputs are ω1 = Heads and ω2 = Tails.
Thus, the sample space is composed of the events ω1 and ω2, that is,
Ω= {ω1, ω2}. We can write the random variable x : Ω→{0, 1}, where
x(ω1) = 1 and x(ω2) = 0. Now, under the assumption of a fair coin,
we can establish the probability distribution of the random variable x
as P(x = 1) =
1
2 and P(x = 0) =
1
2.
More generally, the situation
described may represent the choice between two options A and B, where
the probability assigned to each case can be speciﬁed as P(x = A) = pA
and P(x = B) = pB. This is called the Bernoulli distribution.

RANDOM VARIABLES AND STATISTICAL MODELING
17
EXAMPLE 1.2
A useful extension of the previous example is the repetition of the coin
tossing or considering multiple selection of options A and B. If n denotes
the number of coin tosses or the number of selections between options
A and B, then the sample space to describe this situation is the product
space Ωn = Ω× · · · × Ω= Ωn. The random variable now can be denoted
as xn : Ωn →{0, 1}n, which is called Binomial distribution. Consider
n = 3, a particular case could be, for instance, x3(ω1, ω2, ω3) = (1, 0, 1).
The probability of this event is P[x3 = (1, 0, 1)] = p2
ApB. More generally,
the probability distribution of this random variable is given by
P(x = k) =
n
k

pk(1 −p)n−k,
for k = 0, 1, 2, . . . , n.
EXAMPLE 1.3
In many situations, the observations that we are interested in are con-
tinuous. For example, consider the returns from a ﬁnancial instrument.
A basic question that one may be interested is: What is the probability
that my investment returns at least 8.5% annually. In this situation,
if the random variable x represents the annual percentage return, the
event of interest is ω = {x ≥8.5}. The probability of this event can be
expressed as P(x ≥8.5) = 1−P(x < 8.5) = 1−F(8.5), where F is called
the distribution function of x.
F(x) =
Z x
−∞
f(u)du,
where, f is the density of the distribution function F. For instance, if
we assume that the returns are normally distributed, then the density
is given by
f(x) =
1
√
2πσ e−1
2 (x−µ)2/σ2,
with µ = E(x) and σ2 = Var(x).
Given that ﬁnancial returns commonly do not follow a Gaussian dis-
tribution, see, for example, Figures 1.20 and 1.21, some extensions of the
normal random variables have been considered in the literature. One of
these generalizations is the so-called exponential power family of random
variables, which has density
f(x) =
β
2αΓ(1/β)e−1
2 |x−µ|β/σβ,

18
INTRODUCTION
where the mean of x is μ and the variance is now
Var(x) = α2 Γ(3/β)
Γ(1/β),
with Γ denoting the gamma function. In Figure 1.20, observe that the re-
turns of the S&P500 index are more concentrated around zero and more
dispersed towards the tails than a Gaussian random variable. This be-
havior is further evidenced by Figure 1.21, which shows that the quantile
of the returns are far from those corresponding to a normal distribution,
cf. 1.21(a).
EXAMPLE 1.4
An important distribution in time series analysis is the multivariate nor-
mal distribution. Given the series {y1, y2, . . . , yn}, we can write the ran-
dom vector X = (y1, y2, . . . , yn).
It is said that X is a multivariate
Gaussian random variable, with mean μ and variance-covariance matrix
Σ, denoted as N(μ, Σ), if the density function is
f(X) = (2π)−n/2|Σ|−n/2e−1
2 (X−µ)′Σ−1(X−µ).
-20
-10
0
10
0.0
0.1
0.2
0.3
Log Returns
Density
Figure 1.20
Distribution of S&P500 daily log returns, January 1950 to January
2014. Dotted line, data distribution; heavy line, normal distribution.

RANDOM VARIABLES AND STATISTICAL MODELING
19
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
(a)
Theoretical quantiles
Sample quantiles
-3
-2
-1
0
1
2
3
-6
-4
-2
0
2
4
(b)
Theoretical quantiles
Sample quantiles
Figure 1.21
Quantiles of S&P500 daily log returns, January 1950 to January
2014. (a) Normal sample quantile-quantile plot. (b) Data quantile-quantile plot.
Observe that the elements of Σ correspond to the covariances among
the components of the random vector X, that is, Σt,s = Cov(yt, ys).
If the time series is stationary, there exists a function γ(·) such that
γ(|t −s|) = Cov(yt, ys). In this case, the variance-covariance matrix can
be written as
Σ =


γ(0)
γ(1)
γ(2)
· · ·
γ(n −1)
γ(1)
γ(0)
γ(1)
· · ·
γ(n −2)
...
...
...
...
γ(n −1)
γ(n −2)
γ(n −3)
· · ·
γ(0)

.
Note that if y = AX, where A is an m×n matrix and X is a multivariate
normal random variable with mean µ and variance-covariance matrix Σ,
then y corresponds to a multivariate normal random variable with mean
Aµ and variance-covariance matrix AΣA′.
EXAMPLE 1.5
The Chi-squared or χ2
ν distribution is quite commonly found in statistics.
Here the parameter ν indicates the degree of freedom of the distribution.
If x follows a standard normal distribution, then its square corresponds
to a χ2
1 random variable. Besides, if x1, x2, . . . , xn is a sequence of inde-

20
INTRODUCTION
0
5
10
15
20
25
0.00
0.05
0.10
0.15
(a)
Density
0
10
20
30
40
50
0.00
0.01
0.02
0.03
0.04
0.05
0.06
(b)
Density
Figure 1.22
Density functions of χ2
ν distributions, where ν are the degrees of
freedom. (a) ν = 4. (b) ν = 20 .
pendent standard Gaussian random variables, then n
1 x2
t follows a χ2
n
distribution. The expected value of a χ2
ν is equal to ν.
f(x) = 2−ν/2
Γ(ν/2)xν/2−1e−x/2,
Figure 1.22 exhibits the density functions of two χ2
ν distributions with
ν = 4 and ν = 20 degrees of freedom, respectively.
EXAMPLE 1.6
Consider a time series of counts such as the number of calls received
by a telephone call center within a given interval of time. In this case,
a random variable that could describe the data observed at time t can
be speciﬁed as x : χ →N. An example of such random variable is the
so-called Poisson distribution given by
P(x = k) = e−λ λk
k! ,
for k = 0, 1, 2, . . . , where the parameter λ is the expected value of x,
E(x) = λ. In addition, Var(x) = λ. A simulated series of 365 Pois-
son counts is shown in Figure 1.23 with λ = 4. On the other hand,
Figure 1.24 exhibits an histogram of these data.

RANDOM VARIABLES AND STATISTICAL MODELING
21
Time
Series
0
100
200
300
0
2
4
6
8
10
Figure 1.23
Time series of Poisson counts.
Count
Density
0
2
4
6
8
10
0.00
0.05
0.10
0.15
0.20
Figure 1.24
Histogram of the time series of Poisson counts.

22
INTRODUCTION
Poisson distribution is widely used for modeling data from diﬀerent
ﬁelds, including number of people in a queue, number of patients having
a particular disease and number of shoppers arriving at a store, among
many other examples.
1.3
DISCRETE-TIME MODELS
This book focuses primarily on stochastic processes that are observed at
discrete-times . . . , t0, t1, t2, . . . , as opposed to continuous time, meaning that
the process has been observed at all times in a given interval, for example
t ∈(0, T). Note that most of the models developed in the time series litera-
ture are concerned with equally spaced times. In this case, the observations
can be written as {yt : t ∈Z} for t ∈{. . . , −2, −1, 0, 1, 2, . . . }. There are mod-
els for treating unequally spaced times, but they are usually more complex
to specify and study. In this context, we can also mention the missing data
problem, where the series is not observed for some values of t. Methods for
dealing with this situation are discussed in Chapter 11.
1.4
SERIAL DEPENDENCE
Consider the stochastic process {yt} and suppose that its mean is µt = E(yt).
If this process is Gaussian, then we can decompose additively it as yt = µt+ηt,
where ηt is zero-mean stochastic process. In order to specify the process {yt},
one may give µt some particular structure.
For instance, for a stationary
process, the mean is assumed constant across time so that µt = µ, for all t.
More generally, the mean can be speciﬁed by a linear model that depends
upon time µt = β0 + β1t + · · · + βptp or depends on other covariates µt =
β0 + β1x1t + · · · + βpxpt.
Stationarity, which is formally discussed in Chapter 2, means that the
statistical characteristics of the time series are preserved across time.
In
particular, the mean and variance of the series are constant and that the
relative dependence of an observation with respect to past values remains the
same, regardless of the moment at which it is evaluated. That is, suppose
that there exists a function γ such that
γ(h) = Cov(yt, yt+h).
The existence of this function, denoted as the autocovariance function, means
that the covariance between observations yt and yt+h does not depend on t.
Stationarity is a key assumption in time series analysis for carrying statistical
inferences and prediction.
The autocorrelation function, ACF hereafter, is
then deﬁned as
ρ(h) = γ(h)
γ(0) .

SERIAL DEPENDENCE
23
Empirical estimates of the ACF are given by the so-called moment estima-
tors
ρk = γ(k)
γ(0),
(1.1)
with
γ(k) = 1
n
n−k

t=1
(yt −¯y)(yt+k −¯y).
Examples of the calculation of the sample ACF for some of the time series
reviewed earlier in this chapter are presented next. The sample ACF of the
returns from the S&P500 index is displayed in Figure 1.25(a) while the sample
ACF of the squared returns is exhibited in Figure 1.25(b). Note that the
returns show a very low level of autocorrelation, but the squared returns
display a large level of autocorrelation.
On the other hand, Figure 1.26 shows the sample ACF of the passenger
enplanements data. Observe the clear seasonal pattern that emerges from this
graph. In this case, the period is 12 months, showing the annual cycle of the
airplane traﬃc.
The sample ACF of the HDDs series is exhibited in Figure 1.27. As in the
case of the passenger enplanements, note the seasonal behavior of the auto-
correlation, reﬂecting the summer/winter eﬀects on the heating requirements.
0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 1.25
S&P500 data. (a) Sample ACF of log returns. (b) Sample ACF of
squared log returns.

24
INTRODUCTION
0
5
10
15
20
25
30
35
-1.0
-0.5
0.0
0.5
1.0
Lag
ACF
Figure 1.26
Sample ACF of passenger enplanements from January 2004 to
January 2013.
0
5
10
15
20
25
30
35
-1.0
-0.5
0.0
0.5
1.0
Lag
ACF
Figure 1.27
Heating degree days, European Union, January 1980 to May 2010.

NONSTATIONARITY
25
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
Figure 1.28
Sample ACF of the Nile river yearly minimum level at the Roda
gauge, from 622 A.D. to 1921 A.D.
A typical sample ACF of a time series exhibiting long-range serial depen-
dence structure is provided in Figure 1.28. Notice that the ACF decays very
slowly. This behavior is usual for time series satisfying a long-memory model,
such as the autoregressive fractionally integrated moving-average (ARFIMA)
models described in Chapter 2.
1.5
NONSTATIONARITY
As seen in the examples discussed in Section 1.1, many real-life time series
display nonstationary features such as trends or seasonalities.
Given that
most of the methodologies for analyzing time series rely on the stationar-
ity assumption, there are a number of techniques developed for transforming
nonstationary data into stationary. Among these approaches, variance stabi-
lization, trend estimation through linear regression and diﬀerentiation of the
series are often used.
Variance stabilization is usually achieved by a Box-Cox transformation of
the data. If the original data is denoted as xt, the transformed series yt is
given by
yt =
 α−1(xα
t −1)
if
α ̸= 0,
log xt
if
α = 0.

26
INTRODUCTION
Linear models are tools for removing a deterministic trend from the data.
This regression model typically includes a polynomial in t, an harmonic com-
ponent, or may contain other covariates. Thus, the model may be written
as
yt
=
β0 + β1x1t + · · · + βpxpt + ηt,
=
Xβ + η,
where the matrix X = (1, x1t, . . . , xpt) are the covariates and the vector η =
(η1, η2, . . . , ηn) represents non-systematic errors. The coeﬃcients (β0, β1, . . . , βp)
can be obtained, for instance, by least squares estimates (LSE), which are
studied in Chapter 10. The LSE in this case is given by
bβ = (X′X)−1X′y.
After estimating the regression parameters, the detrended series is obtained
by removing the regression part from the series {yt}, et = yt −bβ0 −bβ1x1t −
· · · −bβpxpt. Afterwards, the time series methods can be applied to this re-
sulting sequence. In many applications, the regressors are either polynomials
or harmonic function, such as in the case of seasonal behavior,
yt
=
m
X
j=1
[αj sin(ωjt) + βj cos(ωjt)] + ηt,
where the coeﬃcients αj and βj are unknown but the frequencies ωj are
usually considered known or obtained from the spectral analysis, as discussed
in Chapter 3.
EXAMPLE 1.7
To illustrate the versatility of regression methods for modeling time se-
ries data, consider the following simple examples. Figure 1.29 exhibits
a simulated process
yt = β0 + β1t + ηt,
with β0 = 0, β1 = 0.01 and ηt is a Gaussian white noise with zero-mean
and unit variance, for t = 1, . . . , n and n = 300.
A more complex data structure is shown in Figure 1.30. This graph
exhibits a set of 300 simulated observations drawn from the trend break
regression model
yt =
 β0 + β1t + ηt,
t ≤T
β2 + β3t + ηt,
t > T,
(1.2)
where β0 = 0, β1 = 0.01, β2 = 0.5, β3 = 0.0067, and T = 150. Observe
that in this example, there is a change in the slope of the linear trend,
but not a discontinuity of the trend.

NONSTATIONARITY
27
Time
Series
0
50
100
150
200
250
300
-2
-1
0
1
2
3
4
Figure 1.29
Simulated linear regression time series.
Time
Series
0
50
100
150
200
250
300
-2
-1
0
1
2
3
4
Figure 1.30
Structural break regression model with change in the slope of the
trend at time T = 150.

28
INTRODUCTION
Time
Series
0
50
100
150
200
250
300
-4
-2
0
2
Figure 1.31
Structural break regression model with change in the slope and the
level of the trend at the point T = 150.
Figure 1.31 shows another example of the structural break model
described by (1.2), with β0 = 0, β1 = 0.01, β2 = −0.0033, and β3 =
−0.0033. In this case, there are two changes involved since both the
slope of the linear trends and the intercepts vary at time T = 150.
Another common situation is described in Figure 1.32, which displays
300 observations from the instrumental variable model
yt = β0 + β1t + β2Dt + ηt,
where
Dt =
 1,
T1 ≤t ≤T2
0,
otherwise.
(1.3)
In this illustration, β0 = 0, β1 = 0.01, β1 = 10, T1 = 150, and
T2 = 160.
On the other hand, harmonic regressions allow us to model a great va-
riety of seasonal patterns. Figure 1.33 shows a series of 400 observations
from the harmonic model
yt = β cos(ωt) + ηt,
with β = 10 and ω = 0.05.

NONSTATIONARITY
29
Time
Series
0
50
100
150
200
250
300
0
5
10
Figure 1.32
Structural break regression model with instrumental variable for
period T1 = 150 to T2 = 160.
(a)
Time
Series
0
100
200
300
400
-10
-5
0
5
10
(b)
Time
Harmonic component
0
100
200
300
400
-10
-5
0
5
10
Figure 1.33
Harmonic regression time series with one frequency. (a) Simulated
data. (b) Underlying harmonic trend.

30
INTRODUCTION
(a)
Time
Series
0
100
200
300
400
-10
-5
0
5
10
15
(b)
Time
Harmonic component
0
100
200
300
400
-10
-5
0
5
10
15
Figure 1.34
Harmonic regression time series with two frequencies. (a) Simulated
data. (b) Underlying harmonic trend.
An example of an harmonic time series with two frequencies is dis-
played in Figure 1.34, for the model
yt = β1 cos(ω1t) + β2 cos(ω2t) + ηt,
with β1 = 10, β2 = 5, ω1 = 0.05 and ω2 = 0.025.
Figure 1.35 shows a series of 800 observations from the three-component
harmonic model
yt = β1 cos(ω1t) + β2 cos(ω2t) + β3 cos(ω3t) + ηt,
with β1 = β2 = β3 = 10,, ω1 = 0.075, ω2 = 0.0325 and ω3 = 0.0125.
Diﬀerentiation.
Another approach for removing a trend in the data is
diﬀerentiation. In this case, however, the underlying trend is assumed to be
nondeterministic or stochastic. Under this framework, the data is assumed to
be generated by some time aggregation process, for example,
yt
=
t

k=1
ηk,
(1.4)
where ηt is a zero-mean, constant variance stationary process, which may be
sequentially correlated. Thus, by diﬀerencing {yt}, we obtain the series {zt}
zt = yt −yt−1 = ηt,

NONSTATIONARITY
31
(a)
Time
Series
0
100
200
300
400
-30
-10
0
10
20
30
(b)
Time
Harmonic component
0
100
200
300
400
-20
-10
0
10
20
30
Figure 1.35
Harmonic regression time series with three frequencies.
(a)
Simulated data. (b) Underlying harmonic trend.
which shares the same stationary properties of {ηt}. A common problem with
this technique is to decide when to stop diﬀerencing. Two basic aspects should
be taken into account.
First, the diﬀerenced series should look stationary
and second, its variance should be no greater than the original series.
A
disproportionate variance increase in the resulting series could indicate over-
diﬀerentiation.
Another usual dilemma is choosing between regression or diﬀerentiation.
Even though there is no general guidances about this, one can apply any of
these techniques and see whether they produce adequate results or not. It
can be shown that applying a regression method to an integrated model will
produce heteroskedastic errors. On the other hand, diﬀerencing a determin-
istic trend will generate artiﬁcially correlated errors. These two aspects can
be explained as follows.
Assume that the series corresponds to an integrated model, as in (1.4) with
white noise sequence ηt, and a linear model yt = β0 + β1t + εt is ﬁtted. Thus,
yt = yt−1 + β1 + εt −εt−1 and εt = t
k=1 ηk −β1t. Clearly, this error variable
has a time-dependent mean E(εt) = β1t and heteroskedasticity E(εt) = tσ2
η.
Therefore, the regression model does not satisfy the basic assumptions about
the non-systematic error term.
Conversely, if the process yt satisﬁes the regression model yt = β0 +β1t+εt
with white noise εt and it is diﬀerenced, we obtain zt = yt −yt−1 = β1 +

32
INTRODUCTION
εt −εt−1.
The variance of resulting series zt is twice the variance of the
original data, Var(zt) = 2σ2
ε, which is an indication of a wrong diﬀerentiation
procedure.
Apart from the two techniques discussed above, there are many other trans-
formation methods for achieving stationarity. On the other hand, there are
methodologies that allows for the direct treatment of nonstationary data,
without transformation. One example of these methods is the so-called lo-
cally stationary models, described in Chapter 5.
1.6
WHITENESS TESTING
A white noise process is a sequence of zero-mean uncorrelated random vari-
ables. If this sequence is Gaussian, then the process is also independent. A
fundamental procedure in time series analysis is testing for whiteness. That
is, testing whether the series is white noise or it processes some more com-
plex mean or dependence structure. Given the sequence y1, . . . , yn, the null
hypothesis is H0 : {yt} is white noise versus H1 : {yt} is not white noise.
Observe that H0 may fail due to many causes. For example, the mean of
the process is not constant, its variance is not constant, its observations are
correlated, or combinations of these aspects.
Whiteness testing procedures usually do not involve checking for indepen-
dence, unless the series is assumed to be Gaussian. It is important to empha-
size at this point that the deﬁnition of white noise refers only to an uncor-
related sequence. In particular, this means that a sequence with correlated
squares is still white noise, according to this deﬁnition. This is usually the
case for ﬁnancial time series: returns are often uncorrelated but volatilities or
squared returns are just often correlated. Typically, a white noise test takes
into consideration the estimated autocorrelations r1, . . . , rL with
rk = bρk,
where bρk is given by (1.1). The Box-Ljung test, a well-known procedure for
checking whether a sequence is white noise or not, can be written as
QL = n(n + 2)
L
X
h=1
r2
k
n −h,
and it can be shown that the statistic QL follows, approximately, a χ2 distri-
bution with L degrees of freedom.
EXAMPLE 1.8
Figure 1.36 shows 500 observations of a Gaussian white noise sequence
with zero-mean and unit variance, while Figure 1.37 exhibits the sample
ACF and the results from a Ljung-Box test with L = 10. Note that, as
expected, the series complies with the test at the 5% signiﬁcance level.

WHITENESS TESTING
33
Time
Series
0
100
200
300
400
500
-3
-2
-1
0
1
2
3
Figure 1.36
Simulated Gaussian white noise sequence with zero-mean, unit
variance, and 500 observations.
2
4
6
8
10
-0.10
-0.05
0.00
0.05
0.10
(a)
Lag
ACF
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
(b)
Lag
Test
Figure 1.37
Sample ACF (a) and Box-Ljung test (b) for the white noise sequence.

34
INTRODUCTION
2
4
6
8
10
-0.10
-0.05
0.00
0.05
0.10
(a)
Lag
ACF
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
(b)
Lag
Test
Figure 1.38
Sample ACF (a) and Box-Ljung test (b) for the S&P500 log returns.
An application of the Box-Ljung test to the S&P500 log returns is
shown in Figure 1.38. Note that in this case, there is evidence that the
series is not white noise, given that the null hypothesis is rejected by the
Box-Ljung test at the 5% signiﬁcance level for all values of L.
EXAMPLE 1.9
The ability of the Box-Ljung to detect deviations from the white noise
assumption can be illustrated as follows. Consider the moving-average
model deﬁned in (1.5). Figure 1.39 exhibits the percentage of rejection
of the white noise hypothesis for diﬀerent values of the parameter ψ in
the case of a time series composed of 1000 observations and considering
a Box-Ljung test with 10 lags. Notice that the percentage of rejection
gets close to the 100% for very low values of |ψ|, in this case, for values
just above 0.2. On the other hand, as ψ get close to zero, as expected,
the rejection rate decays very sharply.
As a second illustration, consider the ﬁrst-order auto-regressive model
(1.9). Figure 1.40 exhibits the percentage of rejection of the white noise
hypothesis for diﬀerent values of the parameter φ in the case of a time
series composed of 1000 observations and considering a Box-Ljung test
with 10 lags. Similar to the AR(1) case, the percentage of rejection gets
close to the 100% for very low values of |φ|.

WHITENESS TESTING
35
-0.3
-0.2
-0.1
0.0
0.1
0.2
0.3
20
40
60
80
100
!
Rejection %
Figure 1.39
Percentage of rejection of the white noise hypothesis for diﬀerent
values of the parameter ψ in the case of a time series composed of 1000 observations
and considering a Box-Ljung test with 10 lags.
-0.3
-0.2
-0.1
0.0
0.1
0.2
0.3
20
40
60
80
100
!
Rejection %
Figure 1.40
Percentage of rejection of the white noise hypothesis for diﬀerent
values of the parameter φ in the case of a time series composed of 1000 observations
and considering a Box-Ljung test with 10 lags.

36
INTRODUCTION
1.7
PARAMETRIC AND NONPARAMETRIC MODELING
An important distinction among statistical procedures is related to parametric
versus non parametric modeling. Before entering in further technical details,
one can think of the data as coming from a model, usually unknown, which
is speciﬁed by a number of coeﬃcients or parameters. This vision is referred
to as Fisher paradigm. In this context, the statistical analysis is essentially
guessing which are the parameters of the model generating the observed data.
In order to accomplish this goal, one must select the model and estimate the
corresponding parameters.
Examples of this procedures are autoregressive
moving-average (ARMA) models, which are deﬁned in Chapter 2. For speci-
fying a parametric model, one can provide a parameter vector θ = (θ1, . . . , θp)
and a parameter space Θ, such that θ ∈Θ. Observe that the dimension of
this parameter is ﬁnite, p. As an illustration, consider the simple model
yt = εt + ψεt−1,
(1.5)
where {εt} is a white noise sequence and ψ is a one-dimensional parameter.
This is an example of the so-called moving-average models studied in Chapter
2. In this case, the model can be speciﬁed by the two-dimensional vector θ =
(ψ, σ), where σ is the white noise standard deviation. Besides, the parameter
space is given by Θ = (R, R+), where R+ denotes the positive real numbers.
A generalization of the simple model (1.5) is considering several parameters
yt = εt + ψ1εt−1 + ψ2εt−2 + · · · + ψqεt−q,
(1.6)
which will be denoted as moving-average model MA(q) in Chapter 2. In this
case, the parameter vector is θ = (ψ1, ψ2, . . . , ψq, σ). Another extension is al-
lowing the coeﬃcients ψj to depend on a speciﬁc ﬁnite-dimensional parameter
vector, ψj(θ) and write
yt
=
εt + ψ1(θ)εt−1 + ψ2(θ)εt−2 + · · ·
(1.7)
=
∞
X
j=0
ψj(θ)εt−j.
(1.8)
In this case, even though there are inﬁnite coeﬃcients ψj(θ), model (1.7) is
still parametric since they depend on a ﬁnite-dimensional parameter θ.
EXAMPLE 1.10
Consider the ﬁrst-order autoregressive model
yt = φyt−1 + εt.
(1.9)

PARAMETRIC AND NONPARAMETRIC MODELING
37
If |φ| < 1, this model can be also written as
yt
=
εt + φεt−1 + φ2εt−2 + · · ·
=
∞
X
j=0
ψj(θ)εt−j,
where ψj(θ) = φj−1 with θ = (φ, σ).
The models previously discussed are called linear in the sense they are
linear combinations of the noise sequence {εt}.
Other parametric models,
called nonlinear, contain multiplicative error terms. For example, consider
the model
yt = εt + ψεt−1εt−2.
This process in nonlinear since it includes the product of two lagged noise
values, εt−1 and εt−2.
Note, however, that this model is still parametric,
since it is deﬁned by the bivariate vector θ = (ψ, σ). More general expressions
for the nonlinear parametric can be provided. For instance,
yt = fθ(εt, εt−1, εt−2, . . . ),
where fθ is a measurable function deﬁned by the ﬁnite-dimensional parameter
θ.
On the other hand, one may not want to specify a ﬁnite parameter model
but rather consider the data as generated by an unspeciﬁed mechanism. In
this context, the statistical analysis focuses on ﬁnding a general function that
describes the data well.
Examples of this approach are kernel smoothing
methods, neural networks and regression trees, among others.
Contrary to the parametric setup, one may consider a nonparametric model
as speciﬁed by an inﬁnite-dimensional parameter space.
For instance, the
observation yt can be speciﬁed by
yt = f(εt, εt−1, εt−2, . . . ),
where {εt} is an input sequence and f(·) is a function. In this case, the model
is speciﬁed by f ∈F, where F is a space of functions.
Still, there is a third approach, the semiparametric modeling which com-
bines some of the parametric and nonparametric aspects. For example, one
may partially specify the behavior of the spectrum around a given frequency,
leaving unspeciﬁed the behavior of the spectrum at other frequencies, see its
deﬁnition in Chapter 3. For instance, if fy(λ) denotes the spectral density
of the process {yt}, then one may specify the behavior of the spectrum in a
neighborhood of the origin as
fy(λ) ∼C|λ|α,
for small λ, where C is a positive constant and α is the parameter of interest,
that is, the rate at which the spectrum converges to zero in the case that α
is positive or diverges in the case that α is negative.

38
INTRODUCTION
1.8
FORECASTING
Regardless of the modeling approach that one may consider, parametric, semi-
parametric, or nonparametric, in time series analysis, one is usually interested
in predicting future values of the process. Given observations up to time t,
one may want to forecast h-steps ahead, the prediction horizon, byt+h.
As
shown later in this text, the best linear predictor is given by the conditional
expectation,
byt+h = E[yt+h|yt, yt−1, yt−2, . . . ].
The best predictor may not necessarily be linear, so that the optimal fore-
cast may have a diﬀerent expression that will depend on the relationship
between yt and its past. For a Gaussian process, the best predictor is pre-
cisely the best linear predictor, independently of the model linking yt+h with
yt, yt−1, yt−2, . . . . Besides, note that in practice only a ﬁnite stretch of data is
available, {y1, y2, . . . , yn}, say. In this case, the ﬁnite past predictor is given
by,
byn+h = E[yn+h|yn, yn−1, yn−2, . . . , y1].
In some other situations, the interest is focused on estimating past values
of the process, the so-called backcasting procedure. In this case, one may want
to estimate, for instance, y0, given {y1, y2, . . . , yn},
by0 = E[y0|y1, y2, . . . , yn].
Another procedure, called smoothing is concerned with estimating the value
of the process at a particular time t ∈{1, 2, . . . , n} given the remaining ob-
servations,
byt = E[yt|y1, . . . , yt−1, yt+1, . . . , yn].
Let {et} be the prediction error sequence, that is, et = yt −byt, for t =
1, 2, . . . , n. One basic criterion for goodness of ﬁt in time series analysis is
that e1, e2, . . . , en is a white noise sequence. This hypothesis can be formally
tested by the Box-Ljung procedure or another technique.
If the resulting
sequence of prediction errors is not white noise, then the model may not be
appropriate for the data.
As in other areas of statistics, when ﬁtting a time series model one set some
observations aside, so that we can assess the out-of-sample performance of the
model.
1.9
TIME SERIES MODELING
This section provides an overview of the process involved in time series para-
metric modeling and prediction. These procedures take into account the topics
discussed in the previous sections of this chapter. As real-life time series data
usually appear nonstationary, there are techniques that transform the data

BIBLIOGRAPHIC NOTES
39
into a more stationary sequence. Next, the basic distributional features of
the data are analyzed, establishing constant mean, variance, and checking for
the presence of outliers or nonnormality. Once these properties have been
found consistent with stationarity, the focus is shifted toward empirically in-
vestigating the autocorrelation structure of the observations. Based on this
analysis, a model can be proposed, for example, an ARMA model as deﬁned
in Chapter 2. The model is usually selected by considering an information
criterion such as Akaike’s information criterion (AIC) or Bayesian informa-
tion criterion (BIC), see Chapter 4 for deﬁnitions of these concepts. The main
idea behind this model selection process is that while adding more parameters
could improve the ﬁtting ability of a particular model, it also diminishes its
degree of freedom to evaluate the ﬁtting and out-of-sample prediction quality.
In this sense, criteria such as AIC or BIC help to strike a balance between
ﬁtting ability and model complexity.
After selecting the model, a battery of tests can be applied to determine
the parameter signiﬁcance as well as the goodness of ﬁt. Moreover, a residual
analysis can be performed to test the whiteness of the model errors. If the
model is appropriate, the residuals should be white noise.
Finally, once the model is judged to be adequate for the data, it can be
used for producing forecasts of future values. In many situations, part of the
data is set aside from the ﬁtting procedure, so that it can be used for an out-
of-sample assessment of the prediction quality. This step is important also
for the nonparametric approach. In this case, a methodology such as neural
network or regression tree can be applied to a portion of the time series, the so-
called training data, and the remaining part of the series is left for evaluating
the out-of-sample forecasting capacity of the nonparametric method.
1.10
BIBLIOGRAPHIC NOTES
There is a pleayade of books in time series analysis.
The monograph by
Brockwell and Davis (2002) is good introductory text to the subject. The
book by Shumway and Stoﬀer (2011) provides another excellent treatment
of the fundamental time series techniques. Diggle (1990) oﬀered an overview
of the subject from a biostatistics standpoint. Kedem and Fokianos (2002)
covered a number of regression time series methodologies, including models
for count data. The books by Tsay (2005) and Tsay (2013) provide excellent
discussions about ﬁnancial time series and a number of techniques for ﬁtting
and predicting heteroskedastic data. Hamilton (1994) is another interesting
text on ﬁnancial time series covering several topics. More advanced texts in
the subject are, for example, Brockwell and Davis (1991) and Fuller (1996).

40
INTRODUCTION
Problems
1.1
Explain the following concepts: (a) Stationarity, (b) Seasonality, (c)
Deterministic trend, (d) Stochastic trend, (e) White noise, and (f) Structural
change.
1.2
What does it mean a sample realization of a time series?
1.3
Why tree rings and mineral sediments time series are important for
climatic and paleoclimatic studies?
1.4
Explain the concept and utility of the autocovariance function and the
autocorrelation function. Write down the corresponding equations that deﬁne
these concepts.
1.5
Suppose that you have a sample realization of n observations given by
{x1, . . . , xn}. Explain how you could estimate the autocovariance and sample
autocorrelation functions.
1.6
Explain the concept of strict stationarity of a time series.
1.7
What are suﬃcient conditions for weak stationarity of a time series? Is
a weakly stationary time series necessarily strictly stationary?
1.8
How is a white noise deﬁned?
1.9
Suppose you have a series yt deﬁned as yt = Pp
j=0 βjtj + ωt. Calculate
the expected value and the autocovariance function of yt.
1.10
Regarding the previous question, propose two weak stationarity-inducing
transformations of yt.
1.11
Let {y1, . . . , yn} be a sample from a stationary process.
Suggest a
procedure to determine if the stochastic process is the sum of a constant and
a white noise?
1.12
Let {ε1, . . . , εn} be Gaussian white noise with zero-mean and unit vari-
ance and suppose that we are interested in simulating a Gaussian stationary
process {y1, y2, . . . , yn} with autocovariance function γ and mean µ. Show
that this process can be simulated by generating a sample of the white noise
ε = {ε1, . . . , εn} and then obtaining y = µ + Aε, where A is a square matrix
satisfying AA′ = Γ with Γi,j = γ(i −j).
1.13
Let x1, x2, . . . , xn be a sample of χ2
1 independent random variables. By
taking into account that the fourth moment of a standard normal distribution
is E(x4) = 3, calculate the variance of the random variable y = x1 + x2 +
· · · + xn.
1.14
Suppose that the model yt = β0 + β1t + ϵt with t = 1, ..., n, E(ϵt) = 0,
E(ϵ2
t) = σ2 and for t ̸= s, E(ϵtϵs) = 0. Deﬁne wt = 1
3
P1
j=−1 yt+j.
(a) Find the expected value of the time series wt.

PROBLEMS
41
(b) Calculate Cov(wt+k, wt) and show that this covariance does not de-
pend on t. Is the sequence {wt} stationary?
1.15
Explain why it is sometimes necessary to apply a functional transfor-
mation of the data, such as a logarithmic or a Box-Cox transformation.
1.16
Suppose that the price of a stock at time t is denoted as Pt and that
the sequence of prices is described by the equation
Pt = Pt−1(1 + rt),
where rt is the return at time t. By taking logarithms and using the approx-
imation log(1 + x) ∼x for small values of x, show that
rt = ∆log Pt = log Pt −log Pt−1.
1.17
Explain the following concepts related to the analysis of ﬁnancial time
series, (a) Return, (b) Volatility, (c) Heteroskedasticity, and (d) Risk.
1.18
Let Pk(t) be a k order polynomial deﬁned by
Pk(t) = a0 + a1t + a2t2 + · · · + aktk.
Show that (1 −B)kPk(t) = c, where c is a constant.


CHAPTER 2
LINEAR PROCESSES
The concept of linear process is fundamental in time series analysis. As in the
time series examples discussed in the previous chapter, many social, physical
and economic phenomena can be analyzed and described by this class of mod-
els. A scientist or a practitioner knows that a phenomenon under study may
be highly complex, but often a linear approach oﬀers a ﬁrst good description of
the data. On the basis of a linear process, more complex models can be built
afterwards. A linear process contains three basic components: an input noise,
a linear ﬁlter, and the output observed data. In practice, one only has a ﬁnite
set of observations, but one can still imagine or assume that the available time
series comes from a linearly ﬁltered noise. Even though this approach seems
to oversimplify the data generating mechanism, it usually provides a powerful
tool for modeling a wide range of time series data. In this chapter, we review
the foundations of the linear processes and study some of their applications.
Three basic representations of a linear process are described, the Wold expan-
sion, the autoregressive expansion and the state space systems. Stationarity,
invertibility, and causality are also reviewed. Another important aspect to
consider in the analysis of a time series is whether the dependence structure
of its observations is weak or strong. This topic is discussed when describing
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
43

44
LINEAR PROCESSES
autoregressive moving-average (ARMA) and autoregressive fractionally inte-
grated moving-average (ARFIMA) models. This chapter also analyzes the
autocovariance structure of these processes, providing methods for computing
autocorrelation (ACF) and partial autocorrelation (PACF).
2.1
DEFINITION
A linear process can be written as,
yt = ψ(B)εt,
(2.1)
where yt are the observed values, ψ(B) is a linear ﬁlter on the backshift
operator B, and εt are the input noise. Recall that the eﬀect of B is lagging
an observation, that is, Byt = yt−1.
The ﬁlter can be written as
ψ(B) =
∞
X
j=−∞
ψjBj,
(2.2)
where P∞
j=−∞ψ2
j < ∞. Consequently, we can write the observed time series
as,
yt =
∞
X
j=−∞
ψjεt−j.
(2.3)
Note that this ﬁlter is said to be linear since it does not contain mixed input
noise terms such as εt−iεt−j. Furthermore, this expression tell us that the
observed value at time t, yt, depends on past, present, and future values of
the input noise, that is, {. . . , ε−3, ε−2, ε−1, ε0, ε1, ε2, ε3, . . . }.
2.2
STATIONARITY
This is an essential concept in time series analysis. Generally speaking, we
can distinguish two deﬁnitions of stationarity. The ﬁrst deﬁnition is focused
on the joint distribution of the process, while the second focuses on the second
order structure of the time series model.
Strict Stationarity. Let yh(ω) = {yt1+h(ω), . . . , ytn+h(ω)} be a trajectory
of the process {yt} with t1 + h, . . . , tn + h ∈Z. The process is said to be
strictly stationary if and only if the distribution of yh is the same regardless
of h.
Weak Stationarity. A process yt is said to be weakly stationary or second-
order stationary if (a) it has a constant mean, (b) it has ﬁnite and constant sec-
ond moment, and (c) there is a function γ(·) such that γ(k) = Cov(yt, yt+|k|)
for any t, k. For example, for the linear process (2.3), we have (a) E(yt) =

STATIONARITY
45
P∞
j=−∞ψj E(εt−j) = 0. On the other hand, given that {εt} are uncorrelated,
E(y2
t ) =
∞
X
j=−∞
ψ2
j E(ε2
t−j) = σ2
∞
X
j=−∞
ψ2
j < ∞.
(2.4)
Furthermore, we can write the autocovariance function γ(k) = E(ytyt+h) as
γ(h)
=
∞
X
i=−∞
∞
X
j=−∞
ψiψj E(εt−iεt+h−j)
=
σ2
∞
X
i=−∞
∞
X
j=−∞
ψiψjδ(h + i −j)
(2.5)
=
σ2
∞
X
j=−∞
ψjψj+h.
Stationarity is an important concept in time series analysis. Loosely speaking,
it means that the statistical properties of the process remain constant over
time. In practice, this implies that all values of the process are comparable,
no matter at what time they were observed. In turn, comparability of the
observations allows us to draw statistical conclusions about the whole process.
A process can be strictly stationary but not necessarily weakly station-
ary and vice versa. For instance, the process {yt : t ∈Z} where, yt are in-
dependent and identically distributed Cauchy random variables, is strictly
stationary but not weakly stationary since the ﬁrst and the second moments
do not exist.
A more sophisticated example of this is the fractionally in-
tegrated generalized autoregressive conditionally heteroskedastic (FIGARCH)
model introduced in Chapter 6. Conversely, let {εt} be a sequence of inde-
pendent and identically distributed normal random variables with zero-mean
and unit variance, and let {ηt} be a sequence of independent and identically
distributed exponential random variables with rate 1. Then the process gen-
erated by yt = εt[t/2] + (ηt −1)[(t + 1)/2], where [·] denotes the integer part
function, is weakly stationary but not strictly stationary. Nevertheless, these
two concepts are equivalent for Gaussian processes.
A strict white noise process is a sequence of independent and identically
distributed random variables, while a weak white noise process is a sequence
of uncorrelated random variables with zero-mean and constant ﬁnite variance,
that is, with an autocovariance function satisfying γ(0) < ∞and γ(h) = 0 for
all h ̸= 0.

46
LINEAR PROCESSES
2.3
INVERTIBILITY
The linear process (2.3) is invertible if there exists a ﬁlter π(B) such that we
can write
π(B)yt =
∞
X
j=−∞
πjyt−j = εt.
The ﬁlter π(B) can be considered as the inverse of the ﬁlter ψ(B), that is,
π(B)ψ(B) = 1. Note that an invertible time series yt can be expressed as
yt =
−1
X
j=−∞
πjyt−j +
∞
X
j=1
πjyt−j + εt.
2.4
CAUSALITY
One way to describe a discrete-time stochastic process is writing it as the
result of the ﬁltering of a white noise sequence {εt},
yt = ϕ(. . . , ε−2, ε−1, ε0, ε1, ε2, . . . ),
where ϕ(·) is a measurable function, that is, the resulting sequence {yt} is
a well-deﬁned random process. Assume now that the noise sequence is gen-
erated simultaneously as the observed process {yt}, so that at any instant t,
the generated noise sequence is . . . , ε−2, ε−1, ε0, ε1, ε2, . . . , εt and the observed
process is given by . . . , y−2, y−1, y0, y1, y2, . . . , yt. In this context, the process
{yt} is causal and it can be written as
yt = ϕ(. . . , ε−2, ε−1, ε0, ε1, ε2, . . . , εt).
Thus, a causal process depends only on past and present noise, and does not
depend on future values of the noise.
This is an important feature of the
process {yt}, meaning that only past or present shocks can aﬀect it. If the
process is not causal, for example, yt = εt+1−θεt, then a future event εt+1 can
aﬀect its present value. Even though this process is not causal, we can still
predict it. As we will see later, the best linear predictor of yt is given by byt =
E[yt|yt−1, yt−2, . . . ]. Note that for |θ| < 1, we can write εt = P∞
j=1 θj−1yt−j,
so that the predictor can be expressed as byt = E[yt|εt, εt−1, . . . ] = −θεt. In
this case, the prediction error et = yt −byt variance is Var(et) = σ2
ε.
2.5
REPRESENTATIONS OF LINEAR PROCESSES
A linear process may be represented in many diﬀerent forms, for instance, as
the Wold decomposition, an autoregressive expansion, or a state space system.
To a large extent, these representations are equivalent and a key issue is how
to pass from one representation to another. We have to keep in mind, however,
that in most cases, these representations are not unique.

REPRESENTATIONS OF LINEAR PROCESSES
47
Time
Series
0
50
100
150
200
-4
-2
0
2
4
Figure 2.1
Several paths of the singular process singular with zero-mean and unit
variance Gaussian random variable Z. Real axis paths of 200 observations each,
with ω = 0.1.
2.5.1
Wold Decomposition
A model described by (2.3) is usually referred to as regular process. Consider
now the process
yt = Ze˙ı ω t = Z cos(ωt) + Z˙ı cos(ωt),
(2.6)
where Z is a zero-mean random variable with variance σ2 and ω is a known
frequency. This time series satisﬁes E(yt) = 0, Var(yt) = σ2, and there is a
function γ(·) such that Cov(yt, ys) = γ(t −s) = σ2e˙ıω(t−s). Thus, according
to the deﬁnition discussed in the previous section, the process {yt} is weakly
stationary. Note that this process has a diﬀerent representation as the one
described in (2.3).
This sequence is an example of the so-called singular
processes.
See Figure 2.1 and Appendix A for further details about these
deﬁnitions.
Even though both a regular process and a singular process may be sta-
tionary, the fundamental diﬀerence between them is that the former cannot
be totally predicted, while the latter can be perfectly predicted. Generally
speaking, as we accumulate more and more observations of a regular process,
we can improve our forecasting ability, reducing, for example, the mean square
prediction error. But there is always a certain level of prediction error. On

48
LINEAR PROCESSES
the contrary, for the case of a singular process, it suﬃces to have only one
observation in order to obtain a perfect predictor of any future value. In our
example (2.6), any future value of the process can be written in terms of the
observation yt as
yt+h = Ze˙ı ω (t+h) = yte˙ıωh.
Thus, given that yt and ω are assumed to be known, we know the exact value
of yt+h.
In this context, an important result called the Wold decomposition estab-
lishes that any stationary process can be written as the sum of a regular and
a singular process and that decomposition is unique. Put it simply, any sta-
tionary process may contain a perfectly predictable or deterministic part and
another non deterministic component.
Given that in practice, we usually have to deal with processes that cannot
be fully predictable, in the remaining of this book we will only consider regular
processes. However, singular processes are of interest in some ﬁelds such as
electrical engineering where signal can be described by a particular frequency
and a random amplitude.
2.5.2
Autoregressive Representation
Assuming that the linear process {yt} is invertible and causal, it can be ex-
pressed as
yt =
∞
X
j=1
πjyt−j + εt.
This is often called the autoregressive representation of the time series since
the present value yt is written in terms of its past values plus a noise.
2.5.3
State Space Systems
The linear processes introduced in (2.3) were described by the Wold decom-
position. However, these processes can also be expressed in terms of a state
space linear system.
A linear state space system may be described by the discrete-time equations
xt+1
=
Fxt + Hεt,
(2.7)
yt
=
Gxt + εt,
(2.8)
where xt is the state vector for all time t, yt ∈IR is the observation sequence,
F : H →H is the state transition matrix or the state matrix, G is the ob-
servation matrix, H is a state noise vector, and {εt} is the state white noise
sequence with variance σ2; (2.7) is called the state equation while (2.8) is
called the observation equation.

WEAK AND STRONG DEPENDENCE
49
EXAMPLE 2.1
As an illustration, consider the following ARMA(1,1) model described
by the equation
yt −φyt−1 = εt −θεt−1,
where |φ| < 1, |θ| < 1 and εt is a white noise sequence. Thus, in order
to obtain the Wold expansion for this process, we can write
(1 −φB)yt = (1 −θB)εt
and then
yt = (1 −φB)−1(1 −θB)εt.
Therefore, the Wold expansion is given by
yt = εt + (φ −θ)εt−1 + φ(φ −θ)εt−2 + φ2(φ −θ)εt−3 + · · ·
On the other hand, by inverting θ(B), we obtain the inﬁnite AR expan-
sion. Therefore, the inﬁnite AR expansion is given by
yt = εt + (θ −φ)yt−1 + θ(θ −φ)yt−2 + θ2(θ −φ)yt−3 + · · ·
A state space representation of this ARMA process is given by
xt+1
=
φxt + (φ −θ)εt
yt
=
xt + εt.
Observe that according to the state equation, we can write
xt+1
=
(1 −φB)−1(1 −θB)xt
=
xt + (φ −θ)xt−1 + φ(φ −θ)xt−2 + φ2(φ −θ)xt−3 · · ·
Finally, by replacing this expression in the observation equation, we
obtain the Wold expansion for the model.
2.6
WEAK AND STRONG DEPENDENCE
Consider a stationary process {yt} with mean µ. An important and funda-
mental problem in time series is ﬁnding estimates of µ. Given a trajectory
y1, y2, . . . , yn of this process, a simple estimator of µ is the sample mean

50
LINEAR PROCESSES
bµn = ¯yn. The variance of this estimator is
Var(bµn)
=
Var
 
1
n
n
X
t=1
yt
!
=
1
n2
n
X
t=1
n
X
s=1
Cov(yt, ys)
=
1
n2
n
X
t=1
n
X
s=1
γ(t −s)
=
1
n2
n−1
X
h=1−n
(n −|h|)γ(h).
Therefore, an upper bound for the variance of bµn is given by
Var(bµn)
≤
2
n
n−1
X
h=0
|γ(h)|.
Consequently, as the sample size gets larger, we conclude that
lim
n→∞Var(bµn)
≤
lim
n→∞
2
n
n−1
X
h=0
|γ(h)| = 2 lim
n→∞|γ(n)|.
From the above expression, if limn→∞γ(n) = 0, then the variance of the
estimator tends to zero, as the sample size tends to inﬁnity. Thus, by an
application of the Chebyshev inequality, we can conclude that limn→∞γ(n) =
0 implies that the sample mean is a consistent estimator of µ.
At this point, one may ask about other important properties of bµn such as
rate of convergence of Var(bµn) to zero, asymptotic normality, and eﬃciency.
In order to study these properties, it is necessary to incorporate to the analysis
the rate at which γ(n) tends to zero as n →∞. If this rate is suﬃciently fast
so that P∞
h=0 |γ(h)| < ∞, then it can be shown that
Var(bµn) = σ2
n
∼
2
n
∞
X
h=0
|γ(h)|,
and that bµn satisﬁes a central limit theorem and is eﬃcient,
bµn
∼
N
 µ, σ2
n

.
Time series satisfying the condition P∞
h=0 |γ(h)| < ∞, are usually referred to
as short-memory or weakly dependent processes.
On the other hand, if the rate of decaying of γ(n) is slow enough so that
P∞
h=0 |γ(h)| = ∞, the analysis of the properties of bµn is more complex. Con-
sider, for example, that the ACF of a process satisﬁes the condition γ(h) ∼

ARMA MODELS
51
C|h|2d−1 for large h. Naturally, in order to satisfy that limn→∞γ(n) = 0,
the parameter d must satisfy d < 1
2. Furthermore, note that for d ∈(0, 1
2),
P∞
h=0 |γ(h)| = ∞. In this case, we can calculate the variance of the sample
mean for large n as follows:
Var(bµn)
=
1
n2
n−1
X
h=1−n
(n −|h|)γ(h) ∼1
n
n−1
X
h=1−n
γ(h) ∼1
n
n−1
X
h=1−n
C|h|2d−1
=
2
n
n−1
X
h=1
C

h
n

2d−1
n2d−1 = 2n2d−1
n−1
X
h=1
C

h
n

2d−1 1
n
∼
2Cn2d−1
Z 1
0
|x|2d−1 dx ∼C1n2d−1.
Observe that in this case, the variance of the sample mean converges to zero
at a slower rate, as compared to the short-memory case. Therefore, stationary
time series satisfying P∞
h=0 |γ(h)| = ∞, are usually referred to as long-memory
or strongly dependent processes.
2.7
ARMA MODELS
ARMA models are fundamental tools for analyzing short-memory time series.
It can be shown that this class of models approximate arbitrarily well any lin-
ear stationary process with continuous spectral density deﬁned in Chapter 3.
Besides, there is a large number of numerical and computational tools for ﬁt-
ting, diagnosing, and forecasting ARMA models. They have been very useful
for modeling a large number of time series exhibiting weak dependence. On the
other hand, autoregressive fractionally integrated moving-average (ARFIMA)
processes have been widely used for ﬁtting time series data exhibiting long-
range dependence. These two classes of models are discussed next.
An ARMA(p, q) process {yt} can be speciﬁed by the discrete-time equation,
φ(B)yt = θ(B)εt,
(2.9)
where φ(B) = 1 −φ1B −· · · −φpBp is an autoregressive polynomial on the
backshift operator B, θ(B) = 1+θ1B+· · ·+θqBq is a moving-average polyno-
mial, with roots diﬀerent from those of φ(B) and {εt} is a white noise sequence
with zero-mean and variance σ2. Before studying the properties of this class
of models, consider the following three examples. Note that an autoregressive
AR(p) process corresponds to an ARMA(p, 0) model. On the other hand, a
moving-average MA(q) is the special case ARMA(0, q).
EXAMPLE 2.2
According to (2.9) an AR(1) model can be expressed as yt = φyt−1 + εt,
where εt is a white noise sequence with variance σ2.
Note that the

52
LINEAR PROCESSES
mean of yt must satisfy E(yt) = φ E(yt−1) since E(εt) = 0. For φ ̸= 0,
the stationarity condition implies that E(yt) = 0. On the other hand,
the variance of this process must satisfy Var(yt) = φ2 Var(yt−1) + σ2.
Under stationarity, we must have (1 −φ2) Var(yt) = σ2. Consequently,
given that variances must be positive, we conclude that |φ| < 1 and that
Var(yt) = σ2/(1−φ2). With the above condition on the parameter φ, the
process yt may be expressed as the Wold expansion yt = P∞
j=0 φjεt−j.
The autocovariance function of this process can be obtained as follows.
Consider h > 1, so that
yt+h = φhyt + φh−1εt + φh−2εt+1 + · · · + εt+h−1.
Thus,
γ(h)
=
E(ytyt+h)
=
E[yt(φhyt + φh−1εt + φh−2εt+1 + · · · + εt+h−1)]
=
φh E y2
t + φh−1 E ytεt + · · · + E ytεt+h−1
=
φh E y2
t = σ2 φh
1 −φ2 .
2.7.1
Invertibility of ARMA Processes
An ARMA(p, q) process is said to be invertible if all the roots z of the poly-
nomial Θ(z) = 1 + θ1z + · · · + θqzq satisfy |z| > 1. This means that we can
write the following expression for the noise sequence
εt = θ(B)−1φ(B)yt = Π(B)yt =
∞
X
j=0
πjyt−j.
2.7.2
Simulated ARMA Processes
In order to gain an insight about how a trajectory of an ARMA time series
looks like in what follows, we present several simulated processes.
EXAMPLE 2.3
As a ﬁrst example, Figure 2.2 shows a simulated trajectory of 1000
observations from an AR(1) process with parameter φ = 0.5. Note that
due to the dependence of the observations, the series seems to have some
trends. An even stronger eﬀect of the dependence can be observed in
Figure 2.3, which shows an AR(1) process with autoregressive parameter
φ = 0.9. In this case, the trends seem much more relevant. Nevertheless,
since these two processes are stationary, there are no true trends in these
series.

ARMA MODELS
53
Time
Series
0
200
400
600
800
1000
-4
-2
0
2
Figure 2.2
Simulated AR(1) time series of length 1000 with φ = 0.5.
Time
Series
0
200
400
600
800
1000
-6
-4
-2
0
2
4
6
Figure 2.3
Simulated AR(1) time series of length 1000 with φ = 0.9.

54
LINEAR PROCESSES
EXAMPLE 2.4
The behavior of moving-average models is explored in Figures 2.4 and
2.5. Both series are of length 1000 observations. Figure 2.4 displays the
trajectory of an MA(2) process with parameters θ = (0.7, −0.6), while
Figure 2.5 shows an MA(4) process with moving-average parameters
θ = (0.3, −0.4, 0.7, 0.5). Similar to the previous AR examples, in this
case, we can also observe the eﬀect of the dependence in the trajectory
of the series.
Time
Series
0
200
400
600
800
1000
-4
-2
0
2
4
Figure 2.4
Simulated MA(2) time series of length 1000 with θ = (0.7, −0.6).
Time
Series
0
200
400
600
800
1000
-4
-2
0
2
4
Figure
2.5
Simulated
MA(4)
time
series
of
length
1000
with
θ
=
(0.3, −0.4, 0.7, 0.5).

ARMA MODELS
55
EXAMPLE 2.5
Finally, Figures 2.6 and 2.7 exhibit two simulated ARMA(1, 1) processes,
with parameters φ = 0.5 and θ = 0.7, and φ = 0.5 and θ = −0.7, respec-
tively. Notice the variety of trajectories generated by these models. In
particular, observe that by just changing the sign of the moving-average
parameter, we obtain a much diﬀerent time series.
Time
Series
0
200
400
600
800
1000
-6
-4
-2
0
2
4
6
Figure 2.6
Simulated ARMA(1, 1) time series of length 1000 with φ = 0.5 and
θ = 0.7.
Time
Series
0
200
400
600
800
1000
-3
-2
-1
0
1
2
3
Figure 2.7
Simulated ARMA(1, 1) time series of length 1000 with φ = 0.5 and
θ = −0.7.

56
LINEAR PROCESSES
2.8
AUTOCOVARIANCE FUNCTION
As deﬁned in Chapter 1, a second-order stationary time series possesses an
autocovariance function γ(·), which measures the level of dependence of the
value at a certain time t and another value at time t + h. For a stationary
process, this statistical dependence indicated by the covariance between both
values does not depend on t but only on the lag h.
Several examples of
calculations of the ACF for ARMA processes are presented next.
Consider the moving-average model MA(q) given by
yt = εt + θ1εt−1 + · · · + θqεt−q.
(2.10)
In this case, the ACF is
γ(h) =



σ2(θ2
1 + · · · + θ2
q)
for
h = 0
σ2 Pq−|h|
j=0 θjθj+|h|
for
|h| = 1, . . . , q
0
for
|h| > q.
(2.11)
Based on this expression, a moving-average process can be identiﬁed from its
empirical ACF. If the sample ACF is not signiﬁcant after a lag q, this is an
indication that a MA(q) process may model the data adequately.
EXAMPLE 2.6
Consider the ARMA(1, 1) satisfying the discrete-time equation
yt −φyt−1 = εt + θεt−1.
(2.12)
There are at least two ways to calculate its autocovariance function.
First Method: From expression (2.5), it suﬃces to calculate the coef-
ﬁcients ψj and then calculate the sum. To this end, we can write
yt
=
(1 −φB)−1(1 + θB)εt
=
[1 + (φ + θ)B + φ(φ + θ)B2 + φ2(φ + θ)B3 + · · · ]εt
(2.13)
=
εt + (φ + θ)εt−1 + φ(φ + θ)εt−2 + φ2(φ + θ)εt−3 + · · · .
Thus, ψ0 = 1, ψj = φj−1(φ + θ) for j ≥1. Consequently,
γ(h) = σ2

ψ|h| + (φ + θ)2φ|h|
1 −φ2

.
(2.14)
Second Method: Another way to compute the autocovariance function
is as follows. Multiplying both sides of (2.12) by yt−h, we get
ytyt−h −φyt−1yt−h = εtyt−h + θεt−1yt−h.
Now, taking expected values, we obtain
E(ytyt−h) −φ E(yt−1yt−h) = E((εtyt−h) + θ E(εt−1yt−h).

ACF AND PARTIAL ACF FUNCTIONS
57
From (2.13), we have that E εtyt = σ2 and E(εt−1yt−h) = ψhσ2. That
is,
γ(h) −φγ(h −1) = E(εtyt−h) + θ E(εt−1yt−h).
For h = 0, we have that
γ(0) −φγ(1) = σ2(1 + θψ1),
(2.15)
and for h = 1, we get
γ(1) −φγ(0) = σ2θ.
(2.16)
Given that εt and εt−1 are uncorrelated with yt−h for h > 2, we get
γ(h) = φγ(h −1).
(2.17)
Note that from (2.15) and (2.16), we conclude that
γ(0) = σ2 1 + 2φθ + θ2
1 −φ2
,
γ(1) = σ2 (φ + θ)(1 + φθ)
1 −φ2
,
and from (2.17),
γ(h) = φ|h|γ(0),
for |h| > 1.
2.9
ACF AND PARTIAL ACF FUNCTIONS
As discussed previously, the ACF is a standardized measure of the dependence
of two observations yt and yt+h, corresponding to the autocovariance function
divided by the variance of the process.
ρ(h) = γ(h)
γ(0) .
(2.18)
From the previous section, we can write the ACF of a moving-average process
MA(q) as follows:
ρ(h) =





1
for
h = 0
Pq−|h|
j=0
θjθj+|h|
θ2
1+···+θ2
q
for
|h| = 1, . . . , q
0
for
|h| > q.
In what follows, we present a number of examples of the behavior of the
ACF for diﬀerent ARMA models. Figure 2.8 exhibits the exact ACF for two
AR(1) processes. Figure 2.8(a) shows the ACF for a model with φ = 0.5

58
LINEAR PROCESSES
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Lag
ACF
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
(b)
Lag
ACF
Figure 2.8
Exact autocorrelation function (ACF) of an AR(1) model. (a) φ =
0.5, (b) φ = 0.9.
while Figure 2.8(b) depicts the ACF for φ = 0.9. Note the diﬀerent decaying
rates of these ACFs. As φ increases, the ACF decays more slowly. Recall,
however, that a necessary condition for ensuring the stationarity of these
AR(1) models is that |φ| < 1. In this situation, as φ approaches the upper
limit 1, the convergence rate to zero is extremely slow, as shown in Figure 2.9
(a) and (b) which depicts the ACF for φ = 0.95 and φ = 0.99, respectively.
The theoretical ACF of MA(q) time series models is exhibited in Figure
2.10. The ACF on Figure 2.10(a) corresponds to an MA(2) model with θ =
(0.7, −0.6) while the ACF on Figure 2.10(b) is for an MA(4) process with
θ = (0.3, −0.4, 0.7, 0.5). As expected, the ACF vanishes for lags greater than
the order q of the model.
Figure 2.11 shows the exact ACF for two ARMA(1, 1) models. The ACF
on Figure 2.11(a) corresponds to a model with parameters φ = 0.5 and θ = 0.7
while the ACF on Figure 2.11(b) corresponds to an ARMA(1, 1) time series
model with parameters φ = 0.5 and θ = −0.7. Note that the decaying rate
of the ACF is governed by φ|h|.
Thus, it is expected that in both cases,
the ACF converges rapidly to zero as the lag increases. But the sign of the
ACF is governed in this case for the moving-average parameter θ, which is
clearly reﬂected on Figure 2.11(b) which corresponds to a negative value of
this parameter.

ACF AND PARTIAL ACF FUNCTIONS
59
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Lag
ACF
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
(b)
Lag
ACF
Figure 2.9
Exact autocorrelation function (ACF) of an AR(1) model. (a) φ =
0.95, (b) φ = 0.99.
0
2
4
6
8
10
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Lag
ACF
0
2
4
6
8
10
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
(b)
Lag
ACF
Figure 2.10
Exact autocorrelation function (ACF) of a MA(q) model, for q = 2
and q = 4. (a) θ = (0.7, −0.6), (b) θ = (0.3, −0.4, 0.7, 0.5).

60
LINEAR PROCESSES
0
2
4
6
8
10
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Lag
ACF
0
2
4
6
8
10
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
(b)
Lag
ACF
Figure 2.11
Exact autocorrelation function (ACF) of an ARMA(1, 1) model. (a)
φ = 0.5 and θ = 0.7, (b) φ = 0.5 and θ = −0.7.
2.9.1
Sample ACF
The sample counterparts of the ACF are shown in the following ﬁgures. All
these illustrations are based on 1000 simulated observations of the respective
processes.
For instance, Figure 2.12 depicts the sample ACF of an AR(1) time series
model with autoregressive parameter φ = 0.5. Note that the sample ACF
decays as expected from Figure 2.8(a).
Similarly, Figure 2.13 exhibits the sample version of the ACF for an AR(1)
model with autoregressive parameter φ = 0.9. This empirical ACF can be
compared to its theoretical counterpart depicted on Figure 2.8(b).
The behavior of the sample ACF for two ARMA(1,1) models is exhibited in
Figures 2.14 and 2.15. The model in Figure 2.14 has parameters φ = 0.5 and
θ = 0.7, while the model in Figure 2.15 has parameters φ = 0.5 and θ = −0.7.
Observe that these two plots are close to their theoretical versions shown in
Figure 2.11.

ACF AND PARTIAL ACF FUNCTIONS
61
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
Figure 2.12
Sample autocorrelation function (SACF) of an AR(1) model with
φ = 0.5, based on 1000 simulated observations from the process.
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
Figure 2.13
Sample autocorrelation function (SACF) of an AR(1) model with
φ = 0.9, based on 1000 simulated observations from the process.

62
LINEAR PROCESSES
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
Figure 2.14
Sample autocorrelation function (SACF) of an ARMA(1, 1) model
with φ = 0.5 and θ = 0.7.
0
5
10
15
20
25
30
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
Figure 2.15
Sample autocorrelation function (SACF) of an ARMA(1, 1) model
with φ = 0.5 and θ = −0.7.

ACF AND PARTIAL ACF FUNCTIONS
63
2.9.2
Partial ACF
Consider the series y1, y2, . . . , yn and the predictors
y1
=
E(y1|y2, . . . , yn),
yn+1
=
E(yn+1|y2, . . . , yn),
for n ≥2 and y1 = y2 = 0. The PACF is the correlation between the errors
e1 = y1 −y1 and en+1 = yn+1 −yn+1, that is,
α(n) =
Cov(en+1, e1)

Var(en+1) Var(e1)
.
Observe that, based on the deﬁnition of the predictors y1 and y0, we have
that the prediction errors are e1 = y1 and e2 = y2 and then α(1) = ρ(1).
Even though this correlation may appear as diﬃcult to interpret, for an
AR(p) α(h) = 0 for h > p.
Thus, it allows for the easy identiﬁcation of
an autoregressive process. Figure 2.16 depicts the PACF of an AR(2) model
with parameters φ1 = 0.4 and φ2 = 0.2.
Figure 2.16(a) shows the exact
PACF, while Figure 2.16(b) exhibits the ample PACF based on a series of 1000
observations. Notice that the theoretical PACF vanishes after the second lag.
Furthermore, its sample version shows a similar behavior when the conﬁdence
bands are taken into account.
5
10
15
20
-0.2
0.0
0.2
0.4
0.6
0.8
(a)
Lag
Partial ACF
0
5
10
15
20
25
30
-0.2
0.0
0.2
0.4
0.6
0.8
Lag
Partial ACF
(b)
Figure 2.16
Partial autocorrelation function (PACF) of an AR(2) model with
φ1 = 0.4 and φ2 = 0.2. (a) Exact PACF, (b) Sample PACF based on a series of
1000 observations.

64
LINEAR PROCESSES
2.10
ARFIMA PROCESSES
In this section, we focus our attention on a particular class of linear time series
called long-memory or long-range-dependent processes. There are several def-
initions of this type of time series in the literature. One fundamentals aspect
is related to the estimation of the mean of a process.
If the autocovariance function of a stationary process is summable, then the
sample mean is root-n consistent, where n is the sample size. This is the case,
for instance, for sequences of independent and identically distributed random
variables or Markovian processes.
Generally speaking, these processes are
said to have short memory. On the contrary, a process has long memory if its
autocovariances are not absolutely summable.
In what follows, we provide a brief overview of these class of time series
models.
2.10.1
Long-Memory Processes
Let γ(h) = ⟨yt, yt+h⟩be the autocovariance function at lag h of the stationary
process {yt : t ∈Z}. A usual deﬁnition of long memory is that
∞
X
h=−∞
|γ(h)| = ∞.
(2.19)
However, there are alternative deﬁnitions. In particular, long memory can be
deﬁned by specifying a hyperbolic decay of the autocovariances
γ(h) ∼h2d−1ℓ1(h),
(2.20)
as h →∞, where d is the so-called long-memory parameter and ℓ1(·) is a
slowly varying function. Recall that a positive measurable function deﬁned
on some neighborhood [a, ∞) of inﬁnity is said to be slowly varying if for
any c > 0, ℓ(cx)/ℓ(x) converges to 1 as x tends to inﬁnity.
Examples of
slowly varying functions are ℓ(x) = log(x) and ℓ(x) = b, where b is a positive
constant. Hereafter, the notation xn ∼yn means that xn/yn →1 as n →∞,
unless speciﬁed otherwise.
A well-known class of long-memory models is the autoregressive fractionally
integrated moving-average (ARFIMA) processes. An ARFIMA process {yt}
may be deﬁned by
φ(B)yt = θ(B)(1 −B)−dεt,
(2.21)
where φ(B) = 1 + φ1B + · · · + φpBp and θ(B) = 1 + θ1B + · · · + θqBq are
the autoregressive and moving-average operators, respectively; φ(B) and θ(B)
have no common roots, (1 −B)−d is a fractional diﬀerencing operator deﬁned
by the binomial expansion
(1 −B)−d =
∞
X
j=0
ηjBj = η(B),

ARFIMA PROCESSES
65
where
ηj =
Γ(j + d)
Γ(j + 1)Γ(d),
(2.22)
for d < 1
2, d ̸= 0, −1, −2, . . . , and {εt} is a white noise sequence with ﬁnite
variance.
Consider the ARFIMA process deﬁned by (2.21). Assume that the poly-
nomials φ(·) and θ(·) have no common zeros and that d ∈(−1, 1
2). Then, the
stationarity, causality, and invertibility of an ARFIMA model can be estab-
lished as follows.
(a) If the zeros of φ(·) lie outside the unit circle {z : |z| = 1}, then there is
a unique stationary solution of (2.21) given by
yt =
∞
X
j=−∞
ψjεt−j,
where ψ(z) = (1 −z)−dθ(z)/φ(z).
(b) If the zeros of φ(·) lie outside the closed unit disk {z: |z| ≤1}, then the
solution {yt} is causal.
(c) If the zeros of θ(·) lie outside the closed unit disk {z: |z| ≤1}, then the
solution {yt} is invertible.
2.10.2
Linear Representations
Inﬁnite AR and MA expansions for an ARFIMA process can be described
as follows.
Under the assumption that the roots of the polynomials φ(B)
and θ(B) are outside the closed unit disk {z : |z| ≤1} and d ∈(−1, 1
2), the
ARFIMA(p, d, q) process is stationary, causal, and invertible. In this case we
can write
yt = (1 −B)−dφ(B)−1θ(B)εt = ψ(B)εt,
and
εt = (1 −B)dφ(B)θ(B)−1yt = π(B)yt.
The MA(∞) coeﬃcients, ψj, and AR(∞) coeﬃcients, πj, satisfy the following
asymptotic relationships,
ψj
∼
θ(1)
φ(1)
jd−1
Γ(d),
(2.23)
πj
∼
φ(1)
θ(1)
j−d−1
Γ(−d),
(2.24)

66
LINEAR PROCESSES
as j →∞.
For a fractional noise process with long-memory parameter d, these coeﬃ-
cients are given by
ψj
=
jY
t=1
t −1 + d
t
=
Γ(j + d)
Γ(d)Γ(j + 1),
πj
=
jY
t=1
t −1 −d
t
=
Γ(j −d)
Γ(−d)Γ(j + 1),
for j ≥1 and ψ0 = π0 = 1.
2.10.3
Autocovariance Function
The autocovariance function of the ARFIMA(0, d, 0) process is given by
γ0(h) = σ2
Γ(1 −2d)
Γ(1 −d)Γ(d)
Γ(h + d)
Γ(1 + h −d),
(2.25)
where Γ(·) is the gamma function and the ACF is
ρ0(h) = Γ(1 −d)
Γ(d)
Γ(h + d)
Γ(1 + h −d).
For the general ARFIMA(p, d, q) process, observe that the polynomial φ(B)
in (2.21) may be written as
φ(B) =
p
Y
i=1
(1 −ρiB).
Assuming that all the roots of φ(B) have multiplicity one, it can be deduced
that
γ(h) = σ2
q
X
i=−q
p
X
j=1
ψ(i)ξjC(d, p + i −h, ρj),
(2.26)
with
ψ(i) =
min(q,q+i)
X
k=max(0,i)
θkθk−i,
ξj =

ρj
p
Y
i=1
(1 −ρiρj)
Y
m̸=j
(ρj −ρm)


−1
,

ARFIMA PROCESSES
67
and
C(d, h, ρ) = γ0(h)
σ2
[ρ2pβ(h) + β(−h) −1],
(2.27)
where β(h) = F(d + h, 1, 1 −d + h, ρ) and F(a, b, c, x) is the Gaussian hyper-
geometric function
F(a, b, c, x) = 1 + a · b
γ · 1x + a · (a + 1) · b · (b + 1)
γ · (γ + 1) · 1 · 2
x2 + · · · .
It can be shown that
γ(h) ∼cγ|h|2d−1,
(2.28)
as |h| →∞, where
cγ = σ2
π
|θ(1)|2
|φ(1)|2 Γ(1 −2d)sin(πd).
2.10.4
Sample Mean
Let y1, y2, . . . , yn be a sample from an ARFIMA(p, d, q) process, and let ¯y be
the sample mean. The variance of ¯y is given by
Var(¯y) = 1
n

2
n−1
X
j=1

1 −j
n

γ(j) + γ(0)

.
By formula (2.28), γ(j) ∼cγj2d−1 for large j. Hence, for large n, we have
Var(¯y)
∼
2cγn2d−1
n−1
X
j=1

1 −j
n
  j
n
2d−1 1
n
∼
2cγn2d−1
Z 1
0
(1 −t)t2d−1dt
∼
cγ
d(2d + 1)n2d−1.
(2.29)
2.10.5
Partial Autocorrelations
Explicit expressions for the PACF for the general ARFIMA model are diﬃ-
cult to ﬁnd. In the particular case of a fractional noise process FN(d), the
coeﬃcients of the best linear predictor
byn+1 = φn1yn + · · · + φnny1
are given by
φnj = −
n
j
Γ(j −d)Γ(n −d −j + 1)
Γ(−d)Γ(n −d + 1)
,

68
LINEAR PROCESSES
for j = 1, . . . , n. Thus, the partial autocorrelations are simply
φnn =
d
n −d
(2.30)
and then φnn ∼d/n for large n.
2.10.6
Illustrations
In the following two simulation examples, we illustrate some of the concepts
discussed in the previous sections about ARFIMA processes. For simplicity,
consider the family of ARFIMA(1, d, 1) models
(1 + φB)yt = (1 + θB)(1 −B)−dεt,
where the white noise sequence satisﬁes {εt} ∼N(0, 1). In these examples,
the sample autocorrelation function has been calculated by ﬁrst estimating
the autocovariance function by means of the expression
bγ(h) = 1
n
n−h
X
t=1
(yt −¯y)(yt+h −¯y),
for h = 0, . . . , n −1, where ¯y is the sample mean and then deﬁning the
autocorrelation function estimate as:
bρ(h) = bγ(h)
bγ(0) .
The theoretical values of the ACF were calculated as follows. According to
formula (2.26), the autocovariance function for the process {yt} is given by
γ(h) = θC(d, −h, −φ) + (1 + θ2)C(d, 1 −h, −φ) + θC(d, 2 −h, −φ)
φ(φ2 −1)
,
where the function C(·, ·, ·) is deﬁned in (2.27).
Hence, we have ρ(h) =
γ(h)/γ(0). On the other hand, the theoretical PACF of a fractional noise
process is given by (2.30) while the theoretical PACF of the ARFIMA(1, d, 1)
model can be computed by means of the Durbin-Levinson algorithm; see
Chapter 5 for further details.
EXAMPLE 2.7
Figure 2.17 shows 1000 simulated values from an ARFIMA(0, d, 0) pro-
cess with d = 0.4. The theoretical and the empirical ACF are shown in
Figure 2.18.
From Figure 2.17, note that this time series exhibits a persistence
in its values, they tend to stay at a certain level for a while and then

ARFIMA PROCESSES
69
Time
Series
0
200
400
600
800
1000
-4
-2
0
2
4
Figure 2.17
Simulated ARFIMA(0, d, 0) time series with 1000 observations with
d = 0.4.
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Lag
ACF
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 2.18
ACF of an ARFIMA(0, d, 0) time series d = 0.4. (a) Theoretical
ACF, (b) Sample ACF

70
LINEAR PROCESSES
jump to another level. The ﬁrst behavior is known as the Joseph eﬀect
while the second is known as the Noah eﬀect, that is, abrupt events that
change dramatically the level of the observations.
On the other hand, notice from Figure 2.18 that the sample autocor-
relations are signiﬁcant, even after a large number of lags. This behavior
is expected from Figure 2.18(a), which shows its theoretical counterpart.
EXAMPLE 2.8
Figure 2.19 exhibits 1000 observations from an ARFIMA(1, d, 1) process
with d = 0.3, φ = 0.6, and θ = −0.2. Notice the persistence of the values
and the apparent long-term trend.
On the other hand, Figure 2.20 displays both the theoretical and a
sample ACF of this model. Observe the slowly decaying of both ACF
and the similarity between the exact and the empirical values.
It is
clear that this time series displays signiﬁcant empirical ACF levels even
beyond lag 20.
Time
Series
0
200
400
600
800
1000
-5
0
5
Figure 2.19
Simulated ARFIMA(1, d, 1) time series of 1000 observations with
d = 0.3, φ = 0.6, and θ = −0.2.

FRACTIONAL GAUSSIAN NOISE
71
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Lag
ACF
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 2.20
ACF of an ARFIMA(1, d, 1) time series with d = 0.3, φ = 0.6, and
θ = −0.2. (a) Theoretical ACF, (b) Sample ACF.
2.11
FRACTIONAL GAUSSIAN NOISE
Another well-known long-range-dependent process is the so-called fractional
Gaussian noise (fGn).
This process may be deﬁned as follows.
Consider
the fractional Brownian motion Bd(t) deﬁned in the Section A.2.8, and let
{yt : t ∈Z} be deﬁned by the increments of Bd(t):
yt = Bd(t + 1) −Bd(t).
(2.31)
The discrete-time process {yt : t ∈Z} is called fractional Gaussian noise (fGn).
Let {yt : t ∈Z} be the process deﬁned by (2.31). Then
(a) {yt : t ∈Z} is stationary for d ∈(−1
2, 1
2).
(b) E(yt) = 0.
(c) E(y2
t ) = E[B(1)2].
(d) The autocovariance function of {yt : t ∈Z} is
γ(h) = σ2
2

|h + 1|2d+1 −2|h|2d+1 + |h −1|2d+1
,

72
LINEAR PROCESSES
where σ2 = Var(yt).
(e) For d ̸= 0, the asymptotic behavior of the ACF is given by
γ(h) ∼σ2d(2d + 1) |h|2d−1,
as |h| →∞.
2.11.1
Sample Mean
Let ¯yn be the sample mean of an fGn described by (2.31). Then, by telescopic
sum, we have
¯yn = 1
n[Bd(n + 1) −Bd(1)].
Thus, an application of formula (A.11) yields
Var(¯yn) = σ2n2d−1.
Consequently, since the process {Bd(t)} is Gaussian, we conclude that
¯yn ∼N(0, σ2n2d−1),
for all n ∈N.
2.12
BIBLIOGRAPHIC NOTES
Several textbooks address the concept of linear processes and ARMA models;
see, for example, Box, Jenkins, and Reinsel (1994) and Brockwell and Davis
(2002). The book by Shumway and Stoﬀer (2011) provides another excellent
treatment of the fundamental time series techniques. ARFIMA models are
discussed, for instance, in Doukhan, Oppenheim, and Taqqu (2003), Rangara-
jan and Ding (2003), Teyssi`ere and Kirman (2007), and Palma (2007), among
others. Deﬁnitions of long-memory processes have been extensively discussed
in the literature; see, for instance, Chapter 3 of Palma (2007). The articles by
Cox (1984) and Hall (1997) give overviews about diﬀerent deﬁnitions of long-
range dependence. On the other hand, the paper by Hosking (1981) discusses
several properties of ARFIMA models, including results about stationarity,
invertibility, autocorrelations, and the like. Formulas for the exact autoco-
variance function of an ARFIMA process were established by Sowell (1992).
A nice review of fractional Gaussian noise processes and their properties is
given in Taqqu (2003).
Problems
2.1
Let yt = εt+X εt−1 be a sequence of independent identically distributed
(i.i.d.) random variables, where X is a random variable with mean µ and

PROBLEMS
73
variance σ2
X, {εt} is a sequence of i.i.d. random variables with zero-mean and
variance σ2
ε. Furthermore, X and {εt} are independent. Is {yt} a weakly
stationary process? If this is true, calculate the autocovariance function of
{yt}.
2.2
Let {εt} be a sequence of independent identically distributed random
variables with zero-mean and variance σ2. Is the sequence {yt : t ∈Z} deﬁned
by yt = εt+εt−1+εt−2 εt−3 a linear process? Is the process yt = εt+εt−1+εt−3
causal?
2.3
Can a stationary process be non-causal? Discuss.
2.4
Let the process {yt : t ∈Z} be deﬁned by yt = φ yt−1 + εt where {εt} is
a sequence of identically distributed Student t random variables with ν = 2
degrees of freedom. Is this process second order stationary?
2.5
Consider the ARMA(1, 1) model
yt −φyt−1 = εt + θεt−1,
where εt ∼WN(0, σ2) with |φ| < 1, |θ| < 1
(a) Let Ψ(z) = (1 −φz)−1(1 + θz) = P∞
j=0 ψjzj. Verify that ψ0 = 1,
ψj = φj−1(φ + θ) j ≥1
(b) Starting from the fact that for k ≥0
γ(k) = σ2
∞
X
j=0
ψjψj+k,
ﬁnd ρ(k) = γ(k)
γ(0) for k ≥1.
2.6
Consider the ARMA(2, 2) model,
yt −0.2yt−1 + 0.5yt−2 = εt + 2εt−1 −εt−2.
Verify if this process is stationary and invertible.
2.7
Consider the ARMA(2,1) process
yt = 1.3 yt−1 −0.4 yt−2 + zt + zt−1.
(a) Specify if the process is stationary and invertible.
(b) Calculate the autocovariance function of yt.
2.8
Let yt be a process satisfying
yt = α + βt + ηt
ηt = φηt−1 + ϵt,
where ϵt is white noise (0, σ2
ϵ ).

74
LINEAR PROCESSES
(a) Assume that the parameter φ is known and equal to 1. What would
you do for modeling yt?
(b) Assume now that you only know that |φ| < 1. What would you do
for modeling yt?
2.9
Consider the following ARMA(2,1) model
yt −yt−1 + 0.1yt−2 = ϵt −0.3ϵt,
where ϵt is white noise (0, σ2
ϵ ).
(a) Show that this process is causal and invertible.
(b) Calculate the coeﬃcients of the Wold expansion.
(c) Calculate the ACF of yt.
2.10
Consider the process {xt} given
xt = φ xt−s + zt,
with {zt} ∼WN(0, σ2), |φ| < 1 and s ∈N. Determine the partial autocorre-
lation function of xt.
2.11
The autocorrelation and partial autocorrelation functions are key tools
for identifying ARMA processes. To assess their ability to recognize ARMA
we simulated six processes and plotted their autocorrelations, see Figure 2.21
to Figure 2.26. Identify the following processes justifying your decision:
(a) (1 −0.5 B) yt = (1 + 0.5 B) zt.
(b) (1 −0.6 B) yt = zt.
(c) yt = (1 + 1.3 B + 0.4 B2) zt.
(d) (1 −0.6 B + 0.05 B2) yt = (1 + 0.7 B) zt.
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
-0.2
0.0
0.2
0.4
0.6
Lag
Partial ACF
(b)
Figure 2.21
Time series I, (a) Sample ACF, (b) Sample PACF.
2.12
Explain the following concepts:
(a) Strict stationarity.
(b) Second order stationarity.

PROBLEMS
75
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
-0.1
0.0
0.1
0.2
0.3
0.4
0.5
Lag
Partial ACF
(b)
Figure 2.22
Time series II, (a) Sample ACF, (b) Sample PACF.
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
-0.4
-0.2
0.0
0.2
0.4
Lag
Partial ACF
(b)
Figure 2.23
Time series III, (a) Sample ACF, (b) Sample PACF.
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
-0.2
0.0
0.2
0.4
0.6
Lag
Partial ACF
(b)
Figure 2.24
Time series IV, (a) Sample ACF, (b) Sample PACF.
(c) How is the decaying of the ACF of an ARMA or an ARFIMA pro-
cess?

76
LINEAR PROCESSES
0
5
10
15
20
25
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
-0.4
-0.3
-0.2
-0.1
0.0
0.1
Lag
Partial ACF
(b)
Figure 2.25
Time series V, (a) Sample ACF, (b) Sample PACF.
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
-0.10
-0.05
0.00
0.05
Lag
Partial ACF
(b)
Figure 2.26
Time series VI, (a) Sample ACF, (b) Sample PACF.
2.13
Show that if {yt} is stationary and |θ| < 1, then for each t, m
j=1 θjyt+1−j
converges in mean square error as m →∞.
2.14
Consider the process ARMA(2,1)
yt = 1.3yt−1 −0.4yt−2 + zt + zt−1.
(a) Analyze if this model is stationary and invertible.
(b) Calculate the ACF of this process.
2.15
Consider the following ARMA(2,1) model,
yt −yt−1 + 0.1yt−2 = ϵt −0.3ϵt,
where ϵt is a white noise sequence with (0, σ2
ϵ ).
(a) Show that this time series is causal and invertible.
(b) Calculate the coeﬃcients of its Wold expansion.
(c) Calculate the ACF of yt.

PROBLEMS
77
2.16
Consider the ARMA(1,1) time series yt deﬁned by
yt −φ yt−1 = εt + θ εt−1,
with σ2 = Var(εt) = 1 and let η = (φ, ψ).
(a) Show that a state space representation of yt is given by
xt+1 = φ xt + ψ εt
yt = xt + εt,
and ﬁnd ψ.
(b) For which values of η the observed process is stationary?
(c) Plot a trajectory of yt for diﬀerent values of η.
2.17
Verify if the time series
yt = 0.20yt−1 + 0.63yt−2 + ϵt + 2.3ϵt−1 −0.50ϵt−2
where ϵt ∼WN(0, σ2) is invertible and stationary.
2.18
Consider the following processes and verify if they are causal, station-
ary and invertible,
(a) yt −0.4yt−1 + 0.03yt−2 = εt, where εt are independent random vari-
ables with Student t distribution of 6 degree of freedom.
(b) yt = εt+1+0.45εt, where εt is a white noise sequence with distribution
N(0, 1).
(c) yt = εt + (α −β) P∞
j=1 αj−1εt−j, where εt is a white noise sequence
with variance σ2
ε, |α| < 1 and |β| < 1.
2.19
Let yt be an ARMA(1,2) time series where Var εt = σ2
ε and
yt −0.5yt−1 = εt −εt−1 + 0.24εt−2.
(a) Verify if this model is causal, stationary and invertible.
(b) Find the ﬁrst ﬁve terms of the Wold expansion of this process,
ψ1, . . . , ψ5.
(c) Find the ACF.
2.20
Show that an ARFIMA model satisﬁes the deﬁnition of a long-memory
process described in this chapter and discuss alternative deﬁnitions of strongly
dependent time series models.
2.21
Let yt = εt yt−1 + εt−1 where εt is a white noise with zero-mean and
unit variance. Is {yt : t ∈Z} a lineal process? Is it causal? Calculate E(yt).
2.22
The ACF of an ARFIMA(0, d, 0) process is given by
γ(h) = σ2
Γ(1 −2d)
Γ(1 −d)Γ(d)
Γ(h + d)
Γ(1 + h −d),

78
LINEAR PROCESSES
where Γ(·) is the gamma function. Besides, the PACF is
φnn = −Γ(n −d)Γ(1 −d)
Γ(−d)Γ(n −d + 1).
(a) Show that as h →∞, we have
γ(h) ∼σ2
Γ(1 −2d)
Γ(1 −d)Γ(d)h2d−1.
(b) Verify that as n tends to inﬁnity, the PACF behaves as
φnn ∼d
n.
Hint: Γ(x + 1) = x Γ(x).
2.23
Prove that
f(λ) ∼σ2
2π
|θ(1)|2
|φ(1)|2 |λ|−2d,
for |λ| →0. Hint: sin(x) ∼x for |x| →0.
2.24
Calculate explicitly the autocovariance function of an ARFIMA(1, d, 0)
and ARFIMA(0, d, 1).
2.25
Use Stirling’s approximation
Γ(x) ∼
√
2πe1−x(x −1)x−1/2,
as x →∞, to show that
Γ(n + α)
Γ(n + β) ∼nα−β,
(2.32)
as n →∞.
2.26
Applying Stirling’s approximation, show directly that for an ARFIMA(0, d, 0)
process the following asymptotic expressions hold
ψk
∼
kd−1
Γ(d)
πk
∼
k−d−1
Γ(−d)
ρ(k)
∼
k2d−1 Γ(1 −d)
Γ(d)
as k →∞.

PROBLEMS
79
2.27
Show that the AR(∞) and MA(∞) coeﬃcients of an ARFIMA(p, d, q)
process satisfy
ψj
∼
θ(1)
φ(1)
jd−1
Γ(d)
πj
∼
φ(1)
θ(1)
j−d−1
Γ(−d)
as j →∞.
2.28
Using the following formula involving the gamma function
Γ(1 −d)Γ(d) =
π
sin(πd),
show that the constant cγ appearing in expression (2.28) may be written as
cγ = σ2 |θ(1)|2
|φ(1)|2
Γ(1 −2d)
Γ(1 −d)Γ(d).
2.29
Show that the autocovariance function of a fractional Gaussian noise
is positive for d ∈(0, 1
2) and negative for d ∈(−1
2, 0).
2.30
Let ηj be the MA(∞) coeﬃcients of an ARFIMA(0, d, 0) process with
d ∈(0, 1
2) and deﬁne ϕ0 = 1 and ϕj = ηj −ηj−1 for j = 1, 2, . . . .
(a) Verify that ϕj = d−1
j ηj−1.
(b) Show that ϕj ∼
jd−2
Γ(d−1) as j →∞.
(c) Prove that
∞
X
j=0
ϕ2
j = Γ(3 −2d)
Γ(2 −d)2 .
2.31
Consider the linear process {yt} with Wold expansion
yt = εt +
∞
X
j=1
εt−j
j
,
where {εt} is a white noise sequence with unit variance.
(a) Show that the autocovariance function of this process is
γ(h) = 1
|h| + 1
|h|
|h|
X
j=1
1
j ,
for |h| > 0 and
γ(0) = 1 + π2
6 .

80
LINEAR PROCESSES
(b) Show that
γ(h) ∼log h
h
,
as h →∞.
(c) Verify that for any m > n > 0,
m
X
h=n
γ(h) >
m
X
h=n
1
h.
(d) Is {yt} a long-memory process?
Hint: The following formulas may be useful,
∞
X
j=1
1
j(j + h) = 1
h
h
X
j=1
1
j ,
for h > 0,
h
X
j=1
1
j = C + log h + O
 1
h

,
where C is the Euler’ constant, C = 0.5772 · · ·
∞
X
j=1
1
j2 = π2
6 .
2.32
Let γj be the autocovariance function of a fractional noise FN(d) with
d < 1
4 and white noise variance σ2.
(a) Show that
∞
X
j=−∞
γ2
j = σ4 Γ(1 −4d)
Γ2(1 −2d).
(b) Prove that
∞
X
j=0
γjγj+1 = σ4
d Γ(1 −4d)
(1 −2d)Γ2(1 −2d).
2.33
Show that for d ∈(−1
2, 1
2),
(1 −φ)−d =
∞
X
j=0
ψjφj,

PROBLEMS
81
where
ψj =
Γ(j + d)
Γ(j + 1)Γ(d).
2.34
Prove that for d ∈(−1
2, 1
2),
∞
X
j=1
jψjφj−1 = ∂
∂φ(1 −φ)−d =
d
(1 −φ)1+d .
2.35
Show that the two following MA(1) processes
xt = zt + θzt−1, {zt} ∼WN(0, σ2)
yt = ˜Zt + 1
θ
˜Zt−1, { ˜Zt} ∼WN(0, σ2θ2)
where 0 < |θ| < 1, have the same autocovariance function.
2.36
Show that
Z π
−π
ei(k−h)λdλ =

2π,
if
k = h,
0,
if
k ̸= h.
2.37
Consider the autoregressive process of order 1 deﬁned by
xt −µ = α(xt−1 −µ) + εt
where {εt} is white noise and −1 < α < 1.
(a) Show that given n observations x1, . . . , xn the least squares estimator
that minimizes
S =
n
X
t=2
[xt −µ −α(xt−1 −µ)]2
is given by
ˆµ = x(2) −ˆα x(1)
1 −ˆα
and
ˆα =
Pn−1
t=1 (xt −ˆµ)(xt+1 −ˆµ)
Pn−1
t=1 (xt −ˆµ)2
where x(1), x(2) are the averages of the ﬁrst and last n −1 observations,
respectively.
(b) Show that if we can make the approximation
x(1) ∼= x(2) ∼= x

82
LINEAR PROCESSES
then
ˆµ = x
and
ˆα =
PN−1
t=1 (xt −x)(xt+1 −x)
Pn−1
t=1 (xt −x)2
(c) Find the autocorrelation function for an AR(1) process. In view of this,
why ˆα given in part (b) is a reasonable estimate of α?
(d) Show that the spectral density of this process is given by
f(ω) =
σ2
x(1 −α2)
π(1 −2α cos(ω) + α2).
2.38
Suppose that the yt series follows the model yt = yt−1 + εt with
εt ∼WN(σ2
ε). Is yt stationary? If this sequence is not stationary, propose a
transformation to induce stationarity.
2.39
Suppose that the series {yt} is modeled by yt = µt + xt, where µt =
µt−1 + εt is not observable and xt = ωt −θωt−1. Assume that εt ∼WN(σ2
ε),
ωt ∼WN(σ2
ω), and Cov(εt, ωs) = 0 for all (t, s). Please answer the following
questions:
(a) What is the mean and the variance of yt?
(b) Is yt stationary?
(c) If part (b) is true, what is the autocovariance function of yt?
(d) If part (b) is false, suggest an appropriate transformation to induce sta-
tionarity in yt. Then, calculate the mean and autocovariance function
of the transformed stationary series.
2.40
Let {xt} a sequence of independent random variables distributed as
xt ∼
 exp(1),
t even,
N(0, 1),
t odd.
(a) Is {xt} stationary?
(b) Is {xt} strictly stationary?
2.41
Let {zt} be a sequence of i.i.d. Bernoulli random variables with pa-
rameter 1
2. Decide on the stationarity of the following series:
(a) {xt; t ∈{0, ±1, ±2, . . .}} where if t is odd, xt is the value of a normal
observation with mean 1
2 and variance 1
4, while for t even, xt = zt.
(b) x0 = c0; xt = 0.6xt−1 + ϵt where ϵt is a sequence of independent
identically distributed random variables with zero-mean.

PROBLEMS
83
2.42
Which of the following processes is weakly stationary for T = {0, 1, 2, . . .}
where ϵt is a sequence of random errors with zero-mean and variance 1 and
a1 and a2 are real constants
(a) ϵ1 + ϵ2 cos(t)
(b) ϵ1 + ϵ2 cos(t) + ϵ3 cos(t)
(c) a1 + ϵ1 cos(t)
(d) a1 + ϵ1at
1 + ϵ2
2.43
Using a normal random number generator, generate 100 observations
of the following series, plot and discuss the diﬀerences.
(a) AR(1), φ = 0.6
(b) MA(1), θ = −0.6
(c) ARMA(1,1) con φ = 0.6 and θ = −0.6
2.44
Consider a series modeled by the following process:
(1 −0.82B + 0.22B2 + 0.28B4)[log(z) −µ] = εt,
where εt is white noise sequence.
(a) Factorize the autoregressive operator, and explain the aspects that
the factorization reveals regarding the autocorrelation function and
the periodic components of this series.
(b) What is the formula which allows the forecasting of this series?
2.45
Let {ϵt}t≥1 be an i.i.d. sequence of random variables N(µ, σ2) and θ
real parameter. Consider the sequence {xt}t≥1 deﬁned by:
x1 = ϵ1,
xt = θxt−1 + ϵt
(t ≥2).
In what follows, consider µ = 0.
(a) Calculate V (xt)
(b) Calculate Cov(xt, xt−k), 0 ≤k ≤t
(c) What is the distribution of xt?
(d) For what values of θ, (xt) converges in distribution?
(e) What is the distribution of (x1, x2, . . . , xn)? Calculate its probability
density.
(f) Is this process stationary?
2.46
Let z1, z2 two random variables such that E[z1] = µ1; E[z2] = µ2;
Var[z1] = σ11; Var[z2] = σ22; Cov[z1, z2] = σ12, let the process x(t, ω) be
deﬁned by x(t, ω) = z1(w)IR−∪0(t) + z2(w)IR+(t).
(a) Describe the trajectories of x.
(b) What should be necessary to make the process stationary?
(c) Calculate µx(t) and γx(t1, t2).
(d) Find necessary and suﬃcient conditions on µ1, µ2, σ1, σ2 and σ12
so that the process x is second order stationary. In this case, ﬁnd
autocovariance and autocorrelation functions.

84
LINEAR PROCESSES
2.47
The sequence W(t), t ∈R is called a Wiener process if it satisﬁes
1. W(0)=0.
2. W(t2) −W(t1) ∼N(0, σ2(t2 −t1)), t2 > t1.
3. If t0 ≤t1 ≤· · · ≤tn, then W(t1) −W(t0), W(t2) −W(t1), . . . , W(tn) −
W(tn−1) are independent.
(a) Calculate the mean µW (t) and Cov(Wt1, Wt2).
(b) Is W(t) a second-order stationary process?
2.48
Find the autocorrelation function of the MA(2) process given by
xt = ϵt + 0.4 ϵt−1 −0.2 ϵt−2.
2.49
Calculate the autocorrelation function of the MA(n) process given by
xt =
n
X
k=0
ϵt−k
n + 1.
2.50
Find the ACF of the AR(1) process deﬁned by xt = 0.7 xt−1 + ϵt and
plot ρx(k) for lags k = ±6, ±5, ±4, ±3, ±2, ±1.
2.51
Let xt be a process given by xt = µ + ϵt + βϵt−1, where µ ∈R is a
constant. Show that the ACF does not depend on µ.
2.52
Find the values of λ1, λ2 ∈R such that the AR(2) process xt =
λ1 xt−1 + λ2 xt−2 + ϵt is stationary.
2.53
Assume that γx(k) is the ACF of the stationary process {xt, t ∈Z}.
Calculate the ACF of the process ∇xt = xt −xt−1 in terms of γx(k). Find
γ∇x(k) if γx(k) = λ−k.
2.54
Find the MA(∞) representation of the process xt = 0.3 xt−1 + ϵt.
2.55
Suppose that the process {xt, t ∈Z} can be represented as ϵt =
P∞
j=0 cjxt−j as well as xt = P∞
j=0 bjϵt−j.
(a) If c0 = b0 = 1, A(s) = P∞
j=0 cjsj, and B(s) = P∞
j=0 bjsj, show that
A(s) · B(s) = 1.
(b) If R(s) = P∞
k=−∞rX(k)sk, show that
R(s) = σ2
ϵ B(s)B(1/s) = σ2
ϵ [A(s)A(1/s)]−1.
2.56
Consider the MA(2) process given by xt = −1.7+ϵt−0.6ϵt−1+0.3ϵt−2.
(a) Is this a stationary process?
(b) Find µx, γx(0), γx(1), γx(2), γx(3), γx(23), ρx(1), ρx(2), ρx(3), and
ρx(23).

PROBLEMS
85
2.57
Consider the ARMA(1,1) process given by: xt = 0.4xt−1+ϵt−0.8ϵt−1.
(a) Is this a linear process?
(b) Is this a stationary process?
(c) Is this an invertible process?
(d) Find µX, γX(0), γX(1), γX(2), γX(3), ρX(0), ρX(1), ρX(2), and
ρX(3).
(e) If x71 = −5, what is the expected value of x72?
(f) Write the process in an AR form.
(g) Write the process in an MA expansion.
2.58
Let {xt, t ∈Z} be an MA(1) process given by xt = ϵt + αat−1. Show
that |ρx(1)| ≤1.
2.59
Let xt = P∞
j=0 ϵt−j be a linear process.
(a) Is this process stationary?
(b) Is ∇xt a stationary process?
(c) Show that the best linear predictor of xt+k based on {xu : u ≤t} is
xt.
2.60
Consider the AR(2) process xt = φ1xt−1 + φ2xt−2 + ϵt.
Calculate
ρX(k) based on the roots of the model, under the following conditions:
(a) ρx(0) = 1, ρx(1) =
φ1
1−φ2
(b) ρx(0) = 1, ρx(1) = ρx(−1).
Compare and discuss the two results.
2.61
Consider the following stochastic processes
1.- xt = 0.5xt−1 + ϵt.
2.- xt = 1.5xt−1 −0.5xt−2 + ϵt.
(a) In both cases express xt as an MA(∞) and an AR(∞).
(b) Calculate ρx(k) and φkk.
2.62
Let xt be an ARMA(p2, q2) process and let yt be an ARMA(p2, q2)
process, such that xt, yt are independent. Deﬁne zt = xt + yt. Verify that zt
is an ARMA(p, q) process such that p ≤p1 +p2 and q ≤max{p1 +q2, q1 +p2}.
2.63
If {xt, t ∈Z} and {yt, t ∈Z} are stationary, is {axt + byt, t ∈Z} also
stationary?
2.64
If an ARMA(p, q) model with p > 0 and q > 0 is stationary, then
provide conditions such that
(a) It is also invertible.
(b) It can be written as an inﬁnite-order MA model.
(c) It can be written as a ﬁnite-order MA model.
(d) It can be written as a ﬁnite-order AR model.

86
LINEAR PROCESSES
2.65
If W1,t = (1 −θ1 B)ε1,t and W2,t = (1 −θ2 B)ε2,t, where ε1,t and ε2,t
are two independent white noise sequences, show that W3,t = W1,t +W2,t can
be written as W3,t = (1 −θ3 B)ε3,t. Find expressions for θ3 and σ2
3,t in terms
of the corresponding parameters of the other two processes.
2.66
Consider ARMA(1,1) process
yt = 10 + 0.8yt−1 + εt −0.5εt−1
(a) Is this a stationary and an invertible process?
(b) Calculate the mean of yt.
(c) Calculate the autocovariance and autocorrelation functions.
(d) If possible, ﬁnd the AR(∞) and MA(∞) representation.
2.67
Let {yt} be an ARMA time series plus a noise deﬁned by
yt = xt + Wt,
where {Wt} ∼WN(0, σ2
w), {xt} is the ARMA(p,q) process satisfying Φ(B)xt =
Θ(B)εt, {εt} ∼WN(0, σ2
ε) and E(Wszt) = 0 for all s and t.
(a) Show that {xt} is stationary and ﬁnd its autocovariance function in
terms of σ2
w and the autocovariance function of {xt}.
(b) Show that Ut := Φ(B)yt, is r-correlated where r = max(p, q). Con-
clude that {yt} is an ARMA(p, r) process.
2.68
Suppose that {xt} is a non-invertible MA(1) process
xt = εt + θεt−1
{zt} ∼WN(0, σ2)
where |θ| > 1. Deﬁne a new process {Wt} as
Wt =
∞
X
j=0
(−θ)−jxt−j
and show that {Wt} ∼WN(0, σ2
w). Express σ2
w in terms of θ and σ2 and show
that {xt} has representation invertible (in terms of {Wt})
xt = Wt + 1
θWt−1.
2.69
If {xt} denotes the unique stationary solution of the autoregressive
equations
xt = φxt−1 + εt,
t = 0, ±1, . . .
where {εt} ∼WN(0, σ2) and |φ| > 1. Deﬁne the new sequence
Wt = xt −1
φxt−1

PROBLEMS
87
show that {Wt} ∼WN(0, σ2
W ) and express σ2
W in terms of σ2 and φ. Show
that {xt} is the (stationary only) solution of the equations
xt = 1
φxt−1 + Wt,
t = 0, ±1, . . .
2.70
Let {B0(t)} be a fractional Brownian motion with d = 0. Verify that
Cov[B0(t), B0(s)] = min{|t|, |s|}.
2.71
Let {Bd(t)} be a fractional Brownian motion with d ∈(−1
2, 1
2). Show
that this process has stationary increments, that is,
Bd(t + h) −Bd(t) ∼Bd(h) −Bd(0),
for all t, h ∈IR.
2.72
Let {Bd(t)} be a fractional Brownian motion with d ∈(−1
2, 1
2). Prove
that for any p > 0,
E[Bd(t + h) −Bd(t)]p = |h|p(d+1/2) E[B(1)p],
and
E
Bd(t + h) −Bd(t)
h
2
= |h|2d−1.
2.73
Let δ ∈(1, 2), n ≥1. Show that
∞
X
j=1
[j(j + n)]−δ = O(n−δ).
2.74
Show that for a < 1,
lim
m→∞m2β
m−1
X
i=1
ai(m −i)−2β =
a
1 −a.
2.75
Assume that ϵt ∼tν with ν > 4. Given that for even n
E(ϵn
t ) =
Γ
n + 1
2

Γ
ν −n
2

√π Γ
ν
2

νn/2,
prove the following results:

88
LINEAR PROCESSES
(a) The kurtosis of ϵt is
ην = 3
ν −2
ν −4

,
and ην > 3 for any ﬁxed ν.
(b) The kurtosis of the process yt deﬁned by the Wold expansion (6.24)
is
κν =
6
ν −4


∞
X
j=0
ψ2
j


−2 

∞
X
j=0
ψ4
j

+ 3,
and κν →3 as ν →∞.
Hint: Recall that xΓ(x) = Γ(x + 1) and Γ( 1
2) = √π.

CHAPTER 3
STATE SPACE MODELS
The linear processes introduced in the previous chapter were described in
terms of the Wold expansion or an inﬁnite moving-average representation.
However, these processes can also be expressed in terms of a state space linear
system. This chapter is devoted to describe these systems and investigate some
of the relationships between Wold expansions and state space representations.
State space models are very useful for calculating estimators, predictors, and
interpolators. Consequently, Wold expansions and state space representations
of linear time series will be extensively used throughout this book.
This chapter begins with a motivating example of state spaces models in
the context of air pollution.
Section 3.2 introduces the state space linear
systems and discusses a number of fundamental concepts such as stability,
observability, controllability, and minimality. Additionally, three equivalent
representations of a linear process including the Wold decomposition, state
space systems, and the Hankel matrix are analyzed in Section 3.3. Section 3.4
describes the Kalman ﬁlter equations to calculate recursively state estimates,
forecasts, and smoothers along with their variances. This section also discusses
techniques for handling missing values and predicting future observations.
Some extensions of these procedures to incorporate exogenous variables are
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
89

90
STATE SPACE MODELS
described in Section 3.5, and further readings on theoretical and practical
issues of state space modeling are suggested in Section 3.6. A list of problems
is given at the end of the chapter.
3.1
INTRODUCTION
State space models are useful representations of linear processes. However,
these systems can also be helpful to model practical situations.
Suppose
that the air pollution is generated by two sources, xt,1 and xt,2. But, these
components are not directly observed.
The instruments measure a combi-
nation of these two components plus an instrumental error, that is, yt =
α1xt,1 + α2xt,2 + εt, where εt is a white noise sequence with zero-mean and
variance σ. An important problem is estimating the parameters α1, α2, σ
and the magnitude of these sources from the observed data. For simplicity,
suppose that we know from the physical dynamic of the air pollution that the
state variables satisfy the equation
xt+1
=
 0.6
0.1
0.2
0.7

xt +
 σ
0
0
σ

vt,
(a)
Time
xt1
0
200
400
600
800
1000
-0.4
-0.2
0.0
0.2
(b)
Time
xt2
0
200
400
600
800
1000
-0.6
-0.2
0.2
Figure 3.1
State space model example. State values xt, t = 1, . . . , 1000. (a) ﬁrst
component xt,1, (b) second component xt,2.

INTRODUCTION
91
where vt is a state noise two-dimensional vector with zero-mean and variance
I2. On the other hand, suppose that the instrumental measurements satisfy
yt
=
 α1
α2

xt + σ wt,
where wt is a normal white noise with zero-mean and unit variance, indepen-
dent of the state noise vt. Figure 3.1 shows the two components of the state
vector for a simulated process with α1 = 0.7, α2 = 0.3, σ = 0.1 and 1000
observations. The variance of the state vector is given by
Var(xt) =
 0.0172
0.0064
0.0064
0.0237

,
and the variance of the observation sequence is Var(yt) = 0.0243 and standard
deviation sd(yt) = 0.1560. On the other hand, Figure 3.2 displays the values
of the observed process yt. The sample ACF and sample PACF of this series
is exhibited in Figure 3.3. Notice the level of dependence of the observation yt
on its past values. Later in this chapter we will come back to this illustrative
example to show the states and parameter estimates.
Time
yt
0
200
400
600
800
1000
-0.6
-0.4
-0.2
0.0
0.2
0.4
Figure 3.2
State space model example. Observations yt, t = 1, . . . , 1000.

92
STATE SPACE MODELS
0
5
10
15
20
25
30
0.0
0.4
0.8
Lag
ACF
(a)
0
5
10
15
20
25
30
0.0
0.2
0.4
Lag
Partial ACF
(b)
Figure 3.3
State space model example. (a) Sample ACF, and (b) sample PACF
of yt, t = 1, . . . , 1000.
3.2
LINEAR DYNAMICAL SYSTEMS
A more general expression for the linear state space system described in the
previous section is may be described by the discrete-time equations
xt+1
=
F xt + vt,
(3.1)
yt
=
G xt + wt,
(3.2)
where xt is the state vector for all time t, yt is the observation sequence, F is
the state matrix, G is the observation matrix, {vt} is the state white noise se-
quence with zero-mean variance σ2
v and {wt} is the observation error sequence
with zero-mean variance σ2
w. Equation (3.1) is called the state equation while
(3.2) is the observation equation.
3.2.1
Stability
A state space system (3.1)–(3.2) is said to be stable if F n converges to zero
as n tends to ∞. The system is said to be exponentially stable if there exist
positive constants c and α such that ∥F n∥≤ce−αn.
The stability of
a state space model means that the state vector does not explode as time
increases and that the eﬀect of the initial value of the state vanishes as time

LINEAR DYNAMICAL SYSTEMS
93
progresses. In what follows we will assume that the system is stable unless
stated otherwise.
EXAMPLE 3.1
Consider the state space of Section 3.1. In this case,
F =

0.6
0.1
0.2
0.7

.
Note that if λ is an eigenvalue of F associated to the eigenvector x, then
F nx = λnx. Thus, if the eigenvalues of F satisfy |λ| < 1 then λn →0 as
n increases. Consequently, F nx also converges to zero as n →∞. In this
case the eigenvalues of F are 0.8 and 0.5 so they satisfy the convergence
condition.
3.2.2
Hankel Matrix
Suppose that ψ0 = 1 and ψj = GF j−1 ∈IR for all j > 0 such that P∞
j=0 ψ2
j <
∞.
Then from (3.1)-(3.2), the process {yt} may be written as the Wold
expansion
yt =
∞
X
j=0
ψjεt−j.
(3.3)
This linear process can be characterized by the Hankel matrix given by
H =





ψ1
ψ2
ψ3
· · ·
ψ2
ψ3
ψ4
· · ·
ψ3
ψ4
ψ5
· · ·
...
...
...




.
Note that this Hankel matrix speciﬁes the Wold expansion (3.3) and vice
versa. Furthermore, the dimensionality of the state space system (3.1)-(3.2)
is closely related to the dimensionality of the matrix H and to the rationality
of the spectral density of the process (3.3). Speciﬁcally, the rank of H is ﬁnite
if and only if the spectral density of (3.3) is rational.
The class of autoregressive moving-average (ARMA) processes have rational
spectrum, hence the rank of H is ﬁnite for these models. In turn, as we will see
later, this means that any state space system representing an ARMA process is
ﬁnite-dimensional. On the contrary, the class of long-memory processes [e.g.,
autoregressive fractionally integrated moving-average (ARFIMA) models] does
not have rational spectrum. Consequently, all state space systems representing
such models are inﬁnite-dimensional.
Since the state space representation of a linear regular process is not neces-
sarily unique, one may ask which is the minimal dimension of the state vector.

94
STATE SPACE MODELS
In order to answer this question it is necessary to introduce the concepts of
observability and controllability.
3.2.3
Observability
Let O = (G′, F ′G′, F ′2G′, . . . )′ be the observability matrix. The system (3.1)–
(3.2) is said to be observable if and only if O is full rank or, equivalently, O′O
is invertible.
The deﬁnition of observability is related to the problem of determining
the value of the unobserved initial state x0 from a trajectory of the observed
process {y0, y1, . . . } in the absence of state or observational noise. Consider,
for example, the deterministic state space system
xt+1
=
Fxt,
yt
=
Gxt,
and let Y = (y0, y1, . . . )′ be a trajectory of the process. Since
y0
=
Gx0,
y1
=
GFx0,
y3
=
GF 2x0,
...
we may write Y = Ox0. Now, if O′O is full rank, we can determine the value
of the initial state explicitly as x0 = (O′O)−1O′Y .
3.2.4
Controllability
Consider the case where the state error is written in terms of the observation
so that vt = H wt and the state space model can be expressed as
xt+1
=
F xt + H wt,
(3.4)
yt
=
G xt + wt.
(3.5)
Let C = (H, FH, F 2H, . . . ) be the controllability matrix. The system (3.4)-
(3.5) is controllable if C is full rank or C′C is invertible.
The key idea behind the concept of controllability of a system is as follows.
Let Et−1 = (. . . , wt−2, wt−1)′ be the history of the state noise process at time
t and suppose that we want the state to reach a particular value xt. The
question now is whether we can choose an adequate sequence Et−1 to achieve
that goal. In order to answer this question, we may write the state at time t
as
xt = Hwt−1 + FHwt−2 + F 2Hwt−3 + · · · = CEt−1.

LINEAR DYNAMICAL SYSTEMS
95
Thus, if the system is controllable, C′C is full rank and we may write
Et−1 = (C′C)−1C′xt.
For a ﬁnite-dimensional state space system the observability and the con-
trollability matrices may be written as
O
=
(G′, F ′G′, . . . , F ′n−1G′)′,
C
=
(H, FH, . . . , F n−1H),
where n = rank(O) = rank(C).
EXAMPLE 3.2
For the state space of Section 3.1 we have
O =
 0.70
0.45
0.30
0.35

.
Since the rank of this matrox is 2, the system is observable. On the
other hand, suppose that we can write
vt =
 1
2

wt,
where wt is a white noise sequence with zero-mean and unit variance.
Thus, the controllability matrix is given by
C =
 1
0.8
2
1.6

.
Therefore, the system is not controllable. However, if
vt =

1
1

wt,
then
C =

1
0.7
1
0.9

.
So, that this model is controllable.
3.2.5
Minimality
A state space system is minimal if F is of minimal dimension among all
representations of the linear process (3.3). The problem of ﬁnding minimal
representations for ﬁnite-dimensional systems minimality is highly relevant
since a state space representation with the smallest dimension may be easier
to interpret or easier to handle numerically. A state space system is minimal
if and only if it is observable and controllable.

96
STATE SPACE MODELS
3.3
STATE SPACE MODELING OF LINEAR PROCESSES
A linear process may be represented in many diﬀerent forms, for instance, as
a state space system, a Wold decomposition, or by its Hankel matrix. To a
large extent, these representations are equivalent and a key issue is how to
pass from one representation to another. We have to keep in mind, however,
that in most cases these representations are not unique.
3.3.1
State Space Form to Wold Decomposition
Given a state space system (3.4)-(3.5) with the condition that the sequence
{GF jH} is square summable, we may ﬁnd the Wold representation (3.3) by
deﬁning the coeﬃcients ψj = GF j−1H. Observe that the stability of F is not
suﬃcient for assuring the above condition. However, square summability is
guaranteed if F is exponentially stable.
3.3.2
Wold Decomposition to State Space Form
A state space representation of the process (3.3) can be speciﬁed by the state
xt
=
[ y(t|t −1)
y(t + 1|t −1)
y(t + 2|t −1)
· · · ]′ ,
(3.6)
where y(t + j|t −1) = E[yt+j|yt−1, yt−2, . . . ] and
F
=


0
1
0
0
0
· · ·
0
0
1
0
0
· · ·
0
0
0
1
0
...
...
...

,
(3.7)
H
=
[ ψ1
ψ2
ψ3
· · · ]′ ,
(3.8)
G
=
[ 1
0
0
· · · ] ,
(3.9)
yt
=
Gxt + εt.
(3.10)
3.3.3
Hankel Matrix to State Space Form
Let A be a linear operator that selects rows of H such that H0 = AH consists
of the basis rows of H.
Thus, H0 is full rank and consequently H0H′
0 is
invertible.
Given the Hankel representation, a state space system can be
speciﬁed by the state vector
xt = H0(εt−1, εt−2, . . . )′,

STATE ESTIMATION
97
and the system matrices
F = A


ψ2
ψ3
ψ4
· · ·
ψ3
ψ4
ψ5
· · ·
ψ4
ψ5
ψ6
· · ·
...
...
...

H′
0(H0H′
0)−1,
H = A(ψ1, ψ2, . . . )′,
and
G = (ψ1, ψ2, . . . )H′
0(H0H′
0)−1.
Let ej = (0, . . . , 0, 1, 0, 0, . . . ) where the 1 is located at the jth position.
Observe that by induction we can prove that F j−1H0e1 = H0ej. For j = 1
is trivial. Suppose that the assertion is valid for j, we will prove that the
formula holds for j + 1:
F jH0e1
=
FH0ej = A


ψ2
ψ3
ψ4
· · ·
ψ3
ψ4
ψ5
· · ·
ψ4
ψ5
ψ6
· · ·
...
...
...

ej
=
A


ψj+1
ψj+2
ψj+3
...

= AHej+1 = H0ej+1.
Therefore,
GF j−1H
=
(ψ1, ψ2, . . . )H′
0(H0H′
0)−1F j−1H0e1
=
(ψ1, ψ2, . . . )H′
0(H0H′
0)−1H0ej.
On the other hand, since (ψ1, ψ2, . . . ) belongs to span of H, it may be written
as (ψ1, ψ2, . . . ) = bH0. Thus,
GF j−1H = bH0ej = (ψ1, ψ2, . . . )ej = ψj.
3.4
STATE ESTIMATION
The state space model described by (3.1)-(3.2) can be readily extended to
handle time-varying state and observation matrices. This makes the system
versatile enough to model, for instance, nonstationary processes. This more
general state space model can be written as,
xt+1
=
Ft xt + vt,
(3.11)
yt
=
Gt xt + wt
(3.12)

98
STATE SPACE MODELS
where Var(vt) = Σt, Var(wt) = σt and Cov(vt, wt) = Γt. As discussed previ-
ously, in many practical situations the state vector of the model is not directly
observed. We have indirect information of the state xt from the observed se-
quence yt. Based on what information is available for estimating the state,
we may consider the following three cases. If only the past of the process
(. . . , yt−2, yt−1) is available the we proceed to the prediction of the state xt.
If the past and the present of the process (. . . , yt−1, yt) is available, then we
proceed to ﬁltering the state xt. Finally, if the full trajectory of the process is
available (. . . , yt−1, yt, yt+1, . . . ), then we consider smoothing of the state xt.
In what follows we summarize the Kalman recursive equations to ﬁnd state
predictors, ﬁlters, and smoothers. The main purpose of these equations is
to simplify the numerical calculation of the estimators. Since in practice we
usually have only a ﬁnite stretch of data, we focus our attention on projections
onto subspaces generated by a ﬁnite trajectories of the process {yt : t ∈Z}.
3.4.1
State Predictor
Let bxt be the predictor of the state xt based on {ys : 1 ≤s ≤t −1} and
let Ωt = E[(xt −bxt)(xt −bxt)′] be the state error variance, with bx1 = 0 and
Ω1 = E[x1x′
1]. Then, the state predictor bxt is given by the following recursive
equations for t ≥1:
∆t
=
Gt Ωt G′
t + σ2
t ,
(3.13)
Kt
=
(Ft Ωt G′
t + Γt)∆−1
t ,
(3.14)
Ωt+1
=
Ft Ωt F ′
t + Σt −∆t Kt K′
t,
(3.15)
νt
=
yt −Gt bxt,
(3.16)
bxt+1
=
Ft bxt + Kt νt.
(3.17)
The sequence Kt is the Kalman gain and {νt} is the innovation sequence that
represents the part of the observation yt which cannot be predicted from its
past.
3.4.2
State Filter
Deﬁne bxt|t as the conditional expectation of xt based on {ys : 1 ≤s ≤t} and
let Ωt|t = E[(xt −bxt|t)(xt −bxt|t)′] be its error variance, with bx1|1 = 0. Then
the state ﬁlter bxt|t is given by the following recursive equations for t ≥1:
Ωt|t
=
Ωt −Ωt G′
t ∆−1
t
Gt Ωt,
bxt|t
=
bxt + Ωt G′
t ∆−1
t
νt.

STATE ESTIMATION
99
3.4.3
State Smoother
Let bxt|s be conditional expectation of xt based on {yj : 1 ≤j ≤s}. The state
smoother bxt|n is given by the following recursive equations for s ≥t:
bxt|s
=
bxt|s−1 + At,s G′
t ∆−1
s
νs,
At,s+1
=
At,s (Ft −Ks ∆−1
s
Gt)′.
The state smoother error variance is obtained from the equation
Ωt|s = Ωt|s−1 −At,sG′
t ∆−1
s
Gt A′
t,s,
with initial conditions At,t = Ωt|t−1 = Ωt and bxt|t−1 = bxt from (3.15) and
(3.17), respectively.
3.4.4
Missing Values
When the series has missing observations , the Kalman prediction equations
(3.15)–(3.17) must be modiﬁed as follows. If the observation yt is missing,
then
Ωt+1
=
Ft Ωt F ′
t + Σt,
νt
=
0,
bxt+1
=
Ftbxt.
As a consequence, the missing value yt aﬀects the estimation of the state at
time t+1 making the innovation term zero and increasing the state prediction
error variance with respect to the observed yt case since the subtracting term
∆t Kt K′
t appearing in (3.15) is absent in the modiﬁed equations.
Furthermore, when yt is missing, the modiﬁed Kalman ﬁltering equations
are
Ωt|t
=
Ωt,
bxt|t
=
bxt,
and the state smoother equations become
bxt|s
=
bxt|s−1,
At,s+1
=
At,s F ′
t,
Ωt|s
=
Ωt|s−1.
Additional details about this modiﬁcations can be found in Subsection 11.2.4.

100
STATE SPACE MODELS
EXAMPLE 3.3
In order to illustrate the application of the Kalman recursions consider
the air pollution state space model discussed in Section 3.1.
Let θ = c(α1, α2, σ) the parameter describing this model.
The R
package FKF provides a fast implementation of the Kalman recursions.
With the help of this package we obtain the following output:
> fit
$par
ar1
ar2
sigma
0.68476597 0.33517491 0.09911715
$value
[1] -577.6894
$counts
function gradient
178
NA
$convergence
[1] 0
$message
NULL
$hessian
ar1
ar2
sigma
ar1
444.0878
337.6504
8584.973
ar2
337.6504
322.5501
5345.319
sigma 8584.9733 5345.3187 203590.721
The approximated variance-covariance matrix of the parameter esti-
mate bθ is given by
Var(bθ) =
 0.1396
−0.0307
0.0838
1.2299
0.7011
0.1144

.
From this matrix we obtain 95% conﬁdence intervals for the parameters,
(0.1396, 1.2299) for α1, (−0.0307, 0.7011) for α2 and (0.0838, 0.1144) for
σ.
Figure 3.4 and Figure 3.5 show the ﬁrst and second components of
the state predictors, respectively. Note that the predictors follows the
trajectory of the state vectors closely.
The standard deviations of these state predictors are exhibited in
Figure 3.6.
The ﬁrst standard deviations were no plotted since they

STATE ESTIMATION
101
Time
xt1
0
200
400
600
800
1000
-0.4
-0.2
0.0
0.2
Figure 3.4
State space model example. State predictors, t = 1, . . . , 1000. Heavy
line, ﬁrst component predictors. Dotted line, ﬁrst component state values.
represent an initial guess, which is usually a very high value, 100 in this
case. Note that the standard deviations decay quite rapidly after a few
steps and converge to 0.1143 and 0.1298, respectively.
On the other hand, Figure 3.7 and Figure 3.8 exhibit the ﬁltered
state vectors. Observe that the estimated states are closer to their true
counterparts than the predicted states.
The standard deviations of the ﬁltered states is depicted in Figure 3.9.
Notice that for both components these standard deviations converge very
quickly to 0.093 and 0.1200, respectively.
Figure 3.10 display the innovation sequence of this ﬁtted state space
model along with the sample ACF and their standard deviations. Note
that the innovations seem to be uncorrelated.
Finally, the Kalman gain vectors are shown in Figure 3.11. Notice
that both components converge fast to 0.4891 and 0.3669, respectively.
Summarizing, this simple example illustrate the versatility of a state
space system to model complex practical situations where the variables
of interest are not directly observed. In this case, the model can identify
the relative contribution of each pollution source and can estimate them
from the observed data.

102
STATE SPACE MODELS
Time
xt2
0
200
400
600
800
1000
-0.6
-0.4
-0.2
0.0
0.2
0.4
Figure 3.5
State space model example. State predictors, t = 1, . . . , 1000. Heavy
line, second component predictors. Dotted line, second component state values.
(a)
Time
!xt1
5
10
15
20
0.114
0.116
0.118
0.120
0.122
(b)
Time
!xt2
5
10
15
20
0.130
0.132
0.134
Figure 3.6
State space model example. Standard deviations of state predictors,
t = 2, . . . , 20. (a) ﬁrst state component, (b) second state component xt,2.

STATE ESTIMATION
103
Time
xt1
0
200
400
600
800
1000
-0.4
-0.2
0.0
0.2
Figure 3.7
State space model example. States ﬁlters, t = 1, . . . , 1000. Heavy line,
ﬁltered ﬁrst component. Dotted line, ﬁrst component state values.
Time
xt2
0
200
400
600
800
1000
-0.6
-0.4
-0.2
0.0
0.2
0.4
Figure 3.8
State space model example. States ﬁlters, t = 1, . . . , 1000. Heavy line,
ﬁltered second component. Dotted line, second component state values.

104
STATE SPACE MODELS
(a)
Time
!xt1
5
10
15
20
0.092
0.096
0.100
(b)
Time
!xt2
5
10
15
20
0.100
0.105
0.110
0.115
0.120
Figure 3.9
State space model example. Standard deviations of state ﬁlters, t =
1, . . . , 20. (a) ﬁrst state component. (b) second state component xt,2.
(a)
Time
!t
0
200
400
600
800
1000
-0.4
0.0
0.4
0
5
10
15
20
25
30
0.0
0.4
0.8
Lag
ACF
(b)
(c)
Time
!!t
5
10
15
0.136
0.142
Figure 3.10
State space model example. (a) Innovations, (b) sample ACF, and
(c) standard deviations for t = 2, . . . , 20.

STATE ESTIMATION
105
(a)
Time
Kt1
5
10
15
20
0.50
0.52
0.54
0.56
(b)
Time
Kt2
5
10
15
20
0.36
0.40
0.44
0.48
Figure 3.11
State space model example. Kalman gain, t = 1, . . . , 20. (a) ﬁrst
component Kt,1, (b) second component Kt,2.
3.4.5
Additive Noise
State space models allows the handling of ARMA or ARFIMA processes ob-
served with error. As an illustration consider the AR(1) model with additive
noise described by
xt+1
=
φxt + vt,
yt
=
xt + wt,
where xt is an AR(1) underlying process and yt is the observed process, with
error wt which is a white noise sequence. Suppose that Var(vt) = σ2
v and
Var(wt) = σ2
w are the state and observational noise variances, respectively.
Assume also that the state and observational noises are uncorrelated. Notice
that the variance of the state and observed process are
Var(xt)
=
σ2
v
1 −φ2 ,
Var(yt)
=
σ2
v
1 −φ2 + σ2
w.

106
STATE SPACE MODELS
Thus, that the variance ratio is
Var(yt)
Var(xt)
=
1 + σ2
w
σ2v
(1 −φ2),
showing that the increase in variance from xt to yt depend on both the noise
variance ratio and the value of the autoregressive parameter.
The Kalman equations in this case are give by
∆t
=
Ωt + σ2
w,
Kt
=
φ Ωt ∆−1
t ,
Ωt+1
=
φ Ωt + σ2
v −∆t K2
t ,
νt
=
yt −bxt,
bxt+1
=
φ bxt + Kt νt.
These equations can be simpliﬁed as follows,
∆t
=
Ωt + σ2
w,
Kt
=
φ Ωt
Ωt + σ2w
,
Ωt+1
=
(φ2σ2
w + σ2
v) Ωt + σ2
vσ2
w
Ωt + σ2w
,
νt
=
yt −bxt,
bxt+1
=
φ bxt +
φ Ωt
Ωt + σ2w
νt.
Figure 3.12 displays 100 values of the state and observations for a simulated
AR(1) plus noise process with φ = 0.9, σ2
v = 1, σ2
w = 2. The sample ACF and
PACF of the process yt is exhibited in Figure 3.13
The evolutions of ∆t and Ωt for the simulated process is shown in Fig-
ure 3.14. Notice that both sequences converge rapidly to their asymptotic
values. On the other hand, Figure 3.15 displays the evolutions of the Kalman
gain Kt as well as the innovation sequence νt. Similarly to the case of the
sequences ∆t and Ωt, the Kalman gains converges fast to its limit. Moreover,
the sample ACF and PACF of the innovations νt, see Figure 3.16 suggest that
this residual sequence seems to be white noise. This hypothesis is formally
tested by means of the Box-Ljung test,
> Box.test(nu,lag=10,type="Ljung")
Box-Ljung test
data:
nu
X-squared = 7.0116, df = 10, p-value = 0.7243

STATE ESTIMATION
107
(a)
Time
xt
0
20
40
60
80
100
-6
-4
-2
0
2
4
6
(b)
Time
yt
0
20
40
60
80
100
-6
-4
-2
0
2
4
6
Figure 3.12
Additive noise state space model. Simulated process with φ = 0.9,
σ2
v = 1, σ2
w = 2, n = 100. (a) state xt, (b) observation yt.
0
5
10
15
20
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
5
10
15
20
-0.2
-0.1
0.0
0.1
0.2
0.3
0.4
Lag
Partial ACF
(b)
Figure 3.13
Additive noise state space model. Sample ACF and PACF of the
simulated process with φ = 0.9, σ2
v = 1, σ2
w = 2, n = 100. (a) sample ACF of yt,
(b) sample PACF of yt.

108
STATE SPACE MODELS
(a)
Time
!t
0
20
40
60
80
100
4.0
4.5
5.0
5.5
6.0
(b)
Time
!t
0
20
40
60
80
100
2.0
2.5
3.0
3.5
4.0
Figure 3.14
Additive noise state space model. Evolution of ∆t and Ωt for the
simulated process with φ = 0.9, σ2
v = 1, σ2
w = 2, n = 100.
(a) state xt, (b)
observation yt.
(a)
Time
Kt
0
20
40
60
80
100
0.45
0.50
0.55
0.60
(b)
Time
!t
0
20
40
60
80
100
-4
-2
0
2
4
6
Figure 3.15
Additive noise state space model. Evolution of Kt and νt for the
simulated process with φ = 0.9, σ2
v = 1, σ2
w = 2, n = 100.
(a) state xt, (b)
observation yt.

STATE ESTIMATION
109
0
5
10
15
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
5
10
15
-0.2
-0.1
0.0
0.1
0.2
Lag
Partial ACF
(b)
Figure 3.16
Additive noise state space model. Sample ACF and sample PACF
of νt for the simulated process with φ = 0.9, σ2
v = 1, σ2
w = 2, n = 100. (a) state xt,
(b) observation yt.
(a)
Time
xt
0
20
40
60
80
100
-6
-4
-2
0
2
4
6
(b)
Time
yt
0
20
40
60
80
100
-6
-4
-2
0
2
4
6
Figure 3.17
Additive noise state space model. Predictions of the simulated process
with φ = 0.9, σ2
v = 1, σ2
w = 2, n = 100. (a) state predictions xt, (b) observation
predictions yt.

110
STATE SPACE MODELS
This procedure indicates that the white noise hypothesis is not rejected at,
for instance, the 5% signiﬁcance level.
Finally, the state and observation predictions are exhibited in Figure 3.17.
Notice that the predicted values follows closely theirs true counterparts.
3.4.6
Structural Models
A very useful class of processes in econometrics are the so-called structural
models, described by
xt+1
=
xt + vt,
yt
=
xt + wt,
where the state xt is an unobservable variable that governs a economical
or ﬁnancial phenomenon and yt is an observable process. Notice that this
state space model is a particular case of the process studied in the previous
subsection. In this situation, the state follows a random walk process with
noise standard deviation σv. In addition, the observed values correspond to
the underlying random walk plus an observational noise wt with standard
deviation σw.
(a)
Time
xt
0
50
100
150
200
250
300
0
5
10
15
20
25
30
(b)
Time
yt
0
50
100
150
200
250
300
0
5
10
15
20
25
30
Figure 3.18
Structural model. Simulated process with σ2
v = 1, σ2
w = 1, n = 300.
(a) state xt, (b) observation yt.

STATE ESTIMATION
111
(a)
Time
xt
0
50
100
150
200
250
300
0
5
10
15
20
25
30
(b)
Time
yt
0
50
100
150
200
250
300
0
5
10
15
20
25
30
Figure 3.19
Structural Model.
Predictions of the simulated process σ2
v = 1,
σ2
w = 1, n = 300. (a) state predictions xt, (b) observation predictions yt.
Figure 3.18 shows a trajectory of this time series structural model with 300
values and parameters σ2
v = 1 and σ2
w = 1. Observe that the series yt is, as
expected, more noisy than the state sequence xt.
On the other hand, Figure 3.19 exhibits the predictions of the state and
observations obtained from de Kalman recursions. Notice that in both cases
the predicted values are close to their theoretical counterparts.
3.4.7
Estimation of Future States
Future values of the state vector can be estimated from the Kalman recursions.
Let h ≥0 and deﬁne Fh = Ft Ft+1 · · · Ft+h, the h-step forward state predictor
is given by
xt+h = Fh xt,
with error variance
Ωh
t = Fh Ωt F′
h +
h−1

j=0
Fj Σj F′
h.

112
STATE SPACE MODELS
Consequently, the h-step predictor of observation yt+h, for h ≥0, given its
ﬁnite past y1, . . . , yt−1 is readily obtained from the state predictor xt+h as
yt+h = Gt+h xt+h,
since the sequence εt+1, εt+2, . . . is orthogonal to the past observations {y1, . . . , yt−1}.
Consequently, we conclude that
yt+h = Gt+h Fhxt,
with h-step prediction error variance
Var(yt+h −yt+h) = ∆t+h = Gt+h Ωh
t G′
t+h + σ2
t+h.
EXAMPLE 3.4
Figure 3.20 and Figure 3.21 exhibit the estimated state components
for t = 1000 and h = 1, . . . , 300, along with their estimated standard
deviations. Notice that both states estimates rapidly converge to zero as
the horizon h increases. Additionally, the standard deviations plots show
that they decrease after a few steps starting at time t = 1, they remain
steady and then start to grow after t = 1, 000 reaching the state standard
deviations sd(xt,1) = 0.1312 and sd(xt,2) = 0.1539, respectively.
(a)
Time
xt1
0
200
400
600
800
1000
1200
-0.3
-0.2
-0.1
0.0
0.1
0.2
(b)
Time
!xt1
0
200
400
600
800
1000
1200
0.09
0.10
0.11
0.12
0.13
Figure 3.20
State space model example. Estimated future states, t = 1000, h =
1, . . . , 300. (a) estimated ﬁrst state component, (b) estimated standard deviations.

EXOGENOUS VARIABLES
113
(a)
Time
xt2
0
200
400
600
800
1000
1200
-0.4
-0.2
0.0
0.1
0.2
(b)
Time
!xt2
0
200
400
600
800
1000
1200
0.12
0.13
0.14
0.15
Figure 3.21
State space model example. Estimated future states, t = 1000, h =
1, . . . , 300. (a) estimated second state component., (b) estimated standard deviations.
3.5
EXOGENOUS VARIABLES
An additional versatility onstate space models is that they can be readily
extended to incorporate an exogenous process {zt}. This sequence is usually
a deterministic component such as a trend or cycle. An extended state space
system may be written as
xt+1
=
F xt + L zt + vt,
(3.18)
yt
=
G xt + wt.
(3.19)
The extended model described by equations (3.18)–(3.19) can be modiﬁed to
ﬁt into the simpler structure (3.1)–(3.2) as follows. Let us deﬁne the variable
rt =
t−1

t=1
F j−1 L zt−j,
for t ≥1 and r0 = 0. With this deﬁnition we have that
rt+1 = F rt + L zt.
(3.20)
Let xt = xt−rt be the modiﬁed state vector and yt = yt−G rt be the modiﬁed
observation at time t. Then, from (3.18)–(3.19) and (3.20) we conclude that
the state xt and the observations yt satisfy the system (3.1)–(3.2).

114
STATE SPACE MODELS
3.6
BIBLIOGRAPHIC NOTES
The monographs by Anderson and Moore (1979), Harvey (1989), Aoki (1990),
and Durbin and Koopman (2001) oﬀer an excellent overview of both method-
ological and applied aspects of state space modeling. Besides, Chapter 12
of Brockwell and Davis (1991) gives a very good introduction to state space
systems in a ﬁnite-dimensional context, including descriptions of Kalman re-
cursions and treatment of missing values.
The Kalman ﬁlter equations were introduced by Kalman (1961) and Kalman
and Bucy (1961).
Applications of state space systems to the analysis of time
series data are reported in ﬁelds as diverse as aeronautics [e.g., Kobayashi and
Simon (2003)] and oceanography [e.g., Bennett (1992)].
In some texts, the term reachability is used instead of controllability. Several
deﬁnitions of stability of state space systems in a very general context of
inﬁnite-dimensional systems is given in Curtain and Zwart (1995).
Fitting time series models with missing data has been extensively discussed
in the state space context. For ARMA and ARIMA models, see, for instance,
Jones (1980), Ansley and Kohn (1983), Kohn and Ansley (1986), and Bell and
Hillmer (1991). On the other hand, for ARFIMA models see, for example,
Palma and Chan (1997), and Ray and Tsay (2002).
Finally, the book by Hannan and Deistler (1988) gives an excellent theo-
retical treatment of the linear systems. In particular, the relationships among
the diﬀerent representations of these processes are analyzed in full detail.
Problems
3.1
Consider the following structural model
yt = µt
2 + εt
µt+1 = µt + νt + ηt
νt+1 = νt + ωt,
where εt, ηt and ωt are uncorrelated white noise sequences.
(a) Write this model in terms of a state space representation, identifying
all its components.
(b) Is the state sequence stationary?
(c) Is the observed process yt stationary?
(d) Write down the recursive Kalman equations for this state space
model.
3.2
Consider ARMA(3,2) process yt = φ1yt−1 + φ2yt−2 + φ3yt−3 + εt −
θ1εt−1 −θ2εt−2, where ϵt is white noise (0, σ2). Find a state space system
representation of yt.

PROBLEMS
115
3.3
Consider the following state space model:
xt+1
=


3
1
0
2
0
0
0
1
2

xt +


1
0
2

εt,
yt
=
 1
1
0 
xt.
(a) Is this state space model stable?
(b) Verify whether this state space model is observable.
(c) Verify whether this state space model is controllable.
(d) What can you conclude about the model?
3.4
Find a minimal state space representation of the process yt = εt +θεt−1
where |θ| < 1 and εt is white noise.
3.5
Consider the linear process yt = φyt−1+εt where |φ| < 1 and εt is white
noise.
(a) Find a minimal state space representation of the process yt.
(b) Verify that the system is stable.
(c) Find the Hankel matrix representing this process.
(d) What is the rank of this Hankel matrix?
3.6
Consider the state space system
xt+1
=


θ1
0
0
0
θ2
0
0
0
θ3

xt +


1
0
0

εt,
yt
=
 1
1
1 
xt.
(a) For which values of the parameter θ = (θ1, θ2, θ3) is this system
stable?
(b) Assume that εt is an independent and identically distributed se-
quence N(0, 1).
Simulate several trajectories of the system for a
sample size n = 1000 and diﬀerent parameters θ.
(c) Implement computationally the Kalman recursions for this state space
system.
3.7
Provide another state space representation based on the inﬁnite au-
toregressive expansion of an ARFIMA process.
Discuss the advantages or
disadvantages of this AR(∞) with respect to the MA(∞) representation.

116
STATE SPACE MODELS
3.8
Consider following the state transition matrix associated to the AR(m)
state space approximation:
F =









π1
π2
· · ·
πm−1
πm
1
0
· · ·
0
0
0
1
· · ·
0
0
...
...
0
0
· · ·
0
0
0
0
· · ·
1
0









.
(a) Show that the eigenvalues of this matrix are the roots of the polyno-
mial
λm −π1λm−1 −· · · −πm−1λ −πm = 0.
(b) Verify that space of eigenvectors is given by the {(λm−1, . . . , λ, 1)′}.
3.9
Consider the following state space model:
xt+1
=


1
0
0
1
0
0
0
1
0

xt +


1
0
0

εt,
yt
=
 1
0
0 
xt.
(a) Is this state space model stable?
(b) Verify whether this state space model is observable.
(c) Verify whether this state space model is controllable.
(d) What can you conclude about the model?
3.10
Consider a ﬁnite-dimensional state space system where xt ∈IRn.
Write a computer program implementing the Kalman recursion equations
(3.13)–(3.17).
3.11
Given a sample {y1, . . . , yn}, verify that Ωt|n ≤Ωt|t ≤Ωt for all t ≤n,
where the matrix inequality A ≤B means that x′(B −A)x ≥0 for all x.
3.12
Consider the following state space system:
xt+1
=
φxt + εt,
yt
=
θxt + εt,
where εt is white noise with unit variance.
(a) For which values of the parameter θ = (φ, θ) is this system stable?
(b) For which values of θ is the system observable or controllable?
(c) For which values of θ are the Kalman recursions stable?
(d) Assume that εt is an independent and identically distributed se-
quence N(0, 1).
Simulate several trajectories of the system for a
sample size n = 1000 and diﬀerent parameters θ.

PROBLEMS
117
3.13
Consider the following state space system for xt ∈Rn with n ≥2:
xt+1
=
Fxt + Hεt,
yt
=
Gxt + εt,
where
F
=


0
1
0
0
· · ·
0
0
0
1
0
· · ·
0
0
0
0
1
· · ·
0
...
...
...
0
0
0
· · ·
0
1
0
0
0
· · ·
0
0


,
G
=
[ ψn
ψn−1
ψn−2
· · · ] ,
H
=
[ 1
0
0
· · ·
0]′ ,
and the coeﬃcients ψj are given by the expansion (3.3).
(a) Verify that this system is stable.
(b) Verify that ∥F∥= supx ∥Fx∥/∥x∥= 1.
(c) Does ∥F∥n converges to zero at an exponential rate?
(d) Find the observability matrix for this system O and verify that it is
of full rank if and only if ψn ̸= 0.
3.14
Consider the AR(p) process given by the equation
yt −φ1yt−1 · · · −φpyt−p = εt.
(a) Show that by deﬁning the state vector xt = (yt+1−p, . . . , yt)′, the
AR(p) process may be written in terms of the following state space
representation:
xt+1
=
Fxt + Hεt+1,
yt
=
Gxt,
where
F
=


0
1
0
0
· · ·
0
0
0
1
0
· · ·
0
0
0
0
1
· · ·
0
...
...
...
0
0
0
· · ·
0
1
φp
φp−1
φp−2
· · ·
φ2
φ1


,
G
=
[ 0
0
0
· · ·
0
1 ] ,
H
=
[ 0
0
0
· · ·
0
1 ]′ .

118
STATE SPACE MODELS
(b) Write down the observation matrix O for this state space model. Is
this system observable?
(c) Find the controllability matrix C for this state space representation
and check whether this system is controllable.
(d) Verify whether this system converges to its steady state or not. If
yes, write down the steady state system equations.
3.15
Given the state space system
xt+1
=
Fxt + Hεt,
yt
=
Gxt + εt,
where {εt} is a white noise sequence, show that the product of the observation
matrix and the controllability matrix yields
OC =





GH
GFH
GF 2H
GF 3H
· · ·
GFH
GF 2H
GF 3H
GF 4H
· · ·
GF 2H
GF 3H
GF 4H
GF 5H
· · ·
...
...
...




.
Is this a Hankel matrix?
3.16
Suppose that the state transition matrix F satisﬁes ∥F j∥≤ce−αj so
that the corresponding state space system is stable.
(a) Verify that in this case,
n
X
j=0
F jzj,
converges for all |z| ≤1.
(b) Show that
(I −zF)−1 =
∞
X
j=0
F jzj,
for all |z| ≤1.
3.17
Consider the following state space model:
xt+1
=
Fxt + Hεt,
yt
=
Gxt + εt,
and let ψ(z) be the operator ψ(z) = 1+ψ1z+ψ2z2+· · · , where ψj = GF j−1H
and |z| ≤1.
(a) Prove that ψ(z) may be written as
ψ(z) = 1 + G(I −zF)−1Hz.

PROBLEMS
119
(b) Assume that F = 0. Show that if |GH| ≤1, then ψ(z) is invertible
for |z| ≤1 .
(c) If F = 0 and |GH| ≤1, verify that the state xt may be expressed as
xt = Hyt−1 + H
∞
X
j=1
(−GH)jyt−1−j.
3.18
Consider the linear state space model where the transition matrix
depends on time
xt+1
=
Ftxt + Hεt,
yt
=
Gxt + εt,
for t ≥1, εt is a white noise sequence and x0 is the initial state.
(a) Show that the state at time t + 1 may be written as
xt+1 =


tY
j=1
Fj

x0 +
t
X
j=0
ϕjHεt−j,
and ﬁnd the coeﬃcients ϕj.
(b) Let zn = Pn
j=1 log ∥Fj∥and assume that the limit
γ = lim
n→∞zn/n
exists. Prove that if γ < 0, then
lim
t→∞


tY
j=1
Fj

x0 = 0,
(3.21)
in probability.
(c) Suppose that ∥Ft∥≤e−αt where α is a positive constant. Show that
(3.21) holds in this situation.
(d) Assume that ∥Ft∥≤t−β where β is a positive constant. Prove that
the limit (3.21) holds under these circumstances.


CHAPTER 4
SPECTRAL ANALYSIS
Some fundamental concepts about spectral analysis are introduced in this
chapter. A time series can be analyzed by studying its time domain related
characteristics such as mean, variance and autocovariances. However, it can
be also described by its frequency domain related properties such as spec-
tral density or Cramer representation. In what follows we describe brieﬂy
these two ways of describing and modeling time series data. As described in
this chapter, the frequency domain is particularly appropriate for analyzing
time series exhibiting periodic or seasonal patterns. These patterns can be
deterministic as in a an harmonic regression or stochastic as in seasonal au-
toregressive model. However, the application of the spectral analysis is also
important for estimating and forecasting time series. As an example of such
application we can mention the Whittle likelihood function which allows for
the eﬃcient calculation of quasi maximum likelihood estimators. A detailed
account of these spectral based parameter estimation methods is provided in
Chapter 5.
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
121

122
SPECTRAL ANALYSIS
4.1
TIME AND FREQUENCY DOMAINS
In the previous chapters we have discussed some basic concepts in time se-
ries analysis such as moments, autocovariances and autocorrelations. Time
lags have played and essential role in these deﬁnitions. Starting from these
concepts, more elaborated techniques can be implemented such as maximum
likelihood estimation which in the Gaussian case is based on the mean and
covariance structure of the observations. Generally speaking, these methods
belong to the so-called time domain. On the other hand, a time series can be
analyzed by taking into account it spectral features, including its spectral dis-
tribution, its spectral density or its Cramer representation. These techniques
are intimately related to the study of data exhibiting seasonal o cyclical be-
havior and they are usually denoted as belonging to the so-called frequency
domain. For example, in many engineering contexts, it is natural to study the
frequency at which a signal is propagated. Nevertheless, as we will see later,
there is a deep relationship between time and frequency domains.
4.2
LINEAR FILTERS
In linear ﬁltering theory, one usually assumes that a signal is the result of the
application of a linear ﬁlter to a white noise input sequence. This simple idea
has proven to be quite powerful and useful in practice. As in the previous
chapters, the data can be written as
yt = ψ(B)εt =
∞
X
j=−∞
ψjεt−j.
(4.1)
where P∞
j=−∞ψ2
j < ∞.
Observe that this condition guarantees that the
process {yt : t ∈Z} possesses ﬁnite variance since
Var(yt) = Var(
∞
X
j=−∞
ψjεt−j) = σ2
∞
X
j=−∞
ψ2
j < ∞.
In some cases, processes with inﬁnite variance are also of interest. However
the analysis of these processes are beyond the scope of this book.
When the ﬁlter ψ is invertible, the process {yt : t ∈Z} satisﬁes the discrete-
time equation
ψ(B)−1yt = π(B)yt =
∞
X
j=−∞
πjyt−j = εt.
(4.2)
For simplicity, in this chapter we will consider linear processes satisfying
the summability condition P∞
h=−∞|hγ(h)| < ∞. Most the results discussed
next can be extended to more general autocovariance structures such as, for
instance, long-memory processes. Nevertheless, the above condition greatly
simplify the exposition.

SPECTRAL DENSITY
123
EXAMPLE 4.1
The autoregressive process introduced in the previous chapter can be
readily represented as the result of a linear ﬁltering. For example, an
AR(p) process
yt = φ1yt−1 + φ2yt−2 + · · · + φpyt−p + εt,
can be readily expressed in terms of (4.2) by setting πj = 0 for j < 0
and j > p, and πj = −φj for j = 1, . . . , p.
EXAMPLE 4.2
Consider the periodic process yt = A cos(ωt+φ) where A is a zero-mean
random variable with ﬁnite variance. Then, this process satisﬁes the
equation
yt −2 cos(ω)yt−1 + yt−2 = 0.
Thus, this periodic process could ﬁt equation (4.2) with πj = 0 for j < 0,
π0 = 1 π1 = −2 cos(ω), π2 = 1 and εt a sequence of zeroes. The ACF of
this process is given by
γ(h) = σ2
A cos(ωh),
where σ2
A is the variance of the random variable A.
4.3
SPECTRAL DENSITY
The spectral density of the process deﬁned by the linear ﬁlter (4.1) is given
by
f(ω) = σ2
2π |ψ(e˙ıω)|2.
It can be readily shown that an alternative deﬁnition of the spectral density
of the process is provided by
f(ω) = 1
2π
∞
X
h=−∞
γ(h)eiωh.
(4.3)

124
SPECTRAL ANALYSIS
This expression is obtained as follows,
f(ω)
=
σ2
2π
∞
X
h=−∞
∞
X
j=−∞
ψjψj+heiωh
=
σ2
2π
∞
X
h=−∞
∞
X
j=−∞
ψje−iωjψj+heiω(j+h)
=
σ2
2π
∞
X
j=−∞
ψje−iωj
∞
X
h=−∞
ψj+heiω(j+h)
Now by changing the index from j + h to k in the second sum above we have
f(ω)
=
σ2
2π
∞
X
j=−∞
ψje−iωj
∞
X
k=−∞
ψkeiωk
=
σ2
2π

∞
X
j=−∞
ψjeiωj

2
.
Note that we can also obtain an expression for γ(h) based on the spectral
density of the process. Multiplying (4.3) by eiω−h on both sides we get
f(ω)eiω−h = 1
2π
∞
X
j=−∞
γ(h)eiω(j−h),
and by integrating both sides with respect to ω
Z π
−π
f(ω)e−iωhdω =
Z π
−π
1
2π
∞
X
j=−∞
γ(h)eiω(j−h)dω.
Recalling that the sequence {γ(h)} is absolutely summable, we obtain
Z π
−π
f(ω)eiω−hdω
=
1
2π
∞
X
j=−∞
γ(h)
Z π
−π
eiω(j−h)dω
But, since
Z π
−π
ei(j−h)λdλ =
 2π
if j = h,
0
if j ̸= h,
see Problem 4.8, we conclude that
γ(h) =
Z π
−π
f(ω)eiω−hdω.

PERIODOGRAM
125
EXAMPLE 4.3
Consider the ARMA(p, q) model
π(B)yt = θ(B)εt,
(4.4)
where π(B) = 1 −π1B −· · · −πpBp, θ(B) = 1 −θ1B −· · · −θqBq and
{εt} is a white noise sequence with variance σ2. In this case, the spectral
density is given by
f(ω) = σ2
2π
|θ(eiω)|2
|π(eiω)|2 = σ2
2π
|1 −θ1eiω −· · · −θqeiωq|2
|1 −π1eiω −· · · −πpeiωp|2 .
EXAMPLE 4.4
Consider the fractional noise model ARFIMA(0, d, 0) model
yt = (1 −B)−dεt,
(4.5)
where (1 −B)−d fractional diﬀerence operator and {εt} is a white noise
sequence with variance σ2. In this case, the spectral density is given by
f(ω) = σ2
2π |1 −eiω|−2d.
4.4
PERIODOGRAM
The periodogram is an estimator of the spectral density. Given the sample
{y1, y2, . . . , yn}, its periodogram is deﬁned as
I(ω) = 1
2n

n
X
t=1
(yt −¯y)eiωt

2
.
(4.6)
This expression can be rewritten as
I(ω)
=
1
2πn
n
X
t=1
n
X
s=1
(yt −¯y)(ys −¯y)eiω(t−s)
=
1
2πn
n−1
X
h=1−n
n−|h|
X
t=1
(yt −¯y)(yt+|h| −¯y)eiωh
=
1
2π
n−1
X
h=1−n
bγ(h)eiωh

126
SPECTRAL ANALYSIS
Thus, we conclude that the spectral density of the process can be written as
I(ω) = 1
2π
n−1
X
h=1−n
bγ(h)eiωh.
Notice that the periodogram is an asymptotically unbiased estimator of the
spectral density, as shown next,
E[I(ω)]
=
1
2π E
 n−1
X
h=1−n
bγ(h)eiωh
!
=
1
2π
n−1
X
h=1−n
E [bγ(h)] eiωh.
However,
E [bγ(h)]
=
1
n E[
n−|h|
X
t=1
(yt −¯y)(yt+|h| −¯y)]
=
1
n
n−|h|
X
t=1
E[(yt −¯y)(yt+|h| −¯y)]
=
1
n
n−|h|
X
t=1
E[(yt −µ)(yt+|h| −µ)] −E(µ −¯y)2
=
1
n
n−|h|
X
t=1
γ(h) −Var(¯y)
=
n −|h|
n
γ(h) −Var(¯y)
=
γ(h) −|h|
n γ(h) −Var(¯y),
and then,
E [bγ(h)] = γ(h) −|h|
n γ(h) −σ2
n,
(4.7)
where σ2
n = Var(¯y). Notice from this expression that as the sample size in-
creases, for a ﬁxed lag h we have
lim
n→∞E [bγ(h)] = γ(h) −lim
n→∞
|h|
n γ(h) −lim
n→∞σ2
n = γ(h).

PERIODOGRAM
127
Thus, bγ(h) is asymptotically unbiased. Now, by virtue of (4.7) we have
E I(ω)
=
1
2π
n−1
X
h=1−n
E [bγ(h)] eiωh
=
1
2π
n−1
X
h=1−n

γ(h) −|h|
n γ(h) −σ2
n

eiωh
=
1
2π
n−1
X
h=1−n
γ(h)eiωh −
1
2πn
n−1
X
h=1−n
|h|γ(h)eiωh −σ2
n
2π
n−1
X
h=1−n
eiωh.
Observe that the last two terms of the expression above vanish as the sample
size tends to inﬁnity.
In the case of the second term, since the ACF are
summable,
lim
n→∞
1
2πn|
n−1
X
h=1−n
|h|γ(h)eiωh| ≤lim
n→∞
1
2πn
n−1
X
h=1−n
|hγ(h)| = 0.
For the third term, we have the identity
n−1
X
h=1−n
eiωh = eiωn −1
eiω −1 + e−iωn −1
e−iω −1 −1.
Therefore,
lim
n→∞
1
2πn
n−1
X
h=1−n
eiωh = lim
n→∞
1
2πn
eiωn −1
eiω −1 + e−iωn −1
e−iω −1 −1

= 0.
Consequently,
lim
n→∞E[I(ω)] = 1
2π
∞
X
h=−∞
γ(h)eiωh = f(ω).
EXAMPLE 4.5
As an illustration consider the white noise concept introduced in Chap-
ter 1. From a time-domain point of view, this process consists of un-
correlated random variables with zero-mean and constant variance. On
the other hand, from a frequency-domain, a white noise sequence can
be characterized by a ﬂat spectrum. Figure 4.1(a) shows the raw peri-
odogram and theoretical spectral density of a Gaussian white noise se-
quence with zero-mean and unit variance. By comparison, Figure 4.1(b)
exhibits the raw periodogram and theoretical spectral density of a Gaus-
sian colored noise sequence. In this case, this colored noise corresponds
to an MA(1) model with θ = 0.5.

128
SPECTRAL ANALYSIS
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.5
1.0
1.5
(a)
Frequency
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
1
2
3
4
(b)
Frequency
Figure 4.1
(a) White and (b) colored noise estimated (gray line) and theoretical
spectral densities (black line).
It is interesting to mention that the name white noise comes from
the white light of the sun.
From a physical standpoint, this light is
a composition of light of diﬀerent colors, but this composition is such
that every frequency is equally represented. Thus, the spectrum of white
light is ﬂat, that is, every frequency makes the same contribution. On the
contrary, in the case of colored noise, some frequencies have a stronger
presence in the spectrum.
4.5
SMOOTHED PERIODOGRAM
The raw periodogram discussed in the previous section can be smoothed via
diﬀerent techniques. One of these methods is weighting the raw periodogram
around a Fourier frequency ωj as follows
f(ωj) = 1
2π
m

h=−m
W(h)I(ωj+h).
The weighting function W(·) usually is symmetric, such that W(h) = W(−h)
and m
h=−m W(h) = 1. One of the most well-known weighting functions is

SMOOTHED PERIODOGRAM
129
the so-called Daniell window deﬁned by
W(ω) =

r
2π,
−π
r ≤ω ≤π
r ,
0
otherwise
It is straightforward to verify that
 π
−π
W(ω)dω = 1.
As an illustration consider the following MA(1) process
yt = εt + θ εt−1,
where εt is a normal white noise with zero-mean and unit variance. Given
that σ = 1, the spectral density of this model is
f(ω) = 1
2π (1 + θ2 + 2 θ cos ω).
Figure 4.2 displays the theoretical spectral density (heavy line), the raw pe-
riodogram (gray line) and the Daniell smoothed periodogram (dotted line).
Notice that the raw periodogram exhibits high variability while the smoothed
periodogram follows closely the theoretical spectral density of the model.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Frequency
Raw Periodogram
Spectral Density
Smoothed Periodogram
Figure 4.2
Estimation of the spectral density of a MA(1) process with θ = 0.8
and σ = 1.

130
SPECTRAL ANALYSIS
4.6
EXAMPLES
As a ﬁrst illustrative example consider the following deterministic harmonic
series, with two frequencies ω1 and ω2,
yt = α1 sin(ω1t) + α2 sin(ω2t).
(4.8)
A sample of this time series is depicted in Figure 4.3 with 512 observations,
ω1 = π/16, ω2 = π/32, α1 = 2, and α2 = 1. The periodogram of this series is
displayed in Figure 4.4. Observe that the two frequencies are clearly detected
by the peaks in the estimated spectral density displayed in Figure 4.4.
Consider now the harmonic time series of the previous example, but with
added white noise, as described by equation (4.9). A sample of 512 observa-
tions from this series is exhibited in Figure 4.5.
yt = α1 sin(ω1t) + α2 sin(ω2t) + εt,
(4.9)
The periodogram of this time series is plotted in Figure 4.6. Notice that it is
very similar to the periodogram shown in Figure 4.4. Thus, it seems that the
periodogram is not greatly aﬀected by the presence of noise in the harmonic
series.
The third example illustrates the estimation of the spectral density in the
case of an AR(1) time series with parameter φ = 0.5. Figure 4.7 shows a
Time
Series
0
100
200
300
400
500
-2
-1
0
1
2
Figure 4.3
Simulated harmonic time series model (4.8) with 512 observations
with ω1 = π/16 and ω2 = π/32.

EXAMPLES
131
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
100
200
300
400
500
Frequency
Spectrum
Figure 4.4
Periodogram of simulated (4.8) with 512 observations with ω1 = π/16
and ω2 = π/32.
sample of this process, with length n = 512. The periodogram is plotted in
Figure 4.8 along with the theoretical spectral density.
As shown in Figure 4.8, the periodogram is usually ragged and it is some-
time diﬃcult to pinpoint which are the key features of the spectrum. One
way to smooth the periodogram is applying a kernel function to the raw pe-
riodogram as described in Section 4.5.
For instance, Figure 4.9 show the
periodogram of the AR(1), along with the theoretical and the smoothed peri-
odogram. Observe that the smoother version of the periodogram is very close
to its theoretical counterpart.
The following example discusses the estimation of the spectral density in
the context of a long-memory process. It considers an ARFIMA(0, d, 0) time
series with long-memory parameter d = 0.4. Figure 4.10 shows one realization
of this process, with n = 512 observations. The periodogram of this series is
displayed in Figure 4.11 along with its theoretical counterpart. Observe that
both lines indicate the presence of a peak near the origin, which is expected
in the case of a strongly dependent process.
Finally, Figure 4.12 shows the heating degree days data introduced in Chap-
ter 1 along with its periodogram. Note that this time series exhibits a annual
seasonal component due to the diﬀerent winter and summer heating require-
ments in Europe. Consequently, in the periodogram we can observe a seasonal
frequency of ω = 2π/12 = π/6.

132
SPECTRAL ANALYSIS
Time
Series
0
100
200
300
400
500
-4
-2
0
2
4
Figure 4.5
Simulated harmonic time series model (4.9) with 512 observations
with ω1 = π/16 and ω2 = π/32.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
100
200
300
400
500
Frequency
Spectrum
Figure 4.6
Periodogram of simulated (4.9).

EXAMPLES
133
Time
Series
0
100
200
300
400
500
-4
-2
0
2
Figure 4.7
Simulated AR(1) time series with 512 observations with φ = 0.5.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
5
10
15
Frequency
Spectrum
Figure 4.8
Periodogram of simulated AR(1) time series with 512 observations
with φ = 0.5.

134
SPECTRAL ANALYSIS
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
5
10
15
Frequency
Spectrum
Raw Periodogram
Spectral Density
Smoothed Periodogram
Figure 4.9
Raw periodogram of simulated AR(1) time series with 512 observations
with φ = 0.5, along with its smoothed version (broken line) and the theoretical
spectral density (heavy line).
Time
Series
0
100
200
300
400
500
-4
-3
-2
-1
0
1
2
Figure 4.10
Simulated ARFIMA(0, d, 0) time series with 512 observations with
d = 0.4.

EXAMPLES
135
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
1
2
3
Frequency
Spectrum
Figure 4.11
Periodogram of simulated ARFIMA(0, d, 0) time series with 512
observations with d = 0.4.
(a)
Time
Heating degree days
1980
1985
1990
1995
2000
2005
2010
0
100
200
300
400
500
600
700
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0e+00
1e+05
2e+05
3e+05
4e+05
5e+05
(b)
Frequency
Spectrum
Figure 4.12
Spectral analysis of the Heating Degree Day data. (a) Time Series.
(b) Periodogram.

136
SPECTRAL ANALYSIS
4.7
WAVELETS
The spectral analysis described in the previous sections correspond to decom-
pose the time series in terms of a sequence of harmonic functions. Neverthe-
less, there are other decompositions that may be very useful for analyzing a
time series. One important example of these decomposition are the so-called
wavelets which we brieﬂy describe next.
A wavelet is a real-valued integrable function ψ(t) satisfying
Z
ψ(t) dt = 0.
(4.10)
A wavelet has n vanishing moments if
Z
tpψ(t) dt = 0,
for p = 0, 1, . . . , n −1.
Consider the following family of dilations and translations of the wavelet
function ψ deﬁned by
ψjk(t) = 2−j/2ψ(2−jt −k),
for j, k ∈Z. In this context, the terms j and 2j are usually called the octave
and the scale, respectively. It can be shown that (see Problem 4.12)
Z
ψ2
jk(t) dt =
Z
ψ2(t) dt.
The discrete wavelet transform (DWT) of a process {y(t)} is then deﬁned
by
djk =
Z
y(t)ψjk(t) dt,
for j, k ∈Z.
Provided that the family {ψjk(t)} forms an orthogonal basis, that is,
Z
ψij(t)ψkℓ(t) dt = 0,
for all i, j, k, ℓ, excepting i = j = k = ℓ, we obtain the following representation
of the process {y(t)}:
y(t) =
∞
X
j=−∞
∞
X
k=−∞
djkψjk(t).

WAVELETS
137
EXAMPLE 4.6
The Haar wavelet system
ψ(t) =



1
if t ∈[0, 1
2)
−1
if t ∈[ 1
2, 1)
0
otherwise,
is a simple example of a function satisfying (4.10). Observe that
Z
tpψ(t) dt =
1
p + 1
 1
2p −1

.
Therefore, this wavelet has a vanishing moment only for p = 0.
EXAMPLE 4.7
The so-called Daubechies wavelets are a family of wavelets that extends
the previous example achieving a greater number of vanishing moments.
This family forms an orthogonal wavelet basis and it is built in terms of
the multiresolution analysis. Only a hint of this procedure is given here
and further references are provided in the bibliographic section.
Starting from a scaling function φ that satisﬁes
φ
  t
2

=
√
2
X
j
ujφ(t −j),
we obtain the mother wavelet ψ by deﬁning
ψ(t) =
√
2
X
j
vjφ(2t −j).
Thus, the Haar wavelet described in the previous example is obtained
by setting φ(t) = 1[0,1](t), u0 = u1 = v0 = −v1 = 1/
√
2 and uj = vj = 0
for j ̸= 0, 1, where 1A denotes the indicator function of the set A, that
is,
1A(t) =
 1
if
t ∈A,
0
if
t ∈Ac.
EXAMPLE 4.8
In order to illustrate the application of the wavelet analysis to real-life
data, consider the heating degree day data introduced in Chapter
1.
The R package wavelets allows for the calculation of the discrete wavelet
transform for univariate and multivariate time series. The lower panel
of Figure 4.13 shows the heating degree data while the upper panel
exhibits the coeﬃcients of the wavelet transform up to 3 levels. Observe
the cyclical regularity of the components.

138
SPECTRAL ANALYSIS
0
100
200
300
0
200
400
600
x
X
Time
T!2W1
T!2W2
T!3W3
T!2V3
Figure 4.13
Discrete wavelet transform for the HDD data.
4.8
SPECTRAL REPRESENTATION
A linear stationary process yt can be also written in terms of the so-called
spectral representation or Cramer representation,
yt =
 π
−π
A(λ) eiλt dB(λ),
(4.11)
where A(λ) is a transfer function and B(λ) is a orthogonal increments process
on [−π, π] such that
Cov[B(λ), B(ω)] = σ2
2π δ(λ −ω)dλdω.
According to these equations, we have that
Cov(yt, ys)
=
Cov
 π
−π
A(λ) eiλt dB(λ),
 π
−π
A(ω) e−iωs dB(ω)

=
 π
−π
 π
−π
A(λ) A(ω) eiλt−iωs Cov[dB(λ), dB(ω)]

SPECTRAL REPRESENTATION
139
Thus,
Cov(yt, ys)
=
σ2
2π
Z π
−π
Z π
−π
A(λ) A(ω) eiλt−iωs δ(λ −ω)dλdω
=
σ2
2π
Z π
−π
|A(λ)|2 eiλ(t−s) dλ
Therefore, we can write the spectral density of the process as
f(λ) = σ2 |A(λ)|2
2π
.
EXAMPLE 4.9
Deﬁne the sequence of random variables
εt =
Z π
−π
eiλt dB(λ).
Thus, we have that
Cov(εt, εs)
=
Cov
Z π
−π
eiλt dB(λ),
Z π
−π
e−iωs dB(ω)

=
Z π
−π
Z π
−π
eiλt−iωs Cov[dB(λ), dB(ω)]
=
σ2
2π
Z π
−π
eiλ(t−s) dλ = σ2δ(t −s).
Consequently, εt is a white noise sequence. Therefore, we can write the
MA(1) model
yt = εt −θ εt−1.
Replacing εt by its deﬁnition we get
yt
=
Z π
−π
eiλt dB(λ) −θ
Z π
−π
eiλ(t−1) dB(λ)
=
Z π
−π
eiλt −θ eiλ(t−1) dB(λ)
=
Z π
−π
[1 −θ e−iλ]eiλt dB(λ).
Thus, by deﬁning the transfer function
A(λ) = 1 −θ e−iλ
we obtain a spectral representation of the process yt.

140
SPECTRAL ANALYSIS
4.9
TIME-VARYING SPECTRUM
As discussed in the context of locally stationary processes, the spectral density
can be deﬁned in terms of both frequency and time.
In the case of non
stationary processes, in some cases it is possible to extend the deﬁnition of the
spectral density to the so-called time-varying spectral density. To illustrate
this extension, consider the a class of LSARMA(1,1) processes deﬁned by
yt −φ
 t
T

yt−1 = εt + θ
 t
T

εt−1,
for t = 1, . . . , T where εt is a white noise sequence with zero-mean and variance
σ2. Rescaling the time to the unit interval we can write with t = [u T]
yt −φ(u)yt−1 = εt + θ(u) εt−1.
The limiting time-varying spectral density of this non stationary process is
f(λ, u)
=
σ2
2π

1 + θ(u) ei λ
1 −φ(u) ei λ

2
=
σ2
2π
1 + θ(u)2 + 2 θ(u) cos λ
1 + φ(u)2 −2 φ(u) cos λ.
EXAMPLE 4.10
As a ﬁrst illustration of the analysis of time-varying spectrum, consider
the LS-MA(1) model with a ﬁrst-order moving-average parameter evolv-
ing according to the linear equation,
θ(u) = 0.1 + 0.7 u.
The spectral density of this process is displayed in Figure 4.14.
On
the other hand, a simulated time series from this model is exhibited in
Figure 4.15.
Observe that the variance of the series seems to increase.
This is
expected from the fact that the variance of this time series evolves in an
increasing manner
Var yt
=
σ2[1 + θ(u)2]
=
σ2(1.01 + 0.14 u + 0.49 u2).
The time-varying periodogram of these data is displayed in Figure 4.16.
Notice that this periodogram looks similar to its theoretical counterpart
shown in Figure 4.14.

TIME-VARYING SPECTRUM
141
Frequency
0.5
1.0
1.5
2.0
2.5
3.0
Time
0.0
0.2
0.4
0.6
0.8
1.0
Spectral Density
0.0
0.2
0.4
0.6
Figure 4.14
Time-varying spectral density of a LS-MA(1) model with θ(u) =
0.1 + 0.7 u.
Time
LS MA(1) Series
0
200
400
600
800
1000
-3
-2
-1
0
1
2
3
4
Figure 4.15
Simulated LS-MA(1) model with θ(u) = 0.1 + 0.7 u.

142
SPECTRAL ANALYSIS
Frequency
0.5
1.0
1.5
2.0
2.5
3.0
u
0.0
0.2
0.4
0.6
0.8
1.0
Periodogram
0.0
0.2
0.4
0.6
Figure 4.16
Time-varying periodogram of a LS-MA(1) model with θ(u) = 0.1 +
0.7 u.
EXAMPLE 4.11
A second illustration of the spectral analysis of non stationary time series
is provided by the LS-AR(1) process with a ﬁrst-order autoregressive
parameter deﬁned by φ(u) = 0.9−0.8 u2. Figure 4.17 show the evolution
of this parameter for scaled time values of u ∈[0, 1]. Observe that in
this case, the parameter φ(u) decays at a quadratic rate from φ(0) = 0.9
to φ(1) = 0.1.
A simulated trajectory of this model with 1000 observations is dis-
played in Figure 4.18.
Note that the variance of this series seems to decrease, as expected
from the asymptotic formula for the variance of the process given by
Var yt
=
σ2
1 −φ(u)2
=
σ2
0.19 + 1.44 u2 −0.62 u4 .
The spectral density of this process is plotted in Figure 4.19 and its
periodogram is exhibited in Figure 4.20. Notice that both graphs have
similar shape in terms of both the frequency axis and the scaled time
axis.

TIME-VARYING SPECTRUM
143
Time
!(u)
0
200
400
600
800
1000
0.2
0.4
0.6
0.8
Figure 4.17
Evolution of the time-varying ﬁrst-order autoregressive parameter of
a LS-AR(1) model with φ(u) = 0.9 −0.8 u2.
Time
LS AR(1) Series
0
200
400
600
800
1000
-4
-2
0
2
4
Figure 4.18
Simulated LS-AR(1) model with φ(u) = 0.9 −0.8 u2.

144
SPECTRAL ANALYSIS
Frequency
0.5
1.0
1.5
2.0
2.5
3.0
Time
0.0
0.2
0.4
0.6
0.8
1.0
Spectral Density
5
10
Figure 4.19
Time-varying spectral density of a LS-AR(1) model with φ(u) =
0.9 −0.8 u2.
Frequency
0.5
1.0
1.5
2.0
2.5
3.0
u
0.0
0.2
0.4
0.6
0.8
1.0
Periodogram
0
2
4
Figure 4.20
Time-varying periodogram of a LS-AR(1) model with φ(u) = 0.9 −
0.8 u2.

BIBLIOGRAPHIC NOTES
145
4.10
BIBLIOGRAPHIC NOTES
Frequency-domain time series analysis has a vast literature in statistics, signal
processing, engineering and many other ﬁelds. Three classical book on this
subject are Priestley (1981a,b) and Brillinger and Krishnaiah (1983).
Ex-
tensions of spectral techniques to nonlinear time series can be found in the
monograph Subba Rao and Gabr (1984). Chapter 11 of Press, Teukolsky, Vet-
terling, and Flannery (2007) provides an excellent account of computational
aspects of the periodogram calculation by means of the fast Fourier transform.
Fourier analysis is reviewed by K¨orner (1989, 1993) while some wavelets fun-
damental concepts are discussed by Flandrin (1999). Other useful references
in spectral analysis are Carmona, Hwang, and Torresani (1998), Koopmans
(1995) and Zurbenko (1986). Several parametric and nonparametric meth-
ods in the frequency domain are addressed by Castani´e (2006, 2011) while
hypothesis testing in this context is reviewed by Dzhaparidze (1986) Haykin
(1979) Jenkins and Watts (1968). Applications of the spectral analysis tech-
niques to economics are discussed in Granger (1964).
Wavelets have been
vastly discussed in the literature. A nice recent revision, especially related to
long-range dependence, is the article by Abry, Flandrin, Taqqu, and Veitch
(2003) while Chapter 9 of Percival and Walden (2006) oﬀers a comprehensive
revision of wavelet methods for stationary processes. Additionally, Chapter 2
of Flandrin (1999) and Chapter 13 of Press, Teukolsky, Vetterling, and Flan-
nery (1992) provide overviews about this topic, including several details about
the Daubechies wavelets.
Problems
4.1
Let {xt} and {yt} be two stationary satisfying
xt −αxt−1
=
wt,
yt −αyt−1
=
xt + zt,
where {wt} and {zt} are two uncorrelated white noise sequences (0, σ2). Find
the spectral density of {yt}.
4.2
If {xt} and {yt} are two uncorrelated stationary processes with auto-
covariance functions γX(·) and γY (·) and spectral densities fX(·) and fY (·),
respectively. Show that the process {zt} = {xt + yt} is stationary with auto-
covariance function γZ = γX(·) + γY (·) and spectral density fZ = fX + fY .
4.3
Consider the periodogram deﬁned as
I(λ) =
1
2πn

n
X
j=1
yje˙ıλj

2
.

146
SPECTRAL ANALYSIS
(a) Show that the periodogram satisﬁes
Z π
−π
e˙ıkλI(λ) dλ =
 w(k, n),
|k| < n,
0,
|k| > n,
where
w(k, n) = 1
n
n−k
X
t=1
(yt −¯yn)(yt+k −¯yn).
(b) Prove that if the process yt is stationary with mean µ we have that
lim
n→∞E w(k, n) = γ(k),
where γ(k) is the autocovariance at lag k.
4.4
Consider the following inner product of two functions f and g given by
⟨f, g⟩= 1
2π
Z π
−π
f(λ)g(λ) dλ.
(a) Let ej(λ) = e˙ıλj for j ∈Z. Show that the functions {ej(λ)} are
orthonormal, that is,
⟨et, es⟩= 0,
for all s, t ∈Z, t ̸= s, and
⟨et, et⟩= 1,
for all t ∈Z.
4.5
Suppose that a function f is deﬁned by
f(λ) =
m
X
j=−m
αjej(λ),
where ej are the functions deﬁned in Problem 4.4.
(a) Verify that the coeﬃcients αj are given by
αj = 1
2π
Z π
−π
f(λ)e−˙ıλjdλ.
The coeﬃcients αj are called the Fourier coeﬃcients of the function
f(·).
(b) Prove that
∥f∥=
v
u
u
t
m
X
j=−m
α2
j.

PROBLEMS
147
4.6
Show that if the spectral density of a stationary process satisﬁes fθ(λ) >
c for all λ ∈(−π, π] where c is a positive constant, then x′Γθx > 0 for all
x ∈IRn, x ̸= 0.
4.7
Suppose that {y1, . . . , yn} follows an ARFIMA model and In(λ) is its
periodogram. Prove that if f(λ) is the spectral density of the ARFIMA pro-
cess and g(λ) is a continuous function on [−π, π] then
lim
n→∞
Z π
−π
g(λ)In(λ) dλ =
Z π
−π
g(λ)f(λ) dλ.
4.8
Show that
Z π
−π
ei(k−h)λdλ =
 2π
if k = h,
0
if k ̸= h
4.9
Assume that {xt} is a non-causal and non-invertible ARMA(1, 1) pro-
cess that satisﬁes
xt −φxt−1 = zt + θzt−1,
zt ∼RB(0, σ2)
where |φ| > 1, |θ| > 1. Deﬁne ˜φ(B) = 1 −1
φ y ˜θ(B) = 1 + 1
θ, and let {Wt}
be a process given by
Wt = ˜θ−1(B)˜φ(B)xt.
(a) Verify that the process {Wt} possesses constant spectral density.
(b) Deduce that {Wt} ∼RB(0, σ2
w) and provide an explicit expression for
σ2
w in terms of φ, θ and σ2.
(c) Deduce that ˜φ(B)xt = ˜θ(B)Wt, such that {xt} is a causal and invertible
ARMA(1, 1) process relative to the white noise sequence{Wt}.
4.10
Prove that if {yt : t ∈Z} is a stationary process such that P∞
k=0 |γ(k)| <
∞, then its spectral density may be written as
f(λ) = 1
2π
∞
X
h=−∞
γ(h)e−˙ıλh,
and this function is symmetric and positive.
4.11
Let Γn = [γ(i −j)]i,j=1,...,n be the variance-covariance matrix of a
linear process with
γ(h) =
Z π
−π
f(λ)e−˙ıλhdλ.

148
SPECTRAL ANALYSIS
Show that the variance of the sample mean of y1, . . . , yn is given by
Var(¯yn) = 1
n2
Z π
−π
f(λ)

n
X
h=1
e−˙ıλh

2
dλ.
4.12
Verify that the L2 norm of ψ and ψik are identical for all j, k ∈Z, that
is,
Z
ψ2
jk(t) dt =
Z
ψ2(t) dt.
4.13
Show that the Haar system generates an orthonormal basis, that is,
Z
ψij(t)ψkℓ(t) dt = δijδkℓ.
4.14
Consider the so-called Littlewood-Paley decomposition
ψ(t) =
 1
if |t| ∈[1/2, 1),
0
otherwise.
Prove that the Littlewood-Paley decomposition generates an orthonormal ba-
sis.
4.15
Consider the following processes:
xt = φ xt−s + zt
yt = Wt + θ Wt−s
with |φ| < 1, |θ| < 1, s ∈N, {zt} ∼WN(0, σ2), {Wt} ∼WN(0, σ2) and
Cov(zk, Wh) = 0 for all k and h. Calculate the spectral density of the process
vt = xt + yt.
4.16
Consider the periodogram deﬁned in (4.6).
(a) Show that the periodogram satisﬁes
Z π
−π
e˙ıkλI(λ) dλ =
 w(k, n),
|k| < n,
0,
|k| > n,
where
w(k, n) = 1
n
n−k
X
t=1
(yt −¯yn)(yt+k −¯yn).
(b) Prove that if the process yt is stationary and ergodic with mean µ
we have that
lim
n→∞w(k, n) = lim
n→∞
1
n
n−k
X
t=1
(yt −µ)(yt+k −µ) = γ(k),

PROBLEMS
149
where γ(k) is the autocovariance at lag k.
4.17
Suppose that {y1, . . . , yn} follows an ARFIMA model and In(λ) is its
periodogram. Based on the previous problem, prove that if f(λ) is the spectral
density of the ARFIMA process and g(λ) is a continuous function on [−π, π]
then
lim
n→∞
Z π
−π
g(λ)In(λ) dλ =
Z π
−π
g(λ)f(λ) dλ.


CHAPTER 5
ESTIMATION METHODS
This chapter reviews several methodologies for estimating time series models.
There are a number of well-known techniques such as the maximum likelihood
estimation and its diﬀerent computational formulations such as Cholesky de-
composition or state space equations. On the other hand, there are approx-
imate maximum likelihood methods including for example the Whittle ap-
proach, moving-average and autoregressive approximations. However, before
reviewing these speciﬁc techniques this chapter begins with an overview of
model building and speciﬁcation.
This is a necessary step in the analysis
of time series before applying a speciﬁc estimation technique. In the model
building and speciﬁcation stage, we investigate the dependence structure of
the data and decide which class of models may ﬁt them more adequately. For
example, decide whether an ARMA or an ARFIMA model would be more
appropriate. In turn, in order to carry out this ﬁrst stage of the time series
analysis it is necessary to ﬁnd estimates of the mean and the autocorrelation
function of the series. Thus, after discussing general aspects of model build-
ing we focus our attention on estimating the mean and the ACF, which are
essential tools for specifying the model.
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
151

152
ESTIMATION METHODS
5.1
MODEL BUILDING
As discussed in previous chapters, real-life time series exhibit a number of
distinctive features. Among these, it is relevant to decide whether the series
will be treated as stationary or not.
In the ﬁrst case, we can proceed to
the modeling stage by looking at the autocorrelation structure shown by the
sample ACF and PACF. Based on these moment estimates, a stationary model
such an ARMA or ARFIMA model can be proposed and ﬁtted. On the other
hand, if the series displays a number of nonstationary characteristics, we can
apply transformations procedures to obtain a stationary series. Among these
procedures we have previously revised the detrending of the data by means
of regression techniques and the diﬀerentiation of the time series data.
If
a regression model is ﬁtted, we can treat the residuals as the series to be
analyzed. If diﬀerentiation is taken place, we can apply an ARIMA model. If
the series displays seasonal behavior, then we can use harmonic regression or
SARIMA models.
The speciﬁcation of a model usually is concerned with selecting a class of
processes such as ARIMA(p, d, q) or SARIMA(p, d, q)×(P, D, Q)s. The orders
of these models can be selected from the sample ACF o PACF. It is common
to consider a nested family of models and then estimate all the model in the
class. For example, ARMA(p, q) with orders p, q = 0, 1, 2, 3. Since the models
are nested, we can use an information criterion such as AIC or BIC to select
appropriate values of p and q.
5.2
PARSIMONY
In theory, if a linear process has continuous spectrum, then we can always
ﬁnd values of p and q such that an ARMA(p, q) approximates it arbitrarily
well. In other words, in practice we can always rely on this class of processes
to model a linear time series. However, this general mathematical result does
not guarantee that the values of the p and q are small. In fact, they eventually
could be quite large.
Having an ARMA model with large autoregressive and moving-average or-
ders could be cumbersome from both numerically and statistical perspectives.
Let us say that p = 45 and q = 37. This model could ﬁt well a data set but it
requires the numerical calculation of 83 parameters and checking if the ﬁtted
model is stationary and invertible.
In this context, it is usually desirable that the ﬁtted model be parsimonious,
that is, the orders are relatively small.
In this sense, there is a trade oﬀ
between the approximation quality of the model, which usually requires larger
values of p and q, and the simplicity of the model or parsimony, which strive
for a small number of parameters.

AKAIKE AND SCHWARTZ INFORMATION CRITERIA
153
This balance is commonly stricken by means of a criterion information that
penalizes the model with the number of parameter estimated. These criteria
are described next.
5.3
AKAIKE AND SCHWARTZ INFORMATION CRITERIA
The Akaike information criterion or AIC for short is deﬁned as Akaike’s in-
formation criterion (AIC)
AIC = −2 log L(bθ) + 2r,
where L(θ) is the likelihood of the data, bθ is the maximum likelihood estimate,
and r is the number of estimated parameters of the model. For example, for
and ARMA(p, q) model r = p + q + 1 since we have to add the estimation of
noise standard deviation parameter σ.
The Schwartz information criterion or Bayesian information criterion (BIC)
is deﬁned by
BIC = −2 log L(bθ) + r log n,
Note that for a sample size n > 8, BIC penalizes more strongly the incre-
ment of the number of parameters in the model, as compared to the AIC.
5.4
ESTIMATION OF THE MEAN
Estimating the mean of a stationary process is a fundamental stage of the
time series analysis. Even though there are several estimators of the mean,
the sample mean and the BLUE are the most commonly considered.
Given the sample Y n = (y1, y2, . . . , yn)′ from a stationary process with
mean µ and variance Γ, the sample mean is deﬁned as
bµ = 1
n
n
X
t=1
yt = 1
n1′Y n,
where 1 = (1, 1, . . . , 1)′ and the BLUE is
eµ = (1′Γ−11)−11′Γ−1Y n.
The large sample behavior of these two well-known estimators depend crit-
ically on the memory of the process. For a short memory process such as an
ARMA model, the sample mean and the BLUE converge to the true mean
at rate O(n−1). Furthermore, their asymptotic variance are similar. In this
sense, the sample mean is an eﬃcient estimator.
On the other hand, for a strongly dependent process with long memory
parameter d, the convergence rate of both estimators is O(n2d−1). Note that

154
ESTIMATION METHODS
since d > 0, this convergence rate is slower than for the short memory case.
Besides, the asymptotic variances of the sample mean and the BLUE are
diﬀerent, implying that the sample mean is not an eﬃcient estimator.
Speciﬁcally, the following expressions can be established
√n ( ¯yn −µ ) →N(0, v),
where v = 2πf(0) = P∞
−∞γ(h). Analogously for the BLUE we have
√n ( ˜µn −µ ) →N(0, v).
Notice that for an ARMA(p, q) process we have that v = σ2 |θ(1)|2
|φ(1)|2 .
For long memory processes,
n1/2−d ( ¯yn −µ ) →N(0, w),
with w = σ2 |θ(1)|2
|φ(1)|2
Γ(1−2d)
d(1+2d)Γ(d)Γ(1−d), and for the BLUE we have
n1/2−d ( ˜µn −µ ) →N(0, w),
with w = σ2 |θ(1)|2
|φ(1)|2
Γ(1−2d)Γ(2−2d)
Γ(1−d)2
.
5.5
ESTIMATION OF AUTOCOVARIANCES
For a stationary process, autocovariances are commonly estimated by means
of moment methods.
That is, given the sample {y1, y2, . . . , yn}, the usual
estimator of the autovariance at a lag h ≥0, γ(h) is given by
bγn(h) = 1
n
n−h
X
t=1
(yt −¯y)(yt+h −¯y).
It can be shown that for a ﬁxed h, this estimate of γ(h) is asymptotically
unbiased, i.e.,
lim
n→∞bγn(h) = γ(h).
Additionally, assuming that the input noise of the process satisﬁes E ε4
t =
ησ2 < ∞. if the autocovariances of the process are absolutely summable,
then
√n [ bγn(h) −γ(h) ] →N(0, v),
where v = (η −3)γ(h)2 + P∞
j=−∞[γ(j)2 + γ(j −h)γ(j + h)].
Similar expressions can be found for the asymptotic behavior of the sample
ACF. In this case, we have
√n [ bρn(h) −ρ(h) ] →N(0, w),

MOMENT ESTIMATION
155
where w is giveb by the Barlett formula
w =
∞
X
j=−∞
{[1 + 2ρ(h)2]ρ(j)2 + ρ(j)ρ(j + 2h) −4ρ(h)ρ(j)ρ(j + h)}.
EXAMPLE 5.1
As an illustration, consider a white noise sequence with ρ(0) = 1 and
ρ(h) = 0 for all h ̸= 0. In this situation, the Barlett formula indicates
that w = 1 and we obtain the usual Barlett conﬁdence bands for the
sample ACF, (−2
n, 2
n).
5.6
MOMENT ESTIMATION
A simple estimation approach is based on comparing the sample ACF with
their theoretical counterparts. Given the sample {y1, y2, . . . , yn} and the spec-
iﬁcation of a time series model with ACF γθ(h), we write the moment equa-
tions
γθ(h) = bγ(h),
for diﬀerent values of h. The solution of this equation system, bθ is the moment
estimator of θ. Analogously, we can write these equations in terms of the
sample autocorrelations
ρθ(h) = bρ(h).
As an example, consider the AR(1) model yt = φ yt−1 + εt.
The ACF
is given by ρ(h) = φ|h|. Thus, we can ﬁnd an estimate for φ based on the
equation
ρφ(1) = bρ(1),
obtaining bφ = bρ(1). Furthermore, an estimate of the variance noise σ2 can be
obtained as follows. Given that
γ(0) =
σ2
1 −φ2 ,
we can write
σ2 = γ(0) (1 −φ2).
Therefore, a moment estimate of the noise variance is given by
bσ2 = bγ(0) (1 −bφ2).

156
ESTIMATION METHODS
On the other hand, for and AR(2) model, the Yule Walker estimate of the
process
yt
=
φ1yt−1 + φ2yt−2 + εt,
(5.1)
are given by
 
bφ1
bφ2
!
=
 bγ(0)
bγ(1)
bγ(1)
bγ(0)
−1  bγ(1)
bγ(2)

.
The idea of comparing theoretical and sample moments can be extended
to ARMA or more complex models. For example, consider the MA(1) model
yt
=
εt + θεt−1.
In this case the moment estimate bθ is given by
bθ =
1
2bρ(1) ±
s
1
2bρ(1)
2
−1
Note that are two possible solutions for bθ. However, the adequate value cor-
responds to the one that satisﬁes the invertibility condition |bθ| < 1.
5.7
MAXIMUM-LIKELIHOOD ESTIMATION
Assume that {yt} is a zero-mean stationary Gaussian process.
Then, the
log-likelihood function of this process is given by
L(θ) = −1
2 log det Γθ −1
2Y ′Γ−1
θ Y ,
(5.2)
where Y = (y1, . . . , yn)′, Γθ = Var(Y ), and θ is the parameter vector. Conse-
quently, the maximum-likelihood (ML) estimate bθ is obtained by maximizing
L(θ).
The log-likelihood function (5.2) requires the calculation of the de-
terminant and the inverse of the variance-covariance matrix Γθ.
However,
these calculations can be conducted by means of the Cholesky decomposition
method. In the following subsections, we review this and other procedures
for computing the function (5.2) such as the Durbin-Levinson algorithm and
state space techniques.
5.7.1
Cholesky Decomposition Method
Given that the matrix Γθ is symmetric positive deﬁnite, it can be written as
Γθ = U ′U,
where U is an upper triangular matrix. According to this Cholesky decompo-
sition, the determinant of Γθ is given by det Γθ = (det U)2 = Qn
j=1 u2
jj, where
ujj denotes the jth diagonal element of the matrix U. Besides, the inverse
of Γθ can be obtained as Γ−1
θ
= U −1(U −1)′, where the inverse of U can be
computed by means of a very simple procedure.

WHITTLE ESTIMATION
157
5.7.2
Durbin-Levinson Algorithm
The Cholesky decomposition could be ineﬃcient for long time series. Thus,
faster methods for calculating the log-likelihood function (5.2) have been de-
veloped. One of these algorithms designed to exploit the Toeplitz structure
of the variance-covariance matrix Γθ, is known as the Durbin-Levinson algo-
rithm.
Suppose that by1 = 0 and byt+1 = φt1yt + · · · + φtty1, t = 1, . . . , n −1,
are the one-step ahead forecasts of the process {yt} based on the ﬁnite past
{y1, . . . , yt−1}, where the regression coeﬃcients φtj are given by the equations
φtt
=
[νt−1]−1
"
γ(t) −
t−1
X
i=1
φt−1,iγ(t −i)
#
,
φtj
=
φt−1,j −φttφt−1,t−j,
j = 1, . . . , t −1,
ν0
=
γ(0),
νt
=
νt−1[1 −φ2
tt],
j = 1, . . . , t −1.
Furthermore, if et = yt −byt is the prediction error and e = (e1, . . . , en)′, then
e = LY where L is the lower triangular matrix:
L =











1
−φ11
1
−φ22
−φ21
1
−φ33
−φ32
−φ31
...
...
1
−φn−1,n−1
−φn−1,n−2
−φn−1,n−3
· · ·
−φn−1,1
1











.
Hence, Γθ may be decomposed as Γθ = LDL′, where D = diag(ν0, . . . , νn−1).
Therefore, det Γθ = Qn
j=1 νj−1 and Y ′Γ−1
θ Y = e′D−1e.
As a result, the
log-likelihood function (5.2) may be expressed as
L(θ) = −1
2
n
X
t=1
log νt−1 −1
2
n
X
t=1
e2
t
νt−1
.
The numerical complexity of this algorithm is O(n2) for a linear stationary
process. Nevertheless, for some Markovian processes such as the family of
ARMA models, the Durbin-Levinson algorithm can be implemented in only
O(n) operations. Unfortunately, this reduction in operations count does not
apply to ARFIMA models since they are not Markovian.
5.8
WHITTLE ESTIMATION
A well-known methodology to obtain approximate maximum-likelihood esti-
mates is based on the calculation of the periodogram—see equation (4.6)—by

158
ESTIMATION METHODS
Table 5.1
Maximum Likelihood Estimation of ARMA models
p
q
φ1
φ2
θ1
θ2
µ
AIC
tφ1
tφ2
tθ1
tθ2
LB
1
0
0.65
0.25
1051.45
15.75
1.36
2
0
0.37
0.43
0.24
984.56
7.5
8.77
0.83
0.47
0
1
0.37
0.25
1150.14
10.3
2.47
1
1
0.88
-0.4
0.24
1010.48
27.36
-7.59
0.85
2
1
0.14
0.58
0.28
0.24
981.02
1.53
9.06
2.54
0.91
0.95
0
2
0.44
0.51
0.25
1040.55
8.43
2.03
1
2
0.77
-0.33
0.26
0.24
992.9
12.6
-3.95
1.02
0.05
2
2
0.15
0.57
0.28
0.01
0.24
983
1.32
5.21
2.17
0.92
0.95
means of the fast Fourier transform (FFT) and the use of the so-called Whittle
approximation of the Gaussian log-likelihood function. Since the calculation
of the FFT has a numerical complexity of order O[n log2(n)], this approach
produces very fast algorithms for computing parameter estimates.
Suppose that the sample vector Y = (y1, . . . , yn)′ is normally distributed
with zero-mean and variance Γθ. Then, the log-likelihood function divided by
the sample size is given by
L(θ) = −1
2n log det Γθ −1
2nY ′Γ−1
θ Y .
(5.3)
Notice that the variance-covariance matrix Γθ may be expressed in terms of
the spectral density of the process fθ(·) as follows:
(Γθ)ij = γθ(i −j),
where
γθ(k) =
Z π
−π
fθ(λ) exp(˙ıλk) dλ.
In order to obtain the Whittle method, two approximations are made. Since
1
n log det Γθ →1
2π
Z π
−π
log[2πfθ(λ)] dλ,
as n →∞, the ﬁrst term in (5.3) is approximated by
1
2n log det Γθ ≈1
4π
Z π
−π
log[2πfθ(λ)] dλ.

WHITTLE ESTIMATION
159
On the other hand, the second term in (5.3) is approximated by
1
2nY ′Γ−1
θ Y
≈
n
X
ℓ=1
n
X
j=1
yℓ

1
8π2n
Z π
−π
f −1
θ
(λ) exp[˙ıλ(ℓ−j)] dλ

yj
=
1
8π2n
Z π
−π
f −1
θ
(λ)
n
X
ℓ=1
n
X
j=1
yℓyj exp[˙ıλ(ℓ−j)] dλ
=
1
8π2n
Z π
−π
f −1
θ
(λ)

n
X
j=1
yj exp(˙ıλj)

2
dλ
=
1
4π
Z π
−π
I(λ)
fθ(λ) dλ,
where
I(λ) =
1
2πn

n
X
j=1
yje˙ıλj

2
is the periodogram of the series {yt} deﬁned in equation (4.6).
Thus, the log-likelihood function is approximated, up to a constant, by
L3(θ) = −1
4π
Z π
−π
log fθ(λ) dλ +
Z π
−π
I(λ)
fθ(λ) dλ

.
(5.4)
The evaluation of the log-likelihood function (5.4) requires the calculation of
integrals. To simplify this computation, the integrals can be substituted by
Riemann sums as follows:
Z π
−π
log fθ(λ) dλ ≈2π
n
n
X
j=1
log fθ(λj),
and
Z π
−π
I(λ)
fθ(λ)dλ ≈2π
n
n
X
j=1
I(λj)
fθ(λj),
where λj = 2πj/n are the Fourier frequencies. Thus, a discrete version of the
log-likelihood function (5.4) is
L4(θ) = −1
2n


n
X
j=1
log fθ(λj) +
n
X
j=1
I(λj)
fθ(λj)

.
Other versions of the Whittle likelihood function are obtained by making
additional assumptions. For instance, if the spectral density is normalized as
Z π
−π
log fθ(λ) dλ = 0,
(5.5)

160
ESTIMATION METHODS
then the Whittle log-likelihood function is reduced to
L5(θ) = −1
4π
Z π
−π
I(λ)
fθ(λ) dλ,
with the corresponding discrete version
L6(θ) = −1
2n
n
X
j=1
I(λj)
fθ(λj).
5.9
STATE SPACE ESTIMATION
The state space methodology may be also used for handling autoregressive
approximations. For instance, starting from the AR(m) truncation (5.6) and
dropping θ from the coeﬃcients πj and the tilde from εt, we have
yt = π1yt−1 + · · · + πmyt−m + εt.
Thus, we may write the following state space system:
xt+1
=
Fxt + Hεt+1,
yt
=
Gxt,
where the state is given by xt = [yt
yt−1 . . . yt−m+2
yt−m+1]′, the state
transition matrix is
F =









π1
π2
· · ·
πm−1
πm
1
0
· · ·
0
0
0
1
· · ·
0
0
...
...
0
0
· · ·
0
0
0
0
· · ·
1
0









,
the observation matrix is
G = [1 0 0 · · · 0],
and the state noise matrix is given by
H = [1 0 0 · · · 0]′.
The variance of the observation noise is R = 0, the covariance between the
state noise and the observation noise is S = 0 and the state noise variance-
covariance matrix is given by
Q = σ2







1
0
· · ·
0
0
0
· · ·
0
...
...
0
0
· · ·
0
0
0
· · ·
0







.

ESTIMATION OF LONG-MEMORY PROCESSES
161
The Kalman recursions for this system are
∆t
=
ωt(1, 1),
Θt
=
" m
X
i=1
πiωt(i, 1) ωt(1, 1)
· · ·
ωt(m −1, 1)
#′
,
ωt+1(1, 1)
=
σ2 +
m
X
i,j=1
πiωt(i, j)πj −
" m
X
i=1
πiωt(i, 1)
#2
/ωt(1, 1),
ωt+1(1, j)
=
m
X
i=1
πiωt(i, j) −
" m
X
i=1
πiωt(i, 1)
#
ωt(1, j)
ωt(1, 1) for j ≥2,
ωt+1(i, j)
=
ωt(i −1, j −1) −ωt(i −1, 1)ωt(j −1, 1)/ωt(1, 1) for i, j ≥2,
bxt+1(1)
=
m
X
i=1
πibxt(i) +
" m
X
i=1
πiωt(i, 1)
#
yt −bxt(1)
ωt(1, 1) ,
bxt+1(j)
=
bxt(j −1) + ωt(j −1, 1) yt −bxt(1)
ωt(1, 1)
for j ≥2,
byt+1
=
bxt+1(1).
The initial conditions for these iteration may be
bx0 = 0,
and
Ω0 =







γ(0)
γ(1)
· · ·
γ(m −1)
γ(1)
γ(0)
· · ·
γ(m −2)
...
...
...
γ(m −2)
γ(m −3)
· · ·
γ(1)
γ(m −1)
γ(m −2)
· · ·
γ(0)







.
The calculation of the log-likelihood proceeds analogously to the previous full
dimension case. This representation is particularly useful for interpolation of
missing values since the state noise is uncorrelated with the observation noise.
5.10
ESTIMATION OF LONG-MEMORY PROCESSES
This section discusses some speciﬁc estimation methods developed to deal with
the estimation of long-range dependent time series. Among these techniques
we consider maximum-likelihood procedures based on AR and MA approx-
imations,a log-periodogram regression, the so-called rescaled range statistic
(R/S), the variance plots, the detrended ﬂuctuation analysis, and a wavelet-
based approach.

162
ESTIMATION METHODS
5.10.1
Autoregressive Approximations
Given that the computation of exact ML estimates is computationally de-
manding, many authors have considered the use of autoregressive approxima-
tions to speed up the calculation of parameter estimates. Let {yt : t ∈Z} be
a long-memory process deﬁned by the autoregressive expansion
yt = εt + π1(θ)yt−1 + π2(θ)yt−2 + π3(θ)yt−3 + · · · ,
where πj(θ) are the coeﬃcients of φ(B)θ−1(B)(1 −B)d. Since in practice
only a ﬁnite number of observations is available, {y1, . . . , yn}, the following
truncated model is considered
yt = eεt + π1(θ)yt−1 + π2(θ)yt−2 + · · · + πm(θ)yt−m,
(5.6)
for m < t ≤n. Then, the approximate maximum-likelihood estimate bθn is
obtained by minimizing the function
L1(θ) =
n
X
t=m+1
[yt −π1(θ)yt−1 −π2(θ)yt−2 −· · · −πm(θ)yt−m]2.
(5.7)
Many improvements can be made on this basic framework to obtain bet-
ter estimates.
In the following subsections, we describe some of these re-
ﬁnements.
For simplicity, an estimator produced by the maximization of
an approximation of the Gaussian likelihood function will be called quasi-
maximum-likelihood estimate (QMLE).
5.10.2
Haslett-Raftery Method
Consider an ARFIMA process.
An approximate one-step forecast of yt is
given by
byt = φ(B)θ(B)−1
t−1
X
j=1
φtjyt−j,
(5.8)
with prediction error variance
vt = Var(yt −byt) = σ2
yκ
t−1
Y
j=1
(1 −φ2
jj),
where σ2
y = Var(yt), κ is the ratio of the innovations variance to the variance
of the ARMA(p,q) process
φtj = −
t
j
Γ(j −d)Γ(t −d −j + 1)
Γ(−d)Γ(t −d + 1)
,
(5.9)

ESTIMATION OF LONG-MEMORY PROCESSES
163
for j = 1, . . . , t.
To avoid the computation of a large number of coeﬃcients φtj, the last
term of the predictor (5.8) is approximated by
t−1
X
j=1
φtjyt−j ≈
M
X
j=1
φtjyt−j −
t−1
X
j=M+1
πjyt−j,
(5.10)
since φtj ∼−πj for large j, where for simplicity πj denotes πj(θ).
An additional approximation is made to the second term on the right-hand
side of (5.10):
t−1
X
j=M+1
πjyt−j ≈MπMd−1
"
1 −
M
t
d#
¯yM+1,t−1−M,
where ¯yM+1,t−1−M =
1
t−1−2M
Pt−1−M
j=M+1 yj. Hence, a QMLE bθn is obtained by
maximizing
L2(θ) = constant −1
2n log[bσ2(θ)],
with
bσ2(θ) = 1
n
n
X
t=1
(yt −byt)2
vt
.
The Haslett-Raftery algorithm has numeric complexity of order O(nM).
Therefore, if the truncation parameter M is ﬁxed, then this method is order
O(n). Thus, it is usually faster than the Cholesky Decomposition and the
Durbin-Levinson method. It has been suggested that M = 100 works ﬁne in
most applications. Besides, by setting M = n we get the exact ML estimator
for the fractional noise process. But, the numerical complexity in situation is
order O(n2).
Another autoregressive approximation method is described next. Consider
the following Gaussian innovation sequence:
εt = yt −
∞
X
j=1
πj(θ)yt−j.
Since the values {yt, t ≤0} are not observed, an approximate innovation
sequence {ut} may be obtained by assuming that yt = 0 for t ≤0,
ut = yt −
t−1
X
j=1
πj(θ)yt−j,
for j = 2, . . . , n. Let rt(θ) = ut(θ)/σ and θ = (σ, φ1, . . . , φp, θ1, . . . , θq, d).
Then, a QMLE for θ is provided by the minimization of
L2(θ) = 2n log(σ) +
n
X
t=2
r2
t (θ).

164
ESTIMATION METHODS
Now, by taking partial derivatives with respect to θ, the minimization problem
is equivalent to solving the nonlinear equations
n
X
t=2
{rt(θ) ˙rt(θ) −E[rt(θ) ˙rt(θ)]} = 0,
(5.11)
where ˙rt(θ) =

∂rt(θ)
∂θ1 , . . . , ∂rt(θ)
∂θr
′
.
5.10.3
A State Space Method
The state space methodology may be also used for handling autoregressive
approximations. For instance, starting from the AR(m) truncation (5.6) and
dropping θ from the coeﬃcients πj and the tilde from εt, we have
yt = π1yt−1 + · · · + πmyt−m + εt.
Thus, we may write the following state space system:
xt+1
=
Fxt + Hεt+1,
yt
=
Gxt,
where the state is given by xt = [yt
yt−1 . . . yt−m+2
yt−m+1]′, the state
transition matrix is
F =









π1
π2
· · ·
πm−1
πm
1
0
· · ·
0
0
0
1
· · ·
0
0
...
...
0
0
· · ·
0
0
0
0
· · ·
1
0









,
the observation matrix is
G = [1 0 0 · · · 0],
and the state noise matrix is given by
H = [1 0 0 · · · 0]′.
The variance of the observation noise is R = 0, the covariance between the
state noise and the observation noise is S = 0 and the state noise variance-
covariance matrix is given by
Q = σ2







1
0
· · ·
0
0
0
· · ·
0
...
...
0
0
· · ·
0
0
0
· · ·
0







.

ESTIMATION OF LONG-MEMORY PROCESSES
165
The Kalman recursions for this system are
∆t
=
ωt(1, 1),
Θt
=
" m
X
i=1
πiωt(i, 1) ωt(1, 1)
· · ·
ωt(m −1, 1)
#′
,
ωt+1(1, 1)
=
σ2 +
m
X
i,j=1
πiωt(i, j)πj −
" m
X
i=1
πiωt(i, 1)
#2
/ωt(1, 1),
ωt+1(1, j)
=
m
X
i=1
πiωt(i, j) −
" m
X
i=1
πiωt(i, 1)
#
ωt(1, j)
ωt(1, 1) for j ≥2,
ωt+1(i, j)
=
ωt(i −1, j −1) −ωt(i −1, 1)ωt(j −1, 1)/ωt(1, 1) for i, j ≥2,
bxt+1(1)
=
m
X
i=1
πibxt(i) +
" m
X
i=1
πiωt(i, 1)
#
yt −bxt(1)
ωt(1, 1) ,
bxt+1(j)
=
bxt(j −1) + ωt(j −1, 1) yt −bxt(1)
ωt(1, 1)
for j ≥2,
byt+1
=
bxt+1(1).
The initial conditions for these iteration may be
bx0 = 0,
and
Ω0 =







γ(0)
γ(1)
· · ·
γ(m −1)
γ(1)
γ(0)
· · ·
γ(m −2)
...
...
...
γ(m −2)
γ(m −3)
· · ·
γ(1)
γ(m −1)
γ(m −2)
· · ·
γ(0)







.
5.10.4
Moving-Average Approximations
An alternative methodology to autoregressive approximations is the trunca-
tion of the Wold expansion of a long-memory process. Two advantages of
this approach are the easy implementation of the Kalman ﬁlter recursions
and the simplicity of the analysis of the theoretical properties of the ML
estimates. Besides, if the long-memory time series is diﬀerenced, then the
resulting moving-average truncation has smaller error variance than the au-
toregressive approximation.
A causal representation of an ARFIMA(p, d, q) process {yt} is given by
yt =
∞
X
j=0
ψjεt−j,
(5.12)

166
ESTIMATION METHODS
and we may consider an approximate model for (5.12) given by
yt =
m
X
j=0
ψjεt−j,
(5.13)
which corresponds to a MA(m) process in contrast to the MA(∞) process
(5.12). A canonical state space representation of the MA(m) model (5.13) is
given by
xt+1
=
Fxt + Hεt,
yt
=
Gxt + εt,
with
xt = [y(t|t −1) y(t + 1|t −1) · · · y(t + m −1|t −1)]′ ,
where y(t + j|t −1) = E[yt+j|yt−1, yt−2, · · · ] and system matrices
F =

0
Im−1
0
0

,
G =
 1
0
· · ·
0 
,
H = [ψ1 · · · ψm ]′ .
The approximate representation of a causal ARFIMA(p, d, q) has compu-
tational advantages over the exact one. In particular, the order of the MLE
algorithm is reduced from O(n3) to O(n).
The log-likelihood function, excepting a constant, is given by
L(θ) = −1
2
( n
X
t=1
log∆t(θ) +
n
X
t=1
[yt −byt(θ)]2
∆t(θ)
)
,
where θ = (φ1, . . . , φp, θ1, . . . , θq, d, σ2) is the parameter vector associated to
the ARFIMA representation (5.2).
In order to evaluate the log-likelihood function L(θ) we may choose the
initial conditions bx1 = E[x1] = 0 and Ω1 = E[x1x′
1] = [ω(i, j)]i,j=1,2,... where
ω(i, j) = P∞
k=0 ψi+kψj+k.
The evolution of the state estimation and its variance, Ωt, is given by the
following recursive equations. Let δi = 1 if i ∈{0, 1, . . . , m −1} and δi = 0
otherwise. Furthermore, let δij = δiδj. Then, the elements of Ωt+1 and xt+1
are as follows:
∆t
=
ωt(1, 1) + 1,
(5.14)
ωt+1(i, j)
=
ωt(i + 1, j + 1)δij + ψiψj
−[ωt(i + 1, 1)δi + ψi][ωt(j + 1, 1)δj + ψj]
ωt(1, 1) + 1
,
(5.15)
the state estimation is
bxt+1(i) = bxt(i + 1)δi + [ωt(i + 1, 1)δi + ψi][yt −bxt(1)]
ωt(1, 1) + 1
,
(5.16)

ESTIMATION OF LONG-MEMORY PROCESSES
167
and the observation predictor is given by
byt = Gbxt = bxt(1).
A faster version of the previous algorithm can be obtained by diﬀerencing
the series {yt} since the inﬁnite MA representation of the diﬀerenced series
converges more rapidly than the MA expansion of the original process. To
illustrate this approach, consider the diﬀerenced process
zt = (1 −B)yt =
∞
X
j=0
ϕjεt−j,
(5.17)
where ϕj = ψj −ψj−1.
Remark 5.1. It is worth noting that the process {zt} is stationary and in-
vertible for any d ∈(0, 1
2), provided that the AR(p) and MA(q) polynomials
do not have common roots and all their roots are outside the closed unit disk.
By truncating the MA(∞) expansion (5.17) after m components, we get
the approximate model
zt =
m
X
j=0
ϕjεt−j.
(5.18)
An advantage of this approach is that, as shown in Problem 2.30, the coef-
ﬁcients ϕj converge faster to zero than the coeﬃcients ψj. Consequently, a
smaller truncation parameter m is necessary to achieve a good approximation
level.
The truncated model (5.18) can be represented in terms of a state space
system as
xt+1
=
 0
Im−1
0
0

xt +


ϕ1
...
ϕm

εt,
zt
=
 1
0
· · ·
0 
xt + εt.
Under normality, the log-likelihood function of the truncated model (5.18)
may be written as
Ln(θ) = −1
2n log det Tn,m(θ) −1
2nz′Tn,m(θ)−1z,
(5.19)
where [Tn,m(θ)]r,s=1,...,n =
R π
−π fm,θ(λ)e˙ıλ(r−s)dλ, is the covariance matrix of
z = (z1, ..., zn)′ with fm,θ(λ) = (2π)−1σ2|ϕm(e˙ıλ)|2 and the polynomial ϕm(·)
is given by
ϕm(e˙ıλ) = 1 + ϕ1e˙ıλ + · · · + ϕmem˙ıλ.

168
ESTIMATION METHODS
The matrices involved in the truncated Kalman equations are of size m×m.
Thus, only O(m2) evaluations are required for each iteration and the algorithm
has an order O(n×m2). For a ﬁxed truncation parameter m, the calculation of
the likelihood function is only of order O(n) for the approximate ML method.
Therefore, for very large samples, it may be desirable to consider truncating
the Kalman recursive equations after m components. With this truncation,
the number of operations required for a single evaluation of the log-likelihood
function is reduced to O(n).
It is worth noting that the autoregressive, AR(m), and the moving-average,
MA(m), approximations produce algorithms with numerical complexity of
order O(n), where n is the sample size. Nevertheless, the quality of these
approximations is governed by the truncation parameter m. The variance of
the truncation error for an AR(m) approximation is of order O(1/m) while
this variance is of order O(m2d−1) in the MA(m) case. On the other hand, the
truncation error variance is of order O(m2d−3) for the diﬀerenced approach.
The methods reviewed so far apply to Gaussian processes. However, if this
assumption is dropped, we still can ﬁnd well-behaved Whittle estimates. For
example, let {yt} be a stationary process with Wold decomposition:
yt =
∞
X
j=0
ψj(θ)εt−j,
where εt is an independent and identically distributed sequence with ﬁnite
four cumulant and P∞
j=0 ψ2
j (θ) < ∞.
The following result establishes the
consistency and the asymptotic normality of the Whittle estimate under these
circumstances. Let bθn be the value that maximizes the log-likelihood function
L5(θ). Then, under some regularity conditions, bθn is consistent and √n(bθn −
θ0) →N[0, Γ(θ0)−1] as n →∞, where Γ(θ0) is the matrix deﬁned in (5.27).
It is important to emphasize that this result does not assume the normality
of the process.
5.10.5
Semiparametric Approach
In this subsection we analyze a generalization of the Whittle approach called
the Gaussian semiparametric estimation method. This technique does not
require the speciﬁcation of a parametric model for the data. It only relies on
the speciﬁcation of the shape of the spectral density of the time series.
Assume that {yt} is a stationary process with spectral density satisfying
f(λ) ∼Gλ1−2H,
as λ →0+, with G ∈(0, ∞) and H ∈(0, 1). Observe that for an ARFIMA
model, the terms G and H correspond to σ2θ(1)2/[2πφ(1)2] and 1
2 + d, re-

ESTIMATION OF LONG-MEMORY PROCESSES
169
spectively. Let us deﬁne Q(G, H) as the objective function
Q(G, H) = 1
m
m
X
j=1
"
log Gλ1−2H
j
+
λ2H−1
j
G
I(λj)
#
,
where m is an integer satisfying m < n/2. If ( bG, bH) is the value that minimizes
Q(G, H), then under some regularity conditions such as
1
m + m
n →0,
as n →∞, the estimator bH is consistent and √m( bH −H0) →N
 0, 1
4

as
n →∞.
5.10.6
Periodogram Regression
Under the assumption that the spectral density of a stationary process may
be written as
f(λ) = f0(λ)[2 sin(λ/2)]−2d,
(5.20)
we may consider the following regression method for parameter estimation.
Taking logarithms on both sides of (5.20) and evaluating the spectral den-
sity at the Fourier frequencies λj = 2πj/n, we have that
log f(λj) = log f0(0) −d log

2 sin λj
2
2
+ log
f0(λj)
f0(0)

.
(5.21)
On the other hand, the logarithm of the periodogram I(λj) may be written
as
log I(λj) = log
 I(λj)
f(λj)

+ log f(λj).
(5.22)
Now, combining (5.21) and (5.22) we have
log I(λj) = log f0(0) −d log

2 sin λj
2
2
+ log
I(λj)[2 sin(λ/2)]2d
f0(0)

.
By deﬁning yj = log I(λj), α = log f0(0), β = −d, xj = log[2 sin(λj/2)]2, and
εj = log
I(λj)[2 sin(λ/2)]2d
f0(0)

,
we obtain the regression equation
yj = α + βxj + εj.

170
ESTIMATION METHODS
In theory, one could expect that for frequencies near zero (that is, for j =
1, . . . , m with m ≪n)
f(λj) ∼f0(0)[2 sin(λj/2)]−2d,
so that
εj ∼log
 I(λj)
f(λj)

.
The least squares estimate of the long-memory parameter d is given by
bdm = −
Pm
j=1(xj −¯x)(yj −¯y)
Pm
j=1(xj −¯x)2
,
where ¯x = Pm
j=1 xj/m and ¯y = Pm
j=1 yj/m.
5.10.7
Rescaled Range Method
Consider the sample {y1, . . . , yn} from a stationary long-memory process and
let xt be the partial sums of {yt}, that is, xt = Pt
j=1 yj for t = 1, . . . , n and
let s2
n = Pn
t=1(yt −¯y)2/(n −1) be the sample variance where ¯y = xn/n.
The rescaled range statistic (R/S) is deﬁned by
Rn = 1
sn

max
1≤t≤n

xt −t
nxn

−min
1≤t≤n

xt −t
nxn

.
This statistic satisﬁes the following asymptotic property. Let {yt : t ∈Z} be
a zero-mean stationary process such that y2
t is ergodic and
n−1
2 −dx[tn] →Bd(t),
in distribution, as n →∞, where Bd(t) is the fractional Brownian motion
deﬁned in Subsection A.2.8. Deﬁne Qn = n−1/2−dRn, then
Qn →Q,
in distribution, as n →∞, where
Q = sup
0≤t≤1
[Bd(t) −tBd(1)] −inf
0≤t≤1 [Bd(t) −tBd(1)] .
Note that log Rn = E Qn + (d + 1
2) log n + (log Qn −E Qn), so that we
can obtain an estimator of the long-memory parameter d by a least squares
technique similar to the one studied in Subsection 5.10.6. For instance, if
Rt,k is the R/S statistic based on the sample of size k, {yt, . . . , yt+k−1} for
1 ≤t ≤n−k+1, then an estimator of d can be obtained by regressing log Rt,k
on log k for 1 ≤t ≤n −k + 1.

ESTIMATION OF LONG-MEMORY PROCESSES
171
5.10.8
Variance Plots
According to (2.29), the variance of the sample mean of a long-memory process
based on m observations behaves like
Var(¯ym) ∼c m2d−1,
for large m, where c is a positive constant. Consequently, by dividing a sample
of size n, {y1, . . . , yn}, into k blocks of size m each with n = k × m, we have
log Var(¯yj) ∼c + (2d −1)log j,
(5.23)
for j = 1, . . . , k, where ¯yj is the average of the jth block, that is,
¯yj = 1
m
j×m
X
t=(j−1)×m+1
yt.
From (5.23), a heuristic least squares estimator of d is
bd = 1
2 −
Pk
j=1(log j −a)[log Var(¯yj) −b]
2 Pk
j=1(log j −a)2
,
where a = (1/k) Pk
j=1 log j and b = (1/k) Pk
j=1 log Var(¯yj).
Thus, for a short-memory process, d = 0, and then the slope of the line
described by equation (5.23) should be −1. On the other hand, for a long-
memory process with parameter d, the slope is 2d −1.
EXAMPLE 5.2
In order to illustrate the use of the variance plot technique, Figure 5.1
displays a variance plot for the Nile river data, from the period 622 A.D.
to 1221 A.D. Notice that heavily line and the dotted line appears to have
a very diﬀerent slope. This is an indication of long-memory behavior of
the data. On the contrary, in the variance plot of a Gaussian white noise
sequence shown in Figure 5.2, the slopes of both lines are similar. This
is expected from a serially uncorrelated time series.
5.10.9
Detrended Fluctuation Analysis
Let {y1, . . . , yn} be a sample from a stationary long-memory process and
let {xt} be the sequence of partial sums of {yt}, that is, xt = Pt
j=1 yj for
t = 1, . . . , n. The so-called detrended ﬂuctuation analysis (DFA) method for
estimating the long-memory parameter d of the process {yt : t ∈Z} proceeds
as follows. The sample {y1, . . . , yn} is divided into k nonoverlapping blocks,
each containing m = n/k observations. Within each block, we ﬁt a linear

172
ESTIMATION METHODS
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
8.0
8.2
8.4
8.6
Log  k
Log  Var (k)
Figure 5.1
Variance Plot for the Nile river Data.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
-3.5
-3.0
-2.5
-2.0
-1.5
-1.0
Log  k
Log  Var (k)
Figure 5.2
Variance Plot for a white noise sequence.

ESTIMATION OF LONG-MEMORY PROCESSES
173
regression model to xt versus t = 1, . . . , m. Let σ2
k be the estimated residual
variance from the regression within block k,
σ2
k = 1
m
m
X
t=1
(xt −bαk −bβkt)2,
where bαk and bβk are the least squares estimators of the intercept and the
slope of the regression line.
Let F 2(k) be the average of these variances
F 2(k) = 1
k
k
X
j=1
σ2
j .
For a random walk this term behaves like
F(k) ∼c k1/2,
while for a long-range sequence,
F(k) ∼c kd+1/2.
Thus, by taking logarithms, we have
log F(k) ∼log c + (d + 1
2) log k.
Therefore, by ﬁtting a least squares regression model to
log F(k) = α + β log k + εk,
(5.24)
for k ∈K, we may obtain an estimate of d as
bd = bβ −1
2,
where bβ is the least squares estimator of the parameter β.
There are several ways to select the set of indexes K. If k0 = min{K}
and k1 = max{K}, then, for example, some authors choose k0 = 4 and k1
a fraction of the sample size n. Of course, for k0 = 2 the regression error
variance is zero since only two observations are ﬁtted by the straight line. On
the other hand, for k1 = n, there is only one block and therefore the average
of error variances is taken over only one sample.
This methodology derives from the following theoretical result about the
behavior of the residual variances {σ2
k}. Let {yt} be a fractional Gaussian
noise process—see deﬁnition (2.31)—and let σ2
k be the residual variance from
the least squares ﬁtting in block k. Then,
E[σ2
k] ∼c(d) m2d+1,
as m →∞where the constant c(d) is given by the formula
c(d) =
1 −2d
(d + 1)(2d + 3)(2d + 5).

174
ESTIMATION METHODS
5.10.10
A Wavelet-Based Method
Consider the discrete wavelet transform coeﬃcients djk and deﬁne the statis-
tics
bµj = 1
nj
nj
X
k=1
bd2
jk,
where nj is the number of coeﬃcients at octave j available to be calculated.
As shown by Veitch and Abry (1999),
bµj ∼zj
nj
Xnj,
where zj = 22djc, c > 0, and Xnj is a chi-squared random variable with nj
degrees of freedom. Thus, by taking logarithms we may write
log2 bµj ∼2dj + log2 c + log Xnj/ log 2 −log2 nj.
Recall that the expected value and the variance of the random variable log Xn
are given by
E(log Xn)
=
ψ(n/2) + log 2,
Var(log Xn)
=
ζ(2, n/2),
where ψ(z) is the psi function, ψ(z) = d/dz log Γ(z), and ζ(z, n/2) is the
Riemann zeta function.
By deﬁning εj = log2 Xnj −log2 nj −gj, where gj = ψ(nj/2)/ log 2 −
log2(nj/2), we conclude that this sequence satisﬁes
E(εj)
=
0,
Var(εj)
=
ζ(2, nj/2)
(log 2)2 .
Therefore, we could write the following heteroskedastic regression equation:
yj = α + βxj + εj,
where yj = log2 bµj −gj, α = log2 c, β = 2d, and xj = j. Thus, once the
weighted linear regression estimate bβ is obtained, an estimate for the long-
memory parameter d is given by bd = bβ/2. Furthermore, an estimate of the
variance of bd is provided by the estimate of the variance of bβ by means of the
expression Var(bd) = Var(bβ)/4.

ESTIMATION OF LONG-MEMORY PROCESSES
175
EXAMPLE 5.3
The R package fArma allows for the estimation of long-range dependent
processes by several methods. The following are the results of the appli-
cation of some of these fractional estimation techniques to the Nile river
data. Notice that in these outputs, the Hurst parameter correspond to
H = d + 1
2.
> perFit(nile)
Title:
Hurst Exponent from Periodgram Method
Call:
perFit(x = nile)
Method:
Periodogram Method
Hurst Exponent:
H
beta
0.9926786 -0.9853571
Hurst Exponent Diagnostic:
Estimate
Std.Err
t-value
Pr(>|t|)
X 0.9926786 0.115791 8.573023 3.56693e-12
Parameter Settings:
n cut.off
663
10
> rsFit(nile)
Title:
Hurst Exponent from R/S Method
Call:
rsFit(x = nile)
Method:
R/S Method
Hurst Exponent:
H
beta
0.8394554 0.8394554
Hurst Exponent Diagnostic:
Estimate
Std.Err
t-value
Pr(>|t|)
X 0.8394554 0.04625034 18.15025 1.711254e-21

176
ESTIMATION METHODS
Parameter Settings:
n
levels
minnpts cut.off1 cut.off2
663
50
3
5
316
> pengFit(nile)
Title:
Hurst Exponent from Peng Method
Call:
pengFit(x = nile)
Method:
Peng Method
Hurst Exponent:
H
beta
0.8962124 1.7924248
Hurst Exponent Diagnostic:
Estimate
Std.Err t-value
Pr(>|t|)
X 0.8962124 0.01609048 55.6983 4.843263e-38
Parameter Settings:
n
levels
minnpts cut.off1 cut.off2
663
50
3
5
316
> waveletFit(nile)
Title:
Hurst Exponent from Wavelet Estimator
Call:
waveletFit(x = nile)
Method:
Wavelet Method
Hurst Exponent:
H
beta
0.9031508 0.8063017
Hurst Exponent Diagnostic:
Estimate
Std.Err
t-value
Pr(>|t|)
X 0.9031508 0.08329106 10.84331 0.001678205
Parameter Settings:
length
order octave1 octave2
512
2
2
6

ESTIMATION OF LONG-MEMORY PROCESSES
177
5.10.11
Computation of Autocovariances
Precise and eﬃcient calculation of the ACF of an ARFIMA process is a cru-
cial aspect in the implementation of the Cholesky and the Durbin-Levinson
algorithms. Recall that a closed form expression for the ACF of an ARFIMA
model was discussed in previous sections.
Another approach for calculating the ACF is the so-called splitting method.
This method is based on the decomposition of the ARFIMA model into its
ARMA and its fractionally integrated (FI) parts. Let γ1(·) be the ACF of
the ARMA component and γ2(·) be the ACF of the fractional noise given by
(2.25). Then, the ACF of the corresponding ARFIMA process is given by the
convolution of these two functions:
γ(h) =
∞
X
j=−∞
γ1(j)γ2(j −h).
If this inﬁnite sum is truncated to m summands, then we obtain the approx-
imation
γ(h) ≈
m
X
j=−m
γ1(j)γ2(j −h).
From this expression, the ACF γ(·) can be eﬃciently calculated with a great
level of precision.
EXAMPLE 5.4
To illustrate the calculation of the ACF of a long-memory process con-
sider the ARFIMA(1, d, 1) model
(1 + φB)yt = (1 + θB)(1 −B)−dεt,
with Var(εt) = σ2 = 1.
An exact formula for the ACF of this model is given by
γ(h) = θC(d, −h, −φ) + (1 + θ2)C(d, 1 −h, −φ) + θC(d, 2 −h, −φ)
φ(φ2 −1)
.
On the other hand, an approximated ACF is obtained by the splitting
algorithm
γ(k) ≈
m
X
h=−m
γ0(h)γARMA(k −h),
where
γ0(h) = σ2
Γ(1 −2d)
Γ(1 −d)Γ(d)
Γ(h + d)
Γ(1 + h −d),

178
ESTIMATION METHODS
Table
5.2
Calculation
of
the
Autocorrelation
Function
of
ARFIMA(1, d, 1) Models
ACF
Lag
Method
d = 0.4, φ = 0.5, θ = 0.2
d = 0.499, φ = −0.9, θ = −0.3
0
Exact
1.6230971100284379
7764.0440304632230
Approx.
1.6230971200957560
7764.0441353477199
1
Exact
0.67605709850269124
7763.5195069108622
Approx.
0.67605707826745276
7763.5196117952073
2
Exact
0.86835879142153161
7762.8534907771409
Approx.
0.86835880133411103
7762.8535956613778
3
Exact
0.66265875439861421
7762.0404144912191
Approx.
0.66265877143805063
7762.0405193753304
998
Exact
0.22351300800718499
7682.7366067938428
Approx.
0.22351301379700828
7682.7367003641175
999
Exact
0.22346824274316196
7682.7212154918925
Approx.
0.22346824853234257
7682.7213090555442
and
γARMA(h) =









1 −2φθ + θ2
1 −φ2
h = 0,
(1 −φθ)(θ −φ)
1 −φ2
(−φ)|h|
h ̸= 0.
We consider two sets of parameters d = 0.4, φ = 0.5, θ = 0.2 and
d = 0.499, φ = −0.9, θ = −0.3, and several lags between 0 and 999. The
results are shown in Table 5.2.
Note that for the set of parameters d = 0.4, φ = 0.5, θ = 0.2, the
accuracy of the splitting method is about six signiﬁcant decimals while
for the second set of parameters, d = 0.499, φ = −0.9, θ = −0.3, the
accuracy drops to about three signiﬁcant decimals for the range of lags
studied.
5.11
NUMERICAL EXPERIMENTS
Table 5.3 displays the results from several simulations comparing ﬁve ML es-
timation methods for Gaussian processes: Exact MLE, Haslett and Raftery’s

NUMERICAL EXPERIMENTS
179
Table
5.3
Finite
Sample
Behavior
of
Maximum
Likelihood
Estimates
d
Exact
HR
AR
MA
Whittle
n = 200
0.40
Mean
0.3652
0.3665
0.3719
0.3670
0.3874
SD
0.0531
0.0537
0.0654
0.0560
0.0672
0.25
Mean
0.2212
0.2219
0.2224
0.2220
0.2156
SD
0.0612
0.0613
0.0692
0.0610
0.0706
0.10
Mean
0.0780
0.0784
0.0808
0.0798
0.0585
SD
0.0527
0.0529
0.0561
0.0525
0.0522
n = 400
0.40
Mean
0.3799
0.3808
0.3837
0.3768
0.3993
SD
0.0393
0.0396
0.0444
0.0402
0.0466
0.25
Mean
0.2336
0.2343
0.2330
0.2330
0.2350
SD
0.0397
0.0397
0.0421
0.0394
0.0440
0.10
Mean
0.0862
0.0865
0.0875
0.0874
0.0753
SD
0.0394
0.0395
0.0410
0.0390
0.0413
approach, AR(40) approximation, MA(40) approximation, and the Whittle
method.
The process considered is a fractional noise ARFIMA(0, d, 0) with three
values of the long-memory parameter: d = 0.1, 0.25, 0.4, Gaussian innovations
with zero-mean and unit variance, and sample sizes n = 200 and n = 400. The
mean and standard deviations of the estimates are based on 1000 repetitions.
All the simulations reported in Table 5.3 were carried out by means of R
programs.
From Table 5.3, it seems that all estimates are somewhat downward bi-
ased for the three values of d and the two sample sizes considered. All the
estimators, excepting the Whittle; seem to behave similarly in terms of bias
and standard deviation. The sample standard deviations of all the methods
considered are relatively close to its theoretical value 0.05513 for n = 200 and
0.03898 for n = 400. Observe that the Whittle method exhibits less bias for
d = 0.4 but greater bias for d = 0.1. Besides, this procedure seems to have
greater standard deviations than the other estimators, for the three values of
d and the two sample sizes under study.

180
ESTIMATION METHODS
5.12
BAYESIAN ESTIMATION
This section discusses some applications of the Bayesian methodology to the
analysis of time series data. It describes a general Bayesian framework for
the analysis of ARMA and ARFIMA processes by means of the Markov chain
Monte Carlo (MCMC) methodology, an important computational tool for
obtaining samples from a posterior distribution. In particular, we describe
applications of the Metropolis-Hastings algorithm and the Gibbs sampler in
the context of long-memory processes.
The implementation of these computational procedures are illustrated with
an example of Bayesian estimation of a stationary Gaussian process. Speciﬁc
issues such as selection of initial values and proposal distributions are also
discussed.
Consider the time series data Y = (y1, . . . , yn)′ and a statistical model
described by the parameter θ. Let f(y|θ) be the likelihood function of the
model and π(θ) a prior distribution for the parameter. According to the Bayes
theorem, the posterior distribution of θ given the data Y is proportional to
π(θ|Y ) ∝f(Y |θ)π(θ).
More speciﬁcally, suppose that the time series follows an ARFIMA(p, d, q)
model described by
φ(B)(yt −µ) = θ(B)(1 −B)−dεt,
where the polynomials φ(B) = 1 + φ1B + · · · + φpBp and θ(B) = 1 + θ1B +
· · · + θpBq do not have common roots and {εt} is a white noise sequence with
zero-mean and variance σ2. Deﬁne Cd = {d : yt is stationary and invertible},
Cφ = {φ1, . . . , φp : yt is stationary}, and Cθ = {θ1, . . . , θq : yt is invertible}.
For this model, the parameter vector may be written as
θ = (d, φ1, . . . , φp, θ1, . . . , θq, µ, σ2),
and the parameter space can be expressed as
Θ = Cd × Cφ × Cθ × IR × (0, ∞).
Sometimes, in order to simplify the speciﬁcation of a prior distribution
over the parameter space Θ, one may consider assigning prior distributions
individually to subsets of parameters. For instance, we may assume uniform
priors for d, φ1, . . . , φp, and θ1 . . . , θq, that is, π(d) = U(Cd), π(φ1, . . . , φp) =
U(Cφ), and π(θ1, . . . , θq) = U(Cθ). Besides, we may assume an improper prior
µ, π(µ) ∝1 and a prior π(σ2) for σ2.
With this speciﬁcation, the prior
distribution of θ is simply
π(θ) ∝π(σ2),

BAYESIAN ESTIMATION
181
and the posterior distribution of θ is given by
π(θ|Y ) ∝f(Y |θ)π(σ2).
(5.25)
Apart from the calculation of this posterior distribution, we are usually
interested in ﬁnding Bayes estimators for θ. For example, we may consider
ﬁnding the value of θ such that the posterior loss be minimal. That is, if
L(θ, Y ) is the loss function, then
bθ = argmin
Z
L(θ, Y )π(θ|Y ) dY .
As a particular case, under the quadratic loss L(θ, Y ) = ∥θ −Y ∥2 we have
that the estimate of θ is the posterior mean
bθ = E[θ|Y ].
Obtaining any of these quantities requires integration. However, in many
practical situations the calculation of these integrals may be extremely diﬃ-
cult. To circumvent this problem, several methodologies have been proposed
in the Bayesian literature, including conjugate prior distributions, numerical
integration, Monte Carlo simulations, Laplace analytical approximation, and
Markov chain Monte Carlo (MCMC) procedures. The analysis of all these
methods is beyond the scope of this text; here we will focus on MCMC tech-
niques.
5.12.1
Markov Chain Monte Carlo Methods
A MCMC algorithm produces a sample of a distribution of interest by a
method that combines Monte Carlo techniques and Markov chains. Consider,
for example, that we want to obtain a sample of the posterior distribution
π(θ|Y ).
Two well-known procedures for this purpose are the Metropolis-
Hastings algorithm and the Gibbs sampler.
5.12.2
Metropolis-Hastings Algorithm
Following the Metropolis-Hastings algorithm, we start with an initial value
for θ, θ(0), say. Suppose that at the stage m we have obtained the value θ(m).
We update this value to θ(m+1) according to the following procedure:
1. Generate the random variable ξ from the proposal distribution q(ξ|θ(m)),
ξ ∼q(ξ|θ(m)).
2. Deﬁne
α = min
(
π(ξ|Y )q(θ(m)|ξ)
π(θ(m)|Y )q(ξ|θ(m))
, 1
)
.

182
ESTIMATION METHODS
3. Generate ω ∼Ber(α).
4. Obtain
θ(m+1) =

ξ
if ω = 1,
θ(m)
if ω = 0.
The convergence of this procedure is guaranteed by the following result: As-
sume that the support of the proposal distribution q contains the support of
the posterior distribution π. Then π(θ(m)|Y ) converges to the unique station-
ary distribution of the Markov chain π(θ|Y ) as m →∞.
5.12.3
Gibbs Sampler
Another well-known iterative method is the Gibbs sampler. Suppose that the
random variable θ can be decomposed as θ = (θ1, . . . , θr) and we are able to
simulate from the conditional densities
θj|θ1, . . . , θj−1, θj+1, . . . , θr ∼fj(θj|θ1, . . . , θj−1, θj+1, . . . , θr),
for j = 1, . . . , r. In order to sample from the joint density of (θ1, . . . , θr) we
proceed according to the following algorithm:
0.
Given the sample (θ(m)
1
, . . . , θ(m)
r
), generate
1.
θ(m+1)
1
∼f1(θ1|θ(m)
2
, θ(m)
3
, . . . , θ(m)
r
),
2.
θ(m+1)
2
∼f2(θ2|θ(m)
1
, θ(m)
3
, . . . , θ(m)
r
),
...
r.
θ(m+1)
r
∼fr(θr|θ(m)
1
, θ(m)
2
, . . . , θ(m)
r−1).
The acceptance rate in this algorithm is always one, that is, all simulated
values are accepted.
A nice property of the Gibbs sampler is that all the
simulations may be univariate. On the other hand, this algorithm requires
that we can actually simulate samples from every conditional density fj for
j = 1, . . . , r. By choosing adequately these densities, it can be shown that the
Gibbs sampler is a particular case of the Metropolis-Hastings algorithm.
EXAMPLE 5.5
If the process {yt} is Gaussian, then the likelihood function is given by
f(Y |θ)
=
(2πσ2)−n/2|Γ(θ)|−1/2
×
exp
(
−(Y −1µ)′ Γ (θ)−1 (Y −1µ)
2σ2
)
,
(5.26)

BAYESIAN ESTIMATION
183
where Γ(θ) = Var(Y ). Hence, the posterior distribution of θ given by
(5.25) is
π(θ|Y )
∝
(2π)−n/2σ−n|Γ(θ)|−1/2
×
exp
(
−
Y −1µ
′Γ(θ)
−1Y −1µ

2σ2
)
π(σ2).
In this case, the MCMC method can be implemented as follows. To
simplify the notation, we write the parameter θ as (d, φ, θ, σ2, µ) where
φ = (φ1, . . . , φp) and θ = (θ1, . . . , θq).
Consider an initial sample for (d, φ, θ), for example,

d(0), φ(0), θ(0)
∼Nr
h
bd, bφ, bθ

, Σ
i
,
where Nr is a multivariate Gaussian distribution with r = p + q + 3 and

bd, bφ, bθ

is the MLE of (d, φ, θ) and Σ may be obtained from the Fisher
information matrix, that is,
Σ =
h
H

bd, bφ, bθ
i−1
,
where H is the Hessian matrix of the log-likelihood function of the data
derived from (5.26).
Given the value
 d(m), φ(m), θ(m)
, we generate ξ from the proposal
distribution q
 ξ|d(m), φ(m), θ(m)
according to
ξ ∼Nr
h
d(m), φ(m), θ(m)
, Σ
i
,
and restricting the random variable ξ to the space Cd ×Cφ ×Cθ to ensure
the stationarity and the invertibility of the ARFIMA process.
Now, we calculate α as
α = min

π[ξ|Y ]
π[d(m), φ(m), θ(m)|Y ], 1

,
since in this case q(θ|ξ) = q(ξ|θ).
Then we proceed to steps 3 and 4 of the MCMC method described
above. Once (d, φ, θ) has been updated, we update µ and σ2.
For updating µ, one may start with an initial drawing from a normal
distribution
µ ∼N(bµ, bσ2
µ),
where bµ is an estimate of the location (e.g., the sample mean) and
bσ2
µ = σ2
π
|θ(1)|2
|φ(1)|2
Γ(1 −2d)
d(2d + 1) sin(πd)n2d−1,

184
ESTIMATION METHODS
cf., equation (2.28), where the values of d, φ(B), θ(B), and σ2 are re-
placed by their respective MLEs. Notice, however, that we may use an
overdispersed distribution (e.g., Student distribution), see for example
Problem 12.5.
Given the sample µ(m), this term may be updated to µ(m+1) by gen-
erating a random variable
ξ ∼N(µ(m), bσ2
µ(m)),
and then calculating
α = min

π[d(m), φ(m), θ(m), ξ, bσ2|Y ]
π[d(m), φ(m), θ(m), µ(m), bσ2|Y ], 1

.
Finally, for updating σ2 one may draw samples from an inverse gamma
distribution IG(α, β) where the coeﬃcients α and β can be chosen in
many diﬀerent ways.
A simple approach is to consider that for an
ARFIMA model with r parameters the MLE of σ2, bσ2, satisﬁes ap-
proximately
bσ2 ∼
σ2
n −rχ2
n−r,
for large n. Therefore, E[bσ2] = σ2 and Var[bσ2] = 2σ4/(n −r). Hence,
by matching these moments with the coeﬃcients α and β we have
α = n −r + 4
2
,
β = bσ2(n −r + 2)
2
.
Naturally, there are many other choices for drawing samples for σ2,
including Gamma distributions.
5.13
STATISTICAL INFERENCE
The maximum likelihood estimates of ARMA, ARIMA, ARFIMA and their
seasonal counterparts are asymptotically unbiased, normally distributed and
eﬃcient. This is formally stated as follows. Let bθn be the value that maximizes
the exact log-likelihood where
θ = (φ1, ..., φp, θ1, ..., θq, d)′
is a p + q + 1 dimensional parameter vector and let θ0 be the true parameter.
Under some regularity conditions we have
(a) Consistency: bθn →θ0 in probability as n →∞.

STATISTICAL INFERENCE
185
(b) Normality: √n(bθn −θ0) →N(0, Γ−1(θ0)), as n →∞, where Γ(θ) =
(Γij(θ)) with
Γij(θ) = 1
4π
Z π
−π
∂log fθ(λ)
∂θi
 ∂log fθ(λ)
∂θj

dλ,
(5.27)
where fθ is the spectral density of the process.
(c) Eﬃciency: bθn is an eﬃcient estimator of θ0.
In what follows we discuss the application of the previous results to the
analysis of the large sample properties of MLE for some well-known time
series models.
EXAMPLE 5.6
As an example, consider an ARMA(1, 1) model described by
yt −φyt−1 = εt −θεt−1.
In this case, the parameter vector is θ = (φ, θ) and the maximum likeli-
hood estimator bθn = (bφn, bθn) satisﬁes the following large sample distri-
bution
√n (bθn −θ) →N
 0, Γ(θ)−1
,
where
Γ(θ)−1) = 1 + φ θ
(φ + θ)2


(1 −φ2)(1 + φ θ)
−(1 −θ2)(1 −φ2)
−(1 −θ2)(1 −φ2)
(1 −θ2)(1 + φ θ)

,
EXAMPLE 5.7
For a fractional noise process with long-memory parameter d, FN(d),
the maximum-likelihood estimate bdn satisﬁes the following limiting dis-
tribution:
√n(bdn −d) →N

0, 6
π2

,
as n →∞. Observe that the asymptotic variance of this estimate does
not depend on the value of d.

186
ESTIMATION METHODS
EXAMPLE 5.8
Consider the ARFIMA(1, d, 1) model
(1 + φB)yt = (1 + θB)(1 −B)−dεt,
where {εt} is independent and identically distributed N(0, σ2).
The
parameter variance-covariance matrix Γ(d, φ, θ) may be calculated as
follows. The spectral density of this process is given by
f(λ) = σ2
2π [2(1 −cos λ)]−d 1 + θ2 + 2θ cos λ
1 + φ2 + 2φ cos λ.
Hence,
log f(λ)
=
log
σ2
2π

−d log[2(1 −cos λ)]
+ log[1 + θ2 + 2θ cos λ] −log[1 + φ2 + 2φ cos λ],
and the gradient is
∇log f(λ) =


−log[2(1 −cos λ)]
−
2[φ + cos λ]
1 + φ2 + 2φ cos λ
2[θ + cos λ]
1 + θ2 + 2θ cos λ


.
Thus, by dropping the parameters d, φ, and θ from the 3 × 3 matrix
Γ(d, φ, θ) we have
Γ11 = 1
4π
Z π
−π
{log[2(1 −cos λ)]}2 dλ = π2
6 ,
Γ12
=
1
4π
Z π
−π
{log[2(1 −cos λ)]}
2[φ + cos λ]
1 + φ2 + 2φ cos λdλ
=
1
2π
φ2 −1
φ
Z π
0
log[2(1 −cos λ)]
1 + φ2 + 2φ cos λdλ
+ 1
φ
Z π
0
log[2(1 −cos λ)]dλ

.
Thus, we have
Γ12 = −log(1 + φ)
φ
.

STATISTICAL INFERENCE
187
Analogously,
Γ13 = log(1 + θ)
θ
.
In addition, for the two ARMA parameters we have
Γ22
=
1
1 −φ2 ,
Γ23
=
−
1
1 −φθ,
Γ33
=
1
1 −θ2 .
Finally,
Γ(d, φ, θ) =


π2
6
−log(1+φ)
φ
log(1+θ)
θ
−log(1+φ)
φ
1
1−φ2
−
1
1−φθ
log(1+θ)
θ
−
1
1−φθ
1
1−θ2


.
(5.28)
Observe that similarly to the fractional noise case, the asymptotic vari-
ance of the MLE of an ARFIMA(1, d, 1) model does not depend on the
value of the long-memory parameter.
EXAMPLE 5.9
The asymptotic variance of the MLE of ARFIMA(1, d, 0) and ARFIMA(0, d, 1)
may be derived analogously to Example 5.8.
For the ARFIMA(1, d, 0) model we have that
Γ(d, φ) =


π2
6
−log(1+φ)
φ
−log(1+φ)
φ
1
1−φ2

,
and for the ARFIMA(0, d, 1) model
Γ(d, θ) =


π2
6
log(1+θ)
θ
log(1+θ)
θ
1
1−θ2

.
From these expressions, we conclude that the asymptotic correlation be-
tween the maximum-likelihood estimates bdn and bφn of the ARFIMA(1, d, 0)

188
ESTIMATION METHODS
model is
lim
n→∞corr (dn, φn) =
√
6
π

1 −φ2 log(1 + φ)
φ
,
(5.29)
which is always positive for φ ∈(−1, 1). On the other hand, the asymp-
totic correlation of the maximum-likelihood estimates dn and θn of an
ARFIMA(0, d, 1) model is given by
lim
n→∞corr (dn, θn) = −
√
6
π

1 −θ2 log(1 + θ)
θ
,
(5.30)
which is always negative for θ ∈(−1, 1). Observe that the asymptotic
correlation formulas (5.29) and (5.30) do not depend on the value of the
long-memory parameter d.
The asymptotic correlation between the maximum-likelihood esti-
mates dn and φn of an ARFIMA(1, d, 0) model provided by formula (5.29)
is displayed in Figure 5.3 for φ ∈(−1, 1).
Additionally, Figure 5.4
exhibits the theoretical asymptotic correlation between the maximum-
likelihood estimates dn and θn of an ARFIMA(0, d, 1) model given by
formula (5.30) for θ ∈(−1, 1). Notice from these ﬁgures that the cor-
relation between the estimators tends to 0 as φ →±1 or θ →±1. The
maximum (minimum) value of the correlation is reached near φ = −0.68
(θ = −0.68).
-1.0
-0.5
0.0
0.5
1.0
0.0
0.2
0.4
0.6
0.8
1.0
!
Correlation
Figure 5.3
ARFIMA(1, d, 0) example:
Asymptotic correlation between the
maximum-likelihood estimates dn and φn.

ILLUSTRATIONS
189
-1.0
-0.5
0.0
0.5
1.0
-1.0
-0.8
-0.6
-0.4
-0.2
0.0
!
Correlation
Figure 5.4
ARFIMA(0, d, 1) example:
Asymptotic correlation between the
maximum-likelihood estimates dn and θn.
5.14
ILLUSTRATIONS
To illustrate how the ﬁnite sample performance of the maximum-likelihood
estimates of ARFIMA models compare to the theoretical results revised in
this chapter consider the following Monte Carlo experiments.
Table 5.4 exhibits the maximum-likelihood parameter estimations from
simulated ARFIMA(1, d, 1) processes with sample size n = 1000 and pa-
rameters d = 0.3, φ = −0.5, and θ = 0.2. The results are based on 1000
replications.
Table 5.4
MLE Simulations for an ARFIMA(1, d, 1) Model with d =
0.3, φ = −0.5, and θ = 0.2
d
φ
θ
Sample mean
0.2775
-0.5054
0.1733
Sample SD
0.0514
0.0469
0.0843
Theoretical SD
0.0487
0.0472
0.0834

190
ESTIMATION METHODS
Notice that the sample mean and standard deviations are close to their
theoretical counterparts. The theoretical standard deviations are calculated
from formula (5.28).
Figure 5.5 shows 1,164 observations from a tree ring time series dataset,
see Appendix C for details. The sample ACF and sample PACF of this series
is exhibited in Figure 5.6
Based on the previous plots, a family of ARMA models is proposed with
orders p, q ≤2 to be selected according to the AIC and the signiﬁcance of the
parameters. Table 5.5 reports the estimated models for all the combinations
of orders.
Note that the ARMA(1, 1) presents the lowest AIC and botgh
parameters appears to be signiﬁcant at the 5% level. Consequently, this model
is selected for further analysis. Figure 5.7 displays both the theoretical ACF
based on the estimated parameters and the sample ACF. Observe that both
plots are similar. Furthermore, a comparison of the theoretical PACF and its
sample version is shown in Figure 5.8. According to these plots, the ACF and
PACF of ﬁtted model seems to be close to their theoretical counterparts.
In order to analysis the residuals of this model, Figure 5.9 exhibit its sample
ACF and the corresponding Box-Ljung tests up to lag 15. Observe that the
Time
Series
0
200
400
600
800
1000
1200
0
50
100
150
Figure 5.5
Tree ring time series data.

ILLUSTRATIONS
191
0
5
10
15
20
25
30
-0.05
0.00
0.05
0.10
0.15
0.20
0.25
Lag
Partial ACF
(a)
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 5.6
Tree ring data: Sample ACF and PACF
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
(a)
Lag
ACF
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Lag
Sample ACF
(b)
Figure 5.7
Tree ring time series data: Theoretical and Sample ACF.

192
ESTIMATION METHODS
2
4
6
8
10
-0.2
-0.1
0.0
0.1
0.2
0.3
0.4
0.5
(a)
Lag
Partial ACF
2
4
6
8
10
-0.2
-0.1
0.0
0.1
0.2
0.3
0.4
0.5
Lag
Sample PACF
(b)
Figure 5.8
Tree ring data: Theoretical and Sample PACF.
2
4
6
8
10
12
14
-0.10
0.00
0.10
(a)
Lag
ACF
2
4
6
8
10
12
14
0.0
0.4
0.8
(b)
Lag
Ljung-Box Test
Figure 5.9
Tree ring data: Theoretical and Sample PACF.

BIBLIOGRAPHIC NOTES
193
Table 5.5
Maximum Likelihood Estimation of ARMA models
p
q
φ1
φ2
θ1
θ2
AIC
tφ1
tφ2
tθ1
tθ2
LB
1
0.24
11522.74
8.38
77.46
0.07
2
0.22
0.08
11516.32
7.48
2.91
71.15
0.69
1
0.2
11534.74
7.7
84.42
0.00
1
1
0.61
-0.4
11514.73
6.16
-3.46
66.50
0.81
2
1
0.89
-0.09
-0.68
11515.44
5.23
-1.41
-4.07
61.98
0.92
2
0.22
0.12
11519.69
7.45
76.46
0.34
1
2
0.73
-0.52
-0.05
11515.75
6.15
-4.15
63.41
0.90
2
2
0.14
0.29
0.07
-0.19
11518.73
-
-
-
66.53
0.81
null hypothesis of white noise is not rejected at the 5% level for all the lags
considered.
5.15
BIBLIOGRAPHIC NOTES
Methods for estimating time series models have been extensively reviewed
in the literature.
A classical reference on ARMA model estimation is the
book by Box, Jenkins, and Reinsel (1994). On the other hand, estimation of
long-memory models have been considered by a large number of authors. An
overview of the technique discussed in this chapter can be found in Palma
(2007).
Autoregressive approximations have been studied by Granger and
Joyeux (1980), Li and McLeod (1986), Hasslett and Raftery (1989), Beran
(1994), Shumway and Stoﬀer (2011), and Bhansali and Kokoszka (2003),
among others. The Haslett-Raftery method discussed in Subsection 5.10.2
was introduced by Hasslett and Raftery (1989). State space estimation of
ARFIMA and related models have been investigated by Chan and Palma
(1998), Grassi and de Magistris (2014) and Dissanayake, Peiris, and Proietti
(2014), among others.
The Durbin-Levinson algorithm is based on the seminal works by Levinson
(1947) and Durbin (1960). The arithmetic complexity of this algorithm for a
linear stationary process have been discussed, for instance, by Ammar (1998).
The Durbin-Levinson algorithm can be implemented for an ARMA process
in only O(n) operations; see, for example, Section 5.3 of Brockwell and Davis
(1991). The splitting algorithm has been applied to the calculation of the
ACF of long-memory processes; see, for example, the numerical experiments
reported by Bertelli and Caporin (2002). Besides, several computational as-
pects of parameter estimation are discussed by Doornik and Ooms (2003).
The asymptotic properties of the MLE have been established by Yajima
(1985) for the fractional noise process and by Dahlhaus (1989); Dahlhaus and
Polonik (2006) for a general class of long-memory processes including the
ARFIMA model.
The so-called Whittle method was proposed by Whittle
(1951). A study comparing the properties of the R/S with other estimators
can be found in Giraitis, Kokoszka, Leipus, and Teyssi`ere (2003). The large
sample behavior of the periodogram of long-range-dependent processes has

194
ESTIMATION METHODS
been extensively studied; see, for example, Fox and Taqqu (1987) and Yajima
(1989), among others. Various estimators of the long-range dependence pa-
rameter including the R/S, DFA, and the Whittle methods are studied in the
article by Taqqu, Teverovsky, and Willinger (1995). The books by Robert
(2001) and Robert and Casella (2004) are excellent references on Bayesian
methodologies.
In particular, Robert (2001, Chapter 9) and Robert and
Casella (2004, Chapters 6 and 7) describe several computational techniques
including MCMC algorithms and the Gibbs sampler. Other good general ref-
erences on Bayesian methods are the books by Box and Tiao (1992) and Press
(2003). There are several versions of the MCMC algorithm. For example, the
one discussed here is based on the works by Metropolis, Rosenbluth, Teller,
and Teller (1953) and Hastings (1970). A comprehensive revision of Markov
chain methods is provided by Tierney (1994).
Problems
5.1
Consider the AR(2) process given by: xt = 1.5xt−1−0.75xt−2+4.1+ϵt.
(a) Is this a stationary process?
(b) Find µx and ρx.
(c) Write down and solve the Yule-Walker equations. Calculate ρx(3),
ρx(4), . . ., ρx(8).
5.2
A key tool for identifying time series processes are the ACF and the
partial ACF. Figures 5.10 to 5.15 show simulated time series corresponding
to the processes described below, along with their sample ACF and partial
ACF. Identify which plots correspond to the models (a) to (d).
(a) yt = 0.6 yt−1 + εt + 0.8 εt−1.
(b) yt = 0.70 yt−1 −0.12 yt−2 + εt.
(c) yt = −0.4 yt−1 + εt −εt−1 + 0.21 εt−2.
(d) yt = εt + 0.8 εt−2.
5.3
Consider the following MA(1) model where {εt} es WN(0, 1):
yt = εt + θ εt−1.
(a) Calculate the autocovariance function of the process.
(b) Find the moment estimator of θ.
(c) Show that the bias of this estimator is −θ/n.
(d) Assume that y1 = ε1 and deﬁne ε = (ε1, . . . , εn), y = (y1, . . . , yn).
Show that we can write Ly = ε where L is a lower triangular matrix
and ﬁnd it.
(e) Find the inverse L−1 and verify that the variance-covariance matrix
of y can be written as Σ = L−1(L−1)′.
(f) Show that Σ−1 = L′L.
5.4
Figure 5.16 shows a time series of 332 observations, its sample ACF
and PACF. We propose to ﬁt an ARMA(p, q) model to these data. Based on

PROBLEMS
195
(a)
Time
Series
0
100
200
300
400
500
-2
0
2
4
0
5
10
15
20
25
0.0
0.4
0.8
Lag
ACF
(b)
0
5
10
15
20
25
-0.1
0.1
0.3
0.5
Lag
Partial ACF
(c)
Figure 5.10
Time series I
(a)
Time
Series
0
100
200
300
400
500
-3
-1
1
3
0
5
10
15
20
25
0.0
0.4
0.8
Lag
ACF
(b)
0
5
10
15
20
25
-0.2
0.2
Lag
Partial ACF
(c)
Figure 5.11
Time series II

196
ESTIMATION METHODS
(a)
Time
Series
0
100
200
300
400
500
-3
-1
1
3
0
5
10
15
20
25
0.0
0.4
0.8
Lag
ACF
(b)
0
5
10
15
20
25
-0.2
0.2
Lag
Partial ACF
(c)
Figure 5.12
Time series III
(a)
Time
Series
0
100
200
300
400
500
-4
0
2
4
6
0
5
10
15
20
25
-0.5
0.5
1.0
Lag
ACF
(b)
0
5
10
15
20
25
-0.8
-0.4
0.0
Lag
Partial ACF
(c)
Figure 5.13
Time series IV

PROBLEMS
197
(a)
Time
Series
0
100
200
300
400
500
-5
0
5
0
5
10
15
20
25
0.0
0.4
0.8
Lag
ACF
(b)
0
5
10
15
20
25
-0.4
0.0
0.4
0.8
Lag
Partial ACF
(c)
Figure 5.14
Time series V
(a)
Time
Series
0
100
200
300
400
500
-4
-2
0
2
0
5
10
15
20
25
0.0
0.4
0.8
Lag
ACF
(b)
0
5
10
15
20
25
-0.2
0.2
0.6
Lag
Partial ACF
(c)
Figure 5.15
Time series VI

198
ESTIMATION METHODS
the R outputs from the ﬁtted models presented below, information criteria,
parameter signiﬁcance, diagnostic plots, etc., indicate if one of these models
adequately ﬁts the time series under study. In that case, select which model
seems more appropriate. Justify your answers.
MODEL 1
arima(x = y, order = c(2, 0, 2))
Coefficients:
ar1
ar2
ma1
ma2
intercept
0.2257
0.5803
0.2448
0.1128
0.0523
s.e.
0.1011
0.0975
0.1135
0.0793
0.3648
sigma^2 estimated as 0.9398:
log likelihood = -461.54,
aic = 935.08
> fit$coef
ar1
ar2
ma1
ma2
intercept
0.22574773 0.58031755 0.24476783 0.11277715 0.05225935
> # Significance tests
> fit$coef/sqrt(diag(fit$var.coef))
ar1
ar2
ma1
ma2 intercept
2.2331375 5.9546637 2.1559896 1.4219900 0.1432622
MODEL 2
arima(x = y, order = c(1, 0, 2))
Coefficients:
ar1
ma1
ma2
intercept
0.8337
-0.3546
0.3427
0.0266
s.e.
0.0398
0.0640
0.0500
0.3185
sigma^2 estimated as 0.9828:
log likelihood = -468.9,
aic = 947.8
> fit$coef
ar1
ma1
ma2
intercept
0.83374714 -0.35456348
0.34270842
0.02662152
> # Significance tests
> fit$coef/sqrt(diag(fit$var.coef))
ar1
ma1
ma2
intercept
20.9293222 -5.5394974
6.8490803
0.0835905

PROBLEMS
199
MODEL 3
arima(x = y, order = c(2, 0, 1))
Coefficients:
ar1
ar2
ma1
intercept
0.1546
0.6712
0.3052
0.0653
s.e.
0.0683
0.0551
0.0863
0.3901
sigma^2 estimated as 0.9456:
log likelihood = -462.55,
aic = 935.1
> fit$coef
ar1
ar2
ma1
intercept
0.15456221 0.67120405 0.30521334 0.06531289
> # Significance tests
> fit$coef/sqrt(diag(fit$var.coef))
ar1
ar2
ma1
intercept
2.2645952 12.1798059
3.5359801
0.1674072
MODEL 4
arima(x = y, order = c(1, 0, 1))
Coefficients:
ar1
ma1
intercept
0.9157
-0.3841
0.0703
s.e.
0.0250
0.0468
0.4104
sigma^2 estimated as 1.106:
log likelihood = -488.39,
aic = 984.79
> fit$coef
ar1
ma1
intercept
0.91573812 -0.38408343
0.07028509
> # Significance tests
> fit$coef/sqrt(diag(fit$var.coef))
ar1
ma1
intercept
36.6959008 -8.2008448
0.1712706
5.5
Find an expression for the maximum-likelihood estimator of σ2.
5.6
Implement computationally the convolution algorithm for estimating
the ACF of an ARFIMA process. What numerical diﬃculties display this
approach?
5.7
Using (5.9) show that φtj ∼−πj for large j.

200
ESTIMATION METHODS
(a)
Time
Series
0
50
100
150
200
250
300
-4
0
2
4
0
5
10
15
20
25
-0.2
0.2
0.6
1.0
Lag
ACF
(b)
5
10
15
20
25
-0.2
0.2
0.6
Lag
Partial ACF
(c)
Figure 5.16
Series, sample ACF and sample PACF
Standardized  Residuals
Time
0
50
100
150
200
250
300
-3
-1
1
3
0
2
4
6
8
10
0.0
0.4
0.8
Lag
ACF
ACF of Residuals
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
p values for Ljung-Box statistic
Lag
p-value
Figure 5.17
Diagnostic graphs of Model 1.

PROBLEMS
201
Standardized  Residuals
Time
0
50
100
150
200
250
300
-3
-1
1
3
0
2
4
6
8
10
-0.2
0.2
0.6
1.0
Lag
ACF
ACF of Residuals
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
p values for Ljung-Box statistic
Lag
p-value
Figure 5.18
Diagnostic graphs of Model 2.
Standardized  Residuals
Time
0
50
100
150
200
250
300
-3
-1
1
3
0
2
4
6
8
10
0.0
0.4
0.8
Lag
ACF
ACF of Residuals
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
p values for Ljung-Box statistic
Lag
p-value
Figure 5.19
Diagnostic graphs of Model 3.

202
ESTIMATION METHODS
Standardized  Residuals
Time
0
50
100
150
200
250
300
-3
-1
1
3
0
2
4
6
8
10
0.0
0.4
0.8
Lag
ACF
ACF of Residuals
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
p values for Ljung-Box statistic
Lag
p-value
Figure 5.20
Diagnostic graphs of Model 4.
5.8
Explain why the Haslett-Raftery method yields exact maximum-likelihood
estimates in the fractional noise case when M = n, where n is the sample size.
5.9
Consider the sample y1, ..., y200 from the ARMA(1,1) model
yt −φ yt−1 = ϵt −θ ϵt,
where ϵt is white noise (0, σ2
ϵ ) and assume that the MLE of (φ, θ) is (0.4, 0.7).
(a) Are these coeﬃcients signiﬁcative at the 5% level?
(b) Build conﬁdence intervals for φ and θ at the 95% level.
(c) Find the spectral density of this process.
5.10
Consider the AR(1) process yt −φ yt−1 = ϵt with Var(ϵt) = σ2. Let
l(φ, σ2) = −
n

i=1
log(σ2
i ) −
n

i=1
e2
i /σ2
i
the log-likelihood function where ei = yi −ˆyi is the prediction error and
σ2
i = E(yi −ˆyi)2 its mean square error. Let y1, y2 be two observations such
that |y1| ̸= |y2|. Find the MLE of φ and σ2.
5.11
Let yt be a seasonal process such that yt = (1 + 0.2 B)(1 −0.8 B12)ϵt,
where σϵ = 1.
(a) Find the coeﬃcients πj of the AR(∞) expansion of the process.
(b) Plot the theoretical ACF of yt.

PROBLEMS
203
(c) Plot the theoretical PACF of this process.
(d) Find the spectral density of this process and plotted it.
5.12
Let {xt} be the seasonal process
(1 −0.7 B2)xt = (1 + −.3 B2)zt,
where {zt} is WN (0, 1).
(a) Find the coeﬃcients {ψj} of the representation xt = P∞
j=0 ψjzt−j.
(b) Find and plot the ﬁrst ﬁve components of the ACF of the process
{xt}.
(c) Simulate 400 observations from this model. Plot the series and cal-
culate the ACF and PACF.
(d) Based on the previous question, estimate the parameters of the sim-
ulated series via maximum likelihood estimation.
5.13
Explain how to select an appropriate time series model based on the
following aspects:
(a) Information criteria such as AIC, BIC and others.
(b) Parsimony the model.
(c) Statistical signiﬁcance of the parameters.
(d) Goodness of ﬁt tests.
(e) Residuals whiteness testing procedures.
(f) Veriﬁcation of the model assumptions, e.g. normality.
5.14
Let Y n = (y1, . . . , yn) be a sequence of an ARFIMA(p, d, q) process
with innovations n(0, σ2) and let θ be a vector containing the ARFIMA pa-
rameters.
(a) Show that
f(yn+1, Y n|θ) = f(yn+1|Y n, θ)f(Y n|θ).
(b) Verify that
yn+1|Y n, θ ∼n(µn+1, ν2
n+1σ2),
with
µn+1
=
n
X
j=1
φnjyn+1−j,
ν2
n+1
=
γ(0)
σ2 Qn
j=1(1 −φ2
jj),
where φij are the partial linear regression coeﬃcients.
5.15
Assume that the process yt is Gaussian such that
Y = (y1, . . . , yn)′ ∼N(θ, Σ).

204
ESTIMATION METHODS
(a) Suppose that the prior distributions of θ and Σ are
π(θ) ∝1,
and
π(Σ) ∝|Σ|−(n+1)/2.
Calculate the posterior distribution π(θ, Σ|Y ).
(b) Verify that the likelihood function may be written as
L(θ, Σ|Y ) ∝|Σ|−(n/2)e−(1/2) tr[Σ−1S(Y )],
where S(Y ) = [(yi −θi)(yj −θj)]i,j=1,...,n.
(c) Suppose that Σ−1 ∼Wishartn(B−1, n), that is,
π(Σ−1) ∝|Σ|−(n−2)/2e−(1/2) tr[Σ−1B].
(d) Prove that the posterior distribution of θ and Σ given Y may be
written as
π(θ, Σ|Y ) ∝|Σ|−n+1e−(1/2) tr[B+S(Y )],
and therefore
θ, Σ|Y ∼Wishartn(B + S(Y ), 2n).
(e) Given that
Z
|Z|(1/2)q−1e−(1/2) tr ZCdZ
=
|C|−(1/2)(q+m−1)2(1/2)m(q+m−1)
× Γm
q + m −1
2

,
where Γp(b) is the generalized gamma function
Γp(b) =

Γ
  1
2
(1/2)p(p−1)
p
Y
α=1
Γ

b + α −p
2

,
with b > (p −1)/2, show that π(θ|Y ) ∝|S(Y )|−n/2.
5.16
Let Q = (qij) be a transition matrix of an arbitrary Markov chain on
the states 0, 1, . . . , S and let αij be given by
αij =
sij
1 + πiqij
πjqji
,

PROBLEMS
205
where πi > 0, i = 0, . . . , S, P πi = 1 and sij is a symmetric function of i and
j chosen such that 0 ≤αij ≤1 for all i, j.
Consider the Markov chain on the states 0, 1, . . . , S with transition matrix
P given by
pij = qijαij,
for i ̸= j and
pii = 1 −
X
i̸=j
pij.
(a) Show that the matrix P is in fact a transition matrix.
(b) Prove that P satisﬁes the reversibility condition that
πipij = πjpji,
for all i and j.
(c) Verify that π = (π0, . . . , πS) is the unique stationary distribution of
P, that is,
π = πP.
5.17
Consider the following two choices of the function sij on the method
discussed in Problem 5.16:
sM
ij =









1 + πiqij
πjqji
if πiqij
πjqji
≤1,
1 + πjqji
πiqij
if πiqij
πjqji
> 1.
and the alternative choice
sB
ij = 1.
(a) Show that both choices satisfy the condition 1 ≤αij ≤1.
(b) Suppose that the matrix Q is symmetric and consider the sampling
scheme
xt+1 =
 j
with probability αij,
i
with probability 1 −αij.
Verify that if πi = πj, then P(xt+1 = j) = 1 for the Metropolis
algorithm and P(xt+1 = j) = 1 for Barker’s method.
(c) According to part (b), which method is preferable?
(d) Consider the choice
sij = g

min
 πiqij
πjqji
, πjqji
πiqij

,

206
ESTIMATION METHODS
where the function g(x) is symmetric and satisﬁes 0 ≤g(x) ≤1 + x.
Verify that for this choice, the condition 0 ≤αij ≤1 holds.
(e) Consider the particular choice g(x) = 1 + 2(x/2)γ for a constant
γ ≥1. Prove that sM
ij is obtained with γ = 1 and sB
ij is obtained
with γ = ∞.
5.18
Consider the following Poisson sampling with
πi = e−λ λi
i! ,
for i = 0, 1, . . . , and
q00 = q01 = 1
2,
and
qij = 1
2, for j = i −1, i + 1, i ̸= 0.
(a) Show that in this case
xt+1 =

















i + 1
with probability



λ
i + 1
if λ ≤i + 1,
1
if λ > i + 1,
i −1
with probability
( i
λ
if λ ≤i,
1
if λ > i.
(b) What is the disadvantage of this method when λ is large?
5.19
Show that for an ARFIMA(1, d, 0) model,
corr(bd, bφ) →
√
6
π ,
as φ →0.
5.20
Show that for an ARFIMA(0, d, 1) model,
corr(bd, bθ) →−
√
6
π ,
as θ →0.
5.21
Let {yt : t ∈Z} be a stationary process with spectral density f(λ) and
let Y = (y1, y2, . . . , yn)′ ∼N(0, Tθ0) where the elements of the n×n variance-
covariance matrix Tθ0 = (Tij) are given by
Tij =
Z π
−π
f(λ)e˙ıλ(i−j)dλ.

PROBLEMS
207
Consider the function
Ln(θ) = 1
n log det Tθ + 1
nY ′T −1
θ
Y .
(a) Prove that
E[Ln(θ0)] = 1 + log det Tθ0.
(b) Show that
lim
n→∞E[Ln(θ0)] = log(2π) + 1
2π
Z π
−π
log f(λ) dλ.
(c) Verify that
lim
n→∞E[Ln(θ0)] = 1 + log σ2.
(d) Verify that
Var[Ln(θ0)] = 2
n,
and prove that Ln(θ0) converges to 1 + log σ2 in probability as n
tends to inﬁnity.


CHAPTER 6
NONLINEAR TIME SERIES
As discussed in previous chapters, linear processes are excellent tools for ana-
lyzing a great number of time series. However, they usually fail to adequately
model more complicated dependence structures. For example, many real-life
time series display almost no autocorrelations but exhibit strong dependence
in their squares. To deal with these situations, several classes of nonlinear pro-
cesses are available. Additionally, a number of testing procedures for linearity
have been developed to help making a decision whether to employ a linear or
a nonlinear approach. If linearity is not rejected through these procedures,
then we could use some of the linear processes discussed in the previous chap-
ters to ﬁt the data. On the contrary, if linearity is rejected, then we could
try out some of the nonlinear models discussed in this chapter. Even though
nonlinear time series appear in many ﬁelds, they are commonly found in the
analysis of ﬁnancial instruments. Financial time series such as returns from
stocks indexes exhibit almost null autocorrelation but they display an impor-
tant level of dependence in their squared returns. This chapter begins deﬁning
a large class of nonlinear processes and the proceed to review some linearity
testing procedures. It also discusses ﬁnancial time series, an important area
of applications for these models.
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
209

210
NONLINEAR TIME SERIES
6.1
INTRODUCTION
A stationary process with mean µ can be written in terms of a Volterra ex-
pansion
yt = µ +
∞

j=−∞
ψi εt−i +
∞

i,j=−∞
ψij εt−iεt−j +
∞

i,j,k=−∞
ψijk εt−iεt−jεt−k + · · · ,
where ψi, ψij, ψijk, . . . are unknown coeﬃcients and {εt} is a sequence of i.i.d.
random variables with zero-mean and variance σ2. The values ψi, ψij, ψijk
are commonly referred to as linear, quadratic, cubic coeﬃcients, respectively.
Note that when the terms ψij, ψijk and so on are all equal to zero, yt
reduces to a linear process. On the other hand, if the coeﬃcients ψijk and
higher are all zero, but ψij are non zero, the resulting process can be written
as,
yt = µ +
∞

j=−∞
ψi εt−i +
∞

i,j=−∞
ψij εt−iεt−j.
A simple example of a quadratic process is yt = εt + θεtεt−1 where εt is
a sequence of i.i.d. random variables with zero-mean and variance σ2. Note
(a)
Time
Series
0
50
100
150
200
250
300
-4
-2
0
2
0
5
10
15
20
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 6.1
Simulated quadratic process with θ = 0.7.
(a) Series of 300
observations. (b) Sample ACF .

TESTING FOR LINEARITY
211
that in this case, E yt = 0 and the autocovariance function is
γ(0) =
 σ2(1 + σ2θ2)
if
h = 0
0
if
h ̸= 0.
Thus, this quadratic process actually corresponds to a white noise sequence.
A simulated sample of 300 observations from this process with θ = 0.7 and
σ2 = 1 is exhibited in Figure 6.1 along with its empirical ACF.
6.2
TESTING FOR LINEARITY
There are several procedures for testing linearity of a stationary time series.
For simplicity, in this section we consider a methodology based on regressing
the observations on their previous values and then computing the resulting
residuals.
Consider regressing the observation yt on 1, yt−1, yt−2, . . . , yt−p for t = p +
1, . . . , n where p is a previously speciﬁed autoregression order and obtain the
residuals ep+1, . . . , en. Let Se = Pn
j=p+1 e2
t be the sum of squared residuals.
Next, regress the squared process y2
t on 1, yt−1, yt−2, . . . , yt−p for t = p +
1, . . . , n obtaining the residuals ξp+1, . . . , ξn.
Finally, regress ep+1, . . . , en on ξp+1, . . . , ξn
et = βξt + ηt,
obtaining the regression coeﬃcient bβ and the residual sum of squares Sξ =
Pn
j=p+1 ξ2
t . Thus, one can test for linearity by means of the statistic
F =
bβ2Sξ(n −2p −2)
Se −bβ2Sξ
.
(6.1)
The distribution of this statistic is approximately Fisher with 1 and n−2p−2
degrees of freedom. As an illustration of the application of the this nonlinear-
ity test consider the quadratic time series presented in the previous section.
The R library nlts provides an implementation of this testing procedure gen-
erating the following output:
order
F
df1
df2
p
1.0000
4.9697
1.0000 297.0000
0.0265
order
F
df1
df2
p
2.000
20.333
3.0000 294.0000
0.0000
order
F
df1
df2
p
3.0000
12.1295
6.0000 290.0000
0.0000

212
NONLINEAR TIME SERIES
order
F
df1
df2
p
4.0000
8.1241
10.0000 285.0000
0.0000
order
F
df1
df2
p
5.0000
5.9962
15.0000 279.0000
0.0000
Based on these results, the linearity hypothesis is rejected at the 5% sig-
niﬁcance level for all the orders considered. Furthermore, this R library also
oﬀers a procedure for estimating the appropriate order. As indicated in the
following output, the program suggests that p = 3 is an adequate order for
the testing procedure.
The estimated order is 3 with a cross-validation error of 0.81
and Gaussian bandwidth 3 (using local polynomial with 2 degrees).
order
cv.min bw.opt
df
GCV.min GCV.bw.opt
GCV.df
1
1 1.0395
10
3.6213 0.96016
1.2
8.9953
2
2 0.8823
5 11.3163 0.79636
2.0
20.8732
3
3 0.8101
3 37.8311 0.75289
3.0
30.7917
4
4 0.8721
4 49.6511 0.77349
4.0
38.8586
5
5 0.9175
10 35.2700 0.81368
5.0
46.5358
6.3
HETEROSKEDASTIC DATA
Time series of returns from ﬁnancial instruments usually exhibit nonlinear-
ities. There is strong empirical evidence that a large number of time series
from ﬁnance and economics show some stylized facts such as clusters of highly
variable observations followed by clusters of observations with low variability
and strong autocorrelations either in the series or its squares. In this chapter
we examine some of the models proposed to account for these features. In par-
ticular, we consider heteroskedastic time series models where the conditional
variance given the past is no longer constant.
As an illustration of the stylized facts frequently found in economic time
series, consider the daily log-returns of the SP500 stock index discussed in
Chapter 1. This dataset spans from January 1, 1950 to May 1, 2014. As
shown in Figure 6.2 displays periods of low levels of volatility followed by
periods of higher volatility. Additionally, it can be observed the presence of
speciﬁc dates with exceptionally high variability. On the other hand, Figure
6.3 shows that the returns exhibit a lower level of autocorrelation as compared
to the squared returns.
Another illustration of the stylized facts is provided by the daily log-returns
of the copper prices from January 4, 2005 to December 31, 2014. These prices
are expressed in terms of USD cents per pound at the London Metal Exchange.
The series {rt} is displayed in panel (a) of Figure 6.4 while its squares are
shown in panel (b).

ARCH MODELS
213
From Figure 6.4(a) we note a period of high volatility during 2008–2009.
Besides, Figure 6.4(b) suggests that the squared series suﬀers from bursts
of high volatility followed by periods of low volatility. On the other hand,
the sample autocorrelation function, Figure 6.5(a), shows some signiﬁcant
autocorrelations in the returns while the sample autocorrelation of the squares
exhibits a strong level of dependence; see Figure 6.5(b).
Several models have been proposed to account for these features. Most of
these models specify an ARMA or an ARFIMA process for the returns and
specify some parametric model for the conditional variance of the series given
its inﬁnite past. In some cases this model resembles an ARMA in the form of
a generalized autoregressive conditionally heteroskedastic (GARCH) process
or it resembles an AR(∞) process in the form of an ARCH(∞) model.
6.4
ARCH MODELS
An ARCH(1) process is deﬁned by the discrete-time equation
yt
=
σtεt,
(6.2)
σ2
t
=
α0 + α1y2
t−1,
(6.3)
where σ2
t = E[y2
t |yt−1] is the conditional variance of the process {yt}, the
ARCH coeﬃcients α0, α1 are positive, α1 < 1 and {ϵt} is sequence of in-
(a)
Time
Log Returns
1950
1960
1970
1980
1990
2000
2010
-0.20
-0.15
-0.10
-0.05
0.00
0.05
0.10
(b)
Time
Squared Log Returns
1950
1960
1970
1980
1990
2000
2010
0.00
0.01
0.02
0.03
0.04
0.05
Figure 6.2
SP500 Index data (1950–2014). (a) Daily log-returns. (b) Squared
daily log-returns.

214
NONLINEAR TIME SERIES
0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 6.3
SP500 Index data (1950–2014). (a) Sample ACF of daily log-returns.
(b) Sample ACF of squared daily log-returns.
dependent and identically distributed zero-mean and unit variance random
variables. Although εt is often assumed to be Gaussian, in some cases it may
be speciﬁed by a t-distribution or a double exponential distribution, among
others. These distributions have a greater ﬂexibility to accommodate a pos-
sible heavy tail behavior of some ﬁnancial time series.
Observe that E y2
t = E σ2
t ε2
t. Given that σt only depend on past values of
the sequence {εt}, σt and εt are independent. Consequently,
E y2
t = E σ2
t E ε2
t = E σ2
t .
Thus,
E y2
t = α0 + α1 E y2
t−1.
In order to be stationary, E y2
t = E y2
t−1, so that by replacing this condition
in the equation above and considering that α1 < 1 we get
E y2
t =
α0
1 −α2
1
.
As an illustration, Figure 6.6 shows 400 observations of an ARCH(1) model
with parameter α1 = 0.9 Additionally, Figure 6.7 exhibits the sample ACF of
this series along with the sample ACF of its squares. Note that the series shows
almost no autocorrelation but its squares display a high level of dependence.

ARCH MODELS
215
This situation is due to the dependence structure of the squares of an ARCH
process. In what follows we show that y2
t corresponds to an AR(1) process.
Let us deﬁne the sequence νt = y2
t −σ2
t . Thus, we can write y2
t = α0 +
α1y2
t−1 + νt. If µ denotes the mean of y2
t , then y2
t −µ = α1(y2
t−1 −µ) + νt.
But νt is a white noise sequence. To see this, E νt = E y2
t −E σ2
t = 0.
Besides, this process is uncorrelated since for h > 0 we have
E νtνt+h = E σ2
t (1 −ε2
t)σ2
t+h(1 −ε2
t+h).
Given that both σ2
t and σt+h depend only values εt+h−1, εt+h−2, . . . , we con-
clude that
E νtνt+h = E σ2
t (1 −ε2
t)σ2
t+h E(1 −ε2
t+h) = 0.
Based on the previous results, the process y2
t satisﬁes an AR(1) model with
autoregressive parameter α1.
The ARCH(1) model can be readily extended to encompass a dependence
of the conditional variance σ2
t on higher lags. The ARCH(r) process is deﬁned
by the discrete-time equation
yt
=
σtεt,
(6.4)
σ2
t
=
α0 + α1y2
t−1 + · · · + αry2
t−r.
(6.5)
In this case, the stationarity condition becomes
α1 + · · · + αr < 1,
(a)
Year
Daily Copper Log Returns
2006
2008
2010
2012
2014
-0.10
0.00
0.05
0.10
(b)
Year
Squared Daily Copper Log Returns
2006
2008
2010
2012
2014
0.000
0.004
0.008
0.012
Figure 6.4
Copper price data (2005–2014): (a) Daily log-returns and (b) squared
log-returns.

216
NONLINEAR TIME SERIES
0
5
10
15
20
25
30
35
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
30
35
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 6.5
Sample autocorrelation function of the copper price data: (a) ACF of
daily log-returns and (b) ACF of squared log-returns.
so that the variance of yt is now given by
E y2
t =
α0
1 −α2
1 −· · · −α2r
.
Analogously to the ARCH(1) model, it can be readily shown that the squares
of an ARCH(r) process y2
t satisﬁes the AR(r) model
y2
t −µ = α1(y2
t−1 −µ) + · · · + αr(y2
t−r −µ) + νt.
6.5
GARCH MODELS
A further extension of the ARCH models is the generalized autoregressive
conditionally heteroskedastic GARCH(r, s) process is given by
yt
=
σtεt,
σ2
t
=
α0 + α1y2
t−1 + · · · + αry2
t−r + β1σ2
t−1 + · · · + βsσ2
t−s,
where σ2
t = E[y2
t |yt−1, yt−2, . . . ] is the conditional variance of the process
{yt}, the GARCH coeﬃcients α0, α1, . . . , αr and β1, . . . , βs are positive and
{ϵt} is sequence of independent and identically distributed zero-mean and unit
variance random variables. In order to be stationary, the coeﬃcients of the

GARCH MODELS
217
(a)
Time
Series
0
100
200
300
400
-0.005
0.000
0.005
(b)
Time
Squared series
0
100
200
300
400
0e+00
4e-05
6e-05
8e-05
Figure 6.6
Simulated ARCH(1) process with α1 = 0.9.
(a) Series of 400
observations. (b) Squared series.
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 6.7
Simulated ARCH(1) process with α1 = 0.9. (a) Sample ACF. (b)
Sample ACF of the squared series.

218
NONLINEAR TIME SERIES
GARCH process must satisfy α1 + · · · + αr + β1 + · · · + βs < 1. Under this
condition, the variance of the process can be written as
Var y2
t =
α0
1 −α2
1 −· · · −α2r −β2
1 −· · · −β2s
.
It has been observed in practice that the returns of a ﬁnancial instrument
may display a low level of autocorrelation and that the squared returns exhibit
strong dependence. This empirical ﬁnding is usually reﬂected in the fact that
the estimated parameters satisfy
bα2
1 + · · · + bα2
r + bβ2
1 + · · · + bβ2
s ∼1,
indicating a near nonstationary behavior of the process due to a high level of
autocorrelation of the squared returns.
In order to account for these phenomena, the GARCH models can be ex-
tended in two directions: introducing an ARMA or ARFIMA structure to
model the autocorrelation of the returns and allowing a strong level of de-
pendence in the squared returns by incorporating, for example, integrated
GARCH processes. We begin the revision of these extension by deﬁning the
ARFIMA-GARCH models in the next section.
6.6
ARFIMA-GARCH MODELS
An ARFIMA(p, d, q)-GARCH(r, s) process is deﬁned by the discrete-time
equation
φ(B)yt
=
θ(B)(1 −B)−dεt,
(6.6)
εt
=
ϵtσt,
(6.7)
σ2
t
=
α0 +
r
X
j=1
αjε2
t−j +
s
X
j=1
βjσ2
t−j,
(6.8)
where σ2
t = E[y2
t |yt−1, yt−2, . . . ] is the conditional variance of the process {yt},
the GARCH coeﬃcients α1, . . . , αr and β1, . . . , βs are positive, Pr
j=1 αj +
Ps
j=1 βj < 1, and {ϵt} is sequence of independent and identically distributed
zero-mean and unit variance random variables. Note that ϵt is assumed to
be Gaussian, but in some cases it may be speciﬁed by a t-distribution or
a double exponential distribution, among others. These distributions have
a greater ﬂexibility to accommodate a possible heavy tail behavior of some
ﬁnancial time series.

ARFIMA-GARCH MODELS
219
EXAMPLE 6.1
In order to explore the structure of the model described by (6.6)–(6.8),
consider the ARFIMA(p, d, q)-GARCH(1, 1) process:
yt
=
∞
X
j=0
ψjεt−j,
εt
=
ϵtσt,
σ2
t
=
α0 + α1ε2
t−1 + β1σ2
t−1,
where ψ(B) = φ(B)−1θ(B)(1 −B)−d and ϵt follows a standard normal
distribution. Thus, we may write
σ2
t
=
α0 + (α1ϵ2
t−1 + β1)σ2
t−1,
=
" n
Y
k=1
(α1ϵ2
t−k + β1)
#
σ2
t−1−n + α0

1 +
n−1
X
k=0
k+1
Y
j=1
(α1ϵ2
t−j + β1)

.
Deﬁne the random variable zn = Pn
k=1 log(α1ϵ2
t−k + β1) and let γn =
zn/n. By the strong law of the large numbers, γn →E[log(α1ϵ2
1 + β1]
almost surely as n →∞. This limit is called the top Lyapunov exponent
of the process, γ.
If γ < 0, then we may write
σ2
t = α0

1 +
∞
X
k=0
k+1
Y
j=1
(α1ϵ2
t−j + β1)

.
(6.9)
Consequently, the process yt may be expressed as
yt = √α0
∞
X
j=0


ψjϵt−j
"
1 +
∞
X
k=0
k+1
Y
i=1
(α1ϵ2
t−j−i + β1)
#1/2

.
Thus, since {yt} corresponds to a transformation of the independent and
identically distributed sequence {ϵt} the process {yt} is stationary. This
result can be readily extended to the general model ARFIMA(p, d, q)-
GARCH(r, s).
Observe that the conditional variance σ2
t speciﬁed by a GARCH(r, s) pro-
cess may be expressed as an ARMA(p, r) model with p = max{r, s} as fol-
lows. Let ut = σ2
t (ϵ2
t −1). This sequence is white noise, since E[ut] = 0,
E[u2
t] = E[σ4
t (ϵ2
t −1)2] = E[σ4
t ] E[(ϵ2
t −1)2], and for k > 0 we have
E[utut+k] = E[E(utut+k|Ft+k−1)] = E[utσ2
t+k E(ϵ2
t+k −1)] = 0.

220
NONLINEAR TIME SERIES
Thus, σ2
t may be written as
(1 −λ1B −· · · −λpBp)σ2
t = α0 +
r
X
j=1
αjut−j,
where λj = αj1{1,...,r}(j) + βj1{1,...,s}(j).
An approximate MLE bθ for the ARFIMA-GARCH model is obtained by
maximizing the conditional log-likelihood
L(θ) = −1
2n
n
X
t=1

log σ2
t + ε2
t
σ2
t

.
(6.10)
Let θ = (θ1, θ2)′, where θ1 = (φ1, . . . , φp, θ1, . . . , θq, d)′ is the parameter vector
involving the ARFIMA components and θ2 = (α0, . . . , αr, β1, . . . , βs)′ is the
parameter vector containing the GARCH component. The following result
establishes some asymptotic properties of this estimate: Let bθn be the value
that maximizes the conditional log-likelihood function (6.10). Then, under
some regularity conditions, bθn is a consistent estimate and √n(bθn −θ0) →
N(0, Ω−1), as n →∞, where Ω= diag(Ω1, Ω2) with
Ω1 = E
 1
σ2
t
∂εt
∂θ1
∂εt
∂θ′
1
+
1
2σ4
t
∂σ2
t
∂θ1
∂σ2
t
∂θ′
1

,
and
Ω2 = E
 1
2σ4
t
∂σ2
t
∂θ2
∂σ2
t
∂θ′
2

.
At this point, it is necessary to introduce the concept of intermediate mem-
ory which will be used in the next section. We say that a second-order sta-
tionary process has intermediate memory if for a large lag h its ACF behaves
like γ(h) ∼ℓ(h)|h|2d−1 with d < 0, where ℓ(·) is a slowly varying function.
Thus, the ACF decays to zero at an hyperbolic rate but it is summable, that
is,
∞
X
h=0
|γ(h)| < ∞.
6.7
ARCH(∞) MODELS
Given that the squares of many ﬁnancial series have similar or greater level
of autocorrelation than their returns, the memory reduction that aﬀects the
squares of an ARFIMA-GARCH process may not be adequate in practice.
This circumstance leads us to explore other classes of processes to model the

ARCH(∞) MODELS
221
strong dependence of the squared returns directly. As an important example
of this approach, consider the following ARCH(∞) model:
yt
=
σtϵt,
(6.11)
σ2
t
=
α0 +
∞
X
j=1
αjy2
t−j,
(6.12)
where {ϵt} is a sequence of independent and identically distributed random
variables with zero-mean and unit variance, α0 is a positive constant, and
αj ≥0 for j ≥1. This model can be formally written as
y2
t = α0 + νt +
∞
X
j=1
αjy2
t−j,
(6.13)
where σ2
n = E[y2
t |yt−1, yt−2, . . . ], νt = y2
t −σ2
t is a white noise sequence.
If E[ϵ2
0
P∞
j=0 αj] < 1, then the conditional variance may be written in terms
of a Volterra expansion
σ2
t = α0
∞
X
k=0
∞
X
j1,...jk=1
αj1αj2 · · · αjkϵ2
t−j1ϵ2
t−j2 · · · ϵ2
t−jk,
(6.14)
and the process {yt} may be expressed as
yt = ϵt

α0
∞
X
k=0
∞
X
j1,...jk=1
αj1αj2 · · · αjkϵ2
t−j1ϵ2
t−j2 · · · ϵ2
t−jk,


1/2
.
In particular, when the coeﬃcients {αj} in (6.13) are speciﬁed by an
ARFIMA(p, d, q) model, the resulting expression deﬁnes the FIGARCH(p, d, q)
model. If π(B) = φ(B)(1 −B)dθ(B)−1, then from (6.13) we get
π(B)y2
t = α0 + νt.
Therefore, by multiplying both sides by θ(B) we conclude that
φ(B)(1 −B)dy2
t = ω + θ(B)νt,
where ω = θ(B)α0. This process is strictly stationary and ergodic but not
second-order stationary. On the other hand, writing this model in terms of
the conditional variance as in (6.12) we have
σ2
t = α0 + [1 −π(B)]y2
t .
(6.15)
From (6.11) we may write log(y2
t ) = log(σ2
t ) + 2 log(|ϵt|). Thus, by con-
sidering log(y2
t ) as the observed returns, it may seem natural to attempt to
specify a long-memory model directly to the term log(σ2
t ) instead of σ2
t . An

222
NONLINEAR TIME SERIES
advantage of this formulation is that log(σ2
t ) is allowed to be negative. There-
fore, unlike the FIGARCH model, no additional conditions on the parameters
are needed to ensure the positivity of σ2
t .
An example of this type of processes is the fractionally integrated exponen-
tial GARCH (FIEGARCH) model speciﬁed by
φ(B)(1 −B)d log(σ2
t ) = α + θ(B)|ϵt−1| + λ(B)ϵt−1,
(6.16)
where φ(B) = 1 + φ1B + · · · + φpBp, α ∈IR, θ(B) = θ1 + · · · + θqBq−1, and
the polynomial λ(B) = λ1 +· · ·+λqBq−1 accounts for the leverage eﬀect, that
is, conditional variances may react distinctly to negative or positive shocks.
Consider the quasi-log-likelihood function
L(θ) = −1
2 log(2π) −1
2
n
X
t=1

log σ2
t + ε2
t
σ2
t

,
(6.17)
where θ = (ω, d, φ1, . . . , φp, , θ1, . . . , θq).
A QMLE bθn can be obtained by
maximizing (6.17).
6.8
APARCH MODELS
Another model that incorporates asymmetry in the conditional variance is
the so-called asymmetric power autoregressive conditionally heteroskedastic
APARCH(r, s) process deﬁned by
yt
=
σtεt,
σδ
t
=
α0 +
r
X
i=1
αi(|yt−i| −γiyt−i)δ +
s
X
j=1
βjσδ
t−j.
Notice that this model corresponds to an ARCH(r) process if δ = 2, γi for
i = 1, . . . , r and βj = 0 for j = 1, . . . , s. On the other hand, it is a GARCH(r, s)
process if δ = 2 and γi for i = 1, . . . , r.
6.9
STOCHASTIC VOLATILITY
A stochastic volatility (SV) process is deﬁned by the equations
rt
=
σtϵt,
(6.18)
σt
=
σ exp(vt/2),
(6.19)
where {ϵt} is an independent and identically distributed sequence with zero-
mean and unit variance, and {vt} is a stationary process independent of {ϵt}.
In particular, {vt} may be speciﬁed as a long-memory ARFIMA(p, d, q) pro-
cess. The resulting process is called long-memory stochastic volatility (LMSV)
model.

NUMERICAL EXPERIMENTS
223
From (6.18), we may write
log(r2
t )
=
log(σ2
t ) + log(ϵ2
t),
log(σ2
t )
=
log(σ2) + vt.
Let yt = log(r2
t ), µ = log(σ2) + E[log(ϵ2
t)] and εt = log(ϵ2
t) −E[log(ϵ2
t)].
Then,
yt = µ + vt + εt.
(6.20)
Consequently, the transformed process {yt} corresponds to a stationary long-
memory process plus an additive noise. The ACF of (6.20) is given by
γy(h) = γv(k) + σ2
εδ0(h),
where δ0(h) = 1 for h = 0 and δ0(h) = 0 otherwise. Furthermore, the spectral
density of {yt}, fy, is given by
fy(λ) = fv(λ) + σ2
ε
2π ,
where fv is the spectral density of the long-memory process {vt}.
In particular, if the process {vt} is an ARFIMA(p, d, q) model
φ(B)vt = θ(B)(1 −B)−dηt,
(6.21)
and θ = (d, σ2
η, σ2
ε, φ1, . . . , φp, θ1, . . . , θq)′ is the parameter vector that speciﬁes
model (6.21), then the spectral density is given by
fθ(λ) = σ2
η
2π
|θ(e˙ıλ)|2
|1 −e˙ıλ|2d|φ(e˙ıλ)|2 + σ2
ε
2π .
The parameter θ can be estimated by minimizing the spectral likelihood
L(θ) = 2π
n
n/2
X
j=1

log fθ(λj) + I(λj)
fθ(λj)

.
(6.22)
Let bθ be the value that minimizes L(θ) over the parameter space Θ. This
estimator satisﬁes the following result: Assume that the parameter vector θ
is an element of the compact parameter space Θ and assume that fθ1 = fθ2
implies that θ1 = θ2. Let θ0 be the true parameter value. Then, bθn →θ0 in
probability as n →∞.
6.10
NUMERICAL EXPERIMENTS
The ﬁnite sample performance of the quasi maximum likelihood estimates
based on the spectral-likelihood (6.22) is studied in this section by means of
several Monte Carlo simulations.

224
NONLINEAR TIME SERIES
Table 6.1
Estimation of Long-Memory Stochastic Volatility Models
d
bd
bση
SD(bd)
SD(bση)
0.15
0.1453300
4.9880340
0.03209314
0.14602178
0.30
0.3058552
5.0067196
0.02877496
0.13827396
0.45
0.4606728
5.0364215
0.02695866
0.12792891
Table 6.2
Estimation of AR Stochastic Volatility Models
φ
bφ
bση
SD(bφ)
SD(bση)
0.30
0.2965264
4.8788172
0.03762665
0.13678478
0.50
0.4892303
4.6579548
0.03076769
0.14218190
0.70
0.6808834
4.2779151
0.02269855
0.17127041
Table 6.3
Estimation of MA Stochastic Volatility Models
θ
bθ
bση
SD(bθ)
SD(bση)
-0.30
-0.2966484
5.0985024
0.03354726
0.14166745
0.30
0.2883408
5.0785268
0.03571493
0.14252039
0.60
0.5396505
5.4969348
0.03396586
0.16060363
The models investigated are the LMSV with an ARFIMA(0, d, 0) serial
dependence structure, an AR(1) process and a MA(1) model. These processes
have noise standard deviation σε = π/
√
2 where ϵt follows a standard normal
distribution, ση = 5, and the sample size is n = 400. Observe that εt =
log ϵ2
t −E[log ϵ2
t]. Thus, given that ϵt ∼N(0, 1) we have that Var(εt) = π2/2.
The results displayed in Table 6.1 for the LMSV model, Table 6.2 for the
AR(1) process and Table 6.3 for the MA(1) model correspond to the quasi
maximum likelihood estimates for diﬀerent values of the parameters. All the
reported results are based on 1000 replications.
From these three tables, observe that the estimates of the long-memory
parameter d, φ, θ and the scale parameter ση are close to their true values.

DATA APPLICATIONS
225
6.11
DATA APPLICATIONS
6.11.1
SP500 Data
The R library fGarch allows the ﬁtting of GARCH models. As an illustration,
the following output corresponds to the modeling of the SP500 data. The
model was selected by taking into account the AIC, the signiﬁcance of the
parameters and the residuals diagnostics. The selected model corresponds to
an AR(2) + GARCH(1, 1) process. From the output, note that the sum of the
estimates α1 and β1 is 0.9937, that is, very close to the stationarity boundary,
anticipated in Section 3 about the stylized facts in ﬁnancial time series.
Title:
GARCH Modeling
Call:
garchFit(formula = ~arma(2, 0) + garch(1, 1), data = z,
include.mean = FALSE,
trace = FALSE)
Mean and Variance Equation:
data ~ arma(2, 0) + garch(1, 1)
Conditional Distribution:
norm
Coefficient(s):
ar1
ar2
omega
alpha1
beta1
1.0405e-01
-2.3687e-02
7.7258e-07
8.1200e-02
9.1282e-01
Std. Errors:
based on Hessian
Error Analysis:
Estimate
Std. Error
t value Pr(>|t|)
ar1
1.040e-01
8.510e-03
12.226
< 2e-16 ***
ar2
-2.369e-02
8.371e-03
-2.829
0.00466 **
omega
7.726e-07
9.577e-08
8.067 6.66e-16 ***
alpha1
8.120e-02
4.225e-03
19.219
< 2e-16 ***
beta1
9.128e-01
4.448e-03
205.207
< 2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Log Likelihood:
54827.28
normalized:
3.404787

226
NONLINEAR TIME SERIES
Standardised Residuals Tests:
Statistic p-Value
Jarque-Bera Test
R
Chi^2
13289.56
0
Shapiro-Wilk Test
R
W
NA
NA
Ljung-Box Test
R
Q(10)
13.16941
0.2143535
Ljung-Box Test
R
Q(15)
17.61137
0.2836476
Ljung-Box Test
R
Q(20)
22.62202
0.3077331
Ljung-Box Test
R^2
Q(10)
16.54757
0.08499406
Ljung-Box Test
R^2
Q(15)
19.55958
0.1894863
Ljung-Box Test
R^2
Q(20)
23.53524
0.2632833
LM Arch Test
R
TR^2
17.63158
0.1273434
Information Criterion Statistics:
AIC
BIC
SIC
HQIC
-6.808952 -6.806566 -6.808952 -6.808163
6.11.2
Gold Data
This section study the monthly gold prices for the period starting on January
1978 through September 2014. These prices correspond to US dollars per troy
Oz. are exhibited in Figure 6.8. From this plot, notice the sharp rise of the
prices around 2000. The log returns are displayed in Figure 6.9 while the
squared returns are exhibited in Figure 6.10. In order to account for serial
dependence of returns and squared returns, a class of ARMA-GARCH model
is proposed for these monthly data.
The R package fGarch allows to ﬁt these models. The selected model has
ARMA(1, 1) dependence structure for the returns and a GARCH(1, 1) depen-
dence structure for the conditional variances. Notice that all the parameters
are statistically signiﬁcant at the 5% level.
Title:
GARCH Modeling
Call:
garchFit(formula = ~arma(1, 1) + garch(1, 1), data = z, trace = FALSE)
Mean and Variance Equation:
data ~ arma(1, 1) + garch(1, 1)
[data = z]
Conditional Distribution:
norm
Coefficient(s):
mu
ar1
ma1
omega
alpha1
beta1
2.1486e-03
-4.8625e-01
7.0064e-01
7.6059e-05
1.8437e-01 7.8914e-01

DATA APPLICATIONS
227
Year
Gold  Price
1980
1990
2000
2010
500
1000
1500
Figure 6.8
Gold monthly prices, January 1978 to September 2014, US Dollars
per Troy Oz.
Year
Gold Log Return
1980
1990
2000
2010
-0.2
-0.1
0.0
0.1
0.2
0.3
0.4
Figure 6.9
Gold monthly log returns, January 1978 to September 2014.

228
NONLINEAR TIME SERIES
Year
Gold Squared Log Returns
1980
1990
2000
2010
0.00
0.05
0.10
0.15
Figure 6.10
Gold squared log returns, January 1978 to September 2014.
0
5
10
15
20
25
0.0
0.4
0.8
Lag
ACF
(a)
0
5
10
15
20
25
0.0
0.4
0.8
Lag
ACF
(b)
Figure 6.11
Gold monthly log returns. (a) Sample ACF of yt. (b) Sample ACF
of y2
t

DATA APPLICATIONS
229
Std. Errors:
based on Hessian
Error Analysis:
Estimate
Std. Error
t value Pr(>|t|)
mu
2.149e-03
2.849e-03
0.754
0.45081
ar1
-4.863e-01
1.718e-01
-2.830
0.00466 **
ma1
7.006e-01
1.399e-01
5.008
5.5e-07 ***
omega
7.606e-05
3.556e-05
2.139
0.03245 *
alpha1
1.844e-01
4.817e-02
3.827
0.00013 ***
beta1
7.891e-01
4.787e-02
16.486
< 2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Log Likelihood:
797.8381
normalized:
1.813269
Description:
Mon Feb
9 00:17:39 2015 by user:
Standardised Residuals Tests:
Statistic p-Value
Jarque-Bera Test
R
Chi^2
104.7054
0
Shapiro-Wilk Test
R
W
0.9744005 5.552453e-07
Ljung-Box Test
R
Q(10)
13.33065
0.2057678
Ljung-Box Test
R
Q(15)
23.08554
0.08234047
Ljung-Box Test
R
Q(20)
25.57881
0.1801772
Ljung-Box Test
R^2
Q(10)
15.77544
0.1062436
Ljung-Box Test
R^2
Q(15)
18.89253
0.2186455
Ljung-Box Test
R^2
Q(20)
20.06762
0.4537067
LM Arch Test
R
TR^2
18.7434
0.09491198
Information Criterion Statistics:
AIC
BIC
SIC
HQIC
-3.599264 -3.543536 -3.599630 -3.577279
On the other hand, Figure 6.12 displays the sample ACF of the stan-
dardized residuals as well as the squared standardized residuals. Notice that
the serial dependence is not statistical signiﬁcant at the 5% level, cf. with
the Ljung -Box test for the standardized residual with p-value of 0.082 and
the corresponding test for the squared standardized residuals with p-value of
0.1062 both considering 10 lags. Similar results are found when considering
20 lags.
Figure 6.13 reports two year ahead monthly forecasts and 95% predictions
bands while a zoom to the last 40 predictions and prediction bands are dis-
played in Figure 6.14.

230
NONLINEAR TIME SERIES
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 6.12
Gold monthly log returns ﬁtted ARMA-GARCH model. (a) Sample
ACF of standardized residuals. (b) Sample ACF of squared standardized residuals.
Time
Prediction Bands
0
100
200
300
400
-0.2
-0.1
0.0
0.1
0.2
0.3
0.4
Figure 6.13
Gold monthly log returns ARMA-GARCH model: Data and two year
ahead monthly forecasts and 95% prediction bands.

DATA APPLICATIONS
231
Time
Prediction Bands
5
10
15
20
-0.10
-0.05
0.00
0.05
0.10
Figure 6.14
Gold monthly log returns ﬁtted ARMA-GARCH model: Two year
ahead monthly forecasts and 95% prediction bands.
6.11.3
Copper Data
The following example examines the daily evolution of copper prices for the
ten year period from January 4, 2005 to December 31, 2014. These prices,
expressed in terms of USD cents per pound, are plotted in Figure 6.15 while
the corresponding log returns are exhibited in Figure 6.16.
Furthermore,
the evolution of the squared returns are displayed ﬁn Figure 6.17.
From
these plots, notice the big drop in copper prices by the end of 2008 and
their recovery starting in 2009. These big ﬂuctuations are well represented
in Figure 6.17 showing the high volatility around that period. On the other
hand, the autocorrelation structure of the returns and the squared returns are
exhibited in Figure 6.18.
As in the previous case of the gold data, here we use the R package fGarch to
ﬁt a family of ARMA-GARCH or ARMA-APARCH models to these daily cop-
per prices data. We have ﬁtted both types of models to the data to see whether
a asymmetry is detected in this case. The selected models have ARMA(1, 0)
dependence structure for the returns and a GARCH(1, 1) or APARCH(1, 1)
dependence structure for the conditional variances.
From the outputs shown below, notice that leverage of the ARMA(1, 0)-
APARCH(1, 1) model is statically signiﬁcant at the 5% level.

232
NONLINEAR TIME SERIES
Year
Daily  Copper  Price
2006
2008
2010
2012
2014
150
200
250
300
350
400
450
Figure 6.15
Daily copper price, January 4, 2005 to December 31, 2014. Nominal
USD cents per pound.
Year
Daily Copper Log Returns
2006
2008
2010
2012
2014
-0.10
-0.05
0.00
0.05
0.10
Figure 6.16
Daily copper log returns, January 4, 2005 to December 31, 2014.

DATA APPLICATIONS
233
Year
Squared Daily Copper Log Returns
2006
2008
2010
2012
2014
0.000
0.002
0.004
0.006
0.008
0.010
0.012
0.014
Figure 6.17
Daily copper squared log returns, January 4, 2005 to December 31,
2014.
0
5
10
15
20
25
30
35
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
30
35
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 6.18
Sample ACF. (a) Daily log returns, (b) Squared returns.

234
NONLINEAR TIME SERIES
Model without Leverage
Title:
GARCH Modeling
Call:
garchFit(formula = ~arma(1, 0) + garch(1, 1), data = z,
trace = FALSE)
Mean and Variance Equation:
data ~ arma(1, 0) + garch(1, 1)
[data = z]
Conditional Distribution:
norm
Coefficient(s):
mu
ar1
omega
alpha1
beta1
2.5867e-04
-6.7762e-02
1.9980e-06
7.3973e-02
9.2140e-01
Std. Errors:
based on Hessian
Error Analysis:
Estimate
Std. Error
t value Pr(>|t|)
mu
2.587e-04
2.727e-04
0.948
0.34288
ar1
-6.776e-02
2.065e-02
-3.282
0.00103 **
omega
1.998e-06
6.868e-07
2.909
0.00362 **
alpha1
7.397e-02
9.505e-03
7.782 7.11e-15 ***
beta1
9.214e-01
9.717e-03
94.823
< 2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Log Likelihood:
6857.303
normalized:
2.71684
Standardised Residuals Tests:
Statistic p-Value
Jarque-Bera Test
R
Chi^2
89.50048
0
Shapiro-Wilk Test
R
W
0.994573
5.239038e-08
Ljung-Box Test
R
Q(10)
19.83763
0.03082641
Ljung-Box Test
R
Q(15)
22.152
0.1038799
Ljung-Box Test
R
Q(20)
28.05984
0.1079903
Ljung-Box Test
R^2
Q(10)
7.296483
0.69719
Ljung-Box Test
R^2
Q(15)
9.462268
0.8521343
Ljung-Box Test
R^2
Q(20)
15.32643
0.7574342
LM Arch Test
R
TR^2
8.271934
0.763535
Information Criterion Statistics:

DATA APPLICATIONS
235
AIC
BIC
SIC
HQIC
-5.429717 -5.418161 -5.429725 -5.425524
Model with Leverage
Title:
GARCH Modeling
Call:
garchFit(formula = ~arma(1, 0) + aparch(1, 1), data = z,
trace = FALSE)
Mean and Variance Equation:
data ~ arma(1, 0) + aparch(1, 1)
[data = z]
Conditional Distribution:
norm
Coefficient(s):
mu
ar1
omega
alpha1
7.3223e-05
-6.6990e-02
1.5967e-06
6.7513e-02
gamma1
beta1
delta
1.3569e-01
9.2796e-01
2.0000e+00
Std. Errors:
based on Hessian
Error Analysis:
Estimate
Std. Error
t value Pr(>|t|)
mu
7.322e-05
2.767e-04
0.265
0.79131
ar1
-6.699e-02
2.072e-02
-3.234
0.00122 **
omega
1.597e-06
7.891e-07
2.023
0.04302 *
alpha1
6.751e-02
1.234e-02
5.471 4.47e-08 ***
gamma1
1.357e-01
5.034e-02
2.696
0.00702 **
beta1
9.280e-01
9.239e-03
100.435
< 2e-16 ***
delta
2.000e+00
4.080e-01
4.902 9.47e-07 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Log Likelihood:
6862.59
normalized:
2.718934
Standardised Residuals Tests:
Statistic p-Value
Jarque-Bera Test
R
Chi^2
69.49289
7.771561e-16
Shapiro-Wilk Test
R
W
0.9954016 4.959389e-07
Ljung-Box Test
R
Q(10)
19.53039
0.03402065

236
NONLINEAR TIME SERIES
Ljung-Box Test
R
Q(15)
21.81076
0.1128628
Ljung-Box Test
R
Q(20)
27.66829
0.1174872
Ljung-Box Test
R^2
Q(10)
7.915388
0.6371015
Ljung-Box Test
R^2
Q(15)
10.32527
0.7987958
Ljung-Box Test
R^2
Q(20)
17.38353
0.6279475
LM Arch Test
R
TR^2
9.140525
0.6908832
Information Criterion Statistics:
AIC
BIC
SIC
HQIC
-5.432322 -5.416143 -5.432337 -5.426451
6.12
VALUE AT RISK
A usual criticism of variance or conditional variance as measures of ﬁnancial
risk is that they do not discriminate whether the return is positive or negative.
Of course, for the investor the sign of the return makes a big diﬀerence. As a
consequence, other methods for assessing investment risk have been developed.
One of these techniques is the Value at Risk, denoted as VaR hereafter. This
concept tries to measure the amount of capital that one investor can loose
when exposed to a ﬁnancial instrument. Let zα be the value satisfying the
equation
P(yt ≤zα) = α,
where α is the probability of the left tail of the return distribution. Based on
this expression, the Value at Risk of a ﬁnancial instrument is given by
VaR = C zα σt,
where C is the invested capital and σt is the conditional standard deviation
at time t.
When we are interested in evaluating the Value at Risk at a h-step horizon,
the return of the ﬁnancial instrument is given by
yt[h] = yt+1 + yt+2 + · · · + yt+h.
Consequently,
Var(yt[h]|Ft) = Var(yt+1 + yt+2 + · · · + yt+h|Ft).
Now, given that the sequence yt is white noise, we conclude that
Var(yt[h]|Ft) = Var(yt+1|Ft) + Var(yt+2|Ft) + · · · + Var(yt+h|Ft).
Consequently,
Var(yt[h]|Ft) =
h
X
j=1
bσ2
t+j.

VALUE AT RISK
237
-6
-4
-2
0
2
0
1
2
3
4
Return
Density
Figure 6.19
Value at risk.
Observe that for the IGARCH model σ2
t+j = σ2
t+1 so that
Var(yt[h]|Ft) = h σ2
t+1.
Therefore, the Value at Risk at horizon h is given by
VaR[h] = C zα
√
h σt+1.
EXAMPLE 6.2
Consider the daily IPSA stock index, from September 1, 2002 to October
14, 2014, see Appendix C for details. In this case, the ﬁtted model is
given by the following output
Title:
GARCH Modeling
Mean and Variance Equation:
data ~ arma(0, 1) + garch(1, 1)
Conditional Distribution:
norm

238
NONLINEAR TIME SERIES
Coefficient(s):
mu
ma1
omega
alpha1
beta1
7.1960e-04
1.8009e-01
2.8536e-06
1.4620e-01
8.2756e-01
Std. Errors:
based on Hessian
Error Analysis:
Estimate
Std. Error
t value Pr(>|t|)
mu
7.196e-04
1.587e-04
4.533 5.80e-06 ***
ma1
1.801e-01
1.848e-02
9.743
< 2e-16 ***
omega
2.854e-06
5.451e-07
5.235 1.65e-07 ***
alpha1 1.462e-01
1.443e-02
10.130
< 2e-16 ***
beta1
8.276e-01
1.589e-02
52.071
< 2e-16 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Log Likelihood:
10634.15
normalized:
3.3315
Standardised Residuals Tests:
Statistic p-Value
Jarque-Bera Test
R
Chi^2
89.30602
0
Shapiro-Wilk Test
R
W
0.9948589 3.945871e-09
Ljung-Box Test
R
Q(10)
12.21397
0.2709916
Ljung-Box Test
R
Q(15)
14.74627
0.4698436
Ljung-Box Test
R
Q(20)
23.24122
0.2771061
Ljung-Box Test
R^2
Q(10)
12.52548
0.2514255
Ljung-Box Test
R^2
Q(15)
15.00001
0.4514162
Ljung-Box Test
R^2
Q(20)
18.51519
0.5535096
LM Arch Test
R
TR^2
13.70263
0.3200994
Information Criterion Statistics:
AIC
BIC
SIC
HQIC
-6.659867 -6.650362 -6.659872 -6.656459
The estimated conditional standard deviation at the end of the period
is σt = 0.01030473. For an investment of $1,000,000, the Value at Risk
for horizons h = 1, . . . , 40 is shown in Figure 6.20.
Figure 6.21 shows GARCH parameters estimates based on windows
of size 400 observations and 20 values shifts. The dotted line indicates
βt, the broken line corresponds to αt while the heavy lines is the sum
αt + βt. Note that these values strongly decay around the year 2010.
As a consequence of the possible changes in the parameters of the
GARCH model, the estimates of the volatility also change. Figure 6.22
exhibits this phenomenon. This plot shows the estimates of σt arising

VALUE AT RISK
239
Horizon
VaR
0
10
20
30
40
20000
40000
60000
80000
100000
120000
Figure 6.20
IPSA Stock Index: Value at Risk (VaR) for horizons h = 1, . . . , 40.
from the sequence of windows (gray lines) and their estimates based on
a single GARCH model (black line).
Furthermore, Figure 6.23 displays the associated Value at Risk esti-
mates for both, the time-varying models indicated by the gray line as
well as the ﬁxed model denotes by the black line.
Table 6.4
IPSA Stock Index Value at Risk.
Horizon
VaR
Horizon
VaR
1
20,197.26
21
92,555.48
2
28,563.24
22
94,733.56
3
34,982.68
23
96,862.67
4
40,394.52
24
98,945.97
5
45,162.45
25
100,986.31
6
49,472.99
26
102,986.23
7
53,436.93
27
104,948.05
8
57,126.48
28
106,873.86
9
60,591.79
29
108,765.58
10
63,869.35
30
110,624.96
11
66,986.74
31
112,453.60
12
69,965.37
32
114,252.97
13
72,822.26
33
116,024.44
14
75,571.23
34
117,769.26
15
78,223.66
35
119,488.61
16
80,789.05
36
121,183.57
17
83,275.44
37
122,855.15
18
85,689.73
38
124,504.28
19
88,037.82
39
126,131.86
20
90,324.90
40
127,738.70

240
NONLINEAR TIME SERIES
Time
IPSA GARCH Estimates
2004
2006
2008
2010
2012
2014
0.0
0.2
0.4
0.6
0.8
1.0
Figure 6.21
IPSA Stock Index: Parameter estimates.
Time
!t
2002
2004
2006
2008
2010
2012
2014
0.01
0.02
0.03
0.04
0.05
0.06
0.07
Figure 6.22
IPSA Stock Index: Volatility estimates, σt.

AUTOCORRELATION OF SQUARES
241
Time
VaR
2002
2004
2006
2008
2010
2012
2014
20000
40000
60000
80000
100000
120000
140000
Figure 6.23
IPSA Stock Index: Value at Risk (VaR) estimates.
6.13
AUTOCORRELATION OF SQUARES
In this section we examine the autocorrelation of square transformation of a
stationary process with a Wold expansion. The analysis of such transforma-
tions may give valuable clues about crucial aspects such as linearity, normality,
or memory of the process. For instance, these issues are particularly impor-
tant when studying the behavior of heteroskedastic processes since in this
context the series usually represents the return of a ﬁnancial instrument and
the squared series is a rough empirical measure of its volatility. In this case,
since we are interested in predicting both the returns and the volatility, we
must analyze the dependence structure of a time series and its squares.
6.13.1
Squares of Gaussian Processes
Let {yt : t ∈Z} be a Gaussian process with E[yt] = 0 and Var[yt) = 1. For
the transformation f(yt) = y2
t , the coeﬃcients of the Hermite expansion are
α0 = 1, α2 = 1, and αj = 0 for all j ̸= 0, 2. Thus, we have ⟨f(yt), f(ys)⟩=
1 + ρ2
y(h)/2. But, E(y2
t ) = 1 so that
Cov[f(yt), f(ys)] = ⟨f(yt), f(ys)⟩−1 = ρ2
y(h)/2,

242
NONLINEAR TIME SERIES
and then the autocorrelation function of f(yt) = y2
t is
ρy2(h) = ρ2
y(h).
(6.23)
From this expression we observe that since |ρy| ≤1, ρy2(h) is smaller or equal
than ρy(h). Consequently, the autocorrelation of the squares is smaller than
the autocorrelation of the original series. Actually, for a Gaussian process this
reduction of the dependence is true for any transformation, as stated in the
following result: Let {yt : t ∈Z} be a Gaussian process and let F be the class
of all measurable transformations such that E[f(yt)] = 0 and E[f(yt)2] = 1.
Then,
sup
f∈F
E[f(yt)f(ys)] = |ρy(t −s)|,
where ρy(t −s) is the correlation between yt and ys.
As a consequence of our previous discussion, in order to account for situ-
ations where the squares exhibit more dependence than the series itself, we
must abandon Gaussianity. In the next section we examine this issue in detail.
6.13.2
Autocorrelation of Squares
Consider the regular linear process {yt} with Wold expansion
yt = ψ(B)εt,
(6.24)
where ψ(B) = P∞
i=0 ψiBi, ψ0 = 1, and P∞
i=0 ψ2
i < ∞.
The input noise
sequence {εt} is assumed to be white noise.
(Linear Process) Assume that {εt} are independent identically distributed
random variables with zero-mean and ﬁnite kurtosis η. Then,
ρy2(h) =
2
κ −1ρ2
y(h) + κ −3
κ −1α(h),
(6.25)
where κ is the kurtosis of yt given by
κ = (η −3)
 ∞
X
i=0
ψ2
i
!−2 ∞
X
i=0
ψ4
i + 3.
(6.26)
Furthermore, if yt is Gaussian, then η = 3 and κ = 3. Therefore, ρy2 = ρ2
y,
which coincides with formula (6.23).
EXAMPLE 6.3
Consider the following AR(1)-ARCH(1) process described by the equa-
tions
yt
=
φyt−1 + εt,
εt
=
ϵtσt,
σ2
t
=
α0 + βε2
t−1,

AUTOCORRELATION OF SQUARES
243
where ϵt is sequence of independent and identically distributed random
variables with distribution N(0, 1). The autocorrelation function of {y2
t }
is given by
ρy2(h) = φ2|h|

1 + η −1
κ −1∆(0)[β|h| −1]

.
In this case, ρy2(h) = O(φ2|h|) and therefore the squared process {y2
t }
has short memory.
6.13.3
Illustrations
Figure 6.24 shows the sample ACF of a series of 1000 observations from a
Gaussian ARFIMA(0, d, 0) process with d = 0.4 and the sample ACF of the
squares. Since this is a Gaussian process, from formula (6.23) we expect that
ρy2 ∼ρ2
y. Additionally, given that ρy(h) ∼Ch2d−1, the ACF of the squared
series should behave like ρy2(h) ∼C2h2 
d−1, where d = 2d −1
2. In this case,
d = 0.3. Thus, the sample ACF of y2
t should decay a bit more rapidly than
the ACF of yt as it seems to be the case when comparing panels (a) and (b).
A similar behavior occurs when d = 0.2; see Figure 6.25, where d = −0.1.
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 6.24
Simulated fractional noise process FN(d), 1000 observations with
d = 0.4. (a) ACF of the series and (b) ACF of the squared series.

244
NONLINEAR TIME SERIES
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 6.25
Simulated fractional noise process FN(d), 1000 observations with
d = 0.2. (a) ACF of the series and (b) ACF of the squared series.
Figure 6.26 displays the sample ACF from 1000 simulated observations of
the ARCH(1) process:
yt
=
ϵt σt,
σ2
t
=
0.1 + 0.8 y2
t−1,
where ϵt is assumed to be a sequence of independent and identically distributed
N(0, 1) random variables. Note that in panel (a), as expected, the sample ACF
of the series yt shows no signiﬁcant correlations. On the contrary, the sample
ACF of the squared series shown in panel (b) exhibits a substantial level of
autocorrelation, which decays at an exponential rate. A similar behavior of
the autocorrelation is displayed by Figure 6.27, which depicts the sample ACF
of 1000 observations from the following GARCH(1, 1) model:
yt
=
ϵt σt,
σ2
t
=
0.1 + 0.7 y2
t−1 + 0.2 σ2
t−1,
where {ϵt} is a sequence of independent and identically distributed N(0, 1)
random variables.

AUTOCORRELATION OF SQUARES
245
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 6.26
Simulated ARCH(1) process: 1000 observations with α0 = 0.1 and
α1 = 0.8. (a) ACF of the series and (b) ACF of the squared series.
Figure 6.28 exhibits a trajectory of 1000 observations from the ARFIMA(0, d, 0)-
GARCH(1, 1) process:
yt
=
∞

j=0
ψj εt−j,
εt
=
ϵt σt,
σ2
t
=
0.1 + 0.7 ε2
t−1 + 0.2 σ2
t−1,
where d = 0.4,
ψj =
Γ(0.4 + j)
Γ(1 + j)Γ(0.4),
and ϵt is an independent and identically distributed Gaussian sequence with
zero-mean and unit variance. Panel (a) shows the series while panel (b) shows
the squares. On the other hand, Figure 6.29 shows the sample ACF of this
series; see panel (a) and the sample ACF of the squares; see panel (b). Note
that in this case both panels seem to exhibit long-memory behavior because
d ∈( 1
4, 1
2).

246
NONLINEAR TIME SERIES
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 6.27
Simulated GARCH(1,1) process: 1000 observations with α0 = 0.1,
α1 = 0.7, and β1 = 0.2. (a) ACF of the series and (b) ACF of the squared series.
(a)
Time
z
0
200
400
600
800
1000
-10
-5
0
5
10
(b)
Time
z^2
0
200
400
600
800
1000
0
20
40
60
80
100
Figure 6.28
Simulated ARFIMA(0, d, 0)-GARCH(1, 1) process: 1000 observations
with d = 0.4, α0 = 0.1, α1 = 0.7, and β1 = 0.2. (a) Series and (b) squared series.

THRESHOLD AUTOREGRESSIVE MODELS
247
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 6.29
Simulated ARFIMA(0, d, 0)-GARCH(1, 1) process: 1000 observations
with d = 0.4, α0 = 0.1, α1 = 0.7, and β1 = 0.2. (a) ACF of the series and (b) ACF
of the squared series.
6.14
THRESHOLD AUTOREGRESSIVE MODELS
Nonlinearity is an extended phenomenon in diﬀerent ﬁelds. In the previous
sections we have examined heteroskedastic processes as tools for modeling ﬁ-
nancial time series. Nevertheless, there is a pleyade of other nonlinear models.
One important example is the so-called threshold time series models. In this
context, the process is assumed to have diﬀerent regimes which are determined
by, for instance, a function of the level of lagged values of the process.
In this section we brieﬂy review two simple examples of threshold autore-
gressive process (TAR) and self-exciting threshold autoregressive process (SE-
TAR).
A simple version of a threshold autoregressive process TAR(p) can be writ-
ten as
yt = µJt +
p

i=1
φJt
i yt−i + ϕJt εt,
where Jt ∈{1, . . . , J} is a regime switching mechanism indicator and εt is an
i.i.d. sequence with zero-mean and variance σ2.

248
NONLINEAR TIME SERIES
Time
Series
0
20
40
60
80
100
-4
-2
0
2
4
6
Figure 6.30
SETAR simulated time series.
A particular case of the above model is the self-exciting threshold autore-
gressive model (SETAR). Consider p = 1, J = 2 and this model can be written
in terms of a thresholding variable zt as follows
yt =

µ1 + φ1 yt−1 + εt
if
zt−d ≤r
µ2 + φ2 yt−1 + ϕ εt
if
zt−d > r.
where r is a threshold parameter and d is a delay parameter. The variable
zt can be deﬁned in terms of values of the process as, for example, zt =
β0 yt + β1 yt−1.
As an illustration of the features of a SETAR process, Figure 6.30 depicts
the trajectory of a SETAR model with µ1 = −1, φ1 = 0.2, µ2 = 1, φ2 = 0.7,
ϕ = 1 and Var(εt) = 1. Notice that the ﬁrst half of the trajectory seems to
be in one regime with mean -1 and the second half of the series seems to be
in second regime which has mean 1. The sample ACF and sample PACF of
this series are shown in Figure 6.31.
The R package tsDyn allows for the estimation of TAR and SETAR models.
The output in this case is
> st=setar(Yt, m=1)
> summary(st)
Non linear autoregressive model

THRESHOLD AUTOREGRESSIVE MODELS
249
0
5
10
15
20
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
5
10
15
20
-0.2
0.0
0.2
0.4
0.6
0.8
Lag
Partial ACF
(b)
Figure 6.31
SETAR simulated time series. (b) Sample ACF, (b) Sample PACF.
SETAR model ( 2 regimes)
Coefficients:
Low regime:
const.L
phiL.1
-1.0779184
0.2183248
High regime:
const.H
phiH.1
1.1472667 0.6479871
Threshold:
-Variable: Z(t) = + (1) X(t)
-Value: 0.9398
Proportion of points in low regime: 71.72%
High regime: 28.28%
Residuals:
Min
1Q
Median
3Q
Max

250
NONLINEAR TIME SERIES
-3.654675 -0.685460
0.019659
0.744333
2.938611
Fit:
residuals variance = 1.099,
AIC = 19, MAPE = 531.6%
Coefficient(s):
Estimate
Std. Error
t value
Pr(>|t|)
const.L
-1.07792
0.21597
-4.9910 2.678e-06 ***
phiL.1
0.21832
0.12389
1.7623
0.08121 .
const.H
1.14727
0.49396
2.3226
0.02231 *
phiH.1
0.64799
0.14221
4.5566 1.530e-05 ***
---
Signif. codes:
0 *** 0.001 ** 0.01 * 0.05 . 0.1
1
Threshold
Variable: Z(t) = + (1) X(t)
Value: 0.9398
Notice that the estimates produced by this package are close to the true
parameter values.
Time
Residuals
0
20
40
60
80
100
-2
-1
0
1
2
Figure 6.32
SETAR ﬁtted model residuals.

THRESHOLD AUTOREGRESSIVE MODELS
251
0
5
10
15
20
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
5
10
15
20
-0.2
-0.1
0.0
0.1
0.2
Lag
Partial ACF
(b)
Figure 6.33
SETAR Residuals. (a) Sample ACF, (b) Sample PACF.
Index
Series
0
20
40
60
80
100
-4
-2
0
2
4
6
Low regime
High regime
Figure 6.34
SETAR Fitting. Threshold and regimen classiﬁcation.

252
NONLINEAR TIME SERIES
-4
-2
0
2
4
6
-4
-2
0
2
4
6
Lag 0
Lag -1
Low regime
High regime
Figure 6.35
SETAR Fitting. Scatterplot of yt and yt−1 along with empirical slope
estimates for the two regimes.
6.15
BIBLIOGRAPHIC NOTES
Engle (1982) proposed the ARCH models to account for the stylized facts
exhibited by many economic and ﬁnancial time series. Based on this semi-
nal work, a plethora of related models have been introduced. Among these
methodologies we ﬁnd the GARCH models [see, for example, Bollerslev (1986)
and Taylor (1986)], the EGARCH models [see, for instance, Nelson (1991)],
the stochastic volatility processes (SV) [see, for example, Harvey, Ruiz, and
Shephard (1994)], the FIGARCH and FIEGARCH models [see, for instance,
Baillie, Bollerslev, and Mikkelsen (1996) and Bollerslev and Mikkelsen (1996)],
and the long-memory generalized autoregressive conditionally heteroskedastic
(LMGARCH) models [see, for example, Robinson (1991), Robinson and Henry
(1999) and Henry (2001)].
Most econometric models dealing with long-memory and heteroskedastic
behaviors are nonlinear in the sense that the noise sequence is not necessarily
independent. In particular, in the context of ARFIMA-GARCH models, the
returns have long-memory and the noise has a conditional heteroskedastic-
ity structure. These processes have received considerable attention; see, for
example, Ling and Li (1997) and references therein.
A related class of interesting models is the extension of the ARCH(p) pro-
cesses to the ARCH(∞) models to encompass the longer dependence observed

PROBLEMS
253
in many squared ﬁnancial series. The ARCH(∞) class was ﬁrst introduced
by Robinson (1991).
On the other hand, extensions of the stochastic volatility processes to the
long-memory case have produced the LMSV models; see Harvey, Ruiz, and
Shephard (1994), Ghysels, Harvey, and Renault (1996), Breidt, Crato, and
de Lima (1998), and Deo and Hurvich (2003). Other estimation procedures
for LMSV using state space systems can be found in Chan and Petris (2000)
and Section 11 of Chan (2002). Furthermore, exact likelihood-based Bayesian
estimation of LMSV is discussed in Section 4 of Brockwell (2004).
Threshold autoregressive processes (TAR) as well as self-exciting threshold
autoregressive (SETAR) models are reviewed, for example, in Tsay (1989)
and Tong (1990, 2011).
Problems
6.1
Suppose that the series y1, . . . , y150 corresponds to the ARCH(1) pro-
cess:
yt
=
εtσt,
σ2
t
=
α0 + α1 y2
t−1,
where εt is white noise (0, 1).
(a) Assume that the MLE of α1 es 0.32. Is this ﬁtted process stationary?
(b) Build a 95% conﬁdence interval for α1.
6.2
Consider the model yt = εtyt−1 where ϵt is white noise (0, σ2)
(a) Show that the conditional variance of yt given yt−1 is σ2y2
t−1
(b) Verify that under the assumption of second order stationarity if σ2 ̸=
1 then the variance of yt is zero or inﬁnite.
(c) Write this model as a Volterra expansion. What assumption on y0
seems reasonable?
6.3
Explain brieﬂy the following concepts.
(a) Return of a ﬁnancial instrument.
(b) Conditional heteroskedasticity.
(c) Non linear process.
(d) Best lineal predictor.
6.4
Consider the following stationary process
xt = θ yt−1 + yt,
yt = (α + β y2
t−1)1/2 zt,
with |θ| < 1, α > 0, 0 ≤β < 1 and {zt} is an i.i.d. N(0, 1) sequence.
(a) Determine the autocovariance function of {yt}.
(b) Determine the autocovariance function of {xt}.

254
NONLINEAR TIME SERIES
Hint: Recall that if X and Y are random variables, then E[g(X)] =
E {E [g(X) | Y ]}.
6.5
Consider the ARCH(2) process that satisﬁes the equation
yt
=
εt σt,
σ2
t
=
α0 + α1 y2
t−1 + α2 y2
t−2,
where {εt} is i.i.d. (0, 1). Let σ2
t (h) ≡E[σ2
t+h|yt, yt−1, . . . ] the h-step predictor
of the future volatility for h ≥1.
(a) Verify that
σ2
t (2) = α0 + α1 σ2
t+1 + α2 y2
t .
(b) Show that
σ2
t (3) = α0 (1 + α1) + (α2
1 + α2) σ2
t+1 + α1 α2 y2
t .
6.6
Consider two ARCH(1) processes {yt} and {xt}, independent, deﬁned
by
yt = εtσt,
σ2
t = α0 + α1y2
t−1,
xt = ηtνt,
ν2
t = β0 + β1x2
t−1,
where {εt} and {ηt} are i.i.d. N(0, 1) sequences.
Deﬁne the new process zt = yt · xt.
(a) Show that E(zt) = 0.
(b) Show that E(z2
t |yt−1, yt−2, . . . ) = σ2
t ν2
t .
(c) Verify that E(z2
t z2
t+k) =

µ2
y + 2αk
1σ4
1−α2
1
 
µ2
x + 2βk
1 ν4
1−β2
1

, where
µy =
α0
1 −α1
y
µx =
β0
1 −β1
σ4 =
α2
0(1 + α1)
(1 −α1)(1 −3α3
1)
y
ν4 =
β2
0(1 + β1)
(1 −β1)(1 −3β3
1)
(d) Calculate Cov(z2
t , z2
t+k).
6.7
Consider the GARCH(1,1) process:
yt
=
εtσt,
σ2
t
=
α0 + α1y2
t−1 + β1σ2
t−1

PROBLEMS
255
where {εt} is i.i.d.
(0, 1) Let σ2
t (h) = E(σ2
t+h|yt, yt−1, . . . ) be the h-step
volatility forecast.
(a) Show that for h ≥2
σ2
t (h) = α0 + (α1 + β1)σ2
t (h −1).
(b) Verify that the limit of σ2
t (h) as h increases satisﬁes
lim
h→∞σ2
t (h) =
α0
1 −(α1 + β1).
6.8
Consider the integrated GARCH model IGARCH(1,1) deﬁned by
yt
=
εtσt,
σ2
t
=
α0 + β1σ2
t−1 + (1 −β1)y2
t−1,
where εt is white noise (0, 1) and β1 ∈(0, 1). Let ηt = y2
t −σ2
t .
(a) Show that ηt is white noise.
(b) Verify that y2
t satisﬁes
y2
t −y2
t−1 = α0 + (1 −β1B)ηt.
(c) Based on the above, What mode satisﬁes y2
t ?
(d) Show that the ℓ-step variance predictor ℓ≥1 is given by
σ2
n(ℓ) = σ2
n(1) + (ℓ−1)α0.
6.9
Consider the following exponential GARCH, EGARCH(1, 0) model
yt
=
εtσt,
(1 −αB) ln(σ2
t )
=
(1 −α)α0 + g(εt−1),
g(εt−1)
=
θεt−1 −γ[|εt−1| −E(|εt−1|)],
where ϵt is white noise N(0, 1).
(a) Show that
(1 −αB) ln(σ2
t ) =

α∗+ (θ + γ)εt−1
si
εt−1 ≥0
α∗+ (θ −γ)εt−1
si
εt−1 < 0,
where α∗= (1 −α)α0 −(
p
2/π)γ.
(b) Verify that
σ2
t = σ2α
t−1 exp(α∗)



exp[(θ + γ)
yt−1
√
σ2
t−1 ]
si
yt−1 ≥0
exp[(θ −γ)
yt−1
√
σ2
t−1 ]
si
yt−1 < 0.

256
NONLINEAR TIME SERIES
(c) What advantages has this model over a standard GARCH process?
6.10
Consider the stochastic volatility process {rt} given by
rt
=
εtσt,
σt
=
σ exp(νt/2),
where {εt} is an independent and identically distributed sequence with zero-
mean and unit variance and {νt} is a regular linear process satisfying
νt =
∞
X
j=0
ψjηt−j,
with P∞
j=0 ψ2
j < ∞and {ηt} an independent and identically distributed se-
quence with zero-mean and unit variance, independent of the sequence {εt}.
Show that the process rt is strictly stationary and ergodic.
6.11
Assume that π(B) = (1 −B)d. Show that π(B)α0 = 0, where α0 is
any real constant and d > 0.
6.12
Show that the FIGARCH process may be written as
θ(B)σ2
t = ω + [θ(B) −φ(B)(1 −B)d]y2
t ,
(6.27)
where ω = θ(B)α0. What conditions must satisfy the polynomial θ(B) in
order to ensure that ω is a positive constant?
6.13
Let λ(d) = (1 −B)−d for |d| < 1
2.
(a) Verify that the ARFIMA(0, d, 0)-GARCH model may be written as
λ(d)εt(d) = c,
where c is a constant with respect to d. Note that the data {yt} do
not depend on d.
(b) Let ψ(B) = P∞
j=0 ψj(d)Bj = (1 −B)−d = λ(d)−1. Show that
∞
X
j=0
 ∂
∂dψj(d)

εt−j(d) +
∞
X
j=0
ψj(d)
 ∂
∂dεt−j(d)

= 0.
(c) Show that
∂
∂dεt(d)
=
−λ(d)
∞
X
j=0
 ∂
∂dψj(d)

εt−j(d)
=
−λ(d)
 ∂
∂dλ(d)

εt(d)
=
−
 ∂
∂d log λ(d)

εt(d).

PROBLEMS
257
(d) Verify that
∂
∂dεt(d) = −
∞
X
j=1
1
j εt−j(d),
and prove that this expansion is well-deﬁned.
6.14
Assume that the sequence {ϵt} in (6.7) corresponds to independent
and identically distributed uniform random variables U(−
√
3,
√
3).
(a) Verify that {ϵt} is a sequence of zero-mean and unit variance random
variables.
(b) Show that for this speciﬁcation of {ϵt}, the top Lyapunov exponent
of the model in Example 6.1 is given by
γ = E[log(α1ϵ2 + β1)] = 2
"
log
p
3α1 + β1 +
r
β1
3α1
arctan
r3α1
β1
−1
#
.
Hint: The following formula could be useful:
Z
log(x2 + a2) dx = x log(x2 + a2) + 2a arctan x
a −2x.
6.15
Consider the ARFIMA(p, d, q)-GARCH(1, 1) process:
yt
=
∞
X
j=0
ψjεt−j,
εt
=
ϵtσt,
σ2
t
=
α0 + αε2
t−1 + βσ2
t−1,
where ϵt is a random variable with density
f(ϵ) =
2
π(1 + ϵ2)2 ,
for −∞< ϵ < ∞.
(a) Verify that the random variable ϵ satisﬁes
E(ϵ) = 0,
Var(ϵ) = 1.
(b) Show that the top Lyapunov exponent in this case is given by
γ = 2

log(√α +
p
β) −
√α
√α + √β

(c) Verify whether the Lyapunov exponent γ is negative for α > 0, β > 0,
and α + β < 1.

258
NONLINEAR TIME SERIES
Hint: The following formula could be useful:
Z ∞
0
log(a2 + b2x2)
dx
(1 + x2)2 = π
2

log(a + b) −
b
a + b

,
for a, b > 0; see Gradshteyn and Ryzhik (2000, p. 557).
6.16
Consider the ARFIMA-GARCH process deﬁned in Problem 6.15 where
ϵt is a random variable with density
f(ϵ) =
1
π
√
1 −ϵ2 ,
for ϵ ∈(−1, 1).
(a) Verify that ϵ is a zero-mean and unit variance random variable.
(b) Prove that the the top Lyapunov exponent in this case is
γ = 2 log
√β + √α + β
2
.
(c) Show that the Lyapunov exponent γ is negative for α > 0, β > 0,
and α + β < 1.
Hint: The following integrals could be useful:
Z
dx
√
1 −x2
=
arcsin x,
Z
x2dx
√
1 −x2
=
1
2(arcsin x −x
p
1 −x2),
and
Z 1
0
log(1 + ax2)
dx
√
1 −x2 = π log 1 + √1 + a
2
,
for a ≥−1; see Gradshteyn and Ryzhik (2000, p. 558).
6.17
A deﬁnition of the FIEGARCH model is
log(σ2
t ) = ω + φ(B)−1(1 −B)−dψ(B)g(ϵt−1),
(6.28)
where φ(B) = 1 + φ1B + · · · + φpBp, ψ(B) = 1 + ψ1B + · · · + ψqBq, and
g(ϵt) = θϵt + γ[|ϵt| −E(|ϵt|)].
Another deﬁnition of a FIEGARCH process is
φ(B)(1 −B)d log(σ2
t ) = a +
q
X
j=1
(bj|ϵt−j| + γjϵt−j).
(6.29)

PROBLEMS
259
(a) Show that φ(B)(1 −B)dω = 0.
(b) Starting from deﬁnition (6.28), prove that
φ(B)(1 −B)d log(σ2
t ) = −ψ(B)γµ + θψ(B)ϵt−1 + γψ(B)|ϵt−1|,
where µ = E(|ϵ1|).
(c) Show that by taking bj = θψj, γj = γψj, and a = −µγ Pq
j=1 ψj, we
obtain deﬁnition (6.29). Observe, however, that in model (6.29) we
could release the parameters bj and γj from the restriction bjγ = γjθ.
(d) Verify that by taking θj = bj, λj = γj, and α = a, deﬁnition (6.16)
is obtained.
6.18
Consider the FIEGARCH model described by equation (6.16) and as-
sume that α = −Pq
j=1 θj E(|ϵ1|).
(a) Verify that
E[α + θ(B)|ϵt−1| + λ(B)ϵt−1] = 0.
(b) Show that conditional variance σ2
t may be formally written as
σ2
t = exp

φ(B)−1(1 −B)−d[θ(B)(|ϵt−1| −E |ϵt−1|) + λ(B)ϵt−1]
	
.
(c) Show that the FIEGARCH process yt may be formally written as
yt = ϵt exp
 1
2φ(B)−1(1 −B)−d[θ(B)(|ϵt−1| −E |ϵt−1|) + λ(B)ϵt−1]
	
.
(d) Consider a FIEGARCH(0, d, 1) where φ(B) = 1, (1−B)−d = P∞
j=0 ψjBj,
θ(B) = θ, and λ(B) = λ. Show that σ2
t may be formally expressed
as
σ2
t =
∞
Y
j=0
exp [θψj(|ϵt−1| −E |ϵt−1|) + λψjϵt−1] .
(e) Under what conditions is the above inﬁnite product well-deﬁned?
6.19
Consider the following tree ring data from a location at Malleco, Chile,
for the period from 1242 A.D. to 1975 A.D. This series is displayed in Figure
6.36.
A researcher proposes that the ring width during year t, say yt, depends
on the past as follows:
yt =

ν + zt,
t = 1242
ν + φ yt−1 + zt,
t = 1243, 1244, . . . , 1975
with {zt} independent random variables with Normal(0, σ) distribution. ν, φ
(|φ| < 1) and σ are parameters.

260
NONLINEAR TIME SERIES
Year
Malleco tree ring series
1400
1600
1800
2000
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Figure 6.36
Tree rings data at Malleco, Chile, for the period from 1242 A.D. to
1975 A.D.
(a) Show that
y1242+k = ν (1 −φk+1)
1 −φ
+
k

j=0
φj z1242+k−j
(b) Show that
Cov(yt, yt+h) = σ2 φh
1 −φ2 (t−1242+1)
1 −φ2

for t = 1242, . . . , 1975 and h = 0, 1, 2, . . ..
6.20
Suppose that the process {xt} follows a TGARCH(1, 1) process deﬁned
as
xt = zt · σt
σ2
t = ω + (α + γ · δt−1) x2
t−1 + β σ2
t−1
δt =
 1,
xt < 0
0,
xt ≥0
with ω > 0; α ≥0, γ ≥0, β ≥0, 0 ≤α + γ + β < 1 y {zt}
iid
∼Normal(0, 1).
Find E(xt) and Var(xt).

PROBLEMS
261
6.21
Consider the following GARCH(3, 2) process
xt = zt · σt,
{zt} ∼WN(0, 1)
σ2
t = α0 +
3
X
i=1
αi x2
t−i +
2
X
j=1
βj σ2
t−j,
with
α0 > 0,
αi ≥0,
βj ≥0,
0 ≤(α1 + α2 + α3 + β1 + β2) < 1.
Show that x2
t corresponds to an ARMA(3, 2) process and its noise is given by
νt = σ2
t (z2
t −1).
6.22
Consider the {xt} process with mean µ given by:
xt −µ = φ1 (xt−1 −µ) + · · · + φp (xt−p −µ) + zt,
(6.30)
with {zt} ∼(0, σ2). Show that (6.30) can be written as
∇xt = φ∗
0 + φ∗
1 xt−1 + φ∗
2 ∇xt−1 + · · · + φ∗
p ∇xt−p+1 + zt,
where φ∗
0 = µ (1−φ1−· · ·−φp), φ∗
1 =
p
X
i=1
φi−1 y φ∗
j = −
p
X
i=j
φi to j = 2, . . . , p.
6.23
Let {x1, . . . , xn} observed values of a time series and ˆρ(h) sample au-
tocorrelation function.
(a) If xt = a + b t, where a y b are constants and b ̸= 0, show that h ≥1,
ˆρ(h) →1, when n →∞.
(b) If xt = c cos(ω t), where c y ω are constants (c ̸= 0 y ω ∈(−π, π]),
show that for any h, ˆρ(h) →cos(ω h), when n →∞.
6.24
An ARMA model is ﬁtted to a series of 500 observations collected
sequentially in time. The results of the ﬁrst 10 empirical autocorrelations is
as follows:
k
1
2
3
4
5
ˆρ(k)
−0.065
0.735
−0.061
0.386
−0.052
ˆα(k)
−0.065
0.734
−0.002
−0.336
−0.005
k
6
7
8
9
10
ˆρ(k)
0.238
−0.030
0.155
−0.033
0.071
ˆα(k)
0.258
0.028
−0.162
−0.068
0.035
,
where the sample variance of the data is 2.708. Suppose that the residuals of
model behave like a white noise of zero-mean and variance σ2.

262
NONLINEAR TIME SERIES
(a) Plot the ACF and PACF together with their empirical conﬁdence
bands. Based on these graphs, which model ARMA you ﬁnd more
suitable for this series?
(b) Find moment estimators of the parameters of the proposed model,
including estimation of σ2, and evaluate them according to the avail-
able information. If more than one possible value is available, choose
the coeﬃcient involving a causal and invertible model.
(c) Specify the asymptotic distribution of the autoregressive coeﬃcients
and/or moving-average model. Are they signiﬁcantly diﬀerent from
zero at a 5% level?.
6.25
Consider a ﬁnancial time series of 1500 observations. Two heteroskedas-
tic models have been ﬁtted to this series, a GARCH(1,1) along with an
ARCH(2) process.
The outputs from these ﬁtted models are reported be-
low. Which of the two models seems to better ﬁt the series? Justify your
answer.
Fitted Model 1
Title:
GARCH Modeling
Call:
garchFit(formula = ~garch(1, 1), data = y, trace = FALSE)
Mean and Variance Equation:
data ~ garch(1, 1)
[data = xx]
Conditional Distribution:
norm
Coefficient(s):
mu
omega
alpha1
beta1
3.2137e-05
9.1888e-07
1.8917e-01
7.1562e-01
Std. Errors:
based on Hessian
Error Analysis:
Estimate
Std. Error
t value Pr(>|t|)
mu
3.214e-05
6.677e-05
0.481
0.63
omega
9.189e-07
2.269e-07
4.050 5.11e-05 ***
alpha1 1.892e-01
3.047e-02
6.208 5.38e-10 ***
beta1
7.156e-01
4.429e-02
16.157
< 2e-16 ***
---

PROBLEMS
263
Signif. codes:
0 ***
0.001 **
0.01 *
0.05
.
0.1
1
Log Likelihood:
6659.775
normalized:
4.43985
Standardised Residuals Tests:
Statistic p-Value
Jarque-Bera Test
R
Chi^2
5.941889
0.0512548
Shapiro-Wilk Test
R
W
0.9983002 0.1345606
Ljung-Box Test
R
Q(10)
10.32037
0.4128496
Ljung-Box Test
R
Q(15)
14.34846
0.4992822
Ljung-Box Test
R
Q(20)
18.88156
0.5295365
Ljung-Box Test
R^2
Q(10)
5.289371
0.8710286
Ljung-Box Test
R^2
Q(15)
8.75091
0.8901772
Ljung-Box Test
R^2
Q(20)
10.02904
0.9676424
LM Arch Test
R
TR^2
7.27484
0.8389293
Information Criterion Statistics:
AIC
BIC
SIC
HQIC
-8.874367 -8.860199 -8.874381 -8.869089
Fitted Model 2
Title:
GARCH Modeling
Call:
garchFit(formula = ~garch(2, 0), data = y, trace = FALSE)
Mean and Variance Equation:
data ~ garch(2, 0)
[data = xx]
Conditional Distribution:
norm
Coefficient(s):
mu
omega
alpha1
alpha2
3.5973e-05
5.4612e-06
2.5347e-01
1.6910e-01
Std. Errors:
based on Hessian
Error Analysis:

264
NONLINEAR TIME SERIES
Estimate
Std. Error
t value Pr(>|t|)
mu
3.597e-05
6.977e-05
0.516
0.606
omega
5.461e-06
3.702e-07
14.752
< 2e-16 ***
alpha1 2.535e-01
4.267e-02
5.941 2.83e-09 ***
alpha2 1.691e-01
3.975e-02
4.254 2.10e-05 ***
---
Signif. codes:
0 *** 0.001 **
0.01 *
0.05 .
0.1
1
Log Likelihood:
6631.342
normalized:
4.420894
Standardised Residuals Tests:
Statistic p-Value
Jarque-Bera Test
R
Chi^2
32.41568
9.141626e-08
Shapiro-Wilk Test
R
W
0.9954953 0.0001870913
Ljung-Box Test
R
Q(10)
10.14591
0.427787
Ljung-Box Test
R
Q(15)
14.62402
0.478823
Ljung-Box Test
R
Q(20)
20.15955
0.4479896
Ljung-Box Test
R^2
Q(10)
46.92196
9.756395e-07
Ljung-Box Test
R^2
Q(15)
56.6825
9.289409e-07
Ljung-Box Test
R^2
Q(20)
60.68203
5.582236e-06
LM Arch Test
R
TR^2
56.86594
8.357072e-08
Information Criterion Statistics:
AIC
BIC
SIC
HQIC
-8.836455 -8.822287 -8.836470 -8.831177
6.26
Consider the following stochastic volatility model
yt = εtσt,
σ2
t = evt,
where {εt} is a zero-mean and unit variance i.i.d. sequence and y{vt} corre-
sponds to a Gaussian MA(1) process, that is,
vt = µ + ηt + θηt−1,
with ηt ∼N(0, σ2
η). Verify that:
(a) E(y2
t |yt−1, yt−2, . . . ) = evt.
(b) log[E(y2
t )] = µ + σ2
η(1 + θ2)/2.
(c) E{log[E(y2
t |yt−1, yt−2, . . . ]} = µ.

PROBLEMS
265
6.27
Suppose that the sequence yt satisﬁes a GARCH(0, q) process deﬁned
as follows.
yt = εtσt,
σ2
t = α0 + β1σ2
t−1 + · · · + βqσ2
t−q,
where {εt} i.i.d. N(0,1). Show that:
(a) E(σ2
t |yt−k, yt−k−1, . . . ) = σ2
t , for all k ∈Z.
(b) For k ≥1.
E(y2
t+k|yt, yt−1, yt−2, . . . ) = σ2
t+k.


CHAPTER 7
PREDICTION
One of the fundamental aspects of the time series analysis is forecasting.
Consequently, this chapter addresses the prediction of linear and nonlinear
processes. Section 7.2 and Section 7.3 examine the formulation of one-step and
multistep ahead predictors based on ﬁnite and inﬁnite past. The innovations
algorithm and approximate predictors are also described.
Forecasting future volatility is a crucial aspect in the context of heteroskedas-
tic time series.
Therefore, Section 7.4 discusses techniques for forecasting
volatility for some of the models described in Chapter 6. Several illustrative
applications are also discussed, including the prediction of ARMA, ARFIMA,
GARCH and combinations of these processes. Building prediction bands are
discussed in Section 7.5. Furthermore, these techniques are applied in Sec-
tion 7.6 to the prediction of the S&P500 returns data introduced in Chapter 1.
Bibliographic notes are given in Section 7.7 and a list of problems is pro-
posed at the end of this chapter. Additionally, some technical aspects such as
vector spaces and the projection theorem are reviewed in Appendix A.
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
267

268
PREDICTION
7.1
OPTIMAL PREDICTION
Given a process {yt : t ∈Z}, ﬁnding optimal predictors of yt+h given the ob-
served values yt, yt−1, yt−2, . . . depends crucially on the deﬁnition of optimal-
ity. Suppose that byt+h denotes the h-step predictor of yt+h. Typically, opti-
mality here means that the variance of the prediction error et+h = yt+h−byt+h,
Var(et+h), is minimal. If we are looking for linear predictors, that is,
byt+h =
∞
X
j=1
αjyt−j,
then the predictor that minimizes Var(et+h) is the conditional expectation
byt+h = E(yt+h|yt, yt−1, yt−2, . . . ).
In what follows, Pt denotes the inﬁnite past of the time series up to time t,
yt, yt−1, . . . . On the other hand, Ft denotes the ﬁnite past of the time series
up to time t, yt, yt−1, . . . , y1. Thus, the previous predictor can be written as
byt+h = E(yt+h|Pt).
7.2
ONE-STEP AHEAD PREDICTORS
Let {yt} be an invertible linear process with Wold representation
yt =
∞
X
j=0
ψjεt−j,
(7.1)
and AR(∞) expansion
yt = εt +
∞
X
j=1
πjyt−j,
(7.2)
where Var(εt) = σ2. As described in the next subsections, one-step predictors
of these processes are diﬀerent depending whether we consider inﬁnite or ﬁnite
past.
7.2.1
Inﬁnite Past
The best linear one-step predictor of yt+1 given its past yt, yt−1, . . . is given
by
byt+1 = E[yt+1|Pt] =
∞
X
j=1
πjyt+1−j =
∞
X
j=1
ψjεt+1−j,
with prediction error variance E[yt+1 −byt+1]2 = σ2.

ONE-STEP AHEAD PREDICTORS
269
7.2.2
Finite Past
Notice that in practice we seldom have the full past, instead we only have
a ﬁnite stretch of data {y1, . . . , yt}, say. Under this circumstance, the best
linear predictor of yt+1 based on its ﬁnite past is given by
eyt+1 = E[yt+1|yt, . . . , y1] = φt1yt + · · · + φtty1,
where φt = (φ1t, . . . , φtt)′ is the unique solution of the linear equation
Γtφt = γt,
with Γt = [γ(i−j)]i,j=1,...,t and γt = [γ(1), . . . , γ(t)]′. The calculation of these
coeﬃcients can be carried out using the Durbin-Levinson algorithm described
in Chapter 5. In particular, the prediction error variance of the one-step ﬁnite
sample predictor eyt+1,
νt = E[yt+1 −eyt+1]2,
can be calculated by means of the recursive equations
νt = νt−1(1 −φ2
tt),
for t ≥1, where φtt are called the partial autocorrelation coeﬃcients and
ν0 = γ(0). Thus, νt may be written as
νt = γ(0)
tY
j=1
(1 −φ2
jj).
7.2.3
Innovations Algorithm
Another way to write the ﬁnite past predictors is based on expressing the
current forecast in terms of the innovations of the previous predictors,
byt+1 = θt1(yt −byt) + θt2(yt−1 −byt−1) + · · · + θtt(y1 −by1).
Note that the terms {yt −byt} correspond to the innovations of the forecasting
process, that is, Cov(yt −byt, ys −bys) = 0 for all t.
The coeﬃcients θi,j can be calculated recursively by following the equa-
tions,
β1
=
γ(0)
θt,t−i
=
1
βi

γ(t −1) +
i−1
X
j=1
θi,i−jθt,t−jβj


βt
=
γ(0) −
t−1
X
j=1
θ2
t,t−jβj.

270
PREDICTION
EXAMPLE 7.1
Consider the MA(1) model
yt = εt + θεt−1,
where Var(εt) = σ2. In this case, by deﬁning ρ =
θ
1+θ2 as the ﬁrst-order
autocorrelation of the process we can write









1
ρ
0
0
0
0
0
ρ
1
ρ
0
· · ·
0
0
0
ρ
1
ρ
0
· · ·
0
...
...
0
0
· · ·
0
ρ
1
ρ
0
0
· · ·
0
0
ρ
1


















φt1
φt2
φt3
...
φt,t−1
φtt









=









ρ
0
0
...
0
0









.
Solving this equation system successively yields
φ11 = ρ,
φ21 =
ρ
1 −ρ2 ,
φ22 =
ρ2
1 −ρ2 ,
φ31 = ρ(1 −ρ2)
1 −2ρ2 ,
φ32 = −
ρ2
1 −2ρ2 ,
φ33 =
ρ3
1 −2ρ2 .
On the other hand, a much simpler expression for the one-step predic-
tor of this MA(1) process can be obtained by means of the Innovations
Algorithm. In this case, an application of the method yields,
by1
=
0
byt
=
θt1(yt −byt),
with θt1 = γ(0)
βt
and θti = 0 for i > 1. Notice that
θt1 →θ,
as t →∞.
EXAMPLE 7.2
For a fractional noise FN(d), the partial autocorrelation function is given
by
φtt =
d
t −d,
(7.3)

ONE-STEP AHEAD PREDICTORS
271
for t ≥1. Hence, the prediction error variance is given by
νt
=
γ(0)
tY
j=1
"
1 −

d
j −d
2#
=
γ(0)
Qt
j=1 j Qt
j=1(j −2d)
hQt
j=1(j −d)
i2
.
But, for any real α we have that
tY
j=1
(j −α) = Γ(t + 1 −α)
Γ(1 −α)
.
Hence,
νt = γ(0)Γ(t + 1)Γ(t + 1 −2d)Γ(1 −d)2
[Γ(t + 1 −d)]2 Γ(1 −2d)
.
Therefore, since from (2.25) the autocovariance function at zero lag is
γ(0) = σ2 Γ(1 −2d)
[Γ(1 −d)]2 ,
we obtain the formula
νt = σ2 Γ(t + 1)Γ(t + 1 −2d)
[Γ(t + 1 −d)]2
,
for t ≥1. Now, an application of expression (2.32) yields
lim
t→∞νt = σ2.
Figure 7.1 displays the evolution of the partial autocorrelation coef-
ﬁcients φtt for t = 1, . . . , 20 for a fractional noise FN(d) process with
d = 0.4, an AR(1) model with φ1 = 0.2 and φ2 = 0.4, and a MA(1)
process with θ1 = 0.4 and θ2 = −0.2.
Notice that for the fractional noise processes, φtt = O(1/t) for large t.
Despite the diﬃculty of ﬁnding explicit expressions for the partial autocorre-
lations for a general class of long-memory models, this rate of convergence to
zero can be extended to any ARFIMA(p, d, q) process, as stated in the follow-
ing result: Let {yt} be an ARFIMA(p, d, q) process with 0 < d < 1
2. Then,
the partial autocorrelations φtt satisfy
|φtt| ∼d
t ,
(7.4)

272
PREDICTION
as t →∞.
Observe that the rate on t given by expression (7.4) does not depend on
the value of the long-memory parameter d.
Figure 7.2 displays the evolution of the mean-squared prediction error νt
for a fractional noise FN(d) with d = 0.10, d = 0.30, d = 0.49, σ2 = 1, and
t = 1, . . . , 40.
As t increases, the eﬀect of the remote past fades out and yt+1 becomes
similar to yt+1. In turn, the prediction error variance of the ﬁnite sample
predictor νt becomes similar to the prediction error variance of the inﬁnite
past predictor, σ2: If {yt} is a stationary process, then ∥yt+1 −yt+1∥→0 and
νt →σ2 as t →∞.
Let δt = ∥yt+1 −yt+1∥2 be the squared distance between the optimal pre-
dictor based on the full past and the best predictor based on the ﬁnite past.
Then, we may write
∥yt+1 −yt+1∥2
=
∥(yt+1 −yt+1) + (yt+1 −yt+1)∥2
=
∥yt+1 −yt+1∥2 + ∥yt+1 −yt+1∥2
−2⟨yt+1 −yt+1, yt+1 −yt+1⟩
=
σ2 + νt −2⟨εt+1, yt+1 −yt+1⟩.
5
10
15
20
-0.2
0.0
0.2
0.4
Lag
Partial ACF
FN
AR
MA
Figure 7.1
Evolution of the partial autocorrelation coeﬃcients φtt for t = 1, . . . , 20
for a fractional noise FN(d) process with d = 0.4 (heavy line), an AR(1) model with
φ1 = 0.2 and φ2 = 0.4 (broken line), and a MA(1) process with θ1 = 0.4 and θ2 = −0.2
(dotted line).

ONE-STEP AHEAD PREDICTORS
273
0
10
20
30
40
1.00
1.05
1.10
1.15
1.20
1.25
j
!t
d=0.49
d=0.30
d=0.10
Figure 7.2
Evolution of the mean-squared prediction error νt for t = 1, . . . , 40 for
a fractional noise FN(d) process with d = 0.10, d = 0.30, d = 0.49, and σ2 = 1.
Since ⟨εt+1, yt+1⟩= σ2 and ⟨εt+1, yt+1⟩= 0, we have
δt = νt −σ2.
A precise rate at which νt converges to σ2 as t increases is as follows: Let
{y} be an ARFIMA(p, d, q) process with unit variance noise and 0 < d < 1
2.
Then,
δt ∼d2
t ,
(7.5)
as t →∞.
7.2.4
An Approximate Predictor
Since the Durbin-Levinson algorithm for calculating the coeﬃcients φtj is
order O(n2), for very large sample sizes it could be desirable a faster algorithm
to obtain ﬁnite sample forecasts. One way to do this is approximating the
regression coeﬃcients φtj by πj, based on the following fact: If {yt} is a
stationary process, then φtj →πj as t →∞.
With this approximation, we introduce the ﬁnite sample predictor
ˇyt+1 =
t

j=1
πjyt+1−j,

274
PREDICTION
with prediction error variance
Var(yt+1 −ˇyt+1) = σ2 + rt,
where rt = Var(P∞
j=t+1 πjyt+1−j) or equivalently
rt = Var(byt+1 −ˇyt+1).
As expected, the prediction error variance of the approximate forecast is larger
than the prediction error variance of eyt+1.
However, as t increases, these
two predictors become similar: If {yt} is a stationary process with AR(∞)
representation satisfying P∞
j=1 |πj| < ∞, then rt →0 as t →∞.
For short-memory processes with autoregressive coeﬃcients {πj} satisfying
|πj| ∼c|φ|j,
for large j and positive constant c we have that
rt ≤c1|φ|2t,
where c1 = Var(y0)[c|φ|/(1 −|φ|)]2. Therefore, rt converges to zero at an
exponential rate.
However, for long-memory processes this rate is slower: If {yt} is a station-
ary and invertible process with AR(∞) and MA(∞) satisfying
πj
∼
j−d−1
ℓ(j)Γ(−d),
ψj
∼
jd−1ℓ(j)
Γ(d)
,
as j →∞, with 0 < d < 1
2 and ℓ(·) is a slowly varying function, then
rt ∼d tan(πd)
πt
,
(7.6)
as t →∞.
Comparing expressions (7.5) and (7.6), we observe that for small values of
d both terms δt and rt behave similarly since
d tan(πd)
πt
∼d2
t ,
as d →0.
On the contrary, when d approaches
1
2, δt is bounded but rt
increases to inﬁnity since tan(π/2) = ∞.

MULTISTEP AHEAD PREDICTORS
275
7.3
MULTISTEP AHEAD PREDICTORS
7.3.1
Inﬁnite Past
Let byt(h) be the best linear predictor of yt+h based on the inﬁnite past Ft for
h ≥1, which may be written as
byt(h) = E[yt+h|Ft] =
∞
X
j=0
πj(h)yt−j =
∞
X
j=0
ψj+hεt−j =
∞
X
j=h
ψjεt+h−j,
(7.7)
where the coeﬃcients πj(h) for j ≥1 are given by
πj(h) =
h−1
X
i=0
ψiπj+h−i.
The prediction error variance of byt(h), σ2(h) = E[yt+h −byt(h)]2, is
σ2(h) = σ2
h−1
X
j=0
ψ2
j .
(7.8)
7.3.2
Finite Past
The best linear predictor of yt+h based on the ﬁnite past Pt is
eyt(h) = φt1(h)yt + · · · + φtt(h)y1,
where φt(h) = [φt1(h), . . . , φtt(h)]′ satisﬁes
Γtφt(h) = γt(h),
with γt(h) = [γ(h), . . . , γ(t + h −1)]′. Besides, the mean-squared prediction
error is deﬁned by
σ2
t (h) = ∥yt+h −eyt(h)∥2.
Analogously to the one-step prediction case, we can use the approximate
ﬁnite sample h-step ahead forecasts given by
ˇyt(h) =
t
X
j=0
πj(h)yt−j,
with prediction error variance
Var[yt+h −ˇyt(h)] = σ2 + rt(h),

276
PREDICTION
where rt(h) = Var[P∞
j=t+1 πj(h)yt−j]. For ﬁxed h, this term behaves similarly
to rt, excepting a constant: Let {yt} be a stationary and invertible process
with AR(∞) and MA(∞) satisfying
πj
∼
j−d−1
ℓ(j)Γ(−d),
ψj
∼
jd−1ℓ(j)
Γ(d)
,
as j →∞, with 0 < d < 1
2 and ℓ(·) is a slowly varying function. If Ph−1
j=0 ψj ̸=
0, then
rt(h) ∼


h−1
X
j=0
ψj


2
d tan(πd)
πt
,
as t →∞.
7.4
HETEROSKEDASTIC MODELS
Consider a general heteroskedastic process {yt : t ∈Z} speciﬁed by the equa-
tions
yt
=
∞
X
j=0
ψjεt−j,
(7.9)
εt
=
ϵtσt,
(7.10)
σ2
t
=
f(ϵt−1, ϵt−2, . . . , ηt, ηt−1, . . . ),
(7.11)
where f is function {ϵt} and {ηt}, which are sequences of independent and
identically distributed random variables with zero-mean and unit variance and
{ϵt} is independent of {ηt}.
Observe that this speciﬁcation includes the ARFIMA-GARCH, the ARCH-
type and the LMSV processes, among others, as shown in the following ex-
amples.
EXAMPLE 7.3
The conditional variance of the ARFIMA-GARCH(1, 1) model may be
written as in (6.9),
f(ϵt−1, ϵt−2, . . . , ηt, ηt−1, . . . ) = α0

1 +
∞
X
k=0
k+1
Y
j=1
(α1ϵ2
t−j + β1)

,
and ηt = 0 for all t.

HETEROSKEDASTIC MODELS
277
EXAMPLE 7.4
The ARCH-type process is also included in speciﬁcation (7.9)–(7.11)
with the Volterra expansion
f(ϵt−1, ϵt−2, . . . ) = α0
∞
X
k=0
∞
X
j1,...,jk=1
αj1αj2 · · · αjkϵ2
t−j1ϵ2
t−j2 · · · ϵ2
t−jk,
and ηt = 0 for all t, cf., expression (6.14).
EXAMPLE 7.5
For the LMSV model introduced in Section 6.9, the conditional variance
is given by
f(ηt, ηt−1, . . . ) = σ2 exp

φ(B)−1(1 −B)−dθ(B)ηt
	
.
Notice that in this case, the conditional variance does not depend di-
rectly on the sequence {ϵt} but it depends on random perturbations {ηt}
such as those appearing in (6.21).
EXAMPLE 7.6
The conditional variance for the FIEGARCH model may be written as
f(ϵt−1, ϵt−2, . . . )
= exp

φ(B)−1(1 −B)−d[θ(B)(|ϵt−1| −E |ϵt−1|) + λ(B)ϵt−1]
	
;
see Problem 6.18.
The following fact is fundamental for the formulation of prediction of the
volatility techniques for the heteroskedastic models described by (7.9)–(7.11).
For all t ∈Z we have
E[ε2
t+h|Pt] =
 E[σ2
t+h|Pt]
h ≥1,
ε2
t+h
h < 1.
(7.12)
This can be veriﬁed as follows. First, notice that for h < 1 the result is triv-
ial. For h ≥1 observe that by the deﬁnition of εt+h we may write E[ε2
t+h|Pt] =
E[ϵ2
t+hσ2
t+h|Pt].
But, since σ2
t+h = f(ϵt+h−1, ϵt+h−2, . . . , ηt+h, ηt+h−1, . . . ),
σ2
t+h and ϵt+h are independent. Furthermore, ϵt+h is independent of yt, yt−1, . . .
for any h ≥1. Thus,
E[ε2
t+h|Pt] = E[ϵ2
t+h] E[σ2
t+h|Pt].
Finally, by noting that E[ϵ2
t+h] = 1, the result is obtained.

278
PREDICTION
7.4.1
Prediction of Returns
In the context of the heteroskedastic model (7.9), the forecasts of the returns
can be obtained similarly to the cases discussed in Section 6.1. On the other
hand, estimation the prediction error conditional variances is addressed in the
next subsection. Practical illustrations of these techniques are provided later
in this chapter.
7.4.2
Prediction of Volatility
Observe that from (7.7) the multistep ahead prediction error for h ≥1 is given
by
et(h) = yt+h −byt(h) = εt+h + ψ1εt+h−1 + · · · + ψh−1εt+1.
Therefore, the mean-squared prediction error is
E[e2
t(h)|Pt]
=
E[ε2
t+h|Pt] + ψ2
1 E[ε2
t+h−1|Pt] +
· · · + ψ2
h−1 E[ε2
t+1|Pt].
Thus, we may write
E[e2
t(h)|Pt]
=
E[σ2
t+h|Pt] + ψ2
1 E[σ2
t+h−1|Pt] +
· · · + ψ2
h−1 E[σ2
t+1|Pt].
Let σ2
t (h) = E[σ2
t+h|Pt], then the h-step prediction error conditional variance
is
E[e2
t(h)|Pt] =
h−1
X
j=0
ψ2
j σ2
t (h −j).
The calculation of the conditional variances σ2
t (j) depends on the speciﬁcation
of the heteroskedastic model. Some speciﬁc examples are discussed next.
EXAMPLE 7.7
Consider an ARCH(1) process and assume that we know the returns
{yn, yn−1, · · · · · · } and we have the equations σ2
n = α0 + α1 y2
n−1 and
σ2
n+1 = α0 + α1 y2
n. In order to forecast the conditional variance σ2
n+2
we can write:
ˆσ2
n+2 = E[σ2
n+2|Pn].
Thus,
ˆσ2
n+2 = E[α0 + α1 y2
n+1|Pn] = α0 + α1 E(y2
n+1|Pn)
Since {y2
t } corresponds to an AR(1) process, we can write
y2
n+1 = α0 + α1 y2
n + νn+1.

HETEROSKEDASTIC MODELS
279
Now, by taking expectation we have
E(y2
n+1 |Fn)
=
α0 + α1 E(y2
n |Fn) + E(νn+1 |Fn)
=
α0 + α1 y2
n.
On the other hand,
bσ2
n+2
=
α0 + α1 [α0 + α1 y2
n] = α0 + α1 α0 + α2
1 y2
n.
Now, for predicting σ2
n+3 we can write,
bσ2
n+3
=
E[α0 + α1 y2
n+2|Fn] = α0 + α1 E(y2
n+2|Fn)
But,
y2
n+2
=
α0 + α1 y2
n+1 + νn+2
so that by taking expectation on both sides
E(y2
n+2 |Fn)
=
α0 + α1 E(y2
n+1 |Fn) + E(νn+2 |Fn)
and then
E(y2
n+2 |Fn)
=
α0 + α1(α0 + α1 y2
n) + 0
=
α0 + α1 α0 + α2
1 y2
n
Thus,
ˆσ2
n+3
=
α0 + α1 α0 + α2
1 α0 + α3
1 y2
n
=
α0 (1 + α1 + α2
1) + α3
1 y2
n.
More generally, the h-step predictor is given by
bσ2
n+h = α0
"h−1
X
i=0
αi
1
#
+ αh
1 y2
n
(7.13)
Notice that the behavior of the predictor (7.13) as the forecasting horizon
increases h →∞is as follows
bσ2
n+h →p α0
" ∞
X
i=0
αi
1
#
=
α0
1 −α1
= Var(yt).

280
PREDICTION
EXAMPLE 7.8
The previous results established for ARCH(1) processes can be readily
extended to the ARCH(p) model. In this case we have
ˆσ2
n(1)
=
α0 + α1 y2
n + · · · + αp y2
n+1−p
ˆσ2
n(2)
=
α0 + α1 ˆσ2
n(1) + α2 y2
n · · · + αp y2
n+2−p,
and more generally,
bσ2
n(ℓ) = α0 +
p
X
i=1
αi bσ2
n(ℓ−i)
(7.14)
where bσ2
n(ℓ−i) = y2
n+ℓ−i para ℓ−i ≤0.
EXAMPLE 7.9
For the ARFIMA-GARCH(1, 1) we have
σ2
t+h = α0 + α1ε2
t+h−1 + β1σ2
t+h−1.
Therefore, an application of (7.12) yields σ2
t (1) = σ2
t+1 and for h ≥2,
σ2
t (h) = α0 + (α1 + β1)σ2
t (h −1).
Solving this recursive equation we ﬁnd the following solution for h ≥1:
σ2
t (h) = α0
1 −(α1 + β1)h−1
1 −(α1 + β1)
+ (α1 + β1)h−1σ2
t+1.
Since 0 ≤α1 + β1 < 1, we have that
lim
h→∞σ2
t (h) =
α0
1 −(α1 + β1),
where the term on the left hand of this equation corresponds to the
variance of {et}.
EXAMPLE 7.10
For the general ARFIMA-GARCH(r, s) it is not hard to check that for
h > max{r, s} we have
σ2
t (h) = α0 +
r
X
j=1
αjσ2
t (h −j) +
s
X
j=1
βjσ2
t (h −j),
and since 0 < Pr
j=1 αj + Ps
j=1 βj < 1,
lim
h→∞σ2
t (h) =
α0
1 −(Pr
j=1 αj + Ps
j=1 βj) = Var(εt).

PREDICTION BANDS
281
EXAMPLE 7.11
For the ARCH(∞) model, we have
σ2
t+h = α0 +
∞
X
j=1
αjy2
t+h−j = α0 +
h−1
X
j=1
αjy2
t+h−j +
∞
X
j=h
αjy2
t+h−j.
Thus,
E[σ2
t+h|Ft] = α0 +
h−1
X
j=1
αj E[y2
t+h−j|Ft] +
∞
X
j=h
αjy2
t+h−j,
and then,
σ2
t (h) = α0 +
h−1
X
j=1
αjσ2
t (h −j) +
∞
X
j=h
αjy2
t+h−j.
Now, if 0 < P∞
j=1 αj < 1, then
lim
h→∞σ2
t (h) =
α0
1 −P∞
j=1 αj
= Var(yt).
7.5
PREDICTION BANDS
Given the forecasts byt+h and the estimated prediction error variances bσ2
t+h,
we can build the approximate prediction bands
[ byt+h −zαbσt+h,
byt+h + zαbσt+h ]
where zα corresponds to the 100 (1−α) percentile. For example, under Gaus-
sianity, for the 90% prediction bands we can use zα = 1.65 while for 95%
prediction bands this value is zα = 1.96.
EXAMPLE 7.12
Figure 7.3 displays a simulated Gaussian ARFIMA(0, d, 0) process with
zero-mean and unit variance white noise, long-memory parameter d =
0.40 and sample size n = 1000. The last 100 observations (dotted line)
will be predicted using the estimated model.
The MLE of d calculated from the ﬁrst 900 observations is bd = 0.4096
with standard deviation bσd = 0.0251.
Besides, the estimated stan-
dard deviation of the white noise is bσ = 0.9735.
Figure 7.4 shows
the observations from t = 800 to t = 1000, along with by900(h) fore-
casts with h = 1, 2, . . . , 100 and 95% prediction bands. These multi-
step ahead predictors are based on the ﬁtted model and on observations

282
PREDICTION
t = 1, 2, . . . , 900. From Figure 7.4, notice that most of the future obser-
vations fall inside the prediction bands.
Figure 7.5 displays the theoretical and the empirical evolution of the
prediction error standard deviation, from t = 901 to t = 1000. The
theoretical prediction error standard deviation is based on the multistep
prediction error variance formula (7.8) which yields
σ(h) = σ




h−1

j=0
ψ2
j ( d),
(7.15)
for h = 1, 2, . . . , 100. Equation (7.15) provides only an approximation
of the theoretical prediction error standard deviation at step h since
we are dealing with a ﬁnite past of 900 observations.
On the other
hand, the empirical prediction error standard deviations are based on
the Kalman ﬁlter output from a truncated state space representation
with m = 50. Notice from this graph that the sample prediction error
standard deviations are very close to their theoretical counterparts.
Time
Series
0
200
400
600
800
1000
-4
-2
0
2
Figure 7.3
Simulated fractional noise process FN(d), with d = 0.40 and unit
variance white noise. The dotted line indicates the last 100 observations that will be
predicted using the estimated model.

PREDICTION BANDS
283
Time
Series
0
50
100
150
200
-4
-2
0
2
Figure 7.4
Simulated fractional noise process FN(d): Multistep forecasts of the
last 100 values and 95% prediction bands.
900
920
940
960
980
1000
1.00
1.05
1.10
1.15
1.20
1.25
1.30
Time
Prediction Error SD
Theoretical
Empirical
Figure 7.5
Simulated fractional noise process FN(d): Theoretical and empirical
prediction error standard deviations of the last 100 observations.

284
PREDICTION
EXAMPLE 7.13
We now illustrate the prediction of a short-memory process. Figure 7.6
shows a simulated trajectory of 1000 observations from an ARMA(1,1)
process with parameters φ = 0.80 and θ = −0.40.
In order to illustrate the application of the forecasting methods dis-
cussed in this chapter, the series has been divided into two parts, the ﬁrst
900 observations to ﬁt the model and the remaining 100 observations for
prediction.
The ﬁtted model based on the 900 observations is reported below and
the diagnostic plots are exhibited Figure 7.7, including the residuals, the
ACF of residual and the Box-Ljung tests.
According to this set of graphs and tests, the ﬁtting of this ARMA(1, 1)
model seems adequate.
Coefficients:
ar1
ma1
intercept
0.8071
-0.3818
0.2215
s.e.
0.0329
0.0527
0.1085
Time
Series
0
200
400
600
800
1000
-4
-2
0
2
4
Figure 7.6
Example of prediction of an ARMA process. Simulated ARMA(1, 1)
time series with φ = 0.8 and θ = −0.4.

PREDICTION BANDS
285
Standardized  Residuals
Time
0
200
400
600
800
1000
-3
-1
1
3
0
2
4
6
8
10
0.0
0.4
0.8
Lag
ACF
ACF of Residuals
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
p values for Ljung-Box statistic
Lag
p-value
Figure 7.7
Example of prediction of an ARMA process. Fitted model diagnostics.
sigma^2 estimated as 1.04:
log likelihood = -1294.96,
aic = 2597.93
Figure 7.8 displays the series along the out-of-sample predictions and
95% prediction bands for observations from t = 901 to t = 1000. The
predictions bands in this case are given by ±2√νt.
Note that in this case the predictors converge to the mean of the
process (µ = 0) and that the prediction bands converge very fast to the
corresponding to ±2

Var(yt).
On the other hand, Figure 7.9 shows the last 100 values of the series
along their corresponding forecasts and prediction bands. From this plot,
observe that, as expected, most of the values lie inside of the prediction
bands.

286
PREDICTION
Time
Series
0
200
400
600
800
1000
1200
-4
-2
0
2
4
Figure 7.8
Example of prediction of an ARMA process. Time series, forecasts and
predictions bands.
Time
Series
900
920
940
960
980
1000
-4
-2
0
2
4
Figure 7.9
Example of prediction of an ARMA process. Last 100 observations,
forecasts and predictions bands.

DATA APPLICATION
287
7.6
DATA APPLICATION
This section illustrates the application of prediction methods discussed in
this chapter to a real-life time series.
We consider the SP500 stock index
introduced in Chapter 1.
Recall that an AR(2)-GARCH(1, 1) model was
ﬁtted to these data in Section 6.11.
Figure 7.10 shows the SP500 returns while Figure 7.11 displays the volatil-
ity estimates for these data. Observe that the volatility estimates behave sim-
ilarly to the variability of the returns. As expected, periods of high volatility
produce large estimates while periods of low volatility generate small esti-
mates. Figure 7.12 exhibits the corresponding estimates of the conditional
standard deviations σt.
On the other hand, Figure 7.13 shows 100 out-of-sample forecasts of the
SP500 returns along with 95% prediction bands.
In order to provide a comparison framework, we have also plotted 200
returns corresponding to the period t = 15, 905 to t = 16, 104. Notice that
the predictors converge rapidly to the mean of the series, zero, and that the
prediction bands increases as the forecasting horizon increases.
Time
SP500 returns series
1950
1960
1970
1980
1990
2000
2010
-0.20
-0.15
-0.10
-0.05
0.00
0.05
0.10
Figure 7.10
SP500 Returns data.

288
PREDICTION
Time
!t
2
1950
1960
1970
1980
1990
2000
2010
0.000
0.001
0.002
0.003
0.004
Figure 7.11
SP500 Returns data. Volatility estimates.
Time
!t
1950
1960
1970
1980
1990
2000
2010
0.01
0.02
0.03
0.04
0.05
0.06
0.07
Figure 7.12
SP500 Returns data. Estimates of σt.

BIBLIOGRAPHIC NOTES
289
Time
Series and Forecasts
2014.0
2014.2
2014.4
2014.6
2014.8
2015.0
2015.2
-0.02
-0.01
0.00
0.01
0.02
Figure 7.13
SP500 Returns data.
Out-of-sample forecasts and prediction 95%
bands.
7.7
BIBLIOGRAPHIC NOTES
The literature about prediction of stationary processes is extensive and spans
several decades since the pioneering works on linear predictors by Kolmogorov,
Wiener, and Wold, among others. The monographs by Rozanov (1967), Han-
nan (1970), and Pourahmadi (2001) oﬀer excellent overviews of the theoretical
problems involved in the prediction of linear processes. For a review of pre-
diction methods in the context of long-memory processes; see, for example,
Bhansali and Kokoszka (2003).
Optimal adaptive prediction with long-range-dependent models has been
analyzed, for instance, by Ray (1993b), Tiao and Tsay (1994) and Basak,
Chan, and Palma (2001).
Problems
7.1
Let {yt : t ∈Z} be an AR(2) process deﬁned by
yt = 0.2yt−1 + 0.6yt−2 + εt,
where {εt} is a white noise sequence with zero-mean and variance σ2 = 2.
(a) Find the best linear predictor of yn+2 based on the inﬁnite past
yn, yn−1, . . . .

290
PREDICTION
(b) Calculate the best linear predictor of yn+2 based on the ﬁnite past
yn, yn−1, . . . , y1.
(c) Calculate the squared prediction error of two the previous forecast.
7.2
Show for the fractional noise process FN(d) we have
φtj ≈πj,
for all ﬁnite j as t →∞, where
φtj = −
n
j
Γ(j −d)Γ(t −d −j + 1)
Γ(−d)Γ(t −d + 1)
,
and
πj =
Γ(j −d)
Γ(j + 1)Γ(−d).
7.3
Consider the following process {yt} with expected value µ for all time
t, deﬁned by the equation
yt = α + φ yt−1 + zt + (θ + η) zt−1 + θ η zt−2,
with α ∈R, |φ| < 1, |θ| < 1, |η| < 1 y {zt} ∼WN(0, σ2).
(a) Find an expression µ in terms of the coeﬃcients of the process.
(b) Show this process can be written as
(1 −φ B) [yt −µ] =

1 + (θ + η) B + θ η B2
zt
(c) Since |θ| < 1 and |η| < 1, then the process {yt −µ} is causal. Show
that the coeﬃcients ψk of its MA(∞) representation are
ψ0 = 1,
ψ1 = φ + (θ + η),
ψk = φk−2 
φ2 + (θ + η) φ + θ η

for k ≥2.
(d) Given that |φ| < 1, the the process {yt −µ} is invertible. Show that
the coeﬃcients πk of its AR(∞) representation are
π0 = 1,
π1 = +(φ+θ+η),
πk = −(−θ)k+(η+φ)
k−1
X
l=0
(−θ)l (−η)k−1−l
for k ≥2
(e) Find the autocovariance function of the process {yt −µ}.
(f) Determine the variance of prediction error h steps, σ2
t (h), under the
assumption that all the past information is available up to time t.
7.4
If yt = zt −θzt−1, where|θ| < 1 and zt is a white noise sequence with
variance σ2, verify that the best linear predictor of yt+1 based on yj, j ≤t is
given by
ˆyt+1 = −
∞
X
j=1
θjyt+1−j,

PROBLEMS
291
and ﬁnd its mean square error.
7.5
Consider the following formula for the coeﬃcients of the best linear
predictor from the Durbin-Levinson algorithm:
φtj = φt−1,j −φttφt−1,t−j,
for j = 1, . . . , n. Let αt = 1 −Pt
j=1 φtj.
(a) Show that αt satisﬁes the following recursive equation:
αt = (1 −φtt)αt−1.
(b) Verify that a solution to the above equation is
αt =
tY
j=1
(1 −φjj),
so that Pt
j=1 φtj = 1 −Qt
j=1(1 −φjj).
7.6
Consider a stationary process {xt} given
xt = zt + θ zt−s,
with {zt} ∼(0, σ2) y s ∈N. Determine the coeﬃcients θn,j of the one-step
predictor of xn+1 given by
ˆxn+1 =
n
X
j=1
θn,j(xn+1−j −ˆxn+1−j),
n = 1, 2, 3, . . .
assuming that ˆX1 = 0.
7.7
Let xt be an ARMA(p, q) process. Show that σ2 is the one-step predic-
tion error variance and that γ(0) is the inﬁnite steps ahead prediction error
variance. Furthermore, verify that γ(0) ≥σ2.
7.8
A quarterly economic time series was modeled by ∇zt = 0.5 + (1 −B +
0.5B2)at with σ2
a = 0.04.
(a) Given z48 = 130 , a47 = −0.3 , a48 = 0.2 , calculate and draw the
predictions ˆZ48(l) for l = 1, 2, . . . , 12
(b) Include 80% prediction bands in the graph.
7.9
Consider the MA(1) process where {εt} is WN(0, σ2) y |θ| < 1,
yt = εt −θ εt−1.
(a) Suppose that we want to ﬁnd the best lineal predictor (BLP) of y0
based on the inﬁnite past {y−1, y−2, y−3, . . . }. Verify that in this
case the BLP of y0 is given by
by0 = −
∞
X
j=1
θjy−j,

292
PREDICTION
and that the prediction error variance is σ2.
(b) Assume now that we only have the pst observations {y−1, y−2} and
we write the BLP of y0 as
by0 = φ1y−1 + φ2y−2.
We know that in this case the vector φ = (φ1, φ2)′ satisﬁes
Γ φ = γ,
where Γ is the variance covariance matrix of y = (y−1, y−2)′ y γ =
[γ(1), γ(2)]′. Verify that
φ =
ρ
1 −ρ2 (1, −ρ),
where ρ = γ(1)
γ(0)
7.10
Consider fractional noise a fractional noise process FN(d) where the
coeﬃcients of the best linear predictor are given by
φtj = −
n
j
Γ(j −d)Γ(t −d −j + 1)
Γ(−d)Γ(t −d + 1)
,
and the coeﬃcients of the inﬁnite AR expansion are
πj =
Γ(j −d)
Γ(j + 1)Γ(−d).
Show that
φtj →πj,
as t →∞.
7.11
Consider the AR(p) model
yt + φ1yt−1 −· · · −φpyt−p = εt,
where {εt} is a white noise sequence with variance σ2. Verify that νt converges
to σ2 after p steps.
7.12
Find an expression for σ2
t (h), h ≥1, for the ARFIMA-GARCH(r, s)
model.
7.13
Show that for the ARFIMA-GARCH(1, 1) model, the conditional vari-
ance at time t + 1 may be written as
σ2
t+1 =
α0
1 −β −1 + α1
∞
X
j=0
βj
1ε2
t−j.

PROBLEMS
293
7.14
Verify the following result for the random variables x, y, z.
If x is
independent of y and z, then
E(xy|z) = E(x) E(y|z).
7.15
Show that for the fractional noise process, FN(d), we have
δt ∼d2
t ,
as t →∞.
7.16
Consider the h-step forecast for an ARCH(1) given by
bσ2
n+h = α0
"h−1
X
i=0
αi
1
#
+ αh
1 y2
n,
cf. (7.13). Prove that bσ2
n+h increases with h.
7.17
Verify that the following expressions for the predictors of an ARCH(p)
model hold:
ˆσ2
n(1)
=
α0 + α1 y2
n + · · · · · · + αp y2
n+1−p
ˆσ2
n(2)
=
α0 + α1 bσ2
n(1) + α2 y2
n · · · + αp y2
n+2−p.
Furthermore, show that for the general case we have,
bσ2
n(ℓ) = α0 +
p
X
i=1
αi bσ2
n(ℓ−i),
with bσ2
n(ℓ) = y2
n+ℓfor ℓ≤0.
7.18
Consider the Hilbert space L2 and a subspace M ⊆L2 . Show that
the orthogonal projection of y ∈L2 onto M is given by the conditional ex-
pectation
by = E(y|M).
7.19
Consider the Hilbert space H = sp{et : t = 0, 1, 2, . . . } where {et} is
an orthonormal basis, that is, ⟨et, es⟩= 0 for all t ̸= s and ⟨et, et⟩= 1 for all
t.
(a) Let x ∈H, verify that this element may be written as
x =
∞
X
t=0
⟨x, et⟩et.
(b) Show that ∥x∥2 = P∞
t=0⟨x, et⟩2.

294
PREDICTION
(c) Let M = sp{e1, . . . , eN} and let bx be the orthogonal projection of x
on M. Show that bx = PN
t=0⟨x, et⟩et.
(d) Verify that ∥x −bx∥2 = P∞
t=N+1⟨x, et⟩2.
7.20
Let {yt : t ∈N} be a sequence in a Hilbert space H such that
P∞
t=1 ∥yt∥< ∞. Show that P∞
t=1 yt converges in H.
7.21
Let H be a Hilbert space and suppose that x, y ∈H are orthogonal
vectors such that ∥x∥= ∥y∥= 1. Show that ∥αx + (1 −α)y∥< 1 for all
α ∈(0, 1). From this, what can you say about the set {y ∈H : ∥y∥≤1}?
7.22
(Parallelogram law) Show that if H is an inner product space then
∥x + y∥2 + ∥x −y∥2 = 2∥x∥2 + 2∥y∥2,
for all x, y ∈H.
7.23
Consider the following stochastic volatility process {rt} deﬁned by
rt
=
εtσt,
σt
=
σ exp(νt/2),
where {εt} is an independent and identically distributed sequence with zero-
mean and unit variance, and {νt} is a linear process:
νt =
∞
X
j=0
ψjηt−j,
with P∞
j=0 ψ2
j < ∞and {ηt} is an independent and identically distributed
sequence with zero-mean and unit variance, independent of the sequence {εt}.
(a) Show that the process rt is stationary.
(b) Find the best linear predictor of yt+1 given the full past of the series.

CHAPTER 8
NONSTATIONARY PROCESSES
As discussed in Chapter 1, most real-life time series display a number of non-
stationary features, including trends, seasonal behavior, explosive variances,
trend breaks, among others. This chapter provides an overview of some of
these problems and the time series analysis techniques developed to deal with
them. In particular, we discuss the concepts of deterministic and stochastic
trends as well as unit root procedures to test for a nonstationary explosive
behavior. Autoregressive integrated moving-average processes are also brieﬂy
reviewed. These are very well-known models for dealing with integrated time
series. On the other hand, techniques for modeling time-varying parameters
are also discussed, focusing on the so-called locally stationary processes. In
this case, the time series model is assumed to evolve very smoothly so that it
can be locally approximated by stationary processes. This chapter also cov-
ers methodologies for handling abrupt structural changes as well as several
examples and data applications.
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
295

296
NONSTATIONARY PROCESSES
8.1
INTRODUCTION
On of the most common features in time series data is the presence of in-
creasing or decreasing trends along with a number trends breaks. A basic
question that arises is whether these trends are the result of a deterministic
underlying pattern or it corresponds to the accumulation of random shocks
over time. Of course, the observed time series may be the result of combi-
nations of these two or other more complex data generation mechanisms. In
what follows, we discuss two well known approaches to understand and model
trends: deterministic and stochastic methodologies.
8.1.1
Deterministic Trends
Under the deterministic approach, the observed process is the result of a
usually unknown underlying pattern f(t) plus a noise εt,
yt = f(t) + εt.
In order to estimate the trend, the function f(t) can be written in terms of
some parameter vector β. For instance, we may write
f(t) = β0 + β1xt1 + β2xt2 + · · · + βpxtp,
where xt1, xt2, . . . , xtp are deterministic covariates. In particular, by setting
xtj = tj we can generate a polynomial trend or polynomial regression. On the
other hand, if xtj = exp(˙ıj) the resulting model corresponds to an harmonic
regression.
Another way of specifying the function f(t) is through local polynomials, so
that the trend is ﬂexible enough to capture short term movements in the data.
In this nonparametric approach, the trend of process yt is locally estimated
as
byt =
m
X
j=−m
wj yt+j,
for t = m + 1, . . . , N −m. The optimal weights wj are usually obtained by
ﬁtting cubic polynomials to the series yt.
8.1.2
Stochastic Trends
If the underlying trend is assumed to be stochastic, the observed process yt
is usually understood as the result of a sequence of random shocks,
yt = ε1 + ε2 + · · · + εt,
where εt is a white noise or a sequence of independent random variables. Note
that in this case, the diﬀerenced series satisﬁes
yt −yt−1 = (1 −B)yt = ∆εt.

UNIT ROOT TESTING
297
Thus, an stochastic trend process is more generally speciﬁed by
∆dyt = εt,
where d is a known diﬀerentiation order and εt is assumed to be a stationary
process. Typically, a process satisfying this equation is referred to as integrated
process of order d, I(d).
Assume that εt is a sequence of i.i.d. random variables with zero-mean
and variance σ2. The variance of an I(d) process is Var(yt) = t σ2. Thus, an
integrated process possesses an explosive variability as t tends to inﬁnity.
In contrast, under the same conditions, the variance of a process with
deterministic trend is Var(yt) = σ. Naturally, in this case the variability is
not explosive.
In this sense, it is relevant to test whether the process possess a unit root,
that is, it corresponds to an integrated process. This issue is discussed next.
8.2
UNIT ROOT TESTING
Consider the model
(1 −φB)yt = εt,
where φ is an autoregressive parameter and εt is a zero-mean stationary se-
quence. As studied in previous chapters, if |φ| < 1 then the process yt can be
expanded as
yt = (1 −φB)−1εt.
Thus, the process yt is stationary. However, if φ = 1 then yt corresponds to a
I(d) process with explosive variance. Therefore, the unit root hypothesis can
be formally deﬁned by H0 : φ = 1. A well known procedure for testing H0
against the alternative hypothesis H1 : φ < 1 is the Dickey-Fuller statistic
which is based on the least squares estimates
bφ
=
Pn
t=1 yt yt−1
Pn
t=1 y2
t−1
bσ2
=
Pn
t=1(yt −bφ yt−1)2
n −1
Naturally, evidence in favor of the unit root hypothesis comes from an esti-
mated value of φ close to one. Following this idea, the Dickey Fuller t-statistics
is given by
DF =
bφ −1
σbφ
=
Pn
t=1 εt yt−1
bσ
qPn
t=1 y2
t−1
.

298
NONSTATIONARY PROCESSES
EXAMPLE 8.1
Consider the logarithm transformation of the SP500 series introduced
in Chapter 1. An application of the Dickey-Fuller test to this series of
log returns we get,
Augmented Dickey-Fuller Test
data:
z
Dickey-Fuller = -2.1531, Lag order = 25, p-value = 0.5135
alternative hypothesis: stationary.
Thus, according to this result, we cannot reject the unit root hypoth-
esis for this series of log returns of the SP500 stock index at the 5%
signiﬁcance level.
EXAMPLE 8.2
When the Dickey-Fuller test is applied to the log returns of the IPSA
stock index we obtain
Augmented Dickey-Fuller Test
data:
y
Dickey-Fuller = -1.6347, Lag order = 14, p-value = 0.733
alternative hypothesis: stationary,
so that based on this output, we cannot reject the unit root hypothesis
for this series of log returns of the IPSA stock index at the 5% signiﬁcance
level.
8.3
ARIMA PROCESSES
An autoregressive integrated moving-average ARIMA(p, d, q) process yt is de-
ﬁned by the equation
φ(B)∆dyt = θ(B)εt,
where φ(B) is an autoregressive polynomial of order p, θ(B) is an moving-
average polynomial of order q, φ(B) and θ(B) have no common roots and εt
is a white noise sequence. Note that the diﬀerenced process ∆dyt satisﬁes an
ARMA(p, q) model.
Figure 8.1 to Figure 8.4 display 1000 simulated observations from ARIMA
model
(1 −φB)(1 −B)dyt = εt −θεt−1,
for diﬀerent diﬀerentiation levels d = 1 and d = 2 as well as for distinct values
of the autoregressive parameter φ. Note that these time series plots show
apparent local trends. However, these are stochastic paths corresponding to

ARIMA PROCESSES
299
Time
Series
0
200
400
600
800
1000
-40
-20
0
20
Figure 8.1
Simulated ARIMA(1, 1, 0) model with φ = 0.6 and σ2 = 1.
integrated processes. There is a clear diﬀerence in the paths of Figure 8.1
and Figure 8.2 due to the change of the parameter from positive to nega-
tive. Furthermore, these two cases are diﬀerent from the series exhibited in
Figure 8.3 and Figure 8.4. The second order integration represented in these
plots reveals smooth but highly nonstationary paths.
After accounting for the diﬀerentiation level of the series, the estimation
of a ARIMA model proceeds analogously to the estimation of an ARMA
process. As an example, consider the ARIMA(1, 2, 1) exhibited in Figure 8.4.
An application of the R function arima.mle produces the following parameter
estimates
> fit
Call:
arima(x = y, order = c(1, 2, 1))
Coefficients:
ar1
ma1
0.2733
0.5750
s.e.
0.0409
0.0342
sigma^2 estimated as 0.9552: log likelihood = -1396.4, aic = 2798.8

300
NONSTATIONARY PROCESSES
Time
Series
0
200
400
600
800
1000
0
5
10
15
20
Figure 8.2
Simulated ARIMA(1, 1, 0) model with φ = −0.6 and σ2 = 1.
Time
Series
0
200
400
600
800
1000
-4000
-3000
-2000
-1000
0
Figure 8.3
Simulated ARIMA(1, 2, 0) model with φ = 0.2 and σ2 = 1.

LOCALLY STATIONARY PROCESSES
301
Time
Series
0
200
400
600
800
1000
-6000
-4000
-2000
0
2000
4000
Figure 8.4
Simulated ARIMA(1, 2, 1) model with φ = 0.2, θ = 0.6 and σ2 = 1.
8.4
LOCALLY STATIONARY PROCESSES
Another approach for modeling non stationary time series data is the class of
locally stationary processes. Recall from Section 4.8 that a stationary process
{yt} can be written in terms of a spectral representation as
yt =
 π
−π
A(λ) eiλt dB(λ),
(8.1)
where A(λ) is a transfer function and B(λ) is an orthogonal increments process
on [−π, π] such that
Cov[B(λ), B(ω)] = σ2
2π δ(λ −ω)dλdω.
The representation (8.1) can be extended allowing the transfer function to
evolve in time as follows,
yt,T =
 π
−π
A0
t,T (λ) eiλt dB(λ),
(8.2)
for t = 1, . . . , T.
The transfer function A0
t,T (λ) of this class of nonstationary processes is as-
sumed to change smoothly over time so that they can be locally approximated
by stationary processes. Some examples are discussed below.

302
NONSTATIONARY PROCESSES
EXAMPLE 8.3
Consider the following time-varying version of the ﬁrst-order moving-
average process, denoted for simplicity as LSMA(1),
yt,T = σ
  t
T
 
1 + θ
  t
T

εt−1

,
(8.3)
t = 1, . . . , T, where {εt} is a zero-mean and unit variance white noise
sequence. The covariance structure of this model is,
κT (s, t) =







σ2   t
T
 
1 + θ2   t
T

,
s = t,
σ
  t
T

σ
  t−1
T

θ
  t
T

,
s = t −1,
σ
  t
T

σ
  t+1
T

θ
  t+1
T

,
s = t + 1,
0
otherwise.
In this case, the transfer function of the process is given by
A0
t,T (λ) = A
  t
T , λ

= σ
  t
T
 h
1 + θ
  t
T

e˙ıλi
.
(8.4)
Furthermore, the time-varying spectral density is
f
  t
T , λ

= |A
  t
T , λ

|2 = σ2   t
T
 
1 + θ2   t
T

+ 2θ
  t
T

cos λ

.
(8.5)
EXAMPLE 8.4
An extension of the previous model is the time-varying MA(∞) moving-
average expansion
yt,T = σ
  t
T
 ∞
X
j=0
ψj
  t
T

εt−j,
(8.6)
t = 1, . . . , T, where {εt} is a zero-mean and unit variance Gaussian white
noise and {ψj(u)} are coeﬃcients satisfying
ψ0 (u) = 1,
∞
X
j=0
ψj (u)2 < ∞,
for all u ∈[0, 1]. This model will be denoted LSMA(∞) hereafter. The
time-varying spectral density of (8.6) is
fθ(u, λ) = σ2(u)|
∞
X
j=0
ψj(u)e˙ıλj|2,
for u ∈[0, 1] and λ ∈[−π, π]. For simplicity, if |ψj(u)| ≤K exp(−aj)
for j ≥1 and u ∈[0, 1] with K and a positive constants, model (8.6)

LOCALLY STATIONARY PROCESSES
303
will be called a short-memory process. On the other hand, if |ψj(u)| ≤
Kjd−1 for u ∈[0, 1] and some d ∈(0, 1/2), model (8.6) will be called a
long-memory process. Another characterization is based on the spectral
density. It is said that a LS process has short memory if its spectral
density is bounded at λ = 0 for u ∈[0, 1].
On the other hand, the
process has long memory if its spectral density is unbounded near the
origin for u ∈[0, 1].
EXAMPLE 8.5
Consider the LS autoregressive process LSAR(1) deﬁned as
yt,T = φ( t
T )yt−1,T + εt,
(8.7)
for T = 1, . . . , T. Suppose that φ(u) = φ(0) for u < 0, and there exists
a positive constant K < 1 such that |φ(u)| ≤K for u < 1. Thus, an
expanded Wold expansion of this process is given by,
yt,T =
∞
X
j=0
ψj (t, T) εt−j,
(8.8)
where ψ0 (t, T) = 1 for all t,T, and for j ≥1,
ψj (t, T) =
j−1
Y
k=0
φ( t−k
T ).
(8.9)
From this, we conclude that the transfer function can be written as
A0
t,T (λ) = 1 +
∞
X
j=1
j−1
Y
k=0
φ( t−k
T )eiλj.
(8.10)
The spectral density of the limiting process is fθ(u, λ) = σ(u)2|1 −
φ(u)e˙ıλj|−2. This process satisﬁes deﬁnition (8.2) and its spectral den-
sity is bounded at the origin for all u. Thus, this is a short-memory
process.
EXAMPLE 8.6
Consider the LS autoregressive process LSAR(p)
yt,T =
p
X
j=1
aj( t
T )yt−j,T + εt,
for T = 1, . . . , T. The spectral density of the limiting process is fθ(u, λ) =
σ(u)2|1 −Pp
j=1 aj(u)e˙ıλj|−2. This process satisﬁes deﬁnition (8.2). In

304
NONSTATIONARY PROCESSES
this case, the spectral density is bounded at the origin under some regu-
larity conditions on the roots of the polynomial a(B) = 1 −Pp
j=1 ajBj.
Thus, these LSAR(p) processes have short memory.
EXAMPLE 8.7
Observe that a stationary fractional noise process (FN) with long-memory
parameter d is given by
yt = σ
∞
X
j=0
ψjεt−j,
(8.11)
where ψj =
Γ(j+d)
Γ(j+1)Γ(d), where Γ(·) is the Gamma function. A nonsta-
tionary extension of this model is the LS fractional noise process (LSFN)
with coeﬃcients ψj (u) =
Γ[j+d(u)]
Γ(j+1)Γ[d(u)], where d(·) is a smoothly time-
varying long-memory parameter. The covariances of a LSFN process
are
κT (s, t) = σ
  s
T

σ
  t
T

Γ

1 −d
  s
T

−d
  t
T

Γ

s −t + d
  s
T

Γ

1 −d
  s
T

Γ

d
  s
T

Γ

s −t + 1 −d
  t
T
,
for s, t = 1, . . . , T, s ≥t. From this expression, and for large s −t we
have that
κT (s, t) ∼σ
  s
T

σ
  t
T
 Γ

1 −d
  s
T

−d
  t
T

Γ

1 −d
  s
T

Γ

d
  s
T
(s −t)
d( s
T )+d
 t
T

−1,
The spectral density of this process is given by
fθ(u, λ) = σ2(u)
2π

2 sin λ
2
−2dθ(u)
,
for λ ∈[−π, π]. Thus, fθ(u, λ) ∼σ2(u)
2π |λ|−2d(u), for |λ| →0. Conse-
quently, fθ(u, λ) has a pole at the origin and then this is a long-memory
process.
EXAMPLE 8.8
Figure 8.5 exhibits a simulated locally stationary MA(1) model
yt = εt + θt εt−1,
where εt is a Gaussian white noise sequence with zero-mean and unit
variance, and the time-varying parameter θt evolves as
θt = 0.9 −1.8 t
n,

LOCALLY STATIONARY PROCESSES
305
with n = 400. Observe that this parameter can be also written in terms
of the rescaled time
θ(u) = 0.9 −1.8 u,
for u ∈[0, 1].
The sample ACF of the generated process is provided in panel (a)
of Figure 8.6. It is interesting to notice that this is just an heuristic
exercise since this non stationary process does not have a well-deﬁned
ACF.
From this panel, we may consider that this time series is white noise.
However, when we take a closer look at the sample ACF for the ﬁrst half
of the series, see Figure 8.6(b), it seems that this process behaves like a
MA(1) model with positive ﬁrst-order moving-average parameter θ.
A similar conclusion could be reached from panel (c) which shows the
sample ACF of the second half of the observations. From this panel, we
may think that the process is a MA(1) with negative parameter θ.
0.0
0.2
0.4
0.6
0.8
1.0
-0.5
0.0
0.5
(a)
u
!(u)
0.0
0.2
0.4
0.6
0.8
1.0
-3
-2
-1
0
1
2
3
(b)
u
y(u)
Figure 8.5
Locally stationary MA(1) model with θ[u] = 0.9−1.8u. (a) Evolution
of θ(u), (b) Observed series yt for t = 1, . . . , 400.

306
NONSTATIONARY PROCESSES
0
2
4
6
8
10
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
2
4
6
8
10
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
0
2
4
6
8
10
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(c)
Figure 8.6
Locally stationary MA(1) model with θ[u] = 0.9 −1.8u. (a) Sample
ACF, (b) sample ACF of yt, for t = 1, . . . , 200, and (c) sample ACF of yt, for
t = 201, . . . , 400.
Frequency
0.5
1.0
1.5
2.0
2.5
3.0
Time
0.0
0.2
0.4
0.6
0.8
1.0
Spectral Density
0.1
0.2
0.3
0.4
0.5
Figure 8.7
Spectral density of the locally stationary MA(1) model with θ(u) =
0.9 −1.8 u.

LOCALLY STATIONARY PROCESSES
307
EXAMPLE 8.9
To illustrate the use of locally stationary models in the context of strongly
dependent time series, consider the following FN(d) process
yt = σt (1 −B)dtεt,
where εt is a Gaussian white noise with zero-mean and unit variance.
The evolution of the long-memory parameter d is in terms of the scaled
time u given by
d(u) = 0.05 + 0.4 u,
while the evolution of the standard deviation parameter σ is
σ(u) = 1 −0.5 u.
The evolutions of these two parameters are plotted in Figure 8.8 while
the ﬁrst 10 terms of the covariance matrix of this model is reported in Ta-
ble 8.1. Notice that since this is not an stationary process, the covariance
matrix is not longer Toeplitz. On the other hand, Figure 8.10 displays
a simulated trajectory of this process with 1000 observations. From this
ﬁgure, the variance of the series seems to decreases with time as expected
from the speciﬁcation of the model. Furthermore, Figure 8.11 exhibits
the sample ACF for three blocks of observations of length 333 each. Note
that the strength of the serial correlation seems to increase with time.
This is due to the speciﬁcation of the long-memory parameter d which
increases from 0.05 to 0.45.
Table 8.1
Covariance Matrix of the LS FN model with d(u) = 0.05 +
0.4 u and σ(u) = 1 −0.5 u.
1
2
3
4
5
6
7
8
9
10
1
0.92
0.12
0.10
0.09
0.09
0.09
0.09
0.09
0.10
0.10
2
0.12
0.84
0.16
0.12
0.11
0.11
0.11
0.11
0.12
0.12
3
0.10
0.16
0.77
0.19
0.15
0.13
0.13
0.14
0.14
0.15
4
0.09
0.12
0.19
0.71
0.22
0.17
0.16
0.16
0.17
0.18
5
0.09
0.11
0.15
0.22
0.66
0.25
0.21
0.20
0.20
0.22
6
0.09
0.11
0.13
0.17
0.25
0.63
0.29
0.25
0.25
0.27
7
0.09
0.11
0.13
0.16
0.21
0.29
0.61
0.34
0.32
0.33
8
0.09
0.11
0.14
0.16
0.20
0.25
0.34
0.62
0.41
0.42
9
0.10
0.12
0.14
0.17
0.20
0.25
0.32
0.41
0.68
0.57
10
0.10
0.12
0.15
0.18
0.22
0.27
0.33
0.42
0.57
0.91

308
NONSTATIONARY PROCESSES
0.0
0.2
0.4
0.6
0.8
1.0
0.1
0.2
0.3
0.4
(a)
u
d(u)
0.0
0.2
0.4
0.6
0.8
1.0
0.4
0.6
0.8
1.0
(b)
u
!(u)2
Figure 8.8
Locally stationary FN(d) model speciﬁcation with d(u) = 0.05 + 0.4 u
and σ(u) = 1 −0.5 u. (a) Evolution of d(u). (b) Evolution of the noise variance
σ(u)2.
8.4.1
State-Space Representations
Given that the state space models described in Chapter 3 provide a very
useful framework for the eﬃcient calculation of estimates and forecasts, in
this section we review the application of this representations to the case of
locally stationary processes. Consider the following state space system,
xt+1,T
=
Ft,T xt,T + Vt,T ,
yt,T
=
Gt,T xt,T + Wt,T ,
(8.12)
where xt,T is a state vector, Ft,T is a state transition matrix, Vt is a state
noise with variance Qt,T , yt,T is the observation, Gt,T is observation matrix
and Wt is a observation noise with variance Rt,T .
The process (8.6) can be represented by the following inﬁnite-dimensional
state space system
xt+1,T
=

0
I∞

xt,T +

1
0
0
· · ·
′ εt+1,
yt,T
=
σ( t
T )

1 ψ1( t
T ) ψ2( t
T ) ψ3( t
T ) · · ·

xt,T ,
(8.13)
for t = 1, . . . , T, Var(xt,T ) = I∞, where I∞= diag{1, 1, . . . }, Rt,T = 0,
Qt,T = (qij) with qij = 1 if i = j = 1 and qij = 0 otherwise.
In some

LOCALLY STATIONARY PROCESSES
309
Frequency
1
2
3
Time
0.0
0.2
0.4
0.6
0.8
1.0
Spectral Density
0.0
0.5
1.0
Figure 8.9
Spectral Density of the locally stationary FN(d) model with d(u) =
0.05 + 0.4 u and σ(u) = 1 −0.5 u.
cases, this state space representation may not be minimal. For instance, for
LSAR(2) processes, the state space is 2-dimensional:
xt+1,T =
 a1( t
T )
a2( t
T )
0
1

xt,T + εt+1,
yt,T = [1 0] xt,T .
It is ussually more practical to approximate the model by,
yt,T = σ
 t
T
 m

j=0
ψj
 t
T

εt−j,
(8.14)
for t = 1, . . . , T and some positive integer m. A ﬁnite-dimensional state state
system for (8.14) is given by
xt+1,T
=

0
0
Im
0

xt,T +

1
0
· · ·
0
′ εt+1
yt,T
=
σ( t
T )

1 ψ1( t
T ) ψ2( t
T ) ψ3( t
T ) · · · ψm( t
T )

xt,T ,
(8.15)
for t = 1, . . . , T, where Ir denotes the r × r identity matrix hereafter. Let
rm = Var[∞
j=m+1 ψj(u)εt−j] be the variance of the truncation error for ap-
proximating {yt,T } by the ﬁnite moving-average expansion (8.14). Then, the
asymptotic magnitude of the truncation error when approximating (8.6) by

310
NONSTATIONARY PROCESSES
0.0
0.2
0.4
0.6
0.8
1.0
-2
-1
0
1
2
u
y(u)
Figure 8.10
Simulated locally stationary FN(d) model with d(u) = 0.05 + 0.4 u
and σ(u) = 1 −0.5 u and 1000 observations.
(8.14) is, rm ∼O(e−am) for a short-memory process and rm ∼O(m2d−1) for
a long-memory process, for large m, where a > 0 and d = supud(u) < 1/2.
8.4.2
Whittle Estimation
Let θ ∈Θ be a parameter vector specifying model (8.2) where the parameter
space Θ is a subset of a ﬁnite-dimensional Euclidean space. Given a sample
{y1,T , . . . , yT,T } of the process (8.2) we can estimate θ by minimizing the
Whittle log-likelihood function
LT (θ) = 1
4π
1
M
 π
−π
M

j=1

log fθ(uj, λ) + IN(uj, λ)
fθ(uj, λ)

dλ,
(8.16)
where fθ(u, λ) = |Aθ(u, λ)|2 is the time-varying spectral density of the limit-
ing process speciﬁed by the parameter θ, IN(u, λ) = |DN(u,λ)|2
2πH2,N(0) is a tapered
periodogram with
DN(u, λ) =
N−1

s=0
h
 s
N

y[uT ]−N/2+s+1,T e−iλs,
Hk,N =
N−1

s=0
h
 s
N
k
e−iλs,

LOCALLY STATIONARY PROCESSES
311
0
5
10
15
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
0
5
10
15
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(c)
Figure 8.11
Sample ACF for the simulated locally stationary FN(d) model with
d(u) = 0.05 + 0.4 u and σ(u) = 1 −0.5 u and 1000 observations. (a) sample ACF
of yt for t = 1, . . . , 333, (b) sample ACF of yt for t = 334, . . . , 666 and (c) sample
ACF of yt for t = 667, . . . , 1000.
T = S(M −1) + N, uj = tj/T, tj = S(j −1) + N/2, j = 1, . . . , M and h(·) is
a data taper. Here, N is a block size, M denotes the number of blocks, and
S is the shift between them.
The Whittle estimator of the parameter vector θ is given by
θT = arg min LT (θ),
(8.17)
where the minimization is over a parameter space Θ.
8.4.3
State Space Estimation
Consider the state space representation (8.13) of yt, T .
The Kalman ﬁlter
equations can be used for estimating model parameters, state vectors, future
observations and missing values. Let ∆t, T = Var(yt, T −yt, T ) be the prediction
error variance and let Ωt, T = Var(xt, T −xt, T ) = (ωi,j(t, T)) be the state
prediction error variance-covariance matrix. The Kalman recursive equations
are as follows for the initial conditions y0, T = (0, 0, . . .), x1 = E(x1) =

312
NONSTATIONARY PROCESSES
(0, 0, . . .) and Ω1, T = E(x1, x′
1) = {1, 1, . . .}:
∆t, T = σ2   t
T

∞
X
i,j=1
ψi−1
  t
T

ωi,j(t, T) ψj−1
  t
T

,
Θt T (i) = σ
  t
T
 ∞
X
j=1
ωi−1,j(t, T) ψj−1
  t
T

,
ωt+1, T (i, j) = ωt, T (i + 1, j + 1) + qi,j −δ(t) Θt, T (i) Θt, T (j)/∆t, T ,
(8.18)
byt, T = σ
  t
T
 ∞
X
j=1
ψj−1
  t
T

bxt, T (j),
bxt+1, T (i) = bxt, T (i −1) + Θt, T (i)(yt, T −bYt, T )/∆t, T ,
where δ(t) = 1 if observation yt, T is available and δ(t) = 0 otherwise.
Let θ be the model parameter vector, then the log-likelihood function (up
to a constant) can be obtained from (8.18),
L(θ) =
T
X
t=1
log ∆t, T +
T
X
t=1
(yt, T −byt, T )2
∆t, T
.
Hence, the exact MLE provided by the Kalman equations (8.18) is given by
bθ = arg max
θ∈Θ L(θ),
where Θ is a parameter space. Observe that the Kalman equations (8.18) can
be applied directly to the general state space representation (8.12) or to the
truncated representation (8.15), yielding in this case an approximate MLE.
EXAMPLE 8.10
Consider the following LSMA(2) process
yt = εt + θ1(t) εt−1 + θ2(t) εt−2,
where the moving-average parameter evolve as
θ1(u)
=
α0 + α1 u,
θ2(u)
=
β0 + β1 u.
Figure 8.12 displays a simulated trajectory of this process with 1000
observations and parameters α0 = 0.2, α1 = 0.5, β0 = 0.8 and β1 =
−0.6. Additionally, the sample ACF of this process is exhibited in Fig-
ure 8.13. Given that this is not a stationary process, we consider heuris-
tic estimates by blocks. Panel (a), ﬁrst block of 333 observations, panel

LOCALLY STATIONARY PROCESSES
313
Time
LS MA(2) Series
0
200
400
600
800
1000
-2
0
2
4
Figure 8.12
Simulated LSMA(2) model with 1000 observations and parameters
α0 = 0.2, α1 = 0.5, β0 = 0.8 and β1 = −0.6.
0
5
10
15
20
25
30
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
30
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
0
5
10
15
20
25
30
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(c)
Figure
8.13
Sample
ACF
of
the
simulated
LSMA(2)
model
with
1000
observations and parameters α0 = 0.2, α1 = 0.5, β0 = 0.8 and β1 = −0.6. (a)
ﬁrst block of 333 observations, (b) second block of 333 observations, and (c) third
block of 334 observations.

314
NONSTATIONARY PROCESSES
Table 8.2
Whittle and Kalman Estimates for the LSMA(2) model
parameters
Parameter
α0
α1
β0
β1
0.2
0.5
0.8
-0.6
Whittle
0.2340
0.4209
0.7675
-0.6083
Kalfman
0.2352
0.4028
0.7739
-0.6097
(b), second block of 333 observations, and panel (c), third block of 334
observations. On the other hand, Table 8.2 reports the average param-
eter estimates for this model, based on 1000 repetitions and using both
the Whittle and the Kalman methods.
Notice that both techniques produce estimates close to their theoret-
ical counterparts.
EXAMPLE 8.11
Consider the following LSFN process
yt = σt (1 −B)−dt εt,
where the parameters evolve as
σ(u)
=
α0 + α1 u,
d(u)
=
β0 + β1 u.
Figure 8.14 displays a simulated trajectory of this LSFN process with
1000 observations a and time-varying parameters α0 = 0.90, α1 = 0.10,
β0 = 0.15 and β1 = 0.30. Moreover, the sample ACF of this process is
exhibited in Figure 8.15.
Analogously to the previous example, since this is a nonstationary
process, we consider heuristic estimates by blocks. Panel (a), ﬁrst block
of 333 observations, panel (b), second block of 333 observations, and
panel (c), third block of 334 observations.
On the other hand, Table 8.3 reports the average parameter estimates
for this model, based on 1000 repetitions and using both the Whittle and
the Kalman methods. From this table, we conclude that both approaches
provide good parameter estimates in the LSFN example.

LOCALLY STATIONARY PROCESSES
315
Time
LSFN Series
0
200
400
600
800
1000
-3
-2
-1
0
1
2
3
Figure 8.14
Simulated LSFN model with 1000 observations and time-varying
parameters α0 = 0.90, α1 = 0.10, β0 = 0.15 and β1 = 0.30.
0
5
10
15
20
25
30
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
30
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
0
5
10
15
20
25
30
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(c)
Figure 8.15
Sample ACF of the simulated LSFN model with 1000 observations
and time-varying parameters α0 = 0.90, α1 = 0.10, β0 = 0.15 and β1 = 0.30. (a)
ﬁrst block of 333 observations, (b) second block of 333 observations, and (c) third
block of 334 observations.

316
NONSTATIONARY PROCESSES
Table 8.3
Whittle and Kalman Estimates for the LSFN model
parameters.
Parameter
α0
α1
β0
β1
0.90
0.10
0.15
0.30
Whittle
0.9383
0.1053
0.1576
0.2491
Kalfman
0.9010
0.1826
0.1675
0.2431
8.4.4
Asymptotic Variance
Let θ0 be the true value of the parameter θ, the Whittle estimator θn satisﬁes
√n(θn −θ0) →N

0, Γ(θ0)−1
,
as n →∞, where
Γ(θ) = 1
4π
Z 1
0
Z π
−π
[∇log fθ(u, λ)][∇log fθ(u, λ)]′dλdu.
(8.19)
EXAMPLE 8.12
For an LSARFIMA(1, d, 1) with polynomial evolution of its time-varying
parameters φ(u), θ(u) and d(u) we have the following formula for the
asymptotic distribution of θT .
Γ =




Γd
Γd φ
Γd θ
0
Γφ d
Γφ
Γφ θ
0
Γθ d
Γθ φ
Γθ
0
0
0
0
Γσ



,
Γd = π2
6

1
i + j −1

i,j=1,...,Pd+1
,
Γφ =
Z 1
0
1
(1 −[φ(u)]2)

ui+j−2
i,j=1,...,Pφ+1 du
Γθ =
Z 1
0
1
(1 −[θ(u)]2)

ui+j−2
i,j=1,...Pθ+1 du;
Γd φ = −
Z 1
0
log[1 + φ(u)]
φ(u)

ui+j−2
i=1,...,Pd+1;j=1,...Pφ+1 du

LOCALLY STATIONARY PROCESSES
317
Γd θ =
Z 1
0
log[1 + θ(u)]
θ(u)

ui+j−2
i=1,...,Pd+1;j=1,...Pθ+1 du
Γφ θ = −
Z 1
0
1
1 −φ(u) θ(u)

ui+j−2
i=1,...,Pφ+1;j=1,...Pθ+1 du
Γσ = 2
Z 1
0
1
[σ(u)]2

ui+j−2
i,j=1,...,Pσ+1 du.
EXAMPLE 8.13
Consider a LS-FN process where d(u) and σ(u) are given by
d(u) = α0 + α1u + · · · + αpup,
σ(u) = β0 + β1u + · · · + βquq,
for u ∈[0, 1]. In this case the parameter vector is (α0, . . . , αp, β0, . . . , βq)′
and the matrix Γ given by (8.19) can be written as
Γ =
 Γα
0
0
Γβ

,
where
Γα =

π2
6 (i + j + 1)

i,j=0,...,p
,
and
Γβ =
Z 1
0
ui+j du
(β0 + β1u + · · · + βquq)2

i,j=0,...,q
.
EXAMPLE 8.14
Consider a LS-FN process where d(u) and σ(u) are harmonic
d(u) = α0 + α1 cos(λ1u) + · · · + αp cos(λpu),
σ(u) = β0 + β1 cos(ω1u) + · · · + βq cos(ωqu),
for u ∈[0, 1], where λ0 = 0, λ2
i ̸= λ2
j for i, j = 0, . . . , p, i ̸= j,
ω0 = 0, and ω2
i ̸= ω2
j for i, j = 0, . . . , q, i ̸= j.
In this case θ =
(α0, . . . , αp, β0, . . . , βq)′ and Γ
Γ =
 Γα
0
0
Γβ

,

318
NONSTATIONARY PROCESSES
where
Γα = π2
12
sin(λi −λj)
λi −λj
+ sin(λi + λj)
λi + λj

i,j=0,...,p
,
Γβ = π2
12
sin(ωi −ωj)
ωi −ωj
+ sin(ωi + ωj)
ωi + ωj

i,j=0,...,q
.
EXAMPLE 8.15
Consider the LSARFIMA process deﬁned by
Φ(t/T, B)Yt,T = Θ(t/T, B)(1 −B)−d(t/T )σ(t/T)εt,
(8.20)
for t = 1, . . . , T, where for u ∈[0, 1],
Φ(u, B) = 1 + φ1(u)B + · · · + φP (u)BP
Θ(u, B) = 1 + θ1(u)B + · · · + θQ(u)BQ
Assume that P = Q = 1 in model (8.20) where σ(u) = 1 and d(u),
Φ(u, B), Θ(u, B) are speciﬁed by
d(u) = α1u,
Φ(u, B) = 1 + φ(u)B,
φ(u) = α2u,
Θ(u, B) = 1 + θ(u)B,
θ(u) = α3u,
for u ∈[0, 1]. In this case, θ = (α1, α2, α3)′ and Γ from (8.19) can be
written as
Γ =


γ11
γ12
γ13
γ21
γ22
γ23
γ31
γ32
γ33

,
where
γ11 =
1
2α3
1
log 1 + α1
1 −α1
−1
α2
1
,
|α1| < 1,
γ12 =
1
(α1α2)3/2 g(α1α2) −
1
α1α2
,

LOCALLY STATIONARY PROCESSES
319
with g(x) = (√x) for x ∈(0, 1) and g(x) = arctan(√−x) for x ∈(−1, 0),
γ13 =
1
2α1
1
2 −1
α1

−

1 −1
α2
1

log(1 + α1)

,
γ22 =
1
2α3
2
log 1 + α2
1 −α2
−1
α2
2
,
|α2| < 1,
γ23 =
1
2α2

1 −1
α2
2

log(1 + α2) −
1
2 −1
α2

,
γ33 = π2
18.
8.4.5
Monte Carlo Experiments
In order to gain some insight into the ﬁnite sample performance of the Whittle
estimator we report next a number of Monte Carlo experiments for the LSFN
model
yt,T = σ(t/T) (1 −B)−d(t/T ) εt,
(8.21)
for t = 1, . . . , T with d(u) = α0 + α1 u, σ(u) = β0 + β1 u and Gaussian white
noise {εt} with unit variance. Denote the parameter vector by = (α0, α1) for
d(·) and = (β0, β1) for the noise scale σ(·).
The samples of the LSFN process are generated by means of the innovations
algorithm. The Whittle estimates in these Monte Carlo simulations have been
computed by using the cosine bell data taper
h(x) = 1
2[1 −cos(2 π x)].
Table 8.4 reports the results from Monte Carlo simulations for several pa-
rameter values, based on 1000 replications. These tables show the average of
the estimates as well as their theoretical and empirical standard deviations
(SD) given by
Γ =

π2
6 (i + j + 1)

i,j=0,1
,
Γ = 2
Z 1
0
ui+j du
σ2(u)

i,j=0,1
.
(8.22)
Observe from this table that the estimated parameters are close to their
true values. Besides, the empirical SD are close to their theoretical coun-
terparts.
These simulations suggest that the ﬁnite sample performance of
the proposed estimators seem to be very good in terms of bias and standard
deviations.

320
NONSTATIONARY PROCESSES
Table 8.4
Whittle maximum likelihood estimation for model (8.21): Sample size
T = 1024, block size N = 128 and shift S = 64.
Parameters
Estimates
Theoretical SD
SD Estimates
α0
α1
bα0
bα1
σ(bα0)
σ(bα1)
bσ(bα0)
bσ(bα1)
0.1000
0.2000
0.0896
0.2014
0.0490
0.0840
0.0534
0.1094
0.1500
0.2500
0.1261
0.2786
0.0490
0.0840
0.0515
0.1085
0.2000
0.2000
0.1853
0.2245
0.0490
0.0840
0.0743
0.1055
0.2000
0.2500
0.1877
0.2670
0.0490
0.0840
0.0573
0.0990
0.2500
0.2000
0.2627
0.2042
0.0490
0.0840
0.0772
0.1017
0.1000
0.2000
0.0755
0.2161
0.0490
0.0840
0.0533
0.1220
0.1500
0.2500
0.1428
0.2650
0.0490
0.0840
0.0689
0.0959
0.2000
0.2000
0.1812
0.2095
0.0490
0.0840
0.0476
0.1156
0.2000
0.2500
0.2030
0.2678
0.0490
0.0840
0.0475
0.0723
0.2500
0.2000
0.2546
0.1920
0.0490
0.0840
0.0510
0.1158
β0
β1
bβ0
bβ1
σ(bβ0)
σ(bβ1)
bσ(bβ0)
bσ(bβ1)
0.5000
0.5000
0.4862
0.5142
0.0270
0.0560
0.0201
0.0734
0.5000
0.5000
0.5018
0.5286
0.0270
0.0560
0.0353
0.0564
0.5000
0.5000
0.4701
0.5030
0.0270
0.0560
0.0255
0.0605
0.5000
0.5000
0.4879
0.5339
0.0270
0.0560
0.0135
0.0677
0.5000
0.5000
0.5020
0.4965
0.0270
0.0560
0.0295
0.0739
1.0000
-0.5000
0.9781
-0.4879
0.0380
0.0560
0.0255
0.0704
1.0000
-0.5000
0.9969
-0.5100
0.0380
0.0560
0.0406
0.0736
1.0000
-0.5000
1.0191
-0.5093
0.0380
0.0560
0.0438
0.0668
1.0000
-0.5000
1.0178
-0.4753
0.0380
0.0560
0.0376
0.0635
1.0000
-0.5000
1.0327
-0.4857
0.0380
0.0560
0.0365
0.0504
8.4.6
Data Application
Tree rings count is a usual procedure in studies of forest mass to determine
growth and yield of both natural forests and forest plantations. These time
series are also useful in paleoclimatology, as discussed in Chapter 1. Forest
analysis can be implemented in species growing in temperate regions, where
it is easy to identify the ring growth. In tropical climates, where there is little
diﬀerentiation among seasons, growth rates are constant, making it diﬃcult

LOCALLY STATIONARY PROCESSES
321
Year
Tree Ring Series
0
500
1000
1500
2000
0.0
0.5
1.0
1.5
2.0
2.5
Figure 8.16
Mammoth Creek Tree Ring Data.
to clearly diﬀerentiate spring and winter wood. Consequently, this data set
can be used as climate proxies and to indicate the chances of temperature and
precipitation conditions in paleoclimatology.
Figure 8.16 displays annual tree-ring width of the Pinus Longaeva, mea-
sured at Mammoth Creek, Utah, from 0 AD to 1989 AD, cf. Chapter 1 and
Appendix C.
Figure 8.17(a) shows the sample ACF of xt,T , and the corresponding vari-
ances of the sample mean, that is varplots, are shown in panel (b). The dashed
line corresponds to its expected behavior for a short-memory case with blocks
of k observations, whereas the continuous line represents the expected behav-
ior for a long-memory case. From both panels, this series seems to exhibit
long-range dependence.
Moreover, a closer look at the sample ACF of the data reveals that the
degree of persistence seems to vary over time. Indeed, Figure 8.18 shows the
sample autocorrelation of three segments of the sample: observations 1 to 500,
observations 750 to 1250 and observations 1490 to 1990. This ﬁgure provides
information for arguing possible changes in the degree of dependence. This
represents a clear evidence of a nonstationary process. Therefore, it seems that
the data has a time-varying long-memory structure. Additionally, Figure 8.19
and Figure 8.20 depict two views of the time-varying periodogram of the data.
In order to handle these features, a locally stationary ARFIMA process
is suggested. Figure 8.21 shows an heuristic estimator of the long-memory

322
NONSTATIONARY PROCESSES
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
-3.8
-3.6
-3.4
-3.2
-3.0
-2.8
-2.6
Log  k
Log  Var (k)
(b)
Figure 8.17
Mammoth Creek Tree Ring Data. (a) Sample ACF, (b) Varplot.
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(c)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
-3.2
-3.0
-2.8
-2.6
-2.4
Log  k
Log  Var (k)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
-4.5
-4.0
-3.5
-3.0
Log  k
Log  Var (k)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
-4.5
-4.0
-3.5
-3.0
-2.5
Log  k
Log  Var (k)
Figure 8.18
Mammoth Creek Tree Ring Data. Sample ACF and Varplots: (a)
Observations 1 to 500, (b) Observations 750 to 1250, (c) Observations 1490 to 1990.

LOCALLY STATIONARY PROCESSES
323
Frequency
1
2
3
Year
0
500
1000
1500
2000
Periodogram
0.05
0.10
Figure 8.19
Mammoth Creek tree ring data. Time-varying periodogram.
Frequency
1
2
3
Year
0
500
1000
1500
2000
Periodogram
0.05
0.10
Figure 8.20
Mammoth Creek tree ring data. Time-varying periodogram.

324
NONSTATIONARY PROCESSES
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.1
0.2
0.3
0.4
0.5
(a)
u
d(u)
0.0
0.2
0.4
0.6
0.8
1.0
-0.2
-0.1
0.0
0.1
0.2
0.3
0.4
0.5
(b)
u
!(u)
0.0
0.2
0.4
0.6
0.8
1.0
0.20
0.25
0.30
0.35
0.40
0.45
0.50
(c)
u
!(u)
Figure 8.21
Mammoth Creek tree ring data. Time-varying estimated parameters.
(a) d(u), (b) theta(u), (c) σ(u).
parameter and the variance of the noise scale along with stationary fractional
noise and locally stationary fractional noise model estimates of these quan-
tities. From this ﬁgure we suggest a linear and quadratic function for d( u)
and σ( u) respectively, that is, a LSARFIMA(1, d, 0) model with time-varying
parameters given by,
d(u) = d0 + d1 u,
θ(u) = θ0 + θ1 u,
σ(u) = β0 + β1 u + β2 u2.
(8.23)
Table 8.5 reports the parameter estimates using the Whittle method. The
standard deviations and the t-tests have been obtained using (8.22) for d( u),
θ(u) and σ( u), respectively. As we can observe in this table, the parameters
(d0, d1) and (β0, β1, β2) are statistically signiﬁcant at the 5% level.
The residuals of the model are plotted in Figure 8.22 along with the sample
ACF, the partial ACF and the Ljung-Box tests. From these panels, it seems
that there are no signiﬁcant autocorrelations in the residuals. This conclusion
is formally supported by the Ljung-Box tests. Consequently, the white noise
hypothesis cannot be rejected at the 5% level.
As described in the previous data application, locally stationary processes
are useful tools for modeling complex nonstationary time dependence struc-
tures.

LOCALLY STATIONARY PROCESSES
325
Table 8.5
Tree Ring Data at Mammoth Creek, Utah.
LSARFIMA(1, d, 0)
parameters estimated with the Whitle method.
Parameter
Estimate
SD
t-value
d0
0.1769622
0.08445276
2.095399
d1
0.8050242
0.39121972
2.057729
d2
-0.9328672
0.37841028
-2.465227
θ0
0.1593663
0.10967577
1.453067
θ1
-1.1863239
0.50051007
-2.370230
θ2
1.0963000
0.48266935
2.271327
β0
0.3707303
0.01648537
22.488441
β1
-0.3597924
0.07597368
-4.735751
β2
0.4371512
0.07541373
5.796706
0
500
1000
1500
2000
−6
−4
−2
0
2
4
6
Standardized Residuals
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
0
2
4
6
8
10
−0.04
−0.02
0.00
0.02
0.04
Lag
Partial ACF
G
G
G
G
G
G
G
G
G
G
0
2
4
6
8
10
0.0
0.2
0.4
0.6
0.8
1.0
Lag
p−value
Figure 8.22
Mammoth Creek tree ring data. Residual diagnostic of the model.

326
NONSTATIONARY PROCESSES
8.5
STRUCTURAL BREAKS
As discussed throughout this book, real-life time series usually display struc-
tural changes. In the previous section, we discussed locally stationary pro-
cesses where the model changes are gradual and continuous. On the other
hand, in some ocassions the structural changes are rather abrupt. In these
cases, there are other methodologies developed to deal with structural breaks.
In what follows, we discuss a methodology for handling these nonstationarities
by modeling the time series in terms of successive blocks of linear trends and
seasonal models. Let yt be a time series which can be described by
yt = Tt + St + εt,
where Tt and St denote sequences of linear trends and seasonal components,
and εt is an error sequence. More speciﬁcally, the linear trends are described
by
Tt = αj + βj t,
for
t ∈(tj−1, tj]
where t0 = 0 and t1, . . . , tm denote the times at which the trend breaks occur.
On the other hand, the seasonal components are deﬁned by
St =

γij
if
t is in season i and t ∈(sj−1, sj]
−Ps−1
i=1 γij
if
t is in season 0 and t ∈(sj−1, sj]
where s0 = 0 and s1, . . . , sp denote the times at which the seasonal breaks
occur. The R package breaks for additive seasonal and trend bfast allows fot
the estimation of the number of trend breaks m, the times t1, . . . , tm, the
number of seasonal breaks p, the times s1, . . . , sp along with the parameters
αj, βj and γij.
EXAMPLE 8.16
Consider the series of passenger enplanements introduced in Chapter 1.
An application of the bfast methodology produces the following trend
break decomposition shown in Figure 8.23.
TREND BREAKPOINTS
Confidence intervals for breakpoints
of optimal 2-segment partition:
Call:
confint.breakpointsfull(object = bp.Vt, het.err = FALSE)
Breakpoints at observation number:
2.5 % breakpoints 97.5 %
1
55
56
57

STRUCTURAL BREAKS
327
50000
60000
70000
Yt
-5000
0
5000
St
50000 55000 60000 65000 70000
Tt
-4000
0
4000
2004
2006
2008
2010
2012
et
Time
Figure 8.23
Breaks for Additive Seasonal and Trend Analysis of Passenger
enplanements data.
Corresponding to breakdates:
2.5 %
breakpoints 97.5 %
1 2008(7) 2008(8)
2008(9)
SEASONAL BREAKPOINTS:
None
The analysis reported by this ﬁgure suggests that there was a trend
change in the number of passenger enplanements around August 2008.
EXAMPLE 8.17
As another illustration of the bfast methodology consider the logarithm
of the US employment in Arts, Entertainment and Recreation, for the
period January 1990 to December 2012. In this case, the bfast method
produces the following trend break decomposition.
TREND BREAKPOINTS
Confidence intervals for breakpoints
of optimal 3-segment partition:
Breakpoints at observation number:
2.5 % breakpoints 97.5 %
1
139
140
141
2
229
230
231

328
NONSTATIONARY PROCESSES
7.0
7.2
7.4
7.6
Yt
-0.15
0.00 0.10
St
7.0
7.2
7.4
7.6
Tt
-0.02
0.02
1990
1995
2000
2005
2010
et
Time
Figure 8.24
Breaks for Additive Seasonal and Trend Analysis of the logarithm of
the US employment in Arts, Entertainment and Recreation, for the period January
1990 to December 2012.
Corresponding to breakdates:
2.5 %
breakpoints 97.5 %
1 2001(7) 2001(8)
2001(9)
2 2009(1) 2009(2)
2009(3)
SEASONAL BREAKPOINTS
Confidence intervals for breakpoints
of optimal 3-segment partition:
Breakpoints at observation number:
2.5 % breakpoints 97.5 %
1
39
43
46
2
129
134
139
Corresponding to breakdates:
2.5 %
breakpoints 97.5 %
1 1993(3) 1993(7)
1993(10)
2 2000(9) 2001(2)
2001(7)
According to the results, the analysis suggests structural changes in
both, the linear trends and the seasonal components.

STRUCTURAL BREAKS
329
EXAMPLE 8.18
As an illustration of structural change in the serial dependence structure
of the data consider the Nile river series described in Chapter 1. It has
been noticed that the ﬁrst 100 observations seems to have a diﬀerent
level of dependence than the rest of the series.
In this example, we
consider a ﬁst block of 100 values, from 622 AD. to 721 AD., and a
second block of 563 observations, from 722 AD. to 1284 AD. We do not
consider the data after the year 1284 AD. in this study because in this
period the series suﬀer a large number of data repetitions or missing
data problems.
By means of the R package arﬁma, we compute the corresponding
ARFIMA models to each block and obtain the following results.
> fit
Number of modes: 1
Call:
arfima(z = x.nile[1:100], order = c(0, 0, 0))
Coefficients for fits:
Coef.1:
SE.1:
d.f
0.014179
0.0883828
Year
Nile River Level
620
640
660
680
700
720
10
11
12
13
14
0
5
10
15
20
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
Figure 8.25
Nile river levels from 622 A.D. to 721 A.D. and sample ACF.

330
NONSTATIONARY PROCESSES
Year
Nile River Level
700
800
900
1000
1100
1200
1300
10
11
12
13
14
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
Figure 8.26
Nile river levels from 722 A.D. to 1284 A.D. and sample ACF.
Fitted mean
11.5133
0.0950155
logl
11.1362
sigma^2
0.808409
Starred fits are close to invertibility/stationarity boundaries
> fit
Number of modes: 1
Call:
arfima(z = x.nile[101:663], order = c(0, 0, 0))
Coefficients for fits:
Coef.1:
SE.1:
d.f
0.44762
0.0302793
Fitted mean
11.4113
0.837629
logl
249.662
sigma^2
0.410754
Starred fits are close to invertibility/stationarity boundaries
As indicated by many studies, the ﬁrst 100 observations displays al-
most null serial correlation which is reﬂected in the very low estimated
parameter d. In fact, the following Ljung-Box white noise tests for lags

BIBLIOGRAPHIC NOTES
331
5, 10 and 20 indicate that this part of the Nile river series is compatible
with a white noise model.
> Box.test(x.nile[1:100],lag=5,type="Ljung")
Box-Ljung test
data:
x.nile[1:100]
X-squared = 4.4495, df = 5, p-value = 0.4867
> Box.test(x.nile[1:100],lag=10,type="Ljung")
Box-Ljung test
data:
x.nile[1:100]
X-squared = 6.7533, df = 10, p-value = 0.7485
> Box.test(x.nile[1:100],lag=20,type="Ljung")
Box-Ljung test
data:
x.nile[1:100]
X-squared = 12.1105, df = 20, p-value = 0.9122
8.6
BIBLIOGRAPHIC NOTES
Techniques for estimating and forecasting ARIMA models are found in Box,
Jenkins, and Reinsel (1994).
Locally stationary processes have been play-
ing an important role in time series analysis. They have provided a sound
statistical methodology for modeling data exhibiting nonstationary features
without resorting to data transformations, trend removals and other related
techniques. The theory of LS processes is based on the principle that a non-
stationary process can be locally approximated by a stationary one if the time
variation of the model parameters is suﬃciently smooth. The idea of devel-
oping techniques for handling directly nonstationary processes dates back to
the sixties. For example, Priestley (1965), Priestley and Tong (1973), Tong
(1973) and others developed the concept of evolutionary spectra. In the nini-
ties, Dahlhaus (1996, 1997) provided a formal deﬁnition of a family of LS pro-
cesses. There are several works on LS processes, including, Dahlhaus (2000),
Jensen and Witcher (2000), Dahlhaus and Polonik (2006, 2009), Chandler
and Polonik (2006), Palma and Olea (2010), Dette, Preuß, and Vetter (2011)
and Palma, Olea, and Ferreira (2013), among others. Other classes of LS
processes have been discussed for example by Wang, Cavanaugh, and Song
(2001), Cavanaugh, Wang, and Davis (2003) and Last and Shumway (2008).
The analysis of the asymptotic properties of the Whittle locally stationary
estimates (8.17) is discussed, for example, in Dahlhaus (1997) and Palma and

332
NONSTATIONARY PROCESSES
Olea (2010).
Furthermore, the R package LSTS allows for the estimation and
prediction of LS models.
Problems
8.1
From the deﬁnition of the backshift operator B, ﬁnd expressions for Bzt
and B2zt, if zt is deﬁned as:
(a) zt = β0,
(b) zt = β0 + β1t,
(c) zt = β0 + β1xt + β2t,
(d) zt = β1xt + β2yt, where β0, β1 and β2 are constants, while xt and yt
are time series.
Hint: The operator B is deﬁned by the relationship Bzt = zt−1 for
all t.
8.2
Let {xt, t ∈Z} a stationary stochastic process with autocorrelation
function ρx(k). Show that {yt = (1 −B)xt, t ∈Z} is a stationary stochastic
process and calculate ρy(k) in terms of ρx(k). If xt is an ARMA(p,q) process,
what can you say about the process ∇xt?
8.3
From the deﬁnition of the operator ∇(∇= (1 −B)), ﬁnd expressions
for ∇zt and ∇2zt, if zt is deﬁned as in the previous exercise what is your guess
about the general expression for ∇dzt if d > 2?
8.4
Consider the following processes zt deﬁned by
(i) zt = β0 + β1ϵt,
(ii) zt = β0 + β1t + ϵt,
(iii) zt = βt
0eϵt, β0 > 0
(iv) zt = β0 + ϵt + β1ϵt−1,
where β0 and β1 are constant and {ϵt} is a white noise process with zero-mean
and variance σ2
ϵ , deﬁne a new process yt (as a function of zt) that is stationary.
Provide E(yt), Var(yt) and Cov(yt,yt+k) for k = 1, 2, . . ..
8.5
Let {εt} be a stationary process and xt = a + b t + εt with a and b
constants.
(a) Show that ∇xt is stationary.
(b) How would you obtain a stationary process stationary if the trend
were quadratic?
8.6
Let {ϵt} be a sequence of independent random variables normally dis-
tributed, zero-mean and variance σ2. Let a, b and c be constants Which of
the following processes are stationary? For each statiuonary process calculate
its expected value and its autocovariance function.
(a) xt = a + b ϵt + cϵt−1,

PROBLEMS
333
(b) xt = a + b ϵ0,
(c) xt = ϵ1 cos(ct) + ϵ2 sin(ct),
(d) xt = ϵ0 cos(ct),
(e) xt = ϵt cos(ct) + ϵt−1 sin(ct),
(f) xt = ϵtϵt−1.
8.7
Let xt = a + b t for t = 1, 2, . . . with a and b constants. Show that the
sample autocorrelations of this sequence ˆρ(k) satisfy ˆρ(k) →1 as n →∞for
each ﬁxed k.
8.8
Let St, t = 0, 1, 2, . . . a random walk with constant jump µ deﬁned as
S0 = 0 and St = µ + St−1 + xt, t = 1, 2, . . . where x1, x2, . . . are i.i.d. random
variables with zero-mean and variance σ2.
(a) Is the process {St} stationary?
(b) Is the sequence {∇St} stationary?
8.9
Consider a simple moving-average ﬁlter with weights aj = (2q + 1)−1,
−q ≤j ≤q.
(a) If mt = c0 + c1t, show that Pj=q
j=−q ajmt−j = mt.
(b) If εt, t = 0, ±1, ±2, . . ., are independent random variables with zero-
mean and variance σ2, show that the moving-average At = Pj=q
j=−q ajεt−j
is small for large q in the sense that E At = 0 and Var(At) =
σ2
2q+1.
8.10
Suppose that mt = c0 + c1t + c2t2, t = 0, ±1, . . .
(a) Show that
mt =
2
X
i=−2
aimt+i =
3
X
i=−3
bimt+i, t = 0, ±1, . . .
where a2 = a−2 = −3
35, a1 = a−1 = 12
35, a0 = 17
35, and b3 = b−3 =
−2
21, b2 = b−2 =
3
21, b1 = b−1 =
6
21, b0 =
7
21.
(b) Suppose that xt = mt + εt where {εt, t = 0, ±1, . . .} is a sequence of
independent random variables normal, with zero-mean and variance
σ2. Consider Ut = P2
i=−2 aixt+i and Vt = P3
i=−3 bixt+i.
(i) Calculate the mean and variance of Ut and Vt.
(ii) Find the correlation between Ut and Ut+1 and between Vt and Vt+1.
(iii) Which of the two ﬁltered series {Ut} or {Vt} would you expect to be
have a smoother path?
8.11
Consider a zero-mean series yt that satisﬁes an ARIMA(p, d, q) model.
Please answer the following questions:
(a) Write the representation of yt in terms of polynomials of the lag
operator B.

334
NONSTATIONARY PROCESSES
(b) Consider the stationary part of yt, is xt = (1 −B)yt? What model
is xt?
(c) What are the required conditions for xt to follow a causal and in-
vertible ARMA(p, q). model?
(d) Derive the minimum means of predictors xt using (i) squared error,
the causal model representation, (ii) the invertible representation of
the model and (iii) the diﬀerence equation representation model.
8.12
Let {xt} be the ARIMA(2,1,0) process
(1 −0.8 B + 0.25 B2)(1 −B)xt = zt,
where {zt} is a WN(0, 1).
(a) Find the function g(h) = Pnxn+h for h ≥0.
(b) Assuming that n is large, calculate σ2
n(h) for h = 1, . . . , 5.
8.13
Show that the seasonal component St in the trend break model can
be expressed as
St =
s−1
X
i=1
γi,j(dt,i −dt,0),
where the seasonal dummy variable dt,i satisﬁes dt,i = 1 if t is in season i and
0 otherwise.
(a) Verify that if t is in season 0, then dt,i −dt,0 = 1.
(b) Show that for all other seasons, dt,i −dt,0 = 1 when t is in season
i ̸= 0.
8.14
Consider a LS-FN process where d(u) and σ(u) are speciﬁed by
ℓ1[ d(u) ] =
p
X
j=0
αjgj(u),
ℓ2[ σ(u) ] =
q
X
j=0
βjhj(u),
(α0, . . . , αp, β0, . . . , βq)′. Show that the matrix Γ in (8.19) is given by
Γ =
 Γα
0
0
Γβ

,
Γα = π2
6
Z 1
0
gi(u) gj(u)
[ℓ′
1(d(u))]2 du

i,j=0,...,p
,
Γβ = 2
"Z 1
0
hi(u) hj(u)
[σ(u)ℓ′
2(σ(u))]2 du
#
i,j=0,...,q
.
8.15
Consider the locally stationary ARFIMA(0, d, 1) model given by
yt,T = σ
  t
T
 
1 −θ
  t
T

B

(1 −B)
−d
 t
T

εt,

PROBLEMS
335
where θ(·) is a smoothly varying moving-average coeﬃcient satisfying |θ(u)| <
1 for u ∈[0, 1]. Verify that the covariances κT (s, t) of this process are given
by
κT (s, t) = σ
  s
T

σ
  t
T

Γ

1 −d
  s
T

−d
  t
T

Γ

s −t + d
  s
T

Γ

1 −d
  s
T

Γ

d
  s
T

Γ

s −t + 1 −d
  t
T

×
"
1 + θ
  s
T

θ
  t
T

−θ
  s
T

s −t −d
  t
T

s −t −1 + d
  s
T
 −θ
  t
T

s −t + d
  s
T

s −t + 1 −d
  t
T

#
,
for s, t = 1, . . . , T, s ≥t.


CHAPTER 9
SEASONALITY
Seasonal patterns arise in a great number of real-life time series data.
For
instance, this phenomenon occurs in revenue series, inﬂation rates, monetary
aggregates, gross national product series, shipping data, and monthly ﬂows
of the Nile River; see Section 9.8 for speciﬁc references. Consequently, sev-
eral statistical methodologies have been proposed to model this type of data
including the Seasonal ARIMA (SARIMA) models, Gegenbauer autoregres-
sive moving-average processes (GARMA), seasonal autoregressive fractionally
integrated moving-average (SARFIMA) models, k-factor GARMA processes,
and ﬂexible seasonal fractionally integrated processes (ﬂexible ARFISMA),
among others.
In this chapter we review some of these statistical methodologies. A general
long-memory seasonal process is described in Section 9.2. This section also
discusses some large sample properties of the MLE and Whittle estimators
such as consistency, central limit theorem, and eﬃciency. Calculation of the
asymptotic variance of maximum-likelihood and quasi-maximum-likelihood
parameter estimates is addressed in Section 9.4. The ﬁnite sample perfor-
mance of these estimators is studied in Section 9.6 by means of Monte Carlo
simulations while Section 9.7 is devoted to the analysis of a real-life data il-
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
337

338
SEASONALITY
lustration of these estimation methodologies. Further reading on this topic
are suggested in Section 9.8 and several problems are listed at the end of this
chapter.
9.1
SARIMA MODELS
A simple seasonally integrated process with period s can be written as
(1 −Bs)yt = εt,
where εt is a white noise sequence, or equivalently,
yt = yt−s + εt.
This basic model establishes that the observation at time t is the same as the
observation at time t −s except by an additive noise.
More generally, a SARIMA(p, d, q) × (P, D, Q) model with one seasonal
component can be written as
φ(B)Φ(Bs)(1 −B)d(1 −Bs)Dyt = θ(B)Θ(Bs)εt,
where εt is a white noise sequence with zero-mean and variance σ2 and the
respective polynomials are given by
φ(B)
=
1 −φ1 B −φ2B2 −· · · −φp Bp,
Φ(Bs)
=
1 −Φ1 Bs −Φ2B2s −· · · −ΦP BP s,
θ(B)
=
1 −θ1 B −θ2B2 −· · · −θq Bq,
Θ(Bs)
=
1 −Θ1 Bs −Θ2B2s −· · · −ΘQ BQs.
EXAMPLE 9.1
Figure 9.1 exhibits a simulated time series with 500 observations from
a SARIMA(1, 0, 1) × (1, 1, 1) model with φ = −0.7, Φ = 0.7, θ = 0.4,
Θ = 0.2 and s = 12. Note that in this case, apart from the seasonal
behavior of the series, there is a random trend pattern generated by the
seasonal diﬀerentiation.
On the other hand, Figure 9.2 shows a trajectory of a SARIMA(1, 1, 1)×
(1, 1, 1) model with φ = −0.3, Φ = −0.3, θ = 0.4, Θ = 0.2 and s = 12.
Notice that the random trend is more extreme now as compared to
the previous example. This could be expected from the fact that we
have both standard diﬀerentiation (1 −B) and seasonal diﬀerentiation
(1 −B12).

SARIMA MODELS
339
Time
Series
0
100
200
300
400
500
-40
-20
0
20
40
60
Figure 9.1
Simulated 500 observations from a SARIMA model with φ = −0.7,
Φ = 0.7, θ = 0.4, Θ = 0.2, s = 12, D = 1 and Gaussian white noise with unit
variance.
Time
Series
0
100
200
300
400
500
0
20
40
60
80
100
120
140
Figure 9.2
Simulated 500 observations from a SARIMA model with φ = −0.3,
Φ = −0.3, θ = 0.4, Θ = 0.2, s = 12, D = 1, and Gaussian white noise with unit
variance.

340
SEASONALITY
EXAMPLE 9.2
Figure 9.3 displays a sample of a SARIMA(1, 0, 1) × (1, 0, 1) model with
φ = 0.5, Φ = 0.6, θ = 0.2, Θ = 0.3 and s = 12. Unlike the erratic
pattern described by the two seasonal time series shown in the previous
example, in this case the trajectory is not explosive. Additionally, the
sample ACF of this series is plotted in Figure 9.4. Notice the cyclical
pattern of this sample ACF.
Time
Series
0
200
400
600
800
1000
-6
-4
-2
0
2
4
6
Figure 9.3
Simulated 1000 observations from a SARIMA model with φ = 0.5,
Φ = 0.6, θ = 0.2, Θ = 0.3, s = 12, and Gaussian white noise with unit variance.
0
10
20
30
40
50
60
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
Series  y
Figure 9.4
Sample ACF of the simulated 1000 observations from a SARIMA
model with φ = 0.5, Φ = 0.6, θ = 0.2, Θ = 0.3 and Gaussian white noise with unit
variance.

SARIMA MODELS
341
9.1.1
Spectral Density
The spectral density of the SARIMA(p, 0, q) × (P, 0, Q) process is given by
f(λ) = σ2
2π

θ(ei λ) Θ(ei λ s)
φ(ei λ) Φ(ei λ s)

2
.
EXAMPLE 9.3
Figure 9.5 displays a sample of a SARIMA(1, 0, 1) × (1, 0, 1) model with
φ = 0.7, Φ = 0.6, θ = 0.4, Θ = 0.2, and seasonal period s = 20.
The spectral density of this series is exhibited in Figure 9.6 while its
periodogram is plotted in Figure 9.7.
Time
Series
0
200
400
600
800
1000
-10
-5
0
5
Figure 9.5
SARIMA model with φ = 0.7, Φ = 0.6, θ = 0.4, Θ = 0.2, s = 20, and
Gaussian white noise with unit variance. Simulated series with 1000 observations.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
5
10
15
20
25
30
Frequency
Spectral Density
Figure 9.6
SARIMA model with φ = 0.7, Φ = 0.6, θ = 0.4, Θ = 0.2, s = 20, and
Gaussian white noise with unit variance. Spectral Density.

342
SEASONALITY
Furthermore, Figure 9.8 shows an smoothed version of the periodogram
which uses a Daniell window.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
10
20
30
40
50
Frequency
Periodogram
Figure 9.7
SARIMA model with φ = 0.7, Φ = 0.6, θ = 0.4, Θ = 0.2, s = 20, and
Gaussian white noise with unit variance. Periodogram.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
5
10
15
Frequency
Smoothed Periodogram
Figure 9.8
SARIMA model with φ = 0.7, Φ = 0.6, θ = 0.4, Θ = 0.2, s = 20,
and Gaussian white noise with unit variance. Smoothed periodogram using a Daniell
kernel.

SARIMA MODELS
343
9.1.2
Several Seasonal Components
The SARIMA models can also be extended to handle more than one seasonal
period. For instance, we can write a SARIMA(p, d, q) × (P1, D1, Q1)s1 · · · ×
(Pm, Dm, Qm)sm model with m seasonal component can be written as
φ(B)Φ1(Bs1) · · · Φm(Bsm(1 −B)d(1 −Bs1)D1 · · · (1 −Bsm)Dm yt
= θ(B)Θ1(Bs1) · · · Θm(Bsm)εt,
where εt is a white noise sequence with zero-mean and variance σ2 and
φ(B)
=
1 −φ1 B −φ2B2 −· · · −φp Bp,
Φi(Bsi)
=
1 −Φi1 Bsi −Φi2B2si −· · · −ΦiP BP si,
θ(B)
=
1 −θ1 B −θ2B2 −· · · −θq Bq,
Θi(Bsi)
=
1 −Θi1 Bsi −Θi2B2si −· · · −ΘiQ BQsi.
Moreover, the spectral density of this multiple seasonal components model is
given by
f(λ) = σ2
2π

θ(ei λ) Θ1(ei λ s1) · · · Θm(ei λ sm)
φ(ei λ) Φ1(ei λ s1) · · · Φm(ei λ sm)

2
.
9.1.3
Estimation
The estimation of SARIMA models are readily extended from the maximum
likelihood techniques discussed in Chapter 5.
In particular some of these
methods are implemented in the statistical software R. Observe that the R
package gsarima allows for the simulation and estimation of SARIMA models.
In this case, the output is as follows,
ARIMA(1,0,1)(1,0,1)[12] with non-zero-mean
Coefficients:
ar1
ma1
sar1
sma1
intercept
0.5925
0.0515
0.5520
0.3466
-0.2413
s.e.
0.0589
0.0739
0.0493
0.0546
0.3138
sigma^2 estimated as 0.8816:
log likelihood=-683.25
AIC=1378.5
AICc=1378.67
BIC=1403.79
9.1.4
Estimator Performance
In this section we study the performance of maximum likelihood estimates of
SARIMA models. As an illustration, consider the SARIMA(1, 0, 1)×(1, 0, 1)s
deﬁned by
(1 −φ B)(1 −ΦBs)yt = (1 + θB)(1 + Θ Bs)εt,

344
SEASONALITY
Time
Series
0
100
200
300
400
500
-4
-2
0
2
4
Figure 9.9
Simulated 500 observations from a SARIMA model with φ = −0.7,
Φ = 0.5, θ = 0.2, Θ = 0.3, and Gaussian white noise with unit variance.
where εt is a Gaussian white noise sequence with zero-mean and unit variance.
Figure 9.9 shows a trajectory of this seasonal model with with φ = −0.7,
Φ = 0.5, θ = 0.2 and Θ = 0.3 while Figure 9.10 displays the sample ACF of
this time series.
Table 9.1 reports the results from several simulations for diﬀerent combina-
tions of parameters φ, Φ, θ and Θ. The results are based on 1000 repetitions
and time series with 500 observations. Notice that the average of the esti-
mates are very close to their theoretical counterparts. Furthermore, Table 9.2
reports the estimates of the stander deviation of these maximum likelihood
estimates. The ﬁrst four columns of this table correspond to the average of
estimates provided by the empirical Hessian matrix given by
Hn(θ) = ∇2Ln(θ)
where Ln is the log-likelihood function, θ the vector of parameters of the
seasonal model and θ the maximum likelihood estimator. Columns 5 to 8 of
Table 9.2 report the empirical standard deviations of the ML estimators based
on the 1000 Monte Carlo repetitions.

SARIMA MODELS
345
0
10
20
30
40
50
60
-0.5
0.0
0.5
1.0
Lag
ACF
Figure 9.10
Sample ACF of the simulated 500 observations from a SARIMA
model with φ = −0.7, Φ = 0.5, θ = 0.2, Θ = 0.3, and Gaussian white noise with
unit variance.
Table 9.1
SARIMA Simulations with Sample Size n = 500 and Seasonal Period
s = 12: Parameter Estimates.
φ
θ
Φ
Θ
φ
θ
Φ
Θ
0.5000
0.2000
0.6000
0.3000
0.4848
0.2021
0.5955
0.3008
0.3000
0.5000
0.3000
0.7000
0.2912
0.4797
0.3065
0.7010
0.3000
0.6000
-0.3000
0.5000
0.2914
0.6059
-0.2799
0.4833
0.7000
0.3000
0.5000
0.2000
0.6920
0.2981
0.4979
0.2024
-0.7000
0.2000
0.5000
0.3000
-0.6888
0.1939
0.4910
0.3059
9.1.5
Heating Degree Day Data Application
As an illustration of the versatility of SARIMA processes to model data ex-
hibiting seasonal patterns as well as serial dependence we revisit the heating
degree day data introduced in Chapter 1, see Figure 9.11. The periodogram
of this time series is displayed in Figure 9.12. As expected from the nature
of these data, the periodogram shows a peak at the frequency correspond-
ing to a period s = 12.
Consequently, the class of SARIMA processes is
proposed for this time series. The selected model via AIC corresponds to a

346
SEASONALITY
Table 9.2
SARIMA Simulations with Sample Size n = 500 and Seasonal Period
s = 12: Standard Deviation Estimates.
Hessian SD estimates
Sample SD estimates
bσ(bφ)
bσ(bθ)
bσ(bΦ)
bσ(bΘ)
bσ(bφ)
bσ(bθ)
bσ(bΦ)
bσ(bΘ)
0.0628
0.0704
0.0477
0.0579
0.0627
0.0701
0.0492
0.0570
0.0640
0.0592
0.0531
0.0420
0.0657
0.0637
0.0552
0.0415
0.0563
0.0470
0.1806
0.1658
0.0576
0.0479
0.2027
0.1915
0.0393
0.0524
0.0624
0.0712
0.0385
0.0528
0.0598
0.0704
0.0568
0.0769
0.0571
0.0634
0.0606
0.0775
0.0563
0.0638
SARIMA(1, 0, 0) × (3, 0, 0) and the ﬁtted model from the R function arima is
as follows,
> fit
Call:
arima(x = y, order = c(1, 0, 0),
seasonal = list(order = c(3, 0, 0), period = 12))
Coefficients:
ar1
sar1
sar2
sar3
intercept
0.3623
0.3168
0.3245
0.3401
275.6695
s.e.
0.0521
0.0518
0.0499
0.0501
58.3368
sigma^2 estimated as 1408:
log likelihood = -1859.91,
aic = 3731.83
Observe that all these coeﬃcients are signiﬁcant at the 5% level.
The residuals from this model are plotted in Figure 9.13.
The sample
ACF along with the Box-Ljung diagnostics test are reported in Figure 9.14.
From these plots, it seems that the residuals do not have serial correlation.
Figure 9.15 corresponds to a normal quantile-quantile plot. If the residuals
were normally distributed, then the dots should be close to the Gaussian
quantile line (heavy line). Notice that their distribution seems to depart from
normality at the tails.
Fitted values are plotted in Figure 9.16 along with the observations. These
in-sample one-step predictions are very close to their true values. On the other
hand, Figure 9.17 exhibits the out-of-sample forecasts up to 48 months ahead.
Finally, 95% prediction bands for these forecasts are plotted in Figure 9.18.

SARIMA MODELS
347
Time
Heating degree days
1980
1985
1990
1995
2000
2005
2010
0
100
200
300
400
500
600
700
Figure 9.11
SARIMA heating day degree application: Time series sata.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0e+00
1e+05
2e+05
3e+05
4e+05
5e+05
Frequency
Periodogram
Figure 9.12
SARIMA heating degree day data application: Periodogram.

348
SEASONALITY
Time
Residuals
0
100
200
300
-4
-2
0
2
4
Figure 9.13
SARIMA heating degree day data application: Residuals.
2
4
6
8
10
12
14
-1.0
-0.5
0.0
0.5
1.0
Lag
ACF
2
4
6
8
10
12
14
0.0
0.2
0.4
0.6
0.8
1.0
Lag
Ljung-Box Test
Figure 9.14
SARIMA heating Degree Day Data Application: Diagnostic plots.

SARIMA MODELS
349
-3
-2
-1
0
1
2
3
-150
-100
-50
0
50
100
150
Theoretical Quantiles
Sample Quantiles
Figure 9.15
SARIMA heating degree day data application: QQ plots.
Time
Fitted Values
0
100
200
300
0
100
200
300
400
500
600
700
Figure 9.16
SARIMA heating degree day data application: Fitted values.

350
SEASONALITY
Time
HDD Data and Predictions
0
100
200
300
400
0
100
200
300
400
500
600
700
Figure 9.17
SARIMA heating degree day data application:
Out-of-sample
predictions, 48 months ahead.
Time
Prediction Bands
400
410
420
430
440
0
100
200
300
400
500
600
700
Figure 9.18
SARIMA heating degree day data application: 95% Prediction bands
for the out-of-sample forecasts, 48 months ahead.

SARFIMA MODELS
351
9.2
SARFIMA MODELS
In order to account for both seasonal and storming serial dependence, a motre
general class ofseasonal long-memory processes may be speciﬁed by the spec-
tral density
f(λ) = g(λ)|λ|−α
r
Y
i=1
mi
Y
j=1
|λ −λij|−αi,
(9.1)
where λ ∈(−π, π], 0 ≤α, αi < 1, i = 1, ..., r, g(λ) is a symmetric, strictly
positive, continuous, bounded function and λij ̸= 0 are poles for j = 1, ..., mi,
i = 1, ..., r. To ensure the symmetry of f, we assume that for any i = 1, ..., r,
j = 1, ..., mi, there is one and only one 1 ≤j′ ≤mi such that λij = −λij′.
As shown in the following examples, the spectral densities of many widely
used models such as the seasonal ARFIMA process and the k-factor GARMA
process satisfy (9.1).
Consider a seasonal ARFIMA model with multiple periods s1, . . . , sr:
φ(B)
r
Y
i=1
Φi(Bsi)yt = θ(B)
r
Y
i=1

Θi(Bsi)(1 −Bsi)−dsi
(1 −B)−dεt,
(9.2)
where φ(B), Φi(Bsi), θ(B), Θi(Bsi) are autoregressive and moving-average
polynomials, for i = 1, . . . , r.
The spectral density of the model described by (9.2) is given by
fs1,...,sr(λ) = σ2
2π
|θ(e˙ıλ)|2
|φ(e˙ıλ)|2 |1 −e˙ıλ|−2d
r
Y
i=1
|Θi(e˙ıλsi)|2|1 −e˙ıλsi|−2dsi
|Φi(e˙ıλsi)|2
.
Observe that this spectral density may be written as
fs1,...,sr(λ) = H(λ)|λ|−2d−2ds1−···−2dsr
r
Y
i=1
si
Y
j=1
|λ −λij|−2dsi,
which is a special case of (9.1) where
H(λ)
=
σ2
2π
|θ(e˙ıλ)|2
|φ(e˙ıλ)|2
r
Y
i=1
|Θi(e˙ıλsi)|2
|Φi(e˙ıλsi)|2
×
|λ|2d+2ds1+···+2dsr Qr
i=1
Qsi
j=1 |λ −λij|2dsi
|1 −e˙ıλ|2d Qr
i=1 |1 −e˙ıλsi|2dsi
,
and λij = 2πj/si for i = 1, ..., r, j = 1, ..., [si/2], λij = 2π([si/2] −j)/si for
i = 1, ..., r, j = [si/2] + 1, . . . , si, α = 2d + 2ds1 + · · · + 2dsr, and αi = 2dsi.
From Figure 9.19 and Figure 9.20 we may visualize the shape of the spectral
density of a SARFIMA model for two sets of parameters. Figure 9.19 displays

352
SEASONALITY
the spectral density of a SARFIMA(0, d, 0) × (0, ds, 0)s process with d = 0.1,
ds = 0.3, s = 10, and σ2 = 1. On the other hand, Figure 9.20 shows the
spectral density of a SARFIMA(0, d, 0) × (1, ds, 1)s process with d = 0.1,
ds = 0.3, Φ = −0.8, Θ = 0.1, s = 10, and σ2 = 1. As expected, these plots
have poles at the frequencies λ = 2πj/s, j = 0, 1, . . . , 5.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
1
2
3
4
5
6
Frequency
Spectral density
Figure 9.19
Spectral density of a SARFIMA(0, d, 0) × (0, ds, 0)s process with
d = 0.1, ds = 0.3, and s = 10.
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0
2
4
6
8
10
Frequency
Spectral density
Figure 9.20
Spectral density of a SARFIMA(0, d, 0) × (1, ds, 1)s process with
d = 0.1, ds = 0.3, Φ = −0.8, Θ = 0.1 and s = 10.

GARMA MODELS
353
9.3
GARMA MODELS
The spectral density of a k-factor GARMA process is given by
f(λ) = c|θ(e˙ıλ)|2|φ(e˙ıλ)|−2
k
Y
j=1
| cos λ −uj|−dj,
(9.3)
where c > 0 is a constant, uj are distinct values, dj ∈(0, 1
2) when |uj| = 1,
and dj ∈(0, 1) when |uj| ̸= 1.
For |uj| ≤1, we may write uj = cos λj and this spectral density may be
written in terms of (9.1) as follows:
f(λ) = H(λ)
k
Y
j=1
|λ −λj|−dj|λ + λj|−dj,
where
H(λ) = c|θ(e˙ıλ)|2|φ(e˙ıλ)|−2
k
Y
j=1

cos λ −cos λj
λ2 −λ2
j

−dj
is a strictly positive, symmetric, continuous function with
lim
λ→±λℓH(λ) = c |θ(e˙ıλℓ)|2
|φ(e˙ıλℓ)|2

sin λℓ
2λℓ

−d
k
Y
j̸=ℓ

cos λℓ−cos λj
λ2
ℓ−λ2
j

−dj
,
for λℓ̸= 0 and for λℓ= 0
lim
λ→0 H(λ) = 2dℓc|θ(1)|2|φ(1)|−2
k
Y
j̸=l

1 −cos λj
λ2
j

−dj
.
Observe that all these limits are ﬁnite and H(λ) is a bounded function.
Figure 9.21 depicts the spectral density of a k-factor GARMA process with
k = 1, λ1 = π/4, and d1 = 0.1. Notice that there is only one pole located at
frequency π/4.
When the singularities λij are known, the exact maximum-likelihood es-
timators of Gaussian time series models with spectral density (9.1) have the
following large-sample properties. Let bθn be the exact MLE and θ0 the true
parameter. Then, under some regularity conditions we have
(a) Consistency: bθn→θ0 in probability as n →∞.
(b) Normality: √n(bθn −θ0) →N(0, Γ−1(θ0)), as n →∞, where Γ(θ) =
(Γij(θ)) with
Γij(θ) = 1
4π
Z π
−π
∂log fθ(λ)
∂θi
 ∂log fθ(λ)
∂θj

dλ,
(9.4)

354
SEASONALITY
Frequency
Spectral Density
0.0
0.5
1.0
1.5
2.0
2.5
3.0
0.15
0.20
0.25
0.30
0.35
0.40
0.45
Figure 9.21
Spectral density of a k-factor GARMA process with k = 1, λ1 = π/4,
and d1 = 0.1.
and fθ(λ) is the spectral density (9.1).
(c) Eﬃciency: bθn is an eﬃcient estimator of θ0.
When the location of the pole is unknown, we still can obtain similar large-
sample properties for quasi-maximum-likelihood parameter estimates. Con-
sider, for example, the class of long-memory seasonal models deﬁned by the
spectral density
f(λ) = σ2
2π
4 sin λ + ω
2
sin λ −ω
2

−α
|h(λ, τ)|2,
(9.5)
where θ = (α, τ) are the parameters related to the long-memory and the
short-memory components, and ω denotes the unknown location of the pole.
Deﬁne the function
S(θ, ω) = 1
en
en
X
j=0
I(λj)
k(λj, θ, ω),
(9.6)
where en = [n/2], I(λj) is the periodogram given by (4.6) evaluated at the
Fourier frequency λj = 2πj/n and
k(λ, , θ, ω) = 2π
σ2 f(λ).

CALCULATION OF THE ASYMPTOTIC VARIANCE
355
Consider the estimators bθn and bωn which minimize S(θ, ω):
 bθn
bωn

= argminΘ×QS(θ, λq),
where Q = {q : q = 0, 1, . . . , en}. Notice that ω belongs to a discrete set of
frequencies λ0, . . . , λen.
9.4
CALCULATION OF THE ASYMPTOTIC VARIANCE
Analytic expressions for the integral in (9.4) are diﬃcult to obtain for an
arbitrary period s. For a SARFIMA(0, d, 0) × (0, ds, 0)s model, the matrix
Γ(θ) may be written as
Γ(θ) =


π2
6
c(s)
c(s)
π2
6

,
(9.7)
with c(s) = (1/π)
R π
−π{log |2 sin(λ/2)|}{log |2 sin[s(λ/2)]|}dλ. An interesting
feature of the asymptotic variance-covariance matrix of the parameter esti-
mates (9.7) is that for a SARFIMA(0, d, 0) × (0, ds, 0)s process, the exact
maximum-likelihood estimators bd and bds have the same variance.
An explicit expression for this integral can be given for s = 2. In this case,
Γ(θ) = π2
12
 2
1
1
2

.
For other values of s, the integral may be evaluated numerically. For in-
stance, Figure 9.22 shows the evolution of Var(bd) as a function of the period
s [see panel (a)] and the evolution of Cov(bd, bds) as s increases [see panel (b)].
Both curves are based on the numerical evaluation of equation (9.7) and then
inverting this matrix to obtain the asymptotic variance-covariance matrix of
the parameters.
Observe that Var(bds), equivalently Var(bd), starts at a value of 8/π2 and
decreases to 6/π2 as s →∞. That is, for a very large period s, the asymptotic
variance of bds is the same as the variance of bd from an ARFIMA(0, d, 0) model.
9.5
AUTOCOVARIANCE FUNCTION
Finding explicit formulae for the ACF of a general seasonal model is rather
diﬃcult.
However, we can obtain an asymptotic expression as the lag in-
creases. Assume that α1 > α2 ≥· · · ≥αr. Let c0, c1, . . . , cm1 be constants.
Then, for large lag h the autocovariance function γ(h) satisﬁes

356
SEASONALITY
0
3
0
2
0
1
0
0.65
0.70
0.75
0.80
(a)
0
3
0
2
0
1
0
-0.4
-0.3
-0.2
-0.1
(b)
Figure 9.22
(a) Values of Var( bds) as a function of the period s and (b) values
of Cov( bd, bds) as a function of s.
(a) If α > α1,
γ(h) ∼c0 |h|α−1.
(b) If α ≤α1,
γ(h) ∼|h|α1−1

c01{α=α1} +
m1
X
j=1
cj cos(hλ1j)

.
Notice that the large lag behavior of the ACF depends on the maximum
value of the exponents α, α1, . . . , αr.
For the SARFIMA process with 0 < d, ds1, . . . , dsr < 1
2 and d + ds1 + · · · +
dsr <
1
2 the maximum exponent is always reached at zero frequency since
α = d+ds1 +· · ·+dsr. Therefore in that case for large lag h the ACF behaves
like
γ(h) ∼c0 |h|2d+2ds1+···+2dsr −1.

AUTOCOVARIANCE FUNCTION
357
EXAMPLE 9.4
As an illustration of the shape of the autocovariance function of a sea-
sonal long-memory process consider the SARFIMA(0, d, 0) × (0, ds, 0)s
process described by the discrete-time equation
yt = (1 −Bs)−ds(1 −B)−dεt,
where {εt} is a zero-mean and unit variance white noise.
In what follows, we plot the theoretical autocovariance function for
three particular cases of this model from lag h = 1 to lag h = 500.
The values of the ACF were calculated following the splitting method
described in Subsection 5.10.11.
Figure 9.23 displays the theoretical ACF of a SARFIMA(0, d, 0) ×
(0, ds, 0)s process with parameters d = 0.1, ds = 0.3, and s = 12.
Using the same method, Figure 9.24 shows the theoretical ACF of
a SARFIMA (0, d, 0)×(0, ds, 0)s process with parameters d = 0.3, ds =
0.15, and s = 12.
Finally, Figure 9.25 exhibits the theoretical ACF of a SARFIMA(0, d, 0)×
(0, ds, 0)s process with parameters d = 0.05, ds = 0.44, and s = 24.
Lag
ACF
0
100
200
300
400
500
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
Figure 9.23
Autocovariance function of a SARFIMA(0, d, 0) × (0, ds, 0)s process
with d = 0.1, ds = 0.3, and s = 12.

358
SEASONALITY
Lag
ACF
0
100
200
300
400
500
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
Figure 9.24
Autocovariance function of a SARFIMA(0, d, 0) × (0, ds, 0)s process
with d = 0.3, ds = 0.15, and s = 12.
Lag
ACF
0
100
200
300
400
500
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Figure 9.25
Autocovariance function of a SARFIMA(0, d, 0) × (0, ds, 0)s process
with d = 0.05, ds = 0.44, and s = 24.

MONTE CARLO STUDIES
359
9.6
MONTE CARLO STUDIES
In order to assess the ﬁnite sample performance of the ML estimates in the
context of long-memory seasonal series, we show a number of Monte Carlo
simulations for the class of SARFIMA models described by the diﬀerence
equation
yt −µ = (1 −Bs)−ds(1 −B)−dεt,
(9.8)
where µ is the mean of the series, and {εt} are independent and identically
distributed normal random variables with zero-mean and unit variance.
Table 9.3 to Table 9.6 report the results from the Monte Carlo simulations
for the SARFIMA(0, d, 0) × (0, ds, 0)s process (9.8) with mean µ = 0 assumed
to be either known or unknown depending on the experiment, for diﬀerent
values of d, ds, sample size n, seasonal period s. The white noise variance is
σ2 = 1 in all the simulations.
The ﬁnite sample performance of the MLE is compared to the Whittle
estimate and the Kalman ﬁlter approach with truncation m = 80.
The results are based on 1000 repetitions, with seasonal series generated
using the Durbin-Levinson algorithm with zero-mean and unit variance Gaus-
sian noise.
Table 9.3
SARFIMA Simulations: Sample Size n = 256 and Seasonal Period s = 6
Known Mean
Exact
Whittle
Kalman
d
ds
bd
bds
bd
bds
bd
bds
0.1
0.3
Mean
0.0945
0.2928
0.0590
0.2842
0.0974
0.3080
S.D
0.0020
0.0018
0.0024
0.0039
0.0024
0.0026
0.2
0.2
Mean
0.1924
0.1901
0.1574
0.1602
0.2098
0.1909
S.D
0.0023
0.0023
0.0034
0.0036
0.0022
0.0028
0.3
0.1
Mean
0.2924
0.0948
0.2591
0.0610
0.3046
0.1003
S.D
0.0022
0.0022
0.0035
0.0024
0.0024
0.0023
Unknown Mean
Exact
Whittle
Kalman
bd
bds
bd
bds
bd
bds
0.1
0.3
Mean
0.0806
0.2842
0.0590
0.2842
0.0749
0.2987
S.D
0.0020
0.0020
0.0024
0.0039
0.0020
0.0030
0.2
0.2
Mean
0.1768
0.1812
0.1574
0.1601
0.1851
0.1799
S.D
0.0027
0.0024
0.0034
0.0036
0.0018
0.0029
0.3
0.1
Mean
0.2755
0.0863
0.2591
0.0610
0.2800
0.0867
S.D
0.0025
0.0022
0.0035
0.0024
0.0027
0.0022

360
SEASONALITY
Table 9.4
SARFIMA Simulations: Sample Size n = 256 and Seasonal Period
s = 10
Known Mean
Exact
Whittle
Kalman
d
ds
bd
bds
bd
bds
bd
bds
0.1
0.3
Mean
0.0955
0.2912
0.0599
0.2886
0.0991
0.3175
S.D
0.0022
0.0016
0.0025
0.0032
0.0032
0.0022
0.2
0.2
Mean
0.1975
0.1916
0.1583
0.1640
0.2005
0.1963
S.D
0.0022
0.0020
0.0034
0.0032
0.0027
0.0026
0.3
0.1
Mean
0.2947
0.0953
0.2601
0.0621
0.2978
0.1014
S.D
0.0022
0.0022
0.0037
0.0023
0.0026
0.0023
Unknown Mean
Exact
Whittle
Kalman
bd
bds
bd
bds
bd
bds
0.1
0.3
Mean
0.0806
0.2840
0.0599
0.2886
0.0725
0.3110
S.D
0.0023
0.0017
0.0025
0.0032
0.0025
0.0025
0.2
0.2
Mean
0.1814
0.1837
0.1583
0.1640
0.1811
0.1894
S.D
0.0025
0.0021
0.0034
0.0032
0.0030
0.0026
0.3
0.1
Mean
0.2781
0.0871
0.2601
0.0621
0.2698
0.0897
S.D
0.0027
0.0022
0.0037
0.0023
0.0028
0.0022
Table 9.5
SARFIMA Simulations: Sample Size n = 512 and Seasonal Period s = 6
Known Mean
Exact
Whittle
Kalman
d
ds
bd
bds
bd
bds
bd
bds
0.1
0.3
Mean
0.0995
0.2951
0.0803
0.2942
0.1057
0.3118
S.D
0.0012
0.0010
0.0014
0.0016
0.0012
0.0013
0.2
0.2
Mean
0.1966
0.1977
0.1795
0.1839
0.1952
0.2021
S.D
0.0013
0.0011
0.0017
0.0015
0.0014
0.0013
0.3
0.1
Mean
0.2962
0.0980
0.2811
0.0792
0.3060
0.0964
S.D
0.0011
0.0011
0.0014
0.0013
0.0014
0.0012
Unknown Mean
Exact
Whittle
Kalman
bd
bds
bd
bds
bd
bds
0.1
0.3
Mean
0.0919
0.2900
0.0803
0.2942
0.0870
0.3045
S.D
0.0012
0.0010
0.0014
0.0016
0.0011
0.0013
0.2
0.2
Mean
0.1880
0.1923
0.1795
0.1839
0.1765
0.1943
S.D
0.0014
0.0012
0.0017
0.0015
0.0011
0.0013
0.3
0.1
Mean
0.2878
0.0932
0.2811
0.0792
0.2849
0.0864
S.D
0.0012
0.0012
0.0014
0.0013
0.0014
0.0012

MONTE CARLO STUDIES
361
Table 9.6
SARFIMA Simulations: Sample Size n = 512 and Seasonal Period
s = 10
Known Mean
Exact
Whittle
Kalman
d
ds
bd
bds
bd
bds
bd
bds
0.1
0.3
Mean
0.0979
0.2959
0.0768
0.3006
0.0995
0.3134
S.D
0.0012
0.0009
0.0014
0.0016
0.0016
0.0014
0.2
0.2
Mean
0.1994
0.1957
0.1813
0.1832
0.2028
0.2007
S.D
0.0012
0.0011
0.0016
0.0016
0.0014
0.0014
0.3
0.1
Mean
0.2963
0.0968
0.2801
0.0783
0.3070
0.0948
S.D
0.0011
0.0012
0.0015
0.0014
0.0015
0.0013
Unknown Mean
Exact
Whittle
Kalman
bd
bds
bd
bds
bd
bds
0.1
0.3
Mean
0.0896
0.2913
0.0768
0.3006
0.0799
0.3074
S.D
0.0012
0.0009
0.0014
0.0016
0.0014
0.0014
0.2
0.2
Mean
0.1908
0.1908
0.1813
0.1832
0.1809
0.1931
S.D
0.0013
0.0012
0.0016
0.0016
0.0015
0.0014
0.3
0.1
Mean
0.2876
0.0921
0.2801
0.0783
0.2890
0.0862
S.D
0.0012
0.0012
0.0015
0.0014
0.0013
0.0012
The autocovariance function was computed by the convolution method of
Subsection 5.10.11.
In order to explore the eﬀect of the estimation of the mean we have con-
sidered two situations: known mean where the process is assumed to have
zero-mean and unknown mean where the expected value of the process is
estimated by the sample mean and then centered before the computations.
The exact MLE method has been implemented computationally by means
of the Durbin-Levinson algorithm discussed in Chapter 5 with autocovariance
calculated by the approach given in Subsection 5.10.11.
The Whittle method has been implemented by minimizing the following
expression; see Section 5.8 and equation (9.6):
S(θ) = 2
n
[n/2]
X
k=1
I(λk)
fθ(λk),
where θ = (d, ds) and λk = 2πk/n, with periodogram given by
I(λk) =
1
2πn

n
X
t=1
yte˙ıtλk

2
,

362
SEASONALITY
Table 9.7
Asymptotic Standard Deviation of bd and bds
n
s = 6
s = 10
256
0.0503
0.0503
512
0.0356
0.0356
see deﬁnition (4.6), and spectral density
fθ(λ) = 1
2π
1 −e˙ıλ
−2d 1 −e˙ıλs
−2ds
.
The approximate Kalman ﬁlter ML estimates are based on a ﬁnite state
space representation of the truncated MA(∞) expansion described in Sec-
tion 5.10.4.
From Table 9.3 to Table 9.6, it seems that for the known mean case the
exact MLE and the Kalman methods display little bias for both sample sizes.
On the other hand, the Whittle method presents a noticeable downward bias
for both estimators bd and bds.
The sample standard deviations of the estimates are close to their theo-
retical counterparts, reported in Table 9.7, for the three methods considered.
However, the exact MLE seems to have slightly lower sample standard devi-
ations than the other methods, for both long-memory parameters and both
sample sizes. The theoretical values of the standard deviations of the esti-
mated parameters given in Table 9.7 are based on formula (9.7).
In the unknown mean case, all the estimates seem to display a downward
bias, which is stronger for the Whittle method. However, the bias displayed
by this estimate is similar to the known mean case, since the Whittle algo-
rithm is not aﬀected by the estimation of the mean. Similarly to the previous
case, the estimated standard deviations are comparable to the theoretical val-
ues, and the exact MLE displays slightly lower sample standard deviations
than the other methods for most long-memory parameters and sample size
combinations.
9.7
ILLUSTRATION
In this section we apply the maximum-likelihood estimation to the analysis
of a time series consisting of hyper text transfer protocol (HTTP) requests to
a World Wide Web server at the University of Saskatchewan. It has been
reported that communication network traﬃc may exhibit long-memory be-
havior. The data analyzed here consist of the logarithm of the number of
requests within one-hour periods.
The Internet traﬃc series is shown in Figure 9.26 while its sample autocor-
relation function is displayed in Figure 9.27. Observe that the sample ACF

ILLUSTRATION
363
Time (hours)
Log number of requests
0
1000
2000
3000
4000
10
12
14
16
Figure 9.26
Logarithm of HTTP requests time series data.
decays slowly and exhibits a 24-hour periodicity. To account for these features
we ﬁt a SARFIMA model to this time series.
Table 9.8 reports the maximum-likelihood parameter estimates and the
t-tests for the SARFIMA(1, 0, 1) × (0, ds, 0)s with s = 24 process:
(1 −φB)(yt −µ) = (1 −θB)(1 −Bs)−dsεt,
where εt is a white noise sequence with variance σ2.
This model was selected by means of the Akaike’s information criterion.
From Table 9.8, notice that all the parameters included in the model are
signiﬁcant at the 5% level. The Student-t values reported on Table 9.8 are
based on the numerical calculation of the inverse of the Hessian matrix, which
approximates the asymptotic variance-variance matrix of the parameters.
Table 9.8
Log Internet Traﬃc Data:
Maximum-Likelihood Estimation of the
SARFIMA(1, 0, 1) × (0, ds, 0)s Model
Parameter
ds
φ
θ
Estimate
0.4456
0.8534
0.3246
Student-t
2.2558
7.5566
2.8623

364
SEASONALITY
0
20
40
60
80
100
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
Figure 9.27
Sample ACF of logarithm HTTP requests time series data.
The standard deviation of the Internet traﬃc series is 0.6522 while the
residual standard deviation is 0.2060. Thus, the ﬁtted seasonal long-memory
model explains roughly two thirds of the total standard deviation of the data.
9.8
BIBLIOGRAPHIC NOTES
Methods for estimating and forecasting SARIMA models are described in the
book by Brockwell and Davis (1991)
as well as in the monograph by Box,
Jenkins, and Reinsel (1994).
Long-range-dependent data with seasonal be-
havior have been reported ﬁelds as diverse as economics, physics, and hydrol-
ogy. For example, inﬂation rates are studied by Hassler and Wolters (1995),
revenue series are analyzed by Ray (1993a), monetary aggregates are consid-
ered by Porter-Hudak (1990), quarterly gross national product and shipping
data are discussed by Ooms (1995), and monthly ﬂows of the Nile River are
studied by Montanari, Rosso, and Taqqu (2000).
Many statistical methodologies have been proposed to model this seasonal
long-range-dependent data.
For example, Abrahams and Dempster (1979)
extend the fractional Gaussian noise process [see Mandelbrot and Van Ness
(1968)] to include seasonal components.
On the other hand, Gray, Zhang, and Woodward (1989) propose the gen-
eralized fractional or Gegenbauer processes (GARMA), Porter-Hudak (1990)
discusses (SARFIMA) models, Hassler (1994) introduces the ﬂexible seasonal

PROBLEMS
365
fractionally integrated processes (ﬂexible ARFISMA), and Woodward, Cheng,
and Gray (1998) introduce the k-factor GARMA processes.
Furthermore, the statistical properties of these models have been investi-
gated by Giraitis and Leipus (1995), Chung (1996), Giraitis, Hidalgo, and
Robinson (2001), and Palma and Chan (2005), among others. Finite sam-
ple performances of a number of estimation techniques for fractional seasonal
models are studied in the papers by Reisen, Rodrigues, and Palma (2006a,b).
Problems
9.1
Let yt = εt + ϕ yt−s be a SARIMA(0, 0, 0) × (1, 0, 0)s model where the
integer s corresponds to the seasonal period. Suppose that |ϕ| < 1 and that
Var(εt) = σ2.
(a) Show that the autocovariance function of this process is given by
γ(h) =
σ2
1 −ϕ2 ϕ[ h
s ],
where [·] denotes the integer function.
(b) Find the spectral density of this seasonal process.
9.2
Let zt = Pn
j=1(Aj cos(λjt)+Bj sin(λjt)), where t = 0, ±1, . . . y λ1, λ2, . . . λn
are positive constant and Aj, Bj are independent random variables, with zero-
mean and variance σ2
j = Var(Aj) = Var(Bj), j = 1, . . . , n.
(a) Is this stationary process?
(b) Find the mean and autocovariance function of zt
9.3
Consider the time series{yt}:
yt
=
A sin(ωt) + xt,
xt
=
a0 + a1 t + a2 t2 + ηt,
where A is a random variable and {ηt} is an integrated process of order 2,
with E(ηt) = 0 for all t. That is, (1 −B2)ηt is a stationary process.
(a) Find the values of the frequency ω satisfying
(1 −B2)yt = (1 −B2)xt.
(b) Verify that for the values of ω found in part (a) we have that
zt ≡(1 −B2)(1 −B)2yt,
is a stationary process.
(c) Calculate the expected value µ = E(zt).
(d) Is the process {zt −µ} a white noise sequence?

366
SEASONALITY
9.4
Let {xt} be the time series deﬁned by
xt
=
A cos(πt/3) + B sin(πt/3) + yt
where yt = zt + 2.5zt−1, {zt} ∼WN(0, σ2), A and B are uncorrelated with
zero-mean and variance ν2, and zt is uncorrelated with A and B for each t.
Find the ACF of {xt}.
9.5
Design a symmetric moving-average ﬁlter which removes seasonal com-
ponents with period 3 and, simultaneously, it does not remove quadratic
trends.
9.6
For which values of α, α1, . . . , αr does the fractional seasonal model
exhibits long memory?
9.7
Let s be a positive integer and d ∈(0, 1
2). Calculate the coeﬃcients ψj
in the expansion
(1 −Bs)−d =
∞
X
j=1
ψjBj.
9.8
Let s be a positive integer and d < 0.
(a) Prove that the expansion
(1 −Bs)d =
∞
X
j=1
πjBj,
is absolutely convergent.
(b) Let z be a random variable such that E(z2) < ∞. Show that
(1 −Bs)dz = 0.
(c) Consider the equation
(1 −Bs)dyt = εt,
(9.9)
where {εt} is a white noise sequence with ﬁnite variance. Prove that
there is a stationary solution {yt} of equation (9.9).
(d) Show that the process {xt} deﬁned as xt = yt +z is also a stationary
solution of equation (9.9).
9.9
Consider the SARFIMA(0, 0, 0) × (0, ds, 0)s process
yt = (1 −Bs)−dsεt,
where εt is a white noise sequence with variance σ = 1. Let γs(k) be the ACF
of the process yt and γ(k) be the ACF of a fractional noise FN(d) process
with unit noise variance. Verify that
γs(k) = γ(sk).

PROBLEMS
367
9.10
Consider the SARFIMA(2, d, 2) × (0, ds, 0)s process
(1 −φ1B −φ2B2)yt = (1 −θ1B −θ2B2)(1 −Bs)−dsεt,
where εt is a white noise sequence with variance σ and
θ = (φ1, φ2, d, θ1, θ2, ds, σ) = (0.2, 0.5, 0.3, −0.2, 0.5, 0.1, 1.3).
(a) Is this process stationary?
(b) Is this process invertible?
9.11
Calculate numerically the variance-covariance matrix Γ(θ) given in
(9.7) for a SARFIMA(0, d, 0) × (0, ds, 0)s process with θ = (d, ds) = (0.1, 0.2)
and s = 12.
9.12
Simulate a sample of 1000 observations from a Gaussian SARFIMA(1, d, 1)×
(0, ds, 0)s process with parameters
θ = (φ1, θ1, d, ds, σ) = (0.6, 0.3, 0.2, 0.2, 1).
9.13
Implement computationally the splitting method for calculating the
theoretical ACF of a SARFIMA(0, d, 0) × (0, ds, 0)s process.
9.14
Write a state space system for the SARFIMA process described in the
previous problem.
9.15
Calculate the MLE for the SARFIMA process in Problem 9.12 by
means of the state space systems and the Whittle method.
9.16
Suppose that T is the variance-covariance matrix of a stationary Gaus-
sian process {y1, y2, . . . , yn} with fractional seasonal spectral density.
(a) Let |T| be the Euclidean norm of T, that is,
|T| = [tr(TT ∗)]1/2.
Show that
|T|
2 =
n
X
i=1
n
X
j=1
γ2(i −j) = n
n−1
X
j=0

1 −j
n

γ2(j).
(b) Verify that yields
γ(j) ≤Kj eα−1,
for j ≥1, where eα = max{α, α1, . . . , αr}.
(c) Show that
|T|
2 ≤n
n−1
X
j=0
γ2(j) ≤Kn
n
X
j=1
j2eα−2.

368
SEASONALITY
(d) Verify that
|T|
2 ≤Kn2eα,
and then
|T| ≤Kn.
9.17
Consider the GARMA process described by the spectral density
f(λ) = H(λ)|λ −λ1|−d1|λ + λ1|−d1,
where H(λ) is a C∞([−π, π]) function. Verify that the ACF of this process
satisﬁes the asymptotic expression
γ(h) ∼c1 cos(hλ1)|h|2d1−1,
as |h| →∞.

CHAPTER 10
TIME SERIES REGRESSION
In the previous chapters we have studied methods for dealing with serially
dependent time series data but we have not discussed the problem of relating
those time series to other covariates or trends. However, in many practical
applications, the behavior of a time series may be related to the behavior of
other data. A widely used approach to model these relationships is the linear
regression analysis. Consequently, in this chapter we explore several aspects
of the statistical analysis of linear regression models with serially dependent
errors. A motivating example is discussed in Section 10.1. Some essential
deﬁnitions about the model under study are given in Section 10.2. We then
proceed to the analysis of some large sample properties of the least squares
estimators (LSE) and the best linear unbiased estimators (BLUE). Aspects
such as strong consistency, the asymptotic variance of the estimators, normal-
ity, and eﬃciency are discussed in Section 10.3 for the LSE and in Section 10.4
for the BLUE. Sections 10.5–10.7 present some important examples, including
the estimation of the mean, the polynomial, and the harmonic regression. A
real life data application to illustrate these regression techniques is presented
in Section 10.8 while some references and further readings are discussed in the
Section 10.9. This chapter concludes with a section of proposed problems.
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
369

370
TIME SERIES REGRESSION
10.1
MOTIVATION
Consider the time series regression model
yt = xt β + εt,
where xt is a regressor and εt is a stationary error sequence. A very simple
example of this model is obtained under two essential assumptions: that the
regression variable xt is deterministic or known and that the error εt is a
sequence of i.i.d. random variables with zero-mean and variance σ2. Under
this assumptions, the best estimate of the parameter β in the sense of unbiased
and minimal variance corresponds to the least squares estimator (LSE) given
by
bβ =
Pn
t=1 yt xt
Pn
t=1 x2
t
.
Furthermore, the variance of this estimator is
Var(bβ) =
σ2
Pn
t=1 x2
t
.
On the other hand, if the error sequence is correlated, the variance of the LSE
diﬀers from the previous formula. For instance, assume that εt satisﬁes an
AR(1) process with autoregressive parameter φ,
εt = φ εt−1 + et,
where the noise et is an i.i.d. sequence with zero-mean and variance (1−φ2)σ2.
Notice that given this parametrization of the variance of εt, we obtain that the
variance of εt is σ2, making both cases comparable. For h = 1 −n, . . . , n −1
deﬁne
αh =
n−|h|
X
t=1
xt xt+|h|,
so that we can write the variance of the LSE in this case as
Var(bβ) =
Pn−1
h=1−n αhγ(h)
Pn
t=1 x2
t
.
Consequently, if r denotes the ratio between the two variances, we have that
r = 1
σ2
n−1
X
h=1−n
αhγ(h).

MOTIVATION
371
Consider the simple case where xt = 1 for all h = 1, . . . , n. Then, after some
algebra, this quantity can be written as
r =
1 + φ −2φn
(1 + φ)(1 −φ)2 .
Since |φ| < 1 we have that as the sample size increases to inﬁnity
r =
1
(1 −φ)2 .
In turn, as φ approaches 1, r tends to inﬁnity. Thus, as the level of serial
dependence of the error sequence increases, the relative imprecision of the
LSE increases as well. The lesson provided by this illustration is that serial
dependence aﬀects the quality of the estimates.
As an illustration, Figure 10.1 exhibits two examples of time series re-
gressions for the linear trend with β = 2, σ = 1 with 200 observations.
Figure 10.1(a) shows then case of independent Gaussian noise while Fig-
ure 10.1(b) displays the regression with AR(1) dependent Gaussian errors
with autoregressive parameter φ = 0.95. Notice that the values on panel (b)
exhibit much more level of persistence, that is, the values tend to stay close
to their respective neighbors.
(a)
Time
yt
0
50
100
150
200
-2
0
2
4
(b)
Time
yt
0
50
100
150
200
-2
0
2
4
Figure 10.1
Time series regressions.
Linear trend with β = 2 and σ =
1.
(a) Independent Gaussian noise.
(b) AR(1) dependent Gaussian errors with
autoregressive parameter φ = 0.95.

372
TIME SERIES REGRESSION
Table 10.1
Time series regression with constant and ﬁrst-order autoregressive
errors. Estimates of β for diﬀerent values of φ.
φ
Independent
AR(1)
SD Independent
SD AR(1)
Ratio r
0.1000
2.0014
2.0014
0.0706
0.0782
1.2247
0.3000
2.0029
2.0040
0.0694
0.0939
1.8309
0.5000
1.9987
1.9989
0.0623
0.1069
2.9508
0.8000
2.0089
2.0274
0.0721
0.2154
8.9155
A similar exercise can be carried out considering the best linear unbiased
estimator (BLUE) instead of the LSE.
To illustrate these issues, Table 10.1 and Table 10.2 report the results from
the simulations of two regressions, a constant mean model
yt = β + εt,
with β = 2, for t = 1, . . . , n, and the linear trend model
yt = β t
n + εt,
with β = 2, n = 200 and t = 1, . . . , n. For the dependent error case, we
consider the Gaussian AR(1) model with and σ = 1 and four diﬀerent values
for the autoregressive parameter φ, from a low level of dependence φ = 0.1 to
a high level of autocorrelation φ = 0.8.
The results reported in both tables are based on 1000 repetitions. Notice
that the least squares estimates of the regression parameter β seem to be
unbiased in both tables and across the distinct values of φ. The eﬀects of the
error dependence can be observed in the levels of estimation error standard
deviations. In particular, notice that in both Monte Carlo simulation sets the
ratio r increases as the level of autocorrelation φ increases.
Table 10.2
Time series regression with linear trend and ﬁrst-order autoregressive
errors. Estimates of β for diﬀerent values of φ.
φ
Independent
AR(1)
SD Independent
SD AR(1)
Ratio r
0.1000
2.0021
2.0024
0.1253
0.1384
1.2195
0.3000
1.9868
1.9823
0.1185
0.1611
1.8482
0.5000
1.9943
1.9904
0.1250
0.2153
2.9667
0.8000
1.9993
1.9996
0.1195
0.3527
8.7111

DEFINITIONS
373
10.2
DEFINITIONS
Consider the linear regression model
yt = xtβ + εt,
(10.1)
for t = 1, 2, . . . , where xt = (xt1, . . . , xtp) is a sequence of regressors, β ∈Rp is
a vector of parameters, and {εt} is a long-range-dependent stationary process
with spectral density
f(λ) = |1 −e˙ıλ|−2df0(λ),
(10.2)
where f0(λ) is a symmetric, positive, piecewise continuous function for λ ∈
(−π, π] and 0 < d < 1
2.
The least squares estimator (LSE) of β is given by
bβn = (X′
nXn)−1X′
nY n,
(10.3)
where Xn is the n × p matrix of regressors, [Xn]ij = xij, i = 1, . . . , n, j =
1, . . . , p, and Y n = (y1, . . . , yn)′. The variance of the LSE is
Var(bβn) = (X′
nXn)−1X′
nΓXn(X′
nXn)−1,
(10.4)
where Γ is the variance-covariance matrix of {yt} with elements
Γij =
Z π
−π
f(λ)e˙ıλ(i−j)dλ.
(10.5)
On the other hand, the BLUE of β is
eβn = (X′
nΓ−1Xn)−1X′
nΓ−1Y n,
(10.6)
which has variance equal to
Var(eβn) = (X′
nΓ−1Xn)−1.
(10.7)
10.2.1
Grenander Conditions
In order to analyze the large sample properties of the LSE and the BLUE, we
introduce the following so-called Grenander conditions on the regressors. Let
Xn(j) be the jth column of the design matrix, Xn.
(1) ∥Xn(j)∥→∞as n →∞for j = 1, . . . , p.
(2) limn→∞∥Xn+1(j)∥/∥Xn(j)∥= 1 for j = 1, . . . , p.
(3) Let Xn,h(j) = (xh+1,j, xh+2,j, . . . , xn,j, 0 . . . , 0)′ for h ≥0 and Xn,h(j) =
(0, . . . , 0, x1,j, x2,j, . . . , xn+h,j)′ for h < 0. Then, there exists a p × p
ﬁnite matrix R(h) such that
⟨Xn,h(i), Xn(j)⟩
∥Xn,h(i)∥∥Xn(j)∥→Rij(h),
(10.8)

374
TIME SERIES REGRESSION
as n →∞with ⟨x, y⟩= x¯y for complex numbers x, y ∈C where ¯y is the
complex conjugate of y.
(4) The matrix R(0) is nonsingular.
Notice ﬁrst that under conditions (1)–(4), the matrix R(h) may be written
as
R(h) =
Z π
−π
e˙ıhλ dM(λ),
(10.9)
where M(λ) is a symmetric matrix function with positive semideﬁnite incre-
ments.
The asymptotic properties of the LSE estimates and the BLUE depend
upon the behavior of Mjj(λ) around frequency zero. Consequently, assume
that for j = 1, . . . , s, Mjj(λ) suﬀers a jump at the origin, that is,
Mjj(0+) > Mjj(0),
j = 1, . . . , s,
and for j = s + 1, . . . , p, Mjj(λ) is continuous at λ = 0, that is,
Mjj(0+) = Mjj(0),
j = s + 1, . . . , p,
where Mjj(0+) = limλ→0+ Mjj(λ).
Before stating the asymptotic behavior of these estimators, the following
deﬁnitions and technical results are needed. Deﬁne the characteristic function
of the design matrix by
M n
ij(λ) =
Z λ
−π
mn
ij(ω) dω,
where
mn
ij(λ) = ⟨Pn
t=1 xtie−˙ıtλ, Pn
t=1 xtje−˙ıtλ⟩
2π∥Xn(i)∥∥Xn(j)∥
.
Also, we deﬁne the function δ(·) such that δ(x) = 1 for x = 0 and δ(x) = 0
for x ̸= 0.
Notice that by Problem 10.14
Z π
−π
g(λ) dM n(λ) →
Z π
−π
g(λ) dM(λ),
as n →∞for any continuous function g(λ) with λ ∈[−π, π].

PROPERTIES OF THE LSE
375
10.3
PROPERTIES OF THE LSE
The large sample behavior of the LSE is studied in this section.
In what follows, the regressors are assumed to be nonstochastic.
Let
σn = λmin(X′
nXn) be the smallest eigenvalue of X′
nXn and let β be the
true parameter. Under some conditions on σn, see Problem 10.17, the LSE
is consistent.
Finding the eigenvalues of the matrix X′
nXn may not be a
simple task in some situations. Fortunately, there is a set of conditions that
guarantees the strong consistency of the LSE which are simpler to verify, see
Problem 10.18.
10.3.1
Asymptotic Variance
The asymptotic variance of the LSE is analyzed in this section. Consider now
the following assumption. For any δ > 0, there exists a positive constant c
such that
Z c
−c
f(λ) dM n
jj(λ) < δ,
(10.10)
for every n and j = s + 1, . . . , p. With this additional condition the following
result is obtained: If the Grenander conditions (1) to (4) are satisﬁed and
that (10.10) holds. Deﬁne the p × p diagonal matrix
Dn = diag(∥Xn(1)∥nd, . . . , ∥Xn(s)∥nd, ∥Xn(s + 1)∥, . . . , ∥Xn(p)∥).
Then,
(a)
For s = 0,
Dn Var(bβn)Dn →2πR(0)−1
Z π
−π
f(λ) dM(λ)R(0)−1,
(10.11)
as n →∞if and only if condition (10.10) holds.
(b)
For s > 0,
D−1
n (X′
nXn) Var(bβn)(X′
nXn)D−1
n
→2πA,
(10.12)
as n →∞where
A =

B
0
0
C

,
(10.13)
and the elements of the s × s matrix B are given by
bij = f0(0) lim
n→∞n−2d
Z π
−π
|1 −e˙ıλ|−2d dM n
ij(λ),
for i, j = 1, . . . , s and the elements of the (p −s) × (p −s) matrix C are
given by
cij =
Z π
−π
f(λ) dMi+s,j+s(λ).

376
TIME SERIES REGRESSION
10.3.2
Asymptotic Normality
Assume that the regression errors {εt} corresponds to a strictly stationary
process with spectral density satisfying (10.2) and Wold decomposition:
εt =
∞
X
j=0
ψjνt−j.
(10.14)
Thus, under some technical conditions, we have that
D−1
n (X′
nXn)(bβn −β) →Np(0, A),
as n →∞where A is the variance-covariance matrix deﬁned by (10.13).
10.4
PROPERTIES OF THE BLUE
Under some conditions, the BLUE satisﬁes
eDn Var(eβn) eDn →2π
Z π
−π
f(λ)−1dM(λ)
−1
,
(10.15)
as n →∞.
10.4.1
Eﬃciency of the LSE Relative to the BLUE
When s > 0, that is, M(λ) displays a jump at the origin for some regressors,
the LSE is not asymptotically eﬃcient compared to the BLUE.
On the other hand, if s = 0 [that is, M(λ) does not jump at zero frequency]
then, under some conditions on the regressors, it can be established that the
LSE is asymptotically eﬃcient.
Assuming that s = 0 and conditions (1)–(5) and (10.10) hold, the LSE is
eﬃcient compared to the BLUE if and only if M increases at no more than p
frequencies and the sum of the ranks of increases is p.
EXAMPLE 10.1
As an illustration of these theorems, consider the following trigonometric
regression:
yt = βxt + εt
where xt = Pq
j=1 e˙ıλjt, λj ̸= 0 for j = 1, . . . , q are known frequencies
and the error sequence has spectral density (10.2).
In this case,
∥Xn∥2
n
= 1
n
n
X
t=1


q
X
j=1
e˙ıλjt


 q
X
k=1
e−˙ıλkt
!
=
q
X
j=1
q
X
k=1
 
1
n
n
X
t=1
e˙ı(λj−λk)t
!
.

PROPERTIES OF THE BLUE
377
 
 
 
 
 
 
 
 
 
Frequency
M(!)
!!
!1
 
!2
 
!3
  .  .  .  
!q
!
1
q
1
q
3
q
.
1
Figure 10.2
An example of trigonometric regression, function M(λ).
Consequently, by Problem 10.16 we conclude that
lim
n→∞
∥Xn∥2
n
=
q

j=1
q

k=1
δ(λj −λk) = q.
Furthermore, dM(λ) = (1/q) q
j=1 δD(λ −λj) dλ, or equivalently,
M(λ) = 1
q
 λ
−π
q

j=1
δD(ω −λj) dω.
This function is displayed in Figure 10.2. Notice that M(λ) exhibits q
jumps located at frequencies λ1, . . . , λq. But, it does not jump at the
origin. Therefore, s = 0 and R(0) = 1.
The asymptotic variance of the LSE satisﬁes
lim
n→∞∥Xn∥2 Var(βn)
=
2π
q
 π
−π
f(λ)
q

j=1
δD(λ −λj) dλ
=
2π
q [f(λ1) + · · · + f(λq)].

378
TIME SERIES REGRESSION
On the other hand, the asymptotic variance of the BLUE is
lim
n→∞∥Xn∥2 Var(eβn)
=
2πq


Z π
−π
f(λ)−1
q
X
j=1
δD(λ −λj) dλ


−1
=
2πq

f(λ1)−1 + · · · + f(λq)−1−1 .
The relative eﬃciency of the LSE compared to the BLUE is deﬁned by
r(d) = lim
n→∞
det Var(eβn)
det Var(bβn)
.
(10.16)
In this case we have
r(q) =
q2
[f(λ1)−1 + · · · + f(λq)−1][f(λ1) + · · · + f(λq)].
Thus, for q = 1, the LSE is asymptotically eﬃcient and by Jensen’s
inequality, for q ≥2 the LSE is not eﬃcient.
Recalling that in this example p = 1, and since for q = 1, M(λ) has
only one jump at frequency λ1 and consequently the LSE is eﬃcient. On
the other hand, for q ≥2, M(λ) has more than one jump and therefore
the LSE is not eﬃcient.
Consider, for instance, the frequencies λj = πj/q for j = 1, . . . , q. In
this case we have
lim
q→∞r(q) =
π2
R π
0 f(λ) dλ
R π
0 f(λ)−1 dλ.
Figure 10.3 shows the behavior of r(q) for a fractional noise model
for diﬀerent values of the long-memory parameter d and q.
For this model, we have that
lim
q→∞r(q) = Γ(1 −d)2Γ(1 + d)2
Γ(1 −2d)Γ(1 + 2d).
As expected, for d = 0 the asymptotic relative eﬃciency is 1. On the
other hand, for d →1
2, the limiting relative eﬃciency is 0.
10.5
ESTIMATION OF THE MEAN
A simple but illustrative example of a linear regression model is
yt = µ + εt
where µ is the unknown mean of the process yt. In this case, the LSE is
bµ = 1
n
n
X
t=1
yt = 1
n1′Y n,

ESTIMATION OF THE MEAN
379
q
r ( q ) 
0
2000
4000
6000
8000
10000
0.4
0.6
0.8
1.0
d=0.100
d=0.250
d=0.499
Figure 10.3
An example of trigonometric regression: Relative eﬃciency.
where 1 = (1, 1, . . . , 1)′ and the BLUE is
µ = (1′Γ−11)−11′Γ−1Y n.
In this case, σn = n and then an application of Problem 10.18 establishes
that the LSE is consistent.
10.5.1
Asymptotic Variance
On the other hand, the Grenander conditions are also satisﬁed: (1) ∥Xn∥=
√n →∞as n →∞; (2) ∥Xn+1∥/∥Xn∥= 1 + 1/√n →1 as n →∞;
(3) R11(h) = limn→∞[1 −|h|/n] = 1 for all ﬁxed h; and (4) R(0) = 1 is
nonsingular. Furthermore, in this case
mn(λ) =
1
2πn

n

t=1
e˙ıλt

2
=
1
2πn
|e˙ınλ −1|2
|e˙ıλ −1|2 =
1
2πn
1 −cos nλ
1 −cos λ ,
which by Problem 10.15 tends to the Dirac functional operator δD(λ) as n →
∞. Therefore,
M(λ) =
 λ
−π
δD(ω) dω = 1[0,π)(λ).

380
TIME SERIES REGRESSION
Hence, M(λ) has a jump at the origin as shown in Figure 10.4, and therefore
s = 1. In this case Dn = nd+1/2 and therefore
n1−2d Var(µ) →2πb,
as n →∞where
b
=
1
2π f0(0) lim
n→∞n−1−2d
 π
−π
|1 −e˙ıλ|−2−2d|1 −e˙ınλ|2dλ
=
Γ(1 −2d)
Γ(d)Γ(1 −d)f0(0)
 1
0
 1
0
|x −y|2d−1dx dy
=
Γ(1 −2d)
d(1 + 2d)Γ(d)Γ(1 −d)f0(0).
For an ARFIMA(p, d, q) model f0(0) = σ2
2π
|θ(1)|2
|φ(1)|2 . Consequently,
Var(µ) ∼cγn2d−1
d(1 + 2d) = σ2 |θ(1)|2
|φ(1)|2

Γ(1 −2d)
d(1 + 2d)Γ(d)Γ(1 −d)

n2d−1. (10.17)
 
 
 
 
 
 
 
 
 
Frequency
M(!)
!!
0
!
0.0
0.2
0.4
0.6
0.8
1.0
Figure 10.4
Estimation of the mean: M(λ).

ESTIMATION OF THE MEAN
381
10.5.2
Relative Eﬃciency
Since in this case M(λ) has a jump at zero frequency, the LSE (sample mean)
is not asymptotically eﬃcient as compared to the BLUE. However, we can
analyze its relative eﬃciency with respect to the BLUE.
Following Adenstedt (1974), the BLUE of the mean of a fractional noise
process with long-memory parameter d is given by
µ =
n

j=1
αjyj,
where the coeﬃcients αj are
αj =
n −1
j −1
Γ(j −d)Γ(n + 1 −j −d)Γ(2 −2d)
Γ(n + 1 −2d)Γ(1 −d)2
.
The variance of this BLUE may be written as
Var(µ) = σ2 Γ(n)Γ(1 −2d)Γ(2 −2d)
Γ(n + 1 −2d)Γ(1 −d)2 ∼σ2n2d−1 Γ(1 −2d)Γ(2 −2d)
Γ(1 −d)2
,
for large n. For an ARFIMA(p, d, q) process, the variance of the BLUE satis-
ﬁes
Var(µ) ∼σ2n2d−1 |θ(1)|2
|φ(1)|2
Γ(1 −2d)Γ(2 −2d)
Γ(1 −d)2
,
(10.18)
0.0
0.1
0.2
0.3
0.4
0.5
0.980
0.985
0.990
0.995
1.000
d
Relative Efficiency   r(d)
Figure 10.5
Relative eﬃciency of the sample mean.

382
TIME SERIES REGRESSION
for large n. Now, from (10.17) and (10.18) we conclude that
r(d) = lim
n→∞
Var(eµ)
Var(bµ) = (1 + 2d)Γ(1 + d)Γ(2 −2d)
Γ(1 −d)
.
Figure 10.5 shows the relative eﬃciency r(d) for d ∈(0, 1
2). The minimal
relative eﬃciency is approximately 0.9813 reached at d = 0.318.
10.6
POLYNOMIAL TREND
Extending the previous example, consider the polynomial regression
yt = β0 + β1t + · · · + βqtq + εt,
where {εt} satisﬁes (10.2). In this case, p = q + 1, Xn(i) = (1, 2i−1, . . . , ni−1)
for i = 1, . . . , p and hence
Xn(i)′Xn(j) =
n
X
t=1
ti+j−2 =
ni+j−1
i + j −1[1 + o(n)],
for i, j = 1, . . . , p.
Thus, Dii = n2i−1+d
2i −1 [1 + o(n)] and
mn
ij(λ) =
Pn
t=1
Pn
s=1 ti−1sj−1e˙ı(s−t)λ
2π∥Xn(i)∥∥Xn(j)∥
.
This term converges to
mij(λ) =
√2i −1√2j −1
i + j −1
δD(λ).
Therefore,
Mij(λ)
=
Z λ
−π
√2i −1√2j −1
i + j −1
δD(λ) dλ
=
√2i −1√2j −1
i + j −1
1(0,π)(λ),
and
Rij(h) =
Z π
−π
e˙ıλh dMij(λ) =
√2i −1√2j −1
i + j −1
,

POLYNOMIAL TREND
383
which actually does not depend on the lag h. For instance, for p = 5 we have
that the matrix R(h) is given by
R(h) =


1
√
3
2
√
5
3
√
7
4
√
9
5
√
3
2
1
√
15
4
√
21
5
√
27
6
√
5
3
√
15
4
1
√
35
6
√
45
7
√
7
4
√
21
5
√
35
6
1
√
63
8
√
9
5
√
27
6
√
45
7
√
63
8
1


.
EXAMPLE 10.2
Consider an ARMA model
φ(B)yt = θ(B)εt,
where εt is a white noise sequence with zero-mean and variance σ2. The
spectral density of this process is given by
f(λ) = σ2
2π

θ(eiλ)
φ(eiλ)

2
.
In particular, at the origin we have
f(0) = σ2
2π

θ(1)
φ(1)

2
.
Let C a square matrix given by
C =
Z π
−π
f(λ)dM(λ),
so that
Cij
=
Z π
−π
f(λ)mij(λ) dλ
=
√2i −1√2j −1
i + j −1
Z π
−π
f(λ)δD(λ) dλ
=
f(0)
√2i −1√2j −1
i + j −1
=
f(0)Rij.
Hence, the variance of the LSE satisﬁes
Dn Var(bβn)Dn →2πf(0) R(0)−1,

384
TIME SERIES REGRESSION
where Dn is a diagonal matrix with diagonal elements approximately
dij = n2i−1
2i−1 . For simplicity, consider the case of a linear trend where the
parameter to estimate is (β0, β1). In this situation,
R(0) =
"
1
√
3
2
√
3
2
1
#
.
and then
R(0)−1 =

4
−2
√
3
−2
√
3
4

.
On the other hand,
Dn ∼
 n
0
0
n3
3

.
Therefore, the large sample variance of the LSE satisﬁes
Var bβ ∼2π f(0)
"
4
n2
−6
√
3
n4
−6
√
3
n4
36
n6
#
.
or more precisely,
Var bβ ∼σ2

θ(1)
φ(1)

2 "
4
n2
−6
√
3
n4
−6
√
3
n4
36
n6
#
.
10.6.1
Consistency
Notice that
∥xn(j)∥2
nδ
= n2j−1−δ
2j −1 [1 + o(n)].
Therefore, by taking δ = 1 we have that
lim inf
n→∞
∥xn(j)∥2
nδ
> 0.
Furthermore, since R(0) is nonsingular, we conclude by Problem 10.18 that
the LSE is consistent.
10.6.2
Asymptotic Variance
Since M(λ) has a jump at the origin, the asymptotic variance of bβn satisﬁes
D−1
n (X′
nXn) Var(bβn)(X′
nXn)D−1
n
→2πB,

HARMONIC REGRESSION
385
where
bij
=
f0(0) lim
n→∞n−2d
Z π
−π
|1 −e˙ıλ|−2dmn
ij(λ) dλ
=
f0(0) lim
n→∞
n−2d
∥Xn(i)∥∥Xn(j)∥
n
X
t=1
n
X
s=1
ti−1sj−1
Z π
−π
|1 −e˙ıλ|−2d
2π
e˙ı(t−s)λ dλ
=
f0(0) lim
n→∞
n−2d
∥Xn(i)∥∥Xn(j)∥
n
X
t=1
n
X
s=1
ti−1sj−1γ(t −s),
where γ(·) is the ACF of a fractional noise process FN(d) with σ2 = 1. There-
fore,
bij
=
f0(0)cγ lim
n→∞
n−2d
∥Xn(i)∥∥Xn(j)∥
n
X
t=1
n
X
s=1
ti−1sj−1|t −s|2d−1
=
f0(0)cγ
p
(2i −1)(2j −1)
×
lim
n→∞n−2d
n
X
t=1
n
X
s=1
 t
n
i−1  s
n
j−1 
t
n −s
n

2d−1 1
n2
=
f0(0)cγ
p
(2i −1)(2j −1)
Z 1
0
Z 1
0
xi−1yj−1|x −y|2d−1dx dy
=
f0(0)cγ
p
(2i −1)(2j −1)B(i, 2d) + B(j, 2d)
i + j + 2d −1
,
see Problem 10.12 for ﬁnding the value of the double integral.
10.6.3
Relative Eﬃciency
Analogous to the estimation of the mean case, since M(λ) has a jump at the
origin, the LSE of the polynomial regression is not asymptotically eﬃcient.
The relative eﬃciency for q = 1 is given by
r(d) = (9 −4d2)
(1 + 2d)Γ(1 + d)Γ(3 −2d)
6Γ(2 −d)
2
.
Figure 10.6 shows r(d) for d ∈(0, 1
2). The minimal relative eﬃciency is 8
9
reached at d = 0.5.
10.7
HARMONIC REGRESSION
Another important example is the harmonic regression
yt = β1e˙ıλ1t + β2e˙ıλ2t + · · · + βqe˙ıλqt + εt,

386
TIME SERIES REGRESSION
0.0
0.1
0.2
0.3
0.4
0.5
0.85
0.90
0.95
1.00
d
Relative Efficiency   r(d)
Figure 10.6
Relative eﬃciency of the polynomial regression with q = 1.
where {εt} is a stationary process with spectral density (10.2) and λj ̸= 0 for
j = 1, . . . , q are known frequencies. In this case, Dii = √n for i = 1, . . . , q,
and by Problem10.15 we have that
⟨Xn,h(i), Xn(j)⟩
∥Xn,h(i)∥∥Xn(j)∥= e˙ıλjh 1
n
n

t=1
e˙ı(λi−λj)t →e˙ıλjhδ(λj −λi),
as n →∞. Hence, Rij(h) = e˙ıλjhδ(λj −λi) and from equation (10.9) we
conclude that dMij(λ) = δ(λj −λi)δD(λ−λj) dλ. Therefore, Mij(λ) does not
have a jump at the origin for any λj ̸= 0.
10.7.1
Consistency
Observe that R(0) = In where In is the n × n identity matrix. Consequently,
it satisﬁes the Grenander condition (4), that is, R(0) is nonsingular. Besides,
∥Xn(j)∥2
nδ
= n1−δ.
Therefore, by taking δ = 1 in Problem 10.18 we conclude that the LSE is
consistent.

HARMONIC REGRESSION
387
10.7.2
Asymptotic Variance
Since Rij(0) = δ(i −j) and Dii = √n we have that
 π
−π
f(λ) dMij(λ) =
 π
−π
f(λ)δD(λ −λj) dλ = f(λj)δ(i −j).
Consequently,
lim
n→∞n Var(βn) = 2π


f(λ1)
0
0
· · ·
0
0
f(λ2)
0
· · ·
0
...
...
...
0
· · ·
0
f(λq−1)
0
0
· · ·
0
0
f(λq)


.
10.7.3
Eﬃciency
As shown in Figure 10.7, Mjj(λ) displays one jump of size one at frequency
λj. Thus, M(λ) increases at q frequencies each of rank one; so that the LSE
is asymptotically eﬃcient.
 
 
 
 
 
 
 
 
 
 
 
 
Frequency
Mjj(!)
!!
!j
!
0.0
0.2
0.4
0.6
0.8
1.0
Figure 10.7
Harmonic regression: Mjj(λ).

388
TIME SERIES REGRESSION
10.8
ILLUSTRATION: AIR POLLUTION DATA
As an illustration of the long-memory regression techniques discussed in this
chapter consider the following air pollution data exhibited in Figure 10.8.
This time series consists of 4014 daily observations of ﬁne particulate matter
with diameter less than 2.5 µm (PM2.5) measured in Santiago, Chile, during
the period 1989–1999; see Section 10.9 for further details about these data.
In order to stabilize the variance of these data, a logarithmic transformation
has been made.
The resulting series is shown in Figure 10.9.
This series
displays a clear seasonal component and a possible downward linear trend.
Consequently, the following model for the log-PM25 is proposed,
yt = β0 + β1t +
k

j=1
[aj sin(ωjt) + cj cos(ωjt)] + εt,
(10.19)
where εt ∼(0, σ2
ε).
An analysis of the periodogram of the detrended data and the ACF reveals
the presence of three plausible seasonal frequencies, ω1 = 2π/7, ω2 = 2π/183,
and ω3 = 2π/365.
The least squares ﬁtting assuming uncorrelated errors is shown in Ta-
ble 10.3. Observe that according to this table all the regression coeﬃcients
Time
PM2.5 daily level
1990
1992
1994
1996
1998
2000
0
100
200
300
400
Figure 10.8
Air pollution data: Daily PM2.5 measurements at Santiago, Chile,
1989 - 1999.

ILLUSTRATION: AIR POLLUTION DATA
389
Time
Log PM2.5 daily level
1990
1992
1994
1996
1998
2000
2
3
4
5
6
Figure 10.9
Air pollution data: Log daily PM2.5 measurements at Santiago,
Chile, 1989 - 1999.
in model (10.19) are signiﬁcant at the 5% level, excepting c1. In particular,
even though the LS estimate of the linear trend coeﬃcient β1 is very small,
bβ1 = −0.0002, it is signiﬁcant at the 5% level. The sample autocorrelation
function of the residuals from the LSE ﬁt is shown in Figure 10.10. As ob-
served in this plot, the components of the autocorrelation function are signif-
icant even after 30 lags. Additionally, the variance plot (see Subsection 4.5.3)
displayed in Figure 10.11 indicates the possible presence of long-range de-
pendence in the data. As a result from these two plots, it seems that the
disturbances εt in the linear regression model (10.19) may have long-memory
correlation structure and the LSE ﬁtting may not be adequate.
To account for the possible long-memory behavior of the errors, the follow-
ing ARFIMA(p, d, q) model is proposed for the regression disturbances {εt}:
φ(B)εt = θ(B)(1 −B)−dηt,
where {ηt} is a white noise sequence with variance σ2. The model selected
according to the Akaike’s information criterion (AIC) is the ARFIMA(0, d, 0),
with bd = 0.4252, td = 34.37, and bση = 0.3557. Table 10.4 shows the results
from the least squares ﬁt with ARFIMA errors.
From this table, observe
that the linear trend coeﬃcient β1 is no longer signiﬁcant at the 5% level.
Similarly, the coeﬃcients c1 and c2 are not signiﬁcant at that level.

390
TIME SERIES REGRESSION
0
5
10
15
20
25
30
35
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
Figure 10.10
Air pollution data: Sample autocorrelation function of the residuals
from the least squares ﬁt.
Table 10.3
Air Pollution Data: Least Squares Fit
Coeﬃcient
Standard Deviation
t-stat
P-value
β0
4.3148
0.0124
347.6882
0.0000
β1
-0.0002
0.0000
-35.5443
0.0000
a1
0.0775
0.0088
8.8556
0.0000
c1
-0.0083
0.0088
-0.9446
0.3449
a2
0.1007
0.0087
11.5086
0.0000
c2
-0.0338
0.0088
-3.8590
0.0001
a3
0.4974
0.0088
56.6914
0.0000
c3
0.4479
0.0088
51.1620
0.0000

ILLUSTRATION: AIR POLLUTION DATA
391
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
-3.5
-3.0
-2.5
Log  k
Log  Var (k)
Figure 10.11
Air pollution data: Variance plot of the residuals from the LSE ﬁt.
Table 10.4
Air Pollution Data: Least Squares Fit with Long-Memory Errors
Coeﬃcient
Standard Deviation
t-stat
P-value
β0
4.3148
0.5969
7.2282
0.0000
β1
-0.0002
0.0003
-0.7405
0.4590
a1
0.0775
0.0084
9.1837
0.0000
c1
-0.0083
0.0084
-0.9803
0.3270
a2
0.1007
0.0363
2.7713
0.0056
c2
-0.0338
0.0335
-1.0094
0.3129
a3
0.4974
0.0539
9.2203
0.0000
c3
0.4479
0.0447
10.0267
0.0000

392
TIME SERIES REGRESSION
10.9
BIBLIOGRAPHIC NOTES
Chapter 7 of Grenander and Rosenblatt (1957), Section VII.4 of Ibragimov
and Rozanov (1978), and Chapter VIII of Hannan (1970) are excellent ref-
erences for the analysis of regression with correlated errors. However, most
of their results do not apply directly to strongly dependent processes. On
the other hand, there is an extensive literature about regression data with
long-memory disturbances; see, for example, K¨unsch (1986), Yajima (1988,
1991), Dahlhaus (1995), Sibbertsen (2001), and Choy and Taniguchi (2001),
among others. Yajima (1991) established many asymptotic results for least
squares error estimates and best linear unbiased estimates. As described in
this chapter and according to Yajima, the convergence rates for the variance
of these estimates as the sample size increases, depend on the structure of
the characteristic function of the design matrix. The Grenander conditions
were introduced by Grenander (1954). Many of the LSE and BLUE results
discussed in this chapter concerning regression with long-memory errors are
due to Yajima (1988, 1991). The air pollution data discussed in Section 10.8
are provided by the Ambient Air Quality Monitoring Network (MACAM in
Spanish, www.sesma.cl) in Santiago, Chile.
Further details about the ﬁne
particulate matter data and other ambient variables can be found in Iglesias,
Jorquera, and Palma (2006).
Problems
10.1
Consider the ﬁrst-order autoregressive model: yt+1 = φyt +ϵt+1 where
ϵt is white noise with zero-mean and variance σ2.
(a) Verify that yt+k = φkyt + Pk
j=0 φjϵt+k−j
(b) Based on part a) verify that the best linear predictor of yt+k given
{yt, yt−1...} is ˆyt+k = φkyt
(c) Show that the prediction error et+k = yt+k −ˆyt+k can be written as
et+k = Pk
j=0 φjϵt+k−j
(d) Calculate Var(et+k).
10.2
Consider the following trend break time series model deﬁned by
E(yt)
=
 β0
t = 1, ..., k,
β0 + β1xt
t = k + 1, ..., n.
Var(yt)
=
 σ2
t = 1, ..., k,
νσ2
t = k + 1, ..., n,
where ν is a positive constant.
(a) Write this model as y = x β+ε, specifying the matrix x and Var(ε) =
V .
(b) Verify that the estimator ˜β = (x′x)−1x′y has greater variance than
the estimator bβ = (x′V −1x)−1x′V −1y.

PROBLEMS
393
10.3
Suppose that you have a simple regression model of the form yt =
βt + γxt + εt, where εt is a Gaussian white noise, t indicates time and xt is
a stationary time series. Show that the least squares estimator of γ for this
model is exactly the same least squares estimator γ for model y∗
t = γx∗
t + νt,
where y∗
t and x∗
t are the regression residuals of regression of yt and xt at time
t.
10.4
Consider the process deﬁned by the equation
yt = ϵt + θ1ϵt−1 + θ2ϵt−2,
where εt is a white noise sequence with zero-mean and variance σ2.
(a) Calculate the ACF of yt.
(b) Obtain the best linear predictors P[{y1}]y2, P[{y1,y2}]y3
(c) Calculate the best linear predictors P[{y2}]y3 y P[{y2}]y1 and then
obtain corr(e3, e1) where e3 = y3 −P[{y2}]y3 y e1 = y1 −P[{y2}]y1.
10.5
Consider the process zt = β + ϵt, where ϵt is and i.i.d. sequence with
zero-mean and variance σ2.
a) Find the least squares estimator (LSE) of β, ˆβ.
b) Show that the k-step predictor, ˜zn+k = E(zn+k|z1, ..., z : n), is ˜zn+h =
β.
c) Given that β is an unknown parameter, you dicide to used the LSE to
predict the value zn+k by means of the formula ˆzn+k = ˆβ. Show that
the mean squared error of this predictor is E(ˆzn+k −zn+k)2 = σ2(1+ 1
n).
10.6
Consider the time series yt deﬁned as
yt = a + b t + wt,
with {wt} ∼(0, σ2). Show that applying a moving-average ﬁlter, the variance
of the resulting process zt deﬁned as
zt =
1
2 k + 1
k
X
j=−k
yt−j
is 1/(2 k + 1) parts of the variance of the series yt.
10.7
Assume that xj denotes the jth column of the n × p design matrix
Xn, that is, xtj = tj−1 and let Dn = diag(∥x1∥n, ∥x2∥n, . . . , ∥xp∥n) where
∥xj∥n = [Pn
t=1 x2
tj]1/2.
(a) Show that
lim
n→∞D−1
n X′
nXnD−1
n
= M,

394
TIME SERIES REGRESSION
where M = (mij)i,j=1,...,p with mij =
√2i −1√2j −1
i + j −1
.
(b) Verify that
lim
n→∞n−2dD−1
n X′
nΓXnD−1
n
= 2πf0(0)H,
where
hij =
√2i −1√2j −1 Γ(1 −2d)
Γ(d)Γ(1 −d)
Z 1
−1
Z 1
−1
xi−1yj−1|x −y|2d−1 dx dy.
10.8
Consider the harmonic regression
yt = α1 sin(λ0t) + α2 cos(λ0t) + εt,
for t = 1, . . . , n where εt is a stationary long-memory process with spectral
density (10.2).
(a) Show that yt may be written as
yt = β1e˙ıλ0t + β2e−˙ıλ0t + εt,
and ﬁnd expressions for α1 and α2 in terms of β1 and β2.
(b) Verify that the LSE of α1 and α2 are
bα1 = 1
2˙ı(˙ıbβ2 + bβ1),
and
bα2 = 1
2˙ı(˙ıbβ2 −bβ1),
where bβ1 and bβ2 are the LSE of β1 and β2, respectively.
(c) Show that
lim
n→∞n Var(bα1) = lim
n→∞n Var(bα2) = πf(λ0).
10.9
Consider the linear regression model
yt = βtpe˙ıλ0 t + εt,
where p ∈{0, 1, 2, . . . } and εt is a stationary long-memory process with spec-
tral density (10.2).
(a) Is the LSE of β consistent?
(b) Are the Grenander conditions satisﬁed in this case?
(c) Is the LSE asymptotically normal?
(d) Is the LSE asymptotically eﬃcient in this case?

PROBLEMS
395
10.10
Let h(λ) ≥0 and deﬁne the n × n matrix Γ with elements
γij =
Z π
−π
h(λ)e˙ı(i−j)λdλ.
(a) Verify that Γ is symmetric, that is, γij = γji.
(b) Show that Γ is positive semideﬁnite, that is, for any x ∈Cn
x∗Γx ≥0.
(c) Observe that since Γ is symmetric, it may be written as
Γ = UDU ∗,
where U is a nonsingular n × n matrix and D = diag(di) with di ≥0
for i = 1, . . . , n. Using this fact and the Cauchy-Schwartz inequality
|x∗y| ≤∥x∥∥y∥,
show that for all u, v ∈Cn
2|u∗Γv| ≤u∗Γu + v∗Γv.
(10.20)
(d) Verify that
Z π
−π
2πh(λ)mn
ij(λ) dλ =
n
X
t=1
n
X
s=1
⟨xti, xsj⟩
∥xn(i)∥∥xn(j)∥γij = u∗Γv
where u = xn(i)/∥xn(i)∥and v = xn(j)/∥xn(j)∥.
(e) Using (10.20) show that

Z π
−π
h(λ)mn
ij(λ) dλ
 ≤
Z π
−π
h(λ)mn
ii(λ) dλ +
Z π
−π
h(λ)mn
jj(λ) dλ.
(f) Deduce that for any f(λ) ≥0,

Z
A
f(λ) dMij(λ)
 ≤
Z
A
f(λ) dMii(λ) +
Z
A
f(λ) dMjj(λ),
cf., Yajima (1991, p. 162).
10.11
Consider the following trend break regression
yt = β0 + β1xt + εt,
for t = 1, . . . , n, where the covariate xt is deﬁned by
xt =





0
if
t ≤n
2 ,
1
if
n
2 < t ≤n.

396
TIME SERIES REGRESSION
and for simplicity we assume that n is even. Let Xn = [Xn(1), Xn(2)] the
design matrix.
(a) Show that ∥Xn(1)∥= √n and ∥Xn(2)∥=
p
n/2.
(b) Following expression (10.8) verify that
⟨Xn,h(1), Xn(2)⟩
∥Xn,h(1)∥∥Xn(2)∥→
1
√
2,
as n →∞and that
R(h) =


1
1
√
2
1
√
2
1

.
(c) Are the Grenander conditions fulﬁlled in this case?
(d) Prove that the matrix M(λ) may be written as
M(λ) = δD(λ)


1
1
√
2
1
√
2
1

.
(e) Let bβn be the LSE of β = (β1, β2)′. Is this estimator consistent?
(f) Find an expression for the asymptotic variance of bβn.
10.12
Show that
Z 1
0
Z 1
0
xαyβ|x −y|γ dx dy = B(α + 1, γ + 1) + B(β + 1, γ + 1)
α + β + γ + 2
,
for α > −1, β > −1 and γ > −1, where B(·, ·) is the beta function. Hint: Try
the change of variables x = uv and y = v.
10.13
Consider the liner regression model
yt = βtα + εt,
for t = 1, 2, . . . , n, where α is known and {εt} is a long-memory stationary
process with spectral density satisfying
f(λ) ∼cf|λ|−2d,
as |λ| →0 with 0 < d <
1
2.
Let bβn be the LSE of β and let Xn =
(1, 2α, . . . , nα)′.
(a) Verify that ∥Xn∥2 ∼n2α+1
2α + 1, for α > −1
2.

PROBLEMS
397
(b) Show that if α > d −1
2, then bβn is strongly consistent.
(c) Prove that
⟨Xn,h, Xn⟩
∥Xn,h∥∥Xn∥→R(h),
as n →∞, where R(h) = 1 for all h ∈Z.
(d) Are the Grenander conditions satisﬁed in this case?
(e) Show that the variance of bβn satisﬁes
Var(bβn) ∼4πcf(2α + 1)2
Γ(d)Γ(1 −d)
B(α + 1, 2d)
2α + 2d + 1 n2d−1−2α,
as n →∞.
(f) Assume that the disturbances {εt} are independent and identically
distributed. Is the LSE bβn asymptotically normal?
10.14
Verify that under assumption (3), M n(λ) converges to M(λ), that
is,
Z π
−π
g(λ) dM n(λ) →
Z π
−π
g(λ) dM(λ),
as n →∞for any continuous function g(λ) with λ ∈[−π, π].
10.15
Let the function fn(·) be deﬁned for n ≥1 as
fn(λ) =
1
2πn

n
X
t=1
e˙ıλt

2
.
Show that the sequence of functions {fn(λ)} converges to the Dirac operator
δD(λ) as n →∞. That is, for any continuous function g(λ), λ ∈[−π, π] we
have that
Z π
−π
fn(λ)g(λ) dλ →
Z π
−π
g(λ)δD(λ) dλ = g(0),
as n →∞.
10.16
Let the sequence of functions {fn} be deﬁned by
fn(λ) = 1
n
n
X
t=1
e˙ıλt.
Show that fn(λ) →δ(λ) as n →∞.
10.17
Consider the linear model (10.1) where the sequence of disturbances
{εt} is a stationary process with spectral density satisfying (10.2) and f0 is a
bounded function. If the following two conditions hold

398
TIME SERIES REGRESSION
(a) n−2dσn →∞as n →∞,
(b) P∞
n=p+2 σ−1
n−1n2d−1 log2 n < ∞,
then bβn →β almost surely as n →∞.
10.18
Consider the linear model (10.1) where the sequence of disturbances
{εt} is a stationary process with spectral density satisfying (10.2) and f0 is
a bounded function. Suppose that the Grenander conditions (3, with h = 0)
and (4) hold. If
0 < lim inf
n→∞
∥xn(j)∥2
nδ
,
for some δ > 2d and j = 1, . . . , p, then bβn →β almost surely as n →∞.

CHAPTER 11
MISSING VALUES AND OUTLIERS
This chapter examines the eﬀects of two relevant data problems of the statisti-
cal time series analysis. Many real-life time series data are incomplete or may
exhibit observations with a magnitude that seems to be very diﬀerent from
the others. Both of these situations may produce important consequences in
the statistical analysis, model ﬁtting and prediction.
Regarding the ﬁrst problem, notice that most of the methodologies studied
so far assume complete data. However, in many practical situations only part
of the data may be available. As a result, in these cases we are forced to
ﬁt models and make statistical inferences based on partially observed time
series.
In this chapter we explore the eﬀects of data gaps on the analysis
of long-memory processes and describe some methodologies to deal with this
situation. As in many other areas of statistics, there are several ways to deal
with incomplete time series data. A fairly usual approach, especially when
there are only a small number of missing values, is to replace them by zero or
the sample mean of the series. Another approach is to use some imputation
technique such as cubic splines and other interpolation methods, including, for
example, the repetition of previous values. In this chapter we discuss some of
these techniques. Special attention is focused on the method of integrating out
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
399

400
MISSING VALUES AND OUTLIERS
the missing values from the likelihood function. As discussed in Section 11.1,
this approach does not introduce artiﬁcial correlation in the data and avoids
some dramatic eﬀects on parameter estimates produced by some imputation
techniques. The deﬁnition of an appropriate likelihood function to account
for unobserved values is studied in Section 11.2. In particular, it is shown that
if the distribution of the location of the missing data is independent of the
distribution of the time series, then the statistical inferences can be carried
out by ignoring the missing observations.
This section also discusses the
computation of the likelihood function for a general class of time series models
and modiﬁcations to the Kalman ﬁlter equations to account for missing values.
Sections 11.3 and 11.4 are dedicated to investigate the eﬀects of data gaps on
estimates and predictions, respectively, establishing some general theoretical
results. A number of illustrations of these results are discussed in Section 11.2.
Estimation of missing values via interpolation is discussed in Section 11.5.
The second problem can be treated by means of speciﬁc techniques. For
instance, outliers and intervention analysis are also discussed in this chapter.
Section 11.7 describes some of the methodologies for modeling outliers. Ad-
ditionally, it discusses the problem of interventions in the time series in the
context of both known and unknown location. Suggestions for further reading
on this topic are given in Section 11.8. This chapter concludes with a list of
proposed problems.
11.1
INTRODUCTION
Missing data is an important problem for the analysis of time series. In par-
ticular, the parameter estimates may suﬀer serious distortions if the missing
values are not appropriately treated. This point is illustrated by Table 11.1
which displays the maximum-likelihood estimates of the parameter φ of an
AR(1) process, along with their standard deviations. The results are based
on 1,000 simulations with φ = 0.8 with sample size of 1,000 observations, for
diﬀerent percentages of missing values. Three maximum-likelihood estimates
are considered, one based on the full sample ( full ), one based on the series
with imputed missing values using a spline interpolation method (imputed ),
and one based only on the available data (NAs). Observe from Table 11.1
that the imputation method changes dramatically the estimated parameter
φ. Besides, the standard deviations are greatly increased when compared to
the MLE based on the full sample. On the other hand, when the missing
values are adequately accounted for (MLE with NAs), the estimates are quite
close to the true value of the ﬁrst-order autoregressive parameter and the
standard deviations are only marginally increased when compared to the full
sample MLE. A similar situation is observed regarding the estimation of σ2
which is reported in Table 11.2.
As discussed at the beginning of this chapter, there are many ways to deal
with missing data in the context of time series. For example, the missing

LIKELIHOOD FUNCTION WITH MISSING VALUES
401
Table 11.1
Maximum-likelihood estimates of φ for simulated AR(1) processes,
based on 500 observations.a
%
φ
SD
Missing
Full
Imputed
NAs
Full
Imputed
NAs
10
0.7934
0.7119
0.7933
0.0275
0.0326
0.0280
20
0.7943
0.6309
0.7956
0.0266
0.0364
0.0312
30
0.7930
0.5519
0.8552
0.0298
0.0385
0.0310
a The three estimates correspond to the full, imputed, and incomplete data, respectively.
Table 11.2
Maximum-likelihood estimates of σ2 for simulated AR(1) processes,
based on 500 observations.a
%

σ2
SD
Missing
Full
Imputed
NAs
Full
Imputed
NAs
10
0.9966
1.2013
0.9970
0.0628
0.1100
0.0659
20
0.9929
1.3065
0.9943
0.0609
0.1365
0.0714
30
0.9967
1.3305
1.0136
0.0686
0.1831
0.0868
a The three estimates correspond to the full, imputed, and incomplete data, respectively.
values can be imputed, the data can be repeated as in the Nile River series,
replaced by zeroes or other values, ignored or integrated out, among many
other techniques. In the next section, we study some techniques for obtaining
a likelihood function based only on the available data.
11.2
LIKELIHOOD FUNCTION WITH MISSING VALUES
An important issue when deﬁning the likelihood function with missing data is
the distribution of the location of the missing values. For example, if there is
censoring in the collection of the data and any value above or below a given
threshold is not reported, then the location of those missing observations will
depend on the distribution of the time series. In other situations, the location
may not depend on the values of the stochastic process but it may depend
on time. For example, daily economical data may miss Saturday and Sunday
observations, producing a systematic pattern of missing data. If the distri-
bution of the location of missing data does not depend on the distribution of
the time series—observed and unobserved—values, then maximum-likelihood
estimations and statistical inferences can be carried out by ignoring the exact
location of the missing values as described below.

402
MISSING VALUES AND OUTLIERS
11.2.1
Integration
Let the full data be the triple (yobs, ymis, η), where yobs denotes the observed
values, ymis represents the unobserved values, and η is the location of the
missing values. In this analysis, η may be regarded as a random variable.
The observed data is then given by the pair (yobs, η), that is, the available
information consists of the observed values and the pattern of the missing
observations. The distribution of the observed data may be obtained by inte-
grating the missing data out of the joint distribution of (yobs, ymis, η):
fθ(yobs, η)
=

fθ(yobs, ymis, η) dymis
=

fθ(η|yobs, ymis)fθ(yobs, ymis) dymis.
If the distribution of η does not depend on (yobs, ymis), then
fθ(yobs, η)
=

fθ(η)fθ(yobs, ymis) dymis
=
fθ(η)

fθ(yobs, ymis) dymis.
Furthermore, if the location η does not depend upon the parameter θ, then
fθ(yobs, η) = f(η)

fθ(yobs, ymis) dymis.
In this case, the likelihood function for θ satisﬁes
L(θ|yobs, η)
∝

fθ(yobs, ymis) dymis = fθ(yobs) = L(θ|yobs).
Thus, maximum-likelihood estimates do not depend on the location of the
missing data. Observe that this result is still valid if η depends upon time.
11.2.2
Maximization
An alternative way of dealing with unobserved data is through the maximiza-
tion of the likelihood function L(θ|yobs, ymis) with respect to ymis. Under this
approach, let the function L be deﬁned by
L(θ|yobs) = max
ymis L(θ|yobs, ymis).
Consider, for instance, the class of random variables y = (y1, . . . , yn)′ ∈IRn,
with zero-mean and joint density given by
fθ(y) = |Σ|−1/2h(y′Σ−1y),
(11.1)

LIKELIHOOD FUNCTION WITH MISSING VALUES
403
where h is a positive real function and Σ is a symmetric deﬁnite positive
matrix. Several well-known distributions are described by (11.1), including
the multivariate normal, mixture of normal distributions and multivariate t,
among others.
For this family of distributions we ﬁnd that
L(θ|yobs, ymis)
=
|Σ|−1/2h[(yobs, ymis)′Σ−1(yobs, ymis)]
=
|A|1/2h[(yobs, ymis)′A(yobs, ymis)],
where
A =

A11
A12
A21
A22

= Σ−1 =

Σ11
Σ12
Σ21
Σ22
−1
,
with Σ11 = Var[yobs], Σ22 = Var[ymis], Σ12 = Cov[yobs, ymis] = Σ′
21.
Thus,
∂L(θ|yobs, ymis)
∂ymis
= 2|A|1/2h′(s)[y′
obsA12 + y′
misA22] = 0,
where s = (yobs, ymis)A(yobs, ymis)′ ≥0. Consequently,
ymis = −A−1
22 A21yobs = Σ21Σ−1
11 yobs,
is a critical point of L(θ|yobs, ymis) as a function of ymis. Furthermore, the
Hessian is
H(ymis) = 2|A|1/2h′(s)A22,
which is negative deﬁnite if h′(s) < 0. This occurs for several distributions.
For example, in the Gaussian case, h(s) = exp(−s2/2), and hence its deriva-
tive satisﬁes
h′(s) = −se−s2/2 < 0,
for any s > 0. Therefore, in this situation ymis is indeed a maximum and
L(θ|yobs)
=
|Σ|−1/2h(y′
obsΣ−1
11 yobs)
=
|Σ22 −Σ21Σ−1
11 Σ12|−1/2|Σ|−1/2
11
h(y′
obsΣ−1
11 yobs)
=
|Σ22 −Σ21Σ−1
11 Σ12|−1/2L(θ|yobs).
Hence, by deﬁning g(θ) = |Σ22 −Σ21Σ−1
11 Σ12|−1/2, we conclude that
L(θ|yobs) = g(θ)L(θ|yobs).
Thus, the maximization of L(θ|yobs) with respect to θ may give a diﬀerent
result than the maximization of L(θ|yobs).

404
MISSING VALUES AND OUTLIERS
11.2.3
Calculation of the Likelihood Function
Naturally, the actual calculation of the likelihood function depends on the
joint distribution of the process. For example, for the class of distributions
described by (11.1), the log-likelihood function is given by
L(θ) = −1
2 log det Σ + log h(y′Σ−1y).
(11.2)
The matrix Σ can be diagonalized as in Chapter 5. That is, since Σ is assumed
to be deﬁnite positive and symmetric, there is a lower triangular matrix L with
ones in the diagonal and a diagonal matrix D = diag{d1, . . . , dn} such that
Σ = L′DL.
Deﬁne e = Ly. Hence, y′Σ−1y = e′De and det Σ = det D. Consequently,
L(θ) = −1
2
n

t=1
log dt + log h
	 n

t=1
e2
t
dt

.
As discussed in the next section, in the presence of data gaps the likelihood
function of this class of distributions may be calculated from the output of
the Kalman ﬁlter equations.
11.2.4
Kalman Filter with Missing Observations
Here we analyze with greater detail the state space approach for dealing with
missing observations that was outlined in Subsection 3.4.4. Consider the state
space system
 xt+1 = Fxt + Hεt,
yt = Gxt + εt,
where xt is the state, yt is the observation, F and G are system matrices, and
the observation noise variance is Var(εt) = σ2.
The following theorem summarizes the modiﬁcations to the Kalman ﬁlter
equations in order to account for the missing observations.
Let xt be the projection of the state xt onto Pt−1 = {observed ys : 1 ≤
s ≤t −1} and let Ωt = E[(xt −xt)(xt −xt)′] be the state error variance,
with x1 = 0 and Ω1 = E[x1x′
1]. Then, the state predictor xt is given by the

EFFECTS OF MISSING VALUES ON ML ESTIMATES
405
following recursive equations for t ≥1:
Δt
=
GΩtG′ + σ2,
Kt
=
(FΩtG′ + σ2H)Δ−1
t ,
Ωt+1
=
 FΩtF ′ + σ2HH′ −ΔtKtK′
t,
yt observed,
FΩtF ′ + σ2HH′,
yt missing,
νt
=
 yt −Gxt,
yt observed,
0,
yt missing,
xt+1
=
F xt + Ktνt,
yt+1
=
Gxt+1.
Remark 11.1. Notice that the equation linking the state and the observation
estimation (B.51) is the same, with or without missing data.
Based on the modiﬁed Kalman equations, the log-likelihood function of
a stochastic process with missing data and probability distribution (11.1) is
given by
L(θ) = −1
2

log Δt + log h
 ν2
t
Δt

,
where the sum is over all the observed values of yt.
EXAMPLE 11.1
Let yt be a stationary process with autocorrelations ρk and variance
σ2
y and suppose that we have observed the values y1, y2, y4, . . . , but
the value y3 is missing. By applying the modiﬁed Kalman equations
described above we conclude that Δ1 = σ2
y, Δ2 = σ2
y(1 −ρ2
1), Δ3 =
σ2
y(1 −ρ2
2), Δ4 = σ2
y(1 −ρ2
1 −ρ2
2 −ρ2
3 −2ρ1ρ2ρ3)/(1 −ρ2
2), . . . Thus, the
magnitude of the jump in the prediction error variance from Δ2 to Δ3
is ρ2
2 −ρ2
1.
11.3
EFFECTS OF MISSING VALUES ON ML ESTIMATES
In Section 11.1 we discussed some of the eﬀects of imputation on ML estimates.
To further illustrate the eﬀects of repeating values to ﬁll data gaps consider
the following example involving the Nile River time series, which is exhibited
in Figure 11.1.
Figure 11.1(a) displays the original data. From this plot, it seems that dur-
ing several periods the data was repeated year after year in order to complete
the series. Figure 11.1(b) shows the same series, but without those repeated
values (ﬁltered series).
Table 11.3 shows the ﬁtted parameters of an ARFIMA(0, d, 0) using an
AR(40) approximation along the modiﬁed Kalman ﬁlter equations of Subsec-
tion 11.2.4 for both the original and the ﬁltered data.

406
MISSING VALUES AND OUTLIERS
Table
11.3
Nile
River
Data
Estimates
Series
d
t 
d
σ
Original data
0.499
25.752
0.651
Filtered data
0.434
19.415
0.718
Observe that for the original data, the estimate of the long-memory param-
eter is 0.5, indicating that the model has reached the nonstationary boundary.
On the other hand, for the ﬁltered data, the estimate of d is inside the sta-
tionary region.
Thus, in this particular case, the presence of data irregularities such as
the replacement of missing data with repeated values induces nonstationarity.
On the other hand, when the missing data is appropriately taken care of, the
resulting model is stationary.
(a)
Year
Nile River Level
600
800
1000
1200
1400
1600
1800
9
10
11
12
13
14
15
(b)
Year
Nile River Level
600
800
1000
1200
1400
1600
1800
9
10
11
12
13
14
15
Figure 11.1
Nile river data (A.D. 622 to A.D. 1921). (a) Original data and (b)
ﬁltered data.

EFFECTS OF MISSING VALUES ON PREDICTION
407
11.3.1
Monte Carlo Experiments
Table 11.4 displays the results from Monte Carlo simulations of Kalman
maximum-likelihood estimates for a fractional noise ARFIMA(0, d, 0) with
missing values at random.
The sample size is n = 400 and the Kalman moving-average (MA) trun-
cation uses m = 40. The long-memory parameters are d = 0.1, 0.2, 0.3, 0.4,
and σ = 1. The number of missing values is 80 (20% of the sample) which
have been selected randomly for each sample. The sample mean and standard
deviations are based on 1000 repetitions.
From Table 11.4, notice that the bias is relatively low for all the values
of the long-memory parameter.
On the other hand, the sample standard
deviation of the estimates seems to be close to the expected values.
The
expected standard deviation is

6/400π2 = 0.0390 for the full sample and

6/320π2 = 0.04359 for the incomplete data case.
Table 11.5 shows the results from Monte Carlo simulations of Kalman
maximum-likelihood estimates for ARFIMA(1, d, 1) processes with missing
values at random. The sample size of the full data is n = 400 and the param-
eters are d = 0.3, φ = −0.5, θ = 0.2, and σ2 = 1. The incomplete data have
80 missing observations.
From Table 11.5, the estimates of the long-memory parameter d seem to
be slightly downward biased for both full and incomplete data.
On the other hand, the sample standard deviations are slightly higher than
expected, for the estimates of d. The theoretical standard deviations reported
in this table are based on formula (5.28), with n = 400 for the full data sample
and n = 320 for the incomplete data case.
11.4
EFFECTS OF MISSING VALUES ON PREDICTION
In this section we turn our attention to the evolution of the one-step mean-
squared prediction error, E[yt −yt]2, for ARFIMA models, during and after a
Table 11.4
Maximum-Likelihood Estimates for Simulated FN(d) Processes Based
on 1000 Observations for Diﬀerent Values of the Long-Memory Parameter d
Full Data
Incomplete Data
d
Mean
SD
Mean
SD
0.1
0.0913
0.0387
0.0898
0.0446
0.2
0.1880
0.0396
0.1866
0.0456
0.3
0.2840
0.0389
0.2862
0.0484
0.4
0.3862
0.0388
0.3913
0.0508

408
MISSING VALUES AND OUTLIERS
Table 11.5
Finite Sample Behavior of Maximum-Likelihood Estimates with
Missing Values.a
Full Data
Incomplete Data
d
φ
θ
d
φ
θ
Mean
0.2559
-0.5102
0.1539
0.2561
-0.5086
0.1639
SD
0.0873
0.0729
0.1394
0.1070
0.0862
0.1795
Theoretical SD
0.0790
0.0721
0.1305
0.0883
0.0806
0.1459
a ARFIMA(1, d, 1) model with sample size n = 400 and parameters d = 0.3, φ = −0.5, and
θ = 0.2. The incomplete data have 80 missing observations selected randomly.
block of missing data. For simplicity, the analysis will be conducted by taking
into account the full past instead of the ﬁnite past of the time series.
EXAMPLE 11.2
In what follows we illustrate the eﬀects of missing values on the error
variance of in-sample and out-of-sample predictors.
Figure 11.2 to Figure 11.4 show the evolution of the one-step pre-
diction error variance for a fractional noise process with d = 0.45 and
σ = 1, from t = 1 to t = 200.
In Figure 11.2, the heavy line represents the evolution for the full
sample (no missing values) and the broken line indicates the evolution
in the presence of a data gap from t = 20 to t = 50. It is clear that the
prediction error variance increases from the beginning of the data gap
up to t = 50 and then decays rapidly to 1.
The eﬀect of missing values on out-of-sample forecasts can be ana-
lyzed with the help of Figure 11.3 and Figure 11.4. Unlike Figure 11.2
where the data from t = 101 to t = 200 were available, Figure 11.3 and
Figure 11.4 depict the evolution of the prediction error variance in the
case where the last observation is made a time t = 100 and no new data
are available from t = 101 to t = 200. Naturally, this situation is similar
to observing values from t = 1 to t = 100 and then predicting the future
values from t = 101 to t = 200.
Observe in Figure 11.3 that the prediction error variance after t = 100
is not greatly aﬀected by the presence of the data gap. On the other
hand, a data gap of the same length but located closer to the end of the
series may have a slightly greater eﬀect on the variance of the forecasts,
as suggested by Figure 11.4.

EFFECTS OF MISSING VALUES ON PREDICTION
409
Time
Prediction variance
0
50
100
150
200
1.0
1.2
1.4
1.6
1.8
2.0
2.2
Figure 11.2
Prediction error variance for a fractional noise process with d = 0.45,
σ2 = 1, and 200 observations. Heavy line: full sample. Broken line: Data gap from
t = 20 to t = 50.
Time
Prediction Variance
0
50
100
150
200
1.0
1.2
1.4
1.6
1.8
2.0
2.2
Figure 11.3
Prediction error variance for a fractional noise process with d = 0.45,
σ2 = 1, and 200 observations. Heavy line: Data gap from t = 100 to t = 200. Broken
line: Data gaps from t = 20 to t = 50 and from t = 100 to t = 200.

410
MISSING VALUES AND OUTLIERS
Time
Prediction variance
0
50
100
150
200
1.0
1.2
1.4
1.6
1.8
2.0
2.2
Figure 11.4
Prediction error variance for a fractional noise process with d = 0.45,
σ2 = 1, and 200 observations. Heavy line: Data gap from t = 100 to t = 200. Broken
line: Data gaps from t = 65 to t = 95 and from t = 100 to t = 200.
11.5
INTERPOLATION OF MISSING DATA
We now focus our attention on the problem of ﬁnding estimates for the miss-
ing values of a long-memory linear process. To this end, let {yt : t ∈Z} be a
stationary process with spectral density f(λ) and autoregressive representa-
tion
∞

j=0
πjyt−j = εt,
(11.3)
where π0 = 1 and {εt} is a white noise sequence with variance σ2.
Let M = {yobs} be the vector space generated by the observed series and
let ymis be a missing value. According to the projection theorem of Chapter 1,
the best linear interpolator of ymis based on the observed data yobs is given
by
ymis = E(ymis|M).
In particular, if ymis = y0 and the full past and full future is available, that
is, yobs = {yt, t ̸= 0}, then the interpolator may be expressed as
ymis = −
∞

j=1
αj(yj + y−j),
(11.4)

INTERPOLATION OF MISSING DATA
411
where the coeﬃcients αj are given by
αj =
∞
i=0 πiπi+j
∞
i=0 π2
i
,
for j ≥1. In this case, the interpolation error variance is
σ2
int = 4π2
 π
−π
dλ
f(λ)
−1
.
(11.5)
On the other hand, if ymis = y0 and the full past and part of the future is
available, that is, yobs = {yt, t ̸= 0, t ≤n}, then the best linear interpolator
of y0 may be written as
ymis = y0 −
n

j=1
βj,n(yj −yj),
where the coeﬃcients βj,n are given by
βj,n =
n−j
i=0 πiπi+j
n
i=0 π2
i
,
for j = 1, 2, . . . , n and yj is the best linear predictor of yj based on {yt, t < 0}.
In this case, the interpolation error variance is
σ2
int(n) =
σ2
n
i=0 π2
i
.
(11.6)
Notice that
σ2
int(n) →
σ2
∞
i=0 π2
i
,
as n →∞. But, recalling that the spectral density of process (11.3) is given
by
f(λ) = σ2
2π

∞

j=0
πje˙ıλj

−2
,
we conclude that
 π
−π
dλ
f(λ)
=
2π
σ2
 π
−π

∞

j=0
πje˙ıλj

2
dλ,
=
2π
σ2
∞

i=0
∞

j=0
πiπj
 π
−π
e˙ıλ(i−j)dλ
=
(2π)2
σ2
∞

i=0
π2
i .

412
MISSING VALUES AND OUTLIERS
Therefore,
4π2
 π
−π
dλ
f(λ)
−1
=
σ2
∞
i=0 π2
i
.
Hence, as expected,
σ2
int(n) →σ2
int,
as n →∞.
Finally, if yobs is a ﬁnite observed trajectory of a stationary time series,
then we may use the projection theorem discussed in Chapter 1 directly to
calculate the best linear interpolator of ymis by means of the formula:
ymis = E(ymisy′
obs)[E(yobsy′
obs)]−1yobs.
(11.7)
Naturally, the calculation of this general expression implies obtaining the
inverse of the variance-covariance matrix of the observed data and this pro-
cedure could be computationally demanding in the long-memory case.
EXAMPLE 11.3
Consider the fractional noise process FN(d). For this model we have
that
αj = Γ(j −d)Γ(1 + d)
Γ(j + d + 1)Γ(−d),
for j = 1, 2, . . . Therefore, the best linear interpolator of y0 based on
the full past and full future is given by
y0 = dΓ(1 + d)
Γ(1 −d)
∞

j=1
Γ(j −d)
Γ(j + d + 1)(yj + y−j),
and its interpolation error variance equals
σ2
int(d) = σ2 Γ(1 + d)2
Γ(1 + 2d).
(11.8)
Figure 11.5 displays the coeﬃcients αj for j = 1, 2, . . . , 40 and d = 0.4.
Observe that they approach zero rapidly. However, this decay rate may
be much faster for short-memory processes. For instance, for an AR(1)
model, αj = 0 for j > 1; see Problem 11.11.
The evolution of σint(d) as d moves from 0 to 0.5 is depicted in Fig-
ure 11.6. In this case, σint(0) = 1 and σint(0.5) = 0.8862.

INTERPOLATION OF MISSING DATA
413
j
j
0
10
20
30
40
-0.3
-0.2
-0.1
0.0
0.1
Figure 11.5
Example 11.3: Behavior of the interpolation coeﬃcients αj for a
fractional noise process with d = 0.4 and j = 1, 2, . . . , 40.
EXAMPLE 11.4
Extending the previous example, let {yt : t ∈Z} be an ARFIMA(1, d, 0)
process
(1 + φB)(1 −B)dyt = εt,
(11.9)
where |φ| < 1, d ∈(−1, 1
2) and {εt} is a white noise process with variance
σ2. Let ηj be the coeﬃcients of the diﬀerencing operator (1 −B)d,
(1 −B)d =
∞

j=0
ηjBj.
Consequently, the coeﬃcients of the autoregressive expansion (11.9)
(1 + φB)(1 −B)d =
∞

j=0
πjBj
are given by
πj =
 1,
j = 0,
ηj + φηj−1,
j ≥1.

414
MISSING VALUES AND OUTLIERS
0.0
0.1
0.2
0.3
0.4
0.5
0.80
0.85
0.90
0.95
1.00
Parameter d
Interpolation  error   S.D.
Figure 11.6
Example 11.3: Interpolation error standard deviation of a fractional
noise process based on the full past and full future, σint(d) for diﬀerent values of the
long-memory parameter d.
Hence,
∞

j=0
π2
j
=
(1 + φ2)
∞

j=0
η2
j + 2φ
∞

j=0
ηjηj+1
=

1 + φ2 −2φd
1 + d
 Γ(1 + 2d)
Γ(1 + d)2 .
Finally, the interpolation error of y0 based on {yt, t ̸= 0} for this ARFIMA(1, d, 0)
model is given by
σ2
int = σ2

1 + d
1 + φ2 + d(1 −φ)2
 Γ(1 + d)2
Γ(1 + 2d).
EXAMPLE 11.5
Consider an ARFIMA(p, d, q) process with autocovariance function γh(d, φ, θ)
for lag h. By analyzing the interpolation error variances (11.5) and (11.6)
we conclude that
σ2
int(n) −σ2
int = σ2
∞
j=n+1 π2
j
n
j=0 π2
j
∞
j=0 π2
j
.

INTERPOLATION OF MISSING DATA
415
But, from (2.24) the AR(∞) coeﬃcients of the ARFIMA process satisfy
πj ∼
φ(1)
θ(1)Γ(−d)j−d−1,
for large j. As a result,
σ2
int(n) −σ2
int ∼

σ2φ(1)2
θ(1)2Γ(−d)2 n
j=0 π2
j
∞
j=0 π2
j

∞

j=n+1
j−2d−2.
Now,
σ2
int(n) −σ2
int ∼

σ2φ(1)2
(2d + 1)θ(1)2Γ(−d)2(∞
j=0 π2
j )2

n−2d−1.
Finally, by taking into account that
∞

j=0
π2
j = γ0(−d, θ, φ)
σ2
,
we conclude
σ2
int(n) −σ2
int ∼

σ6φ(1)2
(2d + 1)θ(1)2Γ(−d)2γ0(−d, θ, φ)2

n−2d−1.
EXAMPLE 11.6
In order to illustrate the prediction and ﬁltering procedures in the con-
text of missing data we consider next the air pollution state space moti-
vating example described in Chapter 3. Assume that there are two data
gaps, one during the period t = 351 −400 and another for the period
t = 701 −800. As in the case analyzed in Chapter 3, here we use the R
package FKF to obtain predicted and ﬁltered states.
Figure 11.7(a) depicts the predicted ﬁrst state component while Fig-
ure 11.7(b) displays their estimated standard deviations.
The corre-
sponding plots of the second state component are shown in Figure 11.8.
One the that hand, the ﬁltered ﬁrst state component along with their
standard deviations are shown in Figure 11.9.
Finally, Figure 11.10
exhibits the ﬁltered second state component and the evolution of the
standard deviations.
From these plots, notice that the standard deviations of predicted
and ﬁltered states increase rapidly as missing observations appear and
then they decrease as the time series is actually observed.

416
MISSING VALUES AND OUTLIERS
(a)
Time
xt1
0
200
400
600
800
1000
-0.3
-0.2
-0.1
0.0
0.1
(b)
Time
xt1
0
200
400
600
800
1000
0.115
0.120
0.125
0.130
Figure 11.7
Air pollution state space model example, predicted states, t = 1000.
(a) predicted ﬁrst state component. (b) prediction standard deviations.
(a)
Time
xt2
0
200
400
600
800
1000
-0.4
-0.2
0.0
0.2
(b)
Time
xt2
0
200
400
600
800
1000
0.125
0.135
0.145
0.155
Figure 11.8
Air pollution state space model example, predicted states, t = 1000.
(a) predicted second state component. (b) prediction standard deviations.

INTERPOLATION OF MISSING DATA
417
(a)
Time
xt1
0
200
400
600
800
1000
-0.4
-0.2
0.0
0.2
(b)
Time
xt1
0
200
400
600
800
1000
0.09
0.10
0.11
0.12
0.13
Figure 11.9
Air pollution state space model example, ﬁltered states, t = 1000.
(a) ﬁltered ﬁrst state component. (b) ﬁltering standard deviations.
(a)
Time
xt2
0
200
400
600
800
1000
-0.4
-0.2
0.0
0.2
(b)
Time
xt2
0
200
400
600
800
1000
0.10
0.12
0.14
Figure 11.10
Air pollution state space model example, ﬁltered states, t = 1000.
(a) ﬁltered second state component. (b) ﬁltering standard deviations.

418
MISSING VALUES AND OUTLIERS
11.6
SPECTRAL ESTIMATION WITH MISSING VALUES
As discussed in previous chapters, spectral estimation is a fundamental tool
for ﬁnding parameter estimates in time series. But, as expected missing val-
ues also aﬀects the estimation of the spectral density. A simple technique for
handling this problem in this context is assigning the mean value to each miss-
ing observation and correcting the number of observations to the eﬀectively
collected. Thus, the modiﬁed periodogram is given by
I(λ) =
1
2πn∗

n

j=1
y∗
j e˙ıλ j

2
.
where
y∗
t =
 yt −¯y,
if yt is observed,
0
if yt is not observed,
and n∗is the number of values actually observed. Another technique is based
on the deﬁnition of the spectral density in terms of the autocovariances of the
process,
f(λ) = 1
2π
∞

h=−∞
γ(h)e˙ıλ h.
Thus, an estimator of the spectral density is
f(λ) = 1
2π
n−1

h=1−n
γ(h)e˙ıλ h.
In order to estimate the autocovariance γ(h) with incomplete data, the Parzen
method suggests the following approach. Deﬁne the variable
at =
 1,
if yt is observed,
0
if yt is not observed.
Then, an estimate of the autocovariance is given by
γ(h) =
n−|h|
t=1
at (yt −¯y)at+h(yt+h −¯y)
n−|h|
t=1
at at+h
EXAMPLE 11.7
Consider a MA(1) model
yt = εt + θ εt−1,

SPECTRAL ESTIMATION WITH MISSING VALUES
419
Table 11.6
Sample ACF of Full and Incomplete Data.
γ(0)
γ(1)
γ(0)
γ(1)
Full Sample
20% Missing
Estimate
1.6331
0.7943
1.6318
0.7934
S.D.
0.0927
0.0698
0.0985
0.0822
γ(0)
γ(1)
γ(0)
γ(1)
Full Sample
40% Missing
Estimate
1.6345
0.7934
1.6336
0.7912
S.D.
0.0914
0.0674
0.1102
0.1083
where εt is a normal white noise with zero-mean and unit variance.
Table 11.6 exhibits two set of simulations of the estimation of γ(0) and
γ(1), with a sample of 1000 observations, θ = 0.8 and two scenarios, 20%
and 40% of missing values. The estimates of the auto covariance function
with missing observation are given by the Parzen method. Note that
the ACF estimators seem to be unbiased, but the standard deviations
increases as the number of missing observations increases.
Additionally, we can illustrate the estimation of the spectral density of this
process which is given by
f(λ) = σ2
2π (1 + θ2 + 2 θ cos λ).
Figure 11.11 displays the theoretical spectral density of this MA(1) process
(heavy line) along with the raw periodogram (gray line), the smoothed pe-
riodogram (broken line) and the Parzen based estimator (dotted line) in the
case of complete data.
Notice that in this case the smoothed periodogram and the Parzen tech-
nique produce roughly similar results and follows the theoretical spectral den-
sity closely.
A similar plot is shown in Figure 11.12, where 40% of the data is missing.
In this case, note that the raw periodogram (gray line) displays lower levels,
as compared to the full sample situation. On the other hand, the smoothed
periodogram seems to underestimate the true spectral density. However, the
Parzen estimate follows closely their theoretical counterpart.

420
MISSING VALUES AND OUTLIERS
0.0
0.5
1.0
1.5
2.0
2.5

I()
0
 4
 2
3 4

Spectral Density
Periodogram
Parzen
Figure 11.11
Spectral estimation with complete data.
0.0
0.5
1.0
1.5
2.0
2.5

I()
0
 4
 2
3 4

Spectral Density
Periodogram
Parzen
Figure 11.12
Spectral estimation with incomplete data (40% of missing values).

OUTLIERS AND INTERVENTION ANALYSIS
421
11.7
OUTLIERS AND INTERVENTION ANALYSIS
Generally speaking, an outlier in statistics is a value that is far beyond what
is expected from its distribution. For example, a value equal to 20 in the case
of a standard normal distribution N(0, 1) is far larger from the usual range
of this random variable. In particular, the probability of observing a value of
20 or greater is practically zero (2.753624e-89). Outliers may cause serious
problems in estimation, inference and forecasting of time series models. To
illustrate this point, consider a very simple ﬁrst-order autoregressive model,
yt = φ yt−1 + εt,
(11.10)
where t = 1, . . . , 200 and εt is a Gaussian white noise sequence with zero-mean
and unit variance. Figure 11.13 shows a realization of this process, assuming
that the value of the input noise at time T = 100 is εT = 20.
The estimated AR(1) model with the original data is given by
Call:
arima(x = y1, order = c(1, 0, 0))
Coefficients:
ar1
intercept
0.6985
-0.1593
s.e.
0.0500
0.2491
Time
Series
0
50
100
150
200
0
5
10
15
20
Figure 11.13
Time series with modiﬁed noise.

422
MISSING VALUES AND OUTLIERS
Time
Residuals
0
50
100
150
200
-3
-2
-1
0
1
2
3
Figure 11.14
Residuals of the original model.
sigma^2 estimated as 1.154:
log likelihood = -298.46,
aic = 602.92
while the estimated model with the modiﬁed data is
fitCall:
arima(x = y2, order = c(1, 0, 0))
Coefficients:
ar1
intercept
0.6807
0.1710
s.e.
0.0512
0.3888
sigma^2 estimated as 3.148:
log likelihood = -398.79,
aic = 803.57
Note that even though the estimated parameter φ is similar in both cases,
the estimated noise variances are quite diﬀerent. This is not surprise since
the variance of the original noise is 1.1615 while the modiﬁed noise is 3.1671.
The impact on the residuals is quite dramatic if we compare Figure 11.14
and Figure 11.15.
On the other hand, the eﬀect of the prediction bands for
these two models is quite important see Figure 11.16. Observe that the bands
from the model ﬁtted to the data with an outlier are much larger than the
prediction bands from the original model.

OUTLIERS AND INTERVENTION ANALYSIS
423
Time
Residuals
0
50
100
150
200
0
5
10
15
20
Figure 11.15
Residuals from the modiﬁed noise model.
Time
Prediction Bands
202
204
206
208
210
-5
0
5
Figure 11.16
95% prediction bands for 10 step ahead forecasts.
Dotted line:
Original model, Broken line: Model with one outlier.

424
MISSING VALUES AND OUTLIERS
11.7.1
Methodology
When analyzing the diﬀerent eﬀects of one outlier on a time series we can
classify some of them into four types: Innovative Outlier (IO), Additive Outlier
(AO), Temporary Change (TC) and Level Shift (LS). These cases are brieﬂy
described next.
For simplicity, assume that the process under study corresponds to a sea-
sonal ARMA (SARMA) model described by
yt =
θ(B)
α(B)φ(B)εt,
where φ(B) and (B) are the autoregressive and moving-average polynomials,
and α(B) is a seasonal polynomial. Suppose now that the observed process
is a modiﬁed version of the original process yt given by
xt = yt + ω ψ(B) It(T),
where It(T) = 1 if t = T and It(T) = 1, otherwise.
In this context, T
represents the location of the outlier, ω indicates the magnitude of the impact
and the polynomial ψ(B) corresponds to the structure of the perturbation.
Among these events we can describe for instance an innovational outlier (IO)
deﬁned by
ψ(B) =
θ(B)
α(B)φ(B).
Note that in this case the observed model is given by
xt =
θ(B)
α(B)φ(B) [εt + ω It(T)].
Thus, an innovational outlier model corresponds to a perturbation of the noise
sequence at a speciﬁc time T. On the other hand, an additive outlier (AO) is
deﬁned by setting ψ(B) = 1, an level shift (LS) corresponds to
ψ(B) =
1
1 −B ,
while an temporary change model (TC) is obtained by setting
ψ(B) =
1
1 −δ B ,
see Figure 11.17 for an illustration of these outlier structures.
11.7.2
Known time of the event
When the time of the event is known, the R package dynia allows for the
estimation of the ARIMA model. In the case of model (11.10), the package
output is as follows,

OUTLIERS AND INTERVENTION ANALYSIS
425
(a)
Time
TC structure
0
50
100
150
200
0
5
10
15
20
(b)
Time
LS structure
0
50
100
150
200
0
5
10
15
20
25
Figure 11.17
TC and LS structures with n = 200, T = 101 and δ = 0.9.
$delta
[1] NA
$Int.Mod
Call:
arima(x = z, order = ..1, xreg = xreg2)
Coefficients:
ar1
intercept
xreg2
0.6882
-0.3406
20.1105
s.e.
0.0711
0.3630
1.1573
sigma^2 estimated as 1.324:
log likelihood = -156.25,
aic = 320.49
$‘p-value‘
NULL
That is, the impact is estimated as 20.1105 which is close to the true value of
20. At the same time, the estimated autoregressive parameter, φ = 0.6882, is
close to its true value φ = 0.7.

426
MISSING VALUES AND OUTLIERS
11.7.3
Unknown time of the event
When the location of the outlier is unknown, the R package tsoutliers can
estimate T, the parameters of the model as well as the magnitude of the
jump.
In what follows, we illustrate the application of this methodology to an
Additive Outlier (AO), Temporary Change (TC) and Level Shift (LS) and a
combination of these cases. Before discussing the estimation of these diﬀerent
types of outliers, we show their eﬀects on a simulated AR(1) series with φ = 0.8
and n = 200 observations.
EXAMPLE 11.8
Consider the AR(1) process deﬁned previously in (11.10) with φ = 0.8,
ω = 25, δ = 0.9 and T = 101. Figure 11.18 displays a simulated trajec-
tory of this model while Figure 11.19 shows the eﬀect of an innovation
outlier at time T = 100 on this trajectory. On the other hand, the im-
pact of an additive outlier is exhibited in Figure 11.20. The impact of a
temporary change outlier is exhibited in Figure 11.21. Finally, the eﬀect
of a level shift outlier is displayed in Figure 11.22. Note in this last case,
the eﬀect of the event is change the mean of the series permanently.
Time
Series
0
50
100
150
200
-4
-2
0
2
4
Figure 11.18
Simulated AR(1) series with φ = 0.8 and n = 200 observations.

OUTLIERS AND INTERVENTION ANALYSIS
427
Time
Series
0
50
100
150
200
-5
0
5
10
15
20
25
Figure 11.19
Simulated AR(1) series with φ = 0.8, n = 200 observations and
one IO at time T = 100.
Time
Series
0
50
100
150
200
-5
0
5
10
15
20
25
Figure 11.20
Simulated AR(1) series with φ = 0.8, n = 200 observations and
one AO at time T = 100.

428
MISSING VALUES AND OUTLIERS
Time
Series
0
50
100
150
200
-5
0
5
10
15
20
Figure 11.21
Simulated AR(1) series with φ = 0.8, n = 200 observations and
one TC at time T = 101.
Time
Series
0
50
100
150
200
0
5
10
15
20
25
30
Figure 11.22
Simulated AR(1) series with φ = 0.8, n = 200 observations and
one LS at time T = 101.

OUTLIERS AND INTERVENTION ANALYSIS
429
EXAMPLE 11.9
Additive Outlier. Based on the illustrative model (11.10), we have
generated an additive outlier of magnitude 20 at location T = 100. The
series is displayed in Figure 11.23. Observe that the tso function of the
tsoutliers package adequately estimates the time of the event as well as
the parameter φ and the magnitude of the impact. A graphical summary
of the ﬁtting is provided by Figure 11.24.
> outlier.detection=tso(y,tsmethod="arima",
args.tsmethod=(list(order=c(1,0,0))),
types=c("AO", "NULL", "NULL"))
> outlier.detection
Series: y
ARIMA(1,0,0) with non-zero-mean
Coefficients:
ar1
intercept
AO100
0.6891
0.5061
19.2970
s.e.
0.0507
0.2296
0.8401
Time
Serie
0
50
100
150
200
0
5
10
15
20
Figure 11.23
Simulated AR(1) with φ = 0.7, n = 200 and additive outlier AO
at location T = 100 and magnitude 20.

430
MISSING VALUES AND OUTLIERS
Original and adjusted series
0
5
10
15
20
Outlier effects
0
5
10
15
20
0
50
100
150
200
Figure 11.24
Detection of AO structure with n = 200 and T = 100.
sigma^2 estimated as 1.041:
log likelihood=-288.09
AIC=584.17
AICc=584.38
BIC=597.37
Outliers:
type ind time coefhat tstat
1
AO 100
100
19.3 22.97
EXAMPLE 11.10
Temporary Change. Similar results are observed when the type of
outlier is TC. The method estimate very well the location of the event,
the autoregressive parameter φ and the magnitude of the impact. Fig-
ure 11.25 exhibits the ﬁtting of the TC model as well as the eﬀects of
the outliers.
> outlier.detection=tso(y,tsmethod="arima",
args.tsmethod=(list(order=c(1,0,0))))
> outlier.detection
Call:
Series: y
ARIMA(1,0,0) with non-zero-mean
Coefficients:

OUTLIERS AND INTERVENTION ANALYSIS
431
Original and adjusted series
0
5
10
15
20
Outlier effects
0
5
10
15
20
0
50
100
150
200
Figure 11.25
Detection of TC structure with n = 200 and T = 100.
ar1
intercept
TC100
0.6978
-0.1561
20.0454
s.e.
0.0500
0.2487
1.0756
sigma^2 estimated as 1.15:
log likelihood=-298.12
AIC=604.24
AICc=604.44
BIC=617.43
Outliers:
type ind time coefhat tstat
1
TC 100
100
20.05 18.64
> plot(outlier.detection)
EXAMPLE 11.11
Level Shift. Consider the AR(1) time series with φ = 0.8 and 200
observations shown in Figure 11.22. This process suﬀers a level change
at time T = 101 of magnitude 25.
Notice that the method detects
adequately the location of the change and estimate very well the autore-
gressive parameter and the magnitude of the impact. The following is
the output from the two function of the tsoutliers package. Moreover,
Figure 11.26 exhibits a graphical analysis of the ﬁtted level shift model.
> outlier.detection=tso(y,tsmethod="arima", args.tsmethod=
(list(order=c(1,0,0))),types=c("NULL", "LS", "NULL"))

432
MISSING VALUES AND OUTLIERS
> outlier.detection
Series: y
ARIMA(1,0,0) with non-zero-mean
Coefficients:
ar1
intercept
LS101
0.7723
-0.0918
25.1193
s.e.
0.0451
0.4190
0.5571
sigma^2 estimated as 1.057:
log likelihood=-289.81
AIC=587.62
AICc=587.83
BIC=600.82
Outliers:
type ind time coefhat tstat
1
LS 101
101
25.12 45.09
Original and adjusted series
−5
0
5
10
20
30
Outlier effects
0
5
10
15
20
25
0
50
100
150
200
Figure 11.26
Detection of LS structure with n = 200, T = 101 and δ = 0.9.

OUTLIERS AND INTERVENTION ANALYSIS
433
EXAMPLE 11.12
Combination of events. In this last illustration we consider the case
where the process suﬀers two events, one temporary change at T = 50
and a level shift at T = 150. Figure 11.27 exhibits a simulated trajectory
of this time series. The magnitudes of the ﬁrst and second events are
20 and 30 respectively. Observe that the method appropriately estimate
the location of these two events as well as their magnitudes. On the
other hand, the estimate of the autoregressive structure is quite close to
its theoretical counterpart. Figure 11.28 displays the ﬁtting of the TC
model as well as the eﬀects of the outliers.
> outlier.detection=tso(y,tsmethod="arima",
args.tsmethod=(list(order=c(1,0,0))), types=c("LS", "TC", "NULL"))
> outlier.detection
Series: y
ARIMA(1,0,0) with non-zero-mean
Coefficients:
ar1
intercept
TC50
LS151
0.6796
-0.6013
20.1919
32.7470
s.e.
0.0528
0.2402
0.9621
0.4556
sigma^2 estimated as 0.918:
log likelihood=-275.55
Time
Serie
0
50
100
150
200
0
10
20
30
Figure 11.27
Detection of LS structure with n = 200, T = 101 and δ = 0.9.

434
MISSING VALUES AND OUTLIERS
Original and adjusted series
0
10
20
30
Outlier effects
0
5
10
15
20
25
30
0
50
100
150
200
Figure 11.28
Detection of a combination of two events, one TC outlier at T = 50
and LS at T = 150 with n = 200 and δ = 0.9.
AIC=561.09
AICc=561.4
BIC=577.58
Outliers:
type ind time coefhat tstat
1
TC
50
49
20.19 20.99
2
LS 151
150
32.75 71.88
11.8
BIBLIOGRAPHIC NOTES
The time series literature on missing values is extensive. For instance, Jones
(1980) develops a Kalman ﬁlter approach to deal with missing values in ARMA
models. Ansley and Kohn (1983), Harvey and Pierse (1984), and Kohn and
Ansley (1986) extend Jones’ result to ARIMA processes. Further extensions
of state space model techniques to ARFIMA models are considered by Palma
and Chan (1997) and Palma (2000). Chapter 4 of Shumway and Stoﬀer (2011)
discusses the Kalman ﬁlter modiﬁcations to account for missing data. An
alternative approach for computing the maximum-likelihood estimates called
the expectation maximization (EM) method to deal with data gaps is also
discussed extensively in that book. Other methods for treating missing values
are studied by Wilson, Tomsett, and Toumi (2003).

PROBLEMS
435
A good general reference to the problem of deﬁning the likelihood function
for incomplete data is the book by Little and Rubin (2002).
Estimation and interpolation of time series with missing values have been
studied by many authors; see, for example, Cheng and Pourahmadi (1997),
Bondon (2002), Damsleth (1980) and Pourahmadi (1989), among others. In
addition, Yajima and Nishino (1999) address the problem of estimating the au-
tocovariance function from an incomplete time series. A detailed study about
interpolation of missing data appears in Chapter 8 of Pourahmadi (2001).
Finally, there is an extensive literature on detection and estimation of out-
liers, see for example Tsay, Pe˜na, and Pankratz (2000), Chen and Liu (1993),
Choy (2001) and Cai and Davies (2003), among others.
Problems
11.1
Let {yt} be a ﬁrst-order autoregressive process such that
yt = φyt−1 + εt,
where the white noise sequence {εt} follows a standard normal distribution.
Suppose that we have observed the values {y1, y2, y3, y5} but y4 is missing.
(a) Calculate the joint density of y1, y2, y3, y4, y5, f(y1, y2, y3, y4, y5).
(b) Find z, the value that maximizes f(y1, y2, y3, y4, y5) with respect to
y4.
(c) Show that z corresponds to the smoother of y4, that is,
z = E[y4|y1, y2, y3, y5].
11.2
Suppose that {yt : t ∈Z} is a discrete-time stochastic process and we
have observed the values y1, . . . , ym−1, ym+1, . . . , yn but ym has not been ob-
served. Let f be the joint density of y1, . . . , yn and let g be the unimodal
conditional density of ym given the observed values. Assume that the mode
and the mean of g coincide. Let z be the value that maximizes f with respect
to ymis. Show that under these circumstances,
z = E[ym|y1, . . . , ym−1, ym+1, . . . , yn].
11.3
Consider an invertible process with autoregressive decomposition, yt =
εt−∞
j=1 πjyt−j and σ = 1. Let et(1) = yt−yt, where yt = Pt−1yt, Pt−1 is the
projection operator onto Ht−1 = {observed yt−1, yt−2, . . . }. Deﬁne the one-
step prediction error by σ2
t (1) = Var[et(1)]. Suppose that yt is the ﬁrst missing
value in the time series. Since up to time t there are no missing observations,
yt = ∞
j=1 πjyt−j, so that yt −yt = εt and σ2
t (1) = Var[yt −yt] = Var[εt] = 1.
(a) Show that since at time t + 1, Ht = Ht−1 (no new information is
available at time t) we have
σ2
t+1(1) = Var[yt+1 −yt+1] = Var[yt+1 −Ptyt+1],

436
MISSING VALUES AND OUTLIERS
with
Ptyt+1 = −π1Ptyt −π2yt−1 −π3yt−2 −· · ·
and
yt+1 −Ptyt+1 = εt+1 −π1(yt −Ptyt) = εt+1 −π1(yt −Pt−1yt).
(b) Verify that
σ2
t+1(1)
=
π2
1 Var[yt −Pt−1yt] + Var[εt+1]
=
π2
1σ2
t (1) + 1 = π2
1 + 1 ≥σ2
t (1).
11.4
Implement computationally the modiﬁed Kalman ﬁlter equations of
Subsection 11.2.4 for an fractional noise process.
11.5
Show that for k ≥1,
σ2
t+k(1) −1 ≥
 πk
πk+1
2
[σ2
t+k+1(1) −1].
11.6
Verify that for a fractional noise process, the one-step MSPE after a
missing value is strictly decreasing. That is, for all k ≥1,
σ2
t+k+1(1) < σ2
t+k(1).
11.7
Show that the one-step MSPE of an ARMA process with unit variance
innovations converges to 1 at a exponential rate, that is,
|σ2
t+k(1) −1| ≤ca−k,
as k →∞where |a| < 1.
11.8
Verify that the one-step MSPE of a fractional ARIMA process with
unit variance innovations converges to 1 at a hyperbolic rate, that is,
|σ2
t+k(1) −1| ≤ck−α,
as k →∞with c > 0 and α = 2 −2d > 0.
11.9
Let yt be the best linear interpolator of yt based on {yj, j ̸= t} and let
σ2 be its error variance
σ2 = Var(yt −yt).
Let xt be the standardized two-side innovation deﬁned by
xt = yt −yt
σ2
.

PROBLEMS
437
Let α(z) = 1 + ∞
j=1 αjzj + ∞
j=1 αjz−j where αj are the coeﬃcients
appearing in the best linear interpolator (11.4) and z ∈C with |z| = 1.
(a) Verify that xt may be formally expressed as
xt = 1
σ2 α(B)ψ(B)εt,
where ψ(B) is the Wold decomposition of the process (11.3).
(b) Prove that
α(z) = cπ(z)π(z−1),
where
c =
1
∞

j=0
π2
j
,
πj are the coeﬃcients in the AR(∞) expansion (11.3) and |z| = 1.
(c) Based on the previous results, show that
xt = c
σ2 π(B−1)εt.
(d) Let f be the spectral density of yt. Verify that the spectral density
of the process xt is given by
fx(λ) =
1
(2π)2f(λ).
11.10
Let ρ(k) be the autocorrelation of order k of a second-order stationary
process. Verify that the coeﬃcients αj of the best linear interpolator of y0
based on {yt, t ̸= 0} satisfy the formula
∞

j=−∞
αjρ(k −j) = 0,
(11.11)
for any k ∈Z, k ̸= 0.
11.11
Let {yt : t ∈Z} be the AR(1) process described by
yt = φyt−1 + εt,
where {εt} is a white noise sequence with variance σ2.
(a) Show that for this AR(1) model, the best linear interpolator of y0
with full past {yt, t < 0} and full future {yt, t > 0} is given by
y0 = φ(y1 + y−1)
1 + φ2
.

438
MISSING VALUES AND OUTLIERS
(b) Prove that the interpolation error variance is
σ2
int =
σ2
1 + φ2 .
(c) Verify formula (11.11) in this case, with
α1 = α−1 = −
φ
1 + φ2 .
11.12
Consider the MA(1) process yt = εt −θεt−1 where {εt} is a white
noise process with variance σ2 and |θ| < 1.
(a) Show that the coeﬃcients αj of the best linear interpolator are given
by
αj = θj,
for j ≥0 and then the best linear interpolator of y0 based on {yt, t ̸=
0} is
y0 = −
∞

j=1
θj(yj + y−j).
(b) Verify that in this case, formula (11.11) may be written as
αj −
1 + θ2
θ

αj−1 + αj−2 = 0,
for k ∈Z, k ̸= 0.
(c) Show that interpolation error variance is given by σ2
int = σ2(1 −θ2).
11.13
Prove that for a fractional noise process FN(d), the coeﬃcients αj of
the best linear interpolator satisfy
αj ∼Γ(1 + d)
Γ(−d) j−1−2d,
as j →∞.
11.14
Calculate the coeﬃcients αj of the best linear interpolator for the
ARFIMA(1, d, 0) discussed in Example 11.4. Recall that
πj =
 1,
j = 0,
ηj + φηj−1,
j ≥1,
and that
∞

i=0
ηiηi+j = γ0(j)
σ2 ,

PROBLEMS
439
where γ0(j) is the autocovariance function of a fractional noise.
11.15
Let yt be a stationary invertible process with AR(∞) representation
yt = εt −∞
j=1 πjyt−j and MA(∞) decomposition yt = ∞
j=1 ψjεt−j, where
the white noise sequence {εt} has variance σ2. Assume that the observations
y0, ..., ym−1 are missing and deﬁne
Hk,m = {ys, s < k, s ̸= 0, 1, . . . , m −1} .
Let e(t, m, k) be the error of the best linear predictor of yt given Hk,m, that
is, given all the available information before time k, and let σ2(t, m, k) be
its variance. The following theorems characterize the behavior of σ2(t, m, k)
during and after the data gap and specify the convergence rates.
Show that the variance σ2(k, m, k) satisﬁes
(a) For k = 0, . . . , m, σ2(k, m, k) = σ2 k
j=0 ψ2
j ,
(b) For k > m, σ2(k, m, k) −σ2 ≤σ2
ym2 max{j≥k−m} π2
j .
(c) limk→∞σ2(k, m, k) = σ2.
11.16
Verify that for an ARFIMA model and ﬁxed data gap length m we
have
(a) σ2
y−σ2(k, m, k) ∼ck2d−1 for some constant c > 0 and large k, k ≤m,
(b) σ2(k, m, k) −σ2 ∼ck−2d−2 for some constant c > 0 and large k,
k ≫m.
11.17
Show that for an ARFIMA(p, d, q) process we have
σ2(k, ∞, k) −σ2 ∼d2
k ,
as k →∞.
11.18
Show that for an ARFIMA(p, d, q) process the mean-squared predic-
tion error of an isolated missing observation behaves as follows:
(a) for k > 0,
σ2(k, 1, k) −σ2 = π2
kσ2(0, 1, k),
(b) for k > 0, if πk is a monotonically decreasing sequence, then σ2(k, 1, k)−
σ2 is a monotonically decreasing sequence converging to zero.
11.19
Show the missing values modiﬁcations of the state space Kalman
recursive ﬁlter equations.


CHAPTER 12
NON-GAUSSIAN TIME SERIES
As discussed in the previous chapters, most real-life time series are not neces-
sarily Gaussian. Moreover, in many situations it is not reasonable to approx-
imate them by a real-valued continuos random variable. Counting data and
positive observations are two examples where it is sometimes preferable to
apply a speciﬁc non-Gaussian modeling approach. Counting data are usually
the result of collecting the number of events occurred at a given time such as,
for instance, the number of daily patients attending an hospital, the number
of passenger enplanements, the number of animals of a certain species living
in a speciﬁc geographic area, among others time series. On the other hand,
positive data is often related to, for example, time between two events, useful
life of a certain machine, height or weight of a person, among other physical
measurements.
Generally speaking, there are two basic approaches to handle non-Gaussian
data exhibiting serial dependence. These are the so-called observation driven
models and the parameter driven processes. This chapter reviews these method-
ologies as well as techniques for handling zero-inﬂated observations.
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
441

442
NON-GAUSSIAN TIME SERIES
12.1
DATA DRIVEN MODELS
In this class of processes, the serial dependence observed in the data is usually
deﬁned in terms of its past values. Two important types of models belonging
to this class are the so-called integer-valued autoregressive (INAR) processes
and the conditional distribution models. In what follows we review some of
these approaches.
12.1.1
INAR Processes
The basic idea of the INAR model is to take advantage of previously well
established models in order to account for speciﬁc conditions of the data. For
example, consider the case of a counting time series with a ﬁrst-order autore-
gressive dependence. In this case, all the observations must be an integer
number, so that the usual AR(1) model,
yt = α yt−1 + εt,
model is no longer adequate since it may produce non-integer values of yt.
Consequently, its deﬁnition can be modiﬁed via the thinning operation α ◦yt−1
to obtain an integer-valued process.
Let α ∈[0, 1] and let z be a non-negative integer-valued random variable
and let yt be a sequence of non-negative integer-valued i.i.d. random variables
with mean α and variance σ2, independent of z. The thinning operation α ◦z
is deﬁned as
α ◦z =
z
X
j=1
yj.
Note that this expression generates a non-negative integer valued random
variable. Based on the previous deﬁnition, we can write an INAR(1) process
as follows,
yt = α ◦yt−1 + εt.
The ACF of yt is ρ(h) = α|h|. Figure 12.1 displays a simulated trajectory of
200 observations from the INAR(1) model with α = 0.6 and Poisson input
noise. Moreover, Figure 12.2 shows the sample ACF and sample PACF of
this process.
It is interesting to notice that the ﬁrst-order autoregressive
dependence structure is fully reﬂected in these integer-valued observations.
On the other hand, Figure 12.3 exhibits a simulated INAR (1) process with
α = 0.9, Poisson input noise and 400 observations. As expected from the high
level of positive dependence, note that the values of the process tend to stay
at the same level of previous observations. This fact is reﬂected in Figure 12.4
which shows the sample ACF and PACF. The ﬁrst-order autoregressive de-
pendence is clearly observable in these two plots.

DATA DRIVEN MODELS
443
Time
INAR series
0
50
100
150
200
0
1
2
3
4
5
6
7
Figure 12.1
Simulated INAR(1) process with α = 0.6, n = 200, and Poisson
input noise.
0
5
10
15
20
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
5
10
15
20
-0.2
-0.1
0.0
0.1
0.2
0.3
0.4
0.5
Lag
Partial ACF
(b)
Figure 12.2
(a) Sample ACF, and (b) sample PACF of the simulated INAR(1)
process with α = 0.6, n = 200 and Poisson input noise.

444
NON-GAUSSIAN TIME SERIES
Time
INAR series
0
100
200
300
400
0
10
20
30
40
Figure 12.3
Simulated INAR(1) process with α = 0.9, n = 400 and Poisson
input noise.
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
Lag
Partial ACF
(b)
Figure 12.4
(a) Sample ACF, and (b) sample PACF of the simulated INAR(1)
process with α = 0.9, n = 400 and Poisson input noise.

DATA DRIVEN MODELS
445
The class of INAR models is versatile enough to handle additional exoge-
nous variables such as trends or cycles. Consider for example the INAR(1)
model with a time regressor described by
yt
=
µt + zt,
µt
=
β0 + β1 t,
zt
=
α ◦zt−1 + εt.
A particular application of the R package inarmix allows for the simulation
and estimation of this model. Figure 12.5 shows a realization of INAR(1)
process with α = 0.8, n = 200 and linear trend µt = t −0.1. The output from
inarmix is given below. Note that the estimated values are very close to their
theoretical counterparts.
> oneclassfit <- inarmix(y~time,nclasses=1,id=subject,data=y)
> summary(oneclassfit)
Call:
inarmix(formula = y ~ time, nclasses = 1, id = subject, data = y)
Estimate
Std.Err
Z value
Pr(>z)
(Intercept) -5.975488e-01
5.472015e-05 -10920.08760 < 2.22e-16 ***
Time
Series
0
50
100
150
200
0
1
2
3
4
5
6
Figure 12.5
Simulated INAR(1) process with α = 0.8, n = 200, and linear trend
µt = t −0.1.

446
NON-GAUSSIAN TIME SERIES
time
1.814660e+00
2.115357e-04
8578.50244 < 2.22e-16 ***
autocorr.
7.867933e-01
1.308610e-03
601.24361 < 2.22e-16 ***
scale
1.243723e+00
8.862073e-03
27.50184 < 2.22e-16 ***
------------------------------
log-likelihood: -211.5405
BIC:
438.9760
AIC:
429.0811
12.1.2
Conditional Distribution Models
The following class of models allows for the handling a large family of time se-
ries data distributions, including as a particular case, the normal distribution.
We begin the discussion with a very simple example. Consider the Gaussian
AR(1) model yt = φ yt−1 +εt, where εt is a sequence of independent normally
distributed random variable with zero-mean and variance σ2. Thus, we can
write the model in terms of the conditional distribution speciﬁcation,
yt|yt−1
∼
N(µt, σ2),
µt
=
φ yt−1.
Note that in this formulation of the AR(1) model, E(yt|yt−1) = µt and
Var(yt|yt−1) = σ2. This simple model can be readily extended to an AR(p)
process,
yt|Ft−1
∼
N(µt, σ2),
µt
=
φ1 yt−1 + φ2 yt−2 + · · · + φp yt−p.
On the other hand, we can also allow for a diﬀerent conditional distribution
as well as a general link function ℓ(·),
yt|Ft−1
∼
D(µt),
ℓ(µt)
=
µ0 + φ1 yt−1 + φ2 yt−2 + · · · + φp yt−p.
Important examples of these more general processes are the Poisson distri-
bution and the Negative Binomial distribution in the case of integer-valued
time series and the Gamma and the log-Normal distributions in the case of
positive data.
EXAMPLE 12.1
As a ﬁrst illustration, Figure 12.6 displays a realization of a conditional
Poisson AR(1) process with φ = 0.7, µ0 = 5.0 and n = 200. Addition-
ally, Figure 12.7 exhibits its sample ACF and sample PACF. As in the
examples of INAR(1) models, in this case the ﬁrst-order autoregressive
dependence of the conditional model is also reﬂected in the observed
values of the process.

DATA DRIVEN MODELS
447
Time
Series
0
50
100
150
200
5
10
15
20
25
30
Figure 12.6
Simulated conditional Poisson process with φ = 0.7, µ0 = 5, and
n = 200.
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
5
10
15
20
0.0
0.2
0.4
0.6
Lag
Partial ACF
(b)
Figure 12.7
(a) Sample ACF, and (b) sample PACF of the simulated conditional
Poisson process with φ = 0.7, µ0 = 5, and n = 200.

448
NON-GAUSSIAN TIME SERIES
EXAMPLE 12.2
The second case considers a Poisson conditional distribution but with
negative ﬁrst-order autocorrelation. Figure 12.8 displays a realization of
a conditional Poisson AR(1) process with φ = −0.7 and n = 300 obser-
vations. Additionally, Figure 12.9 exhibits its sample ACF and sample
PACF. Notice that the negative ﬁrst-order autoregressive dependence of
the conditional model is reﬂected in the observed values of the process
and on their sample ACF and PACF.
Time
Series
0
50
100
150
200
250
300
60
80
100
120
140
Figure 12.8
Simulated conditional Poisson AR(1) process with φ = −0.7 and
n = 300.
0
5
10
15
20
-0.5
0.0
0.5
1.0
Lag
ACF
5
10
15
20
-0.6
-0.4
-0.2
0.0
Lag
Partial ACF
Figure 12.9
(a) Sample ACF, and (b) sample PACF of the simulated Conditional
Poisson AR(1) process with φ = −0.7, and n = 300.

DATA DRIVEN MODELS
449
EXAMPLE 12.3
This illustration considers a more complex serial dependence structure.
Figure 12.10 displays a realization of a conditional Poisson ARMA(1, 1)
process with φ = 0.8, θ = 0.6 and n = 300 while Figure 12.11 exhibits
its sample ACF and sample PACF. Observe that in this case, the ACF
decays slowly to zero.
Time
Series
0
50
100
150
200
250
300
50
100
150
200
Figure 12.10
Simulated conditional Poisson ARMA(1,1) process with φ = 0.8,
θ = 0.6 and n = 300.
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
5
10
15
20
-0.4
-0.2
0.0
0.2
0.4
0.6
0.8
Lag
Partial ACF
Figure 12.11
(a) Sample ACF, and (b) sample PACF of the simulated conditional
Poisson ARMA(1,1) process with φ = 0.8, θ = 0.6, and n = 300.

450
NON-GAUSSIAN TIME SERIES
EXAMPLE 12.4
The following example corresponds to an illustration with positive time
series data. Figure 12.12 exhibits 200 simulated observations of a Gamma
conditional distribution AR(1) model with autoregressive parameter φ =
0.7, µ0 = 3 and rate 1. Notice that in this case the mean of the process
is
E yt =
µ0
1 −φ = 10.
Thus, it is expected that this simulated series displays observations
around that mean.
The sample ACF and sample PACF of this conditional Gamma pro-
cess are shown in Figure 12.13. The AR(1) autocorrelation and partial
autocorrelation structure of the model is clearly noticeable in these plots.
In fact, as expected, the ﬁrst component of the sample PACF has a value
around 0.7.
Finally, Figure 12.14 shows the estimated density of the simulated
series.
Time
Series
0
50
100
150
200
5
10
15
20
Figure 12.12
Simulated conditional Gamma process with φ = 0.7, µ0 = 3, rate =
1 and n = 200.

DATA DRIVEN MODELS
451
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
5
10
15
20
0.0
0.2
0.4
0.6
Lag
Partial ACF
(b)
Figure 12.13
(a) Sample ACF, and (b) sample PACF of the simulated conditional
Gamma process with φ = 0.7, rate = 1, and n = 200.
0
10
20
30
0.00
0.02
0.04
0.06
0.08
Series
Density
Figure 12.14
Estimated density of the simulated Conditional Gamma process
with φ = 0.7, rate = 1, and n = 200.

452
NON-GAUSSIAN TIME SERIES
12.2
PARAMETER DRIVEN MODELS
The conditional distribution method can also be speciﬁed in terms of a latent
process {θt}, as for example,
yt|Ft−1
∼
D(θt),
θt
=
θ0 + α θt−1 + ηt,
where ηt is a white noise sequence. In this case, the parameter θt evolves
according to an AR(1) process and its serial dependence is transferred to the
observed process yt.
As an illustration, consider the following ARMA(1, 1) parameter driven
Poisson model deﬁned by
yt|Ft−1
∼
Poisson(exp(θt)),
θt
=
φ θt−1 + ηt −ϕ ηt−1,
with φ = 0.5, ϕ = 0.7 and σ2 = 1. Figure 12.15 exhibits a realization of
this process with 500 observations. Figure 12.15(a) shows the series yt while
Figure 12.15(b) displays the underlying parameter process θt. Recall that in
the context of a parameter driven modeling, the latent process θt corresponds
to a standard ARMA model. The sample ACF and PACF of this process is
exhibited in Figure 12.16.
(a)
Time
Series
0
100
200
300
400
500
0
50
150
250
350
(b)
Time
!t
0
100
200
300
400
500
-4
-2
0
2
4
6
Figure 12.15
Simulated parameter driven ARMA(1,1) Poisson process with φ =
0.5, ϕ = 0.7, σ2 = 1 and n = 500. (a) Time series, (b) evolution of θt.

ESTIMATION
453
0
5
10
15
20
25
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 12.16
(a) Sample ACF of the simulated parameter driven ARMA(1,1)
Poisson process with φ = 0.5, ϕ = 0.7, σ2 = 1 and n = 500, (b) Sample ACF of θt.
12.3
ESTIMATION
In order to provide a general framework for the estimation of diﬀerent data
contexts, let G(α, β) be a distribution corresponding to a discrete or contin-
uous nonnegative random variable with mean α and variance β. Let g be a
positive function; µ be a constant and {πj}j≥0 be a summable sequence of
real numbers, that is ∞
j=0 |πj| < ∞. A conditional distribution process {yt}
is deﬁned as
yt|Ft−1
∼
G(λt, g(λt)),
(12.1)
λt
=
µ
∞

j=0
πj −
∞

j=1
πjyt−j,
(12.2)
where Ft = {yt, yt−1, ...} represents the information up to time t.
In ad-
dition, the conditional distribution function, G, may depend on other pa-
rameters besides πj and µ. These parameters will be denoted by the vector
η.
Conditional on the information Ft−1, yt has distribution G with vari-
ance
Var[yt|Ft−1] = g(λt), which is a function of the conditional mean
E[yt|Ft−1] = λt.
Model (12.1) - (12.2) can be written in diﬀerent ways.
For instance, if we deﬁne the sequence εt = yt −E[yt|Ft−1] = yt −λt then
E(εt) = 0 and, if E[g(λt)] is ﬁnite and constant, then {εt} is an innovation pro-
cess, that is, a zero-mean, uncorrelated sequence with ﬁnite constant variance.

454
NON-GAUSSIAN TIME SERIES
Consider the model described by (12.1) - (12.2). Then, Var(εt) = E[g(λt)],
Cov(εt, εs) = 0, for all t ̸= s. If Var(εt) is a ﬁnite constant then {εt} is an
innovation process.
By replacing λt = yt −εt in (12.2) and since π0 = 1, we can write the
model as follows,
Π(B)(yt −µ) = εt,
where Π(B) = P∞
j=0 πjBj, B is the backshift operator such that Byt =
yt−1. This representation is familiar in time series analysis with independent
perturbations εt. For example, a conditional ARMA(p, q) can be written as
Π(B) = Φ(B)Θ(B)−1
where Φ(B) = 1 −φ1B −· · · −φpBp and Θ(B) = 1 −θ1B −· · · −θqBq.
Thus, model (12.1) - (12.2) with parametrization (12.3) satisﬁes,
Φ(B) (yt −µ)
=
Θ(B)εt,
(12.3)
εt|Ft−1
∼
(0, g(λt)).
(12.4)
To ensure the causality and invertibility of the ﬁlter (12.3), it is assumed
that the polynomials Φ(B) and Θ(B) have no common roots, these are all
located outside the unit circle. Note that in (12.3), even though the sequence
{εt} is not a strict white noise (independent), it is uncorrelated under some
conditions. Therefore, the process {yt} and the ARMA model with indepen-
dent input error sequence {εt} have the same autocorrelation function
The setup provided by equations (12.3) - (12.4) is general enough to allow
for modeling data with diverse distributions. For instance, the conditional dis-
tribution G, may belong to the exponential family with support in R+ such
as Binomial, Gamma or Poisson distributions. In what follows, we discuss
brieﬂy these examples corresponding to continuous and discrete conditional
distributions G, respectively.
(a) Conditional Poisson: Deﬁne the model as
yt|Ft−1 ∼Poi(λt)
where
Y
= {0, 1, 2, . . .}, η is null, and g(λt) = λt. In this case, E{g(λt)} = E[λt] = µ.
(b) Conditional Binomial: Consider the model yt|Ft−1 ∼Bin(m, pt) where n
is ﬁxed, Y = {0, 1, 2, . . .}, η is null, λt = mpt and g(λt) = λt(m −λt)/m. In
this case, g is concave and bounded by m/4.
(c) Conditional Gamma: Let yt|Ft−1 ∼Gamma(λt/β, β), with η = β > 0, Y
= (0, ∞), and g(λt) = βλt. For this distribution we have E{g(λt)} = βE[λt].
Assume that the time series data {y1, . . . , yn} are generated by model
(12.1)– (12.2) with parametrization (12.3). The vector of unknown parame-
ters is denoted by θ. First, we estimate the mean of the processes. A simple
estimator for the level of the process is the arithmetic mean, ˆµn = 1
n
Pn
t=1 yt.
Once the mean µ is estimated by ˆµn, the parameters δ and η may be
estimated by using the maximum likelihood method. For computing the like-
lihood, we replace µ by ˆµn. The conditional pseudo log-likelihood is given

ESTIMATION
455
by
L(θ) =
n
X
t=2
ℓt(θ).
where ℓt(θ) = log fθ(yt|Ft−1) and the contribution of the ﬁrst observation,
usually negligible for long time series, has been removed.
Given that the
conditional distribution G is a member of the exponential family we write:
fθ(yt|Ft−1) = a(λt, η)ψ(yt) exp
( m
X
i=1
bi(λt, η)Ri(yt)
)
(12.5)
where the functions a∗(·) and bi(·) depend on the information Ft−1 only
through λt and the functions ψ∗and Ri do not depend on δ and η. Then,
ℓt(θ) = C(λt) + ψ(yt) +
m
X
i=1
bi(λt)Ri(yt),
(12.6)
In order to obtain the maximum likelihood estimator (ˆδ, ˆη) and its precision,
we need to calculate the score and the Hessian. The score is given by,
∂L(θ)
∂θ
=
 n
X
t=2
∂ℓt(θ)
∂θ
!
and the Hessian matrix is
∇2L(θ) =
 n
X
t=2
∂2ℓt(θ)
∂2θiθj
!
i,j=1,...,p
.
EXAMPLE 12.5
Consider a conditional Poisson AR(1) model
yt ∼Poi(λt)
λt = µ0 + φ yt−1
In this case the conditional distribution of yt is
f(yt|λt) = e−λt λyt
t
yt! .
Consequently,
ℓt(θ) = −λt + yt log λt −log yt!,
where the parameter of interest is in this case θ = (µ0, φ). The parameter
space is given by Θ = (0, ∞) × (−1, 1).

456
NON-GAUSSIAN TIME SERIES
EXAMPLE 12.6
As an illustration of a time series model consisting of positive values,
consider the conditional Gamma AR(1) model
yt ∼Gamma(λt, β)
λt = µ0 + φ β yt−1
In this case the conditional distribution of yt is
f(yt|λt) =
βλt
Γ(λt)yλt−1
t
e−βyt
Thus,
ℓt(θ) = λt log(β) + log Γ(λt) + (λt −1) log yt −βyt,
where θ = (µ0, φ) is a two dimensional parameter vector.
EXAMPLE 12.7
An ACP(1,1) model is deﬁned
yt|Ft−1
∼
Poi(λt)
λt
=
µ0 + α yt−1 + β λt−1.
The mean of this process is given by
µ = E yt =
µ0
1 −α −β ,
while its variance is
Var yt = µ[1 −(α + β)2 + α2]
1 −(α + β)2
.
Notice that even though the conditional distribution of yt is equidis-
persed the distribution of the process is over dispersed since Var yt ≥
E yt.
Figure 12.17 shows the evolution of the expected value of an ACP(1,1)
process yt as a function of α ∈(0, 0.89) for β = 0.1 and µ0 = 1. On
the other hand, the variance of this ACP(1,1) process is displayed in
Figure 12.18. In addition, the ratio between the expected mean an vari-
ance of the ACP(1,1) process as a function of α ∈(0, 0.89) is reported in
Figure 12.19. Observe that in these three plots, the mean, variance and
ratio increase substantially as the ﬁrst-order autoregressive parameter α
increases.

ESTIMATION
457
0.0
0.2
0.4
0.6
0.8
0
20
40
60
80
100
!
µ
Figure 12.17
Expected value of the ACP(1,1) process yt as a function of α ∈
(0, 0.89) for β = 0.1 and µ0 = 1.
0.0
0.2
0.4
0.6
0.8
0
1000
2000
3000
4000
!
!2
Figure 12.18
Variance of the ACP(1,1) process yt as a function of α ∈(0, 0.89)
for β = 0.1 and µ0 = 1.

458
NON-GAUSSIAN TIME SERIES
0.0
0.2
0.4
0.6
0.8
0
10
20
30
40
!
!2 µ
Figure 12.19
Ratio between the expected mean an variance of the ACP(1,1)
process yt as a function of α ∈(0, 0.89) for β = 0.1 and µ0 = 1.
The autocorrelation structure of the ACP(1,1) model is given by
ρ(h) = (α + β)|h|−1 α [1 −β(α + β)]
1 −(α + β)2 + α2
The R package ACP allows for the estimation and forecasting of con-
ditional ARMA Poisson processes. Consider for example the ACP(1,1)
model with the following parametrization,
yt|Ft−1
∼
Poi(λt)
λt
=
a + b yt−1 + c λt−1.
Figure 12.20 exhibits a simulated ACP(1,1) process with a = 5, b =
0.6, c = 0.3 and n = 200. Furthermore, Figure 12.21 shows the sample
ACF and the sample PACF.
The estimated parameters from the ACP package are reported in the
following output. Notice that even though in this example the program
overestimate the intercept a, the estimators of b and c are close to their
theoretical counterparts.

ESTIMATION
459
0
50
100
150
200
40
60
80
100
Time
Series
Figure 12.20
Simulated ACP(1,1) process with a = 5, b = 0.6, c = 0.3 and
n = 200.
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
5
10
15
20
0.0
0.2
0.4
0.6
0.8
Lag
Partial ACF
(b)
Figure 12.21
(a) Sample ACF, and (b) PACF of the simulated ACP(1,1) process
with a = 5, b = 0.6, c = 0.3 and n = 200.

460
NON-GAUSSIAN TIME SERIES
> summary(mod1)
Call:
acp.formula(formula = z ~ -1, data = zdata, p = 1, q = 1)
Estimate
StdErr t.value
p.value
a
3.400014 1.599880
2.1252
0.03482 *
b 1 0.568413 0.065342
8.6990 1.322e-15 ***
c 1 0.368228 0.078570
4.6866 5.174e-06 ***
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
Figure 12.22 displays the one-step forecasts generated by the ACP
package.
The heavy line corresponds to the observations while the
broken line corresponds to the one=step predictors generated by the
ACP(1, 1) model. Notice that the predicted values are close to the true
values of the process. Observe also that one-step predictors have rel-
atively less variation than the true observations.
In particular, they
usually do not reach the highest o lowest observed values.
Time
Data and  Forecasts
0
50
100
150
200
40
60
80
100
Figure 12.22
Forecasts of the simulated ACP(1,1) process with a = 5, b = 0.6,
c = 0.3 and n = 200. Heavy line: Observations, broken line: One=step predictors.

ESTIMATION
461
12.3.1
Monte Carlo Experiments
In order to gain some insight on the ﬁnite sample performance of the quasi
maximum likelihood estimator described above, the results from several Monte
Carlo experiments are presented next.
In the following examples, we present ﬁrst a simulated trajectory of the
corresponding models and then we report the results for the parameter esti-
mates.
EXAMPLE 12.8
Figure 12.23 displays a simulated trajectory of 500 observations of a
conditional AR(1) Poisson model with φ = 0.7 and µ0 = 5 while Fig-
ure 12.24 shows its sample ACF and sample PACF.
Table 12.1 shows the simulation results from a Poisson conditional
distribution AR(1) model for diﬀerent values of the parameters φ and
µ0, and two sample sizes n = 500 and n = 1000. The simulations results
are based on 1000 repetitions. Observe that the estimated parameters
are close to their true values.
Time
AR(1)  Poisson Series
0
100
200
300
400
500
5
10
15
20
25
30
Figure 12.23
Simulated conditional AR(1) Poisson model with φ = 0.7, µ0 = 5
and n = 500.

462
NON-GAUSSIAN TIME SERIES
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
0.0
0.2
0.4
0.6
Lag
Partial ACF
(b)
Figure 12.24
(a) Sample ACF, and (b) PACF of the simulated conditional AR(1)
Poisson model with φ = 0.7, µ0 = 5 and n = 500.
Table 12.1
Monte Carlo Experiments for Conditional AR(1) Poisson Models.
n = 500
φ
µ0
ˆφ
ˆµ0
σ( ˆφ)
σ(ˆµ0)
0.7
2
0.6961
2.0191
0.0371
0.2282
0.4
2
0.3922
2.0263
0.0485
0.1831
0.7
15
0.6926
15.3586
0.0351
1.7483
0.4
15
0.3915
15.2095
0.0485
1.2391
n = 1000
φ
µ0
ˆφ
ˆµ0
σ( ˆφ)
σ(ˆµ0)
0.7
2
0.6971
2.0172
0.0239
0.1550
0.4
2
0.3994
1.9987
0.0298
0.1082
0.7
15
0.6961
15.2008
0.0231
1.1204
0.4
15
0.3951
15.1268
0.0303
0.7357

ESTIMATION
463
EXAMPLE 12.9
As a another illustration of the estimation methodology of a non-Gaussian
time series model, Figure 12.25 displays a simulated trajectory of 500
observations of a conditional ARMA(1,1) Poisson model with φ = 0.2,
θ = 0.6 and µ0 = 2. Its sample ACF and sample PACF are shown in
Figure 12.26. The mean of this process is given by
µ =
µ0
1 −φ −θ,
which yields in this case µ = 10. Observe that the values of this time
series ﬂuctuate around this mean.
Table 12.2 shows the simulation results from a Poisson conditional
distribution ARMA(1, 1) model for diﬀerent values of the parameters
φ, θ, µ0, and two sample sizes n = 500 and n = 1000. The simula-
tions results are based on 1000 repetitions. Observe that the estimated
parameters are close to their true values.
Time
Poisson ARMA Series
0
100
200
300
400
500
5
10
15
20
Figure 12.25
Simulated conditional ARMA(1,1) Poisson model with φ = 0.2,
θ = 0.6, µ0 = 2 and n = 500.

464
NON-GAUSSIAN TIME SERIES
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
0.0
0.2
0.4
0.6
Lag
Partial ACF
(b)
Figure 12.26
Sample ACF and PACF of the simulated conditional ARMA(1)
Poisson model with φ = 0.2, θ = 0.6, µ0 = 2 and n = 500.
Table 12.2
Estimation of Conditional Poisson ARMA(1,1) models.
n = 500
φ
θ
µ0
ˆφ
ˆθ
ˆµ0
σ( ˆφ)
σ(ˆθ)
σ(ˆµ0)
0.7
0.1
2
0.6974
0.0924
2.0921
0.0513
0.0660
0.4079
0.1
0.7
2
0.1021
0.5870
3.1073
0.0425
0.2830
1.7308
0.6
0.2
2
0.5918
0.1983
2.0984
0.0502
0.0695
0.4553
0.2
0.6
2
0.1988
0.5660
2.3504
0.0452
0.1331
1.1086
n = 1000
φ
θ
µ0
ˆφ
ˆθ
ˆµ0
σ( ˆφ)
σ(ˆθ)
σ(ˆµ0)
0.7
0.1
2
0.6987
0.0974
2.0393
0.0334
0.0436
0.2709
0.1
0.7
2
0.1016
0.6569
2.4127
0.0274
0.1439
1.3249
0.6
0.2
2
0.5970
0.1974
2.0530
0.0336
0.0461
0.2832
0.2
0.6
2
0.2000
0.5862
2.1378
0.0306
0.0726
0.5684

ESTIMATION
465
EXAMPLE 12.10
Finally, Table 12.3 reports the estimation results from a set of Monte
Carlo simulations of conditional Gamma ARMA(1,1) models with 500
observations, for diﬀerent values of the parameters φ, θ and µ0. Similarly
to the previous illustrations, in this case the estimated parameters are
close to their theoretical counterparts, even though the method seems
to overestimate the parameter µ0.
Table 12.3
Simulations of Conditional Gamma ARMA(1,1) models with 500
observations.
φ
θ
µ0
ˆφ
ˆθ
ˆµ0
bσ( ˆφ)
bσ(ˆθ)
bσ(ˆµ0)
0.5
0.3
2
0.4934
0.2936
2.1099
0.0612
0.4242
0.0439
0.3
0.6
2
0.2955
0.5982
2.1319
0.0547
0.6661
0.0381
0.4
0.4
5
0.3930
0.3966
5.2732
0.0687
1.1231
0.0425
0.6
0.3
3
0.5959
0.2927
3.3513
0.0609
0.8592
0.0512
0.2
0.5
2
0.1951
0.4822
2.1475
0.1265
0.7460
0.0388
12.3.2
Diagnostics
Non-Gaussian time series modeling involves both the experience of the analyst
and the type of the problem under study.
In practice, the nature of the
variables deﬁnes the sample space.
For example, counting processes lead
naturally to discrete positive data. Besides, the distribution of data can be
speciﬁed by means of tools such as histograms and q −q plots.
In some
cases, the conditional distribution deﬁnes the form of g(λt), for example,
for a Poisson distribution, g(λt) = λt.
But, in other situations we have
some ﬂexibility when deﬁning g(λt). The task of determining the conditional
variance can be helped by observing the patterns of data and correlation
structure from simulated time series. The sample autocorrelation function
of both the observations and their squares, may give some clues about the
underlying dependence structure of the data. Furthermore, the residuals et =
yt −ˆλt can be used for assessing the goodness of ﬁt, by checking the absence
of correlation on the residuals.
12.3.3
Prediction
The distribution of the process {yt} conditional on the past information, Ft−1,
has mean λt. Therefore a natural one-step predictor of λt is ˆλt which is based

466
NON-GAUSSIAN TIME SERIES
on (12.2),
ˆλt = ˆµ
t−1

j=0
ˆπj −
t−1

j=1
ˆπjyt−j,
where each ˆπj depends on the parameter estimates ˆδ, ˆη. Hence, the estimate
conditional distribution is yt|Ft−1 ∼G(ˆλt, g(ˆλt)) and construction of condi-
tional prediction bands for one-step forecasts is based on the quantiles of this
distribution.
12.4
DATA ILLUSTRATIONS
In what follows we illustrate the application of the techniques discussed in this
chapter to the analysis of two real-life time series. The ﬁrst case corresponds
to the study IBM trading volume and the second application concerns the
time series of glacial varves measurements discussed in Chapter 1.
12.4.1
IBM Trading Volume
Figure 12.27 exhibits the daily IBM trading volume in millions of transactions,
from January 2, 2011 to December 31, 2014. The sample ACF and sample
PACF of this time series are displayed in Figure 12.28.
Time
IBM   Volume
2011
2012
2013
2014
2015
5
10
15
Figure 12.27
Daily trading IBM volume in millions of transactions, from January
2, 2011 to December 31, 2014.

DATA ILLUSTRATIONS
467
0
5
10
15
20
25
30
-0.2
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
30
-0.2
0.0
0.2
0.4
0.6
Lag
Partial ACF
(b)
Figure 12.28
Sample ACF and sample PACF of the daily trading IBM volume
time series.
The ﬁtted ACP model for this time series is
Call:
acp.formula(formula = z ~ -1, data = zdata, p = 1, q = 1)
Estimate
StdErr t.value
p.value
a
1.425442 0.263163
5.4166 7.606e-08 ***
b 1 0.529531 0.048736 10.8653 < 2.2e-16 ***
c 1 0.163728 0.080931
2.0231
0.04333 *
---
Signif. codes:
0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
On the other hand, the standardized residuals from this model are reported
in Figure 12.29 along with the sample ACF. Notice that these residuals seem
to be white noise. This hypothesis is formally tested by means of the Box-
Ljung statistic,
> Box.test(e,lag=10,type="Ljung")
Box-Ljung test
data:
e
X-squared = 11.2483, df = 10, p-value = 0.3385
Furthermore, one-step forecasts are displayed in Figure 12.30. These pre-
dictions (broken line) seem to follows very closely the observe values (gray
line).

468
NON-GAUSSIAN TIME SERIES
(a)
Time
Residuals
0
200
400
600
800
1000
-2
0
2
4
6
8
10
0
5
10
15
20
25
30
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 12.29
(a) Residuals from the ACP(1,1) model of the IBM data.
(b)
Sample ACF of the residuals.
Time
IBM   Volume Predictions
2011
2012
2013
2014
2015
5
10
15
20
Figure 12.30
One-step predictions from the ACP(1,1) model. IBM data (gray
line) and One-step forecasts (broken line).

DATA ILLUSTRATIONS
469
12.4.2
Glacial Varves
As a second real-life data illustration of the techniques discussed in the pre-
vious sections for handling non-Gaussian time series, we consider a dataset
containing measurements of the thicknesses of the yearly varves at one loca-
tion in Massachusetts for the period 11,833–11,200 BC, see Figure 12.31.
As explained in Chapter 1, the analysis of this type of time series is highly
relevant in the context of climatic and paleoclimatic studies.
The time series discussed here corresponds to a sequence of positive obser-
vations with an estimated density depicted in Figure 12.32. From this plot,
it seems that the data could be ﬁtted by a Gamma model. Compare, for
example, with Figure 12.33 which shows the density of a sample of 1000 ob-
servations from a Gamma(λ, β) distribution with shape λ = 1.88 and rate
β = 0.068.
According to this preliminary study, a conditional Gamma ARMA model
is suggested for this time series. The selected model by the Akaike’s criterion
is an ARMA(1, 1) process.
The sample autocorrelation structure of these data is exhibited in Fig-
ure 12.34.
Year
Varve series
-11800
-11700
-11600
-11500
-11400
-11300
-11200
0
50
100
150
Figure 12.31
Glacial varves time series data.

470
NON-GAUSSIAN TIME SERIES
0
50
100
150
0.000
0.005
0.010
0.015
0.020
0.025
Varve data
Density
Figure 12.32
Density of the glacial varves time series.
0
50
100
150
0.000
0.005
0.010
0.015
0.020
0.025
Gamma random varable
Density
Figure 12.33
Density of a Gamma(λ, β) distribution with shape λ = 1.88 and
rate β = 0.068.

DATA ILLUSTRATIONS
471
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
0
5
10
15
20
25
-0.1
0.1
0.2
0.3
0.4
0.5
0.6
Lag
Partial ACF
(b)
Figure 12.34
(a) Sample ACF, and (b) sample PACF of the glacial varves.
The ﬁtted parameters are reported in Table 12.4 along with their standard
errors and Student tests.
Figure 12.35 exhibits the sample ACF of the residuals and the Box-Ljung
test suggests that they are compatible with the white noise hypothesis.
The one-step forecasts for this time series are displayed in Figure 12.36.
Observe that the predictors are close to their observe values. Additionally,
95% prediction bands are provided in Figure 12.37. Notice that the predic-
tions bands in this case are based on the 2.5% and 97.5% quantiles of the
corresponding Gamma(λt, β) distribution.
> Box.test(e,lag=10,type="Ljung")
Box-Ljung test
X-squared = 11.2483, df = 10, p-value = 0.3385
Table 12.4
Gamma ARMA(1,1) estimates for the Glacial Varves Data.
µ0
φ
θ
Parameter
0.2805
0.2762
0.6258
S.D.
0.0778
0.0298
0.0472
t-test
3.6054
9.2699
13.2588

472
NON-GAUSSIAN TIME SERIES
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
Figure 12.35
Sample ACF of the residuals from the ﬁtted glacial varves model.
-11800
-11700
-11600
-11500
-11400
-11300
-11200
0
50
100
150
Time
Varve series
Data
Predictions
Figure 12.36
One-step predictions of the glacial varves time series.

DATA ILLUSTRATIONS
473
-11800
-11700
-11600
-11500
-11400
-11300
-11200
0
50
100
150
200
Time
Varve series
Data
Predictions
Prediction Bands
Figure 12.37
Predictions bands of the glacial varves time series (95%).
12.4.3
Voting Intentions
In this data illustration we consider the UK voting intention time series data
introduced in Chapter 1. Figure 12.38 depicts the combined voting intentions
of both the Conservative Party and the Labor Party. Note that the combined
voting intention of these two political parties is expressed here as a proportion
of the total.
On the other hand, Figure 12.39 displays the sample ACF and the PACF.
Additionally, Figure 12.40 shows the estimated probability density of the data.
Based on the nature of the data as well as their dependence structure, the
following conditional AR(p) Beta model is proposed
yt|yt−1, yt−2, . . .
∼
Beta(at, bt),
E(yt|yt−1, yt−2, . . . )
=
µt,
µt
=
µ0 + φ1 yt−1 + φ2 yt−2 + · · · + φp yt−p,
where the Beta distribution has probability density given by
f(y) = Γ(at + bt)
Γ(at)Γ(bt)yat−1(1 −y)bt−1.
The mean of this Beta distribution is
µt =
at
at + bt

474
NON-GAUSSIAN TIME SERIES
Time
Proportion of Voting Intentions
1985
1990
1995
2000
2005
2010
0.5
0.6
0.7
0.8
0.9
1.0
Figure 12.38
Voting intention of the Conservative Party and the Labor Party
time series.
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
Lag
Partial ACF
(b)
Figure 12.39
(a) Sample ACF, (b) and PACF of the voting intention time series.

DATA ILLUSTRATIONS
475
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
5
Voting Intention
Density
Figure 12.40
Estimated probability density of the voting intention data.
and the variance is
σ2
t =
at bt
(at + bt)2(at + bt + 1).
The mean of the series is 0.7520 and the standard deviation is 0.0563. The
selected model according to the AIC is p = 2.
The maximum likelihood
parameter estimates are reported in Table 12.5, along with their estimated
standard deviations and t-tests.
Figure 12.41 exhibits the one-step predictors while Figure 12.42 shows the
residuals of the ﬁtted Beta AR(2) model along with their sample ACF.
Table 12.5
Beta AR(2) estimates for the Voting Intention Data.
μ0

φ1

φ2
Parameter
0.0664
0.6191
0.2950
S.D.
0.0181
0.0541
0.0541
t-test
3.6639
11.4436
5.4559

476
NON-GAUSSIAN TIME SERIES
Time
Proportion of Voting Intentions
1985
1990
1995
2000
2005
2010
0.5
0.6
0.7
0.8
0.9
1.0
Figure 12.41
Predicted voting intention of the Conservative Party and the Labor
Party time series.
(a)
Time
Residuals
1985
1990
1995
2000
2005
2010
-0.05
0.00
0.05
0.10
0
5
10
15
20
25
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(b)
Figure 12.42
Voting intention Beta model. (a) Residuals, and (b) sample ACF
of the residuals.

ZERO-INFLATED MODELS
477
From these plots, the residuals seem to be a white noise sequence. The
Box-Ljung test reported below indicates that this hypothesis is not rejected
at the 5% signiﬁcance level.
> Box.test(res,lag=10,type="Ljung")
Box-Ljung test
data:
res
X-squared = 6.3684, df = 10, p-value = 0.7834
12.5
ZERO-INFLATED MODELS
An important problem when modeling count time series data is that the pres-
ence of observations with zero counts may be greater than expected from, for
instance, a Poisson model. Due to this situation, there are several so-called
zero-inﬂated models (ZIM) that provide the ﬂexibility of controlling the prob-
ability of zero counts. For example, a zero-inﬂated Poisson distribution is
given by
f(yt = 0|λt)
=
ω + (1 −ω) e−λt
f(yt = h|λt)
=
(1 −ω) λyt
t
Γ(λt) e−λt,
h ≥1.
In this model, the parameter ω controls the level of probability of zero counts
in the time series. For ω = 0, the model corresponds to the usual Poisson
distribution. On the other hand, for ω = 1 all the probability is given to the
zero count. The mean of this zero-inﬂated Poisson distribution (ZIP) is
E(yt) = (1 −ω) λt
and its variance is
Var(yt) = λt(1 −ω) (1 + λt ω).
Another model of interest in this context is the Negative Binomial. In this
case, the probability is deﬁned by
f(yt|pt) = ω I{yt=0} + (1 −ω) Γ(k + yt)
Γ(k) yt! pyt
t (1 −pt)k,
where the probability os success pt is given by
pt =
k
k + λt
,

478
NON-GAUSSIAN TIME SERIES
with k is an overdispersion coeﬃcient and λ is an intensity parameter.
The conditional mean of this zero-inﬂated Negative Binomial (ZINB) dis-
tribution is
E(yt|λt) = (1 −ω) λt
and its conditional variance is given by
Var(yt|λt) = λt(1 −ω)

1 + λt ω + λt
k

.
From this formula, we can see that the parameter k > 0 allows for overdis-
persion of the distribution since as k →0, the variance increases. In fact, the
variance to mean ratio is
Var(yt|λt)
E(yt|λt) = 1 +

ω + 1
k

λt ≥1,
since ω ≥0 and k > 0.
In order to account for serial correlation in the data, we can consider the
latent process zt that satisﬁes, for instance, an AR(p) model
zt = φ1 zt−1 + φ2 zt−2 + · · · + φp zt−p + εt,
where εt is a white noise sequence with zero-mean and variance σ2. Addition-
ally, let st be the state space vector that represents the process zt. Regression
variables can be included in the model via the link function
log λt = log wt + xt β + zt
where log wt is an oﬀset variable and xt is a set of regressors. Thus, we can
write a Poisson-Gamma mixture model
st|st−1
∼
N(Φ st−1, Σ),
ut
∼
Bernoulli(ω)
vt
∼
Gamma(k, 1/k),
yt|st, ut, vt
∼
Poisson((1 −ut) vt λt).
EXAMPLE 12.11
The R package ZIM provides a framework for simulating and estimating
these zero-inﬂated time series models. As an illustration, consider the
following model
zt = φ zt−1 + εt,

ZERO-INFLATED MODELS
479
Time
ZIM  AR(1) series
0
50
100
150
200
0
10
20
30
40
50
60
Figure 12.43
Simulated Zero-inﬂated AR(1) model with φ = 0.4 and 200
observations.
where φ = 0.4, σ = 0.5,
log λt = log w + xt β + zt
where the oﬀset is constant log w = 2, the regressor is
xt =
 0
for
t = 1, . . . , 100,
1
for
t = 101, . . . , 200.
In this case, the regression parameter β = −1 while the zero-inﬂation
parameter ω is 0.3.
A simulated trajectory of 200 observations of this process is shown in
Figure 12.43 while its sample ACF is displayed in Figure 12.44.
Figure 12.45 exhibits the evolution of the underlying state sequence
st and its sample ACF is reported in Figure 12.46. Notice that in this
simple case, the states st correspond to the zt sequence.
On the other hand, the evolution of the Bernoulli sequence ut in
shown Figure 12.47 while the evolution of the Gamma sequence vt is
displayed in Figure 12.48.

480
NON-GAUSSIAN TIME SERIES
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
5
10
15
20
-0.1
0.0
0.1
0.2
0.3
Lag
Partial ACF
(b)
Figure 12.44
Zero-inﬂated time series model. (a) Sample ACF of yt, and (b)
sample PACF of yt.
Time
st
0
50
100
150
200
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
Figure 12.45
Zero-inﬂated time series model: State sequence st.

ZERO-INFLATED MODELS
481
0
5
10
15
20
0.0
0.2
0.4
0.6
0.8
1.0
Lag
ACF
(a)
5
10
15
20
-0.1
0.0
0.1
0.2
0.3
Lag
Partial ACF
(b)
Figure 12.46
Zero-inﬂated time series model.
(a) Sample ACF of the state
sequence st, (b) sample PACF of the state sequence st.
Time
ut
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
Figure 12.47
Zero-Inﬂated Time Series Model: Bernoulli sequence ut .

482
NON-GAUSSIAN TIME SERIES
Time
vt
0
50
100
150
200
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Figure 12.48
Zero-Inﬂated Time Series Model: Gamma sequence vt.
The estimated parameters are reported in the following output:
> dzim(y~X,dist=’zip’)
Call:
dzim(formula = y ~ X,dist = "zip")
(Zero-inflation parameter taken to be 0.3016)
Coefficients (log-linear):
Estimate Std. Error z value
Pr(>|z|)
(Intercept)
1.76327
0.21609
8.1600 3.349e-16 ***
X
-1.18216
0.26444 -4.4704 7.808e-06 ***
Coefficients (autoregressive):
Estimate Std. Error z value Pr(>|z|)
ar1
0.17734
0.21326
0.8316
0.4057
---
Signif. codes: 0 ’***’ 0.001 ’**’ 0.01 ’*’ 0.05 ’.’ 0.1 ’ ’ 1
(Standard deviation parameter taken to be 0.8023)
Criteria for assessing goodness of fit
loglik: -441.858
aic: 893.716
bic: 910.2076
tic: 975.3338

BIBLIOGRAPHIC NOTES
483
Observe that the estimates are close to their theoretical counterparts.
For example, the zero-inﬂation parameter ω is 0.3 and its estimate is
0.3016 while the regression parameter is −1 and its estimate is −1.18216.
On the other hand, the estimate of the autoregressive parameter is lower
than its true value.
12.6
BIBLIOGRAPHIC NOTES
The literature on non-Gaussian time series is vast. For instance, several mod-
els have been proposed to handle integer-valued time series or data with special
features such as positivity. The idea of dividing these models into two groups:
observation-driven and parameter-driven models was ﬁrst suggested by Cox
(1981). MacDonald and Zucchini (1997), Cameron and Trivedi (2013) and
the review byMcKenzie (2003) provide an excellent overview of the literature
in this area. In particular, observation-driven models for Poisson counts have
been developed by Davis, Dunsmuir, and Streett (2003), see also Grunwald,
Hyndman, Tedesco, and Tweedie (2000). Extensions to long-memory models
have been studied by Palma and Zevallos (2011). The INAR models were
introduced by McKenzie (1985) and Al-Osh and Alzaid (1987). Freeland and
McCabe (2004) discuss forecasting of INAR processes. The R package inarmix
is due to Henderson and Rathouz (2014). Kedem and Fokianos (2002) covers
a number of regression time series methodologies, including models for count
data. Methods for zero-inﬂated model are reviewed, for instance, by Yang,
Cavanaugh, and Zamba (2014) and Yang, Zamba, and Cavanaugh (2013),
among others.
Problems
12.1
Let z be a non-negative integer-valued random variable and consider
the thinning operation ◦deﬁned in Section 12.1
(a) Show that
E(α ◦z) = α E(z).
(b) Verify that
E(α ◦z)2 = (α2 + σ2) E(z).
(c) Show that
Var(α ◦z)2 = (α2 + σ2) E(z) −α2 (E z)2.
12.2
Consider the following INAR(1) model.
yt = α ◦yt−1 + εt,

484
NON-GAUSSIAN TIME SERIES
where E yt = µ and E εt = µε. Assume that the process yt is stationary.
(a) Show that
E yt = µ =
µε
1 −α.
(b) Verify that
Var yt = (α2 + σ2)µ −α2µ2 + σ2
ε.
12.3
Consider the conditional Gamma ARMA
yt|Ft−1 ∼Gamma(λt, β),
where
λt = µ0 + φ β yt−1 + θ λt−1,
with µ0 > 1, 0 ≤φ, 1, 0 ≤θ, 1 φ + θ < 1.
(a) Show that
E λt =
µ0
1 −φ −θ.
(b) Verify that
E yt =
µ0
β(1 −φ −θ).
(c) Show that λt can be written in terms of the inﬁnite past of the
observed process as follows
λt =
µ0
1 −θ + φβ
∞
X
j=0
θjyt−1−j.
12.4
Consider the conditional parameter driven Poisson MA(1)
yt|λt ∼Poi(λt),
with
λt
=
exp θt
θt
=
εt + ϕ εt−1,
where εt is a Gaussian white noise sequence with zero-mean and variance σ2.
(a) Verify that θt is a Gaussian sequence with zero-mean and variance
Var θt = (1 + ϕ2)σ2.

PROBLEMS
485
(b) Show that λt is a Log-Normal sequence of random variables with
E λt = exp[(1 + ϕ2)σ2/2],
and
Var λt = (exp[(1 + ϕ2)σ2] −1) exp[(1 + ϕ2)σ2].
(c) Verify that the expected value of yt is given by
E yt = exp[(1 + ϕ2)σ2/2].
12.5
Suppose that σ2 ∼Gamma(α, β).
An overdispersed distribution is
obtained as
F = ησ2
χ2η
,
where η is a parameter indicating the degrees of freedom of the χ2 distribution.
Verify that F ∝Fisher(a, b) and ﬁnd the parameters a and b.


APPENDIX A
COMPLEMENTS
As described in the preface, this text is intended for a general audience inter-
ested in time series analysis. Consequently, it focusses on methodologies and
applications rather than on theoretical aspects of this discipline. Neverthe-
less, the following appendix attempts to provides some additional mathemati-
cal and statistical concepts for the analysis of stationary stochastic processes.
Section A.1 begins with a description of the concepts of vector space, norm
and then proceeds to the deﬁnition of inner product and Hilbert spaces. There
is a strong motivation for introducing these spaces. Prediction, interpolation,
and smoothing are key aspects of time series analysis, and they are usually
obtained in terms of orthogonal projections onto some vector subspaces. A
Hilbert space does not only guarantee the existence of the best linear predic-
tor, interpolator, or smoother but also provides an explicit tool for obtaining
them through the projection theorem.
Several other fundamental concepts
about linear processes such as stationarity, singularity, regularity, causality,
invertibility, and ergodicity are discussed in Section A.2.
*
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
487

488
COMPLEMENTS
A.1
PROJECTION THEOREM
Let M be a vector space. An inner product is a function ⟨·, ·⟩: M →R
such that for all x, y, z ∈M and α, β ∈R it satisﬁes (i) ⟨αx + βy, z⟩=
α⟨x, z⟩+ β⟨y, z⟩, (ii) ⟨x, y⟩= ⟨y, x⟩, and (iii) ⟨x, x⟩≥0 and ⟨x, x⟩= 0 if and
only if x = 0. The vector space M endowed with an inner product ⟨·, ·⟩is
said to be an inner product space.
A norm on a vector space M is a function ∥· ∥: M →[0, ∞) satisfying for
all x, y ∈M and α ∈R (i) ∥x∥= 0 if and only if x = 0, (ii) ∥αx∥= |α|∥x∥,
and (iii) ∥x + y∥≤∥x∥+ ∥y∥. The inner product ⟨·, ·⟩induces the norm
∥x∥=
p
⟨x, x⟩in M. The sequence {xt} in M is said to be a Cauchy sequence
if and only if for all ε > 0 there is an integer n such that ∥xt −xs∥< ε for all
t, s > n. The vector space M is complete if and only if every Cauchy sequence
has a limit in M. With these deﬁnitions we are ready to introduce the Hilbert
spaces.
A Hilbert space is a complete inner product vector space. These spaces are
particularly important for time series analysis mainly because of the following
projection theorem..
Let M be a closed subspace of the Hilbert space H and let x ∈H. Then,
(a) there is a unique point y ∈M such that ∥x−y∥= infz∈M ∥x−z∥and (b)
y ∈M and ∥x −y∥= infz∈M ∥x −z∥if and only if y ∈M and ⟨x −y, z⟩= 0
for all z ∈M.
This results can be proved as follows. (a) By deﬁnition, there is a sequence
{yt} in M such that ∥x −yt∥→α where α = infz∈M ∥x −z∥. This is a
Cauchy sequence since (yt + ys)/2 ∈M and by the parallelogram law (see
Problem 7.22) we have
∥(yt −x) −(ys −x)∥2 + 4∥(yt + ys)/2 −x∥2 = 2∥yt −x∥2 + 2∥ys −x∥2.
Thus, ∥yt −ys∥2 = 2∥yt −x∥2 + 2∥ys −x∥2 −4α2, and then ∥yt −ys∥→0
as t, s →∞. Since H is complete, there is y ∈H such that ∥y −yt∥→0
as t →∞. Furthermore, since M is closed, we conclude that y ∈M. Now,
by the continuity of the norm we have that ∥x −y∥= limt→∞∥x −yt∥= α.
To show that this point is unique assume that there is another vector z ∈M
such that ∥x −z∥= α. By the parallelogram law we have
∥y −z∥2 = 2∥y −x∥2 + 2∥z −x∥2 −4∥(y + z)/2 −x∥2 = 0,
so that y = z. (b) Suppose that y ∈M and ∥x −y∥= infz∈M ∥x −z∥. If
z ∈M, then y + z ∈M and
∥x −y∥2 ≤∥x −(y + z)∥2 = ∥(x −y) −z∥2 = ∥x −y∥2 + ∥z∥2 −2⟨x −y, z⟩.
Therefore, 2⟨x −y, z⟩≤∥z∥2 for any z ∈M.
For ﬁxed z, λz ∈M for
any λ ≥0.
Assuming that ⟨x −y, z⟩≥0 (otherwise take −z), we have

PROJECTION THEOREM
489
0 ≤2λ⟨x −y, z⟩≤λ2∥z∥2, that is, 0 ≤2⟨x −y, z⟩≤λ∥z∥2. Consequently, as
λ →0 we have ⟨x −y, z⟩= 0. Conversely, consider a point y ∈M satisfying
⟨x −y, z⟩= 0 for all z ∈M. Then, for any z ∈M we have
∥x −z∥2 = ∥x −y∥2 + ∥y −z∥2 ≥∥x −y∥2.
Therefore, ∥x −y∥= infz∈M ∥x −z∥.
Given a subset M of a Hilbert space H, the span of M, denoted by spM,
is the subspace generated by all ﬁnite linear combinations of elements of M
and sp M denotes its closure in H, that is, sp M contains all the limits of
sequences in spM. In what follows, we illustrate the concept of Hilbert space
with several well-known examples.
EXAMPLE 1.1
Let n be a positive integer and consider the space Cn endowed with the
Euclidean inner product
⟨x, y⟩= xy =
n
X
j=1
xjyj,
where x = (x1, . . . , xn)′, y = (y1, . . . , yn)′, and y is the complex-conjugate
of y. Then, Cn is a Hilbert space with norm ∥x∥= (Pn
j=1 |xj|2)1/2.
EXAMPLE 1.2
Let {yt : t ∈Z} be a real, zero-mean stochastic process deﬁned on a
probability space (Ω, F, P). Then, L2(Ω, F, P) denotes the Hilbert space
with inner product ⟨x, y⟩= E(xy) and norm ∥y∥=
p
E(y2), where E(·)
stands for the expectation operator. This space will be simply called L2
throughout this book.
EXAMPLE 1.3
Let F be a distribution function and let f(λ) and g(λ) be two complex-
valued functions with domain [−π, π] such that
Z π
−π
|f(λ)|2 dF(λ) < ∞,
Z π
−π
|g(λ)|2 dF(λ) < ∞.
Then,
⟨f, g⟩F =
Z π
−π
f(λ) g(λ) dF(λ)
is an inner product and the generated Hilbert space is denoted by L2(F).
Observe that by H¨older’s inequality this inner product is well-deﬁned

490
COMPLEMENTS
since
|⟨f, g⟩|2 ≤
Z π
−π
|f(λ)|2 dF(λ)
Z π
−π
|g(λ)|2 dF(λ) < ∞.
EXAMPLE 1.4
As a particular case of Example 1.3, if F(λ) corresponds to the Lebesgue
measure over [−π, π] given by dF(λ) = dλ/2π, then the resulting Hilbert
space is denoted by L2(dλ).
A.2
WOLD DECOMPOSITION
The autocovariance function of second-order stationary process, γ(·), can be
written as
γ(h) =
Z π
−π
e˙ıhλ dF(λ),
where the function F is right-continuous, non-decreasing, bounded over [−π, π],
and satisﬁes the condition F(−π) = 0. F is called the spectral distribution of
γ(·). Furthermore, if
F(λ) =
Z λ
−π
f(ω) dω,
then f(·) is called the spectral density of γ(·).
A.2.1
Singularity and Regularity
Let Ft = sp{ys : s < t} be the past of the process at time t and F−∞=
T∞
t=−∞Ft.
The process is said to be deterministic, that is, perfectly pre-
dictable, if and only if yt ∈F−∞for all t ∈Z. In other words, a process
is deterministic if and only if F−∞= F∞. These processes are also-called
singular. On the other hand, a process is said to be purely nondeterministic
or regular if and only if F−∞= {0}.
A.2.2
Wold Decomposition
The following result, known as the Wold decomposition theorem, is a fun-
damental tool for analyzing stationary processes: Any stationary process is
the sum of a regular process and a singular process; these two processes are
orthogonal and the decomposition is unique.

WOLD DECOMPOSITION
491
According to the Wold representation theorem, a stationary purely nonde-
terministic process may be expressed as
yt =
∞
X
j=0
ψjεt−j = ψ(B)εt,
(A.1)
where ψ0 = 1, P∞
j=0 ψ2
j < ∞, {εt} is a white noise sequence with variance σ2.
The Wold expansion (A.1) is unique and εt ∈Ft+1 for all t ∈Z.
Recall now that a complex-valued stochastic process {ε(λ) : λ ∈[−π, π]}
is said to be a right-continuous orthogonal-increment process if it satisﬁes
the following conditions: (i) ∥ε(λ)∥< ∞, (ii) ⟨1, ε(λ)⟩= 0, (iii) ⟨ε(λ2) −
ε(λ1), ε(ω2) −ε(ω1)⟩= 0 for all disjoint intervals (λ1, λ2] and (ω1, ω2], and
(iv) ∥ε(λ + δ) −ε(λ)∥→0, as δ →0+, where ∥· ∥is the L2 norm. The
following result, usually called the spectral representation theorem, establishes
that a stationary process can be written as an integral of a deterministic
function with respect to a right-continuous orthogonal-increment sequence:
Let {yt} be a zero-mean stationary process with spectral distribution F. Then,
there exists a right-continuous orthogonal-increment process {ε(λ)} such that
F(λ) = ∥ε(λ) −ε(−π)∥2 for λ ∈[−π, π] and the process {yt} can be written
as
yt =
Z π
−π
e˙ıtλ dε(λ),
with probability one.
In time series analysis we often need to apply a linear ﬁlter {ψj : j ∈Z}
to a stationary process {xt}, that is, yt = P∞
j=−∞ψjxt−j = ψ(B)xt. The
following result provides general conditions on the linear ﬁlter {ψj : j ∈Z} so
that the ﬁltered process {yt} is also stationary. Furthermore, it establishes
conditions to ensure that this ﬁltering procedure is reversible:
Let {xt} be a zero-mean stationary process with spectral representation
xt =
Z π
−π
e˙ıλt dε(λ)
and spectral distribution F. Let {ψj : j ∈Z} be a sequence such that for
λ ∈(−π, π]
n
X
j=−n
ψje˙ıλj →ψ(e˙ıλ) =
∞
X
j=−∞
ψje˙ıλj,
as n →∞in the Hilbert space L(F) deﬁned in Example 1.3. Then,
(a) The ﬁltered process {yt} deﬁned by
yt =
∞
X
j=−∞
ψjxt−j

492
COMPLEMENTS
is zero-mean stationary.
(b) The spectral distribution of {yt} is given by
Fy(λ) =
Z λ
−π
|ψ(e−˙ıω)|2 dF(ω).
(c) The process {yt} can be expressed as
yt =
Z π
−π
e˙ıλtψ(e−˙ıλ) dε(λ).
(d) Assume that the sequence {ψj} is such that ψ(e−˙ıλ) ̸= 0 for λ ∈Λ,
where Λc has zero F-measure. Then, the process {xt} can be written as
xt =
Z π
−π
e˙ıλtπ(e−˙ıλ) dεy(λ),
where π(e˙ıλ) = 1/ψ(e˙ıλ) and dεy(λ) = ψ(e−˙ıλ) dε(λ).
The following is an another relevant result, especially in then study of
the properties of long-memory processes for proving to prove stationarity,
causality and invertibility.
Let {xt} be a zero-mean stationary process and let {ϕj : j ∈Z} be an
absolutely summable sequence. Then,
(a) The process {yt} deﬁned by
yt =
∞
X
j=−∞
ϕjxt−j = ϕ(B)xt
is zero-mean stationary.
(b) Suppose that the stationary process {xt} is given by
xt =
∞
X
j=−∞
ηjεt−j = η(B)εt.
Then, we can write
yt = ϕ(B)η(B)εt = η(B)ϕ(B)εt = ψ(B)εt,
where these equations are in the mean square sense and ψ(B) = η(B)ϕ(B).
A.2.3
Causality
The process deﬁned by (A.1) is said to be causal since the representation of
yt is based only on the present and past of the input noise, {εt, εt−1, . . . }.
Causality is a key property for predicting future values of the process.

WOLD DECOMPOSITION
493
EXAMPLE 1.5
Stationarity and causality are two related concepts but not necessarily
equivalent. To illustrate this point let {εt} be a white noise sequence
with Var(εt) < ∞and consider, for instance, the process yt = εt + εt+1.
This process is stationary but not causal, since yt depends on future
values of the sequence {εt}. On the other hand, the process yt = εt+εt−1
is causal and stationary.
A.2.4
Invertibility
A linear regular process (A.1) is said to be invertible if there exists a sequence
of coeﬃcients {πj} such that
εt = −
∞
X
j=0
πjyt−j,
(A.2)
where this expansion converges in L2. From (A.2), and assuming π0 = −1,
the process {yt} may be expressed as
yt = εt +
∞
X
j=1
πjyt−j.
(A.3)
A.2.5
Best Linear Predictor
Let {yt} be an invertible process as in (A.3). The best linear prediction of
the observation yt based on its history sp{yt, yt−1, yt−2, . . . } is given by
byt =
∞
X
j=1
πjyt−j.
(A.4)
Consequently, εt = yt −byt is an innovation sequence, that is, an orthogonal
process with zero-mean and constant variance representing the part of {yt}
that cannot be linearly predicted from the past.
On the other hand, suppose that we only have a ﬁnite trajectory of the
stationary process, {y1, y2, . . . , yn}. The best linear predictor of yn+1 based
on its ﬁnite past sp{y1, y2, . . . , yn}, byn+1, satisﬁes the equations
⟨yn+1 −byn+1, yj⟩= 0,
for j = 1, . . . , n. By writing the best linear predictor as
byn+1 = φn1yn + φn2yn−1 + · · · + φnny1,
we conclude that the coeﬃcients φnj satisfy the equations
n
X
i=1
φni⟨yn+1−i, yj⟩= ⟨yn+1, yj⟩,

494
COMPLEMENTS
for j = 1, . . . , n. If ⟨yi, yj⟩= γ(i−j), where γ(·) is an autocovariance function,
then we have that
n
X
i=1
φniγ(n + 1 −i −j) = γ(n + 1 −j).
A.2.6
Szeg¨o-Kolmogorov Formula
The spectral measure of the purely nondeterministic process (A.1) is abso-
lutely continuous with respect to the Lebesgue measure on [−π, π] and has
spectral density
f(λ) = σ2
2π |ψ(e−˙ıλ)|2.
Hence, its autocovariance function γ(·) may be written as
γ(h) =
Z π
−π
f(λ)e−˙ıλh dλ.
(A.5)
An explicit relationship between the spectral density and the variance of
the noise sequence {εt} is given by the Szeg¨o-Kolmogorov formula
σ2 = exp
 1
2π
Z π
−π
log[2πf(λ)] dλ

.
(A.6)
From this expression, observe that for a purely nondeterministic process,
σ2 > 0 or equivalently,
R π
−π log f(λ) dλ > −∞.
A.2.7
Ergodicity
Let {yt : t ∈Z} be a strictly stationary process and consider the σ-algebra
F generated by the sets S = {ω: y0(ω) = [yt1(ω), . . . , ytn(ω)] ∈B} where
B is a Borel set in Rn.
Let T be a shift operator T : Ω→Ωsuch that
TS = {ω: y1(ω) = [yt1+1(ω), . . . , ytn+1(ω)] ∈B}. This operator is measure
preserving since the process is strictly stationary. Let T −1S be the preimage
of S, that is, T −1S = {ω: T(ω) ∈S}. A measurable set is said to be invariant
if and only if T −1S = S. Similarly, a random variable X is said to be invariant
if and only if X(ω) = X(T −1ω) for all ω ∈Ω.
Let S be the set containing all the invariant measurable events. The process
{yt : t ∈Z} is said to be ergodic if and only if for all the events S ∈S, either
P(S) = 0 or P(S) = 1. Loosely speaking, this means that the only invariant
sets are the entire space or zero measure events.
The concept of ergodic process arises from the study of the long run behavior
of the average of a stationary sequence of random variables. Consider, for
example, the strictly stationary process {yt} and the average ¯yn = Pn
t=1 yt/n.

WOLD DECOMPOSITION
495
Notice that the sequence {¯yn} converges under some mild conditions. If {yt}
is a strictly stationary process with E(|yt|) < ∞, then there is an invariant
random variable ¯y such that E(|¯y|) < ∞,
lim
n→∞¯yn
=
¯y,
almost surely, and
E(¯y)
=
E(yt).
It is important to determine under what circumstances the limit ¯y is a
constant. For instance, as discussed in Example 1.6 below, if {yt} is a se-
quence of independent random variables with E |yt| < ∞, then the law of the
large numbers guarantees that the limit ¯y is equal to E(yt). Nevertheless, if
a process is such that all invariant random variables are constant with proba-
bility 1, by the above theorem we can conclude that the limit ¯y is a constant
with probability 1. Stationary sequences satisfying this requirement are called
ergodic processes.
EXAMPLE 1.6
A simple example of a strictly stationary ergodic process is a sequence
of independent and identically distributed random variables {ε1, ε2, . . . }
with E(|εt|) < ∞. In this case, if µ = E(εt) and
¯εn = 1
n
n
X
t=1
εt,
is the sample mean of size n, then by the strong law of large numbers
we have that limn→∞¯εn = µ.
The following result is important for the theoretical treatment of ergodic
systems since it establishes that strict stationarity and ergodicity is preserved
by measurable transformations. If {yt : t ∈Z} is a strictly stationary ergodic
process and φ : R∞→R is a measurable transformation such that xt =
φ(yt, yt−1, . . . ), then {xt} is a strictly stationary ergodic process.
An important consequence is that a linear ﬁlter of an independent and
identically distributed sequence is strictly stationary and ergodic as stated
in the following result.
If {εt} be a sequence of independent and identi-
cally distributed random variables with zero-mean and ﬁnite variance.
If
P∞
j=0 α2
j < ∞, then
yt =
∞
X
j=0
αjεt−j,
(A.7)
is a strictly stationary ergodic process.

496
COMPLEMENTS
EXAMPLE 1.7
For a trivial example of nonergodic process consider yt = εt + θεt−1 +
η where {εt} is a sequence of independent and identically distributed
random variables with zero-mean and unit variance, and η is a random
variable independent of {εt} with E(|η|) < ∞.
The process {yt} is
strictly stationary but not ergodic since ¯yn →η as n →∞and η is an
invariant random variable which is not necessarily a constant.
Ergodicity is an essential tool for carrying out statistical inferences about
a stochastic process. Usually, only one trajectory of the process is available
for analysis. Consequently, there is only one sample of the process at any
given time t. Thanks to the ergodicity of the process we can make up for
this lack of information by using the trajectory to estimate the moments
of the distribution of a strictly stationary process {yt : t ∈Z}. Speciﬁcally,
assume that E(|yi
tyj
s|) < ∞for positive integers i, j and deﬁne µij = E(yi
tyj
s).
Observe that this moment does not depend on t or s since the process is
strictly stationary. Therefore, we may write µij = E(yi
0yj
h), where h = s −t.
The moment µij can be estimated by the time-average
bµij(n) = 1
n
n−h
X
t=1
yi
tyj
t+h.
(A.8)
The following result, known as the pointwise ergodic theorem for stationary
sequences, states that this estimate is strongly consistent for every ﬁxed h If
{yt : t ∈Z} is a strictly stationary and ergodic process such that E(|yt|i+j) <
∞, then bµij(n) →µij almost surely as n →∞.
A.2.8
Fractional Brownian Motion
A Gaussian stochastic process B(t) with continuous sample paths is called
Brownian motion if it satisﬁes (1) B(0) = 0 almost surely, (2) B(t) has in-
dependent increments, (3) E[B(t)] = E[B(s)], and (4) Var[B(t) −B(s)] =
σ2|t −s|.
On the other hand, a standard fractional Brownian motion Bd(t) may be
deﬁned as
Bd(t) = c(d)
Z ∞
−∞
(t −s)d
+ −(−s)d
+ dB(s),
(A.9)
where
s+ =

s
if
s ≥0
0
if
s < 0,
and
c(d) =
p
Γ(2d + 2)cos πd
Γ(d + 1)
.

BIBLIOGRAPHIC NOTES
497
For a precise deﬁnition of the stochastic integral in (A.9), see, for example,
Taqqu (2003). The term standard in this deﬁnition corresponds to the fact
that the process Bd(t) has unitary variance at time t = 1, that is, Var[Bd(1)] =
1. In addition, the standard fractional Brownian motion (A.9) satisﬁes for
d ∈(−1
2, 1
2)
E[Bd(t)]
=
0,
(A.10)
Cov[Bd(t), Bd(s)]
=
1
2
 |t|2d+1 + |s|2d+1 −|t −s|2d+1
.
(A.11)
A real-valued stochastic process {y(t) : t ∈IR} is self-similar with index
H > 0 if for any constant c > 0 and for any time t ∈IR it satisﬁes
y(ct) ∼cHy(t),
where the symbol ∼means that both terms have the same distribution. The
self-similarity index H is called the scaling exponent of the process or the
Hurst exponent, in honor of the British hydrologist H. E. Hurst who developed
methods for modeling the long-range dependence of the Nile River ﬂows.
Observe that from (A.10) and (A.11) we conclude that
Bd(t) ∼N(0, |t|2d+1).
Hence, for any positive constant c we have
Bd(ct) ∼N(0, c2d+1|t|2d+1),
and then
Bd(ct) ∼cd+ 1
2 Bd(t).
Thus, the fractional Brownian motion Bd(t) is a self-similar process with self-
similarity index H = d + 1
2.
A.3
BIBLIOGRAPHIC NOTES
An excellent introduction to Hilbert spaces can be found in chapters 1 and 2
of Conway (1990). Additionally, Chapter 9 of Pourahmadi (2001) discusses
stationary stochastic processes in Hilbert spaces. A very readable proof of
the Szeg¨o-Kolmogorov formula is given in Hannan (1970). The concept of
ergodicity is also described in Chapter IV of that book. A good overview of
stochastic processes is given in the ﬁrst chapter of Taniguchi and Kakizawa
(2000). Carothers (2000) is a helpful reference on real analysis. The book
by Stout (1974) is a very good source for convergence results, especially its
Chapter 3 about limiting theorems for stationary random variables.


APPENDIX B
SOLUTIONS TO SELECTED PROBLEMS
This appendix provides solutions to some selected problems from diﬀerent
chapters throughout the book.
Problem 2.10. We can write
γx(k) =





0,
|k| ̸= n s
φ|k|/s σ2
1−φ2 ,
|k| = n s
Consequently,
ρx(k) =



0,
|k| ̸= n s
φ|k|/s,
|k| = n s
*
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
499

500
SOLUTIONS TO SELECTED PROBLEMS
for n ∈N0 and s ∈N. Finally, Analogously to the AR(1) process, we can
write
α(k) =

φ,
k = s
0,
k ̸= s.
Problem 2.11. From these plots we can obtain some preliminary ﬁndings.
Figure I seems to exhibit an exponential decay in both ACF graphs. It is a
candidate for an ARMA process. Figure II corresponds to a AR(1) process
with positive parameter.
Figure III is an exponential decay in both ACF
graphs, but the ACF decays faster. This series is a candidate for a MA(2)
process.
Figure IV is an exponential decay in both ACF graphs.
It is a
candidate for an ARMA process. Figure V seems to correspond to an AR(1)
process with negative parameter. Figure VI shows no correlation structure,
therefore it seems to indicate a white noise sequence. From this analysis we
can conclude: (a) Figure I. (b) Figure II. (c) Figure III. (d) Figure IV.
Problem 2.70. We consider all possible cases: Case 1). 0 < t < s ⇒|t| < |s|
Cov(B0(s), B0(t))
=
|t| + |s| −|s −t|
2
=
t + s −(s −t)
2
=
t + s −s + t
2
=
t
=
|t|
=
min{|t|, |s|}
Case 2). t < s < 0 ⇒s −t < 0 and ⇒|t| > |s|
Cov(B0(s), B0(t))
=
|t| + |s| −|s −t|
2
=
−t −s −(s −t)
2
=
−t −s −s + t
2
=
−s
=
|s|
=
min{|t|, |s|}

501
Case 3). t < 0 < s : |t| < |s| ⇒0 < |s| −|t| = s + t
Cov(B0(s), B0(t))
=
|t| + |s| −|s −t|
2
=
−t + s −(s + t)
2
=
−t + s −s −t
2
=
−t
=
|t|
=
min{|t|, |s|}
Case 4). t < 0 < s : |t| > |s| ⇒0 < |t| −|s| = −t −s
Cov(B0(s), B0(t))
=
|t| + |s| −|s −t|
2
=
−t + s −(−s −t)
2
=
−t + s + s + t
2
=
s
=
|s|
=
min{|t|, |s|}
Then, we conclude that
Cov(B0(s), B0(t)) = |t| + |s| −|s −t|
2
= min{|t|, |s|}
Problem 2.31. Part (a). The autocovariance function of the MA(∞) process
xt =
∞
X
j=0
ψjεt−j
is given by
γx(h) =
∞
X
j=0
ψjψj+h.
For the process yt we have
ψ0 = 1, ψj = 1
j

502
SOLUTIONS TO SELECTED PROBLEMS
Therefore,
γy(h) =
∞
X
j=0
ψjψj+h = 1 · 1
h +
∞
X
j=1
1
j
1
j + h.
The series can be written as a ﬁnite sum (see Hint), yielding
γy(h) = 1
h + 1
h
h
X
j=1
1
j .
(B.1)
In particular,
γ(0) = σ2
ε

1 +
∞
X
j=1
1
j2

= 1 + π2
6
as σ2
ε = 1 by assumption.
Part (b). By the Hint it follows that
1
h
h
X
j=1
1
j = C
h + log h
h
+ O
  1
h

h
For h →∞the ﬁrst term goes to 0. The last term also as
O
  1
h

h
= O
  1
h

1
h
1
h2 ≤C 1
h2 →0 for h →∞.
Thus it remains log h
h .
Part (c). For any m > n > 0 it holds
n
X
h=m
γ(h) =
n
X
h=m
1
h

1 +
h
X
j=1
1
j

>
n
X
h=m
1
h.
Part (d). From the preceding inequality it follows that Pn
h=0 γ(h) is not a
Cauchy sequence, thus it can not converge. Hence, by deﬁnition yt is a long
memory process.
Problem 2.24. For an ARFIMA(1, d, 0) we have that i = 0, j = 1, ψ(0) =
1,ξ1 = (φ1(1 −φ1))−1 and C = γ0(h)
σ2

φ2
1β(h) + β(−h) −1

. For an ARFIMA(0,d,1)
we have i = 1, j = 0, ψ(1) = ψ1, ξ0 = (1−ψ1)−1 and C = γ0(h)
σ2
[β′(h) + β′(−h) −1].
Here β(h) = F(d + h, 1, 1 −d + h, φ1), β′(h) = F(d + h, 1, 1 −d + h, 1) and
F(.) denotes for the Gaussian hypergeometric function. Finally,
γ0(h) = σ2
Γ(1 −2d)
Γ(1 −d)Γ(d)
Γ(h + d)
Γ(1 + h −d),

503
where σ2is the variance of the white noise process and Γ(.) is the Gamma
function.
Thus, the autocovariance function for an ARFIMA(1, d, 0) is
γ(h) = γ0(h)φ2
1F(d + h, 1, 1 −d + h, φ1) + F(d −h, 1, 1 −d −h, φ1) −1
φ1(1 −φ1)
,
and for an ARFIMA(0,d,1) the ACF is given by
γ(h) = γ0(h)[F(d + h, 1, 1 −d + h, 1) + F(d −h, 1, 1 −d −h, 1) −1]/(1 −ψ1).
The results provided by Hosking(1981) are the autocovariance functions for
(1 −B)dyt = at referred to a process xt = (1 −φB)yt in the case of and
ARFIMA(1,d,0) and (1−B)dxt = at are referred to a process yt = (1−θB)xt
in the case of ARFIMA(0,d,1), where B is the backward shift operator and
{at} is a white noise process.
Problem 2.20. Consider an stationary, causal and invertible ARFIMA(p, d, q)
process
φ(B)yt = θ(B)(1 −B)−dεt,
where εt ∼WN(0, σ2). Note that the model can be expressed as
yt = (1 −B)−dφ(B)−1θ(B)εt.
Then, the spectral density of the process may be written by
f(λ)
=
σ2
2π |1 −e−iλ|−2d |θ(e−iλ)|2
|φ(e−iλ)|2
=
σ2
2π
"
2 sin λ
2
#−2d |θ(e−iλ)|2
|φ(e−iλ)|2,
and when x →0 then sin x ∼x. Thus, the spectral density of the ARFIMA
model is approximated to
f(λ) ∼σ2
2π
|θ(1)|2
|φ(1)|2 |λ|−2d,
when |λ| →0.
Clearly, this spectral density may be also expressed as
f(λ) ∼|λ|−2d ℓ2(1/|λ|),
|λ| →0,
where
ℓ2(1/|λ|) = σ2
2π
|θ(1)|2
|φ(1)|2.
It is shown that the autocovariance function of the ARFIMA model is
γ(h) ∼cγ|h|2d−1,

504
SOLUTIONS TO SELECTED PROBLEMS
when |h| →∞and where
cγ = σ2
π
|θ(1)|2
|φ(1)|2 Γ(1 −2d) sin(πd).
Note that cγ is constant in h, i.e., may be speciﬁed as the slowly varying
function ℓ1(h) = cγ. On the other hand, the model presents the following
asymptotic relationship
ψj ∼θ(1)
φ(1)
jd−1
Γ(d),
when j →∞,
then, we can expressed the asymptotic coeﬃcient of the Wold expansion
ψj ∼jd−1ℓ3(j),
j →∞,
where
ℓ3(j) = θ(1)
φ(1)
1
Γ(d),
is constant in j. Furthermore, the ARFIMA(p, d, q) model follows
∞
X
h=−∞
|γ(h)| = ∞.
To verify this condition, we note that
∞
X
h=−∞
|γ(h)|
≥
∞
X
h=m
|γ(h)|
≥
lim
n→∞
n
X
h=m
|γ(h)|,
for 0 < m < n, where m is a value big enough such that
γ(h) ∼cγ|h|2d−1,
h ∈[m, ∞).
Thus, for d ∈(0, 1
2)
lim
n→∞
n
X
h=m
|cγ||h|2d−1 ∼lim
n→∞
|cγ|
2d |n|2d = ∞.
If d ∈(−1, 0), then
∞
X
h=−∞
|γ(h)| = γ(0)
∞
X
h=−∞
|ρ(h)| = γ(0)
 
1 + 2
∞
X
h=1
|ρ(h)|
!
.

505
It follows that |ρ(h)| < c|h|2d−1, with c > 0 constant. Then
∞
X
h=−∞
|γ(h)| < γ(0)
 
1 + 2
∞
X
h=1
c|h|2d−1
!
= γ(0)
 
1 + 2c
∞
X
h=1
1
|h|1−2d
!
=
γ(0) + 2γ(0)c
∞
X
h=1
1
|h|1−2d.
If 1 −2d ∈(1, 3) the series is convergent, that is, P∞
h=1
1
|h|1−2d < ∞. Also we
know that γ(0) < ∞, then we conclude
∞
X
h=−∞
|γ(h)| < ∞.
Problem 2.23. The spectral density of the process φ(B)yt = θ(B)(1−B)−dεt
is given by f(λ) = σ2
2π

2 sin λ
2
−2d |θ(e−iλ)|
2
|φ(e−1λ)|2 , where φ(B) and θ(B) are respec-
tively the autoregressive and moving-average polynomials of the backward
shift operator B and {εt} is a white noise process. As |λ| →0 those polyno-
mials evaluated at the unit circle converge to φ(1) and θ(1) while 2 sin λ
2 →λ.
Thus, the limiting spectral density function is f(λ) = σ2
2π
|θ(1)|2
|φ(1)|2 |λ|−2d, which
is the desired result.
Problem 2.25.
For a stationary, causal and invertible FN(d) process we
know that
ψk =
Γ(k + d)
Γ(k + 1)Γ(d).
Using Stirling’s approximation
Γ(n + α)
Γ(n + β) ∼nα−β,
n →∞,
it follows that
ψk ∼kd−1
Γ(d),
k →∞.
Now, note that the coeﬃcients correspond to
πk =
Γ(k −d)
Γ(k + 1)Γ(−d),

506
SOLUTIONS TO SELECTED PROBLEMS
Again, using Stirling’s approximation
πk ∼k−d−1
Γ(−d),
k →∞.
Finally, the autocovariance function is deﬁned by
ρ(k) = Γ(1 −d)
Γ(d)
Γ(k + d)
Γ(1 + k −d),
and the limit of ρ(k), when k →∞, is
lim
k→∞ρ(k) = Γ(1 −d)
Γ(d)
lim
k→∞
Γ(k + d)
Γ(1 + k −d) = Γ(1 −d)
Γ(d)
k2d−1.
Problem 2.29. The autocovariance function of a fractional Gaussian noise
is deﬁned by
γ(h) = σ2
2
 |h + 1|2d+1 −2|h|2d+1 + |h −1|2d+1
,
where σ2 = Var(yt).
Note that γ(k) = γ(−k), then we are just going to
analyze the case when k ≥1, because k = 0 is equivalent to the variance.
γ(k)
=
σ2
2
 |k + 1|2d+1 −2|k|2d+1 + |k −1|2d+1
=
σ2
 
|k + 1|2H
2
−|k|2H + |k −1|2H
2
!
.
where H = d + 1
2. If d ∈(0, 1
2) implies that H ∈( 1
2, 1), then the function
f(x) = x2H is strictly convex so we can use
(k + 1)2H + (k −1)2H
2
=
f(k + 1) + f(k −1)
2
≥
f
 
k + 1 + k −1
2
!
= f(k) = k2H.
Thus, it follows that γ(k) ≥0. If d ∈(−1
2, 0), then H ∈(0, 1
2), in this case
the function f(k) = k2H is strictly concave. Then we have that
(k + 1)2H + (k −1)2H
2
=
f(k + 1) + f(k −1)
2
≤
f
 
k + 1 + k −1
2
!
= f(k) = k2H,

507
In this case γ(k) ≤0.
Problem 2.30. If ηj are the coeﬃcients of an FN(d) process, then
ϕj
=
ηj −ηj−1
=
Γ(j + d)
Γ(d)Γ(j + 1) −Γ(j −1 + d)
Γ(d)Γ(j)
=
(j −1 + d)Γ(j −1 + d)
Γ(d)jΓ(j)
−Γ(j −1 + d)
Γ(d)Γ(j)
=
Γ(j −1 + d)
Γ(d)Γ(j)

−j −1 + d
j
−1

=
ηj
j −1 + d
j
−1

=
ηj[j −1 + d −j
j
]
=
ηj
(d −1)
j
.
Therefore,
ϕj
∼
lim
j→∞ηj
(d −1)
j
=
lim
j→∞
Γ(j −1 + d)
Γ(j + 1)
(d −1)
Γ(d)
=
lim
j→∞
Γ(j −1 + d)
Γ(j + 1)
1
Γ(d −1)
=
1
Γ(d −1) lim
j→∞
Γ(j −1 + d)
Γ(j + 1)
=
jd−2
Γ(d −1),
as j →∞.
Problem 2.71 The fractional Brownian motion satisﬁes for d ∈(−1/2, 1/2)
E[Bd(t)]
=
0
Cov[Bd(t), Bd(s)]
=
1
2(| t |2d+1 + | s |2d+1 −| t −s |2d−1).
Then it follows that
Bd(t) ∼N(0, | t |2d+1).
So that,
E[Bd(t + h) −Bd(t)]
=
0,

508
SOLUTIONS TO SELECTED PROBLEMS
and
Var[Bd(t + h) −Bd(t)]
=
Var[Bd(t + h)] + Var[Bd(t)] −2 Cov[Bd(t + h), Bd(t)]
=
| t + h |2d+1 + | t |2d+1 −2[1/2(| t + h |2d+1 + | t |2d+1 −| h |2d+1)]
=
| h |2d+1
On the other hand,
E[Bd(h) −Bd(0)] = 0
and
Var[Bd(h) −Bd(0)]
=
Var[Bd(h)] + Var[Bd(0)] −2 Cov[Bd(h), Bd(0)]
=
| h |2d+1 + | 0 |2d+1 −2[1/2(| h |2d+1 + | 0 |2d+1 −| h |2d+1)]
=
| h |2d+1 .
Therefore for (3) we have
Bd(t + h) −Bd(t)
∼
N(0, | h |2d+1)
Bd(h) −Bd(0)
∼
N(0, | h |2d+1)
Hence Bd(t + h) −Bd(t) ∼Bd(h) −Bd(0), as requested.
Problem 2.72. Given that fractional Brownian motion has stationary incre-
ments, meaning that
Bd(t + h) −Bd(t)
D∼
Bd(h) −Bd(0)
(B.2)
=
Bd(h)
(B.3)
have the same distribution. In particular, the p-th moments are equal
E[Bd(t + h) −Bd(t)]p = EBd(h)p.
Since a fractional Brownian motion is self-similar with H = d + 1/2,
Bd(h · 1)p = |h|p(d+1/2)Bd(1)p,
(B.4)
which proves the claim. In particular for p = 2,
E [Bd(t + h) −Bd(t)]2 = |h|2d+1 · E Bd(1)2.
(B.5)
Note that E Bd(1)2 equals 1, as
1 = Var(Bd(1)) = E (Bd(1) −E Bd(1))2 = EBd(1)2.
(B.6)

509
Thus dividing by h2 we get
E
Bd(t + h) −Bd(t)
h
2
= |h|2d−1.
Problem 2.73. If P∞
j=1[j(j + n)]−δ = O(n−δ) then

1
n−δ
∞
X
j=1
[j(j + n)]−δ

≤C.
Note that
1
n−δ
∞
X
j=1
[j(j + n)]−δ
=
1
n−δ
∞
X
j=1
 
1
j −
1
j + n
!δ 1
nδ
=
∞
X
j=1
 
1
j −
1
j + n
!δ
≤
∞
X
j=1
 
1
j
!δ
.
Since δ ∈(1, 2) we have that P∞
j=1
 
1
j
!δ
< C converge to a constant C > 0.
Thus,

1
n−δ
∞
X
j=1
[j(j + n)]−δ

≤C.
Finally, as this condition is satisﬁed, we conclude that
∞
X
j=1
[j(j + n)]−δ = O(n−δ).
Problem 3.5.
Part a).
Let xt = yt−1, hence the AR(1) process can be
written as
xt+1
=
φxt + εt,
yt
=
φxt + εt.
For this system,
F = φ,
G = φ,
H = 1.
(B.7)

510
SOLUTIONS TO SELECTED PROBLEMS
Thus the observability and controllability matrices are given by
O = (G′, F ′G′, F ′2G′, . . .)′ =



φ
φ2
...


,
and
C = (H, FH, F 2H, . . .) =
 1, φ, φ2, . . .
The system is controllable for all φ, as C has full rank 1 for every φ, it is
observable only for φ ̸= 0. Thus, it is a minimal state space representation if
φ ̸= 0.
Part b). A system is stable if F nx converges to zero in the norm of H for
all x ∈H for n →∞. This system is stable since
||F n|| = |φn| →0 for n →∞as |φ| < 1.
It is exponentially stable as for c > 1 and α = −log |φ| it holds
|φn| ≤ce−αn
for all n. Since |φ| < 1, it holds α > 0.
Part c). An AR(1) process yt has the Wold representation (|φ| < 1)
(1 −φB)yt = εt ⇔yt = (1 −φB)−1εt =
∞
X
j=0
φjεt−j.
Thus the MA(∞) coeﬃcients ψj equal
ψj = φj,
j = 0, 1, 2, . . . ,
with {ψj} ∈ℓ2.
The Hankel matrix is deﬁned as H : ℓ2 →ℓ2 with
H =





ψ1
ψ2
ψ3
. . .
ψ2
ψ3
ψ4
. . .
ψ3
ψ4
ψ5
. . .
...
...
...
...




=





φ1
φ2
φ3
. . .
φ2
φ3
φ4
. . .
φ3
φ4
φ5
. . .
...
...
...
...




= φ ·





1
φ
φ2
. . .
φ
φ2
φ3
. . .
φ2
φ3
φ4
. . .
...
...
...
...





Part d). For φ ̸= 0 the Hankel matrix has rank 1, as every column Hk,
k > 1 can be expressed as a factor multiplied by the ﬁrst column, i.e.
Hk = φ(k −1)H1,
∀k ≥1.
Part d). For φ ̸= 0 the Hankel matrix has rank 1, as one can write xt as
xt = H1(εt−1, εt−2, . . .)′

511
where H1 is the ﬁrst row of H.
Problem 3.7. The variance of the truncation error is of order O(1/m) for the
autoregressive representation truncated in m, and is of order O(m2d−1) for
the MA(∞) process, also truncated in m. This is an advantage of the autore-
gressive process. However, approximation by moving-average have also some
advantages: the algorithm implementation and the analysis of the estimates
theoretical properties are more simple.
Problem 3.9 Part a). Let F be a state matrix such that
F =


1
0
0
1
0
0
0
1
0


Thus we can deﬁne the following pattern for F n.
F
=


1
0
0
1
0
0
0
1
0


F 2
=


1
0
0
1
0
0
1
0
0


F 3
=


1
0
0
1
0
0
1
0
0


...
=
...
Now we can show that if x =
 x1
x2
x3
′, then
F nx =
 x1
x1
x1
′
∀n
Note that F nx ̸→0 when n →∞which indicates that the system is not
stable.
Part b). Let G and F be the observation and transition matrixes respec-
tively
G
=
 1
0
0 
,
F
=


1
0
0
1
0
0
0
1
0

.
The system is observable if the matrix O is full rank, where O is deﬁned
as it follows:
O =
 G′, F ′G′, F ′2G′, . . .
′ .

512
SOLUTIONS TO SELECTED PROBLEMS
Then,
G′
=


1
0
0


F ′G′
=


1
0
0


F ′2G′
=


1
0
0


...
=
...
That is, the observability matrix O is of the form
O =


1
1
1
. . .
0
0
0
. . .
0
0
0
. . .


′
.
Is clear that the rank of O is one and therefore this matrix isn’t full rank,
meaning that the system is not observable.
Part c). Let H and F be the lineal and transition matrices respectively
H
=


1
0
0

,
F
=


1
0
0
1
0
0
0
1
0

.
The system is said to be controllable if the matrix C =
 H, FH, F 2H, . . .

is full rank.

513
H
=


1
0
0


FH
=


1
1
0


F 2H
=


1
1
1


F 3H
=


1
1
1


...
=
...
That is, C is given by
C =


1
1
1
1
. . .
0
1
1
1
. . .
0
0
1
1
. . .

.
Note that C is full rank, then the system is controllable.
Part d). Given that tho state space system is not stable, there is no guar-
anty that the mean of the state vector does not increases through time. More-
over, we do not know if the initial value eﬀect will vanish over time. As the
system is not observable, there we cannot determine its initial state from the
observed process.
Problem 3.12. For this system we have that
F = φ,
G = θ,
H = 1.
Part a). A system is strongly stable if F nx converges to zero for all x ∈H for
n →∞. In the univariate case this means that φnx must converge to zero in
the norm of H. Thus
||φnx|| = φn||x||
must converge to 0. This holds for |φ| < 1, as ||x|| < ∞for every x ∈H. For
exponential stability there must exist c > 0 and α > 0 such that
||F n|| ≤ce−αn.
This means that
|φn| ≤ce−αn ⇔|φ| ≤ce−α

514
SOLUTIONS TO SELECTED PROBLEMS
Setting α = −log |φ| and c > 1 gives the inequality for all n. As |φ| < 1, it
holds α > 0.
Part b). The observability matrix of this system is given by
O = (G′, F ′G′, F ′2G′, . . .)′ =





θ
θφ
θφ2
...




= θ





1
φ
φ2
...





The rank of the matrix O equals 0, if and only if θ = 0. This value of θ
reduces the system to the trivial case yt = εt and the structure of xt gets lost
– i.e. it is not observable. For θ ̸= 0, however, the system is observable.
For controllability we have
C = (H, FH, F 2H, . . .) =
 1, φ, φ2, . . .
,
which is of full rank (equal 1) for all φ ∈R.
A system is minimal if and only if it is controllable and observable. This
can be garanteed for this system iﬀθ ̸= 0.
Part c).
To ensure that the Kalman recursions are stable, F −HG =
φ−θ = Φ must have eigenvalues with modulus smaller than 1, or equivalently
the polynomial 1 −Φz must be invertible for |z| ≤1. This is the case for
|φ −θ| < 1.
Solution to Problem 3.13. Part (a). Let x ∈H, and
F nx = F n−1Fx =










0
0
0
. . .
0
1
0
0
0
. . .
0
0
...
...
...
...
...
...
0
0
0
. . .
0
0
0
0
0
. . .
0
0










·









0
1
0
0
. . .
0
0
0
1
0
. . .
0
0
0
0
1
. . .
0
...
...
...
0
0
0
. . .
0
1
0
0
0
. . .
0
0









x →0
Hence F is stable.
Part (b). Consider the matrix norm induced by the sup norm is
∥F ∥∞= max
1≤i≤n
n
X
j=1
| aij |= max
1≤i≤n[ai1 + ai2 + . . . + ain] = 1

515
Part (d). We can write the observability matrix as,
O
=
(G′, F ′G′, F ′2G′, .......)′
=














ψn
0
0
. . .
. . .
. . .
0
ψn−1
ψn−1
0
. . .
. . .
0
ψn−2
ψn−2
ψn−2
0
0
. . .
0
ψn−3
ψn−3
ψn−3
ψn−3
0
. . .
0
...
...
ψn−4
ψn−4
ψn−4
0
. . .
0
...
...
...
...
...
...
0
ψ1
ψ1
ψ1
. . .
. . .
ψ1














′
.
Hence rank(O) = n if ψn ̸= 0.
Problem 3.14.
Part a).
Solving the system directly we have: yt = yt;
yt+1−p = φpyt−p + ... + φ1y1 + εt+1−p which renders the AR(p) process.
Part b). We can write
G′ =
 0
0
0
...
1
′ ,
F ′G′ =
  φp
φp−1
φp−2
...
φ1
′ ,
F 2′G′ =
 φp−1
φp−2
φp−3
...
P
j
φp−jφp−j−1

,
and so on, for j = 1, ..., n and we note that the observability matrix
O = (G′, F ′G′, F 2′G′, ...)
is full rank and the system is therefore observable.
Part c). We begin by solving
H =
  0
0
0
...
1 ′ ,
FH =
  0
0
0
...
φ1
′ ,
F 2H =
 φp−1
φp−2
φp−3
...
P
j
φp−jφp−j−1
′
,
and we note that the controllability matrix C = (H, FH, F 2H, ...) is full rank
and the system is controllable.

516
SOLUTIONS TO SELECTED PROBLEMS
Problem 3.16. We have that,
∞
X
j=0
∥F jzj∥
=
∞
X
j=0
|zj|∥F j∥
=
∞
X
j=0
|z|j∥F j∥
≤
∞
X
j=0
|z|jce−αj
≤
c
∞
X
j=0
 |z| e−αj
≤
c(1 −|z|e−α)−1.
Note that the series converge, as |z| ≤1 and if the values of α and c are
positive, i.e. always occurs that |e−α| < 1.
Part b). Using the following decomposition
(I −zF)
 I + zF + z2F 2 + . . . + znF n
= I −zn+1F n+1,
the limit is equal to
lim
n→∞(I −zF)
 I + zF + z2F 2 + . . . + znF n
=
lim
n→∞
 I −zn+1F n+1
,
(I −zF)
∞
X
j=0
F jzj
=
lim
n→∞
 I −zn+1F n+1
.
Then P∞
j=0 F jzj is (I −zF)−1 if and only if
lim
n→∞
 I −zn+1F n+1
= I.
Now, we just need to verify the convergence given by
lim
n→∞∥I −zn+1F n+1 −I∥
=
lim
n→∞∥zn+1F n+1∥
=
lim
n→∞|z|n+1∥F n+1∥
≤
lim
n→∞|z|n+1ce−α(n+1)
≤
0.
Which means that limn→∞
 I −zn+1F n+1
= I, i.e.,
(I −zF)
∞
X
j=0
F jzj
=
I,

517
and consequently,
∞
X
j=0
F jzj = (I −zF)−1 .
Problem 4.5. We can write
αj
=
1
2π
Z π
−π
f(λ)e−iλjdλ
=
1
2π
Z π
−π
m
X
k=−m
αkek(λ)e−iλjdλ
=
1
2π
Z π
−π
m
X
k=−m
αkeiλke−iλjdλ
=
1
2π
Z π
−π
m
X
k=−m
αk⟨ek, ej⟩dλ.
But,
⟨ek, ej⟩=

1,
si k = j
0,
otherwise.
That is, the sum is
m
X
k=−m
αk⟨ek, ej⟩= αj.
Hence,
1
2π
Z π
−π
m
X
k=−m
αk⟨ek, ej⟩dλ
=
1
2π
Z π
−π
αjdλ = αj.
Furthermore,
∥f(λ) ∥2
=
⟨f(λ), f(λ)⟩
=

m
X
j=−m
αjej(λ),
m
X
k=−m
αkek(λ)

=
m
X
j=−m
m
X
k=−m
αjαk⟨ej(λ), ek(λ)⟩.
However,
⟨ek, ej⟩=
 1,
si k = j
0,
otherwise.

518
SOLUTIONS TO SELECTED PROBLEMS
Thus,
∥f(λ) ∥2
=
m
X
j=−m
α2
j
and then
∥f(λ) ∥
=
v
u
u
t
m
X
j=−m
α2
j.
Problem 4.6. Assume that x′Γx > 0 for x ∈Rn, x ̸= 0 then Γ is positive
deﬁnite. Note that because 1
n log det(Γ) →
1
2π
πR
−π
log[2πfθ(λ)]∂λ we have that
fθ(λ) > c with c > 0 implies that det(Γ) > 0 for every λ ∈(−π, π] and the
result is proved.
Problem 4.7. Since the function g(λ) is continuos on [−π, π] for any ε > 0
there is a trigonometric polynomial φk(λ) = Pk
l=−k cleiλl such that | g(λ) −
φk(λ) |< ε for all λ ∈[−π, π]. For this polynomial we have
Z π
−π
φk(λ)In(λ)dλ
=
Z π
−π
k
X
l=−k
cleiλlIn(λ)dλ
=
k
X
l=−k
cl
Z π
−π
eiλlIn(λ)dλ
=
k
X
l=−k
clw(l, n)
By taking limits as n →∞
lim
n→∞
Z π
−π
φk(λ)In(λ)dλ
=
lim
n→∞
k
X
l=−k
clw(l, n)
=
k
X
l=−k
clγ(l) =
k
X
l=−k
cl
Z π
−π
eiλlf(λ)dλ
=
Z π
−π
k
X
l=−k
cleiλlf(λ)dλ =
Z π
−π
φk(λ)f(λ)d(λ).
This is true for an arbitrary ε, therefore the result is obtained.

519
Problem 4.10. Provided that {yt : t ∈Z} is stationary and E |γ(h)| < ∞
the Fourier transform of the series is given by
F(h) =
πZ
−π
eiλhf(λ)dλ
which corresponds the autocovariance of the process, γ(h).
On the other
hand, these autocovariances are the Fourier coeﬃcients of the expansion of
f(λ) scaled by 2π, such that f(λ) =
∞
P
h=−∞
Che−iλh with Ch = γ(h)/2π.
Problem 4.12 According to the Wold representation, a stationary process
may be expressed as:
yt =
∞
X
j=0
ψjεt−j
where ψ0 = 1, P∞
j=0 ψ2
j < ∞, ε is a white noise sequence with variance σ2.
Let
γ(h) = E(ytyt+h)
=
E(
∞
X
j=0
ψjεt−j
∞
X
i=0
ψiεt+h−i)
=
E(
∞
X
j=0
∞
X
i=0
ψjψiεt−jεt+h−i) =
∞
X
i,j=0
ψjψiE(εt−jεt+h−i)
=
σ2
∞
X
j=0
ψjψj+h
Similarly for γ(h) = E(yt, yt−h). Hence
γ(h) = σ2
∞
X
j=0
ψjψj+|h|.
Problem 4.16.
a) The periodogram is deﬁned as
I(λ) =
1
2πn

n
X
j=1
yjeiλj
2
,
for λ ∈[−π, π] and |z| =
√
zz, where z is the complex conjugate of z.

520
SOLUTIONS TO SELECTED PROBLEMS
Thus,
1
n

n
X
j=1
yjeiλj
2
=
1
n
n
X
j=1
yjeiλj
n
X
m=1
ymeiλm
=
1
n
n
X
j=1
yjeiλj
n
X
m=1
yme−iλm
=
1
n
n
X
j=1
n
X
m=1
yjymeiλ(j−m)
=
1
n
n
X
j=1
y2
j + 1
n
X
j̸=m
yjymeiλ(j−m)
As the functions eiλs are ortogonal, i.e.
Z π
−π
eiλke−iλsdλ =
(
0
k ̸= s
2π
k = s ,
it follows that the integral is diﬀerent to zero, only for k = −(j −m).
Z π
−π
eiλkI(λ)dλ
=
1
2π
Z π
−π
eiλk

1
n
n
X
j=1
y2
j + 1
n
X
j̸=m
yjymeiλ(j−m)

dλ
=
1
2π
Z π
−π
eiλk

1
n
n
X
j=1
yjyj+ke−iλk

dλ
=
1
n
n
X
j=1
yjyj+k
Note that this sum is only diﬀerent from 0 for |k| < n, as otherwise no
observations are available. Replace now yj by yj −y. This gives the
result.
b) Deﬁne the random variable
δt := (yt −yn)(yt+k −yn).
It holds
Eδt = γ(k),
and by the strong law of large numbers
1
n
n
X
t=1
δt →Eδt,

521
for N →∞.
Additionally we can write
1
n = n −k
n
1
n −k ,
and as for every ﬁxed k the term n−k
n
converges to 1 for n →∞, we get
that
lim
n→∞w(k, n) = γ(k).
Problem 5.5. The log-likelihood function of a zero-mean stationary Gaussian
process {yt} is given by (see Expression 4.1, page 66)
L(θ) = −1
2 log det Γθ + 1
2y′Γ−1
θ y,
where y = (y1, . . . , yn)′ and Γθ = E yy′ and θ is the parameter vector. The
maximum likelihood estimator is obtained by maximizing the log-likelihood
with respect to θ.
By the Durbin Levinson algorithm, the log-likelihood function can be ex-
pressed as
L(θ) = −1
2
n
X
t=1
log νt−1 −1
2
n
X
t=1
e2
t
νt−1
,
where et = yt −byt and νt satisﬁes
φtt
=
(νt−1)2
"
γ(t) −
t−1
X
i=1
φt−1,iγ(t −i)
#
φtj
=
φt−1,j −φttφt−1,t−j, j = 1, . . . , t −1,
ν0
=
σ2
y,
νt
=
νt−1(1 −φ2
tt), j = 1, . . . , t −1.
Another expression for L(θ) can be obtained by the state space approach,
L(θ) = −1
2
"
n log 2π +
n
X
t=1
log ∆t + n log σ2 + 1
σ2
n
X
t=1
(yt −y)2
∆t
#
.
Diﬀerentiating L(θ) with respect to σ2 (omitting the factor −1
2) gives
d
dσ2 L(θ) = n
σ2 −
 1
σ2
2
n
X
t=1
(yt −y)2
∆t

522
SOLUTIONS TO SELECTED PROBLEMS
Setting equal to zero and multiplying by σ4 > 0 gives
n
σ2 −
 1
σ2
2
n
X
t=1
(yt −y)2
∆t
=
0
n
X
t=1
(yt −y)2
∆t
=
nσ2
Thus the ML estimator for σ2 equals
bσ2
ML = 1
n
n
X
t=1
(yt −y)2
∆t
.
Problem 5.8. Let yt be a FN(d) process with d ∈(−1, 1
2). Thus,
yt = (1 −B)−dεt
with εt ∼WN(0, σ2). The process is stationary, causal and invertible. Fur-
thermore, we know that in a process FN(d)
byt+1 = φt1yn + · · · + φtty1,
where the coeﬃcients of the prediction are given by
φtj = −
 t
j
 Γ(j −d)Γ(t −d −j + 1)
Γ(−d)Γ(t −d + 1)
.
On the other hand, we need to determine νt deﬁned by
νt = γ(0)
tY
j=1
(1 −φ2
jj).
In the particular case of fractional noise, we have that
νt = σ2 Γ(t + 1)Γ(t + 1 −2d)
[Γ(t + 1 −d)]2
.
With this results, the log-likelihood function for a sample (y1, . . . , yn) can be
expressed as
L(d, σ2)
=
−1
2
n
X
t=1
log νt−1 −1
2
n
X
t=1
(yt −byt)2
νt−1
=
−1
2
n
X
t=1
log(σ2f(t, d)) −1
2
n
X
t=1
(yt −byt)2
σ2f(t, d) ,
=
−n
2 log(σ2) −1
2
n
X
t=1
log(f(t, d)) −1
2
n
X
t=1
(yt −byt)2
σ2f(t, d) ,

523
where
f(t, d) = Γ(t)Γ(t −2d)
[Γ(t −d)]2 .
Thus, we can maximize L(θ) for σ2, where the estimate of σ2 is given by
bσ2( bd ) = 1
n
n
X
t=1
(yt −byt)2
f(t, bd )
Thus, the estimated of d can be obtained conditioning the likelihood function
on bσ2(d)
L(d, bσ2(d)) = −n
2 log[bσ2(d)] −1
2
n
X
t=1
log(f(t, d)) −1
2.
The term f(t, d) is despicable in the estimation of d, because, given de Stir-
ling’s approximation
f(t, d) = Γ(t)Γ(t −2d)
[Γ(t −d)]2
≈tdt−d ≈1,
that is, Pn
t=1 log(f(t, d)) ≈0, then, maximum likelihood estimation is equiv-
alent to maximize
L1(d, bσ2(d)) = −n
2 log[bσ2(d)].
In the Haslett and Raftery method, if M = n we must maximize
L2(d) = constant −1
2 n log[bσ2
2(d)].
Note that if M = n, the numeric algorithm is of order O(n2) just like the
maximum likelihood case and (yt −byt)2 is also equivalent, so we truncate in
M = n. Then we have
bσ2
2(d) = 1
n
n
X
t=1
(yt −byt)2
υt
,
where υt = κ νt−1. In this way
υt
=
κ σ2 Γ(t)Γ(t −2d)
[Γ(t −d)]2
=
κ σ2f(t, d).
where κ ∝σ−2. Finally bσ2(d) ∝(1/n) Pn
t=1(yt −byt)2/f(t, bd ). Thus,
L2(d) ∝constant −1
2 n log[bσ2(d)].

524
SOLUTIONS TO SELECTED PROBLEMS
We conclude that the two methods L1(d) y L2(d) are equivalent when we
maximize with respect to d.
Problem 5.14.
Consider the following partition (yn+1, yn, . . . , y2, y1) =
(yn+1, yn). Then, the likelihood function can be expressed as
f(yn+1, yn|θ) = (2π)−(n+1)|Γ(θ)|−1/2 exp
(
−1
2 (yn+1 yn) Γ(θ)−1 (yn+1 yn)t
)
,
where the matrix Γ(θ) can be partitioned of the following way
Γ(θ) =
 Γ1,1(θ)
Γ1,n(θ)
Γn,1(θ)
Γn,n(θ)

.
Then, by properties of the Normal distribution, the conditional density is
given by
yn+1|yn, θ ∼N
 Γ1,n(θ)Γn,n(θ)−1yn, Γ1,1(θ) −Γ1,n(θ)Γn,n(θ)−1Γn,1(θ)

where the sub-matrices are
Γ1,1(θ)
=
γ(0)
Γ1,n(θ)
=
[γ(1) γ(2) . . . γ(n)]
Γn,1(θ)
=
[γ(1) γ(2) . . . γ(n)]t
Γn,n(θ)
=


γ(0)
γ(1)
. . .
γ(n −1)
γ(1)
γ(0)
. . .
γ(n −2)
...
...
...
...
γ(n −1)
γ(n −2)
. . .
γ(0)

.
Once these matrices are deﬁned, we can calculate de mean and variance of de
conditional distribution. Note that Γn,n(θ) can be decompose in
Γn,n(θ) = LDL′,
where
D
=
diag (ν0, . . . , νn−1)
L
=


1
−φ11
1
−φ22
−φ21
1
−φ33
−φ32
−φ31
...
...
...
1
−φn−1,n−1
−φn−1,n−2
−φn−1,n−3
. . .
−φn−1,1
1


,

525
and the coeﬃcients φij and νt are given by the equations of the durbin-levinson
algorithm in section 4.1.2, and Γ(θ)−1 = L′D−1L.
Thus, the conditional
means is
Γ1,n(θ)Γn,n(θ)−1yn = Γ1,n(θ)L′D−1Lyn = (L Γn,1(θ))′D−1Lyn
The result of L Γn,1(θ) is
L Γn,1(θ) =


γ(1)
γ(2) −φ11γ(1)
γ(3) −φ21γ(2) −φ22γ(1)
...
γ(n) −Pn−1
t=1 φn−1,tγ(n −t)


=


φ11 ν0
φ22 ν1
φ33 ν2
...
φnn νn−1


.
Then, we have that
(L Γn,1(θ))′D−1 =
 φ11
φ22
φ33
. . .
φnn

,
Thus, the product (L Γn,1(θ))′D−1L is
(L Γn,1(θ))′D−1L =
 φn1
φn2
φn3
. . .
φn,n−1
φn,n

.
Finally,
E(yn+1|yn, θ)
=
(L Γn,1(θ))′D−1Lyn
=
 φn1
φn2
. . .
φn,n



yn
yn−1
...
y1

.
=
n
X
j=1
φnjyn+1−j.
Moreover, note that
V ar(yn+1|yn, θ)
=
Γ1,1(θ) −Γ1,n(θ)Γn,n(θ)−1Γn,1(θ)
=
γ(0) −Γ1,n(θ)L′D−1LΓn,1(θ)
=
γ(0) −(LΓn,1(θ))′D−1LΓn,1(θ)
=
ν0 −
n
X
t=1
φ2νt−1
=
ν0 −
n
X
t=1
(νt−1 −νt)
=
νn.

526
SOLUTIONS TO SELECTED PROBLEMS
According to Durbin-Levinson algorithm we have that νn = νn−1[1−φ2
tt] with
ν0 = γ(0). thus, recursively
νn = γ(0)
n
Y
j=1
(1 −φ2
jj)
With this we can conclude that
V ar(yn+1|yn, θ) = γ(0)
n
Y
j=1
(1 −φ2
jj)
Problem 5.19. For the ARFIMA(1, d, 1) process
Γ(d, φ, θ) =



π2
6
−log(1+φ)
φ
log(1+θ)
θ
−log(1+φ)
φ
1
1−φ2
1
1−φθ
log(1+θ)
θ
1
1−φθ
1
1−θ2


.
Since an ARFIMA(1, d, 0) model is nested in the ARFIMA(1, d, 1) model with
θ = 0, we can use this result and simply omit the third column and row of
Γ(d, φ, θ) to get the 2 × 2 matrix
Γ(d, φ) =
 
π2
6
−log(1+φ)
φ
−log(1+φ)
φ
1
1−φ2
!
.
The asymptotic covariance of bd and bφ equals the oﬀ-diagonal element of
Γ(d, φ)−1 =
1
det Γ(d, φ)
 
1
1−φ2
log(1+φ)
φ
log(1+φ)
φ
π2
6
!
The determinant of Γ(d, φ) equals
det Γ(d, φ) = π2
6
1
1 −φ2 −

−log(1 + φ)
φ
2
(B.8)
As the determinant is a continuous function (summation and multiplica-
tion), the limit for φ →0 equals
lim
φ→0 det Γ(d, φ) = π2
6 −
log(1 + φ)
φ
2
The second term is of the form 0
0, thus by l’Hospitals rule
lim
φ→0
log(1 + φ)
φ
= lim
φ→0
1
1+φ
1
= 1

527
it holds that
lim
φ→0
1
det Γ(d, φ) =
1
π2
6 −1
(B.9)
Thus the limit of the oﬀ-diagonal element of the inverse yields
1
π2
6 −1
lim
φ→0
log(1 + φ)
φ
=
1
π2
6 −1
Since
corr(bd, bφ) :=
cov(bd, bφ)
q
var(bd)
q
var(bφ)
,
the factor
1
det Γ(d,φ) cancels out for all d and φ, and it remains
corr(bd, bφ) =
log(1+φ)
φ
q
π2
6
q
1
1−φ2
.
The limit of log(1+φ)
φ
for φ →0 is of the form 0
0, thus by l’Hospital
lim
φ→0
log(1 + φ)
φ
= lim
φ→0
1
1+φ
1
= 1.
Finally we get
lim
φ→0 corr(bd, bφ) =
limφ→0
log(1+φ)
φ
q
π2
6 limφ→0
q
1
1−φ2
= 1
π
√
6
=
√
6
π
(B.10)
Problem 5.20. We know that for the ARFIMA(1, d, 0) process it holds that
Γ(d, θ) =
 
π2
6
log(1+θ)
θ
log(1+θ)
θ
1
1−θ2
!
The asymptotic correlation of ˆd and ˆθ is given by the oﬀdiagonal element of
the inverse of Γ(d, θ). The inverse is given by
Γ−1(d, θ) =
1
det Γ(d, θ)
 
1
1−θ2
−log(1+θ)
θ
−log(1+θ)
θ
π2
6
!
The correlation is deﬁned as
corr( ˆd, ˆθ)
=
cov( ˆd, ˆθ)
q
var( ˆd)
q
var(ˆθ)
=
−
log (1+θ)
θ
q
π2
6
q
1
1−θ2

528
SOLUTIONS TO SELECTED PROBLEMS
Making θ →0 hence
corr( ˆd, ˆθ) = −
√
6
π
because the rule L’Hˆopital
lim
θ→1
log (1 + θ)
θ
= lim
θ→1
1
1+θ
1
= 1
Problem 6.4. Part (a) We have that
E(yt) = E [E (yt | yt−1)]
= E
n
E
h
(α + β y2
t−1)1/2 zt
 yt−1
io
= E
n
(α + β y2
t−1)1/2 · E
 zt
 yt−1
o
= E
h
(α + β y2
t−1)1/2 · E (zt)
i
,
Since zt has zero-mean,
E(yt) = E
h
(α + β y2
t−1)1/2 · 0
i
= E(0) = 0,
and
E(y2
t ) = E

E
 y2
t | yt−1

= E

E

(α + β y2
t−1) z2
t
 yt−1
	
= E

(α + β y2
t−1) · E
 z2
t
 yt−1
	
= E

(α + β y2
t−1) · E
 z2
t

,
Since zt has unit variance,
E(y2
t ) = E

(α + β y2
t−1) · 1

= α + β E(y2
t−1)
= α + β E(y2
t ).
Now, by stationarity, E(y2
t ) =
α
1−β . Finally, for k > 0
E(yt · yt+k) = E [E (yt · yt+k | yt+k−1)]
= E
n
E
h α + β y2
t−1
1/2 zt ·
 α + β y2
t+k−1
1/2 zt+k | yt+k−1
io
= E
h α + β y2
t−1
1/2 zt ·
 α + β y2
t+k−1
1/2 · E (zt+k | yt+k−1)
i
= E
h α + β y2
t−1
1/2 zt ·
 α + β y2
t+k−1
1/2 · E (zt+k)
i
,

529
Since zt has zero-mean,
E(yt · yt+k) = E
h α + β y2
t−1
1/2 zt ·
 α + β y2
t+k−1
1/2 · 0
i
= E(0) = 0.
Now, for k < 0,
E(yt · yt+k) = E [E (yt · yt+k | yt+k−1)]
= E
n
E
h α + β y2
t−1
1/2 zt ·
 α + β y2
t+k−1
1/2 zt+k | yt−1
io
= E
h α + β y2
t−1
1/2 ·
 α + β y2
t+k−1
1/2 zt+k−1 · E (zt | yt−1)
i
= E
h α + β y2
t−1
1/2 ·
 α + β y2
t+k−1
1/2 zt+k−1 · E (zt)
i
,
Since zt has zero-mean,
E(yt · yt+k) = E
h α + β y2
t−1
1/2 ·
 α + β y2
t+k−1
1/2 zt+k · 0
i
= E(0) = 0.
Therefore,
γy(k) = Cov(yt, yt+k) = E(yt yt+k) −E(yt) E(yt+k)
=





α
1 −β ,
k = 0
0,
k ̸= 0
Pert (b). From part (a) we have that {yt} ∼WN

0,
α
1−β

and given that
{xt} has a MA(1) structure, the function of autocovariance is given by
γx(k) =















α
1 −β
 1 + θ2
,
k = 0
α
1 −β θ,
|k| = 1
0,
|k| > 1.
Problem 6.11.
Since backshift operator B is such that BYt = yt−1, if
Y ≡α0 with α0 ∈then the backshift operator becomes Bα0 = α0 ⇒B ≡1
so if π(B) = (1 −B)d ⇒π(B)α0 = π(1)α0 = (1 −1)dα0 = 0 if d > 0.
Problem 6.12. An ARCH(∞) model is deﬁned as
yt
=
σtεt,
εt
iid
∼
WN(0, σ2
ε),
σ2
t
=
α0 +
∞
X
j=1
αjy2
t−j,

530
SOLUTIONS TO SELECTED PROBLEMS
where α0 > 0 and αj ≥0 for j ≥1.
If the coeﬃcients αj are given by
an ARFIMA(p, d, q) representation, then the ARCH(∞) model is called a
FIGARCH(p, d, q) model.
Let π(B) = φ(B)(1 −B)dθ(B)−1, then
π(B)y2
t = α0 + νt
with νt = y2
t −σ2
t .
Multiplying both sides by θ(B) gives
φ(B)(1 −B)dy2
t = ω + θ(B)νt,
where ω = θ(B)α0. Since α0 is a constant, θ(B)α0 = θ(1)·α0. To ensure that
ω is positive, the MA polynomial must be positive at z = 1 (or λ = 0), that
is, θ(1) > 0.
Problem 6.13.
(a) An ARFIMA(0, d, 0)-GARCH process is deﬁned by
εt(d) = yt(1 −B)d
for | d |< 1/2.Hence
εt(d)
=
yt(1 −B)d
(1 −B)−dεt(d)
=
yt = c
λ(d)−1εt(d)
=
c
where c is a constant with respect to d
(b) Part of a derivative with respect to d is
∂
∂dλ−1(d)εt(d) + λ−1(d) ∂
∂dεt(d) = 0
=
∂
∂d(1 −B)−dεt(d) + (1 −B)−d ∂
∂dεt(d) = 0
=
∞
X
j=0
[ ∂
∂dψj(d)]εt−j(d) +
∞
X
j=0
ψj(d)[ ∂
∂dεt−j(d)] = 0

531
(c) In considering the ﬁrst part of the equation (b)
λ(d)−1 ∂
∂dεt(d)
=
−∂
∂d[λ−1(d)εt(d)]
∂
∂dεt(d)
=
−λ(d) ∂
∂d[λ−1(d)εt(d)]
∂
∂dεt(d)
=
−λ(d)
∞
X
j=0
ψj(d)[ ∂
∂dεt−j(d)]
∂
∂dεt(d)
=
−λ(d)[ ∂
∂dλ−1(d)]εt(d)
∂
∂dεt(d)
=
−[ ∂
∂d log λ−1(d)]εt(d)
(d) Using the result
log(1 + x) =
∞
X
j=1
(−1)j+1
j
xj
and for the part (c). Hence
∂
∂dεt(d)
=
−[ ∂
∂d log λ−1(d)]εt(d)
=
−[ ∂
∂d log (1 −B)−d]εt(d)
=
−[ ∂
∂d −d log (1 −B)]εt(d)
=
−[−log (1 −B)]εt(d)
=
−[−
∞
X
j=1
(−1)j+1(−B)j
j
]εt(d)
=
−[−
∞
X
j=1
(−1)2j+1Bj
j
]εt(d)
=
−
∞
X
j=1
1
j Bjεt(d)
=
−
∞
X
j=1
1
j εt−j(d)
Problem 6.14. a) For a sequence {ϵt}of independent and identically dis-
tributed uniform random variables U(−
√
3,
√
3), the mean of the process is
given by E[ 1
2(−
√
3+
√
3)] = 0 and the variance is E[ 1
12(
√
3+
√
3)2] = 12
12 = 1.
b) Using the formula provided in Gradshteyn and Rizhik (2000, p.236),
and denoting x = 1 and a =
q
β1
3α1 the result for the Lyapunov exponent is
direct.

532
SOLUTIONS TO SELECTED PROBLEMS
Problem 7.2. Note that
φtj
=
−
 t
j
 Γ(j −d)Γ(t −d −j + 1)
Γ(−d)Γ(t −d + 1)
=
−
Γ(t + 1)
Γ(j + 1)Γ(t −j + 1)
Γ(j −d)Γ(t −d −j + 1)
Γ(−d)Γ(t −d + 1)
=
−
Γ(j −d)
Γ(j + 1)Γ(−d)
Γ(t + 1)Γ(t −d −j + 1)
Γ(t −j + 1)Γ(t −d + 1),
and, when t →∞, we have that as t →∞,
−
Γ(j −d)
Γ(j + 1)Γ(−d)
Γ(t + 1)Γ(t −d −j + 1)
Γ(t −j + 1)Γ(t −d + 1)
≈−
Γ(j −d)
Γ(j + 1)Γ(−d) t1+j−1t−d−j+1+d−1
= −
Γ(j −d)
Γ(j + 1)Γ(−d) tjt−j
= −
Γ(j −d)
Γ(j + 1)Γ(−d)
= πj
Problem 7.3. Part (a). Applying expectation to (6.30) we have that
E(yt) = α + φ E(yt−1) + E(zt) + (θ + η) E(zt−1) + θ η E(zt−2)
µ = α + φ µ + 0 + (θ + η) 0 + θ η 0
µ = α + φ µ.
Consequently, by stationarity, µ =
α
1−φ.
Part (b). From Part (a) we have that
α = µ −φ µ
replacing in (6.30) we obtain,
yt = µ −φ µ + φ yt−1 + zt + (θ + η) zt−1 + θ η zt−2.
Hence,
[yt −µ] = φ [yt−1 −µ] + zt + (θ + η) zt−1 + θ η zt−2
= φ [B yt −B µ] + zt + (θ + η) zt−1 + θ η zt−2
= φ B [yt −µ] + zt + (θ + η) zt−1 + θ η zt−2

533
Thus,
[yt −µ] −φ B [yt −µ] = zt + (θ + η) zt−1 + θ η zt−2,
and,
(1 −φ B) [yt −µ] = zt + (θ + η) B zt + θ η B2 zt
(1 −φ B) [yt −µ] =

1 + (θ + η) B + θ η B2
zt.
Pert (c). Since |φ| < 1, we have that
[yt −µ] = (1 −φ B)−1 
1 + (θ + η) B + θ η B2
zt
=
 ∞
X
k=0
φk Bk
!

1 + (θ + η) B + θ η B2
zt
=
 ∞
X
k=0
φk Bk
!
+
 ∞
X
k=1
(θ + η) φk−1 Bk
!
+
 ∞
X
k=2
θ η φk−2 Bk
!
= B0 + [φ + (θ + η)] B1 +
 ∞
X
k=2

φk + φk−1 (θ + η) + θ η φk−2
Bk
!
= B0 + [φ + (θ + η)] B1 +
 ∞
X
k=2
φk−2 
φ2 + φ (θ + η) + θ η

Bk
!
,
that is,
ψ0 = 1,
ψ1 = φ + (θ + η),
ψk = φk−2 
φ2 + φ (θ + η) + θ η

k ≥2,
as requested.
Part (e). The autocovariance function of {yt −µ} is given by
γ(k) = σ2
∞
X
j=0
ψj ψj+|k|
From this expression we can write
γ(0) = σ2

1 + [φ + (θ + η)]2 + [φ2 + (θ + η) φ + θ η]2
φ4
·

φ4
1 −φ2
 
=
σ2
1 −φ2

(1 −φ2) + (1 −φ2) [φ + (θ + η)]2 + [φ2 + (θ + η) φ + θ η]2

=
σ2
1 −φ2

(1 −φ2)

1 + [φ + (θ + η)]2	
+ [φ2 + (θ + η) φ + θ η]2

.

534
SOLUTIONS TO SELECTED PROBLEMS
γ(1) = σ2 
[φ + (θ + η)] + [φ + (θ + η)] [φ2 + (θ + η) φ + θ η]
+ [φ2 + (θ + η) φ + θ η]2
φ3
·

φ4
1 −φ2

=
σ2
1 −φ2

(1 −φ2) [φ + (θ + η)]

1 + [φ2 + (θ + η) φ + θ η]
	
+ [φ2 + (θ + η) φ + θ η]2 φ

γ(k) = σ2 n
φ|k|−2 [φ2 + (θ + η) φ + θ η] + [φ + (θ + η)] φ|k|−1 [φ2 + (θ + η) φ + θ η]+
[φ2 + (θ + η) φ + θ η]2
φ4
·

φ4
1 −φ2

φ|k|

= σ2 φ|k|−2
1 −φ2

(1 −φ2) [φ2 + (θ + η) φ + θ η]
+ (1 −φ2) [φ + (θ + η)] φ [φ2 + (θ + η) φ + θ η]
+ [φ2 + (θ + η) φ + θ η]2 φ2

= σ2 φ|k|−2
1 −φ2

(1 −φ2) [φ2 + (θ + η) φ + θ η] {1 + [φ + (θ + η)] φ}
+ [φ2 + (θ + η) φ + θ η]2 φ2

Part (f). Under the assumption of inﬁnite information, the variance of the
h-step prediction error is
σ2
t (h) = σ2
h−1
X
j=0
ψ2
j .
From this formula we can write,
σ2
t (1) = 1
σ2
t (2) = 1 + [φ + (θ + η)]2
σ2
t (h) = 1 + [φ + (θ + η)]2 +

φ2 + (θ + η) φ + θ η
2
φ4
φ4 −φ2 h
1 −φ2

,
h ≥2.

535
Problem 7.5. If αt = (1 −φtt)αt−1 and α0 = 1, then
αt
=
(1 −φtt)(1 −φt−1,t−1)αt−2
=
(1 −φtt)(1 −φt−1,t−1)....(1 −φt−(t−1),t−(t−1))αt−t
=
(1 −φtt)(1 −φt−1,t−1)....(1 −φ11)α0
=
tY
j=1
(1 −φjj).
Then, by using αt = 1 −Pt
j=1 φtj ⇒Pt
j=1 φtj = 1 −αt = 1 −Qt
j=1(1 −φjj).
Problem 7.6. We have that
γx(k) =



(1 + θ2) σ2,
k = 0
θ σ2,
|k| = s
0,
otherwise.
As an extension of the process MA(1) we may write,
ˆXn+1 =
 0,
n = 0, . . . , s −1
θn, s (xn+1−s −ˆXn+1−s),
n = s, s + 1, s + 2, . . .
where
θn, s = θ σ2
υn−s
and
υn =
 (1 + θ2) σ2,
n = 0, 1, . . . , s −1
(1 + θ2) σ2 −θn, s υn−s,
n = s, s + 1, s + 2, . . .
Problem 7.12. A stochastic process yt is ARFIMA-GARCH(r, s) if it is a
solution to
Φ(B)yt
=
Θ(B)(1 −B)−dεt,
εt
=
ηtσt,
σ2
t
=
α0 +
r
X
j=1
αjε2
t−j +
s
X
k=1
βkσ2
t−k,
where Ft−1 is the set generated by the past observations yt−1, yt−2, . . . , σ2
t =
E

y2
t |Ft−1

is the conditional variance of the process yt, the GARCH coef-
ﬁcients αj, βk are positive, and Pr
j=1 αj + Ps
k=1 βk < 1, and ηt are i.i.d.
zero-mean and unit variance random variables.
Since for all t ∈Z we have
E[ε2
t+h|Ft] =
(
E[σ2
t+h|Ft]
h ≥1,
ε2
t+h
h < 1.

536
SOLUTIONS TO SELECTED PROBLEMS
Furthermore it holds
E[e2
t(h)|Ft] =
h−1
X
j=0
ψ2
j σ2
t (h −j),
(B.11)
where e2
t(h) is the multistep ahead prediction error
et(h) := yt −by(h) = εt+h + ψ1εt+h−1 + . . . + ψh−1εt+1.
For the ARFIMA-GARCH(r, s) model we have
σ2
t+h = α0 +
r
X
j=1
αjε2
t+h−j +
s
X
k=1
βkσ2
t+h−k.
Taking E(·|Ft) on both sides,
σ2
t (h)
=
α0 +
r
X
j=1
αj E(ε2
t+h−j|Ft) +
s
X
k=1
βk E(σ2
t+h−k|Ft)
=
α0 +
h−1
X
j=1
αjσ2
t (h −j) +
r
X
j=h
αjε2
t+h−j +
h−1
X
k=1
βkσ2
t (h −k) +
s
X
k=h
βkσ2
t+h−k.
Problem 7.13. For the ARFIMA-GARCH(1, 1) we have
σ2
t+h = α0 + α1ε2
t+h−1 + β1σ2
t+h−1
Therefore for h = 1
σ2
t+1
=
α0 + α1ε2
t + β1σ2
t
=
α0 + α1ε2
t + β1L(σ2
t+1)
σ2
t+1 (1 −Lβ1)
=
α0 + α1ε2
t
σ2
t+1
=
α0
1 −Lβ1
+ α1
∞
X
j=0
βj
1ε2
t−j
=
α0
1 −β1
+ α1
∞
X
j=0
βj
1ε2
t−j
Finally we have
σ2
t+1 =
α0
1 −β1
+ α1
∞
X
j=0
βj
1ε2
t−j.
Problem 7.22. Since H is an inner product space then we have: ∥x ± y∥2 =
⟨x ± y, x ± y⟩where ⟨·, ·⟩denotes an inner product function. Thus, ∥x + y∥2+

537
∥x −y∥2 = P
i
x2
i +2P
i
xiyi +P
i
y2
i +P
i
x2
i −2P
i
xiyi +P
i
y2
i = 2 P
i
x2
i +2P
i
y2
i =
2 ⟨x, x⟩+ 2 ⟨y, y⟩= 2 ∥x∥2 + 2 ∥y∥2 for i = 1, ..., n.
Problem 9.7. Consider Bs ≡eB
(1 −Bs)−d
=
(1 −eB)−d =
∞
X
j=0
ψj eBj
=
∞
X
j=0
ψjBsj =
∞
X
k=0
ψk/sBk.
But, by writing k = sj and j ∈: j = k
s we have,
(1 −Bs)−d
=
∞
X
k=0
ψk/sBk.
Problem 9.8.
a) The fractional ﬁlter is given by
(1 −B)d =
∞
X
j=0
ηjBj,
where
ηj =
Γ(j + d)
Γ(j + 1)Γ(d) =
d
j

.
This expansion holds for d < 1/2, d ̸= 0, −1, −2, . . . .
The function η(z) = (1−z)−d is analytic in the open unit disk {z : |z| <
1} and analytic in the closed unit disk {z : |z| ≤1} for negative d. Thus
the Taylor expansion is given by
η(z) =
∞
X
j=0
ηjzj.
(B.12)
In the seasonal case we have Bs instead of B for some positive s ∈N.
Deﬁne ˜Bs = Bs and ˜zs = zs. As all convergence conditions are based
on the open and closed unit disk, and the function f(z) = zs preserves
the property of being in this set or not, we can reduce the case of Bs to
˜Bs.

538
SOLUTIONS TO SELECTED PROBLEMS
Consequently we have
(1 −zs)d = (1 −˜zs)d =
∞
X
j=0
ηj ˜zj
s =
∞
X
j=0
ηjzs·j.
(B.13)
And this representation converges absolutely, for the same reasons as
the function (1 −z)d converges to η(z).
b) By assumption z is an element of L2, but constant over time. Hence,
Bz = z.
(B.14)
Thus applying any well-deﬁned ﬁlter a(B) to a random variable z, is
equivalent to multiplying z by a(1). If the ﬁlter has roots for B = 1, it
holds
a(1)z = 0.
(B.15)
The ﬁlter (1 −Bs)d equals zero for B = 1 and d > 0.
c) Following the same arguments as in a) a stationary L2-convergent solu-
tion is given by
yt =
∞
X
j=0
ηjBs·jεt =
∞
X
j=0
ηjεt−s·j,
(B.16)
where
ηj =
Γ(j + d)
Γ(j + 1)Γ(d),
−1 < d < 0.
(B.17)
d) By assumption yt is a solution to
(1 −Bs)dyt = εt.
(B.18)
In part b) we showed that for any random variable z it holds (1−Bs)dz =
0. Using the linearity of the backshift operator we get
(1−Bs)dxt = (1−Bs)d(yt +z) = (1−Bs)dyt +(1−Bs)dz = εt +0 = εt.
(B.19)
Thus xt is also a solution to the stochastic diﬀerence equation.
Problem 9.9. A SARFIMA process {yt} has spectral density function given
by
fs(λ) = 1
2π
1 −eisλ−2ds = 1
2π

2 sin (sλ
2 )
−2ds

539
Since sin sλ
2 →sλ
2 , when λ →0+, one has
fs(λ) ≈1
2π (sλ)−2ds
for 0 < ds < 1
2 Therefore,
γs(k)
=
Z π
−π
e−iλkfs(λ)dλ
=
Z π
−π
e−iλk 1
2π

2 sin (sλ
2 )
−2ds
dλ
≈
Z π
−π
e−iλk 1
2π (sλ)−2dsdλ
≈
Z π
−π
e−iλkf(sλ)dλ
≈
γ(sk)
as
λ →0+
where f(λ) is the spectral density function the a FN(d) process with unit
noise variance. In general we have, for any s ∈N
γs(k)
=
Z π
−π
e−iλk 1
2π

2 sin (sλ
2 )
−2ds
dλ
=
Z π
−π
e−iλsk 1
2π

2 sin (λ
2 )
−2ds
dλ
=
γ(sk)
where λ ∈(−π, π] and 0 < ds < 1
2.
Problem 9.14.
Consider a SARFIMA(0, d, 0) × (0, ds, 0)s process.
This
process is deﬁned by
yt −µ = (1 −Bs)−ds(1 −B)−dεt,
(B.20)
where µ is the mean of the series, and {εt} is a white noise sequence with
ﬁnite variance.
The SARFIMA(0, d, 0) × (0, ds, 0)s process has a MA(∞) representation
yt −µ =
∞
X
j=0
ψjεt−j
(B.21)
where the coeﬃcients ψj are given by the inverse Fourier transform
ψj = 1
2π
Z π
−π
(1 −eiλs)−ds(1 −eiλ)−ddλ,
(B.22)

540
SOLUTIONS TO SELECTED PROBLEMS
or equivalently by the convolution of the two ﬁlters
ψj =
j
X
k=0
akbk−j where ak ↔(1 −B)d, bn ↔(1 −Bs)−ds.
(B.23)
Chapter 4 describes a method to convert a Wold representation to state
space form. Following this method a State space form is given by
xt
=
[y(t|t −1), y(t + 1|t −1), y(t + 2|t −1), . . .]′ ,
(B.24)
F
=





0
1
0
0
. . .
0
0
1
0
. . .
0
0
0
1
. . .
...
...
...
...




,
(B.25)
H
=
[ψ1, ψ2, ψ2, . . .]′
(B.26)
G
=
[1, 0, 0, . . .]′
(B.27)
yt
=
Gxt + εt.
(B.28)
where
y(t + j|t −1) := E(yt+j|yt−1, yt−2, . . .).
(B.29)
Problem 10.6. We have that
Var(yt) = Cov(yt, yt) = Cov(a + bt + wt, a + bt + wt)
= Cov(wt, wt) = γw(0)
= σ2.
Moreover,
zt =
1
2 k + 1
k
X
j=−k
yt−j
=
1
2 k + 1
k
X
j=−k
[a + b (t −j) + wt−j]
=
1
2 k + 1
k
X
j=−k
a +
1
2 k + 1
k
X
j=−k
b t −
1
2 k + 1
k
X
j=−k
j +
1
2 k + 1
k
X
j=−k
wt−j
= a + bt +
1
2 k + 1
k
X
j=−k
wt−j

541
Then,
Var(zt) = Cov(zt, zt)
= Cov

a + bt +
1
2 k + 1
k
X
j=−k
wt−j, a + bt +
1
2 k + 1
k
X
i=−k
wt−i


= Cov


1
2 k + 1
k
X
j=−k
wt−j,
1
2 k + 1
k
X
i=−k
wt−i


=

1
2 k + 1
2
k
X
j=−k
k
X
i=−k
Cov (wt−j, wt−i)
=

1
2 k + 1
2
k
X
j=−k
k
X
i=−k
γw(j −i)
=

1
2 k + 1
2

(2 k + 1) γw(0) +
k−1
X
j=−k
k
X
i=j+1
γw(i −j) +
k−1
X
i=−k
k
X
j=i+1
γw(j −i)


=

1
2 k + 1
2 
(2 k + 1) σ2
=
σ2
2 k + 1.
Therefore,
Var(zt)
Var(yt) =
1
2 k + 1.
Problem 10.7. The design matrix has elements of the following form: xtj =
tj−1. Thus, the product
x′
nxn =


Pn
t=1 x2
t1
Pn
t=1 xt1xt2
. . .
Pn
t=1 xt1xtp
Pn
t=1 xt1xt2
Pn
t=1 x2
t2
. . .
Pn
t=1 xt2xtp
...
...
...
...
Pn
t=1 xt1xtp
Pn
t=1 xt2xt2
. . .
Pn
t=1 x2
tp

.
Multiplying on both sides by D−1
n
we get
D−1
n x′
nxnD−1
n
= W,
where the elements of the matrix W are given by
wij =
Pn
t=1 xtixtj
∥xi∥n∥xj∥n
.

542
SOLUTIONS TO SELECTED PROBLEMS
By replacing xtj = tj−1, the elements of the matrix W are
wij =
Pn
t=1 ti−1tj−1
[Pn
t=1 t2i−2]1/2 [Pn
t=1 t2j−2]1/2 =
Pn
t=1 ti+j−2
[Pn
t=1 t2i−2]1/2 [Pn
t=1 t2j−2]1/2
For large values of n we have,
n
X
t=1
ti+j−2 =
ni+j−1
i + j −1.
Now,
lim
n→∞
Pn
t=1 ti+j−2
[Pn
t=1 t2i−2]1/2 [Pn
t=1 t2j−2]1/2
= lim
n→∞
ni+j−1
i + j −1
"
n2i−1
2i −1
#1/2 "
n2j−1
2j −1
#1/2 =
√2i −1√2j −1
i + j −1
= mij.
Part b). We have that,
lim
n→∞D−1
n (x′
nxn)Var( bβn)(x′
nxn)D−1
n
→2πB
where
bij = f0(0) lim
n→∞
n−2d
∥xi∥n∥xj∥n
n
X
t=1
n
X
s=1
ti−1sj−1γ(t −s).
Note that,
D−1
n (x′
nxn)Var( bβn)(x′
nxn)D−1
n
= D−1
n (x′
nxn)(x′
nxn)−1x′
nΓxn(x′
nxn)−1(x′
nxn)D−1
n
= D−1
n x′
nΓxnD−1
n .
Now,
lim
n→∞n−2dD−1
n (x′
nxn)Var( bβn)(x′
nxn)D−1
n
= lim
n→∞n−2dD−1
n x′
nΓxnD−1
n
→2πδ,
where
δij = f0(0) lim
n→∞n−2d
n−2d
∥xi∥n∥xj∥n
n
X
t=1
n
X
s=1
ti−1sj−1γ(t −s).
For a FN(d) with σ2 = 1 we can see by Stirling’s approximation,
γ(h) =
Γ(1 −2d)
Γ(1 −d)Γ(d)
Γ(h + d)
Γ(1 + h −d)
h→∞
→
Γ(1 −2d)
Γ(1 −d)Γ(d)h2d−1.

543
Moreover, we can write
δij
=
f0(0) lim
n→∞n−2d
n−2d
∥xi∥n∥xj∥n
n
X
t=1
n
X
s=1
ti−1sj−1γ(t −s)
=
f0(0) lim
n→∞n−2d
n−2d
∥xi∥n∥xj∥n
n
X
t=1
n
X
s=1
ti−1sj−1
Γ(1 −2d)
Γ(1 −d)Γ(d)|t −s|2d−1
=
f0(0)
Γ(1 −2d)
Γ(1 −d)Γ(d) lim
n→∞n−2d
n−2d
∥xi∥n∥xj∥n
n
X
t=1
n
X
s=1
ti−1sj−1|t −s|2d−1
=
f0(0)
Γ(1 −2d)
Γ(1 −d)Γ(d) ×
lim
n→∞ni+j−1
n−2d
∥xi∥n∥xj∥n
n
X
t=1
n
X
s=1
 
t
n
!i−1  
s
n
!j−1 
t
n −s
n

2d−1 1
n2.
Note that
∥xi∥n∥xj∥n =
" n
X
t=1
t2i−2
#1/2 " n
X
t=1
t2j−2
#1/2
≃
ni+j−1
√2i −1√2j −1.
Thus,
δij
=
f0(0)
Γ(1 −2d)
Γ(1 −d)Γ(d)
√
2i −1
p
2j −1
lim
n→∞n−2d
n
X
t=1
n
X
s=1
 
t
n
!i−1  
s
n
!j−1 
t
n −s
n

2d−1 1
n2 ×
=
f0(0)
Γ(1 −2d)
Γ(1 −d)Γ(d)
√
2i −1
p
2j −1
Z 1
−1
Z 1
−1
xi−1yj−1|x −y|2d−1dxdy.
Note that
δij = f0(0)hij
Consequently,
lim
n→∞n−2dD−1
n x′
nΓxnD−1
n
= 2πf0(0)H = 2πδ.
Problem 10.9. This time series linear model has one regressor
xn =





1peiλ01
2peiλ02
...
npeiλ0n




.

544
SOLUTIONS TO SELECTED PROBLEMS
Consequently (for h ≥0)
xn,h =












hpeiλ0h
(h + 1)peiλ0(h+1)
...
npeiλ0n
0
...
0












,
with h zeros at the end. For p = 0 and/or λ0 = 0 see the solutions in the text
for the harmonic regression and polynomial trend, respectively. Thus, from
now on, we assume that p > 0 and λ0 ̸= 0.
Part a). Since the Grenander conditions are satisﬁed (see Part b), to prove
the consistency of bβn we have to check if
lim inf
n→∞
||xn||2
nδ
> 0,
for some δ > 2d. Following b) we have
||xn||2
nδ
=
Pn
t=1 t2p
nδ
∼
1
2p + 1
n2p+1
nδ
=
1
2p + 1n2p+1−δ.
Choose δ = 1 > 2d for all d ∈(0, 1/2), then as p ≥0 the last expression is
always greater than zero for all n.
Part b). The norm of xn equals (complex inner product)
||xn||2
=
n
X
t=1
xn,txn,t
=
n
X
t=1
 tpeiλ0n
(tpeiλ0n)
=
n
X
t=1
t2p > n2p,
and clearly diverges for n →∞.

545
The ratio ||xn+1||2
||xn||2
satisﬁes
lim
n→∞
Pn+1
t=1 t2p
Pn
t=1 t2p
=
lim
n→∞
Pn
t=1 t2p + (n + 1)2p
Pn
t=1 t2p
=
1 + lim
n→∞
(n + 1)2p
Pn
t=1 t2p .
Since Pn
t=1 t2p ∼n2p+1, the second term converges to 0, hence the ratio to 1.
Given that there is only one regressor, the matrix R(h) is in fact univariate,
that is, a function of h.
R(h)
=
lim
n→∞
< xn,h, xn >
||xn,h||||xn||
=
lim
n→∞
Pn
t=1
 (t + h)peiλ0(t+h)
(tpeiλ0t)
Pn−h
t=1 (t + h)peiλ0(t+h)(t + h)pe−iλ0(t+h)
1/2
(Pn
t=1 t2p)1/2
=
lim
n→∞
eiλ0h Pn−h
t=1 (t + h)ptp
Pn−h
t=1 (t + h)2p
1/2
(Pn
t=1 t2p)1/2
=
eiλ0h lim
n→∞
Pn−h
t=1 [(t + h)t]p
Pn−h
t=1 (t + h)2p
1/2
(Pn
t=1 t2p)1/2 .
For every ﬁxed h > 0 the right term converges to 1, thus
R(h) = eiλ0h,
which is nonsingular at h = 0, as R(0) = 1, for all λ0 ∈R.
Thus, the
Grenander conditions are satisﬁed.
Part c). Since we have a univariate case, the p×p matrix is a real number.
Dn =
 n
X
t=1
t2p
!1/2
nd ∼np+1/2+d
(B.30)
Part d). We have to calculate M(λ) and show that it does not have a jump
at the origin. The spectral representation of R(h) is given by
R(h) =
Z π
−π
eiλhdM(λ).
(B.31)
As R(h) = eiλ0h it holds
eiλ0h =
Z π
−π
eiλhdM(λ) ⇒M(λ) = δ(λ −λ0)dλ,
(B.32)

546
SOLUTIONS TO SELECTED PROBLEMS
where δ(·) is the Dirac delta function.
Thus M(λ) exhibits a jump at λ0. Thus it has a jump at the origin only for
λ0 = 0. By assumption we excluded this case from the analysis. Conditions
(1)-(4) are satisﬁed, see b). Condition (5) requires that for some δ > 1 −2d
max
1≤t≤n
xn,t
||xn||2 = o(n−δ).
(B.33)
The maximum of xn is achieved for t = n, and ||xn||2 ∼n2p+1, thus
max
1≤t≤n
xt
||xn||2 ∼
np
n2p+1 = 1
n.
(B.34)
Dividing by n−δ gives
1
nnδ = nδ−1.
(B.35)
This expression tends to 0 only for δ < 1. Choose δ = 1 −d. Thus, condi-
tion (5) (δ > 1 −2d) and δ < 1 is satisﬁed.
Assumption (10.11) states that for any δ > 0 there exists a constant c such
that
Z c
−c
f(λ)dM n(λ) < δ.
(B.36)
for every n.
The spectral density equals
f(λ) = |1 −eiλ|−2df0(λ), with 0 < d < 1/2,
and f0(λ) is a well-behaved function (see equation (10.2)).
Under the assumption λ0 ̸= 0, and given M(λ) = δ(λ −λ0)dλ we can
choose c = λ0/2 and get
Z λ0/2
−λ0/2
f(λ)δ(λ −λ0)dnλ = 0.
Hence, the LSE is asymptotically eﬃcient.
Problem 10.10.
(a) By deﬁnition we have
γij(i −j)
=
Z π
−π
h(λ)ei(i−j)λdλ
=
Z π
−π
h(λ)e−i(j−i)λdλ
=
γji(−(i −j))

547
Therefore γij = γji
(b) Let x a vector complex, then
x∗Γx
=
n
X
i,j=1
γ(i −j)xix′
j
=
n
X
i,j=1
Z π
−π
h(λ)ei(i−j)λdλxix′
j
=
Z π
−π

n
X
j=1
xjeiλj

2
h(λ)dλ ≥0
were h(λ) ≥0 for hypothesis
(d) For deﬁnition mn
ij = mn
ij(λ) = <Pn
t=1 xtie−itλ,Pn
t=1 xtjeitλ>
2π∥xn(i)∥∥xn(j)∥
Hence
Z π
−π
2πh(λ)mn
ij(λ)dλ
=
Z π
−π
2πh(λ)< Pn
t=1 xtie−itλ, Pn
t=1 xtjeitλ >
2π ∥xn(i) ∥∥xn(j) ∥
dλ
=
Z π
−π
Pn
t=1
Pn
s=1 < xti, xsj >
∥xn(i)∥∥xn(j)∥
h(λ)ei(i−j)λdλ
=
Pn
t=1
Pn
s=1 < xti, xsj >
∥xn(i)∥∥xn(j)∥
Z π
−π
h(λ)ei(i−j)λdλ
=
n
X
t=1
n
X
s=1
< xti, xsj >
∥xn(i)∥∥xn(j)∥γij
=
n
X
t=1
n
X
s=1
xti
∥xn(i)∥
xsj
∥xn(j)∥γij
=
u∗Γv
where u =
xn(i)
∥xn(i)∥and v =
xn(j)
∥xn(j)∥
(e) The part (d) we have
Z π
−π
2πh(λ)mn
ij(λ)dλ = u∗Γv
Then

Z π
−π
2πh(λ)mn
ij(λ)dλ

=
|u∗Γv| ≤2 |u∗Γv|
≤
u∗Γu + v∗Γv
=
2π
Z π
−π
h(λ)mn
ii(λ)dλ +
Z π
−π
h(λ)mn
jj(λ)dλ


548
SOLUTIONS TO SELECTED PROBLEMS
The inequality is using the part (c). Therefore

Z π
−π
h(λ)mn
ij(λ)
 ≤
Z π
−π
h(λ)mn
ii(λ)dλ +
Z π
−π
h(λ)mn
jj(λ)dλ
(f) We have that M n(λ) converge weakly to M(λ), that is,
Z π
−π
g(λ)dM n(λ) →
Z π
−π
g(λ)dM(λ)
Hence if deﬁned h(λ) = f(λ) ≥0 then by the part (e). We have

Z
A
f(λ)dMij(λ)
 ≤
Z
A
f(λ)dMii(λ) +
Z
A
f(λ)dMjj(λ)
where A ⊂[−π, π]
Problem 10.16. For λ = 0 we have that fn(0) = 1 for all n. On the other
hand, for λ ̸= 0 we have

1
n
n
X
t=1
e˙ıλt
 = 1
n

e˙ıλn −1
e˙ıλ −1
 ≤2
n

1
e˙ıλ −1
 →0,
as n →∞.
Problem 11.1. Part a). The joint probability density of y1, y2, y3, y4, y5 with
|φ| < 1 is given by
f(y1, y2, y3, y4, y5) = f(y5|y4)f(y4|y3)f(y3|y2)f(y2|y1)f(y1),
where
yt|yt−1
∼
N(φyt−1, 1),
t = 2, 3, 4, 5
y1
∼
N(0, (1 −φ2)−1).
Thus
f(y1, y2, y3, y4, y5)
=
(2π)−5/2(1 −φ2)1/2 ×
exp
(
−1
2
5
X
t=2
(yt −φyt−1)2 −1
2 y2
1(1 −φ2)
)
.
Part b). Note that maximize f(y1, . . . , y5) respect to y4 is equivalent to
maximize
Q(y1, y2, y3, y4, y5) = −1
2
5
X
t=2
(yt −φyt−1)2 −1
2 y2
1(1 −φ2).

549
Thus, the value of y4 that maximize Q is given by the equation
∂Q
∂y4
= −y4 + φy3 + φy5 −φ2y4 = 0.
Finally , the value that maximizes the density is
z = ˜y4 =
φ
1 + φ2(y3 + y5).
Part c). We must calculate the conditional distribution of y4|y1, y2, y3, y5,
f(y4|y1, y2, y3, y5) = f(y1, y2, y3, y4, y5)
f(y1, y2, y3, y5)
=
f(y1, y2, y3, y4, y5)
R ∞
−∞f(y1, y2, y3, y4, y5)dy4
.
Thus, we need to calculate the density
f(y1, y2, y3, y5) =
=
Z ∞
∞
(2π)−5/2(1 −φ2)1/2 exp
(
−1
2
5
X
t=2
(yt −φyt−1)2 −1
2 y2
1(1 −φ2)
)
dy4
=
(2π)−5/2(1 −φ2)1/2 exp
(
−1
2
3
X
t=2
(yt −φyt−1)2 −1
2 y2
1(1 −φ2)
)
×
Z ∞
∞
exp
(
−1
2(y5 −φy4)2 −1
2(y4 −φy3)2
)
dy4
=
(2π)−5/2(1 −φ2)1/2 exp
(
−1
2
3
X
t=2
(yt −φyt−1)2 −1
2 y2
1(1 −φ2)
)
×
Z ∞
∞
exp
(
−y2
5
2 + 2φy5y4
2
−φ2y2
4
2
−y2
4
2 + 2φy4y3
2
−φ2y2
3
2
)
dy4
= (2π)−5/2(1 −φ2)1/2
exp
(
−1
2
3
X
t=2
(yt −φyt−1)2 −1
2 y2
1(1 −φ2) −y2
5
2 −φ2y2
3
2
)
×
Z ∞
∞
exp
(
2φy5y4
2
−φ2y2
4
2
−y2
4
2 + 2φy4y3
2
)
dy4
= (2π)−4/2 (1 −φ2)1/2
(1 + φ2)1/2×
exp
(
−1
2
3
X
t=2
(yt −φyt−1)2 −1
2 y2
1(1 −φ2) −y2
5
2 −φ2y2
3
2
+ φ2(y3 + y5)2
2(1 + φ2)2 (1 + φ2)
)
.

550
SOLUTIONS TO SELECTED PROBLEMS
With this result, we can calculate the conditional density
f(y4|y1, y2, y3, y5) =
(2π)−1/2(1 + φ2)1/2 exp
(
2φy4y5
2
−φ2y2
4
2
−y2
4
2 + 2φy4y3
2
−φ2(y3 + y5)2
2(1 + φ2)2 (1 + φ2)
)
.
Finally, we have that
f(y4|y1, y2, y3, y5) = (2π)−1/2(1 + φ2)1/2 exp


−1 + φ2
2
 
y4 −φ(y3 + y5)
(1 + φ2)
!2

.
Thus
E(y4|y1, y2, y3, y5) = φ(y3 + y5)
(1 + φ2) .
which is the same result obtained in part b).
Problem 11.3. By assumption yt is invertible and has an AR(∞) represen-
tation
εt = yt +
∞
X
j=1
πjyt−j ⇔yt = εt −
∞
X
j=1
πjyt−j.
As the ﬁlter π(L) = P∞
j=0 πjLj with π0 = 1 converges in L2, it holds
P∞
j=0 |πj|2 < ∞. The best linear predictor of yt given the inﬁnite past until
t−1 equals the projection of yt on Ht−1. For linear processes this is equivalent
to the conditional expectation
byt = E(yt|Ht−1) = E(εt|Ht−1) −E(
∞
X
j=1
πjyt−j|Ht−1) = −
∞
X
j=1
πjyt−j.
a) By deﬁnition
σ2
t+1(1) = [yt+1 −byt+1] = [yt+1 −Ptyt+1] .
As the t-th observation is missing Ht−1 ≡Ht and consequently the
projection operators are equal Pt = Pt−1.

551
Hence,
Ptyt+1 = Pt−1yt+1
=
E(εt+1|Ht−1) −E(
∞
X
j=1
πjyt+1−j|Ht−1)
=
0 + E(π1yt|Ht−1) −E(
∞
X
j=2
πjyt+1−j|Ht−1)
=
π1Pt−1yt −
∞
X
j=2
πjyt+1−j
=
π1Ptyt −
∞
X
j=2
πjyt+1−j
Therefore we can write the one step ahead prediction error at time t+1
as (using again Pt = Pt−1)
yt+1 −Ptyt+1
=
εt+1 −
∞
X
j=1
πjyt+1−j −

π1Ptyt −
∞
X
j=2
πjyt+1−j


=
εt+1 −π1yt −π1Ptyt
=
εt+1 −π1(yt −Pt−1yt),
which gives the equality
et+1(1) = εt+1 −π1et(1).
Note that εt+1 is uncorrelated with et(1) ∈Ht.
b) As εt+1 and et(1) have expectation zero and are uncorrelated, we can
compute the variance by
σ2
t+1(1)
=
εt+1 + π2
1et(1)
=
σ2
ε + π2
1σ2
t (1)
=
1 + π2
1.
As π2
1 ≥0, the prediction error variance for t + 1 is greater or equal to
the prediction error variance for t, given a missing value at t, i.e.
σ2
t+1(1) ≥σ2
t (1).
Problem 11.7. Note that we have the subspace H generated by {ys, s <
k, s ̸= t}. Thus,
yt+k
=
εt+k −
∞
X
j=1
πjyt+k−j
PHyt+k
=
−π1yt+k−1 −. . . −πk−1yt+1 −πkPHyt −πk+1yt−1 −. . . .

552
SOLUTIONS TO SELECTED PROBLEMS
Hence,
Var(yt+k −byt+k) = Var(εt+k −πk(yt −PHyt)) = Var(εt+k) + π2
kVar(yt −PHyt),
where Var(εt+k) = 1 and Var(yt −PHyt) < 1. Therefore,
|σ2
t+k(1) −1| = |1 + π2
kVar(yt −PHyt) −1| = |π2
kVar(yt −PHyt)| < |π2
k|.
But we know that |πk| < M(1 + δ)−k with 0 < δ < ϵ y M > 0, and once
speciﬁed we have that
|π2
k| = |πk|2 < M 2[(1 + δ)2]−k < ca−k,
where |a| < 1 y c = M 2. Thus, we conclude that
|σ2
t+k(1) −1| ≤ca−k.
Problem 11.9.
a) The interpolator of yt given {yj|j ̸= t} is given by
˜yt = −
∞
X
j=1
αj(yj + y−j) =


∞
X
j=1
αjBj +
∞
X
j=1
αjB−j

yt,
where
αj =
P∞
i=0 πiπi+j
P∞
i=0 π2
i
, for j ≥1.
Hence, xt can be rewritten to
xt
=
yt −˜yt
˜σ2
=
1
˜σ2

yt −


∞
X
j=1
αjBj +
∞
X
j=1
αjB−j

yt


=
1
˜σ2 α(B)yt
=
1
˜σ2 α(B)ψ(B)εt,
where ψ(B)εt is the Wold decomposition of yt.
b) The product of two absolutely convergent series equals


∞
X
j=0
aj


 ∞
X
k=0
bk
!
=
∞
X
n=0
cn

553
where
cn =
n
X
ℓ=0
aℓbn−ℓ.
To obtain the result we proceed by matching terms.
When multiplying π(z) by π(z−1) the coeﬃcients of zj are given by all
πi and πk such that
ziz−k = zj ⇒i −k = j ⇔i = k + j.
As this equation holds for all k = 0, 1, 2, . . ., we must sum over all k to
get the coeﬃcients αj, i.e.
αj = c ·
∞
X
k=0
πkπk+j.
As α0 = 1, the constant must equal
c =
1
P∞
k=0 π2
k
.
c) By deﬁnition the ﬁlter π(B) is the inverse of ψ(B) inverse, that is
π(B)ψ(B) = 1.
Using b) we can write
α(z)ψ(z) = cπ(z)π(z−1)ψ(z) = cπ(z−1)π(z)ψ(z) = cπ(z−1).
(B.37)
Replacing z by B proves the expression.
d) In a) we proved that
xt = 1
˜σ2 α(B)yt,
(B.38)
hence the spectral density of xt is given by
fx(λ) = 1
˜σ4 |α(eiλ)|2f(λ)
(B.39)
Additionally we have that
α(z) = π(z)π(z−1)c ⇔α(z)π(z)
π(z−1)
= c
(B.40)
and since ψ(z)π(z) = 1 we have that
α(z)ψ(z) = π(z)c.
(B.41)

554
SOLUTIONS TO SELECTED PROBLEMS
Therefore,
fx(λ)
=
c2
˜σ4 |π(eiλ)|2 σ2
2π
(B.42)
=
c2
˜σ4 |1
c
α(eiλ)
π(e−iλ)|2 σ2
2π
(B.43)
=
1
˜σ4 | α(eiλ)
π(e−iλ)|2 σ2
2π
(B.44)
=
1
˜σ4 | α(eiλ)
π(e−iλ)
ψ(e−iλ)
ψ(e−iλ)|2 σ2
2π
(B.45)
=
1
˜σ4 |α(eiλ)ψ(e−iλ)
π(e−iλ)
1
ψ(e−iλ)|2 2π
σ2
σ2
2π
σ2
2π
(B.46)
=
1
˜σ4
2π
σ2
σ2
2π
σ2
2π
(B.47)
=
1
˜σ4
1
fy(λ)
σ2
2π
σ2
2π
(B.48)
=
1
fy(λ)
σ2
(2π)2 .
(B.49)
Problem 11.15. Part (a). This is a standard result for the k steps ahead
error variance; see Subsection 7.3. Part (b). Let a(k, m) = Pk
j=k−m+1 πje(k−
j, m, k) and take t = k in the AR(∞) decomposition and subtract from both
sides the best linear predictor. Then all terms vanish, except for yk and those
associated with the missing observations, which yields the identity
e(k, m, k) = εk −a(k, m).
(B.50)
By the orthogonality of εk to all previous observations,
σ2(k, m, k) = σ2 + Var[a(k, m)],
for m ≥1. Now, by noting that
∥a(k, m)∥
≤
k
X
j=k−m+1
|πj|∥e(k −j, m, k)∥
≤
max
{j≥k−m} |πj|
k
X
j=k−m+1
∥e(k −j, m, k)∥
≤
max
{j≥k−m} |πj|mσy,
part (b) is obtained. Part (c) is a direct consequence of part (b) since πj →0
as j →∞and σ2(k, m, k) ≥σ2 for all k.

555
Problem 11.16. Part (a). Notice that for k ≤m we have that σ2(k, m, k) −
σ2
y = σ2 P∞
j=k ψ2
j . Since ψj ∼c0jd−1 for large j, σ2 P∞
j=k ψ2
j ∼ck2d−1 for
large k. Part (b). Observe that from equation (B.50) we have for k ≫m
π−2
k−m+1{σ2(k, m, k) −σ2}
=
Var


k
X
j=k−m+1
πj
πk−m+1
e(k −j, m, k)

,
=
Var


k
X
j=k−m+1
e(k −j, m, k)
+
k
X
j=k−m+1
bkje(k −j, m, k)

,
where bkj = πj/πk−m+1 −1 for j = k −m + 1, . . . , k. For m ﬁxed and large
k this term is bounded by
|bkj| ≤c1
k ,
for j = k −m + 1, . . . , k and c1 > 0. Since ∥e(k −j, m, k)∥≤σy, we conclude
that
π−2
k−m+1[σ2(k, m, k) −σ2] = Var


k
X
j=k−m+1
e(k −j, m, k)

+ O
1
k

,
for large k. Now, by deﬁning the following random variable:
zk =
m−1
X
j=0
yj −E


m−1
X
j=0
yj|yk, ..., ym, y−1, y−2, . . .

,
we have
π−2
k−m+1

σ2(k, m, k) −σ2	
= Var[zk] + O
1
k

.
But, 0 < Var[z∞] ≤Var[zk] ≤Var[zm] < ∞. Therefore σ2(k, m, k) −σ2 ∼
c2π2
k−m+1 ∼ck−2d−2 for k ≫m.
Problem 11.19. The modiﬁcations of the state covariance matrix equation
and the state prediction are obtained by noting that if yt is missing, then Pt =
Pt−1 and bxt+1 = E[xt+1|Pt] = E[Fxt + Hεt|Pt] = F E[xt|Pt] + E[Hεt|Pt] =
F E[xt|Pt−1] = F bxt. Thus, xt+1 −bxt+1 = F(xt −bxt) + Hεt and Ωt+1 =
FΩtF ′ + HH′σ2. On the other hand, since yt is missing, the innovation is

556
SOLUTIONS TO SELECTED PROBLEMS
null, so that νt = 0. Besides, let the observation yt be missing. Hence, by
deﬁnition
byt = E[yt|Pt−1].
From the state space system equation we have
yt = Gxt + εt,
and therefore,
byt = E[Gxt + εt|Pt−1] = Gbxt + E[εt|Pt−1] = Gbxt.
(B.51)

APPENDIX C
DATA AND CODES
The ﬁnancial time series data described in Chapter 1 are available at the Yahoo
Finance website www.yahoo.ﬁnance.com, including also the IPSA stock index
studied in Chapter 6. The Nile river time series is available at StatLib website
www.statlib.cmu.edu. Heating degree days data, passenger enplanements, UK
voting data, are available at the Datamarket website www.datamarket.com.
The source of the dengue data is DengueNet of the World Health Orga-
nization. They correspond to the annual number of combined dengue fever,
dengue hemorrhagic fever and dengue shock syndrome combined. These data
are available at www.who.int/denguenet.
The air pollution data are provided by the Ambient Air Quality Monitoring
Network, www.sesma.cl in Santiago, Chile.
Tree ring data such as the Mammoth Creek series are available at the
National Climatic Data Center, www.ncdc.noaa.gov. They were reported by
Graybill (1990).
*
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
557

558
DATA AND CODES
The mineral deposits series composed by stalagmite layer thickness obser-
vations taken at Shihua Cave, Beijing, China is reported by Tan et al. (2003).
Glacial varves data are available at www.stat.pitt.edu/∼stoﬀer/tsa.html.
Daily, monthly and annual gold and copper prices data are available from at
www.cochilco.cl/english. This site also reports prices of other metals. World
metals inventories are also available at this site.
In this book we have made use of the following R libraries, bfast, fore-
cast, nlts, gsarima, glarma, inarmix, wavelets, acp, ZIM, polynom, xtable, arﬁma,
fArma, see R Core Team (2014) for furthers details.

REFERENCES
M. Abrahams and A. Dempster. (1979).
Research on Seasonal Analysis.
Progress Report on the ASA/Census Project on Seasonal Adjustment. De-
partment of Statistics, Harvard University, Boston, MA.
P. Abry, P. Flandrin, M. S. Taqqu, and D. Veitch. (2003). Self-similarity and
long-range dependence through the wavelet lens. In P. Doukhan, G. Op-
penheim, and M. S. Taqqu, editors, Theory and Applications of Long-Range
Dependence. Birkh¨auser, Boston, MA, pp. 527–556.
R. K. Adenstedt. (1974). On large-sample estimation for the mean of a sta-
tionary random sequence. Annals of Statistics 2, 1095–1107.
M. A. Al-Osh and A. A. Alzaid. (1987). First-order integer-valued autoregres-
sive (inar (1)) process. Journal of Time Series Analysis 8, 261–275.
G. S. Ammar. (1998). Classical foundations of algorithms for solving positive
deﬁnite Toeplitz equations. Calcolo. A Quarterly on Numerical Analysis
and Theory of Computation 33, 99–113.
B. D. O. Anderson and J. B. Moore. (1979). Optimal Filtering. Prentice-Hall,
New York.
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
559

560
REFERENCES
C. F. Ansley and R. Kohn. (1983). Exact likelihood of vector autoregressive-
moving average process with missing or aggregated data. Biometrika 70,
275–278.
M. Aoki. (1990). State Space Modeling of Time Series. Springer, Berlin.
R. T. Baillie, T. Bollerslev, and H. O. Mikkelsen. (1996). Fractionally inte-
grated generalized autoregressive conditional heteroskedasticity. Journal of
Econometrics 74, 3–30.
G. K. Basak, N. H. Chan, and W. Palma. (2001).
The approximation of
long-memory processes by an ARMA model. Journal of Forecasting 20,
367–389.
W. Bell and S. Hillmer. (1991). Initializing the Kalman ﬁlter for nonstationary
time series models. Journal of Time Series Analysis 12, 283–300.
A. F. Bennett. (1992). Inverse Methods in Physical Oceanography. Cambridge
Monographs on Mechanics. Cambridge University Press, Cambridge.
J. Beran. (1994). Statistics for Long-Memory Processes Vol. 61, Monographs
on Statistics and Applied Probability. Chapman and Hall, New York.
S. Bertelli and M. Caporin. (2002). A note on calculating autocovariances of
long-memory processes. Journal of Time Series Analysis 23, 503–508.
R. J. Bhansali and P. S. Kokoszka. (2003). Prediction of long-memory time
series. In P. Doukhan, G. Oppenheim, and M. S. Taqqu, editors, Theory
and Applications of Long-Range Dependence. Birkh¨auser, Boston, MA, pp.
355–367.
T. Bollerslev. (1986). Generalized autoregressive conditional heteroskedastic-
ity. Journal of Econometrics 31, 307–327.
T. Bollerslev and H. O. Mikkelsen. (1996). Modeling and pricing long memory
in stock market volatility. Journal of Econometrics 73, 151–184.
P. Bondon. (2002). Prediction with incomplete past of a stationary process.
Stochastic Processes and their Applications 98, 67–76.
G. E. P. Box, G. M. Jenkins, and G. C. Reinsel. (1994). Time Series Analysis.
Prentice Hall, Englewood Cliﬀs, NJ.
G. E. P. Box and G. C. Tiao. (1992). Bayesian Inference in Statistical Anal-
ysis. John Wiley & Sons, Inc., New York.
F. J. Breidt, N. Crato, and P. de Lima. (1998). The detection and estimation
of long memory in stochastic volatility. Journal of Econometrics 83, 325–
348.

REFERENCES
561
D. R. Brillinger and P. R. Krishnaiah, editors. (1983). Time series in the
frequency domain Vol. 3, Handbook of Statistics. North-Holland Publishing
Co., Amsterdam.
A. E. Brockwell. (2004). A class of generalized long-memory time series mod-
els. Technical Report 813. Department of Statistics, Carnegie Mellon Uni-
versity, Pittsburgh.
P. J. Brockwell and R. A. Davis. (1991). Time Series: Theory and Methods.
Springer, New York.
P. J. Brockwell and R. A. Davis. (2002).
Introduction to time series and
forecasting. Springer Texts in Statistics. Springer-Verlag, New York.
Y. Cai and N. Davies. (2003). A simple diagnostic method of outlier detection
for stationary Gaussian time series. Journal of Applied Statistics 30, 205–
223.
A. C. Cameron and P. K. Trivedi. (2013). Regression analysis of count data
Vol. 53. Cambridge University Press, .
R. Carmona, W. L. Hwang, and B. Torresani. (1998). Practical time-frequency
analysis Vol. 9, Wavelet Analysis and its Applications. Academic Press,
Inc., San Diego, CA.
N. L. Carothers. (2000).
Real Analysis.
Cambridge University Press,
Cambridge.
F. Castani´e, editor. (2006). Spectral analysis. Digital Signal and Image Pro-
cessing Series. ISTE, London.
F. Castani´e, editor. (2011). Digital spectral analysis. Digital Signal and Image
Processing Series. ISTE, London; John Wiley & Sons, Inc., Hoboken, NJ.
J. E. Cavanaugh, Y. Wang, and J. W. Davis. (2003).
Locally self-similar
processes and their wavelet analysis. In Stochastic processes: modelling and
simulation, Vol. 21, Handbook of Statist. North-Holland, Amsterdam, pp.
93–135.
N. H. Chan. (2002). Time Series. Applications to Finance. Wiley Series in
Probability and Statistics. John Wiley & Sons, Inc., New York.
N. H. Chan and W. Palma. (1998). State space modeling of long-memory
processes. Annals of Statistics 26, 719–740.
N. H. Chan and G. Petris. (2000). Recent developments in heteroskedastic
time series. In W. S. Chan, W. K. Li, and H. Tong, editors, Statistics and
Finance: An Interface. Imperial College Press, London, pp. 169–184.

562
REFERENCES
G. Chandler and W. Polonik. (2006).
Discrimination of locally stationary
time series based on the excess mass functional. Journal of the American
Statistical Association 101, 240–253.
C. Chen and L. M. Liu. (1993). Joint estimation of model parameters and
outlier eﬀects in time series. Journal of the American Statistical Association
88, 284–297.
R. Cheng and M. Pourahmadi. (1997). Prediction with incomplete past and
interpolation of missing values. Statistics & Probability Letters 33, 341–346.
K. Choy. (2001).
Outlier detection for stationary time series.
Journal of
Statistical Planning and Inference 99, 111–127.
K. Choy and M. Taniguchi. (2001). Stochastic regression model with depen-
dent disturbances. Journal of Time Series Analysis 22, 175–196.
C. F. Chung. (1996).
A generalized fractionally integrated autoregressive
moving-average process. Journal of Time Series Analysis 17, 111–140.
J. B. Conway. (1990). A Course in Functional Analysis, Vol. 96, Graduate
Texts in Mathematics. Springer, New York.
D. R. Cox. (1981). Statistical analysis of time series: some recent develop-
ments. Scandinavian Journal of Statistics pp. 93–115.
D. R. Cox. (1984). Long-range dependence: A review. In H. A. David and
H. T. David, editors, Statistics: An Appraisal. Iowa State University Press,
Ames, IA, pp. 55–74.
R. F. Curtain and H. Zwart. (1995). An Introduction to Inﬁnite-Dimensional
Linear Systems Theory, Vol. 21, Texts in Applied Mathematics. Springer,
New York.
R. Dahlhaus. (1989). Eﬃcient parameter estimation for self-similar processes.
Annals of Statistics 17, 1749–1766.
R. Dahlhaus. (1995).
Eﬃcient location and regression estimation for long
range dependent regression models. Annals of Statistics 23, 1029–1047.
R. Dahlhaus. (1996). Asymptotic statistical inference for nonstationary pro-
cesses with evolutionary spectra. In Athens Conference on Applied Proba-
bility and Time Series Analysis, Vol. II (1995), Vol. 115, Lecture Notes in
Statist. Springer, New York, pp. 145–159.
R. Dahlhaus. (1997). Fitting time series models to nonstationary processes.
The Annals of Statistics 25, 1–37.
R. Dahlhaus. (2000). A likelihood approximation for locally stationary pro-
cesses. The Annals of Statistics 28, 1762–1794.

REFERENCES
563
R. Dahlhaus and W. Polonik. (2006). Nonparametric quasi-maximum likeli-
hood estimation for Gaussian locally stationary processes. The Annals of
Statistics 34, 2790–2824.
R. Dahlhaus and W. Polonik. (2009). Empirical spectral processes for locally
stationary time series. Bernoulli 15, 1–39.
E. Damsleth. (1980). Interpolating missing values in a time series. Scandina-
vian Journal of Statistics. Theory and Applications 7, 33–39.
R. A. Davis, W. Dunsmuir, and S. B. Streett. (2003). Observation-driven
models for poisson counts. Biometrika 90, 777–790.
R. S. Deo and C. M. Hurvich. (2003). Estimation of long memory in volatility.
In P. Doukhan, G. Oppenheim, and M. S. Taqqu, editors, Theory and
Applications of Long-Range Dependence. Birkh¨auser, Boston, MA, pp. 313–
324.
H. Dette, P. Preuß, and M. Vetter. (2011). A measure of stationarity in locally
stationary processes with applications to testing. Journal of the American
Statistical Association 106, 1113–1124.
P. J. Diggle. (1990). Time series Vol. 5, Oxford Statistical Science Series.
The Clarendon Press, Oxford University Press, New York.
G. S. Dissanayake, M. S. Peiris, and T. Proietti. (2014). State space modeling
of gegenbauer processes with long memory.
Computational Statistics &
Data Analysis.
J. A. Doornik and M. Ooms. (2003). Computational aspects of maximum like-
lihood estimation of autoregressive fractionally integrated moving average
models. Computational Statistics & Data Analysis 42, 333–348.
P. Doukhan, G. Oppenheim, and M. S. Taqqu, editors. (2003). Theory and
Applications of Long-Range Dependence. Birkh¨auser, Boston, MA.
J. Durbin. (1960). The ﬁtting of time series models. International Statistical
Review 28, 233–244.
J. Durbin and S. J. Koopman. (2001). Time Series Analysis by State Space
Methods, Vol. 24, Oxford Statistical Science Series. Oxford University Press,
Oxford.
K. Dzhaparidze. (1986). Parameter estimation and hypothesis testing in spec-
tral analysis of stationary time series. Springer Series in Statistics. Springer-
Verlag, New York.
R. F. Engle. (1982). Autoregressive conditional heteroscedasticity with es-
timates of the variance of United Kingdom inﬂation.
Econometrica 50,
987–1007.

564
REFERENCES
P. Flandrin. (1999). Time-Frequency/Time-Scale Analysis, Vol. 10, Wavelet
Analysis and Its Applications. Academic, San Diego, CA.
R. Fox and M. S. Taqqu. (1987). Central limit theorems for quadratic forms
in random variables having long-range dependence. Probability Theory and
Related Fields 74, 213–240.
R. K. Freeland and B. McCabe. (2004). Forecasting discrete valued low count
time series. International Journal of Forecasting 20, 427–434.
W. A. Fuller. (1996). Introduction to statistical time series. Wiley Series in
Probability and Statistics: Probability and Statistics. John Wiley & Sons,
Inc., New York.
E. Ghysels, A. C. Harvey, and E. Renault. (1996). Stochastic volatility. In Sta-
tistical Methods in Finance, Vol. 14, Handbook of Statistics. North-Holland,
Amsterdam, pp. 119–191.
L. Giraitis, J. Hidalgo, and P. M. Robinson. (2001). Gaussian estimation of
parametric spectral density with unknown pole. Annals of Statistics 29,
987–1023.
L. Giraitis, P. Kokoszka, R. Leipus, and G. Teyssi`ere. (2003). Rescaled vari-
ance and related tests for long memory in volatility and levels. Journal of
Econometrics 112, 265–294.
L. Giraitis and R. Leipus. (1995). A generalized fractionally diﬀerencing ap-
proach in long-memory modeling. Matematikos ir Informatikos Institutas
35, 65–81.
I. S. Gradshteyn and I. M. Ryzhik. (2000). Table of Integrals, Series, and
Products. Academic, San Diego, CA.
C. W. J. Granger. (1964). Spectral analysis of economic time series. In asso-
ciation with M. Hatanaka. Princeton Studies in Mathematical Economics,
No. I. Princeton University Press, Princeton, N.J.
C. W. J. Granger and R. Joyeux. (1980). An introduction to long-memory time
series models and fractional diﬀerencing. Journal of Time Series Analysis
1, 15–29.
S. Grassi and P. S. de Magistris. (2014). When long memory meets the kalman
ﬁlter: A comparative study. Computational Statistics & Data Analysis 76,
301–319.
H. L. Gray, N. F. Zhang, and W. A. Woodward. (1989).
On generalized
fractional processes. Journal of Time Series Analysis 10, 233–257.
D. A. Graybill. (1990). Pinus longaeva tree ring data. Mammoth Creek, Utah,
National Climatic Data Center.

REFERENCES
565
U. Grenander. (1954). On the estimation of regression coeﬃcients in the case
of an autocorrelated disturbance.
Annals of Mathematical Statistics 25,
252–272.
U. Grenander and M. Rosenblatt. (1957). Statistical Analysis of Stationary
Time Series. John Wiley & Sons, Inc., New York.
G. K. Grunwald, R. J. Hyndman, L. Tedesco, and R. L. Tweedie. (2000).
Non-gaussian conditional linear ar (1) models. Australian & New Zealand
Journal of Statistics 42, 479–495.
P. Hall. (1997). Deﬁning and measuring long-range dependence. In Nonlinear
Dynamics and Time Series (Montreal, PQ, 1995), Vol. 11, Fields Inst.
Commun. Amer. Math. Soc., Providence, RI, pp. 153–160.
J. D. Hamilton. (1994). Time Series Analysis. Princeton University Press,
Princeton, NJ.
E. J. Hannan. (1970). Multiple Time Series. John Wiley & Sons, Inc., New
York.
E. J. Hannan and M. Deistler. (1988). The Statistical Theory of Linear Sys-
tems. Wiley, New York.
A. C. Harvey. (1989). Forecasting Structural Time Series and the Kalman
Filter. Cambridge University Press, Cambridge.
A. C. Harvey and R. G. Pierse. (1984). Estimating missing observations in
economic time series. Journal of the American Statistical Association 79,
125–131.
A. C. Harvey, E. Ruiz, and N. Shephard. (1994).
Multivariate stochastic
variance models. Review of Economic Studies 61, 247–265.
U. Hassler. (1994). (Mis)speciﬁcation of long memory in seasonal time series.
Journal of Time Series Analysis 15, 19–30.
U. Hassler and J. Wolters. (1995). Long memory in inﬂation rates: Interna-
tional evidence. Journal of Business and Economic Statistics 13, 37–45.
J. Hasslett and A. E. Raftery. (1989). Space-time modelling with long-memory
dependence: Assessing Ireland’s wind power resource. Journal of Applied
Statistics 38, 1–50.
W. K. Hastings. (1970). Monte Carlo sampling methods using Markov chains
and their applications. Biometrika 57, 97–109.
S. Haykin, editor. (1979).
Nonlinear methods of spectral analysis Vol. 34,
Topics in Applied Physics. Springer-Verlag, Berlin-New York.

566
REFERENCES
M. Henry. (2001).
Averaged periodogram spectral estimation with long-
memory conditional heteroscedasticity.
Journal of Time Series Analysis
22, 431–459.
J. R. M. Hosking. (1981). Fractional diﬀerencing. Biometrika 68, 165–176.
I. A. Ibragimov and Y. A. Rozanov. (1978). Gaussian Random Processes,
Vol. 9, Applications of Mathematics. Springer, New York.
P. Iglesias, H. Jorquera, and W. Palma. (2006). Data analysis using regression
models with missing observations and long-memory: an application study.
Computational Statistics & Data Analysis 50, 2028–2043.
G. M. Jenkins and D. G. Watts. (1968). Spectral analysis and its applications.
Holden-Day, San Francisco, Calif.-Cambridge-Amsterdam.
M. Jensen and B. Witcher. (2000). Time-varying long memory in volatility:
detection and estimation with wavelets. Technical report. EURANDOM.
R. H. Jones. (1980). Maximum likelihood ﬁtting of ARMA models to time
series with missing observations. Technometrics 22, 389–395.
R. E. Kalman. (1961).
A new approach to linear ﬁltering and prediction
problems. Transactions of the American Society of Mechanical Engineers
83D, 35–45.
R. E. Kalman and R. S. Bucy. (1961). New results in linear ﬁltering and
prediction theory. Transactions of the American Society of Mechanical En-
gineers 83, 95–108.
B. Kedem and K. Fokianos. (2002). Regression models for time series anal-
ysis. Wiley Series in Probability and Statistics. John Wiley & Sons, Inc.,
Hoboken, NJ.
T. Kobayashi and D. L. Simon. (2003). Application of a bank of Kalman
ﬁlters for aircraft engine fault diagnostics.
Technical Report E-14088.
National Aeronautics and Space Administration, Washington, DC.
R. Kohn and C. F. Ansley. (1986). Estimation, prediction, and interpolation
for ARIMA models with missing data. Journal of the American Statistical
Association 81, 751–761.
L. H. Koopmans. (1995). The spectral analysis of time series Vol. 22, Proba-
bility and Mathematical Statistics. Academic Press, Inc., San Diego, CA.
T. W. K¨orner. (1989).
Fourier analysis.
Cambridge University Press,
Cambridge.
T. W. K¨orner. (1993). Exercises for Fourier analysis. Cambridge University
Press, Cambridge.

REFERENCES
567
H. K¨unsch. (1986). Discrimination between monotonic trends and long-range
dependence. Journal of Applied Probability 23, 1025–1030.
M. Last and R. H. Shumway. (2008). Detecting abrupt changes in a piecewise
locally stationary time series. Journal of Multivariate Analysis 99, 191–214.
N. Levinson. (1947). The Wiener RMS (root mean square) error criterion in
ﬁlter design and prediction. Journal of Mathematical Physics 25, 261–278.
W. K. Li and A. I. McLeod. (1986).
Fractional time series modelling.
Biometrika 73, 217–221.
S. Ling and W. K. Li. (1997).
On fractionally integrated autoregres-
sive moving-average time series models with conditional heteroscedasticity.
Journal of the American Statistical Association 92, 1184–1194.
R. J. A. Little and D. B. Rubin. (2002). Statistical Analysis with Missing
Data. Wiley Series in Probability and Statistics. Wiley, Hoboken, NJ.
I. L. MacDonald and W. Zucchini. (1997). Hidden Markov and other models
for discrete-valued time series Vol. 110. CRC Press, .
B. B. Mandelbrot and J. W. Van Ness. (1968). Fractional Brownian motions,
fractional noises and applications. SIAM Review 10, 422–437.
E. McKenzie. (1985). Some simple models for discrete variate time series.
Journal of the American Water Resources Association 21, 645–650.
E. McKenzie. (2003). Discrete variate time series. Handbook of statistics 21,
573–606.
N. Metropolis, A. W. Rosenbluth, A. H. Teller, and E. Teller. (1953). Equa-
tions of state calculations by fast computing machines. Journal of Chemical
Physics 21, 1087–1092.
A. Montanari, R. Rosso, and M. S. Taqqu. (2000).
A seasonal fractional
ARIMA model applied to Nile River monthly ﬂows at Aswan. Water Re-
sources Research 36, 1249–1259.
D. B. Nelson. (1991). Conditional heteroskedasticity in asset returns: a new
approach. Econometrica 59, 347–370.
M. Ooms. (1995). Flexible seasonal long memory and economic time series.
Technical Report EI-9515/A. Econometric Institute, Erasmus University,
Rotterdam.
W. Palma. (2000). Missing values in ARFIMA models. In W. S. Chan, W. K.
Li, and H. Tong, editors, Statistics and Finance: An Interface. Imperial
College Press, London, pp. 141–152.

568
REFERENCES
W. Palma. (2007). Long-Memory Time Series: Theory and Methods. Wiley
Series in Probability and Statistics. John Wiley & Sons, Inc., Hoboken, NJ.
W. Palma and N. H. Chan. (1997). Estimation and forecasting of long-memory
processes with missing values. Journal of Forecasting 16, 395–410.
W. Palma and N. H. Chan. (2005). Eﬃcient estimation of seasonal long-range-
dependent processes. Journal of Time Series Analysis 26, 863–892.
W. Palma and R. Olea. (2010). An eﬃcient estimator for locally stationary
Gaussian long-memory processes. The Annals of Statistics 38, 2958–2997.
W. Palma, R. Olea, and G. Ferreira. (2013). Estimation and forecasting of
locally stationary processes. Journal of Forecasting 32, 86–96.
W. Palma and M. Zevallos. (2011).
Fitting non-gaussian persistent data.
Applied Stochastic Models in Business and Industry 27, 23–36.
D. B. Percival and A. T. Walden. (2006). Wavelet Methods for Time Series
Analysis, Vol. 4, Cambridge Series in Statistical and Probabilistic Mathe-
matics. Cambridge University Press, Cambridge.
S. Porter-Hudak. (1990). An application of the seasonal fractionally diﬀer-
enced model to the monetary aggregates. Journal of the American Statis-
tical Association, Applic. Case Studies 85, 338–344.
M. Pourahmadi. (1989). Estimation and interpolation of missing values of a
stationary time series. Journal of Time Series Analysis 10, 149–169.
M. Pourahmadi. (2001). Foundations of Time Series Analysis and Prediction
Theory. John Wiley & Sons, Inc., New York.
S. J. Press. (2003). Subjective and Objective Bayesian Statistics. Wiley Series
in Probability and Statistics. John Wiley & Sons, Inc., Hoboken, NJ.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. (1992).
Numerical Recipes in FORTRAN. Cambridge University Press, Cambridge.
W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. (2007).
Numerical Recipes. Cambridge University Press, Cambridge.
M. B. Priestley. (1965). Evolutionary spectra and non-stationary processes.
Journal of the Royal Statistical Society. Series B. Statistical Methodology
27, 204–237.
M. B. Priestley. (1981a). Spectral Analysis and Time Series. Vol. 1. Academic,
London.
M. B. Priestley. (1981b). Spectral Analysis and Time Series. Vol. 2. Academic,
London.

REFERENCES
569
M. B. Priestley and H. Tong. (1973).
On the analysis of bivariate non-
stationary processes.
Journal of the Royal Statistical Society. Series B.
Methodological 35, 153–166, 179–188.
R Core Team.
R: A Language and Environment for Statistical Comput-
ing. R Foundation for Statistical Computing Vienna, Austria(2014). URL
http://www.R-project.org/.
G. Rangarajan and M. Ding, editors. (2003).
Procesess with Long-Range
Correlations. Springer, Berlin.
B. K. Ray. (1993a). Long-range forecasting of IBM product revenues using
a seasonal fractionally diﬀerenced ARMA model. International Journal of
Forecasting 9, 255–269.
B. K. Ray. (1993b). Modeling long-memory processes for optimal long-range
prediction. Journal of Time Series Analysis 14, 511–525.
B. K. Ray and R. S. Tsay. (2002). Bayesian methods for change-point detec-
tion in long-range dependent processes. Journal of Time Series Analysis
23, 687–705.
V. A. Reisen, A. L. Rodrigues, and W. Palma. (2006a). Estimating seasonal
long-memory processes: a Monte Carlo study. Journal of Statistical Com-
putation and Simulation 76, 305–316.
V. A. Reisen, A. L. Rodrigues, and W. Palma. (2006b). Estimation of seasonal
fractionally integrated processes. Computational Statistics & Data Analysis
50, 568–582.
C. P. Robert. (2001).
The Bayesian Choice.
Springer Texts in Statistics.
Springer, New York.
C. P. Robert and G. Casella. (2004).
Monte Carlo Statistical Methods.
Springer Texts in Statistics. Springer, New York.
P. M. Robinson. (1991). Testing for strong serial correlation and dynamic con-
ditional heteroskedasticity in multiple regression. Journal of Econometrics
47, 67–84.
P. M. Robinson and M. Henry. (1999). Long and short memory conditional
heteroskedasticity in estimating the memory parameter of levels. Econo-
metric Theory 15, 299–336.
Y. A. Rozanov. (1967). Stationary Random Processes. Holden-Day, San Fran-
cisco.
R. H. Shumway and D. S. Stoﬀer. (2011). Time series analysis and its appli-
cations. Springer Texts in Statistics. Springer, New York.

570
REFERENCES
P. Sibbertsen. (2001). S-estimation in the linear regression model with long-
memory error terms under trend.
Journal of Time Series Analysis 22,
353–363.
F. Sowell. (1992). Maximum likelihood estimation of stationary univariate
fractionally integrated time series models.
Journal of Econometrics 53,
165–188.
W. F. Stout. (1974). Almost Sure Convergence. Academic, New York–London.
T. Subba Rao and M. M. Gabr. (1984). An Introduction to Bispectral Anal-
ysis and Bilinear Time Series Models Vol. 24, Lecture Notes in Statistics.
Springer, New York.
M. Taniguchi and Y. Kakizawa. (2000).
Asymptotic Theory of Statistical
Inference for Time Series. Springer Series in Statistics. Springer, New York.
M. S. Taqqu. (2003). Fractional Brownian motion and long-range dependence.
In P. Doukhan, G. Oppenheim, and M. S. Taqqu, editors, Theory and
Applications of Long-Range Dependence. Birkh¨auser, Boston, MA, pp. 5–
38.
M. S. Taqqu, V. Teverovsky, and W. Willinger. (1995). Estimators for long-
range dependence: an empirical study. Fractals 3, 785–788.
S. J. Taylor. (1986). Modelling Financial Time Series. John Wiley & Sons,
Inc., New York.
G. Teyssi`ere and A. Kirman, editors. (2007). Long Memory in Economics.
Springer, Berlin.
G. C. Tiao and R. S. Tsay. (1994). Some advances in non linear and adaptive
modelling in time series. Journal of Forecasting 13, 109–131.
L. Tierney. (1994). Markov chains for exploring posterior distributions. Annals
of Statistics 22, 1701–1762.
H. Tong. (1973). Some comments on spectral representations of non-stationary
stochastic processes. Journal of Applied Probability 10, 881–885.
H. Tong. (1990).
Nonlinear time series Vol. 6, Oxford Statistical Science
Series. The Clarendon Press Oxford University Press, New York.
H. Tong. (2011). Threshold models in time series analysis–30 years on. Statis-
tics and its Interface 4, 107–118.
R. S. Tsay. (1989). Testing and modeling threshold autoregressive processes.
Journal of the American Statistical Association 84, 231–240.

REFERENCES
571
R. S Tsay. (2005). Analysis of Financial Time Series Vol. 543. John Wiley
& Sons, Inc., Hoboken, NJ.
R. S. Tsay. (2013). An introduction to analysis of ﬁnancial data with R. Wiley
Series in Probability and Statistics. John Wiley & Sons, Inc., Hoboken, NJ.
R. S. Tsay, D. Pe˜na, and A. E. Pankratz. (2000). Outliers in multivariate
time series. Biometrika 87, 789–804.
D. Veitch and P. Abry. (1999). A wavelet-based joint estimator of the pa-
rameters of long-range dependence. Institute of Electrical and Electronics
Engineers. Transactions on Information Theory 45, 878–897.
Y. Wang, J. E. Cavanaugh, and C. Song. (2001). Self-similarity index esti-
mation via wavelets for locally self-similar processes. Journal of Statistical
Planning and Inference 99, 91–110.
P. Whittle. (1951). Hypothesis Testing in Time Series Analysis. Hafner, New
York.
P. S. Wilson, A. C. Tomsett, and R. Toumi. (2003). Long-memory analysis
of time series with missing values. Physical Review E 68, 017103 (1)–(4).
W. A. Woodward, Q. C. Cheng, and H. L. Gray. (1998). A k-factor GARMA
long-memory model. Journal of Time Series Analysis 19, 485–504.
Y. Yajima. (1985). On estimation of long-memory time series models. Aus-
tralian Journal of Statistics 27, 303–320.
Y. Yajima. (1988). On estimation of a regression model with long-memory
stationary errors. Annals of Statistics 16, 791–807.
Y. Yajima. (1989). A central limit theorem of Fourier transforms of strongly
dependent stationary processes. Journal of Time Series Analysis 10, 375–
383.
Y. Yajima. (1991). Asymptotic properties of the LSE in a regression model
with long-memory stationary errors. Annals of Statistics 19, 158–177.
Y. Yajima and H. Nishino. (1999). Estimation of the autocorrelation func-
tion of a stationary time series with missing observations. Sankhy¯a. Indian
Journal of Statistics. Series A 61, 189–207.
M. Yang, J. E. Cavanaugh, and G. Zamba. (2014). State-space models for
count time series with excess zeros. Statistical Modelling.
M. Yang, G. Zamba, and J. E. Cavanaugh. (2013). Markov regression mod-
els for count time series with excess zeros: A partial likelihood approach.
Statistical Methodology 14, 26–38.
I. G Zurbenko. (1986). The spectral analysis of time series. North-Holland
Publishing Co., Amsterdam.


TOPIC INDEX
ACP model, 456
ACP, 458
APARCH, 222
AR-ARCH, 242
ARCH, 213, 215, 244, 252
ARCH(∞), 221, 252–253, 281
ARFIMA, 64, 183, 203, 219–220, 224
ARFIMA-GARCH, 220, 245, 252,
256–258, 276, 280, 292
ARFIMA-GARCH, 218–220
ARMA, 93, 114, 157, 162, 177, 180, 187,
193, 213, 219, 434, 436
ARMA-APARCH, 231
ARMA-GARCH, 231
Akaike’s information criterion, 153, 363,
389
deﬁnition, 153
Bayes estimator, 181
Bayes theorem, 180
Bayesian methods, 180–181, 194, 253
Beta distribution, 473
Beta model, 473
Box-Cox transformation, 41
Box-Ljung test, 34
Cauchy sequence, 488
Cauchy-Schwartz inequality, 395
Cholesky decomposition, 156
Cramer representation, 138
Daniell window, 129
Dickey-Fuller test, 298
Durbin-Levinson algorithm, 157, 269,
273, 291, 359, 361
EGARCH, 252
Euclidean inner product, 489
Euclidean norm, 367
Euler’ constant, 80
FIEGARCH, 222, 252, 258–259, 277
FIGARCH, 45, 221–222, 252, 256
FKF, 100
Fisher information, 183
Fourier coeﬃcients, 146
GARCH, 213, 216, 218–220, 244, 252
GARMA, 337, 351, 364–365
Gamma distribution, 446
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
573

574
TOPIC INDEX
Gibbs sampler, 182
Grenander conditions, 373, 379, 396–397
H¨older’s inequality, 489
Hankel Matrix, 93
Hankel matrix, 89, 93, 96, 115, 118
Haslett-Raftery estimate, 162–163, 179,
193, 202
Hessian matrix, 183
Hilbert space, 293–294, 487–489
Hurst exponent, 497
Hurst parameter, 175
INAR, 445
Joseph eﬀect, 70
Kalman ﬁlter, 89, 99, 114–116, 156, 161,
165, 168, 282, 359, 362, 400,
404–405, 407, 434, 436
Kalman gain, 98
Kalman prediction equations, 99
Kalman recursions, 100
Kalman recursive equations, 98
LMGARCH, 252
Lebesgue measure, 490
Lyapunov exponent, 219, 257–258
MSPE, 436
Markov chain Monte Carlo (MCMC),
180–181, 183, 194
Markov chain, 181–182, 204–205
Markovian processes, 64
Metropolis-Hastings algorithm, 181–182
Negative Binomial distribution, 446
Nile river, 6
Noah eﬀect, 70
Parzen method, 418
Poisson distribution, 446
Riemann zeta function, 174
SARFIMA, 337, 351–352, 355–357, 359,
363–364, 366–367
SARMA, 424
SETAR, 247–248
Stirling’s approximation, 78
Student distribution, 184
Szeg¨o-Kolmogorov formula, 494, 497
TAR process, 247–248
TAR, 247–248
Toeplitz structure, 157
Toeplitz, 307
Volterra expansion, 210, 221, 277
Whittle estimate, 158–160, 168, 179,
193–194, 337, 359, 361–362, 367
Wiener process, 84
Wishart distribution, 204
Wold expansion, 46, 48, 79, 88–89, 93,
96, 165, 168, 241–242, 268, 376,
437, 490–491
ZIM, 477–478
ZINB, 478
ZIP, 477
acceptance rate, 182
additive outlier, 424
approximate MLE, 220
asymmetric power autoregressive
conditionally heteroskedastic, 222
best linear interpolator, 410–411,
436–438
best linear unbiased estimator (BLUE),
153, 369, 373–374, 376, 378–379,
381
bfast, 326
causality, 487, 493
causal process, 492
characteristic function, 374, 392
colored noise, 127
complete space, 488
conjugate prior, 181
consistent, 375
controllability, 94, 114
controllable, 94
cumulant, 168
cyclical behavior, 337
data gaps, 399
data
Internet traﬃc, 362–363
Monthly US employment, 6
Nile River, 337, 364, 401, 405, 497
Particulate matter PM2.5, 7
Passenger Enplanements, 9
SP500 daily stock, 2
air pollution, 388–389, 392, 557
copper price, 212
deterministic, 490
detrended ﬂuctuation analysis, 171
discrete wavelet transform, 174
dynia R package, 424
eﬃciency, 376
ergodicity, 494–495

TOPIC INDEX
575
ergodic process, 494
expectation maximization (EM), 434
exponential random variable, 45
exponentially stable, 92
fGarch, 225
fast Fourier transform, 158
ﬂexible ARFISMA, 337
fractional Brownian motion, 71, 87, 497
fractional Gaussian noise, 71–72
fractional noise, 66–68, 163, 177, 179,
185, 187, 193, 202, 270–272, 290,
292–293, 366, 378, 381, 385,
407–408, 412, 436, 438–439
full data, 402
generalized gamma function, 204
gsarima, 343
harmonic regression, 385, 394
heteroskedastic, 212, 241, 276
hyper text transfer protocol (HTTP),
362
hyperbolic decay, 64
hypergeometric function, 67
improper prior, 180
imputation, 405
inarmix, 445
indicator function, 137
inner product, 488–489
innovation sequence, 493
innovational outlier, 424
integer part function, 45
intermediate memory, 220
interpolation error variance, 411–412
invariant, 494
invertible, 493
least squares estimator (LSE), 369,
373–379, 381, 384–387, 389, 394,
396
level shift, 424, 431, 433
leverage
leverage eﬀect, 222
linear ﬁlter, 491
linear regression model, 169, 171, 173,
369, 376, 378, 382, 385, 388–389,
392, 394–395
long-memory (deﬁnition), 64
long-memory stochastic volatility model
(LMSV), 222, 224, 253, 276
loss function, 181
maximum-likelihood estimate (MLE),
166, 178, 183–185, 187, 193, 220,
281, 337, 353, 359, 361–362, 367,
400
missing values, xiv, 89, 99, 114, 161,
399–402, 404–408, 410, 434–436,
439, 554
nlts, 211
norm, 488
observability matrix, 94, 515
observability, 94
observable, 94
observation driven, 441
orthonormal basis, 148, 293
outlier, 421
parallelogram law, 294, 488
parameter driven Poisson model, 452
parameter driven, 441
partial autocorrelation coeﬃcients, 269
particulate matter, 7
perfectly predictable, 490
periodogram, 125, 145, 148, 159, 361,
388
persistence, 371
polynomial regression, 382
posterior distribution, 181–182, 204
posterior mean, 181
prediction error variance, 268
prediction, 267
prior distribution, 180
projection theorem, 410, 412, 488
proposal distribution, 181–183
psi function, 174
purely nondeterministic, 490
quadratic loss, 181
quasi-maximum-likelihood estimate
(QMLE), 162–163, 222
random walk process, 110
raw periodogram, 128
reachability, 114
regular process, 93, 490, 493
relative eﬃciency, 378, 381–382, 385
rescaled range statistic (R/S), 170
reversibility condition, 205
scaling exponent, 497
seasonal long-memory processes, 351
seasonality, 337
self-similar process, 497

576
TOPIC INDEX
semiparametric estimate, 168
singular process, 490
slowly varying function, 64, 220, 274,
276
spectral density, 140, 411, 490
spectral distribution, 490
spectral representation theorem, 491
spectral representation, 138
splitting method, 177
standardized two-side innovation, 436
state space model, 115–116
state space models, 113
state space system, 46, 48, 89–90,
92–98, 114–118, 160, 164,
166–167, 253, 282, 362, 367, 404,
434
exponentially stable, 92
extended, 113
minimal, 95, 115
minimality, 95
observation equation, 48, 92
observation matrix, 48, 92
state predictor, 111–112
state smoother error variance, 99
state smoother, 99
strongly stable, 92
weakly stable, 92
state space systems, xiv, 93
stationary distribution, 182, 205
stationary increments, 87
stochastic volatility model (SV), 222
stochastic volatility, 252–253, 256
strict stationarity, 44, 495
strict white noise, 45
strictly stationary process, 45
strong law of large numbers, 495
structural model, 114
stylized facts, 212
subspace, 489
temporary change, 424
thinning operation, 442, 483
threshold autoregressive model, 247–248
time-varying spectral density, 140
transfer function, 138
transition matrix, 204
trend break regression, 395
tsDyn, 248
tsoutlier, 426, 429
uniform prior, 180
variance plot, 389
volatility, 213
wavelet, 136
Daubechies wavelets, 137, 145
Haar wavelet system, 137
Littlewood-Paley decomposition, 148
discrete wavelet transform (DWT),
136
multiresolution analysis, 137
scaling function, 137
wavelets package, 137
weak convergence, 397
weakly stationary process, 45, 493
white noise, 127
zero-inﬂated Negative Binomial, 478
zero-inﬂated Poisson distribution, 477
zero-inﬂated models, 477

AUTHOR INDEX
Abrahams M., 364
Abry P., 145, 174
Adenstedt R. K., 381
Ammar G. S., 193
Anderson B. D. O., 114
Ansley C. F., 114, 434
Aoki M., 114
Baillie R., 252
Basak G., 289
Bell W., 114
Bennett A. F., 114
Beran J., 72
Bertelli S., 193
Bhansali R. J., 289
Bollerslev T., 252
Bondon P., 435
Box G. E. P., 194, 331, 364
Breidt F. J., 253
Brockwell A. E., 253
Brockwell P. J., 39, 72, 114, 193, 364
Bucy R. S., 114
Caporin M., 193
Carothers N. L., 497
Casella G., 194
Chan N. H., 193, 253, 289, 365, 434
Cheng Q. C., 365
Cheng R., 435
Choy K., 392
Chung C. F., 365
Conway J. B., 497
Cox D. R., 72
Cox D.R., 483
Crato N., 253
Curtain R. F., 114
Dahlhaus R., 193, 392
Damsleth E., 435
Davis R. A., 39, 72, 114, 193, 364
Davis R.A., 483
Deistler M., 114
Dempster A., 364
Deo R., 253
Dette H., 332
Diggle P. J., 39
Dissanayake G. S., 193
Time Series Analysis. First Edition. Wilfredo Palma.
c⃝2016 John Wiley & Sons, Inc. Published 2016 by John Wiley & Sons, Inc.
577

578
AUTHOR INDEX
Doornik J. A., 193
Dunsmuir W., 483
Durbin J., 114, 193
Engle R., 252–253
Ferreira G., 332
Flandrin P., 145
Flannery B. P., 145
Fokianos K., 39, 483
Fox R., 194
Fuller W. A., 39
Ghysels E., 253
Giraitis L., 193, 365
Gradshteyn I. S., 258
Grassi S., 193
Gray H. L., 365
Graybill D. A., 557
Grenander U., 392
Grunwald G. K., 483
Hall P., 72
Hamilton J. D., 39
Hannan E. J., 114, 289, 392, 497
Harvey A. C., 114, 252–253, 434
Haslett J., 163, 179, 193
Hassler U., 364–365
Henry M., 252
Hillmer S., 114
Hosking J. R. M., 72
Hurst H. E., 170
Hurvich C. M., 253
Hyndman R. J., 483
Ibragimov I. A., 392
Iglesias P., 392
Jenkins G. M., 331, 364
Jones R. H., 114, 434
Jorquera H., 392
Kakizawa Y., 497
Kalman R. E., 114
Kedem B., 39, 483
Kobayashi T., 114
Kohn R., 114, 434
Kokoszka P. S., 193
Kokoszka P., 289
Kolmogorov A. N., 289, 494, 497
Koopman S. J., 114
Leipus R., 193
Levinson N., 193
Li W. K., 252
Ling S., 252
Little R. J. A., 435
Mandelbrot B. B., 364
Mikkelsen H. O., 252
Montanari A., 364
Moore J. B., 114
Nelson D. B., 252
Nishino H., 435
Olea R., 332
Ooms M., 193, 364
Palma W., 114, 193, 289, 332, 365, 392,
483
Peiris M. S., 193
Percival D. B., 145
Petris G., 253
Pierse R. G., 434
Porter-Hudak S., 364–365
Pourahmadi M., 289, 435, 497
Press S. J., 194
Press W. H., 145
Preuß P., 332
Proietti
T., 193
Raftery A., 163, 179, 193
Ray B. K., 114, 289, 364
Reinsel G. C., 331, 364
Reisen V., 365
Renault E., 253
Robert C. P., 194
Robinson P. M., 252–253
Rodrigues A., 365
Rosenblatt M., 392
Rosso R., 364
Rozanov Y. A., 289, 392
Rubin D. B., 435
Ruiz E., 253
Ryzhik I. M., 258
Shephard N., 253
Shumway R. H., 434
Shumway R. J., 39, 72
Simon D. L., 114
Sowell F., 72
Stoﬀer D. S., 434
Stoﬀer D., 39, 72
Stout W. F., 497
Streett S.B., 483
Surgailis D., 365
Szeg¨o G., 494, 497
Taniguchi M., 392, 497

AUTHOR INDEX
579
Taqqu M. S., 72, 145, 194, 364, 497
Tedesco L., 483
Teukolsky S. A., 145
Teverovsky V., 194
Teyssi`ere G., 193
Tiao G. C., 194, 289
Tierney L., 194
Tomsett A. C., 434
Tong H., 253
Toumi R., 434
Tsay R. S., 39, 114, 253, 289, 435
Tweedie R. L., 483
Van Ness J. W., 364
Veitch D., 145, 174
Vetter M., 332
Vetterling W. T., 145
Walden A. T., 145
Wiener N., 289
Willinger W., 194
Wilson P. S., 434
Wold H., 289
Woodward W. A., 365
Yajima Y., 194, 392, 395, 435
Zevallos M., 483
Zhang N. F., 365
Zwart H., 114

WILEY SERIES IN PROBABILITY AND STATISTICS
ESTABLISHED BY WALTER A. SHEWHART AND SAMUEL S. WILKS
Editors: David J. Balding, Noel A. C. Cressie, Garrett M. Fitzmaurice,
Geof H. Givens, Harvey Goldstein, Geert Molenberghs, David W. Scott, 
Adrian F. M. Smith, Ruey S. Tsay, Sanford Weisberg
Editors Emeriti: J. Stuart Hunter, Iain M. Johnstone, Joseph B. Kadane, 
Jozef L. Teugels
The Wiley Series in Probability and Statistics is well established and authoritative. It
covers many topics of current research interest in both pure and applied statistics and
probability theory. Written by leading statisticians and institutions, the titles span both
state-of-the-art developments in the field and classical methods.
Reflecting the wide range of current research in statistics, the series encompasses
applied, methodological and theoretical statistics, ranging from applications and new
techniques made possible by advances in computerized practice to rigorous treatment of
theoretical approaches.
This series provides essential and invaluable reading for all statisticians, whether in
aca demia, industry, government, or research.
† ABRAHAM and LEDOLTER · Statistical Methods for Forecasting
AGRESTI · Analysis of Ordinal Categorical Data, Second Edition
AGRESTI · An Introduction to Categorical Data Analysis, Second Edition
AGRESTI · Categorical Data Analysis, Third Edition
AGRESTI · Foundations of Linear and Generalized Linear Models
ALSTON, MENGERSEN and PETTITT (editors) · Case Studies in Bayesian
Statistical Modelling and Analysis
ALTMAN, GILL, and McDONALD · Numerical Issues in Statistical Computing
for the Social Scientist
AMARATUNGA and CABRERA · Exploration and Analysis of DNA Microarray
and Protein Array Data
AMARATUNGA, CABRERA, and SHKEDY · Exploration and Analysis of DNA
Microarray and Other High-Dimensional Data, Second Edition
ANDEˇ L · Mathematics of Chance
ANDERSON · An Introduction to Multivariate Statistical Analysis, Third Edition
*  ANDERSON · The Statistical Analysis of Time Series
ANDERSON, AUQUIER, HAUCK, OAKES, VANDAELE, and WEISBERG ·
Statistical Methods for Comparative Studies
ANDERSON and LOYNES · The Teaching of Practical Statistics
ARMITAGE and DAVID (editors) · Advances in Biometry
ARNOLD, BALAKRISHNAN, and NAGARAJA · Records
*  ARTHANARI and DODGE · Mathematical Programming in Statistics
AUGUSTIN, COOLEN, DE COOMAN and TROFFAES (editors) · Introduction
to Imprecise Probabilities
*  BAILEY · The Elements of Stochastic Processes with Applications to the Natural
Sciences
BAJORSKI · Statistics for Imaging, Optics, and Photonics
BALAKRISHNAN and KOUTRAS · Runs and Scans with Applications
BALAKRISHNAN and NG · Precedence-Type Tests and Applications
BARNETT · Comparative Statistical Inference, Third Edition
BARNETT · Environmental Statistics
BARNETT and LEWIS · Outliers in Statistical Data, Third Edition
*Now available in a lower priced paperback edition in the Wiley Classics Library.
†Now available in a lower priced paperback edition in the Wiley–Interscience Paperback
Series.

BARTHOLOMEW, KNOTT, and MOUSTAKI · Latent Variable Models and
Factor Analysis: A Unified Approach, Third Edition
BARTOSZYNSKI and NIEWIADOMSKA-BUGAJ · Probability and Statistical
Inference, Second Edition
BASILEVSKY · Statistical Factor Analysis and Related Methods: Theory and
Applications
BATES and WATTS · Nonlinear Regression Analysis and Its Applications
BECHHOFER, SANTNER, and GOLDSMAN · Design and Analysis of
Experiments for Statistical Selection, Screening, and Multiple Comparisons
BEH and LOMBARDO · Correspondence Analysis: Theory, Practice and New
Strategies
BEIRLANT, GOEGEBEUR, SEGERS, TEUGELS, and DE WAAL · Statistics of
Extremes: Theory and Applications
BELSLEY · Conditioning Diagnostics: Collinearity and Weak Data in Regression
† BELSLEY, KUH, and WELSCH · Regression Diagnostics: Identifying Influential
Data and Sources of Collinearity
BENDAT and PIERSOL · Random Data: Analysis and Measurement Procedures,
Fourth Edition
BERNARDO and SMITH · Bayesian Theory
BHAT and MILLER · Elements of Applied Stochastic Processes, Third Edition
BHATTACHARYA and WAYMIRE · Stochastic Processes with Applications
BIEMER, GROVES, LYBERG, MATHIOWETZ, and SUDMAN · Measurement
Errors in Surveys
BILLINGSLEY · Convergence of Probability Measures, Second Edition
BILLINGSLEY · Probability and Measure, Anniversary Edition
BIRKES and DODGE · Alternative Methods of Regression
BISGAARD and KULAHCI · Time Series Analysis and Forecasting by Example
BISWAS, DATTA, FINE, and SEGAL · Statistical Advances in the Biomedical
Sciences: Clinical Trials, Epidemiology, Survival Analysis, and Bioinformatics
BLISCHKE and MURTHY (editors) · Case Studies in Reliability and Maintenance
BLISCHKE and MURTHY · Reliability: Modeling, Prediction, and Optimization
BLOOMFIELD · Fourier Analysis of Time Series: An Introduction, Second Edition
BOLLEN · Structural Equations with Latent Variables
BOLLEN and CURRAN · Latent Curve Models: A Structural Equation Perspective
BONNINI, CORAIN, MAROZZI and SALMASO · Nonparametric Hypothesis
Testing: Rank and Permutation Methods with Applications in R
BOROVKOV · Ergodicity and Stability of Stochastic Processes
BOSQ and BLANKE · Inference and Prediction in Large Dimensions
BOULEAU · Numerical Methods for Stochastic Processes
*  BOX and TIAO · Bayesian Inference in Statistical Analysis
BOX · Improving Almost Anything, Revised Edition
*  BOX and DRAPER · Evolutionary Operation: A Statistical Method for Process
Improvement
BOX and DRAPER · Response Surfaces, Mixtures, and Ridge Analyses, Second
Edition
BOX, HUNTER, and HUNTER · Statistics for Experimenters: Design, Innovation,
and Discovery, Second Editon
BOX, JENKINS, REINSEL, and LJUNG · Time Series Analysis: Forecasting and
Control, Fifth  Edition
BOX, LUCEÑO, and PANIAGUA-QUIÑONES · Statistical Control by
Monitoring and Adjustment, Second Edition
*  BROWN and HOLLANDER · Statistics: A Biomedical Introduction
CAIROLI and DALANG · Sequential Stochastic Optimization
*Now available in a lower priced paperback edition in the Wiley Classics Library.
†Now available in a lower priced paperback edition in the Wiley–Interscience Paperback
Series.

CASTILLO, HADI, BALAKRISHNAN, and SARABIA · Extreme Value and
Related Models with Applications in Engineering and Science
CHAN · Time Series: Applications to Finance with R and S-Plus®, Second Edition
CHARALAMBIDES · Combinatorial Methods in Discrete Distributions
CHATTERJEE and HADI · Regression Analysis by Example, Fourth Edition
CHATTERJEE and HADI · Sensitivity Analysis in Linear Regression
CHEN · The Fitness of Information: Quantitative Assessments of Critical Evidence
CHERNICK · Bootstrap Methods: A Guide for Practitioners and Researchers,
Second Edition
CHERNICK and FRIIS · Introductory Biostatistics for the Health Sciences
CHILÈS and DELFINER · Geostatistics: Modeling Spatial Uncertainty, Second
Edition
CHIU, STOYAN, KENDALL and MECKE · Stochastic Geometry and Its
Applications, Third Edition
CHOW and LIU · Design and Analysis of Clinical Trials: Concepts and
Methodologies, Third Edition
CLARKE · Linear Models: The Theory and Application of Analysis of Variance
CLARKE and DISNEY · Probability and Random Processes: A First Course with
Applications, Second Edition
*  COCHRAN and COX · Experimental Designs, Second Edition
COLLINS and LANZA · Latent Class and Latent Transition Analysis: With
Applications in the Social, Behavioral, and Health Sciences
CONGDON · Applied Bayesian Modelling, Second Edition
CONGDON · Bayesian Models for Categorical Data
CONGDON · Bayesian Statistical Modelling, Second Edition
CONOVER · Practical Nonparametric Statistics, Third Edition
COOK · Regression Graphics
COOK and WEISBERG · An Introduction to Regression Graphics
COOK and WEISBERG · Applied Regression Including Computing and Graphics
CORNELL · A Primer on Experiments with Mixtures
CORNELL · Experiments with Mixtures, Designs, Models, and the Analysis of
Mixture Data, Third Edition
COX · A Handbook of Introductory Statistical Methods
CRESSIE · Statistics for Spatial Data, Revised Edition
CRESSIE and WIKLE · Statistics for Spatio-Temporal Data
CSÖRGO´´ and HORVÁTH · Limit Theorems in Change Point Analysis
DAGPUNAR · Simulation and Monte Carlo: With Applications in Finance and
MCMC
DANIEL · Applications of Statistics to Industrial Experimentation
DANIEL · Biostatistics: A Foundation for Analysis in the Health Sciences, Eighth
Edition
*  DANIEL · Fitting Equations to Data: Computer Analysis of Multifactor Data,
Second Edition
DASU and JOHNSON · Exploratory Data Mining and Data Cleaning
DAVID and NAGARAJA · Order Statistics, Third Edition
DAVINO, FURNO and VISTOCCO · Quantile Regression: Theory and
Applications
*  DEGROOT, FIENBERG, and KADANE · Statistics and the Law
DEL CASTILLO · Statistical Process Adjustment for Quality Control
DEMARIS · Regression with Social Data: Modeling Continuous and Limited
Response Variables
DEMIDENKO · Mixed Models: Theory and Applications with R, Second Edition
*Now available in a lower priced paperback edition in the Wiley Classics Library.
†Now available in a lower priced paperback edition in the Wiley–Interscience Paperback
Series.

DENISON, HOLMES, MALLICK, and SMITH · Bayesian Methods for Nonlinear
Classification and Regression
DETTE and STUDDEN · The Theory of Canonical Moments with Applications in
Statistics, Probability, and Analysis
DEY and MUKERJEE · Fractional Factorial Plans
DILLON and GOLDSTEIN · Multivariate Analysis: Methods and  Applications
*  DODGE and ROMIG · Sampling Inspection Tables, Second Edition
*  DOOB · Stochastic Processes
DOWDY, WEARDEN, and CHILKO · Statistics for Research, Third Edition
DRAPER and SMITH · Applied Regression Analysis, Third Edition
DRYDEN and MARDIA · Statistical Shape Analysis
DUDEWICZ and MISHRA · Modern Mathematical Statistics
DUNN and CLARK · Basic Statistics: A Primer for the Biomedical Sciences, Fourth
Edition
DUPUIS and ELLIS · A Weak Convergence Approach to the Theory of Large
Deviations
EDLER and KITSOS · Recent Advances in Quantitative Methods in Cancer and
Human Health Risk Assessment
*  ELANDT-JOHNSON and JOHNSON · Survival Models and Data Analysis
ENDERS · Applied Econometric Time Series, Third Edition
† ETHIER and KURTZ · Markov Processes: Characterization and Convergence
EVANS, HASTINGS, and PEACOCK · Statistical Distributions, Third Edition
EVERITT, LANDAU, LEESE, and STAHL · Cluster Analysis, Fifth Edition
FEDERER and KING · Variations on Split Plot and Split Block Experiment
Designs
FELLER · An Introduction to Probability Theory and Its Applications, Volume I,
Third Edition, Revised; Volume II, Second Edition
FITZMAURICE, LAIRD, and WARE · Applied Longitudinal Analysis, Second
Edition
*  FLEISS · The Design and Analysis of Clinical Experiments
FLEISS · Statistical Methods for Rates and Proportions, Third Edition
† FLEMING and HARRINGTON · Counting Processes and Survival Analysis
FUJIKOSHI, ULYANOV, and SHIMIZU · Multivariate Statistics: High-
Dimensional and Large-Sample Approximations
FULLER · Introduction to Statistical Time Series, Second Edition
† FULLER · Measurement Error Models
GALLANT · Nonlinear Statistical Models
GEISSER · Modes of Parametric Statistical Inference 
GELMAN and MENG · Applied Bayesian Modeling and Causal Inference from
ncomplete-Data Perspectives
GEWEKE · Contemporary Bayesian Econometrics and Statistics 
GHOSH, MUKHOPADHYAY, and SEN · Sequential Estimation
GIESBRECHT and GUMPERTZ · Planning, Construction, and Statistical Analysis
of Comparative Experiments
GIFI · Nonlinear Multivariate Analysis
GIVENS and HOETING · Computational Statistics
GLASSERMAN and YAO · Monotone Structure in Discrete-Event Systems
GNANADESIKAN · Methods for Statistical Data Analysis of Multivariate
Observations, Second Edition
GOLDSTEIN · Multilevel Statistical Models, Fourth Edition
GOLDSTEIN and LEWIS · Assessment: Problems, Development, and Statistical
Issues
GOLDSTEIN and WOOFF · Bayes Linear Statistics
*Now available in a lower priced paperback edition in the Wiley Classics Library.
†Now available in a lower priced paperback edition in the Wiley–Interscience Paperback
Series.

GRAHAM · Markov Chains: Analytic and Monte Carlo Computations
GREENWOOD and NIKULIN · A Guide to Chi-Squared Testing
GROSS, SHORTLE, THOMPSON, and HARRIS · Fundamentals of Queueing
Theory, Fourth Edition
GROSS, SHORTLE, THOMPSON, and HARRIS · Solutions Manual to
Accompany Fundamentals of Queueing Theory, Fourth Edition
*  HAHN and SHAPIRO · Statistical Models in Engineering
HAHN and MEEKER · Statistical Intervals: A Guide for Practitioners
HALD · A History of Probability and Statistics and their Applications Before 1750
† HAMPEL · Robust Statistics: The Approach Based on Influence Functions
HARTUNG, KNAPP, and SINHA · Statistical Meta-Analysis with Applications
HEIBERGER · Computation for the Analysis of Designed Experiments
HEDAYAT and SINHA · Design and Inference in Finite Population Sampling
HEDEKER and GIBBONS · Longitudinal Data Analysis
HELLER · MACSYMA for Statisticians
HERITIER, CANTONI, COPT, and VICTORIA-FESER · Robust Methods in
Biostatistics
HINKELMANN and KEMPTHORNE · Design and Analysis of Experiments,
Volume 1: Introduction to Experimental Design, Second Edition
HINKELMANN and KEMPTHORNE · Design and Analysis of Experiments,
Volume 2: Advanced Experimental Design
HINKELMANN (editor) · Design and Analysis of Experiments, Volume 3: Special
Designs and Applications
HOAGLIN, MOSTELLER, and TUKEY · Fundamentals of Exploratory Analysis
of Variance
*  HOAGLIN, MOSTELLER, and TUKEY · Exploring Data Tables, Trends and
Shapes
*  HOAGLIN, MOSTELLER, and TUKEY · Understanding Robust and Exploratory
Data Analysis
HOCHBERG and TAMHANE · Multiple Comparison Procedures
HOCKING · Methods and Applications of Linear Models: Regression and the
Analysis of Variance, Third Edition
HOEL · Introduction to Mathematical Statistics, Fifth Edition
HOGG and KLUGMAN · Loss Distributions
HOLLANDER, WOLFE, and CHICKEN · Nonparametric Statistical Methods,
Third Edition
HOSMER and LEMESHOW · Applied Logistic Regression, Second Edition
HOSMER, LEMESHOW, and MAY · Applied Survival Analysis: Regression
Modeling  of Time-to-Event Data, Second Edition
HUBER · Data Analysis: What Can Be Learned From the Past 50 Years
HUBER · Robust Statistics
† HUBER and RONCHETTI · Robust Statistics, Second Edition
HUBERTY · Applied Discriminant Analysis, Second Edition
HUBERTY and OLEJNIK · Applied MANOVA and Discriminant Analysis, Second
Edition
HUITEMA · The Analysis of Covariance and Alternatives: Statistical Methods for
Experiments, Quasi-Experiments, and Single-Case Studies, Second Edition
HUNT and KENNEDY · Financial Derivatives in Theory and Practice, Revised
Edition
HURD and MIAMEE · Periodically Correlated Random Sequences: Spectral
Theory and Practice
HUSKOVA, BERAN, and DUPAC · Collected Works of Jaroslav Hajek— with
Commentary
HUZURBAZAR · Flowgraph Models for Multistate Time-to-Event Data
*Now available in a lower priced paperback edition in the Wiley Classics Library.
†Now available in a lower priced paperback edition in the Wiley–Interscience Paperback
Series.

JACKMAN · Bayesian Analysis for the Social Sciences
† JACKSON · A User’s Guide to Principle Components
JOHN · Statistical Methods in Engineering and Quality Assurance
JOHNSON · Multivariate Statistical Simulation
JOHNSON and BALAKRISHNAN · Advances in the Theory and Practice of
Statistics: A Volume in Honor of Samuel Kotz
JOHNSON, KEMP, and KOTZ · Univariate Discrete Distributions, Third Edition
JOHNSON and KOTZ (editors) · Leading Personalities in Statistical Sciences: From
the Seventeenth Century to the Present
JOHNSON, KOTZ, and BALAKRISHNAN · Continuous Univariate
Distributions, Volume 1, Second Edition
JOHNSON, KOTZ, and BALAKRISHNAN · Continuous Univariate
Distributions, Volume 2, Second Edition
JOHNSON, KOTZ, and BALAKRISHNAN · Discrete Multivariate Distributions
JUDGE, GRIFFITHS, HILL, LÜTKEPOHL, and LEE · The Theory and Practice
of Econometrics, Second Edition
JUREK and MASON · Operator-Limit Distributions in Probability Theory
KADANE · Bayesian Methods and Ethics in a Clinical Trial Design
KADANE AND SCHUM · A Probabilistic Analysis of the Sacco and Vanzetti
Evidence
KALBFLEISCH and PRENTICE · The Statistical Analysis of Failure Time Data,
Second Edition
KARIYA and KURATA · Generalized Least Squares
KASS and VOS · Geometrical Foundations of Asymptotic Inference
† KAUFMAN and ROUSSEEUW · Finding Groups in Data: An Introduction to
Cluster Analysis
KEDEM and FOKIANOS · Regression Models for Time Series Analysis
KENDALL, BARDEN, CARNE, and LE · Shape and Shape Theory
KHURI · Advanced Calculus with Applications in Statistics, Second Edition
KHURI, MATHEW, and SINHA · Statistical Tests for Mixed Linear Models
* KISH · Statistical Design for Research
KLEIBER and KOTZ · Statistical Size Distributions in Economics and Actuarial
Sciences
KLEMELÄ · Smoothing of Multivariate Data: Density Estimation and
Visualization
KLUGMAN, PANJER, and WILLMOT · Loss Models: From Data to Decisions,
Third Edition
KLUGMAN, PANJER, and WILLMOT · Loss Models: Further Topics
KLUGMAN, PANJER, and WILLMOT · Solutions Manual to Accompany Loss
Models: From Data to Decisions, Third Edition
KOSKI and NOBLE · Bayesian Networks: An Introduction
KOTZ, BALAKRISHNAN, and JOHNSON · Continuous Multivariate
Distributions, Volume 1, Second Edition
KOTZ and JOHNSON (editors) · Encyclopedia of Statistical Sciences: Volumes 1 to
9 with Index
KOTZ and JOHNSON (editors) · Encyclopedia of Statistical Sciences: Supplement
Volume
KOTZ, READ, and BANKS (editors) · Encyclopedia of Statistical Sciences: Update
Volume 1
KOTZ, READ, and BANKS (editors) · Encyclopedia of Statistical Sciences: Update
Volume 2
KOWALSKI and TU · Modern Applied U-Statistics
KRISHNAMOORTHY and MATHEW · Statistical Tolerance Regions: Theory,
Applications, and Computation
*Now available in a lower priced paperback edition in the Wiley Classics Library.
†Now available in a lower priced paperback edition in the Wiley–Interscience Paperback
Series.

KROESE, TAIMRE, and BOTEV · Handbook of Monte Carlo Methods 
KROONENBERG · Applied Multiway Data Analysis
KULINSKAYA, MORGENTHALER, and STAUDTE · Meta Analysis: A Guide
to Calibrating and Combining Statistical Evidence
KULKARNI and HARMAN · An Elementary Introduction to Statistical Learning
Theory
KUROWICKA and COOKE · Uncertainty Analysis with High Dimensional
Dependence Modelling
KVAM and VIDAKOVIC · Nonparametric Statistics with Applications to Science
and Engineering
LACHIN · Biostatistical Methods: The Assessment of Relative Risks, Second Edition
LAD · Operational Subjective Statistical Methods: A Mathematical, Philosophical,
and Historical Introduction
LAMPERTI · Probability: A Survey of the Mathematical Theory, Second Edition
LAWLESS · Statistical Models and Methods for Lifetime Data, Second Edition
LAWSON · Statistical Methods in Spatial Epidemiology, Second Edition
LE · Applied Categorical Data Analysis, Second Edition
LE · Applied Survival Analysis
LEE · Structural Equation Modeling: A Bayesian Approach
LEE and WANG · Statistical Methods for Survival Data Analysis, Fourth Edition
LEPAGE and BILLARD · Exploring the Limits of Bootstrap
LESSLER and KALSBEEK · Nonsampling Errors in Surveys
LEYLAND and GOLDSTEIN (editors) · Multilevel Modelling of Health Statistics
LIAO · Statistical Group Comparison
LIN · Introductory Stochastic Analysis for Finance and Insurance
LINDLEY · Understanding Uncertainty, Revised Edition
LITTLE and RUBIN · Statistical Analysis with Missing Data, Second Edition
LLOYD · The Statistical Analysis of Categorical Data
LOWEN and TEICH · Fractal-Based Point Processes
MAGNUS and NEUDECKER · Matrix Differential Calculus with Applications in
Statistics and Econometrics, Revised Edition
MALLER and ZHOU · Survival Analysis with Long Term Survivors
MARCHETTE · Random Graphs for Statistical Pattern Recognition
MARDIA and JUPP · Directional Statistics
MARKOVICH · Nonparametric Analysis of Univariate Heavy-Tailed Data:
Research and Practice
MARONNA, MARTIN and YOHAI · Robust Statistics: Theory and Methods
MASON, GUNST, and HESS · Statistical Design and Analysis of Experiments with
Applications to Engineering and Science, Second Edition
McCULLOCH, SEARLE, and NEUHAUS · Generalized, Linear, and Mixed
Models, Second Edition
McFADDEN · Management of Data in Clinical Trials, Second Edition
*  McLACHLAN · Discriminant Analysis and Statistical Pattern Recognition
McLACHLAN, DO, and AMBROISE · Analyzing Microarray Gene Expression
Data
McLACHLAN and KRISHNAN · The EM Algorithm and Extensions, Second
Edition
McLACHLAN and PEEL · Finite Mixture Models
McNEIL · Epidemiological Research Methods
MEEKER and ESCOBAR · Statistical Methods for Reliability Data
MEERSCHAERT and SCHEFFLER · Limit Distributions for Sums of Independent
Random Vectors: Heavy Tails in Theory and Practice
MENGERSEN, ROBERT, and TITTERINGTON · Mixtures: Estimation and
Applications
*Now available in a lower priced paperback edition in the Wiley Classics Library.
†Now available in a lower priced paperback edition in the Wiley–Interscience Paperback
Series.

MICKEY, DUNN, and CLARK · Applied Statistics: Analysis of Variance and
Regression, Third Edition
* MILLER · Survival Analysis, Second Edition
MONTGOMERY, JENNINGS, and KULAHCI · Introduction to Time Series
Analysis and Forecasting, Second Edition
MONTGOMERY, PECK, and VINING · Introduction to Linear Regression
Analysis,  Fifth Edition
MORGENTHALER and TUKEY · Configural Polysampling: A Route to Practical
Robustness
MUIRHEAD · Aspects of Multivariate Statistical Theory
MULLER and STOYAN · Comparison Methods for Stochastic Models and Risks
MURTHY, XIE, and JIANG · Weibull Models
MYERS, MONTGOMERY, and ANDERSON-COOK · Response Surface
Methodology: Process and Product Optimization Using Designed Experiments,
Third Edition
MYERS, MONTGOMERY, VINING, and ROBINSON · Generalized Linear
Models. With Applications in Engineering and the Sciences, Second Edition
NATVIG · Multistate Systems Reliability Theory With Applications
† NELSON · Accelerated Testing, Statistical Models, Test Plans, and Data Analyses
† NELSON · Applied Life Data Analysis
NEWMAN · Biostatistical Methods in Epidemiology
NG, TAIN, and TANG · Dirichlet Theory: Theory, Methods and Applications
OKABE, BOOTS, SUGIHARA, and CHIU · Spatial Tesselations: Concepts and
Applications of Voronoi Diagrams, Second Edition
OLIVER and SMITH · Influence Diagrams, Belief Nets and Decision Analysis
PALMA · Time Series Analysis
PALTA · Quantitative Methods in Population Health: Extensions of Ordinary
Regressions
PANJER · Operational Risk: Modeling and Analytics
PANKRATZ · Forecasting with Dynamic Regression Models
PANKRATZ · Forecasting with Univariate Box-Jenkins Models: Concepts and
Cases
PARDOUX · Markov Processes and Applications: Algorithms, Networks, Genome
and Finance
PARMIGIANI and INOUE · Decision Theory: Principles and Approaches
* PARZEN · Modern Probability Theory and Its Applications
PEÑA, TIAO, and TSAY · A Course in Time Series Analysis
PESARIN and SALMASO · Permutation Tests for Complex Data: Applications and
Software
PIANTADOSI · Clinical Trials: A Methodologic Perspective, Second Edition
POURAHMADI · Foundations of Time Series Analysis and Prediction Theory
POURAHMADI · High-Dimensional Covariance Estimation
POWELL · Approximate Dynamic Programming: Solving the Curses of
Dimensionality, Second Edition
POWELL and RYZHOV · Optimal Learning
PRESS · Subjective and Objective Bayesian Statistics, Second Edition
PRESS and TANUR · The Subjectivity of Scientists and the Bayesian Approach
PURI, VILAPLANA, and WERTZ · New Perspectives in Theoretical and Applied
Statistics
† PUTERMAN · Markov Decision Processes: Discrete Stochastic Dynamic
Programming
QIU · Image Processing and Jump Regression Analysis
* RAO · Linear Statistical Inference and Its Applications, Second Edition
*Now available in a lower priced paperback edition in the Wiley Classics Library.
†Now available in a lower priced paperback edition in the Wiley–Interscience Paperback
Series.

RAO · Statistical Inference for Fractional Diffusion Processes
RAUSAND and HØYLAND · System Reliability Theory: Models, Statistical
Methods, and Applications, Second Edition
RAYNER, THAS, and BEST · Smooth Tests of Goodnes of Fit: Using R, Second
Edition
RENCHER and SCHAALJE · Linear Models in Statistics, Second Edition
RENCHER and CHRISTENSEN · Methods of Multivariate Analysis, Third Edition
RENCHER · Multivariate Statistical Inference with Applications
RIGDON and BASU · Statistical Methods for the Reliability of Repairable Systems
*  RIPLEY · Spatial Statistics
*  RIPLEY · Stochastic Simulation
ROHATGI and SALEH · An Introduction to Probability and Statistics, Third
Edition
ROLSKI, SCHMIDLI, SCHMIDT, and TEUGELS · Stochastic Processes for
Insurance and Finance
ROSENBERGER and LACHIN · Randomization in Clinical Trials: Theory and
Practice
ROSSI, ALLENBY, and MCCULLOCH · Bayesian Statistics and Marketing
† ROUSSEEUW and LEROY · Robust Regression and Outlier Detection
ROYSTON and SAUERBREI · Multivariate Model Building: A Pragmatic
Approach to Regression Analysis Based on Fractional Polynomials for Modeling
Continuous Variables
*  RUBIN · Multiple Imputation for Nonresponse in Surveys
RUBINSTEIN and KROESE · Simulation and the Monte Carlo Method, Second
Edition
RUBINSTEIN and MELAMED · Modern Simulation and Modeling
RUBINSTEIN, RIDDER, and VAISMAN · Fast Sequential Monte Carlo Methods
for Counting and Optimization
RYAN · Modern Engineering Statistics
RYAN · Modern Experimental Design
RYAN · Modern Regression Methods, Second Edition
RYAN · Sample Size Determination and Power
RYAN · Statistical Methods for Quality Improvement, Third Edition
SALEH · Theory of Preliminary Test and Stein-Type Estimation with Applications
SALTELLI, CHAN, and SCOTT (editors) · Sensitivity Analysis
SCHERER · Batch Effects and Noise in Microarray Experiments: Sources and
Solutions 
* SCHEFFE · The Analysis of Variance
SCHIMEK · Smoothing and Regression: Approaches, Computation, and
Application
SCHOTT · Matrix Analysis for Statistics, Second Edition
SCHOUTENS · Levy Processes in Finance: Pricing Financial Derivatives
SCOTT · Multivariate Density Estimation
SCOTT · Multivariate Density Estimation: Theory, Practice, and Visualization
* SEARLE · Linear Models
† SEARLE · Linear Models for Unbalanced Data
† SEARLE · Matrix Algebra Useful for Statistics
† SEARLE, CASELLA, and McCULLOCH · Variance Components
SEARLE and WILLETT · Matrix Algebra for Applied Economics
SEBER · A Matrix Handbook For Statisticians
† SEBER · Multivariate Observations
SEBER and LEE · Linear Regression Analysis, Second Edition
† SEBER and WILD · Nonlinear Regression
*Now available in a lower priced paperback edition in the Wiley Classics Library.
†Now available in a lower priced paperback edition in the Wiley–Interscience Paperback
Series.

SENNOTT · Stochastic Dynamic Programming and the Control of Queueing
Systems
* SERFLING · Approximation Theorems of Mathematical Statistics
SHAFER and VOVK · Probability and Finance: It’s Only a Game!
SHERMAN · Spatial Statistics and Spatio-Temporal Data: Covariance Functions
and Directional Properties
SILVAPULLE and SEN · Constrained Statistical Inference: Inequality, Order, and
Shape Restrictions
SINGPURWALLA · Reliability and Risk: A Bayesian Perspective
SMALL and MCLEISH · Hilbert Space Methods in Probability and Statistical
Inference
SRIVASTAVA · Methods of Multivariate Statistics
STAPLETON · Linear Statistical Models, Second Edition
STAPLETON · Models for Probability and Statistical Inference: Theory and
Applications
STAUDTE and SHEATHER · Robust Estimation and Testing
STOYAN · Counterexamples in Probability, Second Edition
STOYAN and STOYAN · Fractals, Random Shapes and Point Fields: Methods of
Geometrical Statistics
STREET and BURGESS · The Construction of Optimal Stated Choice Experiments:
Theory and Methods
STYAN · The Collected Papers of T. W. Anderson: 1943–1985
SUTTON, ABRAMS, JONES, SHELDON, and SONG · Methods for Meta-
Analysis in Medical Research
TAKEZAWA · Introduction to Nonparametric Regression
TAMHANE · Statistical Analysis of Designed Experiments: Theory and
Applications
TANAKA · Time Series Analysis: Nonstationary and Noninvertible Distribution
Theory
THOMPSON · Empirical Model Building: Data, Models, and Reality, Second
Edition
THOMPSON · Sampling, Third Edition
THOMPSON · Simulation: A Modeler’s Approach
THOMPSON and SEBER · Adaptive Sampling
THOMPSON, WILLIAMS, and FINDLAY · Models for Investors in Real World
Markets
TIERNEY · LISP-STAT: An Object-Oriented Environment for Statistical
Computing
and Dynamic Graphics
TROFFAES and DE COOMAN · Lower Previsions
TSAY · Analysis of Financial Time Series, Third Edition
TSAY · An Introduction to Analysis of Financial Data with R
TSAY · Multivariate Time Series Analysis: With R and Financial Applications
UPTON and FINGLETON · Spatial Data Analysis by Example, Volume II:
Categorical and Directional Data
† VAN BELLE · Statistical Rules of Thumb, Second Edition
VAN BELLE, FISHER, HEAGERTY, and LUMLEY · Biostatistics: A
Methodology for the Health Sciences, Second Edition
VESTRUP · The Theory of Measures and Integration
VIDAKOVIC · Statistical Modeling by Wavelets
VIERTL · Statistical Methods for Fuzzy Data
VINOD and REAGLE · Preparing for the Worst: Incorporating Downside Risk in
Stock Market Investments 
WALLER and GOTWAY · Applied Spatial Statistics for Public Health Data
*Now available in a lower priced paperback edition in the Wiley Classics Library.
†Now available in a lower priced paperback edition in the Wiley–Interscience Paperback
Series.

WEISBERG · Applied Linear Regression, Fourth Edition
WEISBERG · Bias and Causation: Models and Judgment for Valid Comparisons
WELSH · Aspects of Statistical Inference
WESTFALL and YOUNG · Resampling-Based Multiple Testing: Examples and
Methods for p-Value Adjustment
* WHITTAKER · Graphical Models in Applied Multivariate Statistics
WINKER · Optimization Heuristics in Economics: Applications of Threshold
Accepting
WOODWORTH · Biostatistics: A Bayesian Introduction
WOOLSON and CLARKE · Statistical Methods for the Analysis of Biomedical
Data, Second Edition
WU and HAMADA · Experiments: Planning, Analysis, and Parameter Design
Optimization, Second Edition
WU and ZHANG · Nonparametric Regression Methods for Longitudinal Data
Analysis
YAKIR · Extremes in Random Fields
YIN · Clinical Trial Design: Bayesian and Frequentist Adaptive Methods
YOUNG, VALERO-MORA, and FRIENDLY · Visual Statistics: Seeing Data with
Dynamic Interactive Graphics
ZACKS · Examples and Problems in Mathematical Statistics
ZACKS · Stage-Wise Adaptive Designs
* ZELLNER · An Introduction to Bayesian Inference in Econometrics
ZELTERMAN · Discrete Distributions—Applications in the Health Sciences
ZHOU, OBUCHOWSKI, and MCCLISH · Statistical Methods in Diagnostic
Medicine, Second Edition
*Now available in a lower priced paperback edition in the Wiley Classics Library.
†Now available in a lower priced paperback edition in the Wiley–Interscience Paperback
Series.

WILEY END USER LICENSE AGREEMENT
Go to www.wiley.com/go/eula to access Wiley’s ebook EULA.

