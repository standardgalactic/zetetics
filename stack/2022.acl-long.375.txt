Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
Volume 1: Long Papers, pages 5468 - 5485
May 22-27, 2022 c⃝2022 Association for Computational Linguistics
∞-former: Inﬁnite Memory Transformer
Pedro Henrique MartinsÈ
Zita MarinhoÁç
André F. T. MartinsÈÉÆ
ÈInstituto de Telecomunicações ÁDeepMind çInstitute of Systems and Robotics
ÉLUMLIS (Lisbon ELLIS Unit), Instituto Superior Técnico ÆUnbabel
Lisbon, Portugal
pedrohenriqueamartins@tecnico.ulisboa.pt,
zmarinho@google.com,
andre.t.martins@tecnico.ulisboa.pt.
Abstract
Transformers are unable to model long-term
memories effectively, since the amount of com-
putation they need to perform grows with
the context length.
While variations of efﬁ-
cient transformers have been proposed, they
all have a ﬁnite memory capacity and are
forced to drop old information. In this paper,
we propose the ∞-former, which extends the
vanilla transformer with an unbounded long-
term memory. By making use of a continuous-
space attention mechanism to attend over the
long-term memory, the ∞-former’s attention
complexity becomes independent of the con-
text length, trading off memory length with
precision.
In order to control where pre-
cision is more important, ∞-former main-
tains “sticky memories,” being able to model
arbitrarily long contexts while keeping the
computation budget ﬁxed.
Experiments on
a synthetic sorting task, language modeling,
and document grounded dialogue generation
demonstrate the ∞-former’s ability to retain
information from long sequences.1
1
Introduction
When reading or writing a document, it is impor-
tant to keep in memory the information previously
read or written. Humans have a remarkable ability
to remember long-term context, keeping in mem-
ory the relevant details (Carroll, 2007; Kuhbandner,
2020). Recently, transformer-based language mod-
els have achieved impressive results by increasing
the context size (Radford et al., 2018, 2019; Dai
et al., 2019; Rae et al., 2019; Brown et al., 2020).
However, while humans process information se-
quentially, updating their memories continuously,
and recurrent neural networks (RNNs) update a
single memory vector during time, transformers do
not – they exhaustively query every representation
associated to the past events. Thus, the amount
1The code is available at https://github.com/
deep-spin/infinite-former.
of computation they need to perform grows with
the length of the context, and, consequently, trans-
formers have computational limitations about how
much information can ﬁt into memory. For exam-
ple, a vanilla transformer requires quadratic time to
process an input sequence and linear time to attend
to the context when generating every new word.
Several variations have been proposed to address
this problem (Tay et al., 2020b). Some propose
using sparse attention mechanisms, either with
data-dependent patterns (Kitaev et al., 2020; Vyas
et al., 2020; Tay et al., 2020a; Roy et al., 2021;
Wang et al., 2021) or data-independent patterns
(Child et al., 2019; Beltagy et al., 2020; Zaheer
et al., 2020), reducing the self-attention complexity
(Katharopoulos et al., 2020; Choromanski et al.,
2021; Peng et al., 2021; Jaegle et al., 2021), and
caching past representations in a memory (Dai
et al., 2019; Rae et al., 2019). These models are
able to reduce the attention complexity, and, conse-
quently, to scale up to longer contexts. However, as
their complexity still depends on the context length,
they cannot deal with unbounded context.
In this paper, we propose the ∞-former (inﬁnite
former; Fig. 1): a transformer model extended with
an unbounded long-term memory (LTM), which
allows the model to attend to arbitrarily long con-
texts. The key for making the LTM unbounded
is a continuous-space attention framework (Mar-
tins et al., 2020) which trades off the number
of information units that ﬁt into memory (basis
functions) with the granularity of their represen-
tations. In this framework, the input sequence is
represented as a continuous signal, expressed as
a linear combination of N radial basis functions
(RBFs). By doing this, the ∞-former’s attention
complexity is O(L2 + L × N) while the vanilla
transformer’s is O(L×(L+LLTM)), where L and
LLTM correspond to the transformer input size and
the long-term memory length, respectively (details
in §3.1.1). Thus, this representation comes with
5468

two signiﬁcant advantages: (i) the context can be
represented using a number of basis functions N
smaller than the number of tokens, reducing the
attention computational cost; and (ii) N can be
ﬁxed, making it possible to represent unbounded
context in memory, as described in §3.2 and Fig. 2,
without increasing its attention complexity. The
price, of course, is a loss in resolution: using a
smaller number of basis functions leads to lower
precision when representing the input sequence as
a continuous signal, as shown in Fig. 3.
To mitigate the problem of losing resolution, we
introduce the concept of “sticky memories” (§3.3),
in which we attribute a larger space in the LTM’s
signal to regions of the memory more frequently
accessed. This creates a notion of “permanence” in
the LTM, allowing the model to better capture long
contexts without losing the relevant information,
which takes inspiration from long-term potentiation
and plasticity in the brain (Mills et al., 2014; Bamji,
2005).
To sum up, our contributions are the following:
• We propose the ∞-former, in which we ex-
tend the transformer model with a continuous
long-term memory (§3.1). Since the attention
computational complexity is independent of
the context length, the ∞-former is able to
model long contexts.
• We propose a procedure that allows the model
to keep unbounded context in memory (§3.2).
• We introduce sticky memories, a procedure
that enforces the persistence of important in-
formation in the LTM (§3.3).
• We perform empirical comparisons in a syn-
thetic task (§4.1), which considers increas-
ingly long sequences, in language modeling
(§4.2), and in document grounded dialogue
generation (§4.3). These experiments show
the beneﬁts of using an unbounded memory.
2
Background
2.1
Transformer
A transformer (Vaswani et al., 2017) is composed
of several layers, which encompass a multi-head
self-attention layer followed by a feed-forward
layer, along with residual connections (He et al.,
2016) and layer normalization (Ba et al., 2016).
Let
us
denote
the
input
sequence
as
X = [x1, . . . , xL] ∈RL×e,
where
L
is
the
input size and e is the embedding size of the
attention layer. The queries Q, keys K, and values
V , to be used in the multi-head self-attention
computation are obtained by linearly projecting
the input, or the output of the previous layer, X,
for each attention head h:
Qh = XhW Qh, Kh = XhW Kh, Vh = XhW Vh,
(1)
where W Qh, W Kh, W Vh ∈Rd×d are learnable
projection matrices, d = e/H, and H is the num-
ber of heads. Then, the context representation
Zh ∈RL×d, that corresponds to each attention
head h, is obtained as:
Zh = softmax
QhK⊤
h
√
d

Vh,
(2)
where the softmax is performed row-wise. The
head context representations are concatenated to
obtain the ﬁnal context representation Z ∈RL×e:
Z = [Z1, . . . , ZH]W R,
(3)
where W R ∈Re×e is another projection matrix
that aggregates all head’s representations.
2.2
Continuous Attention
Continuous attention mechanisms (Martins et al.,
2020) have been proposed to handle arbitrary con-
tinuous signals, where the attention probability
mass function over words is replaced by a probabil-
ity density over a signal. This allows time intervals
or compact segments to be selected.
To perform continuous attention, the ﬁrst step
is to transform the discrete text sequence rep-
resented by X ∈RL×e into a continuous signal.
This is done by expressing it as a linear combina-
tion of basis functions. To do so, each xi, with
i ∈{1, . . . , L}, is ﬁrst associated with a position
in an interval, ti ∈[0, 1], e.g., by setting ti = i/L.
Then, we obtain a continuous-space representation
¯X(t) ∈Re, for any t ∈[0, 1] as:
¯X(t) = B⊤ψ(t),
(4)
where ψ(t) ∈RN is a vector of N RBFs, e.g.,
ψj(t) = N(t; µj, σj), with µj
∈[0, 1], and
B ∈RN×e is a coefﬁcient matrix. B is obtained
with multivariate ridge regression (Brown et al.,
1980) so that ¯X(ti) ≈xi for each i ∈[L], which
leads to the closed form (see App. A for details):
B⊤= X⊤F ⊤(FF ⊤+ λI)−1 = X⊤G,
(5)
5469

where F = [ψ(t1), . . . , ψ(tL)] ∈RN×L packs the
basis vectors for the L locations. As G ∈RL×N
only depends of F, it can be computed ofﬂine.
Having converted the input sequence into a con-
tinuous signal ¯X(t), the second step is to attend
over this signal. To do so, instead of having a
discrete probability distribution over the input se-
quence as in standard attention mechanisms (like
in Eq. 2), we have a probability density p, which
can be a Gaussian N(t; µ, σ2), where µ and σ2
are computed by a neural component. A unimodal
Gaussian distribution encourages each attention
head to attend to a single region, as opposed to
scattering its attention through many places, and
enables tractable computation. Finally, having p,
we can compute the context vector c as:
c = Ep
 ¯X(t)

.
(6)
Martins et al. (2020) introduced the continuous
attention framework for RNNs. In the following
section (§3.1), we will explain how it can be used
for transformer multi-head attention.
3
Inﬁnite Memory Transformer
To allow the model to access long-range context,
we propose extending the vanilla transformer with
a continuous LTM, which stores the input embed-
dings and hidden states of the previous steps. We
also consider the possibility of having two mem-
ories: the LTM and a short-term memory (STM),
which consists in an extension of the transformer’s
hidden states and is attended to by the transformer’s
self-attention, as in the transformer-XL (Dai et al.,
2019). A diagram of the model is shown in Fig. 1.
3.1
Long-term Memory
For simplicity, let us ﬁrst assume that the long-
term memory contains an explicit input discrete se-
quence X that consists of the past text sequence’s
input embeddings or hidden states,2 depending on
the layer3 (we will later extend this idea to an un-
bounded memory in §3.2).
First, we need to transform X into a continuous
approximation ¯X(t). We compute ¯X(t) as:
¯X(t) = B⊤ψ(t),
(7)
where ψ(t) ∈RN are basis functions and coef-
ﬁcients B ∈RN×e are computed as in Eq.
5,
2We stop the gradient with respect to the word embeddings
or hidden states before storing them in the LTM.
3Each layer of the transformer has a different LTM.
Masked Self-attention
STM
+
Figure 1: ∞-former’s attention diagram with sequence
of text, Xt, of size L = 2 and STM of size LSTM =
2. Circles represent input embeddings or hidden states
(depending on the layer) for head h and query i. Both
the self-attention and the attention over the LTM are
performed in parallel for each head and query.
B⊤= X⊤G. Then, we can compute the LTM
keys, Kh ∈RN×d, and values, Vh ∈RN×d, for
each attention head h, as:
Kh = BhW Kh,
Vh = BhW Vh,
(8)
where W Kh, W Vh ∈Rd×d are learnable projection
matrices.4 For each query qh,i for i ∈{1, . . . , L},
we use a parameterized network, which takes as
input the attention scores, to compute µh,i ∈]0, 1[
and σ2
h,i ∈R>0:
µh,i =sigmoid

aﬃne
Kh qh,i
√
d

(9)
σ2
h,i =softplus

aﬃne
Kh qh,i
√
d

.
(10)
Then, using the continuous softmax transforma-
tion (Martins et al., 2020), we obtain the probability
density ph,i as N(t; µh,i, σ2
h,i).
Finally, having the value function ¯Vh(t) given
as ¯Vh(t) = V ⊤
h ψ(t), we compute the head-speciﬁc
representation vectors as in Eq. 6:
zh,i = Eph,i[ ¯Vh] = V ⊤
h Eph,i[ψ(t)]
(11)
which form the rows of matrix ZLTM,h ∈RL×d
that goes through an afﬁne transformation,
ZLTM = [ZLTM,1, . . . , ZLTM,H]W O.
The long-term representation, ZLTM, is then
summed to the transformer context vector, ZT , to
obtain the ﬁnal context representation Z ∈RL×e:
Z = ZT + ZLTM,
(12)
which will be the input to the feed-forward layer.
4Parameter weights are not shared between layers.
5470

contraction
+ 
function
evaluation
concatenation
regression
Figure 2: Diagram of the unbounded memory update procedure. This is performed in parallel for each embedding
dimension, and repeated throughout the input sequence. We propose two alternatives to select the positions used
for the function evaluation: linearly spaced or sticky memories.
3.1.1
Attention Complexity
As the ∞-former makes use of a continuous-
space attention framework (Martins et al., 2020)
to attend over the LTM signal, its key matrix
size Kh ∈RN×d depends only on the number
of basis functions N, but not on the length
of the context being attended to.
Thus, the
∞-former’s attention complexity is also indepen-
dent of the context’s length.
It corresponds to
O(L × (L + LSTM) + L × N) when also using
a short-term memory and O(L2 + L × N) when
only using the LTM, both ≪O(L × (L + LLTM)),
which would be the complexity of a vanilla trans-
former attending to the same context. For this rea-
son, the ∞-former can attend to arbitrarily long
contexts without increasing the amount of compu-
tation needed.
3.2
Unbounded Memory
When representing the memory as a discrete se-
quence, to extend it, we need to store the new hid-
den states in memory. In a vanilla transformer, this
is not feasible for long contexts due to the high
memory requirements. However, the ∞-former
can attend to unbounded context without increasing
memory requirements by using continuous atten-
tion, as next described and shown in Fig. 2.
To be able to build an unbounded representation,
we ﬁrst sample M locations in [0, 1] and evaluate
¯X(t) at those locations. These locations can simply
be linearly spaced, or sampled according to the
region importance, as described in §3.3.
Then, we concatenate the corresponding vectors
with the new vectors coming from the short-term
memory. For that, we ﬁrst need to contract this
function by a factor of τ ∈]0, 1[ to make room for
the new vectors. We do this by deﬁning:
Xcontracted(t) = X(t/τ) = B⊤ψ(t/τ).
(13)
Then, we can evaluate ¯X(t) at the M locations
0 ≤t1, t2, . . . , tM ≤τ as:
xm = B⊤ψ(tm/τ),
for m ∈[M],
(14)
and deﬁne a matrix Xpast = [x1, x2, . . . , xM]⊤∈
RM×e with these vectors as rows. After that, we
concatenate this matrix with the new vectors Xnew,
obtaining:
X =
h
Xpast⊤, Xnew⊤i⊤
∈R(M+L)×e.
(15)
Finally, we simply need to perform multivari-
ate ridge regression to compute the new coefﬁ-
cient matrix B ∈RN×e, via B⊤= X⊤G, as in
Eq. 5. To do this, we need to associate the vec-
tors in Xpast with positions in [0, τ] and in Xnew
with positions in ]τ, 1] so that we obtain the matrix
G ∈R(M+L)×N. We consider the vectors posi-
tions to be linearly spaced.
3.3
Sticky Memories
When extending the LTM, we evaluate its current
signal at M locations in [0, 1], as shown in Eq. 14.
These locations can be linearly spaced. However,
some regions of the signal can be more relevant
than others, and should consequently be given a
larger “memory space” in the next step LTM’s sig-
nal. To take this into account, we propose sampling
the M locations according to the signal’s relevance
at each region (see Fig. 6 in App. B). To do so,
we construct a histogram based on the attention
given to each interval of the signal on the previ-
ous step. For that, we ﬁrst divide the signal into
5471

D linearly spaced bins {d1, . . . , dD}. Then, we
compute the probability given to each bin, p(dj)
for j ∈{1, . . . , D}, as:
p(dj) ∝
H
X
h=1
L
X
i=1
Z
dj
N(t; µh,i, σ2
h,i) dt,
(16)
where H is the number of attention heads and L
is the sequence length. Note that Eq. 16’s integral
can be evaluated efﬁciently using the erf function:
Z b
a
N(t; µ, σ2) = 1
2

erf
 b
√
2

−erf
 a
√
2

.
(17)
Then, we sample the M locations at which the
LTM’s signal is evaluated at, according to p. By
doing so, we evaluate the LTM’s signal at the re-
gions which were considered more relevant by the
previous step’s attention, and will, consequently
attribute a larger space of the new LTM’s signal to
the memories stored in those regions.
3.4
Implementation and Learning Details
Discrete sequences can be highly irregular and,
consequently, difﬁcult to convert into a continuous
signal through regression. Because of this, before
applying multivariate ridge regression to convert
the discrete sequence X into a continuous signal,
we use a simple convolutional layer (with stride =
1 and width = 3) as a gate, to smooth the sequence:
˜X = sigmoid (CNN(X)) ⊙X.
(18)
To train the model we use the cross entropy loss.
Having a sequence of text X of length L as input,
a language model outputs a probability distribution
of the next word p(xt+1 | xt, . . . , xt−L). Given a
corpus of T tokens, we train the model to minimize
its negative log likelihood:
LNLL = −
T−1
X
t=0
log p(xt+1 | xt, . . . , xt−L). (19)
Additionally, in order to avoid having uniform
distributions over the LTM, we regularize the con-
tinuous attention given to the LTM, by minimizing
the Kullback-Leibler (KL) divergence, DKL, be-
tween the attention probability density, N(µh, σh),
and a Gaussian prior, N(µ0, σ0).
As different
heads can attend to different regions, we set µ0 =
µh, regularizing only the attention variance, and
get:
LKL =
T−1
X
t=0
H
X
h=1
DKL (N(µh, σh) || N(µh, σ0))
(20)
=
T−1
X
t=0
H
X
h=1
1
2
σ2
h
σ2
0
−log
σh
σ0

−1

.
(21)
Thus, the ﬁnal loss that is minimized corre-
sponds to:
L = LNLL + λKLLKL,
(22)
where λKL is a hyperparameter that controls the
amount of KL regularization.
4
Experiments
To understand if the ∞-former is able to model
long contexts, we ﬁrst performed experiments on a
synthetic task, which consists of sorting tokens by
their frequencies in a long sequence (§4.1). Then,
we performed experiments on language modeling
(§4.2) and document grounded dialogue genera-
tion (§4.3) by ﬁne-tuning a pre-trained language
model.5
4.1
Sorting
In this task, the input consists of a sequence of
tokens sampled according to a token probability
distribution (which is not known to the system).
The goal is to generate the tokens in the decreasing
order of their frequencies in the sequence. One
example can be:
1 2 1 3 1 0 3 1 3 2
|
{z
}
1 occurs 4 times; 3 occurs 3 times, etc.
<SEP> 1 3 2 0
To understand if the long-term memory is being
effectively used and the transformer is not only
performing sorting by modeling the most recent
tokens, we design the token probability distribution
to change over time: namely, we set it as a mixture
of two distributions, p = αp0 + (1 −α)p1, where
the mixture coefﬁcient α ∈[0, 1] is progressively
increased from 0 to 1 as the sequence is generated.
The vocabulary has 20 tokens and we experiment
with sequences of length 4,000, 8,000, and 16,000.
5See App.D for further experiments on language modeling.
5472

4000
8000
16000
Sequence length
30
40
50
60
70
80
90
100
Acc(%)
Transformer-XL
Compressive
-former
0
500
1000
1500
2000
Number of basis functions
70.0
72.5
75.0
77.5
80.0
82.5
85.0
87.5
90.0
Acc (%)
Accuracy
Regression error
0.050
0.075
0.100
0.125
0.150
0.175
0.200
0.225
0.250
Regression error
Figure 3: Left: Sorting task accuracy for sequences of length 4,000, 8,000, and 16,000. Right: Sorting task
accuracy vs regression mean error, when varying the number of basis functions, for sequences of length 8,000.
Baselines.
We consider the transformer-XL6
(Dai et al., 2019) and the compressive transformer7
(Rae et al., 2019) as baselines. The transformer-XL
consists of a vanilla transformer (Vaswani et al.,
2017) extended with a short-term memory which is
composed of the hidden states of the previous steps.
The compressive transformer is an extension of the
transformer-XL: besides the short-term memory, it
has a compressive long-term memory which is com-
posed of the old vectors of the short-term memory,
compressed using a CNN. Both the transformer-XL
and the compressive transformer require relative
positional encodings. In contrast, there is no need
for positional encodings in the memory in our ap-
proach since the memory vectors represent basis
coefﬁcients in a predeﬁned continuous space.
For all models we used a transformer with 3 lay-
ers and 6 attention heads, input size L = 1, 024
and memory size 2,048. For the compressive trans-
former, both memories have size 1,024. For the
∞-former, we also consider a STM of size 1,024
and a LTM with N = 1, 024 basis functions, hav-
ing the models the same computational cost. Fur-
ther details are described in App. C.1.
Results.
As can be seen in the left plot of Fig. 3,
the transformer-XL achieves a slightly higher
accuracy than the compressive transformer and
∞-former for a short sequence length (4,000). This
is because the transformer-XL is able to keep al-
most the entire sequence in memory. However,
its accuracy degrades rapidly when the sequence
length is increased. Both the compressive trans-
6We use the authors’ implementation available at https:
//github.com/kimiyoung/transformer-xl.
7We use our implementation of the model.
former and ∞-former also lead to smaller accura-
cies when increasing the sequence length, as ex-
pected. However, this decrease is not so signiﬁcant
for the ∞-former, which indicates that it is better
at modeling long sequences.
Regression error analysis.
To better understand
the trade-off between the ∞-former’s memory pre-
cision and its computational efﬁciency, we ana-
lyze how its regression error and sorting accuracy
vary when varying the number of basis functions
used, on the sorting task with input sequences of
length 8,000. As can be seen in the right plot of
Fig. 3, the sorting accuracy is negatively correlated
with the regression error, which is positively cor-
related with the number of basis functions. It can
also be observed, that when increasing substantially
the number of basis functions the regression error
reaches a plateau and the accuracy starts to drop.
We posit that the latter is caused by the model hav-
ing a harder task at selecting the locations it should
attend to. This shows that, as expected, when in-
creasing ∞-former’s efﬁciency or increasing the
size of context being modeled, the memory loses
precision.
4.2
Language Modeling
To understand if long-term memories can be used to
extend a pre-trained language model, we ﬁne-tune
GPT-2 small (Radford et al., 2019) on Wikitext-
103 (Merity et al., 2017) and a subset of PG-19
(Rae et al., 2019) containing the ﬁrst 2,000 books
(≈200 million tokens) of the training set. To do
so, we extend GPT-2 with a continuous long-term
memory (∞-former) and a compressed memory
(compressive transformer) with a positional bias,
5473

Wikitext-103
PG19
GPT-2
16.85
33.44
Compressive
16.87
33.09
∞-former
16.64
32.61
∞-former (SM)
16.61
32.48
Table 1: Perplexity on Wikitext-103 and PG19.
based on Press et al. (2021).8
For these experiments, we consider transform-
ers with input size L = 512, for the compressive
transformer we use a compressed memory of size
512, and for the ∞-former we consider a LTM with
N = 512 Gaussian RBFs and a memory threshold
of 2,048 tokens, having the same computational
budget for the two models. Further details and
hyperparameters are described in App. C.2.
Results.
The results reported in Table 1 show that
the ∞-former leads to perplexity improvements on
both Wikitext-103 and PG19, while the compres-
sive transformer only has a slight improvement
on the latter. The improvements obtained by the
∞-former are larger on the PG19 dataset, which
can be justiﬁed by the nature of the datasets: books
have more long range dependencies than Wikipedia
articles (Rae et al., 2019).
4.3
Document Grounded Dialogue
In document grounded dialogue generation, besides
the dialogue history, models have access to a doc-
ument concerning the conversation’s topic. In the
CMU Document Grounded Conversation dataset
(CMU-DoG) (Zhou et al., 2018), the dialogues are
about movies and a summary of the movie is given
as the auxiliary document; the auxiliary document
is divided into parts that should be considered for
the different utterances of the dialogue. In this
paper, to evaluate the usefulness of the long-term
memories, we make this task slightly more chal-
lenging by only giving the models access to the
document before the start of the dialogue.
We ﬁne-tune GPT-2 small (Radford et al., 2019)
using an approach based on Wolf et al. (2019).
To allow the model to keep the whole document
on memory, we extend GPT-2 with a continuous
LTM (∞-former) with N = 512 basis functions.
As baselines, we use GPT-2, with and without ac-
8The compressive transformer requires relative positional
encodings. When using only GPT-2’s absolute positional en-
codings the model gives too much attention to the compressed
memory, and, consequently, diverges. Thus, we adapted it by
using positional biases on the attention mechanism.
PPL
F1
Rouge-1 Rouge-L Meteor
GPT-2 w/o doc
19.43
7.82
12.18
10.17
6.10
GPT-2
18.53
8.64
14.61
12.03
7.15
Compressive
18.02
8.78
14.74
12.14
7.29
∞-former
18.02
8.92
15.28
12.51
7.52
∞-former (SM) 18.04
9.01
15.37
12.56
7.55
Table 2: Results on CMU-DoG dataset.
cess (GPT-2 w/o doc) to the auxiliary document,
with input size L = 512, and GPT-2 with a com-
pressed memory with attention positional biases
(compressive), of size 512. Further details and
hyper-parameters are stated in App. C.3.
To evaluate the models we use the metrics: per-
plexity, F1 score, Rouge-1 and Rouge-L (Lin,
2004), and Meteor (Banerjee and Lavie, 2005).
Results.
As shown in Table 2, by keeping
the whole auxiliary document in memory, the
∞-former and the compressive transformer are
able to generate better utterances, according to
all metrics. While the compressive and ∞-former
achieve essentially the same perplexity in this task,
the ∞-former achieves consistently better scores
on all other metrics. Also, using sticky memo-
ries leads to slightly better results on those metrics,
which suggests that attributing a larger space in the
LTM to the most relevant tokens can be beneﬁcial.
Analysis.
In Fig. 4, we show examples of ut-
terances generated by ∞-former along with the
excerpts from the LTM that receive higher atten-
tion throughout the utterances’ generation. In these
examples, we can clearly see that these excerpts
are highly pertinent to the answers being generated.
Also, in Fig. 5, we can see that the phrases which
are attributed larger spaces in the LTM, when using
sticky memories, are relevant to the conversations.
5
Related Work
Continuous attention.
Martins et al. (2020) in-
troduced 1D and 2D continuous attention, using
Gaussians and truncated parabolas as densities.
They applied it to RNN-based document classi-
ﬁcation, machine translation, and visual question
answering. Several other works have also proposed
the use of (discretized) Gaussian attention for natu-
ral language processing tasks: Guo et al. (2019)
proposed a Gaussian prior to the self-attention
mechanism to bias the model to give higher atten-
tion to nearby words, and applied it to natural lan-
guage inference; You et al. (2020) proposed the use
5474

Cast: Macaulay Culkin as Kevin. Joe Pesci as
Harry. Daniel Stern as Marv. John Heard as Peter.
Roberts Blossom as Marley. 
	
	
	
...
The 
film 
stars 
Macaulay 
Culkin 
as 
Kevin
McCallister, a boy who is mistakenly left behind
when his family flies to Paris for their Christmas
vacation. Kevin initially relishes being home alone,
but soon has to contend with two would-be
burglars played by Joe Pesci and Daniel Stern.
The film also features Catherine O'Hara and John
Heard as Kevin's parents. 
Previous utterance: Or maybe rent, anything is
reason to celebrate..I would like to talk about a
movie called "Home Alone"
Answer: Macaulay Culkin is the main actor and it
is a comedy.
Previous utterance: That sounds like a great
movie. Any more details?
Answer: The screenplay came out in 1990 and
has been on the air for quite a while.
Movie 
Name: 
Home 
Alone. 
Rating: 
Rotten
Tomatoes: 62% and average: 5.5/10, Metacritic
Score: 63/100, CinemaScore: A. Year: 1990. The
McCallister family is preparing to spend Christmas
in Paris, gathering at Peter and Kate's home
outside of Chicago on the night before their
departure. Peter and Kate's youngest son, eight-
year-old Kevin, is being ridiculed by his siblings
and cousins. A fight with his older brother, Buzz,
results in Kevin getting sent to the third floor of the
house for punishment, where he wishes that his
	
	
	
	
	
...
Figure 4: Examples of answers generated by ∞-former on a dialogue about the movie “Home Alone”. The
excerpts from the LTM that are more attended to throughout the utterances generation are highlighted on each
color, correspondingly.
Toy Story: Tom Hanks as Woody   |   animated buddy comedy   |   Toy Story was the first feature length computer animated film   | 
produced by Pixar  | toys pretend to be lifeless whenever humans are present  |  focuses on the relationship between Woody and Gold  | 
fashioned pull string cowboy doll
La La Land: Ryan Gosling  |  Emma Stone as Mia  |  Hollywood  |  the city of Los Angeles  |  Meta critics: 93/100  |  2016  |  During a gig
at a restaurant Sebastian slips into a passionate jazz  |  despite warning from the owner  |  Mia overhears the music as she passes by  | 
for his disobedience  
Figure 5: Phrases that hold larger spaces of the LTM, when using sticky memories, for two dialogue examples (in
App. E).
of hard-coded Gaussian attention as input-agnostic
self-attention layer for machine translation; Dubois
et al. (2020) proposed using Gaussian attention as a
location attention mechanism to improve the model
generalization to longer sequences. However, these
approaches still consider discrete sequences and
compute the attention by evaluating the Gaussian
density at the token positions. Farinhas et al. (2021)
extend continuous attention to multimodal densi-
ties, i.e., mixtures of Gaussians, and applied it to
VQA. In this paper, we opt for the simpler case,
an unimodal Gaussian, and leave sparse and multi-
modal continuous attention for future work.
Efﬁcient transformers.
Several methods have
been proposed that reduce the transformer’s at-
tention complexity, and can, consequently, model
longer contexts. Some of these do so by perform-
ing sparse attention, either by selecting pre-deﬁned
attention patterns (Child et al., 2019; Beltagy et al.,
2020; Zaheer et al., 2020), or by learning these
patterns from data (Kitaev et al., 2020; Vyas et al.,
2020; Tay et al., 2020a; Roy et al., 2021; Wang
et al., 2021). Other works focus on directly re-
ducing the attention complexity by applying the
(reversed) kernel trick (Katharopoulos et al., 2020;
Choromanski et al., 2021; Peng et al., 2021; Jae-
gle et al., 2021). Closer to our approach are the
transformer-XL and compressive transformer mod-
els (Dai et al., 2019; Rae et al., 2019), which extend
the vanilla transformer with a bounded memory.
Memory-augmented language models.
RNNs,
LSTMs, and GRUs (Hochreiter et al., 1997; Cho
et al., 2014) have the ability of keeping a memory
state of the past. However, these require backprop-
agation through time which is impractical for long
sequences. Because of this, Graves et al. (2014),
Weston et al. (2014), Joulin and Mikolov (2015)
and Grefenstette et al. (2015) proposed extending
RNNs with an external memory, while Chandar
et al. (2016) and Rae et al. (2016) proposed efﬁ-
cient procedures to read and write from these mem-
ories, using hierarchies and sparsity. Grave et al.
(2016) and Merity et al. (2017) proposed the use
of cache-based memories which store pairs of hid-
den states and output tokens from previous steps.
The distribution over the words in the memory is
then combined with the distribution given by the
language model. More recently, Khandelwal et al.
(2019) and Yogatama et al. (2021) proposed using
nearest neighbors to retrieve words from a key-
based memory constructed based on the training
data. Similarly, Fan et al. (2021) proposed retriev-
ing sentences from a memory based on the training
data and auxiliary information. Khandelwal et al.
(2019) proposed interpolating the retrieved words
probability distributions with the probability over
the vocabulary words when generating a new word,
while Yogatama et al. (2021) and Fan et al. (2021)
proposed combining the information at the architec-
ture level. These models have the disadvantage of
needing to perform a retrieval step when generating
5475

each token / utterance, which can be computation-
ally expensive. These approaches are orthogonal
to the ∞-former’s LTM and in future work the two
can be combined.
6
Conclusions
In this paper, we proposed the ∞-former: a trans-
former extended with an unbounded long-term
memory. By using a continuous-space attention
framework, its attention complexity is independent
of the context’s length, which allows the model
to attend to arbitrarily long contexts while keep-
ing a ﬁxed computation budget. By updating the
memory taking into account past usage, the model
keeps “sticky memories”, enforcing the persistence
of relevant information in memory over time. Ex-
periments on a sorting synthetic task show that ∞-
former scales up to long sequences, maintaining
a high accuracy. Experiments on language model-
ing and document grounded dialogue generation
by ﬁne-tuning a pre-trained language model have
shown improvements across several metrics.
Ethics Statement
Transformer models that attend to long contexts,
to improve their generation quality, need large
amounts of computation and memory to perform
self-attention. In this paper, we propose an exten-
sion to a transformer model that makes the attention
complexity independent of the length of the con-
text being attended to. This can lead to a reduced
number of parameters needed to model the same
context, which can, consequently, lead to gains in
efﬁciency and reduce energy consumption.
On the other hand, the ∞-former, as well as the
other transformer language models, can be used on
questionable scenarios, such as the generation of
fake news (Zellers et al., 2019), defamatory text
(Wallace et al., 2019), or other undesired content.
Acknowledgments
This work was supported by the European Research
Council (ERC StG DeepSPIN 758969), by the
P2020 project MAIA (contract 045909), by the
Fundação para a Ciência e Tecnologia through
project PTDC/CCI-INF/4703/2021 (PRELUNA,
contract UIDB/50008/2020), by the EU H2020
SELMA project (grant agreement No 957017), and
by contract PD/BD/150633/2020 in the scope of
the Doctoral Program FCT - PD/00140/2013 NET-
SyS. We thank Jack Rae, Tom Schaul, the SAR-
DINE team members, and the reviewers for helpful
discussion and feedback.
References
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. 2016. Layer normalization.
Shernaz X Bamji. 2005. Cadherins: actin with the cy-
toskeleton to form synapses. Neuron.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Proc.
Workshop on intrinsic and extrinsic evaluation mea-
sures for machine translation and/or summarization.
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.
Longformer: The long-document transformer.
Philip J Brown, James V Zidek, et al. 1980. Adaptive
multivariate ridge regression. The Annals of Statis-
tics.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language Models are Few-Shot
Learners. In Proc. NeurIPS.
D.W. Carroll. 2007. Psychology of Language. Cen-
gage Learning.
Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal
Vincent, Gerald Tesauro, and Yoshua Bengio. 2016.
Hierarchical memory networks.
Rewon Child, Scott Gray, Alec Radford, and Ilya
Sutskever. 2019.
Generating long sequences with
sparse transformers.
Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014. On the Properties
of Neural Machine Translation: Encoder–Decoder
Approaches. In Proc. Workshop on Syntax, Seman-
tics and Structure in Statistical Translation.
Krzysztof Choromanski, Valerii Likhosherstov, David
Dohan, Xingyou Song, Andreea Gane, Tamas Sar-
los, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
Lukasz Kaiser, et al. 2021.
Rethinking attention
with performers. In Proc. ICLR (To appear).
Zihang Dai, Zhilin Yang, Yiming Yang, William W
Cohen, Jaime Carbonell, Quoc V Le, and Ruslan
Salakhutdinov. 2019. Transformer-xl: Attentive lan-
guage models beyond a ﬁxed-length context.
In
Proc. ACL.
Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and
Elia Bruni. 2020. Location Attention for Extrapola-
tion to Longer Sequences. In Proc. ACL.
5476

Angela Fan, Claire Gardent, Chloé Braud, and An-
toine Bordes. 2021. Augmenting Transformers with
KNN-Based Composite Memory for Dialog. Trans-
actions of the Association for Computational Lin-
guistics.
António Farinhas, André F. T. Martins, and P. Aguiar.
2021.
Multimodal Continuous Visual Attention
Mechanisms.
Edouard Grave, Armand Joulin, and Nicolas Usunier.
2016. Improving Neural Language Models with a
Continuous Cache. In Proc. ICLR.
Alex Graves, Greg Wayne, and Ivo Danihelka. 2014.
Neural turing machines.
Edward Grefenstette, Karl Moritz Hermann, Mustafa
Suleyman, and Phil Blunsom. 2015.
Learning to
transduce with unbounded memory. Proc. NeurIPS.
Maosheng Guo, Yu Zhang, and Ting Liu. 2019. Gaus-
sian Transformer: A Lightweight Approach for Nat-
ural Language Inference. In Proc. AAAI.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recog-
nition. In Proc. CVPR.
Sepp Hochreiter, J urgen Schmidhuber, and Corso
Elvezia. 1997. Long Short-Term Memory. Neural
Computation.
Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew
Zisserman, Oriol Vinyals, and Joao Carreira. 2021.
Perceiver: General Perception with Iterative Atten-
tion.
Armand Joulin and Tomas Mikolov. 2015. Inferring
algorithmic patterns with stack-augmented recurrent
nets. Proc. NeurIPS.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-
pas, and François Fleuret. 2020. Transformers are
rnns: Fast autoregressive transformers with linear at-
tention. In Proc. ICML.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2019. Generalization
through Memorization: Nearest Neighbor Language
Models. In Proc. ICLR.
Diederik P Kingma and Jimmy Ba. 2015. Adam: A
Method for Stochastic Optimization. In Proc. ICLR.
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.
2020. Reformer: The efﬁcient transformer. In Proc.
ICLR.
Christof Kuhbandner. 2020.
Long-Lasting Verbatim
Memory for the Words of Books After a Single
Reading Without Any Learning Intention. Frontiers
in Psychology.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. Workshop on Au-
tomatic Summarization.
André FT Martins, Marcos Treviso, António Farinhas,
Vlad Niculae, Mário AT Figueiredo, and Pedro MQ
Aguiar. 2020.
Sparse and Continuous Attention
Mechanisms. In Proc. NeurIPS.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017.
Pointer Sentinel Mixture
Models. In Proc. ICLR.
Fergil Mills, Thomas E Bartlett, Lasse Dissing-Olesen,
Marta B Wisniewska, Jacek Kuznicki, Brian A
Macvicar, Yu Tian Wang, and Shernaz X Bamji.
2014.
Cognitive ﬂexibility and long-term depres-
sion (LTD) are impaired following β-catenin stabi-
lization in vivo. In Proc. of the National Academy of
Sciences.
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy
Schwartz, Noah Smith, and Lingpeng Kong. 2021.
Random Feature Attention. In Proc. ICLR (To ap-
pear).
Oﬁr Press, Noah A Smith, and Mike Lewis. 2021.
Train short, test long: Attention with linear biases
enables input length extrapolation.
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018.
Improving language under-
standing by generative pre-training.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Jack W Rae, Jonathan J Hunt, Tim Harley, Ivo Dani-
helka, Andrew Senior, Greg Wayne, Alex Graves,
and Timothy P Lillicrap. 2016.
Scaling memory-
augmented neural networks with sparse reads and
writes. In Proc. NeurIPS.
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar,
Chloe Hillier, and Timothy P Lillicrap. 2019. Com-
pressive Transformers for Long-Range Sequence
Modelling. In Proc. ICLR.
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and
David Grangier. 2021.
Efﬁcient content-based
sparse attention with routing transformers. Transac-
tions of the Association for Computational Linguis-
tics, 9:53–68.
Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-
Cheng Juan. 2020a. Sparse sinkhorn attention. In
Proc. ICML.
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald
Metzler. 2020b. Efﬁcient transformers: A survey.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proc. NeurIPS.
Apoorv Vyas, Angelos Katharopoulos, and François
Fleuret. 2020. Fast transformers with clustered at-
tention. In Proc. NeurIPS.
5477

Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-
ner, and Sameer Singh. 2019. Universal Adversarial
Triggers for Attacking and Analyzing NLP. In Proc.
EMNLP-IJCNLP.
Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun
Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing
Liu. 2021. Cluster-Former: Clustering-based Sparse
Transformer for Question Answering. In Proc. ACL
Findings.
Jason Weston, Sumit Chopra, and Antoine Bordes.
2014. Memory networks.
Thomas Wolf, Victor Sanh, Julien Chaumond, and
Clement Delangue. 2019. Transfertransfo: A trans-
fer learning approach for neural network based con-
versational agents.
Dani Yogatama, Cyprien de Masson d’Autume, and
Lingpeng Kong. 2021.
Adaptive Semiparametric
Language Models. Transactions of the Association
for Computational Linguistics, 9:362–373.
Weiqiu You, Simeng Sun, and Mohit Iyyer. 2020.
Hard-Coded Gaussian Attention for Neural Machine
Translation. In Proc. ACL.
Manzil Zaheer, Guru Guruganesh, Avinava Dubey,
Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,
et al. 2020. Big bird: Transformers for longer se-
quences.
Rowan Zellers,
Ari Holtzman,
Hannah Rashkin,
Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi. 2019.
Defending against neural fake
news. Proc. NeurIPS.
Kangyan Zhou, Shrimai Prabhumoye, and Alan W
Black. 2018.
A Dataset for Document Grounded
Conversations. In Proc. EMNLP.
5478

A
Multivariate ridge regression
The coefﬁcient matrix B ∈RN×e is obtained
with multivariate ridge regression criterion so that
¯X(ti) ≈xi for each i ∈[L], which leads to the
closed form:
B⊤= arg min
B⊤
||B⊤F −X⊤||2
F + λ||B||2
F
(23)
= X⊤F ⊤(FF ⊤+ λI)−1 = X⊤G,
where F = [ψ(t1), . . . , ψ(tL)] packs the basis vec-
tors for L locations and || · ||F is the Frobenius
norm. As G ∈RL×N only depends of F, it can be
computed ofﬂine.
B
Sticky memories
We present in Fig. 6 a scheme of the sticky memo-
ries procedure. First we sample M locations from
the previous step LTM attention histogram (Eq.
16). Then, we evaluate the LTM’s signal at the
sampled locations (Eq. 14). Finally, we consider
that the sampled vectors, Xpast, are linearly spaced
in [0, τ]. This way, the model is able to attribute
larger spaces of its memory to the relevant words.
C
Experimental details
C.1
Sorting
For the compressive transformer, we consider com-
pression rates of size 2 for sequences of length
4,000, from 2 to 6 for sequences of length 8,000,
and from 2 to 12 for sequences of length 16,000.
We also experiment training the compressive trans-
former with and without the attention reconstruc-
tion auxiliary loss.
For the ∞-former we con-
sider 1,024 Gaussian RBFs N(t; ˜µ, ˜σ2) with ˜µ lin-
early spaced in [0, 1] and ˜σ ∈{.01, .05}. We set
τ = 0.75 and for the KL regularization we used
λKL = 1 × 10−5 and σ0 = 0.05.
For this task, for each sequence length, we cre-
ated a training set with 8,000 sequences and valida-
tion and test sets with 800 sequences. We trained
all models with batches of size 8 for 20 epochs on 1
Nvidia GeForce RTX 2080 Ti or 1 Nvidia GeForce
GTX 1080 Ti GPU with ≈11 Gb of memory, using
the Adam optimizer (Kingma and Ba, 2015). For
the sequences of length 4,000 and 8,000 we used a
learning rate of 2.5 × 10−4 while for sequences of
length 16,000 we used a learning rate of 2 × 10−4.
The learning rate was decayed to 0 until the end of
training with a cosine schedule.
C.2
Pre-trained Language Models
In these experiments, we ﬁne-tune the GPT-2 small,
which is composed of 12 layers with 12 attention
heads, on the English dataset Wikitext-103 and on
a subset of the English dataset PG199 containing
the ﬁrst 2,000 books. We consider an input size
L = 512 and a long-term memory with N = 512
Gaussian RBFs N(t; ˜µ, ˜σ2) with ˜µ linearly spaced
in [0, 1] and ˜σ ∈{.005, .01} and for the KL regu-
larization we use λKL = 1 × 10−6 and σ0 = 0.05.
We set τ = 0.5. For the compressive transformer
we also consider a compressed memory of size 512
with a compression rate of 4, and train the model
with the auxiliary reconstruction loss.
We ﬁne-tuned GPT-2 small with a batch size of
1 on 1 Nvidia GeForce RTX 2080 Ti or 1 Nvidia
GeForce GTX 1080 Ti GPU with ≈11 Gb of mem-
ory, using the Adam optimizer (Kingma and Ba,
2015) for 1 epoch with a learning rate of 5 × 10−5
for the GPT-2 parameters and a learning rate of
2.5 × 10−4 for the LTM parameters.
C.3
Document Grounded Generation
In these experiments, we ﬁne-tune the GPT-2
small, which is composed of 12 layers with 12
attention heads, on the English dataset CMU -
Document Grounded Conversations10 (CMU-DoG.
CMU-DoG has 4112 conversations, being the pro-
portion of train/validation/test split 0.8/0.05/0.15.
We consider an input size L = 512 and a long-
term memory with N = 512 Gaussian RBFs
N(t; ˜µ, ˜σ2) with ˜µ linearly spaced in [0, 1] and
˜σ ∈{.005, .01} and for the KL regularization we
use λKL = 1 × 10−6 and σ0 = 0.05. We set
τ = 0.5. For the compressive transformer we con-
sider a compressed memory of size 512 with a
compression rate of 3, and train the model with the
auxiliary reconstruction loss. We ﬁne-tuned GPT-2
small with a batch size of 1 on 1 Nvidia GeForce
RTX 2080 Ti or 1 Nvidia GeForce GTX 1080 Ti
GPU with ≈11 Gb of memory, using the Adam
optimizer (Kingma and Ba, 2015) with a linearly
decayed learning rate of 5 × 10−5, for 5 epochs.
D
Additional experiments
We also perform language modeling experiments
on the Wikitext-103 dataset11 (Merity et al., 2017)
9Dataset available at https://github.com/deepmind/pg19.
10Dataset available at https://github.com/festvox/datasets-
CMU_DoG.
11Dataset available at https://blog.einstein.ai/the-wikitext-
long-term-dependency-language-modeling-dataset/.
5479

sampling
attention histogram
function
evaluation
Figure 6: Sticky memories procedure diagram. The dashed vertical lines correspond to the position of the words
in the LTM signal.
which has a training set with 103 million tokens and
validation and test sets with 217,646 and 245,569
tokens, respectively. For that, we follow the stan-
dard architecture of the transformer-XL (Dai et al.,
2019), which consists of a transformer with 16 lay-
ers and 10 attention heads. For the transformer-XL,
we experiment with a memory of size 150. For
the compressive transformer, we consider that both
memories have a size of 150 and a compression
rate of 4. For the ∞-former we consider a short-
term memory of size 150, a continuous long-term
memory with 150 Gaussian RBFs, and a memory
threshold of 900 tokens.
For this experiment, we use a transformer with
16 layers, 10 heads, embeddings of size 410, and
a feed-forward hidden size of 2100. For the com-
pressive transformer, we follow Rae et al. (2019)
and use a compression rate of 4 and the attention
reconstruction auxiliary loss. For the ∞-former we
consider 150 Gaussian RBFs N(t; ˜µ, ˜σ2) with ˜µ
linearly spaced in [0, 1] and ˜σ ∈{.01, .05}. We
set τ = 0.5 and for the KL regularization we used
λKL = 1 × 10−5 and σ0 = 0.1.
We trained all models, from scratch, with
batches of size 40 for 250,000 steps on 1 Nvidia
Titan RTX or 1 Nvidia Quadro RTX 6000 with
≈24 GPU Gb of memory using the Adam opti-
mizer (Kingma and Ba, 2015) with a learning rate
of 2.5 × 10−4. The learning rate was decayed to 0
until the end of training with a cosine schedule.
STM
LTM
Perplexity
Transformer-XL
150
—-
24.52
Compressive
150
150
24.41
∞-former
150
150
24.29
∞-former (Sticky memories)
150
150
24.22
Table 3: Perplexity on Wikitext-103.
Results.
As can be seen in Table 3, extending the
model with a long-term memory leads to a better
perplexity, for both the compressive transformer
and ∞-former. Moreover, the ∞-former slightly
outperforms the compressive transformer. We can
also see that using sticky memories leads to a some-
what lower perplexity, which shows that it helps
the model to focus on the relevant memories.
Analysis.
To better understand whether ∞-
former is paying more attention to the older mem-
ories in the LTM or to the most recent ones, we
plotted a histogram of the attention given to each
region of the long-term memory when predicting
the tokens on the validation set. As can be seen in
Fig. 7, in the ﬁrst and middle layers, the ∞-former
tends to focus more on the older memories, while
in the last layer, the attention pattern is more uni-
form. In Figs. 8 and 9, we present examples of
words for which the ∞-former has lower perplexity
than the transformer-XL along with the attention
given by the ∞-former to the last layer’s LTM. We
can see that the word being predicted is present sev-
5480

eral times in the long-term memory and ∞-former
gives higher attention to those regions.
To know whether the sticky memories approach
attributes a larger space of the LTM’s signal to
relevant phrases or words, we plotted the memory
space given to each word12 present in the long-
term memory of the last layer when using and not
using sticky memories. We present examples in
Figs. 10 and 11 along with the phrases / words
which receive the largest spaces when using sticky
memories. We can see in these examples that this
procedure does in fact attribute large spaces to old
memories, creating memories that stick over time.
We can also see that these memories appear to be
relevant as shown by the words / phrases in the
examples.
E
Additional examples
In Fig. 12, we show additional examples of utter-
ances generated by ∞-former along with the ex-
cerpts from the LTM that receive higher attention
throughout the utterances’ generation.
Additionally, ground truth conversations con-
cerning the movies “Toy Story” and “La La Land”,
for which the sticky memories are stated in Fig. 5,
are shown in Tables 4 and 5, respectively.
12The (Voronoi) memory space attributed to each word is
half the distance from the previous word plus half the distance
to the next word in the LTM’s signal, being the word’s location
computed based on the sampled positions from which we
evaluate the signal when receiving new memory vectors.
5481

0.0
0.2
0.4
0.6
0.8
1.0
LTM signal (first layer)
0.00
0.02
0.04
0.06
0.08
0.10
p
0.0
0.2
0.4
0.6
0.8
1.0
LTM signal (mid layer)
0.00
0.02
0.04
0.06
0.08
0.10
p
0.0
0.2
0.4
0.6
0.8
1.0
LTM signal (last layer)
0.00
0.02
0.04
0.06
0.08
0.10
p
Figure 7: Histograms of attention given to the LTM by ∞-former, for the ﬁrst (on the left), middle (on the middle),
and last (on the right) layers. The dashed vertical lines represent the limits of the memory segments (τ) for the
various memory updates.
the Pet Shop Boys' synthpop cover of the song (titled
"Where the Streets Have No Name (I Can't Take My Eyes
off You) "). Bono parodied this by occasionally adopting the
deadpan vocal style used in the Pet Shop Boys' cover.
Critics welcomed the song in the group's setlist: The
Independent said the song "induces instant euphoria, as U2
do what they're best at, slipping into epic rock mode,
playing music made for the arena". In two other local
newspaper reviews, critics praised the song's inclusion in a
sequence of greatest hits.
For the PopMart Tour of 1997–1998, U2 returned to the
electronic dance arrangement they occasionally played on
the Zoo TV Tour. The set's massive video screen displayed
a video that
Hot Press described as an "astonishing 2001-style trip into
the heart of a swirling, psychedelic tunnel that sucks the
audience in towards a horizontal monolith". Near the end of
the song, peace doves were shown on the screen and
bright beams of light flanking the set's golden arch were
projected upwards. Hot Press said the effect transformed
the stadium into a "UFO landing site". Shortly before the
third leg of the Elevation Tour, the September 11 attacks
occurred in New York City and Washington D.C.. During the
band's first show in New York City following the attacks, the
band performed "Where the Streets Have No Name", and
when the stage lights illuminated the audience, the band
saw tears streaming down the faces of many fans. The
experience was
one inspiration for the song "City of Blinding Lights". The
band paid tribute to the 9/11 victims during their
performance of the song at the Super Bowl XXXVI halftime
show on 3 February 2002. The performance featured the
names of the September 11 victims projected onto a large
white banner behind the band. U2's appearance was later
ranked number 1 on Sports Illustrated's list of "Top 10
Super Bowl Halftime Shows". For the Vertigo Tour, the
group originally considered dropping the song from their
setlists, but Mullen and Clayton successfully argued against
this. All 131 of the Vertigo Tour concerts featured a
performance of the song, which were accompanied by the
stage's LED video curtains displaying African flags. On the
tour's opening night, this reminded Bono that he had
GT: as the respective audio releases of the latter two concerts, Zoo TV Live and Hasta la Vista Baby! U2
Figure 8: Example of attention given by ∞-former to the last layer’s long-term memory, when predicting the
ground truth word “U2”. The words in the LTM which receive higher attention (> 0.05) are shaded.
and fixed defences, Australia may be made practically
invulnerable". According to Air Force historian Alan
Stephens this paper "in effect, defined the anti-lodgment
concept which has been a persistent feature of RAAF
strategic thinking". 
Headlam, completed a flying instructors course in July 1936
and joined the staff of No. 1 FTS. He was promoted to flight
lieutenant on 1 March 1937. Commencing in July 1938, he
was one of six students to take part in the RAAF's first Long
Specialist Navigation Course, run by Flight Lieutenants Bill
Garing and Alister Murdoch at Point Cook. The course
involved several epic training flights that attracted
considerable media attention, including a twelve-day,
10,800-kilometre (6,700 mi) round-Australia trip by
three Avro Ansons, one of which was piloted by Headlam,
in November. The following month, Headlam led the three
Ansons on a six-day journey back and forth over Central
Australia. He subsequently passed the navigation course
with a special distinction. On 27 January 1939 he was
posted to RAAF Station Laverton, Victoria, as a flight
commander. He served initially with No. 2 Squadron, before
transferring to No. 1 Squadron on 29 August. Both units
operated Ansons.
World War II
Following the outbreak of World War II, No. 1 Squadron
was 
engaged 
in 
convoy 
escort 
and 
maritime
reconnaissance 
duties 
off 
south-eastern 
Australia.
Headlam continued to serve with the squadron as a flight
commander until 15 January 1940, when he was assigned
to Headquarters Laverton
as the station navigation officer. On 27 March he was
posted to the staff of RAAF Headquarters, Melbourne. He
was promoted to squadron leader on 1 June 1940. Two
weeks later he married Katherine Bridge at St Paul's
Anglican Church in Frankston; the couple would have a son
and a daughter. Headlam was given command of No. 2
Squadron at Laverton on 15 April 1941, and raised to wing
commander on 1 July. Equipped with Lockheed Hudsons,
the squadron mainly conducted maritime patrols in
southern waters until 5 December, when four of its aircraft
were ordered to Darwin,  Northern Territory, in response to
fears of Japanese aggression in the Pacific. On 7
December, this detachment established itself at Penfui,
near Koepang in Dutch Timor, while No. 2 Squadron's eight
remaining Hudsons
GT: for the first time on 26 January 1942, and attacked regularly thereafter, damaging some aircraft. The intact Hudsons were withdrawn to Darwin but Headlam
Figure 9: Example of attention given by ∞-former to the last layer’s long-term memory, when predicting the
ground truth word “Headlam”. The words in the long-term memory which receive higher attention (bigger than
0.05) are shaded.
5482

Phrases / words:
"transport gasoline"  |  "American Civil Rigths"  |  "along with Michael"  |  "community center"  |  "residents began to move" |  "Landmarks
Comission"  |  "Meridian Main"  |  "projects"  |  "the historic train station"  |  "Weidmann's Restaurant"  |  "Arts"  |  "Meridian Main Street" 
|  "in late 2007"  |  "effort"  |  "Alliance serves"  |  "Plans were underway"  |  "Building"  |  "Mayor Cheri"  |  "the Alliance"  |  "promote
further development"  |  "assist businesses"  |      "Street program"
Figure 10: Example of the memory space attributed to each word in the last layer’s long-term memory (after 5
updates) without / with the sticky memories procedure, along with the words / phrases which have the largest mem-
ory spaces when using sticky memories (top peaks with space> .005). Excerpt of the sequence being generated in
this example: “Given Meridian’s site as a railroad junction, its travelers have attracted the development of many
hotels.” The dashed vertical lines represent the limits of the memory segments for the various memory updates.
Phrases / words:
"July 1936"  |  "Headlam continued to serve"  | "27 March"  |  "in Frankston"  |  "daugther"  |  "four of its aircraft"  |  "in response to fears of
Japanese"  |  "stationed at Darwin"  |  "attacked the Japanese"  |  "forced it aground"  |  "dispersed at Penfui"  |  "three Japanese
floatplanes"  |  "attacked regularly"  |  "withdrawn to Darwin"  |  "his staff remained at Penfui"  |  "ordered to evacuate"  |  "assistance from
Sparrow Force"  |  "Four of No. 2 Squadron's Hudsons were destroyed"  |  "relocated to Daly Waters" 
Figure 11: Example of the memory space attributed to each word in the last layer’s long-term memory (after
5 updates) without / with the sticky memories procedure, along with the words / phrases which have the largest
memory spaces when using sticky memories (top peaks with space> .005) Excerpt of the sequence being generated
in this example: “Headlam became Ofﬁcer Commanding North-Western Area in January 1946. Posted to Britain
at the end of the year, he attended the Royal Air Force Staff College, Andover, and served with RAAF Overseas
Headquarters, London.” The dashed vertical lines represent the limits of the memory segments for the various
memory updates.
5483

Cast: Jesse Eisenberg as Mark Zuckerberg. Andrew
Garfield as Eduardo Saverin. Justin Timberlake as
Sean Parker. Armie Hammer as Cameron and Tyler
Winklevoss. Max Minghella as Divya Narendra. 
Critical Response: David Fincher's film has the rare
quality of being not only as smart as its brilliant hero,
but in the same way.  It is cocksure, impatient, cold,
exciting and instinctively perceptive. The Social
Network is the movie of the year
	
	
	
...
Previous utterance: So, what movie are we going to
chat about today? Right, the one about Zuckerberg?
Answer: Yep, Jesse Eisenberg plays Zuckerberg.
Previous utterance: So, have you seen it?
Answer: Yeah. Its about the founder of Facebook,
Mark Zuckerberg who was basically dumped by his
girlfriend, Erica, so he created "TheFacebook."
In October 2003, 19-year-old Harvard University
student Mark Zuckerberg is dumped by his girlfriend
Erica Albright. Returning to his dorm, Zuckerberg
writes an insulting entry about Albright on his
LiveJournal blog and then creates a campus website
called Facemash by hacking into college databases to
steal photos of female students, then allowing site
visitors to rate their attractiveness. After traffic to the
site crashes parts of Harvard's computer network, 
	
	
	
...
Figure 12: Examples of answers generated by ∞-former on a dialogue about the movie “The Social Network”.
The excerpts from the LTM that are more attended to throughout their generation are highlighted on each color
correspondingly.
- Hi
- Yo you really need to watch Toy Story. It has 100% on Rotten Tomatoes!
- Really! 100% that’s pretty good What’s it about
- It’s an animated buddy-comedy where toys come to life
- who stars in it
- The main characters are voiced by Tom Hanks and Tim Allen
- does it have any other critic ratings
- Yep, metacritic gave it 95/100 and Cinemascore gave it an A
- how old is it?
- It’s a 1995 ﬁlm so 23 years
- The old ones are always good :) I heard there were some sad parts in it is that true
- Yeah actually, the movie starts off pretty sad as the toys fear that they might be replaced and that they have to move
- is this a disney or dreamworks movie
- Disney, pixar to be exact
- Why do the toys think they will be replaced :(
- they thought so because Andy was having a birthday party and might get new toys
- What part does Tom Hanks play
- Woody, the main character
- How about Tim Allen
- Buzz, the main antagonist at ﬁrst then he becomes a friend
- What kind of toy is Woody?
- A cowboy doll
- What is Buzz
- A space ranger
- so do all the toys talk
- yep! but andy doesn’t know that
- Is andy a little kid or a teen
- He’s 6!
- Sounds good. Thanks for the info. Have a great day
Table 4: Ground truth conversation about movie “Toy Story”.
5484

- hey
- hey
- i just watched la la land. It is a movie from 2016 starring ryan gosling and emma stone. they are too artists (one actress and one
panist) and they fall in love and try to achieve their dreams. its a great movie
- It’s a wonderful movie and got a score of 92% on rotten tomatoes
- yes, i think it also won an oscar
- Yes but I thought it was a little dull
- metacritics rating is 93/100 as well its pretty critically acclaimed
- the two leads singing and dancing weren’t exceptional
- i suppose it is not for everyone
- It also sags badly in the middle I like how Sebastian slipped into a passionate jazz despite warnings from the owner.
- what do you think of the cover of "i ran so far away?" in the movie, sebastian found the song an insult for a serious musician
- I don’t know, it is considered an insult for serious musicians not sure why
- yeah
- The idea of a one woman play was daring
- it was interesting how sebastian joined a jazz fusion band he couldnt ﬁnd real happiness in any of the bands he was in its hard
- It is considering she didn’t know of any of that until she attended one of his concerts
- yeah, that is daring the movie kind of speaks to a lot of people. she accussed him of abandoning his dreams but sometimes thats
what you have to do.
- Not nice that she leaves because he told her she liked him better when he was unsuccessful The play was a disaster so he didn’t
miss anything when he missed it.
- yeah, but i dont blame her for dumping him for that
- She should didn’t want to support him and she had to move back
- id be pretty upset as well to boulder city nevada
- yes she didn’t want to forgive him, I didn’t understand that
- well because that was a big deal to her and he missed it
- if she was with him when he was unsuccessful, she could have supported him to follow his dreams or other dreams
- i suppose that is true
- she wasn’t successful either
- yeah she wasnt nobody showed up to her play
- so why the big hulabaloo about him
- not sure
- she was selﬁsh I guess He missed her play because he had to go for a photo shoot with the band that he had previously missed
- yeah but he should have kept better track and scheduled it better
- this shows that he was trying to commit some and follow his dreams although not necessarily like them so she would be please
if he didn’t attend the photo shoot a second time, and came to her show
- i deﬁnitely felt bad for both of them though in that scene
- it’s more of a do or don’t he is still condemned I feel bad for him because he tried he tried to get her back by apologizing as
well she didn’t want any of it
- yeah because she felt like he didnt care enough because he missed it he’s the one that suggested the one woman play
- They could have started all over again just like the beginning
- maybe so
- did she fail because of the one-woman play? she could have tried something else if she felt that
- she wanted to give it a shot
- she did and it failed, he did and it failed they just had to compromise so they could be together again, which was how the
happiness was He signed up for the band after hearing her talking to her mom about how he is working
- on his career I think he did a lot for her
Table 5: Ground truth conversation about movie “La La Land”.
5485

