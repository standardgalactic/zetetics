Phase-based Modiﬁcation Transfer for Video
Simone Meyer1,2
Alexander Sorkine-Hornung2
Markus Gross1,2
1Department of Computer Science, ETH Zurich
2Disney Research
simone.meyer@inf.ethz.ch
alex@disneyresearch.com
Abstract. We present a novel phase-based method for propagating mod-
iﬁcations of one video frame to an entire sequence. Instead of computing
accurate pixel correspondences between frames, e.g. extracting sparse
features or optical ﬂow, we use the assumption that small motion can be
represented as the phase shift of individual pixels. In order to success-
fully apply this idea to transferring image edits, we propose a correction
algorithm, which adapts the phase shift as well as the amplitude of the
modiﬁed images. As our algorithm avoids expensive global optimization
and all computational steps are performed per-pixel, it allows for a simple
and eﬃcient implementation. We evaluate the ﬂexibility of the approach
by applying it to various types of image modiﬁcations, ranging from
compositing and colorization to image ﬁlters.
Keywords: Phase-based method, video processing, edit propagation.
1
Introduction
Many applications in video processing, e.g., frame interpolation or edit propa-
gation, require some form of explicit correspondence mapping between pixels in
consecutive frames. Common approaches are based on matching sparse feature
points, or dense optical ﬂow estimation. However, ﬁnding a pixel-accurate map-
ping is an inherently ill-posed problem, and existing dense approaches usually
require computationally expensive regularization and optimization.
Recently, a number of novel phase-based video processing techniques have
been proposed that are able to solve certain types of problems without the need
for explicit correspondences. Examples include motion magniﬁcation [20], view
synthesis for autostereoscopic displays [5], or frame interpolation for video [13].
The interesting advantage of such techniques over explicit methods is that they
are based on eﬃcient, local per-pixel operations, which do not require knowledge
about the actual image-space motion of pixels between frames, and hence avoid
the need for solving the above mentioned optimization problems. On the other
hand, the price is that phase-based methods are limited to much smaller motions
between frames than, e.g., methods for sparse feature point matching. However,
given today’s steady increase in video resolution and frame rate, there is also an
increasing need for computationally simple and eﬃcient methods.
In this paper, we extend the range of possible applications for phase-based
techniques. We introduce a method to propagate various types of image modi-
ﬁcations over a sequence of video frames, without the need for explicit tracking

2
S. Meyer, A. Sorkine-Hornung, M. Gross
or correspondences. As previous phase-based approaches, we decompose each
frame of a video sequence using a complex-valued steerable pyramid into local
phase and amplitude information. The key question we then address is how to
adjust both phase and amplitude in this decomposition on subsequent frames
in order to transfer edits made on the ﬁrst frame of a sequence to all other
frames. A particular feature of our method is that it works on textureless or ho-
mogeneous image regions, where explicit tracking approaches often struggle or
require strong regularization. We present various applications of our algorithm,
from adding novel image elements like a logo on a surface and video colorization
to propagation of general image ﬁlters.
2
Related Work
Correspondence-based methods.
Most methods for transferring modiﬁca-
tions from one image to others require some form of explicit correspondences
between the pixels. General approaches for such correspondence estimation tech-
niques range from dense optical ﬂow [1] to tracking of sparse features like SIFT
[11]. Some early work using optical ﬂow for propagation edits is proposed by
Levin et al. [9]. Such methods often require expensive global optimization which
is diﬃcult to implement and parallelize.
Tracking particular image elements is a long-studied problem as well [12,17].
It has been used for propagating image edits in unordered image collections
[7,23] as well as for video [16]. While methods for image collections can handle
large displacements well, they are lacking temporal coherence to avoid artifacts
such as ﬂickering when applied to video. A further limitation of tracking-based
approaches is that they usually require suﬃciently well textured surfaces. More
robust are template-based methods [14] for video editing, as they also work with
minimal texture and deformable surfaces. However, the template image and the
restshape needs to be known in advance, which is usually not the case for general
videos.
Another area related to our work is appearance editing like color manip-
ulation and colorization. In these methods, sparse user edits get propagated
spatially and temporally throughout a video, usually by solving optimization
problems proportional to resolution and number of frames. Recent works there-
fore focused in particular on reducing high computational costs and memory
consumption [2, 10, 21]. Yatagawa et al. [22] propose a method independent of
the total length of the video as it only processes two frames at the time. A gen-
eral approach to ensure temporal consistency for various applications including
optical ﬂow and colorization has been proposed by Lang et al. [8]. Instead of op-
timizing directly for temporal consistency, Bonneel et al. [3] propose a method
to restore temporal consistency after a ﬁlter operation has been applied to each
frame of a video independently. This method also uses optical ﬂow as guidance
and assumes that the ﬁlter does not generate new content uncorrelated to its
input. In contrast to such approaches, the advantage of our method is that it

Phase-based Modiﬁcation Transfer for Video
3
allows the propagation of global modiﬁcations without the knowledge of the
particular used image ﬁlter.
Our correspondence-free, phase-based method is designed such that it can
handle appearance editing, e.g. local recolorization and global changes by an
image ﬁlter, as well as detailed edits such as adding novel image elements on a
surface, given that image motion between video frames is small.
Phase-based methods.
Recent works have shown that it is possible to use a
phase representation of the motion between frames for various applications [5,20]
that usually require explicit correspondences. Such a phase-based representation
allows for eﬃcient computation as only per-pixel modiﬁcations are required. As a
drawback, they are limited to small motions between the frames. Eﬀort has been
put into extending the motion range, e.g., by combining it again with tracking
or optical ﬂow [6] or by computing a disparity map [24]. A purely phase-based
approach to extend the range of admissible motion for video frame interpolation
has been proposed by Meyer et al. [13].
All these methods have been used for applications that modify or interpolate
the unmodiﬁed input frames. Complementary, we extend the set of phase-based
applications by a method to propagate edits of modiﬁed frames, which can sig-
niﬁcantly diﬀer from the input frames on which the phase information has been
originally computed.
3
Motion as Phase Shift
Phase-based methods use the intuition that the motion of certain signals or
functions can be represented as a shift of their phase. In this section we ﬁrst
explain the basic mathematical justiﬁcation for the 1D case as well as derive the
consequences and challenges when using it for propagating a modiﬁed signal.
1D case.
The Fourier Shift Theorem motivates the assumption that some
small displacement motion can be encoded using phase diﬀerences. In the one
dimensional case, a function f(x) can be represented in the Fourier domain as
a sum of complex sinusoids over all frequencies ω:
f(x) =
ω=+∞
X
ω=−∞
Aωeiωx =
ω=+∞
X
ω=−∞
Aωeiφω ,
(1)
where Aω and φω represent the amplitude and the phase, respectively. The
shifted version of f(x) by a displacement function δ(t) is then deﬁned as:
f(x −δ(t)) =
ω=+∞
X
ω=−∞
Aωeiω(x−δ(t)) .
(2)
The phase diﬀerence between the original and the shifted function
φω
diﬀ= ωx −ω(x −δ(t)) = ωδ(t)
(3)

4
S. Meyer, A. Sorkine-Hornung, M. Gross
(a)
(b)
Fig. 1: Left: Given a simple sinusoidal input function (blue) and its translated
version (red), a modiﬁcation of the input (cyan) can be translated using the
phase diﬀerence of the unmodiﬁed functions (Equation 4) (orange, right).
(a)
(b)
Fig. 2: For less trivial modiﬁcations of the input function, e.g., adding an addi-
tional frequency (left), transferring the modiﬁcation using only the known phase
diﬀerence (orange solid) does not correspond to the actually required, but gen-
erally unknown frequency dependent phase shift (orange dotted).
encodes the frequency-dependent version of the spatial displacement δ(t). In the
context of phase-based methods δ(t) is also referred to as phase shift φshift.
For propagating modiﬁcations we are interested in using this phase diﬀerence
to translate a modiﬁed input function ˆf(x). Below we describe the challenges
that arise from the fact that the modiﬁed function does not have the same
frequency decomposition anymore as the original input function.
Challenges.
Consider the example in Figure 1a, where as an input we are
given a function in the form of f(x) = A sin(ωx −φ) (blue). In our targeted
application scenario this would correspond to a reference video frame. We also
have a modiﬁcation (cyan), and a translated version of the function (red), which
corresponds to the following video frame that we want to propagate the mod-
iﬁcation to. In this simple example the translation is described by subtracting
π/4 from the phase and the modiﬁcation consists of replacing the old amplitude
with a new amplitude ˆA = 2. We can compute the translation of the modiﬁed
function (Figure 1b, orange) by subtracting the phase diﬀerence:
ˆf(x) = ˆA sin(ωx −φdiﬀ) = ˆA sin(ω(x −φshift)) .
(4)

Phase-based Modiﬁcation Transfer for Video
5
However, for handling less trivial modiﬁcations, e.g., adding new frequencies,
we have to decompose the function according to the frequencies and estimate
the necessary phase diﬀerence for each frequency separately. In our example in
Figure 2a this corresponds to the fact that we know the phase diﬀerence for the
input frequency ω = 1 but not for the added function with ω = 2. This leads us
to the main challenge of using phase diﬀerences to transfer modiﬁcations:
How can we transfer novel frequency content of a modiﬁed function that has
not been present in the two unmodiﬁed input functions?
To solve this problem, we need an algorithm that detects which frequencies
have been added in the modiﬁed function, and which frequencies in the original
input functions represent the relevant motion. Besides addressing this central
question, we also resolve some additional, less obvious issues that arise when
performing phase-based modiﬁcation transfer on video sequences.
4
Our Algorithm
4.1
Decomposition
For images rather than 1D functions, we ﬁrst need to generalize the idea to two
dimensions, where we can separate the sinusoids according to frequency ω and
spatial orientation θ using, e.g., the complex-valued steerable pyramid [15,18,19].
This step is essentially identical to previous phase-based approaches [5, 13, 20]
and we describe it only brieﬂy for completeness. The steerable pyramid ﬁlters
Ψω,θ resemble Gabor wavelets and decompose an input image I into oriented
frequency bands Rω,θ:
Rω,θ(x, y) = (I ∗Ψω,θ)(x, y)
(5)
= Cω,θ(x, y) + i Sω,θ(x, y)
(6)
= Aω,θ(x, y) eiφω,θ(x,y) ,
(7)
where Cω,θ is the cosine part, representing the even-symmetric ﬁlter response,
and Sω,θ is the sine part, representing the odd-symmetric ﬁlter response. By
using such quadrature ﬁlter pairs we can compute the amplitude
Aω,θ(x, y) =
q
Cω,θ(x, y)2 + Sω,θ(x, y)2 ,
(8)
and the phase values
φω,θ(x, y) = arctan(Sω,θ(x, y)/Cω,θ(x, y)) .
(9)
For our following algorithm it is important to note that this provides a decom-
position with ﬁlter responses that are deﬁned in the spatial domain and have
local support, providing per-(multi-resolution)-pixel oriented phase and ampli-
tude values. Please see [13, 20] for a more detailed derivation. In general, this
decomposition allows the computation of phase diﬀerences and amplitudes at
various scales and orientations, which is the key element of phase-based meth-
ods. Establishing and appropriately using the relationships between diﬀerent
decompositions and across levels is the key element of our algorithm.

6
S. Meyer, A. Sorkine-Hornung, M. Gross
4.2
Overview
Given the above decomposition for two input images, It−1 and It, as well as for a
modiﬁed image ˆIt−1, our algorithm allows us to recover the unknown, translated
version of the modiﬁed input ˆIt.
To reconstruct ˆIt, we need to approximate its ﬁlter responses ˆAt
ω,θ and ˆφt
ω,θ
based on the available information. The resulting image is then obtained by
integrating the modiﬁed responses according to Equation 1. Where clear from
the context we omit the indices ω and θ in the equations for improved readability.
In general, all computations are done for each pixel at each level and orientation.
Using again the assumption that small motion is encoded in the phase shift,
we can use the phase diﬀerence φdiﬀbetween the phases of the unmodiﬁed
images It−1 and It, i.e.,
φt−1,t
diﬀ
= atan2(sin(φt−1 −φt), cos(φt−1 −φt)) ,
(10)
as an initial approximation of the motion. Due to the circular property of the
phase values we use the four-quadrant inverse tangent atan2 to get angular phase
values between [−π, π].
The phase of the modiﬁed input image can then be translated by subtracting
the phase diﬀerence from its own phase. Assuming that the motion is small
enough such that only the phase is aﬀected, one could try to simply copy the
amplitude values, i.e:
ˆφt = ˆφt−1 −φt−1,t
diﬀ
,
(11)
ˆAt = ˆAt−1 .
(12)
However, as explained in Section 3, and illustrated in Figure 1 and 2, this
only works when the modiﬁcations do not change the frequency content. Us-
ing this initial solution can lead to incomplete or even wrong propagation of
some frequency levels. This is the case at locations where the amplitude of the
modiﬁed image ( ˆAt−1) is large but not the amplitudes of the unmodiﬁed images
(At−1, At). The smaller the amplitude (as a result of weak ﬁlter responses in
smooth areas), the more noisy are usually the phase values. Artifacts arise in
particular when noisy phase values are used for trying to propagate modiﬁca-
tions from one image another, and get magniﬁed due to larger corresponding
amplitudes of the modiﬁed image. Locations with large amplitude values cor-
respond to strong ﬁlter responses which have more inﬂuence on the ﬁnal pixel
value. Therefore it is important that the corresponding phase values are com-
puted carefully.
We propose an extension to this initial solution in order to handle general
modiﬁcations, which may alter the decomposition signiﬁcantly. Furthermore,
we are not only interested in propagating the modiﬁcation to one additional
frame but to a whole image sequence. Figure 3 provides an overview of our
general procedure. Algorithm 1 provides a summary of the core steps of our
algorithm. In order to solve the challenges stated above we have to solve two main
tasks: First, determine the pixels per frequency band which contain information

Phase-based Modiﬁcation Transfer for Video
7
Fig. 3: Illustration of the general
procedure. The pyramid decom-
position of the input, two unmod-
iﬁed images, I0 and I1, as well as a
modiﬁed image ˆI0, are processed
by our algorithm to generate the
translated modiﬁed image ˆI1. By
repeating this to the next set of
images, the whole sequence can be
processed.
Algorithm 1 The inputs are two unmodi-
ﬁed images I0 and I1 and a modiﬁed image
ˆI0. The output is the modiﬁed image ˆI1. Pi
are the steerable pyramid decompositions
consisting of Ai and φi.
(P0, P1, ˆP0) ←decompose(I0, I1, ˆI0)
▷[15]
φdiﬀ←phaseDiﬀerence(φ0, φ1)
▷Eq. 10
(A0, A1, ˆA0) ←normalize(A0, A1, ˆA0)
ϕ1 ←signiﬁcantMotion(A0, ˆA0)
▷Eq. 13
ϱ1 ←relevantMotion(A0, A1)
▷Eq. 17
for all l = L −1 : 1 do
ˆφl
1 ←compute(ˆφ0, φdiﬀ, ϕ1, ϱ1) ▷Eq. 20
end for
ˆI1 ←reconstruct( ˆA0, ˆφ1)
ˆP1 ←decompose(ˆI1)
ˆA1 ←correctAmpl( ˆA0, ˆA1, ˆφ1)
▷Sec. 4.5
ˆI1 ←reconstruct( ˆP1)
▷See [15]
about the motion and those which have been changed due to the modiﬁcation.
Secondly, using this information to approximate the missing information in order
to propagate the modiﬁcations to succeeding frames. As a guide we can use the
amplitude information as larger amplitudes are the result of strong and reliable
ﬁlter responses. In the following we explain the core algorithmic steps.
4.3
Detecting Missing Phase Information
Detecting the locations where we have new frequency content with unknown
motion is the ﬁrst central step in our algorithm. In principle we of course know
exactly which pixels in the input image have been modiﬁed. However, it is im-
portant to consider which modiﬁcations result in actual frequency and phase
changes, and on which decomposition level these changes happen.
Therefore we perform the detection process in two steps: We ﬁrst detect
pixels with signiﬁcant modiﬁcations, and then decide whether the corresponding
phase diﬀerence between the unmodiﬁed signals represent the motion. To guide
this detection process we employ the available amplitude information, which
indicates how strong the response at a speciﬁc pixel location is.
Amplitude normalization.
Before we can use the amplitude as a guide
we need to normalize the values across the levels such that they are scale-
independent and comparable. Because we are downsampling the image during
the pyramid decomposition the amplitudes have to be rescaled by the scaling

8
S. Meyer, A. Sorkine-Hornung, M. Gross
factor of the pyramid decomposition λ, i.e., A(l, x, y) ←
A(l,x,y)
λl−1 , with l = 1
being the topmost, i.e. ﬁnest, level of the pyramid.
Identiﬁcation of signiﬁcant modiﬁcations.
In general, not all modiﬁca-
tions result in a signiﬁcant change on a speciﬁc frequency level. Signiﬁcant means
in our case that the modiﬁcation results in large amplitude values compared
to the amplitude values of the unmodiﬁed image. Without postprocessing (see
next paragraph) possibly noisy phase values will be used. As a ﬁrst idea to
identify signiﬁcant modiﬁcations one could use the diﬀerence between the am-
plitudes of the unmodiﬁed and modiﬁed image, i.e. | ˆAt−1(x, y) −At−1(x, y)| as
a measurement on how much a pixel has changed. In order to get a relative
measurement between all pixels and a deﬁnition of signiﬁcance, we need to nor-
malize the diﬀerences. Using the absolute diﬀerence and a ﬁxed threshold, i.e.
| ˆAt−1(x, y) −At−1(x, y)| > ϑ would be feasible for a single image pair, but as
we are interested in propagating the edits over a whole image sequence we need
a more robust measurement.
Propagating phase information over several images results in diminishing
response and therefore inevitably leads to smaller amplitudes and loss of infor-
mation for the modiﬁed image. This reduction of the amplitude is shown for
one time step in Figure 4c (left). As a general solution we therefore propose to
standardize the distribution of amplitude diﬀerences:
ϕt(At−1(x, y), ˆAt−1(x, y)) = | ˆAt−1(x, y) −At−1(x, y)| −µt
σt
,
(13)
where µt represents the sample mean over all pixels, orientations and scales
µt = 1
N
X
x,y
| ˆAt−1(x, y) −At−1(x, y)|
(14)
and σt the sample standard deviation
σt =
s
1
N −1
X
x,y
(| ˆAt−1(x, y) −At−1(x, y)| −µt)2 .
(15)
N is the number of all pixels over all orientations and scales. Although the am-
plitude diﬀerences are technically not normal distributed (only positive values,
with a peak close to 0) experiments have shown that the concluded criterion
together with a threshold τϕ independent of t
ϕt(At−1(x, y), ˆAt−1(x, y)) > τϕ
(16)
allows for a robust identiﬁcation of signiﬁcant modiﬁcations.
Estimation of relevant motion.
Until now we have only compared the un-
modiﬁed image It−1 with the modiﬁed one ˆIt−1. In order to estimate how to use
existing phase diﬀerences for edit propagation, we have to identify useful motion
information between the two unmodiﬁed images It−1 and It.

Phase-based Modiﬁcation Transfer for Video
9
I
x
(a) Propagation of edits without phase and
amplitude correction introduces high fre-
quency artifacts and decreases quality.
(b) Closeup left without, right with our
proposed correction algorithm. Note the
reduced low and high frequency artifacts.
I
x
I
x
I
x
(c) The one dimensional signals illustrate the three intermediate steps of our algo-
rithm improving the result (orange) from left to right: Correcting phase diﬀerence at
all locations where signiﬁcant modiﬁcations has been detected. Applying correction
only where necessary, i.e. no relevant motion information is available. Postprocessing
the amplitude to recover details. Our ﬁnal result.
Fig. 4: Improvement of our algorithm for applications which changes the fre-
quency content. The blue and red curve correspond to the unmodiﬁed input
signals at two consecutive time steps. The cyan curve corresponds to the modi-
ﬁcation on the blue input curve, and the orange curve represents our result.
As a consequence of the downsampling, any modiﬁcation aﬀects the ampli-
tude on the lower levels. On the other hand, the low frequency levels correspond
to the general, more global image motion that we are interested in. We therefore
distinguish pixels with signiﬁcant modiﬁcations into two cases: either the corre-
sponding phase diﬀerence already captures the relevant motion or not. Only in
the second case we need to adjust the phase diﬀerences for better propagation.
For reliable motion (i.e., phase diﬀerence) estimation we require reliable phase
information in both unmodiﬁed input images, which, in turn, depends on the
relative strength of the respective amplitudes compared to other pyramid levels.
We therefore measure pixels with relevant phase information using:
ϱt(At−1(x, y), At(x, y)) = min(At−1(x, y), At(x, y)) −µt
σt
,
(17)
where µt and σt are the mean, respectively the standard deviation, of the
min(At−1(x, y), At(x, y)) samples. Pixels with ϱt larger than some threshold τϱ
ϱt(At−1(x, y), At(x, y)) > τϱ
(18)
are deﬁned to have a relevant motion.
The combination of these two criteria, Equation 16 and 18, deﬁne where
we are missing relevant information, i.e., where we have signiﬁcant change in
amplitude information and no reliable motion information. This allows us to

10
S. Meyer, A. Sorkine-Hornung, M. Gross
deﬁne an indicator function in which areas an adaption of the phase information
is required in order to achieve phase-based edit propagation:
IA(x, y) = (ϕt > τϕ) ∧(ϱt < τϱ) .
(19)
The thresholds are independent of t and can be ﬁxed for an image sequence.
4.4
Correction of Phase Diﬀerences
After having detected the locations where we are missing necessary phase diﬀer-
ence information, we need to ﬁll them in with values representing the required
motion. Due to the change of frequency content this corresponds to inferring
phase diﬀerences for frequencies which do not already exist in the input data.
To approximate them we use the available information given by the pyramid de-
composition. Due to the fact that the complex steerable pyramid is translation-
invariant, we can assume that the frequency bands move in a similar way. In
addition, we already know the relevant phase diﬀerences (Equation 18), i.e. the
relevant motion of the unmodiﬁed image pair. Therefore, in our correction algo-
rithm, we substitute missing phase diﬀerences by a reliable phase diﬀerence from
the closest lower level, denoted as k. To propagate the chosen phase diﬀerence
φk
diﬀto the current level, we need to multiply it with the scale factor of the
pyramid λ accordingly. At all other locations we can use the computed phase
diﬀerence to translate the phase of the modiﬁed input image:
ˆφl
t(x, y) =
(ˆφl
t−1 −λk−lφk
diﬀ
if IA(x, y) = 1,
ˆφl
t−1 −φl
diﬀ
otherwise.
(20)
4.5
Correction of Amplitudes
Although the above algorithm improves the results there is still the problem of
the diminishing response in the amplitude, see Figure 4c, which manifests in
images as increasing blur. One reason is the propagation of phase information
from pyramid levels with lower resolution, which can result in a loss of sharpness
of details. Secondly we assume that the motion is only captured in the phase,
and the amplitude remains the same. The resulting artifacts such as ringing and
blurriness are mainly visible at high frequency details such as edges. As we want
to avoid the computation of any explicit correspondences which would allow to
move the amplitude, we propose the following algorithm to recover some of the
details by using only per pixel modiﬁcations.
By comparing the decomposition of the newly synthesized modiﬁed image ˆIt
with the unmodiﬁed image It at the same time step we can detect how much
the amplitudes have changed due to the modiﬁcation. The idea is to increase
the amplitude of the modiﬁed image where necessary using a speciﬁc transfer
function. At locations where the new amplitude is large, it is probably not as
large as it should be. Because a linear transfer function unnecessarily enhances
small amplitudes, we propose a sigmoid function. To get an estimation on how

Phase-based Modiﬁcation Transfer for Video
11
(a) Input and modiﬁcation, t = 0
(b) Input detail, t = 20 (unmodiﬁed)
(c) Blur without amplitude correction
(d) With correction of amplitudes
Fig. 5: Processing the amplitude helps to recover some of the details and reduces
blur, but can also magnify artifacts such as ringing. In order to increase the
visibility of the eﬀect, the modiﬁcation has been propagated over t = 20 frames.
much energy in terms of the amplitudes has been lost, we use the amplitude of
the previous modiﬁed image, i.e. max( ˆAt−1)/ max( ˆAt). The proposed transfer
function η( ˆAt(x, y)) maps the input range of [0, max( ˆAt)] to the range of the
magniﬁcation factor
h
1, max( ˆ
At−1)
max( ˆ
At)
i
:
η( ˆAt(x, y)) =
max

max( ˆ
At−1)
max( ˆ
At) −1, 0

1 +

ˆ
At(x,y)
α max( ˆ
At)+ϵ
−β
+ 1 ,
(21)
where β deﬁnes the steepness of the curve, α max( ˆAt) the midpoint of the transi-
tion and ϵ = 10−4 is used to avoid division by 0. The maximal amplitude values
are computed for each oriented frequency band separately. The advantages of
this approach are demonstrated in Figure 5.
5
Results
We demonstrate the ﬂexibility of our method by applying it to various kinds
of video editing operations, ranging from appearance changes to detailed edits
including adding new image content. More detailed results can be found in the
supplementary video on our project webpage.
Edit image appearance.
The appearance of an image can be modiﬁed either
locally or globally by changing its color or applying a ﬁlter. Modiﬁcations which
only change the color of an image are easier to propagate as they do not change
the frequency content signiﬁcantly. In these cases we do not have to correct the
phase diﬀerence to adapt for changing frequencies. But our proposed correction
of the amplitude is still a necessary step for the quality of the results as shown
in Figure 5. Figure 6 shows the result of a local recolorization, while Figure 7
shows the propagation result of applying an artistic ﬁlter operation.

12
S. Meyer, A. Sorkine-Hornung, M. Gross
(a) Input and modiﬁcation, t = 0
(b) Input detail, t = 0 (modi-
ﬁed) and t = 10 (unmodiﬁed)
(c) Our result,
t = 10
Fig. 6: Propagation result for local recolorization. A longer sequence of this ex-
ample can be found in the supplementary video.
(a) Input and modiﬁcation, t = 0
(b) Input detail, t = 0 (mod-
iﬁed) and t = 5 (unmodiﬁed)
(c) Our result,
t = 5
Fig. 7: Propagation result for applying a ﬁlter to the ﬁrst image. In this case an
artistic ﬁlter has been used, which adds an artistic blur to the image.
Adding image content.
Due to our detection and correction algorithm we
can also propagate image edits which signiﬁcantly change the frequency content,
see Figure 8. Furthermore, our method is especially suitable to handle edits on
homogeneous, textureless surfaces, see Figure 9.
As the ﬁlter responses used in our method have local support, the method
can also be applied to scenes with additional moving objects. Figure 9 shows
such an example where a moving action has been captured while the hand-held
camera was subject to small motion. Because in this case the motion between
any frame and the ﬁrst frame is suﬃcient small, we can apply our algorithm
to process the current frame relatively to the ﬁrst one. This avoids potential
artifacts due to incremental propagation.
Furthermore, as the modiﬁcations only happen locally in a more or less static
area, they can be localized easily, e.g. by using an approximate bounding box
to the diﬀerence image of the unmodiﬁed/modiﬁed image pair. The propagation
then only has to be applied to this area while the rest of the image can be
substituted by the corresponding original frame of the input video sequence.
This avoids potential artifacts in unmodiﬁed areas.
Qualitative comparisons.
The advantage of our method is that it is ap-
plicable to general modiﬁcations independent on whether they contain local or
global modiﬁcations of the input image. The methods mentioned in the related
work section are optimized for speciﬁc use cases. In order to compare our results
visually to correspondence based methods we use a general approach consist-
ing of computing the optical ﬂow ﬁeld [4] and using it to warp the modiﬁed
image. Figure 8 shows that we obtain visually similar results. Because optical

Phase-based Modiﬁcation Transfer for Video
13
(a) Input, t = 0
(b) Optical ﬂow [4]
(c) Our result
Fig. 8: Propagation result (t = 10) for adding image content on a wall and
comparison to using optical ﬂow based propagation.
(a) Input, t = 0
(b) Moving shadow, t = 0 and t = 8
(c) Without our correction, t = 8
(d) Our result, t = 8
Fig. 9: Propagation result for a video with small camera motion while there is an
additional motion happening, in this case a moving shadow on the wall. Due to
the locality of the ﬁlter responses the camera motion and the additional motion
can be processed separately.
ﬂow based approaches use explicit matching they introduce less blur and can
naturally handle longer propagation sequences better, see Figure 10.
Implementation details.
Our proposed phase-based approach has a few pa-
rameters. One set for controlling the pyramid decomposition, the other for the de-
scribed phase and amplitude correction algorithm. The parameters for the pyra-
mid decomposition are a tradeoﬀbetween separability and localization. Smaller
frequency bands are better for separation but have a larger spatial support.
Regarding the correction algorithm, experiments have shown that we obtain fa-
vorable results for a wide set of parameter choices. For the results in this paper
we have used a ﬁxed set of values: For constructing the pyramid we used #θ = 8
number of orientations, a scale factor λ = 1.2, and the number of levels is deter-
mined such that the coarsest level has a minimal dimension of 10 pixels. For the
correction of the phase diﬀerence we have used τϕ = τϱ = 3. The function for
correcting the amplitude has been deﬁned with β = 8 and α = 0.1. Similar to
previous phase-based methods, frequency content which has not been captured
in the pyramid levels and is summarized in real valued high- and low-pass resid-
ual needs to be treated specially. As we have no motion information available for
these two residuals, we just use as an approximation the low-pass residual of the
modiﬁed image ˆIt−1 and ignore the high-pass residual. As the high-pass residual

14
S. Meyer, A. Sorkine-Hornung, M. Gross
(a) Input, t = 0
(b) Optical ﬂow [4] (c) Naive approach
(d) Our result
Fig. 10: The input image has been modiﬁed by adding a patch of Perlin noise to a
ﬂat, textureless wall. Our correction algorithm improves the propagation results,
but propagation over several frames still lead to increased blur, see comparison
with optical ﬂow based propagation. Results are shown at t = 1 and t = 10.
contains high frequency details, adding it without considering motion would po-
tentially add artifacts. We share this open research question with previous works
on phase-based methods.
Discussion and limitations.
While our method provides a novel and eﬃ-
cient alternative to traditional edit propagation algorithms using optical ﬂow
and tracking, it has some limitations. The diﬃculties lie in propagating high
frequencies. As the phase-based encoding of the motion is only an approxima-
tion we lose sharpness in each propagation step. Additionally, the blurring gets
increased by our multi-scale approach, as we are using the motion of lower lev-
els which do not contain the same level of details as the higher levels to which
the information gets propagated. Furthermore, sharp edges can cause ringing
artifacts. Our current transfer function, used for correcting the amplitude and
recovering details, can not distinguish between correct details and these artifacts.
As a result, these get incorrectly ampliﬁed as well, see Figure 5. In general, arti-
facts such as ringing and blurriness become more visible the further the edits get
propagated resulting in a degeneration of quality. But our algorithm still enables
an improved phase-based propagation, see Figure 10.
6
Conclusions
We presented a novel approach for correspondence-free modiﬁcation transfer for
video, extending the existing range of video processing operations possible using
purely phase-based approaches. We believe that, in particular in the context of
the steady increase in video frame rate and resolution, phase-based approaches
provide an interesting and eﬃcient alternative to traditional approaches that
require explicit frame-to-frame correspondences.
The recently regained interest in phase-based methods has opened up a num-
ber of surprising applications that were believed impossible before. We think
that such methods bear potential for many more interesting research and ap-
plications, and hope that our work provides a new step in such a direction. For
more immediate directions of future improvements and research the discussed
limitations of our method provide various opportunities.

Phase-based Modiﬁcation Transfer for Video
15
References
1. Baker, S., Scharstein, D., Lewis, J.P., Roth, S., Black, M.J., Szeliski, R.: A database
and evaluation methodology for optical ﬂow. IJCV 92(1), 1–31 (2011)
2. Bie, X., Huang, H., Wang, W.: Real time edit propagation by eﬃcient sampling.
Comput. Graph. Forum 30(7), 2041–2048 (2011)
3. Bonneel, N., Tompkin, J., Sunkavalli, K., Sun, D., Paris, S., Pﬁster, H.: Blind video
temporal consistency. ACM Trans. Graph. 34(6), 196 (2015)
4. Brox, T., Bruhn, A., Papenberg, N., Weickert, J.: High accuracy optical ﬂow esti-
mation based on a theory for warping. In: ECCV. pp. 25–36 (2004)
5. Didyk, P., Sitthi-amorn, P., Freeman, W.T., Durand, F., Matusik, W.: Joint view
expansion and ﬁltering for automultiscopic 3D displays. ACM Trans. Graph. 32(6),
221 (2013)
6. Elgharib, M.A., Hefeeda, M., Durand, F., Freeman, W.T.: Video magniﬁcation in
presence of large motions. In: CVPR. pp. 4119–4127 (2015)
7. Hasinoﬀ, S.W., Jwiak, M., Durand, F., Freeman, W.T.: Search-and-replace editing
for personal photo collections. In: ICCP. pp. 1–8 (2010)
8. Lang, M., Wang, O., Aydin, T.O., Smolic, A., Gross, M.H.: Practical temporal
consistency for image-based graphics applications. ACM Trans. Graph. (2012)
9. Levin, A., Lischinski, D., Weiss, Y.: Colorization using optimization. ACM Trans.
Graph. 23(3), 689–694 (2004)
10. Li, Y., Ju, T., Hu, S.: Instant propagation of sparse edits on images and videos.
Comput. Graph. Forum 29(7), 2049–2054 (2010)
11. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. IJCV 60(2),
91–110 (2004)
12. Lucas, B.D., Kanade, T.: An iterative image registration technique with an appli-
cation to stereo vision. In: IJCAI. pp. 674–679 (1981)
13. Meyer, S., Wang, O., Zimmer, H., Grosse, M., Sorkine-Hornung, A.: Phase-based
frame interpolation for video. In: CVPR. pp. 1410–1418 (2015)
14. Ngo, D.T., Park, S., Jorstad, A., Crivellaro, A., Yoo, C.D., Fua, P.: Dense image
registration and deformable surface reconstruction in presence of occlusions and
minimal texture. In: ICCV. pp. 2273–2281 (2015)
15. Portilla, J., Simoncelli, E.P.: A parametric texture model based on joint statistics
of complex wavelet coeﬃcients. IJCV 40(1), 49–70 (2000)
16. Rav-acha, A., Kohli, P., Rother, C., Fitzgibbon, A.: Unwrap mosaics: A new rep-
resentation for video editing. ACM Trans. Graph. (2008)
17. Shi, J., Tomasi, C.: Good features to track. In: CVPR. pp. 593–600 (1994)
18. Simoncelli, E.P., Freeman, W.T.: The steerable pyramid: a ﬂexible architecture for
multi-scale derivative computation. In: ICIP. pp. 444–447 (1995)
19. Simoncelli, E.P., Freeman, W.T., Adelson, E.H., Heeger, D.J.: Shiftable multiscale
transforms. IEEE Trans. Information Theory 38(2), 587–607 (1992)
20. Wadhwa, N., Rubinstein, M., Durand, F., Freeman, W.T.: Phase-based video mo-
tion processing. ACM Trans. Graph. 32(4), 80 (2013)
21. Xu, K., Li, Y., Ju, T., Hu, S., Liu, T.: Eﬃcient aﬃnity-based edit propagation
using K-D tree. ACM Trans. Graph. 28(5), 118:1–118:6 (2009)
22. Yatagawa, T., Yamaguchi, Y.: Temporally coherent video editing using an edit
propagation matrix. Computers & Graphics 43, 1–10 (2014)
23. Y¨ucer, K., Jacobson, A., Hornung, A., Sorkine, O.: Transfusive image manipula-
tion. ACM Trans. Graph. 31(6), 176 (2012)
24. Zhang, Z., Liu, Y., Dai, Q.: Light ﬁeld from micro-baseline image pair. In: CVPR.
pp. 3800–3809 (2015)

