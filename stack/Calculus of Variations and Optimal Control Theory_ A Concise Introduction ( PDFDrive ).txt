
Calculus of Variations and
Optimal Control Theory


Calculus of Variations and
Optimal Control Theory
A Concise Introduction
Daniel Liberzon
PRINCETON
UNIVERSITY
PRESS
PRINCETON
AND
OXFORD

Copyright  © 2012 by Princeton University Press 
 
Published by Princeton University Press, 41 William Street, Princeton, New Jersey  
08540 
In the United Kingdom: Princeton University Press, 6 Oxford Street, Woodstock, 
Oxfordshire OX20 1TW 
  
All Rights Reserved 
ISBN:  978-0-691-15187-8 
Library of Congress Control Number: 2011935625 
British Library Cataloging-in-Publication Data is available 
This book has been composed in LA        TEX 
The publisher would like to acknowledge the author of this volume for providing the 
digital files from which this book was printed 
Printed on acid-free paper  ∞ 
press.princeton.edu 
Printed in the United States of America 
10  9  8  7  6  5  4  3  2  1 

Since the building of the universe is perfect and is created by the
wisdom creator, nothing arises in the universe in which one cannot
see the sense of some maximum or minimum.
—Leonhard Euler
The words “control theory” are, of course, of recent origin, but the
subject itself is much older, since it contains the classical calculus
of variations as a special case, and the ﬁrst calculus of variations
problems go back to classical Greece.
—Hector J. Sussmann


Contents
Preface
xiii
1
Introduction
1
1.1
Optimal control problem
1
1.2
Some background on ﬁnite-dimensional optimization
3
1.2.1
Unconstrained optimization . . . . . . . . . . . . . . .
4
1.2.2
Constrained optimization . . . . . . . . . . . . . . . .
11
1.3
Preview of inﬁnite-dimensional optimization
17
1.3.1
Function spaces, norms, and local minima . . . . . . .
18
1.3.2
First variation and ﬁrst-order necessary condition . . .
19
1.3.3
Second variation and second-order conditions . . . . .
21
1.3.4
Global minima and convex problems . . . . . . . . . .
23
1.4
Notes and references for Chapter 1
24
2
Calculus of Variations
26
2.1
Examples of variational problems
26
2.1.1
Dido’s isoperimetric problem
. . . . . . . . . . . . . .
26
2.1.2
Light reﬂection and refraction . . . . . . . . . . . . . .
27
2.1.3
Catenary
. . . . . . . . . . . . . . . . . . . . . . . . .
28
2.1.4
Brachistochrone
. . . . . . . . . . . . . . . . . . . . .
30
2.2
Basic calculus of variations problem
32
2.2.1
Weak and strong extrema . . . . . . . . . . . . . . . .
33
2.3
First-order necessary conditions for weak extrema
34
2.3.1
Euler-Lagrange equation . . . . . . . . . . . . . . . . .
35
2.3.2
Historical remarks . . . . . . . . . . . . . . . . . . . .
39
2.3.3
Technical remarks
. . . . . . . . . . . . . . . . . . . .
40
vii

viii
CONTENTS
2.3.4
Two special cases . . . . . . . . . . . . . . . . . . . . .
41
2.3.5
Variable-endpoint problems . . . . . . . . . . . . . . .
42
2.4
Hamiltonian formalism and mechanics
44
2.4.1
Hamilton’s canonical equations . . . . . . . . . . . . .
45
2.4.2
Legendre transformation . . . . . . . . . . . . . . . . .
46
2.4.3
Principle of least action and conservation laws
. . . .
48
2.5
Variational problems with constraints
51
2.5.1
Integral constraints . . . . . . . . . . . . . . . . . . . .
52
2.5.2
Non-integral constraints . . . . . . . . . . . . . . . . .
55
2.6
Second-order conditions
58
2.6.1
Legendre’s necessary condition for a weak minimum .
59
2.6.2
Suﬃcient condition for a weak minimum . . . . . . . .
62
2.7
Notes and references for Chapter 2
68
3
From Calculus of Variations to Optimal Control
71
3.1
Necessary conditions for strong extrema
71
3.1.1
Weierstrass-Erdmann corner conditions
. . . . . . . .
71
3.1.2
Weierstrass excess function
. . . . . . . . . . . . . . .
76
3.2
Calculus of variations versus optimal control
81
3.3
Optimal control problem formulation and assumptions
83
3.3.1
Control system . . . . . . . . . . . . . . . . . . . . . .
83
3.3.2
Cost functional . . . . . . . . . . . . . . . . . . . . . .
86
3.3.3
Target set . . . . . . . . . . . . . . . . . . . . . . . . .
88
3.4
Variational approach to the ﬁxed-time, free-endpoint problem
89
3.4.1
Preliminaries . . . . . . . . . . . . . . . . . . . . . . .
89
3.4.2
First variation
. . . . . . . . . . . . . . . . . . . . . .
92
3.4.3
Second variation . . . . . . . . . . . . . . . . . . . . .
95
3.4.4
Some comments
. . . . . . . . . . . . . . . . . . . . .
96
3.4.5
Critique of the variational approach and preview of
the maximum principle
. . . . . . . . . . . . . . . . .
98
3.5
Notes and references for Chapter 3
100

CONTENTS
ix
4
The Maximum Principle
102
4.1
Statement of the maximum principle
102
4.1.1
Basic ﬁxed-endpoint control problem . . . . . . . . . . 102
4.1.2
Basic variable-endpoint control problem . . . . . . . . 104
4.2
Proof of the maximum principle
105
4.2.1
From Lagrange to Mayer form
. . . . . . . . . . . . . 107
4.2.2
Temporal control perturbation
. . . . . . . . . . . . . 109
4.2.3
Spatial control perturbation . . . . . . . . . . . . . . . 110
4.2.4
Variational equation . . . . . . . . . . . . . . . . . . . 112
4.2.5
Terminal cone . . . . . . . . . . . . . . . . . . . . . . . 115
4.2.6
Key topological lemma . . . . . . . . . . . . . . . . . . 117
4.2.7
Separating hyperplane . . . . . . . . . . . . . . . . . . 120
4.2.8
Adjoint equation . . . . . . . . . . . . . . . . . . . . . 121
4.2.9
Properties of the Hamiltonian . . . . . . . . . . . . . . 122
4.2.10 Transversality condition . . . . . . . . . . . . . . . . . 126
4.3
Discussion of the maximum principle
128
4.3.1
Changes of variables . . . . . . . . . . . . . . . . . . . 130
4.4
Time-optimal control problems
134
4.4.1
Example: double integrator . . . . . . . . . . . . . . . 135
4.4.2
Bang-bang principle for linear systems . . . . . . . . . 138
4.4.3
Nonlinear systems, singular controls, and Lie brackets 141
4.4.4
Fuller’s problem
. . . . . . . . . . . . . . . . . . . . . 146
4.5
Existence of optimal controls
148
4.6
Notes and references for Chapter 4
153
5
The Hamilton-Jacobi-Bellman Equation
156
5.1
Dynamic programming and the HJB equation
156
5.1.1
Motivation: the discrete problem . . . . . . . . . . . . 156
5.1.2
Principle of optimality . . . . . . . . . . . . . . . . . . 158
5.1.3
HJB equation . . . . . . . . . . . . . . . . . . . . . . . 161
5.1.4
Suﬃcient condition for optimality
. . . . . . . . . . . 165
5.1.5
Historical remarks . . . . . . . . . . . . . . . . . . . . 167
5.2
HJB equation versus the maximum principle
168

x
CONTENTS
5.2.1
Example: nondiﬀerentiable value function . . . . . . . 170
5.3
Viscosity solutions of the HJB equation
172
5.3.1
One-sided diﬀerentials . . . . . . . . . . . . . . . . . . 172
5.3.2
Viscosity solutions of PDEs . . . . . . . . . . . . . . . 174
5.3.3
HJB equation and the value function . . . . . . . . . . 176
5.4
Notes and references for Chapter 5
178
6
The Linear Quadratic Regulator
180
6.1
Finite-horizon LQR problem
180
6.1.1
Candidate optimal feedback law
. . . . . . . . . . . . 181
6.1.2
Riccati diﬀerential equation . . . . . . . . . . . . . . . 183
6.1.3
Value function and optimality . . . . . . . . . . . . . . 185
6.1.4
Global existence of solution for the RDE . . . . . . . . 187
6.2
Inﬁnite-horizon LQR problem
189
6.2.1
Existence and properties of the limit . . . . . . . . . . 190
6.2.2
Inﬁnite-horizon problem and its solution . . . . . . . . 193
6.2.3
Closed-loop stability . . . . . . . . . . . . . . . . . . . 194
6.2.4
Complete result and discussion . . . . . . . . . . . . . 196
6.3
Notes and references for Chapter 6
199
7
Advanced Topics
200
7.1
Maximum principle on manifolds
200
7.1.1
Diﬀerentiable manifolds . . . . . . . . . . . . . . . . . 201
7.1.2
Re-interpreting the maximum principle
. . . . . . . . 203
7.1.3
Symplectic geometry and Hamiltonian ﬂows . . . . . . 206
7.2
HJB equation, canonical equations, and characteristics
207
7.2.1
Method of characteristics
. . . . . . . . . . . . . . . . 208
7.2.2
Canonical equations as characteristics of the HJB
equation . . . . . . . . . . . . . . . . . . . . . . . . . . 211
7.3
Riccati equations and inequalities in robust control
212
7.3.1
L2 gain
. . . . . . . . . . . . . . . . . . . . . . . . . . 213
7.3.2
H∞control problem . . . . . . . . . . . . . . . . . . . 216
7.3.3
Riccati inequalities and LMIs . . . . . . . . . . . . . . 219
7.4
Maximum principle for hybrid control systems
219

CONTENTS
xi
7.4.1
Hybrid optimal control problem . . . . . . . . . . . . . 219
7.4.2
Hybrid maximum principle
. . . . . . . . . . . . . . . 221
7.4.3
Example: light reﬂection . . . . . . . . . . . . . . . . . 222
7.5
Notes and references for Chapter 7
223
Bibliography
225
Index
231


Preface
This book grew out of my lecture notes for a graduate course on optimal con-
trol theory which I taught at the University of Illinois at Urbana-Champaign
during the period from 2005 to 2010. While preparing the lectures, I have
accumulated an entire shelf of textbooks on calculus of variations and opti-
mal control systems. Although some of them are excellent, navigating and
combining the material from so many sources was a daunting task, and I
was unable to ﬁnd a single text with all the features that I wanted:
Appropriate presentation level. I needed a friendly introductory text
accessible to graduate students who have not had extensive training in con-
trol theory or mathematical analysis; a text that is rigorous and builds a
solid understanding of the subject without getting bogged down in techni-
calities.
Logical and notational consistency among topics. There are inti-
mate connections between the classical calculus of variations, the maximum
principle, and the Hamilton-Jacobi-Bellman theory, which I wanted to em-
phasize throughout the course. Due to diﬀerences in notation and presen-
tation style in the existing sources, it is very diﬃcult for students (and even
for instructors) to piece this material together.
Proof of the maximum principle. The maximum principle is a center-
piece of optimal control theory which also elucidates and puts in perspective
earlier developments in calculus of variations, and I felt it was important to
cover its proof. This is a challenging task because a complete proof is rather
long and, without adequate preparation and guidance, the eﬀort required to
understand it can seem prohibitive to most students.
Historical perspective. A course on optimal control gives students a
unique chance to get exposed to the work of some of the greatest mathe-
matical minds and trace the beautiful historical development of the subject,
from early calculus of variations results to modern optimal control theory.
A good text, while of course focusing on the mathematical developments,
should also give students a sense of the history behind them.1
1In Tom Stoppard’s famous play Arcadia there is an interesting argument between a
mathematician and a historian about what is more important: scientiﬁc progress or the
xiii

xiv
PREFACE
Manageable size.
It is very useful to have a text compact enough so
that all of the material in it can be covered in one semester, since very few
students will take more than one course speciﬁcally on this subject.
The present book is an attempt to meet all of the above challenges
(inevitably, with varying degrees of success). The comment about proving
the maximum principle in class deserves some elaboration. I believe that
to build the understanding necessary to correctly and eﬀectively use the
maximum principle, it is very helpful—if not essential—to master its proof.
In my experience, the proof given in this book is not too diﬃcult for students
to follow.
This has to do not just with the clarity and structure of the
proof itself, but also with the fact that special attention is paid to preparing
students for this proof earlier on in the course (as well as helping them digest
it through subsequent discussions). On the other hand, due to the central
role that the proof of the maximum principle plays in this book, instructors
not planning to cover this proof in class will not fully beneﬁt from adopting
this text.
While some prior exposure to control theory, mathematical analysis, and
optimization is certainly helpful, this book is suﬃciently self-contained so
that any motivated graduate student specializing in a technical subject (such
as engineering or mathematics) should be able to follow it. Depending on the
student’s background, some supplementary reading may be necessary from
time to time; the notes and references located at the end of each chapter
should facilitate this process.
Chapters 1–6 form the main part of the book and can serve as the basis
for a one-semester course. Depending on the pace, the instructor may also
have time for some or all of the advanced topics discussed in Chapter 7. In
this last chapter I included only topics that in my opinion directly extend
and enhance the understanding of the core material. The instructor may
give preference to some other areas instead, such as the important issue
of state constraints in optimal control, the classical subject of stochastic
optimal control and estimation, the very popular model predictive control,
or the numerous applications of optimal control theory. Fortunately, good
references covering these topics are readily available.
It is also possible
that the instructor will want to further elaborate on some aspects of the
theory presented in Chapters 1–6; in this regard, the end-of-chapter notes
and references may be a useful resource.
About 60 exercises are interspersed throughout the ﬁrst six chapters and
represent an integral part of the book. My intention is for students to start
working on each exercise as soon as the corresponding material is covered.
A solutions manual is available upon request for instructors.
personalities behind it. In a technical book such as this one, the emphasis is clear, but
some ﬂavor of the history can enrich one’s understanding and appreciation of the subject.

PREFACE
xv
I am grateful to my colleagues Tamer Ba¸sar, Yi Ma, and Bill Perkins
who shared their valuable experiences and lecture notes from their oﬀerings
of the optimal control course at UIUC. I am also indebted to students and
friends who suﬀered through earlier drafts of this book and provided nu-
merous corrections and suggestions, along with stimulating discussions and
much-needed encouragement; I would like to especially thank Sairaj Dho-
ple, Michael Margaliot, Nathan Shemonski, Hyungbo Shim, Guosong Yang,
and Jingjin Yu. I appreciated the helpful and thoughtful comments of the
reviewers and the support and professionalism of Vickie Kearn and other
editorial staﬀat Princeton University Press. I thank Glenda Krupa for the
careful and thorough copyediting. On the artistic side, I thank Polina Ben-
Sira for the very cool cover drawing, and my wife Olga not only for moral
support but also for substantial help with all the ﬁgures inside the book.
Finally, I am thankful to my daughter Ada who told me that I should write
the book and gave me a toy tangerine which helped in the process, as well as
to my little son Eduard whose arrival deﬁnitely boosted my eﬀorts to ﬁnish
the book.
I decided not to eliminate all errors from the book, and instead left
several of them on purpose in undisclosed locations as a way to provide
additional educational experience. To report success stories of correcting
errors, please contact me at my current email address (which is easy to ﬁnd
on the Web).
Daniel Liberzon
Urbana, IL


Calculus of Variations and
Optimal Control Theory


Chapter One
Introduction
1.1
OPTIMAL CONTROL PROBLEM
We begin by describing, very informally and in general terms, the class of
optimal control problems that we want to eventually be able to solve. The
goal of this brief motivational discussion is to ﬁx the basic concepts and
terminology without worrying about technical details.
The ﬁrst basic ingredient of an optimal control problem is a control
system. It generates possible behaviors. In this book, control systems will
be described by ordinary diﬀerential equations (ODEs) of the form
˙x = f(t, x, u),
x(t0) = x0
(1.1)
where x is the state taking values in Rn, u is the control input taking values
in some control set U ⊂Rm, t is time, t0 is the initial time, and x0 is the
initial state. Both x and u are functions of t, but we will often suppress
their time arguments.
The second basic ingredient is the cost functional. It associates a cost
with each possible behavior. For a given initial data (t0, x0), the behaviors
are parameterized by control functions u. Thus, the cost functional assigns
a cost value to each admissible control. In this book, cost functionals will
be denoted by J and will be of the form
J(u) :=
Z tf
t0
L(t, x(t), u(t))dt + K(tf, xf)
(1.2)
where L and K are given functions (running cost and terminal cost, respec-
tively), tf is the ﬁnal (or terminal) time which is either free or ﬁxed, and
xf := x(tf) is the ﬁnal (or terminal) state which is either free or ﬁxed or
belongs to some given target set. Note again that u itself is a function of
time; this is why we say that J is a functional (a real-valued function on a
space of functions).
The optimal control problem can then be posed as follows: Find a control
u that minimizes J(u) over all admissible controls (or at least over nearby
controls).
Later we will need to come back to this problem formulation
1

2
CHAPTER 1
and ﬁll in some technical details.
In particular, we will need to specify
what regularity properties should be imposed on the function f and on the
admissible controls u to ensure that state trajectories of the control system
are well deﬁned.
Several versions of the above problem (depending, for
example, on the role of the ﬁnal time and the ﬁnal state) will be stated
more precisely when we are ready to study them. The reader who wishes to
preview this material can ﬁnd it in Section 3.3.
It can be argued that optimality is a universal principle of life, in the
sense that many—if not most—processes in nature are governed by solu-
tions to some optimization problems (although we may never know exactly
what is being optimized). We will soon see that fundamental laws of me-
chanics can be cast in an optimization context. From an engineering point
of view, optimality provides a very useful design principle, and the cost to
be minimized (or the proﬁt to be maximized) is often naturally contained
in the problem itself. Some examples of optimal control problems arising in
applications include the following:
• Send a rocket to the moon with minimal fuel consumption.
• Produce a given amount of chemical in minimal time and/or with
minimal amount of catalyst used (or maximize the amount produced
in given time).
• Bring sales of a new product to a desired level while minimizing the
amount of money spent on the advertising campaign.
• Maximize throughput or accuracy of information transmission over a
communication channel with a given bandwidth/capacity.
The reader will easily think of other examples.
Several speciﬁc optimal
control problems will be examined in detail later in the book. We brieﬂy
discuss one simple example here to better illustrate the general problem
formulation.
Example 1.1
Consider a simple model of a car moving on a horizontal
line. Let x ∈R be the car’s position and let u be the acceleration which acts
as the control input. We put a bound on the maximal allowable acceleration
by letting the control set U be the bounded interval [−1, 1] (negative acceler-
ation corresponds to braking). The dynamics of the car are ¨x = u. In order
to arrive at a ﬁrst-order diﬀerential equation model of the form (1.1), let us
relabel the car’s position x as x1 and denote its velocity ˙x by x2. This gives
the control system ˙x1 = x2, ˙x2 = u with state
x1
x2

∈R2. Now, suppose
that we want to “park” the car at the origin, i.e., bring it to rest there, in
minimal time. This objective is captured by the cost functional (1.2) with
the constant running cost L ≡1, no terminal cost (K ≡0), and the ﬁxed
ﬁnal state
00

. We will solve this optimal control problem in Section 4.4.1.

INTRODUCTION
3
(The basic form of the optimal control strategy may be intuitively obvious,
but obtaining a complete description of the optimal control requires some
work.)
□
In this book we focus on the mathematical theory of optimal control. We
will not undertake an in-depth study of any of the applications mentioned
above. Instead, we will concentrate on the fundamental aspects common to
all of them. After ﬁnishing this book, the reader familiar with a speciﬁc
application domain should have no diﬃculty reading papers that deal with
applications of optimal control theory to that domain, and will be prepared
to think creatively about new ways of applying the theory.
We can view the optimal control problem as that of choosing the best
path among all paths feasible for the system, with respect to the given cost
function.
In this sense, the problem is inﬁnite-dimensional, because the
space of paths is an inﬁnite-dimensional function space. This problem is also
a dynamic optimization problem, in the sense that it involves a dynamical
system and time. However, to gain appreciation for this problem, it will be
useful to ﬁrst recall some basic facts about the more standard static ﬁnite-
dimensional optimization problem, concerned with ﬁnding a minimum of a
given function f : Rn →R. Then, when we get back to inﬁnite-dimensional
optimization, we will more clearly see the similarities but also the diﬀerences.
The subject studied in this book has a rich and beautiful history; the
topics are ordered in such a way as to allow us to trace its chronological devel-
opment. In particular, we will start with calculus of variations, which deals
with path optimization but not in the setting of control systems. The opti-
mization problems treated by calculus of variations are inﬁnite-dimensional
but not dynamic. We will then make a transition to optimal control theory
and develop a truly dynamic framework. This modern treatment is based on
two key developments, initially independent but ultimately closely related
and complementary to each other: the maximum principle and the principle
of dynamic programming.
1.2
SOME BACKGROUND ON FINITE-DIMENSIONAL
OPTIMIZATION
Consider a function f : Rn →R. Let D be some subset of Rn, which could
be the entire Rn. We denote by | · | the standard Euclidean norm on Rn.
A point x∗∈D is a local minimum of f over D if there exists an ε > 0
such that for all x ∈D satisfying |x −x∗| < ε we have
f(x∗) ≤f(x).
(1.3)
In other words, x∗is a local minimum if in some ball around it, f does not
attain a value smaller than f(x∗). Note that this refers only to points in D;

4
CHAPTER 1
the behavior of f outside D is irrelevant, and in fact we could have taken
the domain of f to be D rather than Rn.
If the inequality in (1.3) is strict for x ̸= x∗, then we have a strict
local minimum. If (1.3) holds for all x ∈D, then the minimum is global
over D. By default, when we say “a minimum” we mean a local minimum.
Obviously, a minimum need not be unique unless it is both strict and global.
The notions of a (local, strict, global) maximum are deﬁned similarly.
If a point is either a maximum or a minimum, it is called an extremum.
Observe that maxima of f are minima of −f, so there is no need to develop
separate results for both. We focus on the minima, i.e., we view f as a cost
function to be minimized (rather than a proﬁt to be maximized).
1.2.1
Unconstrained optimization
The term “unconstrained optimization” usually refers to the situation where
all points x suﬃciently near x∗in Rn are in D, i.e., x∗belongs to D together
with some Rn-neighborhood. The simplest case is when D = Rn, which is
sometimes called the completely unconstrained case. However, as far as local
minimization is concerned, it is enough to assume that x∗is an interior point
of D. This is automatically true if D is an open subset of Rn.
First-order necessary condition for optimality
Suppose that f is a C1 (continuously diﬀerentiable) function and x∗is its
local minimum.
Pick an arbitrary vector d ∈Rn.
Since we are in the
unconstrained case, moving away from x∗in the direction of d or −d cannot
immediately take us outside D. In other words, we have x∗+ αd ∈D for all
α ∈R close enough to 0.
For a ﬁxed d, we can consider f(x∗+ αd) as a function of the real
parameter α, whose domain is some interval containing 0. Let us call this
new function g:
g(α) := f(x∗+ αd).
(1.4)
Since x∗is a minimum of f, it is clear that 0 is a minimum of g. Passing
from f to g is useful because g is a function of a scalar variable and so its
minima can be studied using ordinary calculus. In particular, we can write
down the ﬁrst-order Taylor expansion for g around α = 0:
g(α) = g(0) + g′(0)α + o(α)
(1.5)
where o(α) represents “higher-order terms” which go to 0 faster than α as
α approaches 0, i.e.,
lim
α→0
o(α)
α
= 0.
(1.6)

INTRODUCTION
5
We claim that
g′(0) = 0.
(1.7)
To show this, suppose that g′(0) ̸= 0. Then, in view of (1.6), there exists
an ε > 0 small enough so that for all nonzero α with |α| < ε, the absolute
value of the fraction in (1.6) is less than |g′(0)|. We can write this as
|α| < ε, α ̸= 0
⇒
|o(α)| < |g′(0)α|.
For these values of α, (1.5) gives
g(α) −g(0) < g′(0)α + |g′(0)α|.
(1.8)
If we further restrict α to have the opposite sign to g′(0), then the right-hand
side of (1.8) becomes 0 and we obtain g(α) −g(0) < 0. But this contradicts
the fact that g has a minimum at 0. We have thus shown that (1.7) is indeed
true.
We now need to re-express this result in terms of the original function
f. A simple application of the chain rule from vector calculus yields the
formula
g′(α) = ∇f(x∗+ αd) · d
(1.9)
where
∇f := (fx1, . . . , fxn)T
is the gradient of f and · denotes inner product.1 Whenever there is no
danger of confusion, we use subscripts as a shorthand notation for partial
derivatives: fxi := ∂f/∂xi. Setting α = 0 in (1.9), we have
g′(0) = ∇f(x∗) · d
(1.10)
and this equals 0 by (1.7). Since d was arbitrary, we conclude that
∇f(x∗) = 0
(1.11)
This is the ﬁrst-order necessary condition for optimality.
A point x∗satisfying this condition is called a stationary point. The
condition is “ﬁrst-order” because it is derived using the ﬁrst-order expan-
sion (1.5). We emphasize that the result is valid when f ∈C1 and x∗is an
interior point of D.
1There is no consensus in the literature whether the gradient is a column vector or a
row vector. Treating it as a row vector would simplify the notation since it often appears
in a product with another vector. Geometrically, however, it plays the role of a regular
column vector, and for consistency we follow this latter convention everywhere.

6
CHAPTER 1
Second-order conditions for optimality
We now derive another necessary condition and also a suﬃcient condition
for optimality, under the stronger hypothesis that f is a C2 function (twice
continuously diﬀerentiable).
First, we assume as before that x∗is a local minimum and derive a
necessary condition. For an arbitrary ﬁxed d ∈Rn, let us consider a Taylor
expansion of g(α) = f(x∗+ αd) again, but this time include second-order
terms:
g(α) = g(0) + g′(0)α + 1
2g′′(0)α2 + o(α2)
(1.12)
where
lim
α→0
o(α2)
α2
= 0.
(1.13)
We know from the derivation of the ﬁrst-order necessary condition that g′(0)
must be 0. We claim that
g′′(0) ≥0.
(1.14)
Indeed, suppose that g′′(0) < 0. By (1.13), there exists an ε > 0 such that
|α| < ε, α ̸= 0
⇒
|o(α2)| < 1
2|g′′(0)|α2.
For these values of α, (1.12) reduces to g(α) −g(0) < 0, contradicting that
fact that 0 is a minimum of g. Therefore, (1.14) must hold.
What does this result imply about the original function f? To see what
g′′(0) is in terms of f, we need to diﬀerentiate the formula (1.9). The reader
may ﬁnd it helpful to ﬁrst rewrite (1.9) more explicitly as
g′(α) =
n
X
i=1
fxi(x∗+ αd)di.
Diﬀerentiating both sides with respect to α, we have
g′′(α) =
n
X
i,j=1
fxixj(x∗+ αd)didj
where double subscripts are used to denote second-order partial derivatives.
For α = 0 this gives
g′′(0) =
n
X
i,j=1
fxixj(x∗)didj
or, in matrix notation,
g′′(0) = dT ∇2f(x∗)d
(1.15)

INTRODUCTION
7
where
∇2f :=



fx1x1
. . .
fx1xn
...
...
...
fxnx1
. . .
fxnxn



is the Hessian matrix of f.
In view of (1.14), (1.15), and the fact that
d was arbitrary, we conclude that the matrix ∇2f(x∗) must be positive
semideﬁnite:
∇2f(x∗) ≥0
(positive semideﬁnite)
This is the second-order necessary condition for optimality.
Like the previous ﬁrst-order necessary condition, this second-order con-
dition only applies to the unconstrained case. But, unlike the ﬁrst-order
condition, it requires f to be C2 and not just C1. Another diﬀerence with the
ﬁrst-order condition is that the second-order condition distinguishes minima
from maxima: at a local maximum, the Hessian must be negative semidef-
inite, while the ﬁrst-order condition applies to any extremum (a minimum
or a maximum).
Strengthening the second-order necessary condition and combining it
with the ﬁrst-order necessary condition, we can obtain the following second-
order suﬃcient condition for optimality: If a C2 function f satisﬁes
∇f(x∗) = 0
and
∇2f(x∗) > 0 (positive deﬁnite)
(1.16)
on an interior point x∗of its domain, then x∗is a strict local minimum of
f. To see why this is true, take an arbitrary d ∈Rn and consider again the
second-order expansion (1.12) for g(α) = f(x∗+ αd). We know that g′(0)
is given by (1.10), thus it is 0 because ∇f(x∗) = 0. Next, g′′(0) is given
by (1.15), and so we have
f(x∗+ αd) = f(x∗) + 1
2dT ∇2f(x∗)dα2 + o(α2).
(1.17)
The intuition is that since the Hessian ∇2f(x∗) is a positive deﬁnite matrix,
the second-order term dominates the higher-order term o(α2). To establish
this fact rigorously, note that by the deﬁnition of o(α2) we can pick an ε > 0
small enough so that
|α| < ε, α ̸= 0
⇒
|o(α2)| < 1
2dT ∇2f(x∗)dα2
and for these values of α we deduce from (1.17) that f(x∗+ αd) > f(x∗).
To conclude that x∗is a (strict) local minimum, one more technical detail
is needed. According to the deﬁnition of a local minimum (see page 3), we
must show that f(x∗) is the lowest value of f in some ball around x∗. But
the term o(α2) and hence the value of ε in the above construction depend on

8
CHAPTER 1
the choice of the direction d. It is clear that this dependence is continuous,
since all the other terms in (1.17) are continuous in d.2 Also, without loss
of generality we can restrict d to be of unit length, and then we can take
the minimum of ε over all such d. Since the unit sphere in Rn is compact,
the minimum is well deﬁned (thanks to the Weierstrass Theorem which is
discussed below). This minimal value of ε provides the radius of the desired
ball around x∗in which the lowest value of f is achieved at x∗.
Feasible directions, global minima, and convex problems
The key fact that we used in the previous developments was that for every
d ∈Rn, points of the form x∗+ αd for α suﬃciently close to 0 belong to
D. This is no longer the case if D has a boundary (e.g., D is a closed ball
in Rn) and x∗is a point on this boundary. Such situations do not ﬁt into
the unconstrained optimization scenario as we deﬁned it at the beginning
of Section 1.2.1; however, for simple enough sets D and with some extra
care, a similar analysis is possible. Let us call a vector d ∈Rn a feasible
direction (at x∗) if x∗+ αd ∈D for small enough α > 0 (see Figure 1.1). If
not all directions d are feasible, then the condition ∇f(x∗) = 0 is no longer
necessary for optimality. We can still deﬁne the function (1.4) for every
feasible direction d, but the proof of (1.7) is no longer valid because α is now
nonnegative. We leave it to the reader to modify that argument and show
that if x∗is a local minimum, then ∇f(x∗)·d ≥0 for every feasible direction
d. As for the second-order necessary condition, the inequality (1.14) is still
true if g′(0) = 0, which together with (1.10) and (1.15) implies that we must
have dT ∇2f(x∗)d ≥0 for all feasible directions satisfying ∇f(x∗) · d = 0.
x∗
feasible
not feasible
Figure 1.1: Feasible directions
If the set D is convex, then the line segment connecting x∗to an arbitrary
other point x ∈D lies entirely in D. All points on this line segment take the
2The term o(α2) can be described more precisely using Taylor’s theorem with re-
mainder, which is a higher-order generalization of the Mean Value Theorem; see, e.g.,
[Rud76, Theorem 5.15]. We will discuss this issue in more detail later when deriving the
corresponding result in calculus of variations (see Section 2.6).

INTRODUCTION
9
form x∗+ αd, α ∈[0, ¯α] for some d ∈Rn and ¯α > 0. This means that the
feasible direction approach is particularly suitable for the case of a convex
D. But if D is not convex, then the ﬁrst-order and second-order necessary
conditions in terms of feasible directions are conservative. The next exercise
touches on the issue of suﬃciency.
Exercise 1.1
Suppose that f is a C2 function and x∗is a point of its
domain at which we have ∇f(x∗) · d ≥0 and dT ∇2f(x∗)d > 0 for every
nonzero feasible direction d. Is x∗necessarily a local minimum of f? Prove
or give a counterexample.
□
When we are not dealing with the completely unconstrained case in
which D is the entire Rn, we think of D as the constraint set over which
f is being minimized. Particularly important in optimization theory is the
case when equality constraints are present, so that D is a lower-dimensional
surface in Rn (see Figure 1.2). In such situations, the above method which
utilizes feasible directions represented by straight lines is no longer suitable:
there might not be any feasible directions, and then the corresponding nec-
essary conditions are vacuous. We will describe a reﬁned approach to con-
strained optimization in Section 1.2.2; it essentially replaces straight lines
with arbitrary curves.
x∗
D
Figure 1.2: A surface
So far we have only discussed local minima. In practice, however, one
is typically interested in ﬁnding a global minimum over a given domain
(or constraint set) D, if such a global minimum exists.
We now brieﬂy
discuss how conditions for local optimality can be useful for solving global
optimization problems as well, provided that these global problems have
certain nice features.
The following basic existence result is known as the Weierstrass The-
orem: If f is a continuous function and D is a compact set, then there
exists a global minimum of f over D. The reader will recall that for subsets
of Rn, compactness can be deﬁned in three equivalent ways:
1) D is compact if it is closed and bounded.

10
CHAPTER 1
2) D is compact if every open cover of D has a ﬁnite subcover.
3) D is compact if every sequence in D has a subsequence converging to
some point in D (sequential compactness).
We will revisit compactness and the Weierstrass Theorem in the inﬁnite-
dimensional optimization setting.
The necessary conditions for local optimality that we discussed earlier
suggest the following procedure for ﬁnding a global minimum. First, ﬁnd
all interior points of D satisfying ∇f(x∗) = 0 (the stationary points). If
f is not diﬀerentiable everywhere, include also points where ∇f does not
exist (these points together with the stationary points comprise the critical
points). Next, ﬁnd all boundary points satisfying ∇f(x∗) · d ≥0 for all
feasible d. Finally, compare values at all these candidate points and choose
the smallest one. If one can aﬀord the computation of second derivatives,
then the second-order conditions can be used in combination with the ﬁrst-
order ones.
If D is a convex set and f is a convex function, then the minimization
problem is particularly tractable. First, a local minimum is automatically a
global one. Second, the ﬁrst-order necessary condition (for f ∈C1) is also a
suﬃcient condition. Thus if ∇f(x∗) · d ≥0 for all feasible directions d, or
in particular if x∗is an interior point of D and ∇f(x∗) = 0, then x∗is a
global minimum. These properties are consequences of the fact (illustrated
in Figure 1.3) that the graph of a convex function f lies above that of the
linear approximation x 7→f(x∗) + ∇f(x∗) · (x −x∗).
x
f(x)
x∗
Figure 1.3: A convex function
Eﬃcient numerical algorithms—such as the well-known steepest descent
(or gradient) method—exist for converging to points satisfying ∇f(x∗) = 0
(stationary points).
For convex problems, these algorithms yield conver-
gence to global minima.

INTRODUCTION
11
1.2.2
Constrained optimization
Now suppose that D is a surface in Rn deﬁned by the equality constraints
h1(x) = h2(x) = · · · = hm(x) = 0
(1.18)
where hi, i = 1, . . . , m are C1 functions from Rn to R. We assume that f is
a C1 function and study its minima over D.
First-order necessary condition (Lagrange multipliers)
Let x∗∈D be a local minimum of f over D.
We assume that x∗is a
regular point of D in the sense that the gradients ∇hi, i = 1, . . . , m are
linearly independent at x∗. This is a technical assumption needed to rule
out degenerate situations; see Exercise 1.2 below.
Instead of line segments containing x∗which we used in the uncon-
strained case, we now consider curves in D passing through x∗. Such a curve
is a family of points x(α) ∈D parameterized by α ∈R, with x(0) = x∗. We
require the function x(·) to be C1, at least for α near 0. Given an arbitrary
curve of this kind, we can consider the function
g(α) := f(x(α)).
Note that when there are no equality constraints, functions of the form (1.4)
considered previously can be viewed as special cases of this more general
construction. From the fact that 0 is a minimum of g, we derive exactly as
before that (1.7) holds, i.e., g′(0) = 0. To interpret this result in terms of f,
note that
g′(α) = ∇f(x(α)) · x′(α)
which for α = 0 gives
g′(0) = ∇f(x∗) · x′(0) = 0.
(1.19)
The vector x′(0) ∈Rn is an important object for us here. From the ﬁrst-
order Taylor expansion x(α) = x∗+ x′(0)α + o(α) we see that x′(0) deﬁnes
a linear approximation of x(·) at x∗. Geometrically, it speciﬁes the inﬁnites-
imal direction of the curve (see Figure 1.4). The vector x′(0) is a tangent
vector to D at x∗. It lives in the tangent space to D at x∗, which is denoted
by Tx∗D. (We can think of this space as having its origin at x∗.)
We want to have a more explicit characterization of the tangent space
Tx∗D, which will help us understand it better. Since D was deﬁned as the set
of points satisfying the equalities (1.18), and since the points x(α) lie in D
by construction, we must have hi(x(α)) = 0 for all α and all i ∈{1, . . . , m}.
Diﬀerentiating this formula gives
0 = d
dαhi(x(α)) = ∇hi(x(α)) · x′(α),
i = 1, . . . , m

12
CHAPTER 1
x∗
D
x(α)
x′(0)
Figure 1.4: A tangent vector
for all α (close enough to 0). Setting α = 0 and remembering that x(0) = x∗,
we obtain
0 =
d
dα

α=0
hi(x(α)) = ∇hi(x∗) · x′(0),
i = 1, . . . , m.
We have shown that for an arbitrary C1 curve x(·) in D with x(0) = x∗, its
tangent vector x′(0) must satisfy ∇hi(x∗) · x′(0) = 0 for each i. Actually,
one can show that the converse is also true, namely, every vector d ∈Rn
satisfying
∇hi(x∗) · d = 0,
i = 1, . . . , m
(1.20)
is a tangent vector to D at x∗corresponding to some curve. (We do not give
a proof of this fact but note that it relies on x∗being a regular point of D.)
In other words, the tangent vectors to D at x∗are exactly the vectors d for
which (1.20) holds. This is the characterization of the tangent space Tx∗D
that we were looking for. It is clear from (1.20) that Tx∗D is a subspace of
Rn; in particular, if d is a tangent vector, then so is −d (going from x′(0) to
−x′(0) corresponds to reversing the direction of the curve).
Now let us go back to (1.19), which tells us that ∇f(x∗) · d = 0 for
all d ∈Tx∗D (since the curve x(·) and thus the tangent vector x′(0) were
arbitrary). In view of the characterization of Tx∗D given by (1.20), we can
rewrite this condition as follows:
∇f(x∗) · d = 0
∀d such that ∇hi(x∗) · d = 0, i = 1, . . . , m.
(1.21)
The relation between ∇f(x∗) and ∇hi(x∗) expressed by (1.21) looks some-
what clumsy, since checking it involves a search over d. Can we eliminate d
from this relation and make it more explicit? A careful look at (1.21) should
quickly lead the reader to the following statement.
Claim: The gradient of f at x∗is a linear combination of the gradients of
the constraint functions h1, . . . , hm at x∗:
∇f(x∗) ∈span{∇hi(x∗), i = 1, . . . , m}.
(1.22)

INTRODUCTION
13
Indeed, if the claim were not true, then ∇f(x∗) would have a component
orthogonal to span{∇hi(x∗)}, i.e, there would exist a d ̸= 0 satisfying (1.20)
such that ∇f(x∗) can be written in the form
∇f(x∗) = d −
m
X
i=1
λ∗
i ∇hi(x∗)
(1.23)
for some λ∗
1, . . . , λ∗
m ∈R. Taking the inner product with d on both sides
of (1.23) and using (1.20) gives
∇f(x∗) · d = d · d ̸= 0
and we reach a contradiction with (1.21).
Geometrically, the claim says that ∇f(x∗) is normal to D at x∗. This
situation is illustrated in Figure 1.5 for the case of two constraints in R3.
Note that if there is only one constraint, say h1(x) = 0, then D is a two-
dimensional surface and ∇f(x∗) must be proportional to ∇h1(x∗), the nor-
mal direction to D at x∗. When the second constraint h2(x) = 0 is added,
D becomes a curve (the thick curve in the ﬁgure) and ∇f(x∗) is allowed to
live in the plane spanned by ∇h1(x∗) and ∇h2(x∗), i.e., the normal plane to
D at x∗. In general, the intuition behind the claim is that unless ∇f(x∗) is
normal to D, there are curves in D passing through x∗whose tangent vectors
at x∗make both positive and negative inner products with ∇f(x∗), hence
in particular f can be decreased by moving away from x∗while staying in
D.
∇h1(x∗)
∇h2(x∗)
∇f(x∗)
x∗
Figure 1.5: Gradient vectors and constrained optimality
The condition (1.22) means that there exist real numbers λ∗
1, . . . , λ∗
m such
that
∇f(x∗) + λ∗
1∇h1(x∗) + · · · + λ∗
m∇hm(x∗) = 0
(1.24)

14
CHAPTER 1
This is the ﬁrst-order necessary condition for constrained optimal-
ity. The coeﬃcients λ∗
i , i = 1, . . . , m are called Lagrange multipliers.
Exercise 1.2
Give an example where a local minimum x∗is not a regular
point and the above necessary condition is false (be sure to justify both of
these claims).
□
The above proof of the ﬁrst-order necessary condition for constrained
optimality involves geometric concepts. We also left a gap in it because we
did not prove the converse implication in the equivalent characterization of
the tangent space given by (1.20). We now give a shorter alternative proof
which is purely analytic, and which will be useful when we study problems
with constraints in calculus of variations. However, the geometric intuition
behind the previous proof will be helpful for us later as well. We invite the
reader to study both proofs as a way of testing the mathematical background
knowledge that will be required in the subsequent chapters.
Let us start again by assuming that x∗is a local minimum of f over D,
where D is a surface in Rn deﬁned by the equality constraints (1.18) and
x∗is a regular point of D. Our goal is to rederive the necessary condition
expressed by (1.24). For simplicity, we only give the argument for the case of
a single constraint h(x) = 0, i.e., m = 1; the extension to m > 1 is straight-
forward (see Exercise 1.3 below). Given two arbitrary vectors d1, d2 ∈Rn,
we can consider the following map from R × R to itself:
F : (α1, α2) 7→
 f(x∗+ α1d1 + α2d2), h(x∗+ α1d1 + α2d2)

.
The Jacobian matrix of F at (0, 0) is
∇f(x∗) · d1
∇f(x∗) · d2
∇h(x∗) · d1
∇h(x∗) · d2

.
(1.25)
If this Jacobian matrix were nonsingular, then we could apply the Inverse
Function Theorem (see, e.g., [Rud76, Theorem 9.24]) and conclude that
there are neighborhoods of (0, 0) and F(0, 0) = (f(x∗), 0) on which the map
F is a bijection (has an inverse). This would imply, in particular, that there
are points x arbitrarily close to x∗such that h(x) = 0 and f(x) < f(x∗);
such points would be obtained by taking preimages of points on the ray
directed to the left from F(0, 0) in Figure 1.6. But this cannot be true,
since h(x) = 0 means that x ∈D and we know that x∗is a local minimum
of f over D. Therefore, the matrix (1.25) is singular.
Regularity of x∗in the present case just means that the gradient ∇h(x∗)
is nonzero. Choose a d1 such that ∇h(x∗) · d1 ̸= 0. With this d1 ﬁxed, let
λ∗:= −(∇f(x∗) · d1)/(∇h(x∗) · d1), so that ∇f(x∗) · d1 = −λ∗∇h(x∗) · d1.
Since the matrix (1.25) must be singular for all choices of d2, its ﬁrst row
must be a constant multiple of its second row (the second row being nonzero

INTRODUCTION
15
α1
α2
f
h
f(x∗)
F
Figure 1.6: Illustrating the alternative proof
by our choice of d1).
Thus we have ∇f(x∗) · d2 = −λ∗∇h(x∗) · d2, or
(∇f(x∗) + λ∗∇h(x∗)) · d2 = 0, and this must be true for all d2 ∈Rn. It
follows that ∇f(x∗) + λ∗∇h(x∗) = 0, which proves (1.24) for the case when
m = 1.
Exercise 1.3
Generalize the previous argument to an arbitrary number
m ≥1 of equality constraints (still assuming that x∗is a regular point).
□
The ﬁrst-order necessary condition for constrained optimality generalizes
the corresponding result we derived earlier for the unconstrained case. The
condition (1.24) together with the constraints (1.18) is a system of n + m
equations in n + m unknowns: n components of x∗plus m components of
the Lagrange multiplier vector λ∗= (λ∗
1, . . . , λ∗
m)T . For m = 0, we recover
the condition (1.11) which consists of n equations in n unknowns. To make
this relation even more explicit, consider the function ℓ: Rn × Rm →R
deﬁned by
ℓ(x, λ) := f(x) +
m
X
i=1
λihi(x)
(1.26)
which we call the augmented cost function.
If x∗is a local constrained
minimum of f and λ∗is the corresponding vector of Lagrange multipliers
for which (1.24) holds, then the gradient of ℓat (x∗, λ∗) satisﬁes
∇ℓ(x∗, λ∗) =
ℓx(x∗, λ∗)
ℓλ(x∗, λ∗)

=
∇f(x∗) + Pm
i=1 λ∗
i ∇hi(x∗)
h(x∗)

= 0
(1.27)
where ℓx, ℓλ are the vectors of partial derivatives of ℓwith respect to the
components of x and λ, respectively, and h = (h1, . . . , hm)T is the vector of
constraint functions. We conclude that (x∗, λ∗) is a usual (unconstrained)
stationary point of the augmented cost ℓ. Loosely speaking, adding Lagrange
multipliers converts a constrained problem into an unconstrained one, and
the ﬁrst-order necessary condition (1.24) for constrained optimality is recov-
ered from the ﬁrst-order necessary condition for unconstrained optimality
applied to ℓ.

16
CHAPTER 1
The idea of passing from constrained minimization of the original cost
function to unconstrained minimization of the augmented cost function is
due to Lagrange. If (x∗, λ∗) is a minimum of ℓ, then we must have h(x∗) = 0
(because otherwise we could decrease ℓby changing λ∗), and subject to these
constraints x∗should minimize f (because otherwise it would not minimize
ℓ).
Also, it is clear that (1.27) must hold.
However, it does not follow
that (1.27) is a necessary condition for x∗to be a constrained minimum of
f. Unfortunately, there is no quick way to derive the ﬁrst-order necessary
condition for constrained optimality by working with the augmented cost—
something that Lagrange originally attempted to do. Nevertheless, the basic
form of the augmented cost function (1.26) is fundamental in constrained
optimization theory, and will reappear in various forms several times in this
book.
Even though the condition in terms of Lagrange multipliers is only nec-
essary and not suﬃcient for constrained optimality, it is very useful for
narrowing down candidates for local extrema. The next exercise illustrates
this point for a well-known optimization problem arising in optics.
Exercise 1.4
Consider a curve D in the plane described by the equation
h(x) = 0, where h : R2 →R is a C1 function. Let y and z be two ﬁxed
points in the plane, lying on the same side with respect to D (but not on D
itself). Suppose that a ray of light emanates from y, gets reﬂected oﬀD at
some point x∗∈D, and arrives at z. Consider the following two statements:
(i) x∗must be such that the total Euclidean distance traveled by light to go
from y to z is minimized over all nearby candidate reﬂection points x ∈D
(Fermat’s principle); (ii) the angles that the light ray makes with the line
normal to D at x∗before and after the reﬂection must be the same (the law
of reﬂection). Accepting the ﬁrst statement as a hypothesis, prove that the
second statement follows from it, with the help of the ﬁrst-order necessary
condition for constrained optimality (1.24).
□
Second-order conditions
For the sake of completeness, we quickly state the second-order conditions
for constrained optimality; they will not be used in the sequel.
For the
necessary condition, suppose that x∗is a regular point of D and a local
minimum of f over D, where D is deﬁned by the equality constraints (1.18)
as before. We let λ∗be the vector of Lagrange multipliers provided by the
ﬁrst-order necessary condition, and deﬁne the augmented cost ℓas in (1.26).
We also assume that f is C2. Consider the Hessian of ℓwith respect to x
evaluated at (x∗, λ∗):
ℓxx(x∗, λ∗) = ∇2f(x∗) +
m
X
i=1
λ∗
i ∇2hi(x∗).

INTRODUCTION
17
The second-order necessary condition says that this Hessian matrix must be
positive semideﬁnite on the tangent space to D at x∗, i.e., we must have
dT ℓxx(x∗, λ∗)d ≥0 for all d ∈Tx∗D. Note that this is weaker than asking
the above Hessian matrix to be positive semideﬁnite in the usual sense (on
the entire Rn).
The second-order suﬃcient condition says that a point x∗∈D is a strict
constrained local minimum of f if the ﬁrst-order necessary condition for
constrained optimality (1.24) holds and, in addition, we have
dT ℓxx(x∗, λ∗)d > 0
∀d such that ∇hi(x∗)·d = 0, i = 1, . . . , m. (1.28)
Again, here λ∗is the vector of Lagrange multipliers and ℓis the corre-
sponding augmented cost. Note that regularity of x∗is not needed for this
suﬃcient condition to be true. If x∗is in fact a regular point, then we know
(from our derivation of the ﬁrst-order necessary condition for constrained
optimality) that the condition imposed on d in (1.28) describes exactly the
tangent vectors to D at x∗. In other words, in this case (1.28) is equivalent
to saying that ℓxx(x∗, λ∗) is positive deﬁnite on the tangent space Tx∗D.
1.3
PREVIEW OF INFINITE-DIMENSIONAL OPTIMIZATION
In Section 1.2 we considered the problem of minimizing a function f : Rn →
R. Now, instead of Rn we want to allow a general vector space V , and in fact
we are interested in the case when this vector space V is inﬁnite-dimensional.
Speciﬁcally, V will itself be a space of functions. Let us denote a generic
function in V by y, reserving the letter x for the argument of y. (This x
will typically be a scalar, and has no relation with x ∈Rn from the previous
section.) The function to be minimized is a real-valued function on V , which
we now denote by J. Since J is a function on a space of functions, it is called
a functional. To summarize, we are minimizing a functional J : V →R.
Unlike in the case of Rn, there does not exist a “universal” function
space. Many diﬀerent choices for V are possible, and specifying the desired
space V is part of the problem formulation. Another issue is that in order
to deﬁne local minima of J over V , we need to specify what it means for two
functions in V to be close to each other. Recall that in the deﬁnition of a
local minimum in Section 1.2, a ball of radius ε with respect to the standard
Euclidean norm on Rn was used to deﬁne the notion of closeness. In the
present case we will again employ ε-balls, but we need to specify which norm
we are going to use. While in Rn all norms are equivalent (i.e., are within
a constant multiple of one another), in function spaces diﬀerent choices of a
norm lead to drastically diﬀerent notions of closeness. Thus, the ﬁrst thing
we need to do is become more familiar with function spaces and norms on
them.

18
CHAPTER 1
1.3.1
Function spaces, norms, and local minima
Typical function spaces that we will consider are spaces of functions from
some interval [a, b] to Rn (for some n ≥1). Diﬀerent spaces result from
placing diﬀerent requirements on the regularity of these functions. For ex-
ample, we will frequently work with the function space Ck([a, b], Rn), whose
elements are k-times continuously diﬀerentiable (here k ≥0 is an integer; for
k = 0 the functions are just continuous). Relaxing the Ck assumption, we
can arrive at the spaces of piecewise continuous functions or even measurable
functions (we will deﬁne these more precisely later when we need them). On
the other hand, stronger regularity assumptions lead us to C∞(smooth, or
inﬁnitely many times diﬀerentiable) functions or to real analytic functions
(the latter are C∞functions that agree with their Taylor series around every
point).
We regard these function spaces as linear vector spaces over R. Why
are they inﬁnite-dimensional? One way to see this is to observe that the
monomials 1, x, x2, x3, . . . are linearly independent. Another example of an
inﬁnite set of linearly independent functions is provided by the (trigonomet-
ric) Fourier basis.
As we already mentioned, we also need to equip our function space V
with a norm ∥· ∥. This is a real-valued function on V which is positive
deﬁnite (∥y∥> 0 if y ̸≡0), homogeneous (∥λy∥= |λ| · ∥y∥for all λ ∈R,
y ∈V ), and satisﬁes the triangle inequality (∥y + z∥≤∥y∥+ ∥z∥). The
norm gives us the notion of a distance, or metric, d(y, z) := ∥y −z∥. This
allows us to deﬁne local minima and enables us to talk about topological
concepts such as convergence and continuity (more on this in Section 1.3.4
below). We will see how the norm plays a crucial role in the subsequent
developments.
On the space C0([a, b], Rn), a commonly used norm is
∥y∥0 := max
a≤x≤b |y(x)|
(1.29)
where | · | is the standard Euclidean norm on Rn as before. Replacing the
maximum by a supremum, we can extend the 0-norm (1.29) to functions
that are deﬁned over an inﬁnite interval or are not necessarily continuous.
On C1([a, b], Rn), another natural candidate for a norm is obtained by adding
the 0-norms of y and its ﬁrst derivative:
∥y∥1 := max
a≤x≤b |y(x)| + max
a≤x≤b |y′(x)|.
(1.30)
This construction can be continued in the obvious way to yield the k-norm
on Ck([a, b], Rn) for each k. The k-norm can also be used on Cℓ([a, b], Rn)
for all ℓ≥k. There exist many other norms, such as for example the Lp

INTRODUCTION
19
norm
∥y∥Lp :=
 Z b
a
|y(x)|pdx
1/p
(1.31)
where p is a positive integer. In fact, the 0-norm (1.29) is also known as the
L∞norm.
We are now ready to formally deﬁne local minima of a functional. Let V
be a vector space of functions equipped with a norm ∥· ∥, let A be a subset
of V , and let J be a real-valued functional deﬁned on V (or just on A). A
function y∗∈A is a local minimum of J over A if there exists an ε > 0 such
that for all y ∈A satisfying ∥y −y∗∥< ε we have
J(y∗) ≤J(y).
Note that this deﬁnition of a local minimum is completely analogous to the
one in the previous section, modulo the change of notation x 7→y, D 7→A,
f 7→J, |·| 7→∥·∥(also, implicitly, Rn 7→V ). Strict minima, global minima,
and the corresponding notions of maxima are deﬁned in the same way as
before.
We will continue to refer to minima and maxima collectively as
extrema.
For the norm ∥·∥, we will typically use either the 0-norm (1.29) or the 1-
norm (1.30), with V being C0([a, b], Rn) or C1([a, b], Rn), respectively. In the
remainder of this section we discuss some general conditions for optimality
which apply to both of these norms. However, when we develop more speciﬁc
results later in calculus of variations, our ﬁndings for these two cases will be
quite diﬀerent.
1.3.2
First variation and ﬁrst-order necessary condition
To develop the ﬁrst-order necessary condition for optimality, we need a
notion of derivative for functionals. Let J : V →R be a functional on a
function space V , and consider some function y ∈V . The derivative of J at
y, which will now be called the ﬁrst variation, will also be a functional on V ,
and in fact this functional will be linear. To deﬁne it, we consider functions
in V of the form y + αη, where η ∈V and α is a real parameter (which can
be restricted to some interval around 0). The reader will recognize these
functions as inﬁnite-dimensional analogs of the points x∗+ αd around a
given point x∗∈Rn, which we utilized earlier.
A linear functional δJ|y : V →R is called the ﬁrst variation of J at y if
for all η and all α we have
J(y + αη) = J(y) + δJ|y (η)α + o(α)
(1.32)
where o(α) satisﬁes (1.6).
The somewhat cumbersome notation δJ|y (η)
is meant to emphasize that the linear term in α in the expansion (1.32)

20
CHAPTER 1
depends on both y and η. The requirement that δJ|y must be a linear func-
tional is understood in the usual sense: δJ|y (α1η1 + α2η2) = α1 δJ|y (η1) +
α2 δJ|y (η2) for all η1, η2 ∈V and α1, α2 ∈R.
The ﬁrst variation as deﬁned above corresponds to the so-called Gateaux
derivative of J, which is just the usual derivative of J(y + αη) with respect
to α (for ﬁxed y and η) evaluated at α = 0:
δJ|y (η) = lim
α→0
J(y + αη) −J(y)
α
.
(1.33)
In other words, if we deﬁne
g(α) := J(y + αη)
(1.34)
then
δJ|y (η) = g′(0)
(1.35)
and (1.32) reduces exactly to our earlier ﬁrst-order expansion (1.5).
Now, suppose that y∗is a local minimum of J over some subset A of V .
We call a perturbation3 η ∈V admissible (with respect to the subset A) if
y∗+ αη ∈A for all α suﬃciently close to 0. It follows from our deﬁnitions
of a local minimum and an admissible perturbation that J(y∗+ αη) as a
function of α has a local minimum at α = 0 for each admissible η. Let us
assume that the ﬁrst variation δJ|y∗exists (which is of course not always
the case) so that we have (1.32).
Applying the same reasoning that we
used to derive the necessary condition (1.7) on the basis of (1.5), we quickly
arrive at the ﬁrst-order necessary condition for optimality: For all
admissible perturbations η, we must have
δJ|y∗(η) = 0
(1.36)
As in the ﬁnite-dimensional case, the ﬁrst-order necessary condition applies
to both minima and maxima.
When we were studying a minimum x∗of f : Rn →R with the help
of the function g(α) := f(x∗+ αd), it was easy to translate the equality
g′(0) = 0 via the formula (1.10) into the necessary condition ∇f(x∗) = 0.
The necessary condition (1.36), while conceptually very similar, is much
less constructive. To be able to apply it, we need to learn how to compute
the ﬁrst variation of some useful functionals. This subject will be further
discussed in the next chapter; for now, we oﬀer an example for the reader
to work out.
3With a slight abuse of terminology, we call η a perturbation even though the actual
perturbation is αη.

INTRODUCTION
21
Exercise 1.5
Consider the space V = C0([0, 1], R), let ϕ : R →R be
a C1 function, and deﬁne the functional J on V by J(y) =
R 1
0 ϕ(y(x))dx.
Show that its ﬁrst variation exists and is given by the formula δJ|y (η) =
R 1
0 ϕ′(y(x))η(x)dx.
□
Our notion of the ﬁrst variation, deﬁned via the expansion (1.32), is in-
dependent of the choice of the norm on V . This means that the ﬁrst-order
necessary condition (1.36) is valid for every norm. To obtain a necessary
condition better tailored to a particular norm, we could deﬁne δJ|y diﬀer-
ently, by using the following expansion instead of (1.32):
J(y + η) = J(y) + δJ|y (η) + o(∥η∥).
(1.37)
The diﬀerence with our original formulation is subtle but substantial. The
earlier expansion (1.32) describes how the value of J changes with α for each
ﬁxed η. In (1.37), the higher-order term is a function of ∥η∥and so the ex-
pansion captures the eﬀect of all η at once (while α is no longer needed). We
remark that the ﬁrst variation deﬁned via (1.37) corresponds to the so-called
Fr´echet derivative of J, which is a stronger diﬀerentiability notion than the
Gateaux derivative (1.33). In fact, (1.37) suggests constructing more gen-
eral perturbations: instead of working with functions of the form y + αη,
where η is ﬁxed and α is a scalar parameter, we can consider perturbed
functions y + η which can approach y in a more arbitrary manner as ∥η∥
tends to 0 (multiplying η by a vanishing parameter is just one possibility).
This generalization is conceptually similar to that of passing from the lines
x∗+ αd used in Section 1.2.1 to the curves x(α) utilized in Section 1.2.2.
We will start seeing perturbations of this kind in Chapter 3.
In what follows, we retain our original deﬁnition of the ﬁrst variation in
terms of (1.32). It is somewhat simpler to work with and is adequate for our
needs (at least through Chapter 2). While the norm-dependent formulation
could potentially provide sharper conditions for optimality, it takes more
work to verify (1.37) for all η compared to verifying (1.32) for a ﬁxed η.
Besides, we will eventually abandon the analysis based on the ﬁrst variation
altogether in favor of more powerful tools. However, it is useful to be aware
of the alternative formulation (1.37), and we will occasionally make some
side remarks related to it. This issue will resurface in Chapter 3 where,
although the alternative deﬁnition (1.37) of the ﬁrst variation will not be
speciﬁcally needed, we will use more general perturbations along the lines
of the preceding discussion.
1.3.3
Second variation and second-order conditions
A real-valued functional B on V × V is called bilinear if it is linear in each
argument (when the other one is ﬁxed). Setting Q(y) := B(y, y) we then

22
CHAPTER 1
obtain a quadratic functional, or quadratic form, on V .
This is a direct
generalization of the corresponding familiar concepts for ﬁnite-dimensional
vector spaces.
A quadratic form δ2J

y : V →R is called the second variation of J at y
if for all η ∈V and all α we have
J(y + αη) = J(y) + δJ|y (η)α + δ2J

y (η)α2 + o(α2).
(1.38)
This exactly corresponds to our previous second-order expansion (1.12) for
the function g given by (1.34). Repeating the same argument we used earlier
to prove (1.14), we easily establish the following second-order necessary
condition for optimality: If y∗is a local minimum of J over A ⊂V ,
then for all admissible perturbations η we have
δ2J

y∗(η) ≥0
(1.39)
In other words, the second variation of J at y∗must be positive semideﬁnite
on the space of admissible perturbations. For local maxima, the inequality
in (1.39) is reversed. Of course, the usefulness of the condition will depend
on our ability to compute the second variation of the functionals that we
will want to study.
Exercise 1.6
Consider the same functional J as in Exercise 1.5, but as-
sume now that ϕ is C2. Derive a formula for the second variation of J (make
sure that it is indeed a quadratic form).
□
What about a second-order suﬃcient condition for optimality? By anal-
ogy with the second-order suﬃcient condition (1.16) which we derived for
the ﬁnite-dimensional case, we may guess that we need to combine the ﬁrst-
order necessary condition (1.36) with the strict-inequality counterpart of the
second-order necessary condition (1.39), i.e.,
δ2J

y∗(η) > 0
(1.40)
(this should again hold for all admissible perturbations η with respect to a
subset A of V over which we want y∗to be a minimum). We would then hope
to show that for y = y∗the second-order term in (1.38) dominates the higher-
order term o(α2), which would imply that y∗is a strict local minimum (since
the ﬁrst-order term is 0). Our earlier proof of suﬃciency of (1.16) followed
the same idea. However, examining that proof more closely, the reader will
discover that in the present case the argument does not go through.
We know that there exists an ε > 0 such that for all nonzero α with
|α| < ε we have |o(α2)| < δ2J

y∗(η)α2. Using this inequality and (1.36),
we obtain from (1.38) that J(y∗+ αη) > J(y∗). Note that this does not yet

INTRODUCTION
23
prove that y∗is a (strict) local minimum of J. According to the deﬁnition of
a local minimum, we must show that J(y∗) is the lowest value of J in some
ball around y∗with respect to the selected norm ∥· ∥on V . The problem
is that the term o(α2) and hence the above ε depend on the choice of the
perturbation η. In the ﬁnite-dimensional case we took the minimum of ε
over all perturbations of unit length, but we cannot do that here because
the unit sphere in the inﬁnite-dimensional space V is not compact and the
Weierstrass Theorem does not apply to it (see Section 1.3.4 below).
One way to resolve the above diﬃculty would be as follows. The ﬁrst
step is to strengthen the condition (1.40) to
δ2J

y∗(η) ≥λ∥η∥2
(1.41)
for some number λ > 0. The property (1.41) does not automatically fol-
low from (1.40), again because we are in an inﬁnite-dimensional space.
(Quadratic forms satisfying (1.41) are sometimes called uniformly positive
deﬁnite.) The second step is to modify the deﬁnitions of the ﬁrst and sec-
ond variations by explicitly requiring that the higher-order terms decay uni-
formly with respect to ∥η∥. We already mentioned such an alternative def-
inition of the ﬁrst variation via the expansion (1.37). Similarly, we could
deﬁne δ2J

y via the following expansion in place of (1.38):
J(y + η) = J(y) + δJ|y (η) + δ2J

y (η) + o(∥η∥2).
(1.42)
Adopting these alternative deﬁnitions and assuming that (1.36) and (1.41)
hold, we could easily prove optimality by noting that |o(∥η∥2)| < λ∥η∥2
when ∥η∥is small enough.
With our current deﬁnitions of the ﬁrst and second variations in terms
of (1.32) and (1.38), we do not have a general second-order suﬃcient condi-
tion for optimality. However, in variational problems that we are going to
study, the functional J to be minimized will take a speciﬁc form. This addi-
tional structure will allow us to derive conditions under which second-order
terms dominate higher-order terms, resulting in optimality. The above dis-
cussion was given mainly for illustrative purposes, and will not be directly
used in the sequel.
1.3.4
Global minima and convex problems
Regarding global minima of J over a set A ⊂V , much of the discussion on
global minima given at the end of Section 1.2.1 carries over to the present
case. In particular, the Weierstrass Theorem is still valid, provided that
compactness of A is understood in the sense of the second or third deﬁni-
tion given on page 10 (existence of ﬁnite subcovers or sequential compact-
ness). These two deﬁnitions of compactness are equivalent for linear vector

24
CHAPTER 1
spaces equipped with a norm (or, more generally, a metric). On the other
hand, closed and bounded subsets of an inﬁnite-dimensional vector space are
not necessarily compact—we already mentioned noncompactness of the unit
sphere—and the Weierstrass Theorem does not apply to them; see the next
exercise. We note that since our function space V has a norm, the notions
of continuity of J and convergence, closedness, boundedness, and openness
in V with respect to this norm are deﬁned exactly as their familiar counter-
parts in Rn. We leave it to the reader to write down precise deﬁnitions or
consult the references given at the end of this chapter.
Exercise 1.7
Give an example of a function space V , a norm on V , a
closed and bounded subset A of V , and a continuous functional J on V such
that a global minimum of J over A does not exist. (Be sure to demonstrate
that all the requested properties hold.)
□
If J is a convex functional and A ⊂V is a convex set, then the optimiza-
tion problem enjoys the same properties as the ones mentioned at the end of
Section 1.2.1 for ﬁnite-dimensional convex problems. Namely, a local mini-
mum is automatically a global one, and the ﬁrst-order necessary condition
is also a suﬃcient condition for a minimum. (Convexity of a functional and
convexity of a subset of an inﬁnite-dimensional linear vector space are de-
ﬁned exactly as the corresponding standard notions in the ﬁnite-dimensional
case.) However, imposing extra assumptions to ensure convexity of J would
severely restrict the classes of problems that we want to study. In this book,
we focus on general theory that applies to not necessarily convex problems;
we will not directly use results from convex optimization. Nevertheless, some
basic concepts from (ﬁnite-dimensional) convex analysis will be important
for us later, particularly when we derive the maximum principle.
1.4
NOTES AND REFERENCES FOR CHAPTER 1
Success stories of optimal control theory in various applications are too nu-
merous to be listed here; see [CEHS87, Cla10, ST05, Swa84] for some exam-
ples from engineering and well beyond. The reader interested in applications
will easily ﬁnd many other references.
The material in Section 1.2 can be found in standard texts on optimiza-
tion, such as [Lue84] or [Ber99]. See also Sections 5.2-5.4 of the book [AF66],
which will be one of our main references for the optimal control chapters.
Complete proofs of the results presented in Section 1.2.2, including the fact
that the condition (1.20) is suﬃcient for d to be a tangent vector, are given
in [Lue84, Chapter 10]. The alternative argument based on the inverse func-
tion theorem is adopted from [Mac05, Section 1.4]. The necessary condition
in terms of Lagrange multipliers can also be derived from a cone separation

INTRODUCTION
25
property (via Farkas’s lemma) as shown, e.g., in [Ber99, Section 3.3.6]; we
will see this type of reasoning in the proof of the maximum principle in
Chapter 4.
Section 1.3 is largely based on [GF63], which will be our main reference
for calculus of variations; function spaces, functionals, and the ﬁrst variation
are introduced in the ﬁrst several sections of that book, while the second
variation is discussed later in Chapter 5. Essentially the same material but
in condensed form can be found in [AF66, Section 5.5]. In [GF63] as well as
in [Mac05] the ﬁrst and second variations are deﬁned via (1.37) and (1.42),
while other sources such as [You80] follow the approach based on (1.32)
and (1.38).
For further background on function spaces and relevant topological con-
cepts, the reader can consult [Rud76] or [Sut75] (the latter text is somewhat
more advanced). Another recommended reference on these topics is [Lue69],
where Gateaux and Fr´echet derivatives and their role in functional minimiza-
tion are also thoroughly discussed. A general treatment of convex functionals
and their minimization is given in [Lue69, Chapter 7]; for convexity-based
results more speciﬁc to calculus of variations and optimal control, see the
monograph [Roc74], the more recent papers [RW00] and [GS07], and the
references therein.

Chapter Two
Calculus of Variations
2.1
EXAMPLES OF VARIATIONAL PROBLEMS
We begin this chapter by describing a few examples of classical variational
problems, i.e., problems in which a path is to be chosen from a given family
of admissible paths so as to minimize the value of some functional. These
examples will help us motivate the general problem formulation and will
serve as benchmarks for the theory that we will be developing.
2.1.1
Dido’s isoperimetric problem
According to a legend about the foundation of Carthage (around 850 B.C.),
Dido purchased from a local king the land along the North African coastline
that could be enclosed by the hide of an ox. She sliced the hide into very
thin strips, tied them together, and was able to enclose a sizable area which
became the city of Carthage.
Let us formulate this as an optimization problem. Assume that the coast
is a straight line (represented by the x-axis in Figure 2.1). The hide strips
tied together correspond to a curve of a ﬁxed length. The problem is to
maximize the area under this curve.
x
y
a
b
Figure 2.1: Dido’s problem
26

CALCULUS OF VARIATIONS
27
More formally, admissible curves are graphs of continuous functions1
y : [a, b] →R satisfying the endpoint constraints y(a) = y(b) = 0. The area
under such a curve is
J(y) =
Z b
a
y(x)dx
which is the quantity to be maximized.
Of course, it is easy to convert
this to a minimization problem by working with −J. Keeping in mind the
original problem context, we should actually restrict y to be nonnegative
(so it does not go into the ocean) or at least replace y by max{0, y} in the
deﬁnition of J; but even without these modiﬁcations, it is clear that only
nonnegative y can be optimal. Assuming that the curve is diﬀerentiable (at
least almost everywhere), we can write the length constraint as
Z b
a
p
1 + (y′(x))2dx = C0
(2.1)
where C0 is a ﬁxed constant.
Indeed, by the Pythagorean Theorem the
integrand in (2.1) is the arclength element.
At this point the reader can probably guess what the optimal curve
should be. It is an arc of a circle. This solution was known to Zenodorus (2nd
century B.C.), although its rigorous derivation requires tools from calculus
of variations which will be presented in this chapter.
Note that since we are representing admissible curves by graphs of func-
tions y, we are excluding circular arcs that are longer than a semicircle.
In the context of Dido’s problem, we can think of the interval [a, b] as not
actually being speciﬁed in advance but instead as being chosen based on
the length constraint. Then, once we know that the optimal curve must
be a circular arc, it is straightforward to check that the optimal choice of
the interval is the one in which the circular arc of length C0 is precisely a
semicircle, with the interval serving as the circle’s diameter (and thus having
length 2C0/π).
2.1.2
Light reﬂection and refraction
In free space, light travels along a path of shortest distance—which is of
course a straight line. This is already a solution to a variational problem,
albeit a very simple one. More interesting situations arise when a light ray
encounters the edge of a medium. Two basic phenomena are reﬂection and
refraction of light (see Figure 2.2).
1Actually, the curve in Figure 2.1 is not the graph of a function. There is a tendency
to ignore this diﬀerence between curves and graphs of functions when formulating calculus
of variations problems.

28
CHAPTER 2
θ
θ
θ1
θ2
air
water
Figure 2.2: Light reﬂection and refraction
In the case of reﬂection, Hero of Alexandria (who probably lived in the
1st century A.D.) suggested that light still takes the path of shortest distance
among nearby paths. When the reﬂecting surface is a plane, one can argue
using some simple geometry that the angles between the normal to this
plane and the light ray before and after the reﬂection must then be the
same. This result generalizes to curved reﬂecting surfaces, although proving
it rigorously is not trivial (see Exercise 1.4 in the previous chapter).
Analyzing refraction is more challenging. Ptolemy made a list of angle
pairs (θ1, θ2) corresponding to situations like the one depicted on the right in
Figure 2.2. His list dates back to 140 A.D. and contains quite a few values.
(This is ancient Greek experimental physics!) A pattern in Ptolemy’s results
was found only much later, in 1621, when Snell stated his law:
sin θ1 = n sin θ2
(2.2)
where n is the ratio of the speeds of light in the two media (n ≈1.33
when light passes from air to water).
A satisfactory explanation of this
behavior was ﬁrst given by Fermat around 1650. Fermat’s principle states
that, although the light does not take the path of shortest distance any
more, it travels along the path of shortest time. Snell’s law can be derived
from Fermat’s principle by diﬀerential calculus; in fact, this was one of the
examples that Leibniz gave to illustrate the power of calculus in his original
1684 calculus monograph.
The problems of light reﬂection and refraction are mentioned here mainly
for historical reasons, and we do not proceed to mathematically formalize
them. (However, we will revisit light reﬂection in Section 7.4.3.)
2.1.3
Catenary
Suppose that a chain of a given length, with uniform mass density, is sus-
pended freely between two ﬁxed points (see Figure 2.3). What will be the

CALCULUS OF VARIATIONS
29
shape of this chain? This question was posed by Galileo in the 1630s, and
he claimed—incorrectly—that the solution is a parabola.
x
y
a
b
Figure 2.3: A catenary
Mathematically, the chain is described by a continuous function y :
[a, b] →[0, ∞), and the two suspension points are speciﬁed by endpoint
constraints of the form y(a) = y0, y(b) = y1. The length constraint is again
given by (2.1). In the ﬁgure we took y0 and y1 to be equal, and also assumed
that the suspension points are high enough and far enough apart compared
to the length C0 so that the chain does not touch the ground (y(x) > 0
for all x). The chain will take the shape of minimal potential energy. This
amounts to saying that the y-coordinate of its center of mass should be
minimized. Since the mass density is uniform, we integrate the y-coordinate
of the point along the curve with respect to the arclength and obtain the
functional
J(y) =
Z b
a
y(x)
p
1 + (y′(x))2dx
(the actual center of mass is J(y)/C0). Thus the problem is to minimize
this functional subject to the length constraint (2.1). As in Dido’s problem,
we need to assume that y is diﬀerentiable (at least almost everywhere); this
time we do it also to ensure that the cost is well deﬁned, because y′ appears
inside the integral in J.
The correct description of the catenary curve was obtained by Johann
Bernoulli in 1691. It is given by the formula
y(x) = c cosh(x/c),
c > 0
(2.3)
(modulo a translation of the origin in the (x, y)-plane), unless the chain is
suspended too low and touches the ground. The name “catenary” is derived
from the Latin word catena (chain).

30
CHAPTER 2
2.1.4
Brachistochrone
Given two ﬁxed points in a vertical plane, we want to ﬁnd a path between
them such that a particle sliding without friction along this path takes the
shortest possible time to travel from one point to the other (see Figure 2.4).
The name “brachistochrone” was given to this problem by Johann Bernoulli;
it comes from the Greek words βρ´αχιστoς (shortest) and χρ´oνoς (time).
x
y
a
b
Figure 2.4: The brachistochrone problem
It is tempting to think that the solution is a straight line, but this is not
the case. For example, if the two points are at the same height, then the
particle will not move if placed on the horizontal line (we are assuming that
the particle is initially at rest); on the other hand, traveling along a lower
semicircle, the particle will reach the other point in ﬁnite time. Galileo, who
studied this problem in 1638, showed that for points at diﬀerent heights, it
is also true that sliding down a straight line is slower than along an arc of a
circle. Galileo thought that an arc of a circle is the optimal path, but this
is not true either.
The travel time along a path is given by the integral of the ratio of the
arclength to the particle’s speed. We already know the expression for the
arclength from (2.1). To obtain a formula for the speed, we use the law of
conservation of energy. Let us choose coordinates as shown in Figure 2.4:
the y-axis points downward and the x-axis passes through the initial point
(the higher one). Then the kinetic and potential energy are both 0 initially,
thus the total energy is always 0 and we have
mv2
2
−mgy = 0
(2.4)
where v is the particle’s speed, m is its mass, and g is the gravitational
constant. Choosing suitable units, we can assume without loss of generality
that m = 1 and g = 1/2. Then (2.4) simpliﬁes to
v = √y
(2.5)

CALCULUS OF VARIATIONS
31
and so our problem becomes that of minimizing the value of the functional
J(y) =
Z b
a
p
1 + (y′(x))2
p
y(x)
dx
(2.6)
over all (almost everywhere) diﬀerentiable curves y connecting the two given
points with x-coordinates a and b.
Johann Bernoulli posed the brachistochrone problem in 1696 as a chal-
lenge to his contemporaries.
Besides Bernoulli himself, correct solutions
were obtained by Leibniz, Newton, Johann’s brother Jacob Bernoulli, and
others. The optimal curves are cycloids, deﬁned by the parametric equations
x(θ) = a + c(θ −sin θ),
y(θ) = c(1 −cos θ)
(2.7)
where the parameter θ takes values between 0 and 2π and c > 0 is a constant.
These equations describe the curve traced by a point on a circle of radius c
as this circle rolls without slipping on the horizontal axis (see Figure 2.5). A
somewhat surprising outcome is that the fastest motion may involve dipping
below the desired height to build up speed and then coming back up (whether
or not this happens depends on the relative location of the two points). This
principle also applies to changing an airplane’s altitude.
Figure 2.5: A cycloid
It is interesting to note that Johann Bernoulli’s original solution to the
brachistochrone problem was based on Snell’s law (2.2) for light refraction.
In view of equation (2.5), we can treat the particle as a light ray traveling
in a medium where the speed of light is proportional to the square root
of the height. In a discretized version of this situation, the vertical plane
is divided into horizontal strips and the speed of light is constant in each
strip. The light ray follows a straight line within each strip and bends at
the boundaries according to Snell’s law. From this piecewise linear path, a
cycloid is obtained in the limit as the heights of the strips tend to 0. This
solution can be obtained more easily (and more rigorously) using methods
from calculus of variations and optimal control theory.
Exercise 2.1
Find another example of a variational problem. Describe it
verbally ﬁrst, then formalize it by specifying admissible curves and giving

32
CHAPTER 2
an expression for the functional to be minimized or maximized. You do not
need to solve it.
□
2.2
BASIC CALCULUS OF VARIATIONS PROBLEM
The reader has probably observed that the problems of Dido, catenary,
and brachistochrone, although diﬀerent in their physical meaning, all take
essentially the same mathematical form. We are now ready to turn to a
general problem formulation that captures these examples and many related
ones as special cases. For the moment we ignore the issue of constraints such
as the arclength constraint (2.1), which was present in Dido’s problem and
the catenary problem; we will incorporate constraints of this type later (in
Section 2.5).
The simplest version of the calculus of variations problem can be stated
as follows. Consider a function L : R × R × R →R.
Basic Calculus of Variations Problem: Among all C1 curves y : [a, b] →R
satisfying given boundary conditions
y(a) = y0,
y(b) = y1
(2.8)
ﬁnd (local) minima of the cost functional
J(y) :=
Z b
a
L(x, y(x), y′(x))dx.
(2.9)
Since y takes values in R, it represents a single planar curve connecting
the two ﬁxed points (a, y0) and (b, y1). This is the single-degree-of-freedom
case. In the multiple-degrees-of-freedom case, one has y : [a, b] →Rn and
accordingly L : R × Rn × Rn →R. This generalization is useful for treating
spatial curves (n = 3) or for describing the motion of many particles; the
latter setting was originally proposed by Lagrange in his 1788 monograph
M´ecanique Analytique. The assumption that y ∈C1 is made to ensure that
J is well deﬁned (of course we do not need it if y′ does not appear in L).
We can allow y′ to be discontinuous at some points; we will discuss such
situations and see their importance soon.
The function L is called the Lagrangian, or the running cost. It is clear
that a maximization problem can always be converted into a minimization
problem by ﬂipping the sign of L. In the analysis that follows, it will be
important to remember the following point: Even though y and y′ are the
position and velocity along the curve, L is to be viewed as a function of
three independent variables.
To emphasize this fact, we will sometimes
write L = L(x, y, z). When deriving optimality conditions, we will need to
impose some diﬀerentiability assumptions on L.

CALCULUS OF VARIATIONS
33
2.2.1
Weak and strong extrema
We recall from Section 1.3 that in order to deﬁne local optimality, we must
ﬁrst select a norm, and on the space of C1 curves y : [a, b] →R there
are two natural candidates for the norm: the 0-norm (1.29) and the 1-
norm (1.30). Extrema (minima and maxima) of J with respect to the 0-
norm are called strong extrema, and those with respect to the 1-norm are
called weak extrema.
These two notions will be central to our subsequent developments, and
so it is useful to reﬂect on them for a little while until the distinction be-
tween the two types of extrema becomes clear and there is no possibility of
confusing them. If a C1 curve y∗is a strong extremum, then it is automati-
cally a weak one, but the converse is not true. The reason is that an ε-ball
around y∗with respect to the 0-norm contains the ε-ball with respect to the
1-norm for the same ε, as is clear from the norm deﬁnitions; on the other
hand, the ε-ball with respect to the 1-norm does not contain the ε′-ball with
respect to the 0-norm for any ε′, no matter how small. In other words, it is
harder to satisfy J(y∗) ≤J(y) for all y close enough to y∗if we understand
closeness in the sense of the 0-norm. Closeness in the sense of the 1-norm is
a more restrictive condition, since the derivatives of y and y∗also have to
be close, meaning that there are fewer perturbations to check than for the
0-norm. We will see that, for the same reason, studying weak extrema is
easier than studying strong extrema.
On the other hand, it will become evident later that the concept of a
weak minimum is not very suitable in optimal control. Indeed, an optimal
trajectory y∗should give a lower cost than all nearby trajectories y, and
there is no compelling reason to take into account the diﬀerence between
the derivatives of y∗and y. Also, as we already mentioned, requiring y to be
a C1 curve is often too restrictive. Speciﬁcally, we will want to allow curves
y which are continuous everywhere on [a, b] and whose derivative y′ exists
everywhere except possibly a ﬁnite number of points in [a, b] and is contin-
uous and bounded between these points. Let us agree to call such curves
piecewise C1, to reﬂect the fact that they are concatenations of ﬁnitely many
C1 pieces. (We could deﬁne the class of admissible curves more precisely us-
ing the notion of an absolutely continuous function; we will revisit this issue
in Section 3.3 as we make the transition to optimal control.) If we use the
0-norm, then it makes no diﬀerence whether y is C1 or piecewise C1 or just
C0; this is another advantage of the 0-norm over the 1-norm.
In view of the above remarks, it seems natural to ﬁrst obtain some basic
tools for studying weak minima and then proceed to develop more advanced
tools for investigating strong minima. This is essentially what we will do.
The next example illustrates some of the points that we just made regarding
the 0-norm versus the 1-norm. The exercise that follows should help the

34
CHAPTER 2
reader to better grasp the concepts of weak and strong minima; it is to be
solved using only the deﬁnitions.
Example 2.1
Consider the three curves y0, y1, and y2 shown in Fig-
ure 2.6. We think of y1 and y2 as results of perturbations of y0 which are
small in magnitude. Then y1 is close to y0 with respect to both the 0-norm
and the 1-norm, but y2 is close to y0 only with respect to the 0-norm. In-
deed, the 1-norm of y2 −y0 is large because the derivative of y2 is large in
magnitude due to the sharp spikes. (Technically speaking, to be able to take
the 1-norm we should smoothen the corners of y2.)
y0
y1
y2
Figure 2.6: Closeness in weak and strong sense
We can also consider another curve y3 (not shown in the ﬁgure) which is
of the same form as y2 but slightly out of phase with it. Then y2 and y3 will be
close with respect to the 0-norm but not the 1-norm. We can think of y2 and
y3 as solutions of the control system dy/dx = u ∈{−1, 1}. A small diﬀerence
in phase amounts to a slight shift of the switching times of the control u.
Such a small perturbation of the control should be admissible, which is why
the 0-norm and strong extrema provide a more reasonable notion of local
optimality.
□
Exercise 2.2
Consider the problem of minimizing the functional
J(y) =
Z 1
0
(y′(x))2 1 −(y′(x))2
dx
subject to the boundary conditions y(0) = y(1) = 0. Is the curve y ≡0 a
weak minimum (over C1 curves)? Is it a strong minimum (over piecewise
C1 curves)? Is there another curve that is a strong minimum?
□
2.3
FIRST-ORDER NECESSARY CONDITIONS FOR WEAK
EXTREMA
In this section we will derive the most fundamental result in calculus of
variations: the Euler-Lagrange equation. Unless stated otherwise, we will
be working with the Basic Calculus of Variations Problem deﬁned in Sec-
tion 2.2. Thus our function space V is C1([a, b], R), the subset A consists
of functions y ∈V satisfying the boundary conditions (2.8), and the func-
tional J to be minimized takes the form (2.9). The Euler-Lagrange equation

CALCULUS OF VARIATIONS
35
provides a more explicit characterization of the ﬁrst-order necessary condi-
tion (1.36) for this situation.
In deriving the Euler-Lagrange equation, we will follow the basic varia-
tional approach presented in Section 1.3.2 and consider nearby curves of the
form
y + αη
(2.10)
where the perturbation η : [a, b] →R is another C1 curve and α varies in an
interval around 0 in R. For α close to 0, these perturbed curves are close to y
in the sense of the 1-norm. For this reason, the resulting ﬁrst-order necessary
condition handles weak extrema. However, since a C1 strong extremum is
automatically a weak extremum, every necessary condition for the latter is
necessary for the former as well. Therefore, the Euler-Lagrange equation
will apply to both weak and strong extrema, as long as we insist on working
with C1 functions. As we know, using the 0-norm allows us to relax the
C1 requirement, but this will in turn necessitate a diﬀerent approach (i.e., a
reﬁned perturbation family) when developing optimality conditions. We will
thus refer to the conditions derived in this section as necessary conditions
for weak extrema, in order to distinguish them from sharper conditions to be
given later which apply speciﬁcally to strong extrema. Similar remarks apply
to other necessary conditions to be derived in this chapter. The suﬃcient
condition of Section 2.6.2, on the other hand, will apply to weak minima
only.
2.3.1
Euler-Lagrange equation
We continue to follow the notational convention of Chapter 1 and denote
by Lx, Ly, Lz, Lxx, Lxy, etc. the partial derivatives of the Lagrangian L =
L(x, y, z). To keep things simple, we assume that all derivatives appearing
in our calculations exist and are continuous. While we will not focus on
spelling out the weakest possible regularity assumptions on L, we will make
some remarks to clarify this issue in Section 2.3.3.
Let y = y(x) be a given test curve in A. For a perturbation η in (2.10)
to be admissible, the new curve (2.10) must again satisfy the boundary
conditions (2.8). Clearly, this is true if and only if
η(a) = η(b) = 0.
(2.11)
In other words, we must only consider perturbations vanishing at the end-
points. Now, the ﬁrst-order necessary condition (1.36) says that if y is a local
extremum of J, then for every η satisfying (2.11) we must have δJ|y (η) = 0.
(We denoted extrema by y∗in (1.36) but here we drop the asterisks to avoid
overly cluttered notation.) In the present case we want to go further and use

36
CHAPTER 2
the speciﬁc form of J given by (2.9) to arrive at a more explicit condition
in terms of the Lagrangian L.
Recall that the ﬁrst variation δJ|y was deﬁned via
J(y + αη) = J(y) + δJ|y (η)α + o(α).
(2.12)
The left-hand side of (2.12) is
J(y + αη) =
Z b
a
L(x, y(x) + αη(x), y′(x) + αη′(x))dx.
(2.13)
We can write down its ﬁrst-order Taylor expansion with respect to α by
expanding the expression inside the integral with the help of the chain rule:
J(y + αη) =
Z b
a
 L(x, y(x), y′(x)) + Ly(x, y(x), y′(x))αη(x)
+ Lz(x, y(x), y′(x))αη′(x) + o(α)

dx.
Matching this with the right-hand side of (2.12), we deduce that the ﬁrst
variation is
δJ|y (η) =
Z b
a
 Ly(x, y(x), y′(x))η(x) + Lz(x, y(x), y′(x))η′(x)

dx.
(2.14)
Note that, proceeding slightly diﬀerently, we could arrive at the same result
by remembering from (1.33)–(1.35) that
δJ|y (η) = lim
α→0
J(y + αη) −J(y)
α
=
d
dα

α=0
J(y + αη)
and using diﬀerentiation under the integral sign on the right-hand side
of (2.13).
Exercise 2.3
Prove that δJ|y given by (2.14) also satisﬁes the alternative
deﬁnition of the ﬁrst variation based on (1.37) in which the norm is the 1-
norm. Is this true for the 0-norm as well?
□
We see that the ﬁrst variation depends not just on η but also on η′.
This is not surprising since L has y′ as one of its arguments. However, we
can eliminate the dependence on η′ if we apply integration by parts to the
second term on the right-hand side of (2.14):
δJ|y (η) =
Z b
a

Ly(x, y(x), y′(x))η(x) −η(x) d
dxLz(x, y(x), y′(x))

dx
+ Lz(x, y(x), y′(x))η(x)
b
a
(2.15)

CALCULUS OF VARIATIONS
37
where the last term is 0 when η satisﬁes the boundary conditions (2.11).
Thus we conclude that if y is an extremum, then we must have
Z b
a

Ly(x, y(x), y′(x)) −d
dxLz(x, y(x), y′(x))

η(x)dx = 0
(2.16)
for all C1 curves η vanishing at the endpoints x = a and x = b.
The condition (2.16) does not yet give us a practically useful test for
optimality, because we would need to check it for all admissible perturbations
η. However, it is logical to suspect that the only way (2.16) can hold is if
the term inside the parentheses—which does not depend on η—equals 0 for
all x. The next lemma shows that this is indeed the case.
Lemma 2.1. If a continuous function ξ : [a, b] →R is such that
Z b
a
ξ(x)η(x)dx = 0
for all C1 functions η : [a, b] →R with η(a) = η(b) = 0, then ξ ≡0.
Proof. Suppose that ξ(¯x) ̸= 0 for some ¯x ∈[a, b]. By continuity, ξ is
then nonzero and maintains the same sign on some subinterval [c, d] con-
taining ¯x. Just for concreteness, let us say that ξ is positive on [c, d].
c
η
d
a
b
x
y
Figure 2.7: The graph of η
Construct a function η ∈C1([a, b], R) that is positive on (c, d) and 0
everywhere else (see Figure 2.7). For example, we can set η(x) = (x−c)2(x−
d)2 for x ∈[c, d] and η(x) = 0 otherwise. This gives
R b
a ξ(x)η(x)dx > 0, and
we reach a contradiction.
□
It follows from (2.16) and Lemma 2.1 that for y(·) to be an extremum,
a necessary condition is
Ly(x, y(x), y′(x)) = d
dxLz(x, y(x), y′(x))
∀x ∈[a, b].
(2.17)

38
CHAPTER 2
This is the celebrated Euler-Lagrange equation providing the ﬁrst-order
necessary condition for optimality. It is often written in the shorter
form
Ly = d
dxLy′
(2.18)
We must keep in mind, however, that the correct interpretation of the Euler-
Lagrange equation is (2.17): y and y′ are treated as independent variables
when computing the partial derivatives Ly and Ly′, then one plugs in for
these variables the position y(x) and velocity y′(x) of the curve, and ﬁ-
nally the diﬀerentiation with respect to x is performed using the chain rule.
Written out in detail, the right-hand side of (2.17) is
d
dxLz(x, y(x), y′(x)) = Lzx(x, y(x), y′(x)) + Lzy(x, y(x), y′(x))y′(x)
+ Lzz(x, y(x), y′(x))y′′(x).
(2.19)
It might not be necessary to actually perform all of these operations, though,
as the next example demonstrates.
Example 2.2
Let us ﬁnd the shortest path between two points in the plane.
Of course the answer is obvious, but let us see how we can obtain it from the
Euler-Lagrange equation. We already discussed the functional that describes
the length of a curve in Section 2.1, in the context of Dido’s problem and the
catenary problem (where it played the role of a side constraint rather than
the cost functional). This length functional is J(y) =
R b
a
p
1 + (y′(x))2dx,
hence the Lagrangian is L(x, y, z) =
√
1 + z2. Since Ly = 0, the Euler-
Lagrange equation reads
d
dxLz(x, y(x), y′(x)) = 0. This implies that Lz stays
constant along the shortest path. We have Lz = z/
√
1 + z2, thus
Lz(x, y(x), y′(x)) =
y′(x)
p
1 + (y′(x))2 .
(2.20)
For this to be constant, y′ must be constant, i.e., the path must be a straight
line. The unique straight line connecting two given points is clearly the short-
est path between them. Note that we did not need to compute the derivative
d
dxLz(x, y(x), y′(x)).
□
The functional J to be minimized is given by the integral of the La-
grangian L along a path, while the Euler-Lagrange equation involves deriva-
tives of L and must hold for every point on the optimal path; observe that
the integral has disappeared.
The underlying reason is that if a path is
optimal, then every inﬁnitesimally small portion of it is optimal as well (no
“shortcuts” are possible).
The exact mechanism by which we pass from
the statement that the integral is minimized to the pointwise condition is
revealed by Lemma 2.1 and its proof.

CALCULUS OF VARIATIONS
39
Trajectories satisfying the Euler-Lagrange equation (2.18) are called ex-
tremals (of the functional J). Since the Euler-Lagrange equation is only a
necessary condition for optimality, not every extremal is an extremum. We
see from (2.19) that the equation (2.18) is a second-order diﬀerential equa-
tion; thus we expect that generically, the two boundary conditions (2.8)
should be enough to specify a unique extremal. When this is true—as is the
case in the above example—and when an optimal curve is known to exist, we
can actually conclude that the unique extremal gives the optimal solution.
In general, the question of existence of optimal solutions is not trivial, and
the following example should serve as a warning. (We will come back to this
issue later in the context of optimal control.)
Example 2.3
Consider the problem of minimizing the functional J(y) =
R 1
0 y(x)(y′(x))2dx subject to the boundary conditions y(0) = y(1) = 0. The
Euler-Lagrange equation is
d
dx(2yy′) = (y′)2, and y ≡0 is a solution. Ac-
tually, one can show that this is a unique extremal satisfying the boundary
conditions (we leave the proof of this fact to the reader). But y ≡0 is easily
seen to be neither a minimum nor a maximum.
□
2.3.2
Historical remarks
The equation (2.18) was derived by Euler around 1740. His original deriva-
tion was very diﬀerent from the one we gave, and relied on discretization.
The idea is to approximate a general curve by a piecewise linear one passing
through N points, as in Figure 2.8. The problem of optimizing the locations
of these N points is ﬁnite-dimensional, hence it can be solved using standard
calculus. The equation (2.18) is obtained in the limit as N →∞.
Figure 2.8: Illustrating Euler’s derivation
An alternative way to arrive at the same result, free of geometric con-
siderations and relying on analysis alone, was proposed by Lagrange in 1755
(when he was only 19 years old). Lagrange described his argument in a letter
to Euler, who was quite impressed by it and subsequently coined the term
“calculus of variations” for Lagrange’s method. Even though the problem
formulation and the solution given by Lagrange diﬀer from the modern treat-

40
CHAPTER 2
ment in the notation and other details, the main ideas behind our derivation
of the Euler-Lagrange equation are essentially contained in his work.
2.3.3
Technical remarks
It is straightforward to extend the necessary condition (2.18) to the multiple-
degrees-of-freedom setting, in which y = (y1, . . . , yn)T ∈Rn.
The same
derivation is valid, provided that Ly and Lz are interpreted as gradient
vectors of L with respect to y and z, respectively, and by products of vector
quantities (such as Ly ·η) one means inner products in Rn. The result is the
same Euler-Lagrange equation (2.18), which is now perhaps easier to apply
and interpret if it is written componentwise:
Lyi = d
dxLy′
i,
i = 1, . . . , n.
(2.21)
Returning to the single-degree-of-freedom case, let us examine more care-
fully under what diﬀerentiability assumptions our derivation of the Euler-
Lagrange equation is valid. We applied Lemma 2.1 to the function ξ given
by the expression inside the large parentheses in (2.16), whose second term
is shown in more detail in (2.19). In the lemma, ξ must be continuous. The
appearance of second-order partial derivatives of L in (2.19) suggests that
we should assume L ∈C2. Somewhat more alarmingly, the presence of the
term Lzzy′′ indicates that we should assume y ∈C2, and not merely y ∈C1
as in our original formulation of the Basic Calculus of Variations Problem.
Fortunately, we can avoid making this assumption if we proceed more care-
fully, as follows. Let us apply integration by parts to the ﬁrst term rather
than the second term on the right-hand side of (2.14):
δJ|y (η) =
Z b
a

Lz(x, y(x), y′(x))η′(x) −η′(x)
Z x
a
Ly(w, y(w), y′(w))dw

dx
+ η(x)
Z x
a
Ly(w, y(w), y′(w))dw

b
a
where the last term again vanishes for our class of perturbations η. Thus an
extremum y must satisfy
Z b
a

Lz(x, y(x), y′(x)) −
Z x
a
Ly(w, y(w), y′(w))dw

η′(x)dx = 0.
(2.22)
We now need the following modiﬁcation of Lemma 2.1.
Lemma 2.2. If a continuous function ξ : [a, b] →R is such that
Z b
a
ξ(x)η′(x)dx = 0

CALCULUS OF VARIATIONS
41
for all C1 functions η : [a, b] →R with η(a) = η(b) = 0, then ξ is a constant
function.
Exercise 2.4
Prove Lemma 2.2.
□
From (2.22) and Lemma 2.2 we obtain that along an optimal curve we
must have
Lz(x, y(x), y′(x)) =
Z x
a
Ly(w, y(w), y′(w))dw + C
(2.23)
where C is a constant. It follows that the derivative
d
dxLz(x, y(x), y′(x))
indeed exists and equals Ly(x, y(x), y′(x)), and we do not need to make extra
assumptions to guarantee the existence of this derivative. In other words,
curves on which the ﬁrst variation vanishes automatically enjoy additional
regularity. Thus for the necessary condition described by the Euler-Lagrange
equation to be valid, it is enough to assume that y ∈C1 and L ∈C1.
The integral form (2.23) of the Euler-Lagrange equation actually applies to
extrema over piecewise C1 curves as well, although we would need a more
general version of Lemma 2.2 to establish this fact.2 The C1 assumption on
L can be further relaxed; note, in particular, that the partial derivative Lx
did not appear anywhere in our analysis.
It can be shown that the Euler-Lagrange equation is invariant under arbi-
trary changes of coordinates, i.e., it takes the same form in every coordinate
system. This allows one to pick convenient coordinates when studying a
speciﬁc problem. For example, in certain mechanical problems it is natural
to use polar coordinates; we will come across such a case in Section 2.4.3.
2.3.4
Two special cases
We know from the discussion given towards the end of Section 2.3.1—see
the formula (2.19) in particular—that the Euler-Lagrange equation (2.18)
is a second-order diﬀerential equation for y(·); indeed, it can be written in
more detail as
Ly = Ly′x + Ly′yy′ + Ly′y′y′′.
(2.24)
Note that we are being somewhat informal in denoting the third argument of
L by y′, and also in omitting the x-arguments. We now discuss two special
cases in which (2.24) can be reduced to a diﬀerential equation that is of ﬁrst
order, and therefore more tractable.
2Since we are discussing weak extrema, to accommodate piecewise C1 curves we need
to slightly generalize the deﬁnition of the 1-norm; see Section 3.1.1 for details. Also, the
equation (2.23) holds away from the discontinuities of y′. To prove this, we would need
to modify the proof of Lemma 2.2 to cover functions ξ that are only piecewise continuous
(see page 84 for a precise deﬁnition of this class), while still working with η ∈C1.

42
CHAPTER 2
Special case 1 (“no y”). This refers to the situation where the Lagrangian
does not depend on y, i.e., L = L(x, y′). The Euler-Lagrange equation (2.18)
becomes
d
dxLy′ = 0, which means that Ly′ must stay constant. In other
words, extremals are solutions of the ﬁrst-order diﬀerential equation
Ly′(x, y′(x)) = c
(2.25)
for various values of c ∈R. We already encountered such a situation in
Example 2.2, where we actually had L = L(y′). Due to the presence of the
parameter c, we expect that the family of solutions of (2.25) is rich enough
to contain one (and only one) extremal that passes through two given points.
The quantity Ly′, evaluated along a given curve, is called the momentum.
This terminology will be justiﬁed in Section 2.4.
Special case 2 (“no x”).
Suppose now that the Lagrangian does not
depend on x, i.e., L = L(y, y′).
In this case the partial derivative Ly′x
vanishes from (2.24), and the Euler-Lagrange equation becomes
0 = Ly′yy′ + Ly′y′y′′ −Ly.
Multiplying both sides by y′, we have
0 = Ly′y(y′)2 + Ly′y′y′y′′ −Lyy′ = d
dx
 Ly′y′ −L

where the last equality is easily veriﬁed (the Ly′y′′ terms cancel out). This
means that Ly′y′ −L must remain constant.
Thus, similarly to Case 1,
extremals are described by the family of ﬁrst-order diﬀerential equations
Ly′(y(x), y′(x))y′(x) −L(y(x), y′(x)) = c
parameterized by c ∈R.
The quantity Ly′y′ −L is called the Hamiltonian. Although its signiﬁ-
cance is not yet clear at this point, it will play a crucial role throughout the
rest of the book.
Exercise 2.5
Use the above “no x” result to show that extremals for the
brachistochrone problem are indeed given by the equations (2.7).
□
2.3.5
Variable-endpoint problems
We know that the ﬁrst-order necessary condition (1.36)—which serves as the
basis for the Euler-Lagrange equation—need only hold for admissible per-
turbations. So far we have been considering the Basic Calculus of Variations
Problem, in which the curves have both their endpoints ﬁxed by the bound-
ary conditions (2.8). Accordingly, the class of admissible perturbations is

CALCULUS OF VARIATIONS
43
restricted to those vanishing at the endpoints. This fact, reﬂected in (2.11),
was explicitly used in the derivation of the Euler-Lagrange equation (2.18).
If we change the boundary conditions for the curves of interest, then
the class of admissible perturbations will also change, and in general the
necessary condition for optimality will be diﬀerent.
To give an example
of such a situation, we now consider a simple variable-endpoint problem.
Suppose that the cost functional takes the same form (2.9) as before, the
initial point of the curve is still ﬁxed by the boundary condition y(a) = y0,
but the terminal point y(b) is free. The resulting family of curves is depicted
in Figure 2.9.
x
y
a
b
Figure 2.9: Variable terminal point
The perturbations η must still satisfy η(a) = 0 but η(b) can be arbitrary.
In view of (2.15), the ﬁrst variation is then given by
δJ|y (η) =
Z b
a

Ly(x, y(x), y′(x)) −d
dxLz(x, y(x), y′(x))

η(x)dx
+ Lz(b, y(b), y′(b))η(b)
(2.26)
and this must be 0 if y is to be an extremum.
Perturbations such that
η(b) = 0 are still allowed; let us consider them ﬁrst. They make the last
term in (2.26) disappear, leaving us with (2.16).
Exactly as before, we
deduce from this that the Euler-Lagrange equation must hold, i.e., it is still
a necessary condition for optimality. The Euler-Lagrange equation says that
the expression in the large parentheses inside the integral in (2.26) is 0. But
this means that the entire integral is 0, for all admissible η (not just those
vanishing at x = b). The last term in (2.26) must then also vanish, which
gives us an additional necessary condition for optimality:
Lz(b, y(b), y′(b))η(b) = 0
or, since η(b) is arbitrary,
Lz(b, y(b), y′(b)) = 0.
(2.27)

44
CHAPTER 2
We can think of (2.27) as replacing the boundary condition y(b) = y1.
Recall that we want to have two boundary conditions to uniquely specify
an extremal. Comparing with the Basic Calculus of Variations Problem,
here we have only one endpoint ﬁxed a priori, but on the other hand we
have a richer perturbation family which allows us to obtain one extra con-
dition (2.27).
Example 2.4
Consider again the length functional from Example 2.2,
with Lagrangian L(x, y, z) =
√
1 + z2. In other words, we are looking for a
shortest path from a given point to a vertical line. Using the formula (2.20),
we see that the condition (2.27) amounts to y′(b) = 0. This means that
the optimal path must have a horizontal tangent at the ﬁnal point (i.e., it
must meet the vertical line of possible ﬁnal points orthogonally). We already
know that in order to satisfy the Euler-Lagrange equation, the path must be
a straight line. Thus the only path satisfying the necessary conditions is a
horizontal line, which is of course the optimal solution.
□
Exercise 2.6
Consider a more general version of the above variable-
terminal-point problem, with the vertical line replaced by a curve:
J(y) =
Z xf
a
L(x, y(x), y′(x))dx
where y(a) = y0 is ﬁxed, xf is unspeciﬁed, and y(xf) = ϕ(xf) for a given
C1 function ϕ : R →R. Derive a necessary condition for a weak extremum.
Your answer should contain, besides the Euler-Lagrange equation, an addi-
tional condition (“transversality condition”) which accounts for variations
in xf and which explicitly involves ϕ′.
□
Working on this exercise, the reader will realize that obtaining a transver-
sality condition in the speciﬁed form requires a somewhat more advanced
analysis than what we have done so far. We will employ similar techniques
again soon when deriving conditions for strong minima in Section 3.1.1.
The transversality condition itself is essentially a preview of what we will
see later in the context of the maximum principle. More general variable-
endpoint problems in which the initial point is allowed to vary as well, and
the resulting transversality conditions, will also be mentioned in the optimal
control setting (at the end of Section 4.3).
2.4
HAMILTONIAN FORMALISM AND MECHANICS
We now present an alternative formulation of the results of Euler and La-
grange, proposed many years later by Hamilton. These mathematical de-
velopments will be of great signiﬁcance to us later in the context of optimal

CALCULUS OF VARIATIONS
45
control. We will also discuss the physical interpretation of the relevant con-
cepts and equations, which is somewhat secondary to our main goals but
nevertheless interesting and important.
2.4.1
Hamilton’s canonical equations
In Section 2.3.4 we came across two quantities, which we recall here and for
which we now introduce special symbols. The ﬁrst one was the momentum
p := Ly′(x, y, y′)
(2.28)
which we will usually regard as a function of x associated to a given curve
y = y(x). The second object was the Hamiltonian
H(x, y, y′, p) := p · y′ −L(x, y, y′)
(2.29)
which is written here as a general function of four variables but also becomes
a function of x alone when evaluated along a curve. The inner product sign ·
in the deﬁnition of H reﬂects the fact that in the multiple-degrees-of-freedom
case, y′ and p are vectors.
The variables y and p are called the canonical variables. Suppose now
that y is an extremal, i.e., satisﬁes the Euler-Lagrange equation (2.18). It
turns out that the diﬀerential equations describing the evolution of y and
p along such a curve, when written in terms of the Hamiltonian H, take a
particularly nice form. For y, we have
dy
dx = y′(x) = Hp(x, y(x), y′(x)).
For p, we have
dp
dx = d
dxLy′(x, y(x), y′(x)) = Ly(x, y(x), y′(x)) = −Hy(x, y(x), y′(x))
where the second equality is the Euler-Lagrange equation. In more concise
form, the result is
y′ = Hp,
p′ = −Hy
(2.30)
which is known as the system of Hamilton’s canonical equations. This re-
formulation of the Euler-Lagrange equation was proposed by Hamilton in
1835. Since we are not assuming here that we are in the “no y” case or the
“no x” case of Section 2.3.4, the momentum p and the Hamiltonian H need
not be constant along extremals.
Exercise 2.7
Conﬁrm directly from the equations (2.28)–(2.30) that in
the “no y” case p is constant along extremals and in the “no x” case H is
constant along extremals.
□

46
CHAPTER 2
An important additional observation is that the partial derivative of H
with respect to y′ is
Hy′(x, y, y′, p) = p −Ly′(x, y, y′) = 0
(2.31)
where the last equality follows from the deﬁnition (2.28) of p. This suggests
that, in addition to the canonical equations (2.30), another necessary con-
dition for optimality should be that H has a stationary point as a function
of y′ along an optimal curve. To make this statement more precise, let us
plug the following arguments into the Hamiltonian: an arbitrary x ∈[a, b];
for y, the corresponding position y(x) of the optimal curve; for p, the cor-
responding value of the momentum p(x) = Ly′(x, y(x), y′(x)). Let us keep
the last remaining argument, y′, as a free variable, and relabel it as z for
clarity. This yields the function
H∗(z) := Ly′(x, y(x), y′(x)) · z −L(x, y(x), z).
(2.32)
Our claim is that this function has a stationary point when z equals y′(x),
the velocity of the optimal curve at x. Indeed, it is immediate to check that
dH∗
dz (y′(x)) = 0.
(2.33)
Later we will see that in the context of the maximum principle this stationary
point is actually an extremum, in fact, a maximum. Moreover, the statement
about the maximum remains true when H is not necessarily diﬀerentiable or
when y′ takes values in a set with a boundary and Hy′ ̸= 0 on this boundary;
the basic property is not that the derivative vanishes but that H achieves
the maximum in the above sense.
Mathematically, the Lagrangian L and the Hamiltonian H are related via
a construction known as the Legendre transformation. Since this transforma-
tion is classical and ﬁnds applications in many diverse areas (optimization,
geometry, physics), we now proceed to describe it. However, we will see that
it does not quite provide the right point of view for our future developments,
and it is included here mainly for historical reasons.
2.4.2
Legendre transformation
Consider a function f : R →R, whose argument we denote by ξ (the curve
in Figure 2.10 is a possible graph of f). For simplicity we are considering
the scalar case, but the extension to f : Rn →R is straightforward. The
Legendre transform of f will be a new function, f ∗, of a new variable, p ∈R.
Let p be given. Draw a line through the origin with slope p. Take a
point ξ = ξ(p) at which the (directed) vertical distance from the graph of f
to this line is maximized:
ξ(p) := arg max
ξ {pξ −f(ξ)}.
(2.34)

CALCULUS OF VARIATIONS
47
ξ
f(ξ)
f ∗(p)
slope p
ξ(p)
Figure 2.10: Legendre transformation
(Note that ξ(p) may not exist, so the domain of f ∗is not known a priori.
Also, ξ(p) is not necessarily unique unless f is a strictly convex function.)
Now, deﬁne f ∗(p) to be this maximal value of the gap between pξ and f(ξ):
f∗(p) := pξ(p) −f(ξ(p)) = max
ξ {pξ −f(ξ)}.
(2.35)
We can also write this deﬁnition more symmetrically as
f∗(p) + f(ξ) = pξ
(2.36)
where p and ξ = ξ(p) are related via (2.34). When f is diﬀerentiable, the
maximization condition (2.34) implies that the derivative of pξ −f(ξ) with
respect to ξ must equal 0 at ξ(p):
p −f′(ξ(p)) = 0.
(2.37)
Geometrically, the tangent line to the graph of f at ξ(p) must have slope
p, i.e., it must be parallel to the original line through the origin (see Fig-
ure 2.10). If f is convex then (2.34) and (2.37) are equivalent.
The Legendre transformation has some nice properties. For example,
f∗is a convex function even if f is not convex. The reason is that f ∗is a
pointwise maximum of functions that are aﬃne in p, as is clear from (2.35).
Also, for convex functions the Legendre transformation is involutive: if f is
convex, then f ∗∗= f.
Now let us return to the Hamiltonian H deﬁned in (2.29). We claim
that it can be obtained by applying the Legendre transformation to the
Lagrangian L. More precisely, for arbitrary ﬁxed x and y let us consider
L(x, y, y′) as a function of ξ = y′. The relation (2.37) between p and ξ(p) =
y′(p) becomes
p −Ly′(x, y, y′(p)) = 0
(2.38)
which corresponds to our earlier deﬁnition (2.28) of the momentum p. Next,
(2.35) gives
L∗(x, y, p) = py′(p) −L(x, y, y′(p))
(2.39)

48
CHAPTER 2
which is essentially our earlier deﬁnition (2.29) of the Hamiltonian H. But
there is a diﬀerence: in (2.29) we had y′ as an independent argument of
H, while in (2.39) y′ is a dependent variable expressed in terms of x, y, p
by the implicit relation (2.38). In other words, the Legendre transform of
L(x, y, y′) as a function of y′ (with x, y ﬁxed) is H(x, y, p), which is a function
of p (with x, y ﬁxed) and no longer has y′ as an argument. Note that the
above derivation is formal, i.e., we are ignoring the question of whether or
not (2.38) can indeed be solved for y′. This issue did not arise earlier when
we were working with the Hamiltonian H = H(x, y, y′, p).
The above approach has another, more important drawback. Recall the
observation based on (2.31) that H has a stationary point as a function of y′
along an optimal curve. This property will be crucial later; combined with
the canonical equations (2.30), it will lead us to the maximum principle.
But it only makes sense when we treat y′ as an independent variable in
the deﬁnition of H. On the other hand, Hamilton and other 19th century
mathematicians did not write the Hamiltonian in this way; they followed the
convention of viewing y′ as a dependent variable deﬁned implicitly by (2.38).
This is probably why it was not until the late 1950s that the maximum
principle was discovered.
2.4.3
Principle of least action and conservation laws
Newton’s second law of motion in the three-dimensional space can be written
as the vector equation
d
dt(m ˙q) = −Uq
(2.40)
where q = (x, y, z)T is the vector of coordinates, ˙q = dq/dt is the velocity
vector, and U = U(q) is the potential; consequently, m ˙q is the momentum
and −Uq is the force. Note that we are only considering situations where
the force is conservative, i.e., corresponds to the negative gradient of some
potential function. Planar motion is obtained as a special case by dropping
the z-coordinate.
It turns out that there is a direct relationship between (2.40) and the
Euler-Lagrange equation.
This is diﬃcult to see right now because the
notation in (2.40) is very diﬀerent from the one we have been using. So,
let us modify our earlier notation to better match (2.40).
First, let us
write t instead of x for the independent variable. Second, let us write q
instead of y for the dependent variable. Then also y′ becomes ˙q and we have
L = L(t, q, ˙q). In the new notation, the Euler-Lagrange equation becomes
d
dtL ˙q = Lq.
(2.41)

CALCULUS OF VARIATIONS
49
Note that since q ∈R3, we are referring to the multiple-degrees-of-freedom
version (2.21) of the Euler-Lagrange equation, with n = 3.
Some remarks on the above change of notation are in order (as it is
also a preview of things to come). The change from x to t is conceptually
signiﬁcant, because it implies that the curves are parameterized by time and
thus describe some dynamic behavior (e.g., trajectories of a moving object).
In problems such as Dido’s problem or catenary problem, where there is no
motion with respect to time, this notation would not be justiﬁed. Other
variational problems, such as the brachistochrone problem, do indeed deal
with paths of a moving particle. (We did not, however, explicitly use time
when formulating the brachistochrone problem, and it would not make sense
to just relabel x as t in our earlier formulation of that problem. Instead, we
would need to reparameterize the (x, y)-trajectories with respect to time,
which yields a diﬀerent Lagrangian; we will do this in Section 3.2.)
In
mechanics, as well as in control theory, time is the default independent
variable.
Accordingly, when we come to the optimal control part of the
book, we will make this change of notation permanent. For now, we adopt
it just temporarily while we discuss applications of calculus of variations in
mechanics. As for the (time-)dependent coordinate variables, it is natural
to denote them by x and y for planar curves and by x, y and z for spatial
curves. In general, the selection of a label for the coordinate vector is just
a matter of preference and convention; the mechanics literature typically
favors q or r, while in control theory it is customary to use x.
Let us now compare (2.41) with (2.40).
Is there a choice of the La-
grangian L that would make these two equations the same? The answer
is yes, and the reader will have no diﬃculty in seeing that the following
Lagrangian does the job:
L := 1
2m
 ˙x2 + ˙y2 + ˙z2
−U(q)
(2.42)
which is the diﬀerence between the kinetic energy T := 1
2m( ˙q · ˙q) and the
potential energy. We conclude that Newton’s equations of motion can be re-
covered from a path optimization problem. This important result is known
as Hamilton’s principle of least action: Trajectories of mechanical sys-
tems are extremals of the functional
Z t1
t0
(T −U)dt
which is called the action integral. In general, these extremals are not nec-
essarily minima. However, they are indeed minima—hence the term “least
action” is accurate—if the time horizon is suﬃciently short; this will follow
from the second-order suﬃcient condition for optimality, to be derived in
Section 2.6.2.

50
CHAPTER 2
For example, if the potential is 0, then the trajectories are the extremals
of the functional
R t1
t0
1
2m( ˙q · ˙q)dt, which are straight lines. We saw in Ex-
ample 2.2 that straight lines arise as extremals when the Lagrangian is the
arclength; the kinetic energy gives the same extremals.
In the presence
of gravity, the paths along which the action integral is minimized can be
viewed as “straight lines” (shortest paths, or geodesics) in a curved space
whose metric is determined by gravitational forces. This view of mechanics
forms the basis for Einstein’s theory of general relativity.
Observe the diﬀerence between the law that the derivative of the momen-
tum equals the force and the principle that the action integral is minimized.
The former condition holds pointwise in time, while the latter is a state-
ment about the entire trajectory. However, their relation is not surprising,
because if the action integral is minimized then every small piece of the
trajectory must also deliver minimal action. In the limit as the length of
the piece approaches 0, we recover the diﬀerential statement. We already
discussed this point in the general context of the Euler-Lagrange equation
(see page 38).
Now, what is the physical meaning of the Hamiltonian? Substituting q
for y in (2.28)–(2.29) and using (2.42), we have
H = L ˙q · ˙q −L = 1
2m( ˙q · ˙q) + U = T + U = E
which is the total energy (kinetic plus potential). We see that the Hamilto-
nian not only enables a convenient rewriting of the Euler-Lagrange equation
in the form of the canonical equations (2.30), but also has a very clear
mechanical interpretation.
Exercise 2.8
Reproduce all the derivations of this subsection (up to this
point) for the more general setting of N moving particles.
□
Recall that in Section 2.3.4 we studied two special cases for which we
found conserved quantities, i.e., functions that remain constant along ex-
tremals. We now revisit these two conservation laws—as well as another
related case—in the context of the principle of least action, which permits
us to see their physical meaning.
Conservation of energy. In a conservative system, the potential is ﬁxed
and does not change with time. Since the kinetic energy does not explicitly
depend on time either, we have L = L(q, ˙q). In other words, the Lagrangian
is invariant under time shifting. In our old notation, this corresponds to the
“no x” case from Section 2.3.4, and we know that in this case the Hamilto-
nian is conserved. We also just saw that the Hamiltonian is the total energy
of the system. Therefore, this is nothing but the well-known principle of
conservation of energy.

CALCULUS OF VARIATIONS
51
Conservation of momentum.
Suppose that no force is acting on the
system (i.e., the system is closed). Since the force is given by Lq = −Uq,
this implies that U must be constant. The kinetic energy T depends on ˙q
but not on q. Thus the Lagrangian L = T −U does not explicitly depend
on q, which means that it is invariant under parallel translations.
This
situation corresponds to the “no y” case from Section 2.3.4, where we saw
that the momentum, L ˙q = m ˙q in the present notation, is conserved. A more
general statement is that for each coordinate qi that does not appear in L,
the corresponding component L ˙qi = m ˙qi of the momentum is conserved.
Conservation of angular momentum.
Consider a planar motion in
a central ﬁeld; in polar coordinates (r, θ), this is deﬁned by the property
that U = U(r), i.e., the potential depends only on the radius and not on
the angle. This means that no torque is acting on the system, making the
Lagrangian L invariant under rotations. Now we can use the fact (noted at
the end of Section 2.3.3) that the Euler-Lagrange equation looks the same
in all coordinate systems. In particular, in polar coordinates we have
Lθ = d
dtL ˙θ
(2.43)
and the analogous equation for r (which we do not need here). Arguing
exactly as before, we can show that in the present “no θ” case the corre-
sponding component of the momentum, L ˙θ, is conserved. Converting the
kinetic energy from Cartesian to polar coordinates, we have
T = 1
2m( ˙x2 + ˙y2) = 1
2m( ˙r2 + r2 ˙θ2).
(2.44)
Thus the conserved quantity, in polar and Cartesian coordinates, is
L ˙θ = T ˙θ = mr2 ˙θ = m(x ˙y −y ˙x).
These are familiar expressions for the angular momentum.
We remark that all of the above examples are special instances of a
general result known as Noether’s theorem, which says that invariance of
the action integral under some transformation (e.g., time shift, translation,
rotation) implies the existence of a conserved quantity.
2.5
VARIATIONAL PROBLEMS WITH CONSTRAINTS
In Section 2.3 we showed that the Euler-Lagrange equation is a necessary
condition for optimality in the context of the Basic Calculus of Variations
Problem, where the boundary points are ﬁxed but the curves are otherwise
unconstrained. In this section we generalize that result to situations where

52
CHAPTER 2
equality constraints are imposed on the admissible curves. Before proceed-
ing, the reader should ﬁnd it helpful to review the material in Section 1.2.2
devoted to constrained optimality and the method of Lagrange multipliers
for ﬁnite-dimensional problems. It is also useful to recall the general deﬁni-
tions of an extremum from Section 1.3.1 and of an admissible perturbation
from Section 1.3.2; here the function space V is the same as at the begin-
ning of Section 2.3 while the subset A is smaller because it reﬂects additional
constraints (to be speciﬁed below). Finally, the explanation given at the be-
ginning of Section 2.3 applies here as well: the conditions in this section are
developed mainly with the 1-norm in mind, i.e., they are primarily designed
to test for weak rather than strong minima.
2.5.1
Integral constraints
Suppose that we augment the Basic Calculus of Variations Problem with an
additional constraint of the form
C(y) :=
Z b
a
M(x, y(x), y′(x))dx = C0
(2.45)
where C stands for the “constraint” functional, M is a function from the
same class as L, and C0 is a given constant. In other words, the problem is
to minimize the functional given by (2.9) over C1 curves y(·) satisfying the
boundary conditions (2.8) and subject to the integral constraint (2.45). For
simplicity, we are considering the case of only one constraint. We already
saw examples of such constrained problems, namely, Dido’s problem and the
catenary problem.
Assume that a given curve y is an extremum. What follows is a heuristic
argument motivated by our earlier derivation of the ﬁrst-order necessary
condition for constrained optimality in the ﬁnite-dimensional case (involving
Lagrange multipliers). Let us consider perturbed curves of the familiar form
y + αη.
To be admissible, the perturbation η must preserve the constraint (in addi-
tion to vanishing at the endpoints as before). In other words, we must have
C(y+αη) = C0 for all α suﬃciently close to 0. In terms of the ﬁrst variation
of C, this property is easily seen to imply that
δC|y (η) = 0.
(2.46)
Repeating the same calculation as in our original derivation of the Euler-
Lagrange equation, we obtain from this that
Z b
a

My(x, y(x), y′(x)) −d
dxMy′(x, y(x), y′(x))

η(x)dx = 0.
(2.47)

CALCULUS OF VARIATIONS
53
Now our basic ﬁrst-order necessary condition (1.36) indicates that for every
η satisfying (2.47), we should have
δJ|y (η) =
Z b
a

Ly(x, y(x), y′(x)) −d
dxLy′(x, y(x), y′(x))

η(x)dx = 0.
This conclusion can be summarized as follows:
Z b
a

Ly−d
dxLy′

η(x)dx = 0
∀η such that
Z b
a

My−d
dxMy′

η(x)dx = 0.
(2.48)
The reader will note that (2.48) is quite similar to the condition (1.21) on
page 12.
It also has a similar consequence, namely, that there exists a
constant λ∗(a Lagrange multiplier) such that

Ly −d
dxLy′

+ λ∗

My −d
dxMy′

= 0
(2.49)
for all x ∈[a, b]. Rearranging terms, we see that this is equivalent to
(L + λ∗M)y = d
dx(L + λ∗M)y′
which amounts to saying that the Euler-Lagrange equation holds for the
augmented Lagrangian L + λ∗M. In other words, y is an extremal of the
augmented cost functional
(J + λ∗C)(y) =
Z b
a
 L(x, y(x), y′(x)) + λ∗M(x, y(x), y′(x))

dx.
(2.50)
A closer inspection of the above argument reveals, however, that we left
a couple of gaps. First, we did not justify the step of passing from (2.48)
to (2.49). In the ﬁnite-dimensional case, we had to make the corresponding
step of passing from (1.21) to (1.22) which then gave (1.24); we would need
to construct a similar reasoning here, treating the integrals in (2.48) as
inner products of η with the functions in parentheses (inner products in L2).
Second, there was actually a more serious logical ﬂaw: the condition (2.46)
is necessary for the perturbation η to preserve the constraint (2.45), but we
do not know whether it is suﬃcient. Without this suﬃciency, the validity
of (2.48) is in serious doubt. In the ﬁnite-dimensional case, to reach (1.21)
we used the fact that (1.20) was a necessary and suﬃcient condition for d
to be a tangent vector; we did not, however, give a proof of the suﬃciency
part (which is not trivial).
It is also important to recall that in the ﬁnite-dimensional case studied in
Section 1.2.2, the ﬁrst-order necessary condition for constrained optimality
in terms of Lagrange multipliers is valid only when an additional technical

54
CHAPTER 2
assumption holds, namely, the extremum must be a regular point of the
constraint surface. This assumption is needed to rule out degenerate sit-
uations (see Exercise 1.2); in fact, it enables precisely the suﬃciency part
mentioned in the previous paragraph. It turns out that in the present case,
a degenerate situation arises when the test curve y satisﬁes the constraint
but all nearby curves violate it. This can happen if y is an extremal of the
constraint functional C, i.e., satisﬁes the Euler-Lagrange equation for M.
For example, consider the length constraint C(y) :=
R 1
0
p
1 + (y′)2dx = 1
together with the boundary conditions y(0) = y(1) = 0. Clearly, y ≡0 is
the only admissible curve (it is the unique global minimum of the constraint
functional), hence it automatically solves our constrained problem no mat-
ter what J is. The second integral in (2.48) is 0 for every η since y is an
extremal of C. Thus if (2.48) were true, it would imply that y must be an
extremal of J, but as we just explained this is not necessary. We see that
if we hope for (2.48) to be a necessary condition for constrained optimality,
we need to assume that y is not an extremal of C, so that there exist nearby
curves at which C takes values both larger and smaller than C0.
We can now conjecture the following ﬁrst-order necessary condition
for constrained optimality: If y(·) is an extremum for the constrained
problem and is not an extremal of the constraint functional C (i.e., does not
satisfy the Euler-Lagrange equation for M), then it is an extremal of the
augmented cost functional (2.50) for some λ∗∈R. We can also state this
condition more succinctly, combining the nondegeneracy assumption and the
conclusion into one statement: y must satisfy the Euler-Lagrange equation
for λ∗
0L + λ∗M, where λ∗
0 and λ∗are constants (not both 0). Indeed, this
means that either λ∗
0 = 0 and y is an extremal of C, or λ∗
0 ̸= 0 and y is an
extremal of J + (λ∗/λ∗
0)C. The number λ∗
0 is called the abnormal multiplier
(it also has an analog in optimal control which will appear in Section 4.1).
It turns out that this conjecture is correct. However, rather than ﬁx-
ing the above faulty argument, it is easier to give an alternative proof by
proceeding along the lines of the second proof in Section 1.2.2.
Exercise 2.9
Write down a correct proof of the ﬁrst-order necessary con-
dition for constrained optimality by considering a two-parameter family of
perturbed curves y + α1η1 + α2η2 and using the Inverse Function Theo-
rem.
□
In the unconstrained case, as we noted earlier, the general solution of the
second-order Euler-Lagrange diﬀerential equation depends on two arbitrary
constants whose values are to be determined from the two boundary con-
ditions. Here we have one additional parameter λ∗but also one additional
constraint (2.45), so generically we still expect to obtain a unique extremal.
The generalization of the above necessary condition to problems with

CALCULUS OF VARIATIONS
55
several constraints is straightforward: we need one Lagrange multiplier for
each constraint (cf. Section 1.2.2). The multiple-degrees-of-freedom setting
also presents no complications.
Similarly to the ﬁnite-dimensional case, Lagrange’s original intuition was
to replace constrained minimization of J with respect to y by unconstrained
minimization of
Z b
a
Ldx + λ
 Z b
a
Mdx −C0

(2.51)
with respect to y and λ. For curves satisfying the constraint, the values of the
two functionals coincide. However, for the same reasons as in the discussion
on page 16, considering this augmented cost (which matches (2.50) except
for an additive constant) does not lead to a rigorous justiﬁcation of the
necessary condition.
Equipped with the above necessary condition for the case of integral con-
straints as well as our previous experience with the Euler-Lagrange equation,
we can now study Dido’s isoperimetric problem and the catenary problem.
Exercise 2.10
Show that optimal curves for Dido’s problem are circular
arcs and that optimal curves for the catenary problem satisfy (2.3).
□
2.5.2
Non-integral constraints
We now suppose that instead of the integral constraint (2.45) we have an
equality constraint which must hold pointwise:
M(x, y(x), y′(x)) = 0
(2.52)
for all x ∈[a, b].
A vector-valued function M can be used to describe
multiple constraints, but here we assume for simplicity that there is only
one constraint.
Let y be a test curve. It turns out that the ﬁrst-order necessary condition
for optimality in this case is similar to that for integral constraints, but the
Lagrange multiplier is now a function of x.
In other words, the Euler-
Lagrange equation must hold for the augmented Lagrangian
L + λ∗(x)M
where λ∗: [a, b] →R is some function.
As in Section 2.5.1, we need a
technical assumption to rule out degenerate cases. Here we need to assume
that there are at least two degrees of freedom and that everywhere along
the curve we have My′ ̸= 0 or, if y′ does not appear in (2.52), My ̸= 0.
(This permits us, via the Implicit Function Theorem, to locally solve for

56
CHAPTER 2
one dependent variable component in (2.52) in terms of the others and to
guarantee the existence of other curves near y satisfying the constraint.)
We will not give a proof of the above result, and instead limit ourselves
to an intuitive explanation. The integral constraint (2.45) is global, in the
sense that it applies to the entire curve. In contrast, the non-integral con-
straint (2.52) is local, i.e., applies to each point on the curve. Locally around
each point, there is no essential diﬀerence between the two. This suggests
that for each x there should exist a Lagrange multiplier, and these can be
pieced together to give the desired function λ∗= λ∗(x). Another way to see
the correspondence between the two problems is to follow Lagrange’s idea of
considering an augmented cost functional which coincides with the original
one for curves satisfying the constraint. For integral constraints this was
accomplished by (2.51), while here we can consider the minimization of
Z b
a
Ldx +
Z b
a
λ(x)Mdx
(2.53)
over y and λ.
Note that λ no longer needs to be constant, since M is
identically 0. (The role of integration in the second term is simply to obtain
a cost for the whole curve.)
What if we want to solve the constraint (2.52) for y′ as a function of x
and y? In general, we have fewer constraints than the dimension of y′, i.e.,
the system of equations is under-determined. This means that we will have
some free parameters; denoting by u the vector of these parameters, we can
write the solution in the form
y′ = f(x, y, u).
The reader hopefully recognizes this as a control system, in the disguise of
a somewhat unfamiliar notation compared to (1.1). For example, if n = 2
(two degrees of freedom) and M(x, y, y′) = y′
1−y2, then we have y′
1 = y2 and
y′
2 = u (free). Incidentally, the case of no constraints (our Basic Calculus
of Variations Problem) corresponds to y′ = u. We will turn our attention
to control problems later in the book, and the present lack of depth in our
treatment of non-integral constraints will be amply compensated for.
In
particular, the “distributed” Lagrange multiplier λ∗(·) will correspond to
the adjoint vector, or costate, in the maximum principle.
Holonomic constraints
Consider the special case when the constraint function M does not depend
on y′, so that the constraints take the form
M(x, y(x)) = 0.
(2.54)

CALCULUS OF VARIATIONS
57
Alternatively, we might be able to integrate the constraints (2.52) to bring
them to this form. We then say that the constraints are holonomic. The
equation (2.54) gives us a constraint surface in the (x, y)-space, and we have
two options for studying our constrained optimization problem. The ﬁrst one
is to use the previous necessary condition involving a Lagrange multiplier
function.
The second option is to ﬁnd fewer independent variables that
parameterize the constraint surface, and reformulate the problem in terms
of these variables. The problem then becomes an unconstrained one, and
can be studied via the usual Euler-Lagrange equation. Sometimes this latter
approach turns out to be more eﬀective.
Example 2.5
Consider a simple planar pendulum depicted in Figure 2.11.
The center of coordinates is attached to the pivot point. The length of the
pendulum is ℓand the mass at the tip is m. We want to derive trajectories
of motion for this system, using the principle of least action. Since this is a
mechanical example, we use the time t as the independent variable, treating
x and y as the dependent variables (see Section 2.4.3).
x
y
θ
m
ℓ
Figure 2.11: The pendulum
The constraint is M(x, y) := x2 + y2 −ℓ2 = 0, which is holonomic.
In polar coordinates (r, θ), this means that r ≡ℓwhile the angle θ is free.
Let us follow the second approach described above and express everything in
terms of θ as a single degree of freedom, writing ˙θ for dθ/dt. The kinetic
energy is T = 1
2mℓ2 ˙θ2 (this follows from the formula (2.44) at the end of
Section 2.4.3). The potential energy is U = mgℓ(1−cos θ), up to an additive
constant (the present choice gives U = 0 at the downward equilibrium).
Thus the Lagrangian is L = T −U =
1
2mℓ2 ˙θ2 + mgℓ(cos θ −1) and the
Euler-Lagrange equation (2.43) gives
¨θ = −g
ℓsin θ
(2.55)
which is the familiar pendulum equation.
□

58
CHAPTER 2
Exercise 2.11
Study the above example directly in the (x, y)-coordinates,
with the help of the necessary condition for constrained optimality involving a
Lagrange multiplier function. Go as far as you can in deriving the equations
of motion. Are you able to reproduce (2.55) using this method?
□
In control theory, one is often more interested in the opposite situation
where the constraints are nonholonomic, i.e., cannot be integrated. In this
case, two arbitrary points in the (x, y)-space can be connected by a path
satisfying the constraints, and there is no lower-dimensional constraint sur-
face.
2.6
SECOND-ORDER CONDITIONS
In Section 2.3 we used the ﬁrst variation to obtain the ﬁrst-order neces-
sary condition for optimality expressed by the Euler-Lagrange equation. In
this section we will work with the second variation and derive ﬁrst a neces-
sary condition and then a suﬃcient condition for optimality. The setting is
that of the Basic Calculus of Variations Problem and weak minima (cf. the
discussion at the beginning of Section 2.3).
Recall from Section 1.3.3 the second-order expansion
J(y + αη) = J(y) + δJ|y (η)α + δ2J

y (η)α2 + o(α2)
(2.56)
which deﬁnes the quadratic form δ2J

y called the second variation. The
basic second-order necessary condition (1.39) says that if a curve y is a
local minimum of J, then for all admissible perturbations η we must have
δ2J

y (η) ≥0. In the present context, the function space V and its subset
A are as at the beginning of Section 2.3, and so admissible perturbations
are C1 functions satisfying (2.11). Using the fact that the functional J takes
the form (2.9), we will derive in Section 2.6.1 a more explicit second-order
necessary condition for this situation.
We also discussed in Section 1.3.3 that in order to develop a second-
order suﬃcient condition for a local minimum, we need to assume the ﬁrst-
order necessary condition, strengthen the second-order necessary condition
to δ2J

y (η) > 0, and then try to prove that the second-order term dominates
the higher-order term in the expansion (2.56). For the variational problem
at hand, we will be able to carry out this program in Section 2.6.2.
Of course, second-order conditions for a local maximum are easily ob-
tained by reversing the inequalities (or, equivalently, by replacing J with
−J). For this reason, we conﬁne our attention to minima.

CALCULUS OF VARIATIONS
59
2.6.1
Legendre’s necessary condition for a weak minimum
Let us compute δ2J

y for a given test curve y. Since third-order partial
derivatives of L will appear, we assume that L ∈C3. We work with the
single-degree-of-freedom case for now. The left-hand side of (2.56) is
J(y + αη) =
Z b
a
L(x, y(x) + αη(x), y′(x) + αη′(x))dx.
We need to write down its second-order Taylor expansion with respect to α.
We do this by expanding the function inside the integral with respect to α
(using the chain rule) and separating the terms of diﬀerent orders in α:
J(y + αη) =
Z b
a
L(x, y(x), y′(x))dx + α
Z b
a
 Ly(x, y(x), y′(x))η(x)
+ Ly′(x, y(x), y′(x))η′(x)

dx + α2
2
Z b
a
 Lyy(x, y(x), y′(x))(η(x))2
+ 2Lyy′(x, y(x), y′(x))η(x)η′(x) + Ly′y′(x, y(x), y′(x))(η′(x))2
dx
+ o(α2).
(2.57)
Matching this expression with (2.56) term by term, we deduce that the
second variation is given by
δ2J

y (η) = 1
2
Z b
a
 Lyyη2 + 2Lyy′ηη′ + Ly′y′(η′)2
dx
where the integrand is evaluated along (x, y(x), y′(x)).
This is indeed a
quadratic form as deﬁned in Section 1.3.3. Note that it explicitly depends
on η′ as well as on η. In contrast with the ﬁrst variation (analyzed in detail
in Section 2.3.1), the dependence of the second variation on η′ is essential
and cannot be eliminated.
We can, however, simplify the expression for
δ2J

y (η) by eliminating the “mixed” term containing the product ηη′. We
do this by using—as we did in our earlier derivation of the Euler-Lagrange
equation—the method of integration by parts:
Z b
a
2Lyy′ηη′dx =
Z b
a
Lyy′ d
dx(η2)dx = Lyy′η2b
a −
Z b
a
d
dx(Lyy′)η2dx.
The ﬁrst, non-integral term on the right-hand side vanishes due to the
boundary conditions (2.11). Therefore, the second variation can be writ-
ten as
δ2J

y (η) =
Z b
a
 P(x)(η′(x))2 + Q(x)(η(x))2
dx
(2.58)
where
P(x) := 1
2Ly′y′(x, y(x), y′(x)),
Q(x) := 1
2

Lyy(x, y(x), y′(x)) −d
dxLyy′(x, y(x), y′(x))

.
(2.59)

60
CHAPTER 2
Note that P is continuous, and Q is also continuous at least when y ∈C2.
When we come to the issue of suﬃciency in the next subsection, we will
also need a more precise characterization of the higher-order term labeled
as o(α2) in the expansion (2.57). The next exercise invites the reader to go
back to the derivation of (2.57) and analyze this term in more detail.
Exercise 2.12
Use Taylor’s theorem with remainder (see, e.g., [Rud76,
Theorem 5.15]) to show that the o(α2) term in (2.57) can be written in the
form
o(α2) = α2
Z b
a
  ¯P(x, η(x), η′(x), α)(η′(x))2 + ¯Q(x, η(x), η′(x), α)(η(x))2
dx
(2.60)
where ¯P, ¯Q →0 as α →0. Moreover, show that this convergence is uniform
over η with respect to the 1-norm, in the sense that for every γ > 0 there
exists an ε > 0 such that for |α| < ε we have | ¯P(x, η(x), η′(x), α)| < γ and
| ¯Q(x, η(x), η′(x), α)| < γ for all η with ∥η∥1 = 1 and all x ∈[a, b], but the
same statement with respect to the 0-norm is in general false.
□
We know that if y is a minimum, then for all C1 perturbations η vanishing
at the endpoints the quantity (2.58) must be nonnegative:
Z b
a
 P(x)(η′(x))2 + Q(x)(η(x))2
dx ≥0.
(2.61)
We would like to restate this condition in terms of P and Q only—which are
deﬁned directly from L and y via (2.59)—so that we would not need to check
it for all η. (Recall that we followed a similar route earlier when passing
from the condition (2.16) to the Euler-Lagrange equation via Lemma 2.1.)
What, if anything, does the inequality (2.61) imply about P and Q?
Does it force at least one of these two functions, or perhaps both, to be
nonnegative on [a, b]? The two terms inside the integral in (2.61) are of
course not independent because η′ and η are related.
So, we should try
to see if maybe one of them dominates the other. More speciﬁcally, can it
happen that η′ is large (in magnitude) while η is small, or the other way
around?
To answer these questions, consider a family of perturbations ηε param-
eterized by small ε > 0, depicted in Figure 2.12. The function ηε equals
0 everywhere outside some interval [c, d] ⊂[a, b], and inside this interval it
equals 1 except near the endpoints where it rapidly goes up to 1 and back
down to 0. This rapid transfer is accomplished by the derivative η′
ε having
a short pulse of width approximately ε and height approximately 1/ε right
after c, and a similar negative pulse right before d. Here ε is small compared
to d −c. We base the subsequent argument on this graphical description,

CALCULUS OF VARIATIONS
61
but it is not diﬃcult to specify a formula for ηε and use it to verify the
claims that follow; see, e.g., [GF63, p. 103] for a similar construction.
x
x
ηε(x)
η′
ε(x)
c
d
1
ε
1/ε
Figure 2.12: The graphs of ηε and its derivative
We can see that

Z b
a
Q(x)(ηε(x))2dx
 ≤
Z d
c
|Q(x)|dx
and this bound is uniform over ε. On the other hand, for nonzero P the
integral
R b
a P(x)(η′
ε(x))2dx does not stay bounded as ε →0, because it is of
order 1/ε. In particular, let us see this more clearly for the case when P is
negative on [c, d], so that for some δ > 0 we have P(x) ≤−δ for all x ∈[c, d].
Assume that there is an interval inside [c, c + ε] of length at least ε/2 on
which η′
ε is no smaller than 1/(2ε); this property is completely consistent
with our earlier description of ηε and η′
ε. We then have
Z b
a
P(x)(η′
ε(x))2dx ≤
Z c+ε
c
P(x)(η′
ε(x))2dx ≤−δ 1
4ε2
ε
2 = −δ
8ε.
As ε →0, the above expression tends to −∞, dominating the bounded Q-
dependent term. It follows that the inequality (2.61) cannot hold for all η
if P is negative on some subinterval [c, d]. But this means that for y to be a
minimum, P must be nonnegative everywhere on [a, b]. Indeed, if P(¯x) < 0
for some ¯x ∈[a, b], then by continuity of P we can ﬁnd a subinterval [c, d]

62
CHAPTER 2
containing ¯x on which P is negative, and the above construction can be
applied.3
Recalling the deﬁnition of P in (2.59), we arrive at our second-order
necessary condition for optimality: For all x ∈[a, b] we must have
Ly′y′(x, y(x), y′(x)) ≥0
(2.62)
This condition is known as Legendre’s condition, as it was obtained by
Legendre in 1786. Note that it places no restrictions on the sign of Q; intu-
itively speaking, the Q-dependent term in the second variation is dominated
by the P-dependent term, hence Q can in principle be negative along the
optimal curve (this point will become clearer in the next subsection). For
multiple degrees of freedom, the proof takes a bit more work but the state-
ment of Legendre’s condition is virtually unchanged: Ly′y′(x, y(x), y′(x)),
which becomes a symmetric matrix, must be positive semideﬁnite for all x,
i.e., (2.62) must hold in the matrix sense along the optimal curve.
As a brief digression, let us recall our deﬁnition (2.29) of the Hamiltonian:
H(x, y, y′, p) = p · y′ −L(x, y, y′)
(2.63)
where p = Ly′(x, y, y′). We already noted in Section 2.4.1 that when H
is viewed as a function of y′ with the other arguments evaluated along an
optimal curve y, it should have a stationary point at y′ = y′(x), the velocity
of y at x. More, precisely, the function H∗deﬁned in (2.32) has a stationary
point at z = y′(x), which is a consequence of (2.33). Legendre’s condition
tells us that, in addition, Hy′y′ = −Ly′y′ ≤0 along an optimal curve, which
we can rewrite in terms of H∗as
d2H∗
dz2 (y′(x)) = −Ly′y′(x, y(x), y′(x)) ≤0.
Thus, if the above stationary point is an extremum, then it is necessarily a
maximum. This interpretation of necessary conditions for optimality moves
us one step closer to the maximum principle. The basic idea behind our
derivation of Legendre’s condition will reappear in Section 3.4 in the context
of optimal control, but eventually (in Chapter 4) we will obtain a stronger
result using more advanced techniques.
2.6.2
Suﬃcient condition for a weak minimum
We are now interested in obtaining a second-order suﬃcient condition for
proving optimality of a given test curve y. Looking again at the expan-
sion (2.56) and recalling our earlier discussions, we know that we want to
3Note that while ∥ηε∥1 is large for small ε, once ε is ﬁxed we have ∥αη∥1 →0 as
α →0, so the perturbed curves are still close to y in the sense of the 1-norm.

CALCULUS OF VARIATIONS
63
have δ2J

y (η) > 0 for all admissible perturbations, which means having
a strict inequality in (2.61). In addition, we need some uniformity to be
able to dominate the o(α2) term. Since we saw that the P-dependent term
inside the integral in (2.61) is the dominant term in the second variation,
it is natural to conjecture—as Legendre did—that having P(x) > 0 for all
x ∈[a, b] should be suﬃcient for the second variation to be positive deﬁnite.
Legendre tried to prove this implication using the following clever approach.
For every diﬀerentiable function w = w(x) we have
0 = wη2b
a =
Z b
a
d
dx(wη2)dx =
Z b
a
(w′η2 + 2wηη′)dx
where the ﬁrst equality follows from the constraint η(a) = η(b) = 0. This
lets us rewrite the second variation as
Z b
a
 P(x)(η′(x))2 + Q(x)(η(x))2
dx
=
Z b
a
 P(x)(η′(x))2 + 2w(x)η(x)η′(x) + (Q(x) + w′(x))(η(x))2
dx.
Now, the idea is to ﬁnd a function w that makes the integrand on the right-
hand side into a perfect square. Clearly, such a w needs to satisfy
P(Q + w′) = w2.
(2.64)
This is a quadratic diﬀerential equation, of Riccati type, for the unknown
function w.
Let us suppose that we found a function w satisfying (2.64). Then our
second variation can be written as
Z b
a
p
P(x)η′(x) +
w(x)
p
P(x)
η(x)
2
dx =
Z b
a
P(x)

η′(x) + w(x)
P(x)η(x)
2
dx
(2.65)
(the division by P is permissible since we are operating under the assumption
that P > 0). It is obvious that the right-hand side of (2.65) is nonnegative,
but we claim that it is actually positive for every admissible perturbation η
that is not identically 0. Indeed, if the integral is 0, then η′ + w
P η ≡0. We
also know that η(a) = 0. But there is only one solution η : [a, b] →R of the
ﬁrst-order diﬀerential equation η′ + w
P η = 0 with the zero initial condition,
and this solution is η ≡0. So, it seems that we have δ2J

y (η) > 0 for
all η ̸≡0. At this point we challenge the reader to see a gap in the above
argument.
The problem with the foregoing reasoning is that the Riccati diﬀerential
equation (2.64) may have a ﬁnite escape time, i.e., the solution w may not

64
CHAPTER 2
exist on the whole interval [a, b].
For example, if P ≡1 and Q ≡−1
then (2.64) becomes w′ = w2 + 1. Its solution w(x) = tan(x −c), where
the constant c depends on the choice of the initial condition, blows up when
x −c is an odd integer multiple of π/2. This means that w will not exist on
all of [a, b] for any choice of w(a) if b −a ≥π.
We see that a suﬃcient condition for optimality should involve, in addi-
tion to an inequality like Ly′y′ > 0 holding pointwise along the curve, some
“global” considerations applied to the entire curve. In fact, this becomes
intuitively clear if we observe that a concatenation of optimal curves is not
necessarily optimal. For example, consider the two great-circle arcs on a
sphere shown in Figure 2.13. Each arc minimizes the distance between its
endpoints, but this statement is no longer true for their concatenation—even
when compared with nearby curves. At the same time, the concatenated
arc would still satisfy any pointwise condition fulﬁlled by the two pieces.
Figure 2.13: Concatenation of shortest-distance curves on a sphere is not shortest-
distance
Returning to our analysis, we need to ensure the existence of a solution
for the diﬀerential equation (2.64) on the whole interval [a, b]. This issue,
which escaped Legendre’s attention, was pointed out by Lagrange in 1797.
However, it was only in 1837, after 50 years had passed since Legendre’s
investigation, that Jacobi closed the gap by providing a missing ingredient
which we now describe. The ﬁrst step is to reduce the quadratic ﬁrst-order
diﬀerential equation (2.64) to another diﬀerential equation, linear but of
second order, by making the substitution
w(x) = −Pv′(x)
v(x)
(2.66)
where v is a new (unknown) function, twice diﬀerentiable and not equal to
0 anywhere. Rewriting (2.64) in terms of v, we obtain
P

Q −
d
dx(Pv′)v −P(v′)2
v2

= P 2(v′)2
v2
.

CALCULUS OF VARIATIONS
65
Multiplying both sides of this equation by v (which is nonzero), dividing by
P (which is positive), and canceling terms, we can bring it to the form
Qv = d
dx(Pv′).
(2.67)
This is the so-called accessory, or Jacobi, equation. We will be done if we
can ﬁnd a solution v of the accessory equation (2.67) that does not vanish
anywhere on [a, b], because then we can obtain a desired solution w to the
original equation (2.64) via the formula (2.66).
Since (2.67) is a second-order diﬀerential equation, the initial data at
x = a needed to uniquely specify a solution consists of v(a) and v′(a). In
addition, note that if v is a solution of (2.67) then λv is also a solution for
every constant λ. By adjusting λ appropriately, we can thus assume with
no loss of generality that v′(a) = 1 (since we are not interested in v being
identically 0). Among such solutions, let us consider the one that starts
at 0, i.e., set v(a) = 0. A point c > a is said to be conjugate to a if this
solution v hits 0 again at c, i.e., v(c) = v(a) = 0 (see Figure 2.14). It is clear
that conjugate points are completely determined by P and Q, which in turn
depend, through (2.59), only on the test curve y and the Lagrangian L in
the original variational problem.
x
v
c
a
Figure 2.14: A conjugate point
Conjugate points have a number of interesting properties and interpre-
tations, and their theory is outside the scope of this book. We do mention
the following interesting fact, which involves a concept that we will see again
later when proving the maximum principle. If we consider two neighboring
extremals (solutions of the Euler-Lagrange equation) starting from the same
point at x = a, and if c is a point conjugate to a, then at x = c the distance
between these two extremals becomes small (an inﬁnitesimal of higher or-
der) relative to the distance between the two extremals as well as between
their derivatives over [a, b]. As their distance over [a, b] approaches 0, the
two extremals actually intersect at a point whose x-coordinate approaches
c. The reason behind this phenomenon is that the Jacobi equation is, ap-
proximately, the diﬀerential equation satisﬁed by the diﬀerence between two
neighboring extremals; the next exercise makes this statement precise.
Exercise 2.13
Suppose that y and y + v are two neighboring extremals of
the functional (2.9). Show that then v must satisfy
Qv −d
dx(Pv′) = o(∥v∥)
(2.68)

66
CHAPTER 2
where P and Q are as in (2.59) and ∥· ∥is a suitable norm (specify which
one).
□
We see from (2.68) that v, which is the diﬀerence between the two ex-
tremals, satisﬁes the Jacobi equation (2.67) modulo terms of higher order. A
linear diﬀerential equation that describes, within terms of higher order, the
propagation of the diﬀerence between two nearby solutions of a given diﬀer-
ential equation is called the variational equation (corresponding to the given
diﬀerential equation). In this sense, the Jacobi equation is the variational
equation for the Euler-Lagrange equation. This property can be shown to
imply the claims we made before the exercise. Intuitively speaking, a conju-
gate point is where diﬀerent neighboring extremals starting from the same
point meet again (approximately). If we revisit the example of shortest-
distance curves on a sphere, we see that conjugate points correspond to
diametrically opposite points: all extremals (which are great-circle arcs)
with a given initial point intersect after completing half a circle. We will
encounter the concept of a variational equation again in Section 4.2.4.
Now, suppose that the interval [a, b] contains no points conjugate to
a. Let us see how this may help us in our task of ﬁnding a solution v of
the Jacobi equation (2.67) that does not equal 0 anywhere on [a, b]. The
absence of conjugate points means, by deﬁnition, that the solution with
the initial data v(a) = 0 and v′(a) = 1 never returns to 0 on [a, b]. This
is not yet a desired solution because we cannot have v(a) = 0. What we
can do, however, is make v(a) very small but positive. Using the property
of continuity with respect to initial conditions for solutions of diﬀerential
equations, it is possible to show that such a solution will remain positive
everywhere on [a, b].
In view of our earlier discussion, we conclude that the second variation
δ2J

y is positive deﬁnite (on the space of admissible perturbations) if P(x) >
0 for all x ∈[a, b] and there are no points conjugate to a on [a, b].
We
remark in passing that the absence of points conjugate to a on [a, b] is also a
necessary condition for δ2J

y to be positive deﬁnite, and if δ2J

y is positive
semideﬁnite then no interior point of [a, b] can be conjugate to a. We are
now ready to state the following second-order suﬃcient condition for
optimality: An extremal y(·) is a strict minimum if Ly′y′(x, y(x), y′(x)) > 0
for all x ∈[a, b] and the interval [a, b] contains no points conjugate to a.
Note that we do not yet have a proof of this result. Referring to the
second-order expansion (2.56), we know that under the conditions just listed
δJ|y (η) = 0 (since y is an extremal) and, as we just saw, δ2J

y (η) given
by (2.58) is positive (unless η ≡0). However, we still need to show that
δ2J

y (η)α2 dominates the higher-order term o(α2) which has the properties
established in Exercise 2.12.
Since P(x) =
1
2Ly′y′(x, y(x), y′(x)) > 0 on

CALCULUS OF VARIATIONS
67
[a, b], we can pick a small enough δ > 0 such that P(x) > δ for all x ∈[a, b].
Consider the integral
Z b
a
 (P(x) −δ)(η′(x))2 + Q(x)(η(x))2
dx.
(2.69)
Reducing δ further towards 0 if necessary, we can ensure that no points
conjugate to a on [a, b] are introduced as we pass from P to P −δ (thanks
to continuity of solutions of the accessory equation with respect to param-
eter variations). This guarantees that the functional (2.69) is still positive
deﬁnite, hence
Z b
a
 P(x)(η′(x))2 + Q(x)(η(x))2
dx > δ
Z b
a
(η′(x))2dx
(2.70)
for all admissible perturbations (not identically equal to 0).
In light of our earlier derivation of Legendre’s condition, we know that
the term depending on (η′)2 is in some sense the dominant term in (2.60),
and the inequality (2.70) indicates that we are in good shape. Formally,
we can handle the other, η2-dependent term in (2.60) as follows. Use the
Cauchy-Schwarz inequality with respect to the L2 norm4 to write
η2(x) =
Z x
a
1 · η′(z)dz
2
≤(x −a)
Z x
a
(η′(z))2dz ≤(x −a)
Z b
a
(η′(z))2dz.
From this, we have
Z b
a
η2(x)dx ≤
Z b
a
(x −a)dx
Z b
a
(η′(z))2dz = (b −a)2
2
Z b
a
(η′(x))2dx. (2.71)
Now, Exercise 2.12 tells us that the term o(α2) in (2.56) takes the form (2.60)
where for α close enough to 0 both | ¯P| and | ¯Q(b −a)2/2| are smaller than
δ/2 for all x ∈[a, b] and all η with ∥η∥1 = 1. Combined with (2.58), (2.70),
and (2.71) this implies J(y + αη) > J(y) for these values of α (except of
course α = 0), proving that y is a (strict) weak minimum.
The above suﬃcient condition is not as constructive and practical as the
ﬁrst-order and second-order necessary conditions, because to apply it one
needs to study conjugate points. The simpler necessary conditions can be
exploited ﬁrst, to see if they help narrow down candidates for an optimal
solution.
It should be observed, though, that the existence of conjugate
points can be ruled out if the interval [a, b] is taken to be suﬃciently small.
Exercise 2.14
Justify the term “principle of least action” by showing that
extremals of the action integral considered in Section 2.4.3 are automatically
its minima on suﬃciently small time intervals.
□
4I.e.,
 R
f(x)g(x)dx
2 ≤
R
f 2(x)dx ·
R
g2(x)dx.

68
CHAPTER 2
As for the multiple-degrees-of-freedom setting, let us make the simplify-
ing assumption that Lyy′ is a symmetric matrix (i.e., Lyiy′
j = Lyjy′
i for all
i, j ∈{1, . . . , n}). Then it is not diﬃcult to show, following steps similar to
those that led us to (2.58), that the second variation δ2J

y is given by the
formula
δ2J

y (η) =
Z b
a
 (η′)T (x)P(x)η′(x) + ηT (x)Q(x)η(x)

dx
where P(x) and Q(x) are symmetric matrices still deﬁned by (2.59). In place
of w introduced at the beginning of this subsection we need to consider
a symmetric matrix W, and a suitable modiﬁcation of our earlier square
completion argument yields the Riccati matrix diﬀerential equation
Q + W ′ = WP −1W
(W ′ denotes the derivative of W, not the transpose). This quadratic dif-
ferential equation is reduced to the second-order linear matrix diﬀerential
equation QV =
d
dx(PV ′) by the substitution W = −PV ′V −1, where V is
a matrix. Conjugate points are deﬁned in terms of V becoming singular.
Generalizing the previous results by following this route is straightforward.
Riccati matrix diﬀerential equations and their solutions play a central role
in the linear quadratic regulator problem, which we will study in detail in
Chapter 6.
2.7
NOTES AND REFERENCES FOR CHAPTER 2
Formulations and solutions of the problems of Dido, catenary, and brachis-
tochrone, as well as related historical remarks, are given in [GF63, You80,
Mac05] and many other sources. For an enlightening discussion of light re-
ﬂection and refraction, see [FLS63, Chapter I-26], where there is also an
amusing (although perhaps not entirely politically correct) alternative de-
scription of refraction in terms of choosing the fastest path from the beach
to the water to save a drowning girl.
A comprehensive, insightful, and
mathematically accurate account of the historical development of calculus
of variations is given in [Gol80]; this book traces the roots of the subject to
Fermat’s principle of least time, which allowed the use of calculus for ana-
lyzing light refraction and later inspired Johann Bernoulli’s solution of the
brachistochrone problem. For an in-depth treatment of the brachistochrone
problem we also recommend the paper [SW97]; it is explained there that this
problem eﬀectively marked the birth of the ﬁeld of optimal control, because
it started steady research activity on time-optimal and related variational
problems still studied today in optimal control theory. Regarding the cate-
nary, it is interesting to mention that the inverted catenary shape has been

CALCULUS OF VARIATIONS
69
used for building arches from ancient times to present day (notable examples
include several buildings designed by Gaudi in Catalonia and the Gateway
Arch in St. Louis, Missouri).
Sections 2.2 and 2.3 follow Chapter 1 of [GF63], the text on which most
of the present chapter is based; see also [Mac05]. Exercise 2.2 is borrowed
from [Jur96, p. 341].
Example 2.3 is treated in [Vin00, p. 18], where it
is followed by a discussion of existence of optimal solutions. Diﬀerentia-
bility assumptions under which the Euler-Lagrange equation is valid are
discussed, in addition to [GF63] and [Mac05], in [Sus00, Handout 2]. In-
variance of the Euler-Lagrange equation under changes of coordinates is
demonstrated in [GF63] for a single degree of freedom and in [Sus00, Hand-
out 2] for multiple degrees of freedom. The treatment of variable-endpoint
problems in [GF63] includes the case of both endpoints lying on given verti-
cal lines, as well as a variable-terminal-point version of the brachistochrone
problem. A more general study leading to transversality conditions can be
found in [GF63, Chapter 3] (although it relies on the general formula for the
variation of a functional which we do not give) and in [SW77, Chapter 3].
The material of Section 2.4 is covered in [GF63, Chapter 4]; the Hamil-
tonian and the canonical variables also appear in [GF63, Chapter 3] in the
general formula for the variation of a functional. Other sources of relevant
information include [Arn89], [Mac05], and [Sus00, Handout 3]. Section 14
of [Arn89] mentions several applications of the Legendre transformation,
including an elegant derivation of Young’s inequality. Convexity of the Leg-
endre transform (which is also called the conjugate function) is used in dual
optimization methods; see [BV04, Sections 3.3 and 5.1]. The symmetric way
of writing the Legendre transformation via the formula (2.36) is prompted
by the presentation in [YZ99, pp. 220–221]. All these references also cover
the Legendre transformation for functions of several variables. For an in-
sightful discussion of how the Hamiltonian should be interpreted and why
the maximum principle was not discovered much earlier, see [Sus00, Hand-
out 3] and [SW97]. A nice exposition of the principle of least action can be
found in [FLS63, Chapter II-19], and the reader intrigued by our brief re-
mark about Einstein’s theory of gravitation is advised to check Chapter II-42
of the same book. Conservation laws are derived in [GF63, Chapter 4] with
the help of Noether’s theorem, which is another application of the general
formula for the variation of a functional. Conservation of angular momen-
tum is also discussed in detail in [Arn89] (see in particular Example 2 in
Section 13).
Section 2.5 is based on [GF63, Section 12] and [Mac05, Chapter 5], where
additional details (such as the treatment of several integral constraints and
multiple degrees of freedom, as well as a derivation of the necessary condi-
tion for the case of holonomic non-integral constraints) can be found. The
book [You80] examines Lagrange’s naive argument in detail and criticizes

70
CHAPTER 2
“its reappearance, every so often, in so-called accounts and introductions
that claim to present the calculus of variations to engineers and other sup-
posedly uncritical persons” (see the preamble to Volume II). The pendulum
example is discussed in [Mac05, pp. 37 and 83]. For more information on
control systems with nonholonomic constraints, including optimal control
problems, the reader can consult [Blo03] and the references therein.
Section 2.6 is largely subsumed by Chapter 5 of [GF63]. Among ad-
ditional topics covered there are a detailed study of conjugate points, a
derivation of Legendre’s condition for multiple degrees of freedom, and a con-
nection with Sylvester’s positive deﬁniteness criterion for quadratic forms.
It is possible to prove Legendre’s condition for multiple degrees of freedom
diﬀerently from how it is done in [GF63], without integrating by parts and
without assuming that the matrix Lyy′ is symmetric; namely, one can per-
turb y along directions of eigenvectors of Ly′y′ (at an arbitrary ﬁxed point
x) and then invoke the scalar result, as in [LL50, pp. 94–95]. Our reason-
ing that the absence of conjugate points leads to the existence of a nonzero
solution of the Jacobi equation is close to the argument given in [Mac05,
Section 9.4], where the reader can ﬁnd some missing details.

Chapter Three
From Calculus of Variations to Optimal Control
3.1
NECESSARY CONDITIONS FOR STRONG EXTREMA
As explained at the beginning of Section 2.3 and elsewhere in Chapter 2,
the methods and results discussed in that chapter apply primarily to weak
minima over C1 curves. On the other hand, we illustrated in Section 2.2.1
that ultimately—especially in the context of optimal control—we are more
interested in studying stronger notions of local optimality over less regular
curves. In the next chapter we will realize this objective with the help of the
maximum principle. The present chapter serves as a bridge between calculus
of variations and the maximum principle. In this section, we present two
results on strong minima over piecewise C1 curves for the Basic Calculus of
Variations Problem. In deriving these results we will depart, for the ﬁrst
time, from the familiar family of perturbed curves (2.10). The maximum
principle will require a somewhat more advanced technical machinery than
what we have seen so far, and we will now start “warming up” for it. An
added beneﬁt is that we will be able to maintain continuity in tracing the
historical development of the subject. As we will see, from the maximum
principle we will be able to recover the results given here, and more.
3.1.1
Weierstrass-Erdmann corner conditions
Recall from Section 2.2.1 that a piecewise C1 curve y on [a, b] is C1 everywhere
except possibly at a ﬁnite number of points where it is continuous but its
derivative y′ is discontinuous. Such points of discontinuity of y′ are known as
corner points. A corner point c ∈[a, b] is characterized by the property that
the left-hand derivative y′(c−) := limx↗c y′(x) and the right-hand derivative
y′(c+) := limx↘c y′(x) both exist but have diﬀerent values. For example, if
a hanging chain (catenary) is suspended too close to the ground, then it will
not look as in Figure 2.3 on page 29 but will instead touch the ground and
have two corner points; see Figure 3.1. Below is another example in which
a corner point arises.
Example 3.1
Consider the problem of minimizing the functional J(y) =
R 1
−1 y2(x)(y′(x)−1)2dx subject to the boundary conditions y(−1) = 0, y(1) =
71

72
CHAPTER 3
x
y
a
b
Figure 3.1: An extremal with two corners for the catenary problem
1. It is clear that J(y) ≥0 for all curves y. We can ﬁnd C1 curves giving
values of J(y) arbitrarily close to 0, but cannot achieve J(y) = 0. On the
other hand, the curve
y(x) =
(
0
if
−1 ≤x < 0,
x
if 0 ≤x ≤1
gives J(y) = 0. This curve is piecewise C1 with a corner point at x = 0.
□
Suppose that a piecewise C1 curve y is a strong extremum for the Basic
Calculus of Variations Problem, under the same assumptions as in Sec-
tion 2.3.
Clearly, y is then also a weak extremum, with respect to the
generalized 1-norm
∥y∥1 := max
a≤x≤b |y(x)| + max
a≤x≤b max{|y′(x−)|, |y′(x+)|}.
(3.1)
As we stated in Section 2.3.3 (see in particular footnote 2 there), such an
extremum must satisfy the integral form (2.23) of the Euler-Lagrange equa-
tion almost everywhere, i.e., at all noncorner points. Extending our previous
terminology to the present setting, we will refer to piecewise C1 solutions
of (2.23) as extremals (sometimes extremals with corner points are also
called broken extremals). We now want to investigate what additional con-
ditions must hold at corner points in order for y to be a strong extremum.
We give a direct analysis1 below; later we will mention an alternative way
of deriving these conditions (see Exercise 3.3).
For simplicity, we assume that y has only one (unspeciﬁed) corner point
c ∈[a, b]. As a generalization of (2.10), we will let two separate perturbations
η1 and η2 act on the two portions of y (before and after the corner point). To
make this construction precise, denote these two portions by y1 : [a, c] →R
1The reader who ﬁnds this derivation diﬃcult to follow might wish to skip it at ﬁrst
reading. We also note that the insight gained from solving Exercise 2.6 should be helpful
here.

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
73
and y2 : [c, b] →R; their perturbed versions will then be y1 + αη1 and
y2 +αη2. Clearly, we must have η1(a) = η2(b) = 0 to preserve the endpoints.
Furthermore, since the location of the corner point is not ﬁxed, we should
allow the corner point of the perturbed curve to deviate from c. Let this
new corner point be c + α∆x for some ∆x ∈R, with the same α as before
for convenience. Our family of perturbed curves (parameterized by α) is
thus determined by the two curves η1, η2 and one real number ∆x. We label
these new curves as y(·, α), with y(·, 0) = y. There will be an additional
condition on η1 and η2 to guarantee that y(·, α) is continuous for each α;
see (3.4) below. We take both η1 and η2 to be C1, to ensure that y(·, α) is
piecewise C1 with a single corner point at c+α∆x. (Note that the diﬀerence
y(·, α) −y is piecewise C1 with two corner points, one at c and the other at
c + α∆x.) Figure 3.2 should help visualize this situation and the argument
that follows.
c
c + α∆x
a
b
y1
y2
y1+
y2+
αη2
αη1
y(c)+α∆y
x
y
Figure 3.2: A perturbation of an extremal with a corner
The reader might have noticed a small problem which we need to ﬁx
before proceeding. The domain of y1 is [a, c], whereas we want the domain
of y1 + αη1 to be [a, c + α∆x]. For α∆x > 0, such a perturbed curve is
ill deﬁned. To deal with this issue, let us agree to extend y1 beyond c via
linear continuation: deﬁne y1 for x > c by y1(x) := y(c)+y′(c−)(x−c). The
linearity is actually not crucial, all we need is that the function y1 be C1 at
x = c, with
y1(c) = y(c),
y′
1(c) = y′(c−).
(3.2)
If the perturbation η1 is also deﬁned on an interval extending to the right
of c, then the earlier construction makes sense (at least for α close enough
to 0). Of course we need to make a similar modiﬁcation to y2, extending it
linearly to the left of c.

74
CHAPTER 3
Let us write the functional to be minimized as a sum of two components:
J(y) =
Z b
a
L(x, y(x), y′(x))dx
=
Z c
a
L(x, y1(x), y′
1(x))dx +
Z b
c
L(x, y2(x), y′
2(x))dx =: J1(y1) + J2(y2).
After the perturbation, the ﬁrst functional becomes
J1(y1 + αη1) =
Z c+α∆x
a
L(x, y1(x) + αη1(x), y′
1(x) + αη′
1(x))dx
(note that ∆x should also be an argument on the left-hand side, but we omit
it for simplicity). We can now compute the corresponding ﬁrst variation:
δJ1|y1 (η1) =
d
dα

α=0
J1(y1 + αη1)
=
Z c
a
 Ly(x, y1(x), y′
1(x))η1(x) + Ly′(x, y1(x), y′
1(x))η′
1(x)

dx
+ L(c, y1(c), y′
1(c))∆x.
Applying integration by parts and recalling (3.2) and the constraint η1(a) =
0, we can bring the above expression to the form
δJ1|y1 (η1) =
Z c
a

Ly(x, y1(x), y′
1(x)) −d
dxLy′(x, y1(x), y′
1(x))

η1(x)dx
+ Ly′(c, y(c), y′(c−))η1(c) + L(c, y(c), y′(c−))∆x.
Similarly, for the second functional we have
J2(y2 + αη2) =
Z b
c+α∆x
L(x, y2(x) + αη2(x), y′
2(x) + αη′
2(x))dx
and the ﬁrst variation of J2 at y2 is
δJ2|y2 (η2) =
Z b
c

Ly(x, y2(x), y′
2(x)) −d
dxLy′(x, y2(x), y′
2(x))

η2(x)dx
−Ly′(c, y(c), y′(c+))η2(c) −L(c, y(c), y′(c+))∆x.
For α close to 0, the perturbed curve y(·, α) is close to the original curve
y in the sense of the 0-norm. Therefore, the function α 7→J(y(·, α)) must
attain a minimum at α = 0, implying that
0 =
d
dα

α=0
J(y(·, α)) =
d
dα

α=0

J1(y1 + αη1) + J2(y2 + αη2)

= δJ1|y1 (η1) + δJ2|y2 (η2).

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
75
Next, observe that each of the two portions yi, i = 1, 2 of the optimal
curve y must be an extremal of the corresponding functional Ji. Indeed,
this becomes clear if we consider the special case when the perturbation
ηi vanishes at c and ∆x = 0.
Therefore, the integrals in the preceding
expressions for δJ1|y1 (η1) and δJ2|y2 (η2) should both vanish, and we are
left with the condition
Ly′(c, y(c), y′(c−))η1(c) −Ly′(c, y(c), y′(c+))η2(c)
+ L(c, y(c), y′(c−))∆x −L(c, y(c), y′(c+))∆x = 0.
(3.3)
Now we need to take into account the fact that the two perturbations η1
and η2 are not independent: they have to be such that the perturbed curve
remains continuous at x = c + α∆x. This provides the additional relation
y1(c + α∆x) + αη1(c + α∆x) = y2(c + α∆x) + αη2(c + α∆x)
=: y(c) + α∆y + o(α).
(3.4)
The quantity ∆y describes the ﬁrst-order (in α) vertical displacement of
the corner point, in much the same sense that ∆x describes the ﬁrst-order
horizontal displacement; ∆y and ∆x are independent of each other. Equating
the ﬁrst-order terms with respect to α in (3.4) and using the second equality
in (3.2) along with its counterpart y′
2(c) = y′(c+), we obtain
y′(c−)∆x + η1(c) = y′(c+)∆x + η2(c) = ∆y.
(3.5)
Using (3.5), we can eliminate η1(c) and η2(c) from (3.3) and rewrite that
formula in terms of ∆y and ∆x as follows:
 Ly′(c, y(c), y′(c−)) −Ly′(c, y(c), y′(c+))

∆y −
 Ly′(c, y(c), y′(c−)y′(c−)
−L(c, y(c), y′(c−))

−
 Ly′(c, y(c), y′(c+))y′(c+) −L(c, y(c), y′(c+))

∆x
= −Ly′(x, y(x), y′(x))
c+
c−∆y
+
 Ly′(x, y(x), y′(x))y′(x) −L(x, y(x), y′(x))
c+
c−∆x = 0.
Since ∆x and ∆y are independent and arbitrary, we conclude that the terms
multiplying them must be 0. This means that Ly′ and y′Ly′ −L are in fact
continuous at x = c.
The above reasoning can be extended to multiple corner points, yield-
ing the necessary conditions for optimality known as the Weierstrass-
Erdmann corner conditions: If a curve y is a strong extremum, then
Ly′ and y′Ly′ −L must be continuous at each corner point of y. More pre-
cisely, their discontinuities (due to the fact that y′ does not exist at corner
points) must be removable. The quantities Ly′ and y′Ly′ −L are of course
familiar to us from Chapter 2; they are, respectively, the momentum and

76
CHAPTER 3
the Hamiltonian. Weierstrass presented these conditions in 1865 during his
lectures on calculus of variations, but never formally published them. They
were independently derived and published by Erdmann in 1877.
Exercise 3.1
Let y be a weak extremum (with (3.1) serving as the 1-
norm) but not a strong extremum. Carefully explain where the above proof
breaks down. Show that, nevertheless, the ﬁrst Weierstrass-Erdmann condi-
tion (the one that asserts continuity of Ly′) can still be established by suitably
specializing the proof.
□
For the case of a single corner point, the two Weierstrass-Erdmann corner
conditions together with the two boundary conditions provide four relations,
which is the correct number to uniquely specify two portions of the extremal
(each satisfying the second-order Euler-Lagrange diﬀerential equation). In
general, to uniquely specify an extremal consisting of m portions (i.e., having
m −1 corner points) we need 2m conditions, and these are provided by
2(m −1) corner conditions plus the two boundary conditions.
Exercise 3.2
Consider the problem of minimizing J(y) =
R 1
−1(y′(x))3dx
subject to the boundary conditions y(−1) = y(1) = 0. Characterize all piece-
wise C1 extremals of J. Do any of them satisfy the Weierstrass-Erdmann
corner conditions? For each one that does, check if it is a minimum (weak
or strong).
□
3.1.2
Weierstrass excess function
To continue our search for additional conditions (besides being an extremal)
which are necessary for a piecewise C1 curve y to be a strong minimum, we
now introduce a new concept. For a given Lagrangian L = L(x, y, z), the
Weierstrass excess function, or E-function, is deﬁned as
E(x, y, z, w) := L(x, y, w) −L(x, y, z) −(w −z) · Lz(x, y, z).
(3.6)
The above formula is written to suggest multiple degrees of freedom, but
from now on we specialize to the single-degree-of-freedom case for simplicity.
Note that L(x, y, z) + (w −z)Lz(x, y, z) is the ﬁrst-order Taylor approxima-
tion of L(x, y, w), viewed as a function of w, around w = z. This gives
the geometric interpretation of the E-function as the distance between the
Lagrangian and its linear approximation around w = z; see Figure 3.3.
The Weierstrass necessary condition for a strong minimum states
that if y(·) is a strong minimum, then
E(x, y(x), y′(x), w) ≥0
(3.7)

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
77
w
L(x, y, w)
E(x, y, z, w)
z
w
Figure 3.3: Weierstrass excess function
for all noncorner points x ∈[a, b] and all w ∈R. The geometric meaning of
this condition is that for each x, the graph of the function L(x, y(x), ·) lies
above its tangent line at y′(x), which can be interpreted as a local convexity
property of this function.
The Weierstrass necessary condition can be proved as follows. Suppose
that a curve y is a strong minimum. Let ¯x ∈[a, b] be a noncorner point of
y, let d ∈(¯x, b] be such that the interval [¯x, d] contains no corner points of
y, and pick some w ∈R. We construct a family of perturbed curves y(·, ε),
parameterized by ε ∈[0, d −¯x), which are continuous, coincide with y on
the complement of [¯x, d], are linear with derivative w on [¯x, ¯x+ε], and diﬀer
from y by a linear function on [¯x + ε, d]. The precise deﬁnition is
y(x, ε) :=









y(x)
if a ≤x ≤¯x or d ≤x ≤b,
y(¯x) + w(x −¯x)
if ¯x ≤x ≤¯x + ε,
y(x) +
d −x
d −(¯x + ε)
 y(¯x) + wε −y(¯x + ε)

if ¯x + ε ≤x ≤d.
(3.8)
Such a perturbed curve y(·, ε) is shown in Figure 3.4.
It is clear that
y(·, 0) = y
(3.9)
and that for ε close to 0 the perturbed curve y(·, ε) is close to the original
curve y in the sense of the 0-norm. Therefore, the function ε 7→J(y(·, ε))
must have a minimum at ε = 0. We will now show that the right-sided
derivative of this function at ε = 0 exists and equals E(¯x, y(¯x), y′(¯x), w).
Then, since this derivative must be nonnegative and ¯x and w are arbitrary,
the proof will be complete.
Noting that the behavior of y(·, ε) outside the interval [¯x, d] does not

78
CHAPTER 3
x
y
slope
w
y(·, ε)
y
¯x
¯x+ε
d
a
b
Figure 3.4: The graphs of y and y(·, ε)
depend on ε, we have
d
dεJ(y(·, ε)) = d
dε
 Z ¯x+ε
¯x
L(x, y(x, ε), yx(x, ε))dx
+
Z d
¯x+ε
L(x, y(x, ε), yx(x, ε))dx

.
(3.10)
By (3.8), the ﬁrst integral in (3.10) is
R ¯x+ε
¯x
L(x, y(¯x) + w(x −¯x), w)dx and
its derivative is simply
L(¯x + ε, y(¯x) + wε, w).
(3.11)
The diﬀerentiation of the second integral in (3.10) gives
−L(¯x + ε, y(¯x + ε, ε), yx(¯x + ε, ε)) +
Z d
¯x+ε
Ly(x, y(x, ε), yx(x, ε))yε(x, ε)dx
+
Z d
¯x+ε
Ly′(x, y(x, ε), yx(x, ε))yxε(x, ε)dx
(3.12)
(it is straightforward to check that the partial derivatives yx(x, ε), yε(x, ε),
yxε(x, ε) exist inside the relevant intervals). Since yxε(x, ε) = yεx(x, ε) =
d
dxyε(x, ε), we can use integration by parts to bring the last integral in (3.12)
to the form
Ly′(x, y(x, ε), yx(x, ε))yε(x, ε)
d
¯x+ε
−
Z d
¯x+ε
d
dx

Ly′(x, y(x, ε), yx(x, ε))

yε(x, ε)dx.
(3.13)
In more detail, the ﬁrst term in (3.13) is
Ly′(d, y(d, ε), yx(d, ε))yε(d, ε)−Ly′(¯x+ε, y(¯x+ε, ε), yx(¯x+ε, ε))yε(¯x+ε, ε).
(3.14)

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
79
We have from (3.8) that y(d, ε) = y(d) for all ε, hence yε(d, ε) = 0 and so
the ﬁrst term in (3.14) is 0. Another consequence of (3.8) is the relation
y(¯x + ε, ε) = y(¯x) + wε.
Diﬀerentiating it with respect to ε, we obtain
yx(¯x + ε, ε) + yε(¯x + ε, ε) = w, or yε(¯x + ε, ε) = w −yx(¯x + ε, ε). When
we substitute this expression for yε(¯x + ε, ε) into the second term in (3.14),
that term becomes
−Ly′(¯x + ε, y(¯x + ε, ε), yx(¯x + ε, ε))(w −yx(¯x + ε, ε)).
(3.15)
We see that
d
dεJ(y(·, ε)) is given by the sum of (3.11), the ﬁrst two terms
in (3.12), the integral (with the minus sign) in (3.13), and (3.15). Setting
ε = 0 and rearranging terms, we arrive at
d
dε

ε=0+J(y(·, ε)) = L(¯x, y(¯x), w) −L(¯x, y(¯x, 0), yx(¯x, 0))
−Ly′(¯x, y(¯x, 0), yx(¯x, 0))(w −yx(¯x, 0))
+
Z d
¯x

Ly(x, y(x,0), yx(x, 0)) −d
dxLy′(x, y(x, 0), yx(x, 0))

yε(x, 0)dx.
Now, recall (3.9) which also implies that yx(·, 0) = y′. Thus the integral in
the previous formula equals
Z d
¯x

Ly(x, y(x), y′(x)) −d
dxLy′(x, y(x), y′(x))

yε(x, 0)dx = 0
because y as a strong (hence also weak) minimum must satisfy the Euler-
Lagrange equation. We are left with
d
dε

ε=0+J(y(·, ε)) = L(¯x, y(¯x), w) −L(¯x, y(¯x), y′(¯x))
−Ly′(¯x, y(¯x), y′(¯x))(w −y′(¯x)) = E(¯x, y(¯x), y′(¯x), w)
as desired, and the Weierstrass necessary condition is established. A nec-
essary condition for a strong maximum is analogous but with the reversed
inequality sign; this can be veriﬁed by passing from L to −L or by modifying
the proof in the obvious way. The Weierstrass necessary condition can also
be extended to corner points, either by reﬁning the proof or via a limiting
argument (cf. Exercise 3.3 below).
Weierstrass introduced the above necessary condition during his 1879
lectures on calculus of variations. His original proof relied on an additional
assumption (normality) which was subsequently removed by McShane in
1939. Let us now take a few moments to reﬂect on the perturbation used in
the proof we just gave. First, it is important to observe that the perturbed
curve y(·, ε) is close to the original curve y in the sense of the 0-norm,
but not necessarily in the sense of the 1-norm.
Indeed, it is clear that

80
CHAPTER 3
∥y(·, ε) −y∥0 →0 as ε →0; on the other hand, the derivative of y(·, ε) for
x immediately to the right of ¯x equals w, hence ∥y(·, ε) −y∥1 ≥|w −y′(¯x)|
for all ε > 0, no matter how small. For this reason, the necessary condition
applies only to strong minima, unless we restrict w to be suﬃciently close
to y′(¯x). Note also that the ﬁrst variation was not used in the proof. Thus
we have already departed signiﬁcantly from the variational approach which
we followed in Chapter 2. Derived using a richer class of perturbations, the
Weierstrass necessary condition turns out to be powerful enough to yield as
its corollaries the Weierstrass-Erdmann corner conditions from Section 3.1.1
as well as Legendre’s condition from Section 2.6.1.
Exercise 3.3
Use the Weierstrass necessary condition to prove that a
piecewise C1 strong minimum must satisfy the Weierstrass-Erdmann corner
conditions and Legendre’s condition.
□
When solving this exercise, the reader should keep the following points in
mind. First, we know from Exercise 3.1 that the ﬁrst Weierstrass-Erdmann
corner condition is necessary for weak extrema as well. This condition should
thus follow directly from the fact that y is an extremal—i.e., satisﬁes the inte-
gral form (2.23) of the Euler-Lagrange equation—without the need to apply
the Weierstrass necessary condition. The second Weierstrass-Erdmann cor-
ner condition, on the other hand, is necessary only for strong extrema, and
deducing it requires the full power of the Weierstrass necessary condition
(including a further analysis of what the latter implies for corner points).
As for Legendre’s condition, it can be derived from the local version of the
Weierstrass necessary condition with w restricted to be close to y′(x) for a
given x, thus conﬁrming that Legendre’s condition is also necessary for weak
extrema. Finally, when x is a corner point, Legendre’s condition should read
Ly′y′(x, y(x), y′(x±)) ≥0.
The perturbation used in the above proof of the Weierstrass necessary
condition is already quite close to the ones we will use later in the proof
of the maximum principle.
The main diﬀerence is that in the proof of
the maximum principle, we will not insist on bringing the perturbed curve
back to the original curve after the perturbation stops acting. Instead, we
will analyze how the eﬀect of a perturbation applied on a small interval
propagates up to the terminal point.
There is a very insightful reformulation of the Weierstrass necessary con-
dition which reveals its direct connection to the Hamiltonian maximization
property discussed at the end of Section 2.6.1 (and thus to the maximum
principle which we are steadily approaching). Let us write our Hamilto-
nian (2.63) as
H(x, y, z, p) = zp −L(x, y, z).

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
81
Then a simple manipulation of (3.6) allows us to bring the E-function and
the condition (3.7) to the form
E(x, y(x), y′(x), w) = y′(x)Lz(x, y(x), y′(x)) −L(x, y(x), y′(x))
−
 wLz(x, y(x), y′(x)) −L(x, y(x), w)

= H(x, y(x), y′(x), p(x)) −H(x, y(x), w, p(x)) ≥0
where we used the formula
p(x) = Lz(x, y(x), y′(x))
consistent with our earlier deﬁnition of the momentum (see Section 2.4.1).
Therefore, the Weierstrass necessary condition simply says that if y(·) is an
optimal trajectory and p(·) is the corresponding momentum, then for every x
the function H(x, y(x), ·, p(x)), which is the same as the function H∗deﬁned
in (2.32), has a maximum at y′(x). This interpretation escaped Weierstrass,
not just because it requires bringing in the Hamiltonian but because it de-
mands treating z and p as independent arguments of the Hamiltonian (we
already discussed this point at the end of Section 2.4.2).
Combining the Weierstrass necessary condition with the suﬃcient con-
dition for a weak minimum from Section 2.6.2, one can obtain a suﬃcient
condition for a strong minimum. The precise formulation of this condition
requires a new concept (that of a ﬁeld) and we will not develop it. While
suﬃcient conditions for optimality are theoretically appealing, they tend to
be less practical to apply compared to necessary conditions; we already saw
this in Section 2.6.2 and will see it again in Chapter 5.
3.2
CALCULUS OF VARIATIONS VERSUS OPTIMAL CONTROL
Problems in calculus of variations that we have treated so far are concerned
with minimizing a cost functional of the form J(y) =
R b
a L(x, y(x), y′(x))dx
over a given family of curves y(·)—such as, e.g., all C1 curves with ﬁxed end-
points. Optimal control theory studies similar problems but from a more
dynamic viewpoint, which can be explained as follows.
Rather than re-
garding the curves as given a priori, let us imagine a particle moving in
the (x, y)-space and “drawing a trace” of its motion.
The choice of the
slope y′(x) at each point on the curve can be thought of as an inﬁnitesimal
decision, or control. The resulting curve is thus a trajectory of a simple
control system, which we can write as y′ = u. In order for this curve to
minimize the overall integral cost, optimal control decisions must be taken
everywhere along the curve; this is simply a restatement of a principle that
we have already discussed several times in Chapter 2 (see, in particular,
pages 38 and 50).

82
CHAPTER 3
In realistic scenarios, not all velocities may be feasible everywhere. In
calculus of variations, constraints on available velocities may be modeled
as equalities of the form M(x, y(x), y′(x)) = 0.
We already know from
Section 2.5.2 that if we solve such a constraint for y′ and parameterize the
solution in terms of free variables u, we arrive at a control system y′ =
f(x, y, u). This dynamic description is consistent with the idea of moving
along the curve (and incurring a cost along the way).
The set in which the controls u take values might also be constrained by
some practical considerations, such as inherent bounds on physical quantities
(velocities, forces, and so on).
In the optimal control formulation, such
constraints are incorporated very naturally by working with an appropriate
control set. In calculus of variations, on the other hand, they would make
the description of the space of admissible curves quite cumbersome.
Finally, once we adopt the dynamic viewpoint of a moving particle, it
is natural to consider another transformation which we already encountered
in Section 2.4.3. Namely, it makes sense to parameterize the curves by time
rather than by the spatial variable x. Besides being more intuitive, this
new formulation is also more descriptive because it allows us to distinguish
between two geometrically identical curves traversed with diﬀerent speeds.
In addition, the curves no longer need to be graphs of single-valued functions
of x.
Example 3.2
The brachistochrone problem provides a great example in
which the two formulations—the earlier one of calculus of variations and
the new one of optimal control—are both meaningful and can be clearly
compared.
Our old formulation, given in Section 2.1.4, was in terms of
minimizing the cost functional (2.6). The Lagrangian inside that integral is
the ratio of the arclength to the speed, where we used the fact that the speed
satisﬁes the equation (2.5) as a consequence of the conservation of energy
law (2.4).
Let us now adopt a diﬀerent approach and parameterize the curves by
time. In other words, represent each point on a given curve as (x(t), y(t)),
where the x-axis and the y-axis are as shown in Figure 2.4 and t is the time
at which the particle arrives at this point; we assume that at time t0 the
particle is resting at the initial point (a, 0). The speed constraint (2.5) then
becomes ˙x2 + ˙y2 = y. Deﬁning u1 := ˙x/√y and u2 := ˙y/√y, we can describe
the motion by the control system
˙x = u1
√y,
˙y = u2
√y
subject to the constraint u2
1 + u2
2 = 1 (i.e., the control set is the unit circle).
Now, the quantity to be minimized is the time it takes for the particle
to arrive at a speciﬁed destination (b, y1). If the curve is parameterized by

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
83
t ∈[t0, t1], then this is simply
J = t1 −t0 =
Z t1
t0
1dt.
(3.16)
Thus the Lagrangian is L ≡1. Note that we avoided writing J(y) since we
are no longer working with a curve y = y(x); in fact, it makes more sense
to write J(u1, u2) because the curve is determined by the choice of controls
as functions of time.
The new problem formulation is equivalent to the original one, except
that it is more general in one aspect: as we already mentioned, it avoids the
(sometimes tacit) assumption made in calculus of variations that admissible
curves are graphs of functions of x. Observe that in terms of complexity of
the problem description, the burden has shifted from the cost functional to
the right-hand side of the control system.
□
From this point onward, we will start using t as the independent vari-
able. We will write x = (x1, . . . , xn)T for the (dependent) state variables,
˙x = ( ˙x1, . . . , ˙xn)T for their time derivatives, and u = (u1, . . . , um)T for the
controls. The controls will take values in some control set, such as the unit
circle in the above example. Of course, the simplicity of the Lagrangian
in (3.16) is due to the fact that the cost being minimized is the time (this
is a time-optimal control problem); in general, both the control system and
the cost functional may be quite complex.
3.3
OPTIMAL CONTROL PROBLEM FORMULATION AND
ASSUMPTIONS
Having completed our study of calculus of variations, we have now come full
circle and are ready to tackle the optimal control problem posed at the very
beginning of the book, in Section 1.1. The purpose of this section is to give a
more detailed formulation of that problem, by discussing and comparing its
several speciﬁc variants and spelling out necessary technical assumptions.
3.3.1
Control system
In the notation announced at the end of the previous section, control systems
that we want to study take the form
˙x = f(t, x, u),
x(t0) = x0
(3.17)
which is an exact copy of (1.1). Here x ∈Rn is the state, u ∈U ⊂Rm is the
control, t ∈R is the time, t0 is the initial time, and x0 is the initial state.
Both x and u are functions of time: x = x(t), u = u(t). The control set U is

84
CHAPTER 3
usually a closed subset of Rm and can be the entire Rm; in principle it can
also vary with time, but here we take it to be ﬁxed.
We want to know that for every choice of the initial data (t0, x0) and
every admissible control u(·), the system (3.17) has a unique solution x(·)
on some time interval [t0, t1]. If this property holds, we will say that the sys-
tem is well posed. To guarantee local existence and uniqueness of solutions
for (3.17), we need to impose some regularity conditions on the right-hand
side f and on the admissible controls u.
For the sake of simplicity, we
will usually be willing to make slightly stronger assumptions than neces-
sary; when we do this, we will brieﬂy indicate how our assumptions can be
relaxed.2
Let us begin by considering the case of no controls:
˙x = f(t, x).
(3.18)
First, to assure suﬃcient regularity of f with respect to t, we take f(·, x) to
be piecewise continuous for each ﬁxed x. Here, by a piecewise continuous
function we mean a function having at most a ﬁnite number of discontinu-
ities on every bounded interval, and possessing ﬁnite limits from the right
and from the left at each of these discontinuities. For convenience, we as-
sume that the value of such a function at each discontinuity is equal to
one of these one-sided limits (i.e., the function is either left-continuous or
right-continuous at each point). The assumption of a ﬁnite number of dis-
continuities on each bounded interval is actually not crucial; we can allow
discontinuities to have accumulation points, as long as the function remains
locally bounded (or at least locally integrable).
Second, we need to specify how regular f should be with respect to x.
A standard assumption in this regard is that f is locally Lipschitz in x,
uniformly over t. Namely, for every (t0, x0) there should exist a constant L
such that we have
|f(t, x1) −f(t, x2)| ≤L|x1 −x2|
for all (t, x1) and (t, x2) in some neighborhood of (t0, x0) in R × Rn. We
can in fact be more generous and assume the following: f(t, ·) is C1 for
each ﬁxed t, and fx(·, x) is piecewise continuous for each ﬁxed x. It is easy
to verify using the Mean Value Theorem that such a function f satisﬁes
the previous Lipschitz condition. Note that here and below, we extend our
earlier notation fx := ∂f/∂x to the vector case, so that fx stands for the
Jacobian matrix of partial derivatives of f with respect to x.
If f satisﬁes the above regularity assumptions, then on some interval
[t0, t1] there exists a unique solution x(·) of the system (3.18). Since we did
2The reader not interested in studying these technical conditions at the present time
may skip forward to Section 3.3.2 with no signiﬁcant loss of continuity.

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
85
not assume that f is continuous with respect to t, some care is needed in
interpreting what we mean by a solution of (3.18). In the present situation,
it is reasonable to call a function x(·) a solution of (3.18) if it is continuous
everywhere, C1 almost everywhere, and satisﬁes the corresponding integral
equation
x(t) = x0 +
Z t
t0
f(s, x(s))ds.
A function x(·) that can be represented as an integral of another function
g(·), and thus automatically satisﬁes ˙x = g almost everywhere, is called
absolutely continuous. This class of functions generalizes the piecewise C1
functions that we considered earlier. Basically, the extra generality here is
that the derivative can be discontinuous on a set of points that has measure
zero (e.g., a countable set) rather than at a ﬁnite number of points on a
bounded interval, and can approach inﬁnity near these points. If we insist
that the derivative be locally bounded, we arrive at the slightly smaller class
of locally Lipschitz functions.
We are now ready to go back to the control system (3.17). To guarantee
local existence and uniqueness of its solutions, we can impose assumptions
on f and u that would let us invoke the previous existence and uniqueness
result for the right-hand side
¯f(t, x) := f(t, x, u(t)).
(3.19)
Here is one such set of assumptions which, although not the weakest possible,
is adequate for our purposes: f is continuous in t and u and C1 in x; fx is
continuous in t and u; and u(·) is piecewise continuous as a function of t.
Another, weaker set of hypotheses is obtained by replacing the assumptions
of existence of fx and its continuity with respect to all variables with the
following Lipschitz property: for every bounded subset D of R × Rn × U,
there exists an L such that we have
|f(t, x1, u) −f(t, x2, u)| ≤L|x1 −x2|
for all (t, x1, u), (t, x2, u) ∈D. Note that in either case, diﬀerentiability of f
with respect to u is not assumed.
Exercise 3.4
Verify that each of the two sets of hypotheses just described
guarantees local existence and uniqueness of solutions of (3.17), by applying
the result stated earlier to the system with no controls deﬁned by (3.19).
Explain which hypotheses can be further relaxed.
□
When the right-hand side does not explicitly depend on time, i.e., when
we have f = f(x, u), a convenient way to guarantee that the above conditions
hold is to assume that f is locally Lipschitz (as a function from Rn×U to Rn).

86
CHAPTER 3
In general, f depends on t in two ways: directly through the t-argument,
and indirectly through u. Regarding the ﬁrst dependence, we are willing to
be generous by assuming that f is at least continuous in t. In fact, we can
always eliminate the direct dependence of f on t by introducing the extra
state variable xn+1 := t, with the dynamics ˙xn+1 = 1. Note that in order for
the new system obtained in this way to satisfy our conditions for existence
and uniqueness of solutions, continuity of f in t is not enough and we need
f to be C1 in t (or satisfy an appropriate Lipschitz condition).
On the other hand, as far as regularity of u with respect to t is concerned,
it would be too restrictive to assume anything stronger than piecewise con-
tinuity. In fact, occasionally we may even want to relax this assumption
and allow u to be a locally bounded function with countably many discon-
tinuities. More precisely, the class of admissible controls can be deﬁned to
consist of functions u that are measurable3 and locally bounded. In view
of the remarks made earlier about the system (3.18), local existence and
uniqueness of solutions is still guaranteed for this larger class of controls.
We will rarely need this level of generality, and piecewise continuous controls
will be adequate for most of our purposes (with the exception of some of the
material to be discussed in Sections 4.4 and 4.5—speciﬁcally, Fuller’s prob-
lem and Filippov’s theorem). Similarly to the case of the system (3.18), by a
solution of (3.17) we mean an absolutely continuous function x(·) satisfying
x(t) = x0 +
Z t
t0
f(s, x(s), u(s))ds.
In what follows, we will always assume that the property of local exis-
tence and uniqueness of solutions holds for a given control system. This of
course does not guarantee that solutions exist globally in time. Typically, we
will consider a candidate optimal trajectory deﬁned on some time interval
[t0, t1], and then existence over the same time interval will be automatically
ensured for nearby trajectories.
3.3.2
Cost functional
We will consider cost functionals of the form
J(u) :=
Z tf
t0
L(t, x(t), u(t))dt + K(tf, xf)
(3.20)
which is an exact copy of (1.2). Here tf and xf := x(tf) are the ﬁnal (or
terminal) time and state, L : R × Rn × U →R is the running cost (or
Lagrangian), and K : R×Rn →R is the terminal cost. We will explain how
3The class of measurable functions is obtained from that of piecewise continuous func-
tions by taking the closure with respect to almost-everywhere convergence.

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
87
the ﬁnal time tf is deﬁned in Section 3.3.3 below. Since the cost depends
on the initial data and the ﬁnal time as well as on the control, it would
be more accurate to write J(t0, x0, tf, u), but we write J(u) for simplicity
and to reﬂect the fact that the cost is being minimized over the space of
control functions. Note that even if L does not depend on u, the cost J
depends on the control u(·) through x(·) which is the trajectory that this
control generates. The reader might have remarked that our present choice
of arguments for L deviates from the one we made in calculus of variations;
it seems that L(t, x, ˙x) would have been more consistent. However, we can
always pass from (t, x, ˙x) to (t, x, u) by substituting f(t, x, u) for ˙x, while it
may not be possible to go in the opposite direction. Thus it is more general,
as well as more natural, to let L depend explicitly on u. In contrast with
Section 3.3.1, where the regularity conditions on f and u were dictated by
the goal of having a well-posed control system, there are no such a priori
requirements on the functions L and K. All derivatives that will appear in
our subsequent derivations will be assumed to exist, and depending on the
analysis method we will eventually see what diﬀerentiability assumptions on
L and K are needed.
Optimal control problems in which the cost is given by (3.20) are known
as problems in the Bolza form, or collectively as the Bolza problem. There
are two important special cases of the Bolza problem.
The ﬁrst one is
the Lagrange problem, in which there is no terminal cost: K ≡0. This
problem—and its name—of course come from calculus of variations. The
second special case is the Mayer problem, in which there is no running cost:
L ≡0. We can pass back and forth between these diﬀerent forms by means
of simple transformations. Indeed, given a problem with a terminal cost K,
we can write
K(tf, xf) = K(t0, x0) +
Z tf
t0
d
dtK(t, x(t))dt
= K(t0, x0) +
Z tf
t0
 Kt(t, x(t)) + Kx(t, x(t)) · f(t, x(t), u(t))

dt.
Since K(t0, x0) is a constant independent of u, we arrive at an equivalent
problem in the Lagrange form with Kt(t, x)+Kx(t, x)·f(t, x, u) added to the
original running cost. On the other hand, given a problem with a running
cost L satisfying the same regularity conditions as f, we can introduce an
extra state variable x0 via
˙x0 = L(t, x, u),
x0(t0) = 0
(we use a superscript instead of a subscript to avoid confusion with the
initial state). This yields
Z tf
t0
L(t, x(t), u(t))dt = x0(tf)

88
CHAPTER 3
thus converting the problem to the Mayer form. (The value of x0(t0) can
actually be arbitrary, since it only changes the cost by an additive con-
stant.) Note that the similar trick of introducing the additional state vari-
able xn+1 := t, which we already mentioned in Section 3.3.1, eliminates
the dependence of L and/or K on time; for the Bolza problem this gives
J(u) =
R tf
t0 L(x(t), u(t))dt + K(xf) with x ∈Rn+1.
3.3.3
Target set
As we noted before, the cost functional (3.20) depends on the choice of t0,
x0, and tf. We take the initial time t0 and the initial state x0 to be ﬁxed as
part of the control system (3.17). We now need to explain how to deﬁne the
ﬁnal time tf (which in turn determines the corresponding ﬁnal state xf).
Depending on the control objective, the ﬁnal time and ﬁnal state can be
free or ﬁxed, or can belong to some set. All these possibilities are captured
by introducing a target set S ⊂[t0, ∞) × Rn and letting tf be the smallest
time such that (tf, xf) ∈S. It is clear that tf deﬁned in this way in general
depends on the choice of the control u. We will take S to be a closed set;
hence, if (t, x(t)) ever enters S, the time tf is well deﬁned. If a trajectory
is such that (t, x(t)) does not belong to S for any t, then we consider its
cost as being inﬁnite (or undeﬁned). Note that here we do not allow the
option that tf = ∞may give a valid ﬁnite cost, although we will study such
“inﬁnite-horizon” problems later (in Chapters 5 and 6). Below are some
examples of target sets that we will encounter in the sequel.
The target set S = [t0, ∞) × {x1}, where x1 is a ﬁxed point in Rn, gives
a free-time, ﬁxed-endpoint problem. A generalization of this is to consider a
target set of the form S = [t0, ∞) × S1, where S1 is a surface (manifold) in
Rn. Another natural target set is S = {t1} × Rn, where t1 is a ﬁxed time in
[t0, ∞); this gives a ﬁxed-time, free-endpoint problem. It is useful to observe
that if we start with a ﬁxed-time, free-endpoint problem and consider again
the auxiliary state xn+1 := t, we recover the previous case with S1 ⊂Rn+1
given by {x ∈Rn+1 : xn+1 = t1}.
A target set S = T × S1, where T
is some subset of [t0, ∞) and S1 is some surface in Rn, includes as special
cases all the target sets mentioned above. It also includes target sets of the
form S = {t1} × {x1}, which corresponds to the most restrictive case of a
ﬁxed-time, ﬁxed-endpoint problem. As the opposite extreme, we can have
S = [t0, ∞) × Rn, i.e., a free-time, free-endpoint problem. (The reader may
wonder about the sensibility of this latter problem formulation: when will
the motion stop, or why would it even begin in the ﬁrst place? To answer
these questions, we have to keep in mind that the control objective is to
minimize the cost (3.20). In the case of the Mayer problem, for example,
(tf, xf) will be a point where the terminal cost is minimized, and if this
minimum is unique then we do not need to specify a target set a priori. In

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
89
the presence of a running cost L taking both positive and negative values, it
is clear that remaining at rest at the initial state may not be optimal—and
we also know that we can always bring such a problem to the Mayer form.
When one says “cost” one may often think of it implicitly as a positive
quantity, but remember that this need not be the case; we may be making
a “proﬁt” instead.) Many other target sets can be envisioned. For example,
S = {(t, g(t)) : t ∈[t0, ∞)} for some continuous function g : R →Rn
corresponds to hitting a moving target. A point target can be generalized
to a set by making g set-valued. The familiar trick of incorporating time as
an extra state variable allows us to reduce such target sets to the ones we
already discussed, and so we will not speciﬁcally consider them.
We now have a reﬁned formulation of the optimal control problem: Given
a control system (3.17) satisfying the assumptions of Section 3.3.1, a cost
functional given by (3.20), and a target set S ⊂[t0, ∞) × Rn, ﬁnd a control
u(·) that minimizes the cost. Unlike in calculus of variations, we will usually
interpret optimality in the global sense. (The necessary conditions for opti-
mality furnished by the maximum principle apply to locally optimal controls
as well, provided that we work with an appropriate norm; see Section 4.3
for details.)
3.4
VARIATIONAL APPROACH TO THE FIXED-TIME,
FREE-ENDPOINT PROBLEM
We now want to see how far a variational approach—i.e., an approach based,
as in Chapter 2, on analyzing the ﬁrst (and second) variation of the cost
functional—can take us in studying the optimal control problem formulated
in Section 3.3. We will ﬁnd that with only a modest amount of extra work,
this familiar approach allows us to arrive at the correct statement of the
maximum principle for a speciﬁc problem setting.
We will also discover
that, in general, this approach has serious shortcomings which will force us
to turn our attention to a diﬀerent, richer class of perturbations.
3.4.1
Preliminaries
Consider the optimal control problem from Section 3.3 with the following
additional speciﬁcations: the target set is S = {t1} × Rn, where t1 is a
ﬁxed time (so this is a ﬁxed-time, free-endpoint problem); U = Rm (the
control is unconstrained); and the terminal cost is K = K(xf), with no
direct dependence on the ﬁnal time (just for simplicity). We can rewrite the
cost in terms of the ﬁxed ﬁnal time t1 as
J(u) =
Z t1
t0
L(t, x(t), u(t))dt + K(x(t1)).
(3.21)

90
CHAPTER 3
Our goal is to derive necessary conditions for optimality. Let u∗(·) be an
optimal control, by which we presently mean that it provides a global min-
imum: J(u∗) ≤J(u) for all piecewise continuous controls u. Let x∗(·) be
the corresponding optimal trajectory.
We would like to consider nearby
trajectories of the familiar form
x = x∗+ αη
(3.22)
but we must make sure that these perturbed trajectories are still solutions
of the system (3.17), for suitably chosen controls. Unfortunately, the class of
perturbations η that are admissible in this sense is diﬃcult to characterize if
we start with (3.22). Note also that the cost J, whose ﬁrst variation we will
be computing, is a function of u and not of x. Thus, in the optimal control
context it is more natural to directly perturb the control instead, and then
deﬁne perturbed state trajectories in terms of perturbed controls. To this
end, we consider controls of the form
u = u∗+ αξ
(3.23)
where ξ is a piecewise continuous function from [t0, t1] to Rm and α is a real
parameter as usual. We now want to ﬁnd (if possible) a function η : [t0, t1] →
Rn for which the solutions of (3.17) corresponding to the controls (3.23),
for a ﬁxed ξ, are given by (3.22). Actually, we do not have any reason to
believe that the perturbed trajectory depends linearly on α. Thus we should
replace (3.22) by the more general (and more realistic) expression
x = x∗+ αη + o(α).
(3.24)
It is obvious that η(t0) = 0 since the initial condition does not change. Next,
we derive a diﬀerential equation for η. Let us use the more detailed notation
x(t, α) for the solution of (3.17) at time t corresponding to the control (3.23).
The function x(·, α) coincides with the right-hand side of (3.24) if and only
if
xα(t, 0) = η(t)
(3.25)
for all t. (We are assuming here that the partial derivative xα exists, but
its existence can be shown rigorously; cf. Section 4.2.4.) Diﬀerentiating the
quantity (3.25) with respect to time and interchanging the order of partial
derivatives, we have
˙η(t) = d
dtxα(t, 0) = xαt(t, 0) = xtα(t, 0) =
d
dα

α=0
˙x(t, α)
=
d
dα

α=0
f(t, x(t, α), u∗(t) + αξ(t))
= fx(t, x(t, 0), u∗(t))xα(t, 0) + fu(t, x(t, 0), u∗(t))ξ(t)
= fx(t, x∗(t), u∗(t))η(t) + fu(t, x∗(t), u∗(t))ξ(t)

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
91
which we write more compactly (remembering also the initial condition
η(t0) = 0) as
˙η = fx(t, x∗, u∗)η + fu(t, x∗, u∗)ξ =: fx|∗η + fu|∗ξ,
η(t0) = 0.
(3.26)
Here and below, we use the shorthand notation |∗to indicate that a func-
tion is being evaluated along the optimal trajectory.
The linear time-
varying system (3.26) is nothing but the linearization of the original sys-
tem (3.17) around the optimal trajectory.
To emphasize the linearity of
the system (3.26) we can introduce the notation A∗(t) := fx|∗(t) and
B∗(t) := fu|∗(t) for the matrices appearing in it, bringing it to the form
˙η = A∗(t)η + B∗(t)ξ,
η(t0) = 0.
(3.27)
The optimal control u∗minimizes the cost given by (3.21), and the con-
trol system (3.17) can be viewed as imposing the pointwise-in-time (non-
integral) constraint ˙x(t) −f(t, x(t), u(t)) = 0.
Motivated by Lagrange’s
idea for treating such constraints in calculus of variations, expressed by the
augmented cost (2.53) on page 56, let us rewrite our cost as
J(u) =
Z t1
t0
 L(t, x(t), u(t)) + p(t) · ( ˙x(t) −f(t, x(t), u(t)))

dt + K(x(t1))
for some C1 function p : [t0, t1] →Rn to be selected later. Clearly, the extra
term inside the integral does not change the value of the cost. The function
p(·) is reminiscent of the Lagrange multiplier function λ(·) in Section 2.5.2
(the exact relationship between the two will be clariﬁed in Exercise 3.6 be-
low). As we will see momentarily, p is also closely related to the momentum
from Section 2.4. We will be working in the Hamiltonian framework, which
is why we continue to use the same symbol p by which we denoted the
momentum earlier (while some other sources prefer λ).
We will henceforth use the more explicit notation ⟨·, ·⟩for the inner
product in Rn. Let us introduce the Hamiltonian
H(t, x, u, p) := ⟨p, f(t, x, u)⟩−L(t, x, u).
(3.28)
Note that this deﬁnition matches our earlier deﬁnition of the Hamiltonian
in calculus of variations, where we had H(x, y, y′, p) = ⟨p, y′⟩−L(x, y, y′);
we just need to remember that after we changed the notation from calculus
of variations to optimal control, the independent variable x became t, the
dependent variable y became x, its derivative y′ became ˙x and is given
by (3.17), and the third argument of L is taken to be u rather than ˙x (which
with the current deﬁnition of H makes even more sense). We can rewrite
the cost in terms of the Hamiltonian as
J(u) =
Z t1
t0
 ⟨p(t), ˙x(t)⟩−H(t, x(t), u(t), p(t))

dt + K(x(t1)).
(3.29)

92
CHAPTER 3
3.4.2
First variation
We want to compute and analyze the ﬁrst variation δJ|u∗of the cost func-
tional J at the optimal control function u∗.
To do this, in view of the
deﬁnition (1.32), we must isolate the ﬁrst-order terms with respect to α in
the cost diﬀerence between the perturbed control (3.23) and the optimal
control u∗:
J(u) −J(u∗) = J(u∗+ αξ) −J(u∗) = δJ|u∗(ξ)α + o(α).
(3.30)
The formula (3.29) suggests to regard the diﬀerence J(u) −J(u∗) as being
composed of three distinct terms, which we now inspect in more detail. We
will let ≈denote equality up to terms of order o(α). For the terminal cost,
we have
K(x(t1)) −K(x∗(t1)) = K(x∗(t1) + αη(t1) + o(α)) −K(x∗(t1))
≈⟨Kx(x∗(t1)), αη(t1)⟩.
(3.31)
For the Hamiltonian, omitting the t-arguments inside x and u for brevity,
we have
H(t, x, u, p) −H(t, x∗, u∗, p) = H(t, x∗+ αη + o(α), u∗+ αξ, p)−
H(t, x∗, u∗, p) ≈⟨Hx(t, x∗, u∗, p), αη⟩+ ⟨Hu(t, x∗, u∗, p), αξ⟩.
(3.32)
As for the inner product ⟨p, ˙x −˙x∗⟩, we use integration by parts as we did
several times in calculus of variations:
Z t1
t0
⟨p(t), ˙x(t) −˙x∗(t)⟩dt = ⟨p(t), x(t) −x∗(t)⟩|t1
t0 −
Z t1
t0
⟨˙p(t), x(t) −x∗(t)⟩dt
≈⟨p(t1), αη(t1)⟩−
Z t1
t0
⟨˙p(t), αη(t)⟩dt
(3.33)
where we used the fact that x(t0) = x∗(t0). Combining the formulas (3.29)–
(3.33), we readily see that the ﬁrst variation is given by
δJ|u∗(ξ) = −
Z t1
t0
 ⟨˙p + Hx(t, x∗, u∗, p), η⟩+ ⟨Hu(t, x∗, u∗, p), ξ⟩

dt
+ ⟨Kx(x∗(t1)) + p(t1), η(t1)⟩
(3.34)
where η is related to ξ via the system (3.26).
The familiar ﬁrst-order necessary condition for optimality (from Sec-
tion 1.3.2) says that we must have δJ|u∗(ξ) = 0 for all ξ. This condition is
true for every function p, but becomes particularly revealing if we make a
special choice of p. Namely, let p be the solution of the diﬀerential equation
˙p = −Hx(t, x∗, u∗, p)
(3.35)

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
93
satisfying the boundary condition
p(t1) = −Kx(x∗(t1)).
(3.36)
Note that this boundary condition speciﬁes the value of p at the end of
the interval [t0, t1], i.e., it is a ﬁnal (or terminal) condition rather than an
initial condition.
In case of no terminal cost we treat K as being equal
to 0, which corresponds to p(t1) = 0.
We label the function p deﬁned
by (3.35) and (3.36) as p∗from now on, to reﬂect the fact that it is as-
sociated with the optimal trajectory.
We also extend the notation |∗to
mean evaluation along the optimal trajectory with p = p∗, so that, for
example, H|∗(t) = H(t, x∗(t), u∗(t), p∗(t)). Setting p = p∗and using the
equations (3.35) and (3.36) to simplify the right-hand side of (3.34), we are
left with
δJ|u∗(ξ) = −
Z t1
t0
⟨Hu|∗, ξ⟩dt = 0
(3.37)
for all ξ. We know from Lemma 2.14 that this implies Hu|∗≡0 or, in more
detail,
Hu(t, x∗(t), u∗(t), p∗(t)) = 0
∀t ∈[t0, t1].
(3.38)
The meaning of this condition is that the Hamiltonian has a stationary
point as a function of u along the optimal trajectory. More precisely, the
function H(t, x∗(t), ·, p∗(t)) has a stationary point at u∗(t) for all t. This is
just a reformulation of the property already discussed in Section 2.4.1 in the
context of calculus of variations.
In light of the deﬁnition (3.28) of the Hamiltonian, we can rewrite our
control system (3.17) more compactly as ˙x = Hp(t, x, u). Thus the joint
evolution of x∗and p∗is governed by the system
˙x∗= Hp|∗,
˙p∗= −Hx|∗
(3.39)
which the reader will recognize as the system of Hamilton’s canonical equa-
tions (2.30) from Section 2.4.1. Let us examine the diﬀerential equation for
p∗in (3.39) in more detail. We can expand it with the help of (3.28) as
˙p∗= −(fx)T 
∗p∗+ Lx|∗
where we recall that fx is the Jacobian matrix of f with respect to x. This
is a linear time-varying system of the form ˙p∗= −AT
∗(t)p∗+ Lx|∗where
A∗(·) is the same as in the diﬀerential equation (3.27) derived earlier for the
ﬁrst-order state perturbation η. Two linear systems ˙x = Ax and ˙z = −AT z
4That lemma, translated to the present notation, requires Hu|∗to be continuous
(although it is clear from its proof that piecewise continuity is enough).

94
CHAPTER 3
are said to be adjoint to each other, and for this reason p is called the adjoint
vector; the system-theoretic signiﬁcance of this concept will become clearer
in Section 4.2.8. Note also that we can think of p as acting on the state or,
more precisely, on the state velocity vector, since it always appears inside
inner products such as ⟨p, ˙x⟩; for this reason, p is also called the costate (this
notion will be further explored in Section 7.1).
The purpose of the next two exercises is to recover earlier conditions
from calculus of variations, namely, the Euler-Lagrange equation and the
Lagrange multiplier condition (for multiple degrees of freedom and several
non-integral constraints) from the preliminary necessary conditions for op-
timality derived so far, expressed by the existence of an adjoint vector p∗
satisfying (3.38) and (3.39).
Exercise 3.5
The standard (unconstrained) calculus of variations problem
with n degrees of freedom can be rewritten in the optimal control language
by considering the control system
˙xi = ui,
i = 1, . . . n
together with the cost J(x) =
R tf
t0 L(t, x(t), ˙x(t))dt. Assuming that a given
trajectory satisﬁes (3.38) and (3.39) for this system, prove that the Euler-
Lagrange equations,
d
dtL ˙xi = Lxi, are satisﬁed along this trajectory.
□
Exercise 3.6
Consider now a calculus of variations problem with n de-
grees of freedom and k < n non-integral constraints, represented by the con-
trol system
˙xi = fi(t, x1, . . . , xn, u1, . . . , un−k),
i = 1, . . . , k,
˙xk+i = ui,
i = 1, . . . , n −k
with the same cost as in Exercise 3.5.
Assuming that a given trajectory
satisﬁes (3.38) and (3.39) for this system, prove that there exist functions
λ∗
i : [t0, t1] →R, i = 1, . . . , k such that the Euler-Lagrange equations for the
augmented Lagrangian
L(t, x, ˙x) +
k
X
i=1
λ∗
i (t)
 ˙xi −fi(t, x1, . . . , xn, ˙xk+1, . . . , ˙xn)

are satisﬁed along this trajectory.
□
We caution the reader that the transition between the Hamiltonian and
the Lagrangian requires some care, especially in the context of Exercise 3.6.
The Lagrangian explicitly depends on ˙x (and the partial derivatives L ˙xi
appear in the Euler-Lagrange equations), whereas the Hamiltonian should

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
95
not (it should be a function of t, x, u, and p). The diﬀerential equations can
be used to put H into the right form. A consequence of this, in Exercise 3.6,
is that x will appear inside H in several diﬀerent places.
The Lagrange
multipliers λ∗
i are related to the components of the adjoint vector p∗, but
they are not the same.
3.4.3
Second variation
To better understand the behavior of H as a function of u along the optimal
trajectory, let us bring in the second variation. To do this, we must ﬁrst
augment the description (3.24) of the perturbed state trajectory with the
second-order term in α, writing x = x∗+ αη + α2ζ + o(α2). We then need
to go back to the expressions (3.31)–(3.33) and expand them by adding
second-order terms in α. With p already set equal to p∗as deﬁned above,
it is relatively straightforward to check that the ζ-dependent terms drop
out—exactly in the same way as the η-dependent terms dropped out of the
ﬁrst variation (3.34)—and that the second variation is given by
δ2J

u∗(ξ) = −1
2
Z t1
t0
 ηT
ξT  Hxx
Hxu
Hxu
Huu

∗
η
ξ

dt + 1
2ηT Kxx(x∗(t1))η
(3.40)
where we recall that η is obtained as the state of the system (3.26) driven
by ξ.
Exercise 3.7
Verify the formula (3.40).
□
We know from the second-order necessary condition for optimality (see
Section 1.3.3) that we must have δ2J

u∗(ξ) ≥0 for all ξ. Let us concentrate
on the integrand in (3.40) and ask the following question: does one term in
the Hessian matrix of H dominate the other terms? If yes, then this term
should be nonpositive to ensure the correct sign of δ2J

u∗(ξ). We encoun-
tered a very similar issue in Section 2.6.1 in relation to the inequality (2.61).
We found there that for the overall integral to be nonnegative, the function
multiplying (η′(x))2 must be nonnegative, because η′ may be large while η
itself is small. The present situation is a bit diﬀerent since ξ is not just
the derivative of η, i.e., the system relating the two is not a simple inte-
grator. However, the corresponding conclusion is still valid: ξT Huu|∗ξ is
the dominant term because we may have a large ξ producing a small η (but
not vice versa), as illustrated in Figure 3.5. Thus, in order for the second
variation (3.40) to be nonnegative, we must have ξT Huu|∗ξ ≤0 for all ξ,
which can only happen if the matrix Huu|∗is negative semideﬁnite:
Huu(t, x∗(t), u∗(t), p∗(t)) ≤0
∀t ∈[t0, t1].
(3.41)

96
CHAPTER 3
t
t
˙η = Aη + Bξ
ξ(t)
η(t)
Figure 3.5: Large ξ can produce small η
This condition is known as the Legendre-Clebsch condition (in its Hamil-
tonian formulation).
We already know from (3.38) that for each t ∈[t0, t1], the function
H(t, x∗(t), ·, p∗(t)) must have a stationary point at u∗(t). The Legendre-
Clebsch condition (3.41) tells us that if this stationary point is an extremum,
then it is necessarily a maximum. Even though we have not proved that the
stationary point must indeed be an extremum, it is tempting to conjecture
that this Hamiltonian maximization property is true. In other words, our
ﬁndings up to this point are very suggestive of the following (not yet proved)
necessary conditions for optimality: If u∗(·) is an optimal control and
x∗(·) is the corresponding optimal state trajectory, then there exists an ad-
joint vector (costate) p∗(·) such that:
1) x∗and p∗satisfy the canonical equations (3.39) with respect to the
Hamiltonian (3.28), with the boundary conditions x∗(t0) = x0 and
p∗(t1) = −Kx(x∗(t1)).
2) For each ﬁxed t, the function u 7→H(t, x∗(t), u, p∗(t)) has a (local)
maximum at u = u∗(t):
H(t, x∗(t), u∗(t), p∗(t)) ≥H(t, x∗(t), u, p∗(t))
for all u near u∗(t) and all t ∈[t0, t1].
3.4.4
Some comments
We remark that we could deﬁne the Hamiltonian and the adjoint vector
using a diﬀerent sign convention, as follows:
bH(t, x, u, p) := ⟨p, f(t, x, u)⟩+ L(t, x, u),
ˆp := −p∗.

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
97
Then the function
bH(t, x∗(t), ·, ˆp(t)) = −⟨p∗(t), f(t, x∗(t), ·)⟩+ L(t, x∗(t), ·)
= −H(t, x∗(t), ·, p∗(t))
would have a minimum at u∗(t), while x∗, ˆp would still satisfy the correct
canonical equations with respect to bH:
˙x∗= f(t, x∗, u∗) = bHp(t, x∗, u∗)
and
˙ˆp = −˙p∗= Hx(t, x∗, u∗, p∗) = (fx)T 
∗p∗−Lx|∗
= −(fx)T 
∗ˆp −Lx|∗= −bHx(t, x∗, u∗, ˆp).
At ﬁrst glance, this reformulation in terms of Hamiltonian minimization
(rather than maximization) might seem more natural, because we are solving
the minimization problem for the cost functional J. However, our problem
is equivalent to the maximization problem for the functional −J (deﬁned
by the running cost −L and the terminal cost −K). So, whether we arrive
at a minimum principle or a maximum principle is determined just by the
sign convention, and has nothing to do with whether the cost functional
is being minimized or maximized. There is no consensus in the literature
on this choice of sign. The convention that we established in the previous
subsections and will continue to follow in the rest of the book is consistent
with our deﬁnition of the Hamiltonian in calculus of variations and its me-
chanical interpretation as the total energy of the system; see Sections 2.4.1
and 2.4.3. (In general, however, the cost in the optimal control problem is
artiﬁcial from the physical point of view and is not related to Hamilton’s
action integral.)
Note that the necessary conditions for optimality from Section 3.4.3 are
formulated as an existence statement for the adjoint vector p∗which arises
directly as a solution of the second diﬀerential equation in (3.39); this is in
contrast with Section 2.4.1 where the momentum p was ﬁrst deﬁned by the
formula (2.28) and then a diﬀerential equation for it was obtained from the
Euler-Lagrange equation. In the present setting, (3.38) and (3.39) encode
all the necessary information about p∗, and we will ﬁnd this approach more
fruitful in optimal control. Observe that in the special case of the system
˙x = u which corresponds to the unconstrained calculus of variations setting,
we immediately obtain from (3.28) and (3.38) that p∗must be given by p∗=
Lu(t, x∗, u∗), and the momentum deﬁnition is recovered (up to the change
of notation).
This implies, in particular, that the Weierstrass necessary
condition must also hold, in view of the calculation at the end of Section 3.1.2
(again modulo the change of notation).

98
CHAPTER 3
The total derivative of the Hamiltonian with respect to time along an
optimal trajectory is given by
d
dt H|∗= Ht|∗+ ⟨Hx|∗, ˙x∗⟩+

Hp|∗, ˙p∗
+ ⟨Hu|∗, ˙u∗⟩= Ht|∗
(3.42)
because the canonical equations (3.39) and the Hamiltonian stationarity
condition (3.38) guarantee that the ﬁrst two inner products cancel each other
and the third one equals 0. In particular, if the problem is time-invariant
in the sense that both f and L are independent of t, then Ht = 0 and we
conclude that H(x∗(t), u∗(t), p∗(t)) must remain constant. If we want to
think of H as the system’s energy, the last statement says that this energy
must be conserved along optimal trajectories.
We know that, at least in principle, we can obtain a second-order suﬃ-
cient condition for optimality if we make appropriate assumptions to ensure
that the second variation δ2J

u∗is positive deﬁnite and dominates terms of
order o(α2) in J(u∗+ αξ) −J(u∗). While in general these assumptions take
some work to write down and verify, the next exercise points to a case in
which such a suﬃcient condition is easily established and applied (and the
necessary condition becomes more tractable as well). This is the case when
the system is linear and the cost is quadratic; we will study this class of
problems in detail in Chapter 6.
Exercise 3.8
For the ﬁxed-time, free-endpoint optimal control problem
studied in this section, assume in addition the following: the right-hand side
of the control system takes the form f(t, x, u) = A(t)x + B(t)u for arbitrary
matrix functions A and B; the running cost takes the form L(t, x, u) =
xT Q(t)x + uT R(t)u where Q(t) is symmetric positive semideﬁnite and R(t)
is symmetric positive deﬁnite for all t; and there is no terminal cost (K ≡0).
a) Write down the canonical equations (3.39) for this case, and ﬁnd a for-
mula for the control satisfying the condition (3.38).
b) By analyzing the second variation, show that this control is indeed optimal
(specify in what sense).
□
3.4.5
Critique of the variational approach and preview of the
maximum principle
The variational approach presented in Sections 3.4.1–3.4.3 has led us, quite
quickly, to the necessary conditions for optimality expressed by the canonical
equations and the Hamiltonian maximization property (we did not actually
prove the latter property, but we will see that it is indeed correct). While
it helps us build intuition for what the correct statement of the maximum
principle should look like, the variational approach has several limitations
which, upon closer inspection, turn out to be quite severe.

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
99
Control set. Recall that our starting point was to consider perturbed
controls of the form (3.23). Such perturbations make sense when the values
of u∗are interior points of the control set U. This may not be the case,
though, if U has a boundary, and bounded (or even ﬁnite) control sets are
common in control applications. As we will see in the next chapter, the
statement that the function u 7→H(t, x∗(t), u, p∗(t)) must have a maximum
at u∗(t) is true even in such situations and, moreover, this maximum is
in fact global. However, we cannot hope to establish this fact using the
variational approach, because Hu|∗need not be 0 when the maximum is
achieved at a boundary point of U.
Final state. In the preceding discussion we treated the case when the
ﬁnal state xf is free, but we know (see Section 3.3.3) that in general we may
have some target set S. Consider, for example, the case of a ﬁxed endpoint:
S = {t1} × {x1}. Then the control perturbation ξ is no longer arbitrary,
since the resulting state perturbation η must satisfy η(t1) = 0. In view of
the fact that η and ξ are related by the system (3.27), it is easy to show
that admissible perturbations ξ must satisfy the constraint
Z t1
t0
Φ∗(t1, t)B(t)ξ(t)dt = 0
where Φ∗(·, ·) is the state transition matrix for A∗(·) in (3.27). The second
equation in (3.37) needs to hold only for admissible perturbations, and not
for all ξ. This condition is no longer strong enough to let us conclude that
Hu|∗≡0. We see that the prospects of extending the variational approach
beyond free-endpoint problems do not look very promising.
Differentiability. When developing the ﬁrst variation, we were tacitly
assuming that H is diﬀerentiable with respect to u (as well as x). Since the
Hamiltonian H is deﬁned by (3.28), both f and L must thus be diﬀerentiable
with respect to u. The reader can readily check that diﬀerentiability of f
with respect to u was not one of the assumptions we made in Section 3.3.1
to ensure existence and uniqueness of solutions for our control system. In
other words, the variational approach requires extra regularity assumptions
to be imposed on the system. Having to assume diﬀerentiability of L with
respect to u is also undesirable, as it rules out otherwise quite reasonable
cost functionals like J(u) =
R t1
t0 |u(t)|dt. Furthermore, the analysis based on
the second variation—which is needed to distinguish between a minimum
and a maximum—involves second-order partial derivatives of H with respect
to u. It is clear that the variational approach would take us on a path of
overly restrictive regularity assumptions. Instead, we would like to establish
the Hamiltonian maximization property more directly, not via derivatives.
Control perturbations. When considering the control and state per-
turbations as in (3.23) and (3.24) with α near 0, we are allowing only small

100
CHAPTER 3
deviations in both x and u. For the system ˙x = u, this would correspond ex-
actly to the notion of a weak minimum from calculus of variations. However,
as we already discussed as early as Section 2.2.1 (see in particular Exam-
ple 2.1), we would like to have a larger family of control perturbations. More
precisely, we want to capture optimality with respect to control perturba-
tions that may be large, as long as the corresponding state trajectories are
close to the given one. For example, Figure 3.6 illustrates a particular con-
trol perturbation (for controls that switch between only two values) which
is very reasonable but falls outside the scope of the variational approach.
As we will soon see, working with a richer perturbation family is crucial for
obtaining sharper necessary conditions for optimality.
t
t
t
τ −ε
τ
u(t)
u∗(t)
u(t) −u∗(t)
Figure 3.6: A control perturbation
In summary, while the basic form of the necessary conditions provided
by the maximum principle will be similar to what we obtained using the
variational approach, several shortcomings of the variational approach must
be overcome in order to obtain a more satisfactory result. Speciﬁcally, we
need to accommodate constraints on the control set, constraints on the ﬁnal
state, and weaker diﬀerentiability assumptions. A less restrictive notion of
“closeness” of controls will be the key to achieving these goals. Borrowing a
colorful expression from [PB94], we can describe the task ahead of us as “the
cutting of the umbilical cord between the calculus of variations and optimal
control theory.” The maximum principle is a very nontrivial extension of
the variational approach, and was developed many years later. The proof
of the maximum principle is quite diﬀerent from the argument given in this
section; in particular, it is much more geometric in nature. We are now
ready, in terms of both technical preparation and conceptual motivation, to
tackle this proof in the next chapter.
3.5
NOTES AND REFERENCES FOR CHAPTER 3
Our derivation of the Weierstrass-Erdmann corner conditions is similar to
the one given in [GF63, Section 15] (except that in [GF63] it is deduced from

FROM CALCULUS OF VARIATIONS TO OPTIMAL CONTROL
101
the general formula for the variation of a functional, while we present a self-
contained argument). However, the discussion of these conditions in [GF63]
does not correctly distinguish between weak and strong minima; see [Mac05,
Chapter 11] for a more accurate treatment of this issue. The Weierstrass E-
function is introduced in [GF63, Section 34] as well as [Mac05, Section 11.3].
A proof of the Weierstrass necessary condition is only sketched in [GF63];
a complete proof similar to ours can be found, e.g., in [Lei81] or [BM91].
The interested reader should also peruse McShane’s paper [McS39], which is
based on the monograph [Bli30] by Bliss. Both [Bli30] and [McS39] actually
discuss more general constrained problems, and [Bli30] oﬀers nice historical
remarks at the end. For a careful look at the Hamiltonian reformulation
of the Weierstrass necessary condition, see [Sus00, Handout 3]. A suﬃcient
condition for a strong minimum based on the concept of a ﬁeld is developed
in [GF63, Sections 32–34].
The brachistochrone problem (Example 3.2 in Section 3.2) is studied in
detail from both the calculus of variations and the optimal control viewpoints
in the paper [SW97], which we already mentioned in the notes and references
for Chapter 2. Conditions for existence and uniqueness of solutions, along
the lines of Section 3.3.1, can be found in standard texts; see, e.g., [Kha02,
Section 3.1] or [Son98, Appendix C.3] for ordinary diﬀerential equations
without controls and [AF66, Section 3-18] or [Son98, Section 2.6] for control
systems. The conditions given in [Son98] are sharper than those in [Kha02]
and [AF66] but the proofs are essentially the same. Appendix C of [Son98]
can also be consulted for more information on absolutely continuous and
measurable functions.
From Section 3.3.2 until the end of the chapter, our primary reference
was [AF66] (which, by the way, was apparently the ﬁrst textbook on optimal
control ever written). Cost functionals and transformations between their
various forms are presented in [AF66, Sections 4-12, 5-14, 5-16]. The discus-
sion on target sets is compiled from [AF66, Sections 4-12, 4-13, 5-12, 5-14].
Our presentation of the variational approach follows [AF66, Section 5-7],
although the sign convention in [AF66] is the opposite of ours (see the ex-
planation on page 97 above). Suﬃcient conditions for optimality in terms of
the second variation—the subject that we only touched on in Exercise 3.8—
are developed in [AF66, Section 5-8]. The material of Sections 3.4.4 and 3.4.5
is elaborated upon in [AF66, Sections 5-9, 5-10]. Finally, we mention that
two-point boundary value problems of the kind that we saw in the neces-
sary conditions for optimality stated in Section 3.4.3 (and will see again in
the maximum principle) can be solved numerically using the shooting or
forward-backward sweep methods; see, e.g., [DM70, Chapter 8] for detailed
information on this topic.

Chapter Four
The Maximum Principle
4.1
STATEMENT OF THE MAXIMUM PRINCIPLE
This chapter is devoted to the maximum principle, which is in some sense
the focal point of the book. We already previewed in the previous chapter
the basic form of this result and the technical objectives that it is supposed
to accomplish. We will now describe two special instances of the optimal
control problem formulated in Section 3.3, for which we want to state and
then (in Section 4.2) prove the maximum principle.
Afterwards (in Sec-
tion 4.3.1) we will explain how the maximum principle for other situations
of interest can be deduced from these two speciﬁc cases.
4.1.1
Basic ﬁxed-endpoint control problem
We will label as the Basic Fixed-Endpoint Control Problem the optimal
control problem from Section 3.3 with the following additional speciﬁcations:
f = f(x, u) and L = L(x, u), with no t-argument (the control system and
the running cost are time-independent); f, fx, L, and Lx are continuous (in
other words, both f and L satisfy the stronger set of regularity conditions
from Section 3.3.1); the target set is S = [t0, ∞) × {x1} (this is a free-time,
ﬁxed-endpoint problem); and K ≡0 (the terminal cost is absent). For this
special problem, the maximum principle takes the following form.
Maximum Principle for the Basic Fixed-Endpoint Control Prob-
lem: Let u∗: [t0, tf] →U be an optimal control (in the global sense) and
let x∗: [t0, tf] →Rn be the corresponding optimal state trajectory. Then
there exist a function p∗: [t0, tf] →Rn and a constant p∗
0 ≤0 satisfying
(p∗
0, p∗(t)) ̸= (0, 0) for all t ∈[t0, tf] and having the following properties:
1) x∗and p∗satisfy the canonical equations
˙x∗= Hp(x∗, u∗, p∗, p∗
0),
˙p∗= −Hx(x∗, u∗, p∗, p∗
0)
(4.1)
102

THE MAXIMUM PRINCIPLE
103
with the boundary conditions x∗(t0) = x0 and x∗(tf) = x1, where the
Hamiltonian H : Rn × U × Rn × R →R is deﬁned as
H(x, u, p, p0) := ⟨p, f(x, u)⟩+ p0L(x, u).
(4.2)
2) For each ﬁxed t, the function u 7→H(x∗(t), u, p∗(t), p∗
0) has a global
maximum at u = u∗(t), i.e., the inequality
H(x∗(t), u∗(t), p∗(t), p∗
0) ≥H(x∗(t), u, p∗(t), p∗
0)
holds for all t ∈[t0, tf] and all u ∈U.
3) H(x∗(t), u∗(t), p∗(t), p∗
0) = 0 for all t ∈[t0, tf].
A few clariﬁcations are in order. First, the maximum principle, as stated
here, describes necessary conditions for global optimality. However, we an-
nounced in Section 3.4.5 that one of our goals is to capture an appropriate
notion of local optimality. The proof of the maximum principle will make it
clear that the same conditions are indeed necessary for local optimality in the
sense outlined in Section 3.4.5. We thus postpone further discussion of this
issue until after the proof (see Section 4.3). Second, while the adjoint vector,
or costate, p∗is already familiar from Section 3.4, one diﬀerence with the
necessary conditions derived using the variational approach is the presence
of p∗
0. This nonpositive scalar is called the abnormal multiplier. Similarly
to the abnormal multiplier λ∗
0 from Section 2.5, it equals 0 in degenerate
cases; otherwise p∗
0 ̸= 0 and we can recover our earlier deﬁnition (3.28) of
the Hamiltonian by normalizing (p∗
0, p∗(t)) so that p∗
0 = −1 (note that such
scaling does not aﬀect any of the properties listed in the statement of the
maximum principle). In the future, whenever the abnormal multiplier is not
explicitly written, it is assumed to be equal to −1. Finally, in Section 3.4.4
we saw another scenario where H was constant, but the claim that H ≡0
may seem surprising.
We will see later (in Section 4.3.1) that this is a
special feature of free-time problems. The next exercise provides an early
illustration of the usefulness of the above result.
Exercise 4.1
Consider the optimal control formulation of the brachis-
tochrone problem presented in Example 3.2 in Section 3.2. Use the maxi-
mum principle to conﬁrm the fact (already known from Exercise 2.5) that
optimal curves are cycloids given by (2.7).
□
Let us now ask ourselves how restricted the Basic Fixed-Endpoint Con-
trol Problem really is. Time-independence of f and L and the absence of
the terminal cost do not really introduce a loss of generality. Indeed, we
know from Section 3.3 that we can eliminate t and K from the problem
formulation by introducing the extra state variable xn+1 := t (although this

104
CHAPTER 4
entails stronger regularity assumptions on the original right-hand side as a
function of t) and passing to the new running cost ˆL := L + Kt + Kx · f. On
the other hand, the target set S = [t0, ∞) × {x1} is not very general, as it
does not allow any ﬂexibility in choosing the ﬁnal state. This motivates us
to consider the following reﬁned problem formulation.
4.1.2
Basic variable-endpoint control problem
The Basic Variable-Endpoint Control Problem is the same as the Basic
Fixed-Endpoint Control Problem except the target set is now of the form
S = [t0, ∞) × S1, where S1 is a k-dimensional surface in Rn for some non-
negative integer k ≤n. As in Section 1.2.21 we deﬁne such a surface via
equality constraints:
S1 = {x ∈Rn : h1(x) = h2(x) = · · · = hn−k(x) = 0}
where hi, i = 1, . . . , n −k are C1 functions from Rn to R. We also assume
that every x ∈S1 is a regular point.
As two extreme special cases, for
k = n we obtain S1 = Rn (which gives a free-time, free-endpoint problem)
while for k = 0 the surface S1 reduces either to a single point (as in the
Basic Fixed-Endpoint Control Problem) or to a set consisting of individual
points.
The diﬀerence between the maximum principle for this problem
and the previous one lies only in the boundary conditions for the system of
canonical equations.
Maximum Principle for the Basic Variable-Endpoint Control
Problem: Let u∗: [t0, tf] →U be an optimal control and let x∗: [t0, tf] →
Rn be the corresponding optimal state trajectory. Then there exist a function
p∗: [t0, tf] →Rn and a constant p∗
0 ≤0 satisfying (p∗
0, p∗(t)) ̸= (0, 0) for all
t ∈[t0, tf] and having the following properties:
1) x∗and p∗satisfy the canonical equations (4.1) with respect to the
Hamiltonian (4.2), with the boundary conditions x∗(t0) = x0 and
x∗(tf) ∈S1.
2) H(x∗(t), u∗(t), p∗(t), p∗
0) ≥H(x∗(t), u, p∗(t), p∗
0) for all t ∈[t0, tf] and
all u ∈U.
3) H(x∗(t), u∗(t), p∗(t), p∗
0) = 0 for all t ∈[t0, tf].
4) The vector p∗(tf) is orthogonal to the tangent space to S1 at x∗(tf):
⟨p∗(tf), d⟩= 0
∀d ∈Tx∗(tf)S1.
(4.3)
1The reader might need to revisit that section, as its terminology and notation will
be freely used here.

THE MAXIMUM PRINCIPLE
105
The additional necessary condition (4.3) is called the transversality con-
dition (we encountered its loose analog in Example 2.4 and Exercise 2.6 in
Section 2.3.5). We know from Section 1.2.2 that the tangent space can be
characterized as
Tx∗(tf)S1 = {d ∈Rn : ⟨∇hi(x∗(tf)), d⟩= 0, i = 1, . . . , n −k}
(4.4)
and that (4.3) is equivalent to saying that p∗(tf) is a linear combination of
the gradient vectors ∇hi(x∗(tf)), i = 1, . . . , n −k. Note that when k = n
and hence S1 = Rn, the transversality condition reduces to p∗(tf) = 0
(because the tangent space is the entire Rn). On the other hand, the previous
version of the maximum principle is a special case of the present result: when
S1 = {x1}, its tangent space is 0 and (4.3) is true for all p∗(tf). In general,
here as well as in the Basic Fixed-Endpoint Control Problem, we have n
boundary conditions imposed on (x∗, p∗) at t = t0 and n more at t = tf.
This gives the correct total number of boundary conditions to specify a
solution of the 2n-dimensional system (4.1). However, in the Basic Fixed-
Endpoint Control Problem x∗(tf) was ﬁxed and p∗(tf) was free, while here
we have k degrees of freedom for x∗(tf) ∈S1 but only n −k degrees of
freedom for p∗(tf) ⊥Tx∗(tf)S1. We see that the freer the state, the less
free the costate: each additional degree of freedom for x∗(tf) eliminates one
degree of freedom for p∗(tf).
The maximum principle was developed by the Pontryagin school in the
Soviet Union in the late 1950s. It was presented to the wider research com-
munity at the ﬁrst IFAC World Congress in Moscow in 1960 and in the
celebrated book [PBGM62]. It is worth reﬂecting that the developments we
have covered so far in this book—starting from the Euler-Lagrange equa-
tion, continuing to the Hamiltonian formulation, and culminating in the
maximum principle—span more than 200 years. The progress made dur-
ing this time period is quite remarkable, yet the origins of the maximum
principle are clearly traceable to the early work in calculus of variations.
4.2
PROOF OF THE MAXIMUM PRINCIPLE
Our proof of the two versions of the maximum principle stated in the pre-
vious section will be divided into the following steps.
Step 1: From Lagrange to Mayer form As the ﬁrst step, we will pass
from the given Lagrange problem to an equivalent problem in the
Mayer form by appending an additional state (this technique was al-
ready discussed in Section 3.3.2).
Step 2: Temporal control perturbation In the next step, we will ap-
ply a small perturbation to the length of the time interval over which

106
CHAPTER 4
the optimal control is acting, and will characterize the resulting per-
turbation of the terminal state.
Step 3: Spatial control perturbation In this step, we will replace the
optimal control on a small time interval by an arbitrary constant con-
trol, and will study how the resulting state trajectory deviates from
the optimal one at the end of this time interval. (This perturbation
will be reminiscent of the one we used in the proof of the Weierstrass
necessary condition in Section 3.1.2.)
Step 4: Variational equation We will then derive a linear diﬀerential
equation which propagates, modulo terms of higher order, the eﬀect
of spatial control perturbations up to the terminal time. This is the
variational equation (we already encountered this terminology in Sec-
tion 2.6.2).
Step 5: Terminal cone Combining the eﬀects of temporal and spatial
control perturbations, we will construct a convex cone, with vertex at
the terminal state of the optimal trajectory, which describes inﬁnites-
imal directions of all possible perturbations of the terminal state.
Step 6: Key topological lemma Next, we will use optimality to show
that the terminal cone does not contain in its interior the direction of
decreasing cost.
Step 7: Separating hyperplane We will invoke the separating hyper-
plane theorem to establish the existence of a hyperplane that passes
through the terminal state and separates the terminal cone from the
direction of decreasing cost. We will deﬁne the adjoint vector at the
terminal time as the normal to this separating hyperplane.
Step 8: Adjoint equation We will then introduce the adjoint equation
which propagates the adjoint vector up to the terminal time; it will
match the second canonical equation, as required by the maximum
principle.
Step 9: Properties of the Hamiltonian We will verify that the Hamil-
tonian maximization condition holds and that the Hamiltonian is iden-
tically 0 along the optimal trajectory. This will conclude the proof of
the maximum principle for the Basic Fixed-Endpoint Control Prob-
lem.
Step 10: Transversality condition Finally, we will prove the maximum
principle for the Basic Variable-Endpoint Control Problem by reﬁn-
ing the separation property from steps 6 and 7 and arriving at the
transversality condition.

THE MAXIMUM PRINCIPLE
107
Each of the subsections that follow corresponds to one step in the proof.
Until we reach step 10 in Section 4.2.10, we assume that we are dealing with
the Basic Fixed-Endpoint Control Problem, i.e., S = [t0, ∞) × {x1}. The
next exercise is meant to be worked on in parallel with following the proof.
Exercise 4.2
Consider the double integrator ˙x1 = x2, ˙x2 = u with
u ∈[−1, 1].
Let the initial condition be x(0) =
00

, let the ﬁnal state
be
22

, and let the running cost be L ≡1 so that we have a time-optimal
control problem. It is easy to check that the control u∗(t) = 1, t ∈[0, 2] is
optimal. Specialize the main steps of the proof of the maximum principle
to this problem and this control. Bring the relevant concepts and conclu-
sions to the level of explicit numerical formulas, and draw ﬁgures wherever
appropriate.
□
4.2.1
From Lagrange to Mayer form
As in Section 3.3.2, we deﬁne an additional state variable, x0 ∈R, to be the
solution of
˙x0 = L(x, u),
x0(t0) = 0
and arrive at the augmented system
˙x0 = L(x, u),
˙x = f(x, u)
(4.5)
with the initial condition
 0x0

. The cost can then be rewritten as
J(u) =
Z tf
t0
˙x0(t)dt = x0(tf)
(4.6)
which means that in the new coordinates the problem is in the Mayer form
(there is a terminal cost and no running cost). Also, the target set becomes
[t0, ∞) × R × {x1} =: [t0, ∞) × S′; here S′ is the line in Rn+1 that passes
through
 0x1

and is parallel to the x0-axis. To simplify the notation, we
deﬁne
y :=
x0
x

∈Rn+1
and write the system (4.5) more compactly as
˙y =
L(x, u)
f(x, u)

=: g(y, u)
(4.7)
(the right-hand side actually does not depend on x0). Note that this system
is well posed because we assumed that L has the same regularity properties
as f.

108
CHAPTER 4
An optimal trajectory x∗(·) of the original system in Rn corresponds in
an obvious way to an optimal trajectory y∗(·) of the augmented system in
Rn+1. The ﬁrst component x0,∗of y∗describes the evolution of the cost
in the original problem, and x∗is recovered from y∗by projection onto Rn
parallel to the x0-axis. This situation is depicted in Figure 4.1 (note that L
is not necessarily positive, so x0 need not actually be increasing along y∗).
In this and all subsequent ﬁgures, the x0-axis will be vertical.
S′

0
x0

 0
x1

x1···k
xk+1···n
y∗
x0
x∗
Figure 4.1: The optimal trajectory of the augmented system
From now on, we let t∗denote the terminal time of the optimal trajec-
tory x∗(or, what is the same, of y∗). The next exercise oﬀers a geometric
interpretation of optimality; it will not be directly used in the current proof,
but we will see a related idea in Section 4.2.6.
Exercise 4.3
Let t1 and t2 be arbitrary time instants satisfying t0 ≤t1 <
t2 ≤t∗. Let S′′ be the line passing through

0
x∗(t2)

and parallel to the x0-
axis. Show that no trajectory y starting at y∗(t1) =
x0,∗(t1)
x∗(t1)

can meet S′′
below2 the point y∗(t2) =
x0,∗(t2)
x∗(t2)

at any time t3, even not equal to t2.
□
In the particular case when t2 = t∗, the claim in the exercise should be
obvious: no other trajectory starting from some point y∗(t1) on the optimal
trajectory can hit the line S′ at a point lower than y∗(t∗). In other words, a
ﬁnal portion of the optimal trajectory must itself be optimal with respect to
its starting point as the initial condition. This idea, known as the principle
of optimality, is illustrated in Figure 4.2. (The reader will notice that we
are using diﬀerent axes in diﬀerent ﬁgures.)
2I.e., at a point with a smaller x0-coordinate.

THE MAXIMUM PRINCIPLE
109
im
p
oss
ible
x
x0
S′
x0
x1
y∗
y∗(t∗)
Figure 4.2: Principle of optimality
Another simple observation, which will be useful later, is that the Hamil-
tonian (4.2) can be equivalently represented as the following inner product
in Rn+1:
H(x, u, p, p0) =
p0
p

,
L(x, u)
f(x, u)

.
(4.8)
4.2.2
Temporal control perturbation
Let us see what happens if we introduce a small change in the terminal
time t∗of the optimal trajectory, i.e., let the optimal control act on a little
longer or a little shorter time interval. We formalize this as follows: for an
arbitrary τ ∈R and a small ε > 0, we consider the perturbed control
uτ(t) := u∗(min{t, t∗}),
t ∈[t0, t∗+ ετ]
which is illustrated by the thick curves in Figure 4.3 (for the two cases
depending on the sign of τ).
t
t
uτ(t)
uτ(t)
t∗
t∗
t∗+ ετ
t∗+ ετ
u∗
u∗
Figure 4.3: A temporal perturbation
We are interested in the value of the resulting perturbed trajectory y at
the new terminal time t∗+ ετ. For τ > 0, the ﬁrst-order Taylor expansion

110
CHAPTER 4
of y around t = t∗gives
y(t∗+ ετ) = y∗(t∗) + ˙y(t∗)ετ + o(ε) = y∗(t∗) + g(y∗(t∗), u∗(t∗))ετ + o(ε)
=: y∗(t∗) + εδ(τ) + o(ε).
(4.9)
For τ < 0, we have y(t∗+ ετ) = y∗(t∗+ ετ) and the ﬁrst-order Taylor
expansion of y∗around t = t∗gives the same result.
The vector εδ(τ)
describes the inﬁnitesimal (ﬁrst-order in ε) perturbation of the terminal
point.
By deﬁnition, δ(τ) depends linearly on τ.
As we vary τ over R,
keeping ε ﬁxed, the points y∗(t∗) + εδ(τ) form a line through y∗(t∗). We
denote this line by ⃗ρ; see Figure 4.4. Every point on ⃗ρ corresponds to a
control uτ for some τ. On the other hand, the approximation of y(t∗+ ετ)
by y∗(t∗) + εδ(τ) is valid only in the limit as ε →0. So, δ(τ) tells us the
direction—but not the magnitude—of the terminal point deviation caused
by an inﬁnitesimal change in the terminal time. The arrow over ρ is meant
to indicate that points on the line correspond to perturbation directions.
Note that we are describing deviations of the terminal point in the (x0, x)-
space only, ignoring the diﬀerences in the terminal times; accordingly, the
time axis is not included in the ﬁgures.
x
x0
S′
⃗ρ
x0
x1
y∗
y∗(t∗)
Figure 4.4: The eﬀect of a temporal control perturbation
4.2.3
Spatial control perturbation
We now construct control perturbations known as “needle” perturbations, or
Pontryagin-McShane perturbations. As the former name suggests, they will
be represented by pulses of short duration; the reason for the latter name
is that perturbations of this kind were ﬁrst used by McShane in calculus of
variations (see Section 3.1.2) and later adopted by Pontryagin’s school for
the proof of the maximum principle.

THE MAXIMUM PRINCIPLE
111
Let w be an arbitrary element of the control set U. Consider the interval
I := (b −εa, b] ⊂(t0, t∗), where b ̸= t∗is a point of continuity3 of u∗, a > 0
is arbitrary, and ε > 0 is small. We deﬁne the perturbed control
uw,I(t) :=
(
u∗(t)
if t /∈I,
w
if t ∈I.
Figure 4.5 illustrates this control perturbation and the resulting state tra-
jectory perturbation.
As the ﬁgure suggests, the perturbed trajectory y
corresponding to uw,I will deviate from y∗on the interval I and afterwards
will “run parallel” to y∗. We now proceed to formally characterize the de-
viation over I; the behavior of y over the interval [b, t∗] will be studied in
Section 4.2.4.
t
t
uw,I(t)
y∗
u∗
t0
t0
t∗
t∗
w
b −εa
b −εa
b
b
I
y(t)
Figure 4.5: A spatial control perturbation and its eﬀect on the trajectory
We will let ≈denote equality up to terms of order o(ε). The ﬁrst-order
Taylor expansion of y∗around t = b gives
y∗(b −εa) ≈y∗(b) −˙y∗(b)εa.
(4.10)
Rearranging terms and using the fact that y∗satisﬁes the diﬀerential equa-
tion (4.7) with u = u∗, we have
y∗(b) ≈y∗(b −εa) + g(y∗(b), u∗(b))εa.
(4.11)
On the other hand, the ﬁrst-order Taylor expansion of the perturbed solution
y around t = b −εa yields
y(b) ≈y(b −εa) + ˙y(b −εa)εa
where by ˙y(b−εa) we mean the right-sided derivative of y at t = b−εa. Since
y(b −εa) = y∗(b −εa) by construction and y satisﬁes (4.7) with u = uw,I,
we obtain
y(b) ≈y∗(b −εa) + g(y∗(b −εa), w)εa.
(4.12)
3The reason for this assumption is that the subsequent Taylor expansions rely on y
being diﬀerentiable at t = b.

112
CHAPTER 4
We now apply the Taylor expansion to the last term in (4.12):
g(y∗(b−εa), w)εa ≈g(y∗(b), w)εa+gy(y∗(b), w)(y∗(b−εa)−y∗(b))εa. (4.13)
In view of (4.10), the second term on the right-hand side of (4.13) is of
order ε2; hence we can omit it and the approximation will remain valid.
Thus (4.12) simpliﬁes to
y(b) ≈y∗(b −εa) + g(y∗(b), w)εa.
Comparing this formula with (4.11), we arrive at
y(b) ≈y∗(b) + νb(w)εa
(4.14)
where
νb(w) := g(y∗(b), w) −g(y∗(b), u∗(b)).
(4.15)
Intuitively, this result makes sense: up to terms of order o(ε), the diﬀerence
between the two states y(b) and y∗(b) is the diﬀerence (4.15) between the
state velocities at y = y∗(b) corresponding to u = w and u = u∗(b), mul-
tiplied by the length εa of the time interval on which the perturbation is
acting.
4.2.4
Variational equation
We are now interested in how the diﬀerence between the trajectory y arising
from a spatial (needle) perturbation and the optimal trajectory y∗prop-
agates after the perturbation stops acting, i.e., for t ≥b. To study this
question, let us begin by writing
y(t) = y∗(t) + εψ(t) + o(ε) =: y(t, ε)
(4.16)
for b ≤t ≤t∗, where ψ : [b, t∗] →Rn+1 is a quantity that we want to
characterize and ε is the same as in Section 4.2.3. We know from (4.14) that
ψ(b) exists and is given by
ψ(b) = νb(w)a.
(4.17)
We now derive a diﬀerential equation that ψ must satisfy on the time interval
(b, t∗], where both y and y∗have the same corresponding control u∗. It is
clear from (4.16) that
ψ(t) = yε(t, 0)
(4.18)
(the existence of this partial derivative, and therefore of ψ, for t > b will
become evident in a moment). Let us rewrite the system (4.7) as an integral
equation:
y(t, ε) = y(b, ε) +
Z t
b
g(y(s, ε), u∗(s))ds.

THE MAXIMUM PRINCIPLE
113
Diﬀerentiating both sides of this equation with respect to ε at ε = 0 and
using (4.16) with t = b and (4.17), we obtain
yε(t, 0) = νb(w)a +
Z t
b
gy(y(s, 0), u∗(s))yε(s, 0)ds
which, in view of (4.16) and (4.18), amounts to
ψ(t) = νb(w)a +
Z t
b
gy(y∗(s), u∗(s))ψ(s)ds.
Taking the derivative with respect to t, we conclude that ψ satisﬁes the
diﬀerential equation
˙ψ = gy(y∗, u∗)ψ = gy|∗ψ.
(4.19)
We will use this equation to describe how spatial perturbations propagate
with time. Pictorially, the role of ψ is illustrated in Figure 4.6, with the
understanding that the labels involving ψ are accurate only up to terms of
order o(ε).
x
x0
S′
x0
x1
y∗
y∗(t∗)
y∗(b)
εψ(b)
εψ(t∗)
y
Figure 4.6: Propagation of a spatial perturbation
The equation (4.19) can be written in the form
˙ψ = A∗(t)ψ
(4.20)
where A∗(t) := gy|∗(t). This system is simply the linearization of the orig-
inal system (4.7) around the optimal trajectory y∗.
Recall that a more
detailed description of the system (4.7) is given by (4.5). Letting (η0, η) be
the corresponding components of ψ and writing out the variational equa-
tion (4.19) in terms of these components, we easily arrive at
˙η0 = (Lx)T 
∗η,
˙η = fx|∗η

114
CHAPTER 4
(because L and f do not depend on x0). Here (Lx)T is a row vector (the
transpose of the gradient of L with respect to x) and fx is an n × n matrix
(the Jacobian matrix of f with respect to x). The resulting more explicit
formula for A∗is
A∗(t) =

0
(Lx)T 
∗(t)
0
fx|∗(t)

.
(4.21)
The equation (4.19) is in fact the variational equation corresponding to
the system (4.7), according to the terminology introduced in Section 2.6.2.
The equation (3.26) in Section 3.4 is also similar and was similarly derived,
although it served a diﬀerent purpose (it described the eﬀect of a small
control perturbation on a given trajectory, while here we are studying the
diﬀerence between two nearby trajectories with the same control). Also, we
see that ε in the deﬁnition of a needle perturbation essentially corresponds
to α in the variational approach; we chose to use diﬀerent symbols for these
two parameters because of the diﬀerent speciﬁc ways in which they are
introduced.
The value of ψ at the terminal time t∗gives us an approximation of
the terminal point y(t∗) of the perturbed trajectory. Namely, from (4.16)
evaluated at t = t∗we have
y(t∗) = y∗(t∗) + εψ(t∗) + o(ε).
(4.22)
Let us denote by Φ∗(·, ·) the state transition matrix for the linear time-
varying system (4.20), so that
ψ(t∗) = Φ∗(t∗, b)ψ(b).
The initial value ψ(b) is given by (4.17), hence
ψ(t∗) = Φ∗(t∗, b)νb(w)a
where νb(w) was deﬁned in (4.15). Plugging this expression into (4.22), we
obtain
y(t∗) = y∗(t∗) + εΦ∗(t∗, b)νb(w)a + o(ε).
(4.23)
Let us introduce the notation
δ(w, I) := Φ∗(t∗, b)νb(w)a
(4.24)
to arrive at the more compact formula
y(t∗) = y∗(t∗) + εδ(w, I) + o(ε)
(here the interval I used for constructing the needle perturbation encodes
information about the values of a and b).

THE MAXIMUM PRINCIPLE
115
4.2.5
Terminal cone
We now want to describe geometrically the combined eﬀect of the temporal
and spatial control perturbations on the terminal state. The vector εδ(w, I)
describes the inﬁnitesimal (ﬁrst-order in ε) perturbation of the terminal state
caused by the needle perturbation with parameters w and I. (It corresponds
to the vector labeled as εψ(t∗) in Figure 4.6.) From the deﬁnition (4.24) of
δ(w, I), it is clear that its direction depends only on w and b, but not on
a. We let ⃗ρ(w, b) denote the ray in this direction originating at y∗(t∗). If
we keep w, b, and ε ﬁxed, ⃗ρ(w, b) consists of the points y∗(t∗) + εδ(w, I) for
various values of a. The construction of ⃗ρ(w, b) is analogous to that of ⃗ρ in
Section 4.2.2, except that ⃗ρ(w, b) is unidirectional (because both a and ε are
positive) whereas ⃗ρ was bidirectional. We also let ⃗P denote the union of the
rays ⃗ρ(w, b) for all possible values of w and b. Then ⃗P is a cone with vertex
at y∗(t∗). Note that this cone is not convex; for example, if the control set
U (in which w takes values) is ﬁnite, then ⃗P will in general be a union of
isolated rays starting at y∗(t∗).
Let us now ask ourselves the following question: is there a spatial con-
trol perturbation such that the corresponding ﬁrst-order perturbation of the
terminal point is, say, εδ(w1, I1) + εδ(w2, I2) for some control values w1, w2
and intervals I1 = (b1 −εa1, b1] and I2 = (b2 −εa2, b2]? We will now see that
the right way to “add” two needle perturbations is to concatenate them,
i.e., to perturb u∗both on I1 (by setting it equal to w1 there) and on I2 (by
setting it equal to w2). Here we are assuming that b1 < b2, so that for ε
small enough I1 and I2 do not overlap. Such a spatial perturbation is shown
in Figure 4.7.
t
U
u∗
b1
b2
w1
w2
Figure 4.7: “Adding” spatial perturbations
The resulting ﬁrst-order perturbation of the terminal point will then be
the sum εδ(w1, I1) + εδ(w2, I2). This is true simply because the variational
equation, which propagates ﬁrst-order state perturbations up to the terminal
time, is linear. Indeed, according to the formulas derived in the two previous

116
CHAPTER 4
subsections, we will have
y(b1) = y∗(b1) + νb1(w1)εa1 + o(ε)
at the end of the ﬁrst perturbation interval, then
y(b2) = y∗(b2) + ε
 Φ∗(b2, b1)νb1(w1)a1 + νb2(w2)a2

+ o(ε)
at the end of the second perturbation interval, and ﬁnally, by the semigroup
property Φ∗(t∗, b2)Φ∗(b2, b1) = Φ∗(t∗, b1) of the state transition matrix,
y(t∗) = y∗(t∗) + εΦ∗(t∗, b2)
 Φ∗(b2, b1)νb1(w1)a1 + νb2(w2)a2

+ o(ε)
= y∗(t∗) + εΦ∗(t∗, b1)νb1(w1)a1 + εΦ∗(t∗, b2)νb2(w2)a2 + o(ε)
= y∗(t∗) + εδ(w1, I1) + εδ(w2, I2) + o(ε).
Exercise 4.4
Explain how to “add” two needle perturbations with b1 =
b2. In other words, given two control values w1 and w2 and two intervals
I1 = (b−εa1, b] and I2 = (b−εa2, b] with the same right endpoint, construct
a spatial control perturbation such that the resulting terminal point satisﬁes
y(t∗) = y∗(t∗) + εδ(w1, I1) + εδ(w2, I2) + o(ε). (Keep in mind that w1 + w2
might not be an admissible control value.)
□
More generally, if we want to generate the inﬁnitesimal perturbation
εβ1δ(w1, I1) + εβ2δ(w2, I2) for some β1, β2 > 0, then it is easy to see that
we need to adjust the lengths of the intervals on which the two needle per-
turbations are acting: we need to set u(t) = w1 on ¯I1 := (b1 −εβ1a1, b1] and
u(t) = w2 on ¯I2 := (b2 −εβ2a2, b2]. It is also clear that this construction
immediately extends to linear combinations (with positive coeﬃcients) of
more than two terms.
Recall that ⃗P is the cone with vertex at y∗(t∗) formed by the rays ⃗ρ(w, b)
corresponding to all simple (individual) needle perturbations. The preced-
ing discussion demonstrates that by concatenating diﬀerent needle pertur-
bations, we generate a larger cone (with the same vertex) which consists
exactly of convex combinations of points in ⃗P. We denote this convex cone
by co(⃗P).
In Section 4.2.2 we also constructed the line ⃗ρ of perturbation directions
arising from the temporal perturbations of u∗.
We now add this line to
the convex cone co( ⃗P), in the sense of adding vectors attached to the point
y∗(t∗). More precisely, we consider the set of points of the form
y = y∗(t∗) + ε

β0δ(τ) +
m
X
i=1
βiδ(wi, Ii)

(4.25)
where ε > 0, β0, β1, . . . , βm ≥0, δ(τ) comes from (4.9) for some τ, and each
δ(wi, Ii) comes from (4.24) for some wi and Ii. We denote this set by Ct∗

THE MAXIMUM PRINCIPLE
117
and call it the terminal cone. It is easy to check that Ct∗is again a convex
cone, with vertex at y∗(t∗). This construction is illustrated in Figure 4.8,
where Ct∗is the inﬁnite “wedge” between the two half-planes.
⃗ρ
co(⃗P)
y∗(t∗)
Figure 4.8: The terminal cone
By the same reasoning as before, we can show that for every point y ∈Ct∗
given by (4.25) there exists a perturbation of u∗such that the terminal point
of the perturbed trajectory satisﬁes
y(tf) = y + o(ε).
To obtain the desired control perturbation, we need to apply a concatenated
spatial perturbation as explained above, followed by the temporal pertur-
bation that adjusts the terminal time by β0ετ. Since the intervals Ii are
strictly inside [t0, t∗], they do not interfere with the temporal perturbation
(for small enough ε). The fact that the resulting total ﬁrst-order perturba-
tion of the terminal point is indeed the correct one hinges on the linearity
of the variational equation and on the linear dependence of δ(τ) on τ.
4.2.6
Key topological lemma
Up until now, we have not yet used the fact that u∗is an optimal control and
y∗is an optimal trajectory. As discussed in Section 4.2.1 and demonstrated
in Figure 4.2, optimality means that no other trajectory y corresponding to
another control u can reach the line S′ (the vertical line through
 0x1

in
the y-space) at a point below y∗(t∗). Since the terminal cone Ct∗is a linear
approximation of the set of points that we can reach by applying perturbed
controls, we expect that the terminal cone should face “upward.”
To formalize this observation, consider the vector
µ :=
 −1
0
· · ·
0
T ∈Rn+1
(4.26)
and let ⃗µ be the ray generated by this vector (which points downward) orig-
inating at y∗(t∗). Optimality suggests that ⃗µ should be directed outside of
Ct∗, a situation illustrated in Figure 4.9. Since Ct∗is only an approximation,
the correct claim is actually slightly weaker.

118
CHAPTER 4
Ct∗
⃗µ
y∗(t∗)
x1···k
xk+1···n
x0
Figure 4.9: Illustrating the statement of Lemma 4.1
Lemma 4.1. ⃗µ does not intersect the interior of the cone Ct∗.
In other words, ⃗µ can in principle touch Ct∗along the boundary, but it
cannot lie inside it. We note that since Ct∗is a cone, ⃗µ intersects its interior
if and only if all points of ⃗µ except y∗(t∗) are interior points of Ct∗.
Let us see what would happen if the statement of the lemma were false
and ⃗µ were inside Ct∗. By construction of the terminal cone, as explained
at the end of Section 4.2.5, there would exist a (spatial plus temporal)
perturbation of u∗such that the terminal point of the perturbed trajectory
y would be given by
y(tf) = y∗(t∗) + εβµ + o(ε)
for some (arbitrary) β > 0. Writing this out in terms of the components
(x0, x) of y and recalling the deﬁnition (4.26) of µ and the relation (4.6)
between x0 and the cost, we obtain
J(u) = J(u∗) −εβ + o(ε),
x(tf) = x1 + o(ε)
where u is the perturbed control that generates y. Presently there is no
direct contradiction with optimality of u∗yet, because the terminal point
x(tf) of the perturbed trajectory x is diﬀerent from the prescribed terminal
point x1, i.e., x need not hit the target set. Thus we see that although
Lemma 4.1 certainly seems plausible, it is not obvious.
Let us try to build a more convincing argument in support of Lemma 4.1.
If we suppose that the statement of the lemma is false, then we can pick a
point ˆy on the ray ⃗µ below y∗(t∗) such that ˆy is contained in Ct∗together
with a ball of some positive radius ε around it; let us denote this ball by
Bε. For a suitable value of β > 0, we have ˆy = y∗(t∗) + εβµ. Since the
points in Bε belong to Ct∗, they are of the form (4.25) and can be written as

THE MAXIMUM PRINCIPLE
119
y∗(t∗)+εν where the vectors εν are ﬁrst-order perturbations of the terminal
point arising from control perturbations constructed earlier. We know that
the actual terminal points of trajectories corresponding to these control
perturbations are given by
y∗(t∗) + εν + o(ε).
(4.27)
We denote the set of these terminal points by eBε; we can think of it as a
“warped” version of Bε, since it is o(ε) away from Bε.
In the above discussion, ε > 0 was ﬁxed; we now make it tend to 0. The
point y∗(t∗) + εβµ, which we relabel as ˆyε to emphasize its dependence on
ε, will approach y∗(t∗) along the ray ⃗µ as ε →0 (here β is the same ﬁxed
positive number as in the original expression for ˆy). The ball Bε, which
now stands for the ball of radius ε around ˆyε, will still belong to Ct∗and
consist of the points y∗(t∗) + εν for each value of ε.
Terminal points of
perturbed state trajectories (the perturbations being parameterized by ε)
will still generate a “warped ball” eBε consisting of points of the form (4.27).
Figure 4.10 should help visualize this construction.
ˆyε
⃗µ
y∗(t∗)
εν
ε
eBε
Figure 4.10: Proving Lemma 4.1
Since the center of Bε is on ⃗µ below y∗(t∗), the radius of Bε is ε, and
the “warping” is of order o(ε), for suﬃciently small ε the set eBε will still
intersect the ray ⃗µ below y∗(t∗). But this means that there exists a perturbed
trajectory x which hits the desired terminal point x1 with a lower value of
the cost. The resulting contradiction proves the lemma.
The above claim about a nonempty intersection of eBε and ⃗µ seems intu-
itively obvious. The original proof of the maximum principle in [PBGM62]
states that this fact is obvious, but then adds a lengthy footnote explaining

120
CHAPTER 4
that a rigorous proof can be given using topological arguments. A conceiv-
able scenario that must be ruled out is one in which the set eBε has a hole (or
dent) in it and the ray ⃗µ goes through this hole. It turns out that this is in-
deed impossible, thanks to continuity of the “warping” map that transforms
Bε to eBε. In fact, it can be shown that eBε contains, for ε small enough,
a ball centered at ˆyε whose radius is of order ε −o(ε). One quick way to
prove this is by applying Brouwer’s ﬁxed point theorem (which states that
a continuous map from a ball to itself must have a ﬁxed point).
Exercise 4.5
Prove rigorously that eBε and ⃗µ must have a nonempty
intersection for suﬃciently small ε.
□
4.2.7
Separating hyperplane
A standard result in convex analysis known as the Separating Hyperplane
Theorem (see, e.g., [Ber99, Proposition B.13] or [BV04, Section 2.5.1]) says
that if C and D are two nonempty disjoint convex sets then there exists a
hyperplane that separates them; by this we mean that C is contained in one
of the two closed half-spaces created by the hyperplane and D is contained in
the other. The ray ⃗µ is a convex set, and from the convexity of the terminal
cone Ct∗it is easy to see that its interior is convex as well. Lemma 4.1
guarantees that ⃗µ does not intersect the interior of Ct∗. Therefore, we can
apply the Separating Hyperplane Theorem to conclude the existence of a
hyperplane separating ⃗µ from the interior of Ct∗, and hence from Ct∗itself.4
Obviously, this separating hyperplane must pass through the point y∗(t∗)
which is a common point of Ct∗and ⃗µ. Such a hyperplane need not be
unique. The normal to the hyperplane is a nonzero vector in Rn+1 (it is
deﬁned up to a constant multiple once we ﬁx the hyperplane). Let us denote
this normal vector by
 p∗
0
p∗(t∗)

(4.28)
where p∗
0 ∈R and p∗(t∗) ∈Rn are, by deﬁnition, its x0-component and
x-component, respectively. Then the equation of the hyperplane is
 p∗
0
p∗(t∗)

, y

=
 p∗
0
p∗(t∗)

, y∗(t∗)

and the separation property is formally written as
 p∗
0
p∗(t∗)

, δ

≤0
∀δ such that y∗(t∗) + δ ∈Ct∗
(4.29)
4There is one degenerate case in which we cannot directly apply the Separating Hyper-
plane Theorem as described, namely, the case when the interior of Ct∗is empty. However,
it can be shown that if the interior of the convex set Ct∗is empty then there exists a
hyperplane that contains Ct∗(see, e.g., [dlF00, p. 238]), and this hyperplane trivially
separates ⃗µ and Ct∗.

THE MAXIMUM PRINCIPLE
121
and
 p∗
0
p∗(t∗)

, µ

≥0
(4.30)
where µ is the vector (4.26) which generates the ray ⃗µ. Note that if we were
to ﬂip the direction of the normal vector (4.28), the inequality signs in (4.29)
and (4.30) would be reversed; the present choice is simply a matter of sign
convention (cf. Section 3.4.4). For an illustration, see Figure 4.11 in which
the shaded object represents the separating hyperplane.
Ct∗
⃗µ
y∗(t∗)
 p∗
0
p∗(t∗)

Figure 4.11: A separating hyperplane for the Basic Fixed-Endpoint Control Prob-
lem
In view of the deﬁnition (4.26) of µ, the inequality (4.30) simply says
that p∗
0 ≤0, as required by the statement of the maximum principle. This
will be the only use of (4.30). The normal vector (4.28) will serve as the
terminal condition for the adjoint system, to be deﬁned next.
4.2.8
Adjoint equation
As we already mentioned on page 94, two (time-varying) linear systems of
the form ˙x = Ax and ˙z = −AT z are called adjoint to each other. Solu-
tions of adjoint systems are linked by the property that their inner product
⟨z(t), x(t)⟩remains constant, as shown by the following simple calculation:
d
dt⟨z, x⟩= ⟨˙z, x⟩+ ⟨z, ˙x⟩= (−AT z)T x + zT Ax = 0.
We now consider a speciﬁc pair of adjoint systems on the time interval
[t0, t∗]. As the ﬁrst system (the x-system in the preceding discussion) we
take the variational equation (4.19), described in more detail by the equa-
tions (4.20) and (4.21). The second, adjoint system is then
˙z = −AT
∗(t)z =

0
0
−Lx|∗(t)
−(fx)T 
∗(t)

z
(4.31)

122
CHAPTER 4
where in the last expression, obtained from (4.21), Lx is a column vector
(it is the gradient of L with respect to x) and (fx)T is the transpose of the
Jacobian matrix of f with respect to x. Let us denote the ﬁrst component
of z by p0 and the vector of the remaining n components of z by p. Then
the ﬁrst diﬀerential equation in (4.31) reads ˙p0 = 0 while the rest of the
system (4.31) becomes
˙p = −Lx|∗p0 −(fx)T 
∗p
which, in view of the deﬁnition (4.2) of the Hamiltonian, is equivalent to
˙p = −Hx(x∗, u∗, p, p0).
Now, let us specify the terminal condition for the system (4.31) at time t∗by
setting z(t∗) equal to the vector (4.28). Further, we relabel the p-component
of the solution z corresponding to this terminal condition as p∗. This gives
p0(t) = p∗
0 for all t and
˙p∗= −Hx(x∗, u∗, p∗, p∗
0)
which is the second canonical equation in (4.1). With a slight abuse of ter-
minology, we will sometimes refer to this diﬀerential equation as the adjoint
equation. It is easy to see that the ﬁrst canonical equation in (4.1) also holds
by the deﬁnition of H; thus statement 1 of the maximum principle has been
established. By the aforementioned property of adjoint systems, we have
 p∗
0
p∗(t)

, ψ(t)

=
 p∗
0
p∗(t∗)

, ψ(t∗)

∀t ∈[t0, t∗]
(4.32)
for every solution ψ of the variational equation (4.19).
The vector (4.28), which is normal to the separating hyperplane, is
nonzero. Since (4.31) is a homogeneous (unforced) linear time-varying sys-
tem, we have
 p∗
0
p∗(t)

̸= 0
∀t ∈[t0, t∗]
(4.33)
as required in the statement of the maximum principle. Geometrically, we
can think of the vector in (4.33) as the normal vector to a hyperplane passing
through y∗(t). We can then associate the solution of the adjoint system to
a family of hyperplanes that is “ﬂowing back” along the optimal trajectory.
In view of (4.32), the perturbed trajectory associated with ψ always remains
on the same side of the hyperplane.
4.2.9
Properties of the Hamiltonian
We are now ready to prove the remaining properties of the Hamiltonian,
namely, statements 2 and 3 of the maximum principle for the Basic Fixed-
Endpoint Control Problem (see Section 4.1.1).

THE MAXIMUM PRINCIPLE
123
Statement 2: Hamiltonian maximization condition
Let us go back to the formula (4.23), which says that the inﬁnitesimal pertur-
bation of the terminal point caused by a needle perturbation of the optimal
control with parameters w, b, a is described by the vector εΦ∗(t∗, b)νb(w)a.
We subsequently labeled this vector as εδ(w, I), and by construction y∗(t∗)+
εδ(w, I) belongs to the terminal cone Ct∗. Applying the inequality (4.29)
which encodes the separating hyperplane property, and noting that ε and a
are both positive, we have
 p∗
0
p∗(t∗)

, Φ∗(t∗, b)νb(w)

≤0.
Next, since Φ∗is the state transition matrix for the variational equation
(4.19), we know that Φ∗(t∗, b)νb(w) is the value at time t∗of the solution
of the variational equation passing through νb(w) at time b. Invoking the
adjoint property (4.32), we obtain
 p∗
0
p∗(b)

, νb(w)

≤0.
(4.34)
Since νb(w) was deﬁned in (4.15) and g(y, u) was deﬁned in (4.7), we have
νb(w) = g(y∗(b), w) −g(y∗(b), u∗(b)) =
L(x∗(b), w) −L(x∗(b), u∗(b))
f(x∗(b), w) −f(x∗(b), u∗(b))

.
We can thus expand (4.34) as follows:
 p∗
0
p∗(b)

,
L(x∗(b), w)
f(x∗(b), w)

≤
 p∗
0
p∗(b)

,
L(x∗(b), u∗(b))
f(x∗(b), u∗(b))

.
Recalling the expression (4.8) for the Hamiltonian, we see that this is equiv-
alent to
H(x∗(b), w, p∗(b), p∗
0) ≤H(x∗(b), u∗(b), p∗(b), p∗
0).
(4.35)
In the above derivation, w was an arbitrary element of the control set
U and b was an arbitrary time in the interval (t0, t∗) at which the optimal
control u∗is continuous. Thus we have established that the Hamiltonian
maximization condition holds everywhere except possibly a ﬁnite number of
time instants (discontinuities of u∗). Additionally, recall that u∗is piecewise
continuous and we adopted the convention (see page 84) that the value of
a piecewise continuous function at each discontinuity is equal to the limit
either from the left or from the right. Letting b in (4.35) approach a discon-
tinuity of u∗or an endpoint of [t0, t∗] from an appropriate side, and using
continuity of x∗and p∗in time and continuity of H in all variables, we
see that the Hamiltonian maximization condition must actually hold every-
where.

124
CHAPTER 4
This conclusion can be understood intuitively as follows. The Hamilto-
nian is the inner product of the augmented adjoint vector
p∗
0
p∗

with the
right-hand side of the augmented control system (the velocity of y). When
the optimal control is perturbed, the state trajectory deviates from the op-
timal one in a direction that makes a nonpositive inner product with the
augmented adjoint vector (at the time when the perturbation stops acting).
Therefore, such control perturbations can only decrease the Hamiltonian,
regardless of the value of the perturbed control during the perturbation
interval.
Statement 3: H|∗≡0
The separation property (4.29) applies, in particular, to δ = δ(τ) ∈Ct∗, the
terminal state perturbation vector corresponding to a temporal perturbation
of the control. We know from (4.9) and (4.7) that this vector is given by
δ(τ) =
L(x∗(t∗), u∗(t∗))
f(x∗(t∗), u∗(t∗))

τ.
Since τ can be either positive or negative, the inequality (4.29) can be sat-
isﬁed only if
 p∗
0
p∗(t∗)

,
L(x∗(t∗), u∗(t∗))
f(x∗(t∗), u∗(t∗))

= 0.
By virtue of (4.8), this is equivalent to
H(x∗(t∗), u∗(t∗), p∗(t∗), p∗
0) = 0.
In other words, H|∗equals 0 at the terminal time.5 We need to prove that
it is 0 everywhere.
Let us show that H|∗(·) = H(x∗(·), u∗(·), p∗(·), p∗
0) is a continuous func-
tion of time, even though the optimal control u∗need not be continuous.
The argument that follows is very similar to the one that the reader pre-
sumably used a while ago to solve Exercise 3.3 on page 80. Let t be a point
of discontinuity of u∗.
Of course, x∗and p∗are continuous everywhere.
Applying the Hamiltonian maximization condition (4.35) with b < t and
w = u∗(t+) and making b approach t from the left, we have
H(x∗(t), u∗(t+), p∗(t), p∗
0) ≤H(x∗(t), u∗(t−), p∗(t), p∗
0).
Similarly, applying (4.35) with b > t and w = u∗(t−), in the limit as b
approaches t from the right we obtain
H(x∗(t), u∗(t+), p∗(t), p∗
0) ≥H(x∗(t), u∗(t−), p∗(t), p∗
0).
5Here and below, when using the notation |∗we also mean that p0 = p∗
0.

THE MAXIMUM PRINCIPLE
125
Thus the two quantities must actually be equal, and the continuity claim is
established.
Next, let us show that the function H|∗is constant. In Section 3.4.4,
in the context of the variational approach, we established this property by
simply diﬀerentiating the Hamiltonian with respect to time, but here we
need to be more careful because the existence of Hu has not been assumed.
In view of the Hamiltonian maximization condition, we can write
H(x∗(t), u∗(t), p∗(t), p∗
0) = m(x∗(t), p∗(t))
where
m(x, p) := max
u∈U H(x, u, p, p∗
0).
We just saw that m(x∗(·), p∗(·)) is a continuous function of time. For an
arbitrary pair of times t, t′, we have the inequalities
H(x∗(t′), u∗(t), p∗(t′), p∗
0) −H(x∗(t), u∗(t), p∗(t), p∗
0)
≤m(x∗(t′), p∗(t′)) −m(x∗(t), p∗(t))
≤H(x∗(t′), u∗(t′), p∗(t′), p∗
0) −H(x∗(t), u∗(t′), p∗(t), p∗
0).
(4.36)
In view of the standing assumptions made at the beginning of Section 4.1.1
and the canonical equations (4.1), it is straightforward to show that the
function H(x∗(·), u∗(¯t), p∗(·), p∗
0) is continuously diﬀerentiable for each ﬁxed
¯t ∈[t0, t∗], with an upper bound on the magnitude of its derivative inde-
pendent of ¯t. From this fact and (4.36) we easily conclude that the function
m(x∗(·), p∗(·)) is locally Lipschitz. Therefore, it is absolutely continuous,
and hence diﬀerentiable for almost all t (see page 85). We can now study
its derivative.
Exercise 4.6
Show that m(x∗(·), p∗(·)) has zero derivative (wherever the
derivative exists). Do this by considering, for a pair of nearby times t, t′,
the limit
lim
t′→t
m(x∗(t′), p∗(t′)) −m(x∗(t), p∗(t))
t′ −t
and using the inequalities (4.36).
□
We have shown that the function H(x∗(·), u∗(·), p∗(·), p∗
0) equals 0 at
t = t∗, is continuous everywhere, and has zero derivative almost everywhere.
Thus it is identically 0, as claimed. Our proof of the maximum principle for
the Basic Fixed-Endpoint Control Problem is now complete. At this point
the reader should also ﬁnish Exercise 4.2.

126
CHAPTER 4
4.2.10
Transversality condition
We now turn to the Basic Variable-Endpoint Control Problem. In this case
there is an additional statement to be proved, which is the transversality
condition (4.3). In Section 4.2.6 we had that the terminal cone Ct∗was
separated from the ray ⃗µ; the reason for this was that hitting a point on ⃗µ
below y∗(t∗) contradicted optimality. When the ﬁxed endpoint x1 is replaced
by the surface S1, we would instead have a contradiction with optimality if
we were able to hit a point with a cost lower than x0,∗(t∗) whose x-component
is in S1 (but is not necessarily x∗(t∗)). Let us denote the set of such points
by D. We are looking to establish separation between convex sets; for this
reason, just as we replace the actual set of terminal points with its linear
approximation Ct∗, we will consider the linear approximation of D given by
the linear span of ⃗µ and the tangent space Tx∗(t∗)S1, i.e., the set
T :=

y ∈Rn+1 : y = y∗(t∗) +
0
d

+ βµ, d ∈Tx∗(t∗)S1, β ≥0

.
(4.37)
In Figure 4.12, T is the shaded “semi-plane.” The subspace Tx∗(t∗)S1, when
translated to y∗(t∗), gives the upper line in the ﬁgure. D is the surface (more
precisely, the open semi-surface) consisting of points which lie “directly un-
der” the upper curve in the ﬁgure. T is tangent to D along ⃗µ; both T and
D are bounded from above but extend inﬁnitely far down (and T includes
its upper boundary while D does not).
Lemma 4.2. T does not intersect the interior of the cone Ct∗.
This lemma is proved by an appropriate generalization of the argument
we used to prove Lemma 4.1. Suppose that the statement is not true. Then
we can ﬁnd a point in T which is contained in Ct∗together with some ε-ball
Bε around it. We can write this point as
ˆyε = y∗(t∗) + ε
0
d

+ εβµ
for suitable d ∈Tx∗(t∗)S1 and β > 0 (moving ˆyε slightly down if necessary,
we can ensure that it does not lie on the upper boundary of T). Since Bε
belongs to Ct∗, each point in Bε is given by y∗(t∗)+εν where εν is a ﬁrst-order
terminal state perturbation arising from a temporal and/or spatial control
perturbation. We know that the corresponding exact terminal states are of
the form y∗(t∗) + εν + o(ε) and form a “warped” version of Bε, which we
denote by eBε.
This construction remains valid as ε →0. The ball Bε is centered at
ˆyε ∈T, its radius is ε, and the “warping” that produces eBε is of order
o(ε). As in Exercise 4.5, it can be shown that eBε contains a ball centered

THE MAXIMUM PRINCIPLE
127
x1···k
S1

0
x0

x∗(t∗)
xk+1···n
y∗(t∗)
x0
Tx∗(t∗)S1
{x0,∗(t∗)} × S1
{x0,∗(t∗)} × Tx∗(t∗)S1
⃗µ
Figure 4.12: Illustrating the construction of the set T
at ˆyε whose radius is of order ε −o(ε). Furthermore, since T and D are
tangent to each other along ⃗µ, the distance from ˆyε to D is also of order
o(ε). Hence, for ε small enough, eBε actually intersects D. But this, as we
already noted, contradicts optimality of y∗, and the lemma is established.
The preceding argument is illustrated in Figure 4.13, where the plane and
the curved surface represent T and D, respectively, the shaded object is the
portion of eBε that lies between T and D, and the ray in T containing ˆyε,
ε > 0 is also shown.
By Lemma 4.2 and the Separating Hyperplane Theorem, there exists a
hyperplane that separates T and Ct∗. We denote its normal vector by (4.28)
as before. Figure 4.14 depicts Ct∗, T, and the separating hyperplane. In
view of the deﬁnition (4.37) of T and the fact that d = 0 belongs to Tx∗(t∗)S1,
the separation property still gives us the inequalities (4.29) and (4.30). Thus
all the constructions and conclusions of Sections 4.2.8 and 4.2.9 still apply,
and so we know that the ﬁrst three statements of the maximum principle
are true. On the other hand, writing the separation property for vectors in
T with β (the ⃗µ-component) equal to 0, we obtain the additional inequality
 p∗
0
p∗(t∗)

,
0
d

= ⟨p∗(t∗), d⟩≥0
∀d ∈Tx∗(t∗)S1.
(4.38)
For each d ∈Tx∗(t∗)S1 we also have −d ∈Tx∗(t∗)S1, as is clear from (4.4).
This fact and (4.38) imply that actually ⟨p∗(t∗), d⟩= 0 for all d ∈Tx∗(t∗)S1,

128
CHAPTER 4
⃗µ
y∗(t∗)
T
ˆyε
Figure 4.13: Proving Lemma 4.2
which is precisely the desired transversality condition (4.3). Our proof of
the maximum principle for the Basic Variable-Endpoint Control Problem is
now complete.
Note that in the special case when S1 = Rn (a free-time, free-endpoint
problem), the hyperplane separates Ct∗from the entire (n + 1)-dimensional
half-space that lies below y∗(t∗). Clearly, this hyperplane must be horizontal,
hence its normal must be vertical and we conclude that p∗(t∗) = 0. This is
consistent with (4.3) because Tx∗(t∗)S1 = Rn in this case.
4.3
DISCUSSION OF THE MAXIMUM PRINCIPLE
Our main objective in the remainder of this chapter is to gain a better
understanding of the maximum principle by discussing and interpreting its
statement and by applying it to speciﬁc classes of problems. We begin this
task here by making a few technical remarks.
One should always remember that the maximum principle provides nec-
essary conditions for optimality. Thus it only helps single out optimal con-
trol candidates, each of which needs to be further analyzed to determine
whether it is indeed optimal. The reader should also keep in mind that an
optimal control may not even exist (the existence issue will be addressed in
detail in Section 4.5). For many problems of interest, however, the optimal
solution does exist and the conditions provided by the maximum principle

THE MAXIMUM PRINCIPLE
129
Ct∗
⃗µ
y∗(t∗)
T

p∗
0
p∗(t∗)

Figure 4.14: A separating hyperplane for the Basic Variable-Endpoint Control
Problem
are strong enough to help identify it, either directly or after a routine addi-
tional elimination process. We already saw an example supporting this claim
in Exercise 4.1 and will study other important examples in Section 4.4.
When stating the maximum principle, we ignored the distinction between
diﬀerent kinds of local minima by working with a globally optimal control u∗,
i.e., by assuming that J(u∗) ≤J(u) for all other admissible controls u that
produce state trajectories satisfying the given endpoint constraint. However,
it is clear from the proof that global optimality was not used. The control
perturbations used in the proof produced controls u which diﬀer from u∗on
a small interval of order ε in length, making the L1 norm of the diﬀerence,
R tf
t0 |u(t) −u∗(t)|dt, small for small ε. The resulting perturbed trajectory
x, on the other hand, was close to the optimal trajectory x∗in the sense
of the 0-norm, i.e., maxt0≤t≤tf |x(t) −x∗(t)| was small for small ε (as is
clear from the calculations given in Sections 4.2.2–4.2.4). It can be shown
that the conditions of the maximum principle are in fact necessary for local
optimality when closeness in the (x, u)-space is measured by the 0-norm for
x and L1 norm for u; we stress that the Hamiltonian maximization condition
(statement 2 of the maximum principle) remains global. At this point it may
be instructive to think of the system ˙x = u as an example and to recall the
discussion in Section 3.4.5 related to Figure 3.6. In that context, the notion
of a local minimum with respect to the norm we just described is in between
the notions of weak and strong minima; indeed, weak minima are deﬁned

130
CHAPTER 4
with respect to the 0-norm for both x and u, while strong minima are with
respect to the 0-norm for x with no constraints on u. For strong minima,
the necessary conditions provided by the maximum principle are still valid.
This is not the case for weak minima, because in a needle perturbation the
control value w is no longer arbitrary: it must be close to u∗(b).
The statement of the maximum principle contains the condition (justi-
ﬁed in Section 4.2.8) that (p∗
0, p∗(t)) ̸= (0, 0) for all t. In fact, since the
origin in Rn+1 is an equilibrium of the linear adjoint system (4.31), if p∗
0,
p∗(t) vanish for some t then they must vanish for all t. Thus, the above
condition could be equivalently stated as (p∗
0, p∗(t)) ̸= (0, 0) for some t.
This condition is sometimes called the nontriviality condition, because with
(p∗
0, p∗) ≡(0, 0) all the statements of the maximum principle are trivially
satisﬁed. In some cases, it is possible to show that the adjoint vector itself,
p∗(t), is nonzero for all t. For example, suppose that the running cost L is
everywhere nonzero (this is true, for instance, in time-optimal control prob-
lems, where L ≡1). The Hamiltonian satisﬁes H|∗= ⟨p∗, f|∗⟩+ p∗
0 L|∗≡0
(by statement 3 of the maximum principle). If p∗(t) = 0 for some t, then
we have p∗
0 L|∗(t) = 0, hence p∗
0 = 0 and we reach a contradiction with
the nontriviality condition. We will give another example later involving a
terminal cost (see Exercise 4.7 below). As for the abnormal multiplier p∗
0,
since it is the vertical coordinate of the normal to the separating hyper-
plane, p∗
0 = 0 corresponds to the case when the separating hyperplane is
vertical (and cannot be tilted). The projection of such a hyperplane onto
the x-space is a hyperplane in Rn, and all perturbed controls must bring
the state x to the same side of this projected hyperplane. In a majority of
control problems this does not happen and we can set p∗
0 = −1. We also
know that the separating hyperplane cannot be vertical and p∗
0 cannot be 0
in the free-endpoint case (see the end of Section 4.2.10).
4.3.1
Changes of variables
Some control problems that we are interested in do not ﬁt into the setting
of the Basic Fixed-Endpoint or Variable-Endpoint Control Problem. We
now want to discuss several such scenarios and, for each one, to arrive at
the correct statement of the maximum principle.
One way to do this is
to formally reduce a problem at hand to one of the two basic problems
considered above by changing variables; the changes of variables that we
will use were already discussed in Section 3.3. Another approach is to adapt
the proof of the maximum principle to these new situations; this task can
be somewhat more challenging but also more insightful as it helps us better
understand the proof.
These various cases also help clarify why we say
“maximum principle” (rather than calling it a theorem): this terminology

THE MAXIMUM PRINCIPLE
131
reﬂects the fact that speciﬁc versions of the result for diﬀerent problems are
diﬀerent, but share the same basic features.
Fixed terminal time
Suppose that the terminal time tf is ﬁxed, so that the terminal time t∗of the
optimal trajectory x∗considered in the proof must equal a given value t1.
Temporal control perturbations are then no longer admissible. Accordingly,
the line ⃗ρ formed by the perturbation directions δ(τ) deﬁned in (4.9) must
not be used when generating the terminal cone Ct∗.
We now invite the
reader to check exactly where these perturbation directions δ(τ) were used
in the proof of the maximum principle. As a matter of fact, they were used
in one place only: to show that H|∗(t∗) = 0. Thus we conclude that the
Hamiltonian will now be constant but not necessarily 0 along the optimal
trajectory, while all the other conditions remain unchanged.
Let us conﬁrm this fact by reducing the ﬁxed-time problem to a free-time
one via the familiar trick of introducing the extra state variable xn+1 := t.
The system becomes
˙x = f(x, u),
x(t0) = x0,
˙xn+1 = 1,
xn+1(t0) = t0.
(4.39)
If the original target set was {t1}×S1, then for the new system we can write
the target set as [t0, ∞)×S1 ×{t1}, with the terminal time no longer explic-
itly constrained. The maximum principle for the Basic Variable-Endpoint
Control Problem can now be applied. The Hamiltonian for the new problem
is H = ⟨p, f⟩+pn+1 +p0L = H +pn+1, where H = ⟨p, f⟩+p0L is the Hamil-
tonian for the original ﬁxed-time problem. Clearly, the diﬀerential equation
for p∗is the same as before and the Hamiltonian maximization condition for
H implies the one for H. We know that H

∗= H|∗+ p∗
n+1 ≡0. Moreover,
we have ˙p∗
n+1 = −Hxn+1

∗= −Ht|∗= 0 (since H does not depend on t)
hence p∗
n+1 is constant. Thus H|∗is indeed equal to a constant, namely,
−p∗
n+1.
It is no longer guaranteed to be 0; in fact, since the ﬁnal value
of xn+1 is ﬁxed, the transversality condition gives us no information about
p∗
n+1. This property of the Hamiltonian is also consistent with what we saw
in Section 3.4.4.
Time-dependent system and cost
The same idea of appending the state variable xn+1 = t and passing to
the system (4.39) can be applied when the original system’s right-hand side
f and/or the running cost L depend on t. The Hamiltonian is now time-
dependent:
H(t, x, u, p, p0) := ⟨p, f(t, x, u)⟩+ p0L(t, x, u).
(4.40)

132
CHAPTER 4
The previous discussion remains valid up to and including the equation
˙p∗
n+1 = −Ht|∗but the right-hand side no longer equals 0. Thus, p∗
n+1 and
H|∗= −p∗
n+1 are not constant any more. Instead, we have the diﬀerential
equation
d
dt H|∗= Ht|∗
(4.41)
with the boundary condition H|∗(tf) = −p∗
n+1(tf). If the terminal time
tf in the original problem is free, then the ﬁnal value of xn+1 is free and
the transversality condition yields p∗
n+1(tf) = 0.
In this case we obtain
H|∗(tf) = 0 and, integrating (4.41), H|∗(t) = −
R tf
t
Ht|∗(s)ds. Note that
(4.41) is consistent with the equation obtained in (3.42) in the context of the
variational approach (although the middle portion of (3.42) does not apply
here).
The same conclusion can be reached by following the proof of the maxi-
mum principle and verifying that it carries over to the time-varying scenario
without major changes, except that the argument showing that H|∗is con-
stant becomes invalid and only (4.41) can be established (the reader who
solved Exercise 4.6 will readily see why). We can appreciate, however, that
in the present case the method of changing the variables is much simpler
and more reliable.
Terminal cost
Let us now consider a situation where a terminal cost of the form K = K(xf)
is present. To illustrate just one simple case, we suppose that we are dealing
with a free-time, free-endpoint problem in the Mayer form, i.e., there is no
running cost (L ≡0). We assume the function K to be diﬀerentiable as
many times as desired; everything else is as in the Basic Variable-Endpoint
Control Problem.
Since L ≡0, there is no need to consider the x0-coordinate. Temporal
and spatial control perturbations can be used to construct the terminal cone
Ct∗as before, except that it now lives in the original x-space: Ct∗⊂Rn. By
optimality, and since the terminal state is free, no perturbed state trajectory
can have a terminal cost lower than K(x∗(t∗)), the terminal cost of the
candidate optimal trajectory. Thus we expect that K should not decrease
along any direction in Ct∗, a property that we can write as
⟨−Kx(x∗(t∗)), δ⟩≤0
∀δ such that x∗(t∗) + δ ∈Ct∗.
(4.42)
Geometrically, this means that Ct∗lies on one side of the hyperplane passing
through x∗(t∗) with normal −Kx(x∗(t∗)), which we henceforth assume to be
a nonzero vector. A comparison of (4.42) with (4.29) unmistakably suggests
that we should deﬁne
p∗(t∗) := −Kx(x∗(t∗)).
(4.43)

THE MAXIMUM PRINCIPLE
133
Indeed, we know that in a free-endpoint problem the ﬁnal value of the costate
should be completely constrained (to give the correct total number of bound-
ary conditions for the system of canonical equations). With the above def-
inition of p∗(t∗) the inequality (4.42) matches (4.29), following which the
adjoint equation and the analysis of the Hamiltonian
H(x, u, p) = ⟨p, f(x, u)⟩
(4.44)
are developed in the same way as before. The companion inequality (4.30)
in our earlier proof of the maximum principle was only needed to show that
p∗
0 ≤0; here we do not have such an inequality, and we do not need it
because there is no p∗
0.
The above argument is of course not rigorous, so let us again validate our
conjecture by formally reducing the present scenario to the Basic Variable-
Endpoint Control Problem. A transformation that accomplishes this goal is
provided by the formula
K(xf) = K(x0) +
Z tf
t0
⟨Kx(x(t)), f(x(t), u(t))⟩dt.
Ignoring K(x0) which is a known constant, we arrive at an equivalent prob-
lem in the Lagrange form with L = ⟨Kx, f⟩. For this modiﬁed problem,
the Hamiltonian is H = ⟨¯p, f⟩+ p0 ⟨Kx, f⟩= ⟨¯p + p0Kx, f⟩where ¯p is the
costate. Applying the maximum principle, we obtain the diﬀerential equa-
tion ˙¯p∗= −Hx

∗= −(fx)T 
∗¯p∗−p∗
0 Kxx|∗f|∗−p∗
0(fx)T 
∗Kx|∗with the
boundary condition ¯p∗(tf) = 0. The latter tells us, by the way, that p∗
0 ̸= 0,
hence from now on we assume p∗
0 and ¯p∗to have been normalized so that
p∗
0 = −1. Now, matching the expression for H with (4.44), let us deﬁne the
costate for our original problem to be
p∗(t) := ¯p∗(t) −Kx(x∗(t)).
(4.45)
Its ﬁnal value is consistent with (4.43). We can check that p∗deﬁned in this
way satisﬁes the correct canonical equation.
Exercise 4.7
Verify that p∗given by (4.45) satisﬁes the diﬀerential equa-
tion ˙p∗= −Hx|∗with respect to the Hamiltonian (4.44). Show also that
p∗(t) ̸= 0 for all t.
□
By construction, H is maximized and equals 0 along the optimal trajec-
tory because these properties hold for H. We thus conclude that the state-
ment of the maximum principle for the present case, with respect to the
Hamiltonian (4.44), is obtained from that for the Basic Variable-Endpoint
Control Problem by eliminating p∗
0 and changing the transversality condi-
tion to p∗(tf) = −Kx(x∗(tf)). Recall that we already saw such a boundary
condition for the costate in (3.36).

134
CHAPTER 4
Exercise 4.8
Consider the following more general problem: the system
is ˙x = f(t, x, u), the cost is J(u) =
R tf
t0 L(t, x(t), u(t))dt + K(xf), and the
target set is S = [t0, ∞)×S1 where S1 is a k-dimensional surface in Rn (the
terminal time tf is free). Reduce this problem by a change of variables to the
Basic Variable-Endpoint Control Problem and arrive at a precise statement
of the maximum principle for it.
□
Initial sets
We now want to mention how the maximum principle can be extended in
another direction. We usually assume that the initial state x0 is ﬁxed, while
the ﬁnal state xf may vary within some set S1. Here, let us brieﬂy consider
the possibility that x0 may vary as well, so that we have an initial set instead
of a ﬁxed initial state. We can impose separate constraints on x0 and xf
or, more generally, we can require that
x0
xf

belong to some surface S2 in
R2n. The latter formulation allows us to capture situations where there is a
joint constraint on x0 and xf; for example, if the trajectory is to be closed—
i.e., the initial state is free but the trajectory must return to it—then S2 is
the “diagonal” in R2n. The terminal time tf can be either free or ﬁxed, as
before.
It turns out that this more general set-up can be easily handled by
modifying the transversality condition, which will now involve the values
of the costate both at the initial time and at the ﬁnal time. Namely, the
transversality condition will now say that the vector
 p∗(t0)
−p∗(tf)

must be
orthogonal to the tangent space to S2 at
x∗(t0)
x∗(tf)

:
 p∗(t0)
−p∗(tf)

, d

= 0
∀d ∈Tx∗(t0)
x∗(tf)
S2.
(4.46)
The total number of boundary conditions for the system of canonical equa-
tions is still 2n, since each additional degree of freedom for x∗(t0) leads to
one additional constraint on p∗(t0).
Exercise 4.9
Give a heuristic argument in support of the transversality
condition (4.46), based on an appropriate modiﬁcation of the proof of the
maximum principle.
□
4.4
TIME-OPTIMAL CONTROL PROBLEMS
Time-optimal control problems not only provide a useful illustration of the
maximum principle, but also have several important features that make

THE MAXIMUM PRINCIPLE
135
them interesting in their own right. Among these are the bang-bang prin-
ciple and connections with Lie brackets, to be discussed in this section. We
organize the material so as to progress from more speciﬁc to more general
settings, and we begin by revisiting Example 1.1 from Section 1.1.
4.4.1
Example: double integrator
Consider the system
¨x = u,
u ∈[−1, 1]
(4.47)
which can represent a car with position x ∈R and with bounded acceleration
u acting as the control (negative acceleration corresponds to braking). Let us
study the problem of “parking” the car at the origin, i.e., bringing it to rest
at x = 0, in minimal time. It is clear—and will follow from the analysis we
give below—that the system can indeed be brought to rest at the origin from
every initial condition. However, since the control is bounded, we cannot
do this arbitrarily fast (we are ignoring the trivial case when the system is
initialized at the origin). Thus we expect that there exists an optimal control
u∗which achieves the transfer in the smallest amount of time. (As we have
already mentioned, the issue of existence of optimal controls is important
and nontrivial in general; it will be treated in Section 4.5.)
We know that the dynamics of the double integrator (4.47) are equiva-
lently described by the state-space equations
˙x1 = x2,
˙x2 = u
(4.48)
where we assume that the initial values x1(t0) and x2(t0) are given. The
running cost is L ≡1 (cf. Example 3.2 in Section 3.2 where we discussed
another time-optimal control problem).
Accordingly, the Hamiltonian is
H = p1x2 +p2u+p0. Let u∗be an optimal control. Our problem is a special
case of the Basic Fixed-Endpoint Control Problem, and we now apply the
maximum principle to characterize u∗. The costate p∗=
p∗
1
p∗
2

must satisfy
the adjoint equation
 ˙p∗
1
˙p∗
2

=
−Hx1|∗
−Hx2|∗

=
 0
−p∗
1

.
(4.49)
The ﬁrst line of (4.49) implies that p∗
1 is equal to a constant, say c1. The
second line of (4.49) then says that p∗
2 is given by p∗
2(t) = −c1t + c2, where
c2 is another constant.

136
CHAPTER 4
Next, from the Hamiltonian maximization condition and the fact that
U = [−1, 1] we have
u∗(t) = sgn(p∗
2(t)) :=





1
if p∗
2(t) > 0,
−1
if p∗
2(t) < 0,
?
if p∗
2(t) = 0.
(4.50)
The third case in (4.50) is meant to indicate that when p∗
2 equals 0, making
the Hamiltonian independent of u, the value of u∗can in principle be arbi-
trary. (By our convention that u∗is continuous either from the left or from
the right everywhere, we know that u∗actually cannot take any values other
than 1 or −1.) Can p∗
2 be identically 0 over some time interval? If this hap-
pens, then ˙p∗
2 = −p∗
1 must also be identically 0 on that interval. However, we
already saw (on page 130) that p∗cannot vanish in a time-optimal control
problem, because the fact that H ≡0 (statement 3 of the maximum princi-
ple) would then imply that p∗
0 = 0 and the nontriviality condition would be
violated. Therefore, p∗
2 may only equal 0 at isolated points in time, and the
formula (4.50) deﬁnes u∗uniquely everywhere away from these times. How
many zero crossings can p∗
2 have? We derived a little while ago that p∗
2 is a
linear function of time. Thus p∗
2 can cross the value 0 at most once.
We conclude that the optimal control u∗takes only the values ±1 and
switches between these values at most once. Interpreted in terms of bringing
a car to rest at the origin, the optimal control strategy consists in switching
between maximal acceleration and maximal braking. The initial sign and
the switching time of course depend on the initial condition. The property
that u∗only switches between the extreme values ±1 is intuitively natural
and important; such controls are called bang-bang.
It turns out that for the present problem, the pattern identiﬁed above
uniquely determines the optimal control law for every initial condition. To
see this, let us plot the solutions of the system (4.47) in the (x, ˙x)-plane
for u ≡±1. For u ≡1, repeated integration gives ˙x(t) = t + a and then
x(t) =
1
2t2 + at + b for some constants a and b.
The resulting relation
x = 1
2 ˙x2 + c (where c = b −a2/2) deﬁnes a family of parabolas in the (x, ˙x)-
plane parameterized by c ∈R. Similarly, for u ≡−1 we obtain the family of
parabolas x = −1
2 ˙x2 +c, c ∈R. These curves are shown in Figure 4.15 (a,b),
with the arrows indicating the direction in which they are traversed. It is
easy to see that only two of these trajectories hit the origin (which is the pre-
scribed ﬁnal point). Their union is the thick curve in Figure 4.15 (c), which
we call the switching curve and denote by Γ; it is deﬁned by the relation
x = −1
2| ˙x| ˙x. The optimal control strategy thus consists in applying u = 1
or u = −1 depending on whether the initial point is below or above Γ, then
switching the control value exactly on Γ and subsequently following Γ to the
origin; no switching is needed if the initial point is already on Γ. (Thinking

THE MAXIMUM PRINCIPLE
137
slightly diﬀerently, we can generate all possible optimal trajectories—which
cover the entire plane—by starting at the origin and ﬂowing backward in
time, ﬁrst following Γ and then switching at an arbitrary point on Γ.) Recall-
ing the interpretation of our problem as that of parking a car using bounded
acceleration/braking, the reader can easily relate the optimal trajectories in
the (x, ˙x)-plane with the corresponding motions of the car along the x-axis.
Note that if the car is initially moving away from the origin, then it begins
braking until it stops, turns around, and starts accelerating (this is a “false”
switch because u actually remains constant), and then u switches sign and
the car starts braking again.
x
x
x
˙x
˙x
˙x
Γ
Figure 4.15: Bang-bang time-optimal control of the double integrator: (a) trajec-
tories for u ≡1, (b) trajectories for u ≡−1, (c) the switching curve and optimal
trajectories
The optimal control law that we just found has two important features.
First, as we already said, it is bang-bang. Second, we see that it can be
described in the form of a state feedback law. This is interesting because
in general, the maximum principle only provides an open-loop description
of an optimal control; indeed, u∗(t) depends, besides the state x∗(t), on
the costate p∗(t), but we managed to eliminate this latter dependence here.
It is natural to ask for what more general classes of systems time-optimal
controls have these two properties, i.e., are bang-bang and take the state
feedback form. The bang-bang property will be examined in detail in the
next two subsections. The problem of representing optimal controls as state
feedback laws is rather intricate and will not be treated in this book, except
for the two exercises below.
Exercise 4.10
Suppose that we modify the above problem by removing the
requirement that the ﬁnal velocity be 0. In other words, instead of bringing
the car to rest at the origin, we now want to simply hit the origin with an
arbitrary speed (“slam into a wall”). Use the maximum principle to ﬁnd
the optimal control. Is it bang-bang? Can you express it as a state feedback
and arrive at a complete description of the optimal trajectories in the (x, ˙x)-
plane? Include an explanation of the car’s optimal motions.
□

138
CHAPTER 4
The next exercise is along the same lines but the solution is less obvious.
Exercise 4.11
Consider the problem of bringing to rest at the origin in
minimal time the state of the forced harmonic oscillator ¨x + x = u us-
ing controls satisfying u ∈[−1, 1]. Answer the same questions as in Exer-
cise 4.10.
□
4.4.2
Bang-bang principle for linear systems
Consider now a system with general linear time-invariant dynamics
˙x = Ax + Bu
(4.51)
where x ∈Rn and u ∈U ⊂Rm.
We want to investigate under what
conditions time-optimal controls for this system have a bang-bang property
along the lines of what we saw in the previous example.
Of course, we
need to specify what the control set U is and exactly what we mean by
a bang-bang property for controls taking values in this U. As a natural
generalization of the interval [−1, 1] ⊂R to higher dimensions, we take U
to be an m-dimensional hypercube:
U = {u ∈Rm : ui ∈[−1, 1], i = 1, . . . , m}.
(4.52)
This is a reasonable control set representing independent control actuators.
We take the magnitude constraints on the diﬀerent components of u to be the
same just for simplicity; the extension to diﬀerent constraints is immediate.
Suppose that the control objective is to steer x from a given initial state
x0 to a given ﬁnal state x1 in minimal time. To be sure that this problem
is well posed, we assume that there exists some control u that achieves the
transfer from x0 to x1 (in some time). As we will see in Section 4.5 (Theo-
rem 4.3), this guarantees that a time-optimal control u∗: [t0, t∗] →U exists.
We now use the maximum principle to characterize it. The Hamiltonian is
H(x, u, p, p0) = ⟨p, Ax+Bu⟩+p0. The Hamiltonian maximization condition
implies that
⟨p∗(t), Bu∗(t)⟩= max
u∈U ⟨p∗(t), Bu(t)⟩
(4.53)
for all t ∈[t0, t∗]. We can rewrite this formula in terms of the input compo-
nents as
m
X
i=1
⟨p∗(t), bi⟩u∗
i (t) = max
u∈U
m
X
i=1
⟨p∗(t), bi⟩ui
where b1, . . . , bm are the columns of B. Since the components u∗
i (t) of the
optimal control can be chosen independently, it is clear that each term in
the summation must be maximized:
⟨p∗(t), bi⟩u∗
i (t) = max
|ui|≤1⟨p∗(t), bi⟩ui,
i = 1, . . . , m.

THE MAXIMUM PRINCIPLE
139
It has now become obvious that we must have
u∗
i (t) = sgn(⟨p∗(t), bi⟩) =





1
if ⟨p∗(t), bi⟩> 0,
−1
if ⟨p∗(t), bi⟩< 0,
?
if ⟨p∗(t), bi⟩= 0
(4.54)
for each i. Observe that (4.54) resembles (4.50). Similarly to how we pro-
ceeded from that formula, we now need to investigate the possibility that
⟨p∗, bi⟩≡0 on some interval of time for some i, as this would prevent us
from determining u∗
i on that interval.
The adjoint equation is ˙p∗= −AT p∗, which gives p∗(t) = eAT (t∗−t)p∗(t∗).
From this we obtain ⟨p∗(t), bi⟩= ⟨p∗(t∗), eA(t∗−t)bi⟩. This is a real analytic
function of t; hence, if it vanishes on some time interval, then it vanishes
for all t, together with all its derivatives. Calculating these derivatives at
t = t∗, we arrive at the equalities
⟨p∗(t∗), bi⟩= ⟨p∗(t∗), Abi⟩= · · · = ⟨p∗(t∗), An−1bi⟩= 0.
(4.55)
As we know, p∗(t∗) cannot be 0 in a time-optimal control problem (see
page 130). Since (4.55) means orthogonality of p∗(t∗) to the vectors bi, Abi,
. . . , An−1bi, we can rule it out by assuming that these vectors span Rn,
i.e., that (A, bi) is a controllable pair. We need this to be true for each
i; in other words, the system should be controllable with respect to each
individual input channel. Such linear control systems are called normal.
Let us now collect the properties of the optimal control u∗that we are
able to derive under the above normality assumption. None of the func-
tions ⟨p∗(·), bi⟩equal 0 on any time interval; being real analytic functions,
they only have ﬁnitely many zeros on the interval [t0, t∗]. Using the for-
mula (4.54), we see that each function u∗
i only takes the values ±1 and
switches between these values ﬁnitely many times. Away from these switch-
ing times, u∗
i is uniquely determined by (4.54). We conclude that the overall
optimal control u∗takes values only in the set of vertices of the hypercube
U, has ﬁnitely many discontinuities (switches), and is unique everywhere
else. Generalizing the earlier notion, we say that controls taking values in
the set of vertices of U are bang-bang (or have the bang-bang property); the
result that we have just obtained is a version of the bang-bang principle
for linear systems.
Before closing this discussion, it is instructive to see how the above bang-
bang property can be established in a self-contained way, without relying on
the maximum principle. More precisely, the argument outlined next essen-
tially rederives the maximum principle from scratch for the particular prob-
lem at hand (in the spirit of Section 4.3.1). Solutions of the system (4.51)
take the form
x(t) = eA(t−t0)x0 +
Z t
t0
eA(t−s)Bu(s)ds.

140
CHAPTER 4
For t ≥t0, let us introduce the set of points reachable from x(t0) = x0 at
time t:
Rt(x0) :=

eA(t−t0)x0 +
Z t
t0
eA(t−s)Bu(s)ds : u(s) ∈U, t0 ≤s ≤t

. (4.56)
We know that
x1 = eA(t∗−t0)x0 +
Z t∗
t0
eA(t∗−s)Bu∗(s)ds ∈Rt∗(x0).
(4.57)
In fact, the optimal time t∗is the smallest time t such that x1 ∈Rt(x0). It
follows that x1 must belong to the boundary of the reachable set Rt∗(x0);
indeed, if it were an interior point of Rt∗(x0) then we could reach it sooner.
We will see a little later (in Section 4.5) that the set Rt∗(x0) is compact
and convex. Along the lines of Section 4.2.7, there exists a hyperplane that
passes through x1 and contains Rt∗(x0) on one side; such a hyperplane is said
to support Rt∗(x0) at x1. Denoting a suitably chosen normal vector to this
hyperplane by p∗(t∗), we have ⟨p∗(t∗), x1⟩≥⟨p∗(t∗), x⟩for all x ∈Rt∗(x0).
Using (4.56) and (4.57), we easily obtain that
Z t∗
t0
⟨p∗(t∗), eA(t∗−s)Bu∗(s)⟩ds ≥
Z t∗
t0
⟨p∗(t∗), eA(t∗−s)Bu(s)⟩ds
for all u : [t0, t∗] →U, which in view of the formula eAT (t∗−s)p∗(t∗) = p∗(s)
is equivalent to
Z t∗
t0
⟨p∗(s), Bu∗(s)⟩ds ≥
Z t∗
t0
⟨p∗(s), Bu(s)⟩ds
∀u : [t0, t∗] →U.
From this it is not diﬃcult to recover the fact that (4.53) must hold for
(almost) all t ∈[t0, t∗], and we can proceed from there as before.
Exercise 4.12
Consider the same time-optimal control problem as above,
but take U to be the unit ball in Rm (with respect to the standard Euclidean
norm). Suppose that (A, B) is a controllable pair (do not assume normality).
What can you say about optimal controls?
□
The assumption of normality, which was needed to prove the bang-bang
property of time-optimal controls for U a hypercube, is quite strong.
A
diﬀerent, weaker version of the bang-bang principle could be formulated
as follows. Rather than wishing for every time-optimal control to be bang-
bang, we could ask whether every state x1 reachable from x0 by some control
is also reachable from x0 in the same time by a bang-bang control; in other
words, whether reachable sets for bang-bang controls coincide with reachable
sets for all controls. This would imply that, even though not all time-optimal

THE MAXIMUM PRINCIPLE
141
controls are necessarily bang-bang, we can always select one that is bang-
bang. It turns out that this modiﬁed bang-bang principle holds for every
linear control system (no controllability assumption is necessary) and every
control set U that is a convex polyhedron. The proof requires a reﬁnement
of the above argument and some additional steps; see [Sus83, Section 8.1]
for details.
4.4.3
Nonlinear systems, singular controls, and Lie brackets
Let us now investigate whether the preceding results can be extended be-
yond the class of linear control systems. Regarding the bang-bang principle
cited in the previous paragraph, the hope that it might be true for general
nonlinear systems is quickly shattered by the following example.
Example 4.1
For the planar system
˙x1 = x2
2 −1,
˙x2 = u
(4.58)
with the control constraint u ∈[−1, 1], consider the problem of going from
10

to
00

in minimal time.
The reader should be able to solve it by
inspection. The unique optimal control is u∗≡0. Indeed, it accomplishes
the desired transfer along the x1-axis in time 1, while any other control would
make x2 deviate from 0, slowing down the decrease of x1 and resulting in a
transfer time larger than 1. Note that 0 is an interior point of the control
set U = [−1, 1], hence u∗is not a bang-bang control.
Let us verify that this conclusion is in agreement with the maximum
principle. The Hamiltonian is H = p1(x2
2 −1) + p2u + p0. By the Hamil-
tonian maximization condition, the optimal control must satisfy the same
relation (4.50) as in the example of Section 4.4.1. We see that for u∗to take
the value 0 (or any other value diﬀerent from ±1), p∗
2 must vanish. Turn-
ing our attention to the adjoint equation, we have ˙p∗
1 = 0 and ˙p∗
2 = −2p∗
1x∗
2.
Compared with Section 4.4.1, this situation is a bit more complicated because
the p∗-dynamics and x∗-dynamics are coupled. We know, however, that the
optimal trajectory is completely contained in the x1-axis, i.e., x∗
2 ≡0 along
the optimal trajectory. Thus we have ˙p∗
2 ≡0 and so p∗
2 is a constant. The
value of this constant is determined by the terminal condition p∗(1). Since
we are dealing with a ﬁxed-endpoint problem, there are no constraints on
p∗(1). In particular, p∗
2(1) can be 0 in which case p∗
2 ≡0. Note that p∗
1
can still be nonzero, and there is no contradiction with the maximum prin-
ciple.
□
A distinguishing feature of the above example is that the function p∗
2,
whose sign determines the value of the optimal control u∗, identically van-
ishes. Consequently, the Hamiltonian maximization condition alone does

142
CHAPTER 4
not give us enough information to ﬁnd u∗. In problems where this situa-
tion occurs on some interval of time, the optimal control on that interval is
called singular, and the corresponding piece of the optimal state trajectory
is called a singular arc.
Example 4.1 should not be taken to suggest, however, that we must give
up the hope of formulating a bang-bang principle for nonlinear systems. Af-
ter all, we saw in Section 4.4.2 that even for linear systems, to be able to
prove that all time-optimal controls are bang-bang we need the normality
assumption. It is conceivable that the bang-bang property of time-optimal
controls for certain nonlinear systems can be guaranteed under an appropri-
ate nonlinear counterpart of that assumption.
Motivated by these remarks, our goal now is to better formalize the phe-
nomenon of singularity—and reach a deeper understanding of its reasons—
for a class of systems that includes the linear systems considered in Sec-
tion 4.4.2 as well as the nonlinear system (4.58). This class is composed of
nonlinear systems aﬃne in controls, deﬁned as
˙x = f(x) + G(x)u = f(x) +
m
X
i=1
gi(x)ui
(4.59)
where x ∈Rn, u ∈U ⊂Rm, G(x) is an n × m matrix whose columns
are g1(x), . . . , gm(x), and for the control set U we again take the hyper-
cube (4.52). The Hamiltonian for the time-optimal control problem is
H(x, u, p, p0) =
D
p, f(x) +
m
X
i=1
gi(x)ui
E
+ p0.
From the Hamiltonian maximization condition we obtain, completely analo-
gously to Section 4.4.2, that the components u∗
i (t) of the optimal control are
determined by the signs of the functions ϕi(t) := ⟨p∗(t), gi(x∗(t))⟩. These
functions of time (always associated with a speciﬁc optimal trajectory) are
called the switching functions. To investigate the bang-bang property, we
need to study zeros of the switching functions.
In order to simplify calculations, from this point on we assume that
m = 1, so that the input u is scalar and we have only one switching function
ϕ(t) = ⟨p∗(t), g(x∗(t))⟩.
(4.60)
The optimal control satisﬁes
u∗(t) = sgn(ϕ(t)) =





1
if ϕ(t) > 0,
−1
if ϕ(t) < 0,
?
if ϕ(t) = 0.
(4.61)

THE MAXIMUM PRINCIPLE
143
The canonical equations are ˙x∗= f(x∗) + g(x∗)u∗and
˙p∗= −Hx|∗= −(fx)T 
∗p∗−(gx)T 
∗p∗u∗
where fx and gx are the Jacobian matrices of f and g. Let us now compute
the derivative of ϕ:
˙ϕ = ⟨˙p∗, g(x∗)⟩+ ⟨p∗, gx(x∗) ˙x∗⟩= −

(fx)T p∗, g

∗−

(gx)T p∗, g

∗u∗
+ ⟨p∗, gxf⟩|∗+ ⟨p∗, gxg⟩|∗u∗= ⟨p∗, gxf −fxg⟩|∗.
(4.62)
We see that ˙ϕ is the inner product of p∗with the vector (gxf −fxg)|∗.
Perhaps the vector ﬁeld gxf −fxg, which we have not encountered up to
now, has some signiﬁcant meaning?
Example 4.2
Let us take f and g to be linear vector ﬁelds: f(x) = Ax
and g(x) = Bx for some n × n matrices A and B.
In this case (4.59)
reduces to a bilinear (not linear!) control system ˙x = Ax + Bxu. We have
fx ≡A, gx ≡B, and (gxf −fxg)(x) = BAx −ABx = [B, A]x, where
[B, A] := BA −AB is the commutator, or Lie bracket, of B and A.
□
In general, the Lie bracket of two diﬀerentiable vector ﬁelds f and g is
another vector ﬁeld deﬁned as
[f, g](x) := gx(x)f(x) −fx(x)g(x).
Note that the deﬁnitions of the Lie bracket for matrices (in linear algebra)
and for vector ﬁelds (in diﬀerential geometry) usually follow the opposite sign
conventions. The geometric meaning of the Lie bracket—which justiﬁes its
alternative name “commutator”—is as follows (see Figure 4.16). Suppose
that, starting at some point x0, we move along the vector ﬁeld f for ε units
of time, then along the vector ﬁeld g for ε units of time, after that along −f
(backward along f) for ε units of time, and ﬁnally along −g for ε units of
time. It is straightforward (although quite tedious) to check that for small
ε the resulting motion is approximated, up to terms of higher order in ε, by
ε2[f, g](x0). In particular, we will return to x0 if [f, g] ≡0 in a neighborhood
of x0, in which case we say that f and g commute.
We can now write the result of the calculation (4.62) more informatively
as
˙ϕ(t) = ⟨p∗(t), [f, g](x∗(t))⟩.
(4.63)
Coupled with the law (4.61), this equation reveals a fundamental connection
between Lie brackets and optimal control.
Exercise 4.13
For the multiple-input system (4.59), compute ˙ϕi, i =
1, . . . , m. Express the result in terms of Lie brackets.
□

144
CHAPTER 4
ε2[f, g](x0)
x0
x(4ε)
˙x = f(x)
˙x = g(x)
˙x = −f(x)
˙x = −g(x)
Figure 4.16: Geometric interpretation of the Lie bracket
Lie brackets can help us shed light on the bang-bang property. For a
singular optimal control to exist, ϕ must identically vanish on some time
interval. In view of (4.60) and (4.63), this can happen only if p∗(t) stays
orthogonal to both g(x∗(t)) and [f, g](x∗(t)). We have seen that in time-
optimal problems p∗(t) ̸= 0 for all t. Thus for planar systems (n = 2) we can
rule out singularity if g and [f, g] are linearly independent along the optimal
trajectory.
If n > 2 or g and [f, g] are not linearly independent, then we have to
look at higher derivatives of ϕ and see what it takes for them to vanish as
well. Rather than diﬀerentiating ˙ϕ again, let us revisit our derivation of ˙ϕ
and try to see a general pattern in it. Consider an arbitrary diﬀerentiable
vector ﬁeld h on Rn. Following the same calculation steps as in (4.62) and
using the deﬁnition of the Lie bracket, we easily arrive at
d
dt⟨p∗(t), h(x∗(t))⟩= ⟨p∗(t), [f, h](x∗(t))⟩+⟨p∗(t), [g, h](x∗(t))⟩u∗(t). (4.64)
The formula (4.63) for ˙ϕ is recovered from this result as a special case by
setting h := g which gives [g, h] = [g, g] = 0. Now, if we want to compute ¨ϕ,
we only need to set h := [f, g] to obtain the following expression in terms of
iterated Lie brackets of f and g:
¨ϕ(t) = ⟨p∗(t), [f, [f, g]](x∗(t))⟩+ ⟨p∗(t), [g, [f, g]](x∗(t))⟩u∗(t).
(4.65)
A singular optimal control must make ¨ϕ vanish. The control
u∗(t) = −⟨p∗(t), [f, [f, g]](x∗(t))⟩
⟨p∗(t), [g, [f, g]](x∗(t))⟩
(4.66)

THE MAXIMUM PRINCIPLE
145
can potentially be singular if ⟨p∗, g(x∗)⟩= ⟨p∗, [f, g](x∗)⟩≡0. However,
it should meet the magnitude constraint |u∗(t)| ≤1.
If we assume, for
example, that the relation
[g, [f, g]](x) = α(x)g(x) + β(x)[f, g](x) + γ(x)[f, [f, g]](x)
holds with |γ(x)| < 1 for all x, then (4.66) would not be an admissible
control unless ⟨p∗, [f, [f, g]](x∗)⟩≡0. To investigate the possibility that this
last function does vanish, we need to consider its derivative given by (4.64)
with h := [f, [f, g]], and so on.
Exercise 4.14
Continuing this process, generate a set of conditions that
rule out the existence of singular optimal controls. You might ﬁnd it conve-
nient to write these conditions in terms of the operators (ad f)k, k = 0, 1, . . .
deﬁned by (ad f)0(g) := g and (ad f)k(g) := [f, (ad f)k−1(g)] for k ≥1.
□
We are now in a position to gain a better insight into our earlier obser-
vations by using the language of Lie brackets.
Linear systems (Section 4.4.2).
In the single-input case, we have
f(x) = Ax and g(x) = b. Calculating the relevant Lie brackets, we ob-
tain [f, g] = −Ab, [f, [f, g]] = A2b, [g, [f, g]] = 0, [f, [f, [f, g]]] = −A3b,
[g, [f, [f, g]]] = 0, etc. A crucial consequence of linearity is that iterated Lie
brackets containing two g’s are 0, which makes the derivatives of the switch-
ing function ϕ independent of u. It is easy to see that ϕ cannot vanish if
the vectors b, Ab, . . . , An−1b span Rn, which is precisely the controllability
condition.
Example 4.1 revisited. For the system (4.58), we have f =
x2
2 −1
0

and g =
01

. The ﬁrst Lie bracket is
[f, g](x) = −
0
2x2
0
0
 0
1

=
−2x2
0

.
On the x1-axis, where the singular optimal trajectory lives, [f, g] vanishes
and so g and [f, g] do not span R2. In fact, ⟨p∗, g(x∗)⟩= ⟨p∗, [f, g](x∗)⟩= 0
when p∗
2 = 0. The next Lie bracket that we should then calculate is
[f, [f, g]](x) =
0
−2
0
0
 x2
2 −1
0

−
0
2x2
0
0
 −2x2
0

=
0
0

.
Since u ≡0, (4.65) gives ¨ϕ ≡0. All higher-order Lie brackets are obviously
0, hence ϕ ≡0. We see that all information about the singularity is indeed
encoded in the Lie brackets.
It is worth noting that singular controls are not necessarily complicated.
The optimal control u∗≡0 in Example 4.1 is actually quite simple. For

146
CHAPTER 4
single-input planar systems ˙x = f(x) + g(x)u, x ∈R2, u ∈[−1, 1], with f
and g real analytic, it can be shown that all time-optimal trajectories are
concatenations of a ﬁnite number of “bang” pieces (each corresponding to
either u = 1 or u = −1) and real analytic singular arcs. It is natural to ask
whether a similar claim holds for other optimal control problems in R2 or
for time-optimal control problems in R3. We are about to see that these two
questions are related and that the answer to both is negative.
4.4.4
Fuller’s problem
Consider again the double integrator (4.48) with the same control constraint
u ∈[−1, 1]. Let the cost functional be
J(u) =
Z tf
t0
x2
1(t)dt
(4.67)
and let the target set be S = [t0, ∞) ×
n00
o
. In other words, the ﬁnal
time is free and the ﬁnal state is the origin, but this is not a minimal-
time problem any more. The Hamiltonian is H = p1x2 + p2u + p0x2
1 and
the optimal control must again satisfy (4.50), i.e., the switching function is
ϕ(t) = p∗
2(t). The adjoint equation is
˙p∗
1 = −2p∗
0x∗
1,
˙p∗
2 = −p∗
1.
(4.68)
We claim that singular arcs are ruled out. Indeed, along a singular arc p∗
2
must vanish. Inspecting the diﬀerential equations for p∗
2, p∗
1, and x∗
1, we see
that p∗
1, x∗
1, and x∗
2 must then vanish too. Thus the only possible singular
arc is the trivial one consisting of the equilibrium at the origin.
It follows that all optimal controls are bang-bang, with switches occur-
ring when p∗
2 equals 0. Our ﬁndings up to this point replicate those in the
time-optimal setting of Section 4.4.1. There, we used the fact that p∗
2 de-
pended linearly on t to go further and show that the optimal control has at
most one switch. The present adjoint equation (4.68) is diﬀerent from (4.49)
and so we can no longer reach the same conclusion. Instead, it turns out that
the optimal solution has the following properties (which we state without
proof):
1) Optimal controls are bang-bang with inﬁnitely many switches.
2) Switching takes place on the curve
nx1
x2

: x1 + γ|x2|x2 = 0
o
where
γ ≈0.445.
3) Time intervals between consecutive switches decrease in geometric pro-
gression.

THE MAXIMUM PRINCIPLE
147
The last property guarantees that the ﬁnal time is ﬁnite. The occurrence
of a switching pattern in which switching times form an inﬁnite sequence
accumulating near the ﬁnal time is known as Fuller’s phenomenon (or Zeno
behavior, or “chattering”). Note that while bang-bang controls with ﬁnitely
many switches are piecewise continuous (in fact, piecewise constant), opti-
mal controls for Fuller’s problem are only measurable (see page 86). Fig-
ure 4.17 shows the switching curve6 and a sketch of an optimal state trajec-
tory.
sw
it
c
h
in
g
c
u
r
v
e
x2
x1
Figure 4.17: An optimal trajectory for Fuller’s problem
We note that the switching curves in Fuller’s problem and in the time-
optimal control problem for the double integrator (treated in Section 4.4.1)
are given by the same formula, but in the time-optimal problem we had a
diﬀerent value of γ, namely, γ = 1/2. The nature of switching, however, is
drastically diﬀerent in the two cases. We can embed both problems in the pa-
rameterized family of problems with the cost functional J(u) =
R tf
t0 |x1(t)|νdt
where ν ≥0 is a parameter. For ν = 0 we recover the time-optimal problem
while for ν = 2 we recover Fuller’s problem. Interestingly, one can prove
that there exists a “bifurcation value” ¯ν ≈0.35 with the following property:
for ν ∈[0, ¯ν] the optimal control is bang-bang with at most one switch, while
for ν > ¯ν we obtain Fuller’s phenomenon.
The next exercise shows that Fuller’s phenomenon is also observed in
time-optimal control problems starting with dimension 3. Its solution can
be based on Fuller’s problem.
Exercise 4.15
Give an example of a control system ˙x = f(x) + g(x)u
with x ∈R3 and u ∈[−1, 1] for which transferring the state to the origin in
6The shape of the switching curve in the ﬁgure is slightly distorted for better visual-
ization; in reality the trajectory converges to 0 faster.

148
CHAPTER 4
minimal time involves inﬁnitely many switches between u = 1 and u = −1
(at least for some initial states x0).
□
4.5
EXISTENCE OF OPTIMAL CONTROLS
To motivate the subject of this section, let us consider the following (ob-
viously absurd) claim: The largest positive integer is 1. To “prove” this
statement, let N be the largest positive integer and suppose that N ̸= 1.
Then we have N 2 > N, which contradicts the property of N being the
largest positive integer. Therefore, N = 1.
This argument is known as Perron’s paradox. Although clearly farcical,
it highlights a serious issue which we have been dodging up until now: it
warns us about the danger of assuming the existence of an optimal solu-
tion. Indeed, ﬁnding the largest positive integer is an optimization problem.
Of course, a solution to this problem does not exist. In the language of
this book, the above reasoning correctly shows that N = 1 is a necessary
condition for optimality. Thus a necessary condition can be useless—even
misleading—unless we know that a solution exists.
We know very well that the maximum principle only provides necessary
conditions for optimality. The same is true for the Euler-Lagrange equation
and several other results that we have derived along the way. We have said
repeatedly that fulﬁllment of necessary conditions alone does not guarantee
optimality. However, the basic question of whether an optimal solution even
exists has not been systematically addressed yet, and it is time to do it now.
Suppose that we are given an optimal control problem which falls into
the general class of problems formulated in Section 3.3. How can we ensure
the existence of an optimal control? For a start, we must assume that there
exists at least one control u that drives the state of the system from the
initial condition to the target set S (more precisely, the pair (t, x(t)) should
evolve from (t0, x0) to S). Otherwise, the problem is ill posed (has inﬁnite
cost). We can view this as a type of controllability assumption. It may be
nontrivial to check, especially for general control sets U. Still, it would be
nice if this controllability assumption were enough to guarantee the existence
of an optimal control. Simple examples show that unfortunately this is not
the case.
Example 4.3
For the standard integrator ˙x = u, with x, u ∈R, consider
the problem of steering x from x0 = 0 to x1 = 1 in minimal time.
An
optimal solution does not exist: it is easy to see that arbitrarily small positive
transfer times are achievable, but accomplishing the transfer in time 0 is
impossible.
□

THE MAXIMUM PRINCIPLE
149
Observe that in the above example, arbitrarily fast transfer is possible
because the control set U = R is unbounded. Let us see if the problem
becomes well posed when U is taken to be bounded.
Example 4.4
Consider the same problem as in Example 4.3 except let
U = [0, 1). Now the state can be transferred from 0 to 1 in an arbitrary
time larger than 1, but we cannot achieve the transfer in time 1. Hence, an
optimal solution again does not exist.
□
This time, the diﬃculty is caused by the fact that the control set is
not closed. The reader is probably beginning to guess that we want to have
both boundedness and closedness, i.e., compactness. To make this statement
precise, however, we should be working not with the control set U itself but
with the set of all points reachable from x0 using controls that take values
in U. For t ≥t0, let us denote by Rt(x0) the set of points reachable from
x(t0) = x0 at time t (for a given control set U). We already worked with
reachable sets on page 140, in the context of linear systems. We saw there
that if t∗−t0 is the fastest transfer time from x0 to x1, then x1 must be a
boundary point of Rt∗(x0); this situation is illustrated in Figure 4.18.
x0
x1
Figure 4.18: Propagation of reachable sets Rt(x0), t0 ≤t ≤t∗
In the two scalar examples considered earlier, the reachable set computa-
tion is straightforward and the negative conclusions are readily explained by
the above property being impossible to satisfy. In Example 4.3, Rt(x0) = R
for all t > 0, which is unbounded and cannot contain x1 = 1 on its boundary.
In Example 4.4, R1(x0) = [0, 1) which is not closed and does not include the
boundary point 1, while for each t > 1 the set Rt(x0) already contains 1 in
its interior. We can now formulate a reﬁned guess saying that for optimal
controls to exist, we want to have compact reachable sets. We could general-
ize this discussion to target sets instead of ﬁxed terminal points; we will also
see that compactness of reachable sets is relevant not only for time-optimal
control problems.
For nonlinear systems, explicit computation of reachable sets is usually
not feasible. Instead, we can rely on the following general result known as
Filippov’s theorem: Given a control system in the standard form (3.17)
with u ∈U, assume that its solutions exist on a time interval [t0, tf] for

150
CHAPTER 4
all controls u(·) and that for every pair (t, x) the set {f(t, x, u) : u ∈U}
is compact and convex. Then the reachable set Rt(x0) is compact for each
t ∈[t0, tf].
Filippov’s theorem actually has the following stronger form,
which implies compactness of Rt(x0): Under the above assumptions, the set
of all trajectories of the system, as a subset of C0([t0, tf], Rn), is compact
with respect to the topology induced by the 0-norm ∥· ∥0 (i.e., the topology
of uniform convergence).
We do not prove Filippov’s theorem but make some comments on it. For
the result to be valid, controls must a priori be understood as measurable
functions from [t0, tf] to U; see the discussion on page 86. (We can hope to
show afterwards that optimal controls belong to a nicer class of functions,
e.g., that they enjoy the bang-bang property.) It is important to note that
the ﬁrst hypothesis in the theorem (existence of solutions on a given interval)
does not follow from the second hypothesis (compactness and convexity of
the right-hand side); for example, solutions of ˙x = x2 + u blow up in ﬁnite
time even for U = {0}. Boundedness of the reachable sets can be shown
without the convexity assumption, but convexity is crucial for establishing
closedness (the argument relies on the separating hyperplane theorem). The
next exercise should help clarify the role of the convexity assumption.
Exercise 4.16
Consider the planar system
˙x1 = u,
˙x2 = x2
1
with x0 =
00

and U = {−1, 1}. Show that the reachable sets are not closed.
Use this fact to deﬁne a cost functional such that the resulting optimal control
problem has no solution. Explain how the situation changes if we “convexify”
the problem by redeﬁning U to be the interval [−1, 1].
□
Filippov’s theorem applies to useful classes of systems, most notably, to
systems aﬃne in controls given by (4.59) with compact and convex control
sets U. For such systems, the set {f(x) + G(x)u : u ∈U} is compact and
convex for each x as the image of U under an aﬃne map. As we already
warned the reader, existence of solutions on a time interval of interest needs
to be assumed separately. In the special case of the linear system (4.51),
though, global existence of solutions is automatic; the right-hand side of
the system is allowed to depend on time as well. For linear systems the
reachable set Rt(x0) is also convex, as can be easily seen from convexity of
U and the variation-of-constants formula for solutions.
Filippov’s theorem provides a suﬃcient condition for compactness of
reachable sets. Earlier, we argued that compactness of reachable sets should
be useful for proving existence of optimal controls. Let us now conﬁrm that

THE MAXIMUM PRINCIPLE
151
this is indeed true, at least for certain classes of problems. The connection
between compactness of reachable sets and existence of optimal controls is
especially easy to see for problems in the Mayer form (terminal cost only).
Suppose that the control system satisﬁes the hypotheses of Filippov’s theo-
rem, that the cost is J(u) = K(xf) with K a continuous function, and—in
order to arrive at the simplest case—that the target set is S = {t1} × Rn
(a ﬁxed-time, free-endpoint problem). Since K is a continuous function and
Rt1(x0) is a compact set, the Weierstrass Theorem (see page 9) guarantees
that K has a global minimum over Rt1(x0). By the deﬁnition of Rt1(x0),
there exists at least one control that steers the state to this minimum at
time t1, and every such control is optimal.
For certain ﬁxed-time problems in the Bolza form, it is possible to es-
tablish existence of optimal controls by combining compactness of the set
of system trajectories (provided by the stronger form of Filippov’s theorem)
with continuity of the cost functional on this set and invoking the inﬁnite-
dimensional version of the Weierstrass Theorem (see page 23). Alternatively,
one can reduce the problem to the Mayer form (as in Section 3.3.2) and then
use the previous result. In general, the argument needs to be tailored to a
particular type of problem at hand. We will now show in detail how ex-
istence of optimal controls can be proved for linear time-optimal control
problems. The next result says that compactness and convexity of U and a
controllability assumption (the existence of some control achieving the de-
sired transfer) is all we need for an optimal control to exist. (As we know
from Section 4.4.2, such an optimal control is automatically bang-bang if U
is a hypercube, or can be chosen to be bang-bang if U is an arbitrary convex
polyhedron.)
Theorem 4.3 (Existence of time-optimal controls for linear systems).
Consider the linear control system (4.51) with a compact and convex control
set U.
Let the control objective be to steer x from a given initial state
x(t0) = x0 to a given ﬁnal state x1 in minimal time. Assume that x1 ∈
Rt(x0) for some t ≥t0. Then there exists a time-optimal control.
Proof. Let t∗:= inf{t ≥t0 : x1 ∈Rt(x0)}. The time t∗is well deﬁned
because by the theorem’s hypothesis the set over which the inﬁmum is being
taken is nonempty. (We note that, in view of boundedness of U, we have
t∗> t0 unless x1 = x0.) We will be done if we show that x1 ∈Rt∗(x0), i.e.,
that t∗is actually a minimum.7 This will mean that there exists a control
u∗that transfers x0 to x1 at time t∗, and by the deﬁnition of t∗no control
does it faster.
By the deﬁnition of inﬁmum, there exists a sequence tk ↘t∗such that
x1 ∈Rtk(x0) for each k. Then, by the deﬁnition of Rtk(x0), for each k there
7This situation is to be contrasted with the one in Example 4.4.

152
CHAPTER 4
is a control uk such that
x1 = eA(tk−t0)x0 +
Z tk
t0
eA(tk−s)Buk(s)ds.
The above equation shows that the same point x1 belongs to the reachable
sets Rtk(x0) for diﬀerent k. To be able to use the closedness property of
reachable sets guaranteed by Filippov’s theorem, we would rather work with
diﬀerent points belonging to the same reachable set. To this end, let us
truncate the trajectories corresponding to the controls uk at time t∗and
deﬁne, for each k,
xk := eA(t∗−t0)x0 +
Z t∗
t0
eA(t∗−s)Buk(s)ds.
(4.69)
All these points by construction belong to the same reachable set Rt∗(x0).
We now claim that the sequence of points xk converges to x1. If we
establish this fact then the proof will be ﬁnished, because the closedness of
the reachable set Rt∗(x0) will imply that x1 ∈Rt∗(x0) as needed. To prove
the convergence claim, let us write
x1−xk =
 eA(tk−t∗) −I

eA(t∗−t0)x0 +
 eA(tk−t∗) −I
 Z t∗
t0
eA(t∗−s)Buk(s)ds
+
Z tk
t∗eA(tk−s)Buk(s)ds =
 eAεk −I

xk +
Z εk
0
eA(εk−τ)Buk(t∗+ τ)dτ
where we deﬁned εk := tk −t∗and made the substitution s = t∗+τ. Taking
the norm on both sides, we have
|x1 −xk| ≤
eAεk −I
|xk| + εk max

|eAδBu| : δ ∈[0, εk], u ∈U
	
(4.70)
where the maximum is well deﬁned because U is compact. Similarly, we can
take the norm on both sides of (4.69) to obtain the bound
|xk| ≤
eA(t∗−t0)|x0| + (t∗−t0) max

|eAδBu| : δ ∈[0, t∗−t0], u ∈U
	
which is independent of k, implying that the sequence |xk| is uniformly
bounded.
Using this fact in (4.70) and noting that εk →0 hence also
∥eAεk −I∥→0, we conclude that |x1 −xk| converges to 0 as claimed.
□
Existence results are also available for other, more general classes of
control problems (see Section 4.6 for further information). Although the
proofs of such results are usually not constructive, the knowledge that a
solution exists rules out the risks suggested by Perron’s paradox and provides
a basis for applying the maximum principle—which, as we have seen, often
allows one to actually ﬁnd an optimal control.

THE MAXIMUM PRINCIPLE
153
4.6
NOTES AND REFERENCES FOR CHAPTER 4
The Basic Fixed-Endpoint Control Problem and Basic Variable-Endpoint
Control Problem match the Special Problem 1 and Special Problem 2 in
[AF66], respectively, and the statements of the maximum principle for these
two problems correspond to Theorem 5-5P and Theorem 5-6P in [AF66,
Section 5-15]. Our proof of the maximum principle is also heavily based
on the one presented in [AF66], but there are signiﬁcant diﬀerences be-
tween the two proofs. First, we substantially reorganized the proof structure
of [AF66], changing the main steps and the order in which they are given.
Second, we ﬁlled in some details of proving Lemmas 4.1 and 4.2 not included
in [AF66]; these are taken from the proofs of Lemmas 3 and 10 in the original
book [PBGM62]. Finally, as we already explained earlier, our sign conven-
tion (maximum versus minimum) is the opposite of that in [AF66]. Among
other expositions of the proof of the maximum principle built around similar
ideas, we note the one in [LM67] and the more modern approach of [Sus00].
Our derivation of the variational equation (step 4) proceeded similarly to
the argument in [Kha02, Section 3.3].
A precise deﬁnition of the topology mentioned in Section 4.3 (uniform
convergence for x and L1 convergence for u), which leads to a notion of local
optimality suitable for the maximum principle, can be found in [MO98, p.
23]; that book uses the terminology “convergence in Pontryagin’s sense”
and “Pontryagin minimum.” The book [Vin00] works with the somewhat
diﬀerent concept of a W 1,1 local minimizer (deﬁned there on pp. 287–288).
Changes of variables for deriving the maximum principle for other classes of
problems, including those discussed in Section 4.3.1, are thoroughly covered
in [AF66]. For the reader wishing to reconstruct a proof of the maximum
principle for the Mayer problem in more detail, Section 6.1 of [BP07] should
be of help. In the majority of the literature the initial state is assumed to
be ﬁxed; references that do deal with variable initial states and resulting
transversality conditions include [LM67] and [Vin00].
The example of Section 4.4.1 is standard and appears in many places,
including [PBGM62], [BP07, Section 7.3], [AF66, Section 7-2], [Kno81],
and [Son98, Chapter 10]. The last three references also discuss the bang-
bang principle for linear systems derived in Section 4.4.2. When all eigen-
values of the matrix A are real, a more precise result can be obtained,
namely, each component of the optimal control can switch at most n −1
times (see, e.g., [AF66, Theorem 6-8]).
For further information on syn-
thesizing optimal (in particular, time-optimal) controls in state feedback
form, see [Sus83, PS00, BP04] and the references therein. The weaker bang-
bang principle mentioned at the end of Section 4.4.2 remains true for linear
time-varying systems and for arbitrary compact and convex control sets U,
provided that by bang-bang controls one understands controls taking values

154
CHAPTER 4
in the set of extreme points of U; see, e.g., [LM67, Ces83, Kno81] (the last of
which only addresses the time-varying aspect). However, the proofs become
less elementary. Another advantage of U being a convex polyhedron is that
a bound on the number of switches (which in general depends on the length
of the time interval) can be established, as explained in [Sus83, Section 8.1].
Our treatment of singular optimal controls and their connection with Lie
brackets in Section 4.4.3 was inspired by the nice expositions in [Sus83]
and [Sus00, Handout 5]. The characterization of time-optimal controls in
the plane mentioned at the end of Section 4.4.3 is stated in [Sus83] as The-
orem 8.4.1. Fuller’s problem is studied in [Ful85] and in the earlier work by
the same author cited in that paper. The relevant results are conveniently
summarized in [Rya87], while a detailed analysis can be found in [Jur96,
Chapter 10]. Fuller’s phenomenon is observed in other problems too, such
as controlling a Dubins car (see [AS04, Section 20.6]). The paper [Neu03]
describes how it can play a role in design of marine reserves for optimal
harvesting.8
Perron’s paradox and its ramiﬁcations are carefully examined in [You80].
Examples 4.3 and 4.4 are contained in [Son98, Remark 10.1.2]. Filippov’s
theorem is commonly attributed to [Fil88, §7, Theorem 3], although that
book cites earlier works. An extra step (the so-called Filippov’s Selection
Lemma) is needed to pass from the diﬀerential inclusion setting of [Fil88]
to that of control systems; see, e.g., [Vin00, Section 2.3]. Filippov’s the-
orem is also discussed, among many other sources, in [BP07, Section 3.5].
Boundedness of reachable sets without the convexity assumption is shown
in [LSW96, Proposition 5.1]. For linear systems, compactness of reachable
sets is addressed in a more direct fashion in [Son98, Section 10.1]. Exis-
tence of optimal controls for the Mayer problem with more general target
sets, as well as for the Bolza problem via a reduction to the Mayer prob-
lem, is investigated in [BP07, Chapter 5]. Our proof of Theorem 4.3 fol-
lows [Son98] and [Kno81].
This argument can be extended to nonlinear
systems aﬃne in controls; the essential details are in [Son98, Lemma 10.1.2
and Remark 10.1.10] and [Sus79, pp. 633–634].
The assumption of con-
vexity of U in Theorem 4.3 can actually be dropped, as a consequence of
available bang-bang theorems for linear systems (see [BP07, Theorem 5.1.2]
or [Sus83, Theorem 8.1.1]). Further results on existence of optimal controls
can be found in [Kno81] and [LM67]. For an in-depth treatment of this
issue we recommend the book [Ces83] which dedicates several chapters to it.
The issue of controllability of linear systems with bounded controls, which
is relevant to the material of Sections 4.4.2 and 4.5, is considered in [Jur96,
Chapter 5] (a necessary condition for controllability is that the matrix A
has purely imaginary eigenvalues).
We must stress that the basic setting of this chapter and the maxi-
8We thank Patrick De Leenheer for acquainting us with this interesting application.

THE MAXIMUM PRINCIPLE
155
mum principle that we developed are far from being the most general pos-
sible. As we said earlier, we work with piecewise continuous controls rather
than the larger class of measurable controls considered in [LM67], [Vin00],
and [Sus00]. An analogous maximum principle is valid for Lipschitz and
not necessarily C1 systems, but its derivation requires additional technical
tools; see [Cla89], [Vin00], [Sus07], and the references therein. Although the
classical formulation of the maximum principle relies on ﬁrst-order analysis,
high-order versions are also available; see [Kre77], [Bre85], and [AS04, Chap-
ter 20]. The applicability of the maximum principle has been extended to
other problems; some of these—namely, control problems on manifolds and
hybrid control problems—will be touched upon in Chapter 7, while others
are completely beyond the scope of this book (for example, see [YZ99] for
a stochastic maximum principle). Versions of the maximum principle for
discrete-time systems exist and are surveyed in [NRV84]; see also [Bol78].

Chapter Five
The Hamilton-Jacobi-Bellman Equation
5.1
DYNAMIC PROGRAMMING AND THE HJB EQUATION
Right around the time when the maximum principle was being developed
in the Soviet Union, on the other side of the Atlantic Ocean (and of the
iron curtain) Bellman wrote the following in his book [Bel57]: “In place
of determining the optimal sequence of decisions from the ﬁxed state of
the system, we wish to determine the optimal decision to be made at any
state of the system.
Only if we know the latter, do we understand the
intrinsic structure of the solution.” The approach realizing this idea, known
as dynamic programming, leads to necessary as well as suﬃcient conditions
for optimality expressed in terms of the so-called Hamilton-Jacobi-Bellman
(HJB) partial diﬀerential equation for the optimal cost. These concepts are
the subject of the present chapter. Developed independently from—even,
to some degree, in competition with—the maximum principle during the
cold war era, the resulting theory is very diﬀerent from the one presented
in Chapter 4. Nevertheless, both theories have their roots in calculus of
variations and there are important connections between the two, as we will
explain in Section 5.2 (see also Section 7.2).
5.1.1
Motivation: the discrete problem
The dynamic programming approach is quite general, but to ﬁx ideas we
ﬁrst present it for the purely discrete case. Consider a system of the form
xk+1 = f(xk, uk),
k = 0, 1, . . . , T −1
where xk lives in a ﬁnite set X consisting of N elements, uk lives in a ﬁnite
set U consisting of M elements, and T, N, M are ﬁxed positive integers. We
suppose that each possible transition from some xk to xk+1 corresponding to
some control value uk has a cost assigned to it, and there is also a terminal
cost function on X. For each trajectory, the total cost accumulated at time
T is the sum of the transition costs at time steps 0, . . . , T −1 plus the
terminal cost at xT . For a given initial state x0 we want to minimize this
total cost, the terminal state xT being free.
156

THE HAMILTON-JACOBI-BELLMAN EQUATION
157
The most naive approach to this problem is as follows: starting from
x0, enumerate all possible trajectories going forward up to time T, calculate
the cost for each one, then compare them and select the optimal one. Fig-
ure 5.1 provides a visualization of this scenario. It is easy to estimate the
computational eﬀort required to implement such a solution: there are M T
possible trajectories and we need T additions to compute the cost for each
one, which results in roughly O(M T T) algebraic operations.
...
$
$
$
$
$
$
t
x
x0
0
1
2
T −1
T
Figure 5.1: Discrete case: going forward
We now examine an alternative approach, which might initially appear
counterintuitive: let us go backward in time.
At k = T, terminal costs
are known for each xk. At k = T −1, for each xk we ﬁnd to which xk+1
we should jump so as to have the smallest cost (the one-step running cost
plus the terminal cost). Write this optimal “cost-to-go” next to each xk
and mark the selected path (see Figure 5.2). In case of more than one path
giving the same cost, choose one of them at random. Repeat these steps for
k = T −2, . . . , 0, working with the costs-to-go computed previously in place
of the terminal costs.
...
$
$
$
$
$
$
$
$
$
$
t
x
x0
0
1
T −1
T
Figure 5.2: Discrete case: going backward

158
CHAPTER 5
We claim that when we are done, we will have generated an optimal
trajectory from each x0 to some xT . The justiﬁcation of this claim relies on
the principle of optimality, an observation that we already made during the
proof of the maximum principle (see page 108). In the present context this
principle says that for each time step k, if xk is a point on an optimal trajec-
tory then the remaining decisions (from time k onward) must constitute an
optimal policy with respect to xk as the initial condition. What the principle
of optimality does for us here is guarantee that the paths we discard going
backward cannot be portions of optimal trajectories. On the other hand, in
the previous approach (going forward) we are not able to discard any paths
until we reach the terminal time and ﬁnish the calculations.
Let us assess the computational eﬀort associated with this backward
scheme. At each time k, for each state xk and each control uk we need to add
the cost of the corresponding transition to the cost-to-go already computed
for the resulting xk+1. Thus, the number of required operations is O(NMT).
Comparing this with the O(M T T) operations needed for the earlier forward
scheme, we conclude that the backward computation is more eﬃcient for
large T, with N and M ﬁxed. Of course, the number of operations will still
be large if N and M are large (this is the “curse of dimensionality”).
Actually, the above comparison is not really accurate because the back-
ward scheme provides much more information: it ﬁnds the optimal policy
for every initial condition x0, and in fact it tells us what the optimal decision
is at every xk for all k. We can restate this last property as follows: the
backward scheme yields the optimal control policy in the form of a state
feedback law. In the forward scheme, on the other hand, to handle all initial
conditions we would need O(NM T T) operations, and we would still not
cover all states xk for k > 0; hence, a state feedback is not obtained. We
see that the backward scheme fulﬁlls the objective formulated in Bellman’s
quote at the beginning of this chapter. This recursive scheme serves as an
example of the general method of dynamic programming.
5.1.2
Principle of optimality
We now return to the continuous-time optimal control problem that we have
been studying since Section 3.3, deﬁned by the control system (3.17) and the
Bolza cost functional (3.20). For concreteness, assume that we are dealing
with a ﬁxed-time, free-endpoint problem, i.e., the target set is S = {t1}×Rn.
(Handling other target sets requires some modiﬁcations, on which we brieﬂy
comment in what follows.) We can then write the cost functional as
J(u) =
Z t1
t0
L(t, x(t), u(t))dt + K(x(t1)).

THE HAMILTON-JACOBI-BELLMAN EQUATION
159
As we already remarked in Section 3.3.2, a more accurate notation for this
functional would be J(t0, x0, u) as it depends on the initial data.
The basic idea of dynamic programming is to consider, instead of the
problem of minimizing J(t0, x0, u) for given t0 and x0, the family of mini-
mization problems associated with the cost functionals
J(t, x, u) =
Z t1
t
L(s, x(s), u(s))ds + K(x(t1))
(5.1)
where t ranges over [t0, t1) and x ranges over Rn; here x(·) on the right-
hand side denotes the state trajectory corresponding to the control u(·) and
satisfying x(t) = x. (There is a slight abuse of notation here; the second
argument x of J in (5.1) is a ﬁxed point, and only the third argument u
is a function of time.) In accordance with Bellman’s roadmap, our goal is
to derive a dynamic relationship among these problems, and ultimately to
solve all of them.
To this end, let us introduce the value function
V (t, x) := inf
u[t,t1] J(t, x, u)
(5.2)
where the notation u[t,t1] indicates that the control u is restricted to the
interval [t, t1].
Loosely speaking, we can think of V (t, x) as the optimal
cost (cost-to-go) from (t, x). It is important to note, however, that the exis-
tence of an optimal control—and hence of the optimal cost—is not actually
assumed, which is why we work with an inﬁmum rather than a minimum
in (5.2). If an optimal control exists, then the inﬁmum turns into a mini-
mum and V coincides with the optimal cost-to-go. In general, the inﬁmum
need not be achieved, and might even equal −∞for some (t, x).
It is clear that the value function must satisfy the boundary condition
V (t1, x) = K(x)
∀x ∈Rn.
(5.3)
In particular, if there is no terminal cost (K ≡0) then we have V (t1, x) = 0.
The boundary condition (5.3) is of course a consequence of our speciﬁc
problem formulation.
If the problem involved a more general target set
S ⊂[t0, ∞) × Rn, then the boundary condition would read V (t, x) = K(x)
for (t, x) ∈S.
The basic principle of dynamic programming for the present case is a
continuous-time counterpart of the principle of optimality formulated in
Section 5.1.1, already familiar to us from Chapter 4. Here we can state this
property as follows, calling it again the principle of optimality: For every
(t, x) ∈[t0, t1) × Rn and every ∆t ∈(0, t1 −t], the value function V deﬁned
in (5.2) satisﬁes the relation
V (t, x) =
inf
u[t,t+∆t]
 Z t+∆t
t
L(s, x(s), u(s))ds + V (t + ∆t, x(t + ∆t))

(5.4)

160
CHAPTER 5
where x(·) on the right-hand side is the state trajectory corresponding to the
control u[t,t+∆t] and satisfying x(t) = x. The intuition behind this state-
ment is that to search for an optimal control, we can search over a small
time interval for a control that minimizes the cost over this interval plus
the subsequent optimal cost-to-go. Thus the minimization problem on the
interval [t, t1] is split into two, one on [t, t + ∆t] and the other on [t + ∆t, t1];
see Figure 5.3.
t
x
t
t + ∆t
Figure 5.3: Continuous time: principle of optimality
The above principle of optimality may seem obvious.
However, it is
important to justify it rigorously, especially since we are using an inﬁmum
and not assuming existence of optimal controls. We give “one half” of the
proof by verifying that
V (t, x) ≥V (t, x)
(5.5)
where V (t, x) denotes the right-hand side of (5.4):
V (t, x) :=
inf
u[t,t+∆t]
 Z t+∆t
t
L(s, x(s), u(s))ds + V (t + ∆t, x(t + ∆t))

.
By (5.2) and the deﬁnition of inﬁmum, for every ε > 0 there exists a control
uε on [t, t1] such that
V (t, x) + ε ≥J(t, x, uε).
(5.6)
Writing xε for the corresponding state trajectory, we have
J(t, x, uε) =
Z t+∆t
t
L(s, xε(s), uε(s))ds + J(t + ∆t, xε(t + ∆t), uε)
≥
Z t+∆t
t
L(s, xε(s), uε(s))ds + V (t + ∆t, xε(t + ∆t)) ≥V (t, x)
where the two inequalities follow directly from the deﬁnitions of V and
V , respectively.
Since (5.6) holds with an arbitrary ε > 0, the desired
inequality (5.5) is established.

THE HAMILTON-JACOBI-BELLMAN EQUATION
161
Exercise 5.1
Complete the proof of the principle of optimality by showing
the reverse inequality V (t, x) ≤V (t, x).
□
5.1.3
HJB equation
In the principle of optimality (5.4) the value function V appears on both
sides with diﬀerent arguments. We can thus think of (5.4) as describing a
dynamic relationship among the optimal values of the costs (5.1) for diﬀerent
t and x, which we declared earlier to be our goal. However, this relationship
is rather clumsy and not very convenient to use in its present form. What
we will now do is pass to its more compact inﬁnitesimal version, which will
take the form of a partial diﬀerential equation (PDE). The steps that follow
rely on ﬁrst-order Taylor expansions; the reader will recall that we used
somewhat similar calculations when deriving the maximum principle. First,
write x(t + ∆t) appearing on the right-hand side of (5.4) as
x(t + ∆t) = x + f(t, x, u(t))∆t + o(∆t)
where we remembered that x(t) = x. This allows us to express V (t+∆t, x(t+
∆t)) as
V (t + ∆t, x(t + ∆t)) = V (t, x) + Vt(t, x)∆t + ⟨Vx(t, x), f(t, x, u(t))∆t⟩+ o(∆t)
(5.7)
(for now we proceed under the assumption—whose validity we will examine
later—that V is C1). We also have
Z t+∆t
t
L(s, x(s), u(s))ds = L(t, x, u(t))∆t + o(∆t).
(5.8)
Substituting the expressions given by (5.7) and (5.8) into the right-hand
side of (5.4), we obtain
V (t, x) =
inf
u[t,t+∆t]

L(t, x, u(t))∆t + V (t, x)
+ Vt(t, x)∆t + ⟨Vx(t, x), f(t, x, u(t))∆t⟩+ o(∆t)
	
.
The two V (t, x) terms cancel out (because the one inside the inﬁmum does
not depend on u and can be pulled outside), which leaves us with
0 =
inf
u[t,t+∆t]{L(t, x, u(t))∆t + Vt(t, x)∆t + ⟨Vx(t, x), f(t, x, u(t))∆t⟩+ o(∆t)}.
(5.9)
Let us now divide by ∆t and take it to be small. In the limit as ∆t →0
the higher-order term o(∆t)/∆t disappears, and the inﬁmum is taken over
the instantaneous value of u at time t (in fact, already in (5.9) the control

162
CHAPTER 5
values u(s) for s > t aﬀect the expression inside the inﬁmum only through
the o(∆t) term). Pulling Vt(t, x) outside the inﬁmum as it does not depend
on u, we conclude that the equation
−Vt(t, x) = inf
u∈U
n
L(t, x, u) +

Vx(t, x), f(t, x, u)
o
(5.10)
must hold for all t ∈[t0, t1) and all x ∈Rn. This equation for the value
function is called the Hamilton-Jacobi-Bellman (HJB) equation. It
is a PDE since it contains partial derivatives of V with respect to t and x.
The accompanying boundary condition is (5.3).
Note that the terminal cost appears only in the boundary condition and
not in the HJB equation itself. In fact, the speciﬁcs on the terminal cost
and terminal time did not play a role in our derivation of the HJB equation.
For diﬀerent target sets, the boundary condition changes (as we already
discussed) but the HJB equation remains the same.
However, the HJB
equation will not hold for (t, x) ∈S just like it does not hold at t = t1 in
the ﬁxed-time case, because the principle of optimality is not valid there.
We can apply one more transformation in order to rewrite the HJB
equation in a simpler—and also more insightful—way. It is easy to check
that (5.10) is equivalent to
Vt(t, x) = sup
u∈U
{⟨−Vx(t, x), f(t, x, u)⟩−L(t, x, u)}.
(5.11)
Let us now recall our earlier deﬁnition (3.28) of the Hamiltonian, reproduced
here:
H(t, x, u, p) := ⟨p, f(t, x, u)⟩−L(t, x, u).
We see that the expression inside the supremum in (5.11) is nothing but the
Hamiltonian, with −Vx playing the role of the costate. This brings us to the
Hamiltonian form of the HJB equation:
Vt(t, x) = sup
u∈U
H (t, x, u, −Vx(t, x)) .
(5.12)
So far, the existence of an optimal control has not been assumed. When
an optimal (in the global sense) control u∗does exist, the inﬁmum in the
previous calculations can be replaced by a minimum and this minimum is
achieved when u∗is plugged in.
In particular, the principle of optimal-
ity (5.4) yields
V (t, x∗(t)) =
min
u[t,t+∆t]
 Z t+∆t
t
L(s, x(s), u(s))ds + V (t + ∆t, x(t + ∆t))

=
Z t+∆t
t
L(s, x∗(s), u∗(s))ds + V (t + ∆t, x∗(t + ∆t))

THE HAMILTON-JACOBI-BELLMAN EQUATION
163
where x∗(·) and x(·) are trajectories corresponding to u∗(·) and u(·), re-
spectively, both passing through the same point x∗(t) at time t. From this,
repeating the same steps that led us earlier to the HJB equation (5.10), we
obtain
−Vt(t, x∗(t)) = min
u∈U {L(t, x∗(t), u) + ⟨Vx(t, x∗(t)), f(t, x∗(t), u)⟩}
= L(t, x∗(t), u∗(t)) + ⟨Vx(t, x∗(t)), f(t, x∗(t), u∗(t))⟩.
(5.13)
Expressed in terms of the Hamiltonian, the second equation in (5.13) be-
comes
H(t, x∗(t), u∗(t), −Vx(t, x∗(t))) = max
u∈U H(t, x∗(t), u, −Vx(t, x∗(t))).
(5.14)
This Hamiltonian maximization condition is analogous to the one we had in
the maximum principle. We see that if we can ﬁnd a closed-form expression
for the control that maximizes the Hamiltonian—or, equivalently, the con-
trol that achieves the inﬁmum in the HJB equation (5.10)—then the HJB
equation becomes simpler and more explicit.
Example 5.1
Consider the standard integrator ˙x = u (with x, u ∈R)
and let L(x, u) = x4 + u4. The corresponding HJB equation is
−Vt(t, x) = inf
u∈R{x4 + u4 + Vx(t, x)u}.
(5.15)
Since the expression inside the inﬁmum is polynomial in u, we can easily
ﬁnd the control that achieves the inﬁmum: diﬀerentiating with respect to u,
we have 4u3 + Vx(t, x) = 0 which yields u = −
  1
4Vx(t, x)
1/3. Plugging this
control into the HJB equation (5.15), we obtain
−Vt(t, x) = x4 −3
  1
4Vx(t, x)
4/3.
(5.16)
Assuming that we can solve the PDE (5.16) for the value function V , the op-
timal control is given in the state feedback form1 u∗(t) = −
  1
4Vx(t, x∗(t))
1/3.
Solving (5.16), however, appears to be diﬃcult.
□
Example 5.2
Consider again the minimal-time parking problem from
Section 4.4.1. The HJB equation for this problem is
−Vt(t, x) =
inf
u∈[−1,1]{1 + Vx1(t, x)x2 + Vx2(t, x)u}.
(5.17)
1Note the diﬀerence between the previous expression for the minimizing control u,
which is given pointwise in t and x, and the current expression for u∗which is evaluated
along the corresponding state trajectory x∗. The claim about optimality of u∗is actually
somewhat informal at this point, but will be carefully justiﬁed very soon (in Section 5.1.4).

164
CHAPTER 5
Note that this is a free-time, ﬁxed-endpoint problem, for which the boundary
condition takes the form V (t, 0) = 0 for all t, and (5.17) is valid away from
x = 0. The inﬁmum on the right-hand side of (5.17) is achieved by setting
u = −sgn(Vx2(t, x)) =





1
if Vx2(t, x) < 0,
−1
if Vx2(t, x) > 0,
?
if Vx2(t, x) = 0.
It is informative to compare this expression for u with (4.50). Substituting
it into (5.17), we arrive at the simpliﬁed HJB equation
−Vt(t, x) = 1 + Vx1(t, x)x2 −|Vx2(t, x)|.
(5.18)
The optimal control is given by the feedback law u∗(t) = −sgn(Vx2(t, x∗(t))),
whose implementation of course hinges on our ability to solve (5.18) for V .
We end the example here but invite the reader to play more with it in the
next exercise.
□
Exercise 5.2
Do you see a way to further simplify the HJB equation (5.18)
from Example 5.2? Can you solve this HJB equation and ﬁnd the value func-
tion? Can you use the above analysis to reproduce the results we obtained
in Section 4.4.1 (i.e., the bang-bang principle and the complete description
of the optimal feedback law)? Conversely, can you use those earlier results
to obtain more information about the value function?
□
Infinite-horizon problem
The reader may ﬁnd it somewhat discouraging that even for simple examples
like the ones we just presented, solving the HJB equation analytically is a
challenging task. Nevertheless, the HJB equation gives a considerable insight
into the problem, and one can often apply numerical techniques to obtain
approximate solutions. We now discuss one important situation in which the
HJB equation takes a simpler form. Suppose that both the control system
and the cost functional are time-invariant, i.e., f = f(x, u) and L = L(x, u),
and that there is no terminal cost (K ≡0). Keeping the ﬁnal state free, we
let the ﬁnal time t1 approach ∞; in the limit, the cost functional becomes
J(u) =
R ∞
t0 L(x(t), u(t))dt and we have what is called an inﬁnite-horizon
problem.2 It is clear that in this scenario, the cost does not depend on the
2For this inﬁnite-horizon problem to be well posed, we need to be sure that the cost
is ﬁnite at least for some controls. We will investigate this issue in detail for the linear
quadratic regulator problem in the next chapter. For now we proceed formally, without
worrying about the possibility that the problem might be ill posed.

THE HAMILTON-JACOBI-BELLMAN EQUATION
165
initial time, hence the value function depends on x only: V = V (x). Thus
the partial derivative Vt vanishes and the HJB equation (5.10) reduces to
0 = inf
u∈U {L(x, u) + ⟨Vx(x), f(x, u)⟩} .
(5.19)
We let the reader think of other problem formulations for which the HJB
equation simpliﬁes in the same way.
The PDE (5.19) may still be diﬃcult to solve, but it is certainly more
tractable than the general HJB equation (5.10). In the special case when
x is scalar, (5.19) is actually an ODE. Let us consider the inﬁnite-horizon
version of Example 5.1. The HJB equation becomes x4 −3
  1
4Vx(x)
4/3 = 0
from which we derive Vx(x) = ±
  1
3
3/44x3. We must choose the plus sign
because V should be positive deﬁnite (since L is positive deﬁnite). We do
not even need to solve for V because the optimal feedback law is obtained
from Vx directly: u∗(t) = −
  1
4Vx(x∗(t))
1/3 = −
  1
3
1/4x∗(t).
5.1.4
Suﬃcient condition for optimality
Together, the HJB equation—written as (5.10) or (5.12)—and the Hamil-
tonian maximization condition (5.14) constitute necessary conditions for
optimality. It should be clear that all we proved so far is their necessity.
Indeed, deﬁning V to be the value function, we showed that it must sat-
isfy the HJB equation. Assuming further that an optimal control exists, we
showed that it must maximize the Hamiltonian along the optimal trajec-
tory. However, we will see next that these conditions are also suﬃcient for
optimality. Namely, we will establish the following suﬃcient condition
for optimality: Suppose that a C1 function bV : [t0, t1] × Rn →R satisﬁes
the HJB equation
−bVt(t, x) = inf
u∈U

L(t, x, u) +

bVx(t, x), f(t, x, u)
	
(5.20)
(for all t ∈[t0, t1) and all x ∈Rn) and the boundary condition
bV (t1, x) = K(x).
(5.21)
Suppose that a control ˆu : [t0, t1] →U and the corresponding trajectory ˆx :
[t0, t1] →Rn, with the given initial condition ˆx(t0) = x0, satisfy everywhere
the equation
L(t, ˆx(t), ˆu(t)) +

bVx(t, ˆx(t)), f(t, ˆx(t), ˆu(t))

= min
u∈U

L(t, ˆx(t), u) +

bVx(t, ˆx(t)), f(t, ˆx(t), u)
	
(5.22)
which is equivalent to the Hamiltonian maximization condition
H(t, ˆx(t), ˆu(t), −bVx(t, ˆx(t))) = max
u∈U H(t, ˆx(t), u, −bVx(t, ˆx(t))).

166
CHAPTER 5
Then bV (t0, x0) is the optimal cost (i.e., bV (t0, x0) = V (t0, x0) where V is the
value function) and ˆu is an optimal control. (Note that this optimal control
is not claimed to be unique; there can be multiple controls giving the same
cost.)
To prove this result, let us ﬁrst apply (5.20) with x = ˆx(t). We know
from (5.22) that along ˆx, the inﬁmum is a minimum and it is achieved at ˆu;
hence we have, similarly to (5.13),
−bVt(t, ˆx(t)) = L(t, ˆx(t), ˆu(t)) +

bVx(t, ˆx(t)), f(t, ˆx(t), ˆu(t))

.
We can move the bVt term to the right-hand side and note that together with
the inner product of bVx and f it forms the total time derivative of bV along
ˆx:
0 = L(t, ˆx(t), ˆu(t)) + d
dt
bV (t, ˆx(t)).
Integrating this equality with respect to t from t0 to t1, we have
0 =
Z t1
t0
L(t, ˆx(t), ˆu(t))dt + bV (t1, ˆx(t1)) −bV (t0, ˆx(t0))
which, in view of the boundary condition for bV and the initial condition for
ˆx, gives
bV (t0, x0) =
Z t1
t0
L(t, ˆx(t), ˆu(t))dt + K(ˆx(t1)) = J(t0, x0, ˆu).
(5.23)
On the other hand, if x is another trajectory with the same initial condition
corresponding to an arbitrary control u, then (5.20) implies that
−bVt(t, x(t)) ≤L(t, x(t), u(t)) + ⟨bVx(t, x(t)), f(t, x(t), u(t))⟩
or
0 ≤L(t, x(t), u(t)) + d
dt
bV (t, x(t)).
Integrating over [t0, t1] as before, we obtain
0 ≤
Z t1
t0
L(t, x(t), u(t))dt + bV (t1, x(t1)) −bV (t0, x(t0))
or
bV (t0, x0) ≤
Z t1
t0
L(t, x(t), u(t))dt + K(x(t1)) = J(t0, x0, u).
(5.24)
The equation (5.23) and the inequality (5.24) show that ˆu gives the cost
bV (t0, x0) while no other control u can produce a smaller cost. Thus we have
conﬁrmed that bV is the optimal cost and ˆu is an optimal control.

THE HAMILTON-JACOBI-BELLMAN EQUATION
167
We can regard the function bV in the above suﬃcient condition as provid-
ing a tool for verifying optimality of candidate optimal controls (obtained,
for example, from the maximum principle). This optimality is automatically
global. A simple modiﬁcation of the above argument yields that bV (t, ˆx(t)) is
the optimal cost-to-go from an arbitrary point (t, ˆx(t)) on the trajectory ˆx.
More generally, since bV is deﬁned for all t and x, we could use an arbitrary
pair (t, x) in place of (t0, x0) and obtain optimality with respect to x(t) = x
as the initial condition in the same way. Thus, if we have a family of controls
parameterized by (t, x), each fulﬁlling the Hamiltonian maximization condi-
tion along the corresponding trajectory which starts at x(t) = x, then bV is
the value function and it lets us establish optimality of all these controls. A
typical way in which such a control family can arise is from a state feedback
law description; we will encounter a scenario of this kind in Chapter 6. The
next two exercises oﬀer somewhat diﬀerent twists on the above suﬃcient
condition for optimality.
Exercise 5.3
Suppose that a control ˆu : [t0, t1] →U, the corresponding
trajectory ˆx : [t0, t1] →Rn with ˆx(t0) = x0, and a function bV : [t0, t1]×Rn →
R are such that bV (t0, x0) = J(t0, x0, ˆu) with J as in (5.1), i.e., bV (t0, x0) is
the cost corresponding to the control ˆu. Suppose that bV satisﬁes the HJB
equation (5.20) and the boundary condition (5.21). Show that then bV (t0, x0)
is the optimal cost and ˆu is an optimal control.
□
Exercise 5.4
Formulate and prove a suﬃcient condition for optimality,
analogous to the one proved above, for the inﬁnite-horizon problem described
at the end of Section 5.1.3.
□
5.1.5
Historical remarks
The HJB partial diﬀerential equation has its origins in the work of Hamilton,
with subsequent improvements by Jacobi, done in the context of calculus of
variations in the late 1830s. At that time the equation served as a neces-
sary condition for optimality. Its use as a suﬃcient condition—still in the
calculus of variations setting—was proposed in the work of Carath´eodory,
begun in the 1920s and culminating in his book published in 1935.
(He
established local optimality by working in a neighborhood of a test curve.)
Carath´eodory’s approach became known as the “royal road” of calculus of
variations.
The principle of optimality seems, in hindsight, an almost trivial ob-
servation, which actually dates all the way back to Jacob Bernoulli’s 1697
solution of the brachistochrone problem. In the early 1950s, slightly be-
fore Bellman, the principle of optimality was formalized in the context of
diﬀerential games by Isaacs, who called it the “tenet of transition.” (The

168
CHAPTER 5
fundamental PDE of game theory bears Isaacs’s initial alongside those of
Hamilton, Jacobi, and Bellman.) The term “dynamic programming” was
coined by Bellman, who published a series of papers and the book [Bel57]
on this subject in the 1950s. Bellman’s contribution was to recognize the
power of the method to study value functions globally, and to use it for
solving a variety of calculus of variations and optimal control problems.
It is not clear if Bellman was aware of the close connection between his
work and the Hamilton-Jacobi equation of calculus of variations. This con-
nection was explicitly made in the early 1960s by Kalman, who was appar-
ently the ﬁrst to use the name “HJB equation.” Kalman’s derivation of suf-
ﬁcient conditions for optimal control, combining the ideas of Carath´eodory
and Bellman, provided the basis for the treatment given here and in other
modern sources. (The work of Kalman will also be prominently featured in
the next chapter, where we discuss linear systems and quadratic costs.)
Quite remarkably, the maximum principle was being developed in the So-
viet Union independently around the same time as Bellman’s and Kalman’s
work on dynamic programming was appearing in the United States. We
thus ﬁnd it natural at this point to compare the maximum principle with
the HJB equation and discuss the relationship between the two approaches.
5.2
HJB EQUATION VERSUS THE MAXIMUM PRINCIPLE
Here we focus on the necessary conditions for optimality provided by the
HJB equation (5.10) and the Hamiltonian maximization condition (5.14)
on one hand and by the maximum principle on the other hand. There is a
notable diﬀerence in how these two necessary conditions characterize optimal
controls. In order to see this point more clearly, assume that the system and
the cost are time-invariant. The maximum principle is formulated in terms
of the canonical equations
˙x∗= Hp|∗,
˙p∗= −Hx|∗
(5.25)
and says that at each time t, the value u∗(t) of the optimal control must
maximize H(x∗(t), u, p∗(t)) with respect to u:
u∗(t) = arg max
u∈U H(x∗(t), u, p∗(t)).
(5.26)
This is an open-loop speciﬁcation, because u∗(t) depends not only on the
state x∗(t) but also on the costate p∗(t) which has to be computed from the
adjoint diﬀerential equation. Now, in the context of the HJB equation, the
optimal control must satisfy
u∗(t) = arg max
u∈U H(x∗(t), u, −Vx(t, x∗(t))).
(5.27)

THE HAMILTON-JACOBI-BELLMAN EQUATION
169
This is a closed-loop (feedback) speciﬁcation; indeed, assuming that we know
the value function V everywhere, u∗(t) is completely determined by the cur-
rent state x∗(t). The ability to generate an optimal control policy in the
form of a state feedback law is an important feature of the dynamic pro-
gramming approach, as we in fact already knew from Section 5.1.1. Clearly,
we cannot implement this feedback law unless we can ﬁrst ﬁnd the value
function by solving the HJB partial diﬀerential equation, and we have seen
that this is in general a very diﬃcult task.
Therefore, from the compu-
tational point of view the maximum principle has an advantage in that it
involves only ordinary and not partial diﬀerential equations. In principle,
the dynamic programming approach provides more information (including
suﬃciency), but in reality, the maximum principle is often easier to use
and allows one to solve many optimal control problems for which the HJB
equation is analytically intractable.
As another point of comparison, it is interesting to recall how much
longer and more complicated our proof of the maximum principle was com-
pared with our derivation of the necessary conditions based on the HJB equa-
tion. This diﬀerence is especially perplexing in view of the striking similarity
between the two Hamiltonian maximization conditions (5.26) and (5.27). We
may wonder whether it might actually be possible to give an easier proof of
the maximum principle starting from the HJB equation. Suppose that u∗is
an optimal control and x∗is the corresponding state trajectory. Still assum-
ing for simplicity that f and L are time-independent, we know that (5.27)
must hold, where V is the value function satisfying
−Vt(t, x∗(t)) = L(x∗(t), u∗(t)) + ⟨Vx(t, x∗(t)), f(x∗(t), u∗(t))⟩.
(5.28)
To establish the maximum principle, we need to prove the existence of a
costate p∗with the required properties.
The formulas (5.26) and (5.27)
strongly suggest that we should try to deﬁne it via
p∗(t) := −Vx(t, x∗(t)).
(5.29)
Then, the desired Hamiltonian maximization condition (5.26) automatically
follows from (5.27).
We note also that if V satisﬁes the boundary con-
dition V (t1, x) = K(x) as in (5.3), then the boundary condition for the
costate (5.29) is p∗(t1) = −Kx(x∗(t1)), and this matches the boundary con-
dition (4.43) that we had in the maximum principle for problems with ter-
minal cost. Thus far, the situation looks quite promising, but we do not
have any apparent reason to expect that p∗deﬁned by (5.29) will satisfy the
second diﬀerential equation in (5.25). However, this turns out to be true as
well!
Exercise 5.5
Let p∗be deﬁned by (5.29), with V fulﬁlling (5.28). Prove
that p∗satisﬁes the second canonical equation ˙p∗= −Hx(x∗, u∗, p∗), where
as usual H(x, u, p) = ⟨p, f(x, u)⟩−L(x, u).
□

170
CHAPTER 5
In the proof of the maximum principle, the adjoint vector p∗was deﬁned
as the normal to a suitable hyperplane. In our earlier discussions in Sec-
tion 3.4, it was also related to the momentum and to the vector of Lagrange
multipliers. From (5.29) we now have another interpretation of the adjoint
vector in terms of the gradient of the value function, i.e., the sensitivity of
the optimal cost with respect to the state x. In economic terms, this quan-
tity corresponds to the “marginal value,” or “shadow price”; it tells us by
how much we can increase beneﬁts by increasing resources/spending, or how
much we would be willing to pay someone else for resources and still make
a proﬁt.
At this point, the reader may be puzzled as to why we cannot indeed
deduce the maximum principle from the HJB equation via the reasoning
just given. Upon careful inspection, however, we can identify one gap in
the above argument: it assumes that the value function has a well-deﬁned
gradient and, moreover, that this gradient can be further diﬀerentiated with
respect to time (to obtain the adjoint equation as in Exercise 5.5). In other
words, we need the existence of second-order partial derivatives of V . At
the very least, we need V to be a C1 function—a property that we have in
fact assumed all along, starting with the Taylor expansion (5.7). The next
example demonstrates that, unfortunately, we cannot expect this to be true
in general.
5.2.1
Example: nondiﬀerentiable value function
For the scalar system
˙x = xu
with x ∈R and u ∈[−1, 1], consider a ﬁxed-time, free-endpoint problem
with the cost J(u) = x(t1). The optimal solution is easily found by inspec-
tion: if x0 > 0 then apply u ≡−1 which results in ˙x = −x, hence the cost
is x(t1) = e−(t1−t0)x0; if x0 < 0 then use u ≡1 which gives ˙x = x and the
cost is x(t1) = et1−t0x0; ﬁnally, if x0 = 0 then x ≡0 for all u and the cost is
0. We see that the value function is given by
V (t, x) =





e−(t1−t)x
if x > 0,
et1−tx
if x < 0,
0
if x = 0.
(5.30)
Away from x = 0 it indeed satisﬁes the HJB equation for this example, which
is −Vt = infu {Vx xu} = −|Vx x|, with the boundary condition V (t1, x) = x.
For a ﬁxed t < t1, the graph of V as a function of x is plotted in Figure 5.4.
At x = 0 this function is Lipschitz but not C1. It can actually be shown
that the above HJB equation does not admit any C1 solution.

THE HAMILTON-JACOBI-BELLMAN EQUATION
171
V (t, x)
x
Figure 5.4: Value function nondiﬀerentiable at x = 0
It turns out that this state of aﬀairs is not an exception; in fact, it is
quite typical for problems with bounded controls and terminal cost to have
nondiﬀerentiable value functions. On the other hand, the local Lipschitz
property—which the function (5.30) does possess—is a known attribute of
value functions for some reasonably general classes of optimal control prob-
lems (we will say more on this below).
The above example clariﬁes why we cannot derive the maximum princi-
ple from the HJB equation. There really is no “easy” proof of the maximum
principle (except in settings much less general than the one we considered).
More importantly, the diﬃculty that we just exposed has implications not
only for relating the HJB equation and the maximum principle, but for
the HJB theory itself. Namely, we need to reconsider the assumption that
V ∈C1 and instead work with some generalized concept of a solution to the
HJB partial diﬀerential equation.3 Because of this diﬃculty, the theory of
dynamic programming did not become rigorous until the early 1980s when,
after a series of related developments, the notion of a viscosity solution was
introduced by Crandall and Lions; that work completes the historical time-
line of key contributions listed in Section 5.1.5. (The maximum principle,
on the other hand, was on solid technical ground from the beginning.) We
turn to viscosity solutions in the next section, postponing a discussion of
further links between the HJB equation and the maximum principle until
Section 7.2.
3We note that generalized solution concepts, particularly those relaxing the continuous
diﬀerentiability requirement, are important in the theory of ordinary diﬀerential equations
as well. One such solution concept (albeit a very simple one) is provided by the class of
absolutely continuous functions, as we discussed in Section 3.3.1.

172
CHAPTER 5
5.3
VISCOSITY SOLUTIONS OF THE HJB EQUATION
We ﬁrst need to familiarize ourselves with a few basic notions and results
from nonsmooth analysis.
5.3.1
One-sided diﬀerentials
Let v : Rn →R be a continuous function (nothing beyond continuity is
required from v). A vector ξ ∈Rn is called a super-diﬀerential of v at a
given point x if for all y near x we have the relation
v(y) ≤v(x) + ⟨ξ, y −x⟩+ o(|y −x|).
(5.31)
Geometrically, ξ is a super-diﬀerential if the graph of the linear function
y 7→v(x) + ⟨ξ, y −x⟩, which has ξ as its gradient and takes the value v(x)
at y = x, lies above the graph of v at least locally near x (or is tangent to
the graph of v at x). Figure 5.5 (a) illustrates this situation for the scalar
case (n = 1), in which ξ is the slope of the line. A super-diﬀerential ξ is in
general not unique; we thus have a set of super-diﬀerentials of v at x, which
is denoted by D+v(x).
y
v(y)
v
slope ξ
x
y
v(y)
v
slope ξ
x
Figure 5.5: (a) super-diﬀerential, (b) sub-diﬀerential
Similarly, we say that ξ ∈Rn is a sub-diﬀerential of v at x if
v(y) ≥v(x) + ⟨ξ, y −x⟩−o(|y −x|).
(5.32)
The graph of the linear function with gradient ξ touching the graph of v at
x must now lie below the graph of v in a vicinity of x (or be tangent to it
at x); see Figure 5.5 (b). The set of sub-diﬀerentials of v at x is denoted by
D−v(x).

THE HAMILTON-JACOBI-BELLMAN EQUATION
173
Example 5.3
For the function
v(x) =





0
if x < 0,
√x
if 0 ≤x ≤1,
1
if x > 1
plotted in Figure 5.6, the reader should have no diﬃculty in verifying that
D+v(0) = ∅, D−v(0) = [0, ∞), D+v(1) = [0, 1/2], and D−v(1) = ∅. As
we will see shortly, these points at which v is not diﬀerentiable are the only
“interesting” points to check.
□
y
v(y)
1
Figure 5.6: The function in Example 5.3
We now establish some useful properties of super- and sub-diﬀerentials.
Test functions. ξ ∈D+v(x) if and only if there exists a C1 function
ϕ : Rn →R such that ∇ϕ(x) = ξ, ϕ(x) = v(x), and ϕ(y) ≥v(y) for all y
near x, i.e., ϕ −v has a local minimum at x. Similarly, ξ ∈D−v(x) if and
only if there exists a C1 function ϕ such that ∇ϕ(x) = ξ and ϕ −v has a
local maximum at x. (Note that we can always arrange to have ϕ(x) = v(x)
by adding a constant to ϕ, which does not aﬀect the other conditions.)
y
v(y)
v
slope ξ
x
ϕ
Figure 5.7: Characterization of a super-diﬀerential via a test function
The function ϕ is sometimes called a test function.
For the case of
D+v(x), an example of such a function is shown in Figure 5.7. The above

174
CHAPTER 5
result, whose proof we will not give, will be used for proving the other facts
that follow.
Relation with classical differentials. If v is diﬀerentiable at x, then
D+v(x) = D−v(x) = {∇v(x)}.
(5.33)
If D+v(x) and D−v(x) are both nonempty, then v is diﬀerentiable at x and
the relation (5.33) holds.
We prove both claims with the help of test functions. First, suppose
that v is diﬀerentiable at x. It is clear that the gradient ∇v(x) is both a
super-diﬀerential and a sub-diﬀerential of v at x (indeed, with ξ = ∇v(x)
both (5.31) and (5.32) become equalities). If ϕ −v has a local minimum or
maximum at x for some ϕ ∈C1, then ∇(ϕ−v)(x) = 0 hence ∇ϕ(x) = ∇v(x).
This shows that ξ = ∇v(x) is the only element in D+v(x) and D−v(x).
To prove the second claim, let ϕ1, ϕ2 ∈C1 be such that ϕ1(x) = v(x) =
ϕ2(x) and ϕ1(y) ≤v(y) ≤ϕ2(y) for y near x. Then ϕ1 −ϕ2 has a local
maximum at x, implying that ∇(ϕ1 −ϕ2)(x) = 0 hence ∇ϕ1(x) = ∇ϕ2(x).
For y > x (the case y < x is completely analogous) we can write
ϕ1(y) −ϕ1(x)
y −x
≤v(y) −v(x)
y −x
≤ϕ2(y) −ϕ2(x)
y −x
.
As y →x, the ﬁrst fraction approaches ∇ϕ1(x), the last one approaches
∇ϕ2(x), and we know that the two gradients are equal. Therefore, by the
“Sandwich Theorem” the limit of the middle fraction exists and equals the
other two. This limit must be ∇v(x), and everything is proved.
Non-emptiness and denseness. The sets {x : D+v(x) ̸= ∅} and {x :
D−v(x) ̸= ∅} are both nonempty, and actually dense in the domain of v.
The idea of the proof is sketched in Figure 5.8. The highly irregular
graph in the ﬁgure is that of v. Take an arbitrary point x0. Choosing a
C1 function ϕ steep enough (see the solid curve in the ﬁgure), we can force
ϕ −v to have a local maximum at a nearby point x, as close to x0 as we
want. (For clarity, we also shift the graph of ϕ vertically to produce the
dotted curve in the ﬁgure which touches the graph of v at x.) We then have
∇ϕ(x) ∈D−v(x). A similar argument works for D+.
Exercise 5.6
Give an example of a continuous function v with the property
that for some x in its domain both D+v(x) and D−v(x) are empty.
□
5.3.2
Viscosity solutions of PDEs
We are now ready to introduce the concept of a viscosity solution for PDEs.
Consider a PDE of the form
F(x, v(x), ∇v(x)) = 0
(5.34)

THE HAMILTON-JACOBI-BELLMAN EQUATION
175
y
v(y)
x0 x
Figure 5.8: Proving denseness
where F : Rn×R×Rn →R is a continuous function. A viscosity subsolution
of the PDE (5.34) is a continuous function v : Rn →R such that
F(x, v(x), ξ) ≤0
∀ξ ∈D+v(x), ∀x.
(5.35)
As we know, this is equivalent to saying that at every x we must have
F(x, v(x), ∇ϕ(x)) ≤0 for every C1 test function ϕ such that ϕ −v has a
local minimum at x. Similarly, v is a viscosity supersolution of (5.34) if
F(x, v(x), ξ) ≥0
∀ξ ∈D−v(x), ∀x
(5.36)
or, equivalently, at every x we have F(x, v(x), ∇ϕ(x)) ≥0 for every C1
function ϕ such that ϕ−v has a local maximum at x. Finally, v is a viscosity
solution if it is both a viscosity subsolution and a viscosity supersolution.
The above deﬁnitions of a viscosity subsolution and supersolution impose
conditions on v only at points where D+v, respectively D−v, is nonempty.
We know that the set of these points is dense in the domain of v. At all
points where v is diﬀerentiable, the PDE must hold in the classical sense.
If v is Lipschitz, then by Rademacher’s theorem it is diﬀerentiable almost
everywhere.
Example 5.4
Consider the scalar case (n = 1) and let F(x, v, ξ) = 1−|ξ|,
for which the PDE (5.34) is 1 −|∇v(x)| = 0. The functions v(x) = x and
v(x) = −x are both classical solutions of this PDE (so are the functions
v(x) = ±x+c where c is a constant). We claim that the function v(x) = |x|
is a viscosity solution. For all x ̸= 0 this v is diﬀerentiable and the PDE is
satisﬁed, thus we only need to check what happens at x = 0. First, D+v(0) =
∅hence (5.35) is true. Second, D−v(0) = [−1, 1] and the inequality 1−|ξ| ≥
0 holds for all ξ ∈[−1, 1], making (5.36) true as well.
□
Note the lack of symmetry in the deﬁnition of a viscosity solution: the
sign convention used when writing the PDE is important.
In the above

176
CHAPTER 5
example, if we rewrite the PDE as |∇v(x)| −1 = 0, then it is easy to see
that v(x) = |x| is no longer a viscosity solution.
The terminology “viscosity solutions” is motivated by the fact that a
viscosity solution v of the PDE (5.34) can be obtained from smooth solutions
vε to the family of PDEs
F(x, vε(x), ∇vε(x)) = ε∆vε(x)
(5.37)
(parameterized by ε > 0) in the limit as ε →0. The operator ∆on the right-
hand side of (5.37) denotes the Laplacian (∆v = vx1x1 +· · ·+vxnxn); in ﬂuid
mechanics it appears in the PDE describing the motion of a viscous ﬂuid. To
understand the basic idea behind this convergence result, let ξ ∈D−v(x0)
for some x0. Consider a test function ϕ such that ∇ϕ(x0) = ξ and ϕ−v has
a local maximum at x0. Assume that ϕ ∈C2 (if not, approximate it by a C2
function). If vε is close to v for small ε, then ϕ −vε has a local maximum at
some xε near x0, implying that ∇ϕ(xε) = ∇vε(xε) and ∆ϕ(xε) ≤∆vε(xε).
Since vε solves (5.37), this gives F(xε, vε(xε), ∇ϕ(xε)) ≥ε∆ϕ(xε). Taking
the limit as ε →0, by continuity of F we have F(x0, v(x0), ∇ϕ(x0)) ≥0,
which means that v is a supersolution of (5.34). The argument showing that
v is a subsolution is similar.
5.3.3
HJB equation and the value function
Let us ﬁnally go back to our ﬁxed-time optimal control problem from Sec-
tion 5.1.2 and its HJB equation (5.10) with the boundary condition (5.3),
with the goal of resolving the diﬃculty identiﬁed at the end of Section 5.2.1.
Speciﬁcally, our hope is that the value function (5.2) is a solution of the
HJB equation in the viscosity sense. In order to more closely match the
PDE (5.34), we ﬁrst rewrite the HJB equation as
−Vt(t, x) −inf
u∈U {L(t, x, u) + ⟨Vx(t, x), f(t, x, u)⟩} = 0.
(5.38)
As we saw in the previous subsection, ﬂipping the sign in a PDE aﬀects
its viscosity solutions; it will turn out that the above sign convention is
the correct one. The PDE (5.38) is still not in the form (5.34) because it
contains the additional independent variable t. However, the concepts and
results of Sections 5.3.1 and 5.3.2 extend to this time-space setting without
diﬃculties. Alternatively, we can absorb t into x by introducing the extra
state variable xn+1 := t, a trick already familiar to us from Chapters 3 and 4.
Then the PDE (5.38) takes the form (5.34) with the obvious deﬁnition of the
function F, except that the domain of v is the subset4 [t0, t1] × Rn of Rn+1.
4In Sections 5.3.1 and 5.3.2 we could have easily taken the domain of v to be a subset
of Rn rather than the entire Rn.

THE HAMILTON-JACOBI-BELLMAN EQUATION
177
The control u does not appear as an argument of F since the deﬁnition of
F includes taking the inﬁmum over u.
Now the theory of viscosity solutions can be applied to the HJB equation.
Under suitable technical assumptions on the functions f, L, and K, we
have the following main result: The value function V is a unique viscosity
solution of the HJB equation (5.38) with the boundary condition (5.3). It is
also locally Lipschitz (but, as we know from Section 5.2.1, not necessarily
C1). Regarding the technical assumptions, we will not list them here (see
the references in Section 5.4) but we mention that they are satisﬁed if, for
example, f, L, and K are uniformly continuous, fx, Lx, and Kx are bounded,
and U is a compact set.
We will not attempt to establish the uniqueness and the Lipschitz prop-
erty, but we do want to understand why V is a viscosity solution of (5.38).
To this end, let us prove that V is a viscosity subsolution. The additional
technical assumptions cited above are actually not needed for this claim.
Fix an arbitrary pair (t0, x0). We need to show that for every C1 test func-
tion ϕ = ϕ(t, x) such that ϕ −V attains a local minimum at (t0, x0), the
inequality
−ϕt(t0, x0) −inf
u∈U{L(t0, x0, u) + ⟨ϕx(t0, x0), f(t0, x0, u)⟩} ≤0
must be satisﬁed. Suppose that, on the contrary, there exist a C1 function
ϕ and a control value u0 ∈U such that
ϕ(t0, x0) = V (t0, x0),
ϕ(t, x) ≥V (t, x)
∀(t, x) near (t0, x0)
(5.39)
and
−ϕt(t0, x0) −L(t0, x0, u0) −⟨ϕx(t0, x0), f(t0, x0, u0)⟩> 0.
(5.40)
Taking (t0, x0) as the initial condition, let us consider the state trajectory
x(·) that results from applying the control u ≡u0 on a small time interval
[t0, t0 + ∆t]. We will now demonstrate that the rate of change of the value
function along this trajectory is inconsistent with the principle of optimality.
As long as we pick ∆t to be suﬃciently small, we have
V (t0 + ∆t, x(t0 + ∆t)) −V (t0, x0) ≤ϕ(t0 + ∆t, x(t0 + ∆t)) −ϕ(t0, x0)
=
Z t0+∆t
t0
d
dtϕ(t, x(t))dt =
Z t0+∆t
t0
 ϕt(t, x(t)) + ⟨ϕx(t, x(t)), f(t, x(t), u0)⟩

dt
< −
Z t0+∆t
t0
L(t, x(t), u0)dt
where the ﬁrst inequality is a direct consequence of (5.39) and the last
inequality follows from (5.40) by virtue of continuity of all the functions

178
CHAPTER 5
appearing there. We thus obtain
V (t0, x0) >
Z t0+∆t
t0
L(t, x(t), u0)dt + V (t0 + ∆t, x(t0 + ∆t)).
(5.41)
On the other hand, the principle of optimality (5.4) tells us that
V (t0, x0) ≤
Z t0+∆t
t0
L(t, x(t), u0)dt + V (t0 + ∆t, x(t0 + ∆t))
and we arrive at a contradiction. Provided that optimal controls exist, here
is a slightly diﬀerent way to see why (5.41) cannot be true: it would imply
that the optimal cost-to-go from (t0, x0) is higher than the cost of applying
the constant control u ≡u0 on [t0, t0 + ∆t] followed by an optimal control
on the remaining interval (t0 + ∆t, t1], which is clearly impossible.
Exercise 5.7
Use a similar argument to show that the value function is a
viscosity supersolution of the HJB equation (5.38).
□
We now have at our disposal a more rigorous formulation of the necessary
conditions for optimality from Section 5.1.3. The above reasoning is of course
quite diﬀerent from our original derivation of the HJB equation, but we see
that the principle of optimality still plays a central role.
The suﬃcient
condition for optimality from Section 5.1.4 can also be generalized, a task
that we leave as an exercise.
Exercise 5.8
Formulate and prove a suﬃcient condition for optimality
analogous to that of Exercise 5.3 in the framework of viscosity solutions.
□
5.4
NOTES AND REFERENCES FOR CHAPTER 5
The treatment of the discrete problem in Section 5.1.1 is based on [Son98,
Section 8.1]. The material of Sections 5.1.2–5.1.4 is quite standard; it is
assembled from the texts [AF66, BP07, LM67, Son98, YZ99] which further
elaborate on some aspects of this theory. The brief historical remarks of
Section 5.1.5 are compiled largely from [YZ99]; for additional reading on
how the subject has evolved we recommend [Bry96] and [PB94] (the latter
paper is more technical and explains how Carath´eodory’s work was, along
with subsequent work of Hestenes, also a precursor of the maximum princi-
ple). A derivation of the Hamilton-Jacobi PDE as a necessary condition for
optimality in the context of calculus of variations (via the general formula
for the variation of a functional), as well as an explanation of its connection
to Hamilton’s canonical equations, can be found in [GF63, Section 23]; see
also [YZ99, pp. 222–223].

THE HAMILTON-JACOBI-BELLMAN EQUATION
179
Our discussion of the relationship between the HJB equation and the
maximum principle in Section 5.2 follows [YZ99, pp. 229–230] and [Vin00,
Section 1.6].
The economic interpretation of the adjoint vector and the
value function is developed in more detail in [YZ99, pp. 231–232]; similar
ideas appear in [Lue84, Section 4.4] and are taken further in [Cla10]. The
example of Section 5.2.1 is Example 2.3 in [YZ99, Chapter 4], where it is
proved that the corresponding HJB equation admits no C1 solution. The
same example is also studied in [Vin00, Section 1.7], and a diﬀerent example
with similar features is Example 7.2 in [BP07]. For results showing—under
diﬀerent assumptions—that the value function is locally Lipschitz, see The-
orem 2.5 in [YZ99, Chapter 4], Lemma 8.6.2 in [BP07], or Proposition 12.3.5
in [Vin00].
Section 5.3 is heavily based on the exposition in [BP07, Chapter 8]. The
original reference on viscosity solutions is [CL83]. The convergence result
sketched at the end of Section 5.3.2 is Theorem 8.4.2 in [BP07]; for back-
ground on the motion of viscous ﬂuids the reader can consult [FLS63, Chap-
ter II-41]. A complete proof of the main result in Section 5.3.3, again under
diﬀerent technical assumptions, can be found in [BP07] and [YZ99]. Other
generalized solution concepts for HJB equations have been proposed; see
the notes and references in [YZ99, Chapter 4] and [Vin00, Chapter 12]. For
an in-depth study of HJB equations and their viscosity solutions, including
numerical methods, see [BCD97].

Chapter Six
The Linear Quadratic Regulator
6.1
FINITE-HORIZON LQR PROBLEM
In this chapter we will focus on the special case when the system dynamics
are linear and the cost is quadratic. While this additional structure certainly
makes the optimal control problem more tractable, our goal is not merely
to specialize our earlier results to this simpler setting. Rather, we want to
go deeper and develop a more complete understanding of optimal solutions
compared with what we were able to achieve for the general scenarios treated
in the previous chapters. (We could have followed a diﬀerent path in our
studies and started with this speciﬁc problem class before tackling more
diﬃcult problems. From the pedagogical point of view, each approach has
its own merits. Historically, however, the general nonlinear results—having
their origins in calculus of variations—appeared ﬁrst.)
The (ﬁnite-horizon) Linear Quadratic Regulator (LQR) problem is the
optimal control problem from Section 3.3 with the following additional as-
sumptions: the control system is a linear time-varying system
˙x = A(t)x + B(t)u,
x(t0) = x0
(6.1)
with x ∈Rn and u ∈Rm (the control is unconstrained); the target set is
S = {t1}×Rn, where t1 is a ﬁxed time (so this is a ﬁxed-time, free-endpoint
problem); and the cost functional is
J(u) =
Z t1
t0
 xT (t)Q(t)x(t) + uT (t)R(t)u(t)

dt + xT (t1)Mx(t1)
(6.2)
where Q(·), R(·), M are matrices of appropriate dimensions satisfying M =
MT ≥0 (symmetric positive semideﬁnite), Q(t) = QT (t) ≥0 (symmetric
positive semideﬁnite), and R(t) = RT (t) > 0 (symmetric positive deﬁnite)
for all t ∈[t0, t1]. The quadratic cost (6.2) is very reasonable: since both Q
and R are positive (semi)deﬁnite, it penalizes both the size of the state and
the control eﬀort, with Q and R determining their relative weights. Inci-
dentally, the formula L(t, x, u) = xT Q(t)x + uT R(t)u for the running cost is
another justiﬁcation of the acronym LQR. We require R to be strictly posi-
tive deﬁnite because we will soon need its inverse. Additional assumptions
180

THE LINEAR QUADRATIC REGULATOR
181
will be introduced later as necessary. More general target sets can also be
considered.
6.1.1
Candidate optimal feedback law
We begin our analysis of the LQR problem by inspecting the necessary
conditions for optimality provided by the maximum principle. After some
further manipulation, these conditions will reveal that an optimal control
must be a linear state feedback law. The Hamiltonian is given by
H(t, x, u, p) = pT A(t)x + pT B(t)u −xT Q(t)x −uT R(t)u.
Note that, compared to the general formula (4.40) for the Hamiltonian, we
took the abnormal multiplier p0 to be equal to −1. This is no loss of gen-
erality because, for the present free-endpoint problem in the Bolza form, a
combination of the results in Section 4.3.1 would give us the transversality
condition 0 = p∗(t1) −p∗
0Kx(x∗(t1)) = p∗(t1) −2p∗
0Mx∗(t1) which, in light
of the nontriviality condition, guarantees that p∗
0 ̸= 0. It is also useful to
observe that the LQR problem can be adequately treated with the varia-
tional approach of Section 3.4, which yields essentially the same necessary
conditions as the maximum principle but without the abnormal multiplier
appearing. Indeed, the control is unconstrained, the ﬁnal state is free, and
H is quadratic—hence twice diﬀerentiable—in u; therefore, the technical
issues discussed in Section 3.4.5 do not arise here. In Section 3.4 we proved
that along an optimal trajectory we must have Hu|∗= 0 and Huu|∗≤0,
which is in general diﬀerent from the Hamiltonian maximization condition,
but in the present LQR setting this diﬀerence disappears as we will see in
a moment. In fact, when solving part a) of Exercise 3.8, the reader should
have already written down the necessary conditions for optimality from Sec-
tion 3.4.3 for the LQR problem (with M = 0). We will now rederive these
necessary conditions and examine their consequences in more detail.
The gradient of H with respect to u is Hu = BT (t)p−2R(t)u, and along
an optimal trajectory it must vanish. Using our assumption that R(t) is
invertible for all t, we can solve the resulting equation for u and conclude
that an optimal control u∗(if it exists) must satisfy
u∗(t) = 1
2R−1(t)BT (t)p∗(t).
(6.3)
Moreover, since Huu = −2R(t) < 0, the above control indeed maximizes the
Hamiltonian (globally). We see that (6.3) is the unique control satisfying the
necessary conditions, although we have not yet veriﬁed that it is optimal.
Since the formula (6.3) expresses u∗in terms of the costate p∗, let us
look at p∗more closely. It satisﬁes the adjoint equation
˙p∗= −Hx|∗= 2Q(t)x∗−AT (t)p∗
(6.4)

182
CHAPTER 6
with the boundary condition
p∗(t1) = −Kx(x∗(t1)) = −2Mx∗(t1)
(6.5)
(see Section 4.3.1), where x∗is the optimal state trajectory. Our next goal
is to show that a linear relation of the form
p∗(t) = −2P(t)x∗(t)
(6.6)
holds for all t and not just for t = t1, where P(·) is some matrix-valued
function to be determined. Putting together the dynamics (6.1) of the state,
the control law (6.3), and the dynamics (6.4) of the costate, we can write the
system of canonical equations in the following combined closed-loop form:
 ˙x∗
˙p∗

=
 A(t)
1
2B(t)R−1(t)BT (t)
2Q(t)
−AT (t)
 x∗
p∗

=: H(t)
x∗
p∗

.
(6.7)
The matrix H(t) is sometimes called the Hamiltonian matrix. Let us de-
note the state transition matrix for the linear time-varying system (6.7)
by Φ(·, ·). Then we have, in particular,
x∗(t)
p∗(t)

= Φ(t, t1)
x∗(t1)
p∗(t1)

; here
Φ(t, t1) = Φ−1(t1, t) propagates the solutions backward in time from t1 to t.
Partitioning Φ into n × n blocks as
Φ =
Φ11
Φ12
Φ21
Φ22

gives the more detailed relation
x∗(t)
p∗(t)

=
Φ11(t, t1)
Φ12(t, t1)
Φ21(t, t1)
Φ22(t, t1)
 x∗(t1)
p∗(t1)

which, in view of the terminal condition (6.5), can be written as
x∗(t) = (Φ11(t, t1) −2Φ12(t, t1)M)x∗(t1),
(6.8)
p∗(t) = (Φ21(t, t1) −2Φ22(t, t1)M)x∗(t1).
(6.9)
Solving (6.8) for x∗(t1) and plugging the result into (6.9), we obtain
p∗(t) =
 Φ21(t, t1) −2Φ22(t, t1)M
 Φ11(t, t1) −2Φ12(t, t1)M
−1x∗(t).
We have thus established (6.6) with
P(t) := −1
2
 Φ21(t, t1) −2Φ22(t, t1)M
 Φ11(t, t1) −2Φ12(t, t1)M
−1. (6.10)
A couple of remarks are in order. First, we have not yet justiﬁed the
existence of the inverse in the deﬁnition of P(t). For now, we note that

THE LINEAR QUADRATIC REGULATOR
183
Φ(t1, t1) = I2n×2n, hence Φ11(t1, t1) = In×n, Φ12(t1, t1) = 0n×n, and so
Φ11(t1, t1) −2Φ12(t1, t1)M = In×n. By continuity, Φ11(t, t1) −2Φ12(t, t1)M
stays invertible for t close enough to t1, which means that P(t) is well deﬁned
at least for t near t1. Second, the minus sign and the factor of 1/2 in (6.10),
which stem from the factor of −2 in (6.6), appear to be somewhat arbitrary
at this point. We see from (6.5) and (6.6) that
P(t1) = M.
(6.11)
The reason for the above conventions will become clear later, when we show
that P(t) is symmetric positive semideﬁnite for all t (not just for t = t1)
and is directly related to the optimal cost.
Combining (6.3) and (6.6), we deduce that the optimal control must take
the form
u∗(t) = −R−1(t)BT (t)P(t)x∗(t)
(6.12)
which, as we announced earlier, is a linear state feedback law. This is a re-
markable conclusion, as it shows that the optimal closed-loop system must
be linear.1
Note that the feedback gain in (6.12) is time-varying even if
the system (6.1) is time-invariant, because P from (6.10) is always time-
varying. We remark that we could just as easily derive an open-loop for-
mula for u∗by writing (6.8) with t0 in place of t, i.e., x0 = (Φ11(t0, t1) −
2Φ12(t0, t1)M)x∗(t1), solving it for x∗(t1) and plugging the result into (6.9)
to arrive at
p∗(t) =
 Φ21(t, t1) −2Φ22(t, t1)M
 Φ11(t0, t1) −2Φ12(t0, t1)M
−1x0
(provided that the inverse exists), and then using this expression in (6.3).
However, the feedback form of u∗is theoretically revealing and leads to a
more compact description of the closed-loop system.
As we said, there are two things that we still need to check: optimality of
the control u∗that we found, and global existence of the matrix P(t). These
issues will be tackled in Sections 6.1.3 and 6.1.4, respectively. But ﬁrst, we
want to obtain a nicer description for the matrix P(t), as the formula (6.10)
is rather clumsy and not very useful (since calculating the state transition
matrix Φ analytically is in general impossible).
6.1.2
Riccati diﬀerential equation
We will now derive a diﬀerential equation that the matrix P(·) deﬁned
in (6.10) must satisfy. First, diﬀerentiate both sides of the equality (6.6)
to obtain
˙p∗(t) = −2 ˙P(t)x∗(t) −2P(t) ˙x∗(t).
1Of course, the idea that quadratic optimization problems lead to linear update laws
goes back to Gauss’s least squares method.

184
CHAPTER 6
Next, expand ˙p∗and ˙x∗using the canonical equations (6.7) to arrive at
2Q(t)x∗(t) −AT (t)p∗(t)
= −2 ˙P(t)x∗(t) −2P(t)A(t)x∗(t) −P(t)B(t)R−1(t)BT (t)p∗(t).
Applying (6.6) to eliminate p∗and dividing by 2, we conclude that the
equation
Q(t)x∗(t) + AT (t)P(t)x∗(t)
= −˙P(t)x∗(t) −P(t)A(t)x∗(t) + P(t)B(t)R−1(t)BT (t)P(t)x∗(t)
(6.13)
must hold (for all t at which P(t) is deﬁned). Since the initial state x0 is arbi-
trary and x∗is the state of the linear time-varying system given by (6.1) and
(6.12) whose state transition matrix is nonsingular, x∗(t) can be arbitrary.
It follows that P must be a solution of the matrix diﬀerential equation
˙P(t) = −P(t)A(t) −AT (t)P(t) −Q(t) + P(t)B(t)R−1(t)BT (t)P(t)
(6.14)
which is called the Riccati diﬀerential equation (RDE). We already
know that the boundary condition for it is speciﬁed by (6.11). The solution
P(t) is to be propagated backward in time from t = t1; its global existence
remains to be addressed.
It is interesting to compare the two descriptions that we now have for
P(t).
The RDE (6.14) is a quadratic matrix diﬀerential equation.
The
formula (6.10), on the other hand, is in terms of the state transition matrix Φ
which satisﬁes a linear matrix diﬀerential equation d
dtΦ(t, t1) = H(t)Φ(t, t1)
but has size 2n × 2n (while P is n × n). Ignoring the computational eﬀort
involved in computing the matrix inverse in (6.10), we can say that by
passing from (6.10) to (6.14) we reduced in half the size of the matrix to
be solved for, but traded a linear diﬀerential equation for a quadratic one.
Actually, if we prefer matrix diﬀerential equations that are linear rather
than quadratic, it is possible to compute P(t) somewhat more eﬃciently by
solving a linear system of size 2n × n, as shown in the next exercise.
Exercise 6.1
Let X(t), Y (t) be n × n matrices satisfying the linear dif-
ferential equation
 ˙X
˙Y

= H(t)
X
Y

with the boundary condition X(t1) = I, Y (t1) = −2M, where H(t) is deﬁned
in (6.7). Check that the matrix
P(t) := −1
2Y (t)X−1(t)
(6.15)

THE LINEAR QUADRATIC REGULATOR
185
satisﬁes the RDE (6.14) and the boundary condition (6.11).
□
The idea of reducing a quadratic diﬀerential equation to a linear one of
twice the size is in fact not new to us; we already saw it in Section 2.6.2
in the context of deriving second-order suﬃcient conditions for optimality
in calculus of variations. In the single-degree-of-freedom case, we passed
from the ﬁrst-order quadratic diﬀerential equation (2.64) to the second-order
linear diﬀerential equation (2.67) via the substitution (2.66). In the multiple-
degrees-of-freedom setting, scalar variables need to be replaced by matrices
but a similar transformation can be applied, as we stated (without including
the derivations) at the end of Section 2.6.2.
Associating the matrix W
there with the matrix P here, the reader will readily see the correspondence
between that earlier construction and the one given in Exercise 6.1.
The outcome of applying the necessary conditions of the maximum prin-
ciple to the LQR problem can now be summarized as follows: a unique
candidate for an optimal control is given by the linear feedback law (6.12),
where the matrix P(t) satisﬁes the RDE (6.14) and the boundary condi-
tion (6.11). This is as far as the maximum principle can take us; we need to
employ other tools for investigating whether P(t) exists for all t and whether
the control (6.12) is indeed optimal.
6.1.3
Value function and optimality
We now proceed to show that the control law (6.12) identiﬁed by the max-
imum principle is globally optimal.
We already asked the reader to ex-
amine this issue via a direct analysis of the second variation in part b)
of Exercise 3.8. Since then, however, we learned a general method for es-
tablishing optimality—namely, the suﬃcient condition for optimality from
Section 5.1.4—and it is instructive to see it in action here.
Specialized to our present LQR problem, the HJB equation (5.10) be-
comes
−Vt(t, x) = inf
u∈Rm

xT Q(t)x + uT R(t)u + ⟨Vx(t, x), A(t)x + B(t)u⟩
	
(6.16)
and the boundary condition (5.3) reads
V (t1, x) = xT Mx.
(6.17)
Since R(t) > 0, it is easy to see that the inﬁmum of the quadratic function
of u in (6.16) is a minimum and to calculate (similarly to how we arrived at
the formula (6.3) earlier) that the minimizing control is
u = −1
2R−1(t)BT (t)Vx(t, x).
(6.18)

186
CHAPTER 6
We substitute this control into (6.16) and, after some term cancellations,
bring the HJB equation to the following form:
−Vt(t, x) = xT Q(t)x + (Vx(t, x))T A(t)x
−1
4 (Vx(t, x))T B(t)R−1(t)BT (t)Vx(t, x).
(6.19)
In order to apply the suﬃcient condition for optimality proved in Sec-
tion 5.1.4, we need to ﬁnd a solution V (·, ·) of (6.19). Then, for the feedback
law (6.12) to be optimal, it should match the feedback law given by (6.18)
for this V . (The precise meaning of the last statement is provided by the
formula (5.22) on page 165.) This will in turn be true if we have
1
2Vx(t, x) = P(t)x
(6.20)
for all t and x.
The equation (6.20) suggests that we should look for a
function V that is deﬁned in terms of P. Another observation that supports
this idea is that in view of (6.11), the boundary condition (6.17) for the HJB
equation can be written as
V (t1, x) = xT P(t1)x.
(6.21)
Armed with these facts, let us apply a certain amount of “wishful thinking”
in solving the partial diﬀerential equation (6.19). Namely, let us try to guess
a function V that satisﬁes the simple conditions (6.20) and (6.21), and then
see if it satisﬁes the complicated equation (6.19). If it does, then it must
be the value function and by the previous reasoning our control (6.12) must
be optimal, hence everything will be proved. (In this last step, we are using
the fact that (6.12) is a feedback law which does not depend on the initial
condition; see the remarks immediately following the proof of the suﬃcient
condition for optimality on page 167.) Our argument will be valid for all
t ≤t1 for which P(t) exists.
For the moment, let us proceed under the assumption (which will be
validated momentarily) that P(t) is symmetric for all t. A guess for V is
actually fairly obvious, and might have already occurred to the reader:
V (t, x) = xT P(t)x.
(6.22)
This function clearly satisﬁes (6.21), and since its gradient with respect to
x is Vx(t, x) = 2P(t)x we see that (6.20) is also fulﬁlled. Now, let us check
whether the function (6.22) satisﬁes (6.19).
Since V ∈C1, the viscosity
solution concept is not needed here. Noting that Vt(t, x) = xT ˙P(t)x and
plugging the two expressions for the partial derivatives of V into (6.19), we
obtain
−xT ˙P(t)x = xT Q(t)x + 2xT P(t)A(t)x −xT P(t)B(t)R−1(t)BT (t)P(t)x

THE LINEAR QUADRATIC REGULATOR
187
or, equivalently,
−xT ˙P(t)x = xT  Q(t)+P(t)A(t)+AT (t)P(t)−P(t)B(t)R−1(t)BT (t)P(t)

x.
(6.23)
Since P(t) satisﬁes the RDE (6.14), it immediately follows that (6.23) is a
true identity. We conclude that, indeed, the function (6.22) is the value func-
tion (optimal cost-to-go) and the linear feedback law (6.12) is the optimal
control. (We already know from Section 6.1.1 that an optimal control must
be unique, and we also know that the suﬃcient condition of Section 5.1.4
guarantees global optimality.)
It is useful to reﬂect on how we found the optimal control. First, we
singled out a candidate optimal control by using the maximum principle.
Second, we identiﬁed a candidate value function and veriﬁed that this func-
tion and the candidate control satisfy the suﬃcient condition for optimality.
Thus we followed the typical path outlined in Section 5.1.4. The next exer-
cise takes a closer look at properties of P(t) and closes a gap that we left in
the above argument.
Exercise 6.2
Let t ≤t1 be an arbitrary time at which the solution P(t)
of the RDE (6.14) with the boundary condition (6.11) exists.
a) Prove that P(t) is a symmetric matrix.
b) Prove that P(t) ≥0 (positive semideﬁnite).
c) Can you prove that P(t) > 0 (positive deﬁnite)? If not, can you prove
this by strengthening one of the standing assumptions?
□
It was both insightful and convenient to employ the suﬃcient condition
for optimality in terms of the HJB equation to ﬁnd the expression for the
optimal cost and conﬁrm optimality of the control (6.12). However, having
the solution P(t) of the RDE (6.14) with the boundary condition (6.11)
in hand, it is also possible to solve the LQR problem by direct algebraic
manipulations without relying on any prior theory.
Exercise 6.3
Conﬁrm the facts that (6.12) is the unique optimal control
and (6.22) is the value function by using nothing more than square comple-
tion.
□
6.1.4
Global existence of solution for the RDE
We are ﬁnally in a position to prove that the solution P(·) of the RDE (6.14),
propagated backward in time from the terminal condition (6.11), exists for
all t ≤t1. It follows from the standard theory of local existence and unique-
ness of solutions to ordinary diﬀerential equations that P(t) exists for t suf-
ﬁciently close to t1. (We already came across such results in Section 3.3.1,

188
CHAPTER 6
with the diﬀerence that there solutions were propagated forward in time;
we also know about local existence of P(t) from a diﬀerent argument based
on the formula (6.10), which we gave on page 183.) As for global existence,
the problem is that some entry of P may have a ﬁnite escape time. In other
words, there may exist a time ¯t < t1 and some indices i, j ∈{1, . . . , n} such
that Pij(t) approaches ±∞as t ↘¯t. Such behavior is actually quite typical
for solutions of quadratic diﬀerential equations of Riccati type, as we dis-
cussed in detail on page 64. In the context of the formula (6.10), this would
mean that the matrix Φ11(t, t1) −2Φ12(t, t1)M becomes singular at t = ¯t.
Similarly, in the context of the formula (6.15) the ﬁnite escape would mean
that the matrix X(t) becomes singular at t = ¯t. In Section 2.6.2 we encoun-
tered a closely related situation and formalized it in terms of existence of
conjugate points. Fortunately, in the present setting it is not very diﬃcult
to show by a direct argument that all entries of P(t) remain bounded for all
t, relying on the fact—established in the previous subsection—that xT P(t)x
is the optimal LQR cost-to-go from (t, x) as long as it exists.
Seeking a contradiction, suppose that there is a ¯t < t1 such that P(t)
exists on the interval (¯t, t1] but some entry of P(t) becomes unbounded as
t ↘¯t. We know from Exercise 6.2 that for all t ∈(¯t, t1] the matrix P(t) is
symmetric and positive semideﬁnite, hence all its principal minors must be
nonnegative. If an oﬀ-diagonal entry Pij(t) becomes unbounded as t ↘¯t
while all diagonal entries stay bounded, then a certain 2 × 2 principal minor
of P(t) must be negative for t suﬃciently close to ¯t; namely, this is the
determinant of the matrix
i
j
i
j

∗
Pij(t)
Pij(t)
∗

formed by the i-th and the j-th row and column of P(t). Thus this scenario
is ruled out, and the only remaining possibility is that a diagonal entry Pii(t)
becomes unbounded as t ↘¯t. Consider the vector ei := (0, . . . , 1, . . . , 0)T ∈
Rn, with 1 in the i-th position and zeros everywhere else; then eT
i P(t)ei =
Pii(t) →∞as t ↘¯t. Suppose that the system is in state ei at some time
t > ¯t. We know that eT
i P(t)ei is the optimal cost-to-go from there, and so
this cost must be unbounded as we take t to be closer to ¯t. On the other
hand, this cost cannot exceed the cost of applying, e.g., the zero control on
[t, t1]. The state trajectory corresponding to this control is x(s) = ΦA(s, t)ei
for s ∈[t, t1], where ΦA(·, ·) is the state transition matrix for A(·), and the
associated cost is
Z t1
t
 eT
i ΦT
A(s, t)Q(s)ΦA(s, t)ei

ds + eT
i ΦT
A(t1, t)MΦA(t1, t)ei.

THE LINEAR QUADRATIC REGULATOR
189
It is quite clear that this cost remains bounded as t approaches ¯t, since the
quantity inside the integral is bounded for ¯t ≤t ≤s ≤t1. The resulting
contradiction proves that a ﬁnite escape time ¯t cannot exist.
The existence of the solution P(·) to the RDE (6.14) on the interval
[t0, t1] is now established.
Thus we can be sure that the optimal con-
trol (6.12) is well deﬁned, and the ﬁnite-horizon LQR problem has been
completely solved. We must be able to explicitly solve the RDE, though, if
we want to obtain a closed-form expression for the optimal control law.
Example 6.1
To obtain the simplest possible ﬁnite-horizon LQR problem,
consider the standard (scalar) integrator ˙x = u and let the cost be J(u) =
R t1
t0 (x2(t) + u2(t))dt. All the matrices become scalars here: A = 0, B = 1,
Q = 1, R = 1, M = 0. The RDE is the scalar diﬀerential equation ˙P =
P 2 −1, with the boundary condition P(t1) = 0. Our theory tells us that its
solution exists globally backward in time.2 Let us see if we can compute this
solution explicitly. Separating the variables and integrating, we have
Z 0
P(t)
dP
P 2 −1 =
Z t1
t
ds.
The integral on the left-hand side evaluates to tanh−1 P(t), hence the solu-
tion we were seeking is P(t) = tanh(t1 −t). Consequently, the feedback law
u(t) = −tanh(t1 −t)x(t) is the optimal control.
□
If analytically solving the RDE is not a completely trivial task even for
such an elementary example, we expect closed-form solutions to be obtain-
able only in very special cases. Yet, the LQR problem is much more tractable
compared to the general optimal control problem studied in Chapter 5. The
main simpliﬁcation is that instead of trying to solve the HJB equation which
is a partial diﬀerential equation, we now have to solve the RDE which is an
ordinary diﬀerential equation, and this can be done eﬃciently by standard
numerical methods. In the next section, we deﬁne and study a variant of
the LQR problem which lends itself to an even simpler solution; this devel-
opment will be in line with what we already saw, in a more general context
but in much less detail, towards the end of Section 5.1.3.
6.2
INFINITE-HORIZON LQR PROBLEM
The inﬁnite-horizon version of the LQR problem is a special case of the
general inﬁnite-horizon problem constructed in Section 5.1.3. Starting with
2It is interesting to digress brieﬂy and note that if we had R = −1 then the RDE
would be ˙P = −P 2 −1, which looks very similar but is fundamentally diﬀerent in that
its solutions do not exist globally backward in time (cf. page 64).
Thus our standing
assumption that R is positive is important. We invite the reader to ﬁgure out exactly
where this assumption comes into play in the preceding argument.

190
CHAPTER 6
the ﬁnite-horizon LQR problem deﬁned in Section 6.1, we ﬁrst assume that
both the control system and the cost functional are time-invariant and that
there is no terminal cost; this simply means that A, B, Q, and R are now
constant matrices and M = 0. Then, we want to consider the limit as the
ﬁnal time t1 approaches ∞. The resulting problem does not directly ﬁt into
the basic problem formulation of Section 3.3 and its well-posedness needs
to be proved; in particular, we do not know a priori whether the optimal
cost is ﬁnite (cf. footnote 2 on page 164). We did not try to settle this issue
in the general context of Section 5.1.3, but we will do it here for the LQR
problem. In this section we assume that the reader is familiar with basic
concepts of linear system theory such as controllability and observability.
In preparation for studying the limit as t1 →∞, let us treat the ﬁnal
time t1 as a parameter which, unlike in Section 6.1, is no longer ﬁxed. We
want to make our notation more explicit by displaying the dependence of
relevant quantities on this parameter. Speciﬁcally, from now on let us write
V t1 for the value function and denote by P(t, t1) the solution at time t of the
RDE (6.14) with the boundary condition P(t1) = 0. For each t1, let us also
relabel the optimal control and the optimal state trajectory (passing through
the given initial condition x0 at time t0) for the corresponding ﬁnite-horizon
LQR problem as u∗
t1 and x∗
t1, respectively. In this notation, the results of
Section 6.1.3 say that
u∗
t1(t) = −R−1BT P(t, t1)x∗
t1(t)
(6.24)
and
V t1(t, x) = xT P(t, t1)x.
(6.25)
In particular,
V t1(t0, x0) = xT
0 P(t0, t1)x0
(6.26)
is the ﬁnite-horizon optimal cost.
6.2.1
Existence and properties of the limit
We begin by making a series of observations about the behavior of P(t0, t1)
as a function of t1, with the goal of establishing that limt1→∞P(t0, t1) exists
(under the additional assumption of controllability) and has some interesting
properties. This path will eventually lead us to a complete solution of the
inﬁnite-horizon LQR problem.
Monotonicity. It is not hard to see that the ﬁnite-horizon optimal cost
xT
0 P(t0, t1)x0 is a monotonically nondecreasing function of the ﬁnal time t1.
Indeed, let t2 > t1. Using (6.25), the deﬁnition of the value function, and

THE LINEAR QUADRATIC REGULATOR
191
the standing assumptions that Q ≥0 and R > 0, we have
xT
0 P(t0, t2)x0 = V t2(t0, x0) =
Z t2
t0
 (x∗
t2)T (t)Qx∗
t2(t) + (u∗
t2)T (t)Ru∗
t2(t)

dt
≥
Z t1
t0
 (x∗
t2)T (t)Qx∗
t2(t) + (u∗
t2)T (t)Ru∗
t2(t)

dt ≥V t1(t0, x0) = xT
0 P(t0, t1)x0.
Boundedness. It is not true in general that the optimal cost xT
0 P(t0, t1)x0
remains bounded as t1 →∞. For example, if the system is ˙x = x (no control)
then its solutions are growing exponentials and the inﬁnite-horizon cost is
clearly unbounded. However, we now show that the ﬁnite-horizon optimal
cost xT
0 P(t0, t1)x0 remains bounded as t1 →∞assuming that (A, B) is a
controllable pair. Indeed, controllability guarantees the existence of a time ¯t
and a control ¯u that steers the state from x0 at time t0 to 0 at time ¯t. After
time ¯t, set ¯u equal to 0. This control yields a state trajectory ¯x satisfying
¯x(t) = 0 for all t ≥¯t, and so for every t1 ≥¯t we have
xT
0 P(t0, t1)x0 = V t1(t0, x0) ≤J(¯u) =
Z ¯t
t0
 ¯xT (t)Q¯x(t) + ¯uT (t)R¯u(t)

dt.
Since the above integral does not depend on t1, it provides a uniform bound
for the optimal cost—a single bound that is valid for all suﬃciently large t1,
as desired. We leave the controllability assumption in force for the rest of
this chapter (except for Exercise 6.5 on page 198 where its necessity will be
re-examined).
Existence of the limit. From the previous two claims it immediately
follows that xT
0 P(t0, t1)x0 has a limit as t1 →∞. It turns out that more is
true, namely, the matrix limt1→∞P(t0, t1) is well deﬁned. To see why, let us
consider some speciﬁc initial conditions x0 (we can do this because all the
facts established so far are valid for arbitrary x0). First, let x0 = ei with ei as
deﬁned on page 188 for some i ∈{1, . . . , n}. Then xT
0 P(t0, t1)x0 = Pii(t0, t1),
implying that each diagonal entry of P(t0, t1) has a limit as t1 →∞. Next,
let x0 = ei + ej for some i ̸= j.
Recalling that P(t0, t1) is symmetric
(Exercise 6.2), we have xT
0 P(t0, t1)x0 = Pii(t0, t1) + 2Pij(t0, t1) + Pjj(t0, t1),
from which we can deduce that the oﬀ-diagonal entries of P(t0, t1) converge
as well. We can think of limt1→∞P(t0, t1) as the solution of the RDE (6.14)
that, starting from the zero matrix, has ﬂown backward for inﬁnite time and
reached steady state; Figure 6.1 should help visualize this situation.
Properties of the limit. Since the RDE (6.14) has now become a time-
invariant diﬀerential equation, its solution P(t0, t1) actually depends only
on the diﬀerence t1 −t0.
Thus it is clear that the steady-state solution
limt1→∞P(t0, t1), whose existence we just established, does not depend on

192
CHAPTER 6
t
t
Pij(t)
Pij(t)
t0
t0
t1
t1
Figure 6.1: Steady-state solution of the RDE
t0, i.e., it is a constant matrix. Denoting it simply by P, we have
P = lim
t1→∞P(t, t1)
∀t.
(6.27)
Next, passing to the limit as t1 →∞on both sides of the RDE (6.14), we
see that limt1→∞˙P(t, t1) must also exist and be a constant matrix, which
must then necessarily be the zero matrix. We thus conclude that P is a
solution of the algebraic Riccati equation (ARE)
PA + AT P + Q −PBR−1BT P = 0
(6.28)
Conceptually, the step of passing from the RDE (6.14), which is a matrix
diﬀerential equation, to the ARE (6.28), which is a static matrix equation,
mirrors our earlier step of passing from the general HJB equation (5.10) to
its inﬁnite-horizon counterpart (5.19). In both cases, the time derivative is
eliminated; in the present case, we are left with no derivatives at all! The
ARE (6.28) can be solved analytically or numerically without diﬃculties.
It may happen, though, that the ARE has “spurious” extra solutions other
than (6.27). In this regard, it is useful to note that the matrix P given
by (6.27) must be symmetric positive semideﬁnite (because so is P(t, t1)
for each t1). We can hope that the ARE has only one solution with this
additional property. This is the case in the next exercise, and we will show
in Section 6.2.4 that this is always the case under appropriate assumptions.
Exercise 6.4
Consider the double integrator ˙x1 = x2, ˙x2 = u and the
cost J(u) =
R ∞
t0 (x2
1(t)+x2
2(t)+u2(t))dt. Find P by solving the ARE (6.28).
Verify (either analytically or numerically) that it is indeed the steady-state
solution of the RDE. Is the validity of the last statement aﬀected by changing
the terminal condition for the RDE (i.e., picking M ̸= 0)?
□

THE LINEAR QUADRATIC REGULATOR
193
6.2.2
Inﬁnite-horizon problem and its solution
We have now prepared the ground for taking the limit as t1 →∞and
considering the inﬁnite-horizon LQR problem with the cost
J(u) =
Z ∞
t0
 xT (t)Qx(t) + uT (t)Ru(t)

dt.
(6.29)
Recalling the optimal cost (6.26) and the optimal control (6.24) for the
ﬁnite-horizon case, and passing to the limit as t1 →∞, it is natural to guess
that the inﬁnite-horizon optimal cost and optimal control will be3
V (x0) = xT
0 Px0
(6.30)
and
u∗(t) = −R−1BT Px∗(t)
(6.31)
where P is the matrix limit (6.27) which satisﬁes the ARE (6.28). Note
that the quadratic cost (6.30) is independent of t0 and the linear feedback
law (6.31) is time-invariant, which is consistent with the problem formulation
and with our earlier ﬁndings in Section 5.1.3. Still, optimality of (6.31) is
far from obvious. Strictly speaking, the use of the asterisks in (6.31) is not
yet justiﬁed; at this point, x∗is simply the trajectory of the system under
the action of the feedback law (6.31) with x∗(t0) = x0.
We now show that the above guess is indeed correct. Consider the func-
tion bV (x) := xT Px. Its derivative along the trajectory x∗is
d
dt
bV (x∗(t)) = (x∗)T (t)
 P(A −BR−1BT P) + (AT −PBR−1BT )P

x∗(t)
= (x∗)T (t)(PA + AT P −2PBR−1BT P)x∗(t)
= −(x∗)T (t)(Q + PBR−1BT P)x∗(t)
where the last equality follows from the ARE (6.28). We can then calculate
the portion of the corresponding cost over an arbitrary ﬁnite interval [t0, T]
to be
Z T
t0
 (x∗)T (t)Qx∗(t) + (u∗)T (t)Ru∗(t)

dt
=
Z T
t0
(x∗)T (t)(Q + PBR−1BT P)x∗(t)dt = −
Z T
t0
d
dt
bV (x∗(t))dt
= bV (x0) −bV (x∗(T)) = xT
0 Px0 −(x∗)T (T)Px∗(T) ≤xT
0 Px0
3For consistency with our previous notation, it would have been more accurate to
denote the limiting quantities by V ∞, u∗
∞, etc. We omit the superscripts and subscripts
∞for simplicity, since we will be focusing on the inﬁnite-horizon problem from now on.

194
CHAPTER 6
where the last inequality follows from the fact that P ≥0. Taking the limit
as T →∞, we obtain
J(u∗) ≤xT
0 Px0.
(6.32)
In particular, we can be sure that the inﬁnite-horizon problem is well posed,
because u∗gives a bounded cost. (The control ¯u constructed on page 191
gives a bounded cost as well.) On the other hand, consider another trajec-
tory x with the same initial condition corresponding to an arbitrary control
u. Since xT
0 P(t0, t1)x0 is the ﬁnite-horizon optimal cost, we have for every
ﬁnite t1 that
xT
0 P(t0, t1)x0 ≤
Z t1
t0
 xT (t)Qx(t) + uT (t)Ru(t)

dt
≤
Z ∞
t0
 xT (t)Qx(t) + uT (t)Ru(t)

dt = J(u)
where the second inequality relies on the positive (semi)deﬁniteness of Q
and R. Passing to the limit as t1 →∞yields
xT
0 Px0 ≤J(u).
Comparing this inequality with (6.32) and remembering that u was arbitrary
(and could in particular be equal to u∗), we see that
J(u∗) = xT
0 Px0 ≤J(u)
∀u
hence xT
0 Px0 is the inﬁnite-horizon optimal cost and u∗is an optimal control,
as claimed.
6.2.3
Closed-loop stability
In the previous subsection we were able to obtain a complete solution to
the inﬁnite-horizon LQR problem, under the assumption (enforced since
Section 6.2.1) that the system is controllable. Here we investigate an im-
portant property of the optimal closed-loop system which will motivate us
to introduce one more assumption.
Example 6.2
Consider the scalar system ˙x = x + u and the cost J(u) =
R ∞
t0 u2(t)dt. The optimal control is, quite obviously, u∗≡0 because it gives
the zero cost. It corresponds to P = 0. (Incidentally, the ARE has another
solution P = 2 which must be discarded.) The optimal closed-loop system
˙x∗= x∗is unstable.
□
An optimal control that causes the state to grow unbounded is hardly
acceptable. The reason for this undesirable situation in the above example is
that the cost only takes into account the control eﬀort and does not penalize

THE LINEAR QUADRATIC REGULATOR
195
instability. It is natural to ask under what additional assumption(s) the
optimal control (6.31) automatically ensures that the state converges to 0.
One option is to require Q to be strictly positive deﬁnite, but we will see in
a moment that this would be an overkill. It is well known and easy to show
(for example, via diagonalization) that every symmetric positive semideﬁnite
matrix Q can be written as
Q = CT C
where C is a p × n matrix with p ≤n. Introducing the auxiliary output
y := Cx
we can rewrite the cost (6.29) as
J(u) =
Z ∞
t0
 yT (t)y(t) + uT (t)Ru(t)

dt.
Let us now assume that our system is observable through this output, i.e.,
assume that (A, C) is an observable pair. Note that if Q > 0 then C = Q1/2
(matrix square root) which is n × n and nonsingular, and the observability
assumption automatically holds. On the other hand, in Example 6.2 we had
Q = 0 hence C = 0 and the observability assumption is violated.
It is not diﬃcult to see why observability guarantees that the optimal
closed-loop system is asymptotically stable. We know from (6.32) that the
optimal control u∗gives a bounded cost (for arbitrary x0). This cost is
J(u∗) =
Z ∞
t0
 (y∗)T (t)y∗(t) + (u∗)T (t)Ru∗(t)

dt
(6.33)
where y∗(t) = Cx∗(t) is the output along the optimal trajectory. Next, recall
that u∗is generated by the linear feedback law (6.31), which means that the
optimal closed-loop system is a linear time-invariant system. Its solution x∗
is thus given by a linear combination of (complex) exponential functions of
time, and the same is true about y∗and u∗. This observation and the fact
that R > 0 make it clear that in order for the integral in (6.33) to be bounded
we must have y∗(t) →0 and u∗(t) →0 as t →∞. It is well known that
if the output and the input of an observable linear time-invariant system
converge to 0, then so does the state. (One way to see this is as follows:
by observability, there exists a matrix L such that A −LC has arbitrary
desired eigenvalues with negative real parts; rewriting the system dynamics
as ˙x = (A −LC)x + Ly + Bu, it is easy to show that x →0 when y, u →0.)
Therefore, the optimal closed-loop system must be asymptotically—in fact,
exponentially—stable.

196
CHAPTER 6
6.2.4
Complete result and discussion
We now collect the results obtained in this section so far, as well as a few
additional claims not yet proved, into a single theorem summarizing the
solution of the inﬁnite-horizon LQR problem and its main properties.
Theorem 6.1 (Main results for the inﬁnite-horizon LQR problem). Con-
sider the linear time-invariant control system
˙x = Ax + Bu
and the cost functional
J(u) =
Z ∞
t0
 xT (t)CT Cx(t) + uT (t)Ru(t)

dt
where (A, B) is a controllable pair, (A, C) is an observable pair, and R is
symmetric positive deﬁnite (all vectors and matrices have arbitrary compat-
ible dimensions). Then the following statements are true:
1) The limit P := limt1→∞P(t0, t1) of the solution at time t0 of the
RDE (6.14) with the boundary condition P(t1) = 0 exists; this is a
constant matrix (independent of t0) and it is a unique symmetric pos-
itive deﬁnite solution of the ARE (6.28).
2) The optimal cost is V (x0) = xT
0 Px0, which is the limit as t1 →∞of
the ﬁnite-horizon optimal cost (6.26).
3) The unique optimal control has the linear time-invariant state feedback
form u∗(t) = −R−1BT Px∗(t), which is the limit as t1 →∞of the
ﬁnite-horizon optimal feedback law (6.24).
4) The closed-loop system ˙x∗= (A−BR−1BT P)x∗is exponentially stable.
Proof. Statement 1 of the theorem was proved in Section 6.2.1 with the
exception of two claims: the strict positive deﬁniteness of P (we only showed
that P = P T ≥0) and the uniqueness property. Statements 2 and 3 were
proved in Section 6.2.2, except for the uniqueness of the optimal control.
Statement 4 was proved in Section 6.2.3. With these results in place, we
now establish the remaining claims.
Let us ﬁrst prove that P > 0. We already know that P ≥0. Suppose that
xT
0 Px0 = 0 for some x0, which in view of statement 2 means that for this ini-
tial condition x0 the optimal cost
R ∞
t0 ((x∗)T (t)CT Cx∗(t)+(u∗)T (t)Ru∗(t))dt
equals 0. This is possible only if Cx∗≡0 and u∗≡0 (since R > 0). By
observability we must then have x∗≡0, as it is well known that the output
of an observable linear time-invariant system (with the zero input) can be

THE LINEAR QUADRATIC REGULATOR
197
identically 0 only along the zero trajectory. Thus x0 = 0 which proves that
P is indeed positive deﬁnite.
Let us now prove that P is a unique solution of the ARE in the class of
positive deﬁnite matrices. In fact, we will show that it is unique even in the
class of positive semideﬁnite matrices. Suppose that the ARE has another
positive semideﬁnite solution ¯P. Consider the new cost
¯Jt1(u) :=
Z t1
t0
 xT (t)Qx(t) + uT (t)R(t)u(t)

dt + xT (t1) ¯Px(t1)
and its inﬁnite-horizon counterpart
¯J∞(u) := lim
t1→∞
 Z t1
t0
 xT (t)Qx(t) + uT (t)R(t)u(t)

dt + xT (t1) ¯Px(t1)

.
By statements 4 and 2 of this theorem, the same control u∗as in statement 3
gives
¯J∞(u∗) =
Z ∞
t0
 (x∗)T (t)Qx∗(t) + (u∗)T (t)Ru∗(t)

dt = xT
0 Px0
and this is the optimal cost with respect to ¯J∞because for every other
control u we have
¯J∞(u) ≥
Z ∞
t0
 xT (t)Qx(t) + uT (t)R(t)u(t)

dt ≥xT
0 Px0.
We also know from the results of Section 6.1 that the optimal cost with
respect to ¯Jt1(u) is xT
0 P(t0; ¯P, t1)x0, where P(t0; ¯P, t1) denotes the solution
at time t0 of the RDE (6.14) with the boundary condition P(t1) = ¯P. But
this solution must be ¯P itself, for every t1, because ¯P is an equilibrium
solution of the RDE (by virtue of satisfying the ARE). It follows4 that
¯P = P, and the uniqueness property is conﬁrmed.
Finally, to show that the optimal control is unique, recall the equa-
tions (5.13) and (5.14) in Section 5.1.3 which say that an optimal control
must satisfy
u∗(t) = arg min
u∈U {L(t, x∗(t), u) + ⟨Vx(t, x∗(t)), f(t, x∗(t), u)⟩}
or, what is the same,
u∗(t) = arg max
u∈U H(t, x∗(t), u, −Vx(t, x∗(t))).
4This step is actually not completely obvious; we leave the details for the reader to
work out.

198
CHAPTER 6
For the inﬁnite-horizon LQR problem and the value function V (x) = xT Px
this condition takes the form
u∗(t) = arg min
u∈Rm

(x∗)T (t)Qx∗(t) + uT Ru
+ 2(x∗)T (t)PAx∗(t) + 2(x∗)T (t)PBu
	
and it is easy to check (cf. Section 6.1.3) that this uniquely identiﬁes the
feedback law given in statement 3.
□
We can see from the proof that the observability assumption was only
used for establishing exponential stability of the optimal closed-loop system
(statement 4) and the positive deﬁniteness and uniqueness of P in state-
ment 1 (the uniqueness proof relied on observability indirectly because it
invoked statement 4). The controllability assumption, on the other hand,
was used for showing the existence of P which is crucial for all the other
claims. Nevertheless, there remains the possibility that some or all claims
could be proved without relying on these assumptions.
Exercise 6.5
Suppose that the assumptions of controllability of (A, B)
and observability of (A, C) are replaced by stabilizability and detectability,
respectively. Examine the validity of each claim of Theorem 6.1. (If still
true, justify; otherwise, explain why not and how the statement should be
modiﬁed to become true.)
□
In view of statement 1 of Theorem 6.1 and its proof, we know that
the ARE has exactly one positive semideﬁnite solution P, and this solution
is in fact positive deﬁnite and gives the optimal cost and optimal control
via statements 2 and 3. For example, consider the inﬁnite-horizon version of
Example 6.1, with the system ˙x = u and the cost J(u) =
R ∞
t0 (x2(t)+u2(t))dt.
The ARE P 2 −1 = 0 has two solutions, P = ±1, of which P = 1 is the
one we want. The optimal cost is V (x0) = x2
0 and the optimal control is
u∗= −x∗. The result that the reader obtained in Exercise 6.4 should also
immediately yield the optimal cost for that problem and the exponentially
stable optimal closed-loop system.
Linearity, time-invariance, and exponential stability are very desirable
features of the optimal closed-loop system, indicating that the inﬁnite-
horizon LQR problem provides good control design guidelines. The choice
of the matrices Q and R is often part of the design process, which helps
shape the behavior of the state and the control signal.
Exercise 6.6
Consider the scalar system ˙x = ax+bu and the cost J(u) =
R ∞
t0 (qx2(t) + ru2(t))dt, where a, q, r > 0 and b is arbitrary. Suppose that
a, b, q are ﬁxed but r can vary. Show that for r →0 (the “cheap control” case)
the eigenvalue of the optimal closed-loop system moves oﬀto −∞, while for

THE LINEAR QUADRATIC REGULATOR
199
r →∞(the “expensive control” case) the eigenvalue of the optimal closed-
loop system tends to −a, i.e., the opposite of the open-loop eigenvalue.
□
6.3
NOTES AND REFERENCES FOR CHAPTER 6
The linear quadratic regulator problem is a classical subject covered in many
textbooks.
This chapter has particularly beneﬁted from the expositions
in [AF66, AM90, Bro70, KS72]; basic background facts from linear system
theory can also be found in these books, as well as in more recent texts
such as [Hes09]. Kalman’s original LQR paper [Kal60] is a must-read, as it
contains the ﬁrst treatment of controllability for linear systems and a few
other fundamental concepts and techniques. Among several possible ways
of deriving the solution to the ﬁnite-horizon LQR problem, we favor the ap-
proach followed in [AF66] and [KS72] where the linear state feedback form
of the optimal control is established independently of the Riccati diﬀerential
equation (which is derived later). Section 24 of [Bro70] contains an insightful
discussion of Riccati diﬀerential equations and their solutions. Various meth-
ods for numerical solution of the RDE are described in [AM90, Appendix E],
[KS72, Section 3.5], and the references therein. For the inﬁnite-horizon LQR
problem, diﬀerent sources again take diﬀerent routes to arrive at the main
results. In particular, [AM90] gives a purely linear-algebraic proof of closed-
loop stability which does not rely on the Lyapunov argument that we used
in Section 6.2.2.
Many generalizations of the LQR problem are not discussed here. These
include: more general quadratic costs involving cross-terms; tracking prob-
lems; the output feedback version of the LQR problem and its relation to
optimal state estimation (the Kalman ﬁlter). The references [AF66, AM90,
KS72] cited earlier can be consulted for detailed information on these top-
ics. In Section 7.3 we will discuss robust control problems which can be
considered as extensions of the LQR problem.

Chapter Seven
Advanced Topics
In this chapter we discuss four additional topics (independent from one
another) which extend the developments of Chapters 4–6. The objective
of the present chapter is twofold: to enhance the reader’s understanding of
the earlier material, and to indicate some of the possible avenues for further
study. We will not strive for the same level of completeness and rigor here as
in the previous chapters, and instead will limit ourselves to an introductory
discussion. The references provided at the end of the chapter, as usual, can
be consulted for precise statements of technical results and other details.
7.1
MAXIMUM PRINCIPLE ON MANIFOLDS
Ever since we formulated the general optimal control problem in Section 3.3,
we have allowed the state x to take arbitrary values in Rn. However, in many
situations of interest Rn is not an adequate state space (just like U = Rm is
not always an adequate control space). Recall, for example, the pendulum
dynamics (2.55) that we derived on page 57. The angle variable θ naturally
lives on a circle. Combining it with the angular velocity ˙θ ∈R, we obtain the
state that evolves on a cylinder. State spaces of this kind are quite typical
for mechanical systems.
Surfaces such as a sphere, a cylinder, or a torus are all examples of (diﬀer-
entiable) manifolds. The purpose of this section is to reformulate our optimal
control problem and the maximum principle in the geometric language of
manifolds. Besides a higher level of generality, casting the maximum princi-
ple in the framework of manifolds has another important beneﬁt: it clariﬁes
the intrinsic meaning of the costate (adjoint vector), thereby greatly elu-
cidating the essence of the maximum principle even in the familiar setting
of control problems in Rn. We begin this task in Section 7.1.1, where we
describe manifolds more precisely as surfaces deﬁned by equality constraints
and discuss some fundamental objects associated with manifolds. It is worth
noting that scenarios in which a state space is characterized by inequality
constraints, although equally signiﬁcant, are not captured by the present
set-up.
200

ADVANCED TOPICS
201
7.1.1
Diﬀerentiable manifolds
We say that a set M is a k-dimensional diﬀerentiable manifold embedded in
Rn, or simply a manifold, if it is given by
M = {x ∈Rn : h1(x) = h2(x) = · · · = hn−k(x) = 0}
(7.1)
where hi, i = 1, . . . , n−k are C1 functions from Rn to R such that the gradi-
ent vectors ∇hi(x), i = 1, . . . , n−k are linearly independent for each x ∈M.
This is actually not a new object for us: it is just a k-dimensional surface
in Rn already considered in Section 1.2.2 and later in the formulation of the
Basic Variable-Endpoint Control Problem in Chapter 4. The linear inde-
pendence assumption imposed on the gradients means, in the terminology
of Section 1.2.2, that all points of M are regular.
Intuitively speaking, a k-dimensional manifold M is a subset of Rn that
locally “looks like” Rk. This idea can be made precise by equipping M with
local coordinates, as follows. Fix an arbitrary x ∈M. Since by assumption
the (nonsquare) Jacobian matrix



(h1)x1(x)
· · ·
(h1)xn(x)
...
...
...
(hn−k)x1(x)
· · ·
(hn−k)xn(x)



is of rank n −k, it has n −k linearly independent columns; shuﬄing the
variables x1, . . . , xn if necessary, we can assume that these are the last n −
k columns. Using the Implicit Function Theorem, we can then solve the
equations in (7.1) for the variables xk+1, . . . , xn in terms of x1, . . . , xk in
some neighborhood of x. Consequently, the components x1, . . . , xk can be
used to describe points in M near x, i.e., they provide a local coordinate
chart for M. A simple example of a 1-dimensional manifold in R2 is the unit
circle, for which we invite the reader to work out the above construction.
The standard deﬁnition of a manifold is actually stated in terms of the
existence of local coordinate charts satisfying suitable compatibility condi-
tions, without any explicit reference to the ambient space Rn. However, it
is known that every manifold deﬁned in this way can be embedded in Rn for
some n large enough. The above more concrete notion of an embedded man-
ifold is suﬃcient for our purposes, but the concept of local coordinates will
be useful for us as well. Basically, we have the choice of representing points
in M as n-vectors or locally as k-vectors (cf. the discussion of holonomic
constraints in Section 2.5.2).
Let M be a k-dimensional manifold. We know that at each point x ∈M
we can deﬁne the tangent space TxM (see page 11). This is a k-dimensional
linear vector space, spanned by the tangent vectors associated with all pos-
sible curves in M passing through x. Recall that for a curve x(·) lying in

202
CHAPTER 7
M, with x(0) = x, the corresponding tangent vector is ˙x(0). In local coordi-
nates, if the curve has components x1(·), . . . , xk(·) then the components of
the tangent vector ξ = ˙x(0) are (ξ1, . . . , ξk) = ( ˙x1(0), . . . , ˙xk(0)). It is also
useful to consider the union
TM :=
[
x∈M
TxM
which is called the tangent bundle of M. This is a manifold of dimension
2k. We note that, in general, TM is not globally diﬀeomorphic to the direct
product M × Rk (well-known counterexamples are obtained by taking M
to be the two-dimensional sphere or the Mobius band). It is convenient to
think of vector ﬁelds on M as sections of the tangent bundle, i.e., functions
f : M →TM such that f(x) ∈TxM for all x.
Given a point x on a manifold M, the linear vector space of linear R-
valued functions on TxM is called the cotangent space to M at x, and is
denoted by T ∗
xM. This is the dual space1 to TxM; its elements are called
cotangent vectors, or simply covectors. The simplest example of a covector
is the diﬀerential of a function. Let g : M →R be a C1 function. For each
x ∈M, the linear function dg|x : TxM →R is deﬁned as follows. Given a
tangent vector ξ ∈TxM and an arbitrary curve x(·) in M with x(0) = x
having ξ as its tangent vector (so that ˙x(0) = ξ), let
dg|x (ξ) :=
d
dα

α=0
g(x(α)).
In local coordinates this is represented by dg|x (ξ) = Pk
i=1 gxi(x) ˙xi(0) =
Pk
i=1 gxi(x)ξi, which clearly depends only on ξ and not on the choice of
a speciﬁc curve x(·). The map dg|x assigns to each tangent vector ξ the
derivative of g in the direction of ξ. If x1, . . . , xk are local coordinates on
M, then the corresponding diﬀerentials dx1, . . . , dxk form a basis for T ∗
xM
and hence yield local coordinates on the cotangent space. For a tangent
vector ξ ∈TxM, the numbers dxi(ξ) give the components ξi of ξ. One also
deﬁnes the cotangent bundle
T ∗M =
[
x∈M
T ∗
xM
which is itself a manifold, of dimension 2k. Combining local coordinates
x1, . . . , xk on M with local coordinates on T ∗
xM relative to the basis given
by dx1, . . . , dxk, one obtains local coordinates on the cotangent bundle T ∗M
known as canonical coordinates.
1The use of an asterisk for the dual space is not to be confused with the use of the
same symbol throughout the book to indicate optimality.

ADVANCED TOPICS
203
7.1.2
Re-interpreting the maximum principle
Suppose that we are given a (time-invariant) control system
˙x = f(x, u)
(7.2)
whose state x takes values in some k-dimensional manifold M and whose
control u takes values in some control set U. For its solution x(·) to stay
in M, the velocity vector f(x, u) must be tangent to M at x for all x and
u. Thus we see that there is an important diﬀerence between the states
and the velocity vectors: the former live in M while the latter live in the
tangent bundle TM. When we worked over Rn, which is its own tangent
space, we never explicitly made this distinction. In local coordinates, the
system description takes the form



˙x1
...
˙xk


=



f1(x, u)
...
fk(x, u)


∈Rk
and the diﬀerence between states and tangent vectors becomes “hidden”
once again.
Let us assume for simplicity that an optimal control problem is for-
mulated in the Mayer form (i.e., with terminal cost only). We know that
problems with running cost can always be converted to this form by append-
ing an additional state x0, which would yield a system on the augmented
manifold R × M (cf. Sections 3.3.2 and 4.2.1).
The basic ingredients of
the maximum principle are the costate p and the Hamiltonian H. In the
case when M = Rn, the Hamiltonian for the Mayer problem took the form
H(x, u, p) = ⟨p, f(x, u)⟩. For a general manifold M, we need to ask our-
selves which space p should belong to and how H should be (re)deﬁned.
Our ﬁrst natural guess might be that p, like f(x, u), should be a tangent
vector to M. However, in contrast with f(x, u), there is no clear geometric
reason why p should be a tangent vector. Also, taking p to be a tangent
vector, we cannot assign a new meaning to our earlier deﬁnition of H unless
we equip the tangent space with an inner product. (Introducing an inner
product on each tangent space TxM—called a Riemannian metric on M—is
possible but, as we will see, is neither necessary nor relevant for our present
purposes.) Another option that might come to mind is that p should live
in M itself; however, this choice oﬀers even fewer clues towards any natural
interpretation of the Hamiltonian.
Can we perhaps take a more direct guidance from the fact that in our
old deﬁnition of the Hamiltonian, p appears in an inner product with the
velocity vector f(x, u)? In fact, we already remarked in Section 3.4.2 that
p never appears by itself but always inside inner products such as ⟨p, ˙x⟩; in

204
CHAPTER 7
other words, it acts on velocity vectors. This observation suggests that the
intrinsic role of the costate p is not that of a tangent vector, but that of a
covector. To better understand the diﬀerence between these two types of
objects and why the latter one correctly captures the notion of a costate, let
us look at how they propagate along a ﬂow induced by a dynamical system
on M.
Fix a number τ > 0 and let Φτ : M →M be a C1 map. While the
construction that we are about to describe is valid for every such map, the
map that we have in mind here is the one obtained by ﬂowing forward for
τ units of time along the trajectory of the system (7.2) corresponding to
some ﬁxed control u (which, ultimately, is taken to be an optimal control
for a given initial condition). Let us ﬁrst discuss the transformation that
Φτ induces on tangent vectors. Pick a point x ∈M and a tangent vector
ξ ∈TxM. We know that ξ is tangent to some curve in M passing through x,
namely, ξ = ˙x(0) where x(α) ∈M for real α (around 0) and x(0) = x. The
image Φτ(x(·)) of this curve under the map Φτ is a curve in M which passes
through Φτ(x), as illustrated in Figure 7.1. Denote the tangent vector at
Φτ(x) associated with this new curve by dΦτ|x (ξ); in other words, deﬁne
dΦτ|x (ξ) :=
d
dα

α=0
Φτ(x(α)).
The above quantity depends only on the vector ξ and not on the choice of
a particular curve x(·) with this tangent vector. In this way we obtain a
natural deﬁnition of a linear map
dΦτ|x : TxM →TΦτ(x)M
called the derivative (or diﬀerential) of Φτ at x. It extends to manifolds the
standard notion of the diﬀerential of a function (described by its Jacobian
matrix) from vector calculus; more generally, functions from M to another
manifold N can be considered, and we already discussed the case N = R in
Section 7.1.1. In fact, in the proof of the maximum principle we performed a
closely related computation, showing that an inﬁnitesimal state perturbation
ξ propagates along an optimal trajectory of (7.2) according to the variational
equation
˙ξ = fx|∗ξ
(7.3)
(see Section 4.2.4).
The derivative map dΦτ pushes the tangent vectors
forward in the direction of action of the original map Φτ on M. Objects
such as tangent vectors, which propagate forward along a ﬂow on M in this
sense, are called contravariant.
Now suppose that we are given a covector at x, i.e., a linear function on
TxM. Let us denote it by p|x so as to have p|x (ξ) ∈R for each ξ ∈TxM.
For the same map Φτ : M →M as before, can we deﬁne in a natural

ADVANCED TOPICS
205
x
Φτ(x)
M
ξ
dΦτ|x(ξ)
Φτ
Figure 7.1: Tangent vectors propagate forward
way a linear function p|Φτ(x) on TΦτ(x)M? We must decide what the value
p|Φτ(x) (η) should be for every η ∈TΦτ(x)M. While it is tempting to say
that p|Φτ(x) (η) should equal the value of p|x on the preimage of η under the
map Φτ, this preimage is not well deﬁned unless the map Φτ is invertible.
In fact, the reader will quickly realize that there is no apparent candidate
map for propagating covectors along Φτ similarly to how the derivative map
dΦτ acts on tangent vectors. The reason is that, instead of trying to push
covectors forward, we should pull them back. This revised objective is read-
ily accomplished as follows: given a covector p|Φτ(x) on TΦτ(x)M, deﬁne a
covector p|x on TxM by
p|x (ξ) := p|Φτ(x) (dΦτ|x (ξ)).
(7.4)
As we indicated earlier, the intended meaning of Φτ is that of ﬂowing for-
ward for τ units of time along an optimal trajectory of (7.2), and the in-
ﬁnitesimal (as τ →0) transformation induced by the derivative map dΦτ
is the variational equation (7.3). We can now recognize the formula (7.4)
as expressing—in an intrinsic, coordinate-free fashion—the adjoint property
from Section 4.2.8; indeed, it guarantees that p(ξ) stays constant along the
trajectory. The familiar adjoint equation ˙p = −(fx)T 
∗p is nothing but the
inﬁnitesimal version of (7.4) written in local coordinates. A fact not really
revealed by this diﬀerential equation is that covectors are covariant objects,
in the sense that they propagate backward along a ﬂow on M. This is ex-
actly why we always have terminal rather than initial conditions for the
costate!
Now everything is beginning to fall into place. The Hamiltonian for our
Mayer problem on a manifold M should be deﬁned as
H(x, u, p) = p(f(x, u))
(7.5)
where the costate p is a covector at x (strictly speaking, it would be more
accurate to write it as p|x). The maximum principle postulates the existence

206
CHAPTER 7
of a costate p∗(t) ∈T ∗
x∗(t)M for each t, where x∗is the optimal trajectory
being analyzed. The terminal value p∗(tf) uniquely speciﬁes p∗(t) for all
t ∈[t0, tf] as explained above. The Hamiltonian maximization condition
takes the same form as in Chapter 4. For a formal statement of the max-
imum principle on manifolds along these lines, see the references listed in
Section 7.5. There is, however, one more concept that is usually involved
when such results are stated in the literature, and we examine it brieﬂy in
the next subsection.
7.1.3
Symplectic geometry and Hamiltonian ﬂows
In our discussion of Hamiltonian mechanics in Section 2.4, the Hamiltonian
was given a clear physical interpretation as the total energy of the system.
Hamilton’s canonical diﬀerential equations, on the other hand, were derived
formally from the Euler-Lagrange equation and we never paused to consider
their intrinsic meaning. We ﬁll this gap here by exposing the important
connection between symplectic geometry and Hamiltonian ﬂows, which pro-
vides one further insight into the geometric formulation of the maximum
principle.
Let us return to the cotangent bundle T ∗M which, as explained at the
end of Section 7.1.1, is a 2k-dimensional manifold equipped with canonical
local coordinates. On this manifold there is a natural diﬀerential 2-form
ω2, called a symplectic form (or symplectic structure). This is a bilinear
skew-symmetric form on the tangent space to T ∗M at each point; in other
words, it acts on pairs of tangent vectors to T ∗M (and, moreover, it depends
smoothly on the choice of a point in T ∗M). In canonical coordinates (x, p) =
(x1, . . . , xk, p1, . . . , pk) on T ∗M, the symplectic form that we consider here
is given by
ω2 := dx1 ∧dp1 + · · · + dxk ∧dpk
where the exterior multiplication ∧is deﬁned by
(ω1 ∧ω2)(ξ1, ξ2) := ω1(ξ1)ω2(ξ2) −ω2(ξ1)ω1(ξ2).
(The exterior product is the oriented area spanned by the vectors
ω1(ξ1)
ω1(ξ2)

and
ω2(ξ1)
ω2(ξ2)

, or the determinant of the corresponding matrix.) Given two
tangent vectors to T ∗M with components (y, q) = (y1, . . . , yk, q1, . . . , qk) and
(z, r) = (z1, . . . , zk, r1, . . . , rk), it is straightforward to check that
ω2 (y, q), (z, r)

=
k
X
i=1
(yiri −qizi).
(7.6)

ADVANCED TOPICS
207
The symplectic form allows us to establish a one-to-one correspondence
between tangent vectors to T ∗M and covectors on T ∗M, as follows. To a
tangent vector ξ (at a ﬁxed point of T ∗M) we associate a covector ω1
ξ which
acts on another tangent vector η to T ∗M according to
ω1
ξ(η) := ω2(ξ, η).
(7.7)
The map ξ 7→ω1
ξ is a linear map between vector spaces, whose matrix with
respect to the canonical basis is
0
−I
I
0

.
Now, we can view our Hamiltonian (7.5) as a function on the cotangent
bundle T ∗M (i.e., a function of the variables x and p) parameterized ad-
ditionally by the controls u. Then the diﬀerential dH of the Hamiltonian
gives us a covector on T ∗M at each point. Applying the inverse of the map
constructed above to this covector, we obtain a tangent vector at each point
of T ∗M, or a vector ﬁeld on T ∗M. This vector ﬁeld is called the Hamilto-
nian vector ﬁeld and is denoted by ⃗H. In canonical coordinates, the ﬂow
along ⃗H is described by the familiar diﬀerential equations
˙x = Hp,
˙p = −Hx.
(7.8)
Indeed, for an arbitrary tangent vector η to T ∗M with components (z, r) =
(z1, . . . , zk, r1, . . . , rk), we can easily check using (7.6) that with ⃗H deﬁned
via (7.8) we have
ω2( ⃗H, η) =
k
X
i=1
(Hxizi + Hpiri) = dH(η)
and the desired conclusion follows from (7.7).
Therefore, the statement
of the maximum principle about the existence of a costate satisfying the
adjoint equation (the second canonical equation) can be reformulated as
saying that an optimal state trajectory can be lifted to an integral curve
of the Hamiltonian vector ﬁeld. (The control u remains an argument on
the right-hand side of (7.8), but the maximum principle is of course applied
with an optimal control u∗plugged in.)
7.2
HJB EQUATION, CANONICAL EQUATIONS, AND
CHARACTERISTICS
We know from Section 5.1.3 that the HJB equation is a PDE which is diﬃcult
to solve in general.
In Section 5.2 we compared the HJB equation with
the necessary conditions of the maximum principle and demonstrated, in
particular, how solving the HJB equation for the value function V formally
leads to a solution of the canonical equations (via p := −Vx). In this section

208
CHAPTER 7
we discuss an important piece of the PDE theory which enables one, in
principle, to solve a PDE with the help of a suitable system of ODEs called
the characteristics. We will show that the canonical equations arise as the
characteristics of the HJB equation. Thus, compared to Section 5.2, the
method of characteristics takes us in the opposite direction: from a solution
of the canonical equations to a solution of the HJB equation. We will ﬁrst
introduce this method in the context of general PDEs and then specialize it
to the setting of the HJB equation.
7.2.1
Method of characteristics
Suppose that we are looking for a function v : Rn →R that solves the PDE
F(x, v(x), ∇v(x)) = 0
(7.9)
where F : Rn ×R×Rn →R is a given continuous function. This is the same
equation as (5.34) but here we assume that v is diﬀerentiable in the classical
sense. We are especially interested in the situation where F depends linearly
on ∇v; such PDEs are called quasi-linear. In addition, for simplicity and
better geometric intuition, we start with the case when n = 2. Then (7.9)
takes the form
a(x1, x2, v)vx1 + b(x1, x2, v)vx2 = c(x1, x2, v)
(7.10)
for some functions a, b, and c. Here and below, we omit some or all argu-
ments of functions wherever convenient.
The graph of a solution v = v(x1, x2) to (7.10) is a surface in the
(x1, x2, v)-space deﬁned by the equation h(x1, x2, v) := v(x1, x2) −v = 0.
The gradient vector ∇h = (vx1, vx2, −1)T is normal to this surface at each
point. Noting that the PDE (7.10) can be equivalently written as
* a
b
c
!
,
 vx1
vx2
−1
!+
= 0
we conclude that the vector (a, b, c)T is everywhere orthogonal to the normal
∇h, hence it lies in the tangent space to the solution surface. This is the
geometric interpretation of the quasi-linear PDE (7.10): the vector ﬁeld
(a, b, c)T is tangent to the solution surface v = v(x1, x2) at each point. The
system of ODEs associated with this vector ﬁeld can be written as
dx1
ds = a(x1, x2, v),
dx2
ds = b(x1, x2, v),
dv
ds = c(x1, x2, v).
(7.11)
These are called the characteristic ODEs of the PDE (7.10), and their solu-
tion curves are called the characteristics. With a slight abuse of terminology,

ADVANCED TOPICS
209
we sometimes use the term “characteristics” not only for the solution curves
of the equations (7.11) but also for these equations themselves.
The characteristic curves “ﬁll” our solution surface v = v(x1, x2); skip-
ping ahead to Figure 7.2 may help the reader visualize this situation. As an
example, it is useful to think about the case when c ≡0. Then (7.10) says
that the derivative of v in the direction of the vector (a, b)T is 0, implying
that v stays constant along solutions of the equations ˙x1 = a, ˙x2 = b. Every
function v whose level sets are solution curves of these two equations in the
(x1, x2)-plane is a solution of the PDE, and the characteristics are horizon-
tal slices of the corresponding solution surface. Of course, in general (for
nonzero c) characteristics are not horizontal.
Now, in principle we should be able to solve the characteristic ODEs
and obtain the characteristics.
How does this help us solve the original
PDE (7.10)? Without additional speciﬁcations, there are too many charac-
teristics to identify a desired solution surface v = v(x1, x2); indeed, there is
a characteristic passing through every point in the (x1, x2, v)-space, so they
ﬁll the entire space. We know that for ODEs, to ﬁx a particular solution
we must specify a point through which it passes (an initial condition). The
counterpart of this for the PDE (7.10) is that we must specify a suitable
curve in the (x1, x2, v)-space through which the solution surface passes; we
then obtain what is called a Cauchy (initial value) problem. Geometrically,
the idea is that characteristics that pass through points on this initial curve
will determine the solution surface of the PDE, as illustrated in Figure 7.2
(with the thick curve representing an initial curve).
x1
v
x2
Figure 7.2: Characteristics for an initial value problem

210
CHAPTER 7
An initial curve can be deﬁned in parametric form as
x1 = x1(r),
x2 = x2(r),
v = v(r)
where r ∈R. Then, the desired characteristics—i.e., the solutions of the
characteristic equations (7.11) whose initial conditions lie on this initial
curve—are given by
x1 = x1(r, s),
x2 = x2(r, s),
v = v(r, s)
(7.12)
which yields a description of the solution surface of our PDE (7.10). How-
ever, we are looking for a diﬀerent representation of this surface, namely,
v = v(x1, x2). To bring (7.12) to the desired form, we need to express r, s
as functions of x1, x2 and plug them into v(r, s). For this to be possible,
the map (r, s) 7→(x1(r, s), x2(r, s)) must be invertible. It follows from the
Inverse Function Theorem that invertibility of this map, at least in some
neighborhood of the initial curve, is in turn guaranteed if the corresponding
Jacobian matrix
(x1)r
(x1)s
(x2)r
(x2)s

is nonsingular along the initial curve. What is the geometric signiﬁcance
of this latter condition?
The columns of the above Jacobian matrix are
the (x1, x2)-components of the tangent vectors to the initial curve and to
a characteristic, respectively, and they must be linearly independent.
In
other words, when projected onto the (x1, x2)-plane, the characteristics and
the initial curve must be transversal to each other. We conclude that un-
der this transversality condition, the Cauchy problem for (7.10) is (locally)
uniquely solvable, at least in principle, via the method of characteristics. To
calculate the actual value of v(x1, x2) for some speciﬁc x1 and x2, we need
to ﬁnd a characteristic that connects the initial curve to a point with these
(x1, x2)-coordinates and then integrate the last equation in (7.11) along that
characteristic segment. We will discuss this procedure in more detail later
for the case of the HJB equation.
Characteristics can also be deﬁned for the general PDE (7.9) which for
n = 2 takes the form
F(x1, x2, v, vx1, vx2) = 0.
(7.13)
In the case of the quasi-linear PDE (7.10), we saw that the tangent vec-
tor (a, b, c)T to the solution surface does not depend on the partial deriva-
tives vx1 and vx2 (which give the ﬁrst two components of the normal vec-
tor (vx1, vx2, −1)T to this surface); consequently, the characteristic equa-
tions (7.11) involve x1, x2, and v only. For the more general PDE (7.13),
this is no longer guaranteed and so ﬁve diﬀerential equations—describing
the joint evolution of the variables x1, x2, v, vx1, vx2—are needed instead of

ADVANCED TOPICS
211
three. The solution curves of these equations are called the characteristic
strips. To simplify the notation, let us introduce the symbols
ξ1 := vx1,
ξ2 := vx2.
Then the characteristic strips are deﬁned by the following equations:
dx1
ds = Fξ1,
dx2
ds = Fξ2,
dv
ds = ξ1Fξ1 + ξ2Fξ2,
dξ1
ds = −Fx1 −ξ1Fv,
dξ2
ds = −Fx2 −ξ2Fv.
(7.14)
We do not give a complete derivation of (7.14) but note that the ﬁrst three
equations in (7.14) reduce to the earlier characteristic equations (7.11) in the
case of the quasi-linear PDE (7.10), whereas to arrive at the fourth equation
in (7.14) it is enough to write dξ1
ds = (ξ1)x1
dx1
ds + (ξ1)x2
dx2
ds , use the ﬁrst two
characteristic equations to rewrite this result as (ξ1)x1Fξ1 + (ξ1)x2Fξ2, and
then apply the identity Fx1+Fvξ1+Fξ1(ξ1)x1+Fξ2(ξ2)x1 = 0 which follows by
diﬀerentiating the PDE (7.13) with respect to x1; the last equation in (7.14)
is derived in the same manner.
The last two equations in (7.14), which
describe the evolution of the normal vector to the solution surface of the
PDE, will play an important role in the next subsection where we turn to
the HJB equation.
In the above discussion we assumed that the independent variable x lives
in the plane (i.e., n = 2). However, it is straightforward to write down the
equations of the characteristic strips for a general dimension n ≥2: we
must simply replace the indices 1, 2 in (7.14) by i ∈{1, . . . , n}. In the next
subsection we will work in this more general setting.
7.2.2
Canonical equations as characteristics of the HJB equation
Our goal now is to show that Hamilton’s canonical equations arise as the
characteristics—or, more precisely, characteristic strips—of the HJB equa-
tion. With some abuse of notation, let us write the HJB equation as
Vt(t, x) −H(t, x, −Vx(t, x)) = 0.
(7.15)
Note that we did not include the control u as an argument in H; instead,
we assume that the control that maximizes the Hamiltonian has already
been plugged in. (This formulation also covers the calculus of variations
and mechanics settings, where the velocity variable is eliminated via the
Legendre transformation.)
We know that we can bring the PDE (7.15)
to the form (7.9) by introducing the extra state variable xn+1 := t (cf.
Section 5.3.3). We can thus write down the characteristic equations (7.14)
for this particular PDE. Since the time t is present in (7.15), we can use it as

212
CHAPTER 7
the independent variable and write ˙x1, etc. We also deﬁne the costates pi :=
−ξi = −vxi, which is consistent with our earlier convention (see Section 5.2).
Then the equations (7.14) immediately yield
˙xi = Hpi,
i = 1, . . . , n
(plus one additional equation ˙xn+1 = 1 which is redundant) as well as
˙pi = −Hxi,
i = 1, . . . , n + 1
where we used the fact that H does not depend on V (hence the term Fv
in (7.14) vanishes). Thus we have indeed recovered the familiar canonical
diﬀerential equations.
Finally, there is one more equation in (7.14) which describes the evolution
of v. Here it tells us that the value function satisﬁes
˙V =
n
X
i=1
ξi ˙xi + ξn+1 = H −
n
X
i=1
piHpi
(7.16)
where we applied the HJB equation (7.15) one more time to arrive at the
second equality. (Actually, it is also easy to obtain this result directly by
writing ˙V = Vt + ⟨Vx, ˙x⟩.) It is the equation (7.16) that enables a solution
of the HJB equation via the method of characteristics. We know that the
Cauchy problem for the HJB equation is given by V (t1, x) = K(x), where
K is the terminal cost and t1 is the terminal time (this initial value problem
involves ﬂowing backward in time). To calculate the value V (t, x) for some
speciﬁc time t and state x, we must ﬁrst ﬁnd a point ¯x such that x = x(t)
where (x(·), p(·)) is the solution of the system of canonical equations with the
boundary conditions x(t1) = ¯x, p(t1) = −Kx(¯x). Then, V (t, x) is computed
by integration along the corresponding characteristic:
V (t, x(t)) = K(¯x) −
Z t1
t

H(s, x(s), p(s)) −
n
X
i=1
pi(s)Hpi(s, x(s), p(s))

ds.
Again, the fact that H does not depend on V is helpful here. Figure 7.3
illustrates the procedure.
7.3
RICCATI EQUATIONS AND INEQUALITIES IN ROBUST
CONTROL
This section builds on Chapter 6, with the goal of demonstrating that the
tools and results of that chapter—in particular, optimality conditions ex-
pressed in terms of algebraic Riccati equations—continue to play a central
role in the context of more general control problems. The ﬁrst problem stud-
ied here is a special case of the inﬁnite-horizon LQR problem except that

ADVANCED TOPICS
213
x
V
t
t1
(t1, ¯x)
(t, x)
V
=
K
(x)
Figure 7.3: Solving the HJB equation with the help of characteristics
the matrix Q in the cost functional is no longer positive semideﬁnite, which
leads to its interpretation as the problem of characterizing the disturbance-
to-output L2 gain.
Afterwards, we take a brief look at the H∞control
problem, which has been an important benchmark problem in robust con-
trol theory.
7.3.1
L2 gain
We consider a linear time-invariant system
˙x = Ax + Bu,
y = Cx
(7.17)
and the cost functional
J(u) =
Z ∞
t0

γuT (t)u(t) −1
γ yT (t)y(t)

dt
(7.18)
with γ > 0. In this cost, the squared norms of the input and the output
inside the integral are multiplied by scalar weights of the opposite signs (and
we assume the two weights to have been normalized so that their product
equals −1).
Consequently, it is clear that the optimal cost will now be
nonpositive (just set u ≡0). Nevertheless, as we will see, the form of the
optimal solution is very similar to the one we saw in Section 6.2 and can be
established using similar calculations. Suppose that there exists a matrix P
with the following three properties:
1) P = P T ≥0.

214
CHAPTER 7
2) P is a solution of the ARE
PA + AT P + 1
γ CT C + 1
γ PBBT P = 0.
(7.19)
3) The matrix A + 1
γ BBT P is Hurwitz.2
Then, we claim that the optimal cost is
V (x0) = −xT
0 Px0
(7.20)
and the optimal control is the linear state feedback
u∗(t) = 1
γ BT Px∗(t).
(7.21)
(Notice the “wrong” signs in the formulas (7.19)–(7.21) compared to Sec-
tion 6.2; this sign diﬀerence could be reconciled by working with −P instead
of P here.)
To prove this claim, let us deﬁne the function bV (x) := xT Px.
Its
derivative along solutions of the system (7.17) is
d
dt bV (x(t)) = xT (t)(PA +
AT P)x(t) + 2xT (t)PBu(t), which is easily checked to be equivalent to
d
dt
bV (x(t)) = xT (t)

PA + AT P + 1
γ CT C + 1
γ PBBT P

x(t)
(7.22)
−γ

u(t) −1
γ BT Px(t)
T 
u(t) −1
γ BT Px(t)

+ γuT (t)u(t) −1
γ yT (t)y(t).
We now introduce the auxiliary ﬁnite-horizon cost
¯Jt1(u) :=
Z t1
t0

γuT (t)u(t) −1
γ yT (t)y(t)

dt −xT (t1)Px(t1).
(7.23)
Using the formula (7.22) and noting that the ﬁrst term on its right-hand
side vanishes by (7.19), we can rewrite this cost as
¯Jt1(u) =
Z t1
t0
γ
u(t) −1
γ BT Px(t)

2
dt + xT (t)Px(t)
t1
t0 −xT (t1)Px(t1)
=
Z t1
t0
γ
u(t) −1
γ BT Px(t)

2
dt −xT
0 Px0
which makes it clear that (7.20) and (7.21) are the optimal cost and optimal
control for the cost functional (7.23). We want to show that they are also
2A matrix is called Hurwitz if all its eigenvalues have negative real parts.

ADVANCED TOPICS
215
optimal for the original cost functional (7.18). To this end, we ﬁrst note
that since P ≥0, the following bound holds for all u:
J(u) = lim
t1→∞
Z t1
t0

γuT (t)u(t) −1
γ yT (t)y(t)

dt
≥lim
t1→∞
Z t1
t0

γuT (t)u(t) −1
γ yT (t)y(t)

dt −xT (t1)Px(t1)

= lim
t1→∞Jt1(u) ≥lim
t1→∞(−xT
0 Px0) = −xT
0 Px0.
On the other hand, having already established that ¯Jt1(u∗) equals
Z t1
t0

γ(u∗)T (t)u∗(t) −1
γ (y∗)T (t)y∗(t)

dt −(x∗)T (t1)Px∗(t1) = −xT
0 Px0
where y∗:= Cx∗, we can pass to the limit as t1 →∞in this relation. In view
of the fact that A + 1
γ BBT P is a Hurwitz matrix, the closed-loop system
˙x∗= Ax∗+Bu∗= (A+ 1
γ BBT P)x∗is exponentially stable. We thus obtain
J(u∗) = −xT
0 Px0, and the desired result is proved.
While the formulas appearing here and in Section 6.2 are similar, the
meanings of the two problems are very diﬀerent. The cost (7.18) no longer
reﬂects the objective of keeping both y and u small. Instead, this cost is
small when y is large relative to u. We can regard u here not as a control
that regulates the output but as a disturbance that tries to make the output
large, with the optimal input being in some sense the worst-case disturbance.
Let us try to formulate this idea in more precise terms. The fact that (7.20)
is the optimal cost for the functional (7.18) implies that the inequality
−xT
0 Px0 ≤
Z ∞
t0

γ|u(t)|2 −1
γ |y(t)|2
dt
(7.24)
holds for all u, with the equality achieved by the optimal control u∗. From
now on we focus on the case when x0 = 0. Specializing (7.24) to this case,
after simple manipulations we reach
sup
u∈L2\{0}
qR ∞
t0 |y(t)|2dt
qR ∞
t0 |u(t)|2dt
≤γ.
(7.25)
The fraction on the left-hand side of (7.25) is the ratio of the L2 norms3 of
the input and the output, and the supremum is being taken over all nonzero
inputs with ﬁnite L2 norms. If we view the system (7.17), with the zero
initial condition, as an input/output operator from L2 to L2, then (7.25)
3See the deﬁnition (1.31) on page 19.

216
CHAPTER 7
says that the induced norm of this operator does not exceed γ. This induced
norm is called the L2 gain of the system.
We see that if, for a given value of γ, we can ﬁnd a matrix P with the
three properties listed at the beginning of this subsection, then the system’s
L2 gain is less than or equal to γ. (We do not know whether γ is actually
achieved by some control; note that the optimal control (7.21) is excluded
in (7.25) because it is identically 0 when x0 = 0.) A converse result also
holds: if the L2 gain is less than γ, then a matrix P with the indicated
properties exists. If we sidestep the original optimal control problem and
only seek suﬃcient conditions for the L2 gain to be less than or equal to γ,
then it is not hard to see from our earlier derivation that the conditions on
the matrix P can be relaxed. Namely, it is enough to look for a symmetric
positive semideﬁnite solution of the algebraic Riccati inequality
PA + AT P + 1
γ CT C + 1
γ PBBT P ≤0.
(7.26)
The formula (7.22) then yields
d
dt
bV (x(t)) ≤γ|u(t)|2 −1
γ |y(t)|2.
Integrating both sides from t0 to an arbitrary time T, rearranging terms,
and using the deﬁnition of bV and the fact that P ≥0, we have
1
γ
Z T
t0
|y(t)|2dt ≤γ
Z T
t0
|u(t)|2dt + bV (0) −bV (x(t)) ≤γ
Z T
t0
|u(t)|2dt
and in the limit as T →∞we again arrive at (7.25).
In the frequency domain, the system (7.17) is characterized by the trans-
fer matrix G(s) = C(Is−A)−1B. Using Parseval’s theorem, it can be shown
that the L2 gain equals the largest singular value of G(jω) supremized over
all frequencies ω ∈R; for systems with scalar inputs and outputs, this is
just supω∈R |g(jω)| where g is the transfer function. In view of this fact, the
L2 gain is also called the H∞norm.
7.3.2
H∞control problem
We saw in the previous subsection how u can play the role of a disturbance
input, in contrast with the standard LQR setting of Chapter 6 where, as in
the rest of this book, it plays the role of a control input. Now, let us make
the situation more interesting by allowing both types of inputs to be present
in the system, the task of the control being to stabilize the system and
attenuate the unknown disturbance in some sense. Such control problems
fall into the general framework of robust control theory, which deals with

ADVANCED TOPICS
217
control design methods for providing a desired behavior in the presence of
uncertainty. (We can also think of the control and the disturbance as two
opposing players in a diﬀerential game.) In the speciﬁc problem considered
in this subsection, the level of disturbance attenuation will be measured by
the L2 gain (or H∞norm) of the closed-loop system.
The H∞control problem concerns itself with a system of the form
˙x = Ax + Bu + Dw,
y = Cx,
z = Ex
where u is the control input, w is the disturbance input, y is the measured
output (the quantity available for feedback), and z is the controlled output
(the quantity to be regulated); for simplicity, we assume here that there are
no feedthrough terms from u and w to y and z. The corresponding feedback
control diagram is shown in Figure 7.4. We will ﬁrst consider the simpler
case of state feedback, obtained by setting C := I so that y = x. In this
case, we also restrict our attention to controllers that take the static linear
state feedback form u = Kx.
u
w
z
y
System
Controller
Figure 7.4: H∞control problem setting
The control objective is to stabilize the internal dynamics and attenuate
w in the H∞sense. More speciﬁcally, we want to design the feedback gain
matrix K so that the following two properties hold:
1) The closed-loop system matrix Acl := A + BK is Hurwitz.
2) The L2 gain of the closed-loop system from w to z (or, what is the
same, the H∞norm of the closed-loop transfer matrix G(s) = E(Is −
Acl)−1D) does not exceed a prespeciﬁed value γ > 0.
Note that this is not an optimal control problem because we are not asking
to minimize the gain γ (although ideally of course we would like γ to be
as small as possible). Controls solving problems of this kind are known as
suboptimal.

218
CHAPTER 7
It follows from the results of the previous subsection—applied with the
change of notation from A, B, C, u, y to Acl, D, E, w, z, respectively—that to
have property 2 (the bound on the L2 gain) it is suﬃcient to ﬁnd a positive
semideﬁnite solution of the Riccati inequality
PAcl + AT
clP + 1
γ ET E + 1
γ PDDT P ≤0.
(7.27)
To also guarantee property 1 (internal stability of the closed-loop system)
requires slightly stronger conditions: P needs to be positive deﬁnite and the
inequality in (7.27) needs to be strict. The latter condition can be encoded
via the Riccati equation
PAcl + AT
clP + 1
γ ET E + 1
γ PDDT P + εQ = 0
(7.28)
where Q = QT > 0 and ε > 0. (This implies PAcl + AT
clP < 0 which is
the well-known Lyapunov condition for Acl to be a Hurwitz matrix.) Next,
we need to convert (7.28) into a condition that is veriﬁable in terms of
the original open-loop system data. Introducing a matrix R = RT > 0 as
another design parameter (in addition to Q and ε), suppose that there exists
a solution P > 0 to the Riccati equation
PA + AT P + 1
γ ET E + 1
γ PDDT P −1
εPBR−1BT P + εQ = 0.
(7.29)
Then, if we let K := −1
2εR−1BT P, a straightforward calculation shows
that the feedback law u = Kx enforces (7.28) and thus achieves both of
our control objectives. Conversely, it can be shown that if the system is
stabilizable with an L2 gain less than γ, then (7.29) is solvable for P > 0.
The general case—when the full state is not measured and the controller
is a dynamic output feedback—is more interesting and more complicated.
Without going into details, we mention that a complete solution to this
problem, in the form of necessary and suﬃcient conditions for the existence
of a controller achieving an L2 gain less than γ, is available and consists of
the following ingredients:
1) Finding a solution P1 of a Riccati equation from the state feedback
case.
2) Finding a solution P2 of another Riccati equation obtained from the
ﬁrst one by the substitutions B →C and D ↔E.
3) Checking that the largest singular value of the product P1P2 is less
than γ2.
These elegant conditions in terms of two coupled Riccati equations yield a
controller that can be interpreted as a state feedback law combined with an
estimator (observer).

ADVANCED TOPICS
219
7.3.3
Riccati inequalities and LMIs
As we have seen, a representative example of Riccati matrix inequalities
encountered in robust control is
PA + AT P + CT C + PBBT P ≤0
(7.30)
(this is (7.26) with γ = 1). Similar Riccati inequalities arise in the context
of several other control problems. The inequality (7.30) is quadratic in the
matrix variable P. However, it can be shown to be equivalent to the linear
matrix inequality (LMI)
PA + AT P + CT C
PB
BT P
−I

≤0.
(7.31)
The matrix on the left-hand side of (7.31) is called the Schur complement
of the matrix on the left-hand side of (7.30). The constraint P ≥0 can also
be naturally incorporated into this LMI.
Being convex feasibility problems, LMIs can be eﬃciently solved by
known numerical algorithms from convex optimization. Several dedicated
software packages exist for solving LMIs. This makes conditions expressed
in terms of LMIs attractive from the computational point of view.
7.4
MAXIMUM PRINCIPLE FOR HYBRID CONTROL SYSTEMS
Hybrid systems are systems whose dynamics involve a combination of con-
tinuous evolution and discrete transitions. More speciﬁcally, in this section
we consider hybrid systems described by a ﬁnite collection of control systems
and a ﬁnite sequence of times (called switching times) which partition the
time interval into subintervals. On each subinterval, the state of the system
ﬂows in accordance with one of the systems from a given collection; at a
switching time, the state experiences an instantaneous jump, and another
system from the collection is selected for the next subinterval. While provid-
ing a much richer modeling framework than the continuous control systems
considered elsewhere in this book, hybrid systems violate the assumptions
under which we developed the maximum principle in Chapter 4. In this
section, we discuss a suitably extended version of the maximum principle
which applies to hybrid systems.
7.4.1
Hybrid optimal control problem
We begin by deﬁning the class of hybrid control systems and associated
optimal control problems that we want to study. The ﬁrst ingredient of our

220
CHAPTER 7
hybrid control system is a collection of (time-invariant) control systems
˙x = fq(x, u),
q ∈Q
(7.32)
where Q is a ﬁnite index set; for simplicity, we assume that all the systems in
the collection (7.32) share the same state space Rn and control set U ⊂Rm.
The second ingredient is a collection of switching sets Sq,q′ ⊂R2n, one for
each pair (q, q′) ∈Q × Q. A function x : [t0, tf] →Rn is an admissible
trajectory of our hybrid system corresponding to a control u : [t0, tf] →U
if there exist time instants
t0 < t1 < · · · < tk < tk+1 := tf
and indices q0, q1, . . . , qk ∈Q such that x(·) satisﬁes
˙x(t) = fqi(x(t), u(t))
∀t ∈(ti, ti+1), i = 0, 1, . . . , k
(7.33)
and
x(t−
i )
x(t+
i )

∈Sqi−1,qi,
i = 1, . . . , k.
Here x(t−
i ) and x(t+
i ) are the values of x right before and right after ti,
respectively, and the value x(ti) is taken to be equal to one of these one-sided
limits, depending on the desired convention. At each possible discontinuity
ti, a discrete transition (or switching event) is said to occur. The function q :
[t0, tf] →Q deﬁned by q(t) := qi for t ∈[ti, ti+1) describes the evolution of q
along the trajectory; q is often called the discrete state of the hybrid system.
We can use it to rewrite (7.33) more concisely as ˙x(t) = fq(t)(x(t), u(t)) for
t ̸= ti, i = 1, . . . , k.
We consider cost functionals of the form
J(u, {ti}, {qi}) :=
k
X
i=0
Z ti+1
ti
Lqi(x(t), u(t))dt +
k
X
i=1
Φqi−1,qi(x(t−
i ), x(t+
i ))
(7.34)
where Lq : Rn ×U →R is the usual running cost and Φq,q′ : Rn ×Rn →R is
the switching cost, for q, q′ ∈Q. For simplicity, we do not include a terminal
cost (this is no loss of generality, as terminal cost can be easily incorporated
into the above running-plus-switching cost along the lines of Section 3.3.2).
We also introduce an endpoint constraint, characterized by a set Eq,q′ ⊂R2n
for each pair (q, q′) ∈Q × Q, according to which the trajectory x(·) must
satisfy
x(t0)
x(tf)

∈Eq0,qk.
(7.35)
(Here Eq0,qk plays the same role as S2 at the end of Section 4.3.1.) Then, the
hybrid optimal control problem consists in ﬁnding a control that minimizes

ADVANCED TOPICS
221
the cost (7.34) subject to the endpoint constraint (7.35).
We emphasize
that the choice of a control u is accompanied by the choice of two ﬁnite
sequences {ti} and {qi}, to which we henceforth refer as the time sequence
and switching sequence, respectively.
7.4.2
Hybrid maximum principle
The maximum principle that we are about to state provides necessary con-
ditions for a trajectory x∗(·) of the hybrid control system corresponding to
a control u∗(·), a time sequence {ti}, and a switching sequence {qi} to be
locally optimal over trajectories x(·) with the same switching sequence {qi}
and such that x is close to x∗on each subinterval (ti, ti+1). Most of the
statements of this hybrid maximum principle are more or less familiar to us
from Chapter 4. We proceed with the understanding that suitable technical
assumptions are in place so that all derivatives, tangent spaces, and other
objects appearing below are well deﬁned.
Deﬁne the family of Hamiltonians
Hq(x, u, p, p0) := ⟨p, fq(x, u)⟩+ p0Lq(x, u),
q ∈Q.
The abnormal multiplier must satisfy p∗
0 ≤0 as usual. The costate p∗(·) is
allowed to be discontinuous at the switching times ti of x∗, while between
these times it must satisfy the adjoint equation
˙p∗(t) = −(Hqi)x

∗(t)
∀t ∈(ti, ti+1), i = 0, 1, . . . , k.
The transversality condition says that the vector
 p∗(t0)
−p∗(tf)

must be orthog-
onal to the tangent space to the endpoint constraint set Eq0,qk at
x∗(t0)
x∗(tf)

,
which we write as
 p∗(t0)
−p∗(tf)

⊥Tx∗(t0)
x∗(tf)
Eq0,qk.
(7.36)
At the switching times, there are also switching conditions saying that for
i = 1, . . . , k we must have
−p∗(t−
i )
p∗(t+
i )

+ p∗
0∇Φqi−1,qi(x∗(t−
i ), x∗(t+
i )) ⊥Tx∗(t−
i )
x∗(t+
i )
Sqi−1,qi.
(7.37)
The nontriviality condition says that either p∗
0 ̸= 0 or p∗̸≡0. The Hamil-
tonian maximization condition must hold in the usual sense for each Hqi
on the corresponding interval (ti, ti+1). Moreover, the Hamiltonian remains
constant along the optimal trajectory (in particular, its value is not aﬀected

222
CHAPTER 7
by the switching events). Finally, for free-time problems the Hamiltonian
is 0.
Note that the transversality condition (7.36) is completely analogous to
the transversality condition (4.46) for the case of initial sets discussed at
the end of Section 4.3.1. As for the switching conditions (7.37), the intu-
ition behind them is similar and can be understood as follows. Consider
the continuous portions of x∗which correspond to the subintervals (ti, ti+1),
i = 0, . . . , k. Reparameterize the time individually for each of them so that
their domains are all mapped onto the same interval, say, [s0, sf].
This
allows us to “stack” them all together, i.e., treat them as if they evolve
simultaneously. Then, the transversality condition (7.36) and the switch-
ing conditions (7.37) become one aggregate transversality condition induced
by the endpoint constraint and the switching sets. The appearance of the
gradient of the switching cost in this transversality condition is also not sur-
prising because the switching cost becomes a combination of terminal and
initial cost (see Section 4.3.1 for a discussion of transversality conditions for
problems with terminal cost).
7.4.3
Example: light reﬂection
To illustrate the hybrid maximum principle, we apply it to the familiar light
reﬂection example from Section 2.1.2. We model the propagation of a light
ray through the n-dimensional space via the control system
˙x = c(x)u,
|u| = 1
(7.38)
where x ∈Rn, c : Rn →(0, ∞) is a C1 function that determines the (varying)
speed of light, and u taking values on the unit sphere in Rn deﬁnes the
direction of motion. We assume that the reﬂecting surface is a hyperplane,
and without loss of generality we take it to be S := {x : xn = 0}. The initial
point and the ﬁnal point are assumed to lie in the same open half-space
relative to S.
We seek to derive a necessary condition for a trajectory x∗that starts at
a given initial point x0 at time t0, gets reﬂected oﬀS at some time t1, and
arrives at a given ﬁnal point xf at time tf to be locally time-optimal with
respect to trajectories that hit S at nearby points. This optimal control
problem is not inherently hybrid, since (7.38) is a standard control system
and it is capable of producing reﬂected trajectories. However, the classical
formulation of the maximum principle does not allow us to incorporate the
fact that we are only interested in trajectories that hit S along the way.
With the hybrid formulation, it is easy to do so by considering a hybrid sys-
tem with a single discrete state location q (i.e., Q = {q}) and the switching
set Sq,q :=
n x
x′

: x = x′ ∈S
o
. In this hybrid system, discrete transitions

ADVANCED TOPICS
223
occur when the trajectory hits S, but the underlying control system (7.38)
does not change and the trajectory remains continuous. The switching se-
quence associated with our candidate optimal trajectory x∗is {q, q}, and the
hybrid maximum principle captures local optimality over nearby trajectories
with the same switching sequence—which is precisely what we want.
Applying the hybrid maximum principle to this problem entails just a
few straightforward computations. The Hamiltonian is H = ⟨p, c(x)u⟩+ p0.
The Hamiltonian maximization condition gives u∗= p∗/|p∗| and H|∗=
c(x∗)|p∗| + p∗
0.
Since the ﬁnal time is free, we have H|∗≡0 which in
view of the nontriviality condition implies that p∗
0 ̸= 0 and p∗(t) ̸= 0 for
all t. Normalizing them so that p∗
0 = −1, we obtain c(x∗)|p∗| ≡1 hence
u∗= p∗c(x∗). Both the costate p∗and the optimal control u∗are continuous
except possibly at the switching time t1. Since there is no switching cost, the
switching condition tells us that the vector
−p∗(t−
1 )
p∗(t+
1 )

must be orthogonal
to the tangent space to Sq,q at
x∗(t1)
x∗(t1)

. This tangent space is Sq,q itself,
and vectors in it have the form (x1, . . . , xn−1, 0, x1, . . . , xn−1, 0)T . It follows
that p∗
i (t+
1 ) = p∗
i (t−
1 ) for i = 1, . . . , n −1; in other words, p∗
1, . . . , p∗
n−1 are
continuous at t1, hence so are u∗
1, . . . , u∗
n−1. Only the last component of
u∗can be discontinuous at t1. But since |u∗| =
p
(u∗
1)2 + · · · + (u∗n)2 is to
remain equal to 1, it must be that u∗
n(t+
1 ) = ±u∗
n(t−
1 ), i.e., u∗
n either stays
continuous or ﬂips its sign at t1. Of these two options, only the latter is
possible because the light ray cannot pass through S. We conclude that the
velocity vectors before and after the reﬂection diﬀer only in the component
orthogonal to the reﬂection surface, and the diﬀerence is only in the minus
sign. We have of course recovered the well-known law of reﬂection.
7.5
NOTES AND REFERENCES FOR CHAPTER 7
Our primary reference on manifolds and some related mathematical facts
was [Arn89] (see in particular Sections 18, 34, and 37 of that book). Most
of this material is also presented in [Jur96] where the maximum principle
on manifolds is discussed too. Two other good sources of basic information
about manifolds, on which we occasionally relied in Section 7.1, are [Arn92,
Chapter 5] and [Isi95, Appendix A]; for further reading on this subject we
recommend [Boo03] or [War83]. An in-depth treatment of the maximum
principle on manifolds can be found in [Sus97] as well as in [AS04, Chap-
ter 12]; see also [Cha11].
A general source of information on PDEs and the method of characteris-
tics is [Gar86]; in our presentation in Section 7.2 we drew upon Chapter 2 of
that book (which also has a section on the Hamilton-Jacobi theory). Refer-
ences dealing speciﬁcally with the connection between the HJB equation and

224
CHAPTER 7
the canonical equations via characteristics (without going into the details of
the general PDE theory) are [YZ99, Chapter 5] and [BP07, Chapter 7].
All the ingredients of our treatment of the L2 gain problem are contained
in [Bro70, Sections 23 and 25] and [Kha02, Section 5.3]. Our derivation of
the state feedback H∞controller follows the paper [Pet87], whereas the
general output feedback H∞controller is presented in the paper [DGKF89]
as well as the book [ZDG96].
A good reference on LMIs, methods for
solving them, and their role in system and control theory is [BGFB94],
while [HJ85] supplies relevant technical details on Schur complements. Al-
though we only considered linear systems for simplicity, most of the concepts
described in Section 7.3 can be as naturally developed for nonlinear systems
(with Hamilton-Jacobi partial diﬀerential inequalities replacing Riccati ma-
trix inequalities); besides the already mentioned text [Kha02], this topic is
discussed in much greater detail in [vdS96].
Section 7.4 is based on the paper [Sus99]. Related work is reported in
the papers [GP05] and [DK08], the latter of which contains the “stacking”
argument to which we alluded at the end of Section 7.4.2. A general reference
on hybrid systems is [vdSS00]. Optimal control of hybrid systems is an active
research area; see, e.g., [BWEV05] and the references therein.

Bibliography
[AF66]
M. Athans and P. L. Falb. Optimal Control. McGraw Hill, New
York, 1966. Reprinted by Dover in 2006.
[AM90]
B.D.O. Anderson and J. B. Moore. Optimal Control: Linear
Quadratic Methods. Prentice Hall, New Jersey, 1990. Reprinted
by Dover in 2007.
[Arn89]
V. I. Arnold.
Mathematical Methods of Classical Mechanics.
Springer, New York, 2nd edition, 1989.
[Arn92]
V. I. Arnold. Ordinary Diﬀerential Equations. Springer, Berlin,
3rd edition, 1992.
[AS04]
A. A. Agrachev and Yu. L. Sachkov. Control Theory from the
Geometric Viewpoint. Springer, Berlin, 2004.
[BCD97]
M. Bardi and I. Capuzzo-Dolcetta.
Optimal Control and
Viscosity
Solutions
of
Hamilton-Jacobi-Bellman
Equations.
Birkh¨auser, Boston, 1997.
[Bel57]
R. Bellman. Dynamic Programming. Princeton University Press,
1957.
[Ber99]
D. P. Bertsekas.
Nonlinear Programming.
Athena Scientiﬁc,
Belmont, MA, 2nd edition, 1999.
[BGFB94]
S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan. Linear
Matrix Inequalities in System and Control Theory, volume 15
of SIAM Studies in Applied Mathematics. SIAM, Philadelphia,
1994.
[Bli30]
G. A. Bliss.
On the problem of Lagrange in the calculus of
variations. Amer. J. Math., 52:673–744, 1930.
[Blo03]
A. M. Bloch. Nonholonomic Mechanics and Control. Springer,
New York, 2003.
[BM91]
U. Brechtken-Manderscheid.
Introduction to the Calculus of
Variations. Chapman & Hall, London, 1991.
225

226
BIBLIOGRAPHY
[Bol78]
V. G. Boltyanskii. Optimal Control of Discrete Systems. Wiley,
New York, 1978.
[Boo03]
W. M. Boothby.
An Introduction to Diﬀerentiable Manifolds
and Riemannian Geometry. Academic Press, New York, revised
2nd edition, 2003.
[BP04]
U. Boscain and B. Piccoli. Optimal Syntheses for Control Sys-
tems on 2-D Manifolds. Springer, New York, 2004.
[BP07]
A. Bressan and B. Piccoli.
Introduction to the Mathematical
Theory of Control.
American Institute of Mathematical Sci-
ences, 2007.
[Bre85]
A. Bressan. A high order test for optimality of bang-bang con-
trols. SIAM J. Control Optim., 23:38–48, 1985.
[Bro70]
R. W. Brockett. Finite Dimensional Linear Systems. Wiley,
New York, 1970.
[Bry96]
A. E. Bryson Jr. Optimal control—1950 to 1985. IEEE Control
Systems Magazine, 16:26–33, 1996.
[BV04]
S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge
University Press, 2004.
[BWEV05] M. Boccadoro, Y. Wardi, M. Egerstedt, and E. Verriest. Opti-
mal control of switching surfaces in hybrid dynamical systems.
Discrete Event Dyn. Syst., 15:433–448, 2005.
[CEHS87]
G. S. Christensen, M. E. El-Hawary, and S. A. Soliman. Optimal
Control Applications in Electric Power Systems. Plenum Press,
New York, 1987.
[Ces83]
L. Cesari. Optimization—Theory and Applications. Springer,
New York, 1983.
[Cha11]
D. E. Chang. A simple proof of the Pontryagin maximum prin-
ciple on manifolds. Automatica, 47:630–633, 2011.
[CL83]
M. G. Crandall and P. L. Lions. Viscosity solutions of Hamilton-
Jacobi equations. Trans. Amer. Math. Soc., 277:1–42, 1983.
[Cla89]
F. H. Clarke. Methods of Dynamic and Nonsmooth Optimiza-
tion. SIAM, Philadelphia, 1989.
[Cla10]
C. W. Clark. Mathematical Bioeconomics: The Mathematics of
Conservation. Wiley, New York, 3rd edition, 2010.

BIBLIOGRAPHY
227
[DGKF89] J. C. Doyle, K. Glover, P. P. Khargonekar, and B. A. Francis.
State-space solutions to standard H2 and H∞control problems.
IEEE Trans. Automat. Control, 34:831–847, 1989.
[DK08]
A. V. Dmitruk and A. M. Kaganovich. The Hybrid Maximum
Principle is a consequence of Pontryagin Maximum Principle.
Systems Control Lett., 57:964–970, 2008.
[dlF00]
A. de la Fuente.
Mathematical Methods and Models for
Economists. Cambridge University Press, 2000.
[DM70]
P. Dyer and S. R. McReynolds. The Computation and Theory
of Optimal Control. Academic Press, New York, 1970.
[Fil88]
A. F. Filippov. Diﬀerential Equations with Discontinuous Right-
hand Sides. Kluwer, Dordrecht, 1988.
[FLS63]
R. P. Feynman, R. B. Leighton, and M. Sands. The Feynman
Lectures on Physics. Addison-Wesley, Reading, MA, 1963.
[Ful85]
A. T. Fuller. Minimization of various performance indices for a
system with bounded control. Int. J. Control, 41:1–37, 1985.
[Gar86]
P. R. Garabedian. Partial Diﬀerential Equations. Chelsea Pub.
Co., New York, 2nd edition, 1986.
[GF63]
I. M. Gelfand and S. V. Fomin. Calculus of Variations. Prentice
Hall, New Jersey, 1963. Reprinted by Dover in 2000.
[Gol80]
H. H. Goldstine. A History of the Calculus of Variations from
the 17th through the 19th Century. Springer, New York, 1980.
[GP05]
M. Garavello and B. Piccoli. Hybrid necessary principle. SIAM
J. Control Optim., 43:1867–1887, 2005.
[GS07]
R. Goebel and M. Subbotin. Continuous time linear quadratic
regulator with control constraints via convex duality.
IEEE
Trans. Automat. Control, 52:886–892, 2007.
[Hes09]
J. P. Hespanha. Linear Systems Theory. Princeton University
Press, 2009.
[HJ85]
R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge
University Press, 1985.
[Isi95]
A. Isidori.
Nonlinear Control Systems.
Springer, Berlin, 3rd
edition, 1995.
[Jur96]
V. Jurdjevic. Geometric Control Theory. Cambridge University
Press, 1996.

228
BIBLIOGRAPHY
[Kal60]
R. E. Kalman. Contributions to the theory of optimal control.
Bol. Soc. Mat. Mexicana, 5:102–119, 1960. Reprinted in Control
Theory: Twenty-Five Seminal Papers (1931–1981), T. Basar,
editor, IEEE Press, New York, 2001, pages 149–166.
[Kha02]
H. K. Khalil. Nonlinear Systems. Prentice Hall, New Jersey, 3rd
edition, 2002.
[Kno81]
G. Knowles. An Introduction to Applied Optimal Control. Aca-
demic Press, New York, 1981.
[Kre77]
A. J. Krener. The high order maximal principle and its appli-
cations to singular extremals. SIAM J. Control Optim., 15:256–
293, 1977.
[KS72]
H. Kwakernaak and R. Sivan. Linear Optimal Control Systems.
Wiley, New York, 1972.
[Lei81]
G. Leitmann. The Calculus of Variations and Optimal Control:
An Introduction. Plenum Press, New York, 1981.
[LL50]
M. A. Lavrentiev and L. A. Lusternik. A Course in the Calculus
of Variations. Moscow, 2nd edition, 1950. In Russian.
[LM67]
E. B. Lee and L. Markus. Foundations of Optimal Control The-
ory. Wiley, New York, 1967.
[LSW96]
Y. Lin, E. D. Sontag, and Y. Wang. A smooth converse Lya-
punov theorem for robust stability. SIAM J. Control Optim.,
34:124–160, 1996.
[Lue69]
D. G. Luenberger. Optimization by Vector Space Methods. Wi-
ley, New York, 1969.
[Lue84]
D. G. Luenberger.
Linear and Nonlinear Programming.
Addison-Wesley, Reading, MA, 2nd edition, 1984.
[Mac05]
C. R. MacCluer. Calculus of Variations. Prentice Hall, New
Jersey, 2005.
[McS39]
E. J. McShane. On multipliers for Lagrange problems. Amer.
J. Math., 61:809–819, 1939.
[MO98]
A. A. Milyutin and N. P. Osmolovskii. Calculus of Variations
and Optimal Control. American Mathematical Society, Provi-
dence, RI, 1998.
[Neu03]
M. G. Neubert. Marine reserves and optimal harvesting. Ecology
Letters, 6:843–849, 2003.

BIBLIOGRAPHY
229
[NRV84]
Z. Nahorski, H. F. Ravn, and R.V.V. Vidal. The discrete-time
maximum principle: a survey and some new results.
Int. J.
Control, 40:533–554, 1984.
[PB94]
H. J. Pesch and R. Bulirsch. The maximum principle, Bellman’s
equation, and Carath´eodory’s work. J. Optim. Theory Appl.,
80:199–225, 1994.
[PBGM62] L. S. Pontryagin, V. G. Boltyanskii, R. V. Gamkrelidze, and
E. F. Mishchenko. The Mathematical Theory of Optimal Pro-
cesses. Interscience, New York, 1962.
[Pet87]
I. R. Petersen. Disturbance attenuation and H∞optimization:
a design method based on the algebraic Riccati equation. IEEE
Trans. Automat. Control, 32:427–429, 1987.
[PS00]
B. Piccoli and H. J. Sussmann. Regular synthesis and suﬃciency
conditions for optimality. SIAM J. Control Optim., 39:359–410,
2000.
[Roc74]
R. T. Rockafellar. Conjugate Duality and Optimization. SIAM,
Philadelphia, 1974.
[Rud76]
W. Rudin. Principles of Mathematical Analysis. McGraw Hill,
New York, 3rd edition, 1976.
[RW00]
R. T. Rockafellar and P. R. Wolenski. Convexity in Hamilton–
Jacobi theory I: dynamics and duality. SIAM J. Control Optim.,
39:1323–1350, 2000.
[Rya87]
E. P. Ryan.
Feedback solution of a class of optimal bilinear
control problems. Int. J. Control, 45:1035–1041, 1987.
[Son98]
E. D. Sontag. Mathematical Control Theory: Deterministic Fi-
nite Dimensional Systems.
Springer, New York, 2nd edition,
1998.
[ST05]
S. P. Sethi and G. L. Thompson. Optimal Control Theory: Ap-
plications to Management Science and Economics.
Springer,
New York, 2nd edition, 2005.
[Sus79]
H. J. Sussmann.
A bang-bang theorem with bounds on the
number of switchings.
SIAM J. Control Optim., 17:629–651,
1979.
[Sus83]
H. J. Sussmann. Lie brackets, real analyticity and geometric
control.
In R. W. Brockett, R. S. Millman, and H. J. Suss-
mann, editors, Diﬀerential Geometric Control Theory, pages 1–
116. Birkh¨auser, Boston, 1983.

230
BIBLIOGRAPHY
[Sus97]
H. J. Sussmann. An introduction to the coordinate-free maxi-
mum principle. In B. Jakubczyk and W. Respondek, editors, Ge-
ometry of Feedback and Optimal Control, pages 463–557. Marcel
Dekker, New York, 1997.
[Sus99]
H. J. Sussmann. A maximum principle for hybrid optimal con-
trol problems. In Proc. 38th IEEE Conf. on Decision and Con-
trol, pages 425–430, 1999.
[Sus00]
H.
J.
Sussmann.
Handouts
for
the
course
taught
at
the Weizmann Institute of Science,
2000.
Available at
http://www.math.rutgers.edu/∼sussmann.
[Sus07]
H. J. Sussmann. Set separation, approximating multicones, and
the Lipschitz maximum principle.
J. Diﬀerential Equations,
243:448–488, 2007.
[Sut75]
W. A. Sutherland.
Introduction to Metric and Topological
Spaces. Oxford University Press, 1975.
[SW77]
A. P. Sage and C. C. White. Optimum Systems Control. Prentice
Hall, New Jersey, 2nd edition, 1977.
[SW97]
H. J. Sussmann and J. C. Willems. 300 years of optimal con-
trol: from the brachystochrone to the maximum principle. IEEE
Control Systems Magazine, 17:32–44, 1997.
[Swa84]
G. W. Swan.
Applications of Optimal Control Theory in
Biomedicine. Marcel Dekker, New York, 1984.
[vdS96]
A. van der Schaft. L2-Gain and Passivity Techniques in Non-
linear Control. Springer, London, 1996.
[vdSS00]
A. van der Schaft and H. Schumacher. An Introduction to Hybrid
Dynamical Systems. Springer, London, 2000.
[Vin00]
R. Vinter. Optimal Control. Birkh¨auser, Boston, 2000.
[War83]
F. W. Warner. Foundations of Diﬀerentiable Manifolds and Lie
Groups. Springer, New York, 1983.
[You80]
L. C. Young. Lectures on the Calculus of Variations and Optimal
Control Theory. Chelsea Pub. Co., New York, 2nd edition, 1980.
[YZ99]
J. Yong and X. Y. Zhou. Stochastic Controls: Hamiltonian Sys-
tems and HJB Equations. Springer, New York, 1999.
[ZDG96]
K. Zhou, J. C. Doyle, and K. Glover. Robust and Optimal Con-
trol. Prentice Hall, New Jersey, 1996.

Index
0-norm, 18, 33
1-norm, 18, 33
abnormal multiplier, 54, 103
absolutely continuous function, 85
accessory equation, see Jacobi equa-
tion
action integral, 49
adjoint system, 94, 121
adjoint vector, 94, 122, 170, see also
costate
economic interpretation, 170, 179
admissible perturbation, 20
aﬃne control system, 142
applications of optimal control, 2, 24,
154
arclength, 27
asymptotic stability, 195
augmented cost, 15, 53, 56
augmented system, 107
bang-bang control, 136, 139
bang-bang principle, 139, 141, 154
basic calculus of variations problem,
32
basic ﬁxed-endpoint control problem,
102
basic variable-endpoint control prob-
lem, 104
Bellman, 156, 168
Bernoulli, Johann, 29, 31
bilinear control system, 143
Bolza problem, 87
boundary conditions
correct number of, 39, 44, 54, 76,
105, 134
brachistochrone, 30, 42, 68, 82, 103
Brouwer’s ﬁxed point theorem, 120
Ck function, 18
canonical coordinates, 202
canonical equations, 45, 93, 207
as characteristics of HJB equa-
tion, 211
canonical variables, 45
Carath´eodory, 167, 178
catenary, 28, 55, 68, 72
Cauchy initial value problem, 209
Cauchy-Schwarz inequality, 67
characteristic, 208
characteristic strip, 211
chattering, see Fuller’s phenomenon
cheap control, 198
commutator, see Lie bracket
compactness, 9, 23
of reachable sets, 149, 154
conjugate point, 65
conservation law, 50
conservative force, 48
constrained optimization, 11
constraint
equality, 11, 104, 201
holonomic, 56
integral, 52
nonholonomic, 58, 70
non-integral, 55
contravariance, 204
control set, 1, 83
control system, 1, 83
aﬃne in controls, 142
bilinear, 143
linear, 138, 151, 180, 213
normal, 139
on a manifold, 203
controllability, 139, 154, 191, 198
convexity, 8, 10, 24, 25, 120, 150, 219
coordinate chart, 201
corner point, 71
cost functional, 1
augmented, 53, 56
Bolza form, 87
231

232
INDEX
cost functional (continued)
Lagrange form, 87
Mayer form, 87
costate, 94, see also adjoint vector
as covector, 204
cotangent bundle, 202
cotangent space, 202
cotangent vector, see covector
covariance, 205
covector, 202
critical point, 10
cycloid, 31
Dido’s isoperimetric problem, 26, 55
diﬀerentiable manifold, see manifold
diﬀerential game, 167, 217
diﬀerential of a function, 202, 204
disturbance attenuation, 216
dual space, 202
dynamic programming, 156
discrete, 156
E-function, see Weierstrass excess
function
economic interpretation of adjoint
vector, 170, 179
equality constraint, 11, 104, 201
Erdmann, 76
Euler, 39
Euler-Lagrange equation, 38
coordinate invariance, 41, 69
for multiple degrees of freedom,
40
integral form, 41
existence and uniqueness of solution,
84
existence of optimal solution, 39, 69,
148, 154
for linear time-optimal problems,
151
for Mayer problems, 151
for nonlinear time-optimal prob-
lems, 154
expensive control, 199
exponential stability, 195
extremal, 39
broken, 72
extremum, 4, 19
strong, 33
weak, 33
feasible direction, 8
Fermat’s principle, 16, 28
Filippov’s theorem, 149, 154
ﬁnal state, 1, 86
ﬁnal time, 1, 86
ﬁnite escape time, 63, 188
ﬁnite-dimensional optimization, 3
ﬁrst variation, 19
alternative deﬁnition, 21, 36
ﬁrst-order necessary condition, 5, 20
for constrained optimality, 14,
54, 55
for weak extremum, 38
ﬁxed-time, free-endpoint problem, 88
Fr´echet derivative, 21, 25
free-time, ﬁxed-endpoint problem, 88
Fuller’s phenomenon, 147
Fuller’s problem, 146, 154
function
absolutely continuous, 85
Ck, 18
locally Lipschitz, 85
measurable, 86
piecewise C1, 33
piecewise continuous, 84
functional, 1, 17
bilinear, 21
linear, 20
quadratic, 22
Gateaux derivative, 20, 25
global minimum, 4, 10, 19, 23
gradient, 5
H∞control problem, 217
H∞norm, 216
Hamilton, 44, 167
Hamilton’s canonical equations, see
canonical equations
Hamilton’s principle of least action,
see principle of least action
Hamilton-Jacobi-Bellman (HJB) equ-
ation, 162
and maximum principle, 168, 207
characteristics of, 211
for inﬁnite-horizon problems, 165
in calculus of variations, 168, 178
solving, 163, 179
Hamiltonian, 42, 45, 91, 103
as energy, 50

INDEX
233
classical vs. modern formulation,
48
Hamiltonian matrix, 182
Hamiltonian maximization condition,
46, 62, 81, 96, 103, 123, 163
Hessian matrix, 7
holonomic constraint, 56
Hurwitz matrix, 214
hybrid system, 219
discrete state, 220
hyperplane
separating, 120, 127
supporting, 140
inﬁnite-dimensional optimization, 17
inﬁnite-horizon problem, 88, 164, 189
integral constraint, 52
inverse function theorem, 14, 54
Isaacs, 167
Jacobi, 64, 167
Jacobi equation, 65
Jacobian matrix, 14
Kalman, 168, 199
L2 gain, 216
of a nonlinear system, 224
Lp norm, 19
Lagrange, 16, 32, 39, 55, 64
Lagrange multiplier, 14, 53, 94
distributed, 56
Lagrange problem, 87
Lagrangian, 32, see also running cost
augmented, 53, 55
diﬀerentiability assumptions, 40,
69
Legendre, 62, 64
Legendre transformation, 46, 69
Legendre’s condition, 62, 80
for multiple degrees of freedom,
62, 70
Legendre-Clebsch condition, 96
Lie bracket, 143
light
reﬂection, 16, 28, 222
refraction, 28, 31, 68
linear control system, 138, 151, 180,
213
linear matrix inequality (LMI), 219
linear quadratic regulator (LQR)
problem
ﬁnite-horizon, 180
inﬁnite-horizon, 189
linearization, 91, 113
Lipschitz condition, 84, 85
local coordinates, 201
canonical, 202
local minimum, 3, 19
locally Lipschitz function, 85
Lyapunov-like function, 193, 214
manifold, 200
embedded, 201
maximum, 4, 19
vs. minimum, 4, 32, 58, 97
maximum principle
and HJB equation, 168, 207
for basic ﬁxed-endpoint control
problem, 102
for basic variable-endpoint con-
trol problem, 104
for ﬁxed-time problems, 131
for Mayer problems, 132, 153
for problems with terminal cost,
132
for time-varying problems, 131
higher-order, 155
hybrid, 221
in discrete time, 155
local optimality, 129, 153
nonsmooth, 155
on manifolds, 206
sign convention, 97, 121
stochastic, 155
Mayer problem, 87
McShane, 79, 110
measurable function, 86
minimal time, see time-optimal prob-
lem
minimum
global, 4, 10, 19, 23
local, 3, 19
strict, 4, 19
vs. maximum, 4, 32, 58, 97
momentum, 42, 45
angular, 51
multiple degrees of freedom, 32

234
INDEX
necessary condition, 96, see also max-
imum principle
ﬁrst-order, 5, 20
for constrained optimality, 14,
54, 55
for weak extremum, 38
for strong extremum, 75
for strong maximum, 79
for strong minimum, 76
second-order, 7, 22, 62
for constrained optimality, 17
needle perturbation, 110
Newton’s second law, 48
Noether’s theorem, 51, 69
nonholonomic constraint, 58, 70
non-integral constraint, 55
nontriviality condition, 130
norm, 18
normal control system, 139
observability, 195, 198
optimal control, 1, 89
bang-bang, 136, 139
singular, 142
optimal control problem, 1, 83
Bolza form, 87
hybrid, 219
Lagrange form, 87
Mayer form, 87
on a manifold, 203
parking problem, 2, 135
partial diﬀerential equation (PDE),
162, 174, 208
quasi-linear, 208
pendulum, 57, 200
Perron’s paradox, 148
perturbation
admissible, 20
needle, 110
spatial, 110
temporal, 109
piecewise C1 function, 33
piecewise continuous function, 84
Pontryagin, 105, 110
principle of least action, 49, 67, 69
principle of optimality, 108, 158, 159,
167
quadratic form, 22
quasi-linear PDE, 208
reachable set, 140, 149
regular point, 11
Riccati equation
algebraic, 192
unique positive deﬁnite solu-
tion, 197
diﬀerential, 63, 68, 184
global existence of solution,
187
reduction to linear diﬀerential
equation, 64, 68, 184
solving, 189, 199
steady-state solution, 191
Riccati inequality, 216, 218
reduction to LMI, 219
robust control, 216
running cost, 1, 32, 86
Schur complement, 219
second variation, 22
alternative deﬁnition, 23
second-order necessary condition, 7,
22, 62
for constrained optimality, 17
second-order suﬃcient condition, 7,
22, 66
for constrained optimality, 17
separating hyperplane, 120, 127
shooting method, 101
single degree of freedom, 32
singular arc, 142
Snell’s law, 28, 31
spatial control perturbation, 110
stability, 194
state feedback, 137, 153, 158, 169,
183, 217
state transition matrix, 114
stationary point, 5
strict minimum, 4, 19
strong extremum, 33
sub-diﬀerential, 172
suboptimal control, 217
suﬃcient condition, 7, 22, 98, 165
for constrained optimality, 17
for strong minimum, 81, 101
for weak minimum, 66
super-diﬀerential, 172
supporting hyperplane, 140

INDEX
235
surface, 9, 104, 201
sweep method, 101
switching condition, 221
switching cost, 220
switching curve, 136, 147
switching function, 142
switching set, 220
switching time, 219
symplectic form, 206
tangent bundle, 202
tangent space, 11, 201
characterization of, 12, 24, 105
tangent vector, 11
target set, 88
temporal control perturbation, 109
terminal cone, 117
terminal cost, 1, 86
terminal state, 1, 86
terminal time, 1, 86
test function, 173
time-optimal problem, 2, 83, 134, 151,
154
transversality condition, 44, 69, 105,
126
for Cauchy problem, 210
for initial sets, 134, 222
unconstrained optimization, 4
completely, 4
value function, 159
as viscosity solution, 177
nondiﬀerentiable, 170
variable initial state, 134, 220
variable-endpoint problem, 104
in calculus of variations, 42, 69
variation
alternative deﬁnition, 21, 23
ﬁrst, 19
second, 22
variational equation, 66, 114
viscosity
physical interpretation, 176, 179
solution, 175
of HJB equation, 171, 177
subsolution, 175
supersolution, 175
weak extremum, 33
Weierstrass, 76, 79
Weierstrass excess function, 76
Weierstrass necessary condition, 76
Weierstrass theorem, 9, 23
Weierstrass-Erdmann corner condi-
tions, 75, 80
Zeno behavior, see Fuller’s phenome-
non

