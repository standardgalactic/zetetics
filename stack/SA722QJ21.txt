Solving Knapsack Problems using Radius Particle Swarm Optimization fuse with 
Simulated Annealing  
Mudarmeen Munlin 
Faculty of Engineering and Technology  
Mahanakorn University of Technology 
Bangkok, Thailand 
mmunlin @gmail.com 
 
ABSTRACT1 
 
We present a novel approach to fuse the Radius Particle Swarm 
Optimization and Simulated Annealing (RPSO-SA) to solve the 
Knapsack Problems (KPs). The features RPSO-SA create an 
innovative approach, which can generate high-quality solutions in 
shorter times and more stable convergence characteristics. The 
RPSO takes advantage of group-swarm to keep the balance between 
the global exploration and the local exploitation. The SA gently 
improves the candidate solution by searching for optimal solutions 
within a local neighbourhood. The RPSO-SA combines the strong 
global search ability of RPSO and the strong local search ability of 
SA to reach faster optimal solution. In addition, there are two ways 
of accepting a new solution. The method has been tested against the 
knapsack problems. The results indicate that the combined approach 
outperforms individual implementations of radius particle swarm 
optimization and simulated annealing. 
 
Keywords‚ÄîRadius Particle Swarm Optimzation; Simulated 
Annelling; Combined Algorithm; Knapsack Problem. 
 
 
1. 
INTRODUCTION 
 
The Knapsack Problems  )KPs  (are NP-hard that has been studied 
in the last few decades, involving both and exact algorithms and 
approximated methods. The exact approaches include branch and 
bound [1] and dynamic programming [2]. The approximated 
mainly include the hybrid algorithm [3] and meta-heuristic 
algorithms such as radius particle swarm optimization (RPSO) [4], 
simulated annealing (SA) [5] and genetic algorithm (GA) [6]. 
 
KPs are optimization problems in the binary domain and are used 
for variety of practical problems, such as capital budget control, 
resources allocation [7], project selection [8], cutting and packing 
problems [9] and cryptography [10]. The KP is given a set of items 
with weights and sizes and the capacity value of a knapsack in 
order to maximise the total weight of selected items in the 
knapsack to satisfy the capacity constraint. Due to its practical 
values, Thiongane et al. [11] show the combination method to deal 
with lagrangian heuristics, local searches, variable fixing and re-
optimization by a Sub-Gradient Algorithm (SGA). Bansal and 
Deep [12] propose the Modified Binary PSO (MBPSO) to update 
the MBPSO position term and to provide a new probability of 
selection. Zhang et al. [13] converted the KPs to a directed graph 
using the Network Converting Algorithm (NCA). Wang et al. [14] 
propose an effective hybrid algorithm based on Estimation of 
Distribution Algorithms (EDAs) that included a new probability 
model based on specific knowledge to improve the convergence 
speed. 
                                                           
1 I wish to thank Dr.Samart Moodleah for invaluable comments 
and final proofreading of this paper. 
 
We present a novel approach that combined the radius particle 
swarm optimization and simulated annealing (RPSO-SA) for 
solving the KPs. The results of the proposed method will compare 
with the individual radius particle swarm optimization (RPSO) and 
simulated annealing (SA). 
 
 
2. 
RELATED WORK 
 
This section introduces the knapsack problem (KP), the radius 
particle swarm optimization (RPSO) and the simulated annealing 
algorithm (SA) as well as their equations. 
 
Knapsack Problem 
There has been a lot generalization of the classical knapsack 
problem, depending upon the distribution of the items. In this 
paper, we focus the KPs such that 0-1 knapsack problem (KP): 
each item may be chosen at most once and multi-dimension 
knapsack problem (MKP): If we have n items and m knapsacks 
with capacities not necessarily same and knapsack are to be filled 
simultaneously.   
 
0-1 Knapsack: In the classical knapsack problem (KP) we 
have a set of items (i = 1, ‚Ä¶ , n ) and a knapsack of limited 
capacity. Each item we associated a positive profit p‡≠ß and a 
positive weight w‡≠ß. The problem for finding the set of items with 
maximum overall profit among those whose total weight does not 
exceed the knapsack capacity C. The problem may be formulated 
so as the maximize the total profit f(x) as follows; 
 
ùëÄùëéùë•ùëñùëöùëñùëßùëí ùëì(ùë•)‡∑çùëù‡Øú ùë•‡Øú
‡Øú‚àà‡Ø°
                                                        (1)  
ùëÜùë¢ùëèùëóùëíùëêùë° ùë°ùëú  ‡∑çùë§‡Øúùë•‡Øú ‚â§ùëê
‡Øú‚àà‡Ø°
 ; ùë•‡Øú ‚àà[0,1], ùëñ= 1, ‚Ä¶ ùëõ            
Where ùë•‡Øú is a binary number, is equal 1 if and only if item ùëñ is 
selected. However, we cannot take all items because the total 
weight more than the knapsack capacity. 
 
Multi-dimension Knapsack: The multi-dimension knapsack 
problem (MKP) is a generalization of the classic knapsack 
problem. It consists of selecting a subset of given items in such a 
way that the total profit of the selected items is maximized. 
Therefore, the problem can be given follows; 
 
ùëÄùëéùë•ùëñùëöùëñùëßùëí ùëì(ùë•) ‡∑çùëù‡Øú ùë•‡Øú
‡Øú‚àà‡Ø°
                                                    (2)  
        ùëÜùë¢ùëèùëóùëíùëêùë° ùë°ùëú  ‡∑çùë§‡Øú,‡Øù ùë•‡Øú ‚â§ùê∂‡Øù
‡Øú‚àà‡Ø°
  ‚àÄùëó= 1, . . , ùëö                           
                        ùë•‡Øú ‚àà[0,1], ùëñ= 1, ‚Ä¶ ùëõ                                 
ISSN: 1690-4524                              SYSTEMICS, CYBERNETICS AND INFORMATICS        VOLUME 19 - NUMBER 5 - YEAR 2021                             63

Where ùëõ is the number of items, ùëö is the number of knapsack 
constraints with capacities ùê∂‡Øù, ùëù‡Øú is the positive profit, ùë•‡Øú is a binary 
number, is equal 1 if and only if item ùëñ is selected and  ùë§‡Øú,‡Øù  is the 
positive weight of the knapsack‚Äôs constraints matrix. 
 
Radius Particle Swarm Optimzation Algorithm (RPSO) 
Radius Particle Swarm Optimization (RPSO) [4] is proposed by 
extending the ring topology to include a set of neighbors using a 
radius-based neighborhood in order to speed up convergence and 
avoid the local optima. The RPSO is a simple concept, easy to 
implement and computationally inexpensive. It extends the 
traditional PSO algorithm by grouping particles within the same 
radius into a new particle agent and then iteratively finding the best 
solution under the given objective functions. The significant 
concept of the RPSO is based on finding the agent particle within 
the radius of a circle. Thus, it uses a swarm circle topology to find 
the agent particle within the radius of the circle. Each particle in 
the overlap radius can be in multiple groups. Once a group is 
defined, it finds the best particle in that swarm group and assign to 
the agent particle. Finally, the agent particles are the candidates for 
finding the optimal solution, or the gbest position. To overcome 
the premature convergence problem, the RPSO takes advantage of 
group-swarms to maintain the swarm diversity and evolution by 
sharing information from the agent particles, which effectively 
keep the balance between the global exploration and the local 
exploitation. Obviously, the agent particle guides the neighbouring 
particles to jump out of the local optimum and achieve the global 
best. 
 
The Euclidean distance is used to calculate the radius-
neighbourhood between particle ùëñ and particle ùúÉ using equation 3. 
 
ùëë(ùëù‡Øú,ùëù√ò) = ‡∂®‡∑ç
‡µ´ùëù‡Øú,‡Øù‚àíùëù√ò,‡Øù‡µØ
‡¨∂
‡Ø†
‡Øù‡≠Ä‡¨µ
 ; ùëë‚â§2ùëü , ùëñ ‚â†√ò   , ùëìùëúùëü   1 ‚â§ ùëñ ‚â§ùëõ   
,ùëìùëúùëü    1 ‚â§ √ò ‚â§ùëõ                                                      (3) 
 
where 
ùëù= ‡µõùë•‡¨µ, ùë•‡¨∂,ùë•‡¨∑,‚Ä¶ ,  ùë•‡Ø° ‡µü 
 
 ùëü=  ùúá ‚àôùë£‡Ø†‡Øî‡Ø´; ùúá[0.0,1.0]                                         (4)    
 
In equation (4), the radius value (ùëü) of the particle is obtained. 
Here, ùëü is determined by the maximum velocity (ùë£‡Ø†‡Øî‡Ø´). Therefore, 
ùë£‡Ø†‡Øî‡Ø´ is assigned to the maximum bounds of the search space or the 
feasible bounds in the benchmark function. We consider the 
problem of finding the global optimum using the agent particle 
(abest‡≠ß,‡≠®) within a radius-neighbourhood as given in equation (5).  
  
ùëéùëèùëíùë†ùë°‡Øú,‡Øù=  min
‡∞â‚àà‡∞ò ùëì(ùõΩ)                                                                                     (5) 
     
Therefore, the particle ùëñ in the swarm updates it velocity and 
position as given in equation (6) and equation (7), respectively. 
 
ùë£‡Øú,‡Øù(ùë°+ 1) = ùë§‚àôùë£‡Øú,‡Øù(ùë°) + ùëê‡¨µ‚àôùëÖ‡¨µ ‚àô·âÄùëùùëèùëíùë†ùë°‡Øú,‡Øù‚àíùë•‡Øú,‡Øù(ùë°)·âÅ                        
                       +ùëê‡¨∂‚àôùëÖ‡¨∂‚àô·âÄùëéùëèùëíùë†ùë°‡Øú,‡Øù(ùë°) ‚àíùë•‡Øú,‡Øù(ùë°)·âÅ            (6) 
 ùë•‡Øú,‡Øù(ùë°+ 1) = ùë•‡Øú,‡Øù(ùë°) + ùë£‡Øú,‡Øù(ùë°+ 1)                                                                    (7) 
 
Simulated Annealing Algorithm (SA) 
Simulated annealing (SA) is a gradient method for the global 
optimization problem which is firstly proposed by Kirkpatrick et al. 
[5]. This algorithm gradually improves the solution by searching for 
better solutions within a local neighborhood. There are two ways of 
accepting a new solution: (1) if its fitness value is better than that of 
the current solution; (2) by accepting a solution with a worse fitness 
value with a certain probability. In the second case, the probability 
is computed based on the difference in fitness values between the 
new and current solution. Therefore, this acceptance probability 
ùëÉ(ùë°)is defined by: 
ùëÉ(ùë°) = ùëí‡¨ø‚àÜ‡Øô‡≥ü
‡Øç                                                                                                     (8) 
Where ùëá is the current temperature (scaling parameter), ‚àÜùëì‡Øß  is the 
difference in the values of the result between the current and the 
candidate solutions at step. The temperature has an initial value ùëá‡¨¥ 
and it is reduced progressively according to a predefined cooling 
schedule. The temperature at iteration on ùë° is calculated as follows; 
 
ùëá(ùë°+ 1) =  ùõº‚àôùëá(ùë°);0 ‚â• ùõº ‚â§1                                                            (9) 
 
Where ùõº is called the cooling rate. Note that the purpose of the 
acceptance probability is prevents the algorithm from getting 
trapped in a local optimum by allowing non-improving moves. 
Thus, by allowing the current solution to worsen temporarily with a 
given probability, simulated annealing is opens up the probability to 
find the global optimum. 
 
 
3. 
RADIUS PARTICLE SWARM OPTIMZATION FUSE WITH 
SIMULATED ANNEALING 
 
Motivation 
The idea of RPSO-SA involves a fusion state and final state as 
shown in Fig.1. In the fusion state, RPSO-SA is implemented using 
the temperatures. Thus, the initial temperature is obtained by 
dividing the difference between the maximum and minimum fitness 
of the initial particle in the swarm by the acceptance probability. 
The initial temperature is adaptive in each KP dataset. During 
fusion state, the RPSO-SA finds the best solution and the proposed 
algorithm skips local optima by allowing the exploration of the 
problem space in the direction that leads to a local increase in the 
next solution. In the final state, after the system is cool, each 
particle in the swarm updates position using the best position from 
the fusion state and then opens up to find the global optimum. 
 
 
Fig.1. Convergence of RPSO-SA algorithm for MKP 
 
The framework of RPSO-SA uses the strong global search ability of 
RPSO and the strong local search ability of SA as shown in 
Algorithm 1. In the RPSOSA, the global best position is selected 
from agent particles in the search space. Additionally, there are two 
ways of accepting a new solution. Hence, the RPSO-SA allows the 
fitness of some particles may be accepting a solution with a worse 
fitness within a certain probability by the metropolis process of SA. 
 
The algorithm has the advantages of both RPSO and SA algorithms. 
It solves the KPs with different dimensions and then we will 
compare the results with both individual RPSO and SA algorithms 
regarding solution quality and computational efficiency. 
 
Solution representation  
To solve the KPs, a candidate solution for KPs represent as the 
dimension is the number of items n, as shown in Fig.2. For the 
example, we have seven items and after the position of a particle is 
updated, the position representation is: 0.20, 0.90, 0.10, 0.70, 0.05, 
0.65 and 0.80.  
64                              SYSTEMICS, CYBERNETICS AND INFORMATICS        VOLUME 19 - NUMBER 5 - YEAR 2021                             ISSN: 1690-4524

 
ùíãùüé 
ùíãùüè 
ùíãùüê 
ùíãùüë 
ùíãùüí 
ùíãùüì 
ùíãùüî 
0.20 
0.90 
0.10 
0.70 
0.05 
0.65 
0.80 
 
Fig.2. Representation of the position in each dimension for KPs. 
 
Transfer method 
The basic idea of the position update equation for RPSO-SA in the 
binary domain is taken from position update equation of binary 
PSO. If a random number value is more than the sigmoid value of 
the velocity then the position of particle takes the value 0 or 1, as 
shown in Fig.3. For example, after the position of a particle is 
updated with the sigmoid function and then compared to the 
random number, the position representation is: 0, 1, 0, 1, 1, 1 and 
1. We then select item number 2, 4, 5, and 6. 
 
ùë£‡Øú,‡Øù(ùë°+ 1) 
0.20 
0.90 
0.10 
0.70 
0.05 
0.65 
ùëÜ·âÄùë£‡Øú,‡Øù(ùë°+ 1)·âÅ 
0.55 
0.71 
0.52 
0.67 
0.51 
0.66 
Random number 
0.63 
0.55 
0.97 
0.09 
0.47 
0.26 
ùë•‡Øú,‡Øù(ùë°+ 1) 
0 
1 
0 
1 
1 
1 
 
Fig.3. Representation of transfer of positions for KPs. 
 
 
4. 
E
XPERIMENTS AND RESULTS
 
 
Knapsack test problems 
The benchmarks are selected from MP-Test-data SAC-94 suite. A 
set of 0‚Äì1 knapsack instances are taken from [15]. The proposed 
method is tested using 24 different numbers of items ùëõ and 
knapsack constraints for 0‚Äì1KP and nine different problem classes, 
each with different item/knapsack constraint combinations for 
MKP. Therefore, the comparison is made on the basis of optimum 
rate, best profit and average profit. 
 
Parameter setting  
The size of swarm is 60, iteration number is 4000 or 240000 
function evaluations, ùë•‡Ø†‡Øî‡Ø´ and ùë£‡Ø†‡Øî‡Ø´ are equal and within the 
range of [-4, 4], w is 0.98, the cooling rate Œ± is 0.95 and frozen Œµ is 
0.001. 
 
0‚Äì1 KP result 
Total of 24 0‚Äì1KP test benchmark functions are used to compare 
the performance of RPSO-SA. Table 1 presents the numerical 
result of 0‚Äì1KP produced by the three algorithms: RPSO-SA, 
RPSO and SA.  
 
It is obvious that RPSO-SA presents higher optimum rates for all 
instances. It can also be seen that RPSO-SA is better than RPSO 
and SA from the point of view of best profit and average profit. In 
conclusion, RPSO-FA is relatively better than RPSO and SA. 
 
MKP result 
For the MKP, the convergence results of RPSO, SA and RPSO-SA 
are demonstrated in Fig. 4 to 6. It is clearly shown that the RPSO-
SA has achieved the optimum solution faster than the other two 
methods. 
 
 
Fig. 4.  Convergence of RPSO algorithm for MKP (Sent2.dat) 
 
Fig. 5. Convergence of SA Algorithm for MKP (Sent2.dat) 
 
 
Fig. 6. Convergence of RPSO-SA algorithm  for MKP (Sent2.dat) 
 
The MKP numerical results of the instance datasets are illustrated 
in Table 2. The number of item instances range between 20 and 70. 
Table 2 reports the detailed results produced by the three 
algorithms: RPSO-SA, RPSO and SA. It is clear that RPSO-SA is 
more reliable than RPSO and SA in terms of optimum rate, best 
and average profit. 
 
 
5. 
C
ONCLUSION
 
 
The local optimum is frequently found in the knapsack problems. 
Author proposed a novel approach to solve the knapsack problems 
by combining the radius particle swarm optimization (RPSO) and 
simulated annealing (SA) algorithm. The structure of the method 
enables the advantages of RPSO which has strong global search 
ability and SA which has strong local search ability to obtain 
optimum solution rapidly. The basic idea of the algorithm consists 
of fusion state and final state. In the fusion state, RPSO-SA is 
implemented using the temperatures. The initial temperature is the 
difference between the maximum fitness and minimum fitness of 
the initial particle in swarm. During fusion state, the RPSO find the 
best solution and the SA skip local optimum by allowing the 
exploration of the problem space in directions that lead to a local 
increase in the next solution. The final state, after the system is 
cool, each particle in swarm update position by the best position 
from the fusion state, then opens up to find the global optimum. 
The RPSO-SA is applied to solve the well-known optimization 
KPs and MKPs problems. It employs five datasets of the 0-1 
knapsack and seven datasets of the multi-dimension knapsack 
ISSN: 1690-4524                              SYSTEMICS, CYBERNETICS AND INFORMATICS        VOLUME 19 - NUMBER 5 - YEAR 2021                             65

problem. The experiments have shown that the performance of 
RPSO-SA outperform the RPSO and SA. 
 
 
6. 
REFERENCES 
 
[1]. S. Goyal and A. Parashar, ‚ÄúA Proposed Solution to Knapsack 
Problem Using Branch & Bound Technique‚Äù,  Int. J. for 
Innovative Research in Multidisciplinary Field, V.2(7), 
2016, pp. 240-246. 
[2]. A. Rong, and J. R. Figueira, ‚ÄúComputational performance of 
basic state reduction based dynamic programming algorithms 
for bi-objective 0‚Äì1 knapsack problem‚Äù, Computers and 
Mathematics with Applications, vol. 63, 2012, pp.1462‚Äì
1480.  
[3]. Z. Ren,  Z. Feng , and A. Zhang, ‚ÄúFusing ant colony 
optimization with Lagrangian relaxation for the multiple-
choice multidimensional knapsack problem‚Äù, Information 
Sciences, vol. 182, 2012, pp.15‚Äì29. 
[4]. Mana Anantathanvit and Mudarmeen Munlin, ‚ÄúUsing K-
means Radius Particle Swarm Optimization for the Travelling 
Salesman Problem‚Äù, IETE Technical Review, vol. 33, 2016, 
pp. 172-180. 
[5]. S. Kirkpatrick, C. D. Gelatt Jr., and M. P. Vecchi, 
‚ÄúOptimzation by simulated anneling‚Äù, Science, Vol. 
220(4598),1983,  pp. 671-680. 
[6]. Li Y., He Y., Li H., Guo X., Li Z., ‚ÄúA Binary Particle Swarm 
Optimization for Solving the Bounded Knapsack Problem‚Äù, 
Computational Intelligence and Intelligent Systems, 2019, 
pp. 50-60. 
[7]. K. Iwama and G. Zhang, ‚ÄúOnline knapsack with resource 
augmentation‚Äù, Information Processing Letters, Vol. 
110(22), 2010, pp. 1016-1020. 
[8].  E. Bas, ‚ÄúSurrogate relaxation of a fuzzy multidimensional 0‚Äì
1 knapsack model by surrogate constraint normalization rules 
and a methodology for multi-attribute project portfolio 
selection‚Äù, Engineering Application of Artificial 
Intellience, Vol. 25(5), 2012, pp. 958-970. 
[9]. M.M. Baldi, G. Perboli and R. Tadei, ‚ÄúThe three-dimensional 
knapsack problem with balancing constraints‚Äù, Applied 
Mathematics and Computation, V.218, 2012, pp. 9802‚Äì
9818. 
[10]. A. Pham, ‚ÄúThe improvent knapsack chiper‚Äù, Computer 
Comunications, Vol. 34(3), 2011, pp. 342-343. 
[11]. B.Thiongane, A. Nagih, and G. Plateau, ‚ÄúLagrangean 
heuristics combined with reoptimization for the 0‚Äì1 
bidimensional knapsack problem‚Äù, Discrete Applied 
Mathematics, Vol. 154(15), 2006, pp. 2200‚Äì2211. 
[12]. J. C. Bansal and K. Deep, ‚ÄúA Modified Binary Particle 
Swarm Optimization for Knapsack Problems‚Äù, Applied 
Mathematics and Computation, Vol. 218(22), 2012, 
pp.11042‚Äì11061. 
[13]. X. Zhang, S. Huang, Y. Hu, Y. Zhang, S. Mahadevan, and Y. 
Deng, ‚ÄúSolving 0-1 knapsack problems based on amoeboid 
organism algorithm‚Äù, Applied Mathematics and 
Computation, Vol. 219(19), 2013, pp. 9959‚Äì9970. 
[14]. L.Wang, S.Wang, and Y. Xu, ‚ÄúAn effective hybrid EDA-
based algorithm for solving multidimensional knapsack 
problem‚Äù, Expert Systems with Applications, Vol. 39(5), 
2012, pp. 5593‚Äì5599. 
[15]. MP-Test-Data SAC-94 [Online] Avaliable: 
http://elib.zib.de/pub/mp-testdata/ip/sac94-suite/index.html 
 
 
 
 
 
Algorithm 1 The framework of RPSO-SA for KPs. 
1. 
//Step (1) Initialization 
2. 
Set the parameters including swarm size, dimension size, t,  ùë°‡Ø†‡Øî‡Ø´,  ùë£‡Ø†‡Øú‡Ø°, ùë£‡Ø†‡Øî‡Ø´, ùë•‡Ø†‡Øú‡Ø°, ùë•‡Ø†‡Øî‡Ø´, ùë§, ¬µ, ùëê‡¨µ, 
ùëê‡¨∂ temperatures (ùëá), cooling rate (ùõº) and frozen (ùúÄ) 
3. 
Initialise each dimension of gbest position with ùë•‡Ø†‡Øú‡Ø°    
4. 
Calculate the gbest value of gbest position with the fitness function
5. 
Initialise each particle with random position and velocity  
6. 
for each particle in the swarm do
7. 
       Calculate the fitness value of particle with the fitness function
8. 
       Update its pbest value with its fitness value
9. 
       Assign its position to its pbest position
10.
        if its pbest value better than the gbest value then
11.
               Update the gbest value with its fitness value 
12.
              Assign its position to the gbest position
13.
         end if    
14.
end for 
15.
//Step (2) Reproduction and updating loop 
16.
 t = 0 
17.
temperatures (ùëá) =
‡Ø†‡Øî‡Ø´‡Øú‡Ø†‡Ø®‡Ø†‡Øô‡Øú‡Øß‡Ø°‡Øò‡Ø¶‡Ø¶‡Ø©‡Øî‡Øü‡Ø®‡Øò‡Ø¢‡Øô‡Ø£‡Øî‡Ø•‡Øß‡Øú‡Øñ‡Øü‡Øò‡¨ø‡Ø†‡Øú‡Ø°‡Øú‡Ø†‡Ø®‡Ø†‡Øô‡Øú‡Øß‡Ø°‡Øò‡Ø¶‡Ø¶‡Ø©‡Øî‡Øü‡Ø®‡Øò‡Ø¢‡Øô‡Ø£‡Øî‡Ø•‡Øß‡Øú‡Øñ‡Øü‡Øò
‡≠™‡≠¨‡¨¥.‡¨∫
 
18.
for each iteration ùë° to ùë°‡Ø†‡Øî‡Ø´do 
19.
     for each particle in the swarm do
20.
             for each dimension in the particle do  
21.
                    Calculate the velocity by equation
)
3.4
(
 
22.
                     if  its velocity value more than ùë£‡Ø†‡Øî‡Ø´ 
66                              SYSTEMICS, CYBERNETICS AND INFORMATICS        VOLUME 19 - NUMBER 5 - YEAR 2021                             ISSN: 1690-4524

23.
                          Update its velocity value with ùë£‡Ø†‡Øî‡Ø´ 
24.
                    else if  its velocity value less than ùë£‡Ø†‡Øú‡Ø° 
25.
                         Update its velocity value with ùë£‡Ø†‡Øú‡Ø° 
26.
                    end if  
27.
                   Update the position by equation 
)
3.5
(
 
28.
                   if  its position value more than ùë•‡Ø†‡Øî‡Ø´ 
29.
                         Update its position value with ùë•‡Ø†‡Øî‡Ø´ 
30.
                  else if  its position value less than ùë•‡Ø†‡Øú‡Ø°  
31.
                         Update its position value with ùë•‡Ø†‡Øú‡Ø° 
32.
                   end if 
33.
          end for 
34.
          Calculate the fitness value of particle with the fitness function 
35.
          if its fitness value better than its pbest value then 
36.
               Update its pbest value with its fitness value 
37.
               Assign its position to its pbest position 
38.
          end if  
39.
          //Step (2.1) Find the agent particle on the radius-neighbourhood  
40.
          Define the radius-neighbourhood by equation 3.1 and equation  ) 3.2( 
41.
          Find the abest particle in the radius swarm group by equation )3.3( 
42.
          Step (2.2) Find the gbest on the global neighbourhood 
43.
          for each abest particle do 
44.
             if fitness value of abest particle better than the gbest value then 
45.
                   Update the gbest value with the fitness value of abest particle 
46.
                  Assign abest position to the gbest position 
47.
             else if   (temperatures (T) is more than frozen (ùúÄ) ) 
48.
             //Step (2.3) SA algorithm 
49.
                    Assign the gbest position to current solution 
50.
                    Generate the candidate solution on the current solution 
51.
‚àÜf = the finess value of candidate solution ‚àíthe finess value of current solution 
52.
                    if  the fitness value of candidate solution better than the fitness value of  current 
53.
                      Update the gbest value with the fitness value of candidate solution 
54.
else if Random[0,1] > e‡¨ø‚àÜ‡±ú
‡±êthen
55.
                         Update the gbest value with the fitness value of candidate solution 
56.
                         Assign the candidate solution to the gbest position 
57.
                    end if 
58.
              end if 
59.
         end for   
60.
    end for 
61.
 ùëá= ùõº. ùëá 
62.
end for 
 
 
 
 
 
 
 
ISSN: 1690-4524                              SYSTEMICS, CYBERNETICS AND INFORMATICS        VOLUME 19 - NUMBER 5 - YEAR 2021                             67

Table 1. The 0‚Äì1KP results. 
Dataset 
Items 
OPT. 
Algorithm 
Optimum rate 
(%) 
Profit 
Best 
Average 
ks 8b
8
3813669
RPSO-SA
100
3813669
3813669
RPSO
100
3813669
3813669
SA
100
3813669
3813669
ks 12b 
12
649859
RPSO-SA
100
649859
649859
RPSO
85
649859
647645
SA
66
649859
627645
ks 16b 
16
9352998
RPSO-SA
96
9352998
9352818
RPSO
92
9352998
9352601
SA
74
9352998
9341252
ks 20b 
20
9818261
RPSO-SA
88
9818261
9792115
RPSO
86
9818261
9699235
SA
72
9818261
9655125
ks 24b 
24
12233713
RPSO-SA
70
12233713
12216251
RPSO
65
12233713
12125120
SA
30
12233713
11245230
 
 
 
 
Table 2. The MKP results. 
Dataset 
m 
n 
Opt. 
Algorithm 
Optimum rate 
(%) 
Profit
Best 
Average 
Pet2 
10 
10 
87061 
RPSO-SA 
100 
87061 
87061 
 
RPSO 
58 
87061 
83369 
 
SA 
0 
83369 
80312 
Pb4 
2 
29 
95168 
RPSO-SA 
87 
95168 
94002 
 
RPSO 
40 
95168 
90900 
 
SA 
0 
94801 
90809 
Pb5 
10 
20 
2139 
RPSO-SA 
81 
2139 
2034 
 
RPSO 
70 
2139 
2002 
 
SA 
0 
2088 
1986 
Pb6 
30 
40 
776 
RPSO-SA 
68 
776 
680 
 
RPSO 
15 
776 
652 
 
SA 
0 
686 
642 
Sent1 
30 
60 
7772 
RPSO-SA 
65 
7772 
7723 
 
RPSO 
34 
7772 
7758 
 
SA 
0 
6939 
6272 
Sent2 
30 
60 
8722 
RPSO-SA 
42 
8722 
8512 
 
RPSO 
5 
8722 
8112 
 
SA 
0 
8311 
7995 
Weish20 
5 
70 
9450 
RPSO-SA 
89 
9450 
9206 
 
RPSO 
60 
9450 
9014 
 
SA 
0 
7787 
7078 
68                              SYSTEMICS, CYBERNETICS AND INFORMATICS        VOLUME 19 - NUMBER 5 - YEAR 2021                             ISSN: 1690-4524

