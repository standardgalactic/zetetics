ON DECODER-ONLY ARCHITECTURE FOR SPEECH-TO-TEXT AND LARGE LANGUAGE
MODEL INTEGRATION
Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu,
Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, Yu Wu
Microsoft, One Microsoft Way, Redmond, USA
ABSTRACT
Large language models (LLMs) have achieved remark-
able success in the field of natural language processing, en-
abling better human-computer interaction using natural lan-
guage. However, the seamless integration of speech signals
into LLMs has not been explored well. The “decoder-only”
architecture has also not been well studied for speech pro-
cessing tasks. In this research, we introduce Speech-LLaMA,
a novel approach that effectively incorporates acoustic infor-
mation into text-based large language models. Our method
leverages Connectionist Temporal Classification and a sim-
ple audio encoder to map the compressed acoustic features to
the continuous semantic space of the LLM. In addition, we
further probe the decoder-only architecture for speech-to-text
tasks by training a smaller scale randomly initialized speech-
LLaMA model from speech-text paired data alone. We con-
duct experiments on multilingual speech-to-text translation
tasks and demonstrate a significant improvement over strong
baselines, highlighting the potential advantages of decoder-
only models for speech-to-text conversion.
Index Terms— decoder-only, LLaMA, LoRA, speech
translation
1. INTRODUCTION
In recent times, the large language models (LLMs) have
showcased remarkable achievements across various natu-
ral language benchmarks, encompassing question answer-
ing, machine translation, language understanding and more
[1, 2, 3, 4, 5]. By employing a transformer-based architec-
ture [6] and training to anticipate forthcoming tokens within
a sequence, this language model excels in contextual learning
abilities. Not only does this significantly enhance its model-
ing prowess, but more importantly, it enables seamless user
interaction that effectively connects cutting-edge research
with real-world applications.
As speech represents the most innate and instinctive mode
of human communication, integrating speech and LLMs will
further boost the user experience of human-machine interac-
tion. Based on this intuition, several attempts in combining
speech signals and large language models were carried out
[7, 8, 9]. Among them, the cascaded approach is the most
straightforward solution. In these systems, the speech sig-
nal is firstly transformed into word tokens through existing
automatic speech recognition (ASR) [10] models, and LLM
processes the recognized words for downstream tasks. Later,
inspired by the integration of image information to LLMs [11,
12, 13], researchers also explored the deep combination of
speech signals [14, 15, 16, 17]. In [14], the authors pro-
posed to jointly model the speech and text tasks through a
unified decoder only network. Similarly, in [17], the authors
proposed to optimize the audio token conversion module to-
gether with a off-the-shelf LLM. Instead of word pieces, dis-
crete tokens of speech representation from a self-supervised
model are used in [15].
While there have been promising outcomes, several cru-
cial challenges regarding the integration of speech and LLMs
still require further exploration. Initially, aligning the two
modalities (speech and text) using a pretrained LLM poses
challenges due to the typically longer sequence length of
speech signals compared to text sequences. Moreover, given
the costly nature of training LLMs, finding ways to minimize
the overall integration cost while maintaining exceptional
performance continues to be a challenging task. More im-
portantly, considering the remarkable success of the LLMs,
it is crucial to explore the untapped potential of using a
decoder-only model [3, 14, 18, 19] as the backbone network
architecture for speech to text processing.
In this study, we aim to tackle the aforementioned chal-
lenges by exploring an efficient end-to-end integration of
speech and language models. Our approach involves design-
ing a simple yet effective architecture where a large language
model that operates on text also incorporates acoustic em-
beddings. This integration enables the LM to condition its
transcription or translation on the acoustic information. More
specifically, our proposed method utilizes a pre-existing LLM
and incorporates a duration compressor and an acoustic en-
coder introducing only a small number of free parameters.
Diverging from previous approaches that convert speech into
discretized tokens, our model directly maps the continuous
representation of speech into the semantic space defined by
the LM. During the processing stage, the speech filter bank
arXiv:2307.03917v1  [eess.AS]  8 Jul 2023

signal is initially compressed by the duration compressor
to reduce the sequence length. Subsequently, the acoustic
encoder transforms the compressed speech signal into con-
tinuous vectors in the same semantic space of the text that
can be consumed by the LLM. The final output is generated
through the decoding process of the LLM.
We thoroughly investigate various practical aspects of our
proposed model, such as selecting the appropriate duration
compressor, attention mask, and fine-tuning methods. Addi-
tionally, we apply the proposed model to the task of trans-
lating speech in 12 different languages into English text and
compare its performance against a strong baseline on CoVoST
dataset.
Finally, we demonstrate that the decoder-only model,
even trained from scratch using only speech-text paired data,
exhibits significant potential and several advantages over the
commonly employed encoder-decoder architecture in speech
processing.
In this work, our contribution can be summarized as fol-
lows:
• We introduce an efficient end-to-end integration method
called Speech-LLaMA, which effectively integrates
existing text-based large language models with speech
processing. Through the utilization of Speech-LLaMA,
we have achieved substantial improvements in trans-
lation performance compared to strong baselines on
various speech translation tasks.
• We investigate various practical aspects of the proposed
speech-LM integrations that are crucial for enhancing
performance.
These aspects include audio duration
compression, attention mask selection, and fine-tuning
strategy.
• On large, diverse and real-world data, we show that the
decoder-only architecture can be as competitive as the
encoder-decoder architecture for speech-to-text tasks.
We show that decoder-only to also be more parameter
efficient.
2. RELATED WORK
Our model aims at integrating speech signals into large lan-
guage models, as well as relates to connectionist temporal
classification (CTC) audio length compression and low-rank
adaptation (LoRA). We discuss these topics in the following.
2.1. Large language models
LLMs are generally pre-trained on vast amounts of textual
data that span a wide variety of domains and languages. They
usually consist of a stack of transformer layers, following an
auto-regressive decoder-only architecture, where each output
token is used as the input to predict the next step token. In
this work, we select LLaMA-7B [5] as the backbone LLM
to build the proposed method. LLaMA-7B model consists of
32 Transformer encoder layers with 32 heads and 4096 atten-
tion dimension. The tokenizer from the LLaMA work has a
vocabulary size of 32,000 which covers a group of languages.
2.2. CTC compressor
Connectionist Temporal Classification (CTC) Compressor
[20] was proposed to reduce the sequence length via remov-
ing the redundant information in the features. It was applied
in speech translation task and was shown to yield better mem-
ory consumption and performance. The method adds a linear
CTC branch in a middle layer of the encoder. The hidden
reprsentations of the CTC branch is then compressed accord-
ing to the distributions of the CTC posteriors and later passed
to the succeeding layers. The author investigated a few vari-
ations within this method of sequence length compression.
They found that averaging the consecutive hidden activations
(corresponding to consecutive CTC predictions belonging to
the same class) gives the best performance.
2.3. LoRA
Low-Rank Adaptation (LoRA) [21] is a commonly used tech-
nology to adapt the large models for new dataset or tasks. It
introduces a small amount of free parameters to each layer of
the source large model, while freezing all original model pa-
rameters. Specifically, for each weight matrix W ∈Rd×k in
a transformer layer, 2 new matrices Wa ∈Rd×r and Wb ∈
Rr×k introduced such that r ≪min{d, k}. For each matrix
multiplication during training, the input x is firstly multiplied
with both original weight W and its introduced low-rank ap-
proximation Wa, Wb, then the two outputs are summed to
form the output for later computation. Only Wa and Wb are
updated during fine tuning while W keeps frozen, thus signif-
icantly reducing the memory footprint.
3. OUR APPROACH
In this work, we design an architecture where a text-LLM also
accepts acoustic embedding and learns to condition on it to-
gether with text prompt to generate text outputs. By convert-
ing the speech input to a vector sequence in a space of the text
embeddings, in the aspect of both length and semantics, the
pre-trained text LLM can leverage its in-context learning ca-
pacity to absorb the speech signal and output corresponding
text for speech transcription and translation tasks.
Overall, given the text prompt p and audio signals x,
the generation of the corresponding text sequence y
=
{y0, y1, · · · , yN−1} with a text-LLM is formulated as:
p(y|p, x; ΘLLM) =
N−1
Y
n=0
p(yn|y<n, p, x; ΘLLM)
(1)

Text Prompt
Embedding
Audio Encoder
CTC Compressor
Acoustic Features
LLM
.......
....
....
...
Predicted Tokens
LLM
Fig. 1. High-level architecture of our proposed approach with
LLM. The green blocks indicate the part of the LLM. In this
work, we only learn parameters in the “Audio Encoder”, keep-
ing everything else frozen.
Overview Our proposed neural model consists of three
distinct parts: a pre-trained text neural LM, an audio encoder
and a CTC compressor, as shown in Figure 3. The text-LLM
in our case is a LLaMA-7B [13] but this method can be gen-
eralized to LLMs of any scale.
The CTC compressor re-
duces the sequence length of the input speech filter-bank to
match the length of the text, and the audio encoder transforms
the compressed speech signal into continuous vectors in the
LLM’s semantic space.
CTC compressor Different from the prior work that
trained the CTC compressor jointly with the main task [20],
our CTC compressor is also a pre-trained module, aiming
to match the audio and the text duration to the same scale
by selecting the representative frames from the audio signal.
In this work we explore two ways to reduce the sequence
length of the acoustic features in the CTC compressor: blank-
removing and frame-averaging.
For blank-removing, we
simply discard all the frames that are predicted as the blank
symbol according to the posterior distribution while we aver-
age the hidden states of consecutive frames once their CTC
predictions belong to the same class for frame-averaging.
Audio encoder The audio encoder is used to bridge rep-
resentations generated from the CTC compressor to the text
embeddings of the text-LLM. This module is designed to
be relatively small in size and is initialized with random
weights. During the fine-tuning process, the audio encoder is
optimized to effectively integrate the audio information with
the LLM, enhancing the overall performance of the system.
Different from the methods in [7, 8, 17], where the audio en-
coder is trained to firstly map the speech signal into discrete
tokens, which is then consumed by LLM, the proposed audio
encoder is directly optimized to map the compressed acoustic
signal to the continuous semantic space of LM, allowing a
deep integration between the audio encoder and the language
model.
Instruct learning For each training sample, we prepend
Conv2D Encoder
Decoder
.......
....
Predicted Tokens
<SOS>
Acoustic Features
Decoder
Fig. 2. The architecture of the decoder-only model for the
from-scratch training.
a text prompt that briefly describes the task, e.g., “audio ⇒
English” and “transcribe the audio into English”.
The text prompt are sampled from a pre-defined list, where
some prompts contains the source language ID following the
format “translate [source] audio into English”. Dur-
ing evaluation, we fix the text prompt as “translate the
audio into English” for all testing samples.
LoRA fine-tuning On top of the proposed model, we
apply the LoRA to four attention matrices in each layer of
the LLaMA Transformer (e.g., Wq, Wk, Wv, Wo). The en-
tire system is trained with cross entropy loss between the LM
output and the reference text. To stabilize the training, we
adopt a two-stage training scheme here which means we train
the audio encoder first with the CTC compressor and LLaMA
frozen and then introduce LoRA to the well-trained model
and perform the second stage optimization.
From-scratch training To further explore the potential
of decoder-only architecture as a foundational architecture for
speech modeling, we also include a “from-scratch” training of
a decoder-only architecture. Here, we replace the text prompt,
audio encoder and CTC compressor with a randomly initial-
ized convolutional 2D encoder. We also replace the pretrained
LLaMA network with a much smaller randomly initialized
autoregressive network. This architecture is shown in Fig-
ure 2. We also add a ⟨SOS⟩token at the end of the acoustic
sequence to indicate the starting of the generation. In this
case, the generation of the text sequence y with a decoder-
only model is conditioned purely on audio signal x and pre-
viously generated text sequence:
p(y|x; ΘDEC) =
N−1
Y
n=0
p(yn|y<n, x; ΘDEC)
(2)
where ΘDEC refers to the parameters of the decoder model.
4. EXPERIMENTS
The speech translation (ST) task [22, 23, 24] has been chosen
as the primary evaluation benchmark for assessing the pro-

posed methods. In this task, the goal is to develop a system
that can accurately translate spoken language from 12 source
languages to English.
4.1. Data and metric
We take 12 languages speech data for the training of the mod-
els in this work and each languages contains 1K hours of
in-house data. The 12 languages are German (DE), Chinese
(ZH), Spanish (ES), French (FR), Italian (IT), Dutch (NL),
Japanese (JA), Russian (RU), Portuguese (PT), Estonian (ET),
Swedish (SV), Slovenian (SL). The target transcripts for non-
English speech utterances are fed into in-house translation
service and corresponding English target transcriptions are
generated. All the training data was anonymized with per-
sonally identifiable information removed.
For evaluation, we use CoVoST 2 dataset [25] as the
benchmark and evaluate the BLEU [26] scores for the perfor-
mance comparison.
4.2. Models configuration
4.2.1. CTC compressor
The CTC compressor contains 2 convolution 2D layers fol-
lowed by 4 Transformers for a 4-times subsampling with
15.8M parameters in total. Each transformer layer has a 512
dimensional self-attention with 8 heads and a 2048 feed for-
ward dimension. Each convolution 2D layer has a stride size
of 2 and kernal size of 3. The CTC compressor is pre-trained
with the paired speech and text data (ASR task) from 12 lan-
guages using the CTC objective function and is frozen during
later training stages.
For comparison, we include a convolution based subsam-
pling module as baseline, which shares the same architecture
with CTC compressor but with additional 3 1D convolution
layers on top, allowing 4 × 8 = 32 times feature length re-
duction in total. The convolution based subsampling is jointly
trained with the LLaMA model.
4.2.2. Audio encoder
The audio encoder consists of 4 Transformer layers, where
each layer has the same setting as in the CTC compressor ex-
cept that the output tensor of the last layer is converted to di-
mension of 4096, in order to match the dimension of semantic
embedding in LLaMA.
For each training sample, we concatenate the embeddings
of the text prompt and representations from the audio encoder
in time axis as the prefix feature sequence and feed them to the
LLaMA to generate the target language (EN) transcriptions.
Two attention mask strategies are explored within the
LLaMA model. The first follows the language model train-
ing, where a causal, i.e., lower triangle attention mask is ap-
plied for each transformer layer to constrain the self-attention
to not look into the future. As the proposed model is “non-
streaming” in nature, we also explore a non-causal full atten-
tion mask strategy for the prefix part, i.e., text prompt and
audio encoder representations.
4.2.3. LoRA fine-tuning
We use rank 2 for LoRA fine-tuning, i.e., 8 rank 2 matrices
in shape of 2 × 4096 are introduced to each LLaMA Trans-
former layer as adaptor, which results in 2.1M more param-
eters. The LoRA fine-tuning is conducted on a well-trained
speechLLaMA model, where the CTC-compressor and the
LLaMA parameters are frozen. We still update the audio en-
coder to learn the better representations together with adapted
LLaMA.
4.2.4. Baseline
We adopt a seq2seq [27, 28] based speech translation model
as baseline. More specifically, we use the Whisper [29] archi-
tecture (240M) and train it on the 12k hour data mentioned in
Section 4.1. It contains a 12-layer audio Transformer encoder
and a 12-layer text Transformer encoder, where the attention
dimension and head number is 768 and 12, respectively. We
optimize the model using cross-entropy as the primary objec-
tive function but also augment this architecture with a CTC
loss on the encoder. We train the whole network end-to-end
in multi-task fashion. Please note that, for fair comparison,
we start the seq2seq training from scratch and do not initialize
with pretrained open source Whisper weights. During beam
search inference, we do a joint-decoding (prefix-decoding)
[30] with CTC. To make the comparison with LLaMA boot-
strapped models more just, we also present results with n-
best rescoring of the seq2seq model with LLaMA. To accom-
plish that, we do a simple log-linear interpolation between the
scores from seq2seq and LLaMA for each of n-best hypothe-
sis and then re-rank accordingly. We use n = 5 for seq2seq
beam-search decoding and the re-ranking experiments.
4.2.5. From-scratch training
In this setting, the structure of the convolutional 2D encoder
contains 2 convolutional layers which is same as the one in
CTC compressor and introduces a 4-times subsampling rate.
For Transformer decoder, we follow the implementation of
LLaMA, where pre-normalization and rotary positional em-
beddings (RoPE) [31] are adopted. Similar to the configu-
ration of the seq2seq baseline, each decoder layer contains a
12-head self-attention layer and 768 attention dimension. The
feed-forward dimension is set as 4076.
4.3. Training and evaluation
We extract 80-dim log mel-filterbank using 25 msec window
and 10 msec hop size as the acoustic features.
All mod-

Table 1. BLEU scores of the 12 languages on the baseline and the proposed models.
Model
Seq2seq
Decoder-only
SpeechLLaMA
ID
B1
B2
D1
E0
E1
E2
E3
E4
E5
Compressor
−
−
−
×
CTC (remove)
CTC (average)
Prefix Non-causal Mask
−
−
✓
×
×
×
×
✓
✓
LoRA
−
−
−
×
×
×
E2
×
E4
LLaMA Rescore
−
✓
−
−
−
−
−
−
−
Learnable #Param.
240M
240M
150M
31.2M
15M
15M
17.1M
15M
17.1M
DE
22.6
23.6
21.3
16.9
22.6
24.3
26
24.5
26.7
ZH
7.0
7.2
6.7
3.4
9.6
10.1
11.4
10.7
12.4
ES
23.7
24.9
22.7
19.6
23.5
25.4
27.3
25.8
27.7
FR
21.8
22.7
20.6
15.4
20.9
22.6
24.5
23.2
25
IT
20.7
21.6
19.8
16.7
21.4
23.7
25.3
23.9
25.9
NL
34.6
36
35.2
28.3
32.4
34.1
36
34.2
36.4
JA
15.3
15.7
16.3
10.3
17.5
17.7
19.8
17.9
19.6
RU
26.4
27.7
26
22.8
31
33.3
35.5
33.9
36.3
PT
28.9
30.2
27.2
22.8
26.8
29.2
31.3
29.6
31.9
ET
9.4
9.4
7.4
15.4
17
17.2
18.1
17.9
18.4
SV
24.4
25.6
27.5
26.3
25.3
26.7
27.4
27.2
29
SL
13.3
12.7
13.3
22.2
20.3
22.8
22.2
21.7
22.7
Average
20.7
21.4
20.3
18.3
22.4
23.9
25.4
24.2
26.0
els were trained with AdamW optimizer with β1 = 0.9 and
β2 = 0.98 on 16 V100 GPUs and a warmup and linear de-
cay learning rate strategy is used. Batch size varies with the
model size. CTC compressor was trained for 100K steps with
source language transcriptions, tokenized by LLaMA’s tok-
enizer. The peak learning rate was set to 0.001. In the first
stage training of SpeechLLaMA, We perform a 500K step
training with a peak learning rate of 0.015 while in the later
LoRA finetuning stage, we use additional 100K optimization
steps with a peak learning rate of 0.0002. The from scratch
decoder-only models were trained with a peak learning rate of
0.001 for at most 300K steps. We use beam search algorithm
with a beam size 4 for the decoding of all the models.
5. RESULTS AND DISCUSSIONS
The results of the experiments are presented in Table 1, where
several observations can be gleaned.
5.1. Baselines and effect of LLaMA rescoring
For baselines, we report results on 2 systems. B1 is a seq2seq
model described in Section 4.2.4 and B2 is B1 with LLaMA
n-best rescoring. As expected [32], a 0.7 better BLEU score
can be observed from B2 system over B1. This suggests that
shallow integration with LLM can still bring benefit to the
speech models.
5.2. Deeper integration with SpeechLLaMA
However, when comparing E1∼E5 with B2, we can find all
speech LLaMA models significantly outperform the baseline,
resulting in 2.3 to 5.6 absolute BLEU score improvement
(10.7% to 26.1% relative). This shows the efficacy of the
proposed system and also suggests the necessity for deeper
integration between the speech models and text-LLMs.
5.3. Audio length compressor
When comparing the performance among audio length com-
pressors, i.e., E0, E1 and E2, we observe a consistantly bet-
ter performance for the CTC compressors over the convolu-
tion one, despite the CTC-compressor being frozen during the
training. One hypothesis for the better performance of CTC
compressor is that it leverages the transcription of each source
language during pre-training stage. This observation also sug-
gests that a potentially better performance might be obtained
if the source transcription is also used during the training
stage.
We leave this line of exploration for future works.
Meanwhile, the average frame CTC compressor shows 1.5
better average BLEU score over the sampling ones. We be-
lieve it is because the CTC compressor can’t very reliably
distill all relevant information into non-blank representations.
So, the frames selected by the CTC compressor might lose
some acoustic information. The averaging strategy is more
robust to this compression error.

5.4. Effect of non-causal attention mask
We observe that between E2 and E4, only 0.3 BLEU score
improvement is obtained. E4 has non-causal attention mask
whereas E2 has the causal one. It is expected that the non-
causal mask over acoustic encoder would usually result in
a significantly better speech representation, and significantly
better results. However, this result indicates that the LLM is
robust and not very sensitive to the quality of acoustic rep-
resentations from the speech side. Instead, the duration mis-
match Was a more critical factor when integrating the speech
within the LLM.
5.5. LoRA finetuning
The LoRA finetuning result is shown as E5 in Table 1. Com-
paring with E4, an additional 1.8 BLEU score improvement
is obtained though the LoRA finetuning. Note that only 2.1M
additional parameters are added as adaptor. A potentially bet-
ter performance might be observed when more larger rank is
used. We leave this exploration for future works.
5.6. Decoder-only vs Encoder-Decoder
Finally, the results for randomly initialized decoder-only
model is shown as system D1 in Table 1. This model achieves
only slightly worse (0.4 lower BLEU score) performance
compared to the seq2seq baseline. Considering that the total
parameter for the decoder-only model is significantly lower
than the seq2seq baseline, this performance verifies the po-
tential of the decoder-only architecture for general speech
modeling.
6. CONCLUSION & FUTURE WORK
In this work, we propose a method to infuse an off-the-shelf
large language model with acoustic information. The pro-
posed model presents a deep integration between the audio
with the LLM by directly mapping acoustic representation
into the semantic space of LLM. We also explore several prac-
tical aspects for the proposed model for better performance
including audio-length compression, attention mask design
and adapter fine tuning.
We show that on a 12 language
to English speech translation task, the proposed model sig-
nificantly outperforms a strong sequence-to-sequence base-
line model. We also show that the decoder-only architecture,
trained from scratch, can achieve comparable performance
with around 40% less parameters, which verifies the potential
of decoder-only models for general speech-to-text modeling.
7. REFERENCES
[1] OpenAI, “Introducing chatgpt,” OpenAI Blog, 2022.
[2] OpenAI,
“Gpt-4 technical report,”
arXiv preprint
arXiv:2303.08774, 2023.
[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al., “Language models are few-shot learners,”
Advances in neural information processing systems, vol.
33, pp. 1877–1901, 2020.
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebas-
tian Gehrmann, et al., “Palm: Scaling language model-
ing with pathways,” arXiv preprint arXiv:2204.02311,
2022.
[5] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth´ee Lacroix,
Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal
Azhar, et al., “Llama: Open and efficient foundation lan-
guage models,” arXiv preprint arXiv:2302.13971, 2023.
[6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
and Illia Polosukhin, “Attention is all you need,” Ad-
vances in neural information processing systems, vol.
30, 2017.
[7] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong
Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing
Hong, Jiawei Huang, Jinglin Liu, et al., “Audiogpt: Un-
derstanding and generating speech, music, sound, and
talking head,” arXiv preprint arXiv:2304.12995, 2023.
[8] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang
Zhang, Jing Shi, Shuang Xu, and Bo Xu, “X-llm: Boot-
strapping advanced large language models by treating
multi-modalities as foreign languages,” arXiv preprint
arXiv:2305.04160, 2023.
[9] Soham Deshmukh, Benjamin Elizalde, Rita Singh, and
Huaming Wang, “Pengi: An audio language model for
audio tasks,” arXiv preprint arXiv:2305.11834, 2023.
[10] Jinyu Li,
“Recent advances in end-to-end automatic
speech recognition,”
APSIPA Transactions on Signal
and Information Processing, vol. 11, no. 1, 2022.
[11] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and
Mohamed Elhoseiny,
“Minigpt-4: Enhancing vision-
language understanding with advanced large language
models,” arXiv preprint arXiv:2304.10592, 2023.
[12] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee,
“Visual instruction tuning,”
arXiv preprint
arXiv:2304.08485, 2023.

[13] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie
Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He,
Xiangyu Yue, et al.,
“Llama-adapter v2: Parameter-
efficient visual instruction model,”
arXiv preprint
arXiv:2304.15010, 2023.
[14] Tianrui Wang, Long Zhou, Ziqiang Zhang, Yu Wu, Shu-
jie Liu, Yashesh Gaur, Zhuo Chen, Jinyu Li, and Furu
Wei, “Viola: Unified codec language models for speech
recognition, synthesis, and translation,” arXiv preprint
arXiv:2305.16107, 2023.
[15] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu
Wang, Yaqian Zhou, and Xipeng Qiu,
“Speechgpt:
Empowering large language models with intrinsic
cross-modal conversational abilities,”
arXiv preprint
arXiv:2305.11000, 2023.
[16] Eliya Nachmani, Alon Levkovitch, Julian Salazar,
Chulayutsh
Asawaroengchai,
Soroosh
Mariooryad,
RJ Skerry-Ryan, and Michelle Tadmor Ramanovich,
“Lms with a voice: Spoken language modeling beyond
speech tokens,” arXiv preprint arXiv:2305.15255, 2023.
[17] Paul
K
Rubenstein,
Chulayuth
Asawaroengchai,
Duc Dung Nguyen, Ankur Bapna, Zal´an Borsos, F´elix
de Chaumont Quitry, Peter Chen, Dalia El Badawy,
Wei Han, Eugene Kharitonov, et al., “Audiopalm: A
large language model that can speak and listen,” arXiv
preprint arXiv:2306.12925, 2023.
[18] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,
Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,
Huaming Wang, Jinyu Li, et al.,
“Neural codec lan-
guage models are zero-shot text to speech synthesizers,”
arXiv preprint arXiv:2301.02111, 2023.
[19] Zihao Fu, Wai Lam, Qian Yu, Anthony Man-Cho
So, Shengding Hu, Zhiyuan Liu, and Nigel Collier,
“Decoder-only or encoder-decoder?
interpreting lan-
guage model as a regularized encoder-decoder,” arXiv
preprint arXiv:2304.04052, 2023.
[20] Marco Gaido, Mauro Cettolo, Matteo Negri, and Marco
Turchi, “Ctc-based compression for direct speech trans-
lation,” arXiv preprint arXiv:2102.01578, 2021.
[21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen, “Lora: Low-rank adaptation of large lan-
guage models,” arXiv preprint arXiv:2106.09685, 2021.
[22] Laura Cross Vila, Carlos Escolano, Jos´e AR Fonollosa,
and Marta R Costa-Jussa,
“End-to-end speech trans-
lation with the transformer.,” in Proceedings of Inter-
speech, 2018, pp. 60–63.
[23] Matthias Sperber and Matthias Paulik, “Speech transla-
tion and the end-to-end promise: Taking stock of where
we are,”
in Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics, 2020,
pp. 7409–7421.
[24] Jian Xue, Peidong Wang, Jinyu Li, and Eric Sun,
“A weakly-supervised streaming multilingual speech
model with truly zero-shot capability,” arXiv preprint
arXiv:2211.02499, 2022.
[25] Changhan Wang, Anne Wu, and Juan Pino,
“Covost
2: A massively multilingual speech-to-text translation
corpus,” 2020.
[26] Matt Post, “A call for clarity in reporting BLEU scores,”
in Proceedings of the Third Conference on Machine
Translation: Research Papers, Brussels, Belgium, Oct.
2018, pp. 186–191, Association for Computational Lin-
guistics.
[27] A. Berard, O. Pietquin, C. Servan, and L. Besacier,
“Listen and translate: A proof of concept for end-to-
end speech-to-text translation,” in NIPS Workshop on
End-to-end Learning for Speech and Audio Processing,
2016.
[28] Ron J Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui
Wu, and Zhifeng Chen, “Sequence-to-sequence models
can directly translate foreign speech,” Proc. Interspeech,
pp. 2625–2629, 2017.
[29] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-
man, Christine McLeavey, and Ilya Sutskever, “Robust
speech recognition via large-scale weak supervision,” in
International Conference on Machine Learning. PMLR,
2023, pp. 28492–28518.
[30] Takaaki Hori, Shinji Watanabe, and John Hershey,
“Joint CTC/attention decoding for end-to-end speech
recognition,” in Proceedings of the 55th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), Vancouver, Canada, July 2017,
pp. 518–529, Association for Computational Linguis-
tics.
[31] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu, “Roformer: Enhanced trans-
former with rotary position embedding,” arXiv preprint
arXiv:2104.09864, 2021.
[32] Yuang Li, Yu Wu, Jinyu Li, and Shujie Liu, “Prompt-
ing large language models for zero-shot domain
adaptation in speech recognition,”
arXiv preprint
arXiv:2306.16007, 2023.

