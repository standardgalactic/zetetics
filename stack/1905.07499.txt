LR-GLM: High-dimensional Bayesian
inference using low-rank data approximations
The MIT Faculty has made this article openly available. Please share 
how this access benefits you. Your story matters.
Citation
Trippe, Brian L. et al. “LR-GLM: High-dimensional Bayesian
inference using low-rank data approximations.” Proceedings of the
36th International Conference on Machine Learning, 97 (May 2019):
6315-6324 © 2019 The Author(s)
As Published
http://proceedings.mlr.press/v97/trippe19a.html
Publisher
MIT Press
Version
Author's final manuscript
Citable link
https://hdl.handle.net/1721.1/128775
Terms of Use
Creative Commons Attribution-Noncommercial-Share Alike
Detailed Terms
http://creativecommons.org/licenses/by-nc-sa/4.0/

LR-GLM: High-Dimensional Bayesian Inference Using Low-Rank Data
Approximations
Brian L. Trippe 1 Jonathan H. Huggins 2 Raj Agrawal 1 Tamara Broderick 1
Abstract
Due to the ease of modern data collection, applied
statisticians often have access to a large set of co-
variates that they wish to relate to some observed
outcome. Generalized linear models (GLMs) of-
fer a particularly interpretable framework for such
an analysis. In these high-dimensional problems,
the number of covariates is often large relative to
the number of observations, so we face non-trivial
inferential uncertainty; a Bayesian approach al-
lows coherent quantiﬁcation of this uncertainty.
Unfortunately, existing methods for Bayesian in-
ference in GLMs require running times roughly
cubic in parameter dimension, and so are limited
to settings with at most tens of thousand parame-
ters. We propose to reduce time and memory costs
with a low-rank approximation of the data in an
approach we call LR-GLM. When used with the
Laplace approximation or Markov chain Monte
Carlo, LR-GLM provides a full Bayesian poste-
rior approximation and admits running times re-
duced by a full factor of the parameter dimension.
We rigorously establish the quality of our approx-
imation and show how the choice of rank allows
a tunable computational–statistical trade-off. Ex-
periments support our theory and demonstrate the
efﬁcacy of LR-GLM on real large-scale datasets.
1. Introduction
Scientists, engineers, and social scientists are often inter-
ested in characterizing the relationship between an outcome
and a set of covariates, rather than purely optimizing pre-
dictive accuracy. For example, a biologist may wish to
understand the effect of natural genetic variation on the
presence of a disease or a medical practitioner may wish to
understand the effect of a patient’s history on their future
1Computer Science and Artiﬁcial Intelligence Laboratory, Mas-
sachusetts Institute of Technology, Cambridge, MA 2Department
of Biostatistics, Harvard, Cambridge, MA. Correspondence to:
Brian L. Trippe <btrippe@mit.edu>.
health. In these applications and countless others, the rel-
ative ease of modern data collection methods often yields
particularly large sets of covariates for data analysts to study.
While these rich data should ultimately aid understanding,
they pose a number of practical challenges for data analysis.
One challenge is how to discover interpretable relationships
between the covariates and the outcome. Generalized lin-
ear models (GLMs) are widely used in part because they
provide such interpretability – as well as the ﬂexibility to
accommodate a variety of different outcome types (includ-
ing binary, count, and heavy-tailed responses). A second
challenge is that, unless the number of data points is sub-
stantially larger than the number of covariates, there is likely
to be non-trivial uncertainty about these relationships.
A Bayesian approach to GLM inference provides the desired
coherent uncertainty quantiﬁcation as well as favorable cal-
ibration properties (Dawid, 1982, Theorem 1). Bayesian
methods additionally provide the ability to improve infer-
ence by incorporating expert information and sharing power
across experiments. Using Bayesian GLMs leads to com-
putational challenges, however. Even when the Bayesian
posterior can be computed exactly, conjugate inference costs
O(N 2D) in the case of D ≫N. And most models are suf-
ﬁciently complex as to require expensive approximations.
In this work, we propose to reduce the effective dimension-
ality of the feature set as a pre-processing step to speed
up Bayesian inference, while still performing inference in
the original parameter space; in particular, we show that
low-rank descriptions of the data permit fast Markov chain
Monte Carlo (MCMC) samplers and Laplace approxima-
tions of the Bayesian posterior for the full feature set. We
motivate our proposal with a conjugate linear regression
analysis in the case where the data are exactly low-rank.
When the data are merely approximately low-rank, our pro-
posal is an approximation. Through both theory and experi-
ments, we demonstrate that low-rank data approximations
provide a number of properties that are desirable in an efﬁ-
cient posterior approximation method: (1) soundness: our
approximations admit error bounds directly on the quantities
that practitioners report as well as practical interpretations
of those bounds; (2) tunability: the choice of the rank of the
approximation deﬁnes a tunable trade-off between the com-
arXiv:1905.07499v1  [stat.CO]  17 May 2019

LR-GLM: fast GLMs with low-rank approximations
Table 1. Time complexities of naive inference and LR-GLM with
a rank M approximation when D ≥N.
METHOD
NAIVE
LR-GLM
SPEEDUP
LAPLACE
O(N 2D)
O(NDM)
N/M
MCMC (ITER.)
O(ND)
O(NM + DM)
N/M
putational demands of inference and statistical precision;
and (3) conservativeness: our approximation never reports
less uncertainty than the exact posterior, where uncertainty
is quantiﬁed via either posterior variance or information
entropy. Together, these properties allow a practitioner to
choose how much information to extract from the data on
the basis of computational resources while being able to
conﬁdently trust the conclusions of their analysis.
2. Bayesian inference in GLMs
Suppose we have N data points {(xn, yn)}N
n=1. We collect
our covariates, where xn has dimension D, in the design
matrix X ∈RN×D and our responses in the column vector
Y ∈RN. Let β ∈RD be an unknown parameter char-
acterizing the relationship between the covariates and the
response for each data point. In particular, we take β to
parameterize a GLM likelihood p(Y | X, β) = p(Y | Xβ).
That is, βd describes the effect size of the dth covariate
(e.g., the inﬂuence of a non-reference allele on an indi-
vidual’s height in a genomic association study).
Com-
pleting our Bayesian model speciﬁcation, we assume a
prior p(β), which describes our knowledge of β before
seeing data. Bayes’ theorem gives the Bayesian posterior
p(β | Y, X) = p(β)p(Y | Xβ)/
R
p(β′)p(Y | Xβ′)dβ′,
which captures the updated state of our knowledge after
observing data. We often summarize the β posterior via
its mean and covariance. In all but the simplest settings,
though, computing these posterior summaries is analytically
intractable, and these quantities must be approximated.
Related work. In the setting of large D and large N, ex-
isting Bayesian inference methods for GLMs may lead to
unfavorable trade-offs between accuracy and computation;
see Appendix B for further discussion. While Markov chain
Monte Carlo (MCMC) can approximate Bayesian GLM pos-
teriors arbitrarily well given enough time, standard methods
can be slow, with O(DN) time per likelihood evaluation.
Moreover, in practice, mixing time may scale poorly with
dimension and sample size; algorithms thus require many
iterations and hence many likelihood evaluations. Subsam-
pling MCMC methods can speed up inference, but they are
effective only with tall data (D ≪N; Bardenet et al., 2017).
An alternative to MCMC is to use a deterministic approxi-
mation such as the Laplace approximation (Bishop, 2006,
Chap. 4.4), integrated nested Laplace approximation (Rue
et al., 2009), variational Bayes (VB; Blei et al., 2017), or an
alternative likelihood approximation (Huggins et al., 2017;
Campbell & Broderick, 2019; Huggins et al., 2016). How-
ever these methods are computationally efﬁcient only when
D ≪N (and in some cases also when N ≪D). For exam-
ple, the Laplace approximation requires inverting the Hes-
sian, which uses O(min(N, D)ND) time (Appendix C).
Improving computational tractability by, for example, using
a mean ﬁeld approximation with VB or a factorized Laplace
approximation can produce substantial bias and uncertainty
underestimation (MacKay, 2003; Turner & Sahani, 2011).
A number of papers have explored using random projections
and low-rank approximations in both Bayesian (Lee & Oh,
2013; Spantini et al., 2015; Guhaniyogi & Dunson, 2015;
Geppert et al., 2017) and non-Bayesian (Zhang et al., 2014;
Wang et al., 2017) settings. The Bayesian approaches have a
variety of limitations. E.g., Lee & Oh (2013); Geppert et al.
(2017); Spantini et al. (2015) give results only for certain
conjugate Gaussian models. And Guhaniyogi & Dunson
(2015) provide asymptotic guarantees for prediction but do
not address parameter estimation.
See Section 6 for a demonstration of the empirical disad-
vantages of mean ﬁeld VB, factored Laplace, and random
projections in posterior inference.
3. LR-GLM
The intuition for our low-rank GLM (LR-GLM) approach
is as follows.
Supervised learning problems in high-
dimensional settings often exhibit strongly correlated covari-
ates (Udell & Townsend, 2019). In these cases, the data may
provide little information about the parameter along certain
directions of parameter space. This observation suggests
the following procedure: ﬁrst identify a relatively lower-
dimensional subspace within which the data most directly
inform the posterior, and then perform the data-dependent
computations of posterior inference (only) within this sub-
space, at lower computational expense. In the context of
GLMs with Gaussian priors, the singular value decomposi-
tion (SVD) of the design matrix X provides a natural and
effective mechanism for identifying a subspace. We will
see that this perspective gives rise to simple, efﬁcient, and
accurate approximate inference procedures. In models with
non-Gaussian priors the approximation enables more efﬁ-
cient inference by facilitating faster likelihood evaluations.
Formally, the ﬁrst step of LR-GLM is to choose an integer
M such that 0 < M < D. For any real design matrix X,
its SVD exists and may be written as
X⊤= Udiag(λ)V ⊤+ ¯Udiag(¯λ) ¯V ⊤,
where U ∈RD×M, ¯U ∈RD×(D−M), V ∈RN×M, and
¯V ∈RN×(D−M) are matrices of orthonormal rows, and
λ ∈RM and ¯λ ∈RD−M are vectors of non-increasing

LR-GLM: fast GLMs with low-rank approximations
Figure 1. LR-Laplace with a rank-1 data approximation closely matches the Bayesian posterior of a toy logistic regression model. In
each pair of plots, the left panel depicts the same 2-dimensional dataset with points in two classes (black and white dots) and decision
boundaries (black lines) separating the two classes, which are sampled from the given posterior approximation (see title for each pair). In
the right panel, the red contours represent the marginal posterior approximation of the parameter β (a bias parameter is integrated out).
singular values λ1 ≥· · · ≥λM ≥¯λ1 ≥· · · ≥¯λD−M ≥0.
We replace X with the low-rank approximation XUU ⊤.
Note that the resulting posterior approximation ˜p(β | X, Y )
is still a distribution over the full D-dimensional β vector:
˜p(β | X, Y ) :=
p(β)p(Y | XUU ⊤β)
R
p(β′)p(Y | XUU ⊤β′)dβ′
(1)
In this way, we cast low-rank data approximations for ap-
proximate Bayesian inference as a likelihood approximation.
This perspective facilitates our analysis of posterior approx-
imation quality and provides the ﬂexibility either to use the
likelihood approximation in an otherwise exact MCMC al-
gorithm or to make additional fast approximations such as
the Laplace approximation.
We let LR-Laplace denote the combination of LR-GLM
and the Laplace approximation. Figure 1 illustrates LR-
Laplace on a toy problem and compares it to full Laplace,
the prior, and diagonal Laplace. Diagonal Laplace refers to
a factorized Laplace approximation in which the Hessian
of the log posterior is approximated with only its diagonal.
While this example captures some of the essence of our
proposed approach, we emphasize that our focus in this
paper is on problems that are high-dimensional.
4. Low-rank data approximations for
conjugate Gaussian regression
We now consider the quality of approximate Bayesian infer-
ence using our LR-GLM approach in the case of conjugate
Gaussian regression. We start by assuming that the data is
exactly low rank since it most cleanly illustrates the compu-
tational gains from LR-GLM. We then move on to the case
of conjugate regression with approximately low-rank data
and rigorously characterize the quality of our approximation
via interpretable error bounds. We consider non-conjugate
GLMs in Section 5. We defer all proofs to the Appendix.
4.1. Conjugate regression with exactly low-rank data
Classic linear regression ﬁts into our GLM likelihood frame-
work with p(Y |X, β) = N(Y |Xβ, (τIN)−1), where τ > 0
is the precision and IN is the identity matrix of size N.
For the conjugate prior p(β) = N(β|0, Σβ), we can write
the posterior in closed form: p(β|Y, X) = N(β|µN, ΣN),
where ΣN := (Σ−1
β
+ τXT X)−1 and µN := τΣNX⊤Y.
While conjugacy avoids the cost of approximating Bayesian
inference, it does not avoid the often prohibitive O(ND2 +
D3) cost of calculating ΣN (which requires computing and
then inverting Σ−1
N ) and the O(D2) memory demands of
storing it. In the N ≪D setting, these costs can be miti-
gated by using the Woodbury formula to obtain µN and ΣN
in O(N 2D) time with O(ND) memory (Appendix C). But
this alternative becomes computationally prohibitive as well

LR-GLM: fast GLMs with low-rank approximations
when both N and D are large (e.g., D ≈N > 20, 000).
Now suppose that X is rank M ≪min(D, N) and can
therefore be written as X = XUU T exactly, where U ∈
RD×M denotes the top M right singular vectors of X. Then,
if Σβ = σ2
βID and 1M is the ones vector of length M, we
can write (see Appendix D.1 for details)
ΣN = σ2
β
(
I −Udiag
 
τλ ⊙λ
σ−2
β 1M + τλ ⊙λ
!
U ⊤
)
and
µN = Udiag
 
τλ
σ−2
β 1M + τλ ⊙λ
!
V ⊤Y,
(2)
where multiplication (⊙) and division in the diag input are
component-wise across the vector λ. Eq. (2) provides a
more computationally efﬁcient route to inference.
The
singular vectors in U may be obtained in O(ND log M)
time via a randomized SVD (Halko et al., 2011) or in
O(NDM) time using more standard deterministic meth-
ods (Press et al., 2007). The bottleneck step is ﬁnding λ
via diag(λ ⊙λ) = U ⊤X⊤XU, which can be computed
in O(NDM) time. As for storage, this approach requires
keeping only U, λ, and V ⊤Y , which takes just O(MD)
space. In sum, utilizing low-rank structure via Eq. (2) pro-
vides an order min(N, D)/M-fold improvement in both
time and memory over more naive inference.
4.2. Conjugate regression with low-rank
approximations
While the case with exactly low-rank data is illustrative,
real data are rarely exactly low rank. So, more generally,
LR-GLM will yield an approximation N(β|˜µN, ˜ΣN) to the
posterior N(β|µN, ΣN), rather than the exact posterior as
in Section 4.1. We next provide upper bounds on the error
from our approximation. Since practitioners typically report
posterior means and covariances, we focus on how well
LR-GLM approximates these functionals.
Theorem 4.1. For conjugate Bayesian linear regression,
the LR-GLM approximation Eq. (1) satisﬁes
∥˜µN −µN∥2 ≤
¯λ1
 ¯λ1∥¯U ⊤˜µN∥2 + ∥¯V ⊤Y ∥2

∥τΣβ∥−1
2
+ ¯λ2
D−M
(3)
and
Σ−1
N −˜Σ−1
N = τ(X⊤X −UU ⊤X⊤XUU ⊤). (4)
In particular, ∥Σ−1
N −˜Σ−1
N ∥2 = τ ¯λ2
1.
The major driver of the approximation error of the posterior
mean and covariance is ¯λ1 = ∥X −XUU ⊤∥2, the largest
truncated singular value of X. This result accords with the
intuition that if the data are “approximately low-rank” then
LR-GLM should perform well.
The following corollary shows that the posterior mean esti-
mate is not, in general, consistent for the true parameter. But
it does exhibit reasonable asymptotic behavior. In particular,
˜µN is consistent within the span of U and converges to the
a priori most probable vector with this characteristic (see
the toy example in Figure E.1).
Corollary 4.2. Suppose xn
i.i.d.
∼p∗, for some distribution
p∗, and yn | xn
indep
∼N(x⊤
n µ∗, τ −1), for some µ∗∈RD.
Assume Ep∗[xnx⊤
n ] is nonsingular.
Let the columns of
U∗∈RD×M be the top eigenvectors of Ep∗[xnxT
n]. Then
˜µN converges weakly to the maximum a priori vector ˜µ
satisfying U ⊤
∗˜µ = U ⊤
∗µ∗.
In the special case that Σβ is diagonal this result implies
that ˜µN
p→U∗U ⊤
∗µ∗(Appendices E.3 and F.2). Thus Corol-
lary 4.2 reﬂects the intuition that we are not learning any-
thing about the relation between response and covariates in
the data directions that we truncate away with our approach.
If the response has little dependence on these directions,
¯U∗¯U ⊤
∗µ∗= limN→∞˜µN −µ∗will be small and the er-
ror in our approximation will be low (Appendix E.3). If
the response depends heavily on these directions, our error
will be higher. This challenge is ubiquitous in dealing with
projections of high-dimensional data. Indeed, we often see
explicit assumptions encoding the notion that high-variance
directions in X are also highly predictive of the response
(see, e.g., Zhang et al., 2014, Theorem 2).
Our next corollary captures that LR-GLM never underesti-
mates posterior uncertainty (the conservativeness property).
Corollary 4.3. LR-GLM approximate posterior uncertainty
in any linear combination of parameters is no less than
the exact posterior uncertainty. Equivalently, ˜ΣN −ΣN is
positive semi-deﬁnite.
See Figure 1 for an illustration of this result. From an ap-
proximation perspective, overestimating uncertainty can be
seen as preferable to underestimation as it leads to more
conservative decision-making. An alternative perspective
is that we actually engender additional uncertainty simply
by making an approximation, with more uncertainty for
coarser approximations, and we should express that in re-
porting our inferences. This behavior stands in sharp con-
trast to alternative fast approximate inference methods, such
as diagonal Laplace approximations (Appendix F.8) and
variational Bayes (MacKay, 2003), which can dramatically
underestimate uncertainty. We further characterize the con-
servativeness of LR-GLM in Corollary E.1, which shows
that the LR-GLM posterior never has lower entropy than the
exact posterior and quantiﬁes the bits of information lost
due to approximation.
1This manipulation is purely symbolic. See Appendix F.1 for
details.

LR-GLM: fast GLMs with low-rank approximations
Algorithm 1 LR-Laplace for Bayesian inference in GLMs with low-rank data approximations and zero-mean prior – with
computation costs. See Appendix H for the general algorithm.
1: Input: prior p(β) = N(0, Σβ), data X ∈RN,D, rank M ≪D, GLM mapping φ with ⃗φ′′ (see Eq. (5) and Section 5.1)
2: Pseudo-Code
3: Time Complexity
4: Memory Complexity
5: Data preprocessing — M-Truncated SVD
6: U, diag(λ), V := truncated-SVD(XT , M)
O(NDM)
O(NM + DM)
7: Optimize in projected space and ﬁnd approximate MAP estimate
8: γ∗:= arg maxγ∈RM PN
i=1 φ(yi, xiUγ) −1
2γ⊤U ⊤ΣβUγ
O(NM + DM 2)
O(N + M 2)
9: ˆµ = Uγ∗+ ¯U ¯U ⊤ΣβU(U ⊤ΣβU)−1γ∗
O(DM)
O(DM)
10: Compute approximate posterior covariance
11: W −1 := U ⊤ΣβU −(U ⊤X⊤diag(⃗φ′′(Y, XUU ⊤ˆµ))XU)−1
O(NM 2 + DM)
O(NM)
12: ˆΣ := Σβ −ΣβUWU ⊤Σβ
0 (see footnote1)
O(DM)
13: Compute variances and covariances of parameters
14: Varˆp(βi) = e⊤
i ˆΣei
O(M 2)
O(DM)
15: Covˆp(βi, βj) = e⊤
i ˆΣej
O(M 2)
O(DM)
5. Non-conjugate GLMs with approximately
low-rank data
While the conjugate linear setting facilitates intuition and
theory, GLMs are a larger and more broadly useful class of
models for which efﬁcient and reliable Bayesian inference
is of signiﬁcant practical concern. Assuming conditional
independence of the observations given the covariates and
parameter, the posterior for a GLM likelihood can be written
log p(β | X, Y ) = log p(β) +
N
X
n=1
φ(yn, x⊤
n β) + Z
(5)
for some real-valued mapping function φ and log normal-
izing constant Z. For priors and mapping functions that do
not form a conjugate pair, accessing posterior functionals
of interest is analytically intractable and requires posterior
approximation. One possibility is to use a Monte Carlo
method such as MCMC, which has theoretical guarantees
asymptotic in running time but is relatively slow in prac-
tice. The usual alternative is a deterministic approximation
such as VB or Laplace. These approximations are typically
faster but do not become arbitrarily accurate in the limit
of inﬁnite computation. We next show how LR-GLM can
be applied to facilitate faster MCMC samplers and Laplace
approximations for Bayesian GLMs. We also characterize
the additional error introduced to Laplace approximations
by low-rank data approximations.
5.1. LR-GLM for fast Laplace approximations
The Laplace approximation refers to a Gaussian approxi-
mation obtained via a second-order Taylor approximation
of the log density. In the Bayesian setting, the Laplace ap-
proximation ¯p(β | X, Y ) is typically formed at the max-
imum a posteriori (MAP) parameter: ¯p(β | X, Y ) :=
N(β | ¯µ, ¯Σ), where ¯µ := arg maxβ log p(β | X, Y )
and ¯Σ−1 := −∇2
β log p(β | X, Y )|β=¯µ. When comput-
ing and analyzing Laplace approximations for GLMs, we
will often refer to vectorized ﬁrst, second, and third deriva-
tives ⃗φ′, ⃗φ′′, ⃗φ′′′ ∈RN of the mapping function φ. For
Y, A ∈RN, we deﬁne ⃗φ′(Y, A)n :=
∂
∂aφ(Yn, a)|a=An.
The higher-order derivative deﬁnitions are analogous, with
the derivative order of
∂
∂a increased commensurately.
Laplace approximations are typically much faster than
MCMC for moderate/large N and small D, but they be-
come expensive or intractable for large D. In particular,
they require inverting a D × D Hessian matrix, which is in
general an O(D3) time operation, and storing the resulting
covariance matrix, which requires O(D2) memory.2
As in the conjugate case, LR-GLM permits a faster and
more memory-efﬁcient route to inference. Here, we say
that the LR-Laplace approximation, ˆp(β | X, Y ) = N(β |
ˆµ, ˆΣ), denotes the Laplace approximation to the LR-GLM
approximate posterior. The special case of LR-Laplace with
zero-mean prior is given in Algorithm 1 as it allows us to
easily analyze time and memory complexity. For the more
general LR-Laplace algorithm, see Appendix H.
Theorem 5.1. In a GLM with a zero-mean, structured-
Gaussian prior3 and a log-concave likelihood,4 the rank-
2Notably, as in the conjugate setting, an alternative matrix
inversion using the Woodbury identity reduces this cost when
N < D to O(N 2D) time and O(ND) memory (Appendix C).
3For example (banded) diagonal or diagonal plus low-rank,
such that matrix vector multiplies may be computed in O(D) time.
4This property is standard for common GLMs such as logistic

LR-GLM: fast GLMs with low-rank approximations
M LR-Laplace approximation may be computed via Algo-
rithm 1 in O(NDM) time with O(DM + NM) memory.
Furthermore, any posterior covariance entry can be com-
puted in O(M 2) time.
Algorithm 1 consists of three phases: (1) computation of the
M-truncated SVD of X⊤; (2) MAP optimization to ﬁnd ˆµ;
and (3) estimation of ˆΣ. In the second phase we are able to
efﬁciently compute ˆµ by ﬁrst solving a lower-dimensional
optimization for the quantity γ∗∈RM (Line 8), from which
ˆµ is available analytically. Notably, in the common case
that p(β) is isotropic Gaussian, the expression for ˆµ reduces
to Uγ∗and the full time complexity of MAP estimation
is O(NM + DM). Though computing the covariance for
each pair of parameters and storing ˆΣ explicitly would of
course require a potentially unacceptable O(D2) storage,
the output of Algorithm 1 is smaller and enables arbitrary
parameter variances and covariances to be computed in
O(M 2) time. See Appendix F.1 for additional details.
5.2. Accuracy of the LR-Laplace approximation
We now consider the quality of the LR-Laplace approximate
posterior relative to the usual Laplace approximation. Our
ﬁrst result concerns the difference of the posterior means
Theorem 5.2 (Non-asymptotic). In a generalized linear
model with an α–strongly log concave posterior, the exact
and approximate MAP values, ˆµ = arg maxβ ˜p(β | X, Y )
and ¯µ = arg maxβ p(β | X, Y ), satisfy
∥ˆµ−¯µ∥2 ≤
¯λ1
 ∥⃗φ′(Y, X ˆµ)∥2 + λ1∥¯U ⊤ˆµ∥2∥⃗φ′′(Y, A)∥∞

α
for some vector A ∈RN such that An ∈[x⊤
n UU ⊤ˆµ, x⊤
n ˆµ].
This bound reveals several characteristics of the regimes in
which LR-Laplace performs well. As in conjugate regres-
sion, we see that the bound tightens to 0 as the rank of the
approximation increases to capture all of the variance in the
covariates and ¯λ1 →0.
Remark 5.3. For many common GLMs, ∥⃗φ′∥2, ∥⃗φ′′∥∞, and
∥⃗φ′′′∥∞are well controlled; see Appendix F.4. ∥⃗φ′′′∥∞
appears in an upcoming corollary.
Remark 5.4. The α–strong log concavity of the posterior is
satisﬁed for any strongly log concave prior (e.g., a Gaussian,
in which case we have α ≥∥Σβ∥−1
2 ) and φ(y, ·) is concave
for all y. In this common case, Theorem 5.2 provides a
computable upper bound on the posterior mean error.
Remark 5.5. In contrast to the conjugate case (Corol-
lary 4.2), general LR-GLM parameter estimates are not
necessarily consistent within the span of the projection. That
is, U ⊤ˆµN may not converge to U ⊤β (see Appendix F.5).
and Poisson regression.
We next consider the distance between our approximation
and target posterior under a Wasserstein metric (Villani,
2008). Let Γ(ˆp, ¯p) be the set of all couplings of distributions
ˆp and ¯p, i.e. joint distributions γ(·, ·) satisfying ˆp(β) =
R
γ(β, β′)dβ′ and ¯p(β) =
R
γ(β′, β)dβ′ for all β. Then
the 2-Wasserstein distance between ˆp and ¯p is deﬁned
W2(ˆp, ¯p) =
inf
γ∈Γ(ˆp,¯p) Eγ[∥ˆβ −¯β∥2
2]
1
2 .
(6)
Wasserstein bounds provide tight control of many function-
als of interest, such as means, variances, and standard devia-
tions (Huggins et al., 2018). For example, if ξi ∼qi for any
distribution qi (i = 1, 2), then |E[ξ1]−E[ξ2]| ≤W2(q1, q2)
and |Var[ξ1]
1
2 −Var[ξ2]
1
2 | ≤2W2(q1, q2).
We provide a ﬁnite-sample upper bound on the 2-
Wasserstein distance between the Laplace and LR-Laplace
approximations. In particular, the 2-Wasserstein will de-
crease to 0 as the rank of the LR-Laplace approximation
increases since the largest truncated singular value ¯λ1 will
approach zero.
Corollary 5.6. Assume the prior p(β) is Gaussian with co-
variance Σβ and the mapping function φ(y, a) has bounded
2nd and 3rd derivatives with respect to a. Take A and α as
in Theorem 5.2. Then ¯p(β) and ˆp(β) satisfy
W2(ˆp, ¯p) ≤
√
2¯λ1∥¯Σ∥2
n
c

∥Σ−1
β ∥2 + (λ1 + ¯λ1)2∥⃗φ′′∥∞

+ (λ2
1r + (¯λ1 + 2λ1)∥⃗φ′′∥∞
q
tr(ˆΣ)
o
,
(7)
where c :=
 ∥⃗φ′(Y, X ˆµ)∥2 +λ1∥¯U ⊤ˆµ∥2∥⃗φ′′(Y, A)∥∞

/α
and r := ∥U ⊤ˆµ∥∞∥⃗φ′′′∥∞+ λ1c∥⃗φ′′′∥∞.
When combined with Huggins et al. (2018, Prop. 6.1), this
result guarantees closeness in 2-Wasserstein of LR-Laplace
to the exact posterior.
We conclude with a result showing that the error due to the
LR-GLM approximation cannot grow without bound as the
sample size increases.
Theorem 5.7 (Asymptotic). Under mild regularity condi-
tions, the error in the posterior means, ∥ˆµn −¯µn∥2, con-
verges as n →∞, and the limit is ﬁnite almost surely.
For the formal statement see Theorem F.2 in Appendix F.7.
5.3. LR-MCMC for faster MCMC in GLMs
LR-Laplace is inappropriate when the posterior is poorly ap-
proximated by a Gaussian. This may be the case, for exam-
ple, when the posterior is multi-modal, a common character-
istic of GLMs with sparse priors. To remedy this limitation
of LR-Laplace, we introduce LR-MCMC, a wrapper around
the Metropolis–Hastings algorithm using the LR-GLM ap-
proximation. For a GLM, each full likelihood and gradient

LR-GLM: fast GLMs with low-rank approximations
computation takes O(ND) time but only O(NM + DM)
time with the LR-GLM approximation, resulting in the same
min(N, D)/M-fold speedup obtained by LR-Laplace. See
Appendix G for further details on LR-MCMC.
6. Experiments
We empirically evaluated LR-GLM on real and synthetic
datasets. For synthetic data experiments, we considered lo-
gistic regression with covariates of dimension D = 250 and
D = 500. In each replicate, we generated the latent param-
eter from an isotropic Gaussian prior, β ∼N(0, ID), corre-
lated covariates from a multivariate Gaussian, and responses
from the logistic regression likelihood (see Appendix A.1
for details). We compared to the standard Laplace approx-
imation, the diagonal Laplace approximation, the Laplace
approximation with a low-rank data approximation obtained
via random projections rather than the SVD (“Random-
Laplace”), and mean-ﬁeld automatic differentiation varia-
tional inference in Stan (ADVI-MF).5
Computational–statistical trade-offs. Figure 2A shows
empirically the tunable computational–statistical trade-off
offered by varying M in our low-rank data approximation.
This plot depicts the error in posterior mean and variance
estimates relative to results from the No-U-Turn Sampler
(NUTS) in Stan (Hoffman & Gelman, 2014; Carpenter
et al., 2017), which we treat as ground truth. As expected,
LR-Laplace with larger M takes longer to run but yields
lower errors. Random-Laplace was usually faster but pro-
vided a poor posterior approximation. Interestingly, the
error of the Random-Laplace approximate posterior mean
actually increased with the dimension of the projection. We
conjecture this behavior may be due to Random-Laplace
prioritizing covariate directions that are correlated with di-
rections where the parameter, β, is large.
We also consider predictive performance via the classiﬁ-
cation error rate and the average negative log likelihood.
In particular, we generated a test dataset with covariates
drawn from the same distribution as the observed dataset
and an out-of-sample dataset with covariates drawn from a
different distribution (see Appendix A.1). The computation
time vs. performance trade-offs, presented in Figure A.1 on
the test and out-of-sample datasets, mirror the results for
approximating the posterior mean and variances. In this
evaluation, correctly accounting for posterior uncertainty
appears less important for in-sample prediction. But in the
out-of-sample case, we see a dramatic difference in negative
log likelihood. Notably, ADVI-MF and diagonal Laplace
exhibit much worse performance. These results support the
5We also tested ADVI using a full rank Gaussian approxima-
tion but found it to provide near uniformly worse performance
compared to ADVI-MF. So we exclude full-rank ADVI from the
presented results.
utility of correctly estimating Bayesian uncertainty when
making out-of-sample predictions.
Conservativeness. A beneﬁt of LR-GLM is that the poste-
rior approximation never underestimates the posterior uncer-
tainty (see Corollary 4.3). Figure 2C illustrates this property
for LR-Laplace applied to logistic regression. When LR-
Laplace misestimates posterior variances, it always overes-
timates. Also, when LR-Laplace misestimates means (Fig-
ure A.2), the estimates shrink closer to the prior mean, zero
in this case. These results suggest that LR-GLM interpo-
lates between the exact posterior and the prior. Notably, this
property is not true of all methods. The diagonal Laplace
approximation, by contrast, dramatically underestimates
posterior marginal variances (see Appendix F.8).
Reliability and calibration. Bayesian methods enjoy desir-
able calibration properties under correct model speciﬁcation.
But since LR-Laplace serves as a likelihood approximation,
it does not retain this theoretical guarantee. Therefore, we
assessed its calibration properties empirically by examining
the credible sets of both parameters and predictions. We
found that the parameter credible sets of LR-Laplace are ex-
tremely well calibrated for all values of M between 20 and
400 (Figure 2B and Figure A.4). The prediction credible
sets were well calibrated for all but the smallest value of M
tested (M = 20); in the M = 20 case, LR-Laplace yielded
under-conﬁdent predictions (Figure A.5). The good calibra-
tion of LR-Laplace stood in sharp contrast to the diagonal
Laplace approximation and ADVI-MF. Random-Laplace
also provided inferior calibration (Figures A.4 and A.5).
LR-GLM with MCMC and non-Gaussian priors.
In
Section 5.3 we argued that LR-GLM speeds up MCMC
for GLMs by decreasing the cost of likelihood and gradi-
ent evaluations in black-box MCMC routines. We ﬁrst
examined LR-MCMC with NUTS using Stan on the
same synthetic datasets as we did for LR-Laplace. In Fig-
ures A.3 and A.6, we see a similar conservativeness and
computational–statistical trade-off as for LR-Laplace, and
superior performance relative to alternative methods.
We expect MCMC to yield high-quality posterior approx-
imations across a wider range of models than Laplace ap-
proximations. For example, for multimodal posteriors and
other posteriors that deviate substantially from Gaussian-
ity. We next demonstrate that LR-MCMC is useful in these
more general cases. In high-dimensional settings, practi-
tioners are often interested in identifying a sparse subset
of parameters that signiﬁcantly inﬂuence responses. This
belief may be incorporated in a Bayesian setting through a
sparsity-inducing prior such as the spike and slab prior or
the horseshoe (George & McCulloch, 1993; Carvalho et al.,
2009). However, posteriors in these cases may be multi-
modal, and scalable Bayesian inference with such priors is
a challenging, active area of research (Guan & Stephens,

LR-GLM: fast GLMs with low-rank approximations
Figure 2. Left: Error of the approximate posterior (A1.) mean and (A2.) variances relative to ground truth (running NUTS with Stan).
Lower and further left is better. Right (B.): Credible set calibration across all parameters and repeated experiments. (C.): Approximate
posterior standard deviations for a subset of parameters. The grey line reﬂects zero error.
2011; Yang et al., 2016; Johndrow et al., 2017). To demon-
strate the applicability of low-rank data approximations to
this setting, we ran NUTS using Stan on a logistic regres-
sion model with a regularized horseshoe prior (Carvalho
et al., 2009; Piironen & Vehtari, 2017). In Figure A.7, we
see an attractive trade-off between computational invest-
ment and approximation error. For example, we obtained
relative mean and standard deviation errors of only about
10−2 while reducing computation time by a factor of three.
We also applied LR-MCMC to linear regression with the
regularized horseshoe prior on a dataset with very correlated
covariates and D = 6,238. However, this sampler exhibited
severe mixing problems, both with and without the approxi-
mation, as diagnosed by large ˆR values in pyStan. These
issues reﬂect the innate challenges of high-dimensional
Bayesian inference with the horseshoe prior and correlated
covariates.
Scalability to large-scale real datasets. Finally, we ex-
plored the applicability of LR-Laplace to two real, large-
scale logistic regression tasks (Figure 3). The ﬁrst is the UCI
Farm-Ads dataset, which consists of N = 4,143 online ad-
vertisements for animal-related topics together with binary
labels indicating whether the content provider approved of
the ad; there are D = 54,877 bag-of-words features per ad
(Dheeru & Karra Taniskidou, 2017). As with the synthetic
datasets, we evaluated the error in the approximations of
posterior means and variances. As a baseline to evaluate this
error, we use the usual Laplace approximation because the
computational demands of MCMC preclude the possibility
of using it as a baseline.
As a second real dataset we evaluated our approach on the
Reuters RCV1 text categorization test collection (Amini
et al., 2009; Chang & Lin, 2011). RCV1 consists of D =
47,236 bag-of-words features for N = 20,241 English doc-
uments grouped into two different categories. We were
unable to compare to the full Laplace approximation due to
the high-dimensionality, so we used LR-Laplace with M =
20,000 as a baseline. For both datasets, we ﬁnd that as we
increase the rank of the data approximation, we incur longer
running times but reduced errors in posterior means and
variances. Laplace and Diagonal Laplace do not provide the
same computation–accuracy trade-off.
Figure 3. LR-Laplace approximation quality on Farm-Ads (top)
and RCV-1 (bottom) datasets with varying M. (A.) Farm-Ads
error in the posterior mean and (B.) Farm-Ads error in posterior
variances (C.) RCV-1 error in posterior mean and (D.) RCV-1 error
in posterior variances.
Choosing M. Applying LR-GLM requires choosing the
rank M of the low rank approximation. As we have shown,
this choice characterizes a computational–statistical trade-
off whereby larger M leads to linearly larger computational
demands, but increases the precision of the approximation.
As a practical rule of thumb, we recommend setting M to
be as large as is allowable for the given application without
the resulting inference becoming too slow. For our exper-
iments with LR-Laplace, this limit was M ≈20,000. For
LR-MCMC, the largest manageable choice of M will be
problem dependent but will typically be much smaller than
20,000.

LR-GLM: fast GLMs with low-rank approximations
Acknowledgments
This research is supported in part by an NSF CAREER
Award, an ARO YIP Award, a Google Faculty Research
Award, a Sloan Research Fellowship, and ONR. BLT is
supported by NSF GRFP.
References
Amini, M., Usunier, N., and Goutte, C. Learning from
Multiple Partially Observed Views – an Application to
Multilingual Text Categorization. In Advances in Neural
Information Processing Systems, pp. 28–36, 2009.
Bardenet, R., Doucet, A., and Holmes, C. On Markov
Chain Monte Carlo Methods for Tall Data. The Journal
of Machine Learning Research, 18(1):1515–1557, 2017.
Bishop, C. M. Pattern Recognition and Machine Learning.
Springer, 2006.
Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. Varia-
tional inference: A review for statisticians. Journal of
the American Statistical Association, 112(518):859–877,
2017.
Bolley, F., Gentil, I., and Guillin, A. Convergence to Equi-
librium in Wasserstein Distance for Fokker–Planck Equa-
tions. Journal of Functional Analysis, 263(8):2430–2457,
2012.
Boyd, S. and Vandenberghe, L. Convex Optimization. Cam-
bridge University Press, 2004.
Cai, T. T., Zhang, C.-H., and Zhou, H. H. Optimal Rates
of Convergence for Covariance Matrix Estimation. The
Annals of Statistics, 38(4):2118–2144, 2010.
Campbell, T. and Broderick, T. Bayesian Coreset Construc-
tion via Greedy Iterative Geodesic Ascent. In Proceed-
ings of the 35th International Conference on Machine
Learning, volume 80, pp. 698–706, 2018.
Campbell, T. and Broderick, T.
Automated Scalable
Bayesian Inference via Hilbert Coresets. Journal of Ma-
chine Learning Research, 20(1):551–588, 2019.
Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D.,
Goodrich, B., Betancourt, M., Brubaker, M., Guo, J.,
Li, P., and Riddell, A. Stan: A Probabilistic Program-
ming Language. Journal of Statistical Software, 76(1),
2017.
Carvalho, C. M., Polson, N. G., and Scott, J. G. Handling
Sparsity via the Horseshoe. In Artiﬁcial Intelligence and
Statistics, pp. 73–80, 2009.
Chang, C.-C. and Lin, C.-J. LIBSVM: A Library for Sup-
port Vector Machines. ACM Transactions on Intelligent
Systems and Technology, 2(3):27, 2011.
Davis, C. and Kahan, W. M. The Rotation of Eigenvec-
tors by a Perturbation. III. SIAM Journal on Numerical
Analysis, 7(1):1–46, 1970.
Dawid, A. P. The Well-Calibrated Bayesian. Journal of
the American Statistical Association, 77(379):605–610,
1982.
Dheeru, D. and Karra Taniskidou, E. UCI Machine Learning
Repository, 2017.
George, E. I. and McCulloch, R. E. Variable Selection
via Gibbs Sampling. Journal of the American Statistical
Association, 88(423):881–889, 1993.
Geppert, L. N., Ickstadt, K., Munteanu, A., Quedenfeld, J.,
and Sohler, C. Random Projections for Bayesian Regres-
sion. Statistics and Computing, 27(1):79–101, 2017.
Guan, Y. and Stephens, M. Bayesian Variable Selection
Regression for Genome-Wide Association Studies and
Other Large-Scale Problems.
The Annals of Applied
Statistics, pp. 1780–1815, 2011.
Guhaniyogi, R. and Dunson, D. B. Bayesian Compressed
Regression. Journal of the American Statistical Associa-
tion, 110(512):1500–1514, 2015.
Halko, N., Martinsson, P.-G., and Tropp, J. A. Finding
Structure with Randomness: Probabilistic Algorithms
for Constructing Approximate Matrix Decompositions.
SIAM Review, 53(2):217–288, 2011.
Hastings, W. K. Monte Carlo Sampling Methods Using
Markov Chains and their Applications. Biometrika, 57
(1), 1970.
Hoffman, M. D. and Gelman, A. The No-U-Turn Sampler:
Adaptively Setting Path Lengths in Hamiltonian Monte
Carlo. Journal of Machine Learning Research, 15(1):
1593–1623, 2014.
Huggins, J., Campbell, T., and Broderick, T. Coresets for
Scalable Bayesian Logistic Regression. In Advances in
Neural Information Processing Systems, pp. 4080–4088,
2016.
Huggins, J., Adams, R. P., and Broderick, T. PASS-GLM:
Polynomial Approximate Sufﬁcient Statistics for Scal-
able Bayesian GLM Inference. In Advances in Neural
Information Processing Systems, pp. 3611–3621, 2017.
Huggins, J. H., Campbell, T., Kasprzak, M., and Broderick,
T. Practical Bounds on the Error of Bayesian Posterior
Approximations: A Nonasymptotic Approach. arXiv
preprint arXiv:1809.09505, 2018.

LR-GLM: fast GLMs with low-rank approximations
Johndrow, J. E., Orenstein, P., and Bhattacharya, A. Scal-
able MCMC for Bayes Shrinkage Priors. arXiv preprint
arXiv:1705.00841, 2017.
Lee, J. and Oh, H.-S. Bayesian Regression Based on Princi-
pal Components for High-Dimensional Data. Journal of
Multivariate Analysis, 117:175–192, 2013.
Luenberger, D. G. Optimization by Vector Space Methods.
John Wiley & Sons, 1969.
MacKay, D. J. Information Theory, Inference and Learning
Algorithms. Cambridge University Press, 2003.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N.,
Teller, A. H., and Teller, E. Equation of State Calcu-
lations by Fast Computing Machines. The Journal of
Chemical Physics, 21(6):1087–1092, 1953.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine Learning in Python. Journal of
Machine Learning, 12:2825–2830, 2011.
Petersen, K. B. and Pedersen, M. S. The Matrix Cookbook.
Technical University of Denmark, 7(15):510, 2008.
Piironen, J. and Vehtari, A. Sparsity Information and Regu-
larization in the Horseshoe and Other Shrinkage Priors.
Electronic Journal of Statistics, 11(2):5018–5051, 2017.
Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flan-
nery, B. P. Numerical Recipes 3rd Edition: The Art of
Scientiﬁc Computing. Cambridge University Press, 2007.
Rue, H., Martino, S., and Chopin, N. Approximate Bayesian
Inference for Latent Gaussian Models by Using Inte-
grated Nested Laplace Approximations. Journal of the
Royal Statistical Society: Series B, 71(2):319–392, 2009.
Schmidt, M., Le Roux, N., and Bach, F. Minimizing Finite
Sums with the Stochastic Average Gradient. Mathemati-
cal Programming, 162(1-2):83–112, 2017.
Spantini, A., Solonen, A., Cui, T., Martin, J., Tenorio, L.,
and Marzouk, Y. Optimal Low-Rank Approximations
of Bayesian Linear Inverse Problems. SIAM Journal on
Scientiﬁc Computing, 37(6):A2451–A2487, 2015.
Turner, R. E. and Sahani, M. Two Problems with Varia-
tional Expectation Maximisation for Time-Series Models.
Bayesian Time Series Models, 1(3.1):3–1, 2011.
Udell, M. and Townsend, A. Why Are Big Data Matrices
Approximately Low Rank? SIAM Journal of Mathemati-
cal Data Science, 2019.
Van der Vaart, A. W. Asymptotic Statistics, volume 3. Cam-
bridge University Press, 2000.
Vershynin, R. How Close is the Sample Covariance Matrix
to the Actual Covariance Matrix? Journal of Theoretical
Probability, 25(3):655–686, 2012.
Villani, C. Optimal Transport: Old and New, volume 338.
Springer Science & Business Media, 2008.
Wang, J., Lee, J. D., Mahdavi, M., Kolar, M., and Srebro, N.
Sketching Meets Random Projection in the Dual: A Prov-
able Recovery Algorithm for Big and High-Dimensional
Data. Electronic Journal of Statistics, 11(2):4896–4944,
2017.
Yang, Y., Wainwright, M. J., and Jordan, M. I. On the Com-
putational Complexity of High-Dimensional Bayesian
Variable Selection. The Annals of Statistics, 44(6):2497–
2532, 2016.
Zhang, L., Mahdavi, M., Jin, R., Yang, T., and Zhu, S.
Random Projections for Classiﬁcation: A Recovery Ap-
proach. IEEE Transactions on Information Theory, 60
(11):7300–7316, 2014.
Zhu, C., Byrd, R. H., Lu, P., and Nocedal, J. L-BFGS-B:
Fortran Subroutines for Large-Scale Bound-Constrained
Optimization. ACM Transactions on Mathematical Soft-
ware, 23(4):550–560, 1997.

LR-GLM: fast GLMs with low-rank approximations
A. Additional Experimental Details and Empirical Results
A.1. Experimental Details
For all experiments we sampled β from an isotropic Gaussian prior with unit variance. For all synthetic data results we ﬁrst
generated a design matrix by sampling from a zero-mean Gaussian with diagonal covariance Σ with each Σi,i = 5 ∗1.05−i.
We then used a scikit-learn (Pedregosa et al., 2011) implementation of a randomized SVD algorithm due to Halko et al.
(2011), computed from two iterations (i.e., passes through X).
To assess robustness, in all experiments we used three or more replicate experiments, deﬁned by independently generated
synthetic datasets or train/test splits as well as re-rerunning the randomized truncated SVD.
The performance of the Diagonal Laplace approximation is dependent upon the shape the exact posterior at βMAP. In
particular, using a dataset with axis aligned covariance structure gives Diagonal Laplace an unrealistic advantage given
that in most real applications we do not believe that low-rank structure will be axis aligned. As such, for all synthetic data
experiments presented, we randomly generated a basis of orthonormal vectors and used this basis to rotate our the design
matrix. This rotation preserves the spectral decay of the data but eliminates the axis alignment of the synthetic data.
In all experiments we consider N = 2,500 training examples. We obtained results on “Out of Sample Data” (in Figures A.1
and A.5) by sampling X from an alternative distribution over covariates. Speciﬁcally, we generated these out-of-sample
covariates in the manner described above, but with a different random rotation matrix.
We found MAP estimation using L-BFBS-B to be the most efﬁcient of several available options in the scipy optimize library,
and used this method in all MAP estimation and Laplace approximation experiments.
For all Bayesian predictions, we use the probit approximation to the logistic function to enable fast approximation (Bishop,
2006, Chap. 4.5).
A.2. Additional Figures
In Figure A.1 we present results on prediction performance, in term of classiﬁcation error, as well as negative log likelihood,
reported for “Training”, “Test”, and “Out of Sample Data”. In Figure A.2 we report the error of LR-Laplace and Random-
Laplace relative to NUTS for estimation of posterior means and variances. We see here that the estimates exhibit behavior
increasingly similar to that of the prior as the rank of the approximation, M, decreases. Next, Figure A.3 depicts the same
error trends for LR-MCMC using NUTS in Stan. We report calibration performance of the approximations of interest for
credible sets of parameters (Figure A.4) as well as for prediction (Figure A.5).
We additionally include results analogous to those in the main text for Laplace approximations using low-rank data
approximations to perform faster MCMC using NUTS with Stan (Carpenter et al., 2017), in Figure A.6. Finally, we also
here provide the relative error of posterior mean and standard deviation estimation for logistic regression with a regularized
horseshoe prior using the LR-MCMC approximation in Figure A.7. This experiment uses Stan for inference as well.
A.2.1. HORSESHOE LOGISTIC REGRESSION EXPERIMENT
For the logistic regression experiment using a regularized horseshoe prior we used N = 1,000 data points of dimension
D = 200. We used ten non-zero effects, each of size 10. Our implementation of the regularized horseshoe and inference in
Stan closely followed M. Betancourt’s “Bayes Sparse Regression” case study.6 We generated covariates as described in the
previous section.
A.3. Stan Model Code
First we show Stan code for Bayesian logistic regression.
data {
int<lower=1> N; // # of data
int<lower=1> D; // # of covariates
matrix[N, D] X; // Design matrix
6https://betanalpha.github.io/assets/case_studies/bayes_sparse_regression.html

LR-GLM: fast GLMs with low-rank approximations
Figure A.1. Predictive performance of posterior approximations in Bayesian logistic regression in terms of (Top) classiﬁcation error and
(Bottom) average negative log likelihood (NLL) of responses under approximate posterior predictive distributions on (Left) train, (Center)
test and (Right) out of sample datasets. Lower is better.
Figure A.2. Approximate posterior mean and standard deviation across a parameter subset as M varies. Horizontal axis represents ground
truth from running NUTS using Stan without the LR-GLM approximation. D = 250.
int<lower=0> y[N]; // labels
real<lower=0> sigma;
}
parameters {
vector[D] beta;
}
model {
beta ˜ normal(0, sigma);
y ˜ bernoulli_logit(X * beta);
}
Second, we show Stan code for logistic regression with our low-rank approximation.
data {
int<lower=1> N; // # of data
int<lower=1> D; // # of covariates
int<lower=1> M; // Projected dimension

LR-GLM: fast GLMs with low-rank approximations
Figure A.3. This ﬁgure is analogous to Figure A.2 but examines the trade-off between computation and accuracy of LR-MCMC using
NUTS in Stan. D = 250.
Figure A.4. Credible set calibration. The fraction of parameters in the credible sets deﬁned by different lower tail intervals as a function of
the approximate posterior probability of parameters taking values in that interval. The black dotted line (on the diagonal) reﬂects perfect
calibration.
matrix[D, M] U; // Projection matrix
matrix[N, M] barX; // Projected design matrix
int<lower=0> y[N]; // labels
real<lower=0> sigma;
}
parameters {
vector[D] beta;
}
transformed parameters {
vector[M] bar_beta = U’ * beta;
}
model {
beta ˜ normal(0, sigma);
y ˜ bernoulli_logit(barX * bar_beta);
}

LR-GLM: fast GLMs with low-rank approximations
Figure A.5. Prediction calibration.
B. Related Work on Scalable Bayesian Inference
Developing scalable approximate Bayesian inference for models with many parameters (large D) and many data points (large
N) has been active area of research for decades, and researchers have developed a large variety of methods applicable to
GLMs. Historically, Markov chain Monte Carlo (MCMC) methods based on the Metropolis-Hastings algorithm (Metropolis
et al., 1953; Hastings, 1970) have been dominant. However MCMC is computationally expensive on large-scale problems
in which both D and N are very large. In particular, each likelihood evaluation requires O(DN) time, due to the matrix
vector product Xβ. Further, estimating posterior covariances uniformly well requires O(log D) samples (Cai et al., 2010).
Therefore, the total cost of collecting those samples is O(ND log D) time in the case of perfect, independent Monte Carlo
samples. In practice, though, mixing times may also have unfavorable scaling with dimensionality and sample size; these
issues can lead to even worse scaling in N and D. Several lines of research have explored the use of subsampling methods
to reduce the dependence on N. But these methods either lose the asymptotic guarantees of exact MCMC or fail to provide
faster inference in practice due to poor mixing behavior (Bardenet et al., 2017).
Other work has pursued deterministic approximations to the Bayesian posterior. Some of the most widely used of these
approximations include (1) the Laplace approximation, which is a Gaussian approximation of the posterior deﬁned locally at
the posterior mode, (2) extensions of the Laplace approximation such as the integrated nested Laplace approximation (INLA)

LR-GLM: fast GLMs with low-rank approximations
Figure A.6. This ﬁgure is analogous to Figure 2A but assesses LR-MCMC using NUTS in Stan rather than LR-Laplace. D = 250.
Figure A.7. Bayesian logistic regression with a regularized Horseshoe prior using NUTS in Stan. The red vertical line indicates the
runtime of inference with Stan using the exact likelihood.
(Rue et al., 2009), and (3) variational Bayes; see, e.g., (Bishop, 2006, Chap. 10) and (Blei et al., 2017). However, these
approaches also scale poorly with dimension in general. The Laplace approximation requires computing and inverting the
Hessian of the log posterior which demand O(ND2) and O(D3) time respectively, in order to compute approximate posterior
means and variances. In the N ≪D setting, this cost can be reduced to O(N 2D) time (Appendix C). However, in large-N
settings of interest, the O(N 2D) cost can be prohibitive as well. The cost of inference is further compounded when we give
a fully Bayesian treatment to model hyperparameters as well as parameters; e.g., INLA requires this heavy computation
for each nested approximation. In the face of difﬁculties posed by high dimensionality, practitioners frequently turn to
factorized (or “mean-ﬁeld”) approximations. In the case of VB, the mean-ﬁeld approach can yield biased approximations
that underestimate uncertainty (MacKay, 2003; Turner & Sahani, 2011). Likewise, factorized Laplace approximations,
which approximate the Hessian with only its diagonal elements, similarly underestimate uncertainty (Appendix F.8).
Some more recent work has approached scalable approximate inference in generalized linear models with theoretical
guarantees on quality in the large-N regime by using likelihood approximations that are cheap to evaluate (Huggins et al.,
2017; Campbell & Broderick, 2019; 2018; Huggins et al., 2016). But these methods fail to scale well to the large-D case.
More closely related to the present work, Geppert et al. (2017) and Lee & Oh (2013) focus on conjugate Bayesian regression,
respectively using random projections and principle component analysis to deﬁne low-rank descriptions of the design. Lee
& Oh (2013) restrict their consideration to the exactly low-rank case and primarily discuss the asymptotic consistency of the
resulting posterior mean without discussing computational considerations. Spantini et al. (2015) use conjugate Bayesian
regression as stepping-off point to derive a point estimator for Bayesian inverse problems. Guhaniyogi & Dunson (2015) use
random projections for Bayesian GLMs but focus on predictive performance rather than parameter estimation. Outside the
Bayesian context, Zhang et al. (2014), Wang et al. (2017), and many others have analyzed random projections for regression

LR-GLM: fast GLMs with low-rank approximations
and classiﬁcation using, for example, an M-estimation framework.
C. Fast matrix inversions in the N ≪D setting
In this section we focus on Gaussian conjugate linear regression with N ≪D. In this case, we can detail formulas for more
efﬁcient computation of the posterior mean and covariance. We start from the standard expressions for the posterior mean
µN and covariance ΣN when the prior is mean zero with covariance Σβ; see Section 3 and Section 4.1 for further notation
and setup of the model. These expressions are:
Σ−1
N = Σ−1
β
+ τX⊤X
(C.1)
µN = τΣNX⊤Y.
(C.2)
Using these formulas naively in the D ≫N setting is computationally expensive due to the O(D3) time cost of matrix
inversion and O(D2) storage cost.
Using the Woodbury matrix identity, (A−1 + UCV )−1 = A −AU(C−1 + V AU)−1V A, allows us to write ΣN =
(Σ−1
β
+ X⊤(τIN)X)−1 as
ΣN = Σβ −ΣβX⊤(τ −1IN + XΣβX⊤)−1XΣβ.
(C.3)
Computing ΣN via Eq. (C.3) requires only O(DN 2) cost for the matrix multiplications and an O(N 3) cost for the matrix
inversion. The posterior mean µN may then be computed in O(ND) time by multiplying through by X⊤Y . These time
costs can be signiﬁcant reductions over the naive O(D3) cost when N ≪D.
Fast inversions for the Laplace approximation to the GLM posterior
We here show that the same approach described above may be used for the Laplace approximation in the context of Bayesian
GLMs. We say that we have a GLM likelihood if we can write
p(Y | β, X) =
N
X
n=1
φ(yn, x⊤
n β)
for some mapping function φ : R × R →R. The Bayesian posterior then becomes
log p(β | X, Y ) = log p(β) +
N
X
n=1
φ(yn, x⊤
n β) + Z,
(C.4)
where Z is a typically-intractable log normalizing constant.
Due to the analytic intractability of posterior inference in many common GLMs, approximations are necessary; the Laplace
approximation is a particularly widely used approximation and takes the form
¯p(β) = N(β | ¯µ, ¯Σ),
(C.5)
where ¯µ := arg maxβ log p(β | X, Y ) and ¯Σ :=

−∇2
β log p(β|X, Y )|β=¯µ
−1
. However, as in the conjugate case,
computing this matrix inverse naively can be expensive in the high-dimensional setting, and we are motivated to consider
more computationally efﬁcient routes to evaluate it. In settings when N ≪D and when we have a Gaussian prior
p(β) = N(β | µβ, Σβ), we may take an approach similar to our approach in the conjugate case. We ﬁrst note
∇2
β log p(β | X, Y )|β=¯µ = −Σ−1
β
+ X⊤diag(⃗φ′′(Y, X ¯µ))X,
(C.6)
where ⃗φ′′(Y, A) is a vector in RN deﬁned such that for any n in 1, 2, . . . , N, ⃗φ′′(Y, A)n :=
d2
da2 φ(yi, a)|a=An. Applying
the same trick to this expression as before, we obtain
¯ΣN =
 −∇2
β log p(β | X, Y )|β=¯µ
−1 = Σβ −ΣβX⊤ diag[−⃗φ′′(Y, X ¯µ)]−1 + XΣβX⊤−1XΣβ,
(C.7)
which again can yield computational gains.
It is worth noting however that this route is more computationally efﬁcient only when the prior covariance matrix is structured
in some way that allows for fast matrix-vector and matrix-matrix multiplications. This will be the case, for example, if Σβ is
diagonal, block-diagonal, banded diagonal, or diagonal plus a low-rank matrix.

LR-GLM: fast GLMs with low-rank approximations
D. Conjugate Gaussian regression with exactly low rank design
D.1. Derivation of Eq. (2)
Here we consider the setting of conjugate Bayesian linear regression, with X exactly low rank and Σβ = σ2
βID, as detailed
in Section 4.1. We now derive the expressions (Eq. (2)) for the mean and covariance of the Gaussian posterior for β in this
case. We suppose X = V diag(λ)U ⊤for U, V matrices of orthonormal rows and λ a vector. The preceding equation for X
will capture low rank structure when U ∈RD×M for some M with M ≪min(D, N).
For the covariance, we start from Eq. (C.1). Then we can rewrite ΣN as follows.
ΣN =
 σ−2
β ID + τX⊤X
−1
=
 σ−2
β ID + τUdiag(λ)V ⊤V diag(λ)U ⊤−1
=
 σ−2
β ID + Udiag(τλ ⊙λ)U ⊤−1
where ⊙denotes component-wise multiplication, in this case across the components of the vector λ
= σ2
βI −σ2
βU(diag(τλ ⊙λ)−1 + σ2
βIM)−1U ⊤σ2
β
by the Woodbury matrix identity and U ⊤U = IM
= σ2
βI −σ2
βUdiag
(
1
τλ ⊙λ + σ2
β1M
−1
σ2
β
)
U ⊤
where division within the diag input is component-wise and 1M is the all-ones vector of length M
= σ2
β
 
ID −Udiag
(
τλ ⊙λ
σ−2
β 1M + τλ ⊙λ
)
U ⊤
!
.
Starting from Eq. (C.2), we can rewrite the posterior mean as follows.
µN = τΣNX⊤Y
= τσ2
β
 
ID −Udiag
(
τλ ⊙λ
σ−2
β 1M + τλ ⊙λ
)
U ⊤
!
Udiag(λ)V ⊤Y
from the derivation above and substituting for X
= τσ2
β
 
U −Udiag
(
τλ ⊙λ
σ−2
β 1M + τλ ⊙λ
)!
diag(λ)V ⊤Y
since U ⊤U = IM
= τσ2
βU
 
IM −diag
(
τλ ⊙λ
σ−2
β 1M + τλ ⊙λ
)!
diag(λ)V ⊤Y
= Udiag
(
τλ
σ−2
β 1M + τλ ⊙λ
)
V ⊤Y.
E. Proofs and further results for conjugate Bayesian linear regression with low-rank data
approximations
E.1. Proof of Theorem 4.1
Recall that for conjugate Gaussian Bayesian linear regression, the exact posterior is p(β | X, Y ) = N(β | µN, ΣN), where
µN and ΣN are given in Eqs. (C.1) and (C.2).
Using an orthonormal projection U yields a Gaussian approximate posterior ˜p(β | X, Y ) = N(β | ˜µN, ˜ΣN). Recall
from Section 3 that we obtain this approximate posterior by replacing X with XUU ⊤. Thus, we can ﬁnd ˜µN and ˜ΣN by

LR-GLM: fast GLMs with low-rank approximations
consulting Eqs. (C.1) and (C.2):
˜Σ−1
N = Σ−1
β
+ τUU ⊤X⊤XUU ⊤
(E.1)
˜µN = ˜τΣNUU ⊤X⊤Y.
(E.2)
Upper bound on the posterior mean approximation error
We will obtain our upper bound on the error of the approximate posterior mean relative to the exact posterior mean by upper
bounding the norm of the difference between the gradient of the log posterior with respect to β at the approximate posterior
mean, ˜µN, and the exact posterior mean, µN. Together with the strong convexity of the negative log posterior, this bound
will allow us to arrive at the desired upper bound on ∥µN −˜µN∥2.
First, we bound the norm of the gradient difference. To that end, the gradients of the exact log likelihood and the approximate
log likelihood are given by
∇β log p(Y | X, β) = ∇β
h
−τ
2(Xβ −Y )⊤(Xβ −Y )
i
= −τ(X⊤Xβ −X⊤Y )
and
∇β log ˜p(Y | X, β) = ∇β
h
−τ
2(XUU ⊤β −Y )⊤(XUU ⊤β −Y )
i
= −τ(UU ⊤X⊤XUU ⊤β −UU ⊤X⊤Y ).
We can thus upper bound the norm of the difference between the two log posteriors as follows.
∥∇β log ˜p(β | X, Y ) −∇β log p(β | X, Y )∥2
= ∥∇β log ˜p(Y | X, β) −∇β log p(Y | X, β)∥2
since the prior is the same in both the exact and approximate model
and since the normalizing constant has no β dependence
=
−τ
 UU ⊤X⊤XUU ⊤β −UU ⊤X⊤Y

+ τ
 X⊤Xβ −X⊤Y

2
= τ
 X⊤X −UU ⊤X⊤XUU ⊤
β + UU ⊤X⊤Y −X⊤Y

2
= τ
 ¯U ¯U ⊤X⊤X ¯U ¯U ⊤β −¯U ¯U ⊤X⊤Y

2
where ¯U (above) as well as ¯λ and ¯V (below) are deﬁned in Section 3
= τ
 ¯Udiag(¯λ ⊙¯λ) ¯U ⊤β −¯Udiag(¯λ) ¯V ⊤Y

2
≤τ
  ¯Udiag(¯λ ⊙¯λ) ¯U ⊤β∥2 + ∥¯Udiag(¯λ) ¯V ⊤Y

2

by the triangle inequality
= τ
 diag(¯λ ⊙¯λ) ¯U ⊤β∥2 + ∥diag(¯λ) ¯V ⊤Y

2

since ∥v∥2
2 = v⊤v for a vector v and U ⊤U = IM
≤τ
diag(¯λ ⊙¯λ)

op
 ¯U ⊤β∥2 + ∥diag(¯λ)∥op∥¯V ⊤Y

2

by deﬁnition of the operator norm in this space
= τ
 ¯λ2
1∥¯U ⊤β∥2 + ¯λ1∥¯V ⊤Y ∥2

(E.3)
Second, we need a result that will let us use the strong convexity of the negative log posterior. We prove the following result
in Appendix E.2.
Lemma E.1. Let f, g be twice differentiable functions mapping RD →R and attaining minima at βf = arg minβ f(β)
and βg = arg minβ g(β), respectively. Additionally, assume that f is α–strongly convex for some α > 0 on the set
{tβf + (1 −t)βg|t ∈[0, 1]} and that ∥∇βf(βg) −∇βg(βg)∥2 = ∥∇βf(βg)∥2 ≤c. Then
∥βf −βg∥2 ≤c
α.
(E.4)

LR-GLM: fast GLMs with low-rank approximations
To use the preceding result, we need a lower bound on the strong convexity constant of the negative log posterior; we
now calculate such a bound. We have that µN and ˜µN are the maximum a posteriori values of β under p(β|X, Y, α) and
˜p(β|X, Y, α), respectively; equivalently they minimize the respective negative log of these distributions. For a matrix
A, let λmin(A) denote its minimum eigenvalue. The Hessian of the negative log posterior with respect to β is precisely
Σ−1
β
+ τX⊤X everywhere. So the negative log posterior is α–strongly convex, where
α = λmin(Σ−1
β
+ τX⊤X) ≥λmin(Σ−1
β ) + τλmin(X⊤X) = ∥Σβ∥−1
2
+ τ ¯λ2
D−M.
(E.5)
In the ﬁrst part of the ﬁnal equality above, we use that the spectral norm of a matrix inverse is equal to the reciprocal of the
minimum eigenvalue of the matrix.
Now we have an upper bound on the norm of the difference in gradients of the negative log posteriors (the same as for the
log posteriors, in Eq. (E.3)) and a lower bound on the strong convexity constant from Eq. (E.5). So we can apply these
together with Lemma E.1 to ﬁnd
∥µN −˜µN∥2 ≤τ
 ¯λ2
1∥¯U ⊤˜µN∥2 + ¯λ1∥¯V ⊤Y ∥2

α
by Lemma E.1 taking log p(β|X, Y ) and log ˜p(β|X, Y )
as f and g respectively, with c given by Eq. (E.3)
≤τ
 ¯λ2
1∥¯U ⊤˜µN∥2 + ¯λ1∥¯V ⊤Y ∥2

∥Σβ∥−1
2
+ τ ¯λ2
D−M
by Eq. (E.5)
=
¯λ1
 ¯λ1∥¯U ⊤˜µN∥2 + ∥¯V ⊤Y ∥2

∥τΣβ∥−1
2
+ ¯λ2
D−M
.
Notably, in the common special case that Σβ is diagonal, as we saw in Section 4.1, ˜µN will be in the span of U, and we will
have that ∥¯U ⊤˜µN∥2 = 0.
Error in Posterior Precision
The error in the precision matrices for the approximate and exact posteriors in linear regression are particularly straightfor-
ward since they do not depend on the responses, Y . In particular, we have
Σ−1
N −˜Σ−1
N = (Σ−1
β
+ τX⊤X) −(Σ−1
β
+ τUU ⊤X⊤XUU ⊤)
(E.6)
= τX⊤X −τUU ⊤X⊤XUU ⊤
(E.7)
= τ ¯U ¯U ⊤X⊤X ¯U ¯U ⊤
(E.8)
= τ ¯Udiag(¯λ ⊙¯λ) ¯U ⊤.
(E.9)
Thus, since it is equal to the maximum eigenvalue, the spectral norm of the error in the precisions is precisely ∥Σ−1
N −
˜Σ−1
N ∥2 = τ ¯λ2
1.
E.2. Proof of Lemma E.1
By the fundamental theorem of calculus, we may write
∇βf(β) = ∇βf(βg) +
Z 1
t=0
(β −βg)⊤∇2
βf(tβ + (1 −t)βg)dt.
Considering the norm of ∇βf(β) and applying the triangle inequality provides that for any β in {tβf +(1−t)βg | t ∈[0, 1]},
∥∇βf(β)∥2 ≥

Z 1
t=0
(β −βg)⊤∇2
βf(tβ + (1 −t)βg)dt

2
−∥∇βf(βg)∥2
(E.10)
≥∥β −βg∥2

Z 1
t=0
∇2
βf(tβ + (1 −t)βg)dt

2
−∥∇βf(βg)∥2
(E.11)
≥∥β −βg∥2α −∥∇βf(βg)∥2.
(E.12)

LR-GLM: fast GLMs with low-rank approximations
Figure E.1. Example of posterior approximations with different projections (characterized by U) for increasing sample sizes. Each plot
shows the contours of three densities: the prior, likelihood, and posterior (or approximations thereof). The top row shows the exact
posterior. The middle row shows the approximations found by using the best rank-1 approximation to X. The bottom row shows the
approximations found using the orthogonal rank-1 approximation. The star is at the parameter value used to generate simulated data for
these plots.
We consider this bound at βf. Recall we assume that ∥∇βf(βg)∥2 ≤c. And ∥∇βf(βf)∥2 = 0 since f is twice differentiable.
Therefore, we have that 0 ≥∥βf −βg∥2α −c, and the result follows.
E.3. Proof of Corollary 4.2
Our approach is to show that
˜µN
p→ΣβU∗(U ⊤
∗ΣβU∗)−1U ⊤
∗β.
(E.13)
We then appeal to the following result, which we prove in Appendix E.4:
Lemma E.2. ˜µ := ΣβU(U ⊤ΣβU)−1U ⊤β is the vector of minimum Σ−1
β -norm satisfying U ⊤˜µ = U ⊤β.
Finally, for any closed S ⊂RD, ˜µ = arg minv∈S ∥v∥Σ−1
β
= arg maxv∈S −1
2v⊤Σ−1
β v = arg maxv∈S N(0, Σβ). There-
fore, the ˜µ in Lemma E.2 is the maximum a priori vector satisfying the constraint in Lemma E.2.

LR-GLM: fast GLMs with low-rank approximations
We ﬁrst turn to proving Eq. (E.13). Let UNdiag(λ(N))V ⊤
N denote the M-truncated SVD of the design matrix consisting
of N samples X = (x1, x2, . . . , xN) where xi
i.i.d.
∼p∗. When the low rank approximation is deﬁned by this SVD, from
Eq. (E.1) we have that ˜µN = τ ˜ΣNUNU ⊤
N X⊤Y . Noting that Y = Xβ + 1
τ ϵ for some ϵ ∈RN with ϵi
i.i.d.
∼N(0, 1), we may
expand this out and write:
˜µN = τ(Σ−1
β
+ UNU ⊤
N X⊤τXUNU ⊤
N )−1UNU ⊤
N X⊤(Xβ + 1
τ ϵ)
= τ
n
Σ−1
β
+ UN
h
τdiag(λ(N) ⊙λ(N))
i
U ⊤
N
o−1
UNdiag(λ(N))V ⊤
N

VNdiag(λ(N))U ⊤
N β + 1
τ ϵ

=
n
Σ−1
β
+ UN
h
τdiag(λ(N) ⊙λ(N))
i
U ⊤
N
o−1
UN
h
τdiag(λ(N) ⊙λ(N))
i 
U ⊤
N β + diag(λ(N))−1V ⊤
N
1
τ ϵ

= ΣβUN
h
U ⊤
N ΣβUN + τ −1diag(λ(N))−2i−1 
U ⊤
N β + diag(λ(N))−1V ⊤
N
1
τ ϵ

P→ΣβU∗(U ⊤
∗ΣβU∗)−1U ⊤
∗β,
where in the fourth line we use the matrix identity, (R−1 + W ⊤QW)−1W ⊤Q = RW ⊤(WRW ⊤+ Q−1)−1 (Petersen
& Pedersen, 2008). Convergence in probability in the last line follows since diag(λ(N)−2)
P→0 (Vershynin, 2012) and
UN
P→U.
E.4. Proof of Lemma E.2
We show that β∗= ΣβU(U ⊤ΣβU)−1U ⊤β is the vector of minimum norm satisfying the above constraints in the Hilbert
space RD with inner product ⟨v1, v2⟩= v⊤
1 Σ−1
β v2 for vectors v1, v2 ∈RD.
Deﬁne β∗as
β∗= arg min
v∈RD
∥v∥Σ−1
β
subject to U ⊤v = U ⊤β
(E.14)
First note that the condition U ⊤β∗= U ⊤β may be expressed as a set the M linear constraints
⟨ΣβU[:, i], β∗⟩= U[:, i]⊤β
(E.15)
for i = 1, 2, . . . , M. We thereby see that the constraint restricts β∗to the linear variety β +

ΣβU[:, i]
	M
i=1
⊥, where

A

denotes the subspace generated by the vectors of the set A and

A
⊥denotes the set of all vectors orthogonal to

A

(i.e.
the orthogonal complement of

A

). By the projection theorem (Luenberger, 1969), β∗is orthogonal to

ΣβU[:, i]
	M
i=1
⊥,
or β∗∈

ΣβU[:, i]
	M
i=1
⊥⊥=

ΣβU[:, i]
	M
i=1

. We can therefore write β∗as a linear combination of the vectors

ΣβU[:, i]
	M
i=1; that is, for some c in RM
β∗= ΣβUc.
(E.16)
Our constraints in Eq. (E.15) then demand that ⟨ΣβU[:, i], ΣβUc⟩= U[:, i]⊤β for each i, or equivalently that
U ⊤ΣβΣ−1
β ΣβUc = U ⊤β.
This implies that c = (U ⊤ΣβU)−1U ⊤β.
Plugging this into Eq. (E.16) yields β∗=
ΣβU(U ⊤ΣβU)−1U ⊤β, as desired.
E.5. Proof of Corollary 4.3
Recall that we wish to show that, for conjugate Bayesian regression, under ˜p the uncertainty (i.e., posterior variance) for
any linear combination of parameters, Var˜p[v⊤β], is no smaller than the exact posterior variance. First, we note that this
statement is formally equivalent to stating that v⊤˜ΣNv ≥v⊤ΣNv, or that E := ˜ΣN −ΣN ⪰0 (where ⪰denotes positive
deﬁniteness). By Theorem 4.1, Σ−1
N −˜Σ−1
N = ¯Udiag(¯λ2) ¯U ⊤⪰0. Since this implies that the inverse of the difference
of these matrices is positive deﬁnite, we can then see that (Σ−1
N −˜Σ−1
N )−1 = ˜ΣN(˜ΣN −ΣN)−1ΣN ⪰0. Because, as
valid covariance matrices, ΣN and ˜ΣN are both positive deﬁnite, and because inverses and product of positive deﬁnite
matrices are positive deﬁnite, this implies that ˜Σ−1
N ˜ΣN(˜ΣN −ΣN)−1ΣNΣ−1
N = (˜ΣN −ΣN)−1 ⪰0. Finally, this implies
that ˜ΣN −ΣN ⪰0 as desired.

LR-GLM: fast GLMs with low-rank approximations
E.6. Information loss due the LR-GLM approximation
We see similar behavior to that demonstrated in Corollary 4.3 in the following corollary, which shows that our approximate
posterior never has lower entropy than the exact posterior. Concretely, we look at the reduction of entropy in the approximate
posterior relative to the exact posterior (MacKay, 2003), where entropy is deﬁned as:
H

p(β)

:= Ep[−log2 p(β)]
Corollary E.1. The entropy H

˜p(β|X, Y )

is no less than H

p(β|X, Y )

. Furthermore, when using an isotropic Gaussian
prior Σβ = σ2
βI, the information loss relative to the exact posterior (in nats) is upper bounded as H

˜p(β|X, Y )

−
H

p(β|X, Y )

≤
τσ2
β
2
PD−M
i=1
¯λ2
i .
This result formalizes the intuition that the LR-GLM approximation reduces the information about the parameter that we are
able to extract from the data. Additionally, the upper bound tells us that when U is obtained via an M truncated SVD, at
most τσ2
β¯λ2
1/2 additional nats of information would have been provided by using the M + 1-truncated SVD.
Proof. The entropy of the exact and approximate posteriors are given as:
H(p) = −1
2 log |2πeΣ−1
N | = −1
2

D log 2πe +
D
X
i=1
log(σ−2
β
+ τλ2
i )

and
H(˜p) = −1
2 log |2πe˜Σ−1
N | = −1
2

D log 2πe +
M
X
i=1
log(σ−2
β
+ τλ2
i ) −
D
X
i=M+1
log σ−2
β

.
Therefore, we conclude that
H

˜p(β|X)

−H

p(β|X)

= −1
2
D−M
X
i=1
log σ−2
β
+ 1
2
D−M
X
i=1
log(σ−2
β
+ τ ¯λ2
i )
= 1
2
D−M
X
i=1
log
σ−2
β
+ τ ¯λ2
i
σ−2
β
= 1
2
D−M
X
i=1
log(1 +
τ
σ−2
β
¯λ2
i )
≤1
2
D−M
X
i=1
τ
σ−2
β
¯λ2
i =
τσ2
β
2
D−M
X
i=1
¯λ2
i .
That H

˜p(β|X)

−H

p(β|X)

> 0 follows from the monotonicity of log, that log(1) = 0, and that τσ2
β¯λ2
i > 0 for
i = 1, . . . , D −M.
F. Proofs and further results for LR-Laplace in non-conjugate models
In the main text we introduced LR-Laplace as a method which takes advantage of low-rank approximations to provide
computational gains when computing a Laplace approximation to the Bayesian posterior. In what follows we verify the
theoretical justiﬁcations for this approach. Appendix F.1 provides a derivation of Algorithm 1 and demonstrates the time
complexities of each step, serving as a proof of Theorem 5.1. The remainder of the section is devoted to the proofs and
discussion of the theoretical properties of LR-Laplace.
F.1. Proof of Theorem 5.1
Proof of Theorem 5.1. The LR-Laplace approximation is deﬁned by mean and covariance parameters, ˆµ and ˆΣ. We prove
Theorem 5.1 in two parts. First, we show that ˆµ and ˆΣ do in fact deﬁne the Laplace approximation of ˜p(β|X, Y ), i.e. the

LR-GLM: fast GLMs with low-rank approximations
construction of ˆµ in Line 9 satisﬁes ˆµ = arg maxβ ˜p(β|X, Y ) and that ˆΣ =
 −∇2
β log ˜p(β|X, Y )|β=ˆµ
−1. Second, we
show that each step of Algorithm 1 may be computed in O(NDM) time with O(DM + NM) storage.
Correctness of ˆµ and ˆΣ
In Line 8, the deﬁnition of γ∗implies that γ∗= arg maxγ∈RM ˜pU ⊤β|X,Y (γ|X, Y ) since
log ˜pU ⊤β|X,Y (γ|X, Y ) = log ˜pU ⊤β(γ) + log ˜pY |X,U ⊤β(Y |X, γ) + C
= log pU ⊤β(γ) + log pY |X,β(Y |X, Uγ) + C
= log N(γ|U ⊤µβ, U ⊤ΣβU) +
N
X
i=1
log py|x,β(yi|xi, Uγ) + C
= −1
2γ⊤U ⊤ΣβUγ +
N
X
i=1
φ(yi, x⊤
i Uγ) + C′,
where line 1 uses Bayes’ rule, line 2 uses the deﬁnition of ˜p in Eq. (1), line 3 uses the normality the prior, and the assumed
conditional independence of the responses given β, and line 4 follows from the deﬁnition of φ(·, ·) and the assumption that
µβ = 0. C and C′ are constants which do not depend on γ. This together with the following result (proved in Appendix F.2)
implies that as deﬁned in Line 9 of Algorithm 1, ˆµ = arg maxβ ˜p(β|X, Y ).
Lemma F.1. Suppose a Gaussian prior p(β) = N(µβ, Σβ), and let γ∗:= arg maxγ∈RM log ˜pU ⊤β|X,Y (γ|X, Y ). Then
ˆµ := arg maxβ∈RD log ˜p(β|X, Y ) may be written as ˆµ = Uγ∗+ ¯U ¯U ⊤ΣβU(U ⊤ΣβU)−1γ∗.
We now show that as deﬁned in Line 12 of Algorithm 1, ˆΣ is inverse of the Hessian of the negative log posterior, H. We see
this by writing
H : = ∇2
β −log ˜p(β|X, Y )|β=ˆµ
= ∇2
β −log N(β|µβ, Σβ)|β=ˆµ + ∇2
β
N
X
i=1
−φ(yi, x⊤
i UU ⊤β)|β=ˆµ
= Σ−1
β
+
N
X
i=1
−φ′′(yi, x⊤
i UU ⊤ˆµ)xiUU ⊤x⊤
i
= Σ−1
β
+ UU ⊤X⊤diag
 −⃗φ′′(Y, XUU ⊤ˆµ)

XUU ⊤,
where ⃗φ′′ is the second derivative of φ. The Woodbury matrix lemma then provides that we may compute ˆΣN := H−1 as
ˆΣN = Σβ −ΣβU

U ⊤ΣβU −
n
U ⊤X⊤diag
h
⃗φ′′(Y, XUU ⊤ˆµ)
i
XU
o−1−1
U ⊤Σβ,
which
we
have
written
as
ˆΣ
:=
Σβ −ΣβUWU ⊤Σβ
in
Line
12
with
W −1
=
U ⊤ΣβU
−
n
U ⊤X⊤diag
h
⃗φ′′(Y, XUU ⊤ˆµ)
i
XU
o−1
.
Time complexity of Algorithm 1
We now prove the asserted time and memory complexities for each line of Algorithm 1.
Algorithm 1 begins with the computation of the M-truncated SVD of X⊤≈Udiag(λ)V . As discussed in Section 4.1,
U may be found in O(ND log M) time. At the end of this step we must store the projected data XU ∈RN,M and the
left singular vectors, U ∈RD,M. Which demands O(NM + DM) memory, and the matrix multiply for XU requires
O(NDM) time and is the bottleneck step of the algorithm. The matrix V need not be explicitly computed or stored.
The next stage of the algorithm is solving for ˆµ = arg maxβ log ˜p(β|X, Y ). This is done in two stages: in Line 8
ﬁnd γ∗= arg maxγ∈RM log ˜pU ⊤β|X,Y (γ|X, Y ) as the solution to a convex optimization problem, and in Line 9 ﬁnd
ˆµ as ˆµ = Uγ∗+ ¯U ¯U ⊤ΣβU(U ⊤ΣβU)−1γ∗. Beginning with Line 8, we note that the function log ˜p(U ⊤β|X, Y ) =

LR-GLM: fast GLMs with low-rank approximations
log p(β) + log ˜p(Y |X, β) + c
c= log N(U ⊤β|U ⊤µβ, U ⊤ΣβU) + PN
i=1 log p(yi|x⊤
i UU ⊤β) is a ﬁnite sum of functions
concave in β and therefore also in U ⊤β. γ∗may therefore be solved to a ﬁxed precision in O(NM) time under the
assumptions of our theorem using stochastic optimization algorithms such as stochastic average gradient (Schmidt et al.,
2017). In our experiments we use more standard batch convex optimization algorithm (L-BFGS-B (Zhu et al., 1997)) which
takes at most O(N 2M) time. This latter upper bound on complexity may be seen from observing each gradient evaluation
takes O(NM) time (the cost for the likelihood evaluation, since computing the log prior and its gradient is O(M 2) after
computing U ⊤ΣβU once, which takes O(DM 2) time by assumption) and the number of iterations required can grow up to
linearly in the maximum eigenvalue of Hessian, which in turn grows linearly in N (Boyd & Vandenberghe, 2004).
The second step is computing ˆµ = Uγ∗+ ¯U ¯U ⊤ΣβU(U ⊤ΣβU)−1γ∗. Given γ∗, this may be computed in O(DM) time,
which one may see by noting that ¯U ¯U ⊤(which we never explicitly compute) may be written as ¯U ¯U ⊤= (I −UU ⊤), and
ﬁnding ˆµ as ˆµ = Uγ∗+ ΣβU(U ⊤ΣβU)−1γ∗−UU ⊤ΣβU(U ⊤ΣβU)−1γ∗. By assumption, the structure of Σβ allows us
to compute U ⊤ΣβU in O(DM 2) time and matrix vector products with Σβ in O(D) time.
We now turn to the third stage of the algorithm, solving for the posterior covariance ˆΣ, which is represented as an expression
of U, Σβ and W, deﬁned in Line 11. Computing W requires O(DM) and O(NM 2) matrix multiplications (since
we have precomputed XU), and two O(M 3) matrix inversions which comes to O(NM 2 + DM) time. The memory
complexity of this step is O(NM) since it involves handling XU. Once W has been computed we may use the representation
ˆΣ = Σβ −ΣβUWU ⊤Σβ as presented in Line 12. This representation does not entail performing any additional computation
(which is why we have written O(0)), but as this expression includes U, storing ˆΣ requires O(DM) memory.
Lastly, we may immediately see that computing posterior variances and covariances takes only O(M 2) time as it involves
only indexing into Σβ and U and O(M 2) matrix-vector multiplies.
F.2. Proof of Lemma F.1
We prove the lemma by constructing a rotation of the parameter space by the matrix of singular vectors [U, ¯U], in which we
have the prior
p
 U ⊤β
¯U ⊤β
 
= N
 U ⊤β
¯U ⊤β
 
U ⊤µβ
¯U ⊤µβ

,
U ⊤ΣβU, U ⊤Σβ ¯U
¯U ⊤ΣβU, ¯U ⊤Σβ ¯U
 
.
We have that
ˆµ : = arg max
β∈RD
log ˜p(β|X, Y )
= [U ¯U]
arg max
U ⊤β∈RM, ¯U ⊤β∈RD−M log ˜p
  U ⊤β
¯U ⊤β

|X, Y )
= U arg max
U ⊤β∈RM
 log ˜p(U ⊤β|X, Y ) + ¯U
arg max
¯U ⊤β∈RD−M log ˜p( ¯U ⊤β|U ⊤β, X, Y )

= U arg max
U ⊤β∈RM log ˜p(U ⊤β|X, Y )+
¯U
arg max
¯U ⊤β∈RD−M log N
  ¯U ⊤β| ¯U ⊤ΣβU(U ⊤ΣβU)−1U ⊤β, ¯UΣβ ¯U −¯UΣβU(U ⊤ΣβU)U ⊤Σβ ¯U

= Uγ∗+ ¯U ¯U ⊤ΣβU(U ⊤ΣβU)−1γ∗.
In the second line we simply move to the rotated parameter space. In the third line, we use the chain rule of probability to
separate out two terms. To produce the fourth line, we note that since ˜p(Y |X, β) = p(Y |XUU ⊤β) = ˜p(Y |X, U ⊤β), that
Y and ¯U ⊤β are conditionally independent given U ⊤β. We next note that though arg max ¯U ⊤β∈RD−M log p( ¯U ⊤β|U ⊤β)
depends on U ⊤β, max ¯U ⊤β∈RD−M log p( ¯U ⊤β|U ⊤β) does not depend U ⊤β. This allows us to use the deﬁnition of γ∗to
arrive at the ﬁfth line, as desired.
In the special case that Σβ is diagonal, this expression reduces to Uγ∗. This can be seen by recognizing that ¯U ⊤ΣβU is
then diag(0).

LR-GLM: fast GLMs with low-rank approximations
F.3. Proof of Theorem 5.2
Our approach to proving Theorem 5.2 follows a similar approach to that taken to prove Theorem 4.1. In particular, we begin
by upper bounding the norm of the error of the gradients at the approximate MAP. Noting that the strong log concavity of
the exact posterior, which having been assumed to hold globally, must then also hold on {tˆµ + (1 −t)¯µ|t ∈[0, 1]}, we
obtain an upper-bound on ∥ˆµ −¯µ∥2 by again applying Lemma E.1.
To begin, we ﬁrst recall that the exact and LR-GLM posteriors may be written as
log p(β|X, Y ) = log p(β) +
N
X
n=1
φ(yn|x⊤
n β) −log Z
and
log ˜p(β|X, Y ) = log p(β) +
N
X
n=1
φ(yn, x⊤
n UU ⊤β) −log ˜Z
where φ(·, ·) is such that φ(y, a) = log p(y|x⊤β = a), and Z and ˜Z are the normalizing constants of the exact and
approximate posteriors. As a result, the gradients of these log densities are given as
∇β log p(β|X, Y ) = ∇β log p(β) + X⊤⃗φ′(Y, Xβ)
and
∇β log ˜p(β|X, Y ) = ∇β log p(β) + UU ⊤X⊤⃗φ′(Y, XUU ⊤β),
where ⃗φ′(Y, Xβ) ∈RN is such that for each n ∈[N], ⃗φ′(Y, Xβ)n =
d
daφ(yn, a)|a=x⊤
n β.
And the difference in the gradients is
∇β log p(β|X, Y ) −∇β log ˜p(β|X, Y ) = X⊤⃗φ′(Y, Xβ) −UU ⊤X⊤⃗φ′(Y, XUU ⊤β).
(F.1)
Appealing to Taylor’s theorem, we may write for any β that
φ′(yn, x⊤
n UU ⊤β) = φ′(yn, x⊤
n β) + (x⊤
n UU ⊤β −x⊤
n β)φ′′(yn, an)
for some an ∈[x⊤
n UU ⊤β, x⊤
n β], where φ′′(y, a) :=
d2
da2 φ(y, a).
Using this and introducing vectorized notation for φ′′ to match that used for ⃗φ′, we may rewrite the difference in the gradients
as
∇β log p(β|X, Y ) −∇β log ˜p(β|X, Y )
= X⊤⃗φ′(Y, Xβ) −UU ⊤X⊤⃗φ′(Y, Xβ) −UU ⊤X⊤
(XUU ⊤β −X⊤β) ◦⃗φ′′(Y, A)

= ¯U ¯U ⊤X⊤⃗φ′(Y, Xβ) + UU ⊤X⊤
(X ¯U ¯U ⊤β) ◦⃗φ′′(Y, A)

,
where A ∈RN is such that for each n ∈[N], An ∈[x⊤
n UU ⊤β, x⊤
n β], and ◦denotes element-wise scalar multiplication.
We can use this to derive an upper bound on the norm of the difference of the gradients as
∥∇β log p(β|X, Y ) −∇β log ˜p(β|X, Y )∥2 = ∥¯U ¯U ⊤X⊤⃗φ′ + UU ⊤X⊤
(X ¯U ¯U ⊤β) ◦⃗φ′′
∥2
≤∥¯U ¯U ⊤X⊤⃗φ′∥2 + ∥UU ⊤X⊤
(X ¯U ¯U ⊤β) ◦⃗φ′′
∥2
≤¯λ1∥⃗φ′∥2 + λ1∥(X ¯U ¯U ⊤β) ◦⃗φ′′∥2
≤¯λ1∥⃗φ′∥2 + λ1¯λ1∥¯U ⊤β∥2∥⃗φ′′∥∞
= ¯λ1
 ∥⃗φ′∥2 + λ1∥¯U ⊤β∥2∥⃗φ′′∥∞

,
where we have written ⃗φ′ and ⃗φ′′ in place of ⃗φ′(Y, Xβ) and ⃗φ′′(Y, A), respectively, for brevity despite their dependence on
β.

LR-GLM: fast GLMs with low-rank approximations
Next, let α be the strong log-concavity parameter of p(β|X, Y ). Lemma E.1 then implies that
∥ˆµ −¯µ∥2 ≤
¯λ1
 ∥⃗φ′(Y, X ˆµ)∥2 + λ1∥¯U ⊤ˆµ∥2∥⃗φ′′(Y, A)∥∞

α
as desired, where for each n ∈[N], An ∈[x⊤
n UU ⊤ˆµ, x⊤
n ˆµ].
F.4. Bounds on derivatives of higher order for the log-likelihood in logistic regression and other GLMs
We here provide some additional support for the claim that in Remark 5.3 that the higher order derivatives of the log-likelihood
function, φ, are well-behaved. For logistic regression (which we explore in detail below), for any y in {−1, 1} and a in R, it
holds that | ∂
∂aφ(y, a)| ≤1 and | ∂2
∂2aφ(y, a)| ≤1
4. For Poisson regression with φ(y, a) = log Pois
 y|λ = log(1+exp{a})

,
both | ∂
∂a(y, a)| and | ∂2
∂2aφ(y, a)| are bounded by a small constant factor of y. Additionally, in these cases | ∂3
∂a3 φ(y, a)| is
also well behaved, a fact relevant to Corollary 5.6. However, for alternative mapping functions for Poisson regression, e.g.
deﬁning E[yi|xi, β] = exp{x⊤
i β}, these derivatives will grow exponentially quickly with x⊤
i β, which illustrates that our
provided bounds are sensitive to the particular form chosen for the GLM likelihood.
We now move to compute explicit upper bounds on the derivatives of the log likelihood in logistic regression. This produces
the constants mentioned above, and permits easy computation of upper bounds on the bounds on the approximation error of
LR-Laplace provided in Theorem 5.2 and Corollary 5.6. In particular the logistic regression mapping function (Huggins
et al., 2017) is given as
φ(yn, x⊤
n β) = −log
 1 + exp{−ynx⊤
n β}

,
(F.2)
where each yn ∈{−1, 1}.
The ﬁrst three derivatives of this mapping function and bounds on their absolute values are as follows:
φ′(yn, x⊤
n β) := d
daφ(yn, a)

a=x⊤
n β = yn
exp{−yx⊤
n β}
1 + exp{−ynx⊤
n β}
(F.3)
Notably, ∀a ∈R, y ∈{−1, 1}, |φ′(y, a)| < 1 and
φ′′(yn, x⊤
n β) : = d2
da2 φ(y, a)

a=x⊤
n β = −(1 + exp{x⊤
n β})−1(1 + exp{−x⊤
n β})−1.
(F.4)
Furthermore, for any a in R and y in {−1, 1}, −1
4 ≤φ′′(y, a) < 0. This implies that the Hessian of the negative log
likelihood will be positive semi-deﬁnite everywhere. We additionally have
d3
da3 φ(y, a) = φ′′′(a) =
 exp{a}(exp(−a) −1)

(1 + exp{a})3
(F.5)
which for any a in R satisﬁes, −
1
6
√
3 ≤φ′′′(a) ≤
1
6
√
3.
F.5. Asymptotic inconsistency of the approximate posterior mean within the span of the projections
Consider a Bayesian logistic regression, in which
xi ∼N
 0
0

,
1
0
0
0.99
 
,
β =
 10
1000

,
yi ∼Bern
 (1 + exp{x⊤
i β})−1
.
In this setting, a rank 1 approximation of the design will capture only the ﬁrst dimension of data (i.e. UN →U∗= [1, 0]).
However the second dimension explains almost all of the variance in the responses. As such yi|U ⊤
∗xi, β
d≈Bern(1/2) and
we will get U ⊤
∗β|X, Y = β1|X, Y ≈0.0 under ˜p.

LR-GLM: fast GLMs with low-rank approximations
F.6. Proof of Corollary 5.6
Our proof proceeds via an upper bound on the (2, ˆp)-Fisher distance between ˆp and ¯p (Huggins et al., 2018). Speciﬁcally,
the (2, ˆp)-Fisher distance given by
d2,ˆp(ˆp, ¯p) =
Z
∥∇β log ˆp(β) −∇β log ¯p(β)∥2
2dp(β)
 1
2
.
(F.6)
Given the strong log-concavity of ¯p, our upper bound on this Fisher distance immediately provides an upper-bound on the
2-Wasserstein distance (Huggins et al., 2018).
We ﬁrst recall that ˆp and ¯p are deﬁned by Laplace approximations of ˜p(β|X, Y ) and p(β|X, Y ) respectively. As such we
have that
log ˆp(β)
c= −1
2(β −ˆµ)⊤ Σ−1
β
−UU ⊤X⊤diag(⃗φ′′(Y, XUU ⊤ˆµ))XUU ⊤
(β −ˆµ)
where ⃗φ′′(Y, XUU ⊤ˆµ) is deﬁned as in Algorithm 1 such that ⃗φ′′(Y, Xβ)i =
d2
da2 log p(yi|x⊤β = a)|a=x⊤
i β, and
log ¯p(β)
c= −1
2(β −¯µ)⊤ Σ−1
β
−X⊤diag(⃗φ′′(Y, X ¯µ))X

(β −¯µ).
Accordingly,
∇β log ˆp(β) = −(β −ˆµ)⊤ Σ−1
β
−UU ⊤X⊤diag(⃗φ′′(Y, XUU ⊤ˆµ))XUU ⊤
and
∇β log ¯p(β) = −(β −¯µ)⊤
Σ−1
β
−X⊤diag(⃗φ′′(Y, X ¯µ))X

To deﬁne an upper bound on d2,ˆp(ˆp, p), we must consider the difference between the gradients,
∇β log ˆp(β) −∇β log ¯p(β) = −(β −ˆµ)⊤
Σ−1
β
−UU ⊤X⊤diag[⃗φ′′(Y, XUU ⊤ˆµ)]XUU ⊤	
+ (β −¯µ)⊤
Σ−1
β
−X⊤diag[⃗φ′′(Y, X ¯µ)]X
	
= (ˆµ −¯µ)Σ−1
β
+ (β −ˆµ)⊤UU ⊤X⊤diag[⃗φ′′(Y, XUU ⊤ˆµ)]XUU ⊤
−(β −¯µ)⊤X⊤diag[⃗φ′′(Y, X ¯µ)]X.
Appealing to Taylor’s theorem, we can rewrite ⃗φ′′(Y, XUU ⊤ˆµ) as
⃗φ′′(Y, XUU ⊤ˆµ) = ⃗φ′′(Y, X ¯µ) + (XUU ⊤ˆµ −X ¯µ) ◦⃗φ′′′(Y, A)
= ⃗φ′′(Y, X ¯µ) + (XUU ⊤ˆµ −X ˆµ + X(ˆµ −¯µ)) ◦⃗φ′′′(Y, A)
= ⃗φ′′(Y, X ¯µ) −X ¯U ¯U ⊤◦⃗φ′′′(Y, A) + X(ˆµ −¯µ) ◦⃗φ′′′(Y, A)
= ⃗φ′′(Y, X ¯µ) + R,
where the ﬁrst line follows from Taylor’s theorem by appropriately choosing each Ai ∈[x⊤
i UU ⊤ˆµ, x⊤
i ¯µ], and in the fourth
line we substitute in R := −X ¯U ¯U ⊤◦⃗φ′′′(Y, A) + X(ˆµ −¯µ) ◦⃗φ′′′(Y, A).

LR-GLM: fast GLMs with low-rank approximations
We now can rewrite the difference in the gradients as
∇β log ˆp(β) −∇β log ¯p(β) = (ˆµ −¯µ)Σ−1
β
+ (β −ˆµ)⊤UU ⊤X⊤diag[⃗φ′′(Y, X ¯µ)]XUU ⊤
+ (β −ˆµ)⊤UU ⊤X⊤diag(R)XUU ⊤
−(β −¯µ)⊤X⊤diag(⃗φ′′(Y, X ¯µ))X
= (ˆµ −¯µ)⊤(Σ−1
β
−UU ⊤X⊤diag[⃗φ′′(Y, X ¯µ)]XUU ⊤)
+ (β −ˆµ)⊤UU ⊤X⊤diag(R)XUU ⊤
−(β −¯µ)⊤UU ⊤X⊤diag(⃗φ′′(Y, X ¯µ))X ¯U ¯U ⊤
−(β −¯µ)⊤¯U ¯U ⊤X⊤diag(⃗φ′′(Y, X ¯µ))XUU ⊤
−(β −¯µ)⊤¯U ¯U ⊤X⊤diag(⃗φ′′(Y, X ¯µ))X ¯U ¯U ⊤.
Which
is
obtained
by
ﬁrst
writing
X⊤diag[⃗φ′′(Y, X ¯µ)]X
in
the
fourth
line
as
(UU ⊤X⊤
+
¯U ¯U ⊤X⊤)diag[⃗φ′′(Y, X ¯µ)](XUU ⊤+ X ¯U ¯U ⊤), multiplying through and rearranging the resulting terms.
Given this form of the difference in the gradients, we may upper bound its norm as
∥∇β log ˆp(β) −∇β log ¯p(β)∥2 ≤∥ˆµ −¯µ∥2∥Σ−1
β
−UU ⊤X⊤diag[⃗φ′′(Y, X ¯µ)]XUU ⊤∥2
+ ∥β −ˆµ∥2∥UU ⊤X⊤diag(R)XUU ⊤∥2
+ ∥β −¯µ∥2∥UU ⊤X⊤diag[⃗φ′′(Y, X ¯µ)]X ¯U ¯U ⊤+
¯U ¯U ⊤X⊤diag[⃗φ′′(Y, X ¯µ)]XUU ⊤+ ¯U ¯U ⊤X⊤diag[⃗φ′′(Y, X ¯µ)]X ¯U ¯U ⊤∥2
≤∥ˆµ −¯µ∥2∥Σ−1
β
−UU ⊤X⊤diag[⃗φ′′(Y, X ¯µ)]XUU ⊤∥2
+ ∥β −ˆµ∥2∥UU ⊤X⊤diag(R)XUU ⊤∥2
+ ∥β −¯µ∥2

∥¯U ¯U ⊤X⊤diag[⃗φ′′(Y, X ¯µ)]X ¯U ¯U ⊤∥2+
2∥¯U ¯U ⊤X⊤diag[⃗φ′′(Y, X ¯µ)]XUU ⊤∥2
	
by the triangle inequality.
≤∥ˆµ −¯µ∥2
n
∥Σ−1
β ∥2 + ∥Udiag(λ)V ⊤∥2∥diag[⃗φ′′(Y, X ¯µ)]∥2∥V diag(λ)U ⊤∥2
o
+ ∥β −ˆµ∥2∥Udiag(λ)V ⊤∥2∥diag(R)∥2∥V diag(λ)U ⊤∥2
+ ∥β −¯µ∥2

∥¯Udiag(¯λ) ¯V ⊤∥2∥diag[⃗φ′′(Y, X ¯µ)]∥2∥¯V diag(¯λ) ¯U ⊤∥2+
2∥¯U ⊤diag(¯λ) ¯V ⊤∥2∥diag[⃗φ′′(Y, X ¯µ)]∥2∥V diag(λ)U ⊤∥2
	
by again using the triangle inequality, and decomposing X⊤into Udiag(λ)V ⊤+ ¯Udiag(¯λ) ¯V ⊤.
≤∥ˆµ −¯µ∥2
 ∥Σ−1
β ∥2 + λ2
1∥⃗φ′′∥∞

+ λ2
1∥β −ˆµ∥2∥R∥∞+ (¯λ2
1 + 2λ1¯λ1)∥β −¯µ∥2∥⃗φ′′∥2,
where in the last line we have shortened ⃗φ′′(Y, X ¯µ) to ⃗φ′′ for convenience.
Next noting that ∥¯µ −ˆµ∥2 ≤¯λ1c for c := ∥⃗φ′(Y,X ˆµ)∥2+λ1∥¯U ⊤ˆµ∥2∥⃗φ′′(Y,A)∥∞
α
, where α is the strong log concavity parameter
of p(β|X, Y ) (which follows from Theorem 5.2), we can see that ∥R∥∞≤¯λ1r where r := (∥U ⊤ˆµ∥∞∥⃗φ′′′(Y, A)∥∞+
λ1c∥⃗φ′′′(Y, A)∥∞). That r is bounded follows from the assumption that log p(y|x, β) has bounded third derivatives, an
equivalent to a Lipschitz condition on φ′′. We can next simplify this upper bound to
∥∇β log ˆp(β) −∇β log ¯p(β)∥2 ≤¯λ1c[∥Σ−1
β ∥2 + λ2
1∥⃗φ′′∥∞] + λ2
1¯λ1r∥β −ˆµ∥2 + ¯λ1(¯λ1 + 2λ1)∥β −¯µ∥2∥⃗φ′′∥∞
= ¯λ1

c(∥Σ−1
β ∥2 + λ2
1∥⃗φ′′∥∞) + λ2
1r∥β −ˆµ∥2 + (¯λ1 + 2λ1)∥β −¯µ∥2∥⃗φ′′∥∞

≤¯λ1

c(∥Σ−1
β ∥2 + λ2
1∥⃗φ′′∥∞) + λ2
1r∥β −ˆµ∥2 + (¯λ1 + 2λ1)(∥ˆµ −¯µ∥2 + ∥β −ˆµ∥2)∥⃗φ′′∥∞

by the triangle inequality.

LR-GLM: fast GLMs with low-rank approximations
≤¯λ1

c(∥Σ−1
β ∥2 + λ2
1∥⃗φ′′∥∞) + λ2
1r∥β −ˆµ∥2 + (¯λ1 + 2λ1)(¯λ1c + ∥β −ˆµ∥2)∥⃗φ′′∥∞

= ¯λ1

c(∥Σ−1
β ∥2 + λ2
1∥⃗φ′′∥∞) + c(¯λ2
1 + 2λ1¯λ1)∥⃗φ′′∥∞+ (λ2
1r + (¯λ1 + 2λ1)∥⃗φ′′∥∞)∥β −ˆµ∥2

= ¯λ1

c(∥Σ−1
β ∥2 + (λ1 + ¯λ1)2∥⃗φ′′∥∞) + (λ2
1r + (¯λ1 + 2λ1)∥⃗φ′′∥∞)∥β −ˆµ∥2

.
Thus, taking the expectation of this upper bound on the norm squared over β with respect to ˆp we get
d2
2,ˆp(ˆp, p) ≤Eˆp(β)

¯λ2
1
n
c
h
∥Σ−1
β ∥2 + (λ1 + ¯λ1)2∥⃗φ′′∥∞
i
+
h
λ2
1r + (¯λ1 + 2λ1)∥⃗φ′′∥∞
i
∥β −ˆµ∥2
o2
≤2¯λ2
1Eˆp(β)

c2 h
∥Σ−1
β ∥2 + (λ1 + ¯λ1)2∥⃗φ′′∥∞
i2
+
h
λ2
1r + (¯λ1 + 2λ1)∥⃗φ′′∥∞
i2
∥β −ˆµ∥2
2

since ∀a, b ∈R, (a + b)2 ≤2(a2 + b2)
= 2¯λ2
1

c2 h
∥Σ−1
β ∥2 + (λ1 + ¯λ1)2∥⃗φ′′∥∞
i2
+
h
λ2
1r + (¯λ1 + 2λ1)∥⃗φ′′∥∞
i2
Eˆp(β)[∥β −ˆµ∥2
2]

= 2¯λ2
1

c2 h
∥Σ−1
β ∥2 + (λ1 + ¯λ1)2∥⃗φ′′∥∞
i2
+
h
λ2
1r + (¯λ1 + 2λ1)∥⃗φ′′∥∞
i2
tr(ˆΣ)

.
Next noting that ¯p is strongly ∥¯Σ∥−1
2
log-concave, we may apply Theorem F.1, stated below, to obtain that
W2(ˆp, ¯p) ≤∥¯Σ∥2
s
2¯λ2
1

c2
h
∥Σ−1
β ∥2 + (λ1 + ¯λ1)2∥⃗φ′′∥∞
i2
+
h
λ2
1r + (¯λ1 + 2λ1)∥⃗φ′′∥∞
i2
tr(ˆΣ)

≤
√
2¯λ1∥¯Σ∥2

c
h
∥Σ−1
β ∥2 + (λ1 + ¯λ1)2∥⃗φ′′∥∞
i
+
h
λ2
1r + (¯λ1 + 2λ1)∥⃗φ′′∥∞
i q
tr(ˆΣ)

,
which is our desired upper bound.
Theorem F.1. Suppose that p(β) and q(β) are twice continuously differentiable and that q is α-strongly log concave. Then
W2(p, q) ≤α−1d2,p(p, q),
where W2 denotes the 2-Wasserstein distance between p and q.
Proof. This follows from Huggins et al. (2018) Theorem 5.2, or similarly from Bolley et al. (2012) Lemma 3.3 and
Proposition 3.10.
F.7. Proof of bounded asymptotic error
We here provide a formal statement and proof of Theorem 5.7, detailing the required regularity conditions.
Theorem F.2 (Asymptotic). Assume xi
i.i.d.
∼p∗for some distribution p∗such that Ep∗[xix⊤
i ] exists and is non-singular
with diagonalization Ep∗[xix⊤
i ] = U ⊤
∗diag(λ)U∗+ ¯U ⊤
∗diag(¯λ) ¯U∗such that min(λ) > max(¯λ). Additionally, for a strictly
concave (in its second argument), twice differentiable log-likelihood function φ with bounded second derivatives (in both
arguments) and some β ∈RD, let yi|xi ∼exp{φ(yi, xT
i β)}. Also, suppose that E∥yi∥2
2 < ∞. Then if p(β) is log-concave
and positive on RD, the asymptotic error (in N) of the exact relative to approximate maximum a posteriori parameters,
ˆµ = limN→∞ˆµN and ¯µ = limN→∞¯µN is ﬁnite (where ˆµN and ¯µN are the approximate and exact MAP estimates,
respectively, after N data-points), i.e., limn→∞∥ˆµN −¯µN∥exists and is ﬁnite.
Proof. Before beginning, let P denote a Borel probability measure on the sample space on which our random variables,
{xi} and {yi}, are deﬁned such that these random variables are distributed as assumed according to P. In what follows we
demonstrate the asymptotic error is ﬁnite P-almost surely. To this end, it sufﬁces to show that ˆµN
a.s.
→ˆµ and ¯µN
a.s.
→¯µ for
some ˆµ, ¯µ in RD.

LR-GLM: fast GLMs with low-rank approximations
Strong convergence of the exact MAP (¯µN
a.s.
→¯µ)
This follows from Doob’s consistency theorem (Van der Vaart, 2000, Theorem 10.10). The only nuance required in the
application of this theorem here is that we must accommodate the regression setting. However by constructing a single
measure P governing both the covariates and responses, this simply becomes a special case of the usual theorem for
unconditional models.
Strong convergence of the approximate MAP (ˆµN
a.s.
→ˆµ)
In contrast to the strong consistency of ¯µN, showing convergence of ˆµN requires more work. This is because we cannot rely
on standard results such as Bernstein–Von Mises or Doob’s consistency theorem, which require correct model speciﬁcation.
Since we have introduced the likelihood approximation ˜p(y|x, β) ̸= p(y|x, β), the vector ˆµN is the MAP estimate under a
misspeciﬁed model.
We demonstrate almost sure convergence in two steps; ﬁrst we show that U ⊤
∗ˆµN converges almost surely to some γ∗∈RM;
then we show that ˆµN = U∗U ⊤
∗ˆµN + ¯UN ¯U ⊤
N ˆµN must converge as a result. Since UNU ⊤
N
a.s.
→U∗U ⊤
∗(as follows from
entry-wise almost sure convergence of 1
N X⊤X →Ep∗[xix⊤
i ] and the Davis–Kahan Theorem (Davis & Kahan, 1970)), this
guarantees strong convergence of ˆµN = UNU ⊤
N ˆµN + ¯UN ¯U ⊤
N ˆµN.
Part I: strong convergence of the projected approximate MAP, U∗ˆµN
a.s.
→γ∗
Let U∗∈RD,M be the top M eigenvectors of Ep∗[xix⊤
i ], and recall that by assumption for any y, φ(y, xT
i β) is a strictly
concave function of x⊤
i β, in the sense that for any y and any b, b′ in R and t in (0, 1) with b ̸= b′, φ(y, tb + (1 −t)b′) >
tφ(y, b)+(1−t)φ(y, b′). Then by Lemma F.2 we have that there is a unique maximizer γ∗= arg maxγ∈RM E[φ(y, x⊤U∗γ)]
We next note that the Hessian of the expected approximate negative log likelihood with respect to γ is positive deﬁnite
everywhere,
∇2
γ −Ey∼p(y|x,β),x∼p∗[φ(y, x⊤U∗γ)] = −E[
 ∇γφ′(y, x⊤U∗γ)

x⊤U∗] = −U ⊤
∗E[xφ′′(y, x⊤U∗γ)x⊤]U∗≻0
since the strict log concavity and twice differentiability of φ ensure that −E[xφ′′(y, x⊤U∗γ)x⊤] ≻0.
Now consider any compact neighborhood K ⊂RM containing γ∗as an interior point. Then, by Lemma F.3 the set
F = {fγ : X × Y →R, (x, y) 7→φ(y, x⊤U∗γ)|γ ∈K} is P-Glivenko–Cantelli. As such supfγ∈F | 1
N
PN
i=1 fγ(xi, yi) −
E[fγ(xi, yi)]|
a.s.
→0, that is to say, the empirical average log-likelihood converges uniformly to its expectation across
all γ ∈K. As a result, we have that for γN := arg maxγ∈K log ˜p(U∗β = γ|X, Y ) = arg maxγ∈K
1
N

log p(U T
∗β =
γ) + PN
i=1 φ(yi, x⊤
i U∗γ)

, γN
a.s.
→γ∗.
It remains in this part only to show that convergence of the approximate MAP parameter within this subset K implies
convergence of U ⊤
∗ˆµ, the approximate MAP parameter (across all of RM). However, this follows immediately from the
strict log concavity of the posterior; because γ∗∈K◦, for N large enough each γN ∈K◦and we may construct a
sub-level set such that γN ∈CN ⊂K such that ∀γ /∈CN, log p(U ⊤
∗β = γ) + PN
i=1 φ(yi, x⊤
i U∗γ) < log p(U ⊤
∗β =
γN) + PN
i=1 φ(yi, x⊤
i U∗γN).
Part II: convergence of ¯U∗¯U ⊤
∗ˆµN + U∗γ
Using the result of Part I, we can write that ˆµN = U∗U ⊤
∗ˆµN + ¯U∗¯U ⊤
∗ˆµN →U∗γ∗+ ¯U∗¯U ⊤
∗ˆµN. However, since
¯U∗¯U ⊤
∗β ⊥X, Y |U ⊤
∗β under P, convergence of U ⊤
∗ˆµN →γ∗implies convergence of arg max ¯U ⊤
∗β ˜p( ¯U ⊤
∗β|U ⊤
∗β =
U ⊤
∗ˆµN, X, Y ) = arg max ¯U ⊤
∗β ˜p( ¯U ⊤
∗β|U ⊤
∗β = U ⊤
∗) to some ¯U ⊤
∗ˆµN since continuity of p(β) and ˜p(Y |X, β) imply
continuity of the arg-max. Thus both ˆµN and ¯µN converge, guaranteeing convergence of the asymptotic error.
Lemma F.2. For any φ(·, ·) which is strictly concave in its second argument, if there is a global maximizer β∗=
arg maxβ∈RD V (β) = Ex∼p∗,y∼p(y|x,β)[φ(y, x⊤β)], then there is a unique global maximizer,
γ∗= arg max
γ∈RM
V (U∗γ)
Proof. We ﬁrst note that V (·) must have bounded sub-level sets. Thus W(·) := V (U∗·) must also have bounded sub-level
sets since V −1([a, ∞]) = {β|V (β) ≥a} ⊃{β|∃γ ∈RM s.t. β = U∗γ and V (U∗γ) ≥a} = U∗W −1([a, ∞]). Thus,
since W is strictly concave and has bounded sub-level sets, it has a unique maximizer.

LR-GLM: fast GLMs with low-rank approximations
Lemma F.3. Let K ⊂RM be compact and denote by X and Y the domains of the covariates and responses, respectively.
Then under the assumptions of Theorem F.2, the set F = {fγ : X × Y →R, (x, y) 7→φ(y, x⊤Uγ)|γ ∈K} is P-Glivenko–
Cantelli.
Proof. This result follows from Theorem 19.4 in (Van der Vaart, 2000), and builds from example 19.7 of the same reference;
in particular, the condition of bounded second derivatives of φ implies that for any fγ, fγ′ in F and x in X, y in Y , we have
|fγ(x, y) −fγ′(x, y)| ≤C∥x∥2
2. The previous condition is sufﬁcient to ensure ﬁnite bracketing numbers, and the result
follows. Notably, in keeping with example 19.7 we have that for all x, y and for all γ and γ′ in K,
|fγ(x, y) −fγ′(x, y)| = |
Z x⊤Uγ
x⊤Uγ′ φ′(y, a)da|
= |
Z x⊤Uγ
x⊤Uγ′ φ′(y, x⊤Uγ′) +
Z a
x⊤Uγ′ φ′′(y, b)db da|
≤∥x⊤U(γ −γ′)φ′(y, x⊤Uγ′)∥2 + 1
2∥x⊤U(γ −γ′)∥2
2 sup
a∈R
φ′′(y, a)
≤∥x⊤U∥2(∥y∥2 + ∥x⊤U∥2∥γ′∥2)φ′′
max∥γ −γ′∥2 + 1
2∥x⊤U∥2
2∥γ −γ′∥2
2φ′′
max
≤
3
2∥x⊤U∥2
2diam(K)φ′′
max + ∥x⊤U∥2∥y∥2φ′′
max

∥γ −γ′∥2
≤C(∥x⊤U∥2
2 + ∥x⊤U∥2∥y∥2)∥γ −γ′∥2,
(F.7)
where in the ﬁrst and second lines we use the fundamental theorem of calculus, and in the fourth and ﬁfth lines we rely on
the boundedness of the second derivatives of φ and that the compactness subsets of RM implies boundedness. In the ﬁnal
line C is an absolute constant.
Finally, we note that EP∥x⊤U∥2
2 < ∞since EP∥x⊤U∥2
2 = EPx⊤UU ⊤x < EPx⊤x = Tr(Ep∗xx⊤) < ∞, and by Cauchy
Schwartz, EP∥x⊤U∥2∥y∥2 ≤
p
EP∥x⊤U∥2
2EP∥y∥2
2 ≤∞. This conﬁrms (as in example 19.7 (Van der Vaart, 2000)) that
for all ϵ > 0, the ϵ-bracketing number of F is ﬁnite. By Theorem 19.4 of (Van der Vaart, 2000), this proves that F is
P-Glivenko-Cantelli.
F.8. Factorized Laplace approximations underestimate marginal variances
We here illustrate that the factorized Laplace approximation underestimates marginal variances. Consider for simplicity the
case of a bivariate Gaussian with
Σ =
a
b
b
c

,
for which the Hessian evaluated anywhere is
Σ−1 =
1
ac −b2

c
−b
−b
a

.
Ignoring off diagonal terms and inverting to approximate ΣN, as is done by a diagonal Laplace approximation, yields:
˜Σ =
"
a −b2
c
0
0
c −b2
a
#
.
This approximation reports marginal variances which are lower than the exact marginal variances.
That this approximation underestimates marginal variances in the more general D > 2 dimensional case may be easily
seen from considering the block matrix inversion of Σ, with blocks of dimension 1 × 1, (D −1) × 1, 1 × (D −1) and
(D −1) × (D −1), and noting that the Schur complement of a positive deﬁnite covariance matrix will always be positive
deﬁnite.

LR-GLM: fast GLMs with low-rank approximations
G. LR-MCMC
We provide the LR-MCMC algorithm for performing fast MCMC in generalized linear models with low-rank data approxi-
mations.
Algorithm 2 LR-MCMC for Bayesian inference in GLMs with low-rank data approximations.
1: Input: prior p(β), data X ∈RN,D, rank M ≪D, GLM mapping φ, MCMC transition kernel q(·, ·), number of
MCMC iterations T. Time and memory complexities that are not included depend on the speciﬁc choice of MCMC
transition kernel.
2: Pseudo-Code
3: Time Complexity
4: Memory Complexity
5: Data preprocessing — M-Truncated SVD
6: U, diag(λ), V := truncated-SVD(X⊤, M)
O(NDM)
O(NM + DM)
7: XU = XU
O(NM)
O(NDM)
8: Propose β(t) ∈RD, compute likelihood
9: β(t) ∼q(β(t), β(t−1))
—
—
10: Lt := PN
i=1 φ(yi, x⊤
i UU ⊤β(t)) + log p(β(t))
O(1)
O(NM + MD)
11: Accept or Reject
12: Acceptance probability pA := min

1,
Lt
Lt−1

O(1)
O(1)
13: Accept β(t) with probability pA
O(1)
O(1)
14: Repeat steps 3-6 for T iterations
The transition in Line 9 may additionally beneﬁt from the LR-GLM approximation. In particular, widely used algorithms such
as Hamiltonian Monte Carlo and the No-U-Turn Sampler rely on many O(ND)-time likelihood and gradient evaluations,
the cost of which can be reduced to O(NM + DM) with LR-GLM. An implementation of this approximation is given in
the Stan model in Appendix A.3 with performance results in Figures A.3 and A.6.
H. LR-Laplace with non-Gaussian priors
As discussed in the main text, we can maintain computational advantages of LR-GLM even when we have non-Gaussian
priors. This admits the procedure provided in Algorithm 3.
In order for this more general LR-Laplace algorithm to be computationally efﬁcient, we still require that the prior have
some properties which can accommodate efﬁciency. In particular Line 11 demands that the Hessian of the prior is computed
and inverted, as will true even in the high-dimensional setting when, for example, the prior factorizes across dimensions.
Additionally, properties of the prior such as log concavity will facilitate efﬁcient optimisation in Line 8.
7To keep notation concise we use ⃗φ′′
ˆµ to denote ⃗φ′′(Y, XUU ⊤ˆµ)

LR-GLM: fast GLMs with low-rank approximations
Algorithm 3 LR-Laplace for Bayesian inference in GLMs with low-rank data approximations and twice differentiable prior.
Time and memory complexities which are not included depend on the choice of prior and optimisation method, which can
be problem speciﬁc.
1: Input: twice differentiable prior p(β), data X ∈RN,D, rank M ≪D, GLM mapping φ with φ′′ (see Eq. (5)
and Section 5.1)
2: Pseudo-Code
3: Time Complexity
4: Memory Complexity
5: Data preprocessing — M-Truncated SVD
6: U, diag(λ), V := truncated-SVD(XT , M)
O(NDM)
O(NM + DM)
7: Optimize to ﬁnd approximate MAP estimate (in D-dimensional space)
8: ˆµ := arg maxµ∈RD PN
i=1 φ(yi, xiUU ⊤µ) + log p(β = µ)
—
—
9: Compute approximate posterior covariance7
10: ˆΣ−1 := −∇2
β log pβ(ˆµ) −UU ⊤X⊤diag(⃗φ′′
ˆµ)XUU ⊤
—
—
11: K := [∇2
β log p(β)|β=ˆµ]−1
—
—
12: ˆΣ := −K+KU
 [U ⊤X⊤diag(⃗φ′′
ˆµ)XU]−1+U ⊤KU
−1U ⊤K
—
—
13: Compute variances and covariances of parameters
14: Varˆp(βi) = e⊤
i ˆΣei
—
—
15: Covˆp(βi, βj) = e⊤
i ˆΣej
—
—

