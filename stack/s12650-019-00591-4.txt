REGULAR PAPER
Yi Cao
• Zeyao Mo • Zhiwei Ai • Huawei Wang • Li Xiao • Zhe Zhang
Parallel visualization of large-scale multiﬁeld scientiﬁc
data
Received: 6 July 2019 / Accepted: 31 July 2019 / Published online: 24 August 2019
 The Visualization Society of Japan 2019
Abstract Following the recent rapid growth in supercomputer performance, many real-world problems in
ﬁelds such as nuclear fusion energy and electromagnetic environments can be solved via multiphysics
simulation, which outputs multiﬁeld datasets. However, current multiﬁeld visualization has difﬁculty
handling multiphysics parallel simulation data. First, it is difﬁcult to correctly visualize overlapping mul-
tiﬁeld data with semitransparent properties because of the complex distribution of partitioned data domains
across multicore processors. Second, the interactive visualization performance of large-scale multiﬁeld data
in serial processing mode on a personal computer is often slow because multiphysics simulations can
produce large-scale datasets, i.e., of the order of gigabytes to terabytes. Considering the ﬁdelity and efﬁ-
ciency of large-scale data visualization on supercomputer, a new parallel visualization method is required
for multiﬁeld scientiﬁc data that do not change the original distribution of the mesh data generated by the
multiphysics applications. This paper introduces a hybrid scheduling framework for the parallel visual-
ization of large-scale multiﬁeld scientiﬁc data. This framework is used to overcome problems both in correct
visual representation and in efﬁcient visualization of large-scale multiphysics applications. We discuss the
results of several typical multiphysics applications to verify the feasibility and reliability of our proposed
framework. This framework currently supports scalable in situ visualization of up to 8.5 billion mesh cells
on the 10 k cores of China’s Tianhe-2 supercomputer, which could help domain scientists understand
multiphysics phenomena more clearly and accurately.
Keywords Multiphysics simulation  Multiﬁeld visualization  Parallel visualization  In situ visualization
Y. Cao (&)  Z. Mo  Z. Ai  H. Wang  L. Xiao  Z. Zhang
High Performance Computing Center, Institute of Applied Physics and Computational Mathematics, Beijing, China
E-mail: cao_yi@iapcm.ac.cn
Z. Mo
E-mail: zeyao_mo@iapcm.ac.cn
Z. Ai
E-mail: azw@iapcm.ac.cn
H. Wang
E-mail: wang_huawei@iapcm.ac.cn
L. Xiao
E-mail: xiaoli@iapcm.ac.cn
Z. Zhang
E-mail: zhang_zhe@iapcm.ac.cn
J Vis (2019) 22:1107–1123
https://doi.org/10.1007/s12650-019-00591-4

1 Introduction
In real-world applications, the simulation of large numbers of complex physical phenomena that are usually
composed of various tightly coupled physical processes is known as multiphysics simulation. To simplify
the difﬁculty of numerical simulation, these physical phenomena are usually split into multiple processes
with different physical descriptions. Thus, scientists can propose different two-dimensional (2D) or 3D
physical models and develop different numerical methods and parallel computation applications. Finally,
these programs can be coupled to perform local or global numerical simulations to gain insight into physical
laws. This is a process known as multiphysics simulation. Multiphysics simulation can highlight the
complex physical phenomena through which multiple physical processes are coupled within a complex
system, and it can reveal new physical phenomena that cannot be elucidated via single-physics simulation in
the ﬁelds of science and engineering.
Following the recent rapid growth of supercomputer performance, many real-world problems can now be
overcome using multiphysics simulation. For example, multiphysics simulation is used widely to solve
complex multimaterial, multiscale, and multidiscipline 3D problems. The demand for the visualization of
coupled multiphysics ﬁelds has emerged in several important application domains, e.g., climate change,
weather forecasting, electromagnetic (EM) environments, and nuclear energy. The datasets output by such
applications contain physical multiﬁeld data such as scalar ﬁelds, vector ﬁelds, or tensor ﬁelds that describe
multiphysics processes.
However, most research currently undertaken by the visualization community is limited to single-physics
problems. Multiﬁeld visualization currently faces two considerable challenges. First, it is difﬁcult to cor-
rectly visualize overlapping multiﬁeld data with semitransparent properties because of the complex dis-
tribution of partitioned data domains across multicore processors, especially when the domains also have
features such as concave, disconnected, or non-manifold structures. Second, the interactive visualization
performance of large-scale multiﬁeld data in serial processing mode on a personal computer is slow, even
when local graphics hardware acceleration is employed. This is because multiphysics simulation can pro-
duce large-scale datasets (of the order of gigabytes to terabytes) that can far exceed the capacity of GPU
memory. More importantly, to reduce the data movement overhead and maintain scalability when using
in situ visualization on current supercomputer, a parallel visualization method is required for directly
processing the simulation data without changing the original distribution of the mesh data generated by the
multiphysics applications.
To help domain scientists understand accurately and efﬁciently the complex phenomena behind mul-
tiﬁeld data, we present a scalable parallel visualization framework for large-scale multiﬁeld scientiﬁc data.
The main contributions of this paper comprise the following:
(1)
A multiplex visual channel-based visualization strategy to encode multiﬁeld data based on a proper
combination of visual channels that could enable scientists to correctly decode phenomena from the
resultant images. Multiple visual channels can also be used to separate opaque and transparent objects
within the visualization pipeline, thus reducing the number of objects that require sorting during
translucent rendering. Therefore, not only the visual accuracy but also the efﬁciency of visualization
can be improved.
(2)
A hybrid scheduling framework for parallel visualization that improves the efﬁciency of large-scale
multiﬁeld visualization, by taking advantage of data locality within multiple visual channels. The
parallel sorting between the subdomains in the original object space is converted to a sorting between
the sampling points in the 3D screen space within our framework. The sorting of the sampling points is
independent of the complex mesh data characteristics; therefore, there will be no error arising from
ambiguity regarding the sorting of scene objects. Thus, this framework can solve the parallel
translucent rendering problem of complex domains in multiphysics simulations.
Experimental tests were performed for typical applications in ﬁelds such as fusion energy and EM
environments. It was demonstrated that the parallel visualization framework presented in this paper could
correctly visualize two or three coupled physical single ﬁelds of a multiﬁeld dataset with complex domain
structures or data distribution across multiple cores. Moreover, the proposed framework can also support
scalable in situ visualization of 8.5 billion mesh data under the 10 k cores of China’s Tianhe-2 super-
computer, which could be a promising solution to the challenges of ultra-scale data visualization in exascale
computing systems. The experimental results show that the proposed method can satisfy the visualization
needs of typical multiphysics applications.
1108
Y. Cao et al.

2 Related work
In the ﬁeld of science and engineering computing, multiphysical simulation is involved in many complex
applications (Lethbridge 2005), e.g., fusion and ﬁssion energy, complex EM environments, global climate
change, and weather forecasting. Most of the key application areas cut across multiple disciplines and
domains. With the development of computational science, multiphysics simulation has become one of the
most important approaches for solving such complex problems.
Currently, most typical domain applications still use traditional single-ﬁeld visualization methods to
support multiﬁeld data analysis because of the limitations of multiﬁeld data visualization. Furthermore,
constrained by the interactive performance of large-scale data analysis, domain-driven applications rely
primarily on 2D cross section-based visualization to reveal spatial trends in 3D space. Given the increasing
complexity of real-world problems, these traditional methods lead to inefﬁcient analysis of multiﬁeld data
and low conﬁdence in the derived results. Therefore, a new visualization method is urgently required,
especially when using a high-performance supercomputer for multiphysical simulation.
Sauber et al. (2006) presented a graph approach to visualizing correlations in 3D multiﬁeld scalar data.
Volume rendering (Kajiya and Von Herzen 1984) is a 3D data visualization method suitable for representing
complex structures and physical phenomena within a physical space (Wes Bethel 2009). However, volume
rendering is also one of the most difﬁcult methods to implement in multiﬁeld visualization. Multiﬁeld
volume rendering requires a stage of visual fusion within the visualization pipeline, and traditional rendering
methods only support fusion between volume rendering and geometric rendering (Levoy 1990; Kreeger and
Kaufman 1999). For example, Insley and Grinberg (2011) introduced a hybrid rendering method for fusion
between opaque volume and translucent geometric surfaces. Translucent fusion can reveal additional details
of physical phenomena, but, to obtain the correct fusion results of multiﬁeld data, it also increases the
difﬁculty of algorithm implementation.
Many approaches (Jacq and Roux 1997; Kniss et al. 2003) have mapped the attributes of multiﬁeld data
into color space and implemented fusion at the color level. In this approach, color is the only visual
representation used to identify a physical ﬁeld. However, the new color generated by color fusion interferes
with the recognition of the original physical ﬁeld, causing problems of visual confusion. As mentioned in
Cai and Sakas (1999) and Fuchs and Hauser (2009), in comparison with the image-level fusion method,
fusion results based on the ray accumulation-level method can represent more accurately the depth rela-
tionship of a physical structure. To obtain accurate fusion results, translucent fusion also relies on the sorting
of semitransparent rendering objects based on the depth of the ray in the direction of the human eye.
However, existing sorting methods have certain limitations in terms of both ﬁdelity and efﬁciency: (1) The
Z-buffering method (Eberly 2006) does not support sorting between two arbitrary translucent objects. (2)
The painter algorithm (Porter and Du 1984) cannot handle the problem of intersecting objects with ambi-
guity. (3) The backface culling method (Eberly 2006) cannot be used for sorting concave objects. (4) To
render a scene with depth complexity n, the depth peeling method (Plate et al. 2007) requires n stages to
obtain accurate fusion results, which leads to high algorithm complexity and accelerated hardware
dependency. (5) The mesh subdivision method needs the convex subdivision of a mesh to determine
overlapping areas, and it then utilizes the methods in Eberly (2006) and Porter and Du (1984) to sort the
objects to obtain accurate translucent rendering results. Mesh subdivision results in a rapid increase in data
volume, which makes it unsuitable for large-scale multiﬁeld data visualization.
Most of the methods presented above sort primitives in object space before they are rasterized and are
therefore susceptible to the complex shape of primitives. In contrast, our current research is inspired by the
fact that sampling the points of primitives projected onto 3D screen space does not cause the sorting
problem. Ma and Crockett (2001) presented a parallel cell-projection algorithm instead of ray casting to
achieve a static load balance coupled with an asynchronous communication strategy. However, in that
method, it is crucial to compute the correct visibility ordering of the cells. Giersten (1992) pioneered the use
of sweep algorithms in volume rendering. The input polyhedrons are all scan-converted in an order
determined by passing a plane or a line over the data. We have chosen a cell-projection method similar to
Giertsen (1992) because the regular grid utilized in their simpler 2D ray casting means it is not necessary to
compute explicitly the visibility ordering of the cells. Childs et al. (2006) proposed a hybrid sampling
scheme for a parallel volume rendering method to address the load balancing issues for meshes with large
variation in spatial size. Binyahib et al. (2018) further improved the communication performance of the
original algorithm presented in Childs et al. (2006) by a partial compositing of a group of consecutive
samples before sending them. However, the optimized scheme presented in Binyahib et al. (2018) is not
Parallel visualization of large-scale multiﬁeld scientiﬁc data
1109

suitable for multiﬁeld visualization because it cannot provide an accurate composition of the partial com-
posites and geometric image.
To maintain scalability when using in situ visualization (Yu et al. 2010; Rivi et al. 2012), a new parallel
visualization method is also required for direct processing of the simulation data without changing the
original distribution of the mesh data generated by the multiphysics applications. In recent work, Usher et al.
(2019) and Wang et al. (2018) presented a CPU ray tracing approach for high-quality rendering of general or
adaptive mesh reﬁnement (AMR) data. However, most previous studies in multiﬁeld visualization have been
limited to handling small-scale data on a personal computer. Often, they must even change the data
distribution of the numerical simulation results to facilitate the implementation of parallel visualization. For
the ﬁdelity and efﬁciency of large-scale data visualization on a supercomputer, a new parallel visualization
framework is required for multiphysics simulation data that can maintain the original mesh data generated
by the multiphysics simulations.
3 Mesh systems in multiphysics simulations
Currently, most parallel rendering methods in scientiﬁc visualization consider only simple types of mesh
systems, e.g., convex meshes or domains, in which each domain consists of one or more mesh cells.
However, the mesh systems used in multiphysics simulations are usually complicated. Typical mesh data
characteristics of multiphysics simulations are listed below.
3.1 Adaptive irregular meshes
Computational problem solving for real-world applications often involves complex boundary conditions of
physical models. Therefore, the numerical solver of an application often uses a complex mesh system with
speciﬁc characteristics (e.g., irregular and adaptive meshes) to better adapt to complex boundary problems
(Exascale Programming Challenges 2011). Often, these types of mesh data present complex domain shapes
such as concavities, circles, and holes.
3.2 Domain-decomposed meshes
Domain decomposition is also often used to solve boundary value problems in numerical partial differential
equations (Exascale Programming Challenges 2011). This method can decompose a complex boundary
value problem into a boundary value problem of a smaller subdomain, which is solvable using adjacent
subregions. Because problem solving for such subdomains is reasonably independent, it is highly suit-
able for parallel computing. However, the region shapes of the decomposed domain might become more
complex. For example, an original convex domain could be decomposed into several concave subdomains.
3.3 Mesh data after load balancing
Applications in the ﬁeld of parallel computing often use static or dynamic load balancing to achieve efﬁcient
large-scale numerical simulation (Exascale Programming Challenges 2011). However, load balancing can
change the connectivity of the original problem domain. For example, if a parallel process is newly allocated
mesh data from another process during the load balancing strategy, both the newly allocated and the original
mesh data on this parallel process might constitute a new complex domain with concave, disconnected, or
non-manifold features. Finally, the load balancing strategy of visualization is usually different from the
numerical simulation. Therefore, a visualization process running on a processor core might load mesh data
contained in different simulation processes. These mesh data could create a new domain with concave,
disconnected, or non-manifold features.
3.4 Patch-centric mesh data
In the multiphysics applications of our work, the mesh data distributed on the multicore processors are
divided into three levels (Mo et al. 2010), as shown in Fig. 1, which are deﬁned from the ﬁnest to the
coarsest level, respectively: a mesh cell, a patch composed of several mesh cells, and a subdomain con-
sisting of several patches. Among them, the patch (Mo et al. 2010) is the basic data model of parallel
1110
Y. Cao et al.

computing. A patch with an appropriate size not only better adapts to the complex shapes of physical
models, but also helps to increase the hit rate of the computer cache, which in turn improves parallel
computing performance. In the following ﬁgures in this paper, the basic unit of data distributed on the
multicore processors is a patch.
Because of the complexity of the problem solution, multiphysics simulation will produce a complex data
distribution, which is manifested as the mesh intersection of the overlapping multiﬁeld data and the complex
shape of the subdomain. Figure 2 shows the results of a multiphysics simulation in the ﬁeld of laser-plasma
interaction (Zhang et al. 2013) and the corresponding patch-centric mesh data distribution of different
processor cores. Patch data with the same colors are assigned to the same processor core. This ﬁgure shows
that many subdomains distributed on same processor core have concave or even disconnected features.
3.5 Visualization problem
The above mesh data features of multiphysics simulations will eventually lead to a translucent rendering
problem. Figure 3 shows two subdomains exhibiting a self-occlusion relationship. During general CPU-
based sort-last parallel rendering, the data of these two subdomains are allocated to two processor cores.
Simpler sorting techniques are used, such as sorting primitives by their centroid, to generate a nearly sorted
geometry. According to the direction of the ray shown in Fig. 3, the sort value of each subdomain is equal.
Therefore, sorting will fail during parallel rendering, resulting in incorrect visualization results. Figure 4
shows a translucent rendering example of two intersecting mesh cells. The general sorting-based rendering
method will yield the wrong result, as shown in Fig. 4b. It is difﬁcult to obtain the correct rendering results,
(a) Subdomain                    (b) Patch                (c) Mesh cell
Fig. 1 Three levels of patch-centric mesh data
Fig. 2 Mesh system of a laser-plasma interaction application
Fig. 3 Sort-last rendering of two subdomains with self-occlusion
Parallel visualization of large-scale multiﬁeld scientiﬁc data
1111

as shown in Fig. 4c, unless the data preprocessing of the convex decomposition of the mesh or a highly
complex algorithm such as depth peeling is utilized. However, its more complex data distribution and
rendering algorithm are not suitable for large-scale multiphysics applications.
4 Hybrid scheduling framework for parallel visualization of multiﬁeld scientiﬁc data
To overcome the hybrid parallel rendering problem without changing the original distribution of the mesh
data of the simulation, this paper proposes a hybrid scheduling framework for the parallel visualization of
multiﬁeld scientiﬁc data.
4.1 Visual representation
4.1.1 Multiplex visual channels
The basic task of multiﬁeld visualization is to show the correlation between multiphysics ﬁelds, allowing
users to correctly associate the feature distributions in the physical space. To visually represent computa-
tional multiﬁeld data, we introduced multiplex visual channels (Bertin 1983). Taking advantage of the
multichannel characteristics of the human vision system (HVS model 2019), we consider multiﬁeld visu-
alization as a coding process. By properly encoding the multiﬁeld (e.g., scalar ﬁeld, vector ﬁeld, and feature
ﬁeld) data through a combination of multiple visual channels, domain experts are able to correctly decode
the physical phenomena contained within a multiﬁeld visualization image. The visual encoding method
refers to the hybrid parallel rendering based on multiple channels presented in this paper.
In this paper, multiplex visual channels are divided into two types: geometric and optical channels.
Figure 5 illustrates multiphysics data containing four physics ﬁelds, A, B, C, and D, and the operations that
act between them. Geometric channels, which provide a means for distinguishing visual objects based on
geometric properties, are designed to handle only opaque geometry; therefore, the occlusion of 3D spatial
structure is a problem to be solved. Optical channels, which provide a means for distinguishing visual
objects based on optical effects, are valid and effective for cases where there is no spatial multiﬁeld
overlapping. However, for cases with spatial overlapping, the algorithm complexity of the parallel rendering
in the optical channels will increase, whereas the accuracy of the multiﬁeld visualization will be reduced.
Our solution for visualizing overlapping multiﬁeld data is to use both the geometric and optical channels.
Then, different visual fusion operations, which can be used to separate opaque and transparent objects
within the visualization pipeline, are deﬁned in different kinds of channels. For example, the rendering of
opaque objects in geometric channels only needs a simple depth comparison test, which is independent of
the mesh data characteristics, thereby reducing the number of objects that require complex sorting in the
Fig. 4 Translucent rendering of two intersecting mesh cells
Fig. 5 Multiplex visual channels for multiﬁeld visualization
1112
Y. Cao et al.

translucent rendering of the optical channels. In contrast, the rendering of translucent objects in the optical
channel uses the over operator of the painter’s algorithm during the ray accumulation stage of volume
rendering. Therefore, not only the visualization efﬁciency but also the visual accuracy can be improved.
4.1.2 Domain-speciﬁc combination of visual channels
Constrained by the limitations of human visual perception and the properties of the physical ﬁeld, a visual
channel can also have channel overload problems. When the color (for example, the visible spectrum in the
sensor data) becomes an intrinsic property of a certain physical ﬁeld, it is difﬁcult to use a color blending
method to represent other physics properties while maintaining its own original color representation.
Therefore, the combination of visual channels needs to be well designed based on domain knowledge to
realize the reasonable distribution of a multiphysics ﬁeld in multiple visual channels and effectively avoid
information overload.
The prior knowledge of domain scientists can guide the encoding of a multiphysics ﬁeld with a rea-
sonable combination of multiple visual channels. First, it is an effective combination design method to
classify a multiphysics ﬁeld as main and auxiliary ﬁelds through domain knowledge and then map them to
different types of visual channels that can be used for qualitative or quantitative visual representation.
Second, when a physical ﬁeld is selected as the primary or background ﬁeld, other physical ﬁelds are then
deﬁned as auxiliary ﬁelds, which are then used to display local physical features or phenomena on the
background. The strategy of mapping the primary and auxiliary ﬁelds to different visual channels can
minimize visual confusion due to color blending. It can also reasonably distribute the multiphysics ﬁeld
within the limited screen space, avoiding channel overload. In addition, because the visual cortical cells of
the human eye have different sensitivities to the information from the geometric channel and the optical
channel, when the multiphysics ﬁeld is encoded by different types of channels, the domain scientists can rely
on domain knowledge to correctly distinguish them visually within a visualization image, i.e., to decode it.
4.2 Parallel visualization framework
Based on the design of the geometric and optical channels, the proposed framework utilizes a hybrid sort-
last and sort-middle parallel rendering scheme for multiﬁeld visualization, as shown in Fig. 6. There are
also two predeﬁned conditions for the framework:
Condition 1 During parallel rendering, the rendering process running on each processor core can be
arbitrarily assigned the domain-decomposed mesh and the variable data of multiphysics ﬁelds. The domain
shape of the local multiﬁeld data within each rendering process can be inconsistent, overlapping, and
intersecting.
Condition 2 The rendering tasks on each processor core include the rendering of both the geometric and
the optical channels. The processing data in the geometric channel are deﬁned as geometric data with
opaque properties, the processing of which adopts the opaque geometric rendering method. The optical
channel can process both the transparent object and the opaque object concurrently, and the processing
adopts the CPU ray casting-based parallel 3D volume rendering method.
4.2.1 General parallel rendering
Parallel visualization generally involves ﬁve phase processes: data partitioning, data distribution, rendering,
image composition, and display. The domain decomposition strategy of the numerical simulation determines
how the global data are divided and distributed. Although visualization can regenerate partitioned data that
are more suitable for visualization through preprocessing, the data are unsuitable for large-scale simulation
applications, especially in situ visualization for ultra-scale simulations. Therefore, the parallel visualization
discussed in this paper does not consider changing the original data partitioning results.
For general parallel rendering (Molnar et al. 1994), the rendering is considered as a sort problem based
on screen space, which involves the modes sort-ﬁrst, sort-middle, and sort-last. Mainstream visualization
systems often use the sort-last parallel rendering scheme to achieve efﬁcient visualization. The sort object of
the sort-last pattern is pixel-level data, and it is called a sort-last because the sorting process occurs at the
end of the rendering pipeline. Sort-last rendering can obtain correct visualization results only when each
subdomain distributed in the processor cores behaves as a convex set. The sorting process of sort-middle
occurs between the geometric and rasterization processes. The sort objects of sort-middle rendering are the
Parallel visualization of large-scale multiﬁeld scientiﬁc data
1113

primitives transformed into the screen space. The sampling points converted from a subdomain within the
multiple cores can be sorted correctly in the 3D screen space. Therefore, correct sorting of primitives can be
achieved in the form of sampling points in the 3D screen space. However, global sampling and commu-
nication in multiﬁeld data, whether for opaque or semitransparent objects, can seriously affect parallel
rendering performance, which can reduce the interactive rendering speed.
4.2.2 Hybrid scheduling for parallel rendering
Considering the ﬁdelity and efﬁciency of multiﬁeld visualization, we utilize a hybrid sort-last and sort-
middle parallel rendering framework based on multiple visual channels. The framework is shown in Fig. 6,
which separates opaque and transparent objects within the visualization pipeline according to the sorting
characteristics of different rendering methods. The multiﬁeld data in two channels are rendered ﬁrst in
geometric and then optical order, and the ﬁnal visual fusing the two channel results in the optical channel. In
Fig. 6, G generally represents the geometric primitives and F represents the fragment primitives.
The ﬁrst stage is the sort-last rendering in the geometric channels, and the output pixels of this stage form
a part of the input data for the second stage. Because only opaque geometric objects are processed in this
channel, no object sorting is required here, and thus, no data communication is introduced until the ﬁnal
image is composited. We implemented the binary swap (Ma et al. 2001) and 2–3 swap (Yu et al. 2008)
algorithms for parallel image composition on any number of processor cores. However, to ensure the correct
fusion of the geometric and optical channels, the ﬁnal image needs to be sent to each processor core
responsible for initiating the optical channel rendering, which leads to an additional global broadcast
communication.
The second stage is the sort-middle rendering in the optical channels. To solve the correct parallel
rendering problem of complex subdomains with concave or even disconnected features distributed across
the multiple processor cores, four main steps are adopted in the optical channel rendering: data sampling,
redistribution of sampling data, multichannel data rendering, and image composition. (1) Each processor
core ﬁrst performs a scan converting of all the local mesh data assigned to it, which converts the local mesh
data deﬁned in the object space into the samples distributed over 3D screen space. We chose a cell-
projection method similar to Giertsen (1992) to implement this 3D rasterization process. At this point, the
Fig. 6 Hybrid scheduling scheme for parallel rendering of multiﬁeld scientiﬁc data
1114
Y. Cao et al.

sorting problem between the subdomains in the original object space is converted into a sorting problem
between the sampling points in the screen space. (2) An image partitioning strategy is used to evenly assign
the sampling data and integral computation to processor cores. After the data sampling process, each
processor is assigned an identical image region, with each region representing approximately the same
rendering load. Each processor core sends the sampling data to the processor cores that owns the corre-
sponding partitioning image area and receives the sampling data from other processor cores. As the number
of processors for visualization increases, the increasing number of processor cores creates additional
communication that will slow the process down. To minimize the all-to-all communication cost of sample
redistribution, we implement an asynchronous communication with MPI_Isend and MPI_Irecv. After this
global communication stage, each processor can then render local sampling data independently of other
processors. (3) The multichannel data rendering is then started. Because the x–y plane of the regular
sampling space we utilize is always parallel to the view plane, the relatively expensive operation of 3D ray
casting is replaced with a simpler 2D regular grid ray casting during 3D data volume rendering. That is, the
3D ray emitted by each pixel is comprised of a series of sample points with the same x-/y-coordinate, in
order of increasing z-coordinate. The sorting of the samples during multiﬁeld volume rendering considers
only the depth value, and it is independent of the complex mesh data characteristics. Thus, it can solve the
parallel translucent rendering problem of complex domains in multiphysics simulations. To integrate the
input image from the geometric channel, a ray will be terminated when it encounters the geometric
boundary in depth value that is contained in the image, and the geometric image is blended with the ray
integration result. Because both the geometric and the optical channels contain linearized depth values of the
rendered scene, the visual fusion between multiplex channels can be performed correctly. Subimages are
generated after the multichannel data rendering. (4) The ﬁnal image composition is just an image stitching
process in terms of image partitioning strategy, but it still requires a global collective communication among
subimages.
This hybrid scheduling scheme not only improves the parallel rendering efﬁciency of large-scale mul-
tiﬁeld data with opaque properties by taking advantage of data locality in the geometric channel, but also
provides accurate visualization results for the complex mesh data of multiphysics simulations. The hybrid
parallel rendering introduced in this paper takes place in the regular sampling grid; therefore, it can be used
not only for parallel rendering based on multicore processors but also for hardware-accelerated parallel
rendering to meet the needs of different typical applications.
5 Discussion
In this section, we discuss the application results of the proposed parallel visualization framework in several
typical multiphysics simulation ﬁelds. The multiﬁeld datasets used in this work were generated from the
applications of nuclear fusion energy and EM environments, as shown in Table 1. All three applications
provided in this paper use patch-based structural mesh data distribution. When using in situ visualization, a
numerical simulation determines how the data are partitioned on multicore processors. When the off-line
visualization mode is adopted, because the scale of parallel visualization is usually smaller than that of the
simulation application, a visualization process loads multiple blocks of partitioned data generated from the
simulation application by a contiguous block loading strategy.
5.1 Inertial conﬁnement fusion
Inertial conﬁnement fusion (ICF) is one of the two major routes to releasing energy by the process of nuclear
fusion for energy generation purposes. It is a type of controlled nuclear fusion that can provide a clean and
sustainable energy supply for humans. This process involves cross-coupling relations among multiple
physical processes (Simon 2007). For example, one of the core scientiﬁc problems of ICF is laser-plasma
Table 1 Multiﬁeld dataset used in the experiments
Application
Mesh size per single ﬁeld
Data size per time step
Inertial conﬁnement fusion
160 million
6.7 GB
High-power microwave device
30 million
900 MB
EM radiation shielding
8.5 billion
129 GB
Parallel visualization of large-scale multiﬁeld scientiﬁc data
1115

instability or, more speciﬁcally, the mechanisms causing the instability and how best to control them. The
cause of the instability is interaction between the laser beam and the plasma, which is affected by the
coupling of multiple physical processes such as stimulated Raman scattering, stimulated Brillouin scat-
tering, and the ﬁlament.
Currently, the 3D simulation of ICF radiation hydrodynamics applications, based on the JASMIN
parallel programming framework (Mo et al. 2010), can generate time-varying datasets of the scale of
terabytes through ﬁne physical modeling and the simulation of billions of mesh cells. Without a method for
multiﬁeld visualization that is intuitive and efﬁcient for interactive data analysis, it will be difﬁcult for
domain scientists to analyze and recognize the potential physical phenomena. To visually analyze such
multiphysics phenomena, traditional visualization (Fig. 7c) uses a geometric rendering method to represent
the ﬂuid interface instability of the target region. Isovolumetric extraction is used here to derive the
important shape of ﬂuid interface. Nevertheless, it remains difﬁcult to reﬂect fully the feature-rich trend of
ﬂuid interface in the original multiphysics ﬁeld. Because isovolumetric extraction depends on empirical
parameters such as the input isovalue, it can only give qualitative interface results when it is analyzed for
unknown laws.
Compared with the results of traditional visualization, the multicore CPU-based multiﬁeld visualization
supported by our framework (Fig. 7a) can provide a more intuitive and clear representation of the instability
for ICF domain scientists on 128 CPU cores. A total of two physical ﬁelds, the density ﬁeld of the fuel shell
and the temperature ﬁeld of ICF hot spot, are used. As shown in Fig. 7a, pseudocolor cross-sectional
(a) Multiplex multifield visualization result of fluid instability
(b) Zoomed view
(c)
Traditional visualization method
(d) AMR mesh-based patch data distribution on multi-core processors
(e) Zoomed view of  patch data distribution
Optical Channel
Geometric Channel
Fig. 7 Multicore CPU-based multiﬁeld visualization of ﬂuid instability in ICF
1116
Y. Cao et al.

rendering is adopted in the geometric channel to represent the density ﬁeld of the fuel shell. The volume
rendering method is used in the optical channel to represent the distribution of the interface instability in the
temperature ﬁeld of the hot spot. In contrast to Fig. 7c, Fig. 7a illustrates the evolution of the instability
interfaces distributed over a large numerical span for quantitative analysis, which is more important for
domain experts. As a result, domain scientists prefer to use the multiﬁeld visualization images generated by
our method for analysis. Figure 7d shows the AMR mesh-based patch data distribution on the cross section
of physical space. As shown in Fig. 7e, many subdomains comprising the same color patch data on a core
have concave or even disconnected features. Without convex mesh decomposition, the traditional method
ﬁnds it difﬁcult to obtain the correct translucent rendering results. Our multiﬁeld visualization framework
could help ICF experts rapidly compare ensemble simulation results to deepen their understanding of
hydrodynamic instability issues.
5.2 Electromagnetic environment
The EM environment refers to the overlapping of multiple EM signals distributed in the airspace, time
domain, frequency domain, and energy domain, which has certain inﬂuence on both equipment and per-
sonnel. Computational EM modeling is a typical multiphysical techniques often used in the simulation of
real-world EM applications, e.g., EM pulse radiation and scattering with irregular geometric structures.
The scale of applications in the EM environment can be divided into three levels: device level, platform
level, and regional level. Platform-level simulation is used mainly for multiscale electrical targets such as
aircraft and ships. Regional-level simulation is used for regional scenes on the scale of square kilometers,
and device-level simulation is used for components and devices on the meter–millimeter scale. Currently,
simulations of platform- and regional-scale applications based on the JASMIN framework can reach the
scale of billions to tens of billions of mesh cells, and domain scientists need intuitive visualization methods
to support the analysis of these large-scale datasets.
For the interactive analysis requirements of device-level EM applications, we adopt GPU-based mul-
tiﬁeld visualization using the multiﬁeld visualization framework proposed in this paper, and we realize real-
time interactive visual analysis of high-power microwave simulation data using an NVIDIA k6000 graphics
card. This EM model consists of 30 million mesh cells and 10 million particles, simulating a physical time
of 40 ns, which completes a device-level EM simulation on 24 CPU cores in about 10 h. A total of three
physical ﬁelds are used: the discrete ﬁeld of device conﬁguration, the 3D electron energy ﬁeld, and the 3D
distribution ﬁeld of charged particles. Figure 8a shows the multiﬁeld visualization results of tens of millions
of particles, the electron energy ﬁeld, and the complex device models that satisfy the EM analysis demand of
a device-level application. Figure 8d, e shows that the electric and particle ﬁelds have different patch data
distributions and that there is a nested structure between these two ﬁelds. In particular, the subdomains of
these two ﬁelds comprising the same color patch data have concave and ring features, as shown in Fig. 8c–e.
Compared with traditional methods, our method is more conducive to the structural design of the EM device
and diagnosis of related physical quantities (e.g., ﬁeld distribution, particle phase space, and power
monitoring).
5.3 Large-scale EM simulation application
5.3.1 Visual results
An in situ visualization method (Cao 2017) has also been employed for the ultra-scale simulation data of
EM radiation shielding for an unmanned aerial vehicle with 8.5 billion mesh cells. A total of three physical
ﬁelds are used: the discrete ﬁeld of the model of the unmanned aerial vehicle, 3D electron energy ﬁeld, and
3D magnetic ﬁeld. The multiﬁeld visualization results of this application are shown in Fig. 9a. In the ﬁgure,
a geometric rendering of the model of the unmanned aerial vehicle and the streamline rendering of 3D
magnetic ﬁeld was adopted in the geometric channel. CPU ray casting-based parallel volume rendering of
the energy distribution of the electric ﬁeld was adopted in the optical channel, which is deﬁned as optical
rendering. Compared with the results of traditional visualization, the multiﬁeld visualization framework
proposed in this paper clearly reveals the spatial coupling of the wavefront distribution and the EM direction
in the physical ﬁeld, which would help accelerate the understanding of EM experts regarding EM mech-
anisms such as radiation and scattering.
Parallel visualization of large-scale multiﬁeld scientiﬁc data
1117

As shown in Fig. 9c, d, many subdomains of EM ﬁelds comprising the same color patch data on a core
have concave or even disconnected features. Because the multiﬁeld visualization method proposed in this
paper can keep the data distribution of numerical simulation on multicore processors unchanged, there is no
costly data migration within the supercomputer. Hence, we can then take advantage of the tightly coupled
in situ visualization to support high-ﬁdelity data analysis for large-scale multiphysics applications.
5.3.2 Performance measurement
This EM application used up to 8192 CPU cores with a total simulation comprising 15,000 time steps and
187 time steps for the in situ visualization on China’s Tianhe-2 supercomputer. The total time of the
simulation and visualization when using the 8192 cores was close to 120 min. In terms of parallel visu-
alization, the in situ visualization of this example adopted the multiﬁeld visualization method presented in
this paper. The image resolution reached 1024 9 1024 pixels, and the number of samples contributing to
each pixel was 500. The EM application is a memory-constrained simulation that takes up a lot of memory
on the compute nodes. Each node of the Tianhe-2 supercomputer contains 24 CPU cores. When the large-
scale EM application is running below 2048 cores, we start no more than seven MPI processes at each node
to ensure that the numerical computation can be started normally. We tested the scalability of the large-scale
EM application from 64 cores to 8192 cores.
•
Online simulation versus off-line simulation
In this paper, online simulation is deﬁned to mean that the EM application processes the data by in situ
visualization at runtime, and only image ﬁles are output as results. Off-line simulation indicates that the
application ﬁrst outputs raw data during the period of computation and then performs post-processing after
(a) Multiplex multifield visualization result of  high-power microwave device
(b) Zoomed view
(c) Particle patch data distribution
in  vertical-axial direction  
(d) Electric patch data distribution on multi-core processors                     (e) Particle patch data distribution on multi-core processors
Fig. 8 Hardware-accelerated multiﬁeld visualization of device-level EM applications
1118
Y. Cao et al.

the simulation is completed. The performance results of the online and off-line simulations are given in
Tables 2 and 3. As Table 3 shows, at least 34 s per time step of the EM simulation was required to output
the raw data to disk in the traditional off-line simulation mode. Raw data output on less than 256 cores fails
in off-line mode, which may be due to the large amount of output data that must be output with limited
resources. Conversely, the online simulation mode, or in situ visualization with simulation applications,
avoids the signiﬁcant bottleneck of parallel input and output in the HPC systems and thus only required an
average of 2.441 s per time step on 8192 cores for the large-scale EM simulation and data analysis.
Figure 10 demonstrates that the performance of large-scale EM simulation can be increased by up to 23
times on 8192 cores, demonstrating the efﬁciency of our proposed multiﬁeld visualization method.
•
Scalability
We measured the scalability of our multiﬁeld visualization framework using simulations on 64 cores to
8192 cores of the Tianhe-2 supercomputer. A multiﬁeld parallel visualization with 8.5 billion mesh cells can
be reduced from 28 s on 64 cores to 1.89 s on 8192 cores. A maximum of 16 9 speedup of the visualization
is obtained on 4096 cores, which is lower than the 46 9 speedup obtained by the EM simulation on 8192
(a) Multiplex multifield visualization result of  EM radiation shielding application           
(b)Traditional visualization method 
(c)  Patch data distribution of  electromagnetic fields on multi-core processors 
(d) Zoomed view of the patch data distribution on a vertical section of physical space 
Fig. 9 In situ visualization result of the simulation of EM radiation shielding for an UAV on Tianhe-2 supercomputer
Parallel visualization of large-scale multiﬁeld scientiﬁc data
1119

cores. The change in the time spent on geometric and optical rendering as the number of processors
increases is shown in Table 2. In addition, the performance of the in situ visualization is presented in
Fig. 11. Here, optical rendering includes sampling, communication, and ray casting-based rendering in the
optical channel, whereas the geometric compositing and optical compositing represent the parallel image
composition in the ﬁnal stage of each channel.
As given in Table 2 and Fig. 11, the total visualization time increases as the number of cores increases
from 4098 cores to 8192 cores, indicating that the scalability of optical rendering is relatively weak.
Figure 12 shows the percentage of time spent on optical rendering increased from 20% on 64 cores to nearly
50% on 8192 cores, which is the main factor affecting scalability of the visualization. To analyze the
scalability problem, a load imbalancing model was employed to evaluate the performance of visualization,
calculated as follows:
Load imbalance ¼ Ts=Ta;
ð1Þ
where Ts represents the execution time of the slowest processor core Pi when performing a certain com-
putation and Ta represents the average execution time for all processor cores to perform a certain
computation.
Table 2 Performance breakdown of online simulation(s)
64 cores
128 cores
256 cores
512 cores
1024 cores
2048 cores
4096 cores
8192 cores
EM simulation
25.410
12.562
6.478
3.468
2.041
1.225
0.701
0.550
In situ
Geometric rendering
19.514
9.264
4.745
2.532
1.391
0.790
0.460
0.293
Geometric compositing
0.353
0.339
0.344
0.348
0.391
0.455
0.508
0.468
Optical rendering
7.110
2.998
1.628
0.964
0.680
0.562
0.565
0.913
Optical compositing
1.227
0.240
0.111
0.064
0.037
0.044
0.048
0.042
Image output
0.618
0.152
0.196
0.204
0.170
0.173
0.175
0.175
Total in situ time
28.822
12.993
7.024
4.112
2.669
2.024
1.756
1.891
Total time
54.232
25.555
13.502
7.58
4.71
3.249
2.457
2.441
Table 3 Performance measurement of off-line simulation (s)
64 cores
128 cores
256 cores
512 cores
1024 cores
2048 cores
4096 cores
8192 cores
EM simulation
25.410
12.562
6.478
3.468
2.041
1.225
0.701
0.550
Raw data output
9
9
50
48
42
34
37
57
Total time
9
9
56.478
51.468
44.041
35.225
37.701
57.55
Fig. 10 Performance measurement of the large-scale EM application
1120
Y. Cao et al.

Figure 13 shows that the load imbalance of the data sampling is higher than the sample communication.
When the scale of parallel visualization is increased from 4096 cores to 8192 cores, the load imbalance ratio
of the sample increases from 2.6 to 3.0. Because the overhead of sampling is proportional to the number of
mesh cells to be sampled, it can be seen that the mesh data on 8192 cores is unevenly distributed, which is
determined by the domain decomposition strategy of the numerical simulation. In addition to the load
imbalance of the data distribution, Fig. 14 shows that the global communication of the samples accounted
for 70% of the optical rendering time on 8192 cores, which dominated the optical rendering performance.
This should be due to the increased communications and the large number of samples to be communicated.
At present, we cannot employ a performance optimization method for communication as in Binyahib et al.
(2018), because that would lead to inaccurate mixing of the results between the geometric and optical
channels. We will optimize the performance of this phase in the future work.
Fig. 11 Performance results of the in situ visualization
Fig. 12 Time breakdown of the in situ visualization
Parallel visualization of large-scale multiﬁeld scientiﬁc data
1121

6 Conclusion
In this paper, we proposed a parallel visualization for large-scale multiﬁeld scientiﬁc data that can overcome
the two fundamental problems of multiﬁeld visualization. A multiplex visual channel-based multiﬁeld
visualization scheme was introduced and was used to solve the visual representation problem of complex
multiphysics phenomena. A hybrid scheduling framework for parallel rendering was also presented, which
was used for the scalable visualization of large-scale multiﬁeld data. The framework has been applied in
typical applications, i.e., fusion energy and EM environments. Compared with the traditional method, the
visualization images obtained using our proposed framework are considered more intuitive and of higher
ﬁdelity. Our proposed parallel visualization scheme is scalable because it preserves the original mesh
partitioning, making it suitable for large-scale multiphysics simulation and data analysis on a
supercomputer.
Acknowledgements This work was supported by the National Key R&D Program of China under Grant No.
2017YFB0202203 and the Defense Industrial Technology Development Program of China (Grant No. C1520110002).
References
Bertin J (1983) Semiology of graphics: diagrams. In: Conference on computer networks
Fig. 13 Load imbalance of the sampling-based volume rendering
Fig. 14 Time breakdown of the optical rendering
1122
Y. Cao et al.

Binyahib R, Peterka T, Larsen M, Ma K-L, Childs H (2018) A scalable hybrid scheme for ray-casting of unstructured volume
data. IEEE Trans Vis Comput Graph. https://doi.org/10.1109/tvcg.2018.2833113
Cai W, Sakas G (1999) Data intermixing and multi-volume rendering. Computer Graph Forum 18(3):359–368
Cao Y et al (2017) In situ visualization infrastructure for large scale simulations with structured meshes. In: International
conferences computer graphics, visualization, computer vision and image processing. IADIS Press, Portugal, Lisbon,
pp 139–146
Childs H, Duchaineau MA, Ma K-L (2006) A scalable, hybrid scheme for volume rendering massive data sets, pp 153–161.
https://doi.org/10.2312/egpgv/egpgv06/153-161
Eberly DH (2006) 3D game engine design: a practical approach to real-time computer graphics. Morgan Kaufmann Publishers,
Burlington, p 69 (ISBN 0122290631)
Exascale
Programming
Challenges
(2011).
http://science.energy.gov/*/media/ascr/pdf/program-documents/docs/
ProgrammingChallengesWorkshopReport.pdf
Fuchs R, Hauser H (2009) Visualization of multivariate scientiﬁc data. Computer Graph Forum CGF 28:1670–1690. https://
doi.org/10.1111/j.1467-8659.2009.01429.x
Giertsen C (1992) Volume visualization of sparse irregular meshes. IEEE Computer Graphics and Applications 12(2):40–48
HVS model (2019). https://en.wikipedia.org/wiki/Human_visual_system_model
Insley JA, Grinberg L et al (2011) Visualizing multiscale, multiphysics simulation data: brain blood ﬂow. In: IEEE symposium
on large-scale data analysis and visualization, pp 3–7
Jacq J, Roux C (1997) A direct multi-volume rendering method aiming at comparisons of 3-D images and models. IEEE Trans
Inf Technol Biomed 1:30–43
Kajiya J, Von Herzen B (1984) Ray tracing volume densities. Proc SIGGRAPH 18(3):165–174
Kniss J, Premoze S, Ikits M, Lefohn A, Hansen C, Praun E (2003) Gaussian transfer functions for multi-ﬁeld volume
visualization. In: Proceedings of the IEEE visualization conference, pp 497–504. https://doi.org/10.1109/VISUAL.2003.
1250412
Kreeger KA, Kaufman AE (1999) Mixing translucent polygons with volumes. In: Proceedings of IEEE visualization,
pp 191–198
Lethbridge P (2005) Multiphysics analysis. The Industrial Physicist, Hudson, Ohio
Levoy M (1990) A hybrid ray tracer for rendering polygon and volume data. IEEE Comput Graph Appl 10(2):33–40
Ma K-L, Crockett TW (2001) A scalable parallel cell-projection volume rendering algorithm for three-dimensional
unstructured data. https://doi.org/10.1145/266638.266664
Ma K-L, Painter S, James D, Hansen C, Krogh MF (2001) Parallel volume rendering using binary-swap image composition.
IEEE CG&A, p 14. https://doi.org/10.1145/1508044.1508082
Mo Z, Zhang A, Cao X, Liu Q, Xu X, An H, Pei W, Zhu S (2010) JASMIN: a parallel software infrastructure for scientiﬁc
computing. Front Comput Sci China 4:480–488
Molnar S et al (1994) A sorting classiﬁcation of parallel rendering. IEEE Comput Graph Appl 14(4):23–32
Plate J et al (2007) A ﬂexible multivolume shader framework for arbitrarily intersecting multiresolution datasets. IEEE Trans
Vis Comput Graph 13(6):1584–1591
Porter T, Du T (1984) Compositing digital images. In: SIGGRAPH’84: Proceedings of the 11th annual conference on
Computer graphics and interactive techniques. ACM Press, New York, NY, USA, pp 253–259
Rivi M, Calori L, Muscianisi G, Slavnic´ V (2012) In-situ visualization: state-of-the-art and some use cases. PRACE white
paper
Sauber N, Theisel H, Seidel H-P (2006) Multiﬁeld-graphs: an approach to visualizing correlations in multiﬁeld scalar Data.
IEEE Trans Vis Comput Graph 12:917–924. https://doi.org/10.1109/TVCG.2006.165
Simon H (2007) Modeling and simulation at the Exascale for energy and the environment. DOE report. http://www.sc.doe.gov/
ascr/ProgramDocuments/ProgDocs.html
Usher W, Wald I, Amstutz J, Gu¨nther J, Brownlee C, Pascucci V (2019) Scalable ray tracing using the distributed framebuffer.
Computer Graphics Forum (proceedings of EuroVis) (to appear)
Wang F, Wald I, Wu Q, Usher W, Johnson CR (2018) CPU isosurface ray tracing of adaptive mesh reﬁnement data. IEEE
Trans Vis Comput Graph. https://doi.org/10.1109/tvcg.2018.2864850
Wes Bethel E (2009) Modern scientiﬁc visualization is more than just pretty pictures. Numerical modeling of space plasma
ﬂows: Astronum-2008 (Astronomical Society of the Paciﬁc conference series, St. Thomas, USVI, June 2009, pp 301–317,
LBNL 1450E)
Yu H, Wang C, Ma K-L (2008) Massively parallel volume rendering using 2–3 swap image compositing. In: Proceedings of
ACM/IEEE supercomputing conference, Austin, TX, pp 48-1–48-11
Yu H, Wang C, Grout R, Chen J, Ma K-L (2010) In situ visualization for large-scale combustion simulations. IEEE Comput
Graph Appl 30:45–57. https://doi.org/10.1109/MCG.2010.55
Zhang A, Mo Z et al (2013) Federation parallel computing in JASMIN and its application in multi-physics simulation. Comput
Eng Sci 35(1):15–23
Publisher’s Note
Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional
afﬁliations.
Parallel visualization of large-scale multiﬁeld scientiﬁc data
1123

